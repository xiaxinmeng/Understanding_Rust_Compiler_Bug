{"sha": "be304afc8c2b1a364bd406888b5378897ed82a9f", "node_id": "MDY6Q29tbWl0NzI0NzEyOmJlMzA0YWZjOGMyYjFhMzY0YmQ0MDY4ODhiNTM3ODg5N2VkODJhOWY=", "commit": {"author": {"name": "bors", "email": "bors@rust-lang.org", "date": "2017-03-04T05:44:12Z"}, "committer": {"name": "bors", "email": "bors@rust-lang.org", "date": "2017-03-04T05:44:12Z"}, "message": "Auto merge of #40202 - jseyfried:integrate_tokenstream, r=nrc\n\nsyntax: integrate `TokenStream`\n\nUse `TokenStream` instead of `Vec<TokenTree>` in `TokenTree::Delimited` and elsewhere.\nr? @nrc", "tree": {"sha": "442e2432aa307ac81e894e00aee33bd5a02b1c0b", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/442e2432aa307ac81e894e00aee33bd5a02b1c0b"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/be304afc8c2b1a364bd406888b5378897ed82a9f", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/be304afc8c2b1a364bd406888b5378897ed82a9f", "html_url": "https://github.com/rust-lang/rust/commit/be304afc8c2b1a364bd406888b5378897ed82a9f", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/be304afc8c2b1a364bd406888b5378897ed82a9f/comments", "author": {"login": "bors", "id": 3372342, "node_id": "MDQ6VXNlcjMzNzIzNDI=", "avatar_url": "https://avatars.githubusercontent.com/u/3372342?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bors", "html_url": "https://github.com/bors", "followers_url": "https://api.github.com/users/bors/followers", "following_url": "https://api.github.com/users/bors/following{/other_user}", "gists_url": "https://api.github.com/users/bors/gists{/gist_id}", "starred_url": "https://api.github.com/users/bors/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bors/subscriptions", "organizations_url": "https://api.github.com/users/bors/orgs", "repos_url": "https://api.github.com/users/bors/repos", "events_url": "https://api.github.com/users/bors/events{/privacy}", "received_events_url": "https://api.github.com/users/bors/received_events", "type": "User", "site_admin": false}, "committer": {"login": "bors", "id": 3372342, "node_id": "MDQ6VXNlcjMzNzIzNDI=", "avatar_url": "https://avatars.githubusercontent.com/u/3372342?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bors", "html_url": "https://github.com/bors", "followers_url": "https://api.github.com/users/bors/followers", "following_url": "https://api.github.com/users/bors/following{/other_user}", "gists_url": "https://api.github.com/users/bors/gists{/gist_id}", "starred_url": "https://api.github.com/users/bors/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bors/subscriptions", "organizations_url": "https://api.github.com/users/bors/orgs", "repos_url": "https://api.github.com/users/bors/repos", "events_url": "https://api.github.com/users/bors/events{/privacy}", "received_events_url": "https://api.github.com/users/bors/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "8c6c0f80a399e61d8454bb449962103449b52739", "url": "https://api.github.com/repos/rust-lang/rust/commits/8c6c0f80a399e61d8454bb449962103449b52739", "html_url": "https://github.com/rust-lang/rust/commit/8c6c0f80a399e61d8454bb449962103449b52739"}, {"sha": "0d554139ad7a54bbd59a5166cc3e9ff7842c5266", "url": "https://api.github.com/repos/rust-lang/rust/commits/0d554139ad7a54bbd59a5166cc3e9ff7842c5266", "html_url": "https://github.com/rust-lang/rust/commit/0d554139ad7a54bbd59a5166cc3e9ff7842c5266"}], "stats": {"total": 1090, "additions": 555, "deletions": 535}, "files": [{"sha": "8d7fe655c23b2a9b8cf4e30274160660aea1d51a", "filename": "src/libproc_macro/lib.rs", "status": "modified", "additions": 3, "deletions": 4, "changes": 7, "blob_url": "https://github.com/rust-lang/rust/blob/be304afc8c2b1a364bd406888b5378897ed82a9f/src%2Flibproc_macro%2Flib.rs", "raw_url": "https://github.com/rust-lang/rust/raw/be304afc8c2b1a364bd406888b5378897ed82a9f/src%2Flibproc_macro%2Flib.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibproc_macro%2Flib.rs?ref=be304afc8c2b1a364bd406888b5378897ed82a9f", "patch": "@@ -101,7 +101,7 @@ pub mod __internal {\n \n     pub fn token_stream_parse_items(stream: TokenStream) -> Result<Vec<P<ast::Item>>, LexError> {\n         with_parse_sess(move |sess| {\n-            let mut parser = parse::new_parser_from_ts(sess, stream.inner);\n+            let mut parser = parse::stream_to_parser(sess, stream.inner);\n             let mut items = Vec::new();\n \n             while let Some(item) = try!(parser.parse_item().map_err(super::parse_to_lex_err)) {\n@@ -177,9 +177,8 @@ impl FromStr for TokenStream {\n         __internal::with_parse_sess(|sess| {\n             let src = src.to_string();\n             let name = \"<proc-macro source code>\".to_string();\n-            let tts = parse::parse_tts_from_source_str(name, src, sess);\n-\n-            Ok(__internal::token_stream_wrap(tts.into_iter().collect()))\n+            let stream = parse::parse_stream_from_source_str(name, src, sess);\n+            Ok(__internal::token_stream_wrap(stream))\n         })\n     }\n }"}, {"sha": "0276587ed52b1b7ca55179766580fba50c84cbbb", "filename": "src/libproc_macro_plugin/qquote.rs", "status": "modified", "additions": 10, "deletions": 20, "changes": 30, "blob_url": "https://github.com/rust-lang/rust/blob/be304afc8c2b1a364bd406888b5378897ed82a9f/src%2Flibproc_macro_plugin%2Fqquote.rs", "raw_url": "https://github.com/rust-lang/rust/raw/be304afc8c2b1a364bd406888b5378897ed82a9f/src%2Flibproc_macro_plugin%2Fqquote.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibproc_macro_plugin%2Fqquote.rs?ref=be304afc8c2b1a364bd406888b5378897ed82a9f", "patch": "@@ -17,7 +17,7 @@ use syntax::symbol::Symbol;\n use syntax::tokenstream::{self, Delimited, TokenTree, TokenStream};\n use syntax_pos::DUMMY_SP;\n \n-use std::rc::Rc;\n+use std::iter;\n \n pub fn qquote<'cx>(stream: TokenStream) -> TokenStream {\n     stream.quote()\n@@ -49,10 +49,7 @@ macro_rules! quote_tree {\n }\n \n fn delimit(delim: token::DelimToken, stream: TokenStream) -> TokenStream {\n-    TokenTree::Delimited(DUMMY_SP, Rc::new(Delimited {\n-        delim: delim,\n-        tts: stream.trees().cloned().collect(),\n-    })).into()\n+    TokenTree::Delimited(DUMMY_SP, Delimited { delim: delim, tts: stream.into() }).into()\n }\n \n macro_rules! quote {\n@@ -75,9 +72,9 @@ impl Quote for TokenStream {\n             return quote!(::syntax::tokenstream::TokenStream::empty());\n         }\n \n-        struct Quote<'a>(tokenstream::Cursor<'a>);\n+        struct Quote(iter::Peekable<tokenstream::Cursor>);\n \n-        impl<'a> Iterator for Quote<'a> {\n+        impl Iterator for Quote {\n             type Item = TokenStream;\n \n             fn next(&mut self) -> Option<TokenStream> {\n@@ -89,25 +86,18 @@ impl Quote for TokenStream {\n                     _ => false,\n                 };\n \n-                self.0.next().cloned().map(|tree| {\n+                self.0.next().map(|tree| {\n                     let quoted_tree = if is_unquote { tree.into() } else { tree.quote() };\n                     quote!(::syntax::tokenstream::TokenStream::from((unquote quoted_tree)),)\n                 })\n             }\n         }\n \n-        let quoted = Quote(self.trees()).collect::<TokenStream>();\n+        let quoted = Quote(self.trees().peekable()).collect::<TokenStream>();\n         quote!([(unquote quoted)].iter().cloned().collect::<::syntax::tokenstream::TokenStream>())\n     }\n }\n \n-impl Quote for Vec<TokenTree> {\n-    fn quote(&self) -> TokenStream {\n-        let stream = self.iter().cloned().collect::<TokenStream>();\n-        quote!((quote stream).trees().cloned().collect::<::std::vec::Vec<_> >())\n-    }\n-}\n-\n impl Quote for TokenTree {\n     fn quote(&self) -> TokenStream {\n         match *self {\n@@ -123,12 +113,12 @@ impl Quote for TokenTree {\n     }\n }\n \n-impl Quote for Rc<Delimited> {\n+impl Quote for Delimited {\n     fn quote(&self) -> TokenStream {\n-        quote!(::std::rc::Rc::new(::syntax::tokenstream::Delimited {\n+        quote!(::syntax::tokenstream::Delimited {\n             delim: (quote self.delim),\n-            tts: (quote self.tts),\n-        }))\n+            tts: (quote self.stream()).into(),\n+        })\n     }\n }\n "}, {"sha": "22bc28eb3fec0a00e90a08bdb4fc7af268c5fa1a", "filename": "src/librustc/hir/mod.rs", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/rust-lang/rust/blob/be304afc8c2b1a364bd406888b5378897ed82a9f/src%2Flibrustc%2Fhir%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/be304afc8c2b1a364bd406888b5378897ed82a9f/src%2Flibrustc%2Fhir%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Fhir%2Fmod.rs?ref=be304afc8c2b1a364bd406888b5378897ed82a9f", "patch": "@@ -40,7 +40,7 @@ use syntax::ast::{Ident, Name, NodeId, DUMMY_NODE_ID, AsmDialect};\n use syntax::ast::{Attribute, Lit, StrStyle, FloatTy, IntTy, UintTy, MetaItem};\n use syntax::ptr::P;\n use syntax::symbol::{Symbol, keywords};\n-use syntax::tokenstream::TokenTree;\n+use syntax::tokenstream::TokenStream;\n use syntax::util::ThinVec;\n \n use std::collections::BTreeMap;\n@@ -471,7 +471,7 @@ pub struct MacroDef {\n     pub attrs: HirVec<Attribute>,\n     pub id: NodeId,\n     pub span: Span,\n-    pub body: HirVec<TokenTree>,\n+    pub body: TokenStream,\n }\n \n #[derive(Clone, PartialEq, Eq, RustcEncodable, RustcDecodable, Hash, Debug)]"}, {"sha": "d0eedcac0c06a6fa041f3629bbf9297241b16116", "filename": "src/librustc_incremental/calculate_svh/svh_visitor.rs", "status": "modified", "additions": 5, "deletions": 11, "changes": 16, "blob_url": "https://github.com/rust-lang/rust/blob/be304afc8c2b1a364bd406888b5378897ed82a9f/src%2Flibrustc_incremental%2Fcalculate_svh%2Fsvh_visitor.rs", "raw_url": "https://github.com/rust-lang/rust/raw/be304afc8c2b1a364bd406888b5378897ed82a9f/src%2Flibrustc_incremental%2Fcalculate_svh%2Fsvh_visitor.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_incremental%2Fcalculate_svh%2Fsvh_visitor.rs?ref=be304afc8c2b1a364bd406888b5378897ed82a9f", "patch": "@@ -866,8 +866,8 @@ impl<'a, 'hash, 'tcx> visit::Visitor<'tcx> for StrictVersionHashVisitor<'a, 'has\n         debug!(\"visit_macro_def: st={:?}\", self.st);\n         SawMacroDef.hash(self.st);\n         hash_attrs!(self, &macro_def.attrs);\n-        for tt in &macro_def.body {\n-            self.hash_token_tree(tt);\n+        for tt in macro_def.body.trees() {\n+            self.hash_token_tree(&tt);\n         }\n         visit::walk_macro_def(self, macro_def)\n     }\n@@ -1033,15 +1033,9 @@ impl<'a, 'hash, 'tcx> StrictVersionHashVisitor<'a, 'hash, 'tcx> {\n             }\n             tokenstream::TokenTree::Delimited(span, ref delimited) => {\n                 hash_span!(self, span);\n-                let tokenstream::Delimited {\n-                    ref delim,\n-                    ref tts,\n-                } = **delimited;\n-\n-                delim.hash(self.st);\n-                tts.len().hash(self.st);\n-                for sub_tt in tts {\n-                    self.hash_token_tree(sub_tt);\n+                delimited.delim.hash(self.st);\n+                for sub_tt in delimited.stream().trees() {\n+                    self.hash_token_tree(&sub_tt);\n                 }\n             }\n         }"}, {"sha": "cf2219e0e3df51c5aaf58764232b247be88e0882", "filename": "src/librustc_metadata/cstore_impl.rs", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "blob_url": "https://github.com/rust-lang/rust/blob/be304afc8c2b1a364bd406888b5378897ed82a9f/src%2Flibrustc_metadata%2Fcstore_impl.rs", "raw_url": "https://github.com/rust-lang/rust/raw/be304afc8c2b1a364bd406888b5378897ed82a9f/src%2Flibrustc_metadata%2Fcstore_impl.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_metadata%2Fcstore_impl.rs?ref=be304afc8c2b1a364bd406888b5378897ed82a9f", "patch": "@@ -34,7 +34,7 @@ use std::rc::Rc;\n \n use syntax::ast;\n use syntax::attr;\n-use syntax::parse::filemap_to_tts;\n+use syntax::parse::filemap_to_stream;\n use syntax::symbol::Symbol;\n use syntax_pos::{mk_sp, Span};\n use rustc::hir::svh::Svh;\n@@ -401,7 +401,7 @@ impl CrateStore for cstore::CStore {\n \n         let filemap = sess.parse_sess.codemap().new_filemap(source_name, None, def.body);\n         let local_span = mk_sp(filemap.start_pos, filemap.end_pos);\n-        let body = filemap_to_tts(&sess.parse_sess, filemap);\n+        let body = filemap_to_stream(&sess.parse_sess, filemap);\n \n         // Mark the attrs as used\n         let attrs = data.get_item_attrs(id.index);\n@@ -419,7 +419,7 @@ impl CrateStore for cstore::CStore {\n             id: ast::DUMMY_NODE_ID,\n             span: local_span,\n             attrs: attrs,\n-            body: body,\n+            body: body.into(),\n         })\n     }\n "}, {"sha": "8ddc1642d9e1c2c77451118c3c21887148f1ec74", "filename": "src/librustc_metadata/encoder.rs", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "blob_url": "https://github.com/rust-lang/rust/blob/be304afc8c2b1a364bd406888b5378897ed82a9f/src%2Flibrustc_metadata%2Fencoder.rs", "raw_url": "https://github.com/rust-lang/rust/raw/be304afc8c2b1a364bd406888b5378897ed82a9f/src%2Flibrustc_metadata%2Fencoder.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_metadata%2Fencoder.rs?ref=be304afc8c2b1a364bd406888b5378897ed82a9f", "patch": "@@ -853,9 +853,10 @@ impl<'a, 'tcx> EncodeContext<'a, 'tcx> {\n \n     /// Serialize the text of exported macros\n     fn encode_info_for_macro_def(&mut self, macro_def: &hir::MacroDef) -> Entry<'tcx> {\n+        use syntax::print::pprust;\n         Entry {\n             kind: EntryKind::MacroDef(self.lazy(&MacroDef {\n-                body: ::syntax::print::pprust::tts_to_string(&macro_def.body)\n+                body: pprust::tts_to_string(&macro_def.body.trees().collect::<Vec<_>>()),\n             })),\n             visibility: self.lazy(&ty::Visibility::Public),\n             span: self.lazy(&macro_def.span),"}, {"sha": "751f59d0290ac2b94e610e966846c1d75794bbd9", "filename": "src/librustc_resolve/build_reduced_graph.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/be304afc8c2b1a364bd406888b5378897ed82a9f/src%2Flibrustc_resolve%2Fbuild_reduced_graph.rs", "raw_url": "https://github.com/rust-lang/rust/raw/be304afc8c2b1a364bd406888b5378897ed82a9f/src%2Flibrustc_resolve%2Fbuild_reduced_graph.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_resolve%2Fbuild_reduced_graph.rs?ref=be304afc8c2b1a364bd406888b5378897ed82a9f", "patch": "@@ -516,7 +516,7 @@ impl<'a> Resolver<'a> {\n             expansion: Cell::new(LegacyScope::Empty),\n         });\n         self.invocations.insert(mark, invocation);\n-        macro_rules.body = mark_tts(&macro_rules.body, mark);\n+        macro_rules.body = mark_tts(macro_rules.stream(), mark).into();\n         let ext = Rc::new(macro_rules::compile(&self.session.parse_sess, &macro_rules));\n         self.macro_map.insert(def_id, ext.clone());\n         ext"}, {"sha": "36645418d4f785fca47d0f3e4b30cdd3fee7a29b", "filename": "src/librustc_resolve/macros.rs", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/rust-lang/rust/blob/be304afc8c2b1a364bd406888b5378897ed82a9f/src%2Flibrustc_resolve%2Fmacros.rs", "raw_url": "https://github.com/rust-lang/rust/raw/be304afc8c2b1a364bd406888b5378897ed82a9f/src%2Flibrustc_resolve%2Fmacros.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_resolve%2Fmacros.rs?ref=be304afc8c2b1a364bd406888b5378897ed82a9f", "patch": "@@ -545,7 +545,7 @@ impl<'a> Resolver<'a> {\n \n     pub fn define_macro(&mut self, item: &ast::Item, legacy_scope: &mut LegacyScope<'a>) {\n         let tts = match item.node {\n-            ast::ItemKind::Mac(ref mac) => &mac.node.tts,\n+            ast::ItemKind::Mac(ref mac) => mac.node.stream(),\n             _ => unreachable!(),\n         };\n \n@@ -562,7 +562,7 @@ impl<'a> Resolver<'a> {\n             attrs: item.attrs.clone(),\n             id: ast::DUMMY_NODE_ID,\n             span: item.span,\n-            body: mark_tts(tts, mark),\n+            body: mark_tts(tts, mark).into(),\n         };\n \n         *legacy_scope = LegacyScope::Binding(self.arenas.alloc_legacy_binding(LegacyBinding {"}, {"sha": "34402742e6c3325c18bd749309c082785cd1e9f9", "filename": "src/librustc_save_analysis/span_utils.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/be304afc8c2b1a364bd406888b5378897ed82a9f/src%2Flibrustc_save_analysis%2Fspan_utils.rs", "raw_url": "https://github.com/rust-lang/rust/raw/be304afc8c2b1a364bd406888b5378897ed82a9f/src%2Flibrustc_save_analysis%2Fspan_utils.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_save_analysis%2Fspan_utils.rs?ref=be304afc8c2b1a364bd406888b5378897ed82a9f", "patch": "@@ -284,7 +284,7 @@ impl<'a> SpanUtils<'a> {\n     pub fn signature_string_for_span(&self, span: Span) -> String {\n         let mut toks = self.retokenise_span(span);\n         toks.real_token();\n-        let mut toks = toks.parse_all_token_trees().unwrap().into_iter();\n+        let mut toks = toks.parse_all_token_trees().unwrap().trees();\n         let mut prev = toks.next().unwrap();\n \n         let first_span = prev.span();"}, {"sha": "42928427233d780723a0996911e96dabfe6446e3", "filename": "src/librustdoc/visit_ast.rs", "status": "modified", "additions": 4, "deletions": 2, "changes": 6, "blob_url": "https://github.com/rust-lang/rust/blob/be304afc8c2b1a364bd406888b5378897ed82a9f/src%2Flibrustdoc%2Fvisit_ast.rs", "raw_url": "https://github.com/rust-lang/rust/raw/be304afc8c2b1a364bd406888b5378897ed82a9f/src%2Flibrustdoc%2Fvisit_ast.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustdoc%2Fvisit_ast.rs?ref=be304afc8c2b1a364bd406888b5378897ed82a9f", "patch": "@@ -211,7 +211,8 @@ impl<'a, 'tcx> RustdocVisitor<'a, 'tcx> {\n                     };\n \n                     // FIXME(jseyfried) merge with `self.visit_macro()`\n-                    let matchers = def.body.chunks(4).map(|arm| arm[0].span()).collect();\n+                    let tts = def.stream().trees().collect::<Vec<_>>();\n+                    let matchers = tts.chunks(4).map(|arm| arm[0].span()).collect();\n                     om.macros.push(Macro {\n                         def_id: def_id,\n                         attrs: def.attrs.clone().into(),\n@@ -520,8 +521,9 @@ impl<'a, 'tcx> RustdocVisitor<'a, 'tcx> {\n \n     // convert each exported_macro into a doc item\n     fn visit_local_macro(&self, def: &hir::MacroDef) -> Macro {\n+        let tts = def.body.trees().collect::<Vec<_>>();\n         // Extract the spans of all matchers. They represent the \"interface\" of the macro.\n-        let matchers = def.body.chunks(4).map(|arm| arm[0].span()).collect();\n+        let matchers = tts.chunks(4).map(|arm| arm[0].span()).collect();\n \n         Macro {\n             def_id: self.cx.tcx.hir.local_def_id(def.id),"}, {"sha": "9cc754cbf4d1912590251b479d80716f69f35f68", "filename": "src/libsyntax/ast.rs", "status": "modified", "additions": 15, "deletions": 3, "changes": 18, "blob_url": "https://github.com/rust-lang/rust/blob/be304afc8c2b1a364bd406888b5378897ed82a9f/src%2Flibsyntax%2Fast.rs", "raw_url": "https://github.com/rust-lang/rust/raw/be304afc8c2b1a364bd406888b5378897ed82a9f/src%2Flibsyntax%2Fast.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fast.rs?ref=be304afc8c2b1a364bd406888b5378897ed82a9f", "patch": "@@ -24,7 +24,7 @@ use ext::hygiene::SyntaxContext;\n use print::pprust;\n use ptr::P;\n use symbol::{Symbol, keywords};\n-use tokenstream::{TokenTree};\n+use tokenstream::{ThinTokenStream, TokenStream};\n \n use std::collections::HashSet;\n use std::fmt;\n@@ -1033,7 +1033,13 @@ pub type Mac = Spanned<Mac_>;\n #[derive(Clone, PartialEq, Eq, RustcEncodable, RustcDecodable, Hash, Debug)]\n pub struct Mac_ {\n     pub path: Path,\n-    pub tts: Vec<TokenTree>,\n+    pub tts: ThinTokenStream,\n+}\n+\n+impl Mac_ {\n+    pub fn stream(&self) -> TokenStream {\n+        self.tts.clone().into()\n+    }\n }\n \n #[derive(Clone, PartialEq, Eq, RustcEncodable, RustcDecodable, Hash, Debug, Copy)]\n@@ -1915,7 +1921,13 @@ pub struct MacroDef {\n     pub attrs: Vec<Attribute>,\n     pub id: NodeId,\n     pub span: Span,\n-    pub body: Vec<TokenTree>,\n+    pub body: ThinTokenStream,\n+}\n+\n+impl MacroDef {\n+    pub fn stream(&self) -> TokenStream {\n+        self.body.clone().into()\n+    }\n }\n \n #[cfg(test)]"}, {"sha": "e242cf2777fe59dd00fbed82820a06848bb39c82", "filename": "src/libsyntax/ext/base.rs", "status": "modified", "additions": 6, "deletions": 14, "changes": 20, "blob_url": "https://github.com/rust-lang/rust/blob/be304afc8c2b1a364bd406888b5378897ed82a9f/src%2Flibsyntax%2Fext%2Fbase.rs", "raw_url": "https://github.com/rust-lang/rust/raw/be304afc8c2b1a364bd406888b5378897ed82a9f/src%2Flibsyntax%2Fext%2Fbase.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fext%2Fbase.rs?ref=be304afc8c2b1a364bd406888b5378897ed82a9f", "patch": "@@ -188,10 +188,7 @@ impl<F> AttrProcMacro for F\n \n /// Represents a thing that maps token trees to Macro Results\n pub trait TTMacroExpander {\n-    fn expand<'cx>(&self,\n-                   ecx: &'cx mut ExtCtxt,\n-                   span: Span,\n-                   token_tree: &[tokenstream::TokenTree])\n+    fn expand<'cx>(&self, ecx: &'cx mut ExtCtxt, span: Span, input: TokenStream)\n                    -> Box<MacResult+'cx>;\n }\n \n@@ -200,15 +197,11 @@ pub type MacroExpanderFn =\n                 -> Box<MacResult+'cx>;\n \n impl<F> TTMacroExpander for F\n-    where F : for<'cx> Fn(&'cx mut ExtCtxt, Span, &[tokenstream::TokenTree])\n-                          -> Box<MacResult+'cx>\n+    where F: for<'cx> Fn(&'cx mut ExtCtxt, Span, &[tokenstream::TokenTree]) -> Box<MacResult+'cx>\n {\n-    fn expand<'cx>(&self,\n-                   ecx: &'cx mut ExtCtxt,\n-                   span: Span,\n-                   token_tree: &[tokenstream::TokenTree])\n+    fn expand<'cx>(&self, ecx: &'cx mut ExtCtxt, span: Span, input: TokenStream)\n                    -> Box<MacResult+'cx> {\n-        (*self)(ecx, span, token_tree)\n+        (*self)(ecx, span, &input.trees().collect::<Vec<_>>())\n     }\n }\n \n@@ -654,9 +647,8 @@ impl<'a> ExtCtxt<'a> {\n         expand::MacroExpander::new(self, true)\n     }\n \n-    pub fn new_parser_from_tts(&self, tts: &[tokenstream::TokenTree])\n-        -> parser::Parser<'a> {\n-        parse::tts_to_parser(self.parse_sess, tts.to_vec())\n+    pub fn new_parser_from_tts(&self, tts: &[tokenstream::TokenTree]) -> parser::Parser<'a> {\n+        parse::stream_to_parser(self.parse_sess, tts.iter().cloned().collect())\n     }\n     pub fn codemap(&self) -> &'a CodeMap { self.parse_sess.codemap() }\n     pub fn parse_sess(&self) -> &'a parse::ParseSess { self.parse_sess }"}, {"sha": "f1662284a88206657283952e9c444e4c5ea39397", "filename": "src/libsyntax/ext/expand.rs", "status": "modified", "additions": 20, "deletions": 21, "changes": 41, "blob_url": "https://github.com/rust-lang/rust/blob/be304afc8c2b1a364bd406888b5378897ed82a9f/src%2Flibsyntax%2Fext%2Fexpand.rs", "raw_url": "https://github.com/rust-lang/rust/raw/be304afc8c2b1a364bd406888b5378897ed82a9f/src%2Flibsyntax%2Fext%2Fexpand.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fext%2Fexpand.rs?ref=be304afc8c2b1a364bd406888b5378897ed82a9f", "patch": "@@ -8,7 +8,7 @@\n // option. This file may not be copied, modified, or distributed\n // except according to those terms.\n \n-use ast::{self, Block, Ident, Mac_, PatKind};\n+use ast::{self, Block, Ident, PatKind};\n use ast::{Name, MacStmtStyle, StmtKind, ItemKind};\n use attr::{self, HasAttrs};\n use codemap::{ExpnInfo, NameAndSpan, MacroBang, MacroAttribute};\n@@ -20,16 +20,15 @@ use ext::placeholders::{placeholder, PlaceholderExpander};\n use feature_gate::{self, Features, is_builtin_attr};\n use fold;\n use fold::*;\n+use parse::{filemap_to_stream, ParseSess, DirectoryOwnership, PResult, token};\n use parse::parser::Parser;\n-use parse::token;\n-use parse::{ParseSess, DirectoryOwnership, PResult, filemap_to_tts};\n use print::pprust;\n use ptr::P;\n use std_inject;\n use symbol::Symbol;\n use symbol::keywords;\n use syntax_pos::{self, Span, ExpnId};\n-use tokenstream::{TokenTree, TokenStream};\n+use tokenstream::TokenStream;\n use util::small_vector::SmallVector;\n use visit::Visitor;\n \n@@ -462,8 +461,8 @@ impl<'a, 'b> MacroExpander<'a, 'b> {\n                 kind.expect_from_annotatables(items)\n             }\n             SyntaxExtension::AttrProcMacro(ref mac) => {\n-                let attr_toks = tts_for_attr_args(&attr, &self.cx.parse_sess).into_iter().collect();\n-                let item_toks = tts_for_item(&item, &self.cx.parse_sess).into_iter().collect();\n+                let attr_toks = stream_for_attr_args(&attr, &self.cx.parse_sess);\n+                let item_toks = stream_for_item(&item, &self.cx.parse_sess);\n \n                 let tok_result = mac.expand(self.cx, attr.span, attr_toks, item_toks);\n                 self.parse_expansion(tok_result, kind, name, attr.span)\n@@ -487,11 +486,11 @@ impl<'a, 'b> MacroExpander<'a, 'b> {\n             InvocationKind::Bang { mac, ident, span } => (mac, ident, span),\n             _ => unreachable!(),\n         };\n-        let Mac_ { path, tts, .. } = mac.node;\n+        let path = &mac.node.path;\n \n         let extname = path.segments.last().unwrap().identifier.name;\n         let ident = ident.unwrap_or(keywords::Invalid.ident());\n-        let marked_tts = mark_tts(&tts, mark);\n+        let marked_tts = mark_tts(mac.node.stream(), mark);\n         let opt_expanded = match *ext {\n             NormalTT(ref expandfun, exp_span, allow_internal_unstable) => {\n                 if ident.name != keywords::Invalid.name() {\n@@ -510,7 +509,7 @@ impl<'a, 'b> MacroExpander<'a, 'b> {\n                     },\n                 });\n \n-                kind.make_from(expandfun.expand(self.cx, span, &marked_tts))\n+                kind.make_from(expandfun.expand(self.cx, span, marked_tts))\n             }\n \n             IdentTT(ref expander, tt_span, allow_internal_unstable) => {\n@@ -529,7 +528,8 @@ impl<'a, 'b> MacroExpander<'a, 'b> {\n                     }\n                 });\n \n-                kind.make_from(expander.expand(self.cx, span, ident, marked_tts))\n+                let input: Vec<_> = marked_tts.into_trees().collect();\n+                kind.make_from(expander.expand(self.cx, span, ident, input))\n             }\n \n             MultiDecorator(..) | MultiModifier(..) | SyntaxExtension::AttrProcMacro(..) => {\n@@ -563,8 +563,7 @@ impl<'a, 'b> MacroExpander<'a, 'b> {\n                     },\n                 });\n \n-                let toks = marked_tts.into_iter().collect();\n-                let tok_result = expandfun.expand(self.cx, span, toks);\n+                let tok_result = expandfun.expand(self.cx, span, marked_tts);\n                 Some(self.parse_expansion(tok_result, kind, extname, span))\n             }\n         };\n@@ -647,7 +646,7 @@ impl<'a, 'b> MacroExpander<'a, 'b> {\n \n     fn parse_expansion(&mut self, toks: TokenStream, kind: ExpansionKind, name: Name, span: Span)\n                        -> Expansion {\n-        let mut parser = self.cx.new_parser_from_tts(&toks.trees().cloned().collect::<Vec<_>>());\n+        let mut parser = self.cx.new_parser_from_tts(&toks.into_trees().collect::<Vec<_>>());\n         let expansion = match parser.parse_expansion(kind, false) {\n             Ok(expansion) => expansion,\n             Err(mut err) => {\n@@ -821,23 +820,23 @@ fn find_attr_invoc(attrs: &mut Vec<ast::Attribute>) -> Option<ast::Attribute> {\n // Therefore, we must use the pretty printer (yuck) to turn the AST node into a\n // string, which we then re-tokenise (double yuck), but first we have to patch\n // the pretty-printed string on to the end of the existing codemap (infinity-yuck).\n-fn tts_for_item(item: &Annotatable, parse_sess: &ParseSess) -> Vec<TokenTree> {\n+fn stream_for_item(item: &Annotatable, parse_sess: &ParseSess) -> TokenStream {\n     let text = match *item {\n         Annotatable::Item(ref i) => pprust::item_to_string(i),\n         Annotatable::TraitItem(ref ti) => pprust::trait_item_to_string(ti),\n         Annotatable::ImplItem(ref ii) => pprust::impl_item_to_string(ii),\n     };\n-    string_to_tts(text, parse_sess)\n+    string_to_stream(text, parse_sess)\n }\n \n-fn tts_for_attr_args(attr: &ast::Attribute, parse_sess: &ParseSess) -> Vec<TokenTree> {\n+fn stream_for_attr_args(attr: &ast::Attribute, parse_sess: &ParseSess) -> TokenStream {\n     use ast::MetaItemKind::*;\n     use print::pp::Breaks;\n     use print::pprust::PrintState;\n \n     let token_string = match attr.value.node {\n         // For `#[foo]`, an empty token\n-        Word => return vec![],\n+        Word => return TokenStream::empty(),\n         // For `#[foo(bar, baz)]`, returns `(bar, baz)`\n         List(ref items) => pprust::to_string(|s| {\n             s.popen()?;\n@@ -853,12 +852,12 @@ fn tts_for_attr_args(attr: &ast::Attribute, parse_sess: &ParseSess) -> Vec<Token\n         }),\n     };\n \n-    string_to_tts(token_string, parse_sess)\n+    string_to_stream(token_string, parse_sess)\n }\n \n-fn string_to_tts(text: String, parse_sess: &ParseSess) -> Vec<TokenTree> {\n+fn string_to_stream(text: String, parse_sess: &ParseSess) -> TokenStream {\n     let filename = String::from(\"<macro expansion>\");\n-    filemap_to_tts(parse_sess, parse_sess.codemap().new_filemap(filename, None, text))\n+    filemap_to_stream(parse_sess, parse_sess.codemap().new_filemap(filename, None, text))\n }\n \n impl<'a, 'b> Folder for InvocationCollector<'a, 'b> {\n@@ -1162,6 +1161,6 @@ impl Folder for Marker {\n }\n \n // apply a given mark to the given token trees. Used prior to expansion of a macro.\n-pub fn mark_tts(tts: &[TokenTree], m: Mark) -> Vec<TokenTree> {\n+pub fn mark_tts(tts: TokenStream, m: Mark) -> TokenStream {\n     noop_fold_tts(tts, &mut Marker{mark:m, expn_id: None})\n }"}, {"sha": "e2fb1946e90dbdd05b1299e1a6d6c33dae3a2882", "filename": "src/libsyntax/ext/placeholders.rs", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "blob_url": "https://github.com/rust-lang/rust/blob/be304afc8c2b1a364bd406888b5378897ed82a9f/src%2Flibsyntax%2Fext%2Fplaceholders.rs", "raw_url": "https://github.com/rust-lang/rust/raw/be304afc8c2b1a364bd406888b5378897ed82a9f/src%2Flibsyntax%2Fext%2Fplaceholders.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fext%2Fplaceholders.rs?ref=be304afc8c2b1a364bd406888b5378897ed82a9f", "patch": "@@ -13,6 +13,7 @@ use codemap::{DUMMY_SP, dummy_spanned};\n use ext::base::ExtCtxt;\n use ext::expand::{Expansion, ExpansionKind};\n use ext::hygiene::Mark;\n+use tokenstream::TokenStream;\n use fold::*;\n use ptr::P;\n use symbol::keywords;\n@@ -26,7 +27,7 @@ pub fn placeholder(kind: ExpansionKind, id: ast::NodeId) -> Expansion {\n     fn mac_placeholder() -> ast::Mac {\n         dummy_spanned(ast::Mac_ {\n             path: ast::Path { span: DUMMY_SP, segments: Vec::new() },\n-            tts: Vec::new(),\n+            tts: TokenStream::empty().into(),\n         })\n     }\n "}, {"sha": "69ff726e719a99fb98dae70f3a65aa09baee668c", "filename": "src/libsyntax/ext/quote.rs", "status": "modified", "additions": 21, "deletions": 18, "changes": 39, "blob_url": "https://github.com/rust-lang/rust/blob/be304afc8c2b1a364bd406888b5378897ed82a9f/src%2Flibsyntax%2Fext%2Fquote.rs", "raw_url": "https://github.com/rust-lang/rust/raw/be304afc8c2b1a364bd406888b5378897ed82a9f/src%2Flibsyntax%2Fext%2Fquote.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fext%2Fquote.rs?ref=be304afc8c2b1a364bd406888b5378897ed82a9f", "patch": "@@ -16,7 +16,7 @@ use ext::build::AstBuilder;\n use parse::parser::{Parser, PathStyle};\n use parse::token;\n use ptr::P;\n-use tokenstream::TokenTree;\n+use tokenstream::{TokenStream, TokenTree};\n \n \n /// Quasiquoting works via token trees.\n@@ -35,7 +35,7 @@ pub mod rt {\n     use std::rc::Rc;\n     use symbol::Symbol;\n \n-    use tokenstream::{self, TokenTree};\n+    use tokenstream::{self, TokenTree, TokenStream};\n \n     pub use parse::new_parser_from_tts;\n     pub use syntax_pos::{BytePos, Span, DUMMY_SP};\n@@ -227,10 +227,10 @@ pub mod rt {\n             if self.style == ast::AttrStyle::Inner {\n                 r.push(TokenTree::Token(self.span, token::Not));\n             }\n-            r.push(TokenTree::Delimited(self.span, Rc::new(tokenstream::Delimited {\n+            r.push(TokenTree::Delimited(self.span, tokenstream::Delimited {\n                 delim: token::Bracket,\n-                tts: self.value.to_tokens(cx),\n-            })));\n+                tts: self.value.to_tokens(cx).into_iter().collect::<TokenStream>().into(),\n+            }));\n             r\n         }\n     }\n@@ -244,10 +244,10 @@ pub mod rt {\n \n     impl ToTokens for () {\n         fn to_tokens(&self, _cx: &ExtCtxt) -> Vec<TokenTree> {\n-            vec![TokenTree::Delimited(DUMMY_SP, Rc::new(tokenstream::Delimited {\n+            vec![TokenTree::Delimited(DUMMY_SP, tokenstream::Delimited {\n                 delim: token::Paren,\n-                tts: vec![],\n-            }))]\n+                tts: TokenStream::empty().into(),\n+            })]\n         }\n     }\n \n@@ -355,14 +355,15 @@ pub mod rt {\n         }\n \n         fn parse_tts(&self, s: String) -> Vec<TokenTree> {\n-            parse::parse_tts_from_source_str(\"<quote expansion>\".to_string(), s, self.parse_sess())\n+            let source_name = \"<quote expansion>\".to_owned();\n+            parse::parse_stream_from_source_str(source_name, s, self.parse_sess())\n+                .into_trees().collect()\n         }\n     }\n }\n \n // Replaces `Token::OpenDelim .. Token::CloseDelim` with `TokenTree::Delimited(..)`.\n pub fn unflatten(tts: Vec<TokenTree>) -> Vec<TokenTree> {\n-    use std::rc::Rc;\n     use tokenstream::Delimited;\n \n     let mut results = Vec::new();\n@@ -373,8 +374,10 @@ pub fn unflatten(tts: Vec<TokenTree>) -> Vec<TokenTree> {\n                 results.push(::std::mem::replace(&mut result, Vec::new()));\n             }\n             TokenTree::Token(span, token::CloseDelim(delim)) => {\n-                let tree =\n-                    TokenTree::Delimited(span, Rc::new(Delimited { delim: delim, tts: result }));\n+                let tree = TokenTree::Delimited(span, Delimited {\n+                    delim: delim,\n+                    tts: result.into_iter().map(TokenStream::from).collect::<TokenStream>().into(),\n+                });\n                 result = results.pop().unwrap();\n                 result.push(tree);\n             }\n@@ -747,7 +750,7 @@ fn statements_mk_tt(cx: &ExtCtxt, tt: &TokenTree, quoted: bool) -> Vec<ast::Stmt\n         },\n         TokenTree::Delimited(span, ref delimed) => {\n             let mut stmts = statements_mk_tt(cx, &delimed.open_tt(span), false);\n-            stmts.extend(statements_mk_tts(cx, &delimed.tts));\n+            stmts.extend(statements_mk_tts(cx, delimed.stream()));\n             stmts.extend(statements_mk_tt(cx, &delimed.close_tt(span), false));\n             stmts\n         }\n@@ -810,14 +813,14 @@ fn mk_stmts_let(cx: &ExtCtxt, sp: Span) -> Vec<ast::Stmt> {\n     vec![stmt_let_sp, stmt_let_tt]\n }\n \n-fn statements_mk_tts(cx: &ExtCtxt, tts: &[TokenTree]) -> Vec<ast::Stmt> {\n+fn statements_mk_tts(cx: &ExtCtxt, tts: TokenStream) -> Vec<ast::Stmt> {\n     let mut ss = Vec::new();\n     let mut quoted = false;\n-    for tt in tts {\n-        quoted = match *tt {\n+    for tt in tts.into_trees() {\n+        quoted = match tt {\n             TokenTree::Token(_, token::Dollar) if !quoted => true,\n             _ => {\n-                ss.extend(statements_mk_tt(cx, tt, quoted));\n+                ss.extend(statements_mk_tt(cx, &tt, quoted));\n                 false\n             }\n         }\n@@ -829,7 +832,7 @@ fn expand_tts(cx: &ExtCtxt, sp: Span, tts: &[TokenTree]) -> (P<ast::Expr>, P<ast\n     let (cx_expr, tts) = parse_arguments_to_quote(cx, tts);\n \n     let mut vector = mk_stmts_let(cx, sp);\n-    vector.extend(statements_mk_tts(cx, &tts[..]));\n+    vector.extend(statements_mk_tts(cx, tts.iter().cloned().collect()));\n     vector.push(cx.stmt_expr(cx.expr_ident(sp, id_ext(\"tt\"))));\n     let block = cx.expr_block(cx.block(sp, vector));\n     let unflatten = vec![id_ext(\"syntax\"), id_ext(\"ext\"), id_ext(\"quote\"), id_ext(\"unflatten\")];"}, {"sha": "b9cb3d82d4f7c172a779f9d42007768b580a06b9", "filename": "src/libsyntax/ext/tt/macro_parser.rs", "status": "modified", "additions": 11, "deletions": 21, "changes": 32, "blob_url": "https://github.com/rust-lang/rust/blob/be304afc8c2b1a364bd406888b5378897ed82a9f/src%2Flibsyntax%2Fext%2Ftt%2Fmacro_parser.rs", "raw_url": "https://github.com/rust-lang/rust/raw/be304afc8c2b1a364bd406888b5378897ed82a9f/src%2Flibsyntax%2Fext%2Ftt%2Fmacro_parser.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fext%2Ftt%2Fmacro_parser.rs?ref=be304afc8c2b1a364bd406888b5378897ed82a9f", "patch": "@@ -82,13 +82,13 @@ use ast::Ident;\n use syntax_pos::{self, BytePos, mk_sp, Span};\n use codemap::Spanned;\n use errors::FatalError;\n-use ext::tt::quoted;\n+use ext::tt::quoted::{self, TokenTree};\n use parse::{Directory, ParseSess};\n use parse::parser::{PathStyle, Parser};\n use parse::token::{self, DocComment, Token, Nonterminal};\n use print::pprust;\n use symbol::keywords;\n-use tokenstream::TokenTree;\n+use tokenstream::TokenStream;\n use util::small_vector::SmallVector;\n \n use std::mem;\n@@ -101,8 +101,8 @@ use std::collections::hash_map::Entry::{Vacant, Occupied};\n \n #[derive(Clone)]\n enum TokenTreeOrTokenTreeVec {\n-    Tt(quoted::TokenTree),\n-    TtSeq(Vec<quoted::TokenTree>),\n+    Tt(TokenTree),\n+    TtSeq(Vec<TokenTree>),\n }\n \n impl TokenTreeOrTokenTreeVec {\n@@ -113,7 +113,7 @@ impl TokenTreeOrTokenTreeVec {\n         }\n     }\n \n-    fn get_tt(&self, index: usize) -> quoted::TokenTree {\n+    fn get_tt(&self, index: usize) -> TokenTree {\n         match *self {\n             TtSeq(ref v) => v[index].clone(),\n             Tt(ref tt) => tt.get_tt(index),\n@@ -144,9 +144,7 @@ struct MatcherPos {\n \n pub type NamedParseResult = ParseResult<HashMap<Ident, Rc<NamedMatch>>>;\n \n-pub fn count_names(ms: &[quoted::TokenTree]) -> usize {\n-    use self::quoted::TokenTree;\n-\n+pub fn count_names(ms: &[TokenTree]) -> usize {\n     ms.iter().fold(0, |count, elt| {\n         count + match *elt {\n             TokenTree::Sequence(_, ref seq) => {\n@@ -163,7 +161,7 @@ pub fn count_names(ms: &[quoted::TokenTree]) -> usize {\n     })\n }\n \n-fn initial_matcher_pos(ms: Vec<quoted::TokenTree>, lo: BytePos) -> Box<MatcherPos> {\n+fn initial_matcher_pos(ms: Vec<TokenTree>, lo: BytePos) -> Box<MatcherPos> {\n     let match_idx_hi = count_names(&ms[..]);\n     let matches = create_matches(match_idx_hi);\n     Box::new(MatcherPos {\n@@ -202,10 +200,8 @@ pub enum NamedMatch {\n     MatchedNonterminal(Rc<Nonterminal>)\n }\n \n-fn nameize<I: Iterator<Item=Rc<NamedMatch>>>(sess: &ParseSess, ms: &[quoted::TokenTree], mut res: I)\n+fn nameize<I: Iterator<Item=Rc<NamedMatch>>>(sess: &ParseSess, ms: &[TokenTree], mut res: I)\n                                              -> NamedParseResult {\n-    use self::quoted::TokenTree;\n-\n     fn n_rec<I: Iterator<Item=Rc<NamedMatch>>>(sess: &ParseSess, m: &TokenTree, mut res: &mut I,\n              ret_val: &mut HashMap<Ident, Rc<NamedMatch>>)\n              -> Result<(), (syntax_pos::Span, String)> {\n@@ -289,9 +285,8 @@ fn inner_parse_loop(sess: &ParseSess,\n                     eof_eis: &mut SmallVector<Box<MatcherPos>>,\n                     bb_eis: &mut SmallVector<Box<MatcherPos>>,\n                     token: &Token,\n-                    span: &syntax_pos::Span) -> ParseResult<()> {\n-    use self::quoted::TokenTree;\n-\n+                    span: &syntax_pos::Span)\n+                    -> ParseResult<()> {\n     while let Some(mut ei) = cur_eis.pop() {\n         // When unzipped trees end, remove them\n         while ei.idx >= ei.top_elts.len() {\n@@ -419,13 +414,8 @@ fn inner_parse_loop(sess: &ParseSess,\n     Success(())\n }\n \n-pub fn parse(sess: &ParseSess,\n-             tts: Vec<TokenTree>,\n-             ms: &[quoted::TokenTree],\n-             directory: Option<Directory>)\n+pub fn parse(sess: &ParseSess, tts: TokenStream, ms: &[TokenTree], directory: Option<Directory>)\n              -> NamedParseResult {\n-    use self::quoted::TokenTree;\n-\n     let mut parser = Parser::new(sess, tts, directory, true);\n     let mut cur_eis = SmallVector::one(initial_matcher_pos(ms.to_owned(), parser.span.lo));\n     let mut next_eis = Vec::new(); // or proceed normally"}, {"sha": "1d386c1a3ac930f494630fcefc44039feb0be40b", "filename": "src/libsyntax/ext/tt/macro_rules.rs", "status": "modified", "additions": 9, "deletions": 12, "changes": 21, "blob_url": "https://github.com/rust-lang/rust/blob/be304afc8c2b1a364bd406888b5378897ed82a9f/src%2Flibsyntax%2Fext%2Ftt%2Fmacro_rules.rs", "raw_url": "https://github.com/rust-lang/rust/raw/be304afc8c2b1a364bd406888b5378897ed82a9f/src%2Flibsyntax%2Fext%2Ftt%2Fmacro_rules.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fext%2Ftt%2Fmacro_rules.rs?ref=be304afc8c2b1a364bd406888b5378897ed82a9f", "patch": "@@ -22,9 +22,8 @@ use parse::{Directory, ParseSess};\n use parse::parser::Parser;\n use parse::token::{self, NtTT};\n use parse::token::Token::*;\n-use print;\n use symbol::Symbol;\n-use tokenstream::TokenTree;\n+use tokenstream::{TokenStream, TokenTree};\n \n use std::collections::{HashMap};\n use std::collections::hash_map::{Entry};\n@@ -68,15 +67,15 @@ impl TTMacroExpander for MacroRulesMacroExpander {\n     fn expand<'cx>(&self,\n                    cx: &'cx mut ExtCtxt,\n                    sp: Span,\n-                   arg: &[TokenTree])\n+                   input: TokenStream)\n                    -> Box<MacResult+'cx> {\n         if !self.valid {\n             return DummyResult::any(sp);\n         }\n         generic_extension(cx,\n                           sp,\n                           self.name,\n-                          arg,\n+                          input,\n                           &self.lhses,\n                           &self.rhses)\n     }\n@@ -86,14 +85,12 @@ impl TTMacroExpander for MacroRulesMacroExpander {\n fn generic_extension<'cx>(cx: &'cx ExtCtxt,\n                           sp: Span,\n                           name: ast::Ident,\n-                          arg: &[TokenTree],\n+                          arg: TokenStream,\n                           lhses: &[quoted::TokenTree],\n                           rhses: &[quoted::TokenTree])\n                           -> Box<MacResult+'cx> {\n     if cx.trace_macros() {\n-        println!(\"{}! {{ {} }}\",\n-                 name,\n-                 print::pprust::tts_to_string(arg));\n+        println!(\"{}! {{ {} }}\", name, arg);\n     }\n \n     // Which arm's failure should we report? (the one furthest along)\n@@ -106,7 +103,7 @@ fn generic_extension<'cx>(cx: &'cx ExtCtxt,\n             _ => cx.span_bug(sp, \"malformed macro lhs\")\n         };\n \n-        match TokenTree::parse(cx, lhs_tt, arg) {\n+        match TokenTree::parse(cx, lhs_tt, arg.clone()) {\n             Success(named_matches) => {\n                 let rhs = match rhses[i] {\n                     // ignore delimiters\n@@ -186,7 +183,7 @@ pub fn compile(sess: &ParseSess, def: &ast::MacroDef) -> SyntaxExtension {\n     ];\n \n     // Parse the macro_rules! invocation\n-    let argument_map = match parse(sess, def.body.clone(), &argument_gram, None) {\n+    let argument_map = match parse(sess, def.body.clone().into(), &argument_gram, None) {\n         Success(m) => m,\n         Failure(sp, tok) => {\n             let s = parse_failure_msg(tok);\n@@ -205,7 +202,7 @@ pub fn compile(sess: &ParseSess, def: &ast::MacroDef) -> SyntaxExtension {\n             s.iter().map(|m| {\n                 if let MatchedNonterminal(ref nt) = **m {\n                     if let NtTT(ref tt) = **nt {\n-                        let tt = quoted::parse(&[tt.clone()], true, sess).pop().unwrap();\n+                        let tt = quoted::parse(tt.clone().into(), true, sess).pop().unwrap();\n                         valid &= check_lhs_nt_follows(sess, &tt);\n                         return tt;\n                     }\n@@ -221,7 +218,7 @@ pub fn compile(sess: &ParseSess, def: &ast::MacroDef) -> SyntaxExtension {\n             s.iter().map(|m| {\n                 if let MatchedNonterminal(ref nt) = **m {\n                     if let NtTT(ref tt) = **nt {\n-                        return quoted::parse(&[tt.clone()], false, sess).pop().unwrap();\n+                        return quoted::parse(tt.clone().into(), false, sess).pop().unwrap();\n                     }\n                 }\n                 sess.span_diagnostic.span_bug(def.span, \"wrong-structured lhs\")"}, {"sha": "d56859d805c878c44951b992a7e9858a500a0a87", "filename": "src/libsyntax/ext/tt/quoted.rs", "status": "modified", "additions": 5, "deletions": 5, "changes": 10, "blob_url": "https://github.com/rust-lang/rust/blob/be304afc8c2b1a364bd406888b5378897ed82a9f/src%2Flibsyntax%2Fext%2Ftt%2Fquoted.rs", "raw_url": "https://github.com/rust-lang/rust/raw/be304afc8c2b1a364bd406888b5378897ed82a9f/src%2Flibsyntax%2Fext%2Ftt%2Fquoted.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fext%2Ftt%2Fquoted.rs?ref=be304afc8c2b1a364bd406888b5378897ed82a9f", "patch": "@@ -124,10 +124,10 @@ impl TokenTree {\n     }\n }\n \n-pub fn parse(input: &[tokenstream::TokenTree], expect_matchers: bool, sess: &ParseSess)\n+pub fn parse(input: tokenstream::TokenStream, expect_matchers: bool, sess: &ParseSess)\n              -> Vec<TokenTree> {\n     let mut result = Vec::new();\n-    let mut trees = input.iter().cloned();\n+    let mut trees = input.trees();\n     while let Some(tree) = trees.next() {\n         let tree = parse_tree(tree, &mut trees, expect_matchers, sess);\n         match tree {\n@@ -161,13 +161,13 @@ fn parse_tree<I>(tree: tokenstream::TokenTree,\n {\n     match tree {\n         tokenstream::TokenTree::Token(span, token::Dollar) => match trees.next() {\n-            Some(tokenstream::TokenTree::Delimited(span, ref delimited)) => {\n+            Some(tokenstream::TokenTree::Delimited(span, delimited)) => {\n                 if delimited.delim != token::Paren {\n                     let tok = pprust::token_to_string(&token::OpenDelim(delimited.delim));\n                     let msg = format!(\"expected `(`, found `{}`\", tok);\n                     sess.span_diagnostic.span_err(span, &msg);\n                 }\n-                let sequence = parse(&delimited.tts, expect_matchers, sess);\n+                let sequence = parse(delimited.tts.into(), expect_matchers, sess);\n                 let (separator, op) = parse_sep_and_kleene_op(trees, span, sess);\n                 let name_captures = macro_parser::count_names(&sequence);\n                 TokenTree::Sequence(span, Rc::new(SequenceRepetition {\n@@ -197,7 +197,7 @@ fn parse_tree<I>(tree: tokenstream::TokenTree,\n         tokenstream::TokenTree::Delimited(span, delimited) => {\n             TokenTree::Delimited(span, Rc::new(Delimited {\n                 delim: delimited.delim,\n-                tts: parse(&delimited.tts, expect_matchers, sess),\n+                tts: parse(delimited.tts.into(), expect_matchers, sess),\n             }))\n         }\n     }"}, {"sha": "24004492be2a0835d98c175d1b2063319f6a02c0", "filename": "src/libsyntax/ext/tt/transcribe.rs", "status": "modified", "additions": 20, "deletions": 16, "changes": 36, "blob_url": "https://github.com/rust-lang/rust/blob/be304afc8c2b1a364bd406888b5378897ed82a9f/src%2Flibsyntax%2Fext%2Ftt%2Ftranscribe.rs", "raw_url": "https://github.com/rust-lang/rust/raw/be304afc8c2b1a364bd406888b5378897ed82a9f/src%2Flibsyntax%2Fext%2Ftt%2Ftranscribe.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fext%2Ftt%2Ftranscribe.rs?ref=be304afc8c2b1a364bd406888b5378897ed82a9f", "patch": "@@ -14,7 +14,7 @@ use ext::tt::macro_parser::{NamedMatch, MatchedSeq, MatchedNonterminal};\n use ext::tt::quoted;\n use parse::token::{self, SubstNt, Token, NtIdent, NtTT};\n use syntax_pos::{Span, DUMMY_SP};\n-use tokenstream::{TokenTree, Delimited};\n+use tokenstream::{TokenStream, TokenTree, Delimited};\n use util::small_vector::SmallVector;\n \n use std::rc::Rc;\n@@ -66,11 +66,11 @@ impl Iterator for Frame {\n pub fn transcribe(sp_diag: &Handler,\n                   interp: Option<HashMap<Ident, Rc<NamedMatch>>>,\n                   src: Vec<quoted::TokenTree>)\n-                  -> Vec<TokenTree> {\n+                  -> TokenStream {\n     let mut stack = SmallVector::one(Frame::new(src));\n     let interpolations = interp.unwrap_or_else(HashMap::new); /* just a convenience */\n     let mut repeats = Vec::new();\n-    let mut result = Vec::new();\n+    let mut result: Vec<TokenStream> = Vec::new();\n     let mut result_stack = Vec::new();\n \n     loop {\n@@ -84,8 +84,11 @@ pub fn transcribe(sp_diag: &Handler,\n                     *idx = 0;\n                     if let Some(sep) = sep.clone() {\n                         // repeat same span, I guess\n-                        let prev_span = result.last().map(TokenTree::span).unwrap_or(DUMMY_SP);\n-                        result.push(TokenTree::Token(prev_span, sep));\n+                        let prev_span = match result.last() {\n+                            Some(stream) => stream.trees().next().unwrap().span(),\n+                            None => DUMMY_SP,\n+                        };\n+                        result.push(TokenTree::Token(prev_span, sep).into());\n                     }\n                     continue\n                 }\n@@ -97,14 +100,14 @@ pub fn transcribe(sp_diag: &Handler,\n                 }\n                 Frame::Delimited { forest, span, .. } => {\n                     if result_stack.is_empty() {\n-                        return result;\n+                        return TokenStream::concat(result);\n                     }\n-                    let tree = TokenTree::Delimited(span, Rc::new(Delimited {\n+                    let tree = TokenTree::Delimited(span, Delimited {\n                         delim: forest.delim,\n-                        tts: result,\n-                    }));\n+                        tts: TokenStream::concat(result).into(),\n+                    });\n                     result = result_stack.pop().unwrap();\n-                    result.push(tree);\n+                    result.push(tree.into());\n                 }\n             }\n             continue\n@@ -148,19 +151,20 @@ pub fn transcribe(sp_diag: &Handler,\n             // FIXME #2887: think about span stuff here\n             quoted::TokenTree::Token(sp, SubstNt(ident)) => {\n                 match lookup_cur_matched(ident, &interpolations, &repeats) {\n-                    None => result.push(TokenTree::Token(sp, SubstNt(ident))),\n+                    None => result.push(TokenTree::Token(sp, SubstNt(ident)).into()),\n                     Some(cur_matched) => if let MatchedNonterminal(ref nt) = *cur_matched {\n                         match **nt {\n                             // sidestep the interpolation tricks for ident because\n                             // (a) idents can be in lots of places, so it'd be a pain\n                             // (b) we actually can, since it's a token.\n                             NtIdent(ref sn) => {\n-                                result.push(TokenTree::Token(sn.span, token::Ident(sn.node)));\n+                                let token = TokenTree::Token(sn.span, token::Ident(sn.node));\n+                                result.push(token.into());\n                             }\n-                            NtTT(ref tt) => result.push(tt.clone()),\n+                            NtTT(ref tt) => result.push(tt.clone().into()),\n                             _ => {\n-                                // FIXME(pcwalton): Bad copy\n-                                result.push(TokenTree::Token(sp, token::Interpolated(nt.clone())));\n+                                let token = TokenTree::Token(sp, token::Interpolated(nt.clone()));\n+                                result.push(token.into());\n                             }\n                         }\n                     } else {\n@@ -174,7 +178,7 @@ pub fn transcribe(sp_diag: &Handler,\n                 stack.push(Frame::Delimited { forest: delimited, idx: 0, span: span });\n                 result_stack.push(mem::replace(&mut result, Vec::new()));\n             }\n-            quoted::TokenTree::Token(span, tok) => result.push(TokenTree::Token(span, tok)),\n+            quoted::TokenTree::Token(span, tok) => result.push(TokenTree::Token(span, tok).into()),\n             quoted::TokenTree::MetaVarDecl(..) => panic!(\"unexpected `TokenTree::MetaVarDecl\"),\n         }\n     }"}, {"sha": "4242b0f8b9803d17df7b06e2c18b55987a1ce671", "filename": "src/libsyntax/fold.rs", "status": "modified", "additions": 14, "deletions": 18, "changes": 32, "blob_url": "https://github.com/rust-lang/rust/blob/be304afc8c2b1a364bd406888b5378897ed82a9f/src%2Flibsyntax%2Ffold.rs", "raw_url": "https://github.com/rust-lang/rust/raw/be304afc8c2b1a364bd406888b5378897ed82a9f/src%2Flibsyntax%2Ffold.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Ffold.rs?ref=be304afc8c2b1a364bd406888b5378897ed82a9f", "patch": "@@ -233,11 +233,11 @@ pub trait Folder : Sized {\n         noop_fold_ty_params(tps, self)\n     }\n \n-    fn fold_tt(&mut self, tt: &TokenTree) -> TokenTree {\n+    fn fold_tt(&mut self, tt: TokenTree) -> TokenTree {\n         noop_fold_tt(tt, self)\n     }\n \n-    fn fold_tts(&mut self, tts: &[TokenTree]) -> Vec<TokenTree> {\n+    fn fold_tts(&mut self, tts: TokenStream) -> TokenStream {\n         noop_fold_tts(tts, self)\n     }\n \n@@ -497,8 +497,8 @@ pub fn noop_fold_attribute<T: Folder>(attr: Attribute, fld: &mut T) -> Option<At\n pub fn noop_fold_mac<T: Folder>(Spanned {node, span}: Mac, fld: &mut T) -> Mac {\n     Spanned {\n         node: Mac_ {\n+            tts: fld.fold_tts(node.stream()).into(),\n             path: fld.fold_path(node.path),\n-            tts: fld.fold_tts(&node.tts),\n         },\n         span: fld.new_span(span)\n     }\n@@ -539,23 +539,19 @@ pub fn noop_fold_arg<T: Folder>(Arg {id, pat, ty}: Arg, fld: &mut T) -> Arg {\n     }\n }\n \n-pub fn noop_fold_tt<T: Folder>(tt: &TokenTree, fld: &mut T) -> TokenTree {\n-    match *tt {\n-        TokenTree::Token(span, ref tok) =>\n-            TokenTree::Token(fld.new_span(span), fld.fold_token(tok.clone())),\n-        TokenTree::Delimited(span, ref delimed) => {\n-            TokenTree::Delimited(fld.new_span(span), Rc::new(\n-                            Delimited {\n-                                delim: delimed.delim,\n-                                tts: fld.fold_tts(&delimed.tts),\n-                            }\n-                        ))\n-        },\n+pub fn noop_fold_tt<T: Folder>(tt: TokenTree, fld: &mut T) -> TokenTree {\n+    match tt {\n+        TokenTree::Token(span, tok) =>\n+            TokenTree::Token(fld.new_span(span), fld.fold_token(tok)),\n+        TokenTree::Delimited(span, delimed) => TokenTree::Delimited(fld.new_span(span), Delimited {\n+            tts: fld.fold_tts(delimed.stream()).into(),\n+            delim: delimed.delim,\n+        }),\n     }\n }\n \n-pub fn noop_fold_tts<T: Folder>(tts: &[TokenTree], fld: &mut T) -> Vec<TokenTree> {\n-    tts.iter().map(|tt| fld.fold_tt(tt)).collect()\n+pub fn noop_fold_tts<T: Folder>(tts: TokenStream, fld: &mut T) -> TokenStream {\n+    tts.trees().map(|tt| fld.fold_tt(tt)).collect()\n }\n \n // apply ident folder if it's an ident, apply other folds to interpolated nodes\n@@ -617,7 +613,7 @@ pub fn noop_fold_interpolated<T: Folder>(nt: token::Nonterminal, fld: &mut T)\n         token::NtIdent(id) => token::NtIdent(Spanned::<Ident>{node: fld.fold_ident(id.node), ..id}),\n         token::NtMeta(meta_item) => token::NtMeta(fld.fold_meta_item(meta_item)),\n         token::NtPath(path) => token::NtPath(fld.fold_path(path)),\n-        token::NtTT(tt) => token::NtTT(fld.fold_tt(&tt)),\n+        token::NtTT(tt) => token::NtTT(fld.fold_tt(tt)),\n         token::NtArm(arm) => token::NtArm(fld.fold_arm(arm)),\n         token::NtImplItem(item) =>\n             token::NtImplItem(fld.fold_impl_item(item)"}, {"sha": "554a1fcfc71a6beb3497510e05c3a4aba10160a8", "filename": "src/libsyntax/parse/lexer/tokentrees.rs", "status": "modified", "additions": 11, "deletions": 13, "changes": 24, "blob_url": "https://github.com/rust-lang/rust/blob/be304afc8c2b1a364bd406888b5378897ed82a9f/src%2Flibsyntax%2Fparse%2Flexer%2Ftokentrees.rs", "raw_url": "https://github.com/rust-lang/rust/raw/be304afc8c2b1a364bd406888b5378897ed82a9f/src%2Flibsyntax%2Fparse%2Flexer%2Ftokentrees.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Flexer%2Ftokentrees.rs?ref=be304afc8c2b1a364bd406888b5378897ed82a9f", "patch": "@@ -12,32 +12,30 @@ use print::pprust::token_to_string;\n use parse::lexer::StringReader;\n use parse::{token, PResult};\n use syntax_pos::Span;\n-use tokenstream::{Delimited, TokenTree};\n-\n-use std::rc::Rc;\n+use tokenstream::{Delimited, TokenStream, TokenTree};\n \n impl<'a> StringReader<'a> {\n     // Parse a stream of tokens into a list of `TokenTree`s, up to an `Eof`.\n-    pub fn parse_all_token_trees(&mut self) -> PResult<'a, Vec<TokenTree>> {\n+    pub fn parse_all_token_trees(&mut self) -> PResult<'a, TokenStream> {\n         let mut tts = Vec::new();\n         while self.token != token::Eof {\n-            tts.push(self.parse_token_tree()?);\n+            tts.push(self.parse_token_tree()?.into());\n         }\n-        Ok(tts)\n+        Ok(TokenStream::concat(tts))\n     }\n \n     // Parse a stream of tokens into a list of `TokenTree`s, up to a `CloseDelim`.\n-    fn parse_token_trees_until_close_delim(&mut self) -> Vec<TokenTree> {\n+    fn parse_token_trees_until_close_delim(&mut self) -> TokenStream {\n         let mut tts = vec![];\n         loop {\n             if let token::CloseDelim(..) = self.token {\n-                return tts;\n+                return TokenStream::concat(tts);\n             }\n             match self.parse_token_tree() {\n-                Ok(tt) => tts.push(tt),\n+                Ok(tt) => tts.push(tt.into()),\n                 Err(mut e) => {\n                     e.emit();\n-                    return tts;\n+                    return TokenStream::concat(tts);\n                 }\n             }\n         }\n@@ -111,10 +109,10 @@ impl<'a> StringReader<'a> {\n                     _ => {}\n                 }\n \n-                Ok(TokenTree::Delimited(span, Rc::new(Delimited {\n+                Ok(TokenTree::Delimited(span, Delimited {\n                     delim: delim,\n-                    tts: tts,\n-                })))\n+                    tts: tts.into(),\n+                }))\n             },\n             token::CloseDelim(_) => {\n                 // An unexpected closing delimiter (i.e., there is no"}, {"sha": "c00d2952b3b425bcc87e6ac78be0bf9719c53a2c", "filename": "src/libsyntax/parse/mod.rs", "status": "modified", "additions": 41, "deletions": 46, "changes": 87, "blob_url": "https://github.com/rust-lang/rust/blob/be304afc8c2b1a364bd406888b5378897ed82a9f/src%2Flibsyntax%2Fparse%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/be304afc8c2b1a364bd406888b5378897ed82a9f/src%2Flibsyntax%2Fparse%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Fmod.rs?ref=be304afc8c2b1a364bd406888b5378897ed82a9f", "patch": "@@ -19,7 +19,7 @@ use parse::parser::Parser;\n use ptr::P;\n use str::char_at;\n use symbol::Symbol;\n-use tokenstream;\n+use tokenstream::{TokenStream, TokenTree};\n \n use std::cell::RefCell;\n use std::collections::HashSet;\n@@ -141,9 +141,9 @@ pub fn parse_stmt_from_source_str<'a>(name: String, source: String, sess: &'a Pa\n     new_parser_from_source_str(sess, name, source).parse_stmt()\n }\n \n-pub fn parse_tts_from_source_str<'a>(name: String, source: String, sess: &'a ParseSess)\n-                                     -> Vec<tokenstream::TokenTree> {\n-    filemap_to_tts(sess, sess.codemap().new_filemap(name, None, source))\n+pub fn parse_stream_from_source_str<'a>(name: String, source: String, sess: &'a ParseSess)\n+                                        -> TokenStream {\n+    filemap_to_stream(sess, sess.codemap().new_filemap(name, None, source))\n }\n \n // Create a new parser from a source string\n@@ -175,7 +175,7 @@ pub fn new_sub_parser_from_file<'a>(sess: &'a ParseSess,\n /// Given a filemap and config, return a parser\n pub fn filemap_to_parser<'a>(sess: &'a ParseSess, filemap: Rc<FileMap>, ) -> Parser<'a> {\n     let end_pos = filemap.end_pos;\n-    let mut parser = tts_to_parser(sess, filemap_to_tts(sess, filemap));\n+    let mut parser = stream_to_parser(sess, filemap_to_stream(sess, filemap));\n \n     if parser.token == token::Eof && parser.span == syntax_pos::DUMMY_SP {\n         parser.span = syntax_pos::mk_sp(end_pos, end_pos);\n@@ -186,13 +186,8 @@ pub fn filemap_to_parser<'a>(sess: &'a ParseSess, filemap: Rc<FileMap>, ) -> Par\n \n // must preserve old name for now, because quote! from the *existing*\n // compiler expands into it\n-pub fn new_parser_from_tts<'a>(sess: &'a ParseSess, tts: Vec<tokenstream::TokenTree>)\n-                               -> Parser<'a> {\n-    tts_to_parser(sess, tts)\n-}\n-\n-pub fn new_parser_from_ts<'a>(sess: &'a ParseSess, ts: tokenstream::TokenStream) -> Parser<'a> {\n-    tts_to_parser(sess, ts.trees().cloned().collect())\n+pub fn new_parser_from_tts<'a>(sess: &'a ParseSess, tts: Vec<TokenTree>) -> Parser<'a> {\n+    stream_to_parser(sess, tts.into_iter().collect())\n }\n \n \n@@ -215,15 +210,15 @@ fn file_to_filemap(sess: &ParseSess, path: &Path, spanopt: Option<Span>)\n }\n \n /// Given a filemap, produce a sequence of token-trees\n-pub fn filemap_to_tts(sess: &ParseSess, filemap: Rc<FileMap>) -> Vec<tokenstream::TokenTree> {\n+pub fn filemap_to_stream(sess: &ParseSess, filemap: Rc<FileMap>) -> TokenStream {\n     let mut srdr = lexer::StringReader::new(sess, filemap);\n     srdr.real_token();\n     panictry!(srdr.parse_all_token_trees())\n }\n \n-/// Given tts and the ParseSess, produce a parser\n-pub fn tts_to_parser<'a>(sess: &'a ParseSess, tts: Vec<tokenstream::TokenTree>) -> Parser<'a> {\n-    let mut p = Parser::new(sess, tts, None, false);\n+/// Given stream and the ParseSess, produce a parser\n+pub fn stream_to_parser<'a>(sess: &'a ParseSess, stream: TokenStream) -> Parser<'a> {\n+    let mut p = Parser::new(sess, stream, None, false);\n     p.check_unknown_macro_variable();\n     p\n }\n@@ -603,7 +598,6 @@ pub fn integer_lit(s: &str, suffix: Option<Symbol>, sd: &Handler, sp: Span) -> a\n #[cfg(test)]\n mod tests {\n     use super::*;\n-    use std::rc::Rc;\n     use syntax_pos::{self, Span, BytePos, Pos, NO_EXPANSION};\n     use codemap::Spanned;\n     use ast::{self, Ident, PatKind};\n@@ -614,7 +608,7 @@ mod tests {\n     use print::pprust::item_to_string;\n     use ptr::P;\n     use tokenstream::{self, TokenTree};\n-    use util::parser_testing::{string_to_tts, string_to_parser};\n+    use util::parser_testing::{string_to_stream, string_to_parser};\n     use util::parser_testing::{string_to_expr, string_to_item, string_to_stmt};\n     use util::ThinVec;\n \n@@ -659,8 +653,9 @@ mod tests {\n     // check the token-tree-ization of macros\n     #[test]\n     fn string_to_tts_macro () {\n-        let tts = string_to_tts(\"macro_rules! zip (($a)=>($a))\".to_string());\n-        let tts: &[tokenstream::TokenTree] = &tts[..];\n+        let tts: Vec<_> =\n+            string_to_stream(\"macro_rules! zip (($a)=>($a))\".to_string()).trees().collect();\n+        let tts: &[TokenTree] = &tts[..];\n \n         match (tts.len(), tts.get(0), tts.get(1), tts.get(2), tts.get(3)) {\n             (\n@@ -672,7 +667,7 @@ mod tests {\n             )\n             if name_macro_rules.name == \"macro_rules\"\n             && name_zip.name == \"zip\" => {\n-                let tts = &macro_delimed.tts[..];\n+                let tts = &macro_delimed.stream().trees().collect::<Vec<_>>();\n                 match (tts.len(), tts.get(0), tts.get(1), tts.get(2)) {\n                     (\n                         3,\n@@ -681,17 +676,17 @@ mod tests {\n                         Some(&TokenTree::Delimited(_, ref second_delimed)),\n                     )\n                     if macro_delimed.delim == token::Paren => {\n-                        let tts = &first_delimed.tts[..];\n+                        let tts = &first_delimed.stream().trees().collect::<Vec<_>>();\n                         match (tts.len(), tts.get(0), tts.get(1)) {\n                             (\n                                 2,\n                                 Some(&TokenTree::Token(_, token::Dollar)),\n                                 Some(&TokenTree::Token(_, token::Ident(ident))),\n                             )\n                             if first_delimed.delim == token::Paren && ident.name == \"a\" => {},\n-                            _ => panic!(\"value 3: {:?}\", **first_delimed),\n+                            _ => panic!(\"value 3: {:?}\", *first_delimed),\n                         }\n-                        let tts = &second_delimed.tts[..];\n+                        let tts = &second_delimed.stream().trees().collect::<Vec<_>>();\n                         match (tts.len(), tts.get(0), tts.get(1)) {\n                             (\n                                 2,\n@@ -700,10 +695,10 @@ mod tests {\n                             )\n                             if second_delimed.delim == token::Paren\n                             && ident.name == \"a\" => {},\n-                            _ => panic!(\"value 4: {:?}\", **second_delimed),\n+                            _ => panic!(\"value 4: {:?}\", *second_delimed),\n                         }\n                     },\n-                    _ => panic!(\"value 2: {:?}\", **macro_delimed),\n+                    _ => panic!(\"value 2: {:?}\", *macro_delimed),\n                 }\n             },\n             _ => panic!(\"value: {:?}\",tts),\n@@ -712,31 +707,31 @@ mod tests {\n \n     #[test]\n     fn string_to_tts_1() {\n-        let tts = string_to_tts(\"fn a (b : i32) { b; }\".to_string());\n+        let tts = string_to_stream(\"fn a (b : i32) { b; }\".to_string());\n \n-        let expected = vec![\n-            TokenTree::Token(sp(0, 2), token::Ident(Ident::from_str(\"fn\"))),\n-            TokenTree::Token(sp(3, 4), token::Ident(Ident::from_str(\"a\"))),\n+        let expected = TokenStream::concat(vec![\n+            TokenTree::Token(sp(0, 2), token::Ident(Ident::from_str(\"fn\"))).into(),\n+            TokenTree::Token(sp(3, 4), token::Ident(Ident::from_str(\"a\"))).into(),\n             TokenTree::Delimited(\n                 sp(5, 14),\n-                Rc::new(tokenstream::Delimited {\n+                tokenstream::Delimited {\n                     delim: token::DelimToken::Paren,\n-                    tts: vec![\n-                        TokenTree::Token(sp(6, 7), token::Ident(Ident::from_str(\"b\"))),\n-                        TokenTree::Token(sp(8, 9), token::Colon),\n-                        TokenTree::Token(sp(10, 13), token::Ident(Ident::from_str(\"i32\"))),\n-                    ],\n-                })),\n+                    tts: TokenStream::concat(vec![\n+                        TokenTree::Token(sp(6, 7), token::Ident(Ident::from_str(\"b\"))).into(),\n+                        TokenTree::Token(sp(8, 9), token::Colon).into(),\n+                        TokenTree::Token(sp(10, 13), token::Ident(Ident::from_str(\"i32\"))).into(),\n+                    ]).into(),\n+                }).into(),\n             TokenTree::Delimited(\n                 sp(15, 21),\n-                Rc::new(tokenstream::Delimited {\n+                tokenstream::Delimited {\n                     delim: token::DelimToken::Brace,\n-                    tts: vec![\n-                        TokenTree::Token(sp(17, 18), token::Ident(Ident::from_str(\"b\"))),\n-                        TokenTree::Token(sp(18, 19), token::Semi),\n-                    ],\n-                }))\n-        ];\n+                    tts: TokenStream::concat(vec![\n+                        TokenTree::Token(sp(17, 18), token::Ident(Ident::from_str(\"b\"))).into(),\n+                        TokenTree::Token(sp(18, 19), token::Semi).into(),\n+                    ]).into(),\n+                }).into()\n+        ]);\n \n         assert_eq!(tts, expected);\n     }\n@@ -979,8 +974,8 @@ mod tests {\n         let expr = parse::parse_expr_from_source_str(\"foo\".to_string(),\n             \"foo!( fn main() { body } )\".to_string(), &sess).unwrap();\n \n-        let tts = match expr.node {\n-            ast::ExprKind::Mac(ref mac) => mac.node.tts.clone(),\n+        let tts: Vec<_> = match expr.node {\n+            ast::ExprKind::Mac(ref mac) => mac.node.stream().trees().collect(),\n             _ => panic!(\"not a macro\"),\n         };\n "}, {"sha": "6e3724b5fd87b4ed33bbc27607ec101a084f5066", "filename": "src/libsyntax/parse/parser.rs", "status": "modified", "additions": 138, "deletions": 50, "changes": 188, "blob_url": "https://github.com/rust-lang/rust/blob/be304afc8c2b1a364bd406888b5378897ed82a9f/src%2Flibsyntax%2Fparse%2Fparser.rs", "raw_url": "https://github.com/rust-lang/rust/raw/be304afc8c2b1a364bd406888b5378897ed82a9f/src%2Flibsyntax%2Fparse%2Fparser.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Fparser.rs?ref=be304afc8c2b1a364bd406888b5378897ed82a9f", "patch": "@@ -9,7 +9,7 @@\n // except according to those terms.\n \n use abi::{self, Abi};\n-use ast::BareFnTy;\n+use ast::{AttrStyle, BareFnTy};\n use ast::{RegionTyParamBound, TraitTyParamBound, TraitBoundModifier};\n use ast::Unsafety;\n use ast::{Mod, Arg, Arm, Attribute, BindingMode, TraitItemKind};\n@@ -46,21 +46,21 @@ use errors::{self, DiagnosticBuilder};\n use parse::{self, classify, token};\n use parse::common::SeqSep;\n use parse::lexer::TokenAndSpan;\n+use parse::lexer::comments::{doc_comment_style, strip_doc_comment_decoration};\n use parse::obsolete::ObsoleteSyntax;\n use parse::{new_sub_parser_from_file, ParseSess, Directory, DirectoryOwnership};\n use util::parser::{AssocOp, Fixity};\n use print::pprust;\n use ptr::P;\n use parse::PResult;\n-use tokenstream::{Delimited, TokenTree};\n+use tokenstream::{self, Delimited, ThinTokenStream, TokenTree, TokenStream};\n use symbol::{Symbol, keywords};\n use util::ThinVec;\n \n use std::collections::HashSet;\n-use std::mem;\n+use std::{cmp, mem, slice};\n use std::path::{Path, PathBuf};\n use std::rc::Rc;\n-use std::slice;\n \n bitflags! {\n     flags Restrictions: u8 {\n@@ -175,12 +175,112 @@ pub struct Parser<'a> {\n     /// into modules, and sub-parsers have new values for this name.\n     pub root_module_name: Option<String>,\n     pub expected_tokens: Vec<TokenType>,\n-    pub tts: Vec<(TokenTree, usize)>,\n+    token_cursor: TokenCursor,\n     pub desugar_doc_comments: bool,\n     /// Whether we should configure out of line modules as we parse.\n     pub cfg_mods: bool,\n }\n \n+struct TokenCursor {\n+    frame: TokenCursorFrame,\n+    stack: Vec<TokenCursorFrame>,\n+}\n+\n+struct TokenCursorFrame {\n+    delim: token::DelimToken,\n+    span: Span,\n+    open_delim: bool,\n+    tree_cursor: tokenstream::Cursor,\n+    close_delim: bool,\n+}\n+\n+impl TokenCursorFrame {\n+    fn new(sp: Span, delimited: &Delimited) -> Self {\n+        TokenCursorFrame {\n+            delim: delimited.delim,\n+            span: sp,\n+            open_delim: delimited.delim == token::NoDelim,\n+            tree_cursor: delimited.stream().into_trees(),\n+            close_delim: delimited.delim == token::NoDelim,\n+        }\n+    }\n+}\n+\n+impl TokenCursor {\n+    fn next(&mut self) -> TokenAndSpan {\n+        loop {\n+            let tree = if !self.frame.open_delim {\n+                self.frame.open_delim = true;\n+                Delimited { delim: self.frame.delim, tts: TokenStream::empty().into() }\n+                    .open_tt(self.frame.span)\n+            } else if let Some(tree) = self.frame.tree_cursor.next() {\n+                tree\n+            } else if !self.frame.close_delim {\n+                self.frame.close_delim = true;\n+                Delimited { delim: self.frame.delim, tts: TokenStream::empty().into() }\n+                    .close_tt(self.frame.span)\n+            } else if let Some(frame) = self.stack.pop() {\n+                self.frame = frame;\n+                continue\n+            } else {\n+                return TokenAndSpan { tok: token::Eof, sp: syntax_pos::DUMMY_SP }\n+            };\n+\n+            match tree {\n+                TokenTree::Token(sp, tok) => return TokenAndSpan { tok: tok, sp: sp },\n+                TokenTree::Delimited(sp, ref delimited) => {\n+                    let frame = TokenCursorFrame::new(sp, delimited);\n+                    self.stack.push(mem::replace(&mut self.frame, frame));\n+                }\n+            }\n+        }\n+    }\n+\n+    fn next_desugared(&mut self) -> TokenAndSpan {\n+        let (sp, name) = match self.next() {\n+            TokenAndSpan { sp, tok: token::DocComment(name) } => (sp, name),\n+            tok @ _ => return tok,\n+        };\n+\n+        let stripped = strip_doc_comment_decoration(&name.as_str());\n+\n+        // Searches for the occurrences of `\"#*` and returns the minimum number of `#`s\n+        // required to wrap the text.\n+        let mut num_of_hashes = 0;\n+        let mut count = 0;\n+        for ch in stripped.chars() {\n+            count = match ch {\n+                '\"' => 1,\n+                '#' if count > 0 => count + 1,\n+                _ => 0,\n+            };\n+            num_of_hashes = cmp::max(num_of_hashes, count);\n+        }\n+\n+        let body = TokenTree::Delimited(sp, Delimited {\n+            delim: token::Bracket,\n+            tts: [TokenTree::Token(sp, token::Ident(ast::Ident::from_str(\"doc\"))),\n+                  TokenTree::Token(sp, token::Eq),\n+                  TokenTree::Token(sp, token::Literal(\n+                      token::StrRaw(Symbol::intern(&stripped), num_of_hashes), None))]\n+                .iter().cloned().collect::<TokenStream>().into(),\n+        });\n+\n+        self.stack.push(mem::replace(&mut self.frame, TokenCursorFrame::new(sp, &Delimited {\n+            delim: token::NoDelim,\n+            tts: if doc_comment_style(&name.as_str()) == AttrStyle::Inner {\n+                [TokenTree::Token(sp, token::Pound), TokenTree::Token(sp, token::Not), body]\n+                    .iter().cloned().collect::<TokenStream>().into()\n+            } else {\n+                [TokenTree::Token(sp, token::Pound), body]\n+                    .iter().cloned().collect::<TokenStream>().into()\n+            },\n+        })));\n+\n+        self.next()\n+    }\n+}\n+\n #[derive(PartialEq, Eq, Clone)]\n pub enum TokenType {\n     Token(token::Token),\n@@ -309,14 +409,10 @@ impl From<P<Expr>> for LhsExpr {\n \n impl<'a> Parser<'a> {\n     pub fn new(sess: &'a ParseSess,\n-               tokens: Vec<TokenTree>,\n+               tokens: TokenStream,\n                directory: Option<Directory>,\n                desugar_doc_comments: bool)\n                -> Self {\n-        let tt = TokenTree::Delimited(syntax_pos::DUMMY_SP, Rc::new(Delimited {\n-            delim: token::NoDelim,\n-            tts: tokens,\n-        }));\n         let mut parser = Parser {\n             sess: sess,\n             token: token::Underscore,\n@@ -328,7 +424,13 @@ impl<'a> Parser<'a> {\n             directory: Directory { path: PathBuf::new(), ownership: DirectoryOwnership::Owned },\n             root_module_name: None,\n             expected_tokens: Vec::new(),\n-            tts: if tt.len() > 0 { vec![(tt, 0)] } else { Vec::new() },\n+            token_cursor: TokenCursor {\n+                frame: TokenCursorFrame::new(syntax_pos::DUMMY_SP, &Delimited {\n+                    delim: token::NoDelim,\n+                    tts: tokens.into(),\n+                }),\n+                stack: Vec::new(),\n+            },\n             desugar_doc_comments: desugar_doc_comments,\n             cfg_mods: true,\n         };\n@@ -346,29 +448,14 @@ impl<'a> Parser<'a> {\n     }\n \n     fn next_tok(&mut self) -> TokenAndSpan {\n-        loop {\n-            let tok = if let Some((tts, i)) = self.tts.pop() {\n-                let tt = tts.get_tt(i);\n-                if i + 1 < tts.len() {\n-                    self.tts.push((tts, i + 1));\n-                }\n-                if let TokenTree::Token(sp, tok) = tt {\n-                    TokenAndSpan { tok: tok, sp: sp }\n-                } else {\n-                    self.tts.push((tt, 0));\n-                    continue\n-                }\n-            } else {\n-                TokenAndSpan { tok: token::Eof, sp: self.span }\n-            };\n-\n-            match tok.tok {\n-                token::DocComment(name) if self.desugar_doc_comments => {\n-                    self.tts.push((TokenTree::Token(tok.sp, token::DocComment(name)), 0));\n-                }\n-                _ => return tok,\n-            }\n+        let mut next = match self.desugar_doc_comments {\n+            true => self.token_cursor.next_desugared(),\n+            false => self.token_cursor.next(),\n+        };\n+        if next.sp == syntax_pos::DUMMY_SP {\n+            next.sp = self.prev_span;\n         }\n+        next\n     }\n \n     /// Convert a token to a string using self's reader\n@@ -972,19 +1059,16 @@ impl<'a> Parser<'a> {\n         F: FnOnce(&token::Token) -> R,\n     {\n         if dist == 0 {\n-            return f(&self.token);\n-        }\n-        let mut tok = token::Eof;\n-        if let Some(&(ref tts, mut i)) = self.tts.last() {\n-            i += dist - 1;\n-            if i < tts.len() {\n-                tok = match tts.get_tt(i) {\n-                    TokenTree::Token(_, tok) => tok,\n-                    TokenTree::Delimited(_, delimited) => token::OpenDelim(delimited.delim),\n-                };\n-            }\n+            return f(&self.token)\n         }\n-        f(&tok)\n+\n+        f(&match self.token_cursor.frame.tree_cursor.look_ahead(dist - 1) {\n+            Some(tree) => match tree {\n+                TokenTree::Token(_, tok) => tok,\n+                TokenTree::Delimited(_, delimited) => token::OpenDelim(delimited.delim),\n+            },\n+            None => token::CloseDelim(self.token_cursor.frame.delim),\n+        })\n     }\n     pub fn fatal(&self, m: &str) -> DiagnosticBuilder<'a> {\n         self.sess.span_diagnostic.struct_span_fatal(self.span, m)\n@@ -2022,10 +2106,10 @@ impl<'a> Parser<'a> {\n         })\n     }\n \n-    fn expect_delimited_token_tree(&mut self) -> PResult<'a, (token::DelimToken, Vec<TokenTree>)> {\n+    fn expect_delimited_token_tree(&mut self) -> PResult<'a, (token::DelimToken, ThinTokenStream)> {\n         match self.token {\n             token::OpenDelim(delim) => self.parse_token_tree().map(|tree| match tree {\n-                TokenTree::Delimited(_, delimited) => (delim, delimited.tts.clone()),\n+                TokenTree::Delimited(_, delimited) => (delim, delimited.stream().into()),\n                 _ => unreachable!(),\n             }),\n             _ => Err(self.fatal(\"expected open delimiter\")),\n@@ -2569,10 +2653,14 @@ impl<'a> Parser<'a> {\n     pub fn parse_token_tree(&mut self) -> PResult<'a, TokenTree> {\n         match self.token {\n             token::OpenDelim(..) => {\n-                let tt = self.tts.pop().unwrap().0;\n-                self.span = tt.span();\n+                let frame = mem::replace(&mut self.token_cursor.frame,\n+                                         self.token_cursor.stack.pop().unwrap());\n+                self.span = frame.span;\n                 self.bump();\n-                return Ok(tt);\n+                return Ok(TokenTree::Delimited(frame.span, Delimited {\n+                    delim: frame.delim,\n+                    tts: frame.tree_cursor.original_stream().into(),\n+                }));\n             },\n             token::CloseDelim(_) | token::Eof => unreachable!(),\n             _ => Ok(TokenTree::Token(self.span, self.bump_and_get())),"}, {"sha": "53ef8e8dfa49c3c225a8abfacd59827f6c1da906", "filename": "src/libsyntax/print/pprust.rs", "status": "modified", "additions": 12, "deletions": 12, "changes": 24, "blob_url": "https://github.com/rust-lang/rust/blob/be304afc8c2b1a364bd406888b5378897ed82a9f/src%2Flibsyntax%2Fprint%2Fpprust.rs", "raw_url": "https://github.com/rust-lang/rust/raw/be304afc8c2b1a364bd406888b5378897ed82a9f/src%2Flibsyntax%2Fprint%2Fpprust.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fprint%2Fpprust.rs?ref=be304afc8c2b1a364bd406888b5378897ed82a9f", "patch": "@@ -286,7 +286,7 @@ pub fn token_to_string(tok: &Token) -> String {\n             token::NtStmt(ref e)        => stmt_to_string(&e),\n             token::NtPat(ref e)         => pat_to_string(&e),\n             token::NtIdent(ref e)       => ident_to_string(e.node),\n-            token::NtTT(ref e)          => tt_to_string(&e),\n+            token::NtTT(ref tree)       => tt_to_string(tree.clone()),\n             token::NtArm(ref e)         => arm_to_string(&e),\n             token::NtImplItem(ref e)    => impl_item_to_string(&e),\n             token::NtTraitItem(ref e)   => trait_item_to_string(&e),\n@@ -321,12 +321,12 @@ pub fn lifetime_to_string(e: &ast::Lifetime) -> String {\n     to_string(|s| s.print_lifetime(e))\n }\n \n-pub fn tt_to_string(tt: &tokenstream::TokenTree) -> String {\n+pub fn tt_to_string(tt: tokenstream::TokenTree) -> String {\n     to_string(|s| s.print_tt(tt))\n }\n \n pub fn tts_to_string(tts: &[tokenstream::TokenTree]) -> String {\n-    to_string(|s| s.print_tts(tts))\n+    to_string(|s| s.print_tts(tts.iter().cloned().collect()))\n }\n \n pub fn stmt_to_string(stmt: &ast::Stmt) -> String {\n@@ -1324,7 +1324,7 @@ impl<'a> State<'a> {\n                 self.print_ident(item.ident)?;\n                 self.cbox(INDENT_UNIT)?;\n                 self.popen()?;\n-                self.print_tts(&node.tts[..])?;\n+                self.print_tts(node.stream())?;\n                 self.pclose()?;\n                 word(&mut self.s, \";\")?;\n                 self.end()?;\n@@ -1456,8 +1456,8 @@ impl<'a> State<'a> {\n     /// appropriate macro, transcribe back into the grammar we just parsed from,\n     /// and then pretty-print the resulting AST nodes (so, e.g., we print\n     /// expression arguments as expressions). It can be done! I think.\n-    pub fn print_tt(&mut self, tt: &tokenstream::TokenTree) -> io::Result<()> {\n-        match *tt {\n+    pub fn print_tt(&mut self, tt: tokenstream::TokenTree) -> io::Result<()> {\n+        match tt {\n             TokenTree::Token(_, ref tk) => {\n                 word(&mut self.s, &token_to_string(tk))?;\n                 match *tk {\n@@ -1470,16 +1470,16 @@ impl<'a> State<'a> {\n             TokenTree::Delimited(_, ref delimed) => {\n                 word(&mut self.s, &token_to_string(&delimed.open_token()))?;\n                 space(&mut self.s)?;\n-                self.print_tts(&delimed.tts)?;\n+                self.print_tts(delimed.stream())?;\n                 space(&mut self.s)?;\n                 word(&mut self.s, &token_to_string(&delimed.close_token()))\n             },\n         }\n     }\n \n-    pub fn print_tts(&mut self, tts: &[tokenstream::TokenTree]) -> io::Result<()> {\n+    pub fn print_tts(&mut self, tts: tokenstream::TokenStream) -> io::Result<()> {\n         self.ibox(0)?;\n-        for (i, tt) in tts.iter().enumerate() {\n+        for (i, tt) in tts.into_trees().enumerate() {\n             if i != 0 {\n                 space(&mut self.s)?;\n             }\n@@ -1550,7 +1550,7 @@ impl<'a> State<'a> {\n                 word(&mut self.s, \"! \")?;\n                 self.cbox(INDENT_UNIT)?;\n                 self.popen()?;\n-                self.print_tts(&node.tts[..])?;\n+                self.print_tts(node.stream())?;\n                 self.pclose()?;\n                 word(&mut self.s, \";\")?;\n                 self.end()?\n@@ -1586,7 +1586,7 @@ impl<'a> State<'a> {\n                 word(&mut self.s, \"! \")?;\n                 self.cbox(INDENT_UNIT)?;\n                 self.popen()?;\n-                self.print_tts(&node.tts[..])?;\n+                self.print_tts(node.stream())?;\n                 self.pclose()?;\n                 word(&mut self.s, \";\")?;\n                 self.end()?\n@@ -1779,7 +1779,7 @@ impl<'a> State<'a> {\n             }\n             token::NoDelim => {}\n         }\n-        self.print_tts(&m.node.tts)?;\n+        self.print_tts(m.node.stream())?;\n         match delim {\n             token::Paren => self.pclose(),\n             token::Bracket => word(&mut self.s, \"]\"),"}, {"sha": "2da442a1a53da752fe2ed2bdffd413599afe91dd", "filename": "src/libsyntax/tokenstream.rs", "status": "modified", "additions": 183, "deletions": 217, "changes": 400, "blob_url": "https://github.com/rust-lang/rust/blob/be304afc8c2b1a364bd406888b5378897ed82a9f/src%2Flibsyntax%2Ftokenstream.rs", "raw_url": "https://github.com/rust-lang/rust/raw/be304afc8c2b1a364bd406888b5378897ed82a9f/src%2Flibsyntax%2Ftokenstream.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Ftokenstream.rs?ref=be304afc8c2b1a364bd406888b5378897ed82a9f", "patch": "@@ -22,29 +22,25 @@\n //! and a borrowed TokenStream is sufficient to build an owned TokenStream without taking\n //! ownership of the original.\n \n-use ast::{self, AttrStyle, LitKind};\n use syntax_pos::{BytePos, Span, DUMMY_SP};\n-use codemap::Spanned;\n use ext::base;\n use ext::tt::{macro_parser, quoted};\n-use parse::lexer::comments::{doc_comment_style, strip_doc_comment_decoration};\n-use parse::{self, Directory};\n-use parse::token::{self, Token, Lit};\n+use parse::Directory;\n+use parse::token::{self, Token};\n use print::pprust;\n use serialize::{Decoder, Decodable, Encoder, Encodable};\n-use symbol::Symbol;\n use util::RcSlice;\n \n-use std::{fmt, iter};\n-use std::rc::Rc;\n+use std::{fmt, iter, mem};\n+use std::hash::{self, Hash};\n \n /// A delimited sequence of token trees\n #[derive(Clone, PartialEq, Eq, RustcEncodable, RustcDecodable, Hash, Debug)]\n pub struct Delimited {\n     /// The type of delimiter\n     pub delim: token::DelimToken,\n     /// The delimited sequence of token trees\n-    pub tts: Vec<TokenTree>,\n+    pub tts: ThinTokenStream,\n }\n \n impl Delimited {\n@@ -77,8 +73,8 @@ impl Delimited {\n     }\n \n     /// Returns the token trees inside the delimiters.\n-    pub fn subtrees(&self) -> &[TokenTree] {\n-        &self.tts\n+    pub fn stream(&self) -> TokenStream {\n+        self.tts.clone().into()\n     }\n }\n \n@@ -99,101 +95,28 @@ pub enum TokenTree {\n     /// A single token\n     Token(Span, token::Token),\n     /// A delimited sequence of token trees\n-    Delimited(Span, Rc<Delimited>),\n+    Delimited(Span, Delimited),\n }\n \n impl TokenTree {\n-    pub fn len(&self) -> usize {\n-        match *self {\n-            TokenTree::Token(_, token::DocComment(name)) => {\n-                match doc_comment_style(&name.as_str()) {\n-                    AttrStyle::Outer => 2,\n-                    AttrStyle::Inner => 3,\n-                }\n-            }\n-            TokenTree::Delimited(_, ref delimed) => match delimed.delim {\n-                token::NoDelim => delimed.tts.len(),\n-                _ => delimed.tts.len() + 2,\n-            },\n-            TokenTree::Token(..) => 0,\n-        }\n-    }\n-\n-    pub fn get_tt(&self, index: usize) -> TokenTree {\n-        match (self, index) {\n-            (&TokenTree::Token(sp, token::DocComment(_)), 0) => TokenTree::Token(sp, token::Pound),\n-            (&TokenTree::Token(sp, token::DocComment(name)), 1)\n-                if doc_comment_style(&name.as_str()) == AttrStyle::Inner => {\n-                TokenTree::Token(sp, token::Not)\n-            }\n-            (&TokenTree::Token(sp, token::DocComment(name)), _) => {\n-                let stripped = strip_doc_comment_decoration(&name.as_str());\n-\n-                // Searches for the occurrences of `\"#*` and returns the minimum number of `#`s\n-                // required to wrap the text.\n-                let num_of_hashes = stripped.chars()\n-                    .scan(0, |cnt, x| {\n-                        *cnt = if x == '\"' {\n-                            1\n-                        } else if *cnt != 0 && x == '#' {\n-                            *cnt + 1\n-                        } else {\n-                            0\n-                        };\n-                        Some(*cnt)\n-                    })\n-                    .max()\n-                    .unwrap_or(0);\n-\n-                TokenTree::Delimited(sp, Rc::new(Delimited {\n-                    delim: token::Bracket,\n-                    tts: vec![TokenTree::Token(sp, token::Ident(ast::Ident::from_str(\"doc\"))),\n-                              TokenTree::Token(sp, token::Eq),\n-                              TokenTree::Token(sp, token::Literal(\n-                                  token::StrRaw(Symbol::intern(&stripped), num_of_hashes), None))],\n-                }))\n-            }\n-            (&TokenTree::Delimited(_, ref delimed), _) if delimed.delim == token::NoDelim => {\n-                delimed.tts[index].clone()\n-            }\n-            (&TokenTree::Delimited(span, ref delimed), _) => {\n-                if index == 0 {\n-                    return delimed.open_tt(span);\n-                }\n-                if index == delimed.tts.len() + 1 {\n-                    return delimed.close_tt(span);\n-                }\n-                delimed.tts[index - 1].clone()\n-            }\n-            _ => panic!(\"Cannot expand a token tree\"),\n-        }\n-    }\n-\n     /// Use this token tree as a matcher to parse given tts.\n-    pub fn parse(cx: &base::ExtCtxt, mtch: &[quoted::TokenTree], tts: &[TokenTree])\n+    pub fn parse(cx: &base::ExtCtxt, mtch: &[quoted::TokenTree], tts: TokenStream)\n                  -> macro_parser::NamedParseResult {\n         // `None` is because we're not interpolating\n         let directory = Directory {\n             path: cx.current_expansion.module.directory.clone(),\n             ownership: cx.current_expansion.directory_ownership,\n         };\n-        macro_parser::parse(cx.parse_sess(), tts.iter().cloned().collect(), mtch, Some(directory))\n+        macro_parser::parse(cx.parse_sess(), tts, mtch, Some(directory))\n     }\n \n     /// Check if this TokenTree is equal to the other, regardless of span information.\n     pub fn eq_unspanned(&self, other: &TokenTree) -> bool {\n         match (self, other) {\n             (&TokenTree::Token(_, ref tk), &TokenTree::Token(_, ref tk2)) => tk == tk2,\n             (&TokenTree::Delimited(_, ref dl), &TokenTree::Delimited(_, ref dl2)) => {\n-                (*dl).delim == (*dl2).delim && dl.tts.len() == dl2.tts.len() &&\n-                {\n-                    for (tt1, tt2) in dl.tts.iter().zip(dl2.tts.iter()) {\n-                        if !tt1.eq_unspanned(tt2) {\n-                            return false;\n-                        }\n-                    }\n-                    true\n-                }\n+                dl.delim == dl2.delim &&\n+                dl.stream().trees().zip(dl2.stream().trees()).all(|(tt, tt2)| tt.eq_unspanned(&tt2))\n             }\n             (_, _) => false,\n         }\n@@ -213,64 +136,6 @@ impl TokenTree {\n             _ => false,\n         }\n     }\n-\n-    /// Indicates if the token is an identifier.\n-    pub fn is_ident(&self) -> bool {\n-        self.maybe_ident().is_some()\n-    }\n-\n-    /// Returns an identifier.\n-    pub fn maybe_ident(&self) -> Option<ast::Ident> {\n-        match *self {\n-            TokenTree::Token(_, Token::Ident(t)) => Some(t.clone()),\n-            TokenTree::Delimited(_, ref dl) => {\n-                let tts = dl.subtrees();\n-                if tts.len() != 1 {\n-                    return None;\n-                }\n-                tts[0].maybe_ident()\n-            }\n-            _ => None,\n-        }\n-    }\n-\n-    /// Returns a Token literal.\n-    pub fn maybe_lit(&self) -> Option<token::Lit> {\n-        match *self {\n-            TokenTree::Token(_, Token::Literal(l, _)) => Some(l.clone()),\n-            TokenTree::Delimited(_, ref dl) => {\n-                let tts = dl.subtrees();\n-                if tts.len() != 1 {\n-                    return None;\n-                }\n-                tts[0].maybe_lit()\n-            }\n-            _ => None,\n-        }\n-    }\n-\n-    /// Returns an AST string literal.\n-    pub fn maybe_str(&self) -> Option<ast::Lit> {\n-        match *self {\n-            TokenTree::Token(sp, Token::Literal(Lit::Str_(s), _)) => {\n-                let l = LitKind::Str(Symbol::intern(&parse::str_lit(&s.as_str())),\n-                                     ast::StrStyle::Cooked);\n-                Some(Spanned {\n-                    node: l,\n-                    span: sp,\n-                })\n-            }\n-            TokenTree::Token(sp, Token::Literal(Lit::StrRaw(s, n), _)) => {\n-                let l = LitKind::Str(Symbol::intern(&parse::raw_str_lit(&s.as_str())),\n-                                     ast::StrStyle::Raw(n));\n-                Some(Spanned {\n-                    node: l,\n-                    span: sp,\n-                })\n-            }\n-            _ => None,\n-        }\n-    }\n }\n \n /// # Token Streams\n@@ -299,7 +164,7 @@ impl From<TokenTree> for TokenStream {\n \n impl<T: Into<TokenStream>> iter::FromIterator<T> for TokenStream {\n     fn from_iter<I: IntoIterator<Item = T>>(iter: I) -> Self {\n-        TokenStream::concat(iter.into_iter().map(Into::into))\n+        TokenStream::concat(iter.into_iter().map(Into::into).collect::<Vec<_>>())\n     }\n }\n \n@@ -323,102 +188,185 @@ impl TokenStream {\n         }\n     }\n \n-    pub fn concat<I: IntoIterator<Item = TokenStream>>(streams: I) -> TokenStream {\n-        let mut streams = streams.into_iter().filter(|stream| !stream.is_empty());\n-        let first_stream = match streams.next() {\n-            Some(stream) => stream,\n-            None => return TokenStream::empty(),\n-        };\n-        let second_stream = match streams.next() {\n-            Some(stream) => stream,\n-            None => return first_stream,\n-        };\n-        let mut vec = vec![first_stream, second_stream];\n-        vec.extend(streams);\n-        TokenStream { kind: TokenStreamKind::Stream(RcSlice::new(vec)) }\n+    pub fn concat(mut streams: Vec<TokenStream>) -> TokenStream {\n+        match streams.len() {\n+            0 => TokenStream::empty(),\n+            1 => TokenStream::from(streams.pop().unwrap()),\n+            _ => TokenStream::concat_rc_slice(RcSlice::new(streams)),\n+        }\n     }\n \n-    pub fn trees<'a>(&'a self) -> Cursor {\n+    fn concat_rc_slice(streams: RcSlice<TokenStream>) -> TokenStream {\n+        TokenStream { kind: TokenStreamKind::Stream(streams) }\n+    }\n+\n+    pub fn trees(&self) -> Cursor {\n+        self.clone().into_trees()\n+    }\n+\n+    pub fn into_trees(self) -> Cursor {\n         Cursor::new(self)\n     }\n \n     /// Compares two TokenStreams, checking equality without regarding span information.\n     pub fn eq_unspanned(&self, other: &TokenStream) -> bool {\n         for (t1, t2) in self.trees().zip(other.trees()) {\n-            if !t1.eq_unspanned(t2) {\n+            if !t1.eq_unspanned(&t2) {\n                 return false;\n             }\n         }\n         true\n     }\n }\n \n-pub struct Cursor<'a> {\n-    current_frame: CursorFrame<'a>,\n-    stack: Vec<CursorFrame<'a>>,\n-}\n-\n-impl<'a> Iterator for Cursor<'a> {\n-    type Item = &'a TokenTree;\n+pub struct Cursor(CursorKind);\n \n-    fn next(&mut self) -> Option<&'a TokenTree> {\n-        let tree = self.peek();\n-        self.current_frame = self.stack.pop().unwrap_or(CursorFrame::Empty);\n-        tree\n-    }\n+enum CursorKind {\n+    Empty,\n+    Tree(TokenTree, bool /* consumed? */),\n+    Stream(StreamCursor),\n }\n \n-enum CursorFrame<'a> {\n-    Empty,\n-    Tree(&'a TokenTree),\n-    Stream(&'a RcSlice<TokenStream>, usize),\n+struct StreamCursor {\n+    stream: RcSlice<TokenStream>,\n+    index: usize,\n+    stack: Vec<(RcSlice<TokenStream>, usize)>,\n }\n \n-impl<'a> CursorFrame<'a> {\n-    fn new(stream: &'a TokenStream) -> Self {\n-        match stream.kind {\n-            TokenStreamKind::Empty => CursorFrame::Empty,\n-            TokenStreamKind::Tree(ref tree) => CursorFrame::Tree(tree),\n-            TokenStreamKind::Stream(ref stream) => CursorFrame::Stream(stream, 0),\n+impl Iterator for Cursor {\n+    type Item = TokenTree;\n+\n+    fn next(&mut self) -> Option<TokenTree> {\n+        let cursor = match self.0 {\n+            CursorKind::Stream(ref mut cursor) => cursor,\n+            CursorKind::Tree(ref tree, ref mut consumed @ false) => {\n+                *consumed = true;\n+                return Some(tree.clone());\n+            }\n+            _ => return None,\n+        };\n+\n+        loop {\n+            if cursor.index < cursor.stream.len() {\n+                match cursor.stream[cursor.index].kind.clone() {\n+                    TokenStreamKind::Tree(tree) => {\n+                        cursor.index += 1;\n+                        return Some(tree);\n+                    }\n+                    TokenStreamKind::Stream(stream) => {\n+                        cursor.stack.push((mem::replace(&mut cursor.stream, stream),\n+                                           mem::replace(&mut cursor.index, 0) + 1));\n+                    }\n+                    TokenStreamKind::Empty => {\n+                        cursor.index += 1;\n+                    }\n+                }\n+            } else if let Some((stream, index)) = cursor.stack.pop() {\n+                cursor.stream = stream;\n+                cursor.index = index;\n+            } else {\n+                return None;\n+            }\n         }\n     }\n }\n \n-impl<'a> Cursor<'a> {\n-    fn new(stream: &'a TokenStream) -> Self {\n-        Cursor {\n-            current_frame: CursorFrame::new(stream),\n-            stack: Vec::new(),\n-        }\n+impl Cursor {\n+    fn new(stream: TokenStream) -> Self {\n+        Cursor(match stream.kind {\n+            TokenStreamKind::Empty => CursorKind::Empty,\n+            TokenStreamKind::Tree(tree) => CursorKind::Tree(tree, false),\n+            TokenStreamKind::Stream(stream) => {\n+                CursorKind::Stream(StreamCursor { stream: stream, index: 0, stack: Vec::new() })\n+            }\n+        })\n     }\n \n-    pub fn peek(&mut self) -> Option<&'a TokenTree> {\n-        while let CursorFrame::Stream(stream, index) = self.current_frame {\n-            self.current_frame = if index == stream.len() {\n-                self.stack.pop().unwrap_or(CursorFrame::Empty)\n-            } else {\n-                self.stack.push(CursorFrame::Stream(stream, index + 1));\n-                CursorFrame::new(&stream[index])\n-            };\n+    pub fn original_stream(self) -> TokenStream {\n+        match self.0 {\n+            CursorKind::Empty => TokenStream::empty(),\n+            CursorKind::Tree(tree, _) => tree.into(),\n+            CursorKind::Stream(cursor) => TokenStream::concat_rc_slice({\n+                cursor.stack.get(0).cloned().map(|(stream, _)| stream).unwrap_or(cursor.stream)\n+            }),\n         }\n+    }\n+\n+    pub fn look_ahead(&self, n: usize) -> Option<TokenTree> {\n+        fn look_ahead(streams: &[TokenStream], mut n: usize) -> Result<TokenTree, usize> {\n+            for stream in streams {\n+                n = match stream.kind {\n+                    TokenStreamKind::Tree(ref tree) if n == 0 => return Ok(tree.clone()),\n+                    TokenStreamKind::Tree(..) => n - 1,\n+                    TokenStreamKind::Stream(ref stream) => match look_ahead(stream, n) {\n+                        Ok(tree) => return Ok(tree),\n+                        Err(n) => n,\n+                    },\n+                    _ => n,\n+                };\n+            }\n \n-        match self.current_frame {\n-            CursorFrame::Empty => None,\n-            CursorFrame::Tree(tree) => Some(tree),\n-            CursorFrame::Stream(..) => unreachable!(),\n+            Err(n)\n         }\n+\n+        match self.0 {\n+            CursorKind::Empty | CursorKind::Tree(_, true) => Err(n),\n+            CursorKind::Tree(ref tree, false) => look_ahead(&[tree.clone().into()], n),\n+            CursorKind::Stream(ref cursor) => {\n+                look_ahead(&cursor.stream[cursor.index ..], n).or_else(|mut n| {\n+                    for &(ref stream, index) in cursor.stack.iter().rev() {\n+                        n = match look_ahead(&stream[index..], n) {\n+                            Ok(tree) => return Ok(tree),\n+                            Err(n) => n,\n+                        }\n+                    }\n+\n+                    Err(n)\n+                })\n+            }\n+        }.ok()\n+    }\n+}\n+\n+/// The `TokenStream` type is large enough to represent a single `TokenTree` without allocation.\n+/// `ThinTokenStream` is smaller, but needs to allocate to represent a single `TokenTree`.\n+/// We must use `ThinTokenStream` in `TokenTree::Delimited` to avoid infinite size due to recursion.\n+#[derive(Debug, Clone)]\n+pub struct ThinTokenStream(Option<RcSlice<TokenStream>>);\n+\n+impl From<TokenStream> for ThinTokenStream {\n+    fn from(stream: TokenStream) -> ThinTokenStream {\n+        ThinTokenStream(match stream.kind {\n+            TokenStreamKind::Empty => None,\n+            TokenStreamKind::Tree(tree) => Some(RcSlice::new(vec![tree.into()])),\n+            TokenStreamKind::Stream(stream) => Some(stream),\n+        })\n+    }\n+}\n+\n+impl From<ThinTokenStream> for TokenStream {\n+    fn from(stream: ThinTokenStream) -> TokenStream {\n+        stream.0.map(TokenStream::concat_rc_slice).unwrap_or_else(TokenStream::empty)\n+    }\n+}\n+\n+impl Eq for ThinTokenStream {}\n+\n+impl PartialEq<ThinTokenStream> for ThinTokenStream {\n+    fn eq(&self, other: &ThinTokenStream) -> bool {\n+        TokenStream::from(self.clone()) == TokenStream::from(other.clone())\n     }\n }\n \n impl fmt::Display for TokenStream {\n     fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {\n-        f.write_str(&pprust::tts_to_string(&self.trees().cloned().collect::<Vec<_>>()))\n+        f.write_str(&pprust::tts_to_string(&self.trees().collect::<Vec<_>>()))\n     }\n }\n \n impl Encodable for TokenStream {\n     fn encode<E: Encoder>(&self, encoder: &mut E) -> Result<(), E::Error> {\n-        self.trees().cloned().collect::<Vec<_>>().encode(encoder)\n+        self.trees().collect::<Vec<_>>().encode(encoder)\n     }\n }\n \n@@ -428,17 +376,43 @@ impl Decodable for TokenStream {\n     }\n }\n \n+impl Hash for TokenStream {\n+    fn hash<H: hash::Hasher>(&self, state: &mut H) {\n+        for tree in self.trees() {\n+            tree.hash(state);\n+        }\n+    }\n+}\n+\n+impl Encodable for ThinTokenStream {\n+    fn encode<E: Encoder>(&self, encoder: &mut E) -> Result<(), E::Error> {\n+        TokenStream::from(self.clone()).encode(encoder)\n+    }\n+}\n+\n+impl Decodable for ThinTokenStream {\n+    fn decode<D: Decoder>(decoder: &mut D) -> Result<ThinTokenStream, D::Error> {\n+        TokenStream::decode(decoder).map(Into::into)\n+    }\n+}\n+\n+impl Hash for ThinTokenStream {\n+    fn hash<H: hash::Hasher>(&self, state: &mut H) {\n+        TokenStream::from(self.clone()).hash(state);\n+    }\n+}\n+\n \n #[cfg(test)]\n mod tests {\n     use super::*;\n     use syntax::ast::Ident;\n     use syntax_pos::{Span, BytePos, NO_EXPANSION};\n     use parse::token::Token;\n-    use util::parser_testing::string_to_tts;\n+    use util::parser_testing::string_to_stream;\n \n     fn string_to_ts(string: &str) -> TokenStream {\n-        string_to_tts(string.to_owned()).into_iter().collect()\n+        string_to_stream(string.to_owned())\n     }\n \n     fn sp(a: u32, b: u32) -> Span {\n@@ -454,24 +428,16 @@ mod tests {\n         let test_res = string_to_ts(\"foo::bar::baz\");\n         let test_fst = string_to_ts(\"foo::bar\");\n         let test_snd = string_to_ts(\"::baz\");\n-        let eq_res = TokenStream::concat([test_fst, test_snd].iter().cloned());\n+        let eq_res = TokenStream::concat(vec![test_fst, test_snd]);\n         assert_eq!(test_res.trees().count(), 5);\n         assert_eq!(eq_res.trees().count(), 5);\n         assert_eq!(test_res.eq_unspanned(&eq_res), true);\n     }\n \n-    #[test]\n-    fn test_from_to_bijection() {\n-        let test_start = string_to_tts(\"foo::bar(baz)\".to_string());\n-        let ts = test_start.iter().cloned().collect::<TokenStream>();\n-        let test_end: Vec<TokenTree> = ts.trees().cloned().collect();\n-        assert_eq!(test_start, test_end)\n-    }\n-\n     #[test]\n     fn test_to_from_bijection() {\n         let test_start = string_to_ts(\"foo::bar(baz)\");\n-        let test_end = test_start.trees().cloned().collect();\n+        let test_end = test_start.trees().collect();\n         assert_eq!(test_start, test_end)\n     }\n "}, {"sha": "51eb295b502a70bd764ede600fbbc877025742c4", "filename": "src/libsyntax/util/parser_testing.rs", "status": "modified", "additions": 4, "deletions": 4, "changes": 8, "blob_url": "https://github.com/rust-lang/rust/blob/be304afc8c2b1a364bd406888b5378897ed82a9f/src%2Flibsyntax%2Futil%2Fparser_testing.rs", "raw_url": "https://github.com/rust-lang/rust/raw/be304afc8c2b1a364bd406888b5378897ed82a9f/src%2Flibsyntax%2Futil%2Fparser_testing.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Futil%2Fparser_testing.rs?ref=be304afc8c2b1a364bd406888b5378897ed82a9f", "patch": "@@ -9,17 +9,17 @@\n // except according to those terms.\n \n use ast::{self, Ident};\n-use parse::{ParseSess,PResult,filemap_to_tts};\n+use parse::{ParseSess, PResult, filemap_to_stream};\n use parse::{lexer, new_parser_from_source_str};\n use parse::parser::Parser;\n use ptr::P;\n-use tokenstream;\n+use tokenstream::TokenStream;\n use std::iter::Peekable;\n \n /// Map a string to tts, using a made-up filename:\n-pub fn string_to_tts(source_str: String) -> Vec<tokenstream::TokenTree> {\n+pub fn string_to_stream(source_str: String) -> TokenStream {\n     let ps = ParseSess::new();\n-    filemap_to_tts(&ps, ps.codemap().new_filemap(\"bogofile\".to_string(), None, source_str))\n+    filemap_to_stream(&ps, ps.codemap().new_filemap(\"bogofile\".to_string(), None, source_str))\n }\n \n /// Map string to parser (via tts)"}, {"sha": "195fb23f9d8c75ed8b8c3f0e4ce1042f207b259b", "filename": "src/libsyntax/util/rc_slice.rs", "status": "modified", "additions": 0, "deletions": 7, "changes": 7, "blob_url": "https://github.com/rust-lang/rust/blob/be304afc8c2b1a364bd406888b5378897ed82a9f/src%2Flibsyntax%2Futil%2Frc_slice.rs", "raw_url": "https://github.com/rust-lang/rust/raw/be304afc8c2b1a364bd406888b5378897ed82a9f/src%2Flibsyntax%2Futil%2Frc_slice.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Futil%2Frc_slice.rs?ref=be304afc8c2b1a364bd406888b5378897ed82a9f", "patch": "@@ -8,7 +8,6 @@\n // option. This file may not be copied, modified, or distributed\n // except according to those terms.\n \n-use std::hash::{self, Hash};\n use std::fmt;\n use std::ops::Deref;\n use std::rc::Rc;\n@@ -37,12 +36,6 @@ impl<T> Deref for RcSlice<T> {\n     }\n }\n \n-impl<T: Hash> Hash for RcSlice<T> {\n-    fn hash<H: hash::Hasher>(&self, state: &mut H) {\n-        self.deref().hash(state);\n-    }\n-}\n-\n impl<T: fmt::Debug> fmt::Debug for RcSlice<T> {\n     fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {\n         fmt::Debug::fmt(self.deref(), f)"}, {"sha": "767ec94a0ce61955123341be14a3ee161eea9739", "filename": "src/libsyntax_ext/asm.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/be304afc8c2b1a364bd406888b5378897ed82a9f/src%2Flibsyntax_ext%2Fasm.rs", "raw_url": "https://github.com/rust-lang/rust/raw/be304afc8c2b1a364bd406888b5378897ed82a9f/src%2Flibsyntax_ext%2Fasm.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax_ext%2Fasm.rs?ref=be304afc8c2b1a364bd406888b5378897ed82a9f", "patch": "@@ -107,7 +107,7 @@ pub fn expand_asm<'cx>(cx: &'cx mut ExtCtxt,\n                 if p2.token != token::Eof {\n                     let mut extra_tts = panictry!(p2.parse_all_token_trees());\n                     extra_tts.extend(tts[first_colon..].iter().cloned());\n-                    p = parse::tts_to_parser(cx.parse_sess, extra_tts);\n+                    p = parse::stream_to_parser(cx.parse_sess, extra_tts.into_iter().collect());\n                 }\n \n                 asm = s;"}, {"sha": "5139b68bce7fd0d6686aa3293ee75697567c5c7f", "filename": "src/test/run-pass-fulldeps/ast_stmt_expr_attr.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/be304afc8c2b1a364bd406888b5378897ed82a9f/src%2Ftest%2Frun-pass-fulldeps%2Fast_stmt_expr_attr.rs", "raw_url": "https://github.com/rust-lang/rust/raw/be304afc8c2b1a364bd406888b5378897ed82a9f/src%2Ftest%2Frun-pass-fulldeps%2Fast_stmt_expr_attr.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ftest%2Frun-pass-fulldeps%2Fast_stmt_expr_attr.rs?ref=be304afc8c2b1a364bd406888b5378897ed82a9f", "patch": "@@ -18,7 +18,7 @@ use syntax::ast::*;\n use syntax::attr::*;\n use syntax::ast;\n use syntax::parse;\n-use syntax::parse::{ParseSess,filemap_to_tts, PResult};\n+use syntax::parse::{ParseSess, PResult};\n use syntax::parse::new_parser_from_source_str;\n use syntax::parse::parser::Parser;\n use syntax::parse::token;"}, {"sha": "2f94a440e72da3ca5342883acb0d1660ce27e28c", "filename": "src/test/run-pass-fulldeps/auxiliary/cond_plugin.rs", "status": "modified", "additions": 4, "deletions": 4, "changes": 8, "blob_url": "https://github.com/rust-lang/rust/blob/be304afc8c2b1a364bd406888b5378897ed82a9f/src%2Ftest%2Frun-pass-fulldeps%2Fauxiliary%2Fcond_plugin.rs", "raw_url": "https://github.com/rust-lang/rust/raw/be304afc8c2b1a364bd406888b5378897ed82a9f/src%2Ftest%2Frun-pass-fulldeps%2Fauxiliary%2Fcond_plugin.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ftest%2Frun-pass-fulldeps%2Fauxiliary%2Fcond_plugin.rs?ref=be304afc8c2b1a364bd406888b5378897ed82a9f", "patch": "@@ -32,13 +32,13 @@ pub fn plugin_registrar(reg: &mut Registry) {\n \n fn cond(input: TokenStream) -> TokenStream {\n     let mut conds = Vec::new();\n-    let mut input = input.trees();\n+    let mut input = input.trees().peekable();\n     while let Some(tree) = input.next() {\n-        let cond: TokenStream = match *tree {\n-            TokenTree::Delimited(_, ref delimited) => delimited.tts.iter().cloned().collect(),\n+        let mut cond = match tree {\n+            TokenTree::Delimited(_, ref delimited) => delimited.stream(),\n             _ => panic!(\"Invalid input\"),\n         };\n-        let mut trees = cond.trees().cloned();\n+        let mut trees = cond.trees();\n         let test = trees.next();\n         let rhs = trees.collect::<TokenStream>();\n         if rhs.is_empty() {"}, {"sha": "134e36c587bede9bd15684f3dc20c3fc0c3cf91d", "filename": "src/test/run-pass-fulldeps/auxiliary/plugin_args.rs", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/rust-lang/rust/blob/be304afc8c2b1a364bd406888b5378897ed82a9f/src%2Ftest%2Frun-pass-fulldeps%2Fauxiliary%2Fplugin_args.rs", "raw_url": "https://github.com/rust-lang/rust/raw/be304afc8c2b1a364bd406888b5378897ed82a9f/src%2Ftest%2Frun-pass-fulldeps%2Fauxiliary%2Fplugin_args.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ftest%2Frun-pass-fulldeps%2Fauxiliary%2Fplugin_args.rs?ref=be304afc8c2b1a364bd406888b5378897ed82a9f", "patch": "@@ -26,7 +26,7 @@ use syntax::print::pprust;\n use syntax::ptr::P;\n use syntax::symbol::Symbol;\n use syntax_pos::Span;\n-use syntax::tokenstream;\n+use syntax::tokenstream::TokenStream;\n use rustc_plugin::Registry;\n \n struct Expander {\n@@ -37,7 +37,7 @@ impl TTMacroExpander for Expander {\n     fn expand<'cx>(&self,\n                    ecx: &'cx mut ExtCtxt,\n                    sp: Span,\n-                   _: &[tokenstream::TokenTree]) -> Box<MacResult+'cx> {\n+                   _: TokenStream) -> Box<MacResult+'cx> {\n         let args = self.args.iter().map(|i| pprust::meta_list_item_to_string(i))\n             .collect::<Vec<_>>().join(\", \");\n         MacEager::expr(ecx.expr_str(sp, Symbol::intern(&args)))"}, {"sha": "c9fa96b83c280a6d9325bef110aab04a74332fc4", "filename": "src/test/run-pass-fulldeps/auxiliary/procedural_mbe_matching.rs", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/rust-lang/rust/blob/be304afc8c2b1a364bd406888b5378897ed82a9f/src%2Ftest%2Frun-pass-fulldeps%2Fauxiliary%2Fprocedural_mbe_matching.rs", "raw_url": "https://github.com/rust-lang/rust/raw/be304afc8c2b1a364bd406888b5378897ed82a9f/src%2Ftest%2Frun-pass-fulldeps%2Fauxiliary%2Fprocedural_mbe_matching.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ftest%2Frun-pass-fulldeps%2Fauxiliary%2Fprocedural_mbe_matching.rs?ref=be304afc8c2b1a364bd406888b5378897ed82a9f", "patch": "@@ -35,8 +35,8 @@ fn expand_mbe_matches(cx: &mut ExtCtxt, _: Span, args: &[TokenTree])\n         -> Box<MacResult + 'static> {\n \n     let mbe_matcher = quote_tokens!(cx, $$matched:expr, $$($$pat:pat)|+);\n-    let mbe_matcher = quoted::parse(&mbe_matcher, true, cx.parse_sess);\n-    let map = match TokenTree::parse(cx, &mbe_matcher, args) {\n+    let mbe_matcher = quoted::parse(mbe_matcher.into_iter().collect(), true, cx.parse_sess);\n+    let map = match TokenTree::parse(cx, &mbe_matcher, args.iter().cloned().collect()) {\n         Success(map) => map,\n         Failure(_, tok) => {\n             panic!(\"expected Success, but got Failure: {}\", parse_failure_msg(tok));"}]}