{"sha": "aa6bfaf04b258e3e23d3f7063de4f2d37845ddec", "node_id": "C_kwDOAAsO6NoAKGFhNmJmYWYwNGIyNThlM2UyM2QzZjcwNjNkZTRmMmQzNzg0NWRkZWM", "commit": {"author": {"name": "Nicholas Nethercote", "email": "n.nethercote@gmail.com", "date": "2022-09-25T23:18:23Z"}, "committer": {"name": "Nicholas Nethercote", "email": "n.nethercote@gmail.com", "date": "2022-09-26T03:36:35Z"}, "message": "Make `rustc_lexer::cursor::Cursor` public.\n\n`Cursor` is currently hidden, and the main tokenization path uses\n`rustc_lexer::first_token` which involves constructing a new `Cursor`\nfor every single token, which is weird. Also, `first_token` also can't\nhandle empty input, so callers have to check for that first.\n\nThis commit makes `Cursor` public, so `StringReader` can contain a\n`Cursor`, which results in a simpler structure. The commit also changes\n`StringReader::advance_token` so it returns an `Option<Token>`,\nsimplifying the the empty input case.", "tree": {"sha": "b6735a3ee3d176bdadcb589e37503dd4d22c42a6", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/b6735a3ee3d176bdadcb589e37503dd4d22c42a6"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/aa6bfaf04b258e3e23d3f7063de4f2d37845ddec", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/aa6bfaf04b258e3e23d3f7063de4f2d37845ddec", "html_url": "https://github.com/rust-lang/rust/commit/aa6bfaf04b258e3e23d3f7063de4f2d37845ddec", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/aa6bfaf04b258e3e23d3f7063de4f2d37845ddec/comments", "author": {"login": "nnethercote", "id": 1940286, "node_id": "MDQ6VXNlcjE5NDAyODY=", "avatar_url": "https://avatars.githubusercontent.com/u/1940286?v=4", "gravatar_id": "", "url": "https://api.github.com/users/nnethercote", "html_url": "https://github.com/nnethercote", "followers_url": "https://api.github.com/users/nnethercote/followers", "following_url": "https://api.github.com/users/nnethercote/following{/other_user}", "gists_url": "https://api.github.com/users/nnethercote/gists{/gist_id}", "starred_url": "https://api.github.com/users/nnethercote/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/nnethercote/subscriptions", "organizations_url": "https://api.github.com/users/nnethercote/orgs", "repos_url": "https://api.github.com/users/nnethercote/repos", "events_url": "https://api.github.com/users/nnethercote/events{/privacy}", "received_events_url": "https://api.github.com/users/nnethercote/received_events", "type": "User", "site_admin": false}, "committer": {"login": "nnethercote", "id": 1940286, "node_id": "MDQ6VXNlcjE5NDAyODY=", "avatar_url": "https://avatars.githubusercontent.com/u/1940286?v=4", "gravatar_id": "", "url": "https://api.github.com/users/nnethercote", "html_url": "https://github.com/nnethercote", "followers_url": "https://api.github.com/users/nnethercote/followers", "following_url": "https://api.github.com/users/nnethercote/following{/other_user}", "gists_url": "https://api.github.com/users/nnethercote/gists{/gist_id}", "starred_url": "https://api.github.com/users/nnethercote/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/nnethercote/subscriptions", "organizations_url": "https://api.github.com/users/nnethercote/orgs", "repos_url": "https://api.github.com/users/nnethercote/repos", "events_url": "https://api.github.com/users/nnethercote/events{/privacy}", "received_events_url": "https://api.github.com/users/nnethercote/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "33516ac09af7038efce6332afdedc758a3943609", "url": "https://api.github.com/repos/rust-lang/rust/commits/33516ac09af7038efce6332afdedc758a3943609", "html_url": "https://github.com/rust-lang/rust/commit/33516ac09af7038efce6332afdedc758a3943609"}], "stats": {"total": 62, "additions": 26, "deletions": 36}, "files": [{"sha": "df9b6afdf5625e9b61977342ab71c3d643f75a8c", "filename": "compiler/rustc_lexer/src/cursor.rs", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/rust-lang/rust/blob/aa6bfaf04b258e3e23d3f7063de4f2d37845ddec/compiler%2Frustc_lexer%2Fsrc%2Fcursor.rs", "raw_url": "https://github.com/rust-lang/rust/raw/aa6bfaf04b258e3e23d3f7063de4f2d37845ddec/compiler%2Frustc_lexer%2Fsrc%2Fcursor.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_lexer%2Fsrc%2Fcursor.rs?ref=aa6bfaf04b258e3e23d3f7063de4f2d37845ddec", "patch": "@@ -4,7 +4,7 @@ use std::str::Chars;\n ///\n /// Next characters can be peeked via `first` method,\n /// and position can be shifted forward via `bump` method.\n-pub(crate) struct Cursor<'a> {\n+pub struct Cursor<'a> {\n     initial_len: usize,\n     /// Iterator over chars. Slightly faster than a &str.\n     chars: Chars<'a>,\n@@ -15,7 +15,7 @@ pub(crate) struct Cursor<'a> {\n pub(crate) const EOF_CHAR: char = '\\0';\n \n impl<'a> Cursor<'a> {\n-    pub(crate) fn new(input: &'a str) -> Cursor<'a> {\n+    pub fn new(input: &'a str) -> Cursor<'a> {\n         Cursor {\n             initial_len: input.len(),\n             chars: input.chars(),"}, {"sha": "9182b649bf3d9824d1508c3bd261848a16699cb1", "filename": "compiler/rustc_lexer/src/lib.rs", "status": "modified", "additions": 7, "deletions": 19, "changes": 26, "blob_url": "https://github.com/rust-lang/rust/blob/aa6bfaf04b258e3e23d3f7063de4f2d37845ddec/compiler%2Frustc_lexer%2Fsrc%2Flib.rs", "raw_url": "https://github.com/rust-lang/rust/raw/aa6bfaf04b258e3e23d3f7063de4f2d37845ddec/compiler%2Frustc_lexer%2Fsrc%2Flib.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_lexer%2Fsrc%2Flib.rs?ref=aa6bfaf04b258e3e23d3f7063de4f2d37845ddec", "patch": "@@ -23,7 +23,7 @@\n // We want to be able to build this crate with a stable compiler, so no\n // `#![feature]` attributes should be added.\n \n-mod cursor;\n+pub mod cursor;\n pub mod unescape;\n \n #[cfg(test)]\n@@ -219,13 +219,6 @@ pub fn strip_shebang(input: &str) -> Option<usize> {\n     None\n }\n \n-/// Parses the first token from the provided input string.\n-#[inline]\n-pub fn first_token(input: &str) -> Token {\n-    debug_assert!(!input.is_empty());\n-    Cursor::new(input).advance_token()\n-}\n-\n /// Validates a raw string literal. Used for getting more information about a\n /// problem with a `RawStr`/`RawByteStr` with a `None` field.\n #[inline]\n@@ -242,14 +235,7 @@ pub fn validate_raw_str(input: &str, prefix_len: u32) -> Result<(), RawStrError>\n /// Creates an iterator that produces tokens from the input string.\n pub fn tokenize(input: &str) -> impl Iterator<Item = Token> + '_ {\n     let mut cursor = Cursor::new(input);\n-    std::iter::from_fn(move || {\n-        if cursor.is_eof() {\n-            None\n-        } else {\n-            cursor.reset_len_consumed();\n-            Some(cursor.advance_token())\n-        }\n-    })\n+    std::iter::from_fn(move || cursor.advance_token())\n }\n \n /// True if `c` is considered a whitespace according to Rust language definition.\n@@ -311,8 +297,8 @@ pub fn is_ident(string: &str) -> bool {\n \n impl Cursor<'_> {\n     /// Parses a token from the input string.\n-    fn advance_token(&mut self) -> Token {\n-        let first_char = self.bump().unwrap();\n+    pub fn advance_token(&mut self) -> Option<Token> {\n+        let first_char = self.bump()?;\n         let token_kind = match first_char {\n             // Slash, comment or block comment.\n             '/' => match self.first() {\n@@ -433,7 +419,9 @@ impl Cursor<'_> {\n             }\n             _ => Unknown,\n         };\n-        Token::new(token_kind, self.len_consumed())\n+        let res = Some(Token::new(token_kind, self.len_consumed()));\n+        self.reset_len_consumed();\n+        res\n     }\n \n     fn line_comment(&mut self) -> TokenKind {"}, {"sha": "c182e86332a3c0f7a9b7140cba393d0fa1780519", "filename": "compiler/rustc_parse/src/lexer/mod.rs", "status": "modified", "additions": 13, "deletions": 10, "changes": 23, "blob_url": "https://github.com/rust-lang/rust/blob/aa6bfaf04b258e3e23d3f7063de4f2d37845ddec/compiler%2Frustc_parse%2Fsrc%2Flexer%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/aa6bfaf04b258e3e23d3f7063de4f2d37845ddec/compiler%2Frustc_parse%2Fsrc%2Flexer%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_parse%2Fsrc%2Flexer%2Fmod.rs?ref=aa6bfaf04b258e3e23d3f7063de4f2d37845ddec", "patch": "@@ -4,6 +4,7 @@ use rustc_ast::token::{self, CommentKind, Delimiter, Token, TokenKind};\n use rustc_ast::tokenstream::TokenStream;\n use rustc_ast::util::unicode::contains_text_flow_control_chars;\n use rustc_errors::{error_code, Applicability, DiagnosticBuilder, ErrorGuaranteed, PResult};\n+use rustc_lexer::cursor::Cursor;\n use rustc_lexer::unescape::{self, Mode};\n use rustc_lexer::{Base, DocStyle, RawStrError};\n use rustc_session::lint::builtin::{\n@@ -48,7 +49,9 @@ pub(crate) fn parse_token_trees<'a>(\n         start_pos = start_pos + BytePos::from_usize(shebang_len);\n     }\n \n-    let string_reader = StringReader { sess, start_pos, pos: start_pos, src, override_span };\n+    let cursor = Cursor::new(src);\n+    let string_reader =\n+        StringReader { sess, start_pos, pos: start_pos, src, cursor, override_span };\n     tokentrees::TokenTreesReader::parse_token_trees(string_reader)\n }\n \n@@ -60,6 +63,8 @@ struct StringReader<'a> {\n     pos: BytePos,\n     /// Source text to tokenize.\n     src: &'a str,\n+    /// Cursor for getting lexer tokens.\n+    cursor: Cursor<'a>,\n     override_span: Option<Span>,\n }\n \n@@ -75,15 +80,13 @@ impl<'a> StringReader<'a> {\n \n         // Skip trivial (whitespace & comments) tokens\n         loop {\n-            let start_src_index = self.src_index(self.pos);\n-            let text: &str = &self.src[start_src_index..];\n-\n-            if text.is_empty() {\n-                let span = self.mk_sp(self.pos, self.pos);\n-                return (Token::new(token::Eof, span), preceded_by_whitespace);\n-            }\n-\n-            let token = rustc_lexer::first_token(text);\n+            let token = match self.cursor.advance_token() {\n+                Some(token) => token,\n+                None => {\n+                    let span = self.mk_sp(self.pos, self.pos);\n+                    return (Token::new(token::Eof, span), preceded_by_whitespace);\n+                }\n+            };\n \n             let start = self.pos;\n             self.pos = self.pos + BytePos(token.len);"}, {"sha": "0870d6f382458c815c508877dde14ea51a97c02e", "filename": "src/librustdoc/html/highlight.rs", "status": "modified", "additions": 4, "deletions": 5, "changes": 9, "blob_url": "https://github.com/rust-lang/rust/blob/aa6bfaf04b258e3e23d3f7063de4f2d37845ddec/src%2Flibrustdoc%2Fhtml%2Fhighlight.rs", "raw_url": "https://github.com/rust-lang/rust/raw/aa6bfaf04b258e3e23d3f7063de4f2d37845ddec/src%2Flibrustdoc%2Fhtml%2Fhighlight.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustdoc%2Fhtml%2Fhighlight.rs?ref=aa6bfaf04b258e3e23d3f7063de4f2d37845ddec", "patch": "@@ -13,6 +13,7 @@ use std::collections::VecDeque;\n use std::fmt::{Display, Write};\n \n use rustc_data_structures::fx::FxHashMap;\n+use rustc_lexer::cursor::Cursor;\n use rustc_lexer::{LiteralKind, TokenKind};\n use rustc_span::edition::Edition;\n use rustc_span::symbol::Symbol;\n@@ -408,15 +409,13 @@ enum Highlight<'a> {\n \n struct TokenIter<'a> {\n     src: &'a str,\n+    cursor: Cursor<'a>,\n }\n \n impl<'a> Iterator for TokenIter<'a> {\n     type Item = (TokenKind, &'a str);\n     fn next(&mut self) -> Option<(TokenKind, &'a str)> {\n-        if self.src.is_empty() {\n-            return None;\n-        }\n-        let token = rustc_lexer::first_token(self.src);\n+        let token = self.cursor.advance_token()?;\n         let (text, rest) = self.src.split_at(token.len as usize);\n         self.src = rest;\n         Some((token.kind, text))\n@@ -525,7 +524,7 @@ impl<'a> Classifier<'a> {\n     /// Takes as argument the source code to HTML-ify, the rust edition to use and the source code\n     /// file span which will be used later on by the `span_correspondance_map`.\n     fn new(src: &str, file_span: Span, decoration_info: Option<DecorationInfo>) -> Classifier<'_> {\n-        let tokens = PeekIter::new(TokenIter { src });\n+        let tokens = PeekIter::new(TokenIter { src, cursor: Cursor::new(src) });\n         let decorations = decoration_info.map(Decorations::new);\n         Classifier {\n             tokens,"}]}