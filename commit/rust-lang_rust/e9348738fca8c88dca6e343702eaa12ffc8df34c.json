{"sha": "e9348738fca8c88dca6e343702eaa12ffc8df34c", "node_id": "MDY6Q29tbWl0NzI0NzEyOmU5MzQ4NzM4ZmNhOGM4OGRjYTZlMzQzNzAyZWFhMTJmZmM4ZGYzNGM=", "commit": {"author": {"name": "Alex Crichton", "email": "alex@alexcrichton.com", "date": "2018-04-19T02:36:48Z"}, "committer": {"name": "Alex Crichton", "email": "alex@alexcrichton.com", "date": "2018-04-19T02:36:48Z"}, "message": "proc_macro: Stay on the \"use the cache\" path more\n\nDiscovered in #50061 we're falling off the \"happy path\" of using a stringified\ntoken stream more often than we should. This was due to the fact that a\nuser-written token like `0xf` is equality-different from the stringified token\nof `15` (despite being semantically equivalent).\n\nThis patch updates the call to `eq_unspanned` with an even more awful solution,\n`probably_equal_for_proc_macro`, which ignores the value of each token and\nbasically only compares the structure of the token stream, assuming that the AST\ndoesn't change just one token at a time.\n\nWhile this is a step towards fixing #50061 there is still one regression\nfrom #49154 which needs to be fixed.", "tree": {"sha": "e778a2aabb658e2ef5754798bf0f4d4f63560422", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/e778a2aabb658e2ef5754798bf0f4d4f63560422"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/e9348738fca8c88dca6e343702eaa12ffc8df34c", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/e9348738fca8c88dca6e343702eaa12ffc8df34c", "html_url": "https://github.com/rust-lang/rust/commit/e9348738fca8c88dca6e343702eaa12ffc8df34c", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/e9348738fca8c88dca6e343702eaa12ffc8df34c/comments", "author": {"login": "alexcrichton", "id": 64996, "node_id": "MDQ6VXNlcjY0OTk2", "avatar_url": "https://avatars.githubusercontent.com/u/64996?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alexcrichton", "html_url": "https://github.com/alexcrichton", "followers_url": "https://api.github.com/users/alexcrichton/followers", "following_url": "https://api.github.com/users/alexcrichton/following{/other_user}", "gists_url": "https://api.github.com/users/alexcrichton/gists{/gist_id}", "starred_url": "https://api.github.com/users/alexcrichton/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alexcrichton/subscriptions", "organizations_url": "https://api.github.com/users/alexcrichton/orgs", "repos_url": "https://api.github.com/users/alexcrichton/repos", "events_url": "https://api.github.com/users/alexcrichton/events{/privacy}", "received_events_url": "https://api.github.com/users/alexcrichton/received_events", "type": "User", "site_admin": false}, "committer": {"login": "alexcrichton", "id": 64996, "node_id": "MDQ6VXNlcjY0OTk2", "avatar_url": "https://avatars.githubusercontent.com/u/64996?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alexcrichton", "html_url": "https://github.com/alexcrichton", "followers_url": "https://api.github.com/users/alexcrichton/followers", "following_url": "https://api.github.com/users/alexcrichton/following{/other_user}", "gists_url": "https://api.github.com/users/alexcrichton/gists{/gist_id}", "starred_url": "https://api.github.com/users/alexcrichton/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alexcrichton/subscriptions", "organizations_url": "https://api.github.com/users/alexcrichton/orgs", "repos_url": "https://api.github.com/users/alexcrichton/repos", "events_url": "https://api.github.com/users/alexcrichton/events{/privacy}", "received_events_url": "https://api.github.com/users/alexcrichton/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "ac3c2288f9f9d977acb46406ba60033d65165a7b", "url": "https://api.github.com/repos/rust-lang/rust/commits/ac3c2288f9f9d977acb46406ba60033d65165a7b", "html_url": "https://github.com/rust-lang/rust/commit/ac3c2288f9f9d977acb46406ba60033d65165a7b"}], "stats": {"total": 136, "additions": 127, "deletions": 9}, "files": [{"sha": "44394384c7a7ebc4377261da2e82890ab995aea1", "filename": "src/libsyntax/parse/token.rs", "status": "modified", "additions": 91, "deletions": 9, "changes": 100, "blob_url": "https://github.com/rust-lang/rust/blob/e9348738fca8c88dca6e343702eaa12ffc8df34c/src%2Flibsyntax%2Fparse%2Ftoken.rs", "raw_url": "https://github.com/rust-lang/rust/raw/e9348738fca8c88dca6e343702eaa12ffc8df34c/src%2Flibsyntax%2Fparse%2Ftoken.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Ftoken.rs?ref=e9348738fca8c88dca6e343702eaa12ffc8df34c", "patch": "@@ -26,6 +26,7 @@ use tokenstream::{TokenStream, TokenTree};\n use tokenstream;\n \n use std::{cmp, fmt};\n+use std::mem;\n use rustc_data_structures::sync::{Lrc, Lock};\n \n #[derive(Clone, RustcEncodable, RustcDecodable, PartialEq, Eq, Hash, Debug, Copy)]\n@@ -88,6 +89,12 @@ impl Lit {\n             ByteStr(_) | ByteStrRaw(..) => \"byte string\"\n         }\n     }\n+\n+    // See comments in `interpolated_to_tokenstream` for why we care about\n+    // *probably* equal here rather than actual equality\n+    fn probably_equal_for_proc_macro(&self, other: &Lit) -> bool {\n+        mem::discriminant(self) == mem::discriminant(other)\n+    }\n }\n \n pub(crate) fn ident_can_begin_expr(ident: ast::Ident, is_raw: bool) -> bool {\n@@ -530,14 +537,6 @@ impl Token {\n         // stream they came from. Here we attempt to extract these\n         // lossless token streams before we fall back to the\n         // stringification.\n-        //\n-        // During early phases of the compiler, though, the AST could\n-        // get modified directly (e.g. attributes added or removed) and\n-        // the internal cache of tokens my not be invalidated or\n-        // updated. Consequently if the \"lossless\" token stream\n-        // disagrees with our actuall stringification (which has\n-        // historically been much more battle-tested) then we go with\n-        // the lossy stream anyway (losing span information).\n         let mut tokens = None;\n \n         match nt.0 {\n@@ -569,13 +568,96 @@ impl Token {\n             let source = pprust::token_to_string(self);\n             parse_stream_from_source_str(FileName::MacroExpansion, source, sess, Some(span))\n         });\n+\n+        // During early phases of the compiler the AST could get modified\n+        // directly (e.g. attributes added or removed) and the internal cache\n+        // of tokens my not be invalidated or updated. Consequently if the\n+        // \"lossless\" token stream disagrees with our actual stringification\n+        // (which has historically been much more battle-tested) then we go\n+        // with the lossy stream anyway (losing span information).\n+        //\n+        // Note that the comparison isn't `==` here to avoid comparing spans,\n+        // but it *also* is a \"probable\" equality which is a pretty weird\n+        // definition. We mostly want to catch actual changes to the AST\n+        // like a `#[cfg]` being processed or some weird `macro_rules!`\n+        // expansion.\n+        //\n+        // What we *don't* want to catch is the fact that a user-defined\n+        // literal like `0xf` is stringified as `15`, causing the cached token\n+        // stream to not be literal `==` token-wise (ignoring spans) to the\n+        // token stream we got from stringification.\n+        //\n+        // Instead the \"probably equal\" check here is \"does each token\n+        // recursively have the same discriminant?\" We basically don't look at\n+        // the token values here and assume that such fine grained modifications\n+        // of token streams doesn't happen.\n         if let Some(tokens) = tokens {\n-            if tokens.eq_unspanned(&tokens_for_real) {\n+            if tokens.probably_equal_for_proc_macro(&tokens_for_real) {\n                 return tokens\n             }\n         }\n         return tokens_for_real\n     }\n+\n+    // See comments in `interpolated_to_tokenstream` for why we care about\n+    // *probably* equal here rather than actual equality\n+    pub fn probably_equal_for_proc_macro(&self, other: &Token) -> bool {\n+        if mem::discriminant(self) != mem::discriminant(other) {\n+            return false\n+        }\n+        match (self, other) {\n+            (&Eq, &Eq) |\n+            (&Lt, &Lt) |\n+            (&Le, &Le) |\n+            (&EqEq, &EqEq) |\n+            (&Ne, &Ne) |\n+            (&Ge, &Ge) |\n+            (&Gt, &Gt) |\n+            (&AndAnd, &AndAnd) |\n+            (&OrOr, &OrOr) |\n+            (&Not, &Not) |\n+            (&Tilde, &Tilde) |\n+            (&At, &At) |\n+            (&Dot, &Dot) |\n+            (&DotDot, &DotDot) |\n+            (&DotDotDot, &DotDotDot) |\n+            (&DotDotEq, &DotDotEq) |\n+            (&DotEq, &DotEq) |\n+            (&Comma, &Comma) |\n+            (&Semi, &Semi) |\n+            (&Colon, &Colon) |\n+            (&ModSep, &ModSep) |\n+            (&RArrow, &RArrow) |\n+            (&LArrow, &LArrow) |\n+            (&FatArrow, &FatArrow) |\n+            (&Pound, &Pound) |\n+            (&Dollar, &Dollar) |\n+            (&Question, &Question) |\n+            (&Whitespace, &Whitespace) |\n+            (&Comment, &Comment) |\n+            (&Eof, &Eof) => true,\n+\n+            (&BinOp(a), &BinOp(b)) |\n+            (&BinOpEq(a), &BinOpEq(b)) => a == b,\n+\n+            (&OpenDelim(a), &OpenDelim(b)) |\n+            (&CloseDelim(a), &CloseDelim(b)) => a == b,\n+\n+            (&DocComment(a), &DocComment(b)) |\n+            (&Shebang(a), &Shebang(b)) => a == b,\n+\n+            (&Lifetime(a), &Lifetime(b)) => a.name == b.name,\n+            (&Ident(a, b), &Ident(c, d)) => a.name == c.name && b == d,\n+\n+            (&Literal(ref a, b), &Literal(ref c, d)) => {\n+                b == d && a.probably_equal_for_proc_macro(c)\n+            }\n+\n+            (&Interpolated(_), &Interpolated(_)) => false,\n+\n+            _ => panic!(\"forgot to add a token?\"),\n+        }\n+    }\n }\n \n #[derive(Clone, RustcEncodable, RustcDecodable, Eq, Hash)]"}, {"sha": "e2b5c4e1adfdbc59c8e1247db5f7d045dc237e54", "filename": "src/libsyntax/tokenstream.rs", "status": "modified", "additions": 34, "deletions": 0, "changes": 34, "blob_url": "https://github.com/rust-lang/rust/blob/e9348738fca8c88dca6e343702eaa12ffc8df34c/src%2Flibsyntax%2Ftokenstream.rs", "raw_url": "https://github.com/rust-lang/rust/raw/e9348738fca8c88dca6e343702eaa12ffc8df34c/src%2Flibsyntax%2Ftokenstream.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Ftokenstream.rs?ref=e9348738fca8c88dca6e343702eaa12ffc8df34c", "patch": "@@ -124,6 +124,24 @@ impl TokenTree {\n         }\n     }\n \n+    // See comments in `interpolated_to_tokenstream` for why we care about\n+    // *probably* equal here rather than actual equality\n+    //\n+    // This is otherwise the same as `eq_unspanned`, only recursing with a\n+    // different method.\n+    pub fn probably_equal_for_proc_macro(&self, other: &TokenTree) -> bool {\n+        match (self, other) {\n+            (&TokenTree::Token(_, ref tk), &TokenTree::Token(_, ref tk2)) => {\n+                tk.probably_equal_for_proc_macro(tk2)\n+            }\n+            (&TokenTree::Delimited(_, ref dl), &TokenTree::Delimited(_, ref dl2)) => {\n+                dl.delim == dl2.delim &&\n+                dl.stream().probably_equal_for_proc_macro(&dl2.stream())\n+            }\n+            (_, _) => false,\n+        }\n+    }\n+\n     /// Retrieve the TokenTree's span.\n     pub fn span(&self) -> Span {\n         match *self {\n@@ -250,6 +268,22 @@ impl TokenStream {\n         t1.next().is_none() && t2.next().is_none()\n     }\n \n+    // See comments in `interpolated_to_tokenstream` for why we care about\n+    // *probably* equal here rather than actual equality\n+    //\n+    // This is otherwise the same as `eq_unspanned`, only recursing with a\n+    // different method.\n+    pub fn probably_equal_for_proc_macro(&self, other: &TokenStream) -> bool {\n+        let mut t1 = self.trees();\n+        let mut t2 = other.trees();\n+        for (t1, t2) in t1.by_ref().zip(t2.by_ref()) {\n+            if !t1.probably_equal_for_proc_macro(&t2) {\n+                return false;\n+            }\n+        }\n+        t1.next().is_none() && t2.next().is_none()\n+    }\n+\n     /// Precondition: `self` consists of a single token tree.\n     /// Returns true if the token tree is a joint operation w.r.t. `proc_macro::TokenNode`.\n     pub fn as_tree(self) -> (TokenTree, bool /* joint? */) {"}, {"sha": "edfedebf870cf9d1266e796b648a3ddb5994158c", "filename": "src/test/compile-fail-fulldeps/proc-macro/attribute-with-error.rs", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/e9348738fca8c88dca6e343702eaa12ffc8df34c/src%2Ftest%2Fcompile-fail-fulldeps%2Fproc-macro%2Fattribute-with-error.rs", "raw_url": "https://github.com/rust-lang/rust/raw/e9348738fca8c88dca6e343702eaa12ffc8df34c/src%2Ftest%2Fcompile-fail-fulldeps%2Fproc-macro%2Fattribute-with-error.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ftest%2Fcompile-fail-fulldeps%2Fproc-macro%2Fattribute-with-error.rs?ref=e9348738fca8c88dca6e343702eaa12ffc8df34c", "patch": "@@ -21,6 +21,8 @@ use attribute_with_error::foo;\n fn test1() {\n     let a: i32 = \"foo\";\n     //~^ ERROR: mismatched types\n+    let b: i32 = \"f'oo\";\n+    //~^ ERROR: mismatched types\n }\n \n fn test2() {"}]}