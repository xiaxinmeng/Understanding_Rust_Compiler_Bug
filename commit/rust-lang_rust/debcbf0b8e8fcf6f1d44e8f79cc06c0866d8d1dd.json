{"sha": "debcbf0b8e8fcf6f1d44e8f79cc06c0866d8d1dd", "node_id": "MDY6Q29tbWl0NzI0NzEyOmRlYmNiZjBiOGU4ZmNmNmYxZDQ0ZThmNzljYzA2YzA4NjZkOGQxZGQ=", "commit": {"author": {"name": "Jeffrey Seyfried", "email": "jeffrey.seyfried@gmail.com", "date": "2017-01-13T04:49:20Z"}, "committer": {"name": "Jeffrey Seyfried", "email": "jeffrey.seyfried@gmail.com", "date": "2017-01-17T08:17:26Z"}, "message": "Refactor the parser to consume token trees.", "tree": {"sha": "f61d8ca01c5e888b1f18e25dcb516d80a54b875d", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/f61d8ca01c5e888b1f18e25dcb516d80a54b875d"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/debcbf0b8e8fcf6f1d44e8f79cc06c0866d8d1dd", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/debcbf0b8e8fcf6f1d44e8f79cc06c0866d8d1dd", "html_url": "https://github.com/rust-lang/rust/commit/debcbf0b8e8fcf6f1d44e8f79cc06c0866d8d1dd", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/debcbf0b8e8fcf6f1d44e8f79cc06c0866d8d1dd/comments", "author": {"login": "jseyfried", "id": 8652869, "node_id": "MDQ6VXNlcjg2NTI4Njk=", "avatar_url": "https://avatars.githubusercontent.com/u/8652869?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jseyfried", "html_url": "https://github.com/jseyfried", "followers_url": "https://api.github.com/users/jseyfried/followers", "following_url": "https://api.github.com/users/jseyfried/following{/other_user}", "gists_url": "https://api.github.com/users/jseyfried/gists{/gist_id}", "starred_url": "https://api.github.com/users/jseyfried/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jseyfried/subscriptions", "organizations_url": "https://api.github.com/users/jseyfried/orgs", "repos_url": "https://api.github.com/users/jseyfried/repos", "events_url": "https://api.github.com/users/jseyfried/events{/privacy}", "received_events_url": "https://api.github.com/users/jseyfried/received_events", "type": "User", "site_admin": false}, "committer": {"login": "jseyfried", "id": 8652869, "node_id": "MDQ6VXNlcjg2NTI4Njk=", "avatar_url": "https://avatars.githubusercontent.com/u/8652869?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jseyfried", "html_url": "https://github.com/jseyfried", "followers_url": "https://api.github.com/users/jseyfried/followers", "following_url": "https://api.github.com/users/jseyfried/following{/other_user}", "gists_url": "https://api.github.com/users/jseyfried/gists{/gist_id}", "starred_url": "https://api.github.com/users/jseyfried/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jseyfried/subscriptions", "organizations_url": "https://api.github.com/users/jseyfried/orgs", "repos_url": "https://api.github.com/users/jseyfried/repos", "events_url": "https://api.github.com/users/jseyfried/events{/privacy}", "received_events_url": "https://api.github.com/users/jseyfried/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "de46b247585999ae70674f1fa0543d62f2889c7f", "url": "https://api.github.com/repos/rust-lang/rust/commits/de46b247585999ae70674f1fa0543d62f2889c7f", "html_url": "https://github.com/rust-lang/rust/commit/de46b247585999ae70674f1fa0543d62f2889c7f"}], "stats": {"total": 272, "additions": 59, "deletions": 213}, "files": [{"sha": "7d8f7fcefe6391f697bb6456ab588a78a0bba5f1", "filename": "src/librustc/session/config.rs", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "blob_url": "https://github.com/rust-lang/rust/blob/debcbf0b8e8fcf6f1d44e8f79cc06c0866d8d1dd/src%2Flibrustc%2Fsession%2Fconfig.rs", "raw_url": "https://github.com/rust-lang/rust/raw/debcbf0b8e8fcf6f1d44e8f79cc06c0866d8d1dd/src%2Flibrustc%2Fsession%2Fconfig.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Fsession%2Fconfig.rs?ref=debcbf0b8e8fcf6f1d44e8f79cc06c0866d8d1dd", "patch": "@@ -25,6 +25,7 @@ use lint;\n use middle::cstore;\n \n use syntax::ast::{self, IntTy, UintTy};\n+use syntax::parse::token;\n use syntax::parse;\n use syntax::symbol::Symbol;\n use syntax::feature_gate::UnstableFeatures;\n@@ -1259,7 +1260,7 @@ pub fn parse_cfgspecs(cfgspecs: Vec<String> ) -> ast::CrateConfig {\n \n         let meta_item = panictry!(parser.parse_meta_item());\n \n-        if !parser.reader.is_eof() {\n+        if parser.token != token::Eof {\n             early_error(ErrorOutputType::default(), &format!(\"invalid --cfg argument: {}\", s))\n         } else if meta_item.is_meta_item_list() {\n             let msg ="}, {"sha": "d962d1175527aafbbfc90c00c19e7e591c5406c4", "filename": "src/librustc_metadata/cstore_impl.rs", "status": "modified", "additions": 4, "deletions": 14, "changes": 18, "blob_url": "https://github.com/rust-lang/rust/blob/debcbf0b8e8fcf6f1d44e8f79cc06c0866d8d1dd/src%2Flibrustc_metadata%2Fcstore_impl.rs", "raw_url": "https://github.com/rust-lang/rust/raw/debcbf0b8e8fcf6f1d44e8f79cc06c0866d8d1dd/src%2Flibrustc_metadata%2Fcstore_impl.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_metadata%2Fcstore_impl.rs?ref=debcbf0b8e8fcf6f1d44e8f79cc06c0866d8d1dd", "patch": "@@ -29,7 +29,7 @@ use rustc_back::PanicStrategy;\n \n use syntax::ast;\n use syntax::attr;\n-use syntax::parse::new_parser_from_source_str;\n+use syntax::parse::filemap_to_tts;\n use syntax::symbol::Symbol;\n use syntax_pos::{mk_sp, Span};\n use rustc::hir::svh::Svh;\n@@ -395,19 +395,9 @@ impl<'tcx> CrateStore<'tcx> for cstore::CStore {\n         let (name, def) = data.get_macro(id.index);\n         let source_name = format!(\"<{} macros>\", name);\n \n-        // NB: Don't use parse_tts_from_source_str because it parses with quote_depth > 0.\n-        let mut parser = new_parser_from_source_str(&sess.parse_sess, source_name, def.body);\n-\n-        let lo = parser.span.lo;\n-        let body = match parser.parse_all_token_trees() {\n-            Ok(body) => body,\n-            Err(mut err) => {\n-                err.emit();\n-                sess.abort_if_errors();\n-                unreachable!();\n-            }\n-        };\n-        let local_span = mk_sp(lo, parser.prev_span.hi);\n+        let filemap = sess.parse_sess.codemap().new_filemap(source_name, None, def.body);\n+        let local_span = mk_sp(filemap.start_pos, filemap.end_pos);\n+        let body = filemap_to_tts(&sess.parse_sess, filemap);\n \n         // Mark the attrs as used\n         let attrs = data.get_item_attrs(id.index);"}, {"sha": "edf74e1fe19f155d0a19075f42d50f4fb45ef801", "filename": "src/libsyntax/ext/base.rs", "status": "modified", "additions": 1, "deletions": 3, "changes": 4, "blob_url": "https://github.com/rust-lang/rust/blob/debcbf0b8e8fcf6f1d44e8f79cc06c0866d8d1dd/src%2Flibsyntax%2Fext%2Fbase.rs", "raw_url": "https://github.com/rust-lang/rust/raw/debcbf0b8e8fcf6f1d44e8f79cc06c0866d8d1dd/src%2Flibsyntax%2Fext%2Fbase.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fext%2Fbase.rs?ref=debcbf0b8e8fcf6f1d44e8f79cc06c0866d8d1dd", "patch": "@@ -615,9 +615,7 @@ impl<'a> ExtCtxt<'a> {\n \n     pub fn new_parser_from_tts(&self, tts: &[tokenstream::TokenTree])\n         -> parser::Parser<'a> {\n-        let mut parser = parse::tts_to_parser(self.parse_sess, tts.to_vec());\n-        parser.allow_interpolated_tts = false; // FIXME(jseyfried) `quote!` can't handle these yet\n-        parser\n+        parse::tts_to_parser(self.parse_sess, tts.to_vec())\n     }\n     pub fn codemap(&self) -> &'a CodeMap { self.parse_sess.codemap() }\n     pub fn parse_sess(&self) -> &'a parse::ParseSess { self.parse_sess }"}, {"sha": "46ffc93d2ee697f57b987e92bd6f68634f20907e", "filename": "src/libsyntax/ext/tt/macro_parser.rs", "status": "modified", "additions": 3, "deletions": 4, "changes": 7, "blob_url": "https://github.com/rust-lang/rust/blob/debcbf0b8e8fcf6f1d44e8f79cc06c0866d8d1dd/src%2Flibsyntax%2Fext%2Ftt%2Fmacro_parser.rs", "raw_url": "https://github.com/rust-lang/rust/raw/debcbf0b8e8fcf6f1d44e8f79cc06c0866d8d1dd/src%2Flibsyntax%2Fext%2Ftt%2Fmacro_parser.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fext%2Ftt%2Fmacro_parser.rs?ref=debcbf0b8e8fcf6f1d44e8f79cc06c0866d8d1dd", "patch": "@@ -82,7 +82,6 @@ use ast::Ident;\n use syntax_pos::{self, BytePos, mk_sp, Span};\n use codemap::Spanned;\n use errors::FatalError;\n-use parse::lexer::*; //resolve bug?\n use parse::{Directory, ParseSess};\n use parse::parser::{PathStyle, Parser};\n use parse::token::{DocComment, MatchNt, SubstNt};\n@@ -407,9 +406,9 @@ fn inner_parse_loop(cur_eis: &mut SmallVector<Box<MatcherPos>>,\n     Success(())\n }\n \n-pub fn parse(sess: &ParseSess, rdr: TtReader, ms: &[TokenTree], directory: Option<Directory>)\n+pub fn parse(sess: &ParseSess, tts: Vec<TokenTree>, ms: &[TokenTree], directory: Option<Directory>)\n              -> NamedParseResult {\n-    let mut parser = Parser::new(sess, Box::new(rdr), directory, true);\n+    let mut parser = Parser::new(sess, tts, directory, true);\n     let mut cur_eis = SmallVector::one(initial_matcher_pos(ms.to_owned(), parser.span.lo));\n     let mut next_eis = Vec::new(); // or proceed normally\n \n@@ -527,7 +526,7 @@ fn parse_nt<'a>(p: &mut Parser<'a>, sp: Span, name: &str) -> Nonterminal {\n         \"ident\" => match p.token {\n             token::Ident(sn) => {\n                 p.bump();\n-                token::NtIdent(Spanned::<Ident>{node: sn, span: p.span})\n+                token::NtIdent(Spanned::<Ident>{node: sn, span: p.prev_span})\n             }\n             _ => {\n                 let token_str = pprust::token_to_string(&p.token);"}, {"sha": "585232c5462b4a3cd64e4117493df03e00c7f5ee", "filename": "src/libsyntax/ext/tt/macro_rules.rs", "status": "modified", "additions": 13, "deletions": 7, "changes": 20, "blob_url": "https://github.com/rust-lang/rust/blob/debcbf0b8e8fcf6f1d44e8f79cc06c0866d8d1dd/src%2Flibsyntax%2Fext%2Ftt%2Fmacro_rules.rs", "raw_url": "https://github.com/rust-lang/rust/raw/debcbf0b8e8fcf6f1d44e8f79cc06c0866d8d1dd/src%2Flibsyntax%2Fext%2Ftt%2Fmacro_rules.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fext%2Ftt%2Fmacro_rules.rs?ref=debcbf0b8e8fcf6f1d44e8f79cc06c0866d8d1dd", "patch": "@@ -16,8 +16,8 @@ use ext::expand::{Expansion, ExpansionKind};\n use ext::tt::macro_parser::{Success, Error, Failure};\n use ext::tt::macro_parser::{MatchedSeq, MatchedNonterminal};\n use ext::tt::macro_parser::{parse, parse_failure_msg};\n+use ext::tt::transcribe::new_tt_reader;\n use parse::{Directory, ParseSess};\n-use parse::lexer::new_tt_reader;\n use parse::parser::Parser;\n use parse::token::{self, NtTT, Token};\n use parse::token::Token::*;\n@@ -113,13 +113,21 @@ fn generic_extension<'cx>(cx: &'cx ExtCtxt,\n                     _ => cx.span_bug(sp, \"malformed macro rhs\"),\n                 };\n                 // rhs has holes ( `$id` and `$(...)` that need filled)\n-                let trncbr =\n+                let mut trncbr =\n                     new_tt_reader(&cx.parse_sess.span_diagnostic, Some(named_matches), rhs);\n+                let mut tts = Vec::new();\n+                loop {\n+                    let tok = trncbr.real_token();\n+                    if tok.tok == token::Eof {\n+                        break\n+                    }\n+                    tts.push(TokenTree::Token(tok.sp, tok.tok));\n+                }\n                 let directory = Directory {\n                     path: cx.current_expansion.module.directory.clone(),\n                     ownership: cx.current_expansion.directory_ownership,\n                 };\n-                let mut p = Parser::new(cx.parse_sess(), Box::new(trncbr), Some(directory), false);\n+                let mut p = Parser::new(cx.parse_sess(), tts, Some(directory), false);\n                 p.root_module_name = cx.current_expansion.module.mod_path.last()\n                     .map(|id| (*id.name.as_str()).to_owned());\n \n@@ -187,10 +195,8 @@ pub fn compile(sess: &ParseSess, def: &ast::MacroDef) -> SyntaxExtension {\n         })),\n     ];\n \n-    // Parse the macro_rules! invocation (`none` is for no interpolations):\n-    let arg_reader = new_tt_reader(&sess.span_diagnostic, None, def.body.clone());\n-\n-    let argument_map = match parse(sess, arg_reader, &argument_gram, None) {\n+    // Parse the macro_rules! invocation\n+    let argument_map = match parse(sess, def.body.clone(), &argument_gram, None) {\n         Success(m) => m,\n         Failure(sp, tok) => {\n             let s = parse_failure_msg(tok);"}, {"sha": "82f1e18389565f69631b45bb84e7af355edd5b96", "filename": "src/libsyntax/ext/tt/transcribe.rs", "status": "modified", "additions": 7, "deletions": 4, "changes": 11, "blob_url": "https://github.com/rust-lang/rust/blob/debcbf0b8e8fcf6f1d44e8f79cc06c0866d8d1dd/src%2Flibsyntax%2Fext%2Ftt%2Ftranscribe.rs", "raw_url": "https://github.com/rust-lang/rust/raw/debcbf0b8e8fcf6f1d44e8f79cc06c0866d8d1dd/src%2Flibsyntax%2Fext%2Ftt%2Ftranscribe.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fext%2Ftt%2Ftranscribe.rs?ref=debcbf0b8e8fcf6f1d44e8f79cc06c0866d8d1dd", "patch": "@@ -10,7 +10,7 @@\n use self::LockstepIterSize::*;\n \n use ast::Ident;\n-use errors::{Handler, DiagnosticBuilder};\n+use errors::Handler;\n use ext::tt::macro_parser::{NamedMatch, MatchedSeq, MatchedNonterminal};\n use parse::token::{self, MatchNt, SubstNt, Token, NtIdent};\n use parse::lexer::TokenAndSpan;\n@@ -44,8 +44,12 @@ pub struct TtReader<'a> {\n     /* cached: */\n     pub cur_tok: Token,\n     pub cur_span: Span,\n-    /// Transform doc comments. Only useful in macro invocations\n-    pub fatal_errs: Vec<DiagnosticBuilder<'a>>,\n+}\n+\n+impl<'a> TtReader<'a> {\n+    pub fn real_token(&mut self) -> TokenAndSpan {\n+        tt_next_token(self)\n+    }\n }\n \n /// This can do Macro-By-Example transcription. On the other hand, if\n@@ -76,7 +80,6 @@ pub fn new_tt_reader(sp_diag: &Handler,\n         /* dummy values, never read: */\n         cur_tok: token::Eof,\n         cur_span: DUMMY_SP,\n-        fatal_errs: Vec::new(),\n     };\n     tt_next_token(&mut r); /* get cur_tok and cur_span set up */\n     r"}, {"sha": "12b9130c4743933efaf7d45615e7c69828e6545e", "filename": "src/libsyntax/parse/lexer/mod.rs", "status": "modified", "additions": 0, "deletions": 74, "changes": 74, "blob_url": "https://github.com/rust-lang/rust/blob/debcbf0b8e8fcf6f1d44e8f79cc06c0866d8d1dd/src%2Flibsyntax%2Fparse%2Flexer%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/debcbf0b8e8fcf6f1d44e8f79cc06c0866d8d1dd/src%2Flibsyntax%2Fparse%2Flexer%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Flexer%2Fmod.rs?ref=debcbf0b8e8fcf6f1d44e8f79cc06c0866d8d1dd", "patch": "@@ -12,7 +12,6 @@ use ast::{self, Ident};\n use syntax_pos::{self, BytePos, CharPos, Pos, Span};\n use codemap::CodeMap;\n use errors::{FatalError, DiagnosticBuilder};\n-use ext::tt::transcribe::tt_next_token;\n use parse::{token, ParseSess};\n use str::char_at;\n use symbol::{Symbol, keywords};\n@@ -23,53 +22,10 @@ use std::char;\n use std::mem::replace;\n use std::rc::Rc;\n \n-pub use ext::tt::transcribe::{TtReader, new_tt_reader};\n-\n pub mod comments;\n mod tokentrees;\n mod unicode_chars;\n \n-pub trait Reader {\n-    fn is_eof(&self) -> bool;\n-    fn try_next_token(&mut self) -> Result<TokenAndSpan, ()>;\n-    fn next_token(&mut self) -> TokenAndSpan where Self: Sized {\n-        let res = self.try_next_token();\n-        self.unwrap_or_abort(res)\n-    }\n-    /// Report a fatal error with the current span.\n-    fn fatal(&self, &str) -> FatalError;\n-    /// Report a non-fatal error with the current span.\n-    fn err(&self, &str);\n-    fn emit_fatal_errors(&mut self);\n-    fn unwrap_or_abort(&mut self, res: Result<TokenAndSpan, ()>) -> TokenAndSpan {\n-        match res {\n-            Ok(tok) => tok,\n-            Err(_) => {\n-                self.emit_fatal_errors();\n-                panic!(FatalError);\n-            }\n-        }\n-    }\n-    fn peek(&self) -> TokenAndSpan;\n-    /// Get a token the parser cares about.\n-    fn try_real_token(&mut self) -> Result<TokenAndSpan, ()> {\n-        let mut t = self.try_next_token()?;\n-        loop {\n-            match t.tok {\n-                token::Whitespace | token::Comment | token::Shebang(_) => {\n-                    t = self.try_next_token()?;\n-                }\n-                _ => break,\n-            }\n-        }\n-        Ok(t)\n-    }\n-    fn real_token(&mut self) -> TokenAndSpan {\n-        let res = self.try_real_token();\n-        self.unwrap_or_abort(res)\n-    }\n-}\n-\n #[derive(Clone, PartialEq, Eq, Debug)]\n pub struct TokenAndSpan {\n     pub tok: token::Token,\n@@ -182,36 +138,6 @@ impl<'a> StringReader<'a> {\n     }\n }\n \n-impl<'a> Reader for TtReader<'a> {\n-    fn is_eof(&self) -> bool {\n-        self.peek().tok == token::Eof\n-    }\n-    fn try_next_token(&mut self) -> Result<TokenAndSpan, ()> {\n-        assert!(self.fatal_errs.is_empty());\n-        let r = tt_next_token(self);\n-        debug!(\"TtReader: r={:?}\", r);\n-        Ok(r)\n-    }\n-    fn fatal(&self, m: &str) -> FatalError {\n-        self.sp_diag.span_fatal(self.cur_span, m)\n-    }\n-    fn err(&self, m: &str) {\n-        self.sp_diag.span_err(self.cur_span, m);\n-    }\n-    fn emit_fatal_errors(&mut self) {\n-        for err in &mut self.fatal_errs {\n-            err.emit();\n-        }\n-        self.fatal_errs.clear();\n-    }\n-    fn peek(&self) -> TokenAndSpan {\n-        TokenAndSpan {\n-            tok: self.cur_tok.clone(),\n-            sp: self.cur_span,\n-        }\n-    }\n-}\n-\n impl<'a> StringReader<'a> {\n     /// For comments.rs, which hackily pokes into next_pos and ch\n     pub fn new_raw<'b>(sess: &'a ParseSess, filemap: Rc<syntax_pos::FileMap>) -> Self {"}, {"sha": "0937ef15b4d6dd973002f991c04e756b0f630f4b", "filename": "src/libsyntax/parse/mod.rs", "status": "modified", "additions": 2, "deletions": 3, "changes": 5, "blob_url": "https://github.com/rust-lang/rust/blob/debcbf0b8e8fcf6f1d44e8f79cc06c0866d8d1dd/src%2Flibsyntax%2Fparse%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/debcbf0b8e8fcf6f1d44e8f79cc06c0866d8d1dd/src%2Flibsyntax%2Fparse%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Fmod.rs?ref=debcbf0b8e8fcf6f1d44e8f79cc06c0866d8d1dd", "patch": "@@ -45,7 +45,7 @@ pub mod obsolete;\n \n /// Info about a parsing session.\n pub struct ParseSess {\n-    pub span_diagnostic: Handler, // better be the same as the one in the reader!\n+    pub span_diagnostic: Handler,\n     pub unstable_features: UnstableFeatures,\n     pub config: CrateConfig,\n     /// Used to determine and report recursive mod inclusions\n@@ -227,8 +227,7 @@ pub fn filemap_to_tts(sess: &ParseSess, filemap: Rc<FileMap>) -> Vec<tokenstream\n \n /// Given tts and the ParseSess, produce a parser\n pub fn tts_to_parser<'a>(sess: &'a ParseSess, tts: Vec<tokenstream::TokenTree>) -> Parser<'a> {\n-    let trdr = lexer::new_tt_reader(&sess.span_diagnostic, None, tts);\n-    let mut p = Parser::new(sess, Box::new(trdr), None, false);\n+    let mut p = Parser::new(sess, tts, None, false);\n     p.check_unknown_macro_variable();\n     p\n }"}, {"sha": "608f8688e881024664ad4b15ccb2e5a53d64da93", "filename": "src/libsyntax/parse/parser.rs", "status": "modified", "additions": 18, "deletions": 97, "changes": 115, "blob_url": "https://github.com/rust-lang/rust/blob/debcbf0b8e8fcf6f1d44e8f79cc06c0866d8d1dd/src%2Flibsyntax%2Fparse%2Fparser.rs", "raw_url": "https://github.com/rust-lang/rust/raw/debcbf0b8e8fcf6f1d44e8f79cc06c0866d8d1dd/src%2Flibsyntax%2Fparse%2Fparser.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Fparser.rs?ref=debcbf0b8e8fcf6f1d44e8f79cc06c0866d8d1dd", "patch": "@@ -46,7 +46,7 @@ use ext::tt::macro_parser;\n use parse;\n use parse::classify;\n use parse::common::SeqSep;\n-use parse::lexer::{Reader, TokenAndSpan};\n+use parse::lexer::TokenAndSpan;\n use parse::obsolete::ObsoleteSyntax;\n use parse::token::{self, MatchNt, SubstNt};\n use parse::{new_sub_parser_from_file, ParseSess, Directory, DirectoryOwnership};\n@@ -188,22 +188,18 @@ pub struct Parser<'a> {\n     pub restrictions: Restrictions,\n     pub quote_depth: usize, // not (yet) related to the quasiquoter\n     parsing_token_tree: bool,\n-    pub reader: Box<Reader+'a>,\n     /// The set of seen errors about obsolete syntax. Used to suppress\n     /// extra detail when the same error is seen twice\n     pub obsolete_set: HashSet<ObsoleteSyntax>,\n     /// Used to determine the path to externally loaded source files\n     pub directory: Directory,\n-    /// Stack of open delimiters and their spans. Used for error message.\n-    pub open_braces: Vec<(token::DelimToken, Span)>,\n     /// Name of the root module this parser originated from. If `None`, then the\n     /// name is not known. This does not change while the parser is descending\n     /// into modules, and sub-parsers have new values for this name.\n     pub root_module_name: Option<String>,\n     pub expected_tokens: Vec<TokenType>,\n     pub tts: Vec<(TokenTree, usize)>,\n     pub desugar_doc_comments: bool,\n-    pub allow_interpolated_tts: bool,\n }\n \n #[derive(PartialEq, Eq, Clone)]\n@@ -269,12 +265,17 @@ impl From<P<Expr>> for LhsExpr {\n \n impl<'a> Parser<'a> {\n     pub fn new(sess: &'a ParseSess,\n-               rdr: Box<Reader+'a>,\n+               tokens: Vec<TokenTree>,\n                directory: Option<Directory>,\n                desugar_doc_comments: bool)\n                -> Self {\n+        let tt = TokenTree::Delimited(syntax_pos::DUMMY_SP, Rc::new(Delimited {\n+            delim: token::NoDelim,\n+            open_span: syntax_pos::DUMMY_SP,\n+            tts: tokens,\n+            close_span: syntax_pos::DUMMY_SP,\n+        }));\n         let mut parser = Parser {\n-            reader: rdr,\n             sess: sess,\n             token: token::Underscore,\n             span: syntax_pos::DUMMY_SP,\n@@ -286,12 +287,10 @@ impl<'a> Parser<'a> {\n             parsing_token_tree: false,\n             obsolete_set: HashSet::new(),\n             directory: Directory { path: PathBuf::new(), ownership: DirectoryOwnership::Owned },\n-            open_braces: Vec::new(),\n             root_module_name: None,\n             expected_tokens: Vec::new(),\n-            tts: Vec::new(),\n+            tts: if tt.len() > 0 { vec![(tt, 0)] } else { Vec::new() },\n             desugar_doc_comments: desugar_doc_comments,\n-            allow_interpolated_tts: true,\n         };\n \n         let tok = parser.next_tok();\n@@ -320,7 +319,7 @@ impl<'a> Parser<'a> {\n                     continue\n                 }\n             } else {\n-                self.reader.real_token()\n+                TokenAndSpan { tok: token::Eof, sp: self.span }\n             };\n \n             loop {\n@@ -2688,116 +2687,38 @@ impl<'a> Parser<'a> {\n         // whether something will be a nonterminal or a seq\n         // yet.\n         match self.token {\n-            token::Eof => {\n-                let mut err: DiagnosticBuilder<'a> =\n-                    self.diagnostic().struct_span_err(self.span,\n-                                                      \"this file contains an un-closed delimiter\");\n-                for &(_, sp) in &self.open_braces {\n-                    err.span_help(sp, \"did you mean to close this delimiter?\");\n-                }\n-\n-                Err(err)\n-            },\n             token::OpenDelim(delim) => {\n-                if self.tts.last().map(|&(_, i)| i == 1).unwrap_or(false) {\n+                if self.quote_depth == 0 && self.tts.last().map(|&(_, i)| i == 1).unwrap_or(false) {\n                     let tt = self.tts.pop().unwrap().0;\n                     self.bump();\n-                    return Ok(if self.allow_interpolated_tts {\n-                        // avoid needlessly reparsing token trees in recursive macro expansions\n-                        TokenTree::Token(tt.span(), token::Interpolated(Rc::new(token::NtTT(tt))))\n-                    } else {\n-                        tt\n-                    });\n+                    return Ok(tt);\n                 }\n \n                 let parsing_token_tree = ::std::mem::replace(&mut self.parsing_token_tree, true);\n-                // The span for beginning of the delimited section\n-                let pre_span = self.span;\n-\n-                // Parse the open delimiter.\n-                self.open_braces.push((delim, self.span));\n                 let open_span = self.span;\n                 self.bump();\n-\n-                // Parse the token trees within the delimiters.\n-                // We stop at any delimiter so we can try to recover if the user\n-                // uses an incorrect delimiter.\n                 let tts = self.parse_seq_to_before_tokens(&[&token::CloseDelim(token::Brace),\n                                                             &token::CloseDelim(token::Paren),\n                                                             &token::CloseDelim(token::Bracket)],\n                                                           SeqSep::none(),\n                                                           |p| p.parse_token_tree(),\n                                                           |mut e| e.emit());\n+                self.parsing_token_tree = parsing_token_tree;\n \n                 let close_span = self.span;\n-                // Expand to cover the entire delimited token tree\n-                let span = Span { hi: close_span.hi, ..pre_span };\n-\n-                match self.token {\n-                    // Correct delimiter.\n-                    token::CloseDelim(d) if d == delim => {\n-                        self.open_braces.pop().unwrap();\n-\n-                        // Parse the close delimiter.\n-                        self.bump();\n-                    }\n-                    // Incorrect delimiter.\n-                    token::CloseDelim(other) => {\n-                        let token_str = self.this_token_to_string();\n-                        let mut err = self.diagnostic().struct_span_err(self.span,\n-                            &format!(\"incorrect close delimiter: `{}`\", token_str));\n-                        // This is a conservative error: only report the last unclosed delimiter.\n-                        // The previous unclosed delimiters could actually be closed! The parser\n-                        // just hasn't gotten to them yet.\n-                        if let Some(&(_, sp)) = self.open_braces.last() {\n-                            err.span_note(sp, \"unclosed delimiter\");\n-                        };\n-                        err.emit();\n-\n-                        self.open_braces.pop().unwrap();\n-\n-                        // If the incorrect delimiter matches an earlier opening\n-                        // delimiter, then don't consume it (it can be used to\n-                        // close the earlier one). Otherwise, consume it.\n-                        // E.g., we try to recover from:\n-                        // fn foo() {\n-                        //     bar(baz(\n-                        // }  // Incorrect delimiter but matches the earlier `{`\n-                        if !self.open_braces.iter().any(|&(b, _)| b == other) {\n-                            self.bump();\n-                        }\n-                    }\n-                    token::Eof => {\n-                        // Silently recover, the EOF token will be seen again\n-                        // and an error emitted then. Thus we don't pop from\n-                        // self.open_braces here.\n-                    },\n-                    _ => {}\n-                }\n+                self.bump();\n \n-                self.parsing_token_tree = parsing_token_tree;\n+                let span = Span { lo: open_span.lo, ..close_span };\n                 Ok(TokenTree::Delimited(span, Rc::new(Delimited {\n                     delim: delim,\n                     open_span: open_span,\n                     tts: tts,\n                     close_span: close_span,\n                 })))\n             },\n-            token::CloseDelim(_) => {\n-                // An unexpected closing delimiter (i.e., there is no\n-                // matching opening delimiter).\n-                let token_str = self.this_token_to_string();\n-                let err = self.diagnostic().struct_span_err(self.span,\n-                    &format!(\"unexpected close delimiter: `{}`\", token_str));\n-                Err(err)\n-            },\n-            /* we ought to allow different depths of unquotation */\n-            token::Dollar | token::SubstNt(..) if self.quote_depth > 0 => {\n-                self.parse_unquoted()\n-            }\n-            _ => {\n-                Ok(TokenTree::Token(self.span, self.bump_and_get()))\n-            }\n+            token::CloseDelim(_) | token::Eof => unreachable!(),\n+            token::Dollar | token::SubstNt(..) if self.quote_depth > 0 => self.parse_unquoted(),\n+            _ => Ok(TokenTree::Token(self.span, self.bump_and_get())),\n         }\n     }\n "}, {"sha": "ab5dc8181e05bbc71ff07ff148d6a1d83b42a120", "filename": "src/libsyntax/tokenstream.rs", "status": "modified", "additions": 8, "deletions": 5, "changes": 13, "blob_url": "https://github.com/rust-lang/rust/blob/debcbf0b8e8fcf6f1d44e8f79cc06c0866d8d1dd/src%2Flibsyntax%2Ftokenstream.rs", "raw_url": "https://github.com/rust-lang/rust/raw/debcbf0b8e8fcf6f1d44e8f79cc06c0866d8d1dd/src%2Flibsyntax%2Ftokenstream.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Ftokenstream.rs?ref=debcbf0b8e8fcf6f1d44e8f79cc06c0866d8d1dd", "patch": "@@ -30,7 +30,6 @@ use codemap::{Spanned, combine_spans};\n use ext::base;\n use ext::tt::macro_parser;\n use parse::lexer::comments::{doc_comment_style, strip_doc_comment_decoration};\n-use parse::lexer;\n use parse::{self, Directory};\n use parse::token::{self, Token, Lit, Nonterminal};\n use print::pprust;\n@@ -139,7 +138,10 @@ impl TokenTree {\n                 if let Nonterminal::NtTT(..) = **nt { 1 } else { 0 }\n             },\n             TokenTree::Token(_, token::MatchNt(..)) => 3,\n-            TokenTree::Delimited(_, ref delimed) => delimed.tts.len() + 2,\n+            TokenTree::Delimited(_, ref delimed) => match delimed.delim {\n+                token::NoDelim => delimed.tts.len(),\n+                _ => delimed.tts.len() + 2,\n+            },\n             TokenTree::Sequence(_, ref seq) => seq.tts.len(),\n             TokenTree::Token(..) => 0,\n         }\n@@ -181,6 +183,9 @@ impl TokenTree {\n                     close_span: sp,\n                 }))\n             }\n+            (&TokenTree::Delimited(_, ref delimed), _) if delimed.delim == token::NoDelim => {\n+                delimed.tts[index].clone()\n+            }\n             (&TokenTree::Delimited(_, ref delimed), _) => {\n                 if index == 0 {\n                     return delimed.open_tt();\n@@ -215,14 +220,12 @@ impl TokenTree {\n                  mtch: &[TokenTree],\n                  tts: &[TokenTree])\n                  -> macro_parser::NamedParseResult {\n-        let diag = &cx.parse_sess().span_diagnostic;\n         // `None` is because we're not interpolating\n-        let arg_rdr = lexer::new_tt_reader(diag, None, tts.iter().cloned().collect());\n         let directory = Directory {\n             path: cx.current_expansion.module.directory.clone(),\n             ownership: cx.current_expansion.directory_ownership,\n         };\n-        macro_parser::parse(cx.parse_sess(), arg_rdr, mtch, Some(directory))\n+        macro_parser::parse(cx.parse_sess(), tts.iter().cloned().collect(), mtch, Some(directory))\n     }\n \n     /// Check if this TokenTree is equal to the other, regardless of span information."}, {"sha": "e3c17af82aab403d2eda1ce51af24d796447e050", "filename": "src/test/parse-fail/issue-33569.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/debcbf0b8e8fcf6f1d44e8f79cc06c0866d8d1dd/src%2Ftest%2Fparse-fail%2Fissue-33569.rs", "raw_url": "https://github.com/rust-lang/rust/raw/debcbf0b8e8fcf6f1d44e8f79cc06c0866d8d1dd/src%2Ftest%2Fparse-fail%2Fissue-33569.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ftest%2Fparse-fail%2Fissue-33569.rs?ref=debcbf0b8e8fcf6f1d44e8f79cc06c0866d8d1dd", "patch": "@@ -13,6 +13,6 @@\n macro_rules! foo {\n     { $+ } => { //~ ERROR expected identifier, found `+`\n         $(x)(y) //~ ERROR expected `*` or `+`\n-                //~^ ERROR no rules expected the token `y`\n+                //~^ ERROR no rules expected the token `)`\n     }\n }"}]}