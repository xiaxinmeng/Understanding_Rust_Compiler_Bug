{"sha": "54dd3a47fd54eb466dad7e47b41ec1b5b2dafd5a", "node_id": "MDY6Q29tbWl0NzI0NzEyOjU0ZGQzYTQ3ZmQ1NGViNDY2ZGFkN2U0N2I0MWVjMWI1YjJkYWZkNWE=", "commit": {"author": {"name": "Denis Merigoux", "email": "denis.merigoux@gmail.com", "date": "2018-10-05T13:08:49Z"}, "committer": {"name": "Eduard-Mihai Burtescu", "email": "edy.burt@gmail.com", "date": "2018-11-16T13:08:18Z"}, "message": "All Builder methods now take &mut self instead of &self", "tree": {"sha": "f2507cb57a8212eed8a963415ac5db2a4bccc8a8", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/f2507cb57a8212eed8a963415ac5db2a4bccc8a8"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/54dd3a47fd54eb466dad7e47b41ec1b5b2dafd5a", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/54dd3a47fd54eb466dad7e47b41ec1b5b2dafd5a", "html_url": "https://github.com/rust-lang/rust/commit/54dd3a47fd54eb466dad7e47b41ec1b5b2dafd5a", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/54dd3a47fd54eb466dad7e47b41ec1b5b2dafd5a/comments", "author": {"login": "denismerigoux", "id": 1766128, "node_id": "MDQ6VXNlcjE3NjYxMjg=", "avatar_url": "https://avatars.githubusercontent.com/u/1766128?v=4", "gravatar_id": "", "url": "https://api.github.com/users/denismerigoux", "html_url": "https://github.com/denismerigoux", "followers_url": "https://api.github.com/users/denismerigoux/followers", "following_url": "https://api.github.com/users/denismerigoux/following{/other_user}", "gists_url": "https://api.github.com/users/denismerigoux/gists{/gist_id}", "starred_url": "https://api.github.com/users/denismerigoux/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/denismerigoux/subscriptions", "organizations_url": "https://api.github.com/users/denismerigoux/orgs", "repos_url": "https://api.github.com/users/denismerigoux/repos", "events_url": "https://api.github.com/users/denismerigoux/events{/privacy}", "received_events_url": "https://api.github.com/users/denismerigoux/received_events", "type": "User", "site_admin": false}, "committer": {"login": "eddyb", "id": 77424, "node_id": "MDQ6VXNlcjc3NDI0", "avatar_url": "https://avatars.githubusercontent.com/u/77424?v=4", "gravatar_id": "", "url": "https://api.github.com/users/eddyb", "html_url": "https://github.com/eddyb", "followers_url": "https://api.github.com/users/eddyb/followers", "following_url": "https://api.github.com/users/eddyb/following{/other_user}", "gists_url": "https://api.github.com/users/eddyb/gists{/gist_id}", "starred_url": "https://api.github.com/users/eddyb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/eddyb/subscriptions", "organizations_url": "https://api.github.com/users/eddyb/orgs", "repos_url": "https://api.github.com/users/eddyb/repos", "events_url": "https://api.github.com/users/eddyb/events{/privacy}", "received_events_url": "https://api.github.com/users/eddyb/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "1ebdfbb02641676fb4f8efb1f87cfe8d0d29d2b3", "url": "https://api.github.com/repos/rust-lang/rust/commits/1ebdfbb02641676fb4f8efb1f87cfe8d0d29d2b3", "html_url": "https://github.com/rust-lang/rust/commit/1ebdfbb02641676fb4f8efb1f87cfe8d0d29d2b3"}], "stats": {"total": 1372, "additions": 716, "deletions": 656}, "files": [{"sha": "89e9e12a8a3912e8d78e9615e5161a5a2ac0e10b", "filename": "src/librustc_codegen_llvm/abi.rs", "status": "modified", "additions": 17, "deletions": 17, "changes": 34, "blob_url": "https://github.com/rust-lang/rust/blob/54dd3a47fd54eb466dad7e47b41ec1b5b2dafd5a/src%2Flibrustc_codegen_llvm%2Fabi.rs", "raw_url": "https://github.com/rust-lang/rust/raw/54dd3a47fd54eb466dad7e47b41ec1b5b2dafd5a/src%2Flibrustc_codegen_llvm%2Fabi.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_codegen_llvm%2Fabi.rs?ref=54dd3a47fd54eb466dad7e47b41ec1b5b2dafd5a", "patch": "@@ -171,13 +171,13 @@ pub trait ArgTypeExt<'ll, 'tcx> {\n     fn memory_ty(&self, cx: &CodegenCx<'ll, 'tcx>) -> &'ll Type;\n     fn store(\n         &self,\n-        bx: &Builder<'_, 'll, 'tcx>,\n+        bx: &mut Builder<'_, 'll, 'tcx>,\n         val: &'ll Value,\n         dst: PlaceRef<'tcx, &'ll Value>,\n     );\n     fn store_fn_arg(\n         &self,\n-        bx: &Builder<'_, 'll, 'tcx>,\n+        bx: &mut Builder<'_, 'll, 'tcx>,\n         idx: &mut usize,\n         dst: PlaceRef<'tcx, &'ll Value>,\n     );\n@@ -196,14 +196,13 @@ impl ArgTypeExt<'ll, 'tcx> for ArgType<'tcx, Ty<'tcx>> {\n     /// or results of call/invoke instructions into their destinations.\n     fn store(\n         &self,\n-        bx: &Builder<'_, 'll, 'tcx>,\n+        bx: &mut Builder<'_, 'll, 'tcx>,\n         val: &'ll Value,\n         dst: PlaceRef<'tcx, &'ll Value>,\n     ) {\n         if self.is_ignore() {\n             return;\n         }\n-        let cx = bx.cx();\n         if self.is_sized_indirect() {\n             OperandValue::Ref(val, None, self.layout.align).store(bx, dst)\n         } else if self.is_unsized_indirect() {\n@@ -213,7 +212,8 @@ impl ArgTypeExt<'ll, 'tcx> for ArgType<'tcx, Ty<'tcx>> {\n             // uses it for i16 -> {i8, i8}, but not for i24 -> {i8, i8, i8}.\n             let can_store_through_cast_ptr = false;\n             if can_store_through_cast_ptr {\n-                let cast_dst = bx.pointercast(dst.llval, cx.type_ptr_to(cast.llvm_type(cx)));\n+                let cast_ptr_llty = bx.cx().type_ptr_to(cast.llvm_type(bx.cx()));\n+                let cast_dst = bx.pointercast(dst.llval, cast_ptr_llty);\n                 bx.store(val, cast_dst, self.layout.align);\n             } else {\n                 // The actual return type is a struct, but the ABI\n@@ -231,21 +231,21 @@ impl ArgTypeExt<'ll, 'tcx> for ArgType<'tcx, Ty<'tcx>> {\n                 //   bitcasting to the struct type yields invalid cast errors.\n \n                 // We instead thus allocate some scratch space...\n-                let scratch_size = cast.size(cx);\n-                let scratch_align = cast.align(cx);\n-                let llscratch = bx.alloca(cast.llvm_type(cx), \"abi_cast\", scratch_align);\n+                let scratch_size = cast.size(bx.cx());\n+                let scratch_align = cast.align(bx.cx());\n+                let llscratch = bx.alloca(cast.llvm_type(bx.cx()), \"abi_cast\", scratch_align);\n                 bx.lifetime_start(llscratch, scratch_size);\n \n                 // ...where we first store the value...\n                 bx.store(val, llscratch, scratch_align);\n \n                 // ...and then memcpy it to the intended destination.\n                 bx.memcpy(\n-                    bx.pointercast(dst.llval, cx.type_i8p()),\n+                    dst.llval,\n                     self.layout.align,\n-                    bx.pointercast(llscratch, cx.type_i8p()),\n+                    llscratch,\n                     scratch_align,\n-                    cx.const_usize(self.layout.size.bytes()),\n+                    bx.cx().const_usize(self.layout.size.bytes()),\n                     MemFlags::empty()\n                 );\n \n@@ -258,7 +258,7 @@ impl ArgTypeExt<'ll, 'tcx> for ArgType<'tcx, Ty<'tcx>> {\n \n     fn store_fn_arg(\n         &self,\n-        bx: &Builder<'a, 'll, 'tcx>,\n+        bx: &mut Builder<'a, 'll, 'tcx>,\n         idx: &mut usize,\n         dst: PlaceRef<'tcx, &'ll Value>,\n     ) {\n@@ -284,14 +284,14 @@ impl ArgTypeExt<'ll, 'tcx> for ArgType<'tcx, Ty<'tcx>> {\n \n impl ArgTypeMethods<'tcx> for Builder<'a, 'll, 'tcx> {\n     fn store_fn_arg(\n-        &self,\n+        &mut self,\n         ty: &ArgType<'tcx, Ty<'tcx>>,\n         idx: &mut usize, dst: PlaceRef<'tcx, Self::Value>\n     ) {\n         ty.store_fn_arg(self, idx, dst)\n     }\n     fn store_arg_ty(\n-        &self,\n+        &mut self,\n         ty: &ArgType<'tcx, Ty<'tcx>>,\n         val: &'ll Value,\n         dst: PlaceRef<'tcx, &'ll Value>\n@@ -324,7 +324,7 @@ pub trait FnTypeExt<'tcx> {\n     fn ptr_to_llvm_type(&self, cx: &CodegenCx<'ll, 'tcx>) -> &'ll Type;\n     fn llvm_cconv(&self) -> llvm::CallConv;\n     fn apply_attrs_llfn(&self, llfn: &'ll Value);\n-    fn apply_attrs_callsite(&self, bx: &Builder<'a, 'll, 'tcx>, callsite: &'ll Value);\n+    fn apply_attrs_callsite(&self, bx: &mut Builder<'a, 'll, 'tcx>, callsite: &'ll Value);\n }\n \n impl<'tcx> FnTypeExt<'tcx> for FnType<'tcx, Ty<'tcx>> {\n@@ -761,7 +761,7 @@ impl<'tcx> FnTypeExt<'tcx> for FnType<'tcx, Ty<'tcx>> {\n         }\n     }\n \n-    fn apply_attrs_callsite(&self, bx: &Builder<'a, 'll, 'tcx>, callsite: &'ll Value) {\n+    fn apply_attrs_callsite(&self, bx: &mut Builder<'a, 'll, 'tcx>, callsite: &'ll Value) {\n         let mut i = 0;\n         let mut apply = |attrs: &ArgAttributes| {\n             attrs.apply_callsite(llvm::AttributePlace::Argument(i), callsite);\n@@ -832,7 +832,7 @@ impl AbiMethods<'tcx> for CodegenCx<'ll, 'tcx> {\n \n impl AbiBuilderMethods<'tcx> for Builder<'a, 'll, 'tcx> {\n     fn apply_attrs_callsite(\n-        &self,\n+        &mut self,\n         ty: &FnType<'tcx, Ty<'tcx>>,\n         callsite: Self::Value\n     ) {"}, {"sha": "8bb88ba5a836892efac26b025292a9ed7f2c4954", "filename": "src/librustc_codegen_llvm/asm.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/54dd3a47fd54eb466dad7e47b41ec1b5b2dafd5a/src%2Flibrustc_codegen_llvm%2Fasm.rs", "raw_url": "https://github.com/rust-lang/rust/raw/54dd3a47fd54eb466dad7e47b41ec1b5b2dafd5a/src%2Flibrustc_codegen_llvm%2Fasm.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_codegen_llvm%2Fasm.rs?ref=54dd3a47fd54eb466dad7e47b41ec1b5b2dafd5a", "patch": "@@ -26,7 +26,7 @@ use libc::{c_uint, c_char};\n \n impl AsmBuilderMethods<'tcx> for Builder<'a, 'll, 'tcx> {\n     fn codegen_inline_asm(\n-        &self,\n+        &mut self,\n         ia: &hir::InlineAsm,\n         outputs: Vec<PlaceRef<'tcx, &'ll Value>>,\n         mut inputs: Vec<&'ll Value>"}, {"sha": "c10e98c554682f9cf6170b7c2c169baf15432b9d", "filename": "src/librustc_codegen_llvm/builder.rs", "status": "modified", "additions": 128, "deletions": 124, "changes": 252, "blob_url": "https://github.com/rust-lang/rust/blob/54dd3a47fd54eb466dad7e47b41ec1b5b2dafd5a/src%2Flibrustc_codegen_llvm%2Fbuilder.rs", "raw_url": "https://github.com/rust-lang/rust/raw/54dd3a47fd54eb466dad7e47b41ec1b5b2dafd5a/src%2Flibrustc_codegen_llvm%2Fbuilder.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_codegen_llvm%2Fbuilder.rs?ref=54dd3a47fd54eb466dad7e47b41ec1b5b2dafd5a", "patch": "@@ -201,7 +201,7 @@ impl BuilderMethods<'a, 'tcx> for Builder<'a, 'll, 'tcx> {\n     }\n \n     fn switch(\n-        &self,\n+        &mut self,\n         v: &'ll Value,\n         else_llbb: &'ll BasicBlock,\n         num_cases: usize,\n@@ -212,7 +212,7 @@ impl BuilderMethods<'a, 'tcx> for Builder<'a, 'll, 'tcx> {\n     }\n \n     fn invoke(\n-        &self,\n+        &mut self,\n         llfn: &'ll Value,\n         args: &[&'ll Value],\n         then: &'ll BasicBlock,\n@@ -241,29 +241,29 @@ impl BuilderMethods<'a, 'tcx> for Builder<'a, 'll, 'tcx> {\n         }\n     }\n \n-    fn unreachable(&self) {\n+    fn unreachable(&mut self) {\n         self.count_insn(\"unreachable\");\n         unsafe {\n             llvm::LLVMBuildUnreachable(self.llbuilder);\n         }\n     }\n \n     /* Arithmetic */\n-    fn add(&self, lhs: &'ll Value, rhs: &'ll Value) -> &'ll Value {\n+    fn add(&mut self, lhs: &'ll Value, rhs: &'ll Value) -> &'ll Value {\n         self.count_insn(\"add\");\n         unsafe {\n             llvm::LLVMBuildAdd(self.llbuilder, lhs, rhs, noname())\n         }\n     }\n \n-    fn fadd(&self, lhs: &'ll Value, rhs: &'ll Value) -> &'ll Value {\n+    fn fadd(&mut self, lhs: &'ll Value, rhs: &'ll Value) -> &'ll Value {\n         self.count_insn(\"fadd\");\n         unsafe {\n             llvm::LLVMBuildFAdd(self.llbuilder, lhs, rhs, noname())\n         }\n     }\n \n-    fn fadd_fast(&self, lhs: &'ll Value, rhs: &'ll Value) -> &'ll Value {\n+    fn fadd_fast(&mut self, lhs: &'ll Value, rhs: &'ll Value) -> &'ll Value {\n         self.count_insn(\"fadd\");\n         unsafe {\n             let instr = llvm::LLVMBuildFAdd(self.llbuilder, lhs, rhs, noname());\n@@ -272,21 +272,21 @@ impl BuilderMethods<'a, 'tcx> for Builder<'a, 'll, 'tcx> {\n         }\n     }\n \n-    fn sub(&self, lhs: &'ll Value, rhs: &'ll Value) -> &'ll Value {\n+    fn sub(&mut self, lhs: &'ll Value, rhs: &'ll Value) -> &'ll Value {\n         self.count_insn(\"sub\");\n         unsafe {\n             llvm::LLVMBuildSub(self.llbuilder, lhs, rhs, noname())\n         }\n     }\n \n-    fn fsub(&self, lhs: &'ll Value, rhs: &'ll Value) -> &'ll Value {\n+    fn fsub(&mut self, lhs: &'ll Value, rhs: &'ll Value) -> &'ll Value {\n         self.count_insn(\"fsub\");\n         unsafe {\n             llvm::LLVMBuildFSub(self.llbuilder, lhs, rhs, noname())\n         }\n     }\n \n-    fn fsub_fast(&self, lhs: &'ll Value, rhs: &'ll Value) -> &'ll Value {\n+    fn fsub_fast(&mut self, lhs: &'ll Value, rhs: &'ll Value) -> &'ll Value {\n         self.count_insn(\"fsub\");\n         unsafe {\n             let instr = llvm::LLVMBuildFSub(self.llbuilder, lhs, rhs, noname());\n@@ -295,21 +295,21 @@ impl BuilderMethods<'a, 'tcx> for Builder<'a, 'll, 'tcx> {\n         }\n     }\n \n-    fn mul(&self, lhs: &'ll Value, rhs: &'ll Value) -> &'ll Value {\n+    fn mul(&mut self, lhs: &'ll Value, rhs: &'ll Value) -> &'ll Value {\n         self.count_insn(\"mul\");\n         unsafe {\n             llvm::LLVMBuildMul(self.llbuilder, lhs, rhs, noname())\n         }\n     }\n \n-    fn fmul(&self, lhs: &'ll Value, rhs: &'ll Value) -> &'ll Value {\n+    fn fmul(&mut self, lhs: &'ll Value, rhs: &'ll Value) -> &'ll Value {\n         self.count_insn(\"fmul\");\n         unsafe {\n             llvm::LLVMBuildFMul(self.llbuilder, lhs, rhs, noname())\n         }\n     }\n \n-    fn fmul_fast(&self, lhs: &'ll Value, rhs: &'ll Value) -> &'ll Value {\n+    fn fmul_fast(&mut self, lhs: &'ll Value, rhs: &'ll Value) -> &'ll Value {\n         self.count_insn(\"fmul\");\n         unsafe {\n             let instr = llvm::LLVMBuildFMul(self.llbuilder, lhs, rhs, noname());\n@@ -319,42 +319,42 @@ impl BuilderMethods<'a, 'tcx> for Builder<'a, 'll, 'tcx> {\n     }\n \n \n-    fn udiv(&self, lhs: &'ll Value, rhs: &'ll Value) -> &'ll Value {\n+    fn udiv(&mut self, lhs: &'ll Value, rhs: &'ll Value) -> &'ll Value {\n         self.count_insn(\"udiv\");\n         unsafe {\n             llvm::LLVMBuildUDiv(self.llbuilder, lhs, rhs, noname())\n         }\n     }\n \n-    fn exactudiv(&self, lhs: &'ll Value, rhs: &'ll Value) -> &'ll Value {\n+    fn exactudiv(&mut self, lhs: &'ll Value, rhs: &'ll Value) -> &'ll Value {\n         self.count_insn(\"exactudiv\");\n         unsafe {\n             llvm::LLVMBuildExactUDiv(self.llbuilder, lhs, rhs, noname())\n         }\n     }\n \n-    fn sdiv(&self, lhs: &'ll Value, rhs: &'ll Value) -> &'ll Value {\n+    fn sdiv(&mut self, lhs: &'ll Value, rhs: &'ll Value) -> &'ll Value {\n         self.count_insn(\"sdiv\");\n         unsafe {\n             llvm::LLVMBuildSDiv(self.llbuilder, lhs, rhs, noname())\n         }\n     }\n \n-    fn exactsdiv(&self, lhs: &'ll Value, rhs: &'ll Value) -> &'ll Value {\n+    fn exactsdiv(&mut self, lhs: &'ll Value, rhs: &'ll Value) -> &'ll Value {\n         self.count_insn(\"exactsdiv\");\n         unsafe {\n             llvm::LLVMBuildExactSDiv(self.llbuilder, lhs, rhs, noname())\n         }\n     }\n \n-    fn fdiv(&self, lhs: &'ll Value, rhs: &'ll Value) -> &'ll Value {\n+    fn fdiv(&mut self, lhs: &'ll Value, rhs: &'ll Value) -> &'ll Value {\n         self.count_insn(\"fdiv\");\n         unsafe {\n             llvm::LLVMBuildFDiv(self.llbuilder, lhs, rhs, noname())\n         }\n     }\n \n-    fn fdiv_fast(&self, lhs: &'ll Value, rhs: &'ll Value) -> &'ll Value {\n+    fn fdiv_fast(&mut self, lhs: &'ll Value, rhs: &'ll Value) -> &'ll Value {\n         self.count_insn(\"fdiv\");\n         unsafe {\n             let instr = llvm::LLVMBuildFDiv(self.llbuilder, lhs, rhs, noname());\n@@ -363,28 +363,28 @@ impl BuilderMethods<'a, 'tcx> for Builder<'a, 'll, 'tcx> {\n         }\n     }\n \n-    fn urem(&self, lhs: &'ll Value, rhs: &'ll Value) -> &'ll Value {\n+    fn urem(&mut self, lhs: &'ll Value, rhs: &'ll Value) -> &'ll Value {\n         self.count_insn(\"urem\");\n         unsafe {\n             llvm::LLVMBuildURem(self.llbuilder, lhs, rhs, noname())\n         }\n     }\n \n-    fn srem(&self, lhs: &'ll Value, rhs: &'ll Value) -> &'ll Value {\n+    fn srem(&mut self, lhs: &'ll Value, rhs: &'ll Value) -> &'ll Value {\n         self.count_insn(\"srem\");\n         unsafe {\n             llvm::LLVMBuildSRem(self.llbuilder, lhs, rhs, noname())\n         }\n     }\n \n-    fn frem(&self, lhs: &'ll Value, rhs: &'ll Value) -> &'ll Value {\n+    fn frem(&mut self, lhs: &'ll Value, rhs: &'ll Value) -> &'ll Value {\n         self.count_insn(\"frem\");\n         unsafe {\n             llvm::LLVMBuildFRem(self.llbuilder, lhs, rhs, noname())\n         }\n     }\n \n-    fn frem_fast(&self, lhs: &'ll Value, rhs: &'ll Value) -> &'ll Value {\n+    fn frem_fast(&mut self, lhs: &'ll Value, rhs: &'ll Value) -> &'ll Value {\n         self.count_insn(\"frem\");\n         unsafe {\n             let instr = llvm::LLVMBuildFRem(self.llbuilder, lhs, rhs, noname());\n@@ -393,78 +393,78 @@ impl BuilderMethods<'a, 'tcx> for Builder<'a, 'll, 'tcx> {\n         }\n     }\n \n-    fn shl(&self, lhs: &'ll Value, rhs: &'ll Value) -> &'ll Value {\n+    fn shl(&mut self, lhs: &'ll Value, rhs: &'ll Value) -> &'ll Value {\n         self.count_insn(\"shl\");\n         unsafe {\n             llvm::LLVMBuildShl(self.llbuilder, lhs, rhs, noname())\n         }\n     }\n \n-    fn lshr(&self, lhs: &'ll Value, rhs: &'ll Value) -> &'ll Value {\n+    fn lshr(&mut self, lhs: &'ll Value, rhs: &'ll Value) -> &'ll Value {\n         self.count_insn(\"lshr\");\n         unsafe {\n             llvm::LLVMBuildLShr(self.llbuilder, lhs, rhs, noname())\n         }\n     }\n \n-    fn ashr(&self, lhs: &'ll Value, rhs: &'ll Value) -> &'ll Value {\n+    fn ashr(&mut self, lhs: &'ll Value, rhs: &'ll Value) -> &'ll Value {\n         self.count_insn(\"ashr\");\n         unsafe {\n             llvm::LLVMBuildAShr(self.llbuilder, lhs, rhs, noname())\n         }\n     }\n \n-    fn and(&self, lhs: &'ll Value, rhs: &'ll Value) -> &'ll Value {\n+    fn and(&mut self, lhs: &'ll Value, rhs: &'ll Value) -> &'ll Value {\n         self.count_insn(\"and\");\n         unsafe {\n             llvm::LLVMBuildAnd(self.llbuilder, lhs, rhs, noname())\n         }\n     }\n \n-    fn or(&self, lhs: &'ll Value, rhs: &'ll Value) -> &'ll Value {\n+    fn or(&mut self, lhs: &'ll Value, rhs: &'ll Value) -> &'ll Value {\n         self.count_insn(\"or\");\n         unsafe {\n             llvm::LLVMBuildOr(self.llbuilder, lhs, rhs, noname())\n         }\n     }\n \n-    fn xor(&self, lhs: &'ll Value, rhs: &'ll Value) -> &'ll Value {\n+    fn xor(&mut self, lhs: &'ll Value, rhs: &'ll Value) -> &'ll Value {\n         self.count_insn(\"xor\");\n         unsafe {\n             llvm::LLVMBuildXor(self.llbuilder, lhs, rhs, noname())\n         }\n     }\n \n-    fn neg(&self, v: &'ll Value) -> &'ll Value {\n+    fn neg(&mut self, v: &'ll Value) -> &'ll Value {\n         self.count_insn(\"neg\");\n         unsafe {\n             llvm::LLVMBuildNeg(self.llbuilder, v, noname())\n         }\n     }\n \n-    fn fneg(&self, v: &'ll Value) -> &'ll Value {\n+    fn fneg(&mut self, v: &'ll Value) -> &'ll Value {\n         self.count_insn(\"fneg\");\n         unsafe {\n             llvm::LLVMBuildFNeg(self.llbuilder, v, noname())\n         }\n     }\n \n-    fn not(&self, v: &'ll Value) -> &'ll Value {\n+    fn not(&mut self, v: &'ll Value) -> &'ll Value {\n         self.count_insn(\"not\");\n         unsafe {\n             llvm::LLVMBuildNot(self.llbuilder, v, noname())\n         }\n     }\n \n-    fn alloca(&self, ty: &'ll Type, name: &str, align: Align) -> &'ll Value {\n+    fn alloca(&mut self, ty: &'ll Type, name: &str, align: Align) -> &'ll Value {\n         let mut bx = Builder::with_cx(self.cx);\n         bx.position_at_start(unsafe {\n             llvm::LLVMGetFirstBasicBlock(self.llfn())\n         });\n         bx.dynamic_alloca(ty, name, align)\n     }\n \n-    fn dynamic_alloca(&self, ty: &'ll Type, name: &str, align: Align) -> &'ll Value {\n+    fn dynamic_alloca(&mut self, ty: &'ll Type, name: &str, align: Align) -> &'ll Value {\n         self.count_insn(\"alloca\");\n         unsafe {\n             let alloca = if name.is_empty() {\n@@ -479,7 +479,7 @@ impl BuilderMethods<'a, 'tcx> for Builder<'a, 'll, 'tcx> {\n         }\n     }\n \n-    fn array_alloca(&self,\n+    fn array_alloca(&mut self,\n                         ty: &'ll Type,\n                         len: &'ll Value,\n                         name: &str,\n@@ -498,7 +498,7 @@ impl BuilderMethods<'a, 'tcx> for Builder<'a, 'll, 'tcx> {\n         }\n     }\n \n-    fn load(&self, ptr: &'ll Value, align: Align) -> &'ll Value {\n+    fn load(&mut self, ptr: &'ll Value, align: Align) -> &'ll Value {\n         self.count_insn(\"load\");\n         unsafe {\n             let load = llvm::LLVMBuildLoad(self.llbuilder, ptr, noname());\n@@ -507,7 +507,7 @@ impl BuilderMethods<'a, 'tcx> for Builder<'a, 'll, 'tcx> {\n         }\n     }\n \n-    fn volatile_load(&self, ptr: &'ll Value) -> &'ll Value {\n+    fn volatile_load(&mut self, ptr: &'ll Value) -> &'ll Value {\n         self.count_insn(\"load.volatile\");\n         unsafe {\n             let insn = llvm::LLVMBuildLoad(self.llbuilder, ptr, noname());\n@@ -517,7 +517,7 @@ impl BuilderMethods<'a, 'tcx> for Builder<'a, 'll, 'tcx> {\n     }\n \n     fn atomic_load(\n-        &self,\n+        &mut self,\n         ptr: &'ll Value,\n         order: rustc_codegen_ssa::common::AtomicOrdering,\n         size: Size,\n@@ -537,7 +537,7 @@ impl BuilderMethods<'a, 'tcx> for Builder<'a, 'll, 'tcx> {\n     }\n \n     fn load_operand(\n-        &self,\n+        &mut self,\n         place: PlaceRef<'tcx, &'ll Value>\n     ) -> OperandRef<'tcx, &'ll Value> {\n         debug!(\"PlaceRef::load: {:?}\", place);\n@@ -548,21 +548,25 @@ impl BuilderMethods<'a, 'tcx> for Builder<'a, 'll, 'tcx> {\n             return OperandRef::new_zst(self.cx(), place.layout);\n         }\n \n-        let scalar_load_metadata = |load, scalar: &layout::Scalar| {\n+        fn scalar_load_metadata<'a, 'll, 'tcx>(\n+            bx: &mut Builder<'a, 'll, 'tcx>,\n+            load: &'ll Value,\n+            scalar: &layout::Scalar\n+        ) {\n             let vr = scalar.valid_range.clone();\n             match scalar.value {\n                 layout::Int(..) => {\n-                    let range = scalar.valid_range_exclusive(self.cx());\n+                    let range = scalar.valid_range_exclusive(bx.cx());\n                     if range.start != range.end {\n-                        self.range_metadata(load, range);\n+                        bx.range_metadata(load, range);\n                     }\n                 }\n                 layout::Pointer if vr.start() < vr.end() && !vr.contains(&0) => {\n-                    self.nonnull_metadata(load);\n+                    bx.nonnull_metadata(load);\n                 }\n                 _ => {}\n             }\n-        };\n+        }\n \n         let val = if let Some(llextra) = place.llextra {\n             OperandValue::Ref(place.llval, Some(llextra), place.align)\n@@ -578,16 +582,16 @@ impl BuilderMethods<'a, 'tcx> for Builder<'a, 'll, 'tcx> {\n             let llval = const_llval.unwrap_or_else(|| {\n                 let load = self.load(place.llval, place.align);\n                 if let layout::Abi::Scalar(ref scalar) = place.layout.abi {\n-                    scalar_load_metadata(load, scalar);\n+                    scalar_load_metadata(self, load, scalar);\n                 }\n                 load\n             });\n             OperandValue::Immediate(to_immediate(self, llval, place.layout))\n         } else if let layout::Abi::ScalarPair(ref a, ref b) = place.layout.abi {\n-            let load = |i, scalar: &layout::Scalar| {\n+            let mut load = |i, scalar: &layout::Scalar| {\n                 let llptr = self.struct_gep(place.llval, i as u64);\n                 let load = self.load(llptr, place.align);\n-                scalar_load_metadata(load, scalar);\n+                scalar_load_metadata(self, load, scalar);\n                 if scalar.is_bool() {\n                     self.trunc(load, self.cx().type_i1())\n                 } else {\n@@ -604,7 +608,7 @@ impl BuilderMethods<'a, 'tcx> for Builder<'a, 'll, 'tcx> {\n \n \n \n-    fn range_metadata(&self, load: &'ll Value, range: Range<u128>) {\n+    fn range_metadata(&mut self, load: &'ll Value, range: Range<u128>) {\n         if self.cx().sess().target.target.arch == \"amdgpu\" {\n             // amdgpu/LLVM does something weird and thinks a i64 value is\n             // split into a v2i32, halving the bitwidth LLVM expects,\n@@ -627,19 +631,19 @@ impl BuilderMethods<'a, 'tcx> for Builder<'a, 'll, 'tcx> {\n         }\n     }\n \n-    fn nonnull_metadata(&self, load: &'ll Value) {\n+    fn nonnull_metadata(&mut self, load: &'ll Value) {\n         unsafe {\n             llvm::LLVMSetMetadata(load, llvm::MD_nonnull as c_uint,\n                                   llvm::LLVMMDNodeInContext(self.cx.llcx, ptr::null(), 0));\n         }\n     }\n \n-    fn store(&self, val: &'ll Value, ptr: &'ll Value, align: Align) -> &'ll Value {\n+    fn store(&mut self, val: &'ll Value, ptr: &'ll Value, align: Align) -> &'ll Value {\n         self.store_with_flags(val, ptr, align, MemFlags::empty())\n     }\n \n     fn store_with_flags(\n-        &self,\n+        &mut self,\n         val: &'ll Value,\n         ptr: &'ll Value,\n         align: Align,\n@@ -672,7 +676,7 @@ impl BuilderMethods<'a, 'tcx> for Builder<'a, 'll, 'tcx> {\n         }\n     }\n \n-   fn atomic_store(&self, val: &'ll Value, ptr: &'ll Value,\n+   fn atomic_store(&mut self, val: &'ll Value, ptr: &'ll Value,\n                    order: rustc_codegen_ssa::common::AtomicOrdering, size: Size) {\n         debug!(\"Store {:?} -> {:?}\", val, ptr);\n         self.count_insn(\"store.atomic\");\n@@ -689,15 +693,15 @@ impl BuilderMethods<'a, 'tcx> for Builder<'a, 'll, 'tcx> {\n         }\n     }\n \n-    fn gep(&self, ptr: &'ll Value, indices: &[&'ll Value]) -> &'ll Value {\n+    fn gep(&mut self, ptr: &'ll Value, indices: &[&'ll Value]) -> &'ll Value {\n         self.count_insn(\"gep\");\n         unsafe {\n             llvm::LLVMBuildGEP(self.llbuilder, ptr, indices.as_ptr(),\n                                indices.len() as c_uint, noname())\n         }\n     }\n \n-    fn inbounds_gep(&self, ptr: &'ll Value, indices: &[&'ll Value]) -> &'ll Value {\n+    fn inbounds_gep(&mut self, ptr: &'ll Value, indices: &[&'ll Value]) -> &'ll Value {\n         self.count_insn(\"inboundsgep\");\n         unsafe {\n             llvm::LLVMBuildInBoundsGEP(\n@@ -706,123 +710,123 @@ impl BuilderMethods<'a, 'tcx> for Builder<'a, 'll, 'tcx> {\n     }\n \n     /* Casts */\n-    fn trunc(&self, val: &'ll Value, dest_ty: &'ll Type) -> &'ll Value {\n+    fn trunc(&mut self, val: &'ll Value, dest_ty: &'ll Type) -> &'ll Value {\n         self.count_insn(\"trunc\");\n         unsafe {\n             llvm::LLVMBuildTrunc(self.llbuilder, val, dest_ty, noname())\n         }\n     }\n \n-    fn sext(&self, val: &'ll Value, dest_ty: &'ll Type) -> &'ll Value {\n+    fn sext(&mut self, val: &'ll Value, dest_ty: &'ll Type) -> &'ll Value {\n         self.count_insn(\"sext\");\n         unsafe {\n             llvm::LLVMBuildSExt(self.llbuilder, val, dest_ty, noname())\n         }\n     }\n \n-    fn fptoui(&self, val: &'ll Value, dest_ty: &'ll Type) -> &'ll Value {\n+    fn fptoui(&mut self, val: &'ll Value, dest_ty: &'ll Type) -> &'ll Value {\n         self.count_insn(\"fptoui\");\n         unsafe {\n             llvm::LLVMBuildFPToUI(self.llbuilder, val, dest_ty, noname())\n         }\n     }\n \n-    fn fptosi(&self, val: &'ll Value, dest_ty: &'ll Type) -> &'ll Value {\n+    fn fptosi(&mut self, val: &'ll Value, dest_ty: &'ll Type) -> &'ll Value {\n         self.count_insn(\"fptosi\");\n         unsafe {\n             llvm::LLVMBuildFPToSI(self.llbuilder, val, dest_ty,noname())\n         }\n     }\n \n-    fn uitofp(&self, val: &'ll Value, dest_ty: &'ll Type) -> &'ll Value {\n+    fn uitofp(&mut self, val: &'ll Value, dest_ty: &'ll Type) -> &'ll Value {\n         self.count_insn(\"uitofp\");\n         unsafe {\n             llvm::LLVMBuildUIToFP(self.llbuilder, val, dest_ty, noname())\n         }\n     }\n \n-    fn sitofp(&self, val: &'ll Value, dest_ty: &'ll Type) -> &'ll Value {\n+    fn sitofp(&mut self, val: &'ll Value, dest_ty: &'ll Type) -> &'ll Value {\n         self.count_insn(\"sitofp\");\n         unsafe {\n             llvm::LLVMBuildSIToFP(self.llbuilder, val, dest_ty, noname())\n         }\n     }\n \n-    fn fptrunc(&self, val: &'ll Value, dest_ty: &'ll Type) -> &'ll Value {\n+    fn fptrunc(&mut self, val: &'ll Value, dest_ty: &'ll Type) -> &'ll Value {\n         self.count_insn(\"fptrunc\");\n         unsafe {\n             llvm::LLVMBuildFPTrunc(self.llbuilder, val, dest_ty, noname())\n         }\n     }\n \n-    fn fpext(&self, val: &'ll Value, dest_ty: &'ll Type) -> &'ll Value {\n+    fn fpext(&mut self, val: &'ll Value, dest_ty: &'ll Type) -> &'ll Value {\n         self.count_insn(\"fpext\");\n         unsafe {\n             llvm::LLVMBuildFPExt(self.llbuilder, val, dest_ty, noname())\n         }\n     }\n \n-    fn ptrtoint(&self, val: &'ll Value, dest_ty: &'ll Type) -> &'ll Value {\n+    fn ptrtoint(&mut self, val: &'ll Value, dest_ty: &'ll Type) -> &'ll Value {\n         self.count_insn(\"ptrtoint\");\n         unsafe {\n             llvm::LLVMBuildPtrToInt(self.llbuilder, val, dest_ty, noname())\n         }\n     }\n \n-    fn inttoptr(&self, val: &'ll Value, dest_ty: &'ll Type) -> &'ll Value {\n+    fn inttoptr(&mut self, val: &'ll Value, dest_ty: &'ll Type) -> &'ll Value {\n         self.count_insn(\"inttoptr\");\n         unsafe {\n             llvm::LLVMBuildIntToPtr(self.llbuilder, val, dest_ty, noname())\n         }\n     }\n \n-    fn bitcast(&self, val: &'ll Value, dest_ty: &'ll Type) -> &'ll Value {\n+    fn bitcast(&mut self, val: &'ll Value, dest_ty: &'ll Type) -> &'ll Value {\n         self.count_insn(\"bitcast\");\n         unsafe {\n             llvm::LLVMBuildBitCast(self.llbuilder, val, dest_ty, noname())\n         }\n     }\n \n \n-    fn intcast(&self, val: &'ll Value, dest_ty: &'ll Type, is_signed: bool) -> &'ll Value {\n+    fn intcast(&mut self, val: &'ll Value, dest_ty: &'ll Type, is_signed: bool) -> &'ll Value {\n         self.count_insn(\"intcast\");\n         unsafe {\n             llvm::LLVMRustBuildIntCast(self.llbuilder, val, dest_ty, is_signed)\n         }\n     }\n \n-    fn pointercast(&self, val: &'ll Value, dest_ty: &'ll Type) -> &'ll Value {\n+    fn pointercast(&mut self, val: &'ll Value, dest_ty: &'ll Type) -> &'ll Value {\n         self.count_insn(\"pointercast\");\n         unsafe {\n             llvm::LLVMBuildPointerCast(self.llbuilder, val, dest_ty, noname())\n         }\n     }\n \n     /* Comparisons */\n-    fn icmp(&self, op: IntPredicate, lhs: &'ll Value, rhs: &'ll Value) -> &'ll Value {\n+    fn icmp(&mut self, op: IntPredicate, lhs: &'ll Value, rhs: &'ll Value) -> &'ll Value {\n         self.count_insn(\"icmp\");\n         let op = llvm::IntPredicate::from_generic(op);\n         unsafe {\n             llvm::LLVMBuildICmp(self.llbuilder, op as c_uint, lhs, rhs, noname())\n         }\n     }\n \n-    fn fcmp(&self, op: RealPredicate, lhs: &'ll Value, rhs: &'ll Value) -> &'ll Value {\n+    fn fcmp(&mut self, op: RealPredicate, lhs: &'ll Value, rhs: &'ll Value) -> &'ll Value {\n         self.count_insn(\"fcmp\");\n         unsafe {\n             llvm::LLVMBuildFCmp(self.llbuilder, op as c_uint, lhs, rhs, noname())\n         }\n     }\n \n     /* Miscellaneous instructions */\n-    fn empty_phi(&self, ty: &'ll Type) -> &'ll Value {\n+    fn empty_phi(&mut self, ty: &'ll Type) -> &'ll Value {\n         self.count_insn(\"emptyphi\");\n         unsafe {\n             llvm::LLVMBuildPhi(self.llbuilder, ty, noname())\n         }\n     }\n \n-    fn phi(&self, ty: &'ll Type, vals: &[&'ll Value], bbs: &[&'ll BasicBlock]) -> &'ll Value {\n+    fn phi(&mut self, ty: &'ll Type, vals: &[&'ll Value], bbs: &[&'ll BasicBlock]) -> &'ll Value {\n         assert_eq!(vals.len(), bbs.len());\n         let phi = self.empty_phi(ty);\n         self.count_insn(\"addincoming\");\n@@ -834,7 +838,7 @@ impl BuilderMethods<'a, 'tcx> for Builder<'a, 'll, 'tcx> {\n         }\n     }\n \n-    fn inline_asm_call(&self, asm: *const c_char, cons: *const c_char,\n+    fn inline_asm_call(&mut self, asm: *const c_char, cons: *const c_char,\n                        inputs: &[&'ll Value], output: &'ll Type,\n                        volatile: bool, alignstack: bool,\n                        dia: syntax::ast::AsmDialect) -> Option<&'ll Value> {\n@@ -867,7 +871,7 @@ impl BuilderMethods<'a, 'tcx> for Builder<'a, 'll, 'tcx> {\n         }\n     }\n \n-    fn memcpy(&self, dst: &'ll Value, dst_align: Align,\n+    fn memcpy(&mut self, dst: &'ll Value, dst_align: Align,\n                   src: &'ll Value, src_align: Align,\n                   size: &'ll Value, flags: MemFlags) {\n         if flags.contains(MemFlags::NONTEMPORAL) {\n@@ -887,7 +891,7 @@ impl BuilderMethods<'a, 'tcx> for Builder<'a, 'll, 'tcx> {\n         }\n     }\n \n-    fn memmove(&self, dst: &'ll Value, dst_align: Align,\n+    fn memmove(&mut self, dst: &'ll Value, dst_align: Align,\n                   src: &'ll Value, src_align: Align,\n                   size: &'ll Value, flags: MemFlags) {\n         if flags.contains(MemFlags::NONTEMPORAL) {\n@@ -908,7 +912,7 @@ impl BuilderMethods<'a, 'tcx> for Builder<'a, 'll, 'tcx> {\n     }\n \n     fn memset(\n-        &self,\n+        &mut self,\n         ptr: &'ll Value,\n         fill_byte: &'ll Value,\n         size: &'ll Value,\n@@ -924,14 +928,14 @@ impl BuilderMethods<'a, 'tcx> for Builder<'a, 'll, 'tcx> {\n         self.call(llintrinsicfn, &[ptr, fill_byte, size, align, volatile], None);\n     }\n \n-    fn minnum(&self, lhs: &'ll Value, rhs: &'ll Value) -> &'ll Value {\n+    fn minnum(&mut self, lhs: &'ll Value, rhs: &'ll Value) -> &'ll Value {\n         self.count_insn(\"minnum\");\n         unsafe {\n             let instr = llvm::LLVMRustBuildMinNum(self.llbuilder, lhs, rhs);\n             instr.expect(\"LLVMRustBuildMinNum is not available in LLVM version < 6.0\")\n         }\n     }\n-    fn maxnum(&self, lhs: &'ll Value, rhs: &'ll Value) -> &'ll Value {\n+    fn maxnum(&mut self, lhs: &'ll Value, rhs: &'ll Value) -> &'ll Value {\n         self.count_insn(\"maxnum\");\n         unsafe {\n             let instr = llvm::LLVMRustBuildMaxNum(self.llbuilder, lhs, rhs);\n@@ -940,7 +944,7 @@ impl BuilderMethods<'a, 'tcx> for Builder<'a, 'll, 'tcx> {\n     }\n \n     fn select(\n-        &self, cond: &'ll Value,\n+        &mut self, cond: &'ll Value,\n         then_val: &'ll Value,\n         else_val: &'ll Value,\n     ) -> &'ll Value {\n@@ -951,22 +955,22 @@ impl BuilderMethods<'a, 'tcx> for Builder<'a, 'll, 'tcx> {\n     }\n \n     #[allow(dead_code)]\n-    fn va_arg(&self, list: &'ll Value, ty: &'ll Type) -> &'ll Value {\n+    fn va_arg(&mut self, list: &'ll Value, ty: &'ll Type) -> &'ll Value {\n         self.count_insn(\"vaarg\");\n         unsafe {\n             llvm::LLVMBuildVAArg(self.llbuilder, list, ty, noname())\n         }\n     }\n \n-    fn extract_element(&self, vec: &'ll Value, idx: &'ll Value) -> &'ll Value {\n+    fn extract_element(&mut self, vec: &'ll Value, idx: &'ll Value) -> &'ll Value {\n         self.count_insn(\"extractelement\");\n         unsafe {\n             llvm::LLVMBuildExtractElement(self.llbuilder, vec, idx, noname())\n         }\n     }\n \n     fn insert_element(\n-        &self, vec: &'ll Value,\n+        &mut self, vec: &'ll Value,\n         elt: &'ll Value,\n         idx: &'ll Value,\n     ) -> &'ll Value {\n@@ -976,14 +980,14 @@ impl BuilderMethods<'a, 'tcx> for Builder<'a, 'll, 'tcx> {\n         }\n     }\n \n-    fn shuffle_vector(&self, v1: &'ll Value, v2: &'ll Value, mask: &'ll Value) -> &'ll Value {\n+    fn shuffle_vector(&mut self, v1: &'ll Value, v2: &'ll Value, mask: &'ll Value) -> &'ll Value {\n         self.count_insn(\"shufflevector\");\n         unsafe {\n             llvm::LLVMBuildShuffleVector(self.llbuilder, v1, v2, mask, noname())\n         }\n     }\n \n-    fn vector_splat(&self, num_elts: usize, elt: &'ll Value) -> &'ll Value {\n+    fn vector_splat(&mut self, num_elts: usize, elt: &'ll Value) -> &'ll Value {\n         unsafe {\n             let elt_ty = self.cx.val_ty(elt);\n             let undef = llvm::LLVMGetUndef(self.cx().type_vector(elt_ty, num_elts as u64));\n@@ -993,7 +997,7 @@ impl BuilderMethods<'a, 'tcx> for Builder<'a, 'll, 'tcx> {\n         }\n     }\n \n-    fn vector_reduce_fadd_fast(&self, acc: &'ll Value, src: &'ll Value) -> &'ll Value {\n+    fn vector_reduce_fadd_fast(&mut self, acc: &'ll Value, src: &'ll Value) -> &'ll Value {\n         self.count_insn(\"vector.reduce.fadd_fast\");\n         unsafe {\n             // FIXME: add a non-fast math version once\n@@ -1004,7 +1008,7 @@ impl BuilderMethods<'a, 'tcx> for Builder<'a, 'll, 'tcx> {\n             instr\n         }\n     }\n-    fn vector_reduce_fmul_fast(&self, acc: &'ll Value, src: &'ll Value) -> &'ll Value {\n+    fn vector_reduce_fmul_fast(&mut self, acc: &'ll Value, src: &'ll Value) -> &'ll Value {\n         self.count_insn(\"vector.reduce.fmul_fast\");\n         unsafe {\n             // FIXME: add a non-fast math version once\n@@ -1015,68 +1019,68 @@ impl BuilderMethods<'a, 'tcx> for Builder<'a, 'll, 'tcx> {\n             instr\n         }\n     }\n-    fn vector_reduce_add(&self, src: &'ll Value) -> &'ll Value {\n+    fn vector_reduce_add(&mut self, src: &'ll Value) -> &'ll Value {\n         self.count_insn(\"vector.reduce.add\");\n         unsafe { llvm::LLVMRustBuildVectorReduceAdd(self.llbuilder, src) }\n     }\n-    fn vector_reduce_mul(&self, src: &'ll Value) -> &'ll Value {\n+    fn vector_reduce_mul(&mut self, src: &'ll Value) -> &'ll Value {\n         self.count_insn(\"vector.reduce.mul\");\n         unsafe { llvm::LLVMRustBuildVectorReduceMul(self.llbuilder, src) }\n     }\n-    fn vector_reduce_and(&self, src: &'ll Value) -> &'ll Value {\n+    fn vector_reduce_and(&mut self, src: &'ll Value) -> &'ll Value {\n         self.count_insn(\"vector.reduce.and\");\n         unsafe { llvm::LLVMRustBuildVectorReduceAnd(self.llbuilder, src) }\n     }\n-    fn vector_reduce_or(&self, src: &'ll Value) -> &'ll Value {\n+    fn vector_reduce_or(&mut self, src: &'ll Value) -> &'ll Value {\n         self.count_insn(\"vector.reduce.or\");\n         unsafe { llvm::LLVMRustBuildVectorReduceOr(self.llbuilder, src) }\n     }\n-    fn vector_reduce_xor(&self, src: &'ll Value) -> &'ll Value {\n+    fn vector_reduce_xor(&mut self, src: &'ll Value) -> &'ll Value {\n         self.count_insn(\"vector.reduce.xor\");\n         unsafe { llvm::LLVMRustBuildVectorReduceXor(self.llbuilder, src) }\n     }\n-    fn vector_reduce_fmin(&self, src: &'ll Value) -> &'ll Value {\n+    fn vector_reduce_fmin(&mut self, src: &'ll Value) -> &'ll Value {\n         self.count_insn(\"vector.reduce.fmin\");\n         unsafe { llvm::LLVMRustBuildVectorReduceFMin(self.llbuilder, src, /*NoNaNs:*/ false) }\n     }\n-    fn vector_reduce_fmax(&self, src: &'ll Value) -> &'ll Value {\n+    fn vector_reduce_fmax(&mut self, src: &'ll Value) -> &'ll Value {\n         self.count_insn(\"vector.reduce.fmax\");\n         unsafe { llvm::LLVMRustBuildVectorReduceFMax(self.llbuilder, src, /*NoNaNs:*/ false) }\n     }\n-    fn vector_reduce_fmin_fast(&self, src: &'ll Value) -> &'ll Value {\n+    fn vector_reduce_fmin_fast(&mut self, src: &'ll Value) -> &'ll Value {\n         self.count_insn(\"vector.reduce.fmin_fast\");\n         unsafe {\n             let instr = llvm::LLVMRustBuildVectorReduceFMin(self.llbuilder, src, /*NoNaNs:*/ true);\n             llvm::LLVMRustSetHasUnsafeAlgebra(instr);\n             instr\n         }\n     }\n-    fn vector_reduce_fmax_fast(&self, src: &'ll Value) -> &'ll Value {\n+    fn vector_reduce_fmax_fast(&mut self, src: &'ll Value) -> &'ll Value {\n         self.count_insn(\"vector.reduce.fmax_fast\");\n         unsafe {\n             let instr = llvm::LLVMRustBuildVectorReduceFMax(self.llbuilder, src, /*NoNaNs:*/ true);\n             llvm::LLVMRustSetHasUnsafeAlgebra(instr);\n             instr\n         }\n     }\n-    fn vector_reduce_min(&self, src: &'ll Value, is_signed: bool) -> &'ll Value {\n+    fn vector_reduce_min(&mut self, src: &'ll Value, is_signed: bool) -> &'ll Value {\n         self.count_insn(\"vector.reduce.min\");\n         unsafe { llvm::LLVMRustBuildVectorReduceMin(self.llbuilder, src, is_signed) }\n     }\n-    fn vector_reduce_max(&self, src: &'ll Value, is_signed: bool) -> &'ll Value {\n+    fn vector_reduce_max(&mut self, src: &'ll Value, is_signed: bool) -> &'ll Value {\n         self.count_insn(\"vector.reduce.max\");\n         unsafe { llvm::LLVMRustBuildVectorReduceMax(self.llbuilder, src, is_signed) }\n     }\n \n-    fn extract_value(&self, agg_val: &'ll Value, idx: u64) -> &'ll Value {\n+    fn extract_value(&mut self, agg_val: &'ll Value, idx: u64) -> &'ll Value {\n         self.count_insn(\"extractvalue\");\n         assert_eq!(idx as c_uint as u64, idx);\n         unsafe {\n             llvm::LLVMBuildExtractValue(self.llbuilder, agg_val, idx as c_uint, noname())\n         }\n     }\n \n-    fn insert_value(&self, agg_val: &'ll Value, elt: &'ll Value,\n+    fn insert_value(&mut self, agg_val: &'ll Value, elt: &'ll Value,\n                        idx: u64) -> &'ll Value {\n         self.count_insn(\"insertvalue\");\n         assert_eq!(idx as c_uint as u64, idx);\n@@ -1086,7 +1090,7 @@ impl BuilderMethods<'a, 'tcx> for Builder<'a, 'll, 'tcx> {\n         }\n     }\n \n-    fn landing_pad(&self, ty: &'ll Type, pers_fn: &'ll Value,\n+    fn landing_pad(&mut self, ty: &'ll Type, pers_fn: &'ll Value,\n                        num_clauses: usize) -> &'ll Value {\n         self.count_insn(\"landingpad\");\n         unsafe {\n@@ -1095,27 +1099,27 @@ impl BuilderMethods<'a, 'tcx> for Builder<'a, 'll, 'tcx> {\n         }\n     }\n \n-    fn add_clause(&self, landing_pad: &'ll Value, clause: &'ll Value) {\n+    fn add_clause(&mut self, landing_pad: &'ll Value, clause: &'ll Value) {\n         unsafe {\n             llvm::LLVMAddClause(landing_pad, clause);\n         }\n     }\n \n-    fn set_cleanup(&self, landing_pad: &'ll Value) {\n+    fn set_cleanup(&mut self, landing_pad: &'ll Value) {\n         self.count_insn(\"setcleanup\");\n         unsafe {\n             llvm::LLVMSetCleanup(landing_pad, llvm::True);\n         }\n     }\n \n-    fn resume(&self, exn: &'ll Value) -> &'ll Value {\n+    fn resume(&mut self, exn: &'ll Value) -> &'ll Value {\n         self.count_insn(\"resume\");\n         unsafe {\n             llvm::LLVMBuildResume(self.llbuilder, exn)\n         }\n     }\n \n-    fn cleanup_pad(&self,\n+    fn cleanup_pad(&mut self,\n                        parent: Option<&'ll Value>,\n                        args: &[&'ll Value]) -> Funclet<'ll> {\n         self.count_insn(\"cleanuppad\");\n@@ -1131,7 +1135,7 @@ impl BuilderMethods<'a, 'tcx> for Builder<'a, 'll, 'tcx> {\n     }\n \n     fn cleanup_ret(\n-        &self, funclet: &Funclet<'ll>,\n+        &mut self, funclet: &Funclet<'ll>,\n         unwind: Option<&'ll BasicBlock>,\n     ) -> &'ll Value {\n         self.count_insn(\"cleanupret\");\n@@ -1141,7 +1145,7 @@ impl BuilderMethods<'a, 'tcx> for Builder<'a, 'll, 'tcx> {\n         ret.expect(\"LLVM does not have support for cleanupret\")\n     }\n \n-    fn catch_pad(&self,\n+    fn catch_pad(&mut self,\n                      parent: &'ll Value,\n                      args: &[&'ll Value]) -> Funclet<'ll> {\n         self.count_insn(\"catchpad\");\n@@ -1154,7 +1158,7 @@ impl BuilderMethods<'a, 'tcx> for Builder<'a, 'll, 'tcx> {\n         Funclet::new(ret.expect(\"LLVM does not have support for catchpad\"))\n     }\n \n-    fn catch_ret(&self, funclet: &Funclet<'ll>, unwind: &'ll BasicBlock) -> &'ll Value {\n+    fn catch_ret(&mut self, funclet: &Funclet<'ll>, unwind: &'ll BasicBlock) -> &'ll Value {\n         self.count_insn(\"catchret\");\n         let ret = unsafe {\n             llvm::LLVMRustBuildCatchRet(self.llbuilder, funclet.cleanuppad(), unwind)\n@@ -1163,7 +1167,7 @@ impl BuilderMethods<'a, 'tcx> for Builder<'a, 'll, 'tcx> {\n     }\n \n     fn catch_switch(\n-        &self,\n+        &mut self,\n         parent: Option<&'ll Value>,\n         unwind: Option<&'ll BasicBlock>,\n         num_handlers: usize,\n@@ -1178,21 +1182,21 @@ impl BuilderMethods<'a, 'tcx> for Builder<'a, 'll, 'tcx> {\n         ret.expect(\"LLVM does not have support for catchswitch\")\n     }\n \n-    fn add_handler(&self, catch_switch: &'ll Value, handler: &'ll BasicBlock) {\n+    fn add_handler(&mut self, catch_switch: &'ll Value, handler: &'ll BasicBlock) {\n         unsafe {\n             llvm::LLVMRustAddHandler(catch_switch, handler);\n         }\n     }\n \n-    fn set_personality_fn(&self, personality: &'ll Value) {\n+    fn set_personality_fn(&mut self, personality: &'ll Value) {\n         unsafe {\n             llvm::LLVMSetPersonalityFn(self.llfn(), personality);\n         }\n     }\n \n     // Atomic Operations\n     fn atomic_cmpxchg(\n-        &self,\n+        &mut self,\n         dst: &'ll Value,\n         cmp: &'ll Value,\n         src: &'ll Value,\n@@ -1214,7 +1218,7 @@ impl BuilderMethods<'a, 'tcx> for Builder<'a, 'll, 'tcx> {\n         }\n     }\n     fn atomic_rmw(\n-        &self,\n+        &mut self,\n         op: rustc_codegen_ssa::common::AtomicRmwBinOp,\n         dst: &'ll Value,\n         src: &'ll Value,\n@@ -1232,7 +1236,7 @@ impl BuilderMethods<'a, 'tcx> for Builder<'a, 'll, 'tcx> {\n     }\n \n     fn atomic_fence(\n-        &self,\n+        &mut self,\n         order: rustc_codegen_ssa::common::AtomicOrdering,\n         scope: rustc_codegen_ssa::common::SynchronizationScope\n     ) {\n@@ -1245,27 +1249,27 @@ impl BuilderMethods<'a, 'tcx> for Builder<'a, 'll, 'tcx> {\n         }\n     }\n \n-    fn add_case(&self, s: &'ll Value, on_val: &'ll Value, dest: &'ll BasicBlock) {\n+    fn add_case(&mut self, s: &'ll Value, on_val: &'ll Value, dest: &'ll BasicBlock) {\n         unsafe {\n             llvm::LLVMAddCase(s, on_val, dest)\n         }\n     }\n \n-    fn add_incoming_to_phi(&self, phi: &'ll Value, val: &'ll Value, bb: &'ll BasicBlock) {\n+    fn add_incoming_to_phi(&mut self, phi: &'ll Value, val: &'ll Value, bb: &'ll BasicBlock) {\n         self.count_insn(\"addincoming\");\n         unsafe {\n             llvm::LLVMAddIncoming(phi, &val, &bb, 1 as c_uint);\n         }\n     }\n \n-    fn set_invariant_load(&self, load: &'ll Value) {\n+    fn set_invariant_load(&mut self, load: &'ll Value) {\n         unsafe {\n             llvm::LLVMSetMetadata(load, llvm::MD_invariant_load as c_uint,\n                                   llvm::LLVMMDNodeInContext(self.cx.llcx, ptr::null(), 0));\n         }\n     }\n \n-    fn check_store<'b>(&self,\n+    fn check_store<'b>(&mut self,\n                        val: &'ll Value,\n                        ptr: &'ll Value) -> &'ll Value {\n         let dest_ptr_ty = self.cx.val_ty(ptr);\n@@ -1284,7 +1288,7 @@ impl BuilderMethods<'a, 'tcx> for Builder<'a, 'll, 'tcx> {\n         }\n     }\n \n-    fn check_call<'b>(&self,\n+    fn check_call<'b>(&mut self,\n                       typ: &str,\n                       llfn: &'ll Value,\n                       args: &'b [&'ll Value]) -> Cow<'b, [&'ll Value]> {\n@@ -1326,15 +1330,15 @@ impl BuilderMethods<'a, 'tcx> for Builder<'a, 'll, 'tcx> {\n         Cow::Owned(casted_args)\n     }\n \n-    fn lifetime_start(&self, ptr: &'ll Value, size: Size) {\n+    fn lifetime_start(&mut self, ptr: &'ll Value, size: Size) {\n         self.call_lifetime_intrinsic(\"llvm.lifetime.start\", ptr, size);\n     }\n \n-    fn lifetime_end(&self, ptr: &'ll Value, size: Size) {\n+    fn lifetime_end(&mut self, ptr: &'ll Value, size: Size) {\n         self.call_lifetime_intrinsic(\"llvm.lifetime.end\", ptr, size);\n     }\n \n-    fn call_lifetime_intrinsic(&self, intrinsic: &str, ptr: &'ll Value, size: Size) {\n+    fn call_lifetime_intrinsic(&mut self, intrinsic: &str, ptr: &'ll Value, size: Size) {\n         if self.cx.sess().opts.optimize == config::OptLevel::No {\n             return;\n         }\n@@ -1351,7 +1355,7 @@ impl BuilderMethods<'a, 'tcx> for Builder<'a, 'll, 'tcx> {\n     }\n \n     fn call(\n-        &self,\n+        &mut self,\n         llfn: &'ll Value,\n         args: &[&'ll Value],\n         funclet: Option<&Funclet<'ll>>,\n@@ -1377,14 +1381,14 @@ impl BuilderMethods<'a, 'tcx> for Builder<'a, 'll, 'tcx> {\n         }\n     }\n \n-    fn zext(&self, val: &'ll Value, dest_ty: &'ll Type) -> &'ll Value {\n+    fn zext(&mut self, val: &'ll Value, dest_ty: &'ll Type) -> &'ll Value {\n         self.count_insn(\"zext\");\n         unsafe {\n             llvm::LLVMBuildZExt(self.llbuilder, val, dest_ty, noname())\n         }\n     }\n \n-    fn struct_gep(&self, ptr: &'ll Value, idx: u64) -> &'ll Value {\n+    fn struct_gep(&mut self, ptr: &'ll Value, idx: u64) -> &'ll Value {\n         self.count_insn(\"structgep\");\n         assert_eq!(idx as c_uint as u64, idx);\n         unsafe {\n@@ -1396,13 +1400,13 @@ impl BuilderMethods<'a, 'tcx> for Builder<'a, 'll, 'tcx> {\n         self.cx\n     }\n \n-    fn delete_basic_block(&self, bb: &'ll BasicBlock) {\n+    fn delete_basic_block(&mut self, bb: &'ll BasicBlock) {\n         unsafe {\n             llvm::LLVMDeleteBasicBlock(bb);\n         }\n     }\n \n-    fn do_not_inline(&self, llret: &'ll Value) {\n+    fn do_not_inline(&mut self, llret: &'ll Value) {\n         llvm::Attribute::NoInline.apply_callsite(llvm::AttributePlace::Function, llret);\n     }\n }"}, {"sha": "d6a7f04e1636734a3ae1312a9bec391acbb668b9", "filename": "src/librustc_codegen_llvm/debuginfo/gdb.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/54dd3a47fd54eb466dad7e47b41ec1b5b2dafd5a/src%2Flibrustc_codegen_llvm%2Fdebuginfo%2Fgdb.rs", "raw_url": "https://github.com/rust-lang/rust/raw/54dd3a47fd54eb466dad7e47b41ec1b5b2dafd5a/src%2Flibrustc_codegen_llvm%2Fdebuginfo%2Fgdb.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_codegen_llvm%2Fdebuginfo%2Fgdb.rs?ref=54dd3a47fd54eb466dad7e47b41ec1b5b2dafd5a", "patch": "@@ -23,7 +23,7 @@ use syntax::attr;\n \n /// Inserts a side-effect free instruction sequence that makes sure that the\n /// .debug_gdb_scripts global is referenced, so it isn't removed by the linker.\n-pub fn insert_reference_to_gdb_debug_scripts_section_global(bx: &Builder) {\n+pub fn insert_reference_to_gdb_debug_scripts_section_global(bx: &mut Builder) {\n     if needs_gdb_debug_scripts_section(bx.cx()) {\n         let gdb_debug_scripts_section = get_or_insert_gdb_debug_scripts_section_global(bx.cx());\n         // Load just the first byte as that's all that's necessary to force"}, {"sha": "3a7e393f1dc5c6077e5a7090021fbcf87434deb3", "filename": "src/librustc_codegen_llvm/debuginfo/mod.rs", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "blob_url": "https://github.com/rust-lang/rust/blob/54dd3a47fd54eb466dad7e47b41ec1b5b2dafd5a/src%2Flibrustc_codegen_llvm%2Fdebuginfo%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/54dd3a47fd54eb466dad7e47b41ec1b5b2dafd5a/src%2Flibrustc_codegen_llvm%2Fdebuginfo%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_codegen_llvm%2Fdebuginfo%2Fmod.rs?ref=54dd3a47fd54eb466dad7e47b41ec1b5b2dafd5a", "patch": "@@ -159,7 +159,7 @@ pub fn finalize(cx: &CodegenCx) {\n \n impl DebugInfoBuilderMethods<'tcx> for Builder<'a, 'll, 'tcx> {\n     fn declare_local(\n-        &self,\n+        &mut self,\n         dbg_context: &FunctionDebugContext<&'ll DISubprogram>,\n         variable_name: ast::Name,\n         variable_type: Ty<'tcx>,\n@@ -225,14 +225,14 @@ impl DebugInfoBuilderMethods<'tcx> for Builder<'a, 'll, 'tcx> {\n     }\n \n     fn set_source_location(\n-        &self,\n+        &mut self,\n         debug_context: &FunctionDebugContext<&'ll DISubprogram>,\n         scope: Option<&'ll DIScope>,\n         span: Span,\n     ) {\n         set_source_location(debug_context, &self, scope, span)\n     }\n-    fn insert_reference_to_gdb_debug_scripts_section_global(&self) {\n+    fn insert_reference_to_gdb_debug_scripts_section_global(&mut self) {\n         gdb::insert_reference_to_gdb_debug_scripts_section_global(self)\n     }\n }"}, {"sha": "c2f00d60af25de93e2e100fef42e7d613c6bcdf7", "filename": "src/librustc_codegen_llvm/intrinsic.rs", "status": "modified", "additions": 126, "deletions": 118, "changes": 244, "blob_url": "https://github.com/rust-lang/rust/blob/54dd3a47fd54eb466dad7e47b41ec1b5b2dafd5a/src%2Flibrustc_codegen_llvm%2Fintrinsic.rs", "raw_url": "https://github.com/rust-lang/rust/raw/54dd3a47fd54eb466dad7e47b41ec1b5b2dafd5a/src%2Flibrustc_codegen_llvm%2Fintrinsic.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_codegen_llvm%2Fintrinsic.rs?ref=54dd3a47fd54eb466dad7e47b41ec1b5b2dafd5a", "patch": "@@ -89,15 +89,14 @@ fn get_simple_intrinsic(cx: &CodegenCx<'ll, '_>, name: &str) -> Option<&'ll Valu\n \n impl IntrinsicCallMethods<'tcx> for Builder<'a, 'll, 'tcx> {\n     fn codegen_intrinsic_call(\n-        &self,\n+        &mut self,\n         callee_ty: Ty<'tcx>,\n         fn_ty: &FnType<'tcx, Ty<'tcx>>,\n         args: &[OperandRef<'tcx, &'ll Value>],\n         llresult: &'ll Value,\n         span: Span,\n     ) {\n-        let cx = self.cx();\n-        let tcx = cx.tcx;\n+        let tcx = self.cx().tcx;\n \n         let (def_id, substs) = match callee_ty.sty {\n             ty::FnDef(def_id, substs) => (def_id, substs),\n@@ -110,10 +109,10 @@ impl IntrinsicCallMethods<'tcx> for Builder<'a, 'll, 'tcx> {\n         let ret_ty = sig.output();\n         let name = &*tcx.item_name(def_id).as_str();\n \n-        let llret_ty = cx.layout_of(ret_ty).llvm_type(cx);\n+        let llret_ty = self.cx().layout_of(ret_ty).llvm_type(self.cx());\n         let result = PlaceRef::new_sized(llresult, fn_ty.ret.layout, fn_ty.ret.layout.align);\n \n-        let simple = get_simple_intrinsic(cx, name);\n+        let simple = get_simple_intrinsic(self.cx(), name);\n         let llval = match name {\n             _ if simple.is_some() => {\n                 self.call(simple.unwrap(),\n@@ -124,28 +123,28 @@ impl IntrinsicCallMethods<'tcx> for Builder<'a, 'll, 'tcx> {\n                 return;\n             },\n             \"likely\" => {\n-                let expect = cx.get_intrinsic(&(\"llvm.expect.i1\"));\n-                self.call(expect, &[args[0].immediate(), cx.const_bool(true)], None)\n+                let expect = self.cx().get_intrinsic(&(\"llvm.expect.i1\"));\n+                self.call(expect, &[args[0].immediate(), self.cx().const_bool(true)], None)\n             }\n             \"unlikely\" => {\n-                let expect = cx.get_intrinsic(&(\"llvm.expect.i1\"));\n-                self.call(expect, &[args[0].immediate(), cx.const_bool(false)], None)\n+                let expect = self.cx().get_intrinsic(&(\"llvm.expect.i1\"));\n+                self.call(expect, &[args[0].immediate(), self.cx().const_bool(false)], None)\n             }\n             \"try\" => {\n-                try_intrinsic(self, cx,\n+                try_intrinsic(self,\n                               args[0].immediate(),\n                               args[1].immediate(),\n                               args[2].immediate(),\n                               llresult);\n                 return;\n             }\n             \"breakpoint\" => {\n-                let llfn = cx.get_intrinsic(&(\"llvm.debugtrap\"));\n+                let llfn = self.cx().get_intrinsic(&(\"llvm.debugtrap\"));\n                 self.call(llfn, &[], None)\n             }\n             \"size_of\" => {\n                 let tp_ty = substs.type_at(0);\n-                cx.const_usize(cx.size_of(tp_ty).bytes())\n+                self.cx().const_usize(self.cx().size_of(tp_ty).bytes())\n             }\n             \"size_of_val\" => {\n                 let tp_ty = substs.type_at(0);\n@@ -154,12 +153,12 @@ impl IntrinsicCallMethods<'tcx> for Builder<'a, 'll, 'tcx> {\n                         glue::size_and_align_of_dst(self, tp_ty, Some(meta));\n                     llsize\n                 } else {\n-                    cx.const_usize(cx.size_of(tp_ty).bytes())\n+                    self.cx().const_usize(self.cx().size_of(tp_ty).bytes())\n                 }\n             }\n             \"min_align_of\" => {\n                 let tp_ty = substs.type_at(0);\n-                cx.const_usize(cx.align_of(tp_ty).abi())\n+                self.cx().const_usize(self.cx().align_of(tp_ty).abi())\n             }\n             \"min_align_of_val\" => {\n                 let tp_ty = substs.type_at(0);\n@@ -168,35 +167,35 @@ impl IntrinsicCallMethods<'tcx> for Builder<'a, 'll, 'tcx> {\n                         glue::size_and_align_of_dst(self, tp_ty, Some(meta));\n                     llalign\n                 } else {\n-                    cx.const_usize(cx.align_of(tp_ty).abi())\n+                    self.cx().const_usize(self.cx().align_of(tp_ty).abi())\n                 }\n             }\n             \"pref_align_of\" => {\n                 let tp_ty = substs.type_at(0);\n-                cx.const_usize(cx.align_of(tp_ty).pref())\n+                self.cx().const_usize(self.cx().align_of(tp_ty).pref())\n             }\n             \"type_name\" => {\n                 let tp_ty = substs.type_at(0);\n                 let ty_name = Symbol::intern(&tp_ty.to_string()).as_str();\n-                cx.const_str_slice(ty_name)\n+                self.cx().const_str_slice(ty_name)\n             }\n             \"type_id\" => {\n-                cx.const_u64(cx.tcx.type_id_hash(substs.type_at(0)))\n+                self.cx().const_u64(self.cx().tcx.type_id_hash(substs.type_at(0)))\n             }\n             \"init\" => {\n                 let ty = substs.type_at(0);\n-                if !cx.layout_of(ty).is_zst() {\n+                if !self.cx().layout_of(ty).is_zst() {\n                     // Just zero out the stack slot.\n                     // If we store a zero constant, LLVM will drown in vreg allocation for large\n                     // data structures, and the generated code will be awful. (A telltale sign of\n                     // this is large quantities of `mov [byte ptr foo],0` in the generated code.)\n                     memset_intrinsic(\n-                        &self,\n+                        self,\n                         false,\n                         ty,\n                         llresult,\n-                        cx.const_u8(0),\n-                        cx.const_usize(1)\n+                        self.cx().const_u8(0),\n+                        self.cx().const_usize(1)\n                     );\n                 }\n                 return;\n@@ -208,7 +207,7 @@ impl IntrinsicCallMethods<'tcx> for Builder<'a, 'll, 'tcx> {\n             \"needs_drop\" => {\n                 let tp_ty = substs.type_at(0);\n \n-                cx.const_bool(cx.type_needs_drop(tp_ty))\n+                self.cx().const_bool(self.cx().type_needs_drop(tp_ty))\n             }\n             \"offset\" => {\n                 let ptr = args[0].immediate();\n@@ -222,66 +221,66 @@ impl IntrinsicCallMethods<'tcx> for Builder<'a, 'll, 'tcx> {\n             }\n \n             \"copy_nonoverlapping\" => {\n-                copy_intrinsic(&self, false, false, substs.type_at(0),\n+                copy_intrinsic(self, false, false, substs.type_at(0),\n                                args[1].immediate(), args[0].immediate(), args[2].immediate());\n                 return;\n             }\n             \"copy\" => {\n-                copy_intrinsic(&self, true, false, substs.type_at(0),\n+                copy_intrinsic(self, true, false, substs.type_at(0),\n                                args[1].immediate(), args[0].immediate(), args[2].immediate());\n                 return;\n             }\n             \"write_bytes\" => {\n-                memset_intrinsic(&self, false, substs.type_at(0),\n+                memset_intrinsic(self, false, substs.type_at(0),\n                                  args[0].immediate(), args[1].immediate(), args[2].immediate());\n                 return;\n             }\n \n             \"volatile_copy_nonoverlapping_memory\" => {\n-                copy_intrinsic(&self, false, true, substs.type_at(0),\n+                copy_intrinsic(self, false, true, substs.type_at(0),\n                                args[0].immediate(), args[1].immediate(), args[2].immediate());\n                 return;\n             }\n             \"volatile_copy_memory\" => {\n-                copy_intrinsic(&self, true, true, substs.type_at(0),\n+                copy_intrinsic(self, true, true, substs.type_at(0),\n                                args[0].immediate(), args[1].immediate(), args[2].immediate());\n                 return;\n             }\n             \"volatile_set_memory\" => {\n-                memset_intrinsic(&self, true, substs.type_at(0),\n+                memset_intrinsic(self, true, substs.type_at(0),\n                                  args[0].immediate(), args[1].immediate(), args[2].immediate());\n                 return;\n             }\n             \"volatile_load\" | \"unaligned_volatile_load\" => {\n                 let tp_ty = substs.type_at(0);\n                 let mut ptr = args[0].immediate();\n                 if let PassMode::Cast(ty) = fn_ty.ret.mode {\n-                    ptr = self.pointercast(ptr, cx.type_ptr_to(ty.llvm_type(cx)));\n+                    ptr = self.pointercast(ptr, self.cx().type_ptr_to(ty.llvm_type(self.cx())));\n                 }\n                 let load = self.volatile_load(ptr);\n                 let align = if name == \"unaligned_volatile_load\" {\n                     1\n                 } else {\n-                    cx.align_of(tp_ty).abi() as u32\n+                    self.cx().align_of(tp_ty).abi() as u32\n                 };\n                 unsafe {\n                     llvm::LLVMSetAlignment(load, align);\n                 }\n-                to_immediate(self, load, cx.layout_of(tp_ty))\n+                to_immediate(self, load, self.cx().layout_of(tp_ty))\n             },\n             \"volatile_store\" => {\n-                let dst = args[0].deref(cx);\n+                let dst = args[0].deref(self.cx());\n                 args[1].val.volatile_store(self, dst);\n                 return;\n             },\n             \"unaligned_volatile_store\" => {\n-                let dst = args[0].deref(cx);\n+                let dst = args[0].deref(self.cx());\n                 args[1].val.unaligned_volatile_store(self, dst);\n                 return;\n             },\n             \"prefetch_read_data\" | \"prefetch_write_data\" |\n             \"prefetch_read_instruction\" | \"prefetch_write_instruction\" => {\n-                let expect = cx.get_intrinsic(&(\"llvm.prefetch\"));\n+                let expect = self.cx().get_intrinsic(&(\"llvm.prefetch\"));\n                 let (rw, cache_type) = match name {\n                     \"prefetch_read_data\" => (0, 1),\n                     \"prefetch_write_data\" => (1, 1),\n@@ -291,9 +290,9 @@ impl IntrinsicCallMethods<'tcx> for Builder<'a, 'll, 'tcx> {\n                 };\n                 self.call(expect, &[\n                     args[0].immediate(),\n-                    cx.const_i32(rw),\n+                    self.cx().const_i32(rw),\n                     args[1].immediate(),\n-                    cx.const_i32(cache_type)\n+                    self.cx().const_i32(cache_type)\n                 ], None)\n             },\n             \"ctlz\" | \"ctlz_nonzero\" | \"cttz\" | \"cttz_nonzero\" | \"ctpop\" | \"bswap\" |\n@@ -302,53 +301,63 @@ impl IntrinsicCallMethods<'tcx> for Builder<'a, 'll, 'tcx> {\n             \"unchecked_div\" | \"unchecked_rem\" | \"unchecked_shl\" | \"unchecked_shr\" | \"exact_div\" |\n             \"rotate_left\" | \"rotate_right\" => {\n                 let ty = arg_tys[0];\n-                match int_type_width_signed(ty, cx) {\n+                match int_type_width_signed(ty, self.cx()) {\n                     Some((width, signed)) =>\n                         match name {\n                             \"ctlz\" | \"cttz\" => {\n-                                let y = cx.const_bool(false);\n-                                let llfn = cx.get_intrinsic(&format!(\"llvm.{}.i{}\", name, width));\n+                                let y = self.cx().const_bool(false);\n+                                let llfn = self.cx().get_intrinsic(\n+                                    &format!(\"llvm.{}.i{}\", name, width),\n+                                );\n                                 self.call(llfn, &[args[0].immediate(), y], None)\n                             }\n                             \"ctlz_nonzero\" | \"cttz_nonzero\" => {\n-                                let y = cx.const_bool(true);\n+                                let y = self.cx().const_bool(true);\n                                 let llvm_name = &format!(\"llvm.{}.i{}\", &name[..4], width);\n-                                let llfn = cx.get_intrinsic(llvm_name);\n+                                let llfn = self.cx().get_intrinsic(llvm_name);\n                                 self.call(llfn, &[args[0].immediate(), y], None)\n                             }\n                             \"ctpop\" => self.call(\n-                                cx.get_intrinsic(&format!(\"llvm.ctpop.i{}\", width)),\n+                                self.cx().get_intrinsic(&format!(\"llvm.ctpop.i{}\", width)),\n                                 &[args[0].immediate()],\n                                 None\n                             ),\n                             \"bswap\" => {\n                                 if width == 8 {\n                                     args[0].immediate() // byte swap a u8/i8 is just a no-op\n                                 } else {\n-                                    self.call(cx.get_intrinsic(&format!(\"llvm.bswap.i{}\", width)),\n-                                            &[args[0].immediate()], None)\n+                                    self.call(\n+                                        self.cx().get_intrinsic(\n+                                            &format!(\"llvm.bswap.i{}\", width),\n+                                        ),\n+                                        &[args[0].immediate()],\n+                                        None,\n+                                    )\n                                 }\n                             }\n                             \"bitreverse\" => {\n-                                self.call(cx.get_intrinsic(&format!(\"llvm.bitreverse.i{}\", width)),\n-                                    &[args[0].immediate()], None)\n+                                self.call(\n+                                    self.cx().get_intrinsic(\n+                                        &format!(\"llvm.bitreverse.i{}\", width),\n+                                    ),\n+                                    &[args[0].immediate()],\n+                                    None,\n+                                )\n                             }\n                             \"add_with_overflow\" | \"sub_with_overflow\" | \"mul_with_overflow\" => {\n                                 let intrinsic = format!(\"llvm.{}{}.with.overflow.i{}\",\n                                                         if signed { 's' } else { 'u' },\n                                                         &name[..3], width);\n-                                let llfn = cx.get_intrinsic(&intrinsic);\n+                                let llfn = self.cx().get_intrinsic(&intrinsic);\n \n                                 // Convert `i1` to a `bool`, and write it to the out parameter\n                                 let pair = self.call(llfn, &[\n                                     args[0].immediate(),\n                                     args[1].immediate()\n                                 ], None);\n                                 let val = self.extract_value(pair, 0);\n-                                let overflow = self.zext(\n-                                    self.extract_value(pair, 1),\n-                                    cx.type_bool()\n-                                );\n+                                let overflow = self.extract_value(pair, 1);\n+                                let overflow = self.zext(overflow, self.cx().type_bool());\n \n                                 let dest = result.project_field(self, 0);\n                                 self.store(val, dest.llval, dest.align);\n@@ -393,14 +402,18 @@ impl IntrinsicCallMethods<'tcx> for Builder<'a, 'll, 'tcx> {\n                                     // rotate = funnel shift with first two args the same\n                                     let llvm_name = &format!(\"llvm.fsh{}.i{}\",\n                                                             if is_left { 'l' } else { 'r' }, width);\n-                                    let llfn = cx.get_intrinsic(llvm_name);\n+                                    let llfn = self.cx().get_intrinsic(llvm_name);\n                                     self.call(llfn, &[val, val, raw_shift], None)\n                                 } else {\n                                     // rotate_left: (X << (S % BW)) | (X >> ((BW - S) % BW))\n                                     // rotate_right: (X << ((BW - S) % BW)) | (X >> (S % BW))\n-                                    let width = cx.const_uint(cx.type_ix(width), width);\n+                                    let width = self.cx().const_uint(\n+                                        self.cx().type_ix(width),\n+                                        width,\n+                                    );\n                                     let shift = self.urem(raw_shift, width);\n-                                    let inv_shift = self.urem(self.sub(width, raw_shift), width);\n+                                    let width_minus_raw_shift = self.sub(width, raw_shift);\n+                                    let inv_shift = self.urem(width_minus_raw_shift, width);\n                                     let shift1 = self.shl(\n                                         val,\n                                         if is_left { shift } else { inv_shift },\n@@ -448,11 +461,11 @@ impl IntrinsicCallMethods<'tcx> for Builder<'a, 'll, 'tcx> {\n             },\n \n             \"discriminant_value\" => {\n-                args[0].deref(cx).codegen_get_discr(self, ret_ty)\n+                args[0].deref(self.cx()).codegen_get_discr(self, ret_ty)\n             }\n \n             name if name.starts_with(\"simd_\") => {\n-                match generic_simd_intrinsic(&self, name,\n+                match generic_simd_intrinsic(self, name,\n                                              callee_ty,\n                                              args,\n                                              ret_ty, llret_ty,\n@@ -483,16 +496,16 @@ impl IntrinsicCallMethods<'tcx> for Builder<'a, 'll, 'tcx> {\n                             (SequentiallyConsistent, Monotonic),\n                         \"failacq\" if is_cxchg =>\n                             (SequentiallyConsistent, Acquire),\n-                        _ => cx.sess().fatal(\"unknown ordering in atomic intrinsic\")\n+                        _ => self.cx().sess().fatal(\"unknown ordering in atomic intrinsic\")\n                     },\n                     4 => match (split[2], split[3]) {\n                         (\"acq\", \"failrelaxed\") if is_cxchg =>\n                             (Acquire, Monotonic),\n                         (\"acqrel\", \"failrelaxed\") if is_cxchg =>\n                             (AcquireRelease, Monotonic),\n-                        _ => cx.sess().fatal(\"unknown ordering in atomic intrinsic\")\n+                        _ => self.cx().sess().fatal(\"unknown ordering in atomic intrinsic\")\n                     },\n-                    _ => cx.sess().fatal(\"Atomic intrinsic not in correct format\"),\n+                    _ => self.cx().sess().fatal(\"Atomic intrinsic not in correct format\"),\n                 };\n \n                 let invalid_monomorphization = |ty| {\n@@ -504,7 +517,7 @@ impl IntrinsicCallMethods<'tcx> for Builder<'a, 'll, 'tcx> {\n                 match split[1] {\n                     \"cxchg\" | \"cxchgweak\" => {\n                         let ty = substs.type_at(0);\n-                        if int_type_width_signed(ty, cx).is_some() {\n+                        if int_type_width_signed(ty, self.cx()).is_some() {\n                             let weak = split[1] == \"cxchgweak\";\n                             let pair = self.atomic_cmpxchg(\n                                 args[0].immediate(),\n@@ -514,10 +527,8 @@ impl IntrinsicCallMethods<'tcx> for Builder<'a, 'll, 'tcx> {\n                                 failorder,\n                                 weak);\n                             let val = self.extract_value(pair, 0);\n-                            let success = self.zext(\n-                                self.extract_value(pair, 1),\n-                                cx.type_bool()\n-                            );\n+                            let success = self.extract_value(pair, 1);\n+                            let success = self.zext(success, self.cx().type_bool());\n \n                             let dest = result.project_field(self, 0);\n                             self.store(val, dest.llval, dest.align);\n@@ -531,8 +542,8 @@ impl IntrinsicCallMethods<'tcx> for Builder<'a, 'll, 'tcx> {\n \n                     \"load\" => {\n                         let ty = substs.type_at(0);\n-                        if int_type_width_signed(ty, cx).is_some() {\n-                            let size = cx.size_of(ty);\n+                        if int_type_width_signed(ty, self.cx()).is_some() {\n+                            let size = self.cx().size_of(ty);\n                             self.atomic_load(args[0].immediate(), order, size)\n                         } else {\n                             return invalid_monomorphization(ty);\n@@ -541,8 +552,8 @@ impl IntrinsicCallMethods<'tcx> for Builder<'a, 'll, 'tcx> {\n \n                     \"store\" => {\n                         let ty = substs.type_at(0);\n-                        if int_type_width_signed(ty, cx).is_some() {\n-                            let size = cx.size_of(ty);\n+                        if int_type_width_signed(ty, self.cx()).is_some() {\n+                            let size = self.cx().size_of(ty);\n                             self.atomic_store(\n                                 args[1].immediate(),\n                                 args[0].immediate(),\n@@ -579,11 +590,11 @@ impl IntrinsicCallMethods<'tcx> for Builder<'a, 'll, 'tcx> {\n                             \"min\"   => AtomicRmwBinOp::AtomicMin,\n                             \"umax\"  => AtomicRmwBinOp::AtomicUMax,\n                             \"umin\"  => AtomicRmwBinOp::AtomicUMin,\n-                            _ => cx.sess().fatal(\"unknown atomic operation\")\n+                            _ => self.cx().sess().fatal(\"unknown atomic operation\")\n                         };\n \n                         let ty = substs.type_at(0);\n-                        if int_type_width_signed(ty, cx).is_some() {\n+                        if int_type_width_signed(ty, self.cx()).is_some() {\n                             self.atomic_rmw(\n                                 atom_op,\n                                 args[0].immediate(),\n@@ -598,7 +609,7 @@ impl IntrinsicCallMethods<'tcx> for Builder<'a, 'll, 'tcx> {\n             }\n \n             \"nontemporal_store\" => {\n-                let dst = args[0].deref(cx);\n+                let dst = args[0].deref(self.cx());\n                 args[1].val.nontemporal_store(self, dst);\n                 return;\n             }\n@@ -658,7 +669,7 @@ impl IntrinsicCallMethods<'tcx> for Builder<'a, 'll, 'tcx> {\n                 // arguments to be truncated as needed and pointers to be\n                 // cast.\n                 fn modify_as_needed<'ll, 'tcx>(\n-                    bx: &Builder<'_, 'll, 'tcx>,\n+                    bx: &mut Builder<'_, 'll, 'tcx>,\n                     t: &intrinsics::Type,\n                     arg: &OperandRef<'tcx, &'ll Value>,\n                 ) -> Vec<&'ll Value> {\n@@ -677,7 +688,8 @@ impl IntrinsicCallMethods<'tcx> for Builder<'a, 'll, 'tcx> {\n                             };\n                             let arg = PlaceRef::new_sized(ptr, arg.layout, align);\n                             (0..contents.len()).map(|i| {\n-                                bx.load_operand(arg.project_field(bx, i)).immediate()\n+                                let field = arg.project_field(bx, i);\n+                                bx.load_operand(field).immediate()\n                             }).collect()\n                         }\n                         intrinsics::Type::Pointer(_, Some(ref llvm_elem), _) => {\n@@ -703,21 +715,21 @@ impl IntrinsicCallMethods<'tcx> for Builder<'a, 'll, 'tcx> {\n \n \n                 let inputs = intr.inputs.iter()\n-                                        .flat_map(|t| ty_to_type(cx, t))\n+                                        .flat_map(|t| ty_to_type(self.cx(), t))\n                                         .collect::<Vec<_>>();\n \n-                let outputs = one(ty_to_type(cx, &intr.output));\n+                let outputs = one(ty_to_type(self.cx(), &intr.output));\n \n                 let llargs: Vec<_> = intr.inputs.iter().zip(args).flat_map(|(t, arg)| {\n-                    modify_as_needed(&self, t, arg)\n+                    modify_as_needed(self, t, arg)\n                 }).collect();\n                 assert_eq!(inputs.len(), llargs.len());\n \n                 let val = match intr.definition {\n                     intrinsics::IntrinsicDef::Named(name) => {\n-                        let f = cx.declare_cfn(\n+                        let f = self.cx().declare_cfn(\n                             name,\n-                            cx.type_func(&inputs, outputs),\n+                            self.cx().type_func(&inputs, outputs),\n                         );\n                         self.call(f, &llargs, None)\n                     }\n@@ -742,7 +754,8 @@ impl IntrinsicCallMethods<'tcx> for Builder<'a, 'll, 'tcx> {\n \n         if !fn_ty.ret.is_ignore() {\n             if let PassMode::Cast(ty) = fn_ty.ret.mode {\n-                let ptr = self.pointercast(result.llval, cx.type_ptr_to(ty.llvm_type(cx)));\n+                let ptr_llty = self.cx().type_ptr_to(ty.llvm_type(self.cx()));\n+                let ptr = self.pointercast(result.llval, ptr_llty);\n                 self.store(llval, ptr, result.align);\n             } else {\n                 OperandRef::from_immediate_or_packed_pair(self, llval, result.layout)\n@@ -753,7 +766,7 @@ impl IntrinsicCallMethods<'tcx> for Builder<'a, 'll, 'tcx> {\n }\n \n fn copy_intrinsic(\n-    bx: &Builder<'a, 'll, 'tcx>,\n+    bx: &mut Builder<'a, 'll, 'tcx>,\n     allow_overlap: bool,\n     volatile: bool,\n     ty: Ty<'tcx>,\n@@ -776,26 +789,25 @@ fn copy_intrinsic(\n }\n \n fn memset_intrinsic(\n-    bx: &Builder<'a, 'll, 'tcx>,\n+    bx: &mut Builder<'a, 'll, 'tcx>,\n     volatile: bool,\n     ty: Ty<'tcx>,\n     dst: &'ll Value,\n     val: &'ll Value,\n     count: &'ll Value\n ) {\n     let (size, align) = bx.cx().size_and_align_of(ty);\n-    let size = bx.cx().const_usize(size.bytes());\n+    let size = bx.mul(bx.cx().const_usize(size.bytes()), count);\n     let flags = if volatile {\n         MemFlags::VOLATILE\n     } else {\n         MemFlags::empty()\n     };\n-    bx.memset(dst, val, bx.mul(size, count), align, flags);\n+    bx.memset(dst, val, size, align, flags);\n }\n \n fn try_intrinsic(\n-    bx: &Builder<'a, 'll, 'tcx>,\n-    cx: &CodegenCx<'ll, 'tcx>,\n+    bx: &mut Builder<'a, 'll, 'tcx>,\n     func: &'ll Value,\n     data: &'ll Value,\n     local_ptr: &'ll Value,\n@@ -804,11 +816,11 @@ fn try_intrinsic(\n     if bx.cx().sess().no_landing_pads() {\n         bx.call(func, &[data], None);\n         let ptr_align = bx.tcx().data_layout.pointer_align;\n-        bx.store(cx.const_null(cx.type_i8p()), dest, ptr_align);\n+        bx.store(bx.cx().const_null(bx.cx().type_i8p()), dest, ptr_align);\n     } else if wants_msvc_seh(bx.cx().sess()) {\n-        codegen_msvc_try(bx, cx, func, data, local_ptr, dest);\n+        codegen_msvc_try(bx, func, data, local_ptr, dest);\n     } else {\n-        codegen_gnu_try(bx, cx, func, data, local_ptr, dest);\n+        codegen_gnu_try(bx, func, data, local_ptr, dest);\n     }\n }\n \n@@ -820,21 +832,18 @@ fn try_intrinsic(\n // writing, however, LLVM does not recommend the usage of these new instructions\n // as the old ones are still more optimized.\n fn codegen_msvc_try(\n-    bx: &Builder<'a, 'll, 'tcx>,\n-    cx: &CodegenCx<'ll, 'tcx>,\n+    bx: &mut Builder<'a, 'll, 'tcx>,\n     func: &'ll Value,\n     data: &'ll Value,\n     local_ptr: &'ll Value,\n     dest: &'ll Value,\n ) {\n-    let llfn = get_rust_try_fn(cx, &mut |bx| {\n-        let cx = bx.cx();\n-\n+    let llfn = get_rust_try_fn(bx.cx(), &mut |mut bx| {\n         bx.set_personality_fn(bx.cx().eh_personality());\n \n         let mut normal = bx.build_sibling_block(\"normal\");\n-        let catchswitch = bx.build_sibling_block(\"catchswitch\");\n-        let catchpad = bx.build_sibling_block(\"catchpad\");\n+        let mut catchswitch = bx.build_sibling_block(\"catchswitch\");\n+        let mut catchpad = bx.build_sibling_block(\"catchpad\");\n         let mut caught = bx.build_sibling_block(\"caught\");\n \n         let func = llvm::get_param(bx.llfn(), 0);\n@@ -880,34 +889,35 @@ fn codegen_msvc_try(\n         //      }\n         //\n         // More information can be found in libstd's seh.rs implementation.\n-        let i64p = cx.type_ptr_to(cx.type_i64());\n+        let i64p = bx.cx().type_ptr_to(bx.cx().type_i64());\n         let ptr_align = bx.tcx().data_layout.pointer_align;\n         let slot = bx.alloca(i64p, \"slot\", ptr_align);\n         bx.invoke(func, &[data], normal.llbb(), catchswitch.llbb(), None);\n \n-        normal.ret(cx.const_i32(0));\n+        normal.ret(bx.cx().const_i32(0));\n \n         let cs = catchswitch.catch_switch(None, None, 1);\n         catchswitch.add_handler(cs, catchpad.llbb());\n \n-        let tcx = cx.tcx;\n-        let tydesc = match tcx.lang_items().msvc_try_filter() {\n-            Some(did) => cx.get_static(did),\n+        let tydesc = match bx.tcx().lang_items().msvc_try_filter() {\n+            Some(did) => bx.cx().get_static(did),\n             None => bug!(\"msvc_try_filter not defined\"),\n         };\n-        let funclet = catchpad.catch_pad(cs, &[tydesc, cx.const_i32(0), slot]);\n+        let funclet = catchpad.catch_pad(cs, &[tydesc, bx.cx().const_i32(0), slot]);\n         let addr = catchpad.load(slot, ptr_align);\n \n         let i64_align = bx.tcx().data_layout.i64_align;\n         let arg1 = catchpad.load(addr, i64_align);\n-        let val1 = cx.const_i32(1);\n-        let arg2 = catchpad.load(catchpad.inbounds_gep(addr, &[val1]), i64_align);\n+        let val1 = bx.cx().const_i32(1);\n+        let gep1 = catchpad.inbounds_gep(addr, &[val1]);\n+        let arg2 = catchpad.load(gep1, i64_align);\n         let local_ptr = catchpad.bitcast(local_ptr, i64p);\n+        let gep2 = catchpad.inbounds_gep(local_ptr, &[val1]);\n         catchpad.store(arg1, local_ptr, i64_align);\n-        catchpad.store(arg2, catchpad.inbounds_gep(local_ptr, &[val1]), i64_align);\n+        catchpad.store(arg2, gep2, i64_align);\n         catchpad.catch_ret(&funclet, caught.llbb());\n \n-        caught.ret(cx.const_i32(1));\n+        caught.ret(bx.cx().const_i32(1));\n     });\n \n     // Note that no invoke is used here because by definition this function\n@@ -929,16 +939,13 @@ fn codegen_msvc_try(\n // functions in play. By calling a shim we're guaranteed that our shim will have\n // the right personality function.\n fn codegen_gnu_try(\n-    bx: &Builder<'a, 'll, 'tcx>,\n-    cx: &CodegenCx<'ll, 'tcx>,\n+    bx: &mut Builder<'a, 'll, 'tcx>,\n     func: &'ll Value,\n     data: &'ll Value,\n     local_ptr: &'ll Value,\n     dest: &'ll Value,\n ) {\n-    let llfn = get_rust_try_fn(cx, &mut |bx| {\n-        let cx = bx.cx();\n-\n+    let llfn = get_rust_try_fn(bx.cx(), &mut |mut bx| {\n         // Codegens the shims described above:\n         //\n         //   bx:\n@@ -963,21 +970,22 @@ fn codegen_gnu_try(\n         let data = llvm::get_param(bx.llfn(), 1);\n         let local_ptr = llvm::get_param(bx.llfn(), 2);\n         bx.invoke(func, &[data], then.llbb(), catch.llbb(), None);\n-        then.ret(cx.const_i32(0));\n+        then.ret(bx.cx().const_i32(0));\n \n         // Type indicator for the exception being thrown.\n         //\n         // The first value in this tuple is a pointer to the exception object\n         // being thrown.  The second value is a \"selector\" indicating which of\n         // the landing pad clauses the exception's type had been matched to.\n         // rust_try ignores the selector.\n-        let lpad_ty = cx.type_struct(&[cx.type_i8p(), cx.type_i32()], false);\n+        let lpad_ty = bx.cx().type_struct(&[bx.cx().type_i8p(), bx.cx().type_i32()], false);\n         let vals = catch.landing_pad(lpad_ty, bx.cx().eh_personality(), 1);\n-        catch.add_clause(vals, bx.cx().const_null(cx.type_i8p()));\n+        catch.add_clause(vals, bx.cx().const_null(bx.cx().type_i8p()));\n         let ptr = catch.extract_value(vals, 0);\n         let ptr_align = bx.tcx().data_layout.pointer_align;\n-        catch.store(ptr, catch.bitcast(local_ptr, cx.type_ptr_to(cx.type_i8p())), ptr_align);\n-        catch.ret(cx.const_i32(1));\n+        let bitcast = catch.bitcast(local_ptr, bx.cx().type_ptr_to(bx.cx().type_i8p()));\n+        catch.store(ptr, bitcast, ptr_align);\n+        catch.ret(bx.cx().const_i32(1));\n     });\n \n     // Note that no invoke is used here because by definition this function\n@@ -1043,7 +1051,7 @@ fn span_invalid_monomorphization_error(a: &Session, b: Span, c: &str) {\n }\n \n fn generic_simd_intrinsic(\n-    bx: &Builder<'a, 'll, 'tcx>,\n+    bx: &mut Builder<'a, 'll, 'tcx>,\n     name: &str,\n     callee_ty: Ty<'tcx>,\n     args: &[OperandRef<'tcx, &'ll Value>],\n@@ -1219,7 +1227,7 @@ fn generic_simd_intrinsic(\n         in_elem: &::rustc::ty::TyS,\n         in_ty: &::rustc::ty::TyS,\n         in_len: usize,\n-        bx: &Builder<'a, 'll, 'tcx>,\n+        bx: &mut Builder<'a, 'll, 'tcx>,\n         span: Span,\n         args: &[OperandRef<'tcx, &'ll Value>],\n     ) -> Result<&'ll Value, ()> {"}, {"sha": "81a2d0a538923bfdc80fb5352e91ba738793675f", "filename": "src/librustc_codegen_ssa/base.rs", "status": "modified", "additions": 22, "deletions": 30, "changes": 52, "blob_url": "https://github.com/rust-lang/rust/blob/54dd3a47fd54eb466dad7e47b41ec1b5b2dafd5a/src%2Flibrustc_codegen_ssa%2Fbase.rs", "raw_url": "https://github.com/rust-lang/rust/raw/54dd3a47fd54eb466dad7e47b41ec1b5b2dafd5a/src%2Flibrustc_codegen_ssa%2Fbase.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_codegen_ssa%2Fbase.rs?ref=54dd3a47fd54eb466dad7e47b41ec1b5b2dafd5a", "patch": "@@ -137,7 +137,7 @@ pub fn bin_op_to_fcmp_predicate(op: hir::BinOpKind) -> RealPredicate {\n }\n \n pub fn compare_simd_types<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>>(\n-    bx: &Bx,\n+    bx: &mut Bx,\n     lhs: Bx::Value,\n     rhs: Bx::Value,\n     t: Ty<'tcx>,\n@@ -147,19 +147,21 @@ pub fn compare_simd_types<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>>(\n     let signed = match t.sty {\n         ty::Float(_) => {\n             let cmp = bin_op_to_fcmp_predicate(op);\n-            return bx.sext(bx.fcmp(cmp, lhs, rhs), ret_ty);\n+            let cmp = bx.fcmp(cmp, lhs, rhs);\n+            return bx.sext(cmp, ret_ty);\n         },\n         ty::Uint(_) => false,\n         ty::Int(_) => true,\n         _ => bug!(\"compare_simd_types: invalid SIMD type\"),\n     };\n \n     let cmp = bin_op_to_icmp_predicate(op, signed);\n+    let cmp = bx.icmp(cmp, lhs, rhs);\n     // LLVM outputs an `< size x i1 >`, so we need to perform a sign extension\n     // to get the correctly sized type. This will compile to a single instruction\n     // once the IR is converted to assembly if the SIMD instruction is supported\n     // by the target architecture.\n-    bx.sext(bx.icmp(cmp, lhs, rhs), ret_ty)\n+    bx.sext(cmp, ret_ty)\n }\n \n /// Retrieve the information we are losing (making dynamic) in an unsizing\n@@ -199,7 +201,7 @@ pub fn unsized_info<'tcx, Cx: CodegenMethods<'tcx>>(\n \n /// Coerce `src` to `dst_ty`. `src_ty` must be a thin pointer.\n pub fn unsize_thin_ptr<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>>(\n-    bx: &Bx,\n+    bx: &mut Bx,\n     src: Bx::Value,\n     src_ty: Ty<'tcx>,\n     dst_ty: Ty<'tcx>\n@@ -254,13 +256,13 @@ pub fn unsize_thin_ptr<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>>(\n /// Coerce `src`, which is a reference to a value of type `src_ty`,\n /// to a value of type `dst_ty` and store the result in `dst`\n pub fn coerce_unsized_into<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>>(\n-    bx: &Bx,\n+    bx: &mut Bx,\n     src: PlaceRef<'tcx, Bx::Value>,\n     dst: PlaceRef<'tcx, Bx::Value>\n )  {\n     let src_ty = src.layout.ty;\n     let dst_ty = dst.layout.ty;\n-    let coerce_ptr = || {\n+    let mut coerce_ptr = || {\n         let (base, info) = match bx.load_operand(src).val {\n             OperandValue::Pair(base, info) => {\n                 // fat-ptr to fat-ptr unsize preserves the vtable\n@@ -313,31 +315,20 @@ pub fn coerce_unsized_into<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>>(\n }\n \n pub fn cast_shift_expr_rhs<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>>(\n-    bx: &Bx,\n+    bx: &mut Bx,\n     op: hir::BinOpKind,\n     lhs: Bx::Value,\n     rhs: Bx::Value\n ) -> Bx::Value {\n-    cast_shift_rhs(bx, op, lhs, rhs, |a, b| bx.trunc(a, b), |a, b| bx.zext(a, b))\n+    cast_shift_rhs(bx, op, lhs, rhs)\n }\n \n-fn cast_shift_rhs<'a, 'tcx: 'a, F, G, Bx: BuilderMethods<'a, 'tcx>>(\n-    bx: &Bx,\n+fn cast_shift_rhs<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>>(\n+    bx: &mut Bx,\n     op: hir::BinOpKind,\n     lhs: Bx::Value,\n     rhs: Bx::Value,\n-    trunc: F,\n-    zext: G\n-) -> Bx::Value\n-    where F: FnOnce(\n-        Bx::Value,\n-        Bx::Type\n-    ) -> Bx::Value,\n-    G: FnOnce(\n-        Bx::Value,\n-        Bx::Type\n-    ) -> Bx::Value\n-{\n+) -> Bx::Value {\n     // Shifts may have any size int on the rhs\n     if op.is_shift() {\n         let mut rhs_llty = bx.cx().val_ty(rhs);\n@@ -351,11 +342,11 @@ fn cast_shift_rhs<'a, 'tcx: 'a, F, G, Bx: BuilderMethods<'a, 'tcx>>(\n         let rhs_sz = bx.cx().int_width(rhs_llty);\n         let lhs_sz = bx.cx().int_width(lhs_llty);\n         if lhs_sz < rhs_sz {\n-            trunc(rhs, lhs_llty)\n+            bx.trunc(rhs, lhs_llty)\n         } else if lhs_sz > rhs_sz {\n             // FIXME (#1877: If in the future shifting by negative\n             // values is no longer undefined then this is wrong.\n-            zext(rhs, lhs_llty)\n+            bx.zext(rhs, lhs_llty)\n         } else {\n             rhs\n         }\n@@ -374,15 +365,15 @@ pub fn wants_msvc_seh(sess: &Session) -> bool {\n }\n \n pub fn call_assume<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>>(\n-    bx: &Bx,\n+    bx: &mut Bx,\n     val: Bx::Value\n ) {\n     let assume_intrinsic = bx.cx().get_intrinsic(\"llvm.assume\");\n     bx.call(assume_intrinsic, &[val], None);\n }\n \n pub fn from_immediate<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>>(\n-    bx: &Bx,\n+    bx: &mut Bx,\n     val: Bx::Value\n ) -> Bx::Value {\n     if bx.cx().val_ty(val) == bx.cx().type_i1() {\n@@ -393,7 +384,7 @@ pub fn from_immediate<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>>(\n }\n \n pub fn to_immediate<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>>(\n-    bx: &Bx,\n+    bx: &mut Bx,\n     val: Bx::Value,\n     layout: layout::TyLayout,\n ) -> Bx::Value {\n@@ -404,7 +395,7 @@ pub fn to_immediate<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>>(\n }\n \n pub fn to_immediate_scalar<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>>(\n-    bx: &Bx,\n+    bx: &mut Bx,\n     val: Bx::Value,\n     scalar: &layout::Scalar,\n ) -> Bx::Value {\n@@ -415,7 +406,7 @@ pub fn to_immediate_scalar<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>>(\n }\n \n pub fn memcpy_ty<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>>(\n-    bx: &Bx,\n+    bx: &mut Bx,\n     dst: Bx::Value,\n     dst_align: Align,\n     src: Bx::Value,\n@@ -549,7 +540,8 @@ pub fn maybe_create_entry_wrapper<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>>(\n         };\n \n         let result = bx.call(start_fn, &args, None);\n-        bx.ret(bx.intcast(result, cx.type_int(), true));\n+        let cast = bx.intcast(result, cx.type_int(), true);\n+        bx.ret(cast);\n     }\n }\n "}, {"sha": "1115a74556c1a5ad19b4852d19e1dc4b06a250ec", "filename": "src/librustc_codegen_ssa/common.rs", "status": "modified", "additions": 6, "deletions": 5, "changes": 11, "blob_url": "https://github.com/rust-lang/rust/blob/54dd3a47fd54eb466dad7e47b41ec1b5b2dafd5a/src%2Flibrustc_codegen_ssa%2Fcommon.rs", "raw_url": "https://github.com/rust-lang/rust/raw/54dd3a47fd54eb466dad7e47b41ec1b5b2dafd5a/src%2Flibrustc_codegen_ssa%2Fcommon.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_codegen_ssa%2Fcommon.rs?ref=54dd3a47fd54eb466dad7e47b41ec1b5b2dafd5a", "patch": "@@ -163,7 +163,7 @@ pub fn langcall(tcx: TyCtxt,\n // of Java. (See related discussion on #1877 and #10183.)\n \n pub fn build_unchecked_lshift<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>>(\n-    bx: &Bx,\n+    bx: &mut Bx,\n     lhs: Bx::Value,\n     rhs: Bx::Value\n ) -> Bx::Value {\n@@ -174,7 +174,7 @@ pub fn build_unchecked_lshift<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>>(\n }\n \n pub fn build_unchecked_rshift<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>>(\n-    bx: &Bx,\n+    bx: &mut Bx,\n     lhs_t: Ty<'tcx>,\n     lhs: Bx::Value,\n     rhs: Bx::Value\n@@ -191,15 +191,16 @@ pub fn build_unchecked_rshift<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>>(\n }\n \n fn shift_mask_rhs<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>>(\n-    bx: &Bx,\n+    bx: &mut Bx,\n     rhs: Bx::Value\n ) -> Bx::Value {\n     let rhs_llty = bx.cx().val_ty(rhs);\n-    bx.and(rhs, shift_mask_val(bx, rhs_llty, rhs_llty, false))\n+    let shift_val = shift_mask_val(bx, rhs_llty, rhs_llty, false);\n+    bx.and(rhs, shift_val)\n }\n \n pub fn shift_mask_val<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>>(\n-    bx: &Bx,\n+    bx: &mut Bx,\n     llty: Bx::Type,\n     mask_llty: Bx::Type,\n     invert: bool"}, {"sha": "60485240bd62998758fafa0a11c61eb71aafce63", "filename": "src/librustc_codegen_ssa/glue.rs", "status": "modified", "additions": 15, "deletions": 13, "changes": 28, "blob_url": "https://github.com/rust-lang/rust/blob/54dd3a47fd54eb466dad7e47b41ec1b5b2dafd5a/src%2Flibrustc_codegen_ssa%2Fglue.rs", "raw_url": "https://github.com/rust-lang/rust/raw/54dd3a47fd54eb466dad7e47b41ec1b5b2dafd5a/src%2Flibrustc_codegen_ssa%2Fglue.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_codegen_ssa%2Fglue.rs?ref=54dd3a47fd54eb466dad7e47b41ec1b5b2dafd5a", "patch": "@@ -21,7 +21,7 @@ use rustc::ty::{self, Ty};\n use interfaces::*;\n \n pub fn size_and_align_of_dst<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>>(\n-    bx: &Bx,\n+    bx: &mut Bx,\n     t: Ty<'tcx>,\n     info: Option<Bx::Value>\n ) -> (Bx::Value, Bx::Value) {\n@@ -50,25 +50,24 @@ pub fn size_and_align_of_dst<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>>(\n              bx.cx().const_usize(align.abi()))\n         }\n         _ => {\n-            let cx = bx.cx();\n             // First get the size of all statically known fields.\n             // Don't use size_of because it also rounds up to alignment, which we\n             // want to avoid, as the unsized field's alignment could be smaller.\n             assert!(!t.is_simd());\n-            let layout = cx.layout_of(t);\n+            let layout = bx.cx().layout_of(t);\n             debug!(\"DST {} layout: {:?}\", t, layout);\n \n             let i = layout.fields.count() - 1;\n             let sized_size = layout.fields.offset(i).bytes();\n             let sized_align = layout.align.abi();\n             debug!(\"DST {} statically sized prefix size: {} align: {}\",\n                    t, sized_size, sized_align);\n-            let sized_size = cx.const_usize(sized_size);\n-            let sized_align = cx.const_usize(sized_align);\n+            let sized_size = bx.cx().const_usize(sized_size);\n+            let sized_align = bx.cx().const_usize(sized_align);\n \n             // Recurse to get the size of the dynamically sized field (must be\n             // the last field).\n-            let field_ty = layout.field(cx, i).ty;\n+            let field_ty = layout.field(bx.cx(), i).ty;\n             let (unsized_size, mut unsized_align) = size_and_align_of_dst(bx, field_ty, info);\n \n             // FIXME (#26403, #27023): We should be adding padding\n@@ -95,11 +94,12 @@ pub fn size_and_align_of_dst<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>>(\n                 (Some(sized_align), Some(unsized_align)) => {\n                     // If both alignments are constant, (the sized_align should always be), then\n                     // pick the correct alignment statically.\n-                    cx.const_usize(std::cmp::max(sized_align, unsized_align) as u64)\n+                    bx.cx().const_usize(std::cmp::max(sized_align, unsized_align) as u64)\n+                }\n+                _ => {\n+                    let cmp = bx.icmp(IntPredicate::IntUGT, sized_align, unsized_align);\n+                    bx.select(cmp, sized_align, unsized_align)\n                 }\n-                _ => bx.select(bx.icmp(IntPredicate::IntUGT, sized_align, unsized_align),\n-                               sized_align,\n-                               unsized_align)\n             };\n \n             // Issue #27023: must add any necessary padding to `size`\n@@ -112,9 +112,11 @@ pub fn size_and_align_of_dst<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>>(\n             // emulated via the semi-standard fast bit trick:\n             //\n             //   `(size + (align-1)) & -align`\n-\n-            let addend = bx.sub(align, bx.cx().const_usize(1));\n-            let size = bx.and(bx.add(size, addend), bx.neg(align));\n+            let one = bx.cx().const_usize(1);\n+            let addend = bx.sub(align, one);\n+            let add = bx.add(size, addend);\n+            let neg =  bx.neg(align);\n+            let size = bx.and(add, neg);\n \n             (size, align)\n         }"}, {"sha": "f35eb84813f753a7777b199346e9bec81951b04f", "filename": "src/librustc_codegen_ssa/interfaces/abi.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/54dd3a47fd54eb466dad7e47b41ec1b5b2dafd5a/src%2Flibrustc_codegen_ssa%2Finterfaces%2Fabi.rs", "raw_url": "https://github.com/rust-lang/rust/raw/54dd3a47fd54eb466dad7e47b41ec1b5b2dafd5a/src%2Flibrustc_codegen_ssa%2Finterfaces%2Fabi.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_codegen_ssa%2Finterfaces%2Fabi.rs?ref=54dd3a47fd54eb466dad7e47b41ec1b5b2dafd5a", "patch": "@@ -19,5 +19,5 @@ pub trait AbiMethods<'tcx> {\n }\n \n pub trait AbiBuilderMethods<'tcx>: HasCodegen<'tcx> {\n-    fn apply_attrs_callsite(&self, ty: &FnType<'tcx, Ty<'tcx>>, callsite: Self::Value);\n+    fn apply_attrs_callsite(&mut self, ty: &FnType<'tcx, Ty<'tcx>>, callsite: Self::Value);\n }"}, {"sha": "93e4869e93733e35228f4dd2e7a965aeff1f6f54", "filename": "src/librustc_codegen_ssa/interfaces/asm.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/54dd3a47fd54eb466dad7e47b41ec1b5b2dafd5a/src%2Flibrustc_codegen_ssa%2Finterfaces%2Fasm.rs", "raw_url": "https://github.com/rust-lang/rust/raw/54dd3a47fd54eb466dad7e47b41ec1b5b2dafd5a/src%2Flibrustc_codegen_ssa%2Finterfaces%2Fasm.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_codegen_ssa%2Finterfaces%2Fasm.rs?ref=54dd3a47fd54eb466dad7e47b41ec1b5b2dafd5a", "patch": "@@ -16,7 +16,7 @@ use rustc::hir::{GlobalAsm, InlineAsm};\n pub trait AsmBuilderMethods<'tcx>: HasCodegen<'tcx> {\n     // Take an inline assembly expression and splat it out via LLVM\n     fn codegen_inline_asm(\n-        &self,\n+        &mut self,\n         ia: &InlineAsm,\n         outputs: Vec<PlaceRef<'tcx, Self::Value>>,\n         inputs: Vec<Self::Value>,"}, {"sha": "c80eb27191140d4c7137bd726d3fb551b5fc9f0d", "filename": "src/librustc_codegen_ssa/interfaces/builder.rs", "status": "modified", "additions": 152, "deletions": 117, "changes": 269, "blob_url": "https://github.com/rust-lang/rust/blob/54dd3a47fd54eb466dad7e47b41ec1b5b2dafd5a/src%2Flibrustc_codegen_ssa%2Finterfaces%2Fbuilder.rs", "raw_url": "https://github.com/rust-lang/rust/raw/54dd3a47fd54eb466dad7e47b41ec1b5b2dafd5a/src%2Flibrustc_codegen_ssa%2Finterfaces%2Fbuilder.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_codegen_ssa%2Finterfaces%2Fbuilder.rs?ref=54dd3a47fd54eb466dad7e47b41ec1b5b2dafd5a", "patch": "@@ -53,98 +53,115 @@ pub trait BuilderMethods<'a, 'tcx: 'a>:\n         then_llbb: Self::BasicBlock,\n         else_llbb: Self::BasicBlock,\n     );\n-    fn switch(&self, v: Self::Value, else_llbb: Self::BasicBlock, num_cases: usize) -> Self::Value;\n+    fn switch(\n+        &mut self,\n+        v: Self::Value,\n+        else_llbb: Self::BasicBlock,\n+        num_cases: usize,\n+    ) -> Self::Value;\n     fn invoke(\n-        &self,\n+        &mut self,\n         llfn: Self::Value,\n         args: &[Self::Value],\n         then: Self::BasicBlock,\n         catch: Self::BasicBlock,\n         funclet: Option<&Self::Funclet>,\n     ) -> Self::Value;\n-    fn unreachable(&self);\n-    fn add(&self, lhs: Self::Value, rhs: Self::Value) -> Self::Value;\n-    fn fadd(&self, lhs: Self::Value, rhs: Self::Value) -> Self::Value;\n-    fn fadd_fast(&self, lhs: Self::Value, rhs: Self::Value) -> Self::Value;\n-    fn sub(&self, lhs: Self::Value, rhs: Self::Value) -> Self::Value;\n-    fn fsub(&self, lhs: Self::Value, rhs: Self::Value) -> Self::Value;\n-    fn fsub_fast(&self, lhs: Self::Value, rhs: Self::Value) -> Self::Value;\n-    fn mul(&self, lhs: Self::Value, rhs: Self::Value) -> Self::Value;\n-    fn fmul(&self, lhs: Self::Value, rhs: Self::Value) -> Self::Value;\n-    fn fmul_fast(&self, lhs: Self::Value, rhs: Self::Value) -> Self::Value;\n-    fn udiv(&self, lhs: Self::Value, rhs: Self::Value) -> Self::Value;\n-    fn exactudiv(&self, lhs: Self::Value, rhs: Self::Value) -> Self::Value;\n-    fn sdiv(&self, lhs: Self::Value, rhs: Self::Value) -> Self::Value;\n-    fn exactsdiv(&self, lhs: Self::Value, rhs: Self::Value) -> Self::Value;\n-    fn fdiv(&self, lhs: Self::Value, rhs: Self::Value) -> Self::Value;\n-    fn fdiv_fast(&self, lhs: Self::Value, rhs: Self::Value) -> Self::Value;\n-    fn urem(&self, lhs: Self::Value, rhs: Self::Value) -> Self::Value;\n-    fn srem(&self, lhs: Self::Value, rhs: Self::Value) -> Self::Value;\n-    fn frem(&self, lhs: Self::Value, rhs: Self::Value) -> Self::Value;\n-    fn frem_fast(&self, lhs: Self::Value, rhs: Self::Value) -> Self::Value;\n-    fn shl(&self, lhs: Self::Value, rhs: Self::Value) -> Self::Value;\n-    fn lshr(&self, lhs: Self::Value, rhs: Self::Value) -> Self::Value;\n-    fn ashr(&self, lhs: Self::Value, rhs: Self::Value) -> Self::Value;\n-    fn and(&self, lhs: Self::Value, rhs: Self::Value) -> Self::Value;\n-    fn or(&self, lhs: Self::Value, rhs: Self::Value) -> Self::Value;\n-    fn xor(&self, lhs: Self::Value, rhs: Self::Value) -> Self::Value;\n-    fn neg(&self, v: Self::Value) -> Self::Value;\n-    fn fneg(&self, v: Self::Value) -> Self::Value;\n-    fn not(&self, v: Self::Value) -> Self::Value;\n+    fn unreachable(&mut self);\n+    fn add(&mut self, lhs: Self::Value, rhs: Self::Value) -> Self::Value;\n+    fn fadd(&mut self, lhs: Self::Value, rhs: Self::Value) -> Self::Value;\n+    fn fadd_fast(&mut self, lhs: Self::Value, rhs: Self::Value) -> Self::Value;\n+    fn sub(&mut self, lhs: Self::Value, rhs: Self::Value) -> Self::Value;\n+    fn fsub(&mut self, lhs: Self::Value, rhs: Self::Value) -> Self::Value;\n+    fn fsub_fast(&mut self, lhs: Self::Value, rhs: Self::Value) -> Self::Value;\n+    fn mul(&mut self, lhs: Self::Value, rhs: Self::Value) -> Self::Value;\n+    fn fmul(&mut self, lhs: Self::Value, rhs: Self::Value) -> Self::Value;\n+    fn fmul_fast(&mut self, lhs: Self::Value, rhs: Self::Value) -> Self::Value;\n+    fn udiv(&mut self, lhs: Self::Value, rhs: Self::Value) -> Self::Value;\n+    fn exactudiv(&mut self, lhs: Self::Value, rhs: Self::Value) -> Self::Value;\n+    fn sdiv(&mut self, lhs: Self::Value, rhs: Self::Value) -> Self::Value;\n+    fn exactsdiv(&mut self, lhs: Self::Value, rhs: Self::Value) -> Self::Value;\n+    fn fdiv(&mut self, lhs: Self::Value, rhs: Self::Value) -> Self::Value;\n+    fn fdiv_fast(&mut self, lhs: Self::Value, rhs: Self::Value) -> Self::Value;\n+    fn urem(&mut self, lhs: Self::Value, rhs: Self::Value) -> Self::Value;\n+    fn srem(&mut self, lhs: Self::Value, rhs: Self::Value) -> Self::Value;\n+    fn frem(&mut self, lhs: Self::Value, rhs: Self::Value) -> Self::Value;\n+    fn frem_fast(&mut self, lhs: Self::Value, rhs: Self::Value) -> Self::Value;\n+    fn shl(&mut self, lhs: Self::Value, rhs: Self::Value) -> Self::Value;\n+    fn lshr(&mut self, lhs: Self::Value, rhs: Self::Value) -> Self::Value;\n+    fn ashr(&mut self, lhs: Self::Value, rhs: Self::Value) -> Self::Value;\n+    fn and(&mut self, lhs: Self::Value, rhs: Self::Value) -> Self::Value;\n+    fn or(&mut self, lhs: Self::Value, rhs: Self::Value) -> Self::Value;\n+    fn xor(&mut self, lhs: Self::Value, rhs: Self::Value) -> Self::Value;\n+    fn neg(&mut self, v: Self::Value) -> Self::Value;\n+    fn fneg(&mut self, v: Self::Value) -> Self::Value;\n+    fn not(&mut self, v: Self::Value) -> Self::Value;\n \n-    fn alloca(&self, ty: Self::Type, name: &str, align: Align) -> Self::Value;\n-    fn dynamic_alloca(&self, ty: Self::Type, name: &str, align: Align) -> Self::Value;\n+    fn alloca(&mut self, ty: Self::Type, name: &str, align: Align) -> Self::Value;\n+    fn dynamic_alloca(&mut self, ty: Self::Type, name: &str, align: Align) -> Self::Value;\n     fn array_alloca(\n-        &self,\n+        &mut self,\n         ty: Self::Type,\n         len: Self::Value,\n         name: &str,\n         align: Align,\n     ) -> Self::Value;\n \n-    fn load(&self, ptr: Self::Value, align: Align) -> Self::Value;\n-    fn volatile_load(&self, ptr: Self::Value) -> Self::Value;\n-    fn atomic_load(&self, ptr: Self::Value, order: AtomicOrdering, size: Size) -> Self::Value;\n-    fn load_operand(&self, place: PlaceRef<'tcx, Self::Value>) -> OperandRef<'tcx, Self::Value>;\n+    fn load(&mut self, ptr: Self::Value, align: Align) -> Self::Value;\n+    fn volatile_load(&mut self, ptr: Self::Value) -> Self::Value;\n+    fn atomic_load(&mut self, ptr: Self::Value, order: AtomicOrdering, size: Size) -> Self::Value;\n+    fn load_operand(&mut self, place: PlaceRef<'tcx, Self::Value>)\n+        -> OperandRef<'tcx, Self::Value>;\n \n-    fn range_metadata(&self, load: Self::Value, range: Range<u128>);\n-    fn nonnull_metadata(&self, load: Self::Value);\n+    fn range_metadata(&mut self, load: Self::Value, range: Range<u128>);\n+    fn nonnull_metadata(&mut self, load: Self::Value);\n \n-    fn store(&self, val: Self::Value, ptr: Self::Value, align: Align) -> Self::Value;\n+    fn store(&mut self, val: Self::Value, ptr: Self::Value, align: Align) -> Self::Value;\n     fn store_with_flags(\n-        &self,\n+        &mut self,\n         val: Self::Value,\n         ptr: Self::Value,\n         align: Align,\n         flags: MemFlags,\n     ) -> Self::Value;\n-    fn atomic_store(&self, val: Self::Value, ptr: Self::Value, order: AtomicOrdering, size: Size);\n+    fn atomic_store(\n+        &mut self,\n+        val: Self::Value,\n+        ptr: Self::Value,\n+        order: AtomicOrdering,\n+        size: Size,\n+    );\n \n-    fn gep(&self, ptr: Self::Value, indices: &[Self::Value]) -> Self::Value;\n-    fn inbounds_gep(&self, ptr: Self::Value, indices: &[Self::Value]) -> Self::Value;\n-    fn struct_gep(&self, ptr: Self::Value, idx: u64) -> Self::Value;\n+    fn gep(&mut self, ptr: Self::Value, indices: &[Self::Value]) -> Self::Value;\n+    fn inbounds_gep(&mut self, ptr: Self::Value, indices: &[Self::Value]) -> Self::Value;\n+    fn struct_gep(&mut self, ptr: Self::Value, idx: u64) -> Self::Value;\n \n-    fn trunc(&self, val: Self::Value, dest_ty: Self::Type) -> Self::Value;\n-    fn sext(&self, val: Self::Value, dest_ty: Self::Type) -> Self::Value;\n-    fn fptoui(&self, val: Self::Value, dest_ty: Self::Type) -> Self::Value;\n-    fn fptosi(&self, val: Self::Value, dest_ty: Self::Type) -> Self::Value;\n-    fn uitofp(&self, val: Self::Value, dest_ty: Self::Type) -> Self::Value;\n-    fn sitofp(&self, val: Self::Value, dest_ty: Self::Type) -> Self::Value;\n-    fn fptrunc(&self, val: Self::Value, dest_ty: Self::Type) -> Self::Value;\n-    fn fpext(&self, val: Self::Value, dest_ty: Self::Type) -> Self::Value;\n-    fn ptrtoint(&self, val: Self::Value, dest_ty: Self::Type) -> Self::Value;\n-    fn inttoptr(&self, val: Self::Value, dest_ty: Self::Type) -> Self::Value;\n-    fn bitcast(&self, val: Self::Value, dest_ty: Self::Type) -> Self::Value;\n-    fn intcast(&self, val: Self::Value, dest_ty: Self::Type, is_signed: bool) -> Self::Value;\n-    fn pointercast(&self, val: Self::Value, dest_ty: Self::Type) -> Self::Value;\n+    fn trunc(&mut self, val: Self::Value, dest_ty: Self::Type) -> Self::Value;\n+    fn sext(&mut self, val: Self::Value, dest_ty: Self::Type) -> Self::Value;\n+    fn fptoui(&mut self, val: Self::Value, dest_ty: Self::Type) -> Self::Value;\n+    fn fptosi(&mut self, val: Self::Value, dest_ty: Self::Type) -> Self::Value;\n+    fn uitofp(&mut self, val: Self::Value, dest_ty: Self::Type) -> Self::Value;\n+    fn sitofp(&mut self, val: Self::Value, dest_ty: Self::Type) -> Self::Value;\n+    fn fptrunc(&mut self, val: Self::Value, dest_ty: Self::Type) -> Self::Value;\n+    fn fpext(&mut self, val: Self::Value, dest_ty: Self::Type) -> Self::Value;\n+    fn ptrtoint(&mut self, val: Self::Value, dest_ty: Self::Type) -> Self::Value;\n+    fn inttoptr(&mut self, val: Self::Value, dest_ty: Self::Type) -> Self::Value;\n+    fn bitcast(&mut self, val: Self::Value, dest_ty: Self::Type) -> Self::Value;\n+    fn intcast(&mut self, val: Self::Value, dest_ty: Self::Type, is_signed: bool) -> Self::Value;\n+    fn pointercast(&mut self, val: Self::Value, dest_ty: Self::Type) -> Self::Value;\n \n-    fn icmp(&self, op: IntPredicate, lhs: Self::Value, rhs: Self::Value) -> Self::Value;\n-    fn fcmp(&self, op: RealPredicate, lhs: Self::Value, rhs: Self::Value) -> Self::Value;\n+    fn icmp(&mut self, op: IntPredicate, lhs: Self::Value, rhs: Self::Value) -> Self::Value;\n+    fn fcmp(&mut self, op: RealPredicate, lhs: Self::Value, rhs: Self::Value) -> Self::Value;\n \n-    fn empty_phi(&self, ty: Self::Type) -> Self::Value;\n-    fn phi(&self, ty: Self::Type, vals: &[Self::Value], bbs: &[Self::BasicBlock]) -> Self::Value;\n+    fn empty_phi(&mut self, ty: Self::Type) -> Self::Value;\n+    fn phi(\n+        &mut self,\n+        ty: Self::Type,\n+        vals: &[Self::Value],\n+        bbs: &[Self::BasicBlock],\n+    ) -> Self::Value;\n     fn inline_asm_call(\n-        &self,\n+        &mut self,\n         asm: *const c_char,\n         cons: *const c_char,\n         inputs: &[Self::Value],\n@@ -155,7 +172,7 @@ pub trait BuilderMethods<'a, 'tcx: 'a>:\n     ) -> Option<Self::Value>;\n \n     fn memcpy(\n-        &self,\n+        &mut self,\n         dst: Self::Value,\n         dst_align: Align,\n         src: Self::Value,\n@@ -164,7 +181,7 @@ pub trait BuilderMethods<'a, 'tcx: 'a>:\n         flags: MemFlags,\n     );\n     fn memmove(\n-        &self,\n+        &mut self,\n         dst: Self::Value,\n         dst_align: Align,\n         src: Self::Value,\n@@ -173,64 +190,82 @@ pub trait BuilderMethods<'a, 'tcx: 'a>:\n         flags: MemFlags,\n     );\n     fn memset(\n-        &self,\n+        &mut self,\n         ptr: Self::Value,\n         fill_byte: Self::Value,\n         size: Self::Value,\n         align: Align,\n         flags: MemFlags,\n     );\n \n-    fn minnum(&self, lhs: Self::Value, rhs: Self::Value) -> Self::Value;\n-    fn maxnum(&self, lhs: Self::Value, rhs: Self::Value) -> Self::Value;\n+    fn minnum(&mut self, lhs: Self::Value, rhs: Self::Value) -> Self::Value;\n+    fn maxnum(&mut self, lhs: Self::Value, rhs: Self::Value) -> Self::Value;\n     fn select(\n-        &self,\n+        &mut self,\n         cond: Self::Value,\n         then_val: Self::Value,\n         else_val: Self::Value,\n     ) -> Self::Value;\n \n-    fn va_arg(&self, list: Self::Value, ty: Self::Type) -> Self::Value;\n-    fn extract_element(&self, vec: Self::Value, idx: Self::Value) -> Self::Value;\n-    fn insert_element(&self, vec: Self::Value, elt: Self::Value, idx: Self::Value) -> Self::Value;\n-    fn shuffle_vector(&self, v1: Self::Value, v2: Self::Value, mask: Self::Value) -> Self::Value;\n-    fn vector_splat(&self, num_elts: usize, elt: Self::Value) -> Self::Value;\n-    fn vector_reduce_fadd_fast(&self, acc: Self::Value, src: Self::Value) -> Self::Value;\n-    fn vector_reduce_fmul_fast(&self, acc: Self::Value, src: Self::Value) -> Self::Value;\n-    fn vector_reduce_add(&self, src: Self::Value) -> Self::Value;\n-    fn vector_reduce_mul(&self, src: Self::Value) -> Self::Value;\n-    fn vector_reduce_and(&self, src: Self::Value) -> Self::Value;\n-    fn vector_reduce_or(&self, src: Self::Value) -> Self::Value;\n-    fn vector_reduce_xor(&self, src: Self::Value) -> Self::Value;\n-    fn vector_reduce_fmin(&self, src: Self::Value) -> Self::Value;\n-    fn vector_reduce_fmax(&self, src: Self::Value) -> Self::Value;\n-    fn vector_reduce_fmin_fast(&self, src: Self::Value) -> Self::Value;\n-    fn vector_reduce_fmax_fast(&self, src: Self::Value) -> Self::Value;\n-    fn vector_reduce_min(&self, src: Self::Value, is_signed: bool) -> Self::Value;\n-    fn vector_reduce_max(&self, src: Self::Value, is_signed: bool) -> Self::Value;\n-    fn extract_value(&self, agg_val: Self::Value, idx: u64) -> Self::Value;\n-    fn insert_value(&self, agg_val: Self::Value, elt: Self::Value, idx: u64) -> Self::Value;\n+    fn va_arg(&mut self, list: Self::Value, ty: Self::Type) -> Self::Value;\n+    fn extract_element(&mut self, vec: Self::Value, idx: Self::Value) -> Self::Value;\n+    fn insert_element(\n+        &mut self,\n+        vec: Self::Value,\n+        elt: Self::Value,\n+        idx: Self::Value,\n+    ) -> Self::Value;\n+    fn shuffle_vector(\n+        &mut self,\n+        v1: Self::Value,\n+        v2: Self::Value,\n+        mask: Self::Value,\n+    ) -> Self::Value;\n+    fn vector_splat(&mut self, num_elts: usize, elt: Self::Value) -> Self::Value;\n+    fn vector_reduce_fadd_fast(&mut self, acc: Self::Value, src: Self::Value) -> Self::Value;\n+    fn vector_reduce_fmul_fast(&mut self, acc: Self::Value, src: Self::Value) -> Self::Value;\n+    fn vector_reduce_add(&mut self, src: Self::Value) -> Self::Value;\n+    fn vector_reduce_mul(&mut self, src: Self::Value) -> Self::Value;\n+    fn vector_reduce_and(&mut self, src: Self::Value) -> Self::Value;\n+    fn vector_reduce_or(&mut self, src: Self::Value) -> Self::Value;\n+    fn vector_reduce_xor(&mut self, src: Self::Value) -> Self::Value;\n+    fn vector_reduce_fmin(&mut self, src: Self::Value) -> Self::Value;\n+    fn vector_reduce_fmax(&mut self, src: Self::Value) -> Self::Value;\n+    fn vector_reduce_fmin_fast(&mut self, src: Self::Value) -> Self::Value;\n+    fn vector_reduce_fmax_fast(&mut self, src: Self::Value) -> Self::Value;\n+    fn vector_reduce_min(&mut self, src: Self::Value, is_signed: bool) -> Self::Value;\n+    fn vector_reduce_max(&mut self, src: Self::Value, is_signed: bool) -> Self::Value;\n+    fn extract_value(&mut self, agg_val: Self::Value, idx: u64) -> Self::Value;\n+    fn insert_value(&mut self, agg_val: Self::Value, elt: Self::Value, idx: u64) -> Self::Value;\n \n-    fn landing_pad(&self, ty: Self::Type, pers_fn: Self::Value, num_clauses: usize) -> Self::Value;\n-    fn add_clause(&self, landing_pad: Self::Value, clause: Self::Value);\n-    fn set_cleanup(&self, landing_pad: Self::Value);\n-    fn resume(&self, exn: Self::Value) -> Self::Value;\n-    fn cleanup_pad(&self, parent: Option<Self::Value>, args: &[Self::Value]) -> Self::Funclet;\n-    fn cleanup_ret(&self, funclet: &Self::Funclet, unwind: Option<Self::BasicBlock>)\n-        -> Self::Value;\n-    fn catch_pad(&self, parent: Self::Value, args: &[Self::Value]) -> Self::Funclet;\n-    fn catch_ret(&self, funclet: &Self::Funclet, unwind: Self::BasicBlock) -> Self::Value;\n+    fn landing_pad(\n+        &mut self,\n+        ty: Self::Type,\n+        pers_fn: Self::Value,\n+        num_clauses: usize,\n+    ) -> Self::Value;\n+    fn add_clause(&mut self, landing_pad: Self::Value, clause: Self::Value);\n+    fn set_cleanup(&mut self, landing_pad: Self::Value);\n+    fn resume(&mut self, exn: Self::Value) -> Self::Value;\n+    fn cleanup_pad(&mut self, parent: Option<Self::Value>, args: &[Self::Value]) -> Self::Funclet;\n+    fn cleanup_ret(\n+        &mut self,\n+        funclet: &Self::Funclet,\n+        unwind: Option<Self::BasicBlock>,\n+    ) -> Self::Value;\n+    fn catch_pad(&mut self, parent: Self::Value, args: &[Self::Value]) -> Self::Funclet;\n+    fn catch_ret(&mut self, funclet: &Self::Funclet, unwind: Self::BasicBlock) -> Self::Value;\n     fn catch_switch(\n-        &self,\n+        &mut self,\n         parent: Option<Self::Value>,\n         unwind: Option<Self::BasicBlock>,\n         num_handlers: usize,\n     ) -> Self::Value;\n-    fn add_handler(&self, catch_switch: Self::Value, handler: Self::BasicBlock);\n-    fn set_personality_fn(&self, personality: Self::Value);\n+    fn add_handler(&mut self, catch_switch: Self::Value, handler: Self::BasicBlock);\n+    fn set_personality_fn(&mut self, personality: Self::Value);\n \n     fn atomic_cmpxchg(\n-        &self,\n+        &mut self,\n         dst: Self::Value,\n         cmp: Self::Value,\n         src: Self::Value,\n@@ -239,31 +274,31 @@ pub trait BuilderMethods<'a, 'tcx: 'a>:\n         weak: bool,\n     ) -> Self::Value;\n     fn atomic_rmw(\n-        &self,\n+        &mut self,\n         op: AtomicRmwBinOp,\n         dst: Self::Value,\n         src: Self::Value,\n         order: AtomicOrdering,\n     ) -> Self::Value;\n-    fn atomic_fence(&self, order: AtomicOrdering, scope: SynchronizationScope);\n-    fn add_case(&self, s: Self::Value, on_val: Self::Value, dest: Self::BasicBlock);\n-    fn add_incoming_to_phi(&self, phi: Self::Value, val: Self::Value, bb: Self::BasicBlock);\n-    fn set_invariant_load(&self, load: Self::Value);\n+    fn atomic_fence(&mut self, order: AtomicOrdering, scope: SynchronizationScope);\n+    fn add_case(&mut self, s: Self::Value, on_val: Self::Value, dest: Self::BasicBlock);\n+    fn add_incoming_to_phi(&mut self, phi: Self::Value, val: Self::Value, bb: Self::BasicBlock);\n+    fn set_invariant_load(&mut self, load: Self::Value);\n \n     /// Returns the ptr value that should be used for storing `val`.\n-    fn check_store(&self, val: Self::Value, ptr: Self::Value) -> Self::Value;\n+    fn check_store(&mut self, val: Self::Value, ptr: Self::Value) -> Self::Value;\n \n     /// Returns the args that should be used for a call to `llfn`.\n     fn check_call<'b>(\n-        &self,\n+        &mut self,\n         typ: &str,\n         llfn: Self::Value,\n         args: &'b [Self::Value],\n     ) -> Cow<'b, [Self::Value]>\n     where\n         [Self::Value]: ToOwned;\n-    fn lifetime_start(&self, ptr: Self::Value, size: Size);\n-    fn lifetime_end(&self, ptr: Self::Value, size: Size);\n+    fn lifetime_start(&mut self, ptr: Self::Value, size: Size);\n+    fn lifetime_end(&mut self, ptr: Self::Value, size: Size);\n \n     /// If LLVM lifetime intrinsic support is enabled (i.e. optimizations\n     /// on), and `ptr` is nonzero-sized, then extracts the size of `ptr`\n@@ -273,16 +308,16 @@ pub trait BuilderMethods<'a, 'tcx: 'a>:\n     ///\n     /// If LLVM lifetime intrinsic support is disabled (i.e.  optimizations\n     /// off) or `ptr` is zero-sized, then no-op (does not call `emit`).\n-    fn call_lifetime_intrinsic(&self, intrinsic: &str, ptr: Self::Value, size: Size);\n+    fn call_lifetime_intrinsic(&mut self, intrinsic: &str, ptr: Self::Value, size: Size);\n \n     fn call(\n-        &self,\n+        &mut self,\n         llfn: Self::Value,\n         args: &[Self::Value],\n         funclet: Option<&Self::Funclet>,\n     ) -> Self::Value;\n-    fn zext(&self, val: Self::Value, dest_ty: Self::Type) -> Self::Value;\n+    fn zext(&mut self, val: Self::Value, dest_ty: Self::Type) -> Self::Value;\n \n-    fn delete_basic_block(&self, bb: Self::BasicBlock);\n-    fn do_not_inline(&self, llret: Self::Value);\n+    fn delete_basic_block(&mut self, bb: Self::BasicBlock);\n+    fn do_not_inline(&mut self, llret: Self::Value);\n }"}, {"sha": "643776fcd64f4ee0bb416dec17606e38d0f89d7e", "filename": "src/librustc_codegen_ssa/interfaces/debuginfo.rs", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "blob_url": "https://github.com/rust-lang/rust/blob/54dd3a47fd54eb466dad7e47b41ec1b5b2dafd5a/src%2Flibrustc_codegen_ssa%2Finterfaces%2Fdebuginfo.rs", "raw_url": "https://github.com/rust-lang/rust/raw/54dd3a47fd54eb466dad7e47b41ec1b5b2dafd5a/src%2Flibrustc_codegen_ssa%2Finterfaces%2Fdebuginfo.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_codegen_ssa%2Finterfaces%2Fdebuginfo.rs?ref=54dd3a47fd54eb466dad7e47b41ec1b5b2dafd5a", "patch": "@@ -53,7 +53,7 @@ pub trait DebugInfoMethods<'tcx>: Backend<'tcx> {\n \n pub trait DebugInfoBuilderMethods<'tcx>: HasCodegen<'tcx> {\n     fn declare_local(\n-        &self,\n+        &mut self,\n         dbg_context: &FunctionDebugContext<Self::DIScope>,\n         variable_name: Name,\n         variable_type: Ty<'tcx>,\n@@ -63,10 +63,10 @@ pub trait DebugInfoBuilderMethods<'tcx>: HasCodegen<'tcx> {\n         span: Span,\n     );\n     fn set_source_location(\n-        &self,\n+        &mut self,\n         debug_context: &FunctionDebugContext<Self::DIScope>,\n         scope: Option<Self::DIScope>,\n         span: Span,\n     );\n-    fn insert_reference_to_gdb_debug_scripts_section_global(&self);\n+    fn insert_reference_to_gdb_debug_scripts_section_global(&mut self);\n }"}, {"sha": "53a7878796b3112982d2a746adeb9e48122b84d7", "filename": "src/librustc_codegen_ssa/interfaces/intrinsic.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/54dd3a47fd54eb466dad7e47b41ec1b5b2dafd5a/src%2Flibrustc_codegen_ssa%2Finterfaces%2Fintrinsic.rs", "raw_url": "https://github.com/rust-lang/rust/raw/54dd3a47fd54eb466dad7e47b41ec1b5b2dafd5a/src%2Flibrustc_codegen_ssa%2Finterfaces%2Fintrinsic.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_codegen_ssa%2Finterfaces%2Fintrinsic.rs?ref=54dd3a47fd54eb466dad7e47b41ec1b5b2dafd5a", "patch": "@@ -20,7 +20,7 @@ pub trait IntrinsicCallMethods<'tcx>: HasCodegen<'tcx> {\n     /// and in libcore/intrinsics.rs; if you need access to any llvm intrinsics,\n     /// add them to librustc_codegen_llvm/context.rs\n     fn codegen_intrinsic_call(\n-        &self,\n+        &mut self,\n         callee_ty: Ty<'tcx>,\n         fn_ty: &FnType<'tcx, Ty<'tcx>>,\n         args: &[OperandRef<'tcx, Self::Value>],"}, {"sha": "6d87adb521e9f01ae31d5ca52fda7205ca755b5f", "filename": "src/librustc_codegen_ssa/interfaces/type_.rs", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/rust-lang/rust/blob/54dd3a47fd54eb466dad7e47b41ec1b5b2dafd5a/src%2Flibrustc_codegen_ssa%2Finterfaces%2Ftype_.rs", "raw_url": "https://github.com/rust-lang/rust/raw/54dd3a47fd54eb466dad7e47b41ec1b5b2dafd5a/src%2Flibrustc_codegen_ssa%2Finterfaces%2Ftype_.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_codegen_ssa%2Finterfaces%2Ftype_.rs?ref=54dd3a47fd54eb466dad7e47b41ec1b5b2dafd5a", "patch": "@@ -105,13 +105,13 @@ pub trait LayoutTypeMethods<'tcx>: Backend<'tcx> {\n \n pub trait ArgTypeMethods<'tcx>: HasCodegen<'tcx> {\n     fn store_fn_arg(\n-        &self,\n+        &mut self,\n         ty: &ArgType<'tcx, Ty<'tcx>>,\n         idx: &mut usize,\n         dst: PlaceRef<'tcx, Self::Value>,\n     );\n     fn store_arg_ty(\n-        &self,\n+        &mut self,\n         ty: &ArgType<'tcx, Ty<'tcx>>,\n         val: Self::Value,\n         dst: PlaceRef<'tcx, Self::Value>,"}, {"sha": "ea573640da9a8122e9bb938cf78314a16a3f7ba0", "filename": "src/librustc_codegen_ssa/meth.rs", "status": "modified", "additions": 6, "deletions": 10, "changes": 16, "blob_url": "https://github.com/rust-lang/rust/blob/54dd3a47fd54eb466dad7e47b41ec1b5b2dafd5a/src%2Flibrustc_codegen_ssa%2Fmeth.rs", "raw_url": "https://github.com/rust-lang/rust/raw/54dd3a47fd54eb466dad7e47b41ec1b5b2dafd5a/src%2Flibrustc_codegen_ssa%2Fmeth.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_codegen_ssa%2Fmeth.rs?ref=54dd3a47fd54eb466dad7e47b41ec1b5b2dafd5a", "patch": "@@ -30,7 +30,7 @@ impl<'a, 'tcx: 'a> VirtualIndex {\n \n     pub fn get_fn<Bx: BuilderMethods<'a, 'tcx>>(\n         self,\n-        bx: &Bx,\n+        bx: &mut Bx,\n         llvtable: Bx::Value,\n         fn_ty: &FnType<'tcx, Ty<'tcx>>\n     ) -> Bx::Value {\n@@ -42,10 +42,8 @@ impl<'a, 'tcx: 'a> VirtualIndex {\n             bx.cx().type_ptr_to(bx.cx().fn_ptr_backend_type(fn_ty))\n         );\n         let ptr_align = bx.tcx().data_layout.pointer_align;\n-        let ptr = bx.load(\n-            bx.inbounds_gep(llvtable, &[bx.cx().const_usize(self.0)]),\n-            ptr_align\n-        );\n+        let gep = bx.inbounds_gep(llvtable, &[bx.cx().const_usize(self.0)]);\n+        let ptr = bx.load(gep, ptr_align);\n         bx.nonnull_metadata(ptr);\n         // Vtable loads are invariant\n         bx.set_invariant_load(ptr);\n@@ -54,18 +52,16 @@ impl<'a, 'tcx: 'a> VirtualIndex {\n \n     pub fn get_usize<Bx: BuilderMethods<'a, 'tcx>>(\n         self,\n-        bx: &Bx,\n+        bx: &mut Bx,\n         llvtable: Bx::Value\n     ) -> Bx::Value {\n         // Load the data pointer from the object.\n         debug!(\"get_int({:?}, {:?})\", llvtable, self);\n \n         let llvtable = bx.pointercast(llvtable, bx.cx().type_ptr_to(bx.cx().type_isize()));\n         let usize_align = bx.tcx().data_layout.pointer_align;\n-        let ptr = bx.load(\n-            bx.inbounds_gep(llvtable, &[bx.cx().const_usize(self.0)]),\n-            usize_align\n-        );\n+        let gep = bx.inbounds_gep(llvtable, &[bx.cx().const_usize(self.0)]);\n+        let ptr = bx.load(gep, usize_align);\n         // Vtable loads are invariant\n         bx.set_invariant_load(ptr);\n         ptr"}, {"sha": "e358c057a43a8b6ee6d61436f12143d617ea2eec", "filename": "src/librustc_codegen_ssa/mir/block.rs", "status": "modified", "additions": 57, "deletions": 52, "changes": 109, "blob_url": "https://github.com/rust-lang/rust/blob/54dd3a47fd54eb466dad7e47b41ec1b5b2dafd5a/src%2Flibrustc_codegen_ssa%2Fmir%2Fblock.rs", "raw_url": "https://github.com/rust-lang/rust/raw/54dd3a47fd54eb466dad7e47b41ec1b5b2dafd5a/src%2Flibrustc_codegen_ssa%2Fmir%2Fblock.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_codegen_ssa%2Fmir%2Fblock.rs?ref=54dd3a47fd54eb466dad7e47b41ec1b5b2dafd5a", "patch": "@@ -102,7 +102,7 @@ impl<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>> FunctionCx<'a, 'tcx, Bx> {\n \n                 debug!(\"llblock: creating cleanup trampoline for {:?}\", target);\n                 let name = &format!(\"{:?}_cleanup_trampoline_{:?}\", bb, target);\n-                let trampoline = this.new_block(name);\n+                let mut trampoline = this.new_block(name);\n                 trampoline.cleanup_ret(funclet(this).unwrap(), Some(lltarget));\n                 trampoline.llbb()\n             } else {\n@@ -145,9 +145,9 @@ impl<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>> FunctionCx<'a, 'tcx, Bx> {\n                 bx.apply_attrs_callsite(&fn_ty, invokeret);\n \n                 if let Some((ret_dest, target)) = destination {\n-                    let ret_bx = this.build_block(target);\n-                    this.set_debug_loc(&ret_bx, terminator.source_info);\n-                    this.store_return(&ret_bx, ret_dest, &fn_ty.ret, invokeret);\n+                    let mut ret_bx = this.build_block(target);\n+                    this.set_debug_loc(&mut ret_bx, terminator.source_info);\n+                    this.store_return(&mut ret_bx, ret_dest, &fn_ty.ret, invokeret);\n                 }\n             } else {\n                 let llret = bx.call(fn_ptr, &llargs, funclet(this));\n@@ -169,16 +169,18 @@ impl<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>> FunctionCx<'a, 'tcx, Bx> {\n             }\n         };\n \n-        self.set_debug_loc(&bx, terminator.source_info);\n+        self.set_debug_loc(&mut bx, terminator.source_info);\n         match terminator.kind {\n             mir::TerminatorKind::Resume => {\n                 if let Some(funclet) = funclet(self) {\n                     bx.cleanup_ret(funclet, None);\n                 } else {\n-                    let slot = self.get_personality_slot(&bx);\n-                    let lp0 = bx.load_operand(slot.project_field(&bx, 0)).immediate();\n-                    let lp1 = bx.load_operand(slot.project_field(&bx, 1)).immediate();\n-                    slot.storage_dead(&bx);\n+                    let slot = self.get_personality_slot(&mut bx);\n+                    let lp0 = slot.project_field(&mut bx, 0);\n+                    let lp0 = bx.load_operand(lp0).immediate();\n+                    let lp1 = slot.project_field(&mut bx, 1);\n+                    let lp1 = bx.load_operand(lp1).immediate();\n+                    slot.storage_dead(&mut bx);\n \n                     if !bx.cx().sess().target.target.options.custom_unwind_resume {\n                         let mut lp = bx.cx().const_undef(self.landing_pad_type());\n@@ -204,7 +206,7 @@ impl<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>> FunctionCx<'a, 'tcx, Bx> {\n             }\n \n             mir::TerminatorKind::SwitchInt { ref discr, switch_ty, ref values, ref targets } => {\n-                let discr = self.codegen_operand(&bx, discr);\n+                let discr = self.codegen_operand(&mut bx, discr);\n                 if targets.len() == 2 {\n                     // If there are two targets, emit br instead of switch\n                     let lltrue = llblock(self, targets[0]);\n@@ -249,11 +251,12 @@ impl<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>> FunctionCx<'a, 'tcx, Bx> {\n                     }\n \n                     PassMode::Direct(_) | PassMode::Pair(..) => {\n-                        let op = self.codegen_consume(&bx, &mir::Place::Local(mir::RETURN_PLACE));\n+                        let op =\n+                            self.codegen_consume(&mut bx, &mir::Place::Local(mir::RETURN_PLACE));\n                         if let Ref(llval, _, align) = op.val {\n                             bx.load(llval, align)\n                         } else {\n-                            op.immediate_or_packed_pair(&bx)\n+                            op.immediate_or_packed_pair(&mut bx)\n                         }\n                     }\n \n@@ -271,8 +274,9 @@ impl<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>> FunctionCx<'a, 'tcx, Bx> {\n                         };\n                         let llslot = match op.val {\n                             Immediate(_) | Pair(..) => {\n-                                let scratch = PlaceRef::alloca(&bx, self.fn_ty.ret.layout, \"ret\");\n-                                op.val.store(&bx, scratch);\n+                                let scratch =\n+                                    PlaceRef::alloca(&mut bx, self.fn_ty.ret.layout, \"ret\");\n+                                op.val.store(&mut bx, scratch);\n                                 scratch.llval\n                             }\n                             Ref(llval, _, align) => {\n@@ -281,11 +285,10 @@ impl<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>> FunctionCx<'a, 'tcx, Bx> {\n                                 llval\n                             }\n                         };\n-                        bx.load(\n-                            bx.pointercast(llslot, bx.cx().type_ptr_to(\n-                                bx.cx().cast_backend_type(&cast_ty)\n-                            )),\n-                            self.fn_ty.ret.layout.align)\n+                        let addr = bx.pointercast(llslot, bx.cx().type_ptr_to(\n+                            bx.cx().cast_backend_type(&cast_ty)\n+                        ));\n+                        bx.load(addr, self.fn_ty.ret.layout.align)\n                     }\n                 };\n                 bx.ret(llval);\n@@ -306,7 +309,7 @@ impl<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>> FunctionCx<'a, 'tcx, Bx> {\n                     return\n                 }\n \n-                let place = self.codegen_place(&bx, location);\n+                let place = self.codegen_place(&mut bx, location);\n                 let (args1, args2);\n                 let mut args = if let Some(llextra) = place.llextra {\n                     args2 = [place.llval, llextra];\n@@ -325,7 +328,7 @@ impl<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>> FunctionCx<'a, 'tcx, Bx> {\n                         let fn_ty = bx.cx().new_vtable(sig, &[]);\n                         let vtable = args[1];\n                         args = &args[..1];\n-                        (meth::DESTRUCTOR.get_fn(&bx, vtable, &fn_ty), fn_ty)\n+                        (meth::DESTRUCTOR.get_fn(&mut bx, vtable, &fn_ty), fn_ty)\n                     }\n                     _ => {\n                         (bx.cx().get_fn(drop_fn),\n@@ -338,7 +341,7 @@ impl<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>> FunctionCx<'a, 'tcx, Bx> {\n             }\n \n             mir::TerminatorKind::Assert { ref cond, expected, ref msg, target, cleanup } => {\n-                let cond = self.codegen_operand(&bx, cond).immediate();\n+                let cond = self.codegen_operand(&mut bx, cond).immediate();\n                 let mut const_cond = bx.cx().const_to_opt_u128(cond, false).map(|c| c == 1);\n \n                 // This case can currently arise only from functions marked\n@@ -375,7 +378,7 @@ impl<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>> FunctionCx<'a, 'tcx, Bx> {\n \n                 // After this point, bx is the block for the call to panic.\n                 bx = panic_block;\n-                self.set_debug_loc(&bx, terminator.source_info);\n+                self.set_debug_loc(&mut bx, terminator.source_info);\n \n                 // Get the location information.\n                 let loc = bx.cx().sess().source_map().lookup_char_pos(span.lo());\n@@ -390,8 +393,8 @@ impl<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>> FunctionCx<'a, 'tcx, Bx> {\n                 // Put together the arguments to the panic entry point.\n                 let (lang_item, args) = match *msg {\n                     EvalErrorKind::BoundsCheck { ref len, ref index } => {\n-                        let len = self.codegen_operand(&bx, len).immediate();\n-                        let index = self.codegen_operand(&bx, index).immediate();\n+                        let len = self.codegen_operand(&mut bx, len).immediate();\n+                        let index = self.codegen_operand(&mut bx, index).immediate();\n \n                         let file_line_col = bx.cx().const_struct(&[filename, line, col], false);\n                         let file_line_col = bx.cx().static_addr_of(\n@@ -442,7 +445,7 @@ impl<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>> FunctionCx<'a, 'tcx, Bx> {\n                 from_hir_call: _\n             } => {\n                 // Create the callee. This is a fn ptr or zero-sized and hence a kind of scalar.\n-                let callee = self.codegen_operand(&bx, func);\n+                let callee = self.codegen_operand(&mut bx, func);\n \n                 let (instance, mut llfn) = match callee.layout.ty.sty {\n                     ty::FnDef(def_id, substs) => {\n@@ -476,7 +479,7 @@ impl<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>> FunctionCx<'a, 'tcx, Bx> {\n                 if intrinsic == Some(\"transmute\") {\n                     if let Some(destination_ref) = destination.as_ref() {\n                         let &(ref dest, target) = destination_ref;\n-                        self.codegen_transmute(&bx, &args[0], dest);\n+                        self.codegen_transmute(&mut bx, &args[0], dest);\n                         funclet_br(self, &mut bx, target);\n                     } else {\n                         // If we are trying to transmute to an uninhabited type,\n@@ -567,7 +570,7 @@ impl<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>> FunctionCx<'a, 'tcx, Bx> {\n                 // Prepare the return value destination\n                 let ret_dest = if let Some((ref dest, _)) = *destination {\n                     let is_intrinsic = intrinsic.is_some();\n-                    self.make_return_dest(&bx, dest, &fn_ty.ret, &mut llargs,\n+                    self.make_return_dest(&mut bx, dest, &fn_ty.ret, &mut llargs,\n                                           is_intrinsic)\n                 } else {\n                     ReturnDest::Nothing\n@@ -635,7 +638,7 @@ impl<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>> FunctionCx<'a, 'tcx, Bx> {\n                             }\n                         }\n \n-                        self.codegen_operand(&bx, arg)\n+                        self.codegen_operand(&mut bx, arg)\n                     }).collect();\n \n \n@@ -644,7 +647,7 @@ impl<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>> FunctionCx<'a, 'tcx, Bx> {\n                                                terminator.source_info.span);\n \n                     if let ReturnDest::IndirectOperand(dst, _) = ret_dest {\n-                        self.store_return(&bx, ret_dest, &fn_ty.ret, dst.llval);\n+                        self.store_return(&mut bx, ret_dest, &fn_ty.ret, dst.llval);\n                     }\n \n                     if let Some((_, target)) = *destination {\n@@ -665,7 +668,7 @@ impl<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>> FunctionCx<'a, 'tcx, Bx> {\n                 };\n \n                 'make_args: for (i, arg) in first_args.iter().enumerate() {\n-                    let mut op = self.codegen_operand(&bx, arg);\n+                    let mut op = self.codegen_operand(&mut bx, arg);\n \n                     if let (0, Some(ty::InstanceDef::Virtual(_, idx))) = (i, def) {\n                         if let Pair(..) = op.val {\n@@ -679,7 +682,7 @@ impl<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>> FunctionCx<'a, 'tcx, Bx> {\n                                             && !op.layout.ty.is_region_ptr()\n                             {\n                                 'iter_fields: for i in 0..op.layout.fields.count() {\n-                                    let field = op.extract_field(&bx, i);\n+                                    let field = op.extract_field(&mut bx, i);\n                                     if !field.layout.is_zst() {\n                                         // we found the one non-zero-sized field that is allowed\n                                         // now find *its* non-zero-sized field, or stop if it's a\n@@ -698,7 +701,7 @@ impl<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>> FunctionCx<'a, 'tcx, Bx> {\n                             match op.val {\n                                 Pair(data_ptr, meta) => {\n                                     llfn = Some(meth::VirtualIndex::from_index(idx)\n-                                        .get_fn(&bx, meta, &fn_ty));\n+                                        .get_fn(&mut bx, meta, &fn_ty));\n                                     llargs.push(data_ptr);\n                                     continue 'make_args\n                                 }\n@@ -707,7 +710,7 @@ impl<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>> FunctionCx<'a, 'tcx, Bx> {\n                         } else if let Ref(data_ptr, Some(meta), _) = op.val {\n                             // by-value dynamic dispatch\n                             llfn = Some(meth::VirtualIndex::from_index(idx)\n-                                .get_fn(&bx, meta, &fn_ty));\n+                                .get_fn(&mut bx, meta, &fn_ty));\n                             llargs.push(data_ptr);\n                             continue;\n                         } else {\n@@ -720,17 +723,17 @@ impl<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>> FunctionCx<'a, 'tcx, Bx> {\n                     match (arg, op.val) {\n                         (&mir::Operand::Copy(_), Ref(_, None, _)) |\n                         (&mir::Operand::Constant(_), Ref(_, None, _)) => {\n-                            let tmp = PlaceRef::alloca(&bx, op.layout, \"const\");\n-                            op.val.store(&bx, tmp);\n+                            let tmp = PlaceRef::alloca(&mut bx, op.layout, \"const\");\n+                            op.val.store(&mut bx, tmp);\n                             op.val = Ref(tmp.llval, None, tmp.align);\n                         }\n                         _ => {}\n                     }\n \n-                    self.codegen_argument(&bx, op, &mut llargs, &fn_ty.args[i]);\n+                    self.codegen_argument(&mut bx, op, &mut llargs, &fn_ty.args[i]);\n                 }\n                 if let Some(tup) = untuple {\n-                    self.codegen_arguments_untupled(&bx, tup, &mut llargs,\n+                    self.codegen_arguments_untupled(&mut bx, tup, &mut llargs,\n                         &fn_ty.args[first_args.len()..])\n                 }\n \n@@ -753,7 +756,7 @@ impl<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>> FunctionCx<'a, 'tcx, Bx> {\n \n     fn codegen_argument(\n         &mut self,\n-        bx: &Bx,\n+        bx: &mut Bx,\n         op: OperandRef<'tcx, Bx::Value>,\n         llargs: &mut Vec<Bx::Value>,\n         arg: &ArgType<'tcx, Ty<'tcx>>\n@@ -820,9 +823,10 @@ impl<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>> FunctionCx<'a, 'tcx, Bx> {\n         if by_ref && !arg.is_indirect() {\n             // Have to load the argument, maybe while casting it.\n             if let PassMode::Cast(ty) = arg.mode {\n-                llval = bx.load(bx.pointercast(llval, bx.cx().type_ptr_to(\n+                let addr = bx.pointercast(llval, bx.cx().type_ptr_to(\n                     bx.cx().cast_backend_type(&ty))\n-                ), align.min(arg.layout.align));\n+                );\n+                llval = bx.load(addr, align.min(arg.layout.align));\n             } else {\n                 // We can't use `PlaceRef::load` here because the argument\n                 // may have a type we don't treat as immediate, but the ABI\n@@ -845,7 +849,7 @@ impl<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>> FunctionCx<'a, 'tcx, Bx> {\n \n     fn codegen_arguments_untupled(\n         &mut self,\n-        bx: &Bx,\n+        bx: &mut Bx,\n         operand: &mir::Operand<'tcx>,\n         llargs: &mut Vec<Bx::Value>,\n         args: &[ArgType<'tcx, Ty<'tcx>>]\n@@ -857,7 +861,8 @@ impl<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>> FunctionCx<'a, 'tcx, Bx> {\n             let tuple_ptr = PlaceRef::new_sized(llval, tuple.layout, align);\n             for i in 0..tuple.layout.fields.count() {\n                 let field_ptr = tuple_ptr.project_field(bx, i);\n-                self.codegen_argument(bx, bx.load_operand(field_ptr), llargs, &args[i]);\n+                let field = bx.load_operand(field_ptr);\n+                self.codegen_argument(bx, field, llargs, &args[i]);\n             }\n         } else if let Ref(_, Some(_), _) = tuple.val {\n             bug!(\"closure arguments must be sized\")\n@@ -872,7 +877,7 @@ impl<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>> FunctionCx<'a, 'tcx, Bx> {\n \n     fn get_personality_slot(\n         &mut self,\n-        bx: &Bx\n+        bx: &mut Bx\n     ) -> PlaceRef<'tcx, Bx::Value> {\n         let cx = bx.cx();\n         if let Some(slot) = self.personality_slot {\n@@ -920,9 +925,9 @@ impl<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>> FunctionCx<'a, 'tcx, Bx> {\n         let lp = bx.landing_pad(llretty, llpersonality, 1);\n         bx.set_cleanup(lp);\n \n-        let slot = self.get_personality_slot(&bx);\n-        slot.storage_live(&bx);\n-        Pair(bx.extract_value(lp, 0), bx.extract_value(lp, 1)).store(&bx, slot);\n+        let slot = self.get_personality_slot(&mut bx);\n+        slot.storage_live(&mut bx);\n+        Pair(bx.extract_value(lp, 0), bx.extract_value(lp, 1)).store(&mut bx, slot);\n \n         bx.br(target_bb);\n         bx.llbb()\n@@ -937,7 +942,7 @@ impl<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>> FunctionCx<'a, 'tcx, Bx> {\n         &mut self\n     ) -> Bx::BasicBlock {\n         self.unreachable_block.unwrap_or_else(|| {\n-            let bx = self.new_block(\"unreachable\");\n+            let mut bx = self.new_block(\"unreachable\");\n             bx.unreachable();\n             self.unreachable_block = Some(bx.llbb());\n             bx.llbb()\n@@ -959,7 +964,7 @@ impl<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>> FunctionCx<'a, 'tcx, Bx> {\n \n     fn make_return_dest(\n         &mut self,\n-        bx: &Bx,\n+        bx: &mut Bx,\n         dest: &mir::Place<'tcx>,\n         fn_ret: &ArgType<'tcx, Ty<'tcx>>,\n         llargs: &mut Vec<Bx::Value>, is_intrinsic: bool\n@@ -1019,7 +1024,7 @@ impl<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>> FunctionCx<'a, 'tcx, Bx> {\n \n     fn codegen_transmute(\n         &mut self,\n-        bx: &Bx,\n+        bx: &mut Bx,\n         src: &mir::Operand<'tcx>,\n         dst: &mir::Place<'tcx>\n     ) {\n@@ -1050,7 +1055,7 @@ impl<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>> FunctionCx<'a, 'tcx, Bx> {\n \n     fn codegen_transmute_into(\n         &mut self,\n-        bx: &Bx,\n+        bx: &mut Bx,\n         src: &mir::Operand<'tcx>,\n         dst: PlaceRef<'tcx, Bx::Value>\n     ) {\n@@ -1065,7 +1070,7 @@ impl<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>> FunctionCx<'a, 'tcx, Bx> {\n     // Stores the return value of a function call into it's final location.\n     fn store_return(\n         &mut self,\n-        bx: &Bx,\n+        bx: &mut Bx,\n         dest: ReturnDest<'tcx, Bx::Value>,\n         ret_ty: &ArgType<'tcx, Ty<'tcx>>,\n         llval: Bx::Value"}, {"sha": "9722e2f03b90164119e724ece4e27a64cd5c0ae7", "filename": "src/librustc_codegen_ssa/mir/mod.rs", "status": "modified", "additions": 13, "deletions": 9, "changes": 22, "blob_url": "https://github.com/rust-lang/rust/blob/54dd3a47fd54eb466dad7e47b41ec1b5b2dafd5a/src%2Flibrustc_codegen_ssa%2Fmir%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/54dd3a47fd54eb466dad7e47b41ec1b5b2dafd5a/src%2Flibrustc_codegen_ssa%2Fmir%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_codegen_ssa%2Fmir%2Fmod.rs?ref=54dd3a47fd54eb466dad7e47b41ec1b5b2dafd5a", "patch": "@@ -111,7 +111,7 @@ impl<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>> FunctionCx<'a, 'tcx, Bx> {\n \n     pub fn set_debug_loc(\n         &mut self,\n-        bx: &Bx,\n+        bx: &mut Bx,\n         source_info: mir::SourceInfo\n     ) {\n         let (scope, span) = self.debug_loc(source_info);\n@@ -264,7 +264,7 @@ pub fn codegen_mir<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>>(\n     fx.locals = {\n         let args = arg_local_refs(&mut bx, &fx, &fx.scopes, &memory_locals);\n \n-        let allocate_local = |local| {\n+        let mut allocate_local = |local| {\n             let decl = &mir.local_decls[local];\n             let layout = bx.cx().layout_of(fx.monomorphize(&decl.ty));\n             assert!(!layout.ty.has_erasable_regions());\n@@ -283,11 +283,11 @@ pub fn codegen_mir<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>>(\n                 debug!(\"alloc: {:?} ({}) -> place\", local, name);\n                 if layout.is_unsized() {\n                     let indirect_place =\n-                        PlaceRef::alloca_unsized_indirect(&bx, layout, &name.as_str());\n+                        PlaceRef::alloca_unsized_indirect(&mut bx, layout, &name.as_str());\n                     // FIXME: add an appropriate debuginfo\n                     LocalRef::UnsizedPlace(indirect_place)\n                 } else {\n-                    let place = PlaceRef::alloca(&bx, layout, &name.as_str());\n+                    let place = PlaceRef::alloca(&mut bx, layout, &name.as_str());\n                     if dbg {\n                         let (scope, span) = fx.debug_loc(mir::SourceInfo {\n                             span: decl.source_info.span,\n@@ -308,11 +308,14 @@ pub fn codegen_mir<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>>(\n                 } else if memory_locals.contains(local) {\n                     debug!(\"alloc: {:?} -> place\", local);\n                     if layout.is_unsized() {\n-                        let indirect_place =\n-                            PlaceRef::alloca_unsized_indirect(&bx, layout, &format!(\"{:?}\", local));\n+                        let indirect_place = PlaceRef::alloca_unsized_indirect(\n+                            &mut bx,\n+                            layout,\n+                            &format!(\"{:?}\", local),\n+                        );\n                         LocalRef::UnsizedPlace(indirect_place)\n                     } else {\n-                        LocalRef::Place(PlaceRef::alloca(&bx, layout, &format!(\"{:?}\", local)))\n+                        LocalRef::Place(PlaceRef::alloca(&mut bx, layout, &format!(\"{:?}\", local)))\n                     }\n                 } else {\n                     // If this is an immediate local, we do not create an\n@@ -399,7 +402,7 @@ fn create_funclets<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>>(\n             //          bar();\n             //      }\n             Some(&mir::TerminatorKind::Abort) => {\n-                let cs_bx = bx.build_sibling_block(&format!(\"cs_funclet{:?}\", bb));\n+                let mut cs_bx = bx.build_sibling_block(&format!(\"cs_funclet{:?}\", bb));\n                 let mut cp_bx = bx.build_sibling_block(&format!(\"cp_funclet{:?}\", bb));\n                 ret_llbb = cs_bx.llbb();\n \n@@ -480,7 +483,8 @@ fn arg_local_refs<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>>(\n                 if arg.pad.is_some() {\n                     llarg_idx += 1;\n                 }\n-                bx.store_fn_arg(arg, &mut llarg_idx, place.project_field(bx, i));\n+                let pr_field = place.project_field(bx, i);\n+                bx.store_fn_arg(arg, &mut llarg_idx, pr_field);\n             }\n \n             // Now that we have one alloca that contains the aggregate value,"}, {"sha": "10b1dad5002bde4c4e9de806df5aa6876a1e2a3f", "filename": "src/librustc_codegen_ssa/mir/operand.rs", "status": "modified", "additions": 23, "deletions": 18, "changes": 41, "blob_url": "https://github.com/rust-lang/rust/blob/54dd3a47fd54eb466dad7e47b41ec1b5b2dafd5a/src%2Flibrustc_codegen_ssa%2Fmir%2Foperand.rs", "raw_url": "https://github.com/rust-lang/rust/raw/54dd3a47fd54eb466dad7e47b41ec1b5b2dafd5a/src%2Flibrustc_codegen_ssa%2Fmir%2Foperand.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_codegen_ssa%2Fmir%2Foperand.rs?ref=54dd3a47fd54eb466dad7e47b41ec1b5b2dafd5a", "patch": "@@ -76,7 +76,7 @@ impl<'a, 'tcx: 'a, V: CodegenObject> OperandRef<'tcx, V> {\n     }\n \n     pub fn from_const<Bx: BuilderMethods<'a, 'tcx, Value = V>>(\n-        bx: &Bx,\n+        bx: &mut Bx,\n         val: &'tcx ty::Const<'tcx>\n     ) -> Result<Self, ErrorHandled> {\n         let layout = bx.cx().layout_of(val.ty);\n@@ -160,16 +160,18 @@ impl<'a, 'tcx: 'a, V: CodegenObject> OperandRef<'tcx, V> {\n     /// For other cases, see `immediate`.\n     pub fn immediate_or_packed_pair<Bx: BuilderMethods<'a, 'tcx, Value = V>>(\n         self,\n-        bx: &Bx\n+        bx: &mut Bx\n     ) -> V {\n         if let OperandValue::Pair(a, b) = self.val {\n             let llty = bx.cx().backend_type(self.layout);\n             debug!(\"Operand::immediate_or_packed_pair: packing {:?} into {:?}\",\n                    self, llty);\n             // Reconstruct the immediate aggregate.\n             let mut llpair = bx.cx().const_undef(llty);\n-            llpair = bx.insert_value(llpair, base::from_immediate(bx, a), 0);\n-            llpair = bx.insert_value(llpair, base::from_immediate(bx, b), 1);\n+            let imm_a = base::from_immediate(bx, a);\n+            let imm_b = base::from_immediate(bx, b);\n+            llpair = bx.insert_value(llpair, imm_a, 0);\n+            llpair = bx.insert_value(llpair, imm_b, 1);\n             llpair\n         } else {\n             self.immediate()\n@@ -178,7 +180,7 @@ impl<'a, 'tcx: 'a, V: CodegenObject> OperandRef<'tcx, V> {\n \n     /// If the type is a pair, we return a `Pair`, otherwise, an `Immediate`.\n     pub fn from_immediate_or_packed_pair<Bx: BuilderMethods<'a, 'tcx, Value = V>>(\n-        bx: &Bx,\n+        bx: &mut Bx,\n         llval: V,\n         layout: TyLayout<'tcx>\n     ) -> Self {\n@@ -187,8 +189,10 @@ impl<'a, 'tcx: 'a, V: CodegenObject> OperandRef<'tcx, V> {\n                     llval, layout);\n \n             // Deconstruct the immediate aggregate.\n-            let a_llval = base::to_immediate_scalar(bx, bx.extract_value(llval, 0), a);\n-            let b_llval = base::to_immediate_scalar(bx, bx.extract_value(llval, 1), b);\n+            let a_llval = bx.extract_value(llval, 0);\n+            let a_llval = base::to_immediate_scalar(bx, a_llval, a);\n+            let b_llval = bx.extract_value(llval, 1);\n+            let b_llval = base::to_immediate_scalar(bx, b_llval, b);\n             OperandValue::Pair(a_llval, b_llval)\n         } else {\n             OperandValue::Immediate(llval)\n@@ -198,7 +202,7 @@ impl<'a, 'tcx: 'a, V: CodegenObject> OperandRef<'tcx, V> {\n \n     pub fn extract_field<Bx: BuilderMethods<'a, 'tcx, Value = V>>(\n         &self,\n-        bx: &Bx,\n+        bx: &mut Bx,\n         i: usize\n     ) -> Self {\n         let field = self.layout.field(bx.cx(), i);\n@@ -261,39 +265,39 @@ impl<'a, 'tcx: 'a, V: CodegenObject> OperandRef<'tcx, V> {\n impl<'a, 'tcx: 'a, V: CodegenObject> OperandValue<V> {\n     pub fn store<Bx: BuilderMethods<'a, 'tcx, Value = V>>(\n         self,\n-        bx: &Bx,\n+        bx: &mut Bx,\n         dest: PlaceRef<'tcx, V>\n     ) {\n         self.store_with_flags(bx, dest, MemFlags::empty());\n     }\n \n     pub fn volatile_store<Bx: BuilderMethods<'a, 'tcx, Value = V>>(\n         self,\n-        bx: &Bx,\n+        bx: &mut Bx,\n         dest: PlaceRef<'tcx, V>\n     ) {\n         self.store_with_flags(bx, dest, MemFlags::VOLATILE);\n     }\n \n     pub fn unaligned_volatile_store<Bx: BuilderMethods<'a, 'tcx, Value = V>>(\n         self,\n-        bx: &Bx,\n+        bx: &mut Bx,\n         dest: PlaceRef<'tcx, V>,\n     ) {\n         self.store_with_flags(bx, dest, MemFlags::VOLATILE | MemFlags::UNALIGNED);\n     }\n \n     pub fn nontemporal_store<Bx: BuilderMethods<'a, 'tcx, Value = V>>(\n         self,\n-        bx: &Bx,\n+        bx: &mut Bx,\n         dest: PlaceRef<'tcx, V>\n     ) {\n         self.store_with_flags(bx, dest, MemFlags::NONTEMPORAL);\n     }\n \n     fn store_with_flags<Bx: BuilderMethods<'a, 'tcx, Value = V>>(\n         self,\n-        bx: &Bx,\n+        bx: &mut Bx,\n         dest: PlaceRef<'tcx, V>,\n         flags: MemFlags,\n     ) {\n@@ -326,7 +330,7 @@ impl<'a, 'tcx: 'a, V: CodegenObject> OperandValue<V> {\n     }\n     pub fn store_unsized<Bx: BuilderMethods<'a, 'tcx, Value = V>>(\n         self,\n-        bx: &Bx,\n+        bx: &mut Bx,\n         indirect_dest: PlaceRef<'tcx, V>\n     ) {\n         debug!(\"OperandRef::store_unsized: operand={:?}, indirect_dest={:?}\", self, indirect_dest);\n@@ -361,7 +365,7 @@ impl<'a, 'tcx: 'a, V: CodegenObject> OperandValue<V> {\n impl<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>> FunctionCx<'a, 'tcx, Bx> {\n     fn maybe_codegen_consume_direct(\n         &mut self,\n-        bx: &Bx,\n+        bx: &mut Bx,\n         place: &mir::Place<'tcx>\n     ) -> Option<OperandRef<'tcx, Bx::Value>> {\n         debug!(\"maybe_codegen_consume_direct(place={:?})\", place);\n@@ -409,7 +413,7 @@ impl<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>> FunctionCx<'a, 'tcx, Bx> {\n \n     pub fn codegen_consume(\n         &mut self,\n-        bx: &Bx,\n+        bx: &mut Bx,\n         place: &mir::Place<'tcx>\n     ) -> OperandRef<'tcx, Bx::Value> {\n         debug!(\"codegen_consume(place={:?})\", place);\n@@ -428,12 +432,13 @@ impl<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>> FunctionCx<'a, 'tcx, Bx> {\n \n         // for most places, to consume them we just load them\n         // out from their home\n-        bx.load_operand(self.codegen_place(bx, place))\n+        let place = self.codegen_place(bx, place);\n+        bx.load_operand(place)\n     }\n \n     pub fn codegen_operand(\n         &mut self,\n-        bx: &Bx,\n+        bx: &mut Bx,\n         operand: &mir::Operand<'tcx>\n     ) -> OperandRef<'tcx, Bx::Value> {\n         debug!(\"codegen_operand(operand={:?})\", operand);"}, {"sha": "39574f0c2a91307091affbb8d9a7c0ef9fb2df60", "filename": "src/librustc_codegen_ssa/mir/place.rs", "status": "modified", "additions": 30, "deletions": 27, "changes": 57, "blob_url": "https://github.com/rust-lang/rust/blob/54dd3a47fd54eb466dad7e47b41ec1b5b2dafd5a/src%2Flibrustc_codegen_ssa%2Fmir%2Fplace.rs", "raw_url": "https://github.com/rust-lang/rust/raw/54dd3a47fd54eb466dad7e47b41ec1b5b2dafd5a/src%2Flibrustc_codegen_ssa%2Fmir%2Fplace.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_codegen_ssa%2Fmir%2Fplace.rs?ref=54dd3a47fd54eb466dad7e47b41ec1b5b2dafd5a", "patch": "@@ -52,7 +52,7 @@ impl<'a, 'tcx: 'a, V: CodegenObject> PlaceRef<'tcx, V> {\n     }\n \n     pub fn alloca<Bx: BuilderMethods<'a, 'tcx, Value = V>>(\n-        bx: &Bx,\n+        bx: &mut Bx,\n         layout: TyLayout<'tcx>,\n         name: &str\n     ) -> Self {\n@@ -64,7 +64,7 @@ impl<'a, 'tcx: 'a, V: CodegenObject> PlaceRef<'tcx, V> {\n \n     /// Returns a place for an indirect reference to an unsized place.\n     pub fn alloca_unsized_indirect<Bx: BuilderMethods<'a, 'tcx, Value = V>>(\n-        bx: &Bx,\n+        bx: &mut Bx,\n         layout: TyLayout<'tcx>,\n         name: &str,\n     ) -> Self {\n@@ -96,29 +96,28 @@ impl<'a, 'tcx: 'a, V: CodegenObject> PlaceRef<'tcx, V> {\n impl<'a, 'tcx: 'a, V: CodegenObject> PlaceRef<'tcx, V> {\n     /// Access a field, at a point when the value's case is known.\n     pub fn project_field<Bx: BuilderMethods<'a, 'tcx, Value = V>>(\n-        self, bx: &Bx,\n+        self, bx: &mut Bx,\n         ix: usize,\n     ) -> Self {\n-        let cx = bx.cx();\n-        let field = self.layout.field(cx, ix);\n+        let field = self.layout.field(bx.cx(), ix);\n         let offset = self.layout.fields.offset(ix);\n         let effective_field_align = self.align.restrict_for_offset(offset);\n \n-        let simple = || {\n+        let mut simple = || {\n             // Unions and newtypes only use an offset of 0.\n             let llval = if offset.bytes() == 0 {\n                 self.llval\n             } else if let layout::Abi::ScalarPair(ref a, ref b) = self.layout.abi {\n                 // Offsets have to match either first or second field.\n-                assert_eq!(offset, a.value.size(cx).abi_align(b.value.align(cx)));\n+                assert_eq!(offset, a.value.size(bx.cx()).abi_align(b.value.align(bx.cx())));\n                 bx.struct_gep(self.llval, 1)\n             } else {\n                 bx.struct_gep(self.llval, bx.cx().backend_field_index(self.layout, ix))\n             };\n             PlaceRef {\n                 // HACK(eddyb) have to bitcast pointers until LLVM removes pointee types.\n-                llval: bx.pointercast(llval, cx.type_ptr_to(cx.backend_type(field))),\n-                llextra: if cx.type_has_metadata(field.ty) {\n+                llval: bx.pointercast(llval, bx.cx().type_ptr_to(bx.cx().backend_type(field))),\n+                llextra: if bx.cx().type_has_metadata(field.ty) {\n                     self.llextra\n                 } else {\n                     None\n@@ -168,7 +167,7 @@ impl<'a, 'tcx: 'a, V: CodegenObject> PlaceRef<'tcx, V> {\n \n         let meta = self.llextra;\n \n-        let unaligned_offset = cx.const_usize(offset.bytes());\n+        let unaligned_offset = bx.cx().const_usize(offset.bytes());\n \n         // Get the alignment of the field\n         let (_, unsized_align) = glue::size_and_align_of_dst(bx, field.ty, meta);\n@@ -179,18 +178,19 @@ impl<'a, 'tcx: 'a, V: CodegenObject> PlaceRef<'tcx, V> {\n         //   (unaligned offset + (align - 1)) & -align\n \n         // Calculate offset\n-        let align_sub_1 = bx.sub(unsized_align, cx.const_usize(1u64));\n-        let offset = bx.and(bx.add(unaligned_offset, align_sub_1),\n-        bx.neg(unsized_align));\n+        let align_sub_1 = bx.sub(unsized_align, bx.cx().const_usize(1u64));\n+        let and_lhs = bx.add(unaligned_offset, align_sub_1);\n+        let and_rhs = bx.neg(unsized_align);\n+        let offset = bx.and(and_lhs, and_rhs);\n \n         debug!(\"struct_field_ptr: DST field offset: {:?}\", offset);\n \n         // Cast and adjust pointer\n-        let byte_ptr = bx.pointercast(self.llval, cx.type_i8p());\n+        let byte_ptr = bx.pointercast(self.llval, bx.cx().type_i8p());\n         let byte_ptr = bx.gep(byte_ptr, &[offset]);\n \n         // Finally, cast back to the type expected\n-        let ll_fty = cx.backend_type(field);\n+        let ll_fty = bx.cx().backend_type(field);\n         debug!(\"struct_field_ptr: Field type is {:?}\", ll_fty);\n \n         PlaceRef {\n@@ -204,7 +204,7 @@ impl<'a, 'tcx: 'a, V: CodegenObject> PlaceRef<'tcx, V> {\n     /// Obtain the actual discriminant of a value.\n     pub fn codegen_get_discr<Bx: BuilderMethods<'a, 'tcx, Value = V>>(\n         self,\n-        bx: &Bx,\n+        bx: &mut Bx,\n         cast_to: Ty<'tcx>\n     ) -> V {\n         let cast_to = bx.cx().immediate_backend_type(bx.cx().layout_of(cast_to));\n@@ -252,7 +252,8 @@ impl<'a, 'tcx: 'a, V: CodegenObject> PlaceRef<'tcx, V> {\n                     } else {\n                         bx.cx().const_uint_big(niche_llty, niche_start)\n                     };\n-                    bx.select(bx.icmp(IntPredicate::IntEQ, lldiscr, niche_llval),\n+                    let select_arg = bx.icmp(IntPredicate::IntEQ, lldiscr, niche_llval);\n+                    bx.select(select_arg,\n                         bx.cx().const_uint(cast_to, niche_variants.start().as_u32() as u64),\n                         bx.cx().const_uint(cast_to, dataful_variant.as_u32() as u64))\n                 } else {\n@@ -261,8 +262,10 @@ impl<'a, 'tcx: 'a, V: CodegenObject> PlaceRef<'tcx, V> {\n                     let lldiscr = bx.sub(lldiscr, bx.cx().const_uint_big(niche_llty, delta));\n                     let lldiscr_max =\n                         bx.cx().const_uint(niche_llty, niche_variants.end().as_u32() as u64);\n-                    bx.select(bx.icmp(IntPredicate::IntULE, lldiscr, lldiscr_max),\n-                        bx.intcast(lldiscr, cast_to, false),\n+                    let select_arg = bx.icmp(IntPredicate::IntULE, lldiscr, lldiscr_max);\n+                    let cast = bx.intcast(lldiscr, cast_to, false);\n+                    bx.select(select_arg,\n+                        cast,\n                         bx.cx().const_uint(cast_to, dataful_variant.as_u32() as u64))\n                 }\n             }\n@@ -273,7 +276,7 @@ impl<'a, 'tcx: 'a, V: CodegenObject> PlaceRef<'tcx, V> {\n     /// representation.\n     pub fn codegen_set_discr<Bx: BuilderMethods<'a, 'tcx, Value = V>>(\n         &self,\n-        bx: &Bx,\n+        bx: &mut Bx,\n         variant_index: VariantIdx\n     ) {\n         if self.layout.for_variant(bx.cx(), variant_index).abi.is_uninhabited() {\n@@ -330,7 +333,7 @@ impl<'a, 'tcx: 'a, V: CodegenObject> PlaceRef<'tcx, V> {\n \n     pub fn project_index<Bx: BuilderMethods<'a, 'tcx, Value = V>>(\n         &self,\n-        bx: &Bx,\n+        bx: &mut Bx,\n         llindex: V\n     ) -> Self {\n         PlaceRef {\n@@ -343,7 +346,7 @@ impl<'a, 'tcx: 'a, V: CodegenObject> PlaceRef<'tcx, V> {\n \n     pub fn project_downcast<Bx: BuilderMethods<'a, 'tcx, Value = V>>(\n         &self,\n-        bx: &Bx,\n+        bx: &mut Bx,\n         variant_index: VariantIdx\n     ) -> Self {\n         let mut downcast = *self;\n@@ -356,25 +359,25 @@ impl<'a, 'tcx: 'a, V: CodegenObject> PlaceRef<'tcx, V> {\n         downcast\n     }\n \n-    pub fn storage_live<Bx: BuilderMethods<'a, 'tcx, Value = V>>(&self, bx: &Bx) {\n+    pub fn storage_live<Bx: BuilderMethods<'a, 'tcx, Value = V>>(&self, bx: &mut Bx) {\n         bx.lifetime_start(self.llval, self.layout.size);\n     }\n \n-    pub fn storage_dead<Bx: BuilderMethods<'a, 'tcx, Value = V>>(&self, bx: &Bx) {\n+    pub fn storage_dead<Bx: BuilderMethods<'a, 'tcx, Value = V>>(&self, bx: &mut Bx) {\n         bx.lifetime_end(self.llval, self.layout.size);\n     }\n }\n \n impl<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>> FunctionCx<'a, 'tcx, Bx> {\n     pub fn codegen_place(\n         &mut self,\n-        bx: &Bx,\n+        bx: &mut Bx,\n         place: &mir::Place<'tcx>\n     ) -> PlaceRef<'tcx, Bx::Value> {\n         debug!(\"codegen_place(place={:?})\", place);\n \n-        let cx = bx.cx();\n-        let tcx = cx.tcx();\n+        let cx = self.cx;\n+        let tcx = self.cx.tcx();\n \n         if let mir::Place::Local(index) = *place {\n             match self.locals[index] {"}, {"sha": "2cc7ed12550d83036a00af1cbdbe331572948615", "filename": "src/librustc_codegen_ssa/mir/rvalue.rs", "status": "modified", "additions": 97, "deletions": 92, "changes": 189, "blob_url": "https://github.com/rust-lang/rust/blob/54dd3a47fd54eb466dad7e47b41ec1b5b2dafd5a/src%2Flibrustc_codegen_ssa%2Fmir%2Frvalue.rs", "raw_url": "https://github.com/rust-lang/rust/raw/54dd3a47fd54eb466dad7e47b41ec1b5b2dafd5a/src%2Flibrustc_codegen_ssa%2Fmir%2Frvalue.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_codegen_ssa%2Fmir%2Frvalue.rs?ref=54dd3a47fd54eb466dad7e47b41ec1b5b2dafd5a", "patch": "@@ -40,10 +40,10 @@ impl<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>> FunctionCx<'a, 'tcx, Bx> {\n \n         match *rvalue {\n            mir::Rvalue::Use(ref operand) => {\n-               let cg_operand = self.codegen_operand(&bx, operand);\n+               let cg_operand = self.codegen_operand(&mut bx, operand);\n                // FIXME: consider not copying constants through stack. (fixable by codegenning\n                // constants into OperandValue::Ref, why don\u2019t we do that yet if we don\u2019t?)\n-               cg_operand.val.store(&bx, dest);\n+               cg_operand.val.store(&mut bx, dest);\n                bx\n            }\n \n@@ -53,16 +53,16 @@ impl<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>> FunctionCx<'a, 'tcx, Bx> {\n                 if bx.cx().is_backend_scalar_pair(dest.layout) {\n                     // into-coerce of a thin pointer to a fat pointer - just\n                     // use the operand path.\n-                    let (bx, temp) = self.codegen_rvalue_operand(bx, rvalue);\n-                    temp.val.store(&bx, dest);\n+                    let (mut bx, temp) = self.codegen_rvalue_operand(bx, rvalue);\n+                    temp.val.store(&mut bx, dest);\n                     return bx;\n                 }\n \n                 // Unsize of a nontrivial struct. I would prefer for\n                 // this to be eliminated by MIR building, but\n                 // `CoerceUnsized` can be passed by a where-clause,\n                 // so the (generic) MIR may not be able to expand it.\n-                let operand = self.codegen_operand(&bx, source);\n+                let operand = self.codegen_operand(&mut bx, source);\n                 match operand.val {\n                     OperandValue::Pair(..) |\n                     OperandValue::Immediate(_) => {\n@@ -73,15 +73,15 @@ impl<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>> FunctionCx<'a, 'tcx, Bx> {\n                         // index into the struct, and this case isn't\n                         // important enough for it.\n                         debug!(\"codegen_rvalue: creating ugly alloca\");\n-                        let scratch = PlaceRef::alloca(&bx, operand.layout, \"__unsize_temp\");\n-                        scratch.storage_live(&bx);\n-                        operand.val.store(&bx, scratch);\n-                        base::coerce_unsized_into(&bx, scratch, dest);\n-                        scratch.storage_dead(&bx);\n+                        let scratch = PlaceRef::alloca(&mut bx, operand.layout, \"__unsize_temp\");\n+                        scratch.storage_live(&mut bx);\n+                        operand.val.store(&mut bx, scratch);\n+                        base::coerce_unsized_into(&mut bx, scratch, dest);\n+                        scratch.storage_dead(&mut bx);\n                     }\n                     OperandValue::Ref(llref, None, align) => {\n                         let source = PlaceRef::new_sized(llref, operand.layout, align);\n-                        base::coerce_unsized_into(&bx, source, dest);\n+                        base::coerce_unsized_into(&mut bx, source, dest);\n                     }\n                     OperandValue::Ref(_, Some(_), _) => {\n                         bug!(\"unsized coercion on an unsized rvalue\")\n@@ -91,14 +91,14 @@ impl<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>> FunctionCx<'a, 'tcx, Bx> {\n             }\n \n             mir::Rvalue::Repeat(ref elem, count) => {\n-                let cg_elem = self.codegen_operand(&bx, elem);\n+                let cg_elem = self.codegen_operand(&mut bx, elem);\n \n                 // Do not generate the loop for zero-sized elements or empty arrays.\n                 if dest.layout.is_zst() {\n                     return bx;\n                 }\n-\n-                let start = dest.project_index(&bx, bx.cx().const_usize(0)).llval;\n+                let zero = bx.cx().const_usize(0);\n+                let start = dest.project_index(&mut bx, zero).llval;\n \n                 if let OperandValue::Immediate(v) = cg_elem.val {\n                     let size = bx.cx().const_usize(dest.layout.size.bytes());\n@@ -111,15 +111,15 @@ impl<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>> FunctionCx<'a, 'tcx, Bx> {\n                     }\n \n                     // Use llvm.memset.p0i8.* to initialize byte arrays\n-                    let v = base::from_immediate(&bx, v);\n+                    let v = base::from_immediate(&mut bx, v);\n                     if bx.cx().val_ty(v) == bx.cx().type_i8() {\n                         bx.memset(start, v, size, dest.align, MemFlags::empty());\n                         return bx;\n                     }\n                 }\n \n                 let count = bx.cx().const_usize(count);\n-                let end = dest.project_index(&bx, count).llval;\n+                let end = dest.project_index(&mut bx, count).llval;\n \n                 let mut header_bx = bx.build_sibling_block(\"repeat_loop_header\");\n                 let mut body_bx = bx.build_sibling_block(\"repeat_loop_body\");\n@@ -131,7 +131,7 @@ impl<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>> FunctionCx<'a, 'tcx, Bx> {\n                 let keep_going = header_bx.icmp(IntPredicate::IntNE, current, end);\n                 header_bx.cond_br(keep_going, body_bx.llbb(), next_bx.llbb());\n \n-                cg_elem.val.store(&body_bx,\n+                cg_elem.val.store(&mut body_bx,\n                     PlaceRef::new_sized(current, cg_elem.layout, dest.align));\n \n                 let next = body_bx.inbounds_gep(current, &[bx.cx().const_usize(1)]);\n@@ -144,38 +144,39 @@ impl<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>> FunctionCx<'a, 'tcx, Bx> {\n             mir::Rvalue::Aggregate(ref kind, ref operands) => {\n                 let (dest, active_field_index) = match **kind {\n                     mir::AggregateKind::Adt(adt_def, variant_index, _, _, active_field_index) => {\n-                        dest.codegen_set_discr(&bx, variant_index);\n+                        dest.codegen_set_discr(&mut bx, variant_index);\n                         if adt_def.is_enum() {\n-                            (dest.project_downcast(&bx, variant_index), active_field_index)\n+                            (dest.project_downcast(&mut bx, variant_index), active_field_index)\n                         } else {\n                             (dest, active_field_index)\n                         }\n                     }\n                     _ => (dest, None)\n                 };\n                 for (i, operand) in operands.iter().enumerate() {\n-                    let op = self.codegen_operand(&bx, operand);\n+                    let op = self.codegen_operand(&mut bx, operand);\n                     // Do not generate stores and GEPis for zero-sized fields.\n                     if !op.layout.is_zst() {\n                         let field_index = active_field_index.unwrap_or(i);\n-                        op.val.store(&bx, dest.project_field(&bx, field_index));\n+                        let field = dest.project_field(&mut bx, field_index);\n+                        op.val.store(&mut bx, field);\n                     }\n                 }\n                 bx\n             }\n \n             _ => {\n                 assert!(self.rvalue_creates_operand(rvalue));\n-                let (bx, temp) = self.codegen_rvalue_operand(bx, rvalue);\n-                temp.val.store(&bx, dest);\n+                let (mut bx, temp) = self.codegen_rvalue_operand(bx, rvalue);\n+                temp.val.store(&mut bx, dest);\n                 bx\n             }\n         }\n     }\n \n     pub fn codegen_rvalue_unsized(\n         &mut self,\n-        bx: Bx,\n+        mut bx: Bx,\n         indirect_dest: PlaceRef<'tcx, Bx::Value>,\n         rvalue: &mir::Rvalue<'tcx>,\n     ) -> Bx {\n@@ -184,8 +185,8 @@ impl<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>> FunctionCx<'a, 'tcx, Bx> {\n \n         match *rvalue {\n             mir::Rvalue::Use(ref operand) => {\n-                let cg_operand = self.codegen_operand(&bx, operand);\n-                cg_operand.val.store_unsized(&bx, indirect_dest);\n+                let cg_operand = self.codegen_operand(&mut bx, operand);\n+                cg_operand.val.store_unsized(&mut bx, indirect_dest);\n                 bx\n             }\n \n@@ -195,14 +196,14 @@ impl<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>> FunctionCx<'a, 'tcx, Bx> {\n \n     pub fn codegen_rvalue_operand(\n         &mut self,\n-        bx: Bx,\n+        mut bx: Bx,\n         rvalue: &mir::Rvalue<'tcx>\n     ) -> (Bx, OperandRef<'tcx, Bx::Value>) {\n         assert!(self.rvalue_creates_operand(rvalue), \"cannot codegen {:?} to operand\", rvalue);\n \n         match *rvalue {\n             mir::Rvalue::Cast(ref kind, ref source, mir_cast_ty) => {\n-                let operand = self.codegen_operand(&bx, source);\n+                let operand = self.codegen_operand(&mut bx, source);\n                 debug!(\"cast operand is {:?}\", operand);\n                 let cast = bx.cx().layout_of(self.monomorphize(&mir_cast_ty));\n \n@@ -255,7 +256,7 @@ impl<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>> FunctionCx<'a, 'tcx, Bx> {\n                             }\n                             OperandValue::Immediate(lldata) => {\n                                 // \"standard\" unsize\n-                                let (lldata, llextra) = base::unsize_thin_ptr(&bx, lldata,\n+                                let (lldata, llextra) = base::unsize_thin_ptr(&mut bx, lldata,\n                                     operand.layout.ty, cast.ty);\n                                 OperandValue::Pair(lldata, llextra)\n                             }\n@@ -329,12 +330,14 @@ impl<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>> FunctionCx<'a, 'tcx, Bx> {\n                                     // We want `table[e as usize]` to not\n                                     // have bound checks, and this is the most\n                                     // convenient place to put the `assume`.\n-\n-                                    base::call_assume(&bx, bx.icmp(\n+                                    let ll_t_in_const =\n+                                        bx.cx().const_uint_big(ll_t_in, *scalar.valid_range.end());\n+                                    let cmp = bx.icmp(\n                                         IntPredicate::IntULE,\n                                         llval,\n-                                        bx.cx().const_uint_big(ll_t_in, *scalar.valid_range.end())\n-                                    ));\n+                                        ll_t_in_const\n+                                    );\n+                                    base::call_assume(&mut bx, cmp);\n                                 }\n                             }\n                         }\n@@ -366,11 +369,11 @@ impl<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>> FunctionCx<'a, 'tcx, Bx> {\n                                 bx.inttoptr(usize_llval, ll_t_out)\n                             }\n                             (CastTy::Int(_), CastTy::Float) =>\n-                                cast_int_to_float(&bx, signed, llval, ll_t_in, ll_t_out),\n+                                cast_int_to_float(&mut bx, signed, llval, ll_t_in, ll_t_out),\n                             (CastTy::Float, CastTy::Int(IntTy::I)) =>\n-                                cast_float_to_int(&bx, true, llval, ll_t_in, ll_t_out),\n+                                cast_float_to_int(&mut bx, true, llval, ll_t_in, ll_t_out),\n                             (CastTy::Float, CastTy::Int(_)) =>\n-                                cast_float_to_int(&bx, false, llval, ll_t_in, ll_t_out),\n+                                cast_float_to_int(&mut bx, false, llval, ll_t_in, ll_t_out),\n                             _ => bug!(\"unsupported cast: {:?} to {:?}\", operand.layout.ty, cast.ty)\n                         };\n                         OperandValue::Immediate(newval)\n@@ -383,7 +386,7 @@ impl<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>> FunctionCx<'a, 'tcx, Bx> {\n             }\n \n             mir::Rvalue::Ref(_, bk, ref place) => {\n-                let cg_place = self.codegen_place(&bx, place);\n+                let cg_place = self.codegen_place(&mut bx, place);\n \n                 let ty = cg_place.layout.ty;\n \n@@ -404,7 +407,7 @@ impl<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>> FunctionCx<'a, 'tcx, Bx> {\n             }\n \n             mir::Rvalue::Len(ref place) => {\n-                let size = self.evaluate_array_len(&bx, place);\n+                let size = self.evaluate_array_len(&mut bx, place);\n                 let operand = OperandRef {\n                     val: OperandValue::Immediate(size),\n                     layout: bx.cx().layout_of(bx.tcx().types.usize),\n@@ -413,20 +416,20 @@ impl<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>> FunctionCx<'a, 'tcx, Bx> {\n             }\n \n             mir::Rvalue::BinaryOp(op, ref lhs, ref rhs) => {\n-                let lhs = self.codegen_operand(&bx, lhs);\n-                let rhs = self.codegen_operand(&bx, rhs);\n+                let lhs = self.codegen_operand(&mut bx, lhs);\n+                let rhs = self.codegen_operand(&mut bx, rhs);\n                 let llresult = match (lhs.val, rhs.val) {\n                     (OperandValue::Pair(lhs_addr, lhs_extra),\n                      OperandValue::Pair(rhs_addr, rhs_extra)) => {\n-                        self.codegen_fat_ptr_binop(&bx, op,\n+                        self.codegen_fat_ptr_binop(&mut bx, op,\n                                                  lhs_addr, lhs_extra,\n                                                  rhs_addr, rhs_extra,\n                                                  lhs.layout.ty)\n                     }\n \n                     (OperandValue::Immediate(lhs_val),\n                      OperandValue::Immediate(rhs_val)) => {\n-                        self.codegen_scalar_binop(&bx, op, lhs_val, rhs_val, lhs.layout.ty)\n+                        self.codegen_scalar_binop(&mut bx, op, lhs_val, rhs_val, lhs.layout.ty)\n                     }\n \n                     _ => bug!()\n@@ -439,9 +442,9 @@ impl<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>> FunctionCx<'a, 'tcx, Bx> {\n                 (bx, operand)\n             }\n             mir::Rvalue::CheckedBinaryOp(op, ref lhs, ref rhs) => {\n-                let lhs = self.codegen_operand(&bx, lhs);\n-                let rhs = self.codegen_operand(&bx, rhs);\n-                let result = self.codegen_scalar_checked_binop(&bx, op,\n+                let lhs = self.codegen_operand(&mut bx, lhs);\n+                let rhs = self.codegen_operand(&mut bx, rhs);\n+                let result = self.codegen_scalar_checked_binop(&mut bx, op,\n                                                              lhs.immediate(), rhs.immediate(),\n                                                              lhs.layout.ty);\n                 let val_ty = op.ty(bx.tcx(), lhs.layout.ty, rhs.layout.ty);\n@@ -455,7 +458,7 @@ impl<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>> FunctionCx<'a, 'tcx, Bx> {\n             }\n \n             mir::Rvalue::UnaryOp(op, ref operand) => {\n-                let operand = self.codegen_operand(&bx, operand);\n+                let operand = self.codegen_operand(&mut bx, operand);\n                 let lloperand = operand.immediate();\n                 let is_float = operand.layout.ty.is_fp();\n                 let llval = match op {\n@@ -474,8 +477,8 @@ impl<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>> FunctionCx<'a, 'tcx, Bx> {\n \n             mir::Rvalue::Discriminant(ref place) => {\n                 let discr_ty = rvalue.ty(&*self.mir, bx.tcx());\n-                let discr =  self.codegen_place(&bx, place)\n-                    .codegen_get_discr(&bx, discr_ty);\n+                let discr =  self.codegen_place(&mut bx, place)\n+                    .codegen_get_discr(&mut bx, discr_ty);\n                 (bx, OperandRef {\n                     val: OperandValue::Immediate(discr),\n                     layout: self.cx.layout_of(discr_ty)\n@@ -509,7 +512,8 @@ impl<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>> FunctionCx<'a, 'tcx, Bx> {\n                 };\n                 let instance = ty::Instance::mono(bx.tcx(), def_id);\n                 let r = bx.cx().get_fn(instance);\n-                let val = bx.pointercast(bx.call(r, &[llsize, llalign], None), llty_ptr);\n+                let call = bx.call(r, &[llsize, llalign], None);\n+                let val = bx.pointercast(call, llty_ptr);\n \n                 let operand = OperandRef {\n                     val: OperandValue::Immediate(val),\n@@ -518,7 +522,7 @@ impl<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>> FunctionCx<'a, 'tcx, Bx> {\n                 (bx, operand)\n             }\n             mir::Rvalue::Use(ref operand) => {\n-                let operand = self.codegen_operand(&bx, operand);\n+                let operand = self.codegen_operand(&mut bx, operand);\n                 (bx, operand)\n             }\n             mir::Rvalue::Repeat(..) |\n@@ -534,7 +538,7 @@ impl<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>> FunctionCx<'a, 'tcx, Bx> {\n \n     fn evaluate_array_len(\n         &mut self,\n-        bx: &Bx,\n+        bx: &mut Bx,\n         place: &mir::Place<'tcx>,\n     ) -> Bx::Value {\n         // ZST are passed as operands and require special handling\n@@ -554,7 +558,7 @@ impl<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>> FunctionCx<'a, 'tcx, Bx> {\n \n     pub fn codegen_scalar_binop(\n         &mut self,\n-        bx: &Bx,\n+        bx: &mut Bx,\n         op: mir::BinOp,\n         lhs: Bx::Value,\n         rhs: Bx::Value,\n@@ -622,7 +626,7 @@ impl<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>> FunctionCx<'a, 'tcx, Bx> {\n \n     pub fn codegen_fat_ptr_binop(\n         &mut self,\n-        bx: &Bx,\n+        bx: &mut Bx,\n         op: mir::BinOp,\n         lhs_addr: Bx::Value,\n         lhs_extra: Bx::Value,\n@@ -632,16 +636,14 @@ impl<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>> FunctionCx<'a, 'tcx, Bx> {\n     ) -> Bx::Value {\n         match op {\n             mir::BinOp::Eq => {\n-                bx.and(\n-                    bx.icmp(IntPredicate::IntEQ, lhs_addr, rhs_addr),\n-                    bx.icmp(IntPredicate::IntEQ, lhs_extra, rhs_extra)\n-                )\n+                let lhs = bx.icmp(IntPredicate::IntEQ, lhs_addr, rhs_addr);\n+                let rhs = bx.icmp(IntPredicate::IntEQ, lhs_extra, rhs_extra);\n+                bx.and(lhs, rhs)\n             }\n             mir::BinOp::Ne => {\n-                bx.or(\n-                    bx.icmp(IntPredicate::IntNE, lhs_addr, rhs_addr),\n-                    bx.icmp(IntPredicate::IntNE, lhs_extra, rhs_extra)\n-                )\n+                let lhs = bx.icmp(IntPredicate::IntNE, lhs_addr, rhs_addr);\n+                let rhs = bx.icmp(IntPredicate::IntNE, lhs_extra, rhs_extra);\n+                bx.or(lhs, rhs)\n             }\n             mir::BinOp::Le | mir::BinOp::Lt |\n             mir::BinOp::Ge | mir::BinOp::Gt => {\n@@ -653,14 +655,11 @@ impl<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>> FunctionCx<'a, 'tcx, Bx> {\n                     mir::BinOp::Ge => (IntPredicate::IntUGE, IntPredicate::IntUGT),\n                     _ => bug!(),\n                 };\n-\n-                bx.or(\n-                    bx.icmp(strict_op, lhs_addr, rhs_addr),\n-                    bx.and(\n-                        bx.icmp(IntPredicate::IntEQ, lhs_addr, rhs_addr),\n-                        bx.icmp(op, lhs_extra, rhs_extra)\n-                    )\n-                )\n+                let lhs = bx.icmp(strict_op, lhs_addr, rhs_addr);\n+                let and_lhs = bx.icmp(IntPredicate::IntEQ, lhs_addr, rhs_addr);\n+                let and_rhs = bx.icmp(op, lhs_extra, rhs_extra);\n+                let rhs = bx.and(and_lhs, and_rhs);\n+                bx.or(lhs, rhs)\n             }\n             _ => {\n                 bug!(\"unexpected fat ptr binop\");\n@@ -670,7 +669,7 @@ impl<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>> FunctionCx<'a, 'tcx, Bx> {\n \n     pub fn codegen_scalar_checked_binop(\n         &mut self,\n-        bx: &Bx,\n+        bx: &mut Bx,\n         op: mir::BinOp,\n         lhs: Bx::Value,\n         rhs: Bx::Value,\n@@ -752,7 +751,7 @@ enum OverflowOp {\n \n fn get_overflow_intrinsic<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>>(\n     oop: OverflowOp,\n-    bx: &Bx,\n+    bx: &mut Bx,\n     ty: Ty\n ) -> Bx::Value {\n     use syntax::ast::IntTy::*;\n@@ -820,7 +819,7 @@ fn get_overflow_intrinsic<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>>(\n }\n \n fn cast_int_to_float<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>>(\n-    bx: &Bx,\n+    bx: &mut Bx,\n     signed: bool,\n     x: Bx::Value,\n     int_ty: Bx::Type,\n@@ -843,7 +842,8 @@ fn cast_int_to_float<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>>(\n         let overflow = bx.icmp(IntPredicate::IntUGE, x, max);\n         let infinity_bits = bx.cx().const_u32(ieee::Single::INFINITY.to_bits() as u32);\n         let infinity = bx.bitcast(infinity_bits, float_ty);\n-        bx.select(overflow, infinity, bx.uitofp(x, float_ty))\n+        let fp = bx.uitofp(x, float_ty);\n+        bx.select(overflow, infinity, fp)\n     } else {\n         if signed {\n             bx.sitofp(x, float_ty)\n@@ -854,7 +854,7 @@ fn cast_int_to_float<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>>(\n }\n \n fn cast_float_to_int<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>>(\n-    bx: &Bx,\n+    bx: &mut Bx,\n     signed: bool,\n     x: Bx::Value,\n     float_ty: Bx::Type,\n@@ -869,6 +869,9 @@ fn cast_float_to_int<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>>(\n     if !bx.cx().sess().opts.debugging_opts.saturating_float_casts {\n         return fptosui_result;\n     }\n+\n+    let int_width = bx.cx().int_width(int_ty);\n+    let float_width = bx.cx().float_width(float_ty);\n     // LLVM's fpto[su]i returns undef when the input x is infinite, NaN, or does not fit into the\n     // destination integer type after rounding towards zero. This `undef` value can cause UB in\n     // safe code (see issue #10184), so we implement a saturating conversion on top of it:\n@@ -888,50 +891,50 @@ fn cast_float_to_int<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>>(\n     // On the other hand, f_max works even if int_ty::MAX is greater than float_ty::MAX. Because\n     // we're rounding towards zero, we just get float_ty::MAX (which is always an integer).\n     // This already happens today with u128::MAX = 2^128 - 1 > f32::MAX.\n-    let int_max = |signed: bool, int_ty: Bx::Type| -> u128 {\n-        let shift_amount = 128 - bx.cx().int_width(int_ty);\n+    let int_max = |signed: bool, int_width: u64| -> u128 {\n+        let shift_amount = 128 - int_width;\n         if signed {\n             i128::MAX as u128 >> shift_amount\n         } else {\n             u128::MAX >> shift_amount\n         }\n     };\n-    let int_min = |signed: bool, int_ty: Bx::Type| -> i128 {\n+    let int_min = |signed: bool, int_width: u64| -> i128 {\n         if signed {\n-            i128::MIN >> (128 - bx.cx().int_width(int_ty))\n+            i128::MIN >> (128 - int_width)\n         } else {\n             0\n         }\n     };\n \n     let compute_clamp_bounds_single =\n-    |signed: bool, int_ty: Bx::Type| -> (u128, u128) {\n-        let rounded_min = ieee::Single::from_i128_r(int_min(signed, int_ty), Round::TowardZero);\n+    |signed: bool, int_width: u64| -> (u128, u128) {\n+        let rounded_min = ieee::Single::from_i128_r(int_min(signed, int_width), Round::TowardZero);\n         assert_eq!(rounded_min.status, Status::OK);\n-        let rounded_max = ieee::Single::from_u128_r(int_max(signed, int_ty), Round::TowardZero);\n+        let rounded_max = ieee::Single::from_u128_r(int_max(signed, int_width), Round::TowardZero);\n         assert!(rounded_max.value.is_finite());\n         (rounded_min.value.to_bits(), rounded_max.value.to_bits())\n     };\n     let compute_clamp_bounds_double =\n-    |signed: bool, int_ty: Bx::Type| -> (u128, u128) {\n-        let rounded_min = ieee::Double::from_i128_r(int_min(signed, int_ty), Round::TowardZero);\n+    |signed: bool, int_width: u64| -> (u128, u128) {\n+        let rounded_min = ieee::Double::from_i128_r(int_min(signed, int_width), Round::TowardZero);\n         assert_eq!(rounded_min.status, Status::OK);\n-        let rounded_max = ieee::Double::from_u128_r(int_max(signed, int_ty), Round::TowardZero);\n+        let rounded_max = ieee::Double::from_u128_r(int_max(signed, int_width), Round::TowardZero);\n         assert!(rounded_max.value.is_finite());\n         (rounded_min.value.to_bits(), rounded_max.value.to_bits())\n     };\n \n-    let float_bits_to_llval = |bits| {\n-        let bits_llval = match bx.cx().float_width(float_ty) {\n+    let mut float_bits_to_llval = |bits| {\n+        let bits_llval = match float_width  {\n             32 => bx.cx().const_u32(bits as u32),\n             64 => bx.cx().const_u64(bits as u64),\n             n => bug!(\"unsupported float width {}\", n),\n         };\n         bx.bitcast(bits_llval, float_ty)\n     };\n-    let (f_min, f_max) = match bx.cx().float_width(float_ty) {\n-        32 => compute_clamp_bounds_single(signed, int_ty),\n-        64 => compute_clamp_bounds_double(signed, int_ty),\n+    let (f_min, f_max) = match float_width {\n+        32 => compute_clamp_bounds_single(signed, int_width),\n+        64 => compute_clamp_bounds_double(signed, int_width),\n         n => bug!(\"unsupported float width {}\", n),\n     };\n     let f_min = float_bits_to_llval(f_min);\n@@ -979,8 +982,8 @@ fn cast_float_to_int<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>>(\n     // performed is ultimately up to the backend, but at least x86 does perform them.\n     let less_or_nan = bx.fcmp(RealPredicate::RealULT, x, f_min);\n     let greater = bx.fcmp(RealPredicate::RealOGT, x, f_max);\n-    let int_max = bx.cx().const_uint_big(int_ty, int_max(signed, int_ty));\n-    let int_min = bx.cx().const_uint_big(int_ty, int_min(signed, int_ty) as u128);\n+    let int_max = bx.cx().const_uint_big(int_ty, int_max(signed, int_width));\n+    let int_min = bx.cx().const_uint_big(int_ty, int_min(signed, int_width) as u128);\n     let s0 = bx.select(less_or_nan, int_min, fptosui_result);\n     let s1 = bx.select(greater, int_max, s0);\n \n@@ -989,7 +992,9 @@ fn cast_float_to_int<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>>(\n     // Therefore we only need to execute this step for signed integer types.\n     if signed {\n         // LLVM has no isNaN predicate, so we use (x == x) instead\n-        bx.select(bx.fcmp(RealPredicate::RealOEQ, x, x), s1, bx.cx().const_uint(int_ty, 0))\n+        let zero = bx.cx().const_uint(int_ty, 0);\n+        let cmp = bx.fcmp(RealPredicate::RealOEQ, x, x);\n+        bx.select(cmp, s1, zero)\n     } else {\n         s1\n     }"}, {"sha": "0303a221ac565a652d01a91db723a4e2164d831b", "filename": "src/librustc_codegen_ssa/mir/statement.rs", "status": "modified", "additions": 11, "deletions": 11, "changes": 22, "blob_url": "https://github.com/rust-lang/rust/blob/54dd3a47fd54eb466dad7e47b41ec1b5b2dafd5a/src%2Flibrustc_codegen_ssa%2Fmir%2Fstatement.rs", "raw_url": "https://github.com/rust-lang/rust/raw/54dd3a47fd54eb466dad7e47b41ec1b5b2dafd5a/src%2Flibrustc_codegen_ssa%2Fmir%2Fstatement.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_codegen_ssa%2Fmir%2Fstatement.rs?ref=54dd3a47fd54eb466dad7e47b41ec1b5b2dafd5a", "patch": "@@ -19,12 +19,12 @@ use interfaces::*;\n impl<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>> FunctionCx<'a, 'tcx, Bx> {\n     pub fn codegen_statement(\n         &mut self,\n-        bx: Bx,\n+        mut bx: Bx,\n         statement: &mir::Statement<'tcx>\n     ) -> Bx {\n         debug!(\"codegen_statement(statement={:?})\", statement);\n \n-        self.set_debug_loc(&bx, statement.source_info);\n+        self.set_debug_loc(&mut bx, statement.source_info);\n         match statement.kind {\n             mir::StatementKind::Assign(ref place, ref rvalue) => {\n                 if let mir::Place::Local(index) = *place {\n@@ -53,39 +53,39 @@ impl<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>> FunctionCx<'a, 'tcx, Bx> {\n                         }\n                     }\n                 } else {\n-                    let cg_dest = self.codegen_place(&bx, place);\n+                    let cg_dest = self.codegen_place(&mut bx, place);\n                     self.codegen_rvalue(bx, cg_dest, rvalue)\n                 }\n             }\n             mir::StatementKind::SetDiscriminant{ref place, variant_index} => {\n-                self.codegen_place(&bx, place)\n-                    .codegen_set_discr(&bx, variant_index);\n+                self.codegen_place(&mut bx, place)\n+                    .codegen_set_discr(&mut bx, variant_index);\n                 bx\n             }\n             mir::StatementKind::StorageLive(local) => {\n                 if let LocalRef::Place(cg_place) = self.locals[local] {\n-                    cg_place.storage_live(&bx);\n+                    cg_place.storage_live(&mut bx);\n                 } else if let LocalRef::UnsizedPlace(cg_indirect_place) = self.locals[local] {\n-                    cg_indirect_place.storage_live(&bx);\n+                    cg_indirect_place.storage_live(&mut bx);\n                 }\n                 bx\n             }\n             mir::StatementKind::StorageDead(local) => {\n                 if let LocalRef::Place(cg_place) = self.locals[local] {\n-                    cg_place.storage_dead(&bx);\n+                    cg_place.storage_dead(&mut bx);\n                 } else if let LocalRef::UnsizedPlace(cg_indirect_place) = self.locals[local] {\n-                    cg_indirect_place.storage_dead(&bx);\n+                    cg_indirect_place.storage_dead(&mut bx);\n                 }\n                 bx\n             }\n             mir::StatementKind::InlineAsm { ref asm, ref outputs, ref inputs } => {\n                 let outputs = outputs.iter().map(|output| {\n-                    self.codegen_place(&bx, output)\n+                    self.codegen_place(&mut bx, output)\n                 }).collect();\n \n                 let input_vals = inputs.iter()\n                     .fold(Vec::with_capacity(inputs.len()), |mut acc, (span, input)| {\n-                        let op = self.codegen_operand(&bx, input);\n+                        let op = self.codegen_operand(&mut bx, input);\n                         if let OperandValue::Immediate(_) = op.val {\n                             acc.push(op.immediate());\n                         } else {"}]}