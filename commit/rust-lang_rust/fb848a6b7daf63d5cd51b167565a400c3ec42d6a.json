{"sha": "fb848a6b7daf63d5cd51b167565a400c3ec42d6a", "node_id": "MDY6Q29tbWl0NzI0NzEyOmZiODQ4YTZiN2RhZjYzZDVjZDUxYjE2NzU2NWE0MDBjM2VjNDJkNmE=", "commit": {"author": {"name": "Ralf Jung", "email": "post@ralfj.de", "date": "2020-05-24T07:30:31Z"}, "committer": {"name": "GitHub", "email": "noreply@github.com", "date": "2020-05-24T07:30:31Z"}, "message": "Rollup merge of #72388 - Aaron1011:fix/deep-tokenstream-equality, r=petrochenkov\n\nRecursively expand `TokenKind::Interpolated` in `probably_equal_for_proc_macro`\n\nFixes #68430\n\nWhen comparing the captured and re-parsed `TokenStream` for a `TokenKind::Interpolated`, we currently treat any nested `TokenKind::Interpolated` tokens as unequal. If a `TokenKind::Interpolated` token shows up in the captured `TokenStream` due to a `macro_rules!` expansion, we will throw away the captured `TokenStream`, losing span information.\n\nThis PR recursively invokes `nt_to_tokenstream` on nested `TokenKind::Interpolated` tokens, effectively flattening the stream into a sequence of non-interpolated tokens. This allows it to compare equal with the re-parsed stream, allowing us to keep the original captured `TokenStream` (with span information).\n\nThis requires all of the `probably_equal_for_proc_macro` methods to be moved from `librustc_ast` to `librustc_parse` so that they can call `nt_to_tokenstream`.", "tree": {"sha": "01c6c870fd1fa4652115e7c0f72b0b9e8ed729da", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/01c6c870fd1fa4652115e7c0f72b0b9e8ed729da"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/fb848a6b7daf63d5cd51b167565a400c3ec42d6a", "comment_count": 0, "verification": {"verified": true, "reason": "valid", "signature": "-----BEGIN PGP SIGNATURE-----\n\nwsBcBAABCAAQBQJeyiKXCRBK7hj4Ov3rIwAAdHIIAGSXLgjlyVBJKBWIWUv55njR\nh3HTqSDvegWM2QHHowFPciNzNhPGBWBmnjdZ0A8iz+P42iPNCflRPr5ZXPipC3f1\ndxuBsA2R5aj4M951y614c7IpYfqoKc75/bt0/aWr1DhjdvICjWMZCzdl1DuM2D6V\nbVPPkOftySjmAoc4HuVC6YMOa3WJ1PLs/rIe1joQgQ35QiSgcm+MoOU4V1HkYFQr\nyxyUbLj8ZmrMy40CZkCmCgmH0uFCZHp3yokO7bjwaL4NUvmgxTAa7piFlCLj3Lis\noyyDvu//Qs/eX2tFbk/YEMPwHoGEtXoFrLN72ofbQ0/++iggCbSNvt8xyRrwc5Q=\n=mG2n\n-----END PGP SIGNATURE-----\n", "payload": "tree 01c6c870fd1fa4652115e7c0f72b0b9e8ed729da\nparent 3137f8e2d141d7d7c65040a718a9193f50e1282e\nparent 5685e4dd90ad2f4978c63b9097f0b568e4ce6e5c\nauthor Ralf Jung <post@ralfj.de> 1590305431 +0200\ncommitter GitHub <noreply@github.com> 1590305431 +0200\n\nRollup merge of #72388 - Aaron1011:fix/deep-tokenstream-equality, r=petrochenkov\n\nRecursively expand `TokenKind::Interpolated` in `probably_equal_for_proc_macro`\n\nFixes #68430\n\nWhen comparing the captured and re-parsed `TokenStream` for a `TokenKind::Interpolated`, we currently treat any nested `TokenKind::Interpolated` tokens as unequal. If a `TokenKind::Interpolated` token shows up in the captured `TokenStream` due to a `macro_rules!` expansion, we will throw away the captured `TokenStream`, losing span information.\n\nThis PR recursively invokes `nt_to_tokenstream` on nested `TokenKind::Interpolated` tokens, effectively flattening the stream into a sequence of non-interpolated tokens. This allows it to compare equal with the re-parsed stream, allowing us to keep the original captured `TokenStream` (with span information).\n\nThis requires all of the `probably_equal_for_proc_macro` methods to be moved from `librustc_ast` to `librustc_parse` so that they can call `nt_to_tokenstream`.\n"}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/fb848a6b7daf63d5cd51b167565a400c3ec42d6a", "html_url": "https://github.com/rust-lang/rust/commit/fb848a6b7daf63d5cd51b167565a400c3ec42d6a", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/fb848a6b7daf63d5cd51b167565a400c3ec42d6a/comments", "author": {"login": "RalfJung", "id": 330628, "node_id": "MDQ6VXNlcjMzMDYyOA==", "avatar_url": "https://avatars.githubusercontent.com/u/330628?v=4", "gravatar_id": "", "url": "https://api.github.com/users/RalfJung", "html_url": "https://github.com/RalfJung", "followers_url": "https://api.github.com/users/RalfJung/followers", "following_url": "https://api.github.com/users/RalfJung/following{/other_user}", "gists_url": "https://api.github.com/users/RalfJung/gists{/gist_id}", "starred_url": "https://api.github.com/users/RalfJung/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/RalfJung/subscriptions", "organizations_url": "https://api.github.com/users/RalfJung/orgs", "repos_url": "https://api.github.com/users/RalfJung/repos", "events_url": "https://api.github.com/users/RalfJung/events{/privacy}", "received_events_url": "https://api.github.com/users/RalfJung/received_events", "type": "User", "site_admin": false}, "committer": {"login": "web-flow", "id": 19864447, "node_id": "MDQ6VXNlcjE5ODY0NDQ3", "avatar_url": "https://avatars.githubusercontent.com/u/19864447?v=4", "gravatar_id": "", "url": "https://api.github.com/users/web-flow", "html_url": "https://github.com/web-flow", "followers_url": "https://api.github.com/users/web-flow/followers", "following_url": "https://api.github.com/users/web-flow/following{/other_user}", "gists_url": "https://api.github.com/users/web-flow/gists{/gist_id}", "starred_url": "https://api.github.com/users/web-flow/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/web-flow/subscriptions", "organizations_url": "https://api.github.com/users/web-flow/orgs", "repos_url": "https://api.github.com/users/web-flow/repos", "events_url": "https://api.github.com/users/web-flow/events{/privacy}", "received_events_url": "https://api.github.com/users/web-flow/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "3137f8e2d141d7d7c65040a718a9193f50e1282e", "url": "https://api.github.com/repos/rust-lang/rust/commits/3137f8e2d141d7d7c65040a718a9193f50e1282e", "html_url": "https://github.com/rust-lang/rust/commit/3137f8e2d141d7d7c65040a718a9193f50e1282e"}, {"sha": "5685e4dd90ad2f4978c63b9097f0b568e4ce6e5c", "url": "https://api.github.com/repos/rust-lang/rust/commits/5685e4dd90ad2f4978c63b9097f0b568e4ce6e5c", "html_url": "https://github.com/rust-lang/rust/commit/5685e4dd90ad2f4978c63b9097f0b568e4ce6e5c"}], "stats": {"total": 427, "additions": 241, "deletions": 186}, "files": [{"sha": "19ecd2023c639a4bf9211d770661a5c076aab96d", "filename": "Cargo.lock", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "blob_url": "https://github.com/rust-lang/rust/blob/fb848a6b7daf63d5cd51b167565a400c3ec42d6a/Cargo.lock", "raw_url": "https://github.com/rust-lang/rust/raw/fb848a6b7daf63d5cd51b167565a400c3ec42d6a/Cargo.lock", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/Cargo.lock?ref=fb848a6b7daf63d5cd51b167565a400c3ec42d6a", "patch": "@@ -4178,6 +4178,7 @@ dependencies = [\n  \"rustc_lexer\",\n  \"rustc_session\",\n  \"rustc_span\",\n+ \"smallvec 1.4.0\",\n  \"unicode-normalization\",\n ]\n "}, {"sha": "2e2bc380e844ff55b68f4cb9b3e2804403498859", "filename": "src/librustc_ast/token.rs", "status": "modified", "additions": 0, "deletions": 56, "changes": 56, "blob_url": "https://github.com/rust-lang/rust/blob/fb848a6b7daf63d5cd51b167565a400c3ec42d6a/src%2Flibrustc_ast%2Ftoken.rs", "raw_url": "https://github.com/rust-lang/rust/raw/fb848a6b7daf63d5cd51b167565a400c3ec42d6a/src%2Flibrustc_ast%2Ftoken.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_ast%2Ftoken.rs?ref=fb848a6b7daf63d5cd51b167565a400c3ec42d6a", "patch": "@@ -673,62 +673,6 @@ impl Token {\n \n         Some(Token::new(kind, self.span.to(joint.span)))\n     }\n-\n-    // See comments in `Nonterminal::to_tokenstream` for why we care about\n-    // *probably* equal here rather than actual equality\n-    crate fn probably_equal_for_proc_macro(&self, other: &Token) -> bool {\n-        if mem::discriminant(&self.kind) != mem::discriminant(&other.kind) {\n-            return false;\n-        }\n-        match (&self.kind, &other.kind) {\n-            (&Eq, &Eq)\n-            | (&Lt, &Lt)\n-            | (&Le, &Le)\n-            | (&EqEq, &EqEq)\n-            | (&Ne, &Ne)\n-            | (&Ge, &Ge)\n-            | (&Gt, &Gt)\n-            | (&AndAnd, &AndAnd)\n-            | (&OrOr, &OrOr)\n-            | (&Not, &Not)\n-            | (&Tilde, &Tilde)\n-            | (&At, &At)\n-            | (&Dot, &Dot)\n-            | (&DotDot, &DotDot)\n-            | (&DotDotDot, &DotDotDot)\n-            | (&DotDotEq, &DotDotEq)\n-            | (&Comma, &Comma)\n-            | (&Semi, &Semi)\n-            | (&Colon, &Colon)\n-            | (&ModSep, &ModSep)\n-            | (&RArrow, &RArrow)\n-            | (&LArrow, &LArrow)\n-            | (&FatArrow, &FatArrow)\n-            | (&Pound, &Pound)\n-            | (&Dollar, &Dollar)\n-            | (&Question, &Question)\n-            | (&Whitespace, &Whitespace)\n-            | (&Comment, &Comment)\n-            | (&Eof, &Eof) => true,\n-\n-            (&BinOp(a), &BinOp(b)) | (&BinOpEq(a), &BinOpEq(b)) => a == b,\n-\n-            (&OpenDelim(a), &OpenDelim(b)) | (&CloseDelim(a), &CloseDelim(b)) => a == b,\n-\n-            (&DocComment(a), &DocComment(b)) | (&Shebang(a), &Shebang(b)) => a == b,\n-\n-            (&Literal(a), &Literal(b)) => a == b,\n-\n-            (&Lifetime(a), &Lifetime(b)) => a == b,\n-            (&Ident(a, b), &Ident(c, d)) => {\n-                b == d && (a == c || a == kw::DollarCrate || c == kw::DollarCrate)\n-            }\n-\n-            (&Interpolated(_), &Interpolated(_)) => false,\n-\n-            _ => panic!(\"forgot to add a token?\"),\n-        }\n-    }\n }\n \n impl PartialEq<TokenKind> for Token {"}, {"sha": "9d0199078fa6a354eb4ee9f51305aa95fbb4e4fa", "filename": "src/librustc_ast/tokenstream.rs", "status": "modified", "additions": 0, "deletions": 125, "changes": 125, "blob_url": "https://github.com/rust-lang/rust/blob/fb848a6b7daf63d5cd51b167565a400c3ec42d6a/src%2Flibrustc_ast%2Ftokenstream.rs", "raw_url": "https://github.com/rust-lang/rust/raw/fb848a6b7daf63d5cd51b167565a400c3ec42d6a/src%2Flibrustc_ast%2Ftokenstream.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_ast%2Ftokenstream.rs?ref=fb848a6b7daf63d5cd51b167565a400c3ec42d6a", "patch": "@@ -21,8 +21,6 @@ use rustc_macros::HashStable_Generic;\n use rustc_span::{Span, DUMMY_SP};\n use smallvec::{smallvec, SmallVec};\n \n-use log::debug;\n-\n use std::{iter, mem};\n \n /// When the main rust parser encounters a syntax-extension invocation, it\n@@ -68,23 +66,6 @@ impl TokenTree {\n         }\n     }\n \n-    // See comments in `Nonterminal::to_tokenstream` for why we care about\n-    // *probably* equal here rather than actual equality\n-    //\n-    // This is otherwise the same as `eq_unspanned`, only recursing with a\n-    // different method.\n-    pub fn probably_equal_for_proc_macro(&self, other: &TokenTree) -> bool {\n-        match (self, other) {\n-            (TokenTree::Token(token), TokenTree::Token(token2)) => {\n-                token.probably_equal_for_proc_macro(token2)\n-            }\n-            (TokenTree::Delimited(_, delim, tts), TokenTree::Delimited(_, delim2, tts2)) => {\n-                delim == delim2 && tts.probably_equal_for_proc_macro(&tts2)\n-            }\n-            _ => false,\n-        }\n-    }\n-\n     /// Retrieves the TokenTree's span.\n     pub fn span(&self) -> Span {\n         match self {\n@@ -307,112 +288,6 @@ impl TokenStream {\n         t1.next().is_none() && t2.next().is_none()\n     }\n \n-    // See comments in `Nonterminal::to_tokenstream` for why we care about\n-    // *probably* equal here rather than actual equality\n-    //\n-    // This is otherwise the same as `eq_unspanned`, only recursing with a\n-    // different method.\n-    pub fn probably_equal_for_proc_macro(&self, other: &TokenStream) -> bool {\n-        // When checking for `probably_eq`, we ignore certain tokens that aren't\n-        // preserved in the AST. Because they are not preserved, the pretty\n-        // printer arbitrarily adds or removes them when printing as token\n-        // streams, making a comparison between a token stream generated from an\n-        // AST and a token stream which was parsed into an AST more reliable.\n-        fn semantic_tree(tree: &TokenTree) -> bool {\n-            if let TokenTree::Token(token) = tree {\n-                if let\n-                    // The pretty printer tends to add trailing commas to\n-                    // everything, and in particular, after struct fields.\n-                    | token::Comma\n-                    // The pretty printer emits `NoDelim` as whitespace.\n-                    | token::OpenDelim(DelimToken::NoDelim)\n-                    | token::CloseDelim(DelimToken::NoDelim)\n-                    // The pretty printer collapses many semicolons into one.\n-                    | token::Semi\n-                    // The pretty printer collapses whitespace arbitrarily and can\n-                    // introduce whitespace from `NoDelim`.\n-                    | token::Whitespace\n-                    // The pretty printer can turn `$crate` into `::crate_name`\n-                    | token::ModSep = token.kind {\n-                    return false;\n-                }\n-            }\n-            true\n-        }\n-\n-        // When comparing two `TokenStream`s, we ignore the `IsJoint` information.\n-        //\n-        // However, `rustc_parse::lexer::tokentrees::TokenStreamBuilder` will\n-        // use `Token.glue` on adjacent tokens with the proper `IsJoint`.\n-        // Since we are ignoreing `IsJoint`, a 'glued' token (e.g. `BinOp(Shr)`)\n-        // and its 'split'/'unglued' compoenents (e.g. `Gt, Gt`) are equivalent\n-        // when determining if two `TokenStream`s are 'probably equal'.\n-        //\n-        // Therefore, we use `break_two_token_op` to convert all tokens\n-        // to the 'unglued' form (if it exists). This ensures that two\n-        // `TokenStream`s which differ only in how their tokens are glued\n-        // will be considered 'probably equal', which allows us to keep spans.\n-        //\n-        // This is important when the original `TokenStream` contained\n-        // extra spaces (e.g. `f :: < Vec < _ > > ( ) ;'). These extra spaces\n-        // will be omitted when we pretty-print, which can cause the original\n-        // and reparsed `TokenStream`s to differ in the assignment of `IsJoint`,\n-        // leading to some tokens being 'glued' together in one stream but not\n-        // the other. See #68489 for more details.\n-        fn break_tokens(tree: TokenTree) -> impl Iterator<Item = TokenTree> {\n-            // In almost all cases, we should have either zero or one levels\n-            // of 'unglueing'. However, in some unusual cases, we may need\n-            // to iterate breaking tokens mutliple times. For example:\n-            // '[BinOpEq(Shr)] => [Gt, Ge] -> [Gt, Gt, Eq]'\n-            let mut token_trees: SmallVec<[_; 2]>;\n-            if let TokenTree::Token(token) = &tree {\n-                let mut out = SmallVec::<[_; 2]>::new();\n-                out.push(token.clone());\n-                // Iterate to fixpoint:\n-                // * We start off with 'out' containing our initial token, and `temp` empty\n-                // * If we are able to break any tokens in `out`, then `out` will have\n-                //   at least one more element than 'temp', so we will try to break tokens\n-                //   again.\n-                // * If we cannot break any tokens in 'out', we are done\n-                loop {\n-                    let mut temp = SmallVec::<[_; 2]>::new();\n-                    let mut changed = false;\n-\n-                    for token in out.into_iter() {\n-                        if let Some((first, second)) = token.kind.break_two_token_op() {\n-                            temp.push(Token::new(first, DUMMY_SP));\n-                            temp.push(Token::new(second, DUMMY_SP));\n-                            changed = true;\n-                        } else {\n-                            temp.push(token);\n-                        }\n-                    }\n-                    out = temp;\n-                    if !changed {\n-                        break;\n-                    }\n-                }\n-                token_trees = out.into_iter().map(|t| TokenTree::Token(t)).collect();\n-                if token_trees.len() != 1 {\n-                    debug!(\"break_tokens: broke {:?} to {:?}\", tree, token_trees);\n-                }\n-            } else {\n-                token_trees = SmallVec::new();\n-                token_trees.push(tree);\n-            }\n-            token_trees.into_iter()\n-        }\n-\n-        let mut t1 = self.trees().filter(semantic_tree).flat_map(break_tokens);\n-        let mut t2 = other.trees().filter(semantic_tree).flat_map(break_tokens);\n-        for (t1, t2) in t1.by_ref().zip(t2.by_ref()) {\n-            if !t1.probably_equal_for_proc_macro(&t2) {\n-                return false;\n-            }\n-        }\n-        t1.next().is_none() && t2.next().is_none()\n-    }\n-\n     pub fn map_enumerated<F: FnMut(usize, TokenTree) -> TokenTree>(self, mut f: F) -> TokenStream {\n         TokenStream(Lrc::new(\n             self.0"}, {"sha": "0d31a8c7bc1fb9c9ab0c2adab1269c52861040fc", "filename": "src/librustc_parse/Cargo.toml", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "blob_url": "https://github.com/rust-lang/rust/blob/fb848a6b7daf63d5cd51b167565a400c3ec42d6a/src%2Flibrustc_parse%2FCargo.toml", "raw_url": "https://github.com/rust-lang/rust/raw/fb848a6b7daf63d5cd51b167565a400c3ec42d6a/src%2Flibrustc_parse%2FCargo.toml", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_parse%2FCargo.toml?ref=fb848a6b7daf63d5cd51b167565a400c3ec42d6a", "patch": "@@ -12,6 +12,7 @@ doctest = false\n [dependencies]\n bitflags = \"1.0\"\n log = \"0.4\"\n+smallvec = { version = \"1.0\", features = [\"union\", \"may_dangle\"] }\n rustc_ast_pretty = { path = \"../librustc_ast_pretty\" }\n rustc_data_structures = { path = \"../librustc_data_structures\" }\n rustc_feature = { path = \"../librustc_feature\" }"}, {"sha": "0c817a712819f2894aec19451424e3e75f772e4a", "filename": "src/librustc_parse/lib.rs", "status": "modified", "additions": 209, "deletions": 5, "changes": 214, "blob_url": "https://github.com/rust-lang/rust/blob/fb848a6b7daf63d5cd51b167565a400c3ec42d6a/src%2Flibrustc_parse%2Flib.rs", "raw_url": "https://github.com/rust-lang/rust/raw/fb848a6b7daf63d5cd51b167565a400c3ec42d6a/src%2Flibrustc_parse%2Flib.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_parse%2Flib.rs?ref=fb848a6b7daf63d5cd51b167565a400c3ec42d6a", "patch": "@@ -7,18 +7,22 @@\n #![feature(or_patterns)]\n \n use rustc_ast::ast;\n-use rustc_ast::token::{self, Nonterminal};\n-use rustc_ast::tokenstream::{self, TokenStream, TokenTree};\n+use rustc_ast::token::{self, DelimToken, Nonterminal, Token, TokenKind};\n+use rustc_ast::tokenstream::{self, IsJoint, TokenStream, TokenTree};\n use rustc_ast_pretty::pprust;\n use rustc_data_structures::sync::Lrc;\n use rustc_errors::{Diagnostic, FatalError, Level, PResult};\n use rustc_session::parse::ParseSess;\n-use rustc_span::{FileName, SourceFile, Span};\n+use rustc_span::symbol::kw;\n+use rustc_span::{FileName, SourceFile, Span, DUMMY_SP};\n \n+use smallvec::SmallVec;\n+\n+use std::mem;\n use std::path::Path;\n use std::str;\n \n-use log::info;\n+use log::{debug, info};\n \n pub const MACRO_ARGUMENTS: Option<&'static str> = Some(\"macro arguments\");\n \n@@ -300,7 +304,7 @@ pub fn nt_to_tokenstream(nt: &Nonterminal, sess: &ParseSess, span: Span) -> Toke\n     // modifications, including adding/removing typically non-semantic\n     // tokens such as extra braces and commas, don't happen.\n     if let Some(tokens) = tokens {\n-        if tokens.probably_equal_for_proc_macro(&tokens_for_real) {\n+        if tokenstream_probably_equal_for_proc_macro(&tokens, &tokens_for_real, sess) {\n             return tokens;\n         }\n         info!(\n@@ -373,3 +377,203 @@ fn prepend_attrs(\n     builder.push(tokens.clone());\n     Some(builder.build())\n }\n+\n+// See comments in `Nonterminal::to_tokenstream` for why we care about\n+// *probably* equal here rather than actual equality\n+//\n+// This is otherwise the same as `eq_unspanned`, only recursing with a\n+// different method.\n+pub fn tokenstream_probably_equal_for_proc_macro(\n+    first: &TokenStream,\n+    other: &TokenStream,\n+    sess: &ParseSess,\n+) -> bool {\n+    // When checking for `probably_eq`, we ignore certain tokens that aren't\n+    // preserved in the AST. Because they are not preserved, the pretty\n+    // printer arbitrarily adds or removes them when printing as token\n+    // streams, making a comparison between a token stream generated from an\n+    // AST and a token stream which was parsed into an AST more reliable.\n+    fn semantic_tree(tree: &TokenTree) -> bool {\n+        if let TokenTree::Token(token) = tree {\n+            if let\n+                // The pretty printer tends to add trailing commas to\n+                // everything, and in particular, after struct fields.\n+                | token::Comma\n+                // The pretty printer emits `NoDelim` as whitespace.\n+                | token::OpenDelim(DelimToken::NoDelim)\n+                | token::CloseDelim(DelimToken::NoDelim)\n+                // The pretty printer collapses many semicolons into one.\n+                | token::Semi\n+                // The pretty printer collapses whitespace arbitrarily and can\n+                // introduce whitespace from `NoDelim`.\n+                | token::Whitespace\n+                // The pretty printer can turn `$crate` into `::crate_name`\n+                | token::ModSep = token.kind {\n+                return false;\n+            }\n+        }\n+        true\n+    }\n+\n+    // When comparing two `TokenStream`s, we ignore the `IsJoint` information.\n+    //\n+    // However, `rustc_parse::lexer::tokentrees::TokenStreamBuilder` will\n+    // use `Token.glue` on adjacent tokens with the proper `IsJoint`.\n+    // Since we are ignoreing `IsJoint`, a 'glued' token (e.g. `BinOp(Shr)`)\n+    // and its 'split'/'unglued' compoenents (e.g. `Gt, Gt`) are equivalent\n+    // when determining if two `TokenStream`s are 'probably equal'.\n+    //\n+    // Therefore, we use `break_two_token_op` to convert all tokens\n+    // to the 'unglued' form (if it exists). This ensures that two\n+    // `TokenStream`s which differ only in how their tokens are glued\n+    // will be considered 'probably equal', which allows us to keep spans.\n+    //\n+    // This is important when the original `TokenStream` contained\n+    // extra spaces (e.g. `f :: < Vec < _ > > ( ) ;'). These extra spaces\n+    // will be omitted when we pretty-print, which can cause the original\n+    // and reparsed `TokenStream`s to differ in the assignment of `IsJoint`,\n+    // leading to some tokens being 'glued' together in one stream but not\n+    // the other. See #68489 for more details.\n+    fn break_tokens(tree: TokenTree) -> impl Iterator<Item = TokenTree> {\n+        // In almost all cases, we should have either zero or one levels\n+        // of 'unglueing'. However, in some unusual cases, we may need\n+        // to iterate breaking tokens mutliple times. For example:\n+        // '[BinOpEq(Shr)] => [Gt, Ge] -> [Gt, Gt, Eq]'\n+        let mut token_trees: SmallVec<[_; 2]>;\n+        if let TokenTree::Token(token) = &tree {\n+            let mut out = SmallVec::<[_; 2]>::new();\n+            out.push(token.clone());\n+            // Iterate to fixpoint:\n+            // * We start off with 'out' containing our initial token, and `temp` empty\n+            // * If we are able to break any tokens in `out`, then `out` will have\n+            //   at least one more element than 'temp', so we will try to break tokens\n+            //   again.\n+            // * If we cannot break any tokens in 'out', we are done\n+            loop {\n+                let mut temp = SmallVec::<[_; 2]>::new();\n+                let mut changed = false;\n+\n+                for token in out.into_iter() {\n+                    if let Some((first, second)) = token.kind.break_two_token_op() {\n+                        temp.push(Token::new(first, DUMMY_SP));\n+                        temp.push(Token::new(second, DUMMY_SP));\n+                        changed = true;\n+                    } else {\n+                        temp.push(token);\n+                    }\n+                }\n+                out = temp;\n+                if !changed {\n+                    break;\n+                }\n+            }\n+            token_trees = out.into_iter().map(|t| TokenTree::Token(t)).collect();\n+            if token_trees.len() != 1 {\n+                debug!(\"break_tokens: broke {:?} to {:?}\", tree, token_trees);\n+            }\n+        } else {\n+            token_trees = SmallVec::new();\n+            token_trees.push(tree);\n+        }\n+        token_trees.into_iter()\n+    }\n+\n+    let expand_nt = |tree: TokenTree| {\n+        if let TokenTree::Token(Token { kind: TokenKind::Interpolated(nt), span }) = &tree {\n+            nt_to_tokenstream(nt, sess, *span).into_trees()\n+        } else {\n+            TokenStream::new(vec![(tree, IsJoint::NonJoint)]).into_trees()\n+        }\n+    };\n+\n+    // Break tokens after we expand any nonterminals, so that we break tokens\n+    // that are produced as a result of nonterminal expansion.\n+    let mut t1 = first.trees().filter(semantic_tree).flat_map(expand_nt).flat_map(break_tokens);\n+    let mut t2 = other.trees().filter(semantic_tree).flat_map(expand_nt).flat_map(break_tokens);\n+    for (t1, t2) in t1.by_ref().zip(t2.by_ref()) {\n+        if !tokentree_probably_equal_for_proc_macro(&t1, &t2, sess) {\n+            return false;\n+        }\n+    }\n+    t1.next().is_none() && t2.next().is_none()\n+}\n+\n+// See comments in `Nonterminal::to_tokenstream` for why we care about\n+// *probably* equal here rather than actual equality\n+crate fn token_probably_equal_for_proc_macro(first: &Token, other: &Token) -> bool {\n+    use TokenKind::*;\n+\n+    if mem::discriminant(&first.kind) != mem::discriminant(&other.kind) {\n+        return false;\n+    }\n+    match (&first.kind, &other.kind) {\n+        (&Eq, &Eq)\n+        | (&Lt, &Lt)\n+        | (&Le, &Le)\n+        | (&EqEq, &EqEq)\n+        | (&Ne, &Ne)\n+        | (&Ge, &Ge)\n+        | (&Gt, &Gt)\n+        | (&AndAnd, &AndAnd)\n+        | (&OrOr, &OrOr)\n+        | (&Not, &Not)\n+        | (&Tilde, &Tilde)\n+        | (&At, &At)\n+        | (&Dot, &Dot)\n+        | (&DotDot, &DotDot)\n+        | (&DotDotDot, &DotDotDot)\n+        | (&DotDotEq, &DotDotEq)\n+        | (&Comma, &Comma)\n+        | (&Semi, &Semi)\n+        | (&Colon, &Colon)\n+        | (&ModSep, &ModSep)\n+        | (&RArrow, &RArrow)\n+        | (&LArrow, &LArrow)\n+        | (&FatArrow, &FatArrow)\n+        | (&Pound, &Pound)\n+        | (&Dollar, &Dollar)\n+        | (&Question, &Question)\n+        | (&Whitespace, &Whitespace)\n+        | (&Comment, &Comment)\n+        | (&Eof, &Eof) => true,\n+\n+        (&BinOp(a), &BinOp(b)) | (&BinOpEq(a), &BinOpEq(b)) => a == b,\n+\n+        (&OpenDelim(a), &OpenDelim(b)) | (&CloseDelim(a), &CloseDelim(b)) => a == b,\n+\n+        (&DocComment(a), &DocComment(b)) | (&Shebang(a), &Shebang(b)) => a == b,\n+\n+        (&Literal(a), &Literal(b)) => a == b,\n+\n+        (&Lifetime(a), &Lifetime(b)) => a == b,\n+        (&Ident(a, b), &Ident(c, d)) => {\n+            b == d && (a == c || a == kw::DollarCrate || c == kw::DollarCrate)\n+        }\n+\n+        // Expanded by `tokenstream_probably_equal_for_proc_macro`\n+        (&Interpolated(_), &Interpolated(_)) => unreachable!(),\n+\n+        _ => panic!(\"forgot to add a token?\"),\n+    }\n+}\n+\n+// See comments in `Nonterminal::to_tokenstream` for why we care about\n+// *probably* equal here rather than actual equality\n+//\n+// This is otherwise the same as `eq_unspanned`, only recursing with a\n+// different method.\n+pub fn tokentree_probably_equal_for_proc_macro(\n+    first: &TokenTree,\n+    other: &TokenTree,\n+    sess: &ParseSess,\n+) -> bool {\n+    match (first, other) {\n+        (TokenTree::Token(token), TokenTree::Token(token2)) => {\n+            token_probably_equal_for_proc_macro(token, token2)\n+        }\n+        (TokenTree::Delimited(_, delim, tts), TokenTree::Delimited(_, delim2, tts2)) => {\n+            delim == delim2 && tokenstream_probably_equal_for_proc_macro(&tts, &tts2, sess)\n+        }\n+        _ => false,\n+    }\n+}"}, {"sha": "37436567d70f00996fd5201e0c277e823b37bda4", "filename": "src/test/ui/proc-macro/macro-rules-capture.rs", "status": "added", "additions": 18, "deletions": 0, "changes": 18, "blob_url": "https://github.com/rust-lang/rust/blob/fb848a6b7daf63d5cd51b167565a400c3ec42d6a/src%2Ftest%2Fui%2Fproc-macro%2Fmacro-rules-capture.rs", "raw_url": "https://github.com/rust-lang/rust/raw/fb848a6b7daf63d5cd51b167565a400c3ec42d6a/src%2Ftest%2Fui%2Fproc-macro%2Fmacro-rules-capture.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ftest%2Fui%2Fproc-macro%2Fmacro-rules-capture.rs?ref=fb848a6b7daf63d5cd51b167565a400c3ec42d6a", "patch": "@@ -0,0 +1,18 @@\n+// aux-build: test-macros.rs\n+\n+extern crate test_macros;\n+use test_macros::recollect_attr;\n+\n+macro_rules! reemit {\n+    ($name:ident => $($token:expr)*) => {\n+\n+        #[recollect_attr]\n+        pub fn $name() {\n+            $($token)*;\n+        }\n+    }\n+}\n+\n+reemit! { foo => 45u32.into() } //~ ERROR type annotations\n+\n+fn main() {}"}, {"sha": "6d512846ff7852d136090c55a5d96bed2429d9b2", "filename": "src/test/ui/proc-macro/macro-rules-capture.stderr", "status": "added", "additions": 12, "deletions": 0, "changes": 12, "blob_url": "https://github.com/rust-lang/rust/blob/fb848a6b7daf63d5cd51b167565a400c3ec42d6a/src%2Ftest%2Fui%2Fproc-macro%2Fmacro-rules-capture.stderr", "raw_url": "https://github.com/rust-lang/rust/raw/fb848a6b7daf63d5cd51b167565a400c3ec42d6a/src%2Ftest%2Fui%2Fproc-macro%2Fmacro-rules-capture.stderr", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ftest%2Fui%2Fproc-macro%2Fmacro-rules-capture.stderr?ref=fb848a6b7daf63d5cd51b167565a400c3ec42d6a", "patch": "@@ -0,0 +1,12 @@\n+error[E0282]: type annotations needed\n+  --> $DIR/macro-rules-capture.rs:16:24\n+   |\n+LL | reemit! { foo => 45u32.into() }\n+   |                  ------^^^^--\n+   |                  |     |\n+   |                  |     cannot infer type for type parameter `T` declared on the trait `Into`\n+   |                  this method call resolves to `T`\n+\n+error: aborting due to previous error\n+\n+For more information about this error, try `rustc --explain E0282`."}]}