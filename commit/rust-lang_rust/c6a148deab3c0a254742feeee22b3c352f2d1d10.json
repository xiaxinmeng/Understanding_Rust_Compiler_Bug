{"sha": "c6a148deab3c0a254742feeee22b3c352f2d1d10", "node_id": "MDY6Q29tbWl0NzI0NzEyOmM2YTE0OGRlYWIzYzBhMjU0NzQyZmVlZWUyMmIzYzM1MmYyZDFkMTA=", "commit": {"author": {"name": "Luqman Aden", "email": "me@luqman.ca", "date": "2014-07-09T22:31:45Z"}, "committer": {"name": "Luqman Aden", "email": "me@luqman.ca", "date": "2014-07-09T22:31:45Z"}, "message": "librustc: Don't emit call for intrinsics instead just trans at callsite.", "tree": {"sha": "91b2f163ba91ada8ebae990446162a456ea36bc2", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/91b2f163ba91ada8ebae990446162a456ea36bc2"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/c6a148deab3c0a254742feeee22b3c352f2d1d10", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/c6a148deab3c0a254742feeee22b3c352f2d1d10", "html_url": "https://github.com/rust-lang/rust/commit/c6a148deab3c0a254742feeee22b3c352f2d1d10", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/c6a148deab3c0a254742feeee22b3c352f2d1d10/comments", "author": {"login": "luqmana", "id": 287063, "node_id": "MDQ6VXNlcjI4NzA2Mw==", "avatar_url": "https://avatars.githubusercontent.com/u/287063?v=4", "gravatar_id": "", "url": "https://api.github.com/users/luqmana", "html_url": "https://github.com/luqmana", "followers_url": "https://api.github.com/users/luqmana/followers", "following_url": "https://api.github.com/users/luqmana/following{/other_user}", "gists_url": "https://api.github.com/users/luqmana/gists{/gist_id}", "starred_url": "https://api.github.com/users/luqmana/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/luqmana/subscriptions", "organizations_url": "https://api.github.com/users/luqmana/orgs", "repos_url": "https://api.github.com/users/luqmana/repos", "events_url": "https://api.github.com/users/luqmana/events{/privacy}", "received_events_url": "https://api.github.com/users/luqmana/received_events", "type": "User", "site_admin": false}, "committer": {"login": "luqmana", "id": 287063, "node_id": "MDQ6VXNlcjI4NzA2Mw==", "avatar_url": "https://avatars.githubusercontent.com/u/287063?v=4", "gravatar_id": "", "url": "https://api.github.com/users/luqmana", "html_url": "https://github.com/luqmana", "followers_url": "https://api.github.com/users/luqmana/followers", "following_url": "https://api.github.com/users/luqmana/following{/other_user}", "gists_url": "https://api.github.com/users/luqmana/gists{/gist_id}", "starred_url": "https://api.github.com/users/luqmana/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/luqmana/subscriptions", "organizations_url": "https://api.github.com/users/luqmana/orgs", "repos_url": "https://api.github.com/users/luqmana/repos", "events_url": "https://api.github.com/users/luqmana/events{/privacy}", "received_events_url": "https://api.github.com/users/luqmana/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "8bb34a3146e6ba4bc7902a85de90cf4f8064ace0", "url": "https://api.github.com/repos/rust-lang/rust/commits/8bb34a3146e6ba4bc7902a85de90cf4f8064ace0", "html_url": "https://github.com/rust-lang/rust/commit/8bb34a3146e6ba4bc7902a85de90cf4f8064ace0"}], "stats": {"total": 555, "additions": 541, "deletions": 14}, "files": [{"sha": "915b33d5be5f04ede9b5af780aea2a2b03a05411", "filename": "src/librustc/middle/trans/callee.rs", "status": "modified", "additions": 38, "deletions": 14, "changes": 52, "blob_url": "https://github.com/rust-lang/rust/blob/c6a148deab3c0a254742feeee22b3c352f2d1d10/src%2Flibrustc%2Fmiddle%2Ftrans%2Fcallee.rs", "raw_url": "https://github.com/rust-lang/rust/raw/c6a148deab3c0a254742feeee22b3c352f2d1d10/src%2Flibrustc%2Fmiddle%2Ftrans%2Fcallee.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Fmiddle%2Ftrans%2Fcallee.rs?ref=c6a148deab3c0a254742feeee22b3c352f2d1d10", "patch": "@@ -40,6 +40,7 @@ use middle::trans::expr;\n use middle::trans::glue;\n use middle::trans::inline;\n use middle::trans::foreign;\n+use middle::trans::intrinsic;\n use middle::trans::meth;\n use middle::trans::monomorphize;\n use middle::trans::type_::Type;\n@@ -68,6 +69,8 @@ pub enum CalleeData {\n     // value (which is a pair).\n     Fn(/* llfn */ ValueRef),\n \n+    Intrinsic(ast::NodeId, subst::Substs),\n+\n     TraitMethod(MethodData)\n }\n \n@@ -119,7 +122,21 @@ fn trans<'a>(bcx: &'a Block<'a>, expr: &ast::Expr) -> Callee<'a> {\n \n     fn trans_def<'a>(bcx: &'a Block<'a>, def: def::Def, ref_expr: &ast::Expr)\n                  -> Callee<'a> {\n+        debug!(\"trans_def(def={}, ref_expr={})\", def.repr(bcx.tcx()), ref_expr.repr(bcx.tcx()));\n+        let expr_ty = node_id_type(bcx, ref_expr.id);\n         match def {\n+            def::DefFn(did, _) if match ty::get(expr_ty).sty {\n+                ty::ty_bare_fn(ref f) => f.abi == synabi::RustIntrinsic,\n+                _ => false\n+            } => {\n+                let substs = node_id_substs(bcx, ExprId(ref_expr.id));\n+                let def_id = if did.krate != ast::LOCAL_CRATE {\n+                    inline::maybe_instantiate_inline(bcx.ccx(), did)\n+                } else {\n+                    did\n+                };\n+                Callee { bcx: bcx, data: Intrinsic(def_id.node, substs) }\n+            }\n             def::DefFn(did, _) |\n             def::DefStaticMethod(did, def::FromImpl(_), _) => {\n                 fn_callee(bcx, trans_fn_ref(bcx, did, ExprId(ref_expr.id)))\n@@ -662,6 +679,12 @@ pub fn trans_call_inner<'a>(\n     let callee = get_callee(bcx, cleanup::CustomScope(arg_cleanup_scope));\n     let mut bcx = callee.bcx;\n \n+    let (abi, ret_ty) = match ty::get(callee_ty).sty {\n+        ty::ty_bare_fn(ref f) => (f.abi, f.sig.output),\n+        ty::ty_closure(ref f) => (synabi::Rust, f.sig.output),\n+        _ => fail!(\"expected bare rust fn or closure in trans_call_inner\")\n+    };\n+\n     let (llfn, llenv, llself) = match callee.data {\n         Fn(llfn) => {\n             (llfn, None, None)\n@@ -679,14 +702,15 @@ pub fn trans_call_inner<'a>(\n             let llenv = Load(bcx, llenv);\n             (llfn, Some(llenv), None)\n         }\n-    };\n+        Intrinsic(node, substs) => {\n+            assert!(abi == synabi::RustIntrinsic);\n+            assert!(dest.is_some());\n \n-    let (abi, ret_ty) = match ty::get(callee_ty).sty {\n-        ty::ty_bare_fn(ref f) => (f.abi, f.sig.output),\n-        ty::ty_closure(ref f) => (synabi::Rust, f.sig.output),\n-        _ => fail!(\"expected bare rust fn or closure in trans_call_inner\")\n+            return intrinsic::trans_intrinsic_call(bcx, node, callee_ty,\n+                                                   arg_cleanup_scope, args,\n+                                                   dest.unwrap(), substs);\n+        }\n     };\n-    let is_rust_fn = abi == synabi::Rust || abi == synabi::RustIntrinsic;\n \n     // Generate a location to store the result. If the user does\n     // not care about the result, just make a stack slot.\n@@ -716,7 +740,7 @@ pub fn trans_call_inner<'a>(\n     // and done, either the return value of the function will have been\n     // written in opt_llretslot (if it is Some) or `llresult` will be\n     // set appropriately (otherwise).\n-    if is_rust_fn {\n+    if abi == synabi::Rust {\n         let mut llargs = Vec::new();\n \n         // Push the out-pointer if we use an out-pointer for this\n@@ -816,13 +840,13 @@ pub enum CallArgs<'a> {\n     ArgOverloadedOp(Datum<Expr>, Option<(Datum<Expr>, ast::NodeId)>),\n }\n \n-fn trans_args<'a>(cx: &'a Block<'a>,\n-                  args: CallArgs,\n-                  fn_ty: ty::t,\n-                  llargs: &mut Vec<ValueRef> ,\n-                  arg_cleanup_scope: cleanup::ScopeId,\n-                  ignore_self: bool)\n-                  -> &'a Block<'a> {\n+pub fn trans_args<'a>(cx: &'a Block<'a>,\n+                      args: CallArgs,\n+                      fn_ty: ty::t,\n+                      llargs: &mut Vec<ValueRef> ,\n+                      arg_cleanup_scope: cleanup::ScopeId,\n+                      ignore_self: bool)\n+                      -> &'a Block<'a> {\n     let _icx = push_ctxt(\"trans_args\");\n     let arg_tys = ty::ty_fn_args(fn_ty);\n     let variadic = ty::fn_is_variadic(fn_ty);"}, {"sha": "4963d58e7db7f09197764b12d23614e2de68ebb8", "filename": "src/librustc/middle/trans/intrinsic.rs", "status": "modified", "additions": 503, "deletions": 0, "changes": 503, "blob_url": "https://github.com/rust-lang/rust/blob/c6a148deab3c0a254742feeee22b3c352f2d1d10/src%2Flibrustc%2Fmiddle%2Ftrans%2Fintrinsic.rs", "raw_url": "https://github.com/rust-lang/rust/raw/c6a148deab3c0a254742feeee22b3c352f2d1d10/src%2Flibrustc%2Fmiddle%2Ftrans%2Fintrinsic.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Fmiddle%2Ftrans%2Fintrinsic.rs?ref=c6a148deab3c0a254742feeee22b3c352f2d1d10", "patch": "@@ -14,11 +14,16 @@ use arena::TypedArena;\n use lib::llvm::{SequentiallyConsistent, Acquire, Release, Xchg};\n use lib::llvm::{ValueRef, Pointer, Array, Struct};\n use lib;\n+use middle::subst;\n use middle::subst::FnSpace;\n use middle::trans::base::*;\n use middle::trans::build::*;\n+use middle::trans::callee;\n+use middle::trans::cleanup;\n+use middle::trans::cleanup::CleanupMethods;\n use middle::trans::common::*;\n use middle::trans::datum::*;\n+use middle::trans::expr;\n use middle::trans::glue;\n use middle::trans::type_of::*;\n use middle::trans::type_of;\n@@ -606,3 +611,501 @@ pub fn check_intrinsics(ccx: &CrateContext) {\n     ccx.sess().abort_if_errors();\n }\n \n+pub fn trans_intrinsic_call<'a>(mut bcx: &'a Block<'a>, node: ast::NodeId,\n+                                callee_ty: ty::t, cleanup_scope: cleanup::CustomScopeIndex,\n+                                args: callee::CallArgs, dest: expr::Dest,\n+                                substs: subst::Substs) -> Result<'a> {\n+\n+    let fcx = bcx.fcx;\n+    let ccx = fcx.ccx;\n+    let tcx = bcx.tcx();\n+\n+    let ret_ty = match ty::get(callee_ty).sty {\n+        ty::ty_bare_fn(ref f) => f.sig.output,\n+        _ => fail!(\"expected bare_fn in trans_intrinsic_call\")\n+    };\n+    let llret_ty = type_of::type_of(ccx, ret_ty);\n+\n+    // Get location to store the result. If the user does\n+    // not care about the result, just make a stack slot\n+    let llresult = match dest {\n+        expr::SaveIn(d) => d,\n+        expr::Ignore => {\n+            if !type_is_zero_size(ccx, ret_ty) {\n+                alloc_ty(bcx, ret_ty, \"intrinsic_result\")\n+            } else {\n+                C_undef(llret_ty.ptr_to())\n+            }\n+        }\n+    };\n+\n+    // Push the arguments.\n+    let mut llargs = Vec::new();\n+    bcx = callee::trans_args(bcx, args, callee_ty, &mut llargs,\n+                             cleanup::CustomScope(cleanup_scope), false);\n+\n+    fcx.pop_custom_cleanup_scope(cleanup_scope);\n+\n+    let foreign_item = tcx.map.expect_foreign_item(node);\n+    let name = token::get_ident(foreign_item.ident);\n+    let simple = get_simple_intrinsic(ccx, &*foreign_item);\n+\n+    let llval = match (simple, name.get()) {\n+        (Some(llfn), _) => {\n+            Call(bcx, llfn, llargs.as_slice(), [])\n+        }\n+        (_, \"abort\") => {\n+            let llfn = ccx.get_intrinsic(&(\"llvm.trap\"));\n+            let v = Call(bcx, llfn, [], []);\n+            Unreachable(bcx);\n+            v\n+        }\n+        (_, \"breakpoint\") => {\n+            let llfn = ccx.get_intrinsic(&(\"llvm.debugtrap\"));\n+            Call(bcx, llfn, [], [])\n+        }\n+        (_, \"size_of\") => {\n+            let tp_ty = *substs.types.get(FnSpace, 0);\n+            let lltp_ty = type_of::type_of(ccx, tp_ty);\n+            C_uint(ccx, machine::llsize_of_real(ccx, lltp_ty) as uint)\n+        }\n+        (_, \"min_align_of\") => {\n+            let tp_ty = *substs.types.get(FnSpace, 0);\n+            let lltp_ty = type_of::type_of(ccx, tp_ty);\n+            C_uint(ccx, machine::llalign_of_min(ccx, lltp_ty) as uint)\n+        }\n+        (_, \"pref_align_of\") => {\n+            let tp_ty = *substs.types.get(FnSpace, 0);\n+            let lltp_ty = type_of::type_of(ccx, tp_ty);\n+            C_uint(ccx, machine::llalign_of_pref(ccx, lltp_ty) as uint)\n+        }\n+        (_, \"move_val_init\") => {\n+            // Create a datum reflecting the value being moved.\n+            // Use `appropriate_mode` so that the datum is by ref\n+            // if the value is non-immediate. Note that, with\n+            // intrinsics, there are no argument cleanups to\n+            // concern ourselves with, so we can use an rvalue datum.\n+            let tp_ty = *substs.types.get(FnSpace, 0);\n+            let mode = appropriate_rvalue_mode(ccx, tp_ty);\n+            let src = Datum {\n+                val: *llargs.get(1),\n+                ty: tp_ty,\n+                kind: Rvalue::new(mode)\n+            };\n+            bcx = src.store_to(bcx, *llargs.get(0));\n+            C_nil(ccx)\n+        }\n+        (_, \"get_tydesc\") => {\n+            let tp_ty = *substs.types.get(FnSpace, 0);\n+            let static_ti = get_tydesc(ccx, tp_ty);\n+            glue::lazily_emit_visit_glue(ccx, &*static_ti);\n+\n+            // FIXME (#3730): ideally this shouldn't need a cast,\n+            // but there's a circularity between translating rust types to llvm\n+            // types and having a tydesc type available. So I can't directly access\n+            // the llvm type of intrinsic::TyDesc struct.\n+            PointerCast(bcx, static_ti.tydesc, llret_ty)\n+        }\n+        (_, \"type_id\") => {\n+            let hash = ty::hash_crate_independent(\n+                ccx.tcx(),\n+                *substs.types.get(FnSpace, 0),\n+                &ccx.link_meta.crate_hash);\n+            // NB: This needs to be kept in lockstep with the TypeId struct in\n+            //     the intrinsic module\n+            C_named_struct(llret_ty, [C_u64(ccx, hash)])\n+        }\n+        (_, \"init\") => {\n+            let tp_ty = *substs.types.get(FnSpace, 0);\n+            let lltp_ty = type_of::type_of(ccx, tp_ty);\n+            if return_type_is_void(ccx, tp_ty) {\n+                C_nil(ccx)\n+            } else {\n+                C_null(lltp_ty)\n+            }\n+        }\n+        // Effectively no-ops\n+        (_, \"uninit\") | (_, \"forget\") => {\n+            C_nil(ccx)\n+        }\n+        (_, \"transmute\") => {\n+            let (in_type, out_type) = (*substs.types.get(FnSpace, 0),\n+                                       *substs.types.get(FnSpace, 1));\n+            let llintype = type_of::type_of(ccx, in_type);\n+            let llouttype = type_of::type_of(ccx, out_type);\n+\n+            let in_type_size = machine::llbitsize_of_real(ccx, llintype);\n+            let out_type_size = machine::llbitsize_of_real(ccx, llouttype);\n+            if in_type_size != out_type_size {\n+                /*\n+                let sp = match ccx.tcx.map.get(ref_id.unwrap()) {\n+                    ast_map::NodeExpr(e) => e.span,\n+                    _ => fail!(\"transmute has non-expr arg\"),\n+                };\n+                ccx.sess().span_bug(sp,\n+                */\n+                ccx.sess().bug(\n+                    format!(\"transmute called on types with different sizes: \\\n+                             {} ({} bit{}) to \\\n+                             {} ({} bit{})\",\n+                            ty_to_string(ccx.tcx(), in_type),\n+                            in_type_size,\n+                            if in_type_size == 1 {\"\"} else {\"s\"},\n+                            ty_to_string(ccx.tcx(), out_type),\n+                            out_type_size,\n+                            if out_type_size == 1 {\"\"} else {\"s\"}).as_slice());\n+            }\n+\n+            if !return_type_is_void(ccx, out_type) {\n+                let llsrcval = *llargs.get(0);\n+                if type_is_immediate(ccx, in_type) {\n+                    if type_is_immediate(ccx, out_type) {\n+                        Store(bcx, llsrcval, PointerCast(bcx, llresult, llintype.ptr_to()));\n+                        C_nil(ccx)\n+                    } else {\n+                        match (llintype.kind(), llouttype.kind()) {\n+                            (Pointer, other) | (other, Pointer) if other != Pointer => {\n+                                let tmp = Alloca(bcx, llouttype, \"\");\n+                                Store(bcx, llsrcval, PointerCast(bcx, tmp, llintype.ptr_to()));\n+                                Load(bcx, tmp)\n+                            }\n+                            (Array, _) | (_, Array) | (Struct, _) | (_, Struct) => {\n+                                let tmp = Alloca(bcx, llouttype, \"\");\n+                                Store(bcx, llsrcval, PointerCast(bcx, tmp, llintype.ptr_to()));\n+                                Load(bcx, tmp)\n+                            }\n+                            _ => {\n+                                BitCast(bcx, llsrcval, llouttype)\n+                            }\n+                        }\n+                    }\n+                } else if type_is_immediate(ccx, out_type) {\n+                    Load(bcx, PointerCast(bcx, llsrcval, llouttype.ptr_to()))\n+                } else {\n+                    // NB: Do not use a Load and Store here. This causes massive\n+                    // code bloat when `transmute` is used on large structural\n+                    // types.\n+                    let lldestptr = llresult;\n+                    let lldestptr = PointerCast(bcx, lldestptr, Type::i8p(ccx));\n+                    let llsrcptr = PointerCast(bcx, llsrcval, Type::i8p(ccx));\n+\n+                    let llsize = llsize_of(ccx, llintype);\n+                    call_memcpy(bcx, lldestptr, llsrcptr, llsize, 1);\n+\n+                    C_nil(ccx)\n+                }\n+            } else {\n+                C_nil(ccx)\n+            }\n+        }\n+        (_, \"needs_drop\") => {\n+            let tp_ty = *substs.types.get(FnSpace, 0);\n+            C_bool(ccx, ty::type_needs_drop(ccx.tcx(), tp_ty))\n+        }\n+        (_, \"owns_managed\") => {\n+            let tp_ty = *substs.types.get(FnSpace, 0);\n+            C_bool(ccx, ty::type_contents(ccx.tcx(), tp_ty).owns_managed())\n+        }\n+        (_, \"visit_tydesc\") => {\n+            let td = *llargs.get(0);\n+            let visitor = *llargs.get(1);\n+            let td = PointerCast(bcx, td, ccx.tydesc_type().ptr_to());\n+            glue::call_visit_glue(bcx, visitor, td, None);\n+            C_nil(ccx)\n+        }\n+        (_, \"offset\") => {\n+            let ptr = *llargs.get(0);\n+            let offset = *llargs.get(1);\n+            InBoundsGEP(bcx, ptr, [offset])\n+        }\n+\n+        (_, \"copy_nonoverlapping_memory\") => {\n+            copy_intrinsic(bcx, false, false, *substs.types.get(FnSpace, 0),\n+                           *llargs.get(0), *llargs.get(1), *llargs.get(2))\n+        }\n+        (_, \"copy_memory\") => {\n+            copy_intrinsic(bcx, true, false, *substs.types.get(FnSpace, 0),\n+                           *llargs.get(0), *llargs.get(1), *llargs.get(2))\n+        }\n+        (_, \"set_memory\") => {\n+            memset_intrinsic(bcx, false, *substs.types.get(FnSpace, 0),\n+                             *llargs.get(0), *llargs.get(1), *llargs.get(2))\n+        }\n+\n+        (_, \"volatile_copy_nonoverlapping_memory\") => {\n+            copy_intrinsic(bcx, false, true, *substs.types.get(FnSpace, 0),\n+                           *llargs.get(0), *llargs.get(1), *llargs.get(2))\n+        }\n+        (_, \"volatile_copy_memory\") => {\n+            copy_intrinsic(bcx, true, true, *substs.types.get(FnSpace, 0),\n+                           *llargs.get(0), *llargs.get(1), *llargs.get(2))\n+        }\n+        (_, \"volatile_set_memory\") => {\n+            memset_intrinsic(bcx, true, *substs.types.get(FnSpace, 0),\n+                             *llargs.get(0), *llargs.get(1), *llargs.get(2))\n+        }\n+        (_, \"volatile_load\") => {\n+            VolatileLoad(bcx, *llargs.get(0))\n+        },\n+        (_, \"volatile_store\") => {\n+            VolatileStore(bcx, *llargs.get(1), *llargs.get(0));\n+            C_nil(ccx)\n+        },\n+\n+        (_, \"ctlz8\") => count_zeros_intrinsic(bcx, \"llvm.ctlz.i8\", *llargs.get(0)),\n+        (_, \"ctlz16\") => count_zeros_intrinsic(bcx, \"llvm.ctlz.i16\", *llargs.get(0)),\n+        (_, \"ctlz32\") => count_zeros_intrinsic(bcx, \"llvm.ctlz.i32\", *llargs.get(0)),\n+        (_, \"ctlz64\") => count_zeros_intrinsic(bcx, \"llvm.ctlz.i64\", *llargs.get(0)),\n+        (_, \"cttz8\") => count_zeros_intrinsic(bcx, \"llvm.cttz.i8\", *llargs.get(0)),\n+        (_, \"cttz16\") => count_zeros_intrinsic(bcx, \"llvm.cttz.i16\", *llargs.get(0)),\n+        (_, \"cttz32\") => count_zeros_intrinsic(bcx, \"llvm.cttz.i32\", *llargs.get(0)),\n+        (_, \"cttz64\") => count_zeros_intrinsic(bcx, \"llvm.cttz.i64\", *llargs.get(0)),\n+\n+        (_, \"i8_add_with_overflow\") =>\n+            with_overflow_intrinsic(bcx, \"llvm.sadd.with.overflow.i8\", ret_ty,\n+                                   *llargs.get(0), *llargs.get(1)),\n+        (_, \"i16_add_with_overflow\") =>\n+            with_overflow_intrinsic(bcx, \"llvm.sadd.with.overflow.i16\", ret_ty,\n+                                   *llargs.get(0), *llargs.get(1)),\n+        (_, \"i32_add_with_overflow\") =>\n+            with_overflow_intrinsic(bcx, \"llvm.sadd.with.overflow.i32\", ret_ty,\n+                                   *llargs.get(0), *llargs.get(1)),\n+        (_, \"i64_add_with_overflow\") =>\n+            with_overflow_intrinsic(bcx, \"llvm.sadd.with.overflow.i64\", ret_ty,\n+                                   *llargs.get(0), *llargs.get(1)),\n+\n+        (_, \"u8_add_with_overflow\") =>\n+            with_overflow_intrinsic(bcx, \"llvm.uadd.with.overflow.i8\", ret_ty,\n+                                   *llargs.get(0), *llargs.get(1)),\n+        (_, \"u16_add_with_overflow\") =>\n+            with_overflow_intrinsic(bcx, \"llvm.uadd.with.overflow.i16\", ret_ty,\n+                                   *llargs.get(0), *llargs.get(1)),\n+        (_, \"u32_add_with_overflow\") =>\n+            with_overflow_intrinsic(bcx, \"llvm.uadd.with.overflow.i32\", ret_ty,\n+                                   *llargs.get(0), *llargs.get(1)),\n+        (_, \"u64_add_with_overflow\") =>\n+            with_overflow_intrinsic(bcx, \"llvm.uadd.with.overflow.i64\", ret_ty,\n+                                   *llargs.get(0), *llargs.get(1)),\n+\n+        (_, \"i8_sub_with_overflow\") =>\n+            with_overflow_intrinsic(bcx, \"llvm.ssub.with.overflow.i8\", ret_ty,\n+                                   *llargs.get(0), *llargs.get(1)),\n+        (_, \"i16_sub_with_overflow\") =>\n+            with_overflow_intrinsic(bcx, \"llvm.ssub.with.overflow.i16\", ret_ty,\n+                                   *llargs.get(0), *llargs.get(1)),\n+        (_, \"i32_sub_with_overflow\") =>\n+            with_overflow_intrinsic(bcx, \"llvm.ssub.with.overflow.i32\", ret_ty,\n+                                   *llargs.get(0), *llargs.get(1)),\n+        (_, \"i64_sub_with_overflow\") =>\n+            with_overflow_intrinsic(bcx, \"llvm.ssub.with.overflow.i64\", ret_ty,\n+                                   *llargs.get(0), *llargs.get(1)),\n+\n+        (_, \"u8_sub_with_overflow\") =>\n+            with_overflow_intrinsic(bcx, \"llvm.usub.with.overflow.i8\", ret_ty,\n+                                   *llargs.get(0), *llargs.get(1)),\n+        (_, \"u16_sub_with_overflow\") =>\n+            with_overflow_intrinsic(bcx, \"llvm.usub.with.overflow.i16\", ret_ty,\n+                                   *llargs.get(0), *llargs.get(1)),\n+        (_, \"u32_sub_with_overflow\") =>\n+            with_overflow_intrinsic(bcx, \"llvm.usub.with.overflow.i32\", ret_ty,\n+                                   *llargs.get(0), *llargs.get(1)),\n+        (_, \"u64_sub_with_overflow\") =>\n+            with_overflow_intrinsic(bcx, \"llvm.usub.with.overflow.i64\", ret_ty,\n+                                   *llargs.get(0), *llargs.get(1)),\n+\n+        (_, \"i8_mul_with_overflow\") =>\n+            with_overflow_intrinsic(bcx, \"llvm.smul.with.overflow.i8\", ret_ty,\n+                                   *llargs.get(0), *llargs.get(1)),\n+        (_, \"i16_mul_with_overflow\") =>\n+            with_overflow_intrinsic(bcx, \"llvm.smul.with.overflow.i16\", ret_ty,\n+                                   *llargs.get(0), *llargs.get(1)),\n+        (_, \"i32_mul_with_overflow\") =>\n+            with_overflow_intrinsic(bcx, \"llvm.smul.with.overflow.i32\", ret_ty,\n+                                   *llargs.get(0), *llargs.get(1)),\n+        (_, \"i64_mul_with_overflow\") =>\n+            with_overflow_intrinsic(bcx, \"llvm.smul.with.overflow.i64\", ret_ty,\n+                                   *llargs.get(0), *llargs.get(1)),\n+\n+        (_, \"u8_mul_with_overflow\") =>\n+            with_overflow_intrinsic(bcx, \"llvm.umul.with.overflow.i8\", ret_ty,\n+                                    *llargs.get(0), *llargs.get(1)),\n+        (_, \"u16_mul_with_overflow\") =>\n+            with_overflow_intrinsic(bcx, \"llvm.umul.with.overflow.i16\", ret_ty,\n+                                    *llargs.get(0), *llargs.get(1)),\n+        (_, \"u32_mul_with_overflow\") =>\n+            with_overflow_intrinsic(bcx, \"llvm.umul.with.overflow.i32\", ret_ty,\n+                                    *llargs.get(0), *llargs.get(1)),\n+        (_, \"u64_mul_with_overflow\") =>\n+            with_overflow_intrinsic(bcx, \"llvm.umul.with.overflow.i64\", ret_ty,\n+                                    *llargs.get(0), *llargs.get(1)),\n+\n+        // This requires that atomic intrinsics follow a specific naming pattern:\n+        // \"atomic_<operation>[_<ordering>]\", and no ordering means SeqCst\n+        (_, name) if name.starts_with(\"atomic_\") => {\n+            let split: Vec<&str> = name.split('_').collect();\n+            assert!(split.len() >= 2, \"Atomic intrinsic not correct format\");\n+\n+            let order = if split.len() == 2 {\n+                lib::llvm::SequentiallyConsistent\n+            } else {\n+                match *split.get(2) {\n+                    \"relaxed\" => lib::llvm::Monotonic,\n+                    \"acq\"     => lib::llvm::Acquire,\n+                    \"rel\"     => lib::llvm::Release,\n+                    \"acqrel\"  => lib::llvm::AcquireRelease,\n+                    _ => ccx.sess().fatal(\"unknown ordering in atomic intrinsic\")\n+                }\n+            };\n+\n+            match *split.get(1) {\n+                \"cxchg\" => {\n+                    // See include/llvm/IR/Instructions.h for their implementation\n+                    // of this, I assume that it's good enough for us to use for\n+                    // now.\n+                    let strongest_failure_ordering = match order {\n+                        lib::llvm::NotAtomic | lib::llvm::Unordered =>\n+                            ccx.sess().fatal(\"cmpxchg must be atomic\"),\n+\n+                        lib::llvm::Monotonic | lib::llvm::Release =>\n+                            lib::llvm::Monotonic,\n+\n+                        lib::llvm::Acquire | lib::llvm::AcquireRelease =>\n+                            lib::llvm::Acquire,\n+\n+                        lib::llvm::SequentiallyConsistent =>\n+                            lib::llvm::SequentiallyConsistent\n+                    };\n+\n+                    let res = AtomicCmpXchg(bcx, *llargs.get(0), *llargs.get(1),\n+                                            *llargs.get(2), order,\n+                                            strongest_failure_ordering);\n+                    if unsafe { lib::llvm::llvm::LLVMVersionMinor() >= 5 } {\n+                        ExtractValue(bcx, res, 0)\n+                    } else {\n+                        res\n+                    }\n+                }\n+\n+                \"load\" => {\n+                    AtomicLoad(bcx, *llargs.get(0), order)\n+                }\n+                \"store\" => {\n+                    AtomicStore(bcx, *llargs.get(1), *llargs.get(0), order);\n+                    C_nil(ccx)\n+                }\n+\n+                \"fence\" => {\n+                    AtomicFence(bcx, order);\n+                    C_nil(ccx)\n+                }\n+\n+                // These are all AtomicRMW ops\n+                op => {\n+                    let atom_op = match op {\n+                        \"xchg\"  => lib::llvm::Xchg,\n+                        \"xadd\"  => lib::llvm::Add,\n+                        \"xsub\"  => lib::llvm::Sub,\n+                        \"and\"   => lib::llvm::And,\n+                        \"nand\"  => lib::llvm::Nand,\n+                        \"or\"    => lib::llvm::Or,\n+                        \"xor\"   => lib::llvm::Xor,\n+                        \"max\"   => lib::llvm::Max,\n+                        \"min\"   => lib::llvm::Min,\n+                        \"umax\"  => lib::llvm::UMax,\n+                        \"umin\"  => lib::llvm::UMin,\n+                        _ => ccx.sess().fatal(\"unknown atomic operation\")\n+                    };\n+\n+                    AtomicRMW(bcx, atom_op, *llargs.get(0), *llargs.get(1), order)\n+                }\n+            }\n+\n+        }\n+\n+        (_, _) => ccx.sess().span_bug(foreign_item.span, \"unknown intrinsic\")\n+    };\n+\n+    if val_ty(llval) != Type::void(ccx) &&\n+       machine::llsize_of_alloc(ccx, val_ty(llval)) != 0 {\n+        store_ty(bcx, llval, llresult, ret_ty);\n+    }\n+\n+    // If we made a temporary stack slot, let's clean it up\n+    match dest {\n+        expr::Ignore => {\n+            bcx = glue::drop_ty(bcx, llresult, ret_ty);\n+        }\n+        expr::SaveIn(_) => {}\n+    }\n+\n+    Result::new(bcx, llresult)\n+}\n+\n+fn copy_intrinsic(bcx: &Block, allow_overlap: bool, volatile: bool,\n+                  tp_ty: ty::t, dst: ValueRef, src: ValueRef, count: ValueRef) -> ValueRef {\n+    let ccx = bcx.ccx();\n+    let lltp_ty = type_of::type_of(ccx, tp_ty);\n+    let align = C_i32(ccx, machine::llalign_of_min(ccx, lltp_ty) as i32);\n+    let size = machine::llsize_of(ccx, lltp_ty);\n+    let int_size = machine::llbitsize_of_real(ccx, ccx.int_type);\n+    let name = if allow_overlap {\n+        if int_size == 32 {\n+            \"llvm.memmove.p0i8.p0i8.i32\"\n+        } else {\n+            \"llvm.memmove.p0i8.p0i8.i64\"\n+        }\n+    } else {\n+        if int_size == 32 {\n+            \"llvm.memcpy.p0i8.p0i8.i32\"\n+        } else {\n+            \"llvm.memcpy.p0i8.p0i8.i64\"\n+        }\n+    };\n+\n+    let dst_ptr = PointerCast(bcx, dst, Type::i8p(ccx));\n+    let src_ptr = PointerCast(bcx, src, Type::i8p(ccx));\n+    let llfn = ccx.get_intrinsic(&name);\n+\n+    Call(bcx, llfn, [dst_ptr, src_ptr, Mul(bcx, size, count), align,\n+                     C_bool(ccx, volatile)], [])\n+}\n+\n+fn memset_intrinsic(bcx: &Block, volatile: bool, tp_ty: ty::t,\n+                    dst: ValueRef, val: ValueRef, count: ValueRef) -> ValueRef {\n+    let ccx = bcx.ccx();\n+    let lltp_ty = type_of::type_of(ccx, tp_ty);\n+    let align = C_i32(ccx, machine::llalign_of_min(ccx, lltp_ty) as i32);\n+    let size = machine::llsize_of(ccx, lltp_ty);\n+    let name = if machine::llbitsize_of_real(ccx, ccx.int_type) == 32 {\n+        \"llvm.memset.p0i8.i32\"\n+    } else {\n+        \"llvm.memset.p0i8.i64\"\n+    };\n+\n+    let dst_ptr = PointerCast(bcx, dst, Type::i8p(ccx));\n+    let llfn = ccx.get_intrinsic(&name);\n+\n+    Call(bcx, llfn, [dst_ptr, val, Mul(bcx, size, count), align,\n+                     C_bool(ccx, volatile)], [])\n+}\n+\n+fn count_zeros_intrinsic(bcx: &Block, name: &'static str, val: ValueRef) -> ValueRef {\n+    let y = C_bool(bcx.ccx(), false);\n+    let llfn = bcx.ccx().get_intrinsic(&name);\n+    Call(bcx, llfn, [val, y], [])\n+}\n+\n+fn with_overflow_intrinsic(bcx: &Block, name: &'static str, t: ty::t,\n+                           a: ValueRef, b: ValueRef) -> ValueRef {\n+    let llfn = bcx.ccx().get_intrinsic(&name);\n+\n+    // Convert `i1` to a `bool`, and write it to the out parameter\n+    let val = Call(bcx, llfn, [a, b], []);\n+    let result = ExtractValue(bcx, val, 0);\n+    let overflow = ZExt(bcx, ExtractValue(bcx, val, 1), Type::bool(bcx.ccx()));\n+    let ret = C_undef(type_of::type_of(bcx.ccx(), t));\n+    let ret = InsertValue(bcx, ret, result, 0);\n+    let ret = InsertValue(bcx, ret, overflow, 1);\n+\n+    ret\n+}"}]}