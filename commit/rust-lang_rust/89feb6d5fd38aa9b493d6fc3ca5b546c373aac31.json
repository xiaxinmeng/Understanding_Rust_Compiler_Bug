{"sha": "89feb6d5fd38aa9b493d6fc3ca5b546c373aac31", "node_id": "MDY6Q29tbWl0NzI0NzEyOjg5ZmViNmQ1ZmQzOGFhOWI0OTNkNmZjM2NhNWI1NDZjMzczYWFjMzE=", "commit": {"author": {"name": "Pawe\u0142 Romanowski", "email": "pawroman@gmail.com", "date": "2019-04-18T13:30:50Z"}, "committer": {"name": "Pawe\u0142 Romanowski", "email": "pawroman@gmail.com", "date": "2019-04-18T13:30:50Z"}, "message": "Clean up unicode.py script", "tree": {"sha": "3710eab04a4f1e10d7abe0ab324a0ec77c31fc77", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/3710eab04a4f1e10d7abe0ab324a0ec77c31fc77"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/89feb6d5fd38aa9b493d6fc3ca5b546c373aac31", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/89feb6d5fd38aa9b493d6fc3ca5b546c373aac31", "html_url": "https://github.com/rust-lang/rust/commit/89feb6d5fd38aa9b493d6fc3ca5b546c373aac31", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/89feb6d5fd38aa9b493d6fc3ca5b546c373aac31/comments", "author": {"login": "pawroman", "id": 914977, "node_id": "MDQ6VXNlcjkxNDk3Nw==", "avatar_url": "https://avatars.githubusercontent.com/u/914977?v=4", "gravatar_id": "", "url": "https://api.github.com/users/pawroman", "html_url": "https://github.com/pawroman", "followers_url": "https://api.github.com/users/pawroman/followers", "following_url": "https://api.github.com/users/pawroman/following{/other_user}", "gists_url": "https://api.github.com/users/pawroman/gists{/gist_id}", "starred_url": "https://api.github.com/users/pawroman/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/pawroman/subscriptions", "organizations_url": "https://api.github.com/users/pawroman/orgs", "repos_url": "https://api.github.com/users/pawroman/repos", "events_url": "https://api.github.com/users/pawroman/events{/privacy}", "received_events_url": "https://api.github.com/users/pawroman/received_events", "type": "User", "site_admin": false}, "committer": {"login": "pawroman", "id": 914977, "node_id": "MDQ6VXNlcjkxNDk3Nw==", "avatar_url": "https://avatars.githubusercontent.com/u/914977?v=4", "gravatar_id": "", "url": "https://api.github.com/users/pawroman", "html_url": "https://github.com/pawroman", "followers_url": "https://api.github.com/users/pawroman/followers", "following_url": "https://api.github.com/users/pawroman/following{/other_user}", "gists_url": "https://api.github.com/users/pawroman/gists{/gist_id}", "starred_url": "https://api.github.com/users/pawroman/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/pawroman/subscriptions", "organizations_url": "https://api.github.com/users/pawroman/orgs", "repos_url": "https://api.github.com/users/pawroman/repos", "events_url": "https://api.github.com/users/pawroman/events{/privacy}", "received_events_url": "https://api.github.com/users/pawroman/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "be1dbaffed6858ba176cd69e298c1be365d8f801", "url": "https://api.github.com/repos/rust-lang/rust/commits/be1dbaffed6858ba176cd69e298c1be365d8f801", "html_url": "https://github.com/rust-lang/rust/commit/be1dbaffed6858ba176cd69e298c1be365d8f801"}], "stats": {"total": 373, "additions": 270, "deletions": 103}, "files": [{"sha": "51f3e722ca7d8d21ab096fcdc00609311d960d53", "filename": ".gitignore", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "blob_url": "https://github.com/rust-lang/rust/blob/89feb6d5fd38aa9b493d6fc3ca5b546c373aac31/.gitignore", "raw_url": "https://github.com/rust-lang/rust/raw/89feb6d5fd38aa9b493d6fc3ca5b546c373aac31/.gitignore", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/.gitignore?ref=89feb6d5fd38aa9b493d6fc3ca5b546c373aac31", "patch": "@@ -36,6 +36,7 @@ __pycache__/\n /src/libcore/unicode/Scripts.txt\n /src/libcore/unicode/SpecialCasing.txt\n /src/libcore/unicode/UnicodeData.txt\n+/src/libcore/unicode/downloaded\n /stage[0-9]+/\n /target\n target/"}, {"sha": "97c11fb795ea84ec448e51728ba0ffee07eac870", "filename": "src/libcore/unicode/unicode.py", "status": "modified", "additions": 269, "deletions": 103, "changes": 372, "blob_url": "https://github.com/rust-lang/rust/blob/89feb6d5fd38aa9b493d6fc3ca5b546c373aac31/src%2Flibcore%2Funicode%2Funicode.py", "raw_url": "https://github.com/rust-lang/rust/raw/89feb6d5fd38aa9b493d6fc3ca5b546c373aac31/src%2Flibcore%2Funicode%2Funicode.py", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibcore%2Funicode%2Funicode.py?ref=89feb6d5fd38aa9b493d6fc3ca5b546c373aac31", "patch": "@@ -1,35 +1,71 @@\n #!/usr/bin/env python\n \n-# This script uses the following Unicode tables:\n-# - DerivedCoreProperties.txt\n-# - DerivedNormalizationProps.txt\n-# - EastAsianWidth.txt\n-# - auxiliary/GraphemeBreakProperty.txt\n-# - PropList.txt\n-# - ReadMe.txt\n-# - Scripts.txt\n-# - UnicodeData.txt\n-#\n+\"\"\"\n+Regenerate Unicode tables (tables.rs).\n+\"\"\"\n+\n+# This script uses the Unicode tables as defined\n+# in the UnicodeFiles class.\n+\n # Since this should not require frequent updates, we just store this\n # out-of-line and check the tables.rs file into git.\n \n-import fileinput, re, os, sys, operator, math, datetime\n+# Note that the \"curl\" program is required for operation.\n+# This script is compatible with Python 2.7 and 3.x.\n+\n+import argparse\n+import datetime\n+import fileinput\n+import operator\n+import os\n+import re\n+import textwrap\n+import subprocess\n+\n+from collections import namedtuple\n+\n+\n+# we don't use enum.Enum because of Python 2.7 compatibility\n+class UnicodeFiles(object):\n+    # ReadMe does not contain any unicode data, we\n+    # use it to extract versions.\n+    README = \"ReadMe.txt\"\n+\n+    DERIVED_CORE_PROPERTIES = \"DerivedCoreProperties.txt\"\n+    DERIVED_NORMALIZATION_PROPS = \"DerivedNormalizationProps.txt\"\n+    SPECIAL_CASING = \"SpecialCasing.txt\"\n+    SCRIPTS = \"Scripts.txt\"\n+    PROPS = \"PropList.txt\"\n+    UNICODE_DATA = \"UnicodeData.txt\"\n+\n+\n+UnicodeFiles.ALL_FILES = tuple(\n+    getattr(UnicodeFiles, name) for name in dir(UnicodeFiles)\n+    if not name.startswith(\"_\")\n+)\n \n-# The directory in which this file resides.\n-fdir = os.path.dirname(os.path.realpath(__file__)) + \"/\"\n+# The directory this file is located in.\n+THIS_DIR = os.path.dirname(os.path.realpath(__file__))\n \n-preamble = '''\n+# Where to download the Unicode data.  The downloaded files\n+# will be placed in sub-directories named after Unicode version.\n+FETCH_DIR = os.path.join(THIS_DIR, \"downloaded\")\n+\n+FETCH_URL_LATEST = \"ftp://ftp.unicode.org/Public/UNIDATA/{filename}\"\n+FETCH_URL_VERSION = \"ftp://ftp.unicode.org/Public/{version}/ucd/{filename}\"\n+\n+PREAMBLE = \"\"\"\\\n // NOTE: The following code was generated by \"./unicode.py\", do not edit directly\n \n #![allow(missing_docs, non_upper_case_globals, non_snake_case)]\n \n use unicode::version::UnicodeVersion;\n use unicode::bool_trie::{{BoolTrie, SmallBoolTrie}};\n-'''.format(year = datetime.datetime.now().year)\n+\"\"\".format(year=datetime.datetime.now().year)\n \n # Mapping taken from Table 12 from:\n # http://www.unicode.org/reports/tr44/#General_Category_Values\n-expanded_categories = {\n+EXPANDED_CATEGORIES = {\n     'Lu': ['LC', 'L'], 'Ll': ['LC', 'L'], 'Lt': ['LC', 'L'],\n     'Lm': ['L'], 'Lo': ['L'],\n     'Mn': ['M'], 'Mc': ['M'], 'Me': ['M'],\n@@ -42,22 +78,101 @@\n }\n \n # these are the surrogate codepoints, which are not valid rust characters\n-surrogate_codepoints = (0xd800, 0xdfff)\n+SURROGATE_CODEPOINTS = (0xd800, 0xdfff)\n+\n+UnicodeData = namedtuple(\n+    \"UnicodeData\", (\"canon_decomp\", \"compat_decomp\", \"gencats\", \"combines\",\n+                    \"to_upper\", \"to_lower\", \"to_title\", )\n+)\n+\n+UnicodeVersion = namedtuple(\n+    \"UnicodeVersion\", (\"major\", \"minor\", \"micro\", \"as_str\")\n+)\n+\n+\n+def fetch_files(version=None):\n+    \"\"\"\n+    Fetch all the Unicode files from unicode.org\n+\n+    :param version: The desired Unicode version, as string.\n+        (If None, defaults to latest final release available).\n+    :return: The version downloaded (UnicodeVersion object).\n+    \"\"\"\n+    have_version = should_skip_fetch(version)\n+    if have_version:\n+        return have_version\n+\n+    if version:\n+        # check if the desired version exists on the server\n+        get_fetch_url = lambda name: FETCH_URL_VERSION.format(version=version, filename=name)\n+    else:\n+        # extract the latest version\n+        get_fetch_url = lambda name: FETCH_URL_LATEST.format(filename=name)\n+\n+    readme_url = get_fetch_url(UnicodeFiles.README)\n+\n+    print(\"Fetching: {}\".format(readme_url))\n+    readme_content = subprocess.check_output((\"curl\", readme_url))\n+\n+    unicode_version = parse_unicode_version(\n+        str(readme_content, \"utf8\")\n+    )\n+\n+    download_dir = os.path.join(FETCH_DIR, unicode_version.as_str)\n+    if not os.path.exists(download_dir):\n+        # for 2.7 compat, we don't use exist_ok=True\n+        os.makedirs(download_dir)\n+\n+    for filename in UnicodeFiles.ALL_FILES:\n+        file_path = os.path.join(download_dir, filename)\n+\n+        if filename == UnicodeFiles.README:\n+            with open(file_path, \"wb\") as fd:\n+                fd.write(readme_content)\n+        elif not os.path.exists(file_path):\n+            url = get_fetch_url(filename)\n+            print(\"Fetching: {}\".format(url))\n+            subprocess.check_call((\"curl\", \"-o\", file_path, url))\n+\n+    return unicode_version\n+\n+\n+def should_skip_fetch(version):\n+    if not version:\n+        # should always check latest version\n+        return False\n+\n+    fetch_dir = os.path.join(FETCH_DIR, version)\n+\n+    for filename in UnicodeFiles.ALL_FILES:\n+        file_path = os.path.join(fetch_dir, filename)\n+\n+        if not os.path.exists(file_path):\n+            return False\n+\n+    with open(os.path.join(fetch_dir, UnicodeFiles.README)) as fd:\n+        return parse_unicode_version(fd.read())\n+\n+\n+def parse_unicode_version(readme_content):\n+    # \"raw string\" is necessary for \\d not being treated as escape char\n+    # (for the sake of compat with future Python versions)\n+    # see: https://docs.python.org/3.6/whatsnew/3.6.html#deprecated-python-behavior\n+    pattern = r\"for Version (\\d+)\\.(\\d+)\\.(\\d+) of the Unicode\"\n+    groups = re.search(pattern, readme_content).groups()\n+\n+    return UnicodeVersion(*map(int, groups), as_str=\".\".join(groups))\n+\n \n-def fetch(f):\n-    path = fdir + os.path.basename(f)\n-    if not os.path.exists(path):\n-        os.system(\"curl -o {0}{1} ftp://ftp.unicode.org/Public/UNIDATA/{1}\".format(fdir, f))\n+def get_unicode_file_path(unicode_version, filename):\n+    return os.path.join(FETCH_DIR, unicode_version.as_str, filename)\n \n-    if not os.path.exists(path):\n-        sys.stderr.write(\"cannot load %s\" % f)\n-        exit(1)\n \n def is_surrogate(n):\n-    return surrogate_codepoints[0] <= n <= surrogate_codepoints[1]\n+    return SURROGATE_CODEPOINTS[0] <= n <= SURROGATE_CODEPOINTS[1]\n \n-def load_unicode_data(f):\n-    fetch(f)\n+\n+def load_unicode_data(file_path):\n     gencats = {}\n     to_lower = {}\n     to_upper = {}\n@@ -68,8 +183,8 @@ def load_unicode_data(f):\n \n     udict = {}\n     range_start = -1\n-    for line in fileinput.input(fdir + f):\n-        data = line.split(';')\n+    for line in fileinput.input(file_path):\n+        data = line.split(\";\")\n         if len(data) != 15:\n             continue\n         cp = int(data[0], 16)\n@@ -104,7 +219,7 @@ def load_unicode_data(f):\n \n         # store decomposition, if given\n         if decomp != \"\":\n-            if decomp.startswith('<'):\n+            if decomp.startswith(\"<\"):\n                 seq = []\n                 for i in decomp.split()[1:]:\n                     seq.append(int(i, 16))\n@@ -116,7 +231,7 @@ def load_unicode_data(f):\n                 canon_decomp[code] = seq\n \n         # place letter in categories as appropriate\n-        for cat in [gencat, \"Assigned\"] + expanded_categories.get(gencat, []):\n+        for cat in [gencat, \"Assigned\"] + EXPANDED_CATEGORIES.get(gencat, []):\n             if cat not in gencats:\n                 gencats[cat] = []\n             gencats[cat].append(code)\n@@ -136,12 +251,15 @@ def load_unicode_data(f):\n     gencats = group_cats(gencats)\n     combines = to_combines(group_cats(combines))\n \n-    return (canon_decomp, compat_decomp, gencats, combines, to_upper, to_lower, to_title)\n+    return UnicodeData(\n+        canon_decomp, compat_decomp, gencats, combines, to_upper,\n+        to_lower, to_title,\n+    )\n+\n \n-def load_special_casing(f, to_upper, to_lower, to_title):\n-    fetch(f)\n-    for line in fileinput.input(fdir + f):\n-        data = line.split('#')[0].split(';')\n+def load_special_casing(file_path, unicode_data):\n+    for line in fileinput.input(file_path):\n+        data = line.split(\"#\")[0].split(\";\")\n         if len(data) == 5:\n             code, lower, title, upper, _comment = data\n         elif len(data) == 6:\n@@ -155,20 +273,24 @@ def load_special_casing(f, to_upper, to_lower, to_title):\n         title = title.strip()\n         upper = upper.strip()\n         key = int(code, 16)\n-        for (map_, values) in [(to_lower, lower), (to_upper, upper), (to_title, title)]:\n+        for (map_, values) in ((unicode_data.to_lower, lower),\n+                               (unicode_data.to_upper, upper),\n+                               (unicode_data.to_title, title)):\n             if values != code:\n                 values = [int(i, 16) for i in values.split()]\n                 for _ in range(len(values), 3):\n                     values.append(0)\n                 assert len(values) == 3\n                 map_[key] = values\n \n+\n def group_cats(cats):\n     cats_out = {}\n     for cat in cats:\n         cats_out[cat] = group_cat(cats[cat])\n     return cats_out\n \n+\n def group_cat(cat):\n     cat_out = []\n     letters = sorted(set(cat))\n@@ -185,6 +307,7 @@ def group_cat(cat):\n     cat_out.append((cur_start, cur_end))\n     return cat_out\n \n+\n def ungroup_cat(cat):\n     cat_out = []\n     for (lo, hi) in cat:\n@@ -193,21 +316,24 @@ def ungroup_cat(cat):\n             lo += 1\n     return cat_out\n \n+\n def gen_unassigned(assigned):\n     assigned = set(assigned)\n     return ([i for i in range(0, 0xd800) if i not in assigned] +\n             [i for i in range(0xe000, 0x110000) if i not in assigned])\n \n+\n def to_combines(combs):\n     combs_out = []\n     for comb in combs:\n         for (lo, hi) in combs[comb]:\n             combs_out.append((lo, hi, comb))\n-    combs_out.sort(key=lambda comb: comb[0])\n+    combs_out.sort(key=lambda c: c[0])\n     return combs_out\n \n+\n def format_table_content(f, content, indent):\n-    line = \" \"*indent\n+    line = \" \" * indent\n     first = True\n     for chunk in content.split(\",\"):\n         if len(line) + len(chunk) < 98:\n@@ -218,16 +344,19 @@ def format_table_content(f, content, indent):\n             first = False\n         else:\n             f.write(line + \",\\n\")\n-            line = \" \"*indent + chunk\n+            line = \" \" * indent + chunk\n     f.write(line)\n \n-def load_properties(f, interestingprops):\n-    fetch(f)\n+\n+def load_properties(file_path, interestingprops):\n     props = {}\n-    re1 = re.compile(\"^ *([0-9A-F]+) *; *(\\w+)\")\n-    re2 = re.compile(\"^ *([0-9A-F]+)\\.\\.([0-9A-F]+) *; *(\\w+)\")\n+    # \"raw string\" is necessary for \\w not to be treated as escape char\n+    # (for the sake of compat with future Python versions)\n+    # see: https://docs.python.org/3.6/whatsnew/3.6.html#deprecated-python-behavior\n+    re1 = re.compile(r\"^ *([0-9A-F]+) *; *(\\w+)\")\n+    re2 = re.compile(r\"^ *([0-9A-F]+)\\.\\.([0-9A-F]+) *; *(\\w+)\")\n \n-    for line in fileinput.input(fdir + os.path.basename(f)):\n+    for line in fileinput.input(file_path):\n         prop = None\n         d_lo = 0\n         d_hi = 0\n@@ -258,10 +387,12 @@ def load_properties(f, interestingprops):\n \n     return props\n \n+\n def escape_char(c):\n     return \"'\\\\u{%x}'\" % c if c != 0 else \"'\\\\0'\"\n \n-def emit_table(f, name, t_data, t_type = \"&[(char, char)]\", is_pub=True,\n+\n+def emit_table(f, name, t_data, t_type=\"&[(char, char)]\", is_pub=True,\n         pfun=lambda x: \"(%s,%s)\" % (escape_char(x[0]), escape_char(x[1]))):\n     pub_string = \"\"\n     if is_pub:\n@@ -277,6 +408,7 @@ def emit_table(f, name, t_data, t_type = \"&[(char, char)]\", is_pub=True,\n     format_table_content(f, data, 8)\n     f.write(\"\\n    ];\\n\\n\")\n \n+\n def compute_trie(rawdata, chunksize):\n     root = []\n     childmap = {}\n@@ -288,18 +420,19 @@ def compute_trie(rawdata, chunksize):\n             childmap[child] = len(childmap)\n             child_data.extend(data)\n         root.append(childmap[child])\n-    return (root, child_data)\n+    return root, child_data\n+\n \n def emit_bool_trie(f, name, t_data, is_pub=True):\n-    CHUNK = 64\n+    chunk_size = 64\n     rawdata = [False] * 0x110000\n     for (lo, hi) in t_data:\n         for cp in range(lo, hi + 1):\n             rawdata[cp] = True\n \n     # convert to bitmap chunks of 64 bits each\n     chunks = []\n-    for i in range(0x110000 // CHUNK):\n+    for i in range(0x110000 // chunk_size):\n         chunk = 0\n         for j in range(64):\n             if rawdata[i * 64 + j]:\n@@ -311,12 +444,12 @@ def emit_bool_trie(f, name, t_data, is_pub=True):\n         pub_string = \"pub \"\n     f.write(\"    %sconst %s: &super::BoolTrie = &super::BoolTrie {\\n\" % (pub_string, name))\n     f.write(\"        r1: [\\n\")\n-    data = ','.join('0x%016x' % chunk for chunk in chunks[0:0x800 // CHUNK])\n+    data = ','.join('0x%016x' % chunk for chunk in chunks[0:0x800 // chunk_size])\n     format_table_content(f, data, 12)\n     f.write(\"\\n        ],\\n\")\n \n     # 0x800..0x10000 trie\n-    (r2, r3) = compute_trie(chunks[0x800 // CHUNK : 0x10000 // CHUNK], 64 // CHUNK)\n+    (r2, r3) = compute_trie(chunks[0x800 // chunk_size : 0x10000 // chunk_size], 64 // chunk_size)\n     f.write(\"        r2: [\\n\")\n     data = ','.join(str(node) for node in r2)\n     format_table_content(f, data, 12)\n@@ -327,7 +460,7 @@ def emit_bool_trie(f, name, t_data, is_pub=True):\n     f.write(\"\\n        ],\\n\")\n \n     # 0x10000..0x110000 trie\n-    (mid, r6) = compute_trie(chunks[0x10000 // CHUNK : 0x110000 // CHUNK], 64 // CHUNK)\n+    (mid, r6) = compute_trie(chunks[0x10000 // chunk_size : 0x110000 // chunk_size], 64 // chunk_size)\n     (r4, r5) = compute_trie(mid, 64)\n     f.write(\"        r4: [\\n\")\n     data = ','.join(str(node) for node in r4)\n@@ -344,6 +477,7 @@ def emit_bool_trie(f, name, t_data, is_pub=True):\n \n     f.write(\"    };\\n\\n\")\n \n+\n def emit_small_bool_trie(f, name, t_data, is_pub=True):\n     last_chunk = max(hi // 64 for (lo, hi) in t_data)\n     n_chunks = last_chunk + 1\n@@ -374,6 +508,7 @@ def emit_small_bool_trie(f, name, t_data, is_pub=True):\n \n     f.write(\"    };\\n\\n\")\n \n+\n def emit_property_module(f, mod, tbl, emit):\n     f.write(\"pub mod %s {\\n\" % mod)\n     for cat in sorted(emit):\n@@ -389,7 +524,8 @@ def emit_property_module(f, mod, tbl, emit):\n             f.write(\"    }\\n\\n\")\n     f.write(\"}\\n\\n\")\n \n-def emit_conversions_module(f, to_upper, to_lower, to_title):\n+\n+def emit_conversions_module(f, unicode_data):\n     f.write(\"pub mod conversions {\")\n     f.write(\"\"\"\n     pub fn to_lower(c: char) -> [char; 3] {\n@@ -414,74 +550,104 @@ def emit_conversions_module(f, to_upper, to_lower, to_title):\n     t_type = \"&[(char, [char; 3])]\"\n     pfun = lambda x: \"(%s,[%s,%s,%s])\" % (\n         escape_char(x[0]), escape_char(x[1][0]), escape_char(x[1][1]), escape_char(x[1][2]))\n-    emit_table(f, \"to_lowercase_table\",\n-        sorted(to_lower.items(), key=operator.itemgetter(0)),\n-        is_pub=False, t_type = t_type, pfun=pfun)\n-    emit_table(f, \"to_uppercase_table\",\n-        sorted(to_upper.items(), key=operator.itemgetter(0)),\n-        is_pub=False, t_type = t_type, pfun=pfun)\n-    f.write(\"}\\n\\n\")\n \n-def emit_norm_module(f, canon, compat, combine, norm_props):\n-    canon_keys = sorted(canon.keys())\n+    emit_table(f,\n+               name=\"to_lowercase_table\",\n+               t_data=sorted(unicode_data.to_lower.items(), key=operator.itemgetter(0)),\n+               t_type=t_type,\n+               is_pub=False,\n+               pfun=pfun)\n \n-    compat_keys = sorted(compat.keys())\n+    emit_table(f,\n+               name=\"to_uppercase_table\",\n+               t_data=sorted(unicode_data.to_upper.items(), key=operator.itemgetter(0)),\n+               t_type=t_type,\n+               is_pub=False,\n+               pfun=pfun)\n+\n+    f.write(\"}\\n\")\n+\n+\n+def emit_norm_module(f, unicode_data, norm_props):\n+    canon_keys = sorted(unicode_data.canon_decomp.keys())\n \n     canon_comp = {}\n     comp_exclusions = norm_props[\"Full_Composition_Exclusion\"]\n     for char in canon_keys:\n         if any(lo <= char <= hi for lo, hi in comp_exclusions):\n             continue\n-        decomp = canon[char]\n+        decomp = unicode_data.canon_decomp[char]\n         if len(decomp) == 2:\n             if decomp[0] not in canon_comp:\n                 canon_comp[decomp[0]] = []\n-            canon_comp[decomp[0]].append( (decomp[1], char) )\n-    canon_comp_keys = sorted(canon_comp.keys())\n+            canon_comp[decomp[0]].append((decomp[1], char))\n \n-if __name__ == \"__main__\":\n-    r = fdir + \"tables.rs\"\n-    if os.path.exists(r):\n-        os.remove(r)\n-    with open(r, \"w\") as rf:\n+\n+def parse_args():\n+    parser = argparse.ArgumentParser(description=__doc__)\n+    parser.add_argument(\"-v\", \"--version\", default=None, type=str,\n+                        help=\"Unicode version to use (if not specified,\"\n+                             \" defaults to latest available final release).\")\n+\n+    return parser.parse_args()\n+\n+\n+def main():\n+    args = parse_args()\n+\n+    unicode_version = fetch_files(args.version)\n+    print(\"Using Unicode version: {}\".format(unicode_version.as_str))\n+\n+    tables_rs_path = os.path.join(THIS_DIR, \"tables.rs\")\n+    if os.path.exists(tables_rs_path):\n+        os.remove(tables_rs_path)\n+\n+    with open(tables_rs_path, \"w\") as rf:\n         # write the file's preamble\n-        rf.write(preamble)\n-\n-        # download and parse all the data\n-        fetch(\"ReadMe.txt\")\n-        with open(fdir + \"ReadMe.txt\") as readme:\n-            pattern = \"for Version (\\d+)\\.(\\d+)\\.(\\d+) of the Unicode\"\n-            unicode_version = re.search(pattern, readme.read()).groups()\n-        rf.write(\"\"\"\n-/// The version of [Unicode](http://www.unicode.org/) that the Unicode parts of\n-/// `char` and `str` methods are based on.\n-#[unstable(feature = \"unicode_version\", issue = \"49726\")]\n-pub const UNICODE_VERSION: UnicodeVersion = UnicodeVersion {\n-    major: %s,\n-    minor: %s,\n-    micro: %s,\n-    _priv: (),\n-};\n-\"\"\" % unicode_version)\n-        (canon_decomp, compat_decomp, gencats, combines,\n-                to_upper, to_lower, to_title) = load_unicode_data(\"UnicodeData.txt\")\n-        load_special_casing(\"SpecialCasing.txt\", to_upper, to_lower, to_title)\n+        rf.write(PREAMBLE)\n+\n+        unicode_version_notice = textwrap.dedent(\"\"\"\n+        /// The version of [Unicode](http://www.unicode.org/) that the Unicode parts of\n+        /// `char` and `str` methods are based on.\n+        #[unstable(feature = \"unicode_version\", issue = \"49726\")]\n+        pub const UNICODE_VERSION: UnicodeVersion = UnicodeVersion {{\n+            major: {version.major},\n+            minor: {version.minor},\n+            micro: {version.micro},\n+            _priv: (),\n+        }};\n+        \"\"\").format(version=unicode_version)\n+        rf.write(unicode_version_notice)\n+\n+        get_path = lambda f: get_unicode_file_path(unicode_version, f)\n+\n+        unicode_data = load_unicode_data(get_path(UnicodeFiles.UNICODE_DATA))\n+        load_special_casing(get_path(UnicodeFiles.SPECIAL_CASING), unicode_data)\n+\n         want_derived = [\"XID_Start\", \"XID_Continue\", \"Alphabetic\", \"Lowercase\", \"Uppercase\",\n                         \"Cased\", \"Case_Ignorable\", \"Grapheme_Extend\"]\n-        derived = load_properties(\"DerivedCoreProperties.txt\", want_derived)\n-        scripts = load_properties(\"Scripts.txt\", [])\n-        props = load_properties(\"PropList.txt\",\n-                [\"White_Space\", \"Join_Control\", \"Noncharacter_Code_Point\", \"Pattern_White_Space\"])\n-        norm_props = load_properties(\"DerivedNormalizationProps.txt\",\n-                     [\"Full_Composition_Exclusion\"])\n+        derived = load_properties(get_path(UnicodeFiles.DERIVED_CORE_PROPERTIES), want_derived)\n+\n+        # TODO scripts not used?\n+        scripts = load_properties(get_path(UnicodeFiles.SCRIPTS), [])\n+        props = load_properties(get_path(UnicodeFiles.PROPS),\n+                                [\"White_Space\", \"Join_Control\", \"Noncharacter_Code_Point\",\n+                                 \"Pattern_White_Space\"])\n+        norm_props = load_properties(get_path(UnicodeFiles.DERIVED_NORMALIZATION_PROPS),\n+                                     [\"Full_Composition_Exclusion\"])\n \n         # category tables\n-        for (name, cat, pfuns) in (\"general_category\", gencats, [\"N\", \"Cc\"]), \\\n-                                  (\"derived_property\", derived, want_derived), \\\n-                                  (\"property\", props, [\"White_Space\", \"Pattern_White_Space\"]):\n+        for (name, cat, pfuns) in ((\"general_category\", unicode_data.gencats, [\"N\", \"Cc\"]),\n+                                   (\"derived_property\", derived, want_derived),\n+                                   (\"property\", props, [\"White_Space\", \"Pattern_White_Space\"])):\n             emit_property_module(rf, name, cat, pfuns)\n \n         # normalizations and conversions module\n-        emit_norm_module(rf, canon_decomp, compat_decomp, combines, norm_props)\n-        emit_conversions_module(rf, to_upper, to_lower, to_title)\n+        emit_norm_module(rf, unicode_data, norm_props)\n+        emit_conversions_module(rf, unicode_data)\n+\n     print(\"Regenerated tables.rs.\")\n+\n+\n+if __name__ == \"__main__\":\n+    main()"}]}