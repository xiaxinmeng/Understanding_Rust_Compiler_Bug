{"sha": "ee12b0f173c9780df95df801c8d418245da752b2", "node_id": "C_kwDOAAsO6NoAKGVlMTJiMGYxNzNjOTc4MGRmOTVkZjgwMWM4ZDQxODI0NWRhNzUyYjI", "commit": {"author": {"name": "bors[bot]", "email": "26634292+bors[bot]@users.noreply.github.com", "date": "2021-09-29T13:13:03Z"}, "committer": {"name": "GitHub", "email": "noreply@github.com", "date": "2021-09-29T13:13:03Z"}, "message": "Merge #10181\n\n10181: Begining of lsif r=HKalbasi a=HKalbasi\n\nThis PR adds a `lsif` command to cli, which can be used as `rust-analyzer lsif /path/to/project > dump.lsif`. It now generates a valid, but pretty useless lsif (only supports folding ranges). The propose of this PR is to discussing about the structure of lsif generator, before starting anything serious.\r\n\r\ncc `@matklad` #8696 #3098\r\n\n\nCo-authored-by: hamidreza kalbasi <hamidrezakalbasi@protonmail.com>", "tree": {"sha": "15c2750b0b4c5c0c443d73f4d808c8c851189a1e", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/15c2750b0b4c5c0c443d73f4d808c8c851189a1e"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/ee12b0f173c9780df95df801c8d418245da752b2", "comment_count": 0, "verification": {"verified": true, "reason": "valid", "signature": "-----BEGIN PGP SIGNATURE-----\n\nwsBcBAABCAAQBQJhVGZfCRBK7hj4Ov3rIwAAq20IAD9KaRqES3VWlpk33tmfkcjJ\nY2gbrBg5P18SsejjgqeXyp6t/YebzcVF/86t6Vx8i3xb28ZkGZy1l9+Rm4JsGTJM\nSzAOc4y1uI3xOKxZtHWhiCaLb0gB2VGNnz3MEoR0B64qbWCRvecWr50/CdMVZUeq\nXAm/xs1S+d2bHEmTURgV2GftxlncHBq+rKhE90Zr3TWxWmAEjMOcWsBVyCSYP0ig\nkJIEAv2dwqw9UXk/WIDmrorirFAnDorKsjJNVLm1KbT1la3BnPaaYh765g+bC+Pb\njMqkiCT/dAdEBiFgxRSXLKIGk6FVmxIAfSSH55X65ZKtb2eZjGRiY5XFEhEz0S4=\n=a0dR\n-----END PGP SIGNATURE-----\n", "payload": "tree 15c2750b0b4c5c0c443d73f4d808c8c851189a1e\nparent 1bd14e05304a5daff556875cf476f9afb26a766a\nparent 5bd0f50111cce297b397c6549427e78c7c1da5c0\nauthor bors[bot] <26634292+bors[bot]@users.noreply.github.com> 1632921183 +0000\ncommitter GitHub <noreply@github.com> 1632921183 +0000\n\nMerge #10181\n\n10181: Begining of lsif r=HKalbasi a=HKalbasi\n\nThis PR adds a `lsif` command to cli, which can be used as `rust-analyzer lsif /path/to/project > dump.lsif`. It now generates a valid, but pretty useless lsif (only supports folding ranges). The propose of this PR is to discussing about the structure of lsif generator, before starting anything serious.\r\n\r\ncc `@matklad` #8696 #3098\r\n\n\nCo-authored-by: hamidreza kalbasi <hamidrezakalbasi@protonmail.com>\n"}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/ee12b0f173c9780df95df801c8d418245da752b2", "html_url": "https://github.com/rust-lang/rust/commit/ee12b0f173c9780df95df801c8d418245da752b2", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/ee12b0f173c9780df95df801c8d418245da752b2/comments", "author": {"login": "bors[bot]", "id": 26634292, "node_id": "MDM6Qm90MjY2MzQyOTI=", "avatar_url": "https://avatars.githubusercontent.com/in/1847?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bors%5Bbot%5D", "html_url": "https://github.com/apps/bors", "followers_url": "https://api.github.com/users/bors%5Bbot%5D/followers", "following_url": "https://api.github.com/users/bors%5Bbot%5D/following{/other_user}", "gists_url": "https://api.github.com/users/bors%5Bbot%5D/gists{/gist_id}", "starred_url": "https://api.github.com/users/bors%5Bbot%5D/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bors%5Bbot%5D/subscriptions", "organizations_url": "https://api.github.com/users/bors%5Bbot%5D/orgs", "repos_url": "https://api.github.com/users/bors%5Bbot%5D/repos", "events_url": "https://api.github.com/users/bors%5Bbot%5D/events{/privacy}", "received_events_url": "https://api.github.com/users/bors%5Bbot%5D/received_events", "type": "Bot", "site_admin": false}, "committer": {"login": "web-flow", "id": 19864447, "node_id": "MDQ6VXNlcjE5ODY0NDQ3", "avatar_url": "https://avatars.githubusercontent.com/u/19864447?v=4", "gravatar_id": "", "url": "https://api.github.com/users/web-flow", "html_url": "https://github.com/web-flow", "followers_url": "https://api.github.com/users/web-flow/followers", "following_url": "https://api.github.com/users/web-flow/following{/other_user}", "gists_url": "https://api.github.com/users/web-flow/gists{/gist_id}", "starred_url": "https://api.github.com/users/web-flow/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/web-flow/subscriptions", "organizations_url": "https://api.github.com/users/web-flow/orgs", "repos_url": "https://api.github.com/users/web-flow/repos", "events_url": "https://api.github.com/users/web-flow/events{/privacy}", "received_events_url": "https://api.github.com/users/web-flow/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "1bd14e05304a5daff556875cf476f9afb26a766a", "url": "https://api.github.com/repos/rust-lang/rust/commits/1bd14e05304a5daff556875cf476f9afb26a766a", "html_url": "https://github.com/rust-lang/rust/commit/1bd14e05304a5daff556875cf476f9afb26a766a"}, {"sha": "5bd0f50111cce297b397c6549427e78c7c1da5c0", "url": "https://api.github.com/repos/rust-lang/rust/commits/5bd0f50111cce297b397c6549427e78c7c1da5c0", "html_url": "https://github.com/rust-lang/rust/commit/5bd0f50111cce297b397c6549427e78c7c1da5c0"}], "stats": {"total": 567, "additions": 562, "deletions": 5}, "files": [{"sha": "f3e3cab1d6e1436d14eb1612c6814c3244ea5e19", "filename": ".gitignore", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/ee12b0f173c9780df95df801c8d418245da752b2/.gitignore", "raw_url": "https://github.com/rust-lang/rust/raw/ee12b0f173c9780df95df801c8d418245da752b2/.gitignore", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/.gitignore?ref=ee12b0f173c9780df95df801c8d418245da752b2", "patch": "@@ -11,3 +11,5 @@ generated_assists.adoc\n generated_features.adoc\n generated_diagnostic.adoc\n .DS_Store\n+/out/\n+/dump.lsif"}, {"sha": "472862770184e317e479081e7d626b957156e627", "filename": "Cargo.lock", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/rust-lang/rust/blob/ee12b0f173c9780df95df801c8d418245da752b2/Cargo.lock", "raw_url": "https://github.com/rust-lang/rust/raw/ee12b0f173c9780df95df801c8d418245da752b2/Cargo.lock", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/Cargo.lock?ref=ee12b0f173c9780df95df801c8d418245da752b2", "patch": "@@ -862,9 +862,9 @@ dependencies = [\n \n [[package]]\n name = \"lsp-types\"\n-version = \"0.89.2\"\n+version = \"0.90.0\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"852e0dedfd52cc32325598b2631e0eba31b7b708959676a9f837042f276b09a2\"\n+checksum = \"a7404037aab080771c90b0a499836d9d8a10336ecd07badf969567b65c6d51a1\"\n dependencies = [\n  \"bitflags\",\n  \"serde\","}, {"sha": "2ea6f6a9ab1b6add03a095d35fbfc357d727c995", "filename": "crates/ide/src/fixture.rs", "status": "modified", "additions": 19, "deletions": 0, "changes": 19, "blob_url": "https://github.com/rust-lang/rust/blob/ee12b0f173c9780df95df801c8d418245da752b2/crates%2Fide%2Fsrc%2Ffixture.rs", "raw_url": "https://github.com/rust-lang/rust/raw/ee12b0f173c9780df95df801c8d418245da752b2/crates%2Fide%2Fsrc%2Ffixture.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fide%2Fsrc%2Ffixture.rs?ref=ee12b0f173c9780df95df801c8d418245da752b2", "patch": "@@ -66,3 +66,22 @@ pub(crate) fn annotations(ra_fixture: &str) -> (Analysis, FilePosition, Vec<(Fil\n         .collect();\n     (host.analysis(), FilePosition { file_id, offset }, annotations)\n }\n+\n+/// Creates analysis from a multi-file fixture with annonations without $0\n+pub(crate) fn annotations_without_marker(ra_fixture: &str) -> (Analysis, Vec<(FileRange, String)>) {\n+    let mut host = AnalysisHost::default();\n+    let change_fixture = ChangeFixture::parse(ra_fixture);\n+    host.db.set_enable_proc_attr_macros(true);\n+    host.db.apply_change(change_fixture.change);\n+\n+    let annotations = change_fixture\n+        .files\n+        .iter()\n+        .flat_map(|&file_id| {\n+            let file_text = host.analysis().file_text(file_id).unwrap();\n+            let annotations = extract_annotations(&file_text);\n+            annotations.into_iter().map(move |(range, data)| (FileRange { file_id, range }, data))\n+        })\n+        .collect();\n+    (host.analysis(), annotations)\n+}"}, {"sha": "dbfa99bdf240174355b0e049ad1abefc3610a450", "filename": "crates/ide/src/lib.rs", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/ee12b0f173c9780df95df801c8d418245da752b2/crates%2Fide%2Fsrc%2Flib.rs", "raw_url": "https://github.com/rust-lang/rust/raw/ee12b0f173c9780df95df801c8d418245da752b2/crates%2Fide%2Fsrc%2Flib.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fide%2Fsrc%2Flib.rs?ref=ee12b0f173c9780df95df801c8d418245da752b2", "patch": "@@ -46,6 +46,7 @@ mod references;\n mod rename;\n mod runnables;\n mod ssr;\n+mod static_index;\n mod status;\n mod syntax_highlighting;\n mod syntax_tree;\n@@ -86,6 +87,7 @@ pub use crate::{\n     references::ReferenceSearchResult,\n     rename::RenameError,\n     runnables::{Runnable, RunnableKind, TestId},\n+    static_index::{StaticIndex, StaticIndexedFile, TokenId, TokenStaticData},\n     syntax_highlighting::{\n         tags::{Highlight, HlMod, HlMods, HlOperator, HlPunct, HlTag},\n         HlRange,"}, {"sha": "aa62e2eae5a3e594904d1cf8f7fd3c5e159fb0ce", "filename": "crates/ide/src/static_index.rs", "status": "added", "additions": 256, "deletions": 0, "changes": 256, "blob_url": "https://github.com/rust-lang/rust/blob/ee12b0f173c9780df95df801c8d418245da752b2/crates%2Fide%2Fsrc%2Fstatic_index.rs", "raw_url": "https://github.com/rust-lang/rust/raw/ee12b0f173c9780df95df801c8d418245da752b2/crates%2Fide%2Fsrc%2Fstatic_index.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fide%2Fsrc%2Fstatic_index.rs?ref=ee12b0f173c9780df95df801c8d418245da752b2", "patch": "@@ -0,0 +1,256 @@\n+//! This module provides `StaticIndex` which is used for powering\n+//! read-only code browsers and emitting LSIF\n+\n+use std::collections::HashMap;\n+\n+use hir::Semantics;\n+use hir::{db::HirDatabase, Crate, Module};\n+use ide_db::base_db::{FileId, FileRange, SourceDatabaseExt};\n+use ide_db::defs::Definition;\n+use ide_db::RootDatabase;\n+use rustc_hash::FxHashSet;\n+use syntax::{AstNode, SyntaxKind::*, T};\n+use syntax::{SyntaxToken, TextRange};\n+\n+use crate::display::TryToNav;\n+use crate::hover::hover_for_definition;\n+use crate::{Analysis, Fold, HoverConfig, HoverDocFormat, HoverResult};\n+\n+/// A static representation of fully analyzed source code.\n+///\n+/// The intended use-case is powering read-only code browsers and emitting LSIF\n+pub struct StaticIndex<'a> {\n+    pub files: Vec<StaticIndexedFile>,\n+    pub tokens: TokenStore,\n+    analysis: &'a Analysis,\n+    db: &'a RootDatabase,\n+    def_map: HashMap<Definition, TokenId>,\n+}\n+\n+pub struct ReferenceData {\n+    pub range: FileRange,\n+    pub is_definition: bool,\n+}\n+\n+pub struct TokenStaticData {\n+    pub hover: Option<HoverResult>,\n+    pub definition: Option<FileRange>,\n+    pub references: Vec<ReferenceData>,\n+}\n+\n+#[derive(Clone, Copy, PartialEq, Eq, Hash)]\n+pub struct TokenId(usize);\n+\n+#[derive(Default)]\n+pub struct TokenStore(Vec<TokenStaticData>);\n+\n+impl TokenStore {\n+    pub fn insert(&mut self, data: TokenStaticData) -> TokenId {\n+        let id = TokenId(self.0.len());\n+        self.0.push(data);\n+        id\n+    }\n+\n+    pub fn get_mut(&mut self, id: TokenId) -> Option<&mut TokenStaticData> {\n+        self.0.get_mut(id.0)\n+    }\n+\n+    pub fn get(&self, id: TokenId) -> Option<&TokenStaticData> {\n+        self.0.get(id.0)\n+    }\n+\n+    pub fn iter(self) -> impl Iterator<Item = (TokenId, TokenStaticData)> {\n+        self.0.into_iter().enumerate().map(|(i, x)| (TokenId(i), x))\n+    }\n+}\n+\n+pub struct StaticIndexedFile {\n+    pub file_id: FileId,\n+    pub folds: Vec<Fold>,\n+    pub tokens: Vec<(TextRange, TokenId)>,\n+}\n+\n+fn all_modules(db: &dyn HirDatabase) -> Vec<Module> {\n+    let mut worklist: Vec<_> =\n+        Crate::all(db).into_iter().map(|krate| krate.root_module(db)).collect();\n+    let mut modules = Vec::new();\n+\n+    while let Some(module) = worklist.pop() {\n+        modules.push(module);\n+        worklist.extend(module.children(db));\n+    }\n+\n+    modules\n+}\n+\n+impl StaticIndex<'_> {\n+    fn add_file(&mut self, file_id: FileId) {\n+        let folds = self.analysis.folding_ranges(file_id).unwrap();\n+        // hovers\n+        let sema = hir::Semantics::new(self.db);\n+        let tokens_or_nodes = sema.parse(file_id).syntax().clone();\n+        let tokens = tokens_or_nodes.descendants_with_tokens().filter_map(|x| match x {\n+            syntax::NodeOrToken::Node(_) => None,\n+            syntax::NodeOrToken::Token(x) => Some(x),\n+        });\n+        let hover_config =\n+            HoverConfig { links_in_hover: true, documentation: Some(HoverDocFormat::Markdown) };\n+        let tokens = tokens.filter(|token| match token.kind() {\n+            IDENT | INT_NUMBER | LIFETIME_IDENT | T![self] | T![super] | T![crate] => true,\n+            _ => false,\n+        });\n+        let mut result = StaticIndexedFile { file_id, folds, tokens: vec![] };\n+        for token in tokens {\n+            let range = token.text_range();\n+            let node = token.parent().unwrap();\n+            let def = if let Some(x) = get_definition(&sema, token.clone()) {\n+                x\n+            } else {\n+                continue;\n+            };\n+            let id = if let Some(x) = self.def_map.get(&def) {\n+                *x\n+            } else {\n+                let x = self.tokens.insert(TokenStaticData {\n+                    hover: hover_for_definition(&sema, file_id, def, &node, &hover_config),\n+                    definition: def\n+                        .try_to_nav(self.db)\n+                        .map(|x| FileRange { file_id: x.file_id, range: x.focus_or_full_range() }),\n+                    references: vec![],\n+                });\n+                self.def_map.insert(def, x);\n+                x\n+            };\n+            let token = self.tokens.get_mut(id).unwrap();\n+            token.references.push(ReferenceData {\n+                range: FileRange { range, file_id },\n+                is_definition: if let Some(x) = def.try_to_nav(self.db) {\n+                    x.file_id == file_id && x.focus_or_full_range() == range\n+                } else {\n+                    false\n+                },\n+            });\n+            result.tokens.push((range, id));\n+        }\n+        self.files.push(result);\n+    }\n+\n+    pub fn compute<'a>(db: &'a RootDatabase, analysis: &'a Analysis) -> StaticIndex<'a> {\n+        let work = all_modules(db).into_iter().filter(|module| {\n+            let file_id = module.definition_source(db).file_id.original_file(db);\n+            let source_root = db.file_source_root(file_id);\n+            let source_root = db.source_root(source_root);\n+            !source_root.is_library\n+        });\n+        let mut this = StaticIndex {\n+            files: vec![],\n+            tokens: Default::default(),\n+            analysis,\n+            db,\n+            def_map: Default::default(),\n+        };\n+        let mut visited_files = FxHashSet::default();\n+        for module in work {\n+            let file_id = module.definition_source(db).file_id.original_file(db);\n+            if visited_files.contains(&file_id) {\n+                continue;\n+            }\n+            this.add_file(file_id);\n+            // mark the file\n+            visited_files.insert(file_id);\n+        }\n+        this\n+    }\n+}\n+\n+fn get_definition(sema: &Semantics<RootDatabase>, token: SyntaxToken) -> Option<Definition> {\n+    for token in sema.descend_into_macros_many(token) {\n+        let def = Definition::from_token(&sema, &token);\n+        if let [x] = def.as_slice() {\n+            return Some(*x);\n+        } else {\n+            continue;\n+        };\n+    }\n+    None\n+}\n+\n+#[cfg(test)]\n+mod tests {\n+    use crate::{fixture, StaticIndex};\n+    use ide_db::base_db::FileRange;\n+    use std::collections::HashSet;\n+\n+    fn check_all_ranges(ra_fixture: &str) {\n+        let (analysis, ranges) = fixture::annotations_without_marker(ra_fixture);\n+        let s = StaticIndex::compute(&*analysis.db, &analysis);\n+        let mut range_set: HashSet<_> = ranges.iter().map(|x| x.0).collect();\n+        for f in s.files {\n+            for (range, _) in f.tokens {\n+                let x = FileRange { file_id: f.file_id, range };\n+                if !range_set.contains(&x) {\n+                    panic!(\"additional range {:?}\", x);\n+                }\n+                range_set.remove(&x);\n+            }\n+        }\n+        if !range_set.is_empty() {\n+            panic!(\"unfound ranges {:?}\", range_set);\n+        }\n+    }\n+\n+    fn check_definitions(ra_fixture: &str) {\n+        let (analysis, ranges) = fixture::annotations_without_marker(ra_fixture);\n+        let s = StaticIndex::compute(&*analysis.db, &analysis);\n+        let mut range_set: HashSet<_> = ranges.iter().map(|x| x.0).collect();\n+        for (_, t) in s.tokens.iter() {\n+            if let Some(x) = t.definition {\n+                if !range_set.contains(&x) {\n+                    panic!(\"additional definition {:?}\", x);\n+                }\n+                range_set.remove(&x);\n+            }\n+        }\n+        if !range_set.is_empty() {\n+            panic!(\"unfound definitions {:?}\", range_set);\n+        }\n+    }\n+\n+    #[test]\n+    fn struct_and_enum() {\n+        check_all_ranges(\n+            r#\"\n+struct Foo;\n+     //^^^\n+enum E { X(Foo) }\n+   //^   ^ ^^^\n+\"#,\n+        );\n+        check_definitions(\n+            r#\"\n+struct Foo;\n+     //^^^\n+enum E { X(Foo) }\n+   //^   ^\n+\"#,\n+        );\n+    }\n+\n+    #[test]\n+    fn derives() {\n+        check_all_ranges(\n+            r#\"\n+#[rustc_builtin_macro]\n+pub macro Copy {}\n+        //^^^^\n+#[rustc_builtin_macro]\n+pub macro derive {}\n+        //^^^^^^\n+#[derive(Copy)]\n+//^^^^^^ ^^^^\n+struct Hello(i32);\n+     //^^^^^ ^^^\n+\"#,\n+        );\n+    }\n+}"}, {"sha": "29a3fbb67e069c56688d4d2a73ed0257e35dba17", "filename": "crates/rust-analyzer/Cargo.toml", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/ee12b0f173c9780df95df801c8d418245da752b2/crates%2Frust-analyzer%2FCargo.toml", "raw_url": "https://github.com/rust-lang/rust/raw/ee12b0f173c9780df95df801c8d418245da752b2/crates%2Frust-analyzer%2FCargo.toml", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Frust-analyzer%2FCargo.toml?ref=ee12b0f173c9780df95df801c8d418245da752b2", "patch": "@@ -22,7 +22,7 @@ crossbeam-channel = \"0.5.0\"\n dissimilar = \"1.0.2\"\n itertools = \"0.10.0\"\n jod-thread = \"0.1.0\"\n-lsp-types = { version = \"0.89.0\", features = [\"proposed\"] }\n+lsp-types = { version = \"0.90.0\", features = [\"proposed\"] }\n parking_lot = \"0.11.0\"\n xflags = \"0.2.1\"\n oorandom = \"11.1.2\""}, {"sha": "74c041020be1e9deef084c2b4a99457062a0307d", "filename": "crates/rust-analyzer/src/bin/main.rs", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "blob_url": "https://github.com/rust-lang/rust/blob/ee12b0f173c9780df95df801c8d418245da752b2/crates%2Frust-analyzer%2Fsrc%2Fbin%2Fmain.rs", "raw_url": "https://github.com/rust-lang/rust/raw/ee12b0f173c9780df95df801c8d418245da752b2/crates%2Frust-analyzer%2Fsrc%2Fbin%2Fmain.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Frust-analyzer%2Fsrc%2Fbin%2Fmain.rs?ref=ee12b0f173c9780df95df801c8d418245da752b2", "patch": "@@ -87,6 +87,7 @@ fn try_main() -> Result<()> {\n         flags::RustAnalyzerCmd::Diagnostics(cmd) => cmd.run()?,\n         flags::RustAnalyzerCmd::Ssr(cmd) => cmd.run()?,\n         flags::RustAnalyzerCmd::Search(cmd) => cmd.run()?,\n+        flags::RustAnalyzerCmd::Lsif(cmd) => cmd.run()?,\n     }\n     Ok(())\n }"}, {"sha": "6ccdaa86dd628786d33a9142e3b199f2dbd5bfa2", "filename": "crates/rust-analyzer/src/cli.rs", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "blob_url": "https://github.com/rust-lang/rust/blob/ee12b0f173c9780df95df801c8d418245da752b2/crates%2Frust-analyzer%2Fsrc%2Fcli.rs", "raw_url": "https://github.com/rust-lang/rust/raw/ee12b0f173c9780df95df801c8d418245da752b2/crates%2Frust-analyzer%2Fsrc%2Fcli.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Frust-analyzer%2Fsrc%2Fcli.rs?ref=ee12b0f173c9780df95df801c8d418245da752b2", "patch": "@@ -8,6 +8,7 @@ mod highlight;\n mod analysis_stats;\n mod diagnostics;\n mod ssr;\n+mod lsif;\n \n mod progress_report;\n "}, {"sha": "55a542c3c16d50752d3ccd3655b7ecccdfdffc03", "filename": "crates/rust-analyzer/src/cli/analysis_stats.rs", "status": "modified", "additions": 0, "deletions": 2, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/ee12b0f173c9780df95df801c8d418245da752b2/crates%2Frust-analyzer%2Fsrc%2Fcli%2Fanalysis_stats.rs", "raw_url": "https://github.com/rust-lang/rust/raw/ee12b0f173c9780df95df801c8d418245da752b2/crates%2Frust-analyzer%2Fsrc%2Fcli%2Fanalysis_stats.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Frust-analyzer%2Fsrc%2Fcli%2Fanalysis_stats.rs?ref=ee12b0f173c9780df95df801c8d418245da752b2", "patch": "@@ -367,8 +367,6 @@ fn expr_syntax_range(\n ) -> Option<(VfsPath, LineCol, LineCol)> {\n     let src = sm.expr_syntax(expr_id);\n     if let Ok(src) = src {\n-        // FIXME: it might be nice to have a function (on Analysis?) that goes from Source<T> -> (LineCol, LineCol) directly\n-        // But also, we should just turn the type mismatches into diagnostics and provide these\n         let root = db.parse_or_expand(src.file_id).unwrap();\n         let node = src.map(|e| e.to_node(&root).syntax().clone());\n         let original_range = node.as_ref().original_file_range(db);"}, {"sha": "b759d912c968b437fff810fe99cd761282db59e9", "filename": "crates/rust-analyzer/src/cli/flags.rs", "status": "modified", "additions": 10, "deletions": 0, "changes": 10, "blob_url": "https://github.com/rust-lang/rust/blob/ee12b0f173c9780df95df801c8d418245da752b2/crates%2Frust-analyzer%2Fsrc%2Fcli%2Fflags.rs", "raw_url": "https://github.com/rust-lang/rust/raw/ee12b0f173c9780df95df801c8d418245da752b2/crates%2Frust-analyzer%2Fsrc%2Fcli%2Fflags.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Frust-analyzer%2Fsrc%2Fcli%2Fflags.rs?ref=ee12b0f173c9780df95df801c8d418245da752b2", "patch": "@@ -102,6 +102,10 @@ xflags::xflags! {\n         }\n \n         cmd proc-macro {}\n+\n+        cmd lsif\n+            required path: PathBuf\n+        {}\n     }\n }\n \n@@ -129,6 +133,7 @@ pub enum RustAnalyzerCmd {\n     Ssr(Ssr),\n     Search(Search),\n     ProcMacro(ProcMacro),\n+    Lsif(Lsif),\n }\n \n #[derive(Debug)]\n@@ -190,6 +195,11 @@ pub struct Search {\n #[derive(Debug)]\n pub struct ProcMacro;\n \n+#[derive(Debug)]\n+pub struct Lsif {\n+    pub path: PathBuf,\n+}\n+\n impl RustAnalyzer {\n     pub const HELP: &'static str = Self::HELP_;\n "}, {"sha": "a2354431460fb2676539de67f78c2e4d43123151", "filename": "crates/rust-analyzer/src/cli/lsif.rs", "status": "added", "additions": 268, "deletions": 0, "changes": 268, "blob_url": "https://github.com/rust-lang/rust/blob/ee12b0f173c9780df95df801c8d418245da752b2/crates%2Frust-analyzer%2Fsrc%2Fcli%2Flsif.rs", "raw_url": "https://github.com/rust-lang/rust/raw/ee12b0f173c9780df95df801c8d418245da752b2/crates%2Frust-analyzer%2Fsrc%2Fcli%2Flsif.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Frust-analyzer%2Fsrc%2Fcli%2Flsif.rs?ref=ee12b0f173c9780df95df801c8d418245da752b2", "patch": "@@ -0,0 +1,268 @@\n+//! LSIF (language server index format) generator\n+\n+use std::collections::HashMap;\n+use std::env;\n+use std::time::Instant;\n+\n+use ide::{\n+    Analysis, FileId, FileRange, RootDatabase, StaticIndex, StaticIndexedFile, TokenId,\n+    TokenStaticData,\n+};\n+use ide_db::LineIndexDatabase;\n+\n+use ide_db::base_db::salsa::{self, ParallelDatabase};\n+use lsp_types::{self, lsif};\n+use project_model::{CargoConfig, ProjectManifest, ProjectWorkspace};\n+use vfs::{AbsPathBuf, Vfs};\n+\n+use crate::cli::{\n+    flags,\n+    load_cargo::{load_workspace, LoadCargoConfig},\n+    Result,\n+};\n+use crate::line_index::{LineEndings, LineIndex, OffsetEncoding};\n+use crate::to_proto;\n+\n+/// Need to wrap Snapshot to provide `Clone` impl for `map_with`\n+struct Snap<DB>(DB);\n+impl<DB: ParallelDatabase> Clone for Snap<salsa::Snapshot<DB>> {\n+    fn clone(&self) -> Snap<salsa::Snapshot<DB>> {\n+        Snap(self.0.snapshot())\n+    }\n+}\n+\n+struct LsifManager<'a> {\n+    count: i32,\n+    token_map: HashMap<TokenId, Id>,\n+    range_map: HashMap<FileRange, Id>,\n+    file_map: HashMap<FileId, Id>,\n+    analysis: &'a Analysis,\n+    db: &'a RootDatabase,\n+    vfs: &'a Vfs,\n+}\n+\n+#[derive(Clone, Copy)]\n+struct Id(i32);\n+\n+impl From<Id> for lsp_types::NumberOrString {\n+    fn from(Id(x): Id) -> Self {\n+        lsp_types::NumberOrString::Number(x)\n+    }\n+}\n+\n+impl LsifManager<'_> {\n+    fn new<'a>(analysis: &'a Analysis, db: &'a RootDatabase, vfs: &'a Vfs) -> LsifManager<'a> {\n+        LsifManager {\n+            count: 0,\n+            token_map: HashMap::default(),\n+            range_map: HashMap::default(),\n+            file_map: HashMap::default(),\n+            analysis,\n+            db,\n+            vfs,\n+        }\n+    }\n+\n+    fn add(&mut self, data: lsif::Element) -> Id {\n+        let id = Id(self.count);\n+        self.emit(&serde_json::to_string(&lsif::Entry { id: id.into(), data }).unwrap());\n+        self.count += 1;\n+        id\n+    }\n+\n+    fn add_vertex(&mut self, vertex: lsif::Vertex) -> Id {\n+        self.add(lsif::Element::Vertex(vertex))\n+    }\n+\n+    fn add_edge(&mut self, edge: lsif::Edge) -> Id {\n+        self.add(lsif::Element::Edge(edge))\n+    }\n+\n+    // FIXME: support file in addition to stdout here\n+    fn emit(&self, data: &str) {\n+        println!(\"{}\", data);\n+    }\n+\n+    fn get_token_id(&mut self, id: TokenId) -> Id {\n+        if let Some(x) = self.token_map.get(&id) {\n+            return *x;\n+        }\n+        let result_set_id = self.add_vertex(lsif::Vertex::ResultSet(lsif::ResultSet { key: None }));\n+        self.token_map.insert(id, result_set_id);\n+        result_set_id\n+    }\n+\n+    fn get_range_id(&mut self, id: FileRange) -> Id {\n+        if let Some(x) = self.range_map.get(&id) {\n+            return *x;\n+        }\n+        let file_id = id.file_id;\n+        let doc_id = self.get_file_id(file_id);\n+        let line_index = self.db.line_index(file_id);\n+        let line_index = LineIndex {\n+            index: line_index.clone(),\n+            encoding: OffsetEncoding::Utf16,\n+            endings: LineEndings::Unix,\n+        };\n+        let range_id = self.add_vertex(lsif::Vertex::Range {\n+            range: to_proto::range(&line_index, id.range),\n+            tag: None,\n+        });\n+        self.add_edge(lsif::Edge::Contains(lsif::EdgeDataMultiIn {\n+            in_vs: vec![range_id.into()],\n+            out_v: doc_id.into(),\n+        }));\n+        range_id\n+    }\n+\n+    fn get_file_id(&mut self, id: FileId) -> Id {\n+        if let Some(x) = self.file_map.get(&id) {\n+            return *x;\n+        }\n+        let path = self.vfs.file_path(id);\n+        let path = path.as_path().unwrap();\n+        let doc_id = self.add_vertex(lsif::Vertex::Document(lsif::Document {\n+            language_id: \"rust\".to_string(),\n+            uri: lsp_types::Url::from_file_path(path).unwrap(),\n+        }));\n+        self.file_map.insert(id, doc_id);\n+        doc_id\n+    }\n+\n+    fn add_token(&mut self, id: TokenId, token: TokenStaticData) {\n+        let result_set_id = self.get_token_id(id);\n+        if let Some(hover) = token.hover {\n+            let hover_id = self.add_vertex(lsif::Vertex::HoverResult {\n+                result: lsp_types::Hover {\n+                    contents: lsp_types::HoverContents::Markup(to_proto::markup_content(\n+                        hover.markup,\n+                    )),\n+                    range: None,\n+                },\n+            });\n+            self.add_edge(lsif::Edge::Hover(lsif::EdgeData {\n+                in_v: hover_id.into(),\n+                out_v: result_set_id.into(),\n+            }));\n+        }\n+        if let Some(def) = token.definition {\n+            let result_id = self.add_vertex(lsif::Vertex::DefinitionResult);\n+            let def_vertex = self.get_range_id(def);\n+            self.add_edge(lsif::Edge::Item(lsif::Item {\n+                document: (*self.file_map.get(&def.file_id).unwrap()).into(),\n+                property: None,\n+                edge_data: lsif::EdgeDataMultiIn {\n+                    in_vs: vec![def_vertex.into()],\n+                    out_v: result_id.into(),\n+                },\n+            }));\n+            self.add_edge(lsif::Edge::Definition(lsif::EdgeData {\n+                in_v: result_id.into(),\n+                out_v: result_set_id.into(),\n+            }));\n+        }\n+        if !token.references.is_empty() {\n+            let result_id = self.add_vertex(lsif::Vertex::ReferenceResult);\n+            self.add_edge(lsif::Edge::References(lsif::EdgeData {\n+                in_v: result_id.into(),\n+                out_v: result_set_id.into(),\n+            }));\n+            for x in token.references {\n+                let vertex = *self.range_map.get(&x.range).unwrap();\n+                self.add_edge(lsif::Edge::Item(lsif::Item {\n+                    document: (*self.file_map.get(&x.range.file_id).unwrap()).into(),\n+                    property: Some(if x.is_definition {\n+                        lsif::ItemKind::Definitions\n+                    } else {\n+                        lsif::ItemKind::References\n+                    }),\n+                    edge_data: lsif::EdgeDataMultiIn {\n+                        in_vs: vec![vertex.into()],\n+                        out_v: result_id.into(),\n+                    },\n+                }));\n+            }\n+        }\n+    }\n+\n+    fn add_file(&mut self, file: StaticIndexedFile) {\n+        let StaticIndexedFile { file_id, tokens, folds } = file;\n+        let doc_id = self.get_file_id(file_id);\n+        let text = self.analysis.file_text(file_id).unwrap();\n+        let line_index = self.db.line_index(file_id);\n+        let line_index = LineIndex {\n+            index: line_index.clone(),\n+            encoding: OffsetEncoding::Utf16,\n+            endings: LineEndings::Unix,\n+        };\n+        let result = folds\n+            .into_iter()\n+            .map(|it| to_proto::folding_range(&*text, &line_index, false, it))\n+            .collect();\n+        let folding_id = self.add_vertex(lsif::Vertex::FoldingRangeResult { result });\n+        self.add_edge(lsif::Edge::FoldingRange(lsif::EdgeData {\n+            in_v: folding_id.into(),\n+            out_v: doc_id.into(),\n+        }));\n+        let tokens_id = tokens\n+            .into_iter()\n+            .map(|(range, id)| {\n+                let range_id = self.add_vertex(lsif::Vertex::Range {\n+                    range: to_proto::range(&line_index, range),\n+                    tag: None,\n+                });\n+                self.range_map.insert(FileRange { file_id, range }, range_id);\n+                let result_set_id = self.get_token_id(id);\n+                self.add_edge(lsif::Edge::Next(lsif::EdgeData {\n+                    in_v: result_set_id.into(),\n+                    out_v: range_id.into(),\n+                }));\n+                range_id.into()\n+            })\n+            .collect();\n+        self.add_edge(lsif::Edge::Contains(lsif::EdgeDataMultiIn {\n+            in_vs: tokens_id,\n+            out_v: doc_id.into(),\n+        }));\n+    }\n+}\n+\n+impl flags::Lsif {\n+    pub fn run(self) -> Result<()> {\n+        eprintln!(\"Generating LSIF started...\");\n+        let now = Instant::now();\n+        let cargo_config = CargoConfig::default();\n+        let no_progress = &|_| ();\n+        let load_cargo_config = LoadCargoConfig {\n+            load_out_dirs_from_check: true,\n+            with_proc_macro: true,\n+            prefill_caches: false,\n+        };\n+        let path = AbsPathBuf::assert(env::current_dir()?.join(&self.path));\n+        let manifest = ProjectManifest::discover_single(&path)?;\n+\n+        let workspace = ProjectWorkspace::load(manifest, &cargo_config, no_progress)?;\n+\n+        let (host, vfs, _proc_macro) = load_workspace(workspace, &load_cargo_config)?;\n+        let db = host.raw_database();\n+        let analysis = host.analysis();\n+\n+        let si = StaticIndex::compute(db, &analysis);\n+\n+        let mut lsif = LsifManager::new(&analysis, db, &vfs);\n+        lsif.add_vertex(lsif::Vertex::MetaData(lsif::MetaData {\n+            version: String::from(\"0.5.0\"),\n+            project_root: lsp_types::Url::from_file_path(path).unwrap(),\n+            position_encoding: lsif::Encoding::Utf16,\n+            tool_info: None,\n+        }));\n+        for file in si.files {\n+            lsif.add_file(file);\n+        }\n+        for (id, token) in si.tokens.iter() {\n+            lsif.add_token(id, token);\n+        }\n+        eprintln!(\"Generating LSIF finished in {:?}\", now.elapsed());\n+        Ok(())\n+    }\n+}"}]}