{"sha": "a1bd5d359b8e4ebc1b952f96e4f1d4658bc29b26", "node_id": "MDY6Q29tbWl0NzI0NzEyOmExYmQ1ZDM1OWI4ZTRlYmMxYjk1MmY5NmU0ZjFkNDY1OGJjMjliMjY=", "commit": {"author": {"name": "bors", "email": "bors@rust-lang.org", "date": "2014-07-10T11:01:32Z"}, "committer": {"name": "bors", "email": "bors@rust-lang.org", "date": "2014-07-10T11:01:32Z"}, "message": "auto merge of #15563 : luqmana/rust/nif, r=pcwalton", "tree": {"sha": "aa5c919537101198ce93e32ac5f291493a5e0c94", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/aa5c919537101198ce93e32ac5f291493a5e0c94"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/a1bd5d359b8e4ebc1b952f96e4f1d4658bc29b26", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/a1bd5d359b8e4ebc1b952f96e4f1d4658bc29b26", "html_url": "https://github.com/rust-lang/rust/commit/a1bd5d359b8e4ebc1b952f96e4f1d4658bc29b26", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/a1bd5d359b8e4ebc1b952f96e4f1d4658bc29b26/comments", "author": {"login": "bors", "id": 3372342, "node_id": "MDQ6VXNlcjMzNzIzNDI=", "avatar_url": "https://avatars.githubusercontent.com/u/3372342?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bors", "html_url": "https://github.com/bors", "followers_url": "https://api.github.com/users/bors/followers", "following_url": "https://api.github.com/users/bors/following{/other_user}", "gists_url": "https://api.github.com/users/bors/gists{/gist_id}", "starred_url": "https://api.github.com/users/bors/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bors/subscriptions", "organizations_url": "https://api.github.com/users/bors/orgs", "repos_url": "https://api.github.com/users/bors/repos", "events_url": "https://api.github.com/users/bors/events{/privacy}", "received_events_url": "https://api.github.com/users/bors/received_events", "type": "User", "site_admin": false}, "committer": {"login": "bors", "id": 3372342, "node_id": "MDQ6VXNlcjMzNzIzNDI=", "avatar_url": "https://avatars.githubusercontent.com/u/3372342?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bors", "html_url": "https://github.com/bors", "followers_url": "https://api.github.com/users/bors/followers", "following_url": "https://api.github.com/users/bors/following{/other_user}", "gists_url": "https://api.github.com/users/bors/gists{/gist_id}", "starred_url": "https://api.github.com/users/bors/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bors/subscriptions", "organizations_url": "https://api.github.com/users/bors/orgs", "repos_url": "https://api.github.com/users/bors/repos", "events_url": "https://api.github.com/users/bors/events{/privacy}", "received_events_url": "https://api.github.com/users/bors/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "f8658124512bd06398908f989c9cc9fa05cf43f0", "url": "https://api.github.com/repos/rust-lang/rust/commits/f8658124512bd06398908f989c9cc9fa05cf43f0", "html_url": "https://github.com/rust-lang/rust/commit/f8658124512bd06398908f989c9cc9fa05cf43f0"}, {"sha": "83122af6ca01a6379a53e92630ed2c4eb2d07e2d", "url": "https://api.github.com/repos/rust-lang/rust/commits/83122af6ca01a6379a53e92630ed2c4eb2d07e2d", "html_url": "https://github.com/rust-lang/rust/commit/83122af6ca01a6379a53e92630ed2c4eb2d07e2d"}], "stats": {"total": 1010, "additions": 497, "deletions": 513}, "files": [{"sha": "4d329ebd2a00c2afbf56a178c3cae1e5ecceb9ae", "filename": "src/librustc/middle/trans/base.rs", "status": "modified", "additions": 6, "deletions": 3, "changes": 9, "blob_url": "https://github.com/rust-lang/rust/blob/a1bd5d359b8e4ebc1b952f96e4f1d4658bc29b26/src%2Flibrustc%2Fmiddle%2Ftrans%2Fbase.rs", "raw_url": "https://github.com/rust-lang/rust/raw/a1bd5d359b8e4ebc1b952f96e4f1d4658bc29b26/src%2Flibrustc%2Fmiddle%2Ftrans%2Fbase.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Fmiddle%2Ftrans%2Fbase.rs?ref=a1bd5d359b8e4ebc1b952f96e4f1d4658bc29b26", "patch": "@@ -862,9 +862,12 @@ pub fn trans_external_path(ccx: &CrateContext, did: ast::DefId, t: ty::t) -> Val\n         ty::ty_bare_fn(ref fn_ty) => {\n             match fn_ty.abi.for_target(ccx.sess().targ_cfg.os,\n                                        ccx.sess().targ_cfg.arch) {\n-                Some(Rust) | Some(RustIntrinsic) => {\n+                Some(Rust) => {\n                     get_extern_rust_fn(ccx, t, name.as_slice(), did)\n                 }\n+                Some(RustIntrinsic) => {\n+                    ccx.sess().bug(\"unexpected intrinsic in trans_external_path\")\n+                }\n                 Some(..) | None => {\n                     foreign::register_foreign_item_fn(ccx, fn_ty.abi, t,\n                                                       name.as_slice(), None)\n@@ -1781,9 +1784,9 @@ fn register_fn(ccx: &CrateContext,\n                -> ValueRef {\n     match ty::get(node_type).sty {\n         ty::ty_bare_fn(ref f) => {\n-            assert!(f.abi == Rust || f.abi == RustIntrinsic);\n+            assert!(f.abi == Rust);\n         }\n-        _ => fail!(\"expected bare rust fn or an intrinsic\")\n+        _ => fail!(\"expected bare rust fn\")\n     };\n \n     let llfn = decl_rust_fn(ccx, node_type, sym.as_slice());"}, {"sha": "15415620c5bba2ad40830d0a4372ccd257052d7f", "filename": "src/librustc/middle/trans/callee.rs", "status": "modified", "additions": 44, "deletions": 37, "changes": 81, "blob_url": "https://github.com/rust-lang/rust/blob/a1bd5d359b8e4ebc1b952f96e4f1d4658bc29b26/src%2Flibrustc%2Fmiddle%2Ftrans%2Fcallee.rs", "raw_url": "https://github.com/rust-lang/rust/raw/a1bd5d359b8e4ebc1b952f96e4f1d4658bc29b26/src%2Flibrustc%2Fmiddle%2Ftrans%2Fcallee.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Fmiddle%2Ftrans%2Fcallee.rs?ref=a1bd5d359b8e4ebc1b952f96e4f1d4658bc29b26", "patch": "@@ -19,7 +19,6 @@\n use arena::TypedArena;\n use back::abi;\n use back::link;\n-use driver::session;\n use lib::llvm::ValueRef;\n use lib::llvm::llvm;\n use metadata::csearch;\n@@ -40,6 +39,7 @@ use middle::trans::expr;\n use middle::trans::glue;\n use middle::trans::inline;\n use middle::trans::foreign;\n+use middle::trans::intrinsic;\n use middle::trans::meth;\n use middle::trans::monomorphize;\n use middle::trans::type_::Type;\n@@ -53,7 +53,6 @@ use util::ppaux::Repr;\n use std::gc::Gc;\n use syntax::ast;\n use synabi = syntax::abi;\n-use syntax::ast_map;\n \n pub struct MethodData {\n     pub llfn: ValueRef,\n@@ -68,6 +67,8 @@ pub enum CalleeData {\n     // value (which is a pair).\n     Fn(/* llfn */ ValueRef),\n \n+    Intrinsic(ast::NodeId, subst::Substs),\n+\n     TraitMethod(MethodData)\n }\n \n@@ -119,7 +120,21 @@ fn trans<'a>(bcx: &'a Block<'a>, expr: &ast::Expr) -> Callee<'a> {\n \n     fn trans_def<'a>(bcx: &'a Block<'a>, def: def::Def, ref_expr: &ast::Expr)\n                  -> Callee<'a> {\n+        debug!(\"trans_def(def={}, ref_expr={})\", def.repr(bcx.tcx()), ref_expr.repr(bcx.tcx()));\n+        let expr_ty = node_id_type(bcx, ref_expr.id);\n         match def {\n+            def::DefFn(did, _) if match ty::get(expr_ty).sty {\n+                ty::ty_bare_fn(ref f) => f.abi == synabi::RustIntrinsic,\n+                _ => false\n+            } => {\n+                let substs = node_id_substs(bcx, ExprId(ref_expr.id));\n+                let def_id = if did.krate != ast::LOCAL_CRATE {\n+                    inline::maybe_instantiate_inline(bcx.ccx(), did)\n+                } else {\n+                    did\n+                };\n+                Callee { bcx: bcx, data: Intrinsic(def_id.node, substs) }\n+            }\n             def::DefFn(did, _) |\n             def::DefStaticMethod(did, def::FromImpl(_), _) => {\n                 fn_callee(bcx, trans_fn_ref(bcx, did, ExprId(ref_expr.id)))\n@@ -460,27 +475,8 @@ pub fn trans_fn_ref_with_vtables(\n         }\n     };\n \n-    // We must monomorphise if the fn has type parameters, is a rust\n-    // intrinsic, or is a default method.  In particular, if we see an\n-    // intrinsic that is inlined from a different crate, we want to reemit the\n-    // intrinsic instead of trying to call it in the other crate.\n-    let must_monomorphise = if !substs.types.is_empty() || is_default {\n-        true\n-    } else if def_id.krate == ast::LOCAL_CRATE {\n-        let map_node = session::expect(\n-            ccx.sess(),\n-            tcx.map.find(def_id.node),\n-            || \"local item should be in ast map\".to_string());\n-\n-        match map_node {\n-            ast_map::NodeForeignItem(_) => {\n-                tcx.map.get_foreign_abi(def_id.node) == synabi::RustIntrinsic\n-            }\n-            _ => false\n-        }\n-    } else {\n-        false\n-    };\n+    // We must monomorphise if the fn has type parameters or is a default method.\n+    let must_monomorphise = !substs.types.is_empty() || is_default;\n \n     // Create a monomorphic version of generic functions\n     if must_monomorphise {\n@@ -662,6 +658,12 @@ pub fn trans_call_inner<'a>(\n     let callee = get_callee(bcx, cleanup::CustomScope(arg_cleanup_scope));\n     let mut bcx = callee.bcx;\n \n+    let (abi, ret_ty) = match ty::get(callee_ty).sty {\n+        ty::ty_bare_fn(ref f) => (f.abi, f.sig.output),\n+        ty::ty_closure(ref f) => (synabi::Rust, f.sig.output),\n+        _ => fail!(\"expected bare rust fn or closure in trans_call_inner\")\n+    };\n+\n     let (llfn, llenv, llself) = match callee.data {\n         Fn(llfn) => {\n             (llfn, None, None)\n@@ -679,14 +681,19 @@ pub fn trans_call_inner<'a>(\n             let llenv = Load(bcx, llenv);\n             (llfn, Some(llenv), None)\n         }\n-    };\n+        Intrinsic(node, substs) => {\n+            assert!(abi == synabi::RustIntrinsic);\n+            assert!(dest.is_some());\n \n-    let (abi, ret_ty) = match ty::get(callee_ty).sty {\n-        ty::ty_bare_fn(ref f) => (f.abi, f.sig.output),\n-        ty::ty_closure(ref f) => (synabi::Rust, f.sig.output),\n-        _ => fail!(\"expected bare rust fn or closure in trans_call_inner\")\n+            return intrinsic::trans_intrinsic_call(bcx, node, callee_ty,\n+                                                   arg_cleanup_scope, args,\n+                                                   dest.unwrap(), substs);\n+        }\n     };\n-    let is_rust_fn = abi == synabi::Rust || abi == synabi::RustIntrinsic;\n+\n+    // Intrinsics should not become actual functions.\n+    // We trans them in place in `trans_intrinsic_call`\n+    assert!(abi != synabi::RustIntrinsic);\n \n     // Generate a location to store the result. If the user does\n     // not care about the result, just make a stack slot.\n@@ -716,7 +723,7 @@ pub fn trans_call_inner<'a>(\n     // and done, either the return value of the function will have been\n     // written in opt_llretslot (if it is Some) or `llresult` will be\n     // set appropriately (otherwise).\n-    if is_rust_fn {\n+    if abi == synabi::Rust {\n         let mut llargs = Vec::new();\n \n         // Push the out-pointer if we use an out-pointer for this\n@@ -816,13 +823,13 @@ pub enum CallArgs<'a> {\n     ArgOverloadedOp(Datum<Expr>, Option<(Datum<Expr>, ast::NodeId)>),\n }\n \n-fn trans_args<'a>(cx: &'a Block<'a>,\n-                  args: CallArgs,\n-                  fn_ty: ty::t,\n-                  llargs: &mut Vec<ValueRef> ,\n-                  arg_cleanup_scope: cleanup::ScopeId,\n-                  ignore_self: bool)\n-                  -> &'a Block<'a> {\n+pub fn trans_args<'a>(cx: &'a Block<'a>,\n+                      args: CallArgs,\n+                      fn_ty: ty::t,\n+                      llargs: &mut Vec<ValueRef> ,\n+                      arg_cleanup_scope: cleanup::ScopeId,\n+                      ignore_self: bool)\n+                      -> &'a Block<'a> {\n     let _icx = push_ctxt(\"trans_args\");\n     let arg_tys = ty::ty_fn_args(fn_ty);\n     let variadic = ty::fn_is_variadic(fn_ty);"}, {"sha": "f73984d3b36fa32635528032874dd7a565c94c13", "filename": "src/librustc/middle/trans/foreign.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/a1bd5d359b8e4ebc1b952f96e4f1d4658bc29b26/src%2Flibrustc%2Fmiddle%2Ftrans%2Fforeign.rs", "raw_url": "https://github.com/rust-lang/rust/raw/a1bd5d359b8e4ebc1b952f96e4f1d4658bc29b26/src%2Flibrustc%2Fmiddle%2Ftrans%2Fforeign.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Fmiddle%2Ftrans%2Fforeign.rs?ref=a1bd5d359b8e4ebc1b952f96e4f1d4658bc29b26", "patch": "@@ -76,7 +76,7 @@ pub fn llvm_calling_convention(ccx: &CrateContext,\n     abi.for_target(os, arch).map(|abi| {\n         match abi {\n             RustIntrinsic => {\n-                // Intrinsics are emitted by monomorphic fn\n+                // Intrinsics are emitted at the call site\n                 ccx.sess().bug(\"asked to register intrinsic fn\");\n             }\n "}, {"sha": "309c700bfe89942fa86dd035c224a74cf79b8823", "filename": "src/librustc/middle/trans/intrinsic.rs", "status": "modified", "additions": 442, "deletions": 459, "changes": 901, "blob_url": "https://github.com/rust-lang/rust/blob/a1bd5d359b8e4ebc1b952f96e4f1d4658bc29b26/src%2Flibrustc%2Fmiddle%2Ftrans%2Fintrinsic.rs", "raw_url": "https://github.com/rust-lang/rust/raw/a1bd5d359b8e4ebc1b952f96e4f1d4658bc29b26/src%2Flibrustc%2Fmiddle%2Ftrans%2Fintrinsic.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Fmiddle%2Ftrans%2Fintrinsic.rs?ref=a1bd5d359b8e4ebc1b952f96e4f1d4658bc29b26", "patch": "@@ -10,15 +10,18 @@\n \n #![allow(non_uppercase_pattern_statics)]\n \n-use arena::TypedArena;\n-use lib::llvm::{SequentiallyConsistent, Acquire, Release, Xchg};\n-use lib::llvm::{ValueRef, Pointer, Array, Struct};\n+use lib::llvm::{SequentiallyConsistent, Acquire, Release, Xchg, ValueRef};\n use lib;\n+use middle::subst;\n use middle::subst::FnSpace;\n use middle::trans::base::*;\n use middle::trans::build::*;\n+use middle::trans::callee;\n+use middle::trans::cleanup;\n+use middle::trans::cleanup::CleanupMethods;\n use middle::trans::common::*;\n use middle::trans::datum::*;\n+use middle::trans::expr;\n use middle::trans::glue;\n use middle::trans::type_of::*;\n use middle::trans::type_of;\n@@ -27,7 +30,6 @@ use middle::trans::machine::llsize_of;\n use middle::trans::type_::Type;\n use middle::ty;\n use syntax::ast;\n-use syntax::ast_map;\n use syntax::parse::token;\n use util::ppaux::ty_to_string;\n \n@@ -83,526 +85,507 @@ pub fn get_simple_intrinsic(ccx: &CrateContext, item: &ast::ForeignItem) -> Opti\n     Some(ccx.get_intrinsic(&name))\n }\n \n-pub fn trans_intrinsic(ccx: &CrateContext,\n-                       decl: ValueRef,\n-                       item: &ast::ForeignItem,\n-                       substs: &param_substs,\n-                       ref_id: Option<ast::NodeId>) {\n-    debug!(\"trans_intrinsic(item.ident={})\", token::get_ident(item.ident));\n-\n-    fn with_overflow_instrinsic(bcx: &Block, name: &'static str, t: ty::t) {\n-        let first_real_arg = bcx.fcx.arg_pos(0u);\n-        let a = get_param(bcx.fcx.llfn, first_real_arg);\n-        let b = get_param(bcx.fcx.llfn, first_real_arg + 1);\n-        let llfn = bcx.ccx().get_intrinsic(&name);\n-\n-        // convert `i1` to a `bool`, and write to the out parameter\n-        let val = Call(bcx, llfn, [a, b], []);\n-        let result = ExtractValue(bcx, val, 0);\n-        let overflow = ZExt(bcx, ExtractValue(bcx, val, 1), Type::bool(bcx.ccx()));\n-        let ret = C_undef(type_of::type_of(bcx.ccx(), t));\n-        let ret = InsertValue(bcx, ret, result, 0);\n-        let ret = InsertValue(bcx, ret, overflow, 1);\n-\n-        if type_is_immediate(bcx.ccx(), t) {\n-            Ret(bcx, ret);\n-        } else {\n-            let retptr = get_param(bcx.fcx.llfn, bcx.fcx.out_arg_pos());\n-            Store(bcx, ret, retptr);\n-            RetVoid(bcx);\n+/// Performs late verification that intrinsics are used correctly. At present,\n+/// the only intrinsic that needs such verification is `transmute`.\n+pub fn check_intrinsics(ccx: &CrateContext) {\n+    for transmute_restriction in ccx.tcx\n+                                    .transmute_restrictions\n+                                    .borrow()\n+                                    .iter() {\n+        let llfromtype = type_of::sizing_type_of(ccx,\n+                                                 transmute_restriction.from);\n+        let lltotype = type_of::sizing_type_of(ccx,\n+                                               transmute_restriction.to);\n+        let from_type_size = machine::llbitsize_of_real(ccx, llfromtype);\n+        let to_type_size = machine::llbitsize_of_real(ccx, lltotype);\n+        if from_type_size != to_type_size {\n+            ccx.sess()\n+               .span_err(transmute_restriction.span,\n+                format!(\"transmute called on types with different sizes: \\\n+                         {} ({} bit{}) to {} ({} bit{})\",\n+                        ty_to_string(ccx.tcx(), transmute_restriction.from),\n+                        from_type_size as uint,\n+                        if from_type_size == 1 {\n+                            \"\"\n+                        } else {\n+                            \"s\"\n+                        },\n+                        ty_to_string(ccx.tcx(), transmute_restriction.to),\n+                        to_type_size as uint,\n+                        if to_type_size == 1 {\n+                            \"\"\n+                        } else {\n+                            \"s\"\n+                        }).as_slice());\n         }\n     }\n+    ccx.sess().abort_if_errors();\n+}\n \n-    fn volatile_load_intrinsic(bcx: &Block) {\n-        let first_real_arg = bcx.fcx.arg_pos(0u);\n-        let src = get_param(bcx.fcx.llfn, first_real_arg);\n+pub fn trans_intrinsic_call<'a>(mut bcx: &'a Block<'a>, node: ast::NodeId,\n+                                callee_ty: ty::t, cleanup_scope: cleanup::CustomScopeIndex,\n+                                args: callee::CallArgs, dest: expr::Dest,\n+                                substs: subst::Substs) -> Result<'a> {\n \n-        let val = VolatileLoad(bcx, src);\n-        Ret(bcx, val);\n-    }\n+    let fcx = bcx.fcx;\n+    let ccx = fcx.ccx;\n+    let tcx = bcx.tcx();\n \n-    fn volatile_store_intrinsic(bcx: &Block) {\n-        let first_real_arg = bcx.fcx.arg_pos(0u);\n-        let dst = get_param(bcx.fcx.llfn, first_real_arg);\n-        let val = get_param(bcx.fcx.llfn, first_real_arg + 1);\n+    let ret_ty = match ty::get(callee_ty).sty {\n+        ty::ty_bare_fn(ref f) => f.sig.output,\n+        _ => fail!(\"expected bare_fn in trans_intrinsic_call\")\n+    };\n+    let llret_ty = type_of::type_of(ccx, ret_ty);\n+    let foreign_item = tcx.map.expect_foreign_item(node);\n+    let name = token::get_ident(foreign_item.ident);\n+\n+    // For `transmute` we can just trans the input expr directly into dest\n+    if name.get() == \"transmute\" {\n+        match args {\n+            callee::ArgExprs(arg_exprs) => {\n+                assert_eq!(arg_exprs.len(), 1);\n+\n+                let (in_type, out_type) = (*substs.types.get(FnSpace, 0),\n+                                           *substs.types.get(FnSpace, 1));\n+                let llintype = type_of::type_of(ccx, in_type);\n+                let llouttype = type_of::type_of(ccx, out_type);\n+\n+                let in_type_size = machine::llbitsize_of_real(ccx, llintype);\n+                let out_type_size = machine::llbitsize_of_real(ccx, llouttype);\n+\n+                // This should be caught by the intrinsicck pass\n+                assert_eq!(in_type_size, out_type_size);\n+\n+                // We need to cast the dest so the types work out\n+                let dest = match dest {\n+                    expr::SaveIn(d) => expr::SaveIn(PointerCast(bcx, d, llintype.ptr_to())),\n+                    expr::Ignore => expr::Ignore\n+                };\n+                bcx = expr::trans_into(bcx, &*arg_exprs[0], dest);\n \n-        VolatileStore(bcx, val, dst);\n-        RetVoid(bcx);\n-    }\n+                fcx.pop_custom_cleanup_scope(cleanup_scope);\n \n-    fn copy_intrinsic(bcx: &Block, allow_overlap: bool, volatile: bool, tp_ty: ty::t) {\n-        let ccx = bcx.ccx();\n-        let lltp_ty = type_of::type_of(ccx, tp_ty);\n-        let align = C_i32(ccx, machine::llalign_of_min(ccx, lltp_ty) as i32);\n-        let size = machine::llsize_of(ccx, lltp_ty);\n-        let int_size = machine::llbitsize_of_real(ccx, ccx.int_type);\n-        let name = if allow_overlap {\n-            if int_size == 32 {\n-                \"llvm.memmove.p0i8.p0i8.i32\"\n-            } else {\n-                \"llvm.memmove.p0i8.p0i8.i64\"\n-            }\n-        } else {\n-            if int_size == 32 {\n-                \"llvm.memcpy.p0i8.p0i8.i32\"\n-            } else {\n-                \"llvm.memcpy.p0i8.p0i8.i64\"\n-            }\n-        };\n-\n-        let decl = bcx.fcx.llfn;\n-        let first_real_arg = bcx.fcx.arg_pos(0u);\n-        let dst_ptr = PointerCast(bcx, get_param(decl, first_real_arg), Type::i8p(ccx));\n-        let src_ptr = PointerCast(bcx, get_param(decl, first_real_arg + 1), Type::i8p(ccx));\n-        let count = get_param(decl, first_real_arg + 2);\n-        let llfn = ccx.get_intrinsic(&name);\n-        Call(bcx, llfn,\n-             [dst_ptr, src_ptr, Mul(bcx, size, count), align, C_bool(ccx, volatile)], []);\n-        RetVoid(bcx);\n-    }\n+                return match dest {\n+                    expr::SaveIn(d) => Result::new(bcx, d),\n+                    expr::Ignore => Result::new(bcx, C_undef(llret_ty.ptr_to()))\n+                };\n \n-    fn memset_intrinsic(bcx: &Block, volatile: bool, tp_ty: ty::t) {\n-        let ccx = bcx.ccx();\n-        let lltp_ty = type_of::type_of(ccx, tp_ty);\n-        let align = C_i32(ccx, machine::llalign_of_min(ccx, lltp_ty) as i32);\n-        let size = machine::llsize_of(ccx, lltp_ty);\n-        let name = if machine::llbitsize_of_real(ccx, ccx.int_type) == 32 {\n-            \"llvm.memset.p0i8.i32\"\n-        } else {\n-            \"llvm.memset.p0i8.i64\"\n-        };\n-\n-        let decl = bcx.fcx.llfn;\n-        let first_real_arg = bcx.fcx.arg_pos(0u);\n-        let dst_ptr = PointerCast(bcx, get_param(decl, first_real_arg), Type::i8p(ccx));\n-        let val = get_param(decl, first_real_arg + 1);\n-        let count = get_param(decl, first_real_arg + 2);\n-        let llfn = ccx.get_intrinsic(&name);\n-        Call(bcx, llfn, [dst_ptr, val, Mul(bcx, size, count), align, C_bool(ccx, volatile)], []);\n-        RetVoid(bcx);\n-    }\n+            }\n \n-    fn count_zeros_intrinsic(bcx: &Block, name: &'static str) {\n-        let x = get_param(bcx.fcx.llfn, bcx.fcx.arg_pos(0u));\n-        let y = C_bool(bcx.ccx(), false);\n-        let llfn = bcx.ccx().get_intrinsic(&name);\n-        let llcall = Call(bcx, llfn, [x, y], []);\n-        Ret(bcx, llcall);\n+            _ => {\n+                ccx.sess().bug(\"expected expr as argument for transmute\");\n+            }\n+        }\n     }\n \n-    let output_type = ty::ty_fn_ret(ty::node_id_to_type(ccx.tcx(), item.id));\n-\n-    let arena = TypedArena::new();\n-    let fcx = new_fn_ctxt(ccx, decl, item.id, false, output_type,\n-                          substs, Some(item.span), &arena);\n-    let mut bcx = init_function(&fcx, true, output_type);\n-\n-    set_always_inline(fcx.llfn);\n+    // Get location to store the result. If the user does\n+    // not care about the result, just make a stack slot\n+    let llresult = match dest {\n+        expr::SaveIn(d) => d,\n+        expr::Ignore => {\n+            if !type_is_zero_size(ccx, ret_ty) {\n+                alloc_ty(bcx, ret_ty, \"intrinsic_result\")\n+            } else {\n+                C_undef(llret_ty.ptr_to())\n+            }\n+        }\n+    };\n \n-    let first_real_arg = fcx.arg_pos(0u);\n+    // Push the arguments.\n+    let mut llargs = Vec::new();\n+    bcx = callee::trans_args(bcx, args, callee_ty, &mut llargs,\n+                             cleanup::CustomScope(cleanup_scope), false);\n \n-    let name = token::get_ident(item.ident);\n+    fcx.pop_custom_cleanup_scope(cleanup_scope);\n \n-    // This requires that atomic intrinsics follow a specific naming pattern:\n-    // \"atomic_<operation>[_<ordering>], and no ordering means SeqCst\n-    if name.get().starts_with(\"atomic_\") {\n-        let split: Vec<&str> = name.get().split('_').collect();\n-        assert!(split.len() >= 2, \"Atomic intrinsic not correct format\");\n-        let order = if split.len() == 2 {\n-            lib::llvm::SequentiallyConsistent\n-        } else {\n-            match *split.get(2) {\n-                \"relaxed\" => lib::llvm::Monotonic,\n-                \"acq\"     => lib::llvm::Acquire,\n-                \"rel\"     => lib::llvm::Release,\n-                \"acqrel\"  => lib::llvm::AcquireRelease,\n-                _ => ccx.sess().fatal(\"unknown ordering in atomic intrinsic\")\n-            }\n-        };\n-\n-        match *split.get(1) {\n-            \"cxchg\" => {\n-                // See include/llvm/IR/Instructions.h for their implementation\n-                // of this, I assume that it's good enough for us to use for\n-                // now.\n-                let strongest_failure_ordering = match order {\n-                    lib::llvm::NotAtomic | lib::llvm::Unordered =>\n-                        ccx.sess().fatal(\"cmpxchg must be atomic\"),\n-                    lib::llvm::Monotonic | lib::llvm::Release =>\n-                        lib::llvm::Monotonic,\n-                    lib::llvm::Acquire | lib::llvm::AcquireRelease =>\n-                        lib::llvm::Acquire,\n-                    lib::llvm::SequentiallyConsistent =>\n-                        lib::llvm::SequentiallyConsistent,\n-                };\n-                let res = AtomicCmpXchg(bcx, get_param(decl, first_real_arg),\n-                                        get_param(decl, first_real_arg + 1u),\n-                                        get_param(decl, first_real_arg + 2u),\n-                                        order, strongest_failure_ordering);\n-                if unsafe { lib::llvm::llvm::LLVMVersionMinor() >= 5 } {\n-                    Ret(bcx, ExtractValue(bcx, res, 0));\n-                } else {\n-                    Ret(bcx, res);\n-                }\n-            }\n-            \"load\" => {\n-                let old = AtomicLoad(bcx, get_param(decl, first_real_arg),\n-                                     order);\n-                Ret(bcx, old);\n-            }\n-            \"store\" => {\n-                AtomicStore(bcx, get_param(decl, first_real_arg + 1u),\n-                            get_param(decl, first_real_arg),\n-                            order);\n-                RetVoid(bcx);\n-            }\n-            \"fence\" => {\n-                AtomicFence(bcx, order);\n-                RetVoid(bcx);\n-            }\n-            op => {\n-                // These are all AtomicRMW ops\n-                let atom_op = match op {\n-                    \"xchg\"  => lib::llvm::Xchg,\n-                    \"xadd\"  => lib::llvm::Add,\n-                    \"xsub\"  => lib::llvm::Sub,\n-                    \"and\"   => lib::llvm::And,\n-                    \"nand\"  => lib::llvm::Nand,\n-                    \"or\"    => lib::llvm::Or,\n-                    \"xor\"   => lib::llvm::Xor,\n-                    \"max\"   => lib::llvm::Max,\n-                    \"min\"   => lib::llvm::Min,\n-                    \"umax\"  => lib::llvm::UMax,\n-                    \"umin\"  => lib::llvm::UMin,\n-                    _ => ccx.sess().fatal(\"unknown atomic operation\")\n-                };\n+    let simple = get_simple_intrinsic(ccx, &*foreign_item);\n \n-                let old = AtomicRMW(bcx, atom_op, get_param(decl, first_real_arg),\n-                                    get_param(decl, first_real_arg + 1u),\n-                                    order);\n-                Ret(bcx, old);\n-            }\n+    let llval = match (simple, name.get()) {\n+        (Some(llfn), _) => {\n+            Call(bcx, llfn, llargs.as_slice(), [])\n         }\n-\n-        fcx.cleanup();\n-        return;\n-    }\n-\n-    match name.get() {\n-        \"abort\" => {\n-            let llfn = bcx.ccx().get_intrinsic(&(\"llvm.trap\"));\n-            Call(bcx, llfn, [], []);\n+        (_, \"abort\") => {\n+            let llfn = ccx.get_intrinsic(&(\"llvm.trap\"));\n+            let v = Call(bcx, llfn, [], []);\n             Unreachable(bcx);\n+            v\n         }\n-        \"breakpoint\" => {\n-            let llfn = bcx.ccx().get_intrinsic(&(\"llvm.debugtrap\"));\n-            Call(bcx, llfn, [], []);\n-            RetVoid(bcx);\n+        (_, \"breakpoint\") => {\n+            let llfn = ccx.get_intrinsic(&(\"llvm.debugtrap\"));\n+            Call(bcx, llfn, [], [])\n         }\n-        \"size_of\" => {\n-            let tp_ty = *substs.substs.types.get(FnSpace, 0);\n+        (_, \"size_of\") => {\n+            let tp_ty = *substs.types.get(FnSpace, 0);\n             let lltp_ty = type_of::type_of(ccx, tp_ty);\n-            Ret(bcx, C_uint(ccx, machine::llsize_of_real(ccx, lltp_ty) as uint));\n+            C_uint(ccx, machine::llsize_of_real(ccx, lltp_ty) as uint)\n         }\n-        \"move_val_init\" => {\n+        (_, \"min_align_of\") => {\n+            let tp_ty = *substs.types.get(FnSpace, 0);\n+            let lltp_ty = type_of::type_of(ccx, tp_ty);\n+            C_uint(ccx, machine::llalign_of_min(ccx, lltp_ty) as uint)\n+        }\n+        (_, \"pref_align_of\") => {\n+            let tp_ty = *substs.types.get(FnSpace, 0);\n+            let lltp_ty = type_of::type_of(ccx, tp_ty);\n+            C_uint(ccx, machine::llalign_of_pref(ccx, lltp_ty) as uint)\n+        }\n+        (_, \"move_val_init\") => {\n             // Create a datum reflecting the value being moved.\n             // Use `appropriate_mode` so that the datum is by ref\n             // if the value is non-immediate. Note that, with\n             // intrinsics, there are no argument cleanups to\n             // concern ourselves with, so we can use an rvalue datum.\n-            let tp_ty = *substs.substs.types.get(FnSpace, 0);\n+            let tp_ty = *substs.types.get(FnSpace, 0);\n             let mode = appropriate_rvalue_mode(ccx, tp_ty);\n-            let src = Datum {val: get_param(decl, first_real_arg + 1u),\n-                             ty: tp_ty,\n-                             kind: Rvalue::new(mode)};\n-            bcx = src.store_to(bcx, get_param(decl, first_real_arg));\n-            RetVoid(bcx);\n-        }\n-        \"min_align_of\" => {\n-            let tp_ty = *substs.substs.types.get(FnSpace, 0);\n-            let lltp_ty = type_of::type_of(ccx, tp_ty);\n-            Ret(bcx, C_uint(ccx, machine::llalign_of_min(ccx, lltp_ty) as uint));\n-        }\n-        \"pref_align_of\"=> {\n-            let tp_ty = *substs.substs.types.get(FnSpace, 0);\n-            let lltp_ty = type_of::type_of(ccx, tp_ty);\n-            Ret(bcx, C_uint(ccx, machine::llalign_of_pref(ccx, lltp_ty) as uint));\n+            let src = Datum {\n+                val: *llargs.get(1),\n+                ty: tp_ty,\n+                kind: Rvalue::new(mode)\n+            };\n+            bcx = src.store_to(bcx, *llargs.get(0));\n+            C_nil(ccx)\n         }\n-        \"get_tydesc\" => {\n-            let tp_ty = *substs.substs.types.get(FnSpace, 0);\n+        (_, \"get_tydesc\") => {\n+            let tp_ty = *substs.types.get(FnSpace, 0);\n             let static_ti = get_tydesc(ccx, tp_ty);\n             glue::lazily_emit_visit_glue(ccx, &*static_ti);\n \n             // FIXME (#3730): ideally this shouldn't need a cast,\n             // but there's a circularity between translating rust types to llvm\n             // types and having a tydesc type available. So I can't directly access\n             // the llvm type of intrinsic::TyDesc struct.\n-            let userland_tydesc_ty = type_of::type_of(ccx, output_type);\n-            let td = PointerCast(bcx, static_ti.tydesc, userland_tydesc_ty);\n-            Ret(bcx, td);\n+            PointerCast(bcx, static_ti.tydesc, llret_ty)\n         }\n-        \"type_id\" => {\n+        (_, \"type_id\") => {\n             let hash = ty::hash_crate_independent(\n                 ccx.tcx(),\n-                *substs.substs.types.get(FnSpace, 0),\n+                *substs.types.get(FnSpace, 0),\n                 &ccx.link_meta.crate_hash);\n             // NB: This needs to be kept in lockstep with the TypeId struct in\n-            //     libstd/unstable/intrinsics.rs\n-            let val = C_named_struct(type_of::type_of(ccx, output_type),\n-                                     [C_u64(ccx, hash)]);\n-            match bcx.fcx.llretptr.get() {\n-                Some(ptr) => {\n-                    Store(bcx, val, ptr);\n-                    RetVoid(bcx);\n-                },\n-                None => Ret(bcx, val)\n-            }\n+            //     the intrinsic module\n+            C_named_struct(llret_ty, [C_u64(ccx, hash)])\n         }\n-        \"init\" => {\n-            let tp_ty = *substs.substs.types.get(FnSpace, 0);\n+        (_, \"init\") => {\n+            let tp_ty = *substs.types.get(FnSpace, 0);\n             let lltp_ty = type_of::type_of(ccx, tp_ty);\n-            match bcx.fcx.llretptr.get() {\n-                Some(ptr) => { Store(bcx, C_null(lltp_ty), ptr); RetVoid(bcx); }\n-                None if ty::type_is_nil(tp_ty) => RetVoid(bcx),\n-                None => Ret(bcx, C_null(lltp_ty)),\n-            }\n-        }\n-        \"uninit\" => {\n-            // Do nothing, this is effectively a no-op\n-            let retty = *substs.substs.types.get(FnSpace, 0);\n-            if type_is_immediate(ccx, retty) && !return_type_is_void(ccx, retty) {\n-                unsafe {\n-                    Ret(bcx, lib::llvm::llvm::LLVMGetUndef(arg_type_of(ccx, retty).to_ref()));\n-                }\n+            if return_type_is_void(ccx, tp_ty) {\n+                C_nil(ccx)\n             } else {\n-                RetVoid(bcx)\n+                C_null(lltp_ty)\n             }\n         }\n-        \"forget\" => {\n-            RetVoid(bcx);\n+        // Effectively no-ops\n+        (_, \"uninit\") | (_, \"forget\") => {\n+            C_nil(ccx)\n         }\n-        \"transmute\" => {\n-            let (in_type, out_type) = (*substs.substs.types.get(FnSpace, 0),\n-                                       *substs.substs.types.get(FnSpace, 1));\n-            let llintype = type_of::type_of(ccx, in_type);\n-            let llouttype = type_of::type_of(ccx, out_type);\n-\n-            let in_type_size = machine::llbitsize_of_real(ccx, llintype);\n-            let out_type_size = machine::llbitsize_of_real(ccx, llouttype);\n-            if in_type_size != out_type_size {\n-                let sp = match ccx.tcx.map.get(ref_id.unwrap()) {\n-                    ast_map::NodeExpr(e) => e.span,\n-                    _ => fail!(\"transmute has non-expr arg\"),\n-                };\n-                ccx.sess().span_bug(sp,\n-                    format!(\"transmute called on types with different sizes: \\\n-                             {} ({} bit{}) to \\\n-                             {} ({} bit{})\",\n-                            ty_to_string(ccx.tcx(), in_type),\n-                            in_type_size,\n-                            if in_type_size == 1 {\"\"} else {\"s\"},\n-                            ty_to_string(ccx.tcx(), out_type),\n-                            out_type_size,\n-                            if out_type_size == 1 {\"\"} else {\"s\"}).as_slice());\n-            }\n-\n-            if !return_type_is_void(ccx, out_type) {\n-                let llsrcval = get_param(decl, first_real_arg);\n-                if type_is_immediate(ccx, in_type) {\n-                    match fcx.llretptr.get() {\n-                        Some(llretptr) => {\n-                            Store(bcx, llsrcval, PointerCast(bcx, llretptr, llintype.ptr_to()));\n-                            RetVoid(bcx);\n-                        }\n-                        None => match (llintype.kind(), llouttype.kind()) {\n-                            (Pointer, other) | (other, Pointer) if other != Pointer => {\n-                                let tmp = Alloca(bcx, llouttype, \"\");\n-                                Store(bcx, llsrcval, PointerCast(bcx, tmp, llintype.ptr_to()));\n-                                Ret(bcx, Load(bcx, tmp));\n-                            }\n-                            (Array, _) | (_, Array) | (Struct, _) | (_, Struct) => {\n-                                let tmp = Alloca(bcx, llouttype, \"\");\n-                                Store(bcx, llsrcval, PointerCast(bcx, tmp, llintype.ptr_to()));\n-                                Ret(bcx, Load(bcx, tmp));\n-                            }\n-                            _ => {\n-                                let llbitcast = BitCast(bcx, llsrcval, llouttype);\n-                                Ret(bcx, llbitcast)\n-                            }\n-                        }\n-                    }\n-                } else if type_is_immediate(ccx, out_type) {\n-                    let llsrcptr = PointerCast(bcx, llsrcval, llouttype.ptr_to());\n-                    let ll_load = Load(bcx, llsrcptr);\n-                    Ret(bcx, ll_load);\n-                } else {\n-                    // NB: Do not use a Load and Store here. This causes massive\n-                    // code bloat when `transmute` is used on large structural\n-                    // types.\n-                    let lldestptr = fcx.llretptr.get().unwrap();\n-                    let lldestptr = PointerCast(bcx, lldestptr, Type::i8p(ccx));\n-                    let llsrcptr = PointerCast(bcx, llsrcval, Type::i8p(ccx));\n-\n-                    let llsize = llsize_of(ccx, llintype);\n-                    call_memcpy(bcx, lldestptr, llsrcptr, llsize, 1);\n-                    RetVoid(bcx);\n-                };\n-            } else {\n-                RetVoid(bcx);\n-            }\n+        (_, \"needs_drop\") => {\n+            let tp_ty = *substs.types.get(FnSpace, 0);\n+            C_bool(ccx, ty::type_needs_drop(ccx.tcx(), tp_ty))\n         }\n-        \"needs_drop\" => {\n-            let tp_ty = *substs.substs.types.get(FnSpace, 0);\n-            Ret(bcx, C_bool(ccx, ty::type_needs_drop(ccx.tcx(), tp_ty)));\n+        (_, \"owns_managed\") => {\n+            let tp_ty = *substs.types.get(FnSpace, 0);\n+            C_bool(ccx, ty::type_contents(ccx.tcx(), tp_ty).owns_managed())\n         }\n-        \"owns_managed\" => {\n-            let tp_ty = *substs.substs.types.get(FnSpace, 0);\n-            Ret(bcx, C_bool(ccx, ty::type_contents(ccx.tcx(), tp_ty).owns_managed()));\n-        }\n-        \"visit_tydesc\" => {\n-            let td = get_param(decl, first_real_arg);\n-            let visitor = get_param(decl, first_real_arg + 1u);\n+        (_, \"visit_tydesc\") => {\n+            let td = *llargs.get(0);\n+            let visitor = *llargs.get(1);\n             let td = PointerCast(bcx, td, ccx.tydesc_type().ptr_to());\n             glue::call_visit_glue(bcx, visitor, td, None);\n-            RetVoid(bcx);\n+            C_nil(ccx)\n         }\n-        \"offset\" => {\n-            let ptr = get_param(decl, first_real_arg);\n-            let offset = get_param(decl, first_real_arg + 1);\n-            let lladdr = InBoundsGEP(bcx, ptr, [offset]);\n-            Ret(bcx, lladdr);\n+        (_, \"offset\") => {\n+            let ptr = *llargs.get(0);\n+            let offset = *llargs.get(1);\n+            InBoundsGEP(bcx, ptr, [offset])\n         }\n-        \"copy_nonoverlapping_memory\" => {\n-            copy_intrinsic(bcx, false, false, *substs.substs.types.get(FnSpace, 0))\n+\n+        (_, \"copy_nonoverlapping_memory\") => {\n+            copy_intrinsic(bcx, false, false, *substs.types.get(FnSpace, 0),\n+                           *llargs.get(0), *llargs.get(1), *llargs.get(2))\n         }\n-        \"copy_memory\" => {\n-            copy_intrinsic(bcx, true, false, *substs.substs.types.get(FnSpace, 0))\n+        (_, \"copy_memory\") => {\n+            copy_intrinsic(bcx, true, false, *substs.types.get(FnSpace, 0),\n+                           *llargs.get(0), *llargs.get(1), *llargs.get(2))\n         }\n-        \"set_memory\" => {\n-            memset_intrinsic(bcx, false, *substs.substs.types.get(FnSpace, 0))\n+        (_, \"set_memory\") => {\n+            memset_intrinsic(bcx, false, *substs.types.get(FnSpace, 0),\n+                             *llargs.get(0), *llargs.get(1), *llargs.get(2))\n         }\n \n-        \"volatile_copy_nonoverlapping_memory\" => {\n-            copy_intrinsic(bcx, false, true, *substs.substs.types.get(FnSpace, 0))\n+        (_, \"volatile_copy_nonoverlapping_memory\") => {\n+            copy_intrinsic(bcx, false, true, *substs.types.get(FnSpace, 0),\n+                           *llargs.get(0), *llargs.get(1), *llargs.get(2))\n         }\n-\n-        \"volatile_copy_memory\" => {\n-            copy_intrinsic(bcx, true, true, *substs.substs.types.get(FnSpace, 0))\n+        (_, \"volatile_copy_memory\") => {\n+            copy_intrinsic(bcx, true, true, *substs.types.get(FnSpace, 0),\n+                           *llargs.get(0), *llargs.get(1), *llargs.get(2))\n+        }\n+        (_, \"volatile_set_memory\") => {\n+            memset_intrinsic(bcx, true, *substs.types.get(FnSpace, 0),\n+                             *llargs.get(0), *llargs.get(1), *llargs.get(2))\n         }\n+        (_, \"volatile_load\") => {\n+            VolatileLoad(bcx, *llargs.get(0))\n+        },\n+        (_, \"volatile_store\") => {\n+            VolatileStore(bcx, *llargs.get(1), *llargs.get(0));\n+            C_nil(ccx)\n+        },\n+\n+        (_, \"ctlz8\") => count_zeros_intrinsic(bcx, \"llvm.ctlz.i8\", *llargs.get(0)),\n+        (_, \"ctlz16\") => count_zeros_intrinsic(bcx, \"llvm.ctlz.i16\", *llargs.get(0)),\n+        (_, \"ctlz32\") => count_zeros_intrinsic(bcx, \"llvm.ctlz.i32\", *llargs.get(0)),\n+        (_, \"ctlz64\") => count_zeros_intrinsic(bcx, \"llvm.ctlz.i64\", *llargs.get(0)),\n+        (_, \"cttz8\") => count_zeros_intrinsic(bcx, \"llvm.cttz.i8\", *llargs.get(0)),\n+        (_, \"cttz16\") => count_zeros_intrinsic(bcx, \"llvm.cttz.i16\", *llargs.get(0)),\n+        (_, \"cttz32\") => count_zeros_intrinsic(bcx, \"llvm.cttz.i32\", *llargs.get(0)),\n+        (_, \"cttz64\") => count_zeros_intrinsic(bcx, \"llvm.cttz.i64\", *llargs.get(0)),\n+\n+        (_, \"i8_add_with_overflow\") =>\n+            with_overflow_intrinsic(bcx, \"llvm.sadd.with.overflow.i8\", ret_ty,\n+                                   *llargs.get(0), *llargs.get(1)),\n+        (_, \"i16_add_with_overflow\") =>\n+            with_overflow_intrinsic(bcx, \"llvm.sadd.with.overflow.i16\", ret_ty,\n+                                   *llargs.get(0), *llargs.get(1)),\n+        (_, \"i32_add_with_overflow\") =>\n+            with_overflow_intrinsic(bcx, \"llvm.sadd.with.overflow.i32\", ret_ty,\n+                                   *llargs.get(0), *llargs.get(1)),\n+        (_, \"i64_add_with_overflow\") =>\n+            with_overflow_intrinsic(bcx, \"llvm.sadd.with.overflow.i64\", ret_ty,\n+                                   *llargs.get(0), *llargs.get(1)),\n+\n+        (_, \"u8_add_with_overflow\") =>\n+            with_overflow_intrinsic(bcx, \"llvm.uadd.with.overflow.i8\", ret_ty,\n+                                   *llargs.get(0), *llargs.get(1)),\n+        (_, \"u16_add_with_overflow\") =>\n+            with_overflow_intrinsic(bcx, \"llvm.uadd.with.overflow.i16\", ret_ty,\n+                                   *llargs.get(0), *llargs.get(1)),\n+        (_, \"u32_add_with_overflow\") =>\n+            with_overflow_intrinsic(bcx, \"llvm.uadd.with.overflow.i32\", ret_ty,\n+                                   *llargs.get(0), *llargs.get(1)),\n+        (_, \"u64_add_with_overflow\") =>\n+            with_overflow_intrinsic(bcx, \"llvm.uadd.with.overflow.i64\", ret_ty,\n+                                   *llargs.get(0), *llargs.get(1)),\n+\n+        (_, \"i8_sub_with_overflow\") =>\n+            with_overflow_intrinsic(bcx, \"llvm.ssub.with.overflow.i8\", ret_ty,\n+                                   *llargs.get(0), *llargs.get(1)),\n+        (_, \"i16_sub_with_overflow\") =>\n+            with_overflow_intrinsic(bcx, \"llvm.ssub.with.overflow.i16\", ret_ty,\n+                                   *llargs.get(0), *llargs.get(1)),\n+        (_, \"i32_sub_with_overflow\") =>\n+            with_overflow_intrinsic(bcx, \"llvm.ssub.with.overflow.i32\", ret_ty,\n+                                   *llargs.get(0), *llargs.get(1)),\n+        (_, \"i64_sub_with_overflow\") =>\n+            with_overflow_intrinsic(bcx, \"llvm.ssub.with.overflow.i64\", ret_ty,\n+                                   *llargs.get(0), *llargs.get(1)),\n+\n+        (_, \"u8_sub_with_overflow\") =>\n+            with_overflow_intrinsic(bcx, \"llvm.usub.with.overflow.i8\", ret_ty,\n+                                   *llargs.get(0), *llargs.get(1)),\n+        (_, \"u16_sub_with_overflow\") =>\n+            with_overflow_intrinsic(bcx, \"llvm.usub.with.overflow.i16\", ret_ty,\n+                                   *llargs.get(0), *llargs.get(1)),\n+        (_, \"u32_sub_with_overflow\") =>\n+            with_overflow_intrinsic(bcx, \"llvm.usub.with.overflow.i32\", ret_ty,\n+                                   *llargs.get(0), *llargs.get(1)),\n+        (_, \"u64_sub_with_overflow\") =>\n+            with_overflow_intrinsic(bcx, \"llvm.usub.with.overflow.i64\", ret_ty,\n+                                   *llargs.get(0), *llargs.get(1)),\n+\n+        (_, \"i8_mul_with_overflow\") =>\n+            with_overflow_intrinsic(bcx, \"llvm.smul.with.overflow.i8\", ret_ty,\n+                                   *llargs.get(0), *llargs.get(1)),\n+        (_, \"i16_mul_with_overflow\") =>\n+            with_overflow_intrinsic(bcx, \"llvm.smul.with.overflow.i16\", ret_ty,\n+                                   *llargs.get(0), *llargs.get(1)),\n+        (_, \"i32_mul_with_overflow\") =>\n+            with_overflow_intrinsic(bcx, \"llvm.smul.with.overflow.i32\", ret_ty,\n+                                   *llargs.get(0), *llargs.get(1)),\n+        (_, \"i64_mul_with_overflow\") =>\n+            with_overflow_intrinsic(bcx, \"llvm.smul.with.overflow.i64\", ret_ty,\n+                                   *llargs.get(0), *llargs.get(1)),\n+\n+        (_, \"u8_mul_with_overflow\") =>\n+            with_overflow_intrinsic(bcx, \"llvm.umul.with.overflow.i8\", ret_ty,\n+                                    *llargs.get(0), *llargs.get(1)),\n+        (_, \"u16_mul_with_overflow\") =>\n+            with_overflow_intrinsic(bcx, \"llvm.umul.with.overflow.i16\", ret_ty,\n+                                    *llargs.get(0), *llargs.get(1)),\n+        (_, \"u32_mul_with_overflow\") =>\n+            with_overflow_intrinsic(bcx, \"llvm.umul.with.overflow.i32\", ret_ty,\n+                                    *llargs.get(0), *llargs.get(1)),\n+        (_, \"u64_mul_with_overflow\") =>\n+            with_overflow_intrinsic(bcx, \"llvm.umul.with.overflow.i64\", ret_ty,\n+                                    *llargs.get(0), *llargs.get(1)),\n+\n+        // This requires that atomic intrinsics follow a specific naming pattern:\n+        // \"atomic_<operation>[_<ordering>]\", and no ordering means SeqCst\n+        (_, name) if name.starts_with(\"atomic_\") => {\n+            let split: Vec<&str> = name.split('_').collect();\n+            assert!(split.len() >= 2, \"Atomic intrinsic not correct format\");\n+\n+            let order = if split.len() == 2 {\n+                lib::llvm::SequentiallyConsistent\n+            } else {\n+                match *split.get(2) {\n+                    \"relaxed\" => lib::llvm::Monotonic,\n+                    \"acq\"     => lib::llvm::Acquire,\n+                    \"rel\"     => lib::llvm::Release,\n+                    \"acqrel\"  => lib::llvm::AcquireRelease,\n+                    _ => ccx.sess().fatal(\"unknown ordering in atomic intrinsic\")\n+                }\n+            };\n+\n+            match *split.get(1) {\n+                \"cxchg\" => {\n+                    // See include/llvm/IR/Instructions.h for their implementation\n+                    // of this, I assume that it's good enough for us to use for\n+                    // now.\n+                    let strongest_failure_ordering = match order {\n+                        lib::llvm::NotAtomic | lib::llvm::Unordered =>\n+                            ccx.sess().fatal(\"cmpxchg must be atomic\"),\n+\n+                        lib::llvm::Monotonic | lib::llvm::Release =>\n+                            lib::llvm::Monotonic,\n+\n+                        lib::llvm::Acquire | lib::llvm::AcquireRelease =>\n+                            lib::llvm::Acquire,\n+\n+                        lib::llvm::SequentiallyConsistent =>\n+                            lib::llvm::SequentiallyConsistent\n+                    };\n+\n+                    let res = AtomicCmpXchg(bcx, *llargs.get(0), *llargs.get(1),\n+                                            *llargs.get(2), order,\n+                                            strongest_failure_ordering);\n+                    if unsafe { lib::llvm::llvm::LLVMVersionMinor() >= 5 } {\n+                        ExtractValue(bcx, res, 0)\n+                    } else {\n+                        res\n+                    }\n+                }\n+\n+                \"load\" => {\n+                    AtomicLoad(bcx, *llargs.get(0), order)\n+                }\n+                \"store\" => {\n+                    AtomicStore(bcx, *llargs.get(1), *llargs.get(0), order);\n+                    C_nil(ccx)\n+                }\n+\n+                \"fence\" => {\n+                    AtomicFence(bcx, order);\n+                    C_nil(ccx)\n+                }\n+\n+                // These are all AtomicRMW ops\n+                op => {\n+                    let atom_op = match op {\n+                        \"xchg\"  => lib::llvm::Xchg,\n+                        \"xadd\"  => lib::llvm::Add,\n+                        \"xsub\"  => lib::llvm::Sub,\n+                        \"and\"   => lib::llvm::And,\n+                        \"nand\"  => lib::llvm::Nand,\n+                        \"or\"    => lib::llvm::Or,\n+                        \"xor\"   => lib::llvm::Xor,\n+                        \"max\"   => lib::llvm::Max,\n+                        \"min\"   => lib::llvm::Min,\n+                        \"umax\"  => lib::llvm::UMax,\n+                        \"umin\"  => lib::llvm::UMin,\n+                        _ => ccx.sess().fatal(\"unknown atomic operation\")\n+                    };\n+\n+                    AtomicRMW(bcx, atom_op, *llargs.get(0), *llargs.get(1), order)\n+                }\n+            }\n \n-        \"volatile_set_memory\" => {\n-            memset_intrinsic(bcx, true, *substs.substs.types.get(FnSpace, 0))\n         }\n \n-        \"ctlz8\" => count_zeros_intrinsic(bcx, \"llvm.ctlz.i8\"),\n-        \"ctlz16\" => count_zeros_intrinsic(bcx, \"llvm.ctlz.i16\"),\n-        \"ctlz32\" => count_zeros_intrinsic(bcx, \"llvm.ctlz.i32\"),\n-        \"ctlz64\" => count_zeros_intrinsic(bcx, \"llvm.ctlz.i64\"),\n-        \"cttz8\" => count_zeros_intrinsic(bcx, \"llvm.cttz.i8\"),\n-        \"cttz16\" => count_zeros_intrinsic(bcx, \"llvm.cttz.i16\"),\n-        \"cttz32\" => count_zeros_intrinsic(bcx, \"llvm.cttz.i32\"),\n-        \"cttz64\" => count_zeros_intrinsic(bcx, \"llvm.cttz.i64\"),\n-\n-        \"volatile_load\" => volatile_load_intrinsic(bcx),\n-        \"volatile_store\" => volatile_store_intrinsic(bcx),\n-\n-        \"i8_add_with_overflow\" =>\n-            with_overflow_instrinsic(bcx, \"llvm.sadd.with.overflow.i8\", output_type),\n-        \"i16_add_with_overflow\" =>\n-            with_overflow_instrinsic(bcx, \"llvm.sadd.with.overflow.i16\", output_type),\n-        \"i32_add_with_overflow\" =>\n-            with_overflow_instrinsic(bcx, \"llvm.sadd.with.overflow.i32\", output_type),\n-        \"i64_add_with_overflow\" =>\n-            with_overflow_instrinsic(bcx, \"llvm.sadd.with.overflow.i64\", output_type),\n-\n-        \"u8_add_with_overflow\" =>\n-            with_overflow_instrinsic(bcx, \"llvm.uadd.with.overflow.i8\", output_type),\n-        \"u16_add_with_overflow\" =>\n-            with_overflow_instrinsic(bcx, \"llvm.uadd.with.overflow.i16\", output_type),\n-        \"u32_add_with_overflow\" =>\n-            with_overflow_instrinsic(bcx, \"llvm.uadd.with.overflow.i32\", output_type),\n-        \"u64_add_with_overflow\" =>\n-            with_overflow_instrinsic(bcx, \"llvm.uadd.with.overflow.i64\", output_type),\n-\n-        \"i8_sub_with_overflow\" =>\n-            with_overflow_instrinsic(bcx, \"llvm.ssub.with.overflow.i8\", output_type),\n-        \"i16_sub_with_overflow\" =>\n-            with_overflow_instrinsic(bcx, \"llvm.ssub.with.overflow.i16\", output_type),\n-        \"i32_sub_with_overflow\" =>\n-            with_overflow_instrinsic(bcx, \"llvm.ssub.with.overflow.i32\", output_type),\n-        \"i64_sub_with_overflow\" =>\n-            with_overflow_instrinsic(bcx, \"llvm.ssub.with.overflow.i64\", output_type),\n-\n-        \"u8_sub_with_overflow\" =>\n-            with_overflow_instrinsic(bcx, \"llvm.usub.with.overflow.i8\", output_type),\n-        \"u16_sub_with_overflow\" =>\n-            with_overflow_instrinsic(bcx, \"llvm.usub.with.overflow.i16\", output_type),\n-        \"u32_sub_with_overflow\" =>\n-            with_overflow_instrinsic(bcx, \"llvm.usub.with.overflow.i32\", output_type),\n-        \"u64_sub_with_overflow\" =>\n-            with_overflow_instrinsic(bcx, \"llvm.usub.with.overflow.i64\", output_type),\n-\n-        \"i8_mul_with_overflow\" =>\n-            with_overflow_instrinsic(bcx, \"llvm.smul.with.overflow.i8\", output_type),\n-        \"i16_mul_with_overflow\" =>\n-            with_overflow_instrinsic(bcx, \"llvm.smul.with.overflow.i16\", output_type),\n-        \"i32_mul_with_overflow\" =>\n-            with_overflow_instrinsic(bcx, \"llvm.smul.with.overflow.i32\", output_type),\n-        \"i64_mul_with_overflow\" =>\n-            with_overflow_instrinsic(bcx, \"llvm.smul.with.overflow.i64\", output_type),\n-\n-        \"u8_mul_with_overflow\" =>\n-            with_overflow_instrinsic(bcx, \"llvm.umul.with.overflow.i8\", output_type),\n-        \"u16_mul_with_overflow\" =>\n-            with_overflow_instrinsic(bcx, \"llvm.umul.with.overflow.i16\", output_type),\n-        \"u32_mul_with_overflow\" =>\n-            with_overflow_instrinsic(bcx, \"llvm.umul.with.overflow.i32\", output_type),\n-        \"u64_mul_with_overflow\" =>\n-            with_overflow_instrinsic(bcx, \"llvm.umul.with.overflow.i64\", output_type),\n-\n-        _ => {\n-            // Could we make this an enum rather than a string? does it get\n-            // checked earlier?\n-            ccx.sess().span_bug(item.span, \"unknown intrinsic\");\n+        (_, _) => ccx.sess().span_bug(foreign_item.span, \"unknown intrinsic\")\n+    };\n+\n+    if val_ty(llval) != Type::void(ccx) &&\n+       machine::llsize_of_alloc(ccx, val_ty(llval)) != 0 {\n+        store_ty(bcx, llval, llresult, ret_ty);\n+    }\n+\n+    // If we made a temporary stack slot, let's clean it up\n+    match dest {\n+        expr::Ignore => {\n+            bcx = glue::drop_ty(bcx, llresult, ret_ty);\n         }\n+        expr::SaveIn(_) => {}\n     }\n-    fcx.cleanup();\n+\n+    Result::new(bcx, llresult)\n }\n \n-/// Performs late verification that intrinsics are used correctly. At present,\n-/// the only intrinsic that needs such verification is `transmute`.\n-pub fn check_intrinsics(ccx: &CrateContext) {\n-    for transmute_restriction in ccx.tcx\n-                                    .transmute_restrictions\n-                                    .borrow()\n-                                    .iter() {\n-        let llfromtype = type_of::sizing_type_of(ccx,\n-                                                 transmute_restriction.from);\n-        let lltotype = type_of::sizing_type_of(ccx,\n-                                               transmute_restriction.to);\n-        let from_type_size = machine::llbitsize_of_real(ccx, llfromtype);\n-        let to_type_size = machine::llbitsize_of_real(ccx, lltotype);\n-        if from_type_size != to_type_size {\n-            ccx.sess()\n-               .span_err(transmute_restriction.span,\n-                format!(\"transmute called on types with different sizes: \\\n-                         {} ({} bit{}) to {} ({} bit{})\",\n-                        ty_to_string(ccx.tcx(), transmute_restriction.from),\n-                        from_type_size as uint,\n-                        if from_type_size == 1 {\n-                            \"\"\n-                        } else {\n-                            \"s\"\n-                        },\n-                        ty_to_string(ccx.tcx(), transmute_restriction.to),\n-                        to_type_size as uint,\n-                        if to_type_size == 1 {\n-                            \"\"\n-                        } else {\n-                            \"s\"\n-                        }).as_slice());\n+fn copy_intrinsic(bcx: &Block, allow_overlap: bool, volatile: bool,\n+                  tp_ty: ty::t, dst: ValueRef, src: ValueRef, count: ValueRef) -> ValueRef {\n+    let ccx = bcx.ccx();\n+    let lltp_ty = type_of::type_of(ccx, tp_ty);\n+    let align = C_i32(ccx, machine::llalign_of_min(ccx, lltp_ty) as i32);\n+    let size = machine::llsize_of(ccx, lltp_ty);\n+    let int_size = machine::llbitsize_of_real(ccx, ccx.int_type);\n+    let name = if allow_overlap {\n+        if int_size == 32 {\n+            \"llvm.memmove.p0i8.p0i8.i32\"\n+        } else {\n+            \"llvm.memmove.p0i8.p0i8.i64\"\n         }\n-    }\n-    ccx.sess().abort_if_errors();\n+    } else {\n+        if int_size == 32 {\n+            \"llvm.memcpy.p0i8.p0i8.i32\"\n+        } else {\n+            \"llvm.memcpy.p0i8.p0i8.i64\"\n+        }\n+    };\n+\n+    let dst_ptr = PointerCast(bcx, dst, Type::i8p(ccx));\n+    let src_ptr = PointerCast(bcx, src, Type::i8p(ccx));\n+    let llfn = ccx.get_intrinsic(&name);\n+\n+    Call(bcx, llfn, [dst_ptr, src_ptr, Mul(bcx, size, count), align,\n+                     C_bool(ccx, volatile)], [])\n }\n \n+fn memset_intrinsic(bcx: &Block, volatile: bool, tp_ty: ty::t,\n+                    dst: ValueRef, val: ValueRef, count: ValueRef) -> ValueRef {\n+    let ccx = bcx.ccx();\n+    let lltp_ty = type_of::type_of(ccx, tp_ty);\n+    let align = C_i32(ccx, machine::llalign_of_min(ccx, lltp_ty) as i32);\n+    let size = machine::llsize_of(ccx, lltp_ty);\n+    let name = if machine::llbitsize_of_real(ccx, ccx.int_type) == 32 {\n+        \"llvm.memset.p0i8.i32\"\n+    } else {\n+        \"llvm.memset.p0i8.i64\"\n+    };\n+\n+    let dst_ptr = PointerCast(bcx, dst, Type::i8p(ccx));\n+    let llfn = ccx.get_intrinsic(&name);\n+\n+    Call(bcx, llfn, [dst_ptr, val, Mul(bcx, size, count), align,\n+                     C_bool(ccx, volatile)], [])\n+}\n+\n+fn count_zeros_intrinsic(bcx: &Block, name: &'static str, val: ValueRef) -> ValueRef {\n+    let y = C_bool(bcx.ccx(), false);\n+    let llfn = bcx.ccx().get_intrinsic(&name);\n+    Call(bcx, llfn, [val, y], [])\n+}\n+\n+fn with_overflow_intrinsic(bcx: &Block, name: &'static str, t: ty::t,\n+                           a: ValueRef, b: ValueRef) -> ValueRef {\n+    let llfn = bcx.ccx().get_intrinsic(&name);\n+\n+    // Convert `i1` to a `bool`, and write it to the out parameter\n+    let val = Call(bcx, llfn, [a, b], []);\n+    let result = ExtractValue(bcx, val, 0);\n+    let overflow = ZExt(bcx, ExtractValue(bcx, val, 1), Type::bool(bcx.ccx()));\n+    let ret = C_undef(type_of::type_of(bcx.ccx(), t));\n+    let ret = InsertValue(bcx, ret, result, 0);\n+    let ret = InsertValue(bcx, ret, overflow, 1);\n+\n+    ret\n+}"}, {"sha": "82dfd29320589ac046eedba399b00c2f0159f269", "filename": "src/librustc/middle/trans/monomorphize.rs", "status": "modified", "additions": 1, "deletions": 12, "changes": 13, "blob_url": "https://github.com/rust-lang/rust/blob/a1bd5d359b8e4ebc1b952f96e4f1d4658bc29b26/src%2Flibrustc%2Fmiddle%2Ftrans%2Fmonomorphize.rs", "raw_url": "https://github.com/rust-lang/rust/raw/a1bd5d359b8e4ebc1b952f96e4f1d4658bc29b26/src%2Flibrustc%2Fmiddle%2Ftrans%2Fmonomorphize.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Fmiddle%2Ftrans%2Fmonomorphize.rs?ref=a1bd5d359b8e4ebc1b952f96e4f1d4658bc29b26", "patch": "@@ -18,7 +18,6 @@ use middle::trans::base::{trans_enum_variant, push_ctxt, get_item_val};\n use middle::trans::base::{trans_fn, decl_internal_rust_fn};\n use middle::trans::base;\n use middle::trans::common::*;\n-use middle::trans::intrinsic;\n use middle::ty;\n use middle::typeck;\n use util::ppaux::Repr;\n@@ -158,17 +157,6 @@ pub fn monomorphic_fn(ccx: &CrateContext,\n               }\n             }\n         }\n-        ast_map::NodeForeignItem(i) => {\n-            let simple = intrinsic::get_simple_intrinsic(ccx, &*i);\n-            match simple {\n-                Some(decl) => decl,\n-                None => {\n-                    let d = mk_lldecl();\n-                    intrinsic::trans_intrinsic(ccx, d, &*i, &psubsts, ref_id);\n-                    d\n-                }\n-            }\n-        }\n         ast_map::NodeVariant(v) => {\n             let parent = ccx.tcx.map.get_parent(fn_id.node);\n             let tvs = ty::enum_variants(ccx.tcx(), local_def(parent));\n@@ -223,6 +211,7 @@ pub fn monomorphic_fn(ccx: &CrateContext,\n         }\n \n         // Ugh -- but this ensures any new variants won't be forgotten\n+        ast_map::NodeForeignItem(..) |\n         ast_map::NodeLifetime(..) |\n         ast_map::NodeExpr(..) |\n         ast_map::NodeStmt(..) |"}, {"sha": "f54ab190d5ef8d976f7b2e06d06b4470cfe82cce", "filename": "src/librustc/middle/trans/type_of.rs", "status": "modified", "additions": 3, "deletions": 1, "changes": 4, "blob_url": "https://github.com/rust-lang/rust/blob/a1bd5d359b8e4ebc1b952f96e4f1d4658bc29b26/src%2Flibrustc%2Fmiddle%2Ftrans%2Ftype_of.rs", "raw_url": "https://github.com/rust-lang/rust/raw/a1bd5d359b8e4ebc1b952f96e4f1d4658bc29b26/src%2Flibrustc%2Fmiddle%2Ftrans%2Ftype_of.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Fmiddle%2Ftrans%2Ftype_of.rs?ref=a1bd5d359b8e4ebc1b952f96e4f1d4658bc29b26", "patch": "@@ -75,11 +75,13 @@ pub fn type_of_fn_from_ty(cx: &CrateContext, fty: ty::t) -> Type {\n             type_of_rust_fn(cx, true, f.sig.inputs.as_slice(), f.sig.output)\n         }\n         ty::ty_bare_fn(ref f) => {\n-            if f.abi == abi::Rust || f.abi == abi::RustIntrinsic {\n+            if f.abi == abi::Rust {\n                 type_of_rust_fn(cx,\n                                 false,\n                                 f.sig.inputs.as_slice(),\n                                 f.sig.output)\n+            } else if f.abi == abi::RustIntrinsic {\n+                cx.sess().bug(\"type_of_fn_from_ty given intrinsic\")\n             } else {\n                 foreign::lltype_for_foreign_fn(cx, fty)\n             }"}]}