{"sha": "b86161ad9c2ba27d8b2c899a7adead7d4ebd54bd", "node_id": "MDY6Q29tbWl0NzI0NzEyOmI4NjE2MWFkOWMyYmEyN2Q4YjJjODk5YTdhZGVhZDdkNGViZDU0YmQ=", "commit": {"author": {"name": "Tyson Nottingham", "email": "tgnottingham@gmail.com", "date": "2020-10-05T02:39:17Z"}, "committer": {"name": "Tyson Nottingham", "email": "tgnottingham@gmail.com", "date": "2020-10-05T07:47:26Z"}, "message": "SipHasher128: use more named constants, update comments", "tree": {"sha": "b0c174921341d0f68fe491c58037d1509d690750", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/b0c174921341d0f68fe491c58037d1509d690750"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/b86161ad9c2ba27d8b2c899a7adead7d4ebd54bd", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/b86161ad9c2ba27d8b2c899a7adead7d4ebd54bd", "html_url": "https://github.com/rust-lang/rust/commit/b86161ad9c2ba27d8b2c899a7adead7d4ebd54bd", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/b86161ad9c2ba27d8b2c899a7adead7d4ebd54bd/comments", "author": {"login": "tgnottingham", "id": 3668166, "node_id": "MDQ6VXNlcjM2NjgxNjY=", "avatar_url": "https://avatars.githubusercontent.com/u/3668166?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tgnottingham", "html_url": "https://github.com/tgnottingham", "followers_url": "https://api.github.com/users/tgnottingham/followers", "following_url": "https://api.github.com/users/tgnottingham/following{/other_user}", "gists_url": "https://api.github.com/users/tgnottingham/gists{/gist_id}", "starred_url": "https://api.github.com/users/tgnottingham/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tgnottingham/subscriptions", "organizations_url": "https://api.github.com/users/tgnottingham/orgs", "repos_url": "https://api.github.com/users/tgnottingham/repos", "events_url": "https://api.github.com/users/tgnottingham/events{/privacy}", "received_events_url": "https://api.github.com/users/tgnottingham/received_events", "type": "User", "site_admin": false}, "committer": {"login": "tgnottingham", "id": 3668166, "node_id": "MDQ6VXNlcjM2NjgxNjY=", "avatar_url": "https://avatars.githubusercontent.com/u/3668166?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tgnottingham", "html_url": "https://github.com/tgnottingham", "followers_url": "https://api.github.com/users/tgnottingham/followers", "following_url": "https://api.github.com/users/tgnottingham/following{/other_user}", "gists_url": "https://api.github.com/users/tgnottingham/gists{/gist_id}", "starred_url": "https://api.github.com/users/tgnottingham/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tgnottingham/subscriptions", "organizations_url": "https://api.github.com/users/tgnottingham/orgs", "repos_url": "https://api.github.com/users/tgnottingham/repos", "events_url": "https://api.github.com/users/tgnottingham/events{/privacy}", "received_events_url": "https://api.github.com/users/tgnottingham/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "f6f96e2a8757717ca8d701d26d37b067e95bb584", "url": "https://api.github.com/repos/rust-lang/rust/commits/f6f96e2a8757717ca8d701d26d37b067e95bb584", "html_url": "https://github.com/rust-lang/rust/commit/f6f96e2a8757717ca8d701d26d37b067e95bb584"}], "stats": {"total": 104, "additions": 54, "deletions": 50}, "files": [{"sha": "8b91407acffa9bf5822aac25bcbd0ddc70464f9f", "filename": "compiler/rustc_data_structures/src/sip128.rs", "status": "modified", "additions": 54, "deletions": 50, "changes": 104, "blob_url": "https://github.com/rust-lang/rust/blob/b86161ad9c2ba27d8b2c899a7adead7d4ebd54bd/compiler%2Frustc_data_structures%2Fsrc%2Fsip128.rs", "raw_url": "https://github.com/rust-lang/rust/raw/b86161ad9c2ba27d8b2c899a7adead7d4ebd54bd/compiler%2Frustc_data_structures%2Fsrc%2Fsip128.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_data_structures%2Fsrc%2Fsip128.rs?ref=b86161ad9c2ba27d8b2c899a7adead7d4ebd54bd", "patch": "@@ -7,10 +7,11 @@ use std::ptr;\n #[cfg(test)]\n mod tests;\n \n+const ELEM_SIZE: usize = mem::size_of::<u64>();\n const BUFFER_SIZE_ELEMS: usize = 8;\n-const BUFFER_SIZE_BYTES: usize = BUFFER_SIZE_ELEMS * mem::size_of::<u64>();\n+const BUFFER_SIZE_BYTES: usize = BUFFER_SIZE_ELEMS * ELEM_SIZE;\n const BUFFER_SIZE_ELEMS_SPILL: usize = BUFFER_SIZE_ELEMS + 1;\n-const BUFFER_SIZE_BYTES_SPILL: usize = BUFFER_SIZE_ELEMS_SPILL * mem::size_of::<u64>();\n+const BUFFER_SIZE_BYTES_SPILL: usize = BUFFER_SIZE_ELEMS_SPILL * ELEM_SIZE;\n const BUFFER_SPILL_INDEX: usize = BUFFER_SIZE_ELEMS_SPILL - 1;\n \n #[derive(Debug, Clone)]\n@@ -54,15 +55,16 @@ macro_rules! compress {\n     }};\n }\n \n-// Copies up to 8 bytes from source to destination. This may be faster than\n-// calling `ptr::copy_nonoverlapping` with an arbitrary count, since all of\n-// the copies have fixed sizes and thus avoid calling memcpy.\n+// Copies up to 8 bytes from source to destination. This performs better than\n+// `ptr::copy_nonoverlapping` on microbenchmarks and may perform better on real\n+// workloads since all of the copies have fixed sizes and avoid calling memcpy.\n #[inline]\n unsafe fn copy_nonoverlapping_small(src: *const u8, dst: *mut u8, count: usize) {\n-    debug_assert!(count <= 8);\n+    const COUNT_MAX: usize = 8;\n+    debug_assert!(count <= COUNT_MAX);\n \n-    if count == 8 {\n-        ptr::copy_nonoverlapping(src, dst, 8);\n+    if count == COUNT_MAX {\n+        ptr::copy_nonoverlapping(src, dst, COUNT_MAX);\n         return;\n     }\n \n@@ -85,7 +87,7 @@ unsafe fn copy_nonoverlapping_small(src: *const u8, dst: *mut u8, count: usize)\n     debug_assert_eq!(i, count);\n }\n \n-// Implementation\n+// # Implementation\n //\n // This implementation uses buffering to reduce the hashing cost for inputs\n // consisting of many small integers. Buffering simplifies the integration of\n@@ -99,10 +101,11 @@ unsafe fn copy_nonoverlapping_small(src: *const u8, dst: *mut u8, count: usize)\n //\n // When a write fills the buffer, a buffer processing function is invoked to\n // hash all of the buffered input. The buffer processing functions are marked\n-// #[inline(never)] so that they aren't inlined into the append functions, which\n-// ensures the more frequently called append functions remain inlineable and\n-// don't include register pushing/popping that would only be made necessary by\n-// inclusion of the complex buffer processing path which uses those registers.\n+// `#[inline(never)]` so that they aren't inlined into the append functions,\n+// which ensures the more frequently called append functions remain inlineable\n+// and don't include register pushing/popping that would only be made necessary\n+// by inclusion of the complex buffer processing path which uses those\n+// registers.\n //\n // The buffer includes a \"spill\"--an extra element at the end--which simplifies\n // the integer write buffer processing path. The value that fills the buffer can\n@@ -118,7 +121,7 @@ unsafe fn copy_nonoverlapping_small(src: *const u8, dst: *mut u8, count: usize)\n // efficiently implemented with an uninitialized buffer. On the other hand, an\n // uninitialized buffer may become more important should a larger one be used.\n //\n-// Platform Dependence\n+// # Platform Dependence\n //\n // The SipHash algorithm operates on byte sequences. It parses the input stream\n // as 8-byte little-endian integers. Therefore, given the same byte sequence, it\n@@ -131,14 +134,14 @@ unsafe fn copy_nonoverlapping_small(src: *const u8, dst: *mut u8, count: usize)\n // native size), or independent (by converting to a common size), supposing the\n // values can be represented in 32 bits.\n //\n-// In order to make SipHasher128 consistent with SipHasher in libstd, we choose\n-// to do the integer to byte sequence conversion in the platform-dependent way.\n-// Clients can achieve (nearly) platform-independent hashing by widening `isize`\n-// and `usize` integers to 64 bits on 32-bit systems and byte-swapping integers\n-// on big-endian systems before passing them to the writing functions. This\n-// causes the input byte sequence to look identical on big- and little- endian\n-// systems (supposing `isize` and `usize` values can be represented in 32 bits),\n-// which ensures platform-independent results.\n+// In order to make `SipHasher128` consistent with `SipHasher` in libstd, we\n+// choose to do the integer to byte sequence conversion in the platform-\n+// dependent way. Clients can achieve (nearly) platform-independent hashing by\n+// widening `isize` and `usize` integers to 64 bits on 32-bit systems and\n+// byte-swapping integers on big-endian systems before passing them to the\n+// writing functions. This causes the input byte sequence to look identical on\n+// big- and little- endian systems (supposing `isize` and `usize` values can be\n+// represented in 32 bits), which ensures platform-independent results.\n impl SipHasher128 {\n     #[inline]\n     pub fn new_with_keys(key0: u64, key1: u64) -> SipHasher128 {\n@@ -156,7 +159,7 @@ impl SipHasher128 {\n         };\n \n         unsafe {\n-            // Initialize spill because we read from it in short_write_process_buffer.\n+            // Initialize spill because we read from it in `short_write_process_buffer`.\n             *hasher.buf.get_unchecked_mut(BUFFER_SPILL_INDEX) = MaybeUninit::zeroed();\n         }\n \n@@ -190,9 +193,9 @@ impl SipHasher128 {\n     // A specialized write function for values with size <= 8 that should only\n     // be called when the write would cause the buffer to fill.\n     //\n-    // SAFETY: the write of x into self.buf starting at byte offset self.nbuf\n-    // must cause self.buf to become fully initialized (and not overflow) if it\n-    // wasn't already.\n+    // SAFETY: the write of `x` into `self.buf` starting at byte offset\n+    // `self.nbuf` must cause `self.buf` to become fully initialized (and not\n+    // overflow) if it wasn't already.\n     #[inline(never)]\n     unsafe fn short_write_process_buffer<T>(&mut self, x: T) {\n         let size = mem::size_of::<T>();\n@@ -223,7 +226,7 @@ impl SipHasher128 {\n         ptr::copy_nonoverlapping(src, self.buf.as_mut_ptr() as *mut u8, size - 1);\n \n         // This function should only be called when the write fills the buffer.\n-        // Therefore, when size == 1, the new self.nbuf must be zero. The size\n+        // Therefore, when size == 1, the new `self.nbuf` must be zero. The size\n         // is statically known, so the branch is optimized away.\n         self.nbuf = if size == 1 { 0 } else { nbuf + size - BUFFER_SIZE_BYTES };\n         self.processed += BUFFER_SIZE_BYTES;\n@@ -240,7 +243,7 @@ impl SipHasher128 {\n             unsafe {\n                 let dst = (self.buf.as_mut_ptr() as *mut u8).add(nbuf);\n \n-                if length < 8 {\n+                if length <= 8 {\n                     copy_nonoverlapping_small(msg.as_ptr(), dst, length);\n                 } else {\n                     // This memcpy is *not* optimized away.\n@@ -259,9 +262,9 @@ impl SipHasher128 {\n     // A write function for byte slices that should only be called when the\n     // write would cause the buffer to fill.\n     //\n-    // SAFETY: self.buf must be initialized up to the byte offset self.nbuf, and\n-    // msg must contain enough bytes to initialize the rest of the element\n-    // containing the byte offset self.nbuf.\n+    // SAFETY: `self.buf` must be initialized up to the byte offset `self.nbuf`,\n+    // and `msg` must contain enough bytes to initialize the rest of the element\n+    // containing the byte offset `self.nbuf`.\n     #[inline(never)]\n     unsafe fn slice_write_process_buffer(&mut self, msg: &[u8]) {\n         let length = msg.len();\n@@ -272,19 +275,20 @@ impl SipHasher128 {\n         // Always copy first part of input into current element of buffer.\n         // This function should only be called when the write fills the buffer,\n         // so we know that there is enough input to fill the current element.\n-        let valid_in_elem = nbuf & 0x7;\n-        let needed_in_elem = 8 - valid_in_elem;\n+        let valid_in_elem = nbuf % ELEM_SIZE;\n+        let needed_in_elem = ELEM_SIZE - valid_in_elem;\n \n         let src = msg.as_ptr();\n         let dst = (self.buf.as_mut_ptr() as *mut u8).add(nbuf);\n         copy_nonoverlapping_small(src, dst, needed_in_elem);\n \n         // Process buffer.\n \n-        // Using nbuf / 8 + 1 rather than (nbuf + needed_in_elem) / 8 to show\n-        // the compiler that this loop's upper bound is > 0. We know that is\n-        // true, because last step ensured we have a full element in the buffer.\n-        let last = nbuf / 8 + 1;\n+        // Using `nbuf / ELEM_SIZE + 1` rather than `(nbuf + needed_in_elem) /\n+        // ELEM_SIZE` to show the compiler that this loop's upper bound is > 0.\n+        // We know that is true, because last step ensured we have a full\n+        // element in the buffer.\n+        let last = nbuf / ELEM_SIZE + 1;\n \n         for i in 0..last {\n             let elem = self.buf.get_unchecked(i).assume_init().to_le();\n@@ -293,26 +297,26 @@ impl SipHasher128 {\n             self.state.v0 ^= elem;\n         }\n \n-        // Process the remaining u64-sized chunks of input.\n+        // Process the remaining element-sized chunks of input.\n         let mut processed = needed_in_elem;\n         let input_left = length - processed;\n-        let u64s_left = input_left / 8;\n-        let u8s_left = input_left & 0x7;\n+        let elems_left = input_left / ELEM_SIZE;\n+        let extra_bytes_left = input_left % ELEM_SIZE;\n \n-        for _ in 0..u64s_left {\n+        for _ in 0..elems_left {\n             let elem = (msg.as_ptr().add(processed) as *const u64).read_unaligned().to_le();\n             self.state.v3 ^= elem;\n             Sip24Rounds::c_rounds(&mut self.state);\n             self.state.v0 ^= elem;\n-            processed += 8;\n+            processed += ELEM_SIZE;\n         }\n \n         // Copy remaining input into start of buffer.\n         let src = msg.as_ptr().add(processed);\n         let dst = self.buf.as_mut_ptr() as *mut u8;\n-        copy_nonoverlapping_small(src, dst, u8s_left);\n+        copy_nonoverlapping_small(src, dst, extra_bytes_left);\n \n-        self.nbuf = u8s_left;\n+        self.nbuf = extra_bytes_left;\n         self.processed += nbuf + processed;\n     }\n \n@@ -321,7 +325,7 @@ impl SipHasher128 {\n         debug_assert!(self.nbuf < BUFFER_SIZE_BYTES);\n \n         // Process full elements in buffer.\n-        let last = self.nbuf / 8;\n+        let last = self.nbuf / ELEM_SIZE;\n \n         // Since we're consuming self, avoid updating members for a potential\n         // performance gain.\n@@ -335,14 +339,14 @@ impl SipHasher128 {\n         }\n \n         // Get remaining partial element.\n-        let elem = if self.nbuf % 8 != 0 {\n+        let elem = if self.nbuf % ELEM_SIZE != 0 {\n             unsafe {\n                 // Ensure element is initialized by writing zero bytes. At most\n-                // seven are required given the above check. It's safe to write\n-                // this many because we have the spill element and we maintain\n-                // self.nbuf such that this write will start before the spill.\n+                // `ELEM_SIZE - 1` are required given the above check. It's safe\n+                // to write this many because we have the spill and we maintain\n+                // `self.nbuf` such that this write will start before the spill.\n                 let dst = (self.buf.as_mut_ptr() as *mut u8).add(self.nbuf);\n-                ptr::write_bytes(dst, 0, 7);\n+                ptr::write_bytes(dst, 0, ELEM_SIZE - 1);\n                 self.buf.get_unchecked(last).assume_init().to_le()\n             }\n         } else {"}]}