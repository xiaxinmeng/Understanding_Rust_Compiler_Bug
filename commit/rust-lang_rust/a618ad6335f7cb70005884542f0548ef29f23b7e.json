{"sha": "a618ad6335f7cb70005884542f0548ef29f23b7e", "node_id": "MDY6Q29tbWl0NzI0NzEyOmE2MThhZDYzMzVmN2NiNzAwMDU4ODQ1NDJmMDU0OGVmMjlmMjNiN2U=", "commit": {"author": {"name": "Dan Robertson", "email": "dan@dlrobertson.com", "date": "2019-02-02T16:34:09Z"}, "committer": {"name": "Dan Robertson", "email": "dan@dlrobertson.com", "date": "2019-02-27T15:21:50Z"}, "message": "Refactor FunctionCx::codgen_terminator\n\n - Move closures defined in codegen_terminator into a separate helper\n   structure and implementation.\n - Create helper functions for each of the complex match arms on the\n   terminators kind in codegen_terminator.", "tree": {"sha": "88d5ba249064ee40bc5cdec76a7c0ced9236b664", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/88d5ba249064ee40bc5cdec76a7c0ced9236b664"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/a618ad6335f7cb70005884542f0548ef29f23b7e", "comment_count": 0, "verification": {"verified": true, "reason": "valid", "signature": "-----BEGIN PGP SIGNATURE-----\n\niQIzBAABCAAdFiEEkZ4/WpQ0/P8yWDLRTebu+Etcl4MFAlx2qxAACgkQTebu+Etc\nl4PpSBAAmWr/sTyRJ8o1fBuUKJ7wbUXDmz/pHzqwrERZ2Uf175/5yXkWP9PQXa00\nYGfLxrGw9dssX7Wn2a4+GNfWE8Bfts3dnoWN4dUEnEEyBp+AybfOdt8Jimduqtiu\n/58R46eUs2K9474ebS4yzUCaKxZ2C44Yt7VzI7o7ewL93RquVY//moxrQBcfC1pF\nydN3i/ovbfI/G7k8g88p/0EsGlx6dM/XkQKmvjF008BcEa7SMJqlZGEbmH/kK0RX\n/g/uzFDTPRC3E95CZUy2Z4L8SSuOA/C8iuSQsYHoX3CAEZXXJcd3LthxgbH7hj7n\nK3NWHvPKj7xa2B6g+aQnIUybqDMfqe4MKP0BbLRxNHl6YKkVz69R2k1SqvJSkaHh\nEs+kpjxU0xJqD4CqBCU1W2l4aH/nJXtTy6DdoH8jNOQ8l74cybwGILn9DCIa5/t7\nDMdIPIwLTGIm4Z/n7Mf0uRuvX2JwjW6PYvtXBkdxaKQaeTy7WJHPUYd5rr60foz3\n8QT+De9Y1BaFHsgXmPTs7xc8K76DNSG3oEh8oEnFvKusBXNuwKiBnz7A6gj03MVc\nurUD65uXbByDKSvqZ0p0gdwAqGaPRjT2lG0quz96eAMam//sCl4pjfeX40ke7NoB\njYPwG+a74w70skvxrkdTo0u1kdE8b6ZnCys5H/vsvZlNvB45vhQ=\n=mR7h\n-----END PGP SIGNATURE-----", "payload": "tree 88d5ba249064ee40bc5cdec76a7c0ced9236b664\nparent 1a6e9e24083c3250f76ca1ad6a0142d9ab3223d0\nauthor Dan Robertson <dan@dlrobertson.com> 1549125249 +0000\ncommitter Dan Robertson <dan@dlrobertson.com> 1551280910 -0500\n\nRefactor FunctionCx::codgen_terminator\n\n - Move closures defined in codegen_terminator into a separate helper\n   structure and implementation.\n - Create helper functions for each of the complex match arms on the\n   terminators kind in codegen_terminator.\n"}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/a618ad6335f7cb70005884542f0548ef29f23b7e", "html_url": "https://github.com/rust-lang/rust/commit/a618ad6335f7cb70005884542f0548ef29f23b7e", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/a618ad6335f7cb70005884542f0548ef29f23b7e/comments", "author": {"login": "dlrobertson", "id": 7504153, "node_id": "MDQ6VXNlcjc1MDQxNTM=", "avatar_url": "https://avatars.githubusercontent.com/u/7504153?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dlrobertson", "html_url": "https://github.com/dlrobertson", "followers_url": "https://api.github.com/users/dlrobertson/followers", "following_url": "https://api.github.com/users/dlrobertson/following{/other_user}", "gists_url": "https://api.github.com/users/dlrobertson/gists{/gist_id}", "starred_url": "https://api.github.com/users/dlrobertson/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dlrobertson/subscriptions", "organizations_url": "https://api.github.com/users/dlrobertson/orgs", "repos_url": "https://api.github.com/users/dlrobertson/repos", "events_url": "https://api.github.com/users/dlrobertson/events{/privacy}", "received_events_url": "https://api.github.com/users/dlrobertson/received_events", "type": "User", "site_admin": false}, "committer": {"login": "dlrobertson", "id": 7504153, "node_id": "MDQ6VXNlcjc1MDQxNTM=", "avatar_url": "https://avatars.githubusercontent.com/u/7504153?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dlrobertson", "html_url": "https://github.com/dlrobertson", "followers_url": "https://api.github.com/users/dlrobertson/followers", "following_url": "https://api.github.com/users/dlrobertson/following{/other_user}", "gists_url": "https://api.github.com/users/dlrobertson/gists{/gist_id}", "starred_url": "https://api.github.com/users/dlrobertson/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dlrobertson/subscriptions", "organizations_url": "https://api.github.com/users/dlrobertson/orgs", "repos_url": "https://api.github.com/users/dlrobertson/repos", "events_url": "https://api.github.com/users/dlrobertson/events{/privacy}", "received_events_url": "https://api.github.com/users/dlrobertson/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "1a6e9e24083c3250f76ca1ad6a0142d9ab3223d0", "url": "https://api.github.com/repos/rust-lang/rust/commits/1a6e9e24083c3250f76ca1ad6a0142d9ab3223d0", "html_url": "https://github.com/rust-lang/rust/commit/1a6e9e24083c3250f76ca1ad6a0142d9ab3223d0"}], "stats": {"total": 1442, "additions": 771, "deletions": 671}, "files": [{"sha": "684dfac991b4cbca175228d48cf38782b1336233", "filename": "src/librustc_codegen_ssa/mir/block.rs", "status": "modified", "additions": 771, "deletions": 671, "changes": 1442, "blob_url": "https://github.com/rust-lang/rust/blob/a618ad6335f7cb70005884542f0548ef29f23b7e/src%2Flibrustc_codegen_ssa%2Fmir%2Fblock.rs", "raw_url": "https://github.com/rust-lang/rust/raw/a618ad6335f7cb70005884542f0548ef29f23b7e/src%2Flibrustc_codegen_ssa%2Fmir%2Fblock.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_codegen_ssa%2Fmir%2Fblock.rs?ref=a618ad6335f7cb70005884542f0548ef29f23b7e", "patch": "@@ -13,6 +13,8 @@ use crate::meth;\n \n use crate::traits::*;\n \n+use std::borrow::Cow;\n+\n use syntax::symbol::Symbol;\n use syntax_pos::Pos;\n \n@@ -21,764 +23,862 @@ use super::place::PlaceRef;\n use super::operand::{OperandValue, OperandRef};\n use super::operand::OperandValue::{Pair, Ref, Immediate};\n \n-impl<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>> FunctionCx<'a, 'tcx, Bx> {\n-    pub fn codegen_block(\n-        &mut self,\n-        bb: mir::BasicBlock,\n-    ) {\n-        let mut bx = self.build_block(bb);\n-        let data = &self.mir[bb];\n+/// Used by `FunctionCx::codegen_terminator` for emitting common patterns\n+/// e.g., creating a basic block, calling a function, etc.\n+struct TerminatorCodegenHelper<'a, 'tcx> {\n+    bb: &'a mir::BasicBlock,\n+    terminator: &'a mir::Terminator<'tcx>,\n+    funclet_bb: Option<mir::BasicBlock>,\n+}\n \n-        debug!(\"codegen_block({:?}={:?})\", bb, data);\n+impl<'a, 'tcx> TerminatorCodegenHelper<'a, 'tcx> {\n+    /// Returns the associated funclet from `FunctionCx::funclets` for the\n+    /// `funclet_bb` member if it is not `None`.\n+    fn funclet<'c, 'b, Bx: BuilderMethods<'b, 'tcx>>(\n+        &self,\n+        fx: &'c mut FunctionCx<'b, 'tcx, Bx>,\n+    ) -> Option<&'c Bx::Funclet> {\n+        match self.funclet_bb {\n+            Some(funcl) => fx.funclets[funcl].as_ref(),\n+            None => None,\n+        }\n+    }\n \n-        for statement in &data.statements {\n-            bx = self.codegen_statement(bx, statement);\n+    fn lltarget<'b, 'c, Bx: BuilderMethods<'b, 'tcx>>(\n+        &self,\n+        fx: &'c mut FunctionCx<'b, 'tcx, Bx>,\n+        target: mir::BasicBlock,\n+    ) -> (Bx::BasicBlock, bool) {\n+        let span = self.terminator.source_info.span;\n+        let lltarget = fx.blocks[target];\n+        let target_funclet = fx.cleanup_kinds[target].funclet_bb(target);\n+        match (self.funclet_bb, target_funclet) {\n+            (None, None) => (lltarget, false),\n+            (Some(f), Some(t_f)) if f == t_f || !base::wants_msvc_seh(fx.cx.tcx().sess) =>\n+                (lltarget, false),\n+            // jump *into* cleanup - need a landing pad if GNU\n+            (None, Some(_)) => (fx.landing_pad_to(target), false),\n+            (Some(_), None) => span_bug!(span, \"{:?} - jump out of cleanup?\", self.terminator),\n+            (Some(_), Some(_)) => (fx.landing_pad_to(target), true),\n         }\n+    }\n \n-        self.codegen_terminator(bx, bb, data.terminator());\n+    /// Create a basic block.\n+    fn llblock<'c, 'b, Bx: BuilderMethods<'b, 'tcx>>(\n+        &self,\n+        fx: &'c mut FunctionCx<'b, 'tcx, Bx>,\n+        target: mir::BasicBlock,\n+    ) -> Bx::BasicBlock {\n+        let (lltarget, is_cleanupret) = self.lltarget(fx, target);\n+        if is_cleanupret {\n+            // MSVC cross-funclet jump - need a trampoline\n+\n+            debug!(\"llblock: creating cleanup trampoline for {:?}\", target);\n+            let name = &format!(\"{:?}_cleanup_trampoline_{:?}\", self.bb, target);\n+            let mut trampoline = fx.new_block(name);\n+            trampoline.cleanup_ret(self.funclet(fx).unwrap(),\n+                                   Some(lltarget));\n+            trampoline.llbb()\n+        } else {\n+            lltarget\n+        }\n     }\n \n-    fn codegen_terminator(\n-        &mut self,\n-        mut bx: Bx,\n-        bb: mir::BasicBlock,\n-        terminator: &mir::Terminator<'tcx>\n+    fn funclet_br<'c, 'b, Bx: BuilderMethods<'b, 'tcx>>(\n+        &self,\n+        fx: &'c mut FunctionCx<'b, 'tcx, Bx>,\n+        bx: &mut Bx,\n+        target: mir::BasicBlock,\n     ) {\n-        debug!(\"codegen_terminator: {:?}\", terminator);\n-\n-        // Create the cleanup bundle, if needed.\n-        let tcx = self.cx.tcx();\n-        let span = terminator.source_info.span;\n-        let funclet_bb = self.cleanup_kinds[bb].funclet_bb(bb);\n+        let (lltarget, is_cleanupret) = self.lltarget(fx, target);\n+        if is_cleanupret {\n+            // micro-optimization: generate a `ret` rather than a jump\n+            // to a trampoline.\n+            bx.cleanup_ret(self.funclet(fx).unwrap(), Some(lltarget));\n+        } else {\n+            bx.br(lltarget);\n+        }\n+    }\n \n-        // HACK(eddyb) force the right lifetimes, NLL can't figure them out.\n-        fn funclet_closure_factory<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>>(\n-            funclet_bb: Option<mir::BasicBlock>\n-        ) -> impl for<'b> Fn(\n-            &'b FunctionCx<'a, 'tcx, Bx>,\n-        ) -> Option<&'b Bx::Funclet> {\n-            move |this| {\n-                match funclet_bb {\n-                    Some(funclet_bb) => this.funclets[funclet_bb].as_ref(),\n-                    None => None,\n-                }\n+    /// Call `fn_ptr` of `fn_ty` with the arguments `llargs`, the optional\n+    /// return destination `destination` and the cleanup function `cleanup`.\n+    fn do_call<'c, 'b, Bx: BuilderMethods<'b, 'tcx>>(\n+        &self,\n+        fx: &'c mut FunctionCx<'b, 'tcx, Bx>,\n+        bx: &mut Bx,\n+        fn_ty: FnType<'tcx, Ty<'tcx>>,\n+        fn_ptr: Bx::Value,\n+        llargs: &[Bx::Value],\n+        destination: Option<(ReturnDest<'tcx, Bx::Value>, mir::BasicBlock)>,\n+        cleanup: Option<mir::BasicBlock>,\n+    ) {\n+        if let Some(cleanup) = cleanup {\n+            let ret_bx = if let Some((_, target)) = destination {\n+                fx.blocks[target]\n+            } else {\n+                fx.unreachable_block()\n+            };\n+            let invokeret = bx.invoke(fn_ptr,\n+                                      &llargs,\n+                                      ret_bx,\n+                                      self.llblock(fx, cleanup),\n+                                      self.funclet(fx));\n+            bx.apply_attrs_callsite(&fn_ty, invokeret);\n+\n+            if let Some((ret_dest, target)) = destination {\n+                let mut ret_bx = fx.build_block(target);\n+                fx.set_debug_loc(&mut ret_bx, self.terminator.source_info);\n+                fx.store_return(&mut ret_bx, ret_dest, &fn_ty.ret, invokeret);\n             }\n-        }\n-        let funclet = funclet_closure_factory(funclet_bb);\n-\n-        let lltarget = |this: &mut Self, target: mir::BasicBlock| {\n-            let lltarget = this.blocks[target];\n-            let target_funclet = this.cleanup_kinds[target].funclet_bb(target);\n-            match (funclet_bb, target_funclet) {\n-                (None, None) => (lltarget, false),\n-                (Some(f), Some(t_f))\n-                    if f == t_f || !base::wants_msvc_seh(tcx.sess)\n-                    => (lltarget, false),\n-                (None, Some(_)) => {\n-                    // jump *into* cleanup - need a landing pad if GNU\n-                    (this.landing_pad_to(target), false)\n-                }\n-                (Some(_), None) => span_bug!(span, \"{:?} - jump out of cleanup?\", terminator),\n-                (Some(_), Some(_)) => {\n-                    (this.landing_pad_to(target), true)\n-                }\n+        } else {\n+            let llret = bx.call(fn_ptr, &llargs, self.funclet(fx));\n+            bx.apply_attrs_callsite(&fn_ty, llret);\n+            if fx.mir[*self.bb].is_cleanup {\n+                // Cleanup is always the cold path. Don't inline\n+                // drop glue. Also, when there is a deeply-nested\n+                // struct, there are \"symmetry\" issues that cause\n+                // exponential inlining - see issue #41696.\n+                bx.do_not_inline(llret);\n             }\n-        };\n-\n-        let llblock = |this: &mut Self, target: mir::BasicBlock| {\n-            let (lltarget, is_cleanupret) = lltarget(this, target);\n-            if is_cleanupret {\n-                // MSVC cross-funclet jump - need a trampoline\n \n-                debug!(\"llblock: creating cleanup trampoline for {:?}\", target);\n-                let name = &format!(\"{:?}_cleanup_trampoline_{:?}\", bb, target);\n-                let mut trampoline = this.new_block(name);\n-                trampoline.cleanup_ret(funclet(this).unwrap(), Some(lltarget));\n-                trampoline.llbb()\n+            if let Some((ret_dest, target)) = destination {\n+                fx.store_return(bx, ret_dest, &fn_ty.ret, llret);\n+                self.funclet_br(fx, bx, target);\n             } else {\n-                lltarget\n+                bx.unreachable();\n             }\n-        };\n-\n-        let funclet_br =\n-            |this: &mut Self, bx: &mut Bx, target: mir::BasicBlock| {\n-                let (lltarget, is_cleanupret) = lltarget(this, target);\n-                if is_cleanupret {\n-                    // micro-optimization: generate a `ret` rather than a jump\n-                    // to a trampoline.\n-                    bx.cleanup_ret(funclet(this).unwrap(), Some(lltarget));\n-                } else {\n-                    bx.br(lltarget);\n-                }\n-            };\n+        }\n+    }\n+}\n \n-        let do_call = |\n-            this: &mut Self,\n-            bx: &mut Bx,\n-            fn_ty: FnType<'tcx, Ty<'tcx>>,\n-            fn_ptr: Bx::Value,\n-            llargs: &[Bx::Value],\n-            destination: Option<(ReturnDest<'tcx, Bx::Value>, mir::BasicBlock)>,\n-            cleanup: Option<mir::BasicBlock>\n-        | {\n-            if let Some(cleanup) = cleanup {\n-                let ret_bx = if let Some((_, target)) = destination {\n-                    this.blocks[target]\n-                } else {\n-                    this.unreachable_block()\n-                };\n-                let invokeret = bx.invoke(fn_ptr,\n-                                          &llargs,\n-                                          ret_bx,\n-                                          llblock(this, cleanup),\n-                                          funclet(this));\n-                bx.apply_attrs_callsite(&fn_ty, invokeret);\n-\n-                if let Some((ret_dest, target)) = destination {\n-                    let mut ret_bx = this.build_block(target);\n-                    this.set_debug_loc(&mut ret_bx, terminator.source_info);\n-                    this.store_return(&mut ret_bx, ret_dest, &fn_ty.ret, invokeret);\n-                }\n+/// Codegen implementations for some terminator variants.\n+impl<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>> FunctionCx<'a, 'tcx, Bx> {\n+    /// Generates code for a `Resume` terminator.\n+    fn codegen_resume_terminator<'b>(\n+        &mut self,\n+        helper: TerminatorCodegenHelper<'b, 'tcx>,\n+        mut bx: Bx,\n+    ) {\n+        if let Some(funclet) = helper.funclet(self) {\n+            bx.cleanup_ret(funclet, None);\n+        } else {\n+            let slot = self.get_personality_slot(&mut bx);\n+            let lp0 = slot.project_field(&mut bx, 0);\n+            let lp0 = bx.load_operand(lp0).immediate();\n+            let lp1 = slot.project_field(&mut bx, 1);\n+            let lp1 = bx.load_operand(lp1).immediate();\n+            slot.storage_dead(&mut bx);\n+\n+            if !bx.sess().target.target.options.custom_unwind_resume {\n+                let mut lp = bx.const_undef(self.landing_pad_type());\n+                lp = bx.insert_value(lp, lp0, 0);\n+                lp = bx.insert_value(lp, lp1, 1);\n+                bx.resume(lp);\n             } else {\n-                let llret = bx.call(fn_ptr, &llargs, funclet(this));\n-                bx.apply_attrs_callsite(&fn_ty, llret);\n-                if this.mir[bb].is_cleanup {\n-                    // Cleanup is always the cold path. Don't inline\n-                    // drop glue. Also, when there is a deeply-nested\n-                    // struct, there are \"symmetry\" issues that cause\n-                    // exponential inlining - see issue #41696.\n-                    bx.do_not_inline(llret);\n-                }\n-\n-                if let Some((ret_dest, target)) = destination {\n-                    this.store_return(bx, ret_dest, &fn_ty.ret, llret);\n-                    funclet_br(this, bx, target);\n-                } else {\n-                    bx.unreachable();\n-                }\n+                bx.call(bx.eh_unwind_resume(), &[lp0],\n+                        helper.funclet(self));\n+                bx.unreachable();\n             }\n-        };\n+        }\n+    }\n \n-        self.set_debug_loc(&mut bx, terminator.source_info);\n-        match terminator.kind {\n-            mir::TerminatorKind::Resume => {\n-                if let Some(funclet) = funclet(self) {\n-                    bx.cleanup_ret(funclet, None);\n+    fn codegen_switchint_terminator<'b>(\n+        &mut self,\n+        helper: TerminatorCodegenHelper<'b, 'tcx>,\n+        mut bx: Bx,\n+        discr: &mir::Operand<'tcx>,\n+        switch_ty: Ty<'tcx>,\n+        values: &Cow<'tcx, [u128]>,\n+        targets: &Vec<mir::BasicBlock>,\n+    ) {\n+        let discr = self.codegen_operand(&mut bx, &discr);\n+        if targets.len() == 2 {\n+            // If there are two targets, emit br instead of switch\n+            let lltrue = helper.llblock(self, targets[0]);\n+            let llfalse = helper.llblock(self, targets[1]);\n+            if switch_ty == bx.tcx().types.bool {\n+                // Don't generate trivial icmps when switching on bool\n+                if let [0] = values[..] {\n+                    bx.cond_br(discr.immediate(), llfalse, lltrue);\n                 } else {\n-                    let slot = self.get_personality_slot(&mut bx);\n-                    let lp0 = slot.project_field(&mut bx, 0);\n-                    let lp0 = bx.load_operand(lp0).immediate();\n-                    let lp1 = slot.project_field(&mut bx, 1);\n-                    let lp1 = bx.load_operand(lp1).immediate();\n-                    slot.storage_dead(&mut bx);\n-\n-                    if !bx.sess().target.target.options.custom_unwind_resume {\n-                        let mut lp = bx.const_undef(self.landing_pad_type());\n-                        lp = bx.insert_value(lp, lp0, 0);\n-                        lp = bx.insert_value(lp, lp1, 1);\n-                        bx.resume(lp);\n-                    } else {\n-                        bx.call(bx.eh_unwind_resume(), &[lp0], funclet(self));\n-                        bx.unreachable();\n-                    }\n+                    assert_eq!(&values[..], &[1]);\n+                    bx.cond_br(discr.immediate(), lltrue, llfalse);\n                 }\n+            } else {\n+                let switch_llty = bx.immediate_backend_type(\n+                    bx.layout_of(switch_ty)\n+                );\n+                let llval = bx.const_uint_big(switch_llty, values[0]);\n+                let cmp = bx.icmp(IntPredicate::IntEQ, discr.immediate(), llval);\n+                bx.cond_br(cmp, lltrue, llfalse);\n+            }\n+        } else {\n+            let (otherwise, targets) = targets.split_last().unwrap();\n+            let switch = bx.switch(discr.immediate(),\n+                                   helper.llblock(self, *otherwise),\n+                                   values.len());\n+            let switch_llty = bx.immediate_backend_type(\n+                bx.layout_of(switch_ty)\n+            );\n+            for (&value, target) in values.iter().zip(targets) {\n+                let llval = bx.const_uint_big(switch_llty, value);\n+                let llbb = helper.llblock(self, *target);\n+                bx.add_case(switch, llval, llbb)\n             }\n+        }\n+    }\n \n-            mir::TerminatorKind::Abort => {\n-                bx.abort();\n-                bx.unreachable();\n+    fn codegen_return_terminator<'b>(\n+        &mut self,\n+        mut bx: Bx,\n+    ) {\n+        if self.fn_ty.variadic {\n+            if let Some(va_list) = self.va_list_ref {\n+                bx.va_end(va_list.llval);\n+            }\n+        }\n+        let llval = match self.fn_ty.ret.mode {\n+            PassMode::Ignore(IgnoreMode::Zst) | PassMode::Indirect(..) => {\n+                bx.ret_void();\n+                return;\n             }\n \n-            mir::TerminatorKind::Goto { target } => {\n-                funclet_br(self, &mut bx, target);\n+            PassMode::Ignore(IgnoreMode::CVarArgs) => {\n+                bug!(\"C-variadic arguments should never be the return type\");\n             }\n \n-            mir::TerminatorKind::SwitchInt { ref discr, switch_ty, ref values, ref targets } => {\n-                let discr = self.codegen_operand(&mut bx, discr);\n-                if targets.len() == 2 {\n-                    // If there are two targets, emit br instead of switch\n-                    let lltrue = llblock(self, targets[0]);\n-                    let llfalse = llblock(self, targets[1]);\n-                    if switch_ty == bx.tcx().types.bool {\n-                        // Don't generate trivial icmps when switching on bool\n-                        if let [0] = values[..] {\n-                            bx.cond_br(discr.immediate(), llfalse, lltrue);\n-                        } else {\n-                            assert_eq!(&values[..], &[1]);\n-                            bx.cond_br(discr.immediate(), lltrue, llfalse);\n-                        }\n-                    } else {\n-                        let switch_llty = bx.immediate_backend_type(\n-                            bx.layout_of(switch_ty)\n-                        );\n-                        let llval = bx.const_uint_big(switch_llty, values[0]);\n-                        let cmp = bx.icmp(IntPredicate::IntEQ, discr.immediate(), llval);\n-                        bx.cond_br(cmp, lltrue, llfalse);\n-                    }\n+            PassMode::Direct(_) | PassMode::Pair(..) => {\n+                let op =\n+                    self.codegen_consume(&mut bx, &mir::Place::Local(mir::RETURN_PLACE));\n+                if let Ref(llval, _, align) = op.val {\n+                    bx.load(llval, align)\n                 } else {\n-                    let (otherwise, targets) = targets.split_last().unwrap();\n-                    let switch = bx.switch(discr.immediate(),\n-                                           llblock(self, *otherwise),\n-                                           values.len());\n-                    let switch_llty = bx.immediate_backend_type(\n-                        bx.layout_of(switch_ty)\n-                    );\n-                    for (&value, target) in values.iter().zip(targets) {\n-                        let llval = bx.const_uint_big(switch_llty, value);\n-                        let llbb = llblock(self, *target);\n-                        bx.add_case(switch, llval, llbb)\n-                    }\n+                    op.immediate_or_packed_pair(&mut bx)\n                 }\n             }\n \n-            mir::TerminatorKind::Return => {\n-                if self.fn_ty.variadic {\n-                    if let Some(va_list) = self.va_list_ref {\n-                        bx.va_end(va_list.llval);\n+            PassMode::Cast(cast_ty) => {\n+                let op = match self.locals[mir::RETURN_PLACE] {\n+                    LocalRef::Operand(Some(op)) => op,\n+                    LocalRef::Operand(None) => bug!(\"use of return before def\"),\n+                    LocalRef::Place(cg_place) => {\n+                        OperandRef {\n+                            val: Ref(cg_place.llval, None, cg_place.align),\n+                            layout: cg_place.layout\n+                        }\n                     }\n-                }\n-                let llval = match self.fn_ty.ret.mode {\n-                    PassMode::Ignore(IgnoreMode::Zst) | PassMode::Indirect(..) => {\n-                        bx.ret_void();\n-                        return;\n+                    LocalRef::UnsizedPlace(_) => bug!(\"return type must be sized\"),\n+                };\n+                let llslot = match op.val {\n+                    Immediate(_) | Pair(..) => {\n+                        let scratch =\n+                            PlaceRef::alloca(&mut bx, self.fn_ty.ret.layout, \"ret\");\n+                        op.val.store(&mut bx, scratch);\n+                        scratch.llval\n                     }\n-\n-                    PassMode::Ignore(IgnoreMode::CVarArgs) => {\n-                        bug!(\"C variadic arguments should never be the return type\");\n+                    Ref(llval, _, align) => {\n+                        assert_eq!(align, op.layout.align.abi,\n+                                   \"return place is unaligned!\");\n+                        llval\n                     }\n+                };\n+                let addr = bx.pointercast(llslot, bx.type_ptr_to(\n+                    bx.cast_backend_type(&cast_ty)\n+                ));\n+                bx.load(addr, self.fn_ty.ret.layout.align.abi)\n+            }\n+        };\n+        bx.ret(llval);\n+    }\n \n-                    PassMode::Direct(_) | PassMode::Pair(..) => {\n-                        let op =\n-                            self.codegen_consume(&mut bx, &mir::Place::Local(mir::RETURN_PLACE));\n-                        if let Ref(llval, _, align) = op.val {\n-                            bx.load(llval, align)\n-                        } else {\n-                            op.immediate_or_packed_pair(&mut bx)\n-                        }\n-                    }\n \n-                    PassMode::Cast(cast_ty) => {\n-                        let op = match self.locals[mir::RETURN_PLACE] {\n-                            LocalRef::Operand(Some(op)) => op,\n-                            LocalRef::Operand(None) => bug!(\"use of return before def\"),\n-                            LocalRef::Place(cg_place) => {\n-                                OperandRef {\n-                                    val: Ref(cg_place.llval, None, cg_place.align),\n-                                    layout: cg_place.layout\n-                                }\n-                            }\n-                            LocalRef::UnsizedPlace(_) => bug!(\"return type must be sized\"),\n-                        };\n-                        let llslot = match op.val {\n-                            Immediate(_) | Pair(..) => {\n-                                let scratch =\n-                                    PlaceRef::alloca(&mut bx, self.fn_ty.ret.layout, \"ret\");\n-                                op.val.store(&mut bx, scratch);\n-                                scratch.llval\n-                            }\n-                            Ref(llval, _, align) => {\n-                                assert_eq!(align, op.layout.align.abi,\n-                                           \"return place is unaligned!\");\n-                                llval\n-                            }\n-                        };\n-                        let addr = bx.pointercast(llslot, bx.type_ptr_to(\n-                            bx.cast_backend_type(&cast_ty)\n-                        ));\n-                        bx.load(addr, self.fn_ty.ret.layout.align.abi)\n-                    }\n-                };\n-                bx.ret(llval);\n+    fn codegen_drop_terminator<'b>(\n+        &mut self,\n+        helper: TerminatorCodegenHelper<'b, 'tcx>,\n+        mut bx: Bx,\n+        location: &mir::Place<'tcx>,\n+        target: mir::BasicBlock,\n+        unwind: Option<mir::BasicBlock>,\n+    ) {\n+        let ty = location.ty(self.mir, bx.tcx()).to_ty(bx.tcx());\n+        let ty = self.monomorphize(&ty);\n+        let drop_fn = monomorphize::resolve_drop_in_place(bx.tcx(), ty);\n+\n+        if let ty::InstanceDef::DropGlue(_, None) = drop_fn.def {\n+            // we don't actually need to drop anything.\n+            helper.funclet_br(self, &mut bx, target);\n+            return\n+        }\n+\n+        let place = self.codegen_place(&mut bx, location);\n+        let (args1, args2);\n+        let mut args = if let Some(llextra) = place.llextra {\n+            args2 = [place.llval, llextra];\n+            &args2[..]\n+        } else {\n+            args1 = [place.llval];\n+            &args1[..]\n+        };\n+        let (drop_fn, fn_ty) = match ty.sty {\n+            ty::Dynamic(..) => {\n+                let sig = drop_fn.fn_sig(self.cx.tcx());\n+                let sig = self.cx.tcx().normalize_erasing_late_bound_regions(\n+                    ty::ParamEnv::reveal_all(),\n+                    &sig,\n+                );\n+                let fn_ty = bx.new_vtable(sig, &[]);\n+                let vtable = args[1];\n+                args = &args[..1];\n+                (meth::DESTRUCTOR.get_fn(&mut bx, vtable, &fn_ty), fn_ty)\n             }\n+            _ => {\n+                (bx.get_fn(drop_fn),\n+                 bx.fn_type_of_instance(&drop_fn))\n+            }\n+        };\n+        helper.do_call(self, &mut bx, fn_ty, drop_fn, args,\n+                       Some((ReturnDest::Nothing, target)),\n+                       unwind);\n+    }\n \n-            mir::TerminatorKind::Unreachable => {\n-                bx.unreachable();\n+    fn codegen_assert_terminator<'b>(\n+        &mut self,\n+        helper: TerminatorCodegenHelper<'b, 'tcx>,\n+        mut bx: Bx,\n+        terminator: &mir::Terminator<'tcx>,\n+        cond: &mir::Operand<'tcx>,\n+        expected: bool,\n+        msg: &mir::AssertMessage<'tcx>,\n+        target: mir::BasicBlock,\n+        cleanup: Option<mir::BasicBlock>,\n+    ) {\n+        let span = terminator.source_info.span;\n+        let cond = self.codegen_operand(&mut bx, cond).immediate();\n+        let mut const_cond = bx.const_to_opt_u128(cond, false).map(|c| c == 1);\n+\n+        // This case can currently arise only from functions marked\n+        // with #[rustc_inherit_overflow_checks] and inlined from\n+        // another crate (mostly core::num generic/#[inline] fns),\n+        // while the current crate doesn't use overflow checks.\n+        // NOTE: Unlike binops, negation doesn't have its own\n+        // checked operation, just a comparison with the minimum\n+        // value, so we have to check for the assert message.\n+        if !bx.check_overflow() {\n+            if let mir::interpret::EvalErrorKind::OverflowNeg = *msg {\n+                const_cond = Some(expected);\n             }\n+        }\n \n-            mir::TerminatorKind::Drop { ref location, target, unwind } => {\n-                let ty = location.ty(self.mir, bx.tcx()).to_ty(bx.tcx());\n-                let ty = self.monomorphize(&ty);\n-                let drop_fn = monomorphize::resolve_drop_in_place(bx.tcx(), ty);\n-\n-                if let ty::InstanceDef::DropGlue(_, None) = drop_fn.def {\n-                    // we don't actually need to drop anything.\n-                    funclet_br(self, &mut bx, target);\n-                    return\n-                }\n+        // Don't codegen the panic block if success if known.\n+        if const_cond == Some(expected) {\n+            helper.funclet_br(self, &mut bx, target);\n+            return;\n+        }\n \n-                let place = self.codegen_place(&mut bx, location);\n-                let (args1, args2);\n-                let mut args = if let Some(llextra) = place.llextra {\n-                    args2 = [place.llval, llextra];\n-                    &args2[..]\n-                } else {\n-                    args1 = [place.llval];\n-                    &args1[..]\n-                };\n-                let (drop_fn, fn_ty) = match ty.sty {\n-                    ty::Dynamic(..) => {\n-                        let sig = drop_fn.fn_sig(tcx);\n-                        let sig = tcx.normalize_erasing_late_bound_regions(\n-                            ty::ParamEnv::reveal_all(),\n-                            &sig,\n-                        );\n-                        let fn_ty = bx.new_vtable(sig, &[]);\n-                        let vtable = args[1];\n-                        args = &args[..1];\n-                        (meth::DESTRUCTOR.get_fn(&mut bx, vtable, &fn_ty), fn_ty)\n-                    }\n-                    _ => {\n-                        (bx.get_fn(drop_fn),\n-                         bx.fn_type_of_instance(&drop_fn))\n-                    }\n-                };\n-                do_call(self, &mut bx, fn_ty, drop_fn, args,\n-                        Some((ReturnDest::Nothing, target)),\n-                        unwind);\n+        // Pass the condition through llvm.expect for branch hinting.\n+        let cond = bx.expect(cond, expected);\n+\n+        // Create the failure block and the conditional branch to it.\n+        let lltarget = helper.llblock(self, target);\n+        let panic_block = self.new_block(\"panic\");\n+        if expected {\n+            bx.cond_br(cond, lltarget, panic_block.llbb());\n+        } else {\n+            bx.cond_br(cond, panic_block.llbb(), lltarget);\n+        }\n+\n+        // After this point, bx is the block for the call to panic.\n+        bx = panic_block;\n+        self.set_debug_loc(&mut bx, terminator.source_info);\n+\n+        // Get the location information.\n+        let loc = bx.sess().source_map().lookup_char_pos(span.lo());\n+        let filename = Symbol::intern(&loc.file.name.to_string()).as_str();\n+        let filename = bx.const_str_slice(filename);\n+        let line = bx.const_u32(loc.line as u32);\n+        let col = bx.const_u32(loc.col.to_usize() as u32 + 1);\n+        let align = self.cx.tcx().data_layout.aggregate_align.abi\n+            .max(self.cx.tcx().data_layout.i32_align.abi)\n+            .max(self.cx.tcx().data_layout.pointer_align.abi);\n+\n+        // Put together the arguments to the panic entry point.\n+        let (lang_item, args) = match *msg {\n+            EvalErrorKind::BoundsCheck { ref len, ref index } => {\n+                let len = self.codegen_operand(&mut bx, len).immediate();\n+                let index = self.codegen_operand(&mut bx, index).immediate();\n+\n+                let file_line_col = bx.const_struct(&[filename, line, col], false);\n+                let file_line_col = bx.static_addr_of(\n+                    file_line_col,\n+                    align,\n+                    Some(\"panic_bounds_check_loc\")\n+                );\n+                (lang_items::PanicBoundsCheckFnLangItem,\n+                 vec![file_line_col, index, len])\n             }\n+            _ => {\n+                let str = msg.description();\n+                let msg_str = Symbol::intern(str).as_str();\n+                let msg_str = bx.const_str_slice(msg_str);\n+                let msg_file_line_col = bx.const_struct(\n+                    &[msg_str, filename, line, col],\n+                    false\n+                );\n+                let msg_file_line_col = bx.static_addr_of(\n+                    msg_file_line_col,\n+                    align,\n+                    Some(\"panic_loc\")\n+                );\n+                (lang_items::PanicFnLangItem,\n+                 vec![msg_file_line_col])\n+            }\n+        };\n \n-            mir::TerminatorKind::Assert { ref cond, expected, ref msg, target, cleanup } => {\n-                let cond = self.codegen_operand(&mut bx, cond).immediate();\n-                let mut const_cond = bx.const_to_opt_u128(cond, false).map(|c| c == 1);\n-\n-                // This case can currently arise only from functions marked\n-                // with #[rustc_inherit_overflow_checks] and inlined from\n-                // another crate (mostly core::num generic/#[inline] fns),\n-                // while the current crate doesn't use overflow checks.\n-                // NOTE: Unlike binops, negation doesn't have its own\n-                // checked operation, just a comparison with the minimum\n-                // value, so we have to check for the assert message.\n-                if !bx.check_overflow() {\n-                    if let mir::interpret::EvalErrorKind::OverflowNeg = *msg {\n-                        const_cond = Some(expected);\n-                    }\n-                }\n+        // Obtain the panic entry point.\n+        let def_id = common::langcall(bx.tcx(), Some(span), \"\", lang_item);\n+        let instance = ty::Instance::mono(bx.tcx(), def_id);\n+        let fn_ty = bx.fn_type_of_instance(&instance);\n+        let llfn = bx.get_fn(instance);\n \n-                // Don't codegen the panic block if success if known.\n-                if const_cond == Some(expected) {\n-                    funclet_br(self, &mut bx, target);\n-                    return;\n-                }\n+        // Codegen the actual panic invoke/call.\n+        helper.do_call(self, &mut bx, fn_ty, llfn, &args, None, cleanup);\n+    }\n \n-                // Pass the condition through llvm.expect for branch hinting.\n-                let cond = bx.expect(cond, expected);\n+    fn codegen_call_terminator<'b>(\n+        &mut self,\n+        helper: TerminatorCodegenHelper<'b, 'tcx>,\n+        mut bx: Bx,\n+        terminator: &mir::Terminator<'tcx>,\n+        func: &mir::Operand<'tcx>,\n+        args: &Vec<mir::Operand<'tcx>>,\n+        destination: &Option<(mir::Place<'tcx>, mir::BasicBlock)>,\n+        cleanup: Option<mir::BasicBlock>,\n+    ) {\n+        let span = terminator.source_info.span;\n+        // Create the callee. This is a fn ptr or zero-sized and hence a kind of scalar.\n+        let callee = self.codegen_operand(&mut bx, func);\n+\n+        let (instance, mut llfn) = match callee.layout.ty.sty {\n+            ty::FnDef(def_id, substs) => {\n+                (Some(ty::Instance::resolve(bx.tcx(),\n+                                            ty::ParamEnv::reveal_all(),\n+                                            def_id,\n+                                            substs).unwrap()),\n+                 None)\n+            }\n+            ty::FnPtr(_) => {\n+                (None, Some(callee.immediate()))\n+            }\n+            _ => bug!(\"{} is not callable\", callee.layout.ty),\n+        };\n+        let def = instance.map(|i| i.def);\n+        let sig = callee.layout.ty.fn_sig(bx.tcx());\n+        let sig = bx.tcx().normalize_erasing_late_bound_regions(\n+            ty::ParamEnv::reveal_all(),\n+            &sig,\n+        );\n+        let abi = sig.abi;\n+\n+        // Handle intrinsics old codegen wants Expr's for, ourselves.\n+        let intrinsic = match def {\n+            Some(ty::InstanceDef::Intrinsic(def_id)) =>\n+                Some(bx.tcx().item_name(def_id).as_str()),\n+            _ => None\n+        };\n+        let intrinsic = intrinsic.as_ref().map(|s| &s[..]);\n \n-                // Create the failure block and the conditional branch to it.\n-                let lltarget = llblock(self, target);\n-                let panic_block = self.new_block(\"panic\");\n-                if expected {\n-                    bx.cond_br(cond, lltarget, panic_block.llbb());\n-                } else {\n-                    bx.cond_br(cond, panic_block.llbb(), lltarget);\n-                }\n+        if intrinsic == Some(\"transmute\") {\n+            if let Some(destination_ref) = destination.as_ref() {\n+                let &(ref dest, target) = destination_ref;\n+                self.codegen_transmute(&mut bx, &args[0], dest);\n+                helper.funclet_br(self, &mut bx, target);\n+            } else {\n+                // If we are trying to transmute to an uninhabited type,\n+                // it is likely there is no allotted destination. In fact,\n+                // transmuting to an uninhabited type is UB, which means\n+                // we can do what we like. Here, we declare that transmuting\n+                // into an uninhabited type is impossible, so anything following\n+                // it must be unreachable.\n+                assert_eq!(bx.layout_of(sig.output()).abi, layout::Abi::Uninhabited);\n+                bx.unreachable();\n+            }\n+            return;\n+        }\n \n-                // After this point, bx is the block for the call to panic.\n-                bx = panic_block;\n-                self.set_debug_loc(&mut bx, terminator.source_info);\n+        // The \"spoofed\" `VaList` added to a C-variadic functions signature\n+        // should not be included in the `extra_args` calculation.\n+        let extra_args_start_idx = sig.inputs().len() - if sig.variadic { 1 } else { 0 };\n+        let extra_args = &args[extra_args_start_idx..];\n+        let extra_args = extra_args.iter().map(|op_arg| {\n+            let op_ty = op_arg.ty(self.mir, bx.tcx());\n+            self.monomorphize(&op_ty)\n+        }).collect::<Vec<_>>();\n+\n+        let fn_ty = match def {\n+            Some(ty::InstanceDef::Virtual(..)) => {\n+                bx.new_vtable(sig, &extra_args)\n+            }\n+            Some(ty::InstanceDef::DropGlue(_, None)) => {\n+                // Empty drop glue; a no-op.\n+                let &(_, target) = destination.as_ref().unwrap();\n+                helper.funclet_br(self, &mut bx, target);\n+                return;\n+            }\n+            _ => bx.new_fn_type(sig, &extra_args)\n+        };\n \n-                // Get the location information.\n+        // Emit a panic or a no-op for `panic_if_uninhabited`.\n+        if intrinsic == Some(\"panic_if_uninhabited\") {\n+            let ty = instance.unwrap().substs.type_at(0);\n+            let layout = bx.layout_of(ty);\n+            if layout.abi.is_uninhabited() {\n                 let loc = bx.sess().source_map().lookup_char_pos(span.lo());\n                 let filename = Symbol::intern(&loc.file.name.to_string()).as_str();\n                 let filename = bx.const_str_slice(filename);\n                 let line = bx.const_u32(loc.line as u32);\n                 let col = bx.const_u32(loc.col.to_usize() as u32 + 1);\n-                let align = tcx.data_layout.aggregate_align.abi\n-                    .max(tcx.data_layout.i32_align.abi)\n-                    .max(tcx.data_layout.pointer_align.abi);\n-\n-                // Put together the arguments to the panic entry point.\n-                let (lang_item, args) = match *msg {\n-                    EvalErrorKind::BoundsCheck { ref len, ref index } => {\n-                        let len = self.codegen_operand(&mut bx, len).immediate();\n-                        let index = self.codegen_operand(&mut bx, index).immediate();\n-\n-                        let file_line_col = bx.const_struct(&[filename, line, col], false);\n-                        let file_line_col = bx.static_addr_of(\n-                            file_line_col,\n-                            align,\n-                            Some(\"panic_bounds_check_loc\")\n-                        );\n-                        (lang_items::PanicBoundsCheckFnLangItem,\n-                         vec![file_line_col, index, len])\n-                    }\n-                    _ => {\n-                        let str = msg.description();\n-                        let msg_str = Symbol::intern(str).as_str();\n-                        let msg_str = bx.const_str_slice(msg_str);\n-                        let msg_file_line_col = bx.const_struct(\n-                            &[msg_str, filename, line, col],\n-                            false\n-                        );\n-                        let msg_file_line_col = bx.static_addr_of(\n-                            msg_file_line_col,\n-                            align,\n-                            Some(\"panic_loc\")\n-                        );\n-                        (lang_items::PanicFnLangItem,\n-                         vec![msg_file_line_col])\n-                    }\n-                };\n+                let align = self.cx.tcx().data_layout.aggregate_align.abi\n+                    .max(self.cx.tcx().data_layout.i32_align.abi)\n+                    .max(self.cx.tcx().data_layout.pointer_align.abi);\n+\n+                let str = format!(\n+                    \"Attempted to instantiate uninhabited type {}\",\n+                    ty\n+                );\n+                let msg_str = Symbol::intern(&str).as_str();\n+                let msg_str = bx.const_str_slice(msg_str);\n+                let msg_file_line_col = bx.const_struct(\n+                    &[msg_str, filename, line, col],\n+                    false,\n+                );\n+                let msg_file_line_col = bx.static_addr_of(\n+                    msg_file_line_col,\n+                    align,\n+                    Some(\"panic_loc\"),\n+                );\n \n                 // Obtain the panic entry point.\n-                let def_id = common::langcall(bx.tcx(), Some(span), \"\", lang_item);\n+                let def_id =\n+                    common::langcall(bx.tcx(), Some(span), \"\", lang_items::PanicFnLangItem);\n                 let instance = ty::Instance::mono(bx.tcx(), def_id);\n                 let fn_ty = bx.fn_type_of_instance(&instance);\n                 let llfn = bx.get_fn(instance);\n \n                 // Codegen the actual panic invoke/call.\n-                do_call(self, &mut bx, fn_ty, llfn, &args, None, cleanup);\n-            }\n-\n-            mir::TerminatorKind::DropAndReplace { .. } => {\n-                bug!(\"undesugared DropAndReplace in codegen: {:?}\", terminator);\n+                helper.do_call(\n+                    self,\n+                    &mut bx,\n+                    fn_ty,\n+                    llfn,\n+                    &[msg_file_line_col],\n+                    destination.as_ref().map(|(_, bb)| (ReturnDest::Nothing, *bb)),\n+                    cleanup,\n+                );\n+            } else {\n+                // a NOP\n+                helper.funclet_br(self, &mut bx, destination.as_ref().unwrap().1)\n             }\n+            return;\n+        }\n \n-            mir::TerminatorKind::Call {\n-                ref func,\n-                ref args,\n-                ref destination,\n-                cleanup,\n-                from_hir_call: _\n-            } => {\n-                // Create the callee. This is a fn ptr or zero-sized and hence a kind of scalar.\n-                let callee = self.codegen_operand(&mut bx, func);\n-\n-                let (instance, mut llfn) = match callee.layout.ty.sty {\n-                    ty::FnDef(def_id, substs) => {\n-                        (Some(ty::Instance::resolve(bx.tcx(),\n-                                                    ty::ParamEnv::reveal_all(),\n-                                                    def_id,\n-                                                    substs).unwrap()),\n-                         None)\n-                    }\n-                    ty::FnPtr(_) => {\n-                        (None, Some(callee.immediate()))\n-                    }\n-                    _ => bug!(\"{} is not callable\", callee.layout.ty)\n-                };\n-                let def = instance.map(|i| i.def);\n-                let sig = callee.layout.ty.fn_sig(bx.tcx());\n-                let sig = bx.tcx().normalize_erasing_late_bound_regions(\n-                    ty::ParamEnv::reveal_all(),\n-                    &sig,\n-                );\n-                let abi = sig.abi;\n+        // The arguments we'll be passing. Plus one to account for outptr, if used.\n+        let arg_count = fn_ty.args.len() + fn_ty.ret.is_indirect() as usize;\n+        let mut llargs = Vec::with_capacity(arg_count);\n \n-                // Handle intrinsics old codegen wants Expr's for, ourselves.\n-                let intrinsic = match def {\n-                    Some(ty::InstanceDef::Intrinsic(def_id))\n-                        => Some(bx.tcx().item_name(def_id).as_str()),\n-                    _ => None\n-                };\n-                let intrinsic = intrinsic.as_ref().map(|s| &s[..]);\n+        // Prepare the return value destination\n+        let ret_dest = if let Some((ref dest, _)) = *destination {\n+            let is_intrinsic = intrinsic.is_some();\n+            self.make_return_dest(&mut bx, dest, &fn_ty.ret, &mut llargs,\n+                                  is_intrinsic)\n+        } else {\n+            ReturnDest::Nothing\n+        };\n \n-                if intrinsic == Some(\"transmute\") {\n-                    if let Some(destination_ref) = destination.as_ref() {\n-                        let &(ref dest, target) = destination_ref;\n-                        self.codegen_transmute(&mut bx, &args[0], dest);\n-                        funclet_br(self, &mut bx, target);\n-                    } else {\n-                        // If we are trying to transmute to an uninhabited type,\n-                        // it is likely there is no allotted destination. In fact,\n-                        // transmuting to an uninhabited type is UB, which means\n-                        // we can do what we like. Here, we declare that transmuting\n-                        // into an uninhabited type is impossible, so anything following\n-                        // it must be unreachable.\n-                        assert_eq!(bx.layout_of(sig.output()).abi, layout::Abi::Uninhabited);\n-                        bx.unreachable();\n-                    }\n-                    return;\n-                }\n+        if intrinsic.is_some() && intrinsic != Some(\"drop_in_place\") {\n+            let dest = match ret_dest {\n+                _ if fn_ty.ret.is_indirect() => llargs[0],\n+                ReturnDest::Nothing =>\n+                    bx.const_undef(bx.type_ptr_to(bx.memory_ty(&fn_ty.ret))),\n+                ReturnDest::IndirectOperand(dst, _) | ReturnDest::Store(dst) =>\n+                    dst.llval,\n+                ReturnDest::DirectOperand(_) =>\n+                    bug!(\"Cannot use direct operand with an intrinsic call\"),\n+            };\n \n-                // The \"spoofed\" `VaList` added to a C-variadic functions signature\n-                // should not be included in the `extra_args` calculation.\n-                let extra_args_start_idx = sig.inputs().len() - if sig.variadic { 1 } else { 0 };\n-                let extra_args = &args[extra_args_start_idx..];\n-                let extra_args = extra_args.iter().map(|op_arg| {\n-                    let op_ty = op_arg.ty(self.mir, bx.tcx());\n-                    self.monomorphize(&op_ty)\n-                }).collect::<Vec<_>>();\n-\n-                let fn_ty = match def {\n-                    Some(ty::InstanceDef::Virtual(..)) => {\n-                        bx.new_vtable(sig, &extra_args)\n-                    }\n-                    Some(ty::InstanceDef::DropGlue(_, None)) => {\n-                        // empty drop glue - a nop.\n-                        let &(_, target) = destination.as_ref().unwrap();\n-                        funclet_br(self, &mut bx, target);\n-                        return;\n-                    }\n-                    _ => bx.new_fn_type(sig, &extra_args)\n-                };\n+            let args: Vec<_> = args.iter().enumerate().map(|(i, arg)| {\n+                // The indices passed to simd_shuffle* in the\n+                // third argument must be constant. This is\n+                // checked by const-qualification, which also\n+                // promotes any complex rvalues to constants.\n+                if i == 2 && intrinsic.unwrap().starts_with(\"simd_shuffle\") {\n+                    match *arg {\n+                        // The shuffle array argument is usually not an explicit constant,\n+                        // but specified directly in the code. This means it gets promoted\n+                        // and we can then extract the value by evaluating the promoted.\n+                        mir::Operand::Copy(mir::Place::Promoted(box(index, ty))) |\n+                        mir::Operand::Move(mir::Place::Promoted(box(index, ty))) => {\n+                            let param_env = ty::ParamEnv::reveal_all();\n+                            let cid = mir::interpret::GlobalId {\n+                                instance: self.instance,\n+                                promoted: Some(index),\n+                            };\n+                            let c = bx.tcx().const_eval(param_env.and(cid));\n+                            let (llval, ty) = self.simd_shuffle_indices(\n+                                &bx,\n+                                terminator.source_info.span,\n+                                ty,\n+                                c,\n+                            );\n+                            return OperandRef {\n+                                val: Immediate(llval),\n+                                layout: bx.layout_of(ty),\n+                            };\n \n-                // emit a panic or a NOP for `panic_if_uninhabited`\n-                if intrinsic == Some(\"panic_if_uninhabited\") {\n-                    let ty = instance.unwrap().substs.type_at(0);\n-                    let layout = bx.layout_of(ty);\n-                    if layout.abi.is_uninhabited() {\n-                        let loc = bx.sess().source_map().lookup_char_pos(span.lo());\n-                        let filename = Symbol::intern(&loc.file.name.to_string()).as_str();\n-                        let filename = bx.const_str_slice(filename);\n-                        let line = bx.const_u32(loc.line as u32);\n-                        let col = bx.const_u32(loc.col.to_usize() as u32 + 1);\n-                        let align = tcx.data_layout.aggregate_align.abi\n-                            .max(tcx.data_layout.i32_align.abi)\n-                            .max(tcx.data_layout.pointer_align.abi);\n-\n-                        let str = format!(\n-                            \"Attempted to instantiate uninhabited type {}\",\n-                            ty\n-                        );\n-                        let msg_str = Symbol::intern(&str).as_str();\n-                        let msg_str = bx.const_str_slice(msg_str);\n-                        let msg_file_line_col = bx.const_struct(\n-                            &[msg_str, filename, line, col],\n-                            false,\n-                        );\n-                        let msg_file_line_col = bx.static_addr_of(\n-                            msg_file_line_col,\n-                            align,\n-                            Some(\"panic_loc\"),\n-                        );\n-\n-                        // Obtain the panic entry point.\n-                        let def_id =\n-                            common::langcall(bx.tcx(), Some(span), \"\", lang_items::PanicFnLangItem);\n-                        let instance = ty::Instance::mono(bx.tcx(), def_id);\n-                        let fn_ty = bx.fn_type_of_instance(&instance);\n-                        let llfn = bx.get_fn(instance);\n-\n-                        // Codegen the actual panic invoke/call.\n-                        do_call(\n-                            self,\n-                            &mut bx,\n-                            fn_ty,\n-                            llfn,\n-                            &[msg_file_line_col],\n-                            destination.as_ref().map(|(_, bb)| (ReturnDest::Nothing, *bb)),\n-                            cleanup,\n-                        );\n-                    } else {\n-                        // a NOP\n-                        funclet_br(self, &mut bx, destination.as_ref().unwrap().1);\n+                        }\n+                        mir::Operand::Copy(_) |\n+                        mir::Operand::Move(_) => {\n+                            span_bug!(span, \"shuffle indices must be constant\");\n+                        }\n+                        mir::Operand::Constant(ref constant) => {\n+                            let c = self.eval_mir_constant(&bx, constant);\n+                            let (llval, ty) = self.simd_shuffle_indices(\n+                                &bx,\n+                                constant.span,\n+                                constant.ty,\n+                                c,\n+                            );\n+                            return OperandRef {\n+                                val: Immediate(llval),\n+                                layout: bx.layout_of(ty)\n+                            };\n+                        }\n                     }\n-                    return;\n                 }\n \n-                // The arguments we'll be passing. Plus one to account for outptr, if used.\n-                let arg_count = fn_ty.args.len() + fn_ty.ret.is_indirect() as usize;\n-                let mut llargs = Vec::with_capacity(arg_count);\n-\n-                // Prepare the return value destination\n-                let ret_dest = if let Some((ref dest, _)) = *destination {\n-                    let is_intrinsic = intrinsic.is_some();\n-                    self.make_return_dest(&mut bx, dest, &fn_ty.ret, &mut llargs,\n-                                          is_intrinsic)\n-                } else {\n-                    ReturnDest::Nothing\n-                };\n-\n-                if intrinsic.is_some() && intrinsic != Some(\"drop_in_place\") {\n-                    let dest = match ret_dest {\n-                        _ if fn_ty.ret.is_indirect() => llargs[0],\n-                        ReturnDest::Nothing => {\n-                            bx.const_undef(bx.type_ptr_to(bx.memory_ty(&fn_ty.ret)))\n-                        }\n-                        ReturnDest::IndirectOperand(dst, _) |\n-                        ReturnDest::Store(dst) => dst.llval,\n-                        ReturnDest::DirectOperand(_) =>\n-                            bug!(\"Cannot use direct operand with an intrinsic call\")\n-                    };\n-\n-                    let args: Vec<_> = args.iter().enumerate().map(|(i, arg)| {\n-                        // The indices passed to simd_shuffle* in the\n-                        // third argument must be constant. This is\n-                        // checked by const-qualification, which also\n-                        // promotes any complex rvalues to constants.\n-                        if i == 2 && intrinsic.unwrap().starts_with(\"simd_shuffle\") {\n-                            match *arg {\n-                                // The shuffle array argument is usually not an explicit constant,\n-                                // but specified directly in the code. This means it gets promoted\n-                                // and we can then extract the value by evaluating the promoted.\n-                                mir::Operand::Copy(mir::Place::Promoted(box(index, ty))) |\n-                                mir::Operand::Move(mir::Place::Promoted(box(index, ty))) => {\n-                                    let param_env = ty::ParamEnv::reveal_all();\n-                                    let cid = mir::interpret::GlobalId {\n-                                        instance: self.instance,\n-                                        promoted: Some(index),\n-                                    };\n-                                    let c = bx.tcx().const_eval(param_env.and(cid));\n-                                    let (llval, ty) = self.simd_shuffle_indices(\n-                                        &bx,\n-                                        terminator.source_info.span,\n-                                        ty,\n-                                        c,\n-                                    );\n-                                    return OperandRef {\n-                                        val: Immediate(llval),\n-                                        layout: bx.layout_of(ty),\n-                                    };\n-\n-                                },\n-                                mir::Operand::Copy(_) |\n-                                mir::Operand::Move(_) => {\n-                                    span_bug!(span, \"shuffle indices must be constant\");\n-                                }\n-                                mir::Operand::Constant(ref constant) => {\n-                                    let c = self.eval_mir_constant(&bx, constant);\n-                                    let (llval, ty) = self.simd_shuffle_indices(\n-                                        &bx,\n-                                        constant.span,\n-                                        constant.ty,\n-                                        c,\n-                                    );\n-                                    return OperandRef {\n-                                        val: Immediate(llval),\n-                                        layout: bx.layout_of(ty)\n-                                    };\n-                                }\n-                            }\n-                        }\n+                self.codegen_operand(&mut bx, arg)\n+            }).collect();\n \n-                        self.codegen_operand(&mut bx, arg)\n-                    }).collect();\n \n+            let callee_ty = instance.as_ref().unwrap().ty(bx.tcx());\n+            bx.codegen_intrinsic_call(callee_ty, &fn_ty, &args, dest,\n+                                      terminator.source_info.span);\n \n-                    let callee_ty = instance.as_ref().unwrap().ty(bx.tcx());\n-                    bx.codegen_intrinsic_call(callee_ty, &fn_ty, &args, dest,\n-                                               terminator.source_info.span);\n+            if let ReturnDest::IndirectOperand(dst, _) = ret_dest {\n+                self.store_return(&mut bx, ret_dest, &fn_ty.ret, dst.llval);\n+            }\n \n-                    if let ReturnDest::IndirectOperand(dst, _) = ret_dest {\n-                        self.store_return(&mut bx, ret_dest, &fn_ty.ret, dst.llval);\n-                    }\n+            if let Some((_, target)) = *destination {\n+                helper.funclet_br(self, &mut bx, target);\n+            } else {\n+                bx.unreachable();\n+            }\n \n-                    if let Some((_, target)) = *destination {\n-                        funclet_br(self, &mut bx, target);\n-                    } else {\n-                        bx.unreachable();\n-                    }\n+            return;\n+        }\n \n-                    return;\n-                }\n+        // Split the rust-call tupled arguments off.\n+        let (first_args, untuple) = if abi == Abi::RustCall && !args.is_empty() {\n+            let (tup, args) = args.split_last().unwrap();\n+            (args, Some(tup))\n+        } else {\n+            (&args[..], None)\n+        };\n \n-                // Split the rust-call tupled arguments off.\n-                let (first_args, untuple) = if abi == Abi::RustCall && !args.is_empty() {\n-                    let (tup, args) = args.split_last().unwrap();\n-                    (args, Some(tup))\n-                } else {\n-                    (&args[..], None)\n+        // Useful determining if the current argument is the \"spoofed\" `VaList`\n+        let last_arg_idx = if sig.inputs().is_empty() {\n+            None\n+        } else {\n+            Some(sig.inputs().len() - 1)\n+        };\n+        'make_args: for (i, arg) in first_args.iter().enumerate() {\n+            // If this is a C-variadic function the function signature contains\n+            // an \"spoofed\" `VaList`. This argument is ignored, but we need to\n+            // populate it with a dummy operand so that the users real arguments\n+            // are not overwritten.\n+            let i = if sig.variadic && last_arg_idx.map(|x| x == i).unwrap_or(false) {\n+                let layout = match self.cx.tcx().lang_items().va_list() {\n+                    Some(did) => bx.cx().layout_of(bx.tcx().type_of(did)),\n+                    None => bug!(\"`va_list` language item required for C-variadics\"),\n                 };\n-\n-                // Useful determining if the current argument is the \"spoofed\" `VaList`\n-                let last_arg_idx = if sig.inputs().is_empty() {\n-                    None\n-                } else {\n-                    Some(sig.inputs().len() - 1)\n+                let op = OperandRef {\n+                    val: OperandValue::Immediate(\n+                        bx.cx().const_undef(bx.cx().immediate_backend_type(layout)\n+                    )),\n+                    layout: layout,\n                 };\n-                'make_args: for (i, arg) in first_args.iter().enumerate() {\n-                    // If this is a C-variadic function the function signature contains\n-                    // an \"spoofed\" `VaList`. This argument is ignored, but we need to\n-                    // populate it with a dummy operand so that the users real arguments\n-                    // are not overwritten.\n-                    let i = if sig.variadic && last_arg_idx.map(|x| x == i).unwrap_or(false) {\n-                        let layout = match tcx.lang_items().va_list() {\n-                            Some(did) => bx.cx().layout_of(bx.tcx().type_of(did)),\n-                            None => bug!(\"va_list language item required for C variadics\"),\n-                        };\n-                        let op = OperandRef {\n-                            val: OperandValue::Immediate(\n-                                bx.cx().const_undef(bx.cx().immediate_backend_type(layout))\n-                            ),\n-                            layout: layout,\n-                        };\n-                        self.codegen_argument(&mut bx, op, &mut llargs, &fn_ty.args[i]);\n-                        if i + 1 < fn_ty.args.len() {\n-                            i + 1\n-                        } else {\n-                            break 'make_args\n-                        }\n-                    } else {\n-                        i\n-                    };\n-                    let mut op = self.codegen_operand(&mut bx, arg);\n-\n-                    if let (0, Some(ty::InstanceDef::Virtual(_, idx))) = (i, def) {\n-                        if let Pair(..) = op.val {\n-                            // In the case of Rc<Self>, we need to explicitly pass a\n-                            // *mut RcBox<Self> with a Scalar (not ScalarPair) ABI. This is a hack\n-                            // that is understood elsewhere in the compiler as a method on\n-                            // `dyn Trait`.\n-                            // To get a `*mut RcBox<Self>`, we just keep unwrapping newtypes until\n-                            // we get a value of a built-in pointer type\n-                            'descend_newtypes: while !op.layout.ty.is_unsafe_ptr()\n-                                            && !op.layout.ty.is_region_ptr()\n-                            {\n-                                'iter_fields: for i in 0..op.layout.fields.count() {\n-                                    let field = op.extract_field(&mut bx, i);\n-                                    if !field.layout.is_zst() {\n-                                        // we found the one non-zero-sized field that is allowed\n-                                        // now find *its* non-zero-sized field, or stop if it's a\n-                                        // pointer\n-                                        op = field;\n-                                        continue 'descend_newtypes\n-                                    }\n-                                }\n-\n-                                span_bug!(span, \"receiver has no non-zero-sized fields {:?}\", op);\n+                self.codegen_argument(&mut bx, op, &mut llargs, &fn_ty.args[i]);\n+                if i + 1 < fn_ty.args.len() {\n+                    i + 1\n+                } else {\n+                    break 'make_args\n+                }\n+            } else {\n+                i\n+            };\n+            let mut op = self.codegen_operand(&mut bx, arg);\n+\n+            if let (0, Some(ty::InstanceDef::Virtual(_, idx))) = (i, def) {\n+                if let Pair(..) = op.val {\n+                    // In the case of Rc<Self>, we need to explicitly pass a\n+                    // *mut RcBox<Self> with a Scalar (not ScalarPair) ABI. This is a hack\n+                    // that is understood elsewhere in the compiler as a method on\n+                    // `dyn Trait`.\n+                    // To get a `*mut RcBox<Self>`, we just keep unwrapping newtypes until\n+                    // we get a value of a built-in pointer type\n+                    'descend_newtypes: while !op.layout.ty.is_unsafe_ptr()\n+                                    && !op.layout.ty.is_region_ptr()\n+                    {\n+                        'iter_fields: for i in 0..op.layout.fields.count() {\n+                            let field = op.extract_field(&mut bx, i);\n+                            if !field.layout.is_zst() {\n+                                // we found the one non-zero-sized field that is allowed\n+                                // now find *its* non-zero-sized field, or stop if it's a\n+                                // pointer\n+                                op = field;\n+                                continue 'descend_newtypes\n                             }\n+                        }\n \n-                            // now that we have `*dyn Trait` or `&dyn Trait`, split it up into its\n-                            // data pointer and vtable. Look up the method in the vtable, and pass\n-                            // the data pointer as the first argument\n-                            match op.val {\n-                                Pair(data_ptr, meta) => {\n-                                    llfn = Some(meth::VirtualIndex::from_index(idx)\n-                                        .get_fn(&mut bx, meta, &fn_ty));\n-                                    llargs.push(data_ptr);\n-                                    continue 'make_args\n-                                }\n-                                other => bug!(\"expected a Pair, got {:?}\", other)\n-                            }\n-                        } else if let Ref(data_ptr, Some(meta), _) = op.val {\n-                            // by-value dynamic dispatch\n+                        span_bug!(span, \"receiver has no non-zero-sized fields {:?}\", op);\n+                    }\n+\n+                    // now that we have `*dyn Trait` or `&dyn Trait`, split it up into its\n+                    // data pointer and vtable. Look up the method in the vtable, and pass\n+                    // the data pointer as the first argument\n+                    match op.val {\n+                        Pair(data_ptr, meta) => {\n                             llfn = Some(meth::VirtualIndex::from_index(idx)\n                                 .get_fn(&mut bx, meta, &fn_ty));\n                             llargs.push(data_ptr);\n-                            continue;\n-                        } else {\n-                            span_bug!(span, \"can't codegen a virtual call on {:?}\", op);\n-                        }\n-                    }\n-\n-                    // The callee needs to own the argument memory if we pass it\n-                    // by-ref, so make a local copy of non-immediate constants.\n-                    match (arg, op.val) {\n-                        (&mir::Operand::Copy(_), Ref(_, None, _)) |\n-                        (&mir::Operand::Constant(_), Ref(_, None, _)) => {\n-                            let tmp = PlaceRef::alloca(&mut bx, op.layout, \"const\");\n-                            op.val.store(&mut bx, tmp);\n-                            op.val = Ref(tmp.llval, None, tmp.align);\n+                            continue 'make_args\n                         }\n-                        _ => {}\n+                        other => bug!(\"expected a Pair, got {:?}\", other),\n                     }\n-\n-                    self.codegen_argument(&mut bx, op, &mut llargs, &fn_ty.args[i]);\n+                } else if let Ref(data_ptr, Some(meta), _) = op.val {\n+                    // by-value dynamic dispatch\n+                    llfn = Some(meth::VirtualIndex::from_index(idx)\n+                        .get_fn(&mut bx, meta, &fn_ty));\n+                    llargs.push(data_ptr);\n+                    continue;\n+                } else {\n+                    span_bug!(span, \"can't codegen a virtual call on {:?}\", op);\n                 }\n-                if let Some(tup) = untuple {\n-                    self.codegen_arguments_untupled(&mut bx, tup, &mut llargs,\n-                        &fn_ty.args[first_args.len()..])\n+            }\n+\n+            // The callee needs to own the argument memory if we pass it\n+            // by-ref, so make a local copy of non-immediate constants.\n+            match (arg, op.val) {\n+                (&mir::Operand::Copy(_), Ref(_, None, _)) |\n+                (&mir::Operand::Constant(_), Ref(_, None, _)) => {\n+                    let tmp = PlaceRef::alloca(&mut bx, op.layout, \"const\");\n+                    op.val.store(&mut bx, tmp);\n+                    op.val = Ref(tmp.llval, None, tmp.align);\n                 }\n+                _ => {}\n+            }\n \n-                let fn_ptr = match (llfn, instance) {\n-                    (Some(llfn), _) => llfn,\n-                    (None, Some(instance)) => bx.get_fn(instance),\n-                    _ => span_bug!(span, \"no llfn for call\"),\n-                };\n+            self.codegen_argument(&mut bx, op, &mut llargs, &fn_ty.args[i]);\n+        }\n+        if let Some(tup) = untuple {\n+            self.codegen_arguments_untupled(&mut bx, tup, &mut llargs,\n+                &fn_ty.args[first_args.len()..])\n+        }\n+\n+        let fn_ptr = match (llfn, instance) {\n+            (Some(llfn), _) => llfn,\n+            (None, Some(instance)) => bx.get_fn(instance),\n+            _ => span_bug!(span, \"no llfn for call\"),\n+        };\n+\n+        helper.do_call(self, &mut bx, fn_ty, fn_ptr, &llargs,\n+                       destination.as_ref().map(|&(_, target)| (ret_dest, target)),\n+                       cleanup);\n+    }\n+}\n+\n+impl<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>> FunctionCx<'a, 'tcx, Bx> {\n+    pub fn codegen_block(\n+        &mut self,\n+        bb: mir::BasicBlock,\n+    ) {\n+        let mut bx = self.build_block(bb);\n+        let data = &self.mir[bb];\n+\n+        debug!(\"codegen_block({:?}={:?})\", bb, data);\n+\n+        for statement in &data.statements {\n+            bx = self.codegen_statement(bx, statement);\n+        }\n+\n+        self.codegen_terminator(bx, bb, data.terminator());\n+    }\n \n-                do_call(self, &mut bx, fn_ty, fn_ptr, &llargs,\n-                        destination.as_ref().map(|&(_, target)| (ret_dest, target)),\n-                        cleanup);\n+    fn codegen_terminator(\n+        &mut self,\n+        mut bx: Bx,\n+        bb: mir::BasicBlock,\n+        terminator: &mir::Terminator<'tcx>\n+    ) {\n+        debug!(\"codegen_terminator: {:?}\", terminator);\n+\n+        // Create the cleanup bundle, if needed.\n+        let funclet_bb = self.cleanup_kinds[bb].funclet_bb(bb);\n+        let helper = TerminatorCodegenHelper {\n+            bb: &bb, terminator, funclet_bb\n+        };\n+\n+        self.set_debug_loc(&mut bx, terminator.source_info);\n+        match terminator.kind {\n+            mir::TerminatorKind::Resume => {\n+                self.codegen_resume_terminator(helper, bx)\n+            }\n+\n+            mir::TerminatorKind::Abort => {\n+                bx.abort();\n+                bx.unreachable();\n+            }\n+\n+            mir::TerminatorKind::Goto { target } => {\n+                helper.funclet_br(self, &mut bx, target);\n+            }\n+\n+            mir::TerminatorKind::SwitchInt {\n+                ref discr, switch_ty, ref values, ref targets\n+            } => {\n+                self.codegen_switchint_terminator(helper, bx, discr, switch_ty,\n+                                                  values, targets);\n+            }\n+\n+            mir::TerminatorKind::Return => {\n+                self.codegen_return_terminator(bx);\n+            }\n+\n+            mir::TerminatorKind::Unreachable => {\n+                bx.unreachable();\n+            }\n+\n+            mir::TerminatorKind::Drop { ref location, target, unwind } => {\n+                self.codegen_drop_terminator(helper, bx, location, target, unwind);\n+            }\n+\n+            mir::TerminatorKind::Assert { ref cond, expected, ref msg, target, cleanup } => {\n+                self.codegen_assert_terminator(helper, bx, terminator, cond,\n+                                               expected, msg, target, cleanup);\n+            }\n+\n+            mir::TerminatorKind::DropAndReplace { .. } => {\n+                bug!(\"undesugared DropAndReplace in codegen: {:?}\", terminator);\n+            }\n+\n+            mir::TerminatorKind::Call {\n+                ref func,\n+                ref args,\n+                ref destination,\n+                cleanup,\n+                from_hir_call: _\n+            } => {\n+                self.codegen_call_terminator(helper, bx, terminator, func,\n+                                             args, destination, cleanup);\n             }\n             mir::TerminatorKind::GeneratorDrop |\n             mir::TerminatorKind::Yield { .. } => bug!(\"generator ops in codegen\"),"}]}