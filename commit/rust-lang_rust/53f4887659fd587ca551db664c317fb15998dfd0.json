{"sha": "53f4887659fd587ca551db664c317fb15998dfd0", "node_id": "C_kwDOAAsO6NoAKDUzZjQ4ODc2NTlmZDU4N2NhNTUxZGI2NjRjMzE3ZmIxNTk5OGRmZDA", "commit": {"author": {"name": "Andy Wang", "email": "cbeuw.andy@gmail.com", "date": "2022-05-06T22:46:29Z"}, "committer": {"name": "Andy Wang", "email": "cbeuw.andy@gmail.com", "date": "2022-06-06T18:15:21Z"}, "message": "Use a new AllocationMap to store store buffers in the same allocation", "tree": {"sha": "d7dfd92e6e9c5b7b2d5e09cc172644c4ed8e7d2b", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/d7dfd92e6e9c5b7b2d5e09cc172644c4ed8e7d2b"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/53f4887659fd587ca551db664c317fb15998dfd0", "comment_count": 0, "verification": {"verified": true, "reason": "valid", "signature": "-----BEGIN PGP SIGNATURE-----\n\niQGzBAABCgAdFiEE7dcbcBMl24/h63ldGBtJ+fOPM3QFAmKeRDkACgkQGBtJ+fOP\nM3TAZwv/ZpamLzBHOpxdlvxtTF/RnSgMeIFOYRa0wkOv7g4hsSmP1KLjxXdU9vHc\n2d6ZdBTFl6TD0c6vj6HQWqOUpWAjsS3yHBvjZxQpILGTInxim+cfGcKRPukPFdG5\n0nfZjxWVDvNhro1ya8jpK/4DLZD0M9h9wCUXZeS3f1uLjRtww8H53H0iBZ7TwqP8\n3VbyF7YAoh+jQGT5XIsNVE0kB+1tv3RXwQbRa8htxT2rxt3bKlqEgPcgv0C/w+e8\nO9dKiJZK4IQIyFNueAbRUS1Vo0IAUhflbH8Ksd5SsmKfj2Qm0Lt0jCZ+XvEyWNpN\nRFaAYyRoV2JRvQZj+GC43wWAKWn/W5AAQmLn2qzgPZAfB43PSlWAU+UBqlJrhC9j\nPc/oMszS9srEGxKA0UJRaA2zavUtbsAPMcBDuL5bHLFBCy3rjniBv4ROAF9D6K2d\nPQe8dQFNprOIuBtjfs29RMKknEZjfOXoAZCAFS1FuEjEBo8RpQAHBBFo57YrUde+\nrrFYwdWy\n=wffX\n-----END PGP SIGNATURE-----", "payload": "tree d7dfd92e6e9c5b7b2d5e09cc172644c4ed8e7d2b\nparent ecdab5ff35297e9d70647f076b3aba656c8ad850\nauthor Andy Wang <cbeuw.andy@gmail.com> 1651877189 +0100\ncommitter Andy Wang <cbeuw.andy@gmail.com> 1654539321 +0100\n\nUse a new AllocationMap to store store buffers in the same allocation\n"}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/53f4887659fd587ca551db664c317fb15998dfd0", "html_url": "https://github.com/rust-lang/rust/commit/53f4887659fd587ca551db664c317fb15998dfd0", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/53f4887659fd587ca551db664c317fb15998dfd0/comments", "author": {"login": "cbeuw", "id": 7034308, "node_id": "MDQ6VXNlcjcwMzQzMDg=", "avatar_url": "https://avatars.githubusercontent.com/u/7034308?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cbeuw", "html_url": "https://github.com/cbeuw", "followers_url": "https://api.github.com/users/cbeuw/followers", "following_url": "https://api.github.com/users/cbeuw/following{/other_user}", "gists_url": "https://api.github.com/users/cbeuw/gists{/gist_id}", "starred_url": "https://api.github.com/users/cbeuw/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cbeuw/subscriptions", "organizations_url": "https://api.github.com/users/cbeuw/orgs", "repos_url": "https://api.github.com/users/cbeuw/repos", "events_url": "https://api.github.com/users/cbeuw/events{/privacy}", "received_events_url": "https://api.github.com/users/cbeuw/received_events", "type": "User", "site_admin": false}, "committer": {"login": "cbeuw", "id": 7034308, "node_id": "MDQ6VXNlcjcwMzQzMDg=", "avatar_url": "https://avatars.githubusercontent.com/u/7034308?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cbeuw", "html_url": "https://github.com/cbeuw", "followers_url": "https://api.github.com/users/cbeuw/followers", "following_url": "https://api.github.com/users/cbeuw/following{/other_user}", "gists_url": "https://api.github.com/users/cbeuw/gists{/gist_id}", "starred_url": "https://api.github.com/users/cbeuw/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cbeuw/subscriptions", "organizations_url": "https://api.github.com/users/cbeuw/orgs", "repos_url": "https://api.github.com/users/cbeuw/repos", "events_url": "https://api.github.com/users/cbeuw/events{/privacy}", "received_events_url": "https://api.github.com/users/cbeuw/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "ecdab5ff35297e9d70647f076b3aba656c8ad850", "url": "https://api.github.com/repos/rust-lang/rust/commits/ecdab5ff35297e9d70647f076b3aba656c8ad850", "html_url": "https://github.com/rust-lang/rust/commit/ecdab5ff35297e9d70647f076b3aba656c8ad850"}], "stats": {"total": 381, "additions": 342, "deletions": 39}, "files": [{"sha": "6c14ce165407ffa980de48a121d224ec19a2b2fa", "filename": "src/allocation_map.rs", "status": "added", "additions": 272, "deletions": 0, "changes": 272, "blob_url": "https://github.com/rust-lang/rust/blob/53f4887659fd587ca551db664c317fb15998dfd0/src%2Fallocation_map.rs", "raw_url": "https://github.com/rust-lang/rust/raw/53f4887659fd587ca551db664c317fb15998dfd0/src%2Fallocation_map.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Fallocation_map.rs?ref=53f4887659fd587ca551db664c317fb15998dfd0", "patch": "@@ -0,0 +1,272 @@\n+//! Implements a map from allocation ranges to data.\n+//! This is somewhat similar to RangeMap, but the ranges\n+//! and data are discrete and non-splittable. An allocation in the\n+//! map will always have the same range until explicitly removed\n+\n+use rustc_target::abi::Size;\n+use std::ops::{Index, IndexMut, Range};\n+\n+use rustc_const_eval::interpret::AllocRange;\n+\n+#[derive(Clone, Debug)]\n+struct Elem<T> {\n+    /// The range covered by this element; never empty.\n+    range: AllocRange,\n+    /// The data stored for this element.\n+    data: T,\n+}\n+\n+/// Index of an allocation within the map\n+type Position = usize;\n+\n+#[derive(Clone, Debug)]\n+pub struct AllocationMap<T> {\n+    v: Vec<Elem<T>>,\n+}\n+\n+#[derive(Clone, Debug, PartialEq)]\n+pub enum AccessType {\n+    /// The access perfectly overlaps (same offset and range) with the exsiting allocation\n+    PerfectlyOverlapping(Position),\n+    /// The access does not touch any exising allocation\n+    Empty(Position),\n+    /// The access overlaps with one or more existing allocations\n+    ImperfectlyOverlapping(Range<Position>),\n+}\n+\n+impl<T> AllocationMap<T> {\n+    pub fn new() -> Self {\n+        Self { v: Vec::new() }\n+    }\n+\n+    /// Finds the position of the allocation containing the given offset. If the offset is not\n+    /// in an existing allocation, then returns Err containing the position\n+    /// where such allocation should be inserted\n+    fn find_offset(&self, offset: Size) -> Result<Position, Position> {\n+        // We do a binary search.\n+        let mut left = 0usize; // inclusive\n+        let mut right = self.v.len(); // exclusive\n+        loop {\n+            if left == right {\n+                // No element contains the given offset. But the\n+                // index is where such element should be placed at.\n+                return Err(left);\n+            }\n+            let candidate = left.checked_add(right).unwrap() / 2;\n+            let elem = &self.v[candidate];\n+            if offset < elem.range.start {\n+                // We are too far right (offset is further left).\n+                debug_assert!(candidate < right); // we are making progress\n+                right = candidate;\n+            } else if offset >= elem.range.end() {\n+                // We are too far left (offset is further right).\n+                debug_assert!(candidate >= left); // we are making progress\n+                left = candidate + 1;\n+            } else {\n+                // This is it!\n+                return Ok(candidate);\n+            }\n+        }\n+    }\n+\n+    /// Determines whether a given access on `range` overlaps with\n+    /// an existing allocation\n+    pub fn access_type(&self, range: AllocRange) -> AccessType {\n+        match self.find_offset(range.start) {\n+            Ok(index) => {\n+                // Start of the range belongs to an existing object, now let's check the overlapping situation\n+                let elem = &self.v[index];\n+                // FIXME: derive Eq for AllocRange in rustc\n+                if elem.range.start == range.start && elem.range.size == range.size {\n+                    // Happy case: perfectly overlapping access\n+                    AccessType::PerfectlyOverlapping(index)\n+                } else {\n+                    // FIXME: add a last() method to AllocRange that returns the last inclusive offset (end() is exclusive)\n+                    let end_index = match self.find_offset(range.end() - Size::from_bytes(1)) {\n+                        // If the end lands in an existing object, add one to get the exclusive index\n+                        Ok(inclusive) => inclusive + 1,\n+                        Err(exclusive) => exclusive,\n+                    };\n+\n+                    AccessType::ImperfectlyOverlapping(index..end_index)\n+                }\n+            }\n+            Err(index) => {\n+                // Start of the range doesn't belong to an existing object\n+                match self.find_offset(range.end() - Size::from_bytes(1)) {\n+                    // Neither does the end\n+                    Err(end_index) =>\n+                        if index == end_index {\n+                            // There's nothing between the start and the end, so the range thing is empty\n+                            AccessType::Empty(index)\n+                        } else {\n+                            // Otherwise we have entirely covered an existing object\n+                            AccessType::ImperfectlyOverlapping(index..end_index)\n+                        },\n+                    // Otherwise at least part of it overlaps with something else\n+                    Ok(end_index) => AccessType::ImperfectlyOverlapping(index..end_index + 1),\n+                }\n+            }\n+        }\n+    }\n+\n+    /// Inserts an object and its occupied range at given position\n+    pub fn insert(&mut self, index: Position, range: AllocRange, data: T) {\n+        self.v.insert(index, Elem { range, data });\n+        // If we aren't the first element, then our start must be greater than the preivous element's end\n+        if index > 0 {\n+            debug_assert!(self.v[index - 1].range.end() <= range.start);\n+        }\n+        // If we aren't the last element, then our end must be smaller than next element's start\n+        if index < self.v.len() - 1 {\n+            debug_assert!(range.end() <= self.v[index + 1].range.start);\n+        }\n+    }\n+\n+    /// Removes an object at given position\n+    pub fn remove(&mut self, index: Position) -> T {\n+        self.v.remove(index).data\n+    }\n+}\n+\n+impl<T> Index<Position> for AllocationMap<T> {\n+    type Output = T;\n+\n+    fn index(&self, index: usize) -> &Self::Output {\n+        &self.v[index].data\n+    }\n+}\n+\n+impl<T> IndexMut<Position> for AllocationMap<T> {\n+    fn index_mut(&mut self, index: usize) -> &mut Self::Output {\n+        &mut self.v[index].data\n+    }\n+}\n+\n+#[cfg(test)]\n+mod tests {\n+    use rustc_const_eval::interpret::alloc_range;\n+\n+    use super::*;\n+\n+    #[test]\n+    fn empty_map() {\n+        // FIXME: make Size::from_bytes const\n+        let four = Size::from_bytes(4);\n+        let map = AllocationMap::<()>::new();\n+\n+        // Correctly tells where we should insert the first element (at index 0)\n+        assert_eq!(map.find_offset(Size::from_bytes(3)), Err(0));\n+\n+        // Correctly tells the access type along with the supposed index\n+        assert_eq!(map.access_type(alloc_range(Size::ZERO, four)), AccessType::Empty(0));\n+    }\n+\n+    #[test]\n+    #[should_panic]\n+    fn no_overlapping_inserts() {\n+        let four = Size::from_bytes(4);\n+\n+        let mut map = AllocationMap::<&str>::new();\n+\n+        // |_|_|_|_|#|#|#|#|_|_|_|_|...\n+        //  0 1 2 3 4 5 6 7 8 9 a b c d\n+        map.insert(0, alloc_range(four, four), \"#\");\n+        // |_|_|_|_|#|#|#|#|_|_|_|_|...\n+        //  0 ^ ^ ^ ^ 5 6 7 8 9 a b c d\n+        map.insert(0, alloc_range(Size::from_bytes(1), four), \"@\");\n+    }\n+\n+    #[test]\n+    fn boundaries() {\n+        let four = Size::from_bytes(4);\n+\n+        let mut map = AllocationMap::<&str>::new();\n+\n+        // |#|#|#|#|_|_|...\n+        //  0 1 2 3 4 5\n+        map.insert(0, alloc_range(Size::ZERO, four), \"#\");\n+        // |#|#|#|#|_|_|...\n+        //  0 1 2 3 ^ 5\n+        assert_eq!(map.find_offset(four), Err(1));\n+        // |#|#|#|#|_|_|_|_|_|...\n+        //  0 1 2 3 ^ ^ ^ ^ 8\n+        assert_eq!(map.access_type(alloc_range(four, four)), AccessType::Empty(1));\n+\n+        let eight = Size::from_bytes(8);\n+        // |#|#|#|#|_|_|_|_|@|@|@|@|_|_|...\n+        //  0 1 2 3 4 5 6 7 8 9 a b c d\n+        map.insert(1, alloc_range(eight, four), \"@\");\n+        // |#|#|#|#|_|_|_|_|@|@|@|@|_|_|...\n+        //  0 1 2 3 4 5 6 ^ 8 9 a b c d\n+        assert_eq!(map.find_offset(Size::from_bytes(7)), Err(1));\n+        // |#|#|#|#|_|_|_|_|@|@|@|@|_|_|...\n+        //  0 1 2 3 ^ ^ ^ ^ 8 9 a b c d\n+        assert_eq!(map.access_type(alloc_range(four, four)), AccessType::Empty(1));\n+    }\n+\n+    #[test]\n+    fn perfectly_overlapping() {\n+        let four = Size::from_bytes(4);\n+\n+        let mut map = AllocationMap::<&str>::new();\n+\n+        // |#|#|#|#|_|_|...\n+        //  0 1 2 3 4 5\n+        map.insert(0, alloc_range(Size::ZERO, four), \"#\");\n+        // |#|#|#|#|_|_|...\n+        //  ^ ^ ^ ^ 4 5\n+        assert_eq!(map.find_offset(Size::ZERO), Ok(0));\n+        assert_eq!(\n+            map.access_type(alloc_range(Size::ZERO, four)),\n+            AccessType::PerfectlyOverlapping(0)\n+        );\n+\n+        // |#|#|#|#|@|@|@|@|_|...\n+        //  0 1 2 3 4 5 6 7 8\n+        map.insert(1, alloc_range(four, four), \"@\");\n+        // |#|#|#|#|@|@|@|@|_|...\n+        //  0 1 2 3 ^ ^ ^ ^ 8\n+        assert_eq!(map.find_offset(four), Ok(1));\n+        assert_eq!(map.access_type(alloc_range(four, four)), AccessType::PerfectlyOverlapping(1));\n+    }\n+\n+    #[test]\n+    fn straddling() {\n+        let four = Size::from_bytes(4);\n+\n+        let mut map = AllocationMap::<&str>::new();\n+\n+        // |_|_|_|_|#|#|#|#|_|_|_|_|...\n+        //  0 1 2 3 4 5 6 7 8 9 a b c d\n+        map.insert(0, alloc_range(four, four), \"#\");\n+        // |_|_|_|_|#|#|#|#|_|_|_|_|...\n+        //  0 1 ^ ^ ^ ^ 6 7 8 9 a b c d\n+        assert_eq!(\n+            map.access_type(alloc_range(Size::from_bytes(2), four)),\n+            AccessType::ImperfectlyOverlapping(0..1)\n+        );\n+        // |_|_|_|_|#|#|#|#|_|_|_|_|...\n+        //  0 1 2 3 4 5 ^ ^ ^ ^ a b c d\n+        assert_eq!(\n+            map.access_type(alloc_range(Size::from_bytes(6), four)),\n+            AccessType::ImperfectlyOverlapping(0..1)\n+        );\n+        // |_|_|_|_|#|#|#|#|_|_|_|_|...\n+        //  0 1 ^ ^ ^ ^ ^ ^ ^ ^ a b c d\n+        assert_eq!(\n+            map.access_type(alloc_range(Size::from_bytes(2), Size::from_bytes(8))),\n+            AccessType::ImperfectlyOverlapping(0..1)\n+        );\n+\n+        // |_|_|_|_|#|#|#|#|_|_|@|@|_|_|...\n+        //  0 1 2 3 4 5 6 7 8 9 a b c d\n+        map.insert(1, alloc_range(Size::from_bytes(10), Size::from_bytes(2)), \"@\");\n+        // |_|_|_|_|#|#|#|#|_|_|@|@|_|_|...\n+        //  0 1 2 3 4 5 ^ ^ ^ ^ ^ ^ ^ ^\n+        assert_eq!(\n+            map.access_type(alloc_range(Size::from_bytes(6), Size::from_bytes(8))),\n+            AccessType::ImperfectlyOverlapping(0..2)\n+        );\n+    }\n+}"}, {"sha": "d9bfbc1bdb5a18243716451e23c04586a6ff7c50", "filename": "src/data_race.rs", "status": "modified", "additions": 8, "deletions": 18, "changes": 26, "blob_url": "https://github.com/rust-lang/rust/blob/53f4887659fd587ca551db664c317fb15998dfd0/src%2Fdata_race.rs", "raw_url": "https://github.com/rust-lang/rust/raw/53f4887659fd587ca551db664c317fb15998dfd0/src%2Fdata_race.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Fdata_race.rs?ref=53f4887659fd587ca551db664c317fb15998dfd0", "patch": "@@ -519,7 +519,8 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: MiriEvalContextExt<'mir, 'tcx> {\n                     global.sc_read();\n                 }\n                 let mut rng = this.machine.rng.borrow_mut();\n-                let buffer = alloc_buffers.get_store_buffer(alloc_range(base_offset, place.layout.size));\n+                let buffer =\n+                    alloc_buffers.get_store_buffer(alloc_range(base_offset, place.layout.size));\n                 let loaded = buffer.buffered_read(\n                     global,\n                     atomic == AtomicReadOp::SeqCst,\n@@ -555,12 +556,9 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: MiriEvalContextExt<'mir, 'tcx> {\n             if atomic == AtomicWriteOp::SeqCst {\n                 global.sc_write();\n             }\n-            let mut buffer = alloc_buffers.get_store_buffer_mut(alloc_range(base_offset, dest.layout.size));\n-            buffer.buffered_write(\n-                val,\n-                global,\n-                atomic == AtomicWriteOp::SeqCst,\n-            )?;\n+            let buffer =\n+                alloc_buffers.get_store_buffer_mut(alloc_range(base_offset, dest.layout.size));\n+            buffer.buffered_write(val, global, atomic == AtomicWriteOp::SeqCst)?;\n         }\n \n         Ok(())\n@@ -624,17 +622,9 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: MiriEvalContextExt<'mir, 'tcx> {\n         let lt = this.binary_op(mir::BinOp::Lt, &old, &rhs)?.to_scalar()?.to_bool()?;\n \n         let new_val = if min {\n-            if lt {\n-                &old\n-            } else {\n-                &rhs\n-            }\n+            if lt { &old } else { &rhs }\n         } else {\n-            if lt {\n-                &rhs\n-            } else {\n-                &old\n-            }\n+            if lt { &rhs } else { &old }\n         };\n \n         this.allow_data_races_mut(|this| this.write_immediate(**new_val, &(*place).into()))?;\n@@ -736,7 +726,7 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: MiriEvalContextExt<'mir, 'tcx> {\n                 global.sc_write();\n             }\n             let range = alloc_range(base_offset, place.layout.size);\n-            let mut buffer = alloc_buffers.get_store_buffer_mut(range);\n+            let buffer = alloc_buffers.get_store_buffer_mut(range);\n             buffer.read_from_last_store(global);\n             buffer.buffered_write(new_val, global, atomic == AtomicRwOp::SeqCst)?;\n         }"}, {"sha": "3270a57c4964e1e4cf7abab2ad097e9df7e233dd", "filename": "src/lib.rs", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "blob_url": "https://github.com/rust-lang/rust/blob/53f4887659fd587ca551db664c317fb15998dfd0/src%2Flib.rs", "raw_url": "https://github.com/rust-lang/rust/raw/53f4887659fd587ca551db664c317fb15998dfd0/src%2Flib.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flib.rs?ref=53f4887659fd587ca551db664c317fb15998dfd0", "patch": "@@ -31,6 +31,7 @@ extern crate rustc_session;\n extern crate rustc_span;\n extern crate rustc_target;\n \n+mod allocation_map;\n mod data_race;\n mod diagnostics;\n mod eval;"}, {"sha": "ca7fff7b08b9cc07da6deed097c1fad07e26e3f8", "filename": "src/machine.rs", "status": "modified", "additions": 6, "deletions": 2, "changes": 8, "blob_url": "https://github.com/rust-lang/rust/blob/53f4887659fd587ca551db664c317fb15998dfd0/src%2Fmachine.rs", "raw_url": "https://github.com/rust-lang/rust/raw/53f4887659fd587ca551db664c317fb15998dfd0/src%2Fmachine.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Fmachine.rs?ref=53f4887659fd587ca551db664c317fb15998dfd0", "patch": "@@ -636,13 +636,17 @@ impl<'mir, 'tcx> Machine<'mir, 'tcx> for Evaluator<'mir, 'tcx> {\n         let buffer_alloc = if ecx.machine.weak_memory {\n             // FIXME: if this is an atomic obejct, we want to supply its initial value\n             // while allocating the store buffer here.\n-            Some(weak_memory::AllocExtra::new_allocation(alloc.size()))\n+            Some(weak_memory::AllocExtra::new_allocation())\n         } else {\n             None\n         };\n         let alloc: Allocation<Tag, Self::AllocExtra> = alloc.convert_tag_add_extra(\n             &ecx.tcx,\n-            AllocExtra { stacked_borrows: stacks, data_race: race_alloc, weak_memory: buffer_alloc },\n+            AllocExtra {\n+                stacked_borrows: stacks,\n+                data_race: race_alloc,\n+                weak_memory: buffer_alloc,\n+            },\n             |ptr| Evaluator::tag_alloc_base_pointer(ecx, ptr),\n         );\n         Cow::Owned(alloc)"}, {"sha": "34c669239d5b8e3ec162e1fc358591e0ddcaf61b", "filename": "src/weak_memory.rs", "status": "modified", "additions": 55, "deletions": 19, "changes": 74, "blob_url": "https://github.com/rust-lang/rust/blob/53f4887659fd587ca551db664c317fb15998dfd0/src%2Fweak_memory.rs", "raw_url": "https://github.com/rust-lang/rust/raw/53f4887659fd587ca551db664c317fb15998dfd0/src%2Fweak_memory.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Fweak_memory.rs?ref=53f4887659fd587ca551db664c317fb15998dfd0", "patch": "@@ -12,7 +12,7 @@\n // load by each thread. This optimisation is done in tsan11\n // (https://github.com/ChrisLidbury/tsan11/blob/ecbd6b81e9b9454e01cba78eb9d88684168132c7/lib/tsan/rtl/tsan_relaxed.h#L35-L37)\n // and here.\n-// \n+//\n // 3. \u00a74.5 of the paper wants an SC store to mark all existing stores in the buffer that happens before it\n // as SC. This is not done in the operational semantics but implemented correctly in tsan11\n // (https://github.com/ChrisLidbury/tsan11/blob/ecbd6b81e9b9454e01cba78eb9d88684168132c7/lib/tsan/rtl/tsan_relaxed.cc#L160-L167)\n@@ -25,48 +25,84 @@\n // and here.\n \n use std::{\n-    cell::{Ref, RefCell, RefMut},\n+    cell::{Ref, RefCell},\n     collections::VecDeque,\n };\n \n use rustc_const_eval::interpret::{AllocRange, InterpResult, ScalarMaybeUninit};\n use rustc_data_structures::fx::FxHashMap;\n-use rustc_target::abi::Size;\n \n use crate::{\n+    allocation_map::{AccessType, AllocationMap},\n     data_race::{GlobalState, ThreadClockSet},\n-    RangeMap, Tag, VClock, VTimestamp, VectorIdx,\n+    Tag, VClock, VTimestamp, VectorIdx,\n };\n \n pub type AllocExtra = StoreBufferAlloc;\n+\n #[derive(Debug, Clone)]\n pub struct StoreBufferAlloc {\n     /// Store buffer of each atomic object in this allocation\n-    // Load may modify a StoreBuffer to record the loading thread's\n-    // timestamp so we need interior mutability here.\n-    store_buffer: RefCell<RangeMap<StoreBuffer>>,\n+    // Behind a RefCell because we need to allocate/remove on read access\n+    store_buffer: RefCell<AllocationMap<StoreBuffer>>,\n }\n \n impl StoreBufferAlloc {\n-    pub fn new_allocation(len: Size) -> Self {\n-        Self { store_buffer: RefCell::new(RangeMap::new(len, StoreBuffer::default())) }\n+    pub fn new_allocation() -> Self {\n+        Self { store_buffer: RefCell::new(AllocationMap::new()) }\n     }\n \n     /// Gets a store buffer associated with an atomic object in this allocation\n     pub fn get_store_buffer(&self, range: AllocRange) -> Ref<'_, StoreBuffer> {\n-        Ref::map(self.store_buffer.borrow(), |range_map| {\n-            let (.., store_buffer) = range_map.iter(range.start, range.size).next().unwrap();\n-            store_buffer\n-        })\n+        let access_type = self.store_buffer.borrow().access_type(range);\n+        let index = match access_type {\n+            AccessType::PerfectlyOverlapping(index) => index,\n+            AccessType::Empty(index) => {\n+                // First atomic access on this range, allocate a new StoreBuffer\n+                let mut buffer = self.store_buffer.borrow_mut();\n+                buffer.insert(index, range, StoreBuffer::default());\n+                index\n+            }\n+            AccessType::ImperfectlyOverlapping(index_range) => {\n+                // Accesses that imperfectly overlaps with existing atomic objects\n+                // do not have well-defined behaviours. But we don't throw a UB here\n+                // because we have (or will) checked that all bytes in the current\n+                // access are non-racy.\n+                // The behaviour here is that we delete all the existing objects this\n+                // access touches, and allocate a new and empty one for the exact range.\n+                // A read on an empty buffer returns None, which means the program will\n+                // observe the latest value in modification order at every byte.\n+                let mut buffer = self.store_buffer.borrow_mut();\n+                for index in index_range.clone() {\n+                    buffer.remove(index);\n+                }\n+                buffer.insert(index_range.start, range, StoreBuffer::default());\n+                index_range.start\n+            }\n+        };\n+        Ref::map(self.store_buffer.borrow(), |buffer| &buffer[index])\n     }\n \n-    pub fn get_store_buffer_mut(&self, range: AllocRange) -> RefMut<'_, StoreBuffer> {\n-        RefMut::map(self.store_buffer.borrow_mut(), |range_map| {\n-            let (.., store_buffer) = range_map.iter_mut(range.start, range.size).next().unwrap();\n-            store_buffer\n-        })\n+    /// Gets a mutable store buffer associated with an atomic object in this allocation\n+    pub fn get_store_buffer_mut(&mut self, range: AllocRange) -> &mut StoreBuffer {\n+        let buffer = self.store_buffer.get_mut();\n+        let access_type = buffer.access_type(range);\n+        let index = match access_type {\n+            AccessType::PerfectlyOverlapping(index) => index,\n+            AccessType::Empty(index) => {\n+                buffer.insert(index, range, StoreBuffer::default());\n+                index\n+            }\n+            AccessType::ImperfectlyOverlapping(index_range) => {\n+                for index in index_range.clone() {\n+                    buffer.remove(index);\n+                }\n+                buffer.insert(index_range.start, range, StoreBuffer::default());\n+                index_range.start\n+            }\n+        };\n+        &mut buffer[index]\n     }\n-\n }\n \n const STORE_BUFFER_LIMIT: usize = 128;"}]}