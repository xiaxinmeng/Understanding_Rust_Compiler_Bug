{"sha": "309a7534d7ebf39f5a62bac36749d35272fe0a3e", "node_id": "MDY6Q29tbWl0NzI0NzEyOjMwOWE3NTM0ZDdlYmYzOWY1YTYyYmFjMzY3NDlkMzUyNzJmZTBhM2U=", "commit": {"author": {"name": "Brian Anderson", "email": "banderson@mozilla.com", "date": "2011-08-22T18:48:00Z"}, "committer": {"name": "Brian Anderson", "email": "banderson@mozilla.com", "date": "2011-08-22T19:04:05Z"}, "message": "Move trans::ivec to middle::trans_ivec", "tree": {"sha": "22646dcee13399d42bff440d22ff11f3a7e15719", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/22646dcee13399d42bff440d22ff11f3a7e15719"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/309a7534d7ebf39f5a62bac36749d35272fe0a3e", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/309a7534d7ebf39f5a62bac36749d35272fe0a3e", "html_url": "https://github.com/rust-lang/rust/commit/309a7534d7ebf39f5a62bac36749d35272fe0a3e", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/309a7534d7ebf39f5a62bac36749d35272fe0a3e/comments", "author": {"login": "brson", "id": 147214, "node_id": "MDQ6VXNlcjE0NzIxNA==", "avatar_url": "https://avatars.githubusercontent.com/u/147214?v=4", "gravatar_id": "", "url": "https://api.github.com/users/brson", "html_url": "https://github.com/brson", "followers_url": "https://api.github.com/users/brson/followers", "following_url": "https://api.github.com/users/brson/following{/other_user}", "gists_url": "https://api.github.com/users/brson/gists{/gist_id}", "starred_url": "https://api.github.com/users/brson/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/brson/subscriptions", "organizations_url": "https://api.github.com/users/brson/orgs", "repos_url": "https://api.github.com/users/brson/repos", "events_url": "https://api.github.com/users/brson/events{/privacy}", "received_events_url": "https://api.github.com/users/brson/received_events", "type": "User", "site_admin": false}, "committer": {"login": "brson", "id": 147214, "node_id": "MDQ6VXNlcjE0NzIxNA==", "avatar_url": "https://avatars.githubusercontent.com/u/147214?v=4", "gravatar_id": "", "url": "https://api.github.com/users/brson", "html_url": "https://github.com/brson", "followers_url": "https://api.github.com/users/brson/followers", "following_url": "https://api.github.com/users/brson/following{/other_user}", "gists_url": "https://api.github.com/users/brson/gists{/gist_id}", "starred_url": "https://api.github.com/users/brson/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/brson/subscriptions", "organizations_url": "https://api.github.com/users/brson/orgs", "repos_url": "https://api.github.com/users/brson/repos", "events_url": "https://api.github.com/users/brson/events{/privacy}", "received_events_url": "https://api.github.com/users/brson/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "179658e20fa299b4f7405ea72affd1c8d175d95b", "url": "https://api.github.com/repos/rust-lang/rust/commits/179658e20fa299b4f7405ea72affd1c8d175d95b", "html_url": "https://github.com/rust-lang/rust/commit/179658e20fa299b4f7405ea72affd1c8d175d95b"}], "stats": {"total": 1396, "additions": 705, "deletions": 691}, "files": [{"sha": "abc473c8cb71f5d79c9127938c8c7f3fc07258c9", "filename": "src/comp/middle/trans.rs", "status": "modified", "additions": 1, "deletions": 691, "changes": 692, "blob_url": "https://github.com/rust-lang/rust/blob/309a7534d7ebf39f5a62bac36749d35272fe0a3e/src%2Fcomp%2Fmiddle%2Ftrans.rs", "raw_url": "https://github.com/rust-lang/rust/raw/309a7534d7ebf39f5a62bac36749d35272fe0a3e/src%2Fcomp%2Fmiddle%2Ftrans.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Fcomp%2Fmiddle%2Ftrans.rs?ref=309a7534d7ebf39f5a62bac36749d35272fe0a3e", "patch": "@@ -74,6 +74,7 @@ import trans_common::*;\n \n import trans_objects::trans_anon_obj;\n import trans_objects::trans_obj;\n+import ivec = trans_ivec;\n \n // This function now fails if called on a type with dynamic size (as its\n // return value was always meaningless in that case anyhow). Beware!\n@@ -2642,697 +2643,6 @@ fn trans_evec_append(cx: &@block_ctxt, t: ty::t, lhs: ValueRef,\n                              llelt_tydesc.val, dst, src, skip_null]));\n }\n \n-mod ivec {\n-    fn trans_ivec(bcx: @block_ctxt, args: &[@ast::expr], id: ast::node_id) ->\n-        result {\n-        let typ = node_id_type(bcx_ccx(bcx), id);\n-        let unit_ty;\n-        alt ty::struct(bcx_tcx(bcx), typ) {\n-          ty::ty_vec(mt) { unit_ty = mt.ty; }\n-          _ { bcx_ccx(bcx).sess.bug(\"non-ivec type in trans_ivec\"); }\n-        }\n-        let llunitty = type_of_or_i8(bcx, unit_ty);\n-\n-        let ares = ivec::alloc(bcx, unit_ty);\n-        bcx = ares.bcx;\n-        let llvecptr = ares.llptr;\n-        let unit_sz = ares.llunitsz;\n-        let llalen = ares.llalen;\n-\n-        add_clean_temp(bcx, llvecptr, typ);\n-\n-        let lllen = bcx.build.Mul(C_uint(std::vec::len(args)), unit_sz);\n-        // Allocate the vector pieces and store length and allocated length.\n-\n-        let llfirsteltptr;\n-        if std::vec::len(args) > 0u &&\n-            std::vec::len(args) <= abi::ivec_default_length {\n-            // Interior case.\n-\n-            bcx.build.Store(lllen,\n-                            bcx.build.InBoundsGEP(llvecptr,\n-                                                  [C_int(0),\n-                                                   C_uint(abi::ivec_elt_len)]));\n-            bcx.build.Store(llalen,\n-                            bcx.build.InBoundsGEP(llvecptr,\n-                                                  [C_int(0),\n-                                                   C_uint(abi::ivec_elt_alen)]));\n-            llfirsteltptr =\n-                bcx.build.InBoundsGEP(llvecptr,\n-                                      [C_int(0), C_uint(abi::ivec_elt_elems),\n-                                       C_int(0)]);\n-        } else {\n-            // Heap case.\n-\n-            let stub_z = [C_int(0), C_uint(abi::ivec_heap_stub_elt_zero)];\n-            let stub_a = [C_int(0), C_uint(abi::ivec_heap_stub_elt_alen)];\n-            let stub_p = [C_int(0), C_uint(abi::ivec_heap_stub_elt_ptr)];\n-            let llstubty = T_ivec_heap(llunitty);\n-            let llstubptr = bcx.build.PointerCast(llvecptr, T_ptr(llstubty));\n-            bcx.build.Store(C_int(0), bcx.build.InBoundsGEP(llstubptr, stub_z));\n-            let llheapty = T_ivec_heap_part(llunitty);\n-            if std::vec::len(args) == 0u {\n-                // Null heap pointer indicates a zero-length vector.\n-\n-                bcx.build.Store(llalen, bcx.build.InBoundsGEP(llstubptr, stub_a));\n-                bcx.build.Store(C_null(T_ptr(llheapty)),\n-                                bcx.build.InBoundsGEP(llstubptr, stub_p));\n-                llfirsteltptr = C_null(T_ptr(llunitty));\n-            } else {\n-                bcx.build.Store(lllen, bcx.build.InBoundsGEP(llstubptr, stub_a));\n-\n-                let llheapsz = bcx.build.Add(llsize_of(llheapty), lllen);\n-                let rslt = trans_shared_malloc(bcx, T_ptr(llheapty), llheapsz);\n-                bcx = rslt.bcx;\n-                let llheapptr = rslt.val;\n-                bcx.build.Store(llheapptr,\n-                                bcx.build.InBoundsGEP(llstubptr, stub_p));\n-                let heap_l = [C_int(0), C_uint(abi::ivec_heap_elt_len)];\n-                bcx.build.Store(lllen, bcx.build.InBoundsGEP(llheapptr, heap_l));\n-                llfirsteltptr =\n-                    bcx.build.InBoundsGEP(llheapptr,\n-                                          [C_int(0),\n-                                           C_uint(abi::ivec_heap_elt_elems),\n-                                           C_int(0)]);\n-            }\n-        }\n-        // Store the individual elements.\n-\n-        let i = 0u;\n-        for e: @ast::expr in args {\n-            let lv = trans_lval(bcx, e);\n-            bcx = lv.res.bcx;\n-            let lleltptr;\n-            if ty::type_has_dynamic_size(bcx_tcx(bcx), unit_ty) {\n-                lleltptr =\n-                    bcx.build.InBoundsGEP(llfirsteltptr,\n-                                          [bcx.build.Mul(C_uint(i), unit_sz)]);\n-            } else {\n-                lleltptr = bcx.build.InBoundsGEP(llfirsteltptr, [C_uint(i)]);\n-            }\n-            bcx = move_val_if_temp(bcx, INIT, lleltptr, lv, unit_ty);\n-            i += 1u;\n-        }\n-        ret rslt(bcx, llvecptr);\n-    }\n-\n-    // Returns the length of an interior vector and a pointer to its first\n-    // element, in that order.\n-    fn get_len_and_data(bcx: &@block_ctxt, orig_v: ValueRef, unit_ty: ty::t)\n-       -> {len: ValueRef, data: ValueRef, bcx: @block_ctxt} {\n-        // If this interior vector has dynamic size, we can't assume anything\n-        // about the LLVM type of the value passed in, so we cast it to an\n-        // opaque vector type.\n-        let v;\n-        if ty::type_has_dynamic_size(bcx_tcx(bcx), unit_ty) {\n-            v = bcx.build.PointerCast(orig_v, T_ptr(T_opaque_ivec()));\n-        } else { v = orig_v; }\n-\n-        let llunitty = type_of_or_i8(bcx, unit_ty);\n-        let stack_len =\n-            load_inbounds(bcx, v, [C_int(0), C_uint(abi::ivec_elt_len)]);\n-        let stack_elem =\n-            bcx.build.InBoundsGEP(v,\n-                                  [C_int(0), C_uint(abi::ivec_elt_elems),\n-                                   C_int(0)]);\n-        let on_heap =\n-            bcx.build.ICmp(lib::llvm::LLVMIntEQ, stack_len, C_int(0));\n-        let on_heap_cx = new_sub_block_ctxt(bcx, \"on_heap\");\n-        let next_cx = new_sub_block_ctxt(bcx, \"next\");\n-        bcx.build.CondBr(on_heap, on_heap_cx.llbb, next_cx.llbb);\n-        let heap_stub =\n-            on_heap_cx.build.PointerCast(v, T_ptr(T_ivec_heap(llunitty)));\n-        let heap_ptr =\n-            load_inbounds(on_heap_cx, heap_stub,\n-                          [C_int(0), C_uint(abi::ivec_heap_stub_elt_ptr)]);\n-\n-        // Check whether the heap pointer is null. If it is, the vector length\n-        // is truly zero.\n-\n-        let llstubty = T_ivec_heap(llunitty);\n-        let llheapptrty = struct_elt(llstubty, abi::ivec_heap_stub_elt_ptr);\n-        let heap_ptr_is_null =\n-            on_heap_cx.build.ICmp(lib::llvm::LLVMIntEQ, heap_ptr,\n-                                  C_null(T_ptr(llheapptrty)));\n-        let zero_len_cx = new_sub_block_ctxt(bcx, \"zero_len\");\n-        let nonzero_len_cx = new_sub_block_ctxt(bcx, \"nonzero_len\");\n-        on_heap_cx.build.CondBr(heap_ptr_is_null, zero_len_cx.llbb,\n-                                nonzero_len_cx.llbb);\n-        // Technically this context is unnecessary, but it makes this function\n-        // clearer.\n-\n-        let zero_len = C_int(0);\n-        let zero_elem = C_null(T_ptr(llunitty));\n-        zero_len_cx.build.Br(next_cx.llbb);\n-        // If we're here, then we actually have a heapified vector.\n-\n-        let heap_len =\n-            load_inbounds(nonzero_len_cx, heap_ptr,\n-                          [C_int(0), C_uint(abi::ivec_heap_elt_len)]);\n-        let heap_elem =\n-            {\n-                let v =\n-                    [C_int(0), C_uint(abi::ivec_heap_elt_elems), C_int(0)];\n-                nonzero_len_cx.build.InBoundsGEP(heap_ptr, v)\n-            };\n-\n-        nonzero_len_cx.build.Br(next_cx.llbb);\n-        // Now we can figure out the length of `v` and get a pointer to its\n-        // first element.\n-\n-        let len =\n-            next_cx.build.Phi(T_int(), [stack_len, zero_len, heap_len],\n-                              [bcx.llbb, zero_len_cx.llbb,\n-                               nonzero_len_cx.llbb]);\n-        let elem =\n-            next_cx.build.Phi(T_ptr(llunitty),\n-                              [stack_elem, zero_elem, heap_elem],\n-                              [bcx.llbb, zero_len_cx.llbb,\n-                               nonzero_len_cx.llbb]);\n-        ret {len: len, data: elem, bcx: next_cx};\n-    }\n-\n-    // Returns a tuple consisting of a pointer to the newly-reserved space and\n-    // a block context. Updates the length appropriately.\n-    fn reserve_space(cx: &@block_ctxt, llunitty: TypeRef, v: ValueRef,\n-                     len_needed: ValueRef) -> result {\n-        let stack_len_ptr =\n-            cx.build.InBoundsGEP(v, [C_int(0), C_uint(abi::ivec_elt_len)]);\n-        let stack_len = cx.build.Load(stack_len_ptr);\n-        let alen =\n-            load_inbounds(cx, v, [C_int(0), C_uint(abi::ivec_elt_alen)]);\n-        // There are four cases we have to consider:\n-        // (1) On heap, no resize necessary.\n-        // (2) On heap, need to resize.\n-        // (3) On stack, no resize necessary.\n-        // (4) On stack, need to spill to heap.\n-\n-        let maybe_on_heap =\n-            cx.build.ICmp(lib::llvm::LLVMIntEQ, stack_len, C_int(0));\n-        let maybe_on_heap_cx = new_sub_block_ctxt(cx, \"maybe_on_heap\");\n-        let on_stack_cx = new_sub_block_ctxt(cx, \"on_stack\");\n-        cx.build.CondBr(maybe_on_heap, maybe_on_heap_cx.llbb,\n-                        on_stack_cx.llbb);\n-        let next_cx = new_sub_block_ctxt(cx, \"next\");\n-        // We're possibly on the heap, unless the vector is zero-length.\n-\n-        let stub_p = [C_int(0), C_uint(abi::ivec_heap_stub_elt_ptr)];\n-        let stub_ptr =\n-            maybe_on_heap_cx.build.PointerCast(v,\n-                                               T_ptr(T_ivec_heap(llunitty)));\n-        let heap_ptr = load_inbounds(maybe_on_heap_cx, stub_ptr, stub_p);\n-        let on_heap =\n-            maybe_on_heap_cx.build.ICmp(lib::llvm::LLVMIntNE, heap_ptr,\n-                                        C_null(val_ty(heap_ptr)));\n-        let on_heap_cx = new_sub_block_ctxt(cx, \"on_heap\");\n-        maybe_on_heap_cx.build.CondBr(on_heap, on_heap_cx.llbb,\n-                                      on_stack_cx.llbb);\n-        // We're definitely on the heap. Check whether we need to resize.\n-\n-        let heap_len_ptr =\n-            on_heap_cx.build.InBoundsGEP(heap_ptr,\n-                                         [C_int(0),\n-                                          C_uint(abi::ivec_heap_elt_len)]);\n-        let heap_len = on_heap_cx.build.Load(heap_len_ptr);\n-        let new_heap_len = on_heap_cx.build.Add(heap_len, len_needed);\n-        let heap_len_unscaled =\n-            on_heap_cx.build.UDiv(heap_len, llsize_of(llunitty));\n-        let heap_no_resize_needed =\n-            on_heap_cx.build.ICmp(lib::llvm::LLVMIntULE, new_heap_len, alen);\n-        let heap_no_resize_cx = new_sub_block_ctxt(cx, \"heap_no_resize\");\n-        let heap_resize_cx = new_sub_block_ctxt(cx, \"heap_resize\");\n-        on_heap_cx.build.CondBr(heap_no_resize_needed, heap_no_resize_cx.llbb,\n-                                heap_resize_cx.llbb);\n-        // Case (1): We're on the heap and don't need to resize.\n-\n-        let heap_data_no_resize =\n-            {\n-                let v =\n-                    [C_int(0), C_uint(abi::ivec_heap_elt_elems),\n-                     heap_len_unscaled];\n-                heap_no_resize_cx.build.InBoundsGEP(heap_ptr, v)\n-            };\n-        heap_no_resize_cx.build.Store(new_heap_len, heap_len_ptr);\n-        heap_no_resize_cx.build.Br(next_cx.llbb);\n-        // Case (2): We're on the heap and need to resize. This path is rare,\n-        // so we delegate to cold glue.\n-\n-        {\n-            let p =\n-                heap_resize_cx.build.PointerCast(v, T_ptr(T_opaque_ivec()));\n-            let upcall = bcx_ccx(cx).upcalls.ivec_resize_shared;\n-            heap_resize_cx.build.Call(upcall,\n-                                      [cx.fcx.lltaskptr, p, new_heap_len]);\n-        }\n-        let heap_ptr_resize = load_inbounds(heap_resize_cx, stub_ptr, stub_p);\n-\n-        let heap_data_resize =\n-            {\n-                let v =\n-                    [C_int(0), C_uint(abi::ivec_heap_elt_elems),\n-                     heap_len_unscaled];\n-                heap_resize_cx.build.InBoundsGEP(heap_ptr_resize, v)\n-            };\n-        heap_resize_cx.build.Br(next_cx.llbb);\n-        // We're on the stack. Check whether we need to spill to the heap.\n-\n-        let new_stack_len = on_stack_cx.build.Add(stack_len, len_needed);\n-        let stack_no_spill_needed =\n-            on_stack_cx.build.ICmp(lib::llvm::LLVMIntULE, new_stack_len,\n-                                   alen);\n-        let stack_len_unscaled =\n-            on_stack_cx.build.UDiv(stack_len, llsize_of(llunitty));\n-        let stack_no_spill_cx = new_sub_block_ctxt(cx, \"stack_no_spill\");\n-        let stack_spill_cx = new_sub_block_ctxt(cx, \"stack_spill\");\n-        on_stack_cx.build.CondBr(stack_no_spill_needed,\n-                                 stack_no_spill_cx.llbb, stack_spill_cx.llbb);\n-        // Case (3): We're on the stack and don't need to spill.\n-\n-        let stack_data_no_spill =\n-            stack_no_spill_cx.build.InBoundsGEP(v,\n-                                                [C_int(0),\n-                                                 C_uint(abi::ivec_elt_elems),\n-                                                 stack_len_unscaled]);\n-        stack_no_spill_cx.build.Store(new_stack_len, stack_len_ptr);\n-        stack_no_spill_cx.build.Br(next_cx.llbb);\n-        // Case (4): We're on the stack and need to spill. Like case (2), this\n-        // path is rare, so we delegate to cold glue.\n-\n-        {\n-            let p =\n-                stack_spill_cx.build.PointerCast(v, T_ptr(T_opaque_ivec()));\n-            let upcall = bcx_ccx(cx).upcalls.ivec_spill_shared;\n-            stack_spill_cx.build.Call(upcall,\n-                                      [cx.fcx.lltaskptr, p, new_stack_len]);\n-        }\n-        let spill_stub =\n-            stack_spill_cx.build.PointerCast(v, T_ptr(T_ivec_heap(llunitty)));\n-\n-        let heap_ptr_spill =\n-            load_inbounds(stack_spill_cx, spill_stub, stub_p);\n-\n-        let heap_data_spill =\n-            {\n-                let v =\n-                    [C_int(0), C_uint(abi::ivec_heap_elt_elems),\n-                     stack_len_unscaled];\n-                stack_spill_cx.build.InBoundsGEP(heap_ptr_spill, v)\n-            };\n-        stack_spill_cx.build.Br(next_cx.llbb);\n-        // Phi together the different data pointers to get the result.\n-\n-        let data_ptr =\n-            next_cx.build.Phi(T_ptr(llunitty),\n-                              [heap_data_no_resize, heap_data_resize,\n-                               stack_data_no_spill, heap_data_spill],\n-                              [heap_no_resize_cx.llbb, heap_resize_cx.llbb,\n-                               stack_no_spill_cx.llbb, stack_spill_cx.llbb]);\n-        ret rslt(next_cx, data_ptr);\n-    }\n-    fn trans_append(cx: &@block_ctxt, t: ty::t, orig_lhs: ValueRef,\n-                    orig_rhs: ValueRef) -> result {\n-        // Cast to opaque interior vector types if necessary.\n-        let lhs;\n-        let rhs;\n-        if ty::type_has_dynamic_size(bcx_tcx(cx), t) {\n-            lhs = cx.build.PointerCast(orig_lhs, T_ptr(T_opaque_ivec()));\n-            rhs = cx.build.PointerCast(orig_rhs, T_ptr(T_opaque_ivec()));\n-        } else { lhs = orig_lhs; rhs = orig_rhs; }\n-\n-        let unit_ty = ty::sequence_element_type(bcx_tcx(cx), t);\n-        let llunitty = type_of_or_i8(cx, unit_ty);\n-        alt ty::struct(bcx_tcx(cx), t) {\n-          ty::ty_istr. { }\n-          ty::ty_vec(_) { }\n-          _ { bcx_tcx(cx).sess.bug(\"non-istr/ivec in trans_append\"); }\n-        }\n-\n-        let rs = size_of(cx, unit_ty);\n-        let bcx = rs.bcx;\n-        let unit_sz = rs.val;\n-\n-        // Gather the various type descriptors we'll need.\n-\n-        // FIXME (issue #511): This is needed to prevent a leak.\n-        let no_tydesc_info = none;\n-\n-        rs = get_tydesc(bcx, t, false, no_tydesc_info).result;\n-        bcx = rs.bcx;\n-        rs = get_tydesc(bcx, unit_ty, false, no_tydesc_info).result;\n-        bcx = rs.bcx;\n-        lazily_emit_tydesc_glue(bcx, abi::tydesc_field_take_glue, none);\n-        lazily_emit_tydesc_glue(bcx, abi::tydesc_field_drop_glue, none);\n-        lazily_emit_tydesc_glue(bcx, abi::tydesc_field_free_glue, none);\n-        lazily_emit_tydesc_glue(bcx, abi::tydesc_field_copy_glue, none);\n-        let rhs_len_and_data = get_len_and_data(bcx, rhs, unit_ty);\n-        let rhs_len = rhs_len_and_data.len;\n-        let rhs_data = rhs_len_and_data.data;\n-        bcx = rhs_len_and_data.bcx;\n-        rs = reserve_space(bcx, llunitty, lhs, rhs_len);\n-        let lhs_data = rs.val;\n-        bcx = rs.bcx;\n-        // Work out the end pointer.\n-\n-        let lhs_unscaled_idx = bcx.build.UDiv(rhs_len, llsize_of(llunitty));\n-        let lhs_end = bcx.build.InBoundsGEP(lhs_data, [lhs_unscaled_idx]);\n-        // Now emit the copy loop.\n-\n-        let dest_ptr = alloca(bcx, T_ptr(llunitty));\n-        bcx.build.Store(lhs_data, dest_ptr);\n-        let src_ptr = alloca(bcx, T_ptr(llunitty));\n-        bcx.build.Store(rhs_data, src_ptr);\n-        let copy_loop_header_cx = new_sub_block_ctxt(bcx, \"copy_loop_header\");\n-        bcx.build.Br(copy_loop_header_cx.llbb);\n-        let copy_dest_ptr = copy_loop_header_cx.build.Load(dest_ptr);\n-        let not_yet_at_end =\n-            copy_loop_header_cx.build.ICmp(lib::llvm::LLVMIntNE,\n-                                           copy_dest_ptr, lhs_end);\n-        let copy_loop_body_cx = new_sub_block_ctxt(bcx, \"copy_loop_body\");\n-        let next_cx = new_sub_block_ctxt(bcx, \"next\");\n-        copy_loop_header_cx.build.CondBr(not_yet_at_end,\n-                                         copy_loop_body_cx.llbb,\n-                                         next_cx.llbb);\n-\n-        let copy_src_ptr = copy_loop_body_cx.build.Load(src_ptr);\n-        let copy_src =\n-            load_if_immediate(copy_loop_body_cx, copy_src_ptr, unit_ty);\n-\n-        let post_copy_cx = copy_val\n-            (copy_loop_body_cx, INIT, copy_dest_ptr, copy_src, unit_ty);\n-        // Increment both pointers.\n-        if ty::type_has_dynamic_size(bcx_tcx(cx), t) {\n-            // We have to increment by the dynamically-computed size.\n-            incr_ptr(post_copy_cx, copy_dest_ptr, unit_sz, dest_ptr);\n-            incr_ptr(post_copy_cx, copy_src_ptr, unit_sz, src_ptr);\n-        } else {\n-            incr_ptr(post_copy_cx, copy_dest_ptr, C_int(1), dest_ptr);\n-            incr_ptr(post_copy_cx, copy_src_ptr, C_int(1), src_ptr);\n-        }\n-\n-        post_copy_cx.build.Br(copy_loop_header_cx.llbb);\n-        ret rslt(next_cx, C_nil());\n-    }\n-\n-    type alloc_result =\n-        {bcx: @block_ctxt,\n-         llptr: ValueRef,\n-         llunitsz: ValueRef,\n-         llalen: ValueRef};\n-\n-    fn alloc(cx: &@block_ctxt, unit_ty: ty::t) -> alloc_result {\n-        let dynamic = ty::type_has_dynamic_size(bcx_tcx(cx), unit_ty);\n-\n-        let bcx;\n-        if dynamic {\n-            bcx = llderivedtydescs_block_ctxt(cx.fcx);\n-        } else { bcx = cx; }\n-\n-        let llunitsz;\n-        let rslt = size_of(bcx, unit_ty);\n-        bcx = rslt.bcx;\n-        llunitsz = rslt.val;\n-\n-        if dynamic { cx.fcx.llderivedtydescs = bcx.llbb; }\n-\n-        let llalen =\n-            bcx.build.Mul(llunitsz, C_uint(abi::ivec_default_length));\n-\n-        let llptr;\n-        let llunitty = type_of_or_i8(bcx, unit_ty);\n-        let bcx_result;\n-        if dynamic {\n-            let llarraysz = bcx.build.Add(llsize_of(T_opaque_ivec()), llalen);\n-            let llvecptr = array_alloca(bcx, T_i8(), llarraysz);\n-\n-            bcx_result = cx;\n-            llptr =\n-                bcx_result.build.PointerCast(llvecptr,\n-                                             T_ptr(T_opaque_ivec()));\n-        } else { llptr = alloca(bcx, T_ivec(llunitty)); bcx_result = bcx; }\n-\n-        ret {bcx: bcx_result,\n-             llptr: llptr,\n-             llunitsz: llunitsz,\n-             llalen: llalen};\n-    }\n-\n-    fn trans_add(cx: &@block_ctxt, vec_ty: ty::t, lhs: ValueRef,\n-                 rhs: ValueRef) -> result {\n-        let bcx = cx;\n-        let unit_ty = ty::sequence_element_type(bcx_tcx(bcx), vec_ty);\n-\n-        let ares = alloc(bcx, unit_ty);\n-        bcx = ares.bcx;\n-        let llvecptr = ares.llptr;\n-        let unit_sz = ares.llunitsz;\n-        let llalen = ares.llalen;\n-\n-        add_clean_temp(bcx, llvecptr, vec_ty);\n-\n-        let llunitty = type_of_or_i8(bcx, unit_ty);\n-        let llheappartty = T_ivec_heap_part(llunitty);\n-        let lhs_len_and_data = get_len_and_data(bcx, lhs, unit_ty);\n-        let lhs_len = lhs_len_and_data.len;\n-        let lhs_data = lhs_len_and_data.data;\n-        bcx = lhs_len_and_data.bcx;\n-        let rhs_len_and_data = get_len_and_data(bcx, rhs, unit_ty);\n-        let rhs_len = rhs_len_and_data.len;\n-        let rhs_data = rhs_len_and_data.data;\n-        bcx = rhs_len_and_data.bcx;\n-        let lllen = bcx.build.Add(lhs_len, rhs_len);\n-        // We have three cases to handle here:\n-        // (1) Length is zero ([] + []).\n-        // (2) Copy onto stack.\n-        // (3) Allocate on heap and copy there.\n-\n-        let len_is_zero =\n-            bcx.build.ICmp(lib::llvm::LLVMIntEQ, lllen, C_int(0));\n-        let zero_len_cx = new_sub_block_ctxt(bcx, \"zero_len\");\n-        let nonzero_len_cx = new_sub_block_ctxt(bcx, \"nonzero_len\");\n-        bcx.build.CondBr(len_is_zero, zero_len_cx.llbb, nonzero_len_cx.llbb);\n-        // Case (1): Length is zero.\n-\n-        let stub_z = [C_int(0), C_uint(abi::ivec_heap_stub_elt_zero)];\n-        let stub_a = [C_int(0), C_uint(abi::ivec_heap_stub_elt_alen)];\n-        let stub_p = [C_int(0), C_uint(abi::ivec_heap_stub_elt_ptr)];\n-\n-        let vec_l = [C_int(0), C_uint(abi::ivec_elt_len)];\n-        let vec_a = [C_int(0), C_uint(abi::ivec_elt_alen)];\n-\n-        let stub_ptr_zero =\n-            zero_len_cx.build.PointerCast(llvecptr,\n-                                          T_ptr(T_ivec_heap(llunitty)));\n-        zero_len_cx.build.Store(C_int(0),\n-                                zero_len_cx.build.InBoundsGEP(stub_ptr_zero,\n-                                                              stub_z));\n-        zero_len_cx.build.Store(llalen,\n-                                zero_len_cx.build.InBoundsGEP(stub_ptr_zero,\n-                                                              stub_a));\n-        zero_len_cx.build.Store(C_null(T_ptr(llheappartty)),\n-                                zero_len_cx.build.InBoundsGEP(stub_ptr_zero,\n-                                                              stub_p));\n-        let next_cx = new_sub_block_ctxt(bcx, \"next\");\n-        zero_len_cx.build.Br(next_cx.llbb);\n-        // Determine whether we need to spill to the heap.\n-\n-        let on_stack =\n-            nonzero_len_cx.build.ICmp(lib::llvm::LLVMIntULE, lllen, llalen);\n-        let stack_cx = new_sub_block_ctxt(bcx, \"stack\");\n-        let heap_cx = new_sub_block_ctxt(bcx, \"heap\");\n-        nonzero_len_cx.build.CondBr(on_stack, stack_cx.llbb, heap_cx.llbb);\n-        // Case (2): Copy onto stack.\n-\n-        stack_cx.build.Store(lllen,\n-                             stack_cx.build.InBoundsGEP(llvecptr, vec_l));\n-        stack_cx.build.Store(llalen,\n-                             stack_cx.build.InBoundsGEP(llvecptr, vec_a));\n-        let dest_ptr_stack =\n-            stack_cx.build.InBoundsGEP(llvecptr,\n-                                       [C_int(0), C_uint(abi::ivec_elt_elems),\n-                                        C_int(0)]);\n-        let copy_cx = new_sub_block_ctxt(bcx, \"copy\");\n-        stack_cx.build.Br(copy_cx.llbb);\n-        // Case (3): Allocate on heap and copy there.\n-\n-        let stub_ptr_heap =\n-            heap_cx.build.PointerCast(llvecptr, T_ptr(T_ivec_heap(llunitty)));\n-        heap_cx.build.Store(C_int(0),\n-                            heap_cx.build.InBoundsGEP(stub_ptr_heap, stub_z));\n-        heap_cx.build.Store(lllen,\n-                            heap_cx.build.InBoundsGEP(stub_ptr_heap, stub_a));\n-        let heap_sz = heap_cx.build.Add(llsize_of(llheappartty), lllen);\n-        let rs = trans_shared_malloc(heap_cx, T_ptr(llheappartty), heap_sz);\n-        let heap_part = rs.val;\n-        heap_cx = rs.bcx;\n-        heap_cx.build.Store(heap_part,\n-                            heap_cx.build.InBoundsGEP(stub_ptr_heap, stub_p));\n-        {\n-            let v = [C_int(0), C_uint(abi::ivec_heap_elt_len)];\n-            heap_cx.build.Store(lllen,\n-                                heap_cx.build.InBoundsGEP(heap_part, v));\n-        }\n-        let dest_ptr_heap =\n-            heap_cx.build.InBoundsGEP(heap_part,\n-                                      [C_int(0),\n-                                       C_uint(abi::ivec_heap_elt_elems),\n-                                       C_int(0)]);\n-        heap_cx.build.Br(copy_cx.llbb);\n-        // Emit the copy loop.\n-\n-        let first_dest_ptr =\n-            copy_cx.build.Phi(T_ptr(llunitty),\n-                              [dest_ptr_stack, dest_ptr_heap],\n-                              [stack_cx.llbb, heap_cx.llbb]);\n-\n-        let lhs_end_ptr;\n-        let rhs_end_ptr;\n-        if ty::type_has_dynamic_size(bcx_tcx(cx), unit_ty) {\n-            lhs_end_ptr = copy_cx.build.InBoundsGEP(lhs_data, [lhs_len]);\n-            rhs_end_ptr = copy_cx.build.InBoundsGEP(rhs_data, [rhs_len]);\n-        } else {\n-            let lhs_len_unscaled = copy_cx.build.UDiv(lhs_len, unit_sz);\n-            lhs_end_ptr =\n-                copy_cx.build.InBoundsGEP(lhs_data, [lhs_len_unscaled]);\n-            let rhs_len_unscaled = copy_cx.build.UDiv(rhs_len, unit_sz);\n-            rhs_end_ptr =\n-                copy_cx.build.InBoundsGEP(rhs_data, [rhs_len_unscaled]);\n-        }\n-\n-        let dest_ptr_ptr = alloca(copy_cx, T_ptr(llunitty));\n-        copy_cx.build.Store(first_dest_ptr, dest_ptr_ptr);\n-        let lhs_ptr_ptr = alloca(copy_cx, T_ptr(llunitty));\n-        copy_cx.build.Store(lhs_data, lhs_ptr_ptr);\n-        let rhs_ptr_ptr = alloca(copy_cx, T_ptr(llunitty));\n-        copy_cx.build.Store(rhs_data, rhs_ptr_ptr);\n-        let lhs_copy_cx = new_sub_block_ctxt(bcx, \"lhs_copy\");\n-        copy_cx.build.Br(lhs_copy_cx.llbb);\n-        // Copy in elements from the LHS.\n-\n-        let lhs_ptr = lhs_copy_cx.build.Load(lhs_ptr_ptr);\n-        let not_at_end_lhs =\n-            lhs_copy_cx.build.ICmp(lib::llvm::LLVMIntNE, lhs_ptr,\n-                                   lhs_end_ptr);\n-        let lhs_do_copy_cx = new_sub_block_ctxt(bcx, \"lhs_do_copy\");\n-        let rhs_copy_cx = new_sub_block_ctxt(bcx, \"rhs_copy\");\n-        lhs_copy_cx.build.CondBr(not_at_end_lhs, lhs_do_copy_cx.llbb,\n-                                 rhs_copy_cx.llbb);\n-        let dest_ptr_lhs_copy = lhs_do_copy_cx.build.Load(dest_ptr_ptr);\n-        let lhs_val = load_if_immediate(lhs_do_copy_cx, lhs_ptr, unit_ty);\n-        lhs_do_copy_cx = copy_val(lhs_do_copy_cx, INIT, dest_ptr_lhs_copy,\n-                                  lhs_val, unit_ty);\n-\n-        // Increment both pointers.\n-        if ty::type_has_dynamic_size(bcx_tcx(cx), unit_ty) {\n-            // We have to increment by the dynamically-computed size.\n-            incr_ptr(lhs_do_copy_cx, dest_ptr_lhs_copy, unit_sz,\n-                     dest_ptr_ptr);\n-            incr_ptr(lhs_do_copy_cx, lhs_ptr, unit_sz, lhs_ptr_ptr);\n-        } else {\n-            incr_ptr(lhs_do_copy_cx, dest_ptr_lhs_copy, C_int(1),\n-                     dest_ptr_ptr);\n-            incr_ptr(lhs_do_copy_cx, lhs_ptr, C_int(1), lhs_ptr_ptr);\n-        }\n-\n-        lhs_do_copy_cx.build.Br(lhs_copy_cx.llbb);\n-        // Copy in elements from the RHS.\n-\n-        let rhs_ptr = rhs_copy_cx.build.Load(rhs_ptr_ptr);\n-        let not_at_end_rhs =\n-            rhs_copy_cx.build.ICmp(lib::llvm::LLVMIntNE, rhs_ptr,\n-                                   rhs_end_ptr);\n-        let rhs_do_copy_cx = new_sub_block_ctxt(bcx, \"rhs_do_copy\");\n-        rhs_copy_cx.build.CondBr(not_at_end_rhs, rhs_do_copy_cx.llbb,\n-                                 next_cx.llbb);\n-        let dest_ptr_rhs_copy = rhs_do_copy_cx.build.Load(dest_ptr_ptr);\n-        let rhs_val = load_if_immediate(rhs_do_copy_cx, rhs_ptr, unit_ty);\n-        rhs_do_copy_cx = copy_val(rhs_do_copy_cx, INIT, dest_ptr_rhs_copy,\n-                                  rhs_val, unit_ty);\n-\n-        // Increment both pointers.\n-        if ty::type_has_dynamic_size(bcx_tcx(cx), unit_ty) {\n-            // We have to increment by the dynamically-computed size.\n-            incr_ptr(rhs_do_copy_cx, dest_ptr_rhs_copy, unit_sz,\n-                     dest_ptr_ptr);\n-            incr_ptr(rhs_do_copy_cx, rhs_ptr, unit_sz, rhs_ptr_ptr);\n-        } else {\n-            incr_ptr(rhs_do_copy_cx, dest_ptr_rhs_copy, C_int(1),\n-                     dest_ptr_ptr);\n-            incr_ptr(rhs_do_copy_cx, rhs_ptr, C_int(1), rhs_ptr_ptr);\n-        }\n-\n-        rhs_do_copy_cx.build.Br(rhs_copy_cx.llbb);\n-        // Finally done!\n-\n-        ret rslt(next_cx, llvecptr);\n-    }\n-\n-    // NB: This does *not* adjust reference counts. The caller must have done\n-    // this via take_ty() beforehand.\n-    fn duplicate_heap_part(cx: &@block_ctxt, orig_vptr: ValueRef,\n-                           unit_ty: ty::t) -> result {\n-        // Cast to an opaque interior vector if we can't trust the pointer\n-        // type.\n-        let vptr;\n-        if ty::type_has_dynamic_size(bcx_tcx(cx), unit_ty) {\n-            vptr = cx.build.PointerCast(orig_vptr, T_ptr(T_opaque_ivec()));\n-        } else { vptr = orig_vptr; }\n-\n-        let llunitty = type_of_or_i8(cx, unit_ty);\n-        let llheappartty = T_ivec_heap_part(llunitty);\n-\n-        // Check to see if the vector is heapified.\n-        let stack_len_ptr =\n-            cx.build.InBoundsGEP(vptr, [C_int(0), C_uint(abi::ivec_elt_len)]);\n-        let stack_len = cx.build.Load(stack_len_ptr);\n-        let stack_len_is_zero =\n-            cx.build.ICmp(lib::llvm::LLVMIntEQ, stack_len, C_int(0));\n-        let maybe_on_heap_cx = new_sub_block_ctxt(cx, \"maybe_on_heap\");\n-        let next_cx = new_sub_block_ctxt(cx, \"next\");\n-        cx.build.CondBr(stack_len_is_zero, maybe_on_heap_cx.llbb,\n-                        next_cx.llbb);\n-\n-        let stub_ptr =\n-            maybe_on_heap_cx.build.PointerCast(vptr,\n-                                               T_ptr(T_ivec_heap(llunitty)));\n-        let heap_ptr_ptr =\n-            maybe_on_heap_cx.build.InBoundsGEP(\n-                stub_ptr,\n-                [C_int(0),\n-                 C_uint(abi::ivec_heap_stub_elt_ptr)]);\n-        let heap_ptr = maybe_on_heap_cx.build.Load(heap_ptr_ptr);\n-        let heap_ptr_is_nonnull =\n-            maybe_on_heap_cx.build.ICmp(lib::llvm::LLVMIntNE, heap_ptr,\n-                                        C_null(T_ptr(llheappartty)));\n-        let on_heap_cx = new_sub_block_ctxt(cx, \"on_heap\");\n-        maybe_on_heap_cx.build.CondBr(heap_ptr_is_nonnull, on_heap_cx.llbb,\n-                                      next_cx.llbb);\n-\n-        // Ok, the vector is on the heap. Copy the heap part.\n-        let alen_ptr =\n-            on_heap_cx.build.InBoundsGEP(\n-                stub_ptr,\n-                [C_int(0),\n-                 C_uint(abi::ivec_heap_stub_elt_alen)]);\n-        let alen = on_heap_cx.build.Load(alen_ptr);\n-\n-        let heap_part_sz =\n-            on_heap_cx.build.Add(alen, llsize_of(T_opaque_ivec_heap_part()));\n-        let rs =\n-            trans_shared_malloc(on_heap_cx, T_ptr(llheappartty),\n-                                heap_part_sz);\n-        on_heap_cx = rs.bcx;\n-        let new_heap_ptr = rs.val;\n-\n-        rs = call_memmove(on_heap_cx, new_heap_ptr, heap_ptr, heap_part_sz);\n-        on_heap_cx = rs.bcx;\n-\n-        on_heap_cx.build.Store(new_heap_ptr, heap_ptr_ptr);\n-        on_heap_cx.build.Br(next_cx.llbb);\n-\n-        ret rslt(next_cx, C_nil());\n-    }\n-}\n-\n fn trans_evec_add(cx: &@block_ctxt, t: ty::t, lhs: ValueRef, rhs: ValueRef)\n    -> result {\n     let r = alloc_ty(cx, t);"}, {"sha": "c9cc35ac9a9d0534a7ffbc62e891bf9525459857", "filename": "src/comp/middle/trans_ivec.rs", "status": "added", "additions": 703, "deletions": 0, "changes": 703, "blob_url": "https://github.com/rust-lang/rust/blob/309a7534d7ebf39f5a62bac36749d35272fe0a3e/src%2Fcomp%2Fmiddle%2Ftrans_ivec.rs", "raw_url": "https://github.com/rust-lang/rust/raw/309a7534d7ebf39f5a62bac36749d35272fe0a3e/src%2Fcomp%2Fmiddle%2Ftrans_ivec.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Fcomp%2Fmiddle%2Ftrans_ivec.rs?ref=309a7534d7ebf39f5a62bac36749d35272fe0a3e", "patch": "@@ -0,0 +1,703 @@\n+import std::option::none;\n+import syntax::ast;\n+import lib::llvm::llvm::{ValueRef, TypeRef};\n+import back::abi;\n+import trans::{call_memmove, trans_shared_malloc, llsize_of,\n+               type_of_or_i8, incr_ptr, INIT, copy_val, load_if_immediate,\n+               alloca, array_alloca, size_of, llderivedtydescs_block_ctxt,\n+               lazily_emit_tydesc_glue, get_tydesc, load_inbounds,\n+               move_val_if_temp, trans_lval, node_id_type,\n+               new_sub_block_ctxt};\n+import trans_common::*;\n+\n+export trans_ivec, get_len_and_data, duplicate_heap_part, trans_add,\n+trans_append;\n+\n+fn trans_ivec(bcx: @block_ctxt, args: &[@ast::expr], id: ast::node_id) ->\n+    result {\n+    let typ = node_id_type(bcx_ccx(bcx), id);\n+    let unit_ty;\n+    alt ty::struct(bcx_tcx(bcx), typ) {\n+      ty::ty_vec(mt) { unit_ty = mt.ty; }\n+      _ { bcx_ccx(bcx).sess.bug(\"non-ivec type in trans_ivec\"); }\n+    }\n+    let llunitty = type_of_or_i8(bcx, unit_ty);\n+\n+    let ares = alloc(bcx, unit_ty);\n+    bcx = ares.bcx;\n+    let llvecptr = ares.llptr;\n+    let unit_sz = ares.llunitsz;\n+    let llalen = ares.llalen;\n+\n+    add_clean_temp(bcx, llvecptr, typ);\n+\n+    let lllen = bcx.build.Mul(C_uint(std::vec::len(args)), unit_sz);\n+    // Allocate the vector pieces and store length and allocated length.\n+\n+    let llfirsteltptr;\n+    if std::vec::len(args) > 0u &&\n+        std::vec::len(args) <= abi::ivec_default_length {\n+        // Interior case.\n+\n+        bcx.build.Store(lllen,\n+                        bcx.build.InBoundsGEP(llvecptr,\n+                                              [C_int(0),\n+                                               C_uint(abi::ivec_elt_len)]));\n+        bcx.build.Store(llalen,\n+                        bcx.build.InBoundsGEP(llvecptr,\n+                                              [C_int(0),\n+                                               C_uint(abi::ivec_elt_alen)]));\n+        llfirsteltptr =\n+            bcx.build.InBoundsGEP(llvecptr,\n+                                  [C_int(0), C_uint(abi::ivec_elt_elems),\n+                                   C_int(0)]);\n+    } else {\n+        // Heap case.\n+\n+        let stub_z = [C_int(0), C_uint(abi::ivec_heap_stub_elt_zero)];\n+        let stub_a = [C_int(0), C_uint(abi::ivec_heap_stub_elt_alen)];\n+        let stub_p = [C_int(0), C_uint(abi::ivec_heap_stub_elt_ptr)];\n+        let llstubty = T_ivec_heap(llunitty);\n+        let llstubptr = bcx.build.PointerCast(llvecptr, T_ptr(llstubty));\n+        bcx.build.Store(C_int(0), bcx.build.InBoundsGEP(llstubptr, stub_z));\n+        let llheapty = T_ivec_heap_part(llunitty);\n+        if std::vec::len(args) == 0u {\n+            // Null heap pointer indicates a zero-length vector.\n+\n+            bcx.build.Store(llalen, bcx.build.InBoundsGEP(llstubptr, stub_a));\n+            bcx.build.Store(C_null(T_ptr(llheapty)),\n+                            bcx.build.InBoundsGEP(llstubptr, stub_p));\n+            llfirsteltptr = C_null(T_ptr(llunitty));\n+        } else {\n+            bcx.build.Store(lllen, bcx.build.InBoundsGEP(llstubptr, stub_a));\n+\n+            let llheapsz = bcx.build.Add(llsize_of(llheapty), lllen);\n+            let rslt = trans_shared_malloc(bcx, T_ptr(llheapty), llheapsz);\n+            bcx = rslt.bcx;\n+            let llheapptr = rslt.val;\n+            bcx.build.Store(llheapptr,\n+                            bcx.build.InBoundsGEP(llstubptr, stub_p));\n+            let heap_l = [C_int(0), C_uint(abi::ivec_heap_elt_len)];\n+            bcx.build.Store(lllen, bcx.build.InBoundsGEP(llheapptr, heap_l));\n+            llfirsteltptr =\n+                bcx.build.InBoundsGEP(llheapptr,\n+                                      [C_int(0),\n+                                       C_uint(abi::ivec_heap_elt_elems),\n+                                       C_int(0)]);\n+        }\n+    }\n+    // Store the individual elements.\n+\n+    let i = 0u;\n+    for e: @ast::expr in args {\n+        let lv = trans_lval(bcx, e);\n+        bcx = lv.res.bcx;\n+        let lleltptr;\n+        if ty::type_has_dynamic_size(bcx_tcx(bcx), unit_ty) {\n+            lleltptr =\n+                bcx.build.InBoundsGEP(llfirsteltptr,\n+                                      [bcx.build.Mul(C_uint(i), unit_sz)]);\n+        } else {\n+            lleltptr = bcx.build.InBoundsGEP(llfirsteltptr, [C_uint(i)]);\n+        }\n+        bcx = move_val_if_temp(bcx, INIT, lleltptr, lv, unit_ty);\n+        i += 1u;\n+    }\n+    ret rslt(bcx, llvecptr);\n+}\n+\n+// Returns the length of an interior vector and a pointer to its first\n+// element, in that order.\n+fn get_len_and_data(bcx: &@block_ctxt, orig_v: ValueRef, unit_ty: ty::t)\n+    -> {len: ValueRef, data: ValueRef, bcx: @block_ctxt} {\n+    // If this interior vector has dynamic size, we can't assume anything\n+    // about the LLVM type of the value passed in, so we cast it to an\n+    // opaque vector type.\n+    let v;\n+    if ty::type_has_dynamic_size(bcx_tcx(bcx), unit_ty) {\n+        v = bcx.build.PointerCast(orig_v, T_ptr(T_opaque_ivec()));\n+    } else { v = orig_v; }\n+\n+    let llunitty = type_of_or_i8(bcx, unit_ty);\n+    let stack_len =\n+        load_inbounds(bcx, v, [C_int(0), C_uint(abi::ivec_elt_len)]);\n+    let stack_elem =\n+        bcx.build.InBoundsGEP(v,\n+                              [C_int(0), C_uint(abi::ivec_elt_elems),\n+                               C_int(0)]);\n+    let on_heap =\n+        bcx.build.ICmp(lib::llvm::LLVMIntEQ, stack_len, C_int(0));\n+    let on_heap_cx = new_sub_block_ctxt(bcx, \"on_heap\");\n+    let next_cx = new_sub_block_ctxt(bcx, \"next\");\n+    bcx.build.CondBr(on_heap, on_heap_cx.llbb, next_cx.llbb);\n+    let heap_stub =\n+        on_heap_cx.build.PointerCast(v, T_ptr(T_ivec_heap(llunitty)));\n+    let heap_ptr =\n+        load_inbounds(on_heap_cx, heap_stub,\n+                      [C_int(0), C_uint(abi::ivec_heap_stub_elt_ptr)]);\n+\n+    // Check whether the heap pointer is null. If it is, the vector length\n+    // is truly zero.\n+\n+    let llstubty = T_ivec_heap(llunitty);\n+    let llheapptrty = struct_elt(llstubty, abi::ivec_heap_stub_elt_ptr);\n+    let heap_ptr_is_null =\n+        on_heap_cx.build.ICmp(lib::llvm::LLVMIntEQ, heap_ptr,\n+                              C_null(T_ptr(llheapptrty)));\n+    let zero_len_cx = new_sub_block_ctxt(bcx, \"zero_len\");\n+    let nonzero_len_cx = new_sub_block_ctxt(bcx, \"nonzero_len\");\n+    on_heap_cx.build.CondBr(heap_ptr_is_null, zero_len_cx.llbb,\n+                            nonzero_len_cx.llbb);\n+    // Technically this context is unnecessary, but it makes this function\n+    // clearer.\n+\n+    let zero_len = C_int(0);\n+    let zero_elem = C_null(T_ptr(llunitty));\n+    zero_len_cx.build.Br(next_cx.llbb);\n+    // If we're here, then we actually have a heapified vector.\n+\n+    let heap_len =\n+        load_inbounds(nonzero_len_cx, heap_ptr,\n+                      [C_int(0), C_uint(abi::ivec_heap_elt_len)]);\n+    let heap_elem =\n+        {\n+        let v =\n+            [C_int(0), C_uint(abi::ivec_heap_elt_elems), C_int(0)];\n+        nonzero_len_cx.build.InBoundsGEP(heap_ptr, v)\n+    };\n+\n+    nonzero_len_cx.build.Br(next_cx.llbb);\n+    // Now we can figure out the length of `v` and get a pointer to its\n+    // first element.\n+\n+    let len =\n+        next_cx.build.Phi(T_int(), [stack_len, zero_len, heap_len],\n+                          [bcx.llbb, zero_len_cx.llbb,\n+                           nonzero_len_cx.llbb]);\n+    let elem =\n+        next_cx.build.Phi(T_ptr(llunitty),\n+                          [stack_elem, zero_elem, heap_elem],\n+                          [bcx.llbb, zero_len_cx.llbb,\n+                           nonzero_len_cx.llbb]);\n+    ret {len: len, data: elem, bcx: next_cx};\n+}\n+\n+// Returns a tuple consisting of a pointer to the newly-reserved space and\n+// a block context. Updates the length appropriately.\n+fn reserve_space(cx: &@block_ctxt, llunitty: TypeRef, v: ValueRef,\n+                 len_needed: ValueRef) -> result {\n+    let stack_len_ptr =\n+        cx.build.InBoundsGEP(v, [C_int(0), C_uint(abi::ivec_elt_len)]);\n+    let stack_len = cx.build.Load(stack_len_ptr);\n+    let alen =\n+        load_inbounds(cx, v, [C_int(0), C_uint(abi::ivec_elt_alen)]);\n+    // There are four cases we have to consider:\n+    // (1) On heap, no resize necessary.\n+    // (2) On heap, need to resize.\n+    // (3) On stack, no resize necessary.\n+    // (4) On stack, need to spill to heap.\n+\n+    let maybe_on_heap =\n+        cx.build.ICmp(lib::llvm::LLVMIntEQ, stack_len, C_int(0));\n+    let maybe_on_heap_cx = new_sub_block_ctxt(cx, \"maybe_on_heap\");\n+    let on_stack_cx = new_sub_block_ctxt(cx, \"on_stack\");\n+    cx.build.CondBr(maybe_on_heap, maybe_on_heap_cx.llbb,\n+                    on_stack_cx.llbb);\n+    let next_cx = new_sub_block_ctxt(cx, \"next\");\n+    // We're possibly on the heap, unless the vector is zero-length.\n+\n+    let stub_p = [C_int(0), C_uint(abi::ivec_heap_stub_elt_ptr)];\n+    let stub_ptr =\n+        maybe_on_heap_cx.build.PointerCast(v,\n+                                           T_ptr(T_ivec_heap(llunitty)));\n+    let heap_ptr = load_inbounds(maybe_on_heap_cx, stub_ptr, stub_p);\n+    let on_heap =\n+        maybe_on_heap_cx.build.ICmp(lib::llvm::LLVMIntNE, heap_ptr,\n+                                    C_null(val_ty(heap_ptr)));\n+    let on_heap_cx = new_sub_block_ctxt(cx, \"on_heap\");\n+    maybe_on_heap_cx.build.CondBr(on_heap, on_heap_cx.llbb,\n+                                  on_stack_cx.llbb);\n+    // We're definitely on the heap. Check whether we need to resize.\n+\n+    let heap_len_ptr =\n+        on_heap_cx.build.InBoundsGEP(heap_ptr,\n+                                     [C_int(0),\n+                                      C_uint(abi::ivec_heap_elt_len)]);\n+    let heap_len = on_heap_cx.build.Load(heap_len_ptr);\n+    let new_heap_len = on_heap_cx.build.Add(heap_len, len_needed);\n+    let heap_len_unscaled =\n+        on_heap_cx.build.UDiv(heap_len, llsize_of(llunitty));\n+    let heap_no_resize_needed =\n+        on_heap_cx.build.ICmp(lib::llvm::LLVMIntULE, new_heap_len, alen);\n+    let heap_no_resize_cx = new_sub_block_ctxt(cx, \"heap_no_resize\");\n+    let heap_resize_cx = new_sub_block_ctxt(cx, \"heap_resize\");\n+    on_heap_cx.build.CondBr(heap_no_resize_needed, heap_no_resize_cx.llbb,\n+                            heap_resize_cx.llbb);\n+    // Case (1): We're on the heap and don't need to resize.\n+\n+    let heap_data_no_resize =\n+        {\n+        let v =\n+            [C_int(0), C_uint(abi::ivec_heap_elt_elems),\n+             heap_len_unscaled];\n+        heap_no_resize_cx.build.InBoundsGEP(heap_ptr, v)\n+    };\n+    heap_no_resize_cx.build.Store(new_heap_len, heap_len_ptr);\n+    heap_no_resize_cx.build.Br(next_cx.llbb);\n+    // Case (2): We're on the heap and need to resize. This path is rare,\n+    // so we delegate to cold glue.\n+\n+    {\n+        let p =\n+            heap_resize_cx.build.PointerCast(v, T_ptr(T_opaque_ivec()));\n+        let upcall = bcx_ccx(cx).upcalls.ivec_resize_shared;\n+        heap_resize_cx.build.Call(upcall,\n+                                  [cx.fcx.lltaskptr, p, new_heap_len]);\n+    }\n+    let heap_ptr_resize = load_inbounds(heap_resize_cx, stub_ptr, stub_p);\n+\n+    let heap_data_resize =\n+        {\n+        let v =\n+            [C_int(0), C_uint(abi::ivec_heap_elt_elems),\n+             heap_len_unscaled];\n+        heap_resize_cx.build.InBoundsGEP(heap_ptr_resize, v)\n+    };\n+    heap_resize_cx.build.Br(next_cx.llbb);\n+    // We're on the stack. Check whether we need to spill to the heap.\n+\n+    let new_stack_len = on_stack_cx.build.Add(stack_len, len_needed);\n+    let stack_no_spill_needed =\n+        on_stack_cx.build.ICmp(lib::llvm::LLVMIntULE, new_stack_len,\n+                               alen);\n+    let stack_len_unscaled =\n+        on_stack_cx.build.UDiv(stack_len, llsize_of(llunitty));\n+    let stack_no_spill_cx = new_sub_block_ctxt(cx, \"stack_no_spill\");\n+    let stack_spill_cx = new_sub_block_ctxt(cx, \"stack_spill\");\n+    on_stack_cx.build.CondBr(stack_no_spill_needed,\n+                             stack_no_spill_cx.llbb, stack_spill_cx.llbb);\n+    // Case (3): We're on the stack and don't need to spill.\n+\n+    let stack_data_no_spill =\n+        stack_no_spill_cx.build.InBoundsGEP(v,\n+                                            [C_int(0),\n+                                             C_uint(abi::ivec_elt_elems),\n+                                             stack_len_unscaled]);\n+    stack_no_spill_cx.build.Store(new_stack_len, stack_len_ptr);\n+    stack_no_spill_cx.build.Br(next_cx.llbb);\n+    // Case (4): We're on the stack and need to spill. Like case (2), this\n+    // path is rare, so we delegate to cold glue.\n+\n+    {\n+        let p =\n+            stack_spill_cx.build.PointerCast(v, T_ptr(T_opaque_ivec()));\n+        let upcall = bcx_ccx(cx).upcalls.ivec_spill_shared;\n+        stack_spill_cx.build.Call(upcall,\n+                                  [cx.fcx.lltaskptr, p, new_stack_len]);\n+    }\n+    let spill_stub =\n+        stack_spill_cx.build.PointerCast(v, T_ptr(T_ivec_heap(llunitty)));\n+\n+    let heap_ptr_spill =\n+        load_inbounds(stack_spill_cx, spill_stub, stub_p);\n+\n+    let heap_data_spill =\n+        {\n+        let v =\n+            [C_int(0), C_uint(abi::ivec_heap_elt_elems),\n+             stack_len_unscaled];\n+        stack_spill_cx.build.InBoundsGEP(heap_ptr_spill, v)\n+    };\n+    stack_spill_cx.build.Br(next_cx.llbb);\n+    // Phi together the different data pointers to get the result.\n+\n+    let data_ptr =\n+        next_cx.build.Phi(T_ptr(llunitty),\n+                          [heap_data_no_resize, heap_data_resize,\n+                           stack_data_no_spill, heap_data_spill],\n+                          [heap_no_resize_cx.llbb, heap_resize_cx.llbb,\n+                           stack_no_spill_cx.llbb, stack_spill_cx.llbb]);\n+    ret rslt(next_cx, data_ptr);\n+}\n+fn trans_append(cx: &@block_ctxt, t: ty::t, orig_lhs: ValueRef,\n+                orig_rhs: ValueRef) -> result {\n+    // Cast to opaque interior vector types if necessary.\n+    let lhs;\n+    let rhs;\n+    if ty::type_has_dynamic_size(bcx_tcx(cx), t) {\n+        lhs = cx.build.PointerCast(orig_lhs, T_ptr(T_opaque_ivec()));\n+        rhs = cx.build.PointerCast(orig_rhs, T_ptr(T_opaque_ivec()));\n+    } else { lhs = orig_lhs; rhs = orig_rhs; }\n+\n+    let unit_ty = ty::sequence_element_type(bcx_tcx(cx), t);\n+    let llunitty = type_of_or_i8(cx, unit_ty);\n+    alt ty::struct(bcx_tcx(cx), t) {\n+      ty::ty_istr. { }\n+      ty::ty_vec(_) { }\n+      _ { bcx_tcx(cx).sess.bug(\"non-istr/ivec in trans_append\"); }\n+    }\n+\n+    let rs = size_of(cx, unit_ty);\n+    let bcx = rs.bcx;\n+    let unit_sz = rs.val;\n+\n+    // Gather the various type descriptors we'll need.\n+\n+    // FIXME (issue #511): This is needed to prevent a leak.\n+    let no_tydesc_info = none;\n+\n+    rs = get_tydesc(bcx, t, false, no_tydesc_info).result;\n+    bcx = rs.bcx;\n+    rs = get_tydesc(bcx, unit_ty, false, no_tydesc_info).result;\n+    bcx = rs.bcx;\n+    lazily_emit_tydesc_glue(bcx, abi::tydesc_field_take_glue, none);\n+    lazily_emit_tydesc_glue(bcx, abi::tydesc_field_drop_glue, none);\n+    lazily_emit_tydesc_glue(bcx, abi::tydesc_field_free_glue, none);\n+    lazily_emit_tydesc_glue(bcx, abi::tydesc_field_copy_glue, none);\n+    let rhs_len_and_data = get_len_and_data(bcx, rhs, unit_ty);\n+    let rhs_len = rhs_len_and_data.len;\n+    let rhs_data = rhs_len_and_data.data;\n+    bcx = rhs_len_and_data.bcx;\n+    rs = reserve_space(bcx, llunitty, lhs, rhs_len);\n+    let lhs_data = rs.val;\n+    bcx = rs.bcx;\n+    // Work out the end pointer.\n+\n+    let lhs_unscaled_idx = bcx.build.UDiv(rhs_len, llsize_of(llunitty));\n+    let lhs_end = bcx.build.InBoundsGEP(lhs_data, [lhs_unscaled_idx]);\n+    // Now emit the copy loop.\n+\n+    let dest_ptr = alloca(bcx, T_ptr(llunitty));\n+    bcx.build.Store(lhs_data, dest_ptr);\n+    let src_ptr = alloca(bcx, T_ptr(llunitty));\n+    bcx.build.Store(rhs_data, src_ptr);\n+    let copy_loop_header_cx = new_sub_block_ctxt(bcx, \"copy_loop_header\");\n+    bcx.build.Br(copy_loop_header_cx.llbb);\n+    let copy_dest_ptr = copy_loop_header_cx.build.Load(dest_ptr);\n+    let not_yet_at_end =\n+        copy_loop_header_cx.build.ICmp(lib::llvm::LLVMIntNE,\n+                                       copy_dest_ptr, lhs_end);\n+    let copy_loop_body_cx = new_sub_block_ctxt(bcx, \"copy_loop_body\");\n+    let next_cx = new_sub_block_ctxt(bcx, \"next\");\n+    copy_loop_header_cx.build.CondBr(not_yet_at_end,\n+                                     copy_loop_body_cx.llbb,\n+                                     next_cx.llbb);\n+\n+    let copy_src_ptr = copy_loop_body_cx.build.Load(src_ptr);\n+    let copy_src =\n+        load_if_immediate(copy_loop_body_cx, copy_src_ptr, unit_ty);\n+\n+    let post_copy_cx = copy_val\n+        (copy_loop_body_cx, INIT, copy_dest_ptr, copy_src, unit_ty);\n+    // Increment both pointers.\n+    if ty::type_has_dynamic_size(bcx_tcx(cx), t) {\n+        // We have to increment by the dynamically-computed size.\n+        incr_ptr(post_copy_cx, copy_dest_ptr, unit_sz, dest_ptr);\n+        incr_ptr(post_copy_cx, copy_src_ptr, unit_sz, src_ptr);\n+    } else {\n+        incr_ptr(post_copy_cx, copy_dest_ptr, C_int(1), dest_ptr);\n+        incr_ptr(post_copy_cx, copy_src_ptr, C_int(1), src_ptr);\n+    }\n+\n+    post_copy_cx.build.Br(copy_loop_header_cx.llbb);\n+    ret rslt(next_cx, C_nil());\n+}\n+\n+type alloc_result =\n+    {bcx: @block_ctxt,\n+     llptr: ValueRef,\n+     llunitsz: ValueRef,\n+     llalen: ValueRef};\n+\n+fn alloc(cx: &@block_ctxt, unit_ty: ty::t) -> alloc_result {\n+    let dynamic = ty::type_has_dynamic_size(bcx_tcx(cx), unit_ty);\n+\n+    let bcx;\n+    if dynamic {\n+        bcx = llderivedtydescs_block_ctxt(cx.fcx);\n+    } else { bcx = cx; }\n+\n+    let llunitsz;\n+    let rslt = size_of(bcx, unit_ty);\n+    bcx = rslt.bcx;\n+    llunitsz = rslt.val;\n+\n+    if dynamic { cx.fcx.llderivedtydescs = bcx.llbb; }\n+\n+    let llalen =\n+        bcx.build.Mul(llunitsz, C_uint(abi::ivec_default_length));\n+\n+    let llptr;\n+    let llunitty = type_of_or_i8(bcx, unit_ty);\n+    let bcx_result;\n+    if dynamic {\n+        let llarraysz = bcx.build.Add(llsize_of(T_opaque_ivec()), llalen);\n+        let llvecptr = array_alloca(bcx, T_i8(), llarraysz);\n+\n+        bcx_result = cx;\n+        llptr =\n+            bcx_result.build.PointerCast(llvecptr,\n+                                         T_ptr(T_opaque_ivec()));\n+    } else { llptr = alloca(bcx, T_ivec(llunitty)); bcx_result = bcx; }\n+\n+    ret {bcx: bcx_result,\n+         llptr: llptr,\n+         llunitsz: llunitsz,\n+         llalen: llalen};\n+}\n+\n+fn trans_add(cx: &@block_ctxt, vec_ty: ty::t, lhs: ValueRef,\n+             rhs: ValueRef) -> result {\n+    let bcx = cx;\n+    let unit_ty = ty::sequence_element_type(bcx_tcx(bcx), vec_ty);\n+\n+    let ares = alloc(bcx, unit_ty);\n+    bcx = ares.bcx;\n+    let llvecptr = ares.llptr;\n+    let unit_sz = ares.llunitsz;\n+    let llalen = ares.llalen;\n+\n+    add_clean_temp(bcx, llvecptr, vec_ty);\n+\n+    let llunitty = type_of_or_i8(bcx, unit_ty);\n+    let llheappartty = T_ivec_heap_part(llunitty);\n+    let lhs_len_and_data = get_len_and_data(bcx, lhs, unit_ty);\n+    let lhs_len = lhs_len_and_data.len;\n+    let lhs_data = lhs_len_and_data.data;\n+    bcx = lhs_len_and_data.bcx;\n+    let rhs_len_and_data = get_len_and_data(bcx, rhs, unit_ty);\n+    let rhs_len = rhs_len_and_data.len;\n+    let rhs_data = rhs_len_and_data.data;\n+    bcx = rhs_len_and_data.bcx;\n+    let lllen = bcx.build.Add(lhs_len, rhs_len);\n+    // We have three cases to handle here:\n+    // (1) Length is zero ([] + []).\n+    // (2) Copy onto stack.\n+    // (3) Allocate on heap and copy there.\n+\n+    let len_is_zero =\n+        bcx.build.ICmp(lib::llvm::LLVMIntEQ, lllen, C_int(0));\n+    let zero_len_cx = new_sub_block_ctxt(bcx, \"zero_len\");\n+    let nonzero_len_cx = new_sub_block_ctxt(bcx, \"nonzero_len\");\n+    bcx.build.CondBr(len_is_zero, zero_len_cx.llbb, nonzero_len_cx.llbb);\n+    // Case (1): Length is zero.\n+\n+    let stub_z = [C_int(0), C_uint(abi::ivec_heap_stub_elt_zero)];\n+    let stub_a = [C_int(0), C_uint(abi::ivec_heap_stub_elt_alen)];\n+    let stub_p = [C_int(0), C_uint(abi::ivec_heap_stub_elt_ptr)];\n+\n+    let vec_l = [C_int(0), C_uint(abi::ivec_elt_len)];\n+    let vec_a = [C_int(0), C_uint(abi::ivec_elt_alen)];\n+\n+    let stub_ptr_zero =\n+        zero_len_cx.build.PointerCast(llvecptr,\n+                                      T_ptr(T_ivec_heap(llunitty)));\n+    zero_len_cx.build.Store(C_int(0),\n+                            zero_len_cx.build.InBoundsGEP(stub_ptr_zero,\n+                                                          stub_z));\n+    zero_len_cx.build.Store(llalen,\n+                            zero_len_cx.build.InBoundsGEP(stub_ptr_zero,\n+                                                          stub_a));\n+    zero_len_cx.build.Store(C_null(T_ptr(llheappartty)),\n+                            zero_len_cx.build.InBoundsGEP(stub_ptr_zero,\n+                                                          stub_p));\n+    let next_cx = new_sub_block_ctxt(bcx, \"next\");\n+    zero_len_cx.build.Br(next_cx.llbb);\n+    // Determine whether we need to spill to the heap.\n+\n+    let on_stack =\n+        nonzero_len_cx.build.ICmp(lib::llvm::LLVMIntULE, lllen, llalen);\n+    let stack_cx = new_sub_block_ctxt(bcx, \"stack\");\n+    let heap_cx = new_sub_block_ctxt(bcx, \"heap\");\n+    nonzero_len_cx.build.CondBr(on_stack, stack_cx.llbb, heap_cx.llbb);\n+    // Case (2): Copy onto stack.\n+\n+    stack_cx.build.Store(lllen,\n+                         stack_cx.build.InBoundsGEP(llvecptr, vec_l));\n+    stack_cx.build.Store(llalen,\n+                         stack_cx.build.InBoundsGEP(llvecptr, vec_a));\n+    let dest_ptr_stack =\n+        stack_cx.build.InBoundsGEP(llvecptr,\n+                                   [C_int(0), C_uint(abi::ivec_elt_elems),\n+                                    C_int(0)]);\n+    let copy_cx = new_sub_block_ctxt(bcx, \"copy\");\n+    stack_cx.build.Br(copy_cx.llbb);\n+    // Case (3): Allocate on heap and copy there.\n+\n+    let stub_ptr_heap =\n+        heap_cx.build.PointerCast(llvecptr, T_ptr(T_ivec_heap(llunitty)));\n+    heap_cx.build.Store(C_int(0),\n+                        heap_cx.build.InBoundsGEP(stub_ptr_heap, stub_z));\n+    heap_cx.build.Store(lllen,\n+                        heap_cx.build.InBoundsGEP(stub_ptr_heap, stub_a));\n+    let heap_sz = heap_cx.build.Add(llsize_of(llheappartty), lllen);\n+    let rs = trans_shared_malloc(heap_cx, T_ptr(llheappartty), heap_sz);\n+    let heap_part = rs.val;\n+    heap_cx = rs.bcx;\n+    heap_cx.build.Store(heap_part,\n+                        heap_cx.build.InBoundsGEP(stub_ptr_heap, stub_p));\n+    {\n+        let v = [C_int(0), C_uint(abi::ivec_heap_elt_len)];\n+        heap_cx.build.Store(lllen,\n+                            heap_cx.build.InBoundsGEP(heap_part, v));\n+    }\n+    let dest_ptr_heap =\n+        heap_cx.build.InBoundsGEP(heap_part,\n+                                  [C_int(0),\n+                                   C_uint(abi::ivec_heap_elt_elems),\n+                                   C_int(0)]);\n+    heap_cx.build.Br(copy_cx.llbb);\n+    // Emit the copy loop.\n+\n+    let first_dest_ptr =\n+        copy_cx.build.Phi(T_ptr(llunitty),\n+                          [dest_ptr_stack, dest_ptr_heap],\n+                          [stack_cx.llbb, heap_cx.llbb]);\n+\n+    let lhs_end_ptr;\n+    let rhs_end_ptr;\n+    if ty::type_has_dynamic_size(bcx_tcx(cx), unit_ty) {\n+        lhs_end_ptr = copy_cx.build.InBoundsGEP(lhs_data, [lhs_len]);\n+        rhs_end_ptr = copy_cx.build.InBoundsGEP(rhs_data, [rhs_len]);\n+    } else {\n+        let lhs_len_unscaled = copy_cx.build.UDiv(lhs_len, unit_sz);\n+        lhs_end_ptr =\n+            copy_cx.build.InBoundsGEP(lhs_data, [lhs_len_unscaled]);\n+        let rhs_len_unscaled = copy_cx.build.UDiv(rhs_len, unit_sz);\n+        rhs_end_ptr =\n+            copy_cx.build.InBoundsGEP(rhs_data, [rhs_len_unscaled]);\n+    }\n+\n+    let dest_ptr_ptr = alloca(copy_cx, T_ptr(llunitty));\n+    copy_cx.build.Store(first_dest_ptr, dest_ptr_ptr);\n+    let lhs_ptr_ptr = alloca(copy_cx, T_ptr(llunitty));\n+    copy_cx.build.Store(lhs_data, lhs_ptr_ptr);\n+    let rhs_ptr_ptr = alloca(copy_cx, T_ptr(llunitty));\n+    copy_cx.build.Store(rhs_data, rhs_ptr_ptr);\n+    let lhs_copy_cx = new_sub_block_ctxt(bcx, \"lhs_copy\");\n+    copy_cx.build.Br(lhs_copy_cx.llbb);\n+    // Copy in elements from the LHS.\n+\n+    let lhs_ptr = lhs_copy_cx.build.Load(lhs_ptr_ptr);\n+    let not_at_end_lhs =\n+        lhs_copy_cx.build.ICmp(lib::llvm::LLVMIntNE, lhs_ptr,\n+                               lhs_end_ptr);\n+    let lhs_do_copy_cx = new_sub_block_ctxt(bcx, \"lhs_do_copy\");\n+    let rhs_copy_cx = new_sub_block_ctxt(bcx, \"rhs_copy\");\n+    lhs_copy_cx.build.CondBr(not_at_end_lhs, lhs_do_copy_cx.llbb,\n+                             rhs_copy_cx.llbb);\n+    let dest_ptr_lhs_copy = lhs_do_copy_cx.build.Load(dest_ptr_ptr);\n+    let lhs_val = load_if_immediate(lhs_do_copy_cx, lhs_ptr, unit_ty);\n+    lhs_do_copy_cx = copy_val(lhs_do_copy_cx, INIT, dest_ptr_lhs_copy,\n+                              lhs_val, unit_ty);\n+\n+    // Increment both pointers.\n+    if ty::type_has_dynamic_size(bcx_tcx(cx), unit_ty) {\n+        // We have to increment by the dynamically-computed size.\n+        incr_ptr(lhs_do_copy_cx, dest_ptr_lhs_copy, unit_sz,\n+                 dest_ptr_ptr);\n+        incr_ptr(lhs_do_copy_cx, lhs_ptr, unit_sz, lhs_ptr_ptr);\n+    } else {\n+        incr_ptr(lhs_do_copy_cx, dest_ptr_lhs_copy, C_int(1),\n+                 dest_ptr_ptr);\n+        incr_ptr(lhs_do_copy_cx, lhs_ptr, C_int(1), lhs_ptr_ptr);\n+    }\n+\n+    lhs_do_copy_cx.build.Br(lhs_copy_cx.llbb);\n+    // Copy in elements from the RHS.\n+\n+    let rhs_ptr = rhs_copy_cx.build.Load(rhs_ptr_ptr);\n+    let not_at_end_rhs =\n+        rhs_copy_cx.build.ICmp(lib::llvm::LLVMIntNE, rhs_ptr,\n+                               rhs_end_ptr);\n+    let rhs_do_copy_cx = new_sub_block_ctxt(bcx, \"rhs_do_copy\");\n+    rhs_copy_cx.build.CondBr(not_at_end_rhs, rhs_do_copy_cx.llbb,\n+                             next_cx.llbb);\n+    let dest_ptr_rhs_copy = rhs_do_copy_cx.build.Load(dest_ptr_ptr);\n+    let rhs_val = load_if_immediate(rhs_do_copy_cx, rhs_ptr, unit_ty);\n+    rhs_do_copy_cx = copy_val(rhs_do_copy_cx, INIT, dest_ptr_rhs_copy,\n+                              rhs_val, unit_ty);\n+\n+    // Increment both pointers.\n+    if ty::type_has_dynamic_size(bcx_tcx(cx), unit_ty) {\n+        // We have to increment by the dynamically-computed size.\n+        incr_ptr(rhs_do_copy_cx, dest_ptr_rhs_copy, unit_sz,\n+                 dest_ptr_ptr);\n+        incr_ptr(rhs_do_copy_cx, rhs_ptr, unit_sz, rhs_ptr_ptr);\n+    } else {\n+        incr_ptr(rhs_do_copy_cx, dest_ptr_rhs_copy, C_int(1),\n+                 dest_ptr_ptr);\n+        incr_ptr(rhs_do_copy_cx, rhs_ptr, C_int(1), rhs_ptr_ptr);\n+    }\n+\n+    rhs_do_copy_cx.build.Br(rhs_copy_cx.llbb);\n+    // Finally done!\n+\n+    ret rslt(next_cx, llvecptr);\n+}\n+\n+// NB: This does *not* adjust reference counts. The caller must have done\n+// this via take_ty() beforehand.\n+fn duplicate_heap_part(cx: &@block_ctxt, orig_vptr: ValueRef,\n+                       unit_ty: ty::t) -> result {\n+    // Cast to an opaque interior vector if we can't trust the pointer\n+    // type.\n+    let vptr;\n+    if ty::type_has_dynamic_size(bcx_tcx(cx), unit_ty) {\n+        vptr = cx.build.PointerCast(orig_vptr, T_ptr(T_opaque_ivec()));\n+    } else { vptr = orig_vptr; }\n+\n+    let llunitty = type_of_or_i8(cx, unit_ty);\n+    let llheappartty = T_ivec_heap_part(llunitty);\n+\n+    // Check to see if the vector is heapified.\n+    let stack_len_ptr =\n+        cx.build.InBoundsGEP(vptr, [C_int(0), C_uint(abi::ivec_elt_len)]);\n+    let stack_len = cx.build.Load(stack_len_ptr);\n+    let stack_len_is_zero =\n+        cx.build.ICmp(lib::llvm::LLVMIntEQ, stack_len, C_int(0));\n+    let maybe_on_heap_cx = new_sub_block_ctxt(cx, \"maybe_on_heap\");\n+    let next_cx = new_sub_block_ctxt(cx, \"next\");\n+    cx.build.CondBr(stack_len_is_zero, maybe_on_heap_cx.llbb,\n+                    next_cx.llbb);\n+\n+    let stub_ptr =\n+        maybe_on_heap_cx.build.PointerCast(vptr,\n+                                           T_ptr(T_ivec_heap(llunitty)));\n+    let heap_ptr_ptr =\n+        maybe_on_heap_cx.build.InBoundsGEP(\n+            stub_ptr,\n+            [C_int(0),\n+             C_uint(abi::ivec_heap_stub_elt_ptr)]);\n+    let heap_ptr = maybe_on_heap_cx.build.Load(heap_ptr_ptr);\n+    let heap_ptr_is_nonnull =\n+        maybe_on_heap_cx.build.ICmp(lib::llvm::LLVMIntNE, heap_ptr,\n+                                    C_null(T_ptr(llheappartty)));\n+    let on_heap_cx = new_sub_block_ctxt(cx, \"on_heap\");\n+    maybe_on_heap_cx.build.CondBr(heap_ptr_is_nonnull, on_heap_cx.llbb,\n+                                  next_cx.llbb);\n+\n+    // Ok, the vector is on the heap. Copy the heap part.\n+    let alen_ptr =\n+        on_heap_cx.build.InBoundsGEP(\n+            stub_ptr,\n+            [C_int(0),\n+             C_uint(abi::ivec_heap_stub_elt_alen)]);\n+    let alen = on_heap_cx.build.Load(alen_ptr);\n+\n+    let heap_part_sz =\n+        on_heap_cx.build.Add(alen, llsize_of(T_opaque_ivec_heap_part()));\n+    let rs =\n+        trans_shared_malloc(on_heap_cx, T_ptr(llheappartty),\n+                            heap_part_sz);\n+    on_heap_cx = rs.bcx;\n+    let new_heap_ptr = rs.val;\n+\n+    rs = call_memmove(on_heap_cx, new_heap_ptr, heap_ptr, heap_part_sz);\n+    on_heap_cx = rs.bcx;\n+\n+    on_heap_cx.build.Store(new_heap_ptr, heap_ptr_ptr);\n+    on_heap_cx.build.Br(next_cx.llbb);\n+\n+    ret rslt(next_cx, C_nil());\n+}"}, {"sha": "c2aa2ed5a624da3e140887a75948c414e3d6eb8a", "filename": "src/comp/rustc.rc", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "blob_url": "https://github.com/rust-lang/rust/blob/309a7534d7ebf39f5a62bac36749d35272fe0a3e/src%2Fcomp%2Frustc.rc", "raw_url": "https://github.com/rust-lang/rust/raw/309a7534d7ebf39f5a62bac36749d35272fe0a3e/src%2Fcomp%2Frustc.rc", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Fcomp%2Frustc.rc?ref=309a7534d7ebf39f5a62bac36749d35272fe0a3e", "patch": "@@ -18,6 +18,7 @@ mod middle {\n     mod trans;\n     mod trans_alt;\n     mod trans_objects;\n+    mod trans_ivec;\n     mod ty;\n     mod ast_map;\n     mod resolve;"}]}