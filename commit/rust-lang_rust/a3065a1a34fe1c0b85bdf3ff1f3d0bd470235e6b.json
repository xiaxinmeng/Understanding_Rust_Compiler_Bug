{"sha": "a3065a1a34fe1c0b85bdf3ff1f3d0bd470235e6b", "node_id": "C_kwDOAAsO6NoAKGEzMDY1YTFhMzRmZTFjMGI4NWJkZjNmZjFmM2QwYmQ0NzAyMzVlNmI", "commit": {"author": {"name": "Lukas Bergdoll", "email": "lukas.bergdoll@gmail.com", "date": "2023-01-22T10:55:35Z"}, "committer": {"name": "Lukas Bergdoll", "email": "lukas.bergdoll@gmail.com", "date": "2023-01-22T10:55:35Z"}, "message": "Unify insertion sort implementations\n\nAvoid duplicate insertion sort implementations.\nOptimize implementations.", "tree": {"sha": "6f62b2f63e558b29c94052112181ab147feaa56f", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/6f62b2f63e558b29c94052112181ab147feaa56f"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/a3065a1a34fe1c0b85bdf3ff1f3d0bd470235e6b", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/a3065a1a34fe1c0b85bdf3ff1f3d0bd470235e6b", "html_url": "https://github.com/rust-lang/rust/commit/a3065a1a34fe1c0b85bdf3ff1f3d0bd470235e6b", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/a3065a1a34fe1c0b85bdf3ff1f3d0bd470235e6b/comments", "author": {"login": "Voultapher", "id": 6864584, "node_id": "MDQ6VXNlcjY4NjQ1ODQ=", "avatar_url": "https://avatars.githubusercontent.com/u/6864584?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Voultapher", "html_url": "https://github.com/Voultapher", "followers_url": "https://api.github.com/users/Voultapher/followers", "following_url": "https://api.github.com/users/Voultapher/following{/other_user}", "gists_url": "https://api.github.com/users/Voultapher/gists{/gist_id}", "starred_url": "https://api.github.com/users/Voultapher/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Voultapher/subscriptions", "organizations_url": "https://api.github.com/users/Voultapher/orgs", "repos_url": "https://api.github.com/users/Voultapher/repos", "events_url": "https://api.github.com/users/Voultapher/events{/privacy}", "received_events_url": "https://api.github.com/users/Voultapher/received_events", "type": "User", "site_admin": false}, "committer": {"login": "Voultapher", "id": 6864584, "node_id": "MDQ6VXNlcjY4NjQ1ODQ=", "avatar_url": "https://avatars.githubusercontent.com/u/6864584?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Voultapher", "html_url": "https://github.com/Voultapher", "followers_url": "https://api.github.com/users/Voultapher/followers", "following_url": "https://api.github.com/users/Voultapher/following{/other_user}", "gists_url": "https://api.github.com/users/Voultapher/gists{/gist_id}", "starred_url": "https://api.github.com/users/Voultapher/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Voultapher/subscriptions", "organizations_url": "https://api.github.com/users/Voultapher/orgs", "repos_url": "https://api.github.com/users/Voultapher/repos", "events_url": "https://api.github.com/users/Voultapher/events{/privacy}", "received_events_url": "https://api.github.com/users/Voultapher/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "703ff60d9f885dfe317e3d271d9f341509efac92", "url": "https://api.github.com/repos/rust-lang/rust/commits/703ff60d9f885dfe317e3d271d9f341509efac92", "html_url": "https://github.com/rust-lang/rust/commit/703ff60d9f885dfe317e3d271d9f341509efac92"}], "stats": {"total": 359, "additions": 188, "deletions": 171}, "files": [{"sha": "6bb53b16e6100ce2fdfbf606eafa14a224e3b7a5", "filename": "library/core/src/slice/sort.rs", "status": "modified", "additions": 188, "deletions": 171, "changes": 359, "blob_url": "https://github.com/rust-lang/rust/blob/a3065a1a34fe1c0b85bdf3ff1f3d0bd470235e6b/library%2Fcore%2Fsrc%2Fslice%2Fsort.rs", "raw_url": "https://github.com/rust-lang/rust/raw/a3065a1a34fe1c0b85bdf3ff1f3d0bd470235e6b/library%2Fcore%2Fsrc%2Fslice%2Fsort.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/library%2Fcore%2Fsrc%2Fslice%2Fsort.rs?ref=a3065a1a34fe1c0b85bdf3ff1f3d0bd470235e6b", "patch": "@@ -13,115 +13,178 @@ use crate::cmp;\n use crate::mem::{self, MaybeUninit, SizedTypeProperties};\n use crate::ptr;\n \n-/// When dropped, copies from `src` into `dest`.\n-struct CopyOnDrop<T> {\n+// When dropped, copies from `src` into `dest`.\n+struct InsertionHole<T> {\n     src: *const T,\n     dest: *mut T,\n }\n \n-impl<T> Drop for CopyOnDrop<T> {\n+impl<T> Drop for InsertionHole<T> {\n     fn drop(&mut self) {\n-        // SAFETY: This is a helper class.\n-        //         Please refer to its usage for correctness.\n-        //         Namely, one must be sure that `src` and `dst` does not overlap as required by `ptr::copy_nonoverlapping`.\n         unsafe {\n             ptr::copy_nonoverlapping(self.src, self.dest, 1);\n         }\n     }\n }\n \n-/// Shifts the first element to the right until it encounters a greater or equal element.\n-fn shift_head<T, F>(v: &mut [T], is_less: &mut F)\n+/// Inserts `v[v.len() - 1]` into pre-sorted sequence `v[..v.len() - 1]` so that whole `v[..]`\n+/// becomes sorted.\n+unsafe fn insert_tail<T, F>(v: &mut [T], is_less: &mut F)\n where\n     F: FnMut(&T, &T) -> bool,\n {\n-    let len = v.len();\n-    // SAFETY: The unsafe operations below involves indexing without a bounds check (by offsetting a\n-    // pointer) and copying memory (`ptr::copy_nonoverlapping`).\n-    //\n-    // a. Indexing:\n-    //  1. We checked the size of the array to >=2.\n-    //  2. All the indexing that we will do is always between {0 <= index < len} at most.\n-    //\n-    // b. Memory copying\n-    //  1. We are obtaining pointers to references which are guaranteed to be valid.\n-    //  2. They cannot overlap because we obtain pointers to difference indices of the slice.\n-    //     Namely, `i` and `i-1`.\n-    //  3. If the slice is properly aligned, the elements are properly aligned.\n-    //     It is the caller's responsibility to make sure the slice is properly aligned.\n-    //\n-    // See comments below for further detail.\n+    debug_assert!(v.len() >= 2);\n+\n+    let arr_ptr = v.as_mut_ptr();\n+    let i = v.len() - 1;\n+\n+    // SAFETY: caller must ensure v is at least len 2.\n     unsafe {\n-        // If the first two elements are out-of-order...\n-        if len >= 2 && is_less(v.get_unchecked(1), v.get_unchecked(0)) {\n-            // Read the first element into a stack-allocated variable. If a following comparison\n-            // operation panics, `hole` will get dropped and automatically write the element back\n-            // into the slice.\n-            let tmp = mem::ManuallyDrop::new(ptr::read(v.get_unchecked(0)));\n-            let v = v.as_mut_ptr();\n-            let mut hole = CopyOnDrop { src: &*tmp, dest: v.add(1) };\n-            ptr::copy_nonoverlapping(v.add(1), v.add(0), 1);\n-\n-            for i in 2..len {\n-                if !is_less(&*v.add(i), &*tmp) {\n+        // See insert_head which talks about why this approach is beneficial.\n+        let i_ptr = arr_ptr.add(i);\n+\n+        // It's important that we use i_ptr here. If this check is positive and we continue,\n+        // We want to make sure that no other copy of the value was seen by is_less.\n+        // Otherwise we would have to copy it back.\n+        if is_less(&*i_ptr, &*i_ptr.sub(1)) {\n+            // It's important, that we use tmp for comparison from now on. As it is the value that\n+            // will be copied back. And notionally we could have created a divergence if we copy\n+            // back the wrong value.\n+            let tmp = mem::ManuallyDrop::new(ptr::read(i_ptr));\n+            // Intermediate state of the insertion process is always tracked by `hole`, which\n+            // serves two purposes:\n+            // 1. Protects integrity of `v` from panics in `is_less`.\n+            // 2. Fills the remaining hole in `v` in the end.\n+            //\n+            // Panic safety:\n+            //\n+            // If `is_less` panics at any point during the process, `hole` will get dropped and\n+            // fill the hole in `v` with `tmp`, thus ensuring that `v` still holds every object it\n+            // initially held exactly once.\n+            let mut hole = InsertionHole { src: &*tmp, dest: i_ptr.sub(1) };\n+            ptr::copy_nonoverlapping(hole.dest, i_ptr, 1);\n+\n+            // SAFETY: We know i is at least 1.\n+            for j in (0..(i - 1)).rev() {\n+                let j_ptr = arr_ptr.add(j);\n+                if !is_less(&*tmp, &*j_ptr) {\n                     break;\n                 }\n \n-                // Move `i`-th element one place to the left, thus shifting the hole to the right.\n-                ptr::copy_nonoverlapping(v.add(i), v.add(i - 1), 1);\n-                hole.dest = v.add(i);\n+                ptr::copy_nonoverlapping(j_ptr, hole.dest, 1);\n+                hole.dest = j_ptr;\n             }\n             // `hole` gets dropped and thus copies `tmp` into the remaining hole in `v`.\n         }\n     }\n }\n \n-/// Shifts the last element to the left until it encounters a smaller or equal element.\n-fn shift_tail<T, F>(v: &mut [T], is_less: &mut F)\n+/// Inserts `v[0]` into pre-sorted sequence `v[1..]` so that whole `v[..]` becomes sorted.\n+///\n+/// This is the integral subroutine of insertion sort.\n+unsafe fn insert_head<T, F>(v: &mut [T], is_less: &mut F)\n where\n     F: FnMut(&T, &T) -> bool,\n {\n-    let len = v.len();\n-    // SAFETY: The unsafe operations below involves indexing without a bound check (by offsetting a\n-    // pointer) and copying memory (`ptr::copy_nonoverlapping`).\n-    //\n-    // a. Indexing:\n-    //  1. We checked the size of the array to >= 2.\n-    //  2. All the indexing that we will do is always between `0 <= index < len-1` at most.\n-    //\n-    // b. Memory copying\n-    //  1. We are obtaining pointers to references which are guaranteed to be valid.\n-    //  2. They cannot overlap because we obtain pointers to difference indices of the slice.\n-    //     Namely, `i` and `i+1`.\n-    //  3. If the slice is properly aligned, the elements are properly aligned.\n-    //     It is the caller's responsibility to make sure the slice is properly aligned.\n-    //\n-    // See comments below for further detail.\n+    debug_assert!(v.len() >= 2);\n+\n     unsafe {\n-        // If the last two elements are out-of-order...\n-        if len >= 2 && is_less(v.get_unchecked(len - 1), v.get_unchecked(len - 2)) {\n-            // Read the last element into a stack-allocated variable. If a following comparison\n-            // operation panics, `hole` will get dropped and automatically write the element back\n-            // into the slice.\n-            let tmp = mem::ManuallyDrop::new(ptr::read(v.get_unchecked(len - 1)));\n-            let v = v.as_mut_ptr();\n-            let mut hole = CopyOnDrop { src: &*tmp, dest: v.add(len - 2) };\n-            ptr::copy_nonoverlapping(v.add(len - 2), v.add(len - 1), 1);\n-\n-            for i in (0..len - 2).rev() {\n-                if !is_less(&*tmp, &*v.add(i)) {\n+        if is_less(v.get_unchecked(1), v.get_unchecked(0)) {\n+            let arr_ptr = v.as_mut_ptr();\n+\n+            // There are three ways to implement insertion here:\n+            //\n+            // 1. Swap adjacent elements until the first one gets to its final destination.\n+            //    However, this way we copy data around more than is necessary. If elements are big\n+            //    structures (costly to copy), this method will be slow.\n+            //\n+            // 2. Iterate until the right place for the first element is found. Then shift the\n+            //    elements succeeding it to make room for it and finally place it into the\n+            //    remaining hole. This is a good method.\n+            //\n+            // 3. Copy the first element into a temporary variable. Iterate until the right place\n+            //    for it is found. As we go along, copy every traversed element into the slot\n+            //    preceding it. Finally, copy data from the temporary variable into the remaining\n+            //    hole. This method is very good. Benchmarks demonstrated slightly better\n+            //    performance than with the 2nd method.\n+            //\n+            // All methods were benchmarked, and the 3rd showed best results. So we chose that one.\n+            let tmp = mem::ManuallyDrop::new(ptr::read(arr_ptr));\n+\n+            // Intermediate state of the insertion process is always tracked by `hole`, which\n+            // serves two purposes:\n+            // 1. Protects integrity of `v` from panics in `is_less`.\n+            // 2. Fills the remaining hole in `v` in the end.\n+            //\n+            // Panic safety:\n+            //\n+            // If `is_less` panics at any point during the process, `hole` will get dropped and\n+            // fill the hole in `v` with `tmp`, thus ensuring that `v` still holds every object it\n+            // initially held exactly once.\n+            let mut hole = InsertionHole { src: &*tmp, dest: arr_ptr.add(1) };\n+            ptr::copy_nonoverlapping(arr_ptr.add(1), arr_ptr.add(0), 1);\n+\n+            for i in 2..v.len() {\n+                if !is_less(&v.get_unchecked(i), &*tmp) {\n                     break;\n                 }\n-\n-                // Move `i`-th element one place to the right, thus shifting the hole to the left.\n-                ptr::copy_nonoverlapping(v.add(i), v.add(i + 1), 1);\n-                hole.dest = v.add(i);\n+                ptr::copy_nonoverlapping(arr_ptr.add(i), arr_ptr.add(i - 1), 1);\n+                hole.dest = arr_ptr.add(i);\n             }\n             // `hole` gets dropped and thus copies `tmp` into the remaining hole in `v`.\n         }\n     }\n }\n \n+/// Sort `v` assuming `v[..offset]` is already sorted.\n+///\n+/// Never inline this function to avoid code bloat. It still optimizes nicely and has practically no\n+/// performance impact. Even improving performance in some cases.\n+#[inline(never)]\n+fn insertion_sort_shift_left<T, F>(v: &mut [T], offset: usize, is_less: &mut F)\n+where\n+    F: FnMut(&T, &T) -> bool,\n+{\n+    let len = v.len();\n+\n+    // Using assert here improves performance.\n+    assert!(offset != 0 && offset <= len);\n+\n+    // Shift each element of the unsorted region v[i..] as far left as is needed to make v sorted.\n+    for i in offset..len {\n+        // SAFETY: we tested that `offset` must be at least 1, so this loop is only entered if len\n+        // >= 2.\n+        unsafe {\n+            insert_tail(&mut v[..=i], is_less);\n+        }\n+    }\n+}\n+\n+/// Sort `v` assuming `v[offset..]` is already sorted.\n+///\n+/// Never inline this function to avoid code bloat. It still optimizes nicely and has practically no\n+/// performance impact. Even improving performance in some cases.\n+#[inline(never)]\n+fn insertion_sort_shift_right<T, F>(v: &mut [T], offset: usize, is_less: &mut F)\n+where\n+    F: FnMut(&T, &T) -> bool,\n+{\n+    let len = v.len();\n+\n+    // Using assert here improves performance.\n+    assert!(offset != 0 && offset <= len && len >= 2);\n+\n+    // Shift each element of the unsorted region v[..i] as far left as is needed to make v sorted.\n+    for i in (0..offset).rev() {\n+        // We ensured that the slice length is always at least 2 long.\n+        // We know that start_found will be at least one less than end,\n+        // and the range is exclusive. Which gives us i always <= (end - 2).\n+        unsafe {\n+            insert_head(&mut v[i..len], is_less);\n+        }\n+    }\n+}\n+\n /// Partially sorts a slice by shifting several out-of-order elements around.\n ///\n /// Returns `true` if the slice is sorted at the end. This function is *O*(*n*) worst-case.\n@@ -161,26 +224,19 @@ where\n         // Swap the found pair of elements. This puts them in correct order.\n         v.swap(i - 1, i);\n \n-        // Shift the smaller element to the left.\n-        shift_tail(&mut v[..i], is_less);\n-        // Shift the greater element to the right.\n-        shift_head(&mut v[i..], is_less);\n+        if i >= 2 {\n+            // Shift the smaller element to the left.\n+            insertion_sort_shift_left(&mut v[..i], i - 1, is_less);\n+\n+            // Shift the greater element to the right.\n+            insertion_sort_shift_right(&mut v[..i], 1, is_less);\n+        }\n     }\n \n     // Didn't manage to sort the slice in the limited number of steps.\n     false\n }\n \n-/// Sorts a slice using insertion sort, which is *O*(*n*^2) worst-case.\n-fn insertion_sort<T, F>(v: &mut [T], is_less: &mut F)\n-where\n-    F: FnMut(&T, &T) -> bool,\n-{\n-    for i in 1..v.len() {\n-        shift_tail(&mut v[..i + 1], is_less);\n-    }\n-}\n-\n /// Sorts `v` using heapsort, which guarantees *O*(*n* \\* log(*n*)) worst-case.\n #[cold]\n #[unstable(feature = \"sort_internals\", reason = \"internal to sort module\", issue = \"none\")]\n@@ -507,7 +563,7 @@ where\n \n         // SAFETY: `pivot` is a reference to the first element of `v`, so `ptr::read` is safe.\n         let tmp = mem::ManuallyDrop::new(unsafe { ptr::read(pivot) });\n-        let _pivot_guard = CopyOnDrop { src: &*tmp, dest: pivot };\n+        let _pivot_guard = InsertionHole { src: &*tmp, dest: pivot };\n         let pivot = &*tmp;\n \n         // Find the first pair of out-of-order elements.\n@@ -560,7 +616,7 @@ where\n     // operation panics, the pivot will be automatically written back into the slice.\n     // SAFETY: The pointer here is valid because it is obtained from a reference to a slice.\n     let tmp = mem::ManuallyDrop::new(unsafe { ptr::read(pivot) });\n-    let _pivot_guard = CopyOnDrop { src: &*tmp, dest: pivot };\n+    let _pivot_guard = InsertionHole { src: &*tmp, dest: pivot };\n     let pivot = &*tmp;\n \n     // Now partition the slice.\n@@ -742,7 +798,9 @@ where\n \n         // Very short slices get sorted using insertion sort.\n         if len <= MAX_INSERTION {\n-            insertion_sort(v, is_less);\n+            if len >= 2 {\n+                insertion_sort_shift_left(v, 1, is_less);\n+            }\n             return;\n         }\n \n@@ -844,10 +902,14 @@ fn partition_at_index_loop<'a, T, F>(\n     let mut was_balanced = true;\n \n     loop {\n+        let len = v.len();\n+\n         // For slices of up to this length it's probably faster to simply sort them.\n         const MAX_INSERTION: usize = 10;\n-        if v.len() <= MAX_INSERTION {\n-            insertion_sort(v, is_less);\n+        if len <= MAX_INSERTION {\n+            if len >= 2 {\n+                insertion_sort_shift_left(v, 1, is_less);\n+            }\n             return;\n         }\n \n@@ -887,7 +949,7 @@ fn partition_at_index_loop<'a, T, F>(\n         }\n \n         let (mid, _) = partition(v, pivot, is_less);\n-        was_balanced = cmp::min(mid, v.len() - mid) >= v.len() / 8;\n+        was_balanced = cmp::min(mid, len - mid) >= len / 8;\n \n         // Split the slice into `left`, `pivot`, and `right`.\n         let (left, right) = v.split_at_mut(mid);\n@@ -954,75 +1016,6 @@ where\n     (left, pivot, right)\n }\n \n-/// Inserts `v[0]` into pre-sorted sequence `v[1..]` so that whole `v[..]` becomes sorted.\n-///\n-/// This is the integral subroutine of insertion sort.\n-fn insert_head<T, F>(v: &mut [T], is_less: &mut F)\n-where\n-    F: FnMut(&T, &T) -> bool,\n-{\n-    if v.len() >= 2 && is_less(&v[1], &v[0]) {\n-        // SAFETY: Copy tmp back even if panic, and ensure unique observation.\n-        unsafe {\n-            // There are three ways to implement insertion here:\n-            //\n-            // 1. Swap adjacent elements until the first one gets to its final destination.\n-            //    However, this way we copy data around more than is necessary. If elements are big\n-            //    structures (costly to copy), this method will be slow.\n-            //\n-            // 2. Iterate until the right place for the first element is found. Then shift the\n-            //    elements succeeding it to make room for it and finally place it into the\n-            //    remaining hole. This is a good method.\n-            //\n-            // 3. Copy the first element into a temporary variable. Iterate until the right place\n-            //    for it is found. As we go along, copy every traversed element into the slot\n-            //    preceding it. Finally, copy data from the temporary variable into the remaining\n-            //    hole. This method is very good. Benchmarks demonstrated slightly better\n-            //    performance than with the 2nd method.\n-            //\n-            // All methods were benchmarked, and the 3rd showed best results. So we chose that one.\n-            let tmp = mem::ManuallyDrop::new(ptr::read(&v[0]));\n-\n-            // Intermediate state of the insertion process is always tracked by `hole`, which\n-            // serves two purposes:\n-            // 1. Protects integrity of `v` from panics in `is_less`.\n-            // 2. Fills the remaining hole in `v` in the end.\n-            //\n-            // Panic safety:\n-            //\n-            // If `is_less` panics at any point during the process, `hole` will get dropped and\n-            // fill the hole in `v` with `tmp`, thus ensuring that `v` still holds every object it\n-            // initially held exactly once.\n-            let mut hole = InsertionHole { src: &*tmp, dest: &mut v[1] };\n-            ptr::copy_nonoverlapping(&v[1], &mut v[0], 1);\n-\n-            for i in 2..v.len() {\n-                if !is_less(&v[i], &*tmp) {\n-                    break;\n-                }\n-                ptr::copy_nonoverlapping(&v[i], &mut v[i - 1], 1);\n-                hole.dest = &mut v[i];\n-            }\n-            // `hole` gets dropped and thus copies `tmp` into the remaining hole in `v`.\n-        }\n-    }\n-\n-    // When dropped, copies from `src` into `dest`.\n-    struct InsertionHole<T> {\n-        src: *const T,\n-        dest: *mut T,\n-    }\n-\n-    impl<T> Drop for InsertionHole<T> {\n-        fn drop(&mut self) {\n-            // SAFETY: The caller must ensure that src and dest are correctly set.\n-            unsafe {\n-                ptr::copy_nonoverlapping(self.src, self.dest, 1);\n-            }\n-        }\n-    }\n-}\n-\n /// Merges non-decreasing runs `v[..mid]` and `v[mid..]` using `buf` as temporary storage, and\n /// stores the result into `v[..]`.\n ///\n@@ -1180,8 +1173,6 @@ pub fn merge_sort<T, CmpF, ElemAllocF, ElemDeallocF, RunAllocF, RunDeallocF>(\n {\n     // Slices of up to this length get sorted using insertion sort.\n     const MAX_INSERTION: usize = 20;\n-    // Very short runs are extended using insertion sort to span at least this many elements.\n-    const MIN_RUN: usize = 10;\n \n     // The caller should have already checked that.\n     debug_assert!(!T::IS_ZST);\n@@ -1191,9 +1182,7 @@ pub fn merge_sort<T, CmpF, ElemAllocF, ElemDeallocF, RunAllocF, RunDeallocF>(\n     // Short arrays get sorted in-place via insertion sort to avoid allocations.\n     if len <= MAX_INSERTION {\n         if len >= 2 {\n-            for i in (0..len - 1).rev() {\n-                insert_head(&mut v[i..], is_less);\n-            }\n+            insertion_sort_shift_left(v, 1, is_less);\n         }\n         return;\n     }\n@@ -1236,10 +1225,7 @@ pub fn merge_sort<T, CmpF, ElemAllocF, ElemDeallocF, RunAllocF, RunDeallocF>(\n \n         // Insert some more elements into the run if it's too short. Insertion sort is faster than\n         // merge sort on short sequences, so this significantly improves performance.\n-        while start > 0 && end - start < MIN_RUN {\n-            start -= 1;\n-            insert_head(&mut v[start..end], is_less);\n-        }\n+        start = provide_sorted_batch(v, start, end, is_less);\n \n         // Push this run onto the stack.\n         runs.push(TimSortRun { start, len: end - start });\n@@ -1467,3 +1453,34 @@ pub struct TimSortRun {\n     len: usize,\n     start: usize,\n }\n+\n+/// Takes a range as denoted by start and end, that is already sorted and extends it to the left if\n+/// necessary with sorts optimized for smaller ranges such as insertion sort.\n+#[cfg(not(no_global_oom_handling))]\n+fn provide_sorted_batch<T, F>(v: &mut [T], mut start: usize, end: usize, is_less: &mut F) -> usize\n+where\n+    F: FnMut(&T, &T) -> bool,\n+{\n+    debug_assert!(end > start);\n+\n+    // This value is a balance between least comparisons and best performance, as\n+    // influenced by for example cache locality.\n+    const MIN_INSERTION_RUN: usize = 10;\n+\n+    // Insert some more elements into the run if it's too short. Insertion sort is faster than\n+    // merge sort on short sequences, so this significantly improves performance.\n+    let start_found = start;\n+    let start_end_diff = end - start;\n+\n+    if start_end_diff < MIN_INSERTION_RUN && start != 0 {\n+        // v[start_found..end] are elements that are already sorted in the input. We want to extend\n+        // the sorted region to the left, so we push up MIN_INSERTION_RUN - 1 to the right. Which is\n+        // more efficient that trying to push those already sorted elements to the left.\n+\n+        start = if end >= MIN_INSERTION_RUN { end - MIN_INSERTION_RUN } else { 0 };\n+\n+        insertion_sort_shift_right(&mut v[start..end], start_found - start, is_less);\n+    }\n+\n+    start\n+}"}]}