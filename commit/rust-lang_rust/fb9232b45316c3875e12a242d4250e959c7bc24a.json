{"sha": "fb9232b45316c3875e12a242d4250e959c7bc24a", "node_id": "C_kwDOAAsO6NoAKGZiOTIzMmI0NTMxNmMzODc1ZTEyYTI0MmQ0MjUwZTk1OWM3YmMyNGE", "commit": {"author": {"name": "Yuki Okushi", "email": "jtitor@2k36.org", "date": "2021-10-21T05:11:02Z"}, "committer": {"name": "GitHub", "email": "noreply@github.com", "date": "2021-10-21T05:11:02Z"}, "message": "Rollup merge of #87440 - twetzel59:fix-barrier-no-op, r=yaahc\n\nRemove unnecessary condition in Barrier::wait()\n\nThis is my first pull request for Rust, so feel free to call me out if anything is amiss.\n\nAfter some examination, I realized that the second condition of the \"spurious-wakeup-handler\" loop in ``std::sync::Barrier::wait()`` should always evaluate to ``true``, making it redundant in the ``&&`` expression.\n\nHere is the affected function before the fix:\n```rust\n#[stable(feature = \"rust1\", since = \"1.0.0\")]\npub fn wait(&self) -> BarrierWaitResult {\n    let mut lock = self.lock.lock().unwrap();\n    let local_gen = lock.generation_id;\n    lock.count += 1;\n    if lock.count < self.num_threads {\n        // We need a while loop to guard against spurious wakeups.\n        // https://en.wikipedia.org/wiki/Spurious_wakeup\n        while local_gen == lock.generation_id && lock.count < self.num_threads { // fixme\n            lock = self.cvar.wait(lock).unwrap();\n        }\n        BarrierWaitResult(false)\n    } else {\n        lock.count = 0;\n        lock.generation_id = lock.generation_id.wrapping_add(1);\n        self.cvar.notify_all();\n        BarrierWaitResult(true)\n    }\n}\n```\n\nAt first glance, it seems that the check that ``lock.count < self.num_threads`` would be necessary in order for a thread A to detect when another thread B has caused the barrier to reach its thread count, making thread B the \"leader\".\n\nHowever, the control flow implicitly results in an invariant that makes observing ``!(lock.count < self.num_threads)``, i.e. ``lock.count >= self.num_threads`` impossible from thread A.\n\nWhen thread B, which will be the leader, calls ``.wait()`` on this shared instance of the ``Barrier``, it locks the mutex in the first line and saves the ``MutexGuard`` in the ``lock`` variable. It then increments the value of ``lock.count``. However, it then proceeds to check if ``lock.count < self.num_threads``. Since it is the leader, it is the case that (after the increment of ``lock.count``), the lock count is *equal* to the number of threads. Thus, the second branch is immediately taken and ``lock.count`` is zeroed. Additionally, the generation ID is incremented (with wrap). Then, the condition variable is signalled. But, the other threads are waiting at the line ``lock = self.cvar.wait(lock).unwrap();``, so they cannot resume until thread B's call to ``Barrier::wait()`` returns, which drops the ``MutexGuard`` acquired in the first ``let`` statement and unlocks the mutex.\n\nThe order of events is thus:\n1. A thread A calls `.wait()`\n2. `.wait()` acquires the mutex, increments `lock.count`, and takes the first branch\n3. Thread A enters the ``while`` loop since the generation ID has not changed and the count is less than the number of threads for the ``Barrier``\n3. Spurious wakeups occur, but both conditions hold, so the thread A waits on the condition variable\n4. This process repeats for N - 2 additional times for non-leader threads A'\n5. *Meanwhile*, Thread B calls ``Barrier::wait()`` on the same barrier that threads A, A', A'', etc. are waiting on. The thread count reaches the number of threads for the ``Barrier``, so all threads should now proceed, with B being the leader. B acquires the mutex and increments the value ``lock.count`` only to find that it is not less than ``self.num_threads``. Thus, it immediately clamps ``self.num_threads`` back down to 0 and increments the generation. Then, it signals the condvar to tell the A (prime) threads that they may continue.\n6. The A, A', A''... threads wake up and attempt to re-acquire the ``lock`` as per the internal operation of a condition variable. When each A has exclusive access to the mutex, it finds that ``lock.generation_id`` no longer matches ``local_generation`` **and the ``&&`` expression short-circuits -- and even if it were to evaluate it, ``self.count`` is definitely less than ``self.num_threads`` because it has been reset to ``0`` by thread B *before* B dropped its ``MutexGuard``**.\n\nTherefore, it my understanding that it would be impossible for the non-leader threads to ever see the second boolean expression evaluate to anything other than ``true``. This PR simply removes that condition.\n\nAny input would be appreciated. Sorry if this is terribly verbose. I'm new to the Rust community and concurrency can be hard to explain in words. Thanks!", "tree": {"sha": "0339ebb81b40d4fcc82f8c0cbaecb62fa23caba0", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/0339ebb81b40d4fcc82f8c0cbaecb62fa23caba0"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/fb9232b45316c3875e12a242d4250e959c7bc24a", "comment_count": 0, "verification": {"verified": true, "reason": "valid", "signature": "-----BEGIN PGP SIGNATURE-----\n\nwsBcBAABCAAQBQJhcPZmCRBK7hj4Ov3rIwAAy9gIAA53q50cDvp8YL6t44HMErtp\nOzCx1tiPn/7uMBRe8xuz9yjjWwmPakhQD5RNF/upReydidcbweZG9F78jEPiSC2E\nRPGXjTQ4eOvd35KMk4HOAzRhDjZkNMELJL8fvVpOI3lpRuTrc6H1Q9glOrXlLqtb\nQacApuIIo+LqhcnWwVF0Y42woQ7kaGnAkZiFIZJFZTRYZ+Ys0/MvcnYcBkHy1RXY\nuNjSn9Q8TKfL2PzDNH0B9+cDcfSC9x/zQD2qEhwG8alHyMZOLHJBT8Wx9e3Pumy4\nX5cJBxj3zPz9dIX8Pesll0fQqnbowIIgZRxbhkY7+QBI4igBavZ5ByYqV+usg+E=\n=Z3Y5\n-----END PGP SIGNATURE-----\n", "payload": "tree 0339ebb81b40d4fcc82f8c0cbaecb62fa23caba0\nparent 09de34c10760d31e0781cb506b5b41c649b53e69\nparent d65ab29e2e67bfc329c606923130d6bf5a518800\nauthor Yuki Okushi <jtitor@2k36.org> 1634793062 +0900\ncommitter GitHub <noreply@github.com> 1634793062 +0900\n\nRollup merge of #87440 - twetzel59:fix-barrier-no-op, r=yaahc\n\nRemove unnecessary condition in Barrier::wait()\n\nThis is my first pull request for Rust, so feel free to call me out if anything is amiss.\n\nAfter some examination, I realized that the second condition of the \"spurious-wakeup-handler\" loop in ``std::sync::Barrier::wait()`` should always evaluate to ``true``, making it redundant in the ``&&`` expression.\n\nHere is the affected function before the fix:\n```rust\n#[stable(feature = \"rust1\", since = \"1.0.0\")]\npub fn wait(&self) -> BarrierWaitResult {\n    let mut lock = self.lock.lock().unwrap();\n    let local_gen = lock.generation_id;\n    lock.count += 1;\n    if lock.count < self.num_threads {\n        // We need a while loop to guard against spurious wakeups.\n        // https://en.wikipedia.org/wiki/Spurious_wakeup\n        while local_gen == lock.generation_id && lock.count < self.num_threads { // fixme\n            lock = self.cvar.wait(lock).unwrap();\n        }\n        BarrierWaitResult(false)\n    } else {\n        lock.count = 0;\n        lock.generation_id = lock.generation_id.wrapping_add(1);\n        self.cvar.notify_all();\n        BarrierWaitResult(true)\n    }\n}\n```\n\nAt first glance, it seems that the check that ``lock.count < self.num_threads`` would be necessary in order for a thread A to detect when another thread B has caused the barrier to reach its thread count, making thread B the \"leader\".\n\nHowever, the control flow implicitly results in an invariant that makes observing ``!(lock.count < self.num_threads)``, i.e. ``lock.count >= self.num_threads`` impossible from thread A.\n\nWhen thread B, which will be the leader, calls ``.wait()`` on this shared instance of the ``Barrier``, it locks the mutex in the first line and saves the ``MutexGuard`` in the ``lock`` variable. It then increments the value of ``lock.count``. However, it then proceeds to check if ``lock.count < self.num_threads``. Since it is the leader, it is the case that (after the increment of ``lock.count``), the lock count is *equal* to the number of threads. Thus, the second branch is immediately taken and ``lock.count`` is zeroed. Additionally, the generation ID is incremented (with wrap). Then, the condition variable is signalled. But, the other threads are waiting at the line ``lock = self.cvar.wait(lock).unwrap();``, so they cannot resume until thread B's call to ``Barrier::wait()`` returns, which drops the ``MutexGuard`` acquired in the first ``let`` statement and unlocks the mutex.\n\nThe order of events is thus:\n1. A thread A calls `.wait()`\n2. `.wait()` acquires the mutex, increments `lock.count`, and takes the first branch\n3. Thread A enters the ``while`` loop since the generation ID has not changed and the count is less than the number of threads for the ``Barrier``\n3. Spurious wakeups occur, but both conditions hold, so the thread A waits on the condition variable\n4. This process repeats for N - 2 additional times for non-leader threads A'\n5. *Meanwhile*, Thread B calls ``Barrier::wait()`` on the same barrier that threads A, A', A'', etc. are waiting on. The thread count reaches the number of threads for the ``Barrier``, so all threads should now proceed, with B being the leader. B acquires the mutex and increments the value ``lock.count`` only to find that it is not less than ``self.num_threads``. Thus, it immediately clamps ``self.num_threads`` back down to 0 and increments the generation. Then, it signals the condvar to tell the A (prime) threads that they may continue.\n6. The A, A', A''... threads wake up and attempt to re-acquire the ``lock`` as per the internal operation of a condition variable. When each A has exclusive access to the mutex, it finds that ``lock.generation_id`` no longer matches ``local_generation`` **and the ``&&`` expression short-circuits -- and even if it were to evaluate it, ``self.count`` is definitely less than ``self.num_threads`` because it has been reset to ``0`` by thread B *before* B dropped its ``MutexGuard``**.\n\nTherefore, it my understanding that it would be impossible for the non-leader threads to ever see the second boolean expression evaluate to anything other than ``true``. This PR simply removes that condition.\n\nAny input would be appreciated. Sorry if this is terribly verbose. I'm new to the Rust community and concurrency can be hard to explain in words. Thanks!\n"}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/fb9232b45316c3875e12a242d4250e959c7bc24a", "html_url": "https://github.com/rust-lang/rust/commit/fb9232b45316c3875e12a242d4250e959c7bc24a", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/fb9232b45316c3875e12a242d4250e959c7bc24a/comments", "author": {"login": "JohnTitor", "id": 25030997, "node_id": "MDQ6VXNlcjI1MDMwOTk3", "avatar_url": "https://avatars.githubusercontent.com/u/25030997?v=4", "gravatar_id": "", "url": "https://api.github.com/users/JohnTitor", "html_url": "https://github.com/JohnTitor", "followers_url": "https://api.github.com/users/JohnTitor/followers", "following_url": "https://api.github.com/users/JohnTitor/following{/other_user}", "gists_url": "https://api.github.com/users/JohnTitor/gists{/gist_id}", "starred_url": "https://api.github.com/users/JohnTitor/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/JohnTitor/subscriptions", "organizations_url": "https://api.github.com/users/JohnTitor/orgs", "repos_url": "https://api.github.com/users/JohnTitor/repos", "events_url": "https://api.github.com/users/JohnTitor/events{/privacy}", "received_events_url": "https://api.github.com/users/JohnTitor/received_events", "type": "User", "site_admin": false}, "committer": {"login": "web-flow", "id": 19864447, "node_id": "MDQ6VXNlcjE5ODY0NDQ3", "avatar_url": "https://avatars.githubusercontent.com/u/19864447?v=4", "gravatar_id": "", "url": "https://api.github.com/users/web-flow", "html_url": "https://github.com/web-flow", "followers_url": "https://api.github.com/users/web-flow/followers", "following_url": "https://api.github.com/users/web-flow/following{/other_user}", "gists_url": "https://api.github.com/users/web-flow/gists{/gist_id}", "starred_url": "https://api.github.com/users/web-flow/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/web-flow/subscriptions", "organizations_url": "https://api.github.com/users/web-flow/orgs", "repos_url": "https://api.github.com/users/web-flow/repos", "events_url": "https://api.github.com/users/web-flow/events{/privacy}", "received_events_url": "https://api.github.com/users/web-flow/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "09de34c10760d31e0781cb506b5b41c649b53e69", "url": "https://api.github.com/repos/rust-lang/rust/commits/09de34c10760d31e0781cb506b5b41c649b53e69", "html_url": "https://github.com/rust-lang/rust/commit/09de34c10760d31e0781cb506b5b41c649b53e69"}, {"sha": "d65ab29e2e67bfc329c606923130d6bf5a518800", "url": "https://api.github.com/repos/rust-lang/rust/commits/d65ab29e2e67bfc329c606923130d6bf5a518800", "html_url": "https://github.com/rust-lang/rust/commit/d65ab29e2e67bfc329c606923130d6bf5a518800"}], "stats": {"total": 2, "additions": 1, "deletions": 1}, "files": [{"sha": "11836b7b694b38fd06043ca542b23ceb0244835f", "filename": "library/std/src/sync/barrier.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/fb9232b45316c3875e12a242d4250e959c7bc24a/library%2Fstd%2Fsrc%2Fsync%2Fbarrier.rs", "raw_url": "https://github.com/rust-lang/rust/raw/fb9232b45316c3875e12a242d4250e959c7bc24a/library%2Fstd%2Fsrc%2Fsync%2Fbarrier.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/library%2Fstd%2Fsrc%2Fsync%2Fbarrier.rs?ref=fb9232b45316c3875e12a242d4250e959c7bc24a", "patch": "@@ -130,7 +130,7 @@ impl Barrier {\n         if lock.count < self.num_threads {\n             // We need a while loop to guard against spurious wakeups.\n             // https://en.wikipedia.org/wiki/Spurious_wakeup\n-            while local_gen == lock.generation_id && lock.count < self.num_threads {\n+            while local_gen == lock.generation_id {\n                 lock = self.cvar.wait(lock).unwrap();\n             }\n             BarrierWaitResult(false)"}]}