{"sha": "8692c455958d2cd70e1a20c765ecde143dbf453c", "node_id": "MDY6Q29tbWl0NzI0NzEyOjg2OTJjNDU1OTU4ZDJjZDcwZTFhMjBjNzY1ZWNkZTE0M2RiZjQ1M2M=", "commit": {"author": {"name": "Ralf Jung", "email": "post@ralfj.de", "date": "2020-05-24T10:17:11Z"}, "committer": {"name": "GitHub", "email": "noreply@github.com", "date": "2020-05-24T10:17:11Z"}, "message": "Rollup merge of #72393 - Aaron1011:feature/rewrite-collect-tokens, r=petrochenkov\n\nRewrite `Parser::collect_tokens`\n\nThe previous implementation did not work when called on an opening\ndelimiter, or when called re-entrantly from the same `TokenCursor` stack\ndepth.\n\nI'm not sure how to test this apart from https://github.com/rust-lang/rust/pull/72287", "tree": {"sha": "a54e9bce18380a728eb94aafaaa242cda98d5de9", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/a54e9bce18380a728eb94aafaaa242cda98d5de9"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/8692c455958d2cd70e1a20c765ecde143dbf453c", "comment_count": 0, "verification": {"verified": true, "reason": "valid", "signature": "-----BEGIN PGP SIGNATURE-----\n\nwsBcBAABCAAQBQJeykmnCRBK7hj4Ov3rIwAAdHIIABjzLH2rMyr1kx1AhTaXCWzG\nV3h4eYd/2cg4s9XBtMB51oG8MH0uqIlEQcJdVJgr8vFffJdfQRROKgfA+IhBouuR\n5YDqcvGUsOE4aWa314kzpltO0c+mJOsrPq1XP6cAJr6TAhE2UU3A8ATZWQdJXm9X\n54iELHyEC/FQZfsDhZx/ks0GnnRp649NgdtLSypQVDyo/YvPLkvG/biDvlKbsrWH\n6alh0eGQKpd+9xn081epCuEok93Zq2ajm7VVex8dvC0PiL8zk5m7qqPHdmtIoFsg\nNbBhMmt14/NxjBgu1+fWkvROniky8VOUqNCLadLpv5z4gVDA3C/kqqewhEJxfW4=\n=Dwuc\n-----END PGP SIGNATURE-----\n", "payload": "tree a54e9bce18380a728eb94aafaaa242cda98d5de9\nparent 94d96b1d75d242bf6fcf00e375ac8ea9d5f67bd8\nparent 52bb09abba48f1e68421c8fe18b30cee011c84ad\nauthor Ralf Jung <post@ralfj.de> 1590315431 +0200\ncommitter GitHub <noreply@github.com> 1590315431 +0200\n\nRollup merge of #72393 - Aaron1011:feature/rewrite-collect-tokens, r=petrochenkov\n\nRewrite `Parser::collect_tokens`\n\nThe previous implementation did not work when called on an opening\ndelimiter, or when called re-entrantly from the same `TokenCursor` stack\ndepth.\n\nI'm not sure how to test this apart from https://github.com/rust-lang/rust/pull/72287\n"}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/8692c455958d2cd70e1a20c765ecde143dbf453c", "html_url": "https://github.com/rust-lang/rust/commit/8692c455958d2cd70e1a20c765ecde143dbf453c", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/8692c455958d2cd70e1a20c765ecde143dbf453c/comments", "author": {"login": "RalfJung", "id": 330628, "node_id": "MDQ6VXNlcjMzMDYyOA==", "avatar_url": "https://avatars.githubusercontent.com/u/330628?v=4", "gravatar_id": "", "url": "https://api.github.com/users/RalfJung", "html_url": "https://github.com/RalfJung", "followers_url": "https://api.github.com/users/RalfJung/followers", "following_url": "https://api.github.com/users/RalfJung/following{/other_user}", "gists_url": "https://api.github.com/users/RalfJung/gists{/gist_id}", "starred_url": "https://api.github.com/users/RalfJung/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/RalfJung/subscriptions", "organizations_url": "https://api.github.com/users/RalfJung/orgs", "repos_url": "https://api.github.com/users/RalfJung/repos", "events_url": "https://api.github.com/users/RalfJung/events{/privacy}", "received_events_url": "https://api.github.com/users/RalfJung/received_events", "type": "User", "site_admin": false}, "committer": {"login": "web-flow", "id": 19864447, "node_id": "MDQ6VXNlcjE5ODY0NDQ3", "avatar_url": "https://avatars.githubusercontent.com/u/19864447?v=4", "gravatar_id": "", "url": "https://api.github.com/users/web-flow", "html_url": "https://github.com/web-flow", "followers_url": "https://api.github.com/users/web-flow/followers", "following_url": "https://api.github.com/users/web-flow/following{/other_user}", "gists_url": "https://api.github.com/users/web-flow/gists{/gist_id}", "starred_url": "https://api.github.com/users/web-flow/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/web-flow/subscriptions", "organizations_url": "https://api.github.com/users/web-flow/orgs", "repos_url": "https://api.github.com/users/web-flow/repos", "events_url": "https://api.github.com/users/web-flow/events{/privacy}", "received_events_url": "https://api.github.com/users/web-flow/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "94d96b1d75d242bf6fcf00e375ac8ea9d5f67bd8", "url": "https://api.github.com/repos/rust-lang/rust/commits/94d96b1d75d242bf6fcf00e375ac8ea9d5f67bd8", "html_url": "https://github.com/rust-lang/rust/commit/94d96b1d75d242bf6fcf00e375ac8ea9d5f67bd8"}, {"sha": "52bb09abba48f1e68421c8fe18b30cee011c84ad", "url": "https://api.github.com/repos/rust-lang/rust/commits/52bb09abba48f1e68421c8fe18b30cee011c84ad", "html_url": "https://github.com/rust-lang/rust/commit/52bb09abba48f1e68421c8fe18b30cee011c84ad"}], "stats": {"total": 190, "additions": 115, "deletions": 75}, "files": [{"sha": "b21524cb9bdd2f2258df3ecf5c4d24ac96be4b16", "filename": "src/librustc_parse/parser/mod.rs", "status": "modified", "additions": 113, "deletions": 73, "changes": 186, "blob_url": "https://github.com/rust-lang/rust/blob/8692c455958d2cd70e1a20c765ecde143dbf453c/src%2Flibrustc_parse%2Fparser%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/8692c455958d2cd70e1a20c765ecde143dbf453c/src%2Flibrustc_parse%2Fparser%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_parse%2Fparser%2Fmod.rs?ref=8692c455958d2cd70e1a20c765ecde143dbf453c", "patch": "@@ -118,6 +118,8 @@ impl<'a> Drop for Parser<'a> {\n struct TokenCursor {\n     frame: TokenCursorFrame,\n     stack: Vec<TokenCursorFrame>,\n+    cur_token: Option<TreeAndJoint>,\n+    collecting: Option<Collecting>,\n }\n \n #[derive(Clone)]\n@@ -127,30 +129,24 @@ struct TokenCursorFrame {\n     open_delim: bool,\n     tree_cursor: tokenstream::Cursor,\n     close_delim: bool,\n-    last_token: LastToken,\n }\n \n-/// This is used in `TokenCursorFrame` above to track tokens that are consumed\n-/// by the parser, and then that's transitively used to record the tokens that\n-/// each parse AST item is created with.\n-///\n-/// Right now this has two states, either collecting tokens or not collecting\n-/// tokens. If we're collecting tokens we just save everything off into a local\n-/// `Vec`. This should eventually though likely save tokens from the original\n-/// token stream and just use slicing of token streams to avoid creation of a\n-/// whole new vector.\n-///\n-/// The second state is where we're passively not recording tokens, but the last\n-/// token is still tracked for when we want to start recording tokens. This\n-/// \"last token\" means that when we start recording tokens we'll want to ensure\n-/// that this, the first token, is included in the output.\n-///\n-/// You can find some more example usage of this in the `collect_tokens` method\n-/// on the parser.\n-#[derive(Clone)]\n-enum LastToken {\n-    Collecting(Vec<TreeAndJoint>),\n-    Was(Option<TreeAndJoint>),\n+/// Used to track additional state needed by `collect_tokens`\n+#[derive(Clone, Debug)]\n+struct Collecting {\n+    /// Holds the current tokens captured during the most\n+    /// recent call to `collect_tokens`\n+    buf: Vec<TreeAndJoint>,\n+    /// The depth of the `TokenCursor` stack at the time\n+    /// collection was started. When we encounter a `TokenTree::Delimited`,\n+    /// we want to record the `TokenTree::Delimited` itself,\n+    /// but *not* any of the inner tokens while we are inside\n+    /// the new frame (this would cause us to record duplicate tokens).\n+    ///\n+    /// This `depth` fields tracks stack depth we are recording tokens.\n+    /// Only tokens encountered at this depth will be recorded. See\n+    /// `TokenCursor::next` for more details.\n+    depth: usize,\n }\n \n impl TokenCursorFrame {\n@@ -161,7 +157,6 @@ impl TokenCursorFrame {\n             open_delim: delim == token::NoDelim,\n             tree_cursor: tts.clone().into_trees(),\n             close_delim: delim == token::NoDelim,\n-            last_token: LastToken::Was(None),\n         }\n     }\n }\n@@ -171,25 +166,38 @@ impl TokenCursor {\n         loop {\n             let tree = if !self.frame.open_delim {\n                 self.frame.open_delim = true;\n-                TokenTree::open_tt(self.frame.span, self.frame.delim)\n-            } else if let Some(tree) = self.frame.tree_cursor.next() {\n+                TokenTree::open_tt(self.frame.span, self.frame.delim).into()\n+            } else if let Some(tree) = self.frame.tree_cursor.next_with_joint() {\n                 tree\n             } else if !self.frame.close_delim {\n                 self.frame.close_delim = true;\n-                TokenTree::close_tt(self.frame.span, self.frame.delim)\n+                TokenTree::close_tt(self.frame.span, self.frame.delim).into()\n             } else if let Some(frame) = self.stack.pop() {\n                 self.frame = frame;\n                 continue;\n             } else {\n                 return Token::new(token::Eof, DUMMY_SP);\n             };\n \n-            match self.frame.last_token {\n-                LastToken::Collecting(ref mut v) => v.push(tree.clone().into()),\n-                LastToken::Was(ref mut t) => *t = Some(tree.clone().into()),\n+            // Don't set an open delimiter as our current token - we want\n+            // to leave it as the full `TokenTree::Delimited` from the previous\n+            // iteration of this loop\n+            if !matches!(tree.0, TokenTree::Token(Token { kind: TokenKind::OpenDelim(_), .. })) {\n+                self.cur_token = Some(tree.clone());\n+            }\n+\n+            if let Some(collecting) = &mut self.collecting {\n+                if collecting.depth == self.stack.len() {\n+                    debug!(\n+                        \"TokenCursor::next():  collected {:?} at depth {:?}\",\n+                        tree,\n+                        self.stack.len()\n+                    );\n+                    collecting.buf.push(tree.clone().into())\n+                }\n             }\n \n-            match tree {\n+            match tree.0 {\n                 TokenTree::Token(token) => return token,\n                 TokenTree::Delimited(sp, delim, tts) => {\n                     let frame = TokenCursorFrame::new(sp, delim, &tts);\n@@ -350,6 +358,8 @@ impl<'a> Parser<'a> {\n             token_cursor: TokenCursor {\n                 frame: TokenCursorFrame::new(DelimSpan::dummy(), token::NoDelim, &tokens),\n                 stack: Vec::new(),\n+                cur_token: None,\n+                collecting: None,\n             },\n             desugar_doc_comments,\n             unmatched_angle_bracket_count: 0,\n@@ -1105,65 +1115,95 @@ impl<'a> Parser<'a> {\n         }\n     }\n \n+    /// Records all tokens consumed by the provided callback,\n+    /// including the current token. These tokens are collected\n+    /// into a `TokenStream`, and returned along with the result\n+    /// of the callback.\n+    ///\n+    /// Note: If your callback consumes an opening delimiter\n+    /// (including the case where you call `collect_tokens`\n+    /// when the current token is an opening delimeter),\n+    /// you must also consume the corresponding closing delimiter.\n+    ///\n+    /// That is, you can consume\n+    /// `something ([{ }])` or `([{}])`, but not `([{}]`\n+    ///\n+    /// This restriction shouldn't be an issue in practice,\n+    /// since this function is used to record the tokens for\n+    /// a parsed AST item, which always has matching delimiters.\n     fn collect_tokens<R>(\n         &mut self,\n         f: impl FnOnce(&mut Self) -> PResult<'a, R>,\n     ) -> PResult<'a, (R, TokenStream)> {\n         // Record all tokens we parse when parsing this item.\n-        let mut tokens = Vec::new();\n-        let prev_collecting = match self.token_cursor.frame.last_token {\n-            LastToken::Collecting(ref mut list) => Some(mem::take(list)),\n-            LastToken::Was(ref mut last) => {\n-                tokens.extend(last.take());\n-                None\n-            }\n-        };\n-        self.token_cursor.frame.last_token = LastToken::Collecting(tokens);\n-        let prev = self.token_cursor.stack.len();\n+        let tokens: Vec<TreeAndJoint> = self.token_cursor.cur_token.clone().into_iter().collect();\n+        debug!(\"collect_tokens: starting with {:?}\", tokens);\n+\n+        // We need special handling for the case where `collect_tokens` is called\n+        // on an opening delimeter (e.g. '('). At this point, we have already pushed\n+        // a new frame - however, we want to record the original `TokenTree::Delimited`,\n+        // for consistency with the case where we start recording one token earlier.\n+        // See `TokenCursor::next` to see how `cur_token` is set up.\n+        let prev_depth =\n+            if matches!(self.token_cursor.cur_token, Some((TokenTree::Delimited(..), _))) {\n+                if self.token_cursor.stack.is_empty() {\n+                    // There is nothing below us in the stack that\n+                    // the function could consume, so the only thing it can legally\n+                    // capture is the entire contents of the current frame.\n+                    return Ok((f(self)?, TokenStream::new(tokens)));\n+                }\n+                // We have already recorded the full `TokenTree::Delimited` when we created\n+                // our `tokens` vector at the start of this function. We are now inside\n+                // a new frame corresponding to the `TokenTree::Delimited` we already recoreded.\n+                // We don't want to record any of the tokens inside this frame, since they\n+                // will be duplicates of the tokens nested inside the `TokenTree::Delimited`.\n+                // Therefore, we set our recording depth to the *previous* frame. This allows\n+                // us to record a sequence like: `(foo).bar()`: the `(foo)` will be recored\n+                // as our initial `cur_token`, while the `.bar()` will be recored after we\n+                // pop the `(foo)` frame.\n+                self.token_cursor.stack.len() - 1\n+            } else {\n+                self.token_cursor.stack.len()\n+            };\n+        let prev_collecting =\n+            self.token_cursor.collecting.replace(Collecting { buf: tokens, depth: prev_depth });\n+\n         let ret = f(self);\n-        let last_token = if self.token_cursor.stack.len() == prev {\n-            &mut self.token_cursor.frame.last_token\n-        } else if self.token_cursor.stack.get(prev).is_none() {\n-            // This can happen due to a bad interaction of two unrelated recovery mechanisms with\n-            // mismatched delimiters *and* recovery lookahead on the likely typo `pub ident(`\n-            // (#62881).\n-            return Ok((ret?, TokenStream::default()));\n+\n+        let mut collected_tokens = if let Some(collecting) = self.token_cursor.collecting.take() {\n+            collecting.buf\n         } else {\n-            &mut self.token_cursor.stack[prev].last_token\n+            let msg = format!(\"our vector went away?\");\n+            debug!(\"collect_tokens: {}\", msg);\n+            self.sess.span_diagnostic.delay_span_bug(self.token.span, &msg);\n+            // This can happen due to a bad interaction of two unrelated recovery mechanisms\n+            // with mismatched delimiters *and* recovery lookahead on the likely typo\n+            // `pub ident(` (#62895, different but similar to the case above).\n+            return Ok((ret?, TokenStream::default()));\n         };\n \n-        // Pull out the tokens that we've collected from the call to `f` above.\n-        let mut collected_tokens = match *last_token {\n-            LastToken::Collecting(ref mut v) => mem::take(v),\n-            LastToken::Was(ref was) => {\n-                let msg = format!(\"our vector went away? - found Was({:?})\", was);\n-                debug!(\"collect_tokens: {}\", msg);\n-                self.sess.span_diagnostic.delay_span_bug(self.token.span, &msg);\n-                // This can happen due to a bad interaction of two unrelated recovery mechanisms\n-                // with mismatched delimiters *and* recovery lookahead on the likely typo\n-                // `pub ident(` (#62895, different but similar to the case above).\n-                return Ok((ret?, TokenStream::default()));\n-            }\n-        };\n+        debug!(\"collect_tokens: got raw tokens {:?}\", collected_tokens);\n \n         // If we're not at EOF our current token wasn't actually consumed by\n         // `f`, but it'll still be in our list that we pulled out. In that case\n         // put it back.\n         let extra_token = if self.token != token::Eof { collected_tokens.pop() } else { None };\n \n-        // If we were previously collecting tokens, then this was a recursive\n-        // call. In that case we need to record all the tokens we collected in\n-        // our parent list as well. To do that we push a clone of our stream\n-        // onto the previous list.\n-        match prev_collecting {\n-            Some(mut list) => {\n-                list.extend(collected_tokens.iter().cloned());\n-                list.extend(extra_token);\n-                *last_token = LastToken::Collecting(list);\n-            }\n-            None => {\n-                *last_token = LastToken::Was(extra_token);\n+        if let Some(mut collecting) = prev_collecting {\n+            // If we were previously collecting at the same depth,\n+            // then the previous call to `collect_tokens` needs to see\n+            // the tokens we just recorded.\n+            //\n+            // If we were previously recording at an lower `depth`,\n+            // then the previous `collect_tokens` call already recorded\n+            // this entire frame in the form of a `TokenTree::Delimited`,\n+            // so there is nothing else for us to do.\n+            if collecting.depth == prev_depth {\n+                collecting.buf.extend(collected_tokens.iter().cloned());\n+                collecting.buf.extend(extra_token);\n+                debug!(\"collect_tokens: updating previous buf to {:?}\", collecting);\n             }\n+            self.token_cursor.collecting = Some(collecting)\n         }\n \n         Ok((ret?, TokenStream::new(collected_tokens)))"}, {"sha": "f60b6a00be1298f3afb7fc3ab5fbf1d460084dd6", "filename": "src/test/ui/ast-json/ast-json-noexpand-output.stdout", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/8692c455958d2cd70e1a20c765ecde143dbf453c/src%2Ftest%2Fui%2Fast-json%2Fast-json-noexpand-output.stdout", "raw_url": "https://github.com/rust-lang/rust/raw/8692c455958d2cd70e1a20c765ecde143dbf453c/src%2Ftest%2Fui%2Fast-json%2Fast-json-noexpand-output.stdout", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ftest%2Fui%2Fast-json%2Fast-json-noexpand-output.stdout?ref=8692c455958d2cd70e1a20c765ecde143dbf453c", "patch": "@@ -1 +1 @@\n-{\"module\":{\"inner\":{\"lo\":0,\"hi\":0},\"items\":[{\"attrs\":[],\"id\":0,\"span\":{\"lo\":0,\"hi\":0},\"vis\":{\"node\":\"Inherited\",\"span\":{\"lo\":0,\"hi\":0}},\"ident\":{\"name\":\"core\",\"span\":{\"lo\":0,\"hi\":0}},\"kind\":{\"variant\":\"ExternCrate\",\"fields\":[null]},\"tokens\":{\"_field0\":[[{\"variant\":\"Token\",\"fields\":[{\"kind\":{\"variant\":\"Ident\",\"fields\":[\"extern\",false]},\"span\":{\"lo\":0,\"hi\":0}}]},\"NonJoint\"],[{\"variant\":\"Token\",\"fields\":[{\"kind\":{\"variant\":\"Ident\",\"fields\":[\"crate\",false]},\"span\":{\"lo\":0,\"hi\":0}}]},\"NonJoint\"],[{\"variant\":\"Token\",\"fields\":[{\"kind\":{\"variant\":\"Ident\",\"fields\":[\"core\",false]},\"span\":{\"lo\":0,\"hi\":0}}]},\"NonJoint\"],[{\"variant\":\"Token\",\"fields\":[{\"kind\":\"Semi\",\"span\":{\"lo\":0,\"hi\":0}}]},\"NonJoint\"]]}}],\"inline\":true},\"attrs\":[{\"kind\":{\"variant\":\"Normal\",\"fields\":[{\"path\":{\"span\":{\"lo\":0,\"hi\":0},\"segments\":[{\"ident\":{\"name\":\"crate_type\",\"span\":{\"lo\":0,\"hi\":0}},\"id\":0,\"args\":null}]},\"args\":{\"variant\":\"Eq\",\"fields\":[{\"lo\":0,\"hi\":0},{\"_field0\":[[{\"variant\":\"Token\",\"fields\":[{\"kind\":{\"variant\":\"Literal\",\"fields\":[{\"kind\":\"Str\",\"symbol\":\"lib\",\"suffix\":null}]},\"span\":{\"lo\":0,\"hi\":0}}]},\"NonJoint\"]]}]}}]},\"id\":null,\"style\":\"Inner\",\"span\":{\"lo\":0,\"hi\":0}}],\"span\":{\"lo\":0,\"hi\":0},\"proc_macros\":[]}\n+{\"module\":{\"inner\":{\"lo\":0,\"hi\":0},\"items\":[{\"attrs\":[],\"id\":0,\"span\":{\"lo\":0,\"hi\":0},\"vis\":{\"node\":\"Inherited\",\"span\":{\"lo\":0,\"hi\":0}},\"ident\":{\"name\":\"core\",\"span\":{\"lo\":0,\"hi\":0}},\"kind\":{\"variant\":\"ExternCrate\",\"fields\":[null]},\"tokens\":{\"_field0\":[[{\"variant\":\"Token\",\"fields\":[{\"kind\":{\"variant\":\"Ident\",\"fields\":[\"extern\",false]},\"span\":{\"lo\":0,\"hi\":0}}]},\"NonJoint\"],[{\"variant\":\"Token\",\"fields\":[{\"kind\":{\"variant\":\"Ident\",\"fields\":[\"crate\",false]},\"span\":{\"lo\":0,\"hi\":0}}]},\"NonJoint\"],[{\"variant\":\"Token\",\"fields\":[{\"kind\":{\"variant\":\"Ident\",\"fields\":[\"core\",false]},\"span\":{\"lo\":0,\"hi\":0}}]},\"Joint\"],[{\"variant\":\"Token\",\"fields\":[{\"kind\":\"Semi\",\"span\":{\"lo\":0,\"hi\":0}}]},\"NonJoint\"]]}}],\"inline\":true},\"attrs\":[{\"kind\":{\"variant\":\"Normal\",\"fields\":[{\"path\":{\"span\":{\"lo\":0,\"hi\":0},\"segments\":[{\"ident\":{\"name\":\"crate_type\",\"span\":{\"lo\":0,\"hi\":0}},\"id\":0,\"args\":null}]},\"args\":{\"variant\":\"Eq\",\"fields\":[{\"lo\":0,\"hi\":0},{\"_field0\":[[{\"variant\":\"Token\",\"fields\":[{\"kind\":{\"variant\":\"Literal\",\"fields\":[{\"kind\":\"Str\",\"symbol\":\"lib\",\"suffix\":null}]},\"span\":{\"lo\":0,\"hi\":0}}]},\"NonJoint\"]]}]}}]},\"id\":null,\"style\":\"Inner\",\"span\":{\"lo\":0,\"hi\":0}}],\"span\":{\"lo\":0,\"hi\":0},\"proc_macros\":[]}"}, {"sha": "42e7e789980634d3eb17b435e965b80785a858bf", "filename": "src/test/ui/ast-json/ast-json-output.stdout", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/8692c455958d2cd70e1a20c765ecde143dbf453c/src%2Ftest%2Fui%2Fast-json%2Fast-json-output.stdout", "raw_url": "https://github.com/rust-lang/rust/raw/8692c455958d2cd70e1a20c765ecde143dbf453c/src%2Ftest%2Fui%2Fast-json%2Fast-json-output.stdout", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ftest%2Fui%2Fast-json%2Fast-json-output.stdout?ref=8692c455958d2cd70e1a20c765ecde143dbf453c", "patch": "@@ -1 +1 @@\n-{\"module\":{\"inner\":{\"lo\":0,\"hi\":0},\"items\":[{\"attrs\":[{\"kind\":{\"variant\":\"Normal\",\"fields\":[{\"path\":{\"span\":{\"lo\":0,\"hi\":0},\"segments\":[{\"ident\":{\"name\":\"prelude_import\",\"span\":{\"lo\":0,\"hi\":0}},\"id\":0,\"args\":null}]},\"args\":\"Empty\"}]},\"id\":null,\"style\":\"Outer\",\"span\":{\"lo\":0,\"hi\":0}}],\"id\":0,\"span\":{\"lo\":0,\"hi\":0},\"vis\":{\"node\":\"Inherited\",\"span\":{\"lo\":0,\"hi\":0}},\"ident\":{\"name\":\"\",\"span\":{\"lo\":0,\"hi\":0}},\"kind\":{\"variant\":\"Use\",\"fields\":[{\"prefix\":{\"span\":{\"lo\":0,\"hi\":0},\"segments\":[{\"ident\":{\"name\":\"{{root}}\",\"span\":{\"lo\":0,\"hi\":0}},\"id\":0,\"args\":null},{\"ident\":{\"name\":\"std\",\"span\":{\"lo\":0,\"hi\":0}},\"id\":0,\"args\":null},{\"ident\":{\"name\":\"prelude\",\"span\":{\"lo\":0,\"hi\":0}},\"id\":0,\"args\":null},{\"ident\":{\"name\":\"v1\",\"span\":{\"lo\":0,\"hi\":0}},\"id\":0,\"args\":null}]},\"kind\":\"Glob\",\"span\":{\"lo\":0,\"hi\":0}}]},\"tokens\":null},{\"attrs\":[{\"kind\":{\"variant\":\"Normal\",\"fields\":[{\"path\":{\"span\":{\"lo\":0,\"hi\":0},\"segments\":[{\"ident\":{\"name\":\"macro_use\",\"span\":{\"lo\":0,\"hi\":0}},\"id\":0,\"args\":null}]},\"args\":\"Empty\"}]},\"id\":null,\"style\":\"Outer\",\"span\":{\"lo\":0,\"hi\":0}}],\"id\":0,\"span\":{\"lo\":0,\"hi\":0},\"vis\":{\"node\":\"Inherited\",\"span\":{\"lo\":0,\"hi\":0}},\"ident\":{\"name\":\"std\",\"span\":{\"lo\":0,\"hi\":0}},\"kind\":{\"variant\":\"ExternCrate\",\"fields\":[null]},\"tokens\":null},{\"attrs\":[],\"id\":0,\"span\":{\"lo\":0,\"hi\":0},\"vis\":{\"node\":\"Inherited\",\"span\":{\"lo\":0,\"hi\":0}},\"ident\":{\"name\":\"core\",\"span\":{\"lo\":0,\"hi\":0}},\"kind\":{\"variant\":\"ExternCrate\",\"fields\":[null]},\"tokens\":{\"_field0\":[[{\"variant\":\"Token\",\"fields\":[{\"kind\":{\"variant\":\"Ident\",\"fields\":[\"extern\",false]},\"span\":{\"lo\":0,\"hi\":0}}]},\"NonJoint\"],[{\"variant\":\"Token\",\"fields\":[{\"kind\":{\"variant\":\"Ident\",\"fields\":[\"crate\",false]},\"span\":{\"lo\":0,\"hi\":0}}]},\"NonJoint\"],[{\"variant\":\"Token\",\"fields\":[{\"kind\":{\"variant\":\"Ident\",\"fields\":[\"core\",false]},\"span\":{\"lo\":0,\"hi\":0}}]},\"NonJoint\"],[{\"variant\":\"Token\",\"fields\":[{\"kind\":\"Semi\",\"span\":{\"lo\":0,\"hi\":0}}]},\"NonJoint\"]]}}],\"inline\":true},\"attrs\":[{\"kind\":{\"variant\":\"Normal\",\"fields\":[{\"path\":{\"span\":{\"lo\":0,\"hi\":0},\"segments\":[{\"ident\":{\"name\":\"crate_type\",\"span\":{\"lo\":0,\"hi\":0}},\"id\":0,\"args\":null}]},\"args\":{\"variant\":\"Eq\",\"fields\":[{\"lo\":0,\"hi\":0},{\"_field0\":[[{\"variant\":\"Token\",\"fields\":[{\"kind\":{\"variant\":\"Literal\",\"fields\":[{\"kind\":\"Str\",\"symbol\":\"lib\",\"suffix\":null}]},\"span\":{\"lo\":0,\"hi\":0}}]},\"NonJoint\"]]}]}}]},\"id\":null,\"style\":\"Inner\",\"span\":{\"lo\":0,\"hi\":0}}],\"span\":{\"lo\":0,\"hi\":0},\"proc_macros\":[]}\n+{\"module\":{\"inner\":{\"lo\":0,\"hi\":0},\"items\":[{\"attrs\":[{\"kind\":{\"variant\":\"Normal\",\"fields\":[{\"path\":{\"span\":{\"lo\":0,\"hi\":0},\"segments\":[{\"ident\":{\"name\":\"prelude_import\",\"span\":{\"lo\":0,\"hi\":0}},\"id\":0,\"args\":null}]},\"args\":\"Empty\"}]},\"id\":null,\"style\":\"Outer\",\"span\":{\"lo\":0,\"hi\":0}}],\"id\":0,\"span\":{\"lo\":0,\"hi\":0},\"vis\":{\"node\":\"Inherited\",\"span\":{\"lo\":0,\"hi\":0}},\"ident\":{\"name\":\"\",\"span\":{\"lo\":0,\"hi\":0}},\"kind\":{\"variant\":\"Use\",\"fields\":[{\"prefix\":{\"span\":{\"lo\":0,\"hi\":0},\"segments\":[{\"ident\":{\"name\":\"{{root}}\",\"span\":{\"lo\":0,\"hi\":0}},\"id\":0,\"args\":null},{\"ident\":{\"name\":\"std\",\"span\":{\"lo\":0,\"hi\":0}},\"id\":0,\"args\":null},{\"ident\":{\"name\":\"prelude\",\"span\":{\"lo\":0,\"hi\":0}},\"id\":0,\"args\":null},{\"ident\":{\"name\":\"v1\",\"span\":{\"lo\":0,\"hi\":0}},\"id\":0,\"args\":null}]},\"kind\":\"Glob\",\"span\":{\"lo\":0,\"hi\":0}}]},\"tokens\":null},{\"attrs\":[{\"kind\":{\"variant\":\"Normal\",\"fields\":[{\"path\":{\"span\":{\"lo\":0,\"hi\":0},\"segments\":[{\"ident\":{\"name\":\"macro_use\",\"span\":{\"lo\":0,\"hi\":0}},\"id\":0,\"args\":null}]},\"args\":\"Empty\"}]},\"id\":null,\"style\":\"Outer\",\"span\":{\"lo\":0,\"hi\":0}}],\"id\":0,\"span\":{\"lo\":0,\"hi\":0},\"vis\":{\"node\":\"Inherited\",\"span\":{\"lo\":0,\"hi\":0}},\"ident\":{\"name\":\"std\",\"span\":{\"lo\":0,\"hi\":0}},\"kind\":{\"variant\":\"ExternCrate\",\"fields\":[null]},\"tokens\":null},{\"attrs\":[],\"id\":0,\"span\":{\"lo\":0,\"hi\":0},\"vis\":{\"node\":\"Inherited\",\"span\":{\"lo\":0,\"hi\":0}},\"ident\":{\"name\":\"core\",\"span\":{\"lo\":0,\"hi\":0}},\"kind\":{\"variant\":\"ExternCrate\",\"fields\":[null]},\"tokens\":{\"_field0\":[[{\"variant\":\"Token\",\"fields\":[{\"kind\":{\"variant\":\"Ident\",\"fields\":[\"extern\",false]},\"span\":{\"lo\":0,\"hi\":0}}]},\"NonJoint\"],[{\"variant\":\"Token\",\"fields\":[{\"kind\":{\"variant\":\"Ident\",\"fields\":[\"crate\",false]},\"span\":{\"lo\":0,\"hi\":0}}]},\"NonJoint\"],[{\"variant\":\"Token\",\"fields\":[{\"kind\":{\"variant\":\"Ident\",\"fields\":[\"core\",false]},\"span\":{\"lo\":0,\"hi\":0}}]},\"Joint\"],[{\"variant\":\"Token\",\"fields\":[{\"kind\":\"Semi\",\"span\":{\"lo\":0,\"hi\":0}}]},\"NonJoint\"]]}}],\"inline\":true},\"attrs\":[{\"kind\":{\"variant\":\"Normal\",\"fields\":[{\"path\":{\"span\":{\"lo\":0,\"hi\":0},\"segments\":[{\"ident\":{\"name\":\"crate_type\",\"span\":{\"lo\":0,\"hi\":0}},\"id\":0,\"args\":null}]},\"args\":{\"variant\":\"Eq\",\"fields\":[{\"lo\":0,\"hi\":0},{\"_field0\":[[{\"variant\":\"Token\",\"fields\":[{\"kind\":{\"variant\":\"Literal\",\"fields\":[{\"kind\":\"Str\",\"symbol\":\"lib\",\"suffix\":null}]},\"span\":{\"lo\":0,\"hi\":0}}]},\"NonJoint\"]]}]}}]},\"id\":null,\"style\":\"Inner\",\"span\":{\"lo\":0,\"hi\":0}}],\"span\":{\"lo\":0,\"hi\":0},\"proc_macros\":[]}"}]}