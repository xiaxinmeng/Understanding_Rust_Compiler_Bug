{"sha": "17294d98b915364f64a8a1cbc071aa6ee200a03b", "node_id": "MDY6Q29tbWl0NzI0NzEyOjE3Mjk0ZDk4YjkxNTM2NGY2NGE4YTFjYmMwNzFhYTZlZTIwMGEwM2I=", "commit": {"author": {"name": "Alex Crichton", "email": "alex@alexcrichton.com", "date": "2017-01-19T23:49:34Z"}, "committer": {"name": "Alex Crichton", "email": "alex@alexcrichton.com", "date": "2017-01-20T16:35:47Z"}, "message": "Rollup merge of #39118 - jseyfried:token_tree_based_parser, r=nrc\n\nRefactor the parser to consume token trees\n\nThis is groundwork for efficiently parsing attribute proc macro invocations, bang macro invocations, and `TokenStream`-based attributes and fragment matchers.\n\nThis improves parsing performance by 8-15% and expansion performance by 0-5% on a sampling of the compiler's crates.\n\nr? @nrc", "tree": {"sha": "6a79c290e7f4ee37587b3b24fbd35a5f500a557b", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/6a79c290e7f4ee37587b3b24fbd35a5f500a557b"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/17294d98b915364f64a8a1cbc071aa6ee200a03b", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/17294d98b915364f64a8a1cbc071aa6ee200a03b", "html_url": "https://github.com/rust-lang/rust/commit/17294d98b915364f64a8a1cbc071aa6ee200a03b", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/17294d98b915364f64a8a1cbc071aa6ee200a03b/comments", "author": {"login": "alexcrichton", "id": 64996, "node_id": "MDQ6VXNlcjY0OTk2", "avatar_url": "https://avatars.githubusercontent.com/u/64996?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alexcrichton", "html_url": "https://github.com/alexcrichton", "followers_url": "https://api.github.com/users/alexcrichton/followers", "following_url": "https://api.github.com/users/alexcrichton/following{/other_user}", "gists_url": "https://api.github.com/users/alexcrichton/gists{/gist_id}", "starred_url": "https://api.github.com/users/alexcrichton/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alexcrichton/subscriptions", "organizations_url": "https://api.github.com/users/alexcrichton/orgs", "repos_url": "https://api.github.com/users/alexcrichton/repos", "events_url": "https://api.github.com/users/alexcrichton/events{/privacy}", "received_events_url": "https://api.github.com/users/alexcrichton/received_events", "type": "User", "site_admin": false}, "committer": {"login": "alexcrichton", "id": 64996, "node_id": "MDQ6VXNlcjY0OTk2", "avatar_url": "https://avatars.githubusercontent.com/u/64996?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alexcrichton", "html_url": "https://github.com/alexcrichton", "followers_url": "https://api.github.com/users/alexcrichton/followers", "following_url": "https://api.github.com/users/alexcrichton/following{/other_user}", "gists_url": "https://api.github.com/users/alexcrichton/gists{/gist_id}", "starred_url": "https://api.github.com/users/alexcrichton/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alexcrichton/subscriptions", "organizations_url": "https://api.github.com/users/alexcrichton/orgs", "repos_url": "https://api.github.com/users/alexcrichton/repos", "events_url": "https://api.github.com/users/alexcrichton/events{/privacy}", "received_events_url": "https://api.github.com/users/alexcrichton/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "dd5d85ea761e2d570682fccdb8608319d6bd2bf4", "url": "https://api.github.com/repos/rust-lang/rust/commits/dd5d85ea761e2d570682fccdb8608319d6bd2bf4", "html_url": "https://github.com/rust-lang/rust/commit/dd5d85ea761e2d570682fccdb8608319d6bd2bf4"}, {"sha": "0b9e26f390403aa95620d3b813f046732b371fb1", "url": "https://api.github.com/repos/rust-lang/rust/commits/0b9e26f390403aa95620d3b813f046732b371fb1", "html_url": "https://github.com/rust-lang/rust/commit/0b9e26f390403aa95620d3b813f046732b371fb1"}], "stats": {"total": 840, "additions": 368, "deletions": 472}, "files": [{"sha": "58c677fb507fe5346be4eabac35a319ffadb2d45", "filename": "src/librustc/hir/print.rs", "status": "modified", "additions": 5, "deletions": 8, "changes": 13, "blob_url": "https://github.com/rust-lang/rust/blob/17294d98b915364f64a8a1cbc071aa6ee200a03b/src%2Flibrustc%2Fhir%2Fprint.rs", "raw_url": "https://github.com/rust-lang/rust/raw/17294d98b915364f64a8a1cbc071aa6ee200a03b/src%2Flibrustc%2Fhir%2Fprint.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Fhir%2Fprint.rs?ref=17294d98b915364f64a8a1cbc071aa6ee200a03b", "patch": "@@ -13,6 +13,7 @@ pub use self::AnnNode::*;\n use syntax::abi::Abi;\n use syntax::ast;\n use syntax::codemap::{CodeMap, Spanned};\n+use syntax::parse::ParseSess;\n use syntax::parse::lexer::comments;\n use syntax::print::pp::{self, break_offset, word, space, hardbreak};\n use syntax::print::pp::{Breaks, eof};\n@@ -21,7 +22,6 @@ use syntax::print::pprust::{self as ast_pp, PrintState};\n use syntax::ptr::P;\n use syntax::symbol::keywords;\n use syntax_pos::{self, BytePos};\n-use errors;\n \n use hir;\n use hir::{PatKind, RegionTyParamBound, TraitTyParamBound, TraitBoundModifier};\n@@ -116,16 +116,15 @@ pub const default_columns: usize = 78;\n /// it can scan the input text for comments and literals to\n /// copy forward.\n pub fn print_crate<'a>(cm: &'a CodeMap,\n-                       span_diagnostic: &errors::Handler,\n+                       sess: &ParseSess,\n                        krate: &hir::Crate,\n                        filename: String,\n                        input: &mut Read,\n                        out: Box<Write + 'a>,\n                        ann: &'a PpAnn,\n                        is_expanded: bool)\n                        -> io::Result<()> {\n-    let mut s = State::new_from_input(cm, span_diagnostic, filename, input,\n-                                      out, ann, is_expanded);\n+    let mut s = State::new_from_input(cm, sess, filename, input, out, ann, is_expanded);\n \n     // When printing the AST, we sometimes need to inject `#[no_std]` here.\n     // Since you can't compile the HIR, it's not necessary.\n@@ -137,16 +136,14 @@ pub fn print_crate<'a>(cm: &'a CodeMap,\n \n impl<'a> State<'a> {\n     pub fn new_from_input(cm: &'a CodeMap,\n-                          span_diagnostic: &errors::Handler,\n+                          sess: &ParseSess,\n                           filename: String,\n                           input: &mut Read,\n                           out: Box<Write + 'a>,\n                           ann: &'a PpAnn,\n                           is_expanded: bool)\n                           -> State<'a> {\n-        let (cmnts, lits) = comments::gather_comments_and_literals(span_diagnostic,\n-                                                                   filename,\n-                                                                   input);\n+        let (cmnts, lits) = comments::gather_comments_and_literals(sess, filename, input);\n \n         State::new(cm,\n                    out,"}, {"sha": "7d8f7fcefe6391f697bb6456ab588a78a0bba5f1", "filename": "src/librustc/session/config.rs", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "blob_url": "https://github.com/rust-lang/rust/blob/17294d98b915364f64a8a1cbc071aa6ee200a03b/src%2Flibrustc%2Fsession%2Fconfig.rs", "raw_url": "https://github.com/rust-lang/rust/raw/17294d98b915364f64a8a1cbc071aa6ee200a03b/src%2Flibrustc%2Fsession%2Fconfig.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Fsession%2Fconfig.rs?ref=17294d98b915364f64a8a1cbc071aa6ee200a03b", "patch": "@@ -25,6 +25,7 @@ use lint;\n use middle::cstore;\n \n use syntax::ast::{self, IntTy, UintTy};\n+use syntax::parse::token;\n use syntax::parse;\n use syntax::symbol::Symbol;\n use syntax::feature_gate::UnstableFeatures;\n@@ -1259,7 +1260,7 @@ pub fn parse_cfgspecs(cfgspecs: Vec<String> ) -> ast::CrateConfig {\n \n         let meta_item = panictry!(parser.parse_meta_item());\n \n-        if !parser.reader.is_eof() {\n+        if parser.token != token::Eof {\n             early_error(ErrorOutputType::default(), &format!(\"invalid --cfg argument: {}\", s))\n         } else if meta_item.is_meta_item_list() {\n             let msg ="}, {"sha": "3c8a529bdaee875b980094ca3cacb6a9b6074123", "filename": "src/librustc_driver/pretty.rs", "status": "modified", "additions": 4, "deletions": 4, "changes": 8, "blob_url": "https://github.com/rust-lang/rust/blob/17294d98b915364f64a8a1cbc071aa6ee200a03b/src%2Flibrustc_driver%2Fpretty.rs", "raw_url": "https://github.com/rust-lang/rust/raw/17294d98b915364f64a8a1cbc071aa6ee200a03b/src%2Flibrustc_driver%2Fpretty.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_driver%2Fpretty.rs?ref=17294d98b915364f64a8a1cbc071aa6ee200a03b", "patch": "@@ -838,7 +838,7 @@ pub fn print_after_parsing(sess: &Session,\n                 debug!(\"pretty printing source code {:?}\", s);\n                 let sess = annotation.sess();\n                 pprust::print_crate(sess.codemap(),\n-                                    sess.diagnostic(),\n+                                    &sess.parse_sess,\n                                     krate,\n                                     src_name.to_string(),\n                                     &mut rdr,\n@@ -896,7 +896,7 @@ pub fn print_after_hir_lowering<'tcx, 'a: 'tcx>(sess: &'a Session,\n                     debug!(\"pretty printing source code {:?}\", s);\n                     let sess = annotation.sess();\n                     pprust::print_crate(sess.codemap(),\n-                                        sess.diagnostic(),\n+                                        &sess.parse_sess,\n                                         krate,\n                                         src_name.to_string(),\n                                         &mut rdr,\n@@ -920,7 +920,7 @@ pub fn print_after_hir_lowering<'tcx, 'a: 'tcx>(sess: &'a Session,\n                     debug!(\"pretty printing source code {:?}\", s);\n                     let sess = annotation.sess();\n                     pprust_hir::print_crate(sess.codemap(),\n-                                            sess.diagnostic(),\n+                                            &sess.parse_sess,\n                                             krate,\n                                             src_name.to_string(),\n                                             &mut rdr,\n@@ -945,7 +945,7 @@ pub fn print_after_hir_lowering<'tcx, 'a: 'tcx>(sess: &'a Session,\n                     let sess = annotation.sess();\n                     let ast_map = annotation.ast_map().expect(\"--unpretty missing HIR map\");\n                     let mut pp_state = pprust_hir::State::new_from_input(sess.codemap(),\n-                                                                         sess.diagnostic(),\n+                                                                         &sess.parse_sess,\n                                                                          src_name.to_string(),\n                                                                          &mut rdr,\n                                                                          box out,"}, {"sha": "f6107bc1359153188aad49a5ccd1cc7225860910", "filename": "src/librustc_metadata/cstore_impl.rs", "status": "modified", "additions": 4, "deletions": 14, "changes": 18, "blob_url": "https://github.com/rust-lang/rust/blob/17294d98b915364f64a8a1cbc071aa6ee200a03b/src%2Flibrustc_metadata%2Fcstore_impl.rs", "raw_url": "https://github.com/rust-lang/rust/raw/17294d98b915364f64a8a1cbc071aa6ee200a03b/src%2Flibrustc_metadata%2Fcstore_impl.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_metadata%2Fcstore_impl.rs?ref=17294d98b915364f64a8a1cbc071aa6ee200a03b", "patch": "@@ -29,7 +29,7 @@ use rustc_back::PanicStrategy;\n \n use syntax::ast;\n use syntax::attr;\n-use syntax::parse::new_parser_from_source_str;\n+use syntax::parse::filemap_to_tts;\n use syntax::symbol::Symbol;\n use syntax_pos::{mk_sp, Span};\n use rustc::hir::svh::Svh;\n@@ -399,19 +399,9 @@ impl<'tcx> CrateStore<'tcx> for cstore::CStore {\n         let (name, def) = data.get_macro(id.index);\n         let source_name = format!(\"<{} macros>\", name);\n \n-        // NB: Don't use parse_tts_from_source_str because it parses with quote_depth > 0.\n-        let mut parser = new_parser_from_source_str(&sess.parse_sess, source_name, def.body);\n-\n-        let lo = parser.span.lo;\n-        let body = match parser.parse_all_token_trees() {\n-            Ok(body) => body,\n-            Err(mut err) => {\n-                err.emit();\n-                sess.abort_if_errors();\n-                unreachable!();\n-            }\n-        };\n-        let local_span = mk_sp(lo, parser.prev_span.hi);\n+        let filemap = sess.parse_sess.codemap().new_filemap(source_name, None, def.body);\n+        let local_span = mk_sp(filemap.start_pos, filemap.end_pos);\n+        let body = filemap_to_tts(&sess.parse_sess, filemap);\n \n         // Mark the attrs as used\n         let attrs = data.get_item_attrs(id.index);"}, {"sha": "89525b27ed36af7d5f50d4e1b1b6c152a3fd7c2b", "filename": "src/librustc_save_analysis/span_utils.rs", "status": "modified", "additions": 6, "deletions": 7, "changes": 13, "blob_url": "https://github.com/rust-lang/rust/blob/17294d98b915364f64a8a1cbc071aa6ee200a03b/src%2Flibrustc_save_analysis%2Fspan_utils.rs", "raw_url": "https://github.com/rust-lang/rust/raw/17294d98b915364f64a8a1cbc071aa6ee200a03b/src%2Flibrustc_save_analysis%2Fspan_utils.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_save_analysis%2Fspan_utils.rs?ref=17294d98b915364f64a8a1cbc071aa6ee200a03b", "patch": "@@ -17,9 +17,9 @@ use std::env;\n use std::path::Path;\n \n use syntax::ast;\n-use syntax::parse::lexer::{self, Reader, StringReader};\n+use syntax::parse::filemap_to_tts;\n+use syntax::parse::lexer::{self, StringReader};\n use syntax::parse::token::{self, Token};\n-use syntax::parse::parser::Parser;\n use syntax::symbol::keywords;\n use syntax::tokenstream::TokenTree;\n use syntax_pos::*;\n@@ -85,14 +85,13 @@ impl<'a> SpanUtils<'a> {\n         let filemap = self.sess\n                           .codemap()\n                           .new_filemap(String::from(\"<anon-dxr>\"), None, self.snippet(span));\n-        let s = self.sess;\n-        lexer::StringReader::new(s.diagnostic(), filemap)\n+        lexer::StringReader::new(&self.sess.parse_sess, filemap)\n     }\n \n     fn span_to_tts(&self, span: Span) -> Vec<TokenTree> {\n-        let srdr = self.retokenise_span(span);\n-        let mut p = Parser::new(&self.sess.parse_sess, Box::new(srdr), None, false);\n-        p.parse_all_token_trees().expect(\"Couldn't re-parse span\")\n+        let filename = String::from(\"<anon-dxr>\");\n+        let filemap = self.sess.codemap().new_filemap(filename, None, self.snippet(span));\n+        filemap_to_tts(&self.sess.parse_sess, filemap)\n     }\n \n     // Re-parses a path and returns the span for the last identifier in the path"}, {"sha": "0629e93e7ef5d1f222dea269492fef4d150ad7e7", "filename": "src/librustdoc/html/highlight.rs", "status": "modified", "additions": 7, "deletions": 9, "changes": 16, "blob_url": "https://github.com/rust-lang/rust/blob/17294d98b915364f64a8a1cbc071aa6ee200a03b/src%2Flibrustdoc%2Fhtml%2Fhighlight.rs", "raw_url": "https://github.com/rust-lang/rust/raw/17294d98b915364f64a8a1cbc071aa6ee200a03b/src%2Flibrustdoc%2Fhtml%2Fhighlight.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustdoc%2Fhtml%2Fhighlight.rs?ref=17294d98b915364f64a8a1cbc071aa6ee200a03b", "patch": "@@ -27,7 +27,7 @@ use std::io;\n use std::io::prelude::*;\n \n use syntax::codemap::CodeMap;\n-use syntax::parse::lexer::{self, Reader, TokenAndSpan};\n+use syntax::parse::lexer::{self, TokenAndSpan};\n use syntax::parse::token;\n use syntax::parse;\n use syntax_pos::Span;\n@@ -42,8 +42,7 @@ pub fn render_with_highlighting(src: &str, class: Option<&str>, id: Option<&str>\n     let mut out = Vec::new();\n     write_header(class, id, &mut out).unwrap();\n \n-    let mut classifier = Classifier::new(lexer::StringReader::new(&sess.span_diagnostic, fm),\n-                                         sess.codemap());\n+    let mut classifier = Classifier::new(lexer::StringReader::new(&sess, fm), sess.codemap());\n     if let Err(_) = classifier.write_source(&mut out) {\n         return format!(\"<pre>{}</pre>\", src);\n     }\n@@ -63,8 +62,7 @@ pub fn render_inner_with_highlighting(src: &str) -> io::Result<String> {\n     let fm = sess.codemap().new_filemap(\"<stdin>\".to_string(), None, src.to_string());\n \n     let mut out = Vec::new();\n-    let mut classifier = Classifier::new(lexer::StringReader::new(&sess.span_diagnostic, fm),\n-                                         sess.codemap());\n+    let mut classifier = Classifier::new(lexer::StringReader::new(&sess, fm), sess.codemap());\n     classifier.write_source(&mut out)?;\n \n     Ok(String::from_utf8_lossy(&out).into_owned())\n@@ -185,10 +183,10 @@ impl<'a> Classifier<'a> {\n                 Ok(tas) => tas,\n                 Err(_) => {\n                     self.lexer.emit_fatal_errors();\n-                    self.lexer.span_diagnostic.struct_warn(\"Backing out of syntax highlighting\")\n-                                              .note(\"You probably did not intend to render this \\\n-                                                     as a rust code-block\")\n-                                              .emit();\n+                    self.lexer.sess.span_diagnostic\n+                        .struct_warn(\"Backing out of syntax highlighting\")\n+                        .note(\"You probably did not intend to render this as a rust code-block\")\n+                        .emit();\n                     return Err(io::Error::new(io::ErrorKind::Other, \"\"));\n                 }\n             };"}, {"sha": "edf74e1fe19f155d0a19075f42d50f4fb45ef801", "filename": "src/libsyntax/ext/base.rs", "status": "modified", "additions": 1, "deletions": 3, "changes": 4, "blob_url": "https://github.com/rust-lang/rust/blob/17294d98b915364f64a8a1cbc071aa6ee200a03b/src%2Flibsyntax%2Fext%2Fbase.rs", "raw_url": "https://github.com/rust-lang/rust/raw/17294d98b915364f64a8a1cbc071aa6ee200a03b/src%2Flibsyntax%2Fext%2Fbase.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fext%2Fbase.rs?ref=17294d98b915364f64a8a1cbc071aa6ee200a03b", "patch": "@@ -615,9 +615,7 @@ impl<'a> ExtCtxt<'a> {\n \n     pub fn new_parser_from_tts(&self, tts: &[tokenstream::TokenTree])\n         -> parser::Parser<'a> {\n-        let mut parser = parse::tts_to_parser(self.parse_sess, tts.to_vec());\n-        parser.allow_interpolated_tts = false; // FIXME(jseyfried) `quote!` can't handle these yet\n-        parser\n+        parse::tts_to_parser(self.parse_sess, tts.to_vec())\n     }\n     pub fn codemap(&self) -> &'a CodeMap { self.parse_sess.codemap() }\n     pub fn parse_sess(&self) -> &'a parse::ParseSess { self.parse_sess }"}, {"sha": "d748eec73e850b2798c35f67157d871bf9063c5f", "filename": "src/libsyntax/ext/expand.rs", "status": "modified", "additions": 3, "deletions": 7, "changes": 10, "blob_url": "https://github.com/rust-lang/rust/blob/17294d98b915364f64a8a1cbc071aa6ee200a03b/src%2Flibsyntax%2Fext%2Fexpand.rs", "raw_url": "https://github.com/rust-lang/rust/raw/17294d98b915364f64a8a1cbc071aa6ee200a03b/src%2Flibsyntax%2Fext%2Fexpand.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fext%2Fexpand.rs?ref=17294d98b915364f64a8a1cbc071aa6ee200a03b", "patch": "@@ -21,7 +21,7 @@ use ext::base::*;\n use feature_gate::{self, Features};\n use fold;\n use fold::*;\n-use parse::{ParseSess, DirectoryOwnership, PResult, lexer};\n+use parse::{ParseSess, DirectoryOwnership, PResult, filemap_to_tts};\n use parse::parser::Parser;\n use parse::token;\n use print::pprust;\n@@ -669,12 +669,8 @@ fn tts_for_attr_args(attr: &ast::Attribute, parse_sess: &ParseSess) -> Vec<Token\n }\n \n fn string_to_tts(text: String, parse_sess: &ParseSess) -> Vec<TokenTree> {\n-    let filemap = parse_sess.codemap()\n-                            .new_filemap(String::from(\"<macro expansion>\"), None, text);\n-\n-    let lexer = lexer::StringReader::new(&parse_sess.span_diagnostic, filemap);\n-    let mut parser = Parser::new(parse_sess, Box::new(lexer), None, false);\n-    panictry!(parser.parse_all_token_trees())\n+    let filename = String::from(\"<macro expansion>\");\n+    filemap_to_tts(parse_sess, parse_sess.codemap().new_filemap(filename, None, text))\n }\n \n impl<'a, 'b> Folder for InvocationCollector<'a, 'b> {"}, {"sha": "089c35c694a78e3eb9e9c79f0268a80d27a59dc2", "filename": "src/libsyntax/ext/tt/macro_parser.rs", "status": "modified", "additions": 4, "deletions": 20, "changes": 24, "blob_url": "https://github.com/rust-lang/rust/blob/17294d98b915364f64a8a1cbc071aa6ee200a03b/src%2Flibsyntax%2Fext%2Ftt%2Fmacro_parser.rs", "raw_url": "https://github.com/rust-lang/rust/raw/17294d98b915364f64a8a1cbc071aa6ee200a03b/src%2Flibsyntax%2Fext%2Ftt%2Fmacro_parser.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fext%2Ftt%2Fmacro_parser.rs?ref=17294d98b915364f64a8a1cbc071aa6ee200a03b", "patch": "@@ -82,7 +82,6 @@ use ast::Ident;\n use syntax_pos::{self, BytePos, mk_sp, Span};\n use codemap::Spanned;\n use errors::FatalError;\n-use parse::lexer::*; //resolve bug?\n use parse::{Directory, ParseSess};\n use parse::parser::{PathStyle, Parser};\n use parse::token::{DocComment, MatchNt, SubstNt};\n@@ -407,9 +406,9 @@ fn inner_parse_loop(cur_eis: &mut SmallVector<Box<MatcherPos>>,\n     Success(())\n }\n \n-pub fn parse(sess: &ParseSess, rdr: TtReader, ms: &[TokenTree], directory: Option<Directory>)\n+pub fn parse(sess: &ParseSess, tts: Vec<TokenTree>, ms: &[TokenTree], directory: Option<Directory>)\n              -> NamedParseResult {\n-    let mut parser = Parser::new(sess, Box::new(rdr), directory, true);\n+    let mut parser = Parser::new(sess, tts, directory, true);\n     let mut cur_eis = SmallVector::one(initial_matcher_pos(ms.to_owned(), parser.span.lo));\n     let mut next_eis = Vec::new(); // or proceed normally\n \n@@ -481,23 +480,8 @@ fn parse_nt<'a>(p: &mut Parser<'a>, sp: Span, name: &str) -> Nonterminal {\n     match name {\n         \"tt\" => {\n             p.quote_depth += 1; //but in theory, non-quoted tts might be useful\n-            let mut tt = panictry!(p.parse_token_tree());\n+            let tt = panictry!(p.parse_token_tree());\n             p.quote_depth -= 1;\n-            while let TokenTree::Token(sp, token::Interpolated(nt)) = tt {\n-                if let token::NtTT(..) = *nt {\n-                    match Rc::try_unwrap(nt) {\n-                        Ok(token::NtTT(sub_tt)) => tt = sub_tt,\n-                        Ok(_) => unreachable!(),\n-                        Err(nt_rc) => match *nt_rc {\n-                            token::NtTT(ref sub_tt) => tt = sub_tt.clone(),\n-                            _ => unreachable!(),\n-                        },\n-                    }\n-                } else {\n-                    tt = TokenTree::Token(sp, token::Interpolated(nt.clone()));\n-                    break\n-                }\n-            }\n             return token::NtTT(tt);\n         }\n         _ => {}\n@@ -527,7 +511,7 @@ fn parse_nt<'a>(p: &mut Parser<'a>, sp: Span, name: &str) -> Nonterminal {\n         \"ident\" => match p.token {\n             token::Ident(sn) => {\n                 p.bump();\n-                token::NtIdent(Spanned::<Ident>{node: sn, span: p.span})\n+                token::NtIdent(Spanned::<Ident>{node: sn, span: p.prev_span})\n             }\n             _ => {\n                 let token_str = pprust::token_to_string(&p.token);"}, {"sha": "f6a25d4aceed7a7923b1e249cbeed92f87c7e7b6", "filename": "src/libsyntax/ext/tt/macro_rules.rs", "status": "modified", "additions": 5, "deletions": 8, "changes": 13, "blob_url": "https://github.com/rust-lang/rust/blob/17294d98b915364f64a8a1cbc071aa6ee200a03b/src%2Flibsyntax%2Fext%2Ftt%2Fmacro_rules.rs", "raw_url": "https://github.com/rust-lang/rust/raw/17294d98b915364f64a8a1cbc071aa6ee200a03b/src%2Flibsyntax%2Fext%2Ftt%2Fmacro_rules.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fext%2Ftt%2Fmacro_rules.rs?ref=17294d98b915364f64a8a1cbc071aa6ee200a03b", "patch": "@@ -16,8 +16,8 @@ use ext::expand::{Expansion, ExpansionKind};\n use ext::tt::macro_parser::{Success, Error, Failure};\n use ext::tt::macro_parser::{MatchedSeq, MatchedNonterminal};\n use ext::tt::macro_parser::{parse, parse_failure_msg};\n+use ext::tt::transcribe::transcribe;\n use parse::{Directory, ParseSess};\n-use parse::lexer::new_tt_reader;\n use parse::parser::Parser;\n use parse::token::{self, NtTT, Token};\n use parse::token::Token::*;\n@@ -113,13 +113,12 @@ fn generic_extension<'cx>(cx: &'cx ExtCtxt,\n                     _ => cx.span_bug(sp, \"malformed macro rhs\"),\n                 };\n                 // rhs has holes ( `$id` and `$(...)` that need filled)\n-                let trncbr =\n-                    new_tt_reader(&cx.parse_sess.span_diagnostic, Some(named_matches), rhs);\n+                let tts = transcribe(&cx.parse_sess.span_diagnostic, Some(named_matches), rhs);\n                 let directory = Directory {\n                     path: cx.current_expansion.module.directory.clone(),\n                     ownership: cx.current_expansion.directory_ownership,\n                 };\n-                let mut p = Parser::new(cx.parse_sess(), Box::new(trncbr), Some(directory), false);\n+                let mut p = Parser::new(cx.parse_sess(), tts, Some(directory), false);\n                 p.root_module_name = cx.current_expansion.module.mod_path.last()\n                     .map(|id| (*id.name.as_str()).to_owned());\n \n@@ -187,10 +186,8 @@ pub fn compile(sess: &ParseSess, def: &ast::MacroDef) -> SyntaxExtension {\n         })),\n     ];\n \n-    // Parse the macro_rules! invocation (`none` is for no interpolations):\n-    let arg_reader = new_tt_reader(&sess.span_diagnostic, None, def.body.clone());\n-\n-    let argument_map = match parse(sess, arg_reader, &argument_gram, None) {\n+    // Parse the macro_rules! invocation\n+    let argument_map = match parse(sess, def.body.clone(), &argument_gram, None) {\n         Success(m) => m,\n         Failure(sp, tok) => {\n             let s = parse_failure_msg(tok);"}, {"sha": "38becbe7b1d30a31a5ba0869f065adfd1fcad786", "filename": "src/libsyntax/ext/tt/transcribe.rs", "status": "modified", "additions": 36, "deletions": 62, "changes": 98, "blob_url": "https://github.com/rust-lang/rust/blob/17294d98b915364f64a8a1cbc071aa6ee200a03b/src%2Flibsyntax%2Fext%2Ftt%2Ftranscribe.rs", "raw_url": "https://github.com/rust-lang/rust/raw/17294d98b915364f64a8a1cbc071aa6ee200a03b/src%2Flibsyntax%2Fext%2Ftt%2Ftranscribe.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fext%2Ftt%2Ftranscribe.rs?ref=17294d98b915364f64a8a1cbc071aa6ee200a03b", "patch": "@@ -10,10 +10,9 @@\n use self::LockstepIterSize::*;\n \n use ast::Ident;\n-use errors::{Handler, DiagnosticBuilder};\n+use errors::Handler;\n use ext::tt::macro_parser::{NamedMatch, MatchedSeq, MatchedNonterminal};\n-use parse::token::{self, MatchNt, SubstNt, Token, NtIdent};\n-use parse::lexer::TokenAndSpan;\n+use parse::token::{self, MatchNt, SubstNt, Token, NtIdent, NtTT};\n use syntax_pos::{Span, DUMMY_SP};\n use tokenstream::{self, TokenTree};\n use util::small_vector::SmallVector;\n@@ -32,29 +31,24 @@ struct TtFrame {\n }\n \n #[derive(Clone)]\n-pub struct TtReader<'a> {\n-    pub sp_diag: &'a Handler,\n+struct TtReader<'a> {\n+    sp_diag: &'a Handler,\n     /// the unzipped tree:\n     stack: SmallVector<TtFrame>,\n     /* for MBE-style macro transcription */\n     interpolations: HashMap<Ident, Rc<NamedMatch>>,\n \n     repeat_idx: Vec<usize>,\n     repeat_len: Vec<usize>,\n-    /* cached: */\n-    pub cur_tok: Token,\n-    pub cur_span: Span,\n-    /// Transform doc comments. Only useful in macro invocations\n-    pub fatal_errs: Vec<DiagnosticBuilder<'a>>,\n }\n \n /// This can do Macro-By-Example transcription. On the other hand, if\n /// `src` contains no `TokenTree::Sequence`s, `MatchNt`s or `SubstNt`s, `interp` can\n /// (and should) be None.\n-pub fn new_tt_reader(sp_diag: &Handler,\n-                     interp: Option<HashMap<Ident, Rc<NamedMatch>>>,\n-                     src: Vec<tokenstream::TokenTree>)\n-                     -> TtReader {\n+pub fn transcribe(sp_diag: &Handler,\n+                  interp: Option<HashMap<Ident, Rc<NamedMatch>>>,\n+                  src: Vec<tokenstream::TokenTree>)\n+                  -> Vec<TokenTree> {\n     let mut r = TtReader {\n         sp_diag: sp_diag,\n         stack: SmallVector::one(TtFrame {\n@@ -73,13 +67,15 @@ pub fn new_tt_reader(sp_diag: &Handler,\n         },\n         repeat_idx: Vec::new(),\n         repeat_len: Vec::new(),\n-        /* dummy values, never read: */\n-        cur_tok: token::Eof,\n-        cur_span: DUMMY_SP,\n-        fatal_errs: Vec::new(),\n     };\n-    tt_next_token(&mut r); /* get cur_tok and cur_span set up */\n-    r\n+\n+    let mut tts = Vec::new();\n+    let mut prev_span = DUMMY_SP;\n+    while let Some(tt) = tt_next_token(&mut r, prev_span) {\n+        prev_span = tt.span();\n+        tts.push(tt);\n+    }\n+    tts\n }\n \n fn lookup_cur_matched_by_matched(r: &TtReader, start: Rc<NamedMatch>) -> Rc<NamedMatch> {\n@@ -153,38 +149,24 @@ fn lockstep_iter_size(t: &TokenTree, r: &TtReader) -> LockstepIterSize {\n \n /// Return the next token from the TtReader.\n /// EFFECT: advances the reader's token field\n-pub fn tt_next_token(r: &mut TtReader) -> TokenAndSpan {\n-    // FIXME(pcwalton): Bad copy?\n-    let ret_val = TokenAndSpan {\n-        tok: r.cur_tok.clone(),\n-        sp: r.cur_span.clone(),\n-    };\n+fn tt_next_token(r: &mut TtReader, prev_span: Span) -> Option<TokenTree> {\n     loop {\n-        let should_pop = match r.stack.last() {\n-            None => {\n-                assert_eq!(ret_val.tok, token::Eof);\n-                return ret_val;\n-            }\n-            Some(frame) => {\n-                if frame.idx < frame.forest.len() {\n-                    break;\n-                }\n-                !frame.dotdotdoted ||\n-                    *r.repeat_idx.last().unwrap() == *r.repeat_len.last().unwrap() - 1\n+        let should_pop = if let Some(frame) = r.stack.last() {\n+            if frame.idx < frame.forest.len() {\n+                break;\n             }\n+            !frame.dotdotdoted || *r.repeat_idx.last().unwrap() == *r.repeat_len.last().unwrap() - 1\n+        } else {\n+            return None;\n         };\n \n         /* done with this set; pop or repeat? */\n         if should_pop {\n             let prev = r.stack.pop().unwrap();\n-            match r.stack.last_mut() {\n-                None => {\n-                    r.cur_tok = token::Eof;\n-                    return ret_val;\n-                }\n-                Some(frame) => {\n-                    frame.idx += 1;\n-                }\n+            if let Some(frame) = r.stack.last_mut() {\n+                frame.idx += 1;\n+            } else {\n+                return None;\n             }\n             if prev.dotdotdoted {\n                 r.repeat_idx.pop();\n@@ -194,8 +176,7 @@ pub fn tt_next_token(r: &mut TtReader) -> TokenAndSpan {\n             *r.repeat_idx.last_mut().unwrap() += 1;\n             r.stack.last_mut().unwrap().idx = 0;\n             if let Some(tk) = r.stack.last().unwrap().sep.clone() {\n-                r.cur_tok = tk; // repeat same span, I guess\n-                return ret_val;\n+                return Some(TokenTree::Token(prev_span, tk)); // repeat same span, I guess\n             }\n         }\n     }\n@@ -231,7 +212,7 @@ pub fn tt_next_token(r: &mut TtReader) -> TokenAndSpan {\n                             }\n \n                             r.stack.last_mut().unwrap().idx += 1;\n-                            return tt_next_token(r);\n+                            return tt_next_token(r, prev_span);\n                         }\n                         r.repeat_len.push(len);\n                         r.repeat_idx.push(0);\n@@ -249,9 +230,7 @@ pub fn tt_next_token(r: &mut TtReader) -> TokenAndSpan {\n                 r.stack.last_mut().unwrap().idx += 1;\n                 match lookup_cur_matched(r, ident) {\n                     None => {\n-                        r.cur_span = sp;\n-                        r.cur_tok = SubstNt(ident);\n-                        return ret_val;\n+                        return Some(TokenTree::Token(sp, SubstNt(ident)));\n                         // this can't be 0 length, just like TokenTree::Delimited\n                     }\n                     Some(cur_matched) => if let MatchedNonterminal(ref nt) = *cur_matched {\n@@ -260,15 +239,12 @@ pub fn tt_next_token(r: &mut TtReader) -> TokenAndSpan {\n                             // (a) idents can be in lots of places, so it'd be a pain\n                             // (b) we actually can, since it's a token.\n                             NtIdent(ref sn) => {\n-                                r.cur_span = sn.span;\n-                                r.cur_tok = token::Ident(sn.node);\n-                                return ret_val;\n+                                return Some(TokenTree::Token(sn.span, token::Ident(sn.node)));\n                             }\n+                            NtTT(ref tt) => return Some(tt.clone()),\n                             _ => {\n-                                // FIXME(pcwalton): Bad copy.\n-                                r.cur_span = sp;\n-                                r.cur_tok = token::Interpolated(nt.clone());\n-                                return ret_val;\n+                                // FIXME(pcwalton): Bad copy\n+                                return Some(TokenTree::Token(sp, token::Interpolated(nt.clone())));\n                             }\n                         }\n                     } else {\n@@ -289,11 +265,9 @@ pub fn tt_next_token(r: &mut TtReader) -> TokenAndSpan {\n                 });\n                 // if this could be 0-length, we'd need to potentially recur here\n             }\n-            TokenTree::Token(sp, tok) => {\n-                r.cur_span = sp;\n-                r.cur_tok = tok;\n+            tt @ TokenTree::Token(..) => {\n                 r.stack.last_mut().unwrap().idx += 1;\n-                return ret_val;\n+                return Some(tt);\n             }\n         }\n     }"}, {"sha": "c97b8ddf91972ac27e636e2a09e72fb0a2bf9048", "filename": "src/libsyntax/parse/lexer/comments.rs", "status": "modified", "additions": 4, "deletions": 9, "changes": 13, "blob_url": "https://github.com/rust-lang/rust/blob/17294d98b915364f64a8a1cbc071aa6ee200a03b/src%2Flibsyntax%2Fparse%2Flexer%2Fcomments.rs", "raw_url": "https://github.com/rust-lang/rust/raw/17294d98b915364f64a8a1cbc071aa6ee200a03b/src%2Flibsyntax%2Fparse%2Flexer%2Fcomments.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Flexer%2Fcomments.rs?ref=17294d98b915364f64a8a1cbc071aa6ee200a03b", "patch": "@@ -13,11 +13,8 @@ pub use self::CommentStyle::*;\n use ast;\n use codemap::CodeMap;\n use syntax_pos::{BytePos, CharPos, Pos};\n-use errors;\n-use parse::lexer::is_block_doc_comment;\n-use parse::lexer::{StringReader, TokenAndSpan};\n-use parse::lexer::{is_pattern_whitespace, Reader};\n-use parse::lexer;\n+use parse::lexer::{is_block_doc_comment, is_pattern_whitespace};\n+use parse::lexer::{self, ParseSess, StringReader, TokenAndSpan};\n use print::pprust;\n use str::char_at;\n \n@@ -346,16 +343,14 @@ pub struct Literal {\n \n // it appears this function is called only from pprust... that's\n // probably not a good thing.\n-pub fn gather_comments_and_literals(span_diagnostic: &errors::Handler,\n-                                    path: String,\n-                                    srdr: &mut Read)\n+pub fn gather_comments_and_literals(sess: &ParseSess, path: String, srdr: &mut Read)\n                                     -> (Vec<Comment>, Vec<Literal>) {\n     let mut src = Vec::new();\n     srdr.read_to_end(&mut src).unwrap();\n     let src = String::from_utf8(src).unwrap();\n     let cm = CodeMap::new();\n     let filemap = cm.new_filemap(path, None, src);\n-    let mut rdr = lexer::StringReader::new_raw(span_diagnostic, filemap);\n+    let mut rdr = lexer::StringReader::new_raw(sess, filemap);\n \n     let mut comments: Vec<Comment> = Vec::new();\n     let mut literals: Vec<Literal> = Vec::new();"}, {"sha": "6bc15115b09d3efca57ab6b55100e9c450132996", "filename": "src/libsyntax/parse/lexer/mod.rs", "status": "modified", "additions": 88, "deletions": 128, "changes": 216, "blob_url": "https://github.com/rust-lang/rust/blob/17294d98b915364f64a8a1cbc071aa6ee200a03b/src%2Flibsyntax%2Fparse%2Flexer%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/17294d98b915364f64a8a1cbc071aa6ee200a03b/src%2Flibsyntax%2Fparse%2Flexer%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Flexer%2Fmod.rs?ref=17294d98b915364f64a8a1cbc071aa6ee200a03b", "patch": "@@ -11,9 +11,8 @@\n use ast::{self, Ident};\n use syntax_pos::{self, BytePos, CharPos, Pos, Span};\n use codemap::CodeMap;\n-use errors::{FatalError, Handler, DiagnosticBuilder};\n-use ext::tt::transcribe::tt_next_token;\n-use parse::token;\n+use errors::{FatalError, DiagnosticBuilder};\n+use parse::{token, ParseSess};\n use str::char_at;\n use symbol::{Symbol, keywords};\n use std_unicode::property::Pattern_White_Space;\n@@ -23,52 +22,10 @@ use std::char;\n use std::mem::replace;\n use std::rc::Rc;\n \n-pub use ext::tt::transcribe::{TtReader, new_tt_reader};\n-\n pub mod comments;\n+mod tokentrees;\n mod unicode_chars;\n \n-pub trait Reader {\n-    fn is_eof(&self) -> bool;\n-    fn try_next_token(&mut self) -> Result<TokenAndSpan, ()>;\n-    fn next_token(&mut self) -> TokenAndSpan where Self: Sized {\n-        let res = self.try_next_token();\n-        self.unwrap_or_abort(res)\n-    }\n-    /// Report a fatal error with the current span.\n-    fn fatal(&self, &str) -> FatalError;\n-    /// Report a non-fatal error with the current span.\n-    fn err(&self, &str);\n-    fn emit_fatal_errors(&mut self);\n-    fn unwrap_or_abort(&mut self, res: Result<TokenAndSpan, ()>) -> TokenAndSpan {\n-        match res {\n-            Ok(tok) => tok,\n-            Err(_) => {\n-                self.emit_fatal_errors();\n-                panic!(FatalError);\n-            }\n-        }\n-    }\n-    fn peek(&self) -> TokenAndSpan;\n-    /// Get a token the parser cares about.\n-    fn try_real_token(&mut self) -> Result<TokenAndSpan, ()> {\n-        let mut t = self.try_next_token()?;\n-        loop {\n-            match t.tok {\n-                token::Whitespace | token::Comment | token::Shebang(_) => {\n-                    t = self.try_next_token()?;\n-                }\n-                _ => break,\n-            }\n-        }\n-        Ok(t)\n-    }\n-    fn real_token(&mut self) -> TokenAndSpan {\n-        let res = self.try_real_token();\n-        self.unwrap_or_abort(res)\n-    }\n-}\n-\n #[derive(Clone, PartialEq, Eq, Debug)]\n pub struct TokenAndSpan {\n     pub tok: token::Token,\n@@ -82,7 +39,7 @@ impl Default for TokenAndSpan {\n }\n \n pub struct StringReader<'a> {\n-    pub span_diagnostic: &'a Handler,\n+    pub sess: &'a ParseSess,\n     /// The absolute offset within the codemap of the next character to read\n     pub next_pos: BytePos,\n     /// The absolute offset within the codemap of the current character\n@@ -105,9 +62,44 @@ pub struct StringReader<'a> {\n     // cache a direct reference to the source text, so that we don't have to\n     // retrieve it via `self.filemap.src.as_ref().unwrap()` all the time.\n     source_text: Rc<String>,\n+    /// Stack of open delimiters and their spans. Used for error message.\n+    token: token::Token,\n+    span: Span,\n+    open_braces: Vec<(token::DelimToken, Span)>,\n }\n \n-impl<'a> Reader for StringReader<'a> {\n+impl<'a> StringReader<'a> {\n+    fn next_token(&mut self) -> TokenAndSpan where Self: Sized {\n+        let res = self.try_next_token();\n+        self.unwrap_or_abort(res)\n+    }\n+    fn unwrap_or_abort(&mut self, res: Result<TokenAndSpan, ()>) -> TokenAndSpan {\n+        match res {\n+            Ok(tok) => tok,\n+            Err(_) => {\n+                self.emit_fatal_errors();\n+                panic!(FatalError);\n+            }\n+        }\n+    }\n+    fn try_real_token(&mut self) -> Result<TokenAndSpan, ()> {\n+        let mut t = self.try_next_token()?;\n+        loop {\n+            match t.tok {\n+                token::Whitespace | token::Comment | token::Shebang(_) => {\n+                    t = self.try_next_token()?;\n+                }\n+                _ => break,\n+            }\n+        }\n+        self.token = t.tok.clone();\n+        self.span = t.sp;\n+        Ok(t)\n+    }\n+    pub fn real_token(&mut self) -> TokenAndSpan {\n+        let res = self.try_real_token();\n+        self.unwrap_or_abort(res)\n+    }\n     fn is_eof(&self) -> bool {\n         if self.ch.is_none() {\n             return true;\n@@ -119,7 +111,7 @@ impl<'a> Reader for StringReader<'a> {\n         }\n     }\n     /// Return the next token. EFFECT: advances the string_reader.\n-    fn try_next_token(&mut self) -> Result<TokenAndSpan, ()> {\n+    pub fn try_next_token(&mut self) -> Result<TokenAndSpan, ()> {\n         assert!(self.fatal_errs.is_empty());\n         let ret_val = TokenAndSpan {\n             tok: replace(&mut self.peek_tok, token::Underscore),\n@@ -131,16 +123,13 @@ impl<'a> Reader for StringReader<'a> {\n     fn fatal(&self, m: &str) -> FatalError {\n         self.fatal_span(self.peek_span, m)\n     }\n-    fn err(&self, m: &str) {\n-        self.err_span(self.peek_span, m)\n-    }\n-    fn emit_fatal_errors(&mut self) {\n+    pub fn emit_fatal_errors(&mut self) {\n         for err in &mut self.fatal_errs {\n             err.emit();\n         }\n         self.fatal_errs.clear();\n     }\n-    fn peek(&self) -> TokenAndSpan {\n+    pub fn peek(&self) -> TokenAndSpan {\n         // FIXME(pcwalton): Bad copy!\n         TokenAndSpan {\n             tok: self.peek_tok.clone(),\n@@ -149,59 +138,24 @@ impl<'a> Reader for StringReader<'a> {\n     }\n }\n \n-impl<'a> Reader for TtReader<'a> {\n-    fn is_eof(&self) -> bool {\n-        self.peek().tok == token::Eof\n-    }\n-    fn try_next_token(&mut self) -> Result<TokenAndSpan, ()> {\n-        assert!(self.fatal_errs.is_empty());\n-        let r = tt_next_token(self);\n-        debug!(\"TtReader: r={:?}\", r);\n-        Ok(r)\n-    }\n-    fn fatal(&self, m: &str) -> FatalError {\n-        self.sp_diag.span_fatal(self.cur_span, m)\n-    }\n-    fn err(&self, m: &str) {\n-        self.sp_diag.span_err(self.cur_span, m);\n-    }\n-    fn emit_fatal_errors(&mut self) {\n-        for err in &mut self.fatal_errs {\n-            err.emit();\n-        }\n-        self.fatal_errs.clear();\n-    }\n-    fn peek(&self) -> TokenAndSpan {\n-        TokenAndSpan {\n-            tok: self.cur_tok.clone(),\n-            sp: self.cur_span,\n-        }\n-    }\n-}\n-\n impl<'a> StringReader<'a> {\n     /// For comments.rs, which hackily pokes into next_pos and ch\n-    pub fn new_raw<'b>(span_diagnostic: &'b Handler,\n-                       filemap: Rc<syntax_pos::FileMap>)\n-                       -> StringReader<'b> {\n-        let mut sr = StringReader::new_raw_internal(span_diagnostic, filemap);\n+    pub fn new_raw<'b>(sess: &'a ParseSess, filemap: Rc<syntax_pos::FileMap>) -> Self {\n+        let mut sr = StringReader::new_raw_internal(sess, filemap);\n         sr.bump();\n         sr\n     }\n \n-    fn new_raw_internal<'b>(span_diagnostic: &'b Handler,\n-                            filemap: Rc<syntax_pos::FileMap>)\n-                            -> StringReader<'b> {\n+    fn new_raw_internal(sess: &'a ParseSess, filemap: Rc<syntax_pos::FileMap>) -> Self {\n         if filemap.src.is_none() {\n-            span_diagnostic.bug(&format!(\"Cannot lex filemap \\\n-                                          without source: {}\",\n-                                         filemap.name)[..]);\n+            sess.span_diagnostic.bug(&format!(\"Cannot lex filemap without source: {}\",\n+                                              filemap.name));\n         }\n \n         let source_text = (*filemap.src.as_ref().unwrap()).clone();\n \n         StringReader {\n-            span_diagnostic: span_diagnostic,\n+            sess: sess,\n             next_pos: filemap.start_pos,\n             pos: filemap.start_pos,\n             col: CharPos(0),\n@@ -214,13 +168,14 @@ impl<'a> StringReader<'a> {\n             peek_span: syntax_pos::DUMMY_SP,\n             source_text: source_text,\n             fatal_errs: Vec::new(),\n+            token: token::Eof,\n+            span: syntax_pos::DUMMY_SP,\n+            open_braces: Vec::new(),\n         }\n     }\n \n-    pub fn new<'b>(span_diagnostic: &'b Handler,\n-                   filemap: Rc<syntax_pos::FileMap>)\n-                   -> StringReader<'b> {\n-        let mut sr = StringReader::new_raw(span_diagnostic, filemap);\n+    pub fn new(sess: &'a ParseSess, filemap: Rc<syntax_pos::FileMap>) -> Self {\n+        let mut sr = StringReader::new_raw(sess, filemap);\n         if let Err(_) = sr.advance_token() {\n             sr.emit_fatal_errors();\n             panic!(FatalError);\n@@ -234,12 +189,12 @@ impl<'a> StringReader<'a> {\n \n     /// Report a fatal lexical error with a given span.\n     pub fn fatal_span(&self, sp: Span, m: &str) -> FatalError {\n-        self.span_diagnostic.span_fatal(sp, m)\n+        self.sess.span_diagnostic.span_fatal(sp, m)\n     }\n \n     /// Report a lexical error with a given span.\n     pub fn err_span(&self, sp: Span, m: &str) {\n-        self.span_diagnostic.span_err(sp, m)\n+        self.sess.span_diagnostic.span_err(sp, m)\n     }\n \n \n@@ -274,7 +229,7 @@ impl<'a> StringReader<'a> {\n         for c in c.escape_default() {\n             m.push(c)\n         }\n-        self.span_diagnostic.struct_span_fatal(syntax_pos::mk_sp(from_pos, to_pos), &m[..])\n+        self.sess.span_diagnostic.struct_span_fatal(syntax_pos::mk_sp(from_pos, to_pos), &m[..])\n     }\n \n     /// Report a lexical error spanning [`from_pos`, `to_pos`), appending an\n@@ -298,7 +253,7 @@ impl<'a> StringReader<'a> {\n         for c in c.escape_default() {\n             m.push(c)\n         }\n-        self.span_diagnostic.struct_span_err(syntax_pos::mk_sp(from_pos, to_pos), &m[..])\n+        self.sess.span_diagnostic.struct_span_err(syntax_pos::mk_sp(from_pos, to_pos), &m[..])\n     }\n \n     /// Report a lexical error spanning [`from_pos`, `to_pos`), appending the\n@@ -503,9 +458,8 @@ impl<'a> StringReader<'a> {\n     fn scan_comment(&mut self) -> Option<TokenAndSpan> {\n         if let Some(c) = self.ch {\n             if c.is_whitespace() {\n-                self.span_diagnostic.span_err(syntax_pos::mk_sp(self.pos, self.pos),\n-                                              \"called consume_any_line_comment, but there \\\n-                                               was whitespace\");\n+                let msg = \"called consume_any_line_comment, but there was whitespace\";\n+                self.sess.span_diagnostic.span_err(syntax_pos::mk_sp(self.pos, self.pos), msg);\n             }\n         }\n \n@@ -875,7 +829,7 @@ impl<'a> StringReader<'a> {\n                                     self.scan_unicode_escape(delim) && !ascii_only\n                                 } else {\n                                     let span = syntax_pos::mk_sp(start, self.pos);\n-                                    self.span_diagnostic\n+                                    self.sess.span_diagnostic\n                                         .struct_span_err(span, \"incorrect unicode escape sequence\")\n                                         .span_help(span,\n                                                    \"format of unicode escape sequences is \\\n@@ -1701,35 +1655,41 @@ fn ident_continue(c: Option<char>) -> bool {\n mod tests {\n     use super::*;\n \n-    use ast::Ident;\n+    use ast::{Ident, CrateConfig};\n     use symbol::Symbol;\n     use syntax_pos::{BytePos, Span, NO_EXPANSION};\n     use codemap::CodeMap;\n     use errors;\n+    use feature_gate::UnstableFeatures;\n     use parse::token;\n+    use std::cell::RefCell;\n     use std::io;\n     use std::rc::Rc;\n \n-    fn mk_sh(cm: Rc<CodeMap>) -> errors::Handler {\n-        // FIXME (#22405): Replace `Box::new` with `box` here when/if possible.\n-        let emitter = errors::emitter::EmitterWriter::new(Box::new(io::sink()),\n-                                                Some(cm));\n-        errors::Handler::with_emitter(true, false, Box::new(emitter))\n+    fn mk_sess(cm: Rc<CodeMap>) -> ParseSess {\n+        let emitter = errors::emitter::EmitterWriter::new(Box::new(io::sink()), Some(cm.clone()));\n+        ParseSess {\n+            span_diagnostic: errors::Handler::with_emitter(true, false, Box::new(emitter)),\n+            unstable_features: UnstableFeatures::from_environment(),\n+            config: CrateConfig::new(),\n+            included_mod_stack: RefCell::new(Vec::new()),\n+            code_map: cm,\n+        }\n     }\n \n     // open a string reader for the given string\n     fn setup<'a>(cm: &CodeMap,\n-                 span_handler: &'a errors::Handler,\n+                 sess: &'a ParseSess,\n                  teststr: String)\n                  -> StringReader<'a> {\n         let fm = cm.new_filemap(\"zebra.rs\".to_string(), None, teststr);\n-        StringReader::new(span_handler, fm)\n+        StringReader::new(sess, fm)\n     }\n \n     #[test]\n     fn t1() {\n         let cm = Rc::new(CodeMap::new());\n-        let sh = mk_sh(cm.clone());\n+        let sh = mk_sess(cm.clone());\n         let mut string_reader = setup(&cm,\n                                       &sh,\n                                       \"/* my source file */ fn main() { println!(\\\"zebra\\\"); }\\n\"\n@@ -1781,71 +1741,71 @@ mod tests {\n     #[test]\n     fn doublecolonparsing() {\n         let cm = Rc::new(CodeMap::new());\n-        let sh = mk_sh(cm.clone());\n+        let sh = mk_sess(cm.clone());\n         check_tokenization(setup(&cm, &sh, \"a b\".to_string()),\n                            vec![mk_ident(\"a\"), token::Whitespace, mk_ident(\"b\")]);\n     }\n \n     #[test]\n     fn dcparsing_2() {\n         let cm = Rc::new(CodeMap::new());\n-        let sh = mk_sh(cm.clone());\n+        let sh = mk_sess(cm.clone());\n         check_tokenization(setup(&cm, &sh, \"a::b\".to_string()),\n                            vec![mk_ident(\"a\"), token::ModSep, mk_ident(\"b\")]);\n     }\n \n     #[test]\n     fn dcparsing_3() {\n         let cm = Rc::new(CodeMap::new());\n-        let sh = mk_sh(cm.clone());\n+        let sh = mk_sess(cm.clone());\n         check_tokenization(setup(&cm, &sh, \"a ::b\".to_string()),\n                            vec![mk_ident(\"a\"), token::Whitespace, token::ModSep, mk_ident(\"b\")]);\n     }\n \n     #[test]\n     fn dcparsing_4() {\n         let cm = Rc::new(CodeMap::new());\n-        let sh = mk_sh(cm.clone());\n+        let sh = mk_sess(cm.clone());\n         check_tokenization(setup(&cm, &sh, \"a:: b\".to_string()),\n                            vec![mk_ident(\"a\"), token::ModSep, token::Whitespace, mk_ident(\"b\")]);\n     }\n \n     #[test]\n     fn character_a() {\n         let cm = Rc::new(CodeMap::new());\n-        let sh = mk_sh(cm.clone());\n+        let sh = mk_sess(cm.clone());\n         assert_eq!(setup(&cm, &sh, \"'a'\".to_string()).next_token().tok,\n                    token::Literal(token::Char(Symbol::intern(\"a\")), None));\n     }\n \n     #[test]\n     fn character_space() {\n         let cm = Rc::new(CodeMap::new());\n-        let sh = mk_sh(cm.clone());\n+        let sh = mk_sess(cm.clone());\n         assert_eq!(setup(&cm, &sh, \"' '\".to_string()).next_token().tok,\n                    token::Literal(token::Char(Symbol::intern(\" \")), None));\n     }\n \n     #[test]\n     fn character_escaped() {\n         let cm = Rc::new(CodeMap::new());\n-        let sh = mk_sh(cm.clone());\n+        let sh = mk_sess(cm.clone());\n         assert_eq!(setup(&cm, &sh, \"'\\\\n'\".to_string()).next_token().tok,\n                    token::Literal(token::Char(Symbol::intern(\"\\\\n\")), None));\n     }\n \n     #[test]\n     fn lifetime_name() {\n         let cm = Rc::new(CodeMap::new());\n-        let sh = mk_sh(cm.clone());\n+        let sh = mk_sess(cm.clone());\n         assert_eq!(setup(&cm, &sh, \"'abc\".to_string()).next_token().tok,\n                    token::Lifetime(Ident::from_str(\"'abc\")));\n     }\n \n     #[test]\n     fn raw_string() {\n         let cm = Rc::new(CodeMap::new());\n-        let sh = mk_sh(cm.clone());\n+        let sh = mk_sess(cm.clone());\n         assert_eq!(setup(&cm, &sh, \"r###\\\"\\\"#a\\\\b\\x00c\\\"\\\"###\".to_string())\n                        .next_token()\n                        .tok,\n@@ -1855,7 +1815,7 @@ mod tests {\n     #[test]\n     fn literal_suffixes() {\n         let cm = Rc::new(CodeMap::new());\n-        let sh = mk_sh(cm.clone());\n+        let sh = mk_sess(cm.clone());\n         macro_rules! test {\n             ($input: expr, $tok_type: ident, $tok_contents: expr) => {{\n                 assert_eq!(setup(&cm, &sh, format!(\"{}suffix\", $input)).next_token().tok,\n@@ -1899,7 +1859,7 @@ mod tests {\n     #[test]\n     fn nested_block_comments() {\n         let cm = Rc::new(CodeMap::new());\n-        let sh = mk_sh(cm.clone());\n+        let sh = mk_sess(cm.clone());\n         let mut lexer = setup(&cm, &sh, \"/* /* */ */'a'\".to_string());\n         match lexer.next_token().tok {\n             token::Comment => {}\n@@ -1912,7 +1872,7 @@ mod tests {\n     #[test]\n     fn crlf_comments() {\n         let cm = Rc::new(CodeMap::new());\n-        let sh = mk_sh(cm.clone());\n+        let sh = mk_sess(cm.clone());\n         let mut lexer = setup(&cm, &sh, \"// test\\r\\n/// test\\r\\n\".to_string());\n         let comment = lexer.next_token();\n         assert_eq!(comment.tok, token::Comment);"}, {"sha": "7b6f00e0e8265c7415394698c3480609c55dd562", "filename": "src/libsyntax/parse/lexer/tokentrees.rs", "status": "added", "additions": 138, "deletions": 0, "changes": 138, "blob_url": "https://github.com/rust-lang/rust/blob/17294d98b915364f64a8a1cbc071aa6ee200a03b/src%2Flibsyntax%2Fparse%2Flexer%2Ftokentrees.rs", "raw_url": "https://github.com/rust-lang/rust/raw/17294d98b915364f64a8a1cbc071aa6ee200a03b/src%2Flibsyntax%2Fparse%2Flexer%2Ftokentrees.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Flexer%2Ftokentrees.rs?ref=17294d98b915364f64a8a1cbc071aa6ee200a03b", "patch": "@@ -0,0 +1,138 @@\n+// Copyright 2016 The Rust Project Developers. See the COPYRIGHT\n+// file at the top-level directory of this distribution and at\n+// http://rust-lang.org/COPYRIGHT.\n+//\n+// Licensed under the Apache License, Version 2.0 <LICENSE-APACHE or\n+// http://www.apache.org/licenses/LICENSE-2.0> or the MIT license\n+// <LICENSE-MIT or http://opensource.org/licenses/MIT>, at your\n+// option. This file may not be copied, modified, or distributed\n+// except according to those terms.\n+\n+use print::pprust::token_to_string;\n+use parse::lexer::StringReader;\n+use parse::{token, PResult};\n+use syntax_pos::Span;\n+use tokenstream::{Delimited, TokenTree};\n+\n+use std::rc::Rc;\n+\n+impl<'a> StringReader<'a> {\n+    // Parse a stream of tokens into a list of `TokenTree`s, up to an `Eof`.\n+    pub fn parse_all_token_trees(&mut self) -> PResult<'a, Vec<TokenTree>> {\n+        let mut tts = Vec::new();\n+        while self.token != token::Eof {\n+            tts.push(self.parse_token_tree()?);\n+        }\n+        Ok(tts)\n+    }\n+\n+    // Parse a stream of tokens into a list of `TokenTree`s, up to a `CloseDelim`.\n+    fn parse_token_trees_until_close_delim(&mut self) -> Vec<TokenTree> {\n+        let mut tts = vec![];\n+        loop {\n+            if let token::CloseDelim(..) = self.token {\n+                return tts;\n+            }\n+            match self.parse_token_tree() {\n+                Ok(tt) => tts.push(tt),\n+                Err(mut e) => {\n+                    e.emit();\n+                    return tts;\n+                }\n+            }\n+        }\n+    }\n+\n+    fn parse_token_tree(&mut self) -> PResult<'a, TokenTree> {\n+        match self.token {\n+            token::Eof => {\n+                let msg = \"this file contains an un-closed delimiter\";\n+                let mut err = self.sess.span_diagnostic.struct_span_err(self.span, msg);\n+                for &(_, sp) in &self.open_braces {\n+                    err.span_help(sp, \"did you mean to close this delimiter?\");\n+                }\n+                Err(err)\n+            },\n+            token::OpenDelim(delim) => {\n+                // The span for beginning of the delimited section\n+                let pre_span = self.span;\n+\n+                // Parse the open delimiter.\n+                self.open_braces.push((delim, self.span));\n+                let open_span = self.span;\n+                self.real_token();\n+\n+                // Parse the token trees within the delimiters.\n+                // We stop at any delimiter so we can try to recover if the user\n+                // uses an incorrect delimiter.\n+                let tts = self.parse_token_trees_until_close_delim();\n+\n+                let close_span = self.span;\n+                // Expand to cover the entire delimited token tree\n+                let span = Span { hi: close_span.hi, ..pre_span };\n+\n+                match self.token {\n+                    // Correct delimiter.\n+                    token::CloseDelim(d) if d == delim => {\n+                        self.open_braces.pop().unwrap();\n+\n+                        // Parse the close delimiter.\n+                        self.real_token();\n+                    }\n+                    // Incorrect delimiter.\n+                    token::CloseDelim(other) => {\n+                        let token_str = token_to_string(&self.token);\n+                        let msg = format!(\"incorrect close delimiter: `{}`\", token_str);\n+                        let mut err = self.sess.span_diagnostic.struct_span_err(self.span, &msg);\n+                        // This is a conservative error: only report the last unclosed delimiter.\n+                        // The previous unclosed delimiters could actually be closed! The parser\n+                        // just hasn't gotten to them yet.\n+                        if let Some(&(_, sp)) = self.open_braces.last() {\n+                            err.span_note(sp, \"unclosed delimiter\");\n+                        };\n+                        err.emit();\n+\n+                        self.open_braces.pop().unwrap();\n+\n+                        // If the incorrect delimiter matches an earlier opening\n+                        // delimiter, then don't consume it (it can be used to\n+                        // close the earlier one). Otherwise, consume it.\n+                        // E.g., we try to recover from:\n+                        // fn foo() {\n+                        //     bar(baz(\n+                        // }  // Incorrect delimiter but matches the earlier `{`\n+                        if !self.open_braces.iter().any(|&(b, _)| b == other) {\n+                            self.real_token();\n+                        }\n+                    }\n+                    token::Eof => {\n+                        // Silently recover, the EOF token will be seen again\n+                        // and an error emitted then. Thus we don't pop from\n+                        // self.open_braces here.\n+                    },\n+                    _ => {}\n+                }\n+\n+                Ok(TokenTree::Delimited(span, Rc::new(Delimited {\n+                    delim: delim,\n+                    open_span: open_span,\n+                    tts: tts,\n+                    close_span: close_span,\n+                })))\n+            },\n+            token::CloseDelim(_) => {\n+                // An unexpected closing delimiter (i.e., there is no\n+                // matching opening delimiter).\n+                let token_str = token_to_string(&self.token);\n+                let msg = format!(\"unexpected close delimiter: `{}`\", token_str);\n+                let err = self.sess.span_diagnostic.struct_span_err(self.span, &msg);\n+                Err(err)\n+            },\n+            _ => {\n+                let tt = TokenTree::Token(self.span, self.token.clone());\n+                self.real_token();\n+                Ok(tt)\n+            }\n+        }\n+    }\n+}"}, {"sha": "6da3e5de75cdc98bc6e7c697b08787ec78daef92", "filename": "src/libsyntax/parse/lexer/unicode_chars.rs", "status": "modified", "additions": 2, "deletions": 4, "changes": 6, "blob_url": "https://github.com/rust-lang/rust/blob/17294d98b915364f64a8a1cbc071aa6ee200a03b/src%2Flibsyntax%2Fparse%2Flexer%2Funicode_chars.rs", "raw_url": "https://github.com/rust-lang/rust/raw/17294d98b915364f64a8a1cbc071aa6ee200a03b/src%2Flibsyntax%2Fparse%2Flexer%2Funicode_chars.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Flexer%2Funicode_chars.rs?ref=17294d98b915364f64a8a1cbc071aa6ee200a03b", "patch": "@@ -243,10 +243,8 @@ pub fn check_for_substitution<'a>(reader: &StringReader<'a>,\n                 err.span_help(span, &msg);\n             },\n             None => {\n-                reader\n-                .span_diagnostic\n-                .span_bug_no_panic(span,\n-                                   &format!(\"substitution character not found for '{}'\", ch));\n+                let msg = format!(\"substitution character not found for '{}'\", ch);\n+                reader.sess.span_diagnostic.span_bug_no_panic(span, &msg);\n             }\n         }\n     });"}, {"sha": "08f5df4515ba64dd141efa60a936c05348f6bf72", "filename": "src/libsyntax/parse/mod.rs", "status": "modified", "additions": 6, "deletions": 10, "changes": 16, "blob_url": "https://github.com/rust-lang/rust/blob/17294d98b915364f64a8a1cbc071aa6ee200a03b/src%2Flibsyntax%2Fparse%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/17294d98b915364f64a8a1cbc071aa6ee200a03b/src%2Flibsyntax%2Fparse%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Fmod.rs?ref=17294d98b915364f64a8a1cbc071aa6ee200a03b", "patch": "@@ -45,7 +45,7 @@ pub mod obsolete;\n \n /// Info about a parsing session.\n pub struct ParseSess {\n-    pub span_diagnostic: Handler, // better be the same as the one in the reader!\n+    pub span_diagnostic: Handler,\n     pub unstable_features: UnstableFeatures,\n     pub config: CrateConfig,\n     /// Used to determine and report recursive mod inclusions\n@@ -219,19 +219,15 @@ fn file_to_filemap(sess: &ParseSess, path: &Path, spanopt: Option<Span>)\n }\n \n /// Given a filemap, produce a sequence of token-trees\n-pub fn filemap_to_tts(sess: &ParseSess, filemap: Rc<FileMap>)\n-    -> Vec<tokenstream::TokenTree> {\n-    // it appears to me that the cfg doesn't matter here... indeed,\n-    // parsing tt's probably shouldn't require a parser at all.\n-    let srdr = lexer::StringReader::new(&sess.span_diagnostic, filemap);\n-    let mut p1 = Parser::new(sess, Box::new(srdr), None, false);\n-    panictry!(p1.parse_all_token_trees())\n+pub fn filemap_to_tts(sess: &ParseSess, filemap: Rc<FileMap>) -> Vec<tokenstream::TokenTree> {\n+    let mut srdr = lexer::StringReader::new(sess, filemap);\n+    srdr.real_token();\n+    panictry!(srdr.parse_all_token_trees())\n }\n \n /// Given tts and the ParseSess, produce a parser\n pub fn tts_to_parser<'a>(sess: &'a ParseSess, tts: Vec<tokenstream::TokenTree>) -> Parser<'a> {\n-    let trdr = lexer::new_tt_reader(&sess.span_diagnostic, None, tts);\n-    let mut p = Parser::new(sess, Box::new(trdr), None, false);\n+    let mut p = Parser::new(sess, tts, None, false);\n     p.check_unknown_macro_variable();\n     p\n }"}, {"sha": "fd6abc58b637195bce0f2b579264e99c26e3ec74", "filename": "src/libsyntax/parse/parser.rs", "status": "modified", "additions": 38, "deletions": 157, "changes": 195, "blob_url": "https://github.com/rust-lang/rust/blob/17294d98b915364f64a8a1cbc071aa6ee200a03b/src%2Flibsyntax%2Fparse%2Fparser.rs", "raw_url": "https://github.com/rust-lang/rust/raw/17294d98b915364f64a8a1cbc071aa6ee200a03b/src%2Flibsyntax%2Fparse%2Fparser.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Fparser.rs?ref=17294d98b915364f64a8a1cbc071aa6ee200a03b", "patch": "@@ -46,7 +46,7 @@ use ext::tt::macro_parser;\n use parse;\n use parse::classify;\n use parse::common::SeqSep;\n-use parse::lexer::{Reader, TokenAndSpan};\n+use parse::lexer::TokenAndSpan;\n use parse::obsolete::ObsoleteSyntax;\n use parse::token::{self, MatchNt, SubstNt};\n use parse::{new_sub_parser_from_file, ParseSess, Directory, DirectoryOwnership};\n@@ -156,22 +156,6 @@ enum PrevTokenKind {\n     Other,\n }\n \n-// Simple circular buffer used for keeping few next tokens.\n-#[derive(Default)]\n-struct LookaheadBuffer {\n-    buffer: [TokenAndSpan; LOOKAHEAD_BUFFER_CAPACITY],\n-    start: usize,\n-    end: usize,\n-}\n-\n-const LOOKAHEAD_BUFFER_CAPACITY: usize = 8;\n-\n-impl LookaheadBuffer {\n-    fn len(&self) -> usize {\n-        (LOOKAHEAD_BUFFER_CAPACITY + self.end - self.start) % LOOKAHEAD_BUFFER_CAPACITY\n-    }\n-}\n-\n /* ident is handled by common.rs */\n \n pub struct Parser<'a> {\n@@ -184,27 +168,21 @@ pub struct Parser<'a> {\n     pub prev_span: Span,\n     /// the previous token kind\n     prev_token_kind: PrevTokenKind,\n-    lookahead_buffer: LookaheadBuffer,\n-    pub tokens_consumed: usize,\n     pub restrictions: Restrictions,\n     pub quote_depth: usize, // not (yet) related to the quasiquoter\n     parsing_token_tree: bool,\n-    pub reader: Box<Reader+'a>,\n     /// The set of seen errors about obsolete syntax. Used to suppress\n     /// extra detail when the same error is seen twice\n     pub obsolete_set: HashSet<ObsoleteSyntax>,\n     /// Used to determine the path to externally loaded source files\n     pub directory: Directory,\n-    /// Stack of open delimiters and their spans. Used for error message.\n-    pub open_braces: Vec<(token::DelimToken, Span)>,\n     /// Name of the root module this parser originated from. If `None`, then the\n     /// name is not known. This does not change while the parser is descending\n     /// into modules, and sub-parsers have new values for this name.\n     pub root_module_name: Option<String>,\n     pub expected_tokens: Vec<TokenType>,\n     pub tts: Vec<(TokenTree, usize)>,\n     pub desugar_doc_comments: bool,\n-    pub allow_interpolated_tts: bool,\n }\n \n #[derive(PartialEq, Eq, Clone)]\n@@ -270,30 +248,31 @@ impl From<P<Expr>> for LhsExpr {\n \n impl<'a> Parser<'a> {\n     pub fn new(sess: &'a ParseSess,\n-               rdr: Box<Reader+'a>,\n+               tokens: Vec<TokenTree>,\n                directory: Option<Directory>,\n                desugar_doc_comments: bool)\n                -> Self {\n+        let tt = TokenTree::Delimited(syntax_pos::DUMMY_SP, Rc::new(Delimited {\n+            delim: token::NoDelim,\n+            open_span: syntax_pos::DUMMY_SP,\n+            tts: tokens,\n+            close_span: syntax_pos::DUMMY_SP,\n+        }));\n         let mut parser = Parser {\n-            reader: rdr,\n             sess: sess,\n             token: token::Underscore,\n             span: syntax_pos::DUMMY_SP,\n             prev_span: syntax_pos::DUMMY_SP,\n             prev_token_kind: PrevTokenKind::Other,\n-            lookahead_buffer: Default::default(),\n-            tokens_consumed: 0,\n             restrictions: Restrictions::empty(),\n             quote_depth: 0,\n             parsing_token_tree: false,\n             obsolete_set: HashSet::new(),\n             directory: Directory { path: PathBuf::new(), ownership: DirectoryOwnership::Owned },\n-            open_braces: Vec::new(),\n             root_module_name: None,\n             expected_tokens: Vec::new(),\n-            tts: Vec::new(),\n+            tts: if tt.len() > 0 { vec![(tt, 0)] } else { Vec::new() },\n             desugar_doc_comments: desugar_doc_comments,\n-            allow_interpolated_tts: true,\n         };\n \n         let tok = parser.next_tok();\n@@ -309,8 +288,8 @@ impl<'a> Parser<'a> {\n     }\n \n     fn next_tok(&mut self) -> TokenAndSpan {\n-        'outer: loop {\n-            let mut tok = if let Some((tts, i)) = self.tts.pop() {\n+        loop {\n+            let tok = if let Some((tts, i)) = self.tts.pop() {\n                 let tt = tts.get_tt(i);\n                 if i + 1 < tts.len() {\n                     self.tts.push((tts, i + 1));\n@@ -322,28 +301,14 @@ impl<'a> Parser<'a> {\n                     continue\n                 }\n             } else {\n-                self.reader.real_token()\n+                TokenAndSpan { tok: token::Eof, sp: self.span }\n             };\n \n-            loop {\n-                let nt = match tok.tok {\n-                    token::Interpolated(ref nt) => nt.clone(),\n-                    token::DocComment(name) if self.desugar_doc_comments => {\n-                        self.tts.push((TokenTree::Token(tok.sp, token::DocComment(name)), 0));\n-                        continue 'outer\n-                    }\n-                    _ => return tok,\n-                };\n-                match *nt {\n-                    token::NtTT(TokenTree::Token(sp, ref t)) => {\n-                        tok = TokenAndSpan { tok: t.clone(), sp: sp };\n-                    }\n-                    token::NtTT(ref tt) => {\n-                        self.tts.push((tt.clone(), 0));\n-                        continue 'outer\n-                    }\n-                    _ => return tok,\n+            match tok.tok {\n+                token::DocComment(name) if self.desugar_doc_comments => {\n+                    self.tts.push((TokenTree::Token(tok.sp, token::DocComment(name)), 0));\n                 }\n+                _ => return tok,\n             }\n         }\n     }\n@@ -892,17 +857,9 @@ impl<'a> Parser<'a> {\n             _ => PrevTokenKind::Other,\n         };\n \n-        let next = if self.lookahead_buffer.start == self.lookahead_buffer.end {\n-            self.next_tok()\n-        } else {\n-            // Avoid token copies with `replace`.\n-            let old_start = self.lookahead_buffer.start;\n-            self.lookahead_buffer.start = (old_start + 1) % LOOKAHEAD_BUFFER_CAPACITY;\n-            mem::replace(&mut self.lookahead_buffer.buffer[old_start], Default::default())\n-        };\n+        let next = self.next_tok();\n         self.span = next.sp;\n         self.token = next.tok;\n-        self.tokens_consumed += 1;\n         self.expected_tokens.clear();\n         // check after each token\n         self.check_unknown_macro_variable();\n@@ -935,18 +892,20 @@ impl<'a> Parser<'a> {\n         F: FnOnce(&token::Token) -> R,\n     {\n         if dist == 0 {\n-            f(&self.token)\n-        } else if dist < LOOKAHEAD_BUFFER_CAPACITY {\n-            while self.lookahead_buffer.len() < dist {\n-                self.lookahead_buffer.buffer[self.lookahead_buffer.end] = self.next_tok();\n-                self.lookahead_buffer.end =\n-                    (self.lookahead_buffer.end + 1) % LOOKAHEAD_BUFFER_CAPACITY;\n-            }\n-            let index = (self.lookahead_buffer.start + dist - 1) % LOOKAHEAD_BUFFER_CAPACITY;\n-            f(&self.lookahead_buffer.buffer[index].tok)\n-        } else {\n-            self.bug(\"lookahead distance is too large\");\n+            return f(&self.token);\n+        }\n+        let mut tok = token::Eof;\n+        if let Some(&(ref tts, mut i)) = self.tts.last() {\n+            i += dist - 1;\n+            if i < tts.len() {\n+                tok = match tts.get_tt(i) {\n+                    TokenTree::Token(_, tok) => tok,\n+                    TokenTree::Delimited(_, delimited) => token::OpenDelim(delimited.delim),\n+                    TokenTree::Sequence(..) => token::Dollar,\n+                };\n+            }\n         }\n+        f(&tok)\n     }\n     pub fn fatal(&self, m: &str) -> DiagnosticBuilder<'a> {\n         self.sess.span_diagnostic.struct_span_fatal(self.span, m)\n@@ -2743,116 +2702,38 @@ impl<'a> Parser<'a> {\n         // whether something will be a nonterminal or a seq\n         // yet.\n         match self.token {\n-            token::Eof => {\n-                let mut err: DiagnosticBuilder<'a> =\n-                    self.diagnostic().struct_span_err(self.span,\n-                                                      \"this file contains an un-closed delimiter\");\n-                for &(_, sp) in &self.open_braces {\n-                    err.span_help(sp, \"did you mean to close this delimiter?\");\n-                }\n-\n-                Err(err)\n-            },\n             token::OpenDelim(delim) => {\n-                if self.tts.last().map(|&(_, i)| i == 1).unwrap_or(false) {\n+                if self.quote_depth == 0 && self.tts.last().map(|&(_, i)| i == 1).unwrap_or(false) {\n                     let tt = self.tts.pop().unwrap().0;\n                     self.bump();\n-                    return Ok(if self.allow_interpolated_tts {\n-                        // avoid needlessly reparsing token trees in recursive macro expansions\n-                        TokenTree::Token(tt.span(), token::Interpolated(Rc::new(token::NtTT(tt))))\n-                    } else {\n-                        tt\n-                    });\n+                    return Ok(tt);\n                 }\n \n                 let parsing_token_tree = ::std::mem::replace(&mut self.parsing_token_tree, true);\n-                // The span for beginning of the delimited section\n-                let pre_span = self.span;\n-\n-                // Parse the open delimiter.\n-                self.open_braces.push((delim, self.span));\n                 let open_span = self.span;\n                 self.bump();\n-\n-                // Parse the token trees within the delimiters.\n-                // We stop at any delimiter so we can try to recover if the user\n-                // uses an incorrect delimiter.\n                 let tts = self.parse_seq_to_before_tokens(&[&token::CloseDelim(token::Brace),\n                                                             &token::CloseDelim(token::Paren),\n                                                             &token::CloseDelim(token::Bracket)],\n                                                           SeqSep::none(),\n                                                           |p| p.parse_token_tree(),\n                                                           |mut e| e.emit());\n+                self.parsing_token_tree = parsing_token_tree;\n \n                 let close_span = self.span;\n-                // Expand to cover the entire delimited token tree\n-                let span = Span { hi: close_span.hi, ..pre_span };\n-\n-                match self.token {\n-                    // Correct delimiter.\n-                    token::CloseDelim(d) if d == delim => {\n-                        self.open_braces.pop().unwrap();\n-\n-                        // Parse the close delimiter.\n-                        self.bump();\n-                    }\n-                    // Incorrect delimiter.\n-                    token::CloseDelim(other) => {\n-                        let token_str = self.this_token_to_string();\n-                        let mut err = self.diagnostic().struct_span_err(self.span,\n-                            &format!(\"incorrect close delimiter: `{}`\", token_str));\n-                        // This is a conservative error: only report the last unclosed delimiter.\n-                        // The previous unclosed delimiters could actually be closed! The parser\n-                        // just hasn't gotten to them yet.\n-                        if let Some(&(_, sp)) = self.open_braces.last() {\n-                            err.span_note(sp, \"unclosed delimiter\");\n-                        };\n-                        err.emit();\n-\n-                        self.open_braces.pop().unwrap();\n-\n-                        // If the incorrect delimiter matches an earlier opening\n-                        // delimiter, then don't consume it (it can be used to\n-                        // close the earlier one). Otherwise, consume it.\n-                        // E.g., we try to recover from:\n-                        // fn foo() {\n-                        //     bar(baz(\n-                        // }  // Incorrect delimiter but matches the earlier `{`\n-                        if !self.open_braces.iter().any(|&(b, _)| b == other) {\n-                            self.bump();\n-                        }\n-                    }\n-                    token::Eof => {\n-                        // Silently recover, the EOF token will be seen again\n-                        // and an error emitted then. Thus we don't pop from\n-                        // self.open_braces here.\n-                    },\n-                    _ => {}\n-                }\n+                self.bump();\n \n-                self.parsing_token_tree = parsing_token_tree;\n+                let span = Span { lo: open_span.lo, ..close_span };\n                 Ok(TokenTree::Delimited(span, Rc::new(Delimited {\n                     delim: delim,\n                     open_span: open_span,\n                     tts: tts,\n                     close_span: close_span,\n                 })))\n             },\n-            token::CloseDelim(_) => {\n-                // An unexpected closing delimiter (i.e., there is no\n-                // matching opening delimiter).\n-                let token_str = self.this_token_to_string();\n-                let err = self.diagnostic().struct_span_err(self.span,\n-                    &format!(\"unexpected close delimiter: `{}`\", token_str));\n-                Err(err)\n-            },\n-            /* we ought to allow different depths of unquotation */\n-            token::Dollar | token::SubstNt(..) if self.quote_depth > 0 => {\n-                self.parse_unquoted()\n-            }\n-            _ => {\n-                Ok(TokenTree::Token(self.span, self.bump_and_get()))\n-            }\n+            token::CloseDelim(_) | token::Eof => unreachable!(),\n+            token::Dollar | token::SubstNt(..) if self.quote_depth > 0 => self.parse_unquoted(),\n+            _ => Ok(TokenTree::Token(self.span, self.bump_and_get())),\n         }\n     }\n "}, {"sha": "061e871fe52295a4230a43928ce07b2c2ff336a5", "filename": "src/libsyntax/print/pprust.rs", "status": "modified", "additions": 6, "deletions": 15, "changes": 21, "blob_url": "https://github.com/rust-lang/rust/blob/17294d98b915364f64a8a1cbc071aa6ee200a03b/src%2Flibsyntax%2Fprint%2Fpprust.rs", "raw_url": "https://github.com/rust-lang/rust/raw/17294d98b915364f64a8a1cbc071aa6ee200a03b/src%2Flibsyntax%2Fprint%2Fpprust.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fprint%2Fpprust.rs?ref=17294d98b915364f64a8a1cbc071aa6ee200a03b", "patch": "@@ -18,10 +18,9 @@ use util::parser::AssocOp;\n use attr;\n use codemap::{self, CodeMap};\n use syntax_pos::{self, BytePos};\n-use errors;\n use parse::token::{self, BinOpToken, Token};\n use parse::lexer::comments;\n-use parse;\n+use parse::{self, ParseSess};\n use print::pp::{self, break_offset, word, space, zerobreak, hardbreak};\n use print::pp::{Breaks, eof};\n use print::pp::Breaks::{Consistent, Inconsistent};\n@@ -101,20 +100,15 @@ pub const DEFAULT_COLUMNS: usize = 78;\n /// it can scan the input text for comments and literals to\n /// copy forward.\n pub fn print_crate<'a>(cm: &'a CodeMap,\n-                       span_diagnostic: &errors::Handler,\n+                       sess: &ParseSess,\n                        krate: &ast::Crate,\n                        filename: String,\n                        input: &mut Read,\n                        out: Box<Write+'a>,\n                        ann: &'a PpAnn,\n                        is_expanded: bool) -> io::Result<()> {\n-    let mut s = State::new_from_input(cm,\n-                                      span_diagnostic,\n-                                      filename,\n-                                      input,\n-                                      out,\n-                                      ann,\n-                                      is_expanded);\n+    let mut s = State::new_from_input(cm, sess, filename, input, out, ann, is_expanded);\n+\n     if is_expanded && !std_inject::injected_crate_name(krate).is_none() {\n         // We need to print `#![no_std]` (and its feature gate) so that\n         // compiling pretty-printed source won't inject libstd again.\n@@ -140,16 +134,13 @@ pub fn print_crate<'a>(cm: &'a CodeMap,\n \n impl<'a> State<'a> {\n     pub fn new_from_input(cm: &'a CodeMap,\n-                          span_diagnostic: &errors::Handler,\n+                          sess: &ParseSess,\n                           filename: String,\n                           input: &mut Read,\n                           out: Box<Write+'a>,\n                           ann: &'a PpAnn,\n                           is_expanded: bool) -> State<'a> {\n-        let (cmnts, lits) = comments::gather_comments_and_literals(\n-            span_diagnostic,\n-            filename,\n-            input);\n+        let (cmnts, lits) = comments::gather_comments_and_literals(sess, filename, input);\n \n         State::new(\n             cm,"}, {"sha": "ab5dc8181e05bbc71ff07ff148d6a1d83b42a120", "filename": "src/libsyntax/tokenstream.rs", "status": "modified", "additions": 8, "deletions": 5, "changes": 13, "blob_url": "https://github.com/rust-lang/rust/blob/17294d98b915364f64a8a1cbc071aa6ee200a03b/src%2Flibsyntax%2Ftokenstream.rs", "raw_url": "https://github.com/rust-lang/rust/raw/17294d98b915364f64a8a1cbc071aa6ee200a03b/src%2Flibsyntax%2Ftokenstream.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Ftokenstream.rs?ref=17294d98b915364f64a8a1cbc071aa6ee200a03b", "patch": "@@ -30,7 +30,6 @@ use codemap::{Spanned, combine_spans};\n use ext::base;\n use ext::tt::macro_parser;\n use parse::lexer::comments::{doc_comment_style, strip_doc_comment_decoration};\n-use parse::lexer;\n use parse::{self, Directory};\n use parse::token::{self, Token, Lit, Nonterminal};\n use print::pprust;\n@@ -139,7 +138,10 @@ impl TokenTree {\n                 if let Nonterminal::NtTT(..) = **nt { 1 } else { 0 }\n             },\n             TokenTree::Token(_, token::MatchNt(..)) => 3,\n-            TokenTree::Delimited(_, ref delimed) => delimed.tts.len() + 2,\n+            TokenTree::Delimited(_, ref delimed) => match delimed.delim {\n+                token::NoDelim => delimed.tts.len(),\n+                _ => delimed.tts.len() + 2,\n+            },\n             TokenTree::Sequence(_, ref seq) => seq.tts.len(),\n             TokenTree::Token(..) => 0,\n         }\n@@ -181,6 +183,9 @@ impl TokenTree {\n                     close_span: sp,\n                 }))\n             }\n+            (&TokenTree::Delimited(_, ref delimed), _) if delimed.delim == token::NoDelim => {\n+                delimed.tts[index].clone()\n+            }\n             (&TokenTree::Delimited(_, ref delimed), _) => {\n                 if index == 0 {\n                     return delimed.open_tt();\n@@ -215,14 +220,12 @@ impl TokenTree {\n                  mtch: &[TokenTree],\n                  tts: &[TokenTree])\n                  -> macro_parser::NamedParseResult {\n-        let diag = &cx.parse_sess().span_diagnostic;\n         // `None` is because we're not interpolating\n-        let arg_rdr = lexer::new_tt_reader(diag, None, tts.iter().cloned().collect());\n         let directory = Directory {\n             path: cx.current_expansion.module.directory.clone(),\n             ownership: cx.current_expansion.directory_ownership,\n         };\n-        macro_parser::parse(cx.parse_sess(), arg_rdr, mtch, Some(directory))\n+        macro_parser::parse(cx.parse_sess(), tts.iter().cloned().collect(), mtch, Some(directory))\n     }\n \n     /// Check if this TokenTree is equal to the other, regardless of span information."}, {"sha": "e3c17af82aab403d2eda1ce51af24d796447e050", "filename": "src/test/parse-fail/issue-33569.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/17294d98b915364f64a8a1cbc071aa6ee200a03b/src%2Ftest%2Fparse-fail%2Fissue-33569.rs", "raw_url": "https://github.com/rust-lang/rust/raw/17294d98b915364f64a8a1cbc071aa6ee200a03b/src%2Ftest%2Fparse-fail%2Fissue-33569.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ftest%2Fparse-fail%2Fissue-33569.rs?ref=17294d98b915364f64a8a1cbc071aa6ee200a03b", "patch": "@@ -13,6 +13,6 @@\n macro_rules! foo {\n     { $+ } => { //~ ERROR expected identifier, found `+`\n         $(x)(y) //~ ERROR expected `*` or `+`\n-                //~^ ERROR no rules expected the token `y`\n+                //~^ ERROR no rules expected the token `)`\n     }\n }"}]}