{"sha": "585d4d29d91065c14fb823b8044495a6e5e857c1", "node_id": "MDY6Q29tbWl0NzI0NzEyOjU4NWQ0ZDI5ZDkxMDY1YzE0ZmI4MjNiODA0NDQ5NWE2ZTVlODU3YzE=", "commit": {"author": {"name": "Mazdak Farrokhzad", "email": "twingoow@gmail.com", "date": "2019-02-23T08:25:26Z"}, "committer": {"name": "GitHub", "email": "noreply@github.com", "date": "2019-02-23T08:25:26Z"}, "message": "Rollup merge of #58476 - nnethercote:rm-LazyTokenStream, r=petrochenkov\n\nRemove `LazyTokenStream`.\n\n`LazyTokenStream` was added in #40939. Perhaps it was an effective optimization then, but no longer. This PR removes it, making the code both simpler and faster.\n\nr? @alexcrichton", "tree": {"sha": "baa24cd750a1bcb6a9939d0430bc36b0c11a5d19", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/baa24cd750a1bcb6a9939d0430bc36b0c11a5d19"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/585d4d29d91065c14fb823b8044495a6e5e857c1", "comment_count": 0, "verification": {"verified": true, "reason": "valid", "signature": "-----BEGIN PGP SIGNATURE-----\n\nwsBcBAABCAAQBQJccQN2CRBK7hj4Ov3rIwAAdHIIABKCGz+QnB06+brkTRMMqCn7\n0mHmMpqKmE0qdIccu6QPIwUh2X5D+mT7q5PTy6HP1iTCUO9GW5I+MjE4w36Vrf0a\nG3VMFtO0lReVtCQitAA5Z91ZobshIlrrIyorfBAVdu9H+oAWbB9qLiKDe2gGd7cM\nuyGR7xNfSg/OT5XhZsMwu1WypV5+CgBRchgKXlZRnp51QlKIzm6P6Hb8IDkn3IJv\nhewdxkT5N0p09rpsy8Ezt4D6iWOpnNDu0FMUNYR+rSvyv98mZQm8uwp1gjRJby1v\ng7xFtparJiwHXBX5pOqefjpnVlwv7LSLTt9OSc7fIiI6bwNBHFJiafUsxukkTkM=\n=5nNi\n-----END PGP SIGNATURE-----\n", "payload": "tree baa24cd750a1bcb6a9939d0430bc36b0c11a5d19\nparent 4f99061874f60fa04eb6868ddd70cfea25995fec\nparent 895a79423bf5298e13a177ee6317f43380d437bc\nauthor Mazdak Farrokhzad <twingoow@gmail.com> 1550910326 +0100\ncommitter GitHub <noreply@github.com> 1550910326 +0100\n\nRollup merge of #58476 - nnethercote:rm-LazyTokenStream, r=petrochenkov\n\nRemove `LazyTokenStream`.\n\n`LazyTokenStream` was added in #40939. Perhaps it was an effective optimization then, but no longer. This PR removes it, making the code both simpler and faster.\n\nr? @alexcrichton\n"}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/585d4d29d91065c14fb823b8044495a6e5e857c1", "html_url": "https://github.com/rust-lang/rust/commit/585d4d29d91065c14fb823b8044495a6e5e857c1", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/585d4d29d91065c14fb823b8044495a6e5e857c1/comments", "author": {"login": "Centril", "id": 855702, "node_id": "MDQ6VXNlcjg1NTcwMg==", "avatar_url": "https://avatars.githubusercontent.com/u/855702?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Centril", "html_url": "https://github.com/Centril", "followers_url": "https://api.github.com/users/Centril/followers", "following_url": "https://api.github.com/users/Centril/following{/other_user}", "gists_url": "https://api.github.com/users/Centril/gists{/gist_id}", "starred_url": "https://api.github.com/users/Centril/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Centril/subscriptions", "organizations_url": "https://api.github.com/users/Centril/orgs", "repos_url": "https://api.github.com/users/Centril/repos", "events_url": "https://api.github.com/users/Centril/events{/privacy}", "received_events_url": "https://api.github.com/users/Centril/received_events", "type": "User", "site_admin": false}, "committer": {"login": "web-flow", "id": 19864447, "node_id": "MDQ6VXNlcjE5ODY0NDQ3", "avatar_url": "https://avatars.githubusercontent.com/u/19864447?v=4", "gravatar_id": "", "url": "https://api.github.com/users/web-flow", "html_url": "https://github.com/web-flow", "followers_url": "https://api.github.com/users/web-flow/followers", "following_url": "https://api.github.com/users/web-flow/following{/other_user}", "gists_url": "https://api.github.com/users/web-flow/gists{/gist_id}", "starred_url": "https://api.github.com/users/web-flow/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/web-flow/subscriptions", "organizations_url": "https://api.github.com/users/web-flow/orgs", "repos_url": "https://api.github.com/users/web-flow/repos", "events_url": "https://api.github.com/users/web-flow/events{/privacy}", "received_events_url": "https://api.github.com/users/web-flow/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "4f99061874f60fa04eb6868ddd70cfea25995fec", "url": "https://api.github.com/repos/rust-lang/rust/commits/4f99061874f60fa04eb6868ddd70cfea25995fec", "html_url": "https://github.com/rust-lang/rust/commit/4f99061874f60fa04eb6868ddd70cfea25995fec"}, {"sha": "895a79423bf5298e13a177ee6317f43380d437bc", "url": "https://api.github.com/repos/rust-lang/rust/commits/895a79423bf5298e13a177ee6317f43380d437bc", "html_url": "https://github.com/rust-lang/rust/commit/895a79423bf5298e13a177ee6317f43380d437bc"}], "stats": {"total": 394, "additions": 168, "deletions": 226}, "files": [{"sha": "f6b68682886e713f115ac391f0f0f04f5cdfc691", "filename": "src/librustc/hir/lowering.rs", "status": "modified", "additions": 6, "deletions": 6, "changes": 12, "blob_url": "https://github.com/rust-lang/rust/blob/585d4d29d91065c14fb823b8044495a6e5e857c1/src%2Flibrustc%2Fhir%2Flowering.rs", "raw_url": "https://github.com/rust-lang/rust/raw/585d4d29d91065c14fb823b8044495a6e5e857c1/src%2Flibrustc%2Fhir%2Flowering.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Fhir%2Flowering.rs?ref=585d4d29d91065c14fb823b8044495a6e5e857c1", "patch": "@@ -1124,19 +1124,19 @@ impl<'a> LoweringContext<'a> {\n             TokenTree::Delimited(span, delim, tts) => TokenTree::Delimited(\n                 span,\n                 delim,\n-                self.lower_token_stream(tts.into()).into(),\n+                self.lower_token_stream(tts),\n             ).into(),\n         }\n     }\n \n     fn lower_token(&mut self, token: Token, span: Span) -> TokenStream {\n         match token {\n-            Token::Interpolated(_) => {}\n-            other => return TokenTree::Token(span, other).into(),\n+            Token::Interpolated(nt) => {\n+                let tts = nt.to_tokenstream(&self.sess.parse_sess, span);\n+                self.lower_token_stream(tts)\n+            }\n+            other => TokenTree::Token(span, other).into(),\n         }\n-\n-        let tts = token.interpolated_to_tokenstream(&self.sess.parse_sess, span);\n-        self.lower_token_stream(tts)\n     }\n \n     fn lower_arm(&mut self, arm: &Arm) -> hir::Arm {"}, {"sha": "72aa9570cc2ffa04f8025ec16e992677a65c5955", "filename": "src/librustc/hir/map/def_collector.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/585d4d29d91065c14fb823b8044495a6e5e857c1/src%2Flibrustc%2Fhir%2Fmap%2Fdef_collector.rs", "raw_url": "https://github.com/rust-lang/rust/raw/585d4d29d91065c14fb823b8044495a6e5e857c1/src%2Flibrustc%2Fhir%2Fmap%2Fdef_collector.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Fhir%2Fmap%2Fdef_collector.rs?ref=585d4d29d91065c14fb823b8044495a6e5e857c1", "patch": "@@ -339,7 +339,7 @@ impl<'a> visit::Visitor<'a> for DefCollector<'a> {\n \n     fn visit_token(&mut self, t: Token) {\n         if let Token::Interpolated(nt) = t {\n-            if let token::NtExpr(ref expr) = nt.0 {\n+            if let token::NtExpr(ref expr) = *nt {\n                 if let ExprKind::Mac(..) = expr.node {\n                     self.visit_macro_invoc(expr.id);\n                 }"}, {"sha": "29de5308a3cdb8055cfbeb6d25e5e34714e86aef", "filename": "src/librustc_resolve/build_reduced_graph.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/585d4d29d91065c14fb823b8044495a6e5e857c1/src%2Flibrustc_resolve%2Fbuild_reduced_graph.rs", "raw_url": "https://github.com/rust-lang/rust/raw/585d4d29d91065c14fb823b8044495a6e5e857c1/src%2Flibrustc_resolve%2Fbuild_reduced_graph.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_resolve%2Fbuild_reduced_graph.rs?ref=585d4d29d91065c14fb823b8044495a6e5e857c1", "patch": "@@ -1025,7 +1025,7 @@ impl<'a, 'b> Visitor<'a> for BuildReducedGraphVisitor<'a, 'b> {\n \n     fn visit_token(&mut self, t: Token) {\n         if let Token::Interpolated(nt) = t {\n-            if let token::NtExpr(ref expr) = nt.0 {\n+            if let token::NtExpr(ref expr) = *nt {\n                 if let ast::ExprKind::Mac(..) = expr.node {\n                     self.visit_invoc(expr.id);\n                 }"}, {"sha": "b5fc8507314047a3fffb19635bb1e61b8723e49c", "filename": "src/libsyntax/attr/mod.rs", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/rust-lang/rust/blob/585d4d29d91065c14fb823b8044495a6e5e857c1/src%2Flibsyntax%2Fattr%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/585d4d29d91065c14fb823b8044495a6e5e857c1/src%2Flibsyntax%2Fattr%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fattr%2Fmod.rs?ref=585d4d29d91065c14fb823b8044495a6e5e857c1", "patch": "@@ -517,7 +517,7 @@ impl MetaItem {\n                 let span = span.with_hi(segments.last().unwrap().ident.span.hi());\n                 Path { span, segments }\n             }\n-            Some(TokenTree::Token(_, Token::Interpolated(ref nt))) => match nt.0 {\n+            Some(TokenTree::Token(_, Token::Interpolated(nt))) => match *nt {\n                 token::Nonterminal::NtIdent(ident, _) => Path::from_ident(ident),\n                 token::Nonterminal::NtMeta(ref meta) => return Some(meta.clone()),\n                 token::Nonterminal::NtPath(ref path) => path.clone(),\n@@ -682,7 +682,7 @@ impl LitKind {\n         match token {\n             Token::Ident(ident, false) if ident.name == \"true\" => Some(LitKind::Bool(true)),\n             Token::Ident(ident, false) if ident.name == \"false\" => Some(LitKind::Bool(false)),\n-            Token::Interpolated(ref nt) => match nt.0 {\n+            Token::Interpolated(nt) => match *nt {\n                 token::NtExpr(ref v) | token::NtLiteral(ref v) => match v.node {\n                     ExprKind::Lit(ref lit) => Some(lit.node.clone()),\n                     _ => None,"}, {"sha": "452cc2f2c65ccc68cee03502b9710cb35987f1ad", "filename": "src/libsyntax/ext/base.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/585d4d29d91065c14fb823b8044495a6e5e857c1/src%2Flibsyntax%2Fext%2Fbase.rs", "raw_url": "https://github.com/rust-lang/rust/raw/585d4d29d91065c14fb823b8044495a6e5e857c1/src%2Flibsyntax%2Fext%2Fbase.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fext%2Fbase.rs?ref=585d4d29d91065c14fb823b8044495a6e5e857c1", "patch": "@@ -266,7 +266,7 @@ impl<F> TTMacroExpander for F\n         impl MutVisitor for AvoidInterpolatedIdents {\n             fn visit_tt(&mut self, tt: &mut tokenstream::TokenTree) {\n                 if let tokenstream::TokenTree::Token(_, token::Interpolated(nt)) = tt {\n-                    if let token::NtIdent(ident, is_raw) = nt.0 {\n+                    if let token::NtIdent(ident, is_raw) = **nt {\n                         *tt = tokenstream::TokenTree::Token(ident.span,\n                                                             token::Ident(ident, is_raw));\n                     }"}, {"sha": "b805213bb1a4cad139ed72cebb9c5cdf80d4acde", "filename": "src/libsyntax/ext/expand.rs", "status": "modified", "additions": 3, "deletions": 2, "changes": 5, "blob_url": "https://github.com/rust-lang/rust/blob/585d4d29d91065c14fb823b8044495a6e5e857c1/src%2Flibsyntax%2Fext%2Fexpand.rs", "raw_url": "https://github.com/rust-lang/rust/raw/585d4d29d91065c14fb823b8044495a6e5e857c1/src%2Flibsyntax%2Fext%2Fexpand.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fext%2Fexpand.rs?ref=585d4d29d91065c14fb823b8044495a6e5e857c1", "patch": "@@ -25,6 +25,7 @@ use syntax_pos::{Span, DUMMY_SP, FileName};\n use syntax_pos::hygiene::ExpnFormat;\n \n use rustc_data_structures::fx::FxHashMap;\n+use rustc_data_structures::sync::Lrc;\n use std::fs;\n use std::io::ErrorKind;\n use std::{iter, mem};\n@@ -584,14 +585,14 @@ impl<'a, 'b> MacroExpander<'a, 'b> {\n             }\n             AttrProcMacro(ref mac, ..) => {\n                 self.gate_proc_macro_attr_item(attr.span, &item);\n-                let item_tok = TokenTree::Token(DUMMY_SP, Token::interpolated(match item {\n+                let item_tok = TokenTree::Token(DUMMY_SP, Token::Interpolated(Lrc::new(match item {\n                     Annotatable::Item(item) => token::NtItem(item),\n                     Annotatable::TraitItem(item) => token::NtTraitItem(item.into_inner()),\n                     Annotatable::ImplItem(item) => token::NtImplItem(item.into_inner()),\n                     Annotatable::ForeignItem(item) => token::NtForeignItem(item.into_inner()),\n                     Annotatable::Stmt(stmt) => token::NtStmt(stmt.into_inner()),\n                     Annotatable::Expr(expr) => token::NtExpr(expr),\n-                })).into();\n+                }))).into();\n                 let input = self.extract_proc_macro_attr_input(attr.tokens, attr.span);\n                 let tok_result = mac.expand(self.cx, attr.span, input, item_tok);\n                 let res = self.parse_ast_fragment(tok_result, invoc.fragment_kind,"}, {"sha": "fe1cffb092b1c30a3b7fa3af418a2eff4a42c537", "filename": "src/libsyntax/ext/tt/macro_parser.rs", "status": "modified", "additions": 15, "deletions": 14, "changes": 29, "blob_url": "https://github.com/rust-lang/rust/blob/585d4d29d91065c14fb823b8044495a6e5e857c1/src%2Flibsyntax%2Fext%2Ftt%2Fmacro_parser.rs", "raw_url": "https://github.com/rust-lang/rust/raw/585d4d29d91065c14fb823b8044495a6e5e857c1/src%2Flibsyntax%2Fext%2Ftt%2Fmacro_parser.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fext%2Ftt%2Fmacro_parser.rs?ref=585d4d29d91065c14fb823b8044495a6e5e857c1", "patch": "@@ -88,6 +88,7 @@ use smallvec::{smallvec, SmallVec};\n use syntax_pos::Span;\n \n use rustc_data_structures::fx::FxHashMap;\n+use rustc_data_structures::sync::Lrc;\n use std::collections::hash_map::Entry::{Occupied, Vacant};\n use std::mem;\n use std::ops::{Deref, DerefMut};\n@@ -179,7 +180,7 @@ struct MatcherPos<'root, 'tt: 'root> {\n     /// all bound matches from the submatcher into the shared top-level `matches` vector. If `sep`\n     /// and `up` are `Some`, then `matches` is _not_ the shared top-level list. Instead, if one\n     /// wants the shared `matches`, one should use `up.matches`.\n-    matches: Box<[Rc<NamedMatchVec>]>,\n+    matches: Box<[Lrc<NamedMatchVec>]>,\n     /// The position in `matches` corresponding to the first metavar in this matcher's sequence of\n     /// token trees. In other words, the first metavar in the first token of `top_elts` corresponds\n     /// to `matches[match_lo]`.\n@@ -218,7 +219,7 @@ struct MatcherPos<'root, 'tt: 'root> {\n impl<'root, 'tt> MatcherPos<'root, 'tt> {\n     /// Adds `m` as a named match for the `idx`-th metavar.\n     fn push_match(&mut self, idx: usize, m: NamedMatch) {\n-        let matches = Rc::make_mut(&mut self.matches[idx]);\n+        let matches = Lrc::make_mut(&mut self.matches[idx]);\n         matches.push(m);\n     }\n }\n@@ -295,11 +296,11 @@ pub fn count_names(ms: &[TokenTree]) -> usize {\n }\n \n /// `len` `Vec`s (initially shared and empty) that will store matches of metavars.\n-fn create_matches(len: usize) -> Box<[Rc<NamedMatchVec>]> {\n+fn create_matches(len: usize) -> Box<[Lrc<NamedMatchVec>]> {\n     if len == 0 {\n         vec![]\n     } else {\n-        let empty_matches = Rc::new(SmallVec::new());\n+        let empty_matches = Lrc::new(SmallVec::new());\n         vec![empty_matches; len]\n     }.into_boxed_slice()\n }\n@@ -353,8 +354,8 @@ fn initial_matcher_pos<'root, 'tt>(ms: &'tt [TokenTree], open: Span) -> MatcherP\n /// token tree it was derived from.\n #[derive(Debug, Clone)]\n pub enum NamedMatch {\n-    MatchedSeq(Rc<NamedMatchVec>, DelimSpan),\n-    MatchedNonterminal(Rc<Nonterminal>),\n+    MatchedSeq(Lrc<NamedMatchVec>, DelimSpan),\n+    MatchedNonterminal(Lrc<Nonterminal>),\n }\n \n /// Takes a sequence of token trees `ms` representing a matcher which successfully matched input\n@@ -561,7 +562,7 @@ fn inner_parse_loop<'root, 'tt>(\n                         new_item.match_cur += seq.num_captures;\n                         new_item.idx += 1;\n                         for idx in item.match_cur..item.match_cur + seq.num_captures {\n-                            new_item.push_match(idx, MatchedSeq(Rc::new(smallvec![]), sp));\n+                            new_item.push_match(idx, MatchedSeq(Lrc::new(smallvec![]), sp));\n                         }\n                         cur_items.push(new_item);\n                     }\n@@ -707,7 +708,7 @@ pub fn parse(\n                 let matches = eof_items[0]\n                     .matches\n                     .iter_mut()\n-                    .map(|dv| Rc::make_mut(dv).pop().unwrap());\n+                    .map(|dv| Lrc::make_mut(dv).pop().unwrap());\n                 return nameize(sess, ms, matches);\n             } else if eof_items.len() > 1 {\n                 return Error(\n@@ -780,7 +781,7 @@ pub fn parse(\n                 let match_cur = item.match_cur;\n                 item.push_match(\n                     match_cur,\n-                    MatchedNonterminal(Rc::new(parse_nt(&mut parser, span, &ident.as_str()))),\n+                    MatchedNonterminal(Lrc::new(parse_nt(&mut parser, span, &ident.as_str()))),\n                 );\n                 item.idx += 1;\n                 item.match_cur += 1;\n@@ -829,7 +830,7 @@ fn may_begin_with(name: &str, token: &Token) -> bool {\n         },\n         \"block\" => match *token {\n             Token::OpenDelim(token::Brace) => true,\n-            Token::Interpolated(ref nt) => match nt.0 {\n+            Token::Interpolated(ref nt) => match **nt {\n                 token::NtItem(_)\n                 | token::NtPat(_)\n                 | token::NtTy(_)\n@@ -843,9 +844,9 @@ fn may_begin_with(name: &str, token: &Token) -> bool {\n         },\n         \"path\" | \"meta\" => match *token {\n             Token::ModSep | Token::Ident(..) => true,\n-            Token::Interpolated(ref nt) => match nt.0 {\n+            Token::Interpolated(ref nt) => match **nt {\n                 token::NtPath(_) | token::NtMeta(_) => true,\n-                _ => may_be_ident(&nt.0),\n+                _ => may_be_ident(&nt),\n             },\n             _ => false,\n         },\n@@ -862,12 +863,12 @@ fn may_begin_with(name: &str, token: &Token) -> bool {\n             Token::ModSep |                     // path\n             Token::Lt |                         // path (UFCS constant)\n             Token::BinOp(token::Shl) => true,   // path (double UFCS)\n-            Token::Interpolated(ref nt) => may_be_ident(&nt.0),\n+            Token::Interpolated(ref nt) => may_be_ident(nt),\n             _ => false,\n         },\n         \"lifetime\" => match *token {\n             Token::Lifetime(_) => true,\n-            Token::Interpolated(ref nt) => match nt.0 {\n+            Token::Interpolated(ref nt) => match **nt {\n                 token::NtLifetime(_) | token::NtTT(_) => true,\n                 _ => false,\n             },"}, {"sha": "bd2adb5ac13ba11ded836173314dcb1a69d3b916", "filename": "src/libsyntax/ext/tt/transcribe.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/585d4d29d91065c14fb823b8044495a6e5e857c1/src%2Flibsyntax%2Fext%2Ftt%2Ftranscribe.rs", "raw_url": "https://github.com/rust-lang/rust/raw/585d4d29d91065c14fb823b8044495a6e5e857c1/src%2Flibsyntax%2Fext%2Ftt%2Ftranscribe.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fext%2Ftt%2Ftranscribe.rs?ref=585d4d29d91065c14fb823b8044495a6e5e857c1", "patch": "@@ -149,7 +149,7 @@ pub fn transcribe(cx: &ExtCtxt<'_>,\n                             result.push(tt.clone().into());\n                         } else {\n                             sp = sp.apply_mark(cx.current_expansion.mark);\n-                            let token = TokenTree::Token(sp, Token::interpolated((**nt).clone()));\n+                            let token = TokenTree::Token(sp, Token::Interpolated(nt.clone()));\n                             result.push(token.into());\n                         }\n                     } else {"}, {"sha": "86849f580d081eda2fe59f9ce53c8769bf92070d", "filename": "src/libsyntax/mut_visit.rs", "status": "modified", "additions": 2, "deletions": 3, "changes": 5, "blob_url": "https://github.com/rust-lang/rust/blob/585d4d29d91065c14fb823b8044495a6e5e857c1/src%2Flibsyntax%2Fmut_visit.rs", "raw_url": "https://github.com/rust-lang/rust/raw/585d4d29d91065c14fb823b8044495a6e5e857c1/src%2Flibsyntax%2Fmut_visit.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fmut_visit.rs?ref=585d4d29d91065c14fb823b8044495a6e5e857c1", "patch": "@@ -581,9 +581,8 @@ pub fn noop_visit_token<T: MutVisitor>(t: &mut Token, vis: &mut T) {\n         token::Ident(id, _is_raw) => vis.visit_ident(id),\n         token::Lifetime(id) => vis.visit_ident(id),\n         token::Interpolated(nt) => {\n-            let nt = Lrc::make_mut(nt);\n-            vis.visit_interpolated(&mut nt.0);\n-            nt.1 = token::LazyTokenStream::new();\n+            let mut nt = Lrc::make_mut(nt);\n+            vis.visit_interpolated(&mut nt);\n         }\n         _ => {}\n     }"}, {"sha": "9020c8c6a2dc66fb9c3760e11bab580e4e72b4f0", "filename": "src/libsyntax/parse/attr.rs", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/rust-lang/rust/blob/585d4d29d91065c14fb823b8044495a6e5e857c1/src%2Flibsyntax%2Fparse%2Fattr.rs", "raw_url": "https://github.com/rust-lang/rust/raw/585d4d29d91065c14fb823b8044495a6e5e857c1/src%2Flibsyntax%2Fparse%2Fattr.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Fattr.rs?ref=585d4d29d91065c14fb823b8044495a6e5e857c1", "patch": "@@ -141,7 +141,7 @@ impl<'a> Parser<'a> {\n     /// The delimiters or `=` are still put into the resulting token stream.\n     crate fn parse_meta_item_unrestricted(&mut self) -> PResult<'a, (ast::Path, TokenStream)> {\n         let meta = match self.token {\n-            token::Interpolated(ref nt) => match nt.0 {\n+            token::Interpolated(ref nt) => match **nt {\n                 Nonterminal::NtMeta(ref meta) => Some(meta.clone()),\n                 _ => None,\n             },\n@@ -227,7 +227,7 @@ impl<'a> Parser<'a> {\n     /// meta_item_inner : (meta_item | UNSUFFIXED_LIT) (',' meta_item_inner)? ;\n     pub fn parse_meta_item(&mut self) -> PResult<'a, ast::MetaItem> {\n         let nt_meta = match self.token {\n-            token::Interpolated(ref nt) => match nt.0 {\n+            token::Interpolated(ref nt) => match **nt {\n                 token::NtMeta(ref e) => Some(e.clone()),\n                 _ => None,\n             },"}, {"sha": "bb2e9d8ed59edacea030b912df5bf9436bc7b066", "filename": "src/libsyntax/parse/parser.rs", "status": "modified", "additions": 8, "deletions": 8, "changes": 16, "blob_url": "https://github.com/rust-lang/rust/blob/585d4d29d91065c14fb823b8044495a6e5e857c1/src%2Flibsyntax%2Fparse%2Fparser.rs", "raw_url": "https://github.com/rust-lang/rust/raw/585d4d29d91065c14fb823b8044495a6e5e857c1/src%2Flibsyntax%2Fparse%2Fparser.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Fparser.rs?ref=585d4d29d91065c14fb823b8044495a6e5e857c1", "patch": "@@ -119,7 +119,7 @@ enum BlockMode {\n macro_rules! maybe_whole_expr {\n     ($p:expr) => {\n         if let token::Interpolated(nt) = $p.token.clone() {\n-            match nt.0 {\n+            match *nt {\n                 token::NtExpr(ref e) | token::NtLiteral(ref e) => {\n                     $p.bump();\n                     return Ok((*e).clone());\n@@ -146,7 +146,7 @@ macro_rules! maybe_whole_expr {\n macro_rules! maybe_whole {\n     ($p:expr, $constructor:ident, |$x:ident| $e:expr) => {\n         if let token::Interpolated(nt) = $p.token.clone() {\n-            if let token::$constructor($x) = nt.0.clone() {\n+            if let token::$constructor($x) = (*nt).clone() {\n                 $p.bump();\n                 return Ok($e);\n             }\n@@ -1570,7 +1570,7 @@ impl<'a> Parser<'a> {\n                     Some(body)\n                 }\n                 token::Interpolated(ref nt) => {\n-                    match &nt.0 {\n+                    match **nt {\n                         token::NtBlock(..) => {\n                             *at_end = true;\n                             let (inner_attrs, body) = self.parse_inner_attrs_and_block()?;\n@@ -1913,7 +1913,7 @@ impl<'a> Parser<'a> {\n \n     fn is_named_argument(&mut self) -> bool {\n         let offset = match self.token {\n-            token::Interpolated(ref nt) => match nt.0 {\n+            token::Interpolated(ref nt) => match **nt {\n                 token::NtPat(..) => return self.look_ahead(1, |t| t == &token::Colon),\n                 _ => 0,\n             }\n@@ -2099,7 +2099,7 @@ impl<'a> Parser<'a> {\n     /// Matches `token_lit = LIT_INTEGER | ...`.\n     fn parse_lit_token(&mut self) -> PResult<'a, LitKind> {\n         let out = match self.token {\n-            token::Interpolated(ref nt) => match nt.0 {\n+            token::Interpolated(ref nt) => match **nt {\n                 token::NtExpr(ref v) | token::NtLiteral(ref v) => match v.node {\n                     ExprKind::Lit(ref lit) => { lit.node.clone() }\n                     _ => { return self.unexpected_last(&self.token); }\n@@ -2299,7 +2299,7 @@ impl<'a> Parser<'a> {\n     /// attributes.\n     pub fn parse_path_allowing_meta(&mut self, style: PathStyle) -> PResult<'a, ast::Path> {\n         let meta_ident = match self.token {\n-            token::Interpolated(ref nt) => match nt.0 {\n+            token::Interpolated(ref nt) => match **nt {\n                 token::NtMeta(ref meta) => match meta.node {\n                     ast::MetaItemKind::Word => Some(meta.ident.clone()),\n                     _ => None,\n@@ -3271,7 +3271,7 @@ impl<'a> Parser<'a> {\n                 self.meta_var_span = Some(self.span);\n                 // Interpolated identifier and lifetime tokens are replaced with usual identifier\n                 // and lifetime tokens, so the former are never encountered during normal parsing.\n-                match nt.0 {\n+                match **nt {\n                     token::NtIdent(ident, is_raw) => (token::Ident(ident, is_raw), ident.span),\n                     token::NtLifetime(ident) => (token::Lifetime(ident), ident.span),\n                     _ => return,\n@@ -3403,7 +3403,7 @@ impl<'a> Parser<'a> {\n                     // can't continue an expression after an ident\n                     token::Ident(ident, is_raw) => token::ident_can_begin_expr(ident, is_raw),\n                     token::Literal(..) | token::Pound => true,\n-                    token::Interpolated(ref nt) => match nt.0 {\n+                    token::Interpolated(ref nt) => match **nt {\n                         token::NtIdent(..) | token::NtExpr(..) |\n                         token::NtBlock(..) | token::NtPath(..) => true,\n                         _ => false,"}, {"sha": "eec422d6266c36574f01db02cb660d3510d26833", "filename": "src/libsyntax/parse/token.rs", "status": "modified", "additions": 92, "deletions": 156, "changes": 248, "blob_url": "https://github.com/rust-lang/rust/blob/585d4d29d91065c14fb823b8044495a6e5e857c1/src%2Flibsyntax%2Fparse%2Ftoken.rs", "raw_url": "https://github.com/rust-lang/rust/raw/585d4d29d91065c14fb823b8044495a6e5e857c1/src%2Flibsyntax%2Fparse%2Ftoken.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Ftoken.rs?ref=585d4d29d91065c14fb823b8044495a6e5e857c1", "patch": "@@ -13,16 +13,15 @@ use crate::syntax::parse::parse_stream_from_source_str;\n use crate::syntax::parse::parser::emit_unclosed_delims;\n use crate::tokenstream::{self, DelimSpan, TokenStream, TokenTree};\n \n-use serialize::{Decodable, Decoder, Encodable, Encoder};\n use syntax_pos::symbol::{self, Symbol};\n use syntax_pos::{self, Span, FileName};\n use log::info;\n \n-use std::{cmp, fmt};\n+use std::fmt;\n use std::mem;\n #[cfg(target_arch = \"x86_64\")]\n use rustc_data_structures::static_assert;\n-use rustc_data_structures::sync::{Lrc, Lock};\n+use rustc_data_structures::sync::Lrc;\n \n #[derive(Clone, PartialEq, RustcEncodable, RustcDecodable, Hash, Debug, Copy)]\n pub enum BinOpToken {\n@@ -87,7 +86,7 @@ impl Lit {\n         }\n     }\n \n-    // See comments in `interpolated_to_tokenstream` for why we care about\n+    // See comments in `Nonterminal::to_tokenstream` for why we care about\n     // *probably* equal here rather than actual equality\n     fn probably_equal_for_proc_macro(&self, other: &Lit) -> bool {\n         mem::discriminant(self) == mem::discriminant(other)\n@@ -184,9 +183,8 @@ pub enum Token {\n     Ident(ast::Ident, /* is_raw */ bool),\n     Lifetime(ast::Ident),\n \n-    // The `LazyTokenStream` is a pure function of the `Nonterminal`,\n-    // and so the `LazyTokenStream` can be ignored by Eq, Hash, etc.\n-    Interpolated(Lrc<(Nonterminal, LazyTokenStream)>),\n+    Interpolated(Lrc<Nonterminal>),\n+\n     // Can be expanded into several tokens.\n     /// A doc comment.\n     DocComment(ast::Name),\n@@ -209,10 +207,6 @@ pub enum Token {\n static_assert!(MEM_SIZE_OF_STATEMENT: mem::size_of::<Token>() == 16);\n \n impl Token {\n-    pub fn interpolated(nt: Nonterminal) -> Token {\n-        Token::Interpolated(Lrc::new((nt, LazyTokenStream::new())))\n-    }\n-\n     /// Recovers a `Token` from an `ast::Ident`. This creates a raw identifier if necessary.\n     pub fn from_ast_ident(ident: ast::Ident) -> Token {\n         Ident(ident, ident.is_raw_guess())\n@@ -244,7 +238,7 @@ impl Token {\n             ModSep                            | // global path\n             Lifetime(..)                      | // labeled loop\n             Pound                             => true, // expression attributes\n-            Interpolated(ref nt) => match nt.0 {\n+            Interpolated(ref nt) => match **nt {\n                 NtLiteral(..) |\n                 NtIdent(..)   |\n                 NtExpr(..)    |\n@@ -272,7 +266,7 @@ impl Token {\n             Lifetime(..)                | // lifetime bound in trait object\n             Lt | BinOp(Shl)             | // associated path\n             ModSep                      => true, // global path\n-            Interpolated(ref nt) => match nt.0 {\n+            Interpolated(ref nt) => match **nt {\n                 NtIdent(..) | NtTy(..) | NtPath(..) | NtLifetime(..) => true,\n                 _ => false,\n             },\n@@ -284,7 +278,7 @@ impl Token {\n     pub fn can_begin_const_arg(&self) -> bool {\n         match self {\n             OpenDelim(Brace) => true,\n-            Interpolated(ref nt) => match nt.0 {\n+            Interpolated(ref nt) => match **nt {\n                 NtExpr(..) => true,\n                 NtBlock(..) => true,\n                 NtLiteral(..) => true,\n@@ -316,7 +310,7 @@ impl Token {\n             BinOp(Minus) => true,\n             Ident(ident, false) if ident.name == keywords::True.name() => true,\n             Ident(ident, false) if ident.name == keywords::False.name() => true,\n-            Interpolated(ref nt) => match nt.0 {\n+            Interpolated(ref nt) => match **nt {\n                 NtLiteral(..) => true,\n                 _             => false,\n             },\n@@ -328,7 +322,7 @@ impl Token {\n     pub fn ident(&self) -> Option<(ast::Ident, /* is_raw */ bool)> {\n         match *self {\n             Ident(ident, is_raw) => Some((ident, is_raw)),\n-            Interpolated(ref nt) => match nt.0 {\n+            Interpolated(ref nt) => match **nt {\n                 NtIdent(ident, is_raw) => Some((ident, is_raw)),\n                 _ => None,\n             },\n@@ -339,7 +333,7 @@ impl Token {\n     pub fn lifetime(&self) -> Option<ast::Ident> {\n         match *self {\n             Lifetime(ident) => Some(ident),\n-            Interpolated(ref nt) => match nt.0 {\n+            Interpolated(ref nt) => match **nt {\n                 NtLifetime(ident) => Some(ident),\n                 _ => None,\n             },\n@@ -367,7 +361,7 @@ impl Token {\n     /// Returns `true` if the token is an interpolated path.\n     fn is_path(&self) -> bool {\n         if let Interpolated(ref nt) = *self {\n-            if let NtPath(..) = nt.0 {\n+            if let NtPath(..) = **nt {\n                 return true;\n             }\n         }\n@@ -508,98 +502,7 @@ impl Token {\n         }\n     }\n \n-    pub fn interpolated_to_tokenstream(&self, sess: &ParseSess, span: Span)\n-        -> TokenStream\n-    {\n-        let nt = match *self {\n-            Token::Interpolated(ref nt) => nt,\n-            _ => panic!(\"only works on interpolated tokens\"),\n-        };\n-\n-        // An `Interpolated` token means that we have a `Nonterminal`\n-        // which is often a parsed AST item. At this point we now need\n-        // to convert the parsed AST to an actual token stream, e.g.\n-        // un-parse it basically.\n-        //\n-        // Unfortunately there's not really a great way to do that in a\n-        // guaranteed lossless fashion right now. The fallback here is\n-        // to just stringify the AST node and reparse it, but this loses\n-        // all span information.\n-        //\n-        // As a result, some AST nodes are annotated with the token\n-        // stream they came from. Here we attempt to extract these\n-        // lossless token streams before we fall back to the\n-        // stringification.\n-        let mut tokens = None;\n-\n-        match nt.0 {\n-            Nonterminal::NtItem(ref item) => {\n-                tokens = prepend_attrs(sess, &item.attrs, item.tokens.as_ref(), span);\n-            }\n-            Nonterminal::NtTraitItem(ref item) => {\n-                tokens = prepend_attrs(sess, &item.attrs, item.tokens.as_ref(), span);\n-            }\n-            Nonterminal::NtImplItem(ref item) => {\n-                tokens = prepend_attrs(sess, &item.attrs, item.tokens.as_ref(), span);\n-            }\n-            Nonterminal::NtIdent(ident, is_raw) => {\n-                let token = Token::Ident(ident, is_raw);\n-                tokens = Some(TokenTree::Token(ident.span, token).into());\n-            }\n-            Nonterminal::NtLifetime(ident) => {\n-                let token = Token::Lifetime(ident);\n-                tokens = Some(TokenTree::Token(ident.span, token).into());\n-            }\n-            Nonterminal::NtTT(ref tt) => {\n-                tokens = Some(tt.clone().into());\n-            }\n-            _ => {}\n-        }\n-\n-        let tokens_for_real = nt.1.force(|| {\n-            // FIXME(#43081): Avoid this pretty-print + reparse hack\n-            let source = pprust::token_to_string(self);\n-            let filename = FileName::macro_expansion_source_code(&source);\n-            let (tokens, errors) = parse_stream_from_source_str(\n-                filename, source, sess, Some(span));\n-            emit_unclosed_delims(&errors, &sess.span_diagnostic);\n-            tokens\n-        });\n-\n-        // During early phases of the compiler the AST could get modified\n-        // directly (e.g., attributes added or removed) and the internal cache\n-        // of tokens my not be invalidated or updated. Consequently if the\n-        // \"lossless\" token stream disagrees with our actual stringification\n-        // (which has historically been much more battle-tested) then we go\n-        // with the lossy stream anyway (losing span information).\n-        //\n-        // Note that the comparison isn't `==` here to avoid comparing spans,\n-        // but it *also* is a \"probable\" equality which is a pretty weird\n-        // definition. We mostly want to catch actual changes to the AST\n-        // like a `#[cfg]` being processed or some weird `macro_rules!`\n-        // expansion.\n-        //\n-        // What we *don't* want to catch is the fact that a user-defined\n-        // literal like `0xf` is stringified as `15`, causing the cached token\n-        // stream to not be literal `==` token-wise (ignoring spans) to the\n-        // token stream we got from stringification.\n-        //\n-        // Instead the \"probably equal\" check here is \"does each token\n-        // recursively have the same discriminant?\" We basically don't look at\n-        // the token values here and assume that such fine grained token stream\n-        // modifications, including adding/removing typically non-semantic\n-        // tokens such as extra braces and commas, don't happen.\n-        if let Some(tokens) = tokens {\n-            if tokens.probably_equal_for_proc_macro(&tokens_for_real) {\n-                return tokens\n-            }\n-            info!(\"cached tokens found, but they're not \\\"probably equal\\\", \\\n-                   going with stringified version\");\n-        }\n-        return tokens_for_real\n-    }\n-\n-    // See comments in `interpolated_to_tokenstream` for why we care about\n+    // See comments in `Nonterminal::to_tokenstream` for why we care about\n     // *probably* equal here rather than actual equality\n     crate fn probably_equal_for_proc_macro(&self, other: &Token) -> bool {\n         if mem::discriminant(self) != mem::discriminant(other) {\n@@ -731,61 +634,94 @@ impl fmt::Debug for Nonterminal {\n     }\n }\n \n-crate fn is_op(tok: &Token) -> bool {\n-    match *tok {\n-        OpenDelim(..) | CloseDelim(..) | Literal(..) | DocComment(..) |\n-        Ident(..) | Lifetime(..) | Interpolated(..) |\n-        Whitespace | Comment | Shebang(..) | Eof => false,\n-        _ => true,\n-    }\n-}\n-\n-#[derive(Clone)]\n-pub struct LazyTokenStream(Lock<Option<TokenStream>>);\n-\n-impl cmp::Eq for LazyTokenStream {}\n-impl PartialEq for LazyTokenStream {\n-    fn eq(&self, _other: &LazyTokenStream) -> bool {\n-        true\n-    }\n-}\n-\n-impl fmt::Debug for LazyTokenStream {\n-    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {\n-        fmt::Debug::fmt(&self.clone().0.into_inner(), f)\n-    }\n-}\n+impl Nonterminal {\n+    pub fn to_tokenstream(&self, sess: &ParseSess, span: Span) -> TokenStream {\n+        // A `Nonterminal` is often a parsed AST item. At this point we now\n+        // need to convert the parsed AST to an actual token stream, e.g.\n+        // un-parse it basically.\n+        //\n+        // Unfortunately there's not really a great way to do that in a\n+        // guaranteed lossless fashion right now. The fallback here is to just\n+        // stringify the AST node and reparse it, but this loses all span\n+        // information.\n+        //\n+        // As a result, some AST nodes are annotated with the token stream they\n+        // came from. Here we attempt to extract these lossless token streams\n+        // before we fall back to the stringification.\n+        let tokens = match *self {\n+            Nonterminal::NtItem(ref item) => {\n+                prepend_attrs(sess, &item.attrs, item.tokens.as_ref(), span)\n+            }\n+            Nonterminal::NtTraitItem(ref item) => {\n+                prepend_attrs(sess, &item.attrs, item.tokens.as_ref(), span)\n+            }\n+            Nonterminal::NtImplItem(ref item) => {\n+                prepend_attrs(sess, &item.attrs, item.tokens.as_ref(), span)\n+            }\n+            Nonterminal::NtIdent(ident, is_raw) => {\n+                let token = Token::Ident(ident, is_raw);\n+                Some(TokenTree::Token(ident.span, token).into())\n+            }\n+            Nonterminal::NtLifetime(ident) => {\n+                let token = Token::Lifetime(ident);\n+                Some(TokenTree::Token(ident.span, token).into())\n+            }\n+            Nonterminal::NtTT(ref tt) => {\n+                Some(tt.clone().into())\n+            }\n+            _ => None,\n+        };\n \n-impl LazyTokenStream {\n-    pub fn new() -> Self {\n-        LazyTokenStream(Lock::new(None))\n-    }\n+        // FIXME(#43081): Avoid this pretty-print + reparse hack\n+        let source = pprust::nonterminal_to_string(self);\n+        let filename = FileName::macro_expansion_source_code(&source);\n+        let (tokens_for_real, errors) =\n+            parse_stream_from_source_str(filename, source, sess, Some(span));\n+        emit_unclosed_delims(&errors, &sess.span_diagnostic);\n \n-    fn force<F: FnOnce() -> TokenStream>(&self, f: F) -> TokenStream {\n-        let mut opt_stream = self.0.lock();\n-        if opt_stream.is_none() {\n-            *opt_stream = Some(f());\n+        // During early phases of the compiler the AST could get modified\n+        // directly (e.g., attributes added or removed) and the internal cache\n+        // of tokens my not be invalidated or updated. Consequently if the\n+        // \"lossless\" token stream disagrees with our actual stringification\n+        // (which has historically been much more battle-tested) then we go\n+        // with the lossy stream anyway (losing span information).\n+        //\n+        // Note that the comparison isn't `==` here to avoid comparing spans,\n+        // but it *also* is a \"probable\" equality which is a pretty weird\n+        // definition. We mostly want to catch actual changes to the AST\n+        // like a `#[cfg]` being processed or some weird `macro_rules!`\n+        // expansion.\n+        //\n+        // What we *don't* want to catch is the fact that a user-defined\n+        // literal like `0xf` is stringified as `15`, causing the cached token\n+        // stream to not be literal `==` token-wise (ignoring spans) to the\n+        // token stream we got from stringification.\n+        //\n+        // Instead the \"probably equal\" check here is \"does each token\n+        // recursively have the same discriminant?\" We basically don't look at\n+        // the token values here and assume that such fine grained token stream\n+        // modifications, including adding/removing typically non-semantic\n+        // tokens such as extra braces and commas, don't happen.\n+        if let Some(tokens) = tokens {\n+            if tokens.probably_equal_for_proc_macro(&tokens_for_real) {\n+                return tokens\n+            }\n+            info!(\"cached tokens found, but they're not \\\"probably equal\\\", \\\n+                   going with stringified version\");\n         }\n-        opt_stream.clone().unwrap()\n-    }\n-}\n-\n-impl Encodable for LazyTokenStream {\n-    fn encode<S: Encoder>(&self, _: &mut S) -> Result<(), S::Error> {\n-        Ok(())\n+        return tokens_for_real\n     }\n }\n \n-impl Decodable for LazyTokenStream {\n-    fn decode<D: Decoder>(_: &mut D) -> Result<LazyTokenStream, D::Error> {\n-        Ok(LazyTokenStream::new())\n+crate fn is_op(tok: &Token) -> bool {\n+    match *tok {\n+        OpenDelim(..) | CloseDelim(..) | Literal(..) | DocComment(..) |\n+        Ident(..) | Lifetime(..) | Interpolated(..) |\n+        Whitespace | Comment | Shebang(..) | Eof => false,\n+        _ => true,\n     }\n }\n \n-impl ::std::hash::Hash for LazyTokenStream {\n-    fn hash<H: ::std::hash::Hasher>(&self, _hasher: &mut H) {}\n-}\n-\n fn prepend_attrs(sess: &ParseSess,\n                  attrs: &[ast::Attribute],\n                  tokens: Option<&tokenstream::TokenStream>,"}, {"sha": "dcf9815f6d1ba47a4df2facd444514455fb87293", "filename": "src/libsyntax/print/pprust.rs", "status": "modified", "additions": 28, "deletions": 24, "changes": 52, "blob_url": "https://github.com/rust-lang/rust/blob/585d4d29d91065c14fb823b8044495a6e5e857c1/src%2Flibsyntax%2Fprint%2Fpprust.rs", "raw_url": "https://github.com/rust-lang/rust/raw/585d4d29d91065c14fb823b8044495a6e5e857c1/src%2Flibsyntax%2Fprint%2Fpprust.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fprint%2Fpprust.rs?ref=585d4d29d91065c14fb823b8044495a6e5e857c1", "patch": "@@ -4,7 +4,7 @@ use crate::ast::{Attribute, MacDelimiter, GenericArg};\n use crate::util::parser::{self, AssocOp, Fixity};\n use crate::attr;\n use crate::source_map::{self, SourceMap, Spanned};\n-use crate::parse::token::{self, BinOpToken, Token};\n+use crate::parse::token::{self, BinOpToken, Nonterminal, Token};\n use crate::parse::lexer::comments;\n use crate::parse::{self, ParseSess};\n use crate::print::pp::{self, Breaks};\n@@ -257,29 +257,33 @@ pub fn token_to_string(tok: &Token) -> String {\n         token::Comment              => \"/* */\".to_string(),\n         token::Shebang(s)           => format!(\"/* shebang: {}*/\", s),\n \n-        token::Interpolated(ref nt) => match nt.0 {\n-            token::NtExpr(ref e)        => expr_to_string(e),\n-            token::NtMeta(ref e)        => meta_item_to_string(e),\n-            token::NtTy(ref e)          => ty_to_string(e),\n-            token::NtPath(ref e)        => path_to_string(e),\n-            token::NtItem(ref e)        => item_to_string(e),\n-            token::NtBlock(ref e)       => block_to_string(e),\n-            token::NtStmt(ref e)        => stmt_to_string(e),\n-            token::NtPat(ref e)         => pat_to_string(e),\n-            token::NtIdent(e, false)    => ident_to_string(e),\n-            token::NtIdent(e, true)     => format!(\"r#{}\", ident_to_string(e)),\n-            token::NtLifetime(e)        => ident_to_string(e),\n-            token::NtLiteral(ref e)     => expr_to_string(e),\n-            token::NtTT(ref tree)       => tt_to_string(tree.clone()),\n-            token::NtArm(ref e)         => arm_to_string(e),\n-            token::NtImplItem(ref e)    => impl_item_to_string(e),\n-            token::NtTraitItem(ref e)   => trait_item_to_string(e),\n-            token::NtGenerics(ref e)    => generic_params_to_string(&e.params),\n-            token::NtWhereClause(ref e) => where_clause_to_string(e),\n-            token::NtArg(ref e)         => arg_to_string(e),\n-            token::NtVis(ref e)         => vis_to_string(e),\n-            token::NtForeignItem(ref e) => foreign_item_to_string(e),\n-        }\n+        token::Interpolated(ref nt) => nonterminal_to_string(nt),\n+    }\n+}\n+\n+pub fn nonterminal_to_string(nt: &Nonterminal) -> String {\n+    match *nt {\n+        token::NtExpr(ref e)        => expr_to_string(e),\n+        token::NtMeta(ref e)        => meta_item_to_string(e),\n+        token::NtTy(ref e)          => ty_to_string(e),\n+        token::NtPath(ref e)        => path_to_string(e),\n+        token::NtItem(ref e)        => item_to_string(e),\n+        token::NtBlock(ref e)       => block_to_string(e),\n+        token::NtStmt(ref e)        => stmt_to_string(e),\n+        token::NtPat(ref e)         => pat_to_string(e),\n+        token::NtIdent(e, false)    => ident_to_string(e),\n+        token::NtIdent(e, true)     => format!(\"r#{}\", ident_to_string(e)),\n+        token::NtLifetime(e)        => ident_to_string(e),\n+        token::NtLiteral(ref e)     => expr_to_string(e),\n+        token::NtTT(ref tree)       => tt_to_string(tree.clone()),\n+        token::NtArm(ref e)         => arm_to_string(e),\n+        token::NtImplItem(ref e)    => impl_item_to_string(e),\n+        token::NtTraitItem(ref e)   => trait_item_to_string(e),\n+        token::NtGenerics(ref e)    => generic_params_to_string(&e.params),\n+        token::NtWhereClause(ref e) => where_clause_to_string(e),\n+        token::NtArg(ref e)         => arg_to_string(e),\n+        token::NtVis(ref e)         => vis_to_string(e),\n+        token::NtForeignItem(ref e) => foreign_item_to_string(e),\n     }\n }\n "}, {"sha": "283679e758b54e5f6c45268b1ac4a9998a1b6b18", "filename": "src/libsyntax/tokenstream.rs", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/rust-lang/rust/blob/585d4d29d91065c14fb823b8044495a6e5e857c1/src%2Flibsyntax%2Ftokenstream.rs", "raw_url": "https://github.com/rust-lang/rust/raw/585d4d29d91065c14fb823b8044495a6e5e857c1/src%2Flibsyntax%2Ftokenstream.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Ftokenstream.rs?ref=585d4d29d91065c14fb823b8044495a6e5e857c1", "patch": "@@ -72,7 +72,7 @@ impl TokenTree {\n         }\n     }\n \n-    // See comments in `interpolated_to_tokenstream` for why we care about\n+    // See comments in `Nonterminal::to_tokenstream` for why we care about\n     // *probably* equal here rather than actual equality\n     //\n     // This is otherwise the same as `eq_unspanned`, only recursing with a\n@@ -310,7 +310,7 @@ impl TokenStream {\n         t1.next().is_none() && t2.next().is_none()\n     }\n \n-    // See comments in `interpolated_to_tokenstream` for why we care about\n+    // See comments in `Nonterminal::to_tokenstream` for why we care about\n     // *probably* equal here rather than actual equality\n     //\n     // This is otherwise the same as `eq_unspanned`, only recursing with a"}, {"sha": "cfc3c931598a1df73a4372d3689a49813d0a3277", "filename": "src/libsyntax_ext/deriving/custom.rs", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "blob_url": "https://github.com/rust-lang/rust/blob/585d4d29d91065c14fb823b8044495a6e5e857c1/src%2Flibsyntax_ext%2Fderiving%2Fcustom.rs", "raw_url": "https://github.com/rust-lang/rust/raw/585d4d29d91065c14fb823b8044495a6e5e857c1/src%2Flibsyntax_ext%2Fderiving%2Fcustom.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax_ext%2Fderiving%2Fcustom.rs?ref=585d4d29d91065c14fb823b8044495a6e5e857c1", "patch": "@@ -2,6 +2,7 @@ use crate::proc_macro_impl::EXEC_STRATEGY;\n use crate::proc_macro_server;\n \n use errors::FatalError;\n+use rustc_data_structures::sync::Lrc;\n use syntax::ast::{self, ItemKind, Attribute, Mac};\n use syntax::attr::{mark_used, mark_known};\n use syntax::source_map::Span;\n@@ -65,7 +66,7 @@ impl MultiItemModifier for ProcMacroDerive {\n         // Mark attributes as known, and used.\n         MarkAttrs(&self.attrs).visit_item(&item);\n \n-        let token = Token::interpolated(token::NtItem(item));\n+        let token = Token::Interpolated(Lrc::new(token::NtItem(item)));\n         let input = tokenstream::TokenTree::Token(DUMMY_SP, token).into();\n \n         let server = proc_macro_server::Rustc::new(ecx);"}, {"sha": "699539b62f515b98238bd718e6523ff5aa2864e0", "filename": "src/libsyntax_ext/proc_macro_server.rs", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/rust-lang/rust/blob/585d4d29d91065c14fb823b8044495a6e5e857c1/src%2Flibsyntax_ext%2Fproc_macro_server.rs", "raw_url": "https://github.com/rust-lang/rust/raw/585d4d29d91065c14fb823b8044495a6e5e857c1/src%2Flibsyntax_ext%2Fproc_macro_server.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax_ext%2Fproc_macro_server.rs?ref=585d4d29d91065c14fb823b8044495a6e5e857c1", "patch": "@@ -178,8 +178,8 @@ impl FromInternal<(TreeAndJoint, &'_ ParseSess, &'_ mut Vec<Self>)>\n                 tt!(Punct::new('#', false))\n             }\n \n-            Interpolated(_) => {\n-                let stream = token.interpolated_to_tokenstream(sess, span);\n+            Interpolated(nt) => {\n+                let stream = nt.to_tokenstream(sess, span);\n                 TokenTree::Group(Group {\n                     delimiter: Delimiter::None,\n                     stream,"}]}