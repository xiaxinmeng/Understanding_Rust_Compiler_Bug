{"sha": "4a07f78dadd3e5608157486b204fd4be2cde15a7", "node_id": "C_kwDOAAsO6NoAKDRhMDdmNzhkYWRkM2U1NjA4MTU3NDg2YjIwNGZkNGJlMmNkZTE1YTc", "commit": {"author": {"name": "Andy Wang", "email": "cbeuw.andy@gmail.com", "date": "2022-05-29T14:05:07Z"}, "committer": {"name": "Andy Wang", "email": "cbeuw.andy@gmail.com", "date": "2022-06-06T18:16:00Z"}, "message": "Forbade all racing mixed size atomic accesses", "tree": {"sha": "ab0ccf5aceb96ce23bae6ac8d62a0dffc1429f0a", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/ab0ccf5aceb96ce23bae6ac8d62a0dffc1429f0a"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/4a07f78dadd3e5608157486b204fd4be2cde15a7", "comment_count": 0, "verification": {"verified": true, "reason": "valid", "signature": "-----BEGIN PGP SIGNATURE-----\n\niQGzBAABCgAdFiEE7dcbcBMl24/h63ldGBtJ+fOPM3QFAmKeRGAACgkQGBtJ+fOP\nM3QIQgwAmq42qhoEYhNbNw1yNdd7nHjVnv6pFhuL19Twr5E9/9jLwZJqFB/G3NUS\nmzxdEeCy+MULPhbXuvEIMkqhbV8JgESstcGFe3qzjtxdDdRTUDJmV9u88ZncuWDv\nghiqwwXcnPwAR2noF2WP2trWM8l2mcfNQS838MHsBbJZuY8dpYxTMHMFWXh2do7V\n0+Cn9lo3y5nFrt3XOea+ABK7T4zFQGmEo46I9dimYI3YMohcl7Uc2RauUp+AJGPP\n8zJnnznUbi7atREUqC98xp8WAXPljj9oRh69eQ4zR9b+jYeQ8o4s2T6by4KHep4I\nFktCeW33zMd9GF/3olc1Hm2gL2Uu2fvREOSc0Vy0znY8AsErdLKW9PL/TLWi26PE\ntL09bcCaI9tZwNuiT72fOwslxKKpHBuLRe1yLpPmW3gdNUt+2gdOHa63wKPrKtHm\n2fp6m8b4xF7mDcooo0IEUMT0l9yQ01kalvBS0eAPf1+dYN1Iyg+UBMz+pCC292m2\nnVRgZ9HE\n=Psju\n-----END PGP SIGNATURE-----", "payload": "tree ab0ccf5aceb96ce23bae6ac8d62a0dffc1429f0a\nparent ceb173d64773736e0c60ba6104912c725f07c2c9\nauthor Andy Wang <cbeuw.andy@gmail.com> 1653833107 +0100\ncommitter Andy Wang <cbeuw.andy@gmail.com> 1654539360 +0100\n\nForbade all racing mixed size atomic accesses\n"}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/4a07f78dadd3e5608157486b204fd4be2cde15a7", "html_url": "https://github.com/rust-lang/rust/commit/4a07f78dadd3e5608157486b204fd4be2cde15a7", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/4a07f78dadd3e5608157486b204fd4be2cde15a7/comments", "author": {"login": "cbeuw", "id": 7034308, "node_id": "MDQ6VXNlcjcwMzQzMDg=", "avatar_url": "https://avatars.githubusercontent.com/u/7034308?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cbeuw", "html_url": "https://github.com/cbeuw", "followers_url": "https://api.github.com/users/cbeuw/followers", "following_url": "https://api.github.com/users/cbeuw/following{/other_user}", "gists_url": "https://api.github.com/users/cbeuw/gists{/gist_id}", "starred_url": "https://api.github.com/users/cbeuw/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cbeuw/subscriptions", "organizations_url": "https://api.github.com/users/cbeuw/orgs", "repos_url": "https://api.github.com/users/cbeuw/repos", "events_url": "https://api.github.com/users/cbeuw/events{/privacy}", "received_events_url": "https://api.github.com/users/cbeuw/received_events", "type": "User", "site_admin": false}, "committer": {"login": "cbeuw", "id": 7034308, "node_id": "MDQ6VXNlcjcwMzQzMDg=", "avatar_url": "https://avatars.githubusercontent.com/u/7034308?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cbeuw", "html_url": "https://github.com/cbeuw", "followers_url": "https://api.github.com/users/cbeuw/followers", "following_url": "https://api.github.com/users/cbeuw/following{/other_user}", "gists_url": "https://api.github.com/users/cbeuw/gists{/gist_id}", "starred_url": "https://api.github.com/users/cbeuw/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cbeuw/subscriptions", "organizations_url": "https://api.github.com/users/cbeuw/orgs", "repos_url": "https://api.github.com/users/cbeuw/repos", "events_url": "https://api.github.com/users/cbeuw/events{/privacy}", "received_events_url": "https://api.github.com/users/cbeuw/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "ceb173d64773736e0c60ba6104912c725f07c2c9", "url": "https://api.github.com/repos/rust-lang/rust/commits/ceb173d64773736e0c60ba6104912c725f07c2c9", "html_url": "https://github.com/rust-lang/rust/commit/ceb173d64773736e0c60ba6104912c725f07c2c9"}], "stats": {"total": 207, "additions": 78, "deletions": 129}, "files": [{"sha": "61cd6a3c0c0d2d004dd409573af42df688a670e0", "filename": "src/concurrency/data_race.rs", "status": "modified", "additions": 14, "deletions": 38, "changes": 52, "blob_url": "https://github.com/rust-lang/rust/blob/4a07f78dadd3e5608157486b204fd4be2cde15a7/src%2Fconcurrency%2Fdata_race.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4a07f78dadd3e5608157486b204fd4be2cde15a7/src%2Fconcurrency%2Fdata_race.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Fconcurrency%2Fdata_race.rs?ref=4a07f78dadd3e5608157486b204fd4be2cde15a7", "patch": "@@ -287,20 +287,15 @@ impl MemoryCellClocks {\n         Ok(())\n     }\n \n-    /// Checks if the memory cell write races with any prior atomic read or write\n-    fn write_race_free_with_atomic(&mut self, clocks: &ThreadClockSet) -> bool {\n+    /// Checks if the memory cell access is ordered with all prior atomic reads and writes\n+    fn race_free_with_atomic(&self, clocks: &ThreadClockSet) -> bool {\n         if let Some(atomic) = self.atomic() {\n             atomic.read_vector <= clocks.clock && atomic.write_vector <= clocks.clock\n         } else {\n             true\n         }\n     }\n \n-    /// Checks if the memory cell read races with any prior atomic write\n-    fn read_race_free_with_atomic(&self, clocks: &ThreadClockSet) -> bool {\n-        if let Some(atomic) = self.atomic() { atomic.write_vector <= clocks.clock } else { true }\n-    }\n-\n     /// Update memory cell data-race tracking for atomic\n     /// load relaxed semantics, is a no-op if this memory was\n     /// not used previously as atomic memory.\n@@ -528,7 +523,7 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: MiriEvalContextExt<'mir, 'tcx> {\n         // the *value* (including the associated provenance if this is an AtomicPtr) at this location.\n         // Only metadata on the location itself is used.\n         let scalar = this.allow_data_races_ref(move |this| this.read_scalar(&place.into()))?;\n-        this.validate_overlapping_atomic_read(place)?;\n+        this.validate_overlapping_atomic(place)?;\n         this.buffered_atomic_read(place, atomic, scalar, || {\n             this.validate_atomic_load(place, atomic)\n         })\n@@ -542,7 +537,7 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: MiriEvalContextExt<'mir, 'tcx> {\n         atomic: AtomicWriteOp,\n     ) -> InterpResult<'tcx> {\n         let this = self.eval_context_mut();\n-        this.validate_overlapping_atomic_write(dest)?;\n+        this.validate_overlapping_atomic(dest)?;\n         this.allow_data_races_mut(move |this| this.write_scalar(val, &(*dest).into()))?;\n         this.validate_atomic_store(dest, atomic)?;\n         // FIXME: it's not possible to get the value before write_scalar. A read_scalar will cause\n@@ -563,7 +558,7 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: MiriEvalContextExt<'mir, 'tcx> {\n     ) -> InterpResult<'tcx, ImmTy<'tcx, Tag>> {\n         let this = self.eval_context_mut();\n \n-        this.validate_overlapping_atomic_write(place)?;\n+        this.validate_overlapping_atomic(place)?;\n         let old = this.allow_data_races_mut(|this| this.read_immediate(&place.into()))?;\n \n         // Atomics wrap around on overflow.\n@@ -592,7 +587,7 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: MiriEvalContextExt<'mir, 'tcx> {\n     ) -> InterpResult<'tcx, ScalarMaybeUninit<Tag>> {\n         let this = self.eval_context_mut();\n \n-        this.validate_overlapping_atomic_write(place)?;\n+        this.validate_overlapping_atomic(place)?;\n         let old = this.allow_data_races_mut(|this| this.read_scalar(&place.into()))?;\n         this.allow_data_races_mut(|this| this.write_scalar(new, &(*place).into()))?;\n \n@@ -613,7 +608,7 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: MiriEvalContextExt<'mir, 'tcx> {\n     ) -> InterpResult<'tcx, ImmTy<'tcx, Tag>> {\n         let this = self.eval_context_mut();\n \n-        this.validate_overlapping_atomic_write(place)?;\n+        this.validate_overlapping_atomic(place)?;\n         let old = this.allow_data_races_mut(|this| this.read_immediate(&place.into()))?;\n         let lt = this.binary_op(mir::BinOp::Lt, &old, &rhs)?.to_scalar()?.to_bool()?;\n \n@@ -656,7 +651,7 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: MiriEvalContextExt<'mir, 'tcx> {\n         use rand::Rng as _;\n         let this = self.eval_context_mut();\n \n-        this.validate_overlapping_atomic_write(place)?;\n+        this.validate_overlapping_atomic(place)?;\n         // Failure ordering cannot be stronger than success ordering, therefore first attempt\n         // to read with the failure ordering and if successful then try again with the success\n         // read ordering and write in the success case.\n@@ -706,7 +701,7 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: MiriEvalContextExt<'mir, 'tcx> {\n         atomic: AtomicReadOp,\n     ) -> InterpResult<'tcx> {\n         let this = self.eval_context_ref();\n-        this.validate_overlapping_atomic_read(place)?;\n+        this.validate_overlapping_atomic(place)?;\n         this.validate_atomic_op(\n             place,\n             atomic,\n@@ -729,7 +724,7 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: MiriEvalContextExt<'mir, 'tcx> {\n         atomic: AtomicWriteOp,\n     ) -> InterpResult<'tcx> {\n         let this = self.eval_context_mut();\n-        this.validate_overlapping_atomic_write(place)?;\n+        this.validate_overlapping_atomic(place)?;\n         this.validate_atomic_op(\n             place,\n             atomic,\n@@ -755,7 +750,7 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: MiriEvalContextExt<'mir, 'tcx> {\n         let acquire = matches!(atomic, Acquire | AcqRel | SeqCst);\n         let release = matches!(atomic, Release | AcqRel | SeqCst);\n         let this = self.eval_context_mut();\n-        this.validate_overlapping_atomic_write(place)?;\n+        this.validate_overlapping_atomic(place)?;\n         this.validate_atomic_op(place, atomic, \"Atomic RMW\", move |memory, clocks, index, _| {\n             if acquire {\n                 memory.load_acquire(clocks, index)?;\n@@ -941,9 +936,9 @@ impl VClockAlloc {\n         )\n     }\n \n-    /// Detect racing atomic writes (not data races)\n+    /// Detect racing atomic read and writes (not data races)\n     /// on every byte of the current access range\n-    pub(super) fn read_race_free_with_atomic<'tcx>(\n+    pub(super) fn race_free_with_atomic<'tcx>(\n         &self,\n         range: AllocRange,\n         global: &GlobalState,\n@@ -952,26 +947,7 @@ impl VClockAlloc {\n             let (_, clocks) = global.current_thread_state();\n             let alloc_ranges = self.alloc_ranges.borrow();\n             for (_, range) in alloc_ranges.iter(range.start, range.size) {\n-                if !range.read_race_free_with_atomic(&clocks) {\n-                    return false;\n-                }\n-            }\n-        }\n-        true\n-    }\n-\n-    /// Detect racing atomic read and writes (not data races)\n-    /// on every byte of the current access range\n-    pub(super) fn write_race_free_with_atomic<'tcx>(\n-        &mut self,\n-        range: AllocRange,\n-        global: &GlobalState,\n-    ) -> bool {\n-        if global.race_detecting() {\n-            let (_, clocks) = global.current_thread_state();\n-            let alloc_ranges = self.alloc_ranges.get_mut();\n-            for (_, range) in alloc_ranges.iter_mut(range.start, range.size) {\n-                if !range.write_race_free_with_atomic(&clocks) {\n+                if !range.race_free_with_atomic(&clocks) {\n                     return false;\n                 }\n             }"}, {"sha": "9bf46bb23b0bd9710681058fd8da6f550c535312", "filename": "src/concurrency/weak_memory.rs", "status": "modified", "additions": 7, "deletions": 34, "changes": 41, "blob_url": "https://github.com/rust-lang/rust/blob/4a07f78dadd3e5608157486b204fd4be2cde15a7/src%2Fconcurrency%2Fweak_memory.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4a07f78dadd3e5608157486b204fd4be2cde15a7/src%2Fconcurrency%2Fweak_memory.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Fconcurrency%2Fweak_memory.rs?ref=4a07f78dadd3e5608157486b204fd4be2cde15a7", "patch": "@@ -35,7 +35,8 @@\n //! (such as accessing the top 16 bits of an AtomicU32). These senarios are generally undiscussed in formalisations of C++ memory model.\n //! In Rust, these operations can only be done through a `&mut AtomicFoo` reference or one derived from it, therefore these operations\n //! can only happen after all previous accesses on the same locations. This implementation is adapted to allow these operations.\n-//! A mixed size/atomicity read that races with writes, or a write that races with reads or writes will still cause UBs to be thrown.\n+//! A mixed atomicity read that races with writes, or a write that races with reads or writes will still cause UBs to be thrown.\n+//! Mixed size atomic accesses must not race with any other atomic access, whether read or write, or a UB will be thrown.\n //! You can refer to test cases in weak_memory/extra_cpp.rs and weak_memory/extra_cpp_unsafe.rs for examples of these operations.\n \n // Our and the author's own implementation (tsan11) of the paper have some deviations from the provided operational semantics in \u00a75.3:\n@@ -403,9 +404,9 @@ pub(super) trait EvalContextExt<'mir, 'tcx: 'mir>:\n     crate::MiriEvalContextExt<'mir, 'tcx>\n {\n     // If weak memory emulation is enabled, check if this atomic op imperfectly overlaps with a previous\n-    // atomic write. If it does, then we require it to be ordered (non-racy) with all previous atomic\n-    // writes on all the bytes in range\n-    fn validate_overlapping_atomic_read(&self, place: &MPlaceTy<'tcx, Tag>) -> InterpResult<'tcx> {\n+    // atomic read or write. If it does, then we require it to be ordered (non-racy) with all previous atomic\n+    // accesses on all the bytes in range\n+    fn validate_overlapping_atomic(&self, place: &MPlaceTy<'tcx, Tag>) -> InterpResult<'tcx> {\n         let this = self.eval_context_ref();\n         let (alloc_id, base_offset, ..) = this.ptr_get_alloc_id(place.ptr)?;\n         if let crate::AllocExtra {\n@@ -417,37 +418,9 @@ pub(super) trait EvalContextExt<'mir, 'tcx: 'mir>:\n             let range = alloc_range(base_offset, place.layout.size);\n             if alloc_buffers.is_overlapping(range)\n                 && !alloc_clocks\n-                    .read_race_free_with_atomic(range, this.machine.data_race.as_ref().unwrap())\n+                    .race_free_with_atomic(range, this.machine.data_race.as_ref().unwrap())\n             {\n-                throw_ub_format!(\n-                    \"racy imperfectly overlapping atomic access is not possible in the C++20 memory model\"\n-                );\n-            }\n-        }\n-        Ok(())\n-    }\n-\n-    // Same as above but needs to be ordered with all previous atomic read or writes\n-    fn validate_overlapping_atomic_write(\n-        &mut self,\n-        place: &MPlaceTy<'tcx, Tag>,\n-    ) -> InterpResult<'tcx> {\n-        let this = self.eval_context_mut();\n-        let (alloc_id, base_offset, ..) = this.ptr_get_alloc_id(place.ptr)?;\n-        if let (\n-            crate::AllocExtra {\n-                weak_memory: Some(alloc_buffers),\n-                data_race: Some(alloc_clocks),\n-                ..\n-            },\n-            crate::Evaluator { data_race: Some(global), .. },\n-        ) = this.get_alloc_extra_mut(alloc_id)?\n-        {\n-            let range = alloc_range(base_offset, place.layout.size);\n-            if alloc_buffers.is_overlapping(range)\n-                && !alloc_clocks.write_race_free_with_atomic(range, global)\n-            {\n-                throw_ub_format!(\"racy imperfectly overlapping atomic access\");\n+                throw_ub_format!(\"racy imperfectly overlapping atomic access is not possible in the C++20 memory model\");\n             }\n         }\n         Ok(())"}, {"sha": "6d53670a4e92edb5c2593155cc4d42fd7cad1177", "filename": "tests/compile-fail/weak_memory/racing_mixed_size.rs", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "blob_url": "https://github.com/rust-lang/rust/blob/4a07f78dadd3e5608157486b204fd4be2cde15a7/tests%2Fcompile-fail%2Fweak_memory%2Fracing_mixed_size.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4a07f78dadd3e5608157486b204fd4be2cde15a7/tests%2Fcompile-fail%2Fweak_memory%2Fracing_mixed_size.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/tests%2Fcompile-fail%2Fweak_memory%2Fracing_mixed_size.rs?ref=4a07f78dadd3e5608157486b204fd4be2cde15a7", "patch": "@@ -1,5 +1,4 @@\n // ignore-windows: Concurrency on Windows is not supported yet.\n-// compile-flags: -Zmiri-ignore-leaks\n \n #![feature(core_intrinsics)]\n "}, {"sha": "0129b55aff618f74e2569c2650459e4106020f1b", "filename": "tests/compile-fail/weak_memory/racing_mixed_size_read.rs", "status": "added", "additions": 39, "deletions": 0, "changes": 39, "blob_url": "https://github.com/rust-lang/rust/blob/4a07f78dadd3e5608157486b204fd4be2cde15a7/tests%2Fcompile-fail%2Fweak_memory%2Fracing_mixed_size_read.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4a07f78dadd3e5608157486b204fd4be2cde15a7/tests%2Fcompile-fail%2Fweak_memory%2Fracing_mixed_size_read.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/tests%2Fcompile-fail%2Fweak_memory%2Fracing_mixed_size_read.rs?ref=4a07f78dadd3e5608157486b204fd4be2cde15a7", "patch": "@@ -0,0 +1,39 @@\n+// ignore-windows: Concurrency on Windows is not supported yet.\n+\n+#![feature(core_intrinsics)]\n+\n+use std::sync::atomic::AtomicU32;\n+use std::sync::atomic::Ordering::*;\n+use std::thread::spawn;\n+\n+fn static_atomic(val: u32) -> &'static AtomicU32 {\n+    let ret = Box::leak(Box::new(AtomicU32::new(val)));\n+    ret\n+}\n+\n+fn split_u32_ptr(dword: *const u32) -> *const [u16; 2] {\n+    unsafe { std::mem::transmute::<*const u32, *const [u16; 2]>(dword) }\n+}\n+\n+// Racing mixed size reads may cause two loads to read-from\n+// the same store but observe different values, which doesn't make\n+// sense under the formal model so we forbade this.\n+pub fn main() {\n+    let x = static_atomic(0);\n+\n+    let j1 = spawn(move || {\n+        x.load(Relaxed);\n+    });\n+\n+    let j2 = spawn(move || {\n+        let x_ptr = x as *const AtomicU32 as *const u32;\n+        let x_split = split_u32_ptr(x_ptr);\n+        unsafe {\n+            let hi = &(*x_split)[0] as *const u16;\n+            std::intrinsics::atomic_load_relaxed(hi); //~ ERROR: imperfectly overlapping\n+        }\n+    });\n+\n+    j1.join().unwrap();\n+    j2.join().unwrap();\n+}"}, {"sha": "80cc2fe756bf2f1207d6bcfd7f66f160f6f8c26f", "filename": "tests/compile-fail/weak_memory/racing_mixed_size_read.stderr", "status": "added", "additions": 18, "deletions": 0, "changes": 18, "blob_url": "https://github.com/rust-lang/rust/blob/4a07f78dadd3e5608157486b204fd4be2cde15a7/tests%2Fcompile-fail%2Fweak_memory%2Fracing_mixed_size_read.stderr", "raw_url": "https://github.com/rust-lang/rust/raw/4a07f78dadd3e5608157486b204fd4be2cde15a7/tests%2Fcompile-fail%2Fweak_memory%2Fracing_mixed_size_read.stderr", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/tests%2Fcompile-fail%2Fweak_memory%2Fracing_mixed_size_read.stderr?ref=4a07f78dadd3e5608157486b204fd4be2cde15a7", "patch": "@@ -0,0 +1,18 @@\n+warning: thread support is experimental: the scheduler is not preemptive, and can get stuck in spin loops.\n+         (see https://github.com/rust-lang/miri/issues/1388)\n+\n+error: Undefined Behavior: racy imperfectly overlapping atomic access is not possible in the C++20 memory model\n+  --> $DIR/racing_mixed_size_read.rs:LL:CC\n+   |\n+LL |             std::intrinsics::atomic_load_relaxed(hi);\n+   |             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ racy imperfectly overlapping atomic access is not possible in the C++20 memory model\n+   |\n+   = help: this indicates a bug in the program: it performed an invalid operation, and caused Undefined Behavior\n+   = help: see https://doc.rust-lang.org/nightly/reference/behavior-considered-undefined.html for further information\n+           \n+   = note: inside closure at $DIR/racing_mixed_size_read.rs:LL:CC\n+\n+note: some details are omitted, run with `MIRIFLAGS=-Zmiri-backtrace=full` for a verbose backtrace\n+\n+error: aborting due to previous error; 1 warning emitted\n+"}, {"sha": "478e436e59d1d69cd5896d722efdd77268233a5e", "filename": "tests/run-pass/weak_memory/extra_cpp_unsafe.rs", "status": "modified", "additions": 0, "deletions": 56, "changes": 56, "blob_url": "https://github.com/rust-lang/rust/blob/4a07f78dadd3e5608157486b204fd4be2cde15a7/tests%2Frun-pass%2Fweak_memory%2Fextra_cpp_unsafe.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4a07f78dadd3e5608157486b204fd4be2cde15a7/tests%2Frun-pass%2Fweak_memory%2Fextra_cpp_unsafe.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/tests%2Frun-pass%2Fweak_memory%2Fextra_cpp_unsafe.rs?ref=4a07f78dadd3e5608157486b204fd4be2cde15a7", "patch": "@@ -18,10 +18,6 @@ fn static_atomic(val: u32) -> &'static AtomicU32 {\n     ret\n }\n \n-fn split_u32_ptr(dword: *const u32) -> *const [u16; 2] {\n-    unsafe { std::mem::transmute::<*const u32, *const [u16; 2]>(dword) }\n-}\n-\n // We allow non-atomic and atomic reads to race\n fn racing_mixed_atomicity_read() {\n     let x = static_atomic(0);\n@@ -41,58 +37,6 @@ fn racing_mixed_atomicity_read() {\n     assert_eq!(r2, 42);\n }\n \n-// We allow mixed-size atomic reads to race\n-fn racing_mixed_size_read() {\n-    let x = static_atomic(0);\n-\n-    let j1 = spawn(move || {\n-        x.load(Relaxed);\n-    });\n-\n-    let j2 = spawn(move || {\n-        let x_ptr = x as *const AtomicU32 as *const u32;\n-        let x_split = split_u32_ptr(x_ptr);\n-        unsafe {\n-            let hi = &(*x_split)[0] as *const u16;\n-            std::intrinsics::atomic_load_relaxed(hi);\n-        }\n-    });\n-\n-    j1.join().unwrap();\n-    j2.join().unwrap();\n-}\n-\n-// And we allow the combination of both of the above.\n-fn racing_mixed_atomicity_and_size_read() {\n-    let x = static_atomic(u32::from_be(0xabbafafa));\n-\n-    let j1 = spawn(move || {\n-        x.load(Relaxed);\n-    });\n-\n-    let j2 = spawn(move || {\n-        let x_ptr = x as *const AtomicU32 as *const u32;\n-        unsafe { *x_ptr };\n-    });\n-\n-    let j3 = spawn(move || {\n-        let x_ptr = x as *const AtomicU32 as *const u32;\n-        let x_split = split_u32_ptr(x_ptr);\n-        unsafe {\n-            let hi = &(*x_split)[0] as *const u16;\n-            std::intrinsics::atomic_load_relaxed(hi)\n-        }\n-    });\n-\n-    j1.join().unwrap();\n-    j2.join().unwrap();\n-    let r3 = j3.join().unwrap();\n-\n-    assert_eq!(r3, u16::from_be(0xabba));\n-}\n-\n pub fn main() {\n     racing_mixed_atomicity_read();\n-    racing_mixed_size_read();\n-    racing_mixed_atomicity_and_size_read();\n }"}]}