{"sha": "d42521aa92006a3378c535adec80ae2257bff083", "node_id": "MDY6Q29tbWl0NzI0NzEyOmQ0MjUyMWFhOTIwMDZhMzM3OGM1MzVhZGVjODBhZTIyNTdiZmYwODM=", "commit": {"author": {"name": "bors", "email": "bors@rust-lang.org", "date": "2014-02-03T21:11:35Z"}, "committer": {"name": "bors", "email": "bors@rust-lang.org", "date": "2014-02-03T21:11:35Z"}, "message": "auto merge of #11866 : alexcrichton/rust/atomic-u64, r=brson\n\nLet's try this again.\r\n\r\nThis is an implementation of mutexes which I believe is free from undefined behavior of OS mutexes (the pitfall of the previous implementation).\r\n\r\nThis implementation is not ideal. There's a yield-loop spot, and it's not particularly fair with respect to lockers who steal without going through the normal code paths. That being said, I believe that this is a correct implementation which is a stepping stone to move from.\r\n\r\nI haven't done rigorous benchmarking of this mutex, but preliminary results show that it's about 25% slower in the uncontended case on linux (same runtime on OSX), and it's actually faster than a pthreads mutex on high contention (again, not rigorous benchmarking, I just saw these numbers come up).", "tree": {"sha": "0790d82726f49abc6d37b8ac5c8ca0fbec7e0685", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/0790d82726f49abc6d37b8ac5c8ca0fbec7e0685"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/d42521aa92006a3378c535adec80ae2257bff083", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/d42521aa92006a3378c535adec80ae2257bff083", "html_url": "https://github.com/rust-lang/rust/commit/d42521aa92006a3378c535adec80ae2257bff083", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/d42521aa92006a3378c535adec80ae2257bff083/comments", "author": {"login": "bors", "id": 3372342, "node_id": "MDQ6VXNlcjMzNzIzNDI=", "avatar_url": "https://avatars.githubusercontent.com/u/3372342?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bors", "html_url": "https://github.com/bors", "followers_url": "https://api.github.com/users/bors/followers", "following_url": "https://api.github.com/users/bors/following{/other_user}", "gists_url": "https://api.github.com/users/bors/gists{/gist_id}", "starred_url": "https://api.github.com/users/bors/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bors/subscriptions", "organizations_url": "https://api.github.com/users/bors/orgs", "repos_url": "https://api.github.com/users/bors/repos", "events_url": "https://api.github.com/users/bors/events{/privacy}", "received_events_url": "https://api.github.com/users/bors/received_events", "type": "User", "site_admin": false}, "committer": {"login": "bors", "id": 3372342, "node_id": "MDQ6VXNlcjMzNzIzNDI=", "avatar_url": "https://avatars.githubusercontent.com/u/3372342?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bors", "html_url": "https://github.com/bors", "followers_url": "https://api.github.com/users/bors/followers", "following_url": "https://api.github.com/users/bors/following{/other_user}", "gists_url": "https://api.github.com/users/bors/gists{/gist_id}", "starred_url": "https://api.github.com/users/bors/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bors/subscriptions", "organizations_url": "https://api.github.com/users/bors/orgs", "repos_url": "https://api.github.com/users/bors/repos", "events_url": "https://api.github.com/users/bors/events{/privacy}", "received_events_url": "https://api.github.com/users/bors/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "cb40eba4b1ce12914612914b94bdccd251a9f554", "url": "https://api.github.com/repos/rust-lang/rust/commits/cb40eba4b1ce12914612914b94bdccd251a9f554", "html_url": "https://github.com/rust-lang/rust/commit/cb40eba4b1ce12914612914b94bdccd251a9f554"}, {"sha": "acacfb20fd34162cfba5a4e7b5f1447e0403fa50", "url": "https://api.github.com/repos/rust-lang/rust/commits/acacfb20fd34162cfba5a4e7b5f1447e0403fa50", "html_url": "https://github.com/rust-lang/rust/commit/acacfb20fd34162cfba5a4e7b5f1447e0403fa50"}], "stats": {"total": 2084, "additions": 1589, "deletions": 495}, "files": [{"sha": "afbf34d07535d06bafcd9d6f5f11c4ae2db7e05c", "filename": "src/etc/licenseck.py", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "blob_url": "https://github.com/rust-lang/rust/blob/d42521aa92006a3378c535adec80ae2257bff083/src%2Fetc%2Flicenseck.py", "raw_url": "https://github.com/rust-lang/rust/raw/d42521aa92006a3378c535adec80ae2257bff083/src%2Fetc%2Flicenseck.py", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Fetc%2Flicenseck.py?ref=d42521aa92006a3378c535adec80ae2257bff083", "patch": "@@ -41,6 +41,7 @@\n     \"libstd/sync/mpsc_queue.rs\", # BSD\n     \"libstd/sync/spsc_queue.rs\", # BSD\n     \"libstd/sync/mpmc_bounded_queue.rs\", # BSD\n+    \"libextra/sync/mpsc_intrusive.rs\", # BSD\n ]\n \n def check_license(name, contents):"}, {"sha": "21ebcf1272009435d66428a0f5472389911ff39f", "filename": "src/libextra/sync/mod.rs", "status": "renamed", "additions": 28, "deletions": 20, "changes": 48, "blob_url": "https://github.com/rust-lang/rust/blob/d42521aa92006a3378c535adec80ae2257bff083/src%2Flibextra%2Fsync%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/d42521aa92006a3378c535adec80ae2257bff083/src%2Flibextra%2Fsync%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibextra%2Fsync%2Fmod.rs?ref=d42521aa92006a3378c535adec80ae2257bff083", "patch": "@@ -17,9 +17,8 @@\n  * in std.\n  */\n \n-\n+use std::cast;\n use std::comm;\n-use std::unstable::sync::Exclusive;\n use std::sync::arc::UnsafeArc;\n use std::sync::atomics;\n use std::unstable::finally::Finally;\n@@ -32,6 +31,10 @@ use arc::MutexArc;\n  * Internals\n  ****************************************************************************/\n \n+pub mod mutex;\n+pub mod one;\n+mod mpsc_intrusive;\n+\n // Each waiting task receives on one of these.\n #[doc(hidden)]\n type WaitEnd = Port<()>;\n@@ -54,7 +57,7 @@ impl WaitQueue {\n             comm::Data(ch) => {\n                 // Send a wakeup signal. If the waiter was killed, its port will\n                 // have closed. Keep trying until we get a live task.\n-                if ch.try_send_deferred(()) {\n+                if ch.try_send(()) {\n                     true\n                 } else {\n                     self.signal()\n@@ -69,7 +72,7 @@ impl WaitQueue {\n         loop {\n             match self.head.try_recv() {\n                 comm::Data(ch) => {\n-                    if ch.try_send_deferred(()) {\n+                    if ch.try_send(()) {\n                         count += 1;\n                     }\n                 }\n@@ -81,36 +84,45 @@ impl WaitQueue {\n \n     fn wait_end(&self) -> WaitEnd {\n         let (wait_end, signal_end) = Chan::new();\n-        assert!(self.tail.try_send_deferred(signal_end));\n+        assert!(self.tail.try_send(signal_end));\n         wait_end\n     }\n }\n \n // The building-block used to make semaphores, mutexes, and rwlocks.\n-#[doc(hidden)]\n struct SemInner<Q> {\n+    lock: mutex::Mutex,\n     count: int,\n     waiters:   WaitQueue,\n     // Can be either unit or another waitqueue. Some sems shouldn't come with\n     // a condition variable attached, others should.\n     blocked:   Q\n }\n \n-#[doc(hidden)]\n-struct Sem<Q>(Exclusive<SemInner<Q>>);\n+struct Sem<Q>(UnsafeArc<SemInner<Q>>);\n \n #[doc(hidden)]\n impl<Q:Send> Sem<Q> {\n     fn new(count: int, q: Q) -> Sem<Q> {\n-        Sem(Exclusive::new(SemInner {\n-            count: count, waiters: WaitQueue::new(), blocked: q }))\n+        Sem(UnsafeArc::new(SemInner {\n+            count: count,\n+            waiters: WaitQueue::new(),\n+            blocked: q,\n+            lock: mutex::Mutex::new(),\n+        }))\n+    }\n+\n+    unsafe fn with(&self, f: |&mut SemInner<Q>|) {\n+        let Sem(ref arc) = *self;\n+        let state = arc.get();\n+        let _g = (*state).lock.lock();\n+        f(cast::transmute(state));\n     }\n \n     pub fn acquire(&self) {\n         unsafe {\n             let mut waiter_nobe = None;\n-            let Sem(ref lock) = *self;\n-            lock.with(|state| {\n+            self.with(|state| {\n                 state.count -= 1;\n                 if state.count < 0 {\n                     // Create waiter nobe, enqueue ourself, and tell\n@@ -129,8 +141,7 @@ impl<Q:Send> Sem<Q> {\n \n     pub fn release(&self) {\n         unsafe {\n-            let Sem(ref lock) = *self;\n-            lock.with(|state| {\n+            self.with(|state| {\n                 state.count += 1;\n                 if state.count <= 0 {\n                     state.waiters.signal();\n@@ -210,8 +221,7 @@ impl<'a> Condvar<'a> {\n         let mut out_of_bounds = None;\n         // Release lock, 'atomically' enqueuing ourselves in so doing.\n         unsafe {\n-            let Sem(ref queue) = *self.sem;\n-            queue.with(|state| {\n+            self.sem.with(|state| {\n                 if condvar_id < state.blocked.len() {\n                     // Drop the lock.\n                     state.count += 1;\n@@ -253,8 +263,7 @@ impl<'a> Condvar<'a> {\n         unsafe {\n             let mut out_of_bounds = None;\n             let mut result = false;\n-            let Sem(ref lock) = *self.sem;\n-            lock.with(|state| {\n+            self.sem.with(|state| {\n                 if condvar_id < state.blocked.len() {\n                     result = state.blocked[condvar_id].signal();\n                 } else {\n@@ -276,8 +285,7 @@ impl<'a> Condvar<'a> {\n         let mut out_of_bounds = None;\n         let mut queue = None;\n         unsafe {\n-            let Sem(ref lock) = *self.sem;\n-            lock.with(|state| {\n+            self.sem.with(|state| {\n                 if condvar_id < state.blocked.len() {\n                     // To avoid :broadcast_heavy, we make a new waitqueue,\n                     // swap it out with the old one, and broadcast on the", "previous_filename": "src/libextra/sync.rs"}, {"sha": "0f13a4980d9191c58029d4144549279828dd4232", "filename": "src/libextra/sync/mpsc_intrusive.rs", "status": "added", "additions": 139, "deletions": 0, "changes": 139, "blob_url": "https://github.com/rust-lang/rust/blob/d42521aa92006a3378c535adec80ae2257bff083/src%2Flibextra%2Fsync%2Fmpsc_intrusive.rs", "raw_url": "https://github.com/rust-lang/rust/raw/d42521aa92006a3378c535adec80ae2257bff083/src%2Flibextra%2Fsync%2Fmpsc_intrusive.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibextra%2Fsync%2Fmpsc_intrusive.rs?ref=d42521aa92006a3378c535adec80ae2257bff083", "patch": "@@ -0,0 +1,139 @@\n+/* Copyright (c) 2010-2011 Dmitry Vyukov. All rights reserved.\n+ * Redistribution and use in source and binary forms, with or without\n+ * modification, are permitted provided that the following conditions are met:\n+ *\n+ *    1. Redistributions of source code must retain the above copyright notice,\n+ *       this list of conditions and the following disclaimer.\n+ *\n+ *    2. Redistributions in binary form must reproduce the above copyright\n+ *       notice, this list of conditions and the following disclaimer in the\n+ *       documentation and/or other materials provided with the distribution.\n+ *\n+ * THIS SOFTWARE IS PROVIDED BY DMITRY VYUKOV \"AS IS\" AND ANY EXPRESS OR IMPLIED\n+ * WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF\n+ * MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO\n+ * EVENT SHALL DMITRY VYUKOV OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT,\n+ * INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\n+ * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA,\n+ * OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF\n+ * LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING\n+ * NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n+ * EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n+ *\n+ * The views and conclusions contained in the software and documentation are\n+ * those of the authors and should not be interpreted as representing official\n+ * policies, either expressed or implied, of Dmitry Vyukov.\n+ */\n+\n+//! A mostly lock-free multi-producer, single consumer queue.\n+//!\n+//! This module implements an intrusive MPSC queue. This queue is incredibly\n+//! unsafe (due to use of unsafe pointers for nodes), and hence is not public.\n+\n+// http://www.1024cores.net/home/lock-free-algorithms\n+//                         /queues/intrusive-mpsc-node-based-queue\n+\n+use std::cast;\n+use std::sync::atomics;\n+\n+// NB: all links are done as AtomicUint instead of AtomicPtr to allow for static\n+// initialization.\n+\n+pub struct Node<T> {\n+    next: atomics::AtomicUint,\n+    data: T,\n+}\n+\n+pub struct DummyNode {\n+    next: atomics::AtomicUint,\n+}\n+\n+pub struct Queue<T> {\n+    head: atomics::AtomicUint,\n+    tail: *mut Node<T>,\n+    stub: DummyNode,\n+}\n+\n+impl<T: Send> Queue<T> {\n+    pub fn new() -> Queue<T> {\n+        Queue {\n+            head: atomics::AtomicUint::new(0),\n+            tail: 0 as *mut Node<T>,\n+            stub: DummyNode {\n+                next: atomics::AtomicUint::new(0),\n+            },\n+        }\n+    }\n+\n+    pub unsafe fn push(&mut self, node: *mut Node<T>) {\n+        (*node).next.store(0, atomics::Release);\n+        let prev = self.head.swap(node as uint, atomics::AcqRel);\n+\n+        // Note that this code is slightly modified to allow static\n+        // initialization of these queues with rust's flavor of static\n+        // initialization.\n+        if prev == 0 {\n+            self.stub.next.store(node as uint, atomics::Release);\n+        } else {\n+            let prev = prev as *mut Node<T>;\n+            (*prev).next.store(node as uint, atomics::Release);\n+        }\n+    }\n+\n+    /// You'll note that the other MPSC queue in std::sync is non-intrusive and\n+    /// returns a `PopResult` here to indicate when the queue is inconsistent.\n+    /// An \"inconsistent state\" in the other queue means that a pusher has\n+    /// pushed, but it hasn't finished linking the rest of the chain.\n+    ///\n+    /// This queue also suffers from this problem, but I currently haven't been\n+    /// able to detangle when this actually happens. This code is translated\n+    /// verbatim from the website above, and is more complicated than the\n+    /// non-intrusive version.\n+    ///\n+    /// Right now consumers of this queue must be ready for this fact. Just\n+    /// because `pop` returns `None` does not mean that there is not data\n+    /// on the queue.\n+    pub unsafe fn pop(&mut self) -> Option<*mut Node<T>> {\n+        let tail = self.tail;\n+        let mut tail = if !tail.is_null() {tail} else {\n+            cast::transmute(&self.stub)\n+        };\n+        let mut next = (*tail).next(atomics::Relaxed);\n+        if tail as uint == &self.stub as *DummyNode as uint {\n+            if next.is_null() {\n+                return None;\n+            }\n+            self.tail = next;\n+            tail = next;\n+            next = (*next).next(atomics::Relaxed);\n+        }\n+        if !next.is_null() {\n+            self.tail = next;\n+            return Some(tail);\n+        }\n+        let head = self.head.load(atomics::Acquire) as *mut Node<T>;\n+        if tail != head {\n+            return None;\n+        }\n+        let stub = cast::transmute(&self.stub);\n+        self.push(stub);\n+        next = (*tail).next(atomics::Relaxed);\n+        if !next.is_null() {\n+            self.tail = next;\n+            return Some(tail);\n+        }\n+        return None\n+    }\n+}\n+\n+impl<T: Send> Node<T> {\n+    pub fn new(t: T) -> Node<T> {\n+        Node {\n+            data: t,\n+            next: atomics::AtomicUint::new(0),\n+        }\n+    }\n+    pub unsafe fn next(&mut self, ord: atomics::Ordering) -> *mut Node<T> {\n+        cast::transmute::<uint, *mut Node<T>>(self.next.load(ord))\n+    }\n+}"}, {"sha": "7ea98c0741a296faf8f1b825e6caf8f3accfd456", "filename": "src/libextra/sync/mutex.rs", "status": "added", "additions": 557, "deletions": 0, "changes": 557, "blob_url": "https://github.com/rust-lang/rust/blob/d42521aa92006a3378c535adec80ae2257bff083/src%2Flibextra%2Fsync%2Fmutex.rs", "raw_url": "https://github.com/rust-lang/rust/raw/d42521aa92006a3378c535adec80ae2257bff083/src%2Flibextra%2Fsync%2Fmutex.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibextra%2Fsync%2Fmutex.rs?ref=d42521aa92006a3378c535adec80ae2257bff083", "patch": "@@ -0,0 +1,557 @@\n+// Copyright 2014 The Rust Project Developers. See the COPYRIGHT\n+// file at the top-level directory of this distribution and at\n+// http://rust-lang.org/COPYRIGHT.\n+//\n+// Licensed under the Apache License, Version 2.0 <LICENSE-APACHE or\n+// http://www.apache.org/licenses/LICENSE-2.0> or the MIT license\n+// <LICENSE-MIT or http://opensource.org/licenses/MIT>, at your\n+// option. This file may not be copied, modified, or distributed\n+// except according to those terms.\n+\n+//! A proper mutex implementation regardless of the \"flavor of task\" which is\n+//! acquiring the lock.\n+\n+// # Implementation of Rust mutexes\n+//\n+// Most answers to the question of \"how do I use a mutex\" are \"use pthreads\",\n+// but for Rust this isn't quite sufficient. Green threads cannot acquire an OS\n+// mutex because they can context switch among many OS threads, leading to\n+// deadlocks with other green threads.\n+//\n+// Another problem for green threads grabbing an OS mutex is that POSIX dictates\n+// that unlocking a mutex on a different thread from where it was locked is\n+// undefined behavior. Remember that green threads can migrate among OS threads,\n+// so this would mean that we would have to pin green threads to OS threads,\n+// which is less than ideal.\n+//\n+// ## Using deschedule/reawaken\n+//\n+// We already have primitives for descheduling/reawakening tasks, so they're the\n+// first obvious choice when implementing a mutex. The idea would be to have a\n+// concurrent queue that everyone is pushed on to, and then the owner of the\n+// mutex is the one popping from the queue.\n+//\n+// Unfortunately, this is not very performant for native tasks. The suspected\n+// reason for this is that each native thread is suspended on its own condition\n+// variable, unique from all the other threads. In this situation, the kernel\n+// has no idea what the scheduling semantics are of the user program, so all of\n+// the threads are distributed among all cores on the system. This ends up\n+// having very expensive wakeups of remote cores high up in the profile when\n+// handing off the mutex among native tasks. On the other hand, when using an OS\n+// mutex, the kernel knows that all native threads are contended on the same\n+// mutex, so they're in theory all migrated to a single core (fast context\n+// switching).\n+//\n+// ## Mixing implementations\n+//\n+// From that above information, we have two constraints. The first is that\n+// green threads can't touch os mutexes, and the second is that native tasks\n+// pretty much *must* touch an os mutex.\n+//\n+// As a compromise, the queueing implementation is used for green threads and\n+// the os mutex is used for native threads (why not have both?). This ends up\n+// leading to fairly decent performance for both native threads and green\n+// threads on various workloads (uncontended and contended).\n+//\n+// The crux of this implementation is an atomic work which is CAS'd on many many\n+// times in order to manage a few flags about who's blocking where and whether\n+// it's locked or not.\n+\n+use std::rt::local::Local;\n+use std::rt::task::{BlockedTask, Task};\n+use std::rt::thread::Thread;\n+use std::sync::atomics;\n+use std::unstable::mutex;\n+\n+use q = sync::mpsc_intrusive;\n+\n+pub static LOCKED: uint = 1 << 0;\n+pub static GREEN_BLOCKED: uint = 1 << 1;\n+pub static NATIVE_BLOCKED: uint = 1 << 2;\n+\n+/// A mutual exclusion primitive useful for protecting shared data\n+///\n+/// This mutex is an implementation of a lock for all flavors of tasks which may\n+/// be grabbing. A common problem with green threads is that they cannot grab\n+/// locks (if they reschedule during the lock a contender could deadlock the\n+/// system), but this mutex does *not* suffer this problem.\n+///\n+/// This mutex will properly block tasks waiting for the lock to become\n+/// available. The mutex can also be statically initialized or created via a\n+/// `new` constructor.\n+///\n+/// # Example\n+///\n+/// ```rust\n+/// use extra::sync::mutex::Mutex;\n+///\n+/// let mut m = Mutex::new();\n+/// let guard = m.lock();\n+/// // do some work\n+/// drop(guard); // unlock the lock\n+/// ```\n+pub struct Mutex {\n+    priv lock: StaticMutex,\n+}\n+\n+#[deriving(Eq)]\n+enum Flavor {\n+    Unlocked,\n+    TryLockAcquisition,\n+    GreenAcquisition,\n+    NativeAcquisition,\n+}\n+\n+/// The static mutex type is provided to allow for static allocation of mutexes.\n+///\n+/// Note that this is a separate type because using a Mutex correctly means that\n+/// it needs to have a destructor run. In Rust, statics are not allowed to have\n+/// destructors. As a result, a `StaticMutex` has one extra method when compared\n+/// to a `Mutex`, a `destroy` method. This method is unsafe to call, and\n+/// documentation can be found directly on the method.\n+///\n+/// # Example\n+///\n+/// ```rust\n+/// use extra::sync::mutex::{StaticMutex, MUTEX_INIT};\n+///\n+/// static mut LOCK: StaticMutex = MUTEX_INIT;\n+///\n+/// unsafe {\n+///     let _g = LOCK.lock();\n+///     // do some productive work\n+/// }\n+/// // lock is unlocked here.\n+/// ```\n+pub struct StaticMutex {\n+    /// Current set of flags on this mutex\n+    priv state: atomics::AtomicUint,\n+    /// Type of locking operation currently on this mutex\n+    priv flavor: Flavor,\n+    /// uint-cast of the green thread waiting for this mutex\n+    priv green_blocker: uint,\n+    /// uint-cast of the native thread waiting for this mutex\n+    priv native_blocker: uint,\n+    /// an OS mutex used by native threads\n+    priv lock: mutex::Mutex,\n+\n+    /// A concurrent mpsc queue used by green threads, along with a count used\n+    /// to figure out when to dequeue and enqueue.\n+    priv q: q::Queue<uint>,\n+    priv green_cnt: atomics::AtomicUint,\n+}\n+\n+/// An RAII implementation of a \"scoped lock\" of a mutex. When this structure is\n+/// dropped (falls out of scope), the lock will be unlocked.\n+pub struct Guard<'a> {\n+    priv lock: &'a mut StaticMutex,\n+}\n+\n+/// Static initialization of a mutex. This constant can be used to initialize\n+/// other mutex constants.\n+pub static MUTEX_INIT: StaticMutex = StaticMutex {\n+    lock: mutex::MUTEX_INIT,\n+    state: atomics::INIT_ATOMIC_UINT,\n+    flavor: Unlocked,\n+    green_blocker: 0,\n+    native_blocker: 0,\n+    green_cnt: atomics::INIT_ATOMIC_UINT,\n+    q: q::Queue {\n+        head: atomics::INIT_ATOMIC_UINT,\n+        tail: 0 as *mut q::Node<uint>,\n+        stub: q::DummyNode {\n+            next: atomics::INIT_ATOMIC_UINT,\n+        }\n+    }\n+};\n+\n+impl StaticMutex {\n+    /// Attempts to grab this lock, see `Mutex::try_lock`\n+    pub fn try_lock<'a>(&'a mut self) -> Option<Guard<'a>> {\n+        // Attempt to steal the mutex from an unlocked state.\n+        //\n+        // FIXME: this can mess up the fairness of the mutex, seems bad\n+        match self.state.compare_and_swap(0, LOCKED, atomics::SeqCst) {\n+            0 => {\n+                assert!(self.flavor == Unlocked);\n+                self.flavor = TryLockAcquisition;\n+                Some(Guard::new(self))\n+            }\n+            _ => None\n+        }\n+    }\n+\n+    /// Acquires this lock, see `Mutex::lock`\n+    pub fn lock<'a>(&'a mut self) -> Guard<'a> {\n+        // First, attempt to steal the mutex from an unlocked state. The \"fast\n+        // path\" needs to have as few atomic instructions as possible, and this\n+        // one cmpxchg is already pretty expensive.\n+        //\n+        // FIXME: this can mess up the fairness of the mutex, seems bad\n+        match self.state.compare_and_swap(0, LOCKED, atomics::SeqCst) {\n+            0 => {\n+                assert!(self.flavor == Unlocked);\n+                self.flavor = TryLockAcquisition;\n+                return Guard::new(self)\n+            }\n+            _ => {}\n+        }\n+\n+        // After we've failed the fast path, then we delegate to the differnet\n+        // locking protocols for green/native tasks. This will select two tasks\n+        // to continue further (one native, one green).\n+        let t: ~Task = Local::take();\n+        let can_block = t.can_block();\n+        let native_bit;\n+        if can_block {\n+            self.native_lock(t);\n+            native_bit = NATIVE_BLOCKED;\n+        } else {\n+            self.green_lock(t);\n+            native_bit = GREEN_BLOCKED;\n+        }\n+\n+        // After we've arbitrated among task types, attempt to re-acquire the\n+        // lock (avoids a deschedule). This is very important to do in order to\n+        // allow threads coming out of the native_lock function to try their\n+        // best to not hit a cvar in deschedule.\n+        let mut old = match self.state.compare_and_swap(0, LOCKED,\n+                                                        atomics::SeqCst) {\n+            0 => {\n+                self.flavor = if can_block {\n+                    NativeAcquisition\n+                } else {\n+                    GreenAcquisition\n+                };\n+                return Guard::new(self)\n+            }\n+            old => old,\n+        };\n+\n+        // Alright, everything else failed. We need to deschedule ourselves and\n+        // flag ourselves as waiting. Note that this case should only happen\n+        // regularly in native/green contention. Due to try_lock and the header\n+        // of lock stealing the lock, it's also possible for native/native\n+        // contention to hit this location, but as less common.\n+        let t: ~Task = Local::take();\n+        t.deschedule(1, |task| {\n+            let task = unsafe { task.cast_to_uint() };\n+            if can_block {\n+                assert_eq!(self.native_blocker, 0);\n+                self.native_blocker = task;\n+            } else {\n+                assert_eq!(self.green_blocker, 0);\n+                self.green_blocker = task;\n+            }\n+\n+            loop {\n+                assert_eq!(old & native_bit, 0);\n+                // If the old state was locked, then we need to flag ourselves\n+                // as blocking in the state. If the old state was unlocked, then\n+                // we attempt to acquire the mutex. Everything here is a CAS\n+                // loop that'll eventually make progress.\n+                if old & LOCKED != 0 {\n+                    old = match self.state.compare_and_swap(old,\n+                                                            old | native_bit,\n+                                                            atomics::SeqCst) {\n+                        n if n == old => return Ok(()),\n+                        n => n\n+                    };\n+                } else {\n+                    assert_eq!(old, 0);\n+                    old = match self.state.compare_and_swap(old,\n+                                                            old | LOCKED,\n+                                                            atomics::SeqCst) {\n+                        n if n == old => {\n+                            assert_eq!(self.flavor, Unlocked);\n+                            if can_block {\n+                                self.native_blocker = 0;\n+                                self.flavor = NativeAcquisition;\n+                            } else {\n+                                self.green_blocker = 0;\n+                                self.flavor = GreenAcquisition;\n+                            }\n+                            return Err(unsafe {\n+                                BlockedTask::cast_from_uint(task)\n+                            })\n+                        }\n+                        n => n,\n+                    };\n+                }\n+            }\n+        });\n+\n+        Guard::new(self)\n+    }\n+\n+    // Tasks which can block are super easy. These tasks just call the blocking\n+    // `lock()` function on an OS mutex\n+    fn native_lock(&mut self, t: ~Task) {\n+        Local::put(t);\n+        unsafe { self.lock.lock(); }\n+    }\n+\n+    fn native_unlock(&mut self) {\n+        unsafe { self.lock.unlock(); }\n+    }\n+\n+    fn green_lock(&mut self, t: ~Task) {\n+        // Green threads flag their presence with an atomic counter, and if they\n+        // fail to be the first to the mutex, they enqueue themselves on a\n+        // concurrent internal queue with a stack-allocated node.\n+        //\n+        // FIXME: There isn't a cancellation currently of an enqueue, forcing\n+        //        the unlocker to spin for a bit.\n+        if self.green_cnt.fetch_add(1, atomics::SeqCst) == 0 {\n+            Local::put(t);\n+            return\n+        }\n+\n+        let mut node = q::Node::new(0);\n+        t.deschedule(1, |task| {\n+            unsafe {\n+                node.data = task.cast_to_uint();\n+                self.q.push(&mut node);\n+            }\n+            Ok(())\n+        });\n+    }\n+\n+    fn green_unlock(&mut self) {\n+        // If we're the only green thread, then no need to check the queue,\n+        // otherwise the fixme above forces us to spin for a bit.\n+        if self.green_cnt.fetch_sub(1, atomics::SeqCst) == 1 { return }\n+        let node;\n+        loop {\n+            match unsafe { self.q.pop() } {\n+                Some(t) => { node = t; break; }\n+                None => Thread::yield_now(),\n+            }\n+        }\n+        let task = unsafe { BlockedTask::cast_from_uint((*node).data) };\n+        task.wake().map(|t| t.reawaken());\n+    }\n+\n+    fn unlock(&mut self) {\n+        // Unlocking this mutex is a little tricky. We favor any task that is\n+        // manually blocked (not in each of the separate locks) in order to help\n+        // provide a little fairness (green threads will wake up the pending\n+        // native thread and native threads will wake up the pending green\n+        // thread).\n+        //\n+        // There's also the question of when we unlock the actual green/native\n+        // locking halves as well. If we're waking up someone, then we can wait\n+        // to unlock until we've acquired the task to wake up (we're guaranteed\n+        // the mutex memory is still valid when there's contenders), but as soon\n+        // as we don't find any contenders we must unlock the mutex, and *then*\n+        // flag the mutex as unlocked.\n+        //\n+        // This flagging can fail, leading to another round of figuring out if a\n+        // task needs to be woken, and in this case it's ok that the \"mutex\n+        // halves\" are unlocked, we're just mainly dealing with the atomic state\n+        // of the outer mutex.\n+        let flavor = self.flavor;\n+        self.flavor = Unlocked;\n+\n+        let mut state = self.state.load(atomics::SeqCst);\n+        let mut unlocked = false;\n+        let task;\n+        loop {\n+            assert!(state & LOCKED != 0);\n+            if state & GREEN_BLOCKED != 0 {\n+                self.unset(state, GREEN_BLOCKED);\n+                task = unsafe {\n+                    BlockedTask::cast_from_uint(self.green_blocker)\n+                };\n+                self.green_blocker = 0;\n+                self.flavor = GreenAcquisition;\n+                break;\n+            } else if state & NATIVE_BLOCKED != 0 {\n+                self.unset(state, NATIVE_BLOCKED);\n+                task = unsafe {\n+                    BlockedTask::cast_from_uint(self.native_blocker)\n+                };\n+                self.native_blocker = 0;\n+                self.flavor = NativeAcquisition;\n+                break;\n+            } else {\n+                assert_eq!(state, LOCKED);\n+                if !unlocked {\n+                    match flavor {\n+                        GreenAcquisition => { self.green_unlock(); }\n+                        NativeAcquisition => { self.native_unlock(); }\n+                        TryLockAcquisition => {}\n+                        Unlocked => unreachable!()\n+                    }\n+                    unlocked = true;\n+                }\n+                match self.state.compare_and_swap(LOCKED, 0, atomics::SeqCst) {\n+                    LOCKED => return,\n+                    n => { state = n; }\n+                }\n+            }\n+        }\n+        if !unlocked {\n+            match flavor {\n+                GreenAcquisition => { self.green_unlock(); }\n+                NativeAcquisition => { self.native_unlock(); }\n+                TryLockAcquisition => {}\n+                Unlocked => unreachable!()\n+            }\n+        }\n+\n+        task.wake().map(|t| t.reawaken());\n+    }\n+\n+    /// Loops around a CAS to unset the `bit` in `state`\n+    fn unset(&mut self, mut state: uint, bit: uint) {\n+        loop {\n+            assert!(state & bit != 0);\n+            let new = state ^ bit;\n+            match self.state.compare_and_swap(state, new, atomics::SeqCst) {\n+                n if n == state => break,\n+                n => { state = n; }\n+            }\n+        }\n+    }\n+\n+    /// Deallocates resources associated with this static mutex.\n+    ///\n+    /// This method is unsafe because it provides no guarantees that there are\n+    /// no active users of this mutex, and safety is not guaranteed if there are\n+    /// active users of this mutex.\n+    ///\n+    /// This method is required to ensure that there are no memory leaks on\n+    /// *all* platforms. It may be the case that some platforms do not leak\n+    /// memory if this method is not called, but this is not guaranteed to be\n+    /// true on all platforms.\n+    pub unsafe fn destroy(&mut self) {\n+        self.lock.destroy()\n+    }\n+}\n+\n+impl Mutex {\n+    /// Creates a new mutex in an unlocked state ready for use.\n+    pub fn new() -> Mutex {\n+        Mutex {\n+            lock: StaticMutex {\n+                state: atomics::AtomicUint::new(0),\n+                flavor: Unlocked,\n+                green_blocker: 0,\n+                native_blocker: 0,\n+                green_cnt: atomics::AtomicUint::new(0),\n+                q: q::Queue::new(),\n+                lock: unsafe { mutex::Mutex::new() },\n+            }\n+        }\n+    }\n+\n+    /// Attempts to acquire this lock.\n+    ///\n+    /// If the lock could not be acquired at this time, then `None` is returned.\n+    /// Otherwise, an RAII guard is returned. The lock will be unlocked when the\n+    /// guard is dropped.\n+    ///\n+    /// This function does not block.\n+    pub fn try_lock<'a>(&'a mut self) -> Option<Guard<'a>> {\n+        self.lock.try_lock()\n+    }\n+\n+    /// Acquires a mutex, blocking the current task until it is able to do so.\n+    ///\n+    /// This function will block the local task until it is availble to acquire\n+    /// the mutex. Upon returning, the task is the only task with the mutex\n+    /// held. An RAII guard is returned to allow scoped unlock of the lock. When\n+    /// the guard goes out of scope, the mutex will be unlocked.\n+    pub fn lock<'a>(&'a mut self) -> Guard<'a> { self.lock.lock() }\n+}\n+\n+impl<'a> Guard<'a> {\n+    fn new<'b>(lock: &'b mut StaticMutex) -> Guard<'b> {\n+        if cfg!(debug) {\n+            assert!(lock.flavor != Unlocked);\n+            assert!(lock.state.load(atomics::SeqCst) & LOCKED != 0);\n+        }\n+        Guard { lock: lock }\n+    }\n+}\n+\n+#[unsafe_destructor]\n+impl<'a> Drop for Guard<'a> {\n+    #[inline]\n+    fn drop(&mut self) {\n+        self.lock.unlock();\n+    }\n+}\n+\n+impl Drop for Mutex {\n+    fn drop(&mut self) {\n+        // This is actually safe b/c we know that there is no further usage of\n+        // this mutex (it's up to the user to arrange for a mutex to get\n+        // dropped, that's not our job)\n+        unsafe { self.lock.destroy() }\n+    }\n+}\n+\n+#[cfg(test)]\n+mod test {\n+    extern mod native;\n+    use super::{Mutex, StaticMutex, MUTEX_INIT};\n+\n+    #[test]\n+    fn smoke() {\n+        let mut m = Mutex::new();\n+        drop(m.lock());\n+        drop(m.lock());\n+    }\n+\n+    #[test]\n+    fn smoke_static() {\n+        static mut m: StaticMutex = MUTEX_INIT;\n+        unsafe {\n+            drop(m.lock());\n+            drop(m.lock());\n+            m.destroy();\n+        }\n+    }\n+\n+    #[test]\n+    fn lots_and_lots() {\n+        static mut m: StaticMutex = MUTEX_INIT;\n+        static mut CNT: uint = 0;\n+        static M: uint = 1000;\n+        static N: uint = 3;\n+\n+        fn inc() {\n+            for _ in range(0, M) {\n+                unsafe {\n+                    let _g = m.lock();\n+                    CNT += 1;\n+                }\n+            }\n+        }\n+\n+        let (p, c) = SharedChan::new();\n+        for _ in range(0, N) {\n+            let c2 = c.clone();\n+            native::task::spawn(proc() { inc(); c2.send(()); });\n+            let c2 = c.clone();\n+            spawn(proc() { inc(); c2.send(()); });\n+        }\n+\n+        drop(c);\n+        for _ in range(0, 2 * N) {\n+            p.recv();\n+        }\n+        assert_eq!(unsafe {CNT}, M * N * 2);\n+        unsafe {\n+            m.destroy();\n+        }\n+    }\n+\n+    #[test]\n+    fn trylock() {\n+        let mut m = Mutex::new();\n+        assert!(m.try_lock().is_some());\n+    }\n+}"}, {"sha": "826955d93e8d3834f9eb7d35b5a408e158a5e2d8", "filename": "src/libextra/sync/one.rs", "status": "added", "additions": 168, "deletions": 0, "changes": 168, "blob_url": "https://github.com/rust-lang/rust/blob/d42521aa92006a3378c535adec80ae2257bff083/src%2Flibextra%2Fsync%2Fone.rs", "raw_url": "https://github.com/rust-lang/rust/raw/d42521aa92006a3378c535adec80ae2257bff083/src%2Flibextra%2Fsync%2Fone.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibextra%2Fsync%2Fone.rs?ref=d42521aa92006a3378c535adec80ae2257bff083", "patch": "@@ -0,0 +1,168 @@\n+// Copyright 2014 The Rust Project Developers. See the COPYRIGHT\n+// file at the top-level directory of this distribution and at\n+// http://rust-lang.org/COPYRIGHT.\n+//\n+// Licensed under the Apache License, Version 2.0 <LICENSE-APACHE or\n+// http://www.apache.org/licenses/LICENSE-2.0> or the MIT license\n+// <LICENSE-MIT or http://opensource.org/licenses/MIT>, at your\n+// option. This file may not be copied, modified, or distributed\n+// except according to those terms.\n+\n+//! A \"once initialization\" primitive\n+//!\n+//! This primitive is meant to be used to run one-time initialization. An\n+//! example use case would be for initializing an FFI library.\n+\n+use std::int;\n+use std::sync::atomics;\n+use sync::mutex::{StaticMutex, MUTEX_INIT};\n+\n+/// A type which can be used to run a one-time global initialization. This type\n+/// is *unsafe* to use because it is built on top of the `Mutex` in this module.\n+/// It does not know whether the currently running task is in a green or native\n+/// context, and a blocking mutex should *not* be used under normal\n+/// circumstances on a green task.\n+///\n+/// Despite its unsafety, it is often useful to have a one-time initialization\n+/// routine run for FFI bindings or related external functionality. This type\n+/// can only be statically constructed with the `ONCE_INIT` value.\n+///\n+/// # Example\n+///\n+/// ```rust\n+/// use extra::sync::one::{Once, ONCE_INIT};\n+///\n+/// static mut START: Once = ONCE_INIT;\n+/// unsafe {\n+///     START.doit(|| {\n+///         // run initialization here\n+///     });\n+/// }\n+/// ```\n+pub struct Once {\n+    priv mutex: StaticMutex,\n+    priv cnt: atomics::AtomicInt,\n+    priv lock_cnt: atomics::AtomicInt,\n+}\n+\n+/// Initialization value for static `Once` values.\n+pub static ONCE_INIT: Once = Once {\n+    mutex: MUTEX_INIT,\n+    cnt: atomics::INIT_ATOMIC_INT,\n+    lock_cnt: atomics::INIT_ATOMIC_INT,\n+};\n+\n+impl Once {\n+    /// Perform an initialization routine once and only once. The given closure\n+    /// will be executed if this is the first time `doit` has been called, and\n+    /// otherwise the routine will *not* be invoked.\n+    ///\n+    /// This method will block the calling *os thread* if another initialization\n+    /// routine is currently running.\n+    ///\n+    /// When this function returns, it is guaranteed that some initialization\n+    /// has run and completed (it may not be the closure specified).\n+    pub fn doit(&mut self, f: ||) {\n+        // Implementation-wise, this would seem like a fairly trivial primitive.\n+        // The stickler part is where our mutexes currently require an\n+        // allocation, and usage of a `Once` should't leak this allocation.\n+        //\n+        // This means that there must be a deterministic destroyer of the mutex\n+        // contained within (because it's not needed after the initialization\n+        // has run).\n+        //\n+        // The general scheme here is to gate all future threads once\n+        // initialization has completed with a \"very negative\" count, and to\n+        // allow through threads to lock the mutex if they see a non negative\n+        // count. For all threads grabbing the mutex, exactly one of them should\n+        // be responsible for unlocking the mutex, and this should only be done\n+        // once everyone else is done with the mutex.\n+        //\n+        // This atomicity is achieved by swapping a very negative value into the\n+        // shared count when the initialization routine has completed. This will\n+        // read the number of threads which will at some point attempt to\n+        // acquire the mutex. This count is then squirreled away in a separate\n+        // variable, and the last person on the way out of the mutex is then\n+        // responsible for destroying the mutex.\n+        //\n+        // It is crucial that the negative value is swapped in *after* the\n+        // initialization routine has completed because otherwise new threads\n+        // calling `doit` will return immediately before the initialization has\n+        // completed.\n+\n+        let prev = self.cnt.fetch_add(1, atomics::SeqCst);\n+        if prev < 0 {\n+            // Make sure we never overflow, we'll never have int::MIN\n+            // simultaneous calls to `doit` to make this value go back to 0\n+            self.cnt.store(int::MIN, atomics::SeqCst);\n+            return\n+        }\n+\n+        // If the count is negative, then someone else finished the job,\n+        // otherwise we run the job and record how many people will try to grab\n+        // this lock\n+        {\n+            let _guard = self.mutex.lock();\n+            if self.cnt.load(atomics::SeqCst) > 0 {\n+                f();\n+                let prev = self.cnt.swap(int::MIN, atomics::SeqCst);\n+                self.lock_cnt.store(prev, atomics::SeqCst);\n+            }\n+        }\n+\n+        // Last one out cleans up after everyone else, no leaks!\n+        if self.lock_cnt.fetch_add(-1, atomics::SeqCst) == 1 {\n+            unsafe { self.mutex.destroy() }\n+        }\n+    }\n+}\n+\n+#[cfg(test)]\n+mod test {\n+    use super::{ONCE_INIT, Once};\n+    use std::task;\n+\n+    #[test]\n+    fn smoke_once() {\n+        static mut o: Once = ONCE_INIT;\n+        let mut a = 0;\n+        unsafe { o.doit(|| a += 1); }\n+        assert_eq!(a, 1);\n+        unsafe { o.doit(|| a += 1); }\n+        assert_eq!(a, 1);\n+    }\n+\n+    #[test]\n+    fn stampede_once() {\n+        static mut o: Once = ONCE_INIT;\n+        static mut run: bool = false;\n+\n+        let (p, c) = SharedChan::new();\n+        for _ in range(0, 10) {\n+            let c = c.clone();\n+            spawn(proc() {\n+                for _ in range(0, 4) { task::deschedule() }\n+                unsafe {\n+                    o.doit(|| {\n+                        assert!(!run);\n+                        run = true;\n+                    });\n+                    assert!(run);\n+                }\n+                c.send(());\n+            });\n+        }\n+\n+        unsafe {\n+            o.doit(|| {\n+                assert!(!run);\n+                run = true;\n+            });\n+            assert!(run);\n+        }\n+\n+        for _ in range(0, 10) {\n+            p.recv();\n+        }\n+    }\n+}"}, {"sha": "c8b84d445db2f282f6e0c6a602bc9311d058e5a8", "filename": "src/libgreen/sched.rs", "status": "modified", "additions": 11, "deletions": 16, "changes": 27, "blob_url": "https://github.com/rust-lang/rust/blob/d42521aa92006a3378c535adec80ae2257bff083/src%2Flibgreen%2Fsched.rs", "raw_url": "https://github.com/rust-lang/rust/raw/d42521aa92006a3378c535adec80ae2257bff083/src%2Flibgreen%2Fsched.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibgreen%2Fsched.rs?ref=d42521aa92006a3378c535adec80ae2257bff083", "patch": "@@ -1416,7 +1416,8 @@ mod test {\n \n     #[test]\n     fn test_spawn_sched_blocking() {\n-        use std::unstable::mutex::Mutex;\n+        use std::unstable::mutex::{Mutex, MUTEX_INIT};\n+        static mut LOCK: Mutex = MUTEX_INIT;\n \n         // Testing that a task in one scheduler can block in foreign code\n         // without affecting other schedulers\n@@ -1425,19 +1426,15 @@ mod test {\n             let (start_po, start_ch) = Chan::new();\n             let (fin_po, fin_ch) = Chan::new();\n \n-            let lock = unsafe { Mutex::new() };\n-            let lock2 = unsafe { lock.clone() };\n-\n             let mut handle = pool.spawn_sched();\n             handle.send(PinnedTask(pool.task(TaskOpts::new(), proc() {\n-                let mut lock = lock2;\n                 unsafe {\n-                    lock.lock();\n+                    LOCK.lock();\n \n                     start_ch.send(());\n-                    lock.wait();   // block the scheduler thread\n-                    lock.signal(); // let them know we have the lock\n-                    lock.unlock();\n+                    LOCK.wait();   // block the scheduler thread\n+                    LOCK.signal(); // let them know we have the lock\n+                    LOCK.unlock();\n                 }\n \n                 fin_ch.send(());\n@@ -1469,19 +1466,17 @@ mod test {\n                 child_ch.send(20);\n                 pingpong(&parent_po, &child_ch);\n                 unsafe {\n-                    let mut lock = lock;\n-                    lock.lock();\n-                    lock.signal();   // wakeup waiting scheduler\n-                    lock.wait();     // wait for them to grab the lock\n-                    lock.unlock();\n-                    lock.destroy();  // now we're guaranteed they have no locks\n+                    LOCK.lock();\n+                    LOCK.signal();   // wakeup waiting scheduler\n+                    LOCK.wait();     // wait for them to grab the lock\n+                    LOCK.unlock();\n                 }\n             })));\n             drop(handle);\n \n             fin_po.recv();\n             pool.shutdown();\n         }\n-\n+        unsafe { LOCK.destroy(); }\n     }\n }"}, {"sha": "8db95f55d18db7a13f6f73256ea57d70e7dd7a5a", "filename": "src/libgreen/simple.rs", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "blob_url": "https://github.com/rust-lang/rust/blob/d42521aa92006a3378c535adec80ae2257bff083/src%2Flibgreen%2Fsimple.rs", "raw_url": "https://github.com/rust-lang/rust/raw/d42521aa92006a3378c535adec80ae2257bff083/src%2Flibgreen%2Fsimple.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibgreen%2Fsimple.rs?ref=d42521aa92006a3378c535adec80ae2257bff083", "patch": "@@ -54,7 +54,7 @@ impl Runtime for SimpleTask {\n         }\n         Local::put(cur_task);\n     }\n-    fn reawaken(mut ~self, mut to_wake: ~Task, _can_resched: bool) {\n+    fn reawaken(mut ~self, mut to_wake: ~Task) {\n         let me = &mut *self as *mut SimpleTask;\n         to_wake.put_runtime(self as ~Runtime);\n         unsafe {\n@@ -76,6 +76,7 @@ impl Runtime for SimpleTask {\n     }\n     fn local_io<'a>(&'a mut self) -> Option<rtio::LocalIo<'a>> { None }\n     fn stack_bounds(&self) -> (uint, uint) { fail!() }\n+    fn can_block(&self) -> bool { true }\n     fn wrap(~self) -> ~Any { fail!() }\n }\n "}, {"sha": "4fb61f156809fb200051e6fa0d4477d55a90005f", "filename": "src/libgreen/task.rs", "status": "modified", "additions": 5, "deletions": 8, "changes": 13, "blob_url": "https://github.com/rust-lang/rust/blob/d42521aa92006a3378c535adec80ae2257bff083/src%2Flibgreen%2Ftask.rs", "raw_url": "https://github.com/rust-lang/rust/raw/d42521aa92006a3378c535adec80ae2257bff083/src%2Flibgreen%2Ftask.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibgreen%2Ftask.rs?ref=d42521aa92006a3378c535adec80ae2257bff083", "patch": "@@ -376,7 +376,7 @@ impl Runtime for GreenTask {\n         }\n     }\n \n-    fn reawaken(mut ~self, to_wake: ~Task, can_resched: bool) {\n+    fn reawaken(mut ~self, to_wake: ~Task) {\n         self.put_task(to_wake);\n         assert!(self.sched.is_none());\n \n@@ -409,15 +409,10 @@ impl Runtime for GreenTask {\n         match running_task.maybe_take_runtime::<GreenTask>() {\n             Some(mut running_green_task) => {\n                 running_green_task.put_task(running_task);\n-                let mut sched = running_green_task.sched.take_unwrap();\n+                let sched = running_green_task.sched.take_unwrap();\n \n                 if sched.pool_id == self.pool_id {\n-                    if can_resched {\n-                        sched.run_task(running_green_task, self);\n-                    } else {\n-                        sched.enqueue_task(self);\n-                        running_green_task.put_with_sched(sched);\n-                    }\n+                    sched.run_task(running_green_task, self);\n                 } else {\n                     self.reawaken_remotely();\n \n@@ -462,6 +457,8 @@ impl Runtime for GreenTask {\n          c.current_stack_segment.end() as uint)\n     }\n \n+    fn can_block(&self) -> bool { false }\n+\n     fn wrap(~self) -> ~Any { self as ~Any }\n }\n "}, {"sha": "dd916c8f3c4b964fdf9fe96198be518e1a4a471a", "filename": "src/libnative/io/net.rs", "status": "modified", "additions": 9, "deletions": 4, "changes": 13, "blob_url": "https://github.com/rust-lang/rust/blob/d42521aa92006a3378c535adec80ae2257bff083/src%2Flibnative%2Fio%2Fnet.rs", "raw_url": "https://github.com/rust-lang/rust/raw/d42521aa92006a3378c535adec80ae2257bff083/src%2Flibnative%2Fio%2Fnet.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibnative%2Fio%2Fnet.rs?ref=d42521aa92006a3378c535adec80ae2257bff083", "patch": "@@ -201,14 +201,19 @@ pub fn init() {\n     }\n \n     unsafe {\n-        use std::unstable::mutex::{Once, ONCE_INIT};\n-        static mut INIT: Once = ONCE_INIT;\n-        INIT.doit(|| {\n+        use std::unstable::mutex::{Mutex, MUTEX_INIT};\n+        static mut INITIALIZED: bool = false;\n+        static mut LOCK: Mutex = MUTEX_INIT;\n+\n+        LOCK.lock();\n+        if !INITIALIZED {\n             let mut data: WSADATA = intrinsics::init();\n             let ret = WSAStartup(0x202,      // version 2.2\n                                  &mut data);\n             assert_eq!(ret, 0);\n-        });\n+            INITIALIZED = true;\n+        }\n+        LOCK.unlock();\n     }\n }\n "}, {"sha": "c00b0efadb5eea740eaaf95e39b9ebf8b039c3b1", "filename": "src/libnative/io/timer_helper.rs", "status": "modified", "additions": 8, "deletions": 4, "changes": 12, "blob_url": "https://github.com/rust-lang/rust/blob/d42521aa92006a3378c535adec80ae2257bff083/src%2Flibnative%2Fio%2Ftimer_helper.rs", "raw_url": "https://github.com/rust-lang/rust/raw/d42521aa92006a3378c535adec80ae2257bff083/src%2Flibnative%2Fio%2Ftimer_helper.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibnative%2Fio%2Ftimer_helper.rs?ref=d42521aa92006a3378c535adec80ae2257bff083", "patch": "@@ -22,7 +22,7 @@\n \n use std::cast;\n use std::rt;\n-use std::unstable::mutex::{Once, ONCE_INIT};\n+use std::unstable::mutex::{Mutex, MUTEX_INIT};\n \n use bookkeeping;\n use io::timer::{Req, Shutdown};\n@@ -37,10 +37,12 @@ static mut HELPER_CHAN: *mut SharedChan<Req> = 0 as *mut SharedChan<Req>;\n static mut HELPER_SIGNAL: imp::signal = 0 as imp::signal;\n \n pub fn boot(helper: fn(imp::signal, Port<Req>)) {\n-    static mut INIT: Once = ONCE_INIT;\n+    static mut LOCK: Mutex = MUTEX_INIT;\n+    static mut INITIALIZED: bool = false;\n \n     unsafe {\n-        INIT.doit(|| {\n+        LOCK.lock();\n+        if !INITIALIZED {\n             let (msgp, msgc) = SharedChan::new();\n             HELPER_CHAN = cast::transmute(~msgc);\n             let (receive, send) = imp::new();\n@@ -52,7 +54,9 @@ pub fn boot(helper: fn(imp::signal, Port<Req>)) {\n             });\n \n             rt::at_exit(proc() { shutdown() });\n-        })\n+            INITIALIZED = true;\n+        }\n+        LOCK.unlock();\n     }\n }\n "}, {"sha": "c08e326d903312d80c3561be816e328bc128cfbc", "filename": "src/libnative/task.rs", "status": "modified", "additions": 3, "deletions": 1, "changes": 4, "blob_url": "https://github.com/rust-lang/rust/blob/d42521aa92006a3378c535adec80ae2257bff083/src%2Flibnative%2Ftask.rs", "raw_url": "https://github.com/rust-lang/rust/raw/d42521aa92006a3378c535adec80ae2257bff083/src%2Flibnative%2Ftask.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibnative%2Ftask.rs?ref=d42521aa92006a3378c535adec80ae2257bff083", "patch": "@@ -143,6 +143,8 @@ impl rt::Runtime for Ops {\n \n     fn stack_bounds(&self) -> (uint, uint) { self.stack_bounds }\n \n+    fn can_block(&self) -> bool { true }\n+\n     // This function gets a little interesting. There are a few safety and\n     // ownership violations going on here, but this is all done in the name of\n     // shared state. Additionally, all of the violations are protected with a\n@@ -231,7 +233,7 @@ impl rt::Runtime for Ops {\n \n     // See the comments on `deschedule` for why the task is forgotten here, and\n     // why it's valid to do so.\n-    fn reawaken(mut ~self, mut to_wake: ~Task, _can_resched: bool) {\n+    fn reawaken(mut ~self, mut to_wake: ~Task) {\n         unsafe {\n             let me = &mut *self as *mut Ops;\n             to_wake.put_runtime(self as ~rt::Runtime);"}, {"sha": "e224a06818af18f444914251bd1ec021d80645c6", "filename": "src/librustc/back/link.rs", "status": "modified", "additions": 21, "deletions": 3, "changes": 24, "blob_url": "https://github.com/rust-lang/rust/blob/d42521aa92006a3378c535adec80ae2257bff083/src%2Flibrustc%2Fback%2Flink.rs", "raw_url": "https://github.com/rust-lang/rust/raw/d42521aa92006a3378c535adec80ae2257bff083/src%2Flibrustc%2Fback%2Flink.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Fback%2Flink.rs?ref=d42521aa92006a3378c535adec80ae2257bff083", "patch": "@@ -96,15 +96,33 @@ pub mod write {\n     use lib::llvm::llvm;\n     use lib::llvm::{ModuleRef, TargetMachineRef, PassManagerRef};\n     use lib;\n-    use syntax::abi;\n     use util::common::time;\n+    use syntax::abi;\n \n     use std::c_str::ToCStr;\n     use std::libc::{c_uint, c_int};\n     use std::path::Path;\n     use std::run;\n     use std::str;\n \n+    // On android, we by default compile for armv7 processors. This enables\n+    // things like double word CAS instructions (rather than emulating them)\n+    // which are *far* more efficient. This is obviously undesirable in some\n+    // cases, so if any sort of target feature is specified we don't append v7\n+    // to the feature list.\n+    fn target_feature<'a>(sess: &'a Session) -> &'a str {\n+        match sess.targ_cfg.os {\n+            abi::OsAndroid => {\n+                if \"\" == sess.opts.target_feature {\n+                    \"+v7\"\n+                } else {\n+                    sess.opts.target_feature.as_slice()\n+                }\n+            }\n+            _ => sess.opts.target_feature.as_slice()\n+        }\n+    }\n+\n     pub fn run_passes(sess: Session,\n                       trans: &CrateTranslation,\n                       output_type: OutputType,\n@@ -136,7 +154,7 @@ pub mod write {\n \n             let tm = sess.targ_cfg.target_strs.target_triple.with_c_str(|T| {\n                 sess.opts.target_cpu.with_c_str(|CPU| {\n-                    sess.opts.target_feature.with_c_str(|Features| {\n+                    target_feature(&sess).with_c_str(|Features| {\n                         llvm::LLVMRustCreateTargetMachine(\n                             T, CPU, Features,\n                             lib::llvm::CodeModelDefault,\n@@ -313,7 +331,7 @@ pub mod write {\n     }\n \n     unsafe fn configure_llvm(sess: Session) {\n-        use std::unstable::mutex::{Once, ONCE_INIT};\n+        use extra::sync::one::{Once, ONCE_INIT};\n         static mut INIT: Once = ONCE_INIT;\n \n         // Copy what clang does by turning on loop vectorization at O2 and"}, {"sha": "1ebe4a03cfdd2a5cef44df6175ec81c10462365a", "filename": "src/librustc/middle/trans/base.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/d42521aa92006a3378c535adec80ae2257bff083/src%2Flibrustc%2Fmiddle%2Ftrans%2Fbase.rs", "raw_url": "https://github.com/rust-lang/rust/raw/d42521aa92006a3378c535adec80ae2257bff083/src%2Flibrustc%2Fmiddle%2Ftrans%2Fbase.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Fmiddle%2Ftrans%2Fbase.rs?ref=d42521aa92006a3378c535adec80ae2257bff083", "patch": "@@ -2660,7 +2660,7 @@ pub fn trans_crate(sess: session::Session,\n                    output: &Path) -> CrateTranslation {\n     // Before we touch LLVM, make sure that multithreading is enabled.\n     unsafe {\n-        use std::unstable::mutex::{Once, ONCE_INIT};\n+        use extra::sync::one::{Once, ONCE_INIT};\n         static mut INIT: Once = ONCE_INIT;\n         static mut POISONED: bool = false;\n         INIT.doit(|| {"}, {"sha": "cedd98e261cc7a1c207b7ebf855ce9e74d297af0", "filename": "src/librustc/middle/trans/builder.rs", "status": "modified", "additions": 7, "deletions": 4, "changes": 11, "blob_url": "https://github.com/rust-lang/rust/blob/d42521aa92006a3378c535adec80ae2257bff083/src%2Flibrustc%2Fmiddle%2Ftrans%2Fbuilder.rs", "raw_url": "https://github.com/rust-lang/rust/raw/d42521aa92006a3378c535adec80ae2257bff083/src%2Flibrustc%2Fmiddle%2Ftrans%2Fbuilder.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Fmiddle%2Ftrans%2Fbuilder.rs?ref=d42521aa92006a3378c535adec80ae2257bff083", "patch": "@@ -15,7 +15,7 @@ use lib::llvm::{Opcode, IntPredicate, RealPredicate, False};\n use lib::llvm::{ValueRef, BasicBlockRef, BuilderRef, ModuleRef};\n use middle::trans::base;\n use middle::trans::common::*;\n-use middle::trans::machine::llalign_of_min;\n+use middle::trans::machine::llalign_of_pref;\n use middle::trans::type_::Type;\n use std::cast;\n use std::hashmap::HashMap;\n@@ -461,8 +461,10 @@ impl Builder {\n     pub fn atomic_load(&self, ptr: ValueRef, order: AtomicOrdering) -> ValueRef {\n         self.count_insn(\"load.atomic\");\n         unsafe {\n-            let align = llalign_of_min(self.ccx, self.ccx.int_type);\n-            llvm::LLVMBuildAtomicLoad(self.llbuilder, ptr, noname(), order, align as c_uint)\n+            let ty = Type::from_ref(llvm::LLVMTypeOf(ptr));\n+            let align = llalign_of_pref(self.ccx, ty.element_type());\n+            llvm::LLVMBuildAtomicLoad(self.llbuilder, ptr, noname(), order,\n+                                      align as c_uint)\n         }\n     }\n \n@@ -514,8 +516,9 @@ impl Builder {\n                self.ccx.tn.val_to_str(val),\n                self.ccx.tn.val_to_str(ptr));\n         self.count_insn(\"store.atomic\");\n-        let align = llalign_of_min(self.ccx, self.ccx.int_type);\n         unsafe {\n+            let ty = Type::from_ref(llvm::LLVMTypeOf(ptr));\n+            let align = llalign_of_pref(self.ccx, ty.element_type());\n             llvm::LLVMBuildAtomicStore(self.llbuilder, val, ptr, order, align as c_uint);\n         }\n     }"}, {"sha": "e1679c81a0e2db6c682337587d2b9927c53e864b", "filename": "src/librustc/middle/typeck/check/mod.rs", "status": "modified", "additions": 17, "deletions": 24, "changes": 41, "blob_url": "https://github.com/rust-lang/rust/blob/d42521aa92006a3378c535adec80ae2257bff083/src%2Flibrustc%2Fmiddle%2Ftypeck%2Fcheck%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/d42521aa92006a3378c535adec80ae2257bff083/src%2Flibrustc%2Fmiddle%2Ftypeck%2Fcheck%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Fmiddle%2Ftypeck%2Fcheck%2Fmod.rs?ref=d42521aa92006a3378c535adec80ae2257bff083", "patch": "@@ -4030,29 +4030,32 @@ pub fn check_intrinsic_type(ccx: @CrateCtxt, it: &ast::ForeignItem) {\n \n         //We only care about the operation here\n         match split[1] {\n-            \"cxchg\" => (0, ~[ty::mk_mut_rptr(tcx,\n+            \"cxchg\" => (1, ~[ty::mk_mut_rptr(tcx,\n                                              ty::ReLateBound(it.id, ty::BrAnon(0)),\n-                                             ty::mk_int()),\n-                        ty::mk_int(),\n-                        ty::mk_int()\n-                        ], ty::mk_int()),\n-            \"load\" => (0,\n+                                             param(ccx, 0)),\n+                        param(ccx, 0),\n+                        param(ccx, 0),\n+                        ], param(ccx, 0)),\n+            \"load\" => (1,\n                ~[\n-                  ty::mk_imm_rptr(tcx, ty::ReLateBound(it.id, ty::BrAnon(0)), ty::mk_int())\n+                  ty::mk_imm_rptr(tcx, ty::ReLateBound(it.id, ty::BrAnon(0)),\n+                                  param(ccx, 0))\n                ],\n-              ty::mk_int()),\n-            \"store\" => (0,\n+              param(ccx, 0)),\n+            \"store\" => (1,\n                ~[\n-                  ty::mk_mut_rptr(tcx, ty::ReLateBound(it.id, ty::BrAnon(0)), ty::mk_int()),\n-                  ty::mk_int()\n+                  ty::mk_mut_rptr(tcx, ty::ReLateBound(it.id, ty::BrAnon(0)),\n+                                  param(ccx, 0)),\n+                  param(ccx, 0)\n                ],\n                ty::mk_nil()),\n \n-            \"xchg\" | \"xadd\" | \"xsub\" | \"and\"  | \"nand\" | \"or\"   | \"xor\"  | \"max\"  |\n+            \"xchg\" | \"xadd\" | \"xsub\" | \"and\"  | \"nand\" | \"or\" | \"xor\" | \"max\" |\n             \"min\"  | \"umax\" | \"umin\" => {\n-                (0, ~[ty::mk_mut_rptr(tcx,\n+                (1, ~[ty::mk_mut_rptr(tcx,\n                                       ty::ReLateBound(it.id, ty::BrAnon(0)),\n-                                      ty::mk_int()), ty::mk_int() ], ty::mk_int())\n+                                      param(ccx, 0)), param(ccx, 0) ],\n+                 param(ccx, 0))\n             }\n             \"fence\" => {\n                 (0, ~[], ty::mk_nil())\n@@ -4085,16 +4088,6 @@ pub fn check_intrinsic_type(ccx: @CrateCtxt, it: &ast::ForeignItem) {\n             }\n             \"needs_drop\" => (1u, ~[], ty::mk_bool()),\n             \"owns_managed\" => (1u, ~[], ty::mk_bool()),\n-            \"atomic_xchg\"     | \"atomic_xadd\"     | \"atomic_xsub\"     |\n-            \"atomic_xchg_acq\" | \"atomic_xadd_acq\" | \"atomic_xsub_acq\" |\n-            \"atomic_xchg_rel\" | \"atomic_xadd_rel\" | \"atomic_xsub_rel\" => {\n-              (0,\n-               ~[\n-                  ty::mk_mut_rptr(tcx, ty::ReLateBound(it.id, ty::BrAnon(0)), ty::mk_int()),\n-                  ty::mk_int()\n-               ],\n-               ty::mk_int())\n-            }\n \n             \"get_tydesc\" => {\n               let tydesc_ty = match ty::get_tydesc_ty(ccx.tcx) {"}, {"sha": "30e02b168ca57e8af4e7f5da7453cd6e3dc81d42", "filename": "src/librustuv/idle.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/d42521aa92006a3378c535adec80ae2257bff083/src%2Flibrustuv%2Fidle.rs", "raw_url": "https://github.com/rust-lang/rust/raw/d42521aa92006a3378c535adec80ae2257bff083/src%2Flibrustuv%2Fidle.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustuv%2Fidle.rs?ref=d42521aa92006a3378c535adec80ae2257bff083", "patch": "@@ -122,7 +122,7 @@ mod test {\n                     }\n                 }\n             };\n-            let _ = task.wake().map(|t| t.reawaken(true));\n+            let _ = task.wake().map(|t| t.reawaken());\n         }\n     }\n "}, {"sha": "39d6f851e1722546867365a4ff5e3dc6a5609396", "filename": "src/librustuv/lib.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/d42521aa92006a3378c535adec80ae2257bff083/src%2Flibrustuv%2Flib.rs", "raw_url": "https://github.com/rust-lang/rust/raw/d42521aa92006a3378c535adec80ae2257bff083/src%2Flibrustuv%2Flib.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustuv%2Flib.rs?ref=d42521aa92006a3378c535adec80ae2257bff083", "patch": "@@ -208,7 +208,7 @@ fn wait_until_woken_after(slot: *mut Option<BlockedTask>, f: ||) {\n \n fn wakeup(slot: &mut Option<BlockedTask>) {\n     assert!(slot.is_some());\n-    let _ = slot.take_unwrap().wake().map(|t| t.reawaken(true));\n+    let _ = slot.take_unwrap().wake().map(|t| t.reawaken());\n }\n \n pub struct Request {"}, {"sha": "358582d436b32ec3c162ee7768adcea81ed19adc", "filename": "src/librustuv/queue.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/d42521aa92006a3378c535adec80ae2257bff083/src%2Flibrustuv%2Fqueue.rs", "raw_url": "https://github.com/rust-lang/rust/raw/d42521aa92006a3378c535adec80ae2257bff083/src%2Flibrustuv%2Fqueue.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustuv%2Fqueue.rs?ref=d42521aa92006a3378c535adec80ae2257bff083", "patch": "@@ -67,7 +67,7 @@ extern fn async_cb(handle: *uvll::uv_async_t, status: c_int) {\n     loop {\n         match state.consumer.pop() {\n             mpsc::Data(Task(task)) => {\n-                let _ = task.wake().map(|t| t.reawaken(true));\n+                let _ = task.wake().map(|t| t.reawaken());\n             }\n             mpsc::Data(Increment) => unsafe {\n                 if state.refcnt == 0 {"}, {"sha": "0363cab247d1536081a67660a9a11e364beaaeec", "filename": "src/librustuv/timer.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/d42521aa92006a3378c535adec80ae2257bff083/src%2Flibrustuv%2Ftimer.rs", "raw_url": "https://github.com/rust-lang/rust/raw/d42521aa92006a3378c535adec80ae2257bff083/src%2Flibrustuv%2Ftimer.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustuv%2Ftimer.rs?ref=d42521aa92006a3378c535adec80ae2257bff083", "patch": "@@ -138,7 +138,7 @@ extern fn timer_cb(handle: *uvll::uv_timer_t, status: c_int) {\n \n     match timer.action.take_unwrap() {\n         WakeTask(task) => {\n-            let _ = task.wake().map(|t| t.reawaken(true));\n+            let _ = task.wake().map(|t| t.reawaken());\n         }\n         SendOnce(chan) => { let _ = chan.try_send(()); }\n         SendMany(chan, id) => {"}, {"sha": "366c268fae2b95094beab927cf02ca7a11403589", "filename": "src/libstd/comm/mod.rs", "status": "modified", "additions": 7, "deletions": 13, "changes": 20, "blob_url": "https://github.com/rust-lang/rust/blob/d42521aa92006a3378c535adec80ae2257bff083/src%2Flibstd%2Fcomm%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/d42521aa92006a3378c535adec80ae2257bff083/src%2Flibstd%2Fcomm%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibstd%2Fcomm%2Fmod.rs?ref=d42521aa92006a3378c535adec80ae2257bff083", "patch": "@@ -443,9 +443,9 @@ impl Packet {\n \n     // This function must have had at least an acquire fence before it to be\n     // properly called.\n-    fn wakeup(&mut self, can_resched: bool) {\n+    fn wakeup(&mut self) {\n         match self.to_wake.take_unwrap().wake() {\n-            Some(task) => task.reawaken(can_resched),\n+            Some(task) => task.reawaken(),\n             None => {}\n         }\n         self.selecting.store(false, Relaxed);\n@@ -519,7 +519,7 @@ impl Packet {\n         match self.channels.fetch_sub(1, SeqCst) {\n             1 => {\n                 match self.cnt.swap(DISCONNECTED, SeqCst) {\n-                    -1 => { self.wakeup(true); }\n+                    -1 => { self.wakeup(); }\n                     DISCONNECTED => {}\n                     n => { assert!(n >= 0); }\n                 }\n@@ -595,20 +595,14 @@ impl<T: Send> Chan<T> {\n     ///\n     /// Like `send`, this method will never block. If the failure of send cannot\n     /// be tolerated, then this method should be used instead.\n-    pub fn try_send(&self, t: T) -> bool { self.try(t, true) }\n-\n-    /// This function will not stick around for very long. The purpose of this\n-    /// function is to guarantee that no rescheduling is performed.\n-    pub fn try_send_deferred(&self, t: T) -> bool { self.try(t, false) }\n-\n-    fn try(&self, t: T, can_resched: bool) -> bool {\n+    pub fn try_send(&self, t: T) -> bool {\n         unsafe {\n             let this = cast::transmute_mut(self);\n             this.queue.push(t);\n             let packet = this.queue.packet();\n             match (*packet).increment() {\n                 // As described above, -1 == wakeup\n-                -1 => { (*packet).wakeup(can_resched); true }\n+                -1 => { (*packet).wakeup(); true }\n                 // Also as above, SPSC queues must be >= -2\n                 -2 => true,\n                 // We succeeded if we sent data\n@@ -623,7 +617,7 @@ impl<T: Send> Chan<T> {\n                 // the TLS overhead can be a bit much.\n                 n => {\n                     assert!(n >= 0);\n-                    if can_resched && n > 0 && n % RESCHED_FREQ == 0 {\n+                    if n > 0 && n % RESCHED_FREQ == 0 {\n                         let task: ~Task = Local::take();\n                         task.maybe_yield();\n                     }\n@@ -700,7 +694,7 @@ impl<T: Send> SharedChan<T> {\n \n             match (*packet).increment() {\n                 DISCONNECTED => {} // oh well, we tried\n-                -1 => { (*packet).wakeup(true); }\n+                -1 => { (*packet).wakeup(); }\n                 n => {\n                     if n > 0 && n % RESCHED_FREQ == 0 {\n                         let task: ~Task = Local::take();"}, {"sha": "55425eb2e7226d21f410910c9f8cb31309f5a2a1", "filename": "src/libstd/rt/mod.rs", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "blob_url": "https://github.com/rust-lang/rust/blob/d42521aa92006a3378c535adec80ae2257bff083/src%2Flibstd%2Frt%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/d42521aa92006a3378c535adec80ae2257bff083/src%2Flibstd%2Frt%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibstd%2Frt%2Fmod.rs?ref=d42521aa92006a3378c535adec80ae2257bff083", "patch": "@@ -146,14 +146,15 @@ pub trait Runtime {\n     fn maybe_yield(~self, cur_task: ~Task);\n     fn deschedule(~self, times: uint, cur_task: ~Task,\n                   f: |BlockedTask| -> Result<(), BlockedTask>);\n-    fn reawaken(~self, to_wake: ~Task, can_resched: bool);\n+    fn reawaken(~self, to_wake: ~Task);\n \n     // Miscellaneous calls which are very different depending on what context\n     // you're in.\n     fn spawn_sibling(~self, cur_task: ~Task, opts: TaskOpts, f: proc());\n     fn local_io<'a>(&'a mut self) -> Option<rtio::LocalIo<'a>>;\n     /// The (low, high) edges of the current stack.\n     fn stack_bounds(&self) -> (uint, uint); // (lo, hi)\n+    fn can_block(&self) -> bool;\n \n     // FIXME: This is a serious code smell and this should not exist at all.\n     fn wrap(~self) -> ~Any;"}, {"sha": "fbe82531f69778f48add0ca2860eba8e660dda5d", "filename": "src/libstd/rt/task.rs", "status": "modified", "additions": 8, "deletions": 2, "changes": 10, "blob_url": "https://github.com/rust-lang/rust/blob/d42521aa92006a3378c535adec80ae2257bff083/src%2Flibstd%2Frt%2Ftask.rs", "raw_url": "https://github.com/rust-lang/rust/raw/d42521aa92006a3378c535adec80ae2257bff083/src%2Flibstd%2Frt%2Ftask.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibstd%2Frt%2Ftask.rs?ref=d42521aa92006a3378c535adec80ae2257bff083", "patch": "@@ -250,9 +250,9 @@ impl Task {\n     /// Wakes up a previously blocked task, optionally specifiying whether the\n     /// current task can accept a change in scheduling. This function can only\n     /// be called on tasks that were previously blocked in `deschedule`.\n-    pub fn reawaken(mut ~self, can_resched: bool) {\n+    pub fn reawaken(mut ~self) {\n         let ops = self.imp.take_unwrap();\n-        ops.reawaken(self, can_resched);\n+        ops.reawaken(self);\n     }\n \n     /// Yields control of this task to another task. This function will\n@@ -283,6 +283,12 @@ impl Task {\n     pub fn stack_bounds(&self) -> (uint, uint) {\n         self.imp.get_ref().stack_bounds()\n     }\n+\n+    /// Returns whether it is legal for this task to block the OS thread that it\n+    /// is running on.\n+    pub fn can_block(&self) -> bool {\n+        self.imp.get_ref().can_block()\n+    }\n }\n \n impl Drop for Task {"}, {"sha": "fb62bed9ed0ae674fe88cba64d9381b7036b089d", "filename": "src/libstd/sync/atomics.rs", "status": "modified", "additions": 245, "deletions": 32, "changes": 277, "blob_url": "https://github.com/rust-lang/rust/blob/d42521aa92006a3378c535adec80ae2257bff083/src%2Flibstd%2Fsync%2Fatomics.rs", "raw_url": "https://github.com/rust-lang/rust/raw/d42521aa92006a3378c535adec80ae2257bff083/src%2Flibstd%2Fsync%2Fatomics.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibstd%2Fsync%2Fatomics.rs?ref=d42521aa92006a3378c535adec80ae2257bff083", "patch": "@@ -59,9 +59,25 @@ pub struct AtomicUint {\n     priv nocopy: NonCopyable\n }\n \n+/**\n+ * An unsigned atomic integer type that is forced to be 64-bits. This does not\n+ * support all operations.\n+ */\n+#[cfg(not(stage0))]\n+pub struct AtomicU64 {\n+    priv v: u64,\n+    priv nocopy: NonCopyable\n+}\n+\n /**\n  * An unsafe atomic pointer. Only supports basic atomic operations\n  */\n+#[cfg(not(stage0))]\n+pub struct AtomicPtr<T> {\n+    priv p: uint,\n+    priv nocopy: NonCopyable\n+}\n+#[cfg(stage0)]\n pub struct AtomicPtr<T> {\n     priv p: *mut T,\n     priv nocopy: NonCopyable\n@@ -71,6 +87,12 @@ pub struct AtomicPtr<T> {\n  * An owned atomic pointer. Ensures that only a single reference to the data is held at any time.\n  */\n #[unsafe_no_drop_flag]\n+#[cfg(not(stage0))]\n+pub struct AtomicOption<T> {\n+    priv p: uint,\n+}\n+#[unsafe_no_drop_flag]\n+#[cfg(stage0)]\n pub struct AtomicOption<T> {\n     priv p: *mut u8\n }\n@@ -87,6 +109,8 @@ pub static INIT_ATOMIC_FLAG : AtomicFlag = AtomicFlag { v: 0, nocopy: NonCopyabl\n pub static INIT_ATOMIC_BOOL : AtomicBool = AtomicBool { v: 0, nocopy: NonCopyable };\n pub static INIT_ATOMIC_INT  : AtomicInt  = AtomicInt  { v: 0, nocopy: NonCopyable };\n pub static INIT_ATOMIC_UINT : AtomicUint = AtomicUint { v: 0, nocopy: NonCopyable };\n+#[cfg(not(stage0))]\n+pub static INIT_ATOMIC_U64 : AtomicU64 = AtomicU64 { v: 0, nocopy: NonCopyable };\n \n impl AtomicFlag {\n \n@@ -215,6 +239,43 @@ impl AtomicInt {\n     }\n }\n \n+#[cfg(not(stage0))]\n+impl AtomicU64 {\n+    pub fn new(v: u64) -> AtomicU64 {\n+        AtomicU64 { v:v, nocopy: NonCopyable }\n+    }\n+\n+    #[inline]\n+    pub fn load(&self, order: Ordering) -> u64 {\n+        unsafe { atomic_load(&self.v, order) }\n+    }\n+\n+    #[inline]\n+    pub fn store(&mut self, val: u64, order: Ordering) {\n+        unsafe { atomic_store(&mut self.v, val, order); }\n+    }\n+\n+    #[inline]\n+    pub fn swap(&mut self, val: u64, order: Ordering) -> u64 {\n+        unsafe { atomic_swap(&mut self.v, val, order) }\n+    }\n+\n+    #[inline]\n+    pub fn compare_and_swap(&mut self, old: u64, new: u64, order: Ordering) -> u64 {\n+        unsafe { atomic_compare_and_swap(&mut self.v, old, new, order) }\n+    }\n+\n+    #[inline]\n+    pub fn fetch_add(&mut self, val: u64, order: Ordering) -> u64 {\n+        unsafe { atomic_add(&mut self.v, val, order) }\n+    }\n+\n+    #[inline]\n+    pub fn fetch_sub(&mut self, val: u64, order: Ordering) -> u64 {\n+        unsafe { atomic_sub(&mut self.v, val, order) }\n+    }\n+}\n+\n impl AtomicUint {\n     pub fn new(v: uint) -> AtomicUint {\n         AtomicUint { v:v, nocopy: NonCopyable }\n@@ -254,57 +315,86 @@ impl AtomicUint {\n }\n \n impl<T> AtomicPtr<T> {\n+    #[cfg(stage0)]\n+    pub fn new(p: *mut T) -> AtomicPtr<T> {\n+        AtomicPtr { p: p, nocopy: NonCopyable }\n+    }\n+    #[cfg(not(stage0))]\n     pub fn new(p: *mut T) -> AtomicPtr<T> {\n-        AtomicPtr { p:p, nocopy: NonCopyable }\n+        AtomicPtr { p: p as uint, nocopy: NonCopyable }\n     }\n \n     #[inline]\n+    #[cfg(not(stage0))]\n+    pub fn load(&self, order: Ordering) -> *mut T {\n+        unsafe {\n+            atomic_load(&self.p, order) as *mut T\n+        }\n+    }\n+\n+    #[inline]\n+    #[cfg(not(stage0))]\n+    pub fn store(&mut self, ptr: *mut T, order: Ordering) {\n+        unsafe { atomic_store(&mut self.p, ptr as uint, order); }\n+    }\n+\n+    #[inline]\n+    #[cfg(not(stage0))]\n+    pub fn swap(&mut self, ptr: *mut T, order: Ordering) -> *mut T {\n+        unsafe { atomic_swap(&mut self.p, ptr as uint, order) as *mut T }\n+    }\n+\n+    #[inline]\n+    #[cfg(not(stage0))]\n+    pub fn compare_and_swap(&mut self, old: *mut T, new: *mut T, order: Ordering) -> *mut T {\n+        unsafe {\n+            atomic_compare_and_swap(&mut self.p, old as uint,\n+                                    new as uint, order) as *mut T\n+        }\n+    }\n+\n+    #[inline]\n+    #[cfg(stage0)]\n     pub fn load(&self, order: Ordering) -> *mut T {\n         unsafe { atomic_load(&self.p, order) }\n     }\n \n     #[inline]\n+    #[cfg(stage0)]\n     pub fn store(&mut self, ptr: *mut T, order: Ordering) {\n         unsafe { atomic_store(&mut self.p, ptr, order); }\n     }\n \n     #[inline]\n+    #[cfg(stage0)]\n     pub fn swap(&mut self, ptr: *mut T, order: Ordering) -> *mut T {\n         unsafe { atomic_swap(&mut self.p, ptr, order) }\n     }\n \n     #[inline]\n+    #[cfg(stage0)]\n     pub fn compare_and_swap(&mut self, old: *mut T, new: *mut T, order: Ordering) -> *mut T {\n         unsafe { atomic_compare_and_swap(&mut self.p, old, new, order) }\n     }\n }\n \n impl<T> AtomicOption<T> {\n     pub fn new(p: ~T) -> AtomicOption<T> {\n-        unsafe {\n-            AtomicOption {\n-                p: cast::transmute(p)\n-            }\n-        }\n+        unsafe { AtomicOption { p: cast::transmute(p) } }\n     }\n \n-    pub fn empty() -> AtomicOption<T> {\n-        unsafe {\n-            AtomicOption {\n-                p: cast::transmute(0)\n-            }\n-        }\n-    }\n+    #[cfg(stage0)]\n+    pub fn empty() -> AtomicOption<T> { AtomicOption { p: 0 as *mut u8 } }\n+    #[cfg(not(stage0))]\n+    pub fn empty() -> AtomicOption<T> { AtomicOption { p: 0 } }\n \n     #[inline]\n     pub fn swap(&mut self, val: ~T, order: Ordering) -> Option<~T> {\n         unsafe {\n             let val = cast::transmute(val);\n \n             let p = atomic_swap(&mut self.p, val, order);\n-            let pv : &uint = cast::transmute(&p);\n-\n-            if *pv == 0 {\n+            if p as uint == 0 {\n                 None\n             } else {\n                 Some(cast::transmute(p))\n@@ -314,9 +404,7 @@ impl<T> AtomicOption<T> {\n \n     #[inline]\n     pub fn take(&mut self, order: Ordering) -> Option<~T> {\n-        unsafe {\n-            self.swap(cast::transmute(0), order)\n-        }\n+        unsafe { self.swap(cast::transmute(0), order) }\n     }\n \n     /// A compare-and-swap. Succeeds if the option is 'None' and returns 'None'\n@@ -340,7 +428,7 @@ impl<T> AtomicOption<T> {\n     /// result does not get invalidated by another task after this returns.\n     #[inline]\n     pub fn is_empty(&mut self, order: Ordering) -> bool {\n-        unsafe { atomic_load(&self.p, order) == cast::transmute(0) }\n+        unsafe { atomic_load(&self.p, order) as uint == 0 }\n     }\n }\n \n@@ -351,34 +439,52 @@ impl<T> Drop for AtomicOption<T> {\n     }\n }\n \n+#[cfg(stage0)]\n #[inline]\n pub unsafe fn atomic_store<T>(dst: &mut T, val: T, order:Ordering) {\n     let dst = cast::transmute(dst);\n     let val = cast::transmute(val);\n-\n+    cast::transmute(match order {\n+        Release => intrinsics::atomic_store_rel(dst, val),\n+        Relaxed => intrinsics::atomic_store_relaxed(dst, val),\n+        _       => intrinsics::atomic_store(dst, val)\n+    })\n+}\n+#[cfg(not(stage0))]\n+#[inline]\n+pub unsafe fn atomic_store<T>(dst: &mut T, val: T, order:Ordering) {\n     match order {\n         Release => intrinsics::atomic_store_rel(dst, val),\n         Relaxed => intrinsics::atomic_store_relaxed(dst, val),\n         _       => intrinsics::atomic_store(dst, val)\n     }\n }\n \n+#[cfg(stage0)]\n #[inline]\n pub unsafe fn atomic_load<T>(dst: &T, order:Ordering) -> T {\n     let dst = cast::transmute(dst);\n-\n     cast::transmute(match order {\n         Acquire => intrinsics::atomic_load_acq(dst),\n         Relaxed => intrinsics::atomic_load_relaxed(dst),\n         _       => intrinsics::atomic_load(dst)\n     })\n }\n+#[cfg(not(stage0))]\n+#[inline]\n+pub unsafe fn atomic_load<T>(dst: &T, order:Ordering) -> T {\n+    match order {\n+        Acquire => intrinsics::atomic_load_acq(dst),\n+        Relaxed => intrinsics::atomic_load_relaxed(dst),\n+        _       => intrinsics::atomic_load(dst)\n+    }\n+}\n \n+#[cfg(stage0)]\n #[inline]\n pub unsafe fn atomic_swap<T>(dst: &mut T, val: T, order: Ordering) -> T {\n     let dst = cast::transmute(dst);\n     let val = cast::transmute(val);\n-\n     cast::transmute(match order {\n         Acquire => intrinsics::atomic_xchg_acq(dst, val),\n         Release => intrinsics::atomic_xchg_rel(dst, val),\n@@ -387,13 +493,24 @@ pub unsafe fn atomic_swap<T>(dst: &mut T, val: T, order: Ordering) -> T {\n         _       => intrinsics::atomic_xchg(dst, val)\n     })\n }\n+#[cfg(not(stage0))]\n+#[inline]\n+pub unsafe fn atomic_swap<T>(dst: &mut T, val: T, order: Ordering) -> T {\n+    match order {\n+        Acquire => intrinsics::atomic_xchg_acq(dst, val),\n+        Release => intrinsics::atomic_xchg_rel(dst, val),\n+        AcqRel  => intrinsics::atomic_xchg_acqrel(dst, val),\n+        Relaxed => intrinsics::atomic_xchg_relaxed(dst, val),\n+        _       => intrinsics::atomic_xchg(dst, val)\n+    }\n+}\n \n /// Returns the old value (like __sync_fetch_and_add).\n+#[cfg(stage0)]\n #[inline]\n pub unsafe fn atomic_add<T>(dst: &mut T, val: T, order: Ordering) -> T {\n     let dst = cast::transmute(dst);\n     let val = cast::transmute(val);\n-\n     cast::transmute(match order {\n         Acquire => intrinsics::atomic_xadd_acq(dst, val),\n         Release => intrinsics::atomic_xadd_rel(dst, val),\n@@ -402,13 +519,25 @@ pub unsafe fn atomic_add<T>(dst: &mut T, val: T, order: Ordering) -> T {\n         _       => intrinsics::atomic_xadd(dst, val)\n     })\n }\n+/// Returns the old value (like __sync_fetch_and_add).\n+#[cfg(not(stage0))]\n+#[inline]\n+pub unsafe fn atomic_add<T>(dst: &mut T, val: T, order: Ordering) -> T {\n+    match order {\n+        Acquire => intrinsics::atomic_xadd_acq(dst, val),\n+        Release => intrinsics::atomic_xadd_rel(dst, val),\n+        AcqRel  => intrinsics::atomic_xadd_acqrel(dst, val),\n+        Relaxed => intrinsics::atomic_xadd_relaxed(dst, val),\n+        _       => intrinsics::atomic_xadd(dst, val)\n+    }\n+}\n \n /// Returns the old value (like __sync_fetch_and_sub).\n+#[cfg(stage0)]\n #[inline]\n pub unsafe fn atomic_sub<T>(dst: &mut T, val: T, order: Ordering) -> T {\n     let dst = cast::transmute(dst);\n     let val = cast::transmute(val);\n-\n     cast::transmute(match order {\n         Acquire => intrinsics::atomic_xsub_acq(dst, val),\n         Release => intrinsics::atomic_xsub_rel(dst, val),\n@@ -417,13 +546,25 @@ pub unsafe fn atomic_sub<T>(dst: &mut T, val: T, order: Ordering) -> T {\n         _       => intrinsics::atomic_xsub(dst, val)\n     })\n }\n+/// Returns the old value (like __sync_fetch_and_sub).\n+#[cfg(not(stage0))]\n+#[inline]\n+pub unsafe fn atomic_sub<T>(dst: &mut T, val: T, order: Ordering) -> T {\n+    match order {\n+        Acquire => intrinsics::atomic_xsub_acq(dst, val),\n+        Release => intrinsics::atomic_xsub_rel(dst, val),\n+        AcqRel  => intrinsics::atomic_xsub_acqrel(dst, val),\n+        Relaxed => intrinsics::atomic_xsub_relaxed(dst, val),\n+        _       => intrinsics::atomic_xsub(dst, val)\n+    }\n+}\n \n+#[cfg(stage0)]\n #[inline]\n pub unsafe fn atomic_compare_and_swap<T>(dst:&mut T, old:T, new:T, order: Ordering) -> T {\n     let dst = cast::transmute(dst);\n-    let old = cast::transmute(old);\n     let new = cast::transmute(new);\n-\n+    let old = cast::transmute(old);\n     cast::transmute(match order {\n         Acquire => intrinsics::atomic_cxchg_acq(dst, old, new),\n         Release => intrinsics::atomic_cxchg_rel(dst, old, new),\n@@ -432,12 +573,23 @@ pub unsafe fn atomic_compare_and_swap<T>(dst:&mut T, old:T, new:T, order: Orderi\n         _       => intrinsics::atomic_cxchg(dst, old, new),\n     })\n }\n+#[cfg(not(stage0))]\n+#[inline]\n+pub unsafe fn atomic_compare_and_swap<T>(dst:&mut T, old:T, new:T, order: Ordering) -> T {\n+    match order {\n+        Acquire => intrinsics::atomic_cxchg_acq(dst, old, new),\n+        Release => intrinsics::atomic_cxchg_rel(dst, old, new),\n+        AcqRel  => intrinsics::atomic_cxchg_acqrel(dst, old, new),\n+        Relaxed => intrinsics::atomic_cxchg_relaxed(dst, old, new),\n+        _       => intrinsics::atomic_cxchg(dst, old, new),\n+    }\n+}\n \n+#[cfg(stage0)]\n #[inline]\n pub unsafe fn atomic_and<T>(dst: &mut T, val: T, order: Ordering) -> T {\n     let dst = cast::transmute(dst);\n     let val = cast::transmute(val);\n-\n     cast::transmute(match order {\n         Acquire => intrinsics::atomic_and_acq(dst, val),\n         Release => intrinsics::atomic_and_rel(dst, val),\n@@ -446,13 +598,23 @@ pub unsafe fn atomic_and<T>(dst: &mut T, val: T, order: Ordering) -> T {\n         _       => intrinsics::atomic_and(dst, val)\n     })\n }\n+#[cfg(not(stage0))]\n+#[inline]\n+pub unsafe fn atomic_and<T>(dst: &mut T, val: T, order: Ordering) -> T {\n+    match order {\n+        Acquire => intrinsics::atomic_and_acq(dst, val),\n+        Release => intrinsics::atomic_and_rel(dst, val),\n+        AcqRel  => intrinsics::atomic_and_acqrel(dst, val),\n+        Relaxed => intrinsics::atomic_and_relaxed(dst, val),\n+        _       => intrinsics::atomic_and(dst, val)\n+    }\n+}\n \n-\n+#[cfg(stage0)]\n #[inline]\n pub unsafe fn atomic_nand<T>(dst: &mut T, val: T, order: Ordering) -> T {\n     let dst = cast::transmute(dst);\n     let val = cast::transmute(val);\n-\n     cast::transmute(match order {\n         Acquire => intrinsics::atomic_nand_acq(dst, val),\n         Release => intrinsics::atomic_nand_rel(dst, val),\n@@ -461,13 +623,24 @@ pub unsafe fn atomic_nand<T>(dst: &mut T, val: T, order: Ordering) -> T {\n         _       => intrinsics::atomic_nand(dst, val)\n     })\n }\n+#[cfg(not(stage0))]\n+#[inline]\n+pub unsafe fn atomic_nand<T>(dst: &mut T, val: T, order: Ordering) -> T {\n+    match order {\n+        Acquire => intrinsics::atomic_nand_acq(dst, val),\n+        Release => intrinsics::atomic_nand_rel(dst, val),\n+        AcqRel  => intrinsics::atomic_nand_acqrel(dst, val),\n+        Relaxed => intrinsics::atomic_nand_relaxed(dst, val),\n+        _       => intrinsics::atomic_nand(dst, val)\n+    }\n+}\n \n \n+#[cfg(stage0)]\n #[inline]\n pub unsafe fn atomic_or<T>(dst: &mut T, val: T, order: Ordering) -> T {\n     let dst = cast::transmute(dst);\n     let val = cast::transmute(val);\n-\n     cast::transmute(match order {\n         Acquire => intrinsics::atomic_or_acq(dst, val),\n         Release => intrinsics::atomic_or_rel(dst, val),\n@@ -476,13 +649,24 @@ pub unsafe fn atomic_or<T>(dst: &mut T, val: T, order: Ordering) -> T {\n         _       => intrinsics::atomic_or(dst, val)\n     })\n }\n+#[cfg(not(stage0))]\n+#[inline]\n+pub unsafe fn atomic_or<T>(dst: &mut T, val: T, order: Ordering) -> T {\n+    match order {\n+        Acquire => intrinsics::atomic_or_acq(dst, val),\n+        Release => intrinsics::atomic_or_rel(dst, val),\n+        AcqRel  => intrinsics::atomic_or_acqrel(dst, val),\n+        Relaxed => intrinsics::atomic_or_relaxed(dst, val),\n+        _       => intrinsics::atomic_or(dst, val)\n+    }\n+}\n \n \n+#[cfg(stage0)]\n #[inline]\n pub unsafe fn atomic_xor<T>(dst: &mut T, val: T, order: Ordering) -> T {\n     let dst = cast::transmute(dst);\n     let val = cast::transmute(val);\n-\n     cast::transmute(match order {\n         Acquire => intrinsics::atomic_xor_acq(dst, val),\n         Release => intrinsics::atomic_xor_rel(dst, val),\n@@ -491,6 +675,17 @@ pub unsafe fn atomic_xor<T>(dst: &mut T, val: T, order: Ordering) -> T {\n         _       => intrinsics::atomic_xor(dst, val)\n     })\n }\n+#[cfg(not(stage0))]\n+#[inline]\n+pub unsafe fn atomic_xor<T>(dst: &mut T, val: T, order: Ordering) -> T {\n+    match order {\n+        Acquire => intrinsics::atomic_xor_acq(dst, val),\n+        Release => intrinsics::atomic_xor_rel(dst, val),\n+        AcqRel  => intrinsics::atomic_xor_acqrel(dst, val),\n+        Relaxed => intrinsics::atomic_xor_relaxed(dst, val),\n+        _       => intrinsics::atomic_xor(dst, val)\n+    }\n+}\n \n \n /**\n@@ -599,4 +794,22 @@ mod test {\n             assert!(S_UINT.load(SeqCst) == 0);\n         }\n     }\n+\n+    #[test]\n+    #[cfg(not(stage0))]\n+    fn different_sizes() {\n+        unsafe {\n+            let mut slot = 0u16;\n+            assert_eq!(super::atomic_swap(&mut slot, 1, SeqCst), 0);\n+\n+            let mut slot = 0u8;\n+            assert_eq!(super::atomic_compare_and_swap(&mut slot, 1, 2, SeqCst), 0);\n+\n+            let mut slot = 0u32;\n+            assert_eq!(super::atomic_load(&mut slot, SeqCst), 0);\n+\n+            let mut slot = 0u64;\n+            super::atomic_store(&mut slot, 2, SeqCst);\n+        }\n+    }\n }"}, {"sha": "9b3826b42a59abfb0b6ef2fee26cb54b2e3f42af", "filename": "src/libstd/unstable/intrinsics.rs", "status": "modified", "additions": 101, "deletions": 9, "changes": 110, "blob_url": "https://github.com/rust-lang/rust/blob/d42521aa92006a3378c535adec80ae2257bff083/src%2Flibstd%2Funstable%2Fintrinsics.rs", "raw_url": "https://github.com/rust-lang/rust/raw/d42521aa92006a3378c535adec80ae2257bff083/src%2Flibstd%2Funstable%2Fintrinsics.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibstd%2Funstable%2Fintrinsics.rs?ref=d42521aa92006a3378c535adec80ae2257bff083", "patch": "@@ -172,16 +172,8 @@ pub trait TyVisitor {\n     fn visit_closure_ptr(&mut self, ck: uint) -> bool;\n }\n \n+#[cfg(stage0)]\n extern \"rust-intrinsic\" {\n-    /// Abort the execution of the process.\n-    pub fn abort() -> !;\n-\n-    /// Execute a breakpoint trap, for inspection by a debugger.\n-    pub fn breakpoint();\n-\n-    pub fn volatile_load<T>(src: *T) -> T;\n-    pub fn volatile_store<T>(dst: *mut T, val: T);\n-\n     /// Atomic compare and exchange, sequentially consistent.\n     pub fn atomic_cxchg(dst: &mut int, old: int, src: int) -> int;\n     /// Atomic compare and exchange, acquire ordering.\n@@ -286,6 +278,106 @@ extern \"rust-intrinsic\" {\n     pub fn atomic_fence_acq();\n     pub fn atomic_fence_rel();\n     pub fn atomic_fence_acqrel();\n+}\n+\n+#[cfg(not(stage0))]\n+extern \"rust-intrinsic\" {\n+    pub fn atomic_cxchg<T>(dst: &mut T, old: T, src: T) -> T;\n+    pub fn atomic_cxchg_acq<T>(dst: &mut T, old: T, src: T) -> T;\n+    pub fn atomic_cxchg_rel<T>(dst: &mut T, old: T, src: T) -> T;\n+    pub fn atomic_cxchg_acqrel<T>(dst: &mut T, old: T, src: T) -> T;\n+    pub fn atomic_cxchg_relaxed<T>(dst: &mut T, old: T, src: T) -> T;\n+\n+    pub fn atomic_load<T>(src: &T) -> T;\n+    pub fn atomic_load_acq<T>(src: &T) -> T;\n+    pub fn atomic_load_relaxed<T>(src: &T) -> T;\n+\n+    pub fn atomic_store<T>(dst: &mut T, val: T);\n+    pub fn atomic_store_rel<T>(dst: &mut T, val: T);\n+    pub fn atomic_store_relaxed<T>(dst: &mut T, val: T);\n+\n+    pub fn atomic_xchg<T>(dst: &mut T, src: T) -> T;\n+    pub fn atomic_xchg_acq<T>(dst: &mut T, src: T) -> T;\n+    pub fn atomic_xchg_rel<T>(dst: &mut T, src: T) -> T;\n+    pub fn atomic_xchg_acqrel<T>(dst: &mut T, src: T) -> T;\n+    pub fn atomic_xchg_relaxed<T>(dst: &mut T, src: T) -> T;\n+\n+    pub fn atomic_xadd<T>(dst: &mut T, src: T) -> T;\n+    pub fn atomic_xadd_acq<T>(dst: &mut T, src: T) -> T;\n+    pub fn atomic_xadd_rel<T>(dst: &mut T, src: T) -> T;\n+    pub fn atomic_xadd_acqrel<T>(dst: &mut T, src: T) -> T;\n+    pub fn atomic_xadd_relaxed<T>(dst: &mut T, src: T) -> T;\n+\n+    pub fn atomic_xsub<T>(dst: &mut T, src: T) -> T;\n+    pub fn atomic_xsub_acq<T>(dst: &mut T, src: T) -> T;\n+    pub fn atomic_xsub_rel<T>(dst: &mut T, src: T) -> T;\n+    pub fn atomic_xsub_acqrel<T>(dst: &mut T, src: T) -> T;\n+    pub fn atomic_xsub_relaxed<T>(dst: &mut T, src: T) -> T;\n+\n+    pub fn atomic_and<T>(dst: &mut T, src: T) -> T;\n+    pub fn atomic_and_acq<T>(dst: &mut T, src: T) -> T;\n+    pub fn atomic_and_rel<T>(dst: &mut T, src: T) -> T;\n+    pub fn atomic_and_acqrel<T>(dst: &mut T, src: T) -> T;\n+    pub fn atomic_and_relaxed<T>(dst: &mut T, src: T) -> T;\n+\n+    pub fn atomic_nand<T>(dst: &mut T, src: T) -> T;\n+    pub fn atomic_nand_acq<T>(dst: &mut T, src: T) -> T;\n+    pub fn atomic_nand_rel<T>(dst: &mut T, src: T) -> T;\n+    pub fn atomic_nand_acqrel<T>(dst: &mut T, src: T) -> T;\n+    pub fn atomic_nand_relaxed<T>(dst: &mut T, src: T) -> T;\n+\n+    pub fn atomic_or<T>(dst: &mut T, src: T) -> T;\n+    pub fn atomic_or_acq<T>(dst: &mut T, src: T) -> T;\n+    pub fn atomic_or_rel<T>(dst: &mut T, src: T) -> T;\n+    pub fn atomic_or_acqrel<T>(dst: &mut T, src: T) -> T;\n+    pub fn atomic_or_relaxed<T>(dst: &mut T, src: T) -> T;\n+\n+    pub fn atomic_xor<T>(dst: &mut T, src: T) -> T;\n+    pub fn atomic_xor_acq<T>(dst: &mut T, src: T) -> T;\n+    pub fn atomic_xor_rel<T>(dst: &mut T, src: T) -> T;\n+    pub fn atomic_xor_acqrel<T>(dst: &mut T, src: T) -> T;\n+    pub fn atomic_xor_relaxed<T>(dst: &mut T, src: T) -> T;\n+\n+    pub fn atomic_max<T>(dst: &mut T, src: T) -> T;\n+    pub fn atomic_max_acq<T>(dst: &mut T, src: T) -> T;\n+    pub fn atomic_max_rel<T>(dst: &mut T, src: T) -> T;\n+    pub fn atomic_max_acqrel<T>(dst: &mut T, src: T) -> T;\n+    pub fn atomic_max_relaxed<T>(dst: &mut T, src: T) -> T;\n+\n+    pub fn atomic_min<T>(dst: &mut T, src: T) -> T;\n+    pub fn atomic_min_acq<T>(dst: &mut T, src: T) -> T;\n+    pub fn atomic_min_rel<T>(dst: &mut T, src: T) -> T;\n+    pub fn atomic_min_acqrel<T>(dst: &mut T, src: T) -> T;\n+    pub fn atomic_min_relaxed<T>(dst: &mut T, src: T) -> T;\n+\n+    pub fn atomic_umin<T>(dst: &mut T, src: T) -> T;\n+    pub fn atomic_umin_acq<T>(dst: &mut T, src: T) -> T;\n+    pub fn atomic_umin_rel<T>(dst: &mut T, src: T) -> T;\n+    pub fn atomic_umin_acqrel<T>(dst: &mut T, src: T) -> T;\n+    pub fn atomic_umin_relaxed<T>(dst: &mut T, src: T) -> T;\n+\n+    pub fn atomic_umax<T>(dst: &mut T, src: T) -> T;\n+    pub fn atomic_umax_acq<T>(dst: &mut T, src: T) -> T;\n+    pub fn atomic_umax_rel<T>(dst: &mut T, src: T) -> T;\n+    pub fn atomic_umax_acqrel<T>(dst: &mut T, src: T) -> T;\n+    pub fn atomic_umax_relaxed<T>(dst: &mut T, src: T) -> T;\n+\n+    pub fn atomic_fence();\n+    pub fn atomic_fence_acq();\n+    pub fn atomic_fence_rel();\n+    pub fn atomic_fence_acqrel();\n+}\n+\n+extern \"rust-intrinsic\" {\n+    /// Abort the execution of the process.\n+    pub fn abort() -> !;\n+\n+    /// Execute a breakpoint trap, for inspection by a debugger.\n+    pub fn breakpoint();\n+\n+    pub fn volatile_load<T>(src: *T) -> T;\n+    pub fn volatile_store<T>(dst: *mut T, val: T);\n+\n \n     /// The size of a type in bytes.\n     ///"}, {"sha": "4804de756876f5e042ae2c010355149b5b2d2b08", "filename": "src/libstd/unstable/mutex.rs", "status": "modified", "additions": 217, "deletions": 300, "changes": 517, "blob_url": "https://github.com/rust-lang/rust/blob/d42521aa92006a3378c535adec80ae2257bff083/src%2Flibstd%2Funstable%2Fmutex.rs", "raw_url": "https://github.com/rust-lang/rust/raw/d42521aa92006a3378c535adec80ae2257bff083/src%2Flibstd%2Funstable%2Fmutex.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibstd%2Funstable%2Fmutex.rs?ref=d42521aa92006a3378c535adec80ae2257bff083", "patch": "@@ -47,180 +47,186 @@\n \n #[allow(non_camel_case_types)];\n \n-use int;\n-use sync::atomics;\n-\n pub struct Mutex {\n-    // pointers for the lock/cond handles, atomically updated\n-    priv lock: atomics::AtomicUint,\n-    priv cond: atomics::AtomicUint,\n+    priv inner: imp::Mutex,\n }\n \n pub static MUTEX_INIT: Mutex = Mutex {\n-    lock: atomics::INIT_ATOMIC_UINT,\n-    cond: atomics::INIT_ATOMIC_UINT,\n+    inner: imp::MUTEX_INIT,\n };\n \n impl Mutex {\n-    /// Creates a new mutex, with the lock/condition variable pre-initialized\n+    /// Creates a new mutex\n     pub unsafe fn new() -> Mutex {\n-        Mutex {\n-            lock: atomics::AtomicUint::new(imp::init_lock()),\n-            cond: atomics::AtomicUint::new(imp::init_cond()),\n-        }\n-    }\n-\n-    /// Creates a new mutex, with the lock/condition variable not initialized.\n-    /// This is the same as initializing from the MUTEX_INIT static.\n-    pub unsafe fn empty() -> Mutex {\n-        Mutex {\n-            lock: atomics::AtomicUint::new(0),\n-            cond: atomics::AtomicUint::new(0),\n-        }\n-    }\n-\n-    /// Creates a new copy of this mutex. This is an unsafe operation because\n-    /// there is no reference counting performed on this type.\n-    ///\n-    /// This function may only be called on mutexes which have had both the\n-    /// internal condition variable and lock initialized. This means that the\n-    /// mutex must have been created via `new`, or usage of it has already\n-    /// initialized the internal handles.\n-    ///\n-    /// This is a dangerous function to call as both this mutex and the returned\n-    /// mutex will share the same handles to the underlying mutex/condition\n-    /// variable. Care must be taken to ensure that deallocation happens\n-    /// accordingly.\n-    pub unsafe fn clone(&self) -> Mutex {\n-        let lock = self.lock.load(atomics::Relaxed);\n-        let cond = self.cond.load(atomics::Relaxed);\n-        assert!(lock != 0);\n-        assert!(cond != 0);\n-        Mutex {\n-            lock: atomics::AtomicUint::new(lock),\n-            cond: atomics::AtomicUint::new(cond),\n-        }\n+        Mutex { inner: imp::Mutex::new() }\n     }\n \n     /// Acquires this lock. This assumes that the current thread does not\n     /// already hold the lock.\n-    pub unsafe fn lock(&mut self) { imp::lock(self.getlock()) }\n+    pub unsafe fn lock(&mut self) { self.inner.lock() }\n \n     /// Attempts to acquire the lock. The value returned is whether the lock was\n     /// acquired or not\n-    pub unsafe fn trylock(&mut self) -> bool { imp::trylock(self.getlock()) }\n+    pub unsafe fn trylock(&mut self) -> bool { self.inner.trylock() }\n \n     /// Unlocks the lock. This assumes that the current thread already holds the\n     /// lock.\n-    pub unsafe fn unlock(&mut self) { imp::unlock(self.getlock()) }\n+    pub unsafe fn unlock(&mut self) { self.inner.unlock() }\n \n     /// Block on the internal condition variable.\n     ///\n     /// This function assumes that the lock is already held\n-    pub unsafe fn wait(&mut self) { imp::wait(self.getcond(), self.getlock()) }\n+    pub unsafe fn wait(&mut self) { self.inner.wait() }\n \n     /// Signals a thread in `wait` to wake up\n-    pub unsafe fn signal(&mut self) { imp::signal(self.getcond()) }\n+    pub unsafe fn signal(&mut self) { self.inner.signal() }\n \n     /// This function is especially unsafe because there are no guarantees made\n     /// that no other thread is currently holding the lock or waiting on the\n     /// condition variable contained inside.\n-    pub unsafe fn destroy(&mut self) {\n-        let lock = self.lock.swap(0, atomics::Relaxed);\n-        let cond = self.cond.swap(0, atomics::Relaxed);\n-        if lock != 0 { imp::free_lock(lock) }\n-        if cond != 0 { imp::free_cond(cond) }\n-    }\n-\n-    unsafe fn getlock(&mut self) -> uint{\n-        match self.lock.load(atomics::Relaxed) {\n-            0 => {}\n-            n => return n\n-        }\n-        let lock = imp::init_lock();\n-        match self.lock.compare_and_swap(0, lock, atomics::SeqCst) {\n-            0 => return lock,\n-            _ => {}\n-        }\n-        imp::free_lock(lock);\n-        self.lock.load(atomics::Relaxed)\n-    }\n-\n-    unsafe fn getcond(&mut self) -> uint {\n-        match self.cond.load(atomics::Relaxed) {\n-            0 => {}\n-            n => return n\n-        }\n-        let cond = imp::init_cond();\n-        match self.cond.compare_and_swap(0, cond, atomics::SeqCst) {\n-            0 => return cond,\n-            _ => {}\n-        }\n-        imp::free_cond(cond);\n-        self.cond.load(atomics::Relaxed)\n-    }\n+    pub unsafe fn destroy(&mut self) { self.inner.destroy() }\n }\n \n #[cfg(unix)]\n mod imp {\n     use libc;\n-    use ptr;\n-    use rt::global_heap::malloc_raw;\n+    use self::os::{PTHREAD_MUTEX_INITIALIZER, PTHREAD_COND_INITIALIZER,\n+                   pthread_mutex_t, pthread_cond_t};\n+    use unstable::intrinsics;\n \n-    type pthread_mutex_t = libc::c_void;\n     type pthread_mutexattr_t = libc::c_void;\n-    type pthread_cond_t = libc::c_void;\n     type pthread_condattr_t = libc::c_void;\n \n-    pub unsafe fn init_lock() -> uint {\n-        let block = malloc_raw(rust_pthread_mutex_t_size() as uint) as *mut pthread_mutex_t;\n-        let n = pthread_mutex_init(block, ptr::null());\n-        assert_eq!(n, 0);\n-        return block as uint;\n-    }\n+    #[cfg(target_os = \"freebsd\")]\n+    mod os {\n+        use libc;\n+\n+        pub type pthread_mutex_t = *libc::c_void;\n+        pub type pthread_cond_t = *libc::c_void;\n+\n+        pub static PTHREAD_MUTEX_INITIALIZER: pthread_mutex_t =\n+            0 as pthread_mutex_t;\n+        pub static PTHREAD_COND_INITIALIZER: pthread_cond_t =\n+            0 as pthread_cond_t;\n+    }\n+\n+    #[cfg(target_os = \"macos\")]\n+    mod os {\n+        use libc;\n+\n+        #[cfg(target_arch = \"x86_64\")]\n+        static __PTHREAD_MUTEX_SIZE__: uint = 56;\n+        #[cfg(target_arch = \"x86_64\")]\n+        static __PTHREAD_COND_SIZE__: uint = 40;\n+        #[cfg(target_arch = \"x86\")]\n+        static __PTHREAD_MUTEX_SIZE__: uint = 40;\n+        #[cfg(target_arch = \"x86\")]\n+        static __PTHREAD_COND_SIZE__: uint = 24;\n+        static _PTHREAD_MUTEX_SIG_init: libc::c_long = 0x32AAABA7;\n+        static _PTHREAD_COND_SIG_init: libc::c_long = 0x3CB0B1BB;\n+\n+        pub struct pthread_mutex_t {\n+            __sig: libc::c_long,\n+            __opaque: [u8, ..__PTHREAD_MUTEX_SIZE__],\n+        }\n+        pub struct pthread_cond_t {\n+            __sig: libc::c_long,\n+            __opaque: [u8, ..__PTHREAD_COND_SIZE__],\n+        }\n \n-    pub unsafe fn init_cond() -> uint {\n-        let block = malloc_raw(rust_pthread_cond_t_size() as uint) as *mut pthread_cond_t;\n-        let n = pthread_cond_init(block, ptr::null());\n-        assert_eq!(n, 0);\n-        return block as uint;\n-    }\n+        pub static PTHREAD_MUTEX_INITIALIZER: pthread_mutex_t = pthread_mutex_t {\n+            __sig: _PTHREAD_MUTEX_SIG_init,\n+            __opaque: [0, ..__PTHREAD_MUTEX_SIZE__],\n+        };\n+        pub static PTHREAD_COND_INITIALIZER: pthread_cond_t = pthread_cond_t {\n+            __sig: _PTHREAD_COND_SIG_init,\n+            __opaque: [0, ..__PTHREAD_COND_SIZE__],\n+        };\n+    }\n+\n+    #[cfg(target_os = \"linux\")]\n+    mod os {\n+        use libc;\n+\n+        // minus 8 because we have an 'align' field\n+        #[cfg(target_arch = \"x86_64\")]\n+        static __SIZEOF_PTHREAD_MUTEX_T: uint = 40 - 8;\n+        #[cfg(target_arch = \"x86\")]\n+        static __SIZEOF_PTHREAD_MUTEX_T: uint = 24 - 8;\n+        #[cfg(target_arch = \"x86_64\")]\n+        static __SIZEOF_PTHREAD_COND_T: uint = 48 - 8;\n+        #[cfg(target_arch = \"x86\")]\n+        static __SIZEOF_PTHREAD_COND_T: uint = 48 - 8;\n+\n+        pub struct pthread_mutex_t {\n+            __align: libc::c_long,\n+            size: [u8, ..__SIZEOF_PTHREAD_MUTEX_T],\n+        }\n+        pub struct pthread_cond_t {\n+            __align: libc::c_longlong,\n+            size: [u8, ..__SIZEOF_PTHREAD_COND_T],\n+        }\n \n-    pub unsafe fn free_lock(h: uint) {\n-        let block = h as *mut libc::c_void;\n-        assert_eq!(pthread_mutex_destroy(block), 0);\n-        libc::free(block);\n+        pub static PTHREAD_MUTEX_INITIALIZER: pthread_mutex_t = pthread_mutex_t {\n+            __align: 0,\n+            size: [0, ..__SIZEOF_PTHREAD_MUTEX_T],\n+        };\n+        pub static PTHREAD_COND_INITIALIZER: pthread_cond_t = pthread_cond_t {\n+            __align: 0,\n+            size: [0, ..__SIZEOF_PTHREAD_COND_T],\n+        };\n     }\n+    #[cfg(target_os = \"android\")]\n+    mod os {\n+        use libc;\n \n-    pub unsafe fn free_cond(h: uint) {\n-        let block = h as *mut pthread_cond_t;\n-        assert_eq!(pthread_cond_destroy(block), 0);\n-        libc::free(block);\n-    }\n+        pub struct pthread_mutex_t { value: libc::c_int }\n+        pub struct pthread_cond_t { value: libc::c_int }\n \n-    pub unsafe fn lock(l: uint) {\n-        assert_eq!(pthread_mutex_lock(l as *mut pthread_mutex_t), 0);\n+        pub static PTHREAD_MUTEX_INITIALIZER: pthread_mutex_t = pthread_mutex_t {\n+            value: 0,\n+        };\n+        pub static PTHREAD_COND_INITIALIZER: pthread_cond_t = pthread_cond_t {\n+            value: 0,\n+        };\n     }\n \n-    pub unsafe fn trylock(l: uint) -> bool {\n-        pthread_mutex_trylock(l as *mut pthread_mutex_t) == 0\n+    pub struct Mutex {\n+        priv lock: pthread_mutex_t,\n+        priv cond: pthread_cond_t,\n     }\n \n-    pub unsafe fn unlock(l: uint) {\n-        assert_eq!(pthread_mutex_unlock(l as *mut pthread_mutex_t), 0);\n-    }\n+    pub static MUTEX_INIT: Mutex = Mutex {\n+        lock: PTHREAD_MUTEX_INITIALIZER,\n+        cond: PTHREAD_COND_INITIALIZER,\n+    };\n \n-    pub unsafe fn wait(cond: uint, m: uint) {\n-        assert_eq!(pthread_cond_wait(cond as *mut pthread_cond_t, m as *mut pthread_mutex_t), 0);\n-    }\n+    impl Mutex {\n+        pub unsafe fn new() -> Mutex {\n+            let mut m = Mutex {\n+                lock: intrinsics::init(),\n+                cond: intrinsics::init(),\n+            };\n \n-    pub unsafe fn signal(cond: uint) {\n-        assert_eq!(pthread_cond_signal(cond as *mut pthread_cond_t), 0);\n-    }\n+            pthread_mutex_init(&mut m.lock, 0 as *libc::c_void);\n+            pthread_cond_init(&mut m.cond, 0 as *libc::c_void);\n \n-    extern {\n-        fn rust_pthread_mutex_t_size() -> libc::c_int;\n-        fn rust_pthread_cond_t_size() -> libc::c_int;\n+            return m;\n+        }\n+\n+        pub unsafe fn lock(&mut self) { pthread_mutex_lock(&mut self.lock); }\n+        pub unsafe fn unlock(&mut self) { pthread_mutex_unlock(&mut self.lock); }\n+        pub unsafe fn signal(&mut self) { pthread_cond_signal(&mut self.cond); }\n+        pub unsafe fn wait(&mut self) {\n+            pthread_cond_wait(&mut self.cond, &mut self.lock);\n+        }\n+        pub unsafe fn trylock(&mut self) -> bool {\n+            pthread_mutex_trylock(&mut self.lock) == 0\n+        }\n+        pub unsafe fn destroy(&mut self) {\n+            pthread_mutex_destroy(&mut self.lock);\n+            pthread_cond_destroy(&mut self.cond);\n+        }\n     }\n \n     extern {\n@@ -242,16 +248,96 @@ mod imp {\n \n #[cfg(windows)]\n mod imp {\n-    use libc;\n+    use rt::global_heap::malloc_raw;\n     use libc::{HANDLE, BOOL, LPSECURITY_ATTRIBUTES, c_void, DWORD, LPCSTR};\n+    use libc;\n     use ptr;\n-    use rt::global_heap::malloc_raw;\n+    use sync::atomics;\n \n-    type LPCRITICAL_SECTION = *c_void;\n+    type LPCRITICAL_SECTION = *mut c_void;\n     static SPIN_COUNT: DWORD = 4000;\n+    #[cfg(target_arch = \"x86\")]\n+    static CRIT_SECTION_SIZE: uint = 24;\n+\n+    pub struct Mutex {\n+        // pointers for the lock/cond handles, atomically updated\n+        priv lock: atomics::AtomicUint,\n+        priv cond: atomics::AtomicUint,\n+    }\n+\n+    pub static MUTEX_INIT: Mutex = Mutex {\n+        lock: atomics::INIT_ATOMIC_UINT,\n+        cond: atomics::INIT_ATOMIC_UINT,\n+    };\n+\n+    impl Mutex {\n+        pub unsafe fn new() -> Mutex {\n+            Mutex {\n+                lock: atomics::AtomicUint::new(init_lock()),\n+                cond: atomics::AtomicUint::new(init_cond()),\n+            }\n+        }\n+        pub unsafe fn lock(&mut self) {\n+            EnterCriticalSection(self.getlock() as LPCRITICAL_SECTION)\n+        }\n+        pub unsafe fn trylock(&mut self) -> bool {\n+            TryEnterCriticalSection(self.getlock() as LPCRITICAL_SECTION) != 0\n+        }\n+        pub unsafe fn unlock(&mut self) {\n+            LeaveCriticalSection(self.getlock() as LPCRITICAL_SECTION)\n+        }\n+\n+        pub unsafe fn wait(&mut self) {\n+            self.unlock();\n+            WaitForSingleObject(self.getcond() as HANDLE, libc::INFINITE);\n+            self.lock();\n+        }\n+\n+        pub unsafe fn signal(&mut self) {\n+            assert!(SetEvent(self.getcond() as HANDLE) != 0);\n+        }\n+\n+        /// This function is especially unsafe because there are no guarantees made\n+        /// that no other thread is currently holding the lock or waiting on the\n+        /// condition variable contained inside.\n+        pub unsafe fn destroy(&mut self) {\n+            let lock = self.lock.swap(0, atomics::SeqCst);\n+            let cond = self.cond.swap(0, atomics::SeqCst);\n+            if lock != 0 { free_lock(lock) }\n+            if cond != 0 { free_cond(cond) }\n+        }\n+\n+        unsafe fn getlock(&mut self) -> *mut c_void {\n+            match self.lock.load(atomics::SeqCst) {\n+                0 => {}\n+                n => return n as *mut c_void\n+            }\n+            let lock = init_lock();\n+            match self.lock.compare_and_swap(0, lock, atomics::SeqCst) {\n+                0 => return lock as *mut c_void,\n+                _ => {}\n+            }\n+            free_lock(lock);\n+            return self.lock.load(atomics::SeqCst) as *mut c_void;\n+        }\n+\n+        unsafe fn getcond(&mut self) -> *mut c_void {\n+            match self.cond.load(atomics::SeqCst) {\n+                0 => {}\n+                n => return n as *mut c_void\n+            }\n+            let cond = init_cond();\n+            match self.cond.compare_and_swap(0, cond, atomics::SeqCst) {\n+                0 => return cond as *mut c_void,\n+                _ => {}\n+            }\n+            free_cond(cond);\n+            return self.cond.load(atomics::SeqCst) as *mut c_void;\n+        }\n+    }\n \n     pub unsafe fn init_lock() -> uint {\n-        let block = malloc_raw(rust_crit_section_size() as uint) as *c_void;\n+        let block = malloc_raw(CRIT_SECTION_SIZE as uint) as *mut c_void;\n         InitializeCriticalSectionAndSpinCount(block, SPIN_COUNT);\n         return block as uint;\n     }\n@@ -271,32 +357,6 @@ mod imp {\n         libc::CloseHandle(block);\n     }\n \n-    pub unsafe fn lock(l: uint) {\n-        EnterCriticalSection(l as LPCRITICAL_SECTION)\n-    }\n-\n-    pub unsafe fn trylock(l: uint) -> bool {\n-        TryEnterCriticalSection(l as LPCRITICAL_SECTION) != 0\n-    }\n-\n-    pub unsafe fn unlock(l: uint) {\n-        LeaveCriticalSection(l as LPCRITICAL_SECTION)\n-    }\n-\n-    pub unsafe fn wait(cond: uint, m: uint) {\n-        unlock(m);\n-        WaitForSingleObject(cond as HANDLE, libc::INFINITE);\n-        lock(m);\n-    }\n-\n-    pub unsafe fn signal(cond: uint) {\n-        assert!(SetEvent(cond as HANDLE) != 0);\n-    }\n-\n-    extern {\n-        fn rust_crit_section_size() -> libc::c_int;\n-    }\n-\n     extern \"system\" {\n         fn CreateEventA(lpSecurityAttributes: LPSECURITY_ATTRIBUTES,\n                         bManualReset: BOOL,\n@@ -314,157 +374,14 @@ mod imp {\n     }\n }\n \n-/// A type which can be used to run a one-time global initialization. This type\n-/// is *unsafe* to use because it is built on top of the `Mutex` in this module.\n-/// It does not know whether the currently running task is in a green or native\n-/// context, and a blocking mutex should *not* be used under normal\n-/// circumstances on a green task.\n-///\n-/// Despite its unsafety, it is often useful to have a one-time initialization\n-/// routine run for FFI bindings or related external functionality. This type\n-/// can only be statically constructed with the `ONCE_INIT` value.\n-///\n-/// # Example\n-///\n-/// ```rust\n-/// use std::unstable::mutex::{Once, ONCE_INIT};\n-///\n-/// static mut START: Once = ONCE_INIT;\n-/// unsafe {\n-///     START.doit(|| {\n-///         // run initialization here\n-///     });\n-/// }\n-/// ```\n-pub struct Once {\n-    priv mutex: Mutex,\n-    priv cnt: atomics::AtomicInt,\n-    priv lock_cnt: atomics::AtomicInt,\n-}\n-\n-/// Initialization value for static `Once` values.\n-pub static ONCE_INIT: Once = Once {\n-    mutex: MUTEX_INIT,\n-    cnt: atomics::INIT_ATOMIC_INT,\n-    lock_cnt: atomics::INIT_ATOMIC_INT,\n-};\n-\n-impl Once {\n-    /// Perform an initialization routine once and only once. The given closure\n-    /// will be executed if this is the first time `doit` has been called, and\n-    /// otherwise the routine will *not* be invoked.\n-    ///\n-    /// This method will block the calling *os thread* if another initialization\n-    /// routine is currently running.\n-    ///\n-    /// When this function returns, it is guaranteed that some initialization\n-    /// has run and completed (it may not be the closure specified).\n-    pub fn doit(&mut self, f: ||) {\n-        // Implementation-wise, this would seem like a fairly trivial primitive.\n-        // The stickler part is where our mutexes currently require an\n-        // allocation, and usage of a `Once` should't leak this allocation.\n-        //\n-        // This means that there must be a deterministic destroyer of the mutex\n-        // contained within (because it's not needed after the initialization\n-        // has run).\n-        //\n-        // The general scheme here is to gate all future threads once\n-        // initialization has completed with a \"very negative\" count, and to\n-        // allow through threads to lock the mutex if they see a non negative\n-        // count. For all threads grabbing the mutex, exactly one of them should\n-        // be responsible for unlocking the mutex, and this should only be done\n-        // once everyone else is done with the mutex.\n-        //\n-        // This atomicity is achieved by swapping a very negative value into the\n-        // shared count when the initialization routine has completed. This will\n-        // read the number of threads which will at some point attempt to\n-        // acquire the mutex. This count is then squirreled away in a separate\n-        // variable, and the last person on the way out of the mutex is then\n-        // responsible for destroying the mutex.\n-        //\n-        // It is crucial that the negative value is swapped in *after* the\n-        // initialization routine has completed because otherwise new threads\n-        // calling `doit` will return immediately before the initialization has\n-        // completed.\n-\n-        let prev = self.cnt.fetch_add(1, atomics::SeqCst);\n-        if prev < 0 {\n-            // Make sure we never overflow, we'll never have int::MIN\n-            // simultaneous calls to `doit` to make this value go back to 0\n-            self.cnt.store(int::MIN, atomics::SeqCst);\n-            return\n-        }\n-\n-        // If the count is negative, then someone else finished the job,\n-        // otherwise we run the job and record how many people will try to grab\n-        // this lock\n-        unsafe { self.mutex.lock() }\n-        if self.cnt.load(atomics::SeqCst) > 0 {\n-            f();\n-            let prev = self.cnt.swap(int::MIN, atomics::SeqCst);\n-            self.lock_cnt.store(prev, atomics::SeqCst);\n-        }\n-        unsafe { self.mutex.unlock() }\n-\n-        // Last one out cleans up after everyone else, no leaks!\n-        if self.lock_cnt.fetch_add(-1, atomics::SeqCst) == 1 {\n-            unsafe { self.mutex.destroy() }\n-        }\n-    }\n-}\n-\n #[cfg(test)]\n mod test {\n     use prelude::*;\n \n+    use super::{Mutex, MUTEX_INIT};\n     use rt::thread::Thread;\n-    use super::{ONCE_INIT, Once, Mutex, MUTEX_INIT};\n     use task;\n \n-    #[test]\n-    fn smoke_once() {\n-        static mut o: Once = ONCE_INIT;\n-        let mut a = 0;\n-        unsafe { o.doit(|| a += 1); }\n-        assert_eq!(a, 1);\n-        unsafe { o.doit(|| a += 1); }\n-        assert_eq!(a, 1);\n-    }\n-\n-    #[test]\n-    fn stampede_once() {\n-        static mut o: Once = ONCE_INIT;\n-        static mut run: bool = false;\n-\n-        let (p, c) = SharedChan::new();\n-        for _ in range(0, 10) {\n-            let c = c.clone();\n-            spawn(proc() {\n-                for _ in range(0, 4) { task::deschedule() }\n-                unsafe {\n-                    o.doit(|| {\n-                        assert!(!run);\n-                        run = true;\n-                    });\n-                    assert!(run);\n-                }\n-                c.send(());\n-            });\n-        }\n-\n-        unsafe {\n-            o.doit(|| {\n-                assert!(!run);\n-                run = true;\n-            });\n-            assert!(run);\n-        }\n-\n-        for _ in range(0, 10) {\n-            p.recv();\n-        }\n-    }\n-\n     #[test]\n     fn somke_lock() {\n         static mut lock: Mutex = MUTEX_INIT;\n@@ -493,7 +410,7 @@ mod test {\n     #[test]\n     fn destroy_immediately() {\n         unsafe {\n-            let mut m = Mutex::empty();\n+            let mut m = Mutex::new();\n             m.destroy();\n         }\n     }"}, {"sha": "81eba2984dad0aa8e6f79b1f8f8bb902ece7dbf2", "filename": "src/rt/rust_builtin.c", "status": "modified", "additions": 0, "deletions": 20, "changes": 20, "blob_url": "https://github.com/rust-lang/rust/blob/d42521aa92006a3378c535adec80ae2257bff083/src%2Frt%2Frust_builtin.c", "raw_url": "https://github.com/rust-lang/rust/raw/d42521aa92006a3378c535adec80ae2257bff083/src%2Frt%2Frust_builtin.c", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Frt%2Frust_builtin.c?ref=d42521aa92006a3378c535adec80ae2257bff083", "patch": "@@ -437,26 +437,6 @@ rust_win32_rand_release() {\n \n #endif\n \n-#if defined(__WIN32__)\n-\n-int\n-rust_crit_section_size() { return sizeof(CRITICAL_SECTION); }\n-int\n-rust_pthread_mutex_t_size() { return 0; }\n-int\n-rust_pthread_cond_t_size() { return 0; }\n-\n-#else\n-\n-int\n-rust_crit_section_size() { return 0; }\n-int\n-rust_pthread_mutex_t_size() { return sizeof(pthread_mutex_t); }\n-int\n-rust_pthread_cond_t_size() { return sizeof(pthread_cond_t); }\n-\n-#endif\n-\n //\n // Local Variables:\n // mode: C++"}, {"sha": "07d6df89d220c29e317bebbde85f71f00714c7f9", "filename": "src/test/auxiliary/cci_intrinsic.rs", "status": "modified", "additions": 12, "deletions": 12, "changes": 24, "blob_url": "https://github.com/rust-lang/rust/blob/d42521aa92006a3378c535adec80ae2257bff083/src%2Ftest%2Fauxiliary%2Fcci_intrinsic.rs", "raw_url": "https://github.com/rust-lang/rust/raw/d42521aa92006a3378c535adec80ae2257bff083/src%2Ftest%2Fauxiliary%2Fcci_intrinsic.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ftest%2Fauxiliary%2Fcci_intrinsic.rs?ref=d42521aa92006a3378c535adec80ae2257bff083", "patch": "@@ -10,21 +10,21 @@\n \n pub mod rusti {\n     extern \"rust-intrinsic\" {\n-        pub fn atomic_cxchg(dst: &mut int, old: int, src: int) -> int;\n-        pub fn atomic_cxchg_acq(dst: &mut int, old: int, src: int) -> int;\n-        pub fn atomic_cxchg_rel(dst: &mut int, old: int, src: int) -> int;\n+        pub fn atomic_cxchg<T>(dst: &mut T, old: T, src: T) -> T;\n+        pub fn atomic_cxchg_acq<T>(dst: &mut T, old: T, src: T) -> T;\n+        pub fn atomic_cxchg_rel<T>(dst: &mut T, old: T, src: T) -> T;\n \n-        pub fn atomic_xchg(dst: &mut int, src: int) -> int;\n-        pub fn atomic_xchg_acq(dst: &mut int, src: int) -> int;\n-        pub fn atomic_xchg_rel(dst: &mut int, src: int) -> int;\n+        pub fn atomic_xchg<T>(dst: &mut T, src: T) -> T;\n+        pub fn atomic_xchg_acq<T>(dst: &mut T, src: T) -> T;\n+        pub fn atomic_xchg_rel<T>(dst: &mut T, src: T) -> T;\n \n-        pub fn atomic_xadd(dst: &mut int, src: int) -> int;\n-        pub fn atomic_xadd_acq(dst: &mut int, src: int) -> int;\n-        pub fn atomic_xadd_rel(dst: &mut int, src: int) -> int;\n+        pub fn atomic_xadd<T>(dst: &mut T, src: T) -> T;\n+        pub fn atomic_xadd_acq<T>(dst: &mut T, src: T) -> T;\n+        pub fn atomic_xadd_rel<T>(dst: &mut T, src: T) -> T;\n \n-        pub fn atomic_xsub(dst: &mut int, src: int) -> int;\n-        pub fn atomic_xsub_acq(dst: &mut int, src: int) -> int;\n-        pub fn atomic_xsub_rel(dst: &mut int, src: int) -> int;\n+        pub fn atomic_xsub<T>(dst: &mut T, src: T) -> T;\n+        pub fn atomic_xsub_acq<T>(dst: &mut T, src: T) -> T;\n+        pub fn atomic_xsub_rel<T>(dst: &mut T, src: T) -> T;\n     }\n }\n "}, {"sha": "d6e394a345e228bee26d849175457acfaee0f081", "filename": "src/test/run-pass/intrinsic-atomics.rs", "status": "modified", "additions": 16, "deletions": 16, "changes": 32, "blob_url": "https://github.com/rust-lang/rust/blob/d42521aa92006a3378c535adec80ae2257bff083/src%2Ftest%2Frun-pass%2Fintrinsic-atomics.rs", "raw_url": "https://github.com/rust-lang/rust/raw/d42521aa92006a3378c535adec80ae2257bff083/src%2Ftest%2Frun-pass%2Fintrinsic-atomics.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ftest%2Frun-pass%2Fintrinsic-atomics.rs?ref=d42521aa92006a3378c535adec80ae2257bff083", "patch": "@@ -10,27 +10,27 @@\n \n mod rusti {\n     extern \"rust-intrinsic\" {\n-        pub fn atomic_cxchg(dst: &mut int, old: int, src: int) -> int;\n-        pub fn atomic_cxchg_acq(dst: &mut int, old: int, src: int) -> int;\n-        pub fn atomic_cxchg_rel(dst: &mut int, old: int, src: int) -> int;\n+        pub fn atomic_cxchg<T>(dst: &mut T, old: T, src: T) -> T;\n+        pub fn atomic_cxchg_acq<T>(dst: &mut T, old: T, src: T) -> T;\n+        pub fn atomic_cxchg_rel<T>(dst: &mut T, old: T, src: T) -> T;\n \n-        pub fn atomic_load(src: &int) -> int;\n-        pub fn atomic_load_acq(src: &int) -> int;\n+        pub fn atomic_load<T>(src: &T) -> T;\n+        pub fn atomic_load_acq<T>(src: &T) -> T;\n \n-        pub fn atomic_store(dst: &mut int, val: int);\n-        pub fn atomic_store_rel(dst: &mut int, val: int);\n+        pub fn atomic_store<T>(dst: &mut T, val: T);\n+        pub fn atomic_store_rel<T>(dst: &mut T, val: T);\n \n-        pub fn atomic_xchg(dst: &mut int, src: int) -> int;\n-        pub fn atomic_xchg_acq(dst: &mut int, src: int) -> int;\n-        pub fn atomic_xchg_rel(dst: &mut int, src: int) -> int;\n+        pub fn atomic_xchg<T>(dst: &mut T, src: T) -> T;\n+        pub fn atomic_xchg_acq<T>(dst: &mut T, src: T) -> T;\n+        pub fn atomic_xchg_rel<T>(dst: &mut T, src: T) -> T;\n \n-        pub fn atomic_xadd(dst: &mut int, src: int) -> int;\n-        pub fn atomic_xadd_acq(dst: &mut int, src: int) -> int;\n-        pub fn atomic_xadd_rel(dst: &mut int, src: int) -> int;\n+        pub fn atomic_xadd<T>(dst: &mut T, src: T) -> T;\n+        pub fn atomic_xadd_acq<T>(dst: &mut T, src: T) -> T;\n+        pub fn atomic_xadd_rel<T>(dst: &mut T, src: T) -> T;\n \n-        pub fn atomic_xsub(dst: &mut int, src: int) -> int;\n-        pub fn atomic_xsub_acq(dst: &mut int, src: int) -> int;\n-        pub fn atomic_xsub_rel(dst: &mut int, src: int) -> int;\n+        pub fn atomic_xsub<T>(dst: &mut T, src: T) -> T;\n+        pub fn atomic_xsub_acq<T>(dst: &mut T, src: T) -> T;\n+        pub fn atomic_xsub_rel<T>(dst: &mut T, src: T) -> T;\n     }\n }\n "}]}