{"sha": "5d35f284f5ac70cde5d758e7c63a38eae0fb0b55", "node_id": "MDY6Q29tbWl0NzI0NzEyOjVkMzVmMjg0ZjVhYzcwY2RlNWQ3NThlN2M2M2EzOGVhZTBmYjBiNTU=", "commit": {"author": {"name": "bors[bot]", "email": "bors[bot]@users.noreply.github.com", "date": "2019-04-14T14:16:42Z"}, "committer": {"name": "bors[bot]", "email": "bors[bot]@users.noreply.github.com", "date": "2019-04-14T14:16:42Z"}, "message": "Merge #1138\n\n1138: Add L_DOLLAR and R_DOLLAR r=matklad a=edwin0cheng\n\nAs discussion in issue https://github.com/rust-analyzer/rust-analyzer/issues/1132 and PR #1125 , this PR add 2 `Syntax::Kind` : `L_DOLLAR` and `R_DOLLAR` for representing `Delimiter::None` in mbe and proc_marco. By design, It should not affect the final syntax tree, and will be discard in `TreeSink`.\r\n\r\nMy original idea is handling these 2 tokens case by case, but i found that they will appear in every place in the parser (imagine `tt` matcher). So this PR only handle it in `Parser::do_bump` and `Parser::start`, although It will not fix the `expr` matcher executing order problem in original idea.\r\n\n\nCo-authored-by: Edwin Cheng <edwin0cheng@gmail.com>", "tree": {"sha": "4cde8df0f4a496bb1f38b3e3479ce462fc1f7426", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/4cde8df0f4a496bb1f38b3e3479ce462fc1f7426"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/5d35f284f5ac70cde5d758e7c63a38eae0fb0b55", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/5d35f284f5ac70cde5d758e7c63a38eae0fb0b55", "html_url": "https://github.com/rust-lang/rust/commit/5d35f284f5ac70cde5d758e7c63a38eae0fb0b55", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/5d35f284f5ac70cde5d758e7c63a38eae0fb0b55/comments", "author": {"login": "bors[bot]", "id": 26634292, "node_id": "MDM6Qm90MjY2MzQyOTI=", "avatar_url": "https://avatars.githubusercontent.com/in/1847?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bors%5Bbot%5D", "html_url": "https://github.com/apps/bors", "followers_url": "https://api.github.com/users/bors%5Bbot%5D/followers", "following_url": "https://api.github.com/users/bors%5Bbot%5D/following{/other_user}", "gists_url": "https://api.github.com/users/bors%5Bbot%5D/gists{/gist_id}", "starred_url": "https://api.github.com/users/bors%5Bbot%5D/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bors%5Bbot%5D/subscriptions", "organizations_url": "https://api.github.com/users/bors%5Bbot%5D/orgs", "repos_url": "https://api.github.com/users/bors%5Bbot%5D/repos", "events_url": "https://api.github.com/users/bors%5Bbot%5D/events{/privacy}", "received_events_url": "https://api.github.com/users/bors%5Bbot%5D/received_events", "type": "Bot", "site_admin": false}, "committer": {"login": "bors[bot]", "id": 26634292, "node_id": "MDM6Qm90MjY2MzQyOTI=", "avatar_url": "https://avatars.githubusercontent.com/in/1847?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bors%5Bbot%5D", "html_url": "https://github.com/apps/bors", "followers_url": "https://api.github.com/users/bors%5Bbot%5D/followers", "following_url": "https://api.github.com/users/bors%5Bbot%5D/following{/other_user}", "gists_url": "https://api.github.com/users/bors%5Bbot%5D/gists{/gist_id}", "starred_url": "https://api.github.com/users/bors%5Bbot%5D/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bors%5Bbot%5D/subscriptions", "organizations_url": "https://api.github.com/users/bors%5Bbot%5D/orgs", "repos_url": "https://api.github.com/users/bors%5Bbot%5D/repos", "events_url": "https://api.github.com/users/bors%5Bbot%5D/events{/privacy}", "received_events_url": "https://api.github.com/users/bors%5Bbot%5D/received_events", "type": "Bot", "site_admin": false}, "parents": [{"sha": "fcbd0269545f2b6687a64a868654c74f876b7851", "url": "https://api.github.com/repos/rust-lang/rust/commits/fcbd0269545f2b6687a64a868654c74f876b7851", "html_url": "https://github.com/rust-lang/rust/commit/fcbd0269545f2b6687a64a868654c74f876b7851"}, {"sha": "6646d49f238bb92d55fcb4900830f19faa2994a5", "url": "https://api.github.com/repos/rust-lang/rust/commits/6646d49f238bb92d55fcb4900830f19faa2994a5", "html_url": "https://github.com/rust-lang/rust/commit/6646d49f238bb92d55fcb4900830f19faa2994a5"}], "stats": {"total": 803, "additions": 560, "deletions": 243}, "files": [{"sha": "a530f3b03c2ef145300bf4fdee1af0eb9291d7e3", "filename": "crates/ra_mbe/src/lib.rs", "status": "modified", "additions": 152, "deletions": 2, "changes": 154, "blob_url": "https://github.com/rust-lang/rust/blob/5d35f284f5ac70cde5d758e7c63a38eae0fb0b55/crates%2Fra_mbe%2Fsrc%2Flib.rs", "raw_url": "https://github.com/rust-lang/rust/raw/5d35f284f5ac70cde5d758e7c63a38eae0fb0b55/crates%2Fra_mbe%2Fsrc%2Flib.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fra_mbe%2Fsrc%2Flib.rs?ref=5d35f284f5ac70cde5d758e7c63a38eae0fb0b55", "patch": "@@ -39,7 +39,7 @@ pub enum ExpandError {\n     BindingError(String),\n }\n \n-pub use crate::syntax_bridge::{ast_to_token_tree, token_tree_to_ast_item_list};\n+pub use crate::syntax_bridge::{ast_to_token_tree, token_tree_to_ast_item_list, syntax_node_to_token_tree};\n \n /// This struct contains AST for a single `macro_rules` definition. What might\n /// be very confusing is that AST has almost exactly the same shape as\n@@ -189,9 +189,26 @@ impl_froms!(TokenTree: Leaf, Subtree);\n         rules.expand(&invocation_tt).unwrap()\n     }\n \n+    pub(crate) fn expand_to_syntax(\n+        rules: &MacroRules,\n+        invocation: &str,\n+    ) -> ra_syntax::TreeArc<ast::SourceFile> {\n+        let expanded = expand(rules, invocation);\n+        token_tree_to_ast_item_list(&expanded)\n+    }\n+\n     pub(crate) fn assert_expansion(rules: &MacroRules, invocation: &str, expansion: &str) {\n         let expanded = expand(rules, invocation);\n         assert_eq!(expanded.to_string(), expansion);\n+\n+        let tree = token_tree_to_ast_item_list(&expanded);\n+\n+        // Eat all white space by parse it back and forth\n+        let expansion = ast::SourceFile::parse(expansion);\n+        let expansion = syntax_node_to_token_tree(expansion.syntax()).unwrap().0;\n+        let file = token_tree_to_ast_item_list(&expansion);\n+\n+        assert_eq!(tree.syntax().debug_dump().trim(), file.syntax().debug_dump().trim());\n     }\n \n     #[test]\n@@ -287,6 +304,36 @@ impl_froms!(TokenTree: Leaf, Subtree);\n         assert_expansion(&rules, \"foo! { Foo,# Bar }\", \"struct Foo ; struct Bar ;\");\n     }\n \n+    #[test]\n+    fn test_match_group_pattern_with_multiple_defs() {\n+        let rules = create_rules(\n+            r#\"\n+        macro_rules! foo {\n+            ($ ($ i:ident),*) => ( struct Bar { $ (\n+                fn $ i {}\n+            )*} );            \n+        }\n+\"#,\n+        );\n+\n+        assert_expansion(&rules, \"foo! { foo, bar }\", \"struct Bar {fn foo {} fn bar {}}\");\n+    }\n+\n+    #[test]\n+    fn test_match_group_pattern_with_multiple_statement() {\n+        let rules = create_rules(\n+            r#\"\n+        macro_rules! foo {\n+            ($ ($ i:ident),*) => ( fn baz { $ (\n+                $ i ();\n+            )*} );            \n+        }\n+\"#,\n+        );\n+\n+        assert_expansion(&rules, \"foo! { foo, bar }\", \"fn baz {foo () ; bar () ;}\");\n+    }\n+\n     #[test]\n     fn expand_to_item_list() {\n         let rules = create_rules(\n@@ -415,7 +462,7 @@ SOURCE_FILE@[0; 40)\n         assert_expansion(\n             &rules,\n             \"foo! { bar::<u8>::baz::<u8> }\",\n-            \"fn foo () {let a = bar ::< u8 > ::baz ::< u8 > ;}\",\n+            \"fn foo () {let a = bar :: < u8 > :: baz :: < u8 > ;}\",\n         );\n     }\n \n@@ -432,4 +479,107 @@ SOURCE_FILE@[0; 40)\n         );\n         assert_expansion(&rules, \"foo! { foo, bar }\", \"fn foo () {let a = foo ; let b = bar ;}\");\n     }\n+\n+    #[test]\n+    fn test_path_with_path() {\n+        let rules = create_rules(\n+            r#\"\n+        macro_rules! foo {\n+            ($ i:path) => {\n+                fn foo() { let a = $ i :: bar; }\n+            }\n+        }\n+\"#,\n+        );\n+        assert_expansion(&rules, \"foo! { foo }\", \"fn foo () {let a = foo :: bar ;}\");\n+    }\n+\n+    #[test]\n+    fn test_expr() {\n+        let rules = create_rules(\n+            r#\"\n+        macro_rules! foo {\n+            ($ i:expr) => {\n+                 fn bar() { $ i; } \n+            }\n+        }\n+\"#,\n+        );\n+\n+        assert_expansion(\n+            &rules,\n+            \"foo! { 2 + 2 * baz(3).quux() }\",\n+            \"fn bar () {2 + 2 * baz (3) . quux () ;}\",\n+        );\n+    }\n+\n+    #[test]\n+    fn test_expr_order() {\n+        let rules = create_rules(\n+            r#\"\n+        macro_rules! foo {\n+            ($ i:expr) => {\n+                 fn bar() { $ i * 2; } \n+            }\n+        }\n+\"#,\n+        );\n+\n+        assert_eq!(\n+            expand_to_syntax(&rules, \"foo! { 1 + 1  }\").syntax().debug_dump().trim(),\n+            r#\"SOURCE_FILE@[0; 15)\n+  FN_DEF@[0; 15)\n+    FN_KW@[0; 2) \"fn\"\n+    NAME@[2; 5)\n+      IDENT@[2; 5) \"bar\"\n+    PARAM_LIST@[5; 7)\n+      L_PAREN@[5; 6) \"(\"\n+      R_PAREN@[6; 7) \")\"\n+    BLOCK@[7; 15)\n+      L_CURLY@[7; 8) \"{\"\n+      EXPR_STMT@[8; 14)\n+        BIN_EXPR@[8; 13)\n+          BIN_EXPR@[8; 11)\n+            LITERAL@[8; 9)\n+              INT_NUMBER@[8; 9) \"1\"\n+            PLUS@[9; 10) \"+\"\n+            LITERAL@[10; 11)\n+              INT_NUMBER@[10; 11) \"1\"\n+          STAR@[11; 12) \"*\"\n+          LITERAL@[12; 13)\n+            INT_NUMBER@[12; 13) \"2\"\n+        SEMI@[13; 14) \";\"\n+      R_CURLY@[14; 15) \"}\"\"#,\n+        );\n+    }\n+\n+    #[test]\n+    fn test_ty() {\n+        let rules = create_rules(\n+            r#\"\n+        macro_rules! foo {\n+            ($ i:ty) => (\n+                fn bar() -> $ i { unimplemented!() }\n+            )\n+        }\n+\"#,\n+        );\n+        assert_expansion(\n+            &rules,\n+            \"foo! { Baz<u8> }\",\n+            \"fn bar () -> Baz < u8 > {unimplemented ! ()}\",\n+        );\n+    }\n+\n+    #[test]\n+    fn test_pat_() {\n+        let rules = create_rules(\n+            r#\"\n+        macro_rules! foo {\n+            ($ i:pat) => { fn foo() { let $ i; } }\n+        }\n+\"#,\n+        );\n+        assert_expansion(&rules, \"foo! { (a, b) }\", \"fn foo () {let (a , b) ;}\");\n+    }\n }"}, {"sha": "7a259f33861698e8c898014de5334563481d4eaa", "filename": "crates/ra_mbe/src/mbe_expander.rs", "status": "modified", "additions": 13, "deletions": 0, "changes": 13, "blob_url": "https://github.com/rust-lang/rust/blob/5d35f284f5ac70cde5d758e7c63a38eae0fb0b55/crates%2Fra_mbe%2Fsrc%2Fmbe_expander.rs", "raw_url": "https://github.com/rust-lang/rust/raw/5d35f284f5ac70cde5d758e7c63a38eae0fb0b55/crates%2Fra_mbe%2Fsrc%2Fmbe_expander.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fra_mbe%2Fsrc%2Fmbe_expander.rs?ref=5d35f284f5ac70cde5d758e7c63a38eae0fb0b55", "patch": "@@ -144,6 +144,19 @@ fn match_lhs(pattern: &crate::Subtree, input: &mut TtCursor) -> Result<Bindings,\n                                 input.eat_path().ok_or(ExpandError::UnexpectedToken)?.clone();\n                             res.inner.insert(text.clone(), Binding::Simple(path.into()));\n                         }\n+                        \"expr\" => {\n+                            let expr =\n+                                input.eat_expr().ok_or(ExpandError::UnexpectedToken)?.clone();\n+                            res.inner.insert(text.clone(), Binding::Simple(expr.into()));\n+                        }\n+                        \"ty\" => {\n+                            let ty = input.eat_ty().ok_or(ExpandError::UnexpectedToken)?.clone();\n+                            res.inner.insert(text.clone(), Binding::Simple(ty.into()));\n+                        }\n+                        \"pat\" => {\n+                            let pat = input.eat_pat().ok_or(ExpandError::UnexpectedToken)?.clone();\n+                            res.inner.insert(text.clone(), Binding::Simple(pat.into()));\n+                        }\n                         _ => return Err(ExpandError::UnexpectedToken),\n                     }\n                 }"}, {"sha": "13d5d2169e0ac98379367f2bbeebed2323a8c05e", "filename": "crates/ra_mbe/src/subtree_parser.rs", "status": "modified", "additions": 13, "deletions": 2, "changes": 15, "blob_url": "https://github.com/rust-lang/rust/blob/5d35f284f5ac70cde5d758e7c63a38eae0fb0b55/crates%2Fra_mbe%2Fsrc%2Fsubtree_parser.rs", "raw_url": "https://github.com/rust-lang/rust/raw/5d35f284f5ac70cde5d758e7c63a38eae0fb0b55/crates%2Fra_mbe%2Fsrc%2Fsubtree_parser.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fra_mbe%2Fsrc%2Fsubtree_parser.rs?ref=5d35f284f5ac70cde5d758e7c63a38eae0fb0b55", "patch": "@@ -30,12 +30,23 @@ impl<'a> Parser<'a> {\n         self.parse(ra_parser::parse_path)\n     }\n \n+    pub fn parse_expr(self) -> Option<tt::TokenTree> {\n+        self.parse(ra_parser::parse_expr)\n+    }\n+\n+    pub fn parse_ty(self) -> Option<tt::TokenTree> {\n+        self.parse(ra_parser::parse_ty)\n+    }\n+\n+    pub fn parse_pat(self) -> Option<tt::TokenTree> {\n+        self.parse(ra_parser::parse_pat)\n+    }\n+\n     fn parse<F>(self, f: F) -> Option<tt::TokenTree>\n     where\n         F: FnOnce(&dyn TokenSource, &mut dyn TreeSink),\n     {\n-        let mut src = SubtreeTokenSource::new(self.subtree);\n-        src.start_from_nth(*self.cur_pos);\n+        let mut src = SubtreeTokenSource::new(&self.subtree.token_trees[*self.cur_pos..]);\n         let mut sink = OffsetTokenSink { token_pos: 0 };\n \n         f(&src, &mut sink);"}, {"sha": "0a070b46a04a23ed38b3dc07301d40c4e9b0f239", "filename": "crates/ra_mbe/src/subtree_source.rs", "status": "modified", "additions": 166, "deletions": 200, "changes": 366, "blob_url": "https://github.com/rust-lang/rust/blob/5d35f284f5ac70cde5d758e7c63a38eae0fb0b55/crates%2Fra_mbe%2Fsrc%2Fsubtree_source.rs", "raw_url": "https://github.com/rust-lang/rust/raw/5d35f284f5ac70cde5d758e7c63a38eae0fb0b55/crates%2Fra_mbe%2Fsrc%2Fsubtree_source.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fra_mbe%2Fsrc%2Fsubtree_source.rs?ref=5d35f284f5ac70cde5d758e7c63a38eae0fb0b55", "patch": "@@ -2,6 +2,64 @@ use ra_parser::{TokenSource};\n use ra_syntax::{classify_literal, SmolStr, SyntaxKind, SyntaxKind::*};\n use std::cell::{RefCell};\n \n+// A Sequece of Token,\n+#[derive(Debug, Clone, Eq, PartialEq)]\n+pub(super) enum TokenSeq<'a> {\n+    Subtree(&'a tt::Subtree),\n+    Seq(&'a [tt::TokenTree]),\n+}\n+\n+impl<'a> From<&'a tt::Subtree> for TokenSeq<'a> {\n+    fn from(s: &'a tt::Subtree) -> TokenSeq<'a> {\n+        TokenSeq::Subtree(s)\n+    }\n+}\n+\n+impl<'a> From<&'a [tt::TokenTree]> for TokenSeq<'a> {\n+    fn from(s: &'a [tt::TokenTree]) -> TokenSeq<'a> {\n+        TokenSeq::Seq(s)\n+    }\n+}\n+\n+enum DelimToken<'a> {\n+    Delim(&'a tt::Delimiter, bool),\n+    Token(&'a tt::TokenTree),\n+    End,\n+}\n+\n+impl<'a> TokenSeq<'a> {\n+    fn get(&self, pos: usize) -> DelimToken<'a> {\n+        match self {\n+            TokenSeq::Subtree(subtree) => {\n+                let len = subtree.token_trees.len() + 2;\n+                match pos {\n+                    p if p >= len => DelimToken::End,\n+                    p if p == len - 1 => DelimToken::Delim(&subtree.delimiter, true),\n+                    0 => DelimToken::Delim(&subtree.delimiter, false),\n+                    p => DelimToken::Token(&subtree.token_trees[p - 1]),\n+                }\n+            }\n+            TokenSeq::Seq(tokens) => {\n+                tokens.get(pos).map(DelimToken::Token).unwrap_or(DelimToken::End)\n+            }\n+        }\n+    }\n+\n+    fn len(&self) -> usize {\n+        match self {\n+            TokenSeq::Subtree(subtree) => subtree.token_trees.len() + 2,\n+            TokenSeq::Seq(tokens) => tokens.len(),\n+        }\n+    }\n+\n+    fn child_slice(&self) -> &[tt::TokenTree] {\n+        match self {\n+            TokenSeq::Subtree(subtree) => &subtree.token_trees,\n+            TokenSeq::Seq(tokens) => &tokens,\n+        }\n+    }\n+}\n+\n #[derive(Debug, Clone, Eq, PartialEq)]\n struct TtToken {\n     pub kind: SyntaxKind,\n@@ -12,29 +70,27 @@ struct TtToken {\n \n #[derive(Debug, Clone, Eq, PartialEq)]\n enum WalkCursor {\n-    DelimiterBegin(Option<TtToken>),\n-    Token(usize, Option<TtToken>),\n-    DelimiterEnd(Option<TtToken>),\n+    Token(usize, TtToken),\n     Eof,\n }\n \n #[derive(Debug)]\n struct SubTreeWalker<'a> {\n     pos: usize,\n-    stack: Vec<(&'a tt::Subtree, Option<usize>)>,\n+    stack: Vec<(TokenSeq<'a>, usize)>,\n     cursor: WalkCursor,\n     last_steps: Vec<usize>,\n-    subtree: &'a tt::Subtree,\n+    ts: TokenSeq<'a>,\n }\n \n impl<'a> SubTreeWalker<'a> {\n-    fn new(subtree: &tt::Subtree) -> SubTreeWalker {\n+    fn new(ts: TokenSeq<'a>) -> SubTreeWalker {\n         let mut res = SubTreeWalker {\n             pos: 0,\n             stack: vec![],\n             cursor: WalkCursor::Eof,\n             last_steps: vec![],\n-            subtree,\n+            ts,\n         };\n \n         res.reset();\n@@ -47,209 +103,108 @@ impl<'a> SubTreeWalker<'a> {\n \n     fn reset(&mut self) {\n         self.pos = 0;\n-        self.stack = vec![(self.subtree, None)];\n-        self.cursor = WalkCursor::DelimiterBegin(convert_delim(self.subtree.delimiter, false));\n+        self.stack = vec![];\n         self.last_steps = vec![];\n \n-        while self.is_empty_delimiter() {\n-            self.forward_unchecked();\n-        }\n-    }\n-\n-    // This funciton will fast forward the cursor,\n-    // Such that backward will stop at `start_pos` point\n-    fn start_from_nth(&mut self, start_pos: usize) {\n-        self.reset();\n-        self.pos = start_pos;\n-        self.cursor = self.walk_token(start_pos, 0, false);\n-\n-        while self.is_empty_delimiter() {\n-            self.forward_unchecked();\n+        self.cursor = match self.ts.get(0) {\n+            DelimToken::Token(token) => match token {\n+                tt::TokenTree::Subtree(subtree) => {\n+                    let ts = TokenSeq::from(subtree);\n+                    self.stack.push((ts, 0));\n+                    WalkCursor::Token(0, convert_delim(subtree.delimiter, false))\n+                }\n+                tt::TokenTree::Leaf(leaf) => {\n+                    let next_tokens = self.ts.child_slice();\n+                    WalkCursor::Token(0, convert_leaf(&next_tokens, leaf))\n+                }\n+            },\n+            DelimToken::Delim(delim, is_end) => {\n+                assert!(!is_end);\n+                WalkCursor::Token(0, convert_delim(*delim, false))\n+            }\n+            DelimToken::End => WalkCursor::Eof,\n         }\n     }\n \n     fn current(&self) -> Option<&TtToken> {\n         match &self.cursor {\n-            WalkCursor::DelimiterBegin(t) => t.as_ref(),\n-            WalkCursor::Token(_, t) => t.as_ref(),\n-            WalkCursor::DelimiterEnd(t) => t.as_ref(),\n+            WalkCursor::Token(_, t) => Some(t),\n             WalkCursor::Eof => None,\n         }\n     }\n \n-    fn is_empty_delimiter(&self) -> bool {\n-        match &self.cursor {\n-            WalkCursor::DelimiterBegin(None) => true,\n-            WalkCursor::DelimiterEnd(None) => true,\n-            _ => false,\n-        }\n+    fn top(&self) -> &TokenSeq {\n+        self.stack.last().map(|(t, _)| t).unwrap_or(&self.ts)\n     }\n \n-    /// Move cursor backward by 1 step with empty checking\n+    /// Move cursor backward by 1 step\n     fn backward(&mut self) {\n         if self.last_steps.is_empty() {\n             return;\n         }\n+\n         self.pos -= 1;\n-        loop {\n-            self.backward_unchecked();\n-            // Skip Empty delimiter\n-            if self.last_steps.is_empty() || !self.is_empty_delimiter() {\n-                break;\n-            }\n-        }\n+        let last_step = self.last_steps.pop().unwrap();\n \n-        // Move forward if it is empty delimiter\n-        if self.last_steps.is_empty() {\n-            while self.is_empty_delimiter() {\n-                self.forward_unchecked();\n+        self.cursor = match self.cursor {\n+            WalkCursor::Token(idx, _) => self.walk_token(idx, last_step, true),\n+            WalkCursor::Eof => {\n+                let len = self.top().len();\n+                self.walk_token(len, last_step, true)\n             }\n         }\n     }\n \n-    /// Move cursor backward by 1 step without empty check\n-    ///\n-    /// Depends on the current state of cursor:\n-    ///\n-    /// * Delimiter Begin => Pop the stack, goto last walking token  (`walk_token`)\n-    /// * Token => Goto prev token  (`walk_token`)\n-    /// * Delimiter End => Goto the last child token (`walk_token`)\n-    /// * Eof => push the root subtree, and set it as Delimiter End\n-    fn backward_unchecked(&mut self) {\n-        if self.last_steps.is_empty() {\n-            return;\n-        }\n-\n-        let last_step = self.last_steps.pop().unwrap();\n-        let do_walk_token = match self.cursor {\n-            WalkCursor::DelimiterBegin(_) => None,\n-            WalkCursor::Token(u, _) => Some(u),\n-            WalkCursor::DelimiterEnd(_) => {\n-                let (top, _) = self.stack.last().unwrap();\n-                Some(top.token_trees.len())\n-            }\n-            WalkCursor::Eof => None,\n-        };\n-\n-        self.cursor = match do_walk_token {\n-            Some(u) => self.walk_token(u, last_step, true),\n-            None => match self.cursor {\n-                WalkCursor::Eof => {\n-                    self.stack.push((self.subtree, None));\n-                    WalkCursor::DelimiterEnd(convert_delim(\n-                        self.stack.last().unwrap().0.delimiter,\n-                        true,\n-                    ))\n-                }\n-                _ => {\n-                    let (_, last_top_cursor) = self.stack.pop().unwrap();\n-                    assert!(!self.stack.is_empty());\n-\n-                    self.walk_token(last_top_cursor.unwrap(), last_step, true)\n-                }\n-            },\n-        };\n-    }\n-\n-    /// Move cursor forward by 1 step with empty checking\n+    /// Move cursor forward by 1 step        \n     fn forward(&mut self) {\n         if self.is_eof() {\n             return;\n         }\n-\n         self.pos += 1;\n-        loop {\n-            self.forward_unchecked();\n-            if !self.is_empty_delimiter() {\n-                break;\n-            }\n-        }\n-    }\n-\n-    /// Move cursor forward by 1 step without empty checking\n-    ///\n-    /// Depends on the current state of cursor:\n-    ///\n-    /// * Delimiter Begin => Goto the first child token (`walk_token`)\n-    /// * Token => Goto next token  (`walk_token`)\n-    /// * Delimiter End => Pop the stack, goto last walking token  (`walk_token`)\n-    ///   \n-    fn forward_unchecked(&mut self) {\n-        if self.is_eof() {\n-            return;\n-        }\n \n         let step = self.current().map(|x| x.n_tokens).unwrap_or(1);\n         self.last_steps.push(step);\n \n-        let do_walk_token = match self.cursor {\n-            WalkCursor::DelimiterBegin(_) => Some((0, 0)),\n-            WalkCursor::Token(u, _) => Some((u, step)),\n-            WalkCursor::DelimiterEnd(_) => None,\n-            _ => unreachable!(),\n-        };\n-\n-        self.cursor = match do_walk_token {\n-            Some((u, step)) => self.walk_token(u, step, false),\n-            None => {\n-                let (_, last_top_idx) = self.stack.pop().unwrap();\n-                match self.stack.last() {\n-                    Some(_) => self.walk_token(last_top_idx.unwrap(), 1, false),\n-                    None => WalkCursor::Eof,\n-                }\n-            }\n-        };\n+        if let WalkCursor::Token(u, _) = self.cursor {\n+            self.cursor = self.walk_token(u, step, false)\n+        }\n     }\n \n     /// Traversal child token\n-    /// Depends on the new position, it returns:\n-    ///\n-    /// * new position < 0 => DelimiterBegin\n-    /// * new position > token_tree.len() => DelimiterEnd\n-    /// * if new position is a subtree, depends on traversal direction:\n-    /// ** backward => DelimiterEnd\n-    /// ** forward => DelimiterBegin\n-    /// * if new psoition is a leaf, return walk_leaf()\n     fn walk_token(&mut self, pos: usize, offset: usize, backward: bool) -> WalkCursor {\n-        let (top, _) = self.stack.last().unwrap();\n+        let top = self.stack.last().map(|(t, _)| t).unwrap_or(&self.ts);\n \n         if backward && pos < offset {\n-            return WalkCursor::DelimiterBegin(convert_delim(\n-                self.stack.last().unwrap().0.delimiter,\n-                false,\n-            ));\n-        }\n-\n-        if !backward && pos + offset >= top.token_trees.len() {\n-            return WalkCursor::DelimiterEnd(convert_delim(\n-                self.stack.last().unwrap().0.delimiter,\n-                true,\n-            ));\n+            let (_, last_idx) = self.stack.pop().unwrap();\n+            return self.walk_token(last_idx, offset, backward);\n         }\n \n         let pos = if backward { pos - offset } else { pos + offset };\n \n-        match &top.token_trees[pos] {\n-            tt::TokenTree::Subtree(subtree) => {\n-                self.stack.push((subtree, Some(pos)));\n-                let delim = convert_delim(self.stack.last().unwrap().0.delimiter, backward);\n-                if backward {\n-                    WalkCursor::DelimiterEnd(delim)\n-                } else {\n-                    WalkCursor::DelimiterBegin(delim)\n+        match top.get(pos) {\n+            DelimToken::Token(token) => match token {\n+                tt::TokenTree::Subtree(subtree) => {\n+                    let ts = TokenSeq::from(subtree);\n+                    let new_idx = if backward { ts.len() - 1 } else { 0 };\n+                    self.stack.push((ts, pos));\n+                    WalkCursor::Token(new_idx, convert_delim(subtree.delimiter, backward))\n+                }\n+                tt::TokenTree::Leaf(leaf) => {\n+                    let next_tokens = top.child_slice();\n+                    WalkCursor::Token(pos, convert_leaf(&next_tokens[pos..], leaf))\n                 }\n+            },\n+            DelimToken::Delim(delim, is_end) => {\n+                WalkCursor::Token(pos, convert_delim(*delim, is_end))\n             }\n-            tt::TokenTree::Leaf(leaf) => WalkCursor::Token(pos, Some(self.walk_leaf(leaf, pos))),\n-        }\n-    }\n-\n-    fn walk_leaf(&mut self, leaf: &tt::Leaf, pos: usize) -> TtToken {\n-        match leaf {\n-            tt::Leaf::Literal(l) => convert_literal(l),\n-            tt::Leaf::Ident(ident) => convert_ident(ident),\n-            tt::Leaf::Punct(punct) => {\n-                let (top, _) = self.stack.last().unwrap();\n-                convert_punct(punct, top, pos)\n+            DelimToken::End => {\n+                // it is the top level\n+                if let Some((_, last_idx)) = self.stack.pop() {\n+                    assert!(!backward);\n+                    self.walk_token(last_idx, offset, backward)\n+                } else {\n+                    WalkCursor::Eof\n+                }\n             }\n         }\n     }\n@@ -263,27 +218,20 @@ pub(crate) trait Querier {\n #[derive(Debug)]\n pub(crate) struct WalkerOwner<'a> {\n     walker: RefCell<SubTreeWalker<'a>>,\n-    offset: usize,\n }\n \n impl<'a> WalkerOwner<'a> {\n-    fn new(subtree: &'a tt::Subtree) -> Self {\n-        WalkerOwner { walker: RefCell::new(SubTreeWalker::new(subtree)), offset: 0 }\n+    fn new<I: Into<TokenSeq<'a>>>(ts: I) -> Self {\n+        WalkerOwner { walker: RefCell::new(SubTreeWalker::new(ts.into())) }\n     }\n \n     fn get<'b>(&self, pos: usize) -> Option<TtToken> {\n-        self.set_walker_pos(pos);\n+        self.set_pos(pos);\n         let walker = self.walker.borrow();\n         walker.current().cloned()\n     }\n \n-    fn start_from_nth(&mut self, pos: usize) {\n-        self.offset = pos;\n-        self.walker.borrow_mut().start_from_nth(pos);\n-    }\n-\n-    fn set_walker_pos(&self, mut pos: usize) {\n-        pos += self.offset;\n+    fn set_pos(&self, pos: usize) {\n         let mut walker = self.walker.borrow_mut();\n         while pos > walker.pos && !walker.is_eof() {\n             walker.forward();\n@@ -294,19 +242,26 @@ impl<'a> WalkerOwner<'a> {\n     }\n \n     fn collect_token_trees(&mut self, n: usize) -> Vec<&tt::TokenTree> {\n-        self.start_from_nth(self.offset);\n-\n         let mut res = vec![];\n         let mut walker = self.walker.borrow_mut();\n+        walker.reset();\n \n-        while walker.pos - self.offset < n {\n+        while walker.pos < n {\n             if let WalkCursor::Token(u, tt) = &walker.cursor {\n-                if walker.stack.len() == 1 {\n-                    // We only collect the topmost child\n-                    res.push(&walker.stack[0].0.token_trees[*u]);\n-                    if let Some(tt) = tt {\n-                        for i in 0..tt.n_tokens - 1 {\n-                            res.push(&walker.stack[0].0.token_trees[u + i]);\n+                // We only collect the topmost child\n+                if walker.stack.len() == 0 {\n+                    for i in 0..tt.n_tokens {\n+                        if let DelimToken::Token(token) = walker.ts.get(u + i) {\n+                            res.push(token);\n+                        }\n+                    }\n+                } else if walker.stack.len() == 1 {\n+                    if let DelimToken::Delim(_, is_end) = walker.top().get(*u) {\n+                        if !is_end {\n+                            let (_, last_idx) = &walker.stack[0];\n+                            if let DelimToken::Token(token) = walker.ts.get(*last_idx) {\n+                                res.push(token);\n+                            }\n                         }\n                     }\n                 }\n@@ -331,12 +286,8 @@ pub(crate) struct SubtreeTokenSource<'a> {\n }\n \n impl<'a> SubtreeTokenSource<'a> {\n-    pub fn new(subtree: &tt::Subtree) -> SubtreeTokenSource {\n-        SubtreeTokenSource { walker: WalkerOwner::new(subtree) }\n-    }\n-\n-    pub fn start_from_nth(&mut self, n: usize) {\n-        self.walker.start_from_nth(n);\n+    pub fn new<I: Into<TokenSeq<'a>>>(ts: I) -> SubtreeTokenSource<'a> {\n+        SubtreeTokenSource { walker: WalkerOwner::new(ts) }\n     }\n \n     pub fn querier<'b>(&'a self) -> &'b WalkerOwner<'a>\n@@ -361,10 +312,16 @@ impl<'a> TokenSource for SubtreeTokenSource<'a> {\n         }\n     }\n     fn is_token_joint_to_next(&self, pos: usize) -> bool {\n-        self.walker.get(pos).unwrap().is_joint_to_next\n+        match self.walker.get(pos) {\n+            Some(t) => t.is_joint_to_next,\n+            _ => false,\n+        }\n     }\n     fn is_keyword(&self, pos: usize, kw: &str) -> bool {\n-        self.walker.get(pos).unwrap().text == *kw\n+        match self.walker.get(pos) {\n+            Some(t) => t.text == *kw,\n+            _ => false,\n+        }\n     }\n }\n \n@@ -467,18 +424,18 @@ where\n     None\n }\n \n-fn convert_delim(d: tt::Delimiter, closing: bool) -> Option<TtToken> {\n+fn convert_delim(d: tt::Delimiter, closing: bool) -> TtToken {\n     let (kinds, texts) = match d {\n         tt::Delimiter::Parenthesis => ([L_PAREN, R_PAREN], \"()\"),\n         tt::Delimiter::Brace => ([L_CURLY, R_CURLY], \"{}\"),\n         tt::Delimiter::Bracket => ([L_BRACK, R_BRACK], \"[]\"),\n-        tt::Delimiter::None => return None,\n+        tt::Delimiter::None => ([L_DOLLAR, R_DOLLAR], \"\"),\n     };\n \n     let idx = closing as usize;\n     let kind = kinds[idx];\n-    let text = &texts[idx..texts.len() - (1 - idx)];\n-    Some(TtToken { kind, is_joint_to_next: false, text: SmolStr::new(text), n_tokens: 1 })\n+    let text = if texts.len() > 0 { &texts[idx..texts.len() - (1 - idx)] } else { \"\" };\n+    TtToken { kind, is_joint_to_next: false, text: SmolStr::new(text), n_tokens: 1 }\n }\n \n fn convert_literal(l: &tt::Literal) -> TtToken {\n@@ -495,8 +452,9 @@ fn convert_ident(ident: &tt::Ident) -> TtToken {\n     TtToken { kind, is_joint_to_next: false, text: ident.text.clone(), n_tokens: 1 }\n }\n \n-fn convert_punct(p: &tt::Punct, parent: &tt::Subtree, next: usize) -> TtToken {\n-    let iter = parent.token_trees[next + 1..].iter();\n+fn convert_punct(p: &tt::Punct, next_tokens: &[tt::TokenTree]) -> TtToken {\n+    let mut iter = next_tokens.iter();\n+    iter.next();\n     let mut peek = TokenPeek::new(iter);\n \n     if let Some((kind, is_joint_to_next, text, size)) = convert_multi_char_punct(p, &mut peek) {\n@@ -519,3 +477,11 @@ fn convert_punct(p: &tt::Punct, parent: &tt::Subtree, next: usize) -> TtToken {\n         TtToken { kind, is_joint_to_next: p.spacing == tt::Spacing::Joint, text, n_tokens: 1 }\n     }\n }\n+\n+fn convert_leaf(tokens: &[tt::TokenTree], leaf: &tt::Leaf) -> TtToken {\n+    match leaf {\n+        tt::Leaf::Literal(l) => convert_literal(l),\n+        tt::Leaf::Ident(ident) => convert_ident(ident),\n+        tt::Leaf::Punct(punct) => convert_punct(punct, tokens),\n+    }\n+}"}, {"sha": "28ded7870e1309c64696ac19e5c9eaa9ad829d3f", "filename": "crates/ra_mbe/src/syntax_bridge.rs", "status": "modified", "additions": 31, "deletions": 16, "changes": 47, "blob_url": "https://github.com/rust-lang/rust/blob/5d35f284f5ac70cde5d758e7c63a38eae0fb0b55/crates%2Fra_mbe%2Fsrc%2Fsyntax_bridge.rs", "raw_url": "https://github.com/rust-lang/rust/raw/5d35f284f5ac70cde5d758e7c63a38eae0fb0b55/crates%2Fra_mbe%2Fsrc%2Fsyntax_bridge.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fra_mbe%2Fsrc%2Fsyntax_bridge.rs?ref=5d35f284f5ac70cde5d758e7c63a38eae0fb0b55", "patch": "@@ -22,6 +22,14 @@ pub fn ast_to_token_tree(ast: &ast::TokenTree) -> Option<(tt::Subtree, TokenMap)\n     Some((tt, token_map))\n }\n \n+/// Convert the syntax node to a `TokenTree` (what macro\n+/// will consume).\n+pub fn syntax_node_to_token_tree(node: &SyntaxNode) -> Option<(tt::Subtree, TokenMap)> {\n+    let mut token_map = TokenMap::default();\n+    let tt = convert_tt(&mut token_map, node.range().start(), node)?;\n+    Some((tt, token_map))\n+}\n+\n /// Parses the token tree (result of macro expansion) as a sequence of items\n pub fn token_tree_to_ast_item_list(tt: &tt::Subtree) -> TreeArc<ast::SourceFile> {\n     let token_source = SubtreeTokenSource::new(tt);\n@@ -51,15 +59,17 @@ fn convert_tt(\n ) -> Option<tt::Subtree> {\n     let first_child = tt.first_child_or_token()?;\n     let last_child = tt.last_child_or_token()?;\n-    let delimiter = match (first_child.kind(), last_child.kind()) {\n-        (L_PAREN, R_PAREN) => tt::Delimiter::Parenthesis,\n-        (L_CURLY, R_CURLY) => tt::Delimiter::Brace,\n-        (L_BRACK, R_BRACK) => tt::Delimiter::Bracket,\n-        _ => return None,\n+    let (delimiter, skip_first) = match (first_child.kind(), last_child.kind()) {\n+        (L_PAREN, R_PAREN) => (tt::Delimiter::Parenthesis, true),\n+        (L_CURLY, R_CURLY) => (tt::Delimiter::Brace, true),\n+        (L_BRACK, R_BRACK) => (tt::Delimiter::Bracket, true),\n+        _ => (tt::Delimiter::None, false),\n     };\n+\n     let mut token_trees = Vec::new();\n-    for child in tt.children_with_tokens().skip(1) {\n-        if child == first_child || child == last_child || child.kind().is_trivia() {\n+    for child in tt.children_with_tokens().skip(skip_first as usize) {\n+        if (skip_first && (child == first_child || child == last_child)) || child.kind().is_trivia()\n+        {\n             continue;\n         }\n         match child {\n@@ -127,6 +137,11 @@ impl<'a, Q: Querier> TtTreeSink<'a, Q> {\n \n impl<'a, Q: Querier> TreeSink for TtTreeSink<'a, Q> {\n     fn token(&mut self, kind: SyntaxKind, n_tokens: u8) {\n+        if kind == L_DOLLAR || kind == R_DOLLAR {\n+            self.token_pos += n_tokens as usize;\n+            return;\n+        }\n+\n         for _ in 0..n_tokens {\n             self.buf += &self.src_querier.token(self.token_pos).1;\n             self.token_pos += 1;\n@@ -176,19 +191,19 @@ mod tests {\n \n         let query = tt_src.querier();\n \n-        // [{]\n+        // [${]\n         // [let] [a] [=] ['c'] [;]\n-        assert_eq!(query.token(1 + 3).1, \"'c'\");\n-        assert_eq!(query.token(1 + 3).0, CHAR);\n+        assert_eq!(query.token(2 + 3).1, \"'c'\");\n+        assert_eq!(query.token(2 + 3).0, CHAR);\n         // [let] [c] [=] [1000] [;]\n-        assert_eq!(query.token(1 + 5 + 3).1, \"1000\");\n-        assert_eq!(query.token(1 + 5 + 3).0, INT_NUMBER);\n+        assert_eq!(query.token(2 + 5 + 3).1, \"1000\");\n+        assert_eq!(query.token(2 + 5 + 3).0, INT_NUMBER);\n         // [let] [f] [=] [12E+99_f64] [;]\n-        assert_eq!(query.token(1 + 10 + 3).1, \"12E+99_f64\");\n-        assert_eq!(query.token(1 + 10 + 3).0, FLOAT_NUMBER);\n+        assert_eq!(query.token(2 + 10 + 3).1, \"12E+99_f64\");\n+        assert_eq!(query.token(2 + 10 + 3).0, FLOAT_NUMBER);\n \n         // [let] [s] [=] [\"rust1\"] [;]\n-        assert_eq!(query.token(1 + 15 + 3).1, \"\\\"rust1\\\"\");\n-        assert_eq!(query.token(1 + 15 + 3).0, STRING);\n+        assert_eq!(query.token(2 + 15 + 3).1, \"\\\"rust1\\\"\");\n+        assert_eq!(query.token(2 + 15 + 3).0, STRING);\n     }\n }"}, {"sha": "f6cefe087e5def73a7f80b4eb567cb107607b438", "filename": "crates/ra_mbe/src/tt_cursor.rs", "status": "modified", "additions": 15, "deletions": 0, "changes": 15, "blob_url": "https://github.com/rust-lang/rust/blob/5d35f284f5ac70cde5d758e7c63a38eae0fb0b55/crates%2Fra_mbe%2Fsrc%2Ftt_cursor.rs", "raw_url": "https://github.com/rust-lang/rust/raw/5d35f284f5ac70cde5d758e7c63a38eae0fb0b55/crates%2Fra_mbe%2Fsrc%2Ftt_cursor.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fra_mbe%2Fsrc%2Ftt_cursor.rs?ref=5d35f284f5ac70cde5d758e7c63a38eae0fb0b55", "patch": "@@ -84,6 +84,21 @@ impl<'a> TtCursor<'a> {\n         parser.parse_path()\n     }\n \n+    pub(crate) fn eat_expr(&mut self) -> Option<tt::TokenTree> {\n+        let parser = Parser::new(&mut self.pos, self.subtree);\n+        parser.parse_expr()\n+    }\n+\n+    pub(crate) fn eat_ty(&mut self) -> Option<tt::TokenTree> {\n+        let parser = Parser::new(&mut self.pos, self.subtree);\n+        parser.parse_ty()\n+    }\n+\n+    pub(crate) fn eat_pat(&mut self) -> Option<tt::TokenTree> {\n+        let parser = Parser::new(&mut self.pos, self.subtree);\n+        parser.parse_pat()\n+    }\n+\n     pub(crate) fn expect_char(&mut self, char: char) -> Result<(), ParseError> {\n         if self.at_char(char) {\n             self.bump();"}, {"sha": "5a7a5514190a6189a62375ad143fdb1438393eae", "filename": "crates/ra_parser/src/grammar.rs", "status": "modified", "additions": 12, "deletions": 0, "changes": 12, "blob_url": "https://github.com/rust-lang/rust/blob/5d35f284f5ac70cde5d758e7c63a38eae0fb0b55/crates%2Fra_parser%2Fsrc%2Fgrammar.rs", "raw_url": "https://github.com/rust-lang/rust/raw/5d35f284f5ac70cde5d758e7c63a38eae0fb0b55/crates%2Fra_parser%2Fsrc%2Fgrammar.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fra_parser%2Fsrc%2Fgrammar.rs?ref=5d35f284f5ac70cde5d758e7c63a38eae0fb0b55", "patch": "@@ -53,6 +53,18 @@ pub(crate) fn path(p: &mut Parser) {\n     paths::type_path(p);\n }\n \n+pub(crate) fn expr(p: &mut Parser) {\n+    expressions::expr(p);\n+}\n+\n+pub(crate) fn type_(p: &mut Parser) {\n+    types::type_(p)\n+}\n+\n+pub(crate) fn pattern(p: &mut Parser) {\n+    patterns::pattern(p)\n+}\n+\n pub(crate) fn reparser(\n     node: SyntaxKind,\n     first_child: Option<SyntaxKind>,"}, {"sha": "2955773256f3189a95bb0ac4b3ee9fa55173ab6b", "filename": "crates/ra_parser/src/grammar/expressions.rs", "status": "modified", "additions": 41, "deletions": 9, "changes": 50, "blob_url": "https://github.com/rust-lang/rust/blob/5d35f284f5ac70cde5d758e7c63a38eae0fb0b55/crates%2Fra_parser%2Fsrc%2Fgrammar%2Fexpressions.rs", "raw_url": "https://github.com/rust-lang/rust/raw/5d35f284f5ac70cde5d758e7c63a38eae0fb0b55/crates%2Fra_parser%2Fsrc%2Fgrammar%2Fexpressions.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fra_parser%2Fsrc%2Fgrammar%2Fexpressions.rs?ref=5d35f284f5ac70cde5d758e7c63a38eae0fb0b55", "patch": "@@ -8,17 +8,20 @@ const EXPR_FIRST: TokenSet = LHS_FIRST;\n \n pub(super) fn expr(p: &mut Parser) -> BlockLike {\n     let r = Restrictions { forbid_structs: false, prefer_stmt: false };\n-    expr_bp(p, r, 1).1\n+    let mut dollar_lvl = 0;\n+    expr_bp(p, r, 1, &mut dollar_lvl).1\n }\n \n pub(super) fn expr_stmt(p: &mut Parser) -> (Option<CompletedMarker>, BlockLike) {\n     let r = Restrictions { forbid_structs: false, prefer_stmt: true };\n-    expr_bp(p, r, 1)\n+    let mut dollar_lvl = 0;\n+    expr_bp(p, r, 1, &mut dollar_lvl)\n }\n \n fn expr_no_struct(p: &mut Parser) {\n     let r = Restrictions { forbid_structs: true, prefer_stmt: false };\n-    expr_bp(p, r, 1);\n+    let mut dollar_lvl = 0;\n+    expr_bp(p, r, 1, &mut dollar_lvl);\n }\n \n // test block\n@@ -206,8 +209,23 @@ fn current_op(p: &Parser) -> (u8, Op) {\n }\n \n // Parses expression with binding power of at least bp.\n-fn expr_bp(p: &mut Parser, r: Restrictions, bp: u8) -> (Option<CompletedMarker>, BlockLike) {\n-    let mut lhs = match lhs(p, r) {\n+fn expr_bp(\n+    p: &mut Parser,\n+    r: Restrictions,\n+    mut bp: u8,\n+    dollar_lvl: &mut usize,\n+) -> (Option<CompletedMarker>, BlockLike) {\n+    // `newly_dollar_open` is a flag indicated that dollar is just closed after lhs, e.g.\n+    // `$1$ + a`\n+    // We use this flag to skip handling it.\n+    let mut newly_dollar_open = false;\n+\n+    if p.at_l_dollar() {\n+        *dollar_lvl += p.eat_l_dollars();\n+        newly_dollar_open = true;\n+    }\n+\n+    let mut lhs = match lhs(p, r, dollar_lvl) {\n         Some((lhs, blocklike)) => {\n             // test stmt_bin_expr_ambiguity\n             // fn foo() {\n@@ -223,6 +241,15 @@ fn expr_bp(p: &mut Parser, r: Restrictions, bp: u8) -> (Option<CompletedMarker>,\n     };\n \n     loop {\n+        if *dollar_lvl > 0 && p.at_r_dollar() {\n+            *dollar_lvl -= p.eat_r_dollars(*dollar_lvl);\n+            if !newly_dollar_open {\n+                // We \"pump\" bp for make it highest priority\n+                bp = 255;\n+            }\n+            newly_dollar_open = false;\n+        }\n+\n         let is_range = p.current() == DOTDOT || p.current() == DOTDOTEQ;\n         let (op_bp, op) = current_op(p);\n         if op_bp < bp {\n@@ -235,7 +262,8 @@ fn expr_bp(p: &mut Parser, r: Restrictions, bp: u8) -> (Option<CompletedMarker>,\n                 p.bump_compound(kind, n);\n             }\n         }\n-        expr_bp(p, r, op_bp + 1);\n+\n+        expr_bp(p, r, op_bp + 1, dollar_lvl);\n         lhs = m.complete(p, if is_range { RANGE_EXPR } else { BIN_EXPR });\n     }\n     (Some(lhs), BlockLike::NotBlock)\n@@ -244,7 +272,11 @@ fn expr_bp(p: &mut Parser, r: Restrictions, bp: u8) -> (Option<CompletedMarker>,\n const LHS_FIRST: TokenSet =\n     atom::ATOM_EXPR_FIRST.union(token_set![AMP, STAR, EXCL, DOTDOT, DOTDOTEQ, MINUS]);\n \n-fn lhs(p: &mut Parser, r: Restrictions) -> Option<(CompletedMarker, BlockLike)> {\n+fn lhs(\n+    p: &mut Parser,\n+    r: Restrictions,\n+    dollar_lvl: &mut usize,\n+) -> Option<(CompletedMarker, BlockLike)> {\n     let m;\n     let kind = match p.current() {\n         // test ref_expr\n@@ -275,7 +307,7 @@ fn lhs(p: &mut Parser, r: Restrictions) -> Option<(CompletedMarker, BlockLike)>\n             m = p.start();\n             p.bump();\n             if p.at_ts(EXPR_FIRST) {\n-                expr_bp(p, r, 2);\n+                expr_bp(p, r, 2, dollar_lvl);\n             }\n             return Some((m.complete(p, RANGE_EXPR), BlockLike::NotBlock));\n         }\n@@ -287,7 +319,7 @@ fn lhs(p: &mut Parser, r: Restrictions) -> Option<(CompletedMarker, BlockLike)>\n             ));\n         }\n     };\n-    expr_bp(p, r, 255);\n+    expr_bp(p, r, 255, dollar_lvl);\n     Some((m.complete(p, kind), BlockLike::NotBlock))\n }\n "}, {"sha": "03fa9b71e3011cf5f86aee7c0b93e0c0d39901e3", "filename": "crates/ra_parser/src/grammar/patterns.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/5d35f284f5ac70cde5d758e7c63a38eae0fb0b55/crates%2Fra_parser%2Fsrc%2Fgrammar%2Fpatterns.rs", "raw_url": "https://github.com/rust-lang/rust/raw/5d35f284f5ac70cde5d758e7c63a38eae0fb0b55/crates%2Fra_parser%2Fsrc%2Fgrammar%2Fpatterns.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fra_parser%2Fsrc%2Fgrammar%2Fpatterns.rs?ref=5d35f284f5ac70cde5d758e7c63a38eae0fb0b55", "patch": "@@ -5,7 +5,7 @@ pub(super) const PATTERN_FIRST: TokenSet = expressions::LITERAL_FIRST\n     .union(token_set![REF_KW, MUT_KW, L_PAREN, L_BRACK, AMP, UNDERSCORE, MINUS]);\n \n pub(super) fn pattern(p: &mut Parser) {\n-    pattern_r(p, PAT_RECOVERY_SET)\n+    pattern_r(p, PAT_RECOVERY_SET);\n }\n \n /// Parses a pattern list separated by pipes `|`"}, {"sha": "56755c394b59cc2e276d999f8cd7ae77cba764da", "filename": "crates/ra_parser/src/lib.rs", "status": "modified", "additions": 26, "deletions": 7, "changes": 33, "blob_url": "https://github.com/rust-lang/rust/blob/5d35f284f5ac70cde5d758e7c63a38eae0fb0b55/crates%2Fra_parser%2Fsrc%2Flib.rs", "raw_url": "https://github.com/rust-lang/rust/raw/5d35f284f5ac70cde5d758e7c63a38eae0fb0b55/crates%2Fra_parser%2Fsrc%2Flib.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fra_parser%2Fsrc%2Flib.rs?ref=5d35f284f5ac70cde5d758e7c63a38eae0fb0b55", "patch": "@@ -53,20 +53,39 @@ pub trait TreeSink {\n     fn error(&mut self, error: ParseError);\n }\n \n-/// Parse given tokens into the given sink as a rust file.\n-pub fn parse(token_source: &dyn TokenSource, tree_sink: &mut dyn TreeSink) {\n+fn parse_from_tokens<F>(token_source: &dyn TokenSource, tree_sink: &mut dyn TreeSink, f: F)\n+where\n+    F: FnOnce(&mut parser::Parser),\n+{\n     let mut p = parser::Parser::new(token_source);\n-    grammar::root(&mut p);\n+    f(&mut p);\n     let events = p.finish();\n     event::process(tree_sink, events);\n }\n \n+/// Parse given tokens into the given sink as a rust file.\n+pub fn parse(token_source: &dyn TokenSource, tree_sink: &mut dyn TreeSink) {\n+    parse_from_tokens(token_source, tree_sink, grammar::root);\n+}\n+\n /// Parse given tokens into the given sink as a path\n pub fn parse_path(token_source: &dyn TokenSource, tree_sink: &mut dyn TreeSink) {\n-    let mut p = parser::Parser::new(token_source);\n-    grammar::path(&mut p);\n-    let events = p.finish();\n-    event::process(tree_sink, events);\n+    parse_from_tokens(token_source, tree_sink, grammar::path);\n+}\n+\n+/// Parse given tokens into the given sink as a expression\n+pub fn parse_expr(token_source: &dyn TokenSource, tree_sink: &mut dyn TreeSink) {\n+    parse_from_tokens(token_source, tree_sink, grammar::expr);\n+}\n+\n+/// Parse given tokens into the given sink as a ty\n+pub fn parse_ty(token_source: &dyn TokenSource, tree_sink: &mut dyn TreeSink) {\n+    parse_from_tokens(token_source, tree_sink, grammar::type_);\n+}\n+\n+/// Parse given tokens into the given sink as a pattern\n+pub fn parse_pat(token_source: &dyn TokenSource, tree_sink: &mut dyn TreeSink) {\n+    parse_from_tokens(token_source, tree_sink, grammar::pattern);\n }\n \n /// A parsing function for a specific braced-block."}, {"sha": "71f1f8b302a40f185eda0418c854910006e16a87", "filename": "crates/ra_parser/src/parser.rs", "status": "modified", "additions": 84, "deletions": 6, "changes": 90, "blob_url": "https://github.com/rust-lang/rust/blob/5d35f284f5ac70cde5d758e7c63a38eae0fb0b55/crates%2Fra_parser%2Fsrc%2Fparser.rs", "raw_url": "https://github.com/rust-lang/rust/raw/5d35f284f5ac70cde5d758e7c63a38eae0fb0b55/crates%2Fra_parser%2Fsrc%2Fparser.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fra_parser%2Fsrc%2Fparser.rs?ref=5d35f284f5ac70cde5d758e7c63a38eae0fb0b55", "patch": "@@ -45,8 +45,9 @@ impl<'t> Parser<'t> {\n     ///\n     /// Useful for parsing things like `>>`.\n     pub(crate) fn current2(&self) -> Option<(SyntaxKind, SyntaxKind)> {\n-        let c1 = self.token_source.token_kind(self.token_pos);\n-        let c2 = self.token_source.token_kind(self.token_pos + 1);\n+        let c1 = self.nth(0);\n+        let c2 = self.nth(1);\n+\n         if self.token_source.is_token_joint_to_next(self.token_pos) {\n             Some((c1, c2))\n         } else {\n@@ -59,9 +60,9 @@ impl<'t> Parser<'t> {\n     ///\n     /// Useful for parsing things like `=>>`.\n     pub(crate) fn current3(&self) -> Option<(SyntaxKind, SyntaxKind, SyntaxKind)> {\n-        let c1 = self.token_source.token_kind(self.token_pos);\n-        let c2 = self.token_source.token_kind(self.token_pos + 1);\n-        let c3 = self.token_source.token_kind(self.token_pos + 2);\n+        let c1 = self.nth(0);\n+        let c2 = self.nth(1);\n+        let c3 = self.nth(2);\n         if self.token_source.is_token_joint_to_next(self.token_pos)\n             && self.token_source.is_token_joint_to_next(self.token_pos + 1)\n         {\n@@ -77,7 +78,23 @@ impl<'t> Parser<'t> {\n         let steps = self.steps.get();\n         assert!(steps <= 10_000_000, \"the parser seems stuck\");\n         self.steps.set(steps + 1);\n-        self.token_source.token_kind(self.token_pos + n)\n+\n+        // It is beecause the Dollar will appear between nth\n+        // Following code skips through it\n+        let mut non_dollars_count = 0;\n+        let mut i = 0;\n+\n+        loop {\n+            let kind = self.token_source.token_kind(self.token_pos + i);\n+            i += 1;\n+\n+            match kind {\n+                EOF => return EOF,\n+                SyntaxKind::L_DOLLAR | SyntaxKind::R_DOLLAR => {}\n+                _ if non_dollars_count == n => return kind,\n+                _ => non_dollars_count += 1,\n+            }\n+        }\n     }\n \n     /// Checks if the current token is `kind`.\n@@ -180,13 +197,74 @@ impl<'t> Parser<'t> {\n     }\n \n     fn do_bump(&mut self, kind: SyntaxKind, n_raw_tokens: u8) {\n+        self.eat_dollars();\n         self.token_pos += usize::from(n_raw_tokens);\n         self.push_event(Event::Token { kind, n_raw_tokens });\n     }\n \n     fn push_event(&mut self, event: Event) {\n         self.events.push(event)\n     }\n+\n+    fn eat_dollars(&mut self) {\n+        loop {\n+            match self.token_source.token_kind(self.token_pos) {\n+                k @ SyntaxKind::L_DOLLAR | k @ SyntaxKind::R_DOLLAR => {\n+                    self.token_pos += 1;\n+                    self.push_event(Event::Token { kind: k, n_raw_tokens: 1 });\n+                }\n+                _ => {\n+                    return;\n+                }\n+            }\n+        }\n+    }\n+\n+    pub(crate) fn eat_l_dollars(&mut self) -> usize {\n+        let mut ate_count = 0;\n+        loop {\n+            match self.token_source.token_kind(self.token_pos) {\n+                k @ SyntaxKind::L_DOLLAR => {\n+                    self.token_pos += 1;\n+                    self.push_event(Event::Token { kind: k, n_raw_tokens: 1 });\n+                    ate_count += 1;\n+                }\n+                _ => {\n+                    return ate_count;\n+                }\n+            }\n+        }\n+    }\n+\n+    pub(crate) fn eat_r_dollars(&mut self, max_count: usize) -> usize {\n+        let mut ate_count = 0;\n+        loop {\n+            match self.token_source.token_kind(self.token_pos) {\n+                k @ SyntaxKind::R_DOLLAR => {\n+                    self.token_pos += 1;\n+                    self.push_event(Event::Token { kind: k, n_raw_tokens: 1 });\n+                    ate_count += 1;\n+\n+                    if max_count >= ate_count {\n+                        return ate_count;\n+                    }\n+                }\n+                _ => {\n+                    return ate_count;\n+                }\n+            }\n+        }\n+    }\n+\n+    pub(crate) fn at_l_dollar(&self) -> bool {\n+        let kind = self.token_source.token_kind(self.token_pos);\n+        (kind == SyntaxKind::L_DOLLAR)\n+    }\n+\n+    pub(crate) fn at_r_dollar(&self) -> bool {\n+        let kind = self.token_source.token_kind(self.token_pos);\n+        (kind == SyntaxKind::R_DOLLAR)\n+    }\n }\n \n /// See `Parser::start`."}, {"sha": "498b0e164a660da6c405db0be27558b5d78478ac", "filename": "crates/ra_parser/src/syntax_kind/generated.rs", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "blob_url": "https://github.com/rust-lang/rust/blob/5d35f284f5ac70cde5d758e7c63a38eae0fb0b55/crates%2Fra_parser%2Fsrc%2Fsyntax_kind%2Fgenerated.rs", "raw_url": "https://github.com/rust-lang/rust/raw/5d35f284f5ac70cde5d758e7c63a38eae0fb0b55/crates%2Fra_parser%2Fsrc%2Fsyntax_kind%2Fgenerated.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fra_parser%2Fsrc%2Fsyntax_kind%2Fgenerated.rs?ref=5d35f284f5ac70cde5d758e7c63a38eae0fb0b55", "patch": "@@ -120,6 +120,8 @@ pub enum SyntaxKind {\n     LIFETIME,\n     COMMENT,\n     SHEBANG,\n+    L_DOLLAR,\n+    R_DOLLAR,\n     SOURCE_FILE,\n     STRUCT_DEF,\n     ENUM_DEF,\n@@ -477,6 +479,8 @@ impl SyntaxKind {\n             LIFETIME => &SyntaxInfo { name: \"LIFETIME\" },\n             COMMENT => &SyntaxInfo { name: \"COMMENT\" },\n             SHEBANG => &SyntaxInfo { name: \"SHEBANG\" },\n+            L_DOLLAR => &SyntaxInfo { name: \"L_DOLLAR\" },\n+            R_DOLLAR => &SyntaxInfo { name: \"R_DOLLAR\" },\n             SOURCE_FILE => &SyntaxInfo { name: \"SOURCE_FILE\" },\n             STRUCT_DEF => &SyntaxInfo { name: \"STRUCT_DEF\" },\n             ENUM_DEF => &SyntaxInfo { name: \"ENUM_DEF\" },"}, {"sha": "b412412879b27adda7ac3d73cf72ca7c9e23be19", "filename": "crates/ra_syntax/src/grammar.ron", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/5d35f284f5ac70cde5d758e7c63a38eae0fb0b55/crates%2Fra_syntax%2Fsrc%2Fgrammar.ron", "raw_url": "https://github.com/rust-lang/rust/raw/5d35f284f5ac70cde5d758e7c63a38eae0fb0b55/crates%2Fra_syntax%2Fsrc%2Fgrammar.ron", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fra_syntax%2Fsrc%2Fgrammar.ron?ref=5d35f284f5ac70cde5d758e7c63a38eae0fb0b55", "patch": "@@ -118,6 +118,8 @@ Grammar(\n         \"LIFETIME\",\n         \"COMMENT\",\n         \"SHEBANG\",\n+        \"L_DOLLAR\",\n+        \"R_DOLLAR\",\n     ],\n     nodes: [\n         \"SOURCE_FILE\","}]}