{"sha": "c5059e06231a76be32e46f1b9aa06fb67b77db4c", "node_id": "MDY6Q29tbWl0NzI0NzEyOmM1MDU5ZTA2MjMxYTc2YmUzMmU0NmYxYjlhYTA2ZmI2N2I3N2RiNGM=", "commit": {"author": {"name": "Lukas Wirth", "email": "lukastw97@gmail.com", "date": "2021-08-28T19:18:56Z"}, "committer": {"name": "Lukas Wirth", "email": "lukastw97@gmail.com", "date": "2021-08-28T22:49:57Z"}, "message": "Return all ranges corresponding to a token id in TokenMap", "tree": {"sha": "9451867ee59692209ea909a76ebf4c41be914ec3", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/9451867ee59692209ea909a76ebf4c41be914ec3"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/c5059e06231a76be32e46f1b9aa06fb67b77db4c", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/c5059e06231a76be32e46f1b9aa06fb67b77db4c", "html_url": "https://github.com/rust-lang/rust/commit/c5059e06231a76be32e46f1b9aa06fb67b77db4c", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/c5059e06231a76be32e46f1b9aa06fb67b77db4c/comments", "author": {"login": "Veykril", "id": 3757771, "node_id": "MDQ6VXNlcjM3NTc3NzE=", "avatar_url": "https://avatars.githubusercontent.com/u/3757771?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Veykril", "html_url": "https://github.com/Veykril", "followers_url": "https://api.github.com/users/Veykril/followers", "following_url": "https://api.github.com/users/Veykril/following{/other_user}", "gists_url": "https://api.github.com/users/Veykril/gists{/gist_id}", "starred_url": "https://api.github.com/users/Veykril/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Veykril/subscriptions", "organizations_url": "https://api.github.com/users/Veykril/orgs", "repos_url": "https://api.github.com/users/Veykril/repos", "events_url": "https://api.github.com/users/Veykril/events{/privacy}", "received_events_url": "https://api.github.com/users/Veykril/received_events", "type": "User", "site_admin": false}, "committer": {"login": "Veykril", "id": 3757771, "node_id": "MDQ6VXNlcjM3NTc3NzE=", "avatar_url": "https://avatars.githubusercontent.com/u/3757771?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Veykril", "html_url": "https://github.com/Veykril", "followers_url": "https://api.github.com/users/Veykril/followers", "following_url": "https://api.github.com/users/Veykril/following{/other_user}", "gists_url": "https://api.github.com/users/Veykril/gists{/gist_id}", "starred_url": "https://api.github.com/users/Veykril/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Veykril/subscriptions", "organizations_url": "https://api.github.com/users/Veykril/orgs", "repos_url": "https://api.github.com/users/Veykril/repos", "events_url": "https://api.github.com/users/Veykril/events{/privacy}", "received_events_url": "https://api.github.com/users/Veykril/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "7e31c5ec0d5d7245375b14baa5db26b11b9eb940", "url": "https://api.github.com/repos/rust-lang/rust/commits/7e31c5ec0d5d7245375b14baa5db26b11b9eb940", "html_url": "https://github.com/rust-lang/rust/commit/7e31c5ec0d5d7245375b14baa5db26b11b9eb940"}], "stats": {"total": 172, "additions": 100, "deletions": 72}, "files": [{"sha": "faf192d24e9f7994a66fdbbfe66839ef9b0948a6", "filename": "crates/hir/src/semantics.rs", "status": "modified", "additions": 71, "deletions": 59, "changes": 130, "blob_url": "https://github.com/rust-lang/rust/blob/c5059e06231a76be32e46f1b9aa06fb67b77db4c/crates%2Fhir%2Fsrc%2Fsemantics.rs", "raw_url": "https://github.com/rust-lang/rust/raw/c5059e06231a76be32e46f1b9aa06fb67b77db4c/crates%2Fhir%2Fsrc%2Fsemantics.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fhir%2Fsrc%2Fsemantics.rs?ref=c5059e06231a76be32e46f1b9aa06fb67b77db4c", "patch": "@@ -2,7 +2,7 @@\n \n mod source_to_def;\n \n-use std::{cell::RefCell, fmt, iter::successors};\n+use std::{cell::RefCell, fmt};\n \n use base_db::{FileId, FileRange};\n use hir_def::{\n@@ -14,6 +14,7 @@ use hir_expand::{name::AsName, ExpansionInfo};\n use hir_ty::{associated_type_shorthand_candidates, Interner};\n use itertools::Itertools;\n use rustc_hash::{FxHashMap, FxHashSet};\n+use smallvec::{smallvec, SmallVec};\n use syntax::{\n     algo::find_node_at_offset,\n     ast::{self, GenericParamsOwner, LoopBodyOwner},\n@@ -166,6 +167,10 @@ impl<'db, DB: HirDatabase> Semantics<'db, DB> {\n     }\n \n     pub fn descend_into_macros(&self, token: SyntaxToken) -> SyntaxToken {\n+        self.imp.descend_into_macros(token).pop().unwrap()\n+    }\n+\n+    pub fn descend_into_macros_many(&self, token: SyntaxToken) -> SmallVec<[SyntaxToken; 1]> {\n         self.imp.descend_into_macros(token)\n     }\n \n@@ -440,76 +445,83 @@ impl<'db> SemanticsImpl<'db> {\n         )\n     }\n \n-    fn descend_into_macros(&self, token: SyntaxToken) -> SyntaxToken {\n+    fn descend_into_macros(&self, token: SyntaxToken) -> SmallVec<[SyntaxToken; 1]> {\n         let _p = profile::span(\"descend_into_macros\");\n         let parent = match token.parent() {\n             Some(it) => it,\n-            None => return token,\n+            None => return smallvec![token],\n         };\n         let sa = self.analyze(&parent);\n-\n-        let token = successors(Some(InFile::new(sa.file_id, token)), |token| {\n+        let mut queue = vec![InFile::new(sa.file_id, token)];\n+        let mut res = smallvec![];\n+        while let Some(token) = queue.pop() {\n             self.db.unwind_if_cancelled();\n \n-            for node in token.value.ancestors() {\n-                match_ast! {\n-                    match node {\n-                        ast::MacroCall(macro_call) => {\n-                            let tt = macro_call.token_tree()?;\n-                            let l_delim = match tt.left_delimiter_token() {\n-                                Some(it) => it.text_range().end(),\n-                                None => tt.syntax().text_range().start()\n-                            };\n-                            let r_delim = match tt.right_delimiter_token() {\n-                                Some(it) => it.text_range().start(),\n-                                None => tt.syntax().text_range().end()\n-                            };\n-                            if !TextRange::new(l_delim, r_delim).contains_range(token.value.text_range()) {\n-                                return None;\n-                            }\n-                            let file_id = sa.expand(self.db, token.with_value(&macro_call))?;\n-                            let token = self\n-                                .expansion_info_cache\n-                                .borrow_mut()\n-                                .entry(file_id)\n-                                .or_insert_with(|| file_id.expansion_info(self.db.upcast()))\n-                                .as_ref()?\n-                                .map_token_down(self.db.upcast(), None, token.as_ref())?;\n-\n-                            if let Some(parent) = token.value.parent() {\n-                                self.cache(find_root(&parent), token.file_id);\n-                            }\n-\n-                            return Some(token);\n-                        },\n-                        ast::Item(item) => {\n-                            if let Some(call_id) = self.with_ctx(|ctx| ctx.item_to_macro_call(token.with_value(item.clone()))) {\n-                                let file_id = call_id.as_file();\n-                                let token = self\n-                                    .expansion_info_cache\n-                                    .borrow_mut()\n+            let mapped = (|| {\n+                for node in token.value.ancestors() {\n+                    match_ast! {\n+                        match node {\n+                            ast::MacroCall(macro_call) => {\n+                                let tt = macro_call.token_tree()?;\n+                                let l_delim = match tt.left_delimiter_token() {\n+                                    Some(it) => it.text_range().end(),\n+                                    None => tt.syntax().text_range().start()\n+                                };\n+                                let r_delim = match tt.right_delimiter_token() {\n+                                    Some(it) => it.text_range().start(),\n+                                    None => tt.syntax().text_range().end()\n+                                };\n+                                if !TextRange::new(l_delim, r_delim).contains_range(token.value.text_range()) {\n+                                    return None;\n+                                }\n+                                let file_id = sa.expand(self.db, token.with_value(&macro_call))?;\n+                                let mut cache = self.expansion_info_cache.borrow_mut();\n+                                let tokens = cache\n                                     .entry(file_id)\n                                     .or_insert_with(|| file_id.expansion_info(self.db.upcast()))\n                                     .as_ref()?\n-                                    .map_token_down(self.db.upcast(), Some(item), token.as_ref())?;\n-\n-                                if let Some(parent) = token.value.parent() {\n-                                    self.cache(find_root(&parent), token.file_id);\n+                                    .map_token_down(self.db.upcast(), None, token.as_ref())?;\n+\n+                                queue.extend(tokens.inspect(|token| {\n+                                    if let Some(parent) = token.value.parent() {\n+                                        self.cache(find_root(&parent), token.file_id);\n+                                    }\n+                                }));\n+                                return Some(());\n+                            },\n+                            ast::Item(item) => {\n+                                match self.with_ctx(|ctx| ctx.item_to_macro_call(token.with_value(item))) {\n+                                    Some(call_id) => {\n+                                        let file_id = call_id.as_file();\n+                                        let mut cache = self.expansion_info_cache.borrow_mut();\n+                                        let tokens = cache\n+                                            .entry(file_id)\n+                                            .or_insert_with(|| file_id.expansion_info(self.db.upcast()))\n+                                            .as_ref()?\n+                                            .map_token_down(self.db.upcast(), None, token.as_ref())?;\n+\n+                                        queue.extend(tokens.inspect(|token| {\n+                                            if let Some(parent) = token.value.parent() {\n+                                                self.cache(find_root(&parent), token.file_id);\n+                                            }\n+                                        }));\n+                                        return Some(());\n+                                    }\n+                                    None => {}\n                                 }\n-\n-                                return Some(token);\n-                            }\n-                        },\n-                        _ => {}\n+                            },\n+                            _ => {}\n+                        }\n                     }\n                 }\n+                None\n+            })();\n+            match mapped {\n+                Some(()) => (),\n+                None => res.push(token.value),\n             }\n-\n-            None\n-        })\n-        .last()\n-        .unwrap();\n-        token.value\n+        }\n+        res\n     }\n \n     fn descend_node_at_offset(\n@@ -519,8 +531,8 @@ impl<'db> SemanticsImpl<'db> {\n     ) -> impl Iterator<Item = SyntaxNode> + '_ {\n         // Handle macro token cases\n         node.token_at_offset(offset)\n-            .map(|token| self.descend_into_macros(token))\n-            .map(|it| self.token_ancestors_with_macros(it))\n+            .flat_map(move |token| self.descend_into_macros(token))\n+            .map(move |it| self.token_ancestors_with_macros(it))\n             .flatten()\n     }\n "}, {"sha": "2fc8468faf4ac00f5abbeaf70b7b359503bcfb56", "filename": "crates/hir_expand/src/db.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/c5059e06231a76be32e46f1b9aa06fb67b77db4c/crates%2Fhir_expand%2Fsrc%2Fdb.rs", "raw_url": "https://github.com/rust-lang/rust/raw/c5059e06231a76be32e46f1b9aa06fb67b77db4c/crates%2Fhir_expand%2Fsrc%2Fdb.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fhir_expand%2Fsrc%2Fdb.rs?ref=c5059e06231a76be32e46f1b9aa06fb67b77db4c", "patch": "@@ -163,7 +163,7 @@ pub fn expand_speculative(\n         mbe::token_tree_to_syntax_node(&speculative_expansion.value, fragment_kind).ok()?;\n \n     let token_id = macro_def.map_id_down(token_id);\n-    let range = tmap_2.range_by_token(token_id, token_to_map.kind())?;\n+    let range = tmap_2.first_range_by_token(token_id, token_to_map.kind())?;\n     let token = node.syntax_node().covering_element(range).into_token()?;\n     Some((node.syntax_node(), token))\n }"}, {"sha": "cac484a325e8cda0d93ae9730214eda7ebdc3049", "filename": "crates/hir_expand/src/hygiene.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/c5059e06231a76be32e46f1b9aa06fb67b77db4c/crates%2Fhir_expand%2Fsrc%2Fhygiene.rs", "raw_url": "https://github.com/rust-lang/rust/raw/c5059e06231a76be32e46f1b9aa06fb67b77db4c/crates%2Fhir_expand%2Fsrc%2Fhygiene.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fhir_expand%2Fsrc%2Fhygiene.rs?ref=c5059e06231a76be32e46f1b9aa06fb67b77db4c", "patch": "@@ -171,7 +171,7 @@ impl HygieneInfo {\n             },\n         };\n \n-        let range = token_map.range_by_token(token_id, SyntaxKind::IDENT)?;\n+        let range = token_map.first_range_by_token(token_id, SyntaxKind::IDENT)?;\n         Some((tt.with_value(range + tt.value), origin))\n     }\n }"}, {"sha": "3bbbb5722f33c180b3f1fc6a4e27461508061e37", "filename": "crates/hir_expand/src/lib.rs", "status": "modified", "additions": 7, "deletions": 6, "changes": 13, "blob_url": "https://github.com/rust-lang/rust/blob/c5059e06231a76be32e46f1b9aa06fb67b77db4c/crates%2Fhir_expand%2Fsrc%2Flib.rs", "raw_url": "https://github.com/rust-lang/rust/raw/c5059e06231a76be32e46f1b9aa06fb67b77db4c/crates%2Fhir_expand%2Fsrc%2Flib.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fhir_expand%2Fsrc%2Flib.rs?ref=c5059e06231a76be32e46f1b9aa06fb67b77db4c", "patch": "@@ -368,7 +368,7 @@ impl ExpansionInfo {\n         db: &dyn db::AstDatabase,\n         item: Option<ast::Item>,\n         token: InFile<&SyntaxToken>,\n-    ) -> Option<InFile<SyntaxToken>> {\n+    ) -> Option<impl Iterator<Item = InFile<SyntaxToken>> + '_> {\n         assert_eq!(token.file_id, self.arg.file_id);\n         let token_id = if let Some(item) = item {\n             let call_id = match self.expanded.file_id.0 {\n@@ -411,11 +411,12 @@ impl ExpansionInfo {\n             }\n         };\n \n-        let range = self.exp_map.range_by_token(token_id, token.value.kind())?;\n+        let tokens = self\n+            .exp_map\n+            .ranges_by_token(token_id, token.value.kind())\n+            .flat_map(move |range| self.expanded.value.covering_element(range).into_token());\n \n-        let token = self.expanded.value.covering_element(range).into_token()?;\n-\n-        Some(self.expanded.with_value(token))\n+        Some(tokens.map(move |token| self.expanded.with_value(token)))\n     }\n \n     pub fn map_token_up(\n@@ -453,7 +454,7 @@ impl ExpansionInfo {\n             },\n         };\n \n-        let range = token_map.range_by_token(token_id, token.value.kind())?;\n+        let range = token_map.first_range_by_token(token_id, token.value.kind())?;\n         let token =\n             tt.value.covering_element(range + tt.value.text_range().start()).into_token()?;\n         Some((tt.with_value(token), origin))"}, {"sha": "c8d06eebb75bb71e0555cea999ab49d45542fa08", "filename": "crates/mbe/src/tests/expand.rs", "status": "modified", "additions": 3, "deletions": 2, "changes": 5, "blob_url": "https://github.com/rust-lang/rust/blob/c5059e06231a76be32e46f1b9aa06fb67b77db4c/crates%2Fmbe%2Fsrc%2Ftests%2Fexpand.rs", "raw_url": "https://github.com/rust-lang/rust/raw/c5059e06231a76be32e46f1b9aa06fb67b77db4c/crates%2Fmbe%2Fsrc%2Ftests%2Fexpand.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fmbe%2Fsrc%2Ftests%2Fexpand.rs?ref=c5059e06231a76be32e46f1b9aa06fb67b77db4c", "patch": "@@ -58,8 +58,9 @@ macro_rules! foobar {\n     let (node, token_map) = token_tree_to_syntax_node(&expanded, FragmentKind::Items).unwrap();\n     let content = node.syntax_node().to_string();\n \n-    let get_text =\n-        |id, kind| -> String { content[token_map.range_by_token(id, kind).unwrap()].to_string() };\n+    let get_text = |id, kind| -> String {\n+        content[token_map.first_range_by_token(id, kind).unwrap()].to_string()\n+    };\n \n     assert_eq!(expanded.token_trees.len(), 4);\n     // {($e:ident) => { fn $e() {} }}"}, {"sha": "9053526d203b25d77c62f3d73a6d2443f7812575", "filename": "crates/mbe/src/token_map.rs", "status": "modified", "additions": 17, "deletions": 3, "changes": 20, "blob_url": "https://github.com/rust-lang/rust/blob/c5059e06231a76be32e46f1b9aa06fb67b77db4c/crates%2Fmbe%2Fsrc%2Ftoken_map.rs", "raw_url": "https://github.com/rust-lang/rust/raw/c5059e06231a76be32e46f1b9aa06fb67b77db4c/crates%2Fmbe%2Fsrc%2Ftoken_map.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fmbe%2Fsrc%2Ftoken_map.rs?ref=c5059e06231a76be32e46f1b9aa06fb67b77db4c", "patch": "@@ -46,9 +46,23 @@ impl TokenMap {\n         Some(token_id)\n     }\n \n-    pub fn range_by_token(&self, token_id: tt::TokenId, kind: SyntaxKind) -> Option<TextRange> {\n-        let &(_, range) = self.entries.iter().find(|(tid, _)| *tid == token_id)?;\n-        range.by_kind(kind)\n+    pub fn ranges_by_token(\n+        &self,\n+        token_id: tt::TokenId,\n+        kind: SyntaxKind,\n+    ) -> impl Iterator<Item = TextRange> + '_ {\n+        self.entries\n+            .iter()\n+            .filter(move |&&(tid, _)| tid == token_id)\n+            .filter_map(move |(_, range)| range.by_kind(kind))\n+    }\n+\n+    pub fn first_range_by_token(\n+        &self,\n+        token_id: tt::TokenId,\n+        kind: SyntaxKind,\n+    ) -> Option<TextRange> {\n+        self.ranges_by_token(token_id, kind).next()\n     }\n \n     pub(crate) fn shrink_to_fit(&mut self) {"}]}