{"sha": "49e5493587162d53a693ba7e7833edd8f1718e94", "node_id": "MDY6Q29tbWl0NzI0NzEyOjQ5ZTU0OTM1ODcxNjJkNTNhNjkzYmE3ZTc4MzNlZGQ4ZjE3MThlOTQ=", "commit": {"author": {"name": "Alex Crichton", "email": "alex@alexcrichton.com", "date": "2013-12-13T01:53:05Z"}, "committer": {"name": "Alex Crichton", "email": "alex@alexcrichton.com", "date": "2013-12-24T22:42:00Z"}, "message": "std: Reimplement std::comm without the scheduler\n\nLike the librustuv refactoring, this refactors std::comm to sever all ties with\nthe scheduler. This means that the entire `comm::imp` module can be deleted in\nfavor of implementations outside of libstd.", "tree": {"sha": "687aa565b9fac81b49928b78e2b0e1298afc2bf5", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/687aa565b9fac81b49928b78e2b0e1298afc2bf5"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/49e5493587162d53a693ba7e7833edd8f1718e94", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/49e5493587162d53a693ba7e7833edd8f1718e94", "html_url": "https://github.com/rust-lang/rust/commit/49e5493587162d53a693ba7e7833edd8f1718e94", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/49e5493587162d53a693ba7e7833edd8f1718e94/comments", "author": {"login": "alexcrichton", "id": 64996, "node_id": "MDQ6VXNlcjY0OTk2", "avatar_url": "https://avatars.githubusercontent.com/u/64996?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alexcrichton", "html_url": "https://github.com/alexcrichton", "followers_url": "https://api.github.com/users/alexcrichton/followers", "following_url": "https://api.github.com/users/alexcrichton/following{/other_user}", "gists_url": "https://api.github.com/users/alexcrichton/gists{/gist_id}", "starred_url": "https://api.github.com/users/alexcrichton/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alexcrichton/subscriptions", "organizations_url": "https://api.github.com/users/alexcrichton/orgs", "repos_url": "https://api.github.com/users/alexcrichton/repos", "events_url": "https://api.github.com/users/alexcrichton/events{/privacy}", "received_events_url": "https://api.github.com/users/alexcrichton/received_events", "type": "User", "site_admin": false}, "committer": {"login": "alexcrichton", "id": 64996, "node_id": "MDQ6VXNlcjY0OTk2", "avatar_url": "https://avatars.githubusercontent.com/u/64996?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alexcrichton", "html_url": "https://github.com/alexcrichton", "followers_url": "https://api.github.com/users/alexcrichton/followers", "following_url": "https://api.github.com/users/alexcrichton/following{/other_user}", "gists_url": "https://api.github.com/users/alexcrichton/gists{/gist_id}", "starred_url": "https://api.github.com/users/alexcrichton/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alexcrichton/subscriptions", "organizations_url": "https://api.github.com/users/alexcrichton/orgs", "repos_url": "https://api.github.com/users/alexcrichton/repos", "events_url": "https://api.github.com/users/alexcrichton/events{/privacy}", "received_events_url": "https://api.github.com/users/alexcrichton/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "daaec28c6f71f5d6e2f5bc716ffc2780ef56fa7b", "url": "https://api.github.com/repos/rust-lang/rust/commits/daaec28c6f71f5d6e2f5bc716ffc2780ef56fa7b", "html_url": "https://github.com/rust-lang/rust/commit/daaec28c6f71f5d6e2f5bc716ffc2780ef56fa7b"}], "stats": {"total": 402, "additions": 40, "deletions": 362}, "files": [{"sha": "bd1d6fed901caf9b382bdbe949133582ab09bcea", "filename": "src/libstd/comm/imp.rs", "status": "removed", "additions": 0, "deletions": 337, "changes": 337, "blob_url": "https://github.com/rust-lang/rust/blob/daaec28c6f71f5d6e2f5bc716ffc2780ef56fa7b/src%2Flibstd%2Fcomm%2Fimp.rs", "raw_url": "https://github.com/rust-lang/rust/raw/daaec28c6f71f5d6e2f5bc716ffc2780ef56fa7b/src%2Flibstd%2Fcomm%2Fimp.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibstd%2Fcomm%2Fimp.rs?ref=daaec28c6f71f5d6e2f5bc716ffc2780ef56fa7b", "patch": "@@ -1,337 +0,0 @@\n-// Copyright 2013 The Rust Project Developers. See the COPYRIGHT\n-// file at the top-level directory of this distribution and at\n-// http://rust-lang.org/COPYRIGHT.\n-//\n-// Licensed under the Apache License, Version 2.0 <LICENSE-APACHE or\n-// http://www.apache.org/licenses/LICENSE-2.0> or the MIT license\n-// <LICENSE-MIT or http://opensource.org/licenses/MIT>, at your\n-// option. This file may not be copied, modified, or distributed\n-// except according to those terms.\n-\n-//! One of the major goals behind this channel implementation is to work\n-//! seamlessly on and off the runtime. This also means that the code isn't\n-//! littered with \"if is_green() { ... } else { ... }\". Right now, the rest of\n-//! the runtime isn't quite ready to for this abstraction to be done very\n-//! nicely, so the conditional \"if green\" blocks are all contained in this inner\n-//! module.\n-//!\n-//! The goal of this module is to mirror what the runtime \"should be\", not the\n-//! state that it is currently in today. You'll notice that there is no mention\n-//! of schedulers or is_green inside any of the channel code, it is currently\n-//! entirely contained in this one module.\n-//!\n-//! In the ideal world, nothing in this module exists and it is all implemented\n-//! elsewhere in the runtime (in the proper location). All of this code is\n-//! structured in order to easily refactor this to the correct location whenever\n-//! we have the trait objects in place to serve as the boundary of the\n-//! abstraction.\n-\n-use iter::{range, Iterator};\n-use ops::Drop;\n-use option::{Some, None, Option};\n-use rt::local::Local;\n-use rt::sched::{SchedHandle, Scheduler, TaskFromFriend};\n-use rt::thread::Thread;\n-use rt;\n-use unstable::mutex::Mutex;\n-use unstable::sync::UnsafeArc;\n-\n-// A task handle is a method of waking up a blocked task. The handle itself\n-// is completely opaque and only has a wake() method defined on it. This\n-// method will wake the method regardless of the context of the thread which\n-// is currently calling wake().\n-//\n-// This abstraction should be able to be created when putting a task to\n-// sleep. This should basically be a method on whatever the local Task is,\n-// consuming the local Task.\n-\n-pub struct TaskHandle {\n-    priv inner: TaskRepr\n-}\n-enum TaskRepr {\n-    Green(rt::BlockedTask, *mut SchedHandle),\n-    Native(NativeWakeupStyle),\n-}\n-enum NativeWakeupStyle {\n-    ArcWakeup(UnsafeArc<Mutex>),    // shared mutex to synchronize on\n-    LocalWakeup(*mut Mutex),        // synchronize on the task-local mutex\n-}\n-\n-impl TaskHandle {\n-    // Signal that this handle should be woken up. The `can_resched`\n-    // argument indicates whether the current task could possibly be\n-    // rescheduled or not. This does not have a lot of meaning for the\n-    // native case, but for an M:N case it indicates whether a context\n-    // switch can happen or not.\n-    pub fn wake(self, can_resched: bool) {\n-        match self.inner {\n-            Green(task, handle) => {\n-                // If we have a local scheduler, then use that to run the\n-                // blocked task, otherwise we can use the handle to send the\n-                // task back to its home.\n-                if rt::in_green_task_context() {\n-                    if can_resched {\n-                        task.wake().map(Scheduler::run_task);\n-                    } else {\n-                        let mut s: ~Scheduler = Local::take();\n-                        s.enqueue_blocked_task(task);\n-                        Local::put(s);\n-                    }\n-                } else {\n-                    let task = match task.wake() {\n-                        Some(task) => task, None => return\n-                    };\n-                    // XXX: this is not an easy section of code to refactor.\n-                    //      If this handle is owned by the Task (which it\n-                    //      should be), then this would be a use-after-free\n-                    //      because once the task is pushed onto the message\n-                    //      queue, the handle is gone.\n-                    //\n-                    //      Currently the handle is instead owned by the\n-                    //      Port/Chan pair, which means that because a\n-                    //      channel is invoking this method the handle will\n-                    //      continue to stay alive for the entire duration\n-                    //      of this method. This will require thought when\n-                    //      moving the handle into the task.\n-                    unsafe { (*handle).send(TaskFromFriend(task)) }\n-                }\n-            }\n-\n-            // Note that there are no use-after-free races in this code. In\n-            // the arc-case, we own the lock, and in the local case, we're\n-            // using a lock so it's guranteed that they aren't running while\n-            // we hold the lock.\n-            Native(ArcWakeup(lock)) => {\n-                unsafe {\n-                    let lock = lock.get();\n-                    (*lock).lock();\n-                    (*lock).signal();\n-                    (*lock).unlock();\n-                }\n-            }\n-            Native(LocalWakeup(lock)) => {\n-                unsafe {\n-                    (*lock).lock();\n-                    (*lock).signal();\n-                    (*lock).unlock();\n-                }\n-            }\n-        }\n-    }\n-\n-    // Trashes handle to this task. This ensures that necessary memory is\n-    // deallocated, and there may be some extra assertions as well.\n-    pub fn trash(self) {\n-        match self.inner {\n-            Green(task, _) => task.assert_already_awake(),\n-            Native(..) => {}\n-        }\n-    }\n-}\n-\n-// This structure is an abstraction of what should be stored in the local\n-// task itself. This data is currently stored inside of each channel, but\n-// this should rather be stored in each task (and channels will still\n-// continue to lazily initialize this data).\n-\n-pub struct TaskData {\n-    priv handle: Option<SchedHandle>,\n-    priv lock: Mutex,\n-}\n-\n-impl TaskData {\n-    pub fn new() -> TaskData {\n-        TaskData {\n-            handle: None,\n-            lock: unsafe { Mutex::empty() },\n-        }\n-    }\n-}\n-\n-impl Drop for TaskData {\n-    fn drop(&mut self) {\n-        unsafe { self.lock.destroy() }\n-    }\n-}\n-\n-// Now this is the really fun part. This is where all the M:N/1:1-agnostic\n-// along with recv/select-agnostic blocking information goes. A \"blocking\n-// context\" is really just a stack-allocated structure (which is probably\n-// fine to be a stack-trait-object).\n-//\n-// This has some particularly strange interfaces, but the reason for all\n-// this is to support selection/recv/1:1/M:N all in one bundle.\n-\n-pub struct BlockingContext<'a> {\n-    priv inner: BlockingRepr<'a>\n-}\n-\n-enum BlockingRepr<'a> {\n-    GreenBlock(rt::BlockedTask, &'a mut Scheduler),\n-    NativeBlock(Option<UnsafeArc<Mutex>>),\n-}\n-\n-impl<'a> BlockingContext<'a> {\n-    // Creates one blocking context. The data provided should in theory be\n-    // acquired from the local task, but it is instead acquired from the\n-    // channel currently.\n-    //\n-    // This function will call `f` with a blocking context, plus the data\n-    // that it is given. This function will then return whether this task\n-    // should actually go to sleep or not. If `true` is returned, then this\n-    // function does not return until someone calls `wake()` on the task.\n-    // If `false` is returned, then this function immediately returns.\n-    //\n-    // # Safety note\n-    //\n-    // Note that this stack closure may not be run on the same stack as when\n-    // this function was called. This means that the environment of this\n-    // stack closure could be unsafely aliased. This is currently prevented\n-    // through the guarantee that this function will never return before `f`\n-    // finishes executing.\n-    pub fn one(data: &mut TaskData,\n-               f: |BlockingContext, &mut TaskData| -> bool) {\n-        if rt::in_green_task_context() {\n-            let sched: ~Scheduler = Local::take();\n-            sched.deschedule_running_task_and_then(|sched, task| {\n-                let ctx = BlockingContext { inner: GreenBlock(task, sched) };\n-                // no need to do something on success/failure other than\n-                // returning because the `block` function for a BlockingContext\n-                // takes care of reawakening itself if the blocking procedure\n-                // fails. If this function is successful, then we're already\n-                // blocked, and if it fails, the task will already be\n-                // rescheduled.\n-                f(ctx, data);\n-            });\n-        } else {\n-            unsafe { data.lock.lock(); }\n-            let ctx = BlockingContext { inner: NativeBlock(None) };\n-            if f(ctx, data) {\n-                unsafe { data.lock.wait(); }\n-            }\n-            unsafe { data.lock.unlock(); }\n-        }\n-    }\n-\n-    // Creates many blocking contexts. The intended use case for this\n-    // function is selection over a number of ports. This will create `amt`\n-    // blocking contexts, yielding them to `f` in turn. If `f` returns\n-    // false, then this function aborts and returns immediately. If `f`\n-    // repeatedly returns `true` `amt` times, then this function will block.\n-    pub fn many(amt: uint, f: |BlockingContext| -> bool) {\n-        if rt::in_green_task_context() {\n-            let sched: ~Scheduler = Local::take();\n-            sched.deschedule_running_task_and_then(|sched, task| {\n-                for handle in task.make_selectable(amt) {\n-                    let ctx = BlockingContext {\n-                        inner: GreenBlock(handle, sched)\n-                    };\n-                    // see comment above in `one` for why no further action is\n-                    // necessary here\n-                    if !f(ctx) { break }\n-                }\n-            });\n-        } else {\n-            // In the native case, our decision to block must be shared\n-            // amongst all of the channels. It may be possible to\n-            // stack-allocate this mutex (instead of putting it in an\n-            // UnsafeArc box), but for now in order to prevent\n-            // use-after-free trivially we place this into a box and then\n-            // pass that around.\n-            unsafe {\n-                let mtx = UnsafeArc::new(Mutex::new());\n-                (*mtx.get()).lock();\n-                let success = range(0, amt).all(|_| {\n-                    f(BlockingContext {\n-                        inner: NativeBlock(Some(mtx.clone()))\n-                    })\n-                });\n-                if success {\n-                    (*mtx.get()).wait();\n-                }\n-                (*mtx.get()).unlock();\n-            }\n-        }\n-    }\n-\n-    // This function will consume this BlockingContext, and optionally block\n-    // if according to the atomic `decision` function. The semantics of this\n-    // functions are:\n-    //\n-    //  * `slot` is required to be a `None`-slot (which is owned by the\n-    //    channel)\n-    //  * The `slot` will be filled in with a blocked version of the current\n-    //    task (with `wake`-ability if this function is successful).\n-    //  * If the `decision` function returns true, then this function\n-    //    immediately returns having relinquished ownership of the task.\n-    //  * If the `decision` function returns false, then the `slot` is reset\n-    //    to `None` and the task is re-scheduled if necessary (remember that\n-    //    the task will not resume executing before the outer `one` or\n-    //    `many` function has returned. This function is expected to have a\n-    //    release memory fence in order for the modifications of `to_wake` to be\n-    //    visible to other tasks. Code which attempts to read `to_wake` should\n-    //    have an acquiring memory fence to guarantee that this write is\n-    //    visible.\n-    //\n-    // This function will return whether the blocking occurred or not.\n-    pub fn block(self,\n-                 data: &mut TaskData,\n-                 slot: &mut Option<TaskHandle>,\n-                 decision: || -> bool) -> bool {\n-        assert!(slot.is_none());\n-        match self.inner {\n-            GreenBlock(task, sched) => {\n-                if data.handle.is_none() {\n-                    data.handle = Some(sched.make_handle());\n-                }\n-                let handle = data.handle.get_mut_ref() as *mut SchedHandle;\n-                *slot = Some(TaskHandle { inner: Green(task, handle) });\n-\n-                if !decision() {\n-                    match slot.take_unwrap().inner {\n-                        Green(task, _) => sched.enqueue_blocked_task(task),\n-                        Native(..) => unreachable!()\n-                    }\n-                    false\n-                } else {\n-                    true\n-                }\n-            }\n-            NativeBlock(shared) => {\n-                *slot = Some(TaskHandle {\n-                    inner: Native(match shared {\n-                        Some(arc) => ArcWakeup(arc),\n-                        None => LocalWakeup(&mut data.lock as *mut Mutex),\n-                    })\n-                });\n-\n-                if !decision() {\n-                    *slot = None;\n-                    false\n-                } else {\n-                    true\n-                }\n-            }\n-        }\n-    }\n-}\n-\n-// Agnostic method of forcing a yield of the current task\n-pub fn yield_now() {\n-    if rt::in_green_task_context() {\n-        let sched: ~Scheduler = Local::take();\n-        sched.yield_now();\n-    } else {\n-        Thread::yield_now();\n-    }\n-}\n-\n-// Agnostic method of \"maybe yielding\" in order to provide fairness\n-pub fn maybe_yield() {\n-    if rt::in_green_task_context() {\n-        let sched: ~Scheduler = Local::take();\n-        sched.maybe_yield();\n-    } else {\n-        // the OS decides fairness, nothing for us to do.\n-    }\n-}"}, {"sha": "f5048ec62a402ed2621ea7b72605099d0be50230", "filename": "src/libstd/comm/mod.rs", "status": "modified", "additions": 23, "deletions": 14, "changes": 37, "blob_url": "https://github.com/rust-lang/rust/blob/49e5493587162d53a693ba7e7833edd8f1718e94/src%2Flibstd%2Fcomm%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/49e5493587162d53a693ba7e7833edd8f1718e94/src%2Flibstd%2Fcomm%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibstd%2Fcomm%2Fmod.rs?ref=49e5493587162d53a693ba7e7833edd8f1718e94", "patch": "@@ -233,14 +233,17 @@ use iter::Iterator;\n use kinds::Send;\n use ops::Drop;\n use option::{Option, Some, None};\n+use result::{Ok, Err};\n+use rt::local::Local;\n+use rt::task::{Task, BlockedTask};\n use rt::thread::Thread;\n-use unstable::atomics::{AtomicInt, AtomicBool, SeqCst, Relaxed};\n+use sync::atomics::{AtomicInt, AtomicBool, SeqCst, Relaxed};\n+use task;\n use vec::{ImmutableVector, OwnedVector};\n \n-use spsc = rt::spsc_queue;\n-use mpsc = rt::mpsc_queue;\n+use spsc = sync::spsc_queue;\n+use mpsc = sync::mpsc_queue;\n \n-use self::imp::{TaskHandle, TaskData, BlockingContext};\n pub use self::select::Select;\n \n macro_rules! test (\n@@ -265,7 +268,6 @@ macro_rules! test (\n     )\n )\n \n-mod imp;\n mod select;\n \n ///////////////////////////////////////////////////////////////////////////////\n@@ -326,9 +328,7 @@ pub struct SharedChan<T> {\n struct Packet {\n     cnt: AtomicInt, // How many items are on this channel\n     steals: int,    // How many times has a port received without blocking?\n-    to_wake: Option<TaskHandle>, // Task to wake up\n-\n-    data: TaskData,\n+    to_wake: Option<BlockedTask>, // Task to wake up\n \n     // This lock is used to wake up native threads blocked in select. The\n     // `lock` field is not used because the thread blocking in select must\n@@ -358,7 +358,6 @@ impl Packet {\n             cnt: AtomicInt::new(0),\n             steals: 0,\n             to_wake: None,\n-            data: TaskData::new(),\n             channels: AtomicInt::new(1),\n \n             selecting: AtomicBool::new(false),\n@@ -418,7 +417,10 @@ impl Packet {\n     // This function must have had at least an acquire fence before it to be\n     // properly called.\n     fn wakeup(&mut self, can_resched: bool) {\n-        self.to_wake.take_unwrap().wake(can_resched);\n+        match self.to_wake.take_unwrap().wake() {\n+            Some(task) => task.reawaken(can_resched),\n+            None => {}\n+        }\n         self.selecting.store(false, Relaxed);\n     }\n \n@@ -607,7 +609,7 @@ impl<T: Send> Chan<T> {\n                 n => {\n                     assert!(n >= 0);\n                     if can_resched && n > 0 && n % RESCHED_FREQ == 0 {\n-                        imp::maybe_yield();\n+                        task::deschedule();\n                     }\n                     true\n                 }\n@@ -700,7 +702,7 @@ impl<T: Send> SharedChan<T> {\n                 -1 => { (*packet).wakeup(can_resched); }\n                 n => {\n                     if can_resched && n > 0 && n % RESCHED_FREQ == 0 {\n-                        imp::maybe_yield();\n+                        task::deschedule();\n                     }\n                 }\n             }\n@@ -840,8 +842,15 @@ impl<T: Send> Port<T> {\n         unsafe {\n             this = cast::transmute_mut(self);\n             packet = this.queue.packet();\n-            BlockingContext::one(&mut (*packet).data, |ctx, data| {\n-                ctx.block(data, &mut (*packet).to_wake, || (*packet).decrement())\n+            let task: ~Task = Local::take();\n+            task.deschedule(1, |task| {\n+                assert!((*packet).to_wake.is_none());\n+                (*packet).to_wake = Some(task);\n+                if (*packet).decrement() {\n+                    Ok(())\n+                } else {\n+                    Err((*packet).to_wake.take_unwrap())\n+                }\n             });\n         }\n "}, {"sha": "68e1a05a6530272e4e3fa4e00d0eb2dc5a00a17c", "filename": "src/libstd/comm/select.rs", "status": "modified", "additions": 17, "deletions": 11, "changes": 28, "blob_url": "https://github.com/rust-lang/rust/blob/49e5493587162d53a693ba7e7833edd8f1718e94/src%2Flibstd%2Fcomm%2Fselect.rs", "raw_url": "https://github.com/rust-lang/rust/raw/49e5493587162d53a693ba7e7833edd8f1718e94/src%2Flibstd%2Fcomm%2Fselect.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibstd%2Fcomm%2Fselect.rs?ref=49e5493587162d53a693ba7e7833edd8f1718e94", "patch": "@@ -50,10 +50,13 @@ use kinds::Send;\n use ops::Drop;\n use option::{Some, None, Option};\n use ptr::RawPtr;\n-use super::imp::BlockingContext;\n-use super::{Packet, Port, imp};\n+use result::{Ok, Err};\n+use rt::thread::Thread;\n+use rt::local::Local;\n+use rt::task::Task;\n+use super::{Packet, Port};\n+use sync::atomics::{Relaxed, SeqCst};\n use uint;\n-use unstable::atomics::{Relaxed, SeqCst};\n \n macro_rules! select {\n     (\n@@ -184,19 +187,22 @@ impl Select {\n             // Acquire a number of blocking contexts, and block on each one\n             // sequentially until one fails. If one fails, then abort\n             // immediately so we can go unblock on all the other ports.\n-            BlockingContext::many(amt, |ctx| {\n+            let task: ~Task = Local::take();\n+            task.deschedule(amt, |task| {\n+                // Prepare for the block\n                 let (i, packet) = iter.next().unwrap();\n+                assert!((*packet).to_wake.is_none());\n+                (*packet).to_wake = Some(task);\n                 (*packet).selecting.store(true, SeqCst);\n-                if !ctx.block(&mut (*packet).data,\n-                              &mut (*packet).to_wake,\n-                              || (*packet).decrement()) {\n+\n+                if (*packet).decrement() {\n+                    Ok(())\n+                } else {\n                     (*packet).abort_selection(false);\n                     (*packet).selecting.store(false, SeqCst);\n                     ready_index = i;\n                     ready_id = (*packet).selection_id;\n-                    false\n-                } else {\n-                    true\n+                    Err((*packet).to_wake.take_unwrap())\n                 }\n             });\n \n@@ -225,7 +231,7 @@ impl Select {\n                 if (*packet).abort_selection(true) {\n                     ready_id = (*packet).selection_id;\n                     while (*packet).selecting.load(Relaxed) {\n-                        imp::yield_now();\n+                        task::deschedule();\n                     }\n                 }\n             }"}]}