{"sha": "5b663f1b07233442a3b0b58db453504dcc51ddc9", "node_id": "MDY6Q29tbWl0NzI0NzEyOjViNjYzZjFiMDcyMzM0NDJhM2IwYjU4ZGI0NTM1MDRkY2M1MWRkYzk=", "commit": {"author": {"name": "bors[bot]", "email": "26634292+bors[bot]@users.noreply.github.com", "date": "2021-05-04T19:42:30Z"}, "committer": {"name": "GitHub", "email": "noreply@github.com", "date": "2021-05-04T19:42:30Z"}, "message": "Merge #8732\n\n8732: internal: refactor expansion queries r=matklad a=matklad\n\nbors r+\n\ud83e\udd16\n\nCo-authored-by: Aleksey Kladov <aleksey.kladov@gmail.com>", "tree": {"sha": "83e7822f41dd04db2f5e79d75f88a9c838beceee", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/83e7822f41dd04db2f5e79d75f88a9c838beceee"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/5b663f1b07233442a3b0b58db453504dcc51ddc9", "comment_count": 0, "verification": {"verified": true, "reason": "valid", "signature": "-----BEGIN PGP SIGNATURE-----\n\nwsBcBAABCAAQBQJgkaOmCRBK7hj4Ov3rIwAAWIMIAEWert3OWsjk0b+N0vJZ5ACC\nQmBe7TSzzZkwSDjWvZzLK/Q4Zc+u88/nHpYJVJHXfmzExCCLcAkCMloPwLLVftZp\nK/V79TGWMmWH3FWglWQV4GUowggLkKjeDtZiJv8g+anu/LN3mUnRqWZUx1HY2rZK\nreNbNiB/4WSXss/4ahhRFhUsFUxkFhtiZl6JpsOD4BCfJb9IxizMEyv5cnNHHkCW\n/fDDe1AKoeue1TbWq67GOSIp6lQFdWzD3kaxOfuiypSZNxMzd5Fuzt/MB64aAjX8\n0LK2Do69U8+nd1IFjew9wLzWRiTgbdA8FfZZi9nNM0KocHtDYoq3BWp8W6W+8mQ=\n=eHUi\n-----END PGP SIGNATURE-----\n", "payload": "tree 83e7822f41dd04db2f5e79d75f88a9c838beceee\nparent 010e4c8fe018d703aac38c54307ddbee35edc380\nparent 1ea4dae59699a103209a7ecfc1a03c8df0d211af\nauthor bors[bot] <26634292+bors[bot]@users.noreply.github.com> 1620157350 +0000\ncommitter GitHub <noreply@github.com> 1620157350 +0000\n\nMerge #8732\n\n8732: internal: refactor expansion queries r=matklad a=matklad\n\nbors r+\n\ud83e\udd16\n\nCo-authored-by: Aleksey Kladov <aleksey.kladov@gmail.com>\n"}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/5b663f1b07233442a3b0b58db453504dcc51ddc9", "html_url": "https://github.com/rust-lang/rust/commit/5b663f1b07233442a3b0b58db453504dcc51ddc9", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/5b663f1b07233442a3b0b58db453504dcc51ddc9/comments", "author": {"login": "bors[bot]", "id": 26634292, "node_id": "MDM6Qm90MjY2MzQyOTI=", "avatar_url": "https://avatars.githubusercontent.com/in/1847?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bors%5Bbot%5D", "html_url": "https://github.com/apps/bors", "followers_url": "https://api.github.com/users/bors%5Bbot%5D/followers", "following_url": "https://api.github.com/users/bors%5Bbot%5D/following{/other_user}", "gists_url": "https://api.github.com/users/bors%5Bbot%5D/gists{/gist_id}", "starred_url": "https://api.github.com/users/bors%5Bbot%5D/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bors%5Bbot%5D/subscriptions", "organizations_url": "https://api.github.com/users/bors%5Bbot%5D/orgs", "repos_url": "https://api.github.com/users/bors%5Bbot%5D/repos", "events_url": "https://api.github.com/users/bors%5Bbot%5D/events{/privacy}", "received_events_url": "https://api.github.com/users/bors%5Bbot%5D/received_events", "type": "Bot", "site_admin": false}, "committer": {"login": "web-flow", "id": 19864447, "node_id": "MDQ6VXNlcjE5ODY0NDQ3", "avatar_url": "https://avatars.githubusercontent.com/u/19864447?v=4", "gravatar_id": "", "url": "https://api.github.com/users/web-flow", "html_url": "https://github.com/web-flow", "followers_url": "https://api.github.com/users/web-flow/followers", "following_url": "https://api.github.com/users/web-flow/following{/other_user}", "gists_url": "https://api.github.com/users/web-flow/gists{/gist_id}", "starred_url": "https://api.github.com/users/web-flow/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/web-flow/subscriptions", "organizations_url": "https://api.github.com/users/web-flow/orgs", "repos_url": "https://api.github.com/users/web-flow/repos", "events_url": "https://api.github.com/users/web-flow/events{/privacy}", "received_events_url": "https://api.github.com/users/web-flow/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "010e4c8fe018d703aac38c54307ddbee35edc380", "url": "https://api.github.com/repos/rust-lang/rust/commits/010e4c8fe018d703aac38c54307ddbee35edc380", "html_url": "https://github.com/rust-lang/rust/commit/010e4c8fe018d703aac38c54307ddbee35edc380"}, {"sha": "1ea4dae59699a103209a7ecfc1a03c8df0d211af", "url": "https://api.github.com/repos/rust-lang/rust/commits/1ea4dae59699a103209a7ecfc1a03c8df0d211af", "html_url": "https://github.com/rust-lang/rust/commit/1ea4dae59699a103209a7ecfc1a03c8df0d211af"}], "stats": {"total": 438, "additions": 238, "deletions": 200}, "files": [{"sha": "3e9abd8a1955477752f98e6d42bbc7e89e61ec5f", "filename": "crates/hir_expand/src/db.rs", "status": "modified", "additions": 185, "deletions": 183, "changes": 368, "blob_url": "https://github.com/rust-lang/rust/blob/5b663f1b07233442a3b0b58db453504dcc51ddc9/crates%2Fhir_expand%2Fsrc%2Fdb.rs", "raw_url": "https://github.com/rust-lang/rust/raw/5b663f1b07233442a3b0b58db453504dcc51ddc9/crates%2Fhir_expand%2Fsrc%2Fdb.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fhir_expand%2Fsrc%2Fdb.rs?ref=5b663f1b07233442a3b0b58db453504dcc51ddc9", "patch": "@@ -28,9 +28,9 @@ const TOKEN_LIMIT: usize = 524288;\n #[derive(Debug, Clone, Eq, PartialEq)]\n pub enum TokenExpander {\n     /// Old-style `macro_rules`.\n-    MacroRules(mbe::MacroRules),\n+    MacroRules { mac: mbe::MacroRules, def_site_token_map: mbe::TokenMap },\n     /// AKA macros 2.0.\n-    MacroDef(mbe::MacroDef),\n+    MacroDef { mac: mbe::MacroDef, def_site_token_map: mbe::TokenMap },\n     /// Stuff like `line!` and `file!`.\n     Builtin(BuiltinFnLikeExpander),\n     /// `derive(Copy)` and such.\n@@ -47,8 +47,8 @@ impl TokenExpander {\n         tt: &tt::Subtree,\n     ) -> mbe::ExpandResult<tt::Subtree> {\n         match self {\n-            TokenExpander::MacroRules(it) => it.expand(tt),\n-            TokenExpander::MacroDef(it) => it.expand(tt),\n+            TokenExpander::MacroRules { mac, .. } => mac.expand(tt),\n+            TokenExpander::MacroDef { mac, .. } => mac.expand(tt),\n             TokenExpander::Builtin(it) => it.expand(db, id, tt),\n             // FIXME switch these to ExpandResult as well\n             TokenExpander::BuiltinDerive(it) => it.expand(db, id, tt).into(),\n@@ -63,21 +63,21 @@ impl TokenExpander {\n \n     pub(crate) fn map_id_down(&self, id: tt::TokenId) -> tt::TokenId {\n         match self {\n-            TokenExpander::MacroRules(it) => it.map_id_down(id),\n-            TokenExpander::MacroDef(it) => it.map_id_down(id),\n-            TokenExpander::Builtin(..) => id,\n-            TokenExpander::BuiltinDerive(..) => id,\n-            TokenExpander::ProcMacro(..) => id,\n+            TokenExpander::MacroRules { mac, .. } => mac.map_id_down(id),\n+            TokenExpander::MacroDef { mac, .. } => mac.map_id_down(id),\n+            TokenExpander::Builtin(..)\n+            | TokenExpander::BuiltinDerive(..)\n+            | TokenExpander::ProcMacro(..) => id,\n         }\n     }\n \n     pub(crate) fn map_id_up(&self, id: tt::TokenId) -> (tt::TokenId, mbe::Origin) {\n         match self {\n-            TokenExpander::MacroRules(it) => it.map_id_up(id),\n-            TokenExpander::MacroDef(it) => it.map_id_up(id),\n-            TokenExpander::Builtin(..) => (id, mbe::Origin::Call),\n-            TokenExpander::BuiltinDerive(..) => (id, mbe::Origin::Call),\n-            TokenExpander::ProcMacro(..) => (id, mbe::Origin::Call),\n+            TokenExpander::MacroRules { mac, .. } => mac.map_id_up(id),\n+            TokenExpander::MacroDef { mac, .. } => mac.map_id_up(id),\n+            TokenExpander::Builtin(..)\n+            | TokenExpander::BuiltinDerive(..)\n+            | TokenExpander::ProcMacro(..) => (id, mbe::Origin::Call),\n         }\n     }\n }\n@@ -87,28 +87,48 @@ impl TokenExpander {\n pub trait AstDatabase: SourceDatabase {\n     fn ast_id_map(&self, file_id: HirFileId) -> Arc<AstIdMap>;\n \n+    /// Main public API -- parsis a hir file, not caring whether it's a real\n+    /// file or a macro expansion.\n     #[salsa::transparent]\n     fn parse_or_expand(&self, file_id: HirFileId) -> Option<SyntaxNode>;\n-\n-    #[salsa::interned]\n-    fn intern_macro(&self, macro_call: MacroCallLoc) -> LazyMacroId;\n-    fn macro_arg_text(&self, id: MacroCallId) -> Option<GreenNode>;\n-    #[salsa::transparent]\n-    fn macro_arg(&self, id: MacroCallId) -> Option<Arc<(tt::Subtree, mbe::TokenMap)>>;\n-    fn macro_def(&self, id: MacroDefId) -> Option<Arc<(TokenExpander, mbe::TokenMap)>>;\n+    /// Implementation for the macro case.\n     fn parse_macro_expansion(\n         &self,\n         macro_file: MacroFile,\n     ) -> ExpandResult<Option<(Parse<SyntaxNode>, Arc<mbe::TokenMap>)>>;\n-    fn macro_expand(&self, macro_call: MacroCallId) -> ExpandResult<Option<Arc<tt::Subtree>>>;\n-\n-    /// Firewall query that returns the error from the `macro_expand` query.\n-    fn macro_expand_error(&self, macro_call: MacroCallId) -> Option<ExpandError>;\n \n+    /// Macro ids. That's probably the tricksiest bit in rust-analyzer, and the\n+    /// reason why we use salsa at all.\n+    ///\n+    /// We encode macro definitions into ids of macro calls, this what allows us\n+    /// to be incremental.\n+    #[salsa::interned]\n+    fn intern_macro(&self, macro_call: MacroCallLoc) -> LazyMacroId;\n+    /// Certain built-in macros are eager (`format!(concat!(\"file: \", file!(), \"{}\"\")), 92`).\n+    /// For them, we actually want to encode the whole token tree as an argument.\n     #[salsa::interned]\n     fn intern_eager_expansion(&self, eager: EagerCallLoc) -> EagerMacroId;\n \n+    /// Lowers syntactic macro call to a token tree representation.\n+    #[salsa::transparent]\n+    fn macro_arg(&self, id: MacroCallId) -> Option<Arc<(tt::Subtree, mbe::TokenMap)>>;\n+    /// Extracts syntax node, corresponding to a macro call. That's a firewall\n+    /// query, only typing in the macro call itself changes the returned\n+    /// subtree.\n+    fn macro_arg_text(&self, id: MacroCallId) -> Option<GreenNode>;\n+    /// Gets the expander for this macro. This compiles declarative macros, and\n+    /// just fetches procedural ones.\n+    fn macro_def(&self, id: MacroDefId) -> Option<Arc<TokenExpander>>;\n+\n+    /// Expand macro call to a token tree. This query is LRUed (we keep 128 or so results in memory)\n+    fn macro_expand(&self, macro_call: MacroCallId) -> ExpandResult<Option<Arc<tt::Subtree>>>;\n+    /// Special case of the previous query for procedural macros. We can't LRU\n+    /// proc macros, since they are not deterministic in general, and\n+    /// non-determinism breaks salsa in a very, very, very bad way. @edwin0cheng\n+    /// heroically debugged this once!\n     fn expand_proc_macro(&self, call: MacroCallId) -> Result<tt::Subtree, mbe::ExpandError>;\n+    /// Firewall query that returns the error from the `macro_expand` query.\n+    fn macro_expand_error(&self, macro_call: MacroCallId) -> Option<ExpandError>;\n \n     fn hygiene_frame(&self, file_id: HirFileId) -> Arc<HygieneFrame>;\n }\n@@ -123,88 +143,188 @@ pub fn expand_hypothetical(\n     hypothetical_args: &ast::TokenTree,\n     token_to_map: SyntaxToken,\n ) -> Option<(SyntaxNode, SyntaxToken)> {\n-    let macro_file = MacroFile { macro_call_id: actual_macro_call };\n     let (tt, tmap_1) = mbe::syntax_node_to_token_tree(hypothetical_args.syntax());\n     let range =\n         token_to_map.text_range().checked_sub(hypothetical_args.syntax().text_range().start())?;\n     let token_id = tmap_1.token_by_range(range)?;\n-    let macro_def = expander(db, actual_macro_call)?;\n+\n+    let lazy_id = match actual_macro_call {\n+        MacroCallId::LazyMacro(id) => id,\n+        MacroCallId::EagerMacro(_) => return None,\n+    };\n+\n+    let macro_def = {\n+        let loc = db.lookup_intern_macro(lazy_id);\n+        db.macro_def(loc.def)?\n+    };\n+\n+    let hypothetical_expansion = macro_def.expand(db, lazy_id, &tt);\n+\n+    let fragment_kind = to_fragment_kind(db, actual_macro_call);\n+\n     let (node, tmap_2) =\n-        parse_macro_with_arg(db, macro_file, Some(std::sync::Arc::new((tt, tmap_1)))).value?;\n-    let token_id = macro_def.0.map_id_down(token_id);\n+        mbe::token_tree_to_syntax_node(&hypothetical_expansion.value, fragment_kind).ok()?;\n+\n+    let token_id = macro_def.map_id_down(token_id);\n     let range = tmap_2.range_by_token(token_id)?.by_kind(token_to_map.kind())?;\n     let token = node.syntax_node().covering_element(range).into_token()?;\n     Some((node.syntax_node(), token))\n }\n \n fn ast_id_map(db: &dyn AstDatabase, file_id: HirFileId) -> Arc<AstIdMap> {\n-    let map =\n-        db.parse_or_expand(file_id).map_or_else(AstIdMap::default, |it| AstIdMap::from_source(&it));\n+    let map = db.parse_or_expand(file_id).map(|it| AstIdMap::from_source(&it)).unwrap_or_default();\n     Arc::new(map)\n }\n \n-fn macro_def(db: &dyn AstDatabase, id: MacroDefId) -> Option<Arc<(TokenExpander, mbe::TokenMap)>> {\n+fn parse_or_expand(db: &dyn AstDatabase, file_id: HirFileId) -> Option<SyntaxNode> {\n+    match file_id.0 {\n+        HirFileIdRepr::FileId(file_id) => Some(db.parse(file_id).tree().syntax().clone()),\n+        HirFileIdRepr::MacroFile(macro_file) => {\n+            db.parse_macro_expansion(macro_file).value.map(|(it, _)| it.syntax_node())\n+        }\n+    }\n+}\n+\n+fn parse_macro_expansion(\n+    db: &dyn AstDatabase,\n+    macro_file: MacroFile,\n+) -> ExpandResult<Option<(Parse<SyntaxNode>, Arc<mbe::TokenMap>)>> {\n+    let _p = profile::span(\"parse_macro_expansion\");\n+    let result = db.macro_expand(macro_file.macro_call_id);\n+\n+    if let Some(err) = &result.err {\n+        // Note:\n+        // The final goal we would like to make all parse_macro success,\n+        // such that the following log will not call anyway.\n+        match macro_file.macro_call_id {\n+            MacroCallId::LazyMacro(id) => {\n+                let loc: MacroCallLoc = db.lookup_intern_macro(id);\n+                let node = loc.kind.node(db);\n+\n+                // collect parent information for warning log\n+                let parents = std::iter::successors(loc.kind.file_id().call_node(db), |it| {\n+                    it.file_id.call_node(db)\n+                })\n+                .map(|n| format!(\"{:#}\", n.value))\n+                .collect::<Vec<_>>()\n+                .join(\"\\n\");\n+\n+                log::warn!(\n+                    \"fail on macro_parse: (reason: {:?} macro_call: {:#}) parents: {}\",\n+                    err,\n+                    node.value,\n+                    parents\n+                );\n+            }\n+            _ => {\n+                log::warn!(\"fail on macro_parse: (reason: {:?})\", err);\n+            }\n+        }\n+    }\n+    let tt = match result.value {\n+        Some(tt) => tt,\n+        None => return ExpandResult { value: None, err: result.err },\n+    };\n+\n+    let fragment_kind = to_fragment_kind(db, macro_file.macro_call_id);\n+\n+    log::debug!(\"expanded = {}\", tt.as_debug_string());\n+    log::debug!(\"kind = {:?}\", fragment_kind);\n+\n+    let (parse, rev_token_map) = match mbe::token_tree_to_syntax_node(&tt, fragment_kind) {\n+        Ok(it) => it,\n+        Err(err) => {\n+            log::debug!(\n+                \"failed to parse expanstion to {:?} = {}\",\n+                fragment_kind,\n+                tt.as_debug_string()\n+            );\n+            return ExpandResult::only_err(err);\n+        }\n+    };\n+\n+    match result.err {\n+        Some(err) => {\n+            // Safety check for recursive identity macro.\n+            let node = parse.syntax_node();\n+            let file: HirFileId = macro_file.into();\n+            let call_node = match file.call_node(db) {\n+                Some(it) => it,\n+                None => {\n+                    return ExpandResult::only_err(err);\n+                }\n+            };\n+            if is_self_replicating(&node, &call_node.value) {\n+                return ExpandResult::only_err(err);\n+            } else {\n+                ExpandResult { value: Some((parse, Arc::new(rev_token_map))), err: Some(err) }\n+            }\n+        }\n+        None => {\n+            log::debug!(\"parse = {:?}\", parse.syntax_node().kind());\n+            ExpandResult { value: Some((parse, Arc::new(rev_token_map))), err: None }\n+        }\n+    }\n+}\n+\n+fn macro_arg(db: &dyn AstDatabase, id: MacroCallId) -> Option<Arc<(tt::Subtree, mbe::TokenMap)>> {\n+    let arg = db.macro_arg_text(id)?;\n+    let (tt, tmap) = mbe::syntax_node_to_token_tree(&SyntaxNode::new_root(arg));\n+    Some(Arc::new((tt, tmap)))\n+}\n+\n+fn macro_arg_text(db: &dyn AstDatabase, id: MacroCallId) -> Option<GreenNode> {\n+    let id = match id {\n+        MacroCallId::LazyMacro(id) => id,\n+        MacroCallId::EagerMacro(_id) => {\n+            // FIXME: support macro_arg for eager macro\n+            return None;\n+        }\n+    };\n+    let loc = db.lookup_intern_macro(id);\n+    let arg = loc.kind.arg(db)?;\n+    Some(arg.green())\n+}\n+\n+fn macro_def(db: &dyn AstDatabase, id: MacroDefId) -> Option<Arc<TokenExpander>> {\n     match id.kind {\n         MacroDefKind::Declarative(ast_id) => match ast_id.to_node(db) {\n             ast::Macro::MacroRules(macro_rules) => {\n                 let arg = macro_rules.token_tree()?;\n-                let (tt, tmap) = mbe::ast_to_token_tree(&arg);\n-                let rules = match mbe::MacroRules::parse(&tt) {\n+                let (tt, def_site_token_map) = mbe::ast_to_token_tree(&arg);\n+                let mac = match mbe::MacroRules::parse(&tt) {\n                     Ok(it) => it,\n                     Err(err) => {\n                         let name = macro_rules.name().map(|n| n.to_string()).unwrap_or_default();\n                         log::warn!(\"fail on macro_def parse ({}): {:?} {:#?}\", name, err, tt);\n                         return None;\n                     }\n                 };\n-                Some(Arc::new((TokenExpander::MacroRules(rules), tmap)))\n+                Some(Arc::new(TokenExpander::MacroRules { mac, def_site_token_map }))\n             }\n             ast::Macro::MacroDef(macro_def) => {\n                 let arg = macro_def.body()?;\n-                let (tt, tmap) = mbe::ast_to_token_tree(&arg);\n-                let rules = match mbe::MacroDef::parse(&tt) {\n+                let (tt, def_site_token_map) = mbe::ast_to_token_tree(&arg);\n+                let mac = match mbe::MacroDef::parse(&tt) {\n                     Ok(it) => it,\n                     Err(err) => {\n                         let name = macro_def.name().map(|n| n.to_string()).unwrap_or_default();\n                         log::warn!(\"fail on macro_def parse ({}): {:?} {:#?}\", name, err, tt);\n                         return None;\n                     }\n                 };\n-                Some(Arc::new((TokenExpander::MacroDef(rules), tmap)))\n+                Some(Arc::new(TokenExpander::MacroDef { mac, def_site_token_map }))\n             }\n         },\n-        MacroDefKind::BuiltIn(expander, _) => {\n-            Some(Arc::new((TokenExpander::Builtin(expander), mbe::TokenMap::default())))\n-        }\n+        MacroDefKind::BuiltIn(expander, _) => Some(Arc::new(TokenExpander::Builtin(expander))),\n         MacroDefKind::BuiltInDerive(expander, _) => {\n-            Some(Arc::new((TokenExpander::BuiltinDerive(expander), mbe::TokenMap::default())))\n+            Some(Arc::new(TokenExpander::BuiltinDerive(expander)))\n         }\n         MacroDefKind::BuiltInEager(..) => None,\n-        MacroDefKind::ProcMacro(expander, ..) => {\n-            Some(Arc::new((TokenExpander::ProcMacro(expander), mbe::TokenMap::default())))\n-        }\n+        MacroDefKind::ProcMacro(expander, ..) => Some(Arc::new(TokenExpander::ProcMacro(expander))),\n     }\n }\n \n-fn macro_arg_text(db: &dyn AstDatabase, id: MacroCallId) -> Option<GreenNode> {\n-    let id = match id {\n-        MacroCallId::LazyMacro(id) => id,\n-        MacroCallId::EagerMacro(_id) => {\n-            // FIXME: support macro_arg for eager macro\n-            return None;\n-        }\n-    };\n-    let loc = db.lookup_intern_macro(id);\n-    let arg = loc.kind.arg(db)?;\n-    Some(arg.green())\n-}\n-\n-fn macro_arg(db: &dyn AstDatabase, id: MacroCallId) -> Option<Arc<(tt::Subtree, mbe::TokenMap)>> {\n-    let arg = db.macro_arg_text(id)?;\n-    let (tt, tmap) = mbe::syntax_node_to_token_tree(&SyntaxNode::new_root(arg));\n-    Some(Arc::new((tt, tmap)))\n-}\n-\n fn macro_expand(db: &dyn AstDatabase, id: MacroCallId) -> ExpandResult<Option<Arc<tt::Subtree>>> {\n     macro_expand_with_arg(db, id, None)\n }\n@@ -213,19 +333,6 @@ fn macro_expand_error(db: &dyn AstDatabase, macro_call: MacroCallId) -> Option<E\n     db.macro_expand(macro_call).err\n }\n \n-fn expander(db: &dyn AstDatabase, id: MacroCallId) -> Option<Arc<(TokenExpander, mbe::TokenMap)>> {\n-    let lazy_id = match id {\n-        MacroCallId::LazyMacro(id) => id,\n-        MacroCallId::EagerMacro(_id) => {\n-            return None;\n-        }\n-    };\n-\n-    let loc = db.lookup_intern_macro(lazy_id);\n-    let macro_rules = db.macro_def(loc.def)?;\n-    Some(macro_rules)\n-}\n-\n fn macro_expand_with_arg(\n     db: &dyn AstDatabase,\n     id: MacroCallId,\n@@ -259,7 +366,7 @@ fn macro_expand_with_arg(\n         Some(it) => it,\n         None => return ExpandResult::str_err(\"Fail to find macro definition\".into()),\n     };\n-    let ExpandResult { value: tt, err } = macro_rules.0.expand(db, lazy_id, &macro_arg.0);\n+    let ExpandResult { value: tt, err } = macro_rules.expand(db, lazy_id, &macro_arg.0);\n     // Set a hard limit for the expanded tt\n     let count = tt.count();\n     if count > TOKEN_LIMIT {\n@@ -299,111 +406,6 @@ fn expand_proc_macro(\n     expander.expand(db, loc.krate, &macro_arg.0)\n }\n \n-fn parse_or_expand(db: &dyn AstDatabase, file_id: HirFileId) -> Option<SyntaxNode> {\n-    match file_id.0 {\n-        HirFileIdRepr::FileId(file_id) => Some(db.parse(file_id).tree().syntax().clone()),\n-        HirFileIdRepr::MacroFile(macro_file) => {\n-            db.parse_macro_expansion(macro_file).value.map(|(it, _)| it.syntax_node())\n-        }\n-    }\n-}\n-\n-fn parse_macro_expansion(\n-    db: &dyn AstDatabase,\n-    macro_file: MacroFile,\n-) -> ExpandResult<Option<(Parse<SyntaxNode>, Arc<mbe::TokenMap>)>> {\n-    parse_macro_with_arg(db, macro_file, None)\n-}\n-\n-fn parse_macro_with_arg(\n-    db: &dyn AstDatabase,\n-    macro_file: MacroFile,\n-    arg: Option<Arc<(tt::Subtree, mbe::TokenMap)>>,\n-) -> ExpandResult<Option<(Parse<SyntaxNode>, Arc<mbe::TokenMap>)>> {\n-    let macro_call_id = macro_file.macro_call_id;\n-    let result = if let Some(arg) = arg {\n-        macro_expand_with_arg(db, macro_call_id, Some(arg))\n-    } else {\n-        db.macro_expand(macro_call_id)\n-    };\n-\n-    let _p = profile::span(\"parse_macro_expansion\");\n-\n-    if let Some(err) = &result.err {\n-        // Note:\n-        // The final goal we would like to make all parse_macro success,\n-        // such that the following log will not call anyway.\n-        match macro_call_id {\n-            MacroCallId::LazyMacro(id) => {\n-                let loc: MacroCallLoc = db.lookup_intern_macro(id);\n-                let node = loc.kind.node(db);\n-\n-                // collect parent information for warning log\n-                let parents = std::iter::successors(loc.kind.file_id().call_node(db), |it| {\n-                    it.file_id.call_node(db)\n-                })\n-                .map(|n| format!(\"{:#}\", n.value))\n-                .collect::<Vec<_>>()\n-                .join(\"\\n\");\n-\n-                log::warn!(\n-                    \"fail on macro_parse: (reason: {:?} macro_call: {:#}) parents: {}\",\n-                    err,\n-                    node.value,\n-                    parents\n-                );\n-            }\n-            _ => {\n-                log::warn!(\"fail on macro_parse: (reason: {:?})\", err);\n-            }\n-        }\n-    }\n-    let tt = match result.value {\n-        Some(tt) => tt,\n-        None => return ExpandResult { value: None, err: result.err },\n-    };\n-\n-    let fragment_kind = to_fragment_kind(db, macro_call_id);\n-\n-    log::debug!(\"expanded = {}\", tt.as_debug_string());\n-    log::debug!(\"kind = {:?}\", fragment_kind);\n-\n-    let (parse, rev_token_map) = match mbe::token_tree_to_syntax_node(&tt, fragment_kind) {\n-        Ok(it) => it,\n-        Err(err) => {\n-            log::debug!(\n-                \"failed to parse expanstion to {:?} = {}\",\n-                fragment_kind,\n-                tt.as_debug_string()\n-            );\n-            return ExpandResult::only_err(err);\n-        }\n-    };\n-\n-    match result.err {\n-        Some(err) => {\n-            // Safety check for recursive identity macro.\n-            let node = parse.syntax_node();\n-            let file: HirFileId = macro_file.into();\n-            let call_node = match file.call_node(db) {\n-                Some(it) => it,\n-                None => {\n-                    return ExpandResult::only_err(err);\n-                }\n-            };\n-            if is_self_replicating(&node, &call_node.value) {\n-                return ExpandResult::only_err(err);\n-            } else {\n-                ExpandResult { value: Some((parse, Arc::new(rev_token_map))), err: Some(err) }\n-            }\n-        }\n-        None => {\n-            log::debug!(\"parse = {:?}\", parse.syntax_node().kind());\n-            ExpandResult { value: Some((parse, Arc::new(rev_token_map))), err: None }\n-        }\n-    }\n-}\n-\n fn is_self_replicating(from: &SyntaxNode, to: &SyntaxNode) -> bool {\n     if diff(from, to).is_empty() {\n         return true;"}, {"sha": "ed61ebca3ad34d430b097d137905ad0bfc0be4d8", "filename": "crates/hir_expand/src/hygiene.rs", "status": "modified", "additions": 10, "deletions": 6, "changes": 16, "blob_url": "https://github.com/rust-lang/rust/blob/5b663f1b07233442a3b0b58db453504dcc51ddc9/crates%2Fhir_expand%2Fsrc%2Fhygiene.rs", "raw_url": "https://github.com/rust-lang/rust/raw/5b663f1b07233442a3b0b58db453504dcc51ddc9/crates%2Fhir_expand%2Fsrc%2Fhygiene.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fhir_expand%2Fsrc%2Fhygiene.rs?ref=5b663f1b07233442a3b0b58db453504dcc51ddc9", "patch": "@@ -5,6 +5,7 @@\n use std::sync::Arc;\n \n use base_db::CrateId;\n+use db::TokenExpander;\n use either::Either;\n use mbe::Origin;\n use parser::SyntaxKind;\n@@ -115,7 +116,7 @@ struct HygieneInfo {\n     /// The `macro_rules!` arguments.\n     def_start: Option<InFile<TextSize>>,\n \n-    macro_def: Arc<(db::TokenExpander, mbe::TokenMap)>,\n+    macro_def: Arc<TokenExpander>,\n     macro_arg: Arc<(tt::Subtree, mbe::TokenMap)>,\n     exp_map: Arc<mbe::TokenMap>,\n }\n@@ -124,13 +125,16 @@ impl HygieneInfo {\n     fn map_ident_up(&self, token: TextRange) -> Option<(InFile<TextRange>, Origin)> {\n         let token_id = self.exp_map.token_by_range(token)?;\n \n-        let (token_id, origin) = self.macro_def.0.map_id_up(token_id);\n+        let (token_id, origin) = self.macro_def.map_id_up(token_id);\n         let (token_map, tt) = match origin {\n             mbe::Origin::Call => (&self.macro_arg.1, self.arg_start),\n-            mbe::Origin::Def => (\n-                &self.macro_def.1,\n-                *self.def_start.as_ref().expect(\"`Origin::Def` used with non-`macro_rules!` macro\"),\n-            ),\n+            mbe::Origin::Def => match (&*self.macro_def, self.def_start) {\n+                (TokenExpander::MacroDef { def_site_token_map, .. }, Some(tt))\n+                | (TokenExpander::MacroRules { def_site_token_map, .. }, Some(tt)) => {\n+                    (def_site_token_map, tt)\n+                }\n+                _ => panic!(\"`Origin::Def` used with non-`macro_rules!` macro\"),\n+            },\n         };\n \n         let range = token_map.range_by_token(token_id)?.by_kind(SyntaxKind::IDENT)?;"}, {"sha": "0402640de306ba0739e64f7a3dd172b91a262368", "filename": "crates/hir_expand/src/lib.rs", "status": "modified", "additions": 10, "deletions": 11, "changes": 21, "blob_url": "https://github.com/rust-lang/rust/blob/5b663f1b07233442a3b0b58db453504dcc51ddc9/crates%2Fhir_expand%2Fsrc%2Flib.rs", "raw_url": "https://github.com/rust-lang/rust/raw/5b663f1b07233442a3b0b58db453504dcc51ddc9/crates%2Fhir_expand%2Fsrc%2Flib.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fhir_expand%2Fsrc%2Flib.rs?ref=5b663f1b07233442a3b0b58db453504dcc51ddc9", "patch": "@@ -351,7 +351,7 @@ pub struct ExpansionInfo {\n     /// The `macro_rules!` arguments.\n     def: Option<InFile<ast::TokenTree>>,\n \n-    macro_def: Arc<(db::TokenExpander, mbe::TokenMap)>,\n+    macro_def: Arc<db::TokenExpander>,\n     macro_arg: Arc<(tt::Subtree, mbe::TokenMap)>,\n     exp_map: Arc<mbe::TokenMap>,\n }\n@@ -368,7 +368,7 @@ impl ExpansionInfo {\n         assert_eq!(token.file_id, self.arg.file_id);\n         let range = token.value.text_range().checked_sub(self.arg.value.text_range().start())?;\n         let token_id = self.macro_arg.1.token_by_range(range)?;\n-        let token_id = self.macro_def.0.map_id_down(token_id);\n+        let token_id = self.macro_def.map_id_down(token_id);\n \n         let range = self.exp_map.range_by_token(token_id)?.by_kind(token.value.kind())?;\n \n@@ -383,17 +383,16 @@ impl ExpansionInfo {\n     ) -> Option<(InFile<SyntaxToken>, Origin)> {\n         let token_id = self.exp_map.token_by_range(token.value.text_range())?;\n \n-        let (token_id, origin) = self.macro_def.0.map_id_up(token_id);\n+        let (token_id, origin) = self.macro_def.map_id_up(token_id);\n         let (token_map, tt) = match origin {\n             mbe::Origin::Call => (&self.macro_arg.1, self.arg.clone()),\n-            mbe::Origin::Def => (\n-                &self.macro_def.1,\n-                self.def\n-                    .as_ref()\n-                    .expect(\"`Origin::Def` used with non-`macro_rules!` macro\")\n-                    .as_ref()\n-                    .map(|tt| tt.syntax().clone()),\n-            ),\n+            mbe::Origin::Def => match (&*self.macro_def, self.def.as_ref()) {\n+                (db::TokenExpander::MacroRules { def_site_token_map, .. }, Some(tt))\n+                | (db::TokenExpander::MacroDef { def_site_token_map, .. }, Some(tt)) => {\n+                    (def_site_token_map, tt.as_ref().map(|tt| tt.syntax().clone()))\n+                }\n+                _ => panic!(\"`Origin::Def` used with non-`macro_rules!` macro\"),\n+            },\n         };\n \n         let range = token_map.range_by_token(token_id)?.by_kind(token.value.kind())?;"}, {"sha": "00de7a711755cab84d36aaf806fda0a2ad3b8424", "filename": "docs/dev/style.md", "status": "modified", "additions": 33, "deletions": 0, "changes": 33, "blob_url": "https://github.com/rust-lang/rust/blob/5b663f1b07233442a3b0b58db453504dcc51ddc9/docs%2Fdev%2Fstyle.md", "raw_url": "https://github.com/rust-lang/rust/raw/5b663f1b07233442a3b0b58db453504dcc51ddc9/docs%2Fdev%2Fstyle.md", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/docs%2Fdev%2Fstyle.md?ref=5b663f1b07233442a3b0b58db453504dcc51ddc9", "patch": "@@ -449,6 +449,39 @@ fn query_all(name: String, case_sensitive: bool) -> Vec<Item> { ... }\n fn query_first(name: String, case_sensitive: bool) -> Option<Item> { ... }\n ```\n \n+## Prefer Separate Functions Over Parameters\n+\n+If a function has a `bool` or an `Option` parameter, and it is always called with `true`, `false`, `Some` and `None` literals, split the function in two.\n+\n+```rust\n+// GOOD\n+fn caller_a() {\n+    foo()\n+}\n+\n+fn caller_b() {\n+    foo_with_bar(Bar::new())\n+}\n+\n+fn foo() { ... }\n+fn foo_with_bar(bar: Bar) { ... }\n+\n+// BAD\n+fn caller_a() {\n+    foo(None)\n+}\n+\n+fn caller_b() {\n+    foo(Some(Bar::new()))\n+}\n+\n+fn foo(bar: Option<Bar>) { ... }\n+```\n+\n+**Rationale:** more often than not, such functions display \"`false sharing`\" -- they have additional `if` branching inside for two different cases.\n+Splitting the two different control flows into two functions simplifies each path, and remove cross-dependencies between the two paths.\n+If there's common code between `foo` and `foo_with_bar`, extract *that* into a common helper.\n+\n ## Avoid Monomorphization\n \n Avoid making a lot of code type parametric, *especially* on the boundaries between crates."}]}