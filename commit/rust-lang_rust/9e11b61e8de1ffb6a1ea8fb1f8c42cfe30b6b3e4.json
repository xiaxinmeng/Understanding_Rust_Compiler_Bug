{"sha": "9e11b61e8de1ffb6a1ea8fb1f8c42cfe30b6b3e4", "node_id": "MDY6Q29tbWl0NzI0NzEyOjllMTFiNjFlOGRlMWZmYjZhMWVhOGZiMWY4YzQyY2ZlMzBiNmIzZTQ=", "commit": {"author": {"name": "Eric Huss", "email": "eric@huss.org", "date": "2021-01-11T18:13:08Z"}, "committer": {"name": "Eric Huss", "email": "eric@huss.org", "date": "2021-05-22T23:22:09Z"}, "message": "linkchecker: Organize state into a struct, and add report.\n\nMoves all the state into a struct so it doesn't need to be passed around\nas much.\n\nAlso adds a report showing how long it took and what it found.\n\nThis includes a minor change: a failure to load a file is now an error,\ninstead of being ignored. This should only happen if there is a\npermission error or some other shenanigans going on.", "tree": {"sha": "4a735e0ff20b8cf5f29bf82c82efb1161f2a6856", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/4a735e0ff20b8cf5f29bf82c82efb1161f2a6856"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/9e11b61e8de1ffb6a1ea8fb1f8c42cfe30b6b3e4", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/9e11b61e8de1ffb6a1ea8fb1f8c42cfe30b6b3e4", "html_url": "https://github.com/rust-lang/rust/commit/9e11b61e8de1ffb6a1ea8fb1f8c42cfe30b6b3e4", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/9e11b61e8de1ffb6a1ea8fb1f8c42cfe30b6b3e4/comments", "author": {"login": "ehuss", "id": 43198, "node_id": "MDQ6VXNlcjQzMTk4", "avatar_url": "https://avatars.githubusercontent.com/u/43198?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ehuss", "html_url": "https://github.com/ehuss", "followers_url": "https://api.github.com/users/ehuss/followers", "following_url": "https://api.github.com/users/ehuss/following{/other_user}", "gists_url": "https://api.github.com/users/ehuss/gists{/gist_id}", "starred_url": "https://api.github.com/users/ehuss/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ehuss/subscriptions", "organizations_url": "https://api.github.com/users/ehuss/orgs", "repos_url": "https://api.github.com/users/ehuss/repos", "events_url": "https://api.github.com/users/ehuss/events{/privacy}", "received_events_url": "https://api.github.com/users/ehuss/received_events", "type": "User", "site_admin": false}, "committer": {"login": "ehuss", "id": 43198, "node_id": "MDQ6VXNlcjQzMTk4", "avatar_url": "https://avatars.githubusercontent.com/u/43198?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ehuss", "html_url": "https://github.com/ehuss", "followers_url": "https://api.github.com/users/ehuss/followers", "following_url": "https://api.github.com/users/ehuss/following{/other_user}", "gists_url": "https://api.github.com/users/ehuss/gists{/gist_id}", "starred_url": "https://api.github.com/users/ehuss/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ehuss/subscriptions", "organizations_url": "https://api.github.com/users/ehuss/orgs", "repos_url": "https://api.github.com/users/ehuss/repos", "events_url": "https://api.github.com/users/ehuss/events{/privacy}", "received_events_url": "https://api.github.com/users/ehuss/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "e4ca1662f255db774dcd16ed9c3776bf25ac8361", "url": "https://api.github.com/repos/rust-lang/rust/commits/e4ca1662f255db774dcd16ed9c3776bf25ac8361", "html_url": "https://github.com/rust-lang/rust/commit/e4ca1662f255db774dcd16ed9c3776bf25ac8361"}], "stats": {"total": 465, "additions": 257, "deletions": 208}, "files": [{"sha": "7df4f5a9c46d5e7eff87f9128e69dfa978ccb5c1", "filename": "src/tools/linkchecker/main.rs", "status": "modified", "additions": 257, "deletions": 208, "changes": 465, "blob_url": "https://github.com/rust-lang/rust/blob/9e11b61e8de1ffb6a1ea8fb1f8c42cfe30b6b3e4/src%2Ftools%2Flinkchecker%2Fmain.rs", "raw_url": "https://github.com/rust-lang/rust/raw/9e11b61e8de1ffb6a1ea8fb1f8c42cfe30b6b3e4/src%2Ftools%2Flinkchecker%2Fmain.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ftools%2Flinkchecker%2Fmain.rs?ref=9e11b61e8de1ffb6a1ea8fb1f8c42cfe30b6b3e4", "patch": "@@ -20,6 +20,7 @@ use std::env;\n use std::fs;\n use std::path::{Component, Path, PathBuf};\n use std::rc::Rc;\n+use std::time::Instant;\n \n use once_cell::sync::Lazy;\n use regex::Regex;\n@@ -89,16 +90,41 @@ macro_rules! t {\n fn main() {\n     let docs = env::args_os().nth(1).unwrap();\n     let docs = env::current_dir().unwrap().join(docs);\n-    let mut errors = false;\n-    walk(&mut HashMap::new(), &docs, &docs, &mut errors);\n-    if errors {\n-        panic!(\"found some broken links\");\n+    let mut checker = Checker {\n+        root: docs.clone(),\n+        cache: HashMap::new(),\n+        errors: 0,\n+        start: Instant::now(),\n+        html_files: 0,\n+        html_redirects: 0,\n+        links_checked: 0,\n+        links_ignored_external: 0,\n+        links_ignored_exception: 0,\n+        intra_doc_exceptions: 0,\n+    };\n+    checker.walk(&docs);\n+    checker.report();\n+    if checker.errors != 0 {\n+        println!(\"found some broken links\");\n+        std::process::exit(1);\n     }\n }\n \n+struct Checker {\n+    root: PathBuf,\n+    cache: Cache,\n+    errors: u32,\n+    start: Instant,\n+    html_files: u32,\n+    html_redirects: u32,\n+    links_checked: u32,\n+    links_ignored_external: u32,\n+    links_ignored_exception: u32,\n+    intra_doc_exceptions: u32,\n+}\n+\n #[derive(Debug)]\n pub enum LoadError {\n-    IOError(std::io::Error),\n     BrokenRedirect(PathBuf, std::io::Error),\n     IsRedirect,\n }\n@@ -131,13 +157,13 @@ fn small_url_encode(s: &str) -> String {\n }\n \n impl FileEntry {\n-    fn parse_ids(&mut self, file: &Path, contents: &str, errors: &mut bool) {\n+    fn parse_ids(&mut self, file: &Path, contents: &str, errors: &mut u32) {\n         if self.ids.is_empty() {\n             with_attrs_in_source(contents, \" id\", |fragment, i, _| {\n                 let frag = fragment.trim_start_matches(\"#\").to_owned();\n                 let encoded = small_url_encode(&frag);\n                 if !self.ids.insert(frag) {\n-                    *errors = true;\n+                    *errors += 1;\n                     println!(\"{}:{}: id is not unique: `{}`\", file.display(), i, fragment);\n                 }\n                 // Just in case, we also add the encoded id.\n@@ -147,239 +173,262 @@ impl FileEntry {\n     }\n }\n \n-fn walk(cache: &mut Cache, root: &Path, dir: &Path, errors: &mut bool) {\n-    for entry in t!(dir.read_dir()).map(|e| t!(e)) {\n-        let path = entry.path();\n-        let kind = t!(entry.file_type());\n-        if kind.is_dir() {\n-            walk(cache, root, &path, errors);\n-        } else {\n-            let pretty_path = check(cache, root, &path, errors);\n-            if let Some(pretty_path) = pretty_path {\n-                let entry = cache.get_mut(&pretty_path).unwrap();\n-                // we don't need the source anymore,\n-                // so drop to reduce memory-usage\n-                entry.source = Rc::new(String::new());\n+impl Checker {\n+    fn walk(&mut self, dir: &Path) {\n+        for entry in t!(dir.read_dir()).map(|e| t!(e)) {\n+            let path = entry.path();\n+            let kind = t!(entry.file_type());\n+            if kind.is_dir() {\n+                self.walk(&path);\n+            } else {\n+                let pretty_path = self.check(&path);\n+                if let Some(pretty_path) = pretty_path {\n+                    let entry = self.cache.get_mut(&pretty_path).unwrap();\n+                    // we don't need the source anymore,\n+                    // so drop to reduce memory-usage\n+                    entry.source = Rc::new(String::new());\n+                }\n             }\n         }\n     }\n-}\n \n-fn is_intra_doc_exception(file: &Path, link: &str) -> bool {\n-    if let Some(entry) = INTRA_DOC_LINK_EXCEPTIONS.iter().find(|&(f, _)| file.ends_with(f)) {\n-        entry.1.is_empty() || entry.1.contains(&link)\n-    } else {\n-        false\n-    }\n-}\n-\n-fn is_exception(file: &Path, link: &str) -> bool {\n-    if let Some(entry) = LINKCHECK_EXCEPTIONS.iter().find(|&(f, _)| file.ends_with(f)) {\n-        entry.1.contains(&link)\n-    } else {\n-        // FIXME(#63351): Concat trait in alloc/slice reexported in primitive page\n-        //\n-        // NOTE: This cannot be added to `LINKCHECK_EXCEPTIONS` because the resolved path\n-        // calculated in `check` function is outside `build/<triple>/doc` dir.\n-        // So the `strip_prefix` method just returns the old absolute broken path.\n-        if file.ends_with(\"std/primitive.slice.html\") {\n-            if link.ends_with(\"primitive.slice.html\") {\n-                return true;\n-            }\n+    fn check(&mut self, file: &Path) -> Option<PathBuf> {\n+        // Ignore non-HTML files.\n+        if file.extension().and_then(|s| s.to_str()) != Some(\"html\") {\n+            return None;\n         }\n-        false\n-    }\n-}\n+        self.html_files += 1;\n \n-fn check(cache: &mut Cache, root: &Path, file: &Path, errors: &mut bool) -> Option<PathBuf> {\n-    // Ignore non-HTML files.\n-    if file.extension().and_then(|s| s.to_str()) != Some(\"html\") {\n-        return None;\n-    }\n-\n-    let res = load_file(cache, root, file, SkipRedirect);\n-    let (pretty_file, contents) = match res {\n-        Ok(res) => res,\n-        Err(_) => return None,\n-    };\n-    {\n-        cache.get_mut(&pretty_file).unwrap().parse_ids(&pretty_file, &contents, errors);\n-    }\n-\n-    // Search for anything that's the regex 'href[ ]*=[ ]*\".*?\"'\n-    with_attrs_in_source(&contents, \" href\", |url, i, base| {\n-        // Ignore external URLs\n-        if url.starts_with(\"http:\")\n-            || url.starts_with(\"https:\")\n-            || url.starts_with(\"javascript:\")\n-            || url.starts_with(\"ftp:\")\n-            || url.starts_with(\"irc:\")\n-            || url.starts_with(\"data:\")\n-        {\n-            return;\n-        }\n-        let (url, fragment) = match url.split_once('#') {\n-            None => (url, None),\n-            Some((url, fragment)) => (url, Some(fragment)),\n+        let res = self.load_file(file, SkipRedirect);\n+        let (pretty_file, contents) = match res {\n+            Ok(res) => res,\n+            Err(_) => return None,\n         };\n-        // NB: the `splitn` always succeeds, even if the delimiter is not present.\n-        let url = url.splitn(2, '?').next().unwrap();\n-\n-        // Once we've plucked out the URL, parse it using our base url and\n-        // then try to extract a file path.\n-        let mut path = file.to_path_buf();\n-        if !base.is_empty() || !url.is_empty() {\n-            path.pop();\n-            for part in Path::new(base).join(url).components() {\n-                match part {\n-                    Component::Prefix(_) | Component::RootDir => {\n-                        // Avoid absolute paths as they make the docs not\n-                        // relocatable by making assumptions on where the docs\n-                        // are hosted relative to the site root.\n-                        *errors = true;\n-                        println!(\n-                            \"{}:{}: absolute path - {}\",\n-                            pretty_file.display(),\n-                            i + 1,\n-                            Path::new(base).join(url).display()\n-                        );\n-                        return;\n-                    }\n-                    Component::CurDir => {}\n-                    Component::ParentDir => {\n-                        path.pop();\n-                    }\n-                    Component::Normal(s) => {\n-                        path.push(s);\n-                    }\n-                }\n-            }\n-        }\n-\n-        // Alright, if we've found a file name then this file had better\n-        // exist! If it doesn't then we register and print an error.\n-        if path.exists() {\n-            if path.is_dir() {\n-                // Links to directories show as directory listings when viewing\n-                // the docs offline so it's best to avoid them.\n-                *errors = true;\n-                let pretty_path = path.strip_prefix(root).unwrap_or(&path);\n-                println!(\n-                    \"{}:{}: directory link - {}\",\n-                    pretty_file.display(),\n-                    i + 1,\n-                    pretty_path.display()\n-                );\n+        self.cache.get_mut(&pretty_file).unwrap().parse_ids(\n+            &pretty_file,\n+            &contents,\n+            &mut self.errors,\n+        );\n+\n+        // Search for anything that's the regex 'href[ ]*=[ ]*\".*?\"'\n+        with_attrs_in_source(&contents, \" href\", |url, i, base| {\n+            // Ignore external URLs\n+            if url.starts_with(\"http:\")\n+                || url.starts_with(\"https:\")\n+                || url.starts_with(\"javascript:\")\n+                || url.starts_with(\"ftp:\")\n+                || url.starts_with(\"irc:\")\n+                || url.starts_with(\"data:\")\n+            {\n+                self.links_ignored_external += 1;\n                 return;\n             }\n-            if let Some(extension) = path.extension() {\n-                // Ignore none HTML files.\n-                if extension != \"html\" {\n-                    return;\n+            self.links_checked += 1;\n+            let (url, fragment) = match url.split_once('#') {\n+                None => (url, None),\n+                Some((url, fragment)) => (url, Some(fragment)),\n+            };\n+            // NB: the `splitn` always succeeds, even if the delimiter is not present.\n+            let url = url.splitn(2, '?').next().unwrap();\n+\n+            // Once we've plucked out the URL, parse it using our base url and\n+            // then try to extract a file path.\n+            let mut path = file.to_path_buf();\n+            if !base.is_empty() || !url.is_empty() {\n+                path.pop();\n+                for part in Path::new(base).join(url).components() {\n+                    match part {\n+                        Component::Prefix(_) | Component::RootDir => {\n+                            // Avoid absolute paths as they make the docs not\n+                            // relocatable by making assumptions on where the docs\n+                            // are hosted relative to the site root.\n+                            self.errors += 1;\n+                            println!(\n+                                \"{}:{}: absolute path - {}\",\n+                                pretty_file.display(),\n+                                i + 1,\n+                                Path::new(base).join(url).display()\n+                            );\n+                            return;\n+                        }\n+                        Component::CurDir => {}\n+                        Component::ParentDir => {\n+                            path.pop();\n+                        }\n+                        Component::Normal(s) => {\n+                            path.push(s);\n+                        }\n+                    }\n                 }\n             }\n-            let res = load_file(cache, root, &path, FromRedirect(false));\n-            let (pretty_path, contents) = match res {\n-                Ok(res) => res,\n-                Err(LoadError::IOError(err)) => {\n-                    panic!(\"error loading {}: {}\", path.display(), err);\n-                }\n-                Err(LoadError::BrokenRedirect(target, _)) => {\n-                    *errors = true;\n+\n+            // Alright, if we've found a file name then this file had better\n+            // exist! If it doesn't then we register and print an error.\n+            if path.exists() {\n+                if path.is_dir() {\n+                    // Links to directories show as directory listings when viewing\n+                    // the docs offline so it's best to avoid them.\n+                    self.errors += 1;\n+                    let pretty_path = path.strip_prefix(&self.root).unwrap_or(&path);\n                     println!(\n-                        \"{}:{}: broken redirect to {}\",\n+                        \"{}:{}: directory link - {}\",\n                         pretty_file.display(),\n                         i + 1,\n-                        target.display()\n+                        pretty_path.display()\n                     );\n                     return;\n                 }\n-                Err(LoadError::IsRedirect) => unreachable!(),\n-            };\n-\n-            if let Some(ref fragment) = fragment {\n-                // Fragments like `#1-6` are most likely line numbers to be\n-                // interpreted by javascript, so we're ignoring these\n-                if fragment.splitn(2, '-').all(|f| f.chars().all(|c| c.is_numeric())) {\n-                    return;\n+                if let Some(extension) = path.extension() {\n+                    // Ignore none HTML files.\n+                    if extension != \"html\" {\n+                        return;\n+                    }\n                 }\n+                let res = self.load_file(&path, FromRedirect(false));\n+                let (pretty_path, contents) = match res {\n+                    Ok(res) => res,\n+                    Err(LoadError::BrokenRedirect(target, _)) => {\n+                        self.errors += 1;\n+                        println!(\n+                            \"{}:{}: broken redirect to {}\",\n+                            pretty_file.display(),\n+                            i + 1,\n+                            target.display()\n+                        );\n+                        return;\n+                    }\n+                    Err(LoadError::IsRedirect) => unreachable!(),\n+                };\n \n-                // These appear to be broken in mdbook right now?\n-                if fragment.starts_with('-') {\n-                    return;\n-                }\n+                if let Some(ref fragment) = fragment {\n+                    // Fragments like `#1-6` are most likely line numbers to be\n+                    // interpreted by javascript, so we're ignoring these\n+                    if fragment.splitn(2, '-').all(|f| f.chars().all(|c| c.is_numeric())) {\n+                        return;\n+                    }\n \n-                let entry = &mut cache.get_mut(&pretty_path).unwrap();\n-                entry.parse_ids(&pretty_path, &contents, errors);\n+                    // These appear to be broken in mdbook right now?\n+                    if fragment.starts_with('-') {\n+                        return;\n+                    }\n \n-                if !entry.ids.contains(*fragment) && !is_exception(file, &format!(\"#{}\", fragment))\n-                {\n-                    *errors = true;\n-                    print!(\"{}:{}: broken link fragment \", pretty_file.display(), i + 1);\n-                    println!(\"`#{}` pointing to `{}`\", fragment, pretty_path.display());\n-                };\n+                    let entry = self.cache.get_mut(&pretty_path).unwrap();\n+                    entry.parse_ids(&pretty_path, &contents, &mut self.errors);\n+\n+                    if entry.ids.contains(*fragment) {\n+                        return;\n+                    }\n+\n+                    if is_exception(file, &format!(\"#{}\", fragment)) {\n+                        self.links_ignored_exception += 1;\n+                    } else {\n+                        self.errors += 1;\n+                        print!(\"{}:{}: broken link fragment \", pretty_file.display(), i + 1);\n+                        println!(\"`#{}` pointing to `{}`\", fragment, pretty_path.display());\n+                    };\n+                }\n+            } else {\n+                let pretty_path = path.strip_prefix(&self.root).unwrap_or(&path);\n+                if is_exception(file, pretty_path.to_str().unwrap()) {\n+                } else {\n+                    self.errors += 1;\n+                    print!(\"{}:{}: broken link - \", pretty_file.display(), i + 1);\n+                    println!(\"{}\", pretty_path.display());\n+                }\n             }\n-        } else {\n-            let pretty_path = path.strip_prefix(root).unwrap_or(&path);\n-            if !is_exception(file, pretty_path.to_str().unwrap()) {\n-                *errors = true;\n-                print!(\"{}:{}: broken link - \", pretty_file.display(), i + 1);\n-                println!(\"{}\", pretty_path.display());\n+        });\n+\n+        // Search for intra-doc links that rustdoc didn't warn about\n+        // FIXME(#77199, 77200) Rustdoc should just warn about these directly.\n+        // NOTE: only looks at one line at a time; in practice this should find most links\n+        for (i, line) in contents.lines().enumerate() {\n+            for broken_link in BROKEN_INTRA_DOC_LINK.captures_iter(line) {\n+                if is_intra_doc_exception(file, &broken_link[1]) {\n+                    self.intra_doc_exceptions += 1;\n+                } else {\n+                    self.errors += 1;\n+                    print!(\"{}:{}: broken intra-doc link - \", pretty_file.display(), i + 1);\n+                    println!(\"{}\", &broken_link[0]);\n+                }\n             }\n         }\n-    });\n-\n-    // Search for intra-doc links that rustdoc didn't warn about\n-    // FIXME(#77199, 77200) Rustdoc should just warn about these directly.\n-    // NOTE: only looks at one line at a time; in practice this should find most links\n-    for (i, line) in contents.lines().enumerate() {\n-        for broken_link in BROKEN_INTRA_DOC_LINK.captures_iter(line) {\n-            if !is_intra_doc_exception(file, &broken_link[1]) {\n-                *errors = true;\n-                print!(\"{}:{}: broken intra-doc link - \", pretty_file.display(), i + 1);\n-                println!(\"{}\", &broken_link[0]);\n+        Some(pretty_file)\n+    }\n+\n+    fn load_file(\n+        &mut self,\n+        file: &Path,\n+        redirect: Redirect,\n+    ) -> Result<(PathBuf, Rc<String>), LoadError> {\n+        let pretty_file = PathBuf::from(file.strip_prefix(&self.root).unwrap_or(&file));\n+\n+        let (maybe_redirect, contents) = match self.cache.entry(pretty_file.clone()) {\n+            Entry::Occupied(entry) => (None, entry.get().source.clone()),\n+            Entry::Vacant(entry) => {\n+                let contents = match fs::read_to_string(file) {\n+                    Ok(s) => Rc::new(s),\n+                    Err(err) => {\n+                        return Err(if let FromRedirect(true) = redirect {\n+                            LoadError::BrokenRedirect(file.to_path_buf(), err)\n+                        } else {\n+                            panic!(\"error loading {}: {}\", file.display(), err);\n+                        });\n+                    }\n+                };\n+\n+                let maybe = maybe_redirect(&contents);\n+                if maybe.is_some() {\n+                    self.html_redirects += 1;\n+                    if let SkipRedirect = redirect {\n+                        return Err(LoadError::IsRedirect);\n+                    }\n+                } else {\n+                    entry.insert(FileEntry { source: contents.clone(), ids: HashSet::new() });\n+                }\n+                (maybe, contents)\n             }\n+        };\n+        match maybe_redirect.map(|url| file.parent().unwrap().join(url)) {\n+            Some(redirect_file) => self.load_file(&redirect_file, FromRedirect(true)),\n+            None => Ok((pretty_file, contents)),\n         }\n     }\n-    Some(pretty_file)\n+\n+    fn report(&self) {\n+        println!(\"checked links in: {:.1}s\", self.start.elapsed().as_secs_f64());\n+        println!(\"number of HTML files scanned: {}\", self.html_files);\n+        println!(\"number of HTML redirects found: {}\", self.html_redirects);\n+        println!(\"number of links checked: {}\", self.links_checked);\n+        println!(\"number of links ignored due to external: {}\", self.links_ignored_external);\n+        println!(\"number of links ignored due to exceptions: {}\", self.links_ignored_exception);\n+        println!(\"number of intra doc links ignored: {}\", self.intra_doc_exceptions);\n+        println!(\"errors found: {}\", self.errors);\n+    }\n }\n \n-fn load_file(\n-    cache: &mut Cache,\n-    root: &Path,\n-    file: &Path,\n-    redirect: Redirect,\n-) -> Result<(PathBuf, Rc<String>), LoadError> {\n-    let pretty_file = PathBuf::from(file.strip_prefix(root).unwrap_or(&file));\n-\n-    let (maybe_redirect, contents) = match cache.entry(pretty_file.clone()) {\n-        Entry::Occupied(entry) => (None, entry.get().source.clone()),\n-        Entry::Vacant(entry) => {\n-            let contents = match fs::read_to_string(file) {\n-                Ok(s) => Rc::new(s),\n-                Err(err) => {\n-                    return Err(if let FromRedirect(true) = redirect {\n-                        LoadError::BrokenRedirect(file.to_path_buf(), err)\n-                    } else {\n-                        LoadError::IOError(err)\n-                    });\n-                }\n-            };\n+fn is_intra_doc_exception(file: &Path, link: &str) -> bool {\n+    if let Some(entry) = INTRA_DOC_LINK_EXCEPTIONS.iter().find(|&(f, _)| file.ends_with(f)) {\n+        entry.1.is_empty() || entry.1.contains(&link)\n+    } else {\n+        false\n+    }\n+}\n \n-            let maybe = maybe_redirect(&contents);\n-            if maybe.is_some() {\n-                if let SkipRedirect = redirect {\n-                    return Err(LoadError::IsRedirect);\n-                }\n-            } else {\n-                entry.insert(FileEntry { source: contents.clone(), ids: HashSet::new() });\n+fn is_exception(file: &Path, link: &str) -> bool {\n+    if let Some(entry) = LINKCHECK_EXCEPTIONS.iter().find(|&(f, _)| file.ends_with(f)) {\n+        entry.1.contains(&link)\n+    } else {\n+        // FIXME(#63351): Concat trait in alloc/slice reexported in primitive page\n+        //\n+        // NOTE: This cannot be added to `LINKCHECK_EXCEPTIONS` because the resolved path\n+        // calculated in `check` function is outside `build/<triple>/doc` dir.\n+        // So the `strip_prefix` method just returns the old absolute broken path.\n+        if file.ends_with(\"std/primitive.slice.html\") {\n+            if link.ends_with(\"primitive.slice.html\") {\n+                return true;\n             }\n-            (maybe, contents)\n         }\n-    };\n-    match maybe_redirect.map(|url| file.parent().unwrap().join(url)) {\n-        Some(redirect_file) => load_file(cache, root, &redirect_file, FromRedirect(true)),\n-        None => Ok((pretty_file, contents)),\n+        false\n     }\n }\n "}]}