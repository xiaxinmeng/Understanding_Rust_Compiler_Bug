{"sha": "e7698f4f07dcec9cf42b3de133f9ca171d0677f0", "node_id": "C_kwDOAAsO6NoAKGU3Njk4ZjRmMDdkY2VjOWNmNDJiM2RlMTMzZjljYTE3MWQwNjc3ZjA", "commit": {"author": {"name": "Andy Wang", "email": "cbeuw.andy@gmail.com", "date": "2021-12-27T19:07:23Z"}, "committer": {"name": "Andy Wang", "email": "cbeuw.andy@gmail.com", "date": "2022-06-06T18:15:20Z"}, "message": "Implement weak memory emulation", "tree": {"sha": "2ea50a843bdb4e0162995bec46fe111c6986bda7", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/2ea50a843bdb4e0162995bec46fe111c6986bda7"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/e7698f4f07dcec9cf42b3de133f9ca171d0677f0", "comment_count": 0, "verification": {"verified": true, "reason": "valid", "signature": "-----BEGIN PGP SIGNATURE-----\n\niQGzBAABCgAdFiEE7dcbcBMl24/h63ldGBtJ+fOPM3QFAmKeRDgACgkQGBtJ+fOP\nM3TU2QwAo2ulc9xnOTPIBr3p0YR3W+qzsiGQxqwCzPn1ZGeQ1WFFzl2fQ6uXYh2Q\n9ye6G1uvmpWiXe11eQPl/6s0ZoPiLkIjk5oSFZ1EzFyDiJBrvQ7PNoB+NiK5lmwX\nVf7/Pxr6JAOUQHeZL3E021Riw583iy3JENw7Ah96c9YJMRTcDF3lXXKb57qsY7zE\ns4D+54vdC6YsJi2+1NxRyIOjxzpgKfrIMNMIgxjx9aph4aj+YT4hxZDu7lfqj+97\nme8nwbq/vN4v5MRDrOOs2hAtp9lifBXSg7+mTxQzVFurjtrFGjMsQn2Ku1uscoqW\nADfkCE7/3iQWI2YgWjjI97Sw0VuVdK23yaGJWpwvxnaYeGMTX4rON3AndWndvw8m\nTS/5eU8ZeVVYE6L1F45TfOjbLK8e4d77ATC3jPKFqi+5QoZYQ8szjptsknLcbiT6\nFq8K6bZS3DqfwdebSZ2+zDSI/v9ItjnBdSOKj/TSMKCsZyhIpzaILFiOEIMo+NUZ\nTcEvP13z\n=JJlj\n-----END PGP SIGNATURE-----", "payload": "tree 2ea50a843bdb4e0162995bec46fe111c6986bda7\nparent 16315b1540959e4834ba461471e868868c68c4bc\nauthor Andy Wang <cbeuw.andy@gmail.com> 1640632043 +0000\ncommitter Andy Wang <cbeuw.andy@gmail.com> 1654539320 +0100\n\nImplement weak memory emulation\n"}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/e7698f4f07dcec9cf42b3de133f9ca171d0677f0", "html_url": "https://github.com/rust-lang/rust/commit/e7698f4f07dcec9cf42b3de133f9ca171d0677f0", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/e7698f4f07dcec9cf42b3de133f9ca171d0677f0/comments", "author": {"login": "cbeuw", "id": 7034308, "node_id": "MDQ6VXNlcjcwMzQzMDg=", "avatar_url": "https://avatars.githubusercontent.com/u/7034308?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cbeuw", "html_url": "https://github.com/cbeuw", "followers_url": "https://api.github.com/users/cbeuw/followers", "following_url": "https://api.github.com/users/cbeuw/following{/other_user}", "gists_url": "https://api.github.com/users/cbeuw/gists{/gist_id}", "starred_url": "https://api.github.com/users/cbeuw/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cbeuw/subscriptions", "organizations_url": "https://api.github.com/users/cbeuw/orgs", "repos_url": "https://api.github.com/users/cbeuw/repos", "events_url": "https://api.github.com/users/cbeuw/events{/privacy}", "received_events_url": "https://api.github.com/users/cbeuw/received_events", "type": "User", "site_admin": false}, "committer": {"login": "cbeuw", "id": 7034308, "node_id": "MDQ6VXNlcjcwMzQzMDg=", "avatar_url": "https://avatars.githubusercontent.com/u/7034308?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cbeuw", "html_url": "https://github.com/cbeuw", "followers_url": "https://api.github.com/users/cbeuw/followers", "following_url": "https://api.github.com/users/cbeuw/following{/other_user}", "gists_url": "https://api.github.com/users/cbeuw/gists{/gist_id}", "starred_url": "https://api.github.com/users/cbeuw/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cbeuw/subscriptions", "organizations_url": "https://api.github.com/users/cbeuw/orgs", "repos_url": "https://api.github.com/users/cbeuw/repos", "events_url": "https://api.github.com/users/cbeuw/events{/privacy}", "received_events_url": "https://api.github.com/users/cbeuw/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "16315b1540959e4834ba461471e868868c68c4bc", "url": "https://api.github.com/repos/rust-lang/rust/commits/16315b1540959e4834ba461471e868868c68c4bc", "html_url": "https://github.com/rust-lang/rust/commit/16315b1540959e4834ba461471e868868c68c4bc"}], "stats": {"total": 503, "additions": 476, "deletions": 27}, "files": [{"sha": "82ee32ddee71f275c8f23a5f633ec6f5fd8eccc4", "filename": "src/data_race.rs", "status": "modified", "additions": 144, "deletions": 26, "changes": 170, "blob_url": "https://github.com/rust-lang/rust/blob/e7698f4f07dcec9cf42b3de133f9ca171d0677f0/src%2Fdata_race.rs", "raw_url": "https://github.com/rust-lang/rust/raw/e7698f4f07dcec9cf42b3de133f9ca171d0677f0/src%2Fdata_race.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Fdata_race.rs?ref=e7698f4f07dcec9cf42b3de133f9ca171d0677f0", "patch": "@@ -12,7 +12,7 @@\n //! The implementation also models races with memory allocation and deallocation via treating allocation and\n //! deallocation as a type of write internally for detecting data-races.\n //!\n-//! This does not explore weak memory orders and so can still miss data-races\n+//! Weak memory orders are explored but not all weak behaviours are exhibited, so it can still miss data-races\n //! but should not report false-positives\n //!\n //! Data-race definition from(<https://en.cppreference.com/w/cpp/language/memory_model#Threads_and_data_races>):\n@@ -29,22 +29,6 @@\n //! This means that the thread-index can be safely re-used, starting on the next timestamp for the newly created\n //! thread.\n //!\n-//! The sequentially consistent ordering corresponds to the ordering that the threads\n-//! are currently scheduled, this means that the data-race detector has no additional\n-//! logic for sequentially consistent accesses at the moment since they are indistinguishable\n-//! from acquire/release operations. If weak memory orderings are explored then this\n-//! may need to change or be updated accordingly.\n-//!\n-//! Per the C++ spec for the memory model a sequentially consistent operation:\n-//!   \"A load operation with this memory order performs an acquire operation,\n-//!    a store performs a release operation, and read-modify-write performs\n-//!    both an acquire operation and a release operation, plus a single total\n-//!    order exists in which all threads observe all modifications in the same\n-//!    order (see Sequentially-consistent ordering below) \"\n-//! So in the absence of weak memory effects a seq-cst load & a seq-cst store is identical\n-//! to an acquire load and a release store given the global sequentially consistent order\n-//! of the schedule.\n-//!\n //! The timestamps used in the data-race detector assign each sequence of non-atomic operations\n //! followed by a single atomic or concurrent operation a single timestamp.\n //! Write, Read, Write, ThreadJoin will be represented by a single timestamp value on a thread.\n@@ -67,6 +51,7 @@ use std::{\n     mem,\n };\n \n+use rustc_const_eval::interpret::alloc_range;\n use rustc_data_structures::fx::{FxHashMap, FxHashSet};\n use rustc_index::vec::{Idx, IndexVec};\n use rustc_middle::{mir, ty::layout::TyAndLayout};\n@@ -115,10 +100,10 @@ pub enum AtomicFenceOp {\n /// of a thread, contains the happens-before clock and\n /// additional metadata to model atomic fence operations.\n #[derive(Clone, Default, Debug)]\n-struct ThreadClockSet {\n+pub struct ThreadClockSet {\n     /// The increasing clock representing timestamps\n     /// that happen-before this thread.\n-    clock: VClock,\n+    pub clock: VClock,\n \n     /// The set of timestamps that will happen-before this\n     /// thread once it performs an acquire fence.\n@@ -127,6 +112,12 @@ struct ThreadClockSet {\n     /// The last timestamp of happens-before relations that\n     /// have been released by this thread by a fence.\n     fence_release: VClock,\n+\n+    pub fence_seqcst: VClock,\n+\n+    pub write_seqcst: VClock,\n+\n+    pub read_seqcst: VClock,\n }\n \n impl ThreadClockSet {\n@@ -169,7 +160,7 @@ pub struct DataRace;\n /// common case where no atomic operations\n /// exists on the memory cell.\n #[derive(Clone, PartialEq, Eq, Default, Debug)]\n-struct AtomicMemoryCellClocks {\n+pub struct AtomicMemoryCellClocks {\n     /// The clock-vector of the timestamp of the last atomic\n     /// read operation performed by each thread.\n     /// This detects potential data-races between atomic read\n@@ -514,7 +505,32 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: MiriEvalContextExt<'mir, 'tcx> {\n         atomic: AtomicReadOp,\n     ) -> InterpResult<'tcx, ScalarMaybeUninit<Tag>> {\n         let this = self.eval_context_ref();\n+        // This will read from the last store in the modification order of this location. In case\n+        // weak memory emulation is enabled, this may not be the store we will pick to actually read from and return.\n+        // This is fine with StackedBorrow and race checks because they don't concern metadata on\n+        // the *value* (including the associated provenance if this is an AtomicPtr) at this location.\n+        // Only metadata on the location itself is used.\n         let scalar = this.allow_data_races_ref(move |this| this.read_scalar(&place.into()))?;\n+\n+        if let Some(global) = &this.machine.data_race {\n+            let (alloc_id, base_offset, ..) = this.ptr_get_alloc_id(place.ptr)?;\n+            if let Some(alloc_buffers) = this.get_alloc_extra(alloc_id)?.weak_memory.as_ref() {\n+                if atomic == AtomicReadOp::SeqCst {\n+                    global.sc_read();\n+                }\n+                let mut rng = this.machine.rng.borrow_mut();\n+                let loaded = alloc_buffers.buffered_read(\n+                    alloc_range(base_offset, place.layout.size),\n+                    global,\n+                    atomic == AtomicReadOp::SeqCst,\n+                    &mut *rng,\n+                    || this.validate_atomic_load(place, atomic),\n+                )?;\n+\n+                return Ok(loaded.unwrap_or(scalar));\n+            }\n+        }\n+\n         this.validate_atomic_load(place, atomic)?;\n         Ok(scalar)\n     }\n@@ -528,7 +544,27 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: MiriEvalContextExt<'mir, 'tcx> {\n     ) -> InterpResult<'tcx> {\n         let this = self.eval_context_mut();\n         this.allow_data_races_mut(move |this| this.write_scalar(val, &(*dest).into()))?;\n-        this.validate_atomic_store(dest, atomic)\n+\n+        this.validate_atomic_store(dest, atomic)?;\n+        let (alloc_id, base_offset, ..) = this.ptr_get_alloc_id(dest.ptr)?;\n+        if let (\n+            crate::AllocExtra { weak_memory: Some(alloc_buffers), .. },\n+            crate::Evaluator { data_race: Some(global), .. },\n+        ) = this.get_alloc_extra_mut(alloc_id)?\n+        {\n+            if atomic == AtomicWriteOp::SeqCst {\n+                global.sc_write();\n+            }\n+            let size = dest.layout.size;\n+            alloc_buffers.buffered_write(\n+                val,\n+                alloc_range(base_offset, size),\n+                global,\n+                atomic == AtomicWriteOp::SeqCst,\n+            )?;\n+        }\n+\n+        Ok(())\n     }\n \n     /// Perform an atomic operation on a memory location.\n@@ -550,6 +586,8 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: MiriEvalContextExt<'mir, 'tcx> {\n         this.allow_data_races_mut(|this| this.write_immediate(*val, &(*place).into()))?;\n \n         this.validate_atomic_rmw(place, atomic)?;\n+\n+        this.buffered_atomic_rmw(val.to_scalar_or_uninit(), place, atomic)?;\n         Ok(old)\n     }\n \n@@ -565,7 +603,10 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: MiriEvalContextExt<'mir, 'tcx> {\n \n         let old = this.allow_data_races_mut(|this| this.read_scalar(&place.into()))?;\n         this.allow_data_races_mut(|this| this.write_scalar(new, &(*place).into()))?;\n+\n         this.validate_atomic_rmw(place, atomic)?;\n+\n+        this.buffered_atomic_rmw(new, place, atomic)?;\n         Ok(old)\n     }\n \n@@ -584,15 +625,25 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: MiriEvalContextExt<'mir, 'tcx> {\n         let lt = this.binary_op(mir::BinOp::Lt, &old, &rhs)?.to_scalar()?.to_bool()?;\n \n         let new_val = if min {\n-            if lt { &old } else { &rhs }\n+            if lt {\n+                &old\n+            } else {\n+                &rhs\n+            }\n         } else {\n-            if lt { &rhs } else { &old }\n+            if lt {\n+                &rhs\n+            } else {\n+                &old\n+            }\n         };\n \n         this.allow_data_races_mut(|this| this.write_immediate(**new_val, &(*place).into()))?;\n \n         this.validate_atomic_rmw(place, atomic)?;\n \n+        this.buffered_atomic_rmw(new_val.to_scalar_or_uninit(), place, atomic)?;\n+\n         // Return the old value.\n         Ok(old)\n     }\n@@ -642,14 +693,56 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: MiriEvalContextExt<'mir, 'tcx> {\n         if cmpxchg_success {\n             this.allow_data_races_mut(|this| this.write_scalar(new, &(*place).into()))?;\n             this.validate_atomic_rmw(place, success)?;\n+            this.buffered_atomic_rmw(new, place, success)?;\n         } else {\n             this.validate_atomic_load(place, fail)?;\n+            // A failed compare exchange is equivalent to a load, reading from the latest store\n+            // in the modification order.\n+            // Since `old` is only a value and not the store element, we need to separately\n+            // find it in our store buffer and perform load_impl on it.\n+            if let Some(global) = &this.machine.data_race {\n+                if fail == AtomicReadOp::SeqCst {\n+                    global.sc_read();\n+                }\n+                let size = place.layout.size;\n+                let (alloc_id, base_offset, ..) = this.ptr_get_alloc_id(place.ptr)?;\n+                if let Some(alloc_buffers) = this.get_alloc_extra(alloc_id)?.weak_memory.as_ref() {\n+                    if global.multi_threaded.get() {\n+                        alloc_buffers.read_from_last_store(alloc_range(base_offset, size), global);\n+                    }\n+                }\n+            }\n         }\n \n         // Return the old value.\n         Ok(res)\n     }\n \n+    fn buffered_atomic_rmw(\n+        &mut self,\n+        new_val: ScalarMaybeUninit<Tag>,\n+        place: &MPlaceTy<'tcx, Tag>,\n+        atomic: AtomicRwOp,\n+    ) -> InterpResult<'tcx> {\n+        let this = self.eval_context_mut();\n+        let (alloc_id, base_offset, ..) = this.ptr_get_alloc_id(place.ptr)?;\n+        if let (\n+            crate::AllocExtra { weak_memory: Some(alloc_buffers), .. },\n+            crate::Evaluator { data_race: Some(global), .. },\n+        ) = this.get_alloc_extra_mut(alloc_id)?\n+        {\n+            if atomic == AtomicRwOp::SeqCst {\n+                global.sc_read();\n+                global.sc_write();\n+            }\n+            let size = place.layout.size;\n+            let range = alloc_range(base_offset, size);\n+            alloc_buffers.read_from_last_store(range, global);\n+            alloc_buffers.buffered_write(new_val, range, global, atomic == AtomicRwOp::SeqCst)?;\n+        }\n+        Ok(())\n+    }\n+\n     /// Update the data-race detector for an atomic read occurring at the\n     /// associated memory-place and on the current thread.\n     fn validate_atomic_load(\n@@ -723,7 +816,7 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: MiriEvalContextExt<'mir, 'tcx> {\n     fn validate_atomic_fence(&mut self, atomic: AtomicFenceOp) -> InterpResult<'tcx> {\n         let this = self.eval_context_mut();\n         if let Some(data_race) = &mut this.machine.data_race {\n-            data_race.maybe_perform_sync_operation(move |index, mut clocks| {\n+            data_race.maybe_perform_sync_operation(|index, mut clocks| {\n                 log::trace!(\"Atomic fence on {:?} with ordering {:?}\", index, atomic);\n \n                 // Apply data-race detection for the current fences\n@@ -737,6 +830,11 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: MiriEvalContextExt<'mir, 'tcx> {\n                     // Either Release | AcqRel | SeqCst\n                     clocks.apply_release_fence();\n                 }\n+                if atomic == AtomicFenceOp::SeqCst {\n+                    data_race.last_sc_fence.borrow_mut().set_at_index(&clocks.clock, index);\n+                    clocks.fence_seqcst.join(&data_race.last_sc_fence.borrow());\n+                    clocks.write_seqcst.join(&data_race.last_sc_write.borrow());\n+                }\n \n                 // Increment timestamp in case of release semantics.\n                 Ok(atomic != AtomicFenceOp::Acquire)\n@@ -1116,6 +1214,12 @@ pub struct GlobalState {\n     /// The associated vector index will be moved into re-use candidates\n     /// after the join operation occurs.\n     terminated_threads: RefCell<FxHashMap<ThreadId, VectorIdx>>,\n+\n+    /// The timestamp of last SC fence performed by each thread\n+    last_sc_fence: RefCell<VClock>,\n+\n+    /// The timestamp of last SC write performed by each thread\n+    last_sc_write: RefCell<VClock>,\n }\n \n impl GlobalState {\n@@ -1131,6 +1235,8 @@ impl GlobalState {\n             active_thread_count: Cell::new(1),\n             reuse_candidates: RefCell::new(FxHashSet::default()),\n             terminated_threads: RefCell::new(FxHashMap::default()),\n+            last_sc_fence: RefCell::new(VClock::default()),\n+            last_sc_write: RefCell::new(VClock::default()),\n         };\n \n         // Setup the main-thread since it is not explicitly created:\n@@ -1445,7 +1551,7 @@ impl GlobalState {\n     /// Load the current vector clock in use and the current set of thread clocks\n     /// in use for the vector.\n     #[inline]\n-    fn current_thread_state(&self) -> (VectorIdx, Ref<'_, ThreadClockSet>) {\n+    pub fn current_thread_state(&self) -> (VectorIdx, Ref<'_, ThreadClockSet>) {\n         let index = self.current_index();\n         let ref_vector = self.vector_clocks.borrow();\n         let clocks = Ref::map(ref_vector, |vec| &vec[index]);\n@@ -1455,7 +1561,7 @@ impl GlobalState {\n     /// Load the current vector clock in use and the current set of thread clocks\n     /// in use for the vector mutably for modification.\n     #[inline]\n-    fn current_thread_state_mut(&self) -> (VectorIdx, RefMut<'_, ThreadClockSet>) {\n+    pub fn current_thread_state_mut(&self) -> (VectorIdx, RefMut<'_, ThreadClockSet>) {\n         let index = self.current_index();\n         let ref_vector = self.vector_clocks.borrow_mut();\n         let clocks = RefMut::map(ref_vector, |vec| &mut vec[index]);\n@@ -1468,4 +1574,16 @@ impl GlobalState {\n     fn current_index(&self) -> VectorIdx {\n         self.current_index.get()\n     }\n+\n+    // SC ATOMIC STORE rule in the paper.\n+    fn sc_write(&self) {\n+        let (index, clocks) = self.current_thread_state();\n+        self.last_sc_write.borrow_mut().set_at_index(&clocks.clock, index);\n+    }\n+\n+    // SC ATOMIC READ rule in the paper.\n+    fn sc_read(&self) {\n+        let (.., mut clocks) = self.current_thread_state_mut();\n+        clocks.read_seqcst.join(&self.last_sc_fence.borrow());\n+    }\n }"}, {"sha": "06ab2fabab04f51ecdc774b638829b6b8e764eb6", "filename": "src/lib.rs", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "blob_url": "https://github.com/rust-lang/rust/blob/e7698f4f07dcec9cf42b3de133f9ca171d0677f0/src%2Flib.rs", "raw_url": "https://github.com/rust-lang/rust/raw/e7698f4f07dcec9cf42b3de133f9ca171d0677f0/src%2Flib.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flib.rs?ref=e7698f4f07dcec9cf42b3de133f9ca171d0677f0", "patch": "@@ -45,6 +45,7 @@ mod stacked_borrows;\n mod sync;\n mod thread;\n mod vector_clock;\n+mod weak_memory;\n \n // Establish a \"crate-wide prelude\": we often import `crate::*`.\n "}, {"sha": "aa2a930ccd251c2fd5428eaa83a4f050bf47a89f", "filename": "src/machine.rs", "status": "modified", "additions": 11, "deletions": 1, "changes": 12, "blob_url": "https://github.com/rust-lang/rust/blob/e7698f4f07dcec9cf42b3de133f9ca171d0677f0/src%2Fmachine.rs", "raw_url": "https://github.com/rust-lang/rust/raw/e7698f4f07dcec9cf42b3de133f9ca171d0677f0/src%2Fmachine.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Fmachine.rs?ref=e7698f4f07dcec9cf42b3de133f9ca171d0677f0", "patch": "@@ -190,6 +190,9 @@ pub struct AllocExtra {\n     /// Data race detection via the use of a vector-clock,\n     ///  this is only added if it is enabled.\n     pub data_race: Option<data_race::AllocExtra>,\n+    /// Weak memory emulation via the use of store buffers,\n+    ///  this is only added if it is enabled.\n+    pub weak_memory: Option<weak_memory::AllocExtra>,\n }\n \n /// Precomputed layouts of primitive types\n@@ -630,9 +633,16 @@ impl<'mir, 'tcx> Machine<'mir, 'tcx> for Evaluator<'mir, 'tcx> {\n         } else {\n             None\n         };\n+        let buffer_alloc = if ecx.machine.weak_memory {\n+            // FIXME: if this is an atomic obejct, we want to supply its initial value\n+            // while allocating the store buffer here.\n+            Some(weak_memory::AllocExtra::new_allocation(alloc.size()))\n+        } else {\n+            None\n+        };\n         let alloc: Allocation<Tag, Self::AllocExtra> = alloc.convert_tag_add_extra(\n             &ecx.tcx,\n-            AllocExtra { stacked_borrows: stacks, data_race: race_alloc },\n+            AllocExtra { stacked_borrows: stacks, data_race: race_alloc, weak_memory: buffer_alloc },\n             |ptr| Evaluator::tag_alloc_base_pointer(ecx, ptr),\n         );\n         Cow::Owned(alloc)"}, {"sha": "c82a31d0a89c1ac5606300d12598d7694120ad7e", "filename": "src/weak_memory.rs", "status": "added", "additions": 297, "deletions": 0, "changes": 297, "blob_url": "https://github.com/rust-lang/rust/blob/e7698f4f07dcec9cf42b3de133f9ca171d0677f0/src%2Fweak_memory.rs", "raw_url": "https://github.com/rust-lang/rust/raw/e7698f4f07dcec9cf42b3de133f9ca171d0677f0/src%2Fweak_memory.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Fweak_memory.rs?ref=e7698f4f07dcec9cf42b3de133f9ca171d0677f0", "patch": "@@ -0,0 +1,297 @@\n+//! Implementation of C++11-consistent weak memory emulation using store buffers\n+//! based on Dynamic Race Detection for C++ (\"the paper\"):\n+//! https://www.doc.ic.ac.uk/~afd/homepages/papers/pdfs/2017/POPL.pdf\n+\n+// Our and the author's own implementation (tsan11) of the paper have some deviations from the provided operational semantics in \u00a75.3:\n+// 1. In the operational semantics, store elements keep a copy of the atomic object's vector clock (AtomicCellClocks::sync_vector in miri),\n+// but this is not used anywhere so it's omitted here.\n+//\n+// 2. In the operational semantics, each store element keeps the timestamp of a thread when it loads from the store.\n+// If the same thread loads from the same store element multiple times, then the timestamps at all loads are saved in a list of load elements.\n+// This is not necessary as later loads by the same thread will always have greater timetstamp values, so we only need to record the timestamp of the first\n+// load by each thread. This optimisation is done in tsan11\n+// (https://github.com/ChrisLidbury/tsan11/blob/ecbd6b81e9b9454e01cba78eb9d88684168132c7/lib/tsan/rtl/tsan_relaxed.h#L35-L37)\n+// and here.\n+// \n+// 3. \u00a74.5 of the paper wants an SC store to mark all existing stores in the buffer that happens before it\n+// as SC. This is not done in the operational semantics but implemented correctly in tsan11\n+// (https://github.com/ChrisLidbury/tsan11/blob/ecbd6b81e9b9454e01cba78eb9d88684168132c7/lib/tsan/rtl/tsan_relaxed.cc#L160-L167)\n+// and here.\n+//\n+// 4. W_SC ; R_SC case requires the SC load to ignore all but last store maked SC (stores not marked SC are not\n+// affected). But this rule is applied to all loads in ReadsFromSet from the paper (last two lines of code), not just SC load.\n+// This is implemented correctly in tsan11\n+// (https://github.com/ChrisLidbury/tsan11/blob/ecbd6b81e9b9454e01cba78eb9d88684168132c7/lib/tsan/rtl/tsan_relaxed.cc#L295)\n+// and here.\n+\n+use std::{\n+    cell::{Ref, RefCell, RefMut},\n+    collections::VecDeque,\n+};\n+\n+use rustc_const_eval::interpret::{AllocRange, InterpResult, ScalarMaybeUninit};\n+use rustc_data_structures::fx::FxHashMap;\n+use rustc_target::abi::Size;\n+\n+use crate::{\n+    data_race::{GlobalState, ThreadClockSet},\n+    RangeMap, Tag, VClock, VTimestamp, VectorIdx,\n+};\n+\n+pub type AllocExtra = StoreBufferAlloc;\n+#[derive(Debug, Clone)]\n+pub struct StoreBufferAlloc {\n+    /// Store buffer of each atomic object in this allocation\n+    // Load may modify a StoreBuffer to record the loading thread's\n+    // timestamp so we need interior mutability here.\n+    store_buffer: RefCell<RangeMap<StoreBuffer>>,\n+}\n+\n+impl StoreBufferAlloc {\n+    pub fn new_allocation(len: Size) -> Self {\n+        Self { store_buffer: RefCell::new(RangeMap::new(len, StoreBuffer::default())) }\n+    }\n+\n+    /// Gets a store buffer associated with an atomic object in this allocation\n+    fn get_store_buffer(&self, range: AllocRange) -> Ref<'_, StoreBuffer> {\n+        Ref::map(self.store_buffer.borrow(), |range_map| {\n+            let (.., store_buffer) = range_map.iter(range.start, range.size).next().unwrap();\n+            store_buffer\n+        })\n+    }\n+\n+    fn get_store_buffer_mut(&self, range: AllocRange) -> RefMut<'_, StoreBuffer> {\n+        RefMut::map(self.store_buffer.borrow_mut(), |range_map| {\n+            let (.., store_buffer) = range_map.iter_mut(range.start, range.size).next().unwrap();\n+            store_buffer\n+        })\n+    }\n+\n+    /// Reads from the last store in modification order\n+    pub fn read_from_last_store<'tcx>(&self, range: AllocRange, global: &GlobalState) {\n+        let store_buffer = self.get_store_buffer(range);\n+        let store_elem = store_buffer.buffer.back();\n+        if let Some(store_elem) = store_elem {\n+            let (index, clocks) = global.current_thread_state();\n+            store_elem.load_impl(index, &clocks);\n+        }\n+    }\n+\n+    pub fn buffered_read<'tcx>(\n+        &self,\n+        range: AllocRange,\n+        global: &GlobalState,\n+        is_seqcst: bool,\n+        rng: &mut (impl rand::Rng + ?Sized),\n+        validate: impl FnOnce() -> InterpResult<'tcx>,\n+    ) -> InterpResult<'tcx, Option<ScalarMaybeUninit<Tag>>> {\n+        // Having a live borrow to store_buffer while calling validate_atomic_load is fine\n+        // because the race detector doesn't touch store_buffer\n+        let store_buffer = self.get_store_buffer(range);\n+\n+        let store_elem = {\n+            // The `clocks` we got here must be dropped before calling validate_atomic_load\n+            // as the race detector will update it\n+            let (.., clocks) = global.current_thread_state();\n+            // Load from a valid entry in the store buffer\n+            store_buffer.fetch_store(is_seqcst, &clocks, &mut *rng)\n+        };\n+\n+        // Unlike in write_scalar_atomic, thread clock updates have to be done\n+        // after we've picked a store element from the store buffer, as presented\n+        // in ATOMIC LOAD rule of the paper. This is because fetch_store\n+        // requires access to ThreadClockSet.clock, which is updated by the race detector\n+        validate()?;\n+\n+        let loaded = store_elem.map(|store_elem| {\n+            let (index, clocks) = global.current_thread_state();\n+            store_elem.load_impl(index, &clocks)\n+        });\n+        Ok(loaded)\n+    }\n+\n+    pub fn buffered_write<'tcx>(\n+        &mut self,\n+        val: ScalarMaybeUninit<Tag>,\n+        range: AllocRange,\n+        global: &GlobalState,\n+        is_seqcst: bool,\n+    ) -> InterpResult<'tcx> {\n+        let (index, clocks) = global.current_thread_state();\n+\n+        let mut store_buffer = self.get_store_buffer_mut(range);\n+        store_buffer.store_impl(val, index, &clocks.clock, is_seqcst);\n+        Ok(())\n+    }\n+}\n+\n+const STORE_BUFFER_LIMIT: usize = 128;\n+#[derive(Debug, Clone, PartialEq, Eq)]\n+pub struct StoreBuffer {\n+    // Stores to this location in modification order\n+    buffer: VecDeque<StoreElement>,\n+}\n+\n+impl Default for StoreBuffer {\n+    fn default() -> Self {\n+        let mut buffer = VecDeque::new();\n+        buffer.reserve(STORE_BUFFER_LIMIT);\n+        Self { buffer }\n+    }\n+}\n+\n+impl<'mir, 'tcx: 'mir> StoreBuffer {\n+    /// Selects a valid store element in the buffer.\n+    /// The buffer does not contain the value used to initialise the atomic object\n+    /// so a fresh atomic object has an empty store buffer until an explicit store.\n+    fn fetch_store<R: rand::Rng + ?Sized>(\n+        &self,\n+        is_seqcst: bool,\n+        clocks: &ThreadClockSet,\n+        rng: &mut R,\n+    ) -> Option<&StoreElement> {\n+        use rand::seq::IteratorRandom;\n+        let mut found_sc = false;\n+        // FIXME: this should be an inclusive take_while (stops after a false predicate, but\n+        // includes the element that gave the false), but such function doesn't yet\n+        // exist in the standard libary https://github.com/rust-lang/rust/issues/62208\n+        let mut keep_searching = true;\n+        let candidates = self\n+            .buffer\n+            .iter()\n+            .rev()\n+            .take_while(move |&store_elem| {\n+                if !keep_searching {\n+                    return false;\n+                }\n+                // CoWR: if a store happens-before the current load,\n+                // then we can't read-from anything earlier in modification order.\n+                if store_elem.timestamp <= clocks.clock[store_elem.store_index] {\n+                    log::info!(\"Stopped due to coherent write-read\");\n+                    keep_searching = false;\n+                    return true;\n+                }\n+\n+                // CoRR: if there was a load from this store which happened-before the current load,\n+                // then we cannot read-from anything earlier in modification order.\n+                if store_elem.loads.borrow().iter().any(|(&load_index, &load_timestamp)| {\n+                    load_timestamp <= clocks.clock[load_index]\n+                }) {\n+                    log::info!(\"Stopped due to coherent read-read\");\n+                    keep_searching = false;\n+                    return true;\n+                }\n+\n+                // The current load, which may be sequenced-after an SC fence, can only read-from\n+                // the last store sequenced-before an SC fence in another thread (or any stores\n+                // later than that SC fence)\n+                if store_elem.timestamp <= clocks.fence_seqcst[store_elem.store_index] {\n+                    log::info!(\"Stopped due to coherent load sequenced after sc fence\");\n+                    keep_searching = false;\n+                    return true;\n+                }\n+\n+                // The current non-SC load can only read-from the latest SC store (or any stores later than that\n+                // SC store)\n+                if store_elem.timestamp <= clocks.write_seqcst[store_elem.store_index]\n+                    && store_elem.is_seqcst\n+                {\n+                    log::info!(\"Stopped due to needing to load from the last SC store\");\n+                    keep_searching = false;\n+                    return true;\n+                }\n+\n+                // The current SC load can only read-from the last store sequenced-before\n+                // the last SC fence (or any stores later than the SC fence)\n+                if is_seqcst && store_elem.timestamp <= clocks.read_seqcst[store_elem.store_index] {\n+                    log::info!(\"Stopped due to sc load needing to load from the last SC store before an SC fence\");\n+                    keep_searching = false;\n+                    return true;\n+                }\n+\n+                true\n+            })\n+            .filter(|&store_elem| {\n+                if is_seqcst {\n+                    // An SC load needs to ignore all but last store maked SC (stores not marked SC are not\n+                    // affected)\n+                    let include = !(store_elem.is_seqcst && found_sc);\n+                    found_sc |= store_elem.is_seqcst;\n+                    include\n+                } else {\n+                    true\n+                }\n+            });\n+\n+        candidates.choose(rng)\n+    }\n+\n+    /// ATOMIC STORE IMPL in the paper (except we don't need the location's vector clock)\n+    fn store_impl(\n+        &mut self,\n+        val: ScalarMaybeUninit<Tag>,\n+        index: VectorIdx,\n+        thread_clock: &VClock,\n+        is_seqcst: bool,\n+    ) {\n+        let store_elem = StoreElement {\n+            store_index: index,\n+            timestamp: thread_clock[index],\n+            // In the language provided in the paper, an atomic store takes the value from a\n+            // non-atomic memory location.\n+            // But we already have the immediate value here so we don't need to do the memory\n+            // access\n+            val,\n+            is_seqcst,\n+            loads: RefCell::new(FxHashMap::default()),\n+        };\n+        self.buffer.push_back(store_elem);\n+        if self.buffer.len() > STORE_BUFFER_LIMIT {\n+            self.buffer.pop_front();\n+        }\n+        if is_seqcst {\n+            // Every store that happens before this needs to be marked as SC\n+            // so that in a later SC load, only the last SC store (i.e. this one) or stores that\n+            // aren't ordered by hb with the last SC is picked.\n+            self.buffer.iter_mut().rev().for_each(|elem| {\n+                if elem.timestamp <= thread_clock[elem.store_index] {\n+                    elem.is_seqcst = true;\n+                }\n+            })\n+        }\n+    }\n+}\n+\n+#[derive(Debug, Clone, PartialEq, Eq)]\n+pub struct StoreElement {\n+    /// The identifier of the vector index, corresponding to a thread\n+    /// that performed the store.\n+    store_index: VectorIdx,\n+\n+    /// Whether this store is SC.\n+    is_seqcst: bool,\n+\n+    /// The timestamp of the storing thread when it performed the store\n+    timestamp: VTimestamp,\n+    /// The value of this store\n+    val: ScalarMaybeUninit<Tag>,\n+\n+    /// Timestamp of first loads from this store element by each thread\n+    /// Behind a RefCell to keep load op take &self\n+    loads: RefCell<FxHashMap<VectorIdx, VTimestamp>>,\n+}\n+\n+impl StoreElement {\n+    /// ATOMIC LOAD IMPL in the paper\n+    /// Unlike the operational semantics in the paper, we don't need to keep track\n+    /// of the thread timestamp for every single load. Keeping track of the first (smallest)\n+    /// timestamp of each thread that has loaded from a store is sufficient: if the earliest\n+    /// load of another thread happens before the current one, then we must stop searching the store\n+    /// buffer regardless of subsequent loads by the same thread; if the earliest load of another\n+    /// thread doesn't happen before the current one, then no subsequent load by the other thread\n+    /// can happen before the current one.\n+    fn load_impl(&self, index: VectorIdx, clocks: &ThreadClockSet) -> ScalarMaybeUninit<Tag> {\n+        let _ = self.loads.borrow_mut().try_insert(index, clocks.clock[index]);\n+        self.val\n+    }\n+}"}, {"sha": "b8e780ade1a03688568289315a53c4bca8dc53fc", "filename": "tests/run-pass/concurrency/weak_memory.rs", "status": "modified", "additions": 23, "deletions": 0, "changes": 23, "blob_url": "https://github.com/rust-lang/rust/blob/e7698f4f07dcec9cf42b3de133f9ca171d0677f0/tests%2Frun-pass%2Fconcurrency%2Fweak_memory.rs", "raw_url": "https://github.com/rust-lang/rust/raw/e7698f4f07dcec9cf42b3de133f9ca171d0677f0/tests%2Frun-pass%2Fconcurrency%2Fweak_memory.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/tests%2Frun-pass%2Fconcurrency%2Fweak_memory.rs?ref=e7698f4f07dcec9cf42b3de133f9ca171d0677f0", "patch": "@@ -63,6 +63,28 @@ fn reads_value(loc: &AtomicUsize, val: usize) -> usize {\n     val\n }\n \n+// https://plv.mpi-sws.org/scfix/paper.pdf\n+// Test case SB\n+fn test_sc_store_buffering() {\n+    let x = static_atomic(0);\n+    let y = static_atomic(0);\n+\n+    let j1 = spawn(move || {\n+        x.store(1, SeqCst);\n+        y.load(SeqCst)\n+    });\n+\n+    let j2 = spawn(move || {\n+        y.store(1, SeqCst);\n+        x.load(SeqCst)\n+    });\n+\n+    let a = j1.join().unwrap();\n+    let b = j2.join().unwrap();\n+\n+    assert_ne!((a, b), (0, 0));\n+}\n+\n // https://plv.mpi-sws.org/scfix/paper.pdf\n // 2.2 Second Problem: SC Fences are Too Weak\n fn test_rwc_syncs() {\n@@ -247,6 +269,7 @@ pub fn main() {\n     // prehaps each function should be its own test case so they\n     // can be run in parallel\n     for _ in 0..500 {\n+        test_sc_store_buffering();\n         test_mixed_access();\n         test_load_buffering_acq_rel();\n         test_message_passing();"}]}