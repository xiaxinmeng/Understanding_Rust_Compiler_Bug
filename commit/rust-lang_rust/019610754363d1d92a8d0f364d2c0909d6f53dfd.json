{"sha": "019610754363d1d92a8d0f364d2c0909d6f53dfd", "node_id": "MDY6Q29tbWl0NzI0NzEyOjAxOTYxMDc1NDM2M2QxZDkyYThkMGYzNjRkMmMwOTA5ZDZmNTNkZmQ=", "commit": {"author": {"name": "bors", "email": "bors@rust-lang.org", "date": "2021-02-23T14:38:45Z"}, "committer": {"name": "bors", "email": "bors@rust-lang.org", "date": "2021-02-23T14:38:45Z"}, "message": "Auto merge of #82127 - tgnottingham:tune-ahead-of-time-codegen, r=varkor\n\nrustc_codegen_ssa: tune codegen according to available concurrency\n\nThis change tunes ahead-of-time codegening according to the amount of\nconcurrency available, rather than according to the number of CPUs on\nthe system. This can lower memory usage by reducing the number of\ncompiled LLVM modules in memory at once, particularly across several\nrustc instances.\n\nPreviously, each rustc instance would assume that it should codegen\nahead of time to meet the demand of number-of-CPUs workers. But often, a\nrustc instance doesn't have nearly that much concurrency available to\nit, because the concurrency availability is split, via the jobserver,\nacross all active rustc instances spawned by the driving cargo process,\nand is further limited by the `-j` flag argument. Therefore, each rustc\nmight have had several times the number of LLVM modules in memory than\nit really needed to meet demand. If the modules were large, the effect\non memory usage would be noticeable.\n\nWith this change, the required amount of ahead-of-time codegen scales up\nwith the actual number of workers running within a rustc instance. Note\nthat the number of workers running can be less than the actual\nconcurrency available to a rustc instance. However, if more concurrency\nis actually available, workers are spun up quickly as job tokens are\nacquired, and the ahead-of-time codegen scales up quickly as well.", "tree": {"sha": "618b31afa4f153f3af52d6246b2e24ae0fac2b47", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/618b31afa4f153f3af52d6246b2e24ae0fac2b47"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/019610754363d1d92a8d0f364d2c0909d6f53dfd", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/019610754363d1d92a8d0f364d2c0909d6f53dfd", "html_url": "https://github.com/rust-lang/rust/commit/019610754363d1d92a8d0f364d2c0909d6f53dfd", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/019610754363d1d92a8d0f364d2c0909d6f53dfd/comments", "author": {"login": "bors", "id": 3372342, "node_id": "MDQ6VXNlcjMzNzIzNDI=", "avatar_url": "https://avatars.githubusercontent.com/u/3372342?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bors", "html_url": "https://github.com/bors", "followers_url": "https://api.github.com/users/bors/followers", "following_url": "https://api.github.com/users/bors/following{/other_user}", "gists_url": "https://api.github.com/users/bors/gists{/gist_id}", "starred_url": "https://api.github.com/users/bors/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bors/subscriptions", "organizations_url": "https://api.github.com/users/bors/orgs", "repos_url": "https://api.github.com/users/bors/repos", "events_url": "https://api.github.com/users/bors/events{/privacy}", "received_events_url": "https://api.github.com/users/bors/received_events", "type": "User", "site_admin": false}, "committer": {"login": "bors", "id": 3372342, "node_id": "MDQ6VXNlcjMzNzIzNDI=", "avatar_url": "https://avatars.githubusercontent.com/u/3372342?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bors", "html_url": "https://github.com/bors", "followers_url": "https://api.github.com/users/bors/followers", "following_url": "https://api.github.com/users/bors/following{/other_user}", "gists_url": "https://api.github.com/users/bors/gists{/gist_id}", "starred_url": "https://api.github.com/users/bors/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bors/subscriptions", "organizations_url": "https://api.github.com/users/bors/orgs", "repos_url": "https://api.github.com/users/bors/repos", "events_url": "https://api.github.com/users/bors/events{/privacy}", "received_events_url": "https://api.github.com/users/bors/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "446d4533e89db04f9568be4199e56b5fce0d176d", "url": "https://api.github.com/repos/rust-lang/rust/commits/446d4533e89db04f9568be4199e56b5fce0d176d", "html_url": "https://github.com/rust-lang/rust/commit/446d4533e89db04f9568be4199e56b5fce0d176d"}, {"sha": "5f243d3c2bf4188ec782e43b695e50602c777cd9", "url": "https://api.github.com/repos/rust-lang/rust/commits/5f243d3c2bf4188ec782e43b695e50602c777cd9", "html_url": "https://github.com/rust-lang/rust/commit/5f243d3c2bf4188ec782e43b695e50602c777cd9"}], "stats": {"total": 75, "additions": 64, "deletions": 11}, "files": [{"sha": "4ae1ab2070e29c606b66fa193f92669fe22b9e6e", "filename": "Cargo.lock", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "blob_url": "https://github.com/rust-lang/rust/blob/019610754363d1d92a8d0f364d2c0909d6f53dfd/Cargo.lock", "raw_url": "https://github.com/rust-lang/rust/raw/019610754363d1d92a8d0f364d2c0909d6f53dfd/Cargo.lock", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/Cargo.lock?ref=019610754363d1d92a8d0f364d2c0909d6f53dfd", "patch": "@@ -3654,7 +3654,6 @@ dependencies = [\n  \"jobserver\",\n  \"libc\",\n  \"memmap\",\n- \"num_cpus\",\n  \"pathdiff\",\n  \"rustc_apfloat\",\n  \"rustc_ast\","}, {"sha": "5e2f01b2c9da86b19a2500ca5daf0d27786ec8d7", "filename": "compiler/rustc_codegen_ssa/Cargo.toml", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "blob_url": "https://github.com/rust-lang/rust/blob/019610754363d1d92a8d0f364d2c0909d6f53dfd/compiler%2Frustc_codegen_ssa%2FCargo.toml", "raw_url": "https://github.com/rust-lang/rust/raw/019610754363d1d92a8d0f364d2c0909d6f53dfd/compiler%2Frustc_codegen_ssa%2FCargo.toml", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_codegen_ssa%2FCargo.toml?ref=019610754363d1d92a8d0f364d2c0909d6f53dfd", "patch": "@@ -11,7 +11,6 @@ test = false\n bitflags = \"1.2.1\"\n cc = \"1.0.1\"\n itertools = \"0.9\"\n-num_cpus = \"1.0\"\n memmap = \"0.7\"\n tracing = \"0.1\"\n libc = \"0.2.50\""}, {"sha": "d931c57fba247927a1c12e562e5bbe503f40aa97", "filename": "compiler/rustc_codegen_ssa/src/back/write.rs", "status": "modified", "additions": 64, "deletions": 9, "changes": 73, "blob_url": "https://github.com/rust-lang/rust/blob/019610754363d1d92a8d0f364d2c0909d6f53dfd/compiler%2Frustc_codegen_ssa%2Fsrc%2Fback%2Fwrite.rs", "raw_url": "https://github.com/rust-lang/rust/raw/019610754363d1d92a8d0f364d2c0909d6f53dfd/compiler%2Frustc_codegen_ssa%2Fsrc%2Fback%2Fwrite.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_codegen_ssa%2Fsrc%2Fback%2Fwrite.rs?ref=019610754363d1d92a8d0f364d2c0909d6f53dfd", "patch": "@@ -1193,7 +1193,6 @@ fn start_executing_work<B: ExtraBackendMethods>(\n     // necessary. There's already optimizations in place to avoid sending work\n     // back to the coordinator if LTO isn't requested.\n     return thread::spawn(move || {\n-        let max_workers = num_cpus::get();\n         let mut worker_id_counter = 0;\n         let mut free_worker_ids = Vec::new();\n         let mut get_worker_id = |free_worker_ids: &mut Vec<usize>| {\n@@ -1253,7 +1252,17 @@ fn start_executing_work<B: ExtraBackendMethods>(\n             // For codegenning more CGU or for running them through LLVM.\n             if !codegen_done {\n                 if main_thread_worker_state == MainThreadWorkerState::Idle {\n-                    if !queue_full_enough(work_items.len(), running, max_workers) {\n+                    // Compute the number of workers that will be running once we've taken as many\n+                    // items from the work queue as we can, plus one for the main thread. It's not\n+                    // critically important that we use this instead of just `running`, but it\n+                    // prevents the `queue_full_enough` heuristic from fluctuating just because a\n+                    // worker finished up and we decreased the `running` count, even though we're\n+                    // just going to increase it right after this when we put a new worker to work.\n+                    let extra_tokens = tokens.len().checked_sub(running).unwrap();\n+                    let additional_running = std::cmp::min(extra_tokens, work_items.len());\n+                    let anticipated_running = running + additional_running + 1;\n+\n+                    if !queue_full_enough(work_items.len(), anticipated_running) {\n                         // The queue is not full enough, codegen more items:\n                         if codegen_worker_send.send(Message::CodegenItem).is_err() {\n                             panic!(\"Could not send Message::CodegenItem to main thread\")\n@@ -1529,13 +1538,59 @@ fn start_executing_work<B: ExtraBackendMethods>(\n \n     // A heuristic that determines if we have enough LLVM WorkItems in the\n     // queue so that the main thread can do LLVM work instead of codegen\n-    fn queue_full_enough(\n-        items_in_queue: usize,\n-        workers_running: usize,\n-        max_workers: usize,\n-    ) -> bool {\n-        // Tune me, plz.\n-        items_in_queue > 0 && items_in_queue >= max_workers.saturating_sub(workers_running / 2)\n+    fn queue_full_enough(items_in_queue: usize, workers_running: usize) -> bool {\n+        // This heuristic scales ahead-of-time codegen according to available\n+        // concurrency, as measured by `workers_running`. The idea is that the\n+        // more concurrency we have available, the more demand there will be for\n+        // work items, and the fuller the queue should be kept to meet demand.\n+        // An important property of this approach is that we codegen ahead of\n+        // time only as much as necessary, so as to keep fewer LLVM modules in\n+        // memory at once, thereby reducing memory consumption.\n+        //\n+        // When the number of workers running is less than the max concurrency\n+        // available to us, this heuristic can cause us to instruct the main\n+        // thread to work on an LLVM item (that is, tell it to \"LLVM\") instead\n+        // of codegen, even though it seems like it *should* be codegenning so\n+        // that we can create more work items and spawn more LLVM workers.\n+        //\n+        // But this is not a problem. When the main thread is told to LLVM,\n+        // according to this heuristic and how work is scheduled, there is\n+        // always at least one item in the queue, and therefore at least one\n+        // pending jobserver token request. If there *is* more concurrency\n+        // available, we will immediately receive a token, which will upgrade\n+        // the main thread's LLVM worker to a real one (conceptually), and free\n+        // up the main thread to codegen if necessary. On the other hand, if\n+        // there isn't more concurrency, then the main thread working on an LLVM\n+        // item is appropriate, as long as the queue is full enough for demand.\n+        //\n+        // Speaking of which, how full should we keep the queue? Probably less\n+        // full than you'd think. A lot has to go wrong for the queue not to be\n+        // full enough and for that to have a negative effect on compile times.\n+        //\n+        // Workers are unlikely to finish at exactly the same time, so when one\n+        // finishes and takes another work item off the queue, we often have\n+        // ample time to codegen at that point before the next worker finishes.\n+        // But suppose that codegen takes so long that the workers exhaust the\n+        // queue, and we have one or more workers that have nothing to work on.\n+        // Well, it might not be so bad. Of all the LLVM modules we create and\n+        // optimize, one has to finish last. It's not necessarily the case that\n+        // by losing some concurrency for a moment, we delay the point at which\n+        // that last LLVM module is finished and the rest of compilation can\n+        // proceed. Also, when we can't take advantage of some concurrency, we\n+        // give tokens back to the job server. That enables some other rustc to\n+        // potentially make use of the available concurrency. That could even\n+        // *decrease* overall compile time if we're lucky. But yes, if no other\n+        // rustc can make use of the concurrency, then we've squandered it.\n+        //\n+        // However, keeping the queue full is also beneficial when we have a\n+        // surge in available concurrency. Then items can be taken from the\n+        // queue immediately, without having to wait for codegen.\n+        //\n+        // So, the heuristic below tries to keep one item in the queue for every\n+        // four running workers. Based on limited benchmarking, this appears to\n+        // be more than sufficient to avoid increasing compilation times.\n+        let quarter_of_workers = workers_running - 3 * workers_running / 4;\n+        items_in_queue > 0 && items_in_queue >= quarter_of_workers\n     }\n \n     fn maybe_start_llvm_timer<'a>("}]}