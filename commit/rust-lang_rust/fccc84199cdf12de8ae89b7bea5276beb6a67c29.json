{"sha": "fccc84199cdf12de8ae89b7bea5276beb6a67c29", "node_id": "MDY6Q29tbWl0NzI0NzEyOmZjY2M4NDE5OWNkZjEyZGU4YWU4OWI3YmVhNTI3NmJlYjZhNjdjMjk=", "commit": {"author": {"name": "Wesley Wiser", "email": "wwiser@gmail.com", "date": "2019-02-11T23:11:43Z"}, "committer": {"name": "Wesley Wiser", "email": "wwiser@gmail.com", "date": "2019-03-03T15:07:32Z"}, "message": "Remove profiler output and replace with a raw event dump\n\nRelated to #58372", "tree": {"sha": "304cc1d38a2dbdef46c7e90db84a18fd3ebbc065", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/304cc1d38a2dbdef46c7e90db84a18fd3ebbc065"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/fccc84199cdf12de8ae89b7bea5276beb6a67c29", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/fccc84199cdf12de8ae89b7bea5276beb6a67c29", "html_url": "https://github.com/rust-lang/rust/commit/fccc84199cdf12de8ae89b7bea5276beb6a67c29", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/fccc84199cdf12de8ae89b7bea5276beb6a67c29/comments", "author": {"login": "wesleywiser", "id": 831192, "node_id": "MDQ6VXNlcjgzMTE5Mg==", "avatar_url": "https://avatars.githubusercontent.com/u/831192?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wesleywiser", "html_url": "https://github.com/wesleywiser", "followers_url": "https://api.github.com/users/wesleywiser/followers", "following_url": "https://api.github.com/users/wesleywiser/following{/other_user}", "gists_url": "https://api.github.com/users/wesleywiser/gists{/gist_id}", "starred_url": "https://api.github.com/users/wesleywiser/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wesleywiser/subscriptions", "organizations_url": "https://api.github.com/users/wesleywiser/orgs", "repos_url": "https://api.github.com/users/wesleywiser/repos", "events_url": "https://api.github.com/users/wesleywiser/events{/privacy}", "received_events_url": "https://api.github.com/users/wesleywiser/received_events", "type": "User", "site_admin": false}, "committer": {"login": "wesleywiser", "id": 831192, "node_id": "MDQ6VXNlcjgzMTE5Mg==", "avatar_url": "https://avatars.githubusercontent.com/u/831192?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wesleywiser", "html_url": "https://github.com/wesleywiser", "followers_url": "https://api.github.com/users/wesleywiser/followers", "following_url": "https://api.github.com/users/wesleywiser/following{/other_user}", "gists_url": "https://api.github.com/users/wesleywiser/gists{/gist_id}", "starred_url": "https://api.github.com/users/wesleywiser/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wesleywiser/subscriptions", "organizations_url": "https://api.github.com/users/wesleywiser/orgs", "repos_url": "https://api.github.com/users/wesleywiser/repos", "events_url": "https://api.github.com/users/wesleywiser/events{/privacy}", "received_events_url": "https://api.github.com/users/wesleywiser/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "25b8c614f020c7298eb70198b781d591d81bed7d", "url": "https://api.github.com/repos/rust-lang/rust/commits/25b8c614f020c7298eb70198b781d591d81bed7d", "html_url": "https://github.com/rust-lang/rust/commit/25b8c614f020c7298eb70198b781d591d81bed7d"}], "stats": {"total": 600, "additions": 271, "deletions": 329}, "files": [{"sha": "774ab0333db54a27f89a230c9643f12163029638", "filename": "src/librustc/session/config.rs", "status": "modified", "additions": 1, "deletions": 3, "changes": 4, "blob_url": "https://github.com/rust-lang/rust/blob/fccc84199cdf12de8ae89b7bea5276beb6a67c29/src%2Flibrustc%2Fsession%2Fconfig.rs", "raw_url": "https://github.com/rust-lang/rust/raw/fccc84199cdf12de8ae89b7bea5276beb6a67c29/src%2Flibrustc%2Fsession%2Fconfig.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Fsession%2Fconfig.rs?ref=fccc84199cdf12de8ae89b7bea5276beb6a67c29", "patch": "@@ -1405,9 +1405,7 @@ options! {DebuggingOptions, DebuggingSetter, basic_debugging_options,\n     crate_attr: Vec<String> = (Vec::new(), parse_string_push, [TRACKED],\n         \"inject the given attribute in the crate\"),\n     self_profile: bool = (false, parse_bool, [UNTRACKED],\n-        \"run the self profiler\"),\n-    profile_json: bool = (false, parse_bool, [UNTRACKED],\n-        \"output a json file with profiler results\"),\n+        \"run the self profiler and output the raw event data\"),\n     emit_stack_sizes: bool = (false, parse_bool, [UNTRACKED],\n         \"emits a section containing stack size metadata\"),\n     plt: Option<bool> = (None, parse_opt_bool, [TRACKED],"}, {"sha": "774bc8b450b594eba6d185a94adf811c1b767f72", "filename": "src/librustc/session/mod.rs", "status": "modified", "additions": 1, "deletions": 4, "changes": 5, "blob_url": "https://github.com/rust-lang/rust/blob/fccc84199cdf12de8ae89b7bea5276beb6a67c29/src%2Flibrustc%2Fsession%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/fccc84199cdf12de8ae89b7bea5276beb6a67c29/src%2Flibrustc%2Fsession%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Fsession%2Fmod.rs?ref=fccc84199cdf12de8ae89b7bea5276beb6a67c29", "patch": "@@ -1131,11 +1131,8 @@ pub fn build_session_(\n     source_map: Lrc<source_map::SourceMap>,\n     driver_lint_caps: FxHashMap<lint::LintId, lint::Level>,\n ) -> Session {\n-    let self_profiling_active = sopts.debugging_opts.self_profile ||\n-                                sopts.debugging_opts.profile_json;\n-\n     let self_profiler =\n-        if self_profiling_active { Some(Arc::new(PlMutex::new(SelfProfiler::new()))) }\n+        if sopts.debugging_opts.self_profile { Some(Arc::new(PlMutex::new(SelfProfiler::new()))) }\n         else { None };\n \n     let host_triple = TargetTriple::from_triple(config::host_triple());"}, {"sha": "bd40566065b6bfac641612dad500cf1c24b648aa", "filename": "src/librustc/util/profiling.rs", "status": "modified", "additions": 268, "deletions": 316, "changes": 584, "blob_url": "https://github.com/rust-lang/rust/blob/fccc84199cdf12de8ae89b7bea5276beb6a67c29/src%2Flibrustc%2Futil%2Fprofiling.rs", "raw_url": "https://github.com/rust-lang/rust/raw/fccc84199cdf12de8ae89b7bea5276beb6a67c29/src%2Flibrustc%2Futil%2Fprofiling.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Futil%2Fprofiling.rs?ref=fccc84199cdf12de8ae89b7bea5276beb6a67c29", "patch": "@@ -1,10 +1,12 @@\n-use std::collections::{BTreeMap, HashMap};\n+use std::collections::HashMap;\n use std::fs;\n-use std::io::{self, Write};\n+use std::io::{BufWriter, Write};\n+use std::mem;\n+use std::process;\n use std::thread::ThreadId;\n use std::time::Instant;\n \n-use crate::session::config::{Options, OptLevel};\n+use crate::session::config::Options;\n \n #[derive(Clone, Copy, Debug, PartialEq, Eq, Ord, PartialOrd)]\n pub enum ProfileCategory {\n@@ -23,140 +25,39 @@ pub enum ProfilerEvent {\n     QueryEnd { query_name: &'static str, category: ProfileCategory, time: Instant },\n     GenericActivityStart { category: ProfileCategory, time: Instant },\n     GenericActivityEnd { category: ProfileCategory, time: Instant },\n-    QueryCacheHit { query_name: &'static str, category: ProfileCategory },\n-    QueryCount { query_name: &'static str, category: ProfileCategory, count: usize },\n     IncrementalLoadResultStart { query_name: &'static str, time: Instant },\n     IncrementalLoadResultEnd { query_name: &'static str, time: Instant },\n+    QueryCacheHit { query_name: &'static str, category: ProfileCategory, time: Instant },\n+    QueryCount { query_name: &'static str, category: ProfileCategory, count: usize, time: Instant },\n     QueryBlockedStart { query_name: &'static str, category: ProfileCategory, time: Instant },\n     QueryBlockedEnd { query_name: &'static str, category: ProfileCategory, time: Instant },\n }\n \n impl ProfilerEvent {\n-    fn is_start_event(&self) -> bool {\n+    fn timestamp(&self) -> Instant {\n         use self::ProfilerEvent::*;\n \n         match self {\n-            QueryStart { .. } |\n-            GenericActivityStart { .. } |\n-            IncrementalLoadResultStart { .. } |\n-            QueryBlockedStart { .. } => true,\n-\n-            QueryEnd { .. } |\n-            GenericActivityEnd { .. } |\n-            QueryCacheHit { .. } |\n-            QueryCount { .. } |\n-            IncrementalLoadResultEnd { .. } |\n-            QueryBlockedEnd { .. } => false,\n+            QueryStart { time, .. } |\n+            QueryEnd { time, .. } |\n+            GenericActivityStart { time, .. } |\n+            GenericActivityEnd { time, .. } |\n+            QueryCacheHit { time, .. } |\n+            QueryCount { time, .. } |\n+            IncrementalLoadResultStart { time, .. } |\n+            IncrementalLoadResultEnd { time, .. } |\n+            QueryBlockedStart { time, .. } |\n+            QueryBlockedEnd { time, .. } => *time\n         }\n     }\n }\n \n-pub struct SelfProfiler {\n-    events: HashMap<ThreadId, Vec<ProfilerEvent>>,\n-}\n-\n-struct CategoryResultData {\n-    query_times: BTreeMap<&'static str, u64>,\n-    query_cache_stats: BTreeMap<&'static str, (u64, u64)>, //(hits, total)\n-}\n-\n-impl CategoryResultData {\n-    fn new() -> CategoryResultData {\n-        CategoryResultData {\n-            query_times: BTreeMap::new(),\n-            query_cache_stats: BTreeMap::new(),\n-        }\n-    }\n-\n-    fn total_time(&self) -> u64 {\n-        self.query_times.iter().map(|(_, time)| time).sum()\n-    }\n-\n-    fn total_cache_data(&self) -> (u64, u64) {\n-        let (mut hits, mut total) = (0, 0);\n-\n-        for (_, (h, t)) in &self.query_cache_stats {\n-            hits += h;\n-            total += t;\n-        }\n-\n-        (hits, total)\n-    }\n-}\n-\n-impl Default for CategoryResultData {\n-    fn default() -> CategoryResultData {\n-        CategoryResultData::new()\n-    }\n-}\n-\n-struct CalculatedResults {\n-    categories: BTreeMap<ProfileCategory, CategoryResultData>,\n-    crate_name: Option<String>,\n-    optimization_level: OptLevel,\n-    incremental: bool,\n-    verbose: bool,\n-}\n-\n-impl CalculatedResults {\n-    fn new() -> CalculatedResults {\n-        CalculatedResults {\n-            categories: BTreeMap::new(),\n-            crate_name: None,\n-            optimization_level: OptLevel::No,\n-            incremental: false,\n-            verbose: false,\n-        }\n-    }\n-\n-    fn consolidate(mut cr1: CalculatedResults, cr2: CalculatedResults) -> CalculatedResults {\n-        for (category, data) in cr2.categories {\n-            let cr1_data = cr1.categories.entry(category).or_default();\n-\n-            for (query, time) in data.query_times {\n-                *cr1_data.query_times.entry(query).or_default() += time;\n-            }\n-\n-            for (query, (hits, total)) in data.query_cache_stats {\n-                let (h, t) = cr1_data.query_cache_stats.entry(query).or_insert((0, 0));\n-                *h += hits;\n-                *t += total;\n-            }\n-        }\n-\n-        cr1\n-    }\n-\n-    fn total_time(&self) -> u64 {\n-        self.categories.iter().map(|(_, data)| data.total_time()).sum()\n-    }\n-\n-    fn with_options(mut self, opts: &Options) -> CalculatedResults {\n-        self.crate_name = opts.crate_name.clone();\n-        self.optimization_level = opts.optimize;\n-        self.incremental = opts.incremental.is_some();\n-        self.verbose = opts.debugging_opts.verbose;\n-\n-        self\n-    }\n+fn thread_id_to_u64(tid: ThreadId) -> u64 {\n+    unsafe { mem::transmute::<ThreadId, u64>(tid) }\n }\n \n-fn time_between_ns(start: Instant, end: Instant) -> u64 {\n-    if start < end {\n-        let time = end - start;\n-        (time.as_secs() * 1_000_000_000) + (time.subsec_nanos() as u64)\n-    } else {\n-        debug!(\"time_between_ns: ignorning instance of end < start\");\n-        0\n-    }\n-}\n-\n-fn calculate_percent(numerator: u64, denominator: u64) -> f32 {\n-    if denominator > 0 {\n-        ((numerator as f32) / (denominator as f32)) * 100.0\n-    } else {\n-        0.0\n-    }\n+pub struct SelfProfiler {\n+    events: HashMap<ThreadId, Vec<ProfilerEvent>>,\n }\n \n impl SelfProfiler {\n@@ -197,6 +98,7 @@ impl SelfProfiler {\n             query_name,\n             category,\n             count,\n+            time: Instant::now(),\n         })\n     }\n \n@@ -205,6 +107,7 @@ impl SelfProfiler {\n         self.record(ProfilerEvent::QueryCacheHit {\n             query_name,\n             category,\n+            time: Instant::now(),\n         })\n     }\n \n@@ -268,208 +171,257 @@ impl SelfProfiler {\n         events.push(event);\n     }\n \n-    fn calculate_thread_results(events: &Vec<ProfilerEvent>) -> CalculatedResults {\n+    pub fn dump_raw_events(&self, opts: &Options) {\n         use self::ProfilerEvent::*;\n \n-        assert!(\n-            events.last().map(|e| !e.is_start_event()).unwrap_or(true),\n-            \"there was an event running when calculate_reslts() was called\"\n-        );\n-\n-        let mut results = CalculatedResults::new();\n-\n-        //(event, child time to subtract)\n-        let mut query_stack = Vec::new();\n-\n-        for event in events {\n-            match event {\n-                QueryStart { .. } | GenericActivityStart { .. } => {\n-                    query_stack.push((event, 0));\n-                },\n-                QueryEnd { query_name, category, time: end_time } => {\n-                    let previous_query = query_stack.pop();\n-                    if let Some((QueryStart {\n-                                    query_name: p_query_name,\n-                                    time: start_time,\n-                                    category: _ }, child_time_to_subtract)) = previous_query {\n-                        assert_eq!(\n-                            p_query_name,\n-                            query_name,\n-                            \"Saw a query end but the previous query wasn't the corresponding start\"\n-                        );\n-\n-                        let time_ns = time_between_ns(*start_time, *end_time);\n-                        let self_time_ns = time_ns - child_time_to_subtract;\n-                        let result_data = results.categories.entry(*category).or_default();\n-\n-                        *result_data.query_times.entry(query_name).or_default() += self_time_ns;\n-\n-                        if let Some((_, child_time_to_subtract)) = query_stack.last_mut() {\n-                            *child_time_to_subtract += time_ns;\n-                        }\n-                    } else {\n-                        bug!(\"Saw a query end but the previous event wasn't a query start\");\n-                    }\n+        //find the earliest Instant to use as t=0\n+        //when serializing the events, we'll calculate a Duration\n+        //using (instant - min_instant)\n+        let min_instant =\n+            self.events\n+                .iter()\n+                .map(|(_, values)| values[0].timestamp())\n+                .min()\n+                .unwrap();\n+\n+        let pid = process::id();\n+\n+        let filename =\n+            format!(\"{}.profile_events.json\", opts.crate_name.clone().unwrap_or_default());\n+\n+        let mut file = BufWriter::new(fs::File::create(filename).unwrap());\n+\n+        let threads: Vec<_> =\n+            self.events\n+                .keys()\n+                .into_iter()\n+                .map(|tid| format!(\"{}\", thread_id_to_u64(*tid)))\n+                .collect();\n+\n+        write!(file,\n+            \"{{\\\n+                \\\"processes\\\": {{\\\n+                    \\\"{}\\\": {{\\\n+                        \\\"threads\\\": [{}],\\\n+                        \\\"crate_name\\\": \\\"{}\\\",\\\n+                        \\\"opt_level\\\": \\\"{:?}\\\",\\\n+                        \\\"incremental\\\": {}\\\n+                    }}\\\n+                }},\\\n+                \\\"events\\\": [\\\n+             \",\n+            pid,\n+            threads.join(\",\"),\n+            opts.crate_name.clone().unwrap_or_default(),\n+            opts.optimize,\n+            if opts.incremental.is_some() { \"true\" } else { \"false\" },\n+        ).unwrap();\n+\n+        let mut is_first = true;\n+        for (thread_id, events) in &self.events {\n+            let thread_id = thread_id_to_u64(*thread_id);\n+\n+            for event in events {\n+                if is_first {\n+                    is_first = false;\n+                } else {\n+                    writeln!(file, \",\").unwrap();\n                 }\n-                GenericActivityEnd { category, time: end_time } => {\n-                    let previous_event = query_stack.pop();\n-                    if let Some((GenericActivityStart {\n-                                    category: previous_category,\n-                                    time: start_time }, child_time_to_subtract)) = previous_event {\n-                        assert_eq!(\n-                            previous_category,\n-                            category,\n-                            \"Saw an end but the previous event wasn't the corresponding start\"\n-                        );\n-\n-                        let time_ns = time_between_ns(*start_time, *end_time);\n-                        let self_time_ns = time_ns - child_time_to_subtract;\n-                        let result_data = results.categories.entry(*category).or_default();\n-\n-                        *result_data.query_times\n-                            .entry(\"{time spent not running queries}\")\n-                            .or_default() += self_time_ns;\n-\n-                        if let Some((_, child_time_to_subtract)) = query_stack.last_mut() {\n-                            *child_time_to_subtract += time_ns;\n-                        }\n-                    } else {\n-                        bug!(\"Saw an activity end but the previous event wasn't an activity start\");\n-                    }\n-                },\n-                QueryCacheHit { category, query_name } => {\n-                    let result_data = results.categories.entry(*category).or_default();\n-\n-                    let (hits, total) =\n-                        result_data.query_cache_stats.entry(query_name).or_insert((0, 0));\n-                    *hits += 1;\n-                    *total += 1;\n-                },\n-                QueryCount { category, query_name, count } => {\n-                    let result_data = results.categories.entry(*category).or_default();\n-\n-                    let (_, totals) =\n-                        result_data.query_cache_stats.entry(query_name).or_insert((0, 0));\n-                    *totals += *count as u64;\n-                },\n-                //we don't summarize incremental load result events in the simple output mode\n-                IncrementalLoadResultStart { .. } | IncrementalLoadResultEnd { .. } => { },\n-                //we don't summarize parallel query blocking in the simple output mode\n-                QueryBlockedStart { .. } | QueryBlockedEnd { .. } => { },\n-            }\n-        }\n \n-        //normalize the times to ms\n-        for (_, data) in &mut results.categories {\n-            for (_, time) in &mut data.query_times {\n-                *time = *time / 1_000_000;\n-            }\n-        }\n-\n-        results\n-    }\n-\n-    fn get_results(&self, opts: &Options) -> CalculatedResults {\n-        self.events\n-            .iter()\n-            .map(|(_, r)| SelfProfiler::calculate_thread_results(r))\n-            .fold(CalculatedResults::new(), CalculatedResults::consolidate)\n-            .with_options(opts)\n-    }\n-\n-    pub fn print_results(&mut self, opts: &Options) {\n-        self.end_activity(ProfileCategory::Other);\n-\n-        let results = self.get_results(opts);\n-\n-        let total_time = results.total_time() as f32;\n-\n-        let out = io::stderr();\n-        let mut lock = out.lock();\n-\n-        let crate_name = results.crate_name.map(|n| format!(\" for {}\", n)).unwrap_or_default();\n-\n-        writeln!(lock, \"Self profiling results{}:\", crate_name).unwrap();\n-        writeln!(lock).unwrap();\n-\n-        writeln!(lock, \"| Phase                                     | Time (ms)      \\\n-                        | Time (%) | Queries        | Hits (%)\")\n-            .unwrap();\n-        writeln!(lock, \"| ----------------------------------------- | -------------- \\\n-                        | -------- | -------------- | --------\")\n-            .unwrap();\n-\n-        let mut categories: Vec<_> = results.categories.iter().collect();\n-        categories.sort_by_cached_key(|(_, d)| d.total_time());\n-\n-        for (category, data) in categories.iter().rev() {\n-            let (category_hits, category_total) = data.total_cache_data();\n-            let category_hit_percent = calculate_percent(category_hits, category_total);\n-\n-            writeln!(\n-                lock,\n-                \"| {0: <41} | {1: >14} | {2: >8.2} | {3: >14} | {4: >8}\",\n-                format!(\"{:?}\", category),\n-                data.total_time(),\n-                ((data.total_time() as f32) / total_time) * 100.0,\n-                category_total,\n-                format!(\"{:.2}\", category_hit_percent),\n-            ).unwrap();\n-\n-            //in verbose mode, show individual query data\n-            if results.verbose {\n-                //don't show queries that took less than 1ms\n-                let mut times: Vec<_> = data.query_times.iter().filter(|(_, t)| **t > 0).collect();\n-                times.sort_by(|(_, time1), (_, time2)| time2.cmp(time1));\n-\n-                for (query, time) in times {\n-                    let (hits, total) = data.query_cache_stats.get(query).unwrap_or(&(0, 0));\n-                    let hit_percent = calculate_percent(*hits, *total);\n-\n-                    writeln!(\n-                        lock,\n-                        \"| - {0: <39} | {1: >14} | {2: >8.2} | {3: >14} | {4: >8}\",\n-                        query,\n-                        time,\n-                        ((*time as f32) / total_time) * 100.0,\n-                        total,\n-                        format!(\"{:.2}\", hit_percent),\n-                    ).unwrap();\n+                let (secs, nanos) = {\n+                    let duration = event.timestamp() - min_instant;\n+                    (duration.as_secs(), duration.subsec_nanos())\n+                };\n+\n+                match event {\n+                    QueryStart { query_name, category, time: _ } =>\n+                        write!(file,\n+                            \"{{ \\\n+                                \\\"QueryStart\\\": {{ \\\n+                                    \\\"query_name\\\": \\\"{}\\\",\\\n+                                    \\\"category\\\": \\\"{:?}\\\",\\\n+                                    \\\"time\\\": {{\\\n+                                        \\\"secs\\\": {},\\\n+                                        \\\"nanos\\\": {}\\\n+                                    }},\\\n+                                    \\\"thread_id\\\": {}\\\n+                                }}\\\n+                            }}\",\n+                            query_name,\n+                            category,\n+                            secs,\n+                            nanos,\n+                            thread_id,\n+                        ).unwrap(),\n+                    QueryEnd { query_name, category, time: _ } =>\n+                        write!(file,\n+                            \"{{\\\n+                                \\\"QueryEnd\\\": {{\\\n+                                    \\\"query_name\\\": \\\"{}\\\",\\\n+                                    \\\"category\\\": \\\"{:?}\\\",\\\n+                                    \\\"time\\\": {{\\\n+                                        \\\"secs\\\": {},\\\n+                                        \\\"nanos\\\": {}\\\n+                                    }},\\\n+                                    \\\"thread_id\\\": {}\\\n+                                }}\\\n+                            }}\",\n+                            query_name,\n+                            category,\n+                            secs,\n+                            nanos,\n+                            thread_id,\n+                        ).unwrap(),\n+                    GenericActivityStart { category, time: _ } =>\n+                        write!(file,\n+                            \"{{\n+                                \\\"GenericActivityStart\\\": {{\\\n+                                    \\\"category\\\": \\\"{:?}\\\",\\\n+                                    \\\"time\\\": {{\\\n+                                        \\\"secs\\\": {},\\\n+                                        \\\"nanos\\\": {}\\\n+                                    }},\\\n+                                    \\\"thread_id\\\": {}\\\n+                                }}\\\n+                            }}\",\n+                            category,\n+                            secs,\n+                            nanos,\n+                            thread_id,\n+                        ).unwrap(),\n+                    GenericActivityEnd { category, time: _ } =>\n+                        write!(file,\n+                            \"{{\\\n+                                \\\"GenericActivityEnd\\\": {{\\\n+                                    \\\"category\\\": \\\"{:?}\\\",\\\n+                                    \\\"time\\\": {{\\\n+                                        \\\"secs\\\": {},\\\n+                                        \\\"nanos\\\": {}\\\n+                                    }},\\\n+                                    \\\"thread_id\\\": {}\\\n+                                }}\\\n+                            }}\",\n+                            category,\n+                            secs,\n+                            nanos,\n+                            thread_id,\n+                        ).unwrap(),\n+                    QueryCacheHit { query_name, category, time: _ } =>\n+                        write!(file,\n+                            \"{{\\\n+                                \\\"QueryCacheHit\\\": {{\\\n+                                    \\\"query_name\\\": \\\"{}\\\",\\\n+                                    \\\"category\\\": \\\"{:?}\\\",\\\n+                                    \\\"time\\\": {{\\\n+                                        \\\"secs\\\": {},\\\n+                                        \\\"nanos\\\": {}\\\n+                                    }},\\\n+                                    \\\"thread_id\\\": {}\\\n+                                }}\\\n+                            }}\",\n+                            query_name,\n+                            category,\n+                            secs,\n+                            nanos,\n+                            thread_id,\n+                        ).unwrap(),\n+                    QueryCount { query_name, category, count, time: _ } =>\n+                        write!(file,\n+                            \"{{\\\n+                                \\\"QueryCount\\\": {{\\\n+                                    \\\"query_name\\\": \\\"{}\\\",\\\n+                                    \\\"category\\\": \\\"{:?}\\\",\\\n+                                    \\\"count\\\": {},\\\n+                                    \\\"time\\\": {{\\\n+                                        \\\"secs\\\": {},\\\n+                                        \\\"nanos\\\": {}\\\n+                                    }},\\\n+                                    \\\"thread_id\\\": {}\\\n+                                }}\\\n+                            }}\",\n+                            query_name,\n+                            category,\n+                            count,\n+                            secs,\n+                            nanos,\n+                            thread_id,\n+                        ).unwrap(),\n+                    IncrementalLoadResultStart { query_name, time: _ } =>\n+                        write!(file,\n+                            \"{{\\\n+                                \\\"IncrementalLoadResultStart\\\": {{\\\n+                                    \\\"query_name\\\": \\\"{}\\\",\\\n+                                    \\\"time\\\": {{\\\n+                                        \\\"secs\\\": {},\\\n+                                        \\\"nanos\\\": {}\\\n+                                    }},\\\n+                                    \\\"thread_id\\\": {}\\\n+                                }}\\\n+                            }}\",\n+                            query_name,\n+                            secs,\n+                            nanos,\n+                            thread_id,\n+                        ).unwrap(),\n+                    IncrementalLoadResultEnd { query_name, time: _ } =>\n+                        write!(file,\n+                            \"{{\\\n+                                \\\"IncrementalLoadResultEnd\\\": {{\\\n+                                    \\\"query_name\\\": \\\"{}\\\",\\\n+                                    \\\"time\\\": {{\\\n+                                        \\\"secs\\\": {},\\\n+                                        \\\"nanos\\\": {}\\\n+                                    }},\\\n+                                    \\\"thread_id\\\": {}\\\n+                                }}\\\n+                            }}\",\n+                            query_name,\n+                            secs,\n+                            nanos,\n+                            thread_id,\n+                        ).unwrap(),\n+                    QueryBlockedStart { query_name, category, time: _ } =>\n+                        write!(file,\n+                            \"{{\\\n+                                \\\"QueryBlockedStart\\\": {{\\\n+                                    \\\"query_name\\\": \\\"{}\\\",\\\n+                                    \\\"category\\\": \\\"{:?}\\\",\\\n+                                    \\\"time\\\": {{\\\n+                                        \\\"secs\\\": {},\\\n+                                        \\\"nanos\\\": {}\\\n+                                    }},\\\n+                                    \\\"thread_id\\\": {}\\\n+                                }}\\\n+                            }}\",\n+                            query_name,\n+                            category,\n+                            secs,\n+                            nanos,\n+                            thread_id,\n+                        ).unwrap(),\n+                    QueryBlockedEnd { query_name, category, time: _ } =>\n+                        write!(file,\n+                            \"{{\\\n+                                \\\"QueryBlockedEnd\\\": {{\\\n+                                    \\\"query_name\\\": \\\"{}\\\",\\\n+                                    \\\"category\\\": \\\"{:?}\\\",\\\n+                                    \\\"time\\\": {{\\\n+                                        \\\"secs\\\": {},\\\n+                                        \\\"nanos\\\": {}\\\n+                                    }},\\\n+                                    \\\"thread_id\\\": {}\\\n+                                }}\\\n+                            }}\",\n+                            query_name,\n+                            category,\n+                            secs,\n+                            nanos,\n+                            thread_id,\n+                        ).unwrap()\n                 }\n             }\n         }\n \n-        writeln!(lock).unwrap();\n-        writeln!(lock, \"Optimization level: {:?}\", opts.optimize).unwrap();\n-        writeln!(lock, \"Incremental: {}\", if results.incremental { \"on\" } else { \"off\" }).unwrap();\n-    }\n-\n-    pub fn save_results(&self, opts: &Options) {\n-        let results = self.get_results(opts);\n-\n-        let compilation_options =\n-            format!(\"{{ \\\"optimization_level\\\": \\\"{:?}\\\", \\\"incremental\\\": {} }}\",\n-                    results.optimization_level,\n-                    if results.incremental { \"true\" } else { \"false\" });\n-\n-        let mut category_data = String::new();\n-\n-        for (category, data) in &results.categories {\n-            let (hits, total) = data.total_cache_data();\n-            let hit_percent = calculate_percent(hits, total);\n-\n-            category_data.push_str(&format!(\"{{ \\\"category\\\": \\\"{:?}\\\", \\\"time_ms\\\": {}, \\\n-                                                \\\"query_count\\\": {}, \\\"query_hits\\\": {} }}\",\n-                                            category,\n-                                            data.total_time(),\n-                                            total,\n-                                            format!(\"{:.2}\", hit_percent)));\n-        }\n-\n-        let json = format!(\"{{ \\\"category_data\\\": {}, \\\"compilation_options\\\": {} }}\",\n-                        category_data,\n-                        compilation_options);\n-\n-        fs::write(\"self_profiler_results.json\", json).unwrap();\n+        write!(file, \"] }}\").unwrap();\n     }\n }"}, {"sha": "cc1b8916c1074401e87b08cea6b3843bca4f1258", "filename": "src/librustc_driver/lib.rs", "status": "modified", "additions": 1, "deletions": 6, "changes": 7, "blob_url": "https://github.com/rust-lang/rust/blob/fccc84199cdf12de8ae89b7bea5276beb6a67c29/src%2Flibrustc_driver%2Flib.rs", "raw_url": "https://github.com/rust-lang/rust/raw/fccc84199cdf12de8ae89b7bea5276beb6a67c29/src%2Flibrustc_driver%2Flib.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_driver%2Flib.rs?ref=fccc84199cdf12de8ae89b7bea5276beb6a67c29", "patch": "@@ -276,13 +276,8 @@ fn run_compiler_with_pool<'a>(\n                               &control)\n     };\n \n-\n     if sess.opts.debugging_opts.self_profile {\n-        sess.profiler(|p| p.print_results(&sess.opts));\n-    }\n-\n-    if sess.opts.debugging_opts.profile_json {\n-        sess.profiler(|p| p.save_results(&sess.opts));\n+        sess.profiler(|p| p.dump_raw_events(&sess.opts));\n     }\n \n     (result, Some(sess))"}]}