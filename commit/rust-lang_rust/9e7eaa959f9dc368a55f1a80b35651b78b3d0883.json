{"sha": "9e7eaa959f9dc368a55f1a80b35651b78b3d0883", "node_id": "MDY6Q29tbWl0NzI0NzEyOjllN2VhYTk1OWY5ZGMzNjhhNTVmMWE4MGIzNTY1MWI3OGIzZDA4ODM=", "commit": {"author": {"name": "Veetaha", "email": "gerzoh1@gmail.com", "date": "2020-01-28T05:09:13Z"}, "committer": {"name": "Veetaha", "email": "gerzoh1@gmail.com", "date": "2020-02-03T22:00:55Z"}, "message": "ra_syntax: refactored the lexer design as per @matklad and @kiljacken PR review", "tree": {"sha": "4b1f4af14d9898301949fa937219006d671a72ef", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/4b1f4af14d9898301949fa937219006d671a72ef"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/9e7eaa959f9dc368a55f1a80b35651b78b3d0883", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/9e7eaa959f9dc368a55f1a80b35651b78b3d0883", "html_url": "https://github.com/rust-lang/rust/commit/9e7eaa959f9dc368a55f1a80b35651b78b3d0883", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/9e7eaa959f9dc368a55f1a80b35651b78b3d0883/comments", "author": null, "committer": null, "parents": [{"sha": "bf60661aa3e2a77fedb3e1627675842d05538860", "url": "https://api.github.com/repos/rust-lang/rust/commits/bf60661aa3e2a77fedb3e1627675842d05538860", "html_url": "https://github.com/rust-lang/rust/commit/bf60661aa3e2a77fedb3e1627675842d05538860"}], "stats": {"total": 377, "additions": 199, "deletions": 178}, "files": [{"sha": "9a84c1c8875e2afde662cf6235b797fff6922abb", "filename": "crates/ra_ide/src/references/rename.rs", "status": "modified", "additions": 4, "deletions": 2, "changes": 6, "blob_url": "https://github.com/rust-lang/rust/blob/9e7eaa959f9dc368a55f1a80b35651b78b3d0883/crates%2Fra_ide%2Fsrc%2Freferences%2Frename.rs", "raw_url": "https://github.com/rust-lang/rust/raw/9e7eaa959f9dc368a55f1a80b35651b78b3d0883/crates%2Fra_ide%2Fsrc%2Freferences%2Frename.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fra_ide%2Fsrc%2Freferences%2Frename.rs?ref=9e7eaa959f9dc368a55f1a80b35651b78b3d0883", "patch": "@@ -2,7 +2,9 @@\n \n use hir::ModuleSource;\n use ra_db::{RelativePath, RelativePathBuf, SourceDatabase, SourceDatabaseExt};\n-use ra_syntax::{algo::find_node_at_offset, ast, single_token, AstNode, SyntaxKind, SyntaxNode};\n+use ra_syntax::{\n+    algo::find_node_at_offset, ast, lex_single_valid_syntax_kind, AstNode, SyntaxKind, SyntaxNode,\n+};\n use ra_text_edit::TextEdit;\n \n use crate::{\n@@ -17,7 +19,7 @@ pub(crate) fn rename(\n     position: FilePosition,\n     new_name: &str,\n ) -> Option<RangeInfo<SourceChange>> {\n-    match single_token(new_name)?.token.kind {\n+    match lex_single_valid_syntax_kind(new_name)? {\n         SyntaxKind::IDENT | SyntaxKind::UNDERSCORE => (),\n         _ => return None,\n     }"}, {"sha": "c9f42b3ddef91400b569207f394e56c1e34e6e2b", "filename": "crates/ra_mbe/src/subtree_source.rs", "status": "modified", "additions": 2, "deletions": 3, "changes": 5, "blob_url": "https://github.com/rust-lang/rust/blob/9e7eaa959f9dc368a55f1a80b35651b78b3d0883/crates%2Fra_mbe%2Fsrc%2Fsubtree_source.rs", "raw_url": "https://github.com/rust-lang/rust/raw/9e7eaa959f9dc368a55f1a80b35651b78b3d0883/crates%2Fra_mbe%2Fsrc%2Fsubtree_source.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fra_mbe%2Fsrc%2Fsubtree_source.rs?ref=9e7eaa959f9dc368a55f1a80b35651b78b3d0883", "patch": "@@ -1,7 +1,7 @@\n //! FIXME: write short doc here\n \n use ra_parser::{Token, TokenSource};\n-use ra_syntax::{single_token, SmolStr, SyntaxKind, SyntaxKind::*, T};\n+use ra_syntax::{lex_single_valid_syntax_kind, SmolStr, SyntaxKind, SyntaxKind::*, T};\n use std::cell::{Cell, Ref, RefCell};\n use tt::buffer::{Cursor, TokenBuffer};\n \n@@ -129,8 +129,7 @@ fn convert_delim(d: Option<tt::DelimiterKind>, closing: bool) -> TtToken {\n }\n \n fn convert_literal(l: &tt::Literal) -> TtToken {\n-    let kind = single_token(&l.text)\n-        .map(|parsed| parsed.token.kind)\n+    let kind = lex_single_valid_syntax_kind(&l.text)\n         .filter(|kind| kind.is_literal())\n         .unwrap_or_else(|| match l.text.as_ref() {\n             \"true\" => T![true],"}, {"sha": "f8f4b64c1e459e97dcacbc0fcd5861527b6f2048", "filename": "crates/ra_syntax/src/lib.rs", "status": "modified", "additions": 3, "deletions": 1, "changes": 4, "blob_url": "https://github.com/rust-lang/rust/blob/9e7eaa959f9dc368a55f1a80b35651b78b3d0883/crates%2Fra_syntax%2Fsrc%2Flib.rs", "raw_url": "https://github.com/rust-lang/rust/raw/9e7eaa959f9dc368a55f1a80b35651b78b3d0883/crates%2Fra_syntax%2Fsrc%2Flib.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fra_syntax%2Fsrc%2Flib.rs?ref=9e7eaa959f9dc368a55f1a80b35651b78b3d0883", "patch": "@@ -41,7 +41,9 @@ use crate::syntax_node::GreenNode;\n pub use crate::{\n     algo::InsertPosition,\n     ast::{AstNode, AstToken},\n-    parsing::{first_token, single_token, tokenize, tokenize_append, Token, TokenizeError},\n+    parsing::{\n+        lex_single_syntax_kind, lex_single_valid_syntax_kind, tokenize, Token, TokenizeError,\n+    },\n     ptr::{AstPtr, SyntaxNodePtr},\n     syntax_error::{Location, SyntaxError, SyntaxErrorKind},\n     syntax_node::{"}, {"sha": "e5eb808500de812a871b430db24d9295d50323b9", "filename": "crates/ra_syntax/src/parsing.rs", "status": "modified", "additions": 9, "deletions": 3, "changes": 12, "blob_url": "https://github.com/rust-lang/rust/blob/9e7eaa959f9dc368a55f1a80b35651b78b3d0883/crates%2Fra_syntax%2Fsrc%2Fparsing.rs", "raw_url": "https://github.com/rust-lang/rust/raw/9e7eaa959f9dc368a55f1a80b35651b78b3d0883/crates%2Fra_syntax%2Fsrc%2Fparsing.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fra_syntax%2Fsrc%2Fparsing.rs?ref=9e7eaa959f9dc368a55f1a80b35651b78b3d0883", "patch": "@@ -15,9 +15,15 @@ pub use lexer::*;\n pub(crate) use self::reparsing::incremental_reparse;\n \n pub(crate) fn parse_text(text: &str) -> (GreenNode, Vec<SyntaxError>) {\n-    let ParsedTokens { tokens, errors } = tokenize(&text);\n+    let (tokens, lexer_errors) = tokenize(&text);\n+\n     let mut token_source = TextTokenSource::new(text, &tokens);\n-    let mut tree_sink = TextTreeSink::new(text, &tokens, errors);\n+    let mut tree_sink = TextTreeSink::new(text, &tokens);\n+\n     ra_parser::parse(&mut token_source, &mut tree_sink);\n-    tree_sink.finish()\n+\n+    let (tree, mut parser_errors) = tree_sink.finish();\n+    parser_errors.extend(lexer_errors);\n+\n+    (tree, parser_errors)\n }"}, {"sha": "55755be18dc9ca43f3c318381214601dfdf92375", "filename": "crates/ra_syntax/src/parsing/lexer.rs", "status": "modified", "additions": 165, "deletions": 148, "changes": 313, "blob_url": "https://github.com/rust-lang/rust/blob/9e7eaa959f9dc368a55f1a80b35651b78b3d0883/crates%2Fra_syntax%2Fsrc%2Fparsing%2Flexer.rs", "raw_url": "https://github.com/rust-lang/rust/raw/9e7eaa959f9dc368a55f1a80b35651b78b3d0883/crates%2Fra_syntax%2Fsrc%2Fparsing%2Flexer.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fra_syntax%2Fsrc%2Fparsing%2Flexer.rs?ref=9e7eaa959f9dc368a55f1a80b35651b78b3d0883", "patch": "@@ -16,91 +16,98 @@ pub struct Token {\n     pub len: TextUnit,\n }\n \n-/// Represents the result of parsing one token. Beware that the token may be malformed.\n-#[derive(Debug)]\n-pub struct ParsedToken {\n-    /// Parsed token.\n-    pub token: Token,\n-    /// If error is present then parsed token is malformed.\n-    pub error: Option<SyntaxError>,\n-}\n-\n-#[derive(Debug, Default)]\n-/// Represents the result of parsing source code of Rust language.\n-pub struct ParsedTokens {\n-    /// Parsed tokens in order they appear in source code.\n-    pub tokens: Vec<Token>,\n-    /// Collection of all occured tokenization errors.\n-    /// In general `self.errors.len() <= self.tokens.len()`\n-    pub errors: Vec<SyntaxError>,\n-}\n-impl ParsedTokens {\n-    /// Append `token` and `error` (if pressent) to the result.\n-    pub fn push(&mut self, ParsedToken { token, error }: ParsedToken) {\n-        self.tokens.push(token);\n-        if let Some(error) = error {\n-            self.errors.push(error)\n-        }\n-    }\n-}\n-\n-/// Same as `tokenize_append()`, just a shortcut for creating `ParsedTokens`\n-/// and returning the result the usual way.\n-pub fn tokenize(text: &str) -> ParsedTokens {\n-    let mut parsed = ParsedTokens::default();\n-    tokenize_append(text, &mut parsed);\n-    parsed\n-}\n-\n /// Break a string up into its component tokens.\n-/// Writes to `ParsedTokens` which are basically a pair `(Vec<Token>, Vec<SyntaxError>)`.\n /// Beware that it checks for shebang first and its length contributes to resulting\n /// tokens offsets.\n-pub fn tokenize_append(text: &str, parsed: &mut ParsedTokens) {\n+pub fn tokenize(text: &str) -> (Vec<Token>, Vec<SyntaxError>) {\n     // non-empty string is a precondtion of `rustc_lexer::strip_shebang()`.\n     if text.is_empty() {\n-        return;\n+        return Default::default();\n     }\n \n+    let mut tokens = Vec::new();\n+    let mut errors = Vec::new();\n+\n     let mut offset: usize = rustc_lexer::strip_shebang(text)\n         .map(|shebang_len| {\n-            parsed.tokens.push(Token { kind: SHEBANG, len: TextUnit::from_usize(shebang_len) });\n+            tokens.push(Token { kind: SHEBANG, len: TextUnit::from_usize(shebang_len) });\n             shebang_len\n         })\n         .unwrap_or(0);\n \n     let text_without_shebang = &text[offset..];\n \n     for rustc_token in rustc_lexer::tokenize(text_without_shebang) {\n-        parsed.push(rustc_token_to_parsed_token(&rustc_token, text, TextUnit::from_usize(offset)));\n+        let token_len = TextUnit::from_usize(rustc_token.len);\n+        let token_range = TextRange::offset_len(TextUnit::from_usize(offset), token_len);\n+\n+        let (syntax_kind, error) =\n+            rustc_token_kind_to_syntax_kind(&rustc_token.kind, &text[token_range]);\n+\n+        tokens.push(Token { kind: syntax_kind, len: token_len });\n+\n+        if let Some(error) = error {\n+            errors.push(SyntaxError::new(SyntaxErrorKind::TokenizeError(error), token_range));\n+        }\n+\n         offset += rustc_token.len;\n     }\n+\n+    (tokens, errors)\n }\n \n-/// Returns the first encountered token at the beginning of the string.\n-/// If the string contains zero or *two or more tokens* returns `None`.\n+/// Returns `SyntaxKind` and `Option<SyntaxError>` of the first token\n+/// encountered at the beginning of the string.\n+///\n+/// Returns `None` if the string contains zero *or two or more* tokens.\n+/// The token is malformed if the returned error is not `None`.\n+///\n+/// Beware that unescape errors are not checked at tokenization time.\n+pub fn lex_single_syntax_kind(text: &str) -> Option<(SyntaxKind, Option<SyntaxError>)> {\n+    first_token(text)\n+        .filter(|(token, _)| token.len.to_usize() == text.len())\n+        .map(|(token, error)| (token.kind, error))\n+}\n+\n+/// The same as `single_syntax_kind()` but returns only `SyntaxKind` and\n+/// returns `None` if any tokenization error occured.\n ///\n-/// The main difference between `first_token()` and `single_token()` is that\n-/// the latter returns `None` if the string contains more than one token.\n-pub fn single_token(text: &str) -> Option<ParsedToken> {\n-    first_token(text).filter(|parsed| parsed.token.len.to_usize() == text.len())\n+/// Beware that unescape errors are not checked at tokenization time.\n+pub fn lex_single_valid_syntax_kind(text: &str) -> Option<SyntaxKind> {\n+    first_token(text)\n+        .filter(|(token, error)| !error.is_some() && token.len.to_usize() == text.len())\n+        .map(|(token, _error)| token.kind)\n }\n \n /// Returns the first encountered token at the beginning of the string.\n-/// If the string contains zero tokens returns `None`.\n ///\n-/// The main difference between `first_token() and single_token()` is that\n-/// the latter returns `None` if the string contains more than one token.\n-pub fn first_token(text: &str) -> Option<ParsedToken> {\n+/// Returns `None` if the string contains zero tokens or if the token was parsed\n+/// with an error.\n+///\n+/// Beware that unescape errors are not checked at tokenization time.\n+fn first_token(text: &str) -> Option<(Token, Option<SyntaxError>)> {\n     // non-empty string is a precondtion of `rustc_lexer::first_token()`.\n     if text.is_empty() {\n-        None\n-    } else {\n-        let rustc_token = rustc_lexer::first_token(text);\n-        Some(rustc_token_to_parsed_token(&rustc_token, text, TextUnit::from(0)))\n+        return None;\n     }\n+\n+    let rustc_token = rustc_lexer::first_token(text);\n+    let (syntax_kind, error) = rustc_token_kind_to_syntax_kind(&rustc_token.kind, text);\n+\n+    let token = Token { kind: syntax_kind, len: TextUnit::from_usize(rustc_token.len) };\n+    let error = error.map(|error| {\n+        SyntaxError::new(\n+            SyntaxErrorKind::TokenizeError(error),\n+            TextRange::from_to(TextUnit::from(0), TextUnit::of_str(text)),\n+        )\n+    });\n+\n+    Some((token, error))\n }\n \n+// FIXME: simplify TokenizeError to `SyntaxError(String, TextRange)` as per @matklad advice:\n+// https://github.com/rust-analyzer/rust-analyzer/pull/2911/files#r371175067\n+\n /// Describes the values of `SyntaxErrorKind::TokenizeError` enum variant.\n /// It describes all the types of errors that may happen during the tokenization\n /// of Rust source.\n@@ -136,122 +143,132 @@ pub enum TokenizeError {\n     LifetimeStartsWithNumber,\n }\n \n-/// Mapper function that converts `rustc_lexer::Token` with some additional context\n-/// to `ParsedToken`\n-fn rustc_token_to_parsed_token(\n-    rustc_token: &rustc_lexer::Token,\n-    text: &str,\n-    token_start_offset: TextUnit,\n-) -> ParsedToken {\n+fn rustc_token_kind_to_syntax_kind(\n+    rustc_token_kind: &rustc_lexer::TokenKind,\n+    token_text: &str,\n+) -> (SyntaxKind, Option<TokenizeError>) {\n+    // A note on an intended tradeoff:\n     // We drop some useful infromation here (see patterns with double dots `..`)\n     // Storing that info in `SyntaxKind` is not possible due to its layout requirements of\n-    // being `u16` that come from `rowan::SyntaxKind` type and changes to `rowan::SyntaxKind`\n-    // would mean hell of a rewrite\n+    // being `u16` that come from `rowan::SyntaxKind`.\n \n-    let token_range =\n-        TextRange::offset_len(token_start_offset, TextUnit::from_usize(rustc_token.len));\n-\n-    let token_text = &text[token_range];\n-\n-    let (syntax_kind, error) = {\n+    let syntax_kind = {\n         use rustc_lexer::TokenKind as TK;\n         use TokenizeError as TE;\n \n-        match rustc_token.kind {\n-            TK::LineComment => ok(COMMENT),\n-            TK::BlockComment { terminated } => {\n-                ok_if(terminated, COMMENT, TE::UnterminatedBlockComment)\n+        match rustc_token_kind {\n+            TK::LineComment => COMMENT,\n+\n+            TK::BlockComment { terminated: true } => COMMENT,\n+            TK::BlockComment { terminated: false } => {\n+                return (COMMENT, Some(TE::UnterminatedBlockComment));\n             }\n-            TK::Whitespace => ok(WHITESPACE),\n-            TK::Ident => ok(if token_text == \"_\" {\n-                UNDERSCORE\n-            } else {\n-                SyntaxKind::from_keyword(token_text).unwrap_or(IDENT)\n-            }),\n-            TK::RawIdent => ok(IDENT),\n-            TK::Literal { kind, .. } => match_literal_kind(&kind),\n-            TK::Lifetime { starts_with_number } => {\n-                ok_if(!starts_with_number, LIFETIME, TE::LifetimeStartsWithNumber)\n+\n+            TK::Whitespace => WHITESPACE,\n+\n+            TK::Ident => {\n+                if token_text == \"_\" {\n+                    UNDERSCORE\n+                } else {\n+                    SyntaxKind::from_keyword(token_text).unwrap_or(IDENT)\n+                }\n             }\n-            TK::Semi => ok(SEMI),\n-            TK::Comma => ok(COMMA),\n-            TK::Dot => ok(DOT),\n-            TK::OpenParen => ok(L_PAREN),\n-            TK::CloseParen => ok(R_PAREN),\n-            TK::OpenBrace => ok(L_CURLY),\n-            TK::CloseBrace => ok(R_CURLY),\n-            TK::OpenBracket => ok(L_BRACK),\n-            TK::CloseBracket => ok(R_BRACK),\n-            TK::At => ok(AT),\n-            TK::Pound => ok(POUND),\n-            TK::Tilde => ok(TILDE),\n-            TK::Question => ok(QUESTION),\n-            TK::Colon => ok(COLON),\n-            TK::Dollar => ok(DOLLAR),\n-            TK::Eq => ok(EQ),\n-            TK::Not => ok(EXCL),\n-            TK::Lt => ok(L_ANGLE),\n-            TK::Gt => ok(R_ANGLE),\n-            TK::Minus => ok(MINUS),\n-            TK::And => ok(AMP),\n-            TK::Or => ok(PIPE),\n-            TK::Plus => ok(PLUS),\n-            TK::Star => ok(STAR),\n-            TK::Slash => ok(SLASH),\n-            TK::Caret => ok(CARET),\n-            TK::Percent => ok(PERCENT),\n-            TK::Unknown => ok(ERROR),\n-        }\n-    };\n \n-    return ParsedToken {\n-        token: Token { kind: syntax_kind, len: token_range.len() },\n-        error: error\n-            .map(|error| SyntaxError::new(SyntaxErrorKind::TokenizeError(error), token_range)),\n+            TK::RawIdent => IDENT,\n+            TK::Literal { kind, .. } => return match_literal_kind(&kind),\n+\n+            TK::Lifetime { starts_with_number: false } => LIFETIME,\n+            TK::Lifetime { starts_with_number: true } => {\n+                return (LIFETIME, Some(TE::LifetimeStartsWithNumber))\n+            }\n+\n+            TK::Semi => SEMI,\n+            TK::Comma => COMMA,\n+            TK::Dot => DOT,\n+            TK::OpenParen => L_PAREN,\n+            TK::CloseParen => R_PAREN,\n+            TK::OpenBrace => L_CURLY,\n+            TK::CloseBrace => R_CURLY,\n+            TK::OpenBracket => L_BRACK,\n+            TK::CloseBracket => R_BRACK,\n+            TK::At => AT,\n+            TK::Pound => POUND,\n+            TK::Tilde => TILDE,\n+            TK::Question => QUESTION,\n+            TK::Colon => COLON,\n+            TK::Dollar => DOLLAR,\n+            TK::Eq => EQ,\n+            TK::Not => EXCL,\n+            TK::Lt => L_ANGLE,\n+            TK::Gt => R_ANGLE,\n+            TK::Minus => MINUS,\n+            TK::And => AMP,\n+            TK::Or => PIPE,\n+            TK::Plus => PLUS,\n+            TK::Star => STAR,\n+            TK::Slash => SLASH,\n+            TK::Caret => CARET,\n+            TK::Percent => PERCENT,\n+            TK::Unknown => ERROR,\n+        }\n     };\n \n-    type ParsedSyntaxKind = (SyntaxKind, Option<TokenizeError>);\n+    return (syntax_kind, None);\n \n-    fn match_literal_kind(kind: &rustc_lexer::LiteralKind) -> ParsedSyntaxKind {\n+    fn match_literal_kind(kind: &rustc_lexer::LiteralKind) -> (SyntaxKind, Option<TokenizeError>) {\n         use rustc_lexer::LiteralKind as LK;\n         use TokenizeError as TE;\n \n-        match *kind {\n-            LK::Int { empty_int, .. } => ok_if(!empty_int, INT_NUMBER, TE::EmptyInt),\n-            LK::Float { empty_exponent, .. } => {\n-                ok_if(!empty_exponent, FLOAT_NUMBER, TE::EmptyExponent)\n+        #[rustfmt::skip]\n+        let syntax_kind = match *kind {\n+            LK::Int { empty_int: false, .. } => INT_NUMBER,\n+            LK::Int { empty_int: true, .. } => {\n+                return (INT_NUMBER, Some(TE::EmptyInt))\n+            }\n+\n+            LK::Float { empty_exponent: false, .. } => FLOAT_NUMBER,\n+            LK::Float { empty_exponent: true, .. } => {\n+                return (FLOAT_NUMBER, Some(TE::EmptyExponent))\n+            }\n+\n+            LK::Char { terminated: true } => CHAR,\n+            LK::Char { terminated: false } => {\n+                return (CHAR, Some(TE::UnterminatedChar))\n+            }\n+\n+            LK::Byte { terminated: true } => BYTE,\n+            LK::Byte { terminated: false } => {\n+                return (BYTE, Some(TE::UnterminatedByte))\n             }\n-            LK::Char { terminated } => ok_if(terminated, CHAR, TE::UnterminatedChar),\n-            LK::Byte { terminated } => ok_if(terminated, BYTE, TE::UnterminatedByte),\n-            LK::Str { terminated } => ok_if(terminated, STRING, TE::UnterminatedString),\n-            LK::ByteStr { terminated } => {\n-                ok_if(terminated, BYTE_STRING, TE::UnterminatedByteString)\n+\n+            LK::Str { terminated: true } => STRING,\n+            LK::Str { terminated: false } => {\n+                return (STRING, Some(TE::UnterminatedString))\n+            }\n+\n+\n+            LK::ByteStr { terminated: true } => BYTE_STRING,\n+            LK::ByteStr { terminated: false } => {\n+                return (BYTE_STRING, Some(TE::UnterminatedByteString))\n             }\n \n-            LK::RawStr { started: true, terminated, .. } => {\n-                ok_if(terminated, RAW_STRING, TE::UnterminatedRawString)\n+            LK::RawStr { started: true, terminated: true, .. } => RAW_STRING,\n+            LK::RawStr { started: true, terminated: false, .. } => {\n+                return (RAW_STRING, Some(TE::UnterminatedRawString))\n+            }\n+            LK::RawStr { started: false, .. } => {\n+                return (RAW_STRING, Some(TE::UnstartedRawString))\n             }\n-            LK::RawStr { started: false, .. } => err(RAW_STRING, TE::UnstartedRawString),\n \n-            LK::RawByteStr { started: true, terminated, .. } => {\n-                ok_if(terminated, RAW_BYTE_STRING, TE::UnterminatedRawByteString)\n+            LK::RawByteStr { started: true, terminated: true, .. } => RAW_BYTE_STRING,\n+            LK::RawByteStr { started: true, terminated: false, .. } => {\n+                return (RAW_BYTE_STRING, Some(TE::UnterminatedRawByteString))\n             }\n             LK::RawByteStr { started: false, .. } => {\n-                err(RAW_BYTE_STRING, TE::UnstartedRawByteString)\n+                return (RAW_BYTE_STRING, Some(TE::UnstartedRawByteString))\n             }\n-        }\n-    }\n-    const fn ok(syntax_kind: SyntaxKind) -> ParsedSyntaxKind {\n+        };\n+\n         (syntax_kind, None)\n     }\n-    const fn err(syntax_kind: SyntaxKind, error: TokenizeError) -> ParsedSyntaxKind {\n-        (syntax_kind, Some(error))\n-    }\n-    fn ok_if(cond: bool, syntax_kind: SyntaxKind, error: TokenizeError) -> ParsedSyntaxKind {\n-        if cond {\n-            ok(syntax_kind)\n-        } else {\n-            err(syntax_kind, error)\n-        }\n-    }\n }"}, {"sha": "1f351e9fc8ff9c1750fb7aaf67f8c7cbedda436a", "filename": "crates/ra_syntax/src/parsing/reparsing.rs", "status": "modified", "additions": 13, "deletions": 12, "changes": 25, "blob_url": "https://github.com/rust-lang/rust/blob/9e7eaa959f9dc368a55f1a80b35651b78b3d0883/crates%2Fra_syntax%2Fsrc%2Fparsing%2Freparsing.rs", "raw_url": "https://github.com/rust-lang/rust/raw/9e7eaa959f9dc368a55f1a80b35651b78b3d0883/crates%2Fra_syntax%2Fsrc%2Fparsing%2Freparsing.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fra_syntax%2Fsrc%2Fparsing%2Freparsing.rs?ref=9e7eaa959f9dc368a55f1a80b35651b78b3d0883", "patch": "@@ -12,7 +12,7 @@ use ra_text_edit::AtomTextEdit;\n use crate::{\n     algo,\n     parsing::{\n-        lexer::{single_token, tokenize, ParsedTokens, Token},\n+        lexer::{lex_single_syntax_kind, tokenize, Token},\n         text_token_source::TextTokenSource,\n         text_tree_sink::TextTreeSink,\n     },\n@@ -54,7 +54,7 @@ fn reparse_token<'node>(\n             }\n \n             let mut new_text = get_text_after_edit(prev_token.clone().into(), &edit);\n-            let new_token_kind = single_token(&new_text)?.token.kind;\n+            let (new_token_kind, _error) = lex_single_syntax_kind(&new_text)?;\n \n             if new_token_kind != prev_token_kind\n                 || (new_token_kind == IDENT && is_contextual_kw(&new_text))\n@@ -67,8 +67,8 @@ fn reparse_token<'node>(\n             // `b` no longer remains an identifier, but becomes a part of byte string literal\n             if let Some(next_char) = root.text().char_at(prev_token.text_range().end()) {\n                 new_text.push(next_char);\n-                let token_with_next_char = single_token(&new_text);\n-                if token_with_next_char.is_some() {\n+                let token_with_next_char = lex_single_syntax_kind(&new_text);\n+                if let Some((_kind, _error)) = token_with_next_char {\n                     return None;\n                 }\n                 new_text.pop();\n@@ -88,23 +88,26 @@ fn reparse_block<'node>(\n ) -> Option<(GreenNode, Vec<SyntaxError>, TextRange)> {\n     let (node, reparser) = find_reparsable_node(root, edit.delete)?;\n     let text = get_text_after_edit(node.clone().into(), &edit);\n-    let ParsedTokens { tokens, errors } = tokenize(&text);\n+\n+    let (tokens, new_lexer_errors) = tokenize(&text);\n     if !is_balanced(&tokens) {\n         return None;\n     }\n+\n     let mut token_source = TextTokenSource::new(&text, &tokens);\n-    let mut tree_sink = TextTreeSink::new(&text, &tokens, errors);\n+    let mut tree_sink = TextTreeSink::new(&text, &tokens);\n     reparser.parse(&mut token_source, &mut tree_sink);\n-    let (green, new_errors) = tree_sink.finish();\n-    Some((node.replace_with(green), new_errors, node.text_range()))\n+\n+    let (green, mut new_parser_errors) = tree_sink.finish();\n+    new_parser_errors.extend(new_lexer_errors);\n+\n+    Some((node.replace_with(green), new_parser_errors, node.text_range()))\n }\n \n fn get_text_after_edit(element: SyntaxElement, edit: &AtomTextEdit) -> String {\n     let edit =\n         AtomTextEdit::replace(edit.delete - element.text_range().start(), edit.insert.clone());\n \n-    // Note: we could move this match to a method or even further: use enum_dispatch crate\n-    // https://crates.io/crates/enum_dispatch\n     let text = match element {\n         NodeOrToken::Token(token) => token.text().to_string(),\n         NodeOrToken::Node(node) => node.text().to_string(),\n@@ -122,8 +125,6 @@ fn is_contextual_kw(text: &str) -> bool {\n fn find_reparsable_node(node: &SyntaxNode, range: TextRange) -> Option<(SyntaxNode, Reparser)> {\n     let node = algo::find_covering_element(node, range);\n \n-    // Note: we could move this match to a method or even further: use enum_dispatch crate\n-    // https://crates.io/crates/enum_dispatch\n     let mut ancestors = match node {\n         NodeOrToken::Token(it) => it.parent().ancestors(),\n         NodeOrToken::Node(it) => it.ancestors(),"}, {"sha": "dd202601d9ed631c0a55cf924165c05a0d677081", "filename": "crates/ra_syntax/src/parsing/text_tree_sink.rs", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/rust-lang/rust/blob/9e7eaa959f9dc368a55f1a80b35651b78b3d0883/crates%2Fra_syntax%2Fsrc%2Fparsing%2Ftext_tree_sink.rs", "raw_url": "https://github.com/rust-lang/rust/raw/9e7eaa959f9dc368a55f1a80b35651b78b3d0883/crates%2Fra_syntax%2Fsrc%2Fparsing%2Ftext_tree_sink.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fra_syntax%2Fsrc%2Fparsing%2Ftext_tree_sink.rs?ref=9e7eaa959f9dc368a55f1a80b35651b78b3d0883", "patch": "@@ -92,14 +92,14 @@ impl<'a> TreeSink for TextTreeSink<'a> {\n }\n \n impl<'a> TextTreeSink<'a> {\n-    pub(super) fn new(text: &'a str, tokens: &'a [Token], errors: Vec<SyntaxError>) -> Self {\n+    pub(super) fn new(text: &'a str, tokens: &'a [Token]) -> Self {\n         Self {\n             text,\n             tokens,\n             text_pos: 0.into(),\n             token_pos: 0,\n             state: State::PendingStart,\n-            inner: SyntaxTreeBuilder::new(errors),\n+            inner: SyntaxTreeBuilder::default(),\n         }\n     }\n "}, {"sha": "7c2b18af34e56e46122331340085171f21f8aea2", "filename": "crates/ra_syntax/src/syntax_node.rs", "status": "modified", "additions": 0, "deletions": 6, "changes": 6, "blob_url": "https://github.com/rust-lang/rust/blob/9e7eaa959f9dc368a55f1a80b35651b78b3d0883/crates%2Fra_syntax%2Fsrc%2Fsyntax_node.rs", "raw_url": "https://github.com/rust-lang/rust/raw/9e7eaa959f9dc368a55f1a80b35651b78b3d0883/crates%2Fra_syntax%2Fsrc%2Fsyntax_node.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fra_syntax%2Fsrc%2Fsyntax_node.rs?ref=9e7eaa959f9dc368a55f1a80b35651b78b3d0883", "patch": "@@ -44,12 +44,6 @@ pub struct SyntaxTreeBuilder {\n     inner: GreenNodeBuilder<'static>,\n }\n \n-impl SyntaxTreeBuilder {\n-    pub fn new(errors: Vec<SyntaxError>) -> Self {\n-        Self { errors, inner: GreenNodeBuilder::default() }\n-    }\n-}\n-\n impl SyntaxTreeBuilder {\n     pub(crate) fn finish_raw(self) -> (GreenNode, Vec<SyntaxError>) {\n         let green = self.inner.finish();"}, {"sha": "f79dc4f933ef1e98948866a580b36db0e6e5f393", "filename": "crates/ra_syntax/src/tests.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/9e7eaa959f9dc368a55f1a80b35651b78b3d0883/crates%2Fra_syntax%2Fsrc%2Ftests.rs", "raw_url": "https://github.com/rust-lang/rust/raw/9e7eaa959f9dc368a55f1a80b35651b78b3d0883/crates%2Fra_syntax%2Fsrc%2Ftests.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fra_syntax%2Fsrc%2Ftests.rs?ref=9e7eaa959f9dc368a55f1a80b35651b78b3d0883", "patch": "@@ -11,7 +11,7 @@ use crate::{fuzz, SourceFile};\n fn lexer_tests() {\n     dir_tests(&test_data_dir(), &[\"lexer\"], |text, _| {\n         // FIXME: add tests for errors (their format is up to discussion)\n-        let tokens = crate::tokenize(text).tokens;\n+        let (tokens, _errors) = crate::tokenize(text);\n         dump_tokens(&tokens, text)\n     })\n }"}]}