{"sha": "e009cdc8d9f155e2a5e99202a433b426f0ac2ee4", "node_id": "C_kwDOAAsO6NoAKGUwMDljZGM4ZDlmMTU1ZTJhNWU5OTIwMmE0MzNiNDI2ZjBhYzJlZTQ", "commit": {"author": {"name": "Amos Wenger", "email": "amoswenger@gmail.com", "date": "2022-07-20T16:32:27Z"}, "committer": {"name": "Amos Wenger", "email": "amoswenger@gmail.com", "date": "2022-07-20T17:18:39Z"}, "message": "Move token_stream to separate module", "tree": {"sha": "df99ec6d416c60400dec48d3a963e97cccf01d38", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/df99ec6d416c60400dec48d3a963e97cccf01d38"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/e009cdc8d9f155e2a5e99202a433b426f0ac2ee4", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/e009cdc8d9f155e2a5e99202a433b426f0ac2ee4", "html_url": "https://github.com/rust-lang/rust/commit/e009cdc8d9f155e2a5e99202a433b426f0ac2ee4", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/e009cdc8d9f155e2a5e99202a433b426f0ac2ee4/comments", "author": {"login": "fasterthanlime", "id": 7998310, "node_id": "MDQ6VXNlcjc5OTgzMTA=", "avatar_url": "https://avatars.githubusercontent.com/u/7998310?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fasterthanlime", "html_url": "https://github.com/fasterthanlime", "followers_url": "https://api.github.com/users/fasterthanlime/followers", "following_url": "https://api.github.com/users/fasterthanlime/following{/other_user}", "gists_url": "https://api.github.com/users/fasterthanlime/gists{/gist_id}", "starred_url": "https://api.github.com/users/fasterthanlime/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fasterthanlime/subscriptions", "organizations_url": "https://api.github.com/users/fasterthanlime/orgs", "repos_url": "https://api.github.com/users/fasterthanlime/repos", "events_url": "https://api.github.com/users/fasterthanlime/events{/privacy}", "received_events_url": "https://api.github.com/users/fasterthanlime/received_events", "type": "User", "site_admin": false}, "committer": {"login": "fasterthanlime", "id": 7998310, "node_id": "MDQ6VXNlcjc5OTgzMTA=", "avatar_url": "https://avatars.githubusercontent.com/u/7998310?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fasterthanlime", "html_url": "https://github.com/fasterthanlime", "followers_url": "https://api.github.com/users/fasterthanlime/followers", "following_url": "https://api.github.com/users/fasterthanlime/following{/other_user}", "gists_url": "https://api.github.com/users/fasterthanlime/gists{/gist_id}", "starred_url": "https://api.github.com/users/fasterthanlime/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fasterthanlime/subscriptions", "organizations_url": "https://api.github.com/users/fasterthanlime/orgs", "repos_url": "https://api.github.com/users/fasterthanlime/repos", "events_url": "https://api.github.com/users/fasterthanlime/events{/privacy}", "received_events_url": "https://api.github.com/users/fasterthanlime/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "315b0a7254974f1a2ef9777944531cdbca15be7b", "url": "https://api.github.com/repos/rust-lang/rust/commits/315b0a7254974f1a2ef9777944531cdbca15be7b", "html_url": "https://github.com/rust-lang/rust/commit/315b0a7254974f1a2ef9777944531cdbca15be7b"}], "stats": {"total": 412, "additions": 188, "deletions": 224}, "files": [{"sha": "4c9d3364ba7f5dc00cb6b90cf6bc70e7a61772a1", "filename": "crates/proc-macro-srv/src/abis/abi_sysroot/ra_server.rs", "status": "modified", "additions": 6, "deletions": 224, "changes": 230, "blob_url": "https://github.com/rust-lang/rust/blob/e009cdc8d9f155e2a5e99202a433b426f0ac2ee4/crates%2Fproc-macro-srv%2Fsrc%2Fabis%2Fabi_sysroot%2Fra_server.rs", "raw_url": "https://github.com/rust-lang/rust/raw/e009cdc8d9f155e2a5e99202a433b426f0ac2ee4/crates%2Fproc-macro-srv%2Fsrc%2Fabis%2Fabi_sysroot%2Fra_server.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fproc-macro-srv%2Fsrc%2Fabis%2Fabi_sysroot%2Fra_server.rs?ref=e009cdc8d9f155e2a5e99202a433b426f0ac2ee4", "patch": "@@ -10,11 +10,14 @@\n \n use super::proc_macro::bridge::{self, server};\n \n+mod token_stream;\n+pub use token_stream::*;\n+\n+use std::ascii;\n use std::collections::HashMap;\n use std::hash::Hash;\n use std::iter::FromIterator;\n use std::ops::Bound;\n-use std::{ascii, vec::IntoIter};\n \n type Group = tt::Subtree;\n type TokenTree = tt::TokenTree;\n@@ -23,80 +26,6 @@ type Spacing = tt::Spacing;\n type Literal = tt::Literal;\n type Span = tt::TokenId;\n \n-#[derive(Debug, Default, Clone)]\n-pub struct TokenStream {\n-    pub token_trees: Vec<TokenTree>,\n-}\n-\n-impl TokenStream {\n-    pub fn new() -> Self {\n-        TokenStream::default()\n-    }\n-\n-    pub fn with_subtree(subtree: tt::Subtree) -> Self {\n-        if subtree.delimiter.is_some() {\n-            TokenStream { token_trees: vec![TokenTree::Subtree(subtree)] }\n-        } else {\n-            TokenStream { token_trees: subtree.token_trees }\n-        }\n-    }\n-\n-    pub fn into_subtree(self) -> tt::Subtree {\n-        tt::Subtree { delimiter: None, token_trees: self.token_trees }\n-    }\n-\n-    pub fn is_empty(&self) -> bool {\n-        self.token_trees.is_empty()\n-    }\n-}\n-\n-/// Creates a token stream containing a single token tree.\n-impl From<TokenTree> for TokenStream {\n-    fn from(tree: TokenTree) -> TokenStream {\n-        TokenStream { token_trees: vec![tree] }\n-    }\n-}\n-\n-/// Collects a number of token trees into a single stream.\n-impl FromIterator<TokenTree> for TokenStream {\n-    fn from_iter<I: IntoIterator<Item = TokenTree>>(trees: I) -> Self {\n-        trees.into_iter().map(TokenStream::from).collect()\n-    }\n-}\n-\n-/// A \"flattening\" operation on token streams, collects token trees\n-/// from multiple token streams into a single stream.\n-impl FromIterator<TokenStream> for TokenStream {\n-    fn from_iter<I: IntoIterator<Item = TokenStream>>(streams: I) -> Self {\n-        let mut builder = TokenStreamBuilder::new();\n-        streams.into_iter().for_each(|stream| builder.push(stream));\n-        builder.build()\n-    }\n-}\n-\n-impl Extend<TokenTree> for TokenStream {\n-    fn extend<I: IntoIterator<Item = TokenTree>>(&mut self, trees: I) {\n-        self.extend(trees.into_iter().map(TokenStream::from));\n-    }\n-}\n-\n-impl Extend<TokenStream> for TokenStream {\n-    fn extend<I: IntoIterator<Item = TokenStream>>(&mut self, streams: I) {\n-        for item in streams {\n-            for tkn in item {\n-                match tkn {\n-                    tt::TokenTree::Subtree(subtree) if subtree.delimiter.is_none() => {\n-                        self.token_trees.extend(subtree.token_trees);\n-                    }\n-                    _ => {\n-                        self.token_trees.push(tkn);\n-                    }\n-                }\n-            }\n-        }\n-    }\n-}\n-\n #[derive(Clone)]\n pub struct SourceFile {\n     // FIXME stub\n@@ -158,130 +87,21 @@ impl IdentInterner {\n     }\n }\n \n-pub struct TokenStreamBuilder {\n-    acc: TokenStream,\n-}\n-\n-/// Public implementation details for the `TokenStream` type, such as iterators.\n-pub mod token_stream {\n-    use std::str::FromStr;\n-\n-    use super::{TokenStream, TokenTree};\n-\n-    /// An iterator over `TokenStream`'s `TokenTree`s.\n-    /// The iteration is \"shallow\", e.g., the iterator doesn't recurse into delimited groups,\n-    /// and returns whole groups as token trees.\n-    impl IntoIterator for TokenStream {\n-        type Item = TokenTree;\n-        type IntoIter = super::IntoIter<TokenTree>;\n-\n-        fn into_iter(self) -> Self::IntoIter {\n-            self.token_trees.into_iter()\n-        }\n-    }\n-\n-    type LexError = String;\n-\n-    /// Attempts to break the string into tokens and parse those tokens into a token stream.\n-    /// May fail for a number of reasons, for example, if the string contains unbalanced delimiters\n-    /// or characters not existing in the language.\n-    /// All tokens in the parsed stream get `Span::call_site()` spans.\n-    ///\n-    /// NOTE: some errors may cause panics instead of returning `LexError`. We reserve the right to\n-    /// change these errors into `LexError`s later.\n-    impl FromStr for TokenStream {\n-        type Err = LexError;\n-\n-        fn from_str(src: &str) -> Result<TokenStream, LexError> {\n-            let (subtree, _token_map) =\n-                mbe::parse_to_token_tree(src).ok_or(\"Failed to parse from mbe\")?;\n-\n-            let subtree = subtree_replace_token_ids_with_unspecified(subtree);\n-            Ok(TokenStream::with_subtree(subtree))\n-        }\n-    }\n-\n-    impl ToString for TokenStream {\n-        fn to_string(&self) -> String {\n-            tt::pretty(&self.token_trees)\n-        }\n-    }\n-\n-    fn subtree_replace_token_ids_with_unspecified(subtree: tt::Subtree) -> tt::Subtree {\n-        tt::Subtree {\n-            delimiter: subtree\n-                .delimiter\n-                .map(|d| tt::Delimiter { id: tt::TokenId::unspecified(), ..d }),\n-            token_trees: subtree\n-                .token_trees\n-                .into_iter()\n-                .map(token_tree_replace_token_ids_with_unspecified)\n-                .collect(),\n-        }\n-    }\n-\n-    fn token_tree_replace_token_ids_with_unspecified(tt: tt::TokenTree) -> tt::TokenTree {\n-        match tt {\n-            tt::TokenTree::Leaf(leaf) => {\n-                tt::TokenTree::Leaf(leaf_replace_token_ids_with_unspecified(leaf))\n-            }\n-            tt::TokenTree::Subtree(subtree) => {\n-                tt::TokenTree::Subtree(subtree_replace_token_ids_with_unspecified(subtree))\n-            }\n-        }\n-    }\n-\n-    fn leaf_replace_token_ids_with_unspecified(leaf: tt::Leaf) -> tt::Leaf {\n-        match leaf {\n-            tt::Leaf::Literal(lit) => {\n-                tt::Leaf::Literal(tt::Literal { id: tt::TokenId::unspecified(), ..lit })\n-            }\n-            tt::Leaf::Punct(punct) => {\n-                tt::Leaf::Punct(tt::Punct { id: tt::TokenId::unspecified(), ..punct })\n-            }\n-            tt::Leaf::Ident(ident) => {\n-                tt::Leaf::Ident(tt::Ident { id: tt::TokenId::unspecified(), ..ident })\n-            }\n-        }\n-    }\n-}\n-\n-impl TokenStreamBuilder {\n-    fn new() -> TokenStreamBuilder {\n-        TokenStreamBuilder { acc: TokenStream::new() }\n-    }\n-\n-    fn push(&mut self, stream: TokenStream) {\n-        self.acc.extend(stream.into_iter())\n-    }\n-\n-    fn build(self) -> TokenStream {\n-        self.acc\n-    }\n-}\n-\n pub struct FreeFunctions;\n \n-#[derive(Clone)]\n-pub struct TokenStreamIter {\n-    trees: IntoIter<TokenTree>,\n-}\n-\n #[derive(Default)]\n pub struct RustAnalyzer {\n-    ident_interner: IdentInterner,\n     // FIXME: store span information here.\n }\n \n impl server::Types for RustAnalyzer {\n     type FreeFunctions = FreeFunctions;\n     type TokenStream = TokenStream;\n-    type Ident = IdentId;\n-    type Literal = Literal;\n     type SourceFile = SourceFile;\n+    type MultiSpan = Vec<Span>;\n     type Diagnostic = Diagnostic;\n     type Span = Span;\n-    type MultiSpan = Vec<Span>;\n+    type Symbol = Symbol;\n }\n \n impl server::FreeFunctions for RustAnalyzer {\n@@ -693,46 +513,8 @@ impl server::Server for RustAnalyzer {\n \n #[cfg(test)]\n mod tests {\n-    use super::super::proc_macro::bridge::server::Literal;\n     use super::*;\n \n-    #[test]\n-    fn test_ra_server_literals() {\n-        let mut srv = RustAnalyzer { ident_interner: IdentInterner::default() };\n-        assert_eq!(srv.integer(\"1234\").text, \"1234\");\n-\n-        assert_eq!(srv.typed_integer(\"12\", \"u8\").text, \"12u8\");\n-        assert_eq!(srv.typed_integer(\"255\", \"u16\").text, \"255u16\");\n-        assert_eq!(srv.typed_integer(\"1234\", \"u32\").text, \"1234u32\");\n-        assert_eq!(srv.typed_integer(\"15846685\", \"u64\").text, \"15846685u64\");\n-        assert_eq!(srv.typed_integer(\"15846685258\", \"u128\").text, \"15846685258u128\");\n-        assert_eq!(srv.typed_integer(\"156788984\", \"usize\").text, \"156788984usize\");\n-        assert_eq!(srv.typed_integer(\"127\", \"i8\").text, \"127i8\");\n-        assert_eq!(srv.typed_integer(\"255\", \"i16\").text, \"255i16\");\n-        assert_eq!(srv.typed_integer(\"1234\", \"i32\").text, \"1234i32\");\n-        assert_eq!(srv.typed_integer(\"15846685\", \"i64\").text, \"15846685i64\");\n-        assert_eq!(srv.typed_integer(\"15846685258\", \"i128\").text, \"15846685258i128\");\n-        assert_eq!(srv.float(\"0\").text, \"0.0\");\n-        assert_eq!(srv.float(\"15684.5867\").text, \"15684.5867\");\n-        assert_eq!(srv.f32(\"15684.58\").text, \"15684.58f32\");\n-        assert_eq!(srv.f64(\"15684.58\").text, \"15684.58f64\");\n-\n-        assert_eq!(srv.string(\"hello_world\").text, \"\\\"hello_world\\\"\");\n-        assert_eq!(srv.character('c').text, \"'c'\");\n-        assert_eq!(srv.byte_string(b\"1234586\\x88\").text, \"b\\\"1234586\\\\x88\\\"\");\n-\n-        // u128::max\n-        assert_eq!(\n-            srv.integer(\"340282366920938463463374607431768211455\").text,\n-            \"340282366920938463463374607431768211455\"\n-        );\n-        // i128::min\n-        assert_eq!(\n-            srv.integer(\"-170141183460469231731687303715884105728\").text,\n-            \"-170141183460469231731687303715884105728\"\n-        );\n-    }\n-\n     #[test]\n     fn test_ra_server_to_string() {\n         let s = TokenStream {"}, {"sha": "9a31f2ebf87ccb71864c002debae9106bda8f150", "filename": "crates/proc-macro-srv/src/abis/abi_sysroot/ra_server/token_stream.rs", "status": "added", "additions": 182, "deletions": 0, "changes": 182, "blob_url": "https://github.com/rust-lang/rust/blob/e009cdc8d9f155e2a5e99202a433b426f0ac2ee4/crates%2Fproc-macro-srv%2Fsrc%2Fabis%2Fabi_sysroot%2Fra_server%2Ftoken_stream.rs", "raw_url": "https://github.com/rust-lang/rust/raw/e009cdc8d9f155e2a5e99202a433b426f0ac2ee4/crates%2Fproc-macro-srv%2Fsrc%2Fabis%2Fabi_sysroot%2Fra_server%2Ftoken_stream.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fproc-macro-srv%2Fsrc%2Fabis%2Fabi_sysroot%2Fra_server%2Ftoken_stream.rs?ref=e009cdc8d9f155e2a5e99202a433b426f0ac2ee4", "patch": "@@ -0,0 +1,182 @@\n+use tt::TokenTree;\n+\n+#[derive(Debug, Default, Clone)]\n+pub struct TokenStream {\n+    pub token_trees: Vec<TokenTree>,\n+}\n+\n+impl TokenStream {\n+    pub fn new() -> Self {\n+        TokenStream::default()\n+    }\n+\n+    pub fn with_subtree(subtree: tt::Subtree) -> Self {\n+        if subtree.delimiter.is_some() {\n+            TokenStream { token_trees: vec![TokenTree::Subtree(subtree)] }\n+        } else {\n+            TokenStream { token_trees: subtree.token_trees }\n+        }\n+    }\n+\n+    pub fn into_subtree(self) -> tt::Subtree {\n+        tt::Subtree { delimiter: None, token_trees: self.token_trees }\n+    }\n+\n+    pub fn is_empty(&self) -> bool {\n+        self.token_trees.is_empty()\n+    }\n+}\n+\n+/// Creates a token stream containing a single token tree.\n+impl From<TokenTree> for TokenStream {\n+    fn from(tree: TokenTree) -> TokenStream {\n+        TokenStream { token_trees: vec![tree] }\n+    }\n+}\n+\n+/// Collects a number of token trees into a single stream.\n+impl FromIterator<TokenTree> for TokenStream {\n+    fn from_iter<I: IntoIterator<Item = TokenTree>>(trees: I) -> Self {\n+        trees.into_iter().map(TokenStream::from).collect()\n+    }\n+}\n+\n+/// A \"flattening\" operation on token streams, collects token trees\n+/// from multiple token streams into a single stream.\n+impl FromIterator<TokenStream> for TokenStream {\n+    fn from_iter<I: IntoIterator<Item = TokenStream>>(streams: I) -> Self {\n+        let mut builder = TokenStreamBuilder::new();\n+        streams.into_iter().for_each(|stream| builder.push(stream));\n+        builder.build()\n+    }\n+}\n+\n+impl Extend<TokenTree> for TokenStream {\n+    fn extend<I: IntoIterator<Item = TokenTree>>(&mut self, trees: I) {\n+        self.extend(trees.into_iter().map(TokenStream::from));\n+    }\n+}\n+\n+impl Extend<TokenStream> for TokenStream {\n+    fn extend<I: IntoIterator<Item = TokenStream>>(&mut self, streams: I) {\n+        for item in streams {\n+            for tkn in item {\n+                match tkn {\n+                    tt::TokenTree::Subtree(subtree) if subtree.delimiter.is_none() => {\n+                        self.token_trees.extend(subtree.token_trees);\n+                    }\n+                    _ => {\n+                        self.token_trees.push(tkn);\n+                    }\n+                }\n+            }\n+        }\n+    }\n+}\n+\n+pub struct TokenStreamBuilder {\n+    acc: TokenStream,\n+}\n+\n+/// Public implementation details for the `TokenStream` type, such as iterators.\n+pub mod token_stream {\n+    use std::str::FromStr;\n+\n+    use super::{TokenStream, TokenTree};\n+\n+    /// An iterator over `TokenStream`'s `TokenTree`s.\n+    /// The iteration is \"shallow\", e.g., the iterator doesn't recurse into delimited groups,\n+    /// and returns whole groups as token trees.\n+    impl IntoIterator for TokenStream {\n+        type Item = TokenTree;\n+        type IntoIter = std::vec::IntoIter<TokenTree>;\n+\n+        fn into_iter(self) -> Self::IntoIter {\n+            self.token_trees.into_iter()\n+        }\n+    }\n+\n+    type LexError = String;\n+\n+    /// Attempts to break the string into tokens and parse those tokens into a token stream.\n+    /// May fail for a number of reasons, for example, if the string contains unbalanced delimiters\n+    /// or characters not existing in the language.\n+    /// All tokens in the parsed stream get `Span::call_site()` spans.\n+    ///\n+    /// NOTE: some errors may cause panics instead of returning `LexError`. We reserve the right to\n+    /// change these errors into `LexError`s later.\n+    impl FromStr for TokenStream {\n+        type Err = LexError;\n+\n+        fn from_str(src: &str) -> Result<TokenStream, LexError> {\n+            let (subtree, _token_map) =\n+                mbe::parse_to_token_tree(src).ok_or(\"Failed to parse from mbe\")?;\n+\n+            let subtree = subtree_replace_token_ids_with_unspecified(subtree);\n+            Ok(TokenStream::with_subtree(subtree))\n+        }\n+    }\n+\n+    impl ToString for TokenStream {\n+        fn to_string(&self) -> String {\n+            tt::pretty(&self.token_trees)\n+        }\n+    }\n+\n+    fn subtree_replace_token_ids_with_unspecified(subtree: tt::Subtree) -> tt::Subtree {\n+        tt::Subtree {\n+            delimiter: subtree\n+                .delimiter\n+                .map(|d| tt::Delimiter { id: tt::TokenId::unspecified(), ..d }),\n+            token_trees: subtree\n+                .token_trees\n+                .into_iter()\n+                .map(token_tree_replace_token_ids_with_unspecified)\n+                .collect(),\n+        }\n+    }\n+\n+    fn token_tree_replace_token_ids_with_unspecified(tt: tt::TokenTree) -> tt::TokenTree {\n+        match tt {\n+            tt::TokenTree::Leaf(leaf) => {\n+                tt::TokenTree::Leaf(leaf_replace_token_ids_with_unspecified(leaf))\n+            }\n+            tt::TokenTree::Subtree(subtree) => {\n+                tt::TokenTree::Subtree(subtree_replace_token_ids_with_unspecified(subtree))\n+            }\n+        }\n+    }\n+\n+    fn leaf_replace_token_ids_with_unspecified(leaf: tt::Leaf) -> tt::Leaf {\n+        match leaf {\n+            tt::Leaf::Literal(lit) => {\n+                tt::Leaf::Literal(tt::Literal { id: tt::TokenId::unspecified(), ..lit })\n+            }\n+            tt::Leaf::Punct(punct) => {\n+                tt::Leaf::Punct(tt::Punct { id: tt::TokenId::unspecified(), ..punct })\n+            }\n+            tt::Leaf::Ident(ident) => {\n+                tt::Leaf::Ident(tt::Ident { id: tt::TokenId::unspecified(), ..ident })\n+            }\n+        }\n+    }\n+}\n+\n+impl TokenStreamBuilder {\n+    fn new() -> TokenStreamBuilder {\n+        TokenStreamBuilder { acc: TokenStream::new() }\n+    }\n+\n+    fn push(&mut self, stream: TokenStream) {\n+        self.acc.extend(stream.into_iter())\n+    }\n+\n+    fn build(self) -> TokenStream {\n+        self.acc\n+    }\n+}\n+\n+#[derive(Clone)]\n+pub struct TokenStreamIter {\n+    trees: std::vec::IntoIter<TokenTree>,\n+}"}]}