{"sha": "3f0a2a29414a67d9c52241a94ce373546ca2ddcf", "node_id": "MDY6Q29tbWl0NzI0NzEyOjNmMGEyYTI5NDE0YTY3ZDljNTIyNDFhOTRjZTM3MzU0NmNhMmRkY2Y=", "commit": {"author": {"name": "Ralf Jung", "email": "post@ralfj.de", "date": "2019-04-15T13:36:09Z"}, "committer": {"name": "Ralf Jung", "email": "post@ralfj.de", "date": "2019-04-17T14:02:57Z"}, "message": "rewrite Stacked Borrows Core. this passes stacked-borrows.rs!", "tree": {"sha": "0ac41eb2971dcd77d41849d99190634d46061481", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/0ac41eb2971dcd77d41849d99190634d46061481"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/3f0a2a29414a67d9c52241a94ce373546ca2ddcf", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/3f0a2a29414a67d9c52241a94ce373546ca2ddcf", "html_url": "https://github.com/rust-lang/rust/commit/3f0a2a29414a67d9c52241a94ce373546ca2ddcf", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/3f0a2a29414a67d9c52241a94ce373546ca2ddcf/comments", "author": {"login": "RalfJung", "id": 330628, "node_id": "MDQ6VXNlcjMzMDYyOA==", "avatar_url": "https://avatars.githubusercontent.com/u/330628?v=4", "gravatar_id": "", "url": "https://api.github.com/users/RalfJung", "html_url": "https://github.com/RalfJung", "followers_url": "https://api.github.com/users/RalfJung/followers", "following_url": "https://api.github.com/users/RalfJung/following{/other_user}", "gists_url": "https://api.github.com/users/RalfJung/gists{/gist_id}", "starred_url": "https://api.github.com/users/RalfJung/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/RalfJung/subscriptions", "organizations_url": "https://api.github.com/users/RalfJung/orgs", "repos_url": "https://api.github.com/users/RalfJung/repos", "events_url": "https://api.github.com/users/RalfJung/events{/privacy}", "received_events_url": "https://api.github.com/users/RalfJung/received_events", "type": "User", "site_admin": false}, "committer": {"login": "RalfJung", "id": 330628, "node_id": "MDQ6VXNlcjMzMDYyOA==", "avatar_url": "https://avatars.githubusercontent.com/u/330628?v=4", "gravatar_id": "", "url": "https://api.github.com/users/RalfJung", "html_url": "https://github.com/RalfJung", "followers_url": "https://api.github.com/users/RalfJung/followers", "following_url": "https://api.github.com/users/RalfJung/following{/other_user}", "gists_url": "https://api.github.com/users/RalfJung/gists{/gist_id}", "starred_url": "https://api.github.com/users/RalfJung/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/RalfJung/subscriptions", "organizations_url": "https://api.github.com/users/RalfJung/orgs", "repos_url": "https://api.github.com/users/RalfJung/repos", "events_url": "https://api.github.com/users/RalfJung/events{/privacy}", "received_events_url": "https://api.github.com/users/RalfJung/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "3e8bd4560cac9e0d60b60e5ef6cba3d48f279817", "url": "https://api.github.com/repos/rust-lang/rust/commits/3e8bd4560cac9e0d60b60e5ef6cba3d48f279817", "html_url": "https://github.com/rust-lang/rust/commit/3e8bd4560cac9e0d60b60e5ef6cba3d48f279817"}], "stats": {"total": 1151, "additions": 573, "deletions": 578}, "files": [{"sha": "d8794fed46942fdc3890b4cb4d1a4d916f8ee091", "filename": "src/fn_call.rs", "status": "modified", "additions": 13, "deletions": 15, "changes": 28, "blob_url": "https://github.com/rust-lang/rust/blob/3f0a2a29414a67d9c52241a94ce373546ca2ddcf/src%2Ffn_call.rs", "raw_url": "https://github.com/rust-lang/rust/raw/3f0a2a29414a67d9c52241a94ce373546ca2ddcf/src%2Ffn_call.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ffn_call.rs?ref=3f0a2a29414a67d9c52241a94ce373546ca2ddcf", "patch": "@@ -13,8 +13,8 @@ pub trait EvalContextExt<'a, 'mir, 'tcx: 'a + 'mir>: crate::MiriEvalContextExt<'\n     fn find_fn(\n         &mut self,\n         instance: ty::Instance<'tcx>,\n-        args: &[OpTy<'tcx, Borrow>],\n-        dest: Option<PlaceTy<'tcx, Borrow>>,\n+        args: &[OpTy<'tcx, Tag>],\n+        dest: Option<PlaceTy<'tcx, Tag>>,\n         ret: Option<mir::BasicBlock>,\n     ) -> EvalResult<'tcx, Option<&'mir mir::Mir<'tcx>>> {\n         let this = self.eval_context_mut();\n@@ -55,8 +55,8 @@ pub trait EvalContextExt<'a, 'mir, 'tcx: 'a + 'mir>: crate::MiriEvalContextExt<'\n     fn emulate_foreign_item(\n         &mut self,\n         def_id: DefId,\n-        args: &[OpTy<'tcx, Borrow>],\n-        dest: Option<PlaceTy<'tcx, Borrow>>,\n+        args: &[OpTy<'tcx, Tag>],\n+        dest: Option<PlaceTy<'tcx, Tag>>,\n         ret: Option<mir::BasicBlock>,\n     ) -> EvalResult<'tcx> {\n         let this = self.eval_context_mut();\n@@ -92,7 +92,7 @@ pub trait EvalContextExt<'a, 'mir, 'tcx: 'a + 'mir>: crate::MiriEvalContextExt<'\n                 } else {\n                     let align = this.tcx.data_layout.pointer_align.abi;\n                     let ptr = this.memory_mut().allocate(Size::from_bytes(size), align, MiriMemoryKind::C.into());\n-                    this.write_scalar(Scalar::Ptr(ptr.with_default_tag()), dest)?;\n+                    this.write_scalar(Scalar::Ptr(ptr), dest)?;\n                 }\n             }\n             \"calloc\" => {\n@@ -105,7 +105,7 @@ pub trait EvalContextExt<'a, 'mir, 'tcx: 'a + 'mir>: crate::MiriEvalContextExt<'\n                 } else {\n                     let size = Size::from_bytes(bytes);\n                     let align = this.tcx.data_layout.pointer_align.abi;\n-                    let ptr = this.memory_mut().allocate(size, align, MiriMemoryKind::C.into()).with_default_tag();\n+                    let ptr = this.memory_mut().allocate(size, align, MiriMemoryKind::C.into());\n                     this.memory_mut().get_mut(ptr.alloc_id)?.write_repeat(tcx, ptr, 0, size)?;\n                     this.write_scalar(Scalar::Ptr(ptr), dest)?;\n                 }\n@@ -132,7 +132,7 @@ pub trait EvalContextExt<'a, 'mir, 'tcx: 'a + 'mir>: crate::MiriEvalContextExt<'\n                         Align::from_bytes(align).unwrap(),\n                         MiriMemoryKind::C.into()\n                     );\n-                    this.write_scalar(Scalar::Ptr(ptr.with_default_tag()), ret.into())?;\n+                    this.write_scalar(Scalar::Ptr(ptr), ret.into())?;\n                 }\n                 this.write_null(dest)?;\n             }\n@@ -162,8 +162,7 @@ pub trait EvalContextExt<'a, 'mir, 'tcx: 'a + 'mir>: crate::MiriEvalContextExt<'\n                         Size::from_bytes(size),\n                         Align::from_bytes(align).unwrap(),\n                         MiriMemoryKind::Rust.into()\n-                    )\n-                    .with_default_tag();\n+                    );\n                 this.write_scalar(Scalar::Ptr(ptr), dest)?;\n             }\n             \"__rust_alloc_zeroed\" => {\n@@ -180,8 +179,7 @@ pub trait EvalContextExt<'a, 'mir, 'tcx: 'a + 'mir>: crate::MiriEvalContextExt<'\n                         Size::from_bytes(size),\n                         Align::from_bytes(align).unwrap(),\n                         MiriMemoryKind::Rust.into()\n-                    )\n-                    .with_default_tag();\n+                    );\n                 this.memory_mut()\n                     .get_mut(ptr.alloc_id)?\n                     .write_repeat(tcx, ptr, 0, Size::from_bytes(size))?;\n@@ -222,7 +220,7 @@ pub trait EvalContextExt<'a, 'mir, 'tcx: 'a + 'mir>: crate::MiriEvalContextExt<'\n                     Align::from_bytes(align).unwrap(),\n                     MiriMemoryKind::Rust.into(),\n                 )?;\n-                this.write_scalar(Scalar::Ptr(new_ptr.with_default_tag()), dest)?;\n+                this.write_scalar(Scalar::Ptr(new_ptr), dest)?;\n             }\n \n             \"syscall\" => {\n@@ -428,7 +426,7 @@ pub trait EvalContextExt<'a, 'mir, 'tcx: 'a + 'mir>: crate::MiriEvalContextExt<'\n                         Size::from_bytes((value.len() + 1) as u64),\n                         Align::from_bytes(1).unwrap(),\n                         MiriMemoryKind::Env.into(),\n-                    ).with_default_tag();\n+                    );\n                     {\n                         let alloc = this.memory_mut().get_mut(value_copy.alloc_id)?;\n                         alloc.write_bytes(tcx, value_copy, &value)?;\n@@ -798,13 +796,13 @@ pub trait EvalContextExt<'a, 'mir, 'tcx: 'a + 'mir>: crate::MiriEvalContextExt<'\n         Ok(())\n     }\n \n-    fn write_null(&mut self, dest: PlaceTy<'tcx, Borrow>) -> EvalResult<'tcx> {\n+    fn write_null(&mut self, dest: PlaceTy<'tcx, Tag>) -> EvalResult<'tcx> {\n         self.eval_context_mut().write_scalar(Scalar::from_int(0, dest.layout.size), dest)\n     }\n \n     /// Evaluates the scalar at the specified path. Returns Some(val)\n     /// if the path could be resolved, and None otherwise\n-    fn eval_path_scalar(&mut self, path: &[&str]) -> EvalResult<'tcx, Option<ScalarMaybeUndef<stacked_borrows::Borrow>>> {\n+    fn eval_path_scalar(&mut self, path: &[&str]) -> EvalResult<'tcx, Option<ScalarMaybeUndef<Tag>>> {\n         let this = self.eval_context_mut();\n         if let Ok(instance) = this.resolve_path(path) {\n             let cid = GlobalId {"}, {"sha": "f468d256031ca0e395e58278bb72cc91378d9be8", "filename": "src/helpers.rs", "status": "modified", "additions": 12, "deletions": 12, "changes": 24, "blob_url": "https://github.com/rust-lang/rust/blob/3f0a2a29414a67d9c52241a94ce373546ca2ddcf/src%2Fhelpers.rs", "raw_url": "https://github.com/rust-lang/rust/raw/3f0a2a29414a67d9c52241a94ce373546ca2ddcf/src%2Fhelpers.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Fhelpers.rs?ref=3f0a2a29414a67d9c52241a94ce373546ca2ddcf", "patch": "@@ -47,9 +47,9 @@ pub trait EvalContextExt<'a, 'mir, 'tcx: 'a + 'mir>: crate::MiriEvalContextExt<'\n     /// will be true if this is frozen, false if this is in an `UnsafeCell`.\n     fn visit_freeze_sensitive(\n         &self,\n-        place: MPlaceTy<'tcx, Borrow>,\n+        place: MPlaceTy<'tcx, Tag>,\n         size: Size,\n-        mut action: impl FnMut(Pointer<Borrow>, Size, bool) -> EvalResult<'tcx>,\n+        mut action: impl FnMut(Pointer<Tag>, Size, bool) -> EvalResult<'tcx>,\n     ) -> EvalResult<'tcx> {\n         let this = self.eval_context_ref();\n         trace!(\"visit_frozen(place={:?}, size={:?})\", *place, size);\n@@ -64,7 +64,7 @@ pub trait EvalContextExt<'a, 'mir, 'tcx: 'a + 'mir>: crate::MiriEvalContextExt<'\n         let mut end_ptr = place.ptr;\n         // Called when we detected an `UnsafeCell` at the given offset and size.\n         // Calls `action` and advances `end_ptr`.\n-        let mut unsafe_cell_action = |unsafe_cell_ptr: Scalar<Borrow>, unsafe_cell_size: Size| {\n+        let mut unsafe_cell_action = |unsafe_cell_ptr: Scalar<Tag>, unsafe_cell_size: Size| {\n             if unsafe_cell_size != Size::ZERO {\n                 debug_assert_eq!(unsafe_cell_ptr.to_ptr().unwrap().alloc_id,\n                     end_ptr.to_ptr().unwrap().alloc_id);\n@@ -120,7 +120,7 @@ pub trait EvalContextExt<'a, 'mir, 'tcx: 'a + 'mir>: crate::MiriEvalContextExt<'\n         /// Visiting the memory covered by a `MemPlace`, being aware of\n         /// whether we are inside an `UnsafeCell` or not.\n         struct UnsafeCellVisitor<'ecx, 'a, 'mir, 'tcx, F>\n-            where F: FnMut(MPlaceTy<'tcx, Borrow>) -> EvalResult<'tcx>\n+            where F: FnMut(MPlaceTy<'tcx, Tag>) -> EvalResult<'tcx>\n         {\n             ecx: &'ecx MiriEvalContext<'a, 'mir, 'tcx>,\n             unsafe_cell_action: F,\n@@ -131,17 +131,17 @@ pub trait EvalContextExt<'a, 'mir, 'tcx: 'a + 'mir>: crate::MiriEvalContextExt<'\n         for\n             UnsafeCellVisitor<'ecx, 'a, 'mir, 'tcx, F>\n         where\n-            F: FnMut(MPlaceTy<'tcx, Borrow>) -> EvalResult<'tcx>\n+            F: FnMut(MPlaceTy<'tcx, Tag>) -> EvalResult<'tcx>\n         {\n-            type V = MPlaceTy<'tcx, Borrow>;\n+            type V = MPlaceTy<'tcx, Tag>;\n \n             #[inline(always)]\n             fn ecx(&self) -> &MiriEvalContext<'a, 'mir, 'tcx> {\n                 &self.ecx\n             }\n \n             // Hook to detect `UnsafeCell`.\n-            fn visit_value(&mut self, v: MPlaceTy<'tcx, Borrow>) -> EvalResult<'tcx>\n+            fn visit_value(&mut self, v: MPlaceTy<'tcx, Tag>) -> EvalResult<'tcx>\n             {\n                 trace!(\"UnsafeCellVisitor: {:?} {:?}\", *v, v.layout.ty);\n                 let is_unsafe_cell = match v.layout.ty.sty {\n@@ -163,8 +163,8 @@ pub trait EvalContextExt<'a, 'mir, 'tcx: 'a + 'mir>: crate::MiriEvalContextExt<'\n             // Make sure we visit aggregrates in increasing offset order.\n             fn visit_aggregate(\n                 &mut self,\n-                place: MPlaceTy<'tcx, Borrow>,\n-                fields: impl Iterator<Item=EvalResult<'tcx, MPlaceTy<'tcx, Borrow>>>,\n+                place: MPlaceTy<'tcx, Tag>,\n+                fields: impl Iterator<Item=EvalResult<'tcx, MPlaceTy<'tcx, Tag>>>,\n             ) -> EvalResult<'tcx> {\n                 match place.layout.fields {\n                     layout::FieldPlacement::Array { .. } => {\n@@ -174,7 +174,7 @@ pub trait EvalContextExt<'a, 'mir, 'tcx: 'a + 'mir>: crate::MiriEvalContextExt<'\n                     }\n                     layout::FieldPlacement::Arbitrary { .. } => {\n                         // Gather the subplaces and sort them before visiting.\n-                        let mut places = fields.collect::<EvalResult<'tcx, Vec<MPlaceTy<'tcx, Borrow>>>>()?;\n+                        let mut places = fields.collect::<EvalResult<'tcx, Vec<MPlaceTy<'tcx, Tag>>>>()?;\n                         places.sort_by_key(|place| place.ptr.get_ptr_offset(self.ecx()));\n                         self.walk_aggregate(place, places.into_iter().map(Ok))\n                     }\n@@ -186,7 +186,7 @@ pub trait EvalContextExt<'a, 'mir, 'tcx: 'a + 'mir>: crate::MiriEvalContextExt<'\n             }\n \n             // We have to do *something* for unions.\n-            fn visit_union(&mut self, v: MPlaceTy<'tcx, Borrow>) -> EvalResult<'tcx>\n+            fn visit_union(&mut self, v: MPlaceTy<'tcx, Tag>) -> EvalResult<'tcx>\n             {\n                 // With unions, we fall back to whatever the type says, to hopefully be consistent\n                 // with LLVM IR.\n@@ -200,7 +200,7 @@ pub trait EvalContextExt<'a, 'mir, 'tcx: 'a + 'mir>: crate::MiriEvalContextExt<'\n             }\n \n             // We should never get to a primitive, but always short-circuit somewhere above.\n-            fn visit_primitive(&mut self, _v: MPlaceTy<'tcx, Borrow>) -> EvalResult<'tcx>\n+            fn visit_primitive(&mut self, _v: MPlaceTy<'tcx, Tag>) -> EvalResult<'tcx>\n             {\n                 bug!(\"we should always short-circuit before coming to a primitive\")\n             }"}, {"sha": "a17f576b43b7f3e3105fa601d5c64996121e42e0", "filename": "src/intrinsic.rs", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "blob_url": "https://github.com/rust-lang/rust/blob/3f0a2a29414a67d9c52241a94ce373546ca2ddcf/src%2Fintrinsic.rs", "raw_url": "https://github.com/rust-lang/rust/raw/3f0a2a29414a67d9c52241a94ce373546ca2ddcf/src%2Fintrinsic.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Fintrinsic.rs?ref=3f0a2a29414a67d9c52241a94ce373546ca2ddcf", "patch": "@@ -4,7 +4,7 @@ use rustc::ty::layout::{self, LayoutOf, Size};\n use rustc::ty;\n \n use crate::{\n-    PlaceTy, OpTy, ImmTy, Immediate, Scalar, ScalarMaybeUndef, Borrow,\n+    PlaceTy, OpTy, ImmTy, Immediate, Scalar, ScalarMaybeUndef, Tag,\n     OperatorEvalContextExt\n };\n \n@@ -13,8 +13,8 @@ pub trait EvalContextExt<'a, 'mir, 'tcx: 'a+'mir>: crate::MiriEvalContextExt<'a,\n     fn call_intrinsic(\n         &mut self,\n         instance: ty::Instance<'tcx>,\n-        args: &[OpTy<'tcx, Borrow>],\n-        dest: PlaceTy<'tcx, Borrow>,\n+        args: &[OpTy<'tcx, Tag>],\n+        dest: PlaceTy<'tcx, Tag>,\n     ) -> EvalResult<'tcx> {\n         let this = self.eval_context_mut();\n         if this.emulate_intrinsic(instance, args, dest)? {"}, {"sha": "3dbe922999dabcc77a2be19bc4a407ec77243be5", "filename": "src/lib.rs", "status": "modified", "additions": 48, "deletions": 60, "changes": 108, "blob_url": "https://github.com/rust-lang/rust/blob/3f0a2a29414a67d9c52241a94ce373546ca2ddcf/src%2Flib.rs", "raw_url": "https://github.com/rust-lang/rust/raw/3f0a2a29414a67d9c52241a94ce373546ca2ddcf/src%2Flib.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flib.rs?ref=3f0a2a29414a67d9c52241a94ce373546ca2ddcf", "patch": "@@ -23,6 +23,7 @@ mod stacked_borrows;\n \n use std::collections::HashMap;\n use std::borrow::Cow;\n+use std::rc::Rc;\n \n use rand::rngs::StdRng;\n use rand::SeedableRng;\n@@ -48,7 +49,7 @@ use crate::mono_hash_map::MonoHashMap;\n pub use crate::stacked_borrows::{EvalContextExt as StackedBorEvalContextExt};\n \n // Used by priroda.\n-pub use crate::stacked_borrows::{Borrow, Stack, Stacks, BorStackItem};\n+pub use crate::stacked_borrows::{Tag, Permission, Stack, Stacks, Item};\n \n /// Insert rustc arguments at the beginning of the argument list that Miri wants to be\n /// set per default, for maximal validation power.\n@@ -155,7 +156,7 @@ pub fn create_ecx<'a, 'mir: 'a, 'tcx: 'mir>(\n     // Don't forget `0` terminator.\n     cmd.push(std::char::from_u32(0).unwrap());\n     // Collect the pointers to the individual strings.\n-    let mut argvs = Vec::<Pointer<Borrow>>::new();\n+    let mut argvs = Vec::<Pointer<Tag>>::new();\n     for arg in config.args {\n         // Add `0` terminator.\n         let mut arg = arg.into_bytes();\n@@ -187,7 +188,7 @@ pub fn create_ecx<'a, 'mir: 'a, 'tcx: 'mir>(\n             Size::from_bytes(cmd_utf16.len() as u64 * 2),\n             Align::from_bytes(2).unwrap(),\n             MiriMemoryKind::Env.into(),\n-        ).with_default_tag();\n+        );\n         ecx.machine.cmd_line = Some(cmd_ptr);\n         // Store the UTF-16 string.\n         let char_size = Size::from_bytes(2);\n@@ -214,7 +215,13 @@ pub fn eval_main<'a, 'tcx: 'a>(\n     main_id: DefId,\n     config: MiriConfig,\n ) {\n-    let mut ecx = create_ecx(tcx, main_id, config).expect(\"couldn't create ecx\");\n+    let mut ecx = match create_ecx(tcx, main_id, config) {\n+        Ok(ecx) => ecx,\n+        Err(mut err) => {\n+            err.print_backtrace();\n+            panic!(\"Miri initialziation error: {}\", err.kind)\n+        }\n+    };\n \n     // Perform the main execution.\n     let res: EvalResult = (|| {\n@@ -310,14 +317,14 @@ impl MayLeak for MiriMemoryKind {\n pub struct Evaluator<'tcx> {\n     /// Environment variables set by `setenv`.\n     /// Miri does not expose env vars from the host to the emulated program.\n-    pub(crate) env_vars: HashMap<Vec<u8>, Pointer<Borrow>>,\n+    pub(crate) env_vars: HashMap<Vec<u8>, Pointer<Tag>>,\n \n     /// Program arguments (`Option` because we can only initialize them after creating the ecx).\n     /// These are *pointers* to argc/argv because macOS.\n     /// We also need the full command line as one string because of Windows.\n-    pub(crate) argc: Option<Pointer<Borrow>>,\n-    pub(crate) argv: Option<Pointer<Borrow>>,\n-    pub(crate) cmd_line: Option<Pointer<Borrow>>,\n+    pub(crate) argc: Option<Pointer<Tag>>,\n+    pub(crate) argv: Option<Pointer<Tag>>,\n+    pub(crate) cmd_line: Option<Pointer<Tag>>,\n \n     /// Last OS error.\n     pub(crate) last_error: u32,\n@@ -328,9 +335,6 @@ pub struct Evaluator<'tcx> {\n     /// Whether to enforce the validity invariant.\n     pub(crate) validate: bool,\n \n-    /// Stacked Borrows state.\n-    pub(crate) stacked_borrows: stacked_borrows::State,\n-\n     /// The random number generator to use if Miri\n     /// is running in non-deterministic mode\n     pub(crate) rng: Option<StdRng>\n@@ -346,7 +350,6 @@ impl<'tcx> Evaluator<'tcx> {\n             last_error: 0,\n             tls: TlsData::default(),\n             validate,\n-            stacked_borrows: stacked_borrows::State::default(),\n             rng: seed.map(|s| StdRng::seed_from_u64(s))\n         }\n     }\n@@ -378,9 +381,9 @@ impl<'a, 'mir, 'tcx> Machine<'a, 'mir, 'tcx> for Evaluator<'tcx> {\n     type FrameExtra = stacked_borrows::CallId;\n     type MemoryExtra = stacked_borrows::MemoryState;\n     type AllocExtra = stacked_borrows::Stacks;\n-    type PointerTag = Borrow;\n+    type PointerTag = Tag;\n \n-    type MemoryMap = MonoHashMap<AllocId, (MemoryKind<MiriMemoryKind>, Allocation<Borrow, Self::AllocExtra>)>;\n+    type MemoryMap = MonoHashMap<AllocId, (MemoryKind<MiriMemoryKind>, Allocation<Tag, Self::AllocExtra>)>;\n \n     const STATIC_KIND: Option<MiriMemoryKind> = Some(MiriMemoryKind::MutStatic);\n \n@@ -394,8 +397,8 @@ impl<'a, 'mir, 'tcx> Machine<'a, 'mir, 'tcx> for Evaluator<'tcx> {\n     fn find_fn(\n         ecx: &mut InterpretCx<'a, 'mir, 'tcx, Self>,\n         instance: ty::Instance<'tcx>,\n-        args: &[OpTy<'tcx, Borrow>],\n-        dest: Option<PlaceTy<'tcx, Borrow>>,\n+        args: &[OpTy<'tcx, Tag>],\n+        dest: Option<PlaceTy<'tcx, Tag>>,\n         ret: Option<mir::BasicBlock>,\n     ) -> EvalResult<'tcx, Option<&'mir mir::Mir<'tcx>>> {\n         ecx.find_fn(instance, args, dest, ret)\n@@ -405,8 +408,8 @@ impl<'a, 'mir, 'tcx> Machine<'a, 'mir, 'tcx> for Evaluator<'tcx> {\n     fn call_intrinsic(\n         ecx: &mut rustc_mir::interpret::InterpretCx<'a, 'mir, 'tcx, Self>,\n         instance: ty::Instance<'tcx>,\n-        args: &[OpTy<'tcx, Borrow>],\n-        dest: PlaceTy<'tcx, Borrow>,\n+        args: &[OpTy<'tcx, Tag>],\n+        dest: PlaceTy<'tcx, Tag>,\n     ) -> EvalResult<'tcx> {\n         ecx.call_intrinsic(instance, args, dest)\n     }\n@@ -415,15 +418,15 @@ impl<'a, 'mir, 'tcx> Machine<'a, 'mir, 'tcx> for Evaluator<'tcx> {\n     fn ptr_op(\n         ecx: &rustc_mir::interpret::InterpretCx<'a, 'mir, 'tcx, Self>,\n         bin_op: mir::BinOp,\n-        left: ImmTy<'tcx, Borrow>,\n-        right: ImmTy<'tcx, Borrow>,\n-    ) -> EvalResult<'tcx, (Scalar<Borrow>, bool)> {\n+        left: ImmTy<'tcx, Tag>,\n+        right: ImmTy<'tcx, Tag>,\n+    ) -> EvalResult<'tcx, (Scalar<Tag>, bool)> {\n         ecx.ptr_op(bin_op, left, right)\n     }\n \n     fn box_alloc(\n         ecx: &mut InterpretCx<'a, 'mir, 'tcx, Self>,\n-        dest: PlaceTy<'tcx, Borrow>,\n+        dest: PlaceTy<'tcx, Tag>,\n     ) -> EvalResult<'tcx> {\n         trace!(\"box_alloc for {:?}\", dest.layout.ty);\n         // Call the `exchange_malloc` lang item.\n@@ -467,7 +470,7 @@ impl<'a, 'mir, 'tcx> Machine<'a, 'mir, 'tcx> for Evaluator<'tcx> {\n         def_id: DefId,\n         tcx: TyCtxtAt<'a, 'tcx, 'tcx>,\n         memory_extra: &Self::MemoryExtra,\n-    ) -> EvalResult<'tcx, Cow<'tcx, Allocation<Borrow, Self::AllocExtra>>> {\n+    ) -> EvalResult<'tcx, Cow<'tcx, Allocation<Tag, Self::AllocExtra>>> {\n         let attrs = tcx.get_attrs(def_id);\n         let link_name = match attr::first_attr_value_str_by_name(&attrs, \"link_name\") {\n             Some(name) => name.as_str(),\n@@ -479,7 +482,7 @@ impl<'a, 'mir, 'tcx> Machine<'a, 'mir, 'tcx> for Evaluator<'tcx> {\n                 // This should be all-zero, pointer-sized.\n                 let size = tcx.data_layout.pointer_size;\n                 let data = vec![0; size.bytes() as usize];\n-                let extra = AllocationExtra::memory_allocated(size, memory_extra);\n+                let extra = Stacks::new(size, Tag::default(), Rc::clone(memory_extra));\n                 Allocation::from_bytes(&data, tcx.data_layout.pointer_align.abi, extra)\n             }\n             _ => return err!(Unimplemented(\n@@ -499,16 +502,17 @@ impl<'a, 'mir, 'tcx> Machine<'a, 'mir, 'tcx> for Evaluator<'tcx> {\n     fn adjust_static_allocation<'b>(\n         alloc: &'b Allocation,\n         memory_extra: &Self::MemoryExtra,\n-    ) -> Cow<'b, Allocation<Borrow, Self::AllocExtra>> {\n-        let extra = AllocationExtra::memory_allocated(\n+    ) -> Cow<'b, Allocation<Tag, Self::AllocExtra>> {\n+        let extra = Stacks::new(\n             Size::from_bytes(alloc.bytes.len() as u64),\n-            memory_extra,\n+            Tag::default(),\n+            Rc::clone(memory_extra),\n         );\n-        let alloc: Allocation<Borrow, Self::AllocExtra> = Allocation {\n+        let alloc: Allocation<Tag, Self::AllocExtra> = Allocation {\n             bytes: alloc.bytes.clone(),\n             relocations: Relocations::from_presorted(\n                 alloc.relocations.iter()\n-                    .map(|&(offset, ((), alloc))| (offset, (Borrow::default(), alloc)))\n+                    .map(|&(offset, ((), alloc))| (offset, (Tag::default(), alloc)))\n                     .collect()\n             ),\n             undef_mask: alloc.undef_mask.clone(),\n@@ -519,46 +523,30 @@ impl<'a, 'mir, 'tcx> Machine<'a, 'mir, 'tcx> for Evaluator<'tcx> {\n         Cow::Owned(alloc)\n     }\n \n-    fn tag_dereference(\n-        ecx: &InterpretCx<'a, 'mir, 'tcx, Self>,\n-        place: MPlaceTy<'tcx, Borrow>,\n-        mutability: Option<hir::Mutability>,\n-    ) -> EvalResult<'tcx, Scalar<Borrow>> {\n-        let size = ecx.size_and_align_of_mplace(place)?.map(|(size, _)| size)\n-            // For extern types, just cover what we can.\n-            .unwrap_or_else(|| place.layout.size);\n-        if !ecx.tcx.sess.opts.debugging_opts.mir_emit_retag ||\n-            !Self::enforce_validity(ecx) || size == Size::ZERO\n-        {\n-            // No tracking.\n-            Ok(place.ptr)\n-        } else {\n-            ecx.ptr_dereference(place, size, mutability.into())?;\n-            // We never change the pointer.\n-            Ok(place.ptr)\n-        }\n+    #[inline(always)]\n+    fn new_allocation(\n+        size: Size,\n+        extra: &Self::MemoryExtra,\n+        kind: MemoryKind<MiriMemoryKind>,\n+    ) -> (Self::AllocExtra, Self::PointerTag) {\n+        Stacks::new_allocation(size, extra, kind)\n     }\n \n     #[inline(always)]\n-    fn tag_new_allocation(\n-        ecx: &mut InterpretCx<'a, 'mir, 'tcx, Self>,\n-        ptr: Pointer,\n-        kind: MemoryKind<Self::MemoryKinds>,\n-    ) -> Pointer<Borrow> {\n-        if !ecx.machine.validate {\n-            // No tracking.\n-            ptr.with_default_tag()\n-        } else {\n-            let tag = ecx.tag_new_allocation(ptr.alloc_id, kind);\n-            Pointer::new_with_tag(ptr.alloc_id, ptr.offset, tag)\n-        }\n+    fn tag_dereference(\n+        _ecx: &InterpretCx<'a, 'mir, 'tcx, Self>,\n+        place: MPlaceTy<'tcx, Tag>,\n+        _mutability: Option<hir::Mutability>,\n+    ) -> EvalResult<'tcx, Scalar<Tag>> {\n+        // Nothing happens.\n+        Ok(place.ptr)\n     }\n \n     #[inline(always)]\n     fn retag(\n         ecx: &mut InterpretCx<'a, 'mir, 'tcx, Self>,\n         kind: mir::RetagKind,\n-        place: PlaceTy<'tcx, Borrow>,\n+        place: PlaceTy<'tcx, Tag>,\n     ) -> EvalResult<'tcx> {\n         if !ecx.tcx.sess.opts.debugging_opts.mir_emit_retag || !Self::enforce_validity(ecx) {\n             // No tracking, or no retagging. The latter is possible because a dependency of ours"}, {"sha": "386fc4307b87f4cfcc787252dc711711748991fb", "filename": "src/operator.rs", "status": "modified", "additions": 19, "deletions": 19, "changes": 38, "blob_url": "https://github.com/rust-lang/rust/blob/3f0a2a29414a67d9c52241a94ce373546ca2ddcf/src%2Foperator.rs", "raw_url": "https://github.com/rust-lang/rust/raw/3f0a2a29414a67d9c52241a94ce373546ca2ddcf/src%2Foperator.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Foperator.rs?ref=3f0a2a29414a67d9c52241a94ce373546ca2ddcf", "patch": "@@ -7,39 +7,39 @@ pub trait EvalContextExt<'tcx> {\n     fn ptr_op(\n         &self,\n         bin_op: mir::BinOp,\n-        left: ImmTy<'tcx, Borrow>,\n-        right: ImmTy<'tcx, Borrow>,\n-    ) -> EvalResult<'tcx, (Scalar<Borrow>, bool)>;\n+        left: ImmTy<'tcx, Tag>,\n+        right: ImmTy<'tcx, Tag>,\n+    ) -> EvalResult<'tcx, (Scalar<Tag>, bool)>;\n \n     fn ptr_int_arithmetic(\n         &self,\n         bin_op: mir::BinOp,\n-        left: Pointer<Borrow>,\n+        left: Pointer<Tag>,\n         right: u128,\n         signed: bool,\n-    ) -> EvalResult<'tcx, (Scalar<Borrow>, bool)>;\n+    ) -> EvalResult<'tcx, (Scalar<Tag>, bool)>;\n \n     fn ptr_eq(\n         &self,\n-        left: Scalar<Borrow>,\n-        right: Scalar<Borrow>,\n+        left: Scalar<Tag>,\n+        right: Scalar<Tag>,\n     ) -> EvalResult<'tcx, bool>;\n \n     fn pointer_offset_inbounds(\n         &self,\n-        ptr: Scalar<Borrow>,\n+        ptr: Scalar<Tag>,\n         pointee_ty: Ty<'tcx>,\n         offset: i64,\n-    ) -> EvalResult<'tcx, Scalar<Borrow>>;\n+    ) -> EvalResult<'tcx, Scalar<Tag>>;\n }\n \n impl<'a, 'mir, 'tcx> EvalContextExt<'tcx> for super::MiriEvalContext<'a, 'mir, 'tcx> {\n     fn ptr_op(\n         &self,\n         bin_op: mir::BinOp,\n-        left: ImmTy<'tcx, Borrow>,\n-        right: ImmTy<'tcx, Borrow>,\n-    ) -> EvalResult<'tcx, (Scalar<Borrow>, bool)> {\n+        left: ImmTy<'tcx, Tag>,\n+        right: ImmTy<'tcx, Tag>,\n+    ) -> EvalResult<'tcx, (Scalar<Tag>, bool)> {\n         use rustc::mir::BinOp::*;\n \n         trace!(\"ptr_op: {:?} {:?} {:?}\", *left, bin_op, *right);\n@@ -136,8 +136,8 @@ impl<'a, 'mir, 'tcx> EvalContextExt<'tcx> for super::MiriEvalContext<'a, 'mir, '\n \n     fn ptr_eq(\n         &self,\n-        left: Scalar<Borrow>,\n-        right: Scalar<Borrow>,\n+        left: Scalar<Tag>,\n+        right: Scalar<Tag>,\n     ) -> EvalResult<'tcx, bool> {\n         let size = self.pointer_size();\n         Ok(match (left, right) {\n@@ -233,13 +233,13 @@ impl<'a, 'mir, 'tcx> EvalContextExt<'tcx> for super::MiriEvalContext<'a, 'mir, '\n     fn ptr_int_arithmetic(\n         &self,\n         bin_op: mir::BinOp,\n-        left: Pointer<Borrow>,\n+        left: Pointer<Tag>,\n         right: u128,\n         signed: bool,\n-    ) -> EvalResult<'tcx, (Scalar<Borrow>, bool)> {\n+    ) -> EvalResult<'tcx, (Scalar<Tag>, bool)> {\n         use rustc::mir::BinOp::*;\n \n-        fn map_to_primval((res, over): (Pointer<Borrow>, bool)) -> (Scalar<Borrow>, bool) {\n+        fn map_to_primval((res, over): (Pointer<Tag>, bool)) -> (Scalar<Tag>, bool) {\n             (Scalar::Ptr(res), over)\n         }\n \n@@ -327,10 +327,10 @@ impl<'a, 'mir, 'tcx> EvalContextExt<'tcx> for super::MiriEvalContext<'a, 'mir, '\n     /// allocation, and all the remaining integers pointers their own allocation.\n     fn pointer_offset_inbounds(\n         &self,\n-        ptr: Scalar<Borrow>,\n+        ptr: Scalar<Tag>,\n         pointee_ty: Ty<'tcx>,\n         offset: i64,\n-    ) -> EvalResult<'tcx, Scalar<Borrow>> {\n+    ) -> EvalResult<'tcx, Scalar<Tag>> {\n         // FIXME: assuming here that type size is less than `i64::max_value()`.\n         let pointee_size = self.layout_of(pointee_ty)?.size.bytes() as i64;\n         let offset = offset"}, {"sha": "080200b12a4ea7f825be72096b0ed9d4a8835932", "filename": "src/stacked_borrows.rs", "status": "modified", "additions": 473, "deletions": 464, "changes": 937, "blob_url": "https://github.com/rust-lang/rust/blob/3f0a2a29414a67d9c52241a94ce373546ca2ddcf/src%2Fstacked_borrows.rs", "raw_url": "https://github.com/rust-lang/rust/raw/3f0a2a29414a67d9c52241a94ce373546ca2ddcf/src%2Fstacked_borrows.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Fstacked_borrows.rs?ref=3f0a2a29414a67d9c52241a94ce373546ca2ddcf", "patch": "@@ -1,127 +1,172 @@\n use std::cell::RefCell;\n use std::collections::HashSet;\n use std::rc::Rc;\n+use std::fmt;\n+use std::num::NonZeroU64;\n \n use rustc::ty::{self, layout::Size};\n use rustc::hir::{Mutability, MutMutable, MutImmutable};\n use rustc::mir::RetagKind;\n \n use crate::{\n     EvalResult, InterpError, MiriEvalContext, HelpersEvalContextExt, Evaluator, MutValueVisitor,\n-    MemoryKind, MiriMemoryKind, RangeMap, AllocId, Allocation, AllocationExtra,\n+    MemoryKind, MiriMemoryKind, RangeMap, Allocation, AllocationExtra,\n     Pointer, Immediate, ImmTy, PlaceTy, MPlaceTy,\n };\n \n-pub type Timestamp = u64;\n+pub type PtrId = NonZeroU64;\n pub type CallId = u64;\n \n-/// Information about which kind of borrow was used to create the reference this is tagged with.\n+/// Tracking pointer provenance\n #[derive(Copy, Clone, Debug, Hash, PartialEq, Eq)]\n-pub enum Borrow {\n-    /// A unique (mutable) reference.\n-    Uniq(Timestamp),\n-    /// An aliasing reference. This is also used by raw pointers, which do not track details\n-    /// of how or when they were created, hence the timestamp is optional.\n-    /// `Shr(Some(_))` does *not* mean that the destination of this reference is frozen;\n-    /// that depends on the type! Only those parts outside of an `UnsafeCell` are actually\n-    /// frozen.\n-    Alias(Option<Timestamp>),\n+pub enum Tag {\n+    Tagged(PtrId),\n+    Untagged,\n }\n \n-impl Borrow {\n-    #[inline(always)]\n-    pub fn is_aliasing(self) -> bool {\n-        match self {\n-            Borrow::Alias(_) => true,\n-            _ => false,\n-        }\n-    }\n-\n-    #[inline(always)]\n-    pub fn is_unique(self) -> bool {\n+impl fmt::Display for Tag {\n+    fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {\n         match self {\n-            Borrow::Uniq(_) => true,\n-            _ => false,\n+            Tag::Tagged(id) => write!(f, \"{}\", id),\n+            Tag::Untagged => write!(f, \"<untagged>\"),\n         }\n     }\n }\n \n-impl Default for Borrow {\n-    fn default() -> Self {\n-        Borrow::Alias(None)\n-    }\n+/// Indicates which permission is granted (by this item to some pointers)\n+#[derive(Copy, Clone, Debug, Hash, PartialEq, Eq)]\n+pub enum Permission {\n+    /// Grants unique mutable access.\n+    Unique,\n+    /// Grants shared mutable access.\n+    SharedReadWrite,\n+    /// Greants shared read-only access.\n+    SharedReadOnly,\n }\n \n /// An item in the per-location borrow stack.\n #[derive(Copy, Clone, Debug, Hash, PartialEq, Eq)]\n-pub enum BorStackItem {\n-    /// Indicates the unique reference that may mutate.\n-    Uniq(Timestamp),\n-    /// Indicates that the location has been mutably shared. Used for raw pointers as\n-    /// well as for unfrozen shared references.\n-    Raw,\n+pub enum Item {\n+    /// Grants the given permission for pointers with this tag.\n+    Permission(Permission, Tag),\n     /// A barrier, tracking the function it belongs to by its index on the call stack.\n-    FnBarrier(CallId)\n+    FnBarrier(CallId),\n+}\n+\n+impl fmt::Display for Item {\n+    fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {\n+        match self {\n+            Item::Permission(perm, tag) => write!(f, \"[{:?} for {}]\", perm, tag),\n+            Item::FnBarrier(call) => write!(f, \"[barrier {}]\", call),\n+        }\n+    }\n }\n \n /// Extra per-location state.\n #[derive(Clone, Debug, PartialEq, Eq)]\n pub struct Stack {\n-    /// Used as the stack; never empty.\n-    borrows: Vec<BorStackItem>,\n-    /// A virtual frozen \"item\" on top of the stack.\n-    frozen_since: Option<Timestamp>,\n+    /// Used *mostly* as a stack; never empty.\n+    /// We sometimes push into the middle but never remove from the middle.\n+    /// The same tag may occur multiple times, e.g. from a two-phase borrow.\n+    /// Invariants:\n+    /// * Above a `SharedReadOnly` there can only be barriers and more `SharedReadOnly`.\n+    borrows: Vec<Item>,\n }\n \n-impl Stack {\n-    #[inline(always)]\n-    pub fn is_frozen(&self) -> bool {\n-        self.frozen_since.is_some()\n-    }\n+\n+/// Extra per-allocation state.\n+#[derive(Clone, Debug)]\n+pub struct Stacks {\n+    // Even reading memory can have effects on the stack, so we need a `RefCell` here.\n+    stacks: RefCell<RangeMap<Stack>>,\n+    // Pointer to global state\n+    global: MemoryState,\n }\n \n-/// Indicates which kind of reference is being used.\n-#[derive(Copy, Clone, Debug, Hash, PartialEq, Eq)]\n-pub enum RefKind {\n-    /// `&mut`.\n-    Unique,\n-    /// `&` without interior mutability.\n-    Frozen,\n-    /// `*` (raw pointer) or `&` to `UnsafeCell`.\n-    Raw,\n+/// Extra global state, available to the memory access hooks.\n+#[derive(Debug)]\n+pub struct GlobalState {\n+    next_ptr_id: PtrId,\n+    next_call_id: CallId,\n+    active_calls: HashSet<CallId>,\n }\n+pub type MemoryState = Rc<RefCell<GlobalState>>;\n \n /// Indicates which kind of access is being performed.\n #[derive(Copy, Clone, Debug, Hash, PartialEq, Eq)]\n pub enum AccessKind {\n     Read,\n-    Write,\n-    Dealloc,\n+    Write { dealloc: bool },\n }\n \n-/// Extra global state in the memory, available to the memory access hooks.\n-#[derive(Debug)]\n-pub struct BarrierTracking {\n-    next_id: CallId,\n-    active_calls: HashSet<CallId>,\n+// \"Fake\" constructors\n+impl AccessKind {\n+    fn write() -> AccessKind {\n+        AccessKind::Write { dealloc: false }\n+    }\n+\n+    fn dealloc() -> AccessKind {\n+        AccessKind::Write { dealloc: true }\n+    }\n+}\n+\n+impl fmt::Display for AccessKind {\n+    fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {\n+        match self {\n+            AccessKind::Read => write!(f, \"read\"),\n+            AccessKind::Write { dealloc: false } => write!(f, \"write\"),\n+            AccessKind::Write { dealloc: true } => write!(f, \"deallocation\"),\n+        }\n+    }\n+}\n+\n+/// Indicates which kind of reference is being created.\n+/// Used by `reborrow` to compute which permissions to grant to the\n+/// new pointer.\n+#[derive(Copy, Clone, Debug, Hash, PartialEq, Eq)]\n+pub enum RefKind {\n+    /// `&mut`.\n+    Mutable,\n+    /// `&` with or without interior mutability.\n+    Shared { frozen: bool },\n+    /// `*` (raw pointer).\n+    Raw,\n+}\n+\n+impl fmt::Display for RefKind {\n+    fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {\n+        match self {\n+            RefKind::Mutable => write!(f, \"mutable\"),\n+            RefKind::Shared { frozen: true } => write!(f, \"shared (frozen)\"),\n+            RefKind::Shared { frozen: false } => write!(f, \"shared (mutable)\"),\n+            RefKind::Raw => write!(f, \"raw\"),\n+        }\n+    }\n }\n-pub type MemoryState = Rc<RefCell<BarrierTracking>>;\n \n-impl Default for BarrierTracking {\n+/// Utilities for initialization and ID generation\n+impl Default for GlobalState {\n     fn default() -> Self {\n-        BarrierTracking {\n-            next_id: 0,\n+        GlobalState {\n+            next_ptr_id: NonZeroU64::new(1).unwrap(),\n+            next_call_id: 0,\n             active_calls: HashSet::default(),\n         }\n     }\n }\n \n-impl BarrierTracking {\n+impl GlobalState {\n+    pub fn new_ptr(&mut self) -> PtrId {\n+        let id = self.next_ptr_id;\n+        self.next_ptr_id = NonZeroU64::new(id.get() + 1).unwrap();\n+        id\n+    }\n+\n     pub fn new_call(&mut self) -> CallId {\n-        let id = self.next_id;\n+        let id = self.next_call_id;\n         trace!(\"new_call: Assigning ID {}\", id);\n         self.active_calls.insert(id);\n-        self.next_id += 1;\n+        self.next_call_id = id+1;\n         id\n     }\n \n@@ -134,272 +179,354 @@ impl BarrierTracking {\n     }\n }\n \n-/// Extra global machine state.\n-#[derive(Clone, Debug)]\n-pub struct State {\n-    clock: Timestamp\n-}\n-\n-impl Default for State {\n-    fn default() -> Self {\n-        State { clock: 0 }\n-    }\n-}\n+// # Stacked Borrows Core Begin\n \n-impl State {\n-    fn increment_clock(&mut self) -> Timestamp {\n-        let val = self.clock;\n-        self.clock = val + 1;\n-        val\n-    }\n-}\n-\n-/// Extra per-allocation state.\n-#[derive(Clone, Debug)]\n-pub struct Stacks {\n-    // Even reading memory can have effects on the stack, so we need a `RefCell` here.\n-    stacks: RefCell<RangeMap<Stack>>,\n-    barrier_tracking: MemoryState,\n-}\n-\n-/// Core per-location operations: deref, access, create.\n /// We need to make at least the following things true:\n ///\n /// U1: After creating a `Uniq`, it is at the top (and unfrozen).\n /// U2: If the top is `Uniq` (and unfrozen), accesses must be through that `Uniq` or pop it.\n-/// U3: If an access (deref sufficient?) happens with a `Uniq`, it requires the `Uniq` to be in the stack.\n+/// U3: If an access happens with a `Uniq`, it requires the `Uniq` to be in the stack.\n ///\n /// F1: After creating a `&`, the parts outside `UnsafeCell` are frozen.\n /// F2: If a write access happens, it unfreezes.\n-/// F3: If an access (well, a deref) happens with an `&` outside `UnsafeCell`,\n+/// F3: If an access happens with an `&` outside `UnsafeCell`,\n ///     it requires the location to still be frozen.\n-impl<'tcx> Stack {\n-    /// Deref `bor`: check if the location is frozen and the tag in the stack.\n-    /// This dos *not* constitute an access! \"Deref\" refers to the `*` operator\n-    /// in Rust, and includs cases like `&*x` or `(*x).foo` where no or only part\n-    /// of the memory actually gets accessed. Also we cannot know if we are\n-    /// going to read or write.\n-    /// Returns the index of the item we matched, `None` if it was the frozen one.\n-    /// `kind` indicates which kind of reference is being dereferenced.\n-    fn deref(\n-        &self,\n-        bor: Borrow,\n-        kind: RefKind,\n-    ) -> Result<Option<usize>, String> {\n-        // Exclude unique ref with frozen tag.\n-        if let (RefKind::Unique, Borrow::Alias(Some(_))) = (kind, bor) {\n-            return Err(format!(\"encountered mutable reference with frozen tag ({:?})\", bor));\n+\n+impl Default for Tag {\n+    #[inline(always)]\n+    fn default() -> Tag {\n+        Tag::Untagged\n+    }\n+}\n+\n+/// Core relations on `Permission` define which accesses are allowed:\n+/// On every access, we try to find a *granting* item, and then we remove all\n+/// *incompatible* items above it.\n+impl Permission {\n+    /// This defines for a given permission, whether it permits the given kind of access.\n+    fn grants(self, access: AccessKind) -> bool {\n+        match (self, access) {\n+            // Unique and SharedReadWrite allow any kind of access.\n+            (Permission::Unique, _) |\n+            (Permission::SharedReadWrite, _) =>\n+                true,\n+            // SharedReadOnly only permits read access.\n+            (Permission::SharedReadOnly, AccessKind::Read) =>\n+                true,\n+            (Permission::SharedReadOnly, AccessKind::Write { .. }) =>\n+                false,\n         }\n-        // Checks related to freezing.\n-        match bor {\n-            Borrow::Alias(Some(bor_t)) if kind == RefKind::Frozen => {\n-                // We need the location to be frozen. This ensures F3.\n-                let frozen = self.frozen_since.map_or(false, |itm_t| itm_t <= bor_t);\n-                return if frozen { Ok(None) } else {\n-                    Err(format!(\"location is not frozen long enough\"))\n-                }\n-            }\n-            Borrow::Alias(_) if self.frozen_since.is_some() => {\n-                // Shared deref to frozen location; looking good.\n-                return Ok(None)\n-            }\n-            // Not sufficient; go on looking.\n-            _ => {}\n+    }\n+\n+    /// This defines for a given permission, which other items it can tolerate \"above\" itself\n+    /// for which kinds of accesses.\n+    /// If true, then `other` is allowed to remain on top of `self` when `access` happens.\n+    fn compatible_with(self, access: AccessKind, other: Item) -> bool {\n+        use self::Permission::*;\n+\n+        let other = match other {\n+            Item::Permission(perm, _) => perm,\n+            Item::FnBarrier(_) => return false, // Remove all barriers -- if they are active, cause UB.\n+        };\n+\n+        match (self, access, other) {\n+            // Some cases are impossible.\n+            (SharedReadOnly, _, SharedReadWrite) |\n+            (SharedReadOnly, _, Unique) =>\n+                bug!(\"There can never be a SharedReadWrite or a Unique on top of a SharedReadOnly\"),\n+            // When `other` is `SharedReadOnly`, that is NEVER compatible with\n+            // write accesses.\n+            // This makes sure read-only pointers become invalid on write accesses.\n+            (_, AccessKind::Write { .. }, SharedReadOnly) =>\n+                false,\n+            // When `other` is `Unique`, that is compatible with nothing.\n+            // This makes sure unique pointers become invalid on incompatible accesses (ensures U2).\n+            (_, _, Unique) =>\n+                false,\n+            // When we are unique and this is a write/dealloc, we tolerate nothing.\n+            // This makes sure we re-assert uniqueness on write accesses.\n+            // (This is particularily important such that when a new mutable ref gets created, it gets\n+            // pushed into the right item -- this behaves like a write and we assert uniqueness of the\n+            // pointer from which this comes, *if* it was a unique pointer.)\n+            (Unique, AccessKind::Write { .. }, _) =>\n+                false,\n+            // `SharedReadWrite` items can tolerate any other akin items for any kind of access.\n+            (SharedReadWrite, _, SharedReadWrite) =>\n+                true,\n+            // Any item can tolerate read accesses for shared items.\n+            // This includes unique items!  Reads from unique pointers do not invalidate\n+            // other pointers.\n+            (_, AccessKind::Read, SharedReadWrite) |\n+            (_, AccessKind::Read, SharedReadOnly) =>\n+                true,\n+            // That's it.\n         }\n-        // If we got here, we have to look for our item in the stack.\n-        for (idx, &itm) in self.borrows.iter().enumerate().rev() {\n-            match (itm, bor) {\n-                (BorStackItem::Uniq(itm_t), Borrow::Uniq(bor_t)) if itm_t == bor_t => {\n-                    // Found matching unique item. This satisfies U3.\n-                    return Ok(Some(idx))\n-                }\n-                (BorStackItem::Raw, Borrow::Alias(_)) => {\n-                    // Found matching aliasing/raw item.\n-                    return Ok(Some(idx))\n-                }\n-                // Go on looking. We ignore barriers! When an `&mut` and an `&` alias,\n-                // dereferencing the `&` is still possible (to reborrow), but doing\n-                // an access is not.\n-                _ => {}\n-            }\n+    }\n+}\n+\n+impl<'tcx> RefKind {\n+    /// Defines which kind of access the \"parent\" must grant to create this reference.\n+    fn access(self) -> AccessKind {\n+        match self {\n+            RefKind::Mutable | RefKind::Shared { frozen: false } => AccessKind::write(),\n+            RefKind::Raw | RefKind::Shared { frozen: true } => AccessKind::Read,\n+            // FIXME: Just requiring read-only access for raw means that a raw ptr might not be writeable\n+            // even when we think it should be!  Think about this some more.\n         }\n-        // If we got here, we did not find our item. We have to error to satisfy U3.\n-        Err(format!(\"Borrow being dereferenced ({:?}) does not exist on the borrow stack\", bor))\n     }\n \n-    /// Performs an actual memory access using `bor`. We do not know any types here\n-    /// or whether things should be frozen, but we *do* know if this is reading\n-    /// or writing.\n+    /// This defines the new permission used when a pointer gets created: For raw pointers, whether these are read-only\n+    /// or read-write depends on the permission from which they derive.\n+    fn new_perm(self, derived_from: Permission) -> EvalResult<'tcx, Permission> {\n+        Ok(match (self, derived_from) {\n+            // Do not derive writable safe pointer from read-only pointer!\n+            (RefKind::Mutable, Permission::SharedReadOnly) =>\n+                return err!(MachineError(format!(\n+                    \"deriving mutable reference from read-only pointer\"\n+                ))),\n+            (RefKind::Shared { frozen: false }, Permission::SharedReadOnly) =>\n+                return err!(MachineError(format!(\n+                    \"deriving shared reference with interior mutability from read-only pointer\"\n+                ))),\n+            // Safe pointer cases.\n+            (RefKind::Mutable, _) => Permission::Unique,\n+            (RefKind::Shared { frozen: true }, _) => Permission::SharedReadOnly,\n+            (RefKind::Shared { frozen: false }, _) => Permission::SharedReadWrite,\n+            // Raw pointer cases.\n+            (RefKind::Raw, Permission::SharedReadOnly) => Permission::SharedReadOnly,\n+            (RefKind::Raw, _) => Permission::SharedReadWrite,\n+        })\n+    }\n+}\n+\n+/// Core per-location operations: access, create.\n+impl<'tcx> Stack {\n+    /// Find the item granting the given kind of access to the given tag, and where that item is in the stack.\n+    fn find_granting(&self, access: AccessKind, tag: Tag) -> Option<(usize, Permission)> {\n+        self.borrows.iter()\n+            .enumerate() // we also need to know *where* in the stack\n+            .rev() // search top-to-bottom\n+            // Return permission of first item that grants access.\n+            .filter_map(|(idx, item)| match item {\n+                &Item::Permission(perm, item_tag) if perm.grants(access) && tag == item_tag =>\n+                    Some((idx, perm)),\n+                _ => None,\n+            })\n+            .next()\n+    }\n+\n+    /// Test if a memory `access` using pointer tagged `tag` is granted.\n+    /// If yes, return the index of the item that granted it.\n     fn access(\n         &mut self,\n-        bor: Borrow,\n-        kind: AccessKind,\n-        barrier_tracking: &BarrierTracking,\n-    ) -> EvalResult<'tcx> {\n-        // Check if we can match the frozen \"item\".\n-        // Not possible on writes!\n-        if self.is_frozen() {\n-            if kind == AccessKind::Read {\n-                // When we are frozen, we just accept all reads. No harm in this.\n-                // The deref already checked that `Uniq` items are in the stack, and that\n-                // the location is frozen if it should be.\n-                return Ok(());\n-            }\n-            trace!(\"access: unfreezing\");\n-        }\n-        // Unfreeze on writes. This ensures F2.\n-        self.frozen_since = None;\n-        // Pop the stack until we have something matching.\n-        while let Some(&itm) = self.borrows.last() {\n-            match (itm, bor) {\n-                (BorStackItem::FnBarrier(call), _) if barrier_tracking.is_active(call) => {\n-                    return err!(MachineError(format!(\n-                        \"stopping looking for borrow being accessed ({:?}) because of barrier ({})\",\n-                        bor, call\n-                    )))\n-                }\n-                (BorStackItem::Uniq(itm_t), Borrow::Uniq(bor_t)) if itm_t == bor_t => {\n-                    // Found matching unique item. Continue after the match.\n-                }\n-                (BorStackItem::Raw, _) if kind == AccessKind::Read => {\n-                    // When reading, everything can use a raw item!\n-                    // We do not want to do this when writing: Writing to an `&mut`\n-                    // should reaffirm its exclusivity (i.e., make sure it is\n-                    // on top of the stack). Continue after the match.\n-                }\n-                (BorStackItem::Raw, Borrow::Alias(_)) => {\n-                    // Found matching raw item. Continue after the match.\n-                }\n-                _ => {\n-                    // Pop this, go on. This ensures U2.\n-                    let itm = self.borrows.pop().unwrap();\n-                    trace!(\"access: Popping {:?}\", itm);\n-                    continue\n-                }\n-            }\n-            // If we got here, we found a matching item. Congratulations!\n-            // However, we are not done yet: If this access is deallocating, we must make sure\n-            // there are no active barriers remaining on the stack.\n-            if kind == AccessKind::Dealloc {\n-                for &itm in self.borrows.iter().rev() {\n-                    match itm {\n-                        BorStackItem::FnBarrier(call) if barrier_tracking.is_active(call) => {\n+        access: AccessKind,\n+        tag: Tag,\n+        global: &GlobalState,\n+    ) -> EvalResult<'tcx, usize> {\n+        // Two main steps: Find granting item, remove all incompatible items above.\n+        // Afterwards we just do some post-processing for deallocation accesses.\n+\n+        // Step 1: Find granting item.\n+        let (granting_idx, granting_perm) = self.find_granting(access, tag)\n+            .ok_or_else(|| InterpError::MachineError(format!(\n+                    \"no item granting {} access to tag {} found in borrow stack\",\n+                    access, tag,\n+            )))?;\n+        \n+        // Step 2: Remove everything incompatible above them.\n+        // Implemented with indices because there does not seem to be a nice iterator and range-based\n+        // API for this.\n+        {\n+            let mut cur = granting_idx + 1;\n+            while let Some(item) = self.borrows.get(cur) {\n+                if granting_perm.compatible_with(access, *item) {\n+                    // Keep this, check next.\n+                    cur += 1;\n+                } else {\n+                    // Aha! This is a bad one, remove it, and if it is an *active* barrier\n+                    // we have a problem.\n+                    match self.borrows.remove(cur) {\n+                        Item::FnBarrier(call) if global.is_active(call) => {\n                             return err!(MachineError(format!(\n-                                \"deallocating with active barrier ({})\", call\n-                            )))\n+                                \"not granting access because of barrier ({})\", call\n+                            )));\n                         }\n-                        _ => {},\n+                        _ => {}\n                     }\n                 }\n             }\n-            // Now we are done.\n-            return Ok(())\n         }\n-        // If we got here, we did not find our item.\n-        err!(MachineError(format!(\n-            \"borrow being accessed ({:?}) does not exist on the borrow stack\",\n-            bor\n-        )))\n-    }\n-\n-    /// Initiate `bor`; mostly this means pushing.\n-    /// This operation cannot fail; it is up to the caller to ensure that the precondition\n-    /// is met: We cannot push `Uniq` onto frozen stacks.\n-    /// `kind` indicates which kind of reference is being created.\n-    fn create(&mut self, bor: Borrow, kind: RefKind) {\n-        // When creating a frozen reference, freeze. This ensures F1.\n-        // We also do *not* push anything else to the stack, making sure that no nother kind\n-        // of access (like writing through raw pointers) is permitted.\n-        if kind == RefKind::Frozen {\n-            let bor_t = match bor {\n-                Borrow::Alias(Some(t)) => t,\n-                _ => bug!(\"Creating illegal borrow {:?} for frozen ref\", bor),\n-            };\n-            // It is possible that we already are frozen (e.g., if we just pushed a barrier,\n-            // the redundancy check would not have kicked in).\n-            match self.frozen_since {\n-                Some(loc_t) => assert!(\n-                    loc_t <= bor_t,\n-                    \"trying to freeze location for longer than it was already frozen\"\n-                ),\n-                None => {\n-                    trace!(\"create: Freezing\");\n-                    self.frozen_since = Some(bor_t);\n+\n+        // Post-processing.\n+        // If we got here, we found a matching item. Congratulations!\n+        // However, we are not done yet: If this access is deallocating, we must make sure\n+        // there are no active barriers remaining on the stack.\n+        if access == AccessKind::dealloc() {\n+            for &itm in self.borrows.iter().rev() {\n+                match itm {\n+                    Item::FnBarrier(call) if global.is_active(call) => {\n+                        return err!(MachineError(format!(\n+                            \"deallocating with active barrier ({})\", call\n+                        )))\n+                    }\n+                    _ => {},\n                 }\n             }\n-            return;\n         }\n-        assert!(\n-            self.frozen_since.is_none(),\n-            \"trying to create non-frozen reference to frozen location\"\n-        );\n \n-        // Push new item to the stack.\n-        let itm = match bor {\n-            Borrow::Uniq(t) => BorStackItem::Uniq(t),\n-            Borrow::Alias(_) => BorStackItem::Raw,\n-        };\n-        if *self.borrows.last().unwrap() == itm {\n-            // This is just an optimization, no functional change: Avoid stacking\n-            // multiple `Shr` on top of each other.\n-            assert!(bor.is_aliasing());\n-            trace!(\"create: sharing a shared location is a NOP\");\n-        } else {\n-            // This ensures U1.\n-            trace!(\"create: pushing {:?}\", itm);\n-            self.borrows.push(itm);\n+        // Done.\n+        return Ok(granting_idx);\n+    }\n+\n+    /// `reborrow` helper function.\n+    /// Grant `permisson` to new pointer tagged `tag`, added at `position` in the stack.\n+    fn grant(&mut self, perm: Permission, tag: Tag, position: usize) {\n+        // Simply add it to the \"stack\" -- this might add in the middle.\n+        // As an optimization, do nothing if the new item is identical to one of its neighbors.\n+        let item = Item::Permission(perm, tag);\n+        if self.borrows[position-1] == item || self.borrows.get(position) == Some(&item) {\n+            // Optimization applies, done.\n+            trace!(\"reborrow: avoiding redundant item {}\", item);\n+            return;\n         }\n+        trace!(\"reborrow: pushing item {}\", item);\n+        self.borrows.insert(position, item);\n     }\n \n+    /// `reborrow` helper function.\n     /// Adds a barrier.\n     fn barrier(&mut self, call: CallId) {\n-        let itm = BorStackItem::FnBarrier(call);\n+        let itm = Item::FnBarrier(call);\n         if *self.borrows.last().unwrap() == itm {\n             // This is just an optimization, no functional change: Avoid stacking\n             // multiple identical barriers on top of each other.\n             // This can happen when a function receives several shared references\n             // that overlap.\n-            trace!(\"barrier: avoiding redundant extra barrier\");\n+            trace!(\"reborrow: avoiding redundant extra barrier\");\n         } else {\n-            trace!(\"barrier: pushing barrier for call {}\", call);\n+            trace!(\"reborrow: pushing barrier for call {}\", call);\n             self.borrows.push(itm);\n         }\n     }\n+\n+    /// `reborrow` helper function: test that the stack invariants are still maintained.\n+    fn test_invariants(&self) {\n+        let mut saw_shared_read_only = false;\n+        for item in self.borrows.iter() {\n+            match item {\n+                Item::Permission(Permission::SharedReadOnly, _) => {\n+                    saw_shared_read_only = true;\n+                }\n+                Item::Permission(perm, _) if saw_shared_read_only => {\n+                    panic!(\"Found {:?} on top of a SharedReadOnly!\", perm);\n+                }\n+                _ => {}\n+            }\n+        }\n+    }\n+\n+    /// Derived a new pointer from one with the given tag .\n+    fn reborrow(\n+        &mut self,\n+        derived_from: Tag,\n+        barrier: Option<CallId>,\n+        new_kind: RefKind,\n+        new_tag: Tag,\n+        global: &GlobalState,\n+    ) -> EvalResult<'tcx> {\n+        // Find the permission \"from which we derive\".  To this end we first have to decide\n+        // if we derive from a permission that grants writes or just reads.\n+        let access = new_kind.access();\n+        let (derived_from_idx, derived_from_perm) = self.find_granting(access, derived_from)\n+            .ok_or_else(|| InterpError::MachineError(format!(\n+                    \"no item to reborrow as {} from tag {} found in borrow stack\", new_kind, derived_from,\n+            )))?;\n+        // With this we can compute the permission for the new pointer.\n+        let new_perm = new_kind.new_perm(derived_from_perm)?;\n+\n+        // We behave very differently for the \"unsafe\" case of a shared-read-write pointer\n+        // (\"unsafe\" because this also applies to shared references with interior mutability).\n+        // This is because such pointers may be reborrowed to unique pointers that actually\n+        // remain valid when their \"parents\" get further reborrows!\n+        if new_perm == Permission::SharedReadWrite {\n+            // A very liberal reborrow because the new pointer does not expect any kind of aliasing guarantee.\n+            // Just insert new permission as child of old permission, and maintain everything else.\n+            // This inserts \"as far down as possible\", which is good because it makes this pointer as\n+            // long-lived as possible *and* we want all the items that are incompatible with this\n+            // to actually get removed from the stack.  If we pushed a `SharedReadWrite` on top of\n+            // a `SharedReadOnly`, we'd violate the invariant that `SaredReadOnly` are at the top\n+            // and we'd allow write access without invalidating frozen shared references!\n+            self.grant(new_perm, new_tag, derived_from_idx+1);\n+\n+            // No barrier. They can rightfully alias with `&mut`.\n+            // FIXME: This means that the `dereferencable` attribute on non-frozen shared references\n+            // is incorrect! They are dereferencable when the function is called, but might become\n+            // non-dereferencable during the course of execution.\n+            // Also see [1], [2].\n+            //\n+            // [1]: <https://internals.rust-lang.org/t/\n+            //       is-it-possible-to-be-memory-safe-with-deallocated-self/8457/8>,\n+            // [2]: <https://lists.llvm.org/pipermail/llvm-dev/2018-July/124555.html>\n+        } else {\n+            // A \"safe\" reborrow for a pointer that actually expects some aliasing guarantees.\n+            // Here, creating a reference actually counts as an access, and pops incompatible\n+            // stuff off the stack.\n+            let check_idx = self.access(access, derived_from, global)?;\n+            assert_eq!(check_idx, derived_from_idx, \"somehow we saw different items??\");\n+\n+            // Now is a good time to add the barrier.\n+            if let Some(call) = barrier {\n+                self.barrier(call);\n+            }\n+\n+            // We insert \"as far up as possible\": We know only compatible items are remaining\n+            // on top of `derived_from`, and we want the new item at the top so that we\n+            // get the strongest possible guarantees.\n+            self.grant(new_perm, new_tag, self.borrows.len());\n+        }\n+\n+        // Make sure that after all this, the stack's invariant is still maintained.\n+        if cfg!(debug_assertions) {\n+            self.test_invariants();\n+        }\n+\n+        Ok(())\n+    }\n }\n \n /// Higher-level per-location operations: deref, access, reborrow.\n impl<'tcx> Stacks {\n-    /// Checks that this stack is fine with being dereferenced.\n-    fn deref(\n-        &self,\n-        ptr: Pointer<Borrow>,\n+    /// Creates new stack with initial tag.\n+    pub(crate) fn new(\n         size: Size,\n-        kind: RefKind,\n-    ) -> EvalResult<'tcx> {\n-        trace!(\"deref for tag {:?} as {:?}: {:?}, size {}\",\n-            ptr.tag, kind, ptr, size.bytes());\n-        let stacks = self.stacks.borrow();\n-        for stack in stacks.iter(ptr.offset, size) {\n-            stack.deref(ptr.tag, kind).map_err(InterpError::MachineError)?;\n+        tag: Tag,\n+        extra: MemoryState,\n+    ) -> Self {\n+        let item = Item::Permission(Permission::Unique, tag);\n+        let stack = Stack {\n+            borrows: vec![item],\n+        };\n+        Stacks {\n+            stacks: RefCell::new(RangeMap::new(size, stack)),\n+            global: extra,\n         }\n-        Ok(())\n     }\n \n     /// `ptr` got used, reflect that in the stack.\n     fn access(\n         &self,\n-        ptr: Pointer<Borrow>,\n+        ptr: Pointer<Tag>,\n         size: Size,\n         kind: AccessKind,\n     ) -> EvalResult<'tcx> {\n-        trace!(\"{:?} access of tag {:?}: {:?}, size {}\", kind, ptr.tag, ptr, size.bytes());\n+        trace!(\"{} access of tag {}: {:?}, size {}\", kind, ptr.tag, ptr, size.bytes());\n         // Even reads can have a side-effect, by invalidating other references.\n         // This is fundamentally necessary since `&mut` asserts that there\n         // are no accesses through other references, not even reads.\n-        let barrier_tracking = self.barrier_tracking.borrow();\n+        let global = self.global.borrow();\n         let mut stacks = self.stacks.borrow_mut();\n         for stack in stacks.iter_mut(ptr.offset, size) {\n-            stack.access(ptr.tag, kind, &*barrier_tracking)?;\n+            stack.access(kind, ptr.tag, &*global)?;\n         }\n         Ok(())\n     }\n@@ -408,154 +535,115 @@ impl<'tcx> Stacks {\n     /// This works on `&self` because we might encounter references to constant memory.\n     fn reborrow(\n         &self,\n-        ptr: Pointer<Borrow>,\n+        ptr: Pointer<Tag>,\n         size: Size,\n-        mut barrier: Option<CallId>,\n-        new_bor: Borrow,\n+        barrier: Option<CallId>,\n         new_kind: RefKind,\n+        new_tag: Tag,\n     ) -> EvalResult<'tcx> {\n-        assert_eq!(new_bor.is_unique(), new_kind == RefKind::Unique);\n         trace!(\n-            \"reborrow for tag {:?} to {:?} as {:?}: {:?}, size {}\",\n-            ptr.tag, new_bor, new_kind, ptr, size.bytes(),\n+            \"{} reborrow for tag {} to {}: {:?}, size {}\",\n+            new_kind, ptr.tag, new_tag, ptr, size.bytes(),\n         );\n-        if new_kind == RefKind::Raw {\n-            // No barrier for raw, including `&UnsafeCell`. They can rightfully alias with `&mut`.\n-            // FIXME: This means that the `dereferencable` attribute on non-frozen shared references\n-            // is incorrect! They are dereferencable when the function is called, but might become\n-            // non-dereferencable during the course of execution.\n-            // Also see [1], [2].\n-            //\n-            // [1]: <https://internals.rust-lang.org/t/\n-            //       is-it-possible-to-be-memory-safe-with-deallocated-self/8457/8>,\n-            // [2]: <https://lists.llvm.org/pipermail/llvm-dev/2018-July/124555.html>\n-            barrier = None;\n-        }\n-        let barrier_tracking = self.barrier_tracking.borrow();\n+        let global = self.global.borrow();\n         let mut stacks = self.stacks.borrow_mut();\n         for stack in stacks.iter_mut(ptr.offset, size) {\n-            // Access source `ptr`, create new ref.\n-            let ptr_idx = stack.deref(ptr.tag, new_kind).map_err(InterpError::MachineError)?;\n-            // If we can deref the new tag already, and if that tag lives higher on\n-            // the stack than the one we come from, just use that.\n-            // That is, we check if `new_bor` *already* is \"derived from\" `ptr.tag`.\n-            // This also checks frozenness, if required.\n-            let bor_redundant = barrier.is_none() &&\n-                match (ptr_idx, stack.deref(new_bor, new_kind)) {\n-                    // If the new borrow works with the frozen item, or else if it lives\n-                    // above the old one in the stack, our job here is done.\n-                    (_, Ok(None)) => true,\n-                    (Some(ptr_idx), Ok(Some(new_idx))) if new_idx >= ptr_idx => true,\n-                    // Otherwise, we need to create a new borrow.\n-                    _ => false,\n-                };\n-            if bor_redundant {\n-                assert!(new_bor.is_aliasing(), \"a unique reborrow can never be redundant\");\n-                trace!(\"reborrow is redundant\");\n-                continue;\n-            }\n-            // We need to do some actual work.\n-            let access_kind = if new_kind == RefKind::Unique {\n-                AccessKind::Write\n-            } else {\n-                AccessKind::Read\n-            };\n-            stack.access(ptr.tag, access_kind, &*barrier_tracking)?;\n-            if let Some(call) = barrier {\n-                stack.barrier(call);\n-            }\n-            stack.create(new_bor, new_kind);\n+            stack.reborrow(ptr.tag, barrier, new_kind, new_tag, &*global)?;\n         }\n         Ok(())\n     }\n }\n \n-/// Hooks and glue.\n-impl AllocationExtra<Borrow, MemoryState> for Stacks {\n-    #[inline(always)]\n-    fn memory_allocated<'tcx>(size: Size, extra: &MemoryState) -> Self {\n-        let stack = Stack {\n-            borrows: vec![BorStackItem::Raw],\n-            frozen_since: None,\n+// # Stacked Borrows Core End\n+\n+// Glue code to connect with Miri Machine Hooks\n+\n+impl Stacks {\n+    pub fn new_allocation(\n+        size: Size,\n+        extra: &MemoryState,\n+        kind: MemoryKind<MiriMemoryKind>,\n+    ) -> (Self, Tag) {\n+        let tag = match kind {\n+            MemoryKind::Stack => {\n+                // New unique borrow. This `Uniq` is not accessible by the program,\n+                // so it will only ever be used when using the local directly (i.e.,\n+                // not through a pointer). That is, whenever we directly use a local, this will pop\n+                // everything else off the stack, invalidating all previous pointers,\n+                // and in particular, *all* raw pointers. This subsumes the explicit\n+                // `reset` which the blog post [1] says to perform when accessing a local.\n+                //\n+                // [1]: <https://www.ralfj.de/blog/2018/08/07/stacked-borrows.html>\n+                Tag::Tagged(extra.borrow_mut().new_ptr())\n+            }\n+            _ => {\n+                Tag::Untagged\n+            }\n         };\n-        Stacks {\n-            stacks: RefCell::new(RangeMap::new(size, stack)),\n-            barrier_tracking: Rc::clone(extra),\n-        }\n+        let stack = Stacks::new(size, tag, Rc::clone(extra));\n+        (stack, tag)\n     }\n+}\n \n+impl AllocationExtra<Tag> for Stacks {\n     #[inline(always)]\n     fn memory_read<'tcx>(\n-        alloc: &Allocation<Borrow, Stacks>,\n-        ptr: Pointer<Borrow>,\n+        alloc: &Allocation<Tag, Stacks>,\n+        ptr: Pointer<Tag>,\n         size: Size,\n     ) -> EvalResult<'tcx> {\n         alloc.extra.access(ptr, size, AccessKind::Read)\n     }\n \n     #[inline(always)]\n     fn memory_written<'tcx>(\n-        alloc: &mut Allocation<Borrow, Stacks>,\n-        ptr: Pointer<Borrow>,\n+        alloc: &mut Allocation<Tag, Stacks>,\n+        ptr: Pointer<Tag>,\n         size: Size,\n     ) -> EvalResult<'tcx> {\n-        alloc.extra.access(ptr, size, AccessKind::Write)\n+        alloc.extra.access(ptr, size, AccessKind::write())\n     }\n \n     #[inline(always)]\n     fn memory_deallocated<'tcx>(\n-        alloc: &mut Allocation<Borrow, Stacks>,\n-        ptr: Pointer<Borrow>,\n+        alloc: &mut Allocation<Tag, Stacks>,\n+        ptr: Pointer<Tag>,\n         size: Size,\n     ) -> EvalResult<'tcx> {\n-        alloc.extra.access(ptr, size, AccessKind::Dealloc)\n-    }\n-}\n-\n-impl<'tcx> Stacks {\n-    /// Pushes the first item to the stacks.\n-    pub(crate) fn first_item(\n-        &mut self,\n-        itm: BorStackItem,\n-        size: Size\n-    ) {\n-        for stack in self.stacks.get_mut().iter_mut(Size::ZERO, size) {\n-            assert!(stack.borrows.len() == 1);\n-            assert_eq!(stack.borrows.pop().unwrap(), BorStackItem::Raw);\n-            stack.borrows.push(itm);\n-        }\n+        alloc.extra.access(ptr, size, AccessKind::dealloc())\n     }\n }\n \n impl<'a, 'mir, 'tcx> EvalContextPrivExt<'a, 'mir, 'tcx> for crate::MiriEvalContext<'a, 'mir, 'tcx> {}\n trait EvalContextPrivExt<'a, 'mir, 'tcx: 'a+'mir>: crate::MiriEvalContextExt<'a, 'mir, 'tcx> {\n     fn reborrow(\n         &mut self,\n-        place: MPlaceTy<'tcx, Borrow>,\n+        place: MPlaceTy<'tcx, Tag>,\n         size: Size,\n+        mutbl: Option<Mutability>,\n+        new_tag: Tag,\n         fn_barrier: bool,\n-        new_bor: Borrow\n     ) -> EvalResult<'tcx> {\n         let this = self.eval_context_mut();\n-        let ptr = place.ptr.to_ptr()?;\n         let barrier = if fn_barrier { Some(this.frame().extra) } else { None };\n+        let ptr = place.ptr.to_ptr()?;\n         trace!(\"reborrow: creating new reference for {:?} (pointee {}): {:?}\",\n-            ptr, place.layout.ty, new_bor);\n+            ptr, place.layout.ty, new_tag);\n \n         // Get the allocation. It might not be mutable, so we cannot use `get_mut`.\n         let alloc = this.memory().get(ptr.alloc_id)?;\n         alloc.check_bounds(this, ptr, size)?;\n         // Update the stacks.\n-        if let Borrow::Alias(Some(_)) = new_bor {\n+        if mutbl == Some(MutImmutable) {\n             // Reference that cares about freezing. We need a frozen-sensitive reborrow.\n             this.visit_freeze_sensitive(place, size, |cur_ptr, size, frozen| {\n-                let kind = if frozen { RefKind::Frozen } else { RefKind::Raw };\n-                alloc.extra.reborrow(cur_ptr, size, barrier, new_bor, kind)\n+                let new_kind = RefKind::Shared { frozen };\n+                alloc.extra.reborrow(cur_ptr, size, barrier, new_kind, new_tag)\n             })?;\n         } else {\n             // Just treat this as one big chunk.\n-            let kind = if new_bor.is_unique() { RefKind::Unique } else { RefKind::Raw };\n-            alloc.extra.reborrow(ptr, size, barrier, new_bor, kind)?;\n+            let new_kind = if mutbl == Some(MutMutable) { RefKind::Mutable } else { RefKind::Raw };\n+            alloc.extra.reborrow(ptr, size, barrier, new_kind, new_tag)?;\n         }\n         Ok(())\n     }\n@@ -564,11 +652,11 @@ trait EvalContextPrivExt<'a, 'mir, 'tcx: 'a+'mir>: crate::MiriEvalContextExt<'a,\n     /// `mutbl` can be `None` to make this a raw pointer.\n     fn retag_reference(\n         &mut self,\n-        val: ImmTy<'tcx, Borrow>,\n+        val: ImmTy<'tcx, Tag>,\n         mutbl: Option<Mutability>,\n         fn_barrier: bool,\n         two_phase: bool,\n-    ) -> EvalResult<'tcx, Immediate<Borrow>> {\n+    ) -> EvalResult<'tcx, Immediate<Tag>> {\n         let this = self.eval_context_mut();\n         // We want a place for where the ptr *points to*, so we get one.\n         let place = this.ref_to_mplace(val)?;\n@@ -581,23 +669,24 @@ trait EvalContextPrivExt<'a, 'mir, 'tcx: 'a+'mir>: crate::MiriEvalContextExt<'a,\n         }\n \n         // Compute new borrow.\n-        let time = this.machine.stacked_borrows.increment_clock();\n-        let new_bor = match mutbl {\n-            Some(MutMutable) => Borrow::Uniq(time),\n-            Some(MutImmutable) => Borrow::Alias(Some(time)),\n-            None => Borrow::default(),\n+        let new_tag = match mutbl {\n+            Some(_) => Tag::Tagged(this.memory().extra.borrow_mut().new_ptr()),\n+            None => Tag::Untagged,\n         };\n \n         // Reborrow.\n-        this.reborrow(place, size, fn_barrier, new_bor)?;\n-        let new_place = place.with_tag(new_bor);\n+        this.reborrow(place, size, mutbl, new_tag, fn_barrier)?;\n+        let new_place = place.replace_tag(new_tag);\n         // Handle two-phase borrows.\n         if two_phase {\n             assert!(mutbl == Some(MutMutable), \"two-phase shared borrows make no sense\");\n-            // We immediately share it, to allow read accesses\n-            let two_phase_time = this.machine.stacked_borrows.increment_clock();\n-            let two_phase_bor = Borrow::Alias(Some(two_phase_time));\n-            this.reborrow(new_place, size, false /* fn_barrier */, two_phase_bor)?;\n+            // Grant read access *to the parent pointer* with the old tag.  This means the same pointer\n+            // has multiple items in the stack now!\n+            // FIXME: Think about this some more, in particular about the interaction with cast-to-raw.\n+            // Maybe find a better way to express 2-phase, now that we have a \"more expressive language\"\n+            // in the stack.\n+            let old_tag = place.ptr.to_ptr().unwrap().tag;\n+            this.reborrow(new_place, size, Some(MutImmutable), old_tag, /* fn_barrier: */ false)?;\n         }\n \n         // Return new pointer.\n@@ -607,90 +696,10 @@ trait EvalContextPrivExt<'a, 'mir, 'tcx: 'a+'mir>: crate::MiriEvalContextExt<'a,\n \n impl<'a, 'mir, 'tcx> EvalContextExt<'a, 'mir, 'tcx> for crate::MiriEvalContext<'a, 'mir, 'tcx> {}\n pub trait EvalContextExt<'a, 'mir, 'tcx: 'a+'mir>: crate::MiriEvalContextExt<'a, 'mir, 'tcx> {\n-    fn tag_new_allocation(\n-        &mut self,\n-        id: AllocId,\n-        kind: MemoryKind<MiriMemoryKind>,\n-    ) -> Borrow {\n-        let this = self.eval_context_mut();\n-        let time = match kind {\n-            MemoryKind::Stack => {\n-                // New unique borrow. This `Uniq` is not accessible by the program,\n-                // so it will only ever be used when using the local directly (i.e.,\n-                // not through a pointer). That is, whenever we directly use a local, this will pop\n-                // everything else off the stack, invalidating all previous pointers,\n-                // and in particular, *all* raw pointers. This subsumes the explicit\n-                // `reset` which the blog post [1] says to perform when accessing a local.\n-                //\n-                // [1]: <https://www.ralfj.de/blog/2018/08/07/stacked-borrows.html>\n-                this.machine.stacked_borrows.increment_clock()\n-            }\n-            _ => {\n-                // Nothing to do for everything else.\n-                return Borrow::default()\n-            }\n-        };\n-        // Make this the active borrow for this allocation.\n-        let alloc = this\n-            .memory_mut()\n-            .get_mut(id)\n-            .expect(\"this is a new allocation; it must still exist\");\n-        let size = Size::from_bytes(alloc.bytes.len() as u64);\n-        alloc.extra.first_item(BorStackItem::Uniq(time), size);\n-        Borrow::Uniq(time)\n-    }\n-\n-    /// Called for value-to-place conversion. `mutability` is `None` for raw pointers.\n-    ///\n-    /// Note that this does *not* mean that all this memory will actually get accessed/referenced!\n-    /// We could be in the middle of `&(*var).1`.\n-    fn ptr_dereference(\n-        &self,\n-        place: MPlaceTy<'tcx, Borrow>,\n-        size: Size,\n-        mutability: Option<Mutability>,\n-    ) -> EvalResult<'tcx> {\n-        let this = self.eval_context_ref();\n-        trace!(\n-            \"ptr_dereference: Accessing {} reference for {:?} (pointee {})\",\n-            if let Some(mutability) = mutability {\n-                format!(\"{:?}\", mutability)\n-            } else {\n-                format!(\"raw\")\n-            },\n-            place.ptr, place.layout.ty\n-        );\n-        let ptr = place.ptr.to_ptr()?;\n-        if mutability.is_none() {\n-            // No further checks on raw derefs -- only the access itself will be checked.\n-            return Ok(());\n-        }\n-\n-        // Get the allocation\n-        let alloc = this.memory().get(ptr.alloc_id)?;\n-        alloc.check_bounds(this, ptr, size)?;\n-        // If we got here, we do some checking, *but* we leave the tag unchanged.\n-        if let Borrow::Alias(Some(_)) = ptr.tag {\n-            assert_eq!(mutability, Some(MutImmutable));\n-            // We need a frozen-sensitive check.\n-            this.visit_freeze_sensitive(place, size, |cur_ptr, size, frozen| {\n-                let kind = if frozen { RefKind::Frozen } else { RefKind::Raw };\n-                alloc.extra.deref(cur_ptr, size, kind)\n-            })?;\n-        } else {\n-            // Just treat this as one big chunk.\n-            let kind = if mutability == Some(MutMutable) { RefKind::Unique } else { RefKind::Raw };\n-            alloc.extra.deref(ptr, size, kind)?;\n-        }\n-\n-        // All is good.\n-        Ok(())\n-    }\n-\n     fn retag(\n         &mut self,\n         kind: RetagKind,\n-        place: PlaceTy<'tcx, Borrow>\n+        place: PlaceTy<'tcx, Tag>\n     ) -> EvalResult<'tcx> {\n         let this = self.eval_context_mut();\n         // Determine mutability and whether to add a barrier.\n@@ -734,15 +743,15 @@ pub trait EvalContextExt<'a, 'mir, 'tcx: 'a+'mir>: crate::MiriEvalContextExt<'a,\n         for\n             RetagVisitor<'ecx, 'a, 'mir, 'tcx>\n         {\n-            type V = MPlaceTy<'tcx, Borrow>;\n+            type V = MPlaceTy<'tcx, Tag>;\n \n             #[inline(always)]\n             fn ecx(&mut self) -> &mut MiriEvalContext<'a, 'mir, 'tcx> {\n                 &mut self.ecx\n             }\n \n             // Primitives of reference type, that is the one thing we are interested in.\n-            fn visit_primitive(&mut self, place: MPlaceTy<'tcx, Borrow>) -> EvalResult<'tcx>\n+            fn visit_primitive(&mut self, place: MPlaceTy<'tcx, Tag>) -> EvalResult<'tcx>\n             {\n                 // Cannot use `builtin_deref` because that reports *immutable* for `Box`,\n                 // making it useless."}, {"sha": "9346fba0dcc4b18036cc21576bbb8a98e6870231", "filename": "src/tls.rs", "status": "modified", "additions": 5, "deletions": 5, "changes": 10, "blob_url": "https://github.com/rust-lang/rust/blob/3f0a2a29414a67d9c52241a94ce373546ca2ddcf/src%2Ftls.rs", "raw_url": "https://github.com/rust-lang/rust/raw/3f0a2a29414a67d9c52241a94ce373546ca2ddcf/src%2Ftls.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ftls.rs?ref=3f0a2a29414a67d9c52241a94ce373546ca2ddcf", "patch": "@@ -5,14 +5,14 @@ use rustc::{ty, ty::layout::HasDataLayout, mir};\n \n use crate::{\n     EvalResult, InterpError, StackPopCleanup,\n-    MPlaceTy, Scalar, Borrow,\n+    MPlaceTy, Scalar, Tag,\n };\n \n pub type TlsKey = u128;\n \n #[derive(Copy, Clone, Debug)]\n pub struct TlsEntry<'tcx> {\n-    pub(crate) data: Scalar<Borrow>, // Will eventually become a map from thread IDs to `Scalar`s, if we ever support more than one thread.\n+    pub(crate) data: Scalar<Tag>, // Will eventually become a map from thread IDs to `Scalar`s, if we ever support more than one thread.\n     pub(crate) dtor: Option<ty::Instance<'tcx>>,\n }\n \n@@ -63,7 +63,7 @@ impl<'tcx> TlsData<'tcx> {\n         }\n     }\n \n-    pub fn load_tls(&mut self, key: TlsKey) -> EvalResult<'tcx, Scalar<Borrow>> {\n+    pub fn load_tls(&mut self, key: TlsKey) -> EvalResult<'tcx, Scalar<Tag>> {\n         match self.keys.get(&key) {\n             Some(&TlsEntry { data, .. }) => {\n                 trace!(\"TLS key {} loaded: {:?}\", key, data);\n@@ -73,7 +73,7 @@ impl<'tcx> TlsData<'tcx> {\n         }\n     }\n \n-    pub fn store_tls(&mut self, key: TlsKey, new_data: Scalar<Borrow>) -> EvalResult<'tcx> {\n+    pub fn store_tls(&mut self, key: TlsKey, new_data: Scalar<Tag>) -> EvalResult<'tcx> {\n         match self.keys.get_mut(&key) {\n             Some(&mut TlsEntry { ref mut data, .. }) => {\n                 trace!(\"TLS key {} stored: {:?}\", key, new_data);\n@@ -106,7 +106,7 @@ impl<'tcx> TlsData<'tcx> {\n         &mut self,\n         key: Option<TlsKey>,\n         cx: &impl HasDataLayout,\n-    ) -> Option<(ty::Instance<'tcx>, Scalar<Borrow>, TlsKey)> {\n+    ) -> Option<(ty::Instance<'tcx>, Scalar<Tag>, TlsKey)> {\n         use std::collections::Bound::*;\n \n         let thread_local = &mut self.keys;"}]}