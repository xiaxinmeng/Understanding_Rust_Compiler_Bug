{"sha": "5f1b1ec8fed0b4e79f3b00fac6698213d15b6027", "node_id": "MDY6Q29tbWl0NzI0NzEyOjVmMWIxZWM4ZmVkMGI0ZTc5ZjNiMDBmYWM2Njk4MjEzZDE1YjYwMjc=", "commit": {"author": {"name": "Piotr Czarnecki", "email": "pioczarn@gmail.com", "date": "2015-08-12T07:50:52Z"}, "committer": {"name": "Piotr Czarnecki", "email": "pioczarn@gmail.com", "date": "2016-01-05T10:02:43Z"}, "message": "Rework Arena code", "tree": {"sha": "a9cb9851b16427b546308181a26b6f11cae25043", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/a9cb9851b16427b546308181a26b6f11cae25043"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/5f1b1ec8fed0b4e79f3b00fac6698213d15b6027", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/5f1b1ec8fed0b4e79f3b00fac6698213d15b6027", "html_url": "https://github.com/rust-lang/rust/commit/5f1b1ec8fed0b4e79f3b00fac6698213d15b6027", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/5f1b1ec8fed0b4e79f3b00fac6698213d15b6027/comments", "author": {"login": "pczarn", "id": 3356767, "node_id": "MDQ6VXNlcjMzNTY3Njc=", "avatar_url": "https://avatars.githubusercontent.com/u/3356767?v=4", "gravatar_id": "", "url": "https://api.github.com/users/pczarn", "html_url": "https://github.com/pczarn", "followers_url": "https://api.github.com/users/pczarn/followers", "following_url": "https://api.github.com/users/pczarn/following{/other_user}", "gists_url": "https://api.github.com/users/pczarn/gists{/gist_id}", "starred_url": "https://api.github.com/users/pczarn/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/pczarn/subscriptions", "organizations_url": "https://api.github.com/users/pczarn/orgs", "repos_url": "https://api.github.com/users/pczarn/repos", "events_url": "https://api.github.com/users/pczarn/events{/privacy}", "received_events_url": "https://api.github.com/users/pczarn/received_events", "type": "User", "site_admin": false}, "committer": {"login": "pczarn", "id": 3356767, "node_id": "MDQ6VXNlcjMzNTY3Njc=", "avatar_url": "https://avatars.githubusercontent.com/u/3356767?v=4", "gravatar_id": "", "url": "https://api.github.com/users/pczarn", "html_url": "https://github.com/pczarn", "followers_url": "https://api.github.com/users/pczarn/followers", "following_url": "https://api.github.com/users/pczarn/following{/other_user}", "gists_url": "https://api.github.com/users/pczarn/gists{/gist_id}", "starred_url": "https://api.github.com/users/pczarn/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/pczarn/subscriptions", "organizations_url": "https://api.github.com/users/pczarn/orgs", "repos_url": "https://api.github.com/users/pczarn/repos", "events_url": "https://api.github.com/users/pczarn/events{/privacy}", "received_events_url": "https://api.github.com/users/pczarn/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "0d3160c1f1467b82e791b9902165a7054554cb38", "url": "https://api.github.com/repos/rust-lang/rust/commits/0d3160c1f1467b82e791b9902165a7054554cb38", "html_url": "https://github.com/rust-lang/rust/commit/0d3160c1f1467b82e791b9902165a7054554cb38"}], "stats": {"total": 167, "additions": 82, "deletions": 85}, "files": [{"sha": "3197a9e72bd31546ed73efefe9d9fa63cd290db8", "filename": "src/libarena/lib.rs", "status": "modified", "additions": 82, "deletions": 85, "changes": 167, "blob_url": "https://github.com/rust-lang/rust/blob/5f1b1ec8fed0b4e79f3b00fac6698213d15b6027/src%2Flibarena%2Flib.rs", "raw_url": "https://github.com/rust-lang/rust/raw/5f1b1ec8fed0b4e79f3b00fac6698213d15b6027/src%2Flibarena%2Flib.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibarena%2Flib.rs?ref=5f1b1ec8fed0b4e79f3b00fac6698213d15b6027", "patch": "@@ -50,12 +50,11 @@ use std::ptr;\n use alloc::heap;\n use alloc::raw_vec::RawVec;\n \n-// The way arena uses arrays is really deeply awful. The arrays are\n-// allocated, and have capacities reserved, but the fill for the array\n-// will always stay at 0.\n struct Chunk {\n     data: RawVec<u8>,\n+    /// Index of the first unused byte.\n     fill: Cell<usize>,\n+    /// Indicates whether objects with destructors are stored in this chunk.\n     is_copy: Cell<bool>,\n }\n \n@@ -75,12 +74,37 @@ impl Chunk {\n     unsafe fn as_ptr(&self) -> *const u8 {\n         self.data.ptr()\n     }\n+\n+    // Walk down a chunk, running the destructors for any objects stored\n+    // in it.\n+    unsafe fn destroy(&self) {\n+        let mut idx = 0;\n+        let buf = self.as_ptr();\n+        let fill = self.fill.get();\n+\n+        while idx < fill {\n+            let tydesc_data = buf.offset(idx as isize) as *const usize;\n+            let (tydesc, is_done) = un_bitpack_tydesc_ptr(*tydesc_data);\n+            let (size, align) = ((*tydesc).size, (*tydesc).align);\n+\n+            let after_tydesc = idx + mem::size_of::<*const TyDesc>();\n+\n+            let start = round_up(after_tydesc, align);\n+\n+            if is_done {\n+                ((*tydesc).drop_glue)(buf.offset(start as isize) as *const i8);\n+            }\n+\n+            // Find where the next tydesc lives\n+            idx = round_up(start + size, mem::align_of::<*const TyDesc>());\n+        }\n+    }\n }\n \n /// A slower reflection-based arena that can allocate objects of any type.\n ///\n-/// This arena uses `Vec<u8>` as a backing store to allocate objects from. For\n-/// each allocated object, the arena stores a pointer to the type descriptor\n+/// This arena uses `RawVec<u8>` as a backing store to allocate objects from.\n+/// For each allocated object, the arena stores a pointer to the type descriptor\n /// followed by the object (potentially with alignment padding after each\n /// element). When the arena is destroyed, it iterates through all of its\n /// chunks, and uses the tydesc information to trace through the objects,\n@@ -127,10 +151,10 @@ impl<'a> Arena<'a> {\n impl<'longer_than_self> Drop for Arena<'longer_than_self> {\n     fn drop(&mut self) {\n         unsafe {\n-            destroy_chunk(&*self.head.borrow());\n+            self.head.borrow().destroy();\n             for chunk in self.chunks.borrow().iter() {\n                 if !chunk.is_copy.get() {\n-                    destroy_chunk(chunk);\n+                    chunk.destroy();\n                 }\n             }\n         }\n@@ -142,31 +166,6 @@ fn round_up(base: usize, align: usize) -> usize {\n     (base.checked_add(align - 1)).unwrap() & !(align - 1)\n }\n \n-// Walk down a chunk, running the destructors for any objects stored\n-// in it.\n-unsafe fn destroy_chunk(chunk: &Chunk) {\n-    let mut idx = 0;\n-    let buf = chunk.as_ptr();\n-    let fill = chunk.fill.get();\n-\n-    while idx < fill {\n-        let tydesc_data = buf.offset(idx as isize) as *const usize;\n-        let (tydesc, is_done) = un_bitpack_tydesc_ptr(*tydesc_data);\n-        let (size, align) = ((*tydesc).size, (*tydesc).align);\n-\n-        let after_tydesc = idx + mem::size_of::<*const TyDesc>();\n-\n-        let start = round_up(after_tydesc, align);\n-\n-        if is_done {\n-            ((*tydesc).drop_glue)(buf.offset(start as isize) as *const i8);\n-        }\n-\n-        // Find where the next tydesc lives\n-        idx = round_up(start + size, mem::align_of::<*const TyDesc>());\n-    }\n-}\n-\n // We encode whether the object a tydesc describes has been\n // initialized in the arena in the low bit of the tydesc pointer. This\n // is necessary in order to properly do cleanup if a panic occurs\n@@ -183,6 +182,9 @@ fn un_bitpack_tydesc_ptr(p: usize) -> (*const TyDesc, bool) {\n // HACK(eddyb) TyDesc replacement using a trait object vtable.\n // This could be replaced in the future with a custom DST layout,\n // or `&'static (drop_glue, size, align)` created by a `const fn`.\n+// Requirements:\n+// * rvalue promotion (issue #1056)\n+// * mem::{size_of, align_of} must be const fns\n struct TyDesc {\n     drop_glue: fn(*const i8),\n     size: usize,\n@@ -198,45 +200,52 @@ impl<T: ?Sized> AllTypes for T {}\n unsafe fn get_tydesc<T>() -> *const TyDesc {\n     use std::raw::TraitObject;\n \n-    let ptr = &*(1 as *const T);\n+    let ptr = &*(heap::EMPTY as *const T);\n \n     // Can use any trait that is implemented for all types.\n     let obj = mem::transmute::<&AllTypes, TraitObject>(ptr);\n     obj.vtable as *const TyDesc\n }\n \n impl<'longer_than_self> Arena<'longer_than_self> {\n-    #[inline]\n-    fn chunk_size(&self) -> usize {\n-        self.copy_head.borrow().capacity()\n-    }\n-\n-    // Functions for the POD part of the arena\n+    // Grows a given chunk and returns `false`, or replaces it with a bigger\n+    // chunk and returns `true`.\n+    // This method is shared by both parts of the arena.\n     #[cold]\n-    fn alloc_copy_grow(&self, n_bytes: usize, align: usize) -> *const u8 {\n-        // Allocate a new chunk.\n-        let new_min_chunk_size = cmp::max(n_bytes, self.chunk_size());\n-        let new_chunk = Chunk::new((new_min_chunk_size + 1).next_power_of_two(), true);\n-        let mut copy_head = self.copy_head.borrow_mut();\n-        let old_chunk = mem::replace(&mut *copy_head, new_chunk);\n-        self.chunks.borrow_mut().push(old_chunk);\n-\n-        self.alloc_copy_inner(n_bytes, align)\n+    fn alloc_grow(&self, head: &mut Chunk, used_cap: usize, n_bytes: usize) -> bool {\n+        if head.data.reserve_in_place(used_cap, n_bytes) {\n+            // In-place reallocation succeeded.\n+            false\n+        } else {\n+            // Allocate a new chunk.\n+            let new_min_chunk_size = cmp::max(n_bytes, head.capacity());\n+            let new_chunk = Chunk::new((new_min_chunk_size + 1).next_power_of_two(), false);\n+            let old_chunk = mem::replace(head, new_chunk);\n+            if old_chunk.fill.get() != 0 {\n+                self.chunks.borrow_mut().push(old_chunk);\n+            }\n+            true\n+        }\n     }\n \n+    // Functions for the copyable part of the arena.\n+\n     #[inline]\n     fn alloc_copy_inner(&self, n_bytes: usize, align: usize) -> *const u8 {\n-        let start = round_up(self.copy_head.borrow().fill.get(), align);\n-        let chunk_size = self.chunk_size();\n-\n-        let end = start + n_bytes;\n-        if end > chunk_size {\n-            if !self.copy_head.borrow_mut().data.reserve_in_place(start, n_bytes) {\n-                return self.alloc_copy_grow(n_bytes, align);\n+        let mut copy_head = self.copy_head.borrow_mut();\n+        let fill = copy_head.fill.get();\n+        let mut start = round_up(fill, align);\n+        let mut end = start + n_bytes;\n+\n+        if end > copy_head.capacity() {\n+            if self.alloc_grow(&mut *copy_head, fill, end - fill) {\n+                // Continuing with a newly allocated chunk\n+                start = 0;\n+                end = n_bytes;\n+                copy_head.is_copy.set(true);\n             }\n         }\n \n-        let copy_head = self.copy_head.borrow();\n         copy_head.fill.set(end);\n \n         unsafe { copy_head.as_ptr().offset(start as isize) }\n@@ -254,40 +263,28 @@ impl<'longer_than_self> Arena<'longer_than_self> {\n         }\n     }\n \n-    // Functions for the non-POD part of the arena\n-    fn alloc_noncopy_grow(&self, n_bytes: usize, align: usize) -> (*const u8, *const u8) {\n-        // Allocate a new chunk.\n-        let new_min_chunk_size = cmp::max(n_bytes, self.chunk_size());\n-        let new_chunk = Chunk::new((new_min_chunk_size + 1).next_power_of_two(), false);\n-        let mut head = self.head.borrow_mut();\n-        let old_chunk = mem::replace(&mut *head, new_chunk);\n-        self.chunks.borrow_mut().push(old_chunk);\n-\n-        self.alloc_noncopy_inner(n_bytes, align)\n-    }\n+    // Functions for the non-copyable part of the arena.\n \n     #[inline]\n     fn alloc_noncopy_inner(&self, n_bytes: usize, align: usize) -> (*const u8, *const u8) {\n-        // Be careful to not maintain any `head` borrows active, because\n-        // `alloc_noncopy_grow` borrows it mutably.\n-        let (start, end, tydesc_start, head_capacity) = {\n-            let head = self.head.borrow();\n-            let fill = head.fill.get();\n-\n-            let tydesc_start = fill;\n-            let after_tydesc = fill + mem::size_of::<*const TyDesc>();\n-            let start = round_up(after_tydesc, align);\n-            let end = start + n_bytes;\n-\n-            (start, end, tydesc_start, head.capacity())\n-        };\n-\n-        if end > head_capacity {\n-            return self.alloc_noncopy_grow(n_bytes, align);\n+        let mut head = self.head.borrow_mut();\n+        let fill = head.fill.get();\n+\n+        let mut tydesc_start = fill;\n+        let after_tydesc = fill + mem::size_of::<*const TyDesc>();\n+        let mut start = round_up(after_tydesc, align);\n+        let mut end = round_up(start + n_bytes, mem::align_of::<*const TyDesc>());\n+\n+        if end > head.capacity() {\n+            if self.alloc_grow(&mut *head, tydesc_start, end - tydesc_start) {\n+                // Continuing with a newly allocated chunk\n+                tydesc_start = 0;\n+                start = round_up(mem::size_of::<*const TyDesc>(), align);\n+                end = round_up(start + n_bytes, mem::align_of::<*const TyDesc>());\n+            }\n         }\n \n-        let head = self.head.borrow();\n-        head.fill.set(round_up(end, mem::align_of::<*const TyDesc>()));\n+        head.fill.set(end);\n \n         unsafe {\n             let buf = head.as_ptr();"}]}