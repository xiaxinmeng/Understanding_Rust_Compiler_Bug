{"sha": "bfa9064ba2687eb1d95708f72f41ddd9729a6ba1", "node_id": "MDY6Q29tbWl0NzI0NzEyOmJmYTkwNjRiYTI2ODdlYjFkOTU3MDhmNzJmNDFkZGQ5NzI5YTZiYTE=", "commit": {"author": {"name": "Alex Crichton", "email": "alex@alexcrichton.com", "date": "2013-12-06T01:56:17Z"}, "committer": {"name": "Alex Crichton", "email": "alex@alexcrichton.com", "date": "2013-12-17T01:47:11Z"}, "message": "Rewrite std::comm\n\n* Streams are now ~3x faster than before (fewer allocations and more optimized)\n    * Based on a single-producer single-consumer lock-free queue that doesn't\n      always have to allocate on every send.\n    * Blocking via mutexes/cond vars outside the runtime\n* Streams work in/out of the runtime seamlessly\n* Select now works in/out of the runtime seamlessly\n* Streams will now fail!() on send() if the other end has hung up\n    * try_send() will not fail\n* PortOne/ChanOne removed\n* SharedPort removed\n* MegaPipe removed\n* Generic select removed (only one kind of port now)\n* API redesign\n    * try_recv == never block\n    * recv_opt == block, don't fail\n    * iter() == Iterator<T> for Port<T>\n    * removed peek\n    * Type::new\n* Removed rt::comm", "tree": {"sha": "b10aeff181eff3a8654df495d2ad8826490f6533", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/b10aeff181eff3a8654df495d2ad8826490f6533"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/bfa9064ba2687eb1d95708f72f41ddd9729a6ba1", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/bfa9064ba2687eb1d95708f72f41ddd9729a6ba1", "html_url": "https://github.com/rust-lang/rust/commit/bfa9064ba2687eb1d95708f72f41ddd9729a6ba1", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/bfa9064ba2687eb1d95708f72f41ddd9729a6ba1/comments", "author": {"login": "alexcrichton", "id": 64996, "node_id": "MDQ6VXNlcjY0OTk2", "avatar_url": "https://avatars.githubusercontent.com/u/64996?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alexcrichton", "html_url": "https://github.com/alexcrichton", "followers_url": "https://api.github.com/users/alexcrichton/followers", "following_url": "https://api.github.com/users/alexcrichton/following{/other_user}", "gists_url": "https://api.github.com/users/alexcrichton/gists{/gist_id}", "starred_url": "https://api.github.com/users/alexcrichton/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alexcrichton/subscriptions", "organizations_url": "https://api.github.com/users/alexcrichton/orgs", "repos_url": "https://api.github.com/users/alexcrichton/repos", "events_url": "https://api.github.com/users/alexcrichton/events{/privacy}", "received_events_url": "https://api.github.com/users/alexcrichton/received_events", "type": "User", "site_admin": false}, "committer": {"login": "alexcrichton", "id": 64996, "node_id": "MDQ6VXNlcjY0OTk2", "avatar_url": "https://avatars.githubusercontent.com/u/64996?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alexcrichton", "html_url": "https://github.com/alexcrichton", "followers_url": "https://api.github.com/users/alexcrichton/followers", "following_url": "https://api.github.com/users/alexcrichton/following{/other_user}", "gists_url": "https://api.github.com/users/alexcrichton/gists{/gist_id}", "starred_url": "https://api.github.com/users/alexcrichton/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alexcrichton/subscriptions", "organizations_url": "https://api.github.com/users/alexcrichton/orgs", "repos_url": "https://api.github.com/users/alexcrichton/repos", "events_url": "https://api.github.com/users/alexcrichton/events{/privacy}", "received_events_url": "https://api.github.com/users/alexcrichton/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "000cda611f8224ac780fa37432f869f425cd2bb7", "url": "https://api.github.com/repos/rust-lang/rust/commits/000cda611f8224ac780fa37432f869f425cd2bb7", "html_url": "https://github.com/rust-lang/rust/commit/000cda611f8224ac780fa37432f869f425cd2bb7"}], "stats": {"total": 3065, "additions": 2631, "deletions": 434}, "files": [{"sha": "c5ed464de23c5a972c65ea83d808a87ebc063fea", "filename": "src/libstd/comm.rs", "status": "removed", "additions": 0, "deletions": 311, "changes": 311, "blob_url": "https://github.com/rust-lang/rust/blob/000cda611f8224ac780fa37432f869f425cd2bb7/src%2Flibstd%2Fcomm.rs", "raw_url": "https://github.com/rust-lang/rust/raw/000cda611f8224ac780fa37432f869f425cd2bb7/src%2Flibstd%2Fcomm.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibstd%2Fcomm.rs?ref=000cda611f8224ac780fa37432f869f425cd2bb7", "patch": "@@ -1,311 +0,0 @@\n-// Copyright 2012 The Rust Project Developers. See the COPYRIGHT\n-// file at the top-level directory of this distribution and at\n-// http://rust-lang.org/COPYRIGHT.\n-//\n-// Licensed under the Apache License, Version 2.0 <LICENSE-APACHE or\n-// http://www.apache.org/licenses/LICENSE-2.0> or the MIT license\n-// <LICENSE-MIT or http://opensource.org/licenses/MIT>, at your\n-// option. This file may not be copied, modified, or distributed\n-// except according to those terms.\n-\n-/*!\n-Message passing\n-*/\n-\n-#[allow(missing_doc)];\n-\n-use clone::Clone;\n-use iter::Iterator;\n-use kinds::Send;\n-use option::Option;\n-use rtcomm = rt::comm;\n-\n-/// A trait for things that can send multiple messages.\n-pub trait GenericChan<T> {\n-    /// Sends a message.\n-    fn send(&self, x: T);\n-}\n-\n-/// Things that can send multiple messages and can detect when the receiver\n-/// is closed\n-pub trait GenericSmartChan<T> {\n-    /// Sends a message, or report if the receiver has closed the connection.\n-    fn try_send(&self, x: T) -> bool;\n-}\n-\n-/// Trait for non-rescheduling send operations, similar to `send_deferred` on ChanOne.\n-pub trait SendDeferred<T> {\n-    fn send_deferred(&self, val: T);\n-    fn try_send_deferred(&self, val: T) -> bool;\n-}\n-\n-/// A trait for things that can receive multiple messages.\n-pub trait GenericPort<T> {\n-    /// Receives a message, or fails if the connection closes.\n-    fn recv(&self) -> T;\n-\n-    /// Receives a message, or returns `none` if\n-    /// the connection is closed or closes.\n-    fn try_recv(&self) -> Option<T>;\n-\n-    /// Returns an iterator that breaks once the connection closes.\n-    ///\n-    /// # Example\n-    ///\n-    /// ~~~rust\n-    /// do spawn {\n-    ///     for x in port.recv_iter() {\n-    ///         if pred(x) { break; }\n-    ///         println!(\"{}\", x);\n-    ///     }\n-    /// }\n-    /// ~~~\n-    fn recv_iter<'a>(&'a self) -> RecvIterator<'a, Self> {\n-        RecvIterator { port: self }\n-    }\n-}\n-\n-pub struct RecvIterator<'a, P> {\n-    priv port: &'a P,\n-}\n-\n-impl<'a, T, P: GenericPort<T>> Iterator<T> for RecvIterator<'a, P> {\n-    fn next(&mut self) -> Option<T> {\n-        self.port.try_recv()\n-    }\n-}\n-\n-/// Ports that can `peek`\n-pub trait Peekable<T> {\n-    /// Returns true if a message is available\n-    fn peek(&self) -> bool;\n-}\n-\n-/* priv is disabled to allow users to get at traits like Select. */\n-pub struct PortOne<T> { /* priv */ x: rtcomm::PortOne<T> }\n-pub struct ChanOne<T> { /* priv */ x: rtcomm::ChanOne<T> }\n-\n-pub fn oneshot<T: Send>() -> (PortOne<T>, ChanOne<T>) {\n-    let (p, c) = rtcomm::oneshot();\n-    (PortOne { x: p }, ChanOne { x: c })\n-}\n-\n-pub struct Port<T> { /* priv */ x: rtcomm::Port<T> }\n-pub struct Chan<T> { /* priv */ x: rtcomm::Chan<T> }\n-\n-pub fn stream<T: Send>() -> (Port<T>, Chan<T>) {\n-    let (p, c) = rtcomm::stream();\n-    (Port { x: p }, Chan { x: c })\n-}\n-\n-impl<T: Send> ChanOne<T> {\n-    pub fn send(self, val: T) {\n-        let ChanOne { x: c } = self;\n-        c.send(val)\n-    }\n-\n-    pub fn try_send(self, val: T) -> bool {\n-        let ChanOne { x: c } = self;\n-        c.try_send(val)\n-    }\n-\n-    pub fn send_deferred(self, val: T) {\n-        let ChanOne { x: c } = self;\n-        c.send_deferred(val)\n-    }\n-\n-    pub fn try_send_deferred(self, val: T) -> bool {\n-        let ChanOne{ x: c } = self;\n-        c.try_send_deferred(val)\n-    }\n-}\n-\n-impl<T: Send> PortOne<T> {\n-    pub fn recv(self) -> T {\n-        let PortOne { x: p } = self;\n-        p.recv()\n-    }\n-\n-    pub fn try_recv(self) -> Option<T> {\n-        let PortOne { x: p } = self;\n-        p.try_recv()\n-    }\n-}\n-\n-impl<T: Send> Peekable<T>  for PortOne<T> {\n-    fn peek(&self) -> bool {\n-        let &PortOne { x: ref p } = self;\n-        p.peek()\n-    }\n-}\n-\n-impl<T: Send> GenericChan<T> for Chan<T> {\n-    fn send(&self, val: T) {\n-        let &Chan { x: ref c } = self;\n-        c.send(val)\n-    }\n-}\n-\n-impl<T: Send> GenericSmartChan<T> for Chan<T> {\n-    fn try_send(&self, val: T) -> bool {\n-        let &Chan { x: ref c } = self;\n-        c.try_send(val)\n-    }\n-}\n-\n-impl<T: Send> SendDeferred<T> for Chan<T> {\n-    fn send_deferred(&self, val: T) {\n-        let &Chan { x: ref c } = self;\n-        c.send_deferred(val)\n-    }\n-\n-    fn try_send_deferred(&self, val: T) -> bool {\n-        let &Chan { x: ref c } = self;\n-        c.try_send_deferred(val)\n-    }\n-}\n-\n-impl<T: Send> GenericPort<T> for Port<T> {\n-    fn recv(&self) -> T {\n-        let &Port { x: ref p } = self;\n-        p.recv()\n-    }\n-\n-    fn try_recv(&self) -> Option<T> {\n-        let &Port { x: ref p } = self;\n-        p.try_recv()\n-    }\n-}\n-\n-impl<T: Send> Peekable<T> for Port<T> {\n-    fn peek(&self) -> bool {\n-        let &Port { x: ref p } = self;\n-        p.peek()\n-    }\n-}\n-\n-\n-pub struct SharedChan<T> { /* priv */ x: rtcomm::SharedChan<T> }\n-\n-impl<T: Send> SharedChan<T> {\n-    pub fn new(c: Chan<T>) -> SharedChan<T> {\n-        let Chan { x: c } = c;\n-        SharedChan { x: rtcomm::SharedChan::new(c) }\n-    }\n-}\n-\n-impl<T: Send> GenericChan<T> for SharedChan<T> {\n-    fn send(&self, val: T) {\n-        let &SharedChan { x: ref c } = self;\n-        c.send(val)\n-    }\n-}\n-\n-impl<T: Send> GenericSmartChan<T> for SharedChan<T> {\n-    fn try_send(&self, val: T) -> bool {\n-        let &SharedChan { x: ref c } = self;\n-        c.try_send(val)\n-    }\n-}\n-\n-impl<T: Send> SendDeferred<T> for SharedChan<T> {\n-    fn send_deferred(&self, val: T) {\n-        let &SharedChan { x: ref c } = self;\n-        c.send_deferred(val)\n-    }\n-\n-    fn try_send_deferred(&self, val: T) -> bool {\n-        let &SharedChan { x: ref c } = self;\n-        c.try_send_deferred(val)\n-    }\n-}\n-\n-impl<T: Send> Clone for SharedChan<T> {\n-    fn clone(&self) -> SharedChan<T> {\n-        let &SharedChan { x: ref c } = self;\n-        SharedChan { x: c.clone() }\n-    }\n-}\n-\n-pub struct SharedPort<T> { /* priv */ x: rtcomm::SharedPort<T> }\n-\n-impl<T: Send> SharedPort<T> {\n-    pub fn new(p: Port<T>) -> SharedPort<T> {\n-        let Port { x: p } = p;\n-        SharedPort { x: rtcomm::SharedPort::new(p) }\n-    }\n-}\n-\n-impl<T: Send> GenericPort<T> for SharedPort<T> {\n-    fn recv(&self) -> T {\n-        let &SharedPort { x: ref p } = self;\n-        p.recv()\n-    }\n-\n-    fn try_recv(&self) -> Option<T> {\n-        let &SharedPort { x: ref p } = self;\n-        p.try_recv()\n-    }\n-}\n-\n-impl<T: Send> Clone for SharedPort<T> {\n-    fn clone(&self) -> SharedPort<T> {\n-        let &SharedPort { x: ref p } = self;\n-        SharedPort { x: p.clone() }\n-    }\n-}\n-\n-#[cfg(test)]\n-mod tests {\n-    use comm::*;\n-    use prelude::*;\n-\n-    #[test]\n-    fn test_nested_recv_iter() {\n-        let (port, chan) = stream::<int>();\n-        let (total_port, total_chan) = oneshot::<int>();\n-\n-        do spawn {\n-            let mut acc = 0;\n-            for x in port.recv_iter() {\n-                acc += x;\n-                for x in port.recv_iter() {\n-                    acc += x;\n-                    for x in port.try_recv().move_iter() {\n-                        acc += x;\n-                        total_chan.send(acc);\n-                    }\n-                }\n-            }\n-        }\n-\n-        chan.send(3);\n-        chan.send(1);\n-        chan.send(2);\n-        assert_eq!(total_port.recv(), 6);\n-    }\n-\n-    #[test]\n-    fn test_recv_iter_break() {\n-        let (port, chan) = stream::<int>();\n-        let (count_port, count_chan) = oneshot::<int>();\n-\n-        do spawn {\n-            let mut count = 0;\n-            for x in port.recv_iter() {\n-                if count >= 3 {\n-                    count_chan.send(count);\n-                    break;\n-                } else {\n-                    count += x;\n-                }\n-            }\n-        }\n-\n-        chan.send(2);\n-        chan.send(2);\n-        chan.send(2);\n-        chan.send(2);\n-        assert_eq!(count_port.recv(), 4);\n-    }\n-}"}, {"sha": "bd1d6fed901caf9b382bdbe949133582ab09bcea", "filename": "src/libstd/comm/imp.rs", "status": "added", "additions": 337, "deletions": 0, "changes": 337, "blob_url": "https://github.com/rust-lang/rust/blob/bfa9064ba2687eb1d95708f72f41ddd9729a6ba1/src%2Flibstd%2Fcomm%2Fimp.rs", "raw_url": "https://github.com/rust-lang/rust/raw/bfa9064ba2687eb1d95708f72f41ddd9729a6ba1/src%2Flibstd%2Fcomm%2Fimp.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibstd%2Fcomm%2Fimp.rs?ref=bfa9064ba2687eb1d95708f72f41ddd9729a6ba1", "patch": "@@ -0,0 +1,337 @@\n+// Copyright 2013 The Rust Project Developers. See the COPYRIGHT\n+// file at the top-level directory of this distribution and at\n+// http://rust-lang.org/COPYRIGHT.\n+//\n+// Licensed under the Apache License, Version 2.0 <LICENSE-APACHE or\n+// http://www.apache.org/licenses/LICENSE-2.0> or the MIT license\n+// <LICENSE-MIT or http://opensource.org/licenses/MIT>, at your\n+// option. This file may not be copied, modified, or distributed\n+// except according to those terms.\n+\n+//! One of the major goals behind this channel implementation is to work\n+//! seamlessly on and off the runtime. This also means that the code isn't\n+//! littered with \"if is_green() { ... } else { ... }\". Right now, the rest of\n+//! the runtime isn't quite ready to for this abstraction to be done very\n+//! nicely, so the conditional \"if green\" blocks are all contained in this inner\n+//! module.\n+//!\n+//! The goal of this module is to mirror what the runtime \"should be\", not the\n+//! state that it is currently in today. You'll notice that there is no mention\n+//! of schedulers or is_green inside any of the channel code, it is currently\n+//! entirely contained in this one module.\n+//!\n+//! In the ideal world, nothing in this module exists and it is all implemented\n+//! elsewhere in the runtime (in the proper location). All of this code is\n+//! structured in order to easily refactor this to the correct location whenever\n+//! we have the trait objects in place to serve as the boundary of the\n+//! abstraction.\n+\n+use iter::{range, Iterator};\n+use ops::Drop;\n+use option::{Some, None, Option};\n+use rt::local::Local;\n+use rt::sched::{SchedHandle, Scheduler, TaskFromFriend};\n+use rt::thread::Thread;\n+use rt;\n+use unstable::mutex::Mutex;\n+use unstable::sync::UnsafeArc;\n+\n+// A task handle is a method of waking up a blocked task. The handle itself\n+// is completely opaque and only has a wake() method defined on it. This\n+// method will wake the method regardless of the context of the thread which\n+// is currently calling wake().\n+//\n+// This abstraction should be able to be created when putting a task to\n+// sleep. This should basically be a method on whatever the local Task is,\n+// consuming the local Task.\n+\n+pub struct TaskHandle {\n+    priv inner: TaskRepr\n+}\n+enum TaskRepr {\n+    Green(rt::BlockedTask, *mut SchedHandle),\n+    Native(NativeWakeupStyle),\n+}\n+enum NativeWakeupStyle {\n+    ArcWakeup(UnsafeArc<Mutex>),    // shared mutex to synchronize on\n+    LocalWakeup(*mut Mutex),        // synchronize on the task-local mutex\n+}\n+\n+impl TaskHandle {\n+    // Signal that this handle should be woken up. The `can_resched`\n+    // argument indicates whether the current task could possibly be\n+    // rescheduled or not. This does not have a lot of meaning for the\n+    // native case, but for an M:N case it indicates whether a context\n+    // switch can happen or not.\n+    pub fn wake(self, can_resched: bool) {\n+        match self.inner {\n+            Green(task, handle) => {\n+                // If we have a local scheduler, then use that to run the\n+                // blocked task, otherwise we can use the handle to send the\n+                // task back to its home.\n+                if rt::in_green_task_context() {\n+                    if can_resched {\n+                        task.wake().map(Scheduler::run_task);\n+                    } else {\n+                        let mut s: ~Scheduler = Local::take();\n+                        s.enqueue_blocked_task(task);\n+                        Local::put(s);\n+                    }\n+                } else {\n+                    let task = match task.wake() {\n+                        Some(task) => task, None => return\n+                    };\n+                    // XXX: this is not an easy section of code to refactor.\n+                    //      If this handle is owned by the Task (which it\n+                    //      should be), then this would be a use-after-free\n+                    //      because once the task is pushed onto the message\n+                    //      queue, the handle is gone.\n+                    //\n+                    //      Currently the handle is instead owned by the\n+                    //      Port/Chan pair, which means that because a\n+                    //      channel is invoking this method the handle will\n+                    //      continue to stay alive for the entire duration\n+                    //      of this method. This will require thought when\n+                    //      moving the handle into the task.\n+                    unsafe { (*handle).send(TaskFromFriend(task)) }\n+                }\n+            }\n+\n+            // Note that there are no use-after-free races in this code. In\n+            // the arc-case, we own the lock, and in the local case, we're\n+            // using a lock so it's guranteed that they aren't running while\n+            // we hold the lock.\n+            Native(ArcWakeup(lock)) => {\n+                unsafe {\n+                    let lock = lock.get();\n+                    (*lock).lock();\n+                    (*lock).signal();\n+                    (*lock).unlock();\n+                }\n+            }\n+            Native(LocalWakeup(lock)) => {\n+                unsafe {\n+                    (*lock).lock();\n+                    (*lock).signal();\n+                    (*lock).unlock();\n+                }\n+            }\n+        }\n+    }\n+\n+    // Trashes handle to this task. This ensures that necessary memory is\n+    // deallocated, and there may be some extra assertions as well.\n+    pub fn trash(self) {\n+        match self.inner {\n+            Green(task, _) => task.assert_already_awake(),\n+            Native(..) => {}\n+        }\n+    }\n+}\n+\n+// This structure is an abstraction of what should be stored in the local\n+// task itself. This data is currently stored inside of each channel, but\n+// this should rather be stored in each task (and channels will still\n+// continue to lazily initialize this data).\n+\n+pub struct TaskData {\n+    priv handle: Option<SchedHandle>,\n+    priv lock: Mutex,\n+}\n+\n+impl TaskData {\n+    pub fn new() -> TaskData {\n+        TaskData {\n+            handle: None,\n+            lock: unsafe { Mutex::empty() },\n+        }\n+    }\n+}\n+\n+impl Drop for TaskData {\n+    fn drop(&mut self) {\n+        unsafe { self.lock.destroy() }\n+    }\n+}\n+\n+// Now this is the really fun part. This is where all the M:N/1:1-agnostic\n+// along with recv/select-agnostic blocking information goes. A \"blocking\n+// context\" is really just a stack-allocated structure (which is probably\n+// fine to be a stack-trait-object).\n+//\n+// This has some particularly strange interfaces, but the reason for all\n+// this is to support selection/recv/1:1/M:N all in one bundle.\n+\n+pub struct BlockingContext<'a> {\n+    priv inner: BlockingRepr<'a>\n+}\n+\n+enum BlockingRepr<'a> {\n+    GreenBlock(rt::BlockedTask, &'a mut Scheduler),\n+    NativeBlock(Option<UnsafeArc<Mutex>>),\n+}\n+\n+impl<'a> BlockingContext<'a> {\n+    // Creates one blocking context. The data provided should in theory be\n+    // acquired from the local task, but it is instead acquired from the\n+    // channel currently.\n+    //\n+    // This function will call `f` with a blocking context, plus the data\n+    // that it is given. This function will then return whether this task\n+    // should actually go to sleep or not. If `true` is returned, then this\n+    // function does not return until someone calls `wake()` on the task.\n+    // If `false` is returned, then this function immediately returns.\n+    //\n+    // # Safety note\n+    //\n+    // Note that this stack closure may not be run on the same stack as when\n+    // this function was called. This means that the environment of this\n+    // stack closure could be unsafely aliased. This is currently prevented\n+    // through the guarantee that this function will never return before `f`\n+    // finishes executing.\n+    pub fn one(data: &mut TaskData,\n+               f: |BlockingContext, &mut TaskData| -> bool) {\n+        if rt::in_green_task_context() {\n+            let sched: ~Scheduler = Local::take();\n+            sched.deschedule_running_task_and_then(|sched, task| {\n+                let ctx = BlockingContext { inner: GreenBlock(task, sched) };\n+                // no need to do something on success/failure other than\n+                // returning because the `block` function for a BlockingContext\n+                // takes care of reawakening itself if the blocking procedure\n+                // fails. If this function is successful, then we're already\n+                // blocked, and if it fails, the task will already be\n+                // rescheduled.\n+                f(ctx, data);\n+            });\n+        } else {\n+            unsafe { data.lock.lock(); }\n+            let ctx = BlockingContext { inner: NativeBlock(None) };\n+            if f(ctx, data) {\n+                unsafe { data.lock.wait(); }\n+            }\n+            unsafe { data.lock.unlock(); }\n+        }\n+    }\n+\n+    // Creates many blocking contexts. The intended use case for this\n+    // function is selection over a number of ports. This will create `amt`\n+    // blocking contexts, yielding them to `f` in turn. If `f` returns\n+    // false, then this function aborts and returns immediately. If `f`\n+    // repeatedly returns `true` `amt` times, then this function will block.\n+    pub fn many(amt: uint, f: |BlockingContext| -> bool) {\n+        if rt::in_green_task_context() {\n+            let sched: ~Scheduler = Local::take();\n+            sched.deschedule_running_task_and_then(|sched, task| {\n+                for handle in task.make_selectable(amt) {\n+                    let ctx = BlockingContext {\n+                        inner: GreenBlock(handle, sched)\n+                    };\n+                    // see comment above in `one` for why no further action is\n+                    // necessary here\n+                    if !f(ctx) { break }\n+                }\n+            });\n+        } else {\n+            // In the native case, our decision to block must be shared\n+            // amongst all of the channels. It may be possible to\n+            // stack-allocate this mutex (instead of putting it in an\n+            // UnsafeArc box), but for now in order to prevent\n+            // use-after-free trivially we place this into a box and then\n+            // pass that around.\n+            unsafe {\n+                let mtx = UnsafeArc::new(Mutex::new());\n+                (*mtx.get()).lock();\n+                let success = range(0, amt).all(|_| {\n+                    f(BlockingContext {\n+                        inner: NativeBlock(Some(mtx.clone()))\n+                    })\n+                });\n+                if success {\n+                    (*mtx.get()).wait();\n+                }\n+                (*mtx.get()).unlock();\n+            }\n+        }\n+    }\n+\n+    // This function will consume this BlockingContext, and optionally block\n+    // if according to the atomic `decision` function. The semantics of this\n+    // functions are:\n+    //\n+    //  * `slot` is required to be a `None`-slot (which is owned by the\n+    //    channel)\n+    //  * The `slot` will be filled in with a blocked version of the current\n+    //    task (with `wake`-ability if this function is successful).\n+    //  * If the `decision` function returns true, then this function\n+    //    immediately returns having relinquished ownership of the task.\n+    //  * If the `decision` function returns false, then the `slot` is reset\n+    //    to `None` and the task is re-scheduled if necessary (remember that\n+    //    the task will not resume executing before the outer `one` or\n+    //    `many` function has returned. This function is expected to have a\n+    //    release memory fence in order for the modifications of `to_wake` to be\n+    //    visible to other tasks. Code which attempts to read `to_wake` should\n+    //    have an acquiring memory fence to guarantee that this write is\n+    //    visible.\n+    //\n+    // This function will return whether the blocking occurred or not.\n+    pub fn block(self,\n+                 data: &mut TaskData,\n+                 slot: &mut Option<TaskHandle>,\n+                 decision: || -> bool) -> bool {\n+        assert!(slot.is_none());\n+        match self.inner {\n+            GreenBlock(task, sched) => {\n+                if data.handle.is_none() {\n+                    data.handle = Some(sched.make_handle());\n+                }\n+                let handle = data.handle.get_mut_ref() as *mut SchedHandle;\n+                *slot = Some(TaskHandle { inner: Green(task, handle) });\n+\n+                if !decision() {\n+                    match slot.take_unwrap().inner {\n+                        Green(task, _) => sched.enqueue_blocked_task(task),\n+                        Native(..) => unreachable!()\n+                    }\n+                    false\n+                } else {\n+                    true\n+                }\n+            }\n+            NativeBlock(shared) => {\n+                *slot = Some(TaskHandle {\n+                    inner: Native(match shared {\n+                        Some(arc) => ArcWakeup(arc),\n+                        None => LocalWakeup(&mut data.lock as *mut Mutex),\n+                    })\n+                });\n+\n+                if !decision() {\n+                    *slot = None;\n+                    false\n+                } else {\n+                    true\n+                }\n+            }\n+        }\n+    }\n+}\n+\n+// Agnostic method of forcing a yield of the current task\n+pub fn yield_now() {\n+    if rt::in_green_task_context() {\n+        let sched: ~Scheduler = Local::take();\n+        sched.yield_now();\n+    } else {\n+        Thread::yield_now();\n+    }\n+}\n+\n+// Agnostic method of \"maybe yielding\" in order to provide fairness\n+pub fn maybe_yield() {\n+    if rt::in_green_task_context() {\n+        let sched: ~Scheduler = Local::take();\n+        sched.maybe_yield();\n+    } else {\n+        // the OS decides fairness, nothing for us to do.\n+    }\n+}"}, {"sha": "9a65e9973cb483f2df686533fd972ac7281a39b1", "filename": "src/libstd/comm/mod.rs", "status": "added", "additions": 1371, "deletions": 0, "changes": 1371, "blob_url": "https://github.com/rust-lang/rust/blob/bfa9064ba2687eb1d95708f72f41ddd9729a6ba1/src%2Flibstd%2Fcomm%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/bfa9064ba2687eb1d95708f72f41ddd9729a6ba1/src%2Flibstd%2Fcomm%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibstd%2Fcomm%2Fmod.rs?ref=bfa9064ba2687eb1d95708f72f41ddd9729a6ba1", "patch": "@@ -0,0 +1,1371 @@\n+// Copyright 2013 The Rust Project Developers. See the COPYRIGHT\n+// file at the top-level directory of this distribution and at\n+// http://rust-lang.org/COPYRIGHT.\n+//\n+// Licensed under the Apache License, Version 2.0 <LICENSE-APACHE or\n+// http://www.apache.org/licenses/LICENSE-2.0> or the MIT license\n+// <LICENSE-MIT or http://opensource.org/licenses/MIT>, at your\n+// option. This file may not be copied, modified, or distributed\n+// except according to those terms.\n+\n+//! Rust Communication Primitives\n+//!\n+//! Rust makes it very difficult to share data among tasks to prevent race\n+//! conditions and to improve parallelism, but there is often a need for\n+//! communication between concurrent tasks. The primitives defined in this\n+//! module are the building blocks for synchronization in rust.\n+//!\n+//! This module currently provides three main types:\n+//!\n+//! * `Chan`\n+//! * `Port`\n+//! * `SharedChan`\n+//!\n+//! The `Chan` and `SharedChan` types are used to send data to a `Port`. A\n+//! `SharedChan` is clone-able such that many tasks can send simultaneously to\n+//! one receiving port. These communication primitives are *task blocking*, not\n+//! *thread blocking*. This means that if one task is blocked on a channel,\n+//! other tasks can continue to make progress.\n+//!\n+//! Rust channels can be used as if they have an infinite internal buffer. What\n+//! this means is that the `send` operation will never block. `Port`s, on the\n+//! other hand, will block the task if there is no data to be received.\n+//!\n+//! ## Failure Propagation\n+//!\n+//! In addition to being a core primitive for communicating in rust, channels\n+//! and ports are the points at which failure is propagated among tasks.\n+//! Whenever the one half of channel is closed, the other half will have its\n+//! next operation `fail!`. The purpose of this is to allow propagation of\n+//! failure among tasks that are linked to one another via channels.\n+//!\n+//! There are methods on all of `Chan`, `SharedChan`, and `Port` to perform\n+//! their respective operations without failing, however.\n+//!\n+//! ## Outside the Runtime\n+//!\n+//! All channels and ports work seamlessly inside and outside of the rust\n+//! runtime. This means that code may use channels to communicate information\n+//! inside and outside of the runtime. For example, if rust were embedded as an\n+//! FFI module in another application, the rust runtime would probably be\n+//! running in its own external thread pool. Channels created can communicate\n+//! from the native application threads to the rust threads through the use of\n+//! native mutexes and condition variables.\n+//!\n+//! What this means is that if a native thread is using a channel, execution\n+//! will be blocked accordingly by blocking the OS thread.\n+//!\n+//! # Example\n+//!\n+//! ```rust\n+//! // Create a simple streaming channel\n+//! let (port, chan) = Chan::new();\n+//! do spawn {\n+//!     chan.send(10);\n+//! }\n+//! assert_eq!(port.recv(), 10);\n+//!\n+//! // Create a shared channel which can be sent along from many tasks\n+//! let (port, chan) = SharedChan::new();\n+//! for i in range(0, 10) {\n+//!     let chan = chan.clone();\n+//!     do spawn {\n+//!         chan.send(i);\n+//!     }\n+//! }\n+//!\n+//! for _ in range(0, 10) {\n+//!     let j = port.recv();\n+//!     assert!(0 <= j && j < 10);\n+//! }\n+//!\n+//! // The call to recv() will fail!() because the channel has already hung\n+//! // up (or been deallocated)\n+//! let (port, chan) = Chan::new();\n+//! drop(chan);\n+//! port.recv();\n+//! ```\n+\n+// A description of how Rust's channel implementation works\n+//\n+// Channels are supposed to be the basic building block for all other\n+// concurrent primitives that are used in Rust. As a result, the channel type\n+// needs to be highly optimized, flexible, and broad enough for use everywhere.\n+//\n+// The choice of implementation of all channels is to be built on lock-free data\n+// structures. The channels themselves are then consequently also lock-free data\n+// structures. As always with lock-free code, this is a very \"here be dragons\"\n+// territory, especially because I'm unaware of any academic papers which have\n+// gone into great length about channels of these flavors.\n+//\n+// ## Flavors of channels\n+//\n+// Rust channels come in two flavors: streams and shared channels. A stream has\n+// one sender and one receiver while a shared channel could have multiple\n+// senders. This choice heavily influences the design of the protocol set\n+// forth for both senders/receivers.\n+//\n+// ## Concurrent queues\n+//\n+// The basic idea of Rust's Chan/Port types is that send() never blocks, but\n+// recv() obviously blocks. This means that under the hood there must be some\n+// shared and concurrent queue holding all of the actual data.\n+//\n+// With two flavors of channels, two flavors of queues are also used. We have\n+// chosen to use queues from a well-known author which are abbreviated as SPSC\n+// and MPSC (single producer, single consumer and multiple producer, single\n+// consumer). SPSC queues are used for streams while MPSC queues are used for\n+// shared channels.\n+//\n+// ### SPSC optimizations\n+//\n+// The SPSC queue found online is essentially a linked list of nodes where one\n+// half of the nodes are the \"queue of data\" and the other half of nodes are a\n+// cache of unused nodes. The unused nodes are used such that an allocation is\n+// not required on every push() and a free doesn't need to happen on every\n+// pop().\n+//\n+// As found online, however, the cache of nodes is of an infinite size. This\n+// means that if a channel at one point in its life had 50k items in the queue,\n+// then the queue will always have the capacity for 50k items. I believed that\n+// this was an unnecessary limitation of the implementation, so I have altered\n+// the queue to optionally have a bound on the cache size.\n+//\n+// By default, streams will have an unbounded SPSC queue with a small-ish cache\n+// size. The hope is that the cache is still large enough to have very fast\n+// send() operations while not too large such that millions of channels can\n+// coexist at once.\n+//\n+// ### MPSC optimizations\n+//\n+// Right now the MPSC queue has not been optimized. Like the SPSC queue, it uses\n+// a linked list under the hood to earn its unboundedness, but I have not put\n+// forth much effort into having a cache of nodes similar to the SPSC queue.\n+//\n+// For now, I believe that this is \"ok\" because shared channels are not the most\n+// common type, but soon we may wish to revisit this queue choice and determine\n+// another candidate for backend storage of shared channels.\n+//\n+// ## Overview of the Implementation\n+//\n+// Now that there's a little background on the concurrent queues used, it's\n+// worth going into much more detail about the channels themselves. The basic\n+// pseudocode for a send/recv are:\n+//\n+//\n+//      send(t)                             recv()\n+//        queue.push(t)                       return if queue.pop()\n+//        if increment() == -1                deschedule {\n+//          wakeup()                            if decrement() > 0\n+//                                                cancel_deschedule()\n+//                                            }\n+//                                            queue.pop()\n+//\n+// As mentioned before, there are no locks in this implementation, only atomic\n+// instructions are used.\n+//\n+// ### The internal atomic counter\n+//\n+// Every channel/port/shared channel have a shared counter with their\n+// counterparts to keep track of the size of the queue. This counter is used to\n+// abort descheduling by the receiver and to know when to wake up on the sending\n+// side.\n+//\n+// As seen in the pseudocode, senders will increment this count and receivers\n+// will decrement the count. The theory behind this is that if a sender sees a\n+// -1 count, it will wake up the receiver, and if the receiver sees a 1+ count,\n+// then it doesn't need to block.\n+//\n+// The recv() method has a beginning call to pop(), and if successful, it needs\n+// to decrement the count. It is a crucial implementation detail that this\n+// decrement does *not* happen to the shared counter. If this were the case,\n+// then it would be possible for the counter to be very negative when there were\n+// no receivers waiting, in which case the senders would have to determine when\n+// it was actually appropriate to wake up a receiver.\n+//\n+// Instead, the \"steal count\" is kept track of separately (not atomically\n+// because it's only used by ports), and then the decrement() call when\n+// descheduling will lump in all of the recent steals into one large decrement.\n+//\n+// The implication of this is that if a sender sees a -1 count, then there's\n+// guaranteed to be a waiter waiting!\n+//\n+// ## Native Implementation\n+//\n+// A major goal of these channels is to work seamlessly on and off the runtime.\n+// All of the previous race conditions have been worded in terms of\n+// scheduler-isms (which is obviously not available without the runtime).\n+//\n+// For now, native usage of channels (off the runtime) will fall back onto\n+// mutexes/cond vars for descheduling/atomic decisions. The no-contention path\n+// is still entirely lock-free, the \"deschedule\" blocks above are surrounded by\n+// a mutex and the \"wakeup\" blocks involve grabbing a mutex and signaling on a\n+// condition variable.\n+//\n+// ## Select\n+//\n+// Being able to support selection over channels has greatly influenced this\n+// design, and not only does selection need to work inside the runtime, but also\n+// outside the runtime.\n+//\n+// The implementation is fairly straightforward. The goal of select() is not to\n+// return some data, but only to return which channel can receive data without\n+// blocking. The implementation is essentially the entire blocking procedure\n+// followed by an increment as soon as its woken up. The cancellation procedure\n+// involves an increment and swapping out of to_wake to acquire ownership of the\n+// task to unblock.\n+//\n+// Sadly this current implementation requires multiple allocations, so I have\n+// seen the throughput of select() be much worse than it should be. I do not\n+// believe that there is anything fundamental which needs to change about these\n+// channels, however, in order to support a more efficient select().\n+//\n+// # Conclusion\n+//\n+// And now that you've seen all the races that I found and attempted to fix,\n+// here's the code for you to find some more!\n+\n+use cast;\n+use clone::Clone;\n+use container::Container;\n+use int;\n+use iter::Iterator;\n+use kinds::Send;\n+use ops::Drop;\n+use option::{Option, Some, None};\n+use rt::thread::Thread;\n+use unstable::atomics::{AtomicInt, AtomicBool, SeqCst, Relaxed};\n+use vec::{ImmutableVector, OwnedVector};\n+\n+use spsc = rt::spsc_queue;\n+use mpsc = rt::mpsc_queue;\n+\n+use self::imp::{TaskHandle, TaskData, BlockingContext};\n+pub use self::select::Select;\n+\n+macro_rules! test (\n+    { fn $name:ident() $b:block $($a:attr)*} => (\n+        mod $name {\n+            #[allow(unused_imports)];\n+\n+            use util;\n+            use super::super::*;\n+            use prelude::*;\n+\n+            fn f() $b\n+\n+            $($a)* #[test] fn uv() { f() }\n+            $($a)* #[test] fn native() {\n+                use unstable::run_in_bare_thread;\n+                run_in_bare_thread(f);\n+            }\n+        }\n+    )\n+)\n+\n+mod imp;\n+mod select;\n+\n+///////////////////////////////////////////////////////////////////////////////\n+// Helper type to abstract ports for channels and shared channels\n+///////////////////////////////////////////////////////////////////////////////\n+\n+enum Consumer<T> {\n+    SPSC(spsc::Consumer<T, Packet>),\n+    MPSC(mpsc::Consumer<T, Packet>),\n+}\n+\n+impl<T: Send> Consumer<T>{\n+    unsafe fn packet(&self) -> *mut Packet {\n+        match *self {\n+            SPSC(ref c) => c.packet(),\n+            MPSC(ref c) => c.packet(),\n+        }\n+    }\n+}\n+\n+///////////////////////////////////////////////////////////////////////////////\n+// Public structs\n+///////////////////////////////////////////////////////////////////////////////\n+\n+/// The receiving-half of Rust's channel type. This half can only be owned by\n+/// one task\n+pub struct Port<T> {\n+    priv queue: Consumer<T>,\n+}\n+\n+/// An iterator over messages received on a port, this iterator will block\n+/// whenever `next` is called, waiting for a new message, and `None` will be\n+/// returned when the corresponding channel has hung up.\n+pub struct PortIterator<'a, T> {\n+    priv port: &'a Port<T>\n+}\n+\n+/// The sending-half of Rust's channel type. This half can only be owned by one\n+/// task\n+pub struct Chan<T> {\n+    priv queue: spsc::Producer<T, Packet>,\n+}\n+\n+/// The sending-half of Rust's channel type. This half can be shared among many\n+/// tasks by creating copies of itself through the `clone` method.\n+pub struct SharedChan<T> {\n+    priv queue: mpsc::Producer<T, Packet>,\n+}\n+\n+///////////////////////////////////////////////////////////////////////////////\n+// Internal struct definitions\n+///////////////////////////////////////////////////////////////////////////////\n+\n+struct Packet {\n+    cnt: AtomicInt, // How many items are on this channel\n+    steals: int,    // How many times has a port received without blocking?\n+    to_wake: Option<TaskHandle>, // Task to wake up\n+\n+    data: TaskData,\n+\n+    // This lock is used to wake up native threads blocked in select. The\n+    // `lock` field is not used because the thread blocking in select must\n+    // block on only one mutex.\n+    //selection_lock: Option<UnsafeArc<Mutex>>,\n+\n+    // The number of channels which are currently using this packet. This is\n+    // used to reference count shared channels.\n+    channels: AtomicInt,\n+\n+    selecting: AtomicBool,\n+    selection_id: uint,\n+    select_next: *mut Packet,\n+    select_prev: *mut Packet,\n+}\n+\n+///////////////////////////////////////////////////////////////////////////////\n+// All implementations -- the fun part\n+///////////////////////////////////////////////////////////////////////////////\n+\n+static DISCONNECTED: int = int::min_value;\n+static RESCHED_FREQ: int = 200;\n+\n+impl Packet {\n+    fn new() -> Packet {\n+        Packet {\n+            cnt: AtomicInt::new(0),\n+            steals: 0,\n+            to_wake: None,\n+            data: TaskData::new(),\n+            channels: AtomicInt::new(1),\n+\n+            selecting: AtomicBool::new(false),\n+            selection_id: 0,\n+            select_next: 0 as *mut Packet,\n+            select_prev: 0 as *mut Packet,\n+        }\n+    }\n+\n+    // Increments the channel size count, preserving the disconnected state if\n+    // the other end has disconnected.\n+    fn increment(&mut self) -> int {\n+        match self.cnt.fetch_add(1, SeqCst) {\n+            DISCONNECTED => {\n+                // see the comment in 'try' for a shared channel for why this\n+                // window of \"not disconnected\" is \"ok\".\n+                self.cnt.store(DISCONNECTED, SeqCst);\n+                DISCONNECTED\n+            }\n+            n => n\n+        }\n+    }\n+\n+    // Decrements the reference count of the channel, returning whether the task\n+    // should block or not. This assumes that the task is ready to sleep in that\n+    // the `to_wake` field has already been filled in. Once this decrement\n+    // happens, the task could wake up on the other end.\n+    //\n+    // From an implementation perspective, this is also when our \"steal count\"\n+    // gets merged into the \"channel count\". Our steal count is reset to 0 after\n+    // this function completes.\n+    //\n+    // As with increment(), this preserves the disconnected state if the\n+    // channel is disconnected.\n+    fn decrement(&mut self) -> bool {\n+        let steals = self.steals;\n+        self.steals = 0;\n+        match self.cnt.fetch_sub(1 + steals, SeqCst) {\n+            DISCONNECTED => {\n+                self.cnt.store(DISCONNECTED, SeqCst);\n+                false\n+            }\n+            n => {\n+                assert!(n >= 0);\n+                n - steals <= 0\n+            }\n+        }\n+    }\n+\n+    // Helper function for select, tests whether this port can receive without\n+    // blocking (obviously not an atomic decision).\n+    fn can_recv(&self) -> bool {\n+        let cnt = self.cnt.load(SeqCst);\n+        cnt == DISCONNECTED || cnt - self.steals > 0\n+    }\n+\n+    // This function must have had at least an acquire fence before it to be\n+    // properly called.\n+    fn wakeup(&mut self, can_resched: bool) {\n+        self.to_wake.take_unwrap().wake(can_resched);\n+        self.selecting.store(false, Relaxed);\n+    }\n+\n+    // Aborts the selection process for a port. This happens as part of select()\n+    // once the task has reawoken. This will place the channel back into a\n+    // consistent state which is ready to be received from again.\n+    //\n+    // The method of doing this is a little subtle. These channels have the\n+    // invariant that if -1 is seen, then to_wake is always Some(..) and should\n+    // be woken up. This aborting process at least needs to add 1 to the\n+    // reference count, but that is not guaranteed to make the count positive\n+    // (our steal count subtraction could mean that after the addition the\n+    // channel count is still negative).\n+    //\n+    // In order to get around this, we force our channel count to go above 0 by\n+    // adding a large number >= 1 to it. This way no sender will see -1 unless\n+    // we are indeed blocking. This \"extra lump\" we took out of the channel\n+    // becomes our steal count (which will get re-factored into the count on the\n+    // next blocking recv)\n+    //\n+    // The return value of this method is whether there is data on this channel\n+    // to receive or not.\n+    fn abort_selection(&mut self, take_to_wake: bool) -> bool {\n+        // make sure steals + 1 makes the count go non-negative\n+        let steals = {\n+            let cnt = self.cnt.load(SeqCst);\n+            if cnt < 0 && cnt != DISCONNECTED {-cnt} else {0}\n+        };\n+        let prev = self.cnt.fetch_add(steals + 1, SeqCst);\n+\n+        // If we were previously disconnected, then we know for sure that there\n+        // is no task in to_wake, so just keep going\n+        if prev == DISCONNECTED {\n+            assert!(self.to_wake.is_none());\n+            self.cnt.store(DISCONNECTED, SeqCst);\n+            self.selecting.store(false, SeqCst);\n+            true // there is data, that data is that we're disconnected\n+        } else {\n+            let cur = prev + steals + 1;\n+            assert!(cur >= 0);\n+\n+            // If the previous count was negative, then we just made things go\n+            // positive, hence we passed the -1 boundary and we're responsible\n+            // for removing the to_wake() field and trashing it.\n+            if prev < 0 {\n+                if take_to_wake {\n+                    self.to_wake.take_unwrap().trash();\n+                } else {\n+                    assert!(self.to_wake.is_none());\n+                }\n+\n+                // We woke ourselves up, we're responsible for cancelling\n+                assert!(self.selecting.load(Relaxed));\n+                self.selecting.store(false, Relaxed);\n+            }\n+            assert_eq!(self.steals, 0);\n+            self.steals = steals;\n+\n+            // if we were previously positive, then there's surely data to\n+            // receive\n+            prev >= 0\n+        }\n+    }\n+\n+    // Decrement the reference count on a channel. This is called whenever a\n+    // Chan is dropped and may end up waking up a receiver. It's the receiver's\n+    // responsibility on the other end to figure out that we've disconnected.\n+    unsafe fn drop_chan(&mut self) {\n+        match self.channels.fetch_sub(1, SeqCst) {\n+            1 => {\n+                match self.cnt.swap(DISCONNECTED, SeqCst) {\n+                    -1 => { self.wakeup(false); }\n+                    DISCONNECTED => {}\n+                    n => { assert!(n >= 0); }\n+                }\n+            }\n+            n if n > 1 => {},\n+            n => fail!(\"bad number of channels left {}\", n),\n+        }\n+    }\n+}\n+\n+impl Drop for Packet {\n+    fn drop(&mut self) {\n+        unsafe {\n+            // Note that this load is not only an assert for correctness about\n+            // disconnection, but also a proper fence before the read of\n+            // `to_wake`, so this assert cannot be removed with also removing\n+            // the `to_wake` assert.\n+            assert_eq!(self.cnt.load(SeqCst), DISCONNECTED);\n+            assert!(self.to_wake.is_none());\n+            assert_eq!(self.channels.load(SeqCst), 0);\n+        }\n+    }\n+}\n+\n+impl<T: Send> Chan<T> {\n+    /// Creates a new port/channel pair. All data send on the channel returned\n+    /// will become available on the port as well. See the documentation of\n+    /// `Port` and `Chan` to see what's possible with them.\n+    pub fn new() -> (Port<T>, Chan<T>) {\n+        // arbitrary 128 size cache -- this is just a max cache size, not a\n+        // maximum buffer size\n+        let (c, p) = spsc::queue(128, Packet::new());\n+        let c = SPSC(c);\n+        (Port { queue: c }, Chan { queue: p })\n+    }\n+\n+    /// Sends a value along this channel to be received by the corresponding\n+    /// port.\n+    ///\n+    /// Rust channels are infinitely buffered so this method will never block.\n+    /// This method may trigger a rescheduling, however, in order to wake up a\n+    /// blocked receiver (if one is present). If no scheduling is desired, then\n+    /// the `send_deferred` guarantees that there will be no reschedulings.\n+    ///\n+    /// # Failure\n+    ///\n+    /// This function will fail if the other end of the channel has hung up.\n+    /// This means that if the corresponding port has fallen out of scope, this\n+    /// function will trigger a fail message saying that a message is being sent\n+    /// on a closed channel.\n+    ///\n+    /// Note that if this function does *not* fail, it does not mean that the\n+    /// data will be successfully received. All sends are placed into a queue,\n+    /// so it is possible for a send to succeed (the other end is alive), but\n+    /// then the other end could immediately disconnect.\n+    ///\n+    /// The purpose of this functionality is to propagate failure among tasks.\n+    /// If failure is not desired, then consider using the `try_send` method\n+    pub fn send(&self, t: T) {\n+        if !self.try_send(t) {\n+            fail!(\"sending on a closed channel\");\n+        }\n+    }\n+\n+    /// This function is equivalent in the semantics of `send`, but it\n+    /// guarantees that a rescheduling will never occur when this method is\n+    /// called.\n+    pub fn send_deferred(&self, t: T) {\n+        if !self.try_send_deferred(t) {\n+            fail!(\"sending on a closed channel\");\n+        }\n+    }\n+\n+    /// Attempts to send a value on this channel, returning whether it was\n+    /// successfully sent.\n+    ///\n+    /// A successful send occurs when it is determined that the other end of the\n+    /// channel has not hung up already. An unsuccessful send would be one where\n+    /// the corresponding port has already been deallocated. Note that a return\n+    /// value of `false` means that the data will never be received, but a\n+    /// return value of `true` does *not* mean that the data will be received.\n+    /// It is possible for the corresponding port to hang up immediately after\n+    /// this function returns `true`.\n+    ///\n+    /// Like `send`, this method will never block. If the failure of send cannot\n+    /// be tolerated, then this method should be used instead.\n+    pub fn try_send(&self, t: T) -> bool { self.try(t, true) }\n+\n+    /// This function is equivalent in the semantics of `try_send`, but it\n+    /// guarantees that a rescheduling will never occur when this method is\n+    /// called.\n+    pub fn try_send_deferred(&self, t: T) -> bool { self.try(t, false) }\n+\n+    fn try(&self, t: T, can_resched: bool) -> bool {\n+        unsafe {\n+            let this = cast::transmute_mut(self);\n+            this.queue.push(t);\n+            let packet = this.queue.packet();\n+            match (*packet).increment() {\n+                // As described above, -1 == wakeup\n+                -1 => { (*packet).wakeup(can_resched); true }\n+                // Also as above, SPSC queues must be >= -2\n+                -2 => true,\n+                // We succeeded if we sent data\n+                DISCONNECTED => this.queue.is_empty(),\n+                // In order to prevent starvation of other tasks in situations\n+                // where a task sends repeatedly without ever receiving, we\n+                // occassionally yield instead of doing a send immediately.\n+                // Only doing this if we're doing a rescheduling send, otherwise\n+                // the caller is expecting not to context switch.\n+                //\n+                // Note that we don't unconditionally attempt to yield because\n+                // the TLS overhead can be a bit much.\n+                n => {\n+                    assert!(n >= 0);\n+                    if can_resched && n > 0 && n % RESCHED_FREQ == 0 {\n+                        imp::maybe_yield();\n+                    }\n+                    true\n+                }\n+            }\n+        }\n+    }\n+}\n+\n+#[unsafe_destructor]\n+impl<T: Send> Drop for Chan<T> {\n+    fn drop(&mut self) {\n+        unsafe { (*self.queue.packet()).drop_chan(); }\n+    }\n+}\n+\n+impl<T: Send> SharedChan<T> {\n+    /// Creates a new shared channel and port pair. The purpose of a shared\n+    /// channel is to be cloneable such that many tasks can send data at the\n+    /// same time. All data sent on any channel will become available on the\n+    /// provided port as well.\n+    pub fn new() -> (Port<T>, SharedChan<T>) {\n+        let (c, p) = mpsc::queue(Packet::new());\n+        let c = MPSC(c);\n+        (Port { queue: c }, SharedChan { queue: p })\n+    }\n+\n+    /// Equivalent method to `send` on the `Chan` type (using the same\n+    /// semantics)\n+    pub fn send(&self, t: T) {\n+        if !self.try_send(t) {\n+            fail!(\"sending on a closed channel\");\n+        }\n+    }\n+\n+    /// This function is equivalent in the semantics of `send`, but it\n+    /// guarantees that a rescheduling will never occur when this method is\n+    /// called.\n+    pub fn send_deferred(&self, t: T) {\n+        if !self.try_send_deferred(t) {\n+            fail!(\"sending on a closed channel\");\n+        }\n+    }\n+\n+    /// Equivalent method to `try_send` on the `Chan` type (using the same\n+    /// semantics)\n+    pub fn try_send(&self, t: T) -> bool { self.try(t, true) }\n+\n+    /// This function is equivalent in the semantics of `try_send`, but it\n+    /// guarantees that a rescheduling will never occur when this method is\n+    /// called.\n+    pub fn try_send_deferred(&self, t: T) -> bool { self.try(t, false) }\n+\n+    fn try(&self, t: T, can_resched: bool) -> bool {\n+        unsafe {\n+            // Note that the multiple sender case is a little tricker\n+            // semantically than the single sender case. The logic for\n+            // incrementing is \"add and if disconnected store disconnected\".\n+            // This could end up leading some senders to believe that there\n+            // wasn't a disconnect if in fact there was a disconnect. This means\n+            // that while one thread is attempting to re-store the disconnected\n+            // states, other threads could walk through merrily incrementing\n+            // this very-negative disconnected count. To prevent senders from\n+            // spuriously attempting to send when the channels is actually\n+            // disconnected, the count has a ranged check here.\n+            //\n+            // This is also done for another reason. Remember that the return\n+            // value of this function is:\n+            //\n+            //  `true` == the data *may* be received, this essentially has no\n+            //            meaning\n+            //  `false` == the data will *never* be received, this has a lot of\n+            //             meaning\n+            //\n+            // In the SPSC case, we have a check of 'queue.is_empty()' to see\n+            // whether the data was actually received, but this same condition\n+            // means nothing in a multi-producer context. As a result, this\n+            // preflight check serves as the definitive \"this will never be\n+            // received\". Once we get beyond this check, we have permanently\n+            // entered the realm of \"this may be received\"\n+            let packet = self.queue.packet();\n+            if (*packet).cnt.load(Relaxed) < DISCONNECTED + 1024 {\n+                return false\n+            }\n+\n+            let this = cast::transmute_mut(self);\n+            this.queue.push(t);\n+\n+            match (*packet).increment() {\n+                DISCONNECTED => {} // oh well, we tried\n+                -1 => { (*packet).wakeup(can_resched); }\n+                n => {\n+                    if can_resched && n > 0 && n % RESCHED_FREQ == 0 {\n+                        imp::maybe_yield();\n+                    }\n+                }\n+            }\n+            true\n+        }\n+    }\n+}\n+\n+impl<T: Send> Clone for SharedChan<T> {\n+    fn clone(&self) -> SharedChan<T> {\n+        unsafe { (*self.queue.packet()).channels.fetch_add(1, SeqCst); }\n+        SharedChan { queue: self.queue.clone() }\n+    }\n+}\n+\n+#[unsafe_destructor]\n+impl<T: Send> Drop for SharedChan<T> {\n+    fn drop(&mut self) {\n+        unsafe { (*self.queue.packet()).drop_chan(); }\n+    }\n+}\n+\n+impl<T: Send> Port<T> {\n+    /// Blocks waiting for a value on this port\n+    ///\n+    /// This function will block if necessary to wait for a corresponding send\n+    /// on the channel from its paired `Chan` structure. This port will be woken\n+    /// up when data is ready, and the data will be returned.\n+    ///\n+    /// # Failure\n+    ///\n+    /// Similar to channels, this method will trigger a task failure if the\n+    /// other end of the channel has hung up (been deallocated). The purpose of\n+    /// this is to propagate failure among tasks.\n+    ///\n+    /// If failure is not desired, then there are two options:\n+    ///\n+    /// * If blocking is still desired, the `recv_opt` method will return `None`\n+    ///   when the other end hangs up\n+    ///\n+    /// * If blocking is not desired, then the `try_recv` method will attempt to\n+    ///   peek at a value on this port.\n+    pub fn recv(&self) -> T {\n+        match self.recv_opt() {\n+            Some(t) => t,\n+            None => fail!(\"receiving on a closed channel\"),\n+        }\n+    }\n+\n+    /// Attempts to return a pending value on this port without blocking\n+    ///\n+    /// This method will never block the caller in order to wait for data to\n+    /// become available. Instead, this will always return immediately with a\n+    /// possible option of pending data on the channel.\n+    ///\n+    /// This is useful for a flavor of \"optimistic check\" before deciding to\n+    /// block on a port.\n+    ///\n+    /// This function cannot fail.\n+    pub fn try_recv(&self) -> Option<T> {\n+        self.try_recv_inc(true)\n+    }\n+\n+    fn try_recv_inc(&self, increment: bool) -> Option<T> {\n+        // This is a \"best effort\" situation, so if a queue is inconsistent just\n+        // don't worry about it.\n+        let this = unsafe { cast::transmute_mut(self) };\n+        let ret = match this.queue {\n+            SPSC(ref mut queue) => queue.pop(),\n+            MPSC(ref mut queue) => match queue.pop() {\n+                mpsc::Data(t) => Some(t),\n+                mpsc::Empty => None,\n+\n+                // This is a bit of an interesting case. The channel is\n+                // reported as having data available, but our pop() has\n+                // failed due to the queue being in an inconsistent state.\n+                // This means that there is some pusher somewhere which has\n+                // yet to complete, but we are guaranteed that a pop will\n+                // eventually succeed. In this case, we spin in a yield loop\n+                // because the remote sender should finish their enqueue\n+                // operation \"very quickly\".\n+                //\n+                // Note that this yield loop does *not* attempt to do a green\n+                // yield (regardless of the context), but *always* performs an\n+                // OS-thread yield. The reasoning for this is that the pusher in\n+                // question which is causing the inconsistent state is\n+                // guaranteed to *not* be a blocked task (green tasks can't get\n+                // pre-empted), so it must be on a different OS thread. Also,\n+                // `try_recv` is normally a \"guaranteed no rescheduling\" context\n+                // in a green-thread situation. By yielding control of the\n+                // thread, we will hopefully allow time for the remote task on\n+                // the other OS thread to make progress.\n+                //\n+                // Avoiding this yield loop would require a different queue\n+                // abstraction which provides the guarantee that after M\n+                // pushes have succeeded, at least M pops will succeed. The\n+                // current queues guarantee that if there are N active\n+                // pushes, you can pop N times once all N have finished.\n+                mpsc::Inconsistent => {\n+                    let data;\n+                    loop {\n+                        Thread::yield_now();\n+                        match queue.pop() {\n+                            mpsc::Data(t) => { data = t; break }\n+                            mpsc::Empty => fail!(\"inconsistent => empty\"),\n+                            mpsc::Inconsistent => {}\n+                        }\n+                    }\n+                    Some(data)\n+                }\n+            }\n+        };\n+        if increment && ret.is_some() {\n+            unsafe { (*this.queue.packet()).steals += 1; }\n+        }\n+        return ret;\n+    }\n+\n+    /// Attempt to wait for a value on this port, but does not fail if the\n+    /// corresponding channel has hung up.\n+    ///\n+    /// This implementation of iterators for ports will always block if there is\n+    /// not data available on the port, but it will not fail in the case that\n+    /// the channel has been deallocated.\n+    ///\n+    /// In other words, this function has the same semantics as the `recv`\n+    /// method except for the failure aspect.\n+    ///\n+    /// If the channel has hung up, then `None` is returned. Otherwise `Some` of\n+    /// the value found on the port is returned.\n+    pub fn recv_opt(&self) -> Option<T> {\n+        // optimistic preflight check (scheduling is expensive)\n+        match self.try_recv() { None => {}, data => return data }\n+\n+        let packet;\n+        let this;\n+        unsafe {\n+            this = cast::transmute_mut(self);\n+            packet = this.queue.packet();\n+            BlockingContext::one(&mut (*packet).data, |ctx, data| {\n+                ctx.block(data, &mut (*packet).to_wake, || (*packet).decrement())\n+            });\n+        }\n+\n+        let data = self.try_recv_inc(false);\n+        if data.is_none() &&\n+           unsafe { (*packet).cnt.load(SeqCst) } != DISCONNECTED {\n+            fail!(\"bug: woke up too soon\");\n+        }\n+        return data;\n+    }\n+\n+    /// Returns an iterator which will block waiting for messages, but never\n+    /// `fail!`. It will return `None` when the channel has hung up.\n+    pub fn iter<'a>(&'a self) -> PortIterator<'a, T> {\n+        PortIterator { port: self }\n+    }\n+}\n+\n+impl<'a, T: Send> Iterator<T> for PortIterator<'a, T> {\n+    fn next(&mut self) -> Option<T> { self.port.recv_opt() }\n+}\n+\n+#[unsafe_destructor]\n+impl<T: Send> Drop for Port<T> {\n+    fn drop(&mut self) {\n+        // All we need to do is store that we're disconnected. If the channel\n+        // half has already disconnected, then we'll just deallocate everything\n+        // when the shared packet is deallocated.\n+        unsafe {\n+            (*self.queue.packet()).cnt.store(DISCONNECTED, SeqCst);\n+        }\n+    }\n+}\n+\n+#[cfg(test)]\n+mod test {\n+    use prelude::*;\n+\n+    use task;\n+    use rt::thread::Thread;\n+    use super::*;\n+    use rt::test::*;\n+\n+    test!(fn smoke() {\n+        let (p, c) = Chan::new();\n+        c.send(1);\n+        assert_eq!(p.recv(), 1);\n+    })\n+\n+    test!(fn drop_full() {\n+        let (_p, c) = Chan::new();\n+        c.send(~1);\n+    })\n+\n+    test!(fn drop_full_shared() {\n+        let (_p, c) = SharedChan::new();\n+        c.send(~1);\n+    })\n+\n+    test!(fn smoke_shared() {\n+        let (p, c) = SharedChan::new();\n+        c.send(1);\n+        assert_eq!(p.recv(), 1);\n+        let c = c.clone();\n+        c.send(1);\n+        assert_eq!(p.recv(), 1);\n+    })\n+\n+    #[test]\n+    fn smoke_threads() {\n+        let (p, c) = Chan::new();\n+        do task::spawn_sched(task::SingleThreaded) {\n+            c.send(1);\n+        }\n+        assert_eq!(p.recv(), 1);\n+    }\n+\n+    #[test] #[should_fail]\n+    fn smoke_port_gone() {\n+        let (p, c) = Chan::new();\n+        drop(p);\n+        c.send(1);\n+    }\n+\n+    #[test] #[should_fail]\n+    fn smoke_shared_port_gone() {\n+        let (p, c) = SharedChan::new();\n+        drop(p);\n+        c.send(1);\n+    }\n+\n+    #[test] #[should_fail]\n+    fn smoke_shared_port_gone2() {\n+        let (p, c) = SharedChan::new();\n+        drop(p);\n+        let c2 = c.clone();\n+        drop(c);\n+        c2.send(1);\n+    }\n+\n+    #[test] #[should_fail]\n+    fn port_gone_concurrent() {\n+        let (p, c) = Chan::new();\n+        do task::spawn_sched(task::SingleThreaded) {\n+            p.recv();\n+        }\n+        loop { c.send(1) }\n+    }\n+\n+    #[test] #[should_fail]\n+    fn port_gone_concurrent_shared() {\n+        let (p, c) = SharedChan::new();\n+        let c1 = c.clone();\n+        do task::spawn_sched(task::SingleThreaded) {\n+            p.recv();\n+        }\n+        loop {\n+            c.send(1);\n+            c1.send(1);\n+        }\n+    }\n+\n+    #[test] #[should_fail]\n+    fn smoke_chan_gone() {\n+        let (p, c) = Chan::<int>::new();\n+        drop(c);\n+        p.recv();\n+    }\n+\n+    #[test] #[should_fail]\n+    fn smoke_chan_gone_shared() {\n+        let (p, c) = SharedChan::<()>::new();\n+        let c2 = c.clone();\n+        drop(c);\n+        drop(c2);\n+        p.recv();\n+    }\n+\n+    #[test] #[should_fail]\n+    fn chan_gone_concurrent() {\n+        let (p, c) = Chan::new();\n+        do task::spawn_sched(task::SingleThreaded) {\n+            c.send(1);\n+            c.send(1);\n+        }\n+        loop { p.recv(); }\n+    }\n+\n+    #[test]\n+    fn stress() {\n+        let (p, c) = Chan::new();\n+        do task::spawn_sched(task::SingleThreaded) {\n+            for _ in range(0, 10000) { c.send(1); }\n+        }\n+        for _ in range(0, 10000) {\n+            assert_eq!(p.recv(), 1);\n+        }\n+    }\n+\n+    #[test]\n+    fn stress_shared() {\n+        static AMT: uint = 10000;\n+        static NTHREADS: uint = 8;\n+        let (p, c) = SharedChan::<int>::new();\n+        let (p1, c1) = Chan::new();\n+\n+        do spawn {\n+            for _ in range(0, AMT * NTHREADS) {\n+                assert_eq!(p.recv(), 1);\n+            }\n+            assert_eq!(p.try_recv(), None);\n+            c1.send(());\n+        }\n+\n+        for _ in range(0, NTHREADS) {\n+            let c = c.clone();\n+            do task::spawn_sched(task::SingleThreaded) {\n+                for _ in range(0, AMT) { c.send(1); }\n+            }\n+        }\n+        p1.recv();\n+\n+    }\n+\n+    #[test]\n+    fn send_from_outside_runtime() {\n+        let (p, c) = Chan::<int>::new();\n+        let (p1, c1) = Chan::new();\n+        do spawn {\n+            c1.send(());\n+            for _ in range(0, 40) {\n+                assert_eq!(p.recv(), 1);\n+            }\n+        }\n+        p1.recv();\n+        let t = do Thread::start {\n+            for _ in range(0, 40) {\n+                c.send(1);\n+            }\n+        };\n+        t.join();\n+    }\n+\n+    #[test]\n+    fn recv_from_outside_runtime() {\n+        let (p, c) = Chan::<int>::new();\n+        let t = do Thread::start {\n+            for _ in range(0, 40) {\n+                assert_eq!(p.recv(), 1);\n+            }\n+        };\n+        for _ in range(0, 40) {\n+            c.send(1);\n+        }\n+        t.join();\n+    }\n+\n+    #[test]\n+    fn no_runtime() {\n+        let (p1, c1) = Chan::<int>::new();\n+        let (p2, c2) = Chan::<int>::new();\n+        let t1 = do Thread::start {\n+            assert_eq!(p1.recv(), 1);\n+            c2.send(2);\n+        };\n+        let t2 = do Thread::start {\n+            c1.send(1);\n+            assert_eq!(p2.recv(), 2);\n+        };\n+        t1.join();\n+        t2.join();\n+    }\n+\n+    #[test]\n+    fn oneshot_single_thread_close_port_first() {\n+        // Simple test of closing without sending\n+        do run_in_newsched_task {\n+            let (port, _chan) = Chan::<int>::new();\n+            { let _p = port; }\n+        }\n+    }\n+\n+    #[test]\n+    fn oneshot_single_thread_close_chan_first() {\n+        // Simple test of closing without sending\n+        do run_in_newsched_task {\n+            let (_port, chan) = Chan::<int>::new();\n+            { let _c = chan; }\n+        }\n+    }\n+\n+    #[test] #[should_fail]\n+    fn oneshot_single_thread_send_port_close() {\n+        // Testing that the sender cleans up the payload if receiver is closed\n+        let (port, chan) = Chan::<~int>::new();\n+        { let _p = port; }\n+        chan.send(~0);\n+    }\n+\n+    #[test]\n+    fn oneshot_single_thread_recv_chan_close() {\n+        // Receiving on a closed chan will fail\n+        do run_in_newsched_task {\n+            let res = do spawntask_try {\n+                let (port, chan) = Chan::<~int>::new();\n+                { let _c = chan; }\n+                port.recv();\n+            };\n+            // What is our res?\n+            assert!(res.is_err());\n+        }\n+    }\n+\n+    #[test]\n+    fn oneshot_single_thread_send_then_recv() {\n+        do run_in_newsched_task {\n+            let (port, chan) = Chan::<~int>::new();\n+            chan.send(~10);\n+            assert!(port.recv() == ~10);\n+        }\n+    }\n+\n+    #[test]\n+    fn oneshot_single_thread_try_send_open() {\n+        do run_in_newsched_task {\n+            let (port, chan) = Chan::<int>::new();\n+            assert!(chan.try_send(10));\n+            assert!(port.recv() == 10);\n+        }\n+    }\n+\n+    #[test]\n+    fn oneshot_single_thread_try_send_closed() {\n+        do run_in_newsched_task {\n+            let (port, chan) = Chan::<int>::new();\n+            { let _p = port; }\n+            assert!(!chan.try_send(10));\n+        }\n+    }\n+\n+    #[test]\n+    fn oneshot_single_thread_try_recv_open() {\n+        do run_in_newsched_task {\n+            let (port, chan) = Chan::<int>::new();\n+            chan.send(10);\n+            assert!(port.try_recv() == Some(10));\n+        }\n+    }\n+\n+    #[test]\n+    fn oneshot_single_thread_try_recv_closed() {\n+        do run_in_newsched_task {\n+            let (port, chan) = Chan::<int>::new();\n+            { let _c = chan; }\n+            assert!(port.recv_opt() == None);\n+        }\n+    }\n+\n+    #[test]\n+    fn oneshot_single_thread_peek_data() {\n+        do run_in_newsched_task {\n+            let (port, chan) = Chan::<int>::new();\n+            assert!(port.try_recv().is_none());\n+            chan.send(10);\n+            assert!(port.try_recv().is_some());\n+        }\n+    }\n+\n+    #[test]\n+    fn oneshot_single_thread_peek_close() {\n+        do run_in_newsched_task {\n+            let (port, chan) = Chan::<int>::new();\n+            { let _c = chan; }\n+            assert!(port.try_recv().is_none());\n+            assert!(port.try_recv().is_none());\n+        }\n+    }\n+\n+    #[test]\n+    fn oneshot_single_thread_peek_open() {\n+        do run_in_newsched_task {\n+            let (port, _) = Chan::<int>::new();\n+            assert!(port.try_recv().is_none());\n+        }\n+    }\n+\n+    #[test]\n+    fn oneshot_multi_task_recv_then_send() {\n+        do run_in_newsched_task {\n+            let (port, chan) = Chan::<~int>::new();\n+            do spawntask {\n+                assert!(port.recv() == ~10);\n+            }\n+\n+            chan.send(~10);\n+        }\n+    }\n+\n+    #[test]\n+    fn oneshot_multi_task_recv_then_close() {\n+        do run_in_newsched_task {\n+            let (port, chan) = Chan::<~int>::new();\n+            do spawntask_later {\n+                let _chan = chan;\n+            }\n+            let res = do spawntask_try {\n+                assert!(port.recv() == ~10);\n+            };\n+            assert!(res.is_err());\n+        }\n+    }\n+\n+    #[test]\n+    fn oneshot_multi_thread_close_stress() {\n+        stress_factor().times(|| {\n+            do run_in_newsched_task {\n+                let (port, chan) = Chan::<int>::new();\n+                let thread = do spawntask_thread {\n+                    let _p = port;\n+                };\n+                let _chan = chan;\n+                thread.join();\n+            }\n+        })\n+    }\n+\n+    #[test]\n+    fn oneshot_multi_thread_send_close_stress() {\n+        stress_factor().times(|| {\n+            let (port, chan) = Chan::<int>::new();\n+            do spawn {\n+                let _p = port;\n+            }\n+            do task::try {\n+                chan.send(1);\n+            };\n+        })\n+    }\n+\n+    #[test]\n+    fn oneshot_multi_thread_recv_close_stress() {\n+        stress_factor().times(|| {\n+            let (port, chan) = Chan::<int>::new();\n+            do spawn {\n+                let port = port;\n+                let res = do task::try {\n+                    port.recv();\n+                };\n+                assert!(res.is_err());\n+            };\n+            do spawn {\n+                let chan = chan;\n+                do spawn {\n+                    let _chan = chan;\n+                }\n+            };\n+        })\n+    }\n+\n+    #[test]\n+    fn oneshot_multi_thread_send_recv_stress() {\n+        stress_factor().times(|| {\n+            let (port, chan) = Chan::<~int>::new();\n+            do spawn {\n+                chan.send(~10);\n+            }\n+            do spawn {\n+                assert!(port.recv() == ~10);\n+            }\n+        })\n+    }\n+\n+    #[test]\n+    fn stream_send_recv_stress() {\n+        stress_factor().times(|| {\n+            let (port, chan) = Chan::<~int>::new();\n+\n+            send(chan, 0);\n+            recv(port, 0);\n+\n+            fn send(chan: Chan<~int>, i: int) {\n+                if i == 10 { return }\n+\n+                do spawntask_random {\n+                    chan.send(~i);\n+                    send(chan, i + 1);\n+                }\n+            }\n+\n+            fn recv(port: Port<~int>, i: int) {\n+                if i == 10 { return }\n+\n+                do spawntask_random {\n+                    assert!(port.recv() == ~i);\n+                    recv(port, i + 1);\n+                };\n+            }\n+        })\n+    }\n+\n+    #[test]\n+    fn recv_a_lot() {\n+        // Regression test that we don't run out of stack in scheduler context\n+        do run_in_newsched_task {\n+            let (port, chan) = Chan::new();\n+            10000.times(|| { chan.send(()) });\n+            10000.times(|| { port.recv() });\n+        }\n+    }\n+\n+    #[test]\n+    fn shared_chan_stress() {\n+        do run_in_mt_newsched_task {\n+            let (port, chan) = SharedChan::new();\n+            let total = stress_factor() + 100;\n+            total.times(|| {\n+                let chan_clone = chan.clone();\n+                do spawntask_random {\n+                    chan_clone.send(());\n+                }\n+            });\n+\n+            total.times(|| {\n+                port.recv();\n+            });\n+        }\n+    }\n+\n+    #[test]\n+    fn test_nested_recv_iter() {\n+        let (port, chan) = Chan::<int>::new();\n+        let (total_port, total_chan) = Chan::<int>::new();\n+\n+        do spawn {\n+            let mut acc = 0;\n+            for x in port.iter() {\n+                acc += x;\n+            }\n+            total_chan.send(acc);\n+        }\n+\n+        chan.send(3);\n+        chan.send(1);\n+        chan.send(2);\n+        drop(chan);\n+        assert_eq!(total_port.recv(), 6);\n+    }\n+\n+    #[test]\n+    fn test_recv_iter_break() {\n+        let (port, chan) = Chan::<int>::new();\n+        let (count_port, count_chan) = Chan::<int>::new();\n+\n+        do spawn {\n+            let mut count = 0;\n+            for x in port.iter() {\n+                if count >= 3 {\n+                    break;\n+                } else {\n+                    count += x;\n+                }\n+            }\n+            count_chan.send(count);\n+        }\n+\n+        chan.send(2);\n+        chan.send(2);\n+        chan.send(2);\n+        chan.try_send(2);\n+        drop(chan);\n+        assert_eq!(count_port.recv(), 4);\n+    }\n+}"}, {"sha": "81a77000badcb078c7b2c89cc5eea2e97dfe0c31", "filename": "src/libstd/comm/select.rs", "status": "added", "additions": 498, "deletions": 0, "changes": 498, "blob_url": "https://github.com/rust-lang/rust/blob/bfa9064ba2687eb1d95708f72f41ddd9729a6ba1/src%2Flibstd%2Fcomm%2Fselect.rs", "raw_url": "https://github.com/rust-lang/rust/raw/bfa9064ba2687eb1d95708f72f41ddd9729a6ba1/src%2Flibstd%2Fcomm%2Fselect.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibstd%2Fcomm%2Fselect.rs?ref=bfa9064ba2687eb1d95708f72f41ddd9729a6ba1", "patch": "@@ -0,0 +1,498 @@\n+// Copyright 2013 The Rust Project Developers. See the COPYRIGHT\n+// file at the top-level directory of this distribution and at\n+// http://rust-lang.org/COPYRIGHT.\n+//\n+// Licensed under the Apache License, Version 2.0 <LICENSE-APACHE or\n+// http://www.apache.org/licenses/LICENSE-2.0> or the MIT license\n+// <LICENSE-MIT or http://opensource.org/licenses/MIT>, at your\n+// option. This file may not be copied, modified, or distributed\n+// except according to those terms.\n+\n+//! Selection over an array of ports\n+//!\n+//! This module contains the implementation machinery necessary for selecting\n+//! over a number of ports. One large goal of this module is to provide an\n+//! efficient interface to selecting over any port of any type.\n+//!\n+//! This is achieved through an architecture of a \"port set\" in which ports are\n+//! added to a set and then the entire set is waited on at once. The set can be\n+//! waited on multiple times to prevent re-adding each port to the set.\n+//!\n+//! Usage of this module is currently encouraged to go through the use of the\n+//! `select!` macro. This macro allows naturally binding of variables to the\n+//! received values of ports in a much more natural syntax then usage of the\n+//! `Select` structure directly.\n+//!\n+//! # Example\n+//!\n+//! ```rust\n+//! let (mut p1, c1) = Chan::new();\n+//! let (mut p2, c2) = Chan::new();\n+//!\n+//! c1.send(1);\n+//! c2.send(2);\n+//!\n+//! select! (\n+//!     val = p1.recv() => {\n+//!         assert_eq!(val, 1);\n+//!     }\n+//!     val = p2.recv() => {\n+//!         assert_eq!(val, 2);\n+//!     }\n+//! )\n+\n+use cast;\n+use iter::Iterator;\n+use kinds::Send;\n+use ops::Drop;\n+use option::{Some, None, Option};\n+use ptr::RawPtr;\n+use super::imp::BlockingContext;\n+use super::{Packet, Port, imp};\n+use uint;\n+use unstable::atomics::{Relaxed, SeqCst};\n+\n+macro_rules! select {\n+    (\n+        $name1:pat = $port1:ident.$meth1:ident() => $code1:expr,\n+        $($name:pat = $port:ident.$meth:ident() => $code:expr),*\n+    ) => ({\n+        use std::comm::Select;\n+        let sel = Select::new();\n+        let mut $port1 = sel.add(&mut $port1);\n+        $( let mut $port = sel.add(&mut $port); )*\n+        let ret = sel.wait();\n+        if ret == $port1.id { let $name1 = $port1.$meth1(); $code1 }\n+        $( else if ret == $port.id { let $name = $port.$meth(); $code } )*\n+        else { unreachable!() }\n+    })\n+}\n+\n+/// The \"port set\" of the select interface. This structure is used to manage a\n+/// set of ports which are being selected over.\n+#[no_freeze]\n+#[no_send]\n+pub struct Select {\n+    priv head: *mut Packet,\n+    priv tail: *mut Packet,\n+    priv next_id: uint,\n+}\n+\n+/// A handle to a port which is currently a member of a `Select` set of ports.\n+/// This handle is used to keep the port in the set as well as interact with the\n+/// underlying port.\n+pub struct Handle<'self, T> {\n+    id: uint,\n+    priv selector: &'self Select,\n+    priv port: &'self mut Port<T>,\n+}\n+\n+struct PacketIterator { priv cur: *mut Packet }\n+\n+impl Select {\n+    /// Creates a new selection structure. This set is initially empty and\n+    /// `wait` will fail!() if called.\n+    ///\n+    /// Usage of this struct directly can sometimes be burdensome, and usage is\n+    /// rather much easier through the `select!` macro.\n+    pub fn new() -> Select {\n+        Select {\n+            head: 0 as *mut Packet,\n+            tail: 0 as *mut Packet,\n+            next_id: 1,\n+        }\n+    }\n+\n+    /// Adds a new port to this set, returning a handle which is then used to\n+    /// receive on the port.\n+    ///\n+    /// Note that this port parameter takes `&mut Port` instead of `&Port`. None\n+    /// of the methods of receiving on a port require `&mut self`, but `&mut` is\n+    /// used here in order to have the compiler guarantee that the same port is\n+    /// not added to this set more than once.\n+    ///\n+    /// When the returned handle falls out of scope, the port will be removed\n+    /// from this set. While the handle is in this set, usage of the port can be\n+    /// done through the `Handle`'s receiving methods.\n+    pub fn add<'a, T: Send>(&'a self, port: &'a mut Port<T>) -> Handle<'a, T> {\n+        let this = unsafe { cast::transmute_mut(self) };\n+        let id = this.next_id;\n+        this.next_id += 1;\n+        unsafe {\n+            let packet = port.queue.packet();\n+            assert!(!(*packet).selecting.load(Relaxed));\n+            assert_eq!((*packet).selection_id, 0);\n+            (*packet).selection_id = id;\n+            if this.head.is_null() {\n+                this.head = packet;\n+                this.tail = packet;\n+            } else {\n+                (*packet).select_prev = this.tail;\n+                assert!((*packet).select_next.is_null());\n+                (*this.tail).select_next = packet;\n+                this.tail = packet;\n+            }\n+        }\n+        Handle { id: id, selector: this, port: port }\n+    }\n+\n+    /// Waits for an event on this port set. The returned valus is *not* and\n+    /// index, but rather an id. This id can be queried against any active\n+    /// `Handle` structures (each one has a public `id` field). The handle with\n+    /// the matching `id` will have some sort of event available on it. The\n+    /// event could either be that data is available or the corresponding\n+    /// channel has been closed.\n+    pub fn wait(&self) -> uint {\n+        // Note that this is currently an inefficient implementation. We in\n+        // theory have knowledge about all ports in the set ahead of time, so\n+        // this method shouldn't really have to iterate over all of them yet\n+        // again. The idea with this \"port set\" interface is to get the\n+        // interface right this time around, and later this implementation can\n+        // be optimized.\n+        //\n+        // This implementation can be summarized by:\n+        //\n+        //      fn select(ports) {\n+        //          if any port ready { return ready index }\n+        //          deschedule {\n+        //              block on all ports\n+        //          }\n+        //          unblock on all ports\n+        //          return ready index\n+        //      }\n+        //\n+        // Most notably, the iterations over all of the ports shouldn't be\n+        // necessary.\n+        unsafe {\n+            let mut amt = 0;\n+            for p in self.iter() {\n+                assert!(!(*p).selecting.load(Relaxed));\n+                amt += 1;\n+                if (*p).can_recv() {\n+                    return (*p).selection_id;\n+                }\n+            }\n+            assert!(amt > 0);\n+\n+            let mut ready_index = amt;\n+            let mut ready_id = uint::max_value;\n+            let mut iter = self.iter().enumerate();\n+\n+            // Acquire a number of blocking contexts, and block on each one\n+            // sequentially until one fails. If one fails, then abort\n+            // immediately so we can go unblock on all the other ports.\n+            BlockingContext::many(amt, |ctx| {\n+                let (i, packet) = iter.next().unwrap();\n+                (*packet).selecting.store(true, SeqCst);\n+                if !ctx.block(&mut (*packet).data,\n+                              &mut (*packet).to_wake,\n+                              || (*packet).decrement()) {\n+                    (*packet).abort_selection(false);\n+                    (*packet).selecting.store(false, SeqCst);\n+                    ready_index = i;\n+                    ready_id = (*packet).selection_id;\n+                    false\n+                } else {\n+                    true\n+                }\n+            });\n+\n+            // Abort the selection process on each port. If the abort process\n+            // returns `true`, then that means that the port is ready to receive\n+            // some data. Note that this also means that the port may have yet\n+            // to have fully read the `to_wake` field and woken us up (although\n+            // the wakeup is guaranteed to fail).\n+            //\n+            // This situation happens in the window of where a sender invokes\n+            // increment(), sees -1, and then decides to wake up the task. After\n+            // all this is done, the sending thread will set `selecting` to\n+            // `false`. Until this is done, we cannot return. If we were to\n+            // return, then a sender could wake up a port which has gone back to\n+            // sleep after this call to `select`.\n+            //\n+            // Note that it is a \"fairly small window\" in which an increment()\n+            // views that it should wake a thread up until the `selecting` bit\n+            // is set to false. For now, the implementation currently just spins\n+            // in a yield loop. This is very distasteful, but this\n+            // implementation is already nowhere near what it should ideally be.\n+            // A rewrite should focus on avoiding a yield loop, and for now this\n+            // implementation is tying us over to a more efficient \"don't\n+            // iterate over everything every time\" implementation.\n+            for packet in self.iter().take(ready_index) {\n+                if (*packet).abort_selection(true) {\n+                    ready_id = (*packet).selection_id;\n+                    while (*packet).selecting.load(Relaxed) {\n+                        imp::yield_now();\n+                    }\n+                }\n+            }\n+\n+            // Sanity check for now to make sure that everyone is turned off.\n+            for packet in self.iter() {\n+                assert!(!(*packet).selecting.load(Relaxed));\n+            }\n+\n+            return ready_id;\n+        }\n+    }\n+\n+    unsafe fn remove(&self, packet: *mut Packet) {\n+        let this = cast::transmute_mut(self);\n+        assert!(!(*packet).selecting.load(Relaxed));\n+        if (*packet).select_prev.is_null() {\n+            assert_eq!(packet, this.head);\n+            this.head = (*packet).select_next;\n+        } else {\n+            (*(*packet).select_prev).select_next = (*packet).select_next;\n+        }\n+        if (*packet).select_next.is_null() {\n+            assert_eq!(packet, this.tail);\n+            this.tail = (*packet).select_prev;\n+        } else {\n+            (*(*packet).select_next).select_prev = (*packet).select_prev;\n+        }\n+        (*packet).select_next = 0 as *mut Packet;\n+        (*packet).select_prev = 0 as *mut Packet;\n+        (*packet).selection_id = 0;\n+    }\n+\n+    fn iter(&self) -> PacketIterator { PacketIterator { cur: self.head } }\n+}\n+\n+impl<'self, T: Send> Handle<'self, T> {\n+    /// Receive a value on the underlying port. Has the same semantics as\n+    /// `Port.recv`\n+    pub fn recv(&mut self) -> T { self.port.recv() }\n+    /// Block to receive a value on the underlying port, returning `Some` on\n+    /// success or `None` if the channel disconnects. This function has the same\n+    /// semantics as `Port.recv_opt`\n+    pub fn recv_opt(&mut self) -> Option<T> { self.port.recv_opt() }\n+    /// Immediately attempt to receive a value on a port, this function will\n+    /// never block. Has the same semantics as `Port.try_recv`.\n+    pub fn try_recv(&mut self) -> Option<T> { self.port.try_recv() }\n+}\n+\n+#[unsafe_destructor]\n+impl Drop for Select {\n+    fn drop(&mut self) {\n+        assert!(self.head.is_null());\n+        assert!(self.tail.is_null());\n+    }\n+}\n+\n+#[unsafe_destructor]\n+impl<'self, T: Send> Drop for Handle<'self, T> {\n+    fn drop(&mut self) {\n+        unsafe { self.selector.remove(self.port.queue.packet()) }\n+    }\n+}\n+\n+impl Iterator<*mut Packet> for PacketIterator {\n+    fn next(&mut self) -> Option<*mut Packet> {\n+        if self.cur.is_null() {\n+            None\n+        } else {\n+            let ret = Some(self.cur);\n+            unsafe { self.cur = (*self.cur).select_next; }\n+            ret\n+        }\n+    }\n+}\n+\n+#[cfg(test)]\n+mod test {\n+    use super::super::*;\n+    use prelude::*;\n+\n+    test!(fn smoke() {\n+        let (mut p1, c1) = Chan::<int>::new();\n+        let (mut p2, c2) = Chan::<int>::new();\n+        c1.send(1);\n+        select! (\n+            foo = p1.recv() => { assert_eq!(foo, 1); },\n+            _bar = p2.recv() => { fail!() }\n+        )\n+        c2.send(2);\n+        select! (\n+            _foo = p1.recv() => { fail!() },\n+            bar = p2.recv() => { assert_eq!(bar, 2) }\n+        )\n+        drop(c1);\n+        select! (\n+            foo = p1.recv_opt() => { assert_eq!(foo, None); },\n+            _bar = p2.recv() => { fail!() }\n+        )\n+        drop(c2);\n+        select! (\n+            bar = p2.recv_opt() => { assert_eq!(bar, None); },\n+        )\n+    })\n+\n+    test!(fn smoke2() {\n+        let (mut p1, _c1) = Chan::<int>::new();\n+        let (mut p2, _c2) = Chan::<int>::new();\n+        let (mut p3, _c3) = Chan::<int>::new();\n+        let (mut p4, _c4) = Chan::<int>::new();\n+        let (mut p5, c5) = Chan::<int>::new();\n+        c5.send(4);\n+        select! (\n+            _foo = p1.recv() => { fail!(\"1\") },\n+            _foo = p2.recv() => { fail!(\"2\") },\n+            _foo = p3.recv() => { fail!(\"3\") },\n+            _foo = p4.recv() => { fail!(\"4\") },\n+            foo = p5.recv() => { assert_eq!(foo, 4); }\n+        )\n+    })\n+\n+    test!(fn closed() {\n+        let (mut p1, _c1) = Chan::<int>::new();\n+        let (mut p2, c2) = Chan::<int>::new();\n+        drop(c2);\n+\n+        select! (\n+            _a1 = p1.recv_opt() => { fail!() },\n+            a2 = p2.recv_opt() => { assert_eq!(a2, None); }\n+        )\n+    })\n+\n+    #[test]\n+    fn unblocks() {\n+        use std::io::timer;\n+\n+        let (mut p1, c1) = Chan::<int>::new();\n+        let (mut p2, _c2) = Chan::<int>::new();\n+        let (p3, c3) = Chan::<int>::new();\n+\n+        do spawn {\n+            timer::sleep(3);\n+            c1.send(1);\n+            p3.recv();\n+            timer::sleep(3);\n+        }\n+\n+        select! (\n+            a = p1.recv() => { assert_eq!(a, 1); },\n+            _b = p2.recv() => { fail!() }\n+        )\n+        c3.send(1);\n+        select! (\n+            a = p1.recv_opt() => { assert_eq!(a, None); },\n+            _b = p2.recv() => { fail!() }\n+        )\n+    }\n+\n+    #[test]\n+    fn both_ready() {\n+        use std::io::timer;\n+\n+        let (mut p1, c1) = Chan::<int>::new();\n+        let (mut p2, c2) = Chan::<int>::new();\n+        let (p3, c3) = Chan::<()>::new();\n+\n+        do spawn {\n+            timer::sleep(3);\n+            c1.send(1);\n+            c2.send(2);\n+            p3.recv();\n+        }\n+\n+        select! (\n+            a = p1.recv() => { assert_eq!(a, 1); },\n+            a = p2.recv() => { assert_eq!(a, 2); }\n+        )\n+        select! (\n+            a = p1.recv() => { assert_eq!(a, 1); },\n+            a = p2.recv() => { assert_eq!(a, 2); }\n+        )\n+        c3.send(());\n+    }\n+\n+    #[test]\n+    fn stress() {\n+        static AMT: int = 10000;\n+        let (mut p1, c1) = Chan::<int>::new();\n+        let (mut p2, c2) = Chan::<int>::new();\n+        let (p3, c3) = Chan::<()>::new();\n+\n+        do spawn {\n+            for i in range(0, AMT) {\n+                if i % 2 == 0 {\n+                    c1.send(i);\n+                } else {\n+                    c2.send(i);\n+                }\n+                p3.recv();\n+            }\n+        }\n+\n+        for i in range(0, AMT) {\n+            select! (\n+                i1 = p1.recv() => { assert!(i % 2 == 0 && i == i1); },\n+                i2 = p2.recv() => { assert!(i % 2 == 1 && i == i2); }\n+            )\n+            c3.send(());\n+        }\n+    }\n+\n+    #[test]\n+    fn stress_native() {\n+        use std::rt::thread::Thread;\n+        use std::unstable::run_in_bare_thread;\n+        static AMT: int = 10000;\n+\n+        do run_in_bare_thread {\n+            let (mut p1, c1) = Chan::<int>::new();\n+            let (mut p2, c2) = Chan::<int>::new();\n+            let (p3, c3) = Chan::<()>::new();\n+\n+            let t = do Thread::start {\n+                for i in range(0, AMT) {\n+                    if i % 2 == 0 {\n+                        c1.send(i);\n+                    } else {\n+                        c2.send(i);\n+                    }\n+                    p3.recv();\n+                }\n+            };\n+\n+            for i in range(0, AMT) {\n+                select! (\n+                    i1 = p1.recv() => { assert!(i % 2 == 0 && i == i1); },\n+                    i2 = p2.recv() => { assert!(i % 2 == 1 && i == i2); }\n+                )\n+                c3.send(());\n+            }\n+            t.join();\n+        }\n+    }\n+\n+    #[test]\n+    fn native_both_ready() {\n+        use std::rt::thread::Thread;\n+        use std::unstable::run_in_bare_thread;\n+\n+        do run_in_bare_thread {\n+            let (mut p1, c1) = Chan::<int>::new();\n+            let (mut p2, c2) = Chan::<int>::new();\n+            let (p3, c3) = Chan::<()>::new();\n+\n+            let t = do Thread::start {\n+                c1.send(1);\n+                c2.send(2);\n+                p3.recv();\n+            };\n+\n+            select! (\n+                a = p1.recv() => { assert_eq!(a, 1); },\n+                b = p2.recv() => { assert_eq!(b, 2); }\n+            )\n+            select! (\n+                a = p1.recv() => { assert_eq!(a, 1); },\n+                b = p2.recv() => { assert_eq!(b, 2); }\n+            )\n+            c3.send(());\n+            t.join();\n+        }\n+    }\n+}"}, {"sha": "6948eb60b1f6ccdf07b975a4b6a134709004d207", "filename": "src/libstd/lib.rs", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "blob_url": "https://github.com/rust-lang/rust/blob/bfa9064ba2687eb1d95708f72f41ddd9729a6ba1/src%2Flibstd%2Flib.rs", "raw_url": "https://github.com/rust-lang/rust/raw/bfa9064ba2687eb1d95708f72f41ddd9729a6ba1/src%2Flibstd%2Flib.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibstd%2Flib.rs?ref=bfa9064ba2687eb1d95708f72f41ddd9729a6ba1", "patch": "@@ -203,15 +203,16 @@ pub mod rt;\n mod std {\n     pub use clone;\n     pub use cmp;\n+    pub use comm;\n     pub use condition;\n     pub use fmt;\n+    pub use io;\n     pub use kinds;\n     pub use local_data;\n     pub use logging;\n     pub use logging;\n     pub use option;\n     pub use os;\n-    pub use io;\n     pub use rt;\n     pub use str;\n     pub use to_bytes;"}, {"sha": "1e04e5eb78d59932d5cf34a25623b73485412d2a", "filename": "src/libstd/rt/mpmc_bounded_queue.rs", "status": "modified", "additions": 7, "deletions": 11, "changes": 18, "blob_url": "https://github.com/rust-lang/rust/blob/bfa9064ba2687eb1d95708f72f41ddd9729a6ba1/src%2Flibstd%2Frt%2Fmpmc_bounded_queue.rs", "raw_url": "https://github.com/rust-lang/rust/raw/bfa9064ba2687eb1d95708f72f41ddd9729a6ba1/src%2Flibstd%2Frt%2Fmpmc_bounded_queue.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibstd%2Frt%2Fmpmc_bounded_queue.rs?ref=bfa9064ba2687eb1d95708f72f41ddd9729a6ba1", "patch": "@@ -1,5 +1,4 @@\n-/* Multi-producer/multi-consumer bounded queue\n- * Copyright (c) 2010-2011 Dmitry Vyukov. All rights reserved.\n+/* Copyright (c) 2010-2011 Dmitry Vyukov. All rights reserved.\n  * Redistribution and use in source and binary forms, with or without\n  * modification, are permitted provided that the following conditions are met:\n  *\n@@ -163,7 +162,6 @@ mod tests {\n     use prelude::*;\n     use option::*;\n     use task;\n-    use comm;\n     use super::Queue;\n \n     #[test]\n@@ -174,10 +172,9 @@ mod tests {\n         assert_eq!(None, q.pop());\n \n         for _ in range(0, nthreads) {\n-            let (port, chan)  = comm::stream();\n-            chan.send(q.clone());\n+            let q = q.clone();\n             do task::spawn_sched(task::SingleThreaded) {\n-                let mut q = port.recv();\n+                let mut q = q;\n                 for i in range(0, nmsgs) {\n                     assert!(q.push(i));\n                 }\n@@ -186,12 +183,11 @@ mod tests {\n \n         let mut completion_ports = ~[];\n         for _ in range(0, nthreads) {\n-            let (completion_port, completion_chan) = comm::stream();\n+            let (completion_port, completion_chan) = Chan::new();\n             completion_ports.push(completion_port);\n-            let (port, chan)  = comm::stream();\n-            chan.send(q.clone());\n+            let q = q.clone();\n             do task::spawn_sched(task::SingleThreaded) {\n-                let mut q = port.recv();\n+                let mut q = q;\n                 let mut i = 0u;\n                 loop {\n                     match q.pop() {\n@@ -206,7 +202,7 @@ mod tests {\n             }\n         }\n \n-        for completion_port in completion_ports.iter() {\n+        for completion_port in completion_ports.mut_iter() {\n             assert_eq!(nmsgs, completion_port.recv());\n         }\n     }"}, {"sha": "d575028af704399d07cf8cc70ffe18a542143cb9", "filename": "src/libstd/rt/mpsc_queue.rs", "status": "modified", "additions": 120, "deletions": 110, "changes": 230, "blob_url": "https://github.com/rust-lang/rust/blob/bfa9064ba2687eb1d95708f72f41ddd9729a6ba1/src%2Flibstd%2Frt%2Fmpsc_queue.rs", "raw_url": "https://github.com/rust-lang/rust/raw/bfa9064ba2687eb1d95708f72f41ddd9729a6ba1/src%2Flibstd%2Frt%2Fmpsc_queue.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibstd%2Frt%2Fmpsc_queue.rs?ref=bfa9064ba2687eb1d95708f72f41ddd9729a6ba1", "patch": "@@ -1,5 +1,4 @@\n-/* Multi-producer/single-consumer queue\n- * Copyright (c) 2010-2011 Dmitry Vyukov. All rights reserved.\n+/* Copyright (c) 2010-2011 Dmitry Vyukov. All rights reserved.\n  * Redistribution and use in source and binary forms, with or without\n  * modification, are permitted provided that the following conditions are met:\n  *\n@@ -27,177 +26,188 @@\n  */\n \n //! A mostly lock-free multi-producer, single consumer queue.\n-// http://www.1024cores.net/home/lock-free-algorithms/queues/intrusive-mpsc-node-based-queue\n \n-use unstable::sync::UnsafeArc;\n-use unstable::atomics::{AtomicPtr,Relaxed,Release,Acquire};\n-use ptr::{mut_null, to_mut_unsafe_ptr};\n+// http://www.1024cores.net/home/lock-free-algorithms\n+//                         /queues/non-intrusive-mpsc-node-based-queue\n+\n use cast;\n-use option::*;\n use clone::Clone;\n use kinds::Send;\n+use ops::Drop;\n+use option::{Option, None, Some};\n+use unstable::atomics::{AtomicPtr, Release, Acquire, AcqRel, Relaxed};\n+use unstable::sync::UnsafeArc;\n+\n+pub enum PopResult<T> {\n+    /// Some data has been popped\n+    Data(T),\n+    /// The queue is empty\n+    Empty,\n+    /// The queue is in an inconsistent state. Popping data should succeed, but\n+    /// some pushers have yet to make enough progress in order allow a pop to\n+    /// succeed. It is recommended that a pop() occur \"in the near future\" in\n+    /// order to see if the sender has made progress or not\n+    Inconsistent,\n+}\n \n struct Node<T> {\n     next: AtomicPtr<Node<T>>,\n     value: Option<T>,\n }\n \n-impl<T> Node<T> {\n-    fn empty() -> Node<T> {\n-        Node{next: AtomicPtr::new(mut_null()), value: None}\n-    }\n-\n-    fn with_value(value: T) -> Node<T> {\n-        Node{next: AtomicPtr::new(mut_null()), value: Some(value)}\n-    }\n-}\n-\n-struct State<T> {\n-    pad0: [u8, ..64],\n+struct State<T, P> {\n     head: AtomicPtr<Node<T>>,\n-    pad1: [u8, ..64],\n-    stub: Node<T>,\n-    pad2: [u8, ..64],\n     tail: *mut Node<T>,\n-    pad3: [u8, ..64],\n+    packet: P,\n }\n \n-struct Queue<T> {\n-    priv state: UnsafeArc<State<T>>,\n+pub struct Consumer<T, P> {\n+    priv state: UnsafeArc<State<T, P>>,\n }\n \n-impl<T: Send> Clone for Queue<T> {\n-    fn clone(&self) -> Queue<T> {\n-        Queue {\n-            state: self.state.clone()\n-        }\n-    }\n+pub struct Producer<T, P> {\n+    priv state: UnsafeArc<State<T, P>>,\n }\n \n-impl<T: Send> State<T> {\n-    pub fn new() -> State<T> {\n-        State{\n-            pad0: [0, ..64],\n-            head: AtomicPtr::new(mut_null()),\n-            pad1: [0, ..64],\n-            stub: Node::<T>::empty(),\n-            pad2: [0, ..64],\n-            tail: mut_null(),\n-            pad3: [0, ..64],\n-        }\n+impl<T: Send, P: Send> Clone for Producer<T, P> {\n+    fn clone(&self) -> Producer<T, P> {\n+        Producer { state: self.state.clone() }\n     }\n+}\n \n-    fn init(&mut self) {\n-        let stub = self.get_stub_unsafe();\n-        self.head.store(stub, Relaxed);\n-        self.tail = stub;\n+pub fn queue<T: Send, P: Send>(p: P) -> (Consumer<T, P>, Producer<T, P>) {\n+    unsafe {\n+        let (a, b) = UnsafeArc::new2(State::new(p));\n+        (Consumer { state: a }, Producer { state: b })\n     }\n+}\n \n-    fn get_stub_unsafe(&mut self) -> *mut Node<T> {\n-        to_mut_unsafe_ptr(&mut self.stub)\n+impl<T> Node<T> {\n+    unsafe fn new(v: Option<T>) -> *mut Node<T> {\n+        cast::transmute(~Node {\n+            next: AtomicPtr::new(0 as *mut Node<T>),\n+            value: v,\n+        })\n     }\n+}\n \n-    fn push(&mut self, value: T) {\n-        unsafe {\n-            let node = cast::transmute(~Node::with_value(value));\n-            self.push_node(node);\n+impl<T: Send, P: Send> State<T, P> {\n+    pub unsafe fn new(p: P) -> State<T, P> {\n+        let stub = Node::new(None);\n+        State {\n+            head: AtomicPtr::new(stub),\n+            tail: stub,\n+            packet: p,\n         }\n     }\n \n-    fn push_node(&mut self, node: *mut Node<T>) {\n-        unsafe {\n-            (*node).next.store(mut_null(), Release);\n-            let prev = self.head.swap(node, Relaxed);\n-            (*prev).next.store(node, Release);\n-        }\n+    unsafe fn push(&mut self, t: T) {\n+        let n = Node::new(Some(t));\n+        let prev = self.head.swap(n, AcqRel);\n+        (*prev).next.store(n, Release);\n     }\n \n-    fn pop(&mut self) -> Option<T> {\n-        unsafe {\n-            let mut tail = self.tail;\n-            let mut next = (*tail).next.load(Acquire);\n-            let stub = self.get_stub_unsafe();\n-            if tail == stub {\n-                if mut_null() == next {\n-                    return None\n-                }\n-                self.tail = next;\n-                tail = next;\n-                next = (*next).next.load(Acquire);\n-            }\n-            if next != mut_null() {\n-                let tail: ~Node<T> = cast::transmute(tail);\n-                self.tail = next;\n-                return tail.value\n-            }\n-            let head = self.head.load(Relaxed);\n-            if tail != head {\n-                return None\n-            }\n-            self.push_node(stub);\n-            next = (*tail).next.load(Acquire);\n-            if next != mut_null() {\n-                let tail: ~Node<T> = cast::transmute(tail);\n-                self.tail = next;\n-                return tail.value\n-            }\n+    unsafe fn pop(&mut self) -> PopResult<T> {\n+        let tail = self.tail;\n+        let next = (*tail).next.load(Acquire);\n+\n+        if !next.is_null() {\n+            self.tail = next;\n+            assert!((*tail).value.is_none());\n+            assert!((*next).value.is_some());\n+            let ret = (*next).value.take_unwrap();\n+            let _: ~Node<T> = cast::transmute(tail);\n+            return Data(ret);\n         }\n-        None\n+\n+        if self.head.load(Acquire) == tail {Empty} else {Inconsistent}\n+    }\n+\n+    unsafe fn is_empty(&mut self) -> bool {\n+        return (*self.tail).next.load(Acquire).is_null();\n     }\n }\n \n-impl<T: Send> Queue<T> {\n-    pub fn new() -> Queue<T> {\n+#[unsafe_destructor]\n+impl<T: Send, P: Send> Drop for State<T, P> {\n+    fn drop(&mut self) {\n         unsafe {\n-            let q = Queue{state: UnsafeArc::new(State::new())};\n-            (*q.state.get()).init();\n-            q\n+            let mut cur = self.tail;\n+            while !cur.is_null() {\n+                let next = (*cur).next.load(Relaxed);\n+                let _: ~Node<T> = cast::transmute(cur);\n+                cur = next;\n+            }\n         }\n     }\n+}\n \n+impl<T: Send, P: Send> Producer<T, P> {\n     pub fn push(&mut self, value: T) {\n         unsafe { (*self.state.get()).push(value) }\n     }\n+    pub fn is_empty(&self) -> bool {\n+        unsafe{ (*self.state.get()).is_empty() }\n+    }\n+    pub unsafe fn packet(&self) -> *mut P {\n+        &mut (*self.state.get()).packet as *mut P\n+    }\n+}\n \n-    pub fn pop(&mut self) -> Option<T> {\n-        unsafe{ (*self.state.get()).pop() }\n+impl<T: Send, P: Send> Consumer<T, P> {\n+    pub fn pop(&mut self) -> PopResult<T> {\n+        unsafe { (*self.state.get()).pop() }\n+    }\n+    pub fn casual_pop(&mut self) -> Option<T> {\n+        match self.pop() {\n+            Data(t) => Some(t),\n+            Empty | Inconsistent => None,\n+        }\n+    }\n+    pub unsafe fn packet(&self) -> *mut P {\n+        &mut (*self.state.get()).packet as *mut P\n     }\n }\n \n #[cfg(test)]\n mod tests {\n     use prelude::*;\n-    use option::*;\n+\n     use task;\n-    use comm;\n-    use super::Queue;\n+    use super::{queue, Data, Empty, Inconsistent};\n+\n+    #[test]\n+    fn test_full() {\n+        let (_, mut p) = queue(());\n+        p.push(~1);\n+        p.push(~2);\n+    }\n \n     #[test]\n     fn test() {\n         let nthreads = 8u;\n         let nmsgs = 1000u;\n-        let mut q = Queue::new();\n-        assert_eq!(None, q.pop());\n+        let (mut c, p) = queue(());\n+        match c.pop() {\n+            Empty => {}\n+            Inconsistent | Data(..) => fail!()\n+        }\n \n         for _ in range(0, nthreads) {\n-            let (port, chan)  = comm::stream();\n-            chan.send(q.clone());\n+            let q = p.clone();\n             do task::spawn_sched(task::SingleThreaded) {\n-                let mut q = port.recv();\n+                let mut q = q;\n                 for i in range(0, nmsgs) {\n                     q.push(i);\n                 }\n             }\n         }\n \n         let mut i = 0u;\n-        loop {\n-            match q.pop() {\n-                None => {},\n-                Some(_) => {\n-                    i += 1;\n-                    if i == nthreads*nmsgs { break }\n-                }\n+        while i < nthreads * nmsgs {\n+            match c.pop() {\n+                Empty | Inconsistent => {},\n+                Data(_) => { i += 1 }\n             }\n         }\n     }"}, {"sha": "f14533d726a789bb909d571fcd5c26289989ca6e", "filename": "src/libstd/rt/spsc_queue.rs", "status": "added", "additions": 296, "deletions": 0, "changes": 296, "blob_url": "https://github.com/rust-lang/rust/blob/bfa9064ba2687eb1d95708f72f41ddd9729a6ba1/src%2Flibstd%2Frt%2Fspsc_queue.rs", "raw_url": "https://github.com/rust-lang/rust/raw/bfa9064ba2687eb1d95708f72f41ddd9729a6ba1/src%2Flibstd%2Frt%2Fspsc_queue.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibstd%2Frt%2Fspsc_queue.rs?ref=bfa9064ba2687eb1d95708f72f41ddd9729a6ba1", "patch": "@@ -0,0 +1,296 @@\n+/* Copyright (c) 2010-2011 Dmitry Vyukov. All rights reserved.\n+ * Redistribution and use in source and binary forms, with or without\n+ * modification, are permitted provided that the following conditions are met:\n+ *\n+ *    1. Redistributions of source code must retain the above copyright notice,\n+ *       this list of conditions and the following disclaimer.\n+ *\n+ *    2. Redistributions in binary form must reproduce the above copyright\n+ *       notice, this list of conditions and the following disclaimer in the\n+ *       documentation and/or other materials provided with the distribution.\n+ *\n+ * THIS SOFTWARE IS PROVIDED BY DMITRY VYUKOV \"AS IS\" AND ANY EXPRESS OR IMPLIED\n+ * WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF\n+ * MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT\n+ * SHALL DMITRY VYUKOV OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT,\n+ * INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\n+ * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR\n+ * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF\n+ * LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE\n+ * OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF\n+ * ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n+ *\n+ * The views and conclusions contained in the software and documentation are\n+ * those of the authors and should not be interpreted as representing official\n+ * policies, either expressed or implied, of Dmitry Vyukov.\n+ */\n+\n+// http://www.1024cores.net/home/lock-free-algorithms/queues/unbounded-spsc-queue\n+use cast;\n+use kinds::Send;\n+use ops::Drop;\n+use option::{Some, None, Option};\n+use unstable::atomics::{AtomicPtr, Relaxed, AtomicUint, Acquire, Release};\n+use unstable::sync::UnsafeArc;\n+\n+// Node within the linked list queue of messages to send\n+struct Node<T> {\n+    // XXX: this could be an uninitialized T if we're careful enough, and\n+    //      that would reduce memory usage (and be a bit faster).\n+    //      is it worth it?\n+    value: Option<T>,           // nullable for re-use of nodes\n+    next: AtomicPtr<Node<T>>,   // next node in the queue\n+}\n+\n+// The producer/consumer halves both need access to the `tail` field, and if\n+// they both have access to that we may as well just give them both access\n+// to this whole structure.\n+struct State<T, P> {\n+    // consumer fields\n+    tail: *mut Node<T>, // where to pop from\n+    tail_prev: AtomicPtr<Node<T>>, // where to pop from\n+\n+    // producer fields\n+    head: *mut Node<T>,      // where to push to\n+    first: *mut Node<T>,     // where to get new nodes from\n+    tail_copy: *mut Node<T>, // between first/tail\n+\n+    // Cache maintenance fields. Additions and subtractions are stored\n+    // separately in order to allow them to use nonatomic addition/subtraction.\n+    cache_bound: uint,\n+    cache_additions: AtomicUint,\n+    cache_subtractions: AtomicUint,\n+\n+    packet: P,\n+}\n+\n+pub struct Producer<T, P> {\n+    priv state: UnsafeArc<State<T, P>>,\n+}\n+\n+pub struct Consumer<T, P> {\n+    priv state: UnsafeArc<State<T, P>>,\n+}\n+\n+pub fn queue<T: Send, P: Send>(bound: uint,\n+                               p: P) -> (Consumer<T, P>, Producer<T, P>)\n+{\n+    let n1 = Node::new();\n+    let n2 = Node::new();\n+    unsafe { (*n1).next.store(n2, Relaxed) }\n+    let state = State {\n+        tail: n2,\n+        tail_prev: AtomicPtr::new(n1),\n+        head: n2,\n+        first: n1,\n+        tail_copy: n1,\n+        cache_bound: bound,\n+        cache_additions: AtomicUint::new(0),\n+        cache_subtractions: AtomicUint::new(0),\n+        packet: p,\n+    };\n+    let (arc1, arc2) = UnsafeArc::new2(state);\n+    (Consumer { state: arc1 }, Producer { state: arc2 })\n+}\n+\n+impl<T: Send> Node<T> {\n+    fn new() -> *mut Node<T> {\n+        unsafe {\n+            cast::transmute(~Node {\n+                value: None,\n+                next: AtomicPtr::new(0 as *mut Node<T>),\n+            })\n+        }\n+    }\n+}\n+\n+impl<T: Send, P: Send> Producer<T, P> {\n+    pub fn push(&mut self, t: T) {\n+        unsafe { (*self.state.get()).push(t) }\n+    }\n+    pub fn is_empty(&self) -> bool {\n+        unsafe { (*self.state.get()).is_empty() }\n+    }\n+    pub unsafe fn packet(&self) -> *mut P {\n+        &mut (*self.state.get()).packet as *mut P\n+    }\n+}\n+\n+impl<T: Send, P: Send> Consumer<T, P> {\n+    pub fn pop(&mut self) -> Option<T> {\n+        unsafe { (*self.state.get()).pop() }\n+    }\n+    pub unsafe fn packet(&self) -> *mut P {\n+        &mut (*self.state.get()).packet as *mut P\n+    }\n+}\n+\n+impl<T: Send, P: Send> State<T, P> {\n+    // remember that there is only one thread executing `push` (and only one\n+    // thread executing `pop`)\n+    unsafe fn push(&mut self, t: T) {\n+        // Acquire a node (which either uses a cached one or allocates a new\n+        // one), and then append this to the 'head' node.\n+        let n = self.alloc();\n+        assert!((*n).value.is_none());\n+        (*n).value = Some(t);\n+        (*n).next.store(0 as *mut Node<T>, Relaxed);\n+        (*self.head).next.store(n, Release);\n+        self.head = n;\n+    }\n+\n+    unsafe fn alloc(&mut self) -> *mut Node<T> {\n+        // First try to see if we can consume the 'first' node for our uses.\n+        // We try to avoid as many atomic instructions as possible here, so\n+        // the addition to cache_subtractions is not atomic (plus we're the\n+        // only one subtracting from the cache).\n+        if self.first != self.tail_copy {\n+            if self.cache_bound > 0 {\n+                let b = self.cache_subtractions.load(Relaxed);\n+                self.cache_subtractions.store(b + 1, Relaxed);\n+            }\n+            let ret = self.first;\n+            self.first = (*ret).next.load(Relaxed);\n+            return ret;\n+        }\n+        // If the above fails, then update our copy of the tail and try\n+        // again.\n+        self.tail_copy = self.tail_prev.load(Acquire);\n+        if self.first != self.tail_copy {\n+            if self.cache_bound > 0 {\n+                let b = self.cache_subtractions.load(Relaxed);\n+                self.cache_subtractions.store(b + 1, Relaxed);\n+            }\n+            let ret = self.first;\n+            self.first = (*ret).next.load(Relaxed);\n+            return ret;\n+        }\n+        // If all of that fails, then we have to allocate a new node\n+        // (there's nothing in the node cache).\n+        Node::new()\n+    }\n+\n+    // remember that there is only one thread executing `pop` (and only one\n+    // thread executing `push`)\n+    unsafe fn pop(&mut self) -> Option<T> {\n+        // The `tail` node is not actually a used node, but rather a\n+        // sentinel from where we should start popping from. Hence, look at\n+        // tail's next field and see if we can use it. If we do a pop, then\n+        // the current tail node is a candidate for going into the cache.\n+        let tail = self.tail;\n+        let next = (*tail).next.load(Acquire);\n+        if next.is_null() { return None }\n+        assert!((*next).value.is_some());\n+        let ret = (*next).value.take();\n+\n+        self.tail = next;\n+        if self.cache_bound == 0 {\n+            self.tail_prev.store(tail, Release);\n+        } else {\n+            // XXX: this is dubious with overflow.\n+            let additions = self.cache_additions.load(Relaxed);\n+            let subtractions = self.cache_subtractions.load(Relaxed);\n+            let size = additions - subtractions;\n+\n+            if size < self.cache_bound {\n+                self.tail_prev.store(tail, Release);\n+                self.cache_additions.store(additions + 1, Relaxed);\n+            } else {\n+                (*self.tail_prev.load(Relaxed)).next.store(next, Relaxed);\n+                // We have successfully erased all references to 'tail', so\n+                // now we can safely drop it.\n+                let _: ~Node<T> = cast::transmute(tail);\n+            }\n+        }\n+        return ret;\n+    }\n+\n+    unsafe fn is_empty(&self) -> bool {\n+        let tail = self.tail;\n+        let next = (*tail).next.load(Acquire);\n+        return next.is_null();\n+    }\n+}\n+\n+#[unsafe_destructor]\n+impl<T: Send, P: Send> Drop for State<T, P> {\n+    fn drop(&mut self) {\n+        unsafe {\n+            let mut cur = self.first;\n+            while !cur.is_null() {\n+                let next = (*cur).next.load(Relaxed);\n+                let _n: ~Node<T> = cast::transmute(cur);\n+                cur = next;\n+            }\n+        }\n+    }\n+}\n+\n+#[cfg(test)]\n+mod test {\n+    use prelude::*;\n+    use super::queue;\n+    use task;\n+\n+    #[test]\n+    fn smoke() {\n+        let (mut c, mut p) = queue(0, ());\n+        p.push(1);\n+        p.push(2);\n+        assert_eq!(c.pop(), Some(1));\n+        assert_eq!(c.pop(), Some(2));\n+        assert_eq!(c.pop(), None);\n+        p.push(3);\n+        p.push(4);\n+        assert_eq!(c.pop(), Some(3));\n+        assert_eq!(c.pop(), Some(4));\n+        assert_eq!(c.pop(), None);\n+    }\n+\n+    #[test]\n+    fn drop_full() {\n+        let (_, mut p) = queue(0, ());\n+        p.push(~1);\n+        p.push(~2);\n+    }\n+\n+    #[test]\n+    fn smoke_bound() {\n+        let (mut c, mut p) = queue(1, ());\n+        p.push(1);\n+        p.push(2);\n+        assert_eq!(c.pop(), Some(1));\n+        assert_eq!(c.pop(), Some(2));\n+        assert_eq!(c.pop(), None);\n+        p.push(3);\n+        p.push(4);\n+        assert_eq!(c.pop(), Some(3));\n+        assert_eq!(c.pop(), Some(4));\n+        assert_eq!(c.pop(), None);\n+    }\n+\n+    #[test]\n+    fn stress() {\n+        stress_bound(0);\n+        stress_bound(1);\n+\n+        fn stress_bound(bound: uint) {\n+            let (c, mut p) = queue(bound, ());\n+            do task::spawn_sched(task::SingleThreaded) {\n+                let mut c = c;\n+                for _ in range(0, 100000) {\n+                    loop {\n+                        match c.pop() {\n+                            Some(1) => break,\n+                            Some(_) => fail!(),\n+                            None => {}\n+                        }\n+                    }\n+                }\n+            }\n+            for _ in range(0, 100000) {\n+                p.push(1);\n+            }\n+        }\n+    }\n+}"}, {"sha": "2adc32f33fbec8bd4c1bce76c122c219ad3c26d5", "filename": "src/libstd/rt/task.rs", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "blob_url": "https://github.com/rust-lang/rust/blob/bfa9064ba2687eb1d95708f72f41ddd9729a6ba1/src%2Flibstd%2Frt%2Ftask.rs", "raw_url": "https://github.com/rust-lang/rust/raw/bfa9064ba2687eb1d95708f72f41ddd9729a6ba1/src%2Flibstd%2Frt%2Ftask.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibstd%2Frt%2Ftask.rs?ref=bfa9064ba2687eb1d95708f72f41ddd9729a6ba1", "patch": "@@ -26,7 +26,6 @@ use option::{Option, Some, None};\n use rt::borrowck::BorrowRecord;\n use rt::borrowck;\n use rt::context::Context;\n-use rt::context;\n use rt::env;\n use io::Writer;\n use rt::kill::Death;"}]}