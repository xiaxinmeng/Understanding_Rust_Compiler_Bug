{"sha": "4478ded57c3a6cd86ad2e6b8cac1f6b4d46d1130", "node_id": "MDY6Q29tbWl0NzI0NzEyOjQ0NzhkZWQ1N2MzYTZjZDg2YWQyZTZiOGNhYzFmNmI0ZDQ2ZDExMzA=", "commit": {"author": {"name": "bors", "email": "bors@rust-lang.org", "date": "2013-07-11T22:55:45Z"}, "committer": {"name": "bors", "email": "bors@rust-lang.org", "date": "2013-07-11T22:55:45Z"}, "message": "auto merge of #7623 : graydon/rust/codegen-compiletests, r=pcwalton\n\nThis is some initial sketch-work for #7461 though it will depend on #7459 to be useful for anything. For the time being, just infrastructure.", "tree": {"sha": "578e4e4f601ca368d976d23f296a98175d61d4bc", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/578e4e4f601ca368d976d23f296a98175d61d4bc"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/4478ded57c3a6cd86ad2e6b8cac1f6b4d46d1130", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/4478ded57c3a6cd86ad2e6b8cac1f6b4d46d1130", "html_url": "https://github.com/rust-lang/rust/commit/4478ded57c3a6cd86ad2e6b8cac1f6b4d46d1130", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/4478ded57c3a6cd86ad2e6b8cac1f6b4d46d1130/comments", "author": {"login": "bors", "id": 3372342, "node_id": "MDQ6VXNlcjMzNzIzNDI=", "avatar_url": "https://avatars.githubusercontent.com/u/3372342?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bors", "html_url": "https://github.com/bors", "followers_url": "https://api.github.com/users/bors/followers", "following_url": "https://api.github.com/users/bors/following{/other_user}", "gists_url": "https://api.github.com/users/bors/gists{/gist_id}", "starred_url": "https://api.github.com/users/bors/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bors/subscriptions", "organizations_url": "https://api.github.com/users/bors/orgs", "repos_url": "https://api.github.com/users/bors/repos", "events_url": "https://api.github.com/users/bors/events{/privacy}", "received_events_url": "https://api.github.com/users/bors/received_events", "type": "User", "site_admin": false}, "committer": {"login": "bors", "id": 3372342, "node_id": "MDQ6VXNlcjMzNzIzNDI=", "avatar_url": "https://avatars.githubusercontent.com/u/3372342?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bors", "html_url": "https://github.com/bors", "followers_url": "https://api.github.com/users/bors/followers", "following_url": "https://api.github.com/users/bors/following{/other_user}", "gists_url": "https://api.github.com/users/bors/gists{/gist_id}", "starred_url": "https://api.github.com/users/bors/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bors/subscriptions", "organizations_url": "https://api.github.com/users/bors/orgs", "repos_url": "https://api.github.com/users/bors/repos", "events_url": "https://api.github.com/users/bors/events{/privacy}", "received_events_url": "https://api.github.com/users/bors/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "f92b75ac06a86daac8d230285b605ec2ed97214b", "url": "https://api.github.com/repos/rust-lang/rust/commits/f92b75ac06a86daac8d230285b605ec2ed97214b", "html_url": "https://github.com/rust-lang/rust/commit/f92b75ac06a86daac8d230285b605ec2ed97214b"}, {"sha": "bbdbd3c69d35e3409d6c652c188ebd2d951b2e0e", "url": "https://api.github.com/repos/rust-lang/rust/commits/bbdbd3c69d35e3409d6c652c188ebd2d951b2e0e", "html_url": "https://github.com/rust-lang/rust/commit/bbdbd3c69d35e3409d6c652c188ebd2d951b2e0e"}], "stats": {"total": 989, "additions": 715, "deletions": 274}, "files": [{"sha": "8f757a0715f8cb0266bd1ff6adb86bc4b9f09383", "filename": "configure", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "blob_url": "https://github.com/rust-lang/rust/blob/4478ded57c3a6cd86ad2e6b8cac1f6b4d46d1130/configure", "raw_url": "https://github.com/rust-lang/rust/raw/4478ded57c3a6cd86ad2e6b8cac1f6b4d46d1130/configure", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/configure?ref=4478ded57c3a6cd86ad2e6b8cac1f6b4d46d1130", "patch": "@@ -731,6 +731,7 @@ do\n     make_dir $h/test/perf\n     make_dir $h/test/pretty\n     make_dir $h/test/debug-info\n+    make_dir $h/test/codegen\n     make_dir $h/test/doc-tutorial\n     make_dir $h/test/doc-tutorial-ffi\n     make_dir $h/test/doc-tutorial-macros"}, {"sha": "7a5a5dc15c30e5d08d0419a8a7dc6cfde2ebd42a", "filename": "mk/tests.mk", "status": "modified", "additions": 18, "deletions": 1, "changes": 19, "blob_url": "https://github.com/rust-lang/rust/blob/4478ded57c3a6cd86ad2e6b8cac1f6b4d46d1130/mk%2Ftests.mk", "raw_url": "https://github.com/rust-lang/rust/raw/4478ded57c3a6cd86ad2e6b8cac1f6b4d46d1130/mk%2Ftests.mk", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/mk%2Ftests.mk?ref=4478ded57c3a6cd86ad2e6b8cac1f6b4d46d1130", "patch": "@@ -246,6 +246,7 @@ check-stage$(1)-T-$(2)-H-$(3)-exec:     \t\t\t\t\\\n         check-stage$(1)-T-$(2)-H-$(3)-crates-exec                      \\\n \tcheck-stage$(1)-T-$(2)-H-$(3)-bench-exec\t\t\t\\\n \tcheck-stage$(1)-T-$(2)-H-$(3)-debuginfo-exec \\\n+\tcheck-stage$(1)-T-$(2)-H-$(3)-codegen-exec \\\n \tcheck-stage$(1)-T-$(2)-H-$(3)-doc-exec \\\n \tcheck-stage$(1)-T-$(2)-H-$(3)-pretty-exec\n \n@@ -430,6 +431,8 @@ CFAIL_RS := $(wildcard $(S)src/test/compile-fail/*.rs)\n BENCH_RS := $(wildcard $(S)src/test/bench/*.rs)\n PRETTY_RS := $(wildcard $(S)src/test/pretty/*.rs)\n DEBUGINFO_RS := $(wildcard $(S)src/test/debug-info/*.rs)\n+CODEGEN_RS := $(wildcard $(S)src/test/codegen/*.rs)\n+CODEGEN_CC := $(wildcard $(S)src/test/codegen/*.cc)\n \n # perf tests are the same as bench tests only they run under\n # a performance monitor.\n@@ -443,6 +446,7 @@ BENCH_TESTS := $(BENCH_RS)\n PERF_TESTS := $(PERF_RS)\n PRETTY_TESTS := $(PRETTY_RS)\n DEBUGINFO_TESTS := $(DEBUGINFO_RS)\n+CODEGEN_TESTS := $(CODEGEN_RS) $(CODEGEN_CC)\n \n CTEST_SRC_BASE_rpass = run-pass\n CTEST_BUILD_BASE_rpass = run-pass\n@@ -479,10 +483,19 @@ CTEST_BUILD_BASE_debuginfo = debug-info\n CTEST_MODE_debuginfo = debug-info\n CTEST_RUNTOOL_debuginfo = $(CTEST_RUNTOOL)\n \n+CTEST_SRC_BASE_codegen = codegen\n+CTEST_BUILD_BASE_codegen = codegen\n+CTEST_MODE_codegen = codegen\n+CTEST_RUNTOOL_codegen = $(CTEST_RUNTOOL)\n+\n ifeq ($(CFG_GDB),)\n CTEST_DISABLE_debuginfo = \"no gdb found\"\n endif\n \n+ifeq ($(CFG_CLANG),)\n+CTEST_DISABLE_codegen = \"no clang found\"\n+endif\n+\n ifeq ($(CFG_OSTYPE),apple-darwin)\n CTEST_DISABLE_debuginfo = \"gdb on darwing needs root\"\n endif\n@@ -507,6 +520,8 @@ CTEST_COMMON_ARGS$(1)-T-$(2)-H-$(3) :=\t\t\t\t\t\t\\\n \t\t--compile-lib-path $$(HLIB$(1)_H_$(3))\t\t\t\t\\\n         --run-lib-path $$(TLIB$(1)_T_$(2)_H_$(3))\t\t\t\\\n         --rustc-path $$(HBIN$(1)_H_$(3))/rustc$$(X_$(3))\t\t\t\\\n+        --clang-path $(if $(CFG_CLANG),$(CFG_CLANG),clang) \\\n+        --llvm-bin-path $(CFG_LLVM_INST_DIR_$(CFG_BUILD_TRIPLE))/bin \\\n         --aux-base $$(S)src/test/auxiliary/                 \\\n         --stage-id stage$(1)-$(2)\t\t\t\t\t\t\t\\\n         --target $(2)                                       \\\n@@ -522,6 +537,7 @@ CTEST_DEPS_cfail_$(1)-T-$(2)-H-$(3) = $$(CFAIL_TESTS)\n CTEST_DEPS_bench_$(1)-T-$(2)-H-$(3) = $$(BENCH_TESTS)\n CTEST_DEPS_perf_$(1)-T-$(2)-H-$(3) = $$(PERF_TESTS)\n CTEST_DEPS_debuginfo_$(1)-T-$(2)-H-$(3) = $$(DEBUGINFO_TESTS)\n+CTEST_DEPS_codegen_$(1)-T-$(2)-H-$(3) = $$(CODEGEN_TESTS)\n \n endef\n \n@@ -565,7 +581,7 @@ endif\n \n endef\n \n-CTEST_NAMES = rpass rpass-full rfail cfail bench perf debuginfo\n+CTEST_NAMES = rpass rpass-full rfail cfail bench perf debuginfo codegen\n \n $(foreach host,$(CFG_HOST_TRIPLES), \\\n  $(eval $(foreach target,$(CFG_TARGET_TRIPLES), \\\n@@ -674,6 +690,7 @@ TEST_GROUPS = \\\n \tbench \\\n \tperf \\\n \tdebuginfo \\\n+\tcodegen \\\n \tdoc \\\n \t$(foreach docname,$(DOC_TEST_NAMES),doc-$(docname)) \\\n \tpretty \\"}, {"sha": "df00286c87f2504943308d139497ad5e2ca3a25c", "filename": "src/compiletest/common.rs", "status": "modified", "additions": 7, "deletions": 0, "changes": 7, "blob_url": "https://github.com/rust-lang/rust/blob/4478ded57c3a6cd86ad2e6b8cac1f6b4d46d1130/src%2Fcompiletest%2Fcommon.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4478ded57c3a6cd86ad2e6b8cac1f6b4d46d1130/src%2Fcompiletest%2Fcommon.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Fcompiletest%2Fcommon.rs?ref=4478ded57c3a6cd86ad2e6b8cac1f6b4d46d1130", "patch": "@@ -15,6 +15,7 @@ pub enum mode {\n     mode_run_pass,\n     mode_pretty,\n     mode_debug_info,\n+    mode_codegen\n }\n \n pub struct config {\n@@ -27,6 +28,12 @@ pub struct config {\n     // The rustc executable\n     rustc_path: Path,\n \n+    // The clang executable\n+    clang_path: Option<Path>,\n+\n+    // The llvm binaries path\n+    llvm_bin_path: Option<Path>,\n+\n     // The directory containing the tests to run\n     src_base: Path,\n "}, {"sha": "a411e714247ed4f9b547d3e40a09a986186b0d0c", "filename": "src/compiletest/compiletest.rs", "status": "modified", "additions": 48, "deletions": 18, "changes": 66, "blob_url": "https://github.com/rust-lang/rust/blob/4478ded57c3a6cd86ad2e6b8cac1f6b4d46d1130/src%2Fcompiletest%2Fcompiletest.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4478ded57c3a6cd86ad2e6b8cac1f6b4d46d1130/src%2Fcompiletest%2Fcompiletest.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Fcompiletest%2Fcompiletest.rs?ref=4478ded57c3a6cd86ad2e6b8cac1f6b4d46d1130", "patch": "@@ -19,6 +19,7 @@ extern mod extra;\n use std::os;\n \n use extra::getopts;\n+use extra::getopts::groups::{optopt, optflag, reqopt};\n use extra::test;\n \n use common::config;\n@@ -27,6 +28,7 @@ use common::mode_run_fail;\n use common::mode_compile_fail;\n use common::mode_pretty;\n use common::mode_debug_info;\n+use common::mode_codegen;\n use common::mode;\n use util::logv;\n \n@@ -45,31 +47,54 @@ pub fn main() {\n }\n \n pub fn parse_config(args: ~[~str]) -> config {\n-    let opts =\n-        ~[getopts::reqopt(\"compile-lib-path\"),\n-          getopts::reqopt(\"run-lib-path\"),\n-          getopts::reqopt(\"rustc-path\"), getopts::reqopt(\"src-base\"),\n-          getopts::reqopt(\"build-base\"), getopts::reqopt(\"aux-base\"),\n-          getopts::reqopt(\"stage-id\"),\n-          getopts::reqopt(\"mode\"), getopts::optflag(\"ignored\"),\n-          getopts::optopt(\"runtool\"), getopts::optopt(\"rustcflags\"),\n-          getopts::optflag(\"verbose\"),\n-          getopts::optopt(\"logfile\"),\n-          getopts::optflag(\"jit\"),\n-          getopts::optflag(\"newrt\"),\n-          getopts::optopt(\"target\"),\n-          getopts::optopt(\"adb-path\"),\n-          getopts::optopt(\"adb-test-dir\")\n+\n+    let groups : ~[getopts::groups::OptGroup] =\n+        ~[reqopt(\"\", \"compile-lib-path\", \"path to host shared libraries\", \"PATH\"),\n+          reqopt(\"\", \"run-lib-path\", \"path to target shared libraries\", \"PATH\"),\n+          reqopt(\"\", \"rustc-path\", \"path to rustc to use for compiling\", \"PATH\"),\n+          optopt(\"\", \"clang-path\", \"path to  executable for codegen tests\", \"PATH\"),\n+          optopt(\"\", \"llvm-bin-path\", \"path to directory holding llvm binaries\", \"DIR\"),\n+          reqopt(\"\", \"src-base\", \"directory to scan for test files\", \"PATH\"),\n+          reqopt(\"\", \"build-base\", \"directory to deposit test outputs\", \"PATH\"),\n+          reqopt(\"\", \"aux-base\", \"directory to find auxiliary test files\", \"PATH\"),\n+          reqopt(\"\", \"stage-id\", \"the target-stage identifier\", \"stageN-TARGET\"),\n+          reqopt(\"\", \"mode\", \"which sort of compile tests to run\",\n+                 \"(compile-fail|run-fail|run-pass|pretty|debug-info)\"),\n+          optflag(\"\", \"ignored\", \"run tests marked as ignored / xfailed\"),\n+          optopt(\"\", \"runtool\", \"supervisor program to run tests under \\\n+                                 (eg. emulator, valgrind)\", \"PROGRAM\"),\n+          optopt(\"\", \"rustcflags\", \"flags to pass to rustc\", \"FLAGS\"),\n+          optflag(\"\", \"verbose\", \"run tests verbosely, showing all output\"),\n+          optopt(\"\", \"logfile\", \"file to log test execution to\", \"FILE\"),\n+          optflag(\"\", \"jit\", \"run tests under the JIT\"),\n+          optflag(\"\", \"newrt\", \"run tests on the new runtime / scheduler\"),\n+          optopt(\"\", \"target\", \"the target to build for\", \"TARGET\"),\n+          optopt(\"\", \"adb-path\", \"path to the android debugger\", \"PATH\"),\n+          optopt(\"\", \"adb-test-dir\", \"path to tests for the android debugger\", \"PATH\"),\n+          optflag(\"h\", \"help\", \"show this message\"),\n          ];\n \n     assert!(!args.is_empty());\n+    let argv0 = copy args[0];\n     let args_ = args.tail();\n+    if args[1] == ~\"-h\" || args[1] == ~\"--help\" {\n+        let message = fmt!(\"Usage: %s [OPTIONS] [TESTNAME...]\", argv0);\n+        println(getopts::groups::usage(message, groups));\n+        fail!()\n+    }\n+\n     let matches =\n-        &match getopts::getopts(args_, opts) {\n+        &match getopts::groups::getopts(args_, groups) {\n           Ok(m) => m,\n           Err(f) => fail!(getopts::fail_str(f))\n         };\n \n+    if getopts::opt_present(matches, \"h\") || getopts::opt_present(matches, \"help\") {\n+        let message = fmt!(\"Usage: %s [OPTIONS]  [TESTNAME...]\", argv0);\n+        println(getopts::groups::usage(message, groups));\n+        fail!()\n+    }\n+\n     fn opt_path(m: &getopts::Matches, nm: &str) -> Path {\n         Path(getopts::opt_str(m, nm))\n     }\n@@ -78,6 +103,8 @@ pub fn parse_config(args: ~[~str]) -> config {\n         compile_lib_path: getopts::opt_str(matches, \"compile-lib-path\"),\n         run_lib_path: getopts::opt_str(matches, \"run-lib-path\"),\n         rustc_path: opt_path(matches, \"rustc-path\"),\n+        clang_path: getopts::opt_maybe_str(matches, \"clang-path\").map(|s| Path(*s)),\n+        llvm_bin_path: getopts::opt_maybe_str(matches, \"llvm-bin-path\").map(|s| Path(*s)),\n         src_base: opt_path(matches, \"src-base\"),\n         build_base: opt_path(matches, \"build-base\"),\n         aux_base: opt_path(matches, \"aux-base\"),\n@@ -159,6 +186,7 @@ pub fn str_mode(s: ~str) -> mode {\n       ~\"run-pass\" => mode_run_pass,\n       ~\"pretty\" => mode_pretty,\n       ~\"debug-info\" => mode_debug_info,\n+      ~\"codegen\" => mode_codegen,\n       _ => fail!(\"invalid mode\")\n     }\n }\n@@ -170,6 +198,7 @@ pub fn mode_str(mode: mode) -> ~str {\n       mode_run_pass => ~\"run-pass\",\n       mode_pretty => ~\"pretty\",\n       mode_debug_info => ~\"debug-info\",\n+      mode_codegen => ~\"codegen\",\n     }\n }\n \n@@ -187,8 +216,9 @@ pub fn test_opts(config: &config) -> test::TestOpts {\n         logfile: copy config.logfile,\n         run_tests: true,\n         run_benchmarks: false,\n-        save_results: None,\n-        compare_results: None\n+        ratchet_metrics: None,\n+        ratchet_noise_percent: None,\n+        save_metrics: None,\n     }\n }\n "}, {"sha": "dee07c6de495df1345a7193eaab60044795c26f9", "filename": "src/compiletest/runtest.rs", "status": "modified", "additions": 117, "deletions": 1, "changes": 118, "blob_url": "https://github.com/rust-lang/rust/blob/4478ded57c3a6cd86ad2e6b8cac1f6b4d46d1130/src%2Fcompiletest%2Fruntest.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4478ded57c3a6cd86ad2e6b8cac1f6b4d46d1130/src%2Fcompiletest%2Fruntest.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Fcompiletest%2Fruntest.rs?ref=4478ded57c3a6cd86ad2e6b8cac1f6b4d46d1130", "patch": "@@ -39,7 +39,8 @@ pub fn run(config: config, testfile: ~str) {\n       mode_run_fail => run_rfail_test(&config, &props, &testfile),\n       mode_run_pass => run_rpass_test(&config, &props, &testfile),\n       mode_pretty => run_pretty_test(&config, &props, &testfile),\n-      mode_debug_info => run_debuginfo_test(&config, &props, &testfile)\n+      mode_debug_info => run_debuginfo_test(&config, &props, &testfile),\n+      mode_codegen => run_codegen_test(&config, &props, &testfile)\n     }\n }\n \n@@ -835,3 +836,118 @@ fn _arm_push_aux_shared_library(config: &config, testfile: &Path) {\n         }\n     }\n }\n+\n+// codegen tests (vs. clang)\n+\n+fn make_o_name(config: &config, testfile: &Path) -> Path {\n+    output_base_name(config, testfile).with_filetype(\"o\")\n+}\n+\n+fn append_suffix_to_stem(p: &Path, suffix: &str) -> Path {\n+    if suffix.len() == 0 {\n+        copy *p\n+    } else {\n+        let stem = p.filestem().get();\n+        p.with_filestem(stem + \"-\" + suffix)\n+    }\n+}\n+\n+fn compile_test_and_save_bitcode(config: &config, props: &TestProps,\n+                                 testfile: &Path) -> ProcRes {\n+    let link_args = ~[~\"-L\", aux_output_dir_name(config, testfile).to_str()];\n+    let llvm_args = ~[~\"-c\", ~\"--lib\", ~\"--save-temps\"];\n+    let args = make_compile_args(config, props,\n+                                 link_args + llvm_args,\n+                                 make_o_name, testfile);\n+    compose_and_run_compiler(config, props, testfile, args, None)\n+}\n+\n+fn compile_cc_with_clang_and_save_bitcode(config: &config, _props: &TestProps,\n+                                          testfile: &Path) -> ProcRes {\n+    let bitcodefile = output_base_name(config, testfile).with_filetype(\"bc\");\n+    let bitcodefile = append_suffix_to_stem(&bitcodefile, \"clang\");\n+    let ProcArgs = ProcArgs {\n+        prog: config.clang_path.get_ref().to_str(),\n+        args: ~[~\"-c\",\n+                ~\"-emit-llvm\",\n+                ~\"-o\", bitcodefile.to_str(),\n+                testfile.with_filetype(\"cc\").to_str() ]\n+    };\n+    compose_and_run(config, testfile, ProcArgs, ~[], \"\", None)\n+}\n+\n+fn extract_function_from_bitcode(config: &config, _props: &TestProps,\n+                                 fname: &str, testfile: &Path,\n+                                 suffix: &str) -> ProcRes {\n+    let bitcodefile = output_base_name(config, testfile).with_filetype(\"bc\");\n+    let bitcodefile = append_suffix_to_stem(&bitcodefile, suffix);\n+    let extracted_bc = append_suffix_to_stem(&bitcodefile, \"extract\");\n+    let ProcArgs = ProcArgs {\n+        prog: config.llvm_bin_path.get_ref().push(\"llvm-extract\").to_str(),\n+        args: ~[~\"-func=\" + fname,\n+                ~\"-o=\" + extracted_bc.to_str(),\n+                bitcodefile.to_str() ]\n+    };\n+    compose_and_run(config, testfile, ProcArgs, ~[], \"\", None)\n+}\n+\n+fn disassemble_extract(config: &config, _props: &TestProps,\n+                       testfile: &Path, suffix: &str) -> ProcRes {\n+    let bitcodefile = output_base_name(config, testfile).with_filetype(\"bc\");\n+    let bitcodefile = append_suffix_to_stem(&bitcodefile, suffix);\n+    let extracted_bc = append_suffix_to_stem(&bitcodefile, \"extract\");\n+    let extracted_ll = extracted_bc.with_filetype(\"ll\");\n+    let ProcArgs = ProcArgs {\n+        prog: config.llvm_bin_path.get_ref().push(\"llvm-dis\").to_str(),\n+        args: ~[~\"-o=\" + extracted_ll.to_str(),\n+                extracted_bc.to_str() ]\n+    };\n+    compose_and_run(config, testfile, ProcArgs, ~[], \"\", None)\n+}\n+\n+\n+fn run_codegen_test(config: &config, props: &TestProps, testfile: &Path) {\n+\n+    if config.llvm_bin_path.is_none() {\n+        fatal(~\"missing --llvm-bin-path\");\n+    }\n+\n+    if config.clang_path.is_none() {\n+        fatal(~\"missing --clang-path\");\n+    }\n+\n+    let mut ProcRes = compile_test_and_save_bitcode(config, props, testfile);\n+    if ProcRes.status != 0 {\n+        fatal_ProcRes(~\"compilation failed!\", &ProcRes);\n+    }\n+\n+    ProcRes = extract_function_from_bitcode(config, props, \"test\", testfile, \"\");\n+    if ProcRes.status != 0 {\n+        fatal_ProcRes(~\"extracting 'test' function failed\", &ProcRes);\n+    }\n+\n+    ProcRes = disassemble_extract(config, props, testfile, \"\");\n+    if ProcRes.status != 0 {\n+        fatal_ProcRes(~\"disassembling extract failed\", &ProcRes);\n+    }\n+\n+\n+    let mut ProcRes = compile_cc_with_clang_and_save_bitcode(config, props, testfile);\n+    if ProcRes.status != 0 {\n+        fatal_ProcRes(~\"compilation failed!\", &ProcRes);\n+    }\n+\n+    ProcRes = extract_function_from_bitcode(config, props, \"test\", testfile, \"clang\");\n+    if ProcRes.status != 0 {\n+        fatal_ProcRes(~\"extracting 'test' function failed\", &ProcRes);\n+    }\n+\n+    ProcRes = disassemble_extract(config, props, testfile, \"clang\");\n+    if ProcRes.status != 0 {\n+        fatal_ProcRes(~\"disassembling extract failed\", &ProcRes);\n+    }\n+\n+\n+\n+}\n+"}, {"sha": "2f17e4a741761806d84d37d824b3e7c810ef9390", "filename": "src/libextra/json.rs", "status": "modified", "additions": 13, "deletions": 2, "changes": 15, "blob_url": "https://github.com/rust-lang/rust/blob/4478ded57c3a6cd86ad2e6b8cac1f6b4d46d1130/src%2Flibextra%2Fjson.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4478ded57c3a6cd86ad2e6b8cac1f6b4d46d1130/src%2Flibextra%2Fjson.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibextra%2Fjson.rs?ref=4478ded57c3a6cd86ad2e6b8cac1f6b4d46d1130", "patch": "@@ -27,6 +27,7 @@ use std::to_str;\n use serialize::Encodable;\n use serialize;\n use sort::Sort;\n+use treemap::TreeMap;\n \n /// Represents a json value\n pub enum Json {\n@@ -1225,7 +1226,7 @@ impl Ord for Json {\n }\n \n /// A trait for converting values to JSON\n-trait ToJson {\n+pub trait ToJson {\n     /// Converts the value of `self` to an instance of JSON\n     fn to_json(&self) -> Json;\n }\n@@ -1330,7 +1331,17 @@ impl<A:ToJson> ToJson for ~[A] {\n     fn to_json(&self) -> Json { List(self.map(|elt| elt.to_json())) }\n }\n \n-impl<A:ToJson + Copy> ToJson for HashMap<~str, A> {\n+impl<A:ToJson> ToJson for HashMap<~str, A> {\n+    fn to_json(&self) -> Json {\n+        let mut d = HashMap::new();\n+        for self.iter().advance |(key, value)| {\n+            d.insert(copy *key, value.to_json());\n+        }\n+        Object(~d)\n+    }\n+}\n+\n+impl<A:ToJson> ToJson for TreeMap<~str, A> {\n     fn to_json(&self) -> Json {\n         let mut d = HashMap::new();\n         for self.iter().advance |(key, value)| {"}, {"sha": "b6a2deb1663310e31056061a962def35d6f52531", "filename": "src/libextra/stats.rs", "status": "modified", "additions": 4, "deletions": 1, "changes": 5, "blob_url": "https://github.com/rust-lang/rust/blob/4478ded57c3a6cd86ad2e6b8cac1f6b4d46d1130/src%2Flibextra%2Fstats.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4478ded57c3a6cd86ad2e6b8cac1f6b4d46d1130/src%2Flibextra%2Fstats.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibextra%2Fstats.rs?ref=4478ded57c3a6cd86ad2e6b8cac1f6b4d46d1130", "patch": "@@ -100,6 +100,7 @@ pub trait Stats {\n }\n \n /// Extracted collection of all the summary statistics of a sample set.\n+#[deriving(Eq)]\n struct Summary {\n     sum: f64,\n     min: f64,\n@@ -116,7 +117,9 @@ struct Summary {\n }\n \n impl Summary {\n-    fn new(samples: &[f64]) -> Summary {\n+\n+    /// Construct a new summary of a sample set.\n+    pub fn new(samples: &[f64]) -> Summary {\n         Summary {\n             sum: samples.sum(),\n             min: samples.min(),"}, {"sha": "96ca429676850ff47556b18cbaad0a8a8d1b3459", "filename": "src/libextra/test.rs", "status": "modified", "additions": 491, "deletions": 251, "changes": 742, "blob_url": "https://github.com/rust-lang/rust/blob/4478ded57c3a6cd86ad2e6b8cac1f6b4d46d1130/src%2Flibextra%2Ftest.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4478ded57c3a6cd86ad2e6b8cac1f6b4d46d1130/src%2Flibextra%2Ftest.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibextra%2Ftest.rs?ref=4478ded57c3a6cd86ad2e6b8cac1f6b4d46d1130", "patch": "@@ -17,24 +17,26 @@\n \n \n use getopts;\n+use json::ToJson;\n+use json;\n+use serialize::Decodable;\n use sort;\n use stats::Stats;\n+use stats;\n use term;\n use time::precise_time_ns;\n+use treemap::TreeMap;\n \n use std::comm::{stream, SharedChan};\n use std::either;\n use std::io;\n-use std::num;\n-use std::option;\n-use std::rand::RngUtil;\n-use std::rand;\n use std::result;\n use std::task;\n use std::to_str::ToStr;\n use std::u64;\n-use std::uint;\n-use std::vec;\n+use std::f64;\n+use std::hashmap::HashMap;\n+use std::os;\n \n \n // The name of a test. By convention this follows the rules for rust\n@@ -87,6 +89,25 @@ pub struct TestDescAndFn {\n     testfn: TestFn,\n }\n \n+#[deriving(Encodable,Decodable,Eq)]\n+pub struct Metric {\n+    value: f64,\n+    noise: f64\n+}\n+\n+pub struct MetricMap(TreeMap<~str,Metric>);\n+\n+/// Analysis of a single change in metric\n+pub enum MetricChange {\n+    LikelyNoise,\n+    MetricAdded,\n+    MetricRemoved,\n+    Improvement(f64),\n+    Regression(f64)\n+}\n+\n+pub type MetricDiff = TreeMap<~str,MetricChange>;\n+\n // The default console test runner. It accepts the command line\n // arguments and a vector of test_descs.\n pub fn test_main(args: &[~str], tests: ~[TestDescAndFn]) {\n@@ -127,8 +148,9 @@ pub struct TestOpts {\n     run_ignored: bool,\n     run_tests: bool,\n     run_benchmarks: bool,\n-    save_results: Option<Path>,\n-    compare_results: Option<Path>,\n+    ratchet_metrics: Option<Path>,\n+    ratchet_noise_percent: Option<f64>,\n+    save_metrics: Option<Path>,\n     logfile: Option<Path>\n }\n \n@@ -140,8 +162,9 @@ pub fn parse_opts(args: &[~str]) -> OptRes {\n     let opts = ~[getopts::optflag(\"ignored\"),\n                  getopts::optflag(\"test\"),\n                  getopts::optflag(\"bench\"),\n-                 getopts::optopt(\"save\"),\n-                 getopts::optopt(\"diff\"),\n+                 getopts::optopt(\"save-metrics\"),\n+                 getopts::optopt(\"ratchet-metrics\"),\n+                 getopts::optopt(\"ratchet-noise-percent\"),\n                  getopts::optopt(\"logfile\")];\n     let matches =\n         match getopts::getopts(args_, opts) {\n@@ -151,8 +174,8 @@ pub fn parse_opts(args: &[~str]) -> OptRes {\n \n     let filter =\n         if matches.free.len() > 0 {\n-            option::Some(copy (matches).free[0])\n-        } else { option::None };\n+            Some(copy (matches).free[0])\n+        } else { None };\n \n     let run_ignored = getopts::opt_present(&matches, \"ignored\");\n \n@@ -163,19 +186,24 @@ pub fn parse_opts(args: &[~str]) -> OptRes {\n     let run_tests = ! run_benchmarks ||\n         getopts::opt_present(&matches, \"test\");\n \n-    let save_results = getopts::opt_maybe_str(&matches, \"save\");\n-    let save_results = save_results.map(|s| Path(*s));\n+    let ratchet_metrics = getopts::opt_maybe_str(&matches, \"ratchet-metrics\");\n+    let ratchet_metrics = ratchet_metrics.map(|s| Path(*s));\n+\n+    let ratchet_noise_percent =\n+        getopts::opt_maybe_str(&matches, \"ratchet-noise-percent\");\n+    let ratchet_noise_percent = ratchet_noise_percent.map(|s| f64::from_str(*s).get());\n \n-    let compare_results = getopts::opt_maybe_str(&matches, \"diff\");\n-    let compare_results = compare_results.map(|s| Path(*s));\n+    let save_metrics = getopts::opt_maybe_str(&matches, \"save-metrics\");\n+    let save_metrics = save_metrics.map(|s| Path(*s));\n \n     let test_opts = TestOpts {\n         filter: filter,\n         run_ignored: run_ignored,\n         run_tests: run_tests,\n         run_benchmarks: run_benchmarks,\n-        save_results: save_results,\n-        compare_results: compare_results,\n+        ratchet_metrics: ratchet_metrics,\n+        ratchet_noise_percent: ratchet_noise_percent,\n+        save_metrics: save_metrics,\n         logfile: logfile\n     };\n \n@@ -184,7 +212,7 @@ pub fn parse_opts(args: &[~str]) -> OptRes {\n \n #[deriving(Eq)]\n pub struct BenchSamples {\n-    ns_iter_samples: ~[f64],\n+    ns_iter_summ: stats::Summary,\n     mb_s: uint\n }\n \n@@ -194,181 +222,288 @@ pub enum TestResult { TrOk, TrFailed, TrIgnored, TrBench(BenchSamples) }\n struct ConsoleTestState {\n     out: @io::Writer,\n     log_out: Option<@io::Writer>,\n+    term: Option<term::Terminal>,\n     use_color: bool,\n     total: uint,\n     passed: uint,\n     failed: uint,\n     ignored: uint,\n     benchmarked: uint,\n+    metrics: MetricMap,\n     failures: ~[TestDesc]\n }\n \n-// A simple console test runner\n-pub fn run_tests_console(opts: &TestOpts,\n-                         tests: ~[TestDescAndFn]) -> bool {\n-    fn callback(event: &TestEvent, st: &mut ConsoleTestState) {\n-        debug!(\"callback(event=%?)\", event);\n-        match copy *event {\n-          TeFiltered(ref filtered_tests) => {\n-            st.total = filtered_tests.len();\n-            let noun = if st.total != 1 { ~\"tests\" } else { ~\"test\" };\n-            st.out.write_line(fmt!(\"\\nrunning %u %s\", st.total, noun));\n-          }\n-          TeWait(ref test) => st.out.write_str(\n-              fmt!(\"test %s ... \", test.name.to_str())),\n-          TeResult(test, result) => {\n-            match st.log_out {\n-                Some(f) => write_log(f, copy result, &test),\n-                None => ()\n-            }\n-            match result {\n-              TrOk => {\n-                st.passed += 1;\n-                write_ok(st.out, st.use_color);\n-                st.out.write_line(\"\");\n-              }\n-              TrFailed => {\n-                st.failed += 1;\n-                write_failed(st.out, st.use_color);\n-                st.out.write_line(\"\");\n-                st.failures.push(test);\n-              }\n-              TrIgnored => {\n-                st.ignored += 1;\n-                write_ignored(st.out, st.use_color);\n-                st.out.write_line(\"\");\n-              }\n-              TrBench(bs) => {\n-                st.benchmarked += 1u;\n-                write_bench(st.out, st.use_color);\n-                st.out.write_line(fmt!(\": %s\",\n-                                       fmt_bench_samples(&bs)));\n-              }\n-            }\n-          }\n+impl ConsoleTestState {\n+    pub fn new(opts: &TestOpts) -> ConsoleTestState {\n+        let log_out = match opts.logfile {\n+            Some(ref path) => match io::file_writer(path,\n+                                                    [io::Create,\n+                                                     io::Truncate]) {\n+                result::Ok(w) => Some(w),\n+                result::Err(ref s) => {\n+                    fail!(\"can't open output file: %s\", *s)\n+                }\n+            },\n+            None => None\n+        };\n+        let out = io::stdout();\n+        let term = match term::Terminal::new(out) {\n+            Err(_) => None,\n+            Ok(t) => Some(t)\n+        };\n+        ConsoleTestState {\n+            out: out,\n+            log_out: log_out,\n+            use_color: use_color(),\n+            term: term,\n+            total: 0u,\n+            passed: 0u,\n+            failed: 0u,\n+            ignored: 0u,\n+            benchmarked: 0u,\n+            metrics: MetricMap::new(),\n+            failures: ~[]\n         }\n     }\n \n-    let log_out = match opts.logfile {\n-        Some(ref path) => match io::file_writer(path,\n-                                                [io::Create,\n-                                                 io::Truncate]) {\n-          result::Ok(w) => Some(w),\n-          result::Err(ref s) => {\n-              fail!(\"can't open output file: %s\", *s)\n-          }\n-        },\n-        None => None\n-    };\n+    pub fn write_ok(&self) {\n+        self.write_pretty(\"ok\", term::color::GREEN);\n+    }\n \n-    let st = @mut ConsoleTestState {\n-        out: io::stdout(),\n-        log_out: log_out,\n-        use_color: use_color(),\n-        total: 0u,\n-        passed: 0u,\n-        failed: 0u,\n-        ignored: 0u,\n-        benchmarked: 0u,\n-        failures: ~[]\n-    };\n+    pub fn write_failed(&self) {\n+        self.write_pretty(\"FAILED\", term::color::RED);\n+    }\n \n-    run_tests(opts, tests, |x| callback(&x, st));\n+    pub fn write_ignored(&self) {\n+        self.write_pretty(\"ignored\", term::color::YELLOW);\n+    }\n+\n+    pub fn write_bench(&self) {\n+        self.write_pretty(\"bench\", term::color::CYAN);\n+    }\n \n-    assert!(st.passed + st.failed +\n-                 st.ignored + st.benchmarked == st.total);\n-    let success = st.failed == 0u;\n \n-    if !success {\n-        print_failures(st);\n+    pub fn write_added(&self) {\n+        self.write_pretty(\"added\", term::color::GREEN);\n     }\n \n-    {\n-      let st: &mut ConsoleTestState = st;\n-      st.out.write_str(fmt!(\"\\nresult: \"));\n-      if success {\n-          // There's no parallelism at this point so it's safe to use color\n-          write_ok(st.out, true);\n-      } else {\n-          write_failed(st.out, true);\n-      }\n-      st.out.write_str(fmt!(\". %u passed; %u failed; %u ignored\\n\\n\",\n-                            st.passed, st.failed, st.ignored));\n+    pub fn write_improved(&self) {\n+        self.write_pretty(\"improved\", term::color::GREEN);\n     }\n \n-    return success;\n+    pub fn write_removed(&self) {\n+        self.write_pretty(\"removed\", term::color::YELLOW);\n+    }\n \n-    fn fmt_bench_samples(bs: &BenchSamples) -> ~str {\n-        use stats::Stats;\n-        if bs.mb_s != 0 {\n-            fmt!(\"%u ns/iter (+/- %u) = %u MB/s\",\n-                 bs.ns_iter_samples.median() as uint,\n-                 3 * (bs.ns_iter_samples.median_abs_dev() as uint),\n-                 bs.mb_s)\n-        } else {\n-            fmt!(\"%u ns/iter (+/- %u)\",\n-                 bs.ns_iter_samples.median() as uint,\n-                 3 * (bs.ns_iter_samples.median_abs_dev() as uint))\n+    pub fn write_regressed(&self) {\n+        self.write_pretty(\"regressed\", term::color::RED);\n+    }\n+\n+    pub fn write_pretty(&self,\n+                        word: &str,\n+                        color: term::color::Color) {\n+        match self.term {\n+            None => self.out.write_str(word),\n+            Some(ref t) => {\n+                if self.use_color {\n+                    t.fg(color);\n+                }\n+                self.out.write_str(word);\n+                if self.use_color {\n+                    t.reset();\n+                }\n+            }\n         }\n     }\n \n-    fn write_log(out: @io::Writer, result: TestResult, test: &TestDesc) {\n-        out.write_line(fmt!(\"%s %s\",\n-                    match result {\n-                        TrOk => ~\"ok\",\n-                        TrFailed => ~\"failed\",\n-                        TrIgnored => ~\"ignored\",\n-                        TrBench(ref bs) => fmt_bench_samples(bs)\n-                    }, test.name.to_str()));\n+    pub fn write_run_start(&mut self, len: uint) {\n+        self.total = len;\n+        let noun = if len != 1 { &\"tests\" } else { &\"test\" };\n+        self.out.write_line(fmt!(\"\\nrunning %u %s\", len, noun));\n     }\n \n-    fn write_ok(out: @io::Writer, use_color: bool) {\n-        write_pretty(out, \"ok\", term::color::GREEN, use_color);\n+    pub fn write_test_start(&self, test: &TestDesc) {\n+        self.out.write_str(fmt!(\"test %s ... \", test.name.to_str()));\n     }\n \n-    fn write_failed(out: @io::Writer, use_color: bool) {\n-        write_pretty(out, \"FAILED\", term::color::RED, use_color);\n+    pub fn write_result(&self, result: &TestResult) {\n+        match *result {\n+            TrOk => self.write_ok(),\n+            TrFailed => self.write_failed(),\n+            TrIgnored => self.write_ignored(),\n+            TrBench(ref bs) => {\n+                self.write_bench();\n+                self.out.write_str(\": \" + fmt_bench_samples(bs))\n+            }\n+        }\n+        self.out.write_str(&\"\\n\");\n     }\n \n-    fn write_ignored(out: @io::Writer, use_color: bool) {\n-        write_pretty(out, \"ignored\", term::color::YELLOW, use_color);\n+    pub fn write_log(&self, test: &TestDesc, result: &TestResult) {\n+        match self.log_out {\n+            None => (),\n+            Some(out) => {\n+                out.write_line(fmt!(\"%s %s\",\n+                                    match *result {\n+                                        TrOk => ~\"ok\",\n+                                        TrFailed => ~\"failed\",\n+                                        TrIgnored => ~\"ignored\",\n+                                        TrBench(ref bs) => fmt_bench_samples(bs)\n+                                    }, test.name.to_str()));\n+            }\n+        }\n     }\n \n-    fn write_bench(out: @io::Writer, use_color: bool) {\n-        write_pretty(out, \"bench\", term::color::CYAN, use_color);\n+    pub fn write_failures(&self) {\n+        self.out.write_line(\"\\nfailures:\");\n+        let mut failures = ~[];\n+        for self.failures.iter().advance() |f| {\n+            failures.push(f.name.to_str());\n+        }\n+        sort::tim_sort(failures);\n+        for failures.iter().advance |name| {\n+            self.out.write_line(fmt!(\"    %s\", name.to_str()));\n+        }\n     }\n \n-    fn write_pretty(out: @io::Writer,\n-                    word: &str,\n-                    color: term::color::Color,\n-                    use_color: bool) {\n-        let t = term::Terminal::new(out);\n-        match t {\n-            Ok(term)  => {\n-                if use_color {\n-                    term.fg(color);\n+    pub fn write_metric_diff(&self, diff: &MetricDiff) {\n+        let mut noise = 0;\n+        let mut improved = 0;\n+        let mut regressed = 0;\n+        let mut added = 0;\n+        let mut removed = 0;\n+\n+        for diff.iter().advance() |(k, v)| {\n+            match *v {\n+                LikelyNoise => noise += 1,\n+                MetricAdded => {\n+                    added += 1;\n+                    self.write_added();\n+                    self.out.write_line(fmt!(\": %s\", *k));\n                 }\n-                out.write_str(word);\n-                if use_color {\n-                    term.reset();\n+                MetricRemoved => {\n+                    removed += 1;\n+                    self.write_removed();\n+                    self.out.write_line(fmt!(\": %s\", *k));\n                 }\n-            },\n-            Err(_) => out.write_str(word)\n+                Improvement(pct) => {\n+                    improved += 1;\n+                    self.out.write_str(*k);\n+                    self.out.write_str(\": \");\n+                    self.write_improved();\n+                    self.out.write_line(fmt!(\" by %.2f%%\", pct as float))\n+                }\n+                Regression(pct) => {\n+                    regressed += 1;\n+                    self.out.write_str(*k);\n+                    self.out.write_str(\": \");\n+                    self.write_regressed();\n+                    self.out.write_line(fmt!(\" by %.2f%%\", pct as float))\n+                }\n+            }\n+        }\n+        self.out.write_line(fmt!(\"result of ratchet: %u matrics added, %u removed, \\\n+                                  %u improved, %u regressed, %u noise\",\n+                                 added, removed, improved, regressed, noise));\n+        if regressed == 0 {\n+            self.out.write_line(\"updated ratchet file\")\n+        } else {\n+            self.out.write_line(\"left ratchet file untouched\")\n+        }\n+    }\n+\n+    pub fn write_run_finish(&self,\n+                            ratchet_metrics: &Option<Path>,\n+                            ratchet_pct: Option<f64>) -> bool {\n+        assert!(self.passed + self.failed + self.ignored + self.benchmarked == self.total);\n+\n+        let ratchet_success = match *ratchet_metrics {\n+            None => true,\n+            Some(ref pth) => {\n+                self.out.write_str(fmt!(\"\\nusing metrics ratchet: %s\\n\", pth.to_str()));\n+                match ratchet_pct {\n+                    None => (),\n+                    Some(pct) =>\n+                    self.out.write_str(fmt!(\"with noise-tolerance forced to: %f%%\\n\",\n+                                            pct as float))\n+                }\n+                let (diff, ok) = self.metrics.ratchet(pth, ratchet_pct);\n+                self.write_metric_diff(&diff);\n+                ok\n+            }\n+        };\n+\n+        let test_success = self.failed == 0u;\n+        if !test_success {\n+            self.write_failures();\n         }\n+\n+        let success = ratchet_success && test_success;\n+\n+        self.out.write_str(\"\\ntest result: \");\n+        if success {\n+            // There's no parallelism at this point so it's safe to use color\n+            self.write_ok();\n+        } else {\n+            self.write_failed();\n+        }\n+        self.out.write_str(fmt!(\". %u passed; %u failed; %u ignored, %u benchmarked\\n\\n\",\n+                                self.passed, self.failed, self.ignored, self.benchmarked));\n+        return success;\n+    }\n+}\n+\n+pub fn fmt_bench_samples(bs: &BenchSamples) -> ~str {\n+    if bs.mb_s != 0 {\n+        fmt!(\"%u ns/iter (+/- %u) = %u MB/s\",\n+             bs.ns_iter_summ.median as uint,\n+             (bs.ns_iter_summ.max - bs.ns_iter_summ.min) as uint,\n+             bs.mb_s)\n+    } else {\n+        fmt!(\"%u ns/iter (+/- %u)\",\n+             bs.ns_iter_summ.median as uint,\n+             (bs.ns_iter_summ.max - bs.ns_iter_summ.min) as uint)\n     }\n }\n \n-fn print_failures(st: &ConsoleTestState) {\n-    st.out.write_line(\"\\nfailures:\");\n-    let mut failures = ~[];\n-    for uint::range(0, st.failures.len()) |i| {\n-        let name = copy st.failures[i].name;\n-        failures.push(name.to_str());\n+// A simple console test runner\n+pub fn run_tests_console(opts: &TestOpts,\n+                         tests: ~[TestDescAndFn]) -> bool {\n+    fn callback(event: &TestEvent, st: &mut ConsoleTestState) {\n+        debug!(\"callback(event=%?)\", event);\n+        match copy *event {\n+            TeFiltered(ref filtered_tests) => st.write_run_start(filtered_tests.len()),\n+            TeWait(ref test) => st.write_test_start(test),\n+            TeResult(test, result) => {\n+                st.write_log(&test, &result);\n+                st.write_result(&result);\n+                match result {\n+                    TrOk => st.passed += 1,\n+                    TrIgnored => st.ignored += 1,\n+                    TrBench(bs) => {\n+                        st.metrics.insert_metric(test.name.to_str(),\n+                                                 bs.ns_iter_summ.median,\n+                                                 bs.ns_iter_summ.max - bs.ns_iter_summ.min);\n+                        st.benchmarked += 1\n+                    }\n+                    TrFailed => {\n+                        st.failed += 1;\n+                        st.failures.push(test);\n+                    }\n+                }\n+            }\n+        }\n     }\n-    sort::tim_sort(failures);\n-    for failures.iter().advance |name| {\n-        st.out.write_line(fmt!(\"    %s\", name.to_str()));\n+    let st = @mut ConsoleTestState::new(opts);\n+    run_tests(opts, tests, |x| callback(&x, st));\n+    match opts.save_metrics {\n+        None => (),\n+        Some(ref pth) => {\n+            st.metrics.save(pth);\n+            st.out.write_str(fmt!(\"\\nmetrics saved to: %s\", pth.to_str()));\n+        }\n     }\n+    return st.write_run_finish(&opts.ratchet_metrics, opts.ratchet_noise_percent);\n }\n \n #[test]\n@@ -390,17 +525,19 @@ fn should_sort_failures_before_printing_them() {\n \n         let st = @ConsoleTestState {\n             out: wr,\n-            log_out: option::None,\n+            log_out: None,\n+            term: None,\n             use_color: false,\n             total: 0u,\n             passed: 0u,\n             failed: 0u,\n             ignored: 0u,\n             benchmarked: 0u,\n+            metrics: MetricMap::new(),\n             failures: ~[test_b, test_a]\n         };\n \n-        print_failures(st);\n+        st.write_failures();\n     };\n \n     let apos = s.find_str(\"a\").get();\n@@ -503,15 +640,17 @@ pub fn filter_tests(\n         filtered\n     } else {\n         let filter_str = match opts.filter {\n-          option::Some(ref f) => copy *f,\n-          option::None => ~\"\"\n+          Some(ref f) => copy *f,\n+          None => ~\"\"\n         };\n \n         fn filter_fn(test: TestDescAndFn, filter_str: &str) ->\n             Option<TestDescAndFn> {\n             if test.desc.name.to_str().contains(filter_str) {\n-                return option::Some(test);\n-            } else { return option::None; }\n+                return Some(test);\n+            } else {\n+                return None;\n+            }\n         }\n \n         filtered.consume_iter().filter_map(|x| filter_fn(x, filter_str)).collect()\n@@ -605,6 +744,138 @@ fn calc_result(desc: &TestDesc, task_succeeded: bool) -> TestResult {\n     }\n }\n \n+\n+impl ToJson for Metric {\n+    fn to_json(&self) -> json::Json {\n+        let mut map = ~HashMap::new();\n+        map.insert(~\"value\", json::Number(self.value as float));\n+        map.insert(~\"noise\", json::Number(self.noise as float));\n+        json::Object(map)\n+    }\n+}\n+\n+impl MetricMap {\n+\n+    fn new() -> MetricMap {\n+        MetricMap(TreeMap::new())\n+    }\n+\n+    /// Load MetricDiff from a file.\n+    fn load(p: &Path) -> MetricMap {\n+        assert!(os::path_exists(p));\n+        let f = io::file_reader(p).get();\n+        let mut decoder = json::Decoder(json::from_reader(f).get());\n+        MetricMap(Decodable::decode(&mut decoder))\n+    }\n+\n+    /// Write MetricDiff to a file.\n+    pub fn save(&self, p: &Path) {\n+        let f = io::file_writer(p, [io::Create, io::Truncate]).get();\n+        json::to_pretty_writer(f, &self.to_json());\n+    }\n+\n+    /// Compare against another MetricMap\n+    pub fn compare_to_old(&self, old: MetricMap,\n+                          noise_pct: Option<f64>) -> MetricDiff {\n+        let mut diff : MetricDiff = TreeMap::new();\n+        for old.iter().advance |(k, vold)| {\n+            let r = match self.find(k) {\n+                None => MetricRemoved,\n+                Some(v) => {\n+                    let delta = (v.value - vold.value);\n+                    let noise = match noise_pct {\n+                        None => f64::max(vold.noise.abs(), v.noise.abs()),\n+                        Some(pct) => vold.value * pct / 100.0\n+                    };\n+                    if delta.abs() < noise {\n+                        LikelyNoise\n+                    } else {\n+                        let pct = delta.abs() / v.value * 100.0;\n+                        if vold.noise < 0.0 {\n+                            // When 'noise' is negative, it means we want\n+                            // to see deltas that go up over time, and can\n+                            // only tolerate slight negative movement.\n+                            if delta < 0.0 {\n+                                Regression(pct)\n+                            } else {\n+                                Improvement(pct)\n+                            }\n+                        } else {\n+                            // When 'noise' is positive, it means we want\n+                            // to see deltas that go down over time, and\n+                            // can only tolerate slight positive movements.\n+                            if delta < 0.0 {\n+                                Improvement(pct)\n+                            } else {\n+                                Regression(pct)\n+                            }\n+                        }\n+                    }\n+                }\n+            };\n+            diff.insert(copy *k, r);\n+        }\n+        for self.iter().advance |(k, _)| {\n+            if !diff.contains_key(k) {\n+                diff.insert(copy *k, MetricAdded);\n+            }\n+        }\n+        diff\n+    }\n+\n+    /// Insert a named `value` (+/- `noise`) metric into the map. The value\n+    /// must be non-negative. The `noise` indicates the uncertainty of the\n+    /// metric, which doubles as the \"noise range\" of acceptable\n+    /// pairwise-regressions on this named value, when comparing from one\n+    /// metric to the next using `compare_to_old`.\n+    ///\n+    /// If `noise` is positive, then it means this metric is of a value\n+    /// you want to see grow smaller, so a change larger than `noise` in the\n+    /// positive direction represents a regression.\n+    ///\n+    /// If `noise` is negative, then it means this metric is of a value\n+    /// you want to see grow larger, so a change larger than `noise` in the\n+    /// negative direction represents a regression.\n+    pub fn insert_metric(&mut self, name: &str, value: f64, noise: f64) {\n+        let m = Metric {\n+            value: value,\n+            noise: noise\n+        };\n+        self.insert(name.to_owned(), m);\n+    }\n+\n+    /// Attempt to \"ratchet\" an external metric file. This involves loading\n+    /// metrics from a metric file (if it exists), comparing against\n+    /// the metrics in `self` using `compare_to_old`, and rewriting the\n+    /// file to contain the metrics in `self` if none of the\n+    /// `MetricChange`s are `Regression`. Returns the diff as well\n+    /// as a boolean indicating whether the ratchet succeeded.\n+    pub fn ratchet(&self, p: &Path, pct: Option<f64>) -> (MetricDiff, bool) {\n+        let old = if os::path_exists(p) {\n+            MetricMap::load(p)\n+        } else {\n+            MetricMap::new()\n+        };\n+\n+        let diff : MetricDiff = self.compare_to_old(old, pct);\n+        let ok = do diff.iter().all() |(_, v)| {\n+            match *v {\n+                Regression(_) => false,\n+                _ => true\n+            }\n+        };\n+\n+        if ok {\n+            debug!(\"rewriting file '%s' with updated metrics\");\n+            self.save(p);\n+        }\n+        return (diff, ok)\n+    }\n+}\n+\n+\n+// Benchmarking\n+\n impl BenchHarness {\n     /// Callback for benchmark functions to run in their body.\n     pub fn iter(&mut self, inner:&fn()) {\n@@ -639,105 +910,72 @@ impl BenchHarness {\n         f(self);\n     }\n \n-    // This is the Go benchmark algorithm. It produces a single\n-    // datapoint and always tries to run for 1s.\n-    pub fn go_bench(&mut self, f: &fn(&mut BenchHarness)) {\n-\n-        // Rounds a number down to the nearest power of 10.\n-        fn round_down_10(n: u64) -> u64 {\n-            let mut n = n;\n-            let mut res = 1;\n-            while n > 10 {\n-                n = n / 10;\n-                res *= 10;\n-            }\n-            res\n-        }\n-\n-        // Rounds x up to a number of the form [1eX, 2eX, 5eX].\n-        fn round_up(n: u64) -> u64 {\n-            let base = round_down_10(n);\n-            if n < (2 * base) {\n-                2 * base\n-            } else if n < (5 * base) {\n-                5 * base\n-            } else {\n-                10 * base\n-            }\n-        }\n+    // This is a more statistics-driven benchmark algorithm\n+    pub fn auto_bench(&mut self, f: &fn(&mut BenchHarness)) -> stats::Summary {\n \n         // Initial bench run to get ballpark figure.\n         let mut n = 1_u64;\n         self.bench_n(n, |x| f(x));\n \n-        while n < 1_000_000_000 &&\n-            self.ns_elapsed() < 1_000_000_000 {\n-            let last = n;\n-\n-            // Try to estimate iter count for 1s falling back to 1bn\n-            // iterations if first run took < 1ns.\n-            if self.ns_per_iter() == 0 {\n-                n = 1_000_000_000;\n-            } else {\n-                n = 1_000_000_000 / self.ns_per_iter();\n-            }\n-\n-            n = u64::max(u64::min(n+n/2, 100*last), last+1);\n-            n = round_up(n);\n-            self.bench_n(n, |x| f(x));\n+        // Try to estimate iter count for 1ms falling back to 1m\n+        // iterations if first run took < 1ns.\n+        if self.ns_per_iter() == 0 {\n+            n = 1_000_000;\n+        } else {\n+            n = 1_000_000 / self.ns_per_iter();\n         }\n-    }\n \n-    // This is a more statistics-driven benchmark algorithm.\n-    // It stops as quickly as 50ms, so long as the statistical\n-    // properties are satisfactory. If those properties are\n-    // not met, it may run as long as the Go algorithm.\n-    pub fn auto_bench(&mut self, f: &fn(&mut BenchHarness)) -> ~[f64] {\n+        let mut total_run = 0;\n+        let samples : &mut [f64] = [0.0_f64, ..50];\n+        loop {\n+            let loop_start = precise_time_ns();\n \n-        let mut rng = rand::rng();\n-        let mut magnitude = 10;\n-        let mut prev_madp = 0.0;\n+            for samples.mut_iter().advance() |p| {\n+                self.bench_n(n as u64, |x| f(x));\n+                *p = self.ns_per_iter() as f64;\n+            };\n \n-        loop {\n-            let n_samples = rng.gen_uint_range(50, 60);\n-            let n_iter = rng.gen_uint_range(magnitude,\n-                                            magnitude * 2);\n+            stats::winsorize(samples, 5.0);\n+            let summ = stats::Summary::new(samples);\n \n-            let samples = do vec::from_fn(n_samples) |_| {\n-                self.bench_n(n_iter as u64, |x| f(x));\n-                self.ns_per_iter() as f64\n+            for samples.mut_iter().advance() |p| {\n+                self.bench_n(5 * n as u64, |x| f(x));\n+                *p = self.ns_per_iter() as f64;\n             };\n \n-            // Eliminate outliers\n-            let med = samples.median();\n-            let mad = samples.median_abs_dev();\n-            let samples = do samples.consume_iter().filter |f| {\n-                num::abs(*f - med) <= 3.0 * mad\n-            }.collect::<~[f64]>();\n-\n-            debug!(\"%u samples, median %f, MAD=%f, %u survived filter\",\n-                   n_samples, med as float, mad as float,\n-                   samples.len());\n-\n-            if samples.len() != 0 {\n-                // If we have _any_ cluster of signal...\n-                let curr_madp = samples.median_abs_dev_pct();\n-                if self.ns_elapsed() > 1_000_000 &&\n-                    (curr_madp < 1.0 ||\n-                     num::abs(curr_madp - prev_madp) < 0.1) {\n-                    return samples;\n-                }\n-                prev_madp = curr_madp;\n+            stats::winsorize(samples, 5.0);\n+            let summ5 = stats::Summary::new(samples);\n \n-                if n_iter > 20_000_000 ||\n-                    self.ns_elapsed() > 20_000_000 {\n-                    return samples;\n-                }\n+            debug!(\"%u samples, median %f, MAD=%f, MADP=%f\",\n+                   samples.len(),\n+                   summ.median as float,\n+                   summ.median_abs_dev as float,\n+                   summ.median_abs_dev_pct as float);\n+\n+            let now = precise_time_ns();\n+            let loop_run = now - loop_start;\n+\n+            // If we've run for 100ms an seem to have converged to a\n+            // stable median.\n+            if loop_run > 100_000_000 &&\n+                summ.median_abs_dev_pct < 1.0 &&\n+                summ.median - summ5.median < summ5.median_abs_dev {\n+                return summ5;\n+            }\n+\n+            total_run += loop_run;\n+            // Longest we ever run for is 10s.\n+            if total_run > 10_000_000_000 {\n+                return summ5;\n             }\n \n-            magnitude *= 2;\n+            n *= 2;\n         }\n     }\n+\n+\n+\n+\n }\n \n pub mod bench {\n@@ -752,13 +990,13 @@ pub mod bench {\n             bytes: 0\n         };\n \n-        let ns_iter_samples = bs.auto_bench(f);\n+        let ns_iter_summ = bs.auto_bench(f);\n \n-        let iter_s = 1_000_000_000 / (ns_iter_samples.median() as u64);\n+        let iter_s = 1_000_000_000 / (ns_iter_summ.median as u64);\n         let mb_s = (bs.bytes * iter_s) / 1_000_000;\n \n         BenchSamples {\n-            ns_iter_samples: ns_iter_samples,\n+            ns_iter_summ: ns_iter_summ,\n             mb_s: mb_s as uint\n         }\n     }\n@@ -877,13 +1115,14 @@ mod tests {\n         // unignored tests and flip the ignore flag on the rest to false\n \n         let opts = TestOpts {\n-            filter: option::None,\n+            filter: None,\n             run_ignored: true,\n-            logfile: option::None,\n+            logfile: None,\n             run_tests: true,\n             run_benchmarks: false,\n-            save_results: option::None,\n-            compare_results: option::None\n+            ratchet_noise_percent: None,\n+            ratchet_metrics: None,\n+            save_metrics: None,\n         };\n \n         let tests = ~[\n@@ -914,13 +1153,14 @@ mod tests {\n     #[test]\n     pub fn sort_tests() {\n         let opts = TestOpts {\n-            filter: option::None,\n+            filter: None,\n             run_ignored: false,\n-            logfile: option::None,\n+            logfile: None,\n             run_tests: true,\n             run_benchmarks: false,\n-            save_results: option::None,\n-            compare_results: option::None\n+            ratchet_noise_percent: None,\n+            ratchet_metrics: None,\n+            save_metrics: None,\n         };\n \n         let names ="}, {"sha": "01eae9b16bb6faf9c1cc84d8362726bcb5949cff", "filename": "src/test/codegen/hello.cc", "status": "added", "additions": 12, "deletions": 0, "changes": 12, "blob_url": "https://github.com/rust-lang/rust/blob/4478ded57c3a6cd86ad2e6b8cac1f6b4d46d1130/src%2Ftest%2Fcodegen%2Fhello.cc", "raw_url": "https://github.com/rust-lang/rust/raw/4478ded57c3a6cd86ad2e6b8cac1f6b4d46d1130/src%2Ftest%2Fcodegen%2Fhello.cc", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ftest%2Fcodegen%2Fhello.cc?ref=4478ded57c3a6cd86ad2e6b8cac1f6b4d46d1130", "patch": "@@ -0,0 +1,12 @@\n+#include <stddef.h>\n+\n+struct slice {\n+  char const *p;\n+  size_t len;\n+};\n+\n+extern \"C\"\n+void test() {\n+  struct slice s = { .p = \"hello\",\n+                     .len = 5 };\n+}"}, {"sha": "e7cd84f63f2b09d4c482cd02e1d2ada81def7401", "filename": "src/test/codegen/hello.rs", "status": "added", "additions": 4, "deletions": 0, "changes": 4, "blob_url": "https://github.com/rust-lang/rust/blob/4478ded57c3a6cd86ad2e6b8cac1f6b4d46d1130/src%2Ftest%2Fcodegen%2Fhello.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4478ded57c3a6cd86ad2e6b8cac1f6b4d46d1130/src%2Ftest%2Fcodegen%2Fhello.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ftest%2Fcodegen%2Fhello.rs?ref=4478ded57c3a6cd86ad2e6b8cac1f6b4d46d1130", "patch": "@@ -0,0 +1,4 @@\n+#[no_mangle]\n+fn test() {\n+    let _x = \"hello\";\n+}"}]}