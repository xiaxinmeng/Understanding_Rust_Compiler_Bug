{"sha": "71d4a860a1797300f976b45d7b7a41815eca6207", "node_id": "MDY6Q29tbWl0NzI0NzEyOjcxZDRhODYwYTE3OTczMDBmOTc2YjQ1ZDdiN2E0MTgxNWVjYTYyMDc=", "commit": {"author": {"name": "Jeffrey Seyfried", "email": "jeffrey.seyfried@gmail.com", "date": "2017-06-05T01:41:33Z"}, "committer": {"name": "Jeffrey Seyfried", "email": "jeffrey.seyfried@gmail.com", "date": "2017-06-26T02:06:34Z"}, "message": "Address review comments.", "tree": {"sha": "b96df67fc15071360fa03403d9c89a1c561e4b35", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/b96df67fc15071360fa03403d9c89a1c561e4b35"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/71d4a860a1797300f976b45d7b7a41815eca6207", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/71d4a860a1797300f976b45d7b7a41815eca6207", "html_url": "https://github.com/rust-lang/rust/commit/71d4a860a1797300f976b45d7b7a41815eca6207", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/71d4a860a1797300f976b45d7b7a41815eca6207/comments", "author": {"login": "jseyfried", "id": 8652869, "node_id": "MDQ6VXNlcjg2NTI4Njk=", "avatar_url": "https://avatars.githubusercontent.com/u/8652869?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jseyfried", "html_url": "https://github.com/jseyfried", "followers_url": "https://api.github.com/users/jseyfried/followers", "following_url": "https://api.github.com/users/jseyfried/following{/other_user}", "gists_url": "https://api.github.com/users/jseyfried/gists{/gist_id}", "starred_url": "https://api.github.com/users/jseyfried/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jseyfried/subscriptions", "organizations_url": "https://api.github.com/users/jseyfried/orgs", "repos_url": "https://api.github.com/users/jseyfried/repos", "events_url": "https://api.github.com/users/jseyfried/events{/privacy}", "received_events_url": "https://api.github.com/users/jseyfried/received_events", "type": "User", "site_admin": false}, "committer": {"login": "jseyfried", "id": 8652869, "node_id": "MDQ6VXNlcjg2NTI4Njk=", "avatar_url": "https://avatars.githubusercontent.com/u/8652869?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jseyfried", "html_url": "https://github.com/jseyfried", "followers_url": "https://api.github.com/users/jseyfried/followers", "following_url": "https://api.github.com/users/jseyfried/following{/other_user}", "gists_url": "https://api.github.com/users/jseyfried/gists{/gist_id}", "starred_url": "https://api.github.com/users/jseyfried/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jseyfried/subscriptions", "organizations_url": "https://api.github.com/users/jseyfried/orgs", "repos_url": "https://api.github.com/users/jseyfried/repos", "events_url": "https://api.github.com/users/jseyfried/events{/privacy}", "received_events_url": "https://api.github.com/users/jseyfried/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "1e32a3f15e35075b537741cdfef40f29bb856582", "url": "https://api.github.com/repos/rust-lang/rust/commits/1e32a3f15e35075b537741cdfef40f29bb856582", "html_url": "https://github.com/rust-lang/rust/commit/1e32a3f15e35075b537741cdfef40f29bb856582"}], "stats": {"total": 282, "additions": 172, "deletions": 110}, "files": [{"sha": "06f9634d70613e2a47fe4b7d5a246087aff9a865", "filename": "src/libproc_macro/lib.rs", "status": "modified", "additions": 113, "deletions": 77, "changes": 190, "blob_url": "https://github.com/rust-lang/rust/blob/71d4a860a1797300f976b45d7b7a41815eca6207/src%2Flibproc_macro%2Flib.rs", "raw_url": "https://github.com/rust-lang/rust/raw/71d4a860a1797300f976b45d7b7a41815eca6207/src%2Flibproc_macro%2Flib.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibproc_macro%2Flib.rs?ref=71d4a860a1797300f976b45d7b7a41815eca6207", "patch": "@@ -46,14 +46,14 @@\n extern crate syntax;\n extern crate syntax_pos;\n \n-use std::{fmt, iter, ops};\n+use std::{ascii, fmt, iter};\n use std::str::FromStr;\n \n use syntax::ast;\n use syntax::errors::DiagnosticBuilder;\n use syntax::parse::{self, token, parse_stream_from_source_str};\n use syntax::print::pprust;\n-use syntax::symbol;\n+use syntax::symbol::Symbol;\n use syntax::tokenstream;\n use syntax_pos::DUMMY_SP;\n use syntax_pos::SyntaxContext;\n@@ -68,12 +68,12 @@ use syntax_pos::SyntaxContext;\n /// The API of this type is intentionally bare-bones, but it'll be expanded over\n /// time!\n #[stable(feature = \"proc_macro_lib\", since = \"1.15.0\")]\n-#[derive(Clone)]\n+#[derive(Clone, Debug)]\n pub struct TokenStream(tokenstream::TokenStream);\n \n /// Error returned from `TokenStream::from_str`.\n-#[derive(Debug)]\n #[stable(feature = \"proc_macro_lib\", since = \"1.15.0\")]\n+#[derive(Debug)]\n pub struct LexError {\n     _inner: (),\n }\n@@ -110,24 +110,28 @@ impl fmt::Display for TokenStream {\n #[macro_export]\n macro_rules! quote { () => {} }\n \n+#[unstable(feature = \"proc_macro_internals\", issue = \"27812\")]\n+#[doc(hidden)]\n+mod quote;\n+\n #[unstable(feature = \"proc_macro\", issue = \"38356\")]\n impl From<TokenTree> for TokenStream {\n     fn from(tree: TokenTree) -> TokenStream {\n-        TokenStream(tree.to_raw())\n+        TokenStream(tree.to_internal())\n     }\n }\n \n #[unstable(feature = \"proc_macro\", issue = \"38356\")]\n-impl From<TokenKind> for TokenStream {\n-    fn from(kind: TokenKind) -> TokenStream {\n+impl From<TokenNode> for TokenStream {\n+    fn from(kind: TokenNode) -> TokenStream {\n         TokenTree::from(kind).into()\n     }\n }\n \n #[unstable(feature = \"proc_macro\", issue = \"38356\")]\n impl<T: Into<TokenStream>> iter::FromIterator<T> for TokenStream {\n     fn from_iter<I: IntoIterator<Item = T>>(streams: I) -> Self {\n-        let mut builder = tokenstream::TokenStream::builder();\n+        let mut builder = tokenstream::TokenStreamBuilder::new();\n         for stream in streams {\n             builder.push(stream.into().0);\n         }\n@@ -138,10 +142,10 @@ impl<T: Into<TokenStream>> iter::FromIterator<T> for TokenStream {\n #[unstable(feature = \"proc_macro\", issue = \"38356\")]\n impl IntoIterator for TokenStream {\n     type Item = TokenTree;\n-    type IntoIter = TokenIter;\n+    type IntoIter = TokenTreeIter;\n \n-    fn into_iter(self) -> TokenIter {\n-        TokenIter { cursor: self.0.trees(), next: None }\n+    fn into_iter(self) -> TokenTreeIter {\n+        TokenTreeIter { cursor: self.0.trees(), next: None }\n     }\n }\n \n@@ -161,7 +165,7 @@ impl TokenStream {\n \n /// A region of source code, along with macro expansion information.\n #[unstable(feature = \"proc_macro\", issue = \"38356\")]\n-#[derive(Copy, Clone)]\n+#[derive(Copy, Clone, Debug)]\n pub struct Span(syntax_pos::Span);\n \n #[unstable(feature = \"proc_macro\", issue = \"38356\")]\n@@ -174,6 +178,13 @@ impl Default for Span {\n     }\n }\n \n+/// Quote a `Span` into a `TokenStream`.\n+/// This is needed to implement a custom quoter.\n+#[unstable(feature = \"proc_macro\", issue = \"38356\")]\n+pub fn quote_span(span: Span) -> TokenStream {\n+    TokenStream(quote::Quote::quote(&span.0))\n+}\n+\n impl Span {\n     /// The span of the invocation of the current procedural macro.\n     #[unstable(feature = \"proc_macro\", issue = \"38356\")]\n@@ -184,17 +195,17 @@ impl Span {\n \n /// A single token or a delimited sequence of token trees (e.g. `[1, (), ..]`).\n #[unstable(feature = \"proc_macro\", issue = \"38356\")]\n-#[derive(Clone)]\n+#[derive(Clone, Debug)]\n pub struct TokenTree {\n     /// The `TokenTree`'s span\n     pub span: Span,\n     /// Description of the `TokenTree`\n-    pub kind: TokenKind,\n+    pub kind: TokenNode,\n }\n \n #[unstable(feature = \"proc_macro\", issue = \"38356\")]\n-impl From<TokenKind> for TokenTree {\n-    fn from(kind: TokenKind) -> TokenTree {\n+impl From<TokenNode> for TokenTree {\n+    fn from(kind: TokenNode) -> TokenTree {\n         TokenTree { span: Span::default(), kind: kind }\n     }\n }\n@@ -207,21 +218,21 @@ impl fmt::Display for TokenTree {\n }\n \n /// Description of a `TokenTree`\n-#[derive(Clone)]\n+#[derive(Clone, Debug)]\n #[unstable(feature = \"proc_macro\", issue = \"38356\")]\n-pub enum TokenKind {\n+pub enum TokenNode {\n     /// A delimited tokenstream.\n-    Sequence(Delimiter, TokenStream),\n+    Group(Delimiter, TokenStream),\n     /// A unicode identifier.\n-    Word(Symbol),\n+    Term(Term),\n     /// A punctuation character (`+`, `,`, `$`, etc.).\n-    Op(char, OpKind),\n+    Op(char, Spacing),\n     /// A literal character (`'a'`), string (`\"hello\"`), or number (`2.3`).\n     Literal(Literal),\n }\n \n /// Describes how a sequence of token trees is delimited.\n-#[derive(Copy, Clone)]\n+#[derive(Copy, Clone, Debug)]\n #[unstable(feature = \"proc_macro\", issue = \"38356\")]\n pub enum Delimiter {\n     /// `( ... )`\n@@ -235,45 +246,43 @@ pub enum Delimiter {\n }\n \n /// An interned string.\n-#[derive(Copy, Clone)]\n+#[derive(Copy, Clone, Debug)]\n #[unstable(feature = \"proc_macro\", issue = \"38356\")]\n-pub struct Symbol(symbol::Symbol);\n+pub struct Term(Symbol);\n \n-#[unstable(feature = \"proc_macro\", issue = \"38356\")]\n-impl<'a> From<&'a str> for Symbol {\n-    fn from(string: &'a str) -> Symbol {\n-        Symbol(symbol::Symbol::intern(string))\n+impl Term {\n+    /// Intern a string into a `Term`.\n+    #[unstable(feature = \"proc_macro\", issue = \"38356\")]\n+    pub fn intern(string: &str) -> Term {\n+        Term(Symbol::intern(string))\n     }\n-}\n \n-#[unstable(feature = \"proc_macro\", issue = \"38356\")]\n-impl ops::Deref for Symbol {\n-    type Target = str;\n-\n-    fn deref(&self) -> &str {\n-        unsafe { &*(self.0.as_str().deref() as *const str) }\n+    /// Get a reference to the interned string.\n+    #[unstable(feature = \"proc_macro\", issue = \"38356\")]\n+    pub fn as_str(&self) -> &str {\n+        unsafe { &*(&*self.0.as_str() as *const str) }\n     }\n }\n \n /// Whether an `Op` is either followed immediately by another `Op` or followed by whitespace.\n-#[derive(Copy, Clone)]\n+#[derive(Copy, Clone, Debug)]\n #[unstable(feature = \"proc_macro\", issue = \"38356\")]\n-pub enum OpKind {\n+pub enum Spacing {\n     /// e.g. `+` is `Alone` in `+ =`.\n     Alone,\n     /// e.g. `+` is `Joint` in `+=`.\n     Joint,\n }\n \n /// A literal character (`'a'`), string (`\"hello\"`), or number (`2.3`).\n-#[derive(Clone)]\n+#[derive(Clone, Debug)]\n #[unstable(feature = \"proc_macro\", issue = \"38356\")]\n pub struct Literal(token::Token);\n \n #[unstable(feature = \"proc_macro\", issue = \"38356\")]\n impl fmt::Display for Literal {\n     fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {\n-        TokenTree { kind: TokenKind::Literal(self.clone()), span: Span(DUMMY_SP) }.fmt(f)\n+        TokenTree { kind: TokenNode::Literal(self.clone()), span: Span(DUMMY_SP) }.fmt(f)\n     }\n }\n \n@@ -282,30 +291,51 @@ macro_rules! int_literals {\n         /// Integer literal.\n         #[unstable(feature = \"proc_macro\", issue = \"38356\")]\n         pub fn $int_kind(n: $int_kind) -> Literal {\n-            Literal::integer(n as i128, stringify!($int_kind))\n+            Literal::typed_integer(n as i128, stringify!($int_kind))\n         }\n     )*}\n }\n \n impl Literal {\n+    /// Integer literal\n+    #[unstable(feature = \"proc_macro\", issue = \"38356\")]\n+    pub fn integer(n: i128) -> Literal {\n+        Literal(token::Literal(token::Lit::Integer(Symbol::intern(&n.to_string())), None))\n+    }\n+\n     int_literals!(u8, i8, u16, i16, u32, i32, u64, i64);\n-    fn integer(n: i128, kind: &'static str) -> Literal {\n-        Literal(token::Literal(token::Lit::Integer(symbol::Symbol::intern(&n.to_string())),\n-                               Some(symbol::Symbol::intern(kind))))\n+    fn typed_integer(n: i128, kind: &'static str) -> Literal {\n+        Literal(token::Literal(token::Lit::Integer(Symbol::intern(&n.to_string())),\n+                               Some(Symbol::intern(kind))))\n+    }\n+\n+    /// Floating point literal.\n+    #[unstable(feature = \"proc_macro\", issue = \"38356\")]\n+    pub fn float(n: f64) -> Literal {\n+        if !n.is_finite() {\n+            panic!(\"Invalid float literal {}\", n);\n+        }\n+        Literal(token::Literal(token::Lit::Float(Symbol::intern(&n.to_string())), None))\n     }\n \n     /// Floating point literal.\n     #[unstable(feature = \"proc_macro\", issue = \"38356\")]\n     pub fn f32(n: f32) -> Literal {\n-        Literal(token::Literal(token::Lit::Float(symbol::Symbol::intern(&n.to_string())),\n-                               Some(symbol::Symbol::intern(\"f32\"))))\n+        if !n.is_finite() {\n+            panic!(\"Invalid f32 literal {}\", n);\n+        }\n+        Literal(token::Literal(token::Lit::Float(Symbol::intern(&n.to_string())),\n+                               Some(Symbol::intern(\"f32\"))))\n     }\n \n     /// Floating point literal.\n     #[unstable(feature = \"proc_macro\", issue = \"38356\")]\n-    pub fn f64(n: f32) -> Literal {\n-        Literal(token::Literal(token::Lit::Float(symbol::Symbol::intern(&n.to_string())),\n-                               Some(symbol::Symbol::intern(\"f64\"))))\n+    pub fn f64(n: f64) -> Literal {\n+        if !n.is_finite() {\n+            panic!(\"Invalid f64 literal {}\", n);\n+        }\n+        Literal(token::Literal(token::Lit::Float(Symbol::intern(&n.to_string())),\n+                               Some(Symbol::intern(\"f64\"))))\n     }\n \n     /// String literal.\n@@ -315,36 +345,44 @@ impl Literal {\n         for ch in string.chars() {\n             escaped.extend(ch.escape_unicode());\n         }\n-        Literal(token::Literal(token::Lit::Str_(symbol::Symbol::intern(&escaped)), None))\n+        Literal(token::Literal(token::Lit::Str_(Symbol::intern(&escaped)), None))\n     }\n \n     /// Character literal.\n     #[unstable(feature = \"proc_macro\", issue = \"38356\")]\n     pub fn character(ch: char) -> Literal {\n         let mut escaped = String::new();\n         escaped.extend(ch.escape_unicode());\n-        Literal(token::Literal(token::Lit::Char(symbol::Symbol::intern(&escaped)), None))\n+        Literal(token::Literal(token::Lit::Char(Symbol::intern(&escaped)), None))\n+    }\n+\n+    /// Byte string literal.\n+    #[unstable(feature = \"proc_macro\", issue = \"38356\")]\n+    pub fn byte_string(bytes: &[u8]) -> Literal {\n+        let string = bytes.iter().cloned().flat_map(ascii::escape_default)\n+            .map(Into::<char>::into).collect::<String>();\n+        Literal(token::Literal(token::Lit::ByteStr(Symbol::intern(&string)), None))\n     }\n }\n \n /// An iterator over `TokenTree`s.\n #[unstable(feature = \"proc_macro\", issue = \"38356\")]\n-pub struct TokenIter {\n+pub struct TokenTreeIter {\n     cursor: tokenstream::Cursor,\n     next: Option<tokenstream::TokenStream>,\n }\n \n #[unstable(feature = \"proc_macro\", issue = \"38356\")]\n-impl Iterator for TokenIter {\n+impl Iterator for TokenTreeIter {\n     type Item = TokenTree;\n \n     fn next(&mut self) -> Option<TokenTree> {\n         loop {\n             let next =\n                 unwrap_or!(self.next.take().or_else(|| self.cursor.next_as_stream()), return None);\n-            let tree = TokenTree::from_raw(next, &mut self.next);\n+            let tree = TokenTree::from_internal(next, &mut self.next);\n             if tree.span.0 == DUMMY_SP {\n-                if let TokenKind::Sequence(Delimiter::None, stream) = tree.kind {\n+                if let TokenNode::Group(Delimiter::None, stream) = tree.kind {\n                     self.cursor.insert(stream.0);\n                     continue\n                 }\n@@ -355,7 +393,7 @@ impl Iterator for TokenIter {\n }\n \n impl Delimiter {\n-    fn from_raw(delim: token::DelimToken) -> Delimiter {\n+    fn from_internal(delim: token::DelimToken) -> Delimiter {\n         match delim {\n             token::Paren => Delimiter::Parenthesis,\n             token::Brace => Delimiter::Brace,\n@@ -364,7 +402,7 @@ impl Delimiter {\n         }\n     }\n \n-    fn to_raw(self) -> token::DelimToken {\n+    fn to_internal(self) -> token::DelimToken {\n         match self {\n             Delimiter::Parenthesis => token::Paren,\n             Delimiter::Brace => token::Brace,\n@@ -375,25 +413,25 @@ impl Delimiter {\n }\n \n impl TokenTree {\n-    fn from_raw(stream: tokenstream::TokenStream, next: &mut Option<tokenstream::TokenStream>)\n+    fn from_internal(stream: tokenstream::TokenStream, next: &mut Option<tokenstream::TokenStream>)\n                 -> TokenTree {\n         use syntax::parse::token::*;\n \n         let (tree, is_joint) = stream.as_tree();\n         let (mut span, token) = match tree {\n             tokenstream::TokenTree::Token(span, token) => (span, token),\n             tokenstream::TokenTree::Delimited(span, delimed) => {\n-                let delimiter = Delimiter::from_raw(delimed.delim);\n+                let delimiter = Delimiter::from_internal(delimed.delim);\n                 return TokenTree {\n                     span: Span(span),\n-                    kind: TokenKind::Sequence(delimiter, TokenStream(delimed.tts.into())),\n+                    kind: TokenNode::Group(delimiter, TokenStream(delimed.tts.into())),\n                 };\n             }\n         };\n \n-        let op_kind = if is_joint { OpKind::Joint } else { OpKind::Alone };\n+        let op_kind = if is_joint { Spacing::Joint } else { Spacing::Alone };\n         macro_rules! op {\n-            ($op:expr) => { TokenKind::Op($op, op_kind) }\n+            ($op:expr) => { TokenNode::Op($op, op_kind) }\n         }\n \n         macro_rules! joint {\n@@ -402,12 +440,12 @@ impl TokenTree {\n \n         fn joint(first: char, rest: Token, is_joint: bool, span: &mut syntax_pos::Span,\n                  next: &mut Option<tokenstream::TokenStream>)\n-                 -> TokenKind {\n+                 -> TokenNode {\n             let (first_span, rest_span) = (*span, *span);\n             *span = first_span;\n             let tree = tokenstream::TokenTree::Token(rest_span, rest);\n             *next = Some(if is_joint { tree.joint() } else { tree.into() });\n-            TokenKind::Op(first, OpKind::Joint)\n+            TokenNode::Op(first, Spacing::Joint)\n         }\n \n         let kind = match token {\n@@ -458,11 +496,11 @@ impl TokenTree {\n             Question => op!('?'),\n             Underscore => op!('_'),\n \n-            Ident(ident) | Lifetime(ident) => TokenKind::Word(Symbol(ident.name)),\n-            Literal(..) | DocComment(..) => TokenKind::Literal(self::Literal(token)),\n+            Ident(ident) | Lifetime(ident) => TokenNode::Term(Term(ident.name)),\n+            Literal(..) | DocComment(..) => TokenNode::Literal(self::Literal(token)),\n \n             Interpolated(ref nt) => __internal::with_sess(|(sess, _)| {\n-                TokenKind::Sequence(Delimiter::None, TokenStream(nt.1.force(|| {\n+                TokenNode::Group(Delimiter::None, TokenStream(nt.1.force(|| {\n                     // FIXME(jseyfried): Avoid this pretty-print + reparse hack\n                     let name = \"<macro expansion>\".to_owned();\n                     let source = pprust::token_to_string(&token);\n@@ -477,25 +515,25 @@ impl TokenTree {\n         TokenTree { span: Span(span), kind: kind }\n     }\n \n-    fn to_raw(self) -> tokenstream::TokenStream {\n+    fn to_internal(self) -> tokenstream::TokenStream {\n         use syntax::parse::token::*;\n         use syntax::tokenstream::{TokenTree, Delimited};\n \n         let (op, kind) = match self.kind {\n-            TokenKind::Op(op, kind) => (op, kind),\n-            TokenKind::Sequence(delimiter, tokens) => {\n+            TokenNode::Op(op, kind) => (op, kind),\n+            TokenNode::Group(delimiter, tokens) => {\n                 return TokenTree::Delimited(self.span.0, Delimited {\n-                    delim: delimiter.to_raw(),\n+                    delim: delimiter.to_internal(),\n                     tts: tokens.0.into(),\n                 }).into();\n             },\n-            TokenKind::Word(symbol) => {\n+            TokenNode::Term(symbol) => {\n                 let ident = ast::Ident { name: symbol.0, ctxt: self.span.0.ctxt };\n                 let token =\n                     if symbol.0.as_str().starts_with(\"'\") { Lifetime(ident) } else { Ident(ident) };\n                 return TokenTree::Token(self.span.0, token).into();\n             }\n-            TokenKind::Literal(token) => return TokenTree::Token(self.span.0, token.0).into(),\n+            TokenNode::Literal(token) => return TokenTree::Token(self.span.0, token.0).into(),\n         };\n \n         let token = match op {\n@@ -526,8 +564,8 @@ impl TokenTree {\n \n         let tree = TokenTree::Token(self.span.0, token);\n         match kind {\n-            OpKind::Alone => tree.into(),\n-            OpKind::Joint => tree.joint(),\n+            Spacing::Alone => tree.into(),\n+            Spacing::Joint => tree.joint(),\n         }\n     }\n }\n@@ -543,10 +581,8 @@ impl TokenTree {\n /// all of the contents.\n #[unstable(feature = \"proc_macro_internals\", issue = \"27812\")]\n #[doc(hidden)]\n-#[path = \"\"]\n pub mod __internal {\n-    mod quote;\n-    pub use self::quote::{Quoter, __rt};\n+    pub use quote::{Quoter, __rt};\n \n     use std::cell::Cell;\n "}, {"sha": "bee2c1e0eb6b608a21e273a383bea64221d4a3ba", "filename": "src/libproc_macro/quote.rs", "status": "modified", "additions": 10, "deletions": 6, "changes": 16, "blob_url": "https://github.com/rust-lang/rust/blob/71d4a860a1797300f976b45d7b7a41815eca6207/src%2Flibproc_macro%2Fquote.rs", "raw_url": "https://github.com/rust-lang/rust/raw/71d4a860a1797300f976b45d7b7a41815eca6207/src%2Flibproc_macro%2Fquote.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibproc_macro%2Fquote.rs?ref=71d4a860a1797300f976b45d7b7a41815eca6207", "patch": "@@ -9,13 +9,17 @@\n // except according to those terms.\n \n //! # Quasiquoter\n-//! This file contains the implementation internals of the quasiquoter provided by `qquote!`.\n+//! This file contains the implementation internals of the quasiquoter provided by `quote!`.\n+\n+//! This quasiquoter uses macros 2.0 hygiene to reliably use items from `__rt`,\n+//! including re-exported API `libsyntax`, to build a `syntax::tokenstream::TokenStream`\n+//! and wrap it into a `proc_macro::TokenStream`.\n \n use syntax::ast::Ident;\n use syntax::ext::base::{ExtCtxt, ProcMacro};\n use syntax::parse::token::{self, Token, Lit};\n use syntax::symbol::Symbol;\n-use syntax::tokenstream::{Delimited, TokenTree, TokenStream};\n+use syntax::tokenstream::{Delimited, TokenTree, TokenStream, TokenStreamBuilder};\n use syntax_pos::{DUMMY_SP, Span};\n use syntax_pos::hygiene::SyntaxContext;\n \n@@ -25,7 +29,7 @@ pub mod __rt {\n     pub use syntax::ast::Ident;\n     pub use syntax::parse::token;\n     pub use syntax::symbol::Symbol;\n-    pub use syntax::tokenstream::{TokenStream, TokenTree, Delimited};\n+    pub use syntax::tokenstream::{TokenStream, TokenStreamBuilder, TokenTree, Delimited};\n     pub use super::{ctxt, span};\n \n     pub fn unquote<T: Into<::TokenStream> + Clone>(tokens: &T) -> TokenStream {\n@@ -41,7 +45,7 @@ pub fn span() -> Span {\n     ::Span::default().0\n }\n \n-trait Quote {\n+pub trait Quote {\n     fn quote(&self) -> TokenStream;\n }\n \n@@ -98,8 +102,8 @@ impl<T: Quote> Quote for Option<T> {\n \n impl Quote for TokenStream {\n     fn quote(&self) -> TokenStream {\n-        let mut builder = TokenStream::builder();\n-        builder.push(quote!(rt::TokenStream::builder()));\n+        let mut builder = TokenStreamBuilder::new();\n+        builder.push(quote!(rt::TokenStreamBuilder::new()));\n \n         let mut trees = self.trees();\n         loop {"}, {"sha": "e008a5cd9ea6179e8814a902ac465f2c1a31539d", "filename": "src/librustc_metadata/creader.rs", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "blob_url": "https://github.com/rust-lang/rust/blob/71d4a860a1797300f976b45d7b7a41815eca6207/src%2Flibrustc_metadata%2Fcreader.rs", "raw_url": "https://github.com/rust-lang/rust/raw/71d4a860a1797300f976b45d7b7a41815eca6207/src%2Flibrustc_metadata%2Fcreader.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_metadata%2Fcreader.rs?ref=71d4a860a1797300f976b45d7b7a41815eca6207", "patch": "@@ -14,7 +14,7 @@ use cstore::{self, CStore, CrateSource, MetadataBlob};\n use locator::{self, CratePaths};\n use schema::{CrateRoot, Tracked};\n \n-use rustc::hir::def_id::{CrateNum, DefIndex};\n+use rustc::hir::def_id::{CrateNum, DefIndex, CRATE_DEF_INDEX};\n use rustc::hir::svh::Svh;\n use rustc::middle::cstore::DepKind;\n use rustc::session::Session;\n@@ -35,7 +35,7 @@ use std::path::PathBuf;\n use std::rc::Rc;\n use std::{cmp, fs};\n \n-use syntax::ast::{self, Ident};\n+use syntax::ast;\n use syntax::abi::Abi;\n use syntax::attr;\n use syntax::ext::base::SyntaxExtension;\n@@ -1238,7 +1238,7 @@ fn proc_macro_def_path_table(proc_macros: &[(ast::Name, Rc<SyntaxExtension>)]) -\n         let key = DefKey {\n             parent: Some(CRATE_DEF_INDEX),\n             disambiguated_data: DisambiguatedDefPathData {\n-                data: DefPathData::MacroDef(Ident::with_empty_ctxt(proc_macro.0)),\n+                data: DefPathData::MacroDef(proc_macro.0),\n                 disambiguator: 0,\n             },\n         };"}, {"sha": "66775d8c43d638edc0ef04ce2a772e873f292b4c", "filename": "src/libsyntax/parse/lexer/mod.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/71d4a860a1797300f976b45d7b7a41815eca6207/src%2Flibsyntax%2Fparse%2Flexer%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/71d4a860a1797300f976b45d7b7a41815eca6207/src%2Flibsyntax%2Fparse%2Flexer%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Flexer%2Fmod.rs?ref=71d4a860a1797300f976b45d7b7a41815eca6207", "patch": "@@ -483,7 +483,7 @@ impl<'a> StringReader<'a> {\n         self.with_str_from(start, |string| {\n             if string == \"_\" {\n                 self.sess.span_diagnostic\n-                    .struct_span_warn(mk_sp(start, self.pos),\n+                    .struct_span_warn(self.mk_sp(start, self.pos),\n                                       \"underscore literal suffix is not allowed\")\n                     .warn(\"this was previously accepted by the compiler but is \\\n                           being phased out; it will become a hard error in \\"}, {"sha": "d4198261d3f692a604fa04905e1e1626964b8f85", "filename": "src/libsyntax/parse/token.rs", "status": "modified", "additions": 32, "deletions": 12, "changes": 44, "blob_url": "https://github.com/rust-lang/rust/blob/71d4a860a1797300f976b45d7b7a41815eca6207/src%2Flibsyntax%2Fparse%2Ftoken.rs", "raw_url": "https://github.com/rust-lang/rust/raw/71d4a860a1797300f976b45d7b7a41815eca6207/src%2Flibsyntax%2Fparse%2Ftoken.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Ftoken.rs?ref=71d4a860a1797300f976b45d7b7a41815eca6207", "patch": "@@ -20,8 +20,8 @@ use serialize::{Decodable, Decoder, Encodable, Encoder};\n use symbol::keywords;\n use tokenstream::{TokenStream, TokenTree};\n \n-use std::cell::RefCell;\n-use std::fmt;\n+use std::cell::Cell;\n+use std::{cmp, fmt};\n use std::rc::Rc;\n \n #[derive(Clone, RustcEncodable, RustcDecodable, PartialEq, Eq, Hash, Debug, Copy)]\n@@ -169,7 +169,8 @@ pub enum Token {\n     Underscore,\n     Lifetime(ast::Ident),\n \n-    /* For interpolation */\n+    // The `LazyTokenStream` is a pure function of the `Nonterminal`,\n+    // and so the `LazyTokenStream` can be ignored by Eq, Hash, etc.\n     Interpolated(Rc<(Nonterminal, LazyTokenStream)>),\n     // Can be expanded into several tokens.\n     /// Doc comment\n@@ -468,19 +469,40 @@ pub fn is_op(tok: &Token) -> bool {\n     }\n }\n \n-#[derive(Clone, Eq, PartialEq, Debug)]\n-pub struct LazyTokenStream(RefCell<Option<TokenStream>>);\n+pub struct LazyTokenStream(Cell<Option<TokenStream>>);\n+\n+impl Clone for LazyTokenStream {\n+    fn clone(&self) -> Self {\n+        let opt_stream = self.0.take();\n+        self.0.set(opt_stream.clone());\n+        LazyTokenStream(Cell::new(opt_stream))\n+    }\n+}\n+\n+impl cmp::Eq for LazyTokenStream {}\n+impl PartialEq for LazyTokenStream {\n+    fn eq(&self, _other: &LazyTokenStream) -> bool {\n+        true\n+    }\n+}\n+\n+impl fmt::Debug for LazyTokenStream {\n+    fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {\n+        fmt::Debug::fmt(&self.clone().0.into_inner(), f)\n+    }\n+}\n \n impl LazyTokenStream {\n     pub fn new() -> Self {\n-        LazyTokenStream(RefCell::new(None))\n+        LazyTokenStream(Cell::new(None))\n     }\n \n     pub fn force<F: FnOnce() -> TokenStream>(&self, f: F) -> TokenStream {\n-        let mut opt_stream = self.0.borrow_mut();\n+        let mut opt_stream = self.0.take();\n         if opt_stream.is_none() {\n-            *opt_stream = Some(f());\n-        };\n+            opt_stream = Some(f());\n+        }\n+        self.0.set(opt_stream.clone());\n         opt_stream.clone().unwrap()\n     }\n }\n@@ -498,7 +520,5 @@ impl Decodable for LazyTokenStream {\n }\n \n impl ::std::hash::Hash for LazyTokenStream {\n-    fn hash<H: ::std::hash::Hasher>(&self, hasher: &mut H) {\n-        self.0.borrow().hash(hasher);\n-    }\n+    fn hash<H: ::std::hash::Hasher>(&self, _hasher: &mut H) {}\n }"}, {"sha": "8eee25405df6bca679924f97ca76acd4e41ad9a2", "filename": "src/libsyntax/tokenstream.rs", "status": "modified", "additions": 6, "deletions": 4, "changes": 10, "blob_url": "https://github.com/rust-lang/rust/blob/71d4a860a1797300f976b45d7b7a41815eca6207/src%2Flibsyntax%2Ftokenstream.rs", "raw_url": "https://github.com/rust-lang/rust/raw/71d4a860a1797300f976b45d7b7a41815eca6207/src%2Flibsyntax%2Ftokenstream.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Ftokenstream.rs?ref=71d4a860a1797300f976b45d7b7a41815eca6207", "patch": "@@ -201,10 +201,6 @@ impl TokenStream {\n         }\n     }\n \n-    pub fn builder() -> TokenStreamBuilder {\n-        TokenStreamBuilder(Vec::new())\n-    }\n-\n     pub fn concat(mut streams: Vec<TokenStream>) -> TokenStream {\n         match streams.len() {\n             0 => TokenStream::empty(),\n@@ -235,6 +231,8 @@ impl TokenStream {\n         true\n     }\n \n+    /// Precondition: `self` consists of a single token tree.\n+    /// Returns true if the token tree is a joint operation w.r.t. `proc_macro::TokenNode`.\n     pub fn as_tree(self) -> (TokenTree, bool /* joint? */) {\n         match self.kind {\n             TokenStreamKind::Tree(tree) => (tree, false),\n@@ -277,6 +275,10 @@ impl TokenStream {\n pub struct TokenStreamBuilder(Vec<TokenStream>);\n \n impl TokenStreamBuilder {\n+    pub fn new() -> TokenStreamBuilder {\n+        TokenStreamBuilder(Vec::new())\n+    }\n+\n     pub fn push<T: Into<TokenStream>>(&mut self, stream: T) {\n         let stream = stream.into();\n         let last_tree_if_joint = self.0.last().and_then(TokenStream::last_tree_if_joint);"}, {"sha": "6d6a452b03b627282cb24298f473f8bac3f3e5b3", "filename": "src/test/run-pass-fulldeps/auxiliary/cond_plugin.rs", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "blob_url": "https://github.com/rust-lang/rust/blob/71d4a860a1797300f976b45d7b7a41815eca6207/src%2Ftest%2Frun-pass-fulldeps%2Fauxiliary%2Fcond_plugin.rs", "raw_url": "https://github.com/rust-lang/rust/raw/71d4a860a1797300f976b45d7b7a41815eca6207/src%2Ftest%2Frun-pass-fulldeps%2Fauxiliary%2Fcond_plugin.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ftest%2Frun-pass-fulldeps%2Fauxiliary%2Fcond_plugin.rs?ref=71d4a860a1797300f976b45d7b7a41815eca6207", "patch": "@@ -15,15 +15,15 @@\n \n extern crate proc_macro;\n \n-use proc_macro::{TokenStream, TokenKind, quote};\n+use proc_macro::{TokenStream, TokenNode, quote};\n \n #[proc_macro]\n pub fn cond(input: TokenStream) -> TokenStream {\n     let mut conds = Vec::new();\n     let mut input = input.into_iter().peekable();\n     while let Some(tree) = input.next() {\n         let cond = match tree.kind {\n-            TokenKind::Sequence(_, cond) => cond,\n+            TokenNode::Sequence(_, cond) => cond,\n             _ => panic!(\"Invalid input\"),\n         };\n         let mut cond_trees = cond.clone().into_iter();\n@@ -33,7 +33,7 @@ pub fn cond(input: TokenStream) -> TokenStream {\n             panic!(\"Invalid macro usage in cond: {}\", cond);\n         }\n         let is_else = match test.kind {\n-            TokenKind::Word(word) => *word == *\"else\",\n+            TokenNode::Word(word) => word.as_str() == \"else\",\n             _ => false,\n         };\n         conds.push(if is_else || input.peek().is_none() {"}, {"sha": "e7a0283962d81fa613311ee9afb3d4e83937d1e0", "filename": "src/test/run-pass-fulldeps/proc-macro/auxiliary/count_compound_ops.rs", "status": "modified", "additions": 4, "deletions": 4, "changes": 8, "blob_url": "https://github.com/rust-lang/rust/blob/71d4a860a1797300f976b45d7b7a41815eca6207/src%2Ftest%2Frun-pass-fulldeps%2Fproc-macro%2Fauxiliary%2Fcount_compound_ops.rs", "raw_url": "https://github.com/rust-lang/rust/raw/71d4a860a1797300f976b45d7b7a41815eca6207/src%2Ftest%2Frun-pass-fulldeps%2Fproc-macro%2Fauxiliary%2Fcount_compound_ops.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ftest%2Frun-pass-fulldeps%2Fproc-macro%2Fauxiliary%2Fcount_compound_ops.rs?ref=71d4a860a1797300f976b45d7b7a41815eca6207", "patch": "@@ -15,20 +15,20 @@\n \n extern crate proc_macro;\n \n-use proc_macro::{TokenStream, TokenKind, OpKind, Literal, quote};\n+use proc_macro::{TokenStream, TokenNode, OpKind, Literal, quote};\n \n #[proc_macro]\n pub fn count_compound_ops(input: TokenStream) -> TokenStream {\n     assert_eq!(count_compound_ops_helper(quote!(++ (&&) 4@a)), 3);\n-    TokenKind::Literal(Literal::u32(count_compound_ops_helper(input))).into()\n+    TokenNode::Literal(Literal::u32(count_compound_ops_helper(input))).into()\n }\n \n fn count_compound_ops_helper(input: TokenStream) -> u32 {\n     let mut count = 0;\n     for token in input {\n         match token.kind {\n-            TokenKind::Op(c, OpKind::Alone) => count += 1,\n-            TokenKind::Sequence(_, tokens) => count += count_compound_ops_helper(tokens),\n+            TokenNode::Op(c, OpKind::Alone) => count += 1,\n+            TokenNode::Sequence(_, tokens) => count += count_compound_ops_helper(tokens),\n             _ => {}\n         }\n     }"}]}