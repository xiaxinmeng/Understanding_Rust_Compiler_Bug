{"sha": "2b6f1ab0e2303dce8e6e424514fd2a9e74566c89", "node_id": "MDY6Q29tbWl0NzI0NzEyOjJiNmYxYWIwZTIzMDNkY2U4ZTZlNDI0NTE0ZmQyYTllNzQ1NjZjODk=", "commit": {"author": {"name": "Aleksey Kladov", "email": "aleksey.kladov@gmail.com", "date": "2019-11-18T13:08:41Z"}, "committer": {"name": "Aleksey Kladov", "email": "aleksey.kladov@gmail.com", "date": "2019-11-18T13:08:41Z"}, "message": "Collapse TokenMap and RevTokenMap", "tree": {"sha": "c4eb2e0d8dce47d503d1ed063a6b3e0fadba1c1f", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/c4eb2e0d8dce47d503d1ed063a6b3e0fadba1c1f"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/2b6f1ab0e2303dce8e6e424514fd2a9e74566c89", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/2b6f1ab0e2303dce8e6e424514fd2a9e74566c89", "html_url": "https://github.com/rust-lang/rust/commit/2b6f1ab0e2303dce8e6e424514fd2a9e74566c89", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/2b6f1ab0e2303dce8e6e424514fd2a9e74566c89/comments", "author": {"login": "matklad", "id": 1711539, "node_id": "MDQ6VXNlcjE3MTE1Mzk=", "avatar_url": "https://avatars.githubusercontent.com/u/1711539?v=4", "gravatar_id": "", "url": "https://api.github.com/users/matklad", "html_url": "https://github.com/matklad", "followers_url": "https://api.github.com/users/matklad/followers", "following_url": "https://api.github.com/users/matklad/following{/other_user}", "gists_url": "https://api.github.com/users/matklad/gists{/gist_id}", "starred_url": "https://api.github.com/users/matklad/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/matklad/subscriptions", "organizations_url": "https://api.github.com/users/matklad/orgs", "repos_url": "https://api.github.com/users/matklad/repos", "events_url": "https://api.github.com/users/matklad/events{/privacy}", "received_events_url": "https://api.github.com/users/matklad/received_events", "type": "User", "site_admin": false}, "committer": {"login": "matklad", "id": 1711539, "node_id": "MDQ6VXNlcjE3MTE1Mzk=", "avatar_url": "https://avatars.githubusercontent.com/u/1711539?v=4", "gravatar_id": "", "url": "https://api.github.com/users/matklad", "html_url": "https://github.com/matklad", "followers_url": "https://api.github.com/users/matklad/followers", "following_url": "https://api.github.com/users/matklad/following{/other_user}", "gists_url": "https://api.github.com/users/matklad/gists{/gist_id}", "starred_url": "https://api.github.com/users/matklad/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/matklad/subscriptions", "organizations_url": "https://api.github.com/users/matklad/orgs", "repos_url": "https://api.github.com/users/matklad/repos", "events_url": "https://api.github.com/users/matklad/events{/privacy}", "received_events_url": "https://api.github.com/users/matklad/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "789a0d2a6474f924a0c3239a085b5e8579b1e7f6", "url": "https://api.github.com/repos/rust-lang/rust/commits/789a0d2a6474f924a0c3239a085b5e8579b1e7f6", "html_url": "https://github.com/rust-lang/rust/commit/789a0d2a6474f924a0c3239a085b5e8579b1e7f6"}], "stats": {"total": 224, "additions": 105, "deletions": 119}, "files": [{"sha": "3c11c8a22814f59e576e4be16d536da518b7f7bb", "filename": "crates/ra_hir_expand/src/db.rs", "status": "modified", "additions": 3, "deletions": 5, "changes": 8, "blob_url": "https://github.com/rust-lang/rust/blob/2b6f1ab0e2303dce8e6e424514fd2a9e74566c89/crates%2Fra_hir_expand%2Fsrc%2Fdb.rs", "raw_url": "https://github.com/rust-lang/rust/raw/2b6f1ab0e2303dce8e6e424514fd2a9e74566c89/crates%2Fra_hir_expand%2Fsrc%2Fdb.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fra_hir_expand%2Fsrc%2Fdb.rs?ref=2b6f1ab0e2303dce8e6e424514fd2a9e74566c89", "patch": "@@ -59,10 +59,8 @@ pub trait AstDatabase: SourceDatabase {\n     fn intern_macro(&self, macro_call: MacroCallLoc) -> MacroCallId;\n     fn macro_arg(&self, id: MacroCallId) -> Option<Arc<(tt::Subtree, mbe::TokenMap)>>;\n     fn macro_def(&self, id: MacroDefId) -> Option<Arc<(TokenExpander, mbe::TokenMap)>>;\n-    fn parse_macro(\n-        &self,\n-        macro_file: MacroFile,\n-    ) -> Option<(Parse<SyntaxNode>, Arc<mbe::RevTokenMap>)>;\n+    fn parse_macro(&self, macro_file: MacroFile)\n+        -> Option<(Parse<SyntaxNode>, Arc<mbe::TokenMap>)>;\n     fn macro_expand(&self, macro_call: MacroCallId) -> Result<Arc<tt::Subtree>, String>;\n }\n \n@@ -136,7 +134,7 @@ pub(crate) fn parse_or_expand(db: &dyn AstDatabase, file_id: HirFileId) -> Optio\n pub(crate) fn parse_macro(\n     db: &dyn AstDatabase,\n     macro_file: MacroFile,\n-) -> Option<(Parse<SyntaxNode>, Arc<mbe::RevTokenMap>)> {\n+) -> Option<(Parse<SyntaxNode>, Arc<mbe::TokenMap>)> {\n     let _p = profile(\"parse_macro_query\");\n \n     let macro_call_id = macro_file.macro_call_id;"}, {"sha": "cfe7e6d15909665eafbb3aef6c93f84373fc9642", "filename": "crates/ra_hir_expand/src/lib.rs", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/rust-lang/rust/blob/2b6f1ab0e2303dce8e6e424514fd2a9e74566c89/crates%2Fra_hir_expand%2Fsrc%2Flib.rs", "raw_url": "https://github.com/rust-lang/rust/raw/2b6f1ab0e2303dce8e6e424514fd2a9e74566c89/crates%2Fra_hir_expand%2Fsrc%2Flib.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fra_hir_expand%2Fsrc%2Flib.rs?ref=2b6f1ab0e2303dce8e6e424514fd2a9e74566c89", "patch": "@@ -159,7 +159,7 @@ pub struct ExpansionInfo {\n \n     macro_def: Arc<(db::TokenExpander, mbe::TokenMap)>,\n     macro_arg: Arc<(tt::Subtree, mbe::TokenMap)>,\n-    exp_map: Arc<mbe::RevTokenMap>,\n+    exp_map: Arc<mbe::TokenMap>,\n }\n \n impl ExpansionInfo {\n@@ -186,7 +186,7 @@ impl ExpansionInfo {\n             mbe::Origin::Def => (&self.macro_def.1, &self.def),\n         };\n \n-        let range = token_map.relative_range_of(token_id)?;\n+        let range = token_map.range_by_token(token_id)?;\n         let token = algo::find_covering_element(\n             tt.ast.syntax(),\n             range + tt.ast.syntax().text_range().start(),"}, {"sha": "bbddebe67f4be9bc14b8e888d29202d7acb9177e", "filename": "crates/ra_mbe/src/lib.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/2b6f1ab0e2303dce8e6e424514fd2a9e74566c89/crates%2Fra_mbe%2Fsrc%2Flib.rs", "raw_url": "https://github.com/rust-lang/rust/raw/2b6f1ab0e2303dce8e6e424514fd2a9e74566c89/crates%2Fra_mbe%2Fsrc%2Flib.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fra_mbe%2Fsrc%2Flib.rs?ref=2b6f1ab0e2303dce8e6e424514fd2a9e74566c89", "patch": "@@ -31,7 +31,7 @@ pub enum ExpandError {\n }\n \n pub use crate::syntax_bridge::{\n-    ast_to_token_tree, syntax_node_to_token_tree, token_tree_to_syntax_node, RevTokenMap, TokenMap,\n+    ast_to_token_tree, syntax_node_to_token_tree, token_tree_to_syntax_node, TokenMap,\n };\n \n /// This struct contains AST for a single `macro_rules` definition. What might"}, {"sha": "d1c49c0b3f37439d496806c15c82fff8ee7d3c67", "filename": "crates/ra_mbe/src/syntax_bridge.rs", "status": "modified", "additions": 99, "deletions": 111, "changes": 210, "blob_url": "https://github.com/rust-lang/rust/blob/2b6f1ab0e2303dce8e6e424514fd2a9e74566c89/crates%2Fra_mbe%2Fsrc%2Fsyntax_bridge.rs", "raw_url": "https://github.com/rust-lang/rust/raw/2b6f1ab0e2303dce8e6e424514fd2a9e74566c89/crates%2Fra_mbe%2Fsrc%2Fsyntax_bridge.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fra_mbe%2Fsrc%2Fsyntax_bridge.rs?ref=2b6f1ab0e2303dce8e6e424514fd2a9e74566c89", "patch": "@@ -14,30 +14,22 @@ use crate::ExpandError;\n #[derive(Debug, PartialEq, Eq, Default)]\n pub struct TokenMap {\n     /// Maps `tt::TokenId` to the *relative* source range.\n-    tokens: Vec<TextRange>,\n-}\n-\n-/// Maps relative range of the expanded syntax node to `tt::TokenId`\n-#[derive(Debug, PartialEq, Eq, Default)]\n-pub struct RevTokenMap {\n-    ranges: Vec<(TextRange, tt::TokenId)>,\n+    entries: Vec<(tt::TokenId, TextRange)>,\n }\n \n /// Convert the syntax tree (what user has written) to a `TokenTree` (what macro\n /// will consume).\n pub fn ast_to_token_tree(ast: &ast::TokenTree) -> Option<(tt::Subtree, TokenMap)> {\n-    let mut token_map = TokenMap::default();\n-    let node = ast.syntax();\n-    let tt = convert_tt(&mut token_map, node.text_range().start(), node)?;\n-    Some((tt, token_map))\n+    syntax_node_to_token_tree(ast.syntax())\n }\n \n /// Convert the syntax node to a `TokenTree` (what macro\n /// will consume).\n pub fn syntax_node_to_token_tree(node: &SyntaxNode) -> Option<(tt::Subtree, TokenMap)> {\n-    let mut token_map = TokenMap::default();\n-    let tt = convert_tt(&mut token_map, node.text_range().start(), node)?;\n-    Some((tt, token_map))\n+    let global_offset = node.text_range().start();\n+    let mut c = Convertor { map: TokenMap::default(), global_offset, next_id: 0 };\n+    let subtree = c.go(node)?;\n+    Some((subtree, c.map))\n }\n \n // The following items are what `rustc` macro can be parsed into :\n@@ -55,7 +47,7 @@ pub fn syntax_node_to_token_tree(node: &SyntaxNode) -> Option<(tt::Subtree, Toke\n pub fn token_tree_to_syntax_node(\n     tt: &tt::Subtree,\n     fragment_kind: FragmentKind,\n-) -> Result<(Parse<SyntaxNode>, RevTokenMap), ExpandError> {\n+) -> Result<(Parse<SyntaxNode>, TokenMap), ExpandError> {\n     let tmp;\n     let tokens = match tt {\n         tt::Subtree { delimiter: tt::Delimiter::None, token_trees } => token_trees.as_slice(),\n@@ -78,35 +70,17 @@ pub fn token_tree_to_syntax_node(\n \n impl TokenMap {\n     pub fn token_by_range(&self, relative_range: TextRange) -> Option<tt::TokenId> {\n-        let (idx, _) =\n-            self.tokens.iter().enumerate().find(|(_, range)| **range == relative_range)?;\n-        Some(tt::TokenId(idx as u32))\n-    }\n-\n-    pub fn relative_range_of(&self, token_id: tt::TokenId) -> Option<TextRange> {\n-        let idx = token_id.0 as usize;\n-        self.tokens.get(idx).copied()\n-    }\n-\n-    fn alloc(&mut self, relative_range: TextRange) -> tt::TokenId {\n-        let id = self.tokens.len();\n-        self.tokens.push(relative_range);\n-        tt::TokenId(id as u32)\n-    }\n-}\n-\n-impl RevTokenMap {\n-    pub fn token_by_range(&self, relative_range: TextRange) -> Option<tt::TokenId> {\n-        self.ranges.iter().find(|&it| it.0 == relative_range).map(|it| it.1)\n+        let &(token_id, _) = self.entries.iter().find(|(_, range)| *range == relative_range)?;\n+        Some(token_id)\n     }\n \n     pub fn range_by_token(&self, token_id: tt::TokenId) -> Option<TextRange> {\n-        let &(r, _) = self.ranges.iter().find(|(_, tid)| *tid == token_id)?;\n-        Some(r)\n+        let &(_, range) = self.entries.iter().find(|(tid, _)| *tid == token_id)?;\n+        Some(range)\n     }\n \n-    fn add(&mut self, relative_range: TextRange, token_id: tt::TokenId) {\n-        self.ranges.push((relative_range, token_id.clone()))\n+    fn insert(&mut self, token_id: tt::TokenId, relative_range: TextRange) {\n+        self.entries.push((token_id, relative_range));\n     }\n }\n \n@@ -171,92 +145,106 @@ fn convert_doc_comment(token: &ra_syntax::SyntaxToken) -> Option<Vec<tt::TokenTr\n     }\n }\n \n-fn convert_tt(\n-    token_map: &mut TokenMap,\n+struct Convertor {\n+    map: TokenMap,\n     global_offset: TextUnit,\n-    tt: &SyntaxNode,\n-) -> Option<tt::Subtree> {\n-    // This tree is empty\n-    if tt.first_child_or_token().is_none() {\n-        return Some(tt::Subtree { token_trees: vec![], delimiter: tt::Delimiter::None });\n-    }\n+    next_id: u32,\n+}\n \n-    let first_child = tt.first_child_or_token()?;\n-    let last_child = tt.last_child_or_token()?;\n-    let (delimiter, skip_first) = match (first_child.kind(), last_child.kind()) {\n-        (T!['('], T![')']) => (tt::Delimiter::Parenthesis, true),\n-        (T!['{'], T!['}']) => (tt::Delimiter::Brace, true),\n-        (T!['['], T![']']) => (tt::Delimiter::Bracket, true),\n-        _ => (tt::Delimiter::None, false),\n-    };\n+impl Convertor {\n+    fn go(&mut self, tt: &SyntaxNode) -> Option<tt::Subtree> {\n+        // This tree is empty\n+        if tt.first_child_or_token().is_none() {\n+            return Some(tt::Subtree { token_trees: vec![], delimiter: tt::Delimiter::None });\n+        }\n \n-    let mut token_trees = Vec::new();\n-    let mut child_iter = tt.children_with_tokens().skip(skip_first as usize).peekable();\n+        let first_child = tt.first_child_or_token()?;\n+        let last_child = tt.last_child_or_token()?;\n+        let (delimiter, skip_first) = match (first_child.kind(), last_child.kind()) {\n+            (T!['('], T![')']) => (tt::Delimiter::Parenthesis, true),\n+            (T!['{'], T!['}']) => (tt::Delimiter::Brace, true),\n+            (T!['['], T![']']) => (tt::Delimiter::Bracket, true),\n+            _ => (tt::Delimiter::None, false),\n+        };\n \n-    while let Some(child) = child_iter.next() {\n-        if skip_first && (child == first_child || child == last_child) {\n-            continue;\n-        }\n+        let mut token_trees = Vec::new();\n+        let mut child_iter = tt.children_with_tokens().skip(skip_first as usize).peekable();\n \n-        match child {\n-            NodeOrToken::Token(token) => {\n-                if let Some(doc_tokens) = convert_doc_comment(&token) {\n-                    token_trees.extend(doc_tokens);\n-                } else if token.kind().is_trivia() {\n-                    continue;\n-                } else if token.kind().is_punct() {\n-                    assert!(token.text().len() == 1, \"Input ast::token punct must be single char.\");\n-                    let char = token.text().chars().next().unwrap();\n-\n-                    let spacing = match child_iter.peek() {\n-                        Some(NodeOrToken::Token(token)) => {\n-                            if token.kind().is_punct() {\n-                                tt::Spacing::Joint\n-                            } else {\n-                                tt::Spacing::Alone\n+        while let Some(child) = child_iter.next() {\n+            if skip_first && (child == first_child || child == last_child) {\n+                continue;\n+            }\n+\n+            match child {\n+                NodeOrToken::Token(token) => {\n+                    if let Some(doc_tokens) = convert_doc_comment(&token) {\n+                        token_trees.extend(doc_tokens);\n+                    } else if token.kind().is_trivia() {\n+                        continue;\n+                    } else if token.kind().is_punct() {\n+                        assert!(\n+                            token.text().len() == 1,\n+                            \"Input ast::token punct must be single char.\"\n+                        );\n+                        let char = token.text().chars().next().unwrap();\n+\n+                        let spacing = match child_iter.peek() {\n+                            Some(NodeOrToken::Token(token)) => {\n+                                if token.kind().is_punct() {\n+                                    tt::Spacing::Joint\n+                                } else {\n+                                    tt::Spacing::Alone\n+                                }\n                             }\n-                        }\n-                        _ => tt::Spacing::Alone,\n-                    };\n-\n-                    token_trees.push(tt::Leaf::from(tt::Punct { char, spacing }).into());\n-                } else {\n-                    let child: tt::TokenTree =\n-                        if token.kind() == T![true] || token.kind() == T![false] {\n-                            tt::Leaf::from(tt::Literal { text: token.text().clone() }).into()\n-                        } else if token.kind().is_keyword()\n-                            || token.kind() == IDENT\n-                            || token.kind() == LIFETIME\n-                        {\n-                            let relative_range = token.text_range() - global_offset;\n-                            let id = token_map.alloc(relative_range);\n-                            let text = token.text().clone();\n-                            tt::Leaf::from(tt::Ident { text, id }).into()\n-                        } else if token.kind().is_literal() {\n-                            tt::Leaf::from(tt::Literal { text: token.text().clone() }).into()\n-                        } else {\n-                            return None;\n+                            _ => tt::Spacing::Alone,\n                         };\n+\n+                        token_trees.push(tt::Leaf::from(tt::Punct { char, spacing }).into());\n+                    } else {\n+                        let child: tt::TokenTree =\n+                            if token.kind() == T![true] || token.kind() == T![false] {\n+                                tt::Leaf::from(tt::Literal { text: token.text().clone() }).into()\n+                            } else if token.kind().is_keyword()\n+                                || token.kind() == IDENT\n+                                || token.kind() == LIFETIME\n+                            {\n+                                let id = self.alloc(token.text_range());\n+                                let text = token.text().clone();\n+                                tt::Leaf::from(tt::Ident { text, id }).into()\n+                            } else if token.kind().is_literal() {\n+                                tt::Leaf::from(tt::Literal { text: token.text().clone() }).into()\n+                            } else {\n+                                return None;\n+                            };\n+                        token_trees.push(child);\n+                    }\n+                }\n+                NodeOrToken::Node(node) => {\n+                    let child = self.go(&node)?.into();\n                     token_trees.push(child);\n                 }\n-            }\n-            NodeOrToken::Node(node) => {\n-                let child = convert_tt(token_map, global_offset, &node)?.into();\n-                token_trees.push(child);\n-            }\n-        };\n+            };\n+        }\n+\n+        let res = tt::Subtree { delimiter, token_trees };\n+        Some(res)\n     }\n \n-    let res = tt::Subtree { delimiter, token_trees };\n-    Some(res)\n+    fn alloc(&mut self, absolute_range: TextRange) -> tt::TokenId {\n+        let relative_range = absolute_range - self.global_offset;\n+        let token_id = tt::TokenId(self.next_id);\n+        self.next_id += 1;\n+        self.map.insert(token_id, relative_range);\n+        token_id\n+    }\n }\n \n struct TtTreeSink<'a> {\n     buf: String,\n     cursor: Cursor<'a>,\n     text_pos: TextUnit,\n     inner: SyntaxTreeBuilder,\n-    range_map: RevTokenMap,\n+    token_map: TokenMap,\n \n     // Number of roots\n     // Use for detect ill-form tree which is not single root\n@@ -271,12 +259,12 @@ impl<'a> TtTreeSink<'a> {\n             text_pos: 0.into(),\n             inner: SyntaxTreeBuilder::default(),\n             roots: smallvec::SmallVec::new(),\n-            range_map: RevTokenMap::default(),\n+            token_map: TokenMap::default(),\n         }\n     }\n \n-    fn finish(self) -> (Parse<SyntaxNode>, RevTokenMap) {\n-        (self.inner.finish(), self.range_map)\n+    fn finish(self) -> (Parse<SyntaxNode>, TokenMap) {\n+        (self.inner.finish(), self.token_map)\n     }\n }\n \n@@ -312,7 +300,7 @@ impl<'a> TreeSink for TtTreeSink<'a> {\n                         if kind == IDENT {\n                             let range =\n                                 TextRange::offset_len(self.text_pos, TextUnit::of_str(&ident.text));\n-                            self.range_map.add(range, ident.id);\n+                            self.token_map.insert(ident.id, range);\n                         }\n                     }\n "}]}