{"sha": "e88aa39e39d154c9137eaa0730f0c778fcaaa58f", "node_id": "MDY6Q29tbWl0NzI0NzEyOmU4OGFhMzllMzlkMTU0YzkxMzdlYWEwNzMwZjBjNzc4ZmNhYWE1OGY=", "commit": {"author": {"name": "Yuki Okushi", "email": "huyuumi.dev@gmail.com", "date": "2019-11-10T00:27:10Z"}, "committer": {"name": "GitHub", "email": "noreply@github.com", "date": "2019-11-10T00:27:10Z"}, "message": "Rollup merge of #65719 - pitdicker:refactor_sync_once, r=Amanieu\n\nRefactor sync::Once\n\n`std::sync::Once` contains some tricky code to park and unpark waiting threads. [once_cell](https://github.com/matklad/once_cell) has very similar code copied from here. I tried to add more comments and refactor the code to make it more readable (at least in my opinion). My PR to `once_cell` was rejected, because it is an advantage to remain close to the implementation in std, and because I made a mess of the atomic orderings. So now a PR here, with similar changes to `std::sync::Once`!\n\nThe initial goal was to see if there is some way to detect reentrant initialization instead of deadlocking. No luck there yet, but you first have to understand and document the complexities of the existing code :smile:.\n\n*Maybe not this entire PR will be acceptable, but I hope at least some of the commits can be useful.*\n\nIndividual commits:\n\n#### Rename state to state_and_queue\nJust a more accurate description, although a bit wordy. It helped me on a first read through the code, where before `state` was use to encode a pointer in to nodes of a linked list.\n\n#### Simplify loop conditions in RUNNING and add comments\nIn the relevant loop there are two things to be careful about:\n- make sure not to enqueue the current thread only while still RUNNING, otherwise we will never be woken up (the status may have changed while trying to enqueue this thread).\n- pick up if another thread just replaced the head of the linked list.\n\nBecause the first check was part of the condition of the while loop, the rest of the parking code also had to live in that loop. It took me a while to get the subtlety here, and it should now be clearer.\n\nAlso call out that we really have to wait until signaled, otherwise we leave a dangling reference.\n\n#### Don't mutate waiter nodes\nPreviously while waking up other threads the managing thread would `take()` out the `Thread` struct and use that to unpark the other thread. It is just as easy to clone it, just 24 bytes. This way `Waiter.thread` does not need an `Option`, `Waiter.next` does not need to be a mutable pointer, and there is less data that needs to be synchronised by later atomic operations.\n\n#### Turn Finish into WaiterQueue\nIn my opinion these changes make it just a bit more clear what is going on with the thread parking stuff.\n\n#### Move thread parking to a seperate function\nMaybe controversial, but with this last commit all the thread parking stuff has a reasonably clean seperation from the state changes in `Once`. This is arguably the trickier part of `Once`, compared to the loop in `call_inner`. It may make it easier to reuse parts of this code (see https://github.com/rust-lang/rfcs/pull/2788#discussion_r336729695). Not sure if that ever becomes a reality though.\n\n#### Reduce the amount of comments in call_inner\nWith the changes from the previous commits, the code pretty much speaks for itself, and the amount of comments is hurting readability a bit.\n\n#### Use more precise atomic orderings\nNow the hard one. This is the one change that is not anything but a pure refactor or change of comments.\n\nI have a dislike for using `SeqCst` everywhere, because it hides what the atomics are supposed to do. the rationale was:\n> This cold path uses SeqCst consistently because the performance difference really does not matter there, and SeqCst minimizes the chances of something going wrong.\n\nBut in my opinion, having the proper orderings and some explanation helps to understand what is going on. My rationale for the used orderings (also included as comment):\n\nWhen running `Once` we deal with multiple atomics: `Once.state_and_queue` and an unknown number of `Waiter.signaled`.\n* `state_and_queue` is used (1) as a state flag, (2) for synchronizing the data that is the result of the `Once`, and (3) for synchronizing `Waiter` nodes.\n    - At the end of the `call_inner` function we have to make sure the result of the `Once` is acquired. So every load which can be the only one to load COMPLETED must have at least Acquire ordering, which means all three of them.\n    - `WaiterQueue::Drop` is the only place that may store COMPLETED, and must do so with Release ordering to make result available.\n    - `wait` inserts `Waiter` nodes as a pointer in `state_and_queue`, and needs to make the nodes available with Release ordering. The load in its `compare_and_swap` can be Relaxed because it only has to compare the atomic, not to read other data.\n    - `WaiterQueue::Drop` must see the `Waiter` nodes, so it must load `state_and_queue` with Acquire ordering.\n    - There is just one store where `state_and_queue` is used only as a state flag, without having to synchronize data: switching the state from INCOMPLETE to RUNNING in `call_inner`. This store can be Relaxed, but the read has to be Acquire because of the requirements mentioned above.\n* `Waiter.signaled` is both used as a flag, and to protect a field with interior mutability in `Waiter`. `Waiter.thread` is changed in `WaiterQueue::Drop` which then sets `signaled` with Release ordering. After `wait` loads `signaled` with Acquire and sees it is true, it needs to see the changes to drop the `Waiter` struct correctly.\n* There is one place where the two atomics `Once.state_and_queue` and `Waiter.signaled` come together, and might be reordered by the compiler or processor. Because both use Aquire ordering such a reordering is not allowed, so no need for SeqCst.\n\ncc @matklad", "tree": {"sha": "50f4c194dd7560930562f9956804398bea2b6cb2", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/50f4c194dd7560930562f9956804398bea2b6cb2"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/e88aa39e39d154c9137eaa0730f0c778fcaaa58f", "comment_count": 0, "verification": {"verified": true, "reason": "valid", "signature": "-----BEGIN PGP SIGNATURE-----\n\nwsBcBAABCAAQBQJdx1leCRBK7hj4Ov3rIwAAdHIIABFrziuYxWxhImlY4DviaidW\nXMb1f2Cn61Pa676OOTbHAiHMS13d9+e+4uINbkJKl23H8PfVxYL8bICsfU9kkKZw\nASI52f2VB61iRZWSyvWS2HfSS4HxMWlVP/iiWWJcGd4boHsohLXLBxcmg3Ym3wep\nln8W+aonD8JGhzkMEVeZPLxyG2usBYYkwxD/kbHn/sNe3eT7pSOmltEom70/2gKv\nD4NJ2+IZgNNd1I2YF380kxGisvF03V7tGGVodldfoA8Dy3NszvkBMIk0InwAO64w\nR62m10PZqDVzBh1JrdxEfcoYfItEBVP+LOrHIg+1T8qk9isyPECMfOcEzIx3Nfs=\n=tecX\n-----END PGP SIGNATURE-----\n", "payload": "tree 50f4c194dd7560930562f9956804398bea2b6cb2\nparent ac162c6abe34cdf965afc0389f6cefa79653c63b\nparent b05e200867ce633848d34d8a184bf45c7fa905a4\nauthor Yuki Okushi <huyuumi.dev@gmail.com> 1573345630 +0900\ncommitter GitHub <noreply@github.com> 1573345630 +0900\n\nRollup merge of #65719 - pitdicker:refactor_sync_once, r=Amanieu\n\nRefactor sync::Once\n\n`std::sync::Once` contains some tricky code to park and unpark waiting threads. [once_cell](https://github.com/matklad/once_cell) has very similar code copied from here. I tried to add more comments and refactor the code to make it more readable (at least in my opinion). My PR to `once_cell` was rejected, because it is an advantage to remain close to the implementation in std, and because I made a mess of the atomic orderings. So now a PR here, with similar changes to `std::sync::Once`!\n\nThe initial goal was to see if there is some way to detect reentrant initialization instead of deadlocking. No luck there yet, but you first have to understand and document the complexities of the existing code :smile:.\n\n*Maybe not this entire PR will be acceptable, but I hope at least some of the commits can be useful.*\n\nIndividual commits:\n\n#### Rename state to state_and_queue\nJust a more accurate description, although a bit wordy. It helped me on a first read through the code, where before `state` was use to encode a pointer in to nodes of a linked list.\n\n#### Simplify loop conditions in RUNNING and add comments\nIn the relevant loop there are two things to be careful about:\n- make sure not to enqueue the current thread only while still RUNNING, otherwise we will never be woken up (the status may have changed while trying to enqueue this thread).\n- pick up if another thread just replaced the head of the linked list.\n\nBecause the first check was part of the condition of the while loop, the rest of the parking code also had to live in that loop. It took me a while to get the subtlety here, and it should now be clearer.\n\nAlso call out that we really have to wait until signaled, otherwise we leave a dangling reference.\n\n#### Don't mutate waiter nodes\nPreviously while waking up other threads the managing thread would `take()` out the `Thread` struct and use that to unpark the other thread. It is just as easy to clone it, just 24 bytes. This way `Waiter.thread` does not need an `Option`, `Waiter.next` does not need to be a mutable pointer, and there is less data that needs to be synchronised by later atomic operations.\n\n#### Turn Finish into WaiterQueue\nIn my opinion these changes make it just a bit more clear what is going on with the thread parking stuff.\n\n#### Move thread parking to a seperate function\nMaybe controversial, but with this last commit all the thread parking stuff has a reasonably clean seperation from the state changes in `Once`. This is arguably the trickier part of `Once`, compared to the loop in `call_inner`. It may make it easier to reuse parts of this code (see https://github.com/rust-lang/rfcs/pull/2788#discussion_r336729695). Not sure if that ever becomes a reality though.\n\n#### Reduce the amount of comments in call_inner\nWith the changes from the previous commits, the code pretty much speaks for itself, and the amount of comments is hurting readability a bit.\n\n#### Use more precise atomic orderings\nNow the hard one. This is the one change that is not anything but a pure refactor or change of comments.\n\nI have a dislike for using `SeqCst` everywhere, because it hides what the atomics are supposed to do. the rationale was:\n> This cold path uses SeqCst consistently because the performance difference really does not matter there, and SeqCst minimizes the chances of something going wrong.\n\nBut in my opinion, having the proper orderings and some explanation helps to understand what is going on. My rationale for the used orderings (also included as comment):\n\nWhen running `Once` we deal with multiple atomics: `Once.state_and_queue` and an unknown number of `Waiter.signaled`.\n* `state_and_queue` is used (1) as a state flag, (2) for synchronizing the data that is the result of the `Once`, and (3) for synchronizing `Waiter` nodes.\n    - At the end of the `call_inner` function we have to make sure the result of the `Once` is acquired. So every load which can be the only one to load COMPLETED must have at least Acquire ordering, which means all three of them.\n    - `WaiterQueue::Drop` is the only place that may store COMPLETED, and must do so with Release ordering to make result available.\n    - `wait` inserts `Waiter` nodes as a pointer in `state_and_queue`, and needs to make the nodes available with Release ordering. The load in its `compare_and_swap` can be Relaxed because it only has to compare the atomic, not to read other data.\n    - `WaiterQueue::Drop` must see the `Waiter` nodes, so it must load `state_and_queue` with Acquire ordering.\n    - There is just one store where `state_and_queue` is used only as a state flag, without having to synchronize data: switching the state from INCOMPLETE to RUNNING in `call_inner`. This store can be Relaxed, but the read has to be Acquire because of the requirements mentioned above.\n* `Waiter.signaled` is both used as a flag, and to protect a field with interior mutability in `Waiter`. `Waiter.thread` is changed in `WaiterQueue::Drop` which then sets `signaled` with Release ordering. After `wait` loads `signaled` with Acquire and sees it is true, it needs to see the changes to drop the `Waiter` struct correctly.\n* There is one place where the two atomics `Once.state_and_queue` and `Waiter.signaled` come together, and might be reordered by the compiler or processor. Because both use Aquire ordering such a reordering is not allowed, so no need for SeqCst.\n\ncc @matklad\n"}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/e88aa39e39d154c9137eaa0730f0c778fcaaa58f", "html_url": "https://github.com/rust-lang/rust/commit/e88aa39e39d154c9137eaa0730f0c778fcaaa58f", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/e88aa39e39d154c9137eaa0730f0c778fcaaa58f/comments", "author": {"login": "JohnTitor", "id": 25030997, "node_id": "MDQ6VXNlcjI1MDMwOTk3", "avatar_url": "https://avatars.githubusercontent.com/u/25030997?v=4", "gravatar_id": "", "url": "https://api.github.com/users/JohnTitor", "html_url": "https://github.com/JohnTitor", "followers_url": "https://api.github.com/users/JohnTitor/followers", "following_url": "https://api.github.com/users/JohnTitor/following{/other_user}", "gists_url": "https://api.github.com/users/JohnTitor/gists{/gist_id}", "starred_url": "https://api.github.com/users/JohnTitor/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/JohnTitor/subscriptions", "organizations_url": "https://api.github.com/users/JohnTitor/orgs", "repos_url": "https://api.github.com/users/JohnTitor/repos", "events_url": "https://api.github.com/users/JohnTitor/events{/privacy}", "received_events_url": "https://api.github.com/users/JohnTitor/received_events", "type": "User", "site_admin": false}, "committer": {"login": "web-flow", "id": 19864447, "node_id": "MDQ6VXNlcjE5ODY0NDQ3", "avatar_url": "https://avatars.githubusercontent.com/u/19864447?v=4", "gravatar_id": "", "url": "https://api.github.com/users/web-flow", "html_url": "https://github.com/web-flow", "followers_url": "https://api.github.com/users/web-flow/followers", "following_url": "https://api.github.com/users/web-flow/following{/other_user}", "gists_url": "https://api.github.com/users/web-flow/gists{/gist_id}", "starred_url": "https://api.github.com/users/web-flow/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/web-flow/subscriptions", "organizations_url": "https://api.github.com/users/web-flow/orgs", "repos_url": "https://api.github.com/users/web-flow/repos", "events_url": "https://api.github.com/users/web-flow/events{/privacy}", "received_events_url": "https://api.github.com/users/web-flow/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "ac162c6abe34cdf965afc0389f6cefa79653c63b", "url": "https://api.github.com/repos/rust-lang/rust/commits/ac162c6abe34cdf965afc0389f6cefa79653c63b", "html_url": "https://github.com/rust-lang/rust/commit/ac162c6abe34cdf965afc0389f6cefa79653c63b"}, {"sha": "b05e200867ce633848d34d8a184bf45c7fa905a4", "url": "https://api.github.com/repos/rust-lang/rust/commits/b05e200867ce633848d34d8a184bf45c7fa905a4", "html_url": "https://github.com/rust-lang/rust/commit/b05e200867ce633848d34d8a184bf45c7fa905a4"}], "stats": {"total": 291, "additions": 166, "deletions": 125}, "files": [{"sha": "e8e395247f9c199dc5d2ca4e0e00c7a66c91a639", "filename": "src/libstd/sync/once.rs", "status": "modified", "additions": 166, "deletions": 125, "changes": 291, "blob_url": "https://github.com/rust-lang/rust/blob/e88aa39e39d154c9137eaa0730f0c778fcaaa58f/src%2Flibstd%2Fsync%2Fonce.rs", "raw_url": "https://github.com/rust-lang/rust/raw/e88aa39e39d154c9137eaa0730f0c778fcaaa58f/src%2Flibstd%2Fsync%2Fonce.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibstd%2Fsync%2Fonce.rs?ref=e88aa39e39d154c9137eaa0730f0c778fcaaa58f", "patch": "@@ -51,11 +51,43 @@\n //\n // You'll find a few more details in the implementation, but that's the gist of\n // it!\n-\n+//\n+// Atomic orderings:\n+// When running `Once` we deal with multiple atomics:\n+// `Once.state_and_queue` and an unknown number of `Waiter.signaled`.\n+// * `state_and_queue` is used (1) as a state flag, (2) for synchronizing the\n+//   result of the `Once`, and (3) for synchronizing `Waiter` nodes.\n+//     - At the end of the `call_inner` function we have to make sure the result\n+//       of the `Once` is acquired. So every load which can be the only one to\n+//       load COMPLETED must have at least Acquire ordering, which means all\n+//       three of them.\n+//     - `WaiterQueue::Drop` is the only place that may store COMPLETED, and\n+//       must do so with Release ordering to make the result available.\n+//     - `wait` inserts `Waiter` nodes as a pointer in `state_and_queue`, and\n+//       needs to make the nodes available with Release ordering. The load in\n+//       its `compare_and_swap` can be Relaxed because it only has to compare\n+//       the atomic, not to read other data.\n+//     - `WaiterQueue::Drop` must see the `Waiter` nodes, so it must load\n+//       `state_and_queue` with Acquire ordering.\n+//     - There is just one store where `state_and_queue` is used only as a\n+//       state flag, without having to synchronize data: switching the state\n+//       from INCOMPLETE to RUNNING in `call_inner`. This store can be Relaxed,\n+//       but the read has to be Acquire because of the requirements mentioned\n+//       above.\n+// * `Waiter.signaled` is both used as a flag, and to protect a field with\n+//   interior mutability in `Waiter`. `Waiter.thread` is changed in\n+//   `WaiterQueue::Drop` which then sets `signaled` with Release ordering.\n+//   After `wait` loads `signaled` with Acquire and sees it is true, it needs to\n+//   see the changes to drop the `Waiter` struct correctly.\n+// * There is one place where the two atomics `Once.state_and_queue` and\n+//   `Waiter.signaled` come together, and might be reordered by the compiler or\n+//   processor. Because both use Aquire ordering such a reordering is not\n+//   allowed, so no need for SeqCst.\n+\n+use crate::cell::Cell;\n use crate::fmt;\n use crate::marker;\n-use crate::ptr;\n-use crate::sync::atomic::{AtomicUsize, AtomicBool, Ordering};\n+use crate::sync::atomic::{AtomicBool, AtomicUsize, Ordering};\n use crate::thread::{self, Thread};\n \n /// A synchronization primitive which can be used to run a one-time global\n@@ -78,10 +110,10 @@ use crate::thread::{self, Thread};\n /// ```\n #[stable(feature = \"rust1\", since = \"1.0.0\")]\n pub struct Once {\n-    // This `state` word is actually an encoded version of just a pointer to a\n-    // `Waiter`, so we add the `PhantomData` appropriately.\n-    state: AtomicUsize,\n-    _marker: marker::PhantomData<*mut Waiter>,\n+    // `state_and_queue` is actually an a pointer to a `Waiter` with extra state\n+    // bits, so we add the `PhantomData` appropriately.\n+    state_and_queue: AtomicUsize,\n+    _marker: marker::PhantomData<*const Waiter>,\n }\n \n // The `PhantomData` of a raw pointer removes these two auto traits, but we\n@@ -117,12 +149,12 @@ pub struct OnceState {\n #[rustc_deprecated(\n     since = \"1.38.0\",\n     reason = \"the `new` function is now preferred\",\n-    suggestion = \"Once::new()\",\n+    suggestion = \"Once::new()\"\n )]\n pub const ONCE_INIT: Once = Once::new();\n \n-// Four states that a Once can be in, encoded into the lower bits of `state` in\n-// the Once structure.\n+// Four states that a Once can be in, encoded into the lower bits of\n+// `state_and_queue` in the Once structure.\n const INCOMPLETE: usize = 0x0;\n const POISONED: usize = 0x1;\n const RUNNING: usize = 0x2;\n@@ -132,28 +164,32 @@ const COMPLETE: usize = 0x3;\n // this is in the RUNNING state.\n const STATE_MASK: usize = 0x3;\n \n-// Representation of a node in the linked list of waiters in the RUNNING state.\n+// Representation of a node in the linked list of waiters, used while in the\n+// RUNNING state.\n+// Note: `Waiter` can't hold a mutable pointer to the next thread, because then\n+// `wait` would both hand out a mutable reference to its `Waiter` node, and keep\n+// a shared reference to check `signaled`. Instead we hold shared references and\n+// use interior mutability.\n+#[repr(align(4))] // Ensure the two lower bits are free to use as state bits.\n struct Waiter {\n-    thread: Option<Thread>,\n+    thread: Cell<Option<Thread>>,\n     signaled: AtomicBool,\n-    next: *mut Waiter,\n+    next: *const Waiter,\n }\n \n-// Helper struct used to clean up after a closure call with a `Drop`\n-// implementation to also run on panic.\n-struct Finish<'a> {\n-    panicked: bool,\n-    me: &'a Once,\n+// Head of a linked list of waiters.\n+// Every node is a struct on the stack of a waiting thread.\n+// Will wake up the waiters when it gets dropped, i.e. also on panic.\n+struct WaiterQueue<'a> {\n+    state_and_queue: &'a AtomicUsize,\n+    set_state_on_drop_to: usize,\n }\n \n impl Once {\n     /// Creates a new `Once` value.\n     #[stable(feature = \"once_new\", since = \"1.2.0\")]\n     pub const fn new() -> Once {\n-        Once {\n-            state: AtomicUsize::new(INCOMPLETE),\n-            _marker: marker::PhantomData,\n-        }\n+        Once { state_and_queue: AtomicUsize::new(INCOMPLETE), _marker: marker::PhantomData }\n     }\n \n     /// Performs an initialization routine once and only once. The given closure\n@@ -214,7 +250,10 @@ impl Once {\n     ///\n     /// [poison]: struct.Mutex.html#poisoning\n     #[stable(feature = \"rust1\", since = \"1.0.0\")]\n-    pub fn call_once<F>(&self, f: F) where F: FnOnce() {\n+    pub fn call_once<F>(&self, f: F)\n+    where\n+        F: FnOnce(),\n+    {\n         // Fast path check\n         if self.is_completed() {\n             return;\n@@ -271,16 +310,17 @@ impl Once {\n     /// INIT.call_once(|| {});\n     /// ```\n     #[unstable(feature = \"once_poison\", issue = \"33577\")]\n-    pub fn call_once_force<F>(&self, f: F) where F: FnOnce(&OnceState) {\n+    pub fn call_once_force<F>(&self, f: F)\n+    where\n+        F: FnOnce(&OnceState),\n+    {\n         // Fast path check\n         if self.is_completed() {\n             return;\n         }\n \n         let mut f = Some(f);\n-        self.call_inner(true, &mut |p| {\n-            f.take().unwrap()(&OnceState { poisoned: p })\n-        });\n+        self.call_inner(true, &mut |p| f.take().unwrap()(&OnceState { poisoned: p }));\n     }\n \n     /// Returns `true` if some `call_once` call has completed\n@@ -329,8 +369,8 @@ impl Once {\n         // An `Acquire` load is enough because that makes all the initialization\n         // operations visible to us, and, this being a fast path, weaker\n         // ordering helps with performance. This `Acquire` synchronizes with\n-        // `SeqCst` operations on the slow path.\n-        self.state.load(Ordering::Acquire) == COMPLETE\n+        // `Release` operations on the slow path.\n+        self.state_and_queue.load(Ordering::Acquire) == COMPLETE\n     }\n \n     // This is a non-generic function to reduce the monomorphization cost of\n@@ -345,124 +385,124 @@ impl Once {\n     // currently no way to take an `FnOnce` and call it via virtual dispatch\n     // without some allocation overhead.\n     #[cold]\n-    fn call_inner(&self,\n-                  ignore_poisoning: bool,\n-                  init: &mut dyn FnMut(bool)) {\n-\n-        // This cold path uses SeqCst consistently because the\n-        // performance difference really does not matter there, and\n-        // SeqCst minimizes the chances of something going wrong.\n-        let mut state = self.state.load(Ordering::SeqCst);\n-\n-        'outer: loop {\n-            match state {\n-                // If we're complete, then there's nothing to do, we just\n-                // jettison out as we shouldn't run the closure.\n-                COMPLETE => return,\n-\n-                // If we're poisoned and we're not in a mode to ignore\n-                // poisoning, then we panic here to propagate the poison.\n+    fn call_inner(&self, ignore_poisoning: bool, init: &mut dyn FnMut(bool)) {\n+        let mut state_and_queue = self.state_and_queue.load(Ordering::Acquire);\n+        loop {\n+            match state_and_queue {\n+                COMPLETE => break,\n                 POISONED if !ignore_poisoning => {\n+                    // Panic to propagate the poison.\n                     panic!(\"Once instance has previously been poisoned\");\n                 }\n-\n-                // Otherwise if we see a poisoned or otherwise incomplete state\n-                // we will attempt to move ourselves into the RUNNING state. If\n-                // we succeed, then the queue of waiters starts at null (all 0\n-                // bits).\n-                POISONED |\n-                INCOMPLETE => {\n-                    let old = self.state.compare_and_swap(state, RUNNING,\n-                                                          Ordering::SeqCst);\n-                    if old != state {\n-                        state = old;\n-                        continue\n+                POISONED | INCOMPLETE => {\n+                    // Try to register this thread as the one RUNNING.\n+                    let old = self.state_and_queue.compare_and_swap(\n+                        state_and_queue,\n+                        RUNNING,\n+                        Ordering::Acquire,\n+                    );\n+                    if old != state_and_queue {\n+                        state_and_queue = old;\n+                        continue;\n                     }\n-\n-                    // Run the initialization routine, letting it know if we're\n-                    // poisoned or not. The `Finish` struct is then dropped, and\n-                    // the `Drop` implementation here is responsible for waking\n-                    // up other waiters both in the normal return and panicking\n-                    // case.\n-                    let mut complete = Finish {\n-                        panicked: true,\n-                        me: self,\n+                    // `waiter_queue` will manage other waiting threads, and\n+                    // wake them up on drop.\n+                    let mut waiter_queue = WaiterQueue {\n+                        state_and_queue: &self.state_and_queue,\n+                        set_state_on_drop_to: POISONED,\n                     };\n-                    init(state == POISONED);\n-                    complete.panicked = false;\n-                    return\n+                    // Run the initialization function, letting it know if we're\n+                    // poisoned or not.\n+                    init(state_and_queue == POISONED);\n+                    waiter_queue.set_state_on_drop_to = COMPLETE;\n+                    break;\n                 }\n-\n-                // All other values we find should correspond to the RUNNING\n-                // state with an encoded waiter list in the more significant\n-                // bits. We attempt to enqueue ourselves by moving us to the\n-                // head of the list and bail out if we ever see a state that's\n-                // not RUNNING.\n                 _ => {\n-                    assert!(state & STATE_MASK == RUNNING);\n-                    let mut node = Waiter {\n-                        thread: Some(thread::current()),\n-                        signaled: AtomicBool::new(false),\n-                        next: ptr::null_mut(),\n-                    };\n-                    let me = &mut node as *mut Waiter as usize;\n-                    assert!(me & STATE_MASK == 0);\n-\n-                    while state & STATE_MASK == RUNNING {\n-                        node.next = (state & !STATE_MASK) as *mut Waiter;\n-                        let old = self.state.compare_and_swap(state,\n-                                                              me | RUNNING,\n-                                                              Ordering::SeqCst);\n-                        if old != state {\n-                            state = old;\n-                            continue\n-                        }\n-\n-                        // Once we've enqueued ourselves, wait in a loop.\n-                        // Afterwards reload the state and continue with what we\n-                        // were doing from before.\n-                        while !node.signaled.load(Ordering::SeqCst) {\n-                            thread::park();\n-                        }\n-                        state = self.state.load(Ordering::SeqCst);\n-                        continue 'outer\n-                    }\n+                    // All other values must be RUNNING with possibly a\n+                    // pointer to the waiter queue in the more significant bits.\n+                    assert!(state_and_queue & STATE_MASK == RUNNING);\n+                    wait(&self.state_and_queue, state_and_queue);\n+                    state_and_queue = self.state_and_queue.load(Ordering::Acquire);\n                 }\n             }\n         }\n     }\n }\n \n+fn wait(state_and_queue: &AtomicUsize, mut current_state: usize) {\n+    // Note: the following code was carefully written to avoid creating a\n+    // mutable reference to `node` that gets aliased.\n+    loop {\n+        // Don't queue this thread if the status is no longer running,\n+        // otherwise we will not be woken up.\n+        if current_state & STATE_MASK != RUNNING {\n+            return;\n+        }\n+\n+        // Create the node for our current thread.\n+        let node = Waiter {\n+            thread: Cell::new(Some(thread::current())),\n+            signaled: AtomicBool::new(false),\n+            next: (current_state & !STATE_MASK) as *const Waiter,\n+        };\n+        let me = &node as *const Waiter as usize;\n+\n+        // Try to slide in the node at the head of the linked list, making sure\n+        // that another thread didn't just replace the head of the linked list.\n+        let old = state_and_queue.compare_and_swap(current_state, me | RUNNING, Ordering::Release);\n+        if old != current_state {\n+            current_state = old;\n+            continue;\n+        }\n+\n+        // We have enqueued ourselves, now lets wait.\n+        // It is important not to return before being signaled, otherwise we\n+        // would drop our `Waiter` node and leave a hole in the linked list\n+        // (and a dangling reference). Guard against spurious wakeups by\n+        // reparking ourselves until we are signaled.\n+        while !node.signaled.load(Ordering::Acquire) {\n+            // If the managing thread happens to signal and unpark us before we\n+            // can park ourselves, the result could be this thread never gets\n+            // unparked. Luckily `park` comes with the guarantee that if it got\n+            // an `unpark` just before on an unparked thread is does not park.\n+            thread::park();\n+        }\n+        break;\n+    }\n+}\n+\n #[stable(feature = \"std_debug\", since = \"1.16.0\")]\n impl fmt::Debug for Once {\n     fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {\n         f.pad(\"Once { .. }\")\n     }\n }\n \n-impl Drop for Finish<'_> {\n+impl Drop for WaiterQueue<'_> {\n     fn drop(&mut self) {\n-        // Swap out our state with however we finished. We should only ever see\n-        // an old state which was RUNNING.\n-        let queue = if self.panicked {\n-            self.me.state.swap(POISONED, Ordering::SeqCst)\n-        } else {\n-            self.me.state.swap(COMPLETE, Ordering::SeqCst)\n-        };\n-        assert_eq!(queue & STATE_MASK, RUNNING);\n+        // Swap out our state with however we finished.\n+        let state_and_queue =\n+            self.state_and_queue.swap(self.set_state_on_drop_to, Ordering::AcqRel);\n+\n+        // We should only ever see an old state which was RUNNING.\n+        assert_eq!(state_and_queue & STATE_MASK, RUNNING);\n \n-        // Decode the RUNNING to a list of waiters, then walk that entire list\n-        // and wake them up. Note that it is crucial that after we store `true`\n-        // in the node it can be free'd! As a result we load the `thread` to\n-        // signal ahead of time and then unpark it after the store.\n+        // Walk the entire linked list of waiters and wake them up (in lifo\n+        // order, last to register is first to wake up).\n         unsafe {\n-            let mut queue = (queue & !STATE_MASK) as *mut Waiter;\n+            // Right after setting `node.signaled = true` the other thread may\n+            // free `node` if there happens to be has a spurious wakeup.\n+            // So we have to take out the `thread` field and copy the pointer to\n+            // `next` first.\n+            let mut queue = (state_and_queue & !STATE_MASK) as *const Waiter;\n             while !queue.is_null() {\n                 let next = (*queue).next;\n-                let thread = (*queue).thread.take().unwrap();\n-                (*queue).signaled.store(true, Ordering::SeqCst);\n-                thread.unpark();\n+                let thread = (*queue).thread.replace(None).unwrap();\n+                (*queue).signaled.store(true, Ordering::Release);\n+                // ^- FIXME (maybe): This is another case of issue #55005\n+                // `store()` has a potentially dangling ref to `signaled`.\n                 queue = next;\n+                thread.unpark();\n             }\n         }\n     }\n@@ -518,10 +558,10 @@ impl OnceState {\n \n #[cfg(all(test, not(target_os = \"emscripten\")))]\n mod tests {\n+    use super::Once;\n     use crate::panic;\n     use crate::sync::mpsc::channel;\n     use crate::thread;\n-    use super::Once;\n \n     #[test]\n     fn smoke_once() {\n@@ -541,8 +581,10 @@ mod tests {\n         let (tx, rx) = channel();\n         for _ in 0..10 {\n             let tx = tx.clone();\n-            thread::spawn(move|| {\n-                for _ in 0..4 { thread::yield_now() }\n+            thread::spawn(move || {\n+                for _ in 0..4 {\n+                    thread::yield_now()\n+                }\n                 unsafe {\n                     O.call_once(|| {\n                         assert!(!RUN);\n@@ -631,6 +673,5 @@ mod tests {\n \n         assert!(t1.join().is_ok());\n         assert!(t2.join().is_ok());\n-\n     }\n }"}]}