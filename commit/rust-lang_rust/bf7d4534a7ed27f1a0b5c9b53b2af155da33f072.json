{"sha": "bf7d4534a7ed27f1a0b5c9b53b2af155da33f072", "node_id": "MDY6Q29tbWl0NzI0NzEyOmJmN2Q0NTM0YTdlZDI3ZjFhMGI1YzliNTNiMmFmMTU1ZGEzM2YwNzI=", "commit": {"author": {"name": "Mark-Simulacrum", "email": "mark.simulacrum@gmail.com", "date": "2016-12-11T03:32:44Z"}, "committer": {"name": "Mark Simulacrum", "email": "mark.simulacrum@gmail.com", "date": "2016-12-21T03:01:40Z"}, "message": "Refactor Block into BlockAndBuilder", "tree": {"sha": "cf4ee3c10f1f5f2833d1be9b5ffaff7610658abc", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/cf4ee3c10f1f5f2833d1be9b5ffaff7610658abc"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/bf7d4534a7ed27f1a0b5c9b53b2af155da33f072", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/bf7d4534a7ed27f1a0b5c9b53b2af155da33f072", "html_url": "https://github.com/rust-lang/rust/commit/bf7d4534a7ed27f1a0b5c9b53b2af155da33f072", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/bf7d4534a7ed27f1a0b5c9b53b2af155da33f072/comments", "author": {"login": "Mark-Simulacrum", "id": 5047365, "node_id": "MDQ6VXNlcjUwNDczNjU=", "avatar_url": "https://avatars.githubusercontent.com/u/5047365?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Mark-Simulacrum", "html_url": "https://github.com/Mark-Simulacrum", "followers_url": "https://api.github.com/users/Mark-Simulacrum/followers", "following_url": "https://api.github.com/users/Mark-Simulacrum/following{/other_user}", "gists_url": "https://api.github.com/users/Mark-Simulacrum/gists{/gist_id}", "starred_url": "https://api.github.com/users/Mark-Simulacrum/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Mark-Simulacrum/subscriptions", "organizations_url": "https://api.github.com/users/Mark-Simulacrum/orgs", "repos_url": "https://api.github.com/users/Mark-Simulacrum/repos", "events_url": "https://api.github.com/users/Mark-Simulacrum/events{/privacy}", "received_events_url": "https://api.github.com/users/Mark-Simulacrum/received_events", "type": "User", "site_admin": false}, "committer": {"login": "Mark-Simulacrum", "id": 5047365, "node_id": "MDQ6VXNlcjUwNDczNjU=", "avatar_url": "https://avatars.githubusercontent.com/u/5047365?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Mark-Simulacrum", "html_url": "https://github.com/Mark-Simulacrum", "followers_url": "https://api.github.com/users/Mark-Simulacrum/followers", "following_url": "https://api.github.com/users/Mark-Simulacrum/following{/other_user}", "gists_url": "https://api.github.com/users/Mark-Simulacrum/gists{/gist_id}", "starred_url": "https://api.github.com/users/Mark-Simulacrum/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Mark-Simulacrum/subscriptions", "organizations_url": "https://api.github.com/users/Mark-Simulacrum/orgs", "repos_url": "https://api.github.com/users/Mark-Simulacrum/repos", "events_url": "https://api.github.com/users/Mark-Simulacrum/events{/privacy}", "received_events_url": "https://api.github.com/users/Mark-Simulacrum/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "164619a8cfe6d376d25bd3a6a9a5f2856c8de64d", "url": "https://api.github.com/repos/rust-lang/rust/commits/164619a8cfe6d376d25bd3a6a9a5f2856c8de64d", "html_url": "https://github.com/rust-lang/rust/commit/164619a8cfe6d376d25bd3a6a9a5f2856c8de64d"}], "stats": {"total": 2460, "additions": 1012, "deletions": 1448}, "files": [{"sha": "ce1b23c1ce9b6015ff16f37fac710ebbad93ed52", "filename": "src/librustc_trans/adt.rs", "status": "modified", "additions": 19, "deletions": 24, "changes": 43, "blob_url": "https://github.com/rust-lang/rust/blob/bf7d4534a7ed27f1a0b5c9b53b2af155da33f072/src%2Flibrustc_trans%2Fadt.rs", "raw_url": "https://github.com/rust-lang/rust/raw/bf7d4534a7ed27f1a0b5c9b53b2af155da33f072/src%2Flibrustc_trans%2Fadt.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_trans%2Fadt.rs?ref=bf7d4534a7ed27f1a0b5c9b53b2af155da33f072", "patch": "@@ -304,7 +304,7 @@ fn struct_llfields<'a, 'tcx>(cx: &CrateContext<'a, 'tcx>, fields: &Vec<Ty<'tcx>>\n \n /// Obtain a representation of the discriminant sufficient to translate\n /// destructuring; this may or may not involve the actual discriminant.\n-pub fn trans_switch<'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n+pub fn trans_switch<'blk, 'tcx>(bcx: &BlockAndBuilder<'blk, 'tcx>,\n                                 t: Ty<'tcx>,\n                                 scrutinee: ValueRef,\n                                 range_assert: bool)\n@@ -331,7 +331,7 @@ pub fn is_discr_signed<'tcx>(l: &layout::Layout) -> bool {\n }\n \n /// Obtain the actual discriminant of a value.\n-pub fn trans_get_discr<'blk, 'tcx>(bcx: Block<'blk, 'tcx>, t: Ty<'tcx>,\n+pub fn trans_get_discr<'blk, 'tcx>(bcx: &BlockAndBuilder<'blk, 'tcx>, t: Ty<'tcx>,\n                                    scrutinee: ValueRef, cast_to: Option<Type>,\n                                    range_assert: bool)\n     -> ValueRef {\n@@ -371,8 +371,12 @@ pub fn trans_get_discr<'blk, 'tcx>(bcx: Block<'blk, 'tcx>, t: Ty<'tcx>,\n     }\n }\n \n-fn struct_wrapped_nullable_bitdiscr(bcx: Block, nndiscr: u64, discrfield: &layout::FieldPath,\n-                                    scrutinee: ValueRef) -> ValueRef {\n+fn struct_wrapped_nullable_bitdiscr(\n+    bcx: &BlockAndBuilder,\n+    nndiscr: u64,\n+    discrfield: &layout::FieldPath,\n+    scrutinee: ValueRef\n+) -> ValueRef {\n     let llptrptr = GEPi(bcx, scrutinee,\n         &discrfield.iter().map(|f| *f as usize).collect::<Vec<_>>()[..]);\n     let llptr = Load(bcx, llptrptr);\n@@ -381,7 +385,7 @@ fn struct_wrapped_nullable_bitdiscr(bcx: Block, nndiscr: u64, discrfield: &layou\n }\n \n /// Helper for cases where the discriminant is simply loaded.\n-fn load_discr(bcx: Block, ity: layout::Integer, ptr: ValueRef, min: u64, max: u64,\n+fn load_discr(bcx: &BlockAndBuilder, ity: layout::Integer, ptr: ValueRef, min: u64, max: u64,\n               range_assert: bool)\n     -> ValueRef {\n     let llty = Type::from_integer(bcx.ccx(), ity);\n@@ -409,7 +413,7 @@ fn load_discr(bcx: Block, ity: layout::Integer, ptr: ValueRef, min: u64, max: u6\n /// discriminant-like value returned by `trans_switch`.\n ///\n /// This should ideally be less tightly tied to `_match`.\n-pub fn trans_case<'blk, 'tcx>(bcx: Block<'blk, 'tcx>, t: Ty<'tcx>, value: Disr)\n+pub fn trans_case<'blk, 'tcx>(bcx: &BlockAndBuilder<'blk, 'tcx>, t: Ty<'tcx>, value: Disr)\n                               -> ValueRef {\n     let l = bcx.ccx().layout_of(t);\n     match *l {\n@@ -430,7 +434,7 @@ pub fn trans_case<'blk, 'tcx>(bcx: Block<'blk, 'tcx>, t: Ty<'tcx>, value: Disr)\n \n /// Set the discriminant for a new value of the given case of the given\n /// representation.\n-pub fn trans_set_discr<'blk, 'tcx>(bcx: Block<'blk, 'tcx>, t: Ty<'tcx>,\n+pub fn trans_set_discr<'blk, 'tcx>(bcx: &BlockAndBuilder<'blk, 'tcx>, t: Ty<'tcx>,\n                                    val: ValueRef, to: Disr) {\n     let l = bcx.ccx().layout_of(t);\n     match *l {\n@@ -461,12 +465,11 @@ pub fn trans_set_discr<'blk, 'tcx>(bcx: Block<'blk, 'tcx>, t: Ty<'tcx>,\n                     // Issue #34427: As workaround for LLVM bug on\n                     // ARM, use memset of 0 on whole struct rather\n                     // than storing null to single target field.\n-                    let b = B(bcx);\n-                    let llptr = b.pointercast(val, Type::i8(b.ccx).ptr_to());\n-                    let fill_byte = C_u8(b.ccx, 0);\n-                    let size = C_uint(b.ccx, nonnull.stride().bytes());\n-                    let align = C_i32(b.ccx, nonnull.align.abi() as i32);\n-                    base::call_memset(&b, llptr, fill_byte, size, align, false);\n+                    let llptr = bcx.pointercast(val, Type::i8(bcx.ccx()).ptr_to());\n+                    let fill_byte = C_u8(bcx.ccx(), 0);\n+                    let size = C_uint(bcx.ccx(), nonnull.stride().bytes());\n+                    let align = C_i32(bcx.ccx(), nonnull.align.abi() as i32);\n+                    base::call_memset(bcx, llptr, fill_byte, size, align, false);\n                 } else {\n                     let path = discrfield.iter().map(|&i| i as usize).collect::<Vec<_>>();\n                     let llptrptr = GEPi(bcx, val, &path[..]);\n@@ -479,7 +482,7 @@ pub fn trans_set_discr<'blk, 'tcx>(bcx: Block<'blk, 'tcx>, t: Ty<'tcx>,\n     }\n }\n \n-fn target_sets_discr_via_memset<'blk, 'tcx>(bcx: Block<'blk, 'tcx>) -> bool {\n+fn target_sets_discr_via_memset<'blk, 'tcx>(bcx: &BlockAndBuilder<'blk, 'tcx>) -> bool {\n     bcx.sess().target.target.arch == \"arm\" || bcx.sess().target.target.arch == \"aarch64\"\n }\n \n@@ -492,9 +495,9 @@ fn assert_discr_in_range(min: Disr, max: Disr, discr: Disr) {\n }\n \n /// Access a field, at a point when the value's case is known.\n-pub fn trans_field_ptr<'blk, 'tcx>(bcx: Block<'blk, 'tcx>, t: Ty<'tcx>,\n+pub fn trans_field_ptr<'blk, 'tcx>(bcx: &BlockAndBuilder<'blk, 'tcx>, t: Ty<'tcx>,\n                                    val: MaybeSizedValue, discr: Disr, ix: usize) -> ValueRef {\n-    trans_field_ptr_builder(&bcx.build(), t, val, discr, ix)\n+    trans_field_ptr_builder(bcx, t, val, discr, ix)\n }\n \n /// Access a field, at a point when the value's case is known.\n@@ -530,7 +533,6 @@ pub fn trans_field_ptr_builder<'blk, 'tcx>(bcx: &BlockAndBuilder<'blk, 'tcx>,\n         layout::UntaggedUnion { .. } => {\n             let fields = compute_fields(bcx.ccx(), t, 0, false);\n             let ty = type_of::in_memory_type_of(bcx.ccx(), fields[ix]);\n-            if bcx.is_unreachable() { return C_undef(ty.ptr_to()); }\n             bcx.pointercast(val.value, ty.ptr_to())\n         }\n         layout::RawNullablePointer { nndiscr, .. } |\n@@ -540,17 +542,13 @@ pub fn trans_field_ptr_builder<'blk, 'tcx>(bcx: &BlockAndBuilder<'blk, 'tcx>,\n             // (e.d., Result of Either with (), as one side.)\n             let ty = type_of::type_of(bcx.ccx(), nullfields[ix]);\n             assert_eq!(machine::llsize_of_alloc(bcx.ccx(), ty), 0);\n-            // The contents of memory at this pointer can't matter, but use\n-            // the value that's \"reasonable\" in case of pointer comparison.\n-            if bcx.is_unreachable() { return C_undef(ty.ptr_to()); }\n             bcx.pointercast(val.value, ty.ptr_to())\n         }\n         layout::RawNullablePointer { nndiscr, .. } => {\n             let nnty = compute_fields(bcx.ccx(), t, nndiscr as usize, false)[0];\n             assert_eq!(ix, 0);\n             assert_eq!(discr.0, nndiscr);\n             let ty = type_of::type_of(bcx.ccx(), nnty);\n-            if bcx.is_unreachable() { return C_undef(ty.ptr_to()); }\n             bcx.pointercast(val.value, ty.ptr_to())\n         }\n         layout::StructWrappedNullablePointer { ref nonnull, nndiscr, .. } => {\n@@ -569,9 +567,6 @@ fn struct_field_ptr<'blk, 'tcx>(bcx: &BlockAndBuilder<'blk, 'tcx>,\n     let fty = fields[ix];\n     let ccx = bcx.ccx();\n     let ll_fty = type_of::in_memory_type_of(bcx.ccx(), fty);\n-    if bcx.is_unreachable() {\n-        return C_undef(ll_fty.ptr_to());\n-    }\n \n     let ptr_val = if needs_cast {\n         let fields = st.field_index_by_increasing_offset().map(|i| {"}, {"sha": "1e672e9d109554f9134c310ea6a9eb7b70cfb53f", "filename": "src/librustc_trans/asm.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/bf7d4534a7ed27f1a0b5c9b53b2af155da33f072/src%2Flibrustc_trans%2Fasm.rs", "raw_url": "https://github.com/rust-lang/rust/raw/bf7d4534a7ed27f1a0b5c9b53b2af155da33f072/src%2Flibrustc_trans%2Fasm.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_trans%2Fasm.rs?ref=bf7d4534a7ed27f1a0b5c9b53b2af155da33f072", "patch": "@@ -25,7 +25,7 @@ use syntax::ast::AsmDialect;\n use libc::{c_uint, c_char};\n \n // Take an inline assembly expression and splat it out via LLVM\n-pub fn trans_inline_asm<'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n+pub fn trans_inline_asm<'blk, 'tcx>(bcx: &BlockAndBuilder<'blk, 'tcx>,\n                                     ia: &hir::InlineAsm,\n                                     outputs: Vec<(ValueRef, Ty<'tcx>)>,\n                                     mut inputs: Vec<ValueRef>) {"}, {"sha": "83b40849e2761449bb8f02735b85fc0f4e09daaa", "filename": "src/librustc_trans/base.rs", "status": "modified", "additions": 81, "deletions": 99, "changes": 180, "blob_url": "https://github.com/rust-lang/rust/blob/bf7d4534a7ed27f1a0b5c9b53b2af155da33f072/src%2Flibrustc_trans%2Fbase.rs", "raw_url": "https://github.com/rust-lang/rust/raw/bf7d4534a7ed27f1a0b5c9b53b2af155da33f072/src%2Flibrustc_trans%2Fbase.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_trans%2Fbase.rs?ref=bf7d4534a7ed27f1a0b5c9b53b2af155da33f072", "patch": "@@ -54,11 +54,10 @@ use attributes;\n use build::*;\n use builder::{Builder, noname};\n use callee::{Callee};\n-use common::{Block, C_bool, C_bytes_in_context, C_i32, C_uint};\n+use common::{BlockAndBuilder, C_bool, C_bytes_in_context, C_i32, C_uint};\n use collector::{self, TransItemCollectionMode};\n use common::{C_null, C_struct_in_context, C_u64, C_u8, C_undef};\n use common::{CrateContext, FunctionContext};\n-use common::{Result};\n use common::{fulfill_obligation};\n use common::{type_is_zero_size, val_ty};\n use common;\n@@ -174,11 +173,11 @@ impl<'a, 'tcx> Drop for StatRecorder<'a, 'tcx> {\n     }\n }\n \n-pub fn get_meta(bcx: Block, fat_ptr: ValueRef) -> ValueRef {\n+pub fn get_meta(bcx: &BlockAndBuilder, fat_ptr: ValueRef) -> ValueRef {\n     StructGEP(bcx, fat_ptr, abi::FAT_PTR_EXTRA)\n }\n \n-pub fn get_dataptr(bcx: Block, fat_ptr: ValueRef) -> ValueRef {\n+pub fn get_dataptr(bcx: &BlockAndBuilder, fat_ptr: ValueRef) -> ValueRef {\n     StructGEP(bcx, fat_ptr, abi::FAT_PTR_ADDR)\n }\n \n@@ -190,7 +189,9 @@ pub fn get_dataptr_builder(b: &Builder, fat_ptr: ValueRef) -> ValueRef {\n     b.struct_gep(fat_ptr, abi::FAT_PTR_ADDR)\n }\n \n-fn require_alloc_fn<'blk, 'tcx>(bcx: Block<'blk, 'tcx>, info_ty: Ty<'tcx>, it: LangItem) -> DefId {\n+fn require_alloc_fn<'blk, 'tcx>(\n+    bcx: &BlockAndBuilder<'blk, 'tcx>, info_ty: Ty<'tcx>, it: LangItem\n+) -> DefId {\n     match bcx.tcx().lang_items.require(it) {\n         Ok(id) => id,\n         Err(s) => {\n@@ -202,21 +203,19 @@ fn require_alloc_fn<'blk, 'tcx>(bcx: Block<'blk, 'tcx>, info_ty: Ty<'tcx>, it: L\n // The following malloc_raw_dyn* functions allocate a box to contain\n // a given type, but with a potentially dynamic size.\n \n-pub fn malloc_raw_dyn<'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n+pub fn malloc_raw_dyn<'blk, 'tcx>(bcx: &BlockAndBuilder<'blk, 'tcx>,\n                                   llty_ptr: Type,\n                                   info_ty: Ty<'tcx>,\n                                   size: ValueRef,\n                                   align: ValueRef,\n                                   debug_loc: DebugLoc)\n-                                  -> Result<'blk, 'tcx> {\n+                                  -> ValueRef {\n     let _icx = push_ctxt(\"malloc_raw_exchange\");\n \n     // Allocate space:\n     let def_id = require_alloc_fn(bcx, info_ty, ExchangeMallocFnLangItem);\n-    let r = Callee::def(bcx.ccx(), def_id, bcx.tcx().intern_substs(&[]))\n-        .call(bcx, debug_loc, &[size, align], None);\n-\n-    Result::new(r.bcx, PointerCast(r.bcx, r.val, llty_ptr))\n+    let r = Callee::def(bcx.ccx(), def_id, bcx.tcx().intern_substs(&[])).reify(bcx.ccx());\n+    PointerCast(bcx, Call(bcx, r, &[size, align], debug_loc), llty_ptr)\n }\n \n \n@@ -254,7 +253,7 @@ pub fn bin_op_to_fcmp_predicate(op: hir::BinOp_) -> llvm::RealPredicate {\n     }\n }\n \n-pub fn compare_simd_types<'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n+pub fn compare_simd_types<'blk, 'tcx>(bcx: &BlockAndBuilder<'blk, 'tcx>,\n                                       lhs: ValueRef,\n                                       rhs: ValueRef,\n                                       t: Ty<'tcx>,\n@@ -311,7 +310,7 @@ pub fn unsized_info<'ccx, 'tcx>(ccx: &CrateContext<'ccx, 'tcx>,\n }\n \n /// Coerce `src` to `dst_ty`. `src_ty` must be a thin pointer.\n-pub fn unsize_thin_ptr<'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n+pub fn unsize_thin_ptr<'blk, 'tcx>(bcx: &BlockAndBuilder<'blk, 'tcx>,\n                                    src: ValueRef,\n                                    src_ty: Ty<'tcx>,\n                                    dst_ty: Ty<'tcx>)\n@@ -336,7 +335,7 @@ pub fn unsize_thin_ptr<'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n \n /// Coerce `src`, which is a reference to a value of type `src_ty`,\n /// to a value of type `dst_ty` and store the result in `dst`\n-pub fn coerce_unsized_into<'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n+pub fn coerce_unsized_into<'blk, 'tcx>(bcx: &BlockAndBuilder<'blk, 'tcx>,\n                                        src: ValueRef,\n                                        src_ty: Ty<'tcx>,\n                                        dst: ValueRef,\n@@ -415,7 +414,7 @@ pub fn custom_coerce_unsize_info<'scx, 'tcx>(scx: &SharedCrateContext<'scx, 'tcx\n     }\n }\n \n-pub fn cast_shift_expr_rhs(cx: Block, op: hir::BinOp_, lhs: ValueRef, rhs: ValueRef) -> ValueRef {\n+pub fn cast_shift_expr_rhs(cx: &BlockAndBuilder, op: hir::BinOp_, lhs: ValueRef, rhs: ValueRef) -> ValueRef {\n     cast_shift_rhs(op, lhs, rhs, |a, b| Trunc(cx, a, b), |a, b| ZExt(cx, a, b))\n }\n \n@@ -462,38 +461,38 @@ fn cast_shift_rhs<F, G>(op: hir::BinOp_,\n     }\n }\n \n-pub fn invoke<'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n+pub fn invoke<'blk, 'tcx>(bcx: BlockAndBuilder<'blk, 'tcx>,\n                           llfn: ValueRef,\n                           llargs: &[ValueRef],\n                           debug_loc: DebugLoc)\n-                          -> (ValueRef, Block<'blk, 'tcx>) {\n+                          -> (ValueRef, BlockAndBuilder<'blk, 'tcx>) {\n     let _icx = push_ctxt(\"invoke_\");\n-    if bcx.unreachable.get() {\n+    if bcx.is_unreachable() {\n         return (C_null(Type::i8(bcx.ccx())), bcx);\n     }\n \n-    if need_invoke(bcx) {\n-        debug!(\"invoking {:?} at {:?}\", Value(llfn), bcx.llbb);\n+    if need_invoke(&bcx) {\n+        debug!(\"invoking {:?} at {:?}\", Value(llfn), bcx.llbb());\n         for &llarg in llargs {\n             debug!(\"arg: {:?}\", Value(llarg));\n         }\n-        let normal_bcx = bcx.fcx.new_block(\"normal-return\");\n-        let landing_pad = bcx.fcx.get_landing_pad();\n+        let normal_bcx = bcx.fcx().new_block(\"normal-return\");\n+        let landing_pad = bcx.fcx().get_landing_pad();\n \n-        let llresult = Invoke(bcx,\n+        let llresult = Invoke(&bcx,\n                               llfn,\n                               &llargs[..],\n                               normal_bcx.llbb,\n                               landing_pad,\n                               debug_loc);\n-        return (llresult, normal_bcx);\n+        return (llresult, normal_bcx.build());\n     } else {\n-        debug!(\"calling {:?} at {:?}\", Value(llfn), bcx.llbb);\n+        debug!(\"calling {:?} at {:?}\", Value(llfn), bcx.llbb());\n         for &llarg in llargs {\n             debug!(\"arg: {:?}\", Value(llarg));\n         }\n \n-        let llresult = Call(bcx, llfn, &llargs[..], debug_loc);\n+        let llresult = Call(&bcx, llfn, &llargs[..], debug_loc);\n         return (llresult, bcx);\n     }\n }\n@@ -507,15 +506,11 @@ pub fn wants_msvc_seh(sess: &Session) -> bool {\n     sess.target.target.options.is_like_msvc\n }\n \n-pub fn avoid_invoke(bcx: Block) -> bool {\n-    bcx.sess().no_landing_pads() || bcx.lpad().is_some()\n-}\n-\n-pub fn need_invoke(bcx: Block) -> bool {\n-    if avoid_invoke(bcx) {\n+fn need_invoke(bcx: &BlockAndBuilder) -> bool {\n+    if bcx.sess().no_landing_pads() || bcx.lpad().is_some() {\n         false\n     } else {\n-        bcx.fcx.needs_invoke()\n+        bcx.fcx().needs_invoke()\n     }\n }\n \n@@ -527,11 +522,8 @@ pub fn call_assume<'a, 'tcx>(b: &Builder<'a, 'tcx>, val: ValueRef) {\n /// Helper for loading values from memory. Does the necessary conversion if the in-memory type\n /// differs from the type used for SSA values. Also handles various special cases where the type\n /// gives us better information about what we are loading.\n-pub fn load_ty<'blk, 'tcx>(cx: Block<'blk, 'tcx>, ptr: ValueRef, t: Ty<'tcx>) -> ValueRef {\n-    if cx.unreachable.get() {\n-        return C_undef(type_of::type_of(cx.ccx(), t));\n-    }\n-    load_ty_builder(&B(cx), ptr, t)\n+pub fn load_ty<'blk, 'tcx>(cx: &BlockAndBuilder<'blk, 'tcx>, ptr: ValueRef, t: Ty<'tcx>) -> ValueRef {\n+    load_ty_builder(cx, ptr, t)\n }\n \n pub fn load_ty_builder<'a, 'tcx>(b: &Builder<'a, 'tcx>, ptr: ValueRef, t: Ty<'tcx>) -> ValueRef {\n@@ -569,8 +561,8 @@ pub fn load_ty_builder<'a, 'tcx>(b: &Builder<'a, 'tcx>, ptr: ValueRef, t: Ty<'tc\n \n /// Helper for storing values in memory. Does the necessary conversion if the in-memory type\n /// differs from the type used for SSA values.\n-pub fn store_ty<'blk, 'tcx>(cx: Block<'blk, 'tcx>, v: ValueRef, dst: ValueRef, t: Ty<'tcx>) {\n-    if cx.unreachable.get() {\n+pub fn store_ty<'blk, 'tcx>(cx: &BlockAndBuilder<'blk, 'tcx>, v: ValueRef, dst: ValueRef, t: Ty<'tcx>) {\n+    if cx.is_unreachable() {\n         return;\n     }\n \n@@ -585,7 +577,7 @@ pub fn store_ty<'blk, 'tcx>(cx: Block<'blk, 'tcx>, v: ValueRef, dst: ValueRef, t\n     }\n }\n \n-pub fn store_fat_ptr<'blk, 'tcx>(cx: Block<'blk, 'tcx>,\n+pub fn store_fat_ptr<'blk, 'tcx>(cx: &BlockAndBuilder<'blk, 'tcx>,\n                                  data: ValueRef,\n                                  extra: ValueRef,\n                                  dst: ValueRef,\n@@ -595,18 +587,18 @@ pub fn store_fat_ptr<'blk, 'tcx>(cx: Block<'blk, 'tcx>,\n     Store(cx, extra, get_meta(cx, dst));\n }\n \n-pub fn load_fat_ptr<'blk, 'tcx>(cx: Block<'blk, 'tcx>,\n+pub fn load_fat_ptr<'blk, 'tcx>(cx: &BlockAndBuilder<'blk, 'tcx>,\n                                 src: ValueRef,\n                                 ty: Ty<'tcx>)\n                                 -> (ValueRef, ValueRef)\n {\n-    if cx.unreachable.get() {\n+    if cx.is_unreachable() {\n         // FIXME: remove me\n         return (Load(cx, get_dataptr(cx, src)),\n                 Load(cx, get_meta(cx, src)));\n     }\n \n-    load_fat_ptr_builder(&B(cx), src, ty)\n+    load_fat_ptr_builder(cx, src, ty)\n }\n \n pub fn load_fat_ptr_builder<'a, 'tcx>(\n@@ -629,39 +621,39 @@ pub fn load_fat_ptr_builder<'a, 'tcx>(\n     (ptr, meta)\n }\n \n-pub fn from_immediate(bcx: Block, val: ValueRef) -> ValueRef {\n+pub fn from_immediate(bcx: &BlockAndBuilder, val: ValueRef) -> ValueRef {\n     if val_ty(val) == Type::i1(bcx.ccx()) {\n         ZExt(bcx, val, Type::i8(bcx.ccx()))\n     } else {\n         val\n     }\n }\n \n-pub fn to_immediate(bcx: Block, val: ValueRef, ty: Ty) -> ValueRef {\n+pub fn to_immediate(bcx: &BlockAndBuilder, val: ValueRef, ty: Ty) -> ValueRef {\n     if ty.is_bool() {\n         Trunc(bcx, val, Type::i1(bcx.ccx()))\n     } else {\n         val\n     }\n }\n \n-pub fn with_cond<'blk, 'tcx, F>(bcx: Block<'blk, 'tcx>, val: ValueRef, f: F) -> Block<'blk, 'tcx>\n-    where F: FnOnce(Block<'blk, 'tcx>) -> Block<'blk, 'tcx>\n+pub fn with_cond<'blk, 'tcx, F>(\n+    bcx: BlockAndBuilder<'blk, 'tcx>, val: ValueRef, f: F\n+) -> BlockAndBuilder<'blk, 'tcx>\n+    where F: FnOnce(BlockAndBuilder<'blk, 'tcx>) -> BlockAndBuilder<'blk, 'tcx>\n {\n     let _icx = push_ctxt(\"with_cond\");\n \n-    if bcx.unreachable.get() || common::const_to_opt_uint(val) == Some(0) {\n+    if bcx.is_unreachable() || common::const_to_opt_uint(val) == Some(0) {\n         return bcx;\n     }\n \n-    let fcx = bcx.fcx;\n-    let next_cx = fcx.new_block(\"next\");\n-    let cond_cx = fcx.new_block(\"cond\");\n-    CondBr(bcx, val, cond_cx.llbb, next_cx.llbb, DebugLoc::None);\n+    let fcx = bcx.fcx();\n+    let next_cx = fcx.new_block(\"next\").build();\n+    let cond_cx = fcx.new_block(\"cond\").build();\n+    CondBr(&bcx, val, cond_cx.llbb(), next_cx.llbb(), DebugLoc::None);\n     let after_cx = f(cond_cx);\n-    if !after_cx.terminated.get() {\n-        Br(after_cx, next_cx.llbb, DebugLoc::None);\n-    }\n+    Br(&after_cx, next_cx.llbb(), DebugLoc::None);\n     next_cx\n }\n \n@@ -711,26 +703,25 @@ impl Lifetime {\n     }\n }\n \n-pub fn call_lifetime_start(bcx: Block, ptr: ValueRef) {\n-    if !bcx.unreachable.get() {\n-        Lifetime::Start.call(&bcx.build(), ptr);\n+pub fn call_lifetime_start(bcx: &BlockAndBuilder, ptr: ValueRef) {\n+    if !bcx.is_unreachable() {\n+        Lifetime::Start.call(bcx, ptr);\n     }\n }\n \n-pub fn call_lifetime_end(bcx: Block, ptr: ValueRef) {\n-    if !bcx.unreachable.get() {\n-        Lifetime::End.call(&bcx.build(), ptr);\n+pub fn call_lifetime_end(bcx: &BlockAndBuilder, ptr: ValueRef) {\n+    if !bcx.is_unreachable() {\n+        Lifetime::End.call(bcx, ptr);\n     }\n }\n \n // Generates code for resumption of unwind at the end of a landing pad.\n-pub fn trans_unwind_resume(bcx: Block, lpval: ValueRef) {\n+pub fn trans_unwind_resume(bcx: &BlockAndBuilder, lpval: ValueRef) {\n     if !bcx.sess().target.target.options.custom_unwind_resume {\n-        Resume(bcx, lpval);\n+        bcx.resume(lpval);\n     } else {\n         let exc_ptr = ExtractValue(bcx, lpval, 0);\n-        bcx.fcx.eh_unwind_resume()\n-            .call(bcx, DebugLoc::None, &[exc_ptr], None);\n+        Call(bcx, bcx.fcx().eh_unwind_resume().reify(bcx.ccx()), &[exc_ptr], DebugLoc::None);\n     }\n }\n \n@@ -752,19 +743,19 @@ pub fn call_memcpy<'bcx, 'tcx>(b: &Builder<'bcx, 'tcx>,\n     b.call(memcpy, &[dst_ptr, src_ptr, size, align, volatile], None);\n }\n \n-pub fn memcpy_ty<'blk, 'tcx>(bcx: Block<'blk, 'tcx>, dst: ValueRef, src: ValueRef, t: Ty<'tcx>) {\n+pub fn memcpy_ty<'blk, 'tcx>(bcx: &BlockAndBuilder<'blk, 'tcx>, dst: ValueRef, src: ValueRef, t: Ty<'tcx>) {\n     let _icx = push_ctxt(\"memcpy_ty\");\n     let ccx = bcx.ccx();\n \n-    if type_is_zero_size(ccx, t) || bcx.unreachable.get() {\n+    if type_is_zero_size(ccx, t) || bcx.is_unreachable() {\n         return;\n     }\n \n     if t.is_structural() {\n         let llty = type_of::type_of(ccx, t);\n         let llsz = llsize_of(ccx, llty);\n         let llalign = type_of::align_of(ccx, t);\n-        call_memcpy(&B(bcx), dst, src, llsz, llalign as u32);\n+        call_memcpy(bcx, dst, src, llsz, llalign as u32);\n     } else if common::type_is_fat_ptr(bcx.tcx(), t) {\n         let (data, extra) = load_fat_ptr(bcx, src, t);\n         store_fat_ptr(bcx, data, extra, dst, t);\n@@ -773,13 +764,13 @@ pub fn memcpy_ty<'blk, 'tcx>(bcx: Block<'blk, 'tcx>, dst: ValueRef, src: ValueRe\n     }\n }\n \n-pub fn init_zero_mem<'blk, 'tcx>(cx: Block<'blk, 'tcx>, llptr: ValueRef, t: Ty<'tcx>) {\n-    if cx.unreachable.get() {\n+pub fn init_zero_mem<'blk, 'tcx>(cx: &BlockAndBuilder<'blk, 'tcx>, llptr: ValueRef, t: Ty<'tcx>) {\n+    if cx.is_unreachable() {\n         return;\n     }\n     let _icx = push_ctxt(\"init_zero_mem\");\n     let bcx = cx;\n-    memfill(&B(bcx), llptr, t, 0);\n+    memfill(bcx, llptr, t, 0);\n }\n \n // Always use this function instead of storing a constant byte to the memory\n@@ -812,24 +803,17 @@ pub fn call_memset<'bcx, 'tcx>(b: &Builder<'bcx, 'tcx>,\n     b.call(llintrinsicfn, &[ptr, fill_byte, size, align, volatile], None);\n }\n \n-pub fn alloc_ty<'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n+pub fn alloc_ty<'blk, 'tcx>(bcx: &BlockAndBuilder<'blk, 'tcx>,\n                             ty: Ty<'tcx>,\n                             name: &str) -> ValueRef {\n     assert!(!ty.has_param_types());\n     alloca(bcx, type_of::type_of(bcx.ccx(), ty), name)\n }\n \n-pub fn alloca(cx: Block, ty: Type, name: &str) -> ValueRef {\n+pub fn alloca(cx: &BlockAndBuilder, ty: Type, name: &str) -> ValueRef {\n     let _icx = push_ctxt(\"alloca\");\n-    if cx.unreachable.get() {\n-        unsafe {\n-            return llvm::LLVMGetUndef(ty.ptr_to().to_ref());\n-        }\n-    }\n-    DebugLoc::None.apply(cx.fcx);\n-    let result = Alloca(cx, ty, name);\n-    debug!(\"alloca({:?}) = {:?}\", name, result);\n-    result\n+    DebugLoc::None.apply(cx.fcx());\n+    Alloca(cx, ty, name)\n }\n \n impl<'blk, 'tcx> FunctionContext<'blk, 'tcx> {\n@@ -894,14 +878,14 @@ impl<'blk, 'tcx> FunctionContext<'blk, 'tcx> {\n \n     /// Performs setup on a newly created function, creating the entry\n     /// scope block and allocating space for the return pointer.\n-    pub fn init(&'blk self, skip_retptr: bool) -> Block<'blk, 'tcx> {\n-        let entry_bcx = self.new_block(\"entry-block\");\n+    pub fn init(&'blk self, skip_retptr: bool) -> BlockAndBuilder<'blk, 'tcx> {\n+        let entry_bcx = self.new_block(\"entry-block\").build();\n \n         // Use a dummy instruction as the insertion point for all allocas.\n         // This is later removed in FunctionContext::cleanup.\n         self.alloca_insert_pt.set(Some(unsafe {\n-            Load(entry_bcx, C_null(Type::i8p(self.ccx)));\n-            llvm::LLVMGetFirstInstruction(entry_bcx.llbb)\n+            Load(&entry_bcx, C_null(Type::i8p(self.ccx)));\n+            llvm::LLVMGetFirstInstruction(entry_bcx.llbb())\n         }));\n \n         if !self.fn_ty.ret.is_ignore() && !skip_retptr {\n@@ -929,7 +913,7 @@ impl<'blk, 'tcx> FunctionContext<'blk, 'tcx> {\n \n     /// Ties up the llstaticallocas -> llloadenv -> lltop edges,\n     /// and builds the return block.\n-    pub fn finish(&'blk self, ret_cx: Block<'blk, 'tcx>,\n+    pub fn finish(&'blk self, ret_cx: &BlockAndBuilder<'blk, 'tcx>,\n                   ret_debug_loc: DebugLoc) {\n         let _icx = push_ctxt(\"FunctionContext::finish\");\n \n@@ -940,10 +924,9 @@ impl<'blk, 'tcx> FunctionContext<'blk, 'tcx> {\n     }\n \n     // Builds the return block for a function.\n-    pub fn build_return_block(&self, ret_cx: Block<'blk, 'tcx>,\n+    pub fn build_return_block(&self, ret_cx: &BlockAndBuilder<'blk, 'tcx>,\n                               ret_debug_location: DebugLoc) {\n-        if self.llretslotptr.get().is_none() ||\n-           ret_cx.unreachable.get() ||\n+        if self.llretslotptr.get().is_none() || ret_cx.is_unreachable() ||\n            self.fn_ty.ret.is_indirect() {\n             return RetVoid(ret_cx, ret_debug_location);\n         }\n@@ -978,7 +961,7 @@ impl<'blk, 'tcx> FunctionContext<'blk, 'tcx> {\n                 assert_eq!(cast_ty, None);\n                 let llsz = llsize_of(self.ccx, self.fn_ty.ret.ty);\n                 let llalign = llalign_of_min(self.ccx, self.fn_ty.ret.ty);\n-                call_memcpy(&B(ret_cx), get_param(self.llfn, 0),\n+                call_memcpy(&ret_cx, get_param(self.llfn, 0),\n                             retslot, llsz, llalign as u32);\n                 RetVoid(ret_cx, ret_debug_location)\n             }\n@@ -1080,23 +1063,22 @@ pub fn trans_ctor_shim<'a, 'tcx>(ccx: &CrateContext<'a, 'tcx>,\n         let mut llarg_idx = fcx.fn_ty.ret.is_indirect() as usize;\n         let mut arg_idx = 0;\n         for (i, arg_ty) in sig.inputs().iter().enumerate() {\n-            let lldestptr = adt::trans_field_ptr(bcx, sig.output(), dest_val, Disr::from(disr), i);\n+            let lldestptr = adt::trans_field_ptr(&bcx, sig.output(), dest_val, Disr::from(disr), i);\n             let arg = &fcx.fn_ty.args[arg_idx];\n             arg_idx += 1;\n-            let b = &bcx.build();\n             if common::type_is_fat_ptr(bcx.tcx(), arg_ty) {\n                 let meta = &fcx.fn_ty.args[arg_idx];\n                 arg_idx += 1;\n-                arg.store_fn_arg(b, &mut llarg_idx, get_dataptr(bcx, lldestptr));\n-                meta.store_fn_arg(b, &mut llarg_idx, get_meta(bcx, lldestptr));\n+                arg.store_fn_arg(&bcx, &mut llarg_idx, get_dataptr(&bcx, lldestptr));\n+                meta.store_fn_arg(&bcx, &mut llarg_idx, get_meta(&bcx, lldestptr));\n             } else {\n-                arg.store_fn_arg(b, &mut llarg_idx, lldestptr);\n+                arg.store_fn_arg(&bcx, &mut llarg_idx, lldestptr);\n             }\n         }\n-        adt::trans_set_discr(bcx, sig.output(), dest, disr);\n+        adt::trans_set_discr(&bcx, sig.output(), dest, disr);\n     }\n \n-    fcx.finish(bcx, DebugLoc::None);\n+    fcx.finish(&bcx, DebugLoc::None);\n }\n \n pub fn llvm_linkage_by_name(name: &str) -> Option<Linkage> {"}, {"sha": "bea42950c5512d091570d22efac328dcafff3e50", "filename": "src/librustc_trans/build.rs", "status": "modified", "additions": 473, "deletions": 884, "changes": 1357, "blob_url": "https://github.com/rust-lang/rust/blob/bf7d4534a7ed27f1a0b5c9b53b2af155da33f072/src%2Flibrustc_trans%2Fbuild.rs", "raw_url": "https://github.com/rust-lang/rust/raw/bf7d4534a7ed27f1a0b5c9b53b2af155da33f072/src%2Flibrustc_trans%2Fbuild.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_trans%2Fbuild.rs?ref=bf7d4534a7ed27f1a0b5c9b53b2af155da33f072", "patch": "@@ -18,30 +18,12 @@ use llvm::{ValueRef, BasicBlockRef};\n use common::*;\n use syntax_pos::Span;\n \n-use builder::Builder;\n use type_::Type;\n use value::Value;\n use debuginfo::DebugLoc;\n \n use libc::{c_uint, c_char};\n \n-pub fn terminate(cx: Block, _: &str) {\n-    debug!(\"terminate({})\", cx.to_str());\n-    cx.terminated.set(true);\n-}\n-\n-pub fn check_not_terminated(cx: Block) {\n-    if cx.terminated.get() {\n-        bug!(\"already terminated!\");\n-    }\n-}\n-\n-pub fn B<'blk, 'tcx>(cx: Block<'blk, 'tcx>) -> Builder<'blk, 'tcx> {\n-    let b = cx.fcx.ccx.builder();\n-    b.position_at_end(cx.llbb);\n-    b\n-}\n-\n // The difference between a block being unreachable and being terminated is\n // somewhat obscure, and has to do with error checking. When a block is\n // terminated, we're saying that trying to add any further statements in the\n@@ -50,546 +32,389 @@ pub fn B<'blk, 'tcx>(cx: Block<'blk, 'tcx>) -> Builder<'blk, 'tcx> {\n // for (panic/break/return statements, call to diverging functions, etc), and\n // further instructions to the block should simply be ignored.\n \n-pub fn RetVoid(cx: Block, debug_loc: DebugLoc) {\n-    if cx.unreachable.get() {\n-        return;\n-    }\n-    check_not_terminated(cx);\n-    terminate(cx, \"RetVoid\");\n-    debug_loc.apply(cx.fcx);\n-    B(cx).ret_void();\n+pub fn RetVoid(cx: &BlockAndBuilder, debug_loc: DebugLoc) {\n+    cx.terminate();\n+    debug_loc.apply(cx.fcx());\n+    cx.ret_void();\n }\n \n-pub fn Ret(cx: Block, v: ValueRef, debug_loc: DebugLoc) {\n-    if cx.unreachable.get() {\n-        return;\n-    }\n-    check_not_terminated(cx);\n-    terminate(cx, \"Ret\");\n-    debug_loc.apply(cx.fcx);\n-    B(cx).ret(v);\n+pub fn Ret(cx: &BlockAndBuilder, v: ValueRef, debug_loc: DebugLoc) {\n+    cx.terminate();\n+    debug_loc.apply(cx.fcx());\n+    cx.ret(v);\n }\n \n-pub fn AggregateRet(cx: Block,\n-                    ret_vals: &[ValueRef],\n-                    debug_loc: DebugLoc) {\n-    if cx.unreachable.get() {\n-        return;\n-    }\n-    check_not_terminated(cx);\n-    terminate(cx, \"AggregateRet\");\n-    debug_loc.apply(cx.fcx);\n-    B(cx).aggregate_ret(ret_vals);\n+pub fn AggregateRet(cx: &BlockAndBuilder,\n+    ret_vals: &[ValueRef],\n+    debug_loc: DebugLoc) {\n+    cx.terminate();\n+    debug_loc.apply(cx.fcx());\n+    cx.aggregate_ret(ret_vals);\n }\n \n-pub fn Br(cx: Block, dest: BasicBlockRef, debug_loc: DebugLoc) {\n-    if cx.unreachable.get() {\n-        return;\n-    }\n-    check_not_terminated(cx);\n-    terminate(cx, \"Br\");\n-    debug_loc.apply(cx.fcx);\n-    B(cx).br(dest);\n-}\n-\n-pub fn CondBr(cx: Block,\n-              if_: ValueRef,\n-              then: BasicBlockRef,\n-              else_: BasicBlockRef,\n-              debug_loc: DebugLoc) {\n-    if cx.unreachable.get() {\n-        return;\n-    }\n-    check_not_terminated(cx);\n-    terminate(cx, \"CondBr\");\n-    debug_loc.apply(cx.fcx);\n-    B(cx).cond_br(if_, then, else_);\n+pub fn Br(cx: &BlockAndBuilder, dest: BasicBlockRef, debug_loc: DebugLoc) {\n+    cx.terminate();\n+    debug_loc.apply(cx.fcx());\n+    cx.br(dest);\n }\n \n-pub fn Switch(cx: Block, v: ValueRef, else_: BasicBlockRef, num_cases: usize)\n-    -> ValueRef {\n-    if cx.unreachable.get() { return _Undef(v); }\n-    check_not_terminated(cx);\n-    terminate(cx, \"Switch\");\n-    B(cx).switch(v, else_, num_cases)\n+pub fn CondBr(cx: &BlockAndBuilder,\n+    if_: ValueRef,\n+    then: BasicBlockRef,\n+    else_: BasicBlockRef,\n+    debug_loc: DebugLoc) {\n+    cx.terminate();\n+    debug_loc.apply(cx.fcx());\n+    cx.cond_br(if_, then, else_);\n }\n \n+pub fn Switch(cx: &BlockAndBuilder, v: ValueRef, else_: BasicBlockRef, num_cases: usize)\n+    -> ValueRef {\n+        cx.terminate();\n+        cx.switch(v, else_, num_cases)\n+    }\n+\n pub fn AddCase(s: ValueRef, on_val: ValueRef, dest: BasicBlockRef) {\n     unsafe {\n         if llvm::LLVMIsUndef(s) == llvm::True { return; }\n         llvm::LLVMAddCase(s, on_val, dest);\n     }\n }\n \n-pub fn IndirectBr(cx: Block,\n-                  addr: ValueRef,\n-                  num_dests: usize,\n-                  debug_loc: DebugLoc) {\n-    if cx.unreachable.get() {\n-        return;\n-    }\n-    check_not_terminated(cx);\n-    terminate(cx, \"IndirectBr\");\n-    debug_loc.apply(cx.fcx);\n-    B(cx).indirect_br(addr, num_dests);\n-}\n-\n-pub fn Invoke(cx: Block,\n-              fn_: ValueRef,\n-              args: &[ValueRef],\n-              then: BasicBlockRef,\n-              catch: BasicBlockRef,\n-              debug_loc: DebugLoc)\n-              -> ValueRef {\n-    if cx.unreachable.get() {\n-        return C_null(Type::i8(cx.ccx()));\n-    }\n-    check_not_terminated(cx);\n-    terminate(cx, \"Invoke\");\n-    debug!(\"Invoke({:?} with arguments ({}))\",\n-           Value(fn_),\n-           args.iter().map(|a| {\n-                format!(\"{:?}\", Value(*a))\n-           }).collect::<Vec<String>>().join(\", \"));\n-    debug_loc.apply(cx.fcx);\n-    let bundle = cx.lpad().and_then(|b| b.bundle());\n-    B(cx).invoke(fn_, args, then, catch, bundle)\n-}\n-\n-pub fn Unreachable(cx: Block) {\n-    if cx.unreachable.get() {\n-        return\n-    }\n-    cx.unreachable.set(true);\n-    if !cx.terminated.get() {\n-        B(cx).unreachable();\n-    }\n+pub fn IndirectBr(cx: &BlockAndBuilder,\n+    addr: ValueRef,\n+    num_dests: usize,\n+    debug_loc: DebugLoc) {\n+    cx.terminate();\n+    debug_loc.apply(cx.fcx());\n+    cx.indirect_br(addr, num_dests);\n }\n \n-pub fn _Undef(val: ValueRef) -> ValueRef {\n-    unsafe {\n-        return llvm::LLVMGetUndef(val_ty(val).to_ref());\n-    }\n+pub fn Invoke(cx: &BlockAndBuilder,\n+    fn_: ValueRef,\n+    args: &[ValueRef],\n+    then: BasicBlockRef,\n+    catch: BasicBlockRef,\n+    debug_loc: DebugLoc)\n+    -> ValueRef {\n+    cx.terminate();\n+    debug!(\"Invoke({:?} with arguments ({}))\",\n+    Value(fn_),\n+    args.iter().map(|a| {\n+        format!(\"{:?}\", Value(*a))\n+    }).collect::<Vec<String>>().join(\", \"));\n+    debug_loc.apply(cx.fcx());\n+    let bundle = cx.lpad().and_then(|b| b.bundle());\n+    cx.invoke(fn_, args, then, catch, bundle)\n }\n \n /* Arithmetic */\n-pub fn Add(cx: Block,\n-           lhs: ValueRef,\n-           rhs: ValueRef,\n-           debug_loc: DebugLoc)\n-           -> ValueRef {\n-    if cx.unreachable.get() {\n-        return _Undef(lhs);\n+pub fn Add(cx: &BlockAndBuilder,\n+    lhs: ValueRef,\n+    rhs: ValueRef,\n+    debug_loc: DebugLoc)\n+    -> ValueRef {\n+        debug_loc.apply(cx.fcx());\n+        cx.add(lhs, rhs)\n     }\n-    debug_loc.apply(cx.fcx);\n-    B(cx).add(lhs, rhs)\n-}\n \n-pub fn NSWAdd(cx: Block,\n-              lhs: ValueRef,\n-              rhs: ValueRef,\n-              debug_loc: DebugLoc)\n-              -> ValueRef {\n-    if cx.unreachable.get() {\n-        return _Undef(lhs);\n+pub fn NSWAdd(cx: &BlockAndBuilder,\n+    lhs: ValueRef,\n+    rhs: ValueRef,\n+    debug_loc: DebugLoc)\n+    -> ValueRef {\n+        debug_loc.apply(cx.fcx());\n+        cx.nswadd(lhs, rhs)\n     }\n-    debug_loc.apply(cx.fcx);\n-    B(cx).nswadd(lhs, rhs)\n-}\n \n-pub fn NUWAdd(cx: Block,\n-              lhs: ValueRef,\n-              rhs: ValueRef,\n-              debug_loc: DebugLoc)\n-              -> ValueRef {\n-    if cx.unreachable.get() {\n-        return _Undef(lhs);\n+pub fn NUWAdd(cx: &BlockAndBuilder,\n+    lhs: ValueRef,\n+    rhs: ValueRef,\n+    debug_loc: DebugLoc)\n+    -> ValueRef {\n+        debug_loc.apply(cx.fcx());\n+        cx.nuwadd(lhs, rhs)\n     }\n-    debug_loc.apply(cx.fcx);\n-    B(cx).nuwadd(lhs, rhs)\n-}\n \n-pub fn FAdd(cx: Block,\n-            lhs: ValueRef,\n-            rhs: ValueRef,\n-            debug_loc: DebugLoc)\n-            -> ValueRef {\n-    if cx.unreachable.get() {\n-        return _Undef(lhs);\n+pub fn FAdd(cx: &BlockAndBuilder,\n+    lhs: ValueRef,\n+    rhs: ValueRef,\n+    debug_loc: DebugLoc)\n+    -> ValueRef {\n+        debug_loc.apply(cx.fcx());\n+        cx.fadd(lhs, rhs)\n     }\n-    debug_loc.apply(cx.fcx);\n-    B(cx).fadd(lhs, rhs)\n-}\n \n-pub fn FAddFast(cx: Block,\n-                lhs: ValueRef,\n-                rhs: ValueRef,\n-                debug_loc: DebugLoc)\n-            -> ValueRef {\n-    if cx.unreachable.get() {\n-        return _Undef(lhs);\n+pub fn FAddFast(cx: &BlockAndBuilder,\n+    lhs: ValueRef,\n+    rhs: ValueRef,\n+    debug_loc: DebugLoc)\n+    -> ValueRef {\n+        debug_loc.apply(cx.fcx());\n+        cx.fadd_fast(lhs, rhs)\n     }\n-    debug_loc.apply(cx.fcx);\n-    B(cx).fadd_fast(lhs, rhs)\n-}\n \n-pub fn Sub(cx: Block,\n-           lhs: ValueRef,\n-           rhs: ValueRef,\n-           debug_loc: DebugLoc)\n-           -> ValueRef {\n-    if cx.unreachable.get() {\n-        return _Undef(lhs);\n+pub fn Sub(cx: &BlockAndBuilder,\n+    lhs: ValueRef,\n+    rhs: ValueRef,\n+    debug_loc: DebugLoc)\n+    -> ValueRef {\n+        debug_loc.apply(cx.fcx());\n+        cx.sub(lhs, rhs)\n     }\n-    debug_loc.apply(cx.fcx);\n-    B(cx).sub(lhs, rhs)\n-}\n \n-pub fn NSWSub(cx: Block,\n-              lhs: ValueRef,\n-              rhs: ValueRef,\n-              debug_loc: DebugLoc)\n-              -> ValueRef {\n-    if cx.unreachable.get() {\n-        return _Undef(lhs);\n+pub fn NSWSub(cx: &BlockAndBuilder,\n+    lhs: ValueRef,\n+    rhs: ValueRef,\n+    debug_loc: DebugLoc)\n+    -> ValueRef {\n+        debug_loc.apply(cx.fcx());\n+        cx.nswsub(lhs, rhs)\n     }\n-    debug_loc.apply(cx.fcx);\n-    B(cx).nswsub(lhs, rhs)\n-}\n \n-pub fn NUWSub(cx: Block,\n-              lhs: ValueRef,\n-              rhs: ValueRef,\n-              debug_loc: DebugLoc)\n-              -> ValueRef {\n-    if cx.unreachable.get() {\n-        return _Undef(lhs);\n+pub fn NUWSub(cx: &BlockAndBuilder,\n+    lhs: ValueRef,\n+    rhs: ValueRef,\n+    debug_loc: DebugLoc)\n+    -> ValueRef {\n+        debug_loc.apply(cx.fcx());\n+        cx.nuwsub(lhs, rhs)\n     }\n-    debug_loc.apply(cx.fcx);\n-    B(cx).nuwsub(lhs, rhs)\n-}\n \n-pub fn FSub(cx: Block,\n-            lhs: ValueRef,\n-            rhs: ValueRef,\n-            debug_loc: DebugLoc)\n-            -> ValueRef {\n-    if cx.unreachable.get() {\n-        return _Undef(lhs);\n+pub fn FSub(cx: &BlockAndBuilder,\n+    lhs: ValueRef,\n+    rhs: ValueRef,\n+    debug_loc: DebugLoc)\n+    -> ValueRef {\n+        debug_loc.apply(cx.fcx());\n+        cx.fsub(lhs, rhs)\n     }\n-    debug_loc.apply(cx.fcx);\n-    B(cx).fsub(lhs, rhs)\n-}\n \n-pub fn FSubFast(cx: Block,\n-                lhs: ValueRef,\n-                rhs: ValueRef,\n-                debug_loc: DebugLoc)\n-            -> ValueRef {\n-    if cx.unreachable.get() {\n-        return _Undef(lhs);\n+pub fn FSubFast(cx: &BlockAndBuilder,\n+    lhs: ValueRef,\n+    rhs: ValueRef,\n+    debug_loc: DebugLoc)\n+    -> ValueRef {\n+        debug_loc.apply(cx.fcx());\n+        cx.fsub_fast(lhs, rhs)\n     }\n-    debug_loc.apply(cx.fcx);\n-    B(cx).fsub_fast(lhs, rhs)\n-}\n \n-pub fn Mul(cx: Block,\n-           lhs: ValueRef,\n-           rhs: ValueRef,\n-           debug_loc: DebugLoc)\n-           -> ValueRef {\n-    if cx.unreachable.get() {\n-        return _Undef(lhs);\n+pub fn Mul(cx: &BlockAndBuilder,\n+    lhs: ValueRef,\n+    rhs: ValueRef,\n+    debug_loc: DebugLoc)\n+    -> ValueRef {\n+        debug_loc.apply(cx.fcx());\n+        cx.mul(lhs, rhs)\n     }\n-    debug_loc.apply(cx.fcx);\n-    B(cx).mul(lhs, rhs)\n-}\n \n-pub fn NSWMul(cx: Block,\n-              lhs: ValueRef,\n-              rhs: ValueRef,\n-              debug_loc: DebugLoc)\n-              -> ValueRef {\n-    if cx.unreachable.get() {\n-        return _Undef(lhs);\n+pub fn NSWMul(cx: &BlockAndBuilder,\n+    lhs: ValueRef,\n+    rhs: ValueRef,\n+    debug_loc: DebugLoc)\n+    -> ValueRef {\n+        debug_loc.apply(cx.fcx());\n+        cx.nswmul(lhs, rhs)\n     }\n-    debug_loc.apply(cx.fcx);\n-    B(cx).nswmul(lhs, rhs)\n-}\n \n-pub fn NUWMul(cx: Block,\n-              lhs: ValueRef,\n-              rhs: ValueRef,\n-              debug_loc: DebugLoc)\n-              -> ValueRef {\n-    if cx.unreachable.get() {\n-        return _Undef(lhs);\n+pub fn NUWMul(cx: &BlockAndBuilder,\n+    lhs: ValueRef,\n+    rhs: ValueRef,\n+    debug_loc: DebugLoc)\n+    -> ValueRef {\n+        debug_loc.apply(cx.fcx());\n+        cx.nuwmul(lhs, rhs)\n     }\n-    debug_loc.apply(cx.fcx);\n-    B(cx).nuwmul(lhs, rhs)\n-}\n \n-pub fn FMul(cx: Block,\n-            lhs: ValueRef,\n-            rhs: ValueRef,\n-            debug_loc: DebugLoc)\n-            -> ValueRef {\n-    if cx.unreachable.get() {\n-        return _Undef(lhs);\n+pub fn FMul(cx: &BlockAndBuilder,\n+    lhs: ValueRef,\n+    rhs: ValueRef,\n+    debug_loc: DebugLoc)\n+    -> ValueRef {\n+        debug_loc.apply(cx.fcx());\n+        cx.fmul(lhs, rhs)\n     }\n-    debug_loc.apply(cx.fcx);\n-    B(cx).fmul(lhs, rhs)\n-}\n \n-pub fn FMulFast(cx: Block,\n-                lhs: ValueRef,\n-                rhs: ValueRef,\n-                debug_loc: DebugLoc)\n-            -> ValueRef {\n-    if cx.unreachable.get() {\n-        return _Undef(lhs);\n+pub fn FMulFast(cx: &BlockAndBuilder,\n+    lhs: ValueRef,\n+    rhs: ValueRef,\n+    debug_loc: DebugLoc)\n+    -> ValueRef {\n+        debug_loc.apply(cx.fcx());\n+        cx.fmul_fast(lhs, rhs)\n     }\n-    debug_loc.apply(cx.fcx);\n-    B(cx).fmul_fast(lhs, rhs)\n-}\n \n-pub fn UDiv(cx: Block,\n-            lhs: ValueRef,\n-            rhs: ValueRef,\n-            debug_loc: DebugLoc)\n-            -> ValueRef {\n-    if cx.unreachable.get() {\n-        return _Undef(lhs);\n+pub fn UDiv(cx: &BlockAndBuilder,\n+    lhs: ValueRef,\n+    rhs: ValueRef,\n+    debug_loc: DebugLoc)\n+    -> ValueRef {\n+        debug_loc.apply(cx.fcx());\n+        cx.udiv(lhs, rhs)\n     }\n-    debug_loc.apply(cx.fcx);\n-    B(cx).udiv(lhs, rhs)\n-}\n \n-pub fn SDiv(cx: Block,\n-            lhs: ValueRef,\n-            rhs: ValueRef,\n-            debug_loc: DebugLoc)\n-            -> ValueRef {\n-    if cx.unreachable.get() {\n-        return _Undef(lhs);\n+pub fn SDiv(cx: &BlockAndBuilder,\n+    lhs: ValueRef,\n+    rhs: ValueRef,\n+    debug_loc: DebugLoc)\n+    -> ValueRef {\n+        debug_loc.apply(cx.fcx());\n+        cx.sdiv(lhs, rhs)\n     }\n-    debug_loc.apply(cx.fcx);\n-    B(cx).sdiv(lhs, rhs)\n-}\n \n-pub fn ExactSDiv(cx: Block,\n-                 lhs: ValueRef,\n-                 rhs: ValueRef,\n-                 debug_loc: DebugLoc)\n-                 -> ValueRef {\n-    if cx.unreachable.get() {\n-        return _Undef(lhs);\n+pub fn ExactSDiv(cx: &BlockAndBuilder,\n+    lhs: ValueRef,\n+    rhs: ValueRef,\n+    debug_loc: DebugLoc)\n+    -> ValueRef {\n+        debug_loc.apply(cx.fcx());\n+        cx.exactsdiv(lhs, rhs)\n     }\n-    debug_loc.apply(cx.fcx);\n-    B(cx).exactsdiv(lhs, rhs)\n-}\n \n-pub fn FDiv(cx: Block,\n-            lhs: ValueRef,\n-            rhs: ValueRef,\n-            debug_loc: DebugLoc)\n-            -> ValueRef {\n-    if cx.unreachable.get() {\n-        return _Undef(lhs);\n+pub fn FDiv(cx: &BlockAndBuilder,\n+    lhs: ValueRef,\n+    rhs: ValueRef,\n+    debug_loc: DebugLoc)\n+    -> ValueRef {\n+        debug_loc.apply(cx.fcx());\n+        cx.fdiv(lhs, rhs)\n     }\n-    debug_loc.apply(cx.fcx);\n-    B(cx).fdiv(lhs, rhs)\n-}\n \n-pub fn FDivFast(cx: Block,\n-                lhs: ValueRef,\n-                rhs: ValueRef,\n-                debug_loc: DebugLoc)\n-            -> ValueRef {\n-    if cx.unreachable.get() {\n-        return _Undef(lhs);\n+pub fn FDivFast(cx: &BlockAndBuilder,\n+    lhs: ValueRef,\n+    rhs: ValueRef,\n+    debug_loc: DebugLoc)\n+    -> ValueRef {\n+        debug_loc.apply(cx.fcx());\n+        cx.fdiv_fast(lhs, rhs)\n     }\n-    debug_loc.apply(cx.fcx);\n-    B(cx).fdiv_fast(lhs, rhs)\n-}\n \n-pub fn URem(cx: Block,\n-            lhs: ValueRef,\n-            rhs: ValueRef,\n-            debug_loc: DebugLoc)\n-            -> ValueRef {\n-    if cx.unreachable.get() {\n-        return _Undef(lhs);\n+pub fn URem(cx: &BlockAndBuilder,\n+    lhs: ValueRef,\n+    rhs: ValueRef,\n+    debug_loc: DebugLoc)\n+    -> ValueRef {\n+        debug_loc.apply(cx.fcx());\n+        cx.urem(lhs, rhs)\n     }\n-    debug_loc.apply(cx.fcx);\n-    B(cx).urem(lhs, rhs)\n-}\n \n-pub fn SRem(cx: Block,\n-            lhs: ValueRef,\n-            rhs: ValueRef,\n-            debug_loc: DebugLoc)\n-            -> ValueRef {\n-    if cx.unreachable.get() {\n-        return _Undef(lhs);\n+pub fn SRem(cx: &BlockAndBuilder,\n+    lhs: ValueRef,\n+    rhs: ValueRef,\n+    debug_loc: DebugLoc)\n+    -> ValueRef {\n+        debug_loc.apply(cx.fcx());\n+        cx.srem(lhs, rhs)\n     }\n-    debug_loc.apply(cx.fcx);\n-    B(cx).srem(lhs, rhs)\n-}\n \n-pub fn FRem(cx: Block,\n-            lhs: ValueRef,\n-            rhs: ValueRef,\n-            debug_loc: DebugLoc)\n-            -> ValueRef {\n-    if cx.unreachable.get() {\n-        return _Undef(lhs);\n+pub fn FRem(cx: &BlockAndBuilder,\n+    lhs: ValueRef,\n+    rhs: ValueRef,\n+    debug_loc: DebugLoc)\n+    -> ValueRef {\n+        debug_loc.apply(cx.fcx());\n+        cx.frem(lhs, rhs)\n     }\n-    debug_loc.apply(cx.fcx);\n-    B(cx).frem(lhs, rhs)\n-}\n \n-pub fn FRemFast(cx: Block,\n-                lhs: ValueRef,\n-                rhs: ValueRef,\n-                debug_loc: DebugLoc)\n-            -> ValueRef {\n-    if cx.unreachable.get() {\n-        return _Undef(lhs);\n+pub fn FRemFast(cx: &BlockAndBuilder,\n+    lhs: ValueRef,\n+    rhs: ValueRef,\n+    debug_loc: DebugLoc)\n+    -> ValueRef {\n+        debug_loc.apply(cx.fcx());\n+        cx.frem_fast(lhs, rhs)\n     }\n-    debug_loc.apply(cx.fcx);\n-    B(cx).frem_fast(lhs, rhs)\n-}\n \n-pub fn Shl(cx: Block,\n-           lhs: ValueRef,\n-           rhs: ValueRef,\n-           debug_loc: DebugLoc)\n-           -> ValueRef {\n-    if cx.unreachable.get() {\n-        return _Undef(lhs);\n+pub fn Shl(cx: &BlockAndBuilder,\n+    lhs: ValueRef,\n+    rhs: ValueRef,\n+    debug_loc: DebugLoc)\n+    -> ValueRef {\n+        debug_loc.apply(cx.fcx());\n+        cx.shl(lhs, rhs)\n     }\n-    debug_loc.apply(cx.fcx);\n-    B(cx).shl(lhs, rhs)\n-}\n \n-pub fn LShr(cx: Block,\n-            lhs: ValueRef,\n-            rhs: ValueRef,\n-            debug_loc: DebugLoc)\n-            -> ValueRef {\n-    if cx.unreachable.get() {\n-        return _Undef(lhs);\n+pub fn LShr(cx: &BlockAndBuilder,\n+    lhs: ValueRef,\n+    rhs: ValueRef,\n+    debug_loc: DebugLoc)\n+    -> ValueRef {\n+        debug_loc.apply(cx.fcx());\n+        cx.lshr(lhs, rhs)\n     }\n-    debug_loc.apply(cx.fcx);\n-    B(cx).lshr(lhs, rhs)\n-}\n \n-pub fn AShr(cx: Block,\n-            lhs: ValueRef,\n-            rhs: ValueRef,\n-            debug_loc: DebugLoc)\n-            -> ValueRef {\n-    if cx.unreachable.get() {\n-        return _Undef(lhs);\n+pub fn AShr(cx: &BlockAndBuilder,\n+    lhs: ValueRef,\n+    rhs: ValueRef,\n+    debug_loc: DebugLoc)\n+    -> ValueRef {\n+        debug_loc.apply(cx.fcx());\n+        cx.ashr(lhs, rhs)\n     }\n-    debug_loc.apply(cx.fcx);\n-    B(cx).ashr(lhs, rhs)\n-}\n \n-pub fn And(cx: Block,\n-           lhs: ValueRef,\n-           rhs: ValueRef,\n-           debug_loc: DebugLoc)\n-           -> ValueRef {\n-    if cx.unreachable.get() {\n-        return _Undef(lhs);\n+pub fn And(cx: &BlockAndBuilder,\n+    lhs: ValueRef,\n+    rhs: ValueRef,\n+    debug_loc: DebugLoc)\n+    -> ValueRef {\n+        debug_loc.apply(cx.fcx());\n+        cx.and(lhs, rhs)\n     }\n-    debug_loc.apply(cx.fcx);\n-    B(cx).and(lhs, rhs)\n-}\n \n-pub fn Or(cx: Block,\n-          lhs: ValueRef,\n-          rhs: ValueRef,\n-          debug_loc: DebugLoc)\n-          -> ValueRef {\n-    if cx.unreachable.get() {\n-        return _Undef(lhs);\n+pub fn Or(cx: &BlockAndBuilder,\n+    lhs: ValueRef,\n+    rhs: ValueRef,\n+    debug_loc: DebugLoc)\n+    -> ValueRef {\n+        debug_loc.apply(cx.fcx());\n+        cx.or(lhs, rhs)\n     }\n-    debug_loc.apply(cx.fcx);\n-    B(cx).or(lhs, rhs)\n-}\n \n-pub fn Xor(cx: Block,\n-           lhs: ValueRef,\n-           rhs: ValueRef,\n-           debug_loc: DebugLoc)\n-           -> ValueRef {\n-    if cx.unreachable.get() {\n-        return _Undef(lhs);\n-    }\n-    debug_loc.apply(cx.fcx);\n-    B(cx).xor(lhs, rhs)\n-}\n-\n-pub fn BinOp(cx: Block,\n-             op: Opcode,\n-             lhs: ValueRef,\n-             rhs: ValueRef,\n-             debug_loc: DebugLoc)\n-          -> ValueRef {\n-    if cx.unreachable.get() {\n-        return _Undef(lhs);\n+pub fn Xor(cx: &BlockAndBuilder,\n+    lhs: ValueRef,\n+    rhs: ValueRef,\n+    debug_loc: DebugLoc)\n+    -> ValueRef {\n+        debug_loc.apply(cx.fcx());\n+        cx.xor(lhs, rhs)\n     }\n-    debug_loc.apply(cx.fcx);\n-    B(cx).binop(op, lhs, rhs)\n-}\n \n-pub fn Neg(cx: Block, v: ValueRef, debug_loc: DebugLoc) -> ValueRef {\n-    if cx.unreachable.get() {\n-        return _Undef(v);\n+pub fn BinOp(cx: &BlockAndBuilder,\n+    op: Opcode,\n+    lhs: ValueRef,\n+    rhs: ValueRef,\n+    debug_loc: DebugLoc)\n+    -> ValueRef {\n+        debug_loc.apply(cx.fcx());\n+        cx.binop(op, lhs, rhs)\n     }\n-    debug_loc.apply(cx.fcx);\n-    B(cx).neg(v)\n+\n+pub fn Neg(cx: &BlockAndBuilder, v: ValueRef, debug_loc: DebugLoc) -> ValueRef {\n+    debug_loc.apply(cx.fcx());\n+    cx.neg(v)\n }\n \n-pub fn NSWNeg(cx: Block, v: ValueRef, debug_loc: DebugLoc) -> ValueRef {\n-    if cx.unreachable.get() {\n-        return _Undef(v);\n-    }\n-    debug_loc.apply(cx.fcx);\n-    B(cx).nswneg(v)\n+pub fn NSWNeg(cx: &BlockAndBuilder, v: ValueRef, debug_loc: DebugLoc) -> ValueRef {\n+    debug_loc.apply(cx.fcx());\n+    cx.nswneg(v)\n }\n \n-pub fn NUWNeg(cx: Block, v: ValueRef, debug_loc: DebugLoc) -> ValueRef {\n-    if cx.unreachable.get() {\n-        return _Undef(v);\n-    }\n-    debug_loc.apply(cx.fcx);\n-    B(cx).nuwneg(v)\n+pub fn NUWNeg(cx: &BlockAndBuilder, v: ValueRef, debug_loc: DebugLoc) -> ValueRef {\n+    debug_loc.apply(cx.fcx());\n+    cx.nuwneg(v)\n }\n-pub fn FNeg(cx: Block, v: ValueRef, debug_loc: DebugLoc) -> ValueRef {\n-    if cx.unreachable.get() {\n-        return _Undef(v);\n-    }\n-    debug_loc.apply(cx.fcx);\n-    B(cx).fneg(v)\n+pub fn FNeg(cx: &BlockAndBuilder, v: ValueRef, debug_loc: DebugLoc) -> ValueRef {\n+    debug_loc.apply(cx.fcx());\n+    cx.fneg(v)\n }\n \n-pub fn Not(cx: Block, v: ValueRef, debug_loc: DebugLoc) -> ValueRef {\n-    if cx.unreachable.get() {\n-        return _Undef(v);\n-    }\n-    debug_loc.apply(cx.fcx);\n-    B(cx).not(v)\n+pub fn Not(cx: &BlockAndBuilder, v: ValueRef, debug_loc: DebugLoc) -> ValueRef {\n+    debug_loc.apply(cx.fcx());\n+    cx.not(v)\n }\n \n-pub fn Alloca(cx: Block, ty: Type, name: &str) -> ValueRef {\n-    unsafe {\n-        if cx.unreachable.get() { return llvm::LLVMGetUndef(ty.ptr_to().to_ref()); }\n-        AllocaFcx(cx.fcx, ty, name)\n-    }\n+pub fn Alloca(cx: &BlockAndBuilder, ty: Type, name: &str) -> ValueRef {\n+    AllocaFcx(cx.fcx(), ty, name)\n }\n \n pub fn AllocaFcx(fcx: &FunctionContext, ty: Type, name: &str) -> ValueRef {\n@@ -599,336 +424,179 @@ pub fn AllocaFcx(fcx: &FunctionContext, ty: Type, name: &str) -> ValueRef {\n     b.alloca(ty, name)\n }\n \n-pub fn Free(cx: Block, pointer_val: ValueRef) {\n-    if cx.unreachable.get() { return; }\n-    B(cx).free(pointer_val)\n+pub fn Free(cx: &BlockAndBuilder, pointer_val: ValueRef) {\n+    cx.free(pointer_val)\n }\n \n-pub fn Load(cx: Block, pointer_val: ValueRef) -> ValueRef {\n-    unsafe {\n-        let ccx = cx.fcx.ccx;\n-        if cx.unreachable.get() {\n-            let ty = val_ty(pointer_val);\n-            let eltty = if ty.kind() == llvm::Array {\n-                ty.element_type()\n-            } else {\n-                ccx.int_type()\n-            };\n-            return llvm::LLVMGetUndef(eltty.to_ref());\n-        }\n-        B(cx).load(pointer_val)\n-    }\n+pub fn Load(cx: &BlockAndBuilder, pointer_val: ValueRef) -> ValueRef {\n+    cx.load(pointer_val)\n }\n \n-pub fn VolatileLoad(cx: Block, pointer_val: ValueRef) -> ValueRef {\n-    unsafe {\n-        if cx.unreachable.get() {\n-            return llvm::LLVMGetUndef(Type::nil(cx.ccx()).to_ref());\n-        }\n-        B(cx).volatile_load(pointer_val)\n-    }\n+pub fn VolatileLoad(cx: &BlockAndBuilder, pointer_val: ValueRef) -> ValueRef {\n+    cx.volatile_load(pointer_val)\n }\n \n-pub fn AtomicLoad(cx: Block, pointer_val: ValueRef, order: AtomicOrdering) -> ValueRef {\n-    unsafe {\n-        let ccx = cx.fcx.ccx;\n-        if cx.unreachable.get() {\n-            return llvm::LLVMGetUndef(ccx.int_type().to_ref());\n-        }\n-        B(cx).atomic_load(pointer_val, order)\n-    }\n+pub fn AtomicLoad(cx: &BlockAndBuilder, pointer_val: ValueRef, order: AtomicOrdering) -> ValueRef {\n+    cx.atomic_load(pointer_val, order)\n }\n \n \n-pub fn LoadRangeAssert(cx: Block, pointer_val: ValueRef, lo: u64,\n-                       hi: u64, signed: llvm::Bool) -> ValueRef {\n-    if cx.unreachable.get() {\n-        let ccx = cx.fcx.ccx;\n-        let ty = val_ty(pointer_val);\n-        let eltty = if ty.kind() == llvm::Array {\n-            ty.element_type()\n-        } else {\n-            ccx.int_type()\n-        };\n-        unsafe {\n-            llvm::LLVMGetUndef(eltty.to_ref())\n-        }\n-    } else {\n-        B(cx).load_range_assert(pointer_val, lo, hi, signed)\n-    }\n+pub fn LoadRangeAssert(cx: &BlockAndBuilder, pointer_val: ValueRef, lo: u64,\n+    hi: u64, signed: llvm::Bool) -> ValueRef {\n+    cx.load_range_assert(pointer_val, lo, hi, signed)\n }\n \n-pub fn LoadNonNull(cx: Block, ptr: ValueRef) -> ValueRef {\n-    if cx.unreachable.get() {\n-        let ccx = cx.fcx.ccx;\n-        let ty = val_ty(ptr);\n-        let eltty = if ty.kind() == llvm::Array {\n-            ty.element_type()\n-        } else {\n-            ccx.int_type()\n-        };\n-        unsafe {\n-            llvm::LLVMGetUndef(eltty.to_ref())\n-        }\n-    } else {\n-        B(cx).load_nonnull(ptr)\n-    }\n+pub fn LoadNonNull(cx: &BlockAndBuilder, ptr: ValueRef) -> ValueRef {\n+    cx.load_nonnull(ptr)\n }\n \n-pub fn Store(cx: Block, val: ValueRef, ptr: ValueRef) -> ValueRef {\n-    if cx.unreachable.get() { return C_nil(cx.ccx()); }\n-    B(cx).store(val, ptr)\n+pub fn Store(cx: &BlockAndBuilder, val: ValueRef, ptr: ValueRef) -> ValueRef {\n+    cx.store(val, ptr)\n }\n \n-pub fn VolatileStore(cx: Block, val: ValueRef, ptr: ValueRef) -> ValueRef {\n-    if cx.unreachable.get() { return C_nil(cx.ccx()); }\n-    B(cx).volatile_store(val, ptr)\n+pub fn VolatileStore(cx: &BlockAndBuilder, val: ValueRef, ptr: ValueRef) -> ValueRef {\n+    cx.volatile_store(val, ptr)\n }\n \n-pub fn AtomicStore(cx: Block, val: ValueRef, ptr: ValueRef, order: AtomicOrdering) {\n-    if cx.unreachable.get() { return; }\n-    B(cx).atomic_store(val, ptr, order)\n+pub fn AtomicStore(cx: &BlockAndBuilder, val: ValueRef, ptr: ValueRef, order: AtomicOrdering) {\n+    cx.atomic_store(val, ptr, order)\n }\n \n-pub fn GEP(cx: Block, pointer: ValueRef, indices: &[ValueRef]) -> ValueRef {\n-    unsafe {\n-        if cx.unreachable.get() {\n-            return llvm::LLVMGetUndef(Type::nil(cx.ccx()).ptr_to().to_ref());\n-        }\n-        B(cx).gep(pointer, indices)\n-    }\n+pub fn GEP(cx: &BlockAndBuilder, pointer: ValueRef, indices: &[ValueRef]) -> ValueRef {\n+    cx.gep(pointer, indices)\n }\n \n // Simple wrapper around GEP that takes an array of ints and wraps them\n // in C_i32()\n #[inline]\n-pub fn GEPi(cx: Block, base: ValueRef, ixs: &[usize]) -> ValueRef {\n-    unsafe {\n-        if cx.unreachable.get() {\n-            return llvm::LLVMGetUndef(Type::nil(cx.ccx()).ptr_to().to_ref());\n-        }\n-        B(cx).gepi(base, ixs)\n-    }\n+pub fn GEPi(cx: &BlockAndBuilder, base: ValueRef, ixs: &[usize]) -> ValueRef {\n+    cx.gepi(base, ixs)\n }\n \n-pub fn InBoundsGEP(cx: Block, pointer: ValueRef, indices: &[ValueRef]) -> ValueRef {\n-    unsafe {\n-        if cx.unreachable.get() {\n-            return llvm::LLVMGetUndef(Type::nil(cx.ccx()).ptr_to().to_ref());\n-        }\n-        B(cx).inbounds_gep(pointer, indices)\n-    }\n+pub fn InBoundsGEP(cx: &BlockAndBuilder, pointer: ValueRef, indices: &[ValueRef]) -> ValueRef {\n+    cx.inbounds_gep(pointer, indices)\n }\n \n-pub fn StructGEP(cx: Block, pointer: ValueRef, idx: usize) -> ValueRef {\n-    unsafe {\n-        if cx.unreachable.get() {\n-            return llvm::LLVMGetUndef(Type::nil(cx.ccx()).ptr_to().to_ref());\n-        }\n-        B(cx).struct_gep(pointer, idx)\n-    }\n+pub fn StructGEP(cx: &BlockAndBuilder, pointer: ValueRef, idx: usize) -> ValueRef {\n+    cx.struct_gep(pointer, idx)\n }\n \n-pub fn GlobalString(cx: Block, _str: *const c_char) -> ValueRef {\n-    unsafe {\n-        if cx.unreachable.get() {\n-            return llvm::LLVMGetUndef(Type::i8p(cx.ccx()).to_ref());\n-        }\n-        B(cx).global_string(_str)\n-    }\n+pub fn GlobalString(cx: &BlockAndBuilder, _str: *const c_char) -> ValueRef {\n+    cx.global_string(_str)\n }\n \n-pub fn GlobalStringPtr(cx: Block, _str: *const c_char) -> ValueRef {\n-    unsafe {\n-        if cx.unreachable.get() {\n-            return llvm::LLVMGetUndef(Type::i8p(cx.ccx()).to_ref());\n-        }\n-        B(cx).global_string_ptr(_str)\n-    }\n+pub fn GlobalStringPtr(cx: &BlockAndBuilder, _str: *const c_char) -> ValueRef {\n+    cx.global_string_ptr(_str)\n }\n \n /* Casts */\n-pub fn Trunc(cx: Block, val: ValueRef, dest_ty: Type) -> ValueRef {\n-    unsafe {\n-        if cx.unreachable.get() { return llvm::LLVMGetUndef(dest_ty.to_ref()); }\n-        B(cx).trunc(val, dest_ty)\n-    }\n+pub fn Trunc(cx: &BlockAndBuilder, val: ValueRef, dest_ty: Type) -> ValueRef {\n+    cx.trunc(val, dest_ty)\n }\n \n-pub fn ZExt(cx: Block, val: ValueRef, dest_ty: Type) -> ValueRef {\n-    unsafe {\n-        if cx.unreachable.get() { return llvm::LLVMGetUndef(dest_ty.to_ref()); }\n-        B(cx).zext(val, dest_ty)\n-    }\n+pub fn ZExt(cx: &BlockAndBuilder, val: ValueRef, dest_ty: Type) -> ValueRef {\n+    cx.zext(val, dest_ty)\n }\n \n-pub fn SExt(cx: Block, val: ValueRef, dest_ty: Type) -> ValueRef {\n-    unsafe {\n-        if cx.unreachable.get() { return llvm::LLVMGetUndef(dest_ty.to_ref()); }\n-        B(cx).sext(val, dest_ty)\n-    }\n+pub fn SExt(cx: &BlockAndBuilder, val: ValueRef, dest_ty: Type) -> ValueRef {\n+    cx.sext(val, dest_ty)\n }\n \n-pub fn FPToUI(cx: Block, val: ValueRef, dest_ty: Type) -> ValueRef {\n-    unsafe {\n-        if cx.unreachable.get() { return llvm::LLVMGetUndef(dest_ty.to_ref()); }\n-        B(cx).fptoui(val, dest_ty)\n-    }\n+pub fn FPToUI(cx: &BlockAndBuilder, val: ValueRef, dest_ty: Type) -> ValueRef {\n+    cx.fptoui(val, dest_ty)\n }\n \n-pub fn FPToSI(cx: Block, val: ValueRef, dest_ty: Type) -> ValueRef {\n-    unsafe {\n-        if cx.unreachable.get() { return llvm::LLVMGetUndef(dest_ty.to_ref()); }\n-        B(cx).fptosi(val, dest_ty)\n-    }\n+pub fn FPToSI(cx: &BlockAndBuilder, val: ValueRef, dest_ty: Type) -> ValueRef {\n+    cx.fptosi(val, dest_ty)\n }\n \n-pub fn UIToFP(cx: Block, val: ValueRef, dest_ty: Type) -> ValueRef {\n-    unsafe {\n-        if cx.unreachable.get() { return llvm::LLVMGetUndef(dest_ty.to_ref()); }\n-        B(cx).uitofp(val, dest_ty)\n-    }\n+pub fn UIToFP(cx: &BlockAndBuilder, val: ValueRef, dest_ty: Type) -> ValueRef {\n+    cx.uitofp(val, dest_ty)\n }\n \n-pub fn SIToFP(cx: Block, val: ValueRef, dest_ty: Type) -> ValueRef {\n-    unsafe {\n-        if cx.unreachable.get() { return llvm::LLVMGetUndef(dest_ty.to_ref()); }\n-        B(cx).sitofp(val, dest_ty)\n-    }\n+pub fn SIToFP(cx: &BlockAndBuilder, val: ValueRef, dest_ty: Type) -> ValueRef {\n+    cx.sitofp(val, dest_ty)\n }\n \n-pub fn FPTrunc(cx: Block, val: ValueRef, dest_ty: Type) -> ValueRef {\n-    unsafe {\n-        if cx.unreachable.get() { return llvm::LLVMGetUndef(dest_ty.to_ref()); }\n-        B(cx).fptrunc(val, dest_ty)\n-    }\n+pub fn FPTrunc(cx: &BlockAndBuilder, val: ValueRef, dest_ty: Type) -> ValueRef {\n+    cx.fptrunc(val, dest_ty)\n }\n \n-pub fn FPExt(cx: Block, val: ValueRef, dest_ty: Type) -> ValueRef {\n-    unsafe {\n-        if cx.unreachable.get() { return llvm::LLVMGetUndef(dest_ty.to_ref()); }\n-        B(cx).fpext(val, dest_ty)\n-    }\n+pub fn FPExt(cx: &BlockAndBuilder, val: ValueRef, dest_ty: Type) -> ValueRef {\n+    cx.fpext(val, dest_ty)\n }\n \n-pub fn PtrToInt(cx: Block, val: ValueRef, dest_ty: Type) -> ValueRef {\n-    unsafe {\n-        if cx.unreachable.get() { return llvm::LLVMGetUndef(dest_ty.to_ref()); }\n-        B(cx).ptrtoint(val, dest_ty)\n-    }\n+pub fn PtrToInt(cx: &BlockAndBuilder, val: ValueRef, dest_ty: Type) -> ValueRef {\n+    cx.ptrtoint(val, dest_ty)\n }\n \n-pub fn IntToPtr(cx: Block, val: ValueRef, dest_ty: Type) -> ValueRef {\n-    unsafe {\n-        if cx.unreachable.get() { return llvm::LLVMGetUndef(dest_ty.to_ref()); }\n-        B(cx).inttoptr(val, dest_ty)\n-    }\n+pub fn IntToPtr(cx: &BlockAndBuilder, val: ValueRef, dest_ty: Type) -> ValueRef {\n+    cx.inttoptr(val, dest_ty)\n }\n \n-pub fn BitCast(cx: Block, val: ValueRef, dest_ty: Type) -> ValueRef {\n-    unsafe {\n-        if cx.unreachable.get() { return llvm::LLVMGetUndef(dest_ty.to_ref()); }\n-        B(cx).bitcast(val, dest_ty)\n-    }\n+pub fn BitCast(cx: &BlockAndBuilder, val: ValueRef, dest_ty: Type) -> ValueRef {\n+    cx.bitcast(val, dest_ty)\n }\n \n-pub fn ZExtOrBitCast(cx: Block, val: ValueRef, dest_ty: Type) -> ValueRef {\n-    unsafe {\n-        if cx.unreachable.get() { return llvm::LLVMGetUndef(dest_ty.to_ref()); }\n-        B(cx).zext_or_bitcast(val, dest_ty)\n-    }\n+pub fn ZExtOrBitCast(cx: &BlockAndBuilder, val: ValueRef, dest_ty: Type) -> ValueRef {\n+    cx.zext_or_bitcast(val, dest_ty)\n }\n \n-pub fn SExtOrBitCast(cx: Block, val: ValueRef, dest_ty: Type) -> ValueRef {\n-    unsafe {\n-        if cx.unreachable.get() { return llvm::LLVMGetUndef(dest_ty.to_ref()); }\n-        B(cx).sext_or_bitcast(val, dest_ty)\n-    }\n+pub fn SExtOrBitCast(cx: &BlockAndBuilder, val: ValueRef, dest_ty: Type) -> ValueRef {\n+    cx.sext_or_bitcast(val, dest_ty)\n }\n \n-pub fn TruncOrBitCast(cx: Block, val: ValueRef, dest_ty: Type) -> ValueRef {\n-    unsafe {\n-        if cx.unreachable.get() { return llvm::LLVMGetUndef(dest_ty.to_ref()); }\n-        B(cx).trunc_or_bitcast(val, dest_ty)\n-    }\n+pub fn TruncOrBitCast(cx: &BlockAndBuilder, val: ValueRef, dest_ty: Type) -> ValueRef {\n+    cx.trunc_or_bitcast(val, dest_ty)\n }\n \n-pub fn Cast(cx: Block, op: Opcode, val: ValueRef, dest_ty: Type,\n-            _: *const u8)\n-     -> ValueRef {\n-    unsafe {\n-        if cx.unreachable.get() { return llvm::LLVMGetUndef(dest_ty.to_ref()); }\n-        B(cx).cast(op, val, dest_ty)\n+pub fn Cast(cx: &BlockAndBuilder, op: Opcode, val: ValueRef, dest_ty: Type,\n+    _: *const u8)\n+    -> ValueRef {\n+        cx.cast(op, val, dest_ty)\n     }\n-}\n \n-pub fn PointerCast(cx: Block, val: ValueRef, dest_ty: Type) -> ValueRef {\n-    unsafe {\n-        if cx.unreachable.get() { return llvm::LLVMGetUndef(dest_ty.to_ref()); }\n-        B(cx).pointercast(val, dest_ty)\n-    }\n+pub fn PointerCast(cx: &BlockAndBuilder, val: ValueRef, dest_ty: Type) -> ValueRef {\n+    cx.pointercast(val, dest_ty)\n }\n \n-pub fn IntCast(cx: Block, val: ValueRef, dest_ty: Type) -> ValueRef {\n-    unsafe {\n-        if cx.unreachable.get() { return llvm::LLVMGetUndef(dest_ty.to_ref()); }\n-        B(cx).intcast(val, dest_ty)\n-    }\n+pub fn IntCast(cx: &BlockAndBuilder, val: ValueRef, dest_ty: Type) -> ValueRef {\n+    cx.intcast(val, dest_ty)\n }\n \n-pub fn FPCast(cx: Block, val: ValueRef, dest_ty: Type) -> ValueRef {\n-    unsafe {\n-        if cx.unreachable.get() { return llvm::LLVMGetUndef(dest_ty.to_ref()); }\n-        B(cx).fpcast(val, dest_ty)\n-    }\n+pub fn FPCast(cx: &BlockAndBuilder, val: ValueRef, dest_ty: Type) -> ValueRef {\n+    cx.fpcast(val, dest_ty)\n }\n \n \n /* Comparisons */\n-pub fn ICmp(cx: Block,\n-            op: IntPredicate,\n-            lhs: ValueRef,\n-            rhs: ValueRef,\n-            debug_loc: DebugLoc)\n-            -> ValueRef {\n-    unsafe {\n-        if cx.unreachable.get() {\n-            return llvm::LLVMGetUndef(Type::i1(cx.ccx()).to_ref());\n-        }\n-        debug_loc.apply(cx.fcx);\n-        B(cx).icmp(op, lhs, rhs)\n+pub fn ICmp(cx: &BlockAndBuilder,\n+    op: IntPredicate,\n+    lhs: ValueRef,\n+    rhs: ValueRef,\n+    debug_loc: DebugLoc)\n+    -> ValueRef {\n+        debug_loc.apply(cx.fcx());\n+        cx.icmp(op, lhs, rhs)\n     }\n-}\n \n-pub fn FCmp(cx: Block,\n-            op: RealPredicate,\n-            lhs: ValueRef,\n-            rhs: ValueRef,\n-            debug_loc: DebugLoc)\n-            -> ValueRef {\n-    unsafe {\n-        if cx.unreachable.get() {\n-            return llvm::LLVMGetUndef(Type::i1(cx.ccx()).to_ref());\n-        }\n-        debug_loc.apply(cx.fcx);\n-        B(cx).fcmp(op, lhs, rhs)\n+pub fn FCmp(cx: &BlockAndBuilder,\n+    op: RealPredicate,\n+    lhs: ValueRef,\n+    rhs: ValueRef,\n+    debug_loc: DebugLoc)\n+    -> ValueRef {\n+        debug_loc.apply(cx.fcx());\n+        cx.fcmp(op, lhs, rhs)\n     }\n-}\n \n /* Miscellaneous instructions */\n-pub fn EmptyPhi(cx: Block, ty: Type) -> ValueRef {\n-    unsafe {\n-        if cx.unreachable.get() { return llvm::LLVMGetUndef(ty.to_ref()); }\n-        B(cx).empty_phi(ty)\n-    }\n+pub fn EmptyPhi(cx: &BlockAndBuilder, ty: Type) -> ValueRef {\n+    cx.empty_phi(ty)\n }\n \n-pub fn Phi(cx: Block, ty: Type, vals: &[ValueRef],\n-           bbs: &[BasicBlockRef]) -> ValueRef {\n-    unsafe {\n-        if cx.unreachable.get() { return llvm::LLVMGetUndef(ty.to_ref()); }\n-        B(cx).phi(ty, vals, bbs)\n-    }\n+pub fn Phi(cx: &BlockAndBuilder, ty: Type, vals: &[ValueRef], bbs: &[BasicBlockRef]) -> ValueRef {\n+    cx.phi(ty, vals, bbs)\n }\n \n pub fn AddIncomingToPhi(phi: ValueRef, val: ValueRef, bb: BasicBlockRef) {\n@@ -938,230 +606,151 @@ pub fn AddIncomingToPhi(phi: ValueRef, val: ValueRef, bb: BasicBlockRef) {\n     }\n }\n \n-pub fn _UndefReturn(cx: Block, fn_: ValueRef) -> ValueRef {\n-    unsafe {\n-        let ccx = cx.fcx.ccx;\n-        let ty = val_ty(fn_);\n-        let retty = if ty.kind() == llvm::Function {\n-            ty.return_type()\n-        } else {\n-            ccx.int_type()\n-        };\n-        B(cx).count_insn(\"ret_undef\");\n-        llvm::LLVMGetUndef(retty.to_ref())\n-    }\n-}\n-\n-pub fn add_span_comment(cx: Block, sp: Span, text: &str) {\n-    B(cx).add_span_comment(sp, text)\n+pub fn add_span_comment(cx: &BlockAndBuilder, sp: Span, text: &str) {\n+    cx.add_span_comment(sp, text)\n }\n \n-pub fn add_comment(cx: Block, text: &str) {\n-    B(cx).add_comment(text)\n+pub fn add_comment(cx: &BlockAndBuilder, text: &str) {\n+    cx.add_comment(text)\n }\n \n-pub fn InlineAsmCall(cx: Block, asm: *const c_char, cons: *const c_char,\n-                     inputs: &[ValueRef], output: Type,\n-                     volatile: bool, alignstack: bool,\n-                     dia: AsmDialect) -> ValueRef {\n-    B(cx).inline_asm_call(asm, cons, inputs, output, volatile, alignstack, dia)\n+pub fn InlineAsmCall(cx: &BlockAndBuilder, asm: *const c_char, cons: *const c_char,\n+    inputs: &[ValueRef], output: Type,\n+    volatile: bool, alignstack: bool,\n+    dia: AsmDialect) -> ValueRef {\n+    cx.inline_asm_call(asm, cons, inputs, output, volatile, alignstack, dia)\n }\n \n-pub fn Call(cx: Block,\n-            fn_: ValueRef,\n-            args: &[ValueRef],\n-            debug_loc: DebugLoc)\n-            -> ValueRef {\n-    if cx.unreachable.get() {\n-        return _UndefReturn(cx, fn_);\n+pub fn Call(cx: &BlockAndBuilder,\n+    fn_: ValueRef,\n+    args: &[ValueRef],\n+    debug_loc: DebugLoc)\n+    -> ValueRef {\n+        debug_loc.apply(cx.fcx());\n+        let bundle = cx.lpad().and_then(|b| b.bundle());\n+        cx.call(fn_, args, bundle)\n     }\n-    debug_loc.apply(cx.fcx);\n-    let bundle = cx.lpad.get().and_then(|b| b.bundle());\n-    B(cx).call(fn_, args, bundle)\n-}\n-\n-pub fn AtomicFence(cx: Block, order: AtomicOrdering, scope: SynchronizationScope) {\n-    if cx.unreachable.get() { return; }\n-    B(cx).atomic_fence(order, scope)\n-}\n \n-pub fn Select(cx: Block, if_: ValueRef, then: ValueRef, else_: ValueRef) -> ValueRef {\n-    if cx.unreachable.get() { return _Undef(then); }\n-    B(cx).select(if_, then, else_)\n+pub fn AtomicFence(cx: &BlockAndBuilder, order: AtomicOrdering, scope: SynchronizationScope) {\n+    cx.atomic_fence(order, scope)\n }\n \n-pub fn VAArg(cx: Block, list: ValueRef, ty: Type) -> ValueRef {\n-    unsafe {\n-        if cx.unreachable.get() { return llvm::LLVMGetUndef(ty.to_ref()); }\n-        B(cx).va_arg(list, ty)\n-    }\n+pub fn Select(cx: &BlockAndBuilder, if_: ValueRef, then: ValueRef, else_: ValueRef) -> ValueRef {\n+    cx.select(if_, then, else_)\n }\n \n-pub fn ExtractElement(cx: Block, vec_val: ValueRef, index: ValueRef) -> ValueRef {\n-    unsafe {\n-        if cx.unreachable.get() {\n-            return llvm::LLVMGetUndef(Type::nil(cx.ccx()).to_ref());\n-        }\n-        B(cx).extract_element(vec_val, index)\n-    }\n+pub fn VAArg(cx: &BlockAndBuilder, list: ValueRef, ty: Type) -> ValueRef {\n+    cx.va_arg(list, ty)\n }\n \n-pub fn InsertElement(cx: Block, vec_val: ValueRef, elt_val: ValueRef,\n-                     index: ValueRef) -> ValueRef {\n-    unsafe {\n-        if cx.unreachable.get() {\n-            return llvm::LLVMGetUndef(Type::nil(cx.ccx()).to_ref());\n-        }\n-        B(cx).insert_element(vec_val, elt_val, index)\n-    }\n+pub fn ExtractElement(cx: &BlockAndBuilder, vec_val: ValueRef, index: ValueRef) -> ValueRef {\n+    cx.extract_element(vec_val, index)\n }\n \n-pub fn ShuffleVector(cx: Block, v1: ValueRef, v2: ValueRef,\n-                     mask: ValueRef) -> ValueRef {\n-    unsafe {\n-        if cx.unreachable.get() {\n-            return llvm::LLVMGetUndef(Type::nil(cx.ccx()).to_ref());\n-        }\n-        B(cx).shuffle_vector(v1, v2, mask)\n-    }\n+pub fn InsertElement(cx: &BlockAndBuilder, vec_val: ValueRef, elt_val: ValueRef,\n+    index: ValueRef) -> ValueRef {\n+    cx.insert_element(vec_val, elt_val, index)\n }\n \n-pub fn VectorSplat(cx: Block, num_elts: usize, elt_val: ValueRef) -> ValueRef {\n-    unsafe {\n-        if cx.unreachable.get() {\n-            return llvm::LLVMGetUndef(Type::nil(cx.ccx()).to_ref());\n-        }\n-        B(cx).vector_splat(num_elts, elt_val)\n-    }\n+pub fn ShuffleVector(cx: &BlockAndBuilder, v1: ValueRef, v2: ValueRef,\n+    mask: ValueRef) -> ValueRef {\n+    cx.shuffle_vector(v1, v2, mask)\n }\n \n-pub fn ExtractValue(cx: Block, agg_val: ValueRef, index: usize) -> ValueRef {\n-    unsafe {\n-        if cx.unreachable.get() {\n-            return llvm::LLVMGetUndef(Type::nil(cx.ccx()).to_ref());\n-        }\n-        B(cx).extract_value(agg_val, index)\n-    }\n+pub fn VectorSplat(cx: &BlockAndBuilder, num_elts: usize, elt_val: ValueRef) -> ValueRef {\n+    cx.vector_splat(num_elts, elt_val)\n }\n \n-pub fn InsertValue(cx: Block, agg_val: ValueRef, elt_val: ValueRef, index: usize) -> ValueRef {\n-    unsafe {\n-        if cx.unreachable.get() {\n-            return llvm::LLVMGetUndef(Type::nil(cx.ccx()).to_ref());\n-        }\n-        B(cx).insert_value(agg_val, elt_val, index)\n-    }\n+pub fn ExtractValue(cx: &BlockAndBuilder, agg_val: ValueRef, index: usize) -> ValueRef {\n+    cx.extract_value(agg_val, index)\n }\n \n-pub fn IsNull(cx: Block, val: ValueRef) -> ValueRef {\n-    unsafe {\n-        if cx.unreachable.get() {\n-            return llvm::LLVMGetUndef(Type::i1(cx.ccx()).to_ref());\n-        }\n-        B(cx).is_null(val)\n-    }\n+pub fn InsertValue(cx: &BlockAndBuilder, agg_val: ValueRef, elt_val: ValueRef, index: usize) -> ValueRef {\n+    cx.insert_value(agg_val, elt_val, index)\n }\n \n-pub fn IsNotNull(cx: Block, val: ValueRef) -> ValueRef {\n-    unsafe {\n-        if cx.unreachable.get() {\n-            return llvm::LLVMGetUndef(Type::i1(cx.ccx()).to_ref());\n-        }\n-        B(cx).is_not_null(val)\n-    }\n+pub fn IsNull(cx: &BlockAndBuilder, val: ValueRef) -> ValueRef {\n+    cx.is_null(val)\n }\n \n-pub fn PtrDiff(cx: Block, lhs: ValueRef, rhs: ValueRef) -> ValueRef {\n-    unsafe {\n-        let ccx = cx.fcx.ccx;\n-        if cx.unreachable.get() { return llvm::LLVMGetUndef(ccx.int_type().to_ref()); }\n-        B(cx).ptrdiff(lhs, rhs)\n-    }\n+pub fn IsNotNull(cx: &BlockAndBuilder, val: ValueRef) -> ValueRef {\n+    cx.is_not_null(val)\n }\n \n-pub fn Trap(cx: Block) {\n-    if cx.unreachable.get() { return; }\n-    B(cx).trap();\n+pub fn PtrDiff(cx: &BlockAndBuilder, lhs: ValueRef, rhs: ValueRef) -> ValueRef {\n+    cx.ptrdiff(lhs, rhs)\n }\n \n-pub fn LandingPad(cx: Block, ty: Type, pers_fn: ValueRef,\n-                  num_clauses: usize) -> ValueRef {\n-    check_not_terminated(cx);\n-    assert!(!cx.unreachable.get());\n-    B(cx).landing_pad(ty, pers_fn, num_clauses, cx.fcx.llfn)\n+pub fn Trap(cx: &BlockAndBuilder) {\n+    cx.trap();\n }\n \n-pub fn AddClause(cx: Block, landing_pad: ValueRef, clause: ValueRef) {\n-    B(cx).add_clause(landing_pad, clause)\n+pub fn LandingPad(cx: &BlockAndBuilder, ty: Type, pers_fn: ValueRef,\n+    num_clauses: usize) -> ValueRef {\n+    assert!(!cx.is_unreachable());\n+    cx.landing_pad(ty, pers_fn, num_clauses, cx.fcx().llfn)\n }\n \n-pub fn SetCleanup(cx: Block, landing_pad: ValueRef) {\n-    B(cx).set_cleanup(landing_pad)\n+pub fn AddClause(cx: &BlockAndBuilder, landing_pad: ValueRef, clause: ValueRef) {\n+    cx.add_clause(landing_pad, clause)\n }\n \n-pub fn SetPersonalityFn(cx: Block, f: ValueRef) {\n-    B(cx).set_personality_fn(f)\n+pub fn SetCleanup(cx: &BlockAndBuilder, landing_pad: ValueRef) {\n+    cx.set_cleanup(landing_pad)\n }\n \n-pub fn Resume(cx: Block, exn: ValueRef) -> ValueRef {\n-    check_not_terminated(cx);\n-    terminate(cx, \"Resume\");\n-    B(cx).resume(exn)\n+pub fn SetPersonalityFn(cx: &BlockAndBuilder, f: ValueRef) {\n+    cx.set_personality_fn(f)\n }\n \n // Atomic Operations\n-pub fn AtomicCmpXchg(cx: Block, dst: ValueRef,\n-                     cmp: ValueRef, src: ValueRef,\n-                     order: AtomicOrdering,\n-                     failure_order: AtomicOrdering,\n-                     weak: llvm::Bool) -> ValueRef {\n-    B(cx).atomic_cmpxchg(dst, cmp, src, order, failure_order, weak)\n+pub fn AtomicCmpXchg(cx: &BlockAndBuilder, dst: ValueRef,\n+    cmp: ValueRef, src: ValueRef,\n+    order: AtomicOrdering,\n+    failure_order: AtomicOrdering,\n+    weak: llvm::Bool) -> ValueRef {\n+    cx.atomic_cmpxchg(dst, cmp, src, order, failure_order, weak)\n }\n-pub fn AtomicRMW(cx: Block, op: AtomicRmwBinOp,\n-                 dst: ValueRef, src: ValueRef,\n-                 order: AtomicOrdering) -> ValueRef {\n-    B(cx).atomic_rmw(op, dst, src, order)\n+pub fn AtomicRMW(cx: &BlockAndBuilder, op: AtomicRmwBinOp,\n+    dst: ValueRef, src: ValueRef,\n+    order: AtomicOrdering) -> ValueRef {\n+    cx.atomic_rmw(op, dst, src, order)\n }\n \n-pub fn CleanupPad(cx: Block,\n-                  parent: Option<ValueRef>,\n-                  args: &[ValueRef]) -> ValueRef {\n-    check_not_terminated(cx);\n-    assert!(!cx.unreachable.get());\n-    B(cx).cleanup_pad(parent, args)\n+pub fn CleanupPad(cx: &BlockAndBuilder,\n+    parent: Option<ValueRef>,\n+    args: &[ValueRef]) -> ValueRef {\n+    assert!(!cx.is_unreachable());\n+    cx.cleanup_pad(parent, args)\n }\n \n-pub fn CleanupRet(cx: Block,\n-                  cleanup: ValueRef,\n-                  unwind: Option<BasicBlockRef>) -> ValueRef {\n-    check_not_terminated(cx);\n-    terminate(cx, \"CleanupRet\");\n-    B(cx).cleanup_ret(cleanup, unwind)\n+pub fn CleanupRet(cx: &BlockAndBuilder,\n+    cleanup: ValueRef,\n+    unwind: Option<BasicBlockRef>) -> ValueRef {\n+    cx.terminate();\n+    cx.cleanup_ret(cleanup, unwind)\n }\n \n-pub fn CatchPad(cx: Block,\n-                parent: ValueRef,\n-                args: &[ValueRef]) -> ValueRef {\n-    check_not_terminated(cx);\n-    assert!(!cx.unreachable.get());\n-    B(cx).catch_pad(parent, args)\n+pub fn CatchPad(cx: &BlockAndBuilder,\n+    parent: ValueRef,\n+    args: &[ValueRef]) -> ValueRef {\n+    assert!(!cx.is_unreachable());\n+    cx.catch_pad(parent, args)\n }\n \n-pub fn CatchRet(cx: Block, pad: ValueRef, unwind: BasicBlockRef) -> ValueRef {\n-    check_not_terminated(cx);\n-    terminate(cx, \"CatchRet\");\n-    B(cx).catch_ret(pad, unwind)\n+pub fn CatchRet(cx: &BlockAndBuilder, pad: ValueRef, unwind: BasicBlockRef) -> ValueRef {\n+    cx.terminate();\n+    cx.catch_ret(pad, unwind)\n }\n \n-pub fn CatchSwitch(cx: Block,\n-                   parent: Option<ValueRef>,\n-                   unwind: Option<BasicBlockRef>,\n-                   num_handlers: usize) -> ValueRef {\n-    check_not_terminated(cx);\n-    terminate(cx, \"CatchSwitch\");\n-    B(cx).catch_switch(parent, unwind, num_handlers)\n+pub fn CatchSwitch(cx: &BlockAndBuilder,\n+    parent: Option<ValueRef>,\n+    unwind: Option<BasicBlockRef>,\n+    num_handlers: usize) -> ValueRef {\n+    cx.terminate();\n+    cx.catch_switch(parent, unwind, num_handlers)\n }\n \n-pub fn AddHandler(cx: Block, catch_switch: ValueRef, handler: BasicBlockRef) {\n-    B(cx).add_handler(catch_switch, handler)\n+pub fn AddHandler(cx: &BlockAndBuilder, catch_switch: ValueRef, handler: BasicBlockRef) {\n+    cx.add_handler(catch_switch, handler)\n }"}, {"sha": "4f0a58e00d57d59a40b825cf219bd8433a7e894d", "filename": "src/librustc_trans/callee.rs", "status": "modified", "additions": 28, "deletions": 26, "changes": 54, "blob_url": "https://github.com/rust-lang/rust/blob/bf7d4534a7ed27f1a0b5c9b53b2af155da33f072/src%2Flibrustc_trans%2Fcallee.rs", "raw_url": "https://github.com/rust-lang/rust/raw/bf7d4534a7ed27f1a0b5c9b53b2af155da33f072/src%2Flibrustc_trans%2Fcallee.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_trans%2Fcallee.rs?ref=bf7d4534a7ed27f1a0b5c9b53b2af155da33f072", "patch": "@@ -26,7 +26,9 @@ use attributes;\n use base;\n use base::*;\n use build::*;\n-use common::{self, Block, Result, CrateContext, FunctionContext, SharedCrateContext};\n+use common::{\n+    self, Block, BlockAndBuilder, CrateContext, FunctionContext, SharedCrateContext\n+};\n use consts;\n use debuginfo::DebugLoc;\n use declare;\n@@ -207,11 +209,11 @@ impl<'tcx> Callee<'tcx> {\n     /// For non-lang items, `dest` is always Some, and hence the result is written\n     /// into memory somewhere. Nonetheless we return the actual return value of the\n     /// function.\n-    pub fn call<'a, 'blk>(self, bcx: Block<'blk, 'tcx>,\n+    pub fn call<'a, 'blk>(self, bcx: BlockAndBuilder<'blk, 'tcx>,\n                           debug_loc: DebugLoc,\n                           args: &[ValueRef],\n                           dest: Option<ValueRef>)\n-                          -> Result<'blk, 'tcx> {\n+                          -> (BlockAndBuilder<'blk, 'tcx>, ValueRef) {\n         trans_call_inner(bcx, debug_loc, self, args, dest)\n     }\n \n@@ -370,8 +372,7 @@ fn trans_fn_once_adapter_shim<'a, 'tcx>(\n     let (block_arena, fcx): (TypedArena<_>, FunctionContext);\n     block_arena = TypedArena::new();\n     fcx = FunctionContext::new(ccx, lloncefn, fn_ty, None, &block_arena);\n-    let mut bcx = fcx.init(false);\n-\n+    let bcx = fcx.init(false);\n \n     // the first argument (`self`) will be the (by value) closure env.\n \n@@ -381,9 +382,9 @@ fn trans_fn_once_adapter_shim<'a, 'tcx>(\n     let llenv = if env_arg.is_indirect() {\n         llargs[self_idx]\n     } else {\n-        let scratch = alloc_ty(bcx, closure_ty, \"self\");\n+        let scratch = alloc_ty(&bcx, closure_ty, \"self\");\n         let mut llarg_idx = self_idx;\n-        env_arg.store_fn_arg(&bcx.build(), &mut llarg_idx, scratch);\n+        env_arg.store_fn_arg(&bcx, &mut llarg_idx, scratch);\n         scratch\n     };\n \n@@ -413,11 +414,11 @@ fn trans_fn_once_adapter_shim<'a, 'tcx>(\n     let self_scope = fcx.push_custom_cleanup_scope();\n     fcx.schedule_drop_mem(self_scope, llenv, closure_ty);\n \n-    bcx = callee.call(bcx, DebugLoc::None, &llargs[self_idx..], dest).bcx;\n+    let bcx = callee.call(bcx, DebugLoc::None, &llargs[self_idx..], dest).0;\n \n-    fcx.pop_and_trans_custom_cleanup_scope(bcx, self_scope);\n+    let bcx = fcx.pop_and_trans_custom_cleanup_scope(bcx, self_scope);\n \n-    fcx.finish(bcx, DebugLoc::None);\n+    fcx.finish(&bcx, DebugLoc::None);\n \n     ccx.instances().borrow_mut().insert(method_instance, lloncefn);\n \n@@ -522,15 +523,15 @@ fn trans_fn_pointer_shim<'a, 'tcx>(\n     let (block_arena, fcx): (TypedArena<_>, FunctionContext);\n     block_arena = TypedArena::new();\n     fcx = FunctionContext::new(ccx, llfn, fn_ty, None, &block_arena);\n-    let mut bcx = fcx.init(false);\n+    let bcx = fcx.init(false);\n \n     let llargs = get_params(fcx.llfn);\n \n     let self_idx = fcx.fn_ty.ret.is_indirect() as usize;\n     let llfnpointer = llfnpointer.unwrap_or_else(|| {\n         // the first argument (`self`) will be ptr to the fn pointer\n         if is_by_ref {\n-            Load(bcx, llargs[self_idx])\n+            Load(&bcx, llargs[self_idx])\n         } else {\n             llargs[self_idx]\n         }\n@@ -542,9 +543,8 @@ fn trans_fn_pointer_shim<'a, 'tcx>(\n         data: Fn(llfnpointer),\n         ty: bare_fn_ty\n     };\n-    bcx = callee.call(bcx, DebugLoc::None, &llargs[(self_idx + 1)..], dest).bcx;\n-\n-    fcx.finish(bcx, DebugLoc::None);\n+    let bcx = callee.call(bcx, DebugLoc::None, &llargs[(self_idx + 1)..], dest).0;\n+    fcx.finish(&bcx, DebugLoc::None);\n \n     ccx.fn_pointer_shims().borrow_mut().insert(bare_fn_ty_maybe_ref, llfn);\n \n@@ -653,20 +653,20 @@ fn get_fn<'a, 'tcx>(ccx: &CrateContext<'a, 'tcx>,\n // ______________________________________________________________________\n // Translating calls\n \n-fn trans_call_inner<'a, 'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n+fn trans_call_inner<'a, 'blk, 'tcx>(bcx: BlockAndBuilder<'blk, 'tcx>,\n                                     debug_loc: DebugLoc,\n                                     callee: Callee<'tcx>,\n                                     args: &[ValueRef],\n                                     opt_llretslot: Option<ValueRef>)\n-                                    -> Result<'blk, 'tcx> {\n+                                    -> (BlockAndBuilder<'blk, 'tcx>, ValueRef) {\n     // Introduce a temporary cleanup scope that will contain cleanups\n     // for the arguments while they are being evaluated. The purpose\n     // this cleanup is to ensure that, should a panic occur while\n     // evaluating argument N, the values for arguments 0...N-1 are all\n     // cleaned up. If no panic occurs, the values are handed off to\n     // the callee, and hence none of the cleanups in this temporary\n     // scope will ever execute.\n-    let fcx = bcx.fcx;\n+    let fcx = &bcx.fcx();\n     let ccx = fcx.ccx;\n \n     let fn_ret = callee.ty.fn_ret();\n@@ -689,7 +689,7 @@ fn trans_call_inner<'a, 'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n     if fn_ty.ret.is_indirect() {\n         let mut llretslot = opt_llretslot.unwrap();\n         if let Some(ty) = fn_ty.ret.cast {\n-            llretslot = PointerCast(bcx, llretslot, ty.ptr_to());\n+            llretslot = PointerCast(&bcx, llretslot, ty.ptr_to());\n         }\n         llargs.push(llretslot);\n     }\n@@ -698,9 +698,9 @@ fn trans_call_inner<'a, 'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n         Virtual(idx) => {\n             llargs.push(args[0]);\n \n-            let fn_ptr = meth::get_virtual_method(bcx, args[1], idx);\n-            let llty = fn_ty.llvm_type(bcx.ccx()).ptr_to();\n-            callee = Fn(PointerCast(bcx, fn_ptr, llty));\n+            let fn_ptr = meth::get_virtual_method(&bcx, args[1], idx);\n+            let llty = fn_ty.llvm_type(&bcx.ccx()).ptr_to();\n+            callee = Fn(PointerCast(&bcx, fn_ptr, llty));\n             llargs.extend_from_slice(&args[2..]);\n         }\n         _ => llargs.extend_from_slice(args)\n@@ -712,7 +712,7 @@ fn trans_call_inner<'a, 'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n     };\n \n     let (llret, bcx) = base::invoke(bcx, llfn, &llargs, debug_loc);\n-    if !bcx.unreachable.get() {\n+    if !bcx.is_unreachable() {\n         fn_ty.apply_attrs_callsite(llret);\n \n         // If the function we just called does not use an outpointer,\n@@ -722,14 +722,16 @@ fn trans_call_inner<'a, 'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n         // u64.\n         if !fn_ty.ret.is_indirect() {\n             if let Some(llretslot) = opt_llretslot {\n-                fn_ty.ret.store(&bcx.build(), llret, llretslot);\n+                fn_ty.ret.store(&bcx, llret, llretslot);\n             }\n         }\n     }\n \n     if fn_ret.0.is_never() {\n-        Unreachable(bcx);\n+        assert!(!bcx.is_terminated());\n+        bcx.set_unreachable();\n+        bcx.unreachable();\n     }\n \n-    Result::new(bcx, llret)\n+    (bcx, llret)\n }"}, {"sha": "db74e57dd8884633ed0cb9f50c5879fa99a173cf", "filename": "src/librustc_trans/cleanup.rs", "status": "modified", "additions": 38, "deletions": 38, "changes": 76, "blob_url": "https://github.com/rust-lang/rust/blob/bf7d4534a7ed27f1a0b5c9b53b2af155da33f072/src%2Flibrustc_trans%2Fcleanup.rs", "raw_url": "https://github.com/rust-lang/rust/raw/bf7d4534a7ed27f1a0b5c9b53b2af155da33f072/src%2Flibrustc_trans%2Fcleanup.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_trans%2Fcleanup.rs?ref=bf7d4534a7ed27f1a0b5c9b53b2af155da33f072", "patch": "@@ -120,7 +120,7 @@ use llvm::{BasicBlockRef, ValueRef};\n use base;\n use build;\n use common;\n-use common::{Block, FunctionContext, LandingPad};\n+use common::{BlockAndBuilder, FunctionContext, LandingPad};\n use debuginfo::{DebugLoc};\n use glue;\n use type_::Type;\n@@ -190,9 +190,9 @@ impl<'blk, 'tcx> FunctionContext<'blk, 'tcx> {\n     /// Removes the top cleanup scope from the stack, which must be a temporary scope, and\n     /// generates the code to do its cleanups for normal exit.\n     pub fn pop_and_trans_custom_cleanup_scope(&self,\n-                                              bcx: Block<'blk, 'tcx>,\n+                                              bcx: BlockAndBuilder<'blk, 'tcx>,\n                                               custom_scope: CustomScopeIndex)\n-                                              -> Block<'blk, 'tcx> {\n+                                              -> BlockAndBuilder<'blk, 'tcx> {\n         debug!(\"pop_and_trans_custom_cleanup_scope({:?})\", custom_scope);\n         assert!(self.is_valid_to_pop_custom_scope(custom_scope));\n \n@@ -339,11 +339,11 @@ impl<'blk, 'tcx> FunctionContext<'blk, 'tcx> {\n \n     /// Generates the cleanups for `scope` into `bcx`\n     fn trans_scope_cleanups(&self, // cannot borrow self, will recurse\n-                            bcx: Block<'blk, 'tcx>,\n-                            scope: &CleanupScope<'tcx>) -> Block<'blk, 'tcx> {\n+                            bcx: BlockAndBuilder<'blk, 'tcx>,\n+                            scope: &CleanupScope<'tcx>) -> BlockAndBuilder<'blk, 'tcx> {\n \n         let mut bcx = bcx;\n-        if !bcx.unreachable.get() {\n+        if !bcx.is_unreachable() {\n             for cleanup in scope.cleanups.iter().rev() {\n                 bcx = cleanup.trans(bcx, scope.debug_loc);\n             }\n@@ -419,21 +419,21 @@ impl<'blk, 'tcx> FunctionContext<'blk, 'tcx> {\n                     UnwindExit(val) => {\n                         // Generate a block that will resume unwinding to the\n                         // calling function\n-                        let bcx = self.new_block(\"resume\");\n+                        let bcx = self.new_block(\"resume\").build();\n                         match val {\n                             UnwindKind::LandingPad => {\n                                 let addr = self.landingpad_alloca.get()\n                                                .unwrap();\n-                                let lp = build::Load(bcx, addr);\n-                                base::call_lifetime_end(bcx, addr);\n-                                base::trans_unwind_resume(bcx, lp);\n+                                let lp = build::Load(&bcx, addr);\n+                                base::call_lifetime_end(&bcx, addr);\n+                                base::trans_unwind_resume(&bcx, lp);\n                             }\n                             UnwindKind::CleanupPad(_) => {\n-                                let pad = build::CleanupPad(bcx, None, &[]);\n-                                build::CleanupRet(bcx, pad, None);\n+                                let pad = build::CleanupPad(&bcx, None, &[]);\n+                                build::CleanupRet(&bcx, pad, None);\n                             }\n                         }\n-                        prev_llbb = bcx.llbb;\n+                        prev_llbb = bcx.llbb();\n                         break;\n                     }\n                 }\n@@ -484,16 +484,17 @@ impl<'blk, 'tcx> FunctionContext<'blk, 'tcx> {\n                 let name = scope.block_name(\"clean\");\n                 debug!(\"generating cleanups for {}\", name);\n \n-                let bcx_in = self.new_block(&name[..]);\n-                let exit_label = label.start(bcx_in);\n+                let bcx_in = self.new_block(&name[..]).build();\n+                let exit_label = label.start(&bcx_in);\n+                let next_llbb = bcx_in.llbb();\n                 let mut bcx_out = bcx_in;\n                 let len = scope.cleanups.len();\n                 for cleanup in scope.cleanups.iter().rev().take(len - skip) {\n                     bcx_out = cleanup.trans(bcx_out, scope.debug_loc);\n                 }\n                 skip = 0;\n-                exit_label.branch(bcx_out, prev_llbb);\n-                prev_llbb = bcx_in.llbb;\n+                exit_label.branch(&bcx_out, prev_llbb);\n+                prev_llbb = next_llbb;\n \n                 scope.add_cached_early_exit(exit_label, prev_llbb, len);\n             }\n@@ -527,22 +528,22 @@ impl<'blk, 'tcx> FunctionContext<'blk, 'tcx> {\n                 Some(llbb) => return llbb,\n                 None => {\n                     let name = last_scope.block_name(\"unwind\");\n-                    pad_bcx = self.new_block(&name[..]);\n-                    last_scope.cached_landing_pad = Some(pad_bcx.llbb);\n+                    pad_bcx = self.new_block(&name[..]).build();\n+                    last_scope.cached_landing_pad = Some(pad_bcx.llbb());\n                 }\n             }\n         };\n \n-        let llpersonality = pad_bcx.fcx.eh_personality();\n+        let llpersonality = pad_bcx.fcx().eh_personality();\n \n         let val = if base::wants_msvc_seh(self.ccx.sess()) {\n             // A cleanup pad requires a personality function to be specified, so\n             // we do that here explicitly (happens implicitly below through\n             // creation of the landingpad instruction). We then create a\n             // cleanuppad instruction which has no filters to run cleanup on all\n             // exceptions.\n-            build::SetPersonalityFn(pad_bcx, llpersonality);\n-            let llretval = build::CleanupPad(pad_bcx, None, &[]);\n+            build::SetPersonalityFn(&pad_bcx, llpersonality);\n+            let llretval = build::CleanupPad(&pad_bcx, None, &[]);\n             UnwindKind::CleanupPad(llretval)\n         } else {\n             // The landing pad return type (the type being propagated). Not sure\n@@ -553,31 +554,31 @@ impl<'blk, 'tcx> FunctionContext<'blk, 'tcx> {\n                                         false);\n \n             // The only landing pad clause will be 'cleanup'\n-            let llretval = build::LandingPad(pad_bcx, llretty, llpersonality, 1);\n+            let llretval = build::LandingPad(&pad_bcx, llretty, llpersonality, 1);\n \n             // The landing pad block is a cleanup\n-            build::SetCleanup(pad_bcx, llretval);\n+            build::SetCleanup(&pad_bcx, llretval);\n \n             let addr = match self.landingpad_alloca.get() {\n                 Some(addr) => addr,\n                 None => {\n-                    let addr = base::alloca(pad_bcx, common::val_ty(llretval),\n+                    let addr = base::alloca(&pad_bcx, common::val_ty(llretval),\n                                             \"\");\n-                    base::call_lifetime_start(pad_bcx, addr);\n+                    base::call_lifetime_start(&pad_bcx, addr);\n                     self.landingpad_alloca.set(Some(addr));\n                     addr\n                 }\n             };\n-            build::Store(pad_bcx, llretval, addr);\n+            build::Store(&pad_bcx, llretval, addr);\n             UnwindKind::LandingPad\n         };\n \n         // Generate the cleanup block and branch to it.\n         let label = UnwindExit(val);\n         let cleanup_llbb = self.trans_cleanups_to_exit_scope(label);\n-        label.branch(pad_bcx, cleanup_llbb);\n+        label.branch(&pad_bcx, cleanup_llbb);\n \n-        return pad_bcx.llbb;\n+        return pad_bcx.llbb();\n     }\n }\n \n@@ -628,7 +629,7 @@ impl EarlyExitLabel {\n     /// Transitions from an exit label to other exit labels depend on the type\n     /// of label. For example with MSVC exceptions unwind exit labels will use\n     /// the `cleanupret` instruction instead of the `br` instruction.\n-    fn branch(&self, from_bcx: Block, to_llbb: BasicBlockRef) {\n+    fn branch(&self, from_bcx: &BlockAndBuilder, to_llbb: BasicBlockRef) {\n         if let UnwindExit(UnwindKind::CleanupPad(pad)) = *self {\n             build::CleanupRet(from_bcx, pad, Some(to_llbb));\n         } else {\n@@ -647,15 +648,15 @@ impl EarlyExitLabel {\n     ///\n     /// Returns a new label which will can be used to cache `bcx` in the list of\n     /// early exits.\n-    fn start(&self, bcx: Block) -> EarlyExitLabel {\n+    fn start(&self, bcx: &BlockAndBuilder) -> EarlyExitLabel {\n         match *self {\n             UnwindExit(UnwindKind::CleanupPad(..)) => {\n                 let pad = build::CleanupPad(bcx, None, &[]);\n-                bcx.lpad.set(Some(bcx.fcx.lpad_arena.alloc(LandingPad::msvc(pad))));\n+                bcx.set_lpad_ref(Some(bcx.fcx().lpad_arena.alloc(LandingPad::msvc(pad))));\n                 UnwindExit(UnwindKind::CleanupPad(pad))\n             }\n             UnwindExit(UnwindKind::LandingPad) => {\n-                bcx.lpad.set(Some(bcx.fcx.lpad_arena.alloc(LandingPad::gnu())));\n+                bcx.set_lpad_ref(Some(bcx.fcx().lpad_arena.alloc(LandingPad::gnu())));\n                 *self\n             }\n         }\n@@ -685,20 +686,19 @@ pub struct DropValue<'tcx> {\n \n impl<'tcx> DropValue<'tcx> {\n     fn trans<'blk>(&self,\n-                   bcx: Block<'blk, 'tcx>,\n+                   bcx: BlockAndBuilder<'blk, 'tcx>,\n                    debug_loc: DebugLoc)\n-                   -> Block<'blk, 'tcx> {\n+                   -> BlockAndBuilder<'blk, 'tcx> {\n         let skip_dtor = self.skip_dtor;\n         let _icx = if skip_dtor {\n             base::push_ctxt(\"<DropValue as Cleanup>::trans skip_dtor=true\")\n         } else {\n             base::push_ctxt(\"<DropValue as Cleanup>::trans skip_dtor=false\")\n         };\n-        let bcx = if self.is_immediate {\n+        if self.is_immediate {\n             glue::drop_ty_immediate(bcx, self.val, self.ty, debug_loc, self.skip_dtor)\n         } else {\n             glue::drop_ty_core(bcx, self.val, self.ty, debug_loc, self.skip_dtor)\n-        };\n-        bcx\n+        }\n     }\n }"}, {"sha": "1650d7376bfbcd23886847f0cea29668bca555e1", "filename": "src/librustc_trans/common.rs", "status": "modified", "additions": 25, "deletions": 21, "changes": 46, "blob_url": "https://github.com/rust-lang/rust/blob/bf7d4534a7ed27f1a0b5c9b53b2af155da33f072/src%2Flibrustc_trans%2Fcommon.rs", "raw_url": "https://github.com/rust-lang/rust/raw/bf7d4534a7ed27f1a0b5c9b53b2af155da33f072/src%2Flibrustc_trans%2Fcommon.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_trans%2Fcommon.rs?ref=bf7d4534a7ed27f1a0b5c9b53b2af155da33f072", "patch": "@@ -441,6 +441,7 @@ impl<'a, 'tcx> FunctionContext<'a, 'tcx> {\n // code.  Each basic block we generate is attached to a function, typically\n // with many basic blocks per function.  All the basic blocks attached to a\n // function are organized as a directed graph.\n+#[must_use]\n pub struct BlockS<'blk, 'tcx: 'blk> {\n     // The BasicBlockRef returned from a call to\n     // llvm::LLVMAppendBasicBlock(llfn, name), which adds a basic\n@@ -555,6 +556,7 @@ impl<'blk, 'tcx> Drop for OwnedBuilder<'blk, 'tcx> {\n     }\n }\n \n+#[must_use]\n pub struct BlockAndBuilder<'blk, 'tcx: 'blk> {\n     bcx: Block<'blk, 'tcx>,\n     owned_builder: OwnedBuilder<'blk, 'tcx>,\n@@ -597,10 +599,24 @@ impl<'blk, 'tcx> BlockAndBuilder<'blk, 'tcx> {\n \n     // Methods delegated to bcx\n \n+    pub fn terminate(&self) {\n+        debug!(\"terminate({})\", self.bcx.to_str());\n+        self.bcx.terminated.set(true);\n+    }\n+\n+    pub fn set_unreachable(&self) {\n+        debug!(\"set_unreachable({})\", self.bcx.to_str());\n+        self.bcx.unreachable.set(true);\n+    }\n+\n     pub fn is_unreachable(&self) -> bool {\n         self.bcx.unreachable.get()\n     }\n \n+    pub fn is_terminated(&self) -> bool {\n+        self.bcx.terminated.get()\n+    }\n+\n     pub fn ccx(&self) -> &'blk CrateContext<'blk, 'tcx> {\n         self.bcx.ccx()\n     }\n@@ -700,20 +716,6 @@ impl Clone for LandingPad {\n     }\n }\n \n-pub struct Result<'blk, 'tcx: 'blk> {\n-    pub bcx: Block<'blk, 'tcx>,\n-    pub val: ValueRef\n-}\n-\n-impl<'b, 'tcx> Result<'b, 'tcx> {\n-    pub fn new(bcx: Block<'b, 'tcx>, val: ValueRef) -> Result<'b, 'tcx> {\n-        Result {\n-            bcx: bcx,\n-            val: val,\n-        }\n-    }\n-}\n-\n pub fn val_ty(v: ValueRef) -> Type {\n     unsafe {\n         Type::from_ref(llvm::LLVMTypeOf(v))\n@@ -1016,7 +1018,7 @@ pub fn langcall(tcx: TyCtxt,\n // all shifts). For 32- and 64-bit types, this matches the semantics\n // of Java. (See related discussion on #1877 and #10183.)\n \n-pub fn build_unchecked_lshift<'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n+pub fn build_unchecked_lshift<'blk, 'tcx>(bcx: &BlockAndBuilder<'blk, 'tcx>,\n                                           lhs: ValueRef,\n                                           rhs: ValueRef,\n                                           binop_debug_loc: DebugLoc) -> ValueRef {\n@@ -1026,7 +1028,7 @@ pub fn build_unchecked_lshift<'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n     build::Shl(bcx, lhs, rhs, binop_debug_loc)\n }\n \n-pub fn build_unchecked_rshift<'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n+pub fn build_unchecked_rshift<'blk, 'tcx>(bcx: &BlockAndBuilder<'blk, 'tcx>,\n                                           lhs_t: Ty<'tcx>,\n                                           lhs: ValueRef,\n                                           rhs: ValueRef,\n@@ -1042,17 +1044,19 @@ pub fn build_unchecked_rshift<'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n     }\n }\n \n-fn shift_mask_rhs<'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n+fn shift_mask_rhs<'blk, 'tcx>(bcx: &BlockAndBuilder<'blk, 'tcx>,\n                               rhs: ValueRef,\n                               debug_loc: DebugLoc) -> ValueRef {\n     let rhs_llty = val_ty(rhs);\n     build::And(bcx, rhs, shift_mask_val(bcx, rhs_llty, rhs_llty, false), debug_loc)\n }\n \n-pub fn shift_mask_val<'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n-                              llty: Type,\n-                              mask_llty: Type,\n-                              invert: bool) -> ValueRef {\n+pub fn shift_mask_val<'blk, 'tcx>(\n+    bcx: &BlockAndBuilder<'blk, 'tcx>,\n+    llty: Type,\n+    mask_llty: Type,\n+    invert: bool\n+) -> ValueRef {\n     let kind = llty.kind();\n     match kind {\n         TypeKind::Integer => {"}, {"sha": "f59ecf1d6782fbd6c7d1dc937cf175a0decdadcf", "filename": "src/librustc_trans/debuginfo/mod.rs", "status": "modified", "additions": 5, "deletions": 5, "changes": 10, "blob_url": "https://github.com/rust-lang/rust/blob/bf7d4534a7ed27f1a0b5c9b53b2af155da33f072/src%2Flibrustc_trans%2Fdebuginfo%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/bf7d4534a7ed27f1a0b5c9b53b2af155da33f072/src%2Flibrustc_trans%2Fdebuginfo%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_trans%2Fdebuginfo%2Fmod.rs?ref=bf7d4534a7ed27f1a0b5c9b53b2af155da33f072", "patch": "@@ -27,7 +27,7 @@ use rustc::hir::def_id::DefId;\n use rustc::ty::subst::Substs;\n \n use abi::Abi;\n-use common::{CrateContext, FunctionContext, Block, BlockAndBuilder};\n+use common::{CrateContext, FunctionContext, BlockAndBuilder};\n use monomorphize::{self, Instance};\n use rustc::ty::{self, Ty};\n use rustc::mir;\n@@ -441,7 +441,7 @@ pub fn create_function_debug_context<'a, 'tcx>(cx: &CrateContext<'a, 'tcx>,\n     }\n }\n \n-pub fn declare_local<'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n+pub fn declare_local<'blk, 'tcx>(bcx: &BlockAndBuilder<'blk, 'tcx>,\n                                  variable_name: ast::Name,\n                                  variable_type: Ty<'tcx>,\n                                  scope_metadata: DIScope,\n@@ -494,16 +494,16 @@ pub fn declare_local<'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n                     address_operations.as_ptr(),\n                     address_operations.len() as c_uint,\n                     debug_loc,\n-                    bcx.llbb);\n+                    bcx.llbb());\n \n-                llvm::LLVMSetInstDebugLocation(::build::B(bcx).llbuilder, instr);\n+                llvm::LLVMSetInstDebugLocation(bcx.llbuilder, instr);\n             }\n         }\n     }\n \n     match variable_kind {\n         ArgumentVariable(_) | CapturedVariable => {\n-            assert!(!bcx.fcx\n+            assert!(!bcx.fcx()\n                         .debug_context\n                         .get_ref(span)\n                         .source_locations_enabled"}, {"sha": "a1e18725704f2ed02fe3a61ffaafba1ccbed3e3a", "filename": "src/librustc_trans/glue.rs", "status": "modified", "additions": 84, "deletions": 86, "changes": 170, "blob_url": "https://github.com/rust-lang/rust/blob/bf7d4534a7ed27f1a0b5c9b53b2af155da33f072/src%2Flibrustc_trans%2Fglue.rs", "raw_url": "https://github.com/rust-lang/rust/raw/bf7d4534a7ed27f1a0b5c9b53b2af155da33f072/src%2Flibrustc_trans%2Fglue.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_trans%2Fglue.rs?ref=bf7d4534a7ed27f1a0b5c9b53b2af155da33f072", "patch": "@@ -38,38 +38,39 @@ use Disr;\n use arena::TypedArena;\n use syntax_pos::DUMMY_SP;\n \n-pub fn trans_exchange_free_dyn<'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n+pub fn trans_exchange_free_dyn<'blk, 'tcx>(bcx: BlockAndBuilder<'blk, 'tcx>,\n                                            v: ValueRef,\n                                            size: ValueRef,\n                                            align: ValueRef,\n                                            debug_loc: DebugLoc)\n-                                           -> Block<'blk, 'tcx> {\n+                                           -> BlockAndBuilder<'blk, 'tcx> {\n     let _icx = push_ctxt(\"trans_exchange_free\");\n \n     let def_id = langcall(bcx.tcx(), None, \"\", ExchangeFreeFnLangItem);\n-    let args = [PointerCast(bcx, v, Type::i8p(bcx.ccx())), size, align];\n+    let args = [PointerCast(&bcx, v, Type::i8p(bcx.ccx())), size, align];\n     Callee::def(bcx.ccx(), def_id, bcx.tcx().intern_substs(&[]))\n-        .call(bcx, debug_loc, &args, None).bcx\n+        .call(bcx, debug_loc, &args, None).0\n }\n \n-pub fn trans_exchange_free<'blk, 'tcx>(cx: Block<'blk, 'tcx>,\n+pub fn trans_exchange_free<'blk, 'tcx>(cx: BlockAndBuilder<'blk, 'tcx>,\n                                        v: ValueRef,\n                                        size: u64,\n                                        align: u32,\n                                        debug_loc: DebugLoc)\n-                                       -> Block<'blk, 'tcx> {\n+                                       -> BlockAndBuilder<'blk, 'tcx> {\n+    let ccx = cx.ccx();\n     trans_exchange_free_dyn(cx,\n                             v,\n-                            C_uint(cx.ccx(), size),\n-                            C_uint(cx.ccx(), align),\n+                            C_uint(ccx, size),\n+                            C_uint(ccx, align),\n                             debug_loc)\n }\n \n-pub fn trans_exchange_free_ty<'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n+pub fn trans_exchange_free_ty<'blk, 'tcx>(bcx: BlockAndBuilder<'blk, 'tcx>,\n                                           ptr: ValueRef,\n                                           content_ty: Ty<'tcx>,\n                                           debug_loc: DebugLoc)\n-                                          -> Block<'blk, 'tcx> {\n+                                          -> BlockAndBuilder<'blk, 'tcx> {\n     assert!(type_is_sized(bcx.ccx().tcx(), content_ty));\n     let sizing_type = sizing_type_of(bcx.ccx(), content_ty);\n     let content_size = llsize_of_alloc(bcx.ccx(), sizing_type);\n@@ -129,23 +130,23 @@ pub fn get_drop_glue_type<'a, 'tcx>(tcx: TyCtxt<'a, 'tcx, 'tcx>,\n     }\n }\n \n-pub fn drop_ty<'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n+pub fn drop_ty<'blk, 'tcx>(bcx: BlockAndBuilder<'blk, 'tcx>,\n                            v: ValueRef,\n                            t: Ty<'tcx>,\n-                           debug_loc: DebugLoc) -> Block<'blk, 'tcx> {\n+                           debug_loc: DebugLoc) -> BlockAndBuilder<'blk, 'tcx> {\n     drop_ty_core(bcx, v, t, debug_loc, false)\n }\n \n-pub fn drop_ty_core<'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n+pub fn drop_ty_core<'blk, 'tcx>(bcx: BlockAndBuilder<'blk, 'tcx>,\n                                 v: ValueRef,\n                                 t: Ty<'tcx>,\n                                 debug_loc: DebugLoc,\n                                 skip_dtor: bool)\n-                                -> Block<'blk, 'tcx> {\n+                                -> BlockAndBuilder<'blk, 'tcx> {\n     // NB: v is an *alias* of type t here, not a direct value.\n     debug!(\"drop_ty_core(t={:?}, skip_dtor={})\", t, skip_dtor);\n     let _icx = push_ctxt(\"drop_ty\");\n-    if bcx.fcx.type_needs_drop(t) {\n+    if bcx.fcx().type_needs_drop(t) {\n         let ccx = bcx.ccx();\n         let g = if skip_dtor {\n             DropGlueKind::TyContents(t)\n@@ -155,29 +156,29 @@ pub fn drop_ty_core<'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n         let glue = get_drop_glue_core(ccx, g);\n         let glue_type = get_drop_glue_type(ccx.tcx(), t);\n         let ptr = if glue_type != t {\n-            PointerCast(bcx, v, type_of(ccx, glue_type).ptr_to())\n+            PointerCast(&bcx, v, type_of(ccx, glue_type).ptr_to())\n         } else {\n             v\n         };\n \n         // No drop-hint ==> call standard drop glue\n-        Call(bcx, glue, &[ptr], debug_loc);\n+        Call(&bcx, glue, &[ptr], debug_loc);\n     }\n     bcx\n }\n \n-pub fn drop_ty_immediate<'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n+pub fn drop_ty_immediate<'blk, 'tcx>(bcx: BlockAndBuilder<'blk, 'tcx>,\n                                      v: ValueRef,\n                                      t: Ty<'tcx>,\n                                      debug_loc: DebugLoc,\n                                      skip_dtor: bool)\n-                                     -> Block<'blk, 'tcx> {\n+                                     -> BlockAndBuilder<'blk, 'tcx> {\n     let _icx = push_ctxt(\"drop_ty_immediate\");\n-    let vp = alloc_ty(bcx, t, \"\");\n-    call_lifetime_start(bcx, vp);\n-    store_ty(bcx, v, vp, t);\n+    let vp = alloc_ty(&bcx, t, \"\");\n+    call_lifetime_start(&bcx, vp);\n+    store_ty(&bcx, v, vp, t);\n     let bcx = drop_ty_core(bcx, vp, t, debug_loc, skip_dtor);\n-    call_lifetime_end(bcx, vp);\n+    call_lifetime_end(&bcx, vp);\n     bcx\n }\n \n@@ -248,14 +249,14 @@ pub fn implement_drop_glue<'a, 'tcx>(ccx: &CrateContext<'a, 'tcx>,\n     // type, so we don't need to explicitly cast the function parameter.\n \n     let bcx = make_drop_glue(bcx, get_param(llfn, 0), g);\n-    fcx.finish(bcx, DebugLoc::None);\n+    fcx.finish(&bcx, DebugLoc::None);\n }\n \n-fn trans_custom_dtor<'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n+fn trans_custom_dtor<'blk, 'tcx>(bcx: BlockAndBuilder<'blk, 'tcx>,\n                                  t: Ty<'tcx>,\n                                  v0: ValueRef,\n                                  shallow_drop: bool)\n-                                 -> Block<'blk, 'tcx>\n+                                 -> BlockAndBuilder<'blk, 'tcx>\n {\n     debug!(\"trans_custom_dtor t: {}\", t);\n     let tcx = bcx.tcx();\n@@ -269,12 +270,12 @@ fn trans_custom_dtor<'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n     //\n     // FIXME (#14875) panic-in-drop semantics might be unsupported; we\n     // might well consider changing below to more direct code.\n-    let contents_scope = bcx.fcx.push_custom_cleanup_scope();\n+    let contents_scope = bcx.fcx().push_custom_cleanup_scope();\n \n     // Issue #23611: schedule cleanup of contents, re-inspecting the\n     // discriminant (if any) in case of variant swap in drop code.\n     if !shallow_drop {\n-        bcx.fcx.schedule_drop_adt_contents(contents_scope, v0, t);\n+        bcx.fcx().schedule_drop_adt_contents(contents_scope, v0, t);\n     }\n \n     let (sized_args, unsized_args);\n@@ -284,8 +285,8 @@ fn trans_custom_dtor<'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n     } else {\n         // FIXME(#36457) -- we should pass unsized values to drop glue as two arguments\n         unsized_args = [\n-            Load(bcx, get_dataptr(bcx, v0)),\n-            Load(bcx, get_meta(bcx, v0))\n+            Load(&bcx, get_dataptr(&bcx, v0)),\n+            Load(&bcx, get_meta(&bcx, v0))\n         ];\n         &unsized_args\n     };\n@@ -300,9 +301,9 @@ fn trans_custom_dtor<'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n     };\n     let dtor_did = def.destructor().unwrap();\n     bcx = Callee::def(bcx.ccx(), dtor_did, vtbl.substs)\n-        .call(bcx, DebugLoc::None, args, None).bcx;\n+        .call(bcx, DebugLoc::None, args, None).0;\n \n-    bcx.fcx.pop_and_trans_custom_cleanup_scope(bcx, contents_scope)\n+    bcx.fcx().pop_and_trans_custom_cleanup_scope(bcx, contents_scope)\n }\n \n pub fn size_and_align_of_dst<'blk, 'tcx>(bcx: &BlockAndBuilder<'blk, 'tcx>,\n@@ -416,10 +417,10 @@ pub fn size_and_align_of_dst<'blk, 'tcx>(bcx: &BlockAndBuilder<'blk, 'tcx>,\n     }\n }\n \n-fn make_drop_glue<'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n+fn make_drop_glue<'blk, 'tcx>(bcx: BlockAndBuilder<'blk, 'tcx>,\n                               v0: ValueRef,\n                               g: DropGlueKind<'tcx>)\n-                              -> Block<'blk, 'tcx> {\n+                              -> BlockAndBuilder<'blk, 'tcx> {\n     let t = g.ty();\n \n     let skip_dtor = match g { DropGlueKind::Ty(_) => false, DropGlueKind::TyContents(_) => true };\n@@ -438,27 +439,28 @@ fn make_drop_glue<'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n             // a safe-guard, assert TyBox not used with TyContents.\n             assert!(!skip_dtor);\n             if !type_is_sized(bcx.tcx(), content_ty) {\n-                let llval = get_dataptr(bcx, v0);\n-                let llbox = Load(bcx, llval);\n+                let llval = get_dataptr(&bcx, v0);\n+                let llbox = Load(&bcx, llval);\n                 let bcx = drop_ty(bcx, v0, content_ty, DebugLoc::None);\n                 // FIXME(#36457) -- we should pass unsized values to drop glue as two arguments\n-                let info = get_meta(bcx, v0);\n-                let info = Load(bcx, info);\n-                let (llsize, llalign) =\n-                    size_and_align_of_dst(&bcx.build(), content_ty, info);\n+                let info = get_meta(&bcx, v0);\n+                let info = Load(&bcx, info);\n+                let (llsize, llalign) = size_and_align_of_dst(&bcx, content_ty, info);\n \n                 // `Box<ZeroSizeType>` does not allocate.\n-                let needs_free = ICmp(bcx,\n-                                        llvm::IntNE,\n-                                        llsize,\n-                                        C_uint(bcx.ccx(), 0u64),\n-                                        DebugLoc::None);\n+                let needs_free = ICmp(\n+                    &bcx,\n+                    llvm::IntNE,\n+                    llsize,\n+                    C_uint(bcx.ccx(), 0u64),\n+                    DebugLoc::None\n+                );\n                 with_cond(bcx, needs_free, |bcx| {\n                     trans_exchange_free_dyn(bcx, llbox, llsize, llalign, DebugLoc::None)\n                 })\n             } else {\n                 let llval = v0;\n-                let llbox = Load(bcx, llval);\n+                let llbox = Load(&bcx, llval);\n                 let bcx = drop_ty(bcx, llbox, content_ty, DebugLoc::None);\n                 trans_exchange_free_ty(bcx, llbox, content_ty, DebugLoc::None)\n             }\n@@ -469,12 +471,12 @@ fn make_drop_glue<'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n             // okay with always calling the Drop impl, if any.\n             // FIXME(#36457) -- we should pass unsized values to drop glue as two arguments\n             assert!(!skip_dtor);\n-            let data_ptr = get_dataptr(bcx, v0);\n-            let vtable_ptr = Load(bcx, get_meta(bcx, v0));\n-            let dtor = Load(bcx, vtable_ptr);\n-            Call(bcx,\n+            let data_ptr = get_dataptr(&bcx, v0);\n+            let vtable_ptr = Load(&bcx, get_meta(&bcx, v0));\n+            let dtor = Load(&bcx, vtable_ptr);\n+            Call(&bcx,\n                  dtor,\n-                 &[PointerCast(bcx, Load(bcx, data_ptr), Type::i8p(bcx.ccx()))],\n+                 &[PointerCast(&bcx, Load(&bcx, data_ptr), Type::i8p(bcx.ccx()))],\n                  DebugLoc::None);\n             bcx\n         }\n@@ -485,7 +487,7 @@ fn make_drop_glue<'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n             bcx\n         }\n         _ => {\n-            if bcx.fcx.type_needs_drop(t) {\n+            if bcx.fcx().type_needs_drop(t) {\n                 drop_structural_ty(bcx, v0, t)\n             } else {\n                 bcx\n@@ -495,27 +497,26 @@ fn make_drop_glue<'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n }\n \n // Iterates through the elements of a structural type, dropping them.\n-fn drop_structural_ty<'blk, 'tcx>(cx: Block<'blk, 'tcx>,\n+fn drop_structural_ty<'blk, 'tcx>(cx: BlockAndBuilder<'blk, 'tcx>,\n                                   av: ValueRef,\n                                   t: Ty<'tcx>)\n-                                  -> Block<'blk, 'tcx> {\n+                                  -> BlockAndBuilder<'blk, 'tcx> {\n     let _icx = push_ctxt(\"drop_structural_ty\");\n \n-    fn iter_variant<'blk, 'tcx>(cx: Block<'blk, 'tcx>,\n+    fn iter_variant<'blk, 'tcx>(cx: BlockAndBuilder<'blk, 'tcx>,\n                                 t: Ty<'tcx>,\n                                 av: adt::MaybeSizedValue,\n                                 variant: &'tcx ty::VariantDef,\n                                 substs: &Substs<'tcx>)\n-                                -> Block<'blk, 'tcx> {\n+                                -> BlockAndBuilder<'blk, 'tcx> {\n         let _icx = push_ctxt(\"iter_variant\");\n         let tcx = cx.tcx();\n         let mut cx = cx;\n \n         for (i, field) in variant.fields.iter().enumerate() {\n             let arg = monomorphize::field_ty(tcx, substs, field);\n-            cx = drop_ty(cx,\n-                         adt::trans_field_ptr(cx, t, av, Disr::from(variant.disr_val), i),\n-                         arg, DebugLoc::None);\n+            let field_ptr = adt::trans_field_ptr(&cx, t, av, Disr::from(variant.disr_val), i);\n+            cx = drop_ty(cx, field_ptr, arg, DebugLoc::None);\n         }\n         return cx;\n     }\n@@ -524,21 +525,21 @@ fn drop_structural_ty<'blk, 'tcx>(cx: Block<'blk, 'tcx>,\n         adt::MaybeSizedValue::sized(av)\n     } else {\n         // FIXME(#36457) -- we should pass unsized values as two arguments\n-        let data = Load(cx, get_dataptr(cx, av));\n-        let info = Load(cx, get_meta(cx, av));\n+        let data = Load(&cx, get_dataptr(&cx, av));\n+        let info = Load(&cx, get_meta(&cx, av));\n         adt::MaybeSizedValue::unsized_(data, info)\n     };\n \n     let mut cx = cx;\n     match t.sty {\n         ty::TyClosure(def_id, substs) => {\n             for (i, upvar_ty) in substs.upvar_tys(def_id, cx.tcx()).enumerate() {\n-                let llupvar = adt::trans_field_ptr(cx, t, value, Disr(0), i);\n+                let llupvar = adt::trans_field_ptr(&cx, t, value, Disr(0), i);\n                 cx = drop_ty(cx, llupvar, upvar_ty, DebugLoc::None);\n             }\n         }\n         ty::TyArray(_, n) => {\n-            let base = get_dataptr(cx, value.value);\n+            let base = get_dataptr(&cx, value.value);\n             let len = C_uint(cx.ccx(), n);\n             let unit_ty = t.sequence_element_type(cx.tcx());\n             cx = tvec::slice_for_each(cx, base, unit_ty, len,\n@@ -551,23 +552,23 @@ fn drop_structural_ty<'blk, 'tcx>(cx: Block<'blk, 'tcx>,\n         }\n         ty::TyTuple(ref args) => {\n             for (i, arg) in args.iter().enumerate() {\n-                let llfld_a = adt::trans_field_ptr(cx, t, value, Disr(0), i);\n+                let llfld_a = adt::trans_field_ptr(&cx, t, value, Disr(0), i);\n                 cx = drop_ty(cx, llfld_a, *arg, DebugLoc::None);\n             }\n         }\n         ty::TyAdt(adt, substs) => match adt.adt_kind() {\n             AdtKind::Struct => {\n                 let VariantInfo { fields, discr } = VariantInfo::from_ty(cx.tcx(), t, None);\n                 for (i, &Field(_, field_ty)) in fields.iter().enumerate() {\n-                    let llfld_a = adt::trans_field_ptr(cx, t, value, Disr::from(discr), i);\n+                    let llfld_a = adt::trans_field_ptr(&cx, t, value, Disr::from(discr), i);\n \n                     let val = if type_is_sized(cx.tcx(), field_ty) {\n                         llfld_a\n                     } else {\n                         // FIXME(#36457) -- we should pass unsized values as two arguments\n-                        let scratch = alloc_ty(cx, field_ty, \"__fat_ptr_iter\");\n-                        Store(cx, llfld_a, get_dataptr(cx, scratch));\n-                        Store(cx, value.meta, get_meta(cx, scratch));\n+                        let scratch = alloc_ty(&cx, field_ty, \"__fat_ptr_iter\");\n+                        Store(&cx, llfld_a, get_dataptr(&cx, scratch));\n+                        Store(&cx, value.meta, get_meta(&cx, scratch));\n                         scratch\n                     };\n                     cx = drop_ty(cx, val, field_ty, DebugLoc::None);\n@@ -577,14 +578,14 @@ fn drop_structural_ty<'blk, 'tcx>(cx: Block<'blk, 'tcx>,\n                 bug!(\"Union in `glue::drop_structural_ty`\");\n             }\n             AdtKind::Enum => {\n-                let fcx = cx.fcx;\n+                let fcx = cx.fcx();\n                 let ccx = fcx.ccx;\n                 let n_variants = adt.variants.len();\n \n                 // NB: we must hit the discriminant first so that structural\n                 // comparison know not to proceed when the discriminants differ.\n \n-                match adt::trans_switch(cx, t, av, false) {\n+                match adt::trans_switch(&cx, t, av, false) {\n                     (adt::BranchKind::Single, None) => {\n                         if n_variants != 0 {\n                             assert!(n_variants == 1);\n@@ -593,7 +594,8 @@ fn drop_structural_ty<'blk, 'tcx>(cx: Block<'blk, 'tcx>,\n                         }\n                     }\n                     (adt::BranchKind::Switch, Some(lldiscrim_a)) => {\n-                        cx = drop_ty(cx, lldiscrim_a, cx.tcx().types.isize, DebugLoc::None);\n+                        let tcx = cx.tcx();\n+                        cx = drop_ty(cx, lldiscrim_a, tcx.types.isize, DebugLoc::None);\n \n                         // Create a fall-through basic block for the \"else\" case of\n                         // the switch instruction we're about to generate. Note that\n@@ -608,23 +610,19 @@ fn drop_structural_ty<'blk, 'tcx>(cx: Block<'blk, 'tcx>,\n                         // from the outer function, and any other use case will only\n                         // call this for an already-valid enum in which case the `ret\n                         // void` will never be hit.\n-                        let ret_void_cx = fcx.new_block(\"enum-iter-ret-void\");\n-                        RetVoid(ret_void_cx, DebugLoc::None);\n-                        let llswitch = Switch(cx, lldiscrim_a, ret_void_cx.llbb, n_variants);\n-                        let next_cx = fcx.new_block(\"enum-iter-next\");\n+                        let ret_void_cx = fcx.new_block(\"enum-iter-ret-void\").build();\n+                        RetVoid(&ret_void_cx, DebugLoc::None);\n+                        let llswitch = Switch(&cx, lldiscrim_a, ret_void_cx.llbb(), n_variants);\n+                        let next_cx = fcx.new_block(\"enum-iter-next\").build();\n \n                         for variant in &adt.variants {\n-                            let variant_cx = fcx.new_block(&format!(\"enum-iter-variant-{}\",\n-                                                                        &variant.disr_val\n-                                                                                .to_string()));\n-                            let case_val = adt::trans_case(cx, t, Disr::from(variant.disr_val));\n-                            AddCase(llswitch, case_val, variant_cx.llbb);\n-                            let variant_cx = iter_variant(variant_cx,\n-                                                        t,\n-                                                        value,\n-                                                        variant,\n-                                                        substs);\n-                            Br(variant_cx, next_cx.llbb, DebugLoc::None);\n+                            let variant_cx_name = format!(\"enum-iter-variant-{}\",\n+                                &variant.disr_val.to_string());\n+                            let variant_cx = fcx.new_block(&variant_cx_name).build();\n+                            let case_val = adt::trans_case(&cx, t, Disr::from(variant.disr_val));\n+                            AddCase(llswitch, case_val, variant_cx.llbb());\n+                            let variant_cx = iter_variant(variant_cx, t, value, variant, substs);\n+                            Br(&variant_cx, next_cx.llbb(), DebugLoc::None);\n                         }\n                         cx = next_cx;\n                     }"}, {"sha": "74af7c4e3a7bdba2ca758418b3b8f9c148f63b1c", "filename": "src/librustc_trans/intrinsic.rs", "status": "modified", "additions": 79, "deletions": 104, "changes": 183, "blob_url": "https://github.com/rust-lang/rust/blob/bf7d4534a7ed27f1a0b5c9b53b2af155da33f072/src%2Flibrustc_trans%2Fintrinsic.rs", "raw_url": "https://github.com/rust-lang/rust/raw/bf7d4534a7ed27f1a0b5c9b53b2af155da33f072/src%2Flibrustc_trans%2Fintrinsic.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_trans%2Fintrinsic.rs?ref=bf7d4534a7ed27f1a0b5c9b53b2af155da33f072", "patch": "@@ -87,14 +87,13 @@ fn get_simple_intrinsic(ccx: &CrateContext, name: &str) -> Option<ValueRef> {\n /// Remember to add all intrinsics here, in librustc_typeck/check/mod.rs,\n /// and in libcore/intrinsics.rs; if you need access to any llvm intrinsics,\n /// add them to librustc_trans/trans/context.rs\n-pub fn trans_intrinsic_call<'a, 'blk, 'tcx>(mut bcx: Block<'blk, 'tcx>,\n+pub fn trans_intrinsic_call<'a, 'blk, 'tcx>(bcx: &BlockAndBuilder<'blk, 'tcx>,\n                                             callee_ty: Ty<'tcx>,\n                                             fn_ty: &FnType,\n                                             llargs: &[ValueRef],\n                                             llresult: ValueRef,\n-                                            call_debug_location: DebugLoc)\n-                                            -> Result<'blk, 'tcx> {\n-    let fcx = bcx.fcx;\n+                                            call_debug_location: DebugLoc) {\n+    let fcx = bcx.fcx();\n     let ccx = fcx.ccx;\n     let tcx = bcx.tcx();\n \n@@ -122,11 +121,10 @@ pub fn trans_intrinsic_call<'a, 'blk, 'tcx>(mut bcx: Block<'blk, 'tcx>,\n     if name == \"abort\" {\n         let llfn = ccx.get_intrinsic(&(\"llvm.trap\"));\n         Call(bcx, llfn, &[], call_debug_location);\n-        Unreachable(bcx);\n-        return Result::new(bcx, C_undef(Type::nil(ccx).ptr_to()));\n+        return;\n     } else if name == \"unreachable\" {\n-        Unreachable(bcx);\n-        return Result::new(bcx, C_nil(ccx));\n+        // FIXME: do nothing?\n+        return;\n     }\n \n     let llret_ty = type_of::type_of(ccx, ret_ty);\n@@ -145,8 +143,7 @@ pub fn trans_intrinsic_call<'a, 'blk, 'tcx>(mut bcx: Block<'blk, 'tcx>,\n             Call(bcx, expect, &[llargs[0], C_bool(ccx, false)], call_debug_location)\n         }\n         (_, \"try\") => {\n-            bcx = try_intrinsic(bcx, llargs[0], llargs[1], llargs[2], llresult,\n-                                call_debug_location);\n+            try_intrinsic(bcx, llargs[0], llargs[1], llargs[2], llresult, call_debug_location);\n             C_nil(ccx)\n         }\n         (_, \"breakpoint\") => {\n@@ -162,7 +159,7 @@ pub fn trans_intrinsic_call<'a, 'blk, 'tcx>(mut bcx: Block<'blk, 'tcx>,\n             let tp_ty = substs.type_at(0);\n             if !type_is_sized(tcx, tp_ty) {\n                 let (llsize, _) =\n-                    glue::size_and_align_of_dst(&bcx.build(), tp_ty, llargs[1]);\n+                    glue::size_and_align_of_dst(bcx, tp_ty, llargs[1]);\n                 llsize\n             } else {\n                 let lltp_ty = type_of::type_of(ccx, tp_ty);\n@@ -177,7 +174,7 @@ pub fn trans_intrinsic_call<'a, 'blk, 'tcx>(mut bcx: Block<'blk, 'tcx>,\n             let tp_ty = substs.type_at(0);\n             if !type_is_sized(tcx, tp_ty) {\n                 let (_, llalign) =\n-                    glue::size_and_align_of_dst(&bcx.build(), tp_ty, llargs[1]);\n+                    glue::size_and_align_of_dst(bcx, tp_ty, llargs[1]);\n                 llalign\n             } else {\n                 C_uint(ccx, type_of::align_of(ccx, tp_ty))\n@@ -188,25 +185,6 @@ pub fn trans_intrinsic_call<'a, 'blk, 'tcx>(mut bcx: Block<'blk, 'tcx>,\n             let lltp_ty = type_of::type_of(ccx, tp_ty);\n             C_uint(ccx, machine::llalign_of_pref(ccx, lltp_ty))\n         }\n-        (_, \"drop_in_place\") => {\n-            let tp_ty = substs.type_at(0);\n-            let is_sized = type_is_sized(tcx, tp_ty);\n-            let ptr = if is_sized {\n-                llargs[0]\n-            } else {\n-                // FIXME(#36457) -- we should pass unsized values as two arguments\n-                let scratch = alloc_ty(bcx, tp_ty, \"drop\");\n-                call_lifetime_start(bcx, scratch);\n-                Store(bcx, llargs[0], get_dataptr(bcx, scratch));\n-                Store(bcx, llargs[1], get_meta(bcx, scratch));\n-                scratch\n-            };\n-            glue::drop_ty(bcx, ptr, tp_ty, call_debug_location);\n-            if !is_sized {\n-                call_lifetime_end(bcx, ptr);\n-            }\n-            C_nil(ccx)\n-        }\n         (_, \"type_name\") => {\n             let tp_ty = substs.type_at(0);\n             let ty_name = Symbol::intern(&tp_ty.to_string()).as_str();\n@@ -230,7 +208,7 @@ pub fn trans_intrinsic_call<'a, 'blk, 'tcx>(mut bcx: Block<'blk, 'tcx>,\n         (_, \"needs_drop\") => {\n             let tp_ty = substs.type_at(0);\n \n-            C_bool(ccx, bcx.fcx.type_needs_drop(tp_ty))\n+            C_bool(ccx, bcx.fcx().type_needs_drop(tp_ty))\n         }\n         (_, \"offset\") => {\n             let ptr = llargs[0];\n@@ -613,7 +591,7 @@ pub fn trans_intrinsic_call<'a, 'blk, 'tcx>(mut bcx: Block<'blk, 'tcx>,\n             // qux` to be converted into `foo, bar, baz, qux`, integer\n             // arguments to be truncated as needed and pointers to be\n             // cast.\n-            fn modify_as_needed<'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n+            fn modify_as_needed<'blk, 'tcx>(bcx: &BlockAndBuilder<'blk, 'tcx>,\n                                             t: &intrinsics::Type,\n                                             arg_type: Ty<'tcx>,\n                                             llarg: ValueRef)\n@@ -627,7 +605,7 @@ pub fn trans_intrinsic_call<'a, 'blk, 'tcx>(mut bcx: Block<'blk, 'tcx>,\n                         // This assumes the type is \"simple\", i.e. no\n                         // destructors, and the contents are SIMD\n                         // etc.\n-                        assert!(!bcx.fcx.type_needs_drop(arg_type));\n+                        assert!(!bcx.fcx().type_needs_drop(arg_type));\n                         let arg = adt::MaybeSizedValue::sized(llarg);\n                         (0..contents.len())\n                             .map(|i| {\n@@ -718,11 +696,9 @@ pub fn trans_intrinsic_call<'a, 'blk, 'tcx>(mut bcx: Block<'blk, 'tcx>,\n             store_ty(bcx, llval, llresult, ret_ty);\n         }\n     }\n-\n-    Result::new(bcx, llresult)\n }\n \n-fn copy_intrinsic<'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n+fn copy_intrinsic<'blk, 'tcx>(bcx: &BlockAndBuilder<'blk, 'tcx>,\n                               allow_overlap: bool,\n                               volatile: bool,\n                               tp_ty: Ty<'tcx>,\n@@ -759,7 +735,7 @@ fn copy_intrinsic<'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n          call_debug_location)\n }\n \n-fn memset_intrinsic<'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n+fn memset_intrinsic<'blk, 'tcx>(bcx: &BlockAndBuilder<'blk, 'tcx>,\n                                 volatile: bool,\n                                 tp_ty: Ty<'tcx>,\n                                 dst: ValueRef,\n@@ -788,7 +764,7 @@ fn memset_intrinsic<'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n          call_debug_location)\n }\n \n-fn count_zeros_intrinsic(bcx: Block,\n+fn count_zeros_intrinsic(bcx: &BlockAndBuilder,\n                          name: &str,\n                          val: ValueRef,\n                          call_debug_location: DebugLoc)\n@@ -798,7 +774,7 @@ fn count_zeros_intrinsic(bcx: Block,\n     Call(bcx, llfn, &[val, y], call_debug_location)\n }\n \n-fn with_overflow_intrinsic<'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n+fn with_overflow_intrinsic<'blk, 'tcx>(bcx: &BlockAndBuilder<'blk, 'tcx>,\n                                        name: &str,\n                                        a: ValueRef,\n                                        b: ValueRef,\n@@ -817,20 +793,21 @@ fn with_overflow_intrinsic<'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n     C_nil(bcx.ccx())\n }\n \n-fn try_intrinsic<'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n-                             func: ValueRef,\n-                             data: ValueRef,\n-                             local_ptr: ValueRef,\n-                             dest: ValueRef,\n-                             dloc: DebugLoc) -> Block<'blk, 'tcx> {\n+fn try_intrinsic<'blk, 'tcx>(\n+    bcx: &BlockAndBuilder<'blk, 'tcx>,\n+    func: ValueRef,\n+    data: ValueRef,\n+    local_ptr: ValueRef,\n+    dest: ValueRef,\n+    dloc: DebugLoc\n+) {\n     if bcx.sess().no_landing_pads() {\n         Call(bcx, func, &[data], dloc);\n-        Store(bcx, C_null(Type::i8p(bcx.ccx())), dest);\n-        bcx\n+        Store(bcx, C_null(Type::i8p(&bcx.ccx())), dest);\n     } else if wants_msvc_seh(bcx.sess()) {\n-        trans_msvc_try(bcx, func, data, local_ptr, dest, dloc)\n+        trans_msvc_try(bcx, func, data, local_ptr, dest, dloc);\n     } else {\n-        trans_gnu_try(bcx, func, data, local_ptr, dest, dloc)\n+        trans_gnu_try(bcx, func, data, local_ptr, dest, dloc);\n     }\n }\n \n@@ -841,26 +818,26 @@ fn try_intrinsic<'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n // instructions are meant to work for all targets, as of the time of this\n // writing, however, LLVM does not recommend the usage of these new instructions\n // as the old ones are still more optimized.\n-fn trans_msvc_try<'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n+fn trans_msvc_try<'blk, 'tcx>(bcx: &BlockAndBuilder<'blk, 'tcx>,\n                               func: ValueRef,\n                               data: ValueRef,\n                               local_ptr: ValueRef,\n                               dest: ValueRef,\n-                              dloc: DebugLoc) -> Block<'blk, 'tcx> {\n-    let llfn = get_rust_try_fn(bcx.fcx, &mut |bcx| {\n+                              dloc: DebugLoc) {\n+    let llfn = get_rust_try_fn(bcx.fcx(), &mut |bcx| {\n         let ccx = bcx.ccx();\n         let dloc = DebugLoc::None;\n \n-        SetPersonalityFn(bcx, bcx.fcx.eh_personality());\n+        SetPersonalityFn(&bcx, bcx.fcx().eh_personality());\n \n-        let normal = bcx.fcx.new_block(\"normal\");\n-        let catchswitch = bcx.fcx.new_block(\"catchswitch\");\n-        let catchpad = bcx.fcx.new_block(\"catchpad\");\n-        let caught = bcx.fcx.new_block(\"caught\");\n+        let normal = bcx.fcx().new_block(\"normal\").build();\n+        let catchswitch = bcx.fcx().new_block(\"catchswitch\").build();\n+        let catchpad = bcx.fcx().new_block(\"catchpad\").build();\n+        let caught = bcx.fcx().new_block(\"caught\").build();\n \n-        let func = llvm::get_param(bcx.fcx.llfn, 0);\n-        let data = llvm::get_param(bcx.fcx.llfn, 1);\n-        let local_ptr = llvm::get_param(bcx.fcx.llfn, 2);\n+        let func = llvm::get_param(bcx.fcx().llfn, 0);\n+        let data = llvm::get_param(bcx.fcx().llfn, 1);\n+        let local_ptr = llvm::get_param(bcx.fcx().llfn, 2);\n \n         // We're generating an IR snippet that looks like:\n         //\n@@ -902,37 +879,36 @@ fn trans_msvc_try<'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n         //\n         // More information can be found in libstd's seh.rs implementation.\n         let i64p = Type::i64(ccx).ptr_to();\n-        let slot = Alloca(bcx, i64p, \"slot\");\n-        Invoke(bcx, func, &[data], normal.llbb, catchswitch.llbb, dloc);\n+        let slot = Alloca(&bcx, i64p, \"slot\");\n+        Invoke(&bcx, func, &[data], normal.llbb(), catchswitch.llbb(), dloc);\n \n-        Ret(normal, C_i32(ccx, 0), dloc);\n+        Ret(&normal, C_i32(ccx, 0), dloc);\n \n-        let cs = CatchSwitch(catchswitch, None, None, 1);\n-        AddHandler(catchswitch, cs, catchpad.llbb);\n+        let cs = CatchSwitch(&catchswitch, None, None, 1);\n+        AddHandler(&catchswitch, cs, catchpad.llbb());\n \n         let tcx = ccx.tcx();\n         let tydesc = match tcx.lang_items.msvc_try_filter() {\n             Some(did) => ::consts::get_static(ccx, did),\n             None => bug!(\"msvc_try_filter not defined\"),\n         };\n-        let tok = CatchPad(catchpad, cs, &[tydesc, C_i32(ccx, 0), slot]);\n-        let addr = Load(catchpad, slot);\n-        let arg1 = Load(catchpad, addr);\n+        let tok = CatchPad(&catchpad, cs, &[tydesc, C_i32(ccx, 0), slot]);\n+        let addr = Load(&catchpad, slot);\n+        let arg1 = Load(&catchpad, addr);\n         let val1 = C_i32(ccx, 1);\n-        let arg2 = Load(catchpad, InBoundsGEP(catchpad, addr, &[val1]));\n-        let local_ptr = BitCast(catchpad, local_ptr, i64p);\n-        Store(catchpad, arg1, local_ptr);\n-        Store(catchpad, arg2, InBoundsGEP(catchpad, local_ptr, &[val1]));\n-        CatchRet(catchpad, tok, caught.llbb);\n+        let arg2 = Load(&catchpad, InBoundsGEP(&catchpad, addr, &[val1]));\n+        let local_ptr = BitCast(&catchpad, local_ptr, i64p);\n+        Store(&catchpad, arg1, local_ptr);\n+        Store(&catchpad, arg2, InBoundsGEP(&catchpad, local_ptr, &[val1]));\n+        CatchRet(&catchpad, tok, caught.llbb());\n \n-        Ret(caught, C_i32(ccx, 1), dloc);\n+        Ret(&caught, C_i32(ccx, 1), dloc);\n     });\n \n     // Note that no invoke is used here because by definition this function\n     // can't panic (that's what it's catching).\n     let ret = Call(bcx, llfn, &[func, data, local_ptr], dloc);\n     Store(bcx, ret, dest);\n-    return bcx\n }\n \n // Definition of the standard \"try\" function for Rust using the GNU-like model\n@@ -946,13 +922,13 @@ fn trans_msvc_try<'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n // function calling it, and that function may already have other personality\n // functions in play. By calling a shim we're guaranteed that our shim will have\n // the right personality function.\n-fn trans_gnu_try<'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n+fn trans_gnu_try<'blk, 'tcx>(bcx: &BlockAndBuilder<'blk, 'tcx>,\n                              func: ValueRef,\n                              data: ValueRef,\n                              local_ptr: ValueRef,\n                              dest: ValueRef,\n-                             dloc: DebugLoc) -> Block<'blk, 'tcx> {\n-    let llfn = get_rust_try_fn(bcx.fcx, &mut |bcx| {\n+                             dloc: DebugLoc) {\n+    let llfn = get_rust_try_fn(bcx.fcx(), &mut |bcx| {\n         let ccx = bcx.ccx();\n         let dloc = DebugLoc::None;\n \n@@ -973,14 +949,14 @@ fn trans_gnu_try<'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n         // expected to be `*mut *mut u8` for this to actually work, but that's\n         // managed by the standard library.\n \n-        let then = bcx.fcx.new_block(\"then\");\n-        let catch = bcx.fcx.new_block(\"catch\");\n+        let then = bcx.fcx().new_block(\"then\").build();\n+        let catch = bcx.fcx().new_block(\"catch\").build();\n \n-        let func = llvm::get_param(bcx.fcx.llfn, 0);\n-        let data = llvm::get_param(bcx.fcx.llfn, 1);\n-        let local_ptr = llvm::get_param(bcx.fcx.llfn, 2);\n-        Invoke(bcx, func, &[data], then.llbb, catch.llbb, dloc);\n-        Ret(then, C_i32(ccx, 0), dloc);\n+        let func = llvm::get_param(bcx.fcx().llfn, 0);\n+        let data = llvm::get_param(bcx.fcx().llfn, 1);\n+        let local_ptr = llvm::get_param(bcx.fcx().llfn, 2);\n+        Invoke(&bcx, func, &[data], then.llbb(), catch.llbb(), dloc);\n+        Ret(&then, C_i32(ccx, 0), dloc);\n \n         // Type indicator for the exception being thrown.\n         //\n@@ -990,18 +966,17 @@ fn trans_gnu_try<'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n         // rust_try ignores the selector.\n         let lpad_ty = Type::struct_(ccx, &[Type::i8p(ccx), Type::i32(ccx)],\n                                     false);\n-        let vals = LandingPad(catch, lpad_ty, bcx.fcx.eh_personality(), 1);\n-        AddClause(catch, vals, C_null(Type::i8p(ccx)));\n-        let ptr = ExtractValue(catch, vals, 0);\n-        Store(catch, ptr, BitCast(catch, local_ptr, Type::i8p(ccx).ptr_to()));\n-        Ret(catch, C_i32(ccx, 1), dloc);\n+        let vals = LandingPad(&catch, lpad_ty, bcx.fcx().eh_personality(), 1);\n+        AddClause(&catch, vals, C_null(Type::i8p(ccx)));\n+        let ptr = ExtractValue(&catch, vals, 0);\n+        Store(&catch, ptr, BitCast(&catch, local_ptr, Type::i8p(ccx).ptr_to()));\n+        Ret(&catch, C_i32(ccx, 1), dloc);\n     });\n \n     // Note that no invoke is used here because by definition this function\n     // can't panic (that's what it's catching).\n     let ret = Call(bcx, llfn, &[func, data, local_ptr], dloc);\n     Store(bcx, ret, dest);\n-    return bcx;\n }\n \n // Helper function to give a Block to a closure to translate a shim function.\n@@ -1010,7 +985,7 @@ fn gen_fn<'a, 'tcx>(fcx: &FunctionContext<'a, 'tcx>,\n                     name: &str,\n                     inputs: Vec<Ty<'tcx>>,\n                     output: Ty<'tcx>,\n-                    trans: &mut for<'b> FnMut(Block<'b, 'tcx>))\n+                    trans: &mut for<'b> FnMut(BlockAndBuilder<'b, 'tcx>))\n                     -> ValueRef {\n     let ccx = fcx.ccx;\n     let sig = ccx.tcx().mk_fn_sig(inputs.into_iter(), output, false);\n@@ -1035,7 +1010,7 @@ fn gen_fn<'a, 'tcx>(fcx: &FunctionContext<'a, 'tcx>,\n //\n // This function is only generated once and is then cached.\n fn get_rust_try_fn<'a, 'tcx>(fcx: &FunctionContext<'a, 'tcx>,\n-                             trans: &mut for<'b> FnMut(Block<'b, 'tcx>))\n+                             trans: &mut for<'b> FnMut(BlockAndBuilder<'b, 'tcx>))\n                              -> ValueRef {\n     let ccx = fcx.ccx;\n     if let Some(llfn) = ccx.rust_try_fn().get() {\n@@ -1060,16 +1035,16 @@ fn span_invalid_monomorphization_error(a: &Session, b: Span, c: &str) {\n     span_err!(a, b, E0511, \"{}\", c);\n }\n \n-fn generic_simd_intrinsic<'blk, 'tcx, 'a>\n-    (bcx: Block<'blk, 'tcx>,\n-     name: &str,\n-     callee_ty: Ty<'tcx>,\n-     llargs: &[ValueRef],\n-     ret_ty: Ty<'tcx>,\n-     llret_ty: Type,\n-     call_debug_location: DebugLoc,\n-     span: Span) -> ValueRef\n-{\n+fn generic_simd_intrinsic<'blk, 'tcx, 'a>(\n+    bcx: &BlockAndBuilder<'blk, 'tcx>,\n+    name: &str,\n+    callee_ty: Ty<'tcx>,\n+    llargs: &[ValueRef],\n+    ret_ty: Ty<'tcx>,\n+    llret_ty: Type,\n+    call_debug_location: DebugLoc,\n+    span: Span\n+) -> ValueRef {\n     // macros for error handling:\n     macro_rules! emit_error {\n         ($msg: tt) => {"}, {"sha": "1a93773a9ecc5097bf50e0598f8d4be0f732de0e", "filename": "src/librustc_trans/meth.rs", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "blob_url": "https://github.com/rust-lang/rust/blob/bf7d4534a7ed27f1a0b5c9b53b2af155da33f072/src%2Flibrustc_trans%2Fmeth.rs", "raw_url": "https://github.com/rust-lang/rust/raw/bf7d4534a7ed27f1a0b5c9b53b2af155da33f072/src%2Flibrustc_trans%2Fmeth.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_trans%2Fmeth.rs?ref=bf7d4534a7ed27f1a0b5c9b53b2af155da33f072", "patch": "@@ -32,7 +32,7 @@ use rustc::ty;\n const VTABLE_OFFSET: usize = 3;\n \n /// Extracts a method from a trait object's vtable, at the specified index.\n-pub fn get_virtual_method<'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n+pub fn get_virtual_method<'blk, 'tcx>(bcx: &BlockAndBuilder<'blk, 'tcx>,\n                                       llvtable: ValueRef,\n                                       vtable_index: usize)\n                                       -> ValueRef {\n@@ -94,9 +94,9 @@ pub fn trans_object_shim<'a, 'tcx>(ccx: &'a CrateContext<'a, 'tcx>,\n     let dest = fcx.llretslotptr.get();\n     let llargs = get_params(fcx.llfn);\n     bcx = callee.call(bcx, DebugLoc::None,\n-                      &llargs[fcx.fn_ty.ret.is_indirect() as usize..], dest).bcx;\n+                      &llargs[fcx.fn_ty.ret.is_indirect() as usize..], dest).0;\n \n-    fcx.finish(bcx, DebugLoc::None);\n+    fcx.finish(&bcx, DebugLoc::None);\n \n     llfn\n }"}, {"sha": "9af02f40111f59ed594d35b7bfe5a2b6bc541081", "filename": "src/librustc_trans/mir/block.rs", "status": "modified", "additions": 80, "deletions": 41, "changes": 121, "blob_url": "https://github.com/rust-lang/rust/blob/bf7d4534a7ed27f1a0b5c9b53b2af155da33f072/src%2Flibrustc_trans%2Fmir%2Fblock.rs", "raw_url": "https://github.com/rust-lang/rust/raw/bf7d4534a7ed27f1a0b5c9b53b2af155da33f072/src%2Flibrustc_trans%2Fmir%2Fblock.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_trans%2Fmir%2Fblock.rs?ref=bf7d4534a7ed27f1a0b5c9b53b2af155da33f072", "patch": "@@ -40,6 +40,7 @@ use super::operand::OperandRef;\n use super::operand::OperandValue::{Pair, Ref, Immediate};\n \n use std::cell::Ref as CellRef;\n+use std::ptr;\n \n impl<'bcx, 'tcx> MirContext<'bcx, 'tcx> {\n     pub fn trans_block(&mut self, bb: mir::BasicBlock) {\n@@ -121,10 +122,8 @@ impl<'bcx, 'tcx> MirContext<'bcx, 'tcx> {\n \n                     let ps = self.get_personality_slot(&bcx);\n                     let lp = bcx.load(ps);\n-                    bcx.with_block(|bcx| {\n-                        base::call_lifetime_end(bcx, ps);\n-                        base::trans_unwind_resume(bcx, lp);\n-                    });\n+                    base::call_lifetime_end(&bcx, ps);\n+                    base::trans_unwind_resume(&bcx, lp);\n                 }\n             }\n \n@@ -143,9 +142,7 @@ impl<'bcx, 'tcx> MirContext<'bcx, 'tcx> {\n             mir::TerminatorKind::Switch { ref discr, ref adt_def, ref targets } => {\n                 let discr_lvalue = self.trans_lvalue(&bcx, discr);\n                 let ty = discr_lvalue.ty.to_ty(bcx.tcx());\n-                let discr = bcx.with_block(|bcx|\n-                    adt::trans_get_discr(bcx, ty, discr_lvalue.llval, None, true)\n-                );\n+                let discr = adt::trans_get_discr(&bcx, ty, discr_lvalue.llval, None, true);\n \n                 let mut bb_hist = FxHashMap();\n                 for target in targets {\n@@ -169,8 +166,7 @@ impl<'bcx, 'tcx> MirContext<'bcx, 'tcx> {\n                 for (adt_variant, &target) in adt_def.variants.iter().zip(targets) {\n                     if default_bb != Some(target) {\n                         let llbb = llblock(self, target);\n-                        let llval = bcx.with_block(|bcx| adt::trans_case(\n-                                bcx, ty, Disr::from(adt_variant.disr_val)));\n+                        let llval = adt::trans_case(&bcx, ty, Disr::from(adt_variant.disr_val));\n                         build::AddCase(switch, llval, llbb)\n                     }\n                 }\n@@ -179,7 +175,7 @@ impl<'bcx, 'tcx> MirContext<'bcx, 'tcx> {\n             mir::TerminatorKind::SwitchInt { ref discr, switch_ty, ref values, ref targets } => {\n                 let (otherwise, targets) = targets.split_last().unwrap();\n                 let discr = bcx.load(self.trans_lvalue(&bcx, discr).llval);\n-                let discr = bcx.with_block(|bcx| base::to_immediate(bcx, discr, switch_ty));\n+                let discr = base::to_immediate(&bcx, discr, switch_ty);\n                 let switch = bcx.switch(discr, llblock(self, *otherwise), values.len());\n                 for (value, target) in values.iter().zip(targets) {\n                     let val = Const::from_constval(bcx.ccx(), value.clone(), switch_ty);\n@@ -259,13 +255,11 @@ impl<'bcx, 'tcx> MirContext<'bcx, 'tcx> {\n                     // but I am shooting for a quick fix to #35546\n                     // here that can be cleanly backported to beta, so\n                     // I want to avoid touching all of trans.\n-                    bcx.with_block(|bcx| {\n-                        let scratch = base::alloc_ty(bcx, ty, \"drop\");\n-                        base::call_lifetime_start(bcx, scratch);\n-                        build::Store(bcx, lvalue.llval, base::get_dataptr(bcx, scratch));\n-                        build::Store(bcx, lvalue.llextra, base::get_meta(bcx, scratch));\n-                        scratch\n-                    })\n+                    let scratch = base::alloc_ty(&bcx, ty, \"drop\");\n+                    base::call_lifetime_start(&bcx, scratch);\n+                    build::Store(&bcx, lvalue.llval, base::get_dataptr(&bcx, scratch));\n+                    build::Store(&bcx, lvalue.llextra, base::get_meta(&bcx, scratch));\n+                    scratch\n                 };\n                 if let Some(unwind) = unwind {\n                     bcx.invoke(drop_fn,\n@@ -443,6 +437,65 @@ impl<'bcx, 'tcx> MirContext<'bcx, 'tcx> {\n                     return;\n                 }\n \n+                // FIXME: This should proxy to the drop glue in the future when the ABI matches;\n+                // most of the below code was copied from the match arm for TerminatorKind::Drop.\n+                if intrinsic == Some(\"drop_in_place\") {\n+                    let &(_, target) = destination.as_ref().unwrap();\n+                    let ty = if let ty::TyFnDef(_, substs, _) = callee.ty.sty {\n+                        substs.type_at(0)\n+                    } else {\n+                        bug!(\"Unexpected ty: {}\", callee.ty);\n+                    };\n+\n+                    // Double check for necessity to drop\n+                    if !glue::type_needs_drop(bcx.tcx(), ty) {\n+                        funclet_br(self, bcx, target);\n+                        return;\n+                    }\n+\n+                    let ptr =  self.trans_operand(&bcx, &args[0]);\n+                    let (llval, llextra) = match ptr.val {\n+                        Immediate(llptr) => (llptr, ptr::null_mut()),\n+                        Pair(llptr, llextra) => (llptr, llextra),\n+                        Ref(_) => bug!(\"Deref of by-Ref type {:?}\", ptr.ty)\n+                    };\n+\n+                    let drop_fn = glue::get_drop_glue(bcx.ccx(), ty);\n+                    let drop_ty = glue::get_drop_glue_type(bcx.tcx(), ty);\n+                    let is_sized = common::type_is_sized(bcx.tcx(), ty);\n+                    let llvalue = if is_sized {\n+                        if drop_ty != ty {\n+                            bcx.pointercast(llval, type_of::type_of(bcx.ccx(), drop_ty).ptr_to())\n+                        } else {\n+                            llval\n+                        }\n+                    } else {\n+                        // FIXME(#36457) Currently drop glue takes sized\n+                        // values as a `*(data, meta)`, but elsewhere in\n+                        // MIR we pass `(data, meta)` as two separate\n+                        // arguments. It would be better to fix drop glue,\n+                        // but I am shooting for a quick fix to #35546\n+                        // here that can be cleanly backported to beta, so\n+                        // I want to avoid touching all of trans.\n+                        let scratch = base::alloc_ty(&bcx, ty, \"drop\");\n+                        base::call_lifetime_start(&bcx, scratch);\n+                        build::Store(&bcx, llval, base::get_dataptr(&bcx, scratch));\n+                        build::Store(&bcx, llextra, base::get_meta(&bcx, scratch));\n+                        scratch\n+                    };\n+                    if let Some(unwind) = *cleanup {\n+                        bcx.invoke(drop_fn,\n+                            &[llvalue],\n+                            self.blocks[target].llbb,\n+                            llblock(self, unwind),\n+                            cleanup_bundle);\n+                    } else {\n+                        bcx.call(drop_fn, &[llvalue], cleanup_bundle);\n+                        funclet_br(self, bcx, target);\n+                    }\n+                    return;\n+                }\n+\n                 if intrinsic == Some(\"transmute\") {\n                     let &(ref dest, target) = destination.as_ref().unwrap();\n                     self.with_lvalue_ref(&bcx, dest, |this, dest| {\n@@ -537,10 +590,7 @@ impl<'bcx, 'tcx> MirContext<'bcx, 'tcx> {\n                                 bug!(\"Cannot use direct operand with an intrinsic call\")\n                         };\n \n-                        bcx.with_block(|bcx| {\n-                            trans_intrinsic_call(bcx, callee.ty, &fn_ty,\n-                                                 &llargs, dest, debug_loc);\n-                        });\n+                        trans_intrinsic_call(&bcx, callee.ty, &fn_ty, &llargs, dest, debug_loc);\n \n                         if let ReturnDest::IndirectOperand(dst, _) = ret_dest {\n                             // Make a fake operand for store_return\n@@ -554,8 +604,7 @@ impl<'bcx, 'tcx> MirContext<'bcx, 'tcx> {\n                         if let Some((_, target)) = *destination {\n                             funclet_br(self, bcx, target);\n                         } else {\n-                            // trans_intrinsic_call already used Unreachable.\n-                            // bcx.unreachable();\n+                            bcx.unreachable();\n                         }\n \n                         return;\n@@ -620,9 +669,7 @@ impl<'bcx, 'tcx> MirContext<'bcx, 'tcx> {\n                 let (ptr, meta) = (a, b);\n                 if *next_idx == 0 {\n                     if let Virtual(idx) = *callee {\n-                        let llfn = bcx.with_block(|bcx| {\n-                            meth::get_virtual_method(bcx, meta, idx)\n-                        });\n+                        let llfn = meth::get_virtual_method(bcx, meta, idx);\n                         let llty = fn_ty.llvm_type(bcx.ccx()).ptr_to();\n                         *callee = Fn(bcx.pointercast(llfn, llty));\n                     }\n@@ -768,12 +815,10 @@ impl<'bcx, 'tcx> MirContext<'bcx, 'tcx> {\n             slot\n         } else {\n             let llretty = Type::struct_(ccx, &[Type::i8p(ccx), Type::i32(ccx)], false);\n-            bcx.with_block(|bcx| {\n-                let slot = base::alloca(bcx, llretty, \"personalityslot\");\n-                self.llpersonalityslot = Some(slot);\n-                base::call_lifetime_start(bcx, slot);\n-                slot\n-            })\n+            let slot = base::alloca(bcx, llretty, \"personalityslot\");\n+            self.llpersonalityslot = Some(slot);\n+            base::call_lifetime_start(bcx, slot);\n+            slot\n         }\n     }\n \n@@ -863,18 +908,14 @@ impl<'bcx, 'tcx> MirContext<'bcx, 'tcx> {\n                     return if fn_ret_ty.is_indirect() {\n                         // Odd, but possible, case, we have an operand temporary,\n                         // but the calling convention has an indirect return.\n-                        let tmp = bcx.with_block(|bcx| {\n-                            base::alloc_ty(bcx, ret_ty, \"tmp_ret\")\n-                        });\n+                        let tmp = base::alloc_ty(bcx, ret_ty, \"tmp_ret\");\n                         llargs.push(tmp);\n                         ReturnDest::IndirectOperand(tmp, index)\n                     } else if is_intrinsic {\n                         // Currently, intrinsics always need a location to store\n                         // the result. so we create a temporary alloca for the\n                         // result\n-                        let tmp = bcx.with_block(|bcx| {\n-                            base::alloc_ty(bcx, ret_ty, \"tmp_ret\")\n-                        });\n+                        let tmp = base::alloc_ty(bcx, ret_ty, \"tmp_ret\");\n                         ReturnDest::IndirectOperand(tmp, index)\n                     } else {\n                         ReturnDest::DirectOperand(index)\n@@ -939,9 +980,7 @@ impl<'bcx, 'tcx> MirContext<'bcx, 'tcx> {\n             DirectOperand(index) => {\n                 // If there is a cast, we have to store and reload.\n                 let op = if ret_ty.cast.is_some() {\n-                    let tmp = bcx.with_block(|bcx| {\n-                        base::alloc_ty(bcx, op.ty, \"tmp_ret\")\n-                    });\n+                    let tmp = base::alloc_ty(bcx, op.ty, \"tmp_ret\");\n                     ret_ty.store(bcx, op.immediate(), tmp);\n                     self.trans_load(bcx, tmp, op.ty)\n                 } else {"}, {"sha": "e211a8b68d4f33437a195dc0b6eeeff5223fc602", "filename": "src/librustc_trans/mir/lvalue.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/bf7d4534a7ed27f1a0b5c9b53b2af155da33f072/src%2Flibrustc_trans%2Fmir%2Flvalue.rs", "raw_url": "https://github.com/rust-lang/rust/raw/bf7d4534a7ed27f1a0b5c9b53b2af155da33f072/src%2Flibrustc_trans%2Fmir%2Flvalue.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_trans%2Fmir%2Flvalue.rs?ref=bf7d4534a7ed27f1a0b5c9b53b2af155da33f072", "patch": "@@ -50,7 +50,7 @@ impl<'tcx> LvalueRef<'tcx> {\n                         -> LvalueRef<'tcx>\n     {\n         assert!(!ty.has_erasable_regions());\n-        let lltemp = bcx.with_block(|bcx| base::alloc_ty(bcx, ty, name));\n+        let lltemp = base::alloc_ty(bcx, ty, name);\n         LvalueRef::new_sized(lltemp, LvalueTy::from_ty(ty))\n     }\n "}, {"sha": "174608bdeb98715a499dc37903f95267aab6db43", "filename": "src/librustc_trans/mir/mod.rs", "status": "modified", "additions": 10, "deletions": 16, "changes": 26, "blob_url": "https://github.com/rust-lang/rust/blob/bf7d4534a7ed27f1a0b5c9b53b2af155da33f072/src%2Flibrustc_trans%2Fmir%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/bf7d4534a7ed27f1a0b5c9b53b2af155da33f072/src%2Flibrustc_trans%2Fmir%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_trans%2Fmir%2Fmod.rs?ref=bf7d4534a7ed27f1a0b5c9b53b2af155da33f072", "patch": "@@ -181,7 +181,7 @@ impl<'tcx> LocalRef<'tcx> {\n ///////////////////////////////////////////////////////////////////////////\n \n pub fn trans_mir<'blk, 'tcx: 'blk>(fcx: &'blk FunctionContext<'blk, 'tcx>) {\n-    let bcx = fcx.init(true).build();\n+    let bcx = fcx.init(true);\n     let mir = bcx.mir();\n \n     // Analyze the temps to determine which must be lvalues\n@@ -240,11 +240,9 @@ pub fn trans_mir<'blk, 'tcx: 'blk>(fcx: &'blk FunctionContext<'blk, 'tcx>) {\n                 if dbg {\n                     let dbg_loc = mircx.debug_loc(source_info);\n                     if let DebugLoc::ScopeAt(scope, span) = dbg_loc {\n-                        bcx.with_block(|bcx| {\n-                            declare_local(bcx, name, ty, scope,\n-                                        VariableAccess::DirectVariable { alloca: lvalue.llval },\n-                                        VariableKind::LocalVariable, span);\n-                        });\n+                        declare_local(&bcx, name, ty, scope,\n+                            VariableAccess::DirectVariable { alloca: lvalue.llval },\n+                            VariableKind::LocalVariable, span);\n                     } else {\n                         panic!(\"Unexpected\");\n                     }\n@@ -353,9 +351,7 @@ fn arg_local_refs<'bcx, 'tcx>(bcx: &BlockAndBuilder<'bcx, 'tcx>,\n                 _ => bug!(\"spread argument isn't a tuple?!\")\n             };\n \n-            let lltemp = bcx.with_block(|bcx| {\n-                base::alloc_ty(bcx, arg_ty, &format!(\"arg{}\", arg_index))\n-            });\n+            let lltemp = base::alloc_ty(&bcx, arg_ty, &format!(\"arg{}\", arg_index));\n             for (i, &tupled_arg_ty) in tupled_arg_tys.iter().enumerate() {\n                 let dst = bcx.struct_gep(lltemp, i);\n                 let arg = &fcx.fn_ty.args[idx];\n@@ -376,15 +372,15 @@ fn arg_local_refs<'bcx, 'tcx>(bcx: &BlockAndBuilder<'bcx, 'tcx>,\n \n             // Now that we have one alloca that contains the aggregate value,\n             // we can create one debuginfo entry for the argument.\n-            bcx.with_block(|bcx| arg_scope.map(|scope| {\n+            arg_scope.map(|scope| {\n                 let variable_access = VariableAccess::DirectVariable {\n                     alloca: lltemp\n                 };\n                 declare_local(bcx, arg_decl.name.unwrap_or(keywords::Invalid.name()),\n                               arg_ty, scope, variable_access,\n                               VariableKind::ArgumentVariable(arg_index + 1),\n                               bcx.fcx().span.unwrap_or(DUMMY_SP));\n-            }));\n+            });\n \n             return LocalRef::Lvalue(LvalueRef::new_sized(lltemp, LvalueTy::from_ty(arg_ty)));\n         }\n@@ -433,9 +429,7 @@ fn arg_local_refs<'bcx, 'tcx>(bcx: &BlockAndBuilder<'bcx, 'tcx>,\n             };\n             return LocalRef::Operand(Some(operand.unpack_if_pair(bcx)));\n         } else {\n-            let lltemp = bcx.with_block(|bcx| {\n-                base::alloc_ty(bcx, arg_ty, &format!(\"arg{}\", arg_index))\n-            });\n+            let lltemp = base::alloc_ty(&bcx, arg_ty, &format!(\"arg{}\", arg_index));\n             if common::type_is_fat_ptr(tcx, arg_ty) {\n                 // we pass fat pointers as two words, but we want to\n                 // represent them internally as a pointer to two words,\n@@ -453,7 +447,7 @@ fn arg_local_refs<'bcx, 'tcx>(bcx: &BlockAndBuilder<'bcx, 'tcx>,\n             }\n             lltemp\n         };\n-        bcx.with_block(|bcx| arg_scope.map(|scope| {\n+        arg_scope.map(|scope| {\n             // Is this a regular argument?\n             if arg_index > 0 || mir.upvar_decls.is_empty() {\n                 declare_local(bcx, arg_decl.name.unwrap_or(keywords::Invalid.name()), arg_ty,\n@@ -531,7 +525,7 @@ fn arg_local_refs<'bcx, 'tcx>(bcx: &BlockAndBuilder<'bcx, 'tcx>,\n                               VariableKind::CapturedVariable,\n                               bcx.fcx().span.unwrap_or(DUMMY_SP));\n             }\n-        }));\n+        });\n         LocalRef::Lvalue(LvalueRef::new_sized(llval, LvalueTy::from_ty(arg_ty)))\n     }).collect()\n }"}, {"sha": "a7fdc4330becc6976ff4ffa5e746706da8448f75", "filename": "src/librustc_trans/mir/operand.rs", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "blob_url": "https://github.com/rust-lang/rust/blob/bf7d4534a7ed27f1a0b5c9b53b2af155da33f072/src%2Flibrustc_trans%2Fmir%2Foperand.rs", "raw_url": "https://github.com/rust-lang/rust/raw/bf7d4534a7ed27f1a0b5c9b53b2af155da33f072/src%2Flibrustc_trans%2Fmir%2Foperand.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_trans%2Fmir%2Foperand.rs?ref=bf7d4534a7ed27f1a0b5c9b53b2af155da33f072", "patch": "@@ -14,7 +14,7 @@ use rustc::mir;\n use rustc_data_structures::indexed_vec::Idx;\n \n use base;\n-use common::{self, Block, BlockAndBuilder};\n+use common::{self, BlockAndBuilder};\n use value::Value;\n use type_of;\n use type_::Type;\n@@ -247,11 +247,11 @@ impl<'bcx, 'tcx> MirContext<'bcx, 'tcx> {\n                          operand: OperandRef<'tcx>)\n     {\n         debug!(\"store_operand: operand={:?} lldest={:?}\", operand, lldest);\n-        bcx.with_block(|bcx| self.store_operand_direct(bcx, lldest, operand))\n+        self.store_operand_direct(bcx, lldest, operand)\n     }\n \n     pub fn store_operand_direct(&mut self,\n-                                bcx: Block<'bcx, 'tcx>,\n+                                bcx: &BlockAndBuilder<'bcx, 'tcx>,\n                                 lldest: ValueRef,\n                                 operand: OperandRef<'tcx>)\n     {"}, {"sha": "274871d7552f35dc8b4e1bc756ddfbcee204cdeb", "filename": "src/librustc_trans/mir/rvalue.rs", "status": "modified", "additions": 54, "deletions": 72, "changes": 126, "blob_url": "https://github.com/rust-lang/rust/blob/bf7d4534a7ed27f1a0b5c9b53b2af155da33f072/src%2Flibrustc_trans%2Fmir%2Frvalue.rs", "raw_url": "https://github.com/rust-lang/rust/raw/bf7d4534a7ed27f1a0b5c9b53b2af155da33f072/src%2Flibrustc_trans%2Fmir%2Frvalue.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_trans%2Fmir%2Frvalue.rs?ref=bf7d4534a7ed27f1a0b5c9b53b2af155da33f072", "patch": "@@ -17,7 +17,7 @@ use rustc::mir;\n use asm;\n use base;\n use callee::Callee;\n-use common::{self, val_ty, C_bool, C_null, C_uint, BlockAndBuilder, Result};\n+use common::{self, val_ty, C_bool, C_null, C_uint, BlockAndBuilder};\n use common::{C_integral};\n use debuginfo::DebugLoc;\n use adt;\n@@ -70,30 +70,28 @@ impl<'bcx, 'tcx> MirContext<'bcx, 'tcx> {\n                 // so the (generic) MIR may not be able to expand it.\n                 let operand = self.trans_operand(&bcx, source);\n                 let operand = operand.pack_if_pair(&bcx);\n-                bcx.with_block(|bcx| {\n-                    match operand.val {\n-                        OperandValue::Pair(..) => bug!(),\n-                        OperandValue::Immediate(llval) => {\n-                            // unsize from an immediate structure. We don't\n-                            // really need a temporary alloca here, but\n-                            // avoiding it would require us to have\n-                            // `coerce_unsized_into` use extractvalue to\n-                            // index into the struct, and this case isn't\n-                            // important enough for it.\n-                            debug!(\"trans_rvalue: creating ugly alloca\");\n-                            let lltemp = base::alloc_ty(bcx, operand.ty, \"__unsize_temp\");\n-                            base::store_ty(bcx, llval, lltemp, operand.ty);\n-                            base::coerce_unsized_into(bcx,\n-                                                      lltemp, operand.ty,\n-                                                      dest.llval, cast_ty);\n-                        }\n-                        OperandValue::Ref(llref) => {\n-                            base::coerce_unsized_into(bcx,\n-                                                      llref, operand.ty,\n-                                                      dest.llval, cast_ty);\n-                        }\n+                match operand.val {\n+                    OperandValue::Pair(..) => bug!(),\n+                    OperandValue::Immediate(llval) => {\n+                        // unsize from an immediate structure. We don't\n+                        // really need a temporary alloca here, but\n+                        // avoiding it would require us to have\n+                        // `coerce_unsized_into` use extractvalue to\n+                        // index into the struct, and this case isn't\n+                        // important enough for it.\n+                        debug!(\"trans_rvalue: creating ugly alloca\");\n+                        let lltemp = base::alloc_ty(&bcx, operand.ty, \"__unsize_temp\");\n+                        base::store_ty(&bcx, llval, lltemp, operand.ty);\n+                        base::coerce_unsized_into(&bcx,\n+                            lltemp, operand.ty,\n+                            dest.llval, cast_ty);\n                     }\n-                });\n+                    OperandValue::Ref(llref) => {\n+                        base::coerce_unsized_into(&bcx,\n+                            llref, operand.ty,\n+                            dest.llval, cast_ty);\n+                    }\n+                }\n                 bcx\n             }\n \n@@ -102,11 +100,9 @@ impl<'bcx, 'tcx> MirContext<'bcx, 'tcx> {\n                 let size = count.value.as_u64(bcx.tcx().sess.target.uint_type);\n                 let size = C_uint(bcx.ccx(), size);\n                 let base = base::get_dataptr_builder(&bcx, dest.llval);\n-                let bcx = bcx.map_block(|block| {\n-                    tvec::slice_for_each(block, base, tr_elem.ty, size, |block, llslot| {\n-                        self.store_operand_direct(block, llslot, tr_elem);\n-                        block\n-                    })\n+                let bcx = tvec::slice_for_each(bcx, base, tr_elem.ty, size, |bcx, llslot| {\n+                    self.store_operand_direct(&bcx, llslot, tr_elem);\n+                    bcx\n                 });\n                 bcx\n             }\n@@ -115,10 +111,8 @@ impl<'bcx, 'tcx> MirContext<'bcx, 'tcx> {\n                 match *kind {\n                     mir::AggregateKind::Adt(adt_def, variant_index, _, active_field_index) => {\n                         let disr = Disr::from(adt_def.variants[variant_index].disr_val);\n-                        bcx.with_block(|bcx| {\n-                            adt::trans_set_discr(bcx,\n-                                dest.ty.to_ty(bcx.tcx()), dest.llval, Disr::from(disr));\n-                        });\n+                        adt::trans_set_discr(&bcx,\n+                            dest.ty.to_ty(bcx.tcx()), dest.llval, Disr::from(disr));\n                         for (i, operand) in operands.iter().enumerate() {\n                             let op = self.trans_operand(&bcx, operand);\n                             // Do not generate stores and GEPis for zero-sized fields.\n@@ -171,10 +165,7 @@ impl<'bcx, 'tcx> MirContext<'bcx, 'tcx> {\n                     self.trans_operand(&bcx, input).immediate()\n                 }).collect();\n \n-                bcx.with_block(|bcx| {\n-                    asm::trans_inline_asm(bcx, asm, outputs, input_vals);\n-                });\n-\n+                asm::trans_inline_asm(&bcx, asm, outputs, input_vals);\n                 bcx\n             }\n \n@@ -238,10 +229,8 @@ impl<'bcx, 'tcx> MirContext<'bcx, 'tcx> {\n                             }\n                             OperandValue::Immediate(lldata) => {\n                                 // \"standard\" unsize\n-                                let (lldata, llextra) = bcx.with_block(|bcx| {\n-                                    base::unsize_thin_ptr(bcx, lldata,\n-                                                          operand.ty, cast_ty)\n-                                });\n+                                let (lldata, llextra) = base::unsize_thin_ptr(&bcx, lldata,\n+                                    operand.ty, cast_ty);\n                                 OperandValue::Pair(lldata, llextra)\n                             }\n                             OperandValue::Ref(_) => {\n@@ -281,9 +270,7 @@ impl<'bcx, 'tcx> MirContext<'bcx, 'tcx> {\n                             let discr = match operand.val {\n                                 OperandValue::Immediate(llval) => llval,\n                                 OperandValue::Ref(llptr) => {\n-                                    bcx.with_block(|bcx| {\n-                                        adt::trans_get_discr(bcx, operand.ty, llptr, None, true)\n-                                    })\n+                                    adt::trans_get_discr(&bcx, operand.ty, llptr, None, true)\n                                 }\n                                 OperandValue::Pair(..) => bug!(\"Unexpected Pair operand\")\n                             };\n@@ -468,19 +455,16 @@ impl<'bcx, 'tcx> MirContext<'bcx, 'tcx> {\n                 let llalign = C_uint(bcx.ccx(), align);\n                 let llty_ptr = llty.ptr_to();\n                 let box_ty = bcx.tcx().mk_box(content_ty);\n-                let mut llval = None;\n-                let bcx = bcx.map_block(|bcx| {\n-                    let Result { bcx, val } = base::malloc_raw_dyn(bcx,\n-                                                                   llty_ptr,\n-                                                                   box_ty,\n-                                                                   llsize,\n-                                                                   llalign,\n-                                                                   debug_loc);\n-                    llval = Some(val);\n-                    bcx\n-                });\n+                let val = base::malloc_raw_dyn(\n+                    &bcx,\n+                    llty_ptr,\n+                    box_ty,\n+                    llsize,\n+                    llalign,\n+                    debug_loc\n+                );\n                 let operand = OperandRef {\n-                    val: OperandValue::Immediate(llval.unwrap()),\n+                    val: OperandValue::Immediate(val),\n                     ty: box_ty,\n                 };\n                 (bcx, operand)\n@@ -543,21 +527,21 @@ impl<'bcx, 'tcx> MirContext<'bcx, 'tcx> {\n             mir::BinOp::BitAnd => bcx.and(lhs, rhs),\n             mir::BinOp::BitXor => bcx.xor(lhs, rhs),\n             mir::BinOp::Shl => {\n-                bcx.with_block(|bcx| {\n-                    common::build_unchecked_lshift(bcx,\n-                                                   lhs,\n-                                                   rhs,\n-                                                   DebugLoc::None)\n-                })\n+                common::build_unchecked_lshift(\n+                    &bcx,\n+                    lhs,\n+                    rhs,\n+                    DebugLoc::None\n+                )\n             }\n             mir::BinOp::Shr => {\n-                bcx.with_block(|bcx| {\n-                    common::build_unchecked_rshift(bcx,\n-                                                   input_ty,\n-                                                   lhs,\n-                                                   rhs,\n-                                                   DebugLoc::None)\n-                })\n+                common::build_unchecked_rshift(\n+                    bcx,\n+                    input_ty,\n+                    lhs,\n+                    rhs,\n+                    DebugLoc::None\n+                )\n             }\n             mir::BinOp::Ne | mir::BinOp::Lt | mir::BinOp::Gt |\n             mir::BinOp::Eq | mir::BinOp::Le | mir::BinOp::Ge => if is_nil {\n@@ -677,9 +661,7 @@ impl<'bcx, 'tcx> MirContext<'bcx, 'tcx> {\n             mir::BinOp::Shl | mir::BinOp::Shr => {\n                 let lhs_llty = val_ty(lhs);\n                 let rhs_llty = val_ty(rhs);\n-                let invert_mask = bcx.with_block(|bcx| {\n-                    common::shift_mask_val(bcx, lhs_llty, rhs_llty, true)\n-                });\n+                let invert_mask = common::shift_mask_val(&bcx, lhs_llty, rhs_llty, true);\n                 let outer_bits = bcx.and(rhs, invert_mask);\n \n                 let of = bcx.icmp(llvm::IntNE, outer_bits, C_null(rhs_llty));"}, {"sha": "9c872e214d2f477b78610272dc57d25ef401b1c1", "filename": "src/librustc_trans/mir/statement.rs", "status": "modified", "additions": 4, "deletions": 6, "changes": 10, "blob_url": "https://github.com/rust-lang/rust/blob/bf7d4534a7ed27f1a0b5c9b53b2af155da33f072/src%2Flibrustc_trans%2Fmir%2Fstatement.rs", "raw_url": "https://github.com/rust-lang/rust/raw/bf7d4534a7ed27f1a0b5c9b53b2af155da33f072/src%2Flibrustc_trans%2Fmir%2Fstatement.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_trans%2Fmir%2Fstatement.rs?ref=bf7d4534a7ed27f1a0b5c9b53b2af155da33f072", "patch": "@@ -63,12 +63,10 @@ impl<'bcx, 'tcx> MirContext<'bcx, 'tcx> {\n             mir::StatementKind::SetDiscriminant{ref lvalue, variant_index} => {\n                 let ty = self.monomorphized_lvalue_ty(lvalue);\n                 let lvalue_transed = self.trans_lvalue(&bcx, lvalue);\n-                bcx.with_block(|bcx|\n-                    adt::trans_set_discr(bcx,\n-                                         ty,\n-                                        lvalue_transed.llval,\n-                                        Disr::from(variant_index))\n-                );\n+                adt::trans_set_discr(&bcx,\n+                    ty,\n+                    lvalue_transed.llval,\n+                    Disr::from(variant_index));\n                 bcx\n             }\n             mir::StatementKind::StorageLive(ref lvalue) => {"}, {"sha": "3dbaaff1f48f0da585584ef5fc8e083e0596b17c", "filename": "src/librustc_trans/tvec.rs", "status": "modified", "additions": 21, "deletions": 15, "changes": 36, "blob_url": "https://github.com/rust-lang/rust/blob/bf7d4534a7ed27f1a0b5c9b53b2af155da33f072/src%2Flibrustc_trans%2Ftvec.rs", "raw_url": "https://github.com/rust-lang/rust/raw/bf7d4534a7ed27f1a0b5c9b53b2af155da33f072/src%2Flibrustc_trans%2Ftvec.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_trans%2Ftvec.rs?ref=bf7d4534a7ed27f1a0b5c9b53b2af155da33f072", "patch": "@@ -18,16 +18,16 @@ use common::*;\n use debuginfo::DebugLoc;\n use rustc::ty::Ty;\n \n-pub fn slice_for_each<'blk, 'tcx, F>(bcx: Block<'blk, 'tcx>,\n+pub fn slice_for_each<'blk, 'tcx, F>(bcx: BlockAndBuilder<'blk, 'tcx>,\n                                      data_ptr: ValueRef,\n                                      unit_ty: Ty<'tcx>,\n                                      len: ValueRef,\n                                      f: F)\n-                                     -> Block<'blk, 'tcx> where\n-    F: FnOnce(Block<'blk, 'tcx>, ValueRef) -> Block<'blk, 'tcx>,\n+                                     -> BlockAndBuilder<'blk, 'tcx>\n+    where F: FnOnce(BlockAndBuilder<'blk, 'tcx>, ValueRef) -> BlockAndBuilder<'blk, 'tcx>,\n {\n     let _icx = push_ctxt(\"tvec::slice_for_each\");\n-    let fcx = bcx.fcx;\n+    let fcx = bcx.fcx();\n \n     // Special-case vectors with elements of size 0  so they don't go out of bounds (#9890)\n     let zst = type_is_zero_size(bcx.ccx(), unit_ty);\n@@ -37,27 +37,33 @@ pub fn slice_for_each<'blk, 'tcx, F>(bcx: Block<'blk, 'tcx>,\n         InBoundsGEP(bcx, a, &[b])\n     };\n \n-    let header_bcx = fcx.new_block(\"slice_loop_header\");\n-    let body_bcx = fcx.new_block(\"slice_loop_body\");\n-    let next_bcx = fcx.new_block(\"slice_loop_next\");\n+    let body_bcx = fcx.new_block(\"slice_loop_body\").build();\n+    let next_bcx = fcx.new_block(\"slice_loop_next\").build();\n+    let header_bcx = fcx.new_block(\"slice_loop_header\").build();\n \n     let start = if zst {\n         C_uint(bcx.ccx(), 0 as usize)\n     } else {\n         data_ptr\n     };\n-    let end = add(bcx, start, len);\n+    let end = add(&bcx, start, len);\n \n-    Br(bcx, header_bcx.llbb, DebugLoc::None);\n-    let current = Phi(header_bcx, val_ty(start), &[start], &[bcx.llbb]);\n+    Br(&bcx, header_bcx.llbb(), DebugLoc::None);\n+    let current = Phi(&header_bcx, val_ty(start), &[start], &[bcx.llbb()]);\n \n     let keep_going =\n-        ICmp(header_bcx, llvm::IntNE, current, end, DebugLoc::None);\n-    CondBr(header_bcx, keep_going, body_bcx.llbb, next_bcx.llbb, DebugLoc::None);\n+        ICmp(&header_bcx, llvm::IntNE, current, end, DebugLoc::None);\n+    CondBr(&header_bcx, keep_going, body_bcx.llbb(), next_bcx.llbb(), DebugLoc::None);\n \n     let body_bcx = f(body_bcx, if zst { data_ptr } else { current });\n-    let next = add(body_bcx, current, C_uint(bcx.ccx(), 1usize));\n-    AddIncomingToPhi(current, next, body_bcx.llbb);\n-    Br(body_bcx, header_bcx.llbb, DebugLoc::None);\n+    // FIXME(simulacrum): The code below is identical to the closure (add) above, but using the\n+    // closure doesn't compile due to body_bcx still being borrowed when dropped.\n+    let next = if zst {\n+        Add(&body_bcx, current, C_uint(bcx.ccx(), 1usize), DebugLoc::None)\n+    } else {\n+        InBoundsGEP(&body_bcx, current, &[C_uint(bcx.ccx(), 1usize)])\n+    };\n+    AddIncomingToPhi(current, next, body_bcx.llbb());\n+    Br(&body_bcx, header_bcx.llbb(), DebugLoc::None);\n     next_bcx\n }"}, {"sha": "b314f3ea414f6e446163258b31c8818e583f6994", "filename": "src/librustc_trans/value.rs", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "blob_url": "https://github.com/rust-lang/rust/blob/bf7d4534a7ed27f1a0b5c9b53b2af155da33f072/src%2Flibrustc_trans%2Fvalue.rs", "raw_url": "https://github.com/rust-lang/rust/raw/bf7d4534a7ed27f1a0b5c9b53b2af155da33f072/src%2Flibrustc_trans%2Fvalue.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_trans%2Fvalue.rs?ref=bf7d4534a7ed27f1a0b5c9b53b2af155da33f072", "patch": "@@ -11,7 +11,7 @@\n use llvm;\n use llvm::{UseRef, ValueRef};\n use basic_block::BasicBlock;\n-use common::Block;\n+use common::BlockAndBuilder;\n \n use std::fmt;\n \n@@ -65,11 +65,11 @@ impl Value {\n     /// This only performs a search for a trivially dominating store. The store\n     /// must be the only user of this value, and there must not be any conditional\n     /// branches between the store and the given block.\n-    pub fn get_dominating_store(self, bcx: Block) -> Option<Value> {\n+    pub fn get_dominating_store(self, bcx: &BlockAndBuilder) -> Option<Value> {\n         match self.get_single_user().and_then(|user| user.as_store_inst()) {\n             Some(store) => {\n                 store.get_parent().and_then(|store_bb| {\n-                    let mut bb = BasicBlock(bcx.llbb);\n+                    let mut bb = BasicBlock(bcx.llbb());\n                     let mut ret = Some(store);\n                     while bb.get() != store_bb.get() {\n                         match bb.get_single_predecessor() {"}]}