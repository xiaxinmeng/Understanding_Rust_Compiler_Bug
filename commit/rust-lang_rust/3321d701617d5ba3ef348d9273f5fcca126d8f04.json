{"sha": "3321d701617d5ba3ef348d9273f5fcca126d8f04", "node_id": "MDY6Q29tbWl0NzI0NzEyOjMzMjFkNzAxNjE3ZDViYTNlZjM0OGQ5MjczZjVmY2NhMTI2ZDhmMDQ=", "commit": {"author": {"name": "Aaron Hill", "email": "aa1ronham@gmail.com", "date": "2021-02-13T17:42:43Z"}, "committer": {"name": "Aaron Hill", "email": "aa1ronham@gmail.com", "date": "2021-02-13T18:04:54Z"}, "message": "Address review comments", "tree": {"sha": "20ebc0a01cc1d55ce3323ba89dac5473abe9168d", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/20ebc0a01cc1d55ce3323ba89dac5473abe9168d"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/3321d701617d5ba3ef348d9273f5fcca126d8f04", "comment_count": 0, "verification": {"verified": true, "reason": "valid", "signature": "-----BEGIN PGP SIGNATURE-----\n\niQIzBAABCAAdFiEE7J9Gc3TfBwj2K399tAh+UQ6YsWQFAmAoFMcACgkQtAh+UQ6Y\nsWTWnRAApox6CQ1qFWT8e+CwDuCTYvXbbkzBB33xcEI5AZfj8qJXIbYp7+JXh9fp\nYybfdAooGzpP1SUXQ1HeZwS7U/M5Gc3RdMPPjyDsv+Lmit7RMrA5gn9u023v1X64\nKXYop+fC4eFdcuNh54Z+vYT87RYDUupp+wYldIzqFW1yPwhRBD/Au0iMqrUt1+sG\n+qbWHSz96HP5Vc4JEUOOdSJLJztQ+y6arcZ9H/5I5EFH3o/0l7LAyCqhtmnPvhef\nC3nbypAX8EXd1kaRDbRFeMbNtHlceSGa8RBKovk/6NvQ992wmC168SPovZevizbi\nAFTLnt34PxNG6ypn9lkFdkEfWX+awvXWVmvsqnzPndYvxp3tRbUW0exWm1o9qwrF\nwH5TuWnqsQ1Ms7cApdWO77eEymW5K34LNIrDgbUzEjXlOzqFP7TuL2v8NGYA23qa\nuLLKI4wrsnU/bq5GRMv0xTzTVE52l2PaCz8nIhvdM++OXXHdzehDFNhJTPh7/pd5\n0raSg10t0ilrHg24FiNtI8Q2kHNdCV1OXUiqzy9PGBktDV0f9lE8miEg1eBnBUhB\nFCSEpj0fBe0Je6Cs7fytAqay+U4z3GX/Io6kPDVlxrF1qyVkrpBql3DYO8SBmQq1\nY7w1DHKTtio29l4sj7ZnODydYjxjq1pu0wmGvoVGLMeTleOPWAc=\n=cQ6G\n-----END PGP SIGNATURE-----", "payload": "tree 20ebc0a01cc1d55ce3323ba89dac5473abe9168d\nparent 0b411f56e1a539e2388e9a983feb45f362923dc5\nauthor Aaron Hill <aa1ronham@gmail.com> 1613238163 -0500\ncommitter Aaron Hill <aa1ronham@gmail.com> 1613239494 -0500\n\nAddress review comments\n"}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/3321d701617d5ba3ef348d9273f5fcca126d8f04", "html_url": "https://github.com/rust-lang/rust/commit/3321d701617d5ba3ef348d9273f5fcca126d8f04", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/3321d701617d5ba3ef348d9273f5fcca126d8f04/comments", "author": {"login": "Aaron1011", "id": 1408859, "node_id": "MDQ6VXNlcjE0MDg4NTk=", "avatar_url": "https://avatars.githubusercontent.com/u/1408859?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Aaron1011", "html_url": "https://github.com/Aaron1011", "followers_url": "https://api.github.com/users/Aaron1011/followers", "following_url": "https://api.github.com/users/Aaron1011/following{/other_user}", "gists_url": "https://api.github.com/users/Aaron1011/gists{/gist_id}", "starred_url": "https://api.github.com/users/Aaron1011/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Aaron1011/subscriptions", "organizations_url": "https://api.github.com/users/Aaron1011/orgs", "repos_url": "https://api.github.com/users/Aaron1011/repos", "events_url": "https://api.github.com/users/Aaron1011/events{/privacy}", "received_events_url": "https://api.github.com/users/Aaron1011/received_events", "type": "User", "site_admin": false}, "committer": {"login": "Aaron1011", "id": 1408859, "node_id": "MDQ6VXNlcjE0MDg4NTk=", "avatar_url": "https://avatars.githubusercontent.com/u/1408859?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Aaron1011", "html_url": "https://github.com/Aaron1011", "followers_url": "https://api.github.com/users/Aaron1011/followers", "following_url": "https://api.github.com/users/Aaron1011/following{/other_user}", "gists_url": "https://api.github.com/users/Aaron1011/gists{/gist_id}", "starred_url": "https://api.github.com/users/Aaron1011/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Aaron1011/subscriptions", "organizations_url": "https://api.github.com/users/Aaron1011/orgs", "repos_url": "https://api.github.com/users/Aaron1011/repos", "events_url": "https://api.github.com/users/Aaron1011/events{/privacy}", "received_events_url": "https://api.github.com/users/Aaron1011/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "0b411f56e1a539e2388e9a983feb45f362923dc5", "url": "https://api.github.com/repos/rust-lang/rust/commits/0b411f56e1a539e2388e9a983feb45f362923dc5", "html_url": "https://github.com/rust-lang/rust/commit/0b411f56e1a539e2388e9a983feb45f362923dc5"}], "stats": {"total": 380, "additions": 201, "deletions": 179}, "files": [{"sha": "95d4a48b845ef99952660f06c82dc7ca833712d1", "filename": "compiler/rustc_parse/src/parser/attr.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/3321d701617d5ba3ef348d9273f5fcca126d8f04/compiler%2Frustc_parse%2Fsrc%2Fparser%2Fattr.rs", "raw_url": "https://github.com/rust-lang/rust/raw/3321d701617d5ba3ef348d9273f5fcca126d8f04/compiler%2Frustc_parse%2Fsrc%2Fparser%2Fattr.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_parse%2Fsrc%2Fparser%2Fattr.rs?ref=3321d701617d5ba3ef348d9273f5fcca126d8f04", "patch": "@@ -74,7 +74,7 @@ impl<'a> Parser<'a> {\n                 break;\n             }\n         }\n-        Ok(AttrWrapper { attrs })\n+        Ok(AttrWrapper::new(attrs))\n     }\n \n     /// Matches `attribute = # ! [ meta_item ]`."}, {"sha": "aea7c6b42cf07801566ac1df5a421ef2a31df06f", "filename": "compiler/rustc_parse/src/parser/attr_wrapper.rs", "status": "added", "additions": 185, "deletions": 0, "changes": 185, "blob_url": "https://github.com/rust-lang/rust/blob/3321d701617d5ba3ef348d9273f5fcca126d8f04/compiler%2Frustc_parse%2Fsrc%2Fparser%2Fattr_wrapper.rs", "raw_url": "https://github.com/rust-lang/rust/raw/3321d701617d5ba3ef348d9273f5fcca126d8f04/compiler%2Frustc_parse%2Fsrc%2Fparser%2Fattr_wrapper.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_parse%2Fsrc%2Fparser%2Fattr_wrapper.rs?ref=3321d701617d5ba3ef348d9273f5fcca126d8f04", "patch": "@@ -0,0 +1,185 @@\n+use super::attr;\n+use super::{ForceCollect, Parser, TokenCursor, TrailingToken};\n+use rustc_ast::token::{self, Token, TokenKind};\n+use rustc_ast::tokenstream::{CreateTokenStream, TokenStream, TokenTree, TreeAndSpacing};\n+use rustc_ast::tokenstream::{DelimSpan, LazyTokenStream, Spacing};\n+use rustc_ast::HasTokens;\n+use rustc_ast::{self as ast};\n+use rustc_errors::PResult;\n+use rustc_span::{Span, DUMMY_SP};\n+\n+/// A wrapper type to ensure that the parser handles outer attributes correctly.\n+/// When we parse outer attributes, we need to ensure that we capture tokens\n+/// for the attribute target. This allows us to perform cfg-expansion on\n+/// a token stream before we invoke a derive proc-macro.\n+///\n+/// This wrapper prevents direct access to the underlying `Vec<ast::Attribute>`.\n+/// Parsing code can only get access to the underlying attributes\n+/// by passing an `AttrWrapper` to `collect_tokens_trailing_tokens`.\n+/// This makes it difficult to accidentally construct an AST node\n+/// (which stores a `Vec<ast::Attribute>`) without first collecting tokens.\n+///\n+/// This struct has its own module, to ensure that the parser code\n+/// cannot directly access the `attrs` field\n+#[derive(Debug, Clone)]\n+pub struct AttrWrapper {\n+    attrs: Vec<ast::Attribute>,\n+}\n+\n+impl AttrWrapper {\n+    pub fn empty() -> AttrWrapper {\n+        AttrWrapper { attrs: vec![] }\n+    }\n+    pub fn new(attrs: Vec<ast::Attribute>) -> AttrWrapper {\n+        AttrWrapper { attrs }\n+    }\n+    // FIXME: Delay span bug here?\n+    pub(crate) fn take_for_recovery(self) -> Vec<ast::Attribute> {\n+        self.attrs\n+    }\n+    pub fn is_empty(&self) -> bool {\n+        self.attrs.is_empty()\n+    }\n+}\n+\n+impl<'a> Parser<'a> {\n+    /// Records all tokens consumed by the provided callback,\n+    /// including the current token. These tokens are collected\n+    /// into a `LazyTokenStream`, and returned along with the result\n+    /// of the callback.\n+    ///\n+    /// Note: If your callback consumes an opening delimiter\n+    /// (including the case where you call `collect_tokens`\n+    /// when the current token is an opening delimeter),\n+    /// you must also consume the corresponding closing delimiter.\n+    ///\n+    /// That is, you can consume\n+    /// `something ([{ }])` or `([{}])`, but not `([{}]`\n+    ///\n+    /// This restriction shouldn't be an issue in practice,\n+    /// since this function is used to record the tokens for\n+    /// a parsed AST item, which always has matching delimiters.\n+    pub fn collect_tokens_trailing_token<R: HasTokens>(\n+        &mut self,\n+        attrs: AttrWrapper,\n+        force_collect: ForceCollect,\n+        f: impl FnOnce(&mut Self, Vec<ast::Attribute>) -> PResult<'a, (R, TrailingToken)>,\n+    ) -> PResult<'a, R> {\n+        if matches!(force_collect, ForceCollect::No) && !attr::maybe_needs_tokens(&attrs.attrs) {\n+            return Ok(f(self, attrs.attrs)?.0);\n+        }\n+        let start_token = (self.token.clone(), self.token_spacing);\n+        let cursor_snapshot = self.token_cursor.clone();\n+\n+        let (mut ret, trailing_token) = f(self, attrs.attrs)?;\n+\n+        // Produces a `TokenStream` on-demand. Using `cursor_snapshot`\n+        // and `num_calls`, we can reconstruct the `TokenStream` seen\n+        // by the callback. This allows us to avoid producing a `TokenStream`\n+        // if it is never needed - for example, a captured `macro_rules!`\n+        // argument that is never passed to a proc macro.\n+        // In practice token stream creation happens rarely compared to\n+        // calls to `collect_tokens` (see some statistics in #78736),\n+        // so we are doing as little up-front work as possible.\n+        //\n+        // This also makes `Parser` very cheap to clone, since\n+        // there is no intermediate collection buffer to clone.\n+        #[derive(Clone)]\n+        struct LazyTokenStreamImpl {\n+            start_token: (Token, Spacing),\n+            cursor_snapshot: TokenCursor,\n+            num_calls: usize,\n+            desugar_doc_comments: bool,\n+            append_unglued_token: Option<TreeAndSpacing>,\n+        }\n+        impl CreateTokenStream for LazyTokenStreamImpl {\n+            fn create_token_stream(&self) -> TokenStream {\n+                // The token produced by the final call to `next` or `next_desugared`\n+                // was not actually consumed by the callback. The combination\n+                // of chaining the initial token and using `take` produces the desired\n+                // result - we produce an empty `TokenStream` if no calls were made,\n+                // and omit the final token otherwise.\n+                let mut cursor_snapshot = self.cursor_snapshot.clone();\n+                let tokens = std::iter::once(self.start_token.clone())\n+                    .chain((0..self.num_calls).map(|_| {\n+                        if self.desugar_doc_comments {\n+                            cursor_snapshot.next_desugared()\n+                        } else {\n+                            cursor_snapshot.next()\n+                        }\n+                    }))\n+                    .take(self.num_calls);\n+\n+                make_token_stream(tokens, self.append_unglued_token.clone())\n+            }\n+        }\n+\n+        let mut num_calls = self.token_cursor.num_next_calls - cursor_snapshot.num_next_calls;\n+        match trailing_token {\n+            TrailingToken::None => {}\n+            TrailingToken::Semi => {\n+                assert_eq!(self.token.kind, token::Semi);\n+                num_calls += 1;\n+            }\n+            TrailingToken::MaybeComma => {\n+                if self.token.kind == token::Comma {\n+                    num_calls += 1;\n+                }\n+            }\n+        }\n+\n+        let lazy_impl = LazyTokenStreamImpl {\n+            start_token,\n+            num_calls,\n+            cursor_snapshot,\n+            desugar_doc_comments: self.desugar_doc_comments,\n+            append_unglued_token: self.token_cursor.append_unglued_token.clone(),\n+        };\n+        ret.finalize_tokens(LazyTokenStream::new(lazy_impl));\n+        Ok(ret)\n+    }\n+}\n+\n+/// Converts a flattened iterator of tokens (including open and close delimiter tokens)\n+/// into a `TokenStream`, creating a `TokenTree::Delimited` for each matching pair\n+/// of open and close delims.\n+fn make_token_stream(\n+    tokens: impl Iterator<Item = (Token, Spacing)>,\n+    append_unglued_token: Option<TreeAndSpacing>,\n+) -> TokenStream {\n+    #[derive(Debug)]\n+    struct FrameData {\n+        open: Span,\n+        inner: Vec<(TokenTree, Spacing)>,\n+    }\n+    let mut stack = vec![FrameData { open: DUMMY_SP, inner: vec![] }];\n+    for (token, spacing) in tokens {\n+        match token {\n+            Token { kind: TokenKind::OpenDelim(_), span } => {\n+                stack.push(FrameData { open: span, inner: vec![] });\n+            }\n+            Token { kind: TokenKind::CloseDelim(delim), span } => {\n+                let frame_data = stack.pop().expect(\"Token stack was empty!\");\n+                let dspan = DelimSpan::from_pair(frame_data.open, span);\n+                let stream = TokenStream::new(frame_data.inner);\n+                let delimited = TokenTree::Delimited(dspan, delim, stream);\n+                stack\n+                    .last_mut()\n+                    .unwrap_or_else(|| panic!(\"Bottom token frame is missing for tokens!\"))\n+                    .inner\n+                    .push((delimited, Spacing::Alone));\n+            }\n+            token => {\n+                stack\n+                    .last_mut()\n+                    .expect(\"Bottom token frame is missing!\")\n+                    .inner\n+                    .push((TokenTree::Token(token), spacing));\n+            }\n+        }\n+    }\n+    let mut final_buf = stack.pop().expect(\"Missing final buf!\");\n+    final_buf.inner.extend(append_unglued_token);\n+    assert!(stack.is_empty(), \"Stack should be empty: final_buf={:?} stack={:?}\", final_buf, stack);\n+    TokenStream::new(final_buf.inner)\n+}"}, {"sha": "20430ece05b06b3874d98920c98296e7c0a38dd7", "filename": "compiler/rustc_parse/src/parser/expr.rs", "status": "modified", "additions": 6, "deletions": 15, "changes": 21, "blob_url": "https://github.com/rust-lang/rust/blob/3321d701617d5ba3ef348d9273f5fcca126d8f04/compiler%2Frustc_parse%2Fsrc%2Fparser%2Fexpr.rs", "raw_url": "https://github.com/rust-lang/rust/raw/3321d701617d5ba3ef348d9273f5fcca126d8f04/compiler%2Frustc_parse%2Fsrc%2Fparser%2Fexpr.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_parse%2Fsrc%2Fparser%2Fexpr.rs?ref=3321d701617d5ba3ef348d9273f5fcca126d8f04", "patch": "@@ -1,9 +1,7 @@\n use super::pat::{GateOr, RecoverComma, PARAM_EXPECTED};\n use super::ty::{AllowPlus, RecoverQPath, RecoverReturnSign};\n-use super::{\n-    AttrWrapper, BlockMode, ForceCollect, Parser, PathStyle, Restrictions, TokenType, TrailingToken,\n-};\n-use super::{SemiColonMode, SeqSep, TokenExpectType};\n+use super::{AttrWrapper, BlockMode, ForceCollect, Parser, PathStyle, Restrictions, TokenType};\n+use super::{SemiColonMode, SeqSep, TokenExpectType, TrailingToken};\n use crate::maybe_recover_from_interpolated_ty_qpath;\n \n use rustc_ast::ptr::P;\n@@ -461,16 +459,11 @@ impl<'a> Parser<'a> {\n             _ => RangeLimits::Closed,\n         };\n         let op = AssocOp::from_token(&self.token);\n+        // FIXME: `parse_prefix_range_expr` is called when the current\n+        // token is `DotDot`, `DotDotDot`, or `DotDotEq`. If we haven't already\n+        // parsed attributes, then trying to parse them here will always fail.\n+        // We should figure out how we want attributes on range expressions to work.\n         let attrs = self.parse_or_use_outer_attributes(attrs)?;\n-        // RESOLVED: It looks like we only haev non-empty attributes here when\n-        // this is used as a statement:\n-        // `#[my_attr] 25..;`\n-        // We should still investigate `parse_or_use_outer_attributes`, since we haven't\n-        // yet eaten the '..'\n-        //\n-        // FIXME - does this code ever haev attributes? `let a = #[attr] ..` doesn't even parse\n-        // // We try to aprse attributes *before* bumping the token, so this can only\n-        // ever succeeed if the `attrs` parameter is `Some`\n         self.collect_tokens_for_expr(attrs, |this, attrs| {\n             let lo = this.token.span;\n             this.bump();\n@@ -518,8 +511,6 @@ impl<'a> Parser<'a> {\n                 make_it!(this, attrs, |this, _| this.parse_box_expr(lo))\n             }\n             token::Ident(..) if this.is_mistaken_not_ident_negation() => {\n-                // FIXME - what is our polciy for handling tokens during recovery?\n-                // Should we ever invoke a proc-macro with these tokens?\n                 make_it!(this, attrs, |this, _| this.recover_not_expr(lo))\n             }\n             _ => return this.parse_dot_or_call_expr(Some(attrs.into())),"}, {"sha": "18013f1250bdba70f5317434895909d5303a3bd0", "filename": "compiler/rustc_parse/src/parser/mod.rs", "status": "modified", "additions": 6, "deletions": 159, "changes": 165, "blob_url": "https://github.com/rust-lang/rust/blob/3321d701617d5ba3ef348d9273f5fcca126d8f04/compiler%2Frustc_parse%2Fsrc%2Fparser%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/3321d701617d5ba3ef348d9273f5fcca126d8f04/compiler%2Frustc_parse%2Fsrc%2Fparser%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_parse%2Fsrc%2Fparser%2Fmod.rs?ref=3321d701617d5ba3ef348d9273f5fcca126d8f04", "patch": "@@ -1,4 +1,5 @@\n pub mod attr;\n+mod attr_wrapper;\n mod diagnostics;\n mod expr;\n mod generics;\n@@ -10,14 +11,15 @@ mod stmt;\n mod ty;\n \n use crate::lexer::UnmatchedBrace;\n+pub use attr_wrapper::AttrWrapper;\n pub use diagnostics::AttemptLocalParseRecovery;\n use diagnostics::Error;\n pub use path::PathStyle;\n \n use rustc_ast::ptr::P;\n use rustc_ast::token::{self, DelimToken, Token, TokenKind};\n-use rustc_ast::tokenstream::{self, DelimSpan, LazyTokenStream, Spacing};\n-use rustc_ast::tokenstream::{CreateTokenStream, TokenStream, TokenTree, TreeAndSpacing};\n+use rustc_ast::tokenstream::{self, DelimSpan, Spacing};\n+use rustc_ast::tokenstream::{TokenStream, TokenTree, TreeAndSpacing};\n use rustc_ast::DUMMY_NODE_ID;\n use rustc_ast::{self as ast, AnonConst, AttrStyle, AttrVec, Const, CrateSugar, Extern, HasTokens};\n use rustc_ast::{Async, Expr, ExprKind, MacArgs, MacDelimiter, Mutability, StrLit, Unsafe};\n@@ -69,21 +71,6 @@ pub enum TrailingToken {\n     MaybeComma,\n }\n \n-#[derive(Debug, Clone)]\n-pub struct AttrWrapper {\n-    attrs: Vec<ast::Attribute>,\n-}\n-\n-impl AttrWrapper {\n-    // FIXME: Delay span bug here?\n-    fn take_for_recovery(self) -> Vec<ast::Attribute> {\n-        self.attrs\n-    }\n-    fn is_empty(&self) -> bool {\n-        self.attrs.is_empty()\n-    }\n-}\n-\n /// Like `maybe_whole_expr`, but for things other than expressions.\n #[macro_export]\n macro_rules! maybe_whole {\n@@ -999,7 +986,7 @@ impl<'a> Parser<'a> {\n                     }\n \n                     // Collect tokens because they are used during lowering to HIR.\n-                    let expr = self.collect_tokens(|this| this.parse_expr())?;\n+                    let expr = self.collect_tokens_no_attrs(|this| this.parse_expr())?;\n                     let span = expr.span;\n \n                     match &expr.kind {\n@@ -1251,108 +1238,12 @@ impl<'a> Parser<'a> {\n         // The only reason to call `collect_tokens_no_attrs` is if you want tokens, so use\n         // `ForceCollect::Yes`\n         self.collect_tokens_trailing_token(\n-            AttrWrapper { attrs: Vec::new() },\n+            AttrWrapper::empty(),\n             ForceCollect::Yes,\n             |this, _attrs| Ok((f(this)?, TrailingToken::None)),\n         )\n     }\n \n-    /// Records all tokens consumed by the provided callback,\n-    /// including the current token. These tokens are collected\n-    /// into a `LazyTokenStream`, and returned along with the result\n-    /// of the callback.\n-    ///\n-    /// Note: If your callback consumes an opening delimiter\n-    /// (including the case where you call `collect_tokens`\n-    /// when the current token is an opening delimeter),\n-    /// you must also consume the corresponding closing delimiter.\n-    ///\n-    /// That is, you can consume\n-    /// `something ([{ }])` or `([{}])`, but not `([{}]`\n-    ///\n-    /// This restriction shouldn't be an issue in practice,\n-    /// since this function is used to record the tokens for\n-    /// a parsed AST item, which always has matching delimiters.\n-    pub fn collect_tokens_trailing_token<R: HasTokens>(\n-        &mut self,\n-        attrs: AttrWrapper,\n-        force_collect: ForceCollect,\n-        f: impl FnOnce(&mut Self, Vec<ast::Attribute>) -> PResult<'a, (R, TrailingToken)>,\n-    ) -> PResult<'a, R> {\n-        if matches!(force_collect, ForceCollect::No) && !attr::maybe_needs_tokens(&attrs.attrs) {\n-            return Ok(f(self, attrs.attrs)?.0);\n-        }\n-        let start_token = (self.token.clone(), self.token_spacing);\n-        let cursor_snapshot = self.token_cursor.clone();\n-\n-        let (mut ret, trailing_token) = f(self, attrs.attrs)?;\n-\n-        // Produces a `TokenStream` on-demand. Using `cursor_snapshot`\n-        // and `num_calls`, we can reconstruct the `TokenStream` seen\n-        // by the callback. This allows us to avoid producing a `TokenStream`\n-        // if it is never needed - for example, a captured `macro_rules!`\n-        // argument that is never passed to a proc macro.\n-        // In practice token stream creation happens rarely compared to\n-        // calls to `collect_tokens` (see some statistics in #78736),\n-        // so we are doing as little up-front work as possible.\n-        //\n-        // This also makes `Parser` very cheap to clone, since\n-        // there is no intermediate collection buffer to clone.\n-        #[derive(Clone)]\n-        struct LazyTokenStreamImpl {\n-            start_token: (Token, Spacing),\n-            cursor_snapshot: TokenCursor,\n-            num_calls: usize,\n-            desugar_doc_comments: bool,\n-            append_unglued_token: Option<TreeAndSpacing>,\n-        }\n-        impl CreateTokenStream for LazyTokenStreamImpl {\n-            fn create_token_stream(&self) -> TokenStream {\n-                // The token produced by the final call to `next` or `next_desugared`\n-                // was not actually consumed by the callback. The combination\n-                // of chaining the initial token and using `take` produces the desired\n-                // result - we produce an empty `TokenStream` if no calls were made,\n-                // and omit the final token otherwise.\n-                let mut cursor_snapshot = self.cursor_snapshot.clone();\n-                let tokens = std::iter::once(self.start_token.clone())\n-                    .chain((0..self.num_calls).map(|_| {\n-                        if self.desugar_doc_comments {\n-                            cursor_snapshot.next_desugared()\n-                        } else {\n-                            cursor_snapshot.next()\n-                        }\n-                    }))\n-                    .take(self.num_calls);\n-\n-                make_token_stream(tokens, self.append_unglued_token.clone())\n-            }\n-        }\n-\n-        let mut num_calls = self.token_cursor.num_next_calls - cursor_snapshot.num_next_calls;\n-        match trailing_token {\n-            TrailingToken::None => {}\n-            TrailingToken::Semi => {\n-                assert_eq!(self.token.kind, token::Semi);\n-                num_calls += 1;\n-            }\n-            TrailingToken::MaybeComma => {\n-                if self.token.kind == token::Comma {\n-                    num_calls += 1;\n-                }\n-            }\n-        }\n-\n-        let lazy_impl = LazyTokenStreamImpl {\n-            start_token,\n-            num_calls,\n-            cursor_snapshot,\n-            desugar_doc_comments: self.desugar_doc_comments,\n-            append_unglued_token: self.token_cursor.append_unglued_token.clone(),\n-        };\n-        ret.finalize_tokens(LazyTokenStream::new(lazy_impl));\n-        Ok(ret)\n-    }\n-\n     /// `::{` or `::*`\n     fn is_import_coupler(&mut self) -> bool {\n         self.check(&token::ModSep)\n@@ -1399,47 +1290,3 @@ pub fn emit_unclosed_delims(unclosed_delims: &mut Vec<UnmatchedBrace>, sess: &Pa\n         }\n     }\n }\n-\n-/// Converts a flattened iterator of tokens (including open and close delimiter tokens)\n-/// into a `TokenStream`, creating a `TokenTree::Delimited` for each matching pair\n-/// of open and close delims.\n-fn make_token_stream(\n-    tokens: impl Iterator<Item = (Token, Spacing)>,\n-    append_unglued_token: Option<TreeAndSpacing>,\n-) -> TokenStream {\n-    #[derive(Debug)]\n-    struct FrameData {\n-        open: Span,\n-        inner: Vec<(TokenTree, Spacing)>,\n-    }\n-    let mut stack = vec![FrameData { open: DUMMY_SP, inner: vec![] }];\n-    for (token, spacing) in tokens {\n-        match token {\n-            Token { kind: TokenKind::OpenDelim(_), span } => {\n-                stack.push(FrameData { open: span, inner: vec![] });\n-            }\n-            Token { kind: TokenKind::CloseDelim(delim), span } => {\n-                let frame_data = stack.pop().expect(\"Token stack was empty!\");\n-                let dspan = DelimSpan::from_pair(frame_data.open, span);\n-                let stream = TokenStream::new(frame_data.inner);\n-                let delimited = TokenTree::Delimited(dspan, delim, stream);\n-                stack\n-                    .last_mut()\n-                    .unwrap_or_else(|| panic!(\"Bottom token frame is missing for tokens!\"))\n-                    .inner\n-                    .push((delimited, Spacing::Alone));\n-            }\n-            token => {\n-                stack\n-                    .last_mut()\n-                    .expect(\"Bottom token frame is missing!\")\n-                    .inner\n-                    .push((TokenTree::Token(token), spacing));\n-            }\n-        }\n-    }\n-    let mut final_buf = stack.pop().expect(\"Missing final buf!\");\n-    final_buf.inner.extend(append_unglued_token);\n-    assert!(stack.is_empty(), \"Stack should be empty: final_buf={:?} stack={:?}\", final_buf, stack);\n-    TokenStream::new(final_buf.inner)\n-}"}, {"sha": "40dd938f000e371f8d75bd71f1f79b9d80e76d17", "filename": "compiler/rustc_parse/src/parser/nonterminal.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/3321d701617d5ba3ef348d9273f5fcca126d8f04/compiler%2Frustc_parse%2Fsrc%2Fparser%2Fnonterminal.rs", "raw_url": "https://github.com/rust-lang/rust/raw/3321d701617d5ba3ef348d9273f5fcca126d8f04/compiler%2Frustc_parse%2Fsrc%2Fparser%2Fnonterminal.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_parse%2Fsrc%2Fparser%2Fnonterminal.rs?ref=3321d701617d5ba3ef348d9273f5fcca126d8f04", "patch": "@@ -108,7 +108,7 @@ impl<'a> Parser<'a> {\n                 }\n             },\n             NonterminalKind::Block => {\n-                // While an block *expression* may have attributes (e.g. `#[my_attr] { ... }`),\n+                // While a block *expression* may have attributes (e.g. `#[my_attr] { ... }`),\n                 // the ':block' matcher does not support them\n                 token::NtBlock(self.collect_tokens_no_attrs(|this| this.parse_block())?)\n             }"}, {"sha": "e36ebd5e481137de6733ebcb22f7e1e65e453994", "filename": "compiler/rustc_parse/src/parser/stmt.rs", "status": "modified", "additions": 2, "deletions": 3, "changes": 5, "blob_url": "https://github.com/rust-lang/rust/blob/3321d701617d5ba3ef348d9273f5fcca126d8f04/compiler%2Frustc_parse%2Fsrc%2Fparser%2Fstmt.rs", "raw_url": "https://github.com/rust-lang/rust/raw/3321d701617d5ba3ef348d9273f5fcca126d8f04/compiler%2Frustc_parse%2Fsrc%2Fparser%2Fstmt.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_parse%2Fsrc%2Fparser%2Fstmt.rs?ref=3321d701617d5ba3ef348d9273f5fcca126d8f04", "patch": "@@ -3,9 +3,8 @@ use super::diagnostics::{AttemptLocalParseRecovery, Error};\n use super::expr::LhsExpr;\n use super::pat::{GateOr, RecoverComma};\n use super::path::PathStyle;\n-use super::{\n-    AttrWrapper, BlockMode, ForceCollect, Parser, Restrictions, SemiColonMode, TrailingToken,\n-};\n+use super::TrailingToken;\n+use super::{AttrWrapper, BlockMode, ForceCollect, Parser, Restrictions, SemiColonMode};\n use crate::maybe_whole;\n \n use rustc_ast as ast;"}]}