{"sha": "b9c403be11bef38638b38012be80444ad3f09dde", "node_id": "MDY6Q29tbWl0NzI0NzEyOmI5YzQwM2JlMTFiZWYzODYzOGIzODAxMmJlODA0NDRhZDNmMDlkZGU=", "commit": {"author": {"name": "bors", "email": "bors@rust-lang.org", "date": "2020-12-30T07:04:49Z"}, "committer": {"name": "bors", "email": "bors@rust-lang.org", "date": "2020-12-30T07:04:49Z"}, "message": "Auto merge of #79472 - Aaron1011:new-remove-pretty-print-hack, r=petrochenkov\n\nReplace pretty-print/compare/retokenize hack with targeted workarounds\n\nBased on https://github.com/rust-lang/rust/pull/78296\ncc https://github.com/rust-lang/rust/issues/43081\n\nThe 'pretty-print/compare/retokenize' hack is used to try to avoid passing an outdated `TokenStream` to a proc-macro when the underlying AST is modified in some way (e.g. cfg-stripping before derives). Unfortunately, retokenizing throws away spans (including hygiene information), which causes issues of its own. Every improvement to the accuracy of the pretty-print/retokenize comparison has resulted in non-trivial ecosystem breakage due to hygiene changes. In extreme cases, users deliberately wrote unhygienic `macro_rules!` macros (likely because they did not realize that the compiler's behavior was a bug).\n\nAdditionaly, the comparison between the original and pretty-printed/retoknized token streams comes at a non-trivial runtime cost, as shown by https://github.com/rust-lang/rust/pull/79338\n\nThis PR removes the pretty-print/compare/retokenize logic from `nt_to_tokenstream`. We only discard the original `TokenStream` under two circumstances:\n* Inner attributes are used (detected by examining the AST)\n* `cfg`/`cfg_attr` processing modifies the AST. This is detected by making the visitor update a flag when it performs a modification, instead of trying to detect the modification after-the-fact. Note that a 'matching' `cfg` (e.g. `#[cfg(not(FALSE)]`) does not actually get removed from the AST, allowing us to preserve the original `TokenStream`.\n\nIn all other cases, we preserve the original `TokenStream`.\n\nThis could use a bit of refactoring/renaming - opening for a Crater run.\n\nr? `@ghost`", "tree": {"sha": "493ebc9e7433aa7ab71616eceb90a998b3ba644b", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/493ebc9e7433aa7ab71616eceb90a998b3ba644b"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/b9c403be11bef38638b38012be80444ad3f09dde", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/b9c403be11bef38638b38012be80444ad3f09dde", "html_url": "https://github.com/rust-lang/rust/commit/b9c403be11bef38638b38012be80444ad3f09dde", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/b9c403be11bef38638b38012be80444ad3f09dde/comments", "author": {"login": "bors", "id": 3372342, "node_id": "MDQ6VXNlcjMzNzIzNDI=", "avatar_url": "https://avatars.githubusercontent.com/u/3372342?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bors", "html_url": "https://github.com/bors", "followers_url": "https://api.github.com/users/bors/followers", "following_url": "https://api.github.com/users/bors/following{/other_user}", "gists_url": "https://api.github.com/users/bors/gists{/gist_id}", "starred_url": "https://api.github.com/users/bors/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bors/subscriptions", "organizations_url": "https://api.github.com/users/bors/orgs", "repos_url": "https://api.github.com/users/bors/repos", "events_url": "https://api.github.com/users/bors/events{/privacy}", "received_events_url": "https://api.github.com/users/bors/received_events", "type": "User", "site_admin": false}, "committer": {"login": "bors", "id": 3372342, "node_id": "MDQ6VXNlcjMzNzIzNDI=", "avatar_url": "https://avatars.githubusercontent.com/u/3372342?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bors", "html_url": "https://github.com/bors", "followers_url": "https://api.github.com/users/bors/followers", "following_url": "https://api.github.com/users/bors/following{/other_user}", "gists_url": "https://api.github.com/users/bors/gists{/gist_id}", "starred_url": "https://api.github.com/users/bors/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bors/subscriptions", "organizations_url": "https://api.github.com/users/bors/orgs", "repos_url": "https://api.github.com/users/bors/repos", "events_url": "https://api.github.com/users/bors/events{/privacy}", "received_events_url": "https://api.github.com/users/bors/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "f3eead1c69d5ce9cb128a9068250581ad28103f0", "url": "https://api.github.com/repos/rust-lang/rust/commits/f3eead1c69d5ce9cb128a9068250581ad28103f0", "html_url": "https://github.com/rust-lang/rust/commit/f3eead1c69d5ce9cb128a9068250581ad28103f0"}, {"sha": "530a629635030c47f9d7886bb20a1959fa0198db", "url": "https://api.github.com/repos/rust-lang/rust/commits/530a629635030c47f9d7886bb20a1959fa0198db", "html_url": "https://github.com/rust-lang/rust/commit/530a629635030c47f9d7886bb20a1959fa0198db"}], "stats": {"total": 592, "additions": 184, "deletions": 408}, "files": [{"sha": "0550f53a96fb3838918925dbce66390109035a69", "filename": "compiler/rustc_ast/src/tokenstream.rs", "status": "modified", "additions": 6, "deletions": 0, "changes": 6, "blob_url": "https://github.com/rust-lang/rust/blob/b9c403be11bef38638b38012be80444ad3f09dde/compiler%2Frustc_ast%2Fsrc%2Ftokenstream.rs", "raw_url": "https://github.com/rust-lang/rust/raw/b9c403be11bef38638b38012be80444ad3f09dde/compiler%2Frustc_ast%2Fsrc%2Ftokenstream.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_ast%2Fsrc%2Ftokenstream.rs?ref=b9c403be11bef38638b38012be80444ad3f09dde", "patch": "@@ -44,6 +44,12 @@ pub enum TokenTree {\n     Delimited(DelimSpan, DelimToken, TokenStream),\n }\n \n+#[derive(Copy, Clone)]\n+pub enum CanSynthesizeMissingTokens {\n+    Yes,\n+    No,\n+}\n+\n // Ensure all fields of `TokenTree` is `Send` and `Sync`.\n #[cfg(parallel_compiler)]\n fn _dummy()"}, {"sha": "82616c21891bbbda489793efa89c5f004e08e846", "filename": "compiler/rustc_ast_lowering/src/lib.rs", "status": "modified", "additions": 82, "deletions": 31, "changes": 113, "blob_url": "https://github.com/rust-lang/rust/blob/b9c403be11bef38638b38012be80444ad3f09dde/compiler%2Frustc_ast_lowering%2Fsrc%2Flib.rs", "raw_url": "https://github.com/rust-lang/rust/raw/b9c403be11bef38638b38012be80444ad3f09dde/compiler%2Frustc_ast_lowering%2Fsrc%2Flib.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_ast_lowering%2Fsrc%2Flib.rs?ref=b9c403be11bef38638b38012be80444ad3f09dde", "patch": "@@ -37,7 +37,7 @@\n \n use rustc_ast::node_id::NodeMap;\n use rustc_ast::token::{self, DelimToken, Nonterminal, Token};\n-use rustc_ast::tokenstream::{DelimSpan, TokenStream, TokenTree};\n+use rustc_ast::tokenstream::{CanSynthesizeMissingTokens, DelimSpan, TokenStream, TokenTree};\n use rustc_ast::visit::{self, AssocCtxt, Visitor};\n use rustc_ast::walk_list;\n use rustc_ast::{self as ast, *};\n@@ -206,7 +206,8 @@ pub trait ResolverAstLowering {\n     ) -> LocalDefId;\n }\n \n-type NtToTokenstream = fn(&Nonterminal, &ParseSess, Span) -> TokenStream;\n+type NtToTokenstream =\n+    fn(&Nonterminal, &ParseSess, Span, CanSynthesizeMissingTokens) -> TokenStream;\n \n /// Context of `impl Trait` in code, which determines whether it is allowed in an HIR subtree,\n /// and if so, what meaning it has.\n@@ -393,6 +394,47 @@ enum AnonymousLifetimeMode {\n     PassThrough,\n }\n \n+struct TokenStreamLowering<'a> {\n+    parse_sess: &'a ParseSess,\n+    synthesize_tokens: CanSynthesizeMissingTokens,\n+    nt_to_tokenstream: NtToTokenstream,\n+}\n+\n+impl<'a> TokenStreamLowering<'a> {\n+    fn lower_token_stream(&mut self, tokens: TokenStream) -> TokenStream {\n+        tokens.into_trees().flat_map(|tree| self.lower_token_tree(tree).into_trees()).collect()\n+    }\n+\n+    fn lower_token_tree(&mut self, tree: TokenTree) -> TokenStream {\n+        match tree {\n+            TokenTree::Token(token) => self.lower_token(token),\n+            TokenTree::Delimited(span, delim, tts) => {\n+                TokenTree::Delimited(span, delim, self.lower_token_stream(tts)).into()\n+            }\n+        }\n+    }\n+\n+    fn lower_token(&mut self, token: Token) -> TokenStream {\n+        match token.kind {\n+            token::Interpolated(nt) => {\n+                let tts = (self.nt_to_tokenstream)(\n+                    &nt,\n+                    self.parse_sess,\n+                    token.span,\n+                    self.synthesize_tokens,\n+                );\n+                TokenTree::Delimited(\n+                    DelimSpan::from_single(token.span),\n+                    DelimToken::NoDelim,\n+                    self.lower_token_stream(tts),\n+                )\n+                .into()\n+            }\n+            _ => TokenTree::Token(token).into(),\n+        }\n+    }\n+}\n+\n struct ImplTraitTypeIdVisitor<'a> {\n     ids: &'a mut SmallVec<[NodeId; 1]>,\n }\n@@ -955,40 +997,49 @@ impl<'a, 'hir> LoweringContext<'a, 'hir> {\n         match *args {\n             MacArgs::Empty => MacArgs::Empty,\n             MacArgs::Delimited(dspan, delim, ref tokens) => {\n-                MacArgs::Delimited(dspan, delim, self.lower_token_stream(tokens.clone()))\n-            }\n-            MacArgs::Eq(eq_span, ref tokens) => {\n-                MacArgs::Eq(eq_span, self.lower_token_stream(tokens.clone()))\n-            }\n-        }\n-    }\n-\n-    fn lower_token_stream(&mut self, tokens: TokenStream) -> TokenStream {\n-        tokens.into_trees().flat_map(|tree| self.lower_token_tree(tree).into_trees()).collect()\n-    }\n-\n-    fn lower_token_tree(&mut self, tree: TokenTree) -> TokenStream {\n-        match tree {\n-            TokenTree::Token(token) => self.lower_token(token),\n-            TokenTree::Delimited(span, delim, tts) => {\n-                TokenTree::Delimited(span, delim, self.lower_token_stream(tts)).into()\n+                // This is either a non-key-value attribute, or a `macro_rules!` body.\n+                // We either not have any nonterminals present (in the case of an attribute),\n+                // or have tokens available for all nonterminals in the case of a nested\n+                // `macro_rules`: e.g:\n+                //\n+                // ```rust\n+                // macro_rules! outer {\n+                //     ($e:expr) => {\n+                //         macro_rules! inner {\n+                //             () => { $e }\n+                //         }\n+                //     }\n+                // }\n+                // ```\n+                //\n+                // In both cases, we don't want to synthesize any tokens\n+                MacArgs::Delimited(\n+                    dspan,\n+                    delim,\n+                    self.lower_token_stream(tokens.clone(), CanSynthesizeMissingTokens::No),\n+                )\n             }\n+            // This is an inert key-value attribute - it will never be visible to macros\n+            // after it gets lowered to HIR. Therefore, we can synthesize tokens with fake\n+            // spans to handle nonterminals in `#[doc]` (e.g. `#[doc = $e]`).\n+            MacArgs::Eq(eq_span, ref tokens) => MacArgs::Eq(\n+                eq_span,\n+                self.lower_token_stream(tokens.clone(), CanSynthesizeMissingTokens::Yes),\n+            ),\n         }\n     }\n \n-    fn lower_token(&mut self, token: Token) -> TokenStream {\n-        match token.kind {\n-            token::Interpolated(nt) => {\n-                let tts = (self.nt_to_tokenstream)(&nt, &self.sess.parse_sess, token.span);\n-                TokenTree::Delimited(\n-                    DelimSpan::from_single(token.span),\n-                    DelimToken::NoDelim,\n-                    self.lower_token_stream(tts),\n-                )\n-                .into()\n-            }\n-            _ => TokenTree::Token(token).into(),\n+    fn lower_token_stream(\n+        &self,\n+        tokens: TokenStream,\n+        synthesize_tokens: CanSynthesizeMissingTokens,\n+    ) -> TokenStream {\n+        TokenStreamLowering {\n+            parse_sess: &self.sess.parse_sess,\n+            synthesize_tokens,\n+            nt_to_tokenstream: self.nt_to_tokenstream,\n         }\n+        .lower_token_stream(tokens)\n     }\n \n     /// Given an associated type constraint like one of these:"}, {"sha": "774a0764d114f952f251549f74c1afff1d441c1a", "filename": "compiler/rustc_expand/src/base.rs", "status": "modified", "additions": 9, "deletions": 6, "changes": 15, "blob_url": "https://github.com/rust-lang/rust/blob/b9c403be11bef38638b38012be80444ad3f09dde/compiler%2Frustc_expand%2Fsrc%2Fbase.rs", "raw_url": "https://github.com/rust-lang/rust/raw/b9c403be11bef38638b38012be80444ad3f09dde/compiler%2Frustc_expand%2Fsrc%2Fbase.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_expand%2Fsrc%2Fbase.rs?ref=b9c403be11bef38638b38012be80444ad3f09dde", "patch": "@@ -2,8 +2,8 @@ use crate::expand::{self, AstFragment, Invocation};\n use crate::module::DirectoryOwnership;\n \n use rustc_ast::ptr::P;\n-use rustc_ast::token;\n-use rustc_ast::tokenstream::TokenStream;\n+use rustc_ast::token::{self, Nonterminal};\n+use rustc_ast::tokenstream::{CanSynthesizeMissingTokens, TokenStream};\n use rustc_ast::visit::{AssocCtxt, Visitor};\n use rustc_ast::{self as ast, Attribute, NodeId, PatKind};\n use rustc_attr::{self as attr, Deprecation, HasAttrs, Stability};\n@@ -119,8 +119,8 @@ impl Annotatable {\n         }\n     }\n \n-    crate fn into_tokens(self, sess: &ParseSess) -> TokenStream {\n-        let nt = match self {\n+    crate fn into_nonterminal(self) -> Nonterminal {\n+        match self {\n             Annotatable::Item(item) => token::NtItem(item),\n             Annotatable::TraitItem(item) | Annotatable::ImplItem(item) => {\n                 token::NtItem(P(item.and_then(ast::AssocItem::into_item)))\n@@ -137,8 +137,11 @@ impl Annotatable {\n             | Annotatable::Param(..)\n             | Annotatable::StructField(..)\n             | Annotatable::Variant(..) => panic!(\"unexpected annotatable\"),\n-        };\n-        nt_to_tokenstream(&nt, sess, DUMMY_SP)\n+        }\n+    }\n+\n+    crate fn into_tokens(self, sess: &ParseSess) -> TokenStream {\n+        nt_to_tokenstream(&self.into_nonterminal(), sess, DUMMY_SP, CanSynthesizeMissingTokens::No)\n     }\n \n     pub fn expect_item(self) -> P<ast::Item> {"}, {"sha": "1193f66651ce9dda9c34d8c53ed34a80bae5f1db", "filename": "compiler/rustc_expand/src/config.rs", "status": "modified", "additions": 11, "deletions": 2, "changes": 13, "blob_url": "https://github.com/rust-lang/rust/blob/b9c403be11bef38638b38012be80444ad3f09dde/compiler%2Frustc_expand%2Fsrc%2Fconfig.rs", "raw_url": "https://github.com/rust-lang/rust/raw/b9c403be11bef38638b38012be80444ad3f09dde/compiler%2Frustc_expand%2Fsrc%2Fconfig.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_expand%2Fsrc%2Fconfig.rs?ref=b9c403be11bef38638b38012be80444ad3f09dde", "patch": "@@ -29,6 +29,7 @@ use smallvec::SmallVec;\n pub struct StripUnconfigured<'a> {\n     pub sess: &'a Session,\n     pub features: Option<&'a Features>,\n+    pub modified: bool,\n }\n \n fn get_features(\n@@ -199,7 +200,7 @@ fn get_features(\n \n // `cfg_attr`-process the crate's attributes and compute the crate's features.\n pub fn features(sess: &Session, mut krate: ast::Crate) -> (ast::Crate, Features) {\n-    let mut strip_unconfigured = StripUnconfigured { sess, features: None };\n+    let mut strip_unconfigured = StripUnconfigured { sess, features: None, modified: false };\n \n     let unconfigured_attrs = krate.attrs.clone();\n     let diag = &sess.parse_sess.span_diagnostic;\n@@ -243,7 +244,12 @@ const CFG_ATTR_NOTE_REF: &str = \"for more information, visit \\\n impl<'a> StripUnconfigured<'a> {\n     pub fn configure<T: HasAttrs>(&mut self, mut node: T) -> Option<T> {\n         self.process_cfg_attrs(&mut node);\n-        self.in_cfg(node.attrs()).then_some(node)\n+        if self.in_cfg(node.attrs()) {\n+            Some(node)\n+        } else {\n+            self.modified = true;\n+            None\n+        }\n     }\n \n     /// Parse and expand all `cfg_attr` attributes into a list of attributes\n@@ -270,6 +276,9 @@ impl<'a> StripUnconfigured<'a> {\n             return vec![attr];\n         }\n \n+        // A `#[cfg_attr]` either gets removed, or replaced with a new attribute\n+        self.modified = true;\n+\n         let (cfg_predicate, expanded_attrs) = match self.parse_cfg_attr(&attr) {\n             None => return vec![],\n             Some(r) => r,"}, {"sha": "5d40d478b963d7bce9adef36551494e5010e5650", "filename": "compiler/rustc_expand/src/expand.rs", "status": "modified", "additions": 35, "deletions": 6, "changes": 41, "blob_url": "https://github.com/rust-lang/rust/blob/b9c403be11bef38638b38012be80444ad3f09dde/compiler%2Frustc_expand%2Fsrc%2Fexpand.rs", "raw_url": "https://github.com/rust-lang/rust/raw/b9c403be11bef38638b38012be80444ad3f09dde/compiler%2Frustc_expand%2Fsrc%2Fexpand.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_expand%2Fsrc%2Fexpand.rs?ref=b9c403be11bef38638b38012be80444ad3f09dde", "patch": "@@ -12,7 +12,7 @@ use rustc_ast::ptr::P;\n use rustc_ast::token;\n use rustc_ast::tokenstream::TokenStream;\n use rustc_ast::visit::{self, AssocCtxt, Visitor};\n-use rustc_ast::{self as ast, AttrItem, Block, LitKind, NodeId, PatKind, Path};\n+use rustc_ast::{self as ast, AttrItem, AttrStyle, Block, LitKind, NodeId, PatKind, Path};\n use rustc_ast::{ItemKind, MacArgs, MacCallStmt, MacStmtStyle, StmtKind, Unsafe};\n use rustc_ast_pretty::pprust;\n use rustc_attr::{self as attr, is_builtin_attr, HasAttrs};\n@@ -522,12 +522,29 @@ impl<'a, 'b> MacroExpander<'a, 'b> {\n                         item.visit_attrs(|attrs| attrs.retain(|a| !a.has_name(sym::derive)));\n                         (item, Vec::new())\n                     } else {\n-                        let mut item = StripUnconfigured {\n+                        let mut visitor = StripUnconfigured {\n                             sess: self.cx.sess,\n                             features: self.cx.ecfg.features,\n-                        }\n-                        .fully_configure(item);\n+                            modified: false,\n+                        };\n+                        let mut item = visitor.fully_configure(item);\n                         item.visit_attrs(|attrs| attrs.retain(|a| !a.has_name(sym::derive)));\n+                        if visitor.modified && !derives.is_empty() {\n+                            // Erase the tokens if cfg-stripping modified the item\n+                            // This will cause us to synthesize fake tokens\n+                            // when `nt_to_tokenstream` is called on this item.\n+                            match &mut item {\n+                                Annotatable::Item(item) => item.tokens = None,\n+                                Annotatable::Stmt(stmt) => {\n+                                    if let StmtKind::Item(item) = &mut stmt.kind {\n+                                        item.tokens = None\n+                                    } else {\n+                                        panic!(\"Unexpected stmt {:?}\", stmt);\n+                                    }\n+                                }\n+                                _ => panic!(\"Unexpected annotatable {:?}\", item),\n+                            }\n+                        }\n \n                         invocations.reserve(derives.len());\n                         let derive_placeholders = derives\n@@ -622,7 +639,11 @@ impl<'a, 'b> MacroExpander<'a, 'b> {\n \n         let invocations = {\n             let mut collector = InvocationCollector {\n-                cfg: StripUnconfigured { sess: &self.cx.sess, features: self.cx.ecfg.features },\n+                cfg: StripUnconfigured {\n+                    sess: &self.cx.sess,\n+                    features: self.cx.ecfg.features,\n+                    modified: false,\n+                },\n                 cx: self.cx,\n                 invocations: Vec::new(),\n                 monotonic: self.monotonic,\n@@ -716,7 +737,15 @@ impl<'a, 'b> MacroExpander<'a, 'b> {\n                 SyntaxExtensionKind::Attr(expander) => {\n                     self.gate_proc_macro_input(&item);\n                     self.gate_proc_macro_attr_item(span, &item);\n-                    let tokens = item.into_tokens(&self.cx.sess.parse_sess);\n+                    let tokens = match attr.style {\n+                        AttrStyle::Outer => item.into_tokens(&self.cx.sess.parse_sess),\n+                        // FIXME: Properly collect tokens for inner attributes\n+                        AttrStyle::Inner => rustc_parse::fake_token_stream(\n+                            &self.cx.sess.parse_sess,\n+                            &item.into_nonterminal(),\n+                            span,\n+                        ),\n+                    };\n                     let attr_item = attr.unwrap_normal_item();\n                     if let MacArgs::Eq(..) = attr_item.args {\n                         self.cx.span_err(span, \"key-value macro attributes are not supported\");"}, {"sha": "e8e098b621295b898750510464a46c1bb856d894", "filename": "compiler/rustc_expand/src/proc_macro.rs", "status": "modified", "additions": 7, "deletions": 2, "changes": 9, "blob_url": "https://github.com/rust-lang/rust/blob/b9c403be11bef38638b38012be80444ad3f09dde/compiler%2Frustc_expand%2Fsrc%2Fproc_macro.rs", "raw_url": "https://github.com/rust-lang/rust/raw/b9c403be11bef38638b38012be80444ad3f09dde/compiler%2Frustc_expand%2Fsrc%2Fproc_macro.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_expand%2Fsrc%2Fproc_macro.rs?ref=b9c403be11bef38638b38012be80444ad3f09dde", "patch": "@@ -3,7 +3,7 @@ use crate::proc_macro_server;\n \n use rustc_ast::ptr::P;\n use rustc_ast::token;\n-use rustc_ast::tokenstream::{TokenStream, TokenTree};\n+use rustc_ast::tokenstream::{CanSynthesizeMissingTokens, TokenStream, TokenTree};\n use rustc_ast::{self as ast, *};\n use rustc_data_structures::sync::Lrc;\n use rustc_errors::{struct_span_err, Applicability, ErrorReported};\n@@ -94,7 +94,12 @@ impl MultiItemModifier for ProcMacroDerive {\n         let input = if item.pretty_printing_compatibility_hack() {\n             TokenTree::token(token::Interpolated(Lrc::new(item)), DUMMY_SP).into()\n         } else {\n-            nt_to_tokenstream(&item, &ecx.sess.parse_sess, DUMMY_SP)\n+            nt_to_tokenstream(\n+                &item,\n+                &ecx.sess.parse_sess,\n+                DUMMY_SP,\n+                CanSynthesizeMissingTokens::Yes,\n+            )\n         };\n \n         let server = proc_macro_server::Rustc::new(ecx);"}, {"sha": "02ae842675f37029e50827f3a7733618c8dcc5ae", "filename": "compiler/rustc_expand/src/proc_macro_server.rs", "status": "modified", "additions": 3, "deletions": 2, "changes": 5, "blob_url": "https://github.com/rust-lang/rust/blob/b9c403be11bef38638b38012be80444ad3f09dde/compiler%2Frustc_expand%2Fsrc%2Fproc_macro_server.rs", "raw_url": "https://github.com/rust-lang/rust/raw/b9c403be11bef38638b38012be80444ad3f09dde/compiler%2Frustc_expand%2Fsrc%2Fproc_macro_server.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_expand%2Fsrc%2Fproc_macro_server.rs?ref=b9c403be11bef38638b38012be80444ad3f09dde", "patch": "@@ -2,7 +2,8 @@ use crate::base::ExtCtxt;\n \n use rustc_ast as ast;\n use rustc_ast::token;\n-use rustc_ast::tokenstream::{self, DelimSpan, Spacing::*, TokenStream, TreeAndSpacing};\n+use rustc_ast::tokenstream::{self, CanSynthesizeMissingTokens};\n+use rustc_ast::tokenstream::{DelimSpan, Spacing::*, TokenStream, TreeAndSpacing};\n use rustc_ast_pretty::pprust;\n use rustc_data_structures::sync::Lrc;\n use rustc_errors::Diagnostic;\n@@ -178,7 +179,7 @@ impl FromInternal<(TreeAndSpacing, &'_ ParseSess, &'_ mut Vec<Self>)>\n                 {\n                     TokenTree::Ident(Ident::new(sess, name.name, is_raw, name.span))\n                 } else {\n-                    let stream = nt_to_tokenstream(&nt, sess, span);\n+                    let stream = nt_to_tokenstream(&nt, sess, span, CanSynthesizeMissingTokens::No);\n                     TokenTree::Group(Group {\n                         delimiter: Delimiter::None,\n                         stream,"}, {"sha": "9abffbacfc3b3671b92182e3b6b04bcc0c6f2ad6", "filename": "compiler/rustc_parse/src/lib.rs", "status": "modified", "additions": 31, "deletions": 359, "changes": 390, "blob_url": "https://github.com/rust-lang/rust/blob/b9c403be11bef38638b38012be80444ad3f09dde/compiler%2Frustc_parse%2Fsrc%2Flib.rs", "raw_url": "https://github.com/rust-lang/rust/raw/b9c403be11bef38638b38012be80444ad3f09dde/compiler%2Frustc_parse%2Fsrc%2Flib.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_parse%2Fsrc%2Flib.rs?ref=b9c403be11bef38638b38012be80444ad3f09dde", "patch": "@@ -7,22 +7,18 @@\n \n use rustc_ast as ast;\n use rustc_ast::attr::HasAttrs;\n-use rustc_ast::token::{self, DelimToken, Nonterminal, Token, TokenKind};\n-use rustc_ast::tokenstream::{self, LazyTokenStream, TokenStream, TokenTree};\n+use rustc_ast::token::{self, Nonterminal};\n+use rustc_ast::tokenstream::{self, CanSynthesizeMissingTokens, LazyTokenStream, TokenStream};\n use rustc_ast_pretty::pprust;\n-use rustc_data_structures::fx::FxHashSet;\n use rustc_data_structures::sync::Lrc;\n use rustc_errors::{Diagnostic, FatalError, Level, PResult};\n use rustc_session::parse::ParseSess;\n-use rustc_span::{symbol::kw, FileName, SourceFile, Span, DUMMY_SP};\n+use rustc_span::{FileName, SourceFile, Span};\n \n-use smallvec::SmallVec;\n-use std::cell::RefCell;\n-use std::mem;\n use std::path::Path;\n use std::str;\n \n-use tracing::{debug, info};\n+use tracing::debug;\n \n pub const MACRO_ARGUMENTS: Option<&str> = Some(\"macro arguments\");\n \n@@ -237,7 +233,12 @@ pub fn parse_in<'a, T>(\n // NOTE(Centril): The following probably shouldn't be here but it acknowledges the\n // fact that architecturally, we are using parsing (read on below to understand why).\n \n-pub fn nt_to_tokenstream(nt: &Nonterminal, sess: &ParseSess, span: Span) -> TokenStream {\n+pub fn nt_to_tokenstream(\n+    nt: &Nonterminal,\n+    sess: &ParseSess,\n+    span: Span,\n+    synthesize_tokens: CanSynthesizeMissingTokens,\n+) -> TokenStream {\n     // A `Nonterminal` is often a parsed AST item. At this point we now\n     // need to convert the parsed AST to an actual token stream, e.g.\n     // un-parse it basically.\n@@ -255,9 +256,11 @@ pub fn nt_to_tokenstream(nt: &Nonterminal, sess: &ParseSess, span: Span) -> Toke\n         |tokens: Option<&LazyTokenStream>| tokens.as_ref().map(|t| t.create_token_stream());\n \n     let tokens = match *nt {\n-        Nonterminal::NtItem(ref item) => prepend_attrs(&item.attrs, item.tokens.as_ref()),\n+        Nonterminal::NtItem(ref item) => {\n+            prepend_attrs(sess, &item.attrs, nt, span, item.tokens.as_ref())\n+        }\n         Nonterminal::NtBlock(ref block) => convert_tokens(block.tokens.as_ref()),\n-        Nonterminal::NtStmt(ref stmt) => prepend_attrs(stmt.attrs(), stmt.tokens()),\n+        Nonterminal::NtStmt(ref stmt) => prepend_attrs(sess, stmt.attrs(), nt, span, stmt.tokens()),\n         Nonterminal::NtPat(ref pat) => convert_tokens(pat.tokens.as_ref()),\n         Nonterminal::NtTy(ref ty) => convert_tokens(ty.tokens.as_ref()),\n         Nonterminal::NtIdent(ident, is_raw) => {\n@@ -274,376 +277,45 @@ pub fn nt_to_tokenstream(nt: &Nonterminal, sess: &ParseSess, span: Span) -> Toke\n             if expr.tokens.is_none() {\n                 debug!(\"missing tokens for expr {:?}\", expr);\n             }\n-            prepend_attrs(&expr.attrs, expr.tokens.as_ref())\n+            prepend_attrs(sess, &expr.attrs, nt, span, expr.tokens.as_ref())\n         }\n     };\n \n-    // Caches the stringification of 'good' `TokenStreams` which passed\n-    // `tokenstream_probably_equal_for_proc_macro`. This allows us to avoid\n-    // repeatedly stringifying and comparing the same `TokenStream` for deeply\n-    // nested nonterminals.\n-    //\n-    // We cache by the strinification instead of the `TokenStream` to avoid\n-    // needing to implement `Hash` for `TokenStream`. Note that it's possible to\n-    // have two distinct `TokenStream`s that stringify to the same result\n-    // (e.g. if they differ only in hygiene information). However, any\n-    // information lost during the stringification process is also intentionally\n-    // ignored by `tokenstream_probably_equal_for_proc_macro`, so it's fine\n-    // that a single cache entry may 'map' to multiple distinct `TokenStream`s.\n-    //\n-    // This is a temporary hack to prevent compilation blowup on certain inputs.\n-    // The entire pretty-print/retokenize process will be removed soon.\n-    thread_local! {\n-        static GOOD_TOKEN_CACHE: RefCell<FxHashSet<String>> = Default::default();\n-    }\n-\n-    // FIXME(#43081): Avoid this pretty-print + reparse hack\n-    // Pretty-print the AST struct without inserting any parenthesis\n-    // beyond those explicitly written by the user (e.g. `ExpnKind::Paren`).\n-    // The resulting stream may have incorrect precedence, but it's only\n-    // ever used for a comparison against the capture tokenstream.\n-    let source = pprust::nonterminal_to_string_no_extra_parens(nt);\n-    let filename = FileName::macro_expansion_source_code(&source);\n-    let reparsed_tokens = parse_stream_from_source_str(filename, source.clone(), sess, Some(span));\n-\n-    // During early phases of the compiler the AST could get modified\n-    // directly (e.g., attributes added or removed) and the internal cache\n-    // of tokens my not be invalidated or updated. Consequently if the\n-    // \"lossless\" token stream disagrees with our actual stringification\n-    // (which has historically been much more battle-tested) then we go\n-    // with the lossy stream anyway (losing span information).\n-    //\n-    // Note that the comparison isn't `==` here to avoid comparing spans,\n-    // but it *also* is a \"probable\" equality which is a pretty weird\n-    // definition. We mostly want to catch actual changes to the AST\n-    // like a `#[cfg]` being processed or some weird `macro_rules!`\n-    // expansion.\n-    //\n-    // What we *don't* want to catch is the fact that a user-defined\n-    // literal like `0xf` is stringified as `15`, causing the cached token\n-    // stream to not be literal `==` token-wise (ignoring spans) to the\n-    // token stream we got from stringification.\n-    //\n-    // Instead the \"probably equal\" check here is \"does each token\n-    // recursively have the same discriminant?\" We basically don't look at\n-    // the token values here and assume that such fine grained token stream\n-    // modifications, including adding/removing typically non-semantic\n-    // tokens such as extra braces and commas, don't happen.\n     if let Some(tokens) = tokens {\n-        if GOOD_TOKEN_CACHE.with(|cache| cache.borrow().contains(&source)) {\n-            return tokens;\n-        }\n-\n-        // Compare with a non-relaxed delim match to start.\n-        if tokenstream_probably_equal_for_proc_macro(&tokens, &reparsed_tokens, sess, false) {\n-            GOOD_TOKEN_CACHE.with(|cache| cache.borrow_mut().insert(source.clone()));\n-            return tokens;\n-        }\n-\n-        // The check failed. This time, we pretty-print the AST struct with parenthesis\n-        // inserted to preserve precedence. This may cause `None`-delimiters in the captured\n-        // token stream to match up with inserted parenthesis in the reparsed stream.\n-        let source_with_parens = pprust::nonterminal_to_string(nt);\n-        let filename_with_parens = FileName::macro_expansion_source_code(&source_with_parens);\n-\n-        if GOOD_TOKEN_CACHE.with(|cache| cache.borrow().contains(&source_with_parens)) {\n-            return tokens;\n-        }\n-\n-        let reparsed_tokens_with_parens = parse_stream_from_source_str(\n-            filename_with_parens,\n-            source_with_parens,\n-            sess,\n-            Some(span),\n-        );\n-\n-        // Compare with a relaxed delim match - we want inserted parenthesis in the\n-        // reparsed stream to match `None`-delimiters in the original stream.\n-        if tokenstream_probably_equal_for_proc_macro(\n-            &tokens,\n-            &reparsed_tokens_with_parens,\n-            sess,\n-            true,\n-        ) {\n-            GOOD_TOKEN_CACHE.with(|cache| cache.borrow_mut().insert(source.clone()));\n-            return tokens;\n-        }\n-\n-        info!(\n-            \"cached tokens found, but they're not \\\"probably equal\\\", \\\n-                going with stringified version\"\n-        );\n-        info!(\"cached   tokens: {}\", pprust::tts_to_string(&tokens));\n-        info!(\"reparsed tokens: {}\", pprust::tts_to_string(&reparsed_tokens_with_parens));\n-\n-        info!(\"cached   tokens debug: {:?}\", tokens);\n-        info!(\"reparsed tokens debug: {:?}\", reparsed_tokens_with_parens);\n-    }\n-    reparsed_tokens\n-}\n-\n-// See comments in `Nonterminal::to_tokenstream` for why we care about\n-// *probably* equal here rather than actual equality\n-//\n-// This is otherwise the same as `eq_unspanned`, only recursing with a\n-// different method.\n-pub fn tokenstream_probably_equal_for_proc_macro(\n-    tokens: &TokenStream,\n-    reparsed_tokens: &TokenStream,\n-    sess: &ParseSess,\n-    relaxed_delim_match: bool,\n-) -> bool {\n-    // When checking for `probably_eq`, we ignore certain tokens that aren't\n-    // preserved in the AST. Because they are not preserved, the pretty\n-    // printer arbitrarily adds or removes them when printing as token\n-    // streams, making a comparison between a token stream generated from an\n-    // AST and a token stream which was parsed into an AST more reliable.\n-    fn semantic_tree(tree: &TokenTree) -> bool {\n-        if let TokenTree::Token(token) = tree {\n-            if let\n-                // The pretty printer tends to add trailing commas to\n-                // everything, and in particular, after struct fields.\n-                | token::Comma\n-                // The pretty printer collapses many semicolons into one.\n-                | token::Semi\n-                // We don't preserve leading `|` tokens in patterns, so\n-                // we ignore them entirely\n-                | token::BinOp(token::BinOpToken::Or)\n-                // We don't preserve trailing '+' tokens in trait bounds,\n-                // so we ignore them entirely\n-                | token::BinOp(token::BinOpToken::Plus)\n-                // The pretty printer can turn `$crate` into `::crate_name`\n-                | token::ModSep = token.kind {\n-                return false;\n-            }\n-        }\n-        true\n-    }\n-\n-    // When comparing two `TokenStream`s, we ignore the `IsJoint` information.\n-    //\n-    // However, `rustc_parse::lexer::tokentrees::TokenStreamBuilder` will\n-    // use `Token.glue` on adjacent tokens with the proper `IsJoint`.\n-    // Since we are ignoreing `IsJoint`, a 'glued' token (e.g. `BinOp(Shr)`)\n-    // and its 'split'/'unglued' compoenents (e.g. `Gt, Gt`) are equivalent\n-    // when determining if two `TokenStream`s are 'probably equal'.\n-    //\n-    // Therefore, we use `break_two_token_op` to convert all tokens\n-    // to the 'unglued' form (if it exists). This ensures that two\n-    // `TokenStream`s which differ only in how their tokens are glued\n-    // will be considered 'probably equal', which allows us to keep spans.\n-    //\n-    // This is important when the original `TokenStream` contained\n-    // extra spaces (e.g. `f :: < Vec < _ > > ( ) ;'). These extra spaces\n-    // will be omitted when we pretty-print, which can cause the original\n-    // and reparsed `TokenStream`s to differ in the assignment of `IsJoint`,\n-    // leading to some tokens being 'glued' together in one stream but not\n-    // the other. See #68489 for more details.\n-    fn break_tokens(tree: TokenTree) -> impl Iterator<Item = TokenTree> {\n-        // In almost all cases, we should have either zero or one levels\n-        // of 'unglueing'. However, in some unusual cases, we may need\n-        // to iterate breaking tokens mutliple times. For example:\n-        // '[BinOpEq(Shr)] => [Gt, Ge] -> [Gt, Gt, Eq]'\n-        let mut token_trees: SmallVec<[_; 2]>;\n-        if let TokenTree::Token(token) = tree {\n-            let mut out = SmallVec::<[_; 2]>::new();\n-            out.push(token);\n-            // Iterate to fixpoint:\n-            // * We start off with 'out' containing our initial token, and `temp` empty\n-            // * If we are able to break any tokens in `out`, then `out` will have\n-            //   at least one more element than 'temp', so we will try to break tokens\n-            //   again.\n-            // * If we cannot break any tokens in 'out', we are done\n-            loop {\n-                let mut temp = SmallVec::<[_; 2]>::new();\n-                let mut changed = false;\n-\n-                for token in out.into_iter() {\n-                    if let Some((first, second)) = token.kind.break_two_token_op() {\n-                        temp.push(Token::new(first, DUMMY_SP));\n-                        temp.push(Token::new(second, DUMMY_SP));\n-                        changed = true;\n-                    } else {\n-                        temp.push(token);\n-                    }\n-                }\n-                out = temp;\n-                if !changed {\n-                    break;\n-                }\n-            }\n-            token_trees = out.into_iter().map(TokenTree::Token).collect();\n-        } else {\n-            token_trees = SmallVec::new();\n-            token_trees.push(tree);\n-        }\n-        token_trees.into_iter()\n-    }\n-\n-    fn expand_token(tree: TokenTree, sess: &ParseSess) -> impl Iterator<Item = TokenTree> {\n-        // When checking tokenstreams for 'probable equality', we are comparing\n-        // a captured (from parsing) `TokenStream` to a reparsed tokenstream.\n-        // The reparsed Tokenstream will never have `None`-delimited groups,\n-        // since they are only ever inserted as a result of macro expansion.\n-        // Therefore, inserting a `None`-delimtied group here (when we\n-        // convert a nested `Nonterminal` to a tokenstream) would cause\n-        // a mismatch with the reparsed tokenstream.\n-        //\n-        // Note that we currently do not handle the case where the\n-        // reparsed stream has a `Parenthesis`-delimited group\n-        // inserted. This will cause a spurious mismatch:\n-        // issue #75734 tracks resolving this.\n-\n-        let expanded: SmallVec<[_; 1]> =\n-            if let TokenTree::Token(Token { kind: TokenKind::Interpolated(nt), span }) = &tree {\n-                nt_to_tokenstream(nt, sess, *span)\n-                    .into_trees()\n-                    .flat_map(|t| expand_token(t, sess))\n-                    .collect()\n-            } else {\n-                // Filter before and after breaking tokens,\n-                // since we may want to ignore both glued and unglued tokens.\n-                std::iter::once(tree)\n-                    .filter(semantic_tree)\n-                    .flat_map(break_tokens)\n-                    .filter(semantic_tree)\n-                    .collect()\n-            };\n-        expanded.into_iter()\n-    }\n-\n-    // Break tokens after we expand any nonterminals, so that we break tokens\n-    // that are produced as a result of nonterminal expansion.\n-    let tokens = tokens.trees().flat_map(|t| expand_token(t, sess));\n-    let reparsed_tokens = reparsed_tokens.trees().flat_map(|t| expand_token(t, sess));\n-\n-    tokens.eq_by(reparsed_tokens, |t, rt| {\n-        tokentree_probably_equal_for_proc_macro(&t, &rt, sess, relaxed_delim_match)\n-    })\n-}\n-\n-// See comments in `Nonterminal::to_tokenstream` for why we care about\n-// *probably* equal here rather than actual equality\n-//\n-// This is otherwise the same as `eq_unspanned`, only recursing with a\n-// different method.\n-pub fn tokentree_probably_equal_for_proc_macro(\n-    token: &TokenTree,\n-    reparsed_token: &TokenTree,\n-    sess: &ParseSess,\n-    relaxed_delim_match: bool,\n-) -> bool {\n-    match (token, reparsed_token) {\n-        (TokenTree::Token(token), TokenTree::Token(reparsed_token)) => {\n-            token_probably_equal_for_proc_macro(token, reparsed_token)\n-        }\n-        (\n-            TokenTree::Delimited(_, delim, tokens),\n-            TokenTree::Delimited(_, reparsed_delim, reparsed_tokens),\n-        ) if delim == reparsed_delim => tokenstream_probably_equal_for_proc_macro(\n-            tokens,\n-            reparsed_tokens,\n-            sess,\n-            relaxed_delim_match,\n-        ),\n-        (TokenTree::Delimited(_, DelimToken::NoDelim, tokens), reparsed_token) => {\n-            if relaxed_delim_match {\n-                if let TokenTree::Delimited(_, DelimToken::Paren, reparsed_tokens) = reparsed_token\n-                {\n-                    if tokenstream_probably_equal_for_proc_macro(\n-                        tokens,\n-                        reparsed_tokens,\n-                        sess,\n-                        relaxed_delim_match,\n-                    ) {\n-                        return true;\n-                    }\n-                }\n-            }\n-            tokens.len() == 1\n-                && tokentree_probably_equal_for_proc_macro(\n-                    &tokens.trees().next().unwrap(),\n-                    reparsed_token,\n-                    sess,\n-                    relaxed_delim_match,\n-                )\n-        }\n-        _ => false,\n+        return tokens;\n+    } else if matches!(synthesize_tokens, CanSynthesizeMissingTokens::Yes) {\n+        return fake_token_stream(sess, nt, span);\n+    } else {\n+        let pretty = rustc_ast_pretty::pprust::nonterminal_to_string_no_extra_parens(&nt);\n+        panic!(\"Missing tokens at {:?} for nt {:?}\", span, pretty);\n     }\n }\n \n-// See comments in `Nonterminal::to_tokenstream` for why we care about\n-// *probably* equal here rather than actual equality\n-fn token_probably_equal_for_proc_macro(first: &Token, other: &Token) -> bool {\n-    if mem::discriminant(&first.kind) != mem::discriminant(&other.kind) {\n-        return false;\n-    }\n-    use rustc_ast::token::TokenKind::*;\n-    match (&first.kind, &other.kind) {\n-        (&Eq, &Eq)\n-        | (&Lt, &Lt)\n-        | (&Le, &Le)\n-        | (&EqEq, &EqEq)\n-        | (&Ne, &Ne)\n-        | (&Ge, &Ge)\n-        | (&Gt, &Gt)\n-        | (&AndAnd, &AndAnd)\n-        | (&OrOr, &OrOr)\n-        | (&Not, &Not)\n-        | (&Tilde, &Tilde)\n-        | (&At, &At)\n-        | (&Dot, &Dot)\n-        | (&DotDot, &DotDot)\n-        | (&DotDotDot, &DotDotDot)\n-        | (&DotDotEq, &DotDotEq)\n-        | (&Comma, &Comma)\n-        | (&Semi, &Semi)\n-        | (&Colon, &Colon)\n-        | (&ModSep, &ModSep)\n-        | (&RArrow, &RArrow)\n-        | (&LArrow, &LArrow)\n-        | (&FatArrow, &FatArrow)\n-        | (&Pound, &Pound)\n-        | (&Dollar, &Dollar)\n-        | (&Question, &Question)\n-        | (&Eof, &Eof) => true,\n-\n-        (&BinOp(a), &BinOp(b)) | (&BinOpEq(a), &BinOpEq(b)) => a == b,\n-\n-        (&OpenDelim(a), &OpenDelim(b)) | (&CloseDelim(a), &CloseDelim(b)) => a == b,\n-\n-        (&DocComment(a1, a2, a3), &DocComment(b1, b2, b3)) => a1 == b1 && a2 == b2 && a3 == b3,\n-\n-        (&Literal(a), &Literal(b)) => a == b,\n-\n-        (&Lifetime(a), &Lifetime(b)) => a == b,\n-        (&Ident(a, b), &Ident(c, d)) => {\n-            b == d && (a == c || a == kw::DollarCrate || c == kw::DollarCrate)\n-        }\n-\n-        (&Interpolated(..), &Interpolated(..)) => panic!(\"Unexpanded Interpolated!\"),\n-\n-        _ => panic!(\"forgot to add a token?\"),\n-    }\n+pub fn fake_token_stream(sess: &ParseSess, nt: &Nonterminal, span: Span) -> TokenStream {\n+    let source = pprust::nonterminal_to_string(nt);\n+    let filename = FileName::macro_expansion_source_code(&source);\n+    parse_stream_from_source_str(filename, source, sess, Some(span))\n }\n \n fn prepend_attrs(\n+    sess: &ParseSess,\n     attrs: &[ast::Attribute],\n+    nt: &Nonterminal,\n+    span: Span,\n     tokens: Option<&tokenstream::LazyTokenStream>,\n ) -> Option<tokenstream::TokenStream> {\n-    let tokens = tokens?.create_token_stream();\n     if attrs.is_empty() {\n-        return Some(tokens);\n+        return Some(tokens?.create_token_stream());\n     }\n     let mut builder = tokenstream::TokenStreamBuilder::new();\n     for attr in attrs {\n         // FIXME: Correctly handle tokens for inner attributes.\n         // For now, we fall back to reparsing the original AST node\n         if attr.style == ast::AttrStyle::Inner {\n-            return None;\n+            return Some(fake_token_stream(sess, nt, span));\n         }\n         builder.push(attr.tokens());\n     }\n-    builder.push(tokens);\n+    builder.push(tokens?.create_token_stream());\n     Some(builder.build())\n }"}]}