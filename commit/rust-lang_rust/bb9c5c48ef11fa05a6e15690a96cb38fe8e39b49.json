{"sha": "bb9c5c48ef11fa05a6e15690a96cb38fe8e39b49", "node_id": "MDY6Q29tbWl0NzI0NzEyOmJiOWM1YzQ4ZWYxMWZhMDVhNmUxNTY5MGE5NmNiMzhmZThlMzliNDk=", "commit": {"author": {"name": "bors", "email": "bors@rust-lang.org", "date": "2014-01-03T07:56:53Z"}, "committer": {"name": "bors", "email": "bors@rust-lang.org", "date": "2014-01-03T07:56:53Z"}, "message": "auto merge of #11052 : jvns/rust/testing-tutorial, r=brson\n\nThere's no explanation anywhere right now of how to do testing with Rust, so here's a basic explanation of how to write and run a test.", "tree": {"sha": "dcb4f0365d8b3dcf5d7f47c01878348751b5a361", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/dcb4f0365d8b3dcf5d7f47c01878348751b5a361"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/bb9c5c48ef11fa05a6e15690a96cb38fe8e39b49", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/bb9c5c48ef11fa05a6e15690a96cb38fe8e39b49", "html_url": "https://github.com/rust-lang/rust/commit/bb9c5c48ef11fa05a6e15690a96cb38fe8e39b49", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/bb9c5c48ef11fa05a6e15690a96cb38fe8e39b49/comments", "author": {"login": "bors", "id": 3372342, "node_id": "MDQ6VXNlcjMzNzIzNDI=", "avatar_url": "https://avatars.githubusercontent.com/u/3372342?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bors", "html_url": "https://github.com/bors", "followers_url": "https://api.github.com/users/bors/followers", "following_url": "https://api.github.com/users/bors/following{/other_user}", "gists_url": "https://api.github.com/users/bors/gists{/gist_id}", "starred_url": "https://api.github.com/users/bors/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bors/subscriptions", "organizations_url": "https://api.github.com/users/bors/orgs", "repos_url": "https://api.github.com/users/bors/repos", "events_url": "https://api.github.com/users/bors/events{/privacy}", "received_events_url": "https://api.github.com/users/bors/received_events", "type": "User", "site_admin": false}, "committer": {"login": "bors", "id": 3372342, "node_id": "MDQ6VXNlcjMzNzIzNDI=", "avatar_url": "https://avatars.githubusercontent.com/u/3372342?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bors", "html_url": "https://github.com/bors", "followers_url": "https://api.github.com/users/bors/followers", "following_url": "https://api.github.com/users/bors/following{/other_user}", "gists_url": "https://api.github.com/users/bors/gists{/gist_id}", "starred_url": "https://api.github.com/users/bors/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bors/subscriptions", "organizations_url": "https://api.github.com/users/bors/orgs", "repos_url": "https://api.github.com/users/bors/repos", "events_url": "https://api.github.com/users/bors/events{/privacy}", "received_events_url": "https://api.github.com/users/bors/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "fb46225980bea260ef00574aec95d52d18eb1710", "url": "https://api.github.com/repos/rust-lang/rust/commits/fb46225980bea260ef00574aec95d52d18eb1710", "html_url": "https://github.com/rust-lang/rust/commit/fb46225980bea260ef00574aec95d52d18eb1710"}, {"sha": "f0322371dda75d66d006a5c5ec4ffebb8af030c5", "url": "https://api.github.com/repos/rust-lang/rust/commits/f0322371dda75d66d006a5c5ec4ffebb8af030c5", "html_url": "https://github.com/rust-lang/rust/commit/f0322371dda75d66d006a5c5ec4ffebb8af030c5"}], "stats": {"total": 272, "additions": 272, "deletions": 0}, "files": [{"sha": "b09ba605d2c593b963ea9d02228d7743f1e7bb64", "filename": "doc/tutorial-testing.md", "status": "added", "additions": 263, "deletions": 0, "changes": 263, "blob_url": "https://github.com/rust-lang/rust/blob/bb9c5c48ef11fa05a6e15690a96cb38fe8e39b49/doc%2Ftutorial-testing.md", "raw_url": "https://github.com/rust-lang/rust/raw/bb9c5c48ef11fa05a6e15690a96cb38fe8e39b49/doc%2Ftutorial-testing.md", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/doc%2Ftutorial-testing.md?ref=bb9c5c48ef11fa05a6e15690a96cb38fe8e39b49", "patch": "@@ -0,0 +1,263 @@\n+% Rust Testing Tutorial\n+\n+# Quick start\n+\n+To create test functions, add a `#[test]` attribute like this:\n+\n+```rust\n+fn return_two() -> int {\n+    2\n+}\n+\n+#[test]\n+fn return_two_test() {\n+    let x = return_two();\n+    assert!(x == 2);\n+}\n+```\n+\n+To run these tests, use `rustc --test`:\n+\n+```\n+$ rustc --test foo.rs; ./foo\n+running 1 test\n+test return_two_test ... ok\n+\n+test result: ok. 1 passed; 0 failed; 0 ignored; 0 measured\n+```\n+\n+`rustc foo.rs` will *not* compile the tests, since `#[test]` implies\n+`#[cfg(test)]`. The `--test` flag to `rustc` implies `--cfg test`.\n+\n+\n+# Unit testing in Rust\n+\n+Rust has built in support for simple unit testing. Functions can be\n+marked as unit tests using the 'test' attribute.\n+\n+```rust\n+#[test]\n+fn return_none_if_empty() {\n+    // ... test code ...\n+}\n+```\n+\n+A test function's signature must have no arguments and no return\n+value. To run the tests in a crate, it must be compiled with the\n+'--test' flag: `rustc myprogram.rs --test -o myprogram-tests`. Running\n+the resulting executable will run all the tests in the crate. A test\n+is considered successful if its function returns; if the task running\n+the test fails, through a call to `fail!`, a failed `check` or\n+`assert`, or some other (`assert_eq`, `assert_approx_eq`, ...) means,\n+then the test fails.\n+\n+When compiling a crate with the '--test' flag '--cfg test' is also\n+implied, so that tests can be conditionally compiled.\n+\n+```rust\n+#[cfg(test)]\n+mod tests {\n+    #[test]\n+    fn return_none_if_empty() {\n+      // ... test code ...\n+    }\n+}\n+```\n+\n+Additionally #[test] items behave as if they also have the\n+#[cfg(test)] attribute, and will not be compiled when the --test flag\n+is not used.\n+\n+Tests that should not be run can be annotated with the 'ignore'\n+attribute. The existence of these tests will be noted in the test\n+runner output, but the test will not be run. Tests can also be ignored\n+by configuration so, for example, to ignore a test on windows you can\n+write `#[ignore(cfg(target_os = \"win32\"))]`.\n+\n+Tests that are intended to fail can be annotated with the\n+'should_fail' attribute. The test will be run, and if it causes its\n+task to fail then the test will be counted as successful; otherwise it\n+will be counted as a failure. For example:\n+\n+```rust\n+#[test]\n+#[should_fail]\n+fn test_out_of_bounds_failure() {\n+    let v: [int] = [];\n+    v[0];\n+}\n+```\n+\n+A test runner built with the '--test' flag supports a limited set of\n+arguments to control which tests are run: the first free argument\n+passed to a test runner specifies a filter used to narrow down the set\n+of tests being run; the '--ignored' flag tells the test runner to run\n+only tests with the 'ignore' attribute.\n+\n+## Parallelism\n+\n+By default, tests are run in parallel, which can make interpreting\n+failure output difficult. In these cases you can set the\n+`RUST_TEST_TASKS` environment variable to 1 to make the tests run\n+sequentially.\n+\n+## Benchmarking\n+\n+The test runner also understands a simple form of benchmark execution.\n+Benchmark functions are marked with the `#[bench]` attribute, rather\n+than `#[test]`, and have a different form and meaning. They are\n+compiled along with `#[test]` functions when a crate is compiled with\n+`--test`, but they are not run by default. To run the benchmark\n+component of your testsuite, pass `--bench` to the compiled test\n+runner.\n+\n+The type signature of a benchmark function differs from a unit test:\n+it takes a mutable reference to type `test::BenchHarness`. Inside the\n+benchmark function, any time-variable or \"setup\" code should execute\n+first, followed by a call to `iter` on the benchmark harness, passing\n+a closure that contains the portion of the benchmark you wish to\n+actually measure the per-iteration speed of.\n+\n+For benchmarks relating to processing/generating data, one can set the\n+`bytes` field to the number of bytes consumed/produced in each\n+iteration; this will used to show the throughput of the benchmark.\n+This must be the amount used in each iteration, *not* the total\n+amount.\n+\n+For example:\n+\n+```rust\n+extern mod extra;\n+use std::vec;\n+\n+#[bench]\n+fn bench_sum_1024_ints(b: &mut extra::test::BenchHarness) {\n+    let v = vec::from_fn(1024, |n| n);\n+    b.iter(|| {v.iter().fold(0, |old, new| old + *new);} );\n+}\n+\n+#[bench]\n+fn initialise_a_vector(b: &mut extra::test::BenchHarness) {\n+    b.iter(|| {vec::from_elem(1024, 0u64);} );\n+    b.bytes = 1024 * 8;\n+}\n+```\n+\n+The benchmark runner will calibrate measurement of the benchmark\n+function to run the `iter` block \"enough\" times to get a reliable\n+measure of the per-iteration speed.\n+\n+Advice on writing benchmarks:\n+\n+  - Move setup code outside the `iter` loop; only put the part you\n+    want to measure inside\n+  - Make the code do \"the same thing\" on each iteration; do not\n+    accumulate or change state\n+  - Make the outer function idempotent too; the benchmark runner is\n+    likely to run it many times\n+  - Make the inner `iter` loop short and fast so benchmark runs are\n+    fast and the calibrator can adjust the run-length at fine\n+    resolution\n+  - Make the code in the `iter` loop do something simple, to assist in\n+    pinpointing performance improvements (or regressions)\n+\n+To run benchmarks, pass the `--bench` flag to the compiled\n+test-runner. Benchmarks are compiled-in but not executed by default.\n+\n+## Examples\n+\n+### Typical test run\n+\n+```\n+> mytests\n+\n+running 30 tests\n+running driver::tests::mytest1 ... ok\n+running driver::tests::mytest2 ... ignored\n+... snip ...\n+running driver::tests::mytest30 ... ok\n+\n+result: ok. 28 passed; 0 failed; 2 ignored\n+```\n+\n+### Test run with failures\n+\n+```\n+> mytests\n+\n+running 30 tests\n+running driver::tests::mytest1 ... ok\n+running driver::tests::mytest2 ... ignored\n+... snip ...\n+running driver::tests::mytest30 ... FAILED\n+\n+result: FAILED. 27 passed; 1 failed; 2 ignored\n+```\n+\n+### Running ignored tests\n+\n+```\n+> mytests --ignored\n+\n+running 2 tests\n+running driver::tests::mytest2 ... failed\n+running driver::tests::mytest10 ... ok\n+\n+result: FAILED. 1 passed; 1 failed; 0 ignored\n+```\n+\n+### Running a subset of tests\n+\n+```\n+> mytests mytest1\n+\n+running 11 tests\n+running driver::tests::mytest1 ... ok\n+running driver::tests::mytest10 ... ignored\n+... snip ...\n+running driver::tests::mytest19 ... ok\n+\n+result: ok. 11 passed; 0 failed; 1 ignored\n+```\n+\n+### Running benchmarks\n+\n+```\n+> mytests --bench\n+\n+running 2 tests\n+test bench_sum_1024_ints ... bench: 709 ns/iter (+/- 82)\n+test initialise_a_vector ... bench: 424 ns/iter (+/- 99) = 19320 MB/s\n+\n+test result: ok. 0 passed; 0 failed; 0 ignored; 2 measured\n+```\n+\n+## Saving and ratcheting metrics\n+\n+When running benchmarks or other tests, the test runner can record\n+per-test \"metrics\". Each metric is a scalar `f64` value, plus a noise\n+value which represents uncertainty in the measurement. By default, all\n+`#[bench]` benchmarks are recorded as metrics, which can be saved as\n+JSON in an external file for further reporting.\n+\n+In addition, the test runner supports _ratcheting_ against a metrics\n+file. Ratcheting is like saving metrics, except that after each run,\n+if the output file already exists the results of the current run are\n+compared against the contents of the existing file, and any regression\n+_causes the testsuite to fail_. If the comparison passes -- if all\n+metrics stayed the same (within noise) or improved -- then the metrics\n+file is overwritten with the new values. In this way, a metrics file\n+in your workspace can be used to ensure your work does not regress\n+performance.\n+\n+Test runners take 3 options that are relevant to metrics:\n+\n+  - `--save-metrics=<file.json>` will save the metrics from a test run\n+    to `file.json`\n+  - `--ratchet-metrics=<file.json>` will ratchet the metrics against\n+    the `file.json`\n+  - `--ratchet-noise-percent=N` will override the noise measurements\n+    in `file.json`, and consider a metric change less than `N%` to be\n+    noise. This can be helpful if you are testing in a noisy\n+    environment where the benchmark calibration loop cannot acquire a\n+    clear enough signal."}, {"sha": "a19d8c3e820ffdc8a0d49f58d955633d4706db17", "filename": "doc/tutorial.md", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/bb9c5c48ef11fa05a6e15690a96cb38fe8e39b49/doc%2Ftutorial.md", "raw_url": "https://github.com/rust-lang/rust/raw/bb9c5c48ef11fa05a6e15690a96cb38fe8e39b49/doc%2Ftutorial.md", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/doc%2Ftutorial.md?ref=bb9c5c48ef11fa05a6e15690a96cb38fe8e39b49", "patch": "@@ -3220,6 +3220,7 @@ tutorials on individual topics.\n * [Error-handling and Conditions][conditions]\n * [Packaging up Rust code][rustpkg]\n * [Documenting Rust code][rustdoc]\n+* [Testing Rust code][testing]\n \n There is further documentation on the [wiki], however those tend to be even\n more out of date than this document.\n@@ -3231,6 +3232,7 @@ more out of date than this document.\n [container]: tutorial-container.html\n [conditions]: tutorial-conditions.html\n [rustpkg]: tutorial-rustpkg.html\n+[testing]: tutorial-testing.html\n [rustdoc]: rustdoc.html\n \n [wiki]: https://github.com/mozilla/rust/wiki/Docs"}, {"sha": "f211af700b9363ec34401245b42352048594cff5", "filename": "mk/docs.mk", "status": "modified", "additions": 7, "deletions": 0, "changes": 7, "blob_url": "https://github.com/rust-lang/rust/blob/bb9c5c48ef11fa05a6e15690a96cb38fe8e39b49/mk%2Fdocs.mk", "raw_url": "https://github.com/rust-lang/rust/raw/bb9c5c48ef11fa05a6e15690a96cb38fe8e39b49/mk%2Fdocs.mk", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/mk%2Fdocs.mk?ref=bb9c5c48ef11fa05a6e15690a96cb38fe8e39b49", "patch": "@@ -133,6 +133,13 @@ doc/tutorial-ffi.html: tutorial-ffi.md doc/version_info.html doc/rust.css \\\n \t$(Q)$(CFG_NODE) $(S)doc/prep.js --highlight $< | \\\n \t$(CFG_PANDOC) $(HTML_OPTS) --output=$@\n \n+DOCS += doc/tutorial-testing.html\n+doc/tutorial-testing.html: tutorial-testing.md doc/version_info.html doc/rust.css \\\n+\t\t\t\tdoc/favicon.inc\n+\t@$(call E, pandoc: $@)\n+\t$(Q)$(CFG_NODE) $(S)doc/prep.js --highlight $< | \\\n+\t$(CFG_PANDOC) $(HTML_OPTS) --output=$@\n+\n DOCS += doc/tutorial-borrowed-ptr.html\n doc/tutorial-borrowed-ptr.html: tutorial-borrowed-ptr.md doc/version_info.html doc/rust.css \\\n \t\t\t\tdoc/favicon.inc"}]}