{"sha": "6f313cef8e9afda1172134bceb0dbf7c6362709e", "node_id": "C_kwDOAAsO6NoAKDZmMzEzY2VmOGU5YWZkYTExNzIxMzRiY2ViMGRiZjdjNjM2MjcwOWU", "commit": {"author": {"name": "bors", "email": "bors@rust-lang.org", "date": "2022-11-11T12:19:30Z"}, "committer": {"name": "bors", "email": "bors@rust-lang.org", "date": "2022-11-11T12:19:30Z"}, "message": "Auto merge of #13548 - lowr:fix/tt-punct-spacing, r=Veykril\n\nFix `tt::Punct`'s spacing calculation\n\nFixes #13499\n\nWe currently set a `tt::Punct`'s spacing to `Spacing::Joint` unless its next token is a trivia (i.e. whitespaces or comment). As I understand it, rustc only [sets `Spacing::Joint` if the next token is an operator](https://github.com/rust-lang/rust/blob/5b3e9090757da9a95b22f589fe39b6a4b5455b96/compiler/rustc_parse/src/lexer/tokentrees.rs#L77-L78) and we should follow it to guarantee the consistent behavior of proc macros.", "tree": {"sha": "572601a1829e5af81f53d2a698d28077b495cf94", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/572601a1829e5af81f53d2a698d28077b495cf94"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/6f313cef8e9afda1172134bceb0dbf7c6362709e", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/6f313cef8e9afda1172134bceb0dbf7c6362709e", "html_url": "https://github.com/rust-lang/rust/commit/6f313cef8e9afda1172134bceb0dbf7c6362709e", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/6f313cef8e9afda1172134bceb0dbf7c6362709e/comments", "author": {"login": "bors", "id": 3372342, "node_id": "MDQ6VXNlcjMzNzIzNDI=", "avatar_url": "https://avatars.githubusercontent.com/u/3372342?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bors", "html_url": "https://github.com/bors", "followers_url": "https://api.github.com/users/bors/followers", "following_url": "https://api.github.com/users/bors/following{/other_user}", "gists_url": "https://api.github.com/users/bors/gists{/gist_id}", "starred_url": "https://api.github.com/users/bors/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bors/subscriptions", "organizations_url": "https://api.github.com/users/bors/orgs", "repos_url": "https://api.github.com/users/bors/repos", "events_url": "https://api.github.com/users/bors/events{/privacy}", "received_events_url": "https://api.github.com/users/bors/received_events", "type": "User", "site_admin": false}, "committer": {"login": "bors", "id": 3372342, "node_id": "MDQ6VXNlcjMzNzIzNDI=", "avatar_url": "https://avatars.githubusercontent.com/u/3372342?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bors", "html_url": "https://github.com/bors", "followers_url": "https://api.github.com/users/bors/followers", "following_url": "https://api.github.com/users/bors/following{/other_user}", "gists_url": "https://api.github.com/users/bors/gists{/gist_id}", "starred_url": "https://api.github.com/users/bors/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bors/subscriptions", "organizations_url": "https://api.github.com/users/bors/orgs", "repos_url": "https://api.github.com/users/bors/repos", "events_url": "https://api.github.com/users/bors/events{/privacy}", "received_events_url": "https://api.github.com/users/bors/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "ff78d24e21fe999c65aeec825f3bbb02c1ec5a4c", "url": "https://api.github.com/repos/rust-lang/rust/commits/ff78d24e21fe999c65aeec825f3bbb02c1ec5a4c", "html_url": "https://github.com/rust-lang/rust/commit/ff78d24e21fe999c65aeec825f3bbb02c1ec5a4c"}, {"sha": "5b070610118a98d96869af4cad79575d4edc5941", "url": "https://api.github.com/repos/rust-lang/rust/commits/5b070610118a98d96869af4cad79575d4edc5941", "html_url": "https://github.com/rust-lang/rust/commit/5b070610118a98d96869af4cad79575d4edc5941"}], "stats": {"total": 294, "additions": 229, "deletions": 65}, "files": [{"sha": "fc90c6e9f370f107cacd59606055ecba9a6d94ea", "filename": "crates/hir-def/src/macro_expansion_tests/mbe/matching.rs", "status": "modified", "additions": 5, "deletions": 5, "changes": 10, "blob_url": "https://github.com/rust-lang/rust/blob/6f313cef8e9afda1172134bceb0dbf7c6362709e/crates%2Fhir-def%2Fsrc%2Fmacro_expansion_tests%2Fmbe%2Fmatching.rs", "raw_url": "https://github.com/rust-lang/rust/raw/6f313cef8e9afda1172134bceb0dbf7c6362709e/crates%2Fhir-def%2Fsrc%2Fmacro_expansion_tests%2Fmbe%2Fmatching.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fhir-def%2Fsrc%2Fmacro_expansion_tests%2Fmbe%2Fmatching.rs?ref=6f313cef8e9afda1172134bceb0dbf7c6362709e", "patch": "@@ -94,11 +94,11 @@ macro_rules! m {\n     ($($s:stmt)*) => (stringify!($($s |)*);)\n }\n stringify!(;\n-|;\n-|92|;\n-|let x = 92|;\n+| ;\n+|92| ;\n+|let x = 92| ;\n |loop {}\n-|;\n+| ;\n |);\n \"#]],\n     );\n@@ -118,7 +118,7 @@ m!(.. .. ..);\n macro_rules! m {\n     ($($p:pat)*) => (stringify!($($p |)*);)\n }\n-stringify!(.. .. ..|);\n+stringify!(.. .. .. |);\n \"#]],\n     );\n }"}, {"sha": "118c14ed843fecf9e3e5ad2a59f2f3ae3e1757bc", "filename": "crates/hir-def/src/macro_expansion_tests/proc_macros.rs", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "blob_url": "https://github.com/rust-lang/rust/blob/6f313cef8e9afda1172134bceb0dbf7c6362709e/crates%2Fhir-def%2Fsrc%2Fmacro_expansion_tests%2Fproc_macros.rs", "raw_url": "https://github.com/rust-lang/rust/raw/6f313cef8e9afda1172134bceb0dbf7c6362709e/crates%2Fhir-def%2Fsrc%2Fmacro_expansion_tests%2Fproc_macros.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fhir-def%2Fsrc%2Fmacro_expansion_tests%2Fproc_macros.rs?ref=6f313cef8e9afda1172134bceb0dbf7c6362709e", "patch": "@@ -82,14 +82,14 @@ fn attribute_macro_syntax_completion_2() {\n #[proc_macros::identity_when_valid]\n fn foo() { bar.; blub }\n \"#,\n-        expect![[r##\"\n+        expect![[r#\"\n #[proc_macros::identity_when_valid]\n fn foo() { bar.; blub }\n \n fn foo() {\n-    bar.;\n+    bar. ;\n     blub\n-}\"##]],\n+}\"#]],\n     );\n }\n "}, {"sha": "a4abe75626e6dcacf24df7e822857e404eecc919", "filename": "crates/hir-expand/src/fixup.rs", "status": "modified", "additions": 66, "deletions": 30, "changes": 96, "blob_url": "https://github.com/rust-lang/rust/blob/6f313cef8e9afda1172134bceb0dbf7c6362709e/crates%2Fhir-expand%2Fsrc%2Ffixup.rs", "raw_url": "https://github.com/rust-lang/rust/raw/6f313cef8e9afda1172134bceb0dbf7c6362709e/crates%2Fhir-expand%2Fsrc%2Ffixup.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fhir-expand%2Fsrc%2Ffixup.rs?ref=6f313cef8e9afda1172134bceb0dbf7c6362709e", "patch": "@@ -4,6 +4,7 @@ use std::mem;\n \n use mbe::{SyntheticToken, SyntheticTokenId, TokenMap};\n use rustc_hash::FxHashMap;\n+use smallvec::SmallVec;\n use syntax::{\n     ast::{self, AstNode, HasLoopBody},\n     match_ast, SyntaxElement, SyntaxKind, SyntaxNode, TextRange,\n@@ -292,25 +293,34 @@ pub(crate) fn reverse_fixups(\n     token_map: &TokenMap,\n     undo_info: &SyntaxFixupUndoInfo,\n ) {\n-    tt.token_trees.retain(|tt| match tt {\n-        tt::TokenTree::Leaf(leaf) => {\n-            token_map.synthetic_token_id(leaf.id()).is_none()\n-                || token_map.synthetic_token_id(leaf.id()) != Some(EMPTY_ID)\n-        }\n-        tt::TokenTree::Subtree(st) => st.delimiter.map_or(true, |d| {\n-            token_map.synthetic_token_id(d.id).is_none()\n-                || token_map.synthetic_token_id(d.id) != Some(EMPTY_ID)\n-        }),\n-    });\n-    tt.token_trees.iter_mut().for_each(|tt| match tt {\n-        tt::TokenTree::Subtree(tt) => reverse_fixups(tt, token_map, undo_info),\n-        tt::TokenTree::Leaf(leaf) => {\n-            if let Some(id) = token_map.synthetic_token_id(leaf.id()) {\n-                let original = &undo_info.original[id.0 as usize];\n-                *tt = tt::TokenTree::Subtree(original.clone());\n+    let tts = std::mem::take(&mut tt.token_trees);\n+    tt.token_trees = tts\n+        .into_iter()\n+        .filter(|tt| match tt {\n+            tt::TokenTree::Leaf(leaf) => token_map.synthetic_token_id(leaf.id()) != Some(EMPTY_ID),\n+            tt::TokenTree::Subtree(st) => {\n+                st.delimiter.map_or(true, |d| token_map.synthetic_token_id(d.id) != Some(EMPTY_ID))\n             }\n-        }\n-    });\n+        })\n+        .flat_map(|tt| match tt {\n+            tt::TokenTree::Subtree(mut tt) => {\n+                reverse_fixups(&mut tt, token_map, undo_info);\n+                SmallVec::from_const([tt.into()])\n+            }\n+            tt::TokenTree::Leaf(leaf) => {\n+                if let Some(id) = token_map.synthetic_token_id(leaf.id()) {\n+                    let original = undo_info.original[id.0 as usize].clone();\n+                    if original.delimiter.is_none() {\n+                        original.token_trees.into()\n+                    } else {\n+                        SmallVec::from_const([original.into()])\n+                    }\n+                } else {\n+                    SmallVec::from_const([leaf.into()])\n+                }\n+            }\n+        })\n+        .collect();\n }\n \n #[cfg(test)]\n@@ -319,6 +329,31 @@ mod tests {\n \n     use super::reverse_fixups;\n \n+    // The following three functions are only meant to check partial structural equivalence of\n+    // `TokenTree`s, see the last assertion in `check()`.\n+    fn check_leaf_eq(a: &tt::Leaf, b: &tt::Leaf) -> bool {\n+        match (a, b) {\n+            (tt::Leaf::Literal(a), tt::Leaf::Literal(b)) => a.text == b.text,\n+            (tt::Leaf::Punct(a), tt::Leaf::Punct(b)) => a.char == b.char,\n+            (tt::Leaf::Ident(a), tt::Leaf::Ident(b)) => a.text == b.text,\n+            _ => false,\n+        }\n+    }\n+\n+    fn check_subtree_eq(a: &tt::Subtree, b: &tt::Subtree) -> bool {\n+        a.delimiter.map(|it| it.kind) == b.delimiter.map(|it| it.kind)\n+            && a.token_trees.len() == b.token_trees.len()\n+            && a.token_trees.iter().zip(&b.token_trees).all(|(a, b)| check_tt_eq(a, b))\n+    }\n+\n+    fn check_tt_eq(a: &tt::TokenTree, b: &tt::TokenTree) -> bool {\n+        match (a, b) {\n+            (tt::TokenTree::Leaf(a), tt::TokenTree::Leaf(b)) => check_leaf_eq(a, b),\n+            (tt::TokenTree::Subtree(a), tt::TokenTree::Subtree(b)) => check_subtree_eq(a, b),\n+            _ => false,\n+        }\n+    }\n+\n     #[track_caller]\n     fn check(ra_fixture: &str, mut expect: Expect) {\n         let parsed = syntax::SourceFile::parse(ra_fixture);\n@@ -331,27 +366,28 @@ mod tests {\n             fixups.append,\n         );\n \n-        let mut actual = tt.to_string();\n-        actual.push('\\n');\n+        let actual = format!(\"{}\\n\", tt);\n \n         expect.indent(false);\n         expect.assert_eq(&actual);\n \n         // the fixed-up tree should be syntactically valid\n         let (parse, _) = mbe::token_tree_to_syntax_node(&tt, ::mbe::TopEntryPoint::MacroItems);\n-        assert_eq!(\n-            parse.errors(),\n-            &[],\n+        assert!(\n+            parse.errors().is_empty(),\n             \"parse has syntax errors. parse tree:\\n{:#?}\",\n             parse.syntax_node()\n         );\n \n         reverse_fixups(&mut tt, &tmap, &fixups.undo_info);\n \n         // the fixed-up + reversed version should be equivalent to the original input\n-        // (but token IDs don't matter)\n+        // modulo token IDs and `Punct`s' spacing.\n         let (original_as_tt, _) = mbe::syntax_node_to_token_tree(&parsed.syntax_node());\n-        assert_eq!(tt.to_string(), original_as_tt.to_string());\n+        assert!(\n+            check_subtree_eq(&tt, &original_as_tt),\n+            \"different token tree: {tt:?}, {original_as_tt:?}\"\n+        );\n     }\n \n     #[test]\n@@ -468,7 +504,7 @@ fn foo() {\n }\n \"#,\n             expect![[r#\"\n-fn foo () {a .__ra_fixup}\n+fn foo () {a . __ra_fixup}\n \"#]],\n         )\n     }\n@@ -482,7 +518,7 @@ fn foo() {\n }\n \"#,\n             expect![[r#\"\n-fn foo () {a .__ra_fixup ;}\n+fn foo () {a . __ra_fixup ;}\n \"#]],\n         )\n     }\n@@ -497,7 +533,7 @@ fn foo() {\n }\n \"#,\n             expect![[r#\"\n-fn foo () {a .__ra_fixup ; bar () ;}\n+fn foo () {a . __ra_fixup ; bar () ;}\n \"#]],\n         )\n     }\n@@ -525,7 +561,7 @@ fn foo() {\n }\n \"#,\n             expect![[r#\"\n-fn foo () {let x = a .__ra_fixup ;}\n+fn foo () {let x = a . __ra_fixup ;}\n \"#]],\n         )\n     }\n@@ -541,7 +577,7 @@ fn foo() {\n }\n \"#,\n             expect![[r#\"\n-fn foo () {a .b ; bar () ;}\n+fn foo () {a . b ; bar () ;}\n \"#]],\n         )\n     }"}, {"sha": "cf53c16726bf7fe092c11fe7cf87748f39d2f9b8", "filename": "crates/mbe/src/syntax_bridge.rs", "status": "modified", "additions": 62, "deletions": 27, "changes": 89, "blob_url": "https://github.com/rust-lang/rust/blob/6f313cef8e9afda1172134bceb0dbf7c6362709e/crates%2Fmbe%2Fsrc%2Fsyntax_bridge.rs", "raw_url": "https://github.com/rust-lang/rust/raw/6f313cef8e9afda1172134bceb0dbf7c6362709e/crates%2Fmbe%2Fsrc%2Fsyntax_bridge.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fmbe%2Fsrc%2Fsyntax_bridge.rs?ref=6f313cef8e9afda1172134bceb0dbf7c6362709e", "patch": "@@ -12,6 +12,9 @@ use tt::buffer::{Cursor, TokenBuffer};\n \n use crate::{to_parser_input::to_parser_input, tt_iter::TtIter, TokenMap};\n \n+#[cfg(test)]\n+mod tests;\n+\n /// Convert the syntax node to a `TokenTree` (what macro\n /// will consume).\n pub fn syntax_node_to_token_tree(node: &SyntaxNode) -> (tt::Subtree, TokenMap) {\n@@ -35,7 +38,7 @@ pub fn syntax_node_to_token_tree_with_modifications(\n     append: FxHashMap<SyntaxElement, Vec<SyntheticToken>>,\n ) -> (tt::Subtree, TokenMap, u32) {\n     let global_offset = node.text_range().start();\n-    let mut c = Convertor::new(node, global_offset, existing_token_map, next_id, replace, append);\n+    let mut c = Converter::new(node, global_offset, existing_token_map, next_id, replace, append);\n     let subtree = convert_tokens(&mut c);\n     c.id_alloc.map.shrink_to_fit();\n     always!(c.replace.is_empty(), \"replace: {:?}\", c.replace);\n@@ -100,7 +103,7 @@ pub fn parse_to_token_tree(text: &str) -> Option<(tt::Subtree, TokenMap)> {\n         return None;\n     }\n \n-    let mut conv = RawConvertor {\n+    let mut conv = RawConverter {\n         lexed,\n         pos: 0,\n         id_alloc: TokenIdAlloc {\n@@ -148,7 +151,7 @@ pub fn parse_exprs_with_sep(tt: &tt::Subtree, sep: char) -> Vec<tt::Subtree> {\n     res\n }\n \n-fn convert_tokens<C: TokenConvertor>(conv: &mut C) -> tt::Subtree {\n+fn convert_tokens<C: TokenConverter>(conv: &mut C) -> tt::Subtree {\n     struct StackEntry {\n         subtree: tt::Subtree,\n         idx: usize,\n@@ -228,7 +231,7 @@ fn convert_tokens<C: TokenConvertor>(conv: &mut C) -> tt::Subtree {\n             }\n \n             let spacing = match conv.peek().map(|next| next.kind(conv)) {\n-                Some(kind) if !kind.is_trivia() => tt::Spacing::Joint,\n+                Some(kind) if is_single_token_op(kind) => tt::Spacing::Joint,\n                 _ => tt::Spacing::Alone,\n             };\n             let char = match token.to_char(conv) {\n@@ -307,6 +310,35 @@ fn convert_tokens<C: TokenConvertor>(conv: &mut C) -> tt::Subtree {\n     }\n }\n \n+fn is_single_token_op(kind: SyntaxKind) -> bool {\n+    matches!(\n+        kind,\n+        EQ | L_ANGLE\n+            | R_ANGLE\n+            | BANG\n+            | AMP\n+            | PIPE\n+            | TILDE\n+            | AT\n+            | DOT\n+            | COMMA\n+            | SEMICOLON\n+            | COLON\n+            | POUND\n+            | DOLLAR\n+            | QUESTION\n+            | PLUS\n+            | MINUS\n+            | STAR\n+            | SLASH\n+            | PERCENT\n+            | CARET\n+            // LIFETIME_IDENT will be split into a sequence of `'` (a single quote) and an\n+            // identifier.\n+            | LIFETIME_IDENT\n+    )\n+}\n+\n /// Returns the textual content of a doc comment block as a quoted string\n /// That is, strips leading `///` (or `/**`, etc)\n /// and strips the ending `*/`\n@@ -425,8 +457,8 @@ impl TokenIdAlloc {\n     }\n }\n \n-/// A raw token (straight from lexer) convertor\n-struct RawConvertor<'a> {\n+/// A raw token (straight from lexer) converter\n+struct RawConverter<'a> {\n     lexed: parser::LexedStr<'a>,\n     pos: usize,\n     id_alloc: TokenIdAlloc,\n@@ -442,7 +474,7 @@ trait SrcToken<Ctx>: std::fmt::Debug {\n     fn synthetic_id(&self, ctx: &Ctx) -> Option<SyntheticTokenId>;\n }\n \n-trait TokenConvertor: Sized {\n+trait TokenConverter: Sized {\n     type Token: SrcToken<Self>;\n \n     fn convert_doc_comment(&self, token: &Self::Token) -> Option<Vec<tt::TokenTree>>;\n@@ -454,25 +486,25 @@ trait TokenConvertor: Sized {\n     fn id_alloc(&mut self) -> &mut TokenIdAlloc;\n }\n \n-impl<'a> SrcToken<RawConvertor<'a>> for usize {\n-    fn kind(&self, ctx: &RawConvertor<'a>) -> SyntaxKind {\n+impl<'a> SrcToken<RawConverter<'a>> for usize {\n+    fn kind(&self, ctx: &RawConverter<'a>) -> SyntaxKind {\n         ctx.lexed.kind(*self)\n     }\n \n-    fn to_char(&self, ctx: &RawConvertor<'a>) -> Option<char> {\n+    fn to_char(&self, ctx: &RawConverter<'a>) -> Option<char> {\n         ctx.lexed.text(*self).chars().next()\n     }\n \n-    fn to_text(&self, ctx: &RawConvertor<'_>) -> SmolStr {\n+    fn to_text(&self, ctx: &RawConverter<'_>) -> SmolStr {\n         ctx.lexed.text(*self).into()\n     }\n \n-    fn synthetic_id(&self, _ctx: &RawConvertor<'a>) -> Option<SyntheticTokenId> {\n+    fn synthetic_id(&self, _ctx: &RawConverter<'a>) -> Option<SyntheticTokenId> {\n         None\n     }\n }\n \n-impl<'a> TokenConvertor for RawConvertor<'a> {\n+impl<'a> TokenConverter for RawConverter<'a> {\n     type Token = usize;\n \n     fn convert_doc_comment(&self, &token: &usize) -> Option<Vec<tt::TokenTree>> {\n@@ -504,7 +536,7 @@ impl<'a> TokenConvertor for RawConvertor<'a> {\n     }\n }\n \n-struct Convertor {\n+struct Converter {\n     id_alloc: TokenIdAlloc,\n     current: Option<SyntaxToken>,\n     current_synthetic: Vec<SyntheticToken>,\n@@ -515,19 +547,19 @@ struct Convertor {\n     punct_offset: Option<(SyntaxToken, TextSize)>,\n }\n \n-impl Convertor {\n+impl Converter {\n     fn new(\n         node: &SyntaxNode,\n         global_offset: TextSize,\n         existing_token_map: TokenMap,\n         next_id: u32,\n         mut replace: FxHashMap<SyntaxElement, Vec<SyntheticToken>>,\n         mut append: FxHashMap<SyntaxElement, Vec<SyntheticToken>>,\n-    ) -> Convertor {\n+    ) -> Converter {\n         let range = node.text_range();\n         let mut preorder = node.preorder_with_tokens();\n         let (first, synthetic) = Self::next_token(&mut preorder, &mut replace, &mut append);\n-        Convertor {\n+        Converter {\n             id_alloc: { TokenIdAlloc { map: existing_token_map, global_offset, next_id } },\n             current: first,\n             current_synthetic: synthetic,\n@@ -590,39 +622,39 @@ impl SynToken {\n     }\n }\n \n-impl SrcToken<Convertor> for SynToken {\n-    fn kind(&self, _ctx: &Convertor) -> SyntaxKind {\n+impl SrcToken<Converter> for SynToken {\n+    fn kind(&self, ctx: &Converter) -> SyntaxKind {\n         match self {\n             SynToken::Ordinary(token) => token.kind(),\n-            SynToken::Punch(token, _) => token.kind(),\n+            SynToken::Punch(..) => SyntaxKind::from_char(self.to_char(ctx).unwrap()).unwrap(),\n             SynToken::Synthetic(token) => token.kind,\n         }\n     }\n-    fn to_char(&self, _ctx: &Convertor) -> Option<char> {\n+    fn to_char(&self, _ctx: &Converter) -> Option<char> {\n         match self {\n             SynToken::Ordinary(_) => None,\n             SynToken::Punch(it, i) => it.text().chars().nth((*i).into()),\n             SynToken::Synthetic(token) if token.text.len() == 1 => token.text.chars().next(),\n             SynToken::Synthetic(_) => None,\n         }\n     }\n-    fn to_text(&self, _ctx: &Convertor) -> SmolStr {\n+    fn to_text(&self, _ctx: &Converter) -> SmolStr {\n         match self {\n             SynToken::Ordinary(token) => token.text().into(),\n             SynToken::Punch(token, _) => token.text().into(),\n             SynToken::Synthetic(token) => token.text.clone(),\n         }\n     }\n \n-    fn synthetic_id(&self, _ctx: &Convertor) -> Option<SyntheticTokenId> {\n+    fn synthetic_id(&self, _ctx: &Converter) -> Option<SyntheticTokenId> {\n         match self {\n             SynToken::Synthetic(token) => Some(token.id),\n             _ => None,\n         }\n     }\n }\n \n-impl TokenConvertor for Convertor {\n+impl TokenConverter for Converter {\n     type Token = SynToken;\n     fn convert_doc_comment(&self, token: &Self::Token) -> Option<Vec<tt::TokenTree>> {\n         convert_doc_comment(token.token()?)\n@@ -651,7 +683,7 @@ impl TokenConvertor for Convertor {\n         }\n \n         let curr = self.current.clone()?;\n-        if !&self.range.contains_range(curr.text_range()) {\n+        if !self.range.contains_range(curr.text_range()) {\n             return None;\n         }\n         let (new_current, new_synth) =\n@@ -809,12 +841,15 @@ impl<'a> TtTreeSink<'a> {\n         let next = last.bump();\n         if let (\n             Some(tt::buffer::TokenTreeRef::Leaf(tt::Leaf::Punct(curr), _)),\n-            Some(tt::buffer::TokenTreeRef::Leaf(tt::Leaf::Punct(_), _)),\n+            Some(tt::buffer::TokenTreeRef::Leaf(tt::Leaf::Punct(next), _)),\n         ) = (last.token_tree(), next.token_tree())\n         {\n             // Note: We always assume the semi-colon would be the last token in\n             // other parts of RA such that we don't add whitespace here.\n-            if curr.spacing == tt::Spacing::Alone && curr.char != ';' {\n+            //\n+            // When `next` is a `Punct` of `'`, that's a part of a lifetime identifier so we don't\n+            // need to add whitespace either.\n+            if curr.spacing == tt::Spacing::Alone && curr.char != ';' && next.char != '\\'' {\n                 self.inner.token(WHITESPACE, \" \");\n                 self.text_pos += TextSize::of(' ');\n             }"}, {"sha": "4e04d2bc1c77bcbd972d7d349cb7ed2fe6c32129", "filename": "crates/mbe/src/syntax_bridge/tests.rs", "status": "added", "additions": 93, "deletions": 0, "changes": 93, "blob_url": "https://github.com/rust-lang/rust/blob/6f313cef8e9afda1172134bceb0dbf7c6362709e/crates%2Fmbe%2Fsrc%2Fsyntax_bridge%2Ftests.rs", "raw_url": "https://github.com/rust-lang/rust/raw/6f313cef8e9afda1172134bceb0dbf7c6362709e/crates%2Fmbe%2Fsrc%2Fsyntax_bridge%2Ftests.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fmbe%2Fsrc%2Fsyntax_bridge%2Ftests.rs?ref=6f313cef8e9afda1172134bceb0dbf7c6362709e", "patch": "@@ -0,0 +1,93 @@\n+use std::collections::HashMap;\n+\n+use syntax::{ast, AstNode};\n+use test_utils::extract_annotations;\n+use tt::{\n+    buffer::{TokenBuffer, TokenTreeRef},\n+    Leaf, Punct, Spacing,\n+};\n+\n+use super::syntax_node_to_token_tree;\n+\n+fn check_punct_spacing(fixture: &str) {\n+    let source_file = ast::SourceFile::parse(fixture).ok().unwrap();\n+    let (subtree, token_map) = syntax_node_to_token_tree(source_file.syntax());\n+    let mut annotations: HashMap<_, _> = extract_annotations(fixture)\n+        .into_iter()\n+        .map(|(range, annotation)| {\n+            let token = token_map.token_by_range(range).expect(\"no token found\");\n+            let spacing = match annotation.as_str() {\n+                \"Alone\" => Spacing::Alone,\n+                \"Joint\" => Spacing::Joint,\n+                a => panic!(\"unknown annotation: {}\", a),\n+            };\n+            (token, spacing)\n+        })\n+        .collect();\n+\n+    let buf = TokenBuffer::from_subtree(&subtree);\n+    let mut cursor = buf.begin();\n+    while !cursor.eof() {\n+        while let Some(token_tree) = cursor.token_tree() {\n+            if let TokenTreeRef::Leaf(Leaf::Punct(Punct { spacing, id, .. }), _) = token_tree {\n+                if let Some(expected) = annotations.remove(&id) {\n+                    assert_eq!(expected, *spacing);\n+                }\n+            }\n+            cursor = cursor.bump_subtree();\n+        }\n+        cursor = cursor.bump();\n+    }\n+\n+    assert!(annotations.is_empty(), \"unchecked annotations: {:?}\", annotations);\n+}\n+\n+#[test]\n+fn punct_spacing() {\n+    check_punct_spacing(\n+        r#\"\n+fn main() {\n+    0+0;\n+   //^ Alone\n+    0+(0);\n+   //^ Alone\n+    0<=0;\n+   //^ Joint\n+   // ^ Alone\n+    0<=(0);\n+   // ^ Alone\n+    a=0;\n+   //^ Alone\n+    a=(0);\n+   //^ Alone\n+    a+=0;\n+   //^ Joint\n+   // ^ Alone\n+    a+=(0);\n+   // ^ Alone\n+    a&&b;\n+   //^ Joint\n+   // ^ Alone\n+    a&&(b);\n+   // ^ Alone\n+    foo::bar;\n+   //  ^ Joint\n+   //   ^ Alone\n+    use foo::{bar,baz,};\n+   //       ^ Alone\n+   //            ^ Alone\n+   //                ^ Alone\n+    struct Struct<'a> {};\n+   //            ^ Joint\n+   //             ^ Joint\n+    Struct::<0>;\n+   //       ^ Alone\n+    Struct::<{0}>;\n+   //       ^ Alone\n+    ;;\n+  //^ Joint\n+  // ^ Alone\n+}\n+        \"#,\n+    );\n+}"}]}