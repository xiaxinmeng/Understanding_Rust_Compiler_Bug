{"sha": "335667c7749930f9f30e7045bc29690dd81a85f8", "node_id": "C_kwDOAAsO6NoAKDMzNTY2N2M3NzQ5OTMwZjlmMzBlNzA0NWJjMjk2OTBkZDgxYTg1Zjg", "commit": {"author": {"name": "Andy Wang", "email": "cbeuw.andy@gmail.com", "date": "2022-05-14T00:45:21Z"}, "committer": {"name": "Andy Wang", "email": "cbeuw.andy@gmail.com", "date": "2022-06-06T18:15:53Z"}, "message": "Move buffered functions into their own ext trait", "tree": {"sha": "f8260e803616063059c7c76e4b7dcb029912632a", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/f8260e803616063059c7c76e4b7dcb029912632a"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/335667c7749930f9f30e7045bc29690dd81a85f8", "comment_count": 0, "verification": {"verified": true, "reason": "valid", "signature": "-----BEGIN PGP SIGNATURE-----\n\niQGzBAABCgAdFiEE7dcbcBMl24/h63ldGBtJ+fOPM3QFAmKeRFkACgkQGBtJ+fOP\nM3SuWAwAnpcmFQuoYD9sKYqpH1TjgU0JOdme53STrAYlUkPJplw0eMBOfLXFlS/o\ntNuIsALiu/DuEnUXEGK82aWxUDQA4iReEeY7V5xZyf3fa5et+AiIMKHhAtPMMYp2\n4HhEIW8kH7NjclVgwg6j3X7OmWuflHGQjjzgVWT0co5kRN4CBSQdavoOV+rW8O0W\nfI9JVfKSYZSnsrFmZ/c9GyjQpb/RLYdqroCDsTl0/2kSQiLU7dMEMl463WLXi6Qi\n/92F7mkFDhfH4rL5bAkkmaMEEvyVGaGe3zfqlLQvbiuk3GMJjV9tSKKJiwC6a7J3\n1OD+AYu6rUXA8h+mAdFfLpZ9368YPY+fbP/fu60cOUL0GFlEGFKEX50psW5MmSO6\n13fhfENPVHwp+6ds7xKsnSFdSSBxW37xQXSLk9dIKZmr7+BZL7P5GEArXJorFli6\na+6nc67+XwiHA1byX5fEiuhND5F2eNrAIK1yqrNWsrPq/KnsIpHBFjzaJpR7nofV\n6KFtAPCU\n=e4Be\n-----END PGP SIGNATURE-----", "payload": "tree f8260e803616063059c7c76e4b7dcb029912632a\nparent 8739e45bef31493053816f350a268245b4c0b787\nauthor Andy Wang <cbeuw.andy@gmail.com> 1652489121 +0100\ncommitter Andy Wang <cbeuw.andy@gmail.com> 1654539353 +0100\n\nMove buffered functions into their own ext trait\n"}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/335667c7749930f9f30e7045bc29690dd81a85f8", "html_url": "https://github.com/rust-lang/rust/commit/335667c7749930f9f30e7045bc29690dd81a85f8", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/335667c7749930f9f30e7045bc29690dd81a85f8/comments", "author": {"login": "cbeuw", "id": 7034308, "node_id": "MDQ6VXNlcjcwMzQzMDg=", "avatar_url": "https://avatars.githubusercontent.com/u/7034308?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cbeuw", "html_url": "https://github.com/cbeuw", "followers_url": "https://api.github.com/users/cbeuw/followers", "following_url": "https://api.github.com/users/cbeuw/following{/other_user}", "gists_url": "https://api.github.com/users/cbeuw/gists{/gist_id}", "starred_url": "https://api.github.com/users/cbeuw/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cbeuw/subscriptions", "organizations_url": "https://api.github.com/users/cbeuw/orgs", "repos_url": "https://api.github.com/users/cbeuw/repos", "events_url": "https://api.github.com/users/cbeuw/events{/privacy}", "received_events_url": "https://api.github.com/users/cbeuw/received_events", "type": "User", "site_admin": false}, "committer": {"login": "cbeuw", "id": 7034308, "node_id": "MDQ6VXNlcjcwMzQzMDg=", "avatar_url": "https://avatars.githubusercontent.com/u/7034308?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cbeuw", "html_url": "https://github.com/cbeuw", "followers_url": "https://api.github.com/users/cbeuw/followers", "following_url": "https://api.github.com/users/cbeuw/following{/other_user}", "gists_url": "https://api.github.com/users/cbeuw/gists{/gist_id}", "starred_url": "https://api.github.com/users/cbeuw/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cbeuw/subscriptions", "organizations_url": "https://api.github.com/users/cbeuw/orgs", "repos_url": "https://api.github.com/users/cbeuw/repos", "events_url": "https://api.github.com/users/cbeuw/events{/privacy}", "received_events_url": "https://api.github.com/users/cbeuw/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "8739e45bef31493053816f350a268245b4c0b787", "url": "https://api.github.com/repos/rust-lang/rust/commits/8739e45bef31493053816f350a268245b4c0b787", "html_url": "https://github.com/rust-lang/rust/commit/8739e45bef31493053816f350a268245b4c0b787"}], "stats": {"total": 213, "additions": 128, "deletions": 85}, "files": [{"sha": "22b72dadade7487d5df99be243537ad30f8f63c9", "filename": "src/concurrency/data_race.rs", "status": "modified", "additions": 9, "deletions": 80, "changes": 89, "blob_url": "https://github.com/rust-lang/rust/blob/335667c7749930f9f30e7045bc29690dd81a85f8/src%2Fconcurrency%2Fdata_race.rs", "raw_url": "https://github.com/rust-lang/rust/raw/335667c7749930f9f30e7045bc29690dd81a85f8/src%2Fconcurrency%2Fdata_race.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Fconcurrency%2Fdata_race.rs?ref=335667c7749930f9f30e7045bc29690dd81a85f8", "patch": "@@ -51,14 +51,15 @@ use std::{\n     mem,\n };\n \n-use rustc_const_eval::interpret::alloc_range;\n use rustc_data_structures::fx::{FxHashMap, FxHashSet};\n use rustc_index::vec::{Idx, IndexVec};\n use rustc_middle::{mir, ty::layout::TyAndLayout};\n use rustc_target::abi::Size;\n \n use crate::*;\n \n+use super::weak_memory::EvalContextExt as _;\n+\n pub type AllocExtra = VClockAlloc;\n \n /// Valid atomic read-write operations, alias of atomic::Ordering (not non-exhaustive).\n@@ -517,29 +518,9 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: MiriEvalContextExt<'mir, 'tcx> {\n         // the *value* (including the associated provenance if this is an AtomicPtr) at this location.\n         // Only metadata on the location itself is used.\n         let scalar = this.allow_data_races_ref(move |this| this.read_scalar(&place.into()))?;\n-\n-        if let Some(global) = &this.machine.data_race {\n-            let (alloc_id, base_offset, ..) = this.ptr_get_alloc_id(place.ptr)?;\n-            if let Some(alloc_buffers) = this.get_alloc_extra(alloc_id)?.weak_memory.as_ref() {\n-                if atomic == AtomicReadOp::SeqCst {\n-                    global.sc_read();\n-                }\n-                let mut rng = this.machine.rng.borrow_mut();\n-                let buffer =\n-                    alloc_buffers.get_store_buffer(alloc_range(base_offset, place.layout.size));\n-                let loaded = buffer.buffered_read(\n-                    global,\n-                    atomic == AtomicReadOp::SeqCst,\n-                    &mut *rng,\n-                    || this.validate_atomic_load(place, atomic),\n-                )?;\n-\n-                return Ok(loaded.unwrap_or(scalar));\n-            }\n-        }\n-\n-        this.validate_atomic_load(place, atomic)?;\n-        Ok(scalar)\n+        this.buffered_atomic_read(place, atomic, scalar, || {\n+            this.validate_atomic_load(place, atomic)\n+        })\n     }\n \n     /// Perform an atomic write operation at the memory location.\n@@ -551,23 +532,8 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: MiriEvalContextExt<'mir, 'tcx> {\n     ) -> InterpResult<'tcx> {\n         let this = self.eval_context_mut();\n         this.allow_data_races_mut(move |this| this.write_scalar(val, &(*dest).into()))?;\n-\n         this.validate_atomic_store(dest, atomic)?;\n-        let (alloc_id, base_offset, ..) = this.ptr_get_alloc_id(dest.ptr)?;\n-        if let (\n-            crate::AllocExtra { weak_memory: Some(alloc_buffers), .. },\n-            crate::Evaluator { data_race: Some(global), .. },\n-        ) = this.get_alloc_extra_mut(alloc_id)?\n-        {\n-            if atomic == AtomicWriteOp::SeqCst {\n-                global.sc_write();\n-            }\n-            let buffer =\n-                alloc_buffers.get_store_buffer_mut(alloc_range(base_offset, dest.layout.size));\n-            buffer.buffered_write(val, global, atomic == AtomicWriteOp::SeqCst)?;\n-        }\n-\n-        Ok(())\n+        this.buffered_atomic_write(val, dest, atomic)\n     }\n \n     /// Perform an atomic operation on a memory location.\n@@ -695,50 +661,13 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: MiriEvalContextExt<'mir, 'tcx> {\n             // in the modification order.\n             // Since `old` is only a value and not the store element, we need to separately\n             // find it in our store buffer and perform load_impl on it.\n-            if let Some(global) = &this.machine.data_race {\n-                if fail == AtomicReadOp::SeqCst {\n-                    global.sc_read();\n-                }\n-                let size = place.layout.size;\n-                let (alloc_id, base_offset, ..) = this.ptr_get_alloc_id(place.ptr)?;\n-                if let Some(alloc_buffers) = this.get_alloc_extra(alloc_id)?.weak_memory.as_ref() {\n-                    if global.multi_threaded.get() {\n-                        let buffer = alloc_buffers.get_store_buffer(alloc_range(base_offset, size));\n-                        buffer.read_from_last_store(global);\n-                    }\n-                }\n-            }\n+            this.perform_read_on_buffered_latest(place, fail)?;\n         }\n \n         // Return the old value.\n         Ok(res)\n     }\n \n-    fn buffered_atomic_rmw(\n-        &mut self,\n-        new_val: ScalarMaybeUninit<Tag>,\n-        place: &MPlaceTy<'tcx, Tag>,\n-        atomic: AtomicRwOp,\n-    ) -> InterpResult<'tcx> {\n-        let this = self.eval_context_mut();\n-        let (alloc_id, base_offset, ..) = this.ptr_get_alloc_id(place.ptr)?;\n-        if let (\n-            crate::AllocExtra { weak_memory: Some(alloc_buffers), .. },\n-            crate::Evaluator { data_race: Some(global), .. },\n-        ) = this.get_alloc_extra_mut(alloc_id)?\n-        {\n-            if atomic == AtomicRwOp::SeqCst {\n-                global.sc_read();\n-                global.sc_write();\n-            }\n-            let range = alloc_range(base_offset, place.layout.size);\n-            let buffer = alloc_buffers.get_store_buffer_mut(range);\n-            buffer.read_from_last_store(global);\n-            buffer.buffered_write(new_val, global, atomic == AtomicRwOp::SeqCst)?;\n-        }\n-        Ok(())\n-    }\n-\n     /// Update the data-race detector for an atomic read occurring at the\n     /// associated memory-place and on the current thread.\n     fn validate_atomic_load(\n@@ -1572,13 +1501,13 @@ impl GlobalState {\n     }\n \n     // SC ATOMIC STORE rule in the paper.\n-    fn sc_write(&self) {\n+    pub(super) fn sc_write(&self) {\n         let (index, clocks) = self.current_thread_state();\n         self.last_sc_write.borrow_mut().set_at_index(&clocks.clock, index);\n     }\n \n     // SC ATOMIC READ rule in the paper.\n-    fn sc_read(&self) {\n+    pub(super) fn sc_read(&self) {\n         let (.., mut clocks) = self.current_thread_state_mut();\n         clocks.read_seqcst.join(&self.last_sc_fence.borrow());\n     }"}, {"sha": "fba7d18cdf078dc772aae3a5f78278e5a8c6bd17", "filename": "src/concurrency/weak_memory.rs", "status": "modified", "additions": 119, "deletions": 5, "changes": 124, "blob_url": "https://github.com/rust-lang/rust/blob/335667c7749930f9f30e7045bc29690dd81a85f8/src%2Fconcurrency%2Fweak_memory.rs", "raw_url": "https://github.com/rust-lang/rust/raw/335667c7749930f9f30e7045bc29690dd81a85f8/src%2Fconcurrency%2Fweak_memory.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Fconcurrency%2Fweak_memory.rs?ref=335667c7749930f9f30e7045bc29690dd81a85f8", "patch": "@@ -57,10 +57,12 @@ use std::{\n     collections::VecDeque,\n };\n \n-use rustc_const_eval::interpret::{AllocRange, InterpResult, ScalarMaybeUninit};\n+use rustc_const_eval::interpret::{\n+    alloc_range, AllocRange, InterpResult, MPlaceTy, ScalarMaybeUninit,\n+};\n use rustc_data_structures::fx::FxHashMap;\n \n-use crate::{Tag, VClock, VTimestamp, VectorIdx};\n+use crate::{AtomicReadOp, AtomicRwOp, AtomicWriteOp, Tag, VClock, VTimestamp, VectorIdx};\n \n use super::{\n     allocation_map::{AccessType, AllocationMap},\n@@ -113,7 +115,7 @@ impl StoreBufferAlloc {\n     }\n \n     /// Gets a store buffer associated with an atomic object in this allocation\n-    pub(super) fn get_store_buffer(&self, range: AllocRange) -> Ref<'_, StoreBuffer> {\n+    fn get_store_buffer(&self, range: AllocRange) -> Ref<'_, StoreBuffer> {\n         let access_type = self.store_buffer.borrow().access_type(range);\n         let index = match access_type {\n             AccessType::PerfectlyOverlapping(index) => index,\n@@ -144,7 +146,7 @@ impl StoreBufferAlloc {\n     }\n \n     /// Gets a mutable store buffer associated with an atomic object in this allocation\n-    pub(super) fn get_store_buffer_mut(&mut self, range: AllocRange) -> &mut StoreBuffer {\n+    fn get_store_buffer_mut(&mut self, range: AllocRange) -> &mut StoreBuffer {\n         let buffer = self.store_buffer.get_mut();\n         let access_type = buffer.access_type(range);\n         let index = match access_type {\n@@ -201,7 +203,7 @@ impl<'mir, 'tcx: 'mir> StoreBuffer {\n             self.fetch_store(is_seqcst, &clocks, &mut *rng)\n         };\n \n-        // Unlike in write_scalar_atomic, thread clock updates have to be done\n+        // Unlike in buffered_atomic_write, thread clock updates have to be done\n         // after we've picked a store element from the store buffer, as presented\n         // in ATOMIC LOAD rule of the paper. This is because fetch_store\n         // requires access to ThreadClockSet.clock, which is updated by the race detector\n@@ -353,3 +355,115 @@ impl StoreElement {\n         self.val\n     }\n }\n+\n+impl<'mir, 'tcx: 'mir> EvalContextExt<'mir, 'tcx> for crate::MiriEvalContext<'mir, 'tcx> {}\n+pub(super) trait EvalContextExt<'mir, 'tcx: 'mir>:\n+    crate::MiriEvalContextExt<'mir, 'tcx>\n+{\n+    fn buffered_atomic_rmw(\n+        &mut self,\n+        new_val: ScalarMaybeUninit<Tag>,\n+        place: &MPlaceTy<'tcx, Tag>,\n+        atomic: AtomicRwOp,\n+    ) -> InterpResult<'tcx> {\n+        let this = self.eval_context_mut();\n+        let (alloc_id, base_offset, ..) = this.ptr_get_alloc_id(place.ptr)?;\n+        if let (\n+            crate::AllocExtra { weak_memory: Some(alloc_buffers), .. },\n+            crate::Evaluator { data_race: Some(global), .. },\n+        ) = this.get_alloc_extra_mut(alloc_id)?\n+        {\n+            if atomic == AtomicRwOp::SeqCst {\n+                global.sc_read();\n+                global.sc_write();\n+            }\n+            let range = alloc_range(base_offset, place.layout.size);\n+            let buffer = alloc_buffers.get_store_buffer_mut(range);\n+            buffer.read_from_last_store(global);\n+            buffer.buffered_write(new_val, global, atomic == AtomicRwOp::SeqCst)?;\n+        }\n+        Ok(())\n+    }\n+\n+    fn buffered_atomic_read(\n+        &self,\n+        place: &MPlaceTy<'tcx, Tag>,\n+        atomic: AtomicReadOp,\n+        latest_in_mo: ScalarMaybeUninit<Tag>,\n+        validate: impl FnOnce() -> InterpResult<'tcx>,\n+    ) -> InterpResult<'tcx, ScalarMaybeUninit<Tag>> {\n+        let this = self.eval_context_ref();\n+        if let Some(global) = &this.machine.data_race {\n+            let (alloc_id, base_offset, ..) = this.ptr_get_alloc_id(place.ptr)?;\n+            if let Some(alloc_buffers) = this.get_alloc_extra(alloc_id)?.weak_memory.as_ref() {\n+                if atomic == AtomicReadOp::SeqCst {\n+                    global.sc_read();\n+                }\n+                let mut rng = this.machine.rng.borrow_mut();\n+                let buffer =\n+                    alloc_buffers.get_store_buffer(alloc_range(base_offset, place.layout.size));\n+                let loaded = buffer.buffered_read(\n+                    global,\n+                    atomic == AtomicReadOp::SeqCst,\n+                    &mut *rng,\n+                    validate,\n+                )?;\n+\n+                return Ok(loaded.unwrap_or(latest_in_mo));\n+            }\n+        }\n+\n+        // Race detector or weak memory disabled, simply read the latest value\n+        validate()?;\n+        Ok(latest_in_mo)\n+    }\n+\n+    fn buffered_atomic_write(\n+        &mut self,\n+        val: ScalarMaybeUninit<Tag>,\n+        dest: &MPlaceTy<'tcx, Tag>,\n+        atomic: AtomicWriteOp,\n+    ) -> InterpResult<'tcx> {\n+        let this = self.eval_context_mut();\n+        let (alloc_id, base_offset, ..) = this.ptr_get_alloc_id(dest.ptr)?;\n+        if let (\n+            crate::AllocExtra { weak_memory: Some(alloc_buffers), .. },\n+            crate::Evaluator { data_race: Some(global), .. },\n+        ) = this.get_alloc_extra_mut(alloc_id)?\n+        {\n+            if atomic == AtomicWriteOp::SeqCst {\n+                global.sc_write();\n+            }\n+            let buffer =\n+                alloc_buffers.get_store_buffer_mut(alloc_range(base_offset, dest.layout.size));\n+            buffer.buffered_write(val, global, atomic == AtomicWriteOp::SeqCst)?;\n+        }\n+\n+        // Caller should've written to dest with the vanilla scalar write, we do nothing here\n+        Ok(())\n+    }\n+\n+    /// Caller should never need to consult the store buffer for the latest value.\n+    /// This function is used exclusively for failed atomic_compare_exchange_scalar\n+    /// to perform load_impl on the latest store element\n+    fn perform_read_on_buffered_latest(\n+        &self,\n+        place: &MPlaceTy<'tcx, Tag>,\n+        atomic: AtomicReadOp,\n+    ) -> InterpResult<'tcx> {\n+        let this = self.eval_context_ref();\n+\n+        if let Some(global) = &this.machine.data_race {\n+            if atomic == AtomicReadOp::SeqCst {\n+                global.sc_read();\n+            }\n+            let size = place.layout.size;\n+            let (alloc_id, base_offset, ..) = this.ptr_get_alloc_id(place.ptr)?;\n+            if let Some(alloc_buffers) = this.get_alloc_extra(alloc_id)?.weak_memory.as_ref() {\n+                let buffer = alloc_buffers.get_store_buffer(alloc_range(base_offset, size));\n+                buffer.read_from_last_store(global);\n+            }\n+        }\n+        Ok(())\n+    }\n+}"}]}