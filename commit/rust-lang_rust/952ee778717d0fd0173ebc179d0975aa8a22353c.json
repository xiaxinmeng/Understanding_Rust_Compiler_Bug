{"sha": "952ee778717d0fd0173ebc179d0975aa8a22353c", "node_id": "MDY6Q29tbWl0NzI0NzEyOjk1MmVlNzc4NzE3ZDBmZDAxNzNlYmMxNzlkMDk3NWFhOGEyMjM1M2M=", "commit": {"author": {"name": "Mazdak Farrokhzad", "email": "twingoow@gmail.com", "date": "2019-07-06T00:38:01Z"}, "committer": {"name": "GitHub", "email": "noreply@github.com", "date": "2019-07-06T00:38:01Z"}, "message": "Rollup merge of #62329 - matklad:no-peeking, r=petrochenkov\n\nRemove support for 1-token lookahead from the lexer\n\n`StringReader` maintained `peek_token` and `peek_span_src_raw` for look ahead.\n\n`peek_token` was used only by rustdoc syntax coloring. After moving peeking logic into highlighter, I was able to remove `peek_token` from the lexer. I tried to use `iter::Peekable`, but that wasn't as pretty as I hoped, due to buffered fatal errors. So I went with hand-rolled peeking.\n\nAfter that I've noticed that the only peeking behavior left was for raw tokens to test tt jointness. I've rewritten it in terms of trivia tokens, and not just spans.\n\nAfter that it became possible to simplify the awkward constructor of the lexer, which could return `Err` if the first peeked token contained error.", "tree": {"sha": "92a4ef046603fb80220cde9acaedaf7ee94361e6", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/92a4ef046603fb80220cde9acaedaf7ee94361e6"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/952ee778717d0fd0173ebc179d0975aa8a22353c", "comment_count": 0, "verification": {"verified": true, "reason": "valid", "signature": "-----BEGIN PGP SIGNATURE-----\n\nwsBcBAABCAAQBQJdH+1pCRBK7hj4Ov3rIwAAdHIIAHbUZ2u0Pu9KpHl5g8O7b5F+\nyKkdvt79xTy3ymE8h9X7wRysXiaf8CucSHuQWt2TBSu72RIYveiP9J2rH1bdF8OK\nIxSyqWEmXA67/fMfw4VbK1yig3GO9U1YIRC702d77iaQ/wPJPPW4HzBBNe9J6vu+\nPuNcJ7e2d7KC3rlU988Ooj4Nn8Hy45oy4Y54BQBznrd3opQi0LTe3o4kb5oXSWd1\nmCz4vtrORlaiTieVIiwboIL31gT2hu5BEM1Rssgu2IGtTjyhRJ4nColYpIeFbmic\nI9XpfDV1jP1SG8LhYLcF45JhZkn/KSlw/rsaOKHT4kH7GU772zDSC9AMdUaXo4M=\n=/mWG\n-----END PGP SIGNATURE-----\n", "payload": "tree 92a4ef046603fb80220cde9acaedaf7ee94361e6\nparent 3c4a6c860609fbf135fdd92996a65f14c3eb00da\nparent 3e362a4800932186c7351972753ecdf715050983\nauthor Mazdak Farrokhzad <twingoow@gmail.com> 1562373481 +0200\ncommitter GitHub <noreply@github.com> 1562373481 +0200\n\nRollup merge of #62329 - matklad:no-peeking, r=petrochenkov\n\nRemove support for 1-token lookahead from the lexer\n\n`StringReader` maintained `peek_token` and `peek_span_src_raw` for look ahead.\n\n`peek_token` was used only by rustdoc syntax coloring. After moving peeking logic into highlighter, I was able to remove `peek_token` from the lexer. I tried to use `iter::Peekable`, but that wasn't as pretty as I hoped, due to buffered fatal errors. So I went with hand-rolled peeking.\n\nAfter that I've noticed that the only peeking behavior left was for raw tokens to test tt jointness. I've rewritten it in terms of trivia tokens, and not just spans.\n\nAfter that it became possible to simplify the awkward constructor of the lexer, which could return `Err` if the first peeked token contained error.\n"}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/952ee778717d0fd0173ebc179d0975aa8a22353c", "html_url": "https://github.com/rust-lang/rust/commit/952ee778717d0fd0173ebc179d0975aa8a22353c", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/952ee778717d0fd0173ebc179d0975aa8a22353c/comments", "author": {"login": "Centril", "id": 855702, "node_id": "MDQ6VXNlcjg1NTcwMg==", "avatar_url": "https://avatars.githubusercontent.com/u/855702?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Centril", "html_url": "https://github.com/Centril", "followers_url": "https://api.github.com/users/Centril/followers", "following_url": "https://api.github.com/users/Centril/following{/other_user}", "gists_url": "https://api.github.com/users/Centril/gists{/gist_id}", "starred_url": "https://api.github.com/users/Centril/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Centril/subscriptions", "organizations_url": "https://api.github.com/users/Centril/orgs", "repos_url": "https://api.github.com/users/Centril/repos", "events_url": "https://api.github.com/users/Centril/events{/privacy}", "received_events_url": "https://api.github.com/users/Centril/received_events", "type": "User", "site_admin": false}, "committer": {"login": "web-flow", "id": 19864447, "node_id": "MDQ6VXNlcjE5ODY0NDQ3", "avatar_url": "https://avatars.githubusercontent.com/u/19864447?v=4", "gravatar_id": "", "url": "https://api.github.com/users/web-flow", "html_url": "https://github.com/web-flow", "followers_url": "https://api.github.com/users/web-flow/followers", "following_url": "https://api.github.com/users/web-flow/following{/other_user}", "gists_url": "https://api.github.com/users/web-flow/gists{/gist_id}", "starred_url": "https://api.github.com/users/web-flow/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/web-flow/subscriptions", "organizations_url": "https://api.github.com/users/web-flow/orgs", "repos_url": "https://api.github.com/users/web-flow/repos", "events_url": "https://api.github.com/users/web-flow/events{/privacy}", "received_events_url": "https://api.github.com/users/web-flow/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "3c4a6c860609fbf135fdd92996a65f14c3eb00da", "url": "https://api.github.com/repos/rust-lang/rust/commits/3c4a6c860609fbf135fdd92996a65f14c3eb00da", "html_url": "https://github.com/rust-lang/rust/commit/3c4a6c860609fbf135fdd92996a65f14c3eb00da"}, {"sha": "3e362a4800932186c7351972753ecdf715050983", "url": "https://api.github.com/repos/rust-lang/rust/commits/3e362a4800932186c7351972753ecdf715050983", "html_url": "https://github.com/rust-lang/rust/commit/3e362a4800932186c7351972753ecdf715050983"}], "stats": {"total": 309, "additions": 129, "deletions": 180}, "files": [{"sha": "fb9919d777db17da75d88f1f78eb2a3806291840", "filename": "src/librustc_save_analysis/span_utils.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/952ee778717d0fd0173ebc179d0975aa8a22353c/src%2Flibrustc_save_analysis%2Fspan_utils.rs", "raw_url": "https://github.com/rust-lang/rust/raw/952ee778717d0fd0173ebc179d0975aa8a22353c/src%2Flibrustc_save_analysis%2Fspan_utils.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_save_analysis%2Fspan_utils.rs?ref=952ee778717d0fd0173ebc179d0975aa8a22353c", "patch": "@@ -53,7 +53,7 @@ impl<'a> SpanUtils<'a> {\n     pub fn sub_span_of_token(&self, span: Span, tok: TokenKind) -> Option<Span> {\n         let mut toks = self.retokenise_span(span);\n         loop {\n-            let next = toks.real_token();\n+            let next = toks.next_token();\n             if next == token::Eof {\n                 return None;\n             }"}, {"sha": "8132074d6e0e7dd1fc307d6e61b929ec1a2de66e", "filename": "src/librustdoc/html/highlight.rs", "status": "modified", "additions": 31, "deletions": 20, "changes": 51, "blob_url": "https://github.com/rust-lang/rust/blob/952ee778717d0fd0173ebc179d0975aa8a22353c/src%2Flibrustdoc%2Fhtml%2Fhighlight.rs", "raw_url": "https://github.com/rust-lang/rust/raw/952ee778717d0fd0173ebc179d0975aa8a22353c/src%2Flibrustdoc%2Fhtml%2Fhighlight.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustdoc%2Fhtml%2Fhighlight.rs?ref=952ee778717d0fd0173ebc179d0975aa8a22353c", "patch": "@@ -38,17 +38,17 @@ pub fn render_with_highlighting(\n         FileName::Custom(String::from(\"rustdoc-highlighting\")),\n         src.to_owned(),\n     );\n-    let highlight_result =\n-        lexer::StringReader::new_or_buffered_errs(&sess, fm, None).and_then(|lexer| {\n-            let mut classifier = Classifier::new(lexer, sess.source_map());\n-\n-            let mut highlighted_source = vec![];\n-            if classifier.write_source(&mut highlighted_source).is_err() {\n-                Err(classifier.lexer.buffer_fatal_errors())\n-            } else {\n-                Ok(String::from_utf8_lossy(&highlighted_source).into_owned())\n-            }\n-        });\n+    let highlight_result = {\n+        let lexer = lexer::StringReader::new(&sess, fm, None);\n+        let mut classifier = Classifier::new(lexer, sess.source_map());\n+\n+        let mut highlighted_source = vec![];\n+        if classifier.write_source(&mut highlighted_source).is_err() {\n+            Err(classifier.lexer.buffer_fatal_errors())\n+        } else {\n+            Ok(String::from_utf8_lossy(&highlighted_source).into_owned())\n+        }\n+    };\n \n     match highlight_result {\n         Ok(highlighted_source) => {\n@@ -79,6 +79,7 @@ pub fn render_with_highlighting(\n /// each span of text in sequence.\n struct Classifier<'a> {\n     lexer: lexer::StringReader<'a>,\n+    peek_token: Option<Token>,\n     source_map: &'a SourceMap,\n \n     // State of the classifier.\n@@ -178,6 +179,7 @@ impl<'a> Classifier<'a> {\n     fn new(lexer: lexer::StringReader<'a>, source_map: &'a SourceMap) -> Classifier<'a> {\n         Classifier {\n             lexer,\n+            peek_token: None,\n             source_map,\n             in_attribute: false,\n             in_macro: false,\n@@ -187,10 +189,19 @@ impl<'a> Classifier<'a> {\n \n     /// Gets the next token out of the lexer.\n     fn try_next_token(&mut self) -> Result<Token, HighlightError> {\n-        match self.lexer.try_next_token() {\n-            Ok(token) => Ok(token),\n-            Err(_) => Err(HighlightError::LexError),\n+        if let Some(token) = self.peek_token.take() {\n+            return Ok(token);\n+        }\n+        self.lexer.try_next_token().map_err(|()| HighlightError::LexError)\n+    }\n+\n+    fn peek(&mut self) -> Result<&Token, HighlightError> {\n+        if self.peek_token.is_none() {\n+            self.peek_token = Some(\n+                self.lexer.try_next_token().map_err(|()| HighlightError::LexError)?\n+            );\n         }\n+        Ok(self.peek_token.as_ref().unwrap())\n     }\n \n     /// Exhausts the `lexer` writing the output into `out`.\n@@ -234,7 +245,7 @@ impl<'a> Classifier<'a> {\n             // reference or dereference operator or a reference or pointer type, instead of the\n             // bit-and or multiplication operator.\n             token::BinOp(token::And) | token::BinOp(token::Star)\n-                if self.lexer.peek() != &token::Whitespace => Class::RefKeyWord,\n+                if self.peek()? != &token::Whitespace => Class::RefKeyWord,\n \n             // Consider this as part of a macro invocation if there was a\n             // leading identifier.\n@@ -257,7 +268,7 @@ impl<'a> Classifier<'a> {\n             token::Question => Class::QuestionMark,\n \n             token::Dollar => {\n-                if self.lexer.peek().is_ident() {\n+                if self.peek()?.is_ident() {\n                     self.in_macro_nonterminal = true;\n                     Class::MacroNonTerminal\n                 } else {\n@@ -280,9 +291,9 @@ impl<'a> Classifier<'a> {\n                 // as an attribute.\n \n                 // Case 1: #![inner_attribute]\n-                if self.lexer.peek() == &token::Not {\n+                if self.peek()? == &token::Not {\n                     self.try_next_token()?; // NOTE: consumes `!` token!\n-                    if self.lexer.peek() == &token::OpenDelim(token::Bracket) {\n+                    if self.peek()? == &token::OpenDelim(token::Bracket) {\n                         self.in_attribute = true;\n                         out.enter_span(Class::Attribute)?;\n                     }\n@@ -292,7 +303,7 @@ impl<'a> Classifier<'a> {\n                 }\n \n                 // Case 2: #[outer_attribute]\n-                if self.lexer.peek() == &token::OpenDelim(token::Bracket) {\n+                if self.peek()? == &token::OpenDelim(token::Bracket) {\n                     self.in_attribute = true;\n                     out.enter_span(Class::Attribute)?;\n                 }\n@@ -341,7 +352,7 @@ impl<'a> Classifier<'a> {\n                         if self.in_macro_nonterminal {\n                             self.in_macro_nonterminal = false;\n                             Class::MacroNonTerminal\n-                        } else if self.lexer.peek() == &token::Not {\n+                        } else if self.peek()? == &token::Not {\n                             self.in_macro = true;\n                             Class::Macro\n                         } else {"}, {"sha": "0488153e7cb732e418d7a0ee36dab956d26c3a5c", "filename": "src/librustdoc/passes/check_code_block_syntax.rs", "status": "modified", "additions": 3, "deletions": 2, "changes": 5, "blob_url": "https://github.com/rust-lang/rust/blob/952ee778717d0fd0173ebc179d0975aa8a22353c/src%2Flibrustdoc%2Fpasses%2Fcheck_code_block_syntax.rs", "raw_url": "https://github.com/rust-lang/rust/raw/952ee778717d0fd0173ebc179d0975aa8a22353c/src%2Flibrustdoc%2Fpasses%2Fcheck_code_block_syntax.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustdoc%2Fpasses%2Fcheck_code_block_syntax.rs?ref=952ee778717d0fd0173ebc179d0975aa8a22353c", "patch": "@@ -32,7 +32,8 @@ impl<'a, 'tcx> SyntaxChecker<'a, 'tcx> {\n             dox[code_block.code].to_owned(),\n         );\n \n-        let errors = Lexer::new_or_buffered_errs(&sess, source_file, None).and_then(|mut lexer| {\n+        let errors = {\n+            let mut lexer = Lexer::new(&sess, source_file, None);\n             while let Ok(token::Token { kind, .. }) = lexer.try_next_token() {\n                 if kind == token::Eof {\n                     break;\n@@ -46,7 +47,7 @@ impl<'a, 'tcx> SyntaxChecker<'a, 'tcx> {\n             } else {\n                 Ok(())\n             }\n-        });\n+        };\n \n         if let Err(errors) = errors {\n             let mut diag = if let Some(sp) ="}, {"sha": "988f1aa38d926e814e9df631d7b4b1f118cb3e6a", "filename": "src/libsyntax/parse/lexer/comments.rs", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/rust-lang/rust/blob/952ee778717d0fd0173ebc179d0975aa8a22353c/src%2Flibsyntax%2Fparse%2Flexer%2Fcomments.rs", "raw_url": "https://github.com/rust-lang/rust/raw/952ee778717d0fd0173ebc179d0975aa8a22353c/src%2Flibsyntax%2Fparse%2Flexer%2Fcomments.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Flexer%2Fcomments.rs?ref=952ee778717d0fd0173ebc179d0975aa8a22353c", "patch": "@@ -268,7 +268,7 @@ fn read_block_comment(rdr: &mut StringReader<'_>,\n         while level > 0 {\n             debug!(\"=== block comment level {}\", level);\n             if rdr.is_eof() {\n-                rdr.fatal(\"unterminated block comment\").raise();\n+                rdr.fatal_span_(rdr.pos, rdr.pos, \"unterminated block comment\").raise();\n             }\n             if rdr.ch_is('\\n') {\n                 trim_whitespace_prefix_and_push_line(&mut lines, curr_line, col);\n@@ -346,7 +346,7 @@ pub fn gather_comments(sess: &ParseSess, path: FileName, srdr: &mut dyn Read) ->\n     srdr.read_to_string(&mut src).unwrap();\n     let cm = SourceMap::new(sess.source_map().path_mapping().clone());\n     let source_file = cm.new_source_file(path, src);\n-    let mut rdr = lexer::StringReader::new_raw(sess, source_file, None);\n+    let mut rdr = lexer::StringReader::new(sess, source_file, None);\n \n     let mut comments: Vec<Comment> = Vec::new();\n     let mut code_to_the_left = false; // Only code"}, {"sha": "1abbf0ff1eeb7798e24a36e30e6e0a9eb7daea74", "filename": "src/libsyntax/parse/lexer/mod.rs", "status": "modified", "additions": 74, "deletions": 144, "changes": 218, "blob_url": "https://github.com/rust-lang/rust/blob/952ee778717d0fd0173ebc179d0975aa8a22353c/src%2Flibsyntax%2Fparse%2Flexer%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/952ee778717d0fd0173ebc179d0975aa8a22353c/src%2Flibsyntax%2Fparse%2Flexer%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Flexer%2Fmod.rs?ref=952ee778717d0fd0173ebc179d0975aa8a22353c", "patch": "@@ -38,9 +38,6 @@ pub struct StringReader<'a> {\n     crate source_file: Lrc<syntax_pos::SourceFile>,\n     /// Stop reading src at this index.\n     crate end_src_index: usize,\n-    // cached:\n-    peek_token: Token,\n-    peek_span_src_raw: Span,\n     fatal_errs: Vec<DiagnosticBuilder<'a>>,\n     // cache a direct reference to the source text, so that we don't have to\n     // retrieve it via `self.source_file.src.as_ref().unwrap()` all the time.\n@@ -49,15 +46,59 @@ pub struct StringReader<'a> {\n }\n \n impl<'a> StringReader<'a> {\n-    fn mk_sp(&self, lo: BytePos, hi: BytePos) -> Span {\n-        self.mk_sp_and_raw(lo, hi).0\n+    pub fn new(sess: &'a ParseSess,\n+               source_file: Lrc<syntax_pos::SourceFile>,\n+               override_span: Option<Span>) -> Self {\n+        let mut sr = StringReader::new_internal(sess, source_file, override_span);\n+        sr.bump();\n+        sr\n+    }\n+\n+    pub fn retokenize(sess: &'a ParseSess, mut span: Span) -> Self {\n+        let begin = sess.source_map().lookup_byte_offset(span.lo());\n+        let end = sess.source_map().lookup_byte_offset(span.hi());\n+\n+        // Make the range zero-length if the span is invalid.\n+        if span.lo() > span.hi() || begin.sf.start_pos != end.sf.start_pos {\n+            span = span.shrink_to_lo();\n+        }\n+\n+        let mut sr = StringReader::new_internal(sess, begin.sf, None);\n+\n+        // Seek the lexer to the right byte range.\n+        sr.next_pos = span.lo();\n+        sr.end_src_index = sr.src_index(span.hi());\n+\n+        sr.bump();\n+\n+        sr\n     }\n \n-    fn mk_sp_and_raw(&self, lo: BytePos, hi: BytePos) -> (Span, Span) {\n-        let raw = Span::new(lo, hi, NO_EXPANSION);\n-        let real = self.override_span.unwrap_or(raw);\n+    fn new_internal(sess: &'a ParseSess, source_file: Lrc<syntax_pos::SourceFile>,\n+        override_span: Option<Span>) -> Self\n+    {\n+        if source_file.src.is_none() {\n+            sess.span_diagnostic.bug(&format!(\"Cannot lex source_file without source: {}\",\n+                                              source_file.name));\n+        }\n+\n+        let src = (*source_file.src.as_ref().unwrap()).clone();\n+\n+        StringReader {\n+            sess,\n+            next_pos: source_file.start_pos,\n+            pos: source_file.start_pos,\n+            ch: Some('\\n'),\n+            source_file,\n+            end_src_index: src.len(),\n+            src,\n+            fatal_errs: Vec::new(),\n+            override_span,\n+        }\n+    }\n \n-        (real, raw)\n+    fn mk_sp(&self, lo: BytePos, hi: BytePos) -> Span {\n+        self.override_span.unwrap_or_else(|| Span::new(lo, hi, NO_EXPANSION))\n     }\n \n     fn unwrap_or_abort(&mut self, res: Result<Token, ()>) -> Token {\n@@ -70,35 +111,32 @@ impl<'a> StringReader<'a> {\n         }\n     }\n \n-    fn next_token(&mut self) -> Token where Self: Sized {\n-        let res = self.try_next_token();\n-        self.unwrap_or_abort(res)\n-    }\n-\n-    /// Returns the next token. EFFECT: advances the string_reader.\n+    /// Returns the next token, including trivia like whitespace or comments.\n+    ///\n+    /// `Err(())` means that some errors were encountered, which can be\n+    /// retrieved using `buffer_fatal_errors`.\n     pub fn try_next_token(&mut self) -> Result<Token, ()> {\n         assert!(self.fatal_errs.is_empty());\n-        let ret_val = self.peek_token.take();\n-        self.advance_token()?;\n-        Ok(ret_val)\n-    }\n-\n-    fn try_real_token(&mut self) -> Result<Token, ()> {\n-        let mut t = self.try_next_token()?;\n-        loop {\n-            match t.kind {\n-                token::Whitespace | token::Comment | token::Shebang(_) => {\n-                    t = self.try_next_token()?;\n-                }\n-                _ => break,\n+        match self.scan_whitespace_or_comment() {\n+            Some(comment) => Ok(comment),\n+            None => {\n+                let (kind, start_pos, end_pos) = if self.is_eof() {\n+                    (token::Eof, self.source_file.end_pos, self.source_file.end_pos)\n+                } else {\n+                    let start_pos = self.pos;\n+                    (self.next_token_inner()?, start_pos, self.pos)\n+                };\n+                let span = self.mk_sp(start_pos, end_pos);\n+                Ok(Token::new(kind, span))\n             }\n         }\n-\n-        Ok(t)\n     }\n \n-    pub fn real_token(&mut self) -> Token {\n-        let res = self.try_real_token();\n+    /// Returns the next token, including trivia like whitespace or comments.\n+    ///\n+    /// Aborts in case of an error.\n+    pub fn next_token(&mut self) -> Token {\n+        let res = self.try_next_token();\n         self.unwrap_or_abort(res)\n     }\n \n@@ -120,10 +158,6 @@ impl<'a> StringReader<'a> {\n         FatalError.raise();\n     }\n \n-    fn fatal(&self, m: &str) -> FatalError {\n-        self.fatal_span(self.peek_token.span, m)\n-    }\n-\n     crate fn emit_fatal_errors(&mut self) {\n         for err in &mut self.fatal_errs {\n             err.emit();\n@@ -142,81 +176,6 @@ impl<'a> StringReader<'a> {\n         buffer\n     }\n \n-    pub fn peek(&self) -> &Token {\n-        &self.peek_token\n-    }\n-\n-    /// For comments.rs, which hackily pokes into next_pos and ch\n-    fn new_raw(sess: &'a ParseSess,\n-               source_file: Lrc<syntax_pos::SourceFile>,\n-               override_span: Option<Span>) -> Self {\n-        let mut sr = StringReader::new_raw_internal(sess, source_file, override_span);\n-        sr.bump();\n-\n-        sr\n-    }\n-\n-    fn new_raw_internal(sess: &'a ParseSess, source_file: Lrc<syntax_pos::SourceFile>,\n-        override_span: Option<Span>) -> Self\n-    {\n-        if source_file.src.is_none() {\n-            sess.span_diagnostic.bug(&format!(\"Cannot lex source_file without source: {}\",\n-                                              source_file.name));\n-        }\n-\n-        let src = (*source_file.src.as_ref().unwrap()).clone();\n-\n-        StringReader {\n-            sess,\n-            next_pos: source_file.start_pos,\n-            pos: source_file.start_pos,\n-            ch: Some('\\n'),\n-            source_file,\n-            end_src_index: src.len(),\n-            peek_token: Token::dummy(),\n-            peek_span_src_raw: syntax_pos::DUMMY_SP,\n-            src,\n-            fatal_errs: Vec::new(),\n-            override_span,\n-        }\n-    }\n-\n-    pub fn new_or_buffered_errs(sess: &'a ParseSess,\n-                                source_file: Lrc<syntax_pos::SourceFile>,\n-                                override_span: Option<Span>) -> Result<Self, Vec<Diagnostic>> {\n-        let mut sr = StringReader::new_raw(sess, source_file, override_span);\n-        if sr.advance_token().is_err() {\n-            Err(sr.buffer_fatal_errors())\n-        } else {\n-            Ok(sr)\n-        }\n-    }\n-\n-    pub fn retokenize(sess: &'a ParseSess, mut span: Span) -> Self {\n-        let begin = sess.source_map().lookup_byte_offset(span.lo());\n-        let end = sess.source_map().lookup_byte_offset(span.hi());\n-\n-        // Make the range zero-length if the span is invalid.\n-        if span.lo() > span.hi() || begin.sf.start_pos != end.sf.start_pos {\n-            span = span.shrink_to_lo();\n-        }\n-\n-        let mut sr = StringReader::new_raw_internal(sess, begin.sf, None);\n-\n-        // Seek the lexer to the right byte range.\n-        sr.next_pos = span.lo();\n-        sr.end_src_index = sr.src_index(span.hi());\n-\n-        sr.bump();\n-\n-        if sr.advance_token().is_err() {\n-            sr.emit_fatal_errors();\n-            FatalError.raise();\n-        }\n-\n-        sr\n-    }\n-\n     #[inline]\n     fn ch_is(&self, c: char) -> bool {\n         self.ch == Some(c)\n@@ -269,30 +228,6 @@ impl<'a> StringReader<'a> {\n         self.sess.span_diagnostic.struct_span_fatal(self.mk_sp(from_pos, to_pos), &m[..])\n     }\n \n-    /// Advance peek_token to refer to the next token, and\n-    /// possibly update the interner.\n-    fn advance_token(&mut self) -> Result<(), ()> {\n-        match self.scan_whitespace_or_comment() {\n-            Some(comment) => {\n-                self.peek_span_src_raw = comment.span;\n-                self.peek_token = comment;\n-            }\n-            None => {\n-                let (kind, start_pos, end_pos) = if self.is_eof() {\n-                    (token::Eof, self.source_file.end_pos, self.source_file.end_pos)\n-                } else {\n-                    let start_pos = self.pos;\n-                    (self.next_token_inner()?, start_pos, self.pos)\n-                };\n-                let (real, raw) = self.mk_sp_and_raw(start_pos, end_pos);\n-                self.peek_token = Token::new(kind, real);\n-                self.peek_span_src_raw = raw;\n-            }\n-        }\n-\n-        Ok(())\n-    }\n-\n     #[inline]\n     fn src_index(&self, pos: BytePos) -> usize {\n         (pos - self.source_file.start_pos).to_usize()\n@@ -1462,12 +1397,7 @@ mod tests {\n                  teststr: String)\n                  -> StringReader<'a> {\n         let sf = sm.new_source_file(PathBuf::from(teststr.clone()).into(), teststr);\n-        let mut sr = StringReader::new_raw(sess, sf, None);\n-        if sr.advance_token().is_err() {\n-            sr.emit_fatal_errors();\n-            FatalError.raise();\n-        }\n-        sr\n+        StringReader::new(sess, sf, None)\n     }\n \n     #[test]\n@@ -1489,17 +1419,17 @@ mod tests {\n             assert_eq!(tok1.kind, tok2.kind);\n             assert_eq!(tok1.span, tok2.span);\n             assert_eq!(string_reader.next_token(), token::Whitespace);\n-            // the 'main' id is already read:\n-            assert_eq!(string_reader.pos.clone(), BytePos(28));\n             // read another token:\n             let tok3 = string_reader.next_token();\n+            assert_eq!(string_reader.pos.clone(), BytePos(28));\n             let tok4 = Token::new(\n                 mk_ident(\"main\"),\n                 Span::new(BytePos(24), BytePos(28), NO_EXPANSION),\n             );\n             assert_eq!(tok3.kind, tok4.kind);\n             assert_eq!(tok3.span, tok4.span);\n-            // the lparen is already read:\n+\n+            assert_eq!(string_reader.next_token(), token::OpenDelim(token::Paren));\n             assert_eq!(string_reader.pos.clone(), BytePos(29))\n         })\n     }"}, {"sha": "830fbec58ded952c109184007741491d7a17aaa2", "filename": "src/libsyntax/parse/lexer/tokentrees.rs", "status": "modified", "additions": 17, "deletions": 10, "changes": 27, "blob_url": "https://github.com/rust-lang/rust/blob/952ee778717d0fd0173ebc179d0975aa8a22353c/src%2Flibsyntax%2Fparse%2Flexer%2Ftokentrees.rs", "raw_url": "https://github.com/rust-lang/rust/raw/952ee778717d0fd0173ebc179d0975aa8a22353c/src%2Flibsyntax%2Fparse%2Flexer%2Ftokentrees.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Flexer%2Ftokentrees.rs?ref=952ee778717d0fd0173ebc179d0975aa8a22353c", "patch": "@@ -4,13 +4,14 @@ use crate::print::pprust::token_to_string;\n use crate::parse::lexer::{StringReader, UnmatchedBrace};\n use crate::parse::token::{self, Token};\n use crate::parse::PResult;\n-use crate::tokenstream::{DelimSpan, IsJoint::*, TokenStream, TokenTree, TreeAndJoint};\n+use crate::tokenstream::{DelimSpan, IsJoint::{self, *}, TokenStream, TokenTree, TreeAndJoint};\n \n impl<'a> StringReader<'a> {\n     crate fn into_token_trees(self) -> (PResult<'a, TokenStream>, Vec<UnmatchedBrace>) {\n         let mut tt_reader = TokenTreesReader {\n             string_reader: self,\n             token: Token::dummy(),\n+            joint_to_prev: Joint,\n             open_braces: Vec::new(),\n             unmatched_braces: Vec::new(),\n             matching_delim_spans: Vec::new(),\n@@ -24,6 +25,7 @@ impl<'a> StringReader<'a> {\n struct TokenTreesReader<'a> {\n     string_reader: StringReader<'a>,\n     token: Token,\n+    joint_to_prev: IsJoint,\n     /// Stack of open delimiters and their spans. Used for error message.\n     open_braces: Vec<(token::DelimToken, Span)>,\n     unmatched_braces: Vec<UnmatchedBrace>,\n@@ -203,21 +205,26 @@ impl<'a> TokenTreesReader<'a> {\n             },\n             _ => {\n                 let tt = TokenTree::Token(self.token.take());\n-                // Note that testing for joint-ness here is done via the raw\n-                // source span as the joint-ness is a property of the raw source\n-                // rather than wanting to take `override_span` into account.\n-                // Additionally, we actually check if the *next* pair of tokens\n-                // is joint, but this is equivalent to checking the current pair.\n-                let raw = self.string_reader.peek_span_src_raw;\n                 self.real_token();\n-                let is_joint = raw.hi() == self.string_reader.peek_span_src_raw.lo()\n-                    && self.token.is_op();\n+                let is_joint = self.joint_to_prev == Joint && self.token.is_op();\n                 Ok((tt, if is_joint { Joint } else { NonJoint }))\n             }\n         }\n     }\n \n     fn real_token(&mut self) {\n-        self.token = self.string_reader.real_token();\n+        self.joint_to_prev = Joint;\n+        loop {\n+            let token = self.string_reader.next_token();\n+            match token.kind {\n+                token::Whitespace | token::Comment | token::Shebang(_) => {\n+                    self.joint_to_prev = NonJoint;\n+                }\n+                _ => {\n+                    self.token = token;\n+                    return;\n+                },\n+            }\n+        }\n     }\n }"}, {"sha": "4c4551b1757acefd44615353243d9ae937f961ea", "filename": "src/libsyntax/parse/mod.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/952ee778717d0fd0173ebc179d0975aa8a22353c/src%2Flibsyntax%2Fparse%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/952ee778717d0fd0173ebc179d0975aa8a22353c/src%2Flibsyntax%2Fparse%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Fmod.rs?ref=952ee778717d0fd0173ebc179d0975aa8a22353c", "patch": "@@ -308,7 +308,7 @@ pub fn maybe_file_to_stream(\n     source_file: Lrc<SourceFile>,\n     override_span: Option<Span>,\n ) -> Result<(TokenStream, Vec<lexer::UnmatchedBrace>), Vec<Diagnostic>> {\n-    let srdr = lexer::StringReader::new_or_buffered_errs(sess, source_file, override_span)?;\n+    let srdr = lexer::StringReader::new(sess, source_file, override_span);\n     let (token_trees, unmatched_braces) = srdr.into_token_trees();\n \n     match token_trees {"}]}