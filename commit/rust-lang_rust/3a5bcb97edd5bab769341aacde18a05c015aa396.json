{"sha": "3a5bcb97edd5bab769341aacde18a05c015aa396", "node_id": "MDY6Q29tbWl0NzI0NzEyOjNhNWJjYjk3ZWRkNWJhYjc2OTM0MWFhY2RlMThhMDVjMDE1YWEzOTY=", "commit": {"author": {"name": "Ralf Jung", "email": "post@ralfj.de", "date": "2020-06-28T07:47:20Z"}, "committer": {"name": "Ralf Jung", "email": "post@ralfj.de", "date": "2020-06-28T09:28:46Z"}, "message": "move rwlock dequeuing to shared code, and use that code for Windows rwlocks", "tree": {"sha": "99d06d97bb7738d0613527feff0d403447643e03", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/99d06d97bb7738d0613527feff0d403447643e03"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/3a5bcb97edd5bab769341aacde18a05c015aa396", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/3a5bcb97edd5bab769341aacde18a05c015aa396", "html_url": "https://github.com/rust-lang/rust/commit/3a5bcb97edd5bab769341aacde18a05c015aa396", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/3a5bcb97edd5bab769341aacde18a05c015aa396/comments", "author": {"login": "RalfJung", "id": 330628, "node_id": "MDQ6VXNlcjMzMDYyOA==", "avatar_url": "https://avatars.githubusercontent.com/u/330628?v=4", "gravatar_id": "", "url": "https://api.github.com/users/RalfJung", "html_url": "https://github.com/RalfJung", "followers_url": "https://api.github.com/users/RalfJung/followers", "following_url": "https://api.github.com/users/RalfJung/following{/other_user}", "gists_url": "https://api.github.com/users/RalfJung/gists{/gist_id}", "starred_url": "https://api.github.com/users/RalfJung/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/RalfJung/subscriptions", "organizations_url": "https://api.github.com/users/RalfJung/orgs", "repos_url": "https://api.github.com/users/RalfJung/repos", "events_url": "https://api.github.com/users/RalfJung/events{/privacy}", "received_events_url": "https://api.github.com/users/RalfJung/received_events", "type": "User", "site_admin": false}, "committer": {"login": "RalfJung", "id": 330628, "node_id": "MDQ6VXNlcjMzMDYyOA==", "avatar_url": "https://avatars.githubusercontent.com/u/330628?v=4", "gravatar_id": "", "url": "https://api.github.com/users/RalfJung", "html_url": "https://github.com/RalfJung", "followers_url": "https://api.github.com/users/RalfJung/followers", "following_url": "https://api.github.com/users/RalfJung/following{/other_user}", "gists_url": "https://api.github.com/users/RalfJung/gists{/gist_id}", "starred_url": "https://api.github.com/users/RalfJung/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/RalfJung/subscriptions", "organizations_url": "https://api.github.com/users/RalfJung/orgs", "repos_url": "https://api.github.com/users/RalfJung/repos", "events_url": "https://api.github.com/users/RalfJung/events{/privacy}", "received_events_url": "https://api.github.com/users/RalfJung/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "a9dc2796cac8d49f67f6055d3fe3561a13f604b7", "url": "https://api.github.com/repos/rust-lang/rust/commits/a9dc2796cac8d49f67f6055d3fe3561a13f604b7", "html_url": "https://github.com/rust-lang/rust/commit/a9dc2796cac8d49f67f6055d3fe3561a13f604b7"}], "stats": {"total": 329, "additions": 164, "deletions": 165}, "files": [{"sha": "cce0ddc930df0f64bad9235849338273edb88e6f", "filename": "src/shims/posix/sync.rs", "status": "modified", "additions": 1, "deletions": 21, "changes": 22, "blob_url": "https://github.com/rust-lang/rust/blob/3a5bcb97edd5bab769341aacde18a05c015aa396/src%2Fshims%2Fposix%2Fsync.rs", "raw_url": "https://github.com/rust-lang/rust/raw/3a5bcb97edd5bab769341aacde18a05c015aa396/src%2Fshims%2Fposix%2Fsync.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Fshims%2Fposix%2Fsync.rs?ref=3a5bcb97edd5bab769341aacde18a05c015aa396", "patch": "@@ -1,6 +1,5 @@\n use std::convert::TryInto;\n use std::time::{Duration, SystemTime};\n-use std::ops::Not;\n \n use crate::*;\n use stacked_borrows::Tag;\n@@ -548,27 +547,8 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: crate::MiriEvalContextExt<'mir, 'tcx\n         let active_thread = this.get_active_thread();\n \n         if this.rwlock_reader_unlock(id, active_thread) {\n-            // The thread was a reader.\n-            if this.rwlock_is_locked(id).not() {\n-                // No more readers owning the lock. Give it to a writer if there\n-                // is any.\n-                this.rwlock_dequeue_and_lock_writer(id);\n-            }\n             Ok(0)\n-        } else if Some(active_thread) == this.rwlock_writer_unlock(id) {\n-            // The thread was a writer.\n-            //\n-            // We are prioritizing writers here against the readers. As a\n-            // result, not only readers can starve writers, but also writers can\n-            // starve readers.\n-            if this.rwlock_dequeue_and_lock_writer(id) {\n-                // Someone got the write lock, nice.\n-            } else {\n-                // Give the lock to all readers.\n-                while this.rwlock_dequeue_and_lock_reader(id) {\n-                    // Rinse and repeat.\n-                }\n-            }\n+        } else if this.rwlock_writer_unlock(id, active_thread) {\n             Ok(0)\n         } else {\n             throw_ub_format!(\"unlocked an rwlock that was not locked by the active thread\");"}, {"sha": "e8937bbb3085558915e8a178b54569e634475811", "filename": "src/shims/windows/foreign_items.rs", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "blob_url": "https://github.com/rust-lang/rust/blob/3a5bcb97edd5bab769341aacde18a05c015aa396/src%2Fshims%2Fwindows%2Fforeign_items.rs", "raw_url": "https://github.com/rust-lang/rust/raw/3a5bcb97edd5bab769341aacde18a05c015aa396/src%2Fshims%2Fwindows%2Fforeign_items.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Fshims%2Fwindows%2Fforeign_items.rs?ref=3a5bcb97edd5bab769341aacde18a05c015aa396", "patch": "@@ -257,7 +257,7 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: crate::MiriEvalContextExt<'mir, 'tcx\n \n             // Better error for attempts to create a thread\n             \"CreateThread\" => {\n-                throw_unsup_format!(\"Miri does not support threading\");\n+                throw_unsup_format!(\"Miri does not support concurrency on Windows\");\n             }\n \n             // Incomplete shims that we \"stub out\" just to get pre-main initialization code to work.\n@@ -292,7 +292,7 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: crate::MiriEvalContextExt<'mir, 'tcx\n             if this.frame().instance.to_string().starts_with(\"std::sys::windows::\") => {\n                 #[allow(non_snake_case)]\n                 let &[_lpCriticalSection] = check_arg_count(args)?;\n-                assert_eq!(this.get_total_thread_count(), 1, \"concurrency on Windows not supported\");\n+                assert_eq!(this.get_total_thread_count(), 1, \"concurrency on Windows is not supported\");\n                 // Nothing to do, not even a return value.\n                 // (Windows locks are reentrant, and we have only 1 thread,\n                 // so not doing any futher checks here is at least not incorrect.)\n@@ -301,7 +301,7 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: crate::MiriEvalContextExt<'mir, 'tcx\n             if this.frame().instance.to_string().starts_with(\"std::sys::windows::\") => {\n                 #[allow(non_snake_case)]\n                 let &[_lpCriticalSection] = check_arg_count(args)?;\n-                assert_eq!(this.get_total_thread_count(), 1, \"concurrency on Windows not supported\");\n+                assert_eq!(this.get_total_thread_count(), 1, \"concurrency on Windows is not supported\");\n                 // There is only one thread, so this always succeeds and returns TRUE.\n                 this.write_scalar(Scalar::from_i32(1), dest)?;\n             }"}, {"sha": "7bad3c08a598f499c62478cf30c964b253f0cc21", "filename": "src/shims/windows/sync.rs", "status": "modified", "additions": 54, "deletions": 80, "changes": 134, "blob_url": "https://github.com/rust-lang/rust/blob/3a5bcb97edd5bab769341aacde18a05c015aa396/src%2Fshims%2Fwindows%2Fsync.rs", "raw_url": "https://github.com/rust-lang/rust/raw/3a5bcb97edd5bab769341aacde18a05c015aa396/src%2Fshims%2Fwindows%2Fsync.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Fshims%2Fwindows%2Fsync.rs?ref=3a5bcb97edd5bab769341aacde18a05c015aa396", "patch": "@@ -1,19 +1,22 @@\n-use rustc_target::abi::Size;\n-\n use crate::*;\n \n // Locks are pointer-sized pieces of data, initialized to 0.\n-// We use them to count readers, with usize::MAX representing the write-locked state.\n+// We use the first 4 bytes to store the RwLockId.\n \n-fn deref_lock<'mir, 'tcx: 'mir>(\n+fn srwlock_get_or_create_id<'mir, 'tcx: 'mir>(\n     ecx: &mut MiriEvalContext<'mir, 'tcx>,\n     lock_op: OpTy<'tcx, Tag>,\n-) -> InterpResult<'tcx, MPlaceTy<'tcx, Tag>> {\n-    // `lock` is a pointer to `void*`; cast it to a pointer to `usize`.\n-    let lock = ecx.deref_operand(lock_op)?;\n-    let usize = ecx.machine.layouts.usize;\n-    assert_eq!(lock.layout.size, usize.size);\n-    Ok(lock.offset(Size::ZERO, MemPlaceMeta::None, usize, ecx)?)\n+) -> InterpResult<'tcx, RwLockId> {\n+    let id = ecx.read_scalar_at_offset(lock_op, 0, ecx.machine.layouts.u32)?.to_u32()?;\n+    if id == 0 {\n+        // 0 is a default value and also not a valid rwlock id. Need to allocate\n+        // a new rwlock.\n+        let id = ecx.rwlock_create();\n+        ecx.write_scalar_at_offset(lock_op, 0, id.to_u32_scalar(), ecx.machine.layouts.u32)?;\n+        Ok(id)\n+    } else {\n+        Ok(RwLockId::from_u32(id))\n+    }\n }\n \n impl<'mir, 'tcx> EvalContextExt<'mir, 'tcx> for crate::MiriEvalContext<'mir, 'tcx> {}\n@@ -24,17 +27,20 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: crate::MiriEvalContextExt<'mir, 'tcx\n         lock_op: OpTy<'tcx, Tag>,\n     ) -> InterpResult<'tcx> {\n         let this = self.eval_context_mut();\n-        assert_eq!(this.get_total_thread_count(), 1, \"concurrency on Windows is not supported\");\n-\n-        let lock = deref_lock(this, lock_op)?;\n-        let lock_val = this.read_scalar(lock.into())?.to_machine_usize(this)?;\n-        if lock_val == 0 {\n-            // Currently not locked. Lock it.\n-            let new_val = Scalar::from_machine_usize(this.machine_usize_max(), this);\n-            this.write_scalar(new_val, lock.into())?;\n+        let id = srwlock_get_or_create_id(this, lock_op)?;\n+        let active_thread = this.get_active_thread();\n+\n+        if this.rwlock_is_locked(id) {\n+            // Note: this will deadlock if the lock is already locked by this\n+            // thread in any way.\n+            //\n+            // FIXME: Detect and report the deadlock proactively. (We currently\n+            // report the deadlock only when no thread can continue execution,\n+            // but we could detect that this lock is already locked and report\n+            // an error.)\n+            this.rwlock_enqueue_and_block_writer(id, active_thread);\n         } else {\n-            // Lock is already held. This is a deadlock.\n-            throw_machine_stop!(TerminationInfo::Deadlock);\n+            this.rwlock_writer_lock(id, active_thread);\n         }\n \n         Ok(())\n@@ -46,18 +52,15 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: crate::MiriEvalContextExt<'mir, 'tcx\n         lock_op: OpTy<'tcx, Tag>,\n     ) -> InterpResult<'tcx, u8> {\n         let this = self.eval_context_mut();\n-        assert_eq!(this.get_total_thread_count(), 1, \"concurrency on Windows is not supported\");\n-\n-        let lock = deref_lock(this, lock_op)?;\n-        let lock_val = this.read_scalar(lock.into())?.to_machine_usize(this)?;\n-        if lock_val == 0 {\n-            // Currently not locked. Lock it.\n-            let new_val = this.machine_usize_max();\n-            this.write_scalar(Scalar::from_machine_usize(new_val, this), lock.into())?;\n-            Ok(1)\n-        } else {\n+        let id = srwlock_get_or_create_id(this, lock_op)?;\n+        let active_thread = this.get_active_thread();\n+\n+        if this.rwlock_is_locked(id) {\n             // Lock is already held.\n             Ok(0)\n+        } else {\n+            this.rwlock_writer_lock(id, active_thread);\n+            Ok(1)\n         }\n     }\n \n@@ -67,17 +70,12 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: crate::MiriEvalContextExt<'mir, 'tcx\n         lock_op: OpTy<'tcx, Tag>,\n     ) -> InterpResult<'tcx> {\n         let this = self.eval_context_mut();\n-        assert_eq!(this.get_total_thread_count(), 1, \"concurrency on Windows is not supported\");\n-\n-        let lock = deref_lock(this, lock_op)?;\n-        let lock_val = this.read_scalar(lock.into())?.to_machine_usize(this)?;\n-        if lock_val == this.machine_usize_max() {\n-            // Currently locked. Unlock it.\n-            let new_val = 0;\n-            this.write_scalar(Scalar::from_machine_usize(new_val, this), lock.into())?;\n-        } else {\n-            // Lock is not locked.\n-            throw_ub_format!(\"calling ReleaseSRWLockExclusive on an SRWLock that is not exclusively locked\");\n+        let id = srwlock_get_or_create_id(this, lock_op)?;\n+        let active_thread = this.get_active_thread();\n+\n+        if !this.rwlock_writer_unlock(id, active_thread) {\n+            // The docs do not say anything about this case, but it seems better to not allow it.\n+            throw_ub_format!(\"calling ReleaseSRWLockExclusive on an SRWLock that is not exclusively locked by the current thread\");\n         }\n \n         Ok(())\n@@ -89,21 +87,13 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: crate::MiriEvalContextExt<'mir, 'tcx\n         lock_op: OpTy<'tcx, Tag>,\n     ) -> InterpResult<'tcx> {\n         let this = self.eval_context_mut();\n-        assert_eq!(this.get_total_thread_count(), 1, \"concurrency on Windows is not supported\");\n+        let id = srwlock_get_or_create_id(this, lock_op)?;\n+        let active_thread = this.get_active_thread();\n \n-        let lock = deref_lock(this, lock_op)?;\n-        let lock_val = this.read_scalar(lock.into())?.to_machine_usize(this)?;\n-        if lock_val == this.machine_usize_max() {\n-            // Currently write locked. This is a deadlock.\n-            throw_machine_stop!(TerminationInfo::Deadlock);\n+        if this.rwlock_is_write_locked(id) {\n+            this.rwlock_enqueue_and_block_reader(id, active_thread);\n         } else {\n-            // Bump up read counter (cannot overflow as we just checkd against usize::MAX);\n-            let new_val = lock_val+1;\n-            // Make sure this does not reach the \"write locked\" flag.\n-            if new_val == this.machine_usize_max() {\n-                throw_unsup_format!(\"SRWLock read-acquired too many times\");\n-            }\n-            this.write_scalar(Scalar::from_machine_usize(new_val, this), lock.into())?;\n+            this.rwlock_reader_lock(id, active_thread);\n         }\n \n         Ok(())\n@@ -115,21 +105,13 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: crate::MiriEvalContextExt<'mir, 'tcx\n         lock_op: OpTy<'tcx, Tag>,\n     ) -> InterpResult<'tcx, u8> {\n         let this = self.eval_context_mut();\n-        assert_eq!(this.get_total_thread_count(), 1, \"concurrency on Windows is not supported\");\n+        let id = srwlock_get_or_create_id(this, lock_op)?;\n+        let active_thread = this.get_active_thread();\n \n-        let lock = deref_lock(this, lock_op)?;\n-        let lock_val = this.read_scalar(lock.into())?.to_machine_usize(this)?;\n-        if lock_val == this.machine_usize_max() {\n-            // Currently write locked.\n+        if this.rwlock_is_write_locked(id) {\n             Ok(0)\n         } else {\n-            // Bump up read counter (cannot overflow as we just checkd against usize::MAX);\n-            let new_val = lock_val+1;\n-            // Make sure this does not reach the \"write locked\" flag.\n-            if new_val == this.machine_usize_max() {\n-                throw_unsup_format!(\"SRWLock read-acquired too many times\");\n-            }\n-            this.write_scalar(Scalar::from_machine_usize(new_val, this), lock.into())?;\n+            this.rwlock_reader_lock(id, active_thread);\n             Ok(1)\n         }\n     }\n@@ -140,20 +122,12 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: crate::MiriEvalContextExt<'mir, 'tcx\n         lock_op: OpTy<'tcx, Tag>,\n     ) -> InterpResult<'tcx> {\n         let this = self.eval_context_mut();\n-        assert_eq!(this.get_total_thread_count(), 1, \"concurrency on Windows is not supported\");\n-\n-        let lock = deref_lock(this, lock_op)?;\n-        let lock_val = this.read_scalar(lock.into())?.to_machine_usize(this)?;\n-        if lock_val == this.machine_usize_max() {\n-            // Currently write locked. This is a UB.\n-            throw_ub_format!(\"calling ReleaseSRWLockShared on write-locked SRWLock\");\n-        } else if lock_val == 0 {\n-            // Currently not locked at all.\n-            throw_ub_format!(\"calling ReleaseSRWLockShared on unlocked SRWLock\");\n-        } else {\n-            // Decrement read counter (cannot overflow as we just checkd against 0);\n-            let new_val = lock_val-1;\n-            this.write_scalar(Scalar::from_machine_usize(new_val, this), lock.into())?;\n+        let id = srwlock_get_or_create_id(this, lock_op)?;\n+        let active_thread = this.get_active_thread();\n+\n+        if !this.rwlock_reader_unlock(id, active_thread) {\n+            // The docs do not say anything about this case, but it seems better to not allow it.\n+            throw_ub_format!(\"calling ReleaseSRWLockShared on an SRWLock that is not locked by the current thread\");\n         }\n \n         Ok(())"}, {"sha": "7e3c27b386dfe0ba694c07154f2b6f2aef010373", "filename": "src/sync.rs", "status": "modified", "additions": 105, "deletions": 60, "changes": 165, "blob_url": "https://github.com/rust-lang/rust/blob/3a5bcb97edd5bab769341aacde18a05c015aa396/src%2Fsync.rs", "raw_url": "https://github.com/rust-lang/rust/raw/3a5bcb97edd5bab769341aacde18a05c015aa396/src%2Fsync.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Fsync.rs?ref=3a5bcb97edd5bab769341aacde18a05c015aa396", "patch": "@@ -3,6 +3,8 @@ use std::convert::TryFrom;\n use std::num::NonZeroU32;\n use std::ops::Not;\n \n+use log::trace;\n+\n use rustc_index::vec::{Idx, IndexVec};\n \n use crate::*;\n@@ -102,6 +104,52 @@ pub(super) struct SynchronizationState {\n     condvars: IndexVec<CondvarId, Condvar>,\n }\n \n+// Private extension trait for local helper methods\n+impl<'mir, 'tcx: 'mir> EvalContextExtPriv<'mir, 'tcx> for crate::MiriEvalContext<'mir, 'tcx> {}\n+trait EvalContextExtPriv<'mir, 'tcx: 'mir>: crate::MiriEvalContextExt<'mir, 'tcx> {\n+    /// Take a reader out of the queue waiting for the lock.\n+    /// Returns `true` if some thread got the rwlock.\n+    #[inline]\n+    fn rwlock_dequeue_and_lock_reader(&mut self, id: RwLockId) -> bool {\n+        let this = self.eval_context_mut();\n+        if let Some(reader) = this.machine.threads.sync.rwlocks[id].reader_queue.pop_front() {\n+            this.unblock_thread(reader);\n+            this.rwlock_reader_lock(id, reader);\n+            true\n+        } else {\n+            false\n+        }\n+    }\n+\n+    /// Take the writer out of the queue waiting for the lock.\n+    /// Returns `true` if some thread got the rwlock.\n+    #[inline]\n+    fn rwlock_dequeue_and_lock_writer(&mut self, id: RwLockId) -> bool {\n+        let this = self.eval_context_mut();\n+        if let Some(writer) = this.machine.threads.sync.rwlocks[id].writer_queue.pop_front() {\n+            this.unblock_thread(writer);\n+            this.rwlock_writer_lock(id, writer);\n+            true\n+        } else {\n+            false\n+        }\n+    }\n+\n+    /// Take a thread out of the queue waiting for the mutex, and lock\n+    /// the mutex for it. Returns `true` if some thread has the mutex now.\n+    #[inline]\n+    fn mutex_dequeue_and_lock(&mut self, id: MutexId) -> bool {\n+        let this = self.eval_context_mut();\n+        if let Some(thread) = this.machine.threads.sync.mutexes[id].queue.pop_front() {\n+            this.unblock_thread(thread);\n+            this.mutex_lock(id, thread);\n+            true\n+        } else {\n+            false\n+        }\n+    }\n+}\n+\n // Public interface to synchronization primitives. Please note that in most\n // cases, the function calls are infallible and it is the client's (shim\n // implementation's) responsibility to detect and deal with erroneous\n@@ -124,8 +172,8 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: crate::MiriEvalContextExt<'mir, 'tcx\n \n     #[inline]\n     /// Check if locked.\n-    fn mutex_is_locked(&mut self, id: MutexId) -> bool {\n-        let this = self.eval_context_mut();\n+    fn mutex_is_locked(&self, id: MutexId) -> bool {\n+        let this = self.eval_context_ref();\n         this.machine.threads.sync.mutexes[id].owner.is_some()\n     }\n \n@@ -174,7 +222,7 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: crate::MiriEvalContextExt<'mir, 'tcx\n             }\n             Some(old_lock_count)\n         } else {\n-            // Mutex is unlocked.\n+            // Mutex is not locked.\n             None\n         }\n     }\n@@ -188,20 +236,6 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: crate::MiriEvalContextExt<'mir, 'tcx\n         this.block_thread(thread);\n     }\n \n-    #[inline]\n-    /// Take a thread out of the queue waiting for the mutex, and lock\n-    /// the mutex for it. Returns `true` if some thread has the mutex now.\n-    fn mutex_dequeue_and_lock(&mut self, id: MutexId) -> bool {\n-        let this = self.eval_context_mut();\n-        if let Some(thread) = this.machine.threads.sync.mutexes[id].queue.pop_front() {\n-            this.unblock_thread(thread);\n-            this.mutex_lock(id, thread);\n-            true\n-        } else {\n-            false\n-        }\n-    }\n-\n     #[inline]\n     /// Create state for a new read write lock.\n     fn rwlock_create(&mut self) -> RwLockId {\n@@ -211,30 +245,37 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: crate::MiriEvalContextExt<'mir, 'tcx\n \n     #[inline]\n     /// Check if locked.\n-    fn rwlock_is_locked(&mut self, id: RwLockId) -> bool {\n-        let this = self.eval_context_mut();\n-        this.machine.threads.sync.rwlocks[id].writer.is_some()\n-            || this.machine.threads.sync.rwlocks[id].readers.is_empty().not()\n+    fn rwlock_is_locked(&self, id: RwLockId) -> bool {\n+        let this = self.eval_context_ref();\n+        let rwlock = &this.machine.threads.sync.rwlocks[id];\n+        trace!(\n+            \"rwlock_is_locked: {:?} writer is {:?} and there are {} reader threads (some of which could hold multiple read locks)\",\n+            id, rwlock.writer, rwlock.readers.len(),\n+        );\n+        rwlock.writer.is_some()|| rwlock.readers.is_empty().not()\n     }\n \n     #[inline]\n     /// Check if write locked.\n-    fn rwlock_is_write_locked(&mut self, id: RwLockId) -> bool {\n-        let this = self.eval_context_mut();\n-        this.machine.threads.sync.rwlocks[id].writer.is_some()\n+    fn rwlock_is_write_locked(&self, id: RwLockId) -> bool {\n+        let this = self.eval_context_ref();\n+        let rwlock = &this.machine.threads.sync.rwlocks[id];\n+        trace!(\"rwlock_is_write_locked: {:?} writer is {:?}\", id, rwlock.writer);\n+        rwlock.writer.is_some()\n     }\n \n     /// Read-lock the lock by adding the `reader` the list of threads that own\n     /// this lock.\n     fn rwlock_reader_lock(&mut self, id: RwLockId, reader: ThreadId) {\n         let this = self.eval_context_mut();\n         assert!(!this.rwlock_is_write_locked(id), \"the lock is write locked\");\n+        trace!(\"rwlock_reader_lock: {:?} now also held (one more time) by {:?}\", id, reader);\n         let count = this.machine.threads.sync.rwlocks[id].readers.entry(reader).or_insert(0);\n         *count = count.checked_add(1).expect(\"the reader counter overflowed\");\n     }\n \n-    /// Try read-unlock the lock for `reader`. Returns `true` if succeeded,\n-    /// `false` if this `reader` did not hold the lock.\n+    /// Try read-unlock the lock for `reader` and potentially give the lock to a new owner.\n+    /// Returns `true` if succeeded, `false` if this `reader` did not hold the lock.\n     fn rwlock_reader_unlock(&mut self, id: RwLockId, reader: ThreadId) -> bool {\n         let this = self.eval_context_mut();\n         match this.machine.threads.sync.rwlocks[id].readers.entry(reader) {\n@@ -243,12 +284,19 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: crate::MiriEvalContextExt<'mir, 'tcx\n                 assert!(*count > 0, \"rwlock locked with count == 0\");\n                 *count -= 1;\n                 if *count == 0 {\n+                    trace!(\"rwlock_reader_unlock: {:?} no longer held by {:?}\", id, reader);\n                     entry.remove();\n+                } else {\n+                    trace!(\"rwlock_reader_unlock: {:?} held one less time by {:?}\", id, reader);\n                 }\n-                true\n             }\n-            Entry::Vacant(_) => false,\n+            Entry::Vacant(_) => return false, // we did not even own this lock\n+        }\n+        // The thread was a reader. If the lock is not held any more, give it to a writer.\n+        if this.rwlock_is_locked(id).not() {\n+            this.rwlock_dequeue_and_lock_writer(id);\n         }\n+        true\n     }\n \n     #[inline]\n@@ -259,38 +307,49 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: crate::MiriEvalContextExt<'mir, 'tcx\n         reader: ThreadId,\n     ) {\n         let this = self.eval_context_mut();\n-        assert!(this.rwlock_is_write_locked(id), \"queueing on not write locked lock\");\n+        assert!(this.rwlock_is_write_locked(id), \"read-queueing on not write locked rwlock\");\n         this.machine.threads.sync.rwlocks[id].reader_queue.push_back(reader);\n         this.block_thread(reader);\n     }\n \n-    #[inline]\n-    /// Take a reader out the queue waiting for the lock.\n-    /// Returns `true` if some thread got the rwlock.\n-    fn rwlock_dequeue_and_lock_reader(&mut self, id: RwLockId) -> bool {\n-        let this = self.eval_context_mut();\n-        if let Some(reader) = this.machine.threads.sync.rwlocks[id].reader_queue.pop_front() {\n-            this.unblock_thread(reader);\n-            this.rwlock_reader_lock(id, reader);\n-            true\n-        } else {\n-            false\n-        }\n-    }\n-\n     #[inline]\n     /// Lock by setting the writer that owns the lock.\n     fn rwlock_writer_lock(&mut self, id: RwLockId, writer: ThreadId) {\n         let this = self.eval_context_mut();\n         assert!(!this.rwlock_is_locked(id), \"the rwlock is already locked\");\n+        trace!(\"rwlock_writer_lock: {:?} now held by {:?}\", id, writer);\n         this.machine.threads.sync.rwlocks[id].writer = Some(writer);\n     }\n \n     #[inline]\n     /// Try to unlock by removing the writer.\n-    fn rwlock_writer_unlock(&mut self, id: RwLockId) -> Option<ThreadId> {\n+    fn rwlock_writer_unlock(&mut self, id: RwLockId, expected_writer: ThreadId) -> bool {\n         let this = self.eval_context_mut();\n-        this.machine.threads.sync.rwlocks[id].writer.take()\n+        let rwlock = &mut this.machine.threads.sync.rwlocks[id];\n+        if let Some(current_writer) = rwlock.writer {\n+            if current_writer != expected_writer {\n+                // Only the owner can unlock the rwlock.\n+                return false;\n+            }\n+            rwlock.writer = None;\n+            trace!(\"rwlock_writer_unlock: {:?} unlocked by {:?}\", id, expected_writer);\n+            // The thread was a writer.\n+            //\n+            // We are prioritizing writers here against the readers. As a\n+            // result, not only readers can starve writers, but also writers can\n+            // starve readers.\n+            if this.rwlock_dequeue_and_lock_writer(id) {\n+                // Someone got the write lock, nice.\n+            } else {\n+                // Give the lock to all readers.\n+                while this.rwlock_dequeue_and_lock_reader(id) {\n+                    // Rinse and repeat.\n+                }\n+            }\n+            true\n+        } else {\n+            false\n+        }\n     }\n \n     #[inline]\n@@ -301,25 +360,11 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: crate::MiriEvalContextExt<'mir, 'tcx\n         writer: ThreadId,\n     ) {\n         let this = self.eval_context_mut();\n-        assert!(this.rwlock_is_locked(id), \"queueing on unlocked lock\");\n+        assert!(this.rwlock_is_locked(id), \"write-queueing on unlocked rwlock\");\n         this.machine.threads.sync.rwlocks[id].writer_queue.push_back(writer);\n         this.block_thread(writer);\n     }\n \n-    #[inline]\n-    /// Take the writer out the queue waiting for the lock.\n-    /// Returns `true` if some thread got the rwlock.\n-    fn rwlock_dequeue_and_lock_writer(&mut self, id: RwLockId) -> bool {\n-        let this = self.eval_context_mut();\n-        if let Some(writer) = this.machine.threads.sync.rwlocks[id].writer_queue.pop_front() {\n-            this.unblock_thread(writer);\n-            this.rwlock_writer_lock(id, writer);\n-            true\n-        } else {\n-            false\n-        }\n-    }\n-\n     #[inline]\n     /// Create state for a new conditional variable.\n     fn condvar_create(&mut self) -> CondvarId {"}, {"sha": "27760eed8dba9b05965601d01a71dcf87b86084b", "filename": "tests/compile-fail/concurrency/thread-spawn.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/3a5bcb97edd5bab769341aacde18a05c015aa396/tests%2Fcompile-fail%2Fconcurrency%2Fthread-spawn.rs", "raw_url": "https://github.com/rust-lang/rust/raw/3a5bcb97edd5bab769341aacde18a05c015aa396/tests%2Fcompile-fail%2Fconcurrency%2Fthread-spawn.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/tests%2Fcompile-fail%2Fconcurrency%2Fthread-spawn.rs?ref=3a5bcb97edd5bab769341aacde18a05c015aa396", "patch": "@@ -3,7 +3,7 @@\n \n use std::thread;\n \n-// error-pattern: Miri does not support threading\n+// error-pattern: Miri does not support concurrency on Windows\n \n fn main() {\n     thread::spawn(|| {});"}]}