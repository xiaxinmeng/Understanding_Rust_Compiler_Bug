{"sha": "d8b34e9a74a4e91c4283ba4002a050ac0150cec6", "node_id": "MDY6Q29tbWl0NzI0NzEyOmQ4YjM0ZTlhNzRhNGU5MWM0MjgzYmE0MDAyYTA1MGFjMDE1MGNlYzY=", "commit": {"author": {"name": "Jeffrey Seyfried", "email": "jeffrey.seyfried@gmail.com", "date": "2017-01-29T08:38:44Z"}, "committer": {"name": "Jeffrey Seyfried", "email": "jeffrey.seyfried@gmail.com", "date": "2017-02-28T22:14:29Z"}, "message": "Add `syntax::ext::tt::quoted::{TokenTree, ..}` and remove `tokenstream::TokenTree::Sequence`.", "tree": {"sha": "fc62b9e970fd9120e078856dd6c9727bcb55ac89", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/fc62b9e970fd9120e078856dd6c9727bcb55ac89"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/d8b34e9a74a4e91c4283ba4002a050ac0150cec6", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/d8b34e9a74a4e91c4283ba4002a050ac0150cec6", "html_url": "https://github.com/rust-lang/rust/commit/d8b34e9a74a4e91c4283ba4002a050ac0150cec6", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/d8b34e9a74a4e91c4283ba4002a050ac0150cec6/comments", "author": {"login": "jseyfried", "id": 8652869, "node_id": "MDQ6VXNlcjg2NTI4Njk=", "avatar_url": "https://avatars.githubusercontent.com/u/8652869?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jseyfried", "html_url": "https://github.com/jseyfried", "followers_url": "https://api.github.com/users/jseyfried/followers", "following_url": "https://api.github.com/users/jseyfried/following{/other_user}", "gists_url": "https://api.github.com/users/jseyfried/gists{/gist_id}", "starred_url": "https://api.github.com/users/jseyfried/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jseyfried/subscriptions", "organizations_url": "https://api.github.com/users/jseyfried/orgs", "repos_url": "https://api.github.com/users/jseyfried/repos", "events_url": "https://api.github.com/users/jseyfried/events{/privacy}", "received_events_url": "https://api.github.com/users/jseyfried/received_events", "type": "User", "site_admin": false}, "committer": {"login": "jseyfried", "id": 8652869, "node_id": "MDQ6VXNlcjg2NTI4Njk=", "avatar_url": "https://avatars.githubusercontent.com/u/8652869?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jseyfried", "html_url": "https://github.com/jseyfried", "followers_url": "https://api.github.com/users/jseyfried/followers", "following_url": "https://api.github.com/users/jseyfried/following{/other_user}", "gists_url": "https://api.github.com/users/jseyfried/gists{/gist_id}", "starred_url": "https://api.github.com/users/jseyfried/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jseyfried/subscriptions", "organizations_url": "https://api.github.com/users/jseyfried/orgs", "repos_url": "https://api.github.com/users/jseyfried/repos", "events_url": "https://api.github.com/users/jseyfried/events{/privacy}", "received_events_url": "https://api.github.com/users/jseyfried/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "247188803356234ae5d6ecf947ffb2308688dc90", "url": "https://api.github.com/repos/rust-lang/rust/commits/247188803356234ae5d6ecf947ffb2308688dc90", "html_url": "https://github.com/rust-lang/rust/commit/247188803356234ae5d6ecf947ffb2308688dc90"}], "stats": {"total": 882, "additions": 398, "deletions": 484}, "files": [{"sha": "b33caefbcd2ecbac73e3b73a7b1d88ca4af3f2a5", "filename": "src/libproc_macro/lib.rs", "status": "modified", "additions": 1, "deletions": 2, "changes": 3, "blob_url": "https://github.com/rust-lang/rust/blob/d8b34e9a74a4e91c4283ba4002a050ac0150cec6/src%2Flibproc_macro%2Flib.rs", "raw_url": "https://github.com/rust-lang/rust/raw/d8b34e9a74a4e91c4283ba4002a050ac0150cec6/src%2Flibproc_macro%2Flib.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibproc_macro%2Flib.rs?ref=d8b34e9a74a4e91c4283ba4002a050ac0150cec6", "patch": "@@ -173,8 +173,7 @@ impl FromStr for TokenStream {\n         __internal::with_parse_sess(|sess| {\n             let src = src.to_string();\n             let name = \"<proc-macro source code>\".to_string();\n-            let tts = try!(parse::parse_tts_from_source_str(name, src, sess)\n-                .map_err(parse_to_lex_err));\n+            let tts = parse::parse_tts_from_source_str(name, src, sess);\n \n             Ok(__internal::token_stream_wrap(tts.into_iter().collect()))\n         })"}, {"sha": "dc7c96a4e27672d88fb2f501d5d2b7b0e69b322d", "filename": "src/libproc_macro_plugin/qquote.rs", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "blob_url": "https://github.com/rust-lang/rust/blob/d8b34e9a74a4e91c4283ba4002a050ac0150cec6/src%2Flibproc_macro_plugin%2Fqquote.rs", "raw_url": "https://github.com/rust-lang/rust/raw/d8b34e9a74a4e91c4283ba4002a050ac0150cec6/src%2Flibproc_macro_plugin%2Fqquote.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibproc_macro_plugin%2Fqquote.rs?ref=d8b34e9a74a4e91c4283ba4002a050ac0150cec6", "patch": "@@ -119,7 +119,6 @@ impl Quote for TokenTree {\n                 ::syntax::tokenstream::TokenTree::Delimited(::syntax::ext::quote::rt::DUMMY_SP,\n                                                             (quote delimited))\n             },\n-            _ => panic!(\"unexpected `TokenTree::Sequence` in `qquote`\"),\n         }\n     }\n }"}, {"sha": "bf0fa25cdd3ed57dbdb6ba377ffa1b1a959d6393", "filename": "src/librustc_incremental/calculate_svh/svh_visitor.rs", "status": "modified", "additions": 0, "deletions": 20, "changes": 20, "blob_url": "https://github.com/rust-lang/rust/blob/d8b34e9a74a4e91c4283ba4002a050ac0150cec6/src%2Flibrustc_incremental%2Fcalculate_svh%2Fsvh_visitor.rs", "raw_url": "https://github.com/rust-lang/rust/raw/d8b34e9a74a4e91c4283ba4002a050ac0150cec6/src%2Flibrustc_incremental%2Fcalculate_svh%2Fsvh_visitor.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_incremental%2Fcalculate_svh%2Fsvh_visitor.rs?ref=d8b34e9a74a4e91c4283ba4002a050ac0150cec6", "patch": "@@ -1044,26 +1044,6 @@ impl<'a, 'hash, 'tcx> StrictVersionHashVisitor<'a, 'hash, 'tcx> {\n                     self.hash_token_tree(sub_tt);\n                 }\n             }\n-            tokenstream::TokenTree::Sequence(span, ref sequence_repetition) => {\n-                hash_span!(self, span);\n-                let tokenstream::SequenceRepetition {\n-                    ref tts,\n-                    ref separator,\n-                    op,\n-                    num_captures,\n-                } = **sequence_repetition;\n-\n-                tts.len().hash(self.st);\n-                for sub_tt in tts {\n-                    self.hash_token_tree(sub_tt);\n-                }\n-                self.hash_discriminant(separator);\n-                if let Some(ref separator) = *separator {\n-                    self.hash_token(separator, span);\n-                }\n-                op.hash(self.st);\n-                num_captures.hash(self.st);\n-            }\n         }\n     }\n "}, {"sha": "6c93744f014a3c2b678be69ef78b351c56e8bc2f", "filename": "src/librustc_save_analysis/span_utils.rs", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "blob_url": "https://github.com/rust-lang/rust/blob/d8b34e9a74a4e91c4283ba4002a050ac0150cec6/src%2Flibrustc_save_analysis%2Fspan_utils.rs", "raw_url": "https://github.com/rust-lang/rust/raw/d8b34e9a74a4e91c4283ba4002a050ac0150cec6/src%2Flibrustc_save_analysis%2Fspan_utils.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_save_analysis%2Fspan_utils.rs?ref=d8b34e9a74a4e91c4283ba4002a050ac0150cec6", "patch": "@@ -287,7 +287,7 @@ impl<'a> SpanUtils<'a> {\n         let mut toks = toks.parse_all_token_trees().unwrap().into_iter();\n         let mut prev = toks.next().unwrap();\n \n-        let first_span = prev.get_span();\n+        let first_span = prev.span();\n         let mut angle_count = 0;\n         for tok in toks {\n             if let TokenTree::Token(_, ref tok) = prev {\n@@ -305,10 +305,10 @@ impl<'a> SpanUtils<'a> {\n                 continue;\n             }\n             if let TokenTree::Token(_, token::Semi) = tok {\n-                return self.snippet(mk_sp(first_span.lo, prev.get_span().hi));\n+                return self.snippet(mk_sp(first_span.lo, prev.span().hi));\n             } else if let TokenTree::Delimited(_, ref d) = tok {\n                 if d.delim == token::Brace {\n-                    return self.snippet(mk_sp(first_span.lo, prev.get_span().hi));\n+                    return self.snippet(mk_sp(first_span.lo, prev.span().hi));\n                 }\n             }\n             prev = tok;"}, {"sha": "236d9f230b5d470718467b351ed6ce75d16fafa5", "filename": "src/librustdoc/visit_ast.rs", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/rust-lang/rust/blob/d8b34e9a74a4e91c4283ba4002a050ac0150cec6/src%2Flibrustdoc%2Fvisit_ast.rs", "raw_url": "https://github.com/rust-lang/rust/raw/d8b34e9a74a4e91c4283ba4002a050ac0150cec6/src%2Flibrustdoc%2Fvisit_ast.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustdoc%2Fvisit_ast.rs?ref=d8b34e9a74a4e91c4283ba4002a050ac0150cec6", "patch": "@@ -211,7 +211,7 @@ impl<'a, 'tcx> RustdocVisitor<'a, 'tcx> {\n                     };\n \n                     // FIXME(jseyfried) merge with `self.visit_macro()`\n-                    let matchers = def.body.chunks(4).map(|arm| arm[0].get_span()).collect();\n+                    let matchers = def.body.chunks(4).map(|arm| arm[0].span()).collect();\n                     om.macros.push(Macro {\n                         def_id: def_id,\n                         attrs: def.attrs.clone().into(),\n@@ -521,7 +521,7 @@ impl<'a, 'tcx> RustdocVisitor<'a, 'tcx> {\n     // convert each exported_macro into a doc item\n     fn visit_local_macro(&self, def: &hir::MacroDef) -> Macro {\n         // Extract the spans of all matchers. They represent the \"interface\" of the macro.\n-        let matchers = def.body.chunks(4).map(|arm| arm[0].get_span()).collect();\n+        let matchers = def.body.chunks(4).map(|arm| arm[0].span()).collect();\n \n         Macro {\n             def_id: self.cx.tcx.hir.local_def_id(def.id),"}, {"sha": "b1b69c80f4d0016c790c4554d0df6cc73350fd7b", "filename": "src/libsyntax/ext/quote.rs", "status": "modified", "additions": 45, "deletions": 105, "changes": 150, "blob_url": "https://github.com/rust-lang/rust/blob/d8b34e9a74a4e91c4283ba4002a050ac0150cec6/src%2Flibsyntax%2Fext%2Fquote.rs", "raw_url": "https://github.com/rust-lang/rust/raw/d8b34e9a74a4e91c4283ba4002a050ac0150cec6/src%2Flibsyntax%2Fext%2Fquote.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fext%2Fquote.rs?ref=d8b34e9a74a4e91c4283ba4002a050ac0150cec6", "patch": "@@ -14,10 +14,9 @@ use ext::base::ExtCtxt;\n use ext::base;\n use ext::build::AstBuilder;\n use parse::parser::{Parser, PathStyle};\n-use parse::token::*;\n use parse::token;\n use ptr::P;\n-use tokenstream::{self, TokenTree};\n+use tokenstream::TokenTree;\n \n \n /// Quasiquoting works via token trees.\n@@ -356,14 +355,35 @@ pub mod rt {\n         }\n \n         fn parse_tts(&self, s: String) -> Vec<TokenTree> {\n-            panictry!(parse::parse_tts_from_source_str(\n-                \"<quote expansion>\".to_string(),\n-                s,\n-                self.parse_sess()))\n+            parse::parse_tts_from_source_str(\"<quote expansion>\".to_string(), s, self.parse_sess())\n         }\n     }\n }\n \n+// Replaces `Token::OpenDelim .. Token::CloseDelim` with `TokenTree::Delimited(..)`.\n+pub fn unflatten(tts: Vec<TokenTree>) -> Vec<TokenTree> {\n+    use std::rc::Rc;\n+    use tokenstream::Delimited;\n+\n+    let mut results = Vec::new();\n+    let mut result = Vec::new();\n+    for tree in tts {\n+        match tree {\n+            TokenTree::Token(_, token::OpenDelim(..)) => {\n+                results.push(::std::mem::replace(&mut result, Vec::new()));\n+            }\n+            TokenTree::Token(span, token::CloseDelim(delim)) => {\n+                let tree =\n+                    TokenTree::Delimited(span, Rc::new(Delimited { delim: delim, tts: result }));\n+                result = results.pop().unwrap();\n+                result.push(tree);\n+            }\n+            tree @ _ => result.push(tree),\n+        }\n+    }\n+    result\n+}\n+\n // These panicking parsing functions are used by the quote_*!() syntax extensions,\n // but shouldn't be used otherwise.\n pub fn parse_expr_panic(parser: &mut Parser) -> P<Expr> {\n@@ -510,20 +530,6 @@ pub fn expand_quote_path(cx: &mut ExtCtxt,\n     base::MacEager::expr(expanded)\n }\n \n-pub fn expand_quote_matcher(cx: &mut ExtCtxt,\n-                            sp: Span,\n-                            tts: &[TokenTree])\n-                            -> Box<base::MacResult+'static> {\n-    let (cx_expr, tts) = parse_arguments_to_quote(cx, tts);\n-    let mut vector = mk_stmts_let(cx, sp);\n-    vector.extend(statements_mk_tts(cx, &tts[..], true));\n-    vector.push(cx.stmt_expr(cx.expr_ident(sp, id_ext(\"tt\"))));\n-    let block = cx.expr_block(cx.block(sp, vector));\n-\n-    let expanded = expand_wrapper(cx, sp, cx_expr, block, &[&[\"syntax\", \"ext\", \"quote\", \"rt\"]]);\n-    base::MacEager::expr(expanded)\n-}\n-\n fn ids_ext(strs: Vec<String>) -> Vec<ast::Ident> {\n     strs.iter().map(|s| ast::Ident::from_str(s)).collect()\n }\n@@ -669,12 +675,6 @@ fn expr_mk_token(cx: &ExtCtxt, sp: Span, tok: &token::Token) -> P<ast::Expr> {\n                                 vec![mk_name(cx, sp, ast::Ident::with_empty_ctxt(ident))]);\n         }\n \n-        token::MatchNt(name, kind) => {\n-            return cx.expr_call(sp,\n-                                mk_token_path(cx, sp, \"MatchNt\"),\n-                                vec![mk_ident(cx, sp, name), mk_ident(cx, sp, kind)]);\n-        }\n-\n         token::Interpolated(_) => panic!(\"quote! with interpolated token\"),\n \n         _ => ()\n@@ -712,9 +712,9 @@ fn expr_mk_token(cx: &ExtCtxt, sp: Span, tok: &token::Token) -> P<ast::Expr> {\n     mk_token_path(cx, sp, name)\n }\n \n-fn statements_mk_tt(cx: &ExtCtxt, tt: &TokenTree, matcher: bool) -> Vec<ast::Stmt> {\n+fn statements_mk_tt(cx: &ExtCtxt, tt: &TokenTree, quoted: bool) -> Vec<ast::Stmt> {\n     match *tt {\n-        TokenTree::Token(sp, SubstNt(ident)) => {\n+        TokenTree::Token(sp, token::Ident(ident)) if quoted => {\n             // tt.extend($ident.to_tokens(ext_cx))\n \n             let e_to_toks =\n@@ -733,13 +733,6 @@ fn statements_mk_tt(cx: &ExtCtxt, tt: &TokenTree, matcher: bool) -> Vec<ast::Stm\n \n             vec![cx.stmt_expr(e_push)]\n         }\n-        ref tt @ TokenTree::Token(_, MatchNt(..)) if !matcher => {\n-            let mut seq = vec![];\n-            for i in 0..tt.len() {\n-                seq.push(tt.get_tt(i));\n-            }\n-            statements_mk_tts(cx, &seq[..], matcher)\n-        }\n         TokenTree::Token(sp, ref tok) => {\n             let e_sp = cx.expr_ident(sp, id_ext(\"_sp\"));\n             let e_tok = cx.expr_call(sp,\n@@ -753,77 +746,17 @@ fn statements_mk_tt(cx: &ExtCtxt, tt: &TokenTree, matcher: bool) -> Vec<ast::Stm\n             vec![cx.stmt_expr(e_push)]\n         },\n         TokenTree::Delimited(span, ref delimed) => {\n-            statements_mk_tt(cx, &delimed.open_tt(span), matcher).into_iter()\n-                .chain(delimed.tts.iter()\n-                                  .flat_map(|tt| statements_mk_tt(cx, tt, matcher)))\n-                .chain(statements_mk_tt(cx, &delimed.close_tt(span), matcher))\n-                .collect()\n-        },\n-        TokenTree::Sequence(sp, ref seq) => {\n-            if !matcher {\n-                panic!(\"TokenTree::Sequence in quote!\");\n-            }\n-\n-            let e_sp = cx.expr_ident(sp, id_ext(\"_sp\"));\n-\n-            let stmt_let_tt = cx.stmt_let(sp, true, id_ext(\"tt\"), cx.expr_vec_ng(sp));\n-            let mut tts_stmts = vec![stmt_let_tt];\n-            tts_stmts.extend(statements_mk_tts(cx, &seq.tts[..], matcher));\n-            tts_stmts.push(cx.stmt_expr(cx.expr_ident(sp, id_ext(\"tt\"))));\n-            let e_tts = cx.expr_block(cx.block(sp, tts_stmts));\n-\n-            let e_separator = match seq.separator {\n-                Some(ref sep) => cx.expr_some(sp, expr_mk_token(cx, sp, sep)),\n-                None => cx.expr_none(sp),\n-            };\n-            let e_op = match seq.op {\n-                tokenstream::KleeneOp::ZeroOrMore => \"ZeroOrMore\",\n-                tokenstream::KleeneOp::OneOrMore => \"OneOrMore\",\n-            };\n-            let e_op_idents = vec![\n-                id_ext(\"syntax\"),\n-                id_ext(\"tokenstream\"),\n-                id_ext(\"KleeneOp\"),\n-                id_ext(e_op),\n-            ];\n-            let e_op = cx.expr_path(cx.path_global(sp, e_op_idents));\n-            let fields = vec![cx.field_imm(sp, id_ext(\"tts\"), e_tts),\n-                              cx.field_imm(sp, id_ext(\"separator\"), e_separator),\n-                              cx.field_imm(sp, id_ext(\"op\"), e_op),\n-                              cx.field_imm(sp, id_ext(\"num_captures\"),\n-                                               cx.expr_usize(sp, seq.num_captures))];\n-            let seq_path = vec![id_ext(\"syntax\"),\n-                                id_ext(\"tokenstream\"),\n-                                id_ext(\"SequenceRepetition\")];\n-            let e_seq_struct = cx.expr_struct(sp, cx.path_global(sp, seq_path), fields);\n-            let e_rc_new = cx.expr_call_global(sp, vec![id_ext(\"std\"),\n-                                                        id_ext(\"rc\"),\n-                                                        id_ext(\"Rc\"),\n-                                                        id_ext(\"new\")],\n-                                                   vec![e_seq_struct]);\n-            let e_tok = cx.expr_call(sp,\n-                                     mk_tt_path(cx, sp, \"Sequence\"),\n-                                     vec![e_sp, e_rc_new]);\n-            let e_push =\n-                cx.expr_method_call(sp,\n-                                    cx.expr_ident(sp, id_ext(\"tt\")),\n-                                    id_ext(\"push\"),\n-                                    vec![e_tok]);\n-            vec![cx.stmt_expr(e_push)]\n+            let mut stmts = statements_mk_tt(cx, &delimed.open_tt(span), false);\n+            stmts.extend(statements_mk_tts(cx, &delimed.tts));\n+            stmts.extend(statements_mk_tt(cx, &delimed.close_tt(span), false));\n+            stmts\n         }\n     }\n }\n \n fn parse_arguments_to_quote(cx: &ExtCtxt, tts: &[TokenTree])\n                             -> (P<ast::Expr>, Vec<TokenTree>) {\n-    // NB: It appears that the main parser loses its mind if we consider\n-    // $foo as a SubstNt during the main parse, so we have to re-parse\n-    // under quote_depth > 0. This is silly and should go away; the _guess_ is\n-    // it has to do with transition away from supporting old-style macros, so\n-    // try removing it when enough of them are gone.\n-\n     let mut p = cx.new_parser_from_tts(tts);\n-    p.quote_depth += 1;\n \n     let cx_expr = panictry!(p.parse_expr());\n     if !p.eat(&token::Comma) {\n@@ -877,24 +810,31 @@ fn mk_stmts_let(cx: &ExtCtxt, sp: Span) -> Vec<ast::Stmt> {\n     vec![stmt_let_sp, stmt_let_tt]\n }\n \n-fn statements_mk_tts(cx: &ExtCtxt, tts: &[TokenTree], matcher: bool) -> Vec<ast::Stmt> {\n+fn statements_mk_tts(cx: &ExtCtxt, tts: &[TokenTree]) -> Vec<ast::Stmt> {\n     let mut ss = Vec::new();\n+    let mut quoted = false;\n     for tt in tts {\n-        ss.extend(statements_mk_tt(cx, tt, matcher));\n+        quoted = match *tt {\n+            TokenTree::Token(_, token::Dollar) if !quoted => true,\n+            _ => {\n+                ss.extend(statements_mk_tt(cx, tt, quoted));\n+                false\n+            }\n+        }\n     }\n     ss\n }\n \n-fn expand_tts(cx: &ExtCtxt, sp: Span, tts: &[TokenTree])\n-              -> (P<ast::Expr>, P<ast::Expr>) {\n+fn expand_tts(cx: &ExtCtxt, sp: Span, tts: &[TokenTree]) -> (P<ast::Expr>, P<ast::Expr>) {\n     let (cx_expr, tts) = parse_arguments_to_quote(cx, tts);\n \n     let mut vector = mk_stmts_let(cx, sp);\n-    vector.extend(statements_mk_tts(cx, &tts[..], false));\n+    vector.extend(statements_mk_tts(cx, &tts[..]));\n     vector.push(cx.stmt_expr(cx.expr_ident(sp, id_ext(\"tt\"))));\n     let block = cx.expr_block(cx.block(sp, vector));\n+    let unflatten = vec![id_ext(\"syntax\"), id_ext(\"ext\"), id_ext(\"quote\"), id_ext(\"unflatten\")];\n \n-    (cx_expr, block)\n+    (cx_expr, cx.expr_call_global(sp, unflatten, vec![block]))\n }\n \n fn expand_wrapper(cx: &ExtCtxt,"}, {"sha": "5761a61342b2b50f43014b8d02698d0e47cc62fd", "filename": "src/libsyntax/ext/tt/macro_parser.rs", "status": "modified", "additions": 24, "deletions": 20, "changes": 44, "blob_url": "https://github.com/rust-lang/rust/blob/d8b34e9a74a4e91c4283ba4002a050ac0150cec6/src%2Flibsyntax%2Fext%2Ftt%2Fmacro_parser.rs", "raw_url": "https://github.com/rust-lang/rust/raw/d8b34e9a74a4e91c4283ba4002a050ac0150cec6/src%2Flibsyntax%2Fext%2Ftt%2Fmacro_parser.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fext%2Ftt%2Fmacro_parser.rs?ref=d8b34e9a74a4e91c4283ba4002a050ac0150cec6", "patch": "@@ -82,13 +82,14 @@ use ast::Ident;\n use syntax_pos::{self, BytePos, mk_sp, Span};\n use codemap::Spanned;\n use errors::FatalError;\n+use ext::tt::quoted;\n use parse::{Directory, ParseSess};\n use parse::parser::{PathStyle, Parser};\n-use parse::token::{DocComment, MatchNt, SubstNt};\n+use parse::token::{DocComment, MatchNt};\n use parse::token::{Token, Nonterminal};\n use parse::token;\n use print::pprust;\n-use tokenstream::{self, TokenTree};\n+use tokenstream::TokenTree;\n use util::small_vector::SmallVector;\n \n use std::mem;\n@@ -101,8 +102,8 @@ use std::collections::hash_map::Entry::{Vacant, Occupied};\n \n #[derive(Clone)]\n enum TokenTreeOrTokenTreeVec {\n-    Tt(tokenstream::TokenTree),\n-    TtSeq(Vec<tokenstream::TokenTree>),\n+    Tt(quoted::TokenTree),\n+    TtSeq(Vec<quoted::TokenTree>),\n }\n \n impl TokenTreeOrTokenTreeVec {\n@@ -113,7 +114,7 @@ impl TokenTreeOrTokenTreeVec {\n         }\n     }\n \n-    fn get_tt(&self, index: usize) -> TokenTree {\n+    fn get_tt(&self, index: usize) -> quoted::TokenTree {\n         match *self {\n             TtSeq(ref v) => v[index].clone(),\n             Tt(ref tt) => tt.get_tt(index),\n@@ -144,7 +145,9 @@ struct MatcherPos {\n \n pub type NamedParseResult = ParseResult<HashMap<Ident, Rc<NamedMatch>>>;\n \n-pub fn count_names(ms: &[TokenTree]) -> usize {\n+pub fn count_names(ms: &[quoted::TokenTree]) -> usize {\n+    use self::quoted::TokenTree;\n+\n     ms.iter().fold(0, |count, elt| {\n         count + match *elt {\n             TokenTree::Sequence(_, ref seq) => {\n@@ -161,7 +164,7 @@ pub fn count_names(ms: &[TokenTree]) -> usize {\n     })\n }\n \n-fn initial_matcher_pos(ms: Vec<TokenTree>, lo: BytePos) -> Box<MatcherPos> {\n+fn initial_matcher_pos(ms: Vec<quoted::TokenTree>, lo: BytePos) -> Box<MatcherPos> {\n     let match_idx_hi = count_names(&ms[..]);\n     let matches = create_matches(match_idx_hi);\n     Box::new(MatcherPos {\n@@ -200,7 +203,10 @@ pub enum NamedMatch {\n     MatchedNonterminal(Rc<Nonterminal>)\n }\n \n-fn nameize<I: Iterator<Item=Rc<NamedMatch>>>(ms: &[TokenTree], mut res: I) -> NamedParseResult {\n+fn nameize<I: Iterator<Item=Rc<NamedMatch>>>(ms: &[quoted::TokenTree], mut res: I)\n+                                             -> NamedParseResult {\n+    use self::quoted::TokenTree;\n+\n     fn n_rec<I: Iterator<Item=Rc<NamedMatch>>>(m: &TokenTree, mut res: &mut I,\n              ret_val: &mut HashMap<Ident, Rc<NamedMatch>>)\n              -> Result<(), (syntax_pos::Span, String)> {\n@@ -225,9 +231,6 @@ fn nameize<I: Iterator<Item=Rc<NamedMatch>>>(ms: &[TokenTree], mut res: I) -> Na\n                     }\n                 }\n             }\n-            TokenTree::Token(sp, SubstNt(..)) => {\n-                return Err((sp, \"missing fragment specifier\".to_string()))\n-            }\n             TokenTree::Token(..) => (),\n         }\n \n@@ -281,6 +284,8 @@ fn inner_parse_loop(cur_eis: &mut SmallVector<Box<MatcherPos>>,\n                     eof_eis: &mut SmallVector<Box<MatcherPos>>,\n                     bb_eis: &mut SmallVector<Box<MatcherPos>>,\n                     token: &Token, span: &syntax_pos::Span) -> ParseResult<()> {\n+    use self::quoted::TokenTree;\n+\n     while let Some(mut ei) = cur_eis.pop() {\n         // When unzipped trees end, remove them\n         while ei.idx >= ei.top_elts.len() {\n@@ -346,7 +351,7 @@ fn inner_parse_loop(cur_eis: &mut SmallVector<Box<MatcherPos>>,\n             match ei.top_elts.get_tt(idx) {\n                 /* need to descend into sequence */\n                 TokenTree::Sequence(sp, seq) => {\n-                    if seq.op == tokenstream::KleeneOp::ZeroOrMore {\n+                    if seq.op == quoted::KleeneOp::ZeroOrMore {\n                         // Examine the case where there are 0 matches of this sequence\n                         let mut new_ei = ei.clone();\n                         new_ei.match_cur += seq.num_captures;\n@@ -380,9 +385,6 @@ fn inner_parse_loop(cur_eis: &mut SmallVector<Box<MatcherPos>>,\n                         _ => bb_eis.push(ei),\n                     }\n                 }\n-                TokenTree::Token(sp, SubstNt(..)) => {\n-                    return Error(sp, \"missing fragment specifier\".to_string())\n-                }\n                 seq @ TokenTree::Delimited(..) | seq @ TokenTree::Token(_, DocComment(..)) => {\n                     let lower_elts = mem::replace(&mut ei.top_elts, Tt(seq));\n                     let idx = ei.idx;\n@@ -406,8 +408,13 @@ fn inner_parse_loop(cur_eis: &mut SmallVector<Box<MatcherPos>>,\n     Success(())\n }\n \n-pub fn parse(sess: &ParseSess, tts: Vec<TokenTree>, ms: &[TokenTree], directory: Option<Directory>)\n+pub fn parse(sess: &ParseSess,\n+             tts: Vec<TokenTree>,\n+             ms: &[quoted::TokenTree],\n+             directory: Option<Directory>)\n              -> NamedParseResult {\n+    use self::quoted::TokenTree;\n+\n     let mut parser = Parser::new(sess, tts, directory, true);\n     let mut cur_eis = SmallVector::one(initial_matcher_pos(ms.to_owned(), parser.span.lo));\n     let mut next_eis = Vec::new(); // or proceed normally\n@@ -479,10 +486,7 @@ pub fn parse(sess: &ParseSess, tts: Vec<TokenTree>, ms: &[TokenTree], directory:\n fn parse_nt<'a>(p: &mut Parser<'a>, sp: Span, name: &str) -> Nonterminal {\n     match name {\n         \"tt\" => {\n-            p.quote_depth += 1; //but in theory, non-quoted tts might be useful\n-            let tt = panictry!(p.parse_token_tree());\n-            p.quote_depth -= 1;\n-            return token::NtTT(tt);\n+            return token::NtTT(panictry!(p.parse_token_tree()));\n         }\n         _ => {}\n     }"}, {"sha": "5da401d48eeced8b4faccfd66b70dd3b5d5af02c", "filename": "src/libsyntax/ext/tt/macro_rules.rs", "status": "modified", "additions": 42, "deletions": 34, "changes": 76, "blob_url": "https://github.com/rust-lang/rust/blob/d8b34e9a74a4e91c4283ba4002a050ac0150cec6/src%2Flibsyntax%2Fext%2Ftt%2Fmacro_rules.rs", "raw_url": "https://github.com/rust-lang/rust/raw/d8b34e9a74a4e91c4283ba4002a050ac0150cec6/src%2Flibsyntax%2Fext%2Ftt%2Fmacro_rules.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fext%2Ftt%2Fmacro_rules.rs?ref=d8b34e9a74a4e91c4283ba4002a050ac0150cec6", "patch": "@@ -16,14 +16,15 @@ use ext::expand::{Expansion, ExpansionKind};\n use ext::tt::macro_parser::{Success, Error, Failure};\n use ext::tt::macro_parser::{MatchedSeq, MatchedNonterminal};\n use ext::tt::macro_parser::{parse, parse_failure_msg};\n+use ext::tt::quoted;\n use ext::tt::transcribe::transcribe;\n use parse::{Directory, ParseSess};\n use parse::parser::Parser;\n use parse::token::{self, NtTT, Token};\n use parse::token::Token::*;\n use print;\n use symbol::Symbol;\n-use tokenstream::{self, TokenTree};\n+use tokenstream::TokenTree;\n \n use std::collections::{HashMap};\n use std::collections::hash_map::{Entry};\n@@ -58,8 +59,8 @@ impl<'a> ParserAnyMacro<'a> {\n \n struct MacroRulesMacroExpander {\n     name: ast::Ident,\n-    lhses: Vec<TokenTree>,\n-    rhses: Vec<TokenTree>,\n+    lhses: Vec<quoted::TokenTree>,\n+    rhses: Vec<quoted::TokenTree>,\n     valid: bool,\n }\n \n@@ -86,8 +87,8 @@ fn generic_extension<'cx>(cx: &'cx ExtCtxt,\n                           sp: Span,\n                           name: ast::Ident,\n                           arg: &[TokenTree],\n-                          lhses: &[TokenTree],\n-                          rhses: &[TokenTree])\n+                          lhses: &[quoted::TokenTree],\n+                          rhses: &[quoted::TokenTree])\n                           -> Box<MacResult+'cx> {\n     if cx.trace_macros() {\n         println!(\"{}! {{ {} }}\",\n@@ -101,15 +102,15 @@ fn generic_extension<'cx>(cx: &'cx ExtCtxt,\n \n     for (i, lhs) in lhses.iter().enumerate() { // try each arm's matchers\n         let lhs_tt = match *lhs {\n-            TokenTree::Delimited(_, ref delim) => &delim.tts[..],\n+            quoted::TokenTree::Delimited(_, ref delim) => &delim.tts[..],\n             _ => cx.span_bug(sp, \"malformed macro lhs\")\n         };\n \n         match TokenTree::parse(cx, lhs_tt, arg) {\n             Success(named_matches) => {\n                 let rhs = match rhses[i] {\n                     // ignore delimiters\n-                    TokenTree::Delimited(_, ref delimed) => delimed.tts.clone(),\n+                    quoted::TokenTree::Delimited(_, ref delimed) => delimed.tts.clone(),\n                     _ => cx.span_bug(sp, \"malformed macro rhs\"),\n                 };\n                 // rhs has holes ( `$id` and `$(...)` that need filled)\n@@ -167,21 +168,21 @@ pub fn compile(sess: &ParseSess, def: &ast::MacroDef) -> SyntaxExtension {\n     let match_lhs_tok = MatchNt(lhs_nm, ast::Ident::from_str(\"tt\"));\n     let match_rhs_tok = MatchNt(rhs_nm, ast::Ident::from_str(\"tt\"));\n     let argument_gram = vec![\n-        TokenTree::Sequence(DUMMY_SP, Rc::new(tokenstream::SequenceRepetition {\n+        quoted::TokenTree::Sequence(DUMMY_SP, Rc::new(quoted::SequenceRepetition {\n             tts: vec![\n-                TokenTree::Token(DUMMY_SP, match_lhs_tok),\n-                TokenTree::Token(DUMMY_SP, token::FatArrow),\n-                TokenTree::Token(DUMMY_SP, match_rhs_tok),\n+                quoted::TokenTree::Token(DUMMY_SP, match_lhs_tok),\n+                quoted::TokenTree::Token(DUMMY_SP, token::FatArrow),\n+                quoted::TokenTree::Token(DUMMY_SP, match_rhs_tok),\n             ],\n             separator: Some(token::Semi),\n-            op: tokenstream::KleeneOp::OneOrMore,\n+            op: quoted::KleeneOp::OneOrMore,\n             num_captures: 2,\n         })),\n         // to phase into semicolon-termination instead of semicolon-separation\n-        TokenTree::Sequence(DUMMY_SP, Rc::new(tokenstream::SequenceRepetition {\n-            tts: vec![TokenTree::Token(DUMMY_SP, token::Semi)],\n+        quoted::TokenTree::Sequence(DUMMY_SP, Rc::new(quoted::SequenceRepetition {\n+            tts: vec![quoted::TokenTree::Token(DUMMY_SP, token::Semi)],\n             separator: None,\n-            op: tokenstream::KleeneOp::ZeroOrMore,\n+            op: quoted::KleeneOp::ZeroOrMore,\n             num_captures: 0\n         })),\n     ];\n@@ -206,12 +207,13 @@ pub fn compile(sess: &ParseSess, def: &ast::MacroDef) -> SyntaxExtension {\n             s.iter().map(|m| {\n                 if let MatchedNonterminal(ref nt) = **m {\n                     if let NtTT(ref tt) = **nt {\n-                        valid &= check_lhs_nt_follows(sess, tt);\n-                        return (*tt).clone();\n+                        let tt = quoted::parse(&[tt.clone()], true, sess).pop().unwrap();\n+                        valid &= check_lhs_nt_follows(sess, &tt);\n+                        return tt;\n                     }\n                 }\n                 sess.span_diagnostic.span_bug(def.span, \"wrong-structured lhs\")\n-            }).collect::<Vec<TokenTree>>()\n+            }).collect::<Vec<quoted::TokenTree>>()\n         }\n         _ => sess.span_diagnostic.span_bug(def.span, \"wrong-structured lhs\")\n     };\n@@ -221,11 +223,11 @@ pub fn compile(sess: &ParseSess, def: &ast::MacroDef) -> SyntaxExtension {\n             s.iter().map(|m| {\n                 if let MatchedNonterminal(ref nt) = **m {\n                     if let NtTT(ref tt) = **nt {\n-                        return (*tt).clone();\n+                        return quoted::parse(&[tt.clone()], false, sess).pop().unwrap();\n                     }\n                 }\n                 sess.span_diagnostic.span_bug(def.span, \"wrong-structured lhs\")\n-            }).collect()\n+            }).collect::<Vec<quoted::TokenTree>>()\n         }\n         _ => sess.span_diagnostic.span_bug(def.span, \"wrong-structured rhs\")\n     };\n@@ -249,14 +251,14 @@ pub fn compile(sess: &ParseSess, def: &ast::MacroDef) -> SyntaxExtension {\n     NormalTT(exp, Some(def.span), attr::contains_name(&def.attrs, \"allow_internal_unstable\"))\n }\n \n-fn check_lhs_nt_follows(sess: &ParseSess, lhs: &TokenTree) -> bool {\n+fn check_lhs_nt_follows(sess: &ParseSess, lhs: &quoted::TokenTree) -> bool {\n     // lhs is going to be like TokenTree::Delimited(...), where the\n     // entire lhs is those tts. Or, it can be a \"bare sequence\", not wrapped in parens.\n     match lhs {\n-        &TokenTree::Delimited(_, ref tts) => check_matcher(sess, &tts.tts),\n+        &quoted::TokenTree::Delimited(_, ref tts) => check_matcher(sess, &tts.tts),\n         _ => {\n             let msg = \"invalid macro matcher; matchers must be contained in balanced delimiters\";\n-            sess.span_diagnostic.span_err(lhs.get_span(), msg);\n+            sess.span_diagnostic.span_err(lhs.span(), msg);\n             false\n         }\n     }\n@@ -266,7 +268,8 @@ fn check_lhs_nt_follows(sess: &ParseSess, lhs: &TokenTree) -> bool {\n \n /// Check that the lhs contains no repetition which could match an empty token\n /// tree, because then the matcher would hang indefinitely.\n-fn check_lhs_no_empty_seq(sess: &ParseSess, tts: &[TokenTree]) -> bool {\n+fn check_lhs_no_empty_seq(sess: &ParseSess, tts: &[quoted::TokenTree]) -> bool {\n+    use self::quoted::TokenTree;\n     for tt in tts {\n         match *tt {\n             TokenTree::Token(_, _) => (),\n@@ -278,7 +281,7 @@ fn check_lhs_no_empty_seq(sess: &ParseSess, tts: &[TokenTree]) -> bool {\n                     if seq.tts.iter().all(|seq_tt| {\n                         match *seq_tt {\n                             TokenTree::Sequence(_, ref sub_seq) =>\n-                                sub_seq.op == tokenstream::KleeneOp::ZeroOrMore,\n+                                sub_seq.op == quoted::KleeneOp::ZeroOrMore,\n                             _ => false,\n                         }\n                     }) {\n@@ -296,15 +299,15 @@ fn check_lhs_no_empty_seq(sess: &ParseSess, tts: &[TokenTree]) -> bool {\n     true\n }\n \n-fn check_rhs(sess: &ParseSess, rhs: &TokenTree) -> bool {\n+fn check_rhs(sess: &ParseSess, rhs: &quoted::TokenTree) -> bool {\n     match *rhs {\n-        TokenTree::Delimited(..) => return true,\n-        _ => sess.span_diagnostic.span_err(rhs.get_span(), \"macro rhs must be delimited\")\n+        quoted::TokenTree::Delimited(..) => return true,\n+        _ => sess.span_diagnostic.span_err(rhs.span(), \"macro rhs must be delimited\")\n     }\n     false\n }\n \n-fn check_matcher(sess: &ParseSess, matcher: &[TokenTree]) -> bool {\n+fn check_matcher(sess: &ParseSess, matcher: &[quoted::TokenTree]) -> bool {\n     let first_sets = FirstSets::new(matcher);\n     let empty_suffix = TokenSet::empty();\n     let err = sess.span_diagnostic.err_count();\n@@ -335,7 +338,9 @@ struct FirstSets {\n }\n \n impl FirstSets {\n-    fn new(tts: &[TokenTree]) -> FirstSets {\n+    fn new(tts: &[quoted::TokenTree]) -> FirstSets {\n+        use self::quoted::TokenTree;\n+\n         let mut sets = FirstSets { first: HashMap::new() };\n         build_recur(&mut sets, tts);\n         return sets;\n@@ -382,7 +387,7 @@ impl FirstSets {\n                         }\n \n                         // Reverse scan: Sequence comes before `first`.\n-                        if subfirst.maybe_empty || seq_rep.op == tokenstream::KleeneOp::ZeroOrMore {\n+                        if subfirst.maybe_empty || seq_rep.op == quoted::KleeneOp::ZeroOrMore {\n                             // If sequence is potentially empty, then\n                             // union them (preserving first emptiness).\n                             first.add_all(&TokenSet { maybe_empty: true, ..subfirst });\n@@ -401,7 +406,9 @@ impl FirstSets {\n \n     // walks forward over `tts` until all potential FIRST tokens are\n     // identified.\n-    fn first(&self, tts: &[TokenTree]) -> TokenSet {\n+    fn first(&self, tts: &[quoted::TokenTree]) -> TokenSet {\n+        use self::quoted::TokenTree;\n+\n         let mut first = TokenSet::empty();\n         for tt in tts.iter() {\n             assert!(first.maybe_empty);\n@@ -430,7 +437,7 @@ impl FirstSets {\n                             assert!(first.maybe_empty);\n                             first.add_all(subfirst);\n                             if subfirst.maybe_empty ||\n-                               seq_rep.op == tokenstream::KleeneOp::ZeroOrMore {\n+                               seq_rep.op == quoted::KleeneOp::ZeroOrMore {\n                                 // continue scanning for more first\n                                 // tokens, but also make sure we\n                                 // restore empty-tracking state\n@@ -549,9 +556,10 @@ impl TokenSet {\n // see `FirstSets::new`.\n fn check_matcher_core(sess: &ParseSess,\n                       first_sets: &FirstSets,\n-                      matcher: &[TokenTree],\n+                      matcher: &[quoted::TokenTree],\n                       follow: &TokenSet) -> TokenSet {\n     use print::pprust::token_to_string;\n+    use self::quoted::TokenTree;\n \n     let mut last = TokenSet::empty();\n "}, {"sha": "1170bcabb77e1da03088fd26e1cce7a6e30e615b", "filename": "src/libsyntax/ext/tt/quoted.rs", "status": "added", "additions": 230, "deletions": 0, "changes": 230, "blob_url": "https://github.com/rust-lang/rust/blob/d8b34e9a74a4e91c4283ba4002a050ac0150cec6/src%2Flibsyntax%2Fext%2Ftt%2Fquoted.rs", "raw_url": "https://github.com/rust-lang/rust/raw/d8b34e9a74a4e91c4283ba4002a050ac0150cec6/src%2Flibsyntax%2Fext%2Ftt%2Fquoted.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fext%2Ftt%2Fquoted.rs?ref=d8b34e9a74a4e91c4283ba4002a050ac0150cec6", "patch": "@@ -0,0 +1,230 @@\n+// Copyright 2017 The Rust Project Developers. See the COPYRIGHT\n+// file at the top-level directory of this distribution and at\n+// http://rust-lang.org/COPYRIGHT.\n+//\n+// Licensed under the Apache License, Version 2.0 <LICENSE-APACHE or\n+// http://www.apache.org/licenses/LICENSE-2.0> or the MIT license\n+// <LICENSE-MIT or http://opensource.org/licenses/MIT>, at your\n+// option. This file may not be copied, modified, or distributed\n+// except according to those terms.\n+\n+use ast;\n+use ext::tt::macro_parser;\n+use parse::{ParseSess, token};\n+use print::pprust;\n+use symbol::{keywords, Symbol};\n+use syntax_pos::{DUMMY_SP, Span, BytePos};\n+use tokenstream;\n+\n+use std::rc::Rc;\n+\n+#[derive(Clone, PartialEq, Eq, RustcEncodable, RustcDecodable, Hash, Debug)]\n+pub struct Delimited {\n+    pub delim: token::DelimToken,\n+    pub tts: Vec<TokenTree>,\n+}\n+\n+impl Delimited {\n+    pub fn open_token(&self) -> token::Token {\n+        token::OpenDelim(self.delim)\n+    }\n+\n+    pub fn close_token(&self) -> token::Token {\n+        token::CloseDelim(self.delim)\n+    }\n+\n+    pub fn open_tt(&self, span: Span) -> TokenTree {\n+        let open_span = match span {\n+            DUMMY_SP => DUMMY_SP,\n+            _ => Span { hi: span.lo + BytePos(self.delim.len() as u32), ..span },\n+        };\n+        TokenTree::Token(open_span, self.open_token())\n+    }\n+\n+    pub fn close_tt(&self, span: Span) -> TokenTree {\n+        let close_span = match span {\n+            DUMMY_SP => DUMMY_SP,\n+            _ => Span { lo: span.hi - BytePos(self.delim.len() as u32), ..span },\n+        };\n+        TokenTree::Token(close_span, self.close_token())\n+    }\n+}\n+\n+#[derive(Clone, PartialEq, Eq, RustcEncodable, RustcDecodable, Hash, Debug)]\n+pub struct SequenceRepetition {\n+    /// The sequence of token trees\n+    pub tts: Vec<TokenTree>,\n+    /// The optional separator\n+    pub separator: Option<token::Token>,\n+    /// Whether the sequence can be repeated zero (*), or one or more times (+)\n+    pub op: KleeneOp,\n+    /// The number of `MatchNt`s that appear in the sequence (and subsequences)\n+    pub num_captures: usize,\n+}\n+\n+/// A Kleene-style [repetition operator](http://en.wikipedia.org/wiki/Kleene_star)\n+/// for token sequences.\n+#[derive(Clone, PartialEq, Eq, RustcEncodable, RustcDecodable, Hash, Debug, Copy)]\n+pub enum KleeneOp {\n+    ZeroOrMore,\n+    OneOrMore,\n+}\n+\n+/// Similar to `tokenstream::TokenTree`, except that `$i`, `$i:ident`, and `$(...)`\n+/// are \"first-class\" token trees.\n+#[derive(Debug, Clone, PartialEq, Eq, RustcEncodable, RustcDecodable, Hash)]\n+pub enum TokenTree {\n+    Token(Span, token::Token),\n+    Delimited(Span, Rc<Delimited>),\n+    /// A kleene-style repetition sequence with a span\n+    Sequence(Span, Rc<SequenceRepetition>),\n+}\n+\n+impl TokenTree {\n+    pub fn len(&self) -> usize {\n+        match *self {\n+            TokenTree::Delimited(_, ref delimed) => match delimed.delim {\n+                token::NoDelim => delimed.tts.len(),\n+                _ => delimed.tts.len() + 2,\n+            },\n+            TokenTree::Sequence(_, ref seq) => seq.tts.len(),\n+            TokenTree::Token(..) => 0,\n+        }\n+    }\n+\n+    pub fn get_tt(&self, index: usize) -> TokenTree {\n+        match (self, index) {\n+            (&TokenTree::Delimited(_, ref delimed), _) if delimed.delim == token::NoDelim => {\n+                delimed.tts[index].clone()\n+            }\n+            (&TokenTree::Delimited(span, ref delimed), _) => {\n+                if index == 0 {\n+                    return delimed.open_tt(span);\n+                }\n+                if index == delimed.tts.len() + 1 {\n+                    return delimed.close_tt(span);\n+                }\n+                delimed.tts[index - 1].clone()\n+            }\n+            (&TokenTree::Sequence(_, ref seq), _) => seq.tts[index].clone(),\n+            _ => panic!(\"Cannot expand a token tree\"),\n+        }\n+    }\n+\n+    /// Retrieve the TokenTree's span.\n+    pub fn span(&self) -> Span {\n+        match *self {\n+            TokenTree::Token(sp, _) |\n+            TokenTree::Delimited(sp, _) |\n+            TokenTree::Sequence(sp, _) => sp,\n+        }\n+    }\n+}\n+\n+pub fn parse(input: &[tokenstream::TokenTree], expect_matchers: bool, sess: &ParseSess)\n+             -> Vec<TokenTree> {\n+    let mut result = Vec::new();\n+    let mut trees = input.iter().cloned();\n+    while let Some(tree) = trees.next() {\n+        let tree = parse_tree(tree, &mut trees, expect_matchers, sess);\n+        match tree {\n+            TokenTree::Token(start_sp, token::SubstNt(ident)) if expect_matchers => {\n+                let span = match trees.next() {\n+                    Some(tokenstream::TokenTree::Token(span, token::Colon)) => match trees.next() {\n+                        Some(tokenstream::TokenTree::Token(end_sp, token::Ident(kind))) => {\n+                            let span = Span { lo: start_sp.lo, ..end_sp };\n+                            result.push(TokenTree::Token(span, token::MatchNt(ident, kind)));\n+                            continue\n+                        }\n+                        tree @ _ => tree.as_ref().map(tokenstream::TokenTree::span).unwrap_or(span),\n+                    },\n+                    tree @ _ => tree.as_ref().map(tokenstream::TokenTree::span).unwrap_or(start_sp),\n+                };\n+                sess.span_diagnostic.span_err(span, \"missing fragment specifier\");\n+            }\n+            _ => result.push(tree),\n+        }\n+    }\n+    result\n+}\n+\n+fn parse_tree<I>(tree: tokenstream::TokenTree,\n+                 trees: &mut I,\n+                 expect_matchers: bool,\n+                 sess: &ParseSess)\n+                 -> TokenTree\n+    where I: Iterator<Item = tokenstream::TokenTree>,\n+{\n+    match tree {\n+        tokenstream::TokenTree::Token(span, token::Dollar) => match trees.next() {\n+            Some(tokenstream::TokenTree::Delimited(span, ref delimited)) => {\n+                if delimited.delim != token::Paren {\n+                    let tok = pprust::token_to_string(&token::OpenDelim(delimited.delim));\n+                    let msg = format!(\"expected `(`, found `{}`\", tok);\n+                    sess.span_diagnostic.span_err(span, &msg);\n+                }\n+                let sequence = parse(&delimited.tts, expect_matchers, sess);\n+                let (separator, op) = parse_sep_and_kleene_op(trees, span, sess);\n+                let name_captures = macro_parser::count_names(&sequence);\n+                TokenTree::Sequence(span, Rc::new(SequenceRepetition {\n+                    tts: sequence,\n+                    separator: separator,\n+                    op: op,\n+                    num_captures: name_captures,\n+                }))\n+            }\n+            Some(tokenstream::TokenTree::Token(ident_span, token::Ident(ident))) => {\n+                let span = Span { lo: span.lo, ..ident_span };\n+                if ident.name == keywords::Crate.name() {\n+                    let ident = ast::Ident { name: Symbol::intern(\"$crate\"), ..ident };\n+                    TokenTree::Token(span, token::Ident(ident))\n+                } else {\n+                    TokenTree::Token(span, token::SubstNt(ident))\n+                }\n+            }\n+            Some(tokenstream::TokenTree::Token(span, tok)) => {\n+                let msg = format!(\"expected identifier, found `{}`\", pprust::token_to_string(&tok));\n+                sess.span_diagnostic.span_err(span, &msg);\n+                TokenTree::Token(span, token::SubstNt(keywords::Invalid.ident()))\n+            }\n+            None => TokenTree::Token(span, token::Dollar),\n+        },\n+        tokenstream::TokenTree::Token(span, tok) => TokenTree::Token(span, tok),\n+        tokenstream::TokenTree::Delimited(span, delimited) => {\n+            TokenTree::Delimited(span, Rc::new(Delimited {\n+                delim: delimited.delim,\n+                tts: parse(&delimited.tts, expect_matchers, sess),\n+            }))\n+        }\n+    }\n+}\n+\n+fn parse_sep_and_kleene_op<I>(input: &mut I, span: Span, sess: &ParseSess)\n+                              -> (Option<token::Token>, KleeneOp)\n+    where I: Iterator<Item = tokenstream::TokenTree>,\n+{\n+    fn kleene_op(token: &token::Token) -> Option<KleeneOp> {\n+        match *token {\n+            token::BinOp(token::Star) => Some(KleeneOp::ZeroOrMore),\n+            token::BinOp(token::Plus) => Some(KleeneOp::OneOrMore),\n+            _ => None,\n+        }\n+    }\n+\n+    let span = match input.next() {\n+        Some(tokenstream::TokenTree::Token(span, tok)) => match kleene_op(&tok) {\n+            Some(op) => return (None, op),\n+            None => match input.next() {\n+                Some(tokenstream::TokenTree::Token(span, tok2)) => match kleene_op(&tok2) {\n+                    Some(op) => return (Some(tok), op),\n+                    None => span,\n+                },\n+                tree @ _ => tree.as_ref().map(tokenstream::TokenTree::span).unwrap_or(span),\n+            }\n+        },\n+        tree @ _ => tree.as_ref().map(tokenstream::TokenTree::span).unwrap_or(span),\n+    };\n+\n+    sess.span_diagnostic.span_err(span, \"expected `*` or `+`\");\n+    (None, KleeneOp::ZeroOrMore)\n+}"}, {"sha": "856294433a8b38e82abfb276a1aa5dd7801bdb8e", "filename": "src/libsyntax/ext/tt/transcribe.rs", "status": "modified", "additions": 17, "deletions": 34, "changes": 51, "blob_url": "https://github.com/rust-lang/rust/blob/d8b34e9a74a4e91c4283ba4002a050ac0150cec6/src%2Flibsyntax%2Fext%2Ftt%2Ftranscribe.rs", "raw_url": "https://github.com/rust-lang/rust/raw/d8b34e9a74a4e91c4283ba4002a050ac0150cec6/src%2Flibsyntax%2Fext%2Ftt%2Ftranscribe.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fext%2Ftt%2Ftranscribe.rs?ref=d8b34e9a74a4e91c4283ba4002a050ac0150cec6", "patch": "@@ -11,9 +11,10 @@\n use ast::Ident;\n use errors::Handler;\n use ext::tt::macro_parser::{NamedMatch, MatchedSeq, MatchedNonterminal};\n+use ext::tt::quoted;\n use parse::token::{self, MatchNt, SubstNt, Token, NtIdent, NtTT};\n use syntax_pos::{Span, DUMMY_SP};\n-use tokenstream::{self, TokenTree, Delimited, SequenceRepetition};\n+use tokenstream::{TokenTree, Delimited};\n use util::small_vector::SmallVector;\n \n use std::rc::Rc;\n@@ -24,34 +25,28 @@ use std::collections::HashMap;\n // An iterator over the token trees in a delimited token tree (`{ ... }`) or a sequence (`$(...)`).\n enum Frame {\n     Delimited {\n-        forest: Rc<Delimited>,\n-        idx: usize,\n-        span: Span,\n-    },\n-    MatchNt {\n-        name: Ident,\n-        kind: Ident,\n+        forest: Rc<quoted::Delimited>,\n         idx: usize,\n         span: Span,\n     },\n     Sequence {\n-        forest: Rc<SequenceRepetition>,\n+        forest: Rc<quoted::SequenceRepetition>,\n         idx: usize,\n         sep: Option<Token>,\n     },\n }\n \n impl Frame {\n-    fn new(tts: Vec<TokenTree>) -> Frame {\n-        let forest = Rc::new(tokenstream::Delimited { delim: token::NoDelim, tts: tts });\n+    fn new(tts: Vec<quoted::TokenTree>) -> Frame {\n+        let forest = Rc::new(quoted::Delimited { delim: token::NoDelim, tts: tts });\n         Frame::Delimited { forest: forest, idx: 0, span: DUMMY_SP }\n     }\n }\n \n impl Iterator for Frame {\n-    type Item = TokenTree;\n+    type Item = quoted::TokenTree;\n \n-    fn next(&mut self) -> Option<TokenTree> {\n+    fn next(&mut self) -> Option<quoted::TokenTree> {\n         match *self {\n             Frame::Delimited { ref forest, ref mut idx, .. } => {\n                 *idx += 1;\n@@ -61,15 +56,6 @@ impl Iterator for Frame {\n                 *idx += 1;\n                 forest.tts.get(*idx - 1).cloned()\n             }\n-            Frame::MatchNt { ref mut idx, name, kind, span } => {\n-                *idx += 1;\n-                match *idx {\n-                    1 => Some(TokenTree::Token(span, token::SubstNt(name))),\n-                    2 => Some(TokenTree::Token(span, token::Colon)),\n-                    3 => Some(TokenTree::Token(span, token::Ident(kind))),\n-                    _ => None,\n-                }\n-            }\n         }\n     }\n }\n@@ -79,7 +65,7 @@ impl Iterator for Frame {\n /// (and should) be None.\n pub fn transcribe(sp_diag: &Handler,\n                   interp: Option<HashMap<Ident, Rc<NamedMatch>>>,\n-                  src: Vec<tokenstream::TokenTree>)\n+                  src: Vec<quoted::TokenTree>)\n                   -> Vec<TokenTree> {\n     let mut stack = SmallVector::one(Frame::new(src));\n     let interpolations = interp.unwrap_or_else(HashMap::new); /* just a convenience */\n@@ -121,15 +107,14 @@ pub fn transcribe(sp_diag: &Handler,\n                     result = result_stack.pop().unwrap();\n                     result.push(tree);\n                 }\n-                _ => {}\n             }\n             continue\n         };\n \n         match tree {\n-            TokenTree::Sequence(sp, seq) => {\n+            quoted::TokenTree::Sequence(sp, seq) => {\n                 // FIXME(pcwalton): Bad copy.\n-                match lockstep_iter_size(&TokenTree::Sequence(sp, seq.clone()),\n+                match lockstep_iter_size(&quoted::TokenTree::Sequence(sp, seq.clone()),\n                                          &interpolations,\n                                          &repeat_idx) {\n                     LockstepIterSize::Unconstrained => {\n@@ -145,7 +130,7 @@ pub fn transcribe(sp_diag: &Handler,\n                     }\n                     LockstepIterSize::Constraint(len, _) => {\n                         if len == 0 {\n-                            if seq.op == tokenstream::KleeneOp::OneOrMore {\n+                            if seq.op == quoted::KleeneOp::OneOrMore {\n                                 // FIXME #2887 blame invoker\n                                 panic!(sp_diag.span_fatal(sp.clone(),\n                                                           \"this must repeat at least once\"));\n@@ -163,7 +148,7 @@ pub fn transcribe(sp_diag: &Handler,\n                 }\n             }\n             // FIXME #2887: think about span stuff here\n-            TokenTree::Token(sp, SubstNt(ident)) => {\n+            quoted::TokenTree::Token(sp, SubstNt(ident)) => {\n                 match lookup_cur_matched(ident, &interpolations, &repeat_idx) {\n                     None => result.push(TokenTree::Token(sp, SubstNt(ident))),\n                     Some(cur_matched) => if let MatchedNonterminal(ref nt) = *cur_matched {\n@@ -187,14 +172,11 @@ pub fn transcribe(sp_diag: &Handler,\n                     }\n                 }\n             }\n-            TokenTree::Delimited(span, delimited) => {\n+            quoted::TokenTree::Delimited(span, delimited) => {\n                 stack.push(Frame::Delimited { forest: delimited, idx: 0, span: span });\n                 result_stack.push(mem::replace(&mut result, Vec::new()));\n             }\n-            TokenTree::Token(span, MatchNt(name, kind)) => {\n-                stack.push(Frame::MatchNt { name: name, kind: kind, idx: 0, span: span });\n-            }\n-            tt @ TokenTree::Token(..) => result.push(tt),\n+            quoted::TokenTree::Token(span, tok) => result.push(TokenTree::Token(span, tok)),\n         }\n     }\n }\n@@ -245,10 +227,11 @@ impl Add for LockstepIterSize {\n     }\n }\n \n-fn lockstep_iter_size(tree: &TokenTree,\n+fn lockstep_iter_size(tree: &quoted::TokenTree,\n                       interpolations: &HashMap<Ident, Rc<NamedMatch>>,\n                       repeat_idx: &[usize])\n                       -> LockstepIterSize {\n+    use self::quoted::TokenTree;\n     match *tree {\n         TokenTree::Delimited(_, ref delimed) => {\n             delimed.tts.iter().fold(LockstepIterSize::Unconstrained, |size, tt| {"}, {"sha": "c33d945d60e15189ed0b976d48e3c7ef60257b20", "filename": "src/libsyntax/fold.rs", "status": "modified", "additions": 0, "deletions": 7, "changes": 7, "blob_url": "https://github.com/rust-lang/rust/blob/d8b34e9a74a4e91c4283ba4002a050ac0150cec6/src%2Flibsyntax%2Ffold.rs", "raw_url": "https://github.com/rust-lang/rust/raw/d8b34e9a74a4e91c4283ba4002a050ac0150cec6/src%2Flibsyntax%2Ffold.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Ffold.rs?ref=d8b34e9a74a4e91c4283ba4002a050ac0150cec6", "patch": "@@ -551,13 +551,6 @@ pub fn noop_fold_tt<T: Folder>(tt: &TokenTree, fld: &mut T) -> TokenTree {\n                             }\n                         ))\n         },\n-        TokenTree::Sequence(span, ref seq) =>\n-            TokenTree::Sequence(fld.new_span(span),\n-                       Rc::new(SequenceRepetition {\n-                           tts: fld.fold_tts(&seq.tts),\n-                           separator: seq.separator.clone().map(|tok| fld.fold_token(tok)),\n-                           ..**seq\n-                       })),\n     }\n }\n "}, {"sha": "39a9aff48bf27163d0b8dcea76916863b54bd7de", "filename": "src/libsyntax/lib.rs", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "blob_url": "https://github.com/rust-lang/rust/blob/d8b34e9a74a4e91c4283ba4002a050ac0150cec6/src%2Flibsyntax%2Flib.rs", "raw_url": "https://github.com/rust-lang/rust/raw/d8b34e9a74a4e91c4283ba4002a050ac0150cec6/src%2Flibsyntax%2Flib.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Flib.rs?ref=d8b34e9a74a4e91c4283ba4002a050ac0150cec6", "patch": "@@ -139,6 +139,7 @@ pub mod ext {\n         pub mod transcribe;\n         pub mod macro_parser;\n         pub mod macro_rules;\n+        pub mod quoted;\n     }\n }\n "}, {"sha": "78fd706b27ac5270ef791af0d993818bfd2cfef4", "filename": "src/libsyntax/parse/mod.rs", "status": "modified", "additions": 3, "deletions": 7, "changes": 10, "blob_url": "https://github.com/rust-lang/rust/blob/d8b34e9a74a4e91c4283ba4002a050ac0150cec6/src%2Flibsyntax%2Fparse%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/d8b34e9a74a4e91c4283ba4002a050ac0150cec6/src%2Flibsyntax%2Fparse%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Fmod.rs?ref=d8b34e9a74a4e91c4283ba4002a050ac0150cec6", "patch": "@@ -139,13 +139,9 @@ pub fn parse_stmt_from_source_str<'a>(name: String, source: String, sess: &'a Pa\n     new_parser_from_source_str(sess, name, source).parse_stmt()\n }\n \n-// Warning: This parses with quote_depth > 0, which is not the default.\n pub fn parse_tts_from_source_str<'a>(name: String, source: String, sess: &'a ParseSess)\n-                                     -> PResult<'a, Vec<tokenstream::TokenTree>> {\n-    let mut p = new_parser_from_source_str(sess, name, source);\n-    p.quote_depth += 1;\n-    // right now this is re-creating the token trees from ... token trees.\n-    p.parse_all_token_trees()\n+                                     -> Vec<tokenstream::TokenTree> {\n+    filemap_to_tts(sess, sess.codemap().new_filemap(name, None, source))\n }\n \n // Create a new parser from a source string\n@@ -986,7 +982,7 @@ mod tests {\n             _ => panic!(\"not a macro\"),\n         };\n \n-        let span = tts.iter().rev().next().unwrap().get_span();\n+        let span = tts.iter().rev().next().unwrap().span();\n \n         match sess.codemap().span_to_snippet(span) {\n             Ok(s) => assert_eq!(&s[..], \"{ body }\"),"}, {"sha": "ab965e2763300a441ed379f07944f1ce6f33bee0", "filename": "src/libsyntax/parse/parser.rs", "status": "modified", "additions": 13, "deletions": 148, "changes": 161, "blob_url": "https://github.com/rust-lang/rust/blob/d8b34e9a74a4e91c4283ba4002a050ac0150cec6/src%2Flibsyntax%2Fparse%2Fparser.rs", "raw_url": "https://github.com/rust-lang/rust/raw/d8b34e9a74a4e91c4283ba4002a050ac0150cec6/src%2Flibsyntax%2Fparse%2Fparser.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Fparser.rs?ref=d8b34e9a74a4e91c4283ba4002a050ac0150cec6", "patch": "@@ -43,19 +43,16 @@ use {ast, attr};\n use codemap::{self, CodeMap, Spanned, spanned, respan};\n use syntax_pos::{self, Span, Pos, BytePos, mk_sp};\n use errors::{self, DiagnosticBuilder};\n-use ext::tt::macro_parser;\n-use parse;\n-use parse::classify;\n+use parse::{self, classify, token};\n use parse::common::SeqSep;\n use parse::lexer::TokenAndSpan;\n use parse::obsolete::ObsoleteSyntax;\n-use parse::token::{self, MatchNt, SubstNt};\n use parse::{new_sub_parser_from_file, ParseSess, Directory, DirectoryOwnership};\n use util::parser::{AssocOp, Fixity};\n use print::pprust;\n use ptr::P;\n use parse::PResult;\n-use tokenstream::{self, Delimited, SequenceRepetition, TokenTree};\n+use tokenstream::{Delimited, TokenTree};\n use symbol::{Symbol, keywords};\n use util::ThinVec;\n \n@@ -168,8 +165,6 @@ pub struct Parser<'a> {\n     /// the previous token kind\n     prev_token_kind: PrevTokenKind,\n     pub restrictions: Restrictions,\n-    pub quote_depth: usize, // not (yet) related to the quasiquoter\n-    parsing_token_tree: bool,\n     /// The set of seen errors about obsolete syntax. Used to suppress\n     /// extra detail when the same error is seen twice\n     pub obsolete_set: HashSet<ObsoleteSyntax>,\n@@ -329,8 +324,6 @@ impl<'a> Parser<'a> {\n             prev_span: syntax_pos::DUMMY_SP,\n             prev_token_kind: PrevTokenKind::Other,\n             restrictions: Restrictions::empty(),\n-            quote_depth: 0,\n-            parsing_token_tree: false,\n             obsolete_set: HashSet::new(),\n             directory: Directory { path: PathBuf::new(), ownership: DirectoryOwnership::Owned },\n             root_module_name: None,\n@@ -359,20 +352,11 @@ impl<'a> Parser<'a> {\n                 if i + 1 < tts.len() {\n                     self.tts.push((tts, i + 1));\n                 }\n-                // FIXME(jseyfried): remove after fixing #39390 in #39419.\n-                if self.quote_depth > 0 {\n-                    if let TokenTree::Sequence(sp, _) = tt {\n-                        self.span_err(sp, \"attempted to repeat an expression containing no \\\n-                                           syntax variables matched as repeating at this depth\");\n-                    }\n-                }\n-                match tt {\n-                    TokenTree::Token(sp, tok) => TokenAndSpan { tok: tok, sp: sp },\n-                    _ if tt.len() > 0 => {\n-                        self.tts.push((tt, 0));\n-                        continue\n-                    }\n-                    _ => continue,\n+                if let TokenTree::Token(sp, tok) = tt {\n+                    TokenAndSpan { tok: tok, sp: sp }\n+                } else {\n+                    self.tts.push((tt, 0));\n+                    continue\n                 }\n             } else {\n                 TokenAndSpan { tok: token::Eof, sp: self.span }\n@@ -997,7 +981,6 @@ impl<'a> Parser<'a> {\n                 tok = match tts.get_tt(i) {\n                     TokenTree::Token(_, tok) => tok,\n                     TokenTree::Delimited(_, delimited) => token::OpenDelim(delimited.delim),\n-                    TokenTree::Sequence(..) => token::Dollar,\n                 };\n             }\n         }\n@@ -2586,139 +2569,21 @@ impl<'a> Parser<'a> {\n         return Ok(e);\n     }\n \n-    // Parse unquoted tokens after a `$` in a token tree\n-    fn parse_unquoted(&mut self) -> PResult<'a, TokenTree> {\n-        let mut sp = self.span;\n-        let name = match self.token {\n-            token::Dollar => {\n-                self.bump();\n-\n-                if self.token == token::OpenDelim(token::Paren) {\n-                    let Spanned { node: seq, span: seq_span } = self.parse_seq(\n-                        &token::OpenDelim(token::Paren),\n-                        &token::CloseDelim(token::Paren),\n-                        SeqSep::none(),\n-                        |p| p.parse_token_tree()\n-                    )?;\n-                    let (sep, repeat) = self.parse_sep_and_kleene_op()?;\n-                    let name_num = macro_parser::count_names(&seq);\n-                    return Ok(TokenTree::Sequence(mk_sp(sp.lo, seq_span.hi),\n-                                      Rc::new(SequenceRepetition {\n-                                          tts: seq,\n-                                          separator: sep,\n-                                          op: repeat,\n-                                          num_captures: name_num\n-                                      })));\n-                } else if self.token.is_keyword(keywords::Crate) {\n-                    let ident = match self.token {\n-                        token::Ident(id) => ast::Ident { name: Symbol::intern(\"$crate\"), ..id },\n-                        _ => unreachable!(),\n-                    };\n-                    self.bump();\n-                    return Ok(TokenTree::Token(sp, token::Ident(ident)));\n-                } else {\n-                    sp = mk_sp(sp.lo, self.span.hi);\n-                    self.parse_ident().unwrap_or_else(|mut e| {\n-                        e.emit();\n-                        keywords::Invalid.ident()\n-                    })\n-                }\n-            }\n-            token::SubstNt(name) => {\n-                self.bump();\n-                name\n-            }\n-            _ => unreachable!()\n-        };\n-        // continue by trying to parse the `:ident` after `$name`\n-        if self.token == token::Colon &&\n-                self.look_ahead(1, |t| t.is_ident() && !t.is_any_keyword()) {\n-            self.bump();\n-            sp = mk_sp(sp.lo, self.span.hi);\n-            let nt_kind = self.parse_ident()?;\n-            Ok(TokenTree::Token(sp, MatchNt(name, nt_kind)))\n-        } else {\n-            Ok(TokenTree::Token(sp, SubstNt(name)))\n-        }\n-    }\n-\n     pub fn check_unknown_macro_variable(&mut self) {\n-        if self.quote_depth == 0 && !self.parsing_token_tree {\n-            match self.token {\n-                token::SubstNt(name) =>\n-                    self.fatal(&format!(\"unknown macro variable `{}`\", name)).emit(),\n-                _ => {}\n-            }\n-        }\n-    }\n-\n-    /// Parse an optional separator followed by a Kleene-style\n-    /// repetition token (+ or *).\n-    pub fn parse_sep_and_kleene_op(&mut self)\n-                                   -> PResult<'a, (Option<token::Token>, tokenstream::KleeneOp)> {\n-        fn parse_kleene_op<'a>(parser: &mut Parser<'a>) ->\n-          PResult<'a,  Option<tokenstream::KleeneOp>> {\n-            match parser.token {\n-                token::BinOp(token::Star) => {\n-                    parser.bump();\n-                    Ok(Some(tokenstream::KleeneOp::ZeroOrMore))\n-                },\n-                token::BinOp(token::Plus) => {\n-                    parser.bump();\n-                    Ok(Some(tokenstream::KleeneOp::OneOrMore))\n-                },\n-                _ => Ok(None)\n-            }\n-        };\n-\n-        if let Some(kleene_op) = parse_kleene_op(self)? {\n-            return Ok((None, kleene_op));\n-        }\n-\n-        let separator = match self.token {\n-            token::CloseDelim(..) => None,\n-            _ => Some(self.bump_and_get()),\n-        };\n-        match parse_kleene_op(self)? {\n-            Some(zerok) => Ok((separator, zerok)),\n-            None => return Err(self.fatal(\"expected `*` or `+`\"))\n+        if let token::SubstNt(name) = self.token {\n+            self.fatal(&format!(\"unknown macro variable `{}`\", name)).emit()\n         }\n     }\n \n     /// parse a single token tree from the input.\n     pub fn parse_token_tree(&mut self) -> PResult<'a, TokenTree> {\n-        // FIXME #6994: currently, this is too eager. It\n-        // parses token trees but also identifies TokenType::Sequence's\n-        // and token::SubstNt's; it's too early to know yet\n-        // whether something will be a nonterminal or a seq\n-        // yet.\n         match self.token {\n-            token::OpenDelim(delim) => {\n-                if self.quote_depth == 0 {\n-                    let tt = self.tts.pop().unwrap().0;\n-                    self.bump();\n-                    return Ok(tt);\n-                }\n-\n-                let parsing_token_tree = ::std::mem::replace(&mut self.parsing_token_tree, true);\n-                let lo = self.span.lo;\n-                self.bump();\n-                let tts = self.parse_seq_to_before_tokens(&[&token::CloseDelim(token::Brace),\n-                                                            &token::CloseDelim(token::Paren),\n-                                                            &token::CloseDelim(token::Bracket)],\n-                                                          SeqSep::none(),\n-                                                          |p| p.parse_token_tree(),\n-                                                          |mut e| e.emit());\n-                self.parsing_token_tree = parsing_token_tree;\n+            token::OpenDelim(..) => {\n+                let tt = self.tts.pop().unwrap().0;\n                 self.bump();\n-\n-                Ok(TokenTree::Delimited(Span { lo: lo, ..self.prev_span }, Rc::new(Delimited {\n-                    delim: delim,\n-                    tts: tts,\n-                })))\n+                return Ok(tt);\n             },\n-            token::CloseDelim(..) | token::Eof => Ok(TokenTree::Token(self.span, token::Eof)),\n-            token::Dollar | token::SubstNt(..) if self.quote_depth > 0 => self.parse_unquoted(),\n+            token::CloseDelim(_) | token::Eof => unreachable!(),\n             _ => Ok(TokenTree::Token(self.span, self.bump_and_get())),\n         }\n     }"}, {"sha": "593d551046b788e375e011645e07d01b39983e87", "filename": "src/libsyntax/print/pprust.rs", "status": "modified", "additions": 0, "deletions": 14, "changes": 14, "blob_url": "https://github.com/rust-lang/rust/blob/d8b34e9a74a4e91c4283ba4002a050ac0150cec6/src%2Flibsyntax%2Fprint%2Fpprust.rs", "raw_url": "https://github.com/rust-lang/rust/raw/d8b34e9a74a4e91c4283ba4002a050ac0150cec6/src%2Flibsyntax%2Fprint%2Fpprust.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fprint%2Fpprust.rs?ref=d8b34e9a74a4e91c4283ba4002a050ac0150cec6", "patch": "@@ -1475,20 +1475,6 @@ impl<'a> State<'a> {\n                 space(&mut self.s)?;\n                 word(&mut self.s, &token_to_string(&delimed.close_token()))\n             },\n-            TokenTree::Sequence(_, ref seq) => {\n-                word(&mut self.s, \"$(\")?;\n-                for tt_elt in &seq.tts {\n-                    self.print_tt(tt_elt)?;\n-                }\n-                word(&mut self.s, \")\")?;\n-                if let Some(ref tk) = seq.separator {\n-                    word(&mut self.s, &token_to_string(tk))?;\n-                }\n-                match seq.op {\n-                    tokenstream::KleeneOp::ZeroOrMore => word(&mut self.s, \"*\"),\n-                    tokenstream::KleeneOp::OneOrMore => word(&mut self.s, \"+\"),\n-                }\n-            }\n         }\n     }\n "}, {"sha": "666540467213326d13bc18a0ac7a2691793897e6", "filename": "src/libsyntax/tokenstream.rs", "status": "modified", "additions": 5, "deletions": 57, "changes": 62, "blob_url": "https://github.com/rust-lang/rust/blob/d8b34e9a74a4e91c4283ba4002a050ac0150cec6/src%2Flibsyntax%2Ftokenstream.rs", "raw_url": "https://github.com/rust-lang/rust/raw/d8b34e9a74a4e91c4283ba4002a050ac0150cec6/src%2Flibsyntax%2Ftokenstream.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Ftokenstream.rs?ref=d8b34e9a74a4e91c4283ba4002a050ac0150cec6", "patch": "@@ -12,9 +12,7 @@\n //!\n //! TokenStreams represent syntactic objects before they are converted into ASTs.\n //! A `TokenStream` is, roughly speaking, a sequence (eg stream) of `TokenTree`s,\n-//! which are themselves either a single Token, a Delimited subsequence of tokens,\n-//! or a SequenceRepetition specifier (for the purpose of sequence generation during macro\n-//! expansion).\n+//! which are themselves a single `Token` or a `Delimited` subsequence of tokens.\n //!\n //! ## Ownership\n //! TokenStreams are persistent data structures constructed as ropes with reference\n@@ -28,10 +26,10 @@ use ast::{self, AttrStyle, LitKind};\n use syntax_pos::{BytePos, Span, DUMMY_SP};\n use codemap::Spanned;\n use ext::base;\n-use ext::tt::macro_parser;\n+use ext::tt::{macro_parser, quoted};\n use parse::lexer::comments::{doc_comment_style, strip_doc_comment_decoration};\n use parse::{self, Directory};\n-use parse::token::{self, Token, Lit, Nonterminal};\n+use parse::token::{self, Token, Lit};\n use print::pprust;\n use serialize::{Decoder, Decodable, Encoder, Encodable};\n use symbol::Symbol;\n@@ -84,27 +82,6 @@ impl Delimited {\n     }\n }\n \n-/// A sequence of token trees\n-#[derive(Clone, PartialEq, Eq, RustcEncodable, RustcDecodable, Hash, Debug)]\n-pub struct SequenceRepetition {\n-    /// The sequence of token trees\n-    pub tts: Vec<TokenTree>,\n-    /// The optional separator\n-    pub separator: Option<token::Token>,\n-    /// Whether the sequence can be repeated zero (*), or one or more times (+)\n-    pub op: KleeneOp,\n-    /// The number of `MatchNt`s that appear in the sequence (and subsequences)\n-    pub num_captures: usize,\n-}\n-\n-/// A Kleene-style [repetition operator](http://en.wikipedia.org/wiki/Kleene_star)\n-/// for token sequences.\n-#[derive(Clone, PartialEq, Eq, RustcEncodable, RustcDecodable, Hash, Debug, Copy)]\n-pub enum KleeneOp {\n-    ZeroOrMore,\n-    OneOrMore,\n-}\n-\n /// When the main rust parser encounters a syntax-extension invocation, it\n /// parses the arguments to the invocation as a token-tree. This is a very\n /// loose structure, such that all sorts of different AST-fragments can\n@@ -123,10 +100,6 @@ pub enum TokenTree {\n     Token(Span, token::Token),\n     /// A delimited sequence of token trees\n     Delimited(Span, Rc<Delimited>),\n-\n-    // This only makes sense in MBE macros.\n-    /// A kleene-style repetition sequence with a span\n-    Sequence(Span, Rc<SequenceRepetition>),\n }\n \n impl TokenTree {\n@@ -138,15 +111,10 @@ impl TokenTree {\n                     AttrStyle::Inner => 3,\n                 }\n             }\n-            TokenTree::Token(_, token::Interpolated(ref nt)) => {\n-                if let Nonterminal::NtTT(..) = **nt { 1 } else { 0 }\n-            },\n-            TokenTree::Token(_, token::MatchNt(..)) => 3,\n             TokenTree::Delimited(_, ref delimed) => match delimed.delim {\n                 token::NoDelim => delimed.tts.len(),\n                 _ => delimed.tts.len() + 2,\n             },\n-            TokenTree::Sequence(_, ref seq) => seq.tts.len(),\n             TokenTree::Token(..) => 0,\n         }\n     }\n@@ -197,30 +165,12 @@ impl TokenTree {\n                 }\n                 delimed.tts[index - 1].clone()\n             }\n-            (&TokenTree::Token(sp, token::MatchNt(name, kind)), _) => {\n-                let v = [TokenTree::Token(sp, token::SubstNt(name)),\n-                         TokenTree::Token(sp, token::Colon),\n-                         TokenTree::Token(sp, token::Ident(kind))];\n-                v[index].clone()\n-            }\n-            (&TokenTree::Sequence(_, ref seq), _) => seq.tts[index].clone(),\n             _ => panic!(\"Cannot expand a token tree\"),\n         }\n     }\n \n-    /// Returns the `Span` corresponding to this token tree.\n-    pub fn get_span(&self) -> Span {\n-        match *self {\n-            TokenTree::Token(span, _) => span,\n-            TokenTree::Delimited(span, _) => span,\n-            TokenTree::Sequence(span, _) => span,\n-        }\n-    }\n-\n     /// Use this token tree as a matcher to parse given tts.\n-    pub fn parse(cx: &base::ExtCtxt,\n-                 mtch: &[TokenTree],\n-                 tts: &[TokenTree])\n+    pub fn parse(cx: &base::ExtCtxt, mtch: &[quoted::TokenTree], tts: &[TokenTree])\n                  -> macro_parser::NamedParseResult {\n         // `None` is because we're not interpolating\n         let directory = Directory {\n@@ -252,9 +202,7 @@ impl TokenTree {\n     /// Retrieve the TokenTree's span.\n     pub fn span(&self) -> Span {\n         match *self {\n-            TokenTree::Token(sp, _) |\n-            TokenTree::Delimited(sp, _) |\n-            TokenTree::Sequence(sp, _) => sp,\n+            TokenTree::Token(sp, _) | TokenTree::Delimited(sp, _) => sp,\n         }\n     }\n "}, {"sha": "f92cde4019f67b06e9147c6264badd7e4e41cfac", "filename": "src/libsyntax_ext/lib.rs", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "blob_url": "https://github.com/rust-lang/rust/blob/d8b34e9a74a4e91c4283ba4002a050ac0150cec6/src%2Flibsyntax_ext%2Flib.rs", "raw_url": "https://github.com/rust-lang/rust/raw/d8b34e9a74a4e91c4283ba4002a050ac0150cec6/src%2Flibsyntax_ext%2Flib.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax_ext%2Flib.rs?ref=d8b34e9a74a4e91c4283ba4002a050ac0150cec6", "patch": "@@ -79,7 +79,6 @@ pub fn register_builtins(resolver: &mut syntax::ext::base::Resolver,\n             quote_pat: expand_quote_pat,\n             quote_arm: expand_quote_arm,\n             quote_stmt: expand_quote_stmt,\n-            quote_matcher: expand_quote_matcher,\n             quote_attr: expand_quote_attr,\n             quote_arg: expand_quote_arg,\n             quote_block: expand_quote_block,"}, {"sha": "63e1c6f16b3e6be4b8ac4727833ef6f6cf182d9c", "filename": "src/test/compile-fail-fulldeps/gated-quote.rs", "status": "modified", "additions": 0, "deletions": 2, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/d8b34e9a74a4e91c4283ba4002a050ac0150cec6/src%2Ftest%2Fcompile-fail-fulldeps%2Fgated-quote.rs", "raw_url": "https://github.com/rust-lang/rust/raw/d8b34e9a74a4e91c4283ba4002a050ac0150cec6/src%2Ftest%2Fcompile-fail-fulldeps%2Fgated-quote.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ftest%2Fcompile-fail-fulldeps%2Fgated-quote.rs?ref=d8b34e9a74a4e91c4283ba4002a050ac0150cec6", "patch": "@@ -54,8 +54,6 @@ pub fn main() {\n     //~^ ERROR cannot find macro `quote_arm!` in this scope\n     let x = quote_stmt!(ecx, 3);\n     //~^ ERROR cannot find macro `quote_stmt!` in this scope\n-    let x = quote_matcher!(ecx, 3);\n-    //~^ ERROR cannot find macro `quote_matcher!` in this scope\n     let x = quote_attr!(ecx, 3);\n     //~^ ERROR cannot find macro `quote_attr!` in this scope\n     let x = quote_arg!(ecx, 3);"}, {"sha": "5f54f269c6c55694fb7596043f44abf457c81683", "filename": "src/test/compile-fail/issue-35450.rs", "status": "modified", "additions": 2, "deletions": 3, "changes": 5, "blob_url": "https://github.com/rust-lang/rust/blob/d8b34e9a74a4e91c4283ba4002a050ac0150cec6/src%2Ftest%2Fcompile-fail%2Fissue-35450.rs", "raw_url": "https://github.com/rust-lang/rust/raw/d8b34e9a74a4e91c4283ba4002a050ac0150cec6/src%2Ftest%2Fcompile-fail%2Fissue-35450.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ftest%2Fcompile-fail%2Fissue-35450.rs?ref=d8b34e9a74a4e91c4283ba4002a050ac0150cec6", "patch": "@@ -8,9 +8,8 @@\n // option. This file may not be copied, modified, or distributed\n // except according to those terms.\n \n-macro_rules! m { ($t:tt) => { $t } }\n+macro_rules! m { ($($t:tt)*) => { $($t)* } }\n \n fn main() {\n-    m!($t); //~ ERROR unknown macro variable\n-            //~| ERROR expected expression\n+    m!($t); //~ ERROR expected expression\n }"}, {"sha": "82a5aa487291320cd4bc27dda7c77b30e7a9b459", "filename": "src/test/compile-fail/macro-error.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/d8b34e9a74a4e91c4283ba4002a050ac0150cec6/src%2Ftest%2Fcompile-fail%2Fmacro-error.rs", "raw_url": "https://github.com/rust-lang/rust/raw/d8b34e9a74a4e91c4283ba4002a050ac0150cec6/src%2Ftest%2Fcompile-fail%2Fmacro-error.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ftest%2Fcompile-fail%2Fmacro-error.rs?ref=d8b34e9a74a4e91c4283ba4002a050ac0150cec6", "patch": "@@ -9,7 +9,7 @@\n // except according to those terms.\n \n macro_rules! foo {\n-    ($a:expr) => $a; //~ ERROR macro rhs must be delimited\n+    ($a:expr) => a; //~ ERROR macro rhs must be delimited\n }\n \n fn main() {"}, {"sha": "0af171b43fe5c072814d479f7f873801258755e5", "filename": "src/test/compile-fail/macro-match-nonterminal.rs", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "blob_url": "https://github.com/rust-lang/rust/blob/d8b34e9a74a4e91c4283ba4002a050ac0150cec6/src%2Ftest%2Fcompile-fail%2Fmacro-match-nonterminal.rs", "raw_url": "https://github.com/rust-lang/rust/raw/d8b34e9a74a4e91c4283ba4002a050ac0150cec6/src%2Ftest%2Fcompile-fail%2Fmacro-match-nonterminal.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ftest%2Fcompile-fail%2Fmacro-match-nonterminal.rs?ref=d8b34e9a74a4e91c4283ba4002a050ac0150cec6", "patch": "@@ -8,7 +8,8 @@\n // option. This file may not be copied, modified, or distributed\n // except according to those terms.\n \n-macro_rules! test { ($a, $b) => (()); } //~ ERROR missing fragment\n+macro_rules! test { ($a, //~ ERROR missing fragment\n+                     $b) => (()); } //~ ERROR missing fragment\n \n fn main() {\n     test!()"}, {"sha": "7255e7d00b6115c1a60caa9f279a969fb3c9cfe1", "filename": "src/test/compile-fail/macro-tt-matchers.rs", "status": "modified", "additions": 1, "deletions": 12, "changes": 13, "blob_url": "https://github.com/rust-lang/rust/blob/d8b34e9a74a4e91c4283ba4002a050ac0150cec6/src%2Ftest%2Fcompile-fail%2Fmacro-tt-matchers.rs", "raw_url": "https://github.com/rust-lang/rust/raw/d8b34e9a74a4e91c4283ba4002a050ac0150cec6/src%2Ftest%2Fcompile-fail%2Fmacro-tt-matchers.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ftest%2Fcompile-fail%2Fmacro-tt-matchers.rs?ref=d8b34e9a74a4e91c4283ba4002a050ac0150cec6", "patch": "@@ -17,16 +17,5 @@ macro_rules! foo {\n \n foo!(Box);\n \n-macro_rules! bar {\n-    ($x:tt) => {\n-        macro_rules! baz {\n-            ($x:tt, $y:tt) => { ($x, $y) }\n-        }\n-    }\n-}\n-\n #[rustc_error]\n-fn main() { //~ ERROR compilation successful\n-    bar!($y);\n-    let _: (i8, i16) = baz!(0i8, 0i16);\n-}\n+fn main() {} //~ ERROR compilation successful"}, {"sha": "0b437be5393edac4aafdf4b3f4b36dc0c57382a2", "filename": "src/test/compile-fail/malformed_macro_lhs.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/d8b34e9a74a4e91c4283ba4002a050ac0150cec6/src%2Ftest%2Fcompile-fail%2Fmalformed_macro_lhs.rs", "raw_url": "https://github.com/rust-lang/rust/raw/d8b34e9a74a4e91c4283ba4002a050ac0150cec6/src%2Ftest%2Fcompile-fail%2Fmalformed_macro_lhs.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ftest%2Fcompile-fail%2Fmalformed_macro_lhs.rs?ref=d8b34e9a74a4e91c4283ba4002a050ac0150cec6", "patch": "@@ -9,7 +9,7 @@\n // except according to those terms.\n \n macro_rules! my_precioooous {\n-    $($t:tt)* => (1); //~ ERROR invalid macro matcher\n+    t => (1); //~ ERROR invalid macro matcher\n }\n \n fn main() {"}, {"sha": "d72bd8aab89cf8caf45b67e87b70d1cd282ae325", "filename": "src/test/parse-fail/issue-33569.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/d8b34e9a74a4e91c4283ba4002a050ac0150cec6/src%2Ftest%2Fparse-fail%2Fissue-33569.rs", "raw_url": "https://github.com/rust-lang/rust/raw/d8b34e9a74a4e91c4283ba4002a050ac0150cec6/src%2Ftest%2Fparse-fail%2Fissue-33569.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ftest%2Fparse-fail%2Fissue-33569.rs?ref=d8b34e9a74a4e91c4283ba4002a050ac0150cec6", "patch": "@@ -12,7 +12,7 @@\n \n macro_rules! foo {\n     { $+ } => { //~ ERROR expected identifier, found `+`\n+                //~^ ERROR missing fragment specifier\n         $(x)(y) //~ ERROR expected `*` or `+`\n-                //~^ ERROR no rules expected the token `)`\n     }\n }"}, {"sha": "3db69f2167cc6e4ba7bc5fb89456a7761c532004", "filename": "src/test/run-pass-fulldeps/auxiliary/procedural_mbe_matching.rs", "status": "modified", "additions": 3, "deletions": 1, "changes": 4, "blob_url": "https://github.com/rust-lang/rust/blob/d8b34e9a74a4e91c4283ba4002a050ac0150cec6/src%2Ftest%2Frun-pass-fulldeps%2Fauxiliary%2Fprocedural_mbe_matching.rs", "raw_url": "https://github.com/rust-lang/rust/raw/d8b34e9a74a4e91c4283ba4002a050ac0150cec6/src%2Ftest%2Frun-pass-fulldeps%2Fauxiliary%2Fprocedural_mbe_matching.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ftest%2Frun-pass-fulldeps%2Fauxiliary%2Fprocedural_mbe_matching.rs?ref=d8b34e9a74a4e91c4283ba4002a050ac0150cec6", "patch": "@@ -23,6 +23,7 @@ use syntax::ast::{Ident, Pat};\n use syntax::tokenstream::{TokenTree};\n use syntax::ext::base::{ExtCtxt, MacResult, MacEager};\n use syntax::ext::build::AstBuilder;\n+use syntax::ext::tt::quoted;\n use syntax::ext::tt::macro_parser::{MatchedSeq, MatchedNonterminal};\n use syntax::ext::tt::macro_parser::{Success, Failure, Error};\n use syntax::ext::tt::macro_parser::parse_failure_msg;\n@@ -33,7 +34,8 @@ use rustc_plugin::Registry;\n fn expand_mbe_matches(cx: &mut ExtCtxt, _: Span, args: &[TokenTree])\n         -> Box<MacResult + 'static> {\n \n-    let mbe_matcher = quote_matcher!(cx, $matched:expr, $($pat:pat)|+);\n+    let mbe_matcher = quote_tokens!(cx, $$matched:expr, $$($$pat:pat)|+);\n+    let mbe_matcher = quoted::parse(&mbe_matcher, true, cx.parse_sess);\n     let map = match TokenTree::parse(cx, &mbe_matcher, args) {\n         Success(map) => map,\n         Failure(_, tok) => {"}, {"sha": "822b2c9b93b4a070d921f681fec3c50818b0c8e8", "filename": "src/test/run-pass-fulldeps/mbe_matching_test_macro.rs", "status": "modified", "additions": 0, "deletions": 4, "changes": 4, "blob_url": "https://github.com/rust-lang/rust/blob/d8b34e9a74a4e91c4283ba4002a050ac0150cec6/src%2Ftest%2Frun-pass-fulldeps%2Fmbe_matching_test_macro.rs", "raw_url": "https://github.com/rust-lang/rust/raw/d8b34e9a74a4e91c4283ba4002a050ac0150cec6/src%2Ftest%2Frun-pass-fulldeps%2Fmbe_matching_test_macro.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ftest%2Frun-pass-fulldeps%2Fmbe_matching_test_macro.rs?ref=d8b34e9a74a4e91c4283ba4002a050ac0150cec6", "patch": "@@ -14,11 +14,7 @@\n #![feature(plugin)]\n #![plugin(procedural_mbe_matching)]\n \n-#[no_link]\n-extern crate procedural_mbe_matching;\n-\n pub fn main() {\n-    let abc = 123u32;\n     assert_eq!(matches!(Some(123), None | Some(0)), false);\n     assert_eq!(matches!(Some(123), None | Some(123)), true);\n     assert_eq!(matches!(true, true), true);"}, {"sha": "8e6a69cb58479e1af384cb19729e1554ca4d12dc", "filename": "src/test/run-pass-fulldeps/quote-tokens.rs", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "blob_url": "https://github.com/rust-lang/rust/blob/d8b34e9a74a4e91c4283ba4002a050ac0150cec6/src%2Ftest%2Frun-pass-fulldeps%2Fquote-tokens.rs", "raw_url": "https://github.com/rust-lang/rust/raw/d8b34e9a74a4e91c4283ba4002a050ac0150cec6/src%2Ftest%2Frun-pass-fulldeps%2Fquote-tokens.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ftest%2Frun-pass-fulldeps%2Fquote-tokens.rs?ref=d8b34e9a74a4e91c4283ba4002a050ac0150cec6", "patch": "@@ -37,7 +37,6 @@ fn syntax_extension(cx: &ExtCtxt) {\n \n     let _l: P<syntax::ast::Ty> = quote_ty!(cx, &isize);\n \n-    let _m: Vec<syntax::tokenstream::TokenTree> = quote_matcher!(cx, $($foo:tt,)* bar);\n     let _n: syntax::ast::Attribute = quote_attr!(cx, #![cfg(foo, bar = \"baz\")]);\n \n     let _o: Option<P<syntax::ast::Item>> = quote_item!(cx, fn foo<T: ?Sized>() {});"}, {"sha": "ebca9312a64b0fc11df3756ec531029a4543ac29", "filename": "src/test/run-pass/issue-39709.rs", "status": "renamed", "additions": 1, "deletions": 2, "changes": 3, "blob_url": "https://github.com/rust-lang/rust/blob/d8b34e9a74a4e91c4283ba4002a050ac0150cec6/src%2Ftest%2Frun-pass%2Fissue-39709.rs", "raw_url": "https://github.com/rust-lang/rust/raw/d8b34e9a74a4e91c4283ba4002a050ac0150cec6/src%2Ftest%2Frun-pass%2Fissue-39709.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ftest%2Frun-pass%2Fissue-39709.rs?ref=d8b34e9a74a4e91c4283ba4002a050ac0150cec6", "patch": "@@ -9,7 +9,6 @@\n // except according to those terms.\n \n fn main() {\n-    println!(\"{}\", { macro_rules! x { ($()*) => {} } 33 });\n-    //~^ ERROR no syntax variables matched as repeating at this depth\n+    println!(\"{}\", { macro_rules! x { ($(t:tt)*) => {} } 33 });\n }\n ", "previous_filename": "src/test/compile-fail/issue-39709.rs"}]}