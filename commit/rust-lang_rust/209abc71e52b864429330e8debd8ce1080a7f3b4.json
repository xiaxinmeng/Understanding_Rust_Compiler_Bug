{"sha": "209abc71e52b864429330e8debd8ce1080a7f3b4", "node_id": "MDY6Q29tbWl0NzI0NzEyOjIwOWFiYzcxZTUyYjg2NDQyOTMzMGU4ZGViZDhjZTEwODBhN2YzYjQ=", "commit": {"author": {"name": "Eduard-Mihai Burtescu", "email": "edy.burt@gmail.com", "date": "2018-01-05T05:12:32Z"}, "committer": {"name": "Eduard-Mihai Burtescu", "email": "edy.burt@gmail.com", "date": "2018-01-14T06:56:44Z"}, "message": "rustc_trans: rename bcx to bx.", "tree": {"sha": "af9317ace56d23b793075a8b83ff4cdc521ebb4d", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/af9317ace56d23b793075a8b83ff4cdc521ebb4d"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/209abc71e52b864429330e8debd8ce1080a7f3b4", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/209abc71e52b864429330e8debd8ce1080a7f3b4", "html_url": "https://github.com/rust-lang/rust/commit/209abc71e52b864429330e8debd8ce1080a7f3b4", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/209abc71e52b864429330e8debd8ce1080a7f3b4/comments", "author": {"login": "eddyb", "id": 77424, "node_id": "MDQ6VXNlcjc3NDI0", "avatar_url": "https://avatars.githubusercontent.com/u/77424?v=4", "gravatar_id": "", "url": "https://api.github.com/users/eddyb", "html_url": "https://github.com/eddyb", "followers_url": "https://api.github.com/users/eddyb/followers", "following_url": "https://api.github.com/users/eddyb/following{/other_user}", "gists_url": "https://api.github.com/users/eddyb/gists{/gist_id}", "starred_url": "https://api.github.com/users/eddyb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/eddyb/subscriptions", "organizations_url": "https://api.github.com/users/eddyb/orgs", "repos_url": "https://api.github.com/users/eddyb/repos", "events_url": "https://api.github.com/users/eddyb/events{/privacy}", "received_events_url": "https://api.github.com/users/eddyb/received_events", "type": "User", "site_admin": false}, "committer": {"login": "eddyb", "id": 77424, "node_id": "MDQ6VXNlcjc3NDI0", "avatar_url": "https://avatars.githubusercontent.com/u/77424?v=4", "gravatar_id": "", "url": "https://api.github.com/users/eddyb", "html_url": "https://github.com/eddyb", "followers_url": "https://api.github.com/users/eddyb/followers", "following_url": "https://api.github.com/users/eddyb/following{/other_user}", "gists_url": "https://api.github.com/users/eddyb/gists{/gist_id}", "starred_url": "https://api.github.com/users/eddyb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/eddyb/subscriptions", "organizations_url": "https://api.github.com/users/eddyb/orgs", "repos_url": "https://api.github.com/users/eddyb/repos", "events_url": "https://api.github.com/users/eddyb/events{/privacy}", "received_events_url": "https://api.github.com/users/eddyb/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "e69dacb431fb65dfb1ed099c9e9ae74a212fbfa1", "url": "https://api.github.com/repos/rust-lang/rust/commits/e69dacb431fb65dfb1ed099c9e9ae74a212fbfa1", "html_url": "https://github.com/rust-lang/rust/commit/e69dacb431fb65dfb1ed099c9e9ae74a212fbfa1"}], "stats": {"total": 1812, "additions": 906, "deletions": 906}, "files": [{"sha": "07f9b8fed8b57593c709d94cbef020083e8b732a", "filename": "src/librustc_trans/abi.rs", "status": "modified", "additions": 17, "deletions": 17, "changes": 34, "blob_url": "https://github.com/rust-lang/rust/blob/209abc71e52b864429330e8debd8ce1080a7f3b4/src%2Flibrustc_trans%2Fabi.rs", "raw_url": "https://github.com/rust-lang/rust/raw/209abc71e52b864429330e8debd8ce1080a7f3b4/src%2Flibrustc_trans%2Fabi.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_trans%2Fabi.rs?ref=209abc71e52b864429330e8debd8ce1080a7f3b4", "patch": "@@ -555,20 +555,20 @@ impl<'a, 'tcx> ArgType<'tcx> {\n     /// place for the original Rust type of this argument/return.\n     /// Can be used for both storing formal arguments into Rust variables\n     /// or results of call/invoke instructions into their destinations.\n-    pub fn store(&self, bcx: &Builder<'a, 'tcx>, val: ValueRef, dst: PlaceRef<'tcx>) {\n+    pub fn store(&self, bx: &Builder<'a, 'tcx>, val: ValueRef, dst: PlaceRef<'tcx>) {\n         if self.is_ignore() {\n             return;\n         }\n-        let cx = bcx.cx;\n+        let cx = bx.cx;\n         if self.is_indirect() {\n-            OperandValue::Ref(val, self.layout.align).store(bcx, dst)\n+            OperandValue::Ref(val, self.layout.align).store(bx, dst)\n         } else if let PassMode::Cast(cast) = self.mode {\n             // FIXME(eddyb): Figure out when the simpler Store is safe, clang\n             // uses it for i16 -> {i8, i8}, but not for i24 -> {i8, i8, i8}.\n             let can_store_through_cast_ptr = false;\n             if can_store_through_cast_ptr {\n-                let cast_dst = bcx.pointercast(dst.llval, cast.llvm_type(cx).ptr_to());\n-                bcx.store(val, cast_dst, self.layout.align);\n+                let cast_dst = bx.pointercast(dst.llval, cast.llvm_type(cx).ptr_to());\n+                bx.store(val, cast_dst, self.layout.align);\n             } else {\n                 // The actual return type is a struct, but the ABI\n                 // adaptation code has cast it into some scalar type.  The\n@@ -587,42 +587,42 @@ impl<'a, 'tcx> ArgType<'tcx> {\n                 // We instead thus allocate some scratch space...\n                 let scratch_size = cast.size(cx);\n                 let scratch_align = cast.align(cx);\n-                let llscratch = bcx.alloca(cast.llvm_type(cx), \"abi_cast\", scratch_align);\n-                bcx.lifetime_start(llscratch, scratch_size);\n+                let llscratch = bx.alloca(cast.llvm_type(cx), \"abi_cast\", scratch_align);\n+                bx.lifetime_start(llscratch, scratch_size);\n \n                 // ...where we first store the value...\n-                bcx.store(val, llscratch, scratch_align);\n+                bx.store(val, llscratch, scratch_align);\n \n                 // ...and then memcpy it to the intended destination.\n-                base::call_memcpy(bcx,\n-                                  bcx.pointercast(dst.llval, Type::i8p(cx)),\n-                                  bcx.pointercast(llscratch, Type::i8p(cx)),\n+                base::call_memcpy(bx,\n+                                  bx.pointercast(dst.llval, Type::i8p(cx)),\n+                                  bx.pointercast(llscratch, Type::i8p(cx)),\n                                   C_usize(cx, self.layout.size.bytes()),\n                                   self.layout.align.min(scratch_align));\n \n-                bcx.lifetime_end(llscratch, scratch_size);\n+                bx.lifetime_end(llscratch, scratch_size);\n             }\n         } else {\n-            OperandValue::Immediate(val).store(bcx, dst);\n+            OperandValue::Immediate(val).store(bx, dst);\n         }\n     }\n \n-    pub fn store_fn_arg(&self, bcx: &Builder<'a, 'tcx>, idx: &mut usize, dst: PlaceRef<'tcx>) {\n+    pub fn store_fn_arg(&self, bx: &Builder<'a, 'tcx>, idx: &mut usize, dst: PlaceRef<'tcx>) {\n         if self.pad.is_some() {\n             *idx += 1;\n         }\n         let mut next = || {\n-            let val = llvm::get_param(bcx.llfn(), *idx as c_uint);\n+            let val = llvm::get_param(bx.llfn(), *idx as c_uint);\n             *idx += 1;\n             val\n         };\n         match self.mode {\n             PassMode::Ignore => {},\n             PassMode::Pair(..) => {\n-                OperandValue::Pair(next(), next()).store(bcx, dst);\n+                OperandValue::Pair(next(), next()).store(bx, dst);\n             }\n             PassMode::Direct(_) | PassMode::Indirect(_) | PassMode::Cast(_) => {\n-                self.store(bcx, next(), dst);\n+                self.store(bx, next(), dst);\n             }\n         }\n     }"}, {"sha": "c7be0c4e67d712a3617e9a16f5884f23314dbedd", "filename": "src/librustc_trans/asm.rs", "status": "modified", "additions": 13, "deletions": 13, "changes": 26, "blob_url": "https://github.com/rust-lang/rust/blob/209abc71e52b864429330e8debd8ce1080a7f3b4/src%2Flibrustc_trans%2Fasm.rs", "raw_url": "https://github.com/rust-lang/rust/raw/209abc71e52b864429330e8debd8ce1080a7f3b4/src%2Flibrustc_trans%2Fasm.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_trans%2Fasm.rs?ref=209abc71e52b864429330e8debd8ce1080a7f3b4", "patch": "@@ -27,7 +27,7 @@ use libc::{c_uint, c_char};\n \n // Take an inline assembly expression and splat it out via LLVM\n pub fn trans_inline_asm<'a, 'tcx>(\n-    bcx: &Builder<'a, 'tcx>,\n+    bx: &Builder<'a, 'tcx>,\n     ia: &hir::InlineAsm,\n     outputs: Vec<PlaceRef<'tcx>>,\n     mut inputs: Vec<ValueRef>\n@@ -39,13 +39,13 @@ pub fn trans_inline_asm<'a, 'tcx>(\n     let mut indirect_outputs = vec![];\n     for (i, (out, place)) in ia.outputs.iter().zip(&outputs).enumerate() {\n         if out.is_rw {\n-            inputs.push(place.load(bcx).immediate());\n+            inputs.push(place.load(bx).immediate());\n             ext_constraints.push(i.to_string());\n         }\n         if out.is_indirect {\n-            indirect_outputs.push(place.load(bcx).immediate());\n+            indirect_outputs.push(place.load(bx).immediate());\n         } else {\n-            output_types.push(place.layout.llvm_type(bcx.cx));\n+            output_types.push(place.layout.llvm_type(bx.cx));\n         }\n     }\n     if !indirect_outputs.is_empty() {\n@@ -58,7 +58,7 @@ pub fn trans_inline_asm<'a, 'tcx>(\n \n     // Default per-arch clobbers\n     // Basically what clang does\n-    let arch_clobbers = match &bcx.sess().target.target.arch[..] {\n+    let arch_clobbers = match &bx.sess().target.target.arch[..] {\n         \"x86\" | \"x86_64\" => vec![\"~{dirflag}\", \"~{fpsr}\", \"~{flags}\"],\n         _                => Vec::new()\n     };\n@@ -76,9 +76,9 @@ pub fn trans_inline_asm<'a, 'tcx>(\n     // Depending on how many outputs we have, the return type is different\n     let num_outputs = output_types.len();\n     let output_type = match num_outputs {\n-        0 => Type::void(bcx.cx),\n+        0 => Type::void(bx.cx),\n         1 => output_types[0],\n-        _ => Type::struct_(bcx.cx, &output_types, false)\n+        _ => Type::struct_(bx.cx, &output_types, false)\n     };\n \n     let dialect = match ia.dialect {\n@@ -88,7 +88,7 @@ pub fn trans_inline_asm<'a, 'tcx>(\n \n     let asm = CString::new(ia.asm.as_str().as_bytes()).unwrap();\n     let constraint_cstr = CString::new(all_constraints).unwrap();\n-    let r = bcx.inline_asm_call(\n+    let r = bx.inline_asm_call(\n         asm.as_ptr(),\n         constraint_cstr.as_ptr(),\n         &inputs,\n@@ -101,21 +101,21 @@ pub fn trans_inline_asm<'a, 'tcx>(\n     // Again, based on how many outputs we have\n     let outputs = ia.outputs.iter().zip(&outputs).filter(|&(ref o, _)| !o.is_indirect);\n     for (i, (_, &place)) in outputs.enumerate() {\n-        let v = if num_outputs == 1 { r } else { bcx.extract_value(r, i as u64) };\n-        OperandValue::Immediate(v).store(bcx, place);\n+        let v = if num_outputs == 1 { r } else { bx.extract_value(r, i as u64) };\n+        OperandValue::Immediate(v).store(bx, place);\n     }\n \n     // Store mark in a metadata node so we can map LLVM errors\n     // back to source locations.  See #17552.\n     unsafe {\n         let key = \"srcloc\";\n-        let kind = llvm::LLVMGetMDKindIDInContext(bcx.cx.llcx,\n+        let kind = llvm::LLVMGetMDKindIDInContext(bx.cx.llcx,\n             key.as_ptr() as *const c_char, key.len() as c_uint);\n \n-        let val: llvm::ValueRef = C_i32(bcx.cx, ia.ctxt.outer().as_u32() as i32);\n+        let val: llvm::ValueRef = C_i32(bx.cx, ia.ctxt.outer().as_u32() as i32);\n \n         llvm::LLVMSetMetadata(r, kind,\n-            llvm::LLVMMDNodeInContext(bcx.cx.llcx, &val, 1));\n+            llvm::LLVMMDNodeInContext(bx.cx.llcx, &val, 1));\n     }\n }\n "}, {"sha": "633ed9b32cd1e59880f0708182e7027495810ec2", "filename": "src/librustc_trans/base.rs", "status": "modified", "additions": 54, "deletions": 54, "changes": 108, "blob_url": "https://github.com/rust-lang/rust/blob/209abc71e52b864429330e8debd8ce1080a7f3b4/src%2Flibrustc_trans%2Fbase.rs", "raw_url": "https://github.com/rust-lang/rust/raw/209abc71e52b864429330e8debd8ce1080a7f3b4/src%2Flibrustc_trans%2Fbase.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_trans%2Fbase.rs?ref=209abc71e52b864429330e8debd8ce1080a7f3b4", "patch": "@@ -158,7 +158,7 @@ pub fn bin_op_to_fcmp_predicate(op: hir::BinOp_) -> llvm::RealPredicate {\n }\n \n pub fn compare_simd_types<'a, 'tcx>(\n-    bcx: &Builder<'a, 'tcx>,\n+    bx: &Builder<'a, 'tcx>,\n     lhs: ValueRef,\n     rhs: ValueRef,\n     t: Ty<'tcx>,\n@@ -168,7 +168,7 @@ pub fn compare_simd_types<'a, 'tcx>(\n     let signed = match t.sty {\n         ty::TyFloat(_) => {\n             let cmp = bin_op_to_fcmp_predicate(op);\n-            return bcx.sext(bcx.fcmp(cmp, lhs, rhs), ret_ty);\n+            return bx.sext(bx.fcmp(cmp, lhs, rhs), ret_ty);\n         },\n         ty::TyUint(_) => false,\n         ty::TyInt(_) => true,\n@@ -180,7 +180,7 @@ pub fn compare_simd_types<'a, 'tcx>(\n     // to get the correctly sized type. This will compile to a single instruction\n     // once the IR is converted to assembly if the SIMD instruction is supported\n     // by the target architecture.\n-    bcx.sext(bcx.icmp(cmp, lhs, rhs), ret_ty)\n+    bx.sext(bx.icmp(cmp, lhs, rhs), ret_ty)\n }\n \n /// Retrieve the information we are losing (making dynamic) in an unsizing\n@@ -219,7 +219,7 @@ pub fn unsized_info<'cx, 'tcx>(cx: &CodegenCx<'cx, 'tcx>,\n \n /// Coerce `src` to `dst_ty`. `src_ty` must be a thin pointer.\n pub fn unsize_thin_ptr<'a, 'tcx>(\n-    bcx: &Builder<'a, 'tcx>,\n+    bx: &Builder<'a, 'tcx>,\n     src: ValueRef,\n     src_ty: Ty<'tcx>,\n     dst_ty: Ty<'tcx>\n@@ -232,68 +232,68 @@ pub fn unsize_thin_ptr<'a, 'tcx>(\n          &ty::TyRawPtr(ty::TypeAndMut { ty: b, .. })) |\n         (&ty::TyRawPtr(ty::TypeAndMut { ty: a, .. }),\n          &ty::TyRawPtr(ty::TypeAndMut { ty: b, .. })) => {\n-            assert!(bcx.cx.type_is_sized(a));\n-            let ptr_ty = bcx.cx.layout_of(b).llvm_type(bcx.cx).ptr_to();\n-            (bcx.pointercast(src, ptr_ty), unsized_info(bcx.cx, a, b, None))\n+            assert!(bx.cx.type_is_sized(a));\n+            let ptr_ty = bx.cx.layout_of(b).llvm_type(bx.cx).ptr_to();\n+            (bx.pointercast(src, ptr_ty), unsized_info(bx.cx, a, b, None))\n         }\n         (&ty::TyAdt(def_a, _), &ty::TyAdt(def_b, _)) if def_a.is_box() && def_b.is_box() => {\n             let (a, b) = (src_ty.boxed_ty(), dst_ty.boxed_ty());\n-            assert!(bcx.cx.type_is_sized(a));\n-            let ptr_ty = bcx.cx.layout_of(b).llvm_type(bcx.cx).ptr_to();\n-            (bcx.pointercast(src, ptr_ty), unsized_info(bcx.cx, a, b, None))\n+            assert!(bx.cx.type_is_sized(a));\n+            let ptr_ty = bx.cx.layout_of(b).llvm_type(bx.cx).ptr_to();\n+            (bx.pointercast(src, ptr_ty), unsized_info(bx.cx, a, b, None))\n         }\n         (&ty::TyAdt(def_a, _), &ty::TyAdt(def_b, _)) => {\n             assert_eq!(def_a, def_b);\n \n-            let src_layout = bcx.cx.layout_of(src_ty);\n-            let dst_layout = bcx.cx.layout_of(dst_ty);\n+            let src_layout = bx.cx.layout_of(src_ty);\n+            let dst_layout = bx.cx.layout_of(dst_ty);\n             let mut result = None;\n             for i in 0..src_layout.fields.count() {\n-                let src_f = src_layout.field(bcx.cx, i);\n+                let src_f = src_layout.field(bx.cx, i);\n                 assert_eq!(src_layout.fields.offset(i).bytes(), 0);\n                 assert_eq!(dst_layout.fields.offset(i).bytes(), 0);\n                 if src_f.is_zst() {\n                     continue;\n                 }\n                 assert_eq!(src_layout.size, src_f.size);\n \n-                let dst_f = dst_layout.field(bcx.cx, i);\n+                let dst_f = dst_layout.field(bx.cx, i);\n                 assert_ne!(src_f.ty, dst_f.ty);\n                 assert_eq!(result, None);\n-                result = Some(unsize_thin_ptr(bcx, src, src_f.ty, dst_f.ty));\n+                result = Some(unsize_thin_ptr(bx, src, src_f.ty, dst_f.ty));\n             }\n             let (lldata, llextra) = result.unwrap();\n             // HACK(eddyb) have to bitcast pointers until LLVM removes pointee types.\n-            (bcx.bitcast(lldata, dst_layout.scalar_pair_element_llvm_type(bcx.cx, 0)),\n-             bcx.bitcast(llextra, dst_layout.scalar_pair_element_llvm_type(bcx.cx, 1)))\n+            (bx.bitcast(lldata, dst_layout.scalar_pair_element_llvm_type(bx.cx, 0)),\n+             bx.bitcast(llextra, dst_layout.scalar_pair_element_llvm_type(bx.cx, 1)))\n         }\n         _ => bug!(\"unsize_thin_ptr: called on bad types\"),\n     }\n }\n \n /// Coerce `src`, which is a reference to a value of type `src_ty`,\n /// to a value of type `dst_ty` and store the result in `dst`\n-pub fn coerce_unsized_into<'a, 'tcx>(bcx: &Builder<'a, 'tcx>,\n+pub fn coerce_unsized_into<'a, 'tcx>(bx: &Builder<'a, 'tcx>,\n                                      src: PlaceRef<'tcx>,\n                                      dst: PlaceRef<'tcx>) {\n     let src_ty = src.layout.ty;\n     let dst_ty = dst.layout.ty;\n     let coerce_ptr = || {\n-        let (base, info) = match src.load(bcx).val {\n+        let (base, info) = match src.load(bx).val {\n             OperandValue::Pair(base, info) => {\n                 // fat-ptr to fat-ptr unsize preserves the vtable\n                 // i.e. &'a fmt::Debug+Send => &'a fmt::Debug\n                 // So we need to pointercast the base to ensure\n                 // the types match up.\n-                let thin_ptr = dst.layout.field(bcx.cx, abi::FAT_PTR_ADDR);\n-                (bcx.pointercast(base, thin_ptr.llvm_type(bcx.cx)), info)\n+                let thin_ptr = dst.layout.field(bx.cx, abi::FAT_PTR_ADDR);\n+                (bx.pointercast(base, thin_ptr.llvm_type(bx.cx)), info)\n             }\n             OperandValue::Immediate(base) => {\n-                unsize_thin_ptr(bcx, base, src_ty, dst_ty)\n+                unsize_thin_ptr(bx, base, src_ty, dst_ty)\n             }\n             OperandValue::Ref(..) => bug!()\n         };\n-        OperandValue::Pair(base, info).store(bcx, dst);\n+        OperandValue::Pair(base, info).store(bx, dst);\n     };\n     match (&src_ty.sty, &dst_ty.sty) {\n         (&ty::TyRef(..), &ty::TyRef(..)) |\n@@ -309,18 +309,18 @@ pub fn coerce_unsized_into<'a, 'tcx>(bcx: &Builder<'a, 'tcx>,\n             assert_eq!(def_a, def_b);\n \n             for i in 0..def_a.variants[0].fields.len() {\n-                let src_f = src.project_field(bcx, i);\n-                let dst_f = dst.project_field(bcx, i);\n+                let src_f = src.project_field(bx, i);\n+                let dst_f = dst.project_field(bx, i);\n \n                 if dst_f.layout.is_zst() {\n                     continue;\n                 }\n \n                 if src_f.layout.ty == dst_f.layout.ty {\n-                    memcpy_ty(bcx, dst_f.llval, src_f.llval, src_f.layout,\n+                    memcpy_ty(bx, dst_f.llval, src_f.llval, src_f.layout,\n                         src_f.align.min(dst_f.align));\n                 } else {\n-                    coerce_unsized_into(bcx, src_f, dst_f);\n+                    coerce_unsized_into(bx, src_f, dst_f);\n                 }\n             }\n         }\n@@ -388,47 +388,47 @@ pub fn wants_msvc_seh(sess: &Session) -> bool {\n     sess.target.target.options.is_like_msvc\n }\n \n-pub fn call_assume<'a, 'tcx>(b: &Builder<'a, 'tcx>, val: ValueRef) {\n-    let assume_intrinsic = b.cx.get_intrinsic(\"llvm.assume\");\n-    b.call(assume_intrinsic, &[val], None);\n+pub fn call_assume<'a, 'tcx>(bx: &Builder<'a, 'tcx>, val: ValueRef) {\n+    let assume_intrinsic = bx.cx.get_intrinsic(\"llvm.assume\");\n+    bx.call(assume_intrinsic, &[val], None);\n }\n \n-pub fn from_immediate(bcx: &Builder, val: ValueRef) -> ValueRef {\n-    if val_ty(val) == Type::i1(bcx.cx) {\n-        bcx.zext(val, Type::i8(bcx.cx))\n+pub fn from_immediate(bx: &Builder, val: ValueRef) -> ValueRef {\n+    if val_ty(val) == Type::i1(bx.cx) {\n+        bx.zext(val, Type::i8(bx.cx))\n     } else {\n         val\n     }\n }\n \n-pub fn to_immediate(bcx: &Builder, val: ValueRef, layout: layout::TyLayout) -> ValueRef {\n+pub fn to_immediate(bx: &Builder, val: ValueRef, layout: layout::TyLayout) -> ValueRef {\n     if let layout::Abi::Scalar(ref scalar) = layout.abi {\n         if scalar.is_bool() {\n-            return bcx.trunc(val, Type::i1(bcx.cx));\n+            return bx.trunc(val, Type::i1(bx.cx));\n         }\n     }\n     val\n }\n \n-pub fn call_memcpy(b: &Builder,\n+pub fn call_memcpy(bx: &Builder,\n                    dst: ValueRef,\n                    src: ValueRef,\n                    n_bytes: ValueRef,\n                    align: Align) {\n-    let cx = b.cx;\n+    let cx = bx.cx;\n     let ptr_width = &cx.sess().target.target.target_pointer_width;\n     let key = format!(\"llvm.memcpy.p0i8.p0i8.i{}\", ptr_width);\n     let memcpy = cx.get_intrinsic(&key);\n-    let src_ptr = b.pointercast(src, Type::i8p(cx));\n-    let dst_ptr = b.pointercast(dst, Type::i8p(cx));\n-    let size = b.intcast(n_bytes, cx.isize_ty, false);\n+    let src_ptr = bx.pointercast(src, Type::i8p(cx));\n+    let dst_ptr = bx.pointercast(dst, Type::i8p(cx));\n+    let size = bx.intcast(n_bytes, cx.isize_ty, false);\n     let align = C_i32(cx, align.abi() as i32);\n     let volatile = C_bool(cx, false);\n-    b.call(memcpy, &[dst_ptr, src_ptr, size, align, volatile], None);\n+    bx.call(memcpy, &[dst_ptr, src_ptr, size, align, volatile], None);\n }\n \n pub fn memcpy_ty<'a, 'tcx>(\n-    bcx: &Builder<'a, 'tcx>,\n+    bx: &Builder<'a, 'tcx>,\n     dst: ValueRef,\n     src: ValueRef,\n     layout: TyLayout<'tcx>,\n@@ -439,20 +439,20 @@ pub fn memcpy_ty<'a, 'tcx>(\n         return;\n     }\n \n-    call_memcpy(bcx, dst, src, C_usize(bcx.cx, size), align);\n+    call_memcpy(bx, dst, src, C_usize(bx.cx, size), align);\n }\n \n-pub fn call_memset<'a, 'tcx>(b: &Builder<'a, 'tcx>,\n+pub fn call_memset<'a, 'tcx>(bx: &Builder<'a, 'tcx>,\n                              ptr: ValueRef,\n                              fill_byte: ValueRef,\n                              size: ValueRef,\n                              align: ValueRef,\n                              volatile: bool) -> ValueRef {\n-    let ptr_width = &b.cx.sess().target.target.target_pointer_width;\n+    let ptr_width = &bx.cx.sess().target.target.target_pointer_width;\n     let intrinsic_key = format!(\"llvm.memset.p0i8.i{}\", ptr_width);\n-    let llintrinsicfn = b.cx.get_intrinsic(&intrinsic_key);\n-    let volatile = C_bool(b.cx, volatile);\n-    b.call(llintrinsicfn, &[ptr, fill_byte, size, align, volatile], None)\n+    let llintrinsicfn = bx.cx.get_intrinsic(&intrinsic_key);\n+    let volatile = C_bool(bx.cx, volatile);\n+    bx.call(llintrinsicfn, &[ptr, fill_byte, size, align, volatile], None)\n }\n \n pub fn trans_instance<'a, 'tcx>(cx: &CodegenCx<'a, 'tcx>, instance: Instance<'tcx>) {\n@@ -575,29 +575,29 @@ fn maybe_create_entry_wrapper(cx: &CodegenCx) {\n         // `main` should respect same config for frame pointer elimination as rest of code\n         attributes::set_frame_pointer_elimination(cx, llfn);\n \n-        let bld = Builder::new_block(cx, llfn, \"top\");\n+        let bx = Builder::new_block(cx, llfn, \"top\");\n \n-        debuginfo::gdb::insert_reference_to_gdb_debug_scripts_section_global(cx, &bld);\n+        debuginfo::gdb::insert_reference_to_gdb_debug_scripts_section_global(&bx);\n \n         // Params from native main() used as args for rust start function\n         let param_argc = get_param(llfn, 0);\n         let param_argv = get_param(llfn, 1);\n-        let arg_argc = bld.intcast(param_argc, cx.isize_ty, true);\n+        let arg_argc = bx.intcast(param_argc, cx.isize_ty, true);\n         let arg_argv = param_argv;\n \n         let (start_fn, args) = if use_start_lang_item {\n             let start_def_id = cx.tcx.require_lang_item(StartFnLangItem);\n             let start_fn = callee::resolve_and_get_fn(cx, start_def_id, cx.tcx.mk_substs(\n                 iter::once(Kind::from(main_ret_ty))));\n-            (start_fn, vec![bld.pointercast(rust_main, Type::i8p(cx).ptr_to()),\n+            (start_fn, vec![bx.pointercast(rust_main, Type::i8p(cx).ptr_to()),\n                             arg_argc, arg_argv])\n         } else {\n             debug!(\"using user-defined start fn\");\n             (rust_main, vec![arg_argc, arg_argv])\n         };\n \n-        let result = bld.call(start_fn, &args, None);\n-        bld.ret(bld.intcast(result, Type::c_int(cx), true));\n+        let result = bx.call(start_fn, &args, None);\n+        bx.ret(bx.intcast(result, Type::c_int(cx), true));\n     }\n }\n "}, {"sha": "5ab8d03b8c71893c1522e3a107b9f5eba23def6c", "filename": "src/librustc_trans/builder.rs", "status": "modified", "additions": 6, "deletions": 6, "changes": 12, "blob_url": "https://github.com/rust-lang/rust/blob/209abc71e52b864429330e8debd8ce1080a7f3b4/src%2Flibrustc_trans%2Fbuilder.rs", "raw_url": "https://github.com/rust-lang/rust/raw/209abc71e52b864429330e8debd8ce1080a7f3b4/src%2Flibrustc_trans%2Fbuilder.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_trans%2Fbuilder.rs?ref=209abc71e52b864429330e8debd8ce1080a7f3b4", "patch": "@@ -52,7 +52,7 @@ fn noname() -> *const c_char {\n \n impl<'a, 'tcx> Builder<'a, 'tcx> {\n     pub fn new_block<'b>(cx: &'a CodegenCx<'a, 'tcx>, llfn: ValueRef, name: &'b str) -> Self {\n-        let builder = Builder::with_cx(cx);\n+        let bx = Builder::with_cx(cx);\n         let llbb = unsafe {\n             let name = CString::new(name).unwrap();\n             llvm::LLVMAppendBasicBlockInContext(\n@@ -61,8 +61,8 @@ impl<'a, 'tcx> Builder<'a, 'tcx> {\n                 name.as_ptr()\n             )\n         };\n-        builder.position_at_end(llbb);\n-        builder\n+        bx.position_at_end(llbb);\n+        bx\n     }\n \n     pub fn with_cx(cx: &'a CodegenCx<'a, 'tcx>) -> Self {\n@@ -489,11 +489,11 @@ impl<'a, 'tcx> Builder<'a, 'tcx> {\n     }\n \n     pub fn alloca(&self, ty: Type, name: &str, align: Align) -> ValueRef {\n-        let builder = Builder::with_cx(self.cx);\n-        builder.position_at_start(unsafe {\n+        let bx = Builder::with_cx(self.cx);\n+        bx.position_at_start(unsafe {\n             llvm::LLVMGetFirstBasicBlock(self.llfn())\n         });\n-        builder.dynamic_alloca(ty, name, align)\n+        bx.dynamic_alloca(ty, name, align)\n     }\n \n     pub fn dynamic_alloca(&self, ty: Type, name: &str, align: Align) -> ValueRef {"}, {"sha": "9e745c3a1f5dd77d46c38b4c5c34832a5801d9b4", "filename": "src/librustc_trans/common.rs", "status": "modified", "additions": 14, "deletions": 14, "changes": 28, "blob_url": "https://github.com/rust-lang/rust/blob/209abc71e52b864429330e8debd8ce1080a7f3b4/src%2Flibrustc_trans%2Fcommon.rs", "raw_url": "https://github.com/rust-lang/rust/raw/209abc71e52b864429330e8debd8ce1080a7f3b4/src%2Flibrustc_trans%2Fcommon.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_trans%2Fcommon.rs?ref=209abc71e52b864429330e8debd8ce1080a7f3b4", "patch": "@@ -328,37 +328,37 @@ pub fn langcall(tcx: TyCtxt,\n // of Java. (See related discussion on #1877 and #10183.)\n \n pub fn build_unchecked_lshift<'a, 'tcx>(\n-    bcx: &Builder<'a, 'tcx>,\n+    bx: &Builder<'a, 'tcx>,\n     lhs: ValueRef,\n     rhs: ValueRef\n ) -> ValueRef {\n-    let rhs = base::cast_shift_expr_rhs(bcx, hir::BinOp_::BiShl, lhs, rhs);\n+    let rhs = base::cast_shift_expr_rhs(bx, hir::BinOp_::BiShl, lhs, rhs);\n     // #1877, #10183: Ensure that input is always valid\n-    let rhs = shift_mask_rhs(bcx, rhs);\n-    bcx.shl(lhs, rhs)\n+    let rhs = shift_mask_rhs(bx, rhs);\n+    bx.shl(lhs, rhs)\n }\n \n pub fn build_unchecked_rshift<'a, 'tcx>(\n-    bcx: &Builder<'a, 'tcx>, lhs_t: Ty<'tcx>, lhs: ValueRef, rhs: ValueRef\n+    bx: &Builder<'a, 'tcx>, lhs_t: Ty<'tcx>, lhs: ValueRef, rhs: ValueRef\n ) -> ValueRef {\n-    let rhs = base::cast_shift_expr_rhs(bcx, hir::BinOp_::BiShr, lhs, rhs);\n+    let rhs = base::cast_shift_expr_rhs(bx, hir::BinOp_::BiShr, lhs, rhs);\n     // #1877, #10183: Ensure that input is always valid\n-    let rhs = shift_mask_rhs(bcx, rhs);\n+    let rhs = shift_mask_rhs(bx, rhs);\n     let is_signed = lhs_t.is_signed();\n     if is_signed {\n-        bcx.ashr(lhs, rhs)\n+        bx.ashr(lhs, rhs)\n     } else {\n-        bcx.lshr(lhs, rhs)\n+        bx.lshr(lhs, rhs)\n     }\n }\n \n-fn shift_mask_rhs<'a, 'tcx>(bcx: &Builder<'a, 'tcx>, rhs: ValueRef) -> ValueRef {\n+fn shift_mask_rhs<'a, 'tcx>(bx: &Builder<'a, 'tcx>, rhs: ValueRef) -> ValueRef {\n     let rhs_llty = val_ty(rhs);\n-    bcx.and(rhs, shift_mask_val(bcx, rhs_llty, rhs_llty, false))\n+    bx.and(rhs, shift_mask_val(bx, rhs_llty, rhs_llty, false))\n }\n \n pub fn shift_mask_val<'a, 'tcx>(\n-    bcx: &Builder<'a, 'tcx>,\n+    bx: &Builder<'a, 'tcx>,\n     llty: Type,\n     mask_llty: Type,\n     invert: bool\n@@ -375,8 +375,8 @@ pub fn shift_mask_val<'a, 'tcx>(\n             }\n         },\n         TypeKind::Vector => {\n-            let mask = shift_mask_val(bcx, llty.element_type(), mask_llty.element_type(), invert);\n-            bcx.vector_splat(mask_llty.vector_length(), mask)\n+            let mask = shift_mask_val(bx, llty.element_type(), mask_llty.element_type(), invert);\n+            bx.vector_splat(mask_llty.vector_length(), mask)\n         },\n         _ => bug!(\"shift_mask_val: expected Integer or Vector, found {:?}\", kind),\n     }"}, {"sha": "355d8f91c4d0a4e7604333cfdb7def306d8ad71d", "filename": "src/librustc_trans/debuginfo/doc.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/209abc71e52b864429330e8debd8ce1080a7f3b4/src%2Flibrustc_trans%2Fdebuginfo%2Fdoc.rs", "raw_url": "https://github.com/rust-lang/rust/raw/209abc71e52b864429330e8debd8ce1080a7f3b4/src%2Flibrustc_trans%2Fdebuginfo%2Fdoc.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_trans%2Fdebuginfo%2Fdoc.rs?ref=209abc71e52b864429330e8debd8ce1080a7f3b4", "patch": "@@ -32,7 +32,7 @@\n //! The public API of the module is a set of functions that will insert the\n //! correct metadata into the LLVM IR when called with the right parameters.\n //! The module is thus driven from an outside client with functions like\n-//! `debuginfo::create_local_var_metadata(bcx: block, local: &ast::local)`.\n+//! `debuginfo::create_local_var_metadata(bx: block, local: &ast::local)`.\n //!\n //! Internally the module will try to reuse already created metadata by\n //! utilizing a cache. The way to get a shared metadata node when needed is"}, {"sha": "03e7c63dbca36a87a2c3a2677ebafc441e524847", "filename": "src/librustc_trans/debuginfo/gdb.rs", "status": "modified", "additions": 6, "deletions": 6, "changes": 12, "blob_url": "https://github.com/rust-lang/rust/blob/209abc71e52b864429330e8debd8ce1080a7f3b4/src%2Flibrustc_trans%2Fdebuginfo%2Fgdb.rs", "raw_url": "https://github.com/rust-lang/rust/raw/209abc71e52b864429330e8debd8ce1080a7f3b4/src%2Flibrustc_trans%2Fdebuginfo%2Fgdb.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_trans%2Fdebuginfo%2Fgdb.rs?ref=209abc71e52b864429330e8debd8ce1080a7f3b4", "patch": "@@ -24,14 +24,14 @@ use syntax::attr;\n \n /// Inserts a side-effect free instruction sequence that makes sure that the\n /// .debug_gdb_scripts global is referenced, so it isn't removed by the linker.\n-pub fn insert_reference_to_gdb_debug_scripts_section_global(cx: &CodegenCx, builder: &Builder) {\n-    if needs_gdb_debug_scripts_section(cx) {\n-        let gdb_debug_scripts_section_global = get_or_insert_gdb_debug_scripts_section_global(cx);\n+pub fn insert_reference_to_gdb_debug_scripts_section_global(bx: &Builder) {\n+    if needs_gdb_debug_scripts_section(bx.cx) {\n+        let gdb_debug_scripts_section = get_or_insert_gdb_debug_scripts_section_global(bx.cx);\n         // Load just the first byte as that's all that's necessary to force\n         // LLVM to keep around the reference to the global.\n-        let indices = [C_i32(cx, 0), C_i32(cx, 0)];\n-        let element = builder.inbounds_gep(gdb_debug_scripts_section_global, &indices);\n-        let volative_load_instruction = builder.volatile_load(element);\n+        let indices = [C_i32(bx.cx, 0), C_i32(bx.cx, 0)];\n+        let element = bx.inbounds_gep(gdb_debug_scripts_section, &indices);\n+        let volative_load_instruction = bx.volatile_load(element);\n         unsafe {\n             llvm::LLVMSetAlignment(volative_load_instruction, 1);\n         }"}, {"sha": "b46e12d9d5b67e8f9a68b9aacb42447bdfd3f9f4", "filename": "src/librustc_trans/debuginfo/mod.rs", "status": "modified", "additions": 7, "deletions": 7, "changes": 14, "blob_url": "https://github.com/rust-lang/rust/blob/209abc71e52b864429330e8debd8ce1080a7f3b4/src%2Flibrustc_trans%2Fdebuginfo%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/209abc71e52b864429330e8debd8ce1080a7f3b4/src%2Flibrustc_trans%2Fdebuginfo%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_trans%2Fdebuginfo%2Fmod.rs?ref=209abc71e52b864429330e8debd8ce1080a7f3b4", "patch": "@@ -455,15 +455,15 @@ pub fn create_function_debug_context<'a, 'tcx>(cx: &CodegenCx<'a, 'tcx>,\n     }\n }\n \n-pub fn declare_local<'a, 'tcx>(bcx: &Builder<'a, 'tcx>,\n+pub fn declare_local<'a, 'tcx>(bx: &Builder<'a, 'tcx>,\n                                dbg_context: &FunctionDebugContext,\n                                variable_name: ast::Name,\n                                variable_type: Ty<'tcx>,\n                                scope_metadata: DIScope,\n                                variable_access: VariableAccess,\n                                variable_kind: VariableKind,\n                                span: Span) {\n-    let cx = bcx.cx;\n+    let cx = bx.cx;\n \n     let file = span_start(cx, span).file;\n     let file_metadata = file_metadata(cx,\n@@ -499,28 +499,28 @@ pub fn declare_local<'a, 'tcx>(bcx: &Builder<'a, 'tcx>,\n                     align.abi() as u32,\n                 )\n             };\n-            source_loc::set_debug_location(bcx,\n+            source_loc::set_debug_location(bx,\n                 InternalDebugLocation::new(scope_metadata, loc.line, loc.col.to_usize()));\n             unsafe {\n-                let debug_loc = llvm::LLVMGetCurrentDebugLocation(bcx.llbuilder);\n+                let debug_loc = llvm::LLVMGetCurrentDebugLocation(bx.llbuilder);\n                 let instr = llvm::LLVMRustDIBuilderInsertDeclareAtEnd(\n                     DIB(cx),\n                     alloca,\n                     metadata,\n                     address_operations.as_ptr(),\n                     address_operations.len() as c_uint,\n                     debug_loc,\n-                    bcx.llbb());\n+                    bx.llbb());\n \n-                llvm::LLVMSetInstDebugLocation(bcx.llbuilder, instr);\n+                llvm::LLVMSetInstDebugLocation(bx.llbuilder, instr);\n             }\n         }\n     }\n \n     match variable_kind {\n         ArgumentVariable(_) | CapturedVariable => {\n             assert!(!dbg_context.get_ref(span).source_locations_enabled.get());\n-            source_loc::set_debug_location(bcx, UnknownLocation);\n+            source_loc::set_debug_location(bx, UnknownLocation);\n         }\n         _ => { /* nothing to do */ }\n     }"}, {"sha": "7440296ce5d7bae7c8323d1cf47fa08e543e3b6c", "filename": "src/librustc_trans/debuginfo/source_loc.rs", "status": "modified", "additions": 8, "deletions": 8, "changes": 16, "blob_url": "https://github.com/rust-lang/rust/blob/209abc71e52b864429330e8debd8ce1080a7f3b4/src%2Flibrustc_trans%2Fdebuginfo%2Fsource_loc.rs", "raw_url": "https://github.com/rust-lang/rust/raw/209abc71e52b864429330e8debd8ce1080a7f3b4/src%2Flibrustc_trans%2Fdebuginfo%2Fsource_loc.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_trans%2Fdebuginfo%2Fsource_loc.rs?ref=209abc71e52b864429330e8debd8ce1080a7f3b4", "patch": "@@ -26,25 +26,25 @@ use syntax_pos::{Span, Pos};\n ///\n /// Maps to a call to llvm::LLVMSetCurrentDebugLocation(...).\n pub fn set_source_location(\n-    debug_context: &FunctionDebugContext, builder: &Builder, scope: DIScope, span: Span\n+    debug_context: &FunctionDebugContext, bx: &Builder, scope: DIScope, span: Span\n ) {\n     let function_debug_context = match *debug_context {\n         FunctionDebugContext::DebugInfoDisabled => return,\n         FunctionDebugContext::FunctionWithoutDebugInfo => {\n-            set_debug_location(builder, UnknownLocation);\n+            set_debug_location(bx, UnknownLocation);\n             return;\n         }\n         FunctionDebugContext::RegularContext(ref data) => data\n     };\n \n     let dbg_loc = if function_debug_context.source_locations_enabled.get() {\n-        debug!(\"set_source_location: {}\", builder.sess().codemap().span_to_string(span));\n-        let loc = span_start(builder.cx, span);\n+        debug!(\"set_source_location: {}\", bx.sess().codemap().span_to_string(span));\n+        let loc = span_start(bx.cx, span);\n         InternalDebugLocation::new(scope, loc.line, loc.col.to_usize())\n     } else {\n         UnknownLocation\n     };\n-    set_debug_location(builder, dbg_loc);\n+    set_debug_location(bx, dbg_loc);\n }\n \n /// Enables emitting source locations for the given functions.\n@@ -79,7 +79,7 @@ impl InternalDebugLocation {\n     }\n }\n \n-pub fn set_debug_location(builder: &Builder, debug_location: InternalDebugLocation) {\n+pub fn set_debug_location(bx: &Builder, debug_location: InternalDebugLocation) {\n     let metadata_node = match debug_location {\n         KnownLocation { scope, line, .. } => {\n             // Always set the column to zero like Clang and GCC\n@@ -88,7 +88,7 @@ pub fn set_debug_location(builder: &Builder, debug_location: InternalDebugLocati\n \n             unsafe {\n                 llvm::LLVMRustDIBuilderCreateDebugLocation(\n-                    debug_context(builder.cx).llcontext,\n+                    debug_context(bx.cx).llcontext,\n                     line as c_uint,\n                     col as c_uint,\n                     scope,\n@@ -102,6 +102,6 @@ pub fn set_debug_location(builder: &Builder, debug_location: InternalDebugLocati\n     };\n \n     unsafe {\n-        llvm::LLVMSetCurrentDebugLocation(builder.llbuilder, metadata_node);\n+        llvm::LLVMSetCurrentDebugLocation(bx.llbuilder, metadata_node);\n     }\n }"}, {"sha": "c7275d094018567f7221192ea2cec1c20ecaf7a7", "filename": "src/librustc_trans/glue.rs", "status": "modified", "additions": 16, "deletions": 16, "changes": 32, "blob_url": "https://github.com/rust-lang/rust/blob/209abc71e52b864429330e8debd8ce1080a7f3b4/src%2Flibrustc_trans%2Fglue.rs", "raw_url": "https://github.com/rust-lang/rust/raw/209abc71e52b864429330e8debd8ce1080a7f3b4/src%2Flibrustc_trans%2Fglue.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_trans%2Fglue.rs?ref=209abc71e52b864429330e8debd8ce1080a7f3b4", "patch": "@@ -23,34 +23,34 @@ use rustc::ty::layout::LayoutOf;\n use rustc::ty::{self, Ty};\n use value::Value;\n \n-pub fn size_and_align_of_dst<'a, 'tcx>(bcx: &Builder<'a, 'tcx>, t: Ty<'tcx>, info: ValueRef)\n+pub fn size_and_align_of_dst<'a, 'tcx>(bx: &Builder<'a, 'tcx>, t: Ty<'tcx>, info: ValueRef)\n                                        -> (ValueRef, ValueRef) {\n     debug!(\"calculate size of DST: {}; with lost info: {:?}\",\n            t, Value(info));\n-    if bcx.cx.type_is_sized(t) {\n-        let (size, align) = bcx.cx.size_and_align_of(t);\n+    if bx.cx.type_is_sized(t) {\n+        let (size, align) = bx.cx.size_and_align_of(t);\n         debug!(\"size_and_align_of_dst t={} info={:?} size: {:?} align: {:?}\",\n                t, Value(info), size, align);\n-        let size = C_usize(bcx.cx, size.bytes());\n-        let align = C_usize(bcx.cx, align.abi());\n+        let size = C_usize(bx.cx, size.bytes());\n+        let align = C_usize(bx.cx, align.abi());\n         return (size, align);\n     }\n     assert!(!info.is_null());\n     match t.sty {\n         ty::TyDynamic(..) => {\n             // load size/align from vtable\n-            (meth::SIZE.get_usize(bcx, info), meth::ALIGN.get_usize(bcx, info))\n+            (meth::SIZE.get_usize(bx, info), meth::ALIGN.get_usize(bx, info))\n         }\n         ty::TySlice(_) | ty::TyStr => {\n-            let unit = t.sequence_element_type(bcx.tcx());\n+            let unit = t.sequence_element_type(bx.tcx());\n             // The info in this case is the length of the str, so the size is that\n             // times the unit size.\n-            let (size, align) = bcx.cx.size_and_align_of(unit);\n-            (bcx.mul(info, C_usize(bcx.cx, size.bytes())),\n-             C_usize(bcx.cx, align.abi()))\n+            let (size, align) = bx.cx.size_and_align_of(unit);\n+            (bx.mul(info, C_usize(bx.cx, size.bytes())),\n+             C_usize(bx.cx, align.abi()))\n         }\n         _ => {\n-            let cx = bcx.cx;\n+            let cx = bx.cx;\n             // First get the size of all statically known fields.\n             // Don't use size_of because it also rounds up to alignment, which we\n             // want to avoid, as the unsized field's alignment could be smaller.\n@@ -69,7 +69,7 @@ pub fn size_and_align_of_dst<'a, 'tcx>(bcx: &Builder<'a, 'tcx>, t: Ty<'tcx>, inf\n             // Recurse to get the size of the dynamically sized field (must be\n             // the last field).\n             let field_ty = layout.field(cx, i).ty;\n-            let (unsized_size, mut unsized_align) = size_and_align_of_dst(bcx, field_ty, info);\n+            let (unsized_size, mut unsized_align) = size_and_align_of_dst(bx, field_ty, info);\n \n             // FIXME (#26403, #27023): We should be adding padding\n             // to `sized_size` (to accommodate the `unsized_align`\n@@ -79,7 +79,7 @@ pub fn size_and_align_of_dst<'a, 'tcx>(bcx: &Builder<'a, 'tcx>, t: Ty<'tcx>, inf\n             // here. But this is where the add would go.)\n \n             // Return the sum of sizes and max of aligns.\n-            let size = bcx.add(sized_size, unsized_size);\n+            let size = bx.add(sized_size, unsized_size);\n \n             // Packed types ignore the alignment of their fields.\n             if let ty::TyAdt(def, _) = t.sty {\n@@ -97,7 +97,7 @@ pub fn size_and_align_of_dst<'a, 'tcx>(bcx: &Builder<'a, 'tcx>, t: Ty<'tcx>, inf\n                     // pick the correct alignment statically.\n                     C_usize(cx, std::cmp::max(sized_align, unsized_align) as u64)\n                 }\n-                _ => bcx.select(bcx.icmp(llvm::IntUGT, sized_align, unsized_align),\n+                _ => bx.select(bx.icmp(llvm::IntUGT, sized_align, unsized_align),\n                                 sized_align,\n                                 unsized_align)\n             };\n@@ -113,8 +113,8 @@ pub fn size_and_align_of_dst<'a, 'tcx>(bcx: &Builder<'a, 'tcx>, t: Ty<'tcx>, inf\n             //\n             //   `(size + (align-1)) & -align`\n \n-            let addend = bcx.sub(align, C_usize(bcx.cx, 1));\n-            let size = bcx.and(bcx.add(size, addend), bcx.neg(align));\n+            let addend = bx.sub(align, C_usize(bx.cx, 1));\n+            let size = bx.and(bx.add(size, addend), bx.neg(align));\n \n             (size, align)\n         }"}, {"sha": "b1f1fb52c907d099c6a5d75b69344ee5e57a5248", "filename": "src/librustc_trans/intrinsic.rs", "status": "modified", "additions": 171, "deletions": 171, "changes": 342, "blob_url": "https://github.com/rust-lang/rust/blob/209abc71e52b864429330e8debd8ce1080a7f3b4/src%2Flibrustc_trans%2Fintrinsic.rs", "raw_url": "https://github.com/rust-lang/rust/raw/209abc71e52b864429330e8debd8ce1080a7f3b4/src%2Flibrustc_trans%2Fintrinsic.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_trans%2Fintrinsic.rs?ref=209abc71e52b864429330e8debd8ce1080a7f3b4", "patch": "@@ -85,13 +85,13 @@ fn get_simple_intrinsic(cx: &CodegenCx, name: &str) -> Option<ValueRef> {\n /// Remember to add all intrinsics here, in librustc_typeck/check/mod.rs,\n /// and in libcore/intrinsics.rs; if you need access to any llvm intrinsics,\n /// add them to librustc_trans/trans/context.rs\n-pub fn trans_intrinsic_call<'a, 'tcx>(bcx: &Builder<'a, 'tcx>,\n+pub fn trans_intrinsic_call<'a, 'tcx>(bx: &Builder<'a, 'tcx>,\n                                       callee_ty: Ty<'tcx>,\n                                       fn_ty: &FnType<'tcx>,\n                                       args: &[OperandRef<'tcx>],\n                                       llresult: ValueRef,\n                                       span: Span) {\n-    let cx = bcx.cx;\n+    let cx = bx.cx;\n     let tcx = cx.tcx;\n \n     let (def_id, substs) = match callee_ty.sty {\n@@ -111,7 +111,7 @@ pub fn trans_intrinsic_call<'a, 'tcx>(bcx: &Builder<'a, 'tcx>,\n     let simple = get_simple_intrinsic(cx, name);\n     let llval = match name {\n         _ if simple.is_some() => {\n-            bcx.call(simple.unwrap(),\n+            bx.call(simple.unwrap(),\n                      &args.iter().map(|arg| arg.immediate()).collect::<Vec<_>>(),\n                      None)\n         }\n@@ -120,14 +120,14 @@ pub fn trans_intrinsic_call<'a, 'tcx>(bcx: &Builder<'a, 'tcx>,\n         },\n         \"likely\" => {\n             let expect = cx.get_intrinsic(&(\"llvm.expect.i1\"));\n-            bcx.call(expect, &[args[0].immediate(), C_bool(cx, true)], None)\n+            bx.call(expect, &[args[0].immediate(), C_bool(cx, true)], None)\n         }\n         \"unlikely\" => {\n             let expect = cx.get_intrinsic(&(\"llvm.expect.i1\"));\n-            bcx.call(expect, &[args[0].immediate(), C_bool(cx, false)], None)\n+            bx.call(expect, &[args[0].immediate(), C_bool(cx, false)], None)\n         }\n         \"try\" => {\n-            try_intrinsic(bcx, cx,\n+            try_intrinsic(bx, cx,\n                           args[0].immediate(),\n                           args[1].immediate(),\n                           args[2].immediate(),\n@@ -136,7 +136,7 @@ pub fn trans_intrinsic_call<'a, 'tcx>(bcx: &Builder<'a, 'tcx>,\n         }\n         \"breakpoint\" => {\n             let llfn = cx.get_intrinsic(&(\"llvm.debugtrap\"));\n-            bcx.call(llfn, &[], None)\n+            bx.call(llfn, &[], None)\n         }\n         \"size_of\" => {\n             let tp_ty = substs.type_at(0);\n@@ -146,7 +146,7 @@ pub fn trans_intrinsic_call<'a, 'tcx>(bcx: &Builder<'a, 'tcx>,\n             let tp_ty = substs.type_at(0);\n             if let OperandValue::Pair(_, meta) = args[0].val {\n                 let (llsize, _) =\n-                    glue::size_and_align_of_dst(bcx, tp_ty, meta);\n+                    glue::size_and_align_of_dst(bx, tp_ty, meta);\n                 llsize\n             } else {\n                 C_usize(cx, cx.size_of(tp_ty).bytes())\n@@ -160,7 +160,7 @@ pub fn trans_intrinsic_call<'a, 'tcx>(bcx: &Builder<'a, 'tcx>,\n             let tp_ty = substs.type_at(0);\n             if let OperandValue::Pair(_, meta) = args[0].val {\n                 let (_, llalign) =\n-                    glue::size_and_align_of_dst(bcx, tp_ty, meta);\n+                    glue::size_and_align_of_dst(bx, tp_ty, meta);\n                 llalign\n             } else {\n                 C_usize(cx, cx.align_of(tp_ty).abi())\n@@ -185,7 +185,7 @@ pub fn trans_intrinsic_call<'a, 'tcx>(bcx: &Builder<'a, 'tcx>,\n                 // If we store a zero constant, LLVM will drown in vreg allocation for large data\n                 // structures, and the generated code will be awful. (A telltale sign of this is\n                 // large quantities of `mov [byte ptr foo],0` in the generated code.)\n-                memset_intrinsic(bcx, false, ty, llresult, C_u8(cx, 0), C_usize(cx, 1));\n+                memset_intrinsic(bx, false, ty, llresult, C_u8(cx, 0), C_usize(cx, 1));\n             }\n             return;\n         }\n@@ -196,73 +196,73 @@ pub fn trans_intrinsic_call<'a, 'tcx>(bcx: &Builder<'a, 'tcx>,\n         \"needs_drop\" => {\n             let tp_ty = substs.type_at(0);\n \n-            C_bool(cx, bcx.cx.type_needs_drop(tp_ty))\n+            C_bool(cx, bx.cx.type_needs_drop(tp_ty))\n         }\n         \"offset\" => {\n             let ptr = args[0].immediate();\n             let offset = args[1].immediate();\n-            bcx.inbounds_gep(ptr, &[offset])\n+            bx.inbounds_gep(ptr, &[offset])\n         }\n         \"arith_offset\" => {\n             let ptr = args[0].immediate();\n             let offset = args[1].immediate();\n-            bcx.gep(ptr, &[offset])\n+            bx.gep(ptr, &[offset])\n         }\n \n         \"copy_nonoverlapping\" => {\n-            copy_intrinsic(bcx, false, false, substs.type_at(0),\n+            copy_intrinsic(bx, false, false, substs.type_at(0),\n                            args[1].immediate(), args[0].immediate(), args[2].immediate())\n         }\n         \"copy\" => {\n-            copy_intrinsic(bcx, true, false, substs.type_at(0),\n+            copy_intrinsic(bx, true, false, substs.type_at(0),\n                            args[1].immediate(), args[0].immediate(), args[2].immediate())\n         }\n         \"write_bytes\" => {\n-            memset_intrinsic(bcx, false, substs.type_at(0),\n+            memset_intrinsic(bx, false, substs.type_at(0),\n                              args[0].immediate(), args[1].immediate(), args[2].immediate())\n         }\n \n         \"volatile_copy_nonoverlapping_memory\" => {\n-            copy_intrinsic(bcx, false, true, substs.type_at(0),\n+            copy_intrinsic(bx, false, true, substs.type_at(0),\n                            args[0].immediate(), args[1].immediate(), args[2].immediate())\n         }\n         \"volatile_copy_memory\" => {\n-            copy_intrinsic(bcx, true, true, substs.type_at(0),\n+            copy_intrinsic(bx, true, true, substs.type_at(0),\n                            args[0].immediate(), args[1].immediate(), args[2].immediate())\n         }\n         \"volatile_set_memory\" => {\n-            memset_intrinsic(bcx, true, substs.type_at(0),\n+            memset_intrinsic(bx, true, substs.type_at(0),\n                              args[0].immediate(), args[1].immediate(), args[2].immediate())\n         }\n         \"volatile_load\" => {\n             let tp_ty = substs.type_at(0);\n             let mut ptr = args[0].immediate();\n             if let PassMode::Cast(ty) = fn_ty.ret.mode {\n-                ptr = bcx.pointercast(ptr, ty.llvm_type(cx).ptr_to());\n+                ptr = bx.pointercast(ptr, ty.llvm_type(cx).ptr_to());\n             }\n-            let load = bcx.volatile_load(ptr);\n+            let load = bx.volatile_load(ptr);\n             unsafe {\n                 llvm::LLVMSetAlignment(load, cx.align_of(tp_ty).abi() as u32);\n             }\n-            to_immediate(bcx, load, cx.layout_of(tp_ty))\n+            to_immediate(bx, load, cx.layout_of(tp_ty))\n         },\n         \"volatile_store\" => {\n             let tp_ty = substs.type_at(0);\n-            let dst = args[0].deref(bcx.cx);\n+            let dst = args[0].deref(bx.cx);\n             if let OperandValue::Pair(a, b) = args[1].val {\n-                bcx.volatile_store(a, dst.project_field(bcx, 0).llval);\n-                bcx.volatile_store(b, dst.project_field(bcx, 1).llval);\n+                bx.volatile_store(a, dst.project_field(bx, 0).llval);\n+                bx.volatile_store(b, dst.project_field(bx, 1).llval);\n             } else {\n                 let val = if let OperandValue::Ref(ptr, align) = args[1].val {\n-                    bcx.load(ptr, align)\n+                    bx.load(ptr, align)\n                 } else {\n                     if dst.layout.is_zst() {\n                         return;\n                     }\n-                    from_immediate(bcx, args[1].immediate())\n+                    from_immediate(bx, args[1].immediate())\n                 };\n-                let ptr = bcx.pointercast(dst.llval, val_ty(val).ptr_to());\n-                let store = bcx.volatile_store(val, ptr);\n+                let ptr = bx.pointercast(dst.llval, val_ty(val).ptr_to());\n+                let store = bx.volatile_store(val, ptr);\n                 unsafe {\n                     llvm::LLVMSetAlignment(store, cx.align_of(tp_ty).abi() as u32);\n                 }\n@@ -279,7 +279,7 @@ pub fn trans_intrinsic_call<'a, 'tcx>(bcx: &Builder<'a, 'tcx>,\n                 \"prefetch_write_instruction\" => (1, 0),\n                 _ => bug!()\n             };\n-            bcx.call(expect, &[\n+            bx.call(expect, &[\n                 args[0].immediate(),\n                 C_i32(cx, rw),\n                 args[1].immediate(),\n@@ -295,68 +295,68 @@ pub fn trans_intrinsic_call<'a, 'tcx>(bcx: &Builder<'a, 'tcx>,\n                 Some((width, signed)) =>\n                     match name {\n                         \"ctlz\" | \"cttz\" => {\n-                            let y = C_bool(bcx.cx, false);\n+                            let y = C_bool(bx.cx, false);\n                             let llfn = cx.get_intrinsic(&format!(\"llvm.{}.i{}\", name, width));\n-                            bcx.call(llfn, &[args[0].immediate(), y], None)\n+                            bx.call(llfn, &[args[0].immediate(), y], None)\n                         }\n                         \"ctlz_nonzero\" | \"cttz_nonzero\" => {\n-                            let y = C_bool(bcx.cx, true);\n+                            let y = C_bool(bx.cx, true);\n                             let llvm_name = &format!(\"llvm.{}.i{}\", &name[..4], width);\n                             let llfn = cx.get_intrinsic(llvm_name);\n-                            bcx.call(llfn, &[args[0].immediate(), y], None)\n+                            bx.call(llfn, &[args[0].immediate(), y], None)\n                         }\n-                        \"ctpop\" => bcx.call(cx.get_intrinsic(&format!(\"llvm.ctpop.i{}\", width)),\n+                        \"ctpop\" => bx.call(cx.get_intrinsic(&format!(\"llvm.ctpop.i{}\", width)),\n                                         &[args[0].immediate()], None),\n                         \"bswap\" => {\n                             if width == 8 {\n                                 args[0].immediate() // byte swap a u8/i8 is just a no-op\n                             } else {\n-                                bcx.call(cx.get_intrinsic(&format!(\"llvm.bswap.i{}\", width)),\n+                                bx.call(cx.get_intrinsic(&format!(\"llvm.bswap.i{}\", width)),\n                                         &[args[0].immediate()], None)\n                             }\n                         }\n                         \"add_with_overflow\" | \"sub_with_overflow\" | \"mul_with_overflow\" => {\n                             let intrinsic = format!(\"llvm.{}{}.with.overflow.i{}\",\n                                                     if signed { 's' } else { 'u' },\n                                                     &name[..3], width);\n-                            let llfn = bcx.cx.get_intrinsic(&intrinsic);\n+                            let llfn = bx.cx.get_intrinsic(&intrinsic);\n \n                             // Convert `i1` to a `bool`, and write it to the out parameter\n-                            let pair = bcx.call(llfn, &[\n+                            let pair = bx.call(llfn, &[\n                                 args[0].immediate(),\n                                 args[1].immediate()\n                             ], None);\n-                            let val = bcx.extract_value(pair, 0);\n-                            let overflow = bcx.zext(bcx.extract_value(pair, 1), Type::bool(cx));\n+                            let val = bx.extract_value(pair, 0);\n+                            let overflow = bx.zext(bx.extract_value(pair, 1), Type::bool(cx));\n \n-                            let dest = result.project_field(bcx, 0);\n-                            bcx.store(val, dest.llval, dest.align);\n-                            let dest = result.project_field(bcx, 1);\n-                            bcx.store(overflow, dest.llval, dest.align);\n+                            let dest = result.project_field(bx, 0);\n+                            bx.store(val, dest.llval, dest.align);\n+                            let dest = result.project_field(bx, 1);\n+                            bx.store(overflow, dest.llval, dest.align);\n \n                             return;\n                         },\n-                        \"overflowing_add\" => bcx.add(args[0].immediate(), args[1].immediate()),\n-                        \"overflowing_sub\" => bcx.sub(args[0].immediate(), args[1].immediate()),\n-                        \"overflowing_mul\" => bcx.mul(args[0].immediate(), args[1].immediate()),\n+                        \"overflowing_add\" => bx.add(args[0].immediate(), args[1].immediate()),\n+                        \"overflowing_sub\" => bx.sub(args[0].immediate(), args[1].immediate()),\n+                        \"overflowing_mul\" => bx.mul(args[0].immediate(), args[1].immediate()),\n                         \"unchecked_div\" =>\n                             if signed {\n-                                bcx.sdiv(args[0].immediate(), args[1].immediate())\n+                                bx.sdiv(args[0].immediate(), args[1].immediate())\n                             } else {\n-                                bcx.udiv(args[0].immediate(), args[1].immediate())\n+                                bx.udiv(args[0].immediate(), args[1].immediate())\n                             },\n                         \"unchecked_rem\" =>\n                             if signed {\n-                                bcx.srem(args[0].immediate(), args[1].immediate())\n+                                bx.srem(args[0].immediate(), args[1].immediate())\n                             } else {\n-                                bcx.urem(args[0].immediate(), args[1].immediate())\n+                                bx.urem(args[0].immediate(), args[1].immediate())\n                             },\n-                        \"unchecked_shl\" => bcx.shl(args[0].immediate(), args[1].immediate()),\n+                        \"unchecked_shl\" => bx.shl(args[0].immediate(), args[1].immediate()),\n                         \"unchecked_shr\" =>\n                             if signed {\n-                                bcx.ashr(args[0].immediate(), args[1].immediate())\n+                                bx.ashr(args[0].immediate(), args[1].immediate())\n                             } else {\n-                                bcx.lshr(args[0].immediate(), args[1].immediate())\n+                                bx.lshr(args[0].immediate(), args[1].immediate())\n                             },\n                         _ => bug!(),\n                     },\n@@ -375,11 +375,11 @@ pub fn trans_intrinsic_call<'a, 'tcx>(bcx: &Builder<'a, 'tcx>,\n             match float_type_width(sty) {\n                 Some(_width) =>\n                     match name {\n-                        \"fadd_fast\" => bcx.fadd_fast(args[0].immediate(), args[1].immediate()),\n-                        \"fsub_fast\" => bcx.fsub_fast(args[0].immediate(), args[1].immediate()),\n-                        \"fmul_fast\" => bcx.fmul_fast(args[0].immediate(), args[1].immediate()),\n-                        \"fdiv_fast\" => bcx.fdiv_fast(args[0].immediate(), args[1].immediate()),\n-                        \"frem_fast\" => bcx.frem_fast(args[0].immediate(), args[1].immediate()),\n+                        \"fadd_fast\" => bx.fadd_fast(args[0].immediate(), args[1].immediate()),\n+                        \"fsub_fast\" => bx.fsub_fast(args[0].immediate(), args[1].immediate()),\n+                        \"fmul_fast\" => bx.fmul_fast(args[0].immediate(), args[1].immediate()),\n+                        \"fdiv_fast\" => bx.fdiv_fast(args[0].immediate(), args[1].immediate()),\n+                        \"frem_fast\" => bx.frem_fast(args[0].immediate(), args[1].immediate()),\n                         _ => bug!(),\n                     },\n                 None => {\n@@ -394,23 +394,23 @@ pub fn trans_intrinsic_call<'a, 'tcx>(bcx: &Builder<'a, 'tcx>,\n         },\n \n         \"discriminant_value\" => {\n-            args[0].deref(bcx.cx).trans_get_discr(bcx, ret_ty)\n+            args[0].deref(bx.cx).trans_get_discr(bx, ret_ty)\n         }\n \n         \"align_offset\" => {\n             // `ptr as usize`\n-            let ptr_val = bcx.ptrtoint(args[0].immediate(), bcx.cx.isize_ty);\n+            let ptr_val = bx.ptrtoint(args[0].immediate(), bx.cx.isize_ty);\n             // `ptr_val % align`\n             let align = args[1].immediate();\n-            let offset = bcx.urem(ptr_val, align);\n-            let zero = C_null(bcx.cx.isize_ty);\n+            let offset = bx.urem(ptr_val, align);\n+            let zero = C_null(bx.cx.isize_ty);\n             // `offset == 0`\n-            let is_zero = bcx.icmp(llvm::IntPredicate::IntEQ, offset, zero);\n+            let is_zero = bx.icmp(llvm::IntPredicate::IntEQ, offset, zero);\n             // `if offset == 0 { 0 } else { align - offset }`\n-            bcx.select(is_zero, zero, bcx.sub(align, offset))\n+            bx.select(is_zero, zero, bx.sub(align, offset))\n         }\n         name if name.starts_with(\"simd_\") => {\n-            match generic_simd_intrinsic(bcx, name,\n+            match generic_simd_intrinsic(bx, name,\n                                          callee_ty,\n                                          args,\n                                          ret_ty, llret_ty,\n@@ -462,20 +462,20 @@ pub fn trans_intrinsic_call<'a, 'tcx>(bcx: &Builder<'a, 'tcx>,\n                     let ty = substs.type_at(0);\n                     if int_type_width_signed(ty, cx).is_some() {\n                         let weak = if split[1] == \"cxchgweak\" { llvm::True } else { llvm::False };\n-                        let pair = bcx.atomic_cmpxchg(\n+                        let pair = bx.atomic_cmpxchg(\n                             args[0].immediate(),\n                             args[1].immediate(),\n                             args[2].immediate(),\n                             order,\n                             failorder,\n                             weak);\n-                        let val = bcx.extract_value(pair, 0);\n-                        let success = bcx.zext(bcx.extract_value(pair, 1), Type::bool(bcx.cx));\n+                        let val = bx.extract_value(pair, 0);\n+                        let success = bx.zext(bx.extract_value(pair, 1), Type::bool(bx.cx));\n \n-                        let dest = result.project_field(bcx, 0);\n-                        bcx.store(val, dest.llval, dest.align);\n-                        let dest = result.project_field(bcx, 1);\n-                        bcx.store(success, dest.llval, dest.align);\n+                        let dest = result.project_field(bx, 0);\n+                        bx.store(val, dest.llval, dest.align);\n+                        let dest = result.project_field(bx, 1);\n+                        bx.store(success, dest.llval, dest.align);\n                         return;\n                     } else {\n                         return invalid_monomorphization(ty);\n@@ -486,7 +486,7 @@ pub fn trans_intrinsic_call<'a, 'tcx>(bcx: &Builder<'a, 'tcx>,\n                     let ty = substs.type_at(0);\n                     if int_type_width_signed(ty, cx).is_some() {\n                         let align = cx.align_of(ty);\n-                        bcx.atomic_load(args[0].immediate(), order, align)\n+                        bx.atomic_load(args[0].immediate(), order, align)\n                     } else {\n                         return invalid_monomorphization(ty);\n                     }\n@@ -496,20 +496,20 @@ pub fn trans_intrinsic_call<'a, 'tcx>(bcx: &Builder<'a, 'tcx>,\n                     let ty = substs.type_at(0);\n                     if int_type_width_signed(ty, cx).is_some() {\n                         let align = cx.align_of(ty);\n-                        bcx.atomic_store(args[1].immediate(), args[0].immediate(), order, align);\n+                        bx.atomic_store(args[1].immediate(), args[0].immediate(), order, align);\n                         return;\n                     } else {\n                         return invalid_monomorphization(ty);\n                     }\n                 }\n \n                 \"fence\" => {\n-                    bcx.atomic_fence(order, llvm::SynchronizationScope::CrossThread);\n+                    bx.atomic_fence(order, llvm::SynchronizationScope::CrossThread);\n                     return;\n                 }\n \n                 \"singlethreadfence\" => {\n-                    bcx.atomic_fence(order, llvm::SynchronizationScope::SingleThread);\n+                    bx.atomic_fence(order, llvm::SynchronizationScope::SingleThread);\n                     return;\n                 }\n \n@@ -532,7 +532,7 @@ pub fn trans_intrinsic_call<'a, 'tcx>(bcx: &Builder<'a, 'tcx>,\n \n                     let ty = substs.type_at(0);\n                     if int_type_width_signed(ty, cx).is_some() {\n-                        bcx.atomic_rmw(atom_op, args[0].immediate(), args[1].immediate(), order)\n+                        bx.atomic_rmw(atom_op, args[0].immediate(), args[1].immediate(), order)\n                     } else {\n                         return invalid_monomorphization(ty);\n                     }\n@@ -542,14 +542,14 @@ pub fn trans_intrinsic_call<'a, 'tcx>(bcx: &Builder<'a, 'tcx>,\n \n         \"nontemporal_store\" => {\n             let tp_ty = substs.type_at(0);\n-            let dst = args[0].deref(bcx.cx);\n+            let dst = args[0].deref(bx.cx);\n             let val = if let OperandValue::Ref(ptr, align) = args[1].val {\n-                bcx.load(ptr, align)\n+                bx.load(ptr, align)\n             } else {\n-                from_immediate(bcx, args[1].immediate())\n+                from_immediate(bx, args[1].immediate())\n             };\n-            let ptr = bcx.pointercast(dst.llval, val_ty(val).ptr_to());\n-            let store = bcx.nontemporal_store(val, ptr);\n+            let ptr = bx.pointercast(dst.llval, val_ty(val).ptr_to());\n+            let store = bx.nontemporal_store(val, ptr);\n             unsafe {\n                 llvm::LLVMSetAlignment(store, cx.align_of(tp_ty).abi() as u32);\n             }\n@@ -607,7 +607,7 @@ pub fn trans_intrinsic_call<'a, 'tcx>(bcx: &Builder<'a, 'tcx>,\n             // qux` to be converted into `foo, bar, baz, qux`, integer\n             // arguments to be truncated as needed and pointers to be\n             // cast.\n-            fn modify_as_needed<'a, 'tcx>(bcx: &Builder<'a, 'tcx>,\n+            fn modify_as_needed<'a, 'tcx>(bx: &Builder<'a, 'tcx>,\n                                           t: &intrinsics::Type,\n                                           arg: &OperandRef<'tcx>)\n                                           -> Vec<ValueRef>\n@@ -620,29 +620,29 @@ pub fn trans_intrinsic_call<'a, 'tcx>(bcx: &Builder<'a, 'tcx>,\n                         // This assumes the type is \"simple\", i.e. no\n                         // destructors, and the contents are SIMD\n                         // etc.\n-                        assert!(!bcx.cx.type_needs_drop(arg.layout.ty));\n+                        assert!(!bx.cx.type_needs_drop(arg.layout.ty));\n                         let (ptr, align) = match arg.val {\n                             OperandValue::Ref(ptr, align) => (ptr, align),\n                             _ => bug!()\n                         };\n                         let arg = PlaceRef::new_sized(ptr, arg.layout, align);\n                         (0..contents.len()).map(|i| {\n-                            arg.project_field(bcx, i).load(bcx).immediate()\n+                            arg.project_field(bx, i).load(bx).immediate()\n                         }).collect()\n                     }\n                     intrinsics::Type::Pointer(_, Some(ref llvm_elem), _) => {\n-                        let llvm_elem = one(ty_to_type(bcx.cx, llvm_elem));\n-                        vec![bcx.pointercast(arg.immediate(), llvm_elem.ptr_to())]\n+                        let llvm_elem = one(ty_to_type(bx.cx, llvm_elem));\n+                        vec![bx.pointercast(arg.immediate(), llvm_elem.ptr_to())]\n                     }\n                     intrinsics::Type::Vector(_, Some(ref llvm_elem), length) => {\n-                        let llvm_elem = one(ty_to_type(bcx.cx, llvm_elem));\n-                        vec![bcx.bitcast(arg.immediate(), Type::vector(&llvm_elem, length as u64))]\n+                        let llvm_elem = one(ty_to_type(bx.cx, llvm_elem));\n+                        vec![bx.bitcast(arg.immediate(), Type::vector(&llvm_elem, length as u64))]\n                     }\n                     intrinsics::Type::Integer(_, width, llvm_width) if width != llvm_width => {\n                         // the LLVM intrinsic uses a smaller integer\n                         // size than the C intrinsic's signature, so\n                         // we have to trim it down here.\n-                        vec![bcx.trunc(arg.immediate(), Type::ix(bcx.cx, llvm_width as u64))]\n+                        vec![bx.trunc(arg.immediate(), Type::ix(bx.cx, llvm_width as u64))]\n                     }\n                     _ => vec![arg.immediate()],\n                 }\n@@ -656,7 +656,7 @@ pub fn trans_intrinsic_call<'a, 'tcx>(bcx: &Builder<'a, 'tcx>,\n             let outputs = one(ty_to_type(cx, &intr.output));\n \n             let llargs: Vec<_> = intr.inputs.iter().zip(args).flat_map(|(t, arg)| {\n-                modify_as_needed(bcx, t, arg)\n+                modify_as_needed(bx, t, arg)\n             }).collect();\n             assert_eq!(inputs.len(), llargs.len());\n \n@@ -665,7 +665,7 @@ pub fn trans_intrinsic_call<'a, 'tcx>(bcx: &Builder<'a, 'tcx>,\n                     let f = declare::declare_cfn(cx,\n                                                  name,\n                                                  Type::func(&inputs, &outputs));\n-                    bcx.call(f, &llargs, None)\n+                    bx.call(f, &llargs, None)\n                 }\n             };\n \n@@ -675,9 +675,9 @@ pub fn trans_intrinsic_call<'a, 'tcx>(bcx: &Builder<'a, 'tcx>,\n                     assert!(!flatten);\n \n                     for i in 0..elems.len() {\n-                        let dest = result.project_field(bcx, i);\n-                        let val = bcx.extract_value(val, i as u64);\n-                        bcx.store(val, dest.llval, dest.align);\n+                        let dest = result.project_field(bx, i);\n+                        let val = bx.extract_value(val, i as u64);\n+                        bx.store(val, dest.llval, dest.align);\n                     }\n                     return;\n                 }\n@@ -688,24 +688,24 @@ pub fn trans_intrinsic_call<'a, 'tcx>(bcx: &Builder<'a, 'tcx>,\n \n     if !fn_ty.ret.is_ignore() {\n         if let PassMode::Cast(ty) = fn_ty.ret.mode {\n-            let ptr = bcx.pointercast(result.llval, ty.llvm_type(cx).ptr_to());\n-            bcx.store(llval, ptr, result.align);\n+            let ptr = bx.pointercast(result.llval, ty.llvm_type(cx).ptr_to());\n+            bx.store(llval, ptr, result.align);\n         } else {\n-            OperandRef::from_immediate_or_packed_pair(bcx, llval, result.layout)\n-                .val.store(bcx, result);\n+            OperandRef::from_immediate_or_packed_pair(bx, llval, result.layout)\n+                .val.store(bx, result);\n         }\n     }\n }\n \n-fn copy_intrinsic<'a, 'tcx>(bcx: &Builder<'a, 'tcx>,\n+fn copy_intrinsic<'a, 'tcx>(bx: &Builder<'a, 'tcx>,\n                             allow_overlap: bool,\n                             volatile: bool,\n                             ty: Ty<'tcx>,\n                             dst: ValueRef,\n                             src: ValueRef,\n                             count: ValueRef)\n                             -> ValueRef {\n-    let cx = bcx.cx;\n+    let cx = bx.cx;\n     let (size, align) = cx.size_and_align_of(ty);\n     let size = C_usize(cx, size.bytes());\n     let align = C_i32(cx, align.abi() as i32);\n@@ -719,51 +719,51 @@ fn copy_intrinsic<'a, 'tcx>(bcx: &Builder<'a, 'tcx>,\n     let name = format!(\"llvm.{}.p0i8.p0i8.i{}\", operation,\n                        cx.data_layout().pointer_size.bits());\n \n-    let dst_ptr = bcx.pointercast(dst, Type::i8p(cx));\n-    let src_ptr = bcx.pointercast(src, Type::i8p(cx));\n+    let dst_ptr = bx.pointercast(dst, Type::i8p(cx));\n+    let src_ptr = bx.pointercast(src, Type::i8p(cx));\n     let llfn = cx.get_intrinsic(&name);\n \n-    bcx.call(llfn,\n+    bx.call(llfn,\n         &[dst_ptr,\n         src_ptr,\n-        bcx.mul(size, count),\n+        bx.mul(size, count),\n         align,\n         C_bool(cx, volatile)],\n         None)\n }\n \n fn memset_intrinsic<'a, 'tcx>(\n-    bcx: &Builder<'a, 'tcx>,\n+    bx: &Builder<'a, 'tcx>,\n     volatile: bool,\n     ty: Ty<'tcx>,\n     dst: ValueRef,\n     val: ValueRef,\n     count: ValueRef\n ) -> ValueRef {\n-    let cx = bcx.cx;\n+    let cx = bx.cx;\n     let (size, align) = cx.size_and_align_of(ty);\n     let size = C_usize(cx, size.bytes());\n     let align = C_i32(cx, align.abi() as i32);\n-    let dst = bcx.pointercast(dst, Type::i8p(cx));\n-    call_memset(bcx, dst, val, bcx.mul(size, count), align, volatile)\n+    let dst = bx.pointercast(dst, Type::i8p(cx));\n+    call_memset(bx, dst, val, bx.mul(size, count), align, volatile)\n }\n \n fn try_intrinsic<'a, 'tcx>(\n-    bcx: &Builder<'a, 'tcx>,\n+    bx: &Builder<'a, 'tcx>,\n     cx: &CodegenCx,\n     func: ValueRef,\n     data: ValueRef,\n     local_ptr: ValueRef,\n     dest: ValueRef,\n ) {\n-    if bcx.sess().no_landing_pads() {\n-        bcx.call(func, &[data], None);\n-        let ptr_align = bcx.tcx().data_layout.pointer_align;\n-        bcx.store(C_null(Type::i8p(&bcx.cx)), dest, ptr_align);\n-    } else if wants_msvc_seh(bcx.sess()) {\n-        trans_msvc_try(bcx, cx, func, data, local_ptr, dest);\n+    if bx.sess().no_landing_pads() {\n+        bx.call(func, &[data], None);\n+        let ptr_align = bx.tcx().data_layout.pointer_align;\n+        bx.store(C_null(Type::i8p(&bx.cx)), dest, ptr_align);\n+    } else if wants_msvc_seh(bx.sess()) {\n+        trans_msvc_try(bx, cx, func, data, local_ptr, dest);\n     } else {\n-        trans_gnu_try(bcx, cx, func, data, local_ptr, dest);\n+        trans_gnu_try(bx, cx, func, data, local_ptr, dest);\n     }\n }\n \n@@ -774,25 +774,25 @@ fn try_intrinsic<'a, 'tcx>(\n // instructions are meant to work for all targets, as of the time of this\n // writing, however, LLVM does not recommend the usage of these new instructions\n // as the old ones are still more optimized.\n-fn trans_msvc_try<'a, 'tcx>(bcx: &Builder<'a, 'tcx>,\n+fn trans_msvc_try<'a, 'tcx>(bx: &Builder<'a, 'tcx>,\n                             cx: &CodegenCx,\n                             func: ValueRef,\n                             data: ValueRef,\n                             local_ptr: ValueRef,\n                             dest: ValueRef) {\n-    let llfn = get_rust_try_fn(cx, &mut |bcx| {\n-        let cx = bcx.cx;\n+    let llfn = get_rust_try_fn(cx, &mut |bx| {\n+        let cx = bx.cx;\n \n-        bcx.set_personality_fn(bcx.cx.eh_personality());\n+        bx.set_personality_fn(bx.cx.eh_personality());\n \n-        let normal = bcx.build_sibling_block(\"normal\");\n-        let catchswitch = bcx.build_sibling_block(\"catchswitch\");\n-        let catchpad = bcx.build_sibling_block(\"catchpad\");\n-        let caught = bcx.build_sibling_block(\"caught\");\n+        let normal = bx.build_sibling_block(\"normal\");\n+        let catchswitch = bx.build_sibling_block(\"catchswitch\");\n+        let catchpad = bx.build_sibling_block(\"catchpad\");\n+        let caught = bx.build_sibling_block(\"caught\");\n \n-        let func = llvm::get_param(bcx.llfn(), 0);\n-        let data = llvm::get_param(bcx.llfn(), 1);\n-        let local_ptr = llvm::get_param(bcx.llfn(), 2);\n+        let func = llvm::get_param(bx.llfn(), 0);\n+        let data = llvm::get_param(bx.llfn(), 1);\n+        let local_ptr = llvm::get_param(bx.llfn(), 2);\n \n         // We're generating an IR snippet that looks like:\n         //\n@@ -834,9 +834,9 @@ fn trans_msvc_try<'a, 'tcx>(bcx: &Builder<'a, 'tcx>,\n         //\n         // More information can be found in libstd's seh.rs implementation.\n         let i64p = Type::i64(cx).ptr_to();\n-        let ptr_align = bcx.tcx().data_layout.pointer_align;\n-        let slot = bcx.alloca(i64p, \"slot\", ptr_align);\n-        bcx.invoke(func, &[data], normal.llbb(), catchswitch.llbb(),\n+        let ptr_align = bx.tcx().data_layout.pointer_align;\n+        let slot = bx.alloca(i64p, \"slot\", ptr_align);\n+        bx.invoke(func, &[data], normal.llbb(), catchswitch.llbb(),\n             None);\n \n         normal.ret(C_i32(cx, 0));\n@@ -852,7 +852,7 @@ fn trans_msvc_try<'a, 'tcx>(bcx: &Builder<'a, 'tcx>,\n         let tok = catchpad.catch_pad(cs, &[tydesc, C_i32(cx, 0), slot]);\n         let addr = catchpad.load(slot, ptr_align);\n \n-        let i64_align = bcx.tcx().data_layout.i64_align;\n+        let i64_align = bx.tcx().data_layout.i64_align;\n         let arg1 = catchpad.load(addr, i64_align);\n         let val1 = C_i32(cx, 1);\n         let arg2 = catchpad.load(catchpad.inbounds_gep(addr, &[val1]), i64_align);\n@@ -866,9 +866,9 @@ fn trans_msvc_try<'a, 'tcx>(bcx: &Builder<'a, 'tcx>,\n \n     // Note that no invoke is used here because by definition this function\n     // can't panic (that's what it's catching).\n-    let ret = bcx.call(llfn, &[func, data, local_ptr], None);\n-    let i32_align = bcx.tcx().data_layout.i32_align;\n-    bcx.store(ret, dest, i32_align);\n+    let ret = bx.call(llfn, &[func, data, local_ptr], None);\n+    let i32_align = bx.tcx().data_layout.i32_align;\n+    bx.store(ret, dest, i32_align);\n }\n \n // Definition of the standard \"try\" function for Rust using the GNU-like model\n@@ -882,18 +882,18 @@ fn trans_msvc_try<'a, 'tcx>(bcx: &Builder<'a, 'tcx>,\n // function calling it, and that function may already have other personality\n // functions in play. By calling a shim we're guaranteed that our shim will have\n // the right personality function.\n-fn trans_gnu_try<'a, 'tcx>(bcx: &Builder<'a, 'tcx>,\n+fn trans_gnu_try<'a, 'tcx>(bx: &Builder<'a, 'tcx>,\n                            cx: &CodegenCx,\n                            func: ValueRef,\n                            data: ValueRef,\n                            local_ptr: ValueRef,\n                            dest: ValueRef) {\n-    let llfn = get_rust_try_fn(cx, &mut |bcx| {\n-        let cx = bcx.cx;\n+    let llfn = get_rust_try_fn(cx, &mut |bx| {\n+        let cx = bx.cx;\n \n         // Translates the shims described above:\n         //\n-        //   bcx:\n+        //   bx:\n         //      invoke %func(%args...) normal %normal unwind %catch\n         //\n         //   normal:\n@@ -908,13 +908,13 @@ fn trans_gnu_try<'a, 'tcx>(bcx: &Builder<'a, 'tcx>,\n         // expected to be `*mut *mut u8` for this to actually work, but that's\n         // managed by the standard library.\n \n-        let then = bcx.build_sibling_block(\"then\");\n-        let catch = bcx.build_sibling_block(\"catch\");\n+        let then = bx.build_sibling_block(\"then\");\n+        let catch = bx.build_sibling_block(\"catch\");\n \n-        let func = llvm::get_param(bcx.llfn(), 0);\n-        let data = llvm::get_param(bcx.llfn(), 1);\n-        let local_ptr = llvm::get_param(bcx.llfn(), 2);\n-        bcx.invoke(func, &[data], then.llbb(), catch.llbb(), None);\n+        let func = llvm::get_param(bx.llfn(), 0);\n+        let data = llvm::get_param(bx.llfn(), 1);\n+        let local_ptr = llvm::get_param(bx.llfn(), 2);\n+        bx.invoke(func, &[data], then.llbb(), catch.llbb(), None);\n         then.ret(C_i32(cx, 0));\n \n         // Type indicator for the exception being thrown.\n@@ -925,19 +925,19 @@ fn trans_gnu_try<'a, 'tcx>(bcx: &Builder<'a, 'tcx>,\n         // rust_try ignores the selector.\n         let lpad_ty = Type::struct_(cx, &[Type::i8p(cx), Type::i32(cx)],\n                                     false);\n-        let vals = catch.landing_pad(lpad_ty, bcx.cx.eh_personality(), 1);\n+        let vals = catch.landing_pad(lpad_ty, bx.cx.eh_personality(), 1);\n         catch.add_clause(vals, C_null(Type::i8p(cx)));\n         let ptr = catch.extract_value(vals, 0);\n-        let ptr_align = bcx.tcx().data_layout.pointer_align;\n+        let ptr_align = bx.tcx().data_layout.pointer_align;\n         catch.store(ptr, catch.bitcast(local_ptr, Type::i8p(cx).ptr_to()), ptr_align);\n         catch.ret(C_i32(cx, 1));\n     });\n \n     // Note that no invoke is used here because by definition this function\n     // can't panic (that's what it's catching).\n-    let ret = bcx.call(llfn, &[func, data, local_ptr], None);\n-    let i32_align = bcx.tcx().data_layout.i32_align;\n-    bcx.store(ret, dest, i32_align);\n+    let ret = bx.call(llfn, &[func, data, local_ptr], None);\n+    let i32_align = bx.tcx().data_layout.i32_align;\n+    bx.store(ret, dest, i32_align);\n }\n \n // Helper function to give a Block to a closure to translate a shim function.\n@@ -956,8 +956,8 @@ fn gen_fn<'a, 'tcx>(cx: &CodegenCx<'a, 'tcx>,\n         Abi::Rust\n     )));\n     let llfn = declare::define_internal_fn(cx, name, rust_fn_ty);\n-    let bcx = Builder::new_block(cx, llfn, \"entry-block\");\n-    trans(bcx);\n+    let bx = Builder::new_block(cx, llfn, \"entry-block\");\n+    trans(bx);\n     llfn\n }\n \n@@ -993,7 +993,7 @@ fn span_invalid_monomorphization_error(a: &Session, b: Span, c: &str) {\n }\n \n fn generic_simd_intrinsic<'a, 'tcx>(\n-    bcx: &Builder<'a, 'tcx>,\n+    bx: &Builder<'a, 'tcx>,\n     name: &str,\n     callee_ty: Ty<'tcx>,\n     args: &[OperandRef<'tcx>],\n@@ -1008,7 +1008,7 @@ fn generic_simd_intrinsic<'a, 'tcx>(\n         };\n         ($msg: tt, $($fmt: tt)*) => {\n             span_invalid_monomorphization_error(\n-                bcx.sess(), span,\n+                bx.sess(), span,\n                 &format!(concat!(\"invalid monomorphization of `{}` intrinsic: \",\n                                  $msg),\n                          name, $($fmt)*));\n@@ -1030,7 +1030,7 @@ fn generic_simd_intrinsic<'a, 'tcx>(\n \n \n \n-    let tcx = bcx.tcx();\n+    let tcx = bx.tcx();\n     let sig = tcx.erase_late_bound_regions_and_normalize(&callee_ty.fn_sig(tcx));\n     let arg_tys = sig.inputs();\n \n@@ -1064,7 +1064,7 @@ fn generic_simd_intrinsic<'a, 'tcx>(\n                  ret_ty,\n                  ret_ty.simd_type(tcx));\n \n-        return Ok(compare_simd_types(bcx,\n+        return Ok(compare_simd_types(bx,\n                                      args[0].immediate(),\n                                      args[1].immediate(),\n                                      in_elem,\n@@ -1109,7 +1109,7 @@ fn generic_simd_intrinsic<'a, 'tcx>(\n                                     arg_idx, total_len);\n                         None\n                     }\n-                    Some(idx) => Some(C_i32(bcx.cx, idx as i32)),\n+                    Some(idx) => Some(C_i32(bx.cx, idx as i32)),\n                 }\n             })\n             .collect();\n@@ -1118,7 +1118,7 @@ fn generic_simd_intrinsic<'a, 'tcx>(\n             None => return Ok(C_null(llret_ty))\n         };\n \n-        return Ok(bcx.shuffle_vector(args[0].immediate(),\n+        return Ok(bx.shuffle_vector(args[0].immediate(),\n                                      args[1].immediate(),\n                                      C_vector(&indices)))\n     }\n@@ -1127,15 +1127,15 @@ fn generic_simd_intrinsic<'a, 'tcx>(\n         require!(in_elem == arg_tys[2],\n                  \"expected inserted type `{}` (element of input `{}`), found `{}`\",\n                  in_elem, in_ty, arg_tys[2]);\n-        return Ok(bcx.insert_element(args[0].immediate(),\n+        return Ok(bx.insert_element(args[0].immediate(),\n                                      args[2].immediate(),\n                                      args[1].immediate()))\n     }\n     if name == \"simd_extract\" {\n         require!(ret_ty == in_elem,\n                  \"expected return type `{}` (element of input `{}`), found `{}`\",\n                  in_elem, in_ty, ret_ty);\n-        return Ok(bcx.extract_element(args[0].immediate(), args[1].immediate()))\n+        return Ok(bx.extract_element(args[0].immediate(), args[1].immediate()))\n     }\n \n     if name == \"simd_cast\" {\n@@ -1171,34 +1171,34 @@ fn generic_simd_intrinsic<'a, 'tcx>(\n         match (in_style, out_style) {\n             (Style::Int(in_is_signed), Style::Int(_)) => {\n                 return Ok(match in_width.cmp(&out_width) {\n-                    Ordering::Greater => bcx.trunc(args[0].immediate(), llret_ty),\n+                    Ordering::Greater => bx.trunc(args[0].immediate(), llret_ty),\n                     Ordering::Equal => args[0].immediate(),\n                     Ordering::Less => if in_is_signed {\n-                        bcx.sext(args[0].immediate(), llret_ty)\n+                        bx.sext(args[0].immediate(), llret_ty)\n                     } else {\n-                        bcx.zext(args[0].immediate(), llret_ty)\n+                        bx.zext(args[0].immediate(), llret_ty)\n                     }\n                 })\n             }\n             (Style::Int(in_is_signed), Style::Float) => {\n                 return Ok(if in_is_signed {\n-                    bcx.sitofp(args[0].immediate(), llret_ty)\n+                    bx.sitofp(args[0].immediate(), llret_ty)\n                 } else {\n-                    bcx.uitofp(args[0].immediate(), llret_ty)\n+                    bx.uitofp(args[0].immediate(), llret_ty)\n                 })\n             }\n             (Style::Float, Style::Int(out_is_signed)) => {\n                 return Ok(if out_is_signed {\n-                    bcx.fptosi(args[0].immediate(), llret_ty)\n+                    bx.fptosi(args[0].immediate(), llret_ty)\n                 } else {\n-                    bcx.fptoui(args[0].immediate(), llret_ty)\n+                    bx.fptoui(args[0].immediate(), llret_ty)\n                 })\n             }\n             (Style::Float, Style::Float) => {\n                 return Ok(match in_width.cmp(&out_width) {\n-                    Ordering::Greater => bcx.fptrunc(args[0].immediate(), llret_ty),\n+                    Ordering::Greater => bx.fptrunc(args[0].immediate(), llret_ty),\n                     Ordering::Equal => args[0].immediate(),\n-                    Ordering::Less => bcx.fpext(args[0].immediate(), llret_ty)\n+                    Ordering::Less => bx.fpext(args[0].immediate(), llret_ty)\n                 })\n             }\n             _ => {/* Unsupported. Fallthrough. */}\n@@ -1213,7 +1213,7 @@ fn generic_simd_intrinsic<'a, 'tcx>(\n             $(if name == stringify!($name) {\n                 match in_elem.sty {\n                     $($(ty::$p(_))|* => {\n-                        return Ok(bcx.$call(args[0].immediate(), args[1].immediate()))\n+                        return Ok(bx.$call(args[0].immediate(), args[1].immediate()))\n                     })*\n                     _ => {},\n                 }"}, {"sha": "6b542ae2e9364cad6e81d2b640baa6956e19af94", "filename": "src/librustc_trans/meth.rs", "status": "modified", "additions": 11, "deletions": 11, "changes": 22, "blob_url": "https://github.com/rust-lang/rust/blob/209abc71e52b864429330e8debd8ce1080a7f3b4/src%2Flibrustc_trans%2Fmeth.rs", "raw_url": "https://github.com/rust-lang/rust/raw/209abc71e52b864429330e8debd8ce1080a7f3b4/src%2Flibrustc_trans%2Fmeth.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_trans%2Fmeth.rs?ref=209abc71e52b864429330e8debd8ce1080a7f3b4", "patch": "@@ -33,30 +33,30 @@ impl<'a, 'tcx> VirtualIndex {\n         VirtualIndex(index as u64 + 3)\n     }\n \n-    pub fn get_fn(self, bcx: &Builder<'a, 'tcx>,\n+    pub fn get_fn(self, bx: &Builder<'a, 'tcx>,\n                   llvtable: ValueRef,\n                   fn_ty: &FnType<'tcx>) -> ValueRef {\n         // Load the data pointer from the object.\n         debug!(\"get_fn({:?}, {:?})\", Value(llvtable), self);\n \n-        let llvtable = bcx.pointercast(llvtable, fn_ty.llvm_type(bcx.cx).ptr_to().ptr_to());\n-        let ptr_align = bcx.tcx().data_layout.pointer_align;\n-        let ptr = bcx.load(bcx.inbounds_gep(llvtable, &[C_usize(bcx.cx, self.0)]), ptr_align);\n-        bcx.nonnull_metadata(ptr);\n+        let llvtable = bx.pointercast(llvtable, fn_ty.llvm_type(bx.cx).ptr_to().ptr_to());\n+        let ptr_align = bx.tcx().data_layout.pointer_align;\n+        let ptr = bx.load(bx.inbounds_gep(llvtable, &[C_usize(bx.cx, self.0)]), ptr_align);\n+        bx.nonnull_metadata(ptr);\n         // Vtable loads are invariant\n-        bcx.set_invariant_load(ptr);\n+        bx.set_invariant_load(ptr);\n         ptr\n     }\n \n-    pub fn get_usize(self, bcx: &Builder<'a, 'tcx>, llvtable: ValueRef) -> ValueRef {\n+    pub fn get_usize(self, bx: &Builder<'a, 'tcx>, llvtable: ValueRef) -> ValueRef {\n         // Load the data pointer from the object.\n         debug!(\"get_int({:?}, {:?})\", Value(llvtable), self);\n \n-        let llvtable = bcx.pointercast(llvtable, Type::isize(bcx.cx).ptr_to());\n-        let usize_align = bcx.tcx().data_layout.pointer_align;\n-        let ptr = bcx.load(bcx.inbounds_gep(llvtable, &[C_usize(bcx.cx, self.0)]), usize_align);\n+        let llvtable = bx.pointercast(llvtable, Type::isize(bx.cx).ptr_to());\n+        let usize_align = bx.tcx().data_layout.pointer_align;\n+        let ptr = bx.load(bx.inbounds_gep(llvtable, &[C_usize(bx.cx, self.0)]), usize_align);\n         // Vtable loads are invariant\n-        bcx.set_invariant_load(ptr);\n+        bx.set_invariant_load(ptr);\n         ptr\n     }\n }"}, {"sha": "c23d8d43b1e8d7946d735b345244d5ce3f8278cf", "filename": "src/librustc_trans/mir/block.rs", "status": "modified", "additions": 187, "deletions": 187, "changes": 374, "blob_url": "https://github.com/rust-lang/rust/blob/209abc71e52b864429330e8debd8ce1080a7f3b4/src%2Flibrustc_trans%2Fmir%2Fblock.rs", "raw_url": "https://github.com/rust-lang/rust/raw/209abc71e52b864429330e8debd8ce1080a7f3b4/src%2Flibrustc_trans%2Fmir%2Fblock.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_trans%2Fmir%2Fblock.rs?ref=209abc71e52b864429330e8debd8ce1080a7f3b4", "patch": "@@ -37,27 +37,27 @@ use super::operand::OperandValue::{Pair, Ref, Immediate};\n \n impl<'a, 'tcx> MirContext<'a, 'tcx> {\n     pub fn trans_block(&mut self, bb: mir::BasicBlock) {\n-        let mut bcx = self.get_builder(bb);\n+        let mut bx = self.build_block(bb);\n         let data = &self.mir[bb];\n \n         debug!(\"trans_block({:?}={:?})\", bb, data);\n \n         for statement in &data.statements {\n-            bcx = self.trans_statement(bcx, statement);\n+            bx = self.trans_statement(bx, statement);\n         }\n \n-        self.trans_terminator(bcx, bb, data.terminator());\n+        self.trans_terminator(bx, bb, data.terminator());\n     }\n \n     fn trans_terminator(&mut self,\n-                        mut bcx: Builder<'a, 'tcx>,\n+                        mut bx: Builder<'a, 'tcx>,\n                         bb: mir::BasicBlock,\n                         terminator: &mir::Terminator<'tcx>)\n     {\n         debug!(\"trans_terminator: {:?}\", terminator);\n \n         // Create the cleanup bundle, if needed.\n-        let tcx = bcx.tcx();\n+        let tcx = bx.tcx();\n         let span = terminator.source_info.span;\n         let funclet_bb = self.cleanup_kinds[bb].funclet_bb(bb);\n         let funclet = funclet_bb.and_then(|funclet_bb| self.funclets[funclet_bb].as_ref());\n@@ -99,46 +99,46 @@ impl<'a, 'tcx> MirContext<'a, 'tcx> {\n             }\n         };\n \n-        let funclet_br = |this: &mut Self, bcx: Builder, target: mir::BasicBlock| {\n+        let funclet_br = |this: &mut Self, bx: Builder, target: mir::BasicBlock| {\n             let (lltarget, is_cleanupret) = lltarget(this, target);\n             if is_cleanupret {\n                 // micro-optimization: generate a `ret` rather than a jump\n                 // to a trampoline.\n-                bcx.cleanup_ret(cleanup_pad.unwrap(), Some(lltarget));\n+                bx.cleanup_ret(cleanup_pad.unwrap(), Some(lltarget));\n             } else {\n-                bcx.br(lltarget);\n+                bx.br(lltarget);\n             }\n         };\n \n         let do_call = |\n             this: &mut Self,\n-            bcx: Builder<'a, 'tcx>,\n+            bx: Builder<'a, 'tcx>,\n             fn_ty: FnType<'tcx>,\n             fn_ptr: ValueRef,\n             llargs: &[ValueRef],\n             destination: Option<(ReturnDest<'tcx>, mir::BasicBlock)>,\n             cleanup: Option<mir::BasicBlock>\n         | {\n             if let Some(cleanup) = cleanup {\n-                let ret_bcx = if let Some((_, target)) = destination {\n+                let ret_bx = if let Some((_, target)) = destination {\n                     this.blocks[target]\n                 } else {\n                     this.unreachable_block()\n                 };\n-                let invokeret = bcx.invoke(fn_ptr,\n+                let invokeret = bx.invoke(fn_ptr,\n                                            &llargs,\n-                                           ret_bcx,\n+                                           ret_bx,\n                                            llblock(this, cleanup),\n                                            cleanup_bundle);\n                 fn_ty.apply_attrs_callsite(invokeret);\n \n                 if let Some((ret_dest, target)) = destination {\n-                    let ret_bcx = this.get_builder(target);\n-                    this.set_debug_loc(&ret_bcx, terminator.source_info);\n-                    this.store_return(&ret_bcx, ret_dest, &fn_ty.ret, invokeret);\n+                    let ret_bx = this.build_block(target);\n+                    this.set_debug_loc(&ret_bx, terminator.source_info);\n+                    this.store_return(&ret_bx, ret_dest, &fn_ty.ret, invokeret);\n                 }\n             } else {\n-                let llret = bcx.call(fn_ptr, &llargs, cleanup_bundle);\n+                let llret = bx.call(fn_ptr, &llargs, cleanup_bundle);\n                 fn_ty.apply_attrs_callsite(llret);\n                 if this.mir[bb].is_cleanup {\n                     // Cleanup is always the cold path. Don't inline\n@@ -149,83 +149,83 @@ impl<'a, 'tcx> MirContext<'a, 'tcx> {\n                 }\n \n                 if let Some((ret_dest, target)) = destination {\n-                    this.store_return(&bcx, ret_dest, &fn_ty.ret, llret);\n-                    funclet_br(this, bcx, target);\n+                    this.store_return(&bx, ret_dest, &fn_ty.ret, llret);\n+                    funclet_br(this, bx, target);\n                 } else {\n-                    bcx.unreachable();\n+                    bx.unreachable();\n                 }\n             }\n         };\n \n-        self.set_debug_loc(&bcx, terminator.source_info);\n+        self.set_debug_loc(&bx, terminator.source_info);\n         match terminator.kind {\n             mir::TerminatorKind::Resume => {\n                 if let Some(cleanup_pad) = cleanup_pad {\n-                    bcx.cleanup_ret(cleanup_pad, None);\n+                    bx.cleanup_ret(cleanup_pad, None);\n                 } else {\n-                    let slot = self.get_personality_slot(&bcx);\n-                    let lp0 = slot.project_field(&bcx, 0).load(&bcx).immediate();\n-                    let lp1 = slot.project_field(&bcx, 1).load(&bcx).immediate();\n-                    slot.storage_dead(&bcx);\n+                    let slot = self.get_personality_slot(&bx);\n+                    let lp0 = slot.project_field(&bx, 0).load(&bx).immediate();\n+                    let lp1 = slot.project_field(&bx, 1).load(&bx).immediate();\n+                    slot.storage_dead(&bx);\n \n-                    if !bcx.sess().target.target.options.custom_unwind_resume {\n+                    if !bx.sess().target.target.options.custom_unwind_resume {\n                         let mut lp = C_undef(self.landing_pad_type());\n-                        lp = bcx.insert_value(lp, lp0, 0);\n-                        lp = bcx.insert_value(lp, lp1, 1);\n-                        bcx.resume(lp);\n+                        lp = bx.insert_value(lp, lp0, 0);\n+                        lp = bx.insert_value(lp, lp1, 1);\n+                        bx.resume(lp);\n                     } else {\n-                        bcx.call(bcx.cx.eh_unwind_resume(), &[lp0], cleanup_bundle);\n-                        bcx.unreachable();\n+                        bx.call(bx.cx.eh_unwind_resume(), &[lp0], cleanup_bundle);\n+                        bx.unreachable();\n                     }\n                 }\n             }\n \n             mir::TerminatorKind::Abort => {\n                 // Call core::intrinsics::abort()\n-                let fnname = bcx.cx.get_intrinsic(&(\"llvm.trap\"));\n-                bcx.call(fnname, &[], None);\n-                bcx.unreachable();\n+                let fnname = bx.cx.get_intrinsic(&(\"llvm.trap\"));\n+                bx.call(fnname, &[], None);\n+                bx.unreachable();\n             }\n \n             mir::TerminatorKind::Goto { target } => {\n-                funclet_br(self, bcx, target);\n+                funclet_br(self, bx, target);\n             }\n \n             mir::TerminatorKind::SwitchInt { ref discr, switch_ty, ref values, ref targets } => {\n-                let discr = self.trans_operand(&bcx, discr);\n-                if switch_ty == bcx.tcx().types.bool {\n+                let discr = self.trans_operand(&bx, discr);\n+                if switch_ty == bx.tcx().types.bool {\n                     let lltrue = llblock(self, targets[0]);\n                     let llfalse = llblock(self, targets[1]);\n                     if let [ConstInt::U8(0)] = values[..] {\n-                        bcx.cond_br(discr.immediate(), llfalse, lltrue);\n+                        bx.cond_br(discr.immediate(), llfalse, lltrue);\n                     } else {\n-                        bcx.cond_br(discr.immediate(), lltrue, llfalse);\n+                        bx.cond_br(discr.immediate(), lltrue, llfalse);\n                     }\n                 } else {\n                     let (otherwise, targets) = targets.split_last().unwrap();\n-                    let switch = bcx.switch(discr.immediate(),\n+                    let switch = bx.switch(discr.immediate(),\n                                             llblock(self, *otherwise), values.len());\n                     for (value, target) in values.iter().zip(targets) {\n-                        let val = Const::from_constint(bcx.cx, value);\n+                        let val = Const::from_constint(bx.cx, value);\n                         let llbb = llblock(self, *target);\n-                        bcx.add_case(switch, val.llval, llbb)\n+                        bx.add_case(switch, val.llval, llbb)\n                     }\n                 }\n             }\n \n             mir::TerminatorKind::Return => {\n                 let llval = match self.fn_ty.ret.mode {\n                     PassMode::Ignore | PassMode::Indirect(_) => {\n-                        bcx.ret_void();\n+                        bx.ret_void();\n                         return;\n                     }\n \n                     PassMode::Direct(_) | PassMode::Pair(..) => {\n-                        let op = self.trans_consume(&bcx, &mir::Place::Local(mir::RETURN_PLACE));\n+                        let op = self.trans_consume(&bx, &mir::Place::Local(mir::RETURN_PLACE));\n                         if let Ref(llval, align) = op.val {\n-                            bcx.load(llval, align)\n+                            bx.load(llval, align)\n                         } else {\n-                            op.immediate_or_packed_pair(&bcx)\n+                            op.immediate_or_packed_pair(&bx)\n                         }\n                     }\n \n@@ -242,8 +242,8 @@ impl<'a, 'tcx> MirContext<'a, 'tcx> {\n                         };\n                         let llslot = match op.val {\n                             Immediate(_) | Pair(..) => {\n-                                let scratch = PlaceRef::alloca(&bcx, self.fn_ty.ret.layout, \"ret\");\n-                                op.val.store(&bcx, scratch);\n+                                let scratch = PlaceRef::alloca(&bx, self.fn_ty.ret.layout, \"ret\");\n+                                op.val.store(&bx, scratch);\n                                 scratch.llval\n                             }\n                             Ref(llval, align) => {\n@@ -252,53 +252,53 @@ impl<'a, 'tcx> MirContext<'a, 'tcx> {\n                                 llval\n                             }\n                         };\n-                        bcx.load(\n-                            bcx.pointercast(llslot, cast_ty.llvm_type(bcx.cx).ptr_to()),\n+                        bx.load(\n+                            bx.pointercast(llslot, cast_ty.llvm_type(bx.cx).ptr_to()),\n                             self.fn_ty.ret.layout.align)\n                     }\n                 };\n-                bcx.ret(llval);\n+                bx.ret(llval);\n             }\n \n             mir::TerminatorKind::Unreachable => {\n-                bcx.unreachable();\n+                bx.unreachable();\n             }\n \n             mir::TerminatorKind::Drop { ref location, target, unwind } => {\n-                let ty = location.ty(self.mir, bcx.tcx()).to_ty(bcx.tcx());\n+                let ty = location.ty(self.mir, bx.tcx()).to_ty(bx.tcx());\n                 let ty = self.monomorphize(&ty);\n-                let drop_fn = monomorphize::resolve_drop_in_place(bcx.cx.tcx, ty);\n+                let drop_fn = monomorphize::resolve_drop_in_place(bx.cx.tcx, ty);\n \n                 if let ty::InstanceDef::DropGlue(_, None) = drop_fn.def {\n                     // we don't actually need to drop anything.\n-                    funclet_br(self, bcx, target);\n+                    funclet_br(self, bx, target);\n                     return\n                 }\n \n-                let place = self.trans_place(&bcx, location);\n+                let place = self.trans_place(&bx, location);\n                 let mut args: &[_] = &[place.llval, place.llextra];\n                 args = &args[..1 + place.has_extra() as usize];\n                 let (drop_fn, fn_ty) = match ty.sty {\n                     ty::TyDynamic(..) => {\n-                        let fn_ty = drop_fn.ty(bcx.cx.tcx);\n-                        let sig = common::ty_fn_sig(bcx.cx, fn_ty);\n-                        let sig = bcx.tcx().erase_late_bound_regions_and_normalize(&sig);\n-                        let fn_ty = FnType::new_vtable(bcx.cx, sig, &[]);\n+                        let fn_ty = drop_fn.ty(bx.cx.tcx);\n+                        let sig = common::ty_fn_sig(bx.cx, fn_ty);\n+                        let sig = bx.tcx().erase_late_bound_regions_and_normalize(&sig);\n+                        let fn_ty = FnType::new_vtable(bx.cx, sig, &[]);\n                         args = &args[..1];\n-                        (meth::DESTRUCTOR.get_fn(&bcx, place.llextra, &fn_ty), fn_ty)\n+                        (meth::DESTRUCTOR.get_fn(&bx, place.llextra, &fn_ty), fn_ty)\n                     }\n                     _ => {\n-                        (callee::get_fn(bcx.cx, drop_fn),\n-                         FnType::of_instance(bcx.cx, &drop_fn))\n+                        (callee::get_fn(bx.cx, drop_fn),\n+                         FnType::of_instance(bx.cx, &drop_fn))\n                     }\n                 };\n-                do_call(self, bcx, fn_ty, drop_fn, args,\n+                do_call(self, bx, fn_ty, drop_fn, args,\n                         Some((ReturnDest::Nothing, target)),\n                         unwind);\n             }\n \n             mir::TerminatorKind::Assert { ref cond, expected, ref msg, target, cleanup } => {\n-                let cond = self.trans_operand(&bcx, cond).immediate();\n+                let cond = self.trans_operand(&bx, cond).immediate();\n                 let mut const_cond = common::const_to_opt_u128(cond, false).map(|c| c == 1);\n \n                 // This case can currently arise only from functions marked\n@@ -308,7 +308,7 @@ impl<'a, 'tcx> MirContext<'a, 'tcx> {\n                 // NOTE: Unlike binops, negation doesn't have its own\n                 // checked operation, just a comparison with the minimum\n                 // value, so we have to check for the assert message.\n-                if !bcx.cx.check_overflow {\n+                if !bx.cx.check_overflow {\n                     use rustc_const_math::ConstMathErr::Overflow;\n                     use rustc_const_math::Op::Neg;\n \n@@ -319,42 +319,42 @@ impl<'a, 'tcx> MirContext<'a, 'tcx> {\n \n                 // Don't translate the panic block if success if known.\n                 if const_cond == Some(expected) {\n-                    funclet_br(self, bcx, target);\n+                    funclet_br(self, bx, target);\n                     return;\n                 }\n \n                 // Pass the condition through llvm.expect for branch hinting.\n-                let expect = bcx.cx.get_intrinsic(&\"llvm.expect.i1\");\n-                let cond = bcx.call(expect, &[cond, C_bool(bcx.cx, expected)], None);\n+                let expect = bx.cx.get_intrinsic(&\"llvm.expect.i1\");\n+                let cond = bx.call(expect, &[cond, C_bool(bx.cx, expected)], None);\n \n                 // Create the failure block and the conditional branch to it.\n                 let lltarget = llblock(self, target);\n                 let panic_block = self.new_block(\"panic\");\n                 if expected {\n-                    bcx.cond_br(cond, lltarget, panic_block.llbb());\n+                    bx.cond_br(cond, lltarget, panic_block.llbb());\n                 } else {\n-                    bcx.cond_br(cond, panic_block.llbb(), lltarget);\n+                    bx.cond_br(cond, panic_block.llbb(), lltarget);\n                 }\n \n-                // After this point, bcx is the block for the call to panic.\n-                bcx = panic_block;\n-                self.set_debug_loc(&bcx, terminator.source_info);\n+                // After this point, bx is the block for the call to panic.\n+                bx = panic_block;\n+                self.set_debug_loc(&bx, terminator.source_info);\n \n                 // Get the location information.\n-                let loc = bcx.sess().codemap().lookup_char_pos(span.lo());\n+                let loc = bx.sess().codemap().lookup_char_pos(span.lo());\n                 let filename = Symbol::intern(&loc.file.name.to_string()).as_str();\n-                let filename = C_str_slice(bcx.cx, filename);\n-                let line = C_u32(bcx.cx, loc.line as u32);\n-                let col = C_u32(bcx.cx, loc.col.to_usize() as u32 + 1);\n+                let filename = C_str_slice(bx.cx, filename);\n+                let line = C_u32(bx.cx, loc.line as u32);\n+                let col = C_u32(bx.cx, loc.col.to_usize() as u32 + 1);\n                 let align = tcx.data_layout.aggregate_align\n                     .max(tcx.data_layout.i32_align)\n                     .max(tcx.data_layout.pointer_align);\n \n                 // Put together the arguments to the panic entry point.\n                 let (lang_item, args, const_err) = match *msg {\n                     mir::AssertMessage::BoundsCheck { ref len, ref index } => {\n-                        let len = self.trans_operand(&mut bcx, len).immediate();\n-                        let index = self.trans_operand(&mut bcx, index).immediate();\n+                        let len = self.trans_operand(&mut bx, len).immediate();\n+                        let index = self.trans_operand(&mut bx, index).immediate();\n \n                         let const_err = common::const_to_opt_u128(len, false)\n                             .and_then(|len| common::const_to_opt_u128(index, false)\n@@ -363,8 +363,8 @@ impl<'a, 'tcx> MirContext<'a, 'tcx> {\n                                     index: index as u64\n                                 }));\n \n-                        let file_line_col = C_struct(bcx.cx, &[filename, line, col], false);\n-                        let file_line_col = consts::addr_of(bcx.cx,\n+                        let file_line_col = C_struct(bx.cx, &[filename, line, col], false);\n+                        let file_line_col = consts::addr_of(bx.cx,\n                                                             file_line_col,\n                                                             align,\n                                                             \"panic_bounds_check_loc\");\n@@ -374,11 +374,11 @@ impl<'a, 'tcx> MirContext<'a, 'tcx> {\n                     }\n                     mir::AssertMessage::Math(ref err) => {\n                         let msg_str = Symbol::intern(err.description()).as_str();\n-                        let msg_str = C_str_slice(bcx.cx, msg_str);\n-                        let msg_file_line_col = C_struct(bcx.cx,\n+                        let msg_str = C_str_slice(bx.cx, msg_str);\n+                        let msg_file_line_col = C_struct(bx.cx,\n                                                      &[msg_str, filename, line, col],\n                                                      false);\n-                        let msg_file_line_col = consts::addr_of(bcx.cx,\n+                        let msg_file_line_col = consts::addr_of(bx.cx,\n                                                                 msg_file_line_col,\n                                                                 align,\n                                                                 \"panic_loc\");\n@@ -394,11 +394,11 @@ impl<'a, 'tcx> MirContext<'a, 'tcx> {\n                             \"generator resumed after panicking\"\n                         };\n                         let msg_str = Symbol::intern(str).as_str();\n-                        let msg_str = C_str_slice(bcx.cx, msg_str);\n-                        let msg_file_line_col = C_struct(bcx.cx,\n+                        let msg_str = C_str_slice(bx.cx, msg_str);\n+                        let msg_file_line_col = C_struct(bx.cx,\n                                                      &[msg_str, filename, line, col],\n                                                      false);\n-                        let msg_file_line_col = consts::addr_of(bcx.cx,\n+                        let msg_file_line_col = consts::addr_of(bx.cx,\n                                                                 msg_file_line_col,\n                                                                 align,\n                                                                 \"panic_loc\");\n@@ -413,21 +413,21 @@ impl<'a, 'tcx> MirContext<'a, 'tcx> {\n                 if const_cond == Some(!expected) {\n                     if let Some(err) = const_err {\n                         let err = ConstEvalErr{ span: span, kind: err };\n-                        let mut diag = bcx.tcx().sess.struct_span_warn(\n+                        let mut diag = bx.tcx().sess.struct_span_warn(\n                             span, \"this expression will panic at run-time\");\n-                        err.note(bcx.tcx(), span, \"expression\", &mut diag);\n+                        err.note(bx.tcx(), span, \"expression\", &mut diag);\n                         diag.emit();\n                     }\n                 }\n \n                 // Obtain the panic entry point.\n-                let def_id = common::langcall(bcx.tcx(), Some(span), \"\", lang_item);\n-                let instance = ty::Instance::mono(bcx.tcx(), def_id);\n-                let fn_ty = FnType::of_instance(bcx.cx, &instance);\n-                let llfn = callee::get_fn(bcx.cx, instance);\n+                let def_id = common::langcall(bx.tcx(), Some(span), \"\", lang_item);\n+                let instance = ty::Instance::mono(bx.tcx(), def_id);\n+                let fn_ty = FnType::of_instance(bx.cx, &instance);\n+                let llfn = callee::get_fn(bx.cx, instance);\n \n                 // Translate the actual panic invoke/call.\n-                do_call(self, bcx, fn_ty, llfn, &args, None, cleanup);\n+                do_call(self, bx, fn_ty, llfn, &args, None, cleanup);\n             }\n \n             mir::TerminatorKind::DropAndReplace { .. } => {\n@@ -436,11 +436,11 @@ impl<'a, 'tcx> MirContext<'a, 'tcx> {\n \n             mir::TerminatorKind::Call { ref func, ref args, ref destination, cleanup } => {\n                 // Create the callee. This is a fn ptr or zero-sized and hence a kind of scalar.\n-                let callee = self.trans_operand(&bcx, func);\n+                let callee = self.trans_operand(&bx, func);\n \n                 let (instance, mut llfn) = match callee.layout.ty.sty {\n                     ty::TyFnDef(def_id, substs) => {\n-                        (Some(ty::Instance::resolve(bcx.cx.tcx,\n+                        (Some(ty::Instance::resolve(bx.cx.tcx,\n                                                     ty::ParamEnv::empty(traits::Reveal::All),\n                                                     def_id,\n                                                     substs).unwrap()),\n@@ -452,42 +452,42 @@ impl<'a, 'tcx> MirContext<'a, 'tcx> {\n                     _ => bug!(\"{} is not callable\", callee.layout.ty)\n                 };\n                 let def = instance.map(|i| i.def);\n-                let sig = callee.layout.ty.fn_sig(bcx.tcx());\n-                let sig = bcx.tcx().erase_late_bound_regions_and_normalize(&sig);\n+                let sig = callee.layout.ty.fn_sig(bx.tcx());\n+                let sig = bx.tcx().erase_late_bound_regions_and_normalize(&sig);\n                 let abi = sig.abi;\n \n                 // Handle intrinsics old trans wants Expr's for, ourselves.\n                 let intrinsic = match def {\n                     Some(ty::InstanceDef::Intrinsic(def_id))\n-                        => Some(bcx.tcx().item_name(def_id)),\n+                        => Some(bx.tcx().item_name(def_id)),\n                     _ => None\n                 };\n                 let intrinsic = intrinsic.as_ref().map(|s| &s[..]);\n \n                 if intrinsic == Some(\"transmute\") {\n                     let &(ref dest, target) = destination.as_ref().unwrap();\n-                    self.trans_transmute(&bcx, &args[0], dest);\n-                    funclet_br(self, bcx, target);\n+                    self.trans_transmute(&bx, &args[0], dest);\n+                    funclet_br(self, bx, target);\n                     return;\n                 }\n \n                 let extra_args = &args[sig.inputs().len()..];\n                 let extra_args = extra_args.iter().map(|op_arg| {\n-                    let op_ty = op_arg.ty(self.mir, bcx.tcx());\n+                    let op_ty = op_arg.ty(self.mir, bx.tcx());\n                     self.monomorphize(&op_ty)\n                 }).collect::<Vec<_>>();\n \n                 let fn_ty = match def {\n                     Some(ty::InstanceDef::Virtual(..)) => {\n-                        FnType::new_vtable(bcx.cx, sig, &extra_args)\n+                        FnType::new_vtable(bx.cx, sig, &extra_args)\n                     }\n                     Some(ty::InstanceDef::DropGlue(_, None)) => {\n                         // empty drop glue - a nop.\n                         let &(_, target) = destination.as_ref().unwrap();\n-                        funclet_br(self, bcx, target);\n+                        funclet_br(self, bx, target);\n                         return;\n                     }\n-                    _ => FnType::new(bcx.cx, sig, &extra_args)\n+                    _ => FnType::new(bx.cx, sig, &extra_args)\n                 };\n \n                 // The arguments we'll be passing. Plus one to account for outptr, if used.\n@@ -497,7 +497,7 @@ impl<'a, 'tcx> MirContext<'a, 'tcx> {\n                 // Prepare the return value destination\n                 let ret_dest = if let Some((ref dest, _)) = *destination {\n                     let is_intrinsic = intrinsic.is_some();\n-                    self.make_return_dest(&bcx, dest, &fn_ty.ret, &mut llargs,\n+                    self.make_return_dest(&bx, dest, &fn_ty.ret, &mut llargs,\n                                           is_intrinsic)\n                 } else {\n                     ReturnDest::Nothing\n@@ -509,7 +509,7 @@ impl<'a, 'tcx> MirContext<'a, 'tcx> {\n                     let dest = match ret_dest {\n                         _ if fn_ty.ret.is_indirect() => llargs[0],\n                         ReturnDest::Nothing => {\n-                            C_undef(fn_ty.ret.memory_ty(bcx.cx).ptr_to())\n+                            C_undef(fn_ty.ret.memory_ty(bx.cx).ptr_to())\n                         }\n                         ReturnDest::IndirectOperand(dst, _) |\n                         ReturnDest::Store(dst) => dst.llval,\n@@ -529,31 +529,31 @@ impl<'a, 'tcx> MirContext<'a, 'tcx> {\n                                     span_bug!(span, \"shuffle indices must be constant\");\n                                 }\n                                 mir::Operand::Constant(ref constant) => {\n-                                    let val = self.trans_constant(&bcx, constant);\n+                                    let val = self.trans_constant(&bx, constant);\n                                     return OperandRef {\n                                         val: Immediate(val.llval),\n-                                        layout: bcx.cx.layout_of(val.ty)\n+                                        layout: bx.cx.layout_of(val.ty)\n                                     };\n                                 }\n                             }\n                         }\n \n-                        self.trans_operand(&bcx, arg)\n+                        self.trans_operand(&bx, arg)\n                     }).collect();\n \n \n-                    let callee_ty = instance.as_ref().unwrap().ty(bcx.cx.tcx);\n-                    trans_intrinsic_call(&bcx, callee_ty, &fn_ty, &args, dest,\n+                    let callee_ty = instance.as_ref().unwrap().ty(bx.cx.tcx);\n+                    trans_intrinsic_call(&bx, callee_ty, &fn_ty, &args, dest,\n                                          terminator.source_info.span);\n \n                     if let ReturnDest::IndirectOperand(dst, _) = ret_dest {\n-                        self.store_return(&bcx, ret_dest, &fn_ty.ret, dst.llval);\n+                        self.store_return(&bx, ret_dest, &fn_ty.ret, dst.llval);\n                     }\n \n                     if let Some((_, target)) = *destination {\n-                        funclet_br(self, bcx, target);\n+                        funclet_br(self, bx, target);\n                     } else {\n-                        bcx.unreachable();\n+                        bx.unreachable();\n                     }\n \n                     return;\n@@ -568,11 +568,11 @@ impl<'a, 'tcx> MirContext<'a, 'tcx> {\n                 };\n \n                 for (i, arg) in first_args.iter().enumerate() {\n-                    let mut op = self.trans_operand(&bcx, arg);\n+                    let mut op = self.trans_operand(&bx, arg);\n                     if let (0, Some(ty::InstanceDef::Virtual(_, idx))) = (i, def) {\n                         if let Pair(data_ptr, meta) = op.val {\n                             llfn = Some(meth::VirtualIndex::from_index(idx)\n-                                .get_fn(&bcx, meta, &fn_ty));\n+                                .get_fn(&bx, meta, &fn_ty));\n                             llargs.push(data_ptr);\n                             continue;\n                         }\n@@ -583,27 +583,27 @@ impl<'a, 'tcx> MirContext<'a, 'tcx> {\n                     match (arg, op.val) {\n                         (&mir::Operand::Copy(_), Ref(..)) |\n                         (&mir::Operand::Constant(_), Ref(..)) => {\n-                            let tmp = PlaceRef::alloca(&bcx, op.layout, \"const\");\n-                            op.val.store(&bcx, tmp);\n+                            let tmp = PlaceRef::alloca(&bx, op.layout, \"const\");\n+                            op.val.store(&bx, tmp);\n                             op.val = Ref(tmp.llval, tmp.align);\n                         }\n                         _ => {}\n                     }\n \n-                    self.trans_argument(&bcx, op, &mut llargs, &fn_ty.args[i]);\n+                    self.trans_argument(&bx, op, &mut llargs, &fn_ty.args[i]);\n                 }\n                 if let Some(tup) = untuple {\n-                    self.trans_arguments_untupled(&bcx, tup, &mut llargs,\n+                    self.trans_arguments_untupled(&bx, tup, &mut llargs,\n                         &fn_ty.args[first_args.len()..])\n                 }\n \n                 let fn_ptr = match (llfn, instance) {\n                     (Some(llfn), _) => llfn,\n-                    (None, Some(instance)) => callee::get_fn(bcx.cx, instance),\n+                    (None, Some(instance)) => callee::get_fn(bx.cx, instance),\n                     _ => span_bug!(span, \"no llfn for call\"),\n                 };\n \n-                do_call(self, bcx, fn_ty, fn_ptr, &llargs,\n+                do_call(self, bx, fn_ty, fn_ptr, &llargs,\n                         destination.as_ref().map(|&(_, target)| (ret_dest, target)),\n                         cleanup);\n             }\n@@ -614,13 +614,13 @@ impl<'a, 'tcx> MirContext<'a, 'tcx> {\n     }\n \n     fn trans_argument(&mut self,\n-                      bcx: &Builder<'a, 'tcx>,\n+                      bx: &Builder<'a, 'tcx>,\n                       op: OperandRef<'tcx>,\n                       llargs: &mut Vec<ValueRef>,\n                       arg: &ArgType<'tcx>) {\n         // Fill padding with undef value, where applicable.\n         if let Some(ty) = arg.pad {\n-            llargs.push(C_undef(ty.llvm_type(bcx.cx)));\n+            llargs.push(C_undef(ty.llvm_type(bx.cx)));\n         }\n \n         if arg.is_ignore() {\n@@ -643,12 +643,12 @@ impl<'a, 'tcx> MirContext<'a, 'tcx> {\n             Immediate(_) | Pair(..) => {\n                 match arg.mode {\n                     PassMode::Indirect(_) | PassMode::Cast(_) => {\n-                        let scratch = PlaceRef::alloca(bcx, arg.layout, \"arg\");\n-                        op.val.store(bcx, scratch);\n+                        let scratch = PlaceRef::alloca(bx, arg.layout, \"arg\");\n+                        op.val.store(bx, scratch);\n                         (scratch.llval, scratch.align, true)\n                     }\n                     _ => {\n-                        (op.immediate_or_packed_pair(bcx), arg.layout.align, false)\n+                        (op.immediate_or_packed_pair(bx), arg.layout.align, false)\n                     }\n                 }\n             }\n@@ -658,8 +658,8 @@ impl<'a, 'tcx> MirContext<'a, 'tcx> {\n                     // think that ATM (Rust 1.16) we only pass temporaries, but we shouldn't\n                     // have scary latent bugs around.\n \n-                    let scratch = PlaceRef::alloca(bcx, arg.layout, \"arg\");\n-                    base::memcpy_ty(bcx, scratch.llval, llval, op.layout, align);\n+                    let scratch = PlaceRef::alloca(bx, arg.layout, \"arg\");\n+                    base::memcpy_ty(bx, scratch.llval, llval, op.layout, align);\n                     (scratch.llval, scratch.align, true)\n                 } else {\n                     (llval, align, true)\n@@ -670,61 +670,61 @@ impl<'a, 'tcx> MirContext<'a, 'tcx> {\n         if by_ref && !arg.is_indirect() {\n             // Have to load the argument, maybe while casting it.\n             if let PassMode::Cast(ty) = arg.mode {\n-                llval = bcx.load(bcx.pointercast(llval, ty.llvm_type(bcx.cx).ptr_to()),\n+                llval = bx.load(bx.pointercast(llval, ty.llvm_type(bx.cx).ptr_to()),\n                                  align.min(arg.layout.align));\n             } else {\n                 // We can't use `PlaceRef::load` here because the argument\n                 // may have a type we don't treat as immediate, but the ABI\n                 // used for this call is passing it by-value. In that case,\n                 // the load would just produce `OperandValue::Ref` instead\n                 // of the `OperandValue::Immediate` we need for the call.\n-                llval = bcx.load(llval, align);\n+                llval = bx.load(llval, align);\n                 if let layout::Abi::Scalar(ref scalar) = arg.layout.abi {\n                     if scalar.is_bool() {\n-                        bcx.range_metadata(llval, 0..2);\n+                        bx.range_metadata(llval, 0..2);\n                     }\n                 }\n                 // We store bools as i8 so we need to truncate to i1.\n-                llval = base::to_immediate(bcx, llval, arg.layout);\n+                llval = base::to_immediate(bx, llval, arg.layout);\n             }\n         }\n \n         llargs.push(llval);\n     }\n \n     fn trans_arguments_untupled(&mut self,\n-                                bcx: &Builder<'a, 'tcx>,\n+                                bx: &Builder<'a, 'tcx>,\n                                 operand: &mir::Operand<'tcx>,\n                                 llargs: &mut Vec<ValueRef>,\n                                 args: &[ArgType<'tcx>]) {\n-        let tuple = self.trans_operand(bcx, operand);\n+        let tuple = self.trans_operand(bx, operand);\n \n         // Handle both by-ref and immediate tuples.\n         if let Ref(llval, align) = tuple.val {\n             let tuple_ptr = PlaceRef::new_sized(llval, tuple.layout, align);\n             for i in 0..tuple.layout.fields.count() {\n-                let field_ptr = tuple_ptr.project_field(bcx, i);\n-                self.trans_argument(bcx, field_ptr.load(bcx), llargs, &args[i]);\n+                let field_ptr = tuple_ptr.project_field(bx, i);\n+                self.trans_argument(bx, field_ptr.load(bx), llargs, &args[i]);\n             }\n         } else {\n             // If the tuple is immediate, the elements are as well.\n             for i in 0..tuple.layout.fields.count() {\n-                let op = tuple.extract_field(bcx, i);\n-                self.trans_argument(bcx, op, llargs, &args[i]);\n+                let op = tuple.extract_field(bx, i);\n+                self.trans_argument(bx, op, llargs, &args[i]);\n             }\n         }\n     }\n \n-    fn get_personality_slot(&mut self, bcx: &Builder<'a, 'tcx>) -> PlaceRef<'tcx> {\n-        let cx = bcx.cx;\n+    fn get_personality_slot(&mut self, bx: &Builder<'a, 'tcx>) -> PlaceRef<'tcx> {\n+        let cx = bx.cx;\n         if let Some(slot) = self.personality_slot {\n             slot\n         } else {\n             let layout = cx.layout_of(cx.tcx.intern_tup(&[\n                 cx.tcx.mk_mut_ptr(cx.tcx.types.u8),\n                 cx.tcx.types.i32\n             ], false));\n-            let slot = PlaceRef::alloca(bcx, layout, \"personalityslot\");\n+            let slot = PlaceRef::alloca(bx, layout, \"personalityslot\");\n             self.personality_slot = Some(slot);\n             slot\n         }\n@@ -749,19 +749,19 @@ impl<'a, 'tcx> MirContext<'a, 'tcx> {\n             span_bug!(self.mir.span, \"landing pad was not inserted?\")\n         }\n \n-        let bcx = self.new_block(\"cleanup\");\n+        let bx = self.new_block(\"cleanup\");\n \n         let llpersonality = self.cx.eh_personality();\n         let llretty = self.landing_pad_type();\n-        let lp = bcx.landing_pad(llretty, llpersonality, 1);\n-        bcx.set_cleanup(lp);\n+        let lp = bx.landing_pad(llretty, llpersonality, 1);\n+        bx.set_cleanup(lp);\n \n-        let slot = self.get_personality_slot(&bcx);\n-        slot.storage_live(&bcx);\n-        Pair(bcx.extract_value(lp, 0), bcx.extract_value(lp, 1)).store(&bcx, slot);\n+        let slot = self.get_personality_slot(&bx);\n+        slot.storage_live(&bx);\n+        Pair(bx.extract_value(lp, 0), bx.extract_value(lp, 1)).store(&bx, slot);\n \n-        bcx.br(target_bb);\n-        bcx.llbb()\n+        bx.br(target_bb);\n+        bx.llbb()\n     }\n \n     fn landing_pad_type(&self) -> Type {\n@@ -782,13 +782,13 @@ impl<'a, 'tcx> MirContext<'a, 'tcx> {\n         Builder::new_block(self.cx, self.llfn, name)\n     }\n \n-    pub fn get_builder(&self, bb: mir::BasicBlock) -> Builder<'a, 'tcx> {\n-        let builder = Builder::with_cx(self.cx);\n-        builder.position_at_end(self.blocks[bb]);\n-        builder\n+    pub fn build_block(&self, bb: mir::BasicBlock) -> Builder<'a, 'tcx> {\n+        let bx = Builder::with_cx(self.cx);\n+        bx.position_at_end(self.blocks[bb]);\n+        bx\n     }\n \n-    fn make_return_dest(&mut self, bcx: &Builder<'a, 'tcx>,\n+    fn make_return_dest(&mut self, bx: &Builder<'a, 'tcx>,\n                         dest: &mir::Place<'tcx>, fn_ret: &ArgType<'tcx>,\n                         llargs: &mut Vec<ValueRef>, is_intrinsic: bool)\n                         -> ReturnDest<'tcx> {\n@@ -805,16 +805,16 @@ impl<'a, 'tcx> MirContext<'a, 'tcx> {\n                     return if fn_ret.is_indirect() {\n                         // Odd, but possible, case, we have an operand temporary,\n                         // but the calling convention has an indirect return.\n-                        let tmp = PlaceRef::alloca(bcx, fn_ret.layout, \"tmp_ret\");\n-                        tmp.storage_live(bcx);\n+                        let tmp = PlaceRef::alloca(bx, fn_ret.layout, \"tmp_ret\");\n+                        tmp.storage_live(bx);\n                         llargs.push(tmp.llval);\n                         ReturnDest::IndirectOperand(tmp, index)\n                     } else if is_intrinsic {\n                         // Currently, intrinsics always need a location to store\n                         // the result. so we create a temporary alloca for the\n                         // result\n-                        let tmp = PlaceRef::alloca(bcx, fn_ret.layout, \"tmp_ret\");\n-                        tmp.storage_live(bcx);\n+                        let tmp = PlaceRef::alloca(bx, fn_ret.layout, \"tmp_ret\");\n+                        tmp.storage_live(bx);\n                         ReturnDest::IndirectOperand(tmp, index)\n                     } else {\n                         ReturnDest::DirectOperand(index)\n@@ -825,7 +825,7 @@ impl<'a, 'tcx> MirContext<'a, 'tcx> {\n                 }\n             }\n         } else {\n-            self.trans_place(bcx, dest)\n+            self.trans_place(bx, dest)\n         };\n         if fn_ret.is_indirect() {\n             if dest.align.abi() < dest.layout.align.abi() {\n@@ -844,20 +844,20 @@ impl<'a, 'tcx> MirContext<'a, 'tcx> {\n         }\n     }\n \n-    fn trans_transmute(&mut self, bcx: &Builder<'a, 'tcx>,\n+    fn trans_transmute(&mut self, bx: &Builder<'a, 'tcx>,\n                        src: &mir::Operand<'tcx>,\n                        dst: &mir::Place<'tcx>) {\n         if let mir::Place::Local(index) = *dst {\n             match self.locals[index] {\n-                LocalRef::Place(place) => self.trans_transmute_into(bcx, src, place),\n+                LocalRef::Place(place) => self.trans_transmute_into(bx, src, place),\n                 LocalRef::Operand(None) => {\n-                    let dst_layout = bcx.cx.layout_of(self.monomorphized_place_ty(dst));\n+                    let dst_layout = bx.cx.layout_of(self.monomorphized_place_ty(dst));\n                     assert!(!dst_layout.ty.has_erasable_regions());\n-                    let place = PlaceRef::alloca(bcx, dst_layout, \"transmute_temp\");\n-                    place.storage_live(bcx);\n-                    self.trans_transmute_into(bcx, src, place);\n-                    let op = place.load(bcx);\n-                    place.storage_dead(bcx);\n+                    let place = PlaceRef::alloca(bx, dst_layout, \"transmute_temp\");\n+                    place.storage_live(bx);\n+                    self.trans_transmute_into(bx, src, place);\n+                    let op = place.load(bx);\n+                    place.storage_dead(bx);\n                     self.locals[index] = LocalRef::Operand(Some(op));\n                 }\n                 LocalRef::Operand(Some(op)) => {\n@@ -866,49 +866,49 @@ impl<'a, 'tcx> MirContext<'a, 'tcx> {\n                 }\n             }\n         } else {\n-            let dst = self.trans_place(bcx, dst);\n-            self.trans_transmute_into(bcx, src, dst);\n+            let dst = self.trans_place(bx, dst);\n+            self.trans_transmute_into(bx, src, dst);\n         }\n     }\n \n-    fn trans_transmute_into(&mut self, bcx: &Builder<'a, 'tcx>,\n+    fn trans_transmute_into(&mut self, bx: &Builder<'a, 'tcx>,\n                             src: &mir::Operand<'tcx>,\n                             dst: PlaceRef<'tcx>) {\n-        let src = self.trans_operand(bcx, src);\n-        let llty = src.layout.llvm_type(bcx.cx);\n-        let cast_ptr = bcx.pointercast(dst.llval, llty.ptr_to());\n+        let src = self.trans_operand(bx, src);\n+        let llty = src.layout.llvm_type(bx.cx);\n+        let cast_ptr = bx.pointercast(dst.llval, llty.ptr_to());\n         let align = src.layout.align.min(dst.layout.align);\n-        src.val.store(bcx, PlaceRef::new_sized(cast_ptr, src.layout, align));\n+        src.val.store(bx, PlaceRef::new_sized(cast_ptr, src.layout, align));\n     }\n \n \n     // Stores the return value of a function call into it's final location.\n     fn store_return(&mut self,\n-                    bcx: &Builder<'a, 'tcx>,\n+                    bx: &Builder<'a, 'tcx>,\n                     dest: ReturnDest<'tcx>,\n                     ret_ty: &ArgType<'tcx>,\n                     llval: ValueRef) {\n         use self::ReturnDest::*;\n \n         match dest {\n             Nothing => (),\n-            Store(dst) => ret_ty.store(bcx, llval, dst),\n+            Store(dst) => ret_ty.store(bx, llval, dst),\n             IndirectOperand(tmp, index) => {\n-                let op = tmp.load(bcx);\n-                tmp.storage_dead(bcx);\n+                let op = tmp.load(bx);\n+                tmp.storage_dead(bx);\n                 self.locals[index] = LocalRef::Operand(Some(op));\n             }\n             DirectOperand(index) => {\n                 // If there is a cast, we have to store and reload.\n                 let op = if let PassMode::Cast(_) = ret_ty.mode {\n-                    let tmp = PlaceRef::alloca(bcx, ret_ty.layout, \"tmp_ret\");\n-                    tmp.storage_live(bcx);\n-                    ret_ty.store(bcx, llval, tmp);\n-                    let op = tmp.load(bcx);\n-                    tmp.storage_dead(bcx);\n+                    let tmp = PlaceRef::alloca(bx, ret_ty.layout, \"tmp_ret\");\n+                    tmp.storage_live(bx);\n+                    ret_ty.store(bx, llval, tmp);\n+                    let op = tmp.load(bx);\n+                    tmp.storage_dead(bx);\n                     op\n                 } else {\n-                    OperandRef::from_immediate_or_packed_pair(bcx, llval, ret_ty.layout)\n+                    OperandRef::from_immediate_or_packed_pair(bx, llval, ret_ty.layout)\n                 };\n                 self.locals[index] = LocalRef::Operand(Some(op));\n             }"}, {"sha": "ae8a61e73ab35d86363b829157634aa29126aa2f", "filename": "src/librustc_trans/mir/constant.rs", "status": "modified", "additions": 5, "deletions": 5, "changes": 10, "blob_url": "https://github.com/rust-lang/rust/blob/209abc71e52b864429330e8debd8ce1080a7f3b4/src%2Flibrustc_trans%2Fmir%2Fconstant.rs", "raw_url": "https://github.com/rust-lang/rust/raw/209abc71e52b864429330e8debd8ce1080a7f3b4/src%2Flibrustc_trans%2Fmir%2Fconstant.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_trans%2Fmir%2Fconstant.rs?ref=209abc71e52b864429330e8debd8ce1080a7f3b4", "patch": "@@ -1120,7 +1120,7 @@ unsafe fn cast_const_int_to_float(cx: &CodegenCx,\n \n impl<'a, 'tcx> MirContext<'a, 'tcx> {\n     pub fn trans_constant(&mut self,\n-                          bcx: &Builder<'a, 'tcx>,\n+                          bx: &Builder<'a, 'tcx>,\n                           constant: &mir::Constant<'tcx>)\n                           -> Const<'tcx>\n     {\n@@ -1129,21 +1129,21 @@ impl<'a, 'tcx> MirContext<'a, 'tcx> {\n         let result = match constant.literal.clone() {\n             mir::Literal::Promoted { index } => {\n                 let mir = &self.mir.promoted[index];\n-                MirConstContext::new(bcx.cx, mir, self.param_substs, IndexVec::new()).trans()\n+                MirConstContext::new(bx.cx, mir, self.param_substs, IndexVec::new()).trans()\n             }\n             mir::Literal::Value { value } => {\n                 if let ConstVal::Unevaluated(def_id, substs) = value.val {\n                     let substs = self.monomorphize(&substs);\n-                    MirConstContext::trans_def(bcx.cx, def_id, substs, IndexVec::new())\n+                    MirConstContext::trans_def(bx.cx, def_id, substs, IndexVec::new())\n                 } else {\n-                    Ok(Const::from_constval(bcx.cx, &value.val, ty))\n+                    Ok(Const::from_constval(bx.cx, &value.val, ty))\n                 }\n             }\n         };\n \n         let result = result.unwrap_or_else(|_| {\n             // We've errored, so we don't have to produce working code.\n-            let llty = bcx.cx.layout_of(ty).llvm_type(bcx.cx);\n+            let llty = bx.cx.layout_of(ty).llvm_type(bx.cx);\n             Const::new(C_undef(llty), ty)\n         });\n "}, {"sha": "d1d7564f1f8d2f100106e770868e2a65fb1fdc4f", "filename": "src/librustc_trans/mir/mod.rs", "status": "modified", "additions": 50, "deletions": 50, "changes": 100, "blob_url": "https://github.com/rust-lang/rust/blob/209abc71e52b864429330e8debd8ce1080a7f3b4/src%2Flibrustc_trans%2Fmir%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/209abc71e52b864429330e8debd8ce1080a7f3b4/src%2Flibrustc_trans%2Fmir%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_trans%2Fmir%2Fmod.rs?ref=209abc71e52b864429330e8debd8ce1080a7f3b4", "patch": "@@ -109,9 +109,9 @@ impl<'a, 'tcx> MirContext<'a, 'tcx> {\n         self.cx.tcx.trans_apply_param_substs(self.param_substs, value)\n     }\n \n-    pub fn set_debug_loc(&mut self, bcx: &Builder, source_info: mir::SourceInfo) {\n+    pub fn set_debug_loc(&mut self, bx: &Builder, source_info: mir::SourceInfo) {\n         let (scope, span) = self.debug_loc(source_info);\n-        debuginfo::set_source_location(&self.debug_context, bcx, scope, span);\n+        debuginfo::set_source_location(&self.debug_context, bx, scope, span);\n     }\n \n     pub fn debug_loc(&mut self, source_info: mir::SourceInfo) -> (DIScope, Span) {\n@@ -201,36 +201,36 @@ pub fn trans_mir<'a, 'tcx: 'a>(\n     debug!(\"fn_ty: {:?}\", fn_ty);\n     let debug_context =\n         debuginfo::create_function_debug_context(cx, instance, sig, llfn, mir);\n-    let bcx = Builder::new_block(cx, llfn, \"start\");\n+    let bx = Builder::new_block(cx, llfn, \"start\");\n \n     if mir.basic_blocks().iter().any(|bb| bb.is_cleanup) {\n-        bcx.set_personality_fn(cx.eh_personality());\n+        bx.set_personality_fn(cx.eh_personality());\n     }\n \n     let cleanup_kinds = analyze::cleanup_kinds(&mir);\n     // Allocate a `Block` for every basic block, except\n     // the start block, if nothing loops back to it.\n     let reentrant_start_block = !mir.predecessors_for(mir::START_BLOCK).is_empty();\n-    let block_bcxs: IndexVec<mir::BasicBlock, BasicBlockRef> =\n+    let block_bxs: IndexVec<mir::BasicBlock, BasicBlockRef> =\n         mir.basic_blocks().indices().map(|bb| {\n             if bb == mir::START_BLOCK && !reentrant_start_block {\n-                bcx.llbb()\n+                bx.llbb()\n             } else {\n-                bcx.build_sibling_block(&format!(\"{:?}\", bb)).llbb()\n+                bx.build_sibling_block(&format!(\"{:?}\", bb)).llbb()\n             }\n         }).collect();\n \n     // Compute debuginfo scopes from MIR scopes.\n     let scopes = debuginfo::create_mir_scopes(cx, mir, &debug_context);\n-    let (landing_pads, funclets) = create_funclets(&bcx, &cleanup_kinds, &block_bcxs);\n+    let (landing_pads, funclets) = create_funclets(&bx, &cleanup_kinds, &block_bxs);\n \n     let mut mircx = MirContext {\n         mir,\n         llfn,\n         fn_ty,\n         cx,\n         personality_slot: None,\n-        blocks: block_bcxs,\n+        blocks: block_bxs,\n         unreachable_block: None,\n         cleanup_kinds,\n         landing_pads,\n@@ -248,28 +248,28 @@ pub fn trans_mir<'a, 'tcx: 'a>(\n \n     // Allocate variable and temp allocas\n     mircx.locals = {\n-        let args = arg_local_refs(&bcx, &mircx, &mircx.scopes, &memory_locals);\n+        let args = arg_local_refs(&bx, &mircx, &mircx.scopes, &memory_locals);\n \n         let mut allocate_local = |local| {\n             let decl = &mir.local_decls[local];\n-            let layout = bcx.cx.layout_of(mircx.monomorphize(&decl.ty));\n+            let layout = bx.cx.layout_of(mircx.monomorphize(&decl.ty));\n             assert!(!layout.ty.has_erasable_regions());\n \n             if let Some(name) = decl.name {\n                 // User variable\n                 let debug_scope = mircx.scopes[decl.source_info.scope];\n-                let dbg = debug_scope.is_valid() && bcx.sess().opts.debuginfo == FullDebugInfo;\n+                let dbg = debug_scope.is_valid() && bx.sess().opts.debuginfo == FullDebugInfo;\n \n                 if !memory_locals.contains(local.index()) && !dbg {\n                     debug!(\"alloc: {:?} ({}) -> operand\", local, name);\n-                    return LocalRef::new_operand(bcx.cx, layout);\n+                    return LocalRef::new_operand(bx.cx, layout);\n                 }\n \n                 debug!(\"alloc: {:?} ({}) -> place\", local, name);\n-                let place = PlaceRef::alloca(&bcx, layout, &name.as_str());\n+                let place = PlaceRef::alloca(&bx, layout, &name.as_str());\n                 if dbg {\n                     let (scope, span) = mircx.debug_loc(decl.source_info);\n-                    declare_local(&bcx, &mircx.debug_context, name, layout.ty, scope,\n+                    declare_local(&bx, &mircx.debug_context, name, layout.ty, scope,\n                         VariableAccess::DirectVariable { alloca: place.llval },\n                         VariableKind::LocalVariable, span);\n                 }\n@@ -282,13 +282,13 @@ pub fn trans_mir<'a, 'tcx: 'a>(\n                     LocalRef::Place(PlaceRef::new_sized(llretptr, layout, layout.align))\n                 } else if memory_locals.contains(local.index()) {\n                     debug!(\"alloc: {:?} -> place\", local);\n-                    LocalRef::Place(PlaceRef::alloca(&bcx, layout, &format!(\"{:?}\", local)))\n+                    LocalRef::Place(PlaceRef::alloca(&bx, layout, &format!(\"{:?}\", local)))\n                 } else {\n                     // If this is an immediate local, we do not create an\n                     // alloca in advance. Instead we wait until we see the\n                     // definition and update the operand there.\n                     debug!(\"alloc: {:?} -> operand\", local);\n-                    LocalRef::new_operand(bcx.cx, layout)\n+                    LocalRef::new_operand(bx.cx, layout)\n                 }\n             }\n         };\n@@ -302,7 +302,7 @@ pub fn trans_mir<'a, 'tcx: 'a>(\n \n     // Branch to the START block, if it's not the entry block.\n     if reentrant_start_block {\n-        bcx.br(mircx.blocks[mir::START_BLOCK]);\n+        bx.br(mircx.blocks[mir::START_BLOCK]);\n     }\n \n     // Up until here, IR instructions for this function have explicitly not been annotated with\n@@ -333,19 +333,19 @@ pub fn trans_mir<'a, 'tcx: 'a>(\n }\n \n fn create_funclets<'a, 'tcx>(\n-    bcx: &Builder<'a, 'tcx>,\n+    bx: &Builder<'a, 'tcx>,\n     cleanup_kinds: &IndexVec<mir::BasicBlock, CleanupKind>,\n-    block_bcxs: &IndexVec<mir::BasicBlock, BasicBlockRef>)\n+    block_bxs: &IndexVec<mir::BasicBlock, BasicBlockRef>)\n     -> (IndexVec<mir::BasicBlock, Option<BasicBlockRef>>,\n         IndexVec<mir::BasicBlock, Option<Funclet>>)\n {\n-    block_bcxs.iter_enumerated().zip(cleanup_kinds).map(|((bb, &llbb), cleanup_kind)| {\n+    block_bxs.iter_enumerated().zip(cleanup_kinds).map(|((bb, &llbb), cleanup_kind)| {\n         match *cleanup_kind {\n-            CleanupKind::Funclet if base::wants_msvc_seh(bcx.sess()) => {\n-                let cleanup_bcx = bcx.build_sibling_block(&format!(\"funclet_{:?}\", bb));\n-                let cleanup = cleanup_bcx.cleanup_pad(None, &[]);\n-                cleanup_bcx.br(llbb);\n-                (Some(cleanup_bcx.llbb()), Some(Funclet::new(cleanup)))\n+            CleanupKind::Funclet if base::wants_msvc_seh(bx.sess()) => {\n+                let cleanup_bx = bx.build_sibling_block(&format!(\"funclet_{:?}\", bb));\n+                let cleanup = cleanup_bx.cleanup_pad(None, &[]);\n+                cleanup_bx.br(llbb);\n+                (Some(cleanup_bx.llbb()), Some(Funclet::new(cleanup)))\n             }\n             _ => (None, None)\n         }\n@@ -355,19 +355,19 @@ fn create_funclets<'a, 'tcx>(\n /// Produce, for each argument, a `ValueRef` pointing at the\n /// argument's value. As arguments are places, these are always\n /// indirect.\n-fn arg_local_refs<'a, 'tcx>(bcx: &Builder<'a, 'tcx>,\n+fn arg_local_refs<'a, 'tcx>(bx: &Builder<'a, 'tcx>,\n                             mircx: &MirContext<'a, 'tcx>,\n                             scopes: &IndexVec<mir::VisibilityScope, debuginfo::MirDebugScope>,\n                             memory_locals: &BitVector)\n                             -> Vec<LocalRef<'tcx>> {\n     let mir = mircx.mir;\n-    let tcx = bcx.tcx();\n+    let tcx = bx.tcx();\n     let mut idx = 0;\n     let mut llarg_idx = mircx.fn_ty.ret.is_indirect() as usize;\n \n     // Get the argument scope, if it exists and if we need it.\n     let arg_scope = scopes[mir::ARGUMENT_VISIBILITY_SCOPE];\n-    let arg_scope = if arg_scope.is_valid() && bcx.sess().opts.debuginfo == FullDebugInfo {\n+    let arg_scope = if arg_scope.is_valid() && bx.sess().opts.debuginfo == FullDebugInfo {\n         Some(arg_scope.scope_metadata)\n     } else {\n         None\n@@ -398,11 +398,11 @@ fn arg_local_refs<'a, 'tcx>(bcx: &Builder<'a, 'tcx>,\n                 _ => bug!(\"spread argument isn't a tuple?!\")\n             };\n \n-            let place = PlaceRef::alloca(bcx, bcx.cx.layout_of(arg_ty), &name);\n+            let place = PlaceRef::alloca(bx, bx.cx.layout_of(arg_ty), &name);\n             for i in 0..tupled_arg_tys.len() {\n                 let arg = &mircx.fn_ty.args[idx];\n                 idx += 1;\n-                arg.store_fn_arg(bcx, &mut llarg_idx, place.project_field(bcx, i));\n+                arg.store_fn_arg(bx, &mut llarg_idx, place.project_field(bx, i));\n             }\n \n             // Now that we have one alloca that contains the aggregate value,\n@@ -412,7 +412,7 @@ fn arg_local_refs<'a, 'tcx>(bcx: &Builder<'a, 'tcx>,\n                     alloca: place.llval\n                 };\n                 declare_local(\n-                    bcx,\n+                    bx,\n                     &mircx.debug_context,\n                     arg_decl.name.unwrap_or(keywords::Invalid.name()),\n                     arg_ty, scope,\n@@ -438,22 +438,22 @@ fn arg_local_refs<'a, 'tcx>(bcx: &Builder<'a, 'tcx>,\n             let local = |op| LocalRef::Operand(Some(op));\n             match arg.mode {\n                 PassMode::Ignore => {\n-                    return local(OperandRef::new_zst(bcx.cx, arg.layout));\n+                    return local(OperandRef::new_zst(bx.cx, arg.layout));\n                 }\n                 PassMode::Direct(_) => {\n-                    let llarg = llvm::get_param(bcx.llfn(), llarg_idx as c_uint);\n-                    bcx.set_value_name(llarg, &name);\n+                    let llarg = llvm::get_param(bx.llfn(), llarg_idx as c_uint);\n+                    bx.set_value_name(llarg, &name);\n                     llarg_idx += 1;\n                     return local(\n-                        OperandRef::from_immediate_or_packed_pair(bcx, llarg, arg.layout));\n+                        OperandRef::from_immediate_or_packed_pair(bx, llarg, arg.layout));\n                 }\n                 PassMode::Pair(..) => {\n-                    let a = llvm::get_param(bcx.llfn(), llarg_idx as c_uint);\n-                    bcx.set_value_name(a, &(name.clone() + \".0\"));\n+                    let a = llvm::get_param(bx.llfn(), llarg_idx as c_uint);\n+                    bx.set_value_name(a, &(name.clone() + \".0\"));\n                     llarg_idx += 1;\n \n-                    let b = llvm::get_param(bcx.llfn(), llarg_idx as c_uint);\n-                    bcx.set_value_name(b, &(name + \".1\"));\n+                    let b = llvm::get_param(bx.llfn(), llarg_idx as c_uint);\n+                    bx.set_value_name(b, &(name + \".1\"));\n                     llarg_idx += 1;\n \n                     return local(OperandRef {\n@@ -469,13 +469,13 @@ fn arg_local_refs<'a, 'tcx>(bcx: &Builder<'a, 'tcx>,\n             // Don't copy an indirect argument to an alloca, the caller\n             // already put it in a temporary alloca and gave it up.\n             // FIXME: lifetimes\n-            let llarg = llvm::get_param(bcx.llfn(), llarg_idx as c_uint);\n-            bcx.set_value_name(llarg, &name);\n+            let llarg = llvm::get_param(bx.llfn(), llarg_idx as c_uint);\n+            bx.set_value_name(llarg, &name);\n             llarg_idx += 1;\n             PlaceRef::new_sized(llarg, arg.layout, arg.layout.align)\n         } else {\n-            let tmp = PlaceRef::alloca(bcx, arg.layout, &name);\n-            arg.store_fn_arg(bcx, &mut llarg_idx, tmp);\n+            let tmp = PlaceRef::alloca(bx, arg.layout, &name);\n+            arg.store_fn_arg(bx, &mut llarg_idx, tmp);\n             tmp\n         };\n         arg_scope.map(|scope| {\n@@ -498,7 +498,7 @@ fn arg_local_refs<'a, 'tcx>(bcx: &Builder<'a, 'tcx>,\n                 }\n \n                 declare_local(\n-                    bcx,\n+                    bx,\n                     &mircx.debug_context,\n                     arg_decl.name.unwrap_or(keywords::Invalid.name()),\n                     arg.layout.ty,\n@@ -512,7 +512,7 @@ fn arg_local_refs<'a, 'tcx>(bcx: &Builder<'a, 'tcx>,\n \n             // Or is it the closure environment?\n             let (closure_layout, env_ref) = match arg.layout.ty.sty {\n-                ty::TyRef(_, mt) | ty::TyRawPtr(mt) => (bcx.cx.layout_of(mt.ty), true),\n+                ty::TyRef(_, mt) | ty::TyRawPtr(mt) => (bx.cx.layout_of(mt.ty), true),\n                 _ => (arg.layout, false)\n             };\n \n@@ -530,10 +530,10 @@ fn arg_local_refs<'a, 'tcx>(bcx: &Builder<'a, 'tcx>,\n             // doesn't actually strip the offset when splitting the closure\n             // environment into its components so it ends up out of bounds.\n             let env_ptr = if !env_ref {\n-                let scratch = PlaceRef::alloca(bcx,\n-                    bcx.cx.layout_of(tcx.mk_mut_ptr(arg.layout.ty)),\n+                let scratch = PlaceRef::alloca(bx,\n+                    bx.cx.layout_of(tcx.mk_mut_ptr(arg.layout.ty)),\n                     \"__debuginfo_env_ptr\");\n-                bcx.store(place.llval, scratch.llval, scratch.align);\n+                bx.store(place.llval, scratch.llval, scratch.align);\n                 scratch.llval\n             } else {\n                 place.llval\n@@ -567,7 +567,7 @@ fn arg_local_refs<'a, 'tcx>(bcx: &Builder<'a, 'tcx>,\n                     address_operations: &ops\n                 };\n                 declare_local(\n-                    bcx,\n+                    bx,\n                     &mircx.debug_context,\n                     decl.debug_name,\n                     ty,"}, {"sha": "277a3c75920206e5f4b619dfcb65894f91761f80", "filename": "src/librustc_trans/mir/operand.rs", "status": "modified", "additions": 40, "deletions": 40, "changes": 80, "blob_url": "https://github.com/rust-lang/rust/blob/209abc71e52b864429330e8debd8ce1080a7f3b4/src%2Flibrustc_trans%2Fmir%2Foperand.rs", "raw_url": "https://github.com/rust-lang/rust/raw/209abc71e52b864429330e8debd8ce1080a7f3b4/src%2Flibrustc_trans%2Fmir%2Foperand.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_trans%2Fmir%2Foperand.rs?ref=209abc71e52b864429330e8debd8ce1080a7f3b4", "patch": "@@ -118,23 +118,23 @@ impl<'a, 'tcx> OperandRef<'tcx> {\n \n     /// If this operand is a `Pair`, we return an aggregate with the two values.\n     /// For other cases, see `immediate`.\n-    pub fn immediate_or_packed_pair(self, bcx: &Builder<'a, 'tcx>) -> ValueRef {\n+    pub fn immediate_or_packed_pair(self, bx: &Builder<'a, 'tcx>) -> ValueRef {\n         if let OperandValue::Pair(a, b) = self.val {\n-            let llty = self.layout.llvm_type(bcx.cx);\n+            let llty = self.layout.llvm_type(bx.cx);\n             debug!(\"Operand::immediate_or_packed_pair: packing {:?} into {:?}\",\n                    self, llty);\n             // Reconstruct the immediate aggregate.\n             let mut llpair = C_undef(llty);\n-            llpair = bcx.insert_value(llpair, a, 0);\n-            llpair = bcx.insert_value(llpair, b, 1);\n+            llpair = bx.insert_value(llpair, a, 0);\n+            llpair = bx.insert_value(llpair, b, 1);\n             llpair\n         } else {\n             self.immediate()\n         }\n     }\n \n     /// If the type is a pair, we return a `Pair`, otherwise, an `Immediate`.\n-    pub fn from_immediate_or_packed_pair(bcx: &Builder<'a, 'tcx>,\n+    pub fn from_immediate_or_packed_pair(bx: &Builder<'a, 'tcx>,\n                                          llval: ValueRef,\n                                          layout: TyLayout<'tcx>)\n                                          -> OperandRef<'tcx> {\n@@ -143,23 +143,23 @@ impl<'a, 'tcx> OperandRef<'tcx> {\n                     llval, layout);\n \n             // Deconstruct the immediate aggregate.\n-            OperandValue::Pair(bcx.extract_value(llval, 0),\n-                               bcx.extract_value(llval, 1))\n+            OperandValue::Pair(bx.extract_value(llval, 0),\n+                               bx.extract_value(llval, 1))\n         } else {\n             OperandValue::Immediate(llval)\n         };\n         OperandRef { val, layout }\n     }\n \n-    pub fn extract_field(&self, bcx: &Builder<'a, 'tcx>, i: usize) -> OperandRef<'tcx> {\n-        let field = self.layout.field(bcx.cx, i);\n+    pub fn extract_field(&self, bx: &Builder<'a, 'tcx>, i: usize) -> OperandRef<'tcx> {\n+        let field = self.layout.field(bx.cx, i);\n         let offset = self.layout.fields.offset(i);\n \n         let mut val = match (self.val, &self.layout.abi) {\n             // If we're uninhabited, or the field is ZST, it has no data.\n             _ if self.layout.abi == layout::Abi::Uninhabited || field.is_zst() => {\n                 return OperandRef {\n-                    val: OperandValue::Immediate(C_undef(field.immediate_llvm_type(bcx.cx))),\n+                    val: OperandValue::Immediate(C_undef(field.immediate_llvm_type(bx.cx))),\n                     layout: field\n                 };\n             }\n@@ -174,20 +174,20 @@ impl<'a, 'tcx> OperandRef<'tcx> {\n             // Extract a scalar component from a pair.\n             (OperandValue::Pair(a_llval, b_llval), &layout::Abi::ScalarPair(ref a, ref b)) => {\n                 if offset.bytes() == 0 {\n-                    assert_eq!(field.size, a.value.size(bcx.cx));\n+                    assert_eq!(field.size, a.value.size(bx.cx));\n                     OperandValue::Immediate(a_llval)\n                 } else {\n-                    assert_eq!(offset, a.value.size(bcx.cx)\n-                        .abi_align(b.value.align(bcx.cx)));\n-                    assert_eq!(field.size, b.value.size(bcx.cx));\n+                    assert_eq!(offset, a.value.size(bx.cx)\n+                        .abi_align(b.value.align(bx.cx)));\n+                    assert_eq!(field.size, b.value.size(bx.cx));\n                     OperandValue::Immediate(b_llval)\n                 }\n             }\n \n             // `#[repr(simd)]` types are also immediate.\n             (OperandValue::Immediate(llval), &layout::Abi::Vector { .. }) => {\n                 OperandValue::Immediate(\n-                    bcx.extract_element(llval, C_usize(bcx.cx, i as u64)))\n+                    bx.extract_element(llval, C_usize(bx.cx, i as u64)))\n             }\n \n             _ => bug!(\"OperandRef::extract_field({:?}): not applicable\", self)\n@@ -196,11 +196,11 @@ impl<'a, 'tcx> OperandRef<'tcx> {\n         // HACK(eddyb) have to bitcast pointers until LLVM removes pointee types.\n         match val {\n             OperandValue::Immediate(ref mut llval) => {\n-                *llval = bcx.bitcast(*llval, field.immediate_llvm_type(bcx.cx));\n+                *llval = bx.bitcast(*llval, field.immediate_llvm_type(bx.cx));\n             }\n             OperandValue::Pair(ref mut a, ref mut b) => {\n-                *a = bcx.bitcast(*a, field.scalar_pair_element_llvm_type(bcx.cx, 0));\n-                *b = bcx.bitcast(*b, field.scalar_pair_element_llvm_type(bcx.cx, 1));\n+                *a = bx.bitcast(*a, field.scalar_pair_element_llvm_type(bx.cx, 0));\n+                *b = bx.bitcast(*b, field.scalar_pair_element_llvm_type(bx.cx, 1));\n             }\n             OperandValue::Ref(..) => bug!()\n         }\n@@ -213,7 +213,7 @@ impl<'a, 'tcx> OperandRef<'tcx> {\n }\n \n impl<'a, 'tcx> OperandValue {\n-    pub fn store(self, bcx: &Builder<'a, 'tcx>, dest: PlaceRef<'tcx>) {\n+    pub fn store(self, bx: &Builder<'a, 'tcx>, dest: PlaceRef<'tcx>) {\n         debug!(\"OperandRef::store: operand={:?}, dest={:?}\", self, dest);\n         // Avoid generating stores of zero-sized values, because the only way to have a zero-sized\n         // value is through `undef`, and store itself is useless.\n@@ -222,19 +222,19 @@ impl<'a, 'tcx> OperandValue {\n         }\n         match self {\n             OperandValue::Ref(r, source_align) =>\n-                base::memcpy_ty(bcx, dest.llval, r, dest.layout,\n+                base::memcpy_ty(bx, dest.llval, r, dest.layout,\n                                 source_align.min(dest.align)),\n             OperandValue::Immediate(s) => {\n-                bcx.store(base::from_immediate(bcx, s), dest.llval, dest.align);\n+                bx.store(base::from_immediate(bx, s), dest.llval, dest.align);\n             }\n             OperandValue::Pair(a, b) => {\n                 for (i, &x) in [a, b].iter().enumerate() {\n-                    let mut llptr = bcx.struct_gep(dest.llval, i as u64);\n+                    let mut llptr = bx.struct_gep(dest.llval, i as u64);\n                     // Make sure to always store i1 as i8.\n-                    if common::val_ty(x) == Type::i1(bcx.cx) {\n-                        llptr = bcx.pointercast(llptr, Type::i8p(bcx.cx));\n+                    if common::val_ty(x) == Type::i1(bx.cx) {\n+                        llptr = bx.pointercast(llptr, Type::i8p(bx.cx));\n                     }\n-                    bcx.store(base::from_immediate(bcx, x), llptr, dest.align);\n+                    bx.store(base::from_immediate(bx, x), llptr, dest.align);\n                 }\n             }\n         }\n@@ -243,7 +243,7 @@ impl<'a, 'tcx> OperandValue {\n \n impl<'a, 'tcx> MirContext<'a, 'tcx> {\n     fn maybe_trans_consume_direct(&mut self,\n-                                  bcx: &Builder<'a, 'tcx>,\n+                                  bx: &Builder<'a, 'tcx>,\n                                   place: &mir::Place<'tcx>)\n                                    -> Option<OperandRef<'tcx>>\n     {\n@@ -267,19 +267,19 @@ impl<'a, 'tcx> MirContext<'a, 'tcx> {\n \n         // Moves out of scalar and scalar pair fields are trivial.\n         if let &mir::Place::Projection(ref proj) = place {\n-            if let Some(o) = self.maybe_trans_consume_direct(bcx, &proj.base) {\n+            if let Some(o) = self.maybe_trans_consume_direct(bx, &proj.base) {\n                 match proj.elem {\n                     mir::ProjectionElem::Field(ref f, _) => {\n-                        return Some(o.extract_field(bcx, f.index()));\n+                        return Some(o.extract_field(bx, f.index()));\n                     }\n                     mir::ProjectionElem::Index(_) |\n                     mir::ProjectionElem::ConstantIndex { .. } => {\n                         // ZSTs don't require any actual memory access.\n                         // FIXME(eddyb) deduplicate this with the identical\n                         // checks in `trans_consume` and `extract_field`.\n-                        let elem = o.layout.field(bcx.cx, 0);\n+                        let elem = o.layout.field(bx.cx, 0);\n                         if elem.is_zst() {\n-                            return Some(OperandRef::new_zst(bcx.cx, elem));\n+                            return Some(OperandRef::new_zst(bx.cx, elem));\n                         }\n                     }\n                     _ => {}\n@@ -291,31 +291,31 @@ impl<'a, 'tcx> MirContext<'a, 'tcx> {\n     }\n \n     pub fn trans_consume(&mut self,\n-                         bcx: &Builder<'a, 'tcx>,\n+                         bx: &Builder<'a, 'tcx>,\n                          place: &mir::Place<'tcx>)\n                          -> OperandRef<'tcx>\n     {\n         debug!(\"trans_consume(place={:?})\", place);\n \n         let ty = self.monomorphized_place_ty(place);\n-        let layout = bcx.cx.layout_of(ty);\n+        let layout = bx.cx.layout_of(ty);\n \n         // ZSTs don't require any actual memory access.\n         if layout.is_zst() {\n-            return OperandRef::new_zst(bcx.cx, layout);\n+            return OperandRef::new_zst(bx.cx, layout);\n         }\n \n-        if let Some(o) = self.maybe_trans_consume_direct(bcx, place) {\n+        if let Some(o) = self.maybe_trans_consume_direct(bx, place) {\n             return o;\n         }\n \n         // for most places, to consume them we just load them\n         // out from their home\n-        self.trans_place(bcx, place).load(bcx)\n+        self.trans_place(bx, place).load(bx)\n     }\n \n     pub fn trans_operand(&mut self,\n-                         bcx: &Builder<'a, 'tcx>,\n+                         bx: &Builder<'a, 'tcx>,\n                          operand: &mir::Operand<'tcx>)\n                          -> OperandRef<'tcx>\n     {\n@@ -324,15 +324,15 @@ impl<'a, 'tcx> MirContext<'a, 'tcx> {\n         match *operand {\n             mir::Operand::Copy(ref place) |\n             mir::Operand::Move(ref place) => {\n-                self.trans_consume(bcx, place)\n+                self.trans_consume(bx, place)\n             }\n \n             mir::Operand::Constant(ref constant) => {\n-                let val = self.trans_constant(&bcx, constant);\n-                let operand = val.to_operand(bcx.cx);\n+                let val = self.trans_constant(&bx, constant);\n+                let operand = val.to_operand(bx.cx);\n                 if let OperandValue::Ref(ptr, align) = operand.val {\n                     // If this is a OperandValue::Ref to an immediate constant, load it.\n-                    PlaceRef::new_sized(ptr, operand.layout, align).load(bcx)\n+                    PlaceRef::new_sized(ptr, operand.layout, align).load(bx)\n                 } else {\n                     operand\n                 }"}, {"sha": "c33b341d8c60820cf338c609d8c687d5ab1c4fea", "filename": "src/librustc_trans/mir/place.rs", "status": "modified", "additions": 83, "deletions": 83, "changes": 166, "blob_url": "https://github.com/rust-lang/rust/blob/209abc71e52b864429330e8debd8ce1080a7f3b4/src%2Flibrustc_trans%2Fmir%2Fplace.rs", "raw_url": "https://github.com/rust-lang/rust/raw/209abc71e52b864429330e8debd8ce1080a7f3b4/src%2Flibrustc_trans%2Fmir%2Fplace.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_trans%2Fmir%2Fplace.rs?ref=209abc71e52b864429330e8debd8ce1080a7f3b4", "patch": "@@ -56,10 +56,10 @@ impl<'a, 'tcx> PlaceRef<'tcx> {\n         }\n     }\n \n-    pub fn alloca(bcx: &Builder<'a, 'tcx>, layout: TyLayout<'tcx>, name: &str)\n+    pub fn alloca(bx: &Builder<'a, 'tcx>, layout: TyLayout<'tcx>, name: &str)\n                   -> PlaceRef<'tcx> {\n         debug!(\"alloca({:?}: {:?})\", name, layout);\n-        let tmp = bcx.alloca(layout.llvm_type(bcx.cx), name, layout.align);\n+        let tmp = bx.alloca(layout.llvm_type(bx.cx), name, layout.align);\n         Self::new_sized(tmp, layout, layout.align)\n     }\n \n@@ -81,19 +81,19 @@ impl<'a, 'tcx> PlaceRef<'tcx> {\n         !self.llextra.is_null()\n     }\n \n-    pub fn load(&self, bcx: &Builder<'a, 'tcx>) -> OperandRef<'tcx> {\n+    pub fn load(&self, bx: &Builder<'a, 'tcx>) -> OperandRef<'tcx> {\n         debug!(\"PlaceRef::load: {:?}\", self);\n \n         assert!(!self.has_extra());\n \n         if self.layout.is_zst() {\n-            return OperandRef::new_zst(bcx.cx, self.layout);\n+            return OperandRef::new_zst(bx.cx, self.layout);\n         }\n \n         let scalar_load_metadata = |load, scalar: &layout::Scalar| {\n             let (min, max) = (scalar.valid_range.start, scalar.valid_range.end);\n             let max_next = max.wrapping_add(1);\n-            let bits = scalar.value.size(bcx.cx).bits();\n+            let bits = scalar.value.size(bx.cx).bits();\n             assert!(bits <= 128);\n             let mask = !0u128 >> (128 - bits);\n             // For a (max) value of -1, max will be `-1 as usize`, which overflows.\n@@ -106,10 +106,10 @@ impl<'a, 'tcx> PlaceRef<'tcx> {\n                 layout::Int(..) if max_next & mask != min & mask => {\n                     // llvm::ConstantRange can deal with ranges that wrap around,\n                     // so an overflow on (max + 1) is fine.\n-                    bcx.range_metadata(load, min..max_next);\n+                    bx.range_metadata(load, min..max_next);\n                 }\n                 layout::Pointer if 0 < min && min < max => {\n-                    bcx.nonnull_metadata(load);\n+                    bx.nonnull_metadata(load);\n                 }\n                 _ => {}\n             }\n@@ -127,24 +127,24 @@ impl<'a, 'tcx> PlaceRef<'tcx> {\n             let llval = if !const_llval.is_null() {\n                 const_llval\n             } else {\n-                let load = bcx.load(self.llval, self.align);\n+                let load = bx.load(self.llval, self.align);\n                 if let layout::Abi::Scalar(ref scalar) = self.layout.abi {\n                     scalar_load_metadata(load, scalar);\n                 }\n                 load\n             };\n-            OperandValue::Immediate(base::to_immediate(bcx, llval, self.layout))\n+            OperandValue::Immediate(base::to_immediate(bx, llval, self.layout))\n         } else if let layout::Abi::ScalarPair(ref a, ref b) = self.layout.abi {\n             let load = |i, scalar: &layout::Scalar| {\n-                let mut llptr = bcx.struct_gep(self.llval, i as u64);\n+                let mut llptr = bx.struct_gep(self.llval, i as u64);\n                 // Make sure to always load i1 as i8.\n                 if scalar.is_bool() {\n-                    llptr = bcx.pointercast(llptr, Type::i8p(bcx.cx));\n+                    llptr = bx.pointercast(llptr, Type::i8p(bx.cx));\n                 }\n-                let load = bcx.load(llptr, self.align);\n+                let load = bx.load(llptr, self.align);\n                 scalar_load_metadata(load, scalar);\n                 if scalar.is_bool() {\n-                    bcx.trunc(load, Type::i1(bcx.cx))\n+                    bx.trunc(load, Type::i1(bx.cx))\n                 } else {\n                     load\n                 }\n@@ -158,8 +158,8 @@ impl<'a, 'tcx> PlaceRef<'tcx> {\n     }\n \n     /// Access a field, at a point when the value's case is known.\n-    pub fn project_field(self, bcx: &Builder<'a, 'tcx>, ix: usize) -> PlaceRef<'tcx> {\n-        let cx = bcx.cx;\n+    pub fn project_field(self, bx: &Builder<'a, 'tcx>, ix: usize) -> PlaceRef<'tcx> {\n+        let cx = bx.cx;\n         let field = self.layout.field(cx, ix);\n         let offset = self.layout.fields.offset(ix);\n         let align = self.align.min(self.layout.align).min(field.align);\n@@ -171,13 +171,13 @@ impl<'a, 'tcx> PlaceRef<'tcx> {\n             } else if let layout::Abi::ScalarPair(ref a, ref b) = self.layout.abi {\n                 // Offsets have to match either first or second field.\n                 assert_eq!(offset, a.value.size(cx).abi_align(b.value.align(cx)));\n-                bcx.struct_gep(self.llval, 1)\n+                bx.struct_gep(self.llval, 1)\n             } else {\n-                bcx.struct_gep(self.llval, self.layout.llvm_field_index(ix))\n+                bx.struct_gep(self.llval, self.layout.llvm_field_index(ix))\n             };\n             PlaceRef {\n                 // HACK(eddyb) have to bitcast pointers until LLVM removes pointee types.\n-                llval: bcx.pointercast(llval, field.llvm_type(cx).ptr_to()),\n+                llval: bx.pointercast(llval, field.llvm_type(cx).ptr_to()),\n                 llextra: if cx.type_has_metadata(field.ty) {\n                     self.llextra\n                 } else {\n@@ -231,39 +231,39 @@ impl<'a, 'tcx> PlaceRef<'tcx> {\n         let unaligned_offset = C_usize(cx, offset.bytes());\n \n         // Get the alignment of the field\n-        let (_, unsized_align) = glue::size_and_align_of_dst(bcx, field.ty, meta);\n+        let (_, unsized_align) = glue::size_and_align_of_dst(bx, field.ty, meta);\n \n         // Bump the unaligned offset up to the appropriate alignment using the\n         // following expression:\n         //\n         //   (unaligned offset + (align - 1)) & -align\n \n         // Calculate offset\n-        let align_sub_1 = bcx.sub(unsized_align, C_usize(cx, 1u64));\n-        let offset = bcx.and(bcx.add(unaligned_offset, align_sub_1),\n-        bcx.neg(unsized_align));\n+        let align_sub_1 = bx.sub(unsized_align, C_usize(cx, 1u64));\n+        let offset = bx.and(bx.add(unaligned_offset, align_sub_1),\n+        bx.neg(unsized_align));\n \n         debug!(\"struct_field_ptr: DST field offset: {:?}\", Value(offset));\n \n         // Cast and adjust pointer\n-        let byte_ptr = bcx.pointercast(self.llval, Type::i8p(cx));\n-        let byte_ptr = bcx.gep(byte_ptr, &[offset]);\n+        let byte_ptr = bx.pointercast(self.llval, Type::i8p(cx));\n+        let byte_ptr = bx.gep(byte_ptr, &[offset]);\n \n         // Finally, cast back to the type expected\n         let ll_fty = field.llvm_type(cx);\n         debug!(\"struct_field_ptr: Field type is {:?}\", ll_fty);\n \n         PlaceRef {\n-            llval: bcx.pointercast(byte_ptr, ll_fty.ptr_to()),\n+            llval: bx.pointercast(byte_ptr, ll_fty.ptr_to()),\n             llextra: self.llextra,\n             layout: field,\n             align,\n         }\n     }\n \n     /// Obtain the actual discriminant of a value.\n-    pub fn trans_get_discr(self, bcx: &Builder<'a, 'tcx>, cast_to: Ty<'tcx>) -> ValueRef {\n-        let cast_to = bcx.cx.layout_of(cast_to).immediate_llvm_type(bcx.cx);\n+    pub fn trans_get_discr(self, bx: &Builder<'a, 'tcx>, cast_to: Ty<'tcx>) -> ValueRef {\n+        let cast_to = bx.cx.layout_of(cast_to).immediate_llvm_type(bx.cx);\n         match self.layout.variants {\n             layout::Variants::Single { index } => {\n                 return C_uint(cast_to, index as u64);\n@@ -272,24 +272,24 @@ impl<'a, 'tcx> PlaceRef<'tcx> {\n             layout::Variants::NicheFilling { .. } => {},\n         }\n \n-        let discr = self.project_field(bcx, 0);\n-        let lldiscr = discr.load(bcx).immediate();\n+        let discr = self.project_field(bx, 0);\n+        let lldiscr = discr.load(bx).immediate();\n         match self.layout.variants {\n             layout::Variants::Single { .. } => bug!(),\n             layout::Variants::Tagged { ref discr, .. } => {\n                 let signed = match discr.value {\n                     layout::Int(_, signed) => signed,\n                     _ => false\n                 };\n-                bcx.intcast(lldiscr, cast_to, signed)\n+                bx.intcast(lldiscr, cast_to, signed)\n             }\n             layout::Variants::NicheFilling {\n                 dataful_variant,\n                 ref niche_variants,\n                 niche_start,\n                 ..\n             } => {\n-                let niche_llty = discr.layout.immediate_llvm_type(bcx.cx);\n+                let niche_llty = discr.layout.immediate_llvm_type(bx.cx);\n                 if niche_variants.start == niche_variants.end {\n                     // FIXME(eddyb) Check the actual primitive type here.\n                     let niche_llval = if niche_start == 0 {\n@@ -298,16 +298,16 @@ impl<'a, 'tcx> PlaceRef<'tcx> {\n                     } else {\n                         C_uint_big(niche_llty, niche_start)\n                     };\n-                    bcx.select(bcx.icmp(llvm::IntEQ, lldiscr, niche_llval),\n+                    bx.select(bx.icmp(llvm::IntEQ, lldiscr, niche_llval),\n                         C_uint(cast_to, niche_variants.start as u64),\n                         C_uint(cast_to, dataful_variant as u64))\n                 } else {\n                     // Rebase from niche values to discriminant values.\n                     let delta = niche_start.wrapping_sub(niche_variants.start as u128);\n-                    let lldiscr = bcx.sub(lldiscr, C_uint_big(niche_llty, delta));\n+                    let lldiscr = bx.sub(lldiscr, C_uint_big(niche_llty, delta));\n                     let lldiscr_max = C_uint(niche_llty, niche_variants.end as u64);\n-                    bcx.select(bcx.icmp(llvm::IntULE, lldiscr, lldiscr_max),\n-                        bcx.intcast(lldiscr, cast_to, false),\n+                    bx.select(bx.icmp(llvm::IntULE, lldiscr, lldiscr_max),\n+                        bx.intcast(lldiscr, cast_to, false),\n                         C_uint(cast_to, dataful_variant as u64))\n                 }\n             }\n@@ -316,20 +316,20 @@ impl<'a, 'tcx> PlaceRef<'tcx> {\n \n     /// Set the discriminant for a new value of the given case of the given\n     /// representation.\n-    pub fn trans_set_discr(&self, bcx: &Builder<'a, 'tcx>, variant_index: usize) {\n-        if self.layout.for_variant(bcx.cx, variant_index).abi == layout::Abi::Uninhabited {\n+    pub fn trans_set_discr(&self, bx: &Builder<'a, 'tcx>, variant_index: usize) {\n+        if self.layout.for_variant(bx.cx, variant_index).abi == layout::Abi::Uninhabited {\n             return;\n         }\n         match self.layout.variants {\n             layout::Variants::Single { index } => {\n                 assert_eq!(index, variant_index);\n             }\n             layout::Variants::Tagged { .. } => {\n-                let ptr = self.project_field(bcx, 0);\n+                let ptr = self.project_field(bx, 0);\n                 let to = self.layout.ty.ty_adt_def().unwrap()\n-                    .discriminant_for_variant(bcx.tcx(), variant_index)\n+                    .discriminant_for_variant(bx.tcx(), variant_index)\n                     .to_u128_unchecked() as u64;\n-                bcx.store(C_int(ptr.layout.llvm_type(bcx.cx), to as i64),\n+                bx.store(C_int(ptr.layout.llvm_type(bx.cx), to as i64),\n                     ptr.llval, ptr.align);\n             }\n             layout::Variants::NicheFilling {\n@@ -339,20 +339,20 @@ impl<'a, 'tcx> PlaceRef<'tcx> {\n                 ..\n             } => {\n                 if variant_index != dataful_variant {\n-                    if bcx.sess().target.target.arch == \"arm\" ||\n-                       bcx.sess().target.target.arch == \"aarch64\" {\n+                    if bx.sess().target.target.arch == \"arm\" ||\n+                       bx.sess().target.target.arch == \"aarch64\" {\n                         // Issue #34427: As workaround for LLVM bug on ARM,\n                         // use memset of 0 before assigning niche value.\n-                        let llptr = bcx.pointercast(self.llval, Type::i8(bcx.cx).ptr_to());\n-                        let fill_byte = C_u8(bcx.cx, 0);\n+                        let llptr = bx.pointercast(self.llval, Type::i8(bx.cx).ptr_to());\n+                        let fill_byte = C_u8(bx.cx, 0);\n                         let (size, align) = self.layout.size_and_align();\n-                        let size = C_usize(bcx.cx, size.bytes());\n-                        let align = C_u32(bcx.cx, align.abi() as u32);\n-                        base::call_memset(bcx, llptr, fill_byte, size, align, false);\n+                        let size = C_usize(bx.cx, size.bytes());\n+                        let align = C_u32(bx.cx, align.abi() as u32);\n+                        base::call_memset(bx, llptr, fill_byte, size, align, false);\n                     }\n \n-                    let niche = self.project_field(bcx, 0);\n-                    let niche_llty = niche.layout.immediate_llvm_type(bcx.cx);\n+                    let niche = self.project_field(bx, 0);\n+                    let niche_llty = niche.layout.immediate_llvm_type(bx.cx);\n                     let niche_value = ((variant_index - niche_variants.start) as u128)\n                         .wrapping_add(niche_start);\n                     // FIXME(eddyb) Check the actual primitive type here.\n@@ -362,51 +362,51 @@ impl<'a, 'tcx> PlaceRef<'tcx> {\n                     } else {\n                         C_uint_big(niche_llty, niche_value)\n                     };\n-                    OperandValue::Immediate(niche_llval).store(bcx, niche);\n+                    OperandValue::Immediate(niche_llval).store(bx, niche);\n                 }\n             }\n         }\n     }\n \n-    pub fn project_index(&self, bcx: &Builder<'a, 'tcx>, llindex: ValueRef)\n+    pub fn project_index(&self, bx: &Builder<'a, 'tcx>, llindex: ValueRef)\n                          -> PlaceRef<'tcx> {\n         PlaceRef {\n-            llval: bcx.inbounds_gep(self.llval, &[C_usize(bcx.cx, 0), llindex]),\n+            llval: bx.inbounds_gep(self.llval, &[C_usize(bx.cx, 0), llindex]),\n             llextra: ptr::null_mut(),\n-            layout: self.layout.field(bcx.cx, 0),\n+            layout: self.layout.field(bx.cx, 0),\n             align: self.align\n         }\n     }\n \n-    pub fn project_downcast(&self, bcx: &Builder<'a, 'tcx>, variant_index: usize)\n+    pub fn project_downcast(&self, bx: &Builder<'a, 'tcx>, variant_index: usize)\n                             -> PlaceRef<'tcx> {\n         let mut downcast = *self;\n-        downcast.layout = self.layout.for_variant(bcx.cx, variant_index);\n+        downcast.layout = self.layout.for_variant(bx.cx, variant_index);\n \n         // Cast to the appropriate variant struct type.\n-        let variant_ty = downcast.layout.llvm_type(bcx.cx);\n-        downcast.llval = bcx.pointercast(downcast.llval, variant_ty.ptr_to());\n+        let variant_ty = downcast.layout.llvm_type(bx.cx);\n+        downcast.llval = bx.pointercast(downcast.llval, variant_ty.ptr_to());\n \n         downcast\n     }\n \n-    pub fn storage_live(&self, bcx: &Builder<'a, 'tcx>) {\n-        bcx.lifetime_start(self.llval, self.layout.size);\n+    pub fn storage_live(&self, bx: &Builder<'a, 'tcx>) {\n+        bx.lifetime_start(self.llval, self.layout.size);\n     }\n \n-    pub fn storage_dead(&self, bcx: &Builder<'a, 'tcx>) {\n-        bcx.lifetime_end(self.llval, self.layout.size);\n+    pub fn storage_dead(&self, bx: &Builder<'a, 'tcx>) {\n+        bx.lifetime_end(self.llval, self.layout.size);\n     }\n }\n \n impl<'a, 'tcx> MirContext<'a, 'tcx> {\n     pub fn trans_place(&mut self,\n-                        bcx: &Builder<'a, 'tcx>,\n+                        bx: &Builder<'a, 'tcx>,\n                         place: &mir::Place<'tcx>)\n                         -> PlaceRef<'tcx> {\n         debug!(\"trans_place(place={:?})\", place);\n \n-        let cx = bcx.cx;\n+        let cx = bx.cx;\n         let tcx = cx.tcx;\n \n         if let mir::Place::Local(index) = *place {\n@@ -431,58 +431,58 @@ impl<'a, 'tcx> MirContext<'a, 'tcx> {\n                 elem: mir::ProjectionElem::Deref\n             }) => {\n                 // Load the pointer from its location.\n-                self.trans_consume(bcx, base).deref(bcx.cx)\n+                self.trans_consume(bx, base).deref(bx.cx)\n             }\n             mir::Place::Projection(ref projection) => {\n-                let tr_base = self.trans_place(bcx, &projection.base);\n+                let tr_base = self.trans_place(bx, &projection.base);\n \n                 match projection.elem {\n                     mir::ProjectionElem::Deref => bug!(),\n                     mir::ProjectionElem::Field(ref field, _) => {\n-                        tr_base.project_field(bcx, field.index())\n+                        tr_base.project_field(bx, field.index())\n                     }\n                     mir::ProjectionElem::Index(index) => {\n                         let index = &mir::Operand::Copy(mir::Place::Local(index));\n-                        let index = self.trans_operand(bcx, index);\n+                        let index = self.trans_operand(bx, index);\n                         let llindex = index.immediate();\n-                        tr_base.project_index(bcx, llindex)\n+                        tr_base.project_index(bx, llindex)\n                     }\n                     mir::ProjectionElem::ConstantIndex { offset,\n                                                          from_end: false,\n                                                          min_length: _ } => {\n-                        let lloffset = C_usize(bcx.cx, offset as u64);\n-                        tr_base.project_index(bcx, lloffset)\n+                        let lloffset = C_usize(bx.cx, offset as u64);\n+                        tr_base.project_index(bx, lloffset)\n                     }\n                     mir::ProjectionElem::ConstantIndex { offset,\n                                                          from_end: true,\n                                                          min_length: _ } => {\n-                        let lloffset = C_usize(bcx.cx, offset as u64);\n-                        let lllen = tr_base.len(bcx.cx);\n-                        let llindex = bcx.sub(lllen, lloffset);\n-                        tr_base.project_index(bcx, llindex)\n+                        let lloffset = C_usize(bx.cx, offset as u64);\n+                        let lllen = tr_base.len(bx.cx);\n+                        let llindex = bx.sub(lllen, lloffset);\n+                        tr_base.project_index(bx, llindex)\n                     }\n                     mir::ProjectionElem::Subslice { from, to } => {\n-                        let mut subslice = tr_base.project_index(bcx,\n-                            C_usize(bcx.cx, from as u64));\n+                        let mut subslice = tr_base.project_index(bx,\n+                            C_usize(bx.cx, from as u64));\n                         let projected_ty = PlaceTy::Ty { ty: tr_base.layout.ty }\n-                            .projection_ty(tcx, &projection.elem).to_ty(bcx.tcx());\n-                        subslice.layout = bcx.cx.layout_of(self.monomorphize(&projected_ty));\n+                            .projection_ty(tcx, &projection.elem).to_ty(bx.tcx());\n+                        subslice.layout = bx.cx.layout_of(self.monomorphize(&projected_ty));\n \n                         if subslice.layout.is_unsized() {\n                             assert!(tr_base.has_extra());\n-                            subslice.llextra = bcx.sub(tr_base.llextra,\n-                                C_usize(bcx.cx, (from as u64) + (to as u64)));\n+                            subslice.llextra = bx.sub(tr_base.llextra,\n+                                C_usize(bx.cx, (from as u64) + (to as u64)));\n                         }\n \n                         // Cast the place pointer type to the new\n                         // array or slice type (*[%_; new_len]).\n-                        subslice.llval = bcx.pointercast(subslice.llval,\n-                            subslice.layout.llvm_type(bcx.cx).ptr_to());\n+                        subslice.llval = bx.pointercast(subslice.llval,\n+                            subslice.layout.llvm_type(bx.cx).ptr_to());\n \n                         subslice\n                     }\n                     mir::ProjectionElem::Downcast(_, v) => {\n-                        tr_base.project_downcast(bcx, v)\n+                        tr_base.project_downcast(bx, v)\n                     }\n                 }\n             }"}, {"sha": "ce15ca8565157b38d85d54604dd88d37b4ba2d2f", "filename": "src/librustc_trans/mir/rvalue.rs", "status": "modified", "additions": 197, "deletions": 197, "changes": 394, "blob_url": "https://github.com/rust-lang/rust/blob/209abc71e52b864429330e8debd8ce1080a7f3b4/src%2Flibrustc_trans%2Fmir%2Frvalue.rs", "raw_url": "https://github.com/rust-lang/rust/raw/209abc71e52b864429330e8debd8ce1080a7f3b4/src%2Flibrustc_trans%2Fmir%2Frvalue.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_trans%2Fmir%2Frvalue.rs?ref=209abc71e52b864429330e8debd8ce1080a7f3b4", "patch": "@@ -36,7 +36,7 @@ use super::place::PlaceRef;\n \n impl<'a, 'tcx> MirContext<'a, 'tcx> {\n     pub fn trans_rvalue(&mut self,\n-                        bcx: Builder<'a, 'tcx>,\n+                        bx: Builder<'a, 'tcx>,\n                         dest: PlaceRef<'tcx>,\n                         rvalue: &mir::Rvalue<'tcx>)\n                         -> Builder<'a, 'tcx>\n@@ -46,11 +46,11 @@ impl<'a, 'tcx> MirContext<'a, 'tcx> {\n \n         match *rvalue {\n            mir::Rvalue::Use(ref operand) => {\n-               let tr_operand = self.trans_operand(&bcx, operand);\n+               let tr_operand = self.trans_operand(&bx, operand);\n                // FIXME: consider not copying constants through stack. (fixable by translating\n                // constants into OperandValue::Ref, why don\u2019t we do that yet if we don\u2019t?)\n-               tr_operand.val.store(&bcx, dest);\n-               bcx\n+               tr_operand.val.store(&bx, dest);\n+               bx\n            }\n \n             mir::Rvalue::Cast(mir::CastKind::Unsize, ref source, _) => {\n@@ -59,16 +59,16 @@ impl<'a, 'tcx> MirContext<'a, 'tcx> {\n                 if dest.layout.is_llvm_scalar_pair() {\n                     // into-coerce of a thin pointer to a fat pointer - just\n                     // use the operand path.\n-                    let (bcx, temp) = self.trans_rvalue_operand(bcx, rvalue);\n-                    temp.val.store(&bcx, dest);\n-                    return bcx;\n+                    let (bx, temp) = self.trans_rvalue_operand(bx, rvalue);\n+                    temp.val.store(&bx, dest);\n+                    return bx;\n                 }\n \n                 // Unsize of a nontrivial struct. I would prefer for\n                 // this to be eliminated by MIR translation, but\n                 // `CoerceUnsized` can be passed by a where-clause,\n                 // so the (generic) MIR may not be able to expand it.\n-                let operand = self.trans_operand(&bcx, source);\n+                let operand = self.trans_operand(&bx, source);\n                 match operand.val {\n                     OperandValue::Pair(..) |\n                     OperandValue::Immediate(_) => {\n@@ -79,124 +79,124 @@ impl<'a, 'tcx> MirContext<'a, 'tcx> {\n                         // index into the struct, and this case isn't\n                         // important enough for it.\n                         debug!(\"trans_rvalue: creating ugly alloca\");\n-                        let scratch = PlaceRef::alloca(&bcx, operand.layout, \"__unsize_temp\");\n-                        scratch.storage_live(&bcx);\n-                        operand.val.store(&bcx, scratch);\n-                        base::coerce_unsized_into(&bcx, scratch, dest);\n-                        scratch.storage_dead(&bcx);\n+                        let scratch = PlaceRef::alloca(&bx, operand.layout, \"__unsize_temp\");\n+                        scratch.storage_live(&bx);\n+                        operand.val.store(&bx, scratch);\n+                        base::coerce_unsized_into(&bx, scratch, dest);\n+                        scratch.storage_dead(&bx);\n                     }\n                     OperandValue::Ref(llref, align) => {\n                         let source = PlaceRef::new_sized(llref, operand.layout, align);\n-                        base::coerce_unsized_into(&bcx, source, dest);\n+                        base::coerce_unsized_into(&bx, source, dest);\n                     }\n                 }\n-                bcx\n+                bx\n             }\n \n             mir::Rvalue::Repeat(ref elem, count) => {\n-                let tr_elem = self.trans_operand(&bcx, elem);\n+                let tr_elem = self.trans_operand(&bx, elem);\n \n                 // Do not generate the loop for zero-sized elements or empty arrays.\n                 if dest.layout.is_zst() {\n-                    return bcx;\n+                    return bx;\n                 }\n \n-                let start = dest.project_index(&bcx, C_usize(bcx.cx, 0)).llval;\n+                let start = dest.project_index(&bx, C_usize(bx.cx, 0)).llval;\n \n                 if let OperandValue::Immediate(v) = tr_elem.val {\n-                    let align = C_i32(bcx.cx, dest.align.abi() as i32);\n-                    let size = C_usize(bcx.cx, dest.layout.size.bytes());\n+                    let align = C_i32(bx.cx, dest.align.abi() as i32);\n+                    let size = C_usize(bx.cx, dest.layout.size.bytes());\n \n                     // Use llvm.memset.p0i8.* to initialize all zero arrays\n                     if common::is_const_integral(v) && common::const_to_uint(v) == 0 {\n-                        let fill = C_u8(bcx.cx, 0);\n-                        base::call_memset(&bcx, start, fill, size, align, false);\n-                        return bcx;\n+                        let fill = C_u8(bx.cx, 0);\n+                        base::call_memset(&bx, start, fill, size, align, false);\n+                        return bx;\n                     }\n \n                     // Use llvm.memset.p0i8.* to initialize byte arrays\n-                    let v = base::from_immediate(&bcx, v);\n-                    if common::val_ty(v) == Type::i8(bcx.cx) {\n-                        base::call_memset(&bcx, start, v, size, align, false);\n-                        return bcx;\n+                    let v = base::from_immediate(&bx, v);\n+                    if common::val_ty(v) == Type::i8(bx.cx) {\n+                        base::call_memset(&bx, start, v, size, align, false);\n+                        return bx;\n                     }\n                 }\n \n                 let count = count.as_u64();\n-                let count = C_usize(bcx.cx, count);\n-                let end = dest.project_index(&bcx, count).llval;\n+                let count = C_usize(bx.cx, count);\n+                let end = dest.project_index(&bx, count).llval;\n \n-                let header_bcx = bcx.build_sibling_block(\"repeat_loop_header\");\n-                let body_bcx = bcx.build_sibling_block(\"repeat_loop_body\");\n-                let next_bcx = bcx.build_sibling_block(\"repeat_loop_next\");\n+                let header_bx = bx.build_sibling_block(\"repeat_loop_header\");\n+                let body_bx = bx.build_sibling_block(\"repeat_loop_body\");\n+                let next_bx = bx.build_sibling_block(\"repeat_loop_next\");\n \n-                bcx.br(header_bcx.llbb());\n-                let current = header_bcx.phi(common::val_ty(start), &[start], &[bcx.llbb()]);\n+                bx.br(header_bx.llbb());\n+                let current = header_bx.phi(common::val_ty(start), &[start], &[bx.llbb()]);\n \n-                let keep_going = header_bcx.icmp(llvm::IntNE, current, end);\n-                header_bcx.cond_br(keep_going, body_bcx.llbb(), next_bcx.llbb());\n+                let keep_going = header_bx.icmp(llvm::IntNE, current, end);\n+                header_bx.cond_br(keep_going, body_bx.llbb(), next_bx.llbb());\n \n-                tr_elem.val.store(&body_bcx,\n+                tr_elem.val.store(&body_bx,\n                     PlaceRef::new_sized(current, tr_elem.layout, dest.align));\n \n-                let next = body_bcx.inbounds_gep(current, &[C_usize(bcx.cx, 1)]);\n-                body_bcx.br(header_bcx.llbb());\n-                header_bcx.add_incoming_to_phi(current, next, body_bcx.llbb());\n+                let next = body_bx.inbounds_gep(current, &[C_usize(bx.cx, 1)]);\n+                body_bx.br(header_bx.llbb());\n+                header_bx.add_incoming_to_phi(current, next, body_bx.llbb());\n \n-                next_bcx\n+                next_bx\n             }\n \n             mir::Rvalue::Aggregate(ref kind, ref operands) => {\n                 let (dest, active_field_index) = match **kind {\n                     mir::AggregateKind::Adt(adt_def, variant_index, _, active_field_index) => {\n-                        dest.trans_set_discr(&bcx, variant_index);\n+                        dest.trans_set_discr(&bx, variant_index);\n                         if adt_def.is_enum() {\n-                            (dest.project_downcast(&bcx, variant_index), active_field_index)\n+                            (dest.project_downcast(&bx, variant_index), active_field_index)\n                         } else {\n                             (dest, active_field_index)\n                         }\n                     }\n                     _ => (dest, None)\n                 };\n                 for (i, operand) in operands.iter().enumerate() {\n-                    let op = self.trans_operand(&bcx, operand);\n+                    let op = self.trans_operand(&bx, operand);\n                     // Do not generate stores and GEPis for zero-sized fields.\n                     if !op.layout.is_zst() {\n                         let field_index = active_field_index.unwrap_or(i);\n-                        op.val.store(&bcx, dest.project_field(&bcx, field_index));\n+                        op.val.store(&bx, dest.project_field(&bx, field_index));\n                     }\n                 }\n-                bcx\n+                bx\n             }\n \n             _ => {\n                 assert!(self.rvalue_creates_operand(rvalue));\n-                let (bcx, temp) = self.trans_rvalue_operand(bcx, rvalue);\n-                temp.val.store(&bcx, dest);\n-                bcx\n+                let (bx, temp) = self.trans_rvalue_operand(bx, rvalue);\n+                temp.val.store(&bx, dest);\n+                bx\n             }\n         }\n     }\n \n     pub fn trans_rvalue_operand(&mut self,\n-                                bcx: Builder<'a, 'tcx>,\n+                                bx: Builder<'a, 'tcx>,\n                                 rvalue: &mir::Rvalue<'tcx>)\n                                 -> (Builder<'a, 'tcx>, OperandRef<'tcx>)\n     {\n         assert!(self.rvalue_creates_operand(rvalue), \"cannot trans {:?} to operand\", rvalue);\n \n         match *rvalue {\n             mir::Rvalue::Cast(ref kind, ref source, mir_cast_ty) => {\n-                let operand = self.trans_operand(&bcx, source);\n+                let operand = self.trans_operand(&bx, source);\n                 debug!(\"cast operand is {:?}\", operand);\n-                let cast = bcx.cx.layout_of(self.monomorphize(&mir_cast_ty));\n+                let cast = bx.cx.layout_of(self.monomorphize(&mir_cast_ty));\n \n                 let val = match *kind {\n                     mir::CastKind::ReifyFnPointer => {\n                         match operand.layout.ty.sty {\n                             ty::TyFnDef(def_id, substs) => {\n                                 OperandValue::Immediate(\n-                                    callee::resolve_and_get_fn(bcx.cx, def_id, substs))\n+                                    callee::resolve_and_get_fn(bx.cx, def_id, substs))\n                             }\n                             _ => {\n                                 bug!(\"{} cannot be reified to a fn ptr\", operand.layout.ty)\n@@ -207,8 +207,8 @@ impl<'a, 'tcx> MirContext<'a, 'tcx> {\n                         match operand.layout.ty.sty {\n                             ty::TyClosure(def_id, substs) => {\n                                 let instance = monomorphize::resolve_closure(\n-                                    bcx.cx.tcx, def_id, substs, ty::ClosureKind::FnOnce);\n-                                OperandValue::Immediate(callee::get_fn(bcx.cx, instance))\n+                                    bx.cx.tcx, def_id, substs, ty::ClosureKind::FnOnce);\n+                                OperandValue::Immediate(callee::get_fn(bx.cx, instance))\n                             }\n                             _ => {\n                                 bug!(\"{} cannot be cast to a fn ptr\", operand.layout.ty)\n@@ -230,13 +230,13 @@ impl<'a, 'tcx> MirContext<'a, 'tcx> {\n \n                                 // HACK(eddyb) have to bitcast pointers\n                                 // until LLVM removes pointee types.\n-                                let lldata = bcx.pointercast(lldata,\n-                                    cast.scalar_pair_element_llvm_type(bcx.cx, 0));\n+                                let lldata = bx.pointercast(lldata,\n+                                    cast.scalar_pair_element_llvm_type(bx.cx, 0));\n                                 OperandValue::Pair(lldata, llextra)\n                             }\n                             OperandValue::Immediate(lldata) => {\n                                 // \"standard\" unsize\n-                                let (lldata, llextra) = base::unsize_thin_ptr(&bcx, lldata,\n+                                let (lldata, llextra) = base::unsize_thin_ptr(&bx, lldata,\n                                     operand.layout.ty, cast.ty);\n                                 OperandValue::Pair(lldata, llextra)\n                             }\n@@ -249,14 +249,14 @@ impl<'a, 'tcx> MirContext<'a, 'tcx> {\n                     mir::CastKind::Misc if operand.layout.is_llvm_scalar_pair() => {\n                         if let OperandValue::Pair(data_ptr, meta) = operand.val {\n                             if cast.is_llvm_scalar_pair() {\n-                                let data_cast = bcx.pointercast(data_ptr,\n-                                    cast.scalar_pair_element_llvm_type(bcx.cx, 0));\n+                                let data_cast = bx.pointercast(data_ptr,\n+                                    cast.scalar_pair_element_llvm_type(bx.cx, 0));\n                                 OperandValue::Pair(data_cast, meta)\n                             } else { // cast to thin-ptr\n                                 // Cast of fat-ptr to thin-ptr is an extraction of data-ptr and\n                                 // pointer-cast of that pointer to desired pointer type.\n-                                let llcast_ty = cast.immediate_llvm_type(bcx.cx);\n-                                let llval = bcx.pointercast(data_ptr, llcast_ty);\n+                                let llcast_ty = cast.immediate_llvm_type(bx.cx);\n+                                let llval = bx.pointercast(data_ptr, llcast_ty);\n                                 OperandValue::Immediate(llval)\n                             }\n                         } else {\n@@ -268,8 +268,8 @@ impl<'a, 'tcx> MirContext<'a, 'tcx> {\n                         let r_t_in = CastTy::from_ty(operand.layout.ty)\n                             .expect(\"bad input type for cast\");\n                         let r_t_out = CastTy::from_ty(cast.ty).expect(\"bad output type for cast\");\n-                        let ll_t_in = operand.layout.immediate_llvm_type(bcx.cx);\n-                        let ll_t_out = cast.immediate_llvm_type(bcx.cx);\n+                        let ll_t_in = operand.layout.immediate_llvm_type(bx.cx);\n+                        let ll_t_out = cast.immediate_llvm_type(bx.cx);\n                         let llval = operand.immediate();\n \n                         let mut signed = false;\n@@ -282,7 +282,7 @@ impl<'a, 'tcx> MirContext<'a, 'tcx> {\n                                     // have bound checks, and this is the most\n                                     // convenient place to put the `assume`.\n \n-                                    base::call_assume(&bcx, bcx.icmp(\n+                                    base::call_assume(&bx, bx.icmp(\n                                         llvm::IntULE,\n                                         llval,\n                                         C_uint_big(ll_t_in, scalar.valid_range.end)\n@@ -293,60 +293,60 @@ impl<'a, 'tcx> MirContext<'a, 'tcx> {\n \n                         let newval = match (r_t_in, r_t_out) {\n                             (CastTy::Int(_), CastTy::Int(_)) => {\n-                                bcx.intcast(llval, ll_t_out, signed)\n+                                bx.intcast(llval, ll_t_out, signed)\n                             }\n                             (CastTy::Float, CastTy::Float) => {\n                                 let srcsz = ll_t_in.float_width();\n                                 let dstsz = ll_t_out.float_width();\n                                 if dstsz > srcsz {\n-                                    bcx.fpext(llval, ll_t_out)\n+                                    bx.fpext(llval, ll_t_out)\n                                 } else if srcsz > dstsz {\n-                                    bcx.fptrunc(llval, ll_t_out)\n+                                    bx.fptrunc(llval, ll_t_out)\n                                 } else {\n                                     llval\n                                 }\n                             }\n                             (CastTy::Ptr(_), CastTy::Ptr(_)) |\n                             (CastTy::FnPtr, CastTy::Ptr(_)) |\n                             (CastTy::RPtr(_), CastTy::Ptr(_)) =>\n-                                bcx.pointercast(llval, ll_t_out),\n+                                bx.pointercast(llval, ll_t_out),\n                             (CastTy::Ptr(_), CastTy::Int(_)) |\n                             (CastTy::FnPtr, CastTy::Int(_)) =>\n-                                bcx.ptrtoint(llval, ll_t_out),\n+                                bx.ptrtoint(llval, ll_t_out),\n                             (CastTy::Int(_), CastTy::Ptr(_)) => {\n-                                let usize_llval = bcx.intcast(llval, bcx.cx.isize_ty, signed);\n-                                bcx.inttoptr(usize_llval, ll_t_out)\n+                                let usize_llval = bx.intcast(llval, bx.cx.isize_ty, signed);\n+                                bx.inttoptr(usize_llval, ll_t_out)\n                             }\n                             (CastTy::Int(_), CastTy::Float) =>\n-                                cast_int_to_float(&bcx, signed, llval, ll_t_in, ll_t_out),\n+                                cast_int_to_float(&bx, signed, llval, ll_t_in, ll_t_out),\n                             (CastTy::Float, CastTy::Int(IntTy::I)) =>\n-                                cast_float_to_int(&bcx, true, llval, ll_t_in, ll_t_out),\n+                                cast_float_to_int(&bx, true, llval, ll_t_in, ll_t_out),\n                             (CastTy::Float, CastTy::Int(_)) =>\n-                                cast_float_to_int(&bcx, false, llval, ll_t_in, ll_t_out),\n+                                cast_float_to_int(&bx, false, llval, ll_t_in, ll_t_out),\n                             _ => bug!(\"unsupported cast: {:?} to {:?}\", operand.layout.ty, cast.ty)\n                         };\n                         OperandValue::Immediate(newval)\n                     }\n                 };\n-                (bcx, OperandRef {\n+                (bx, OperandRef {\n                     val,\n                     layout: cast\n                 })\n             }\n \n             mir::Rvalue::Ref(_, bk, ref place) => {\n-                let tr_place = self.trans_place(&bcx, place);\n+                let tr_place = self.trans_place(&bx, place);\n \n                 let ty = tr_place.layout.ty;\n \n                 // Note: places are indirect, so storing the `llval` into the\n                 // destination effectively creates a reference.\n-                let val = if !bcx.cx.type_has_metadata(ty) {\n+                let val = if !bx.cx.type_has_metadata(ty) {\n                     OperandValue::Immediate(tr_place.llval)\n                 } else {\n                     OperandValue::Pair(tr_place.llval, tr_place.llextra)\n                 };\n-                (bcx, OperandRef {\n+                (bx, OperandRef {\n                     val,\n                     layout: self.cx.layout_of(self.cx.tcx.mk_ref(\n                         self.cx.tcx.types.re_erased,\n@@ -356,136 +356,136 @@ impl<'a, 'tcx> MirContext<'a, 'tcx> {\n             }\n \n             mir::Rvalue::Len(ref place) => {\n-                let size = self.evaluate_array_len(&bcx, place);\n+                let size = self.evaluate_array_len(&bx, place);\n                 let operand = OperandRef {\n                     val: OperandValue::Immediate(size),\n-                    layout: bcx.cx.layout_of(bcx.tcx().types.usize),\n+                    layout: bx.cx.layout_of(bx.tcx().types.usize),\n                 };\n-                (bcx, operand)\n+                (bx, operand)\n             }\n \n             mir::Rvalue::BinaryOp(op, ref lhs, ref rhs) => {\n-                let lhs = self.trans_operand(&bcx, lhs);\n-                let rhs = self.trans_operand(&bcx, rhs);\n+                let lhs = self.trans_operand(&bx, lhs);\n+                let rhs = self.trans_operand(&bx, rhs);\n                 let llresult = match (lhs.val, rhs.val) {\n                     (OperandValue::Pair(lhs_addr, lhs_extra),\n                      OperandValue::Pair(rhs_addr, rhs_extra)) => {\n-                        self.trans_fat_ptr_binop(&bcx, op,\n+                        self.trans_fat_ptr_binop(&bx, op,\n                                                  lhs_addr, lhs_extra,\n                                                  rhs_addr, rhs_extra,\n                                                  lhs.layout.ty)\n                     }\n \n                     (OperandValue::Immediate(lhs_val),\n                      OperandValue::Immediate(rhs_val)) => {\n-                        self.trans_scalar_binop(&bcx, op, lhs_val, rhs_val, lhs.layout.ty)\n+                        self.trans_scalar_binop(&bx, op, lhs_val, rhs_val, lhs.layout.ty)\n                     }\n \n                     _ => bug!()\n                 };\n                 let operand = OperandRef {\n                     val: OperandValue::Immediate(llresult),\n-                    layout: bcx.cx.layout_of(\n-                        op.ty(bcx.tcx(), lhs.layout.ty, rhs.layout.ty)),\n+                    layout: bx.cx.layout_of(\n+                        op.ty(bx.tcx(), lhs.layout.ty, rhs.layout.ty)),\n                 };\n-                (bcx, operand)\n+                (bx, operand)\n             }\n             mir::Rvalue::CheckedBinaryOp(op, ref lhs, ref rhs) => {\n-                let lhs = self.trans_operand(&bcx, lhs);\n-                let rhs = self.trans_operand(&bcx, rhs);\n-                let result = self.trans_scalar_checked_binop(&bcx, op,\n+                let lhs = self.trans_operand(&bx, lhs);\n+                let rhs = self.trans_operand(&bx, rhs);\n+                let result = self.trans_scalar_checked_binop(&bx, op,\n                                                              lhs.immediate(), rhs.immediate(),\n                                                              lhs.layout.ty);\n-                let val_ty = op.ty(bcx.tcx(), lhs.layout.ty, rhs.layout.ty);\n-                let operand_ty = bcx.tcx().intern_tup(&[val_ty, bcx.tcx().types.bool], false);\n+                let val_ty = op.ty(bx.tcx(), lhs.layout.ty, rhs.layout.ty);\n+                let operand_ty = bx.tcx().intern_tup(&[val_ty, bx.tcx().types.bool], false);\n                 let operand = OperandRef {\n                     val: result,\n-                    layout: bcx.cx.layout_of(operand_ty)\n+                    layout: bx.cx.layout_of(operand_ty)\n                 };\n \n-                (bcx, operand)\n+                (bx, operand)\n             }\n \n             mir::Rvalue::UnaryOp(op, ref operand) => {\n-                let operand = self.trans_operand(&bcx, operand);\n+                let operand = self.trans_operand(&bx, operand);\n                 let lloperand = operand.immediate();\n                 let is_float = operand.layout.ty.is_fp();\n                 let llval = match op {\n-                    mir::UnOp::Not => bcx.not(lloperand),\n+                    mir::UnOp::Not => bx.not(lloperand),\n                     mir::UnOp::Neg => if is_float {\n-                        bcx.fneg(lloperand)\n+                        bx.fneg(lloperand)\n                     } else {\n-                        bcx.neg(lloperand)\n+                        bx.neg(lloperand)\n                     }\n                 };\n-                (bcx, OperandRef {\n+                (bx, OperandRef {\n                     val: OperandValue::Immediate(llval),\n                     layout: operand.layout,\n                 })\n             }\n \n             mir::Rvalue::Discriminant(ref place) => {\n-                let discr_ty = rvalue.ty(&*self.mir, bcx.tcx());\n-                let discr =  self.trans_place(&bcx, place)\n-                    .trans_get_discr(&bcx, discr_ty);\n-                (bcx, OperandRef {\n+                let discr_ty = rvalue.ty(&*self.mir, bx.tcx());\n+                let discr =  self.trans_place(&bx, place)\n+                    .trans_get_discr(&bx, discr_ty);\n+                (bx, OperandRef {\n                     val: OperandValue::Immediate(discr),\n                     layout: self.cx.layout_of(discr_ty)\n                 })\n             }\n \n             mir::Rvalue::NullaryOp(mir::NullOp::SizeOf, ty) => {\n-                assert!(bcx.cx.type_is_sized(ty));\n-                let val = C_usize(bcx.cx, bcx.cx.size_of(ty).bytes());\n-                let tcx = bcx.tcx();\n-                (bcx, OperandRef {\n+                assert!(bx.cx.type_is_sized(ty));\n+                let val = C_usize(bx.cx, bx.cx.size_of(ty).bytes());\n+                let tcx = bx.tcx();\n+                (bx, OperandRef {\n                     val: OperandValue::Immediate(val),\n                     layout: self.cx.layout_of(tcx.types.usize),\n                 })\n             }\n \n             mir::Rvalue::NullaryOp(mir::NullOp::Box, content_ty) => {\n                 let content_ty: Ty<'tcx> = self.monomorphize(&content_ty);\n-                let (size, align) = bcx.cx.size_and_align_of(content_ty);\n-                let llsize = C_usize(bcx.cx, size.bytes());\n-                let llalign = C_usize(bcx.cx, align.abi());\n-                let box_layout = bcx.cx.layout_of(bcx.tcx().mk_box(content_ty));\n-                let llty_ptr = box_layout.llvm_type(bcx.cx);\n+                let (size, align) = bx.cx.size_and_align_of(content_ty);\n+                let llsize = C_usize(bx.cx, size.bytes());\n+                let llalign = C_usize(bx.cx, align.abi());\n+                let box_layout = bx.cx.layout_of(bx.tcx().mk_box(content_ty));\n+                let llty_ptr = box_layout.llvm_type(bx.cx);\n \n                 // Allocate space:\n-                let def_id = match bcx.tcx().lang_items().require(ExchangeMallocFnLangItem) {\n+                let def_id = match bx.tcx().lang_items().require(ExchangeMallocFnLangItem) {\n                     Ok(id) => id,\n                     Err(s) => {\n-                        bcx.sess().fatal(&format!(\"allocation of `{}` {}\", box_layout.ty, s));\n+                        bx.sess().fatal(&format!(\"allocation of `{}` {}\", box_layout.ty, s));\n                     }\n                 };\n-                let instance = ty::Instance::mono(bcx.tcx(), def_id);\n-                let r = callee::get_fn(bcx.cx, instance);\n-                let val = bcx.pointercast(bcx.call(r, &[llsize, llalign], None), llty_ptr);\n+                let instance = ty::Instance::mono(bx.tcx(), def_id);\n+                let r = callee::get_fn(bx.cx, instance);\n+                let val = bx.pointercast(bx.call(r, &[llsize, llalign], None), llty_ptr);\n \n                 let operand = OperandRef {\n                     val: OperandValue::Immediate(val),\n                     layout: box_layout,\n                 };\n-                (bcx, operand)\n+                (bx, operand)\n             }\n             mir::Rvalue::Use(ref operand) => {\n-                let operand = self.trans_operand(&bcx, operand);\n-                (bcx, operand)\n+                let operand = self.trans_operand(&bx, operand);\n+                (bx, operand)\n             }\n             mir::Rvalue::Repeat(..) |\n             mir::Rvalue::Aggregate(..) => {\n                 // According to `rvalue_creates_operand`, only ZST\n                 // aggregate rvalues are allowed to be operands.\n                 let ty = rvalue.ty(self.mir, self.cx.tcx);\n-                (bcx, OperandRef::new_zst(self.cx,\n+                (bx, OperandRef::new_zst(self.cx,\n                     self.cx.layout_of(self.monomorphize(&ty))))\n             }\n         }\n     }\n \n     fn evaluate_array_len(&mut self,\n-                          bcx: &Builder<'a, 'tcx>,\n+                          bx: &Builder<'a, 'tcx>,\n                           place: &mir::Place<'tcx>) -> ValueRef\n     {\n         // ZST are passed as operands and require special handling\n@@ -494,17 +494,17 @@ impl<'a, 'tcx> MirContext<'a, 'tcx> {\n             if let LocalRef::Operand(Some(op)) = self.locals[index] {\n                 if let ty::TyArray(_, n) = op.layout.ty.sty {\n                     let n = n.val.to_const_int().unwrap().to_u64().unwrap();\n-                    return common::C_usize(bcx.cx, n);\n+                    return common::C_usize(bx.cx, n);\n                 }\n             }\n         }\n         // use common size calculation for non zero-sized types\n-        let tr_value = self.trans_place(&bcx, place);\n-        return tr_value.len(bcx.cx);\n+        let tr_value = self.trans_place(&bx, place);\n+        return tr_value.len(bx.cx);\n     }\n \n     pub fn trans_scalar_binop(&mut self,\n-                              bcx: &Builder<'a, 'tcx>,\n+                              bx: &Builder<'a, 'tcx>,\n                               op: mir::BinOp,\n                               lhs: ValueRef,\n                               rhs: ValueRef,\n@@ -515,63 +515,63 @@ impl<'a, 'tcx> MirContext<'a, 'tcx> {\n         let is_bool = input_ty.is_bool();\n         match op {\n             mir::BinOp::Add => if is_float {\n-                bcx.fadd(lhs, rhs)\n+                bx.fadd(lhs, rhs)\n             } else {\n-                bcx.add(lhs, rhs)\n+                bx.add(lhs, rhs)\n             },\n             mir::BinOp::Sub => if is_float {\n-                bcx.fsub(lhs, rhs)\n+                bx.fsub(lhs, rhs)\n             } else {\n-                bcx.sub(lhs, rhs)\n+                bx.sub(lhs, rhs)\n             },\n             mir::BinOp::Mul => if is_float {\n-                bcx.fmul(lhs, rhs)\n+                bx.fmul(lhs, rhs)\n             } else {\n-                bcx.mul(lhs, rhs)\n+                bx.mul(lhs, rhs)\n             },\n             mir::BinOp::Div => if is_float {\n-                bcx.fdiv(lhs, rhs)\n+                bx.fdiv(lhs, rhs)\n             } else if is_signed {\n-                bcx.sdiv(lhs, rhs)\n+                bx.sdiv(lhs, rhs)\n             } else {\n-                bcx.udiv(lhs, rhs)\n+                bx.udiv(lhs, rhs)\n             },\n             mir::BinOp::Rem => if is_float {\n-                bcx.frem(lhs, rhs)\n+                bx.frem(lhs, rhs)\n             } else if is_signed {\n-                bcx.srem(lhs, rhs)\n+                bx.srem(lhs, rhs)\n             } else {\n-                bcx.urem(lhs, rhs)\n+                bx.urem(lhs, rhs)\n             },\n-            mir::BinOp::BitOr => bcx.or(lhs, rhs),\n-            mir::BinOp::BitAnd => bcx.and(lhs, rhs),\n-            mir::BinOp::BitXor => bcx.xor(lhs, rhs),\n-            mir::BinOp::Offset => bcx.inbounds_gep(lhs, &[rhs]),\n-            mir::BinOp::Shl => common::build_unchecked_lshift(bcx, lhs, rhs),\n-            mir::BinOp::Shr => common::build_unchecked_rshift(bcx, input_ty, lhs, rhs),\n+            mir::BinOp::BitOr => bx.or(lhs, rhs),\n+            mir::BinOp::BitAnd => bx.and(lhs, rhs),\n+            mir::BinOp::BitXor => bx.xor(lhs, rhs),\n+            mir::BinOp::Offset => bx.inbounds_gep(lhs, &[rhs]),\n+            mir::BinOp::Shl => common::build_unchecked_lshift(bx, lhs, rhs),\n+            mir::BinOp::Shr => common::build_unchecked_rshift(bx, input_ty, lhs, rhs),\n             mir::BinOp::Ne | mir::BinOp::Lt | mir::BinOp::Gt |\n             mir::BinOp::Eq | mir::BinOp::Le | mir::BinOp::Ge => if is_nil {\n-                C_bool(bcx.cx, match op {\n+                C_bool(bx.cx, match op {\n                     mir::BinOp::Ne | mir::BinOp::Lt | mir::BinOp::Gt => false,\n                     mir::BinOp::Eq | mir::BinOp::Le | mir::BinOp::Ge => true,\n                     _ => unreachable!()\n                 })\n             } else if is_float {\n-                bcx.fcmp(\n+                bx.fcmp(\n                     base::bin_op_to_fcmp_predicate(op.to_hir_binop()),\n                     lhs, rhs\n                 )\n             } else {\n                 let (lhs, rhs) = if is_bool {\n                     // FIXME(#36856) -- extend the bools into `i8` because\n                     // LLVM's i1 comparisons are broken.\n-                    (bcx.zext(lhs, Type::i8(bcx.cx)),\n-                     bcx.zext(rhs, Type::i8(bcx.cx)))\n+                    (bx.zext(lhs, Type::i8(bx.cx)),\n+                     bx.zext(rhs, Type::i8(bx.cx)))\n                 } else {\n                     (lhs, rhs)\n                 };\n \n-                bcx.icmp(\n+                bx.icmp(\n                     base::bin_op_to_icmp_predicate(op.to_hir_binop(), is_signed),\n                     lhs, rhs\n                 )\n@@ -580,7 +580,7 @@ impl<'a, 'tcx> MirContext<'a, 'tcx> {\n     }\n \n     pub fn trans_fat_ptr_binop(&mut self,\n-                               bcx: &Builder<'a, 'tcx>,\n+                               bx: &Builder<'a, 'tcx>,\n                                op: mir::BinOp,\n                                lhs_addr: ValueRef,\n                                lhs_extra: ValueRef,\n@@ -590,15 +590,15 @@ impl<'a, 'tcx> MirContext<'a, 'tcx> {\n                                -> ValueRef {\n         match op {\n             mir::BinOp::Eq => {\n-                bcx.and(\n-                    bcx.icmp(llvm::IntEQ, lhs_addr, rhs_addr),\n-                    bcx.icmp(llvm::IntEQ, lhs_extra, rhs_extra)\n+                bx.and(\n+                    bx.icmp(llvm::IntEQ, lhs_addr, rhs_addr),\n+                    bx.icmp(llvm::IntEQ, lhs_extra, rhs_extra)\n                 )\n             }\n             mir::BinOp::Ne => {\n-                bcx.or(\n-                    bcx.icmp(llvm::IntNE, lhs_addr, rhs_addr),\n-                    bcx.icmp(llvm::IntNE, lhs_extra, rhs_extra)\n+                bx.or(\n+                    bx.icmp(llvm::IntNE, lhs_addr, rhs_addr),\n+                    bx.icmp(llvm::IntNE, lhs_extra, rhs_extra)\n                 )\n             }\n             mir::BinOp::Le | mir::BinOp::Lt |\n@@ -612,11 +612,11 @@ impl<'a, 'tcx> MirContext<'a, 'tcx> {\n                     _ => bug!(),\n                 };\n \n-                bcx.or(\n-                    bcx.icmp(strict_op, lhs_addr, rhs_addr),\n-                    bcx.and(\n-                        bcx.icmp(llvm::IntEQ, lhs_addr, rhs_addr),\n-                        bcx.icmp(op, lhs_extra, rhs_extra)\n+                bx.or(\n+                    bx.icmp(strict_op, lhs_addr, rhs_addr),\n+                    bx.and(\n+                        bx.icmp(llvm::IntEQ, lhs_addr, rhs_addr),\n+                        bx.icmp(op, lhs_extra, rhs_extra)\n                     )\n                 )\n             }\n@@ -627,7 +627,7 @@ impl<'a, 'tcx> MirContext<'a, 'tcx> {\n     }\n \n     pub fn trans_scalar_checked_binop(&mut self,\n-                                      bcx: &Builder<'a, 'tcx>,\n+                                      bx: &Builder<'a, 'tcx>,\n                                       op: mir::BinOp,\n                                       lhs: ValueRef,\n                                       rhs: ValueRef,\n@@ -636,17 +636,17 @@ impl<'a, 'tcx> MirContext<'a, 'tcx> {\n         // with #[rustc_inherit_overflow_checks] and inlined from\n         // another crate (mostly core::num generic/#[inline] fns),\n         // while the current crate doesn't use overflow checks.\n-        if !bcx.cx.check_overflow {\n-            let val = self.trans_scalar_binop(bcx, op, lhs, rhs, input_ty);\n-            return OperandValue::Pair(val, C_bool(bcx.cx, false));\n+        if !bx.cx.check_overflow {\n+            let val = self.trans_scalar_binop(bx, op, lhs, rhs, input_ty);\n+            return OperandValue::Pair(val, C_bool(bx.cx, false));\n         }\n \n         // First try performing the operation on constants, which\n         // will only succeed if both operands are constant.\n         // This is necessary to determine when an overflow Assert\n         // will always panic at runtime, and produce a warning.\n-        if let Some((val, of)) = const_scalar_checked_binop(bcx.tcx(), op, lhs, rhs, input_ty) {\n-            return OperandValue::Pair(val, C_bool(bcx.cx, of));\n+        if let Some((val, of)) = const_scalar_checked_binop(bx.tcx(), op, lhs, rhs, input_ty) {\n+            return OperandValue::Pair(val, C_bool(bx.cx, of));\n         }\n \n         let (val, of) = match op {\n@@ -658,20 +658,20 @@ impl<'a, 'tcx> MirContext<'a, 'tcx> {\n                     mir::BinOp::Mul => OverflowOp::Mul,\n                     _ => unreachable!()\n                 };\n-                let intrinsic = get_overflow_intrinsic(oop, bcx, input_ty);\n-                let res = bcx.call(intrinsic, &[lhs, rhs], None);\n+                let intrinsic = get_overflow_intrinsic(oop, bx, input_ty);\n+                let res = bx.call(intrinsic, &[lhs, rhs], None);\n \n-                (bcx.extract_value(res, 0),\n-                 bcx.extract_value(res, 1))\n+                (bx.extract_value(res, 0),\n+                 bx.extract_value(res, 1))\n             }\n             mir::BinOp::Shl | mir::BinOp::Shr => {\n                 let lhs_llty = val_ty(lhs);\n                 let rhs_llty = val_ty(rhs);\n-                let invert_mask = common::shift_mask_val(&bcx, lhs_llty, rhs_llty, true);\n-                let outer_bits = bcx.and(rhs, invert_mask);\n+                let invert_mask = common::shift_mask_val(&bx, lhs_llty, rhs_llty, true);\n+                let outer_bits = bx.and(rhs, invert_mask);\n \n-                let of = bcx.icmp(llvm::IntNE, outer_bits, C_null(rhs_llty));\n-                let val = self.trans_scalar_binop(bcx, op, lhs, rhs, input_ty);\n+                let of = bx.icmp(llvm::IntNE, outer_bits, C_null(rhs_llty));\n+                let val = self.trans_scalar_binop(bx, op, lhs, rhs, input_ty);\n \n                 (val, of)\n             }\n@@ -712,12 +712,12 @@ enum OverflowOp {\n     Add, Sub, Mul\n }\n \n-fn get_overflow_intrinsic(oop: OverflowOp, bcx: &Builder, ty: Ty) -> ValueRef {\n+fn get_overflow_intrinsic(oop: OverflowOp, bx: &Builder, ty: Ty) -> ValueRef {\n     use syntax::ast::IntTy::*;\n     use syntax::ast::UintTy::*;\n     use rustc::ty::{TyInt, TyUint};\n \n-    let tcx = bcx.tcx();\n+    let tcx = bx.tcx();\n \n     let new_sty = match ty.sty {\n         TyInt(Isize) => match &tcx.sess.target.target.target_pointer_width[..] {\n@@ -784,10 +784,10 @@ fn get_overflow_intrinsic(oop: OverflowOp, bcx: &Builder, ty: Ty) -> ValueRef {\n         },\n     };\n \n-    bcx.cx.get_intrinsic(&name)\n+    bx.cx.get_intrinsic(&name)\n }\n \n-fn cast_int_to_float(bcx: &Builder,\n+fn cast_int_to_float(bx: &Builder,\n                      signed: bool,\n                      x: ValueRef,\n                      int_ty: Type,\n@@ -800,31 +800,31 @@ fn cast_int_to_float(bcx: &Builder,\n         // All inputs greater or equal to (f32::MAX + 0.5 ULP) are rounded to infinity,\n         // and for everything else LLVM's uitofp works just fine.\n         let max = C_uint_big(int_ty, MAX_F32_PLUS_HALF_ULP);\n-        let overflow = bcx.icmp(llvm::IntUGE, x, max);\n-        let infinity_bits = C_u32(bcx.cx, ieee::Single::INFINITY.to_bits() as u32);\n+        let overflow = bx.icmp(llvm::IntUGE, x, max);\n+        let infinity_bits = C_u32(bx.cx, ieee::Single::INFINITY.to_bits() as u32);\n         let infinity = consts::bitcast(infinity_bits, float_ty);\n-        bcx.select(overflow, infinity, bcx.uitofp(x, float_ty))\n+        bx.select(overflow, infinity, bx.uitofp(x, float_ty))\n     } else {\n         if signed {\n-            bcx.sitofp(x, float_ty)\n+            bx.sitofp(x, float_ty)\n         } else {\n-            bcx.uitofp(x, float_ty)\n+            bx.uitofp(x, float_ty)\n         }\n     }\n }\n \n-fn cast_float_to_int(bcx: &Builder,\n+fn cast_float_to_int(bx: &Builder,\n                      signed: bool,\n                      x: ValueRef,\n                      float_ty: Type,\n                      int_ty: Type) -> ValueRef {\n     let fptosui_result = if signed {\n-        bcx.fptosi(x, int_ty)\n+        bx.fptosi(x, int_ty)\n     } else {\n-        bcx.fptoui(x, int_ty)\n+        bx.fptoui(x, int_ty)\n     };\n \n-    if !bcx.sess().opts.debugging_opts.saturating_float_casts {\n+    if !bx.sess().opts.debugging_opts.saturating_float_casts {\n         return fptosui_result;\n     }\n     // LLVM's fpto[su]i returns undef when the input x is infinite, NaN, or does not fit into the\n@@ -870,8 +870,8 @@ fn cast_float_to_int(bcx: &Builder,\n     }\n     let float_bits_to_llval = |bits| {\n         let bits_llval = match float_ty.float_width() {\n-            32 => C_u32(bcx.cx, bits as u32),\n-            64 => C_u64(bcx.cx, bits as u64),\n+            32 => C_u32(bx.cx, bits as u32),\n+            64 => C_u64(bx.cx, bits as u64),\n             n => bug!(\"unsupported float width {}\", n),\n         };\n         consts::bitcast(bits_llval, float_ty)\n@@ -924,19 +924,19 @@ fn cast_float_to_int(bcx: &Builder,\n     // negation, and the negation can be merged into the select. Therefore, it not necessarily any\n     // more expensive than a ordered (\"normal\") comparison. Whether these optimizations will be\n     // performed is ultimately up to the backend, but at least x86 does perform them.\n-    let less_or_nan = bcx.fcmp(llvm::RealULT, x, f_min);\n-    let greater = bcx.fcmp(llvm::RealOGT, x, f_max);\n+    let less_or_nan = bx.fcmp(llvm::RealULT, x, f_min);\n+    let greater = bx.fcmp(llvm::RealOGT, x, f_max);\n     let int_max = C_uint_big(int_ty, int_max(signed, int_ty));\n     let int_min = C_uint_big(int_ty, int_min(signed, int_ty) as u128);\n-    let s0 = bcx.select(less_or_nan, int_min, fptosui_result);\n-    let s1 = bcx.select(greater, int_max, s0);\n+    let s0 = bx.select(less_or_nan, int_min, fptosui_result);\n+    let s1 = bx.select(greater, int_max, s0);\n \n     // Step 3: NaN replacement.\n     // For unsigned types, the above step already yielded int_ty::MIN == 0 if x is NaN.\n     // Therefore we only need to execute this step for signed integer types.\n     if signed {\n         // LLVM has no isNaN predicate, so we use (x == x) instead\n-        bcx.select(bcx.fcmp(llvm::RealOEQ, x, x), s1, C_uint(int_ty, 0))\n+        bx.select(bx.fcmp(llvm::RealOEQ, x, x), s1, C_uint(int_ty, 0))\n     } else {\n         s1\n     }"}, {"sha": "5a1c92ff5a8bb59ab13abcbfeeec7912efe0b807", "filename": "src/librustc_trans/mir/statement.rs", "status": "modified", "additions": 20, "deletions": 20, "changes": 40, "blob_url": "https://github.com/rust-lang/rust/blob/209abc71e52b864429330e8debd8ce1080a7f3b4/src%2Flibrustc_trans%2Fmir%2Fstatement.rs", "raw_url": "https://github.com/rust-lang/rust/raw/209abc71e52b864429330e8debd8ce1080a7f3b4/src%2Flibrustc_trans%2Fmir%2Fstatement.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_trans%2Fmir%2Fstatement.rs?ref=209abc71e52b864429330e8debd8ce1080a7f3b4", "patch": "@@ -18,23 +18,23 @@ use super::LocalRef;\n \n impl<'a, 'tcx> MirContext<'a, 'tcx> {\n     pub fn trans_statement(&mut self,\n-                           bcx: Builder<'a, 'tcx>,\n+                           bx: Builder<'a, 'tcx>,\n                            statement: &mir::Statement<'tcx>)\n                            -> Builder<'a, 'tcx> {\n         debug!(\"trans_statement(statement={:?})\", statement);\n \n-        self.set_debug_loc(&bcx, statement.source_info);\n+        self.set_debug_loc(&bx, statement.source_info);\n         match statement.kind {\n             mir::StatementKind::Assign(ref place, ref rvalue) => {\n                 if let mir::Place::Local(index) = *place {\n                     match self.locals[index] {\n                         LocalRef::Place(tr_dest) => {\n-                            self.trans_rvalue(bcx, tr_dest, rvalue)\n+                            self.trans_rvalue(bx, tr_dest, rvalue)\n                         }\n                         LocalRef::Operand(None) => {\n-                            let (bcx, operand) = self.trans_rvalue_operand(bcx, rvalue);\n+                            let (bx, operand) = self.trans_rvalue_operand(bx, rvalue);\n                             self.locals[index] = LocalRef::Operand(Some(operand));\n-                            bcx\n+                            bx\n                         }\n                         LocalRef::Operand(Some(op)) => {\n                             if !op.layout.is_zst() {\n@@ -45,46 +45,46 @@ impl<'a, 'tcx> MirContext<'a, 'tcx> {\n \n                             // If the type is zero-sized, it's already been set here,\n                             // but we still need to make sure we translate the operand\n-                            self.trans_rvalue_operand(bcx, rvalue).0\n+                            self.trans_rvalue_operand(bx, rvalue).0\n                         }\n                     }\n                 } else {\n-                    let tr_dest = self.trans_place(&bcx, place);\n-                    self.trans_rvalue(bcx, tr_dest, rvalue)\n+                    let tr_dest = self.trans_place(&bx, place);\n+                    self.trans_rvalue(bx, tr_dest, rvalue)\n                 }\n             }\n             mir::StatementKind::SetDiscriminant{ref place, variant_index} => {\n-                self.trans_place(&bcx, place)\n-                    .trans_set_discr(&bcx, variant_index);\n-                bcx\n+                self.trans_place(&bx, place)\n+                    .trans_set_discr(&bx, variant_index);\n+                bx\n             }\n             mir::StatementKind::StorageLive(local) => {\n                 if let LocalRef::Place(tr_place) = self.locals[local] {\n-                    tr_place.storage_live(&bcx);\n+                    tr_place.storage_live(&bx);\n                 }\n-                bcx\n+                bx\n             }\n             mir::StatementKind::StorageDead(local) => {\n                 if let LocalRef::Place(tr_place) = self.locals[local] {\n-                    tr_place.storage_dead(&bcx);\n+                    tr_place.storage_dead(&bx);\n                 }\n-                bcx\n+                bx\n             }\n             mir::StatementKind::InlineAsm { ref asm, ref outputs, ref inputs } => {\n                 let outputs = outputs.iter().map(|output| {\n-                    self.trans_place(&bcx, output)\n+                    self.trans_place(&bx, output)\n                 }).collect();\n \n                 let input_vals = inputs.iter().map(|input| {\n-                    self.trans_operand(&bcx, input).immediate()\n+                    self.trans_operand(&bx, input).immediate()\n                 }).collect();\n \n-                asm::trans_inline_asm(&bcx, asm, outputs, input_vals);\n-                bcx\n+                asm::trans_inline_asm(&bx, asm, outputs, input_vals);\n+                bx\n             }\n             mir::StatementKind::EndRegion(_) |\n             mir::StatementKind::Validate(..) |\n-            mir::StatementKind::Nop => bcx,\n+            mir::StatementKind::Nop => bx,\n         }\n     }\n }"}]}