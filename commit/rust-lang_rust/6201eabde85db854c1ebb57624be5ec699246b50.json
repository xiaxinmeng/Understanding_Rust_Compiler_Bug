{"sha": "6201eabde85db854c1ebb57624be5ec699246b50", "node_id": "C_kwDOAAsO6NoAKDYyMDFlYWJkZTg1ZGI4NTRjMWViYjU3NjI0YmU1ZWM2OTkyNDZiNTA", "commit": {"author": {"name": "bors", "email": "bors@rust-lang.org", "date": "2022-09-28T08:14:04Z"}, "committer": {"name": "bors", "email": "bors@rust-lang.org", "date": "2022-09-28T08:14:04Z"}, "message": "Auto merge of #102302 - nnethercote:more-lexer-improvements, r=matklad\n\nMore lexer improvements\n\nA follow-up to #99884.\n\nr? `@matklad`", "tree": {"sha": "41fd8394b4f257b7829380704e9496f3f1ba3077", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/41fd8394b4f257b7829380704e9496f3f1ba3077"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/6201eabde85db854c1ebb57624be5ec699246b50", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/6201eabde85db854c1ebb57624be5ec699246b50", "html_url": "https://github.com/rust-lang/rust/commit/6201eabde85db854c1ebb57624be5ec699246b50", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/6201eabde85db854c1ebb57624be5ec699246b50/comments", "author": {"login": "bors", "id": 3372342, "node_id": "MDQ6VXNlcjMzNzIzNDI=", "avatar_url": "https://avatars.githubusercontent.com/u/3372342?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bors", "html_url": "https://github.com/bors", "followers_url": "https://api.github.com/users/bors/followers", "following_url": "https://api.github.com/users/bors/following{/other_user}", "gists_url": "https://api.github.com/users/bors/gists{/gist_id}", "starred_url": "https://api.github.com/users/bors/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bors/subscriptions", "organizations_url": "https://api.github.com/users/bors/orgs", "repos_url": "https://api.github.com/users/bors/repos", "events_url": "https://api.github.com/users/bors/events{/privacy}", "received_events_url": "https://api.github.com/users/bors/received_events", "type": "User", "site_admin": false}, "committer": {"login": "bors", "id": 3372342, "node_id": "MDQ6VXNlcjMzNzIzNDI=", "avatar_url": "https://avatars.githubusercontent.com/u/3372342?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bors", "html_url": "https://github.com/bors", "followers_url": "https://api.github.com/users/bors/followers", "following_url": "https://api.github.com/users/bors/following{/other_user}", "gists_url": "https://api.github.com/users/bors/gists{/gist_id}", "starred_url": "https://api.github.com/users/bors/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bors/subscriptions", "organizations_url": "https://api.github.com/users/bors/orgs", "repos_url": "https://api.github.com/users/bors/repos", "events_url": "https://api.github.com/users/bors/events{/privacy}", "received_events_url": "https://api.github.com/users/bors/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "837bf370de144a682041e68bb67469b9f68a55ce", "url": "https://api.github.com/repos/rust-lang/rust/commits/837bf370de144a682041e68bb67469b9f68a55ce", "html_url": "https://github.com/rust-lang/rust/commit/837bf370de144a682041e68bb67469b9f68a55ce"}, {"sha": "d0a26acb2ae2d000e516eca92ae8feb08d1f6ea0", "url": "https://api.github.com/repos/rust-lang/rust/commits/d0a26acb2ae2d000e516eca92ae8feb08d1f6ea0", "html_url": "https://github.com/rust-lang/rust/commit/d0a26acb2ae2d000e516eca92ae8feb08d1f6ea0"}], "stats": {"total": 872, "additions": 429, "deletions": 443}, "files": [{"sha": "fa6162c51847a39bb218182b60fdcfba015e84e6", "filename": "compiler/rustc_ast/src/token.rs", "status": "modified", "additions": 1, "deletions": 6, "changes": 7, "blob_url": "https://github.com/rust-lang/rust/blob/6201eabde85db854c1ebb57624be5ec699246b50/compiler%2Frustc_ast%2Fsrc%2Ftoken.rs", "raw_url": "https://github.com/rust-lang/rust/raw/6201eabde85db854c1ebb57624be5ec699246b50/compiler%2Frustc_ast%2Fsrc%2Ftoken.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_ast%2Fsrc%2Ftoken.rs?ref=6201eabde85db854c1ebb57624be5ec699246b50", "patch": "@@ -13,7 +13,7 @@ use rustc_span::symbol::{kw, sym};\n use rustc_span::symbol::{Ident, Symbol};\n use rustc_span::{self, edition::Edition, Span, DUMMY_SP};\n use std::borrow::Cow;\n-use std::{fmt, mem};\n+use std::fmt;\n \n #[derive(Clone, Copy, PartialEq, Encodable, Decodable, Debug, HashStable_Generic)]\n pub enum CommentKind {\n@@ -335,11 +335,6 @@ impl Token {\n         Token::new(Ident(ident.name, ident.is_raw_guess()), ident.span)\n     }\n \n-    /// Return this token by value and leave a dummy token in its place.\n-    pub fn take(&mut self) -> Self {\n-        mem::replace(self, Token::dummy())\n-    }\n-\n     /// For interpolated tokens, returns a span of the fragment to which the interpolated\n     /// token refers. For all other tokens this is just a regular span.\n     /// It is particularly important to use this for identifiers and lifetimes"}, {"sha": "d3effb7c1c0f4b914f45237930e5adcb13bc77db", "filename": "compiler/rustc_errors/src/lib.rs", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "blob_url": "https://github.com/rust-lang/rust/blob/6201eabde85db854c1ebb57624be5ec699246b50/compiler%2Frustc_errors%2Fsrc%2Flib.rs", "raw_url": "https://github.com/rust-lang/rust/raw/6201eabde85db854c1ebb57624be5ec699246b50/compiler%2Frustc_errors%2Fsrc%2Flib.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_errors%2Fsrc%2Flib.rs?ref=6201eabde85db854c1ebb57624be5ec699246b50", "patch": "@@ -62,7 +62,8 @@ pub mod translation;\n pub use diagnostic_builder::IntoDiagnostic;\n pub use snippet::Style;\n \n-pub type PResult<'a, T> = Result<T, DiagnosticBuilder<'a, ErrorGuaranteed>>;\n+pub type PErr<'a> = DiagnosticBuilder<'a, ErrorGuaranteed>;\n+pub type PResult<'a, T> = Result<T, PErr<'a>>;\n \n // `PResult` is used a lot. Make sure it doesn't unintentionally get bigger.\n // (See also the comment on `DiagnosticBuilder`'s `diagnostic` field.)"}, {"sha": "eceef59802eb951d178d03157b380c2748fa0354", "filename": "compiler/rustc_lexer/src/cursor.rs", "status": "modified", "additions": 8, "deletions": 8, "changes": 16, "blob_url": "https://github.com/rust-lang/rust/blob/6201eabde85db854c1ebb57624be5ec699246b50/compiler%2Frustc_lexer%2Fsrc%2Fcursor.rs", "raw_url": "https://github.com/rust-lang/rust/raw/6201eabde85db854c1ebb57624be5ec699246b50/compiler%2Frustc_lexer%2Fsrc%2Fcursor.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_lexer%2Fsrc%2Fcursor.rs?ref=6201eabde85db854c1ebb57624be5ec699246b50", "patch": "@@ -4,8 +4,8 @@ use std::str::Chars;\n ///\n /// Next characters can be peeked via `first` method,\n /// and position can be shifted forward via `bump` method.\n-pub(crate) struct Cursor<'a> {\n-    initial_len: usize,\n+pub struct Cursor<'a> {\n+    len_remaining: usize,\n     /// Iterator over chars. Slightly faster than a &str.\n     chars: Chars<'a>,\n     #[cfg(debug_assertions)]\n@@ -15,9 +15,9 @@ pub(crate) struct Cursor<'a> {\n pub(crate) const EOF_CHAR: char = '\\0';\n \n impl<'a> Cursor<'a> {\n-    pub(crate) fn new(input: &'a str) -> Cursor<'a> {\n+    pub fn new(input: &'a str) -> Cursor<'a> {\n         Cursor {\n-            initial_len: input.len(),\n+            len_remaining: input.len(),\n             chars: input.chars(),\n             #[cfg(debug_assertions)]\n             prev: EOF_CHAR,\n@@ -61,13 +61,13 @@ impl<'a> Cursor<'a> {\n     }\n \n     /// Returns amount of already consumed symbols.\n-    pub(crate) fn len_consumed(&self) -> u32 {\n-        (self.initial_len - self.chars.as_str().len()) as u32\n+    pub(crate) fn pos_within_token(&self) -> u32 {\n+        (self.len_remaining - self.chars.as_str().len()) as u32\n     }\n \n     /// Resets the number of bytes consumed to 0.\n-    pub(crate) fn reset_len_consumed(&mut self) {\n-        self.initial_len = self.chars.as_str().len();\n+    pub(crate) fn reset_pos_within_token(&mut self) {\n+        self.len_remaining = self.chars.as_str().len();\n     }\n \n     /// Moves to the next character."}, {"sha": "c71e6ffe34d9b85dd93042c207b1d736953a8daf", "filename": "compiler/rustc_lexer/src/lib.rs", "status": "modified", "additions": 26, "deletions": 27, "changes": 53, "blob_url": "https://github.com/rust-lang/rust/blob/6201eabde85db854c1ebb57624be5ec699246b50/compiler%2Frustc_lexer%2Fsrc%2Flib.rs", "raw_url": "https://github.com/rust-lang/rust/raw/6201eabde85db854c1ebb57624be5ec699246b50/compiler%2Frustc_lexer%2Fsrc%2Flib.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_lexer%2Fsrc%2Flib.rs?ref=6201eabde85db854c1ebb57624be5ec699246b50", "patch": "@@ -29,9 +29,11 @@ pub mod unescape;\n #[cfg(test)]\n mod tests;\n \n+pub use crate::cursor::Cursor;\n+\n use self::LiteralKind::*;\n use self::TokenKind::*;\n-use crate::cursor::{Cursor, EOF_CHAR};\n+use crate::cursor::EOF_CHAR;\n use std::convert::TryFrom;\n \n /// Parsed token.\n@@ -139,6 +141,9 @@ pub enum TokenKind {\n \n     /// Unknown token, not expected by the lexer, e.g. \"\u2116\"\n     Unknown,\n+\n+    /// End of input.\n+    Eof,\n }\n \n #[derive(Clone, Copy, Debug, PartialEq, Eq)]\n@@ -219,13 +224,6 @@ pub fn strip_shebang(input: &str) -> Option<usize> {\n     None\n }\n \n-/// Parses the first token from the provided input string.\n-#[inline]\n-pub fn first_token(input: &str) -> Token {\n-    debug_assert!(!input.is_empty());\n-    Cursor::new(input).advance_token()\n-}\n-\n /// Validates a raw string literal. Used for getting more information about a\n /// problem with a `RawStr`/`RawByteStr` with a `None` field.\n #[inline]\n@@ -243,12 +241,8 @@ pub fn validate_raw_str(input: &str, prefix_len: u32) -> Result<(), RawStrError>\n pub fn tokenize(input: &str) -> impl Iterator<Item = Token> + '_ {\n     let mut cursor = Cursor::new(input);\n     std::iter::from_fn(move || {\n-        if cursor.is_eof() {\n-            None\n-        } else {\n-            cursor.reset_len_consumed();\n-            Some(cursor.advance_token())\n-        }\n+        let token = cursor.advance_token();\n+        if token.kind != TokenKind::Eof { Some(token) } else { None }\n     })\n }\n \n@@ -311,8 +305,11 @@ pub fn is_ident(string: &str) -> bool {\n \n impl Cursor<'_> {\n     /// Parses a token from the input string.\n-    fn advance_token(&mut self) -> Token {\n-        let first_char = self.bump().unwrap();\n+    pub fn advance_token(&mut self) -> Token {\n+        let first_char = match self.bump() {\n+            Some(c) => c,\n+            None => return Token::new(TokenKind::Eof, 0),\n+        };\n         let token_kind = match first_char {\n             // Slash, comment or block comment.\n             '/' => match self.first() {\n@@ -329,7 +326,7 @@ impl Cursor<'_> {\n                 ('#', c1) if is_id_start(c1) => self.raw_ident(),\n                 ('#', _) | ('\"', _) => {\n                     let res = self.raw_double_quoted_string(1);\n-                    let suffix_start = self.len_consumed();\n+                    let suffix_start = self.pos_within_token();\n                     if res.is_ok() {\n                         self.eat_literal_suffix();\n                     }\n@@ -344,7 +341,7 @@ impl Cursor<'_> {\n                 ('\\'', _) => {\n                     self.bump();\n                     let terminated = self.single_quoted_string();\n-                    let suffix_start = self.len_consumed();\n+                    let suffix_start = self.pos_within_token();\n                     if terminated {\n                         self.eat_literal_suffix();\n                     }\n@@ -354,7 +351,7 @@ impl Cursor<'_> {\n                 ('\"', _) => {\n                     self.bump();\n                     let terminated = self.double_quoted_string();\n-                    let suffix_start = self.len_consumed();\n+                    let suffix_start = self.pos_within_token();\n                     if terminated {\n                         self.eat_literal_suffix();\n                     }\n@@ -364,7 +361,7 @@ impl Cursor<'_> {\n                 ('r', '\"') | ('r', '#') => {\n                     self.bump();\n                     let res = self.raw_double_quoted_string(2);\n-                    let suffix_start = self.len_consumed();\n+                    let suffix_start = self.pos_within_token();\n                     if res.is_ok() {\n                         self.eat_literal_suffix();\n                     }\n@@ -381,7 +378,7 @@ impl Cursor<'_> {\n             // Numeric literal.\n             c @ '0'..='9' => {\n                 let literal_kind = self.number(c);\n-                let suffix_start = self.len_consumed();\n+                let suffix_start = self.pos_within_token();\n                 self.eat_literal_suffix();\n                 TokenKind::Literal { kind: literal_kind, suffix_start }\n             }\n@@ -420,7 +417,7 @@ impl Cursor<'_> {\n             // String literal.\n             '\"' => {\n                 let terminated = self.double_quoted_string();\n-                let suffix_start = self.len_consumed();\n+                let suffix_start = self.pos_within_token();\n                 if terminated {\n                     self.eat_literal_suffix();\n                 }\n@@ -433,7 +430,9 @@ impl Cursor<'_> {\n             }\n             _ => Unknown,\n         };\n-        Token::new(token_kind, self.len_consumed())\n+        let res = Token::new(token_kind, self.pos_within_token());\n+        self.reset_pos_within_token();\n+        res\n     }\n \n     fn line_comment(&mut self) -> TokenKind {\n@@ -618,7 +617,7 @@ impl Cursor<'_> {\n \n         if !can_be_a_lifetime {\n             let terminated = self.single_quoted_string();\n-            let suffix_start = self.len_consumed();\n+            let suffix_start = self.pos_within_token();\n             if terminated {\n                 self.eat_literal_suffix();\n             }\n@@ -643,7 +642,7 @@ impl Cursor<'_> {\n         if self.first() == '\\'' {\n             self.bump();\n             let kind = Char { terminated: true };\n-            Literal { kind, suffix_start: self.len_consumed() }\n+            Literal { kind, suffix_start: self.pos_within_token() }\n         } else {\n             Lifetime { starts_with_number }\n         }\n@@ -724,7 +723,7 @@ impl Cursor<'_> {\n \n     fn raw_string_unvalidated(&mut self, prefix_len: u32) -> Result<u32, RawStrError> {\n         debug_assert!(self.prev() == 'r');\n-        let start_pos = self.len_consumed();\n+        let start_pos = self.pos_within_token();\n         let mut possible_terminator_offset = None;\n         let mut max_hashes = 0;\n \n@@ -778,7 +777,7 @@ impl Cursor<'_> {\n                 // Keep track of possible terminators to give a hint about\n                 // where there might be a missing terminator\n                 possible_terminator_offset =\n-                    Some(self.len_consumed() - start_pos - n_end_hashes + prefix_len);\n+                    Some(self.pos_within_token() - start_pos - n_end_hashes + prefix_len);\n                 max_hashes = n_end_hashes;\n             }\n         }"}, {"sha": "bcd078a89677e9e4df7e996ee581acbab6ee36ba", "filename": "compiler/rustc_parse/src/lexer/mod.rs", "status": "modified", "additions": 195, "deletions": 195, "changes": 390, "blob_url": "https://github.com/rust-lang/rust/blob/6201eabde85db854c1ebb57624be5ec699246b50/compiler%2Frustc_parse%2Fsrc%2Flexer%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/6201eabde85db854c1ebb57624be5ec699246b50/compiler%2Frustc_parse%2Fsrc%2Flexer%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_parse%2Fsrc%2Flexer%2Fmod.rs?ref=6201eabde85db854c1ebb57624be5ec699246b50", "patch": "@@ -1,10 +1,11 @@\n use crate::lexer::unicode_chars::UNICODE_ARRAY;\n use rustc_ast::ast::{self, AttrStyle};\n use rustc_ast::token::{self, CommentKind, Delimiter, Token, TokenKind};\n-use rustc_ast::tokenstream::{Spacing, TokenStream};\n+use rustc_ast::tokenstream::TokenStream;\n use rustc_ast::util::unicode::contains_text_flow_control_chars;\n use rustc_errors::{error_code, Applicability, DiagnosticBuilder, ErrorGuaranteed, PResult};\n use rustc_lexer::unescape::{self, Mode};\n+use rustc_lexer::Cursor;\n use rustc_lexer::{Base, DocStyle, RawStrError};\n use rustc_session::lint::builtin::{\n     RUST_2021_PREFIXES_INCOMPATIBLE_SYNTAX, TEXT_DIRECTION_CODEPOINT_IN_COMMENT,\n@@ -38,11 +39,20 @@ pub struct UnmatchedBrace {\n \n pub(crate) fn parse_token_trees<'a>(\n     sess: &'a ParseSess,\n-    src: &'a str,\n-    start_pos: BytePos,\n+    mut src: &'a str,\n+    mut start_pos: BytePos,\n     override_span: Option<Span>,\n ) -> (PResult<'a, TokenStream>, Vec<UnmatchedBrace>) {\n-    StringReader { sess, start_pos, pos: start_pos, src, override_span }.into_token_trees()\n+    // Skip `#!`, if present.\n+    if let Some(shebang_len) = rustc_lexer::strip_shebang(src) {\n+        src = &src[shebang_len..];\n+        start_pos = start_pos + BytePos::from_usize(shebang_len);\n+    }\n+\n+    let cursor = Cursor::new(src);\n+    let string_reader =\n+        StringReader { sess, start_pos, pos: start_pos, src, cursor, override_span };\n+    tokentrees::TokenTreesReader::parse_token_trees(string_reader)\n }\n \n struct StringReader<'a> {\n@@ -53,6 +63,8 @@ struct StringReader<'a> {\n     pos: BytePos,\n     /// Source text to tokenize.\n     src: &'a str,\n+    /// Cursor for getting lexer tokens.\n+    cursor: Cursor<'a>,\n     override_span: Option<Span>,\n }\n \n@@ -61,42 +73,195 @@ impl<'a> StringReader<'a> {\n         self.override_span.unwrap_or_else(|| Span::with_root_ctxt(lo, hi))\n     }\n \n-    /// Returns the next token, and info about preceding whitespace, if any.\n-    fn next_token(&mut self) -> (Spacing, Token) {\n-        let mut spacing = Spacing::Joint;\n-\n-        // Skip `#!` at the start of the file\n-        if self.pos == self.start_pos\n-            && let Some(shebang_len) = rustc_lexer::strip_shebang(self.src)\n-        {\n-            self.pos = self.pos + BytePos::from_usize(shebang_len);\n-            spacing = Spacing::Alone;\n-        }\n+    /// Returns the next token, paired with a bool indicating if the token was\n+    /// preceded by whitespace.\n+    fn next_token(&mut self) -> (Token, bool) {\n+        let mut preceded_by_whitespace = false;\n \n         // Skip trivial (whitespace & comments) tokens\n         loop {\n-            let start_src_index = self.src_index(self.pos);\n-            let text: &str = &self.src[start_src_index..];\n-\n-            if text.is_empty() {\n-                let span = self.mk_sp(self.pos, self.pos);\n-                return (spacing, Token::new(token::Eof, span));\n-            }\n-\n-            let token = rustc_lexer::first_token(text);\n-\n+            let token = self.cursor.advance_token();\n             let start = self.pos;\n             self.pos = self.pos + BytePos(token.len);\n \n             debug!(\"next_token: {:?}({:?})\", token.kind, self.str_from(start));\n \n-            match self.cook_lexer_token(token.kind, start) {\n-                Some(kind) => {\n+            // Now \"cook\" the token, converting the simple `rustc_lexer::TokenKind` enum into a\n+            // rich `rustc_ast::TokenKind`. This turns strings into interned symbols and runs\n+            // additional validation.\n+            let kind = match token.kind {\n+                rustc_lexer::TokenKind::LineComment { doc_style } => {\n+                    // Skip non-doc comments\n+                    let Some(doc_style) = doc_style else {\n+                        self.lint_unicode_text_flow(start);\n+                        preceded_by_whitespace = true;\n+                        continue;\n+                    };\n+\n+                    // Opening delimiter of the length 3 is not included into the symbol.\n+                    let content_start = start + BytePos(3);\n+                    let content = self.str_from(content_start);\n+                    self.cook_doc_comment(content_start, content, CommentKind::Line, doc_style)\n+                }\n+                rustc_lexer::TokenKind::BlockComment { doc_style, terminated } => {\n+                    if !terminated {\n+                        self.report_unterminated_block_comment(start, doc_style);\n+                    }\n+\n+                    // Skip non-doc comments\n+                    let Some(doc_style) = doc_style else {\n+                        self.lint_unicode_text_flow(start);\n+                        preceded_by_whitespace = true;\n+                        continue;\n+                    };\n+\n+                    // Opening delimiter of the length 3 and closing delimiter of the length 2\n+                    // are not included into the symbol.\n+                    let content_start = start + BytePos(3);\n+                    let content_end = self.pos - BytePos(if terminated { 2 } else { 0 });\n+                    let content = self.str_from_to(content_start, content_end);\n+                    self.cook_doc_comment(content_start, content, CommentKind::Block, doc_style)\n+                }\n+                rustc_lexer::TokenKind::Whitespace => {\n+                    preceded_by_whitespace = true;\n+                    continue;\n+                }\n+                rustc_lexer::TokenKind::Ident => {\n+                    let sym = nfc_normalize(self.str_from(start));\n                     let span = self.mk_sp(start, self.pos);\n-                    return (spacing, Token::new(kind, span));\n+                    self.sess.symbol_gallery.insert(sym, span);\n+                    token::Ident(sym, false)\n                 }\n-                None => spacing = Spacing::Alone,\n-            }\n+                rustc_lexer::TokenKind::RawIdent => {\n+                    let sym = nfc_normalize(self.str_from(start + BytePos(2)));\n+                    let span = self.mk_sp(start, self.pos);\n+                    self.sess.symbol_gallery.insert(sym, span);\n+                    if !sym.can_be_raw() {\n+                        self.err_span(span, &format!(\"`{}` cannot be a raw identifier\", sym));\n+                    }\n+                    self.sess.raw_identifier_spans.borrow_mut().push(span);\n+                    token::Ident(sym, true)\n+                }\n+                rustc_lexer::TokenKind::UnknownPrefix => {\n+                    self.report_unknown_prefix(start);\n+                    let sym = nfc_normalize(self.str_from(start));\n+                    let span = self.mk_sp(start, self.pos);\n+                    self.sess.symbol_gallery.insert(sym, span);\n+                    token::Ident(sym, false)\n+                }\n+                rustc_lexer::TokenKind::InvalidIdent\n+                    // Do not recover an identifier with emoji if the codepoint is a confusable\n+                    // with a recoverable substitution token, like `\u2796`.\n+                    if !UNICODE_ARRAY\n+                        .iter()\n+                        .any(|&(c, _, _)| {\n+                            let sym = self.str_from(start);\n+                            sym.chars().count() == 1 && c == sym.chars().next().unwrap()\n+                        }) =>\n+                {\n+                    let sym = nfc_normalize(self.str_from(start));\n+                    let span = self.mk_sp(start, self.pos);\n+                    self.sess.bad_unicode_identifiers.borrow_mut().entry(sym).or_default()\n+                        .push(span);\n+                    token::Ident(sym, false)\n+                }\n+                rustc_lexer::TokenKind::Literal { kind, suffix_start } => {\n+                    let suffix_start = start + BytePos(suffix_start);\n+                    let (kind, symbol) = self.cook_lexer_literal(start, suffix_start, kind);\n+                    let suffix = if suffix_start < self.pos {\n+                        let string = self.str_from(suffix_start);\n+                        if string == \"_\" {\n+                            self.sess\n+                                .span_diagnostic\n+                                .struct_span_warn(\n+                                    self.mk_sp(suffix_start, self.pos),\n+                                    \"underscore literal suffix is not allowed\",\n+                                )\n+                                .warn(\n+                                    \"this was previously accepted by the compiler but is \\\n+                                       being phased out; it will become a hard error in \\\n+                                       a future release!\",\n+                                )\n+                                .note(\n+                                    \"see issue #42326 \\\n+                                     <https://github.com/rust-lang/rust/issues/42326> \\\n+                                     for more information\",\n+                                )\n+                                .emit();\n+                            None\n+                        } else {\n+                            Some(Symbol::intern(string))\n+                        }\n+                    } else {\n+                        None\n+                    };\n+                    token::Literal(token::Lit { kind, symbol, suffix })\n+                }\n+                rustc_lexer::TokenKind::Lifetime { starts_with_number } => {\n+                    // Include the leading `'` in the real identifier, for macro\n+                    // expansion purposes. See #12512 for the gory details of why\n+                    // this is necessary.\n+                    let lifetime_name = self.str_from(start);\n+                    if starts_with_number {\n+                        self.err_span_(start, self.pos, \"lifetimes cannot start with a number\");\n+                    }\n+                    let ident = Symbol::intern(lifetime_name);\n+                    token::Lifetime(ident)\n+                }\n+                rustc_lexer::TokenKind::Semi => token::Semi,\n+                rustc_lexer::TokenKind::Comma => token::Comma,\n+                rustc_lexer::TokenKind::Dot => token::Dot,\n+                rustc_lexer::TokenKind::OpenParen => token::OpenDelim(Delimiter::Parenthesis),\n+                rustc_lexer::TokenKind::CloseParen => token::CloseDelim(Delimiter::Parenthesis),\n+                rustc_lexer::TokenKind::OpenBrace => token::OpenDelim(Delimiter::Brace),\n+                rustc_lexer::TokenKind::CloseBrace => token::CloseDelim(Delimiter::Brace),\n+                rustc_lexer::TokenKind::OpenBracket => token::OpenDelim(Delimiter::Bracket),\n+                rustc_lexer::TokenKind::CloseBracket => token::CloseDelim(Delimiter::Bracket),\n+                rustc_lexer::TokenKind::At => token::At,\n+                rustc_lexer::TokenKind::Pound => token::Pound,\n+                rustc_lexer::TokenKind::Tilde => token::Tilde,\n+                rustc_lexer::TokenKind::Question => token::Question,\n+                rustc_lexer::TokenKind::Colon => token::Colon,\n+                rustc_lexer::TokenKind::Dollar => token::Dollar,\n+                rustc_lexer::TokenKind::Eq => token::Eq,\n+                rustc_lexer::TokenKind::Bang => token::Not,\n+                rustc_lexer::TokenKind::Lt => token::Lt,\n+                rustc_lexer::TokenKind::Gt => token::Gt,\n+                rustc_lexer::TokenKind::Minus => token::BinOp(token::Minus),\n+                rustc_lexer::TokenKind::And => token::BinOp(token::And),\n+                rustc_lexer::TokenKind::Or => token::BinOp(token::Or),\n+                rustc_lexer::TokenKind::Plus => token::BinOp(token::Plus),\n+                rustc_lexer::TokenKind::Star => token::BinOp(token::Star),\n+                rustc_lexer::TokenKind::Slash => token::BinOp(token::Slash),\n+                rustc_lexer::TokenKind::Caret => token::BinOp(token::Caret),\n+                rustc_lexer::TokenKind::Percent => token::BinOp(token::Percent),\n+\n+                rustc_lexer::TokenKind::Unknown | rustc_lexer::TokenKind::InvalidIdent => {\n+                    let c = self.str_from(start).chars().next().unwrap();\n+                    let mut err =\n+                        self.struct_err_span_char(start, self.pos, \"unknown start of token\", c);\n+                    // FIXME: the lexer could be used to turn the ASCII version of unicode\n+                    // homoglyphs, instead of keeping a table in `check_for_substitution`into the\n+                    // token. Ideally, this should be inside `rustc_lexer`. However, we should\n+                    // first remove compound tokens like `<<` from `rustc_lexer`, and then add\n+                    // fancier error recovery to it, as there will be less overall work to do this\n+                    // way.\n+                    let token = unicode_chars::check_for_substitution(self, start, c, &mut err);\n+                    if c == '\\x00' {\n+                        err.help(\"source files must contain UTF-8 encoded text, unexpected null bytes might occur when a different encoding is used\");\n+                    }\n+                    err.emit();\n+                    if let Some(token) = token {\n+                        token\n+                    } else {\n+                        preceded_by_whitespace = true;\n+                        continue;\n+                    }\n+                }\n+                rustc_lexer::TokenKind::Eof => token::Eof,\n+            };\n+            let span = self.mk_sp(start, self.pos);\n+            return (Token::new(kind, span), preceded_by_whitespace);\n         }\n     }\n \n@@ -162,171 +327,6 @@ impl<'a> StringReader<'a> {\n         }\n     }\n \n-    /// Turns simple `rustc_lexer::TokenKind` enum into a rich\n-    /// `rustc_ast::TokenKind`. This turns strings into interned\n-    /// symbols and runs additional validation.\n-    fn cook_lexer_token(&self, token: rustc_lexer::TokenKind, start: BytePos) -> Option<TokenKind> {\n-        Some(match token {\n-            rustc_lexer::TokenKind::LineComment { doc_style } => {\n-                // Skip non-doc comments\n-                let Some(doc_style) = doc_style else {\n-                    self.lint_unicode_text_flow(start);\n-                    return None;\n-                };\n-\n-                // Opening delimiter of the length 3 is not included into the symbol.\n-                let content_start = start + BytePos(3);\n-                let content = self.str_from(content_start);\n-                self.cook_doc_comment(content_start, content, CommentKind::Line, doc_style)\n-            }\n-            rustc_lexer::TokenKind::BlockComment { doc_style, terminated } => {\n-                if !terminated {\n-                    self.report_unterminated_block_comment(start, doc_style);\n-                }\n-\n-                // Skip non-doc comments\n-                let Some(doc_style) = doc_style else {\n-                    self.lint_unicode_text_flow(start);\n-                    return None;\n-                };\n-\n-                // Opening delimiter of the length 3 and closing delimiter of the length 2\n-                // are not included into the symbol.\n-                let content_start = start + BytePos(3);\n-                let content_end = self.pos - BytePos(if terminated { 2 } else { 0 });\n-                let content = self.str_from_to(content_start, content_end);\n-                self.cook_doc_comment(content_start, content, CommentKind::Block, doc_style)\n-            }\n-            rustc_lexer::TokenKind::Whitespace => return None,\n-            rustc_lexer::TokenKind::Ident\n-            | rustc_lexer::TokenKind::RawIdent\n-            | rustc_lexer::TokenKind::UnknownPrefix => {\n-                let is_raw_ident = token == rustc_lexer::TokenKind::RawIdent;\n-                let is_unknown_prefix = token == rustc_lexer::TokenKind::UnknownPrefix;\n-                let mut ident_start = start;\n-                if is_raw_ident {\n-                    ident_start = ident_start + BytePos(2);\n-                }\n-                if is_unknown_prefix {\n-                    self.report_unknown_prefix(start);\n-                }\n-                let sym = nfc_normalize(self.str_from(ident_start));\n-                let span = self.mk_sp(start, self.pos);\n-                self.sess.symbol_gallery.insert(sym, span);\n-                if is_raw_ident {\n-                    if !sym.can_be_raw() {\n-                        self.err_span(span, &format!(\"`{}` cannot be a raw identifier\", sym));\n-                    }\n-                    self.sess.raw_identifier_spans.borrow_mut().push(span);\n-                }\n-                token::Ident(sym, is_raw_ident)\n-            }\n-            rustc_lexer::TokenKind::InvalidIdent\n-                // Do not recover an identifier with emoji if the codepoint is a confusable\n-                // with a recoverable substitution token, like `\u2796`.\n-                if !UNICODE_ARRAY\n-                    .iter()\n-                    .any(|&(c, _, _)| {\n-                        let sym = self.str_from(start);\n-                        sym.chars().count() == 1 && c == sym.chars().next().unwrap()\n-                    })\n-                     =>\n-            {\n-                let sym = nfc_normalize(self.str_from(start));\n-                let span = self.mk_sp(start, self.pos);\n-                self.sess.bad_unicode_identifiers.borrow_mut().entry(sym).or_default().push(span);\n-                token::Ident(sym, false)\n-            }\n-            rustc_lexer::TokenKind::Literal { kind, suffix_start } => {\n-                let suffix_start = start + BytePos(suffix_start);\n-                let (kind, symbol) = self.cook_lexer_literal(start, suffix_start, kind);\n-                let suffix = if suffix_start < self.pos {\n-                    let string = self.str_from(suffix_start);\n-                    if string == \"_\" {\n-                        self.sess\n-                            .span_diagnostic\n-                            .struct_span_warn(\n-                                self.mk_sp(suffix_start, self.pos),\n-                                \"underscore literal suffix is not allowed\",\n-                            )\n-                            .warn(\n-                                \"this was previously accepted by the compiler but is \\\n-                                   being phased out; it will become a hard error in \\\n-                                   a future release!\",\n-                            )\n-                            .note(\n-                                \"see issue #42326 \\\n-                                 <https://github.com/rust-lang/rust/issues/42326> \\\n-                                 for more information\",\n-                            )\n-                            .emit();\n-                        None\n-                    } else {\n-                        Some(Symbol::intern(string))\n-                    }\n-                } else {\n-                    None\n-                };\n-                token::Literal(token::Lit { kind, symbol, suffix })\n-            }\n-            rustc_lexer::TokenKind::Lifetime { starts_with_number } => {\n-                // Include the leading `'` in the real identifier, for macro\n-                // expansion purposes. See #12512 for the gory details of why\n-                // this is necessary.\n-                let lifetime_name = self.str_from(start);\n-                if starts_with_number {\n-                    self.err_span_(start, self.pos, \"lifetimes cannot start with a number\");\n-                }\n-                let ident = Symbol::intern(lifetime_name);\n-                token::Lifetime(ident)\n-            }\n-            rustc_lexer::TokenKind::Semi => token::Semi,\n-            rustc_lexer::TokenKind::Comma => token::Comma,\n-            rustc_lexer::TokenKind::Dot => token::Dot,\n-            rustc_lexer::TokenKind::OpenParen => token::OpenDelim(Delimiter::Parenthesis),\n-            rustc_lexer::TokenKind::CloseParen => token::CloseDelim(Delimiter::Parenthesis),\n-            rustc_lexer::TokenKind::OpenBrace => token::OpenDelim(Delimiter::Brace),\n-            rustc_lexer::TokenKind::CloseBrace => token::CloseDelim(Delimiter::Brace),\n-            rustc_lexer::TokenKind::OpenBracket => token::OpenDelim(Delimiter::Bracket),\n-            rustc_lexer::TokenKind::CloseBracket => token::CloseDelim(Delimiter::Bracket),\n-            rustc_lexer::TokenKind::At => token::At,\n-            rustc_lexer::TokenKind::Pound => token::Pound,\n-            rustc_lexer::TokenKind::Tilde => token::Tilde,\n-            rustc_lexer::TokenKind::Question => token::Question,\n-            rustc_lexer::TokenKind::Colon => token::Colon,\n-            rustc_lexer::TokenKind::Dollar => token::Dollar,\n-            rustc_lexer::TokenKind::Eq => token::Eq,\n-            rustc_lexer::TokenKind::Bang => token::Not,\n-            rustc_lexer::TokenKind::Lt => token::Lt,\n-            rustc_lexer::TokenKind::Gt => token::Gt,\n-            rustc_lexer::TokenKind::Minus => token::BinOp(token::Minus),\n-            rustc_lexer::TokenKind::And => token::BinOp(token::And),\n-            rustc_lexer::TokenKind::Or => token::BinOp(token::Or),\n-            rustc_lexer::TokenKind::Plus => token::BinOp(token::Plus),\n-            rustc_lexer::TokenKind::Star => token::BinOp(token::Star),\n-            rustc_lexer::TokenKind::Slash => token::BinOp(token::Slash),\n-            rustc_lexer::TokenKind::Caret => token::BinOp(token::Caret),\n-            rustc_lexer::TokenKind::Percent => token::BinOp(token::Percent),\n-\n-            rustc_lexer::TokenKind::Unknown | rustc_lexer::TokenKind::InvalidIdent => {\n-                let c = self.str_from(start).chars().next().unwrap();\n-                let mut err =\n-                    self.struct_err_span_char(start, self.pos, \"unknown start of token\", c);\n-                // FIXME: the lexer could be used to turn the ASCII version of unicode homoglyphs,\n-                // instead of keeping a table in `check_for_substitution`into the token. Ideally,\n-                // this should be inside `rustc_lexer`. However, we should first remove compound\n-                // tokens like `<<` from `rustc_lexer`, and then add fancier error recovery to it,\n-                // as there will be less overall work to do this way.\n-                let token = unicode_chars::check_for_substitution(self, start, c, &mut err);\n-                if c == '\\x00' {\n-                    err.help(\"source files must contain UTF-8 encoded text, unexpected null bytes might occur when a different encoding is used\");\n-                }\n-                err.emit();\n-                token?\n-            }\n-        })\n-    }\n-\n     fn cook_doc_comment(\n         &self,\n         content_start: BytePos,"}, {"sha": "364753154db0555e379d71ef30643a05e6946c45", "filename": "compiler/rustc_parse/src/lexer/tokentrees.rs", "status": "modified", "additions": 191, "deletions": 203, "changes": 394, "blob_url": "https://github.com/rust-lang/rust/blob/6201eabde85db854c1ebb57624be5ec699246b50/compiler%2Frustc_parse%2Fsrc%2Flexer%2Ftokentrees.rs", "raw_url": "https://github.com/rust-lang/rust/raw/6201eabde85db854c1ebb57624be5ec699246b50/compiler%2Frustc_parse%2Fsrc%2Flexer%2Ftokentrees.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_parse%2Fsrc%2Flexer%2Ftokentrees.rs?ref=6201eabde85db854c1ebb57624be5ec699246b50", "patch": "@@ -1,31 +1,15 @@\n use super::{StringReader, UnmatchedBrace};\n-\n use rustc_ast::token::{self, Delimiter, Token};\n use rustc_ast::tokenstream::{DelimSpan, Spacing, TokenStream, TokenTree};\n use rustc_ast_pretty::pprust::token_to_string;\n use rustc_data_structures::fx::FxHashMap;\n-use rustc_errors::PResult;\n+use rustc_errors::{PErr, PResult};\n use rustc_span::Span;\n \n-impl<'a> StringReader<'a> {\n-    pub(super) fn into_token_trees(self) -> (PResult<'a, TokenStream>, Vec<UnmatchedBrace>) {\n-        let mut tt_reader = TokenTreesReader {\n-            string_reader: self,\n-            token: Token::dummy(),\n-            open_braces: Vec::new(),\n-            unmatched_braces: Vec::new(),\n-            matching_delim_spans: Vec::new(),\n-            last_unclosed_found_span: None,\n-            last_delim_empty_block_spans: FxHashMap::default(),\n-            matching_block_spans: Vec::new(),\n-        };\n-        let res = tt_reader.parse_all_token_trees();\n-        (res, tt_reader.unmatched_braces)\n-    }\n-}\n-\n-struct TokenTreesReader<'a> {\n+pub(super) struct TokenTreesReader<'a> {\n     string_reader: StringReader<'a>,\n+    /// The \"next\" token, which has been obtained from the `StringReader` but\n+    /// not yet handled by the `TokenTreesReader`.\n     token: Token,\n     /// Stack of open delimiters and their spans. Used for error message.\n     open_braces: Vec<(Delimiter, Span)>,\n@@ -43,231 +27,235 @@ struct TokenTreesReader<'a> {\n }\n \n impl<'a> TokenTreesReader<'a> {\n+    pub(super) fn parse_token_trees(\n+        string_reader: StringReader<'a>,\n+    ) -> (PResult<'a, TokenStream>, Vec<UnmatchedBrace>) {\n+        let mut tt_reader = TokenTreesReader {\n+            string_reader,\n+            token: Token::dummy(),\n+            open_braces: Vec::new(),\n+            unmatched_braces: Vec::new(),\n+            matching_delim_spans: Vec::new(),\n+            last_unclosed_found_span: None,\n+            last_delim_empty_block_spans: FxHashMap::default(),\n+            matching_block_spans: Vec::new(),\n+        };\n+        let res = tt_reader.parse_all_token_trees();\n+        (res, tt_reader.unmatched_braces)\n+    }\n+\n     // Parse a stream of tokens into a list of `TokenTree`s, up to an `Eof`.\n     fn parse_all_token_trees(&mut self) -> PResult<'a, TokenStream> {\n+        self.token = self.string_reader.next_token().0;\n         let mut buf = TokenStreamBuilder::default();\n-\n-        self.bump();\n-        while self.token != token::Eof {\n-            buf.push(self.parse_token_tree()?);\n+        loop {\n+            match self.token.kind {\n+                token::OpenDelim(delim) => buf.push(self.parse_token_tree_open_delim(delim)),\n+                token::CloseDelim(delim) => return Err(self.close_delim_err(delim)),\n+                token::Eof => return Ok(buf.into_token_stream()),\n+                _ => buf.push(self.parse_token_tree_non_delim_non_eof()),\n+            }\n         }\n-\n-        Ok(buf.into_token_stream())\n     }\n \n     // Parse a stream of tokens into a list of `TokenTree`s, up to a `CloseDelim`.\n     fn parse_token_trees_until_close_delim(&mut self) -> TokenStream {\n         let mut buf = TokenStreamBuilder::default();\n         loop {\n-            if let token::CloseDelim(..) = self.token.kind {\n-                return buf.into_token_stream();\n-            }\n-\n-            match self.parse_token_tree() {\n-                Ok(tree) => buf.push(tree),\n-                Err(mut e) => {\n-                    e.emit();\n+            match self.token.kind {\n+                token::OpenDelim(delim) => buf.push(self.parse_token_tree_open_delim(delim)),\n+                token::CloseDelim(..) => return buf.into_token_stream(),\n+                token::Eof => {\n+                    self.eof_err().emit();\n                     return buf.into_token_stream();\n                 }\n+                _ => buf.push(self.parse_token_tree_non_delim_non_eof()),\n             }\n         }\n     }\n \n-    fn parse_token_tree(&mut self) -> PResult<'a, TokenTree> {\n-        let sm = self.string_reader.sess.source_map();\n-\n-        match self.token.kind {\n-            token::Eof => {\n-                let msg = \"this file contains an unclosed delimiter\";\n-                let mut err =\n-                    self.string_reader.sess.span_diagnostic.struct_span_err(self.token.span, msg);\n-                for &(_, sp) in &self.open_braces {\n-                    err.span_label(sp, \"unclosed delimiter\");\n-                    self.unmatched_braces.push(UnmatchedBrace {\n-                        expected_delim: Delimiter::Brace,\n-                        found_delim: None,\n-                        found_span: self.token.span,\n-                        unclosed_span: Some(sp),\n-                        candidate_span: None,\n-                    });\n-                }\n+    fn eof_err(&mut self) -> PErr<'a> {\n+        let msg = \"this file contains an unclosed delimiter\";\n+        let mut err = self.string_reader.sess.span_diagnostic.struct_span_err(self.token.span, msg);\n+        for &(_, sp) in &self.open_braces {\n+            err.span_label(sp, \"unclosed delimiter\");\n+            self.unmatched_braces.push(UnmatchedBrace {\n+                expected_delim: Delimiter::Brace,\n+                found_delim: None,\n+                found_span: self.token.span,\n+                unclosed_span: Some(sp),\n+                candidate_span: None,\n+            });\n+        }\n \n-                if let Some((delim, _)) = self.open_braces.last() {\n-                    if let Some((_, open_sp, close_sp)) =\n-                        self.matching_delim_spans.iter().find(|(d, open_sp, close_sp)| {\n-                            if let Some(close_padding) = sm.span_to_margin(*close_sp) {\n-                                if let Some(open_padding) = sm.span_to_margin(*open_sp) {\n-                                    return delim == d && close_padding != open_padding;\n-                                }\n-                            }\n-                            false\n-                        })\n-                    // these are in reverse order as they get inserted on close, but\n-                    {\n-                        // we want the last open/first close\n-                        err.span_label(*open_sp, \"this delimiter might not be properly closed...\");\n-                        err.span_label(\n-                            *close_sp,\n-                            \"...as it matches this but it has different indentation\",\n-                        );\n+        if let Some((delim, _)) = self.open_braces.last() {\n+            if let Some((_, open_sp, close_sp)) =\n+                self.matching_delim_spans.iter().find(|(d, open_sp, close_sp)| {\n+                    let sm = self.string_reader.sess.source_map();\n+                    if let Some(close_padding) = sm.span_to_margin(*close_sp) {\n+                        if let Some(open_padding) = sm.span_to_margin(*open_sp) {\n+                            return delim == d && close_padding != open_padding;\n+                        }\n                     }\n-                }\n-                Err(err)\n+                    false\n+                })\n+            // these are in reverse order as they get inserted on close, but\n+            {\n+                // we want the last open/first close\n+                err.span_label(*open_sp, \"this delimiter might not be properly closed...\");\n+                err.span_label(*close_sp, \"...as it matches this but it has different indentation\");\n             }\n-            token::OpenDelim(delim) => {\n-                // The span for beginning of the delimited section\n-                let pre_span = self.token.span;\n-\n-                // Parse the open delimiter.\n-                self.open_braces.push((delim, self.token.span));\n-                self.bump();\n+        }\n+        err\n+    }\n \n-                // Parse the token trees within the delimiters.\n-                // We stop at any delimiter so we can try to recover if the user\n-                // uses an incorrect delimiter.\n-                let tts = self.parse_token_trees_until_close_delim();\n+    fn parse_token_tree_open_delim(&mut self, open_delim: Delimiter) -> TokenTree {\n+        // The span for beginning of the delimited section\n+        let pre_span = self.token.span;\n \n-                // Expand to cover the entire delimited token tree\n-                let delim_span = DelimSpan::from_pair(pre_span, self.token.span);\n+        // Move past the open delimiter.\n+        self.open_braces.push((open_delim, self.token.span));\n+        self.token = self.string_reader.next_token().0;\n \n-                match self.token.kind {\n-                    // Correct delimiter.\n-                    token::CloseDelim(d) if d == delim => {\n-                        let (open_brace, open_brace_span) = self.open_braces.pop().unwrap();\n-                        let close_brace_span = self.token.span;\n+        // Parse the token trees within the delimiters.\n+        // We stop at any delimiter so we can try to recover if the user\n+        // uses an incorrect delimiter.\n+        let tts = self.parse_token_trees_until_close_delim();\n \n-                        if tts.is_empty() {\n-                            let empty_block_span = open_brace_span.to(close_brace_span);\n-                            if !sm.is_multiline(empty_block_span) {\n-                                // Only track if the block is in the form of `{}`, otherwise it is\n-                                // likely that it was written on purpose.\n-                                self.last_delim_empty_block_spans.insert(delim, empty_block_span);\n-                            }\n-                        }\n+        // Expand to cover the entire delimited token tree\n+        let delim_span = DelimSpan::from_pair(pre_span, self.token.span);\n \n-                        //only add braces\n-                        if let (Delimiter::Brace, Delimiter::Brace) = (open_brace, delim) {\n-                            self.matching_block_spans.push((open_brace_span, close_brace_span));\n-                        }\n+        match self.token.kind {\n+            // Correct delimiter.\n+            token::CloseDelim(close_delim) if close_delim == open_delim => {\n+                let (open_brace, open_brace_span) = self.open_braces.pop().unwrap();\n+                let close_brace_span = self.token.span;\n \n-                        if self.open_braces.is_empty() {\n-                            // Clear up these spans to avoid suggesting them as we've found\n-                            // properly matched delimiters so far for an entire block.\n-                            self.matching_delim_spans.clear();\n-                        } else {\n-                            self.matching_delim_spans.push((\n-                                open_brace,\n-                                open_brace_span,\n-                                close_brace_span,\n-                            ));\n-                        }\n-                        // Parse the closing delimiter.\n-                        self.bump();\n+                if tts.is_empty() {\n+                    let empty_block_span = open_brace_span.to(close_brace_span);\n+                    let sm = self.string_reader.sess.source_map();\n+                    if !sm.is_multiline(empty_block_span) {\n+                        // Only track if the block is in the form of `{}`, otherwise it is\n+                        // likely that it was written on purpose.\n+                        self.last_delim_empty_block_spans.insert(open_delim, empty_block_span);\n                     }\n-                    // Incorrect delimiter.\n-                    token::CloseDelim(other) => {\n-                        let mut unclosed_delimiter = None;\n-                        let mut candidate = None;\n+                }\n+\n+                //only add braces\n+                if let (Delimiter::Brace, Delimiter::Brace) = (open_brace, open_delim) {\n+                    self.matching_block_spans.push((open_brace_span, close_brace_span));\n+                }\n+\n+                if self.open_braces.is_empty() {\n+                    // Clear up these spans to avoid suggesting them as we've found\n+                    // properly matched delimiters so far for an entire block.\n+                    self.matching_delim_spans.clear();\n+                } else {\n+                    self.matching_delim_spans.push((open_brace, open_brace_span, close_brace_span));\n+                }\n+                // Move past the closing delimiter.\n+                self.token = self.string_reader.next_token().0;\n+            }\n+            // Incorrect delimiter.\n+            token::CloseDelim(close_delim) => {\n+                let mut unclosed_delimiter = None;\n+                let mut candidate = None;\n \n-                        if self.last_unclosed_found_span != Some(self.token.span) {\n-                            // do not complain about the same unclosed delimiter multiple times\n-                            self.last_unclosed_found_span = Some(self.token.span);\n-                            // This is a conservative error: only report the last unclosed\n-                            // delimiter. The previous unclosed delimiters could actually be\n-                            // closed! The parser just hasn't gotten to them yet.\n-                            if let Some(&(_, sp)) = self.open_braces.last() {\n-                                unclosed_delimiter = Some(sp);\n-                            };\n-                            if let Some(current_padding) = sm.span_to_margin(self.token.span) {\n-                                for (brace, brace_span) in &self.open_braces {\n-                                    if let Some(padding) = sm.span_to_margin(*brace_span) {\n-                                        // high likelihood of these two corresponding\n-                                        if current_padding == padding && brace == &other {\n-                                            candidate = Some(*brace_span);\n-                                        }\n-                                    }\n+                if self.last_unclosed_found_span != Some(self.token.span) {\n+                    // do not complain about the same unclosed delimiter multiple times\n+                    self.last_unclosed_found_span = Some(self.token.span);\n+                    // This is a conservative error: only report the last unclosed\n+                    // delimiter. The previous unclosed delimiters could actually be\n+                    // closed! The parser just hasn't gotten to them yet.\n+                    if let Some(&(_, sp)) = self.open_braces.last() {\n+                        unclosed_delimiter = Some(sp);\n+                    };\n+                    let sm = self.string_reader.sess.source_map();\n+                    if let Some(current_padding) = sm.span_to_margin(self.token.span) {\n+                        for (brace, brace_span) in &self.open_braces {\n+                            if let Some(padding) = sm.span_to_margin(*brace_span) {\n+                                // high likelihood of these two corresponding\n+                                if current_padding == padding && brace == &close_delim {\n+                                    candidate = Some(*brace_span);\n                                 }\n                             }\n-                            let (tok, _) = self.open_braces.pop().unwrap();\n-                            self.unmatched_braces.push(UnmatchedBrace {\n-                                expected_delim: tok,\n-                                found_delim: Some(other),\n-                                found_span: self.token.span,\n-                                unclosed_span: unclosed_delimiter,\n-                                candidate_span: candidate,\n-                            });\n-                        } else {\n-                            self.open_braces.pop();\n                         }\n-\n-                        // If the incorrect delimiter matches an earlier opening\n-                        // delimiter, then don't consume it (it can be used to\n-                        // close the earlier one). Otherwise, consume it.\n-                        // E.g., we try to recover from:\n-                        // fn foo() {\n-                        //     bar(baz(\n-                        // }  // Incorrect delimiter but matches the earlier `{`\n-                        if !self.open_braces.iter().any(|&(b, _)| b == other) {\n-                            self.bump();\n-                        }\n-                    }\n-                    token::Eof => {\n-                        // Silently recover, the EOF token will be seen again\n-                        // and an error emitted then. Thus we don't pop from\n-                        // self.open_braces here.\n                     }\n-                    _ => {}\n+                    let (tok, _) = self.open_braces.pop().unwrap();\n+                    self.unmatched_braces.push(UnmatchedBrace {\n+                        expected_delim: tok,\n+                        found_delim: Some(close_delim),\n+                        found_span: self.token.span,\n+                        unclosed_span: unclosed_delimiter,\n+                        candidate_span: candidate,\n+                    });\n+                } else {\n+                    self.open_braces.pop();\n                 }\n \n-                Ok(TokenTree::Delimited(delim_span, delim, tts))\n+                // If the incorrect delimiter matches an earlier opening\n+                // delimiter, then don't consume it (it can be used to\n+                // close the earlier one). Otherwise, consume it.\n+                // E.g., we try to recover from:\n+                // fn foo() {\n+                //     bar(baz(\n+                // }  // Incorrect delimiter but matches the earlier `{`\n+                if !self.open_braces.iter().any(|&(b, _)| b == close_delim) {\n+                    self.token = self.string_reader.next_token().0;\n+                }\n             }\n-            token::CloseDelim(delim) => {\n-                // An unexpected closing delimiter (i.e., there is no\n-                // matching opening delimiter).\n-                let token_str = token_to_string(&self.token);\n-                let msg = format!(\"unexpected closing delimiter: `{}`\", token_str);\n-                let mut err =\n-                    self.string_reader.sess.span_diagnostic.struct_span_err(self.token.span, &msg);\n-\n-                // Braces are added at the end, so the last element is the biggest block\n-                if let Some(parent) = self.matching_block_spans.last() {\n-                    if let Some(span) = self.last_delim_empty_block_spans.remove(&delim) {\n-                        // Check if the (empty block) is in the last properly closed block\n-                        if (parent.0.to(parent.1)).contains(span) {\n-                            err.span_label(\n-                                span,\n-                                \"block is empty, you might have not meant to close it\",\n-                            );\n-                        } else {\n-                            err.span_label(parent.0, \"this opening brace...\");\n+            token::Eof => {\n+                // Silently recover, the EOF token will be seen again\n+                // and an error emitted then. Thus we don't pop from\n+                // self.open_braces here.\n+            }\n+            _ => unreachable!(),\n+        }\n \n-                            err.span_label(parent.1, \"...matches this closing brace\");\n-                        }\n-                    } else {\n-                        err.span_label(parent.0, \"this opening brace...\");\n+        TokenTree::Delimited(delim_span, open_delim, tts)\n+    }\n \n-                        err.span_label(parent.1, \"...matches this closing brace\");\n-                    }\n-                }\n+    fn close_delim_err(&mut self, delim: Delimiter) -> PErr<'a> {\n+        // An unexpected closing delimiter (i.e., there is no\n+        // matching opening delimiter).\n+        let token_str = token_to_string(&self.token);\n+        let msg = format!(\"unexpected closing delimiter: `{}`\", token_str);\n+        let mut err =\n+            self.string_reader.sess.span_diagnostic.struct_span_err(self.token.span, &msg);\n \n-                err.span_label(self.token.span, \"unexpected closing delimiter\");\n-                Err(err)\n-            }\n-            _ => {\n-                let tok = self.token.take();\n-                let mut spacing = self.bump();\n-                if !self.token.is_op() {\n-                    spacing = Spacing::Alone;\n+        // Braces are added at the end, so the last element is the biggest block\n+        if let Some(parent) = self.matching_block_spans.last() {\n+            if let Some(span) = self.last_delim_empty_block_spans.remove(&delim) {\n+                // Check if the (empty block) is in the last properly closed block\n+                if (parent.0.to(parent.1)).contains(span) {\n+                    err.span_label(span, \"block is empty, you might have not meant to close it\");\n+                } else {\n+                    err.span_label(parent.0, \"this opening brace...\");\n+                    err.span_label(parent.1, \"...matches this closing brace\");\n                 }\n-                Ok(TokenTree::Token(tok, spacing))\n+            } else {\n+                err.span_label(parent.0, \"this opening brace...\");\n+                err.span_label(parent.1, \"...matches this closing brace\");\n             }\n         }\n+\n+        err.span_label(self.token.span, \"unexpected closing delimiter\");\n+        err\n     }\n \n-    fn bump(&mut self) -> Spacing {\n-        let (spacing, token) = self.string_reader.next_token();\n-        self.token = token;\n-        spacing\n+    #[inline]\n+    fn parse_token_tree_non_delim_non_eof(&mut self) -> TokenTree {\n+        // `this_spacing` for the returned token refers to whether the token is\n+        // immediately followed by another op token. It is determined by the\n+        // next token: its kind and its `preceded_by_whitespace` status.\n+        let (next_tok, is_next_tok_preceded_by_whitespace) = self.string_reader.next_token();\n+        let this_spacing = if is_next_tok_preceded_by_whitespace || !next_tok.is_op() {\n+            Spacing::Alone\n+        } else {\n+            Spacing::Joint\n+        };\n+        let this_tok = std::mem::replace(&mut self.token, next_tok);\n+        TokenTree::Token(this_tok, this_spacing)\n     }\n }\n "}, {"sha": "78b98431b190dad2bb36f8459239e0637c4de737", "filename": "src/librustdoc/html/highlight.rs", "status": "modified", "additions": 6, "deletions": 3, "changes": 9, "blob_url": "https://github.com/rust-lang/rust/blob/6201eabde85db854c1ebb57624be5ec699246b50/src%2Flibrustdoc%2Fhtml%2Fhighlight.rs", "raw_url": "https://github.com/rust-lang/rust/raw/6201eabde85db854c1ebb57624be5ec699246b50/src%2Flibrustdoc%2Fhtml%2Fhighlight.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustdoc%2Fhtml%2Fhighlight.rs?ref=6201eabde85db854c1ebb57624be5ec699246b50", "patch": "@@ -13,6 +13,7 @@ use std::collections::VecDeque;\n use std::fmt::{Display, Write};\n \n use rustc_data_structures::fx::FxHashMap;\n+use rustc_lexer::Cursor;\n use rustc_lexer::{LiteralKind, TokenKind};\n use rustc_span::edition::Edition;\n use rustc_span::symbol::Symbol;\n@@ -408,15 +409,16 @@ enum Highlight<'a> {\n \n struct TokenIter<'a> {\n     src: &'a str,\n+    cursor: Cursor<'a>,\n }\n \n impl<'a> Iterator for TokenIter<'a> {\n     type Item = (TokenKind, &'a str);\n     fn next(&mut self) -> Option<(TokenKind, &'a str)> {\n-        if self.src.is_empty() {\n+        let token = self.cursor.advance_token();\n+        if token.kind == TokenKind::Eof {\n             return None;\n         }\n-        let token = rustc_lexer::first_token(self.src);\n         let (text, rest) = self.src.split_at(token.len as usize);\n         self.src = rest;\n         Some((token.kind, text))\n@@ -525,7 +527,7 @@ impl<'a> Classifier<'a> {\n     /// Takes as argument the source code to HTML-ify, the rust edition to use and the source code\n     /// file span which will be used later on by the `span_correspondance_map`.\n     fn new(src: &str, file_span: Span, decoration_info: Option<DecorationInfo>) -> Classifier<'_> {\n-        let tokens = PeekIter::new(TokenIter { src });\n+        let tokens = PeekIter::new(TokenIter { src, cursor: Cursor::new(src) });\n         let decorations = decoration_info.map(Decorations::new);\n         Classifier {\n             tokens,\n@@ -850,6 +852,7 @@ impl<'a> Classifier<'a> {\n                 Class::Ident(self.new_span(before, text))\n             }\n             TokenKind::Lifetime { .. } => Class::Lifetime,\n+            TokenKind::Eof => panic!(\"Eof in advance\"),\n         };\n         // Anything that didn't return above is the simple case where we the\n         // class just spans a single token, so we can use the `string` method."}]}