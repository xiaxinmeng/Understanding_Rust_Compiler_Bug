{"sha": "5171cc76c264fd46f32e140c2e460c77ca87d5e5", "node_id": "MDY6Q29tbWl0NzI0NzEyOjUxNzFjYzc2YzI2NGZkNDZmMzJlMTQwYzJlNDYwYzc3Y2E4N2Q1ZTU=", "commit": {"author": {"name": "bors", "email": "bors@rust-lang.org", "date": "2020-10-25T09:23:45Z"}, "committer": {"name": "bors", "email": "bors@rust-lang.org", "date": "2020-10-25T09:23:45Z"}, "message": "Auto merge of #77476 - tgnottingham:buffered_siphasher128, r=nnethercote\n\nperf: buffer SipHasher128\n\nThis is an attempt to improve Siphasher128 performance by buffering input. Although it reduces instruction count, I'm not confident the effect on wall times, or lack-thereof, is worth the change.\n\n---\n\nAdditional notes not reflected in source comments:\n\n* Implementation choices were guided by a combination of results from rustc-perf and micro-benchmarks, mostly the former.\n* ~~I tried a couple of different struct layouts that might be more cache friendly with no obvious effect.~~ Update: a particular struct layout was chosen, but it's not critical to performance. See comments in source and discussion below.\n* I suspect that buffering would be important to a SIMD-accelerated algorithm, but from what I've read and my own tests, SipHash does not seem very amenable to SIMD acceleration, at least by SSE.", "tree": {"sha": "93c64eee7dceb17e762532d38cabc51125fb187d", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/93c64eee7dceb17e762532d38cabc51125fb187d"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/5171cc76c264fd46f32e140c2e460c77ca87d5e5", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/5171cc76c264fd46f32e140c2e460c77ca87d5e5", "html_url": "https://github.com/rust-lang/rust/commit/5171cc76c264fd46f32e140c2e460c77ca87d5e5", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/5171cc76c264fd46f32e140c2e460c77ca87d5e5/comments", "author": {"login": "bors", "id": 3372342, "node_id": "MDQ6VXNlcjMzNzIzNDI=", "avatar_url": "https://avatars.githubusercontent.com/u/3372342?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bors", "html_url": "https://github.com/bors", "followers_url": "https://api.github.com/users/bors/followers", "following_url": "https://api.github.com/users/bors/following{/other_user}", "gists_url": "https://api.github.com/users/bors/gists{/gist_id}", "starred_url": "https://api.github.com/users/bors/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bors/subscriptions", "organizations_url": "https://api.github.com/users/bors/orgs", "repos_url": "https://api.github.com/users/bors/repos", "events_url": "https://api.github.com/users/bors/events{/privacy}", "received_events_url": "https://api.github.com/users/bors/received_events", "type": "User", "site_admin": false}, "committer": {"login": "bors", "id": 3372342, "node_id": "MDQ6VXNlcjMzNzIzNDI=", "avatar_url": "https://avatars.githubusercontent.com/u/3372342?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bors", "html_url": "https://github.com/bors", "followers_url": "https://api.github.com/users/bors/followers", "following_url": "https://api.github.com/users/bors/following{/other_user}", "gists_url": "https://api.github.com/users/bors/gists{/gist_id}", "starred_url": "https://api.github.com/users/bors/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bors/subscriptions", "organizations_url": "https://api.github.com/users/bors/orgs", "repos_url": "https://api.github.com/users/bors/repos", "events_url": "https://api.github.com/users/bors/events{/privacy}", "received_events_url": "https://api.github.com/users/bors/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "3e0dd24a6c0812eedbb02182a75c352f8a7e184a", "url": "https://api.github.com/repos/rust-lang/rust/commits/3e0dd24a6c0812eedbb02182a75c352f8a7e184a", "html_url": "https://github.com/rust-lang/rust/commit/3e0dd24a6c0812eedbb02182a75c352f8a7e184a"}, {"sha": "a602d155f06bb3fb7129c036f372e1cb4595ab01", "url": "https://api.github.com/repos/rust-lang/rust/commits/a602d155f06bb3fb7129c036f372e1cb4595ab01", "html_url": "https://github.com/rust-lang/rust/commit/a602d155f06bb3fb7129c036f372e1cb4595ab01"}], "stats": {"total": 585, "additions": 389, "deletions": 196}, "files": [{"sha": "7669b78834c3f90e9713accb7e04aa0eef4cdb94", "filename": "compiler/rustc_data_structures/src/lib.rs", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "blob_url": "https://github.com/rust-lang/rust/blob/5171cc76c264fd46f32e140c2e460c77ca87d5e5/compiler%2Frustc_data_structures%2Fsrc%2Flib.rs", "raw_url": "https://github.com/rust-lang/rust/raw/5171cc76c264fd46f32e140c2e460c77ca87d5e5/compiler%2Frustc_data_structures%2Fsrc%2Flib.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_data_structures%2Fsrc%2Flib.rs?ref=5171cc76c264fd46f32e140c2e460c77ca87d5e5", "patch": "@@ -28,6 +28,7 @@\n #![feature(const_panic)]\n #![feature(min_const_generics)]\n #![feature(once_cell)]\n+#![feature(maybe_uninit_uninit_array)]\n #![allow(rustc::default_hash_types)]\n \n #[macro_use]"}, {"sha": "53062b9c20da8b8e6e6118730a518ff98c3bbf3f", "filename": "compiler/rustc_data_structures/src/sip128.rs", "status": "modified", "additions": 343, "deletions": 196, "changes": 539, "blob_url": "https://github.com/rust-lang/rust/blob/5171cc76c264fd46f32e140c2e460c77ca87d5e5/compiler%2Frustc_data_structures%2Fsrc%2Fsip128.rs", "raw_url": "https://github.com/rust-lang/rust/raw/5171cc76c264fd46f32e140c2e460c77ca87d5e5/compiler%2Frustc_data_structures%2Fsrc%2Fsip128.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_data_structures%2Fsrc%2Fsip128.rs?ref=5171cc76c264fd46f32e140c2e460c77ca87d5e5", "patch": "@@ -1,21 +1,53 @@\n //! This is a copy of `core::hash::sip` adapted to providing 128 bit hashes.\n \n-use std::cmp;\n use std::hash::Hasher;\n-use std::mem;\n+use std::mem::{self, MaybeUninit};\n use std::ptr;\n \n #[cfg(test)]\n mod tests;\n \n+// The SipHash algorithm operates on 8-byte chunks.\n+const ELEM_SIZE: usize = mem::size_of::<u64>();\n+\n+// Size of the buffer in number of elements, not including the spill.\n+//\n+// The selection of this size was guided by rustc-perf benchmark comparisons of\n+// different buffer sizes. It should be periodically reevaluated as the compiler\n+// implementation and input characteristics change.\n+//\n+// Using the same-sized buffer for everything we hash is a performance versus\n+// complexity tradeoff. The ideal buffer size, and whether buffering should even\n+// be used, depends on what is being hashed. It may be worth it to size the\n+// buffer appropriately (perhaps by making SipHasher128 generic over the buffer\n+// size) or disable buffering depending on what is being hashed. But at this\n+// time, we use the same buffer size for everything.\n+const BUFFER_CAPACITY: usize = 8;\n+\n+// Size of the buffer in bytes, not including the spill.\n+const BUFFER_SIZE: usize = BUFFER_CAPACITY * ELEM_SIZE;\n+\n+// Size of the buffer in number of elements, including the spill.\n+const BUFFER_WITH_SPILL_CAPACITY: usize = BUFFER_CAPACITY + 1;\n+\n+// Size of the buffer in bytes, including the spill.\n+const BUFFER_WITH_SPILL_SIZE: usize = BUFFER_WITH_SPILL_CAPACITY * ELEM_SIZE;\n+\n+// Index of the spill element in the buffer.\n+const BUFFER_SPILL_INDEX: usize = BUFFER_WITH_SPILL_CAPACITY - 1;\n+\n #[derive(Debug, Clone)]\n+#[repr(C)]\n pub struct SipHasher128 {\n-    k0: u64,\n-    k1: u64,\n-    length: usize, // how many bytes we've processed\n-    state: State,  // hash State\n-    tail: u64,     // unprocessed bytes le\n-    ntail: usize,  // how many bytes in tail are valid\n+    // The access pattern during hashing consists of accesses to `nbuf` and\n+    // `buf` until the buffer is full, followed by accesses to `state` and\n+    // `processed`, and then repetition of that pattern until hashing is done.\n+    // This is the basis for the ordering of fields below. However, in practice\n+    // the cache miss-rate for data access is extremely low regardless of order.\n+    nbuf: usize, // how many bytes in buf are valid\n+    buf: [MaybeUninit<u64>; BUFFER_WITH_SPILL_CAPACITY], // unprocessed bytes le\n+    state: State, // hash State\n+    processed: usize, // how many bytes we've processed\n }\n \n #[derive(Debug, Clone, Copy)]\n@@ -51,271 +83,386 @@ macro_rules! compress {\n     }};\n }\n \n-/// Loads an integer of the desired type from a byte stream, in LE order. Uses\n-/// `copy_nonoverlapping` to let the compiler generate the most efficient way\n-/// to load it from a possibly unaligned address.\n-///\n-/// Unsafe because: unchecked indexing at i..i+size_of(int_ty)\n-macro_rules! load_int_le {\n-    ($buf:expr, $i:expr, $int_ty:ident) => {{\n-        debug_assert!($i + mem::size_of::<$int_ty>() <= $buf.len());\n-        let mut data = 0 as $int_ty;\n-        ptr::copy_nonoverlapping(\n-            $buf.get_unchecked($i),\n-            &mut data as *mut _ as *mut u8,\n-            mem::size_of::<$int_ty>(),\n-        );\n-        data.to_le()\n-    }};\n-}\n-\n-/// Loads a u64 using up to 7 bytes of a byte slice. It looks clumsy but the\n-/// `copy_nonoverlapping` calls that occur (via `load_int_le!`) all have fixed\n-/// sizes and avoid calling `memcpy`, which is good for speed.\n-///\n-/// Unsafe because: unchecked indexing at start..start+len\n+// Copies up to 8 bytes from source to destination. This performs better than\n+// `ptr::copy_nonoverlapping` on microbenchmarks and may perform better on real\n+// workloads since all of the copies have fixed sizes and avoid calling memcpy.\n+//\n+// This is specifically designed for copies of up to 8 bytes, because that's the\n+// maximum of number bytes needed to fill an 8-byte-sized element on which\n+// SipHash operates. Note that for variable-sized copies which are known to be\n+// less than 8 bytes, this function will perform more work than necessary unless\n+// the compiler is able to optimize the extra work away.\n #[inline]\n-unsafe fn u8to64_le(buf: &[u8], start: usize, len: usize) -> u64 {\n-    debug_assert!(len < 8);\n-    let mut i = 0; // current byte index (from LSB) in the output u64\n-    let mut out = 0;\n-    if i + 3 < len {\n-        out = load_int_le!(buf, start + i, u32) as u64;\n+unsafe fn copy_nonoverlapping_small(src: *const u8, dst: *mut u8, count: usize) {\n+    debug_assert!(count <= 8);\n+\n+    if count == 8 {\n+        ptr::copy_nonoverlapping(src, dst, 8);\n+        return;\n+    }\n+\n+    let mut i = 0;\n+    if i + 3 < count {\n+        ptr::copy_nonoverlapping(src.add(i), dst.add(i), 4);\n         i += 4;\n     }\n-    if i + 1 < len {\n-        out |= (load_int_le!(buf, start + i, u16) as u64) << (i * 8);\n+\n+    if i + 1 < count {\n+        ptr::copy_nonoverlapping(src.add(i), dst.add(i), 2);\n         i += 2\n     }\n-    if i < len {\n-        out |= (*buf.get_unchecked(start + i) as u64) << (i * 8);\n+\n+    if i < count {\n+        *dst.add(i) = *src.add(i);\n         i += 1;\n     }\n-    debug_assert_eq!(i, len);\n-    out\n+\n+    debug_assert_eq!(i, count);\n }\n \n+// # Implementation\n+//\n+// This implementation uses buffering to reduce the hashing cost for inputs\n+// consisting of many small integers. Buffering simplifies the integration of\n+// integer input--the integer write function typically just appends to the\n+// buffer with a statically sized write, updates metadata, and returns.\n+//\n+// Buffering also prevents alternating between writes that do and do not trigger\n+// the hashing process. Only when the entire buffer is full do we transition\n+// into hashing. This allows us to keep the hash state in registers for longer,\n+// instead of loading and storing it before and after processing each element.\n+//\n+// When a write fills the buffer, a buffer processing function is invoked to\n+// hash all of the buffered input. The buffer processing functions are marked\n+// `#[inline(never)]` so that they aren't inlined into the append functions,\n+// which ensures the more frequently called append functions remain inlineable\n+// and don't include register pushing/popping that would only be made necessary\n+// by inclusion of the complex buffer processing path which uses those\n+// registers.\n+//\n+// The buffer includes a \"spill\"--an extra element at the end--which simplifies\n+// the integer write buffer processing path. The value that fills the buffer can\n+// be written with a statically sized write that may spill over into the spill.\n+// After the buffer is processed, the part of the value that spilled over can be\n+// written from the spill to the beginning of the buffer with another statically\n+// sized write. This write may copy more bytes than actually spilled over, but\n+// we maintain the metadata such that any extra copied bytes will be ignored by\n+// subsequent processing. Due to the static sizes, this scheme performs better\n+// than copying the exact number of bytes needed into the end and beginning of\n+// the buffer.\n+//\n+// The buffer is uninitialized, which improves performance, but may preclude\n+// efficient implementation of alternative approaches. The improvement is not so\n+// large that an alternative approach should be disregarded because it cannot be\n+// efficiently implemented with an uninitialized buffer. On the other hand, an\n+// uninitialized buffer may become more important should a larger one be used.\n+//\n+// # Platform Dependence\n+//\n+// The SipHash algorithm operates on byte sequences. It parses the input stream\n+// as 8-byte little-endian integers. Therefore, given the same byte sequence, it\n+// produces the same result on big- and little-endian hardware.\n+//\n+// However, the Hasher trait has methods which operate on multi-byte integers.\n+// How they are converted into byte sequences can be endian-dependent (by using\n+// native byte order) or independent (by consistently using either LE or BE byte\n+// order). It can also be `isize` and `usize` size dependent (by using the\n+// native size), or independent (by converting to a common size), supposing the\n+// values can be represented in 32 bits.\n+//\n+// In order to make `SipHasher128` consistent with `SipHasher` in libstd, we\n+// choose to do the integer to byte sequence conversion in the platform-\n+// dependent way. Clients can achieve platform-independent hashing by widening\n+// `isize` and `usize` integers to 64 bits on 32-bit systems and byte-swapping\n+// integers on big-endian systems before passing them to the writing functions.\n+// This causes the input byte sequence to look identical on big- and little-\n+// endian systems (supposing `isize` and `usize` values can be represented in 32\n+// bits), which ensures platform-independent results.\n impl SipHasher128 {\n     #[inline]\n     pub fn new_with_keys(key0: u64, key1: u64) -> SipHasher128 {\n-        let mut state = SipHasher128 {\n-            k0: key0,\n-            k1: key1,\n-            length: 0,\n-            state: State { v0: 0, v1: 0, v2: 0, v3: 0 },\n-            tail: 0,\n-            ntail: 0,\n+        let mut hasher = SipHasher128 {\n+            nbuf: 0,\n+            buf: MaybeUninit::uninit_array(),\n+            state: State {\n+                v0: key0 ^ 0x736f6d6570736575,\n+                // The XOR with 0xee is only done on 128-bit algorithm version.\n+                v1: key1 ^ (0x646f72616e646f6d ^ 0xee),\n+                v2: key0 ^ 0x6c7967656e657261,\n+                v3: key1 ^ 0x7465646279746573,\n+            },\n+            processed: 0,\n         };\n-        state.reset();\n-        state\n+\n+        unsafe {\n+            // Initialize spill because we read from it in `short_write_process_buffer`.\n+            *hasher.buf.get_unchecked_mut(BUFFER_SPILL_INDEX) = MaybeUninit::zeroed();\n+        }\n+\n+        hasher\n     }\n \n+    // A specialized write function for values with size <= 8.\n     #[inline]\n-    fn reset(&mut self) {\n-        self.length = 0;\n-        self.state.v0 = self.k0 ^ 0x736f6d6570736575;\n-        self.state.v1 = self.k1 ^ 0x646f72616e646f6d;\n-        self.state.v2 = self.k0 ^ 0x6c7967656e657261;\n-        self.state.v3 = self.k1 ^ 0x7465646279746573;\n-        self.ntail = 0;\n-\n-        // This is only done in the 128 bit version:\n-        self.state.v1 ^= 0xee;\n+    fn short_write<T>(&mut self, x: T) {\n+        let size = mem::size_of::<T>();\n+        let nbuf = self.nbuf;\n+        debug_assert!(size <= 8);\n+        debug_assert!(nbuf < BUFFER_SIZE);\n+        debug_assert!(nbuf + size < BUFFER_WITH_SPILL_SIZE);\n+\n+        if nbuf + size < BUFFER_SIZE {\n+            unsafe {\n+                // The memcpy call is optimized away because the size is known.\n+                let dst = (self.buf.as_mut_ptr() as *mut u8).add(nbuf);\n+                ptr::copy_nonoverlapping(&x as *const _ as *const u8, dst, size);\n+            }\n+\n+            self.nbuf = nbuf + size;\n+\n+            return;\n+        }\n+\n+        unsafe { self.short_write_process_buffer(x) }\n     }\n \n-    // A specialized write function for values with size <= 8.\n-    //\n-    // The input must be zero-extended to 64-bits by the caller. This extension\n-    // isn't hashed, but the implementation requires it for correctness.\n+    // A specialized write function for values with size <= 8 that should only\n+    // be called when the write would cause the buffer to fill.\n     //\n-    // This function, given the same integer size and value, has the same effect\n-    // on both little- and big-endian hardware. It operates on values without\n-    // depending on their sequence in memory, so is independent of endianness.\n-    //\n-    // However, we want SipHasher128 to be platform-dependent, in order to be\n-    // consistent with the platform-dependent SipHasher in libstd. In other\n-    // words, we want:\n-    //\n-    // - little-endian: `write_u32(0xDDCCBBAA)` == `write([0xAA, 0xBB, 0xCC, 0xDD])`\n-    // - big-endian:    `write_u32(0xDDCCBBAA)` == `write([0xDD, 0xCC, 0xBB, 0xAA])`\n-    //\n-    // Therefore, in order to produce endian-dependent results, SipHasher128's\n-    // `write_xxx` Hasher trait methods byte-swap `x` prior to zero-extending.\n-    //\n-    // If clients of SipHasher128 itself want platform-independent results, they\n-    // *also* must byte-swap integer inputs before invoking the `write_xxx`\n-    // methods on big-endian hardware (that is, two byte-swaps must occur--one\n-    // in the client, and one in SipHasher128). Additionally, they must extend\n-    // `usize` and `isize` types to 64 bits on 32-bit systems.\n-    #[inline]\n-    fn short_write<T>(&mut self, _x: T, x: u64) {\n+    // SAFETY: the write of `x` into `self.buf` starting at byte offset\n+    // `self.nbuf` must cause `self.buf` to become fully initialized (and not\n+    // overflow) if it wasn't already.\n+    #[inline(never)]\n+    unsafe fn short_write_process_buffer<T>(&mut self, x: T) {\n         let size = mem::size_of::<T>();\n-        self.length += size;\n-\n-        // The original number must be zero-extended, not sign-extended.\n-        debug_assert!(if size < 8 { x >> (8 * size) == 0 } else { true });\n-\n-        // The number of bytes needed to fill `self.tail`.\n-        let needed = 8 - self.ntail;\n-\n-        // SipHash parses the input stream as 8-byte little-endian integers.\n-        // Inputs are put into `self.tail` until 8 bytes of data have been\n-        // collected, and then that word is processed.\n-        //\n-        // For example, imagine that `self.tail` is 0x0000_00EE_DDCC_BBAA,\n-        // `self.ntail` is 5 (because 5 bytes have been put into `self.tail`),\n-        // and `needed` is therefore 3.\n-        //\n-        // - Scenario 1, `self.write_u8(0xFF)`: we have already zero-extended\n-        //   the input to 0x0000_0000_0000_00FF. We now left-shift it five\n-        //   bytes, giving 0x0000_FF00_0000_0000. We then bitwise-OR that value\n-        //   into `self.tail`, resulting in 0x0000_FFEE_DDCC_BBAA.\n-        //   (Zero-extension of the original input is critical in this scenario\n-        //   because we don't want the high two bytes of `self.tail` to be\n-        //   touched by the bitwise-OR.) `self.tail` is not yet full, so we\n-        //   return early, after updating `self.ntail` to 6.\n-        //\n-        // - Scenario 2, `self.write_u32(0xIIHH_GGFF)`: we have already\n-        //   zero-extended the input to 0x0000_0000_IIHH_GGFF. We now\n-        //   left-shift it five bytes, giving 0xHHGG_FF00_0000_0000. We then\n-        //   bitwise-OR that value into `self.tail`, resulting in\n-        //   0xHHGG_FFEE_DDCC_BBAA. `self.tail` is now full, and we can use it\n-        //   to update `self.state`. (As mentioned above, this assumes a\n-        //   little-endian machine; on a big-endian machine we would have\n-        //   byte-swapped 0xIIHH_GGFF in the caller, giving 0xFFGG_HHII, and we\n-        //   would then end up bitwise-ORing 0xGGHH_II00_0000_0000 into\n-        //   `self.tail`).\n-        //\n-        self.tail |= x << (8 * self.ntail);\n-        if size < needed {\n-            self.ntail += size;\n+        let nbuf = self.nbuf;\n+        debug_assert!(size <= 8);\n+        debug_assert!(nbuf < BUFFER_SIZE);\n+        debug_assert!(nbuf + size >= BUFFER_SIZE);\n+        debug_assert!(nbuf + size < BUFFER_WITH_SPILL_SIZE);\n+\n+        // Copy first part of input into end of buffer, possibly into spill\n+        // element. The memcpy call is optimized away because the size is known.\n+        let dst = (self.buf.as_mut_ptr() as *mut u8).add(nbuf);\n+        ptr::copy_nonoverlapping(&x as *const _ as *const u8, dst, size);\n+\n+        // Process buffer.\n+        for i in 0..BUFFER_CAPACITY {\n+            let elem = self.buf.get_unchecked(i).assume_init().to_le();\n+            self.state.v3 ^= elem;\n+            Sip24Rounds::c_rounds(&mut self.state);\n+            self.state.v0 ^= elem;\n+        }\n+\n+        // Copy remaining input into start of buffer by copying size - 1\n+        // elements from spill (at most size - 1 bytes could have overflowed\n+        // into the spill). The memcpy call is optimized away because the size\n+        // is known. And the whole copy is optimized away for size == 1.\n+        let src = self.buf.get_unchecked(BUFFER_SPILL_INDEX) as *const _ as *const u8;\n+        ptr::copy_nonoverlapping(src, self.buf.as_mut_ptr() as *mut u8, size - 1);\n+\n+        // This function should only be called when the write fills the buffer.\n+        // Therefore, when size == 1, the new `self.nbuf` must be zero. The size\n+        // is statically known, so the branch is optimized away.\n+        self.nbuf = if size == 1 { 0 } else { nbuf + size - BUFFER_SIZE };\n+        self.processed += BUFFER_SIZE;\n+    }\n+\n+    // A write function for byte slices.\n+    #[inline]\n+    fn slice_write(&mut self, msg: &[u8]) {\n+        let length = msg.len();\n+        let nbuf = self.nbuf;\n+        debug_assert!(nbuf < BUFFER_SIZE);\n+\n+        if nbuf + length < BUFFER_SIZE {\n+            unsafe {\n+                let dst = (self.buf.as_mut_ptr() as *mut u8).add(nbuf);\n+\n+                if length <= 8 {\n+                    copy_nonoverlapping_small(msg.as_ptr(), dst, length);\n+                } else {\n+                    // This memcpy is *not* optimized away.\n+                    ptr::copy_nonoverlapping(msg.as_ptr(), dst, length);\n+                }\n+            }\n+\n+            self.nbuf = nbuf + length;\n+\n             return;\n         }\n \n-        // `self.tail` is full, process it.\n-        self.state.v3 ^= self.tail;\n-        Sip24Rounds::c_rounds(&mut self.state);\n-        self.state.v0 ^= self.tail;\n-\n-        // Continuing scenario 2: we have one byte left over from the input. We\n-        // set `self.ntail` to 1 and `self.tail` to `0x0000_0000_IIHH_GGFF >>\n-        // 8*3`, which is 0x0000_0000_0000_00II. (Or on a big-endian machine\n-        // the prior byte-swapping would leave us with 0x0000_0000_0000_00FF.)\n-        //\n-        // The `if` is needed to avoid shifting by 64 bits, which Rust\n-        // complains about.\n-        self.ntail = size - needed;\n-        self.tail = if needed < 8 { x >> (8 * needed) } else { 0 };\n+        unsafe { self.slice_write_process_buffer(msg) }\n+    }\n+\n+    // A write function for byte slices that should only be called when the\n+    // write would cause the buffer to fill.\n+    //\n+    // SAFETY: `self.buf` must be initialized up to the byte offset `self.nbuf`,\n+    // and `msg` must contain enough bytes to initialize the rest of the element\n+    // containing the byte offset `self.nbuf`.\n+    #[inline(never)]\n+    unsafe fn slice_write_process_buffer(&mut self, msg: &[u8]) {\n+        let length = msg.len();\n+        let nbuf = self.nbuf;\n+        debug_assert!(nbuf < BUFFER_SIZE);\n+        debug_assert!(nbuf + length >= BUFFER_SIZE);\n+\n+        // Always copy first part of input into current element of buffer.\n+        // This function should only be called when the write fills the buffer,\n+        // so we know that there is enough input to fill the current element.\n+        let valid_in_elem = nbuf % ELEM_SIZE;\n+        let needed_in_elem = ELEM_SIZE - valid_in_elem;\n+\n+        let src = msg.as_ptr();\n+        let dst = (self.buf.as_mut_ptr() as *mut u8).add(nbuf);\n+        copy_nonoverlapping_small(src, dst, needed_in_elem);\n+\n+        // Process buffer.\n+\n+        // Using `nbuf / ELEM_SIZE + 1` rather than `(nbuf + needed_in_elem) /\n+        // ELEM_SIZE` to show the compiler that this loop's upper bound is > 0.\n+        // We know that is true, because last step ensured we have a full\n+        // element in the buffer.\n+        let last = nbuf / ELEM_SIZE + 1;\n+\n+        for i in 0..last {\n+            let elem = self.buf.get_unchecked(i).assume_init().to_le();\n+            self.state.v3 ^= elem;\n+            Sip24Rounds::c_rounds(&mut self.state);\n+            self.state.v0 ^= elem;\n+        }\n+\n+        // Process the remaining element-sized chunks of input.\n+        let mut processed = needed_in_elem;\n+        let input_left = length - processed;\n+        let elems_left = input_left / ELEM_SIZE;\n+        let extra_bytes_left = input_left % ELEM_SIZE;\n+\n+        for _ in 0..elems_left {\n+            let elem = (msg.as_ptr().add(processed) as *const u64).read_unaligned().to_le();\n+            self.state.v3 ^= elem;\n+            Sip24Rounds::c_rounds(&mut self.state);\n+            self.state.v0 ^= elem;\n+            processed += ELEM_SIZE;\n+        }\n+\n+        // Copy remaining input into start of buffer.\n+        let src = msg.as_ptr().add(processed);\n+        let dst = self.buf.as_mut_ptr() as *mut u8;\n+        copy_nonoverlapping_small(src, dst, extra_bytes_left);\n+\n+        self.nbuf = extra_bytes_left;\n+        self.processed += nbuf + processed;\n     }\n \n     #[inline]\n     pub fn finish128(mut self) -> (u64, u64) {\n-        let b: u64 = ((self.length as u64 & 0xff) << 56) | self.tail;\n+        debug_assert!(self.nbuf < BUFFER_SIZE);\n \n-        self.state.v3 ^= b;\n-        Sip24Rounds::c_rounds(&mut self.state);\n-        self.state.v0 ^= b;\n+        // Process full elements in buffer.\n+        let last = self.nbuf / ELEM_SIZE;\n \n-        self.state.v2 ^= 0xee;\n-        Sip24Rounds::d_rounds(&mut self.state);\n-        let _0 = self.state.v0 ^ self.state.v1 ^ self.state.v2 ^ self.state.v3;\n+        // Since we're consuming self, avoid updating members for a potential\n+        // performance gain.\n+        let mut state = self.state;\n+\n+        for i in 0..last {\n+            let elem = unsafe { self.buf.get_unchecked(i).assume_init().to_le() };\n+            state.v3 ^= elem;\n+            Sip24Rounds::c_rounds(&mut state);\n+            state.v0 ^= elem;\n+        }\n+\n+        // Get remaining partial element.\n+        let elem = if self.nbuf % ELEM_SIZE != 0 {\n+            unsafe {\n+                // Ensure element is initialized by writing zero bytes. At most\n+                // `ELEM_SIZE - 1` are required given the above check. It's safe\n+                // to write this many because we have the spill and we maintain\n+                // `self.nbuf` such that this write will start before the spill.\n+                let dst = (self.buf.as_mut_ptr() as *mut u8).add(self.nbuf);\n+                ptr::write_bytes(dst, 0, ELEM_SIZE - 1);\n+                self.buf.get_unchecked(last).assume_init().to_le()\n+            }\n+        } else {\n+            0\n+        };\n+\n+        // Finalize the hash.\n+        let length = self.processed + self.nbuf;\n+        let b: u64 = ((length as u64 & 0xff) << 56) | elem;\n+\n+        state.v3 ^= b;\n+        Sip24Rounds::c_rounds(&mut state);\n+        state.v0 ^= b;\n+\n+        state.v2 ^= 0xee;\n+        Sip24Rounds::d_rounds(&mut state);\n+        let _0 = state.v0 ^ state.v1 ^ state.v2 ^ state.v3;\n+\n+        state.v1 ^= 0xdd;\n+        Sip24Rounds::d_rounds(&mut state);\n+        let _1 = state.v0 ^ state.v1 ^ state.v2 ^ state.v3;\n \n-        self.state.v1 ^= 0xdd;\n-        Sip24Rounds::d_rounds(&mut self.state);\n-        let _1 = self.state.v0 ^ self.state.v1 ^ self.state.v2 ^ self.state.v3;\n         (_0, _1)\n     }\n }\n \n impl Hasher for SipHasher128 {\n     #[inline]\n     fn write_u8(&mut self, i: u8) {\n-        self.short_write(i, i as u64);\n+        self.short_write(i);\n     }\n \n     #[inline]\n     fn write_u16(&mut self, i: u16) {\n-        self.short_write(i, i.to_le() as u64);\n+        self.short_write(i);\n     }\n \n     #[inline]\n     fn write_u32(&mut self, i: u32) {\n-        self.short_write(i, i.to_le() as u64);\n+        self.short_write(i);\n     }\n \n     #[inline]\n     fn write_u64(&mut self, i: u64) {\n-        self.short_write(i, i.to_le() as u64);\n+        self.short_write(i);\n     }\n \n     #[inline]\n     fn write_usize(&mut self, i: usize) {\n-        self.short_write(i, i.to_le() as u64);\n+        self.short_write(i);\n     }\n \n     #[inline]\n     fn write_i8(&mut self, i: i8) {\n-        self.short_write(i, i as u8 as u64);\n+        self.short_write(i as u8);\n     }\n \n     #[inline]\n     fn write_i16(&mut self, i: i16) {\n-        self.short_write(i, (i as u16).to_le() as u64);\n+        self.short_write(i as u16);\n     }\n \n     #[inline]\n     fn write_i32(&mut self, i: i32) {\n-        self.short_write(i, (i as u32).to_le() as u64);\n+        self.short_write(i as u32);\n     }\n \n     #[inline]\n     fn write_i64(&mut self, i: i64) {\n-        self.short_write(i, (i as u64).to_le() as u64);\n+        self.short_write(i as u64);\n     }\n \n     #[inline]\n     fn write_isize(&mut self, i: isize) {\n-        self.short_write(i, (i as usize).to_le() as u64);\n+        self.short_write(i as usize);\n     }\n \n     #[inline]\n     fn write(&mut self, msg: &[u8]) {\n-        let length = msg.len();\n-        self.length += length;\n-\n-        let mut needed = 0;\n-\n-        if self.ntail != 0 {\n-            needed = 8 - self.ntail;\n-            self.tail |= unsafe { u8to64_le(msg, 0, cmp::min(length, needed)) } << (8 * self.ntail);\n-            if length < needed {\n-                self.ntail += length;\n-                return;\n-            } else {\n-                self.state.v3 ^= self.tail;\n-                Sip24Rounds::c_rounds(&mut self.state);\n-                self.state.v0 ^= self.tail;\n-                self.ntail = 0;\n-            }\n-        }\n-\n-        // Buffered tail is now flushed, process new input.\n-        let len = length - needed;\n-        let left = len & 0x7;\n-\n-        let mut i = needed;\n-        while i < len - left {\n-            let mi = unsafe { load_int_le!(msg, i, u64) };\n-\n-            self.state.v3 ^= mi;\n-            Sip24Rounds::c_rounds(&mut self.state);\n-            self.state.v0 ^= mi;\n-\n-            i += 8;\n-        }\n-\n-        self.tail = unsafe { u8to64_le(msg, i, left) };\n-        self.ntail = left;\n+        self.slice_write(msg);\n     }\n \n     fn finish(&self) -> u64 {"}, {"sha": "5fe967c4158fe3141ea6a23ec6f4c3b61765d697", "filename": "compiler/rustc_data_structures/src/sip128/tests.rs", "status": "modified", "additions": 45, "deletions": 0, "changes": 45, "blob_url": "https://github.com/rust-lang/rust/blob/5171cc76c264fd46f32e140c2e460c77ca87d5e5/compiler%2Frustc_data_structures%2Fsrc%2Fsip128%2Ftests.rs", "raw_url": "https://github.com/rust-lang/rust/raw/5171cc76c264fd46f32e140c2e460c77ca87d5e5/compiler%2Frustc_data_structures%2Fsrc%2Fsip128%2Ftests.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_data_structures%2Fsrc%2Fsip128%2Ftests.rs?ref=5171cc76c264fd46f32e140c2e460c77ca87d5e5", "patch": "@@ -450,3 +450,48 @@ fn test_short_write_works() {\n \n     assert_eq!(h1_hash, h2_hash);\n }\n+\n+macro_rules! test_fill_buffer {\n+    ($type:ty, $write_method:ident) => {{\n+        // Test filling and overfilling the buffer from all possible offsets\n+        // for a given integer type and its corresponding write method.\n+        const SIZE: usize = std::mem::size_of::<$type>();\n+        let input = [42; BUFFER_SIZE];\n+        let x = 0x01234567_89ABCDEF_76543210_FEDCBA98_u128 as $type;\n+        let x_bytes = &x.to_ne_bytes();\n+\n+        for i in 1..=SIZE {\n+            let s = &input[..BUFFER_SIZE - i];\n+\n+            let mut h1 = SipHasher128::new_with_keys(7, 13);\n+            h1.write(s);\n+            h1.$write_method(x);\n+\n+            let mut h2 = SipHasher128::new_with_keys(7, 13);\n+            h2.write(s);\n+            h2.write(x_bytes);\n+\n+            let h1_hash = h1.finish128();\n+            let h2_hash = h2.finish128();\n+\n+            assert_eq!(h1_hash, h2_hash);\n+        }\n+    }};\n+}\n+\n+#[test]\n+fn test_fill_buffer() {\n+    test_fill_buffer!(u8, write_u8);\n+    test_fill_buffer!(u16, write_u16);\n+    test_fill_buffer!(u32, write_u32);\n+    test_fill_buffer!(u64, write_u64);\n+    test_fill_buffer!(u128, write_u128);\n+    test_fill_buffer!(usize, write_usize);\n+\n+    test_fill_buffer!(i8, write_i8);\n+    test_fill_buffer!(i16, write_i16);\n+    test_fill_buffer!(i32, write_i32);\n+    test_fill_buffer!(i64, write_i64);\n+    test_fill_buffer!(i128, write_i128);\n+    test_fill_buffer!(isize, write_isize);\n+}"}]}