{"sha": "2780d9dd5410a5c093f27eacfb1684ddbfcb4632", "node_id": "MDY6Q29tbWl0NzI0NzEyOjI3ODBkOWRkNTQxMGE1YzA5M2YyN2VhY2ZiMTY4NGRkYmZjYjQ2MzI=", "commit": {"author": {"name": "bors", "email": "bors@rust-lang.org", "date": "2014-02-09T10:21:22Z"}, "committer": {"name": "bors", "email": "bors@rust-lang.org", "date": "2014-02-09T10:21:22Z"}, "message": "auto merge of #12119 : huonw/rust/guide-testing, r=brson\n\nbe more precise about what's being benchmarked.\r\n\r\nAlso, reorganise the layout a bit, to put examples directly in their\r\nsections.", "tree": {"sha": "a35668fca339066fd8559c19dd21f6c6f12ebfeb", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/a35668fca339066fd8559c19dd21f6c6f12ebfeb"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/2780d9dd5410a5c093f27eacfb1684ddbfcb4632", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/2780d9dd5410a5c093f27eacfb1684ddbfcb4632", "html_url": "https://github.com/rust-lang/rust/commit/2780d9dd5410a5c093f27eacfb1684ddbfcb4632", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/2780d9dd5410a5c093f27eacfb1684ddbfcb4632/comments", "author": {"login": "bors", "id": 3372342, "node_id": "MDQ6VXNlcjMzNzIzNDI=", "avatar_url": "https://avatars.githubusercontent.com/u/3372342?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bors", "html_url": "https://github.com/bors", "followers_url": "https://api.github.com/users/bors/followers", "following_url": "https://api.github.com/users/bors/following{/other_user}", "gists_url": "https://api.github.com/users/bors/gists{/gist_id}", "starred_url": "https://api.github.com/users/bors/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bors/subscriptions", "organizations_url": "https://api.github.com/users/bors/orgs", "repos_url": "https://api.github.com/users/bors/repos", "events_url": "https://api.github.com/users/bors/events{/privacy}", "received_events_url": "https://api.github.com/users/bors/received_events", "type": "User", "site_admin": false}, "committer": {"login": "bors", "id": 3372342, "node_id": "MDQ6VXNlcjMzNzIzNDI=", "avatar_url": "https://avatars.githubusercontent.com/u/3372342?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bors", "html_url": "https://github.com/bors", "followers_url": "https://api.github.com/users/bors/followers", "following_url": "https://api.github.com/users/bors/following{/other_user}", "gists_url": "https://api.github.com/users/bors/gists{/gist_id}", "starred_url": "https://api.github.com/users/bors/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bors/subscriptions", "organizations_url": "https://api.github.com/users/bors/orgs", "repos_url": "https://api.github.com/users/bors/repos", "events_url": "https://api.github.com/users/bors/events{/privacy}", "received_events_url": "https://api.github.com/users/bors/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "f0e0d9e101d36957102c38de3ec9e089c1d43dbb", "url": "https://api.github.com/repos/rust-lang/rust/commits/f0e0d9e101d36957102c38de3ec9e089c1d43dbb", "html_url": "https://github.com/rust-lang/rust/commit/f0e0d9e101d36957102c38de3ec9e089c1d43dbb"}, {"sha": "a7719a7347feeabb4f348c57273b949862bd968d", "url": "https://api.github.com/repos/rust-lang/rust/commits/a7719a7347feeabb4f348c57273b949862bd968d", "html_url": "https://github.com/rust-lang/rust/commit/a7719a7347feeabb4f348c57273b949862bd968d"}], "stats": {"total": 186, "additions": 130, "deletions": 56}, "files": [{"sha": "455c65cef8b7f8cfda260f1961238034faae7c59", "filename": "src/doc/guide-testing.md", "status": "modified", "additions": 130, "deletions": 56, "changes": 186, "blob_url": "https://github.com/rust-lang/rust/blob/2780d9dd5410a5c093f27eacfb1684ddbfcb4632/src%2Fdoc%2Fguide-testing.md", "raw_url": "https://github.com/rust-lang/rust/raw/2780d9dd5410a5c093f27eacfb1684ddbfcb4632/src%2Fdoc%2Fguide-testing.md", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Fdoc%2Fguide-testing.md?ref=2780d9dd5410a5c093f27eacfb1684ddbfcb4632", "patch": "@@ -16,10 +16,12 @@ fn return_two_test() {\n }\n ~~~\n \n-To run these tests, use `rustc --test`:\n+To run these tests, compile with `rustc --test` and run the resulting\n+binary:\n \n ~~~ {.notrust}\n-$ rustc --test foo.rs; ./foo\n+$ rustc --test foo.rs\n+$ ./foo\n running 1 test\n test return_two_test ... ok\n \n@@ -47,8 +49,8 @@ value. To run the tests in a crate, it must be compiled with the\n `--test` flag: `rustc myprogram.rs --test -o myprogram-tests`. Running\n the resulting executable will run all the tests in the crate. A test\n is considered successful if its function returns; if the task running\n-the test fails, through a call to `fail!`, a failed `check` or\n-`assert`, or some other (`assert_eq`, ...) means, then the test fails.\n+the test fails, through a call to `fail!`, a failed `assert`, or some\n+other (`assert_eq`, ...) means, then the test fails.\n \n When compiling a crate with the `--test` flag `--cfg test` is also\n implied, so that tests can be conditionally compiled.\n@@ -100,7 +102,63 @@ failure output difficult. In these cases you can set the\n `RUST_TEST_TASKS` environment variable to 1 to make the tests run\n sequentially.\n \n-## Benchmarking\n+## Examples\n+\n+### Typical test run\n+\n+~~~ {.notrust}\n+$ mytests\n+\n+running 30 tests\n+running driver::tests::mytest1 ... ok\n+running driver::tests::mytest2 ... ignored\n+... snip ...\n+running driver::tests::mytest30 ... ok\n+\n+result: ok. 28 passed; 0 failed; 2 ignored\n+~~~\n+\n+### Test run with failures\n+\n+~~~ {.notrust}\n+$ mytests\n+\n+running 30 tests\n+running driver::tests::mytest1 ... ok\n+running driver::tests::mytest2 ... ignored\n+... snip ...\n+running driver::tests::mytest30 ... FAILED\n+\n+result: FAILED. 27 passed; 1 failed; 2 ignored\n+~~~\n+\n+### Running ignored tests\n+\n+~~~ {.notrust}\n+$ mytests --ignored\n+\n+running 2 tests\n+running driver::tests::mytest2 ... failed\n+running driver::tests::mytest10 ... ok\n+\n+result: FAILED. 1 passed; 1 failed; 0 ignored\n+~~~\n+\n+### Running a subset of tests\n+\n+~~~ {.notrust}\n+$ mytests mytest1\n+\n+running 11 tests\n+running driver::tests::mytest1 ... ok\n+running driver::tests::mytest10 ... ignored\n+... snip ...\n+running driver::tests::mytest19 ... ok\n+\n+result: ok. 11 passed; 0 failed; 1 ignored\n+~~~\n+\n+# Microbenchmarking\n \n The test runner also understands a simple form of benchmark execution.\n Benchmark functions are marked with the `#[bench]` attribute, rather\n@@ -111,11 +169,12 @@ component of your testsuite, pass `--bench` to the compiled test\n runner.\n \n The type signature of a benchmark function differs from a unit test:\n-it takes a mutable reference to type `test::BenchHarness`. Inside the\n-benchmark function, any time-variable or \"setup\" code should execute\n-first, followed by a call to `iter` on the benchmark harness, passing\n-a closure that contains the portion of the benchmark you wish to\n-actually measure the per-iteration speed of.\n+it takes a mutable reference to type\n+`extra::test::BenchHarness`. Inside the benchmark function, any\n+time-variable or \"setup\" code should execute first, followed by a call\n+to `iter` on the benchmark harness, passing a closure that contains\n+the portion of the benchmark you wish to actually measure the\n+per-iteration speed of.\n \n For benchmarks relating to processing/generating data, one can set the\n `bytes` field to the number of bytes consumed/produced in each\n@@ -128,15 +187,16 @@ For example:\n ~~~\n extern mod extra;\n use std::vec;\n+use extra::test::BenchHarness;\n \n #[bench]\n-fn bench_sum_1024_ints(b: &mut extra::test::BenchHarness) {\n+fn bench_sum_1024_ints(b: &mut BenchHarness) {\n     let v = vec::from_fn(1024, |n| n);\n     b.iter(|| {v.iter().fold(0, |old, new| old + *new);} );\n }\n \n #[bench]\n-fn initialise_a_vector(b: &mut extra::test::BenchHarness) {\n+fn initialise_a_vector(b: &mut BenchHarness) {\n     b.iter(|| {vec::from_elem(1024, 0u64);} );\n     b.bytes = 1024 * 8;\n }\n@@ -163,74 +223,88 @@ Advice on writing benchmarks:\n To run benchmarks, pass the `--bench` flag to the compiled\n test-runner. Benchmarks are compiled-in but not executed by default.\n \n-## Examples\n-\n-### Typical test run\n-\n ~~~ {.notrust}\n-> mytests\n+$ rustc mytests.rs -O --test\n+$ mytests --bench\n \n-running 30 tests\n-running driver::tests::mytest1 ... ok\n-running driver::tests::mytest2 ... ignored\n-... snip ...\n-running driver::tests::mytest30 ... ok\n+running 2 tests\n+test bench_sum_1024_ints ... bench: 709 ns/iter (+/- 82)\n+test initialise_a_vector ... bench: 424 ns/iter (+/- 99) = 19320 MB/s\n \n-result: ok. 28 passed; 0 failed; 2 ignored\n-~~~ {.notrust}\n+test result: ok. 0 passed; 0 failed; 0 ignored; 2 measured\n+~~~\n \n-### Test run with failures\n+## Benchmarks and the optimizer\n \n-~~~ {.notrust}\n-> mytests\n+Benchmarks compiled with optimizations activated can be dramatically\n+changed by the optimizer so that the benchmark is no longer\n+benchmarking what one expects. For example, the compiler might\n+recognize that some calculation has no external effects and remove\n+it entirely.\n \n-running 30 tests\n-running driver::tests::mytest1 ... ok\n-running driver::tests::mytest2 ... ignored\n-... snip ...\n-running driver::tests::mytest30 ... FAILED\n+~~~\n+extern mod extra;\n+use extra::test::BenchHarness;\n \n-result: FAILED. 27 passed; 1 failed; 2 ignored\n+#[bench]\n+fn bench_xor_1000_ints(bh: &mut BenchHarness) {\n+    bh.iter(|| {\n+            range(0, 1000).fold(0, |old, new| old ^ new);\n+        });\n+}\n ~~~\n \n-### Running ignored tests\n+gives the following results\n \n ~~~ {.notrust}\n-> mytests --ignored\n-\n-running 2 tests\n-running driver::tests::mytest2 ... failed\n-running driver::tests::mytest10 ... ok\n+running 1 test\n+test bench_xor_1000_ints ... bench:         0 ns/iter (+/- 0)\n \n-result: FAILED. 1 passed; 1 failed; 0 ignored\n+test result: ok. 0 passed; 0 failed; 0 ignored; 1 measured\n ~~~\n \n-### Running a subset of tests\n+The benchmarking runner offers two ways to avoid this. Either, the\n+closure that the `iter` method receives can return an arbitrary value\n+which forces the optimizer to consider the result used and ensures it\n+cannot remove the computation entirely. This could be done for the\n+example above by adjusting the `bh.iter` call to\n \n-~~~ {.notrust}\n-> mytests mytest1\n+~~~\n+bh.iter(|| range(0, 1000).fold(0, |old, new| old ^ new))\n+~~~\n \n-running 11 tests\n-running driver::tests::mytest1 ... ok\n-running driver::tests::mytest10 ... ignored\n-... snip ...\n-running driver::tests::mytest19 ... ok\n+Or, the other option is to call the generic `extra::test::black_box`\n+function, which is an opaque \"black box\" to the optimizer and so\n+forces it to consider any argument as used.\n \n-result: ok. 11 passed; 0 failed; 1 ignored\n ~~~\n+use extra::test::black_box\n \n-### Running benchmarks\n+bh.iter(|| {\n+        black_box(range(0, 1000).fold(0, |old, new| old ^ new));\n+    });\n+~~~\n \n-~~~ {.notrust}\n-> mytests --bench\n+Neither of these read or modify the value, and are very cheap for\n+small values. Larger values can be passed indirectly to reduce\n+overhead (e.g. `black_box(&huge_struct)`).\n \n-running 2 tests\n-test bench_sum_1024_ints ... bench: 709 ns/iter (+/- 82)\n-test initialise_a_vector ... bench: 424 ns/iter (+/- 99) = 19320 MB/s\n+Performing either of the above changes gives the following\n+benchmarking results\n \n-test result: ok. 0 passed; 0 failed; 0 ignored; 2 measured\n+~~~ {.notrust}\n+running 1 test\n+test bench_xor_1000_ints ... bench:       375 ns/iter (+/- 148)\n+\n+test result: ok. 0 passed; 0 failed; 0 ignored; 1 measured\n ~~~\n \n+However, the optimizer can still modify a testcase in an undesirable\n+manner even when using either of the above. Benchmarks can be checked\n+by hand by looking at the output of the compiler using the `--emit=ir`\n+(for LLVM IR), `--emit=asm` (for assembly) or compiling normally and\n+using any method for examining object code.\n+\n ## Saving and ratcheting metrics\n \n When running benchmarks or other tests, the test runner can record"}]}