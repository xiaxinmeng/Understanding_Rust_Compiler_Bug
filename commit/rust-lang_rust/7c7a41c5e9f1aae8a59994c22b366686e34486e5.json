{"sha": "7c7a41c5e9f1aae8a59994c22b366686e34486e5", "node_id": "MDY6Q29tbWl0NzI0NzEyOjdjN2E0MWM1ZTlmMWFhZThhNTk5OTRjMjJiMzY2Njg2ZTM0NDg2ZTU=", "commit": {"author": {"name": "bors[bot]", "email": "26634292+bors[bot]@users.noreply.github.com", "date": "2021-08-28T23:38:34Z"}, "committer": {"name": "GitHub", "email": "noreply@github.com", "date": "2021-08-28T23:38:34Z"}, "message": "Merge #10067\n\n10067: Downmap tokens to all token descendants instead of just the first r=Veykril a=Veykril\n\nWith this we can now resolve usages of identifiers inside (proc-)macros even if they are used for different purposes multiple times inside the expansion.\r\nExample here being with the cursor being on the `no_send_sync_value` function causing us to still highlight the identifier in the attribute invocation correctly as we now resolve its usages in there. Prior we only saw the first usage of the identifier which is for a definition only, as such we bailed and didn't highlight it. \r\n![image](https://user-images.githubusercontent.com/3757771/131233056-7e645b1d-b82f-468c-bf19-d3335a2cf7c2.png)\r\n\r\nNote that this has to be explicitly switched over for most IDE features now as pretty much everything expects a single node/token as a result from descending.\n\nCo-authored-by: Lukas Wirth <lukastw97@gmail.com>", "tree": {"sha": "724d29aee980ba22a960dd59b84b45fc325dd96a", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/724d29aee980ba22a960dd59b84b45fc325dd96a"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/7c7a41c5e9f1aae8a59994c22b366686e34486e5", "comment_count": 0, "verification": {"verified": true, "reason": "valid", "signature": "-----BEGIN PGP SIGNATURE-----\n\nwsBcBAABCAAQBQJhKsj6CRBK7hj4Ov3rIwAAxZQIAH74eWc8aSM3Kirqdz+FoJW+\nsOkGEgIh9D9qhFhajh8Gs+iEpX+y71zNA045EGwO/0cgfgcSC1AG9yOj22XeGIPj\nYOZS1dtJW0qzAa6LIlySCN4HLdPsevSQtmTQhIa9CPox7iFA8FLVtcfwO2ewjJob\n4lmQv2eLVWFZuIvLHvTtCRNyhipu0eSK1WP8qokHTFebsRD8Lb/UyrghK9Y/vl2K\njo0//mBuct9sxDuEgev08UYOyCIlr49vnOipEjNu+LHSPV/+rTzEeMuASJd6Og4q\n2gk3hrbIPLpe1x90qs/PsgaqGxlajrAzoocjSFPW2TO1/xVNw5uQj6u7wgCLPiM=\n=WGGy\n-----END PGP SIGNATURE-----\n", "payload": "tree 724d29aee980ba22a960dd59b84b45fc325dd96a\nparent 7e31c5ec0d5d7245375b14baa5db26b11b9eb940\nparent 6993a607cb7fde96ecf34c7d29f342e06e2142d2\nauthor bors[bot] <26634292+bors[bot]@users.noreply.github.com> 1630193914 +0000\ncommitter GitHub <noreply@github.com> 1630193914 +0000\n\nMerge #10067\n\n10067: Downmap tokens to all token descendants instead of just the first r=Veykril a=Veykril\n\nWith this we can now resolve usages of identifiers inside (proc-)macros even if they are used for different purposes multiple times inside the expansion.\r\nExample here being with the cursor being on the `no_send_sync_value` function causing us to still highlight the identifier in the attribute invocation correctly as we now resolve its usages in there. Prior we only saw the first usage of the identifier which is for a definition only, as such we bailed and didn't highlight it. \r\n![image](https://user-images.githubusercontent.com/3757771/131233056-7e645b1d-b82f-468c-bf19-d3335a2cf7c2.png)\r\n\r\nNote that this has to be explicitly switched over for most IDE features now as pretty much everything expects a single node/token as a result from descending.\n\nCo-authored-by: Lukas Wirth <lukastw97@gmail.com>\n"}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/7c7a41c5e9f1aae8a59994c22b366686e34486e5", "html_url": "https://github.com/rust-lang/rust/commit/7c7a41c5e9f1aae8a59994c22b366686e34486e5", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/7c7a41c5e9f1aae8a59994c22b366686e34486e5/comments", "author": {"login": "bors[bot]", "id": 26634292, "node_id": "MDM6Qm90MjY2MzQyOTI=", "avatar_url": "https://avatars.githubusercontent.com/in/1847?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bors%5Bbot%5D", "html_url": "https://github.com/apps/bors", "followers_url": "https://api.github.com/users/bors%5Bbot%5D/followers", "following_url": "https://api.github.com/users/bors%5Bbot%5D/following{/other_user}", "gists_url": "https://api.github.com/users/bors%5Bbot%5D/gists{/gist_id}", "starred_url": "https://api.github.com/users/bors%5Bbot%5D/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bors%5Bbot%5D/subscriptions", "organizations_url": "https://api.github.com/users/bors%5Bbot%5D/orgs", "repos_url": "https://api.github.com/users/bors%5Bbot%5D/repos", "events_url": "https://api.github.com/users/bors%5Bbot%5D/events{/privacy}", "received_events_url": "https://api.github.com/users/bors%5Bbot%5D/received_events", "type": "Bot", "site_admin": false}, "committer": {"login": "web-flow", "id": 19864447, "node_id": "MDQ6VXNlcjE5ODY0NDQ3", "avatar_url": "https://avatars.githubusercontent.com/u/19864447?v=4", "gravatar_id": "", "url": "https://api.github.com/users/web-flow", "html_url": "https://github.com/web-flow", "followers_url": "https://api.github.com/users/web-flow/followers", "following_url": "https://api.github.com/users/web-flow/following{/other_user}", "gists_url": "https://api.github.com/users/web-flow/gists{/gist_id}", "starred_url": "https://api.github.com/users/web-flow/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/web-flow/subscriptions", "organizations_url": "https://api.github.com/users/web-flow/orgs", "repos_url": "https://api.github.com/users/web-flow/repos", "events_url": "https://api.github.com/users/web-flow/events{/privacy}", "received_events_url": "https://api.github.com/users/web-flow/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "7e31c5ec0d5d7245375b14baa5db26b11b9eb940", "url": "https://api.github.com/repos/rust-lang/rust/commits/7e31c5ec0d5d7245375b14baa5db26b11b9eb940", "html_url": "https://github.com/rust-lang/rust/commit/7e31c5ec0d5d7245375b14baa5db26b11b9eb940"}, {"sha": "6993a607cb7fde96ecf34c7d29f342e06e2142d2", "url": "https://api.github.com/repos/rust-lang/rust/commits/6993a607cb7fde96ecf34c7d29f342e06e2142d2", "html_url": "https://github.com/rust-lang/rust/commit/6993a607cb7fde96ecf34c7d29f342e06e2142d2"}], "stats": {"total": 305, "additions": 199, "deletions": 106}, "files": [{"sha": "9f03c6467bf52bb1e4cbf372a8d5225ef49cc46d", "filename": "crates/hir/src/semantics.rs", "status": "modified", "additions": 85, "deletions": 62, "changes": 147, "blob_url": "https://github.com/rust-lang/rust/blob/7c7a41c5e9f1aae8a59994c22b366686e34486e5/crates%2Fhir%2Fsrc%2Fsemantics.rs", "raw_url": "https://github.com/rust-lang/rust/raw/7c7a41c5e9f1aae8a59994c22b366686e34486e5/crates%2Fhir%2Fsrc%2Fsemantics.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fhir%2Fsrc%2Fsemantics.rs?ref=7c7a41c5e9f1aae8a59994c22b366686e34486e5", "patch": "@@ -2,7 +2,7 @@\n \n mod source_to_def;\n \n-use std::{cell::RefCell, fmt, iter::successors};\n+use std::{cell::RefCell, fmt};\n \n use base_db::{FileId, FileRange};\n use hir_def::{\n@@ -14,6 +14,7 @@ use hir_expand::{name::AsName, ExpansionInfo};\n use hir_ty::{associated_type_shorthand_candidates, Interner};\n use itertools::Itertools;\n use rustc_hash::{FxHashMap, FxHashSet};\n+use smallvec::{smallvec, SmallVec};\n use syntax::{\n     algo::find_node_at_offset,\n     ast::{self, GenericParamsOwner, LoopBodyOwner},\n@@ -165,7 +166,13 @@ impl<'db, DB: HirDatabase> Semantics<'db, DB> {\n         self.imp.speculative_expand(actual_macro_call, speculative_args, token_to_map)\n     }\n \n+    // FIXME: Rename to descend_into_macros_single\n     pub fn descend_into_macros(&self, token: SyntaxToken) -> SyntaxToken {\n+        self.imp.descend_into_macros(token).pop().unwrap()\n+    }\n+\n+    // FIXME: Rename to descend_into_macros\n+    pub fn descend_into_macros_many(&self, token: SyntaxToken) -> SmallVec<[SyntaxToken; 1]> {\n         self.imp.descend_into_macros(token)\n     }\n \n@@ -174,7 +181,7 @@ impl<'db, DB: HirDatabase> Semantics<'db, DB> {\n         node: &SyntaxNode,\n         offset: TextSize,\n     ) -> Option<N> {\n-        self.imp.descend_node_at_offset(node, offset).find_map(N::cast)\n+        self.imp.descend_node_at_offset(node, offset).flatten().find_map(N::cast)\n     }\n \n     pub fn hir_file_for(&self, syntax_node: &SyntaxNode) -> HirFileId {\n@@ -228,7 +235,17 @@ impl<'db, DB: HirDatabase> Semantics<'db, DB> {\n             return Some(it);\n         }\n \n-        self.imp.descend_node_at_offset(node, offset).find_map(N::cast)\n+        self.imp.descend_node_at_offset(node, offset).flatten().find_map(N::cast)\n+    }\n+\n+    /// Find an AstNode by offset inside SyntaxNode, if it is inside *MacroCall*,\n+    /// descend it and find again\n+    pub fn find_nodes_at_offset_with_descend<'slf, N: AstNode + 'slf>(\n+        &'slf self,\n+        node: &SyntaxNode,\n+        offset: TextSize,\n+    ) -> impl Iterator<Item = N> + 'slf {\n+        self.imp.descend_node_at_offset(node, offset).filter_map(|mut it| it.find_map(N::cast))\n     }\n \n     pub fn resolve_lifetime_param(&self, lifetime: &ast::Lifetime) -> Option<LifetimeParam> {\n@@ -440,87 +457,93 @@ impl<'db> SemanticsImpl<'db> {\n         )\n     }\n \n-    fn descend_into_macros(&self, token: SyntaxToken) -> SyntaxToken {\n+    fn descend_into_macros(&self, token: SyntaxToken) -> SmallVec<[SyntaxToken; 1]> {\n         let _p = profile::span(\"descend_into_macros\");\n         let parent = match token.parent() {\n             Some(it) => it,\n-            None => return token,\n+            None => return smallvec![token],\n         };\n         let sa = self.analyze(&parent);\n-\n-        let token = successors(Some(InFile::new(sa.file_id, token)), |token| {\n+        let mut queue = vec![InFile::new(sa.file_id, token)];\n+        let mut cache = self.expansion_info_cache.borrow_mut();\n+        let mut res = smallvec![];\n+        while let Some(token) = queue.pop() {\n             self.db.unwind_if_cancelled();\n \n-            for node in token.value.ancestors() {\n-                match_ast! {\n-                    match node {\n-                        ast::MacroCall(macro_call) => {\n-                            let tt = macro_call.token_tree()?;\n-                            let l_delim = match tt.left_delimiter_token() {\n-                                Some(it) => it.text_range().end(),\n-                                None => tt.syntax().text_range().start()\n-                            };\n-                            let r_delim = match tt.right_delimiter_token() {\n-                                Some(it) => it.text_range().start(),\n-                                None => tt.syntax().text_range().end()\n-                            };\n-                            if !TextRange::new(l_delim, r_delim).contains_range(token.value.text_range()) {\n-                                return None;\n-                            }\n-                            let file_id = sa.expand(self.db, token.with_value(&macro_call))?;\n-                            let token = self\n-                                .expansion_info_cache\n-                                .borrow_mut()\n-                                .entry(file_id)\n-                                .or_insert_with(|| file_id.expansion_info(self.db.upcast()))\n-                                .as_ref()?\n-                                .map_token_down(self.db.upcast(), None, token.as_ref())?;\n-\n-                            if let Some(parent) = token.value.parent() {\n-                                self.cache(find_root(&parent), token.file_id);\n-                            }\n-\n-                            return Some(token);\n-                        },\n-                        ast::Item(item) => {\n-                            if let Some(call_id) = self.with_ctx(|ctx| ctx.item_to_macro_call(token.with_value(item.clone()))) {\n-                                let file_id = call_id.as_file();\n-                                let token = self\n-                                    .expansion_info_cache\n-                                    .borrow_mut()\n+            let was_not_remapped = (|| {\n+                for node in token.value.ancestors() {\n+                    match_ast! {\n+                        match node {\n+                            ast::MacroCall(macro_call) => {\n+                                let tt = macro_call.token_tree()?;\n+                                let l_delim = match tt.left_delimiter_token() {\n+                                    Some(it) => it.text_range().end(),\n+                                    None => tt.syntax().text_range().start()\n+                                };\n+                                let r_delim = match tt.right_delimiter_token() {\n+                                    Some(it) => it.text_range().start(),\n+                                    None => tt.syntax().text_range().end()\n+                                };\n+                                if !TextRange::new(l_delim, r_delim).contains_range(token.value.text_range()) {\n+                                    return None;\n+                                }\n+                                let file_id = sa.expand(self.db, token.with_value(&macro_call))?;\n+                                let tokens = cache\n                                     .entry(file_id)\n                                     .or_insert_with(|| file_id.expansion_info(self.db.upcast()))\n                                     .as_ref()?\n-                                    .map_token_down(self.db.upcast(), Some(item), token.as_ref())?;\n-\n-                                if let Some(parent) = token.value.parent() {\n-                                    self.cache(find_root(&parent), token.file_id);\n+                                    .map_token_down(self.db.upcast(), None, token.as_ref())?;\n+\n+                                let len = queue.len();\n+                                queue.extend(tokens.inspect(|token| {\n+                                    if let Some(parent) = token.value.parent() {\n+                                        self.cache(find_root(&parent), token.file_id);\n+                                    }\n+                                }));\n+                                return (queue.len() != len).then(|| ());\n+                            },\n+                            ast::Item(item) => {\n+                                if let Some(call_id) = self.with_ctx(|ctx| ctx.item_to_macro_call(token.with_value(item.clone()))) {\n+                                    let file_id = call_id.as_file();\n+                                    let tokens = cache\n+                                        .entry(file_id)\n+                                        .or_insert_with(|| file_id.expansion_info(self.db.upcast()))\n+                                        .as_ref()?\n+                                        .map_token_down(self.db.upcast(), Some(item), token.as_ref())?;\n+\n+                                    let len = queue.len();\n+                                    queue.extend(tokens.inspect(|token| {\n+                                        if let Some(parent) = token.value.parent() {\n+                                            self.cache(find_root(&parent), token.file_id);\n+                                        }\n+                                    }));\n+                                    return (queue.len() != len).then(|| ());\n                                 }\n-\n-                                return Some(token);\n-                            }\n-                        },\n-                        _ => {}\n+                            },\n+                            _ => {}\n+                        }\n                     }\n                 }\n+                None\n+            })().is_none();\n+            if was_not_remapped {\n+                res.push(token.value)\n             }\n-\n-            None\n-        })\n-        .last()\n-        .unwrap();\n-        token.value\n+        }\n+        res\n     }\n \n+    // Note this return type is deliberate as [`find_nodes_at_offset_with_descend`] wants to stop\n+    // traversing the inner iterator when it finds a node.\n     fn descend_node_at_offset(\n         &self,\n         node: &SyntaxNode,\n         offset: TextSize,\n-    ) -> impl Iterator<Item = SyntaxNode> + '_ {\n+    ) -> impl Iterator<Item = impl Iterator<Item = SyntaxNode> + '_> + '_ {\n         // Handle macro token cases\n         node.token_at_offset(offset)\n-            .map(|token| self.descend_into_macros(token))\n-            .map(|it| self.token_ancestors_with_macros(it))\n+            .map(move |token| self.descend_into_macros(token))\n+            .map(|it| it.into_iter().map(move |it| self.token_ancestors_with_macros(it)))\n             .flatten()\n     }\n "}, {"sha": "2fc8468faf4ac00f5abbeaf70b7b359503bcfb56", "filename": "crates/hir_expand/src/db.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/7c7a41c5e9f1aae8a59994c22b366686e34486e5/crates%2Fhir_expand%2Fsrc%2Fdb.rs", "raw_url": "https://github.com/rust-lang/rust/raw/7c7a41c5e9f1aae8a59994c22b366686e34486e5/crates%2Fhir_expand%2Fsrc%2Fdb.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fhir_expand%2Fsrc%2Fdb.rs?ref=7c7a41c5e9f1aae8a59994c22b366686e34486e5", "patch": "@@ -163,7 +163,7 @@ pub fn expand_speculative(\n         mbe::token_tree_to_syntax_node(&speculative_expansion.value, fragment_kind).ok()?;\n \n     let token_id = macro_def.map_id_down(token_id);\n-    let range = tmap_2.range_by_token(token_id, token_to_map.kind())?;\n+    let range = tmap_2.first_range_by_token(token_id, token_to_map.kind())?;\n     let token = node.syntax_node().covering_element(range).into_token()?;\n     Some((node.syntax_node(), token))\n }"}, {"sha": "cac484a325e8cda0d93ae9730214eda7ebdc3049", "filename": "crates/hir_expand/src/hygiene.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/7c7a41c5e9f1aae8a59994c22b366686e34486e5/crates%2Fhir_expand%2Fsrc%2Fhygiene.rs", "raw_url": "https://github.com/rust-lang/rust/raw/7c7a41c5e9f1aae8a59994c22b366686e34486e5/crates%2Fhir_expand%2Fsrc%2Fhygiene.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fhir_expand%2Fsrc%2Fhygiene.rs?ref=7c7a41c5e9f1aae8a59994c22b366686e34486e5", "patch": "@@ -171,7 +171,7 @@ impl HygieneInfo {\n             },\n         };\n \n-        let range = token_map.range_by_token(token_id, SyntaxKind::IDENT)?;\n+        let range = token_map.first_range_by_token(token_id, SyntaxKind::IDENT)?;\n         Some((tt.with_value(range + tt.value), origin))\n     }\n }"}, {"sha": "3bbbb5722f33c180b3f1fc6a4e27461508061e37", "filename": "crates/hir_expand/src/lib.rs", "status": "modified", "additions": 7, "deletions": 6, "changes": 13, "blob_url": "https://github.com/rust-lang/rust/blob/7c7a41c5e9f1aae8a59994c22b366686e34486e5/crates%2Fhir_expand%2Fsrc%2Flib.rs", "raw_url": "https://github.com/rust-lang/rust/raw/7c7a41c5e9f1aae8a59994c22b366686e34486e5/crates%2Fhir_expand%2Fsrc%2Flib.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fhir_expand%2Fsrc%2Flib.rs?ref=7c7a41c5e9f1aae8a59994c22b366686e34486e5", "patch": "@@ -368,7 +368,7 @@ impl ExpansionInfo {\n         db: &dyn db::AstDatabase,\n         item: Option<ast::Item>,\n         token: InFile<&SyntaxToken>,\n-    ) -> Option<InFile<SyntaxToken>> {\n+    ) -> Option<impl Iterator<Item = InFile<SyntaxToken>> + '_> {\n         assert_eq!(token.file_id, self.arg.file_id);\n         let token_id = if let Some(item) = item {\n             let call_id = match self.expanded.file_id.0 {\n@@ -411,11 +411,12 @@ impl ExpansionInfo {\n             }\n         };\n \n-        let range = self.exp_map.range_by_token(token_id, token.value.kind())?;\n+        let tokens = self\n+            .exp_map\n+            .ranges_by_token(token_id, token.value.kind())\n+            .flat_map(move |range| self.expanded.value.covering_element(range).into_token());\n \n-        let token = self.expanded.value.covering_element(range).into_token()?;\n-\n-        Some(self.expanded.with_value(token))\n+        Some(tokens.map(move |token| self.expanded.with_value(token)))\n     }\n \n     pub fn map_token_up(\n@@ -453,7 +454,7 @@ impl ExpansionInfo {\n             },\n         };\n \n-        let range = token_map.range_by_token(token_id, token.value.kind())?;\n+        let range = token_map.first_range_by_token(token_id, token.value.kind())?;\n         let token =\n             tt.value.covering_element(range + tt.value.text_range().start()).into_token()?;\n         Some((tt.with_value(token), origin))"}, {"sha": "67ad263fa238169d570941b2cc71e3b6c59f4997", "filename": "crates/ide/src/highlight_related.rs", "status": "modified", "additions": 81, "deletions": 25, "changes": 106, "blob_url": "https://github.com/rust-lang/rust/blob/7c7a41c5e9f1aae8a59994c22b366686e34486e5/crates%2Fide%2Fsrc%2Fhighlight_related.rs", "raw_url": "https://github.com/rust-lang/rust/raw/7c7a41c5e9f1aae8a59994c22b366686e34486e5/crates%2Fide%2Fsrc%2Fhighlight_related.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fide%2Fsrc%2Fhighlight_related.rs?ref=7c7a41c5e9f1aae8a59994c22b366686e34486e5", "patch": "@@ -6,13 +6,15 @@ use ide_db::{\n     search::{FileReference, ReferenceAccess, SearchScope},\n     RootDatabase,\n };\n+use rustc_hash::FxHashSet;\n use syntax::{\n     ast::{self, LoopBodyOwner},\n     match_ast, AstNode, SyntaxNode, SyntaxToken, TextRange, TextSize, T,\n };\n \n use crate::{display::TryToNav, references, NavigationTarget};\n \n+#[derive(PartialEq, Eq, Hash)]\n pub struct HighlightedRange {\n     pub range: TextRange,\n     pub access: Option<ReferenceAccess>,\n@@ -70,7 +72,7 @@ fn highlight_references(\n     syntax: &SyntaxNode,\n     FilePosition { offset, file_id }: FilePosition,\n ) -> Option<Vec<HighlightedRange>> {\n-    let defs = find_defs(sema, syntax, offset)?;\n+    let defs = find_defs(sema, syntax, offset);\n     let usages = defs\n         .iter()\n         .flat_map(|&d| {\n@@ -99,7 +101,12 @@ fn highlight_references(\n         })\n     });\n \n-    Some(declarations.chain(usages).collect())\n+    let res: FxHashSet<_> = declarations.chain(usages).collect();\n+    if res.is_empty() {\n+        None\n+    } else {\n+        Some(res.into_iter().collect())\n+    }\n }\n \n fn highlight_exit_points(\n@@ -270,29 +277,40 @@ fn find_defs(\n     sema: &Semantics<RootDatabase>,\n     syntax: &SyntaxNode,\n     offset: TextSize,\n-) -> Option<Vec<Definition>> {\n-    let defs = match sema.find_node_at_offset_with_descend(syntax, offset)? {\n-        ast::NameLike::NameRef(name_ref) => match NameRefClass::classify(sema, &name_ref)? {\n-            NameRefClass::Definition(def) => vec![def],\n-            NameRefClass::FieldShorthand { local_ref, field_ref } => {\n-                vec![Definition::Local(local_ref), Definition::Field(field_ref)]\n-            }\n-        },\n-        ast::NameLike::Name(name) => match NameClass::classify(sema, &name)? {\n-            NameClass::Definition(it) | NameClass::ConstReference(it) => vec![it],\n-            NameClass::PatFieldShorthand { local_def, field_ref } => {\n-                vec![Definition::Local(local_def), Definition::Field(field_ref)]\n-            }\n-        },\n-        ast::NameLike::Lifetime(lifetime) => NameRefClass::classify_lifetime(sema, &lifetime)\n-            .and_then(|class| match class {\n-                NameRefClass::Definition(it) => Some(it),\n-                _ => None,\n+) -> FxHashSet<Definition> {\n+    sema.find_nodes_at_offset_with_descend(syntax, offset)\n+        .flat_map(|name_like| {\n+            Some(match name_like {\n+                ast::NameLike::NameRef(name_ref) => {\n+                    match NameRefClass::classify(sema, &name_ref)? {\n+                        NameRefClass::Definition(def) => vec![def],\n+                        NameRefClass::FieldShorthand { local_ref, field_ref } => {\n+                            vec![Definition::Local(local_ref), Definition::Field(field_ref)]\n+                        }\n+                    }\n+                }\n+                ast::NameLike::Name(name) => match NameClass::classify(sema, &name)? {\n+                    NameClass::Definition(it) | NameClass::ConstReference(it) => vec![it],\n+                    NameClass::PatFieldShorthand { local_def, field_ref } => {\n+                        vec![Definition::Local(local_def), Definition::Field(field_ref)]\n+                    }\n+                },\n+                ast::NameLike::Lifetime(lifetime) => {\n+                    NameRefClass::classify_lifetime(sema, &lifetime)\n+                        .and_then(|class| match class {\n+                            NameRefClass::Definition(it) => Some(it),\n+                            _ => None,\n+                        })\n+                        .or_else(|| {\n+                            NameClass::classify_lifetime(sema, &lifetime)\n+                                .and_then(NameClass::defined)\n+                        })\n+                        .map(|it| vec![it])?\n+                }\n             })\n-            .or_else(|| NameClass::classify_lifetime(sema, &lifetime).and_then(NameClass::defined))\n-            .map(|it| vec![it])?,\n-    };\n-    Some(defs)\n+        })\n+        .flatten()\n+        .collect()\n }\n \n #[cfg(test)]\n@@ -392,6 +410,45 @@ fn foo() {\n         );\n     }\n \n+    #[test]\n+    fn test_multi_macro_usage() {\n+        check(\n+            r#\"\n+macro_rules! foo {\n+    ($ident:ident) => {\n+        fn $ident() -> $ident { loop {} }\n+        struct $ident;\n+    }\n+}\n+\n+foo!(bar$0);\n+  // ^^^\n+fn foo() {\n+    let bar: bar = bar();\n+          // ^^^\n+                // ^^^\n+}\n+\"#,\n+        );\n+        check(\n+            r#\"\n+macro_rules! foo {\n+    ($ident:ident) => {\n+        fn $ident() -> $ident { loop {} }\n+        struct $ident;\n+    }\n+}\n+\n+foo!(bar);\n+  // ^^^\n+fn foo() {\n+    let bar: bar$0 = bar();\n+          // ^^^\n+}\n+\"#,\n+        );\n+    }\n+\n     #[test]\n     fn test_hl_yield_points() {\n         check(\n@@ -813,7 +870,6 @@ fn function(field: u32) {\n           //^^^^^\n     Struct { field$0 }\n            //^^^^^ read\n-           //^^^^^ read\n }\n \"#,\n         );"}, {"sha": "719f424fd2dc0c9d23e1704ee544664f3c2ad765", "filename": "crates/ide_db/src/defs.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/7c7a41c5e9f1aae8a59994c22b366686e34486e5/crates%2Fide_db%2Fsrc%2Fdefs.rs", "raw_url": "https://github.com/rust-lang/rust/raw/7c7a41c5e9f1aae8a59994c22b366686e34486e5/crates%2Fide_db%2Fsrc%2Fdefs.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fide_db%2Fsrc%2Fdefs.rs?ref=7c7a41c5e9f1aae8a59994c22b366686e34486e5", "patch": "@@ -17,7 +17,7 @@ use syntax::{\n use crate::RootDatabase;\n \n // FIXME: a more precise name would probably be `Symbol`?\n-#[derive(Debug, PartialEq, Eq, Copy, Clone)]\n+#[derive(Debug, PartialEq, Eq, Copy, Clone, Hash)]\n pub enum Definition {\n     Macro(MacroDef),\n     Field(Field),"}, {"sha": "855675be421abeb05f5d35b9dec6825a1e103d20", "filename": "crates/ide_db/src/search.rs", "status": "modified", "additions": 3, "deletions": 5, "changes": 8, "blob_url": "https://github.com/rust-lang/rust/blob/7c7a41c5e9f1aae8a59994c22b366686e34486e5/crates%2Fide_db%2Fsrc%2Fsearch.rs", "raw_url": "https://github.com/rust-lang/rust/raw/7c7a41c5e9f1aae8a59994c22b366686e34486e5/crates%2Fide_db%2Fsrc%2Fsearch.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fide_db%2Fsrc%2Fsearch.rs?ref=7c7a41c5e9f1aae8a59994c22b366686e34486e5", "patch": "@@ -61,7 +61,7 @@ pub struct FileReference {\n     pub access: Option<ReferenceAccess>,\n }\n \n-#[derive(Debug, Copy, Clone, PartialEq)]\n+#[derive(Debug, Copy, Clone, PartialEq, Eq, Hash)]\n pub enum ReferenceAccess {\n     Read,\n     Write,\n@@ -393,7 +393,7 @@ impl<'a> FindUsages<'a> {\n                     continue;\n                 }\n \n-                if let Some(name) = sema.find_node_at_offset_with_descend(&tree, offset) {\n+                for name in sema.find_nodes_at_offset_with_descend(&tree, offset) {\n                     if match name {\n                         ast::NameLike::NameRef(name_ref) => self.found_name_ref(&name_ref, sink),\n                         ast::NameLike::Name(name) => self.found_name(&name, sink),\n@@ -410,9 +410,7 @@ impl<'a> FindUsages<'a> {\n                         continue;\n                     }\n \n-                    if let Some(ast::NameLike::NameRef(name_ref)) =\n-                        sema.find_node_at_offset_with_descend(&tree, offset)\n-                    {\n+                    for name_ref in sema.find_nodes_at_offset_with_descend(&tree, offset) {\n                         if self.found_self_ty_name_ref(self_ty, &name_ref, sink) {\n                             return;\n                         }"}, {"sha": "c8d06eebb75bb71e0555cea999ab49d45542fa08", "filename": "crates/mbe/src/tests/expand.rs", "status": "modified", "additions": 3, "deletions": 2, "changes": 5, "blob_url": "https://github.com/rust-lang/rust/blob/7c7a41c5e9f1aae8a59994c22b366686e34486e5/crates%2Fmbe%2Fsrc%2Ftests%2Fexpand.rs", "raw_url": "https://github.com/rust-lang/rust/raw/7c7a41c5e9f1aae8a59994c22b366686e34486e5/crates%2Fmbe%2Fsrc%2Ftests%2Fexpand.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fmbe%2Fsrc%2Ftests%2Fexpand.rs?ref=7c7a41c5e9f1aae8a59994c22b366686e34486e5", "patch": "@@ -58,8 +58,9 @@ macro_rules! foobar {\n     let (node, token_map) = token_tree_to_syntax_node(&expanded, FragmentKind::Items).unwrap();\n     let content = node.syntax_node().to_string();\n \n-    let get_text =\n-        |id, kind| -> String { content[token_map.range_by_token(id, kind).unwrap()].to_string() };\n+    let get_text = |id, kind| -> String {\n+        content[token_map.first_range_by_token(id, kind).unwrap()].to_string()\n+    };\n \n     assert_eq!(expanded.token_trees.len(), 4);\n     // {($e:ident) => { fn $e() {} }}"}, {"sha": "9053526d203b25d77c62f3d73a6d2443f7812575", "filename": "crates/mbe/src/token_map.rs", "status": "modified", "additions": 17, "deletions": 3, "changes": 20, "blob_url": "https://github.com/rust-lang/rust/blob/7c7a41c5e9f1aae8a59994c22b366686e34486e5/crates%2Fmbe%2Fsrc%2Ftoken_map.rs", "raw_url": "https://github.com/rust-lang/rust/raw/7c7a41c5e9f1aae8a59994c22b366686e34486e5/crates%2Fmbe%2Fsrc%2Ftoken_map.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fmbe%2Fsrc%2Ftoken_map.rs?ref=7c7a41c5e9f1aae8a59994c22b366686e34486e5", "patch": "@@ -46,9 +46,23 @@ impl TokenMap {\n         Some(token_id)\n     }\n \n-    pub fn range_by_token(&self, token_id: tt::TokenId, kind: SyntaxKind) -> Option<TextRange> {\n-        let &(_, range) = self.entries.iter().find(|(tid, _)| *tid == token_id)?;\n-        range.by_kind(kind)\n+    pub fn ranges_by_token(\n+        &self,\n+        token_id: tt::TokenId,\n+        kind: SyntaxKind,\n+    ) -> impl Iterator<Item = TextRange> + '_ {\n+        self.entries\n+            .iter()\n+            .filter(move |&&(tid, _)| tid == token_id)\n+            .filter_map(move |(_, range)| range.by_kind(kind))\n+    }\n+\n+    pub fn first_range_by_token(\n+        &self,\n+        token_id: tt::TokenId,\n+        kind: SyntaxKind,\n+    ) -> Option<TextRange> {\n+        self.ranges_by_token(token_id, kind).next()\n     }\n \n     pub(crate) fn shrink_to_fit(&mut self) {"}]}