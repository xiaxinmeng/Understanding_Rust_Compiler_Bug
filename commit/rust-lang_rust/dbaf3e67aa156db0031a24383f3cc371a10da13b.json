{"sha": "dbaf3e67aa156db0031a24383f3cc371a10da13b", "node_id": "C_kwDOAAsO6NoAKGRiYWYzZTY3YWExNTZkYjAwMzFhMjQzODNmM2NjMzcxYTEwZGExM2I", "commit": {"author": {"name": "bors", "email": "bors@rust-lang.org", "date": "2022-10-03T04:49:46Z"}, "committer": {"name": "bors", "email": "bors@rust-lang.org", "date": "2022-10-03T04:49:46Z"}, "message": "Auto merge of #102508 - nnethercote:even-more-lexer-improvements, r=matklad\n\nEven more lexer improvements\n\nThese are just about code clarity, rather than performance.\n\nr? `@matklad`", "tree": {"sha": "fcbed3ea075d10d504a083833960fcfee7dbf1f0", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/fcbed3ea075d10d504a083833960fcfee7dbf1f0"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/dbaf3e67aa156db0031a24383f3cc371a10da13b", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/dbaf3e67aa156db0031a24383f3cc371a10da13b", "html_url": "https://github.com/rust-lang/rust/commit/dbaf3e67aa156db0031a24383f3cc371a10da13b", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/dbaf3e67aa156db0031a24383f3cc371a10da13b/comments", "author": {"login": "bors", "id": 3372342, "node_id": "MDQ6VXNlcjMzNzIzNDI=", "avatar_url": "https://avatars.githubusercontent.com/u/3372342?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bors", "html_url": "https://github.com/bors", "followers_url": "https://api.github.com/users/bors/followers", "following_url": "https://api.github.com/users/bors/following{/other_user}", "gists_url": "https://api.github.com/users/bors/gists{/gist_id}", "starred_url": "https://api.github.com/users/bors/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bors/subscriptions", "organizations_url": "https://api.github.com/users/bors/orgs", "repos_url": "https://api.github.com/users/bors/repos", "events_url": "https://api.github.com/users/bors/events{/privacy}", "received_events_url": "https://api.github.com/users/bors/received_events", "type": "User", "site_admin": false}, "committer": {"login": "bors", "id": 3372342, "node_id": "MDQ6VXNlcjMzNzIzNDI=", "avatar_url": "https://avatars.githubusercontent.com/u/3372342?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bors", "html_url": "https://github.com/bors", "followers_url": "https://api.github.com/users/bors/followers", "following_url": "https://api.github.com/users/bors/following{/other_user}", "gists_url": "https://api.github.com/users/bors/gists{/gist_id}", "starred_url": "https://api.github.com/users/bors/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bors/subscriptions", "organizations_url": "https://api.github.com/users/bors/orgs", "repos_url": "https://api.github.com/users/bors/repos", "events_url": "https://api.github.com/users/bors/events{/privacy}", "received_events_url": "https://api.github.com/users/bors/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "607b8296e07cc1bf5b95eeb60a21b5af684f7631", "url": "https://api.github.com/repos/rust-lang/rust/commits/607b8296e07cc1bf5b95eeb60a21b5af684f7631", "html_url": "https://github.com/rust-lang/rust/commit/607b8296e07cc1bf5b95eeb60a21b5af684f7631"}, {"sha": "4e5ddf1adf09c5d1c425b1afeef8f1ac19f05562", "url": "https://api.github.com/repos/rust-lang/rust/commits/4e5ddf1adf09c5d1c425b1afeef8f1ac19f05562", "html_url": "https://github.com/rust-lang/rust/commit/4e5ddf1adf09c5d1c425b1afeef8f1ac19f05562"}], "stats": {"total": 148, "additions": 69, "deletions": 79}, "files": [{"sha": "16224d71e45696f524a04288b399a939f8cfa595", "filename": "compiler/rustc_ast/src/token.rs", "status": "modified", "additions": 8, "deletions": 11, "changes": 19, "blob_url": "https://github.com/rust-lang/rust/blob/dbaf3e67aa156db0031a24383f3cc371a10da13b/compiler%2Frustc_ast%2Fsrc%2Ftoken.rs", "raw_url": "https://github.com/rust-lang/rust/raw/dbaf3e67aa156db0031a24383f3cc371a10da13b/compiler%2Frustc_ast%2Fsrc%2Ftoken.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_ast%2Fsrc%2Ftoken.rs?ref=dbaf3e67aa156db0031a24383f3cc371a10da13b", "patch": "@@ -345,17 +345,14 @@ impl Token {\n     }\n \n     pub fn is_op(&self) -> bool {\n-        !matches!(\n-            self.kind,\n-            OpenDelim(..)\n-                | CloseDelim(..)\n-                | Literal(..)\n-                | DocComment(..)\n-                | Ident(..)\n-                | Lifetime(..)\n-                | Interpolated(..)\n-                | Eof\n-        )\n+        match self.kind {\n+            Eq | Lt | Le | EqEq | Ne | Ge | Gt | AndAnd | OrOr | Not | Tilde | BinOp(_)\n+            | BinOpEq(_) | At | Dot | DotDot | DotDotDot | DotDotEq | Comma | Semi | Colon\n+            | ModSep | RArrow | LArrow | FatArrow | Pound | Dollar | Question | SingleQuote => true,\n+\n+            OpenDelim(..) | CloseDelim(..) | Literal(..) | DocComment(..) | Ident(..)\n+            | Lifetime(..) | Interpolated(..) | Eof => false,\n+        }\n     }\n \n     pub fn is_like_plus(&self) -> bool {"}, {"sha": "4d2049cbc41eececb36c492c37684c8edb557fd3", "filename": "compiler/rustc_ast/src/tokenstream.rs", "status": "modified", "additions": 11, "deletions": 0, "changes": 11, "blob_url": "https://github.com/rust-lang/rust/blob/dbaf3e67aa156db0031a24383f3cc371a10da13b/compiler%2Frustc_ast%2Fsrc%2Ftokenstream.rs", "raw_url": "https://github.com/rust-lang/rust/raw/dbaf3e67aa156db0031a24383f3cc371a10da13b/compiler%2Frustc_ast%2Fsrc%2Ftokenstream.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_ast%2Fsrc%2Ftokenstream.rs?ref=dbaf3e67aa156db0031a24383f3cc371a10da13b", "patch": "@@ -304,9 +304,20 @@ pub struct AttributesData {\n #[derive(Clone, Debug, Default, Encodable, Decodable)]\n pub struct TokenStream(pub(crate) Lrc<Vec<TokenTree>>);\n \n+/// Similar to `proc_macro::Spacing`, but for tokens.\n+///\n+/// Note that all `ast::TokenTree::Token` instances have a `Spacing`, but when\n+/// we convert to `proc_macro::TokenTree` for proc macros only `Punct`\n+/// `TokenTree`s have a `proc_macro::Spacing`.\n #[derive(Clone, Copy, Debug, PartialEq, Encodable, Decodable, HashStable_Generic)]\n pub enum Spacing {\n+    /// The token is not immediately followed by an operator token (as\n+    /// determined by `Token::is_op`). E.g. a `+` token is `Alone` in `+ =`,\n+    /// `+/*foo*/=`, `+ident`, and `+()`.\n     Alone,\n+\n+    /// The token is immediately followed by an operator token. E.g. a `+`\n+    /// token is `Joint` in `+=` and `++`.\n     Joint,\n }\n "}, {"sha": "ff09a9fb87a7fe29546d0bfa05bccd5a489a08fe", "filename": "compiler/rustc_expand/src/proc_macro_server.rs", "status": "modified", "additions": 6, "deletions": 2, "changes": 8, "blob_url": "https://github.com/rust-lang/rust/blob/dbaf3e67aa156db0031a24383f3cc371a10da13b/compiler%2Frustc_expand%2Fsrc%2Fproc_macro_server.rs", "raw_url": "https://github.com/rust-lang/rust/raw/dbaf3e67aa156db0031a24383f3cc371a10da13b/compiler%2Frustc_expand%2Fsrc%2Fproc_macro_server.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_expand%2Fsrc%2Fproc_macro_server.rs?ref=dbaf3e67aa156db0031a24383f3cc371a10da13b", "patch": "@@ -110,10 +110,14 @@ impl FromInternal<(TokenStream, &mut Rustc<'_, '_>)> for Vec<TokenTree<TokenStre\n                 tokenstream::TokenTree::Token(token, spacing) => (token, spacing == Joint),\n             };\n \n+            // Split the operator into one or more `Punct`s, one per character.\n+            // The final one inherits the jointness of the original token. Any\n+            // before that get `joint = true`.\n             let mut op = |s: &str| {\n                 assert!(s.is_ascii());\n-                trees.extend(s.as_bytes().iter().enumerate().map(|(idx, &ch)| {\n-                    TokenTree::Punct(Punct { ch, joint: joint || idx != s.len() - 1, span })\n+                trees.extend(s.bytes().enumerate().map(|(idx, ch)| {\n+                    let is_final = idx == s.len() - 1;\n+                    TokenTree::Punct(Punct { ch, joint: if is_final { joint } else { true }, span })\n                 }));\n             };\n "}, {"sha": "88540e13ef2434708ad4bc9d40d77ccb1438658f", "filename": "compiler/rustc_parse/src/lexer/mod.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/dbaf3e67aa156db0031a24383f3cc371a10da13b/compiler%2Frustc_parse%2Fsrc%2Flexer%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/dbaf3e67aa156db0031a24383f3cc371a10da13b/compiler%2Frustc_parse%2Fsrc%2Flexer%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_parse%2Fsrc%2Flexer%2Fmod.rs?ref=dbaf3e67aa156db0031a24383f3cc371a10da13b", "patch": "@@ -52,7 +52,7 @@ pub(crate) fn parse_token_trees<'a>(\n     let cursor = Cursor::new(src);\n     let string_reader =\n         StringReader { sess, start_pos, pos: start_pos, src, cursor, override_span };\n-    tokentrees::TokenTreesReader::parse_token_trees(string_reader)\n+    tokentrees::TokenTreesReader::parse_all_token_trees(string_reader)\n }\n \n struct StringReader<'a> {"}, {"sha": "b2701817d489ba89c22e8c5692b0e906229bcc36", "filename": "compiler/rustc_parse/src/lexer/tokentrees.rs", "status": "modified", "additions": 38, "deletions": 64, "changes": 102, "blob_url": "https://github.com/rust-lang/rust/blob/dbaf3e67aa156db0031a24383f3cc371a10da13b/compiler%2Frustc_parse%2Fsrc%2Flexer%2Ftokentrees.rs", "raw_url": "https://github.com/rust-lang/rust/raw/dbaf3e67aa156db0031a24383f3cc371a10da13b/compiler%2Frustc_parse%2Fsrc%2Flexer%2Ftokentrees.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_parse%2Fsrc%2Flexer%2Ftokentrees.rs?ref=dbaf3e67aa156db0031a24383f3cc371a10da13b", "patch": "@@ -27,7 +27,7 @@ pub(super) struct TokenTreesReader<'a> {\n }\n \n impl<'a> TokenTreesReader<'a> {\n-    pub(super) fn parse_token_trees(\n+    pub(super) fn parse_all_token_trees(\n         string_reader: StringReader<'a>,\n     ) -> (PResult<'a, TokenStream>, Vec<UnmatchedBrace>) {\n         let mut tt_reader = TokenTreesReader {\n@@ -40,36 +40,51 @@ impl<'a> TokenTreesReader<'a> {\n             last_delim_empty_block_spans: FxHashMap::default(),\n             matching_block_spans: Vec::new(),\n         };\n-        let res = tt_reader.parse_all_token_trees();\n+        let res = tt_reader.parse_token_trees(/* is_delimited */ false);\n         (res, tt_reader.unmatched_braces)\n     }\n \n-    // Parse a stream of tokens into a list of `TokenTree`s, up to an `Eof`.\n-    fn parse_all_token_trees(&mut self) -> PResult<'a, TokenStream> {\n+    // Parse a stream of tokens into a list of `TokenTree`s.\n+    fn parse_token_trees(&mut self, is_delimited: bool) -> PResult<'a, TokenStream> {\n         self.token = self.string_reader.next_token().0;\n-        let mut buf = TokenStreamBuilder::default();\n+        let mut buf = Vec::new();\n         loop {\n             match self.token.kind {\n                 token::OpenDelim(delim) => buf.push(self.parse_token_tree_open_delim(delim)),\n-                token::CloseDelim(delim) => return Err(self.close_delim_err(delim)),\n-                token::Eof => return Ok(buf.into_token_stream()),\n-                _ => buf.push(self.parse_token_tree_non_delim_non_eof()),\n-            }\n-        }\n-    }\n-\n-    // Parse a stream of tokens into a list of `TokenTree`s, up to a `CloseDelim`.\n-    fn parse_token_trees_until_close_delim(&mut self) -> TokenStream {\n-        let mut buf = TokenStreamBuilder::default();\n-        loop {\n-            match self.token.kind {\n-                token::OpenDelim(delim) => buf.push(self.parse_token_tree_open_delim(delim)),\n-                token::CloseDelim(..) => return buf.into_token_stream(),\n+                token::CloseDelim(delim) => {\n+                    return if is_delimited {\n+                        Ok(TokenStream::new(buf))\n+                    } else {\n+                        Err(self.close_delim_err(delim))\n+                    };\n+                }\n                 token::Eof => {\n-                    self.eof_err().emit();\n-                    return buf.into_token_stream();\n+                    if is_delimited {\n+                        self.eof_err().emit();\n+                    }\n+                    return Ok(TokenStream::new(buf));\n+                }\n+                _ => {\n+                    // Get the next normal token. This might require getting multiple adjacent\n+                    // single-char tokens and joining them together.\n+                    let (this_spacing, next_tok) = loop {\n+                        let (next_tok, is_next_tok_preceded_by_whitespace) =\n+                            self.string_reader.next_token();\n+                        if !is_next_tok_preceded_by_whitespace {\n+                            if let Some(glued) = self.token.glue(&next_tok) {\n+                                self.token = glued;\n+                            } else {\n+                                let this_spacing =\n+                                    if next_tok.is_op() { Spacing::Joint } else { Spacing::Alone };\n+                                break (this_spacing, next_tok);\n+                            }\n+                        } else {\n+                            break (Spacing::Alone, next_tok);\n+                        }\n+                    };\n+                    let this_tok = std::mem::replace(&mut self.token, next_tok);\n+                    buf.push(TokenTree::Token(this_tok, this_spacing));\n                 }\n-                _ => buf.push(self.parse_token_tree_non_delim_non_eof()),\n             }\n         }\n     }\n@@ -113,14 +128,12 @@ impl<'a> TokenTreesReader<'a> {\n         // The span for beginning of the delimited section\n         let pre_span = self.token.span;\n \n-        // Move past the open delimiter.\n         self.open_braces.push((open_delim, self.token.span));\n-        self.token = self.string_reader.next_token().0;\n \n         // Parse the token trees within the delimiters.\n         // We stop at any delimiter so we can try to recover if the user\n         // uses an incorrect delimiter.\n-        let tts = self.parse_token_trees_until_close_delim();\n+        let tts = self.parse_token_trees(/* is_delimited */ true).unwrap();\n \n         // Expand to cover the entire delimited token tree\n         let delim_span = DelimSpan::from_pair(pre_span, self.token.span);\n@@ -242,43 +255,4 @@ impl<'a> TokenTreesReader<'a> {\n         err.span_label(self.token.span, \"unexpected closing delimiter\");\n         err\n     }\n-\n-    #[inline]\n-    fn parse_token_tree_non_delim_non_eof(&mut self) -> TokenTree {\n-        // `this_spacing` for the returned token refers to whether the token is\n-        // immediately followed by another op token. It is determined by the\n-        // next token: its kind and its `preceded_by_whitespace` status.\n-        let (next_tok, is_next_tok_preceded_by_whitespace) = self.string_reader.next_token();\n-        let this_spacing = if is_next_tok_preceded_by_whitespace || !next_tok.is_op() {\n-            Spacing::Alone\n-        } else {\n-            Spacing::Joint\n-        };\n-        let this_tok = std::mem::replace(&mut self.token, next_tok);\n-        TokenTree::Token(this_tok, this_spacing)\n-    }\n-}\n-\n-#[derive(Default)]\n-struct TokenStreamBuilder {\n-    buf: Vec<TokenTree>,\n-}\n-\n-impl TokenStreamBuilder {\n-    #[inline(always)]\n-    fn push(&mut self, tree: TokenTree) {\n-        if let Some(TokenTree::Token(prev_token, Spacing::Joint)) = self.buf.last()\n-            && let TokenTree::Token(token, joint) = &tree\n-            && let Some(glued) = prev_token.glue(token)\n-        {\n-            self.buf.pop();\n-            self.buf.push(TokenTree::Token(glued, *joint));\n-        } else {\n-            self.buf.push(tree)\n-        }\n-    }\n-\n-    fn into_token_stream(self) -> TokenStream {\n-        TokenStream::new(self.buf)\n-    }\n }"}, {"sha": "b934e087608a56e0b88ad8d5c5f09d3db88169f2", "filename": "compiler/rustc_parse/src/parser/mod.rs", "status": "modified", "additions": 5, "deletions": 1, "changes": 6, "blob_url": "https://github.com/rust-lang/rust/blob/dbaf3e67aa156db0031a24383f3cc371a10da13b/compiler%2Frustc_parse%2Fsrc%2Fparser%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/dbaf3e67aa156db0031a24383f3cc371a10da13b/compiler%2Frustc_parse%2Fsrc%2Fparser%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_parse%2Fsrc%2Fparser%2Fmod.rs?ref=dbaf3e67aa156db0031a24383f3cc371a10da13b", "patch": "@@ -302,7 +302,10 @@ impl TokenCursor {\n \n     fn desugar(&mut self, attr_style: AttrStyle, data: Symbol, span: Span) -> (Token, Spacing) {\n         // Searches for the occurrences of `\"#*` and returns the minimum number of `#`s\n-        // required to wrap the text.\n+        // required to wrap the text. E.g.\n+        // - `abc d` is wrapped as `r\"abc d\"` (num_of_hashes = 0)\n+        // - `abc \"d\"` is wrapped as `r#\"abc \"d\"\"#` (num_of_hashes = 1)\n+        // - `abc \"##d##\"` is wrapped as `r###\"abc \"d\"\"###` (num_of_hashes = 3)\n         let mut num_of_hashes = 0;\n         let mut count = 0;\n         for ch in data.as_str().chars() {\n@@ -314,6 +317,7 @@ impl TokenCursor {\n             num_of_hashes = cmp::max(num_of_hashes, count);\n         }\n \n+        // `/// foo` becomes `doc = r\"foo\".\n         let delim_span = DelimSpan::from_single(span);\n         let body = TokenTree::Delimited(\n             delim_span,"}]}