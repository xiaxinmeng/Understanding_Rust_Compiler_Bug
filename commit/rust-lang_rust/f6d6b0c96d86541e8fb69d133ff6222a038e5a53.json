{"sha": "f6d6b0c96d86541e8fb69d133ff6222a038e5a53", "node_id": "MDY6Q29tbWl0NzI0NzEyOmY2ZDZiMGM5NmQ4NjU0MWU4ZmI2OWQxMzNmZjYyMjJhMDM4ZTVhNTM=", "commit": {"author": {"name": "Tyson Nottingham", "email": "tgnottingham@gmail.com", "date": "2020-11-26T09:10:43Z"}, "committer": {"name": "Tyson Nottingham", "email": "tgnottingham@gmail.com", "date": "2020-12-22T22:12:57Z"}, "message": "rustc_query_system: share previous graph data with current graph\n\nReduce memory consumption by taking advantage of red/green algorithm\nproperties to share the previous dependency graph's node data with the\ncurrent graph instead of storing node data redundantly. Red nodes can\nshare the `DepNode`, and green nodes can share the `DepNode` and\n`Fingerprint`. Edges will be shared when possible in a later change.", "tree": {"sha": "7fbc81fd2efaf4345f982840d75afb67cab60066", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/7fbc81fd2efaf4345f982840d75afb67cab60066"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/f6d6b0c96d86541e8fb69d133ff6222a038e5a53", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/f6d6b0c96d86541e8fb69d133ff6222a038e5a53", "html_url": "https://github.com/rust-lang/rust/commit/f6d6b0c96d86541e8fb69d133ff6222a038e5a53", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/f6d6b0c96d86541e8fb69d133ff6222a038e5a53/comments", "author": {"login": "tgnottingham", "id": 3668166, "node_id": "MDQ6VXNlcjM2NjgxNjY=", "avatar_url": "https://avatars.githubusercontent.com/u/3668166?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tgnottingham", "html_url": "https://github.com/tgnottingham", "followers_url": "https://api.github.com/users/tgnottingham/followers", "following_url": "https://api.github.com/users/tgnottingham/following{/other_user}", "gists_url": "https://api.github.com/users/tgnottingham/gists{/gist_id}", "starred_url": "https://api.github.com/users/tgnottingham/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tgnottingham/subscriptions", "organizations_url": "https://api.github.com/users/tgnottingham/orgs", "repos_url": "https://api.github.com/users/tgnottingham/repos", "events_url": "https://api.github.com/users/tgnottingham/events{/privacy}", "received_events_url": "https://api.github.com/users/tgnottingham/received_events", "type": "User", "site_admin": false}, "committer": {"login": "tgnottingham", "id": 3668166, "node_id": "MDQ6VXNlcjM2NjgxNjY=", "avatar_url": "https://avatars.githubusercontent.com/u/3668166?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tgnottingham", "html_url": "https://github.com/tgnottingham", "followers_url": "https://api.github.com/users/tgnottingham/followers", "following_url": "https://api.github.com/users/tgnottingham/following{/other_user}", "gists_url": "https://api.github.com/users/tgnottingham/gists{/gist_id}", "starred_url": "https://api.github.com/users/tgnottingham/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tgnottingham/subscriptions", "organizations_url": "https://api.github.com/users/tgnottingham/orgs", "repos_url": "https://api.github.com/users/tgnottingham/repos", "events_url": "https://api.github.com/users/tgnottingham/events{/privacy}", "received_events_url": "https://api.github.com/users/tgnottingham/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "bb1fbbf84455fbad9afd26c17e0f725019322655", "url": "https://api.github.com/repos/rust-lang/rust/commits/bb1fbbf84455fbad9afd26c17e0f725019322655", "html_url": "https://github.com/rust-lang/rust/commit/bb1fbbf84455fbad9afd26c17e0f725019322655"}], "stats": {"total": 715, "additions": 499, "deletions": 216}, "files": [{"sha": "3bc93f63905bdcd482bdec923ae53aa130f5d51b", "filename": "compiler/rustc_query_system/src/dep_graph/graph.rs", "status": "modified", "additions": 495, "deletions": 212, "changes": 707, "blob_url": "https://github.com/rust-lang/rust/blob/f6d6b0c96d86541e8fb69d133ff6222a038e5a53/compiler%2Frustc_query_system%2Fsrc%2Fdep_graph%2Fgraph.rs", "raw_url": "https://github.com/rust-lang/rust/raw/f6d6b0c96d86541e8fb69d133ff6222a038e5a53/compiler%2Frustc_query_system%2Fsrc%2Fdep_graph%2Fgraph.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_query_system%2Fsrc%2Fdep_graph%2Fgraph.rs?ref=f6d6b0c96d86541e8fb69d133ff6222a038e5a53", "patch": "@@ -68,7 +68,7 @@ struct DepGraphData<K: DepKind> {\n     /// The new encoding of the dependency graph, optimized for red/green\n     /// tracking. The `current` field is the dependency graph of only the\n     /// current compilation session: We don't merge the previous dep-graph into\n-    /// current one anymore.\n+    /// current one anymore, but we do reference shared data to save space.\n     current: CurrentDepGraph<K>,\n \n     /// The dep-graph from the previous compilation session. It contains all\n@@ -134,16 +134,44 @@ impl<K: DepKind> DepGraph<K> {\n     }\n \n     pub fn query(&self) -> DepGraphQuery<K> {\n-        let data = self.data.as_ref().unwrap().current.data.lock();\n-        let nodes: Vec<_> = data.iter().map(|n| n.node).collect();\n-        let mut edges = Vec::new();\n-        for (from, edge_targets) in data.iter().map(|d| (d.node, &d.edges)) {\n-            for &edge_target in edge_targets.iter() {\n-                let to = data[edge_target].node;\n-                edges.push((from, to));\n+        let data = self.data.as_ref().unwrap();\n+        let previous = &data.previous;\n+        let data = data.current.data.lock();\n+\n+        let node_count = data.hybrid_indices.len();\n+\n+        let edge_count = data.new.edges.iter().map(|e| e.len()).sum::<usize>()\n+            + data.red.edges.iter().map(|e| e.len()).sum::<usize>()\n+            + data.green.edges.iter().map(|e| e.len()).sum::<usize>();\n+\n+        let mut nodes = Vec::with_capacity(node_count);\n+        let mut edges = Vec::with_capacity(edge_count);\n+\n+        for (index, &hybrid_index) in data.hybrid_indices.iter_enumerated() {\n+            let src = index.index();\n+\n+            match hybrid_index.into() {\n+                HybridIndex::New(new_index) => {\n+                    let new = &data.new;\n+                    nodes.push(new.nodes[new_index]);\n+                    edges.extend(new.edges[new_index].iter().map(|dst| (src, dst.index())));\n+                }\n+                HybridIndex::Red(red_index) => {\n+                    let red = &data.red;\n+                    nodes.push(previous.index_to_node(red.node_indices[red_index]));\n+                    edges.extend(red.edges[red_index].iter().map(|dst| (src, dst.index())));\n+                }\n+                HybridIndex::Green(green_index) => {\n+                    let green = &data.green;\n+                    nodes.push(previous.index_to_node(green.node_indices[green_index]));\n+                    edges.extend(green.edges[green_index].iter().map(|dst| (src, dst.index())));\n+                }\n             }\n         }\n \n+        debug_assert_eq!(nodes.len(), node_count);\n+        debug_assert_eq!(edges.len(), edge_count);\n+\n         DepGraphQuery::new(&nodes[..], &edges[..])\n     }\n \n@@ -212,7 +240,6 @@ impl<K: DepKind> DepGraph<K> {\n                     phantom_data: PhantomData,\n                 })\n             },\n-            |data, key, fingerprint, task| data.complete_task(key, task.unwrap(), fingerprint),\n             hash_result,\n         )\n     }\n@@ -225,12 +252,6 @@ impl<K: DepKind> DepGraph<K> {\n         no_tcx: bool,\n         task: fn(Ctxt, A) -> R,\n         create_task: fn(DepNode<K>) -> Option<TaskDeps<K>>,\n-        finish_task_and_alloc_depnode: fn(\n-            &CurrentDepGraph<K>,\n-            DepNode<K>,\n-            Fingerprint,\n-            Option<TaskDeps<K>>,\n-        ) -> DepNodeIndex,\n         hash_result: impl FnOnce(&mut Ctxt::StableHashingContext, &R) -> Option<Fingerprint>,\n     ) -> (R, DepNodeIndex) {\n         if let Some(ref data) = self.data {\n@@ -249,39 +270,54 @@ impl<K: DepKind> DepGraph<K> {\n                 K::with_deps(task_deps.as_ref(), || task(cx, arg))\n             };\n \n-            let current_fingerprint = hash_result(&mut hcx, &result);\n+            let edges = task_deps.map_or_else(|| smallvec![], |lock| lock.into_inner().reads);\n \n-            let dep_node_index = finish_task_and_alloc_depnode(\n-                &data.current,\n-                key,\n-                current_fingerprint.unwrap_or(Fingerprint::ZERO),\n-                task_deps.map(|lock| lock.into_inner()),\n-            );\n+            let current_fingerprint = hash_result(&mut hcx, &result);\n \n             let print_status = cfg!(debug_assertions) && cx.debug_dep_tasks();\n \n-            // Determine the color of the new DepNode.\n-            if let Some(prev_index) = data.previous.node_to_index_opt(&key) {\n-                let prev_fingerprint = data.previous.fingerprint_by_index(prev_index);\n-\n-                let color = if let Some(current_fingerprint) = current_fingerprint {\n-                    if current_fingerprint == prev_fingerprint {\n+            // Intern the new `DepNode`.\n+            let dep_node_index = if let Some(prev_index) = data.previous.node_to_index_opt(&key) {\n+                // Determine the color and index of the new `DepNode`.\n+                let (color, dep_node_index) = if let Some(current_fingerprint) = current_fingerprint\n+                {\n+                    if current_fingerprint == data.previous.fingerprint_by_index(prev_index) {\n                         if print_status {\n                             eprintln!(\"[task::green] {:?}\", key);\n                         }\n-                        DepNodeColor::Green(dep_node_index)\n+\n+                        let dep_node_index =\n+                            data.current.intern_green_node(&data.previous, prev_index, edges);\n+\n+                        (DepNodeColor::Green(dep_node_index), dep_node_index)\n                     } else {\n                         if print_status {\n                             eprintln!(\"[task::red] {:?}\", key);\n                         }\n-                        DepNodeColor::Red\n+\n+                        let dep_node_index = data.current.intern_red_node(\n+                            &data.previous,\n+                            prev_index,\n+                            edges,\n+                            current_fingerprint,\n+                        );\n+\n+                        (DepNodeColor::Red, dep_node_index)\n                     }\n                 } else {\n                     if print_status {\n                         eprintln!(\"[task::unknown] {:?}\", key);\n                     }\n+\n+                    let dep_node_index = data.current.intern_red_node(\n+                        &data.previous,\n+                        prev_index,\n+                        edges,\n+                        Fingerprint::ZERO,\n+                    );\n+\n                     // Mark the node as Red if we can't hash the result\n-                    DepNodeColor::Red\n+                    (DepNodeColor::Red, dep_node_index)\n                 };\n \n                 debug_assert!(\n@@ -292,9 +328,19 @@ impl<K: DepKind> DepGraph<K> {\n                 );\n \n                 data.colors.insert(prev_index, color);\n-            } else if print_status {\n-                eprintln!(\"[task::new] {:?}\", key);\n-            }\n+                dep_node_index\n+            } else {\n+                if print_status {\n+                    eprintln!(\"[task::new] {:?}\", key);\n+                }\n+\n+                data.current.intern_node(\n+                    &data.previous,\n+                    key,\n+                    edges,\n+                    current_fingerprint.unwrap_or(Fingerprint::ZERO),\n+                )\n+            };\n \n             (result, dep_node_index)\n         } else {\n@@ -308,13 +354,36 @@ impl<K: DepKind> DepGraph<K> {\n     where\n         OP: FnOnce() -> R,\n     {\n+        debug_assert!(!dep_kind.is_eval_always());\n+\n         if let Some(ref data) = self.data {\n             let task_deps = Lock::new(TaskDeps::default());\n-\n             let result = K::with_deps(Some(&task_deps), op);\n             let task_deps = task_deps.into_inner();\n \n-            let dep_node_index = data.current.complete_anon_task(dep_kind, task_deps);\n+            // The dep node indices are hashed here instead of hashing the dep nodes of the\n+            // dependencies. These indices may refer to different nodes per session, but this isn't\n+            // a problem here because we that ensure the final dep node hash is per session only by\n+            // combining it with the per session random number `anon_id_seed`. This hash only need\n+            // to map the dependencies to a single value on a per session basis.\n+            let mut hasher = StableHasher::new();\n+            task_deps.reads.hash(&mut hasher);\n+\n+            let target_dep_node = DepNode {\n+                kind: dep_kind,\n+                // Fingerprint::combine() is faster than sending Fingerprint\n+                // through the StableHasher (at least as long as StableHasher\n+                // is so slow).\n+                hash: data.current.anon_id_seed.combine(hasher.finish()).into(),\n+            };\n+\n+            let dep_node_index = data.current.intern_node(\n+                &data.previous,\n+                target_dep_node,\n+                task_deps.reads,\n+                Fingerprint::ZERO,\n+            );\n+\n             (result, dep_node_index)\n         } else {\n             (op(), self.next_virtual_depnode_index())\n@@ -331,69 +400,104 @@ impl<K: DepKind> DepGraph<K> {\n         task: fn(Ctxt, A) -> R,\n         hash_result: impl FnOnce(&mut Ctxt::StableHashingContext, &R) -> Option<Fingerprint>,\n     ) -> (R, DepNodeIndex) {\n-        self.with_task_impl(\n-            key,\n-            cx,\n-            arg,\n-            false,\n-            task,\n-            |_| None,\n-            |data, key, fingerprint, _| data.alloc_node(key, smallvec![], fingerprint),\n-            hash_result,\n-        )\n+        self.with_task_impl(key, cx, arg, false, task, |_| None, hash_result)\n     }\n \n     #[inline]\n-    pub fn read(&self, v: DepNode<K>) {\n+    pub fn read_index(&self, dep_node_index: DepNodeIndex) {\n         if let Some(ref data) = self.data {\n-            let map = data.current.node_to_node_index.get_shard_by_value(&v).lock();\n-            if let Some(dep_node_index) = map.get(&v).copied() {\n-                std::mem::drop(map);\n-                data.read_index(dep_node_index);\n-            } else {\n-                panic!(\"DepKind {:?} should be pre-allocated but isn't.\", v.kind)\n-            }\n+            K::read_deps(|task_deps| {\n+                if let Some(task_deps) = task_deps {\n+                    let mut task_deps = task_deps.lock();\n+                    let task_deps = &mut *task_deps;\n+                    if cfg!(debug_assertions) {\n+                        data.current.total_read_count.fetch_add(1, Relaxed);\n+                    }\n+\n+                    // As long as we only have a low number of reads we can avoid doing a hash\n+                    // insert and potentially allocating/reallocating the hashmap\n+                    let new_read = if task_deps.reads.len() < TASK_DEPS_READS_CAP {\n+                        task_deps.reads.iter().all(|other| *other != dep_node_index)\n+                    } else {\n+                        task_deps.read_set.insert(dep_node_index)\n+                    };\n+                    if new_read {\n+                        task_deps.reads.push(dep_node_index);\n+                        if task_deps.reads.len() == TASK_DEPS_READS_CAP {\n+                            // Fill `read_set` with what we have so far so we can use the hashset\n+                            // next time\n+                            task_deps.read_set.extend(task_deps.reads.iter().copied());\n+                        }\n+\n+                        #[cfg(debug_assertions)]\n+                        {\n+                            if let Some(target) = task_deps.node {\n+                                if let Some(ref forbidden_edge) = data.current.forbidden_edge {\n+                                    let src = self.dep_node_of(dep_node_index);\n+                                    if forbidden_edge.test(&src, &target) {\n+                                        panic!(\"forbidden edge {:?} -> {:?} created\", src, target)\n+                                    }\n+                                }\n+                            }\n+                        }\n+                    } else if cfg!(debug_assertions) {\n+                        data.current.total_duplicate_read_count.fetch_add(1, Relaxed);\n+                    }\n+                }\n+            })\n         }\n     }\n \n     #[inline]\n-    pub fn read_index(&self, dep_node_index: DepNodeIndex) {\n-        if let Some(ref data) = self.data {\n-            data.read_index(dep_node_index);\n-        }\n+    pub fn dep_node_index_of(&self, dep_node: &DepNode<K>) -> DepNodeIndex {\n+        self.dep_node_index_of_opt(dep_node).unwrap()\n     }\n \n     #[inline]\n-    pub fn dep_node_index_of(&self, dep_node: &DepNode<K>) -> DepNodeIndex {\n-        self.data\n-            .as_ref()\n-            .unwrap()\n-            .current\n-            .node_to_node_index\n-            .get_shard_by_value(dep_node)\n-            .lock()\n-            .get(dep_node)\n-            .cloned()\n-            .unwrap()\n+    pub fn dep_node_index_of_opt(&self, dep_node: &DepNode<K>) -> Option<DepNodeIndex> {\n+        let data = self.data.as_ref().unwrap();\n+        let current = &data.current;\n+\n+        if let Some(prev_index) = data.previous.node_to_index_opt(dep_node) {\n+            current.prev_index_to_index.lock()[prev_index]\n+        } else {\n+            current.new_node_to_index.get_shard_by_value(dep_node).lock().get(dep_node).copied()\n+        }\n     }\n \n     #[inline]\n     pub fn dep_node_exists(&self, dep_node: &DepNode<K>) -> bool {\n-        if let Some(ref data) = self.data {\n-            data.current\n-                .node_to_node_index\n-                .get_shard_by_value(&dep_node)\n-                .lock()\n-                .contains_key(dep_node)\n-        } else {\n-            false\n+        self.data.is_some() && self.dep_node_index_of_opt(dep_node).is_some()\n+    }\n+\n+    #[inline]\n+    pub fn dep_node_of(&self, dep_node_index: DepNodeIndex) -> DepNode<K> {\n+        let data = self.data.as_ref().unwrap();\n+        let previous = &data.previous;\n+        let data = data.current.data.lock();\n+\n+        match data.hybrid_indices[dep_node_index].into() {\n+            HybridIndex::New(new_index) => data.new.nodes[new_index],\n+            HybridIndex::Red(red_index) => previous.index_to_node(data.red.node_indices[red_index]),\n+            HybridIndex::Green(green_index) => {\n+                previous.index_to_node(data.green.node_indices[green_index])\n+            }\n         }\n     }\n \n     #[inline]\n     pub fn fingerprint_of(&self, dep_node_index: DepNodeIndex) -> Fingerprint {\n-        let data = self.data.as_ref().expect(\"dep graph enabled\").current.data.lock();\n-        data[dep_node_index].fingerprint\n+        let data = self.data.as_ref().unwrap();\n+        let previous = &data.previous;\n+        let data = data.current.data.lock();\n+\n+        match data.hybrid_indices[dep_node_index].into() {\n+            HybridIndex::New(new_index) => data.new.fingerprints[new_index],\n+            HybridIndex::Red(red_index) => data.red.fingerprints[red_index],\n+            HybridIndex::Green(green_index) => {\n+                previous.fingerprint_by_index(data.green.node_indices[green_index])\n+            }\n+        }\n     }\n \n     pub fn prev_fingerprint_of(&self, dep_node: &DepNode<K>) -> Option<Fingerprint> {\n@@ -444,29 +548,62 @@ impl<K: DepKind> DepGraph<K> {\n     }\n \n     pub fn serialize(&self) -> SerializedDepGraph<K> {\n-        let data = self.data.as_ref().unwrap().current.data.lock();\n+        type SDNI = SerializedDepNodeIndex;\n \n-        let fingerprints: IndexVec<SerializedDepNodeIndex, _> =\n-            data.iter().map(|d| d.fingerprint).collect();\n-        let nodes: IndexVec<SerializedDepNodeIndex, _> = data.iter().map(|d| d.node).collect();\n+        let data = self.data.as_ref().unwrap();\n+        let previous = &data.previous;\n+        let data = data.current.data.lock();\n \n-        let total_edge_count: usize = data.iter().map(|d| d.edges.len()).sum();\n+        let node_count = data.hybrid_indices.len();\n \n-        let mut edge_list_indices = IndexVec::with_capacity(nodes.len());\n-        let mut edge_list_data = Vec::with_capacity(total_edge_count);\n+        let edge_count = data.new.edges.iter().map(|e| e.len()).sum::<usize>()\n+            + data.red.edges.iter().map(|e| e.len()).sum::<usize>()\n+            + data.green.edges.iter().map(|e| e.len()).sum::<usize>();\n \n-        for (current_dep_node_index, edges) in data.iter_enumerated().map(|(i, d)| (i, &d.edges)) {\n+        let mut nodes = IndexVec::with_capacity(node_count);\n+        let mut fingerprints = IndexVec::with_capacity(node_count);\n+        let mut edge_list_indices = IndexVec::with_capacity(node_count);\n+        let mut edge_list_data = Vec::with_capacity(edge_count);\n+\n+        fn add_edges<'a, I: Iterator<Item = &'a DepNodeIndex>>(\n+            edge_list_indices: &mut IndexVec<SerializedDepNodeIndex, (u32, u32)>,\n+            edge_list_data: &mut Vec<SerializedDepNodeIndex>,\n+            iter: I,\n+        ) {\n             let start = edge_list_data.len() as u32;\n-            // This should really just be a memcpy :/\n-            edge_list_data.extend(edges.iter().map(|i| SerializedDepNodeIndex::new(i.index())));\n+            edge_list_data.extend(iter.map(|i| SDNI::new(i.index())));\n             let end = edge_list_data.len() as u32;\n-\n-            debug_assert_eq!(current_dep_node_index.index(), edge_list_indices.len());\n             edge_list_indices.push((start, end));\n+        };\n+\n+        for &hybrid_index in data.hybrid_indices.iter() {\n+            match hybrid_index.into() {\n+                HybridIndex::New(i) => {\n+                    let new = &data.new;\n+                    nodes.push(new.nodes[i]);\n+                    fingerprints.push(new.fingerprints[i]);\n+                    add_edges(&mut edge_list_indices, &mut edge_list_data, new.edges[i].iter());\n+                }\n+                HybridIndex::Red(i) => {\n+                    let red = &data.red;\n+                    nodes.push(previous.index_to_node(red.node_indices[i]));\n+                    fingerprints.push(red.fingerprints[i]);\n+                    add_edges(&mut edge_list_indices, &mut edge_list_data, red.edges[i].iter());\n+                }\n+                HybridIndex::Green(i) => {\n+                    let green = &data.green;\n+                    nodes.push(previous.index_to_node(green.node_indices[i]));\n+                    fingerprints.push(previous.fingerprint_by_index(green.node_indices[i]));\n+                    add_edges(&mut edge_list_indices, &mut edge_list_data, green.edges[i].iter());\n+                }\n+            }\n         }\n \n+        debug_assert_eq!(nodes.len(), node_count);\n+        debug_assert_eq!(fingerprints.len(), node_count);\n+        debug_assert_eq!(edge_list_indices.len(), node_count);\n+        debug_assert_eq!(edge_list_data.len(), edge_count);\n         debug_assert!(edge_list_data.len() <= u32::MAX as usize);\n-        debug_assert_eq!(edge_list_data.len(), total_edge_count);\n \n         SerializedDepGraph { nodes, fingerprints, edge_list_indices, edge_list_data }\n     }\n@@ -540,14 +677,7 @@ impl<K: DepKind> DepGraph<K> {\n \n         #[cfg(not(parallel_compiler))]\n         {\n-            debug_assert!(\n-                !data\n-                    .current\n-                    .node_to_node_index\n-                    .get_shard_by_value(dep_node)\n-                    .lock()\n-                    .contains_key(dep_node)\n-            );\n+            debug_assert!(!self.dep_node_exists(dep_node));\n             debug_assert!(data.colors.get(prev_dep_node_index).is_none());\n         }\n \n@@ -690,13 +820,9 @@ impl<K: DepKind> DepGraph<K> {\n         // There may be multiple threads trying to mark the same dep node green concurrently\n \n         let dep_node_index = {\n-            // Copy the fingerprint from the previous graph,\n-            // so we don't have to recompute it\n-            let fingerprint = data.previous.fingerprint_by_index(prev_dep_node_index);\n-\n             // We allocating an entry for the node in the current dependency graph and\n             // adding all the appropriate edges imported from the previous graph\n-            data.current.intern_node(*dep_node, current_deps, fingerprint)\n+            data.current.intern_green_node(&data.previous, prev_dep_node_index, current_deps)\n         };\n \n         // ... emitting any stored diagnostic ...\n@@ -871,31 +997,181 @@ pub struct WorkProduct {\n     pub saved_file: Option<String>,\n }\n \n-#[derive(Clone)]\n+// The maximum value of the follow index types leaves the upper two bits unused\n+// so that we can store multiple index types in `CompressedHybridIndex`, and use\n+// those bits to encode which index type it contains.\n+\n+// Index type for `NewDepNodeData`.\n+rustc_index::newtype_index! {\n+    struct NewDepNodeIndex {\n+        MAX = 0x7FFF_FFFF\n+    }\n+}\n+\n+// Index type for `RedDepNodeData`.\n+rustc_index::newtype_index! {\n+    struct RedDepNodeIndex {\n+        MAX = 0x7FFF_FFFF\n+    }\n+}\n+\n+// Index type for `GreenDepNodeData`.\n+rustc_index::newtype_index! {\n+    struct GreenDepNodeIndex {\n+        MAX = 0x7FFF_FFFF\n+    }\n+}\n+\n+/// Compressed representation of `HybridIndex` enum. Bits unused by the\n+/// contained index types are used to encode which index type it contains.\n+#[derive(Copy, Clone)]\n+struct CompressedHybridIndex(u32);\n+\n+impl CompressedHybridIndex {\n+    const NEW_TAG: u32 = 0b0000_0000_0000_0000_0000_0000_0000_0000;\n+    const RED_TAG: u32 = 0b0100_0000_0000_0000_0000_0000_0000_0000;\n+    const GREEN_TAG: u32 = 0b1000_0000_0000_0000_0000_0000_0000_0000;\n+\n+    const TAG_MASK: u32 = 0b1100_0000_0000_0000_0000_0000_0000_0000;\n+    const INDEX_MASK: u32 = !Self::TAG_MASK;\n+}\n+\n+impl From<NewDepNodeIndex> for CompressedHybridIndex {\n+    #[inline]\n+    fn from(index: NewDepNodeIndex) -> Self {\n+        CompressedHybridIndex(Self::NEW_TAG | index.as_u32())\n+    }\n+}\n+\n+impl From<RedDepNodeIndex> for CompressedHybridIndex {\n+    #[inline]\n+    fn from(index: RedDepNodeIndex) -> Self {\n+        CompressedHybridIndex(Self::RED_TAG | index.as_u32())\n+    }\n+}\n+\n+impl From<GreenDepNodeIndex> for CompressedHybridIndex {\n+    #[inline]\n+    fn from(index: GreenDepNodeIndex) -> Self {\n+        CompressedHybridIndex(Self::GREEN_TAG | index.as_u32())\n+    }\n+}\n+\n+/// Contains an index into one of several node data collections. Elsewhere, we\n+/// store `CompressedHyridIndex` instead of this to save space, but convert to\n+/// this type during processing to take advantage of the enum match ergonomics.\n+enum HybridIndex {\n+    New(NewDepNodeIndex),\n+    Red(RedDepNodeIndex),\n+    Green(GreenDepNodeIndex),\n+}\n+\n+impl From<CompressedHybridIndex> for HybridIndex {\n+    #[inline]\n+    fn from(hybrid_index: CompressedHybridIndex) -> Self {\n+        let index = hybrid_index.0 & CompressedHybridIndex::INDEX_MASK;\n+\n+        match hybrid_index.0 & CompressedHybridIndex::TAG_MASK {\n+            CompressedHybridIndex::NEW_TAG => HybridIndex::New(NewDepNodeIndex::from_u32(index)),\n+            CompressedHybridIndex::RED_TAG => HybridIndex::Red(RedDepNodeIndex::from_u32(index)),\n+            CompressedHybridIndex::GREEN_TAG => {\n+                HybridIndex::Green(GreenDepNodeIndex::from_u32(index))\n+            }\n+            _ => unreachable!(),\n+        }\n+    }\n+}\n+\n+/// Data for nodes in the current graph, divided into different collections\n+/// based on their presence in the previous graph, and if present, their color.\n+/// We divide nodes this way because different types of nodes are able to share\n+/// more or less data with the previous graph.\n+///\n+/// Node data is stored in parallel vectors to eliminate the padding between\n+/// elements that would be needed to satisfy alignment requirements of the\n+/// structure that would contain all of a node's data. We could group tightly\n+/// packing subsets of node data together and use fewer vectors, but for\n+/// consistency's sake, we use separate vectors for each piece of data.\n struct DepNodeData<K> {\n-    node: DepNode<K>,\n-    edges: EdgesVec,\n-    fingerprint: Fingerprint,\n+    /// Data for nodes not in previous graph.\n+    new: NewDepNodeData<K>,\n+\n+    /// Data for nodes in previous graph that have been marked red.\n+    red: RedDepNodeData,\n+\n+    /// Data for nodes in previous graph that have been marked green.\n+    green: GreenDepNodeData,\n+\n+    /// Mapping from `DepNodeIndex` to an index into a collection above.\n+    /// Indicates which of the above collections contains a node's data.\n+    ///\n+    /// This collection is wasteful in time and space during incr-full builds,\n+    /// because for those, all nodes are new. However, the waste is relatively\n+    /// small, and the maintenance cost of avoiding using this for incr-full\n+    /// builds is somewhat high and prone to bugginess. It does not seem worth\n+    /// it at the time of this writing, but we may want to revisit the idea.\n+    hybrid_indices: IndexVec<DepNodeIndex, CompressedHybridIndex>,\n+}\n+\n+/// Data for nodes not in previous graph. Since we cannot share any data with\n+/// the previous graph, so we must store all of such a node's data here.\n+struct NewDepNodeData<K> {\n+    nodes: IndexVec<NewDepNodeIndex, DepNode<K>>,\n+    edges: IndexVec<NewDepNodeIndex, EdgesVec>,\n+    fingerprints: IndexVec<NewDepNodeIndex, Fingerprint>,\n+}\n+\n+/// Data for nodes in previous graph that have been marked red. We can share the\n+/// dep node with the previous graph, but the edges may be different, and the\n+/// fingerprint is known to be different, so we store the latter two directly.\n+struct RedDepNodeData {\n+    node_indices: IndexVec<RedDepNodeIndex, SerializedDepNodeIndex>,\n+    edges: IndexVec<RedDepNodeIndex, EdgesVec>,\n+    fingerprints: IndexVec<RedDepNodeIndex, Fingerprint>,\n+}\n+\n+/// Data for nodes in previous graph that have been marked green. We can share\n+/// both the dep node and the fingerprint with previous graph, but the edges may\n+/// be different, so we store the latter directly.\n+struct GreenDepNodeData {\n+    node_indices: IndexVec<GreenDepNodeIndex, SerializedDepNodeIndex>,\n+    edges: IndexVec<GreenDepNodeIndex, EdgesVec>,\n }\n \n-/// `CurrentDepGraph` stores the dependency graph for the current session.\n-/// It will be populated as we run queries or tasks.\n+/// `CurrentDepGraph` stores the dependency graph for the current session. It\n+/// will be populated as we run queries or tasks. We never remove nodes from the\n+/// graph: they are only added.\n ///\n-/// The nodes in it are identified by an index (`DepNodeIndex`).\n-/// The data for each node is stored in its `DepNodeData`, found in the `data` field.\n+/// The nodes in it are identified by a `DepNodeIndex`. Internally, this maps to\n+/// a `HybridIndex`, which identifies which collection in the `data` field\n+/// contains a node's data. Which collection is used for a node depends on\n+/// whether the node was present in the `PreviousDepGraph`, and if so, the color\n+/// of the node. Each type of node can share more or less data with the previous\n+/// graph. When possible, we can store just the index of the node in the\n+/// previous graph, rather than duplicating its data in our own collections.\n+/// This is important, because these graph structures are some of the largest in\n+/// the compiler.\n ///\n-/// We never remove nodes from the graph: they are only added.\n+/// For the same reason, we also avoid storing `DepNode`s more than once as map\n+/// keys. The `new_node_to_index` map only contains nodes not in the previous\n+/// graph, and we map nodes in the previous graph to indices via a two-step\n+/// mapping. `PreviousDepGraph` maps from `DepNode` to `SerializedDepNodeIndex`,\n+/// and the `prev_index_to_index` vector (which is more compact and faster than\n+/// using a map) maps from `SerializedDepNodeIndex` to `DepNodeIndex`.\n ///\n-/// This struct uses two locks internally. The `data` and `node_to_node_index` fields are\n-/// locked separately. Operations that take a `DepNodeIndex` typically just access\n-/// the data field.\n+/// This struct uses three locks internally. The `data`, `new_node_to_index`,\n+/// and `prev_index_to_index` fields are locked separately. Operations that take\n+/// a `DepNodeIndex` typically just access the `data` field.\n ///\n-/// The only operation that must manipulate both locks is adding new nodes, in which case\n-/// we first acquire the `node_to_node_index` lock and then, once a new node is to be inserted,\n-/// acquire the lock on `data.`\n+/// We only need to manipulate at most two locks simultaneously:\n+/// `new_node_to_index` and `data`, or `prev_index_to_index` and `data`. The\n+/// only operation that must manipulate both locks is adding new nodes, in which\n+/// case we first acquire the `new_node_to_index` or `prev_index_to_index` lock\n+/// and then, once a new node is to be inserted, acquire the lock on `data`.\n pub(super) struct CurrentDepGraph<K> {\n-    data: Lock<IndexVec<DepNodeIndex, DepNodeData<K>>>,\n-    node_to_node_index: Sharded<FxHashMap<DepNode<K>, DepNodeIndex>>,\n+    data: Lock<DepNodeData<K>>,\n+    new_node_to_index: Sharded<FxHashMap<DepNode<K>, DepNodeIndex>>,\n+    prev_index_to_index: Lock<IndexVec<SerializedDepNodeIndex, Option<DepNodeIndex>>>,\n \n     /// Used to trap when a specific edge is added to the graph.\n     /// This is used for debug purposes and is only active with `debug_assertions`.\n@@ -944,132 +1220,139 @@ impl<K: DepKind> CurrentDepGraph<K> {\n \n         // Pre-allocate the dep node structures. We over-allocate a little so\n         // that we hopefully don't have to re-allocate during this compilation\n-        // session. The over-allocation is 2% plus a small constant to account\n-        // for the fact that in very small crates 2% might not be enough.\n-        let new_node_count_estimate = (prev_graph_node_count * 102) / 100 + 200;\n+        // session. The over-allocation for new nodes is 2% plus a small\n+        // constant to account for the fact that in very small crates 2% might\n+        // not be enough. The allocation for red and green node data doesn't\n+        // include a constant, as we don't want to allocate anything for these\n+        // structures during full incremental builds, where they aren't used.\n+        let new_node_count_estimate = (prev_graph_node_count * 2) / 100 + 200;\n+        let red_node_count_estimate = (prev_graph_node_count * 3) / 100;\n+        let green_node_count_estimate = (prev_graph_node_count * 95) / 100;\n+        let total_node_count_estimate = prev_graph_node_count + new_node_count_estimate;\n+\n+        // We store a large collection of these in `prev_index_to_index` during\n+        // non-full incremental builds, and want to ensure that the element size\n+        // doesn't inadvertently increase.\n+        static_assert_size!(Option<DepNodeIndex>, 4);\n \n         CurrentDepGraph {\n-            data: Lock::new(IndexVec::with_capacity(new_node_count_estimate)),\n-            node_to_node_index: Sharded::new(|| {\n+            data: Lock::new(DepNodeData {\n+                new: NewDepNodeData {\n+                    nodes: IndexVec::with_capacity(new_node_count_estimate),\n+                    edges: IndexVec::with_capacity(new_node_count_estimate),\n+                    fingerprints: IndexVec::with_capacity(new_node_count_estimate),\n+                },\n+                red: RedDepNodeData {\n+                    node_indices: IndexVec::with_capacity(red_node_count_estimate),\n+                    edges: IndexVec::with_capacity(red_node_count_estimate),\n+                    fingerprints: IndexVec::with_capacity(red_node_count_estimate),\n+                },\n+                green: GreenDepNodeData {\n+                    node_indices: IndexVec::with_capacity(green_node_count_estimate),\n+                    edges: IndexVec::with_capacity(green_node_count_estimate),\n+                },\n+                hybrid_indices: IndexVec::with_capacity(total_node_count_estimate),\n+            }),\n+            new_node_to_index: Sharded::new(|| {\n                 FxHashMap::with_capacity_and_hasher(\n                     new_node_count_estimate / sharded::SHARDS,\n                     Default::default(),\n                 )\n             }),\n+            prev_index_to_index: Lock::new(IndexVec::from_elem_n(None, prev_graph_node_count)),\n             anon_id_seed: stable_hasher.finish(),\n             forbidden_edge,\n             total_read_count: AtomicU64::new(0),\n             total_duplicate_read_count: AtomicU64::new(0),\n         }\n     }\n \n-    fn complete_task(\n-        &self,\n-        node: DepNode<K>,\n-        task_deps: TaskDeps<K>,\n-        fingerprint: Fingerprint,\n-    ) -> DepNodeIndex {\n-        self.alloc_node(node, task_deps.reads, fingerprint)\n-    }\n-\n-    fn complete_anon_task(&self, kind: K, task_deps: TaskDeps<K>) -> DepNodeIndex {\n-        debug_assert!(!kind.is_eval_always());\n-\n-        let mut hasher = StableHasher::new();\n-\n-        // The dep node indices are hashed here instead of hashing the dep nodes of the\n-        // dependencies. These indices may refer to different nodes per session, but this isn't\n-        // a problem here because we that ensure the final dep node hash is per session only by\n-        // combining it with the per session random number `anon_id_seed`. This hash only need\n-        // to map the dependencies to a single value on a per session basis.\n-        task_deps.reads.hash(&mut hasher);\n-\n-        let target_dep_node = DepNode {\n-            kind,\n-\n-            // Fingerprint::combine() is faster than sending Fingerprint\n-            // through the StableHasher (at least as long as StableHasher\n-            // is so slow).\n-            hash: self.anon_id_seed.combine(hasher.finish()).into(),\n-        };\n-\n-        self.intern_node(target_dep_node, task_deps.reads, Fingerprint::ZERO)\n-    }\n-\n-    fn alloc_node(\n+    fn intern_node(\n         &self,\n+        prev_graph: &PreviousDepGraph<K>,\n         dep_node: DepNode<K>,\n         edges: EdgesVec,\n         fingerprint: Fingerprint,\n     ) -> DepNodeIndex {\n         debug_assert!(\n-            !self.node_to_node_index.get_shard_by_value(&dep_node).lock().contains_key(&dep_node)\n+            prev_graph.node_to_index_opt(&dep_node).is_none(),\n+            \"node in previous graph should be interned using \\\n+            `intern_red_node` or `intern_green_node`\"\n         );\n-        self.intern_node(dep_node, edges, fingerprint)\n+\n+        match self.new_node_to_index.get_shard_by_value(&dep_node).lock().entry(dep_node) {\n+            Entry::Occupied(entry) => *entry.get(),\n+            Entry::Vacant(entry) => {\n+                let mut data = self.data.lock();\n+                let new_index = data.new.nodes.push(dep_node);\n+                data.new.edges.push(edges);\n+                data.new.fingerprints.push(fingerprint);\n+                let dep_node_index = data.hybrid_indices.push(new_index.into());\n+                entry.insert(dep_node_index);\n+                dep_node_index\n+            }\n+        }\n     }\n \n-    fn intern_node(\n+    fn intern_red_node(\n         &self,\n-        dep_node: DepNode<K>,\n+        prev_graph: &PreviousDepGraph<K>,\n+        prev_index: SerializedDepNodeIndex,\n         edges: EdgesVec,\n         fingerprint: Fingerprint,\n     ) -> DepNodeIndex {\n-        match self.node_to_node_index.get_shard_by_value(&dep_node).lock().entry(dep_node) {\n-            Entry::Occupied(entry) => *entry.get(),\n-            Entry::Vacant(entry) => {\n+        self.debug_assert_not_in_new_nodes(prev_graph, prev_index);\n+\n+        let mut prev_index_to_index = self.prev_index_to_index.lock();\n+\n+        match prev_index_to_index[prev_index] {\n+            Some(dep_node_index) => dep_node_index,\n+            None => {\n                 let mut data = self.data.lock();\n-                let dep_node_index = DepNodeIndex::new(data.len());\n-                data.push(DepNodeData { node: dep_node, edges, fingerprint });\n-                entry.insert(dep_node_index);\n+                let red_index = data.red.node_indices.push(prev_index);\n+                data.red.edges.push(edges);\n+                data.red.fingerprints.push(fingerprint);\n+                let dep_node_index = data.hybrid_indices.push(red_index.into());\n+                prev_index_to_index[prev_index] = Some(dep_node_index);\n                 dep_node_index\n             }\n         }\n     }\n-}\n \n-impl<K: DepKind> DepGraphData<K> {\n-    #[inline(never)]\n-    fn read_index(&self, source: DepNodeIndex) {\n-        K::read_deps(|task_deps| {\n-            if let Some(task_deps) = task_deps {\n-                let mut task_deps = task_deps.lock();\n-                let task_deps = &mut *task_deps;\n-                if cfg!(debug_assertions) {\n-                    self.current.total_read_count.fetch_add(1, Relaxed);\n-                }\n+    fn intern_green_node(\n+        &self,\n+        prev_graph: &PreviousDepGraph<K>,\n+        prev_index: SerializedDepNodeIndex,\n+        edges: EdgesVec,\n+    ) -> DepNodeIndex {\n+        self.debug_assert_not_in_new_nodes(prev_graph, prev_index);\n \n-                // As long as we only have a low number of reads we can avoid doing a hash\n-                // insert and potentially allocating/reallocating the hashmap\n-                let new_read = if task_deps.reads.len() < TASK_DEPS_READS_CAP {\n-                    task_deps.reads.iter().all(|other| *other != source)\n-                } else {\n-                    task_deps.read_set.insert(source)\n-                };\n-                if new_read {\n-                    task_deps.reads.push(source);\n-                    if task_deps.reads.len() == TASK_DEPS_READS_CAP {\n-                        // Fill `read_set` with what we have so far so we can use the hashset next\n-                        // time\n-                        task_deps.read_set.extend(task_deps.reads.iter().copied());\n-                    }\n+        let mut prev_index_to_index = self.prev_index_to_index.lock();\n \n-                    #[cfg(debug_assertions)]\n-                    {\n-                        if let Some(target) = task_deps.node {\n-                            let data = self.current.data.lock();\n-                            if let Some(ref forbidden_edge) = self.current.forbidden_edge {\n-                                let source = data[source].node;\n-                                if forbidden_edge.test(&source, &target) {\n-                                    panic!(\"forbidden edge {:?} -> {:?} created\", source, target)\n-                                }\n-                            }\n-                        }\n-                    }\n-                } else if cfg!(debug_assertions) {\n-                    self.current.total_duplicate_read_count.fetch_add(1, Relaxed);\n-                }\n+        match prev_index_to_index[prev_index] {\n+            Some(dep_node_index) => dep_node_index,\n+            None => {\n+                let mut data = self.data.lock();\n+                let green_index = data.green.node_indices.push(prev_index);\n+                data.green.edges.push(edges);\n+                let dep_node_index = data.hybrid_indices.push(green_index.into());\n+                prev_index_to_index[prev_index] = Some(dep_node_index);\n+                dep_node_index\n             }\n-        })\n+        }\n+    }\n+\n+    #[inline]\n+    fn debug_assert_not_in_new_nodes(\n+        &self,\n+        prev_graph: &PreviousDepGraph<K>,\n+        prev_index: SerializedDepNodeIndex,\n+    ) {\n+        let node = &prev_graph.index_to_node(prev_index);\n+        debug_assert!(\n+            !self.new_node_to_index.get_shard_by_value(node).lock().contains_key(node),\n+            \"node from previous graph present in new node collection\"\n+        );\n     }\n }\n "}, {"sha": "82e99d64afeb68b3d731102614673ffddb226bb1", "filename": "compiler/rustc_query_system/src/dep_graph/query.rs", "status": "modified", "additions": 4, "deletions": 4, "changes": 8, "blob_url": "https://github.com/rust-lang/rust/blob/f6d6b0c96d86541e8fb69d133ff6222a038e5a53/compiler%2Frustc_query_system%2Fsrc%2Fdep_graph%2Fquery.rs", "raw_url": "https://github.com/rust-lang/rust/raw/f6d6b0c96d86541e8fb69d133ff6222a038e5a53/compiler%2Frustc_query_system%2Fsrc%2Fdep_graph%2Fquery.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_query_system%2Fsrc%2Fdep_graph%2Fquery.rs?ref=f6d6b0c96d86541e8fb69d133ff6222a038e5a53", "patch": "@@ -9,16 +9,16 @@ pub struct DepGraphQuery<K> {\n }\n \n impl<K: DepKind> DepGraphQuery<K> {\n-    pub fn new(nodes: &[DepNode<K>], edges: &[(DepNode<K>, DepNode<K>)]) -> DepGraphQuery<K> {\n+    pub fn new(nodes: &[DepNode<K>], edges: &[(usize, usize)]) -> DepGraphQuery<K> {\n         let mut graph = Graph::with_capacity(nodes.len(), edges.len());\n         let mut indices = FxHashMap::default();\n         for node in nodes {\n             indices.insert(*node, graph.add_node(*node));\n         }\n \n-        for &(ref source, ref target) in edges {\n-            let source = indices[source];\n-            let target = indices[target];\n+        for &(source, target) in edges {\n+            let source = indices[&nodes[source]];\n+            let target = indices[&nodes[target]];\n             graph.add_edge(source, target, ());\n         }\n "}]}