{"sha": "a2b2010891224cbaf448d5d799b3c47c87e863be", "node_id": "C_kwDOAAsO6NoAKGEyYjIwMTA4OTEyMjRjYmFmNDQ4ZDVkNzk5YjNjNDdjODdlODYzYmU", "commit": {"author": {"name": "The 8472", "email": "git@infinite-source.de", "date": "2022-11-03T22:31:00Z"}, "committer": {"name": "The 8472", "email": "git@infinite-source.de", "date": "2022-11-15T17:30:31Z"}, "message": "- convert from core::arch to core::simd\n- bump simd compare to 32bytes\n- import small slice compare code from memmem crate\n- try a few different probe bytes to avoid degenerate cases\n  - but special-case 2-byte needles", "tree": {"sha": "d9374617a8ba44a415ec10859974bdcbfdf2db21", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/d9374617a8ba44a415ec10859974bdcbfdf2db21"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/a2b2010891224cbaf448d5d799b3c47c87e863be", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/a2b2010891224cbaf448d5d799b3c47c87e863be", "html_url": "https://github.com/rust-lang/rust/commit/a2b2010891224cbaf448d5d799b3c47c87e863be", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/a2b2010891224cbaf448d5d799b3c47c87e863be/comments", "author": {"login": "the8472", "id": 1065730, "node_id": "MDQ6VXNlcjEwNjU3MzA=", "avatar_url": "https://avatars.githubusercontent.com/u/1065730?v=4", "gravatar_id": "", "url": "https://api.github.com/users/the8472", "html_url": "https://github.com/the8472", "followers_url": "https://api.github.com/users/the8472/followers", "following_url": "https://api.github.com/users/the8472/following{/other_user}", "gists_url": "https://api.github.com/users/the8472/gists{/gist_id}", "starred_url": "https://api.github.com/users/the8472/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/the8472/subscriptions", "organizations_url": "https://api.github.com/users/the8472/orgs", "repos_url": "https://api.github.com/users/the8472/repos", "events_url": "https://api.github.com/users/the8472/events{/privacy}", "received_events_url": "https://api.github.com/users/the8472/received_events", "type": "User", "site_admin": false}, "committer": {"login": "the8472", "id": 1065730, "node_id": "MDQ6VXNlcjEwNjU3MzA=", "avatar_url": "https://avatars.githubusercontent.com/u/1065730?v=4", "gravatar_id": "", "url": "https://api.github.com/users/the8472", "html_url": "https://github.com/the8472", "followers_url": "https://api.github.com/users/the8472/followers", "following_url": "https://api.github.com/users/the8472/following{/other_user}", "gists_url": "https://api.github.com/users/the8472/gists{/gist_id}", "starred_url": "https://api.github.com/users/the8472/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/the8472/subscriptions", "organizations_url": "https://api.github.com/users/the8472/orgs", "repos_url": "https://api.github.com/users/the8472/repos", "events_url": "https://api.github.com/users/the8472/events{/privacy}", "received_events_url": "https://api.github.com/users/the8472/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "c37e8fae57862383fad43f3201a28b1fb8249904", "url": "https://api.github.com/repos/rust-lang/rust/commits/c37e8fae57862383fad43f3201a28b1fb8249904", "html_url": "https://github.com/rust-lang/rust/commit/c37e8fae57862383fad43f3201a28b1fb8249904"}], "stats": {"total": 234, "additions": 182, "deletions": 52}, "files": [{"sha": "c5be32861f9a5a3441637eabec43400735b03bcc", "filename": "library/core/src/str/pattern.rs", "status": "modified", "additions": 182, "deletions": 52, "changes": 234, "blob_url": "https://github.com/rust-lang/rust/blob/a2b2010891224cbaf448d5d799b3c47c87e863be/library%2Fcore%2Fsrc%2Fstr%2Fpattern.rs", "raw_url": "https://github.com/rust-lang/rust/raw/a2b2010891224cbaf448d5d799b3c47c87e863be/library%2Fcore%2Fsrc%2Fstr%2Fpattern.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/library%2Fcore%2Fsrc%2Fstr%2Fpattern.rs?ref=a2b2010891224cbaf448d5d799b3c47c87e863be", "patch": "@@ -956,15 +956,20 @@ impl<'a, 'b> Pattern<'a> for &'b str {\n \n         match self.len().cmp(&haystack.len()) {\n             Ordering::Less => {\n+                if self.len() == 1 {\n+                    return haystack.as_bytes().contains(&self.as_bytes()[0]);\n+                }\n+\n                 #[cfg(all(target_arch = \"x86_64\", target_feature = \"sse2\"))]\n-                if self.as_bytes().len() <= 8 {\n-                    return simd_contains(self, haystack);\n+                if self.len() <= 32 {\n+                    if let Some(result) = simd_contains(self, haystack) {\n+                        return result;\n+                    }\n                 }\n \n                 self.into_searcher(haystack).next_match().is_some()\n             }\n-            Ordering::Equal => self == haystack,\n-            Ordering::Greater => false,\n+            _ => self == haystack,\n         }\n     }\n \n@@ -1707,82 +1712,207 @@ impl TwoWayStrategy for RejectAndMatch {\n     }\n }\n \n+/// SIMD search for short needles based on\n+/// Wojciech Mu\u0142a's \"SIMD-friendly algorithms for substring searching\"[0]\n+///\n+/// It skips ahead by the vector width on each iteration (rather than the needle length as two-way\n+/// does) by probing the first and last byte of the needle for the whole vector width\n+/// and only doing full needle comparisons when the vectorized probe indicated potential matches.\n+///\n+/// Since the x86_64 baseline only offers SSE2 we only use u8x16 here.\n+/// If we ever ship std with for x86-64-v3 or adapt this for other platforms then wider vectors\n+/// should be evaluated.\n+///\n+/// For haystacks smaller than vector-size + needle length it falls back to\n+/// a naive O(n*m) search so this implementation should not be called on larger needles.\n+///\n+/// [0]: http://0x80.pl/articles/simd-strfind.html#sse-avx2\n #[cfg(all(target_arch = \"x86_64\", target_feature = \"sse2\"))]\n #[inline]\n-fn simd_contains(needle: &str, haystack: &str) -> bool {\n+fn simd_contains(needle: &str, haystack: &str) -> Option<bool> {\n     let needle = needle.as_bytes();\n     let haystack = haystack.as_bytes();\n \n-    if needle.len() == 1 {\n-        return haystack.contains(&needle[0]);\n-    }\n-\n-    const CHUNK: usize = 16;\n+    debug_assert!(needle.len() > 1);\n+\n+    use crate::ops::BitAnd;\n+    use crate::simd::mask8x16 as Mask;\n+    use crate::simd::u8x16 as Block;\n+    use crate::simd::{SimdPartialEq, ToBitMask};\n+\n+    let first_probe = needle[0];\n+\n+    // the offset used for the 2nd vector\n+    let second_probe_offset = if needle.len() == 2 {\n+        // never bail out on len=2 needles because the probes will fully cover them and have\n+        // no degenerate cases.\n+        1\n+    } else {\n+        // try a few bytes in case first and last byte of the needle are the same\n+        let Some(second_probe_offset) = (needle.len().saturating_sub(4)..needle.len()).rfind(|&idx| needle[idx] != first_probe) else {\n+            // fall back to other search methods if we can't find any different bytes\n+            // since we could otherwise hit some degenerate cases\n+            return None;\n+        };\n+        second_probe_offset\n+    };\n \n-    // do a naive search if if the haystack is too small to fit\n-    if haystack.len() < CHUNK + needle.len() - 1 {\n-        return haystack.windows(needle.len()).any(|c| c == needle);\n+    // do a naive search if the haystack is too small to fit\n+    if haystack.len() < Block::LANES + second_probe_offset {\n+        return Some(haystack.windows(needle.len()).any(|c| c == needle));\n     }\n \n-    use crate::arch::x86_64::{\n-        __m128i, _mm_and_si128, _mm_cmpeq_epi8, _mm_loadu_si128, _mm_movemask_epi8, _mm_set1_epi8,\n-    };\n-\n-    // SAFETY: no preconditions other than sse2 being available\n-    let first: __m128i = unsafe { _mm_set1_epi8(needle[0] as i8) };\n-    // SAFETY: no preconditions other than sse2 being available\n-    let last: __m128i = unsafe { _mm_set1_epi8(*needle.last().unwrap() as i8) };\n+    let first_probe: Block = Block::splat(first_probe);\n+    let second_probe: Block = Block::splat(needle[second_probe_offset]);\n+    // first byte are already checked by the outer loop. to verify a match only the\n+    // remainder has to be compared.\n+    let trimmed_needle = &needle[1..];\n \n+    // this #[cold] is load-bearing, benchmark before removing it...\n     let check_mask = #[cold]\n-    |idx, mut mask: u32| -> bool {\n+    |idx, mask: u16, skip: bool| -> bool {\n+        if skip {\n+            return false;\n+        }\n+\n+        // and so is this. optimizations are weird.\n+        let mut mask = mask;\n+\n         while mask != 0 {\n             let trailing = mask.trailing_zeros();\n             let offset = idx + trailing as usize + 1;\n-            let sub = &haystack[offset..][..needle.len() - 2];\n-            let trimmed_needle = &needle[1..needle.len() - 1];\n-\n-            if sub == trimmed_needle {\n-                return true;\n+            // SAFETY: mask is between 0 and 15 trailing zeroes, we skip one additional byte that was already compared\n+            // and then take trimmed_needle.len() bytes. This is within the bounds defined by the outer loop\n+            unsafe {\n+                let sub = haystack.get_unchecked(offset..).get_unchecked(..trimmed_needle.len());\n+                if small_slice_eq(sub, trimmed_needle) {\n+                    return true;\n+                }\n             }\n             mask &= !(1 << trailing);\n         }\n         return false;\n     };\n \n-    let test_chunk = |i| -> bool {\n-        // SAFETY: this requires at least CHUNK bytes being readable at offset i\n+    let test_chunk = |idx| -> u16 {\n+        // SAFETY: this requires at least LANES bytes being readable at idx\n         // that is ensured by the loop ranges (see comments below)\n-        let a: __m128i = unsafe { _mm_loadu_si128(haystack.as_ptr().add(i) as *const _) };\n-        let b: __m128i =\n-            // SAFETY: this requires CHUNK + needle.len() - 1 bytes being readable at offset i\n-            unsafe { _mm_loadu_si128(haystack.as_ptr().add(i + needle.len() - 1) as *const _) };\n-\n-        // SAFETY: no preconditions other than sse2 being available\n-        let eq_first: __m128i = unsafe { _mm_cmpeq_epi8(first, a) };\n-        // SAFETY: no preconditions other than sse2 being available\n-        let eq_last: __m128i = unsafe { _mm_cmpeq_epi8(last, b) };\n-\n-        // SAFETY: no preconditions other than sse2 being available\n-        let mask: u32 = unsafe { _mm_movemask_epi8(_mm_and_si128(eq_first, eq_last)) } as u32;\n+        let a: Block = unsafe { haystack.as_ptr().add(idx).cast::<Block>().read_unaligned() };\n+        // SAFETY: this requires LANES + block_offset bytes being readable at idx\n+        let b: Block = unsafe {\n+            haystack.as_ptr().add(idx).add(second_probe_offset).cast::<Block>().read_unaligned()\n+        };\n+        let eq_first: Mask = a.simd_eq(first_probe);\n+        let eq_last: Mask = b.simd_eq(second_probe);\n+        let both = eq_first.bitand(eq_last);\n+        let mask = both.to_bitmask();\n \n-        if mask != 0 {\n-            return check_mask(i, mask);\n-        }\n-        return false;\n+        return mask;\n     };\n \n     let mut i = 0;\n     let mut result = false;\n-    while !result && i + CHUNK + needle.len() <= haystack.len() {\n-        result |= test_chunk(i);\n-        i += CHUNK;\n+    // The loop condition must ensure that there's enough headroom to read LANE bytes,\n+    // and not only at the current index but also at the index shifted by block_offset\n+    const UNROLL: usize = 4;\n+    while i + second_probe_offset + UNROLL * Block::LANES < haystack.len() && !result {\n+        let mut masks = [0u16; UNROLL];\n+        for j in 0..UNROLL {\n+            masks[j] = test_chunk(i + j * Block::LANES);\n+        }\n+        for j in 0..UNROLL {\n+            let mask = masks[j];\n+            if mask != 0 {\n+                result |= check_mask(i + j * Block::LANES, mask, result);\n+            }\n+        }\n+        i += UNROLL * Block::LANES;\n+    }\n+    while i + second_probe_offset + Block::LANES < haystack.len() && !result {\n+        let mask = test_chunk(i);\n+        if mask != 0 {\n+            result |= check_mask(i, mask, result);\n+        }\n+        i += Block::LANES;\n     }\n \n-    // process the tail that didn't fit into CHUNK-sized steps\n-    // this simply repeats the same procedure but as right-aligned chunk instead\n+    // Process the tail that didn't fit into LANES-sized steps.\n+    // This simply repeats the same procedure but as right-aligned chunk instead\n     // of a left-aligned one. The last byte must be exactly flush with the string end so\n     // we don't miss a single byte or read out of bounds.\n-    result |= test_chunk(haystack.len() + 1 - needle.len() - CHUNK);\n+    let i = haystack.len() - second_probe_offset - Block::LANES;\n+    let mask = test_chunk(i);\n+    if mask != 0 {\n+        result |= check_mask(i, mask, result);\n+    }\n+\n+    Some(result)\n+}\n+\n+/// Compares short slices for equality.\n+///\n+/// It avoids a call to libc's memcmp which is faster on long slices\n+/// due to SIMD optimizations but it incurs a function call overhead.\n+///\n+/// # Safety\n+///\n+/// Both slices must have the same length.\n+#[cfg(all(target_arch = \"x86_64\", target_feature = \"sse2\"))] // only called on x86\n+#[inline]\n+unsafe fn small_slice_eq(x: &[u8], y: &[u8]) -> bool {\n+    // This function is adapted from\n+    // https://github.com/BurntSushi/memchr/blob/8037d11b4357b0f07be2bb66dc2659d9cf28ad32/src/memmem/util.rs#L32\n \n-    return result;\n+    // If we don't have enough bytes to do 4-byte at a time loads, then\n+    // fall back to the naive slow version.\n+    //\n+    // Potential alternative: We could do a copy_nonoverlapping combined with a mask instead\n+    // of a loop. Benchmark it.\n+    if x.len() < 4 {\n+        for (&b1, &b2) in x.iter().zip(y) {\n+            if b1 != b2 {\n+                return false;\n+            }\n+        }\n+        return true;\n+    }\n+    // When we have 4 or more bytes to compare, then proceed in chunks of 4 at\n+    // a time using unaligned loads.\n+    //\n+    // Also, why do 4 byte loads instead of, say, 8 byte loads? The reason is\n+    // that this particular version of memcmp is likely to be called with tiny\n+    // needles. That means that if we do 8 byte loads, then a higher proportion\n+    // of memcmp calls will use the slower variant above. With that said, this\n+    // is a hypothesis and is only loosely supported by benchmarks. There's\n+    // likely some improvement that could be made here. The main thing here\n+    // though is to optimize for latency, not throughput.\n+\n+    // SAFETY: Via the conditional above, we know that both `px` and `py`\n+    // have the same length, so `px < pxend` implies that `py < pyend`.\n+    // Thus, derefencing both `px` and `py` in the loop below is safe.\n+    //\n+    // Moreover, we set `pxend` and `pyend` to be 4 bytes before the actual\n+    // end of of `px` and `py`. Thus, the final dereference outside of the\n+    // loop is guaranteed to be valid. (The final comparison will overlap with\n+    // the last comparison done in the loop for lengths that aren't multiples\n+    // of four.)\n+    //\n+    // Finally, we needn't worry about alignment here, since we do unaligned\n+    // loads.\n+    unsafe {\n+        let (mut px, mut py) = (x.as_ptr(), y.as_ptr());\n+        let (pxend, pyend) = (px.add(x.len() - 4), py.add(y.len() - 4));\n+        while px < pxend {\n+            let vx = (px as *const u32).read_unaligned();\n+            let vy = (py as *const u32).read_unaligned();\n+            if vx != vy {\n+                return false;\n+            }\n+            px = px.add(4);\n+            py = py.add(4);\n+        }\n+        let vx = (pxend as *const u32).read_unaligned();\n+        let vy = (pyend as *const u32).read_unaligned();\n+        vx == vy\n+    }\n }"}]}