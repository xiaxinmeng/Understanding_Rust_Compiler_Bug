{"sha": "ad24976da38482948c586bdbc16004273662ff7e", "node_id": "MDY6Q29tbWl0NzI0NzEyOmFkMjQ5NzZkYTM4NDgyOTQ4YzU4NmJkYmMxNjAwNDI3MzY2MmZmN2U=", "commit": {"author": {"name": "Veetaha", "email": "gerzoh1@gmail.com", "date": "2020-01-24T01:39:23Z"}, "committer": {"name": "Veetaha", "email": "gerzoh1@gmail.com", "date": "2020-02-03T22:00:55Z"}, "message": "ra_syntax: changed added diagnostics information returned from tokenize() (implemented with iterators)", "tree": {"sha": "9a54ae3eb36b123c82634b557df21e0d47848834", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/9a54ae3eb36b123c82634b557df21e0d47848834"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/ad24976da38482948c586bdbc16004273662ff7e", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/ad24976da38482948c586bdbc16004273662ff7e", "html_url": "https://github.com/rust-lang/rust/commit/ad24976da38482948c586bdbc16004273662ff7e", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/ad24976da38482948c586bdbc16004273662ff7e/comments", "author": null, "committer": null, "parents": [{"sha": "b090ee5a65f9630146c2842bc51fcfcc8da08da1", "url": "https://api.github.com/repos/rust-lang/rust/commits/b090ee5a65f9630146c2842bc51fcfcc8da08da1", "html_url": "https://github.com/rust-lang/rust/commit/b090ee5a65f9630146c2842bc51fcfcc8da08da1"}], "stats": {"total": 311, "additions": 237, "deletions": 74}, "files": [{"sha": "acf677e7d78edec061df51cf72441c9dd6196932", "filename": "crates/ra_syntax/src/algo.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/ad24976da38482948c586bdbc16004273662ff7e/crates%2Fra_syntax%2Fsrc%2Falgo.rs", "raw_url": "https://github.com/rust-lang/rust/raw/ad24976da38482948c586bdbc16004273662ff7e/crates%2Fra_syntax%2Fsrc%2Falgo.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fra_syntax%2Fsrc%2Falgo.rs?ref=ad24976da38482948c586bdbc16004273662ff7e", "patch": "@@ -81,7 +81,7 @@ impl TreeDiff {\n /// Specifically, returns a map whose keys are descendants of `from` and values\n /// are descendants of `to`, such that  `replace_descendants(from, map) == to`.\n ///\n-/// A trivial solution is a singletom map `{ from: to }`, but this function\n+/// A trivial solution is a singleton map `{ from: to }`, but this function\n /// tries to find a more fine-grained diff.\n pub fn diff(from: &SyntaxNode, to: &SyntaxNode) -> TreeDiff {\n     let mut buf = FxHashMap::default();"}, {"sha": "9dca7d747ea5ba001d7eae447a99c7ec8a9ddbff", "filename": "crates/ra_syntax/src/parsing/lexer.rs", "status": "modified", "additions": 229, "deletions": 70, "changes": 299, "blob_url": "https://github.com/rust-lang/rust/blob/ad24976da38482948c586bdbc16004273662ff7e/crates%2Fra_syntax%2Fsrc%2Fparsing%2Flexer.rs", "raw_url": "https://github.com/rust-lang/rust/raw/ad24976da38482948c586bdbc16004273662ff7e/crates%2Fra_syntax%2Fsrc%2Fparsing%2Flexer.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fra_syntax%2Fsrc%2Fparsing%2Flexer.rs?ref=ad24976da38482948c586bdbc16004273662ff7e", "patch": "@@ -1,4 +1,6 @@\n-//! FIXME: write short doc here\n+//! Lexer analyzes raw input string and produces lexemes (tokens).\n+\n+use std::iter::{FromIterator, IntoIterator};\n \n use crate::{\n     SyntaxKind::{self, *},\n@@ -13,85 +15,242 @@ pub struct Token {\n     /// The length of the token.\n     pub len: TextUnit,\n }\n+impl Token {\n+    pub const fn new(kind: SyntaxKind, len: TextUnit) -> Self {\n+        Self { kind, len }\n+    }\n+}\n \n-fn match_literal_kind(kind: rustc_lexer::LiteralKind) -> SyntaxKind {\n-    match kind {\n-        rustc_lexer::LiteralKind::Int { .. } => INT_NUMBER,\n-        rustc_lexer::LiteralKind::Float { .. } => FLOAT_NUMBER,\n-        rustc_lexer::LiteralKind::Char { .. } => CHAR,\n-        rustc_lexer::LiteralKind::Byte { .. } => BYTE,\n-        rustc_lexer::LiteralKind::Str { .. } => STRING,\n-        rustc_lexer::LiteralKind::ByteStr { .. } => BYTE_STRING,\n-        rustc_lexer::LiteralKind::RawStr { .. } => RAW_STRING,\n-        rustc_lexer::LiteralKind::RawByteStr { .. } => RAW_BYTE_STRING,\n+#[derive(Debug)]\n+/// Represents the result of parsing one token.\n+pub struct ParsedToken {\n+    /// Parsed token.\n+    pub token: Token,\n+    /// If error is present then parsed token is malformed.\n+    pub error: Option<TokenizeError>,\n+}\n+impl ParsedToken {\n+    pub const fn new(token: Token, error: Option<TokenizeError>) -> Self {\n+        Self { token, error }\n     }\n }\n \n+#[derive(Debug, Default)]\n+/// Represents the result of parsing one token.\n+pub struct ParsedTokens {\n+    /// Parsed token.\n+    pub tokens: Vec<Token>,\n+    /// If error is present then parsed token is malformed.\n+    pub errors: Vec<TokenizeError>,\n+}\n+\n+impl FromIterator<ParsedToken> for ParsedTokens {\n+    fn from_iter<I: IntoIterator<Item = ParsedToken>>(iter: I) -> Self {\n+        let res = Self::default();\n+        for entry in iter {\n+            res.tokens.push(entry.token);\n+            if let Some(error) = entry.error {\n+                res.errors.push(error);\n+            }\n+        }\n+        res\n+    }\n+}\n+\n+/// Returns the first encountered token from the string.\n+/// If the string contains zero or two or more tokens returns `None`.\n+pub fn single_token(text: &str) -> Option<ParsedToken> {\n+    // TODO: test whether this condition indeed checks for a single token\n+    first_token(text).filter(|parsed| parsed.token.len.to_usize() == text.len())\n+}\n+\n+/*\n+/// Returns `ParsedTokens` which are basically a pair `(Vec<Token>, Vec<TokenizeError>)`\n+/// This is just a shorthand for `tokenize(text).collect()`\n+pub fn tokenize_to_vec_with_errors(text: &str) -> ParsedTokens {\n+    tokenize(text).collect()\n+}\n+\n+/// The simplest version of tokenize, it just retunst a ready-made `Vec<Token>`.\n+/// It discards all tokenization errors while parsing. If you need that infromation\n+/// consider using `tokenize()` or `tokenize_to_vec_with_errors()`.\n+pub fn tokenize_to_vec(text: &str) -> Vec<Token> {\n+    tokenize(text).map(|parsed_token| parsed_token.token).collect()\n+}\n+*/\n+\n /// Break a string up into its component tokens\n-pub fn tokenize(text: &str) -> Vec<Token> {\n-    if text.is_empty() {\n-        return vec![];\n+/// This is the core function, all other `tokenize*()` functions are simply\n+/// handy shortcuts for this one.\n+pub fn tokenize(text: &str) -> impl Iterator<Item = ParsedToken> + '_ {\n+    let shebang = rustc_lexer::strip_shebang(text).map(|shebang_len| {\n+        text = &text[shebang_len..];\n+        ParsedToken::new(Token::new(SHEBANG, TextUnit::from_usize(shebang_len)), None)\n+    });\n+\n+    // Notice that we eagerly evaluate shebang since it may change text slice\n+    // and we cannot simplify this into a single method call chain\n+    shebang.into_iter().chain(tokenize_without_shebang(text))\n+}\n+\n+pub fn tokenize_without_shebang(text: &str) -> impl Iterator<Item = ParsedToken> + '_ {\n+    rustc_lexer::tokenize(text).map(|rustc_token| {\n+        let token_text = &text[..rustc_token.len];\n+        text = &text[rustc_token.len..];\n+        rustc_token_kind_to_parsed_token(&rustc_token.kind, token_text)\n+    })\n+}\n+\n+#[derive(Debug)]\n+pub enum TokenizeError {\n+    /// Base prefix was provided, but there were no digits\n+    /// after it, e.g. `0x`.\n+    EmptyInt,\n+    /// Float exponent lacks digits e.g. `e+`, `E+`, `e-`, `E-`,\n+    EmptyExponent,\n+\n+    /// Block comment lacks trailing delimiter `*/`\n+    UnterminatedBlockComment,\n+    /// Character literal lacks trailing delimiter `'`\n+    UnterminatedChar,\n+    /// Characterish byte literal lacks trailing delimiter `'`\n+    UnterminatedByte,\n+    /// String literal lacks trailing delimiter `\"`\n+    UnterminatedString,\n+    /// Byte string literal lacks trailing delimiter `\"`\n+    UnterminatedByteString,\n+    /// Raw literal lacks trailing delimiter e.g. `\"##`\n+    UnterminatedRawString,\n+    /// Raw byte string literal lacks trailing delimiter e.g. `\"##`\n+    UnterminatedRawByteString,\n+\n+    /// Raw string lacks a quote after pound characters e.g. `r###`\n+    UnstartedRawString,\n+    /// Raw byte string lacks a quote after pound characters e.g. `br###`\n+    UnstartedRawByteString,\n+\n+    /// Lifetime starts with a number e.g. `'4ever`\n+    LifetimeStartsWithNumber,\n+}\n+\n+fn rustc_token_kind_to_parsed_token(\n+    rustc_token_kind: &rustc_lexer::TokenKind,\n+    token_text: &str,\n+) -> ParsedToken {\n+    use rustc_lexer::TokenKind as TK;\n+    use TokenizeError as TE;\n+\n+    // We drop some useful infromation here (see patterns with double dots `..`)\n+    // Storing that info in `SyntaxKind` is not possible due to its layout requirements of\n+    // being `u16` that come from `rowan::SyntaxKind` type and changes to `rowan::SyntaxKind`\n+    // would mean hell of a rewrite.\n+\n+    let (syntax_kind, error) = match *rustc_token_kind {\n+        TK::LineComment => ok(COMMENT),\n+        TK::BlockComment { terminated } => ok_if(terminated, COMMENT, TE::UnterminatedBlockComment),\n+        TK::Whitespace => ok(WHITESPACE),\n+        TK::Ident => ok(if token_text == \"_\" {\n+            UNDERSCORE\n+        } else {\n+            SyntaxKind::from_keyword(token_text).unwrap_or(IDENT)\n+        }),\n+        TK::RawIdent => ok(IDENT),\n+        TK::Literal { kind, .. } => match_literal_kind(&kind),\n+        TK::Lifetime { starts_with_number } => {\n+            ok_if(!starts_with_number, LIFETIME, TE::LifetimeStartsWithNumber)\n+        }\n+        TK::Semi => ok(SEMI),\n+        TK::Comma => ok(COMMA),\n+        TK::Dot => ok(DOT),\n+        TK::OpenParen => ok(L_PAREN),\n+        TK::CloseParen => ok(R_PAREN),\n+        TK::OpenBrace => ok(L_CURLY),\n+        TK::CloseBrace => ok(R_CURLY),\n+        TK::OpenBracket => ok(L_BRACK),\n+        TK::CloseBracket => ok(R_BRACK),\n+        TK::At => ok(AT),\n+        TK::Pound => ok(POUND),\n+        TK::Tilde => ok(TILDE),\n+        TK::Question => ok(QUESTION),\n+        TK::Colon => ok(COLON),\n+        TK::Dollar => ok(DOLLAR),\n+        TK::Eq => ok(EQ),\n+        TK::Not => ok(EXCL),\n+        TK::Lt => ok(L_ANGLE),\n+        TK::Gt => ok(R_ANGLE),\n+        TK::Minus => ok(MINUS),\n+        TK::And => ok(AMP),\n+        TK::Or => ok(PIPE),\n+        TK::Plus => ok(PLUS),\n+        TK::Star => ok(STAR),\n+        TK::Slash => ok(SLASH),\n+        TK::Caret => ok(CARET),\n+        TK::Percent => ok(PERCENT),\n+        TK::Unknown => ok(ERROR),\n+    };\n+\n+    return ParsedToken::new(\n+        Token::new(syntax_kind, TextUnit::from_usize(token_text.len())),\n+        error,\n+    );\n+\n+    type ParsedSyntaxKind = (SyntaxKind, Option<TokenizeError>);\n+\n+    const fn ok(syntax_kind: SyntaxKind) -> ParsedSyntaxKind {\n+        (syntax_kind, None)\n     }\n-    let mut text = text;\n-    let mut acc = Vec::new();\n-    if let Some(len) = rustc_lexer::strip_shebang(text) {\n-        acc.push(Token { kind: SHEBANG, len: TextUnit::from_usize(len) });\n-        text = &text[len..];\n+    const fn ok_if(cond: bool, syntax_kind: SyntaxKind, error: TokenizeError) -> ParsedSyntaxKind {\n+        if cond {\n+            ok(syntax_kind)\n+        } else {\n+            err(syntax_kind, error)\n+        }\n     }\n-    while !text.is_empty() {\n-        let rustc_token = rustc_lexer::first_token(text);\n-        let kind = match rustc_token.kind {\n-            rustc_lexer::TokenKind::LineComment => COMMENT,\n-            rustc_lexer::TokenKind::BlockComment { .. } => COMMENT,\n-            rustc_lexer::TokenKind::Whitespace => WHITESPACE,\n-            rustc_lexer::TokenKind::Ident => {\n-                let token_text = &text[..rustc_token.len];\n-                if token_text == \"_\" {\n-                    UNDERSCORE\n-                } else {\n-                    SyntaxKind::from_keyword(&text[..rustc_token.len]).unwrap_or(IDENT)\n-                }\n+    const fn err(syntax_kind: SyntaxKind, error: TokenizeError) -> ParsedSyntaxKind {\n+        (syntax_kind, Some(error))\n+    }\n+\n+    const fn match_literal_kind(kind: &rustc_lexer::LiteralKind) -> ParsedSyntaxKind {\n+        use rustc_lexer::LiteralKind as LK;\n+        match *kind {\n+            LK::Int { empty_int, .. } => ok_if(!empty_int, INT_NUMBER, TE::EmptyInt),\n+            LK::Float { empty_exponent, .. } => {\n+                ok_if(!empty_exponent, FLOAT_NUMBER, TE::EmptyExponent)\n             }\n-            rustc_lexer::TokenKind::RawIdent => IDENT,\n-            rustc_lexer::TokenKind::Literal { kind, .. } => match_literal_kind(kind),\n-            rustc_lexer::TokenKind::Lifetime { .. } => LIFETIME,\n-            rustc_lexer::TokenKind::Semi => SEMI,\n-            rustc_lexer::TokenKind::Comma => COMMA,\n-            rustc_lexer::TokenKind::Dot => DOT,\n-            rustc_lexer::TokenKind::OpenParen => L_PAREN,\n-            rustc_lexer::TokenKind::CloseParen => R_PAREN,\n-            rustc_lexer::TokenKind::OpenBrace => L_CURLY,\n-            rustc_lexer::TokenKind::CloseBrace => R_CURLY,\n-            rustc_lexer::TokenKind::OpenBracket => L_BRACK,\n-            rustc_lexer::TokenKind::CloseBracket => R_BRACK,\n-            rustc_lexer::TokenKind::At => AT,\n-            rustc_lexer::TokenKind::Pound => POUND,\n-            rustc_lexer::TokenKind::Tilde => TILDE,\n-            rustc_lexer::TokenKind::Question => QUESTION,\n-            rustc_lexer::TokenKind::Colon => COLON,\n-            rustc_lexer::TokenKind::Dollar => DOLLAR,\n-            rustc_lexer::TokenKind::Eq => EQ,\n-            rustc_lexer::TokenKind::Not => EXCL,\n-            rustc_lexer::TokenKind::Lt => L_ANGLE,\n-            rustc_lexer::TokenKind::Gt => R_ANGLE,\n-            rustc_lexer::TokenKind::Minus => MINUS,\n-            rustc_lexer::TokenKind::And => AMP,\n-            rustc_lexer::TokenKind::Or => PIPE,\n-            rustc_lexer::TokenKind::Plus => PLUS,\n-            rustc_lexer::TokenKind::Star => STAR,\n-            rustc_lexer::TokenKind::Slash => SLASH,\n-            rustc_lexer::TokenKind::Caret => CARET,\n-            rustc_lexer::TokenKind::Percent => PERCENT,\n-            rustc_lexer::TokenKind::Unknown => ERROR,\n-        };\n-        let token = Token { kind, len: TextUnit::from_usize(rustc_token.len) };\n-        acc.push(token);\n-        text = &text[rustc_token.len..];\n+            LK::Char { terminated } => ok_if(terminated, CHAR, TE::UnterminatedChar),\n+            LK::Byte { terminated } => ok_if(terminated, BYTE, TE::UnterminatedByte),\n+            LK::Str { terminated } => ok_if(terminated, STRING, TE::UnterminatedString),\n+            LK::ByteStr { terminated } => {\n+                ok_if(terminated, BYTE_STRING, TE::UnterminatedByteString)\n+            }\n+\n+            LK::RawStr { started: true, terminated, .. } => {\n+                ok_if(terminated, RAW_STRING, TE::UnterminatedRawString)\n+            }\n+            LK::RawStr { started: false, .. } => err(RAW_STRING, TE::UnstartedRawString),\n+\n+            LK::RawByteStr { started: true, terminated, .. } => {\n+                ok_if(terminated, RAW_BYTE_STRING, TE::UnterminatedRawByteString)\n+            }\n+            LK::RawByteStr { started: false, .. } => {\n+                err(RAW_BYTE_STRING, TE::UnstartedRawByteString)\n+            }\n+        }\n+    }\n+}\n+\n+pub fn first_token(text: &str) -> Option<ParsedToken> {\n+    // Checking for emptyness because of `rustc_lexer::first_token()` invariant (see its body)\n+    if text.is_empty() {\n+        None\n+    } else {\n+        let rustc_token = rustc_lexer::first_token(text);\n+        Some(rustc_token_kind_to_parsed_token(&rustc_token.kind, &text[..rustc_token.len]))\n     }\n-    acc\n }\n \n-pub fn classify_literal(text: &str) -> Option<Token> {\n+// TODO: think what to do with this ad hoc function\n+pub fn classify_literal(text: &str) -> Option<ParsedToken> {\n     let t = rustc_lexer::first_token(text);\n     if t.len != text.len() {\n         return None;\n@@ -100,5 +259,5 @@ pub fn classify_literal(text: &str) -> Option<Token> {\n         rustc_lexer::TokenKind::Literal { kind, .. } => match_literal_kind(kind),\n         _ => return None,\n     };\n-    Some(Token { kind, len: TextUnit::from_usize(t.len) })\n+    Some(ParsedToken::new(Token::new(kind, TextUnit::from_usize(t.len))))\n }"}, {"sha": "3abc0987735a5fa6b472b2b2e0d89d4b2716843d", "filename": "crates/ra_syntax/src/parsing/reparsing.rs", "status": "modified", "additions": 1, "deletions": 2, "changes": 3, "blob_url": "https://github.com/rust-lang/rust/blob/ad24976da38482948c586bdbc16004273662ff7e/crates%2Fra_syntax%2Fsrc%2Fparsing%2Freparsing.rs", "raw_url": "https://github.com/rust-lang/rust/raw/ad24976da38482948c586bdbc16004273662ff7e/crates%2Fra_syntax%2Fsrc%2Fparsing%2Freparsing.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fra_syntax%2Fsrc%2Fparsing%2Freparsing.rs?ref=ad24976da38482948c586bdbc16004273662ff7e", "patch": "@@ -46,8 +46,7 @@ fn reparse_token<'node>(\n         WHITESPACE | COMMENT | IDENT | STRING | RAW_STRING => {\n             if token.kind() == WHITESPACE || token.kind() == COMMENT {\n                 // removing a new line may extends previous token\n-                if token.text().to_string()[edit.delete - token.text_range().start()].contains('\\n')\n-                {\n+                if token.text()[edit.delete - token.text_range().start()].contains('\\n') {\n                     return None;\n                 }\n             }"}, {"sha": "9122dda29e04ab8e53a02c541e9e344f217af2a7", "filename": "crates/ra_syntax/src/syntax_error.rs", "status": "modified", "additions": 6, "deletions": 1, "changes": 7, "blob_url": "https://github.com/rust-lang/rust/blob/ad24976da38482948c586bdbc16004273662ff7e/crates%2Fra_syntax%2Fsrc%2Fsyntax_error.rs", "raw_url": "https://github.com/rust-lang/rust/raw/ad24976da38482948c586bdbc16004273662ff7e/crates%2Fra_syntax%2Fsrc%2Fsyntax_error.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fra_syntax%2Fsrc%2Fsyntax_error.rs?ref=ad24976da38482948c586bdbc16004273662ff7e", "patch": "@@ -4,14 +4,18 @@ use std::fmt;\n \n use ra_parser::ParseError;\n \n-use crate::{validation::EscapeError, TextRange, TextUnit};\n+use crate::{validation::EscapeError, TextRange, TextUnit, TokenizeError};\n \n #[derive(Debug, Clone, PartialEq, Eq, Hash)]\n pub struct SyntaxError {\n     kind: SyntaxErrorKind,\n     location: Location,\n }\n \n+// FIXME: Location should be just `Location(TextRange)`\n+// TextUnit enum member just unnecessarily compicates things,\n+// we should'n treat it specially, it just as a `TextRange { start: x, end: x + 1 }`\n+// see `location_to_range()` in ra_ide/src/diagnostics\n #[derive(Clone, PartialEq, Eq, Hash)]\n pub enum Location {\n     Offset(TextUnit),\n@@ -79,6 +83,7 @@ impl fmt::Display for SyntaxError {\n pub enum SyntaxErrorKind {\n     ParseError(ParseError),\n     EscapeError(EscapeError),\n+    TokenizeError(TokenizeError),\n     InvalidBlockAttr,\n     InvalidMatchInnerAttr,\n     InvalidTupleIndexFormat,"}]}