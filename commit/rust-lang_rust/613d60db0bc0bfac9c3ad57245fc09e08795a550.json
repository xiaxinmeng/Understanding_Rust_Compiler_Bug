{"sha": "613d60db0bc0bfac9c3ad57245fc09e08795a550", "node_id": "C_kwDOAAsO6NoAKDYxM2Q2MGRiMGJjMGJmYWM5YzNhZDU3MjQ1ZmMwOWUwODc5NWE1NTA", "commit": {"author": {"name": "Andy Wang", "email": "cbeuw.andy@gmail.com", "date": "2022-05-25T19:46:08Z"}, "committer": {"name": "Andy Wang", "email": "cbeuw.andy@gmail.com", "date": "2022-06-06T18:15:58Z"}, "message": "Allow non-racy mixed size accesses", "tree": {"sha": "fc25c13cf768061e7dc987e77c170cf405d17bce", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/fc25c13cf768061e7dc987e77c170cf405d17bce"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/613d60db0bc0bfac9c3ad57245fc09e08795a550", "comment_count": 0, "verification": {"verified": true, "reason": "valid", "signature": "-----BEGIN PGP SIGNATURE-----\n\niQGzBAABCgAdFiEE7dcbcBMl24/h63ldGBtJ+fOPM3QFAmKeRF4ACgkQGBtJ+fOP\nM3RLeQv/dQIx6OjxsVCnOiSIpwDYwb+gr/X/tgo+4FwM9iutnkVQRKKdAk4AT9WR\ncP7F6ClScyzjz/Ys2NdLeh4rEgSWigDN/Sqg9PCDZct4avcmwZa2fK6OuAuBo/To\n/qr+bR99gw9Ma67g3uNeWNFKXyMRL5caF+99GEQjwS69bT4Y423x9f+on8A+Eicx\n+WMdu2ae/7ibCPTPep0hWtU/X4+cZ8/LruZjQDYROsGMa+FXu3GmsbuSwIjsWQQq\nuV7eo8D0GDodEurGnvjXHJmrL2hsHXj2tmPgVJRrhsWyYMCSfEI0y1+7IU74V3p7\nDpb9TTS2/83sNPLqAkZAggpIv0sd4Q+lQmaPjXfAUUTHvGxPOBR1cEBPq4NSMb/g\ns76Qv9zM71vP/qxaX85+2gTXR/bdeEnyTNPJNuSirGO2K0+5tT/jwU5OxCeLLH6C\nm2UP15RNbnclzaduoDKS2IwGPHTCiKCaygbrcl0z4ti1qiExEs5CmTWXMFfzwZUL\nXOjsF6Yh\n=LOoJ\n-----END PGP SIGNATURE-----", "payload": "tree fc25c13cf768061e7dc987e77c170cf405d17bce\nparent 226ed41cca4cf83cd6afc18f0d3ce4ef2cdc8691\nauthor Andy Wang <cbeuw.andy@gmail.com> 1653507968 +0100\ncommitter Andy Wang <cbeuw.andy@gmail.com> 1654539358 +0100\n\nAllow non-racy mixed size accesses\n"}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/613d60db0bc0bfac9c3ad57245fc09e08795a550", "html_url": "https://github.com/rust-lang/rust/commit/613d60db0bc0bfac9c3ad57245fc09e08795a550", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/613d60db0bc0bfac9c3ad57245fc09e08795a550/comments", "author": {"login": "cbeuw", "id": 7034308, "node_id": "MDQ6VXNlcjcwMzQzMDg=", "avatar_url": "https://avatars.githubusercontent.com/u/7034308?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cbeuw", "html_url": "https://github.com/cbeuw", "followers_url": "https://api.github.com/users/cbeuw/followers", "following_url": "https://api.github.com/users/cbeuw/following{/other_user}", "gists_url": "https://api.github.com/users/cbeuw/gists{/gist_id}", "starred_url": "https://api.github.com/users/cbeuw/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cbeuw/subscriptions", "organizations_url": "https://api.github.com/users/cbeuw/orgs", "repos_url": "https://api.github.com/users/cbeuw/repos", "events_url": "https://api.github.com/users/cbeuw/events{/privacy}", "received_events_url": "https://api.github.com/users/cbeuw/received_events", "type": "User", "site_admin": false}, "committer": {"login": "cbeuw", "id": 7034308, "node_id": "MDQ6VXNlcjcwMzQzMDg=", "avatar_url": "https://avatars.githubusercontent.com/u/7034308?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cbeuw", "html_url": "https://github.com/cbeuw", "followers_url": "https://api.github.com/users/cbeuw/followers", "following_url": "https://api.github.com/users/cbeuw/following{/other_user}", "gists_url": "https://api.github.com/users/cbeuw/gists{/gist_id}", "starred_url": "https://api.github.com/users/cbeuw/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cbeuw/subscriptions", "organizations_url": "https://api.github.com/users/cbeuw/orgs", "repos_url": "https://api.github.com/users/cbeuw/repos", "events_url": "https://api.github.com/users/cbeuw/events{/privacy}", "received_events_url": "https://api.github.com/users/cbeuw/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "226ed41cca4cf83cd6afc18f0d3ce4ef2cdc8691", "url": "https://api.github.com/repos/rust-lang/rust/commits/226ed41cca4cf83cd6afc18f0d3ce4ef2cdc8691", "html_url": "https://github.com/rust-lang/rust/commit/226ed41cca4cf83cd6afc18f0d3ce4ef2cdc8691"}], "stats": {"total": 288, "additions": 262, "deletions": 26}, "files": [{"sha": "8b8694ac18352434b86bd9fcf745ba8b496799b7", "filename": "src/concurrency/data_race.rs", "status": "modified", "additions": 62, "deletions": 1, "changes": 63, "blob_url": "https://github.com/rust-lang/rust/blob/613d60db0bc0bfac9c3ad57245fc09e08795a550/src%2Fconcurrency%2Fdata_race.rs", "raw_url": "https://github.com/rust-lang/rust/raw/613d60db0bc0bfac9c3ad57245fc09e08795a550/src%2Fconcurrency%2Fdata_race.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Fconcurrency%2Fdata_race.rs?ref=613d60db0bc0bfac9c3ad57245fc09e08795a550", "patch": "@@ -287,6 +287,20 @@ impl MemoryCellClocks {\n         Ok(())\n     }\n \n+    /// Checks if the memory cell write races with any prior atomic read or write\n+    fn write_race_free_with_atomic(&mut self, clocks: &ThreadClockSet) -> bool {\n+        if let Some(atomic) = self.atomic() {\n+            atomic.read_vector <= clocks.clock && atomic.write_vector <= clocks.clock\n+        } else {\n+            true\n+        }\n+    }\n+\n+    /// Checks if the memory cell read races with any prior atomic write\n+    fn read_race_free_with_atomic(&self, clocks: &ThreadClockSet) -> bool {\n+        if let Some(atomic) = self.atomic() { atomic.write_vector <= clocks.clock } else { true }\n+    }\n+\n     /// Update memory cell data-race tracking for atomic\n     /// load relaxed semantics, is a no-op if this memory was\n     /// not used previously as atomic memory.\n@@ -514,6 +528,7 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: MiriEvalContextExt<'mir, 'tcx> {\n         // the *value* (including the associated provenance if this is an AtomicPtr) at this location.\n         // Only metadata on the location itself is used.\n         let scalar = this.allow_data_races_ref(move |this| this.read_scalar(&place.into()))?;\n+        this.validate_overlapping_atomic_read(place)?;\n         this.buffered_atomic_read(place, atomic, scalar, || {\n             this.validate_atomic_load(place, atomic)\n         })\n@@ -527,6 +542,7 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: MiriEvalContextExt<'mir, 'tcx> {\n         atomic: AtomicWriteOp,\n     ) -> InterpResult<'tcx> {\n         let this = self.eval_context_mut();\n+        this.validate_overlapping_atomic_write(dest)?;\n         this.allow_data_races_mut(move |this| this.write_scalar(val, &(*dest).into()))?;\n         this.validate_atomic_store(dest, atomic)?;\n         // FIXME: it's not possible to get the value before write_scalar. A read_scalar will cause\n@@ -547,6 +563,7 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: MiriEvalContextExt<'mir, 'tcx> {\n     ) -> InterpResult<'tcx, ImmTy<'tcx, Tag>> {\n         let this = self.eval_context_mut();\n \n+        this.validate_overlapping_atomic_write(place)?;\n         let old = this.allow_data_races_mut(|this| this.read_immediate(&place.into()))?;\n \n         // Atomics wrap around on overflow.\n@@ -575,6 +592,7 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: MiriEvalContextExt<'mir, 'tcx> {\n     ) -> InterpResult<'tcx, ScalarMaybeUninit<Tag>> {\n         let this = self.eval_context_mut();\n \n+        this.validate_overlapping_atomic_write(place)?;\n         let old = this.allow_data_races_mut(|this| this.read_scalar(&place.into()))?;\n         this.allow_data_races_mut(|this| this.write_scalar(new, &(*place).into()))?;\n \n@@ -595,6 +613,7 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: MiriEvalContextExt<'mir, 'tcx> {\n     ) -> InterpResult<'tcx, ImmTy<'tcx, Tag>> {\n         let this = self.eval_context_mut();\n \n+        this.validate_overlapping_atomic_write(place)?;\n         let old = this.allow_data_races_mut(|this| this.read_immediate(&place.into()))?;\n         let lt = this.binary_op(mir::BinOp::Lt, &old, &rhs)?.to_scalar()?.to_bool()?;\n \n@@ -637,6 +656,7 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: MiriEvalContextExt<'mir, 'tcx> {\n         use rand::Rng as _;\n         let this = self.eval_context_mut();\n \n+        this.validate_overlapping_atomic_write(place)?;\n         // Failure ordering cannot be stronger than success ordering, therefore first attempt\n         // to read with the failure ordering and if successful then try again with the success\n         // read ordering and write in the success case.\n@@ -686,6 +706,7 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: MiriEvalContextExt<'mir, 'tcx> {\n         atomic: AtomicReadOp,\n     ) -> InterpResult<'tcx> {\n         let this = self.eval_context_ref();\n+        this.validate_overlapping_atomic_read(place)?;\n         this.validate_atomic_op(\n             place,\n             atomic,\n@@ -708,6 +729,7 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: MiriEvalContextExt<'mir, 'tcx> {\n         atomic: AtomicWriteOp,\n     ) -> InterpResult<'tcx> {\n         let this = self.eval_context_mut();\n+        this.validate_overlapping_atomic_write(place)?;\n         this.validate_atomic_op(\n             place,\n             atomic,\n@@ -733,6 +755,7 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: MiriEvalContextExt<'mir, 'tcx> {\n         let acquire = matches!(atomic, Acquire | AcqRel | SeqCst);\n         let release = matches!(atomic, Release | AcqRel | SeqCst);\n         let this = self.eval_context_mut();\n+        this.validate_overlapping_atomic_write(place)?;\n         this.validate_atomic_op(place, atomic, \"Atomic RMW\", move |memory, clocks, index, _| {\n             if acquire {\n                 memory.load_acquire(clocks, index)?;\n@@ -918,6 +941,44 @@ impl VClockAlloc {\n         )\n     }\n \n+    /// Detect racing atomic writes (not data races)\n+    /// on every byte of the current access range\n+    pub(super) fn read_race_free_with_atomic<'tcx>(\n+        &self,\n+        range: AllocRange,\n+        global: &GlobalState,\n+    ) -> bool {\n+        if global.race_detecting() {\n+            let (_, clocks) = global.current_thread_state();\n+            let alloc_ranges = self.alloc_ranges.borrow();\n+            for (_, range) in alloc_ranges.iter(range.start, range.size) {\n+                if !range.read_race_free_with_atomic(&clocks) {\n+                    return false;\n+                }\n+            }\n+        }\n+        true\n+    }\n+\n+    /// Detect racing atomic read and writes (not data races)\n+    /// on every byte of the current access range\n+    pub(super) fn write_race_free_with_atomic<'tcx>(\n+        &mut self,\n+        range: AllocRange,\n+        global: &GlobalState,\n+    ) -> bool {\n+        if global.race_detecting() {\n+            let (_, clocks) = global.current_thread_state();\n+            let alloc_ranges = self.alloc_ranges.get_mut();\n+            for (_, range) in alloc_ranges.iter_mut(range.start, range.size) {\n+                if !range.write_race_free_with_atomic(&clocks) {\n+                    return false;\n+                }\n+            }\n+        }\n+        true\n+    }\n+\n     /// Detect data-races for an unsynchronized read operation, will not perform\n     /// data-race detection if `race_detecting()` is false, either due to no threads\n     /// being created or if it is temporarily disabled during a racy read or write\n@@ -1027,7 +1088,7 @@ trait EvalContextPrivExt<'mir, 'tcx: 'mir>: MiriEvalContextExt<'mir, 'tcx> {\n                 let (alloc_id, base_offset, _tag) = this.ptr_get_alloc_id(place.ptr)?;\n                 // Load and log the atomic operation.\n                 // Note that atomic loads are possible even from read-only allocations, so `get_alloc_extra_mut` is not an option.\n-                let alloc_meta = &this.get_alloc_extra(alloc_id)?.data_race.as_ref().unwrap();\n+                let alloc_meta = this.get_alloc_extra(alloc_id)?.data_race.as_ref().unwrap();\n                 log::trace!(\n                     \"Atomic op({}) with ordering {:?} on {:?} (size={})\",\n                     description,"}, {"sha": "a4fbd14f4374e08729668208f1ce6088cf9fbe9b", "filename": "src/concurrency/weak_memory.rs", "status": "modified", "additions": 74, "deletions": 18, "changes": 92, "blob_url": "https://github.com/rust-lang/rust/blob/613d60db0bc0bfac9c3ad57245fc09e08795a550/src%2Fconcurrency%2Fweak_memory.rs", "raw_url": "https://github.com/rust-lang/rust/raw/613d60db0bc0bfac9c3ad57245fc09e08795a550/src%2Fconcurrency%2Fweak_memory.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Fconcurrency%2Fweak_memory.rs?ref=613d60db0bc0bfac9c3ad57245fc09e08795a550", "patch": "@@ -29,6 +29,13 @@\n //! Additionally, writes in our implementation do not have globally unique timestamps attached. In the other two models this timestamp is\n //! used to make sure a value in a thread's view is not overwritten by a write that occured earlier than the one in the existing view.\n //! In our implementation, this is detected using read information attached to store elements, as there is no data strucutre representing reads.\n+//!\n+//! Safe/sound Rust allows for more operations on atomic locations than the C++20 atomic API was intended to allow, such as non-atomically accessing\n+//! a previously atomically accessed location, or accessing previously atomically accessed locations with a differently sized operation\n+//! (such as accessing the top 16 bits of an AtomicU32). These senarios are generally undiscussed in formalisations of C++ memory model.\n+//! In Rust, these operations can only be done through a `&mut AtomicFoo` reference or one derived from it, therefore these operations\n+//! can only happen after all previous accesses on the same locations. This implementation is adapted to allow these operations.\n+//! A mixed size/atomicity read that races with writes, or a write that races with reads or writes will still cause UBs to be thrown.\n \n // Our and the author's own implementation (tsan11) of the paper have some deviations from the provided operational semantics in \u00a75.3:\n // 1. In the operational semantics, store elements keep a copy of the atomic object's vector clock (AtomicCellClocks::sync_vector in miri),\n@@ -117,6 +124,14 @@ impl StoreBufferAlloc {\n         Self { store_buffers: RefCell::new(AllocationMap::new()) }\n     }\n \n+    /// Checks if the range imperfectly overlaps with existing buffers\n+    /// Used to determine if mixed-size atomic accesses\n+    fn is_overlapping(&self, range: AllocRange) -> bool {\n+        let buffers = self.store_buffers.borrow();\n+        let access_type = buffers.access_type(range);\n+        matches!(access_type, AccessType::ImperfectlyOverlapping(_))\n+    }\n+\n     /// When a non-atomic access happens on a location that has been atomically accessed\n     /// before without data race, we can determine that the non-atomic access fully happens\n     /// before all the prior atomic accesses so the location no longer needs to exhibit\n@@ -148,21 +163,16 @@ impl StoreBufferAlloc {\n         let pos = match access_type {\n             AccessType::PerfectlyOverlapping(pos) => pos,\n             AccessType::Empty(pos) => {\n-                let new_buffer = StoreBuffer::new(init);\n                 let mut buffers = self.store_buffers.borrow_mut();\n-                buffers.insert_at_pos(pos, range, new_buffer);\n+                buffers.insert_at_pos(pos, range, StoreBuffer::new(init));\n                 pos\n             }\n             AccessType::ImperfectlyOverlapping(pos_range) => {\n-                // Accesses that imperfectly overlaps with existing atomic objects\n-                // do not have well-defined behaviours.\n-                // FIXME: if this access happens before all previous accesses on every object it overlaps\n-                // with, then we would like to tolerate it. However this is not easy to check.\n-                if pos_range.start + 1 == pos_range.end {\n-                    throw_ub_format!(\"mixed-size access on an existing atomic object\");\n-                } else {\n-                    throw_ub_format!(\"access overlaps with multiple existing atomic objects\");\n-                }\n+                // Once we reach here we would've already checked that this access is not racy\n+                let mut buffers = self.store_buffers.borrow_mut();\n+                buffers.remove_pos_range(pos_range.clone());\n+                buffers.insert_at_pos(pos_range.start, range, StoreBuffer::new(init));\n+                pos_range.start\n             }\n         };\n         Ok(Ref::map(self.store_buffers.borrow(), |buffer| &buffer[pos]))\n@@ -179,16 +189,13 @@ impl StoreBufferAlloc {\n         let pos = match access_type {\n             AccessType::PerfectlyOverlapping(pos) => pos,\n             AccessType::Empty(pos) => {\n-                let new_buffer = StoreBuffer::new(init);\n-                buffers.insert_at_pos(pos, range, new_buffer);\n+                buffers.insert_at_pos(pos, range, StoreBuffer::new(init));\n                 pos\n             }\n             AccessType::ImperfectlyOverlapping(pos_range) => {\n-                if pos_range.start + 1 == pos_range.end {\n-                    throw_ub_format!(\"mixed-size access on an existing atomic object\");\n-                } else {\n-                    throw_ub_format!(\"access overlaps with multiple existing atomic objects\");\n-                }\n+                buffers.remove_pos_range(pos_range.clone());\n+                buffers.insert_at_pos(pos_range.start, range, StoreBuffer::new(init));\n+                pos_range.start\n             }\n         };\n         Ok(&mut buffers[pos])\n@@ -392,6 +399,55 @@ impl<'mir, 'tcx: 'mir> EvalContextExt<'mir, 'tcx> for crate::MiriEvalContext<'mi\n pub(super) trait EvalContextExt<'mir, 'tcx: 'mir>:\n     crate::MiriEvalContextExt<'mir, 'tcx>\n {\n+    // If weak memory emulation is enabled, check if this atomic op imperfectly overlaps with a previous\n+    // atomic write. If it does, then we require it to be ordered (non-racy) with all previous atomic\n+    // writes on all the bytes in range\n+    fn validate_overlapping_atomic_read(&self, place: &MPlaceTy<'tcx, Tag>) -> InterpResult<'tcx> {\n+        let this = self.eval_context_ref();\n+        let (alloc_id, base_offset, ..) = this.ptr_get_alloc_id(place.ptr)?;\n+        if let crate::AllocExtra {\n+            weak_memory: Some(alloc_buffers),\n+            data_race: Some(alloc_clocks),\n+            ..\n+        } = this.get_alloc_extra(alloc_id)?\n+        {\n+            let range = alloc_range(base_offset, place.layout.size);\n+            if alloc_buffers.is_overlapping(range)\n+                && !alloc_clocks\n+                    .read_race_free_with_atomic(range, this.machine.data_race.as_ref().unwrap())\n+            {\n+                throw_ub_format!(\"racy imperfectly overlapping atomic access\");\n+            }\n+        }\n+        Ok(())\n+    }\n+\n+    // Same as above but needs to be ordered with all previous atomic read or writes\n+    fn validate_overlapping_atomic_write(\n+        &mut self,\n+        place: &MPlaceTy<'tcx, Tag>,\n+    ) -> InterpResult<'tcx> {\n+        let this = self.eval_context_mut();\n+        let (alloc_id, base_offset, ..) = this.ptr_get_alloc_id(place.ptr)?;\n+        if let (\n+            crate::AllocExtra {\n+                weak_memory: Some(alloc_buffers),\n+                data_race: Some(alloc_clocks),\n+                ..\n+            },\n+            crate::Evaluator { data_race: Some(global), .. },\n+        ) = this.get_alloc_extra_mut(alloc_id)?\n+        {\n+            let range = alloc_range(base_offset, place.layout.size);\n+            if alloc_buffers.is_overlapping(range)\n+                && !alloc_clocks.write_race_free_with_atomic(range, global)\n+            {\n+                throw_ub_format!(\"racy imperfectly overlapping atomic access\");\n+            }\n+        }\n+        Ok(())\n+    }\n+\n     fn buffered_atomic_rmw(\n         &mut self,\n         new_val: ScalarMaybeUninit<Tag>,"}, {"sha": "7dad0a12e5ddc5a101128eec7a4bf70adf20f761", "filename": "tests/compile-fail/weak_memory/cpp20_rwc_syncs.rs", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "blob_url": "https://github.com/rust-lang/rust/blob/613d60db0bc0bfac9c3ad57245fc09e08795a550/tests%2Fcompile-fail%2Fweak_memory%2Fcpp20_rwc_syncs.rs", "raw_url": "https://github.com/rust-lang/rust/raw/613d60db0bc0bfac9c3ad57245fc09e08795a550/tests%2Fcompile-fail%2Fweak_memory%2Fcpp20_rwc_syncs.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/tests%2Fcompile-fail%2Fweak_memory%2Fcpp20_rwc_syncs.rs?ref=613d60db0bc0bfac9c3ad57245fc09e08795a550", "patch": "@@ -9,8 +9,8 @@\n // so we have to stick to C++11 emulation from existing research.\n \n use std::sync::atomic::Ordering::*;\n-use std::thread::spawn;\n use std::sync::atomic::{fence, AtomicUsize};\n+use std::thread::spawn;\n \n // Spins until it reads the given value\n fn reads_value(loc: &AtomicUsize, val: usize) -> usize {\n@@ -25,7 +25,7 @@ fn reads_value(loc: &AtomicUsize, val: usize) -> usize {\n fn static_atomic(val: usize) -> &'static AtomicUsize {\n     let ret = Box::leak(Box::new(AtomicUsize::new(val)));\n     // A workaround to put the initialization value in the store buffer.\n-    ret.store(val, Relaxed);\n+    ret.load(Relaxed);\n     ret\n }\n \n@@ -82,4 +82,4 @@ pub fn main() {\n     for _ in 0..500 {\n         test_cpp20_rwc_syncs();\n     }\n-}\n\\ No newline at end of file\n+}"}, {"sha": "d4ba48afe39d2d5cca79bcc38b334ac42855dabb", "filename": "tests/compile-fail/weak_memory/racing_mixed_size.rs", "status": "added", "additions": 38, "deletions": 0, "changes": 38, "blob_url": "https://github.com/rust-lang/rust/blob/613d60db0bc0bfac9c3ad57245fc09e08795a550/tests%2Fcompile-fail%2Fweak_memory%2Fracing_mixed_size.rs", "raw_url": "https://github.com/rust-lang/rust/raw/613d60db0bc0bfac9c3ad57245fc09e08795a550/tests%2Fcompile-fail%2Fweak_memory%2Fracing_mixed_size.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/tests%2Fcompile-fail%2Fweak_memory%2Fracing_mixed_size.rs?ref=613d60db0bc0bfac9c3ad57245fc09e08795a550", "patch": "@@ -0,0 +1,38 @@\n+// compile-flags: -Zmiri-ignore-leaks\n+\n+#![feature(core_intrinsics)]\n+\n+use std::sync::atomic::AtomicU32;\n+use std::sync::atomic::Ordering::*;\n+use std::thread::spawn;\n+\n+fn static_atomic_u32(val: u32) -> &'static AtomicU32 {\n+    let ret = Box::leak(Box::new(AtomicU32::new(val)));\n+    ret\n+}\n+\n+fn split_u32_ptr(dword: *const u32) -> *const [u16; 2] {\n+    unsafe { std::mem::transmute::<*const u32, *const [u16; 2]>(dword) }\n+}\n+\n+// Wine's SRWLock implementation does this, which is definitely undefined in C++ memory model\n+// https://github.com/wine-mirror/wine/blob/303f8042f9db508adaca02ef21f8de4992cb9c03/dlls/ntdll/sync.c#L543-L566\n+// Though it probably works just fine on x86\n+pub fn main() {\n+    let x = static_atomic_u32(0);\n+    let j1 = spawn(move || {\n+        x.store(1, Relaxed);\n+    });\n+\n+    let j2 = spawn(move || {\n+        let x_ptr = x as *const AtomicU32 as *const u32;\n+        let x_split = split_u32_ptr(x_ptr);\n+        unsafe {\n+            let hi = &(*x_split)[0] as *const u16;\n+            std::intrinsics::atomic_load_relaxed(hi); //~ ERROR: imperfectly overlapping\n+        }\n+    });\n+\n+    j1.join().unwrap();\n+    j2.join().unwrap();\n+}"}, {"sha": "b1a683798bb326710a7602491d3e7863109d1442", "filename": "tests/run-pass/weak_memory/extra_cpp.rs", "status": "modified", "additions": 85, "deletions": 4, "changes": 89, "blob_url": "https://github.com/rust-lang/rust/blob/613d60db0bc0bfac9c3ad57245fc09e08795a550/tests%2Frun-pass%2Fweak_memory%2Fextra_cpp.rs", "raw_url": "https://github.com/rust-lang/rust/raw/613d60db0bc0bfac9c3ad57245fc09e08795a550/tests%2Frun-pass%2Fweak_memory%2Fextra_cpp.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/tests%2Frun-pass%2Fweak_memory%2Fextra_cpp.rs?ref=613d60db0bc0bfac9c3ad57245fc09e08795a550", "patch": "@@ -4,20 +4,30 @@\n // but doable in safe (at least sound) Rust.\n \n #![feature(atomic_from_mut)]\n+#![feature(core_intrinsics)]\n \n use std::sync::atomic::Ordering::*;\n-use std::sync::atomic::{AtomicU16, AtomicU32, AtomicUsize};\n+use std::sync::atomic::{AtomicU16, AtomicU32};\n use std::thread::spawn;\n \n-fn static_atomic_mut(val: usize) -> &'static mut AtomicUsize {\n-    let ret = Box::leak(Box::new(AtomicUsize::new(val)));\n+fn static_atomic_mut(val: u32) -> &'static mut AtomicU32 {\n+    let ret = Box::leak(Box::new(AtomicU32::new(val)));\n+    ret\n+}\n+\n+fn static_atomic(val: u32) -> &'static AtomicU32 {\n+    let ret = Box::leak(Box::new(AtomicU32::new(val)));\n     ret\n }\n \n fn split_u32(dword: &mut u32) -> &mut [u16; 2] {\n     unsafe { std::mem::transmute::<&mut u32, &mut [u16; 2]>(dword) }\n }\n \n+fn split_u32_ptr(dword: *const u32) -> *const [u16; 2] {\n+    unsafe { std::mem::transmute::<*const u32, *const [u16; 2]>(dword) }\n+}\n+\n fn mem_replace() {\n     let mut x = AtomicU32::new(0);\n \n@@ -31,7 +41,7 @@ fn assign_to_mut() {\n     let x = static_atomic_mut(0);\n     x.store(1, Relaxed);\n \n-    *x = AtomicUsize::new(2);\n+    *x = AtomicU32::new(2);\n \n     assert_eq!(x.load(Relaxed), 2);\n }\n@@ -70,10 +80,81 @@ fn from_mut_split() {\n     assert_eq!(x_lo_atomic.load(Relaxed), u16::from_be(0xfafa));\n }\n \n+// Although not possible to do in safe Rust,\n+// we allow non-atomic and atomic reads to race\n+// as this should be sound\n+fn racing_mixed_atomicity_read() {\n+    let x = static_atomic(0);\n+    x.store(42, Relaxed);\n+\n+    let j1 = spawn(move || x.load(Relaxed));\n+\n+    let j2 = spawn(move || {\n+        let x_ptr = x as *const AtomicU32 as *const u32;\n+        unsafe { std::intrinsics::atomic_load_relaxed(x_ptr) }\n+    });\n+\n+    let r1 = j1.join().unwrap();\n+    let r2 = j2.join().unwrap();\n+\n+    assert_eq!(r1, 42);\n+    assert_eq!(r2, 42);\n+}\n+\n+fn racing_mixed_size_read() {\n+    let x = static_atomic(0);\n+\n+    let j1 = spawn(move || {\n+        x.load(Relaxed);\n+    });\n+\n+    let j2 = spawn(move || {\n+        let x_ptr = x as *const AtomicU32 as *const u32;\n+        let x_split = split_u32_ptr(x_ptr);\n+        unsafe {\n+            let hi = &(*x_split)[0] as *const u16;\n+            std::intrinsics::atomic_load_relaxed(hi); //~ ERROR: imperfectly overlapping\n+        }\n+    });\n+\n+    j1.join().unwrap();\n+    j2.join().unwrap();\n+}\n+\n+fn racing_mixed_atomicity_and_size_read() {\n+    let x = static_atomic(u32::from_be(0xabbafafa));\n+\n+    let j1 = spawn(move || {\n+        x.load(Relaxed);\n+    });\n+\n+    let j2 = spawn(move || {\n+        let x_ptr = x as *const AtomicU32 as *const u32;\n+        unsafe { *x_ptr };\n+    });\n+\n+    let j3 = spawn(move || {\n+        let x_ptr = x as *const AtomicU32 as *const u32;\n+        let x_split = split_u32_ptr(x_ptr);\n+        unsafe {\n+            let hi = &(*x_split)[0] as *const u16;\n+            std::intrinsics::atomic_load_relaxed(hi)\n+        }\n+    });\n+\n+    j1.join().unwrap();\n+    j2.join().unwrap();\n+    let r3 = j3.join().unwrap();\n+\n+    assert_eq!(r3, u16::from_be(0xabba));\n+}\n \n pub fn main() {\n     get_mut_write();\n     from_mut_split();\n     assign_to_mut();\n     mem_replace();\n+    racing_mixed_atomicity_read();\n+    racing_mixed_size_read();\n+    racing_mixed_atomicity_and_size_read();\n }"}]}