{"sha": "b04c5329e1e145fb2fb46c5a7e775638712b03aa", "node_id": "C_kwDOAAsO6NoAKGIwNGM1MzI5ZTFlMTQ1ZmIyZmI0NmM1YTdlNzc1NjM4NzEyYjAzYWE", "commit": {"author": {"name": "bors", "email": "bors@rust-lang.org", "date": "2022-04-21T15:42:50Z"}, "committer": {"name": "bors", "email": "bors@rust-lang.org", "date": "2022-04-21T15:42:50Z"}, "message": "Auto merge of #96210 - nnethercote:speed-up-TokenCursor, r=petrochenkov\n\nSpeed up `TokenCursor`\n\nPlus a few related clean-ups.\n\nr? `@petrochenkov`", "tree": {"sha": "8c1679de53558157b02052094a149952561d96d7", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/8c1679de53558157b02052094a149952561d96d7"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/b04c5329e1e145fb2fb46c5a7e775638712b03aa", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/b04c5329e1e145fb2fb46c5a7e775638712b03aa", "html_url": "https://github.com/rust-lang/rust/commit/b04c5329e1e145fb2fb46c5a7e775638712b03aa", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/b04c5329e1e145fb2fb46c5a7e775638712b03aa/comments", "author": {"login": "bors", "id": 3372342, "node_id": "MDQ6VXNlcjMzNzIzNDI=", "avatar_url": "https://avatars.githubusercontent.com/u/3372342?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bors", "html_url": "https://github.com/bors", "followers_url": "https://api.github.com/users/bors/followers", "following_url": "https://api.github.com/users/bors/following{/other_user}", "gists_url": "https://api.github.com/users/bors/gists{/gist_id}", "starred_url": "https://api.github.com/users/bors/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bors/subscriptions", "organizations_url": "https://api.github.com/users/bors/orgs", "repos_url": "https://api.github.com/users/bors/repos", "events_url": "https://api.github.com/users/bors/events{/privacy}", "received_events_url": "https://api.github.com/users/bors/received_events", "type": "User", "site_admin": false}, "committer": {"login": "bors", "id": 3372342, "node_id": "MDQ6VXNlcjMzNzIzNDI=", "avatar_url": "https://avatars.githubusercontent.com/u/3372342?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bors", "html_url": "https://github.com/bors", "followers_url": "https://api.github.com/users/bors/followers", "following_url": "https://api.github.com/users/bors/following{/other_user}", "gists_url": "https://api.github.com/users/bors/gists{/gist_id}", "starred_url": "https://api.github.com/users/bors/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bors/subscriptions", "organizations_url": "https://api.github.com/users/bors/orgs", "repos_url": "https://api.github.com/users/bors/repos", "events_url": "https://api.github.com/users/bors/events{/privacy}", "received_events_url": "https://api.github.com/users/bors/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "1dec35a1b0df406da5d7cae55a7fa8d186a2b028", "url": "https://api.github.com/repos/rust-lang/rust/commits/1dec35a1b0df406da5d7cae55a7fa8d186a2b028", "html_url": "https://github.com/rust-lang/rust/commit/1dec35a1b0df406da5d7cae55a7fa8d186a2b028"}, {"sha": "643e9f707ed4ca13a158b6e290b424e520809ca6", "url": "https://api.github.com/repos/rust-lang/rust/commits/643e9f707ed4ca13a158b6e290b424e520809ca6", "html_url": "https://github.com/rust-lang/rust/commit/643e9f707ed4ca13a158b6e290b424e520809ca6"}], "stats": {"total": 258, "additions": 118, "deletions": 140}, "files": [{"sha": "d609fa6720502c5ca1b679ad2cd0b0c99c813be7", "filename": "compiler/rustc_ast/src/tokenstream.rs", "status": "modified", "additions": 12, "deletions": 15, "changes": 27, "blob_url": "https://github.com/rust-lang/rust/blob/b04c5329e1e145fb2fb46c5a7e775638712b03aa/compiler%2Frustc_ast%2Fsrc%2Ftokenstream.rs", "raw_url": "https://github.com/rust-lang/rust/raw/b04c5329e1e145fb2fb46c5a7e775638712b03aa/compiler%2Frustc_ast%2Fsrc%2Ftokenstream.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_ast%2Fsrc%2Ftokenstream.rs?ref=b04c5329e1e145fb2fb46c5a7e775638712b03aa", "patch": "@@ -94,16 +94,6 @@ impl TokenTree {\n         TokenTree::Token(Token::new(kind, span))\n     }\n \n-    /// Returns the opening delimiter as a token tree.\n-    pub fn open_tt(span: DelimSpan, delim: DelimToken) -> TokenTree {\n-        TokenTree::token(token::OpenDelim(delim), span.open)\n-    }\n-\n-    /// Returns the closing delimiter as a token tree.\n-    pub fn close_tt(span: DelimSpan, delim: DelimToken) -> TokenTree {\n-        TokenTree::token(token::CloseDelim(delim), span.close)\n-    }\n-\n     pub fn uninterpolate(self) -> TokenTree {\n         match self {\n             TokenTree::Token(token) => TokenTree::Token(token.uninterpolate().into_owned()),\n@@ -585,13 +575,20 @@ impl Cursor {\n         Cursor { stream, index: 0 }\n     }\n \n+    #[inline]\n     pub fn next_with_spacing(&mut self) -> Option<TreeAndSpacing> {\n-        if self.index < self.stream.len() {\n+        self.stream.0.get(self.index).map(|tree| {\n             self.index += 1;\n-            Some(self.stream.0[self.index - 1].clone())\n-        } else {\n-            None\n-        }\n+            tree.clone()\n+        })\n+    }\n+\n+    #[inline]\n+    pub fn next_with_spacing_ref(&mut self) -> Option<&TreeAndSpacing> {\n+        self.stream.0.get(self.index).map(|tree| {\n+            self.index += 1;\n+            tree\n+        })\n     }\n \n     pub fn index(&self) -> usize {"}, {"sha": "02749088c3139e50e9745dee12243021de5a73ac", "filename": "compiler/rustc_parse/src/parser/attr_wrapper.rs", "status": "modified", "additions": 6, "deletions": 11, "changes": 17, "blob_url": "https://github.com/rust-lang/rust/blob/b04c5329e1e145fb2fb46c5a7e775638712b03aa/compiler%2Frustc_parse%2Fsrc%2Fparser%2Fattr_wrapper.rs", "raw_url": "https://github.com/rust-lang/rust/raw/b04c5329e1e145fb2fb46c5a7e775638712b03aa/compiler%2Frustc_parse%2Fsrc%2Fparser%2Fattr_wrapper.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_parse%2Fsrc%2Fparser%2Fattr_wrapper.rs?ref=b04c5329e1e145fb2fb46c5a7e775638712b03aa", "patch": "@@ -100,21 +100,16 @@ rustc_data_structures::static_assert_size!(LazyTokenStreamImpl, 144);\n \n impl CreateTokenStream for LazyTokenStreamImpl {\n     fn create_token_stream(&self) -> AttrAnnotatedTokenStream {\n-        // The token produced by the final call to `{,inlined_}next` or\n-        // `{,inlined_}next_desugared` was not actually consumed by the\n-        // callback. The combination of chaining the initial token and using\n-        // `take` produces the desired result - we produce an empty\n-        // `TokenStream` if no calls were made, and omit the final token\n-        // otherwise.\n+        // The token produced by the final call to `{,inlined_}next` was not\n+        // actually consumed by the callback. The combination of chaining the\n+        // initial token and using `take` produces the desired result - we\n+        // produce an empty `TokenStream` if no calls were made, and omit the\n+        // final token otherwise.\n         let mut cursor_snapshot = self.cursor_snapshot.clone();\n         let tokens =\n             std::iter::once((FlatToken::Token(self.start_token.0.clone()), self.start_token.1))\n                 .chain((0..self.num_calls).map(|_| {\n-                    let token = if cursor_snapshot.desugar_doc_comments {\n-                        cursor_snapshot.next_desugared()\n-                    } else {\n-                        cursor_snapshot.next()\n-                    };\n+                    let token = cursor_snapshot.next(cursor_snapshot.desugar_doc_comments);\n                     (FlatToken::Token(token.0), token.1)\n                 }))\n                 .take(self.num_calls);"}, {"sha": "1686c5873e1833f4b5627fd627300aad055e950d", "filename": "compiler/rustc_parse/src/parser/mod.rs", "status": "modified", "additions": 93, "deletions": 111, "changes": 204, "blob_url": "https://github.com/rust-lang/rust/blob/b04c5329e1e145fb2fb46c5a7e775638712b03aa/compiler%2Frustc_parse%2Fsrc%2Fparser%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/b04c5329e1e145fb2fb46c5a7e775638712b03aa/compiler%2Frustc_parse%2Fsrc%2Fparser%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_parse%2Fsrc%2Fparser%2Fmod.rs?ref=b04c5329e1e145fb2fb46c5a7e775638712b03aa", "patch": "@@ -123,8 +123,8 @@ pub struct Parser<'a> {\n     pub capture_cfg: bool,\n     restrictions: Restrictions,\n     expected_tokens: Vec<TokenType>,\n-    // Important: This must only be advanced from `next_tok`\n-    // to ensure that `token_cursor.num_next_calls` is updated properly\n+    // Important: This must only be advanced from `bump` to ensure that\n+    // `token_cursor.num_next_calls` is updated properly.\n     token_cursor: TokenCursor,\n     desugar_doc_comments: bool,\n     /// This field is used to keep track of how many left angle brackets we have seen. This is\n@@ -150,6 +150,11 @@ pub struct Parser<'a> {\n     pub current_closure: Option<ClosureSpans>,\n }\n \n+// This type is used a lot, e.g. it's cloned when matching many declarative macro rules. Make sure\n+// it doesn't unintentionally get bigger.\n+#[cfg(all(target_arch = \"x86_64\", target_pointer_width = \"64\"))]\n+rustc_data_structures::static_assert_size!(Parser<'_>, 328);\n+\n /// Stores span information about a closure.\n #[derive(Clone)]\n pub struct ClosureSpans {\n@@ -203,12 +208,15 @@ impl<'a> Drop for Parser<'a> {\n \n #[derive(Clone)]\n struct TokenCursor {\n+    // The current (innermost) frame. `frame` and `stack` could be combined,\n+    // but it's faster to have them separately to access `frame` directly\n+    // rather than via something like `stack.last().unwrap()` or\n+    // `stack[stack.len() - 1]`.\n     frame: TokenCursorFrame,\n+    // Additional frames that enclose `frame`.\n     stack: Vec<TokenCursorFrame>,\n     desugar_doc_comments: bool,\n-    // Counts the number of calls to `{,inlined_}next` or\n-    // `{,inlined_}next_desugared`, depending on whether\n-    // `desugar_doc_comments` is set.\n+    // Counts the number of calls to `{,inlined_}next`.\n     num_next_calls: usize,\n     // During parsing, we may sometimes need to 'unglue' a\n     // glued token into two component tokens\n@@ -238,73 +246,60 @@ struct TokenCursor {\n struct TokenCursorFrame {\n     delim: token::DelimToken,\n     span: DelimSpan,\n-    open_delim: bool,\n     tree_cursor: tokenstream::Cursor,\n-    close_delim: bool,\n }\n \n impl TokenCursorFrame {\n     fn new(span: DelimSpan, delim: DelimToken, tts: TokenStream) -> Self {\n-        TokenCursorFrame {\n-            delim,\n-            span,\n-            open_delim: false,\n-            tree_cursor: tts.into_trees(),\n-            close_delim: false,\n-        }\n+        TokenCursorFrame { delim, span, tree_cursor: tts.into_trees() }\n     }\n }\n \n impl TokenCursor {\n-    fn next(&mut self) -> (Token, Spacing) {\n-        self.inlined_next()\n+    fn next(&mut self, desugar_doc_comments: bool) -> (Token, Spacing) {\n+        self.inlined_next(desugar_doc_comments)\n     }\n \n     /// This always-inlined version should only be used on hot code paths.\n     #[inline(always)]\n-    fn inlined_next(&mut self) -> (Token, Spacing) {\n+    fn inlined_next(&mut self, desugar_doc_comments: bool) -> (Token, Spacing) {\n         loop {\n-            let (tree, spacing) = if !self.frame.open_delim {\n-                self.frame.open_delim = true;\n-                TokenTree::open_tt(self.frame.span, self.frame.delim).into()\n-            } else if let Some(tree) = self.frame.tree_cursor.next_with_spacing() {\n-                tree\n-            } else if !self.frame.close_delim {\n-                self.frame.close_delim = true;\n-                TokenTree::close_tt(self.frame.span, self.frame.delim).into()\n+            // FIXME: we currently don't return `NoDelim` open/close delims. To fix #67062 we will\n+            // need to, whereupon the `delim != DelimToken::NoDelim` conditions below can be\n+            // removed, as well as the loop.\n+            if let Some((tree, spacing)) = self.frame.tree_cursor.next_with_spacing_ref() {\n+                match tree {\n+                    &TokenTree::Token(ref token) => match (desugar_doc_comments, token) {\n+                        (true, &Token { kind: token::DocComment(_, attr_style, data), span }) => {\n+                            return self.desugar(attr_style, data, span);\n+                        }\n+                        _ => return (token.clone(), *spacing),\n+                    },\n+                    &TokenTree::Delimited(sp, delim, ref tts) => {\n+                        // Set `open_delim` to true here because we deal with it immediately.\n+                        let frame = TokenCursorFrame::new(sp, delim, tts.clone());\n+                        self.stack.push(mem::replace(&mut self.frame, frame));\n+                        if delim != DelimToken::NoDelim {\n+                            return (Token::new(token::OpenDelim(delim), sp.open), Spacing::Alone);\n+                        }\n+                        // No open delimeter to return; continue on to the next iteration.\n+                    }\n+                };\n             } else if let Some(frame) = self.stack.pop() {\n+                let delim = self.frame.delim;\n+                let span = self.frame.span;\n                 self.frame = frame;\n-                continue;\n-            } else {\n-                (TokenTree::Token(Token::new(token::Eof, DUMMY_SP)), Spacing::Alone)\n-            };\n-\n-            match tree {\n-                TokenTree::Token(token) => {\n-                    return (token, spacing);\n-                }\n-                TokenTree::Delimited(sp, delim, tts) => {\n-                    let frame = TokenCursorFrame::new(sp, delim, tts);\n-                    self.stack.push(mem::replace(&mut self.frame, frame));\n+                if delim != DelimToken::NoDelim {\n+                    return (Token::new(token::CloseDelim(delim), span.close), Spacing::Alone);\n                 }\n+                // No close delimiter to return; continue on to the next iteration.\n+            } else {\n+                return (Token::new(token::Eof, DUMMY_SP), Spacing::Alone);\n             }\n         }\n     }\n \n-    fn next_desugared(&mut self) -> (Token, Spacing) {\n-        self.inlined_next_desugared()\n-    }\n-\n-    /// This always-inlined version should only be used on hot code paths.\n-    #[inline(always)]\n-    fn inlined_next_desugared(&mut self) -> (Token, Spacing) {\n-        let (data, attr_style, sp) = match self.inlined_next() {\n-            (Token { kind: token::DocComment(_, attr_style, data), span }, _) => {\n-                (data, attr_style, span)\n-            }\n-            tok => return tok,\n-        };\n-\n+    fn desugar(&mut self, attr_style: AttrStyle, data: Symbol, span: Span) -> (Token, Spacing) {\n         // Searches for the occurrences of `\"#*` and returns the minimum number of `#`s\n         // required to wrap the text.\n         let mut num_of_hashes = 0;\n@@ -318,14 +313,14 @@ impl TokenCursor {\n             num_of_hashes = cmp::max(num_of_hashes, count);\n         }\n \n-        let delim_span = DelimSpan::from_single(sp);\n+        let delim_span = DelimSpan::from_single(span);\n         let body = TokenTree::Delimited(\n             delim_span,\n             token::Bracket,\n             [\n-                TokenTree::token(token::Ident(sym::doc, false), sp),\n-                TokenTree::token(token::Eq, sp),\n-                TokenTree::token(TokenKind::lit(token::StrRaw(num_of_hashes), data, None), sp),\n+                TokenTree::token(token::Ident(sym::doc, false), span),\n+                TokenTree::token(token::Eq, span),\n+                TokenTree::token(TokenKind::lit(token::StrRaw(num_of_hashes), data, None), span),\n             ]\n             .iter()\n             .cloned()\n@@ -338,20 +333,20 @@ impl TokenCursor {\n                 delim_span,\n                 token::NoDelim,\n                 if attr_style == AttrStyle::Inner {\n-                    [TokenTree::token(token::Pound, sp), TokenTree::token(token::Not, sp), body]\n+                    [TokenTree::token(token::Pound, span), TokenTree::token(token::Not, span), body]\n                         .iter()\n                         .cloned()\n                         .collect::<TokenStream>()\n                 } else {\n-                    [TokenTree::token(token::Pound, sp), body]\n+                    [TokenTree::token(token::Pound, span), body]\n                         .iter()\n                         .cloned()\n                         .collect::<TokenStream>()\n                 },\n             ),\n         ));\n \n-        self.next()\n+        self.next(/* desugar_doc_comments */ false)\n     }\n }\n \n@@ -436,9 +431,9 @@ impl<'a> Parser<'a> {\n         desugar_doc_comments: bool,\n         subparser_name: Option<&'static str>,\n     ) -> Self {\n-        let mut start_frame = TokenCursorFrame::new(DelimSpan::dummy(), token::NoDelim, tokens);\n-        start_frame.open_delim = true;\n-        start_frame.close_delim = true;\n+        // Note: because of the way `TokenCursor::inlined_next` is structured, the `span` and\n+        // `delim` arguments here are never used.\n+        let start_frame = TokenCursorFrame::new(DelimSpan::dummy(), token::NoDelim, tokens);\n \n         let mut parser = Parser {\n             sess,\n@@ -476,33 +471,6 @@ impl<'a> Parser<'a> {\n         parser\n     }\n \n-    #[inline]\n-    fn next_tok(&mut self, fallback_span: Span) -> (Token, Spacing) {\n-        loop {\n-            let (mut next, spacing) = if self.desugar_doc_comments {\n-                self.token_cursor.inlined_next_desugared()\n-            } else {\n-                self.token_cursor.inlined_next()\n-            };\n-            self.token_cursor.num_next_calls += 1;\n-            // We've retrieved an token from the underlying\n-            // cursor, so we no longer need to worry about\n-            // an unglued token. See `break_and_eat` for more details\n-            self.token_cursor.break_last_token = false;\n-            if next.span.is_dummy() {\n-                // Tweak the location for better diagnostics, but keep syntactic context intact.\n-                next.span = fallback_span.with_ctxt(next.span.ctxt());\n-            }\n-            if matches!(\n-                next.kind,\n-                token::OpenDelim(token::NoDelim) | token::CloseDelim(token::NoDelim)\n-            ) {\n-                continue;\n-            }\n-            return (next, spacing);\n-        }\n-    }\n-\n     pub fn unexpected<T>(&mut self) -> PResult<'a, T> {\n         match self.expect_one_of(&[], &[]) {\n             Err(e) => Err(e),\n@@ -697,7 +665,7 @@ impl<'a> Parser<'a> {\n                 //\n                 // If we consume any additional tokens, then this token\n                 // is not needed (we'll capture the entire 'glued' token),\n-                // and `next_tok` will set this field to `None`\n+                // and `bump` will set this field to `None`\n                 self.token_cursor.break_last_token = true;\n                 // Use the spacing of the glued token as the spacing\n                 // of the unglued second token.\n@@ -1019,12 +987,6 @@ impl<'a> Parser<'a> {\n     /// This always-inlined version should only be used on hot code paths.\n     #[inline(always)]\n     fn inlined_bump_with(&mut self, (next_token, next_spacing): (Token, Spacing)) {\n-        // Bumping after EOF is a bad sign, usually an infinite loop.\n-        if self.prev_token.kind == TokenKind::Eof {\n-            let msg = \"attempted to bump the parser past EOF (may be stuck in a loop)\";\n-            self.span_bug(self.token.span, msg);\n-        }\n-\n         // Update the current and previous tokens.\n         self.prev_token = mem::replace(&mut self.token, next_token);\n         self.token_spacing = next_spacing;\n@@ -1035,8 +997,24 @@ impl<'a> Parser<'a> {\n \n     /// Advance the parser by one token.\n     pub fn bump(&mut self) {\n-        let next_token = self.next_tok(self.token.span);\n-        self.inlined_bump_with(next_token);\n+        // Note: destructuring here would give nicer code, but it was found in #96210 to be slower\n+        // than `.0`/`.1` access.\n+        let mut next = self.token_cursor.inlined_next(self.desugar_doc_comments);\n+        self.token_cursor.num_next_calls += 1;\n+        // We've retrieved an token from the underlying\n+        // cursor, so we no longer need to worry about\n+        // an unglued token. See `break_and_eat` for more details\n+        self.token_cursor.break_last_token = false;\n+        if next.0.span.is_dummy() {\n+            // Tweak the location for better diagnostics, but keep syntactic context intact.\n+            let fallback_span = self.token.span;\n+            next.0.span = fallback_span.with_ctxt(next.0.span.ctxt());\n+        }\n+        debug_assert!(!matches!(\n+            next.0.kind,\n+            token::OpenDelim(token::NoDelim) | token::CloseDelim(token::NoDelim)\n+        ));\n+        self.inlined_bump_with(next)\n     }\n \n     /// Look-ahead `dist` tokens of `self.token` and get access to that token there.\n@@ -1069,7 +1047,7 @@ impl<'a> Parser<'a> {\n         let mut i = 0;\n         let mut token = Token::dummy();\n         while i < dist {\n-            token = cursor.next().0;\n+            token = cursor.next(/* desugar_doc_comments */ false).0;\n             if matches!(\n                 token.kind,\n                 token::OpenDelim(token::NoDelim) | token::CloseDelim(token::NoDelim)\n@@ -1217,24 +1195,28 @@ impl<'a> Parser<'a> {\n     pub(crate) fn parse_token_tree(&mut self) -> TokenTree {\n         match self.token.kind {\n             token::OpenDelim(..) => {\n-                let depth = self.token_cursor.stack.len();\n+                // Grab the tokens from this frame.\n+                let frame = &self.token_cursor.frame;\n+                let stream = frame.tree_cursor.stream.clone();\n+                let span = frame.span;\n+                let delim = frame.delim;\n \n-                // We keep advancing the token cursor until we hit\n-                // the matching `CloseDelim` token.\n-                while !(depth == self.token_cursor.stack.len()\n-                    && matches!(self.token.kind, token::CloseDelim(_)))\n-                {\n+                // Advance the token cursor through the entire delimited\n+                // sequence. After getting the `OpenDelim` we are *within* the\n+                // delimited sequence, i.e. at depth `d`. After getting the\n+                // matching `CloseDelim` we are *after* the delimited sequence,\n+                // i.e. at depth `d - 1`.\n+                let target_depth = self.token_cursor.stack.len() - 1;\n+                loop {\n                     // Advance one token at a time, so `TokenCursor::next()`\n                     // can capture these tokens if necessary.\n                     self.bump();\n+                    if self.token_cursor.stack.len() == target_depth {\n+                        debug_assert!(matches!(self.token.kind, token::CloseDelim(_)));\n+                        break;\n+                    }\n                 }\n-                // We are still inside the frame corresponding\n-                // to the delimited stream we captured, so grab\n-                // the tokens from this frame.\n-                let frame = &self.token_cursor.frame;\n-                let stream = frame.tree_cursor.stream.clone();\n-                let span = frame.span;\n-                let delim = frame.delim;\n+\n                 // Consume close delimiter\n                 self.bump();\n                 TokenTree::Delimited(span, delim, stream)"}, {"sha": "691bfdb01a43ebfef224e141869c992c1f0ce57a", "filename": "compiler/rustc_parse/src/parser/nonterminal.rs", "status": "modified", "additions": 7, "deletions": 3, "changes": 10, "blob_url": "https://github.com/rust-lang/rust/blob/b04c5329e1e145fb2fb46c5a7e775638712b03aa/compiler%2Frustc_parse%2Fsrc%2Fparser%2Fnonterminal.rs", "raw_url": "https://github.com/rust-lang/rust/raw/b04c5329e1e145fb2fb46c5a7e775638712b03aa/compiler%2Frustc_parse%2Fsrc%2Fparser%2Fnonterminal.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_parse%2Fsrc%2Fparser%2Fnonterminal.rs?ref=b04c5329e1e145fb2fb46c5a7e775638712b03aa", "patch": "@@ -11,8 +11,10 @@ use crate::parser::{FollowedByType, ForceCollect, NtOrTt, Parser, PathStyle};\n impl<'a> Parser<'a> {\n     /// Checks whether a non-terminal may begin with a particular token.\n     ///\n-    /// Returning `false` is a *stability guarantee* that such a matcher will *never* begin with that\n-    /// token. Be conservative (return true) if not sure.\n+    /// Returning `false` is a *stability guarantee* that such a matcher will *never* begin with\n+    /// that token. Be conservative (return true) if not sure. Inlined because it has a single call\n+    /// site.\n+    #[inline]\n     pub fn nonterminal_may_begin_with(kind: NonterminalKind, token: &Token) -> bool {\n         /// Checks whether the non-terminal may contain a single (non-keyword) identifier.\n         fn may_be_ident(nt: &token::Nonterminal) -> bool {\n@@ -95,7 +97,9 @@ impl<'a> Parser<'a> {\n         }\n     }\n \n-    /// Parse a non-terminal (e.g. MBE `:pat` or `:ident`).\n+    /// Parse a non-terminal (e.g. MBE `:pat` or `:ident`). Inlined because there is only one call\n+    /// site.\n+    #[inline]\n     pub fn parse_nonterminal(&mut self, kind: NonterminalKind) -> PResult<'a, NtOrTt> {\n         // Any `Nonterminal` which stores its tokens (currently `NtItem` and `NtExpr`)\n         // needs to have them force-captured here."}]}