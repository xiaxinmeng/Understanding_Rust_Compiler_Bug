{"sha": "1055a6111adc55e99e5f63884a8f4fbf5e7bb28d", "node_id": "C_kwDOAAsO6NoAKDEwNTVhNjExMWFkYzU1ZTk5ZTVmNjM4ODRhOGY0ZmJmNWU3YmIyOGQ", "commit": {"author": {"name": "Aleksey Kladov", "email": "aleksey.kladov@gmail.com", "date": "2021-12-12T16:06:40Z"}, "committer": {"name": "Aleksey Kladov", "email": "aleksey.kladov@gmail.com", "date": "2021-12-12T16:06:40Z"}, "message": "port mbe to soa tokens", "tree": {"sha": "17f88c0807c454396af29ec8e8810e6b5feee193", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/17f88c0807c454396af29ec8e8810e6b5feee193"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/1055a6111adc55e99e5f63884a8f4fbf5e7bb28d", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/1055a6111adc55e99e5f63884a8f4fbf5e7bb28d", "html_url": "https://github.com/rust-lang/rust/commit/1055a6111adc55e99e5f63884a8f4fbf5e7bb28d", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/1055a6111adc55e99e5f63884a8f4fbf5e7bb28d/comments", "author": {"login": "matklad", "id": 1711539, "node_id": "MDQ6VXNlcjE3MTE1Mzk=", "avatar_url": "https://avatars.githubusercontent.com/u/1711539?v=4", "gravatar_id": "", "url": "https://api.github.com/users/matklad", "html_url": "https://github.com/matklad", "followers_url": "https://api.github.com/users/matklad/followers", "following_url": "https://api.github.com/users/matklad/following{/other_user}", "gists_url": "https://api.github.com/users/matklad/gists{/gist_id}", "starred_url": "https://api.github.com/users/matklad/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/matklad/subscriptions", "organizations_url": "https://api.github.com/users/matklad/orgs", "repos_url": "https://api.github.com/users/matklad/repos", "events_url": "https://api.github.com/users/matklad/events{/privacy}", "received_events_url": "https://api.github.com/users/matklad/received_events", "type": "User", "site_admin": false}, "committer": {"login": "matklad", "id": 1711539, "node_id": "MDQ6VXNlcjE3MTE1Mzk=", "avatar_url": "https://avatars.githubusercontent.com/u/1711539?v=4", "gravatar_id": "", "url": "https://api.github.com/users/matklad", "html_url": "https://github.com/matklad", "followers_url": "https://api.github.com/users/matklad/followers", "following_url": "https://api.github.com/users/matklad/following{/other_user}", "gists_url": "https://api.github.com/users/matklad/gists{/gist_id}", "starred_url": "https://api.github.com/users/matklad/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/matklad/subscriptions", "organizations_url": "https://api.github.com/users/matklad/orgs", "repos_url": "https://api.github.com/users/matklad/repos", "events_url": "https://api.github.com/users/matklad/events{/privacy}", "received_events_url": "https://api.github.com/users/matklad/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "965585748e6bdff8b0b83d3b1b2185ea30108221", "url": "https://api.github.com/repos/rust-lang/rust/commits/965585748e6bdff8b0b83d3b1b2185ea30108221", "html_url": "https://github.com/rust-lang/rust/commit/965585748e6bdff8b0b83d3b1b2185ea30108221"}], "stats": {"total": 313, "additions": 130, "deletions": 183}, "files": [{"sha": "1a56878fdb55b93701993dcf7557708f2505ce8e", "filename": "crates/mbe/src/lib.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/1055a6111adc55e99e5f63884a8f4fbf5e7bb28d/crates%2Fmbe%2Fsrc%2Flib.rs", "raw_url": "https://github.com/rust-lang/rust/raw/1055a6111adc55e99e5f63884a8f4fbf5e7bb28d/crates%2Fmbe%2Fsrc%2Flib.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fmbe%2Fsrc%2Flib.rs?ref=1055a6111adc55e99e5f63884a8f4fbf5e7bb28d", "patch": "@@ -10,7 +10,7 @@ mod parser;\n mod expander;\n mod syntax_bridge;\n mod tt_iter;\n-mod subtree_source;\n+mod to_parser_tokens;\n \n #[cfg(test)]\n mod benchmark;"}, {"sha": "6bdd787e301adca61bf26bc0e1f3cc06f77707d2", "filename": "crates/mbe/src/subtree_source.rs", "status": "removed", "additions": 0, "deletions": 174, "changes": 174, "blob_url": "https://github.com/rust-lang/rust/blob/965585748e6bdff8b0b83d3b1b2185ea30108221/crates%2Fmbe%2Fsrc%2Fsubtree_source.rs", "raw_url": "https://github.com/rust-lang/rust/raw/965585748e6bdff8b0b83d3b1b2185ea30108221/crates%2Fmbe%2Fsrc%2Fsubtree_source.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fmbe%2Fsrc%2Fsubtree_source.rs?ref=965585748e6bdff8b0b83d3b1b2185ea30108221", "patch": "@@ -1,174 +0,0 @@\n-//! Our parser is generic over the source of tokens it parses.\n-//!\n-//! This module defines tokens sourced from declarative macros.\n-\n-use parser::{Token, TokenSource};\n-use syntax::{lex_single_syntax_kind, SmolStr, SyntaxKind, SyntaxKind::*, T};\n-use tt::buffer::TokenBuffer;\n-\n-#[derive(Debug, Clone, Eq, PartialEq)]\n-struct TtToken {\n-    tt: Token,\n-    text: SmolStr,\n-}\n-\n-pub(crate) struct SubtreeTokenSource {\n-    cached: Vec<TtToken>,\n-    curr: (Token, usize),\n-}\n-\n-impl<'a> SubtreeTokenSource {\n-    pub(crate) fn new(buffer: &TokenBuffer) -> SubtreeTokenSource {\n-        let mut current = buffer.begin();\n-        let mut cached = Vec::with_capacity(100);\n-\n-        while !current.eof() {\n-            let cursor = current;\n-            let tt = cursor.token_tree();\n-\n-            // Check if it is lifetime\n-            if let Some(tt::buffer::TokenTreeRef::Leaf(tt::Leaf::Punct(punct), _)) = tt {\n-                if punct.char == '\\'' {\n-                    let next = cursor.bump();\n-                    if let Some(tt::buffer::TokenTreeRef::Leaf(tt::Leaf::Ident(ident), _)) =\n-                        next.token_tree()\n-                    {\n-                        let text = SmolStr::new(\"'\".to_string() + &ident.text);\n-                        cached.push(TtToken {\n-                            tt: Token { kind: LIFETIME_IDENT, is_jointed_to_next: false },\n-                            text,\n-                        });\n-                        current = next.bump();\n-                        continue;\n-                    } else {\n-                        panic!(\"Next token must be ident : {:#?}\", next.token_tree());\n-                    }\n-                }\n-            }\n-\n-            current = match tt {\n-                Some(tt::buffer::TokenTreeRef::Leaf(leaf, _)) => {\n-                    cached.push(convert_leaf(leaf));\n-                    cursor.bump()\n-                }\n-                Some(tt::buffer::TokenTreeRef::Subtree(subtree, _)) => {\n-                    if let Some(d) = subtree.delimiter_kind() {\n-                        cached.push(convert_delim(d, false));\n-                    }\n-                    cursor.subtree().unwrap()\n-                }\n-                None => match cursor.end() {\n-                    Some(subtree) => {\n-                        if let Some(d) = subtree.delimiter_kind() {\n-                            cached.push(convert_delim(d, true));\n-                        }\n-                        cursor.bump()\n-                    }\n-                    None => continue,\n-                },\n-            };\n-        }\n-\n-        let mut res = SubtreeTokenSource {\n-            curr: (Token { kind: EOF, is_jointed_to_next: false }, 0),\n-            cached,\n-        };\n-        res.curr = (res.token(0), 0);\n-        res\n-    }\n-\n-    fn token(&self, pos: usize) -> Token {\n-        match self.cached.get(pos) {\n-            Some(it) => it.tt,\n-            None => Token { kind: EOF, is_jointed_to_next: false },\n-        }\n-    }\n-}\n-\n-impl<'a> TokenSource for SubtreeTokenSource {\n-    fn current(&self) -> Token {\n-        self.curr.0\n-    }\n-\n-    /// Lookahead n token\n-    fn lookahead_nth(&self, n: usize) -> Token {\n-        self.token(self.curr.1 + n)\n-    }\n-\n-    /// bump cursor to next token\n-    fn bump(&mut self) {\n-        if self.current().kind == EOF {\n-            return;\n-        }\n-        self.curr = (self.token(self.curr.1 + 1), self.curr.1 + 1);\n-    }\n-\n-    /// Is the current token a specified keyword?\n-    fn is_keyword(&self, kw: &str) -> bool {\n-        match self.cached.get(self.curr.1) {\n-            Some(t) => t.text == *kw,\n-            None => false,\n-        }\n-    }\n-}\n-\n-fn convert_delim(d: tt::DelimiterKind, closing: bool) -> TtToken {\n-    let (kinds, texts) = match d {\n-        tt::DelimiterKind::Parenthesis => ([T!['('], T![')']], \"()\"),\n-        tt::DelimiterKind::Brace => ([T!['{'], T!['}']], \"{}\"),\n-        tt::DelimiterKind::Bracket => ([T!['['], T![']']], \"[]\"),\n-    };\n-\n-    let idx = closing as usize;\n-    let kind = kinds[idx];\n-    let text = &texts[idx..texts.len() - (1 - idx)];\n-    TtToken { tt: Token { kind, is_jointed_to_next: false }, text: SmolStr::new(text) }\n-}\n-\n-fn convert_literal(l: &tt::Literal) -> TtToken {\n-    let is_negated = l.text.starts_with('-');\n-    let inner_text = &l.text[if is_negated { 1 } else { 0 }..];\n-\n-    let kind = lex_single_syntax_kind(inner_text)\n-        .map(|(kind, _error)| kind)\n-        .filter(|kind| {\n-            kind.is_literal() && (!is_negated || matches!(kind, FLOAT_NUMBER | INT_NUMBER))\n-        })\n-        .unwrap_or_else(|| panic!(\"Fail to convert given literal {:#?}\", &l));\n-\n-    TtToken { tt: Token { kind, is_jointed_to_next: false }, text: l.text.clone() }\n-}\n-\n-fn convert_ident(ident: &tt::Ident) -> TtToken {\n-    let kind = match ident.text.as_ref() {\n-        \"true\" => T![true],\n-        \"false\" => T![false],\n-        \"_\" => UNDERSCORE,\n-        i if i.starts_with('\\'') => LIFETIME_IDENT,\n-        _ => SyntaxKind::from_keyword(ident.text.as_str()).unwrap_or(IDENT),\n-    };\n-\n-    TtToken { tt: Token { kind, is_jointed_to_next: false }, text: ident.text.clone() }\n-}\n-\n-fn convert_punct(p: tt::Punct) -> TtToken {\n-    let kind = match SyntaxKind::from_char(p.char) {\n-        None => panic!(\"{:#?} is not a valid punct\", p),\n-        Some(kind) => kind,\n-    };\n-\n-    let text = {\n-        let mut buf = [0u8; 4];\n-        let s: &str = p.char.encode_utf8(&mut buf);\n-        SmolStr::new(s)\n-    };\n-    TtToken { tt: Token { kind, is_jointed_to_next: p.spacing == tt::Spacing::Joint }, text }\n-}\n-\n-fn convert_leaf(leaf: &tt::Leaf) -> TtToken {\n-    match leaf {\n-        tt::Leaf::Literal(l) => convert_literal(l),\n-        tt::Leaf::Ident(ident) => convert_ident(ident),\n-        tt::Leaf::Punct(punct) => convert_punct(*punct),\n-    }\n-}"}, {"sha": "28a23f6be2c5978066e1c6c39512562761f4aa44", "filename": "crates/mbe/src/syntax_bridge.rs", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "blob_url": "https://github.com/rust-lang/rust/blob/1055a6111adc55e99e5f63884a8f4fbf5e7bb28d/crates%2Fmbe%2Fsrc%2Fsyntax_bridge.rs", "raw_url": "https://github.com/rust-lang/rust/raw/1055a6111adc55e99e5f63884a8f4fbf5e7bb28d/crates%2Fmbe%2Fsrc%2Fsyntax_bridge.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fmbe%2Fsrc%2Fsyntax_bridge.rs?ref=1055a6111adc55e99e5f63884a8f4fbf5e7bb28d", "patch": "@@ -12,7 +12,7 @@ use syntax::{\n use tt::buffer::{Cursor, TokenBuffer};\n \n use crate::{\n-    subtree_source::SubtreeTokenSource, tt_iter::TtIter, ExpandError, ParserEntryPoint, TokenMap,\n+    to_parser_tokens::to_parser_tokens, tt_iter::TtIter, ExpandError, ParserEntryPoint, TokenMap,\n };\n \n /// Convert the syntax node to a `TokenTree` (what macro\n@@ -56,9 +56,9 @@ pub fn token_tree_to_syntax_node(\n         }\n         _ => TokenBuffer::from_subtree(tt),\n     };\n-    let mut token_source = SubtreeTokenSource::new(&buffer);\n+    let parser_tokens = to_parser_tokens(&buffer);\n     let mut tree_sink = TtTreeSink::new(buffer.begin());\n-    parser::parse(&mut token_source, &mut tree_sink, entry_point);\n+    parser::parse(&parser_tokens, &mut tree_sink, entry_point);\n     if tree_sink.roots.len() != 1 {\n         return Err(ExpandError::ConversionError);\n     }"}, {"sha": "435226342ec306a2d9082ca310f5d15bf58859ef", "filename": "crates/mbe/src/to_parser_tokens.rs", "status": "added", "additions": 97, "deletions": 0, "changes": 97, "blob_url": "https://github.com/rust-lang/rust/blob/1055a6111adc55e99e5f63884a8f4fbf5e7bb28d/crates%2Fmbe%2Fsrc%2Fto_parser_tokens.rs", "raw_url": "https://github.com/rust-lang/rust/raw/1055a6111adc55e99e5f63884a8f4fbf5e7bb28d/crates%2Fmbe%2Fsrc%2Fto_parser_tokens.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fmbe%2Fsrc%2Fto_parser_tokens.rs?ref=1055a6111adc55e99e5f63884a8f4fbf5e7bb28d", "patch": "@@ -0,0 +1,97 @@\n+//! Convert macro-by-example tokens which are specific to macro expansion into a\n+//! format that works for our parser.\n+\n+use syntax::{lex_single_syntax_kind, SyntaxKind, SyntaxKind::*, T};\n+use tt::buffer::TokenBuffer;\n+\n+pub(crate) fn to_parser_tokens(buffer: &TokenBuffer) -> parser::Tokens {\n+    let mut res = parser::Tokens::default();\n+\n+    let mut current = buffer.begin();\n+\n+    while !current.eof() {\n+        let cursor = current;\n+        let tt = cursor.token_tree();\n+\n+        // Check if it is lifetime\n+        if let Some(tt::buffer::TokenTreeRef::Leaf(tt::Leaf::Punct(punct), _)) = tt {\n+            if punct.char == '\\'' {\n+                let next = cursor.bump();\n+                match next.token_tree() {\n+                    Some(tt::buffer::TokenTreeRef::Leaf(tt::Leaf::Ident(_ident), _)) => {\n+                        res.push(LIFETIME_IDENT);\n+                        current = next.bump();\n+                        continue;\n+                    }\n+                    _ => panic!(\"Next token must be ident : {:#?}\", next.token_tree()),\n+                }\n+            }\n+        }\n+\n+        current = match tt {\n+            Some(tt::buffer::TokenTreeRef::Leaf(leaf, _)) => {\n+                match leaf {\n+                    tt::Leaf::Literal(lit) => {\n+                        let is_negated = lit.text.starts_with('-');\n+                        let inner_text = &lit.text[if is_negated { 1 } else { 0 }..];\n+\n+                        let kind = lex_single_syntax_kind(inner_text)\n+                            .map(|(kind, _error)| kind)\n+                            .filter(|kind| {\n+                                kind.is_literal()\n+                                    && (!is_negated || matches!(kind, FLOAT_NUMBER | INT_NUMBER))\n+                            })\n+                            .unwrap_or_else(|| panic!(\"Fail to convert given literal {:#?}\", &lit));\n+\n+                        res.push(kind);\n+                    }\n+                    tt::Leaf::Ident(ident) => match ident.text.as_ref() {\n+                        \"_\" => res.push(T![_]),\n+                        i if i.starts_with('\\'') => res.push(LIFETIME_IDENT),\n+                        _ => match SyntaxKind::from_keyword(&ident.text) {\n+                            Some(kind) => res.push(kind),\n+                            None => {\n+                                let contextual_keyword =\n+                                    SyntaxKind::from_contextual_keyword(&ident.text)\n+                                        .unwrap_or(SyntaxKind::IDENT);\n+                                res.push_ident(contextual_keyword);\n+                            }\n+                        },\n+                    },\n+                    tt::Leaf::Punct(punct) => {\n+                        let kind = SyntaxKind::from_char(punct.char)\n+                            .unwrap_or_else(|| panic!(\"{:#?} is not a valid punct\", punct));\n+                        res.push(kind);\n+                        res.was_joint(punct.spacing == tt::Spacing::Joint);\n+                    }\n+                }\n+                cursor.bump()\n+            }\n+            Some(tt::buffer::TokenTreeRef::Subtree(subtree, _)) => {\n+                if let Some(d) = subtree.delimiter_kind() {\n+                    res.push(match d {\n+                        tt::DelimiterKind::Parenthesis => T!['('],\n+                        tt::DelimiterKind::Brace => T!['{'],\n+                        tt::DelimiterKind::Bracket => T!['['],\n+                    });\n+                }\n+                cursor.subtree().unwrap()\n+            }\n+            None => match cursor.end() {\n+                Some(subtree) => {\n+                    if let Some(d) = subtree.delimiter_kind() {\n+                        res.push(match d {\n+                            tt::DelimiterKind::Parenthesis => T![')'],\n+                            tt::DelimiterKind::Brace => T!['}'],\n+                            tt::DelimiterKind::Bracket => T![']'],\n+                        })\n+                    }\n+                    cursor.bump()\n+                }\n+                None => continue,\n+            },\n+        };\n+    }\n+\n+    res\n+}"}, {"sha": "d05e84b0f027b9918304ea0734958a85746eb0ef", "filename": "crates/mbe/src/tt_iter.rs", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "blob_url": "https://github.com/rust-lang/rust/blob/1055a6111adc55e99e5f63884a8f4fbf5e7bb28d/crates%2Fmbe%2Fsrc%2Ftt_iter.rs", "raw_url": "https://github.com/rust-lang/rust/raw/1055a6111adc55e99e5f63884a8f4fbf5e7bb28d/crates%2Fmbe%2Fsrc%2Ftt_iter.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fmbe%2Fsrc%2Ftt_iter.rs?ref=1055a6111adc55e99e5f63884a8f4fbf5e7bb28d", "patch": "@@ -1,7 +1,7 @@\n //! A \"Parser\" structure for token trees. We use this when parsing a declarative\n //! macro definition into a list of patterns and templates.\n \n-use crate::{subtree_source::SubtreeTokenSource, ExpandError, ExpandResult, ParserEntryPoint};\n+use crate::{to_parser_tokens::to_parser_tokens, ExpandError, ExpandResult, ParserEntryPoint};\n \n use parser::TreeSink;\n use syntax::SyntaxKind;\n@@ -116,10 +116,10 @@ impl<'a> TtIter<'a> {\n         }\n \n         let buffer = TokenBuffer::from_tokens(self.inner.as_slice());\n-        let mut src = SubtreeTokenSource::new(&buffer);\n+        let parser_tokens = to_parser_tokens(&buffer);\n         let mut sink = OffsetTokenSink { cursor: buffer.begin(), error: false };\n \n-        parser::parse(&mut src, &mut sink, entry_point);\n+        parser::parse(&parser_tokens, &mut sink, entry_point);\n \n         let mut err = if !sink.cursor.is_root() || sink.error {\n             Some(err!(\"expected {:?}\", entry_point))"}, {"sha": "2e2d96d02759fae99a5ebb9549b422ff689338bc", "filename": "crates/parser/src/lib.rs", "status": "modified", "additions": 5, "deletions": 2, "changes": 7, "blob_url": "https://github.com/rust-lang/rust/blob/1055a6111adc55e99e5f63884a8f4fbf5e7bb28d/crates%2Fparser%2Fsrc%2Flib.rs", "raw_url": "https://github.com/rust-lang/rust/raw/1055a6111adc55e99e5f63884a8f4fbf5e7bb28d/crates%2Fparser%2Fsrc%2Flib.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fparser%2Fsrc%2Flib.rs?ref=1055a6111adc55e99e5f63884a8f4fbf5e7bb28d", "patch": "@@ -1,8 +1,11 @@\n //! The Rust parser.\n //!\n+//! NOTE: The crate is undergoing refactors, don't believe everything the docs\n+//! say :-)\n+//!\n //! The parser doesn't know about concrete representation of tokens and syntax\n-//! trees. Abstract [`TokenSource`] and [`TreeSink`] traits are used instead.\n-//! As a consequence, this crate does not contain a lexer.\n+//! trees. Abstract [`TokenSource`] and [`TreeSink`] traits are used instead. As\n+//! a consequence, this crate does not contain a lexer.\n //!\n //! The [`Parser`] struct from the [`parser`] module is a cursor into the\n //! sequence of tokens.  Parsing routines use [`Parser`] to inspect current"}, {"sha": "dff5e583b1cc6b7ae28cf07796534db78c6dc14c", "filename": "crates/parser/src/tokens.rs", "status": "modified", "additions": 21, "deletions": 0, "changes": 21, "blob_url": "https://github.com/rust-lang/rust/blob/1055a6111adc55e99e5f63884a8f4fbf5e7bb28d/crates%2Fparser%2Fsrc%2Ftokens.rs", "raw_url": "https://github.com/rust-lang/rust/raw/1055a6111adc55e99e5f63884a8f4fbf5e7bb28d/crates%2Fparser%2Fsrc%2Ftokens.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fparser%2Fsrc%2Ftokens.rs?ref=1055a6111adc55e99e5f63884a8f4fbf5e7bb28d", "patch": "@@ -1,3 +1,8 @@\n+//! Input for the parser -- a sequence of tokens.\n+//!\n+//! As of now, parser doesn't have access to the *text* of the tokens, and makes\n+//! decisions based solely on their classification.\n+\n use crate::SyntaxKind;\n \n #[allow(non_camel_case_types)]\n@@ -28,6 +33,22 @@ impl Tokens {\n     pub fn push(&mut self, kind: SyntaxKind) {\n         self.push_impl(kind, SyntaxKind::EOF)\n     }\n+    /// Sets jointness for the last token we've pushed.\n+    ///\n+    /// This is a separate API rather than an argument to the `push` to make it\n+    /// convenient both for textual and mbe tokens. With text, you know whether\n+    /// the *previous* token was joint, with mbe, you know whether the *current*\n+    /// one is joint. This API allows for styles of usage:\n+    ///\n+    /// ```\n+    /// // In text:\n+    /// tokens.was_joint(prev_joint);\n+    /// tokens.push(curr);\n+    ///\n+    /// // In MBE:\n+    /// token.push(curr);\n+    /// tokens.push(curr_joint)\n+    /// ```\n     pub fn was_joint(&mut self, yes: bool) {\n         let idx = self.len();\n         if yes && idx > 0 {"}]}