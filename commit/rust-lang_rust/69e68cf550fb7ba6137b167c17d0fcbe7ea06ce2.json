{"sha": "69e68cf550fb7ba6137b167c17d0fcbe7ea06ce2", "node_id": "MDY6Q29tbWl0NzI0NzEyOjY5ZTY4Y2Y1NTBmYjdiYTYxMzdiMTY3YzE3ZDBmY2JlN2VhMDZjZTI=", "commit": {"author": {"name": "bors", "email": "bors@rust-lang.org", "date": "2020-10-26T06:49:34Z"}, "committer": {"name": "bors", "email": "bors@rust-lang.org", "date": "2020-10-26T06:49:34Z"}, "message": "Auto merge of #75728 - nagisa:improve_align_offset_2, r=Mark-Simulacrum\n\nOptimise align_offset for stride=1 further\n\n`stride == 1` case can be computed more efficiently through `-p (mod\na)`. That, then translates to a nice and short sequence of LLVM\ninstructions:\n\n    %address = ptrtoint i8* %p to i64\n    %negptr = sub i64 0, %address\n    %offset = and i64 %negptr, %a_minus_one\n\nAnd produces pretty much ideal code-gen when this function is used in\nisolation.\n\nTypical use of this function will, however, involve use of\nthe result to offset a pointer, i.e.\n\n    %aligned = getelementptr inbounds i8, i8* %p, i64 %offset\n\nThis still looks very good, but LLVM does not really translate that to\nwhat would be considered ideal machine code (on any target). For example\nthat's the codegen we obtain for an unknown alignment:\n\n    ; x86_64\n    dec     rsi\n    mov     rax, rdi\n    neg     rax\n    and     rax, rsi\n    add     rax, rdi\n\nIn particular negating a pointer is not something that\u2019s going to be\noptimised for in the design of CISC architectures like x86_64. They\nare much better at offsetting pointers. And so we\u2019d love to utilize this\nability and produce code that's more like this:\n\n    ; x86_64\n    lea     rax, [rsi + rdi - 1]\n    neg     rsi\n    and     rax, rsi\n\nTo achieve this we need to give LLVM an opportunity to apply its\nvarious peep-hole optimisations that it does during DAG selection. In\nparticular, the `and` instruction appears to be a major inhibitor here.\nWe cannot, sadly, get rid of this load-bearing operation, but we can\nreorder operations such that LLVM has more to work with around this\ninstruction.\n\nOne such ordering is proposed in #75579 and results in LLVM IR that\nlooks broadly like this:\n\n    ; using add enables `lea` and similar CISCisms\n    %offset_ptr = add i64 %address, %a_minus_one\n    %mask = sub i64 0, %a\n    %masked = and i64 %offset_ptr, %mask\n    ; can be folded with `gepi` that may follow\n    %offset = sub i64 %masked, %address\n\n\u2026and generates the intended x86_64 machine code.\nOne might also wonder how the increased amount of code would impact a\nRISC target. Turns out not much:\n\n    ; aarch64 previous                 ; aarch64 new\n    sub     x8, x1, #1                 add     x8, x1, x0\n    neg     x9, x0                     sub     x8, x8, #1\n    and     x8, x9, x8                 neg     x9, x1\n    add     x0, x0, x8                 and     x0, x8, x9\n\n    (and similarly for ppc, sparc, mips, riscv, etc)\n\nThe only target that seems to do worse is\u2026 wasm32.\n\nOnto actual measurements \u2013 the best way to evaluate snipets like these\nis to use llvm-mca. Much like Aarch64 assembly would allow to suspect,\nthere isn\u2019t any performance difference to be found. Both snippets\nexecute in same number of cycles for the CPUs I tried. On x86_64,\nwe get throughput improvement of >50%!\n\nFixes #75579", "tree": {"sha": "100808e3882234321d291d4c2a91d8709689ac1b", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/100808e3882234321d291d4c2a91d8709689ac1b"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/69e68cf550fb7ba6137b167c17d0fcbe7ea06ce2", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/69e68cf550fb7ba6137b167c17d0fcbe7ea06ce2", "html_url": "https://github.com/rust-lang/rust/commit/69e68cf550fb7ba6137b167c17d0fcbe7ea06ce2", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/69e68cf550fb7ba6137b167c17d0fcbe7ea06ce2/comments", "author": {"login": "bors", "id": 3372342, "node_id": "MDQ6VXNlcjMzNzIzNDI=", "avatar_url": "https://avatars.githubusercontent.com/u/3372342?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bors", "html_url": "https://github.com/bors", "followers_url": "https://api.github.com/users/bors/followers", "following_url": "https://api.github.com/users/bors/following{/other_user}", "gists_url": "https://api.github.com/users/bors/gists{/gist_id}", "starred_url": "https://api.github.com/users/bors/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bors/subscriptions", "organizations_url": "https://api.github.com/users/bors/orgs", "repos_url": "https://api.github.com/users/bors/repos", "events_url": "https://api.github.com/users/bors/events{/privacy}", "received_events_url": "https://api.github.com/users/bors/received_events", "type": "User", "site_admin": false}, "committer": {"login": "bors", "id": 3372342, "node_id": "MDQ6VXNlcjMzNzIzNDI=", "avatar_url": "https://avatars.githubusercontent.com/u/3372342?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bors", "html_url": "https://github.com/bors", "followers_url": "https://api.github.com/users/bors/followers", "following_url": "https://api.github.com/users/bors/following{/other_user}", "gists_url": "https://api.github.com/users/bors/gists{/gist_id}", "starred_url": "https://api.github.com/users/bors/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bors/subscriptions", "organizations_url": "https://api.github.com/users/bors/orgs", "repos_url": "https://api.github.com/users/bors/repos", "events_url": "https://api.github.com/users/bors/events{/privacy}", "received_events_url": "https://api.github.com/users/bors/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "1cd97cad6e5f85bed455f505f330ead1d5cd8432", "url": "https://api.github.com/repos/rust-lang/rust/commits/1cd97cad6e5f85bed455f505f330ead1d5cd8432", "html_url": "https://github.com/rust-lang/rust/commit/1cd97cad6e5f85bed455f505f330ead1d5cd8432"}, {"sha": "4bfacffb903d382333a39c1646ee2b3956bd59bf", "url": "https://api.github.com/repos/rust-lang/rust/commits/4bfacffb903d382333a39c1646ee2b3956bd59bf", "html_url": "https://github.com/rust-lang/rust/commit/4bfacffb903d382333a39c1646ee2b3956bd59bf"}], "stats": {"total": 17, "additions": 14, "deletions": 3}, "files": [{"sha": "39117b1890ef6c5f74e8ca72ddabb5ed339e321c", "filename": "library/core/src/ptr/mod.rs", "status": "modified", "additions": 14, "deletions": 3, "changes": 17, "blob_url": "https://github.com/rust-lang/rust/blob/69e68cf550fb7ba6137b167c17d0fcbe7ea06ce2/library%2Fcore%2Fsrc%2Fptr%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/69e68cf550fb7ba6137b167c17d0fcbe7ea06ce2/library%2Fcore%2Fsrc%2Fptr%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/library%2Fcore%2Fsrc%2Fptr%2Fmod.rs?ref=69e68cf550fb7ba6137b167c17d0fcbe7ea06ce2", "patch": "@@ -1143,7 +1143,9 @@ pub unsafe fn write_volatile<T>(dst: *mut T, src: T) {\n pub(crate) unsafe fn align_offset<T: Sized>(p: *const T, a: usize) -> usize {\n     // FIXME(#75598): Direct use of these intrinsics improves codegen significantly at opt-level <=\n     // 1, where the method versions of these operations are not inlined.\n-    use intrinsics::{unchecked_shl, unchecked_shr, unchecked_sub, wrapping_mul, wrapping_sub};\n+    use intrinsics::{\n+        unchecked_shl, unchecked_shr, unchecked_sub, wrapping_add, wrapping_mul, wrapping_sub,\n+    };\n \n     /// Calculate multiplicative modular inverse of `x` modulo `m`.\n     ///\n@@ -1198,8 +1200,17 @@ pub(crate) unsafe fn align_offset<T: Sized>(p: *const T, a: usize) -> usize {\n     // SAFETY: `a` is a power-of-two, therefore non-zero.\n     let a_minus_one = unsafe { unchecked_sub(a, 1) };\n     if stride == 1 {\n-        // `stride == 1` case can be computed more efficiently through `-p (mod a)`.\n-        return wrapping_sub(0, p as usize) & a_minus_one;\n+        // `stride == 1` case can be computed more simply through `-p (mod a)`, but doing so\n+        // inhibits LLVM's ability to select instructions like `lea`. Instead we compute\n+        //\n+        //    round_up_to_next_alignment(p, a) - p\n+        //\n+        // which distributes operations around the load-bearing, but pessimizing `and` sufficiently\n+        // for LLVM to be able to utilize the various optimizations it knows about.\n+        return wrapping_sub(\n+            wrapping_add(p as usize, a_minus_one) & wrapping_sub(0, a),\n+            p as usize,\n+        );\n     }\n \n     let pmoda = p as usize & a_minus_one;"}]}