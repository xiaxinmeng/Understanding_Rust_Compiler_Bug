{"sha": "4bbef261b6dd8529dadfd41c04b72137657bc7e7", "node_id": "C_kwDOAAsO6NoAKDRiYmVmMjYxYjZkZDg1MjlkYWRmZDQxYzA0YjcyMTM3NjU3YmM3ZTc", "commit": {"author": {"name": "Jubilee", "email": "46493976+workingjubilee@users.noreply.github.com", "date": "2021-12-30T09:22:01Z"}, "committer": {"name": "GitHub", "email": "noreply@github.com", "date": "2021-12-30T09:22:01Z"}, "message": "Merge portable-simd#210 - ./wrap-shifts\n\nRefactor ops.rs with wrapping shifts\r\n\r\nThis approaches reducing macro nesting in a slightly different way. Instead of just flattening details, make one macro apply another. This allows specifying all details up-front in the first macro invocation, making it easier to audit and refactor in the future.\r\n\r\nThis refactor also has some functional changes. Only one is a true behavior change, however:\r\n- The visible one is that SIMD shifts are now wrapping, not panicking on overflow\r\n- `core::simd` now has a lot more instances of `#[must_use]`, which merely lints\r\n- div/rem now perform a SIMD check but remain as before, which should improve performance but be invisible", "tree": {"sha": "d8c9f81a7b723a019008b3348fe611bc3ec9ba63", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/d8c9f81a7b723a019008b3348fe611bc3ec9ba63"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/4bbef261b6dd8529dadfd41c04b72137657bc7e7", "comment_count": 0, "verification": {"verified": true, "reason": "valid", "signature": "-----BEGIN PGP SIGNATURE-----\n\nwsBcBAABCAAQBQJhzXo5CRBK7hj4Ov3rIwAAdZkIAHMdaAJpyfKrbcZJPwAJ45fK\nxANR6tKsTrfQeVy+rln1vlPcJ7+oIPQ5R1XFgFL6W1TBLOKYvh3jA8FQgSHGi2vG\n397T3d05un09VD9g/2Qs3jyEI3SES28kc9AGlAfLb5Nblr+uuxSFE1W7NZrm/Czq\nSJJwLV+GxqhfYzMS5fIN23WLgCd3l/Z67CVH8aUICol8TU52pM7RGkPQ8u6lgnwD\nrHI1i/RjEB22UqSNC3cqO0fC2POClYnm5wLl1DDA02yZHK6XSjFMQ5g+mvtgCTnl\nS1Sqfbc/1jH6tEq2q1TjJLxaOEA2C4GfJI6RvF6YBU8y+4IbGrjhpo0JQ1BH+Dw=\n=vEpt\n-----END PGP SIGNATURE-----\n", "payload": "tree d8c9f81a7b723a019008b3348fe611bc3ec9ba63\nparent 533f0fc81ab9ba097779fcd27c8f9ea12261fef5\nparent a42420583bdb6ea788c2f7ec0a0360d99934f2a7\nauthor Jubilee <46493976+workingjubilee@users.noreply.github.com> 1640856121 -0800\ncommitter GitHub <noreply@github.com> 1640856121 -0800\n\nMerge portable-simd#210 - ./wrap-shifts\n\nRefactor ops.rs with wrapping shifts\r\n\r\nThis approaches reducing macro nesting in a slightly different way. Instead of just flattening details, make one macro apply another. This allows specifying all details up-front in the first macro invocation, making it easier to audit and refactor in the future.\r\n\r\nThis refactor also has some functional changes. Only one is a true behavior change, however:\r\n- The visible one is that SIMD shifts are now wrapping, not panicking on overflow\r\n- `core::simd` now has a lot more instances of `#[must_use]`, which merely lints\r\n- div/rem now perform a SIMD check but remain as before, which should improve performance but be invisible"}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/4bbef261b6dd8529dadfd41c04b72137657bc7e7", "html_url": "https://github.com/rust-lang/rust/commit/4bbef261b6dd8529dadfd41c04b72137657bc7e7", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/4bbef261b6dd8529dadfd41c04b72137657bc7e7/comments", "author": {"login": "workingjubilee", "id": 46493976, "node_id": "MDQ6VXNlcjQ2NDkzOTc2", "avatar_url": "https://avatars.githubusercontent.com/u/46493976?v=4", "gravatar_id": "", "url": "https://api.github.com/users/workingjubilee", "html_url": "https://github.com/workingjubilee", "followers_url": "https://api.github.com/users/workingjubilee/followers", "following_url": "https://api.github.com/users/workingjubilee/following{/other_user}", "gists_url": "https://api.github.com/users/workingjubilee/gists{/gist_id}", "starred_url": "https://api.github.com/users/workingjubilee/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/workingjubilee/subscriptions", "organizations_url": "https://api.github.com/users/workingjubilee/orgs", "repos_url": "https://api.github.com/users/workingjubilee/repos", "events_url": "https://api.github.com/users/workingjubilee/events{/privacy}", "received_events_url": "https://api.github.com/users/workingjubilee/received_events", "type": "User", "site_admin": false}, "committer": {"login": "web-flow", "id": 19864447, "node_id": "MDQ6VXNlcjE5ODY0NDQ3", "avatar_url": "https://avatars.githubusercontent.com/u/19864447?v=4", "gravatar_id": "", "url": "https://api.github.com/users/web-flow", "html_url": "https://github.com/web-flow", "followers_url": "https://api.github.com/users/web-flow/followers", "following_url": "https://api.github.com/users/web-flow/following{/other_user}", "gists_url": "https://api.github.com/users/web-flow/gists{/gist_id}", "starred_url": "https://api.github.com/users/web-flow/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/web-flow/subscriptions", "organizations_url": "https://api.github.com/users/web-flow/orgs", "repos_url": "https://api.github.com/users/web-flow/repos", "events_url": "https://api.github.com/users/web-flow/events{/privacy}", "received_events_url": "https://api.github.com/users/web-flow/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "533f0fc81ab9ba097779fcd27c8f9ea12261fef5", "url": "https://api.github.com/repos/rust-lang/rust/commits/533f0fc81ab9ba097779fcd27c8f9ea12261fef5", "html_url": "https://github.com/rust-lang/rust/commit/533f0fc81ab9ba097779fcd27c8f9ea12261fef5"}, {"sha": "a42420583bdb6ea788c2f7ec0a0360d99934f2a7", "url": "https://api.github.com/repos/rust-lang/rust/commits/a42420583bdb6ea788c2f7ec0a0360d99934f2a7", "html_url": "https://github.com/rust-lang/rust/commit/a42420583bdb6ea788c2f7ec0a0360d99934f2a7"}], "stats": {"total": 377, "additions": 175, "deletions": 202}, "files": [{"sha": "82b007aa6966da71a68e4c7e6a3a656a23071dd9", "filename": "crates/core_simd/src/ops.rs", "status": "modified", "additions": 175, "deletions": 202, "changes": 377, "blob_url": "https://github.com/rust-lang/rust/blob/4bbef261b6dd8529dadfd41c04b72137657bc7e7/crates%2Fcore_simd%2Fsrc%2Fops.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4bbef261b6dd8529dadfd41c04b72137657bc7e7/crates%2Fcore_simd%2Fsrc%2Fops.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fcore_simd%2Fsrc%2Fops.rs?ref=4bbef261b6dd8529dadfd41c04b72137657bc7e7", "patch": "@@ -1,4 +1,3 @@\n-use crate::simd::intrinsics;\n use crate::simd::{LaneCount, Simd, SimdElement, SupportedLaneCount};\n use core::ops::{Add, Mul};\n use core::ops::{BitAnd, BitOr, BitXor};\n@@ -32,232 +31,206 @@ where\n     }\n }\n \n-/// Checks if the right-hand side argument of a left- or right-shift would cause overflow.\n-fn invalid_shift_rhs<T>(rhs: T) -> bool\n-where\n-    T: Default + PartialOrd + core::convert::TryFrom<usize>,\n-    <T as core::convert::TryFrom<usize>>::Error: core::fmt::Debug,\n-{\n-    let bits_in_type = T::try_from(8 * core::mem::size_of::<T>()).unwrap();\n-    rhs < T::default() || rhs >= bits_in_type\n+macro_rules! unsafe_base {\n+    ($lhs:ident, $rhs:ident, {$simd_call:ident}, $($_:tt)*) => {\n+        unsafe { $crate::intrinsics::$simd_call($lhs, $rhs) }\n+    };\n }\n \n-/// Automatically implements operators over references in addition to the provided operator.\n-macro_rules! impl_ref_ops {\n-    // binary op\n-    {\n-        impl<const $lanes:ident: usize> core::ops::$trait:ident<$rhs:ty> for $type:ty\n-        where\n-            LaneCount<$lanes2:ident>: SupportedLaneCount,\n-        {\n-            type Output = $output:ty;\n-\n-            $(#[$attrs:meta])*\n-            fn $fn:ident($self_tok:ident, $rhs_arg:ident: $rhs_arg_ty:ty) -> Self::Output $body:tt\n-        }\n-    } => {\n-        impl<const $lanes: usize> core::ops::$trait<$rhs> for $type\n-        where\n-            LaneCount<$lanes2>: SupportedLaneCount,\n-        {\n-            type Output = $output;\n-\n-            $(#[$attrs])*\n-            fn $fn($self_tok, $rhs_arg: $rhs_arg_ty) -> Self::Output $body\n+/// SAFETY: This macro should not be used for anything except Shl or Shr, and passed the appropriate shift intrinsic.\n+/// It handles performing a bitand in addition to calling the shift operator, so that the result\n+/// is well-defined: LLVM can return a poison value if you shl, lshr, or ashr if rhs >= <Int>::BITS\n+/// At worst, this will maybe add another instruction and cycle,\n+/// at best, it may open up more optimization opportunities,\n+/// or simply be elided entirely, especially for SIMD ISAs which default to this.\n+///\n+// FIXME: Consider implementing this in cg_llvm instead?\n+// cg_clif defaults to this, and scalar MIR shifts also default to wrapping\n+macro_rules! wrap_bitshift {\n+    ($lhs:ident, $rhs:ident, {$simd_call:ident}, $int:ident) => {\n+        unsafe {\n+            $crate::intrinsics::$simd_call($lhs, $rhs.bitand(Simd::splat(<$int>::BITS as $int - 1)))\n         }\n     };\n }\n \n-/// Automatically implements operators over vectors and scalars for a particular vector.\n-macro_rules! impl_op {\n-    { impl Add for $scalar:ty } => {\n-        impl_op! { @binary $scalar, Add::add, simd_add }\n-    };\n-    { impl Sub for $scalar:ty } => {\n-        impl_op! { @binary $scalar, Sub::sub, simd_sub }\n-    };\n-    { impl Mul for $scalar:ty } => {\n-        impl_op! { @binary $scalar, Mul::mul, simd_mul }\n-    };\n-    { impl Div for $scalar:ty } => {\n-        impl_op! { @binary $scalar, Div::div, simd_div }\n-    };\n-    { impl Rem for $scalar:ty } => {\n-        impl_op! { @binary $scalar, Rem::rem, simd_rem }\n-    };\n-    { impl Shl for $scalar:ty } => {\n-        impl_op! { @binary $scalar, Shl::shl, simd_shl }\n-    };\n-    { impl Shr for $scalar:ty } => {\n-        impl_op! { @binary $scalar, Shr::shr, simd_shr }\n-    };\n-    { impl BitAnd for $scalar:ty } => {\n-        impl_op! { @binary $scalar, BitAnd::bitand, simd_and }\n-    };\n-    { impl BitOr for $scalar:ty } => {\n-        impl_op! { @binary $scalar, BitOr::bitor, simd_or }\n-    };\n-    { impl BitXor for $scalar:ty } => {\n-        impl_op! { @binary $scalar, BitXor::bitxor, simd_xor }\n+// Division by zero is poison, according to LLVM.\n+// So is dividing the MIN value of a signed integer by -1,\n+// since that would return MAX + 1.\n+// FIXME: Rust allows <SInt>::MIN / -1,\n+// so we should probably figure out how to make that safe.\n+macro_rules! int_divrem_guard {\n+    (   $lhs:ident,\n+        $rhs:ident,\n+        {   const PANIC_ZERO: &'static str = $zero:literal;\n+            const PANIC_OVERFLOW: &'static str = $overflow:literal;\n+            $simd_call:ident\n+        },\n+        $int:ident ) => {\n+        if $rhs.lanes_eq(Simd::splat(0)).any() {\n+            panic!($zero);\n+        } else if <$int>::MIN != 0\n+            && ($lhs.lanes_eq(Simd::splat(<$int>::MIN)) & $rhs.lanes_eq(Simd::splat(-1 as _))).any()\n+        {\n+            panic!($overflow);\n+        } else {\n+            unsafe { $crate::intrinsics::$simd_call($lhs, $rhs) }\n+        }\n     };\n+}\n \n-    // generic binary op with assignment when output is `Self`\n-    { @binary $scalar:ty, $trait:ident :: $trait_fn:ident, $intrinsic:ident } => {\n-        impl_ref_ops! {\n-            impl<const LANES: usize> core::ops::$trait<Self> for Simd<$scalar, LANES>\n-            where\n-                LaneCount<LANES>: SupportedLaneCount,\n-            {\n-                type Output = Self;\n+macro_rules! for_base_types {\n+    (   T = ($($scalar:ident),*);\n+        type Lhs = Simd<T, N>;\n+        type Rhs = Simd<T, N>;\n+        type Output = $out:ty;\n+\n+        impl $op:ident::$call:ident {\n+            $macro_impl:ident $inner:tt\n+        }) => {\n+            $(\n+                impl<const N: usize> $op<Self> for Simd<$scalar, N>\n+                where\n+                    $scalar: SimdElement,\n+                    LaneCount<N>: SupportedLaneCount,\n+                {\n+                    type Output = $out;\n \n-                #[inline]\n-                fn $trait_fn(self, rhs: Self) -> Self::Output {\n-                    unsafe {\n-                        intrinsics::$intrinsic(self, rhs)\n+                    #[inline]\n+                    #[must_use = \"operator returns a new vector without mutating the inputs\"]\n+                    fn $call(self, rhs: Self) -> Self::Output {\n+                        $macro_impl!(self, rhs, $inner, $scalar)\n                     }\n-                }\n-            }\n-        }\n-    };\n+                })*\n+    }\n }\n \n-/// Implements floating-point operators for the provided types.\n-macro_rules! impl_float_ops {\n-    { $($scalar:ty),* } => {\n-        $(\n-            impl_op! { impl Add for $scalar }\n-            impl_op! { impl Sub for $scalar }\n-            impl_op! { impl Mul for $scalar }\n-            impl_op! { impl Div for $scalar }\n-            impl_op! { impl Rem for $scalar }\n-        )*\n+// A \"TokenTree muncher\": takes a set of scalar types `T = {};`\n+// type parameters for the ops it implements, `Op::fn` names,\n+// and a macro that expands into an expr, substituting in an intrinsic.\n+// It passes that to for_base_types, which expands an impl for the types,\n+// using the expanded expr in the function, and recurses with itself.\n+//\n+// tl;dr impls a set of ops::{Traits} for a set of types\n+macro_rules! for_base_ops {\n+    (\n+        T = $types:tt;\n+        type Lhs = Simd<T, N>;\n+        type Rhs = Simd<T, N>;\n+        type Output = $out:ident;\n+        impl $op:ident::$call:ident\n+            $inner:tt\n+        $($rest:tt)*\n+    ) => {\n+        for_base_types! {\n+            T = $types;\n+            type Lhs = Simd<T, N>;\n+            type Rhs = Simd<T, N>;\n+            type Output = $out;\n+            impl $op::$call\n+                $inner\n+        }\n+        for_base_ops! {\n+            T = $types;\n+            type Lhs = Simd<T, N>;\n+            type Rhs = Simd<T, N>;\n+            type Output = $out;\n+            $($rest)*\n+        }\n     };\n+    ($($done:tt)*) => {\n+        // Done.\n+    }\n }\n \n-/// Implements unsigned integer operators for the provided types.\n-macro_rules! impl_unsigned_int_ops {\n-    { $($scalar:ty),* } => {\n-        $(\n-            impl_op! { impl Add for $scalar }\n-            impl_op! { impl Sub for $scalar }\n-            impl_op! { impl Mul for $scalar }\n-            impl_op! { impl BitAnd for $scalar }\n-            impl_op! { impl BitOr  for $scalar }\n-            impl_op! { impl BitXor for $scalar }\n+// Integers can always accept add, mul, sub, bitand, bitor, and bitxor.\n+// For all of these operations, simd_* intrinsics apply wrapping logic.\n+for_base_ops! {\n+    T = (i8, i16, i32, i64, isize, u8, u16, u32, u64, usize);\n+    type Lhs = Simd<T, N>;\n+    type Rhs = Simd<T, N>;\n+    type Output = Self;\n \n-            // Integers panic on divide by 0\n-            impl_ref_ops! {\n-                impl<const LANES: usize> core::ops::Div<Self> for Simd<$scalar, LANES>\n-                where\n-                    LaneCount<LANES>: SupportedLaneCount,\n-                {\n-                    type Output = Self;\n+    impl Add::add {\n+        unsafe_base { simd_add }\n+    }\n \n-                    #[inline]\n-                    fn div(self, rhs: Self) -> Self::Output {\n-                        if rhs.as_array()\n-                            .iter()\n-                            .any(|x| *x == 0)\n-                        {\n-                            panic!(\"attempt to divide by zero\");\n-                        }\n+    impl Mul::mul {\n+        unsafe_base { simd_mul }\n+    }\n \n-                        // Guards for div(MIN, -1),\n-                        // this check only applies to signed ints\n-                        if <$scalar>::MIN != 0 && self.as_array().iter()\n-                                .zip(rhs.as_array().iter())\n-                                .any(|(x,y)| *x == <$scalar>::MIN && *y == -1 as _) {\n-                            panic!(\"attempt to divide with overflow\");\n-                        }\n-                        unsafe { intrinsics::simd_div(self, rhs) }\n-                    }\n-                }\n-            }\n+    impl Sub::sub {\n+        unsafe_base { simd_sub }\n+    }\n \n-            // remainder panics on zero divisor\n-            impl_ref_ops! {\n-                impl<const LANES: usize> core::ops::Rem<Self> for Simd<$scalar, LANES>\n-                where\n-                    LaneCount<LANES>: SupportedLaneCount,\n-                {\n-                    type Output = Self;\n+    impl BitAnd::bitand {\n+        unsafe_base { simd_and }\n+    }\n \n-                    #[inline]\n-                    fn rem(self, rhs: Self) -> Self::Output {\n-                        if rhs.as_array()\n-                            .iter()\n-                            .any(|x| *x == 0)\n-                        {\n-                            panic!(\"attempt to calculate the remainder with a divisor of zero\");\n-                        }\n+    impl BitOr::bitor {\n+        unsafe_base { simd_or }\n+    }\n \n-                        // Guards for rem(MIN, -1)\n-                        // this branch applies the check only to signed ints\n-                        if <$scalar>::MIN != 0 && self.as_array().iter()\n-                                .zip(rhs.as_array().iter())\n-                                .any(|(x,y)| *x == <$scalar>::MIN && *y == -1 as _) {\n-                            panic!(\"attempt to calculate the remainder with overflow\");\n-                        }\n-                        unsafe { intrinsics::simd_rem(self, rhs) }\n-                    }\n-                }\n-            }\n+    impl BitXor::bitxor {\n+        unsafe_base { simd_xor }\n+    }\n \n-            // shifts panic on overflow\n-            impl_ref_ops! {\n-                impl<const LANES: usize> core::ops::Shl<Self> for Simd<$scalar, LANES>\n-                where\n-                    LaneCount<LANES>: SupportedLaneCount,\n-                {\n-                    type Output = Self;\n+    impl Div::div {\n+        int_divrem_guard {\n+            const PANIC_ZERO: &'static str = \"attempt to divide by zero\";\n+            const PANIC_OVERFLOW: &'static str = \"attempt to divide with overflow\";\n+            simd_div\n+        }\n+    }\n \n-                    #[inline]\n-                    fn shl(self, rhs: Self) -> Self::Output {\n-                        // TODO there is probably a better way of doing this\n-                        if rhs.as_array()\n-                            .iter()\n-                            .copied()\n-                            .any(invalid_shift_rhs)\n-                        {\n-                            panic!(\"attempt to shift left with overflow\");\n-                        }\n-                        unsafe { intrinsics::simd_shl(self, rhs) }\n-                    }\n-                }\n-            }\n+    impl Rem::rem {\n+        int_divrem_guard {\n+            const PANIC_ZERO: &'static str = \"attempt to calculate the remainder with a divisor of zero\";\n+            const PANIC_OVERFLOW: &'static str = \"attempt to calculate the remainder with overflow\";\n+            simd_rem\n+        }\n+    }\n \n-            impl_ref_ops! {\n-                impl<const LANES: usize> core::ops::Shr<Self> for Simd<$scalar, LANES>\n-                where\n-                    LaneCount<LANES>: SupportedLaneCount,\n-                {\n-                    type Output = Self;\n+    // The only question is how to handle shifts >= <Int>::BITS?\n+    // Our current solution uses wrapping logic.\n+    impl Shl::shl {\n+        wrap_bitshift { simd_shl }\n+    }\n \n-                    #[inline]\n-                    fn shr(self, rhs: Self) -> Self::Output {\n-                        // TODO there is probably a better way of doing this\n-                        if rhs.as_array()\n-                            .iter()\n-                            .copied()\n-                            .any(invalid_shift_rhs)\n-                        {\n-                            panic!(\"attempt to shift with overflow\");\n-                        }\n-                        unsafe { intrinsics::simd_shr(self, rhs) }\n-                    }\n-                }\n-            }\n-        )*\n-    };\n+    impl Shr::shr {\n+        wrap_bitshift {\n+            // This automatically monomorphizes to lshr or ashr, depending,\n+            // so it's fine to use it for both UInts and SInts.\n+            simd_shr\n+        }\n+    }\n }\n \n-/// Implements unsigned integer operators for the provided types.\n-macro_rules! impl_signed_int_ops {\n-    { $($scalar:ty),* } => {\n-        impl_unsigned_int_ops! { $($scalar),* }\n-    };\n-}\n+// We don't need any special precautions here:\n+// Floats always accept arithmetic ops, but may become NaN.\n+for_base_ops! {\n+    T = (f32, f64);\n+    type Lhs = Simd<T, N>;\n+    type Rhs = Simd<T, N>;\n+    type Output = Self;\n+\n+    impl Add::add {\n+        unsafe_base { simd_add }\n+    }\n \n-impl_unsigned_int_ops! { u8, u16, u32, u64, usize }\n-impl_signed_int_ops! { i8, i16, i32, i64, isize }\n-impl_float_ops! { f32, f64 }\n+    impl Mul::mul {\n+        unsafe_base { simd_mul }\n+    }\n+\n+    impl Sub::sub {\n+        unsafe_base { simd_sub }\n+    }\n+\n+    impl Div::div {\n+        unsafe_base { simd_div }\n+    }\n+\n+    impl Rem::rem {\n+        unsafe_base { simd_rem }\n+    }\n+}"}]}