{"sha": "ceb29e2ac45474a560b04ce4061d8a6cc50e1a33", "node_id": "MDY6Q29tbWl0NzI0NzEyOmNlYjI5ZTJhYzQ1NDc0YTU2MGIwNGNlNDA2MWQ4YTZjYzUwZTFhMzM=", "commit": {"author": {"name": "bjorn3", "email": "bjorn3@users.noreply.github.com", "date": "2018-11-27T18:00:25Z"}, "committer": {"name": "bjorn3", "email": "bjorn3@users.noreply.github.com", "date": "2018-11-29T17:19:44Z"}, "message": "Use implicit deref instead of BuilderMethods::cx()", "tree": {"sha": "876117be9a8cc977e6956e7082bd673117318485", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/876117be9a8cc977e6956e7082bd673117318485"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/ceb29e2ac45474a560b04ce4061d8a6cc50e1a33", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/ceb29e2ac45474a560b04ce4061d8a6cc50e1a33", "html_url": "https://github.com/rust-lang/rust/commit/ceb29e2ac45474a560b04ce4061d8a6cc50e1a33", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/ceb29e2ac45474a560b04ce4061d8a6cc50e1a33/comments", "author": {"login": "bjorn3", "id": 17426603, "node_id": "MDQ6VXNlcjE3NDI2NjAz", "avatar_url": "https://avatars.githubusercontent.com/u/17426603?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bjorn3", "html_url": "https://github.com/bjorn3", "followers_url": "https://api.github.com/users/bjorn3/followers", "following_url": "https://api.github.com/users/bjorn3/following{/other_user}", "gists_url": "https://api.github.com/users/bjorn3/gists{/gist_id}", "starred_url": "https://api.github.com/users/bjorn3/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bjorn3/subscriptions", "organizations_url": "https://api.github.com/users/bjorn3/orgs", "repos_url": "https://api.github.com/users/bjorn3/repos", "events_url": "https://api.github.com/users/bjorn3/events{/privacy}", "received_events_url": "https://api.github.com/users/bjorn3/received_events", "type": "User", "site_admin": false}, "committer": {"login": "bjorn3", "id": 17426603, "node_id": "MDQ6VXNlcjE3NDI2NjAz", "avatar_url": "https://avatars.githubusercontent.com/u/17426603?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bjorn3", "html_url": "https://github.com/bjorn3", "followers_url": "https://api.github.com/users/bjorn3/followers", "following_url": "https://api.github.com/users/bjorn3/following{/other_user}", "gists_url": "https://api.github.com/users/bjorn3/gists{/gist_id}", "starred_url": "https://api.github.com/users/bjorn3/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bjorn3/subscriptions", "organizations_url": "https://api.github.com/users/bjorn3/orgs", "repos_url": "https://api.github.com/users/bjorn3/repos", "events_url": "https://api.github.com/users/bjorn3/events{/privacy}", "received_events_url": "https://api.github.com/users/bjorn3/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "e45733048eb06da0976e736bb44fe906495d65e9", "url": "https://api.github.com/repos/rust-lang/rust/commits/e45733048eb06da0976e736bb44fe906495d65e9", "html_url": "https://github.com/rust-lang/rust/commit/e45733048eb06da0976e736bb44fe906495d65e9"}], "stats": {"total": 541, "additions": 270, "deletions": 271}, "files": [{"sha": "5b6d157043d864ad6b8195d38acbdb2c395fe941", "filename": "src/librustc_codegen_llvm/abi.rs", "status": "modified", "additions": 7, "deletions": 7, "changes": 14, "blob_url": "https://github.com/rust-lang/rust/blob/ceb29e2ac45474a560b04ce4061d8a6cc50e1a33/src%2Flibrustc_codegen_llvm%2Fabi.rs", "raw_url": "https://github.com/rust-lang/rust/raw/ceb29e2ac45474a560b04ce4061d8a6cc50e1a33/src%2Flibrustc_codegen_llvm%2Fabi.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_codegen_llvm%2Fabi.rs?ref=ceb29e2ac45474a560b04ce4061d8a6cc50e1a33", "patch": "@@ -212,7 +212,7 @@ impl ArgTypeExt<'ll, 'tcx> for ArgType<'tcx, Ty<'tcx>> {\n             // uses it for i16 -> {i8, i8}, but not for i24 -> {i8, i8, i8}.\n             let can_store_through_cast_ptr = false;\n             if can_store_through_cast_ptr {\n-                let cast_ptr_llty = bx.cx().type_ptr_to(cast.llvm_type(bx.cx()));\n+                let cast_ptr_llty = bx.type_ptr_to(cast.llvm_type(bx));\n                 let cast_dst = bx.pointercast(dst.llval, cast_ptr_llty);\n                 bx.store(val, cast_dst, self.layout.align.abi);\n             } else {\n@@ -231,9 +231,9 @@ impl ArgTypeExt<'ll, 'tcx> for ArgType<'tcx, Ty<'tcx>> {\n                 //   bitcasting to the struct type yields invalid cast errors.\n \n                 // We instead thus allocate some scratch space...\n-                let scratch_size = cast.size(bx.cx());\n-                let scratch_align = cast.align(bx.cx());\n-                let llscratch = bx.alloca(cast.llvm_type(bx.cx()), \"abi_cast\", scratch_align);\n+                let scratch_size = cast.size(bx);\n+                let scratch_align = cast.align(bx);\n+                let llscratch = bx.alloca(cast.llvm_type(bx), \"abi_cast\", scratch_align);\n                 bx.lifetime_start(llscratch, scratch_size);\n \n                 // ...where we first store the value...\n@@ -245,7 +245,7 @@ impl ArgTypeExt<'ll, 'tcx> for ArgType<'tcx, Ty<'tcx>> {\n                     self.layout.align.abi,\n                     llscratch,\n                     scratch_align,\n-                    bx.cx().const_usize(self.layout.size.bytes()),\n+                    bx.const_usize(self.layout.size.bytes()),\n                     MemFlags::empty()\n                 );\n \n@@ -299,7 +299,7 @@ impl ArgTypeMethods<'tcx> for Builder<'a, 'll, 'tcx> {\n         ty.store(self, val, dst)\n     }\n     fn memory_ty(&self, ty: &ArgType<'tcx, Ty<'tcx>>) -> &'ll Type {\n-        ty.memory_ty(self.cx())\n+        ty.memory_ty(self)\n     }\n }\n \n@@ -780,7 +780,7 @@ impl<'tcx> FnTypeExt<'tcx> for FnType<'tcx, Ty<'tcx>> {\n             // by the LLVM verifier.\n             if let layout::Int(..) = scalar.value {\n                 if !scalar.is_bool() {\n-                    let range = scalar.valid_range_exclusive(bx.cx());\n+                    let range = scalar.valid_range_exclusive(bx);\n                     if range.start != range.end {\n                         bx.range_metadata(callsite, range);\n                     }"}, {"sha": "294596cea5f157ff0145ded25094d96742c1cc1f", "filename": "src/librustc_codegen_llvm/asm.rs", "status": "modified", "additions": 6, "deletions": 6, "changes": 12, "blob_url": "https://github.com/rust-lang/rust/blob/ceb29e2ac45474a560b04ce4061d8a6cc50e1a33/src%2Flibrustc_codegen_llvm%2Fasm.rs", "raw_url": "https://github.com/rust-lang/rust/raw/ceb29e2ac45474a560b04ce4061d8a6cc50e1a33/src%2Flibrustc_codegen_llvm%2Fasm.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_codegen_llvm%2Fasm.rs?ref=ceb29e2ac45474a560b04ce4061d8a6cc50e1a33", "patch": "@@ -57,7 +57,7 @@ impl AsmBuilderMethods<'tcx> for Builder<'a, 'll, 'tcx> {\n \n         // Default per-arch clobbers\n         // Basically what clang does\n-        let arch_clobbers = match &self.cx().sess().target.target.arch[..] {\n+        let arch_clobbers = match &self.sess().target.target.arch[..] {\n             \"x86\" | \"x86_64\"  => vec![\"~{dirflag}\", \"~{fpsr}\", \"~{flags}\"],\n             \"mips\" | \"mips64\" => vec![\"~{$1}\"],\n             _                 => Vec::new()\n@@ -76,9 +76,9 @@ impl AsmBuilderMethods<'tcx> for Builder<'a, 'll, 'tcx> {\n         // Depending on how many outputs we have, the return type is different\n         let num_outputs = output_types.len();\n         let output_type = match num_outputs {\n-            0 => self.cx().type_void(),\n+            0 => self.type_void(),\n             1 => output_types[0],\n-            _ => self.cx().type_struct(&output_types, false)\n+            _ => self.type_struct(&output_types, false)\n         };\n \n         let asm = CString::new(ia.asm.as_str().as_bytes()).unwrap();\n@@ -108,13 +108,13 @@ impl AsmBuilderMethods<'tcx> for Builder<'a, 'll, 'tcx> {\n         // back to source locations.  See #17552.\n         unsafe {\n             let key = \"srcloc\";\n-            let kind = llvm::LLVMGetMDKindIDInContext(self.cx().llcx,\n+            let kind = llvm::LLVMGetMDKindIDInContext(self.llcx,\n                 key.as_ptr() as *const c_char, key.len() as c_uint);\n \n-            let val: &'ll Value = self.cx().const_i32(ia.ctxt.outer().as_u32() as i32);\n+            let val: &'ll Value = self.const_i32(ia.ctxt.outer().as_u32() as i32);\n \n             llvm::LLVMSetMetadata(r, kind,\n-                llvm::LLVMMDNodeInContext(self.cx().llcx, &val, 1));\n+                llvm::LLVMMDNodeInContext(self.llcx, &val, 1));\n         }\n \n         true"}, {"sha": "91c650f1b53296158b45ccb4dddfb02f6f32932a", "filename": "src/librustc_codegen_llvm/builder.rs", "status": "modified", "additions": 29, "deletions": 29, "changes": 58, "blob_url": "https://github.com/rust-lang/rust/blob/ceb29e2ac45474a560b04ce4061d8a6cc50e1a33/src%2Flibrustc_codegen_llvm%2Fbuilder.rs", "raw_url": "https://github.com/rust-lang/rust/raw/ceb29e2ac45474a560b04ce4061d8a6cc50e1a33/src%2Flibrustc_codegen_llvm%2Fbuilder.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_codegen_llvm%2Fbuilder.rs?ref=ceb29e2ac45474a560b04ce4061d8a6cc50e1a33", "patch": "@@ -143,11 +143,11 @@ impl BuilderMethods<'a, 'tcx> for Builder<'a, 'll, 'tcx> {\n     }\n \n     fn count_insn(&self, category: &str) {\n-        if self.cx().sess().codegen_stats() {\n-            self.cx().stats.borrow_mut().n_llvm_insns += 1;\n+        if self.sess().codegen_stats() {\n+            self.stats.borrow_mut().n_llvm_insns += 1;\n         }\n-        if self.cx().sess().count_llvm_insns() {\n-            *self.cx().stats\n+        if self.sess().count_llvm_insns() {\n+            *self.stats\n                       .borrow_mut()\n                       .llvm_insns\n                       .entry(category.to_string())\n@@ -475,8 +475,8 @@ impl BuilderMethods<'a, 'tcx> for Builder<'a, 'll, 'tcx> {\n         use rustc::ty::{Int, Uint};\n \n         let new_sty = match ty.sty {\n-            Int(Isize) => Int(self.cx().tcx.sess.target.isize_ty),\n-            Uint(Usize) => Uint(self.cx().tcx.sess.target.usize_ty),\n+            Int(Isize) => Int(self.tcx.sess.target.isize_ty),\n+            Uint(Usize) => Uint(self.tcx.sess.target.usize_ty),\n             ref t @ Uint(_) | ref t @ Int(_) => t.clone(),\n             _ => panic!(\"tried to get overflow intrinsic for op applied to non-int type\")\n         };\n@@ -529,7 +529,7 @@ impl BuilderMethods<'a, 'tcx> for Builder<'a, 'll, 'tcx> {\n             },\n         };\n \n-        let intrinsic = self.cx().get_intrinsic(&name);\n+        let intrinsic = self.get_intrinsic(&name);\n         let res = self.call(intrinsic, &[lhs, rhs], None);\n         (\n             self.extract_value(res, 0),\n@@ -637,7 +637,7 @@ impl BuilderMethods<'a, 'tcx> for Builder<'a, 'll, 'tcx> {\n             let vr = scalar.valid_range.clone();\n             match scalar.value {\n                 layout::Int(..) => {\n-                    let range = scalar.valid_range_exclusive(bx.cx());\n+                    let range = scalar.valid_range_exclusive(bx);\n                     if range.start != range.end {\n                         bx.range_metadata(load, range);\n                     }\n@@ -676,7 +676,7 @@ impl BuilderMethods<'a, 'tcx> for Builder<'a, 'll, 'tcx> {\n                 let load = self.load(llptr, align);\n                 scalar_load_metadata(self, load, scalar);\n                 if scalar.is_bool() {\n-                    self.trunc(load, self.cx().type_i1())\n+                    self.trunc(load, self.type_i1())\n                 } else {\n                     load\n                 }\n@@ -696,7 +696,7 @@ impl BuilderMethods<'a, 'tcx> for Builder<'a, 'll, 'tcx> {\n \n \n     fn range_metadata(&mut self, load: &'ll Value, range: Range<u128>) {\n-        if self.cx().sess().target.target.arch == \"amdgpu\" {\n+        if self.sess().target.target.arch == \"amdgpu\" {\n             // amdgpu/LLVM does something weird and thinks a i64 value is\n             // split into a v2i32, halving the bitwidth LLVM expects,\n             // tripping an assertion. So, for now, just disable this\n@@ -942,7 +942,7 @@ impl BuilderMethods<'a, 'tcx> for Builder<'a, 'll, 'tcx> {\n         }).collect::<Vec<_>>();\n \n         debug!(\"Asm Output Type: {:?}\", output);\n-        let fty = self.cx().type_func(&argtys[..], output);\n+        let fty = self.type_func(&argtys[..], output);\n         unsafe {\n             // Ask LLVM to verify that the constraints are well-formed.\n             let constraints_ok = llvm::LLVMRustInlineAsmVerify(fty, cons.as_ptr());\n@@ -970,14 +970,14 @@ impl BuilderMethods<'a, 'tcx> for Builder<'a, 'll, 'tcx> {\n         if flags.contains(MemFlags::NONTEMPORAL) {\n             // HACK(nox): This is inefficient but there is no nontemporal memcpy.\n             let val = self.load(src, src_align);\n-            let ptr = self.pointercast(dst, self.cx().type_ptr_to(self.cx().val_ty(val)));\n+            let ptr = self.pointercast(dst, self.type_ptr_to(self.val_ty(val)));\n             self.store_with_flags(val, ptr, dst_align, flags);\n             return;\n         }\n-        let size = self.intcast(size, self.cx().type_isize(), false);\n+        let size = self.intcast(size, self.type_isize(), false);\n         let is_volatile = flags.contains(MemFlags::VOLATILE);\n-        let dst = self.pointercast(dst, self.cx().type_i8p());\n-        let src = self.pointercast(src, self.cx().type_i8p());\n+        let dst = self.pointercast(dst, self.type_i8p());\n+        let src = self.pointercast(src, self.type_i8p());\n         unsafe {\n             llvm::LLVMRustBuildMemCpy(self.llbuilder, dst, dst_align.bytes() as c_uint,\n                                       src, src_align.bytes() as c_uint, size, is_volatile);\n@@ -990,14 +990,14 @@ impl BuilderMethods<'a, 'tcx> for Builder<'a, 'll, 'tcx> {\n         if flags.contains(MemFlags::NONTEMPORAL) {\n             // HACK(nox): This is inefficient but there is no nontemporal memmove.\n             let val = self.load(src, src_align);\n-            let ptr = self.pointercast(dst, self.cx().type_ptr_to(self.cx().val_ty(val)));\n+            let ptr = self.pointercast(dst, self.type_ptr_to(self.val_ty(val)));\n             self.store_with_flags(val, ptr, dst_align, flags);\n             return;\n         }\n-        let size = self.intcast(size, self.cx().type_isize(), false);\n+        let size = self.intcast(size, self.type_isize(), false);\n         let is_volatile = flags.contains(MemFlags::VOLATILE);\n-        let dst = self.pointercast(dst, self.cx().type_i8p());\n-        let src = self.pointercast(src, self.cx().type_i8p());\n+        let dst = self.pointercast(dst, self.type_i8p());\n+        let src = self.pointercast(src, self.type_i8p());\n         unsafe {\n             llvm::LLVMRustBuildMemMove(self.llbuilder, dst, dst_align.bytes() as c_uint,\n                                       src, src_align.bytes() as c_uint, size, is_volatile);\n@@ -1012,12 +1012,12 @@ impl BuilderMethods<'a, 'tcx> for Builder<'a, 'll, 'tcx> {\n         align: Align,\n         flags: MemFlags,\n     ) {\n-        let ptr_width = &self.cx().sess().target.target.target_pointer_width;\n+        let ptr_width = &self.sess().target.target.target_pointer_width;\n         let intrinsic_key = format!(\"llvm.memset.p0i8.i{}\", ptr_width);\n-        let llintrinsicfn = self.cx().get_intrinsic(&intrinsic_key);\n-        let ptr = self.pointercast(ptr, self.cx().type_i8p());\n-        let align = self.cx().const_u32(align.bytes() as u32);\n-        let volatile = self.cx().const_bool(flags.contains(MemFlags::VOLATILE));\n+        let llintrinsicfn = self.get_intrinsic(&intrinsic_key);\n+        let ptr = self.pointercast(ptr, self.type_i8p());\n+        let align = self.const_u32(align.bytes() as u32);\n+        let volatile = self.const_bool(flags.contains(MemFlags::VOLATILE));\n         self.call(llintrinsicfn, &[ptr, fill_byte, size, align, volatile], None);\n     }\n \n@@ -1083,10 +1083,10 @@ impl BuilderMethods<'a, 'tcx> for Builder<'a, 'll, 'tcx> {\n     fn vector_splat(&mut self, num_elts: usize, elt: &'ll Value) -> &'ll Value {\n         unsafe {\n             let elt_ty = self.cx.val_ty(elt);\n-            let undef = llvm::LLVMGetUndef(self.cx().type_vector(elt_ty, num_elts as u64));\n+            let undef = llvm::LLVMGetUndef(self.type_vector(elt_ty, num_elts as u64));\n             let vec = self.insert_element(undef, elt, self.cx.const_i32(0));\n-            let vec_i32_ty = self.cx().type_vector(self.cx().type_i32(), num_elts as u64);\n-            self.shuffle_vector(vec, undef, self.cx().const_null(vec_i32_ty))\n+            let vec_i32_ty = self.type_vector(self.type_i32(), num_elts as u64);\n+            self.shuffle_vector(vec, undef, self.const_null(vec_i32_ty))\n         }\n     }\n \n@@ -1397,7 +1397,7 @@ impl BuilderMethods<'a, 'tcx> for Builder<'a, 'll, 'tcx> {\n         let param_tys = self.cx.func_params_types(fn_ty);\n \n         let all_args_match = param_tys.iter()\n-            .zip(args.iter().map(|&v| self.cx().val_ty(v)))\n+            .zip(args.iter().map(|&v| self.val_ty(v)))\n             .all(|(expected_ty, actual_ty)| *expected_ty == actual_ty);\n \n         if all_args_match {\n@@ -1408,7 +1408,7 @@ impl BuilderMethods<'a, 'tcx> for Builder<'a, 'll, 'tcx> {\n             .zip(args.iter())\n             .enumerate()\n             .map(|(i, (expected_ty, &actual_val))| {\n-                let actual_ty = self.cx().val_ty(actual_val);\n+                let actual_ty = self.val_ty(actual_val);\n                 if expected_ty != actual_ty {\n                     debug!(\"Type mismatch in function call of {:?}. \\\n                             Expected {:?} for param {}, got {:?}; injecting bitcast\","}, {"sha": "4be93d826b88faf0983678327f59fcbcbc82aac1", "filename": "src/librustc_codegen_llvm/debuginfo/gdb.rs", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "blob_url": "https://github.com/rust-lang/rust/blob/ceb29e2ac45474a560b04ce4061d8a6cc50e1a33/src%2Flibrustc_codegen_llvm%2Fdebuginfo%2Fgdb.rs", "raw_url": "https://github.com/rust-lang/rust/raw/ceb29e2ac45474a560b04ce4061d8a6cc50e1a33/src%2Flibrustc_codegen_llvm%2Fdebuginfo%2Fgdb.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_codegen_llvm%2Fdebuginfo%2Fgdb.rs?ref=ceb29e2ac45474a560b04ce4061d8a6cc50e1a33", "patch": "@@ -24,11 +24,11 @@ use syntax::attr;\n /// Inserts a side-effect free instruction sequence that makes sure that the\n /// .debug_gdb_scripts global is referenced, so it isn't removed by the linker.\n pub fn insert_reference_to_gdb_debug_scripts_section_global(bx: &mut Builder) {\n-    if needs_gdb_debug_scripts_section(bx.cx()) {\n-        let gdb_debug_scripts_section = get_or_insert_gdb_debug_scripts_section_global(bx.cx());\n+    if needs_gdb_debug_scripts_section(bx) {\n+        let gdb_debug_scripts_section = get_or_insert_gdb_debug_scripts_section_global(bx);\n         // Load just the first byte as that's all that's necessary to force\n         // LLVM to keep around the reference to the global.\n-        let indices = [bx.cx().const_i32(0), bx.cx().const_i32(0)];\n+        let indices = [bx.const_i32(0), bx.const_i32(0)];\n         let element = bx.inbounds_gep(gdb_debug_scripts_section, &indices);\n         let volative_load_instruction = bx.volatile_load(element);\n         unsafe {"}, {"sha": "95196287ab6ee1eb4100bfeeb4c14dce49b3aae0", "filename": "src/librustc_codegen_llvm/debuginfo/source_loc.rs", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/rust-lang/rust/blob/ceb29e2ac45474a560b04ce4061d8a6cc50e1a33/src%2Flibrustc_codegen_llvm%2Fdebuginfo%2Fsource_loc.rs", "raw_url": "https://github.com/rust-lang/rust/raw/ceb29e2ac45474a560b04ce4061d8a6cc50e1a33/src%2Flibrustc_codegen_llvm%2Fdebuginfo%2Fsource_loc.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_codegen_llvm%2Fdebuginfo%2Fsource_loc.rs?ref=ceb29e2ac45474a560b04ce4061d8a6cc50e1a33", "patch": "@@ -41,7 +41,7 @@ pub fn set_source_location<D>(\n     };\n \n     let dbg_loc = if function_debug_context.source_locations_enabled.get() {\n-        debug!(\"set_source_location: {}\", bx.cx().sess().source_map().span_to_string(span));\n+        debug!(\"set_source_location: {}\", bx.sess().source_map().span_to_string(span));\n         let loc = span_start(bx.cx(), span);\n         InternalDebugLocation::new(scope.unwrap(), loc.line, loc.col.to_usize())\n     } else {\n@@ -76,7 +76,7 @@ pub fn set_debug_location(\n             // For MSVC, set the column number to zero.\n             // Otherwise, emit it. This mimics clang behaviour.\n             // See discussion in https://github.com/rust-lang/rust/issues/42921\n-            let col_used =  if bx.cx().sess().target.target.options.is_like_msvc {\n+            let col_used =  if bx.sess().target.target.options.is_like_msvc {\n                 UNKNOWN_COLUMN_NUMBER\n             } else {\n                 col as c_uint"}, {"sha": "92c6d56a3d5976a067efa93d8d978d91afcc0b7c", "filename": "src/librustc_codegen_llvm/intrinsic.rs", "status": "modified", "additions": 119, "deletions": 119, "changes": 238, "blob_url": "https://github.com/rust-lang/rust/blob/ceb29e2ac45474a560b04ce4061d8a6cc50e1a33/src%2Flibrustc_codegen_llvm%2Fintrinsic.rs", "raw_url": "https://github.com/rust-lang/rust/raw/ceb29e2ac45474a560b04ce4061d8a6cc50e1a33/src%2Flibrustc_codegen_llvm%2Fintrinsic.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_codegen_llvm%2Fintrinsic.rs?ref=ceb29e2ac45474a560b04ce4061d8a6cc50e1a33", "patch": "@@ -96,7 +96,7 @@ impl IntrinsicCallMethods<'tcx> for Builder<'a, 'll, 'tcx> {\n         llresult: &'ll Value,\n         span: Span,\n     ) {\n-        let tcx = self.cx().tcx;\n+        let tcx = self.tcx;\n \n         let (def_id, substs) = match callee_ty.sty {\n             ty::FnDef(def_id, substs) => (def_id, substs),\n@@ -109,10 +109,10 @@ impl IntrinsicCallMethods<'tcx> for Builder<'a, 'll, 'tcx> {\n         let ret_ty = sig.output();\n         let name = &*tcx.item_name(def_id).as_str();\n \n-        let llret_ty = self.cx().layout_of(ret_ty).llvm_type(self.cx());\n+        let llret_ty = self.layout_of(ret_ty).llvm_type(self);\n         let result = PlaceRef::new_sized(llresult, fn_ty.ret.layout, fn_ty.ret.layout.align.abi);\n \n-        let simple = get_simple_intrinsic(self.cx(), name);\n+        let simple = get_simple_intrinsic(self, name);\n         let llval = match name {\n             _ if simple.is_some() => {\n                 self.call(simple.unwrap(),\n@@ -123,12 +123,12 @@ impl IntrinsicCallMethods<'tcx> for Builder<'a, 'll, 'tcx> {\n                 return;\n             },\n             \"likely\" => {\n-                let expect = self.cx().get_intrinsic(&(\"llvm.expect.i1\"));\n-                self.call(expect, &[args[0].immediate(), self.cx().const_bool(true)], None)\n+                let expect = self.get_intrinsic(&(\"llvm.expect.i1\"));\n+                self.call(expect, &[args[0].immediate(), self.const_bool(true)], None)\n             }\n             \"unlikely\" => {\n-                let expect = self.cx().get_intrinsic(&(\"llvm.expect.i1\"));\n-                self.call(expect, &[args[0].immediate(), self.cx().const_bool(false)], None)\n+                let expect = self.get_intrinsic(&(\"llvm.expect.i1\"));\n+                self.call(expect, &[args[0].immediate(), self.const_bool(false)], None)\n             }\n             \"try\" => {\n                 try_intrinsic(self,\n@@ -139,12 +139,12 @@ impl IntrinsicCallMethods<'tcx> for Builder<'a, 'll, 'tcx> {\n                 return;\n             }\n             \"breakpoint\" => {\n-                let llfn = self.cx().get_intrinsic(&(\"llvm.debugtrap\"));\n+                let llfn = self.get_intrinsic(&(\"llvm.debugtrap\"));\n                 self.call(llfn, &[], None)\n             }\n             \"size_of\" => {\n                 let tp_ty = substs.type_at(0);\n-                self.cx().const_usize(self.cx().size_of(tp_ty).bytes())\n+                self.const_usize(self.size_of(tp_ty).bytes())\n             }\n             \"size_of_val\" => {\n                 let tp_ty = substs.type_at(0);\n@@ -153,12 +153,12 @@ impl IntrinsicCallMethods<'tcx> for Builder<'a, 'll, 'tcx> {\n                         glue::size_and_align_of_dst(self, tp_ty, Some(meta));\n                     llsize\n                 } else {\n-                    self.cx().const_usize(self.cx().size_of(tp_ty).bytes())\n+                    self.const_usize(self.size_of(tp_ty).bytes())\n                 }\n             }\n             \"min_align_of\" => {\n                 let tp_ty = substs.type_at(0);\n-                self.cx().const_usize(self.cx().align_of(tp_ty).bytes())\n+                self.const_usize(self.align_of(tp_ty).bytes())\n             }\n             \"min_align_of_val\" => {\n                 let tp_ty = substs.type_at(0);\n@@ -167,24 +167,24 @@ impl IntrinsicCallMethods<'tcx> for Builder<'a, 'll, 'tcx> {\n                         glue::size_and_align_of_dst(self, tp_ty, Some(meta));\n                     llalign\n                 } else {\n-                    self.cx().const_usize(self.cx().align_of(tp_ty).bytes())\n+                    self.const_usize(self.align_of(tp_ty).bytes())\n                 }\n             }\n             \"pref_align_of\" => {\n                 let tp_ty = substs.type_at(0);\n-                self.cx().const_usize(self.cx().layout_of(tp_ty).align.pref.bytes())\n+                self.const_usize(self.layout_of(tp_ty).align.pref.bytes())\n             }\n             \"type_name\" => {\n                 let tp_ty = substs.type_at(0);\n                 let ty_name = Symbol::intern(&tp_ty.to_string()).as_str();\n-                self.cx().const_str_slice(ty_name)\n+                self.const_str_slice(ty_name)\n             }\n             \"type_id\" => {\n-                self.cx().const_u64(self.cx().tcx.type_id_hash(substs.type_at(0)))\n+                self.const_u64(self.tcx.type_id_hash(substs.type_at(0)))\n             }\n             \"init\" => {\n                 let ty = substs.type_at(0);\n-                if !self.cx().layout_of(ty).is_zst() {\n+                if !self.layout_of(ty).is_zst() {\n                     // Just zero out the stack slot.\n                     // If we store a zero constant, LLVM will drown in vreg allocation for large\n                     // data structures, and the generated code will be awful. (A telltale sign of\n@@ -194,8 +194,8 @@ impl IntrinsicCallMethods<'tcx> for Builder<'a, 'll, 'tcx> {\n                         false,\n                         ty,\n                         llresult,\n-                        self.cx().const_u8(0),\n-                        self.cx().const_usize(1)\n+                        self.const_u8(0),\n+                        self.const_usize(1)\n                     );\n                 }\n                 return;\n@@ -207,7 +207,7 @@ impl IntrinsicCallMethods<'tcx> for Builder<'a, 'll, 'tcx> {\n             \"needs_drop\" => {\n                 let tp_ty = substs.type_at(0);\n \n-                self.cx().const_bool(self.cx().type_needs_drop(tp_ty))\n+                self.const_bool(self.type_needs_drop(tp_ty))\n             }\n             \"offset\" => {\n                 let ptr = args[0].immediate();\n@@ -255,18 +255,18 @@ impl IntrinsicCallMethods<'tcx> for Builder<'a, 'll, 'tcx> {\n                 let tp_ty = substs.type_at(0);\n                 let mut ptr = args[0].immediate();\n                 if let PassMode::Cast(ty) = fn_ty.ret.mode {\n-                    ptr = self.pointercast(ptr, self.cx().type_ptr_to(ty.llvm_type(self.cx())));\n+                    ptr = self.pointercast(ptr, self.type_ptr_to(ty.llvm_type(self)));\n                 }\n                 let load = self.volatile_load(ptr);\n                 let align = if name == \"unaligned_volatile_load\" {\n                     1\n                 } else {\n-                    self.cx().align_of(tp_ty).bytes() as u32\n+                    self.align_of(tp_ty).bytes() as u32\n                 };\n                 unsafe {\n                     llvm::LLVMSetAlignment(load, align);\n                 }\n-                to_immediate(self, load, self.cx().layout_of(tp_ty))\n+                to_immediate(self, load, self.layout_of(tp_ty))\n             },\n             \"volatile_store\" => {\n                 let dst = args[0].deref(self.cx());\n@@ -280,7 +280,7 @@ impl IntrinsicCallMethods<'tcx> for Builder<'a, 'll, 'tcx> {\n             },\n             \"prefetch_read_data\" | \"prefetch_write_data\" |\n             \"prefetch_read_instruction\" | \"prefetch_write_instruction\" => {\n-                let expect = self.cx().get_intrinsic(&(\"llvm.prefetch\"));\n+                let expect = self.get_intrinsic(&(\"llvm.prefetch\"));\n                 let (rw, cache_type) = match name {\n                     \"prefetch_read_data\" => (0, 1),\n                     \"prefetch_write_data\" => (1, 1),\n@@ -290,9 +290,9 @@ impl IntrinsicCallMethods<'tcx> for Builder<'a, 'll, 'tcx> {\n                 };\n                 self.call(expect, &[\n                     args[0].immediate(),\n-                    self.cx().const_i32(rw),\n+                    self.const_i32(rw),\n                     args[1].immediate(),\n-                    self.cx().const_i32(cache_type)\n+                    self.const_i32(cache_type)\n                 ], None)\n             },\n             \"ctlz\" | \"ctlz_nonzero\" | \"cttz\" | \"cttz_nonzero\" | \"ctpop\" | \"bswap\" |\n@@ -301,24 +301,24 @@ impl IntrinsicCallMethods<'tcx> for Builder<'a, 'll, 'tcx> {\n             \"unchecked_div\" | \"unchecked_rem\" | \"unchecked_shl\" | \"unchecked_shr\" | \"exact_div\" |\n             \"rotate_left\" | \"rotate_right\" => {\n                 let ty = arg_tys[0];\n-                match int_type_width_signed(ty, self.cx()) {\n+                match int_type_width_signed(ty, self) {\n                     Some((width, signed)) =>\n                         match name {\n                             \"ctlz\" | \"cttz\" => {\n-                                let y = self.cx().const_bool(false);\n-                                let llfn = self.cx().get_intrinsic(\n+                                let y = self.const_bool(false);\n+                                let llfn = self.get_intrinsic(\n                                     &format!(\"llvm.{}.i{}\", name, width),\n                                 );\n                                 self.call(llfn, &[args[0].immediate(), y], None)\n                             }\n                             \"ctlz_nonzero\" | \"cttz_nonzero\" => {\n-                                let y = self.cx().const_bool(true);\n+                                let y = self.const_bool(true);\n                                 let llvm_name = &format!(\"llvm.{}.i{}\", &name[..4], width);\n-                                let llfn = self.cx().get_intrinsic(llvm_name);\n+                                let llfn = self.get_intrinsic(llvm_name);\n                                 self.call(llfn, &[args[0].immediate(), y], None)\n                             }\n                             \"ctpop\" => self.call(\n-                                self.cx().get_intrinsic(&format!(\"llvm.ctpop.i{}\", width)),\n+                                self.get_intrinsic(&format!(\"llvm.ctpop.i{}\", width)),\n                                 &[args[0].immediate()],\n                                 None\n                             ),\n@@ -327,7 +327,7 @@ impl IntrinsicCallMethods<'tcx> for Builder<'a, 'll, 'tcx> {\n                                     args[0].immediate() // byte swap a u8/i8 is just a no-op\n                                 } else {\n                                     self.call(\n-                                        self.cx().get_intrinsic(\n+                                        self.get_intrinsic(\n                                             &format!(\"llvm.bswap.i{}\", width),\n                                         ),\n                                         &[args[0].immediate()],\n@@ -337,7 +337,7 @@ impl IntrinsicCallMethods<'tcx> for Builder<'a, 'll, 'tcx> {\n                             }\n                             \"bitreverse\" => {\n                                 self.call(\n-                                    self.cx().get_intrinsic(\n+                                    self.get_intrinsic(\n                                         &format!(\"llvm.bitreverse.i{}\", width),\n                                     ),\n                                     &[args[0].immediate()],\n@@ -348,7 +348,7 @@ impl IntrinsicCallMethods<'tcx> for Builder<'a, 'll, 'tcx> {\n                                 let intrinsic = format!(\"llvm.{}{}.with.overflow.i{}\",\n                                                         if signed { 's' } else { 'u' },\n                                                         &name[..3], width);\n-                                let llfn = self.cx().get_intrinsic(&intrinsic);\n+                                let llfn = self.get_intrinsic(&intrinsic);\n \n                                 // Convert `i1` to a `bool`, and write it to the out parameter\n                                 let pair = self.call(llfn, &[\n@@ -357,7 +357,7 @@ impl IntrinsicCallMethods<'tcx> for Builder<'a, 'll, 'tcx> {\n                                 ], None);\n                                 let val = self.extract_value(pair, 0);\n                                 let overflow = self.extract_value(pair, 1);\n-                                let overflow = self.zext(overflow, self.cx().type_bool());\n+                                let overflow = self.zext(overflow, self.type_bool());\n \n                                 let dest = result.project_field(self, 0);\n                                 self.store(val, dest.llval, dest.align);\n@@ -402,13 +402,13 @@ impl IntrinsicCallMethods<'tcx> for Builder<'a, 'll, 'tcx> {\n                                     // rotate = funnel shift with first two args the same\n                                     let llvm_name = &format!(\"llvm.fsh{}.i{}\",\n                                                             if is_left { 'l' } else { 'r' }, width);\n-                                    let llfn = self.cx().get_intrinsic(llvm_name);\n+                                    let llfn = self.get_intrinsic(llvm_name);\n                                     self.call(llfn, &[val, val, raw_shift], None)\n                                 } else {\n                                     // rotate_left: (X << (S % BW)) | (X >> ((BW - S) % BW))\n                                     // rotate_right: (X << ((BW - S) % BW)) | (X >> (S % BW))\n-                                    let width = self.cx().const_uint(\n-                                        self.cx().type_ix(width),\n+                                    let width = self.const_uint(\n+                                        self.type_ix(width),\n                                         width,\n                                     );\n                                     let shift = self.urem(raw_shift, width);\n@@ -496,16 +496,16 @@ impl IntrinsicCallMethods<'tcx> for Builder<'a, 'll, 'tcx> {\n                             (SequentiallyConsistent, Monotonic),\n                         \"failacq\" if is_cxchg =>\n                             (SequentiallyConsistent, Acquire),\n-                        _ => self.cx().sess().fatal(\"unknown ordering in atomic intrinsic\")\n+                        _ => self.sess().fatal(\"unknown ordering in atomic intrinsic\")\n                     },\n                     4 => match (split[2], split[3]) {\n                         (\"acq\", \"failrelaxed\") if is_cxchg =>\n                             (Acquire, Monotonic),\n                         (\"acqrel\", \"failrelaxed\") if is_cxchg =>\n                             (AcquireRelease, Monotonic),\n-                        _ => self.cx().sess().fatal(\"unknown ordering in atomic intrinsic\")\n+                        _ => self.sess().fatal(\"unknown ordering in atomic intrinsic\")\n                     },\n-                    _ => self.cx().sess().fatal(\"Atomic intrinsic not in correct format\"),\n+                    _ => self.sess().fatal(\"Atomic intrinsic not in correct format\"),\n                 };\n \n                 let invalid_monomorphization = |ty| {\n@@ -517,7 +517,7 @@ impl IntrinsicCallMethods<'tcx> for Builder<'a, 'll, 'tcx> {\n                 match split[1] {\n                     \"cxchg\" | \"cxchgweak\" => {\n                         let ty = substs.type_at(0);\n-                        if int_type_width_signed(ty, self.cx()).is_some() {\n+                        if int_type_width_signed(ty, self).is_some() {\n                             let weak = split[1] == \"cxchgweak\";\n                             let pair = self.atomic_cmpxchg(\n                                 args[0].immediate(),\n@@ -528,7 +528,7 @@ impl IntrinsicCallMethods<'tcx> for Builder<'a, 'll, 'tcx> {\n                                 weak);\n                             let val = self.extract_value(pair, 0);\n                             let success = self.extract_value(pair, 1);\n-                            let success = self.zext(success, self.cx().type_bool());\n+                            let success = self.zext(success, self.type_bool());\n \n                             let dest = result.project_field(self, 0);\n                             self.store(val, dest.llval, dest.align);\n@@ -542,8 +542,8 @@ impl IntrinsicCallMethods<'tcx> for Builder<'a, 'll, 'tcx> {\n \n                     \"load\" => {\n                         let ty = substs.type_at(0);\n-                        if int_type_width_signed(ty, self.cx()).is_some() {\n-                            let size = self.cx().size_of(ty);\n+                        if int_type_width_signed(ty, self).is_some() {\n+                            let size = self.size_of(ty);\n                             self.atomic_load(args[0].immediate(), order, size)\n                         } else {\n                             return invalid_monomorphization(ty);\n@@ -552,8 +552,8 @@ impl IntrinsicCallMethods<'tcx> for Builder<'a, 'll, 'tcx> {\n \n                     \"store\" => {\n                         let ty = substs.type_at(0);\n-                        if int_type_width_signed(ty, self.cx()).is_some() {\n-                            let size = self.cx().size_of(ty);\n+                        if int_type_width_signed(ty, self).is_some() {\n+                            let size = self.size_of(ty);\n                             self.atomic_store(\n                                 args[1].immediate(),\n                                 args[0].immediate(),\n@@ -590,11 +590,11 @@ impl IntrinsicCallMethods<'tcx> for Builder<'a, 'll, 'tcx> {\n                             \"min\"   => AtomicRmwBinOp::AtomicMin,\n                             \"umax\"  => AtomicRmwBinOp::AtomicUMax,\n                             \"umin\"  => AtomicRmwBinOp::AtomicUMin,\n-                            _ => self.cx().sess().fatal(\"unknown atomic operation\")\n+                            _ => self.sess().fatal(\"unknown atomic operation\")\n                         };\n \n                         let ty = substs.type_at(0);\n-                        if int_type_width_signed(ty, self.cx()).is_some() {\n+                        if int_type_width_signed(ty, self).is_some() {\n                             self.atomic_rmw(\n                                 atom_op,\n                                 args[0].immediate(),\n@@ -681,7 +681,7 @@ impl IntrinsicCallMethods<'tcx> for Builder<'a, 'll, 'tcx> {\n                             // This assumes the type is \"simple\", i.e. no\n                             // destructors, and the contents are SIMD\n                             // etc.\n-                            assert!(!bx.cx().type_needs_drop(arg.layout.ty));\n+                            assert!(!bx.type_needs_drop(arg.layout.ty));\n                             let (ptr, align) = match arg.val {\n                                 OperandValue::Ref(ptr, None, align) => (ptr, align),\n                                 _ => bug!()\n@@ -693,32 +693,32 @@ impl IntrinsicCallMethods<'tcx> for Builder<'a, 'll, 'tcx> {\n                             }).collect()\n                         }\n                         intrinsics::Type::Pointer(_, Some(ref llvm_elem), _) => {\n-                            let llvm_elem = one(ty_to_type(bx.cx(), llvm_elem));\n-                            vec![bx.pointercast(arg.immediate(), bx.cx().type_ptr_to(llvm_elem))]\n+                            let llvm_elem = one(ty_to_type(bx, llvm_elem));\n+                            vec![bx.pointercast(arg.immediate(), bx.type_ptr_to(llvm_elem))]\n                         }\n                         intrinsics::Type::Vector(_, Some(ref llvm_elem), length) => {\n-                            let llvm_elem = one(ty_to_type(bx.cx(), llvm_elem));\n+                            let llvm_elem = one(ty_to_type(bx, llvm_elem));\n                             vec![\n                                 bx.bitcast(arg.immediate(),\n-                                bx.cx().type_vector(llvm_elem, length as u64))\n+                                bx.type_vector(llvm_elem, length as u64))\n                             ]\n                         }\n                         intrinsics::Type::Integer(_, width, llvm_width) if width != llvm_width => {\n                             // the LLVM intrinsic uses a smaller integer\n                             // size than the C intrinsic's signature, so\n                             // we have to trim it down here.\n-                            vec![bx.trunc(arg.immediate(), bx.cx().type_ix(llvm_width as u64))]\n+                            vec![bx.trunc(arg.immediate(), bx.type_ix(llvm_width as u64))]\n                         }\n                         _ => vec![arg.immediate()],\n                     }\n                 }\n \n \n                 let inputs = intr.inputs.iter()\n-                                        .flat_map(|t| ty_to_type(self.cx(), t))\n+                                        .flat_map(|t| ty_to_type(self, t))\n                                         .collect::<Vec<_>>();\n \n-                let outputs = one(ty_to_type(self.cx(), &intr.output));\n+                let outputs = one(ty_to_type(self, &intr.output));\n \n                 let llargs: Vec<_> = intr.inputs.iter().zip(args).flat_map(|(t, arg)| {\n                     modify_as_needed(self, t, arg)\n@@ -727,9 +727,9 @@ impl IntrinsicCallMethods<'tcx> for Builder<'a, 'll, 'tcx> {\n \n                 let val = match intr.definition {\n                     intrinsics::IntrinsicDef::Named(name) => {\n-                        let f = self.cx().declare_cfn(\n+                        let f = self.declare_cfn(\n                             name,\n-                            self.cx().type_func(&inputs, outputs),\n+                            self.type_func(&inputs, outputs),\n                         );\n                         self.call(f, &llargs, None)\n                     }\n@@ -754,7 +754,7 @@ impl IntrinsicCallMethods<'tcx> for Builder<'a, 'll, 'tcx> {\n \n         if !fn_ty.ret.is_ignore() {\n             if let PassMode::Cast(ty) = fn_ty.ret.mode {\n-                let ptr_llty = self.cx().type_ptr_to(ty.llvm_type(self.cx()));\n+                let ptr_llty = self.type_ptr_to(ty.llvm_type(self));\n                 let ptr = self.pointercast(result.llval, ptr_llty);\n                 self.store(llval, ptr, result.align);\n             } else {\n@@ -765,18 +765,18 @@ impl IntrinsicCallMethods<'tcx> for Builder<'a, 'll, 'tcx> {\n     }\n \n     fn abort(&mut self) {\n-        let fnname = self.cx().get_intrinsic(&(\"llvm.trap\"));\n+        let fnname = self.get_intrinsic(&(\"llvm.trap\"));\n         self.call(fnname, &[], None);\n     }\n \n     fn assume(&mut self, val: Self::Value) {\n-        let assume_intrinsic = self.cx().get_intrinsic(\"llvm.assume\");\n+        let assume_intrinsic = self.get_intrinsic(\"llvm.assume\");\n         self.call(assume_intrinsic, &[val], None);\n     }\n \n     fn expect(&mut self, cond: Self::Value, expected: bool) -> Self::Value {\n-        let expect = self.cx().get_intrinsic(&\"llvm.expect.i1\");\n-        self.call(expect, &[cond, self.cx().const_bool(expected)], None)\n+        let expect = self.get_intrinsic(&\"llvm.expect.i1\");\n+        self.call(expect, &[cond, self.const_bool(expected)], None)\n     }\n }\n \n@@ -789,8 +789,8 @@ fn copy_intrinsic(\n     src: &'ll Value,\n     count: &'ll Value,\n ) {\n-    let (size, align) = bx.cx().size_and_align_of(ty);\n-    let size = bx.mul(bx.cx().const_usize(size.bytes()), count);\n+    let (size, align) = bx.size_and_align_of(ty);\n+    let size = bx.mul(bx.const_usize(size.bytes()), count);\n     let flags = if volatile {\n         MemFlags::VOLATILE\n     } else {\n@@ -811,8 +811,8 @@ fn memset_intrinsic(\n     val: &'ll Value,\n     count: &'ll Value\n ) {\n-    let (size, align) = bx.cx().size_and_align_of(ty);\n-    let size = bx.mul(bx.cx().const_usize(size.bytes()), count);\n+    let (size, align) = bx.size_and_align_of(ty);\n+    let size = bx.mul(bx.const_usize(size.bytes()), count);\n     let flags = if volatile {\n         MemFlags::VOLATILE\n     } else {\n@@ -828,11 +828,11 @@ fn try_intrinsic(\n     local_ptr: &'ll Value,\n     dest: &'ll Value,\n ) {\n-    if bx.cx().sess().no_landing_pads() {\n+    if bx.sess().no_landing_pads() {\n         bx.call(func, &[data], None);\n         let ptr_align = bx.tcx().data_layout.pointer_align.abi;\n-        bx.store(bx.cx().const_null(bx.cx().type_i8p()), dest, ptr_align);\n-    } else if wants_msvc_seh(bx.cx().sess()) {\n+        bx.store(bx.const_null(bx.type_i8p()), dest, ptr_align);\n+    } else if wants_msvc_seh(bx.sess()) {\n         codegen_msvc_try(bx, func, data, local_ptr, dest);\n     } else {\n         codegen_gnu_try(bx, func, data, local_ptr, dest);\n@@ -853,8 +853,8 @@ fn codegen_msvc_try(\n     local_ptr: &'ll Value,\n     dest: &'ll Value,\n ) {\n-    let llfn = get_rust_try_fn(bx.cx(), &mut |mut bx| {\n-        bx.set_personality_fn(bx.cx().eh_personality());\n+    let llfn = get_rust_try_fn(bx, &mut |mut bx| {\n+        bx.set_personality_fn(bx.eh_personality());\n \n         let mut normal = bx.build_sibling_block(\"normal\");\n         let mut catchswitch = bx.build_sibling_block(\"catchswitch\");\n@@ -904,12 +904,12 @@ fn codegen_msvc_try(\n         //      }\n         //\n         // More information can be found in libstd's seh.rs implementation.\n-        let i64p = bx.cx().type_ptr_to(bx.cx().type_i64());\n+        let i64p = bx.type_ptr_to(bx.type_i64());\n         let ptr_align = bx.tcx().data_layout.pointer_align.abi;\n         let slot = bx.alloca(i64p, \"slot\", ptr_align);\n         bx.invoke(func, &[data], normal.llbb(), catchswitch.llbb(), None);\n \n-        normal.ret(bx.cx().const_i32(0));\n+        normal.ret(bx.const_i32(0));\n \n         let cs = catchswitch.catch_switch(None, None, 1);\n         catchswitch.add_handler(cs, catchpad.llbb());\n@@ -918,12 +918,12 @@ fn codegen_msvc_try(\n             Some(did) => bx.cx().get_static(did),\n             None => bug!(\"msvc_try_filter not defined\"),\n         };\n-        let funclet = catchpad.catch_pad(cs, &[tydesc, bx.cx().const_i32(0), slot]);\n+        let funclet = catchpad.catch_pad(cs, &[tydesc, bx.const_i32(0), slot]);\n         let addr = catchpad.load(slot, ptr_align);\n \n         let i64_align = bx.tcx().data_layout.i64_align.abi;\n         let arg1 = catchpad.load(addr, i64_align);\n-        let val1 = bx.cx().const_i32(1);\n+        let val1 = bx.const_i32(1);\n         let gep1 = catchpad.inbounds_gep(addr, &[val1]);\n         let arg2 = catchpad.load(gep1, i64_align);\n         let local_ptr = catchpad.bitcast(local_ptr, i64p);\n@@ -932,7 +932,7 @@ fn codegen_msvc_try(\n         catchpad.store(arg2, gep2, i64_align);\n         catchpad.catch_ret(&funclet, caught.llbb());\n \n-        caught.ret(bx.cx().const_i32(1));\n+        caught.ret(bx.const_i32(1));\n     });\n \n     // Note that no invoke is used here because by definition this function\n@@ -960,7 +960,7 @@ fn codegen_gnu_try(\n     local_ptr: &'ll Value,\n     dest: &'ll Value,\n ) {\n-    let llfn = get_rust_try_fn(bx.cx(), &mut |mut bx| {\n+    let llfn = get_rust_try_fn(bx, &mut |mut bx| {\n         // Codegens the shims described above:\n         //\n         //   bx:\n@@ -985,22 +985,22 @@ fn codegen_gnu_try(\n         let data = llvm::get_param(bx.llfn(), 1);\n         let local_ptr = llvm::get_param(bx.llfn(), 2);\n         bx.invoke(func, &[data], then.llbb(), catch.llbb(), None);\n-        then.ret(bx.cx().const_i32(0));\n+        then.ret(bx.const_i32(0));\n \n         // Type indicator for the exception being thrown.\n         //\n         // The first value in this tuple is a pointer to the exception object\n         // being thrown.  The second value is a \"selector\" indicating which of\n         // the landing pad clauses the exception's type had been matched to.\n         // rust_try ignores the selector.\n-        let lpad_ty = bx.cx().type_struct(&[bx.cx().type_i8p(), bx.cx().type_i32()], false);\n-        let vals = catch.landing_pad(lpad_ty, bx.cx().eh_personality(), 1);\n-        catch.add_clause(vals, bx.cx().const_null(bx.cx().type_i8p()));\n+        let lpad_ty = bx.type_struct(&[bx.type_i8p(), bx.type_i32()], false);\n+        let vals = catch.landing_pad(lpad_ty, bx.eh_personality(), 1);\n+        catch.add_clause(vals, bx.const_null(bx.type_i8p()));\n         let ptr = catch.extract_value(vals, 0);\n         let ptr_align = bx.tcx().data_layout.pointer_align.abi;\n-        let bitcast = catch.bitcast(local_ptr, bx.cx().type_ptr_to(bx.cx().type_i8p()));\n+        let bitcast = catch.bitcast(local_ptr, bx.type_ptr_to(bx.type_i8p()));\n         catch.store(ptr, bitcast, ptr_align);\n-        catch.ret(bx.cx().const_i32(1));\n+        catch.ret(bx.const_i32(1));\n     });\n \n     // Note that no invoke is used here because by definition this function\n@@ -1081,7 +1081,7 @@ fn generic_simd_intrinsic(\n         };\n         ($msg: tt, $($fmt: tt)*) => {\n             span_invalid_monomorphization_error(\n-                bx.cx().sess(), span,\n+                bx.sess(), span,\n                 &format!(concat!(\"invalid monomorphization of `{}` intrinsic: \", $msg),\n                          name, $($fmt)*));\n         }\n@@ -1142,7 +1142,7 @@ fn generic_simd_intrinsic(\n                   found `{}` with length {}\",\n                  in_len, in_ty,\n                  ret_ty, out_len);\n-        require!(bx.cx().type_kind(bx.cx().element_type(llret_ty)) == TypeKind::Integer,\n+        require!(bx.type_kind(bx.element_type(llret_ty)) == TypeKind::Integer,\n                  \"expected return type with integer elements, found `{}` with non-integer `{}`\",\n                  ret_ty,\n                  ret_ty.simd_type(tcx));\n@@ -1178,8 +1178,8 @@ fn generic_simd_intrinsic(\n         let indices: Option<Vec<_>> = (0..n)\n             .map(|i| {\n                 let arg_idx = i;\n-                let val = bx.cx().const_get_elt(vector, i as u64);\n-                match bx.cx().const_to_opt_u128(val, true) {\n+                let val = bx.const_get_elt(vector, i as u64);\n+                match bx.const_to_opt_u128(val, true) {\n                     None => {\n                         emit_error!(\"shuffle index #{} is not a constant\", arg_idx);\n                         None\n@@ -1189,18 +1189,18 @@ fn generic_simd_intrinsic(\n                                     arg_idx, total_len);\n                         None\n                     }\n-                    Some(idx) => Some(bx.cx().const_i32(idx as i32)),\n+                    Some(idx) => Some(bx.const_i32(idx as i32)),\n                 }\n             })\n             .collect();\n         let indices = match indices {\n             Some(i) => i,\n-            None => return Ok(bx.cx().const_null(llret_ty))\n+            None => return Ok(bx.const_null(llret_ty))\n         };\n \n         return Ok(bx.shuffle_vector(args[0].immediate(),\n                                     args[1].immediate(),\n-                                    bx.cx().const_vector(&indices)))\n+                                    bx.const_vector(&indices)))\n     }\n \n     if name == \"simd_insert\" {\n@@ -1231,8 +1231,8 @@ fn generic_simd_intrinsic(\n             _ => return_error!(\"mask element type is `{}`, expected `i_`\", m_elem_ty)\n         }\n         // truncate the mask to a vector of i1s\n-        let i1 = bx.cx().type_i1();\n-        let i1xn = bx.cx().type_vector(i1, m_len as u64);\n+        let i1 = bx.type_i1();\n+        let i1xn = bx.type_vector(i1, m_len as u64);\n         let m_i1s = bx.trunc(args[0].immediate(), i1xn);\n         return Ok(bx.select(m_i1s, args[1].immediate(), args[2].immediate()));\n     }\n@@ -1252,7 +1252,7 @@ fn generic_simd_intrinsic(\n             };\n             ($msg: tt, $($fmt: tt)*) => {\n                 span_invalid_monomorphization_error(\n-                    bx.cx().sess(), span,\n+                    bx.sess(), span,\n                     &format!(concat!(\"invalid monomorphization of `{}` intrinsic: \", $msg),\n                              name, $($fmt)*));\n             }\n@@ -1293,7 +1293,7 @@ fn generic_simd_intrinsic(\n         };\n \n         let llvm_name = &format!(\"llvm.{0}.v{1}{2}\", name, in_len, ety);\n-        let intrinsic = bx.cx().get_intrinsic(&llvm_name);\n+        let intrinsic = bx.get_intrinsic(&llvm_name);\n         let c = bx.call(intrinsic,\n                         &args.iter().map(|arg| arg.immediate()).collect::<Vec<_>>(),\n                         None);\n@@ -1450,28 +1450,28 @@ fn generic_simd_intrinsic(\n         }\n \n         // Alignment of T, must be a constant integer value:\n-        let alignment_ty = bx.cx().type_i32();\n-        let alignment = bx.cx().const_i32(bx.cx().align_of(in_elem).bytes() as i32);\n+        let alignment_ty = bx.type_i32();\n+        let alignment = bx.const_i32(bx.align_of(in_elem).bytes() as i32);\n \n         // Truncate the mask vector to a vector of i1s:\n         let (mask, mask_ty) = {\n-            let i1 = bx.cx().type_i1();\n-            let i1xn = bx.cx().type_vector(i1, in_len as u64);\n+            let i1 = bx.type_i1();\n+            let i1xn = bx.type_vector(i1, in_len as u64);\n             (bx.trunc(args[2].immediate(), i1xn), i1xn)\n         };\n \n         // Type of the vector of pointers:\n-        let llvm_pointer_vec_ty = llvm_vector_ty(bx.cx(), underlying_ty, in_len, pointer_count);\n+        let llvm_pointer_vec_ty = llvm_vector_ty(bx, underlying_ty, in_len, pointer_count);\n         let llvm_pointer_vec_str = llvm_vector_str(underlying_ty, in_len, pointer_count);\n \n         // Type of the vector of elements:\n-        let llvm_elem_vec_ty = llvm_vector_ty(bx.cx(), underlying_ty, in_len, pointer_count - 1);\n+        let llvm_elem_vec_ty = llvm_vector_ty(bx, underlying_ty, in_len, pointer_count - 1);\n         let llvm_elem_vec_str = llvm_vector_str(underlying_ty, in_len, pointer_count - 1);\n \n         let llvm_intrinsic = format!(\"llvm.masked.gather.{}.{}\",\n                                      llvm_elem_vec_str, llvm_pointer_vec_str);\n-        let f = bx.cx().declare_cfn(&llvm_intrinsic,\n-                                     bx.cx().type_func(&[\n+        let f = bx.declare_cfn(&llvm_intrinsic,\n+                                     bx.type_func(&[\n                                          llvm_pointer_vec_ty,\n                                          alignment_ty,\n                                          mask_ty,\n@@ -1550,30 +1550,30 @@ fn generic_simd_intrinsic(\n         }\n \n         // Alignment of T, must be a constant integer value:\n-        let alignment_ty = bx.cx().type_i32();\n-        let alignment = bx.cx().const_i32(bx.cx().align_of(in_elem).bytes() as i32);\n+        let alignment_ty = bx.type_i32();\n+        let alignment = bx.const_i32(bx.align_of(in_elem).bytes() as i32);\n \n         // Truncate the mask vector to a vector of i1s:\n         let (mask, mask_ty) = {\n-            let i1 = bx.cx().type_i1();\n-            let i1xn = bx.cx().type_vector(i1, in_len as u64);\n+            let i1 = bx.type_i1();\n+            let i1xn = bx.type_vector(i1, in_len as u64);\n             (bx.trunc(args[2].immediate(), i1xn), i1xn)\n         };\n \n-        let ret_t = bx.cx().type_void();\n+        let ret_t = bx.type_void();\n \n         // Type of the vector of pointers:\n-        let llvm_pointer_vec_ty = llvm_vector_ty(bx.cx(), underlying_ty, in_len, pointer_count);\n+        let llvm_pointer_vec_ty = llvm_vector_ty(bx, underlying_ty, in_len, pointer_count);\n         let llvm_pointer_vec_str = llvm_vector_str(underlying_ty, in_len, pointer_count);\n \n         // Type of the vector of elements:\n-        let llvm_elem_vec_ty = llvm_vector_ty(bx.cx(), underlying_ty, in_len, pointer_count - 1);\n+        let llvm_elem_vec_ty = llvm_vector_ty(bx, underlying_ty, in_len, pointer_count - 1);\n         let llvm_elem_vec_str = llvm_vector_str(underlying_ty, in_len, pointer_count - 1);\n \n         let llvm_intrinsic = format!(\"llvm.masked.scatter.{}.{}\",\n                                      llvm_elem_vec_str, llvm_pointer_vec_str);\n-        let f = bx.cx().declare_cfn(&llvm_intrinsic,\n-                                     bx.cx().type_func(&[llvm_elem_vec_ty,\n+        let f = bx.declare_cfn(&llvm_intrinsic,\n+                                     bx.type_func(&[llvm_elem_vec_ty,\n                                                   llvm_pointer_vec_ty,\n                                                   alignment_ty,\n                                                   mask_ty], ret_t));\n@@ -1613,7 +1613,7 @@ fn generic_simd_intrinsic(\n                             //   code is generated\n                             // * if the accumulator of the fmul isn't 1, incorrect\n                             //   code is generated\n-                            match bx.cx().const_get_real(acc) {\n+                            match bx.const_get_real(acc) {\n                                 None => return_error!(\"accumulator of {} is not a constant\", $name),\n                                 Some((v, loses_info)) => {\n                                     if $name.contains(\"mul\") && v != 1.0_f64 {\n@@ -1629,8 +1629,8 @@ fn generic_simd_intrinsic(\n                         } else {\n                             // unordered arithmetic reductions do not:\n                             match f.bit_width() {\n-                                32 => bx.cx().const_undef(bx.cx().type_f32()),\n-                                64 => bx.cx().const_undef(bx.cx().type_f64()),\n+                                32 => bx.const_undef(bx.type_f32()),\n+                                64 => bx.const_undef(bx.type_f64()),\n                                 v => {\n                                     return_error!(r#\"\n unsupported {} from `{}` with element `{}` of size `{}` to `{}`\"#,\n@@ -1707,8 +1707,8 @@ unsupported {} from `{}` with element `{}` of size `{}` to `{}`\"#,\n                     }\n \n                     // boolean reductions operate on vectors of i1s:\n-                    let i1 = bx.cx().type_i1();\n-                    let i1xn = bx.cx().type_vector(i1, in_len as u64);\n+                    let i1 = bx.type_i1();\n+                    let i1xn = bx.type_vector(i1, in_len as u64);\n                     bx.trunc(args[0].immediate(), i1xn)\n                 };\n                 return match in_elem.sty {\n@@ -1718,7 +1718,7 @@ unsupported {} from `{}` with element `{}` of size `{}` to `{}`\"#,\n                             if !$boolean {\n                                 r\n                             } else {\n-                                bx.zext(r, bx.cx().type_bool())\n+                                bx.zext(r, bx.type_bool())\n                             }\n                         )\n                     },"}, {"sha": "8c53129abc315ebd601030ef0f2227dce2ca7937", "filename": "src/librustc_codegen_ssa/common.rs", "status": "modified", "additions": 8, "deletions": 8, "changes": 16, "blob_url": "https://github.com/rust-lang/rust/blob/ceb29e2ac45474a560b04ce4061d8a6cc50e1a33/src%2Flibrustc_codegen_ssa%2Fcommon.rs", "raw_url": "https://github.com/rust-lang/rust/raw/ceb29e2ac45474a560b04ce4061d8a6cc50e1a33/src%2Flibrustc_codegen_ssa%2Fcommon.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_codegen_ssa%2Fcommon.rs?ref=ceb29e2ac45474a560b04ce4061d8a6cc50e1a33", "patch": "@@ -194,7 +194,7 @@ fn shift_mask_rhs<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>>(\n     bx: &mut Bx,\n     rhs: Bx::Value\n ) -> Bx::Value {\n-    let rhs_llty = bx.cx().val_ty(rhs);\n+    let rhs_llty = bx.val_ty(rhs);\n     let shift_val = shift_mask_val(bx, rhs_llty, rhs_llty, false);\n     bx.and(rhs, shift_val)\n }\n@@ -205,25 +205,25 @@ pub fn shift_mask_val<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>>(\n     mask_llty: Bx::Type,\n     invert: bool\n ) -> Bx::Value {\n-    let kind = bx.cx().type_kind(llty);\n+    let kind = bx.type_kind(llty);\n     match kind {\n         TypeKind::Integer => {\n             // i8/u8 can shift by at most 7, i16/u16 by at most 15, etc.\n-            let val = bx.cx().int_width(llty) - 1;\n+            let val = bx.int_width(llty) - 1;\n             if invert {\n-                bx.cx().const_int(mask_llty, !val as i64)\n+                bx.const_int(mask_llty, !val as i64)\n             } else {\n-                bx.cx().const_uint(mask_llty, val)\n+                bx.const_uint(mask_llty, val)\n             }\n         },\n         TypeKind::Vector => {\n             let mask = shift_mask_val(\n                 bx,\n-                bx.cx().element_type(llty),\n-                bx.cx().element_type(mask_llty),\n+                bx.element_type(llty),\n+                bx.element_type(mask_llty),\n                 invert\n             );\n-            bx.vector_splat(bx.cx().vector_length(mask_llty), mask)\n+            bx.vector_splat(bx.vector_length(mask_llty), mask)\n         },\n         _ => bug!(\"shift_mask_val: expected Integer or Vector, found {:?}\", kind),\n     }"}, {"sha": "b3257dbc36b90f8124965f18d3c096141cd1dcfe", "filename": "src/librustc_codegen_ssa/glue.rs", "status": "modified", "additions": 13, "deletions": 14, "changes": 27, "blob_url": "https://github.com/rust-lang/rust/blob/ceb29e2ac45474a560b04ce4061d8a6cc50e1a33/src%2Flibrustc_codegen_ssa%2Fglue.rs", "raw_url": "https://github.com/rust-lang/rust/raw/ceb29e2ac45474a560b04ce4061d8a6cc50e1a33/src%2Flibrustc_codegen_ssa%2Fglue.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_codegen_ssa%2Fglue.rs?ref=ceb29e2ac45474a560b04ce4061d8a6cc50e1a33", "patch": "@@ -16,7 +16,6 @@ use std;\n \n use common::IntPredicate;\n use meth;\n-use rustc::ty::layout::LayoutOf;\n use rustc::ty::{self, Ty};\n use traits::*;\n \n@@ -25,12 +24,12 @@ pub fn size_and_align_of_dst<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>>(\n     t: Ty<'tcx>,\n     info: Option<Bx::Value>\n ) -> (Bx::Value, Bx::Value) {\n-    let layout = bx.cx().layout_of(t);\n+    let layout = bx.layout_of(t);\n     debug!(\"size_and_align_of_dst(ty={}, info={:?}): layout: {:?}\",\n            t, info, layout);\n     if !layout.is_unsized() {\n-        let size = bx.cx().const_usize(layout.size.bytes());\n-        let align = bx.cx().const_usize(layout.align.abi.bytes());\n+        let size = bx.const_usize(layout.size.bytes());\n+        let align = bx.const_usize(layout.align.abi.bytes());\n         return (size, align);\n     }\n     match t.sty {\n@@ -40,11 +39,11 @@ pub fn size_and_align_of_dst<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>>(\n             (meth::SIZE.get_usize(bx, vtable), meth::ALIGN.get_usize(bx, vtable))\n         }\n         ty::Slice(_) | ty::Str => {\n-            let unit = layout.field(bx.cx(), 0);\n+            let unit = layout.field(bx, 0);\n             // The info in this case is the length of the str, so the size is that\n             // times the unit size.\n-            (bx.mul(info.unwrap(), bx.cx().const_usize(unit.size.bytes())),\n-             bx.cx().const_usize(unit.align.abi.bytes()))\n+            (bx.mul(info.unwrap(), bx.const_usize(unit.size.bytes())),\n+             bx.const_usize(unit.align.abi.bytes()))\n         }\n         _ => {\n             // First get the size of all statically known fields.\n@@ -58,12 +57,12 @@ pub fn size_and_align_of_dst<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>>(\n             let sized_align = layout.align.abi.bytes();\n             debug!(\"DST {} statically sized prefix size: {} align: {}\",\n                    t, sized_size, sized_align);\n-            let sized_size = bx.cx().const_usize(sized_size);\n-            let sized_align = bx.cx().const_usize(sized_align);\n+            let sized_size = bx.const_usize(sized_size);\n+            let sized_align = bx.const_usize(sized_align);\n \n             // Recurse to get the size of the dynamically sized field (must be\n             // the last field).\n-            let field_ty = layout.field(bx.cx(), i).ty;\n+            let field_ty = layout.field(bx, i).ty;\n             let (unsized_size, mut unsized_align) = size_and_align_of_dst(bx, field_ty, info);\n \n             // FIXME (#26403, #27023): We should be adding padding\n@@ -85,12 +84,12 @@ pub fn size_and_align_of_dst<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>>(\n \n             // Choose max of two known alignments (combined value must\n             // be aligned according to more restrictive of the two).\n-            let align = match (bx.cx().const_to_opt_u128(sized_align, false),\n-                               bx.cx().const_to_opt_u128(unsized_align, false)) {\n+            let align = match (bx.const_to_opt_u128(sized_align, false),\n+                               bx.const_to_opt_u128(unsized_align, false)) {\n                 (Some(sized_align), Some(unsized_align)) => {\n                     // If both alignments are constant, (the sized_align should always be), then\n                     // pick the correct alignment statically.\n-                    bx.cx().const_usize(std::cmp::max(sized_align, unsized_align) as u64)\n+                    bx.const_usize(std::cmp::max(sized_align, unsized_align) as u64)\n                 }\n                 _ => {\n                     let cmp = bx.icmp(IntPredicate::IntUGT, sized_align, unsized_align);\n@@ -108,7 +107,7 @@ pub fn size_and_align_of_dst<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>>(\n             // emulated via the semi-standard fast bit trick:\n             //\n             //   `(size + (align-1)) & -align`\n-            let one = bx.cx().const_usize(1);\n+            let one = bx.const_usize(1);\n             let addend = bx.sub(align, one);\n             let add = bx.add(size, addend);\n             let neg =  bx.neg(align);"}, {"sha": "3880935f0f426fbe5aa7a0cd171dd879d931639d", "filename": "src/librustc_codegen_ssa/meth.rs", "status": "modified", "additions": 4, "deletions": 4, "changes": 8, "blob_url": "https://github.com/rust-lang/rust/blob/ceb29e2ac45474a560b04ce4061d8a6cc50e1a33/src%2Flibrustc_codegen_ssa%2Fmeth.rs", "raw_url": "https://github.com/rust-lang/rust/raw/ceb29e2ac45474a560b04ce4061d8a6cc50e1a33/src%2Flibrustc_codegen_ssa%2Fmeth.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_codegen_ssa%2Fmeth.rs?ref=ceb29e2ac45474a560b04ce4061d8a6cc50e1a33", "patch": "@@ -39,10 +39,10 @@ impl<'a, 'tcx: 'a> VirtualIndex {\n \n         let llvtable = bx.pointercast(\n             llvtable,\n-            bx.cx().type_ptr_to(bx.cx().fn_ptr_backend_type(fn_ty))\n+            bx.type_ptr_to(bx.fn_ptr_backend_type(fn_ty))\n         );\n         let ptr_align = bx.tcx().data_layout.pointer_align.abi;\n-        let gep = bx.inbounds_gep(llvtable, &[bx.cx().const_usize(self.0)]);\n+        let gep = bx.inbounds_gep(llvtable, &[bx.const_usize(self.0)]);\n         let ptr = bx.load(gep, ptr_align);\n         bx.nonnull_metadata(ptr);\n         // Vtable loads are invariant\n@@ -58,9 +58,9 @@ impl<'a, 'tcx: 'a> VirtualIndex {\n         // Load the data pointer from the object.\n         debug!(\"get_int({:?}, {:?})\", llvtable, self);\n \n-        let llvtable = bx.pointercast(llvtable, bx.cx().type_ptr_to(bx.cx().type_isize()));\n+        let llvtable = bx.pointercast(llvtable, bx.type_ptr_to(bx.type_isize()));\n         let usize_align = bx.tcx().data_layout.pointer_align.abi;\n-        let gep = bx.inbounds_gep(llvtable, &[bx.cx().const_usize(self.0)]);\n+        let gep = bx.inbounds_gep(llvtable, &[bx.const_usize(self.0)]);\n         let ptr = bx.load(gep, usize_align);\n         // Vtable loads are invariant\n         bx.set_invariant_load(ptr);"}, {"sha": "a3bfbc2211ce3559f1d7955011bc1baa71a4ca8e", "filename": "src/librustc_codegen_ssa/mir/block.rs", "status": "modified", "additions": 52, "deletions": 52, "changes": 104, "blob_url": "https://github.com/rust-lang/rust/blob/ceb29e2ac45474a560b04ce4061d8a6cc50e1a33/src%2Flibrustc_codegen_ssa%2Fmir%2Fblock.rs", "raw_url": "https://github.com/rust-lang/rust/raw/ceb29e2ac45474a560b04ce4061d8a6cc50e1a33/src%2Flibrustc_codegen_ssa%2Fmir%2Fblock.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_codegen_ssa%2Fmir%2Fblock.rs?ref=ceb29e2ac45474a560b04ce4061d8a6cc50e1a33", "patch": "@@ -182,13 +182,13 @@ impl<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>> FunctionCx<'a, 'tcx, Bx> {\n                     let lp1 = bx.load_operand(lp1).immediate();\n                     slot.storage_dead(&mut bx);\n \n-                    if !bx.cx().sess().target.target.options.custom_unwind_resume {\n-                        let mut lp = bx.cx().const_undef(self.landing_pad_type());\n+                    if !bx.sess().target.target.options.custom_unwind_resume {\n+                        let mut lp = bx.const_undef(self.landing_pad_type());\n                         lp = bx.insert_value(lp, lp0, 0);\n                         lp = bx.insert_value(lp, lp1, 1);\n                         bx.resume(lp);\n                     } else {\n-                        bx.call(bx.cx().eh_unwind_resume(), &[lp0], funclet(self));\n+                        bx.call(bx.eh_unwind_resume(), &[lp0], funclet(self));\n                         bx.unreachable();\n                     }\n                 }\n@@ -218,10 +218,10 @@ impl<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>> FunctionCx<'a, 'tcx, Bx> {\n                             bx.cond_br(discr.immediate(), lltrue, llfalse);\n                         }\n                     } else {\n-                        let switch_llty = bx.cx().immediate_backend_type(\n-                            bx.cx().layout_of(switch_ty)\n+                        let switch_llty = bx.immediate_backend_type(\n+                            bx.layout_of(switch_ty)\n                         );\n-                        let llval = bx.cx().const_uint_big(switch_llty, values[0]);\n+                        let llval = bx.const_uint_big(switch_llty, values[0]);\n                         let cmp = bx.icmp(IntPredicate::IntEQ, discr.immediate(), llval);\n                         bx.cond_br(cmp, lltrue, llfalse);\n                     }\n@@ -230,11 +230,11 @@ impl<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>> FunctionCx<'a, 'tcx, Bx> {\n                     let switch = bx.switch(discr.immediate(),\n                                            llblock(self, *otherwise),\n                                            values.len());\n-                    let switch_llty = bx.cx().immediate_backend_type(\n-                        bx.cx().layout_of(switch_ty)\n+                    let switch_llty = bx.immediate_backend_type(\n+                        bx.layout_of(switch_ty)\n                     );\n                     for (&value, target) in values.iter().zip(targets) {\n-                        let llval = bx.cx().const_uint_big(switch_llty, value);\n+                        let llval = bx.const_uint_big(switch_llty, value);\n                         let llbb = llblock(self, *target);\n                         bx.add_case(switch, llval, llbb)\n                     }\n@@ -283,8 +283,8 @@ impl<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>> FunctionCx<'a, 'tcx, Bx> {\n                                 llval\n                             }\n                         };\n-                        let addr = bx.pointercast(llslot, bx.cx().type_ptr_to(\n-                            bx.cx().cast_backend_type(&cast_ty)\n+                        let addr = bx.pointercast(llslot, bx.type_ptr_to(\n+                            bx.cast_backend_type(&cast_ty)\n                         ));\n                         bx.load(addr, self.fn_ty.ret.layout.align.abi)\n                     }\n@@ -299,7 +299,7 @@ impl<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>> FunctionCx<'a, 'tcx, Bx> {\n             mir::TerminatorKind::Drop { ref location, target, unwind } => {\n                 let ty = location.ty(self.mir, bx.tcx()).to_ty(bx.tcx());\n                 let ty = self.monomorphize(&ty);\n-                let drop_fn = monomorphize::resolve_drop_in_place(bx.cx().tcx(), ty);\n+                let drop_fn = monomorphize::resolve_drop_in_place(bx.tcx(), ty);\n \n                 if let ty::InstanceDef::DropGlue(_, None) = drop_fn.def {\n                     // we don't actually need to drop anything.\n@@ -323,14 +323,14 @@ impl<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>> FunctionCx<'a, 'tcx, Bx> {\n                             ty::ParamEnv::reveal_all(),\n                             &sig,\n                         );\n-                        let fn_ty = bx.cx().new_vtable(sig, &[]);\n+                        let fn_ty = bx.new_vtable(sig, &[]);\n                         let vtable = args[1];\n                         args = &args[..1];\n                         (meth::DESTRUCTOR.get_fn(&mut bx, vtable, &fn_ty), fn_ty)\n                     }\n                     _ => {\n-                        (bx.cx().get_fn(drop_fn),\n-                         bx.cx().fn_type_of_instance(&drop_fn))\n+                        (bx.get_fn(drop_fn),\n+                         bx.fn_type_of_instance(&drop_fn))\n                     }\n                 };\n                 do_call(self, &mut bx, fn_ty, drop_fn, args,\n@@ -340,7 +340,7 @@ impl<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>> FunctionCx<'a, 'tcx, Bx> {\n \n             mir::TerminatorKind::Assert { ref cond, expected, ref msg, target, cleanup } => {\n                 let cond = self.codegen_operand(&mut bx, cond).immediate();\n-                let mut const_cond = bx.cx().const_to_opt_u128(cond, false).map(|c| c == 1);\n+                let mut const_cond = bx.const_to_opt_u128(cond, false).map(|c| c == 1);\n \n                 // This case can currently arise only from functions marked\n                 // with #[rustc_inherit_overflow_checks] and inlined from\n@@ -349,7 +349,7 @@ impl<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>> FunctionCx<'a, 'tcx, Bx> {\n                 // NOTE: Unlike binops, negation doesn't have its own\n                 // checked operation, just a comparison with the minimum\n                 // value, so we have to check for the assert message.\n-                if !bx.cx().check_overflow() {\n+                if !bx.check_overflow() {\n                     if let mir::interpret::EvalErrorKind::OverflowNeg = *msg {\n                         const_cond = Some(expected);\n                     }\n@@ -378,11 +378,11 @@ impl<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>> FunctionCx<'a, 'tcx, Bx> {\n                 self.set_debug_loc(&mut bx, terminator.source_info);\n \n                 // Get the location information.\n-                let loc = bx.cx().sess().source_map().lookup_char_pos(span.lo());\n+                let loc = bx.sess().source_map().lookup_char_pos(span.lo());\n                 let filename = Symbol::intern(&loc.file.name.to_string()).as_str();\n-                let filename = bx.cx().const_str_slice(filename);\n-                let line = bx.cx().const_u32(loc.line as u32);\n-                let col = bx.cx().const_u32(loc.col.to_usize() as u32 + 1);\n+                let filename = bx.const_str_slice(filename);\n+                let line = bx.const_u32(loc.line as u32);\n+                let col = bx.const_u32(loc.col.to_usize() as u32 + 1);\n                 let align = tcx.data_layout.aggregate_align.abi\n                     .max(tcx.data_layout.i32_align.abi)\n                     .max(tcx.data_layout.pointer_align.abi);\n@@ -393,8 +393,8 @@ impl<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>> FunctionCx<'a, 'tcx, Bx> {\n                         let len = self.codegen_operand(&mut bx, len).immediate();\n                         let index = self.codegen_operand(&mut bx, index).immediate();\n \n-                        let file_line_col = bx.cx().const_struct(&[filename, line, col], false);\n-                        let file_line_col = bx.cx().static_addr_of(\n+                        let file_line_col = bx.const_struct(&[filename, line, col], false);\n+                        let file_line_col = bx.static_addr_of(\n                             file_line_col,\n                             align,\n                             Some(\"panic_bounds_check_loc\")\n@@ -405,12 +405,12 @@ impl<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>> FunctionCx<'a, 'tcx, Bx> {\n                     _ => {\n                         let str = msg.description();\n                         let msg_str = Symbol::intern(str).as_str();\n-                        let msg_str = bx.cx().const_str_slice(msg_str);\n-                        let msg_file_line_col = bx.cx().const_struct(\n+                        let msg_str = bx.const_str_slice(msg_str);\n+                        let msg_file_line_col = bx.const_struct(\n                             &[msg_str, filename, line, col],\n                             false\n                         );\n-                        let msg_file_line_col = bx.cx().static_addr_of(\n+                        let msg_file_line_col = bx.static_addr_of(\n                             msg_file_line_col,\n                             align,\n                             Some(\"panic_loc\")\n@@ -423,8 +423,8 @@ impl<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>> FunctionCx<'a, 'tcx, Bx> {\n                 // Obtain the panic entry point.\n                 let def_id = common::langcall(bx.tcx(), Some(span), \"\", lang_item);\n                 let instance = ty::Instance::mono(bx.tcx(), def_id);\n-                let fn_ty = bx.cx().fn_type_of_instance(&instance);\n-                let llfn = bx.cx().get_fn(instance);\n+                let fn_ty = bx.fn_type_of_instance(&instance);\n+                let llfn = bx.get_fn(instance);\n \n                 // Codegen the actual panic invoke/call.\n                 do_call(self, &mut bx, fn_ty, llfn, &args, None, cleanup);\n@@ -446,7 +446,7 @@ impl<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>> FunctionCx<'a, 'tcx, Bx> {\n \n                 let (instance, mut llfn) = match callee.layout.ty.sty {\n                     ty::FnDef(def_id, substs) => {\n-                        (Some(ty::Instance::resolve(bx.cx().tcx(),\n+                        (Some(ty::Instance::resolve(bx.tcx(),\n                                                     ty::ParamEnv::reveal_all(),\n                                                     def_id,\n                                                     substs).unwrap()),\n@@ -485,7 +485,7 @@ impl<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>> FunctionCx<'a, 'tcx, Bx> {\n                         // we can do what we like. Here, we declare that transmuting\n                         // into an uninhabited type is impossible, so anything following\n                         // it must be unreachable.\n-                        assert_eq!(bx.cx().layout_of(sig.output()).abi, layout::Abi::Uninhabited);\n+                        assert_eq!(bx.layout_of(sig.output()).abi, layout::Abi::Uninhabited);\n                         bx.unreachable();\n                     }\n                     return;\n@@ -499,26 +499,26 @@ impl<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>> FunctionCx<'a, 'tcx, Bx> {\n \n                 let fn_ty = match def {\n                     Some(ty::InstanceDef::Virtual(..)) => {\n-                        bx.cx().new_vtable(sig, &extra_args)\n+                        bx.new_vtable(sig, &extra_args)\n                     }\n                     Some(ty::InstanceDef::DropGlue(_, None)) => {\n                         // empty drop glue - a nop.\n                         let &(_, target) = destination.as_ref().unwrap();\n                         funclet_br(self, &mut bx, target);\n                         return;\n                     }\n-                    _ => bx.cx().new_fn_type(sig, &extra_args)\n+                    _ => bx.new_fn_type(sig, &extra_args)\n                 };\n \n                 // emit a panic instead of instantiating an uninhabited type\n                 if (intrinsic == Some(\"init\") || intrinsic == Some(\"uninit\")) &&\n                     fn_ty.ret.layout.abi.is_uninhabited()\n                 {\n-                    let loc = bx.cx().sess().source_map().lookup_char_pos(span.lo());\n+                    let loc = bx.sess().source_map().lookup_char_pos(span.lo());\n                     let filename = Symbol::intern(&loc.file.name.to_string()).as_str();\n-                    let filename = bx.cx().const_str_slice(filename);\n-                    let line = bx.cx().const_u32(loc.line as u32);\n-                    let col = bx.cx().const_u32(loc.col.to_usize() as u32 + 1);\n+                    let filename = bx.const_str_slice(filename);\n+                    let line = bx.const_u32(loc.line as u32);\n+                    let col = bx.const_u32(loc.col.to_usize() as u32 + 1);\n                     let align = tcx.data_layout.aggregate_align.abi\n                         .max(tcx.data_layout.i32_align.abi)\n                         .max(tcx.data_layout.pointer_align.abi);\n@@ -529,12 +529,12 @@ impl<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>> FunctionCx<'a, 'tcx, Bx> {\n                         if intrinsic == Some(\"init\") { \"zeroed\" } else { \"uninitialized\" }\n                     );\n                     let msg_str = Symbol::intern(&str).as_str();\n-                    let msg_str = bx.cx().const_str_slice(msg_str);\n-                    let msg_file_line_col = bx.cx().const_struct(\n+                    let msg_str = bx.const_str_slice(msg_str);\n+                    let msg_file_line_col = bx.const_struct(\n                         &[msg_str, filename, line, col],\n                         false,\n                     );\n-                    let msg_file_line_col = bx.cx().static_addr_of(\n+                    let msg_file_line_col = bx.static_addr_of(\n                         msg_file_line_col,\n                         align,\n                         Some(\"panic_loc\"),\n@@ -544,8 +544,8 @@ impl<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>> FunctionCx<'a, 'tcx, Bx> {\n                     let def_id =\n                         common::langcall(bx.tcx(), Some(span), \"\", lang_items::PanicFnLangItem);\n                     let instance = ty::Instance::mono(bx.tcx(), def_id);\n-                    let fn_ty = bx.cx().fn_type_of_instance(&instance);\n-                    let llfn = bx.cx().get_fn(instance);\n+                    let fn_ty = bx.fn_type_of_instance(&instance);\n+                    let llfn = bx.get_fn(instance);\n \n                     // Codegen the actual panic invoke/call.\n                     do_call(\n@@ -577,7 +577,7 @@ impl<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>> FunctionCx<'a, 'tcx, Bx> {\n                     let dest = match ret_dest {\n                         _ if fn_ty.ret.is_indirect() => llargs[0],\n                         ReturnDest::Nothing => {\n-                            bx.cx().const_undef(bx.cx().type_ptr_to(bx.memory_ty(&fn_ty.ret)))\n+                            bx.const_undef(bx.type_ptr_to(bx.memory_ty(&fn_ty.ret)))\n                         }\n                         ReturnDest::IndirectOperand(dst, _) |\n                         ReturnDest::Store(dst) => dst.llval,\n@@ -611,7 +611,7 @@ impl<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>> FunctionCx<'a, 'tcx, Bx> {\n                                     );\n                                     return OperandRef {\n                                         val: Immediate(llval),\n-                                        layout: bx.cx().layout_of(ty),\n+                                        layout: bx.layout_of(ty),\n                                     };\n \n                                 },\n@@ -629,7 +629,7 @@ impl<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>> FunctionCx<'a, 'tcx, Bx> {\n                                     );\n                                     return OperandRef {\n                                         val: Immediate(llval),\n-                                        layout: bx.cx().layout_of(ty)\n+                                        layout: bx.layout_of(ty)\n                                     };\n                                 }\n                             }\n@@ -639,7 +639,7 @@ impl<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>> FunctionCx<'a, 'tcx, Bx> {\n                     }).collect();\n \n \n-                    let callee_ty = instance.as_ref().unwrap().ty(bx.cx().tcx());\n+                    let callee_ty = instance.as_ref().unwrap().ty(bx.tcx());\n                     bx.codegen_intrinsic_call(callee_ty, &fn_ty, &args, dest,\n                                                terminator.source_info.span);\n \n@@ -736,7 +736,7 @@ impl<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>> FunctionCx<'a, 'tcx, Bx> {\n \n                 let fn_ptr = match (llfn, instance) {\n                     (Some(llfn), _) => llfn,\n-                    (None, Some(instance)) => bx.cx().get_fn(instance),\n+                    (None, Some(instance)) => bx.get_fn(instance),\n                     _ => span_bug!(span, \"no llfn for call\"),\n                 };\n \n@@ -760,7 +760,7 @@ impl<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>> FunctionCx<'a, 'tcx, Bx> {\n     ) {\n         // Fill padding with undef value, where applicable.\n         if let Some(ty) = arg.pad {\n-            llargs.push(bx.cx().const_undef(bx.cx().reg_backend_type(&ty)))\n+            llargs.push(bx.const_undef(bx.reg_backend_type(&ty)))\n         }\n \n         if arg.is_ignore() {\n@@ -820,8 +820,8 @@ impl<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>> FunctionCx<'a, 'tcx, Bx> {\n         if by_ref && !arg.is_indirect() {\n             // Have to load the argument, maybe while casting it.\n             if let PassMode::Cast(ty) = arg.mode {\n-                let addr = bx.pointercast(llval, bx.cx().type_ptr_to(\n-                    bx.cx().cast_backend_type(&ty))\n+                let addr = bx.pointercast(llval, bx.type_ptr_to(\n+                    bx.cast_backend_type(&ty))\n                 );\n                 llval = bx.load(addr, align.min(arg.layout.align.abi));\n             } else {\n@@ -1030,7 +1030,7 @@ impl<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>> FunctionCx<'a, 'tcx, Bx> {\n                 LocalRef::Place(place) => self.codegen_transmute_into(bx, src, place),\n                 LocalRef::UnsizedPlace(_) => bug!(\"transmute must not involve unsized locals\"),\n                 LocalRef::Operand(None) => {\n-                    let dst_layout = bx.cx().layout_of(self.monomorphized_place_ty(dst));\n+                    let dst_layout = bx.layout_of(self.monomorphized_place_ty(dst));\n                     assert!(!dst_layout.ty.has_erasable_regions());\n                     let place = PlaceRef::alloca(bx, dst_layout, \"transmute_temp\");\n                     place.storage_live(bx);\n@@ -1057,8 +1057,8 @@ impl<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>> FunctionCx<'a, 'tcx, Bx> {\n         dst: PlaceRef<'tcx, Bx::Value>\n     ) {\n         let src = self.codegen_operand(bx, src);\n-        let llty = bx.cx().backend_type(src.layout);\n-        let cast_ptr = bx.pointercast(dst.llval, bx.cx().type_ptr_to(llty));\n+        let llty = bx.backend_type(src.layout);\n+        let cast_ptr = bx.pointercast(dst.llval, bx.type_ptr_to(llty));\n         let align = src.layout.align.abi.min(dst.align);\n         src.val.store(bx, PlaceRef::new_sized(cast_ptr, src.layout, align));\n     }"}, {"sha": "c03fff78063304300cc001128e11bfed7c25a7b2", "filename": "src/librustc_codegen_ssa/mir/constant.rs", "status": "modified", "additions": 7, "deletions": 7, "changes": 14, "blob_url": "https://github.com/rust-lang/rust/blob/ceb29e2ac45474a560b04ce4061d8a6cc50e1a33/src%2Flibrustc_codegen_ssa%2Fmir%2Fconstant.rs", "raw_url": "https://github.com/rust-lang/rust/raw/ceb29e2ac45474a560b04ce4061d8a6cc50e1a33/src%2Flibrustc_codegen_ssa%2Fmir%2Fconstant.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_codegen_ssa%2Fmir%2Fconstant.rs?ref=ceb29e2ac45474a560b04ce4061d8a6cc50e1a33", "patch": "@@ -14,7 +14,7 @@ use rustc::mir;\n use rustc_data_structures::indexed_vec::Idx;\n use rustc::mir::interpret::{GlobalId, ConstValue};\n use rustc::ty::{self, Ty};\n-use rustc::ty::layout::{self, LayoutOf};\n+use rustc::ty::layout;\n use syntax::source_map::Span;\n use traits::*;\n \n@@ -75,20 +75,20 @@ impl<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>> FunctionCx<'a, 'tcx, Bx> {\n                         c,\n                     )?;\n                     if let Some(prim) = field.val.try_to_scalar() {\n-                        let layout = bx.cx().layout_of(field_ty);\n+                        let layout = bx.layout_of(field_ty);\n                         let scalar = match layout.abi {\n                             layout::Abi::Scalar(ref x) => x,\n                             _ => bug!(\"from_const: invalid ByVal layout: {:#?}\", layout)\n                         };\n-                        Ok(bx.cx().scalar_to_backend(\n+                        Ok(bx.scalar_to_backend(\n                             prim, scalar,\n-                            bx.cx().immediate_backend_type(layout),\n+                            bx.immediate_backend_type(layout),\n                         ))\n                     } else {\n                         bug!(\"simd shuffle field {:?}\", field)\n                     }\n                 }).collect();\n-                let llval = bx.cx().const_struct(&values?, false);\n+                let llval = bx.const_struct(&values?, false);\n                 Ok((llval, c.ty))\n             })\n             .unwrap_or_else(|_| {\n@@ -98,8 +98,8 @@ impl<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>> FunctionCx<'a, 'tcx, Bx> {\n                 );\n                 // We've errored, so we don't have to produce working code.\n                 let ty = self.monomorphize(&ty);\n-                let llty = bx.cx().backend_type(bx.cx().layout_of(ty));\n-                (bx.cx().const_undef(llty), ty)\n+                let llty = bx.backend_type(bx.layout_of(ty));\n+                (bx.const_undef(llty), ty)\n             })\n     }\n }"}, {"sha": "a992364959e66734a537a6a2656e992e681790cf", "filename": "src/librustc_codegen_ssa/mir/mod.rs", "status": "modified", "additions": 18, "deletions": 18, "changes": 36, "blob_url": "https://github.com/rust-lang/rust/blob/ceb29e2ac45474a560b04ce4061d8a6cc50e1a33/src%2Flibrustc_codegen_ssa%2Fmir%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/ceb29e2ac45474a560b04ce4061d8a6cc50e1a33/src%2Flibrustc_codegen_ssa%2Fmir%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_codegen_ssa%2Fmir%2Fmod.rs?ref=ceb29e2ac45474a560b04ce4061d8a6cc50e1a33", "patch": "@@ -10,7 +10,7 @@\n \n use libc::c_uint;\n use rustc::ty::{self, Ty, TypeFoldable, UpvarSubsts};\n-use rustc::ty::layout::{LayoutOf, TyLayout, HasTyCtxt};\n+use rustc::ty::layout::{TyLayout, HasTyCtxt};\n use rustc::mir::{self, Mir};\n use rustc::ty::subst::Substs;\n use rustc::session::config::DebugInfo;\n@@ -266,14 +266,14 @@ pub fn codegen_mir<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>>(\n \n         let mut allocate_local = |local| {\n             let decl = &mir.local_decls[local];\n-            let layout = bx.cx().layout_of(fx.monomorphize(&decl.ty));\n+            let layout = bx.layout_of(fx.monomorphize(&decl.ty));\n             assert!(!layout.ty.has_erasable_regions());\n \n             if let Some(name) = decl.name {\n                 // User variable\n                 let debug_scope = fx.scopes[decl.visibility_scope];\n                 let dbg = debug_scope.is_valid() &&\n-                    bx.cx().sess().opts.debuginfo == DebugInfo::Full;\n+                    bx.sess().opts.debuginfo == DebugInfo::Full;\n \n                 if !memory_locals.contains(local) && !dbg {\n                     debug!(\"alloc: {:?} ({}) -> operand\", local, name);\n@@ -376,7 +376,7 @@ fn create_funclets<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>>(\n {\n     block_bxs.iter_enumerated().zip(cleanup_kinds).map(|((bb, &llbb), cleanup_kind)| {\n         match *cleanup_kind {\n-            CleanupKind::Funclet if base::wants_msvc_seh(bx.cx().sess()) => {}\n+            CleanupKind::Funclet if base::wants_msvc_seh(bx.sess()) => {}\n             _ => return (None, None)\n         }\n \n@@ -415,8 +415,8 @@ fn create_funclets<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>>(\n                 // C++ personality function, but `catch (...)` has no type so\n                 // it's null. The 64 here is actually a bitfield which\n                 // represents that this is a catch-all block.\n-                let null = bx.cx().const_null(bx.cx().type_i8p());\n-                let sixty_four = bx.cx().const_i32(64);\n+                let null = bx.const_null(bx.type_i8p());\n+                let sixty_four = bx.const_i32(64);\n                 funclet = cp_bx.catch_pad(cs, &[null, sixty_four, null]);\n                 cp_bx.br(llbb);\n             }\n@@ -451,7 +451,7 @@ fn arg_local_refs<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>>(\n \n     // Get the argument scope, if it exists and if we need it.\n     let arg_scope = scopes[mir::OUTERMOST_SOURCE_SCOPE];\n-    let arg_scope = if bx.cx().sess().opts.debuginfo == DebugInfo::Full {\n+    let arg_scope = if bx.sess().opts.debuginfo == DebugInfo::Full {\n         arg_scope.scope_metadata\n     } else {\n         None\n@@ -478,7 +478,7 @@ fn arg_local_refs<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>>(\n                 _ => bug!(\"spread argument isn't a tuple?!\")\n             };\n \n-            let place = PlaceRef::alloca(bx, bx.cx().layout_of(arg_ty), &name);\n+            let place = PlaceRef::alloca(bx, bx.layout_of(arg_ty), &name);\n             for i in 0..tupled_arg_tys.len() {\n                 let arg = &fx.fn_ty.args[idx];\n                 idx += 1;\n@@ -524,18 +524,18 @@ fn arg_local_refs<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>>(\n                     return local(OperandRef::new_zst(bx.cx(), arg.layout));\n                 }\n                 PassMode::Direct(_) => {\n-                    let llarg = bx.cx().get_param(bx.llfn(), llarg_idx as c_uint);\n+                    let llarg = bx.get_param(bx.llfn(), llarg_idx as c_uint);\n                     bx.set_value_name(llarg, &name);\n                     llarg_idx += 1;\n                     return local(\n                         OperandRef::from_immediate_or_packed_pair(bx, llarg, arg.layout));\n                 }\n                 PassMode::Pair(..) => {\n-                    let a = bx.cx().get_param(bx.llfn(), llarg_idx as c_uint);\n+                    let a = bx.get_param(bx.llfn(), llarg_idx as c_uint);\n                     bx.set_value_name(a, &(name.clone() + \".0\"));\n                     llarg_idx += 1;\n \n-                    let b = bx.cx().get_param(bx.llfn(), llarg_idx as c_uint);\n+                    let b = bx.get_param(bx.llfn(), llarg_idx as c_uint);\n                     bx.set_value_name(b, &(name + \".1\"));\n                     llarg_idx += 1;\n \n@@ -552,16 +552,16 @@ fn arg_local_refs<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>>(\n             // Don't copy an indirect argument to an alloca, the caller\n             // already put it in a temporary alloca and gave it up.\n             // FIXME: lifetimes\n-            let llarg = bx.cx().get_param(bx.llfn(), llarg_idx as c_uint);\n+            let llarg = bx.get_param(bx.llfn(), llarg_idx as c_uint);\n             bx.set_value_name(llarg, &name);\n             llarg_idx += 1;\n             PlaceRef::new_sized(llarg, arg.layout, arg.layout.align.abi)\n         } else if arg.is_unsized_indirect() {\n             // As the storage for the indirect argument lives during\n             // the whole function call, we just copy the fat pointer.\n-            let llarg = bx.cx().get_param(bx.llfn(), llarg_idx as c_uint);\n+            let llarg = bx.get_param(bx.llfn(), llarg_idx as c_uint);\n             llarg_idx += 1;\n-            let llextra = bx.cx().get_param(bx.llfn(), llarg_idx as c_uint);\n+            let llextra = bx.get_param(bx.llfn(), llarg_idx as c_uint);\n             llarg_idx += 1;\n             let indirect_operand = OperandValue::Pair(llarg, llextra);\n \n@@ -599,7 +599,7 @@ fn arg_local_refs<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>>(\n             // Or is it the closure environment?\n             let (closure_layout, env_ref) = match arg.layout.ty.sty {\n                 ty::RawPtr(ty::TypeAndMut { ty, .. }) |\n-                ty::Ref(_, ty, _)  => (bx.cx().layout_of(ty), true),\n+                ty::Ref(_, ty, _)  => (bx.layout_of(ty), true),\n                 _ => (arg.layout, false)\n             };\n \n@@ -618,10 +618,10 @@ fn arg_local_refs<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>>(\n             // doesn't actually strip the offset when splitting the closure\n             // environment into its components so it ends up out of bounds.\n             // (cuviper) It seems to be fine without the alloca on LLVM 6 and later.\n-            let env_alloca = !env_ref && bx.cx().closure_env_needs_indirect_debuginfo();\n+            let env_alloca = !env_ref && bx.closure_env_needs_indirect_debuginfo();\n             let env_ptr = if env_alloca {\n                 let scratch = PlaceRef::alloca(bx,\n-                    bx.cx().layout_of(tcx.mk_mut_ptr(arg.layout.ty)),\n+                    bx.layout_of(tcx.mk_mut_ptr(arg.layout.ty)),\n                     \"__debuginfo_env_ptr\");\n                 bx.store(place.llval, scratch.llval, scratch.align);\n                 scratch.llval\n@@ -632,7 +632,7 @@ fn arg_local_refs<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>>(\n             for (i, (decl, ty)) in mir.upvar_decls.iter().zip(upvar_tys).enumerate() {\n                 let byte_offset_of_var_in_env = closure_layout.fields.offset(i).bytes();\n \n-                let ops = bx.cx().debuginfo_upvar_decls_ops_sequence(byte_offset_of_var_in_env);\n+                let ops = bx.debuginfo_upvar_decls_ops_sequence(byte_offset_of_var_in_env);\n \n                 // The environment and the capture can each be indirect.\n "}, {"sha": "568a7e7e1600f3fe3a3019c11e366f7d66eb8a0d", "filename": "src/librustc_codegen_ssa/mir/statement.rs", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/rust-lang/rust/blob/ceb29e2ac45474a560b04ce4061d8a6cc50e1a33/src%2Flibrustc_codegen_ssa%2Fmir%2Fstatement.rs", "raw_url": "https://github.com/rust-lang/rust/raw/ceb29e2ac45474a560b04ce4061d8a6cc50e1a33/src%2Flibrustc_codegen_ssa%2Fmir%2Fstatement.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_codegen_ssa%2Fmir%2Fstatement.rs?ref=ceb29e2ac45474a560b04ce4061d8a6cc50e1a33", "patch": "@@ -89,7 +89,7 @@ impl<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>> FunctionCx<'a, 'tcx, Bx> {\n                         if let OperandValue::Immediate(_) = op.val {\n                             acc.push(op.immediate());\n                         } else {\n-                            span_err!(bx.cx().sess(), span.to_owned(), E0669,\n+                            span_err!(bx.sess(), span.to_owned(), E0669,\n                                      \"invalid value for constraint in inline assembly\");\n                         }\n                         acc\n@@ -98,7 +98,7 @@ impl<'a, 'tcx: 'a, Bx: BuilderMethods<'a, 'tcx>> FunctionCx<'a, 'tcx, Bx> {\n                 if input_vals.len() == inputs.len() {\n                     let res = bx.codegen_inline_asm(asm, outputs, input_vals);\n                     if !res {\n-                        span_err!(bx.cx().sess(), statement.source_info.span, E0668,\n+                        span_err!(bx.sess(), statement.source_info.span, E0668,\n                                   \"malformed inline assembly\");\n                     }\n                 }"}]}