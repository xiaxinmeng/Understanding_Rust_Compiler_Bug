{"sha": "c3a7ca1125e017ecf5e46685f8ee6feccceec0c7", "node_id": "C_kwDOAAsO6NoAKGMzYTdjYTExMjVlMDE3ZWNmNWU0NjY4NWY4ZWU2ZmVjY2NlZWMwYzc", "commit": {"author": {"name": "Ralf Jung", "email": "post@ralfj.de", "date": "2022-11-06T12:44:50Z"}, "committer": {"name": "Ralf Jung", "email": "post@ralfj.de", "date": "2022-11-06T13:17:10Z"}, "message": "move InitMask to its own module", "tree": {"sha": "bd477112ff75138a4494286a9eef7564b58fca86", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/bd477112ff75138a4494286a9eef7564b58fca86"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/c3a7ca1125e017ecf5e46685f8ee6feccceec0c7", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/c3a7ca1125e017ecf5e46685f8ee6feccceec0c7", "html_url": "https://github.com/rust-lang/rust/commit/c3a7ca1125e017ecf5e46685f8ee6feccceec0c7", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/c3a7ca1125e017ecf5e46685f8ee6feccceec0c7/comments", "author": {"login": "RalfJung", "id": 330628, "node_id": "MDQ6VXNlcjMzMDYyOA==", "avatar_url": "https://avatars.githubusercontent.com/u/330628?v=4", "gravatar_id": "", "url": "https://api.github.com/users/RalfJung", "html_url": "https://github.com/RalfJung", "followers_url": "https://api.github.com/users/RalfJung/followers", "following_url": "https://api.github.com/users/RalfJung/following{/other_user}", "gists_url": "https://api.github.com/users/RalfJung/gists{/gist_id}", "starred_url": "https://api.github.com/users/RalfJung/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/RalfJung/subscriptions", "organizations_url": "https://api.github.com/users/RalfJung/orgs", "repos_url": "https://api.github.com/users/RalfJung/repos", "events_url": "https://api.github.com/users/RalfJung/events{/privacy}", "received_events_url": "https://api.github.com/users/RalfJung/received_events", "type": "User", "site_admin": false}, "committer": {"login": "RalfJung", "id": 330628, "node_id": "MDQ6VXNlcjMzMDYyOA==", "avatar_url": "https://avatars.githubusercontent.com/u/330628?v=4", "gravatar_id": "", "url": "https://api.github.com/users/RalfJung", "html_url": "https://github.com/RalfJung", "followers_url": "https://api.github.com/users/RalfJung/followers", "following_url": "https://api.github.com/users/RalfJung/following{/other_user}", "gists_url": "https://api.github.com/users/RalfJung/gists{/gist_id}", "starred_url": "https://api.github.com/users/RalfJung/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/RalfJung/subscriptions", "organizations_url": "https://api.github.com/users/RalfJung/orgs", "repos_url": "https://api.github.com/users/RalfJung/repos", "events_url": "https://api.github.com/users/RalfJung/events{/privacy}", "received_events_url": "https://api.github.com/users/RalfJung/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "3477645a23bbb25b61d5e17d6d45a06821188876", "url": "https://api.github.com/repos/rust-lang/rust/commits/3477645a23bbb25b61d5e17d6d45a06821188876", "html_url": "https://github.com/rust-lang/rust/commit/3477645a23bbb25b61d5e17d6d45a06821188876"}], "stats": {"total": 1180, "additions": 582, "deletions": 598}, "files": [{"sha": "b23074935e1bbea63f03e158d184a629f850cbb7", "filename": "compiler/rustc_codegen_llvm/src/consts.rs", "status": "modified", "additions": 1, "deletions": 3, "changes": 4, "blob_url": "https://github.com/rust-lang/rust/blob/c3a7ca1125e017ecf5e46685f8ee6feccceec0c7/compiler%2Frustc_codegen_llvm%2Fsrc%2Fconsts.rs", "raw_url": "https://github.com/rust-lang/rust/raw/c3a7ca1125e017ecf5e46685f8ee6feccceec0c7/compiler%2Frustc_codegen_llvm%2Fsrc%2Fconsts.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_codegen_llvm%2Fsrc%2Fconsts.rs?ref=c3a7ca1125e017ecf5e46685f8ee6feccceec0c7", "patch": "@@ -38,9 +38,7 @@ pub fn const_alloc_to_llvm<'ll>(cx: &CodegenCx<'ll, '_>, alloc: ConstAllocation<\n         alloc: &'a Allocation,\n         range: Range<usize>,\n     ) {\n-        let chunks = alloc\n-            .init_mask()\n-            .range_as_init_chunks(Size::from_bytes(range.start), Size::from_bytes(range.end));\n+        let chunks = alloc.init_mask().range_as_init_chunks(range.clone().into());\n \n         let chunk_to_llval = move |chunk| match chunk {\n             InitChunk::Init(range) => {"}, {"sha": "4eda9882da39ed02d0b9cdf4ec437358d3b86917", "filename": "compiler/rustc_const_eval/src/interpret/memory.rs", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "blob_url": "https://github.com/rust-lang/rust/blob/c3a7ca1125e017ecf5e46685f8ee6feccceec0c7/compiler%2Frustc_const_eval%2Fsrc%2Finterpret%2Fmemory.rs", "raw_url": "https://github.com/rust-lang/rust/raw/c3a7ca1125e017ecf5e46685f8ee6feccceec0c7/compiler%2Frustc_const_eval%2Fsrc%2Finterpret%2Fmemory.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_const_eval%2Fsrc%2Finterpret%2Fmemory.rs?ref=c3a7ca1125e017ecf5e46685f8ee6feccceec0c7", "patch": "@@ -1089,7 +1089,7 @@ impl<'mir, 'tcx: 'mir, M: Machine<'mir, 'tcx>> InterpCx<'mir, 'tcx, M> {\n             .prepare_copy(src_range, dest_offset, num_copies, self)\n             .map_err(|e| e.to_interp_error(dest_alloc_id))?;\n         // Prepare a copy of the initialization mask.\n-        let init = src_alloc.compress_uninit_range(src_range);\n+        let init = src_alloc.init_mask().prepare_copy(src_range);\n \n         // Destination alloc preparations and access hooks.\n         let (dest_alloc, extra) = self.get_alloc_raw_mut(dest_alloc_id)?;\n@@ -1155,8 +1155,8 @@ impl<'mir, 'tcx: 'mir, M: Machine<'mir, 'tcx>> InterpCx<'mir, 'tcx, M> {\n         }\n \n         // now fill in all the \"init\" data\n-        dest_alloc.mark_compressed_init_range(\n-            &init,\n+        dest_alloc.init_mask_apply_copy(\n+            init,\n             alloc_range(dest_offset, size), // just a single copy (i.e., not full `dest_range`)\n             num_copies,\n         );"}, {"sha": "4a80088ca2f91a6f2897330c5abdeeedc29a907c", "filename": "compiler/rustc_middle/src/mir/interpret/allocation.rs", "status": "modified", "additions": 36, "deletions": 583, "changes": 619, "blob_url": "https://github.com/rust-lang/rust/blob/c3a7ca1125e017ecf5e46685f8ee6feccceec0c7/compiler%2Frustc_middle%2Fsrc%2Fmir%2Finterpret%2Fallocation.rs", "raw_url": "https://github.com/rust-lang/rust/raw/c3a7ca1125e017ecf5e46685f8ee6feccceec0c7/compiler%2Frustc_middle%2Fsrc%2Fmir%2Finterpret%2Fallocation.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_middle%2Fsrc%2Fmir%2Finterpret%2Fallocation.rs?ref=c3a7ca1125e017ecf5e46685f8ee6feccceec0c7", "patch": "@@ -1,12 +1,11 @@\n //! The virtual memory representation of the MIR interpreter.\n \n+mod init_mask;\n mod provenance_map;\n \n use std::borrow::Cow;\n-use std::convert::{TryFrom, TryInto};\n use std::fmt;\n use std::hash;\n-use std::iter;\n use std::ops::Range;\n use std::ptr;\n \n@@ -21,8 +20,11 @@ use super::{\n     UnsupportedOpInfo,\n };\n use crate::ty;\n+use init_mask::*;\n use provenance_map::*;\n \n+pub use init_mask::{InitChunk, InitChunkIter};\n+\n /// This type represents an Allocation in the Miri/CTFE core engine.\n ///\n /// Its public API is rather low-level, working directly with allocation offsets and a custom error\n@@ -110,7 +112,7 @@ pub struct ConstAllocation<'tcx, Prov = AllocId, Extra = ()>(\n \n impl<'tcx> fmt::Debug for ConstAllocation<'tcx> {\n     fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {\n-        // The debug-representation of this is very verbose and basically useless,\n+        // The debug representation of this is very verbose and basically useless,\n         // so don't print it.\n         write!(f, \"ConstAllocation {{ .. }}\")\n     }\n@@ -185,12 +187,21 @@ pub fn alloc_range(start: Size, size: Size) -> AllocRange {\n     AllocRange { start, size }\n }\n \n-impl AllocRange {\n+impl From<Range<Size>> for AllocRange {\n     #[inline]\n-    pub fn from(r: Range<Size>) -> Self {\n+    fn from(r: Range<Size>) -> Self {\n         alloc_range(r.start, r.end - r.start) // `Size` subtraction (overflow-checked)\n     }\n+}\n+\n+impl From<Range<usize>> for AllocRange {\n+    #[inline]\n+    fn from(r: Range<usize>) -> Self {\n+        AllocRange::from(Size::from_bytes(r.start)..Size::from_bytes(r.end))\n+    }\n+}\n \n+impl AllocRange {\n     #[inline(always)]\n     pub fn end(self) -> Size {\n         self.start + self.size // This does overflow checking.\n@@ -351,7 +362,12 @@ impl<Prov: Provenance, Extra> Allocation<Prov, Extra> {\n         cx: &impl HasDataLayout,\n         range: AllocRange,\n     ) -> AllocResult<&[u8]> {\n-        self.check_init(range)?;\n+        self.init_mask.is_range_initialized(range).map_err(|uninit_range| {\n+            AllocError::InvalidUninitBytes(Some(UninitBytesAccess {\n+                access: range,\n+                uninit: uninit_range,\n+            }))\n+        })?;\n         if !Prov::OFFSET_IS_ADDR {\n             if !self.provenance.range_empty(range, cx) {\n                 return Err(AllocError::ReadPointerAsBytes);\n@@ -395,6 +411,15 @@ impl<Prov: Provenance, Extra> Allocation<Prov, Extra> {\n \n /// Reading and writing.\n impl<Prov: Provenance, Extra> Allocation<Prov, Extra> {\n+    /// Sets the init bit for the given range.\n+    fn mark_init(&mut self, range: AllocRange, is_init: bool) {\n+        if range.size.bytes() == 0 {\n+            return;\n+        }\n+        assert!(self.mutability == Mutability::Mut);\n+        self.init_mask.set_range(range, is_init);\n+    }\n+\n     /// Reads a *non-ZST* scalar.\n     ///\n     /// If `read_provenance` is `true`, this will also read provenance; otherwise (if the machine\n@@ -412,7 +437,7 @@ impl<Prov: Provenance, Extra> Allocation<Prov, Extra> {\n         read_provenance: bool,\n     ) -> AllocResult<Scalar<Prov>> {\n         // First and foremost, if anything is uninit, bail.\n-        if self.is_init(range).is_err() {\n+        if self.init_mask.is_range_initialized(range).is_err() {\n             return Err(AllocError::InvalidUninitBytes(None));\n         }\n \n@@ -505,7 +530,7 @@ impl<Prov: Provenance, Extra> Allocation<Prov, Extra> {\n         return Ok(());\n     }\n \n-    /// Applies a provenance copy.\n+    /// Applies a previously prepared provenance copy.\n     /// The affected range, as defined in the parameters to `provenance().prepare_copy` is expected\n     /// to be clear of provenance.\n     ///\n@@ -514,584 +539,12 @@ impl<Prov: Provenance, Extra> Allocation<Prov, Extra> {\n     pub fn provenance_apply_copy(&mut self, copy: ProvenanceCopy<Prov>) {\n         self.provenance.apply_copy(copy)\n     }\n-}\n-\n-////////////////////////////////////////////////////////////////////////////////\n-// Uninitialized byte tracking\n-////////////////////////////////////////////////////////////////////////////////\n-\n-type Block = u64;\n-\n-/// A bitmask where each bit refers to the byte with the same index. If the bit is `true`, the byte\n-/// is initialized. If it is `false` the byte is uninitialized.\n-// Note: for performance reasons when interning, some of the `InitMask` fields can be partially\n-// hashed. (see the `Hash` impl below for more details), so the impl is not derived.\n-#[derive(Clone, Debug, Eq, PartialEq, PartialOrd, Ord, TyEncodable, TyDecodable)]\n-#[derive(HashStable)]\n-pub struct InitMask {\n-    blocks: Vec<Block>,\n-    len: Size,\n-}\n-\n-// Const allocations are only hashed for interning. However, they can be large, making the hashing\n-// expensive especially since it uses `FxHash`: it's better suited to short keys, not potentially\n-// big buffers like the allocation's init mask. We can partially hash some fields when they're\n-// large.\n-impl hash::Hash for InitMask {\n-    fn hash<H: hash::Hasher>(&self, state: &mut H) {\n-        const MAX_BLOCKS_TO_HASH: usize = MAX_BYTES_TO_HASH / std::mem::size_of::<Block>();\n-        const MAX_BLOCKS_LEN: usize = MAX_HASHED_BUFFER_LEN / std::mem::size_of::<Block>();\n-\n-        // Partially hash the `blocks` buffer when it is large. To limit collisions with common\n-        // prefixes and suffixes, we hash the length and some slices of the buffer.\n-        let block_count = self.blocks.len();\n-        if block_count > MAX_BLOCKS_LEN {\n-            // Hash the buffer's length.\n-            block_count.hash(state);\n-\n-            // And its head and tail.\n-            self.blocks[..MAX_BLOCKS_TO_HASH].hash(state);\n-            self.blocks[block_count - MAX_BLOCKS_TO_HASH..].hash(state);\n-        } else {\n-            self.blocks.hash(state);\n-        }\n-\n-        // Hash the other fields as usual.\n-        self.len.hash(state);\n-    }\n-}\n-\n-impl InitMask {\n-    pub const BLOCK_SIZE: u64 = 64;\n-\n-    #[inline]\n-    fn bit_index(bits: Size) -> (usize, usize) {\n-        // BLOCK_SIZE is the number of bits that can fit in a `Block`.\n-        // Each bit in a `Block` represents the initialization state of one byte of an allocation,\n-        // so we use `.bytes()` here.\n-        let bits = bits.bytes();\n-        let a = bits / InitMask::BLOCK_SIZE;\n-        let b = bits % InitMask::BLOCK_SIZE;\n-        (usize::try_from(a).unwrap(), usize::try_from(b).unwrap())\n-    }\n-\n-    #[inline]\n-    fn size_from_bit_index(block: impl TryInto<u64>, bit: impl TryInto<u64>) -> Size {\n-        let block = block.try_into().ok().unwrap();\n-        let bit = bit.try_into().ok().unwrap();\n-        Size::from_bytes(block * InitMask::BLOCK_SIZE + bit)\n-    }\n-\n-    pub fn new(size: Size, state: bool) -> Self {\n-        let mut m = InitMask { blocks: vec![], len: Size::ZERO };\n-        m.grow(size, state);\n-        m\n-    }\n-\n-    pub fn set_range(&mut self, start: Size, end: Size, new_state: bool) {\n-        let len = self.len;\n-        if end > len {\n-            self.grow(end - len, new_state);\n-        }\n-        self.set_range_inbounds(start, end, new_state);\n-    }\n-\n-    pub fn set_range_inbounds(&mut self, start: Size, end: Size, new_state: bool) {\n-        let (blocka, bita) = Self::bit_index(start);\n-        let (blockb, bitb) = Self::bit_index(end);\n-        if blocka == blockb {\n-            // First set all bits except the first `bita`,\n-            // then unset the last `64 - bitb` bits.\n-            let range = if bitb == 0 {\n-                u64::MAX << bita\n-            } else {\n-                (u64::MAX << bita) & (u64::MAX >> (64 - bitb))\n-            };\n-            if new_state {\n-                self.blocks[blocka] |= range;\n-            } else {\n-                self.blocks[blocka] &= !range;\n-            }\n-            return;\n-        }\n-        // across block boundaries\n-        if new_state {\n-            // Set `bita..64` to `1`.\n-            self.blocks[blocka] |= u64::MAX << bita;\n-            // Set `0..bitb` to `1`.\n-            if bitb != 0 {\n-                self.blocks[blockb] |= u64::MAX >> (64 - bitb);\n-            }\n-            // Fill in all the other blocks (much faster than one bit at a time).\n-            for block in (blocka + 1)..blockb {\n-                self.blocks[block] = u64::MAX;\n-            }\n-        } else {\n-            // Set `bita..64` to `0`.\n-            self.blocks[blocka] &= !(u64::MAX << bita);\n-            // Set `0..bitb` to `0`.\n-            if bitb != 0 {\n-                self.blocks[blockb] &= !(u64::MAX >> (64 - bitb));\n-            }\n-            // Fill in all the other blocks (much faster than one bit at a time).\n-            for block in (blocka + 1)..blockb {\n-                self.blocks[block] = 0;\n-            }\n-        }\n-    }\n-\n-    #[inline]\n-    pub fn get(&self, i: Size) -> bool {\n-        let (block, bit) = Self::bit_index(i);\n-        (self.blocks[block] & (1 << bit)) != 0\n-    }\n-\n-    #[inline]\n-    pub fn set(&mut self, i: Size, new_state: bool) {\n-        let (block, bit) = Self::bit_index(i);\n-        self.set_bit(block, bit, new_state);\n-    }\n-\n-    #[inline]\n-    fn set_bit(&mut self, block: usize, bit: usize, new_state: bool) {\n-        if new_state {\n-            self.blocks[block] |= 1 << bit;\n-        } else {\n-            self.blocks[block] &= !(1 << bit);\n-        }\n-    }\n-\n-    pub fn grow(&mut self, amount: Size, new_state: bool) {\n-        if amount.bytes() == 0 {\n-            return;\n-        }\n-        let unused_trailing_bits =\n-            u64::try_from(self.blocks.len()).unwrap() * Self::BLOCK_SIZE - self.len.bytes();\n-        if amount.bytes() > unused_trailing_bits {\n-            let additional_blocks = amount.bytes() / Self::BLOCK_SIZE + 1;\n-            self.blocks.extend(\n-                // FIXME(oli-obk): optimize this by repeating `new_state as Block`.\n-                iter::repeat(0).take(usize::try_from(additional_blocks).unwrap()),\n-            );\n-        }\n-        let start = self.len;\n-        self.len += amount;\n-        self.set_range_inbounds(start, start + amount, new_state); // `Size` operation\n-    }\n-\n-    /// Returns the index of the first bit in `start..end` (end-exclusive) that is equal to is_init.\n-    fn find_bit(&self, start: Size, end: Size, is_init: bool) -> Option<Size> {\n-        /// A fast implementation of `find_bit`,\n-        /// which skips over an entire block at a time if it's all 0s (resp. 1s),\n-        /// and finds the first 1 (resp. 0) bit inside a block using `trailing_zeros` instead of a loop.\n-        ///\n-        /// Note that all examples below are written with 8 (instead of 64) bit blocks for simplicity,\n-        /// and with the least significant bit (and lowest block) first:\n-        /// ```text\n-        ///        00000000|00000000\n-        ///        ^      ^ ^      ^\n-        /// index: 0      7 8      15\n-        /// ```\n-        /// Also, if not stated, assume that `is_init = true`, that is, we are searching for the first 1 bit.\n-        fn find_bit_fast(\n-            init_mask: &InitMask,\n-            start: Size,\n-            end: Size,\n-            is_init: bool,\n-        ) -> Option<Size> {\n-            /// Search one block, returning the index of the first bit equal to `is_init`.\n-            fn search_block(\n-                bits: Block,\n-                block: usize,\n-                start_bit: usize,\n-                is_init: bool,\n-            ) -> Option<Size> {\n-                // For the following examples, assume this function was called with:\n-                //   bits = 0b00111011\n-                //   start_bit = 3\n-                //   is_init = false\n-                // Note that, for the examples in this function, the most significant bit is written first,\n-                // which is backwards compared to the comments in `find_bit`/`find_bit_fast`.\n-\n-                // Invert bits so we're always looking for the first set bit.\n-                //        ! 0b00111011\n-                //   bits = 0b11000100\n-                let bits = if is_init { bits } else { !bits };\n-                // Mask off unused start bits.\n-                //          0b11000100\n-                //        & 0b11111000\n-                //   bits = 0b11000000\n-                let bits = bits & (!0 << start_bit);\n-                // Find set bit, if any.\n-                //   bit = trailing_zeros(0b11000000)\n-                //   bit = 6\n-                if bits == 0 {\n-                    None\n-                } else {\n-                    let bit = bits.trailing_zeros();\n-                    Some(InitMask::size_from_bit_index(block, bit))\n-                }\n-            }\n-\n-            if start >= end {\n-                return None;\n-            }\n-\n-            // Convert `start` and `end` to block indexes and bit indexes within each block.\n-            // We must convert `end` to an inclusive bound to handle block boundaries correctly.\n-            //\n-            // For example:\n-            //\n-            //   (a) 00000000|00000000    (b) 00000000|\n-            //       ^~~~~~~~~~~^             ^~~~~~~~~^\n-            //     start       end          start     end\n-            //\n-            // In both cases, the block index of `end` is 1.\n-            // But we do want to search block 1 in (a), and we don't in (b).\n-            //\n-            // We subtract 1 from both end positions to make them inclusive:\n-            //\n-            //   (a) 00000000|00000000    (b) 00000000|\n-            //       ^~~~~~~~~~^              ^~~~~~~^\n-            //     start    end_inclusive   start end_inclusive\n-            //\n-            // For (a), the block index of `end_inclusive` is 1, and for (b), it's 0.\n-            // This provides the desired behavior of searching blocks 0 and 1 for (a),\n-            // and searching only block 0 for (b).\n-            // There is no concern of overflows since we checked for `start >= end` above.\n-            let (start_block, start_bit) = InitMask::bit_index(start);\n-            let end_inclusive = Size::from_bytes(end.bytes() - 1);\n-            let (end_block_inclusive, _) = InitMask::bit_index(end_inclusive);\n-\n-            // Handle first block: need to skip `start_bit` bits.\n-            //\n-            // We need to handle the first block separately,\n-            // because there may be bits earlier in the block that should be ignored,\n-            // such as the bit marked (1) in this example:\n-            //\n-            //       (1)\n-            //       -|------\n-            //   (c) 01000000|00000000|00000001\n-            //          ^~~~~~~~~~~~~~~~~~^\n-            //        start              end\n-            if let Some(i) =\n-                search_block(init_mask.blocks[start_block], start_block, start_bit, is_init)\n-            {\n-                // If the range is less than a block, we may find a matching bit after `end`.\n-                //\n-                // For example, we shouldn't successfully find bit (2), because it's after `end`:\n-                //\n-                //             (2)\n-                //       -------|\n-                //   (d) 00000001|00000000|00000001\n-                //        ^~~~~^\n-                //      start end\n-                //\n-                // An alternative would be to mask off end bits in the same way as we do for start bits,\n-                // but performing this check afterwards is faster and simpler to implement.\n-                if i < end {\n-                    return Some(i);\n-                } else {\n-                    return None;\n-                }\n-            }\n-\n-            // Handle remaining blocks.\n-            //\n-            // We can skip over an entire block at once if it's all 0s (resp. 1s).\n-            // The block marked (3) in this example is the first block that will be handled by this loop,\n-            // and it will be skipped for that reason:\n-            //\n-            //                   (3)\n-            //                --------\n-            //   (e) 01000000|00000000|00000001\n-            //          ^~~~~~~~~~~~~~~~~~^\n-            //        start              end\n-            if start_block < end_block_inclusive {\n-                // This loop is written in a specific way for performance.\n-                // Notably: `..end_block_inclusive + 1` is used for an inclusive range instead of `..=end_block_inclusive`,\n-                // and `.zip(start_block + 1..)` is used to track the index instead of `.enumerate().skip().take()`,\n-                // because both alternatives result in significantly worse codegen.\n-                // `end_block_inclusive + 1` is guaranteed not to wrap, because `end_block_inclusive <= end / BLOCK_SIZE`,\n-                // and `BLOCK_SIZE` (the number of bits per block) will always be at least 8 (1 byte).\n-                for (&bits, block) in init_mask.blocks[start_block + 1..end_block_inclusive + 1]\n-                    .iter()\n-                    .zip(start_block + 1..)\n-                {\n-                    if let Some(i) = search_block(bits, block, 0, is_init) {\n-                        // If this is the last block, we may find a matching bit after `end`.\n-                        //\n-                        // For example, we shouldn't successfully find bit (4), because it's after `end`:\n-                        //\n-                        //                               (4)\n-                        //                         -------|\n-                        //   (f) 00000001|00000000|00000001\n-                        //          ^~~~~~~~~~~~~~~~~~^\n-                        //        start              end\n-                        //\n-                        // As above with example (d), we could handle the end block separately and mask off end bits,\n-                        // but unconditionally searching an entire block at once and performing this check afterwards\n-                        // is faster and much simpler to implement.\n-                        if i < end {\n-                            return Some(i);\n-                        } else {\n-                            return None;\n-                        }\n-                    }\n-                }\n-            }\n-\n-            None\n-        }\n-\n-        #[cfg_attr(not(debug_assertions), allow(dead_code))]\n-        fn find_bit_slow(\n-            init_mask: &InitMask,\n-            start: Size,\n-            end: Size,\n-            is_init: bool,\n-        ) -> Option<Size> {\n-            (start..end).find(|&i| init_mask.get(i) == is_init)\n-        }\n-\n-        let result = find_bit_fast(self, start, end, is_init);\n-\n-        debug_assert_eq!(\n-            result,\n-            find_bit_slow(self, start, end, is_init),\n-            \"optimized implementation of find_bit is wrong for start={:?} end={:?} is_init={} init_mask={:#?}\",\n-            start,\n-            end,\n-            is_init,\n-            self\n-        );\n-\n-        result\n-    }\n-}\n-\n-/// A contiguous chunk of initialized or uninitialized memory.\n-pub enum InitChunk {\n-    Init(Range<Size>),\n-    Uninit(Range<Size>),\n-}\n-\n-impl InitChunk {\n-    #[inline]\n-    pub fn is_init(&self) -> bool {\n-        match self {\n-            Self::Init(_) => true,\n-            Self::Uninit(_) => false,\n-        }\n-    }\n-\n-    #[inline]\n-    pub fn range(&self) -> Range<Size> {\n-        match self {\n-            Self::Init(r) => r.clone(),\n-            Self::Uninit(r) => r.clone(),\n-        }\n-    }\n-}\n-\n-impl InitMask {\n-    /// Checks whether the range `start..end` (end-exclusive) is entirely initialized.\n-    ///\n-    /// Returns `Ok(())` if it's initialized. Otherwise returns a range of byte\n-    /// indexes for the first contiguous span of the uninitialized access.\n-    #[inline]\n-    pub fn is_range_initialized(&self, start: Size, end: Size) -> Result<(), AllocRange> {\n-        if end > self.len {\n-            return Err(AllocRange::from(self.len..end));\n-        }\n-\n-        let uninit_start = self.find_bit(start, end, false);\n-\n-        match uninit_start {\n-            Some(uninit_start) => {\n-                let uninit_end = self.find_bit(uninit_start, end, true).unwrap_or(end);\n-                Err(AllocRange::from(uninit_start..uninit_end))\n-            }\n-            None => Ok(()),\n-        }\n-    }\n-\n-    /// Returns an iterator, yielding a range of byte indexes for each contiguous region\n-    /// of initialized or uninitialized bytes inside the range `start..end` (end-exclusive).\n-    ///\n-    /// The iterator guarantees the following:\n-    /// - Chunks are nonempty.\n-    /// - Chunks are adjacent (each range's start is equal to the previous range's end).\n-    /// - Chunks span exactly `start..end` (the first starts at `start`, the last ends at `end`).\n-    /// - Chunks alternate between [`InitChunk::Init`] and [`InitChunk::Uninit`].\n-    #[inline]\n-    pub fn range_as_init_chunks(&self, start: Size, end: Size) -> InitChunkIter<'_> {\n-        assert!(end <= self.len);\n-\n-        let is_init = if start < end {\n-            self.get(start)\n-        } else {\n-            // `start..end` is empty: there are no chunks, so use some arbitrary value\n-            false\n-        };\n-\n-        InitChunkIter { init_mask: self, is_init, start, end }\n-    }\n-}\n-\n-/// Yields [`InitChunk`]s. See [`InitMask::range_as_init_chunks`].\n-#[derive(Clone)]\n-pub struct InitChunkIter<'a> {\n-    init_mask: &'a InitMask,\n-    /// Whether the next chunk we will return is initialized.\n-    /// If there are no more chunks, contains some arbitrary value.\n-    is_init: bool,\n-    /// The current byte index into `init_mask`.\n-    start: Size,\n-    /// The end byte index into `init_mask`.\n-    end: Size,\n-}\n-\n-impl<'a> Iterator for InitChunkIter<'a> {\n-    type Item = InitChunk;\n-\n-    #[inline]\n-    fn next(&mut self) -> Option<Self::Item> {\n-        if self.start >= self.end {\n-            return None;\n-        }\n-\n-        let end_of_chunk =\n-            self.init_mask.find_bit(self.start, self.end, !self.is_init).unwrap_or(self.end);\n-        let range = self.start..end_of_chunk;\n-\n-        let ret =\n-            Some(if self.is_init { InitChunk::Init(range) } else { InitChunk::Uninit(range) });\n-\n-        self.is_init = !self.is_init;\n-        self.start = end_of_chunk;\n-\n-        ret\n-    }\n-}\n \n-/// Uninitialized bytes.\n-impl<Prov: Copy, Extra> Allocation<Prov, Extra> {\n-    /// Checks whether the given range  is entirely initialized.\n-    ///\n-    /// Returns `Ok(())` if it's initialized. Otherwise returns the range of byte\n-    /// indexes of the first contiguous uninitialized access.\n-    fn is_init(&self, range: AllocRange) -> Result<(), AllocRange> {\n-        self.init_mask.is_range_initialized(range.start, range.end()) // `Size` addition\n-    }\n-\n-    /// Checks that a range of bytes is initialized. If not, returns the `InvalidUninitBytes`\n-    /// error which will report the first range of bytes which is uninitialized.\n-    fn check_init(&self, range: AllocRange) -> AllocResult {\n-        self.is_init(range).map_err(|uninit_range| {\n-            AllocError::InvalidUninitBytes(Some(UninitBytesAccess {\n-                access: range,\n-                uninit: uninit_range,\n-            }))\n-        })\n-    }\n-\n-    fn mark_init(&mut self, range: AllocRange, is_init: bool) {\n-        if range.size.bytes() == 0 {\n-            return;\n-        }\n-        assert!(self.mutability == Mutability::Mut);\n-        self.init_mask.set_range(range.start, range.end(), is_init);\n-    }\n-}\n-\n-/// Run-length encoding of the uninit mask.\n-/// Used to copy parts of a mask multiple times to another allocation.\n-pub struct InitMaskCompressed {\n-    /// Whether the first range is initialized.\n-    initial: bool,\n-    /// The lengths of ranges that are run-length encoded.\n-    /// The initialization state of the ranges alternate starting with `initial`.\n-    ranges: smallvec::SmallVec<[u64; 1]>,\n-}\n-\n-impl InitMaskCompressed {\n-    pub fn no_bytes_init(&self) -> bool {\n-        // The `ranges` are run-length encoded and of alternating initialization state.\n-        // So if `ranges.len() > 1` then the second block is an initialized range.\n-        !self.initial && self.ranges.len() == 1\n-    }\n-}\n-\n-/// Transferring the initialization mask to other allocations.\n-impl<Prov, Extra> Allocation<Prov, Extra> {\n-    /// Creates a run-length encoding of the initialization mask; panics if range is empty.\n-    ///\n-    /// This is essentially a more space-efficient version of\n-    /// `InitMask::range_as_init_chunks(...).collect::<Vec<_>>()`.\n-    pub fn compress_uninit_range(&self, range: AllocRange) -> InitMaskCompressed {\n-        // Since we are copying `size` bytes from `src` to `dest + i * size` (`for i in 0..repeat`),\n-        // a naive initialization mask copying algorithm would repeatedly have to read the initialization mask from\n-        // the source and write it to the destination. Even if we optimized the memory accesses,\n-        // we'd be doing all of this `repeat` times.\n-        // Therefore we precompute a compressed version of the initialization mask of the source value and\n-        // then write it back `repeat` times without computing any more information from the source.\n-\n-        // A precomputed cache for ranges of initialized / uninitialized bits\n-        // 0000010010001110 will become\n-        // `[5, 1, 2, 1, 3, 3, 1]`,\n-        // where each element toggles the state.\n-\n-        let mut ranges = smallvec::SmallVec::<[u64; 1]>::new();\n-\n-        let mut chunks = self.init_mask.range_as_init_chunks(range.start, range.end()).peekable();\n-\n-        let initial = chunks.peek().expect(\"range should be nonempty\").is_init();\n-\n-        // Here we rely on `range_as_init_chunks` to yield alternating init/uninit chunks.\n-        for chunk in chunks {\n-            let len = chunk.range().end.bytes() - chunk.range().start.bytes();\n-            ranges.push(len);\n-        }\n-\n-        InitMaskCompressed { ranges, initial }\n-    }\n-\n-    /// Applies multiple instances of the run-length encoding to the initialization mask.\n+    /// Applies a previously prepared copy of the init mask.\n     ///\n     /// This is dangerous to use as it can violate internal `Allocation` invariants!\n     /// It only exists to support an efficient implementation of `mem_copy_repeatedly`.\n-    pub fn mark_compressed_init_range(\n-        &mut self,\n-        defined: &InitMaskCompressed,\n-        range: AllocRange,\n-        repeat: u64,\n-    ) {\n-        // An optimization where we can just overwrite an entire range of initialization\n-        // bits if they are going to be uniformly `1` or `0`.\n-        if defined.ranges.len() <= 1 {\n-            self.init_mask.set_range_inbounds(\n-                range.start,\n-                range.start + range.size * repeat, // `Size` operations\n-                defined.initial,\n-            );\n-            return;\n-        }\n-\n-        for mut j in 0..repeat {\n-            j *= range.size.bytes();\n-            j += range.start.bytes();\n-            let mut cur = defined.initial;\n-            for range in &defined.ranges {\n-                let old_j = j;\n-                j += range;\n-                self.init_mask.set_range_inbounds(\n-                    Size::from_bytes(old_j),\n-                    Size::from_bytes(j),\n-                    cur,\n-                );\n-                cur = !cur;\n-            }\n-        }\n+    pub fn init_mask_apply_copy(&mut self, copy: InitCopy, range: AllocRange, repeat: u64) {\n+        self.init_mask.apply_copy(copy, range, repeat)\n     }\n }"}, {"sha": "4f406cc5ef86e196ee787f49e86cefbc07125fd6", "filename": "compiler/rustc_middle/src/mir/interpret/allocation/init_mask.rs", "status": "added", "additions": 530, "deletions": 0, "changes": 530, "blob_url": "https://github.com/rust-lang/rust/blob/c3a7ca1125e017ecf5e46685f8ee6feccceec0c7/compiler%2Frustc_middle%2Fsrc%2Fmir%2Finterpret%2Fallocation%2Finit_mask.rs", "raw_url": "https://github.com/rust-lang/rust/raw/c3a7ca1125e017ecf5e46685f8ee6feccceec0c7/compiler%2Frustc_middle%2Fsrc%2Fmir%2Finterpret%2Fallocation%2Finit_mask.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_middle%2Fsrc%2Fmir%2Finterpret%2Fallocation%2Finit_mask.rs?ref=c3a7ca1125e017ecf5e46685f8ee6feccceec0c7", "patch": "@@ -0,0 +1,530 @@\n+use std::hash;\n+use std::iter;\n+use std::ops::Range;\n+\n+use rustc_target::abi::Size;\n+\n+use super::AllocRange;\n+\n+type Block = u64;\n+\n+/// A bitmask where each bit refers to the byte with the same index. If the bit is `true`, the byte\n+/// is initialized. If it is `false` the byte is uninitialized.\n+// Note: for performance reasons when interning, some of the `InitMask` fields can be partially\n+// hashed. (see the `Hash` impl below for more details), so the impl is not derived.\n+#[derive(Clone, Debug, Eq, PartialEq, PartialOrd, Ord, TyEncodable, TyDecodable)]\n+#[derive(HashStable)]\n+pub struct InitMask {\n+    blocks: Vec<Block>,\n+    len: Size,\n+}\n+\n+// Const allocations are only hashed for interning. However, they can be large, making the hashing\n+// expensive especially since it uses `FxHash`: it's better suited to short keys, not potentially\n+// big buffers like the allocation's init mask. We can partially hash some fields when they're\n+// large.\n+impl hash::Hash for InitMask {\n+    fn hash<H: hash::Hasher>(&self, state: &mut H) {\n+        const MAX_BLOCKS_TO_HASH: usize = super::MAX_BYTES_TO_HASH / std::mem::size_of::<Block>();\n+        const MAX_BLOCKS_LEN: usize = super::MAX_HASHED_BUFFER_LEN / std::mem::size_of::<Block>();\n+\n+        // Partially hash the `blocks` buffer when it is large. To limit collisions with common\n+        // prefixes and suffixes, we hash the length and some slices of the buffer.\n+        let block_count = self.blocks.len();\n+        if block_count > MAX_BLOCKS_LEN {\n+            // Hash the buffer's length.\n+            block_count.hash(state);\n+\n+            // And its head and tail.\n+            self.blocks[..MAX_BLOCKS_TO_HASH].hash(state);\n+            self.blocks[block_count - MAX_BLOCKS_TO_HASH..].hash(state);\n+        } else {\n+            self.blocks.hash(state);\n+        }\n+\n+        // Hash the other fields as usual.\n+        self.len.hash(state);\n+    }\n+}\n+\n+impl InitMask {\n+    pub const BLOCK_SIZE: u64 = 64;\n+\n+    pub fn new(size: Size, state: bool) -> Self {\n+        let mut m = InitMask { blocks: vec![], len: Size::ZERO };\n+        m.grow(size, state);\n+        m\n+    }\n+\n+    #[inline]\n+    fn bit_index(bits: Size) -> (usize, usize) {\n+        // BLOCK_SIZE is the number of bits that can fit in a `Block`.\n+        // Each bit in a `Block` represents the initialization state of one byte of an allocation,\n+        // so we use `.bytes()` here.\n+        let bits = bits.bytes();\n+        let a = bits / InitMask::BLOCK_SIZE;\n+        let b = bits % InitMask::BLOCK_SIZE;\n+        (usize::try_from(a).unwrap(), usize::try_from(b).unwrap())\n+    }\n+\n+    #[inline]\n+    fn size_from_bit_index(block: impl TryInto<u64>, bit: impl TryInto<u64>) -> Size {\n+        let block = block.try_into().ok().unwrap();\n+        let bit = bit.try_into().ok().unwrap();\n+        Size::from_bytes(block * InitMask::BLOCK_SIZE + bit)\n+    }\n+\n+    /// Checks whether the `range` is entirely initialized.\n+    ///\n+    /// Returns `Ok(())` if it's initialized. Otherwise returns a range of byte\n+    /// indexes for the first contiguous span of the uninitialized access.\n+    #[inline]\n+    pub fn is_range_initialized(&self, range: AllocRange) -> Result<(), AllocRange> {\n+        let end = range.end();\n+        if end > self.len {\n+            return Err(AllocRange::from(self.len..end));\n+        }\n+\n+        let uninit_start = self.find_bit(range.start, end, false);\n+\n+        match uninit_start {\n+            Some(uninit_start) => {\n+                let uninit_end = self.find_bit(uninit_start, end, true).unwrap_or(end);\n+                Err(AllocRange::from(uninit_start..uninit_end))\n+            }\n+            None => Ok(()),\n+        }\n+    }\n+\n+    pub fn set_range(&mut self, range: AllocRange, new_state: bool) {\n+        let end = range.end();\n+        let len = self.len;\n+        if end > len {\n+            self.grow(end - len, new_state);\n+        }\n+        self.set_range_inbounds(range.start, end, new_state);\n+    }\n+\n+    fn set_range_inbounds(&mut self, start: Size, end: Size, new_state: bool) {\n+        let (blocka, bita) = Self::bit_index(start);\n+        let (blockb, bitb) = Self::bit_index(end);\n+        if blocka == blockb {\n+            // First set all bits except the first `bita`,\n+            // then unset the last `64 - bitb` bits.\n+            let range = if bitb == 0 {\n+                u64::MAX << bita\n+            } else {\n+                (u64::MAX << bita) & (u64::MAX >> (64 - bitb))\n+            };\n+            if new_state {\n+                self.blocks[blocka] |= range;\n+            } else {\n+                self.blocks[blocka] &= !range;\n+            }\n+            return;\n+        }\n+        // across block boundaries\n+        if new_state {\n+            // Set `bita..64` to `1`.\n+            self.blocks[blocka] |= u64::MAX << bita;\n+            // Set `0..bitb` to `1`.\n+            if bitb != 0 {\n+                self.blocks[blockb] |= u64::MAX >> (64 - bitb);\n+            }\n+            // Fill in all the other blocks (much faster than one bit at a time).\n+            for block in (blocka + 1)..blockb {\n+                self.blocks[block] = u64::MAX;\n+            }\n+        } else {\n+            // Set `bita..64` to `0`.\n+            self.blocks[blocka] &= !(u64::MAX << bita);\n+            // Set `0..bitb` to `0`.\n+            if bitb != 0 {\n+                self.blocks[blockb] &= !(u64::MAX >> (64 - bitb));\n+            }\n+            // Fill in all the other blocks (much faster than one bit at a time).\n+            for block in (blocka + 1)..blockb {\n+                self.blocks[block] = 0;\n+            }\n+        }\n+    }\n+\n+    #[inline]\n+    fn get(&self, i: Size) -> bool {\n+        let (block, bit) = Self::bit_index(i);\n+        (self.blocks[block] & (1 << bit)) != 0\n+    }\n+\n+    fn grow(&mut self, amount: Size, new_state: bool) {\n+        if amount.bytes() == 0 {\n+            return;\n+        }\n+        let unused_trailing_bits =\n+            u64::try_from(self.blocks.len()).unwrap() * Self::BLOCK_SIZE - self.len.bytes();\n+        if amount.bytes() > unused_trailing_bits {\n+            let additional_blocks = amount.bytes() / Self::BLOCK_SIZE + 1;\n+            self.blocks.extend(\n+                // FIXME(oli-obk): optimize this by repeating `new_state as Block`.\n+                iter::repeat(0).take(usize::try_from(additional_blocks).unwrap()),\n+            );\n+        }\n+        let start = self.len;\n+        self.len += amount;\n+        self.set_range_inbounds(start, start + amount, new_state); // `Size` operation\n+    }\n+\n+    /// Returns the index of the first bit in `start..end` (end-exclusive) that is equal to is_init.\n+    fn find_bit(&self, start: Size, end: Size, is_init: bool) -> Option<Size> {\n+        /// A fast implementation of `find_bit`,\n+        /// which skips over an entire block at a time if it's all 0s (resp. 1s),\n+        /// and finds the first 1 (resp. 0) bit inside a block using `trailing_zeros` instead of a loop.\n+        ///\n+        /// Note that all examples below are written with 8 (instead of 64) bit blocks for simplicity,\n+        /// and with the least significant bit (and lowest block) first:\n+        /// ```text\n+        ///        00000000|00000000\n+        ///        ^      ^ ^      ^\n+        /// index: 0      7 8      15\n+        /// ```\n+        /// Also, if not stated, assume that `is_init = true`, that is, we are searching for the first 1 bit.\n+        fn find_bit_fast(\n+            init_mask: &InitMask,\n+            start: Size,\n+            end: Size,\n+            is_init: bool,\n+        ) -> Option<Size> {\n+            /// Search one block, returning the index of the first bit equal to `is_init`.\n+            fn search_block(\n+                bits: Block,\n+                block: usize,\n+                start_bit: usize,\n+                is_init: bool,\n+            ) -> Option<Size> {\n+                // For the following examples, assume this function was called with:\n+                //   bits = 0b00111011\n+                //   start_bit = 3\n+                //   is_init = false\n+                // Note that, for the examples in this function, the most significant bit is written first,\n+                // which is backwards compared to the comments in `find_bit`/`find_bit_fast`.\n+\n+                // Invert bits so we're always looking for the first set bit.\n+                //        ! 0b00111011\n+                //   bits = 0b11000100\n+                let bits = if is_init { bits } else { !bits };\n+                // Mask off unused start bits.\n+                //          0b11000100\n+                //        & 0b11111000\n+                //   bits = 0b11000000\n+                let bits = bits & (!0 << start_bit);\n+                // Find set bit, if any.\n+                //   bit = trailing_zeros(0b11000000)\n+                //   bit = 6\n+                if bits == 0 {\n+                    None\n+                } else {\n+                    let bit = bits.trailing_zeros();\n+                    Some(InitMask::size_from_bit_index(block, bit))\n+                }\n+            }\n+\n+            if start >= end {\n+                return None;\n+            }\n+\n+            // Convert `start` and `end` to block indexes and bit indexes within each block.\n+            // We must convert `end` to an inclusive bound to handle block boundaries correctly.\n+            //\n+            // For example:\n+            //\n+            //   (a) 00000000|00000000    (b) 00000000|\n+            //       ^~~~~~~~~~~^             ^~~~~~~~~^\n+            //     start       end          start     end\n+            //\n+            // In both cases, the block index of `end` is 1.\n+            // But we do want to search block 1 in (a), and we don't in (b).\n+            //\n+            // We subtract 1 from both end positions to make them inclusive:\n+            //\n+            //   (a) 00000000|00000000    (b) 00000000|\n+            //       ^~~~~~~~~~^              ^~~~~~~^\n+            //     start    end_inclusive   start end_inclusive\n+            //\n+            // For (a), the block index of `end_inclusive` is 1, and for (b), it's 0.\n+            // This provides the desired behavior of searching blocks 0 and 1 for (a),\n+            // and searching only block 0 for (b).\n+            // There is no concern of overflows since we checked for `start >= end` above.\n+            let (start_block, start_bit) = InitMask::bit_index(start);\n+            let end_inclusive = Size::from_bytes(end.bytes() - 1);\n+            let (end_block_inclusive, _) = InitMask::bit_index(end_inclusive);\n+\n+            // Handle first block: need to skip `start_bit` bits.\n+            //\n+            // We need to handle the first block separately,\n+            // because there may be bits earlier in the block that should be ignored,\n+            // such as the bit marked (1) in this example:\n+            //\n+            //       (1)\n+            //       -|------\n+            //   (c) 01000000|00000000|00000001\n+            //          ^~~~~~~~~~~~~~~~~~^\n+            //        start              end\n+            if let Some(i) =\n+                search_block(init_mask.blocks[start_block], start_block, start_bit, is_init)\n+            {\n+                // If the range is less than a block, we may find a matching bit after `end`.\n+                //\n+                // For example, we shouldn't successfully find bit (2), because it's after `end`:\n+                //\n+                //             (2)\n+                //       -------|\n+                //   (d) 00000001|00000000|00000001\n+                //        ^~~~~^\n+                //      start end\n+                //\n+                // An alternative would be to mask off end bits in the same way as we do for start bits,\n+                // but performing this check afterwards is faster and simpler to implement.\n+                if i < end {\n+                    return Some(i);\n+                } else {\n+                    return None;\n+                }\n+            }\n+\n+            // Handle remaining blocks.\n+            //\n+            // We can skip over an entire block at once if it's all 0s (resp. 1s).\n+            // The block marked (3) in this example is the first block that will be handled by this loop,\n+            // and it will be skipped for that reason:\n+            //\n+            //                   (3)\n+            //                --------\n+            //   (e) 01000000|00000000|00000001\n+            //          ^~~~~~~~~~~~~~~~~~^\n+            //        start              end\n+            if start_block < end_block_inclusive {\n+                // This loop is written in a specific way for performance.\n+                // Notably: `..end_block_inclusive + 1` is used for an inclusive range instead of `..=end_block_inclusive`,\n+                // and `.zip(start_block + 1..)` is used to track the index instead of `.enumerate().skip().take()`,\n+                // because both alternatives result in significantly worse codegen.\n+                // `end_block_inclusive + 1` is guaranteed not to wrap, because `end_block_inclusive <= end / BLOCK_SIZE`,\n+                // and `BLOCK_SIZE` (the number of bits per block) will always be at least 8 (1 byte).\n+                for (&bits, block) in init_mask.blocks[start_block + 1..end_block_inclusive + 1]\n+                    .iter()\n+                    .zip(start_block + 1..)\n+                {\n+                    if let Some(i) = search_block(bits, block, 0, is_init) {\n+                        // If this is the last block, we may find a matching bit after `end`.\n+                        //\n+                        // For example, we shouldn't successfully find bit (4), because it's after `end`:\n+                        //\n+                        //                               (4)\n+                        //                         -------|\n+                        //   (f) 00000001|00000000|00000001\n+                        //          ^~~~~~~~~~~~~~~~~~^\n+                        //        start              end\n+                        //\n+                        // As above with example (d), we could handle the end block separately and mask off end bits,\n+                        // but unconditionally searching an entire block at once and performing this check afterwards\n+                        // is faster and much simpler to implement.\n+                        if i < end {\n+                            return Some(i);\n+                        } else {\n+                            return None;\n+                        }\n+                    }\n+                }\n+            }\n+\n+            None\n+        }\n+\n+        #[cfg_attr(not(debug_assertions), allow(dead_code))]\n+        fn find_bit_slow(\n+            init_mask: &InitMask,\n+            start: Size,\n+            end: Size,\n+            is_init: bool,\n+        ) -> Option<Size> {\n+            (start..end).find(|&i| init_mask.get(i) == is_init)\n+        }\n+\n+        let result = find_bit_fast(self, start, end, is_init);\n+\n+        debug_assert_eq!(\n+            result,\n+            find_bit_slow(self, start, end, is_init),\n+            \"optimized implementation of find_bit is wrong for start={:?} end={:?} is_init={} init_mask={:#?}\",\n+            start,\n+            end,\n+            is_init,\n+            self\n+        );\n+\n+        result\n+    }\n+}\n+\n+/// A contiguous chunk of initialized or uninitialized memory.\n+pub enum InitChunk {\n+    Init(Range<Size>),\n+    Uninit(Range<Size>),\n+}\n+\n+impl InitChunk {\n+    #[inline]\n+    pub fn is_init(&self) -> bool {\n+        match self {\n+            Self::Init(_) => true,\n+            Self::Uninit(_) => false,\n+        }\n+    }\n+\n+    #[inline]\n+    pub fn range(&self) -> Range<Size> {\n+        match self {\n+            Self::Init(r) => r.clone(),\n+            Self::Uninit(r) => r.clone(),\n+        }\n+    }\n+}\n+\n+impl InitMask {\n+    /// Returns an iterator, yielding a range of byte indexes for each contiguous region\n+    /// of initialized or uninitialized bytes inside the range `start..end` (end-exclusive).\n+    ///\n+    /// The iterator guarantees the following:\n+    /// - Chunks are nonempty.\n+    /// - Chunks are adjacent (each range's start is equal to the previous range's end).\n+    /// - Chunks span exactly `start..end` (the first starts at `start`, the last ends at `end`).\n+    /// - Chunks alternate between [`InitChunk::Init`] and [`InitChunk::Uninit`].\n+    #[inline]\n+    pub fn range_as_init_chunks(&self, range: AllocRange) -> InitChunkIter<'_> {\n+        let start = range.start;\n+        let end = range.end();\n+        assert!(end <= self.len);\n+\n+        let is_init = if start < end {\n+            self.get(start)\n+        } else {\n+            // `start..end` is empty: there are no chunks, so use some arbitrary value\n+            false\n+        };\n+\n+        InitChunkIter { init_mask: self, is_init, start, end }\n+    }\n+}\n+\n+/// Yields [`InitChunk`]s. See [`InitMask::range_as_init_chunks`].\n+#[derive(Clone)]\n+pub struct InitChunkIter<'a> {\n+    init_mask: &'a InitMask,\n+    /// Whether the next chunk we will return is initialized.\n+    /// If there are no more chunks, contains some arbitrary value.\n+    is_init: bool,\n+    /// The current byte index into `init_mask`.\n+    start: Size,\n+    /// The end byte index into `init_mask`.\n+    end: Size,\n+}\n+\n+impl<'a> Iterator for InitChunkIter<'a> {\n+    type Item = InitChunk;\n+\n+    #[inline]\n+    fn next(&mut self) -> Option<Self::Item> {\n+        if self.start >= self.end {\n+            return None;\n+        }\n+\n+        let end_of_chunk =\n+            self.init_mask.find_bit(self.start, self.end, !self.is_init).unwrap_or(self.end);\n+        let range = self.start..end_of_chunk;\n+\n+        let ret =\n+            Some(if self.is_init { InitChunk::Init(range) } else { InitChunk::Uninit(range) });\n+\n+        self.is_init = !self.is_init;\n+        self.start = end_of_chunk;\n+\n+        ret\n+    }\n+}\n+\n+/// Run-length encoding of the uninit mask.\n+/// Used to copy parts of a mask multiple times to another allocation.\n+pub struct InitCopy {\n+    /// Whether the first range is initialized.\n+    initial: bool,\n+    /// The lengths of ranges that are run-length encoded.\n+    /// The initialization state of the ranges alternate starting with `initial`.\n+    ranges: smallvec::SmallVec<[u64; 1]>,\n+}\n+\n+impl InitCopy {\n+    pub fn no_bytes_init(&self) -> bool {\n+        // The `ranges` are run-length encoded and of alternating initialization state.\n+        // So if `ranges.len() > 1` then the second block is an initialized range.\n+        !self.initial && self.ranges.len() == 1\n+    }\n+}\n+\n+/// Transferring the initialization mask to other allocations.\n+impl InitMask {\n+    /// Creates a run-length encoding of the initialization mask; panics if range is empty.\n+    ///\n+    /// This is essentially a more space-efficient version of\n+    /// `InitMask::range_as_init_chunks(...).collect::<Vec<_>>()`.\n+    pub fn prepare_copy(&self, range: AllocRange) -> InitCopy {\n+        // Since we are copying `size` bytes from `src` to `dest + i * size` (`for i in 0..repeat`),\n+        // a naive initialization mask copying algorithm would repeatedly have to read the initialization mask from\n+        // the source and write it to the destination. Even if we optimized the memory accesses,\n+        // we'd be doing all of this `repeat` times.\n+        // Therefore we precompute a compressed version of the initialization mask of the source value and\n+        // then write it back `repeat` times without computing any more information from the source.\n+\n+        // A precomputed cache for ranges of initialized / uninitialized bits\n+        // 0000010010001110 will become\n+        // `[5, 1, 2, 1, 3, 3, 1]`,\n+        // where each element toggles the state.\n+\n+        let mut ranges = smallvec::SmallVec::<[u64; 1]>::new();\n+\n+        let mut chunks = self.range_as_init_chunks(range).peekable();\n+\n+        let initial = chunks.peek().expect(\"range should be nonempty\").is_init();\n+\n+        // Here we rely on `range_as_init_chunks` to yield alternating init/uninit chunks.\n+        for chunk in chunks {\n+            let len = chunk.range().end.bytes() - chunk.range().start.bytes();\n+            ranges.push(len);\n+        }\n+\n+        InitCopy { ranges, initial }\n+    }\n+\n+    /// Applies multiple instances of the run-length encoding to the initialization mask.\n+    pub fn apply_copy(&mut self, defined: InitCopy, range: AllocRange, repeat: u64) {\n+        // An optimization where we can just overwrite an entire range of initialization\n+        // bits if they are going to be uniformly `1` or `0`.\n+        if defined.ranges.len() <= 1 {\n+            self.set_range_inbounds(\n+                range.start,\n+                range.start + range.size * repeat, // `Size` operations\n+                defined.initial,\n+            );\n+            return;\n+        }\n+\n+        for mut j in 0..repeat {\n+            j *= range.size.bytes();\n+            j += range.start.bytes();\n+            let mut cur = defined.initial;\n+            for range in &defined.ranges {\n+                let old_j = j;\n+                j += range;\n+                self.set_range_inbounds(Size::from_bytes(old_j), Size::from_bytes(j), cur);\n+                cur = !cur;\n+            }\n+        }\n+    }\n+}"}, {"sha": "15bd151bc06818b3fda2d523241f22608ca95d04", "filename": "compiler/rustc_middle/src/mir/interpret/allocation/provenance_map.rs", "status": "modified", "additions": 0, "deletions": 3, "changes": 3, "blob_url": "https://github.com/rust-lang/rust/blob/c3a7ca1125e017ecf5e46685f8ee6feccceec0c7/compiler%2Frustc_middle%2Fsrc%2Fmir%2Finterpret%2Fallocation%2Fprovenance_map.rs", "raw_url": "https://github.com/rust-lang/rust/raw/c3a7ca1125e017ecf5e46685f8ee6feccceec0c7/compiler%2Frustc_middle%2Fsrc%2Fmir%2Finterpret%2Fallocation%2Fprovenance_map.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_middle%2Fsrc%2Fmir%2Finterpret%2Fallocation%2Fprovenance_map.rs?ref=c3a7ca1125e017ecf5e46685f8ee6feccceec0c7", "patch": "@@ -264,9 +264,6 @@ impl<Prov: Provenance> ProvenanceMap<Prov> {\n     /// Applies a provenance copy.\n     /// The affected range, as defined in the parameters to `prepare_copy` is expected\n     /// to be clear of provenance.\n-    ///\n-    /// This is dangerous to use as it can violate internal `Allocation` invariants!\n-    /// It only exists to support an efficient implementation of `mem_copy_repeatedly`.\n     pub fn apply_copy(&mut self, copy: ProvenanceCopy<Prov>) {\n         self.ptrs.insert_presorted(copy.dest_ptrs);\n         self.bytes.insert_presorted(copy.dest_bytes);"}, {"sha": "e247a79339d6786b63225421decfd555c3a9a50b", "filename": "compiler/rustc_middle/src/mir/interpret/mod.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/c3a7ca1125e017ecf5e46685f8ee6feccceec0c7/compiler%2Frustc_middle%2Fsrc%2Fmir%2Finterpret%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/c3a7ca1125e017ecf5e46685f8ee6feccceec0c7/compiler%2Frustc_middle%2Fsrc%2Fmir%2Finterpret%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_middle%2Fsrc%2Fmir%2Finterpret%2Fmod.rs?ref=c3a7ca1125e017ecf5e46685f8ee6feccceec0c7", "patch": "@@ -128,7 +128,7 @@ pub use self::value::{get_slice_bytes, ConstAlloc, ConstValue, Scalar};\n \n pub use self::allocation::{\n     alloc_range, AllocError, AllocRange, AllocResult, Allocation, ConstAllocation, InitChunk,\n-    InitChunkIter, InitMask,\n+    InitChunkIter,\n };\n \n pub use self::pointer::{Pointer, PointerArithmetic, Provenance};"}, {"sha": "72035319b8b45e50090f246fc36ccf93212d1f88", "filename": "compiler/rustc_middle/src/mir/pretty.rs", "status": "modified", "additions": 11, "deletions": 5, "changes": 16, "blob_url": "https://github.com/rust-lang/rust/blob/c3a7ca1125e017ecf5e46685f8ee6feccceec0c7/compiler%2Frustc_middle%2Fsrc%2Fmir%2Fpretty.rs", "raw_url": "https://github.com/rust-lang/rust/raw/c3a7ca1125e017ecf5e46685f8ee6feccceec0c7/compiler%2Frustc_middle%2Fsrc%2Fmir%2Fpretty.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_middle%2Fsrc%2Fmir%2Fpretty.rs?ref=c3a7ca1125e017ecf5e46685f8ee6feccceec0c7", "patch": "@@ -12,8 +12,8 @@ use rustc_data_structures::fx::FxHashMap;\n use rustc_hir::def_id::DefId;\n use rustc_index::vec::Idx;\n use rustc_middle::mir::interpret::{\n-    read_target_uint, AllocId, Allocation, ConstAllocation, ConstValue, GlobalAlloc, Pointer,\n-    Provenance,\n+    alloc_range, read_target_uint, AllocId, Allocation, ConstAllocation, ConstValue, GlobalAlloc,\n+    Pointer, Provenance,\n };\n use rustc_middle::mir::visit::Visitor;\n use rustc_middle::mir::MirSource;\n@@ -884,7 +884,7 @@ fn write_allocation_bytes<'tcx, Prov: Provenance, Extra>(\n         }\n         if let Some(prov) = alloc.provenance().get_ptr(i) {\n             // Memory with provenance must be defined\n-            assert!(alloc.init_mask().is_range_initialized(i, i + ptr_size).is_ok());\n+            assert!(alloc.init_mask().is_range_initialized(alloc_range(i, ptr_size)).is_ok());\n             let j = i.bytes_usize();\n             let offset = alloc\n                 .inspect_with_uninit_and_ptr_outside_interpreter(j..j + ptr_size.bytes_usize());\n@@ -943,15 +943,21 @@ fn write_allocation_bytes<'tcx, Prov: Provenance, Extra>(\n             }\n         } else if let Some(prov) = alloc.provenance().get(i, &tcx) {\n             // Memory with provenance must be defined\n-            assert!(alloc.init_mask().is_range_initialized(i, i + Size::from_bytes(1)).is_ok());\n+            assert!(\n+                alloc.init_mask().is_range_initialized(alloc_range(i, Size::from_bytes(1))).is_ok()\n+            );\n             ascii.push('\u2501'); // HEAVY HORIZONTAL\n             // We have two characters to display this, which is obviously not enough.\n             // Format is similar to \"oversized\" above.\n             let j = i.bytes_usize();\n             let c = alloc.inspect_with_uninit_and_ptr_outside_interpreter(j..j + 1)[0];\n             write!(w, \"\u257e{:02x}{:#?} (1 ptr byte)\u257c\", c, prov)?;\n             i += Size::from_bytes(1);\n-        } else if alloc.init_mask().is_range_initialized(i, i + Size::from_bytes(1)).is_ok() {\n+        } else if alloc\n+            .init_mask()\n+            .is_range_initialized(alloc_range(i, Size::from_bytes(1)))\n+            .is_ok()\n+        {\n             let j = i.bytes_usize();\n \n             // Checked definedness (and thus range) and provenance. This access also doesn't"}]}