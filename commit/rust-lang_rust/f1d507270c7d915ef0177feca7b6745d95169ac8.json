{"sha": "f1d507270c7d915ef0177feca7b6745d95169ac8", "node_id": "MDY6Q29tbWl0NzI0NzEyOmYxZDUwNzI3MGM3ZDkxNWVmMDE3N2ZlY2E3YjY3NDVkOTUxNjlhYzg=", "commit": {"author": {"name": "bors[bot]", "email": "26634292+bors[bot]@users.noreply.github.com", "date": "2020-08-06T01:44:38Z"}, "committer": {"name": "GitHub", "email": "noreply@github.com", "date": "2020-08-06T01:44:38Z"}, "message": "Merge #5526\n\n5526: Handle semantic token deltas r=kjeremy a=kjeremy\n\nThis basically takes the naive approach where we always compute the tokens but save space sending over the wire which apparently solves some GC problems with vscode.\r\n\r\nThis is waiting for https://github.com/gluon-lang/lsp-types/pull/174 to be merged. I am also unsure of the best way to stash the tokens into `DocumentData` in a safe manner.\n\nCo-authored-by: kjeremy <kjeremy@gmail.com>\nCo-authored-by: Jeremy Kolb <kjeremy@gmail.com>", "tree": {"sha": "f2c039eba062235b458686151441530051723d5f", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/f2c039eba062235b458686151441530051723d5f"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/f1d507270c7d915ef0177feca7b6745d95169ac8", "comment_count": 0, "verification": {"verified": true, "reason": "valid", "signature": "-----BEGIN PGP SIGNATURE-----\n\nwsBcBAABCAAQBQJfK2CGCRBK7hj4Ov3rIwAAdHIIAFlh944OprWd/okIu7x9duBs\nzKNcrkqVyqHs4iZ+QELRVCSVQvMmlYr4+LR8oiDN2DVD2M0lUPIPgp1oit72K1z1\nbrGX92SVZPM8yPmFcDUcBn5rF6MxCbTrWlm+cyxxJXBePgF2kUVhCHXtQ2JhcOtJ\nhtLj1ict68H38elMD+mYWx8D2KwXBgeZrrp3oqE4eOq35/5629BjJbk2m14Hv/wC\nMVHj5Htj7uWk/m31mxJ5JnVPhL4gBanv4HBsnG13yDuixaA3++Gxgp50RxYf6s3T\nQkRIThNQGdPyj+wapPBqKDkFr/q67mFQRzpsaYjgC3xWDEc1hvwU2JbgTksGl0M=\n=LiBL\n-----END PGP SIGNATURE-----\n", "payload": "tree f2c039eba062235b458686151441530051723d5f\nparent 2cb079ba9add594908f40d48ee2c9ac553306b33\nparent 195111d7698c39fb4d653da3a39a8cb52c9260e4\nauthor bors[bot] <26634292+bors[bot]@users.noreply.github.com> 1596678278 +0000\ncommitter GitHub <noreply@github.com> 1596678278 +0000\n\nMerge #5526\n\n5526: Handle semantic token deltas r=kjeremy a=kjeremy\n\nThis basically takes the naive approach where we always compute the tokens but save space sending over the wire which apparently solves some GC problems with vscode.\r\n\r\nThis is waiting for https://github.com/gluon-lang/lsp-types/pull/174 to be merged. I am also unsure of the best way to stash the tokens into `DocumentData` in a safe manner.\n\nCo-authored-by: kjeremy <kjeremy@gmail.com>\nCo-authored-by: Jeremy Kolb <kjeremy@gmail.com>\n"}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/f1d507270c7d915ef0177feca7b6745d95169ac8", "html_url": "https://github.com/rust-lang/rust/commit/f1d507270c7d915ef0177feca7b6745d95169ac8", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/f1d507270c7d915ef0177feca7b6745d95169ac8/comments", "author": {"login": "bors[bot]", "id": 26634292, "node_id": "MDM6Qm90MjY2MzQyOTI=", "avatar_url": "https://avatars.githubusercontent.com/in/1847?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bors%5Bbot%5D", "html_url": "https://github.com/apps/bors", "followers_url": "https://api.github.com/users/bors%5Bbot%5D/followers", "following_url": "https://api.github.com/users/bors%5Bbot%5D/following{/other_user}", "gists_url": "https://api.github.com/users/bors%5Bbot%5D/gists{/gist_id}", "starred_url": "https://api.github.com/users/bors%5Bbot%5D/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bors%5Bbot%5D/subscriptions", "organizations_url": "https://api.github.com/users/bors%5Bbot%5D/orgs", "repos_url": "https://api.github.com/users/bors%5Bbot%5D/repos", "events_url": "https://api.github.com/users/bors%5Bbot%5D/events{/privacy}", "received_events_url": "https://api.github.com/users/bors%5Bbot%5D/received_events", "type": "Bot", "site_admin": false}, "committer": {"login": "web-flow", "id": 19864447, "node_id": "MDQ6VXNlcjE5ODY0NDQ3", "avatar_url": "https://avatars.githubusercontent.com/u/19864447?v=4", "gravatar_id": "", "url": "https://api.github.com/users/web-flow", "html_url": "https://github.com/web-flow", "followers_url": "https://api.github.com/users/web-flow/followers", "following_url": "https://api.github.com/users/web-flow/following{/other_user}", "gists_url": "https://api.github.com/users/web-flow/gists{/gist_id}", "starred_url": "https://api.github.com/users/web-flow/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/web-flow/subscriptions", "organizations_url": "https://api.github.com/users/web-flow/orgs", "repos_url": "https://api.github.com/users/web-flow/repos", "events_url": "https://api.github.com/users/web-flow/events{/privacy}", "received_events_url": "https://api.github.com/users/web-flow/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "2cb079ba9add594908f40d48ee2c9ac553306b33", "url": "https://api.github.com/repos/rust-lang/rust/commits/2cb079ba9add594908f40d48ee2c9ac553306b33", "html_url": "https://github.com/rust-lang/rust/commit/2cb079ba9add594908f40d48ee2c9ac553306b33"}, {"sha": "195111d7698c39fb4d653da3a39a8cb52c9260e4", "url": "https://api.github.com/repos/rust-lang/rust/commits/195111d7698c39fb4d653da3a39a8cb52c9260e4", "html_url": "https://github.com/rust-lang/rust/commit/195111d7698c39fb4d653da3a39a8cb52c9260e4"}], "stats": {"total": 222, "additions": 208, "deletions": 14}, "files": [{"sha": "92a743fd8e7b5f028f8707b7c49f237e9c943907", "filename": "crates/rust-analyzer/src/caps.rs", "status": "modified", "additions": 3, "deletions": 1, "changes": 4, "blob_url": "https://github.com/rust-lang/rust/blob/f1d507270c7d915ef0177feca7b6745d95169ac8/crates%2Frust-analyzer%2Fsrc%2Fcaps.rs", "raw_url": "https://github.com/rust-lang/rust/raw/f1d507270c7d915ef0177feca7b6745d95169ac8/crates%2Frust-analyzer%2Fsrc%2Fcaps.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Frust-analyzer%2Fsrc%2Fcaps.rs?ref=f1d507270c7d915ef0177feca7b6745d95169ac8", "patch": "@@ -76,7 +76,9 @@ pub fn server_capabilities(client_caps: &ClientCapabilities) -> ServerCapabiliti\n                     token_modifiers: semantic_tokens::SUPPORTED_MODIFIERS.to_vec(),\n                 },\n \n-                document_provider: Some(SemanticTokensDocumentProvider::Bool(true)),\n+                document_provider: Some(SemanticTokensDocumentProvider::Edits {\n+                    edits: Some(true),\n+                }),\n                 range_provider: Some(true),\n                 work_done_progress_options: Default::default(),\n             }"}, {"sha": "e882c9865c0b0daa3a0c78c2783df284b8fb47ab", "filename": "crates/rust-analyzer/src/document.rs", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "blob_url": "https://github.com/rust-lang/rust/blob/f1d507270c7d915ef0177feca7b6745d95169ac8/crates%2Frust-analyzer%2Fsrc%2Fdocument.rs", "raw_url": "https://github.com/rust-lang/rust/raw/f1d507270c7d915ef0177feca7b6745d95169ac8/crates%2Frust-analyzer%2Fsrc%2Fdocument.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Frust-analyzer%2Fsrc%2Fdocument.rs?ref=f1d507270c7d915ef0177feca7b6745d95169ac8", "patch": "@@ -1,9 +1,9 @@\n //! In-memory document information.\n \n /// Information about a document that the Language Client\n-// knows about.\n-// Its lifetime is driven by the textDocument/didOpen and textDocument/didClose\n-// client notifications.\n+/// knows about.\n+/// Its lifetime is driven by the textDocument/didOpen and textDocument/didClose\n+/// client notifications.\n #[derive(Debug, Clone)]\n pub(crate) struct DocumentData {\n     pub version: Option<i64>,"}, {"sha": "0e592ac1bea4eb679909f59075986e459647e723", "filename": "crates/rust-analyzer/src/global_state.rs", "status": "modified", "additions": 6, "deletions": 2, "changes": 8, "blob_url": "https://github.com/rust-lang/rust/blob/f1d507270c7d915ef0177feca7b6745d95169ac8/crates%2Frust-analyzer%2Fsrc%2Fglobal_state.rs", "raw_url": "https://github.com/rust-lang/rust/raw/f1d507270c7d915ef0177feca7b6745d95169ac8/crates%2Frust-analyzer%2Fsrc%2Fglobal_state.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Frust-analyzer%2Fsrc%2Fglobal_state.rs?ref=f1d507270c7d915ef0177feca7b6745d95169ac8", "patch": "@@ -7,8 +7,8 @@ use std::{sync::Arc, time::Instant};\n \n use crossbeam_channel::{unbounded, Receiver, Sender};\n use flycheck::FlycheckHandle;\n-use lsp_types::Url;\n-use parking_lot::RwLock;\n+use lsp_types::{SemanticTokens, Url};\n+use parking_lot::{Mutex, RwLock};\n use ra_db::{CrateId, VfsPath};\n use ra_ide::{Analysis, AnalysisChange, AnalysisHost, FileId};\n use ra_project_model::{CargoWorkspace, ProcMacroClient, ProjectWorkspace, Target};\n@@ -71,6 +71,7 @@ pub(crate) struct GlobalState {\n     pub(crate) analysis_host: AnalysisHost,\n     pub(crate) diagnostics: DiagnosticCollection,\n     pub(crate) mem_docs: FxHashMap<VfsPath, DocumentData>,\n+    pub(crate) semantic_tokens_cache: Arc<Mutex<FxHashMap<Url, SemanticTokens>>>,\n     pub(crate) vfs: Arc<RwLock<(vfs::Vfs, FxHashMap<FileId, LineEndings>)>>,\n     pub(crate) status: Status,\n     pub(crate) source_root_config: SourceRootConfig,\n@@ -86,6 +87,7 @@ pub(crate) struct GlobalStateSnapshot {\n     pub(crate) check_fixes: CheckFixes,\n     pub(crate) latest_requests: Arc<RwLock<LatestRequests>>,\n     mem_docs: FxHashMap<VfsPath, DocumentData>,\n+    pub semantic_tokens_cache: Arc<Mutex<FxHashMap<Url, SemanticTokens>>>,\n     vfs: Arc<RwLock<(vfs::Vfs, FxHashMap<FileId, LineEndings>)>>,\n     pub(crate) workspaces: Arc<Vec<ProjectWorkspace>>,\n }\n@@ -120,6 +122,7 @@ impl GlobalState {\n             analysis_host,\n             diagnostics: Default::default(),\n             mem_docs: FxHashMap::default(),\n+            semantic_tokens_cache: Arc::new(Default::default()),\n             vfs: Arc::new(RwLock::new((vfs::Vfs::default(), FxHashMap::default()))),\n             status: Status::default(),\n             source_root_config: SourceRootConfig::default(),\n@@ -186,6 +189,7 @@ impl GlobalState {\n             latest_requests: Arc::clone(&self.latest_requests),\n             check_fixes: Arc::clone(&self.diagnostics.check_fixes),\n             mem_docs: self.mem_docs.clone(),\n+            semantic_tokens_cache: Arc::clone(&self.semantic_tokens_cache),\n         }\n     }\n "}, {"sha": "067259e246723e6c99e3532b2b26a1102a9a5290", "filename": "crates/rust-analyzer/src/handlers.rs", "status": "modified", "additions": 38, "deletions": 3, "changes": 41, "blob_url": "https://github.com/rust-lang/rust/blob/f1d507270c7d915ef0177feca7b6745d95169ac8/crates%2Frust-analyzer%2Fsrc%2Fhandlers.rs", "raw_url": "https://github.com/rust-lang/rust/raw/f1d507270c7d915ef0177feca7b6745d95169ac8/crates%2Frust-analyzer%2Fsrc%2Fhandlers.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Frust-analyzer%2Fsrc%2Fhandlers.rs?ref=f1d507270c7d915ef0177feca7b6745d95169ac8", "patch": "@@ -13,9 +13,10 @@ use lsp_types::{\n     CallHierarchyOutgoingCall, CallHierarchyOutgoingCallsParams, CallHierarchyPrepareParams,\n     CodeActionKind, CodeLens, Command, CompletionItem, Diagnostic, DocumentFormattingParams,\n     DocumentHighlight, DocumentSymbol, FoldingRange, FoldingRangeParams, HoverContents, Location,\n-    Position, PrepareRenameResponse, Range, RenameParams, SemanticTokensParams,\n-    SemanticTokensRangeParams, SemanticTokensRangeResult, SemanticTokensResult, SymbolInformation,\n-    SymbolTag, TextDocumentIdentifier, Url, WorkspaceEdit,\n+    Position, PrepareRenameResponse, Range, RenameParams, SemanticTokensEditResult,\n+    SemanticTokensEditsParams, SemanticTokensParams, SemanticTokensRangeParams,\n+    SemanticTokensRangeResult, SemanticTokensResult, SymbolInformation, SymbolTag,\n+    TextDocumentIdentifier, Url, WorkspaceEdit,\n };\n use ra_ide::{\n     FileId, FilePosition, FileRange, HoverAction, HoverGotoTypeData, NavigationTarget, Query,\n@@ -1179,6 +1180,40 @@ pub(crate) fn handle_semantic_tokens(\n \n     let highlights = snap.analysis.highlight(file_id)?;\n     let semantic_tokens = to_proto::semantic_tokens(&text, &line_index, highlights);\n+\n+    // Unconditionally cache the tokens\n+    snap.semantic_tokens_cache.lock().insert(params.text_document.uri, semantic_tokens.clone());\n+\n+    Ok(Some(semantic_tokens.into()))\n+}\n+\n+pub(crate) fn handle_semantic_tokens_edits(\n+    snap: GlobalStateSnapshot,\n+    params: SemanticTokensEditsParams,\n+) -> Result<Option<SemanticTokensEditResult>> {\n+    let _p = profile(\"handle_semantic_tokens_edits\");\n+\n+    let file_id = from_proto::file_id(&snap, &params.text_document.uri)?;\n+    let text = snap.analysis.file_text(file_id)?;\n+    let line_index = snap.analysis.file_line_index(file_id)?;\n+\n+    let highlights = snap.analysis.highlight(file_id)?;\n+\n+    let semantic_tokens = to_proto::semantic_tokens(&text, &line_index, highlights);\n+\n+    let mut cache = snap.semantic_tokens_cache.lock();\n+    let cached_tokens = cache.entry(params.text_document.uri).or_default();\n+\n+    if let Some(prev_id) = &cached_tokens.result_id {\n+        if *prev_id == params.previous_result_id {\n+            let edits = to_proto::semantic_token_edits(&cached_tokens, &semantic_tokens);\n+            *cached_tokens = semantic_tokens;\n+            return Ok(Some(edits.into()));\n+        }\n+    }\n+\n+    *cached_tokens = semantic_tokens.clone();\n+\n     Ok(Some(semantic_tokens.into()))\n }\n "}, {"sha": "ceddb2b056305dfbb6913a33b9ddf9bae812f183", "filename": "crates/rust-analyzer/src/main_loop.rs", "status": "modified", "additions": 5, "deletions": 0, "changes": 5, "blob_url": "https://github.com/rust-lang/rust/blob/f1d507270c7d915ef0177feca7b6745d95169ac8/crates%2Frust-analyzer%2Fsrc%2Fmain_loop.rs", "raw_url": "https://github.com/rust-lang/rust/raw/f1d507270c7d915ef0177feca7b6745d95169ac8/crates%2Frust-analyzer%2Fsrc%2Fmain_loop.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Frust-analyzer%2Fsrc%2Fmain_loop.rs?ref=f1d507270c7d915ef0177feca7b6745d95169ac8", "patch": "@@ -386,6 +386,9 @@ impl GlobalState {\n                 handlers::handle_call_hierarchy_outgoing,\n             )?\n             .on::<lsp_types::request::SemanticTokensRequest>(handlers::handle_semantic_tokens)?\n+            .on::<lsp_types::request::SemanticTokensEditsRequest>(\n+                handlers::handle_semantic_tokens_edits,\n+            )?\n             .on::<lsp_types::request::SemanticTokensRangeRequest>(\n                 handlers::handle_semantic_tokens_range,\n             )?\n@@ -443,6 +446,8 @@ impl GlobalState {\n                         None => log::error!(\"orphan DidCloseTextDocument: {}\", path),\n                     }\n \n+                    this.semantic_tokens_cache.lock().remove(&params.text_document.uri);\n+\n                     if let Some(path) = path.as_path() {\n                         this.loader.handle.invalidate(path.to_path_buf());\n                     }"}, {"sha": "afc38fb4e8683de50c1481f9997958731eb8e966", "filename": "crates/rust-analyzer/src/semantic_tokens.rs", "status": "modified", "additions": 136, "deletions": 3, "changes": 139, "blob_url": "https://github.com/rust-lang/rust/blob/f1d507270c7d915ef0177feca7b6745d95169ac8/crates%2Frust-analyzer%2Fsrc%2Fsemantic_tokens.rs", "raw_url": "https://github.com/rust-lang/rust/raw/f1d507270c7d915ef0177feca7b6745d95169ac8/crates%2Frust-analyzer%2Fsrc%2Fsemantic_tokens.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Frust-analyzer%2Fsrc%2Fsemantic_tokens.rs?ref=f1d507270c7d915ef0177feca7b6745d95169ac8", "patch": "@@ -2,7 +2,10 @@\n \n use std::ops;\n \n-use lsp_types::{Range, SemanticToken, SemanticTokenModifier, SemanticTokenType, SemanticTokens};\n+use lsp_types::{\n+    Range, SemanticToken, SemanticTokenModifier, SemanticTokenType, SemanticTokens,\n+    SemanticTokensEdit,\n+};\n \n macro_rules! define_semantic_token_types {\n     ($(($ident:ident, $string:literal)),*$(,)?) => {\n@@ -89,14 +92,18 @@ impl ops::BitOrAssign<SemanticTokenModifier> for ModifierSet {\n /// Tokens are encoded relative to each other.\n ///\n /// This is a direct port of https://github.com/microsoft/vscode-languageserver-node/blob/f425af9de46a0187adb78ec8a46b9b2ce80c5412/server/src/sematicTokens.proposed.ts#L45\n-#[derive(Default)]\n pub(crate) struct SemanticTokensBuilder {\n+    id: String,\n     prev_line: u32,\n     prev_char: u32,\n     data: Vec<SemanticToken>,\n }\n \n impl SemanticTokensBuilder {\n+    pub fn new(id: String) -> Self {\n+        SemanticTokensBuilder { id, prev_line: 0, prev_char: 0, data: Default::default() }\n+    }\n+\n     /// Push a new token onto the builder\n     pub fn push(&mut self, range: Range, token_index: u32, modifier_bitset: u32) {\n         let mut push_line = range.start.line as u32;\n@@ -127,10 +134,136 @@ impl SemanticTokensBuilder {\n     }\n \n     pub fn build(self) -> SemanticTokens {\n-        SemanticTokens { result_id: None, data: self.data }\n+        SemanticTokens { result_id: Some(self.id), data: self.data }\n+    }\n+}\n+\n+pub fn diff_tokens(old: &[SemanticToken], new: &[SemanticToken]) -> Vec<SemanticTokensEdit> {\n+    let offset = new.iter().zip(old.iter()).take_while(|&(n, p)| n == p).count();\n+\n+    let (_, old) = old.split_at(offset);\n+    let (_, new) = new.split_at(offset);\n+\n+    let offset_from_end =\n+        new.iter().rev().zip(old.iter().rev()).take_while(|&(n, p)| n == p).count();\n+\n+    let (old, _) = old.split_at(old.len() - offset_from_end);\n+    let (new, _) = new.split_at(new.len() - offset_from_end);\n+\n+    if old.is_empty() && new.is_empty() {\n+        vec![]\n+    } else {\n+        // The lsp data field is actually a byte-diff but we\n+        // travel in tokens so `start` and `delete_count` are in multiples of the\n+        // serialized size of `SemanticToken`.\n+        vec![SemanticTokensEdit {\n+            start: 5 * offset as u32,\n+            delete_count: 5 * old.len() as u32,\n+            data: Some(new.into()),\n+        }]\n     }\n }\n \n pub fn type_index(type_: SemanticTokenType) -> u32 {\n     SUPPORTED_TYPES.iter().position(|it| *it == type_).unwrap() as u32\n }\n+\n+#[cfg(test)]\n+mod tests {\n+    use super::*;\n+\n+    fn from(t: (u32, u32, u32, u32, u32)) -> SemanticToken {\n+        SemanticToken {\n+            delta_line: t.0,\n+            delta_start: t.1,\n+            length: t.2,\n+            token_type: t.3,\n+            token_modifiers_bitset: t.4,\n+        }\n+    }\n+\n+    #[test]\n+    fn test_diff_insert_at_end() {\n+        let before = [from((1, 2, 3, 4, 5)), from((6, 7, 8, 9, 10))];\n+        let after = [from((1, 2, 3, 4, 5)), from((6, 7, 8, 9, 10)), from((11, 12, 13, 14, 15))];\n+\n+        let edits = diff_tokens(&before, &after);\n+        assert_eq!(\n+            edits[0],\n+            SemanticTokensEdit {\n+                start: 10,\n+                delete_count: 0,\n+                data: Some(vec![from((11, 12, 13, 14, 15))])\n+            }\n+        );\n+    }\n+\n+    #[test]\n+    fn test_diff_insert_at_beginning() {\n+        let before = [from((1, 2, 3, 4, 5)), from((6, 7, 8, 9, 10))];\n+        let after = [from((11, 12, 13, 14, 15)), from((1, 2, 3, 4, 5)), from((6, 7, 8, 9, 10))];\n+\n+        let edits = diff_tokens(&before, &after);\n+        assert_eq!(\n+            edits[0],\n+            SemanticTokensEdit {\n+                start: 0,\n+                delete_count: 0,\n+                data: Some(vec![from((11, 12, 13, 14, 15))])\n+            }\n+        );\n+    }\n+\n+    #[test]\n+    fn test_diff_insert_in_middle() {\n+        let before = [from((1, 2, 3, 4, 5)), from((6, 7, 8, 9, 10))];\n+        let after = [\n+            from((1, 2, 3, 4, 5)),\n+            from((10, 20, 30, 40, 50)),\n+            from((60, 70, 80, 90, 100)),\n+            from((6, 7, 8, 9, 10)),\n+        ];\n+\n+        let edits = diff_tokens(&before, &after);\n+        assert_eq!(\n+            edits[0],\n+            SemanticTokensEdit {\n+                start: 5,\n+                delete_count: 0,\n+                data: Some(vec![from((10, 20, 30, 40, 50)), from((60, 70, 80, 90, 100))])\n+            }\n+        );\n+    }\n+\n+    #[test]\n+    fn test_diff_remove_from_end() {\n+        let before = [from((1, 2, 3, 4, 5)), from((6, 7, 8, 9, 10)), from((11, 12, 13, 14, 15))];\n+        let after = [from((1, 2, 3, 4, 5)), from((6, 7, 8, 9, 10))];\n+\n+        let edits = diff_tokens(&before, &after);\n+        assert_eq!(edits[0], SemanticTokensEdit { start: 10, delete_count: 5, data: Some(vec![]) });\n+    }\n+\n+    #[test]\n+    fn test_diff_remove_from_beginning() {\n+        let before = [from((11, 12, 13, 14, 15)), from((1, 2, 3, 4, 5)), from((6, 7, 8, 9, 10))];\n+        let after = [from((1, 2, 3, 4, 5)), from((6, 7, 8, 9, 10))];\n+\n+        let edits = diff_tokens(&before, &after);\n+        assert_eq!(edits[0], SemanticTokensEdit { start: 0, delete_count: 5, data: Some(vec![]) });\n+    }\n+\n+    #[test]\n+    fn test_diff_remove_from_middle() {\n+        let before = [\n+            from((1, 2, 3, 4, 5)),\n+            from((10, 20, 30, 40, 50)),\n+            from((60, 70, 80, 90, 100)),\n+            from((6, 7, 8, 9, 10)),\n+        ];\n+        let after = [from((1, 2, 3, 4, 5)), from((6, 7, 8, 9, 10))];\n+\n+        let edits = diff_tokens(&before, &after);\n+        assert_eq!(edits[0], SemanticTokensEdit { start: 5, delete_count: 10, data: Some(vec![]) });\n+    }\n+}"}, {"sha": "5eba1f15553450ef2570fd3fc9e42287f8577f40", "filename": "crates/rust-analyzer/src/to_proto.rs", "status": "modified", "additions": 17, "deletions": 2, "changes": 19, "blob_url": "https://github.com/rust-lang/rust/blob/f1d507270c7d915ef0177feca7b6745d95169ac8/crates%2Frust-analyzer%2Fsrc%2Fto_proto.rs", "raw_url": "https://github.com/rust-lang/rust/raw/f1d507270c7d915ef0177feca7b6745d95169ac8/crates%2Frust-analyzer%2Fsrc%2Fto_proto.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Frust-analyzer%2Fsrc%2Fto_proto.rs?ref=f1d507270c7d915ef0177feca7b6745d95169ac8", "patch": "@@ -1,5 +1,8 @@\n //! Conversion of rust-analyzer specific types to lsp_types equivalents.\n-use std::path::{self, Path};\n+use std::{\n+    path::{self, Path},\n+    sync::atomic::{AtomicU32, Ordering},\n+};\n \n use itertools::Itertools;\n use ra_db::{FileId, FileRange};\n@@ -303,12 +306,15 @@ pub(crate) fn inlay_int(line_index: &LineIndex, inlay_hint: InlayHint) -> lsp_ex\n     }\n }\n \n+static TOKEN_RESULT_COUNTER: AtomicU32 = AtomicU32::new(1);\n+\n pub(crate) fn semantic_tokens(\n     text: &str,\n     line_index: &LineIndex,\n     highlights: Vec<HighlightedRange>,\n ) -> lsp_types::SemanticTokens {\n-    let mut builder = semantic_tokens::SemanticTokensBuilder::default();\n+    let id = TOKEN_RESULT_COUNTER.fetch_add(1, Ordering::SeqCst).to_string();\n+    let mut builder = semantic_tokens::SemanticTokensBuilder::new(id);\n \n     for highlight_range in highlights {\n         let (type_, mods) = semantic_token_type_and_modifiers(highlight_range.highlight);\n@@ -328,6 +334,15 @@ pub(crate) fn semantic_tokens(\n     builder.build()\n }\n \n+pub(crate) fn semantic_token_edits(\n+    previous: &lsp_types::SemanticTokens,\n+    current: &lsp_types::SemanticTokens,\n+) -> lsp_types::SemanticTokensEdits {\n+    let result_id = current.result_id.clone();\n+    let edits = semantic_tokens::diff_tokens(&previous.data, &current.data);\n+    lsp_types::SemanticTokensEdits { result_id, edits }\n+}\n+\n fn semantic_token_type_and_modifiers(\n     highlight: Highlight,\n ) -> (lsp_types::SemanticTokenType, semantic_tokens::ModifierSet) {"}]}