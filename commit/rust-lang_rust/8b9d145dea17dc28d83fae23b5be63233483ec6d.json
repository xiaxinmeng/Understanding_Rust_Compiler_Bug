{"sha": "8b9d145dea17dc28d83fae23b5be63233483ec6d", "node_id": "C_kwDOAAsO6NoAKDhiOWQxNDVkZWExN2RjMjhkODNmYWUyM2I1YmU2MzIzMzQ4M2VjNmQ", "commit": {"author": {"name": "Aleksey Kladov", "email": "aleksey.kladov@gmail.com", "date": "2021-12-18T12:31:50Z"}, "committer": {"name": "Aleksey Kladov", "email": "aleksey.kladov@gmail.com", "date": "2021-12-18T12:31:50Z"}, "message": "soa all the things", "tree": {"sha": "74ffd24cab3a46f9289cb546a1f5191fe5aed6bf", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/74ffd24cab3a46f9289cb546a1f5191fe5aed6bf"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/8b9d145dea17dc28d83fae23b5be63233483ec6d", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/8b9d145dea17dc28d83fae23b5be63233483ec6d", "html_url": "https://github.com/rust-lang/rust/commit/8b9d145dea17dc28d83fae23b5be63233483ec6d", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/8b9d145dea17dc28d83fae23b5be63233483ec6d/comments", "author": {"login": "matklad", "id": 1711539, "node_id": "MDQ6VXNlcjE3MTE1Mzk=", "avatar_url": "https://avatars.githubusercontent.com/u/1711539?v=4", "gravatar_id": "", "url": "https://api.github.com/users/matklad", "html_url": "https://github.com/matklad", "followers_url": "https://api.github.com/users/matklad/followers", "following_url": "https://api.github.com/users/matklad/following{/other_user}", "gists_url": "https://api.github.com/users/matklad/gists{/gist_id}", "starred_url": "https://api.github.com/users/matklad/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/matklad/subscriptions", "organizations_url": "https://api.github.com/users/matklad/orgs", "repos_url": "https://api.github.com/users/matklad/repos", "events_url": "https://api.github.com/users/matklad/events{/privacy}", "received_events_url": "https://api.github.com/users/matklad/received_events", "type": "User", "site_admin": false}, "committer": {"login": "matklad", "id": 1711539, "node_id": "MDQ6VXNlcjE3MTE1Mzk=", "avatar_url": "https://avatars.githubusercontent.com/u/1711539?v=4", "gravatar_id": "", "url": "https://api.github.com/users/matklad", "html_url": "https://github.com/matklad", "followers_url": "https://api.github.com/users/matklad/followers", "following_url": "https://api.github.com/users/matklad/following{/other_user}", "gists_url": "https://api.github.com/users/matklad/gists{/gist_id}", "starred_url": "https://api.github.com/users/matklad/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/matklad/subscriptions", "organizations_url": "https://api.github.com/users/matklad/orgs", "repos_url": "https://api.github.com/users/matklad/repos", "events_url": "https://api.github.com/users/matklad/events{/privacy}", "received_events_url": "https://api.github.com/users/matklad/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "799941e05ee3da6c4f302adb8962a0f15152949b", "url": "https://api.github.com/repos/rust-lang/rust/commits/799941e05ee3da6c4f302adb8962a0f15152949b", "html_url": "https://github.com/rust-lang/rust/commit/799941e05ee3da6c4f302adb8962a0f15152949b"}], "stats": {"total": 109, "additions": 75, "deletions": 34}, "files": [{"sha": "595b60722938f30a76377a42db1b8160e54208f3", "filename": "crates/parser/src/lexed_str.rs", "status": "renamed", "additions": 61, "deletions": 24, "changes": 85, "blob_url": "https://github.com/rust-lang/rust/blob/8b9d145dea17dc28d83fae23b5be63233483ec6d/crates%2Fparser%2Fsrc%2Flexed_str.rs", "raw_url": "https://github.com/rust-lang/rust/raw/8b9d145dea17dc28d83fae23b5be63233483ec6d/crates%2Fparser%2Fsrc%2Flexed_str.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fparser%2Fsrc%2Flexed_str.rs?ref=8b9d145dea17dc28d83fae23b5be63233483ec6d", "patch": "@@ -4,48 +4,55 @@\n //! on tokens which originated from text. Macros, eg, can synthesize tokes out\n //! of thin air. So, ideally, lexer should be an orthogonal crate. It is however\n //! convenient to include a text-based lexer here!\n+//!\n+//! Note that these tokens, unlike the tokens we feed into the parser, do\n+//! include info about comments and whitespace. \n \n use crate::{\n     SyntaxKind::{self, *},\n     T,\n };\n \n-#[derive(Debug, Clone, PartialEq, Eq, PartialOrd, Ord, Hash)]\n-pub struct LexerToken {\n-    pub kind: SyntaxKind,\n-    pub len: usize,\n-    pub error: Option<String>,\n+pub struct LexedStr<'a> {\n+    text: &'a str,\n+    kind: Vec<SyntaxKind>,\n+    start: Vec<u32>,\n+    error: Vec<LexError>,\n }\n \n-impl LexerToken {\n-    pub fn new(kind: SyntaxKind, len: usize) -> Self {\n-        Self { kind, len, error: None }\n-    }\n+struct LexError {\n+    msg: String,\n+    token: u32,\n+}\n \n-    /// Lexes text as a sequence of tokens.\n-    pub fn tokenize(text: &str) -> Vec<LexerToken> {\n-        let mut res = Vec::new();\n-        let mut offset = 0;\n+impl<'a> LexedStr<'a> {\n+    pub fn new(text: &'a str) -> LexedStr<'a> {\n+        let mut res = LexedStr { text, kind: Vec::new(), start: Vec::new(), error: Vec::new() };\n \n+        let mut offset = 0;\n         if let Some(shebang_len) = rustc_lexer::strip_shebang(text) {\n-            res.push(LexerToken::new(SHEBANG, shebang_len));\n+            res.push(SHEBANG, offset);\n             offset = shebang_len\n         };\n-\n         for token in rustc_lexer::tokenize(&text[offset..]) {\n             let token_text = &text[offset..][..token.len];\n-            offset += token.len;\n \n             let (kind, err) = from_rustc(&token.kind, token_text);\n-            let mut token = LexerToken::new(kind, token.len);\n-            token.error = err.map(|it| it.to_string());\n-            res.push(token);\n+            res.push(kind, offset);\n+            offset += token.len;\n+\n+            if let Some(err) = err {\n+                let token = res.len() as u32;\n+                let msg = err.to_string();\n+                res.error.push(LexError { msg, token });\n+            }\n         }\n+        res.push(EOF, offset);\n \n         res\n     }\n-    /// Lexes text as a single token. Returns `None` if there's leftover text.\n-    pub fn from_str(text: &str) -> Option<LexerToken> {\n+\n+    pub fn single_token(text: &'a str) -> Option<SyntaxKind> {\n         if text.is_empty() {\n             return None;\n         }\n@@ -56,10 +63,40 @@ impl LexerToken {\n         }\n \n         let (kind, err) = from_rustc(&token.kind, text);\n+        if err.is_some() {\n+            return None;\n+        }\n+\n+        Some(kind)\n+    }\n+\n+    pub fn as_str(&self) -> &str {\n+        self.text\n+    }\n+\n+    pub fn len(&self) -> usize {\n+        self.kind.len() - 1\n+    }\n+\n+    pub fn kind(&self, i: usize) -> SyntaxKind {\n+        assert!(i < self.len());\n+        self.kind[i]\n+    }\n+    pub fn text(&self, i: usize) -> &str {\n+        assert!(i < self.len());\n+        let lo = self.start[i] as usize;\n+        let hi = self.start[i + 1] as usize;\n+        &self.text[lo..hi]\n+    }\n+    pub fn error(&self, i: usize) -> Option<&str> {\n+        assert!(i < self.len());\n+        let err = self.error.binary_search_by_key(&(i as u32), |i| i.token).ok()?;\n+        Some(self.error[err].msg.as_str())\n+    }\n \n-        let mut token = LexerToken::new(kind, token.len);\n-        token.error = err.map(|it| it.to_string());\n-        Some(token)\n+    fn push(&mut self, kind: SyntaxKind, offset: usize) {\n+        self.kind.push(kind);\n+        self.start.push(offset as u32);\n     }\n }\n ", "previous_filename": "crates/parser/src/lexer_token.rs"}, {"sha": "dc02ae6e83f44a1a3c1928e22303c47b2baad19b", "filename": "crates/parser/src/lib.rs", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/rust-lang/rust/blob/8b9d145dea17dc28d83fae23b5be63233483ec6d/crates%2Fparser%2Fsrc%2Flib.rs", "raw_url": "https://github.com/rust-lang/rust/raw/8b9d145dea17dc28d83fae23b5be63233483ec6d/crates%2Fparser%2Fsrc%2Flib.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fparser%2Fsrc%2Flib.rs?ref=8b9d145dea17dc28d83fae23b5be63233483ec6d", "patch": "@@ -18,7 +18,7 @@\n //! [`Parser`]: crate::parser::Parser\n #![allow(rustdoc::private_intra_doc_links)]\n \n-mod lexer_token;\n+mod lexed_str;\n mod token_set;\n mod syntax_kind;\n mod event;\n@@ -31,7 +31,7 @@ mod tests;\n \n pub(crate) use token_set::TokenSet;\n \n-pub use crate::{lexer_token::LexerToken, syntax_kind::SyntaxKind, tokens::Tokens};\n+pub use crate::{lexed_str::LexedStr, syntax_kind::SyntaxKind, tokens::Tokens};\n \n #[derive(Debug, Clone, PartialEq, Eq, Hash)]\n pub struct ParseError(pub Box<String>);"}, {"sha": "ebba9925618b6d720a5ba6d200d4a52bc342cba8", "filename": "crates/parser/src/tests.rs", "status": "modified", "additions": 10, "deletions": 7, "changes": 17, "blob_url": "https://github.com/rust-lang/rust/blob/8b9d145dea17dc28d83fae23b5be63233483ec6d/crates%2Fparser%2Fsrc%2Ftests.rs", "raw_url": "https://github.com/rust-lang/rust/raw/8b9d145dea17dc28d83fae23b5be63233483ec6d/crates%2Fparser%2Fsrc%2Ftests.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fparser%2Fsrc%2Ftests.rs?ref=8b9d145dea17dc28d83fae23b5be63233483ec6d", "patch": "@@ -6,7 +6,7 @@ use std::{\n \n use expect_test::expect_file;\n \n-use crate::LexerToken;\n+use crate::LexedStr;\n \n #[test]\n fn valid_lexes_input() {\n@@ -25,13 +25,16 @@ fn invalid_lexes_input() {\n }\n \n fn lex(text: &str) -> String {\n+    let lexed = LexedStr::new(text);\n+\n     let mut res = String::new();\n-    let mut offset = 0;\n-    for token in LexerToken::tokenize(text) {\n-        let token_text = &text[offset..][..token.len];\n-        offset += token.len;\n-        let err = token.error.map(|err| format!(\" error: {}\", err)).unwrap_or_default();\n-        writeln!(res, \"{:?} {:?}{}\", token.kind, token_text, err).unwrap();\n+    for i in 0..lexed.len() {\n+        let kind = lexed.kind(i);\n+        let text = lexed.text(i);\n+        let error = lexed.error(i);\n+\n+        let error = error.map(|err| format!(\" error: {}\", err)).unwrap_or_default();\n+        writeln!(res, \"{:?} {:?}{}\", kind, text, error).unwrap();\n     }\n     res\n }"}, {"sha": "4fc2361add21f9a10a306391e214d20796fbdf65", "filename": "crates/parser/src/tokens.rs", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "blob_url": "https://github.com/rust-lang/rust/blob/8b9d145dea17dc28d83fae23b5be63233483ec6d/crates%2Fparser%2Fsrc%2Ftokens.rs", "raw_url": "https://github.com/rust-lang/rust/raw/8b9d145dea17dc28d83fae23b5be63233483ec6d/crates%2Fparser%2Fsrc%2Ftokens.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fparser%2Fsrc%2Ftokens.rs?ref=8b9d145dea17dc28d83fae23b5be63233483ec6d", "patch": "@@ -1,7 +1,8 @@\n //! Input for the parser -- a sequence of tokens.\n //!\n //! As of now, parser doesn't have access to the *text* of the tokens, and makes\n-//! decisions based solely on their classification.\n+//! decisions based solely on their classification. Unlike `LexerToken`, the\n+//! `Tokens` doesn't include whitespace and comments.\n \n use crate::SyntaxKind;\n "}]}