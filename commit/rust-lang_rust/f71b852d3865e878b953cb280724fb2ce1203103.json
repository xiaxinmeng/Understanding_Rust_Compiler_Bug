{"sha": "f71b852d3865e878b953cb280724fb2ce1203103", "node_id": "MDY6Q29tbWl0NzI0NzEyOmY3MWI4NTJkMzg2NWU4NzhiOTUzY2IyODA3MjRmYjJjZTEyMDMxMDM=", "commit": {"author": {"name": "Jakub Bukaj", "email": "jakub@jakub.cc", "date": "2014-11-19T21:41:05Z"}, "committer": {"name": "Jakub Bukaj", "email": "jakub@jakub.cc", "date": "2014-11-19T21:41:05Z"}, "message": "rollup merge of #19103: huonw/literal-suffixes\n\nFutureproof Rust for fancier suffixed literals. The Rust compiler tokenises a literal followed immediately (no whitespace) by an identifier as a single token: (for example) the text sequences `\"foo\"bar`, `1baz` and `1u1024` are now a single token rather than the pairs `\"foo\"` `bar`, `1` `baz` and `1u` `1024` respectively.\n\nThe compiler rejects all such suffixes in the parser, except for the 12 numeric suffixes we have now.\n\nI'm fairly sure this will affect very few programs, since it's not currently legal to have `<literal><identifier>` in a Rust program, except in a macro invocation. Any macro invocation relying on this behaviour can simply separate the two tokens with whitespace: `foo!(\"bar\"baz)` becomes `foo!(\"bar\" baz)`.\n\nThis implements [RFC 463](https://github.com/rust-lang/rfcs/blob/master/text/0463-future-proof-literal-suffixes.md), and so closes https://github.com/rust-lang/rust/issues/19088.", "tree": {"sha": "4cf08dabab8901fcbac433e2d3f964abeaab2d94", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/4cf08dabab8901fcbac433e2d3f964abeaab2d94"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/f71b852d3865e878b953cb280724fb2ce1203103", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/f71b852d3865e878b953cb280724fb2ce1203103", "html_url": "https://github.com/rust-lang/rust/commit/f71b852d3865e878b953cb280724fb2ce1203103", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/f71b852d3865e878b953cb280724fb2ce1203103/comments", "author": null, "committer": null, "parents": [{"sha": "00ffcca0164504ecae8453750a053e2b7206264a", "url": "https://api.github.com/repos/rust-lang/rust/commits/00ffcca0164504ecae8453750a053e2b7206264a", "html_url": "https://github.com/rust-lang/rust/commit/00ffcca0164504ecae8453750a053e2b7206264a"}, {"sha": "a11078f8c3a69f0d4ea20ca10aaf96622770615b", "url": "https://api.github.com/repos/rust-lang/rust/commits/a11078f8c3a69f0d4ea20ca10aaf96622770615b", "html_url": "https://github.com/rust-lang/rust/commit/a11078f8c3a69f0d4ea20ca10aaf96622770615b"}], "stats": {"total": 833, "additions": 476, "deletions": 357}, "files": [{"sha": "a9f45907b8110b50716f02bbe0ebefd43fcb00b5", "filename": "mk/grammar.mk", "status": "modified", "additions": 12, "deletions": 4, "changes": 16, "blob_url": "https://github.com/rust-lang/rust/blob/f71b852d3865e878b953cb280724fb2ce1203103/mk%2Fgrammar.mk", "raw_url": "https://github.com/rust-lang/rust/raw/f71b852d3865e878b953cb280724fb2ce1203103/mk%2Fgrammar.mk", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/mk%2Fgrammar.mk?ref=f71b852d3865e878b953cb280724fb2ce1203103", "patch": "@@ -30,17 +30,25 @@ endef\n $(BG):\n \t$(Q)mkdir -p $(BG)\n \n-$(BG)RustLexer.class: $(SG)RustLexer.g4\n+$(BG)RustLexer.class: $(BG) $(SG)RustLexer.g4\n \t$(Q)$(CFG_ANTLR4) -o $(B)grammar $(SG)RustLexer.g4\n \t$(Q)$(CFG_JAVAC) -d $(BG) $(BG)RustLexer.java\n \n-$(BG)verify: $(SG)verify.rs rustc-stage2-H-$(CFG_BUILD) $(LD)stamp.regex_macros $(LD)stamp.rustc\n-\t$(Q)$(RUSTC) -O --out-dir $(BG) -L $(L) $(SG)verify.rs\n+check-build-lexer-verifier: $(BG)verify\n+\n+ifeq ($(NO_REBUILD),)\n+VERIFY_DEPS :=  rustc-stage2-H-$(CFG_BUILD) $(LD)stamp.regex_macros $(LD)stamp.rustc\n+else\n+VERIFY_DEPS :=\n+endif\n+\n+$(BG)verify: $(BG) $(SG)verify.rs $(VERIFY_DEPS)\n+\t$(Q)$(RUSTC) --out-dir $(BG) -L $(L) $(SG)verify.rs\n \n ifdef CFG_JAVAC\n ifdef CFG_ANTLR4\n ifdef CFG_GRUN\n-check-lexer: $(BG) $(BG)RustLexer.class $(BG)verify\n+check-lexer: $(BG) $(BG)RustLexer.class check-build-lexer-verifier\n \t$(info Verifying libsyntax against the reference lexer ...)\n \t$(Q)$(SG)check.sh $(S) \"$(BG)\" \\\n \t\t\"$(CFG_GRUN)\" \"$(BG)verify\" \"$(BG)RustLexer.tokens\""}, {"sha": "63a34e0f01007a4376058bf7c4b66940c2016e9c", "filename": "mk/tests.mk", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/f71b852d3865e878b953cb280724fb2ce1203103/mk%2Ftests.mk", "raw_url": "https://github.com/rust-lang/rust/raw/f71b852d3865e878b953cb280724fb2ce1203103/mk%2Ftests.mk", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/mk%2Ftests.mk?ref=f71b852d3865e878b953cb280724fb2ce1203103", "patch": "@@ -199,7 +199,7 @@ check-docs: cleantestlibs cleantmptestlogs check-stage2-docs\n \n # Some less critical tests that are not prone to breakage.\n # Not run as part of the normal test suite, but tested by bors on checkin.\n-check-secondary: check-build-compiletest check-lexer check-pretty\n+check-secondary: check-build-compiletest check-build-lexer-verifier check-lexer check-pretty\n \n # check + check-secondary.\n #"}, {"sha": "5e8018f81c6d9f29b2699202db38f789f21d6c1f", "filename": "src/doc/reference.md", "status": "modified", "additions": 20, "deletions": 23, "changes": 43, "blob_url": "https://github.com/rust-lang/rust/blob/f71b852d3865e878b953cb280724fb2ce1203103/src%2Fdoc%2Freference.md", "raw_url": "https://github.com/rust-lang/rust/raw/f71b852d3865e878b953cb280724fb2ce1203103/src%2Fdoc%2Freference.md", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Fdoc%2Freference.md?ref=f71b852d3865e878b953cb280724fb2ce1203103", "patch": "@@ -216,9 +216,15 @@ rather than referring to it by name or some other evaluation rule. A literal is\n a form of constant expression, so is evaluated (primarily) at compile time.\n \n ```{.ebnf .gram}\n-literal : string_lit | char_lit | byte_string_lit | byte_lit | num_lit ;\n+lit_suffix : ident;\n+literal : [ string_lit | char_lit | byte_string_lit | byte_lit | num_lit ] lit_suffix ?;\n ```\n \n+The optional suffix is only used for certain numeric literals, but is\n+reserved for future extension, that is, the above gives the lexical\n+grammar, but a Rust parser will reject everything but the 12 special\n+cases mentioned in [Number literals](#number-literals) below.\n+\n #### Character and string literals\n \n ```{.ebnf .gram}\n@@ -371,27 +377,20 @@ b\"\\\\x52\"; br\"\\x52\";                  // \\x52\n #### Number literals\n \n ```{.ebnf .gram}\n-num_lit : nonzero_dec [ dec_digit | '_' ] * num_suffix ?\n-        | '0' [       [ dec_digit | '_' ] * num_suffix ?\n-              | 'b'   [ '1' | '0' | '_' ] + int_suffix ?\n-              | 'o'   [ oct_digit | '_' ] + int_suffix ?\n-              | 'x'   [ hex_digit | '_' ] + int_suffix ? ] ;\n-\n-num_suffix : int_suffix | float_suffix ;\n+num_lit : nonzero_dec [ dec_digit | '_' ] * float_suffix ?\n+        | '0' [       [ dec_digit | '_' ] * float_suffix ?\n+              | 'b'   [ '1' | '0' | '_' ] +\n+              | 'o'   [ oct_digit | '_' ] +\n+              | 'x'   [ hex_digit | '_' ] +  ] ;\n \n-int_suffix : 'u' int_suffix_size ?\n-           | 'i' int_suffix_size ? ;\n-int_suffix_size : [ '8' | \"16\" | \"32\" | \"64\" ] ;\n+float_suffix : [ exponent | '.' dec_lit exponent ? ] ? ;\n \n-float_suffix : [ exponent | '.' dec_lit exponent ? ] ? float_suffix_ty ? ;\n-float_suffix_ty : 'f' [ \"32\" | \"64\" ] ;\n exponent : ['E' | 'e'] ['-' | '+' ] ? dec_lit ;\n dec_lit : [ dec_digit | '_' ] + ;\n ```\n \n A _number literal_ is either an _integer literal_ or a _floating-point\n-literal_. The grammar for recognizing the two kinds of literals is mixed, as\n-they are differentiated by suffixes.\n+literal_. The grammar for recognizing the two kinds of literals is mixed.\n \n ##### Integer literals\n \n@@ -406,9 +405,9 @@ An _integer literal_ has one of four forms:\n * A _binary literal_ starts with the character sequence `U+0030` `U+0062`\n   (`0b`) and continues as any mixture of binary digits and underscores.\n \n-An integer literal may be followed (immediately, without any spaces) by an\n-_integer suffix_, which changes the type of the literal. There are two kinds of\n-integer literal suffix:\n+Like any literal, an integer literal may be followed (immediately,\n+without any spaces) by an _integer suffix_, which forcibly sets the\n+type of the literal. There are 10 valid values for an integer suffix:\n \n * The `i` and `u` suffixes give the literal type `int` or `uint`,\n   respectively.\n@@ -443,11 +442,9 @@ A _floating-point literal_ has one of two forms:\n * A single _decimal literal_ followed by an _exponent_.\n \n By default, a floating-point literal has a generic type, and, like integer\n-literals, the type must be uniquely determined from the context. A\n-floating-point literal may be followed (immediately, without any spaces) by a\n-_floating-point suffix_, which changes the type of the literal. There are two\n-floating-point suffixes: `f32`, and `f64` (the 32-bit and 64-bit floating point\n-types).\n+literals, the type must be uniquely determined from the context. There are two valid\n+_floating-point suffixes_, `f32` and `f64` (the 32-bit and 64-bit floating point\n+types), which explicitly determine the type of the literal.\n \n Examples of floating-point literals of various forms:\n "}, {"sha": "0ff9af7aca133625e3b98670951babfeafc3b1b0", "filename": "src/grammar/RustLexer.g4", "status": "modified", "additions": 15, "deletions": 29, "changes": 44, "blob_url": "https://github.com/rust-lang/rust/blob/f71b852d3865e878b953cb280724fb2ce1203103/src%2Fgrammar%2FRustLexer.g4", "raw_url": "https://github.com/rust-lang/rust/raw/f71b852d3865e878b953cb280724fb2ce1203103/src%2Fgrammar%2FRustLexer.g4", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Fgrammar%2FRustLexer.g4?ref=f71b852d3865e878b953cb280724fb2ce1203103", "patch": "@@ -92,49 +92,35 @@ fragment CHAR_ESCAPE\n   | 'U' HEXIT HEXIT HEXIT HEXIT HEXIT HEXIT HEXIT HEXIT\n   ;\n \n-LIT_CHAR\n-  : '\\'' ( '\\\\' CHAR_ESCAPE | ~[\\\\'\\n\\t\\r] ) '\\''\n+fragment SUFFIX\n+  : IDENT\n   ;\n \n-LIT_BYTE\n-  : 'b\\'' ( '\\\\' ( [xX] HEXIT HEXIT | [nrt\\\\'\"0] ) | ~[\\\\'\\n\\t\\r] ) '\\''\n+LIT_CHAR\n+  : '\\'' ( '\\\\' CHAR_ESCAPE | ~[\\\\'\\n\\t\\r] ) '\\'' SUFFIX?\n   ;\n \n-fragment INT_SUFFIX\n-  : 'i'\n-  | 'i8'\n-  | 'i16'\n-  | 'i32'\n-  | 'i64'\n-  | 'u'\n-  | 'u8'\n-  | 'u16'\n-  | 'u32'\n-  | 'u64'\n+LIT_BYTE\n+  : 'b\\'' ( '\\\\' ( [xX] HEXIT HEXIT | [nrt\\\\'\"0] ) | ~[\\\\'\\n\\t\\r] ) '\\'' SUFFIX?\n   ;\n \n LIT_INTEGER\n-  : [0-9][0-9_]* INT_SUFFIX?\n-  | '0b' [01][01_]* INT_SUFFIX?\n-  | '0o' [0-7][0-7_]* INT_SUFFIX?\n-  | '0x' [0-9a-fA-F][0-9a-fA-F_]* INT_SUFFIX?\n-  ;\n-\n-fragment FLOAT_SUFFIX\n-  : 'f32'\n-  | 'f64'\n+  : [0-9][0-9_]* SUFFIX?\n+  | '0b' [01][01_]* SUFFIX?\n+  | '0o' [0-7][0-7_]* SUFFIX?\n+  | '0x' [0-9a-fA-F][0-9a-fA-F_]* SUFFIX?\n   ;\n \n LIT_FLOAT\n-  : [0-9][0-9_]* ('.' | ('.' [0-9][0-9_]*)? ([eE] [-+]? [0-9][0-9_]*)? FLOAT_SUFFIX?)\n+  : [0-9][0-9_]* ('.' | ('.' [0-9][0-9_]*)? ([eE] [-+]? [0-9][0-9_]*)? SUFFIX?)\n   ;\n \n LIT_STR\n-  : '\"' ('\\\\\\n' | '\\\\\\r\\n' | '\\\\' CHAR_ESCAPE | .)*? '\"'\n+  : '\"' ('\\\\\\n' | '\\\\\\r\\n' | '\\\\' CHAR_ESCAPE | .)*? '\"' SUFFIX?\n   ;\n \n-LIT_BINARY : 'b' LIT_STR ;\n-LIT_BINARY_RAW : 'rb' LIT_STR_RAW ;\n+LIT_BINARY : 'b' LIT_STR SUFFIX?;\n+LIT_BINARY_RAW : 'rb' LIT_STR_RAW SUFFIX?;\n \n /* this is a bit messy */\n \n@@ -148,7 +134,7 @@ fragment LIT_STR_RAW_INNER2\n   ;\n \n LIT_STR_RAW\n-  : 'r' LIT_STR_RAW_INNER\n+  : 'r' LIT_STR_RAW_INNER SUFFIX?\n   ;\n \n IDENT : XID_start XID_continue* ;"}, {"sha": "e3ff20f7874bf88239f76197eda765a377d7ecf7", "filename": "src/grammar/verify.rs", "status": "modified", "additions": 47, "deletions": 44, "changes": 91, "blob_url": "https://github.com/rust-lang/rust/blob/f71b852d3865e878b953cb280724fb2ce1203103/src%2Fgrammar%2Fverify.rs", "raw_url": "https://github.com/rust-lang/rust/raw/f71b852d3865e878b953cb280724fb2ce1203103/src%2Fgrammar%2Fverify.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Fgrammar%2Fverify.rs?ref=f71b852d3865e878b953cb280724fb2ce1203103", "patch": "@@ -26,21 +26,21 @@ use std::io::File;\n \n use syntax::parse;\n use syntax::parse::lexer;\n-use rustc::driver::{session, config};\n+use rustc::session::{mod, config};\n \n use syntax::ast;\n use syntax::ast::Name;\n use syntax::parse::token;\n use syntax::parse::lexer::TokenAndSpan;\n \n-fn parse_token_list(file: &str) -> HashMap<String, Token> {\n-    fn id() -> Token {\n+fn parse_token_list(file: &str) -> HashMap<String, token::Token> {\n+    fn id() -> token::Token {\n         token::Ident(ast::Ident { name: Name(0), ctxt: 0, }, token::Plain)\n     }\n \n     let mut res = HashMap::new();\n \n-    res.insert(\"-1\".to_string(), EOF);\n+    res.insert(\"-1\".to_string(), token::Eof);\n \n     for line in file.split('\\n') {\n         let eq = match line.trim().rfind('=') {\n@@ -60,8 +60,8 @@ fn parse_token_list(file: &str) -> HashMap<String, Token> {\n             \"INT_SUFFIX\"        => id(),\n             \"SHL\"               => token::BinOp(token::Shl),\n             \"LBRACE\"            => token::OpenDelim(token::Brace),\n-            \"RARROW\"            => token::Rarrow,\n-            \"LIT_STR\"           => token::LitStr(Name(0)),\n+            \"RARROW\"            => token::RArrow,\n+            \"LIT_STR\"           => token::Literal(token::Str_(Name(0))),\n             \"DOTDOT\"            => token::DotDot,\n             \"MOD_SEP\"           => token::ModSep,\n             \"DOTDOTDOT\"         => token::DotDotDot,\n@@ -71,17 +71,17 @@ fn parse_token_list(file: &str) -> HashMap<String, Token> {\n             \"ANDAND\"            => token::AndAnd,\n             \"AT\"                => token::At,\n             \"LBRACKET\"          => token::OpenDelim(token::Bracket),\n-            \"LIT_STR_RAW\"       => token::LitStrRaw(Name(0), 0),\n+            \"LIT_STR_RAW\"       => token::Literal(token::StrRaw(Name(0), 0)),\n             \"RPAREN\"            => token::CloseDelim(token::Paren),\n             \"SLASH\"             => token::BinOp(token::Slash),\n             \"COMMA\"             => token::Comma,\n             \"LIFETIME\"          => token::Lifetime(ast::Ident { name: Name(0), ctxt: 0 }),\n             \"CARET\"             => token::BinOp(token::Caret),\n             \"TILDE\"             => token::Tilde,\n-            \"IDENT\"             => token::Id(),\n+            \"IDENT\"             => id(),\n             \"PLUS\"              => token::BinOp(token::Plus),\n-            \"LIT_CHAR\"          => token::LitChar(Name(0)),\n-            \"LIT_BYTE\"          => token::LitByte(Name(0)),\n+            \"LIT_CHAR\"          => token::Literal(token::Char(Name(0))),\n+            \"LIT_BYTE\"          => token::Literal(token::Byte(Name(0))),\n             \"EQ\"                => token::Eq,\n             \"RBRACKET\"          => token::CloseDelim(token::Bracket),\n             \"COMMENT\"           => token::Comment,\n@@ -95,9 +95,9 @@ fn parse_token_list(file: &str) -> HashMap<String, Token> {\n             \"BINOP\"             => token::BinOp(token::Plus),\n             \"POUND\"             => token::Pound,\n             \"OROR\"              => token::OrOr,\n-            \"LIT_INTEGER\"       => token::LitInteger(Name(0)),\n+            \"LIT_INTEGER\"       => token::Literal(token::Integer(Name(0))),\n             \"BINOPEQ\"           => token::BinOpEq(token::Plus),\n-            \"LIT_FLOAT\"         => token::LitFloat(Name(0)),\n+            \"LIT_FLOAT\"         => token::Literal(token::Float(Name(0))),\n             \"WHITESPACE\"        => token::Whitespace,\n             \"UNDERSCORE\"        => token::Underscore,\n             \"MINUS\"             => token::BinOp(token::Minus),\n@@ -107,8 +107,8 @@ fn parse_token_list(file: &str) -> HashMap<String, Token> {\n             \"OR\"                => token::BinOp(token::Or),\n             \"GT\"                => token::Gt,\n             \"LE\"                => token::Le,\n-            \"LIT_BINARY\"        => token::LitBinary(Name(0)),\n-            \"LIT_BINARY_RAW\"    => token::LitBinaryRaw(Name(0), 0),\n+            \"LIT_BINARY\"        => token::Literal(token::Binary(Name(0))),\n+            \"LIT_BINARY_RAW\"    => token::Literal(token::BinaryRaw(Name(0), 0)),\n             _                   => continue,\n         };\n \n@@ -119,7 +119,7 @@ fn parse_token_list(file: &str) -> HashMap<String, Token> {\n     res\n }\n \n-fn str_to_binop(s: &str) -> BinOpToken {\n+fn str_to_binop(s: &str) -> token::BinOpToken {\n     match s {\n         \"+\"     => token::Plus,\n         \"/\"     => token::Slash,\n@@ -167,7 +167,7 @@ fn count(lit: &str) -> uint {\n     lit.chars().take_while(|c| *c == '#').count()\n }\n \n-fn parse_antlr_token(s: &str, tokens: &HashMap<String, Token>) -> TokenAndSpan {\n+fn parse_antlr_token(s: &str, tokens: &HashMap<String, token::Token>) -> TokenAndSpan {\n     let re = regex!(\n       r\"\\[@(?P<seq>\\d+),(?P<start>\\d+):(?P<end>\\d+)='(?P<content>.+?)',<(?P<toknum>-?\\d+)>,\\d+:\\d+]\"\n     );\n@@ -178,7 +178,7 @@ fn parse_antlr_token(s: &str, tokens: &HashMap<String, Token>) -> TokenAndSpan {\n     let toknum = m.name(\"toknum\");\n     let content = m.name(\"content\");\n \n-    let proto_tok = tokens.get(&toknum).expect(format!(\"didn't find token {} in the map\",\n+    let proto_tok = tokens.get(toknum).expect(format!(\"didn't find token {} in the map\",\n                                                               toknum).as_slice());\n \n     let nm = parse::token::intern(content);\n@@ -189,22 +189,25 @@ fn parse_antlr_token(s: &str, tokens: &HashMap<String, Token>) -> TokenAndSpan {\n         token::BinOp(..)           => token::BinOp(str_to_binop(content)),\n         token::BinOpEq(..)         => token::BinOpEq(str_to_binop(content.slice_to(\n                                                                     content.len() - 1))),\n-        token::LitStr(..)          => token::LitStr(fix(content)),\n-        token::LitStrRaw(..)       => token::LitStrRaw(fix(content), count(content)),\n-        token::LitChar(..)         => token::LitChar(fixchar(content)),\n-        token::LitByte(..)         => token::LitByte(fixchar(content)),\n+        token::Literal(token::Str_(..))      => token::Literal(token::Str_(fix(content))),\n+        token::Literal(token::StrRaw(..))    => token::Literal(token::StrRaw(fix(content),\n+                                                                             count(content))),\n+        token::Literal(token::Char(..))      => token::Literal(token::Char(fixchar(content))),\n+        token::Literal(token::Byte(..))      => token::Literal(token::Byte(fixchar(content))),\n         token::DocComment(..)      => token::DocComment(nm),\n-        token::LitInteger(..)      => token::LitInteger(nm),\n-        token::LitFloat(..)        => token::LitFloat(nm),\n-        token::LitBinary(..)       => token::LitBinary(nm),\n-        token::LitBinaryRaw(..)    => token::LitBinaryRaw(fix(content), count(content)),\n+        token::Literal(token::Integer(..))   => token::Literal(token::Integer(nm)),\n+        token::Literal(token::Float(..))     => token::Literal(token::Float(nm)),\n+        token::Literal(token::Binary(..))    => token::Literal(token::Binary(nm)),\n+        token::Literal(token::BinaryRaw(..)) => token::Literal(token::BinaryRaw(fix(content),\n+                                                                                count(content))),\n         token::Ident(..)           => token::Ident(ast::Ident { name: nm, ctxt: 0 },\n                                                    token::ModName),\n         token::Lifetime(..)        => token::Lifetime(ast::Ident { name: nm, ctxt: 0 }),\n         ref t => t.clone()\n     };\n \n-    let offset = if real_tok == EOF {\n+    let offset = if real_tok == token::Eof\n+ {\n         1\n     } else {\n         0\n@@ -222,7 +225,7 @@ fn parse_antlr_token(s: &str, tokens: &HashMap<String, Token>) -> TokenAndSpan {\n     }\n }\n \n-fn tok_cmp(a: &Token, b: &Token) -> bool {\n+fn tok_cmp(a: &token::Token, b: &token::Token) -> bool {\n     match a {\n         &token::Ident(id, _) => match b {\n                 &token::Ident(id2, _) => id == id2,\n@@ -240,25 +243,25 @@ fn main() {\n \n     let args = std::os::args();\n \n-    let mut token_file = File::open(&Path::new(args.get(2).as_slice()));\n+    let mut token_file = File::open(&Path::new(args[2].as_slice()));\n     let token_map = parse_token_list(token_file.read_to_string().unwrap().as_slice());\n \n     let mut stdin = std::io::stdin();\n     let mut antlr_tokens = stdin.lines().map(|l| parse_antlr_token(l.unwrap().as_slice().trim(),\n                                                                    &token_map));\n \n-    let code = File::open(&Path::new(args.get(1).as_slice())).unwrap().read_to_string().unwrap();\n+    let code = File::open(&Path::new(args[1].as_slice())).unwrap().read_to_string().unwrap();\n     let options = config::basic_options();\n     let session = session::build_session(options, None,\n-                                         syntax::diagnostics::registry::Registry::new([]));\n+                                         syntax::diagnostics::registry::Registry::new(&[]));\n     let filemap = parse::string_to_filemap(&session.parse_sess,\n                                            code,\n                                            String::from_str(\"<n/a>\"));\n     let mut lexer = lexer::StringReader::new(session.diagnostic(), filemap);\n \n     for antlr_tok in antlr_tokens {\n         let rustc_tok = next(&mut lexer);\n-        if rustc_tok.tok == EOF && antlr_tok.tok == EOF {\n+        if rustc_tok.tok == token::Eof && antlr_tok.tok == token::Eof {\n             continue\n         }\n \n@@ -284,19 +287,19 @@ fn main() {\n         )\n \n         matches!(\n-            LitByte(..),\n-            LitChar(..),\n-            LitInteger(..),\n-            LitFloat(..),\n-            LitStr(..),\n-            LitStrRaw(..),\n-            LitBinary(..),\n-            LitBinaryRaw(..),\n-            Ident(..),\n-            Lifetime(..),\n-            Interpolated(..),\n-            DocComment(..),\n-            Shebang(..)\n+            token::Literal(token::Byte(..)),\n+            token::Literal(token::Char(..)),\n+            token::Literal(token::Integer(..)),\n+            token::Literal(token::Float(..)),\n+            token::Literal(token::Str_(..)),\n+            token::Literal(token::StrRaw(..)),\n+            token::Literal(token::Binary(..)),\n+            token::Literal(token::BinaryRaw(..)),\n+            token::Ident(..),\n+            token::Lifetime(..),\n+            token::Interpolated(..),\n+            token::DocComment(..),\n+            token::Shebang(..)\n         );\n     }\n }"}, {"sha": "111650f565cf6bb5643ab7117b5452655d206ee7", "filename": "src/librustdoc/html/highlight.rs", "status": "modified", "additions": 11, "deletions": 6, "changes": 17, "blob_url": "https://github.com/rust-lang/rust/blob/f71b852d3865e878b953cb280724fb2ce1203103/src%2Flibrustdoc%2Fhtml%2Fhighlight.rs", "raw_url": "https://github.com/rust-lang/rust/raw/f71b852d3865e878b953cb280724fb2ce1203103/src%2Flibrustdoc%2Fhtml%2Fhighlight.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustdoc%2Fhtml%2Fhighlight.rs?ref=f71b852d3865e878b953cb280724fb2ce1203103", "patch": "@@ -128,12 +128,17 @@ fn doit(sess: &parse::ParseSess, mut lexer: lexer::StringReader,\n                 }\n             }\n \n-            // text literals\n-            token::LitByte(..) | token::LitBinary(..) | token::LitBinaryRaw(..) |\n-                token::LitChar(..) | token::LitStr(..) | token::LitStrRaw(..) => \"string\",\n-\n-            // number literals\n-            token::LitInteger(..) | token::LitFloat(..) => \"number\",\n+            token::Literal(lit, _suf) => {\n+                match lit {\n+                    // text literals\n+                    token::Byte(..) | token::Char(..) |\n+                        token::Binary(..) | token::BinaryRaw(..) |\n+                        token::Str_(..) | token::StrRaw(..) => \"string\",\n+\n+                    // number literals\n+                    token::Integer(..) | token::Float(..) => \"number\",\n+                }\n+            }\n \n             // keywords are also included in the identifier set\n             token::Ident(ident, _is_mod_sep) => {"}, {"sha": "e3058c0a248b708a1bdd6889f8960b45fe6c6ee1", "filename": "src/libsyntax/ast.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/f71b852d3865e878b953cb280724fb2ce1203103/src%2Flibsyntax%2Fast.rs", "raw_url": "https://github.com/rust-lang/rust/raw/f71b852d3865e878b953cb280724fb2ce1203103/src%2Flibsyntax%2Fast.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fast.rs?ref=f71b852d3865e878b953cb280724fb2ce1203103", "patch": "@@ -838,7 +838,7 @@ impl TokenTree {\n                     tts: vec![TtToken(sp, token::Ident(token::str_to_ident(\"doc\"),\n                                                        token::Plain)),\n                               TtToken(sp, token::Eq),\n-                              TtToken(sp, token::LitStr(name))],\n+                              TtToken(sp, token::Literal(token::Str_(name), None))],\n                     close_span: sp,\n                 }))\n             }"}, {"sha": "281bde3129aba4e642a2479e827bd57f1328f952", "filename": "src/libsyntax/diagnostics/plugin.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/f71b852d3865e878b953cb280724fb2ce1203103/src%2Flibsyntax%2Fdiagnostics%2Fplugin.rs", "raw_url": "https://github.com/rust-lang/rust/raw/f71b852d3865e878b953cb280724fb2ce1203103/src%2Flibsyntax%2Fdiagnostics%2Fplugin.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fdiagnostics%2Fplugin.rs?ref=f71b852d3865e878b953cb280724fb2ce1203103", "patch": "@@ -87,7 +87,7 @@ pub fn expand_register_diagnostic<'cx>(ecx: &'cx mut ExtCtxt,\n         },\n         [ast::TtToken(_, token::Ident(ref code, _)),\n          ast::TtToken(_, token::Comma),\n-         ast::TtToken(_, token::LitStrRaw(description, _))] => {\n+         ast::TtToken(_, token::Literal(token::StrRaw(description, _), None))] => {\n             (code, Some(description))\n         }\n         _ => unreachable!()"}, {"sha": "eaa3632cf499e558b77984b43c8242d9ffa7d447", "filename": "src/libsyntax/ext/quote.rs", "status": "modified", "additions": 22, "deletions": 18, "changes": 40, "blob_url": "https://github.com/rust-lang/rust/blob/f71b852d3865e878b953cb280724fb2ce1203103/src%2Flibsyntax%2Fext%2Fquote.rs", "raw_url": "https://github.com/rust-lang/rust/raw/f71b852d3865e878b953cb280724fb2ce1203103/src%2Flibsyntax%2Fext%2Fquote.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fext%2Fquote.rs?ref=f71b852d3865e878b953cb280724fb2ce1203103", "patch": "@@ -542,6 +542,16 @@ fn mk_delim(cx: &ExtCtxt, sp: Span, delim: token::DelimToken) -> P<ast::Expr> {\n \n #[allow(non_upper_case_globals)]\n fn mk_token(cx: &ExtCtxt, sp: Span, tok: &token::Token) -> P<ast::Expr> {\n+    macro_rules! mk_lit {\n+        ($name: expr, $suffix: expr, $($args: expr),*) => {{\n+            let inner = cx.expr_call(sp, mk_token_path(cx, sp, $name), vec![$($args),*]);\n+            let suffix = match $suffix {\n+                Some(name) => cx.expr_some(sp, mk_name(cx, sp, ast::Ident::new(name))),\n+                None => cx.expr_none(sp)\n+            };\n+            cx.expr_call(sp, mk_token_path(cx, sp, \"Literal\"), vec![inner, suffix])\n+        }}\n+    }\n     match *tok {\n         token::BinOp(binop) => {\n             return cx.expr_call(sp, mk_token_path(cx, sp, \"BinOp\"), vec!(mk_binop(cx, sp, binop)));\n@@ -560,38 +570,32 @@ fn mk_token(cx: &ExtCtxt, sp: Span, tok: &token::Token) -> P<ast::Expr> {\n                                 vec![mk_delim(cx, sp, delim)]);\n         }\n \n-        token::LitByte(i) => {\n+        token::Literal(token::Byte(i), suf) => {\n             let e_byte = mk_name(cx, sp, i.ident());\n-\n-            return cx.expr_call(sp, mk_token_path(cx, sp, \"LitByte\"), vec!(e_byte));\n+            return mk_lit!(\"Byte\", suf, e_byte);\n         }\n \n-        token::LitChar(i) => {\n+        token::Literal(token::Char(i), suf) => {\n             let e_char = mk_name(cx, sp, i.ident());\n-\n-            return cx.expr_call(sp, mk_token_path(cx, sp, \"LitChar\"), vec!(e_char));\n+            return mk_lit!(\"Char\", suf, e_char);\n         }\n \n-        token::LitInteger(i) => {\n+        token::Literal(token::Integer(i), suf) => {\n             let e_int = mk_name(cx, sp, i.ident());\n-            return cx.expr_call(sp, mk_token_path(cx, sp, \"LitInteger\"), vec!(e_int));\n+            return mk_lit!(\"Integer\", suf, e_int);\n         }\n \n-        token::LitFloat(fident) => {\n+        token::Literal(token::Float(fident), suf) => {\n             let e_fident = mk_name(cx, sp, fident.ident());\n-            return cx.expr_call(sp, mk_token_path(cx, sp, \"LitFloat\"), vec!(e_fident));\n+            return mk_lit!(\"Float\", suf, e_fident);\n         }\n \n-        token::LitStr(ident) => {\n-            return cx.expr_call(sp,\n-                                mk_token_path(cx, sp, \"LitStr\"),\n-                                vec!(mk_name(cx, sp, ident.ident())));\n+        token::Literal(token::Str_(ident), suf) => {\n+            return mk_lit!(\"Str_\", suf, mk_name(cx, sp, ident.ident()))\n         }\n \n-        token::LitStrRaw(ident, n) => {\n-            return cx.expr_call(sp,\n-                                mk_token_path(cx, sp, \"LitStrRaw\"),\n-                                vec!(mk_name(cx, sp, ident.ident()), cx.expr_uint(sp, n)));\n+        token::Literal(token::StrRaw(ident, n), suf) => {\n+            return mk_lit!(\"StrRaw\", suf, mk_name(cx, sp, ident.ident()), cx.expr_uint(sp, n))\n         }\n \n         token::Ident(ident, style) => {"}, {"sha": "fbca4868255ff92353af1b88584ea3bd4d4f0eb9", "filename": "src/libsyntax/parse/lexer/mod.rs", "status": "modified", "additions": 85, "deletions": 94, "changes": 179, "blob_url": "https://github.com/rust-lang/rust/blob/f71b852d3865e878b953cb280724fb2ce1203103/src%2Flibsyntax%2Fparse%2Flexer%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/f71b852d3865e878b953cb280724fb2ce1203103/src%2Flibsyntax%2Fparse%2Flexer%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Flexer%2Fmod.rs?ref=f71b852d3865e878b953cb280724fb2ce1203103", "patch": "@@ -369,6 +369,25 @@ impl<'a> StringReader<'a> {\n         self.nextnextch() == Some(c)\n     }\n \n+    /// Eats <XID_start><XID_continue>*, if possible.\n+    fn scan_optional_raw_name(&mut self) -> Option<ast::Name> {\n+        if !ident_start(self.curr) {\n+            return None\n+        }\n+        let start = self.last_pos;\n+        while ident_continue(self.curr) {\n+            self.bump();\n+        }\n+\n+        self.with_str_from(start, |string| {\n+            if string == \"_\" {\n+                None\n+            } else {\n+                Some(token::intern(string))\n+            }\n+        })\n+    }\n+\n     /// PRECONDITION: self.curr is not whitespace\n     /// Eats any kind of comment.\n     fn scan_comment(&mut self) -> Option<TokenAndSpan> {\n@@ -638,7 +657,7 @@ impl<'a> StringReader<'a> {\n     }\n \n     /// Lex a LIT_INTEGER or a LIT_FLOAT\n-    fn scan_number(&mut self, c: char) -> token::Token {\n+    fn scan_number(&mut self, c: char) -> token::Lit {\n         let mut num_digits;\n         let mut base = 10;\n         let start_bpos = self.last_pos;\n@@ -653,19 +672,9 @@ impl<'a> StringReader<'a> {\n                 '0'...'9' | '_' | '.' => {\n                     num_digits = self.scan_digits(10) + 1;\n                 }\n-                'u' | 'i' => {\n-                    self.scan_int_suffix();\n-                    return token::LitInteger(self.name_from(start_bpos));\n-                },\n-                'f' => {\n-                    let last_pos = self.last_pos;\n-                    self.scan_float_suffix();\n-                    self.check_float_base(start_bpos, last_pos, base);\n-                    return token::LitFloat(self.name_from(start_bpos));\n-                }\n                 _ => {\n                     // just a 0\n-                    return token::LitInteger(self.name_from(start_bpos));\n+                    return token::Integer(self.name_from(start_bpos));\n                 }\n             }\n         } else if c.is_digit_radix(10) {\n@@ -676,9 +685,7 @@ impl<'a> StringReader<'a> {\n \n         if num_digits == 0 {\n             self.err_span_(start_bpos, self.last_pos, \"no valid digits found for number\");\n-            // eat any suffix\n-            self.scan_int_suffix();\n-            return token::LitInteger(token::intern(\"0\"));\n+            return token::Integer(token::intern(\"0\"));\n         }\n \n         // might be a float, but don't be greedy if this is actually an\n@@ -692,29 +699,20 @@ impl<'a> StringReader<'a> {\n             if self.curr.unwrap_or('\\0').is_digit_radix(10) {\n                 self.scan_digits(10);\n                 self.scan_float_exponent();\n-                self.scan_float_suffix();\n             }\n             let last_pos = self.last_pos;\n             self.check_float_base(start_bpos, last_pos, base);\n-            return token::LitFloat(self.name_from(start_bpos));\n-        } else if self.curr_is('f') {\n-            // or it might be an integer literal suffixed as a float\n-            self.scan_float_suffix();\n-            let last_pos = self.last_pos;\n-            self.check_float_base(start_bpos, last_pos, base);\n-            return token::LitFloat(self.name_from(start_bpos));\n+            return token::Float(self.name_from(start_bpos));\n         } else {\n             // it might be a float if it has an exponent\n             if self.curr_is('e') || self.curr_is('E') {\n                 self.scan_float_exponent();\n-                self.scan_float_suffix();\n                 let last_pos = self.last_pos;\n                 self.check_float_base(start_bpos, last_pos, base);\n-                return token::LitFloat(self.name_from(start_bpos));\n+                return token::Float(self.name_from(start_bpos));\n             }\n             // but we certainly have an integer!\n-            self.scan_int_suffix();\n-            return token::LitInteger(self.name_from(start_bpos));\n+            return token::Integer(self.name_from(start_bpos));\n         }\n     }\n \n@@ -850,55 +848,6 @@ impl<'a> StringReader<'a> {\n         true\n     }\n \n-    /// Scan over an int literal suffix.\n-    fn scan_int_suffix(&mut self) {\n-        match self.curr {\n-            Some('i') | Some('u') => {\n-                self.bump();\n-\n-                if self.curr_is('8') {\n-                    self.bump();\n-                } else if self.curr_is('1') {\n-                    if !self.nextch_is('6') {\n-                        self.err_span_(self.last_pos, self.pos,\n-                                      \"illegal int suffix\");\n-                    } else {\n-                        self.bump(); self.bump();\n-                    }\n-                } else if self.curr_is('3') {\n-                    if !self.nextch_is('2') {\n-                        self.err_span_(self.last_pos, self.pos,\n-                                      \"illegal int suffix\");\n-                    } else {\n-                        self.bump(); self.bump();\n-                    }\n-                } else if self.curr_is('6') {\n-                    if !self.nextch_is('4') {\n-                        self.err_span_(self.last_pos, self.pos,\n-                                      \"illegal int suffix\");\n-                    } else {\n-                        self.bump(); self.bump();\n-                    }\n-                }\n-            },\n-            _ => { }\n-        }\n-    }\n-\n-    /// Scan over a float literal suffix\n-    fn scan_float_suffix(&mut self) {\n-        if self.curr_is('f') {\n-            if (self.nextch_is('3') && self.nextnextch_is('2'))\n-            || (self.nextch_is('6') && self.nextnextch_is('4')) {\n-                self.bump();\n-                self.bump();\n-                self.bump();\n-            } else {\n-                self.err_span_(self.last_pos, self.pos, \"illegal float suffix\");\n-            }\n-        }\n-    }\n-\n     /// Scan over a float exponent.\n     fn scan_float_exponent(&mut self) {\n         if self.curr_is('e') || self.curr_is('E') {\n@@ -967,7 +916,10 @@ impl<'a> StringReader<'a> {\n         }\n \n         if is_dec_digit(c) {\n-            return self.scan_number(c.unwrap());\n+            let num = self.scan_number(c.unwrap());\n+            let suffix = self.scan_optional_raw_name();\n+            debug!(\"next_token_inner: scanned number {}, {}\", num, suffix);\n+            return token::Literal(num, suffix)\n         }\n \n         if self.read_embedded_ident {\n@@ -1126,17 +1078,19 @@ impl<'a> StringReader<'a> {\n             }\n             let id = if valid { self.name_from(start) } else { token::intern(\"0\") };\n             self.bump(); // advance curr past token\n-            return token::LitChar(id);\n+            let suffix = self.scan_optional_raw_name();\n+            return token::Literal(token::Char(id), suffix);\n           }\n           'b' => {\n             self.bump();\n-            return match self.curr {\n+            let lit = match self.curr {\n                 Some('\\'') => self.scan_byte(),\n                 Some('\"') => self.scan_byte_string(),\n                 Some('r') => self.scan_raw_byte_string(),\n                 _ => unreachable!()  // Should have been a token::Ident above.\n             };\n-\n+            let suffix = self.scan_optional_raw_name();\n+            return token::Literal(lit, suffix);\n           }\n           '\"' => {\n             let start_bpos = self.last_pos;\n@@ -1157,7 +1111,8 @@ impl<'a> StringReader<'a> {\n             let id = if valid { self.name_from(start_bpos + BytePos(1)) }\n                      else { token::intern(\"??\") };\n             self.bump();\n-            return token::LitStr(id);\n+            let suffix = self.scan_optional_raw_name();\n+            return token::Literal(token::Str_(id), suffix);\n           }\n           'r' => {\n             let start_bpos = self.last_pos;\n@@ -1224,7 +1179,8 @@ impl<'a> StringReader<'a> {\n             } else {\n                 token::intern(\"??\")\n             };\n-            return token::LitStrRaw(id, hash_count);\n+            let suffix = self.scan_optional_raw_name();\n+            return token::Literal(token::StrRaw(id, hash_count), suffix);\n           }\n           '-' => {\n             if self.nextch_is('>') {\n@@ -1293,7 +1249,7 @@ impl<'a> StringReader<'a> {\n      || (self.curr_is('#') && self.nextch_is('!') && !self.nextnextch_is('['))\n     }\n \n-    fn scan_byte(&mut self) -> token::Token {\n+    fn scan_byte(&mut self) -> token::Lit {\n         self.bump();\n         let start = self.last_pos;\n \n@@ -1314,10 +1270,10 @@ impl<'a> StringReader<'a> {\n \n         let id = if valid { self.name_from(start) } else { token::intern(\"??\") };\n         self.bump(); // advance curr past token\n-        return token::LitByte(id);\n+        return token::Byte(id);\n     }\n \n-    fn scan_byte_string(&mut self) -> token::Token {\n+    fn scan_byte_string(&mut self) -> token::Lit {\n         self.bump();\n         let start = self.last_pos;\n         let mut valid = true;\n@@ -1336,10 +1292,10 @@ impl<'a> StringReader<'a> {\n         }\n         let id = if valid { self.name_from(start) } else { token::intern(\"??\") };\n         self.bump();\n-        return token::LitBinary(id);\n+        return token::Binary(id);\n     }\n \n-    fn scan_raw_byte_string(&mut self) -> token::Token {\n+    fn scan_raw_byte_string(&mut self) -> token::Lit {\n         let start_bpos = self.last_pos;\n         self.bump();\n         let mut hash_count = 0u;\n@@ -1387,8 +1343,9 @@ impl<'a> StringReader<'a> {\n             self.bump();\n         }\n         self.bump();\n-        return token::LitBinaryRaw(self.name_from_to(content_start_bpos, content_end_bpos),\n-                                     hash_count);\n+        return token::BinaryRaw(self.name_from_to(content_start_bpos,\n+                                                  content_end_bpos),\n+                                hash_count);\n     }\n }\n \n@@ -1535,17 +1492,17 @@ mod test {\n \n     #[test] fn character_a() {\n         assert_eq!(setup(&mk_sh(), \"'a'\".to_string()).next_token().tok,\n-                   token::LitChar(token::intern(\"a\")));\n+                   token::Literal(token::Char(token::intern(\"a\")), None));\n     }\n \n     #[test] fn character_space() {\n         assert_eq!(setup(&mk_sh(), \"' '\".to_string()).next_token().tok,\n-                   token::LitChar(token::intern(\" \")));\n+                   token::Literal(token::Char(token::intern(\" \")), None));\n     }\n \n     #[test] fn character_escaped() {\n         assert_eq!(setup(&mk_sh(), \"'\\\\n'\".to_string()).next_token().tok,\n-                   token::LitChar(token::intern(\"\\\\n\")));\n+                   token::Literal(token::Char(token::intern(\"\\\\n\")), None));\n     }\n \n     #[test] fn lifetime_name() {\n@@ -1557,7 +1514,41 @@ mod test {\n         assert_eq!(setup(&mk_sh(),\n                          \"r###\\\"\\\"#a\\\\b\\x00c\\\"\\\"###\".to_string()).next_token()\n                                                                  .tok,\n-                   token::LitStrRaw(token::intern(\"\\\"#a\\\\b\\x00c\\\"\"), 3));\n+                   token::Literal(token::StrRaw(token::intern(\"\\\"#a\\\\b\\x00c\\\"\"), 3), None));\n+    }\n+\n+    #[test] fn literal_suffixes() {\n+        macro_rules! test {\n+            ($input: expr, $tok_type: ident, $tok_contents: expr) => {{\n+                assert_eq!(setup(&mk_sh(), format!(\"{}suffix\", $input)).next_token().tok,\n+                           token::Literal(token::$tok_type(token::intern($tok_contents)),\n+                                          Some(token::intern(\"suffix\"))));\n+                // with a whitespace separator:\n+                assert_eq!(setup(&mk_sh(), format!(\"{} suffix\", $input)).next_token().tok,\n+                           token::Literal(token::$tok_type(token::intern($tok_contents)),\n+                                          None));\n+            }}\n+        }\n+\n+        test!(\"'a'\", Char, \"a\");\n+        test!(\"b'a'\", Byte, \"a\");\n+        test!(\"\\\"a\\\"\", Str_, \"a\");\n+        test!(\"b\\\"a\\\"\", Binary, \"a\");\n+        test!(\"1234\", Integer, \"1234\");\n+        test!(\"0b101\", Integer, \"0b101\");\n+        test!(\"0xABC\", Integer, \"0xABC\");\n+        test!(\"1.0\", Float, \"1.0\");\n+        test!(\"1.0e10\", Float, \"1.0e10\");\n+\n+        assert_eq!(setup(&mk_sh(), \"2u\".to_string()).next_token().tok,\n+                   token::Literal(token::Integer(token::intern(\"2\")),\n+                                  Some(token::intern(\"u\"))));\n+        assert_eq!(setup(&mk_sh(), \"r###\\\"raw\\\"###suffix\".to_string()).next_token().tok,\n+                   token::Literal(token::StrRaw(token::intern(\"raw\"), 3),\n+                                  Some(token::intern(\"suffix\"))));\n+        assert_eq!(setup(&mk_sh(), \"br###\\\"raw\\\"###suffix\".to_string()).next_token().tok,\n+                   token::Literal(token::BinaryRaw(token::intern(\"raw\"), 3),\n+                                  Some(token::intern(\"suffix\"))));\n     }\n \n     #[test] fn line_doc_comments() {\n@@ -1573,7 +1564,7 @@ mod test {\n             token::Comment => { },\n             _ => panic!(\"expected a comment!\")\n         }\n-        assert_eq!(lexer.next_token().tok, token::LitChar(token::intern(\"a\")));\n+        assert_eq!(lexer.next_token().tok, token::Literal(token::Char(token::intern(\"a\")), None));\n     }\n \n }"}, {"sha": "96659031e6a37db4aa2c7bbfd424d1d84df8c124", "filename": "src/libsyntax/parse/mod.rs", "status": "modified", "additions": 76, "deletions": 69, "changes": 145, "blob_url": "https://github.com/rust-lang/rust/blob/f71b852d3865e878b953cb280724fb2ce1203103/src%2Flibsyntax%2Fparse%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/f71b852d3865e878b953cb280724fb2ce1203103/src%2Flibsyntax%2Fparse%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Fmod.rs?ref=f71b852d3865e878b953cb280724fb2ce1203103", "patch": "@@ -511,28 +511,41 @@ pub fn raw_str_lit(lit: &str) -> String {\n     res\n }\n \n-pub fn float_lit(s: &str) -> ast::Lit_ {\n-    debug!(\"float_lit: {}\", s);\n-    // FIXME #2252: bounds checking float literals is defered until trans\n-    let s2 = s.chars().filter(|&c| c != '_').collect::<String>();\n-    let s = s2.as_slice();\n-\n-    let mut ty = None;\n-\n-    if s.ends_with(\"f32\") {\n-        ty = Some(ast::TyF32);\n-    } else if s.ends_with(\"f64\") {\n-        ty = Some(ast::TyF64);\n-    }\n+// check if `s` looks like i32 or u1234 etc.\n+fn looks_like_width_suffix(first_chars: &[char], s: &str) -> bool {\n+    s.len() > 1 &&\n+        first_chars.contains(&s.char_at(0)) &&\n+        s.slice_from(1).chars().all(|c| '0' <= c && c <= '9')\n+}\n \n+fn filtered_float_lit(data: token::InternedString, suffix: Option<&str>,\n+                      sd: &SpanHandler, sp: Span) -> ast::Lit_ {\n+    debug!(\"filtered_float_lit: {}, {}\", data, suffix);\n+    match suffix {\n+        Some(\"f32\") => ast::LitFloat(data, ast::TyF32),\n+        Some(\"f64\") => ast::LitFloat(data, ast::TyF64),\n+        Some(suf) => {\n+            if suf.len() >= 2 && looks_like_width_suffix(&['f'], suf) {\n+                // if it looks like a width, lets try to be helpful.\n+                sd.span_err(sp, &*format!(\"illegal width `{}` for float literal, \\\n+                                          valid widths are 32 and 64\", suf.slice_from(1)));\n+            } else {\n+                sd.span_err(sp, &*format!(\"illegal suffix `{}` for float literal, \\\n+                                          valid suffixes are `f32` and `f64`\", suf));\n+            }\n \n-    match ty {\n-        Some(t) => {\n-            ast::LitFloat(token::intern_and_get_ident(s.slice_to(s.len() - t.suffix_len())), t)\n-        },\n-        None => ast::LitFloatUnsuffixed(token::intern_and_get_ident(s))\n+            ast::LitFloatUnsuffixed(data)\n+        }\n+        None => ast::LitFloatUnsuffixed(data)\n     }\n }\n+pub fn float_lit(s: &str, suffix: Option<&str>, sd: &SpanHandler, sp: Span) -> ast::Lit_ {\n+    debug!(\"float_lit: {}, {}\", s, suffix);\n+    // FIXME #2252: bounds checking float literals is defered until trans\n+    let s = s.chars().filter(|&c| c != '_').collect::<String>();\n+    let data = token::intern_and_get_ident(&*s);\n+    filtered_float_lit(data, suffix, sd, sp)\n+}\n \n /// Parse a string representing a byte literal into its final form. Similar to `char_lit`\n pub fn byte_lit(lit: &str) -> (u8, uint) {\n@@ -626,24 +639,19 @@ pub fn binary_lit(lit: &str) -> Rc<Vec<u8>> {\n     Rc::new(res)\n }\n \n-pub fn integer_lit(s: &str, sd: &SpanHandler, sp: Span) -> ast::Lit_ {\n+pub fn integer_lit(s: &str, suffix: Option<&str>, sd: &SpanHandler, sp: Span) -> ast::Lit_ {\n     // s can only be ascii, byte indexing is fine\n \n     let s2 = s.chars().filter(|&c| c != '_').collect::<String>();\n     let mut s = s2.as_slice();\n \n-    debug!(\"parse_integer_lit: {}\", s);\n-\n-    if s.len() == 1 {\n-        let n = (s.char_at(0)).to_digit(10).unwrap();\n-        return ast::LitInt(n as u64, ast::UnsuffixedIntLit(ast::Sign::new(n)));\n-    }\n+    debug!(\"integer_lit: {}, {}\", s, suffix);\n \n     let mut base = 10;\n     let orig = s;\n     let mut ty = ast::UnsuffixedIntLit(ast::Plus);\n \n-    if s.char_at(0) == '0' {\n+    if s.char_at(0) == '0' && s.len() > 1 {\n         match s.char_at(1) {\n             'x' => base = 16,\n             'o' => base = 8,\n@@ -652,57 +660,56 @@ pub fn integer_lit(s: &str, sd: &SpanHandler, sp: Span) -> ast::Lit_ {\n         }\n     }\n \n+    // 1f64 and 2f32 etc. are valid float literals.\n+    match suffix {\n+        Some(suf) if looks_like_width_suffix(&['f'], suf) => {\n+            match base {\n+                16u => sd.span_err(sp, \"hexadecimal float literal is not supported\"),\n+                8u => sd.span_err(sp, \"octal float literal is not supported\"),\n+                2u => sd.span_err(sp, \"binary float literal is not supported\"),\n+                _ => ()\n+            }\n+            let ident = token::intern_and_get_ident(&*s);\n+            return filtered_float_lit(ident, suffix, sd, sp)\n+        }\n+        _ => {}\n+    }\n+\n     if base != 10 {\n         s = s.slice_from(2);\n     }\n \n-    let last = s.len() - 1;\n-    match s.char_at(last) {\n-        'i' => ty = ast::SignedIntLit(ast::TyI, ast::Plus),\n-        'u' => ty = ast::UnsignedIntLit(ast::TyU),\n-        '8' => {\n-            if s.len() > 2 {\n-                match s.char_at(last - 1) {\n-                    'i' => ty = ast::SignedIntLit(ast::TyI8, ast::Plus),\n-                    'u' => ty = ast::UnsignedIntLit(ast::TyU8),\n-                    _ => { }\n-                }\n-            }\n-        },\n-        '6' => {\n-            if s.len() > 3 && s.char_at(last - 1) == '1' {\n-                match s.char_at(last - 2) {\n-                    'i' => ty = ast::SignedIntLit(ast::TyI16, ast::Plus),\n-                    'u' => ty = ast::UnsignedIntLit(ast::TyU16),\n-                    _ => { }\n-                }\n-            }\n-        },\n-        '2' => {\n-            if s.len() > 3 && s.char_at(last - 1) == '3' {\n-                match s.char_at(last - 2) {\n-                    'i' => ty = ast::SignedIntLit(ast::TyI32, ast::Plus),\n-                    'u' => ty = ast::UnsignedIntLit(ast::TyU32),\n-                    _ => { }\n-                }\n-            }\n-        },\n-        '4' => {\n-            if s.len() > 3 && s.char_at(last - 1) == '6' {\n-                match s.char_at(last - 2) {\n-                    'i' => ty = ast::SignedIntLit(ast::TyI64, ast::Plus),\n-                    'u' => ty = ast::UnsignedIntLit(ast::TyU64),\n-                    _ => { }\n+    if let Some(suf) = suffix {\n+        if suf.is_empty() { sd.span_bug(sp, \"found empty literal suffix in Some\")}\n+        ty = match suf {\n+            \"i\"   => ast::SignedIntLit(ast::TyI, ast::Plus),\n+            \"i8\"  => ast::SignedIntLit(ast::TyI8, ast::Plus),\n+            \"i16\" => ast::SignedIntLit(ast::TyI16, ast::Plus),\n+            \"i32\" => ast::SignedIntLit(ast::TyI32, ast::Plus),\n+            \"i64\" => ast::SignedIntLit(ast::TyI64, ast::Plus),\n+            \"u\"   => ast::UnsignedIntLit(ast::TyU),\n+            \"u8\"  => ast::UnsignedIntLit(ast::TyU8),\n+            \"u16\" => ast::UnsignedIntLit(ast::TyU16),\n+            \"u32\" => ast::UnsignedIntLit(ast::TyU32),\n+            \"u64\" => ast::UnsignedIntLit(ast::TyU64),\n+            _ => {\n+                // i<digits> and u<digits> look like widths, so lets\n+                // give an error message along those lines\n+                if looks_like_width_suffix(&['i', 'u'], suf) {\n+                    sd.span_err(sp, &*format!(\"illegal width `{}` for integer literal; \\\n+                                              valid widths are 8, 16, 32 and 64\",\n+                                              suf.slice_from(1)));\n+                } else {\n+                    sd.span_err(sp, &*format!(\"illegal suffix `{}` for numeric literal\", suf));\n                 }\n+\n+                ty\n             }\n-        },\n-        _ => { }\n+        }\n     }\n \n-    debug!(\"The suffix is {}, base {}, the new string is {}, the original \\\n-           string was {}\", ty, base, s, orig);\n-\n-    s = s.slice_to(s.len() - ty.suffix_len());\n+    debug!(\"integer_lit: the type is {}, base {}, the new string is {}, the original \\\n+           string was {}, the original suffix was {}\", ty, base, s, orig, suffix);\n \n     let res: u64 = match ::std::num::from_str_radix(s, base) {\n         Some(r) => r,"}, {"sha": "a6fe3902395454803910ace6547023216846afe3", "filename": "src/libsyntax/parse/parser.rs", "status": "modified", "additions": 92, "deletions": 30, "changes": 122, "blob_url": "https://github.com/rust-lang/rust/blob/f71b852d3865e878b953cb280724fb2ce1203103/src%2Flibsyntax%2Fparse%2Fparser.rs", "raw_url": "https://github.com/rust-lang/rust/raw/f71b852d3865e878b953cb280724fb2ce1203103/src%2Flibsyntax%2Fparse%2Fparser.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Fparser.rs?ref=f71b852d3865e878b953cb280724fb2ce1203103", "patch": "@@ -646,6 +646,20 @@ impl<'a> Parser<'a> {\n         }\n     }\n \n+    pub fn expect_no_suffix(&mut self, sp: Span, kind: &str, suffix: Option<ast::Name>) {\n+        match suffix {\n+            None => {/* everything ok */}\n+            Some(suf) => {\n+                let text = suf.as_str();\n+                if text.is_empty() {\n+                    self.span_bug(sp, \"found empty literal suffix in Some\")\n+                }\n+                self.span_err(sp, &*format!(\"{} with a suffix is illegal\", kind));\n+            }\n+        }\n+    }\n+\n+\n     /// Attempt to consume a `<`. If `<<` is seen, replace it with a single\n     /// `<` and continue. If a `<` is not seen, return false.\n     ///\n@@ -968,6 +982,9 @@ impl<'a> Parser<'a> {\n     pub fn span_err(&mut self, sp: Span, m: &str) {\n         self.sess.span_diagnostic.span_err(sp, m)\n     }\n+    pub fn span_bug(&mut self, sp: Span, m: &str) -> ! {\n+        self.sess.span_diagnostic.span_bug(sp, m)\n+    }\n     pub fn abort_if_errors(&mut self) {\n         self.sess.span_diagnostic.handler().abort_if_errors();\n     }\n@@ -1640,24 +1657,53 @@ impl<'a> Parser<'a> {\n     /// Matches token_lit = LIT_INTEGER | ...\n     pub fn lit_from_token(&mut self, tok: &token::Token) -> Lit_ {\n         match *tok {\n-            token::LitByte(i) => LitByte(parse::byte_lit(i.as_str()).val0()),\n-            token::LitChar(i) => LitChar(parse::char_lit(i.as_str()).val0()),\n-            token::LitInteger(s) => parse::integer_lit(s.as_str(),\n-                                                        &self.sess.span_diagnostic,\n-                                                       self.last_span),\n-            token::LitFloat(s) => parse::float_lit(s.as_str()),\n-            token::LitStr(s) => {\n-                LitStr(token::intern_and_get_ident(parse::str_lit(s.as_str()).as_slice()),\n-                       ast::CookedStr)\n-            }\n-            token::LitStrRaw(s, n) => {\n-                LitStr(token::intern_and_get_ident(parse::raw_str_lit(s.as_str()).as_slice()),\n-                       ast::RawStr(n))\n+            token::Literal(lit, suf) => {\n+                let (suffix_illegal, out) = match lit {\n+                    token::Byte(i) => (true, LitByte(parse::byte_lit(i.as_str()).val0())),\n+                    token::Char(i) => (true, LitChar(parse::char_lit(i.as_str()).val0())),\n+\n+                    // there are some valid suffixes for integer and\n+                    // float literals, so all the handling is done\n+                    // internally.\n+                    token::Integer(s) => {\n+                        (false, parse::integer_lit(s.as_str(),\n+                                                   suf.as_ref().map(|s| s.as_str()),\n+                                                   &self.sess.span_diagnostic,\n+                                                   self.last_span))\n+                    }\n+                    token::Float(s) => {\n+                        (false, parse::float_lit(s.as_str(),\n+                                                 suf.as_ref().map(|s| s.as_str()),\n+                                                  &self.sess.span_diagnostic,\n+                                                 self.last_span))\n+                    }\n+\n+                    token::Str_(s) => {\n+                        (true,\n+                         LitStr(token::intern_and_get_ident(parse::str_lit(s.as_str()).as_slice()),\n+                                ast::CookedStr))\n+                    }\n+                    token::StrRaw(s, n) => {\n+                        (true,\n+                         LitStr(\n+                            token::intern_and_get_ident(\n+                                parse::raw_str_lit(s.as_str()).as_slice()),\n+                            ast::RawStr(n)))\n+                    }\n+                    token::Binary(i) =>\n+                        (true, LitBinary(parse::binary_lit(i.as_str()))),\n+                    token::BinaryRaw(i, _) =>\n+                        (true,\n+                         LitBinary(Rc::new(i.as_str().as_bytes().iter().map(|&x| x).collect()))),\n+                };\n+\n+                if suffix_illegal {\n+                    let sp = self.last_span;\n+                    self.expect_no_suffix(sp, &*format!(\"{} literal\", lit.short_name()), suf)\n+                }\n+\n+                out\n             }\n-            token::LitBinary(i) =>\n-                LitBinary(parse::binary_lit(i.as_str())),\n-            token::LitBinaryRaw(i, _) =>\n-                LitBinary(Rc::new(i.as_str().as_bytes().iter().map(|&x| x).collect())),\n             _ => { self.unexpected_last(tok); }\n         }\n     }\n@@ -2424,7 +2470,10 @@ impl<'a> Parser<'a> {\n                         }\n                     }\n                   }\n-                  token::LitInteger(n) => {\n+                  token::Literal(token::Integer(n), suf) => {\n+                    let sp = self.span;\n+                    self.expect_no_suffix(sp, \"tuple index\", suf);\n+\n                     let index = n.as_str();\n                     let dot = self.last_span.hi;\n                     hi = self.span.hi;\n@@ -2449,7 +2498,7 @@ impl<'a> Parser<'a> {\n                         }\n                     }\n                   }\n-                  token::LitFloat(n) => {\n+                  token::Literal(token::Float(n), _suf) => {\n                     self.bump();\n                     let last_span = self.last_span;\n                     let fstr = n.as_str();\n@@ -5085,12 +5134,17 @@ impl<'a> Parser<'a> {\n                 self.expect(&token::Semi);\n                 (path, the_ident)\n             },\n-            token::LitStr(..) | token::LitStrRaw(..) => {\n-                let path = self.parse_str();\n+            token::Literal(token::Str_(..), suf) | token::Literal(token::StrRaw(..), suf) => {\n+                let sp = self.span;\n+                self.expect_no_suffix(sp, \"extern crate name\", suf);\n+                // forgo the internal suffix check of `parse_str` to\n+                // avoid repeats (this unwrap will always succeed due\n+                // to the restriction of the `match`)\n+                let (s, style, _) = self.parse_optional_str().unwrap();\n                 self.expect_keyword(keywords::As);\n                 let the_ident = self.parse_ident();\n                 self.expect(&token::Semi);\n-                (Some(path), the_ident)\n+                (Some((s, style)), the_ident)\n             },\n             _ => {\n                 let span = self.span;\n@@ -5267,7 +5321,9 @@ impl<'a> Parser<'a> {\n     /// the `extern` keyword, if one is found.\n     fn parse_opt_abi(&mut self) -> Option<abi::Abi> {\n         match self.token {\n-            token::LitStr(s) | token::LitStrRaw(s, _) => {\n+            token::Literal(token::Str_(s), suf) | token::Literal(token::StrRaw(s, _), suf) => {\n+                let sp = self.span;\n+                self.expect_no_suffix(sp, \"ABI spec\", suf);\n                 self.bump();\n                 let the_string = s.as_str();\n                 match abi::lookup(the_string) {\n@@ -5910,21 +5966,27 @@ impl<'a> Parser<'a> {\n     }\n \n     pub fn parse_optional_str(&mut self)\n-                              -> Option<(InternedString, ast::StrStyle)> {\n-        let (s, style) = match self.token {\n-            token::LitStr(s) => (self.id_to_interned_str(s.ident()), ast::CookedStr),\n-            token::LitStrRaw(s, n) => {\n-                (self.id_to_interned_str(s.ident()), ast::RawStr(n))\n+                              -> Option<(InternedString, ast::StrStyle, Option<ast::Name>)> {\n+        let ret = match self.token {\n+            token::Literal(token::Str_(s), suf) => {\n+                (self.id_to_interned_str(s.ident()), ast::CookedStr, suf)\n+            }\n+            token::Literal(token::StrRaw(s, n), suf) => {\n+                (self.id_to_interned_str(s.ident()), ast::RawStr(n), suf)\n             }\n             _ => return None\n         };\n         self.bump();\n-        Some((s, style))\n+        Some(ret)\n     }\n \n     pub fn parse_str(&mut self) -> (InternedString, StrStyle) {\n         match self.parse_optional_str() {\n-            Some(s) => { s }\n+            Some((s, style, suf)) => {\n+                let sp = self.last_span;\n+                self.expect_no_suffix(sp, \"str literal\", suf);\n+                (s, style)\n+            }\n             _ =>  self.fatal(\"expected string literal\")\n         }\n     }"}, {"sha": "4272b57a4dc5145f309f819802d4cf95929855ab", "filename": "src/libsyntax/parse/token.rs", "status": "modified", "additions": 30, "deletions": 25, "changes": 55, "blob_url": "https://github.com/rust-lang/rust/blob/f71b852d3865e878b953cb280724fb2ce1203103/src%2Flibsyntax%2Fparse%2Ftoken.rs", "raw_url": "https://github.com/rust-lang/rust/raw/f71b852d3865e878b953cb280724fb2ce1203103/src%2Flibsyntax%2Fparse%2Ftoken.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Ftoken.rs?ref=f71b852d3865e878b953cb280724fb2ce1203103", "patch": "@@ -12,6 +12,7 @@ pub use self::BinOpToken::*;\n pub use self::Nonterminal::*;\n pub use self::DelimToken::*;\n pub use self::IdentStyle::*;\n+pub use self::Lit::*;\n pub use self::Token::*;\n \n use ast;\n@@ -59,6 +60,31 @@ pub enum IdentStyle {\n     Plain,\n }\n \n+#[deriving(Clone, Encodable, Decodable, PartialEq, Eq, Hash, Show)]\n+pub enum Lit {\n+    Byte(ast::Name),\n+    Char(ast::Name),\n+    Integer(ast::Name),\n+    Float(ast::Name),\n+    Str_(ast::Name),\n+    StrRaw(ast::Name, uint), /* raw str delimited by n hash symbols */\n+    Binary(ast::Name),\n+    BinaryRaw(ast::Name, uint), /* raw binary str delimited by n hash symbols */\n+}\n+\n+impl Lit {\n+    pub fn short_name(&self) -> &'static str {\n+        match *self {\n+            Byte(_) => \"byte\",\n+            Char(_) => \"char\",\n+            Integer(_) => \"integer\",\n+            Float(_) => \"float\",\n+            Str_(_) | StrRaw(..) => \"str\",\n+            Binary(_) | BinaryRaw(..) => \"binary str\"\n+        }\n+    }\n+}\n+\n #[allow(non_camel_case_types)]\n #[deriving(Clone, Encodable, Decodable, PartialEq, Eq, Hash, Show)]\n pub enum Token {\n@@ -98,14 +124,7 @@ pub enum Token {\n     CloseDelim(DelimToken),\n \n     /* Literals */\n-    LitByte(ast::Name),\n-    LitChar(ast::Name),\n-    LitInteger(ast::Name),\n-    LitFloat(ast::Name),\n-    LitStr(ast::Name),\n-    LitStrRaw(ast::Name, uint), /* raw str delimited by n hash symbols */\n-    LitBinary(ast::Name),\n-    LitBinaryRaw(ast::Name, uint), /* raw binary str delimited by n hash symbols */\n+    Literal(Lit, Option<ast::Name>),\n \n     /* Name components */\n     Ident(ast::Ident, IdentStyle),\n@@ -145,14 +164,7 @@ impl Token {\n             Ident(_, _)                 => true,\n             Underscore                  => true,\n             Tilde                       => true,\n-            LitByte(_)                  => true,\n-            LitChar(_)                  => true,\n-            LitInteger(_)               => true,\n-            LitFloat(_)                 => true,\n-            LitStr(_)                   => true,\n-            LitStrRaw(_, _)             => true,\n-            LitBinary(_)                => true,\n-            LitBinaryRaw(_, _)          => true,\n+            Literal(_, _)               => true,\n             Pound                       => true,\n             At                          => true,\n             Not                         => true,\n@@ -173,15 +185,8 @@ impl Token {\n     /// Returns `true` if the token is any literal\n     pub fn is_lit(&self) -> bool {\n         match *self {\n-            LitByte(_)          => true,\n-            LitChar(_)          => true,\n-            LitInteger(_)       => true,\n-            LitFloat(_)         => true,\n-            LitStr(_)           => true,\n-            LitStrRaw(_, _)     => true,\n-            LitBinary(_)        => true,\n-            LitBinaryRaw(_, _)  => true,\n-            _                   => false,\n+            Literal(_, _) => true,\n+            _          => false,\n         }\n     }\n "}, {"sha": "8e7804aaa713f4bcdb2836a8a1ba830ec4c118af", "filename": "src/libsyntax/print/pprust.rs", "status": "modified", "additions": 22, "deletions": 12, "changes": 34, "blob_url": "https://github.com/rust-lang/rust/blob/f71b852d3865e878b953cb280724fb2ce1203103/src%2Flibsyntax%2Fprint%2Fpprust.rs", "raw_url": "https://github.com/rust-lang/rust/raw/f71b852d3865e878b953cb280724fb2ce1203103/src%2Flibsyntax%2Fprint%2Fpprust.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fprint%2Fpprust.rs?ref=f71b852d3865e878b953cb280724fb2ce1203103", "patch": "@@ -236,18 +236,28 @@ pub fn token_to_string(tok: &Token) -> String {\n         token::Question             => \"?\".into_string(),\n \n         /* Literals */\n-        token::LitByte(b)           => format!(\"b'{}'\", b.as_str()),\n-        token::LitChar(c)           => format!(\"'{}'\", c.as_str()),\n-        token::LitFloat(c)          => c.as_str().into_string(),\n-        token::LitInteger(c)        => c.as_str().into_string(),\n-        token::LitStr(s)            => format!(\"\\\"{}\\\"\", s.as_str()),\n-        token::LitStrRaw(s, n)      => format!(\"r{delim}\\\"{string}\\\"{delim}\",\n-                                               delim=\"#\".repeat(n),\n-                                               string=s.as_str()),\n-        token::LitBinary(v)         => format!(\"b\\\"{}\\\"\", v.as_str()),\n-        token::LitBinaryRaw(s, n)   => format!(\"br{delim}\\\"{string}\\\"{delim}\",\n-                                               delim=\"#\".repeat(n),\n-                                               string=s.as_str()),\n+        token::Literal(lit, suf) => {\n+            let mut out = match lit {\n+                token::Byte(b)           => format!(\"b'{}'\", b.as_str()),\n+                token::Char(c)           => format!(\"'{}'\", c.as_str()),\n+                token::Float(c)          => c.as_str().into_string(),\n+                token::Integer(c)        => c.as_str().into_string(),\n+                token::Str_(s)           => format!(\"\\\"{}\\\"\", s.as_str()),\n+                token::StrRaw(s, n)      => format!(\"r{delim}\\\"{string}\\\"{delim}\",\n+                                                    delim=\"#\".repeat(n),\n+                                                    string=s.as_str()),\n+                token::Binary(v)         => format!(\"b\\\"{}\\\"\", v.as_str()),\n+                token::BinaryRaw(s, n)   => format!(\"br{delim}\\\"{string}\\\"{delim}\",\n+                                                    delim=\"#\".repeat(n),\n+                                                    string=s.as_str()),\n+            };\n+\n+            if let Some(s) = suf {\n+                out.push_str(s.as_str())\n+            }\n+\n+            out\n+        }\n \n         /* Name components */\n         token::Ident(s, _)          => token::get_ident(s).get().into_string(),"}, {"sha": "e142365a8ca078014f720c454d6c3d242682a5f7", "filename": "src/test/compile-fail/bad-lit-suffixes.rs", "status": "added", "additions": 41, "deletions": 0, "changes": 41, "blob_url": "https://github.com/rust-lang/rust/blob/f71b852d3865e878b953cb280724fb2ce1203103/src%2Ftest%2Fcompile-fail%2Fbad-lit-suffixes.rs", "raw_url": "https://github.com/rust-lang/rust/raw/f71b852d3865e878b953cb280724fb2ce1203103/src%2Ftest%2Fcompile-fail%2Fbad-lit-suffixes.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ftest%2Fcompile-fail%2Fbad-lit-suffixes.rs?ref=f71b852d3865e878b953cb280724fb2ce1203103", "patch": "@@ -0,0 +1,41 @@\n+// Copyright 2014 The Rust Project Developers. See the COPYRIGHT\n+// file at the top-level directory of this distribution and at\n+// http://rust-lang.org/COPYRIGHT.\n+//\n+// Licensed under the Apache License, Version 2.0 <LICENSE-APACHE or\n+// http://www.apache.org/licenses/LICENSE-2.0> or the MIT license\n+// <LICENSE-MIT or http://opensource.org/licenses/MIT>, at your\n+// option. This file may not be copied, modified, or distributed\n+// except according to those terms.\n+\n+\n+extern crate\n+    \"foo\"suffix //~ ERROR extern crate name with a suffix is illegal\n+     as foo;\n+\n+extern\n+    \"C\"suffix //~ ERROR ABI spec with a suffix is illegal\n+    fn foo() {}\n+\n+extern\n+    \"C\"suffix //~ ERROR ABI spec with a suffix is illegal\n+{}\n+\n+fn main() {\n+    \"\"suffix; //~ ERROR str literal with a suffix is illegal\n+    b\"\"suffix; //~ ERROR binary str literal with a suffix is illegal\n+    r#\"\"#suffix; //~ ERROR str literal with a suffix is illegal\n+    br#\"\"#suffix; //~ ERROR binary str literal with a suffix is illegal\n+    'a'suffix; //~ ERROR char literal with a suffix is illegal\n+    b'a'suffix; //~ ERROR byte literal with a suffix is illegal\n+\n+    1234u1024; //~ ERROR illegal width `1024` for integer literal\n+    1234i1024; //~ ERROR illegal width `1024` for integer literal\n+    1234f1024; //~ ERROR illegal width `1024` for float literal\n+    1234.5f1024; //~ ERROR illegal width `1024` for float literal\n+\n+    1234suffix; //~ ERROR illegal suffix `suffix` for numeric literal\n+    0b101suffix; //~ ERROR illegal suffix `suffix` for numeric literal\n+    1.0suffix; //~ ERROR illegal suffix `suffix` for numeric literal\n+    1.0e10suffix; //~ ERROR illegal suffix `suffix` for numeric literal\n+}"}]}