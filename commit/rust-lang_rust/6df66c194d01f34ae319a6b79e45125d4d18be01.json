{"sha": "6df66c194d01f34ae319a6b79e45125d4d18be01", "node_id": "MDY6Q29tbWl0NzI0NzEyOjZkZjY2YzE5NGQwMWYzNGFlMzE5YTZiNzllNDUxMjVkNGQxOGJlMDE=", "commit": {"author": {"name": "bors", "email": "bors@rust-lang.org", "date": "2013-06-15T11:07:03Z"}, "committer": {"name": "bors", "email": "bors@rust-lang.org", "date": "2013-06-15T11:07:03Z"}, "message": "auto merge of #7109 : bblum/rust/rwlocks, r=brson\n\nr? @brson\r\n\r\nlinks to issues: #7065 the race that's fixed; #7066 the perf improvement I added. There are also some minor cleanup commits here.\r\n\r\nTo measure the performance improvement from replacing the exclusive with an atomic uint, I edited the ```msgsend-ring-rw-arcs``` bench test to do a ```write_downgrade``` instead of just a ```write```, so that it stressed the code paths that accessed ```read_count```. (At first I was still using ```write``` and saw no performance difference whatsoever, whoooops.)\r\n\r\nThe bench test measures how long it takes to send 1,000,000 messages by using rwarcs to emulate pipes. I also measured the performance difference imposed by the fix to the ```access_lock``` race (which involves taking an extra semaphore in the ```cond.wait()``` path). The net result is that fixing the race imposes a 4% to 5% slowdown, but doing the atomic uint optimization gives a 6% to 8% speedup.\r\n\r\nNote that this speedup will be most visible in read- or downgrade-heavy workloads. If an RWARC's only users are writers, the optimization doesn't matter. All the same, I think this more than justifies the extra complexity I mentioned in #7066.\r\n\r\nThe raw numbers are:\r\n```\r\nwith xadd read count\r\n        before write_cond fix\r\n                4.18 to 4.26 us/message\r\n        with write_cond fix\r\n                4.35 to 4.39 us/message\r\n                \r\nwith exclusive read count\r\n        before write_cond fix\r\n                4.41 to 4.47 us/message\r\n        with write_cond fix\r\n                4.65 to 4.76 us/message\r\n```", "tree": {"sha": "11f7772a9f2841b6e9bcd770a6ade1c287ce1625", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/11f7772a9f2841b6e9bcd770a6ade1c287ce1625"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/6df66c194d01f34ae319a6b79e45125d4d18be01", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/6df66c194d01f34ae319a6b79e45125d4d18be01", "html_url": "https://github.com/rust-lang/rust/commit/6df66c194d01f34ae319a6b79e45125d4d18be01", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/6df66c194d01f34ae319a6b79e45125d4d18be01/comments", "author": {"login": "bors", "id": 3372342, "node_id": "MDQ6VXNlcjMzNzIzNDI=", "avatar_url": "https://avatars.githubusercontent.com/u/3372342?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bors", "html_url": "https://github.com/bors", "followers_url": "https://api.github.com/users/bors/followers", "following_url": "https://api.github.com/users/bors/following{/other_user}", "gists_url": "https://api.github.com/users/bors/gists{/gist_id}", "starred_url": "https://api.github.com/users/bors/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bors/subscriptions", "organizations_url": "https://api.github.com/users/bors/orgs", "repos_url": "https://api.github.com/users/bors/repos", "events_url": "https://api.github.com/users/bors/events{/privacy}", "received_events_url": "https://api.github.com/users/bors/received_events", "type": "User", "site_admin": false}, "committer": {"login": "bors", "id": 3372342, "node_id": "MDQ6VXNlcjMzNzIzNDI=", "avatar_url": "https://avatars.githubusercontent.com/u/3372342?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bors", "html_url": "https://github.com/bors", "followers_url": "https://api.github.com/users/bors/followers", "following_url": "https://api.github.com/users/bors/following{/other_user}", "gists_url": "https://api.github.com/users/bors/gists{/gist_id}", "starred_url": "https://api.github.com/users/bors/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bors/subscriptions", "organizations_url": "https://api.github.com/users/bors/orgs", "repos_url": "https://api.github.com/users/bors/repos", "events_url": "https://api.github.com/users/bors/events{/privacy}", "received_events_url": "https://api.github.com/users/bors/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "da42e6b7a0fcadcca819d221738894dcb6c4b76d", "url": "https://api.github.com/repos/rust-lang/rust/commits/da42e6b7a0fcadcca819d221738894dcb6c4b76d", "html_url": "https://github.com/rust-lang/rust/commit/da42e6b7a0fcadcca819d221738894dcb6c4b76d"}, {"sha": "2ef8774ac5b56ae264224d46ffa0078f5d39ce6c", "url": "https://api.github.com/repos/rust-lang/rust/commits/2ef8774ac5b56ae264224d46ffa0078f5d39ce6c", "html_url": "https://github.com/rust-lang/rust/commit/2ef8774ac5b56ae264224d46ffa0078f5d39ce6c"}], "stats": {"total": 319, "additions": 229, "deletions": 90}, "files": [{"sha": "b1bec1f95dbd72d991bae07229644c384848b21e", "filename": "src/libextra/arc.rs", "status": "modified", "additions": 67, "deletions": 7, "changes": 74, "blob_url": "https://github.com/rust-lang/rust/blob/6df66c194d01f34ae319a6b79e45125d4d18be01/src%2Flibextra%2Farc.rs", "raw_url": "https://github.com/rust-lang/rust/raw/6df66c194d01f34ae319a6b79e45125d4d18be01/src%2Flibextra%2Farc.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibextra%2Farc.rs?ref=6df66c194d01f34ae319a6b79e45125d4d18be01", "patch": "@@ -281,7 +281,6 @@ struct RWARCInner<T> { lock: RWlock, failed: bool, data: T }\n #[mutable]\n struct RWARC<T> {\n     x: UnsafeAtomicRcBox<RWARCInner<T>>,\n-    cant_nest: ()\n }\n \n /// Create a reader/writer ARC with the supplied data.\n@@ -299,15 +298,14 @@ pub fn rw_arc_with_condvars<T:Const + Owned>(\n     let data =\n         RWARCInner { lock: rwlock_with_condvars(num_condvars),\n                      failed: false, data: user_data };\n-    RWARC { x: UnsafeAtomicRcBox::new(data), cant_nest: () }\n+    RWARC { x: UnsafeAtomicRcBox::new(data), }\n }\n \n impl<T:Const + Owned> RWARC<T> {\n     /// Duplicate a rwlock-protected ARC, as arc::clone.\n     pub fn clone(&self) -> RWARC<T> {\n         RWARC {\n             x: self.x.clone(),\n-            cant_nest: (),\n         }\n     }\n \n@@ -382,12 +380,12 @@ impl<T:Const + Owned> RWARC<T> {\n      * # Example\n      *\n      * ~~~ {.rust}\n-     * do arc.write_downgrade |write_mode| {\n-     *     do (&write_mode).write_cond |state, condvar| {\n+     * do arc.write_downgrade |mut write_token| {\n+     *     do write_token.write_cond |state, condvar| {\n      *         ... exclusive access with mutable state ...\n      *     }\n-     *     let read_mode = arc.downgrade(write_mode);\n-     *     do (&read_mode).read |state| {\n+     *     let read_token = arc.downgrade(write_token);\n+     *     do read_token.read |state| {\n      *         ... shared access with immutable state ...\n      *     }\n      * }\n@@ -815,4 +813,66 @@ mod tests {\n \n         wp2.recv(); // complete handshake with writer\n     }\n+    #[cfg(test)]\n+    fn test_rw_write_cond_downgrade_read_race_helper() {\n+        // Tests that when a downgrader hands off the \"reader cloud\" lock\n+        // because of a contending reader, a writer can't race to get it\n+        // instead, which would result in readers_and_writers. This tests\n+        // the sync module rather than this one, but it's here because an\n+        // rwarc gives us extra shared state to help check for the race.\n+        // If you want to see this test fail, go to sync.rs and replace the\n+        // line in RWlock::write_cond() that looks like:\n+        //     \"blk(&Condvar { order: opt_lock, ..*cond })\"\n+        // with just \"blk(cond)\".\n+        let x = ~RWARC(true);\n+        let (wp, wc) = comm::stream();\n+\n+        // writer task\n+        let xw = (*x).clone();\n+        do task::spawn {\n+            do xw.write_cond |state, c| {\n+                wc.send(()); // tell downgrader it's ok to go\n+                c.wait();\n+                // The core of the test is here: the condvar reacquire path\n+                // must involve order_lock, so that it cannot race with a reader\n+                // trying to receive the \"reader cloud lock hand-off\".\n+                *state = false;\n+            }\n+        }\n+\n+        wp.recv(); // wait for writer to get in\n+\n+        do x.write_downgrade |mut write_mode| {\n+            do write_mode.write_cond |state, c| {\n+                assert!(*state);\n+                // make writer contend in the cond-reacquire path\n+                c.signal();\n+            }\n+            // make a reader task to trigger the \"reader cloud lock\" handoff\n+            let xr = (*x).clone();\n+            let (rp, rc) = comm::stream();\n+            do task::spawn {\n+                rc.send(());\n+                do xr.read |_state| { }\n+            }\n+            rp.recv(); // wait for reader task to exist\n+\n+            let read_mode = x.downgrade(write_mode);\n+            do read_mode.read |state| {\n+                // if writer mistakenly got in, make sure it mutates state\n+                // before we assert on it\n+                for 5.times { task::yield(); }\n+                // make sure writer didn't get in.\n+                assert!(*state);\n+            }\n+        }\n+    }\n+    #[test]\n+    fn test_rw_write_cond_downgrade_read_race() {\n+        // Ideally the above test case would have yield statements in it that\n+        // helped to expose the race nearly 100% of the time... but adding\n+        // yields in the intuitively-right locations made it even less likely,\n+        // and I wasn't sure why :( . This is a mediocre \"next best\" option.\n+        for 8.times { test_rw_write_cond_downgrade_read_race_helper() }\n+    }\n }"}, {"sha": "5930bf50ff7806d7972391df00f05b2b10002fcf", "filename": "src/libextra/sync.rs", "status": "modified", "additions": 154, "deletions": 79, "changes": 233, "blob_url": "https://github.com/rust-lang/rust/blob/6df66c194d01f34ae319a6b79e45125d4d18be01/src%2Flibextra%2Fsync.rs", "raw_url": "https://github.com/rust-lang/rust/raw/6df66c194d01f34ae319a6b79e45125d4d18be01/src%2Flibextra%2Fsync.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibextra%2Fsync.rs?ref=6df66c194d01f34ae319a6b79e45125d4d18be01", "patch": "@@ -20,7 +20,8 @@ use core::prelude::*;\n use core::borrow;\n use core::comm;\n use core::task;\n-use core::unstable::sync::{Exclusive, exclusive};\n+use core::unstable::sync::{Exclusive, exclusive, UnsafeAtomicRcBox};\n+use core::unstable::atomics;\n use core::util;\n \n /****************************************************************************\n@@ -37,6 +38,7 @@ type SignalEnd = comm::ChanOne<()>;\n struct Waitqueue { head: comm::Port<SignalEnd>,\n                    tail: comm::Chan<SignalEnd> }\n \n+#[doc(hidden)]\n fn new_waitqueue() -> Waitqueue {\n     let (block_head, block_tail) = comm::stream();\n     Waitqueue { head: block_head, tail: block_tail }\n@@ -166,31 +168,55 @@ impl Sem<~[Waitqueue]> {\n // FIXME(#3588) should go inside of access()\n #[doc(hidden)]\n type SemRelease<'self> = SemReleaseGeneric<'self, ()>;\n+#[doc(hidden)]\n type SemAndSignalRelease<'self> = SemReleaseGeneric<'self, ~[Waitqueue]>;\n+#[doc(hidden)]\n struct SemReleaseGeneric<'self, Q> { sem: &'self Sem<Q> }\n \n+#[doc(hidden)]\n #[unsafe_destructor]\n impl<'self, Q:Owned> Drop for SemReleaseGeneric<'self, Q> {\n     fn finalize(&self) {\n         self.sem.release();\n     }\n }\n \n+#[doc(hidden)]\n fn SemRelease<'r>(sem: &'r Sem<()>) -> SemRelease<'r> {\n     SemReleaseGeneric {\n         sem: sem\n     }\n }\n \n+#[doc(hidden)]\n fn SemAndSignalRelease<'r>(sem: &'r Sem<~[Waitqueue]>)\n                         -> SemAndSignalRelease<'r> {\n     SemReleaseGeneric {\n         sem: sem\n     }\n }\n \n+// FIXME(#3598): Want to use an Option down below, but we need a custom enum\n+// that's not polymorphic to get around the fact that lifetimes are invariant\n+// inside of type parameters.\n+enum ReacquireOrderLock<'self> {\n+    Nothing, // c.c\n+    Just(&'self Semaphore),\n+}\n+\n /// A mechanism for atomic-unlock-and-deschedule blocking and signalling.\n-pub struct Condvar<'self> { priv sem: &'self Sem<~[Waitqueue]> }\n+pub struct Condvar<'self> {\n+    // The 'Sem' object associated with this condvar. This is the one that's\n+    // atomically-unlocked-and-descheduled upon and reacquired during wakeup.\n+    priv sem: &'self Sem<~[Waitqueue]>,\n+    // This is (can be) an extra semaphore which is held around the reacquire\n+    // operation on the first one. This is only used in cvars associated with\n+    // rwlocks, and is needed to ensure that, when a downgrader is trying to\n+    // hand off the access lock (which would be the first field, here), a 2nd\n+    // writer waking up from a cvar wait can't race with a reader to steal it,\n+    // See the comment in write_cond for more detail.\n+    priv order: ReacquireOrderLock<'self>,\n+}\n \n #[unsafe_destructor]\n impl<'self> Drop for Condvar<'self> { fn finalize(&self) {} }\n@@ -247,7 +273,8 @@ impl<'self> Condvar<'self> {\n                 // unkillably reacquire the lock needs to happen atomically\n                 // wrt enqueuing.\n                 if out_of_bounds.is_none() {\n-                    reacquire = Some(SemAndSignalReacquire(self.sem));\n+                    reacquire = Some(CondvarReacquire { sem:   self.sem,\n+                                                        order: self.order });\n                 }\n             }\n         }\n@@ -261,28 +288,29 @@ impl<'self> Condvar<'self> {\n         // This is needed for a failing condition variable to reacquire the\n         // mutex during unwinding. As long as the wrapper (mutex, etc) is\n         // bounded in when it gets released, this shouldn't hang forever.\n-        struct SemAndSignalReacquire<'self> {\n+        struct CondvarReacquire<'self> {\n             sem: &'self Sem<~[Waitqueue]>,\n+            order: ReacquireOrderLock<'self>,\n         }\n \n         #[unsafe_destructor]\n-        impl<'self> Drop for SemAndSignalReacquire<'self> {\n+        impl<'self> Drop for CondvarReacquire<'self> {\n             fn finalize(&self) {\n                 unsafe {\n                     // Needs to succeed, instead of itself dying.\n                     do task::unkillable {\n-                        self.sem.acquire();\n+                        match self.order {\n+                            Just(lock) => do lock.access {\n+                                self.sem.acquire();\n+                            },\n+                            Nothing => {\n+                                self.sem.acquire();\n+                            },\n+                        }\n                     }\n                 }\n             }\n         }\n-\n-        fn SemAndSignalReacquire<'r>(sem: &'r Sem<~[Waitqueue]>)\n-                                  -> SemAndSignalReacquire<'r> {\n-            SemAndSignalReacquire {\n-                sem: sem\n-            }\n-        }\n     }\n \n     /// Wake up a blocked task. Returns false if there was no blocked task.\n@@ -350,9 +378,12 @@ fn check_cvar_bounds<U>(out_of_bounds: Option<uint>, id: uint, act: &str,\n \n #[doc(hidden)]\n impl Sem<~[Waitqueue]> {\n-    // The only other place that condvars get built is rwlock_write_mode.\n+    // The only other places that condvars get built are rwlock.write_cond()\n+    // and rwlock_write_mode.\n     pub fn access_cond<U>(&self, blk: &fn(c: &Condvar) -> U) -> U {\n-        do self.access { blk(&Condvar { sem: self }) }\n+        do self.access {\n+            blk(&Condvar { sem: self, order: Nothing })\n+        }\n     }\n }\n \n@@ -441,8 +472,23 @@ impl Mutex {\n \n #[doc(hidden)]\n struct RWlockInner {\n+    // You might ask, \"Why don't you need to use an atomic for the mode flag?\"\n+    // This flag affects the behaviour of readers (for plain readers, they\n+    // assert on it; for downgraders, they use it to decide which mode to\n+    // unlock for). Consider that the flag is only unset when the very last\n+    // reader exits; therefore, it can never be unset during a reader/reader\n+    // (or reader/downgrader) race.\n+    // By the way, if we didn't care about the assert in the read unlock path,\n+    // we could instead store the mode flag in write_downgrade's stack frame,\n+    // and have the downgrade tokens store a borrowed pointer to it.\n     read_mode:  bool,\n-    read_count: uint\n+    // The only way the count flag is ever accessed is with xadd. Since it is\n+    // a read-modify-write operation, multiple xadds on different cores will\n+    // always be consistent with respect to each other, so a monotonic/relaxed\n+    // consistency ordering suffices (i.e., no extra barriers are needed).\n+    // FIXME(#6598): The atomics module has no relaxed ordering flag, so I use\n+    // acquire/release orderings superfluously. Change these someday.\n+    read_count: atomics::AtomicUint,\n }\n \n /**\n@@ -455,7 +501,7 @@ struct RWlockInner {\n pub struct RWlock {\n     priv order_lock:  Semaphore,\n     priv access_lock: Sem<~[Waitqueue]>,\n-    priv state:       Exclusive<RWlockInner>\n+    priv state:       UnsafeAtomicRcBox<RWlockInner>,\n }\n \n /// Create a new rwlock, with one associated condvar.\n@@ -466,10 +512,13 @@ pub fn RWlock() -> RWlock { rwlock_with_condvars(1) }\n  * Similar to mutex_with_condvars.\n  */\n pub fn rwlock_with_condvars(num_condvars: uint) -> RWlock {\n-    RWlock { order_lock: semaphore(1),\n+    let state = UnsafeAtomicRcBox::new(RWlockInner {\n+        read_mode:  false,\n+        read_count: atomics::AtomicUint::new(0),\n+    });\n+    RWlock { order_lock:  semaphore(1),\n              access_lock: new_sem_and_signal(1, num_condvars),\n-             state: exclusive(RWlockInner { read_mode:  false,\n-                                             read_count: 0 }) }\n+             state:       state, }\n }\n \n impl RWlock {\n@@ -489,20 +538,11 @@ impl RWlock {\n         unsafe {\n             do task::unkillable {\n                 do (&self.order_lock).access {\n-                    let mut first_reader = false;\n-                    do self.state.with |state| {\n-                        first_reader = (state.read_count == 0);\n-                        state.read_count += 1;\n-                    }\n-                    if first_reader {\n+                    let state = &mut *self.state.get();\n+                    let old_count = state.read_count.fetch_add(1, atomics::Acquire);\n+                    if old_count == 0 {\n                         (&self.access_lock).acquire();\n-                        do self.state.with |state| {\n-                            // Must happen *after* getting access_lock. If\n-                            // this is set while readers are waiting, but\n-                            // while a writer holds the lock, the writer will\n-                            // be confused if they downgrade-then-unlock.\n-                            state.read_mode = true;\n-                        }\n+                        state.read_mode = true;\n                     }\n                 }\n                 release = Some(RWlockReleaseRead(self));\n@@ -534,17 +574,41 @@ impl RWlock {\n      * was signalled might reacquire the lock before other waiting writers.)\n      */\n     pub fn write_cond<U>(&self, blk: &fn(c: &Condvar) -> U) -> U {\n-        // NB: You might think I should thread the order_lock into the cond\n-        // wait call, so that it gets waited on before access_lock gets\n-        // reacquired upon being woken up. However, (a) this would be not\n-        // pleasant to implement (and would mandate a new 'rw_cond' type) and\n-        // (b) I think violating no-starvation in that case is appropriate.\n+        // It's important to thread our order lock into the condvar, so that\n+        // when a cond.wait() wakes up, it uses it while reacquiring the\n+        // access lock. If we permitted a waking-up writer to \"cut in line\",\n+        // there could arise a subtle race when a downgrader attempts to hand\n+        // off the reader cloud lock to a waiting reader. This race is tested\n+        // in arc.rs (test_rw_write_cond_downgrade_read_race) and looks like:\n+        // T1 (writer)              T2 (downgrader)             T3 (reader)\n+        // [in cond.wait()]\n+        //                          [locks for writing]\n+        //                          [holds access_lock]\n+        // [is signalled, perhaps by\n+        //  downgrader or a 4th thread]\n+        // tries to lock access(!)\n+        //                                                      lock order_lock\n+        //                                                      xadd read_count[0->1]\n+        //                                                      tries to lock access\n+        //                          [downgrade]\n+        //                          xadd read_count[1->2]\n+        //                          unlock access\n+        // Since T1 contended on the access lock before T3 did, it will steal\n+        // the lock handoff. Adding order_lock in the condvar reacquire path\n+        // solves this because T1 will hold order_lock while waiting on access,\n+        // which will cause T3 to have to wait until T1 finishes its write,\n+        // which can't happen until T2 finishes the downgrade-read entirely.\n+        // The astute reader will also note that making waking writers use the\n+        // order_lock is better for not starving readers.\n         unsafe {\n             do task::unkillable {\n                 (&self.order_lock).acquire();\n                 do (&self.access_lock).access_cond |cond| {\n                     (&self.order_lock).release();\n-                    do task::rekillable { blk(cond) }\n+                    do task::rekillable {\n+                        let opt_lock = Just(&self.order_lock);\n+                        blk(&Condvar { order: opt_lock, ..*cond })\n+                    }\n                 }\n             }\n         }\n@@ -560,12 +624,12 @@ impl RWlock {\n      * # Example\n      *\n      * ~~~ {.rust}\n-     * do lock.write_downgrade |write_mode| {\n-     *     do (&write_mode).write_cond |condvar| {\n+     * do lock.write_downgrade |mut write_token| {\n+     *     do write_token.write_cond |condvar| {\n      *         ... exclusive access ...\n      *     }\n-     *     let read_mode = lock.downgrade(write_mode);\n-     *     do (&read_mode).read {\n+     *     let read_token = lock.downgrade(write_token);\n+     *     do read_token.read {\n      *         ... shared access ...\n      *     }\n      * }\n@@ -594,18 +658,20 @@ impl RWlock {\n         }\n         unsafe {\n             do task::unkillable {\n-                let mut first_reader = false;\n-                do self.state.with |state| {\n-                    assert!(!state.read_mode);\n-                    state.read_mode = true;\n-                    first_reader = (state.read_count == 0);\n-                    state.read_count += 1;\n-                }\n-                if !first_reader {\n+                let state = &mut *self.state.get();\n+                assert!(!state.read_mode);\n+                state.read_mode = true;\n+                // If a reader attempts to enter at this point, both the\n+                // downgrader and reader will set the mode flag. This is fine.\n+                let old_count = state.read_count.fetch_add(1, atomics::Release);\n+                // If another reader was already blocking, we need to hand-off\n+                // the \"reader cloud\" access lock to them.\n+                if old_count != 0 {\n                     // Guaranteed not to let another writer in, because\n                     // another reader was holding the order_lock. Hence they\n                     // must be the one to get the access_lock (because all\n-                    // access_locks are acquired with order_lock held).\n+                    // access_locks are acquired with order_lock held). See\n+                    // the comment in write_cond for more justification.\n                     (&self.access_lock).release();\n                 }\n             }\n@@ -620,29 +686,30 @@ struct RWlockReleaseRead<'self> {\n     lock: &'self RWlock,\n }\n \n+#[doc(hidden)]\n #[unsafe_destructor]\n impl<'self> Drop for RWlockReleaseRead<'self> {\n     fn finalize(&self) {\n         unsafe {\n             do task::unkillable {\n-                let mut last_reader = false;\n-                do self.lock.state.with |state| {\n-                    assert!(state.read_mode);\n-                    assert!(state.read_count > 0);\n-                    state.read_count -= 1;\n-                    if state.read_count == 0 {\n-                        last_reader = true;\n-                        state.read_mode = false;\n-                    }\n-                }\n-                if last_reader {\n+                let state = &mut *self.lock.state.get();\n+                assert!(state.read_mode);\n+                let old_count = state.read_count.fetch_sub(1, atomics::Release);\n+                assert!(old_count > 0);\n+                if old_count == 1 {\n+                    state.read_mode = false;\n+                    // Note: this release used to be outside of a locked access\n+                    // to exclusive-protected state. If this code is ever\n+                    // converted back to such (instead of using atomic ops),\n+                    // this access MUST NOT go inside the exclusive access.\n                     (&self.lock.access_lock).release();\n                 }\n             }\n         }\n     }\n }\n \n+#[doc(hidden)]\n fn RWlockReleaseRead<'r>(lock: &'r RWlock) -> RWlockReleaseRead<'r> {\n     RWlockReleaseRead {\n         lock: lock\n@@ -656,37 +723,42 @@ struct RWlockReleaseDowngrade<'self> {\n     lock: &'self RWlock,\n }\n \n+#[doc(hidden)]\n #[unsafe_destructor]\n impl<'self> Drop for RWlockReleaseDowngrade<'self> {\n     fn finalize(&self) {\n         unsafe {\n             do task::unkillable {\n-                let mut writer_or_last_reader = false;\n-                do self.lock.state.with |state| {\n-                    if state.read_mode {\n-                        assert!(state.read_count > 0);\n-                        state.read_count -= 1;\n-                        if state.read_count == 0 {\n-                            // Case 1: Writer downgraded & was the last reader\n-                            writer_or_last_reader = true;\n-                            state.read_mode = false;\n-                        } else {\n-                            // Case 2: Writer downgraded & was not the last\n-                            // reader\n-                        }\n-                    } else {\n-                        // Case 3: Writer did not downgrade\n+                let writer_or_last_reader;\n+                // Check if we're releasing from read mode or from write mode.\n+                let state = &mut *self.lock.state.get();\n+                if state.read_mode {\n+                    // Releasing from read mode.\n+                    let old_count = state.read_count.fetch_sub(1, atomics::Release);\n+                    assert!(old_count > 0);\n+                    // Check if other readers remain.\n+                    if old_count == 1 {\n+                        // Case 1: Writer downgraded & was the last reader\n                         writer_or_last_reader = true;\n+                        state.read_mode = false;\n+                    } else {\n+                        // Case 2: Writer downgraded & was not the last reader\n+                        writer_or_last_reader = false;\n                     }\n+                } else {\n+                    // Case 3: Writer did not downgrade\n+                    writer_or_last_reader = true;\n                 }\n                 if writer_or_last_reader {\n+                    // Nobody left inside; release the \"reader cloud\" lock.\n                     (&self.lock.access_lock).release();\n                 }\n             }\n         }\n     }\n }\n \n+#[doc(hidden)]\n fn RWlockReleaseDowngrade<'r>(lock: &'r RWlock)\n                            -> RWlockReleaseDowngrade<'r> {\n     RWlockReleaseDowngrade {\n@@ -709,7 +781,10 @@ impl<'self> RWlockWriteMode<'self> {\n     pub fn write<U>(&self, blk: &fn() -> U) -> U { blk() }\n     /// Access the pre-downgrade rwlock in write mode with a condvar.\n     pub fn write_cond<U>(&self, blk: &fn(c: &Condvar) -> U) -> U {\n-        blk(&Condvar { sem: &self.lock.access_lock })\n+        // Need to make the condvar use the order lock when reacquiring the\n+        // access lock. See comment in RWlock::write_cond for why.\n+        blk(&Condvar { sem:        &self.lock.access_lock,\n+                       order: Just(&self.lock.order_lock), })\n     }\n }\n "}, {"sha": "aa70897ad486f60d6ebe4713e97bd38342e1df09", "filename": "src/libstd/unstable/atomics.rs", "status": "modified", "additions": 6, "deletions": 0, "changes": 6, "blob_url": "https://github.com/rust-lang/rust/blob/6df66c194d01f34ae319a6b79e45125d4d18be01/src%2Flibstd%2Funstable%2Fatomics.rs", "raw_url": "https://github.com/rust-lang/rust/raw/6df66c194d01f34ae319a6b79e45125d4d18be01/src%2Flibstd%2Funstable%2Fatomics.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibstd%2Funstable%2Fatomics.rs?ref=6df66c194d01f34ae319a6b79e45125d4d18be01", "patch": "@@ -155,11 +155,13 @@ impl AtomicInt {\n         unsafe { atomic_compare_and_swap(&mut self.v, old, new, order) }\n     }\n \n+    /// Returns the old value (like __sync_fetch_and_add).\n     #[inline(always)]\n     pub fn fetch_add(&mut self, val: int, order: Ordering) -> int {\n         unsafe { atomic_add(&mut self.v, val, order) }\n     }\n \n+    /// Returns the old value (like __sync_fetch_and_sub).\n     #[inline(always)]\n     pub fn fetch_sub(&mut self, val: int, order: Ordering) -> int {\n         unsafe { atomic_sub(&mut self.v, val, order) }\n@@ -191,11 +193,13 @@ impl AtomicUint {\n         unsafe { atomic_compare_and_swap(&mut self.v, old, new, order) }\n     }\n \n+    /// Returns the old value (like __sync_fetch_and_add).\n     #[inline(always)]\n     pub fn fetch_add(&mut self, val: uint, order: Ordering) -> uint {\n         unsafe { atomic_add(&mut self.v, val, order) }\n     }\n \n+    /// Returns the old value (like __sync_fetch_and_sub)..\n     #[inline(always)]\n     pub fn fetch_sub(&mut self, val: uint, order: Ordering) -> uint {\n         unsafe { atomic_sub(&mut self.v, val, order) }\n@@ -315,6 +319,7 @@ pub unsafe fn atomic_swap<T>(dst: &mut T, val: T, order: Ordering) -> T {\n     })\n }\n \n+/// Returns the old value (like __sync_fetch_and_add).\n #[inline(always)]\n pub unsafe fn atomic_add<T>(dst: &mut T, val: T, order: Ordering) -> T {\n     let dst = cast::transmute(dst);\n@@ -327,6 +332,7 @@ pub unsafe fn atomic_add<T>(dst: &mut T, val: T, order: Ordering) -> T {\n     })\n }\n \n+/// Returns the old value (like __sync_fetch_and_sub).\n #[inline(always)]\n pub unsafe fn atomic_sub<T>(dst: &mut T, val: T, order: Ordering) -> T {\n     let dst = cast::transmute(dst);"}, {"sha": "61c284f580c0feebb43114b61b4bd0e4a7bcbfd4", "filename": "src/libstd/util.rs", "status": "modified", "additions": 2, "deletions": 4, "changes": 6, "blob_url": "https://github.com/rust-lang/rust/blob/6df66c194d01f34ae319a6b79e45125d4d18be01/src%2Flibstd%2Futil.rs", "raw_url": "https://github.com/rust-lang/rust/raw/6df66c194d01f34ae319a6b79e45125d4d18be01/src%2Flibstd%2Futil.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibstd%2Futil.rs?ref=6df66c194d01f34ae319a6b79e45125d4d18be01", "patch": "@@ -75,13 +75,11 @@ pub fn replace<T>(dest: &mut T, mut src: T) -> T {\n }\n \n /// A non-copyable dummy type.\n-pub struct NonCopyable {\n-    priv i: (),\n-}\n+pub struct NonCopyable;\n \n impl NonCopyable {\n     /// Creates a dummy non-copyable structure and returns it for use.\n-    pub fn new() -> NonCopyable { NonCopyable { i: () } }\n+    pub fn new() -> NonCopyable { NonCopyable }\n }\n \n impl Drop for NonCopyable {"}]}