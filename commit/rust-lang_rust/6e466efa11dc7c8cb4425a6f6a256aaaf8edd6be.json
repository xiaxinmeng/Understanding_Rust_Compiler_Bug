{"sha": "6e466efa11dc7c8cb4425a6f6a256aaaf8edd6be", "node_id": "MDY6Q29tbWl0NzI0NzEyOjZlNDY2ZWZhMTFkYzdjOGNiNDQyNWE2ZjZhMjU2YWFhZjhlZGQ2YmU=", "commit": {"author": {"name": "Aaron Hill", "email": "aa1ronham@gmail.com", "date": "2020-11-23T05:32:51Z"}, "committer": {"name": "Aaron Hill", "email": "aa1ronham@gmail.com", "date": "2020-11-23T07:40:57Z"}, "message": "Cache pretty-print/retokenize result to avoid compile time blowup\n\nFixes #79242\n\nIf a `macro_rules!` recursively builds up a nested nonterminal\n(passing it to a proc-macro at each step), we will end up repeatedly\npretty-printing/retokenizing the same nonterminals. Unfortunately, the\n'probable equality' check we do has a non-trivial cost, which leads to a\nblowup in compilation time.\n\nAs a workaround, we cache the result of the 'probable equality' check,\nwhich eliminates the compilation time blowup for the linked issue. This\ncommit only touches a single file (other than adding tests), so it\nshould be easy to backport.\n\nThe proper solution is to remove the pretty-print/retokenize hack\nentirely. However, this will almost certainly break a large number of\ncrates that were relying on hygiene bugs created by using the reparsed\n`TokenStream`. As a result, we will definitely not want to backport\nsuch a change.", "tree": {"sha": "0b6f85dcf94a85e05f5afd77c4ce1a06d5870d91", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/0b6f85dcf94a85e05f5afd77c4ce1a06d5870d91"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/6e466efa11dc7c8cb4425a6f6a256aaaf8edd6be", "comment_count": 0, "verification": {"verified": true, "reason": "valid", "signature": "-----BEGIN PGP SIGNATURE-----\n\niQIzBAABCAAdFiEE7J9Gc3TfBwj2K399tAh+UQ6YsWQFAl+7Z4kACgkQtAh+UQ6Y\nsWTBbw/9EhUT7J/FPaOIMHDD64j2CSzKIVHJ0l8vzSakZCN+uNFsIXniyHgp6x6v\nw4ntftH3OAaVCQb5FQXCZ+zHc7nv3r3cDPww+yHA9HTgsvAS64wIY++srz+qDVxQ\nTH/QYRMgCqISRH1hFCWLLmBmRvDLx38xI0/Rz1Zfu37Gc/zrGKui6wy18I8VbgvL\nHfw2K0r81K2Bgpa8amDvZQkZEowHpitBi4+O0o+xy8YqNxAa4YPm79vPAJX46CpG\ndagQOrgV2KC0i++C6FEMPFuBuPFWkOKJrYDyfDDAbnJapWrLsymU69i+bRB73w7j\n34VfxZ+cEqwA66Gf7insZlW3cuJoYmO7tjoBNZapwr+JDwOyP2OAcfpaNusc4lPk\ncqp4PEuxwYuqJ9mAswPl759RmzgU+ud6Rci4ljTFIQ8jO6q7XhOv8IYZRlUhdzWD\nUbWzjU5MDb1Kg/yD4Vwpo1F8rWt4L8Xi3NcTEGtFETpd/He2G+IPl2KRF0l4lt5L\nRsTbAGhbBiBLr2zLSAraTGJ6CHPvAo72FWSAXkZOGRnCZWiUzgEPy5HyyEqPGXzO\n+PR5BAiK+cKgjkh3cvmEbTI3DZo1zTZqJ5ttbuEDsYo9N7UoTvRQ/THq9Ux4Ur9I\nOkrhGAXa4uZ+jnjirmDbaE+6X2h4E1L+vtP/7tguZvlXZvwn1js=\n=iaNW\n-----END PGP SIGNATURE-----", "payload": "tree 0b6f85dcf94a85e05f5afd77c4ce1a06d5870d91\nparent a0d664bae6ca79c54cc054aa2403198e105190a2\nauthor Aaron Hill <aa1ronham@gmail.com> 1606109571 -0500\ncommitter Aaron Hill <aa1ronham@gmail.com> 1606117257 -0500\n\nCache pretty-print/retokenize result to avoid compile time blowup\n\nFixes #79242\n\nIf a `macro_rules!` recursively builds up a nested nonterminal\n(passing it to a proc-macro at each step), we will end up repeatedly\npretty-printing/retokenizing the same nonterminals. Unfortunately, the\n'probable equality' check we do has a non-trivial cost, which leads to a\nblowup in compilation time.\n\nAs a workaround, we cache the result of the 'probable equality' check,\nwhich eliminates the compilation time blowup for the linked issue. This\ncommit only touches a single file (other than adding tests), so it\nshould be easy to backport.\n\nThe proper solution is to remove the pretty-print/retokenize hack\nentirely. However, this will almost certainly break a large number of\ncrates that were relying on hygiene bugs created by using the reparsed\n`TokenStream`. As a result, we will definitely not want to backport\nsuch a change.\n"}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/6e466efa11dc7c8cb4425a6f6a256aaaf8edd6be", "html_url": "https://github.com/rust-lang/rust/commit/6e466efa11dc7c8cb4425a6f6a256aaaf8edd6be", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/6e466efa11dc7c8cb4425a6f6a256aaaf8edd6be/comments", "author": {"login": "Aaron1011", "id": 1408859, "node_id": "MDQ6VXNlcjE0MDg4NTk=", "avatar_url": "https://avatars.githubusercontent.com/u/1408859?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Aaron1011", "html_url": "https://github.com/Aaron1011", "followers_url": "https://api.github.com/users/Aaron1011/followers", "following_url": "https://api.github.com/users/Aaron1011/following{/other_user}", "gists_url": "https://api.github.com/users/Aaron1011/gists{/gist_id}", "starred_url": "https://api.github.com/users/Aaron1011/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Aaron1011/subscriptions", "organizations_url": "https://api.github.com/users/Aaron1011/orgs", "repos_url": "https://api.github.com/users/Aaron1011/repos", "events_url": "https://api.github.com/users/Aaron1011/events{/privacy}", "received_events_url": "https://api.github.com/users/Aaron1011/received_events", "type": "User", "site_admin": false}, "committer": {"login": "Aaron1011", "id": 1408859, "node_id": "MDQ6VXNlcjE0MDg4NTk=", "avatar_url": "https://avatars.githubusercontent.com/u/1408859?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Aaron1011", "html_url": "https://github.com/Aaron1011", "followers_url": "https://api.github.com/users/Aaron1011/followers", "following_url": "https://api.github.com/users/Aaron1011/following{/other_user}", "gists_url": "https://api.github.com/users/Aaron1011/gists{/gist_id}", "starred_url": "https://api.github.com/users/Aaron1011/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Aaron1011/subscriptions", "organizations_url": "https://api.github.com/users/Aaron1011/orgs", "repos_url": "https://api.github.com/users/Aaron1011/repos", "events_url": "https://api.github.com/users/Aaron1011/events{/privacy}", "received_events_url": "https://api.github.com/users/Aaron1011/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "a0d664bae6ca79c54cc054aa2403198e105190a2", "url": "https://api.github.com/repos/rust-lang/rust/commits/a0d664bae6ca79c54cc054aa2403198e105190a2", "html_url": "https://github.com/rust-lang/rust/commit/a0d664bae6ca79c54cc054aa2403198e105190a2"}], "stats": {"total": 88, "additions": 85, "deletions": 3}, "files": [{"sha": "f93b952345bdb3c628179f8f0f2f37080b860558", "filename": "compiler/rustc_parse/src/lib.rs", "status": "modified", "additions": 35, "deletions": 3, "changes": 38, "blob_url": "https://github.com/rust-lang/rust/blob/6e466efa11dc7c8cb4425a6f6a256aaaf8edd6be/compiler%2Frustc_parse%2Fsrc%2Flib.rs", "raw_url": "https://github.com/rust-lang/rust/raw/6e466efa11dc7c8cb4425a6f6a256aaaf8edd6be/compiler%2Frustc_parse%2Fsrc%2Flib.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_parse%2Fsrc%2Flib.rs?ref=6e466efa11dc7c8cb4425a6f6a256aaaf8edd6be", "patch": "@@ -10,12 +10,14 @@ use rustc_ast as ast;\n use rustc_ast::token::{self, DelimToken, Nonterminal, Token, TokenKind};\n use rustc_ast::tokenstream::{self, LazyTokenStream, TokenStream, TokenTree};\n use rustc_ast_pretty::pprust;\n+use rustc_data_structures::fx::FxHashSet;\n use rustc_data_structures::sync::Lrc;\n use rustc_errors::{Diagnostic, FatalError, Level, PResult};\n use rustc_session::parse::ParseSess;\n use rustc_span::{symbol::kw, FileName, SourceFile, Span, DUMMY_SP};\n \n use smallvec::SmallVec;\n+use std::cell::RefCell;\n use std::mem;\n use std::path::Path;\n use std::str;\n@@ -282,14 +284,33 @@ pub fn nt_to_tokenstream(nt: &Nonterminal, sess: &ParseSess, span: Span) -> Toke\n         }\n     };\n \n+    // Caches the stringification of 'good' `TokenStreams` which passed\n+    // `tokenstream_probably_equal_for_proc_macro`. This allows us to avoid\n+    // repeatedly stringifying and comparing the same `TokenStream` for deeply\n+    // nested nonterminals.\n+    //\n+    // We cache by the strinification instead of the `TokenStream` to avoid\n+    // needing to implement `Hash` for `TokenStream`. Note that it's possible to\n+    // have two distinct `TokenStream`s that stringify to the same result\n+    // (e.g. if they differ only in hygiene information). However, any\n+    // information lost during the stringification process is also intentionally\n+    // ignored by `tokenstream_probably_equal_for_proc_macro`, so it's fine\n+    // that a single cache entry may 'map' to multiple distinct `TokenStream`s.\n+    //\n+    // This is a temporary hack to prevent compilation blowup on certain inputs.\n+    // The entire pretty-print/retokenize process will be removed soon.\n+    thread_local! {\n+        static GOOD_TOKEN_CACHE: RefCell<FxHashSet<String>> = Default::default();\n+    }\n+\n     // FIXME(#43081): Avoid this pretty-print + reparse hack\n     // Pretty-print the AST struct without inserting any parenthesis\n     // beyond those explicitly written by the user (e.g. `ExpnKind::Paren`).\n     // The resulting stream may have incorrect precedence, but it's only\n     // ever used for a comparison against the capture tokenstream.\n     let source = pprust::nonterminal_to_string_no_extra_parens(nt);\n     let filename = FileName::macro_expansion_source_code(&source);\n-    let reparsed_tokens = parse_stream_from_source_str(filename, source, sess, Some(span));\n+    let reparsed_tokens = parse_stream_from_source_str(filename, source.clone(), sess, Some(span));\n \n     // During early phases of the compiler the AST could get modified\n     // directly (e.g., attributes added or removed) and the internal cache\n@@ -315,8 +336,13 @@ pub fn nt_to_tokenstream(nt: &Nonterminal, sess: &ParseSess, span: Span) -> Toke\n     // modifications, including adding/removing typically non-semantic\n     // tokens such as extra braces and commas, don't happen.\n     if let Some(tokens) = tokens {\n+        if GOOD_TOKEN_CACHE.with(|cache| cache.borrow().contains(&source)) {\n+            return tokens;\n+        }\n+\n         // Compare with a non-relaxed delim match to start.\n         if tokenstream_probably_equal_for_proc_macro(&tokens, &reparsed_tokens, sess, false) {\n+            GOOD_TOKEN_CACHE.with(|cache| cache.borrow_mut().insert(source.clone()));\n             return tokens;\n         }\n \n@@ -325,6 +351,11 @@ pub fn nt_to_tokenstream(nt: &Nonterminal, sess: &ParseSess, span: Span) -> Toke\n         // token stream to match up with inserted parenthesis in the reparsed stream.\n         let source_with_parens = pprust::nonterminal_to_string(nt);\n         let filename_with_parens = FileName::macro_expansion_source_code(&source_with_parens);\n+\n+        if GOOD_TOKEN_CACHE.with(|cache| cache.borrow().contains(&source_with_parens)) {\n+            return tokens;\n+        }\n+\n         let reparsed_tokens_with_parens = parse_stream_from_source_str(\n             filename_with_parens,\n             source_with_parens,\n@@ -340,6 +371,7 @@ pub fn nt_to_tokenstream(nt: &Nonterminal, sess: &ParseSess, span: Span) -> Toke\n             sess,\n             true,\n         ) {\n+            GOOD_TOKEN_CACHE.with(|cache| cache.borrow_mut().insert(source.clone()));\n             return tokens;\n         }\n \n@@ -419,9 +451,9 @@ pub fn tokenstream_probably_equal_for_proc_macro(\n         // to iterate breaking tokens mutliple times. For example:\n         // '[BinOpEq(Shr)] => [Gt, Ge] -> [Gt, Gt, Eq]'\n         let mut token_trees: SmallVec<[_; 2]>;\n-        if let TokenTree::Token(token) = &tree {\n+        if let TokenTree::Token(token) = tree {\n             let mut out = SmallVec::<[_; 2]>::new();\n-            out.push(token.clone());\n+            out.push(token);\n             // Iterate to fixpoint:\n             // * We start off with 'out' containing our initial token, and `temp` empty\n             // * If we are able to break any tokens in `out`, then `out` will have"}, {"sha": "e586980f0ad8eabfe3a105a3ddaf958aded11939", "filename": "src/test/ui/proc-macro/auxiliary/issue-79242.rs", "status": "added", "additions": 16, "deletions": 0, "changes": 16, "blob_url": "https://github.com/rust-lang/rust/blob/6e466efa11dc7c8cb4425a6f6a256aaaf8edd6be/src%2Ftest%2Fui%2Fproc-macro%2Fauxiliary%2Fissue-79242.rs", "raw_url": "https://github.com/rust-lang/rust/raw/6e466efa11dc7c8cb4425a6f6a256aaaf8edd6be/src%2Ftest%2Fui%2Fproc-macro%2Fauxiliary%2Fissue-79242.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ftest%2Fui%2Fproc-macro%2Fauxiliary%2Fissue-79242.rs?ref=6e466efa11dc7c8cb4425a6f6a256aaaf8edd6be", "patch": "@@ -0,0 +1,16 @@\n+// force-host\n+// no-prefer-dynamic\n+\n+#![crate_type = \"proc-macro\"]\n+\n+extern crate proc_macro;\n+\n+use proc_macro::TokenStream;\n+\n+#[proc_macro]\n+pub fn dummy(input: TokenStream) -> TokenStream {\n+    // Iterate to force internal conversion of nonterminals\n+    // to `proc_macro` structs\n+    for _ in input {}\n+    TokenStream::new()\n+}"}, {"sha": "b68f19c5dd21dd9df7bd73fdf2f60f489ef451b4", "filename": "src/test/ui/proc-macro/issue-79242-slow-retokenize-check.rs", "status": "added", "additions": 34, "deletions": 0, "changes": 34, "blob_url": "https://github.com/rust-lang/rust/blob/6e466efa11dc7c8cb4425a6f6a256aaaf8edd6be/src%2Ftest%2Fui%2Fproc-macro%2Fissue-79242-slow-retokenize-check.rs", "raw_url": "https://github.com/rust-lang/rust/raw/6e466efa11dc7c8cb4425a6f6a256aaaf8edd6be/src%2Ftest%2Fui%2Fproc-macro%2Fissue-79242-slow-retokenize-check.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ftest%2Fui%2Fproc-macro%2Fissue-79242-slow-retokenize-check.rs?ref=6e466efa11dc7c8cb4425a6f6a256aaaf8edd6be", "patch": "@@ -0,0 +1,34 @@\n+// check-pass\n+// aux-build:issue-79242.rs\n+\n+// Regression test for issue #79242\n+// Tests that compilation time doesn't blow up for a proc-macro\n+// invocation with deeply nested nonterminals\n+\n+#![allow(unused)]\n+\n+extern crate issue_79242;\n+\n+macro_rules! declare_nats {\n+    ($prev:ty) => {};\n+    ($prev:ty, $n:literal$(, $tail:literal)*) => {\n+\n+        issue_79242::dummy! {\n+            $prev\n+        }\n+\n+        declare_nats!(Option<$prev>$(, $tail)*);\n+    };\n+    (0, $($n:literal),+) => {\n+        pub struct N0;\n+        declare_nats!(N0, $($n),+);\n+    };\n+}\n+\n+declare_nats! {\n+    0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,\n+    17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28\n+}\n+\n+\n+fn main() {}"}]}