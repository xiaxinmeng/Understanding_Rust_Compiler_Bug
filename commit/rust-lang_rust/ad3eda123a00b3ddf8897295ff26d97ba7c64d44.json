{"sha": "ad3eda123a00b3ddf8897295ff26d97ba7c64d44", "node_id": "MDY6Q29tbWl0NzI0NzEyOmFkM2VkYTEyM2EwMGIzZGRmODg5NzI5NWZmMjZkOTdiYTdjNjRkNDQ=", "commit": {"author": {"name": "bors", "email": "bors@rust-lang.org", "date": "2014-07-09T08:46:35Z"}, "committer": {"name": "bors", "email": "bors@rust-lang.org", "date": "2014-07-09T08:46:35Z"}, "message": "auto merge of #15339 : cmr/rust/rewrite-lexer2, r=huonw\n\nMostly minor things that rebasing is becoming painful.", "tree": {"sha": "0cb4cd8360a223b72c5ce0bc3e3701786a381bd4", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/0cb4cd8360a223b72c5ce0bc3e3701786a381bd4"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/ad3eda123a00b3ddf8897295ff26d97ba7c64d44", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/ad3eda123a00b3ddf8897295ff26d97ba7c64d44", "html_url": "https://github.com/rust-lang/rust/commit/ad3eda123a00b3ddf8897295ff26d97ba7c64d44", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/ad3eda123a00b3ddf8897295ff26d97ba7c64d44/comments", "author": {"login": "bors", "id": 3372342, "node_id": "MDQ6VXNlcjMzNzIzNDI=", "avatar_url": "https://avatars.githubusercontent.com/u/3372342?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bors", "html_url": "https://github.com/bors", "followers_url": "https://api.github.com/users/bors/followers", "following_url": "https://api.github.com/users/bors/following{/other_user}", "gists_url": "https://api.github.com/users/bors/gists{/gist_id}", "starred_url": "https://api.github.com/users/bors/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bors/subscriptions", "organizations_url": "https://api.github.com/users/bors/orgs", "repos_url": "https://api.github.com/users/bors/repos", "events_url": "https://api.github.com/users/bors/events{/privacy}", "received_events_url": "https://api.github.com/users/bors/received_events", "type": "User", "site_admin": false}, "committer": {"login": "bors", "id": 3372342, "node_id": "MDQ6VXNlcjMzNzIzNDI=", "avatar_url": "https://avatars.githubusercontent.com/u/3372342?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bors", "html_url": "https://github.com/bors", "followers_url": "https://api.github.com/users/bors/followers", "following_url": "https://api.github.com/users/bors/following{/other_user}", "gists_url": "https://api.github.com/users/bors/gists{/gist_id}", "starred_url": "https://api.github.com/users/bors/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bors/subscriptions", "organizations_url": "https://api.github.com/users/bors/orgs", "repos_url": "https://api.github.com/users/bors/repos", "events_url": "https://api.github.com/users/bors/events{/privacy}", "received_events_url": "https://api.github.com/users/bors/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "5716abe3f019ab7d9c8cdde9879332040191cf88", "url": "https://api.github.com/repos/rust-lang/rust/commits/5716abe3f019ab7d9c8cdde9879332040191cf88", "html_url": "https://github.com/rust-lang/rust/commit/5716abe3f019ab7d9c8cdde9879332040191cf88"}, {"sha": "69a0cdf49195d2bc042b44f75e309eb280bcc475", "url": "https://api.github.com/repos/rust-lang/rust/commits/69a0cdf49195d2bc042b44f75e309eb280bcc475", "html_url": "https://github.com/rust-lang/rust/commit/69a0cdf49195d2bc042b44f75e309eb280bcc475"}], "stats": {"total": 4360, "additions": 2297, "deletions": 2063}, "files": [{"sha": "f94d5a5e4b5e3b6fe31f02a687189afdfd3d2b92", "filename": "src/libcore/str.rs", "status": "modified", "additions": 5, "deletions": 2, "changes": 7, "blob_url": "https://github.com/rust-lang/rust/blob/ad3eda123a00b3ddf8897295ff26d97ba7c64d44/src%2Flibcore%2Fstr.rs", "raw_url": "https://github.com/rust-lang/rust/raw/ad3eda123a00b3ddf8897295ff26d97ba7c64d44/src%2Flibcore%2Fstr.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibcore%2Fstr.rs?ref=ad3eda123a00b3ddf8897295ff26d97ba7c64d44", "patch": "@@ -1764,7 +1764,9 @@ impl<'a> StrSlice<'a> for &'a str {\n \n     #[inline]\n     fn slice(&self, begin: uint, end: uint) -> &'a str {\n-        assert!(self.is_char_boundary(begin) && self.is_char_boundary(end));\n+        assert!(self.is_char_boundary(begin) && self.is_char_boundary(end),\n+                \"index {} and/or {} in `{}` do not lie on character boundary\", begin,\n+                end, *self);\n         unsafe { raw::slice_bytes(*self, begin, end) }\n     }\n \n@@ -1775,7 +1777,8 @@ impl<'a> StrSlice<'a> for &'a str {\n \n     #[inline]\n     fn slice_to(&self, end: uint) -> &'a str {\n-        assert!(self.is_char_boundary(end));\n+        assert!(self.is_char_boundary(end), \"index {} in `{}` does not lie on \\\n+                a character boundary\", end, *self);\n         unsafe { raw::slice_bytes(*self, 0, end) }\n     }\n "}, {"sha": "ae401b9d6f15c969a0e748ba58d5b5e4b47aa2ee", "filename": "src/librustc/lint/builtin.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/ad3eda123a00b3ddf8897295ff26d97ba7c64d44/src%2Flibrustc%2Flint%2Fbuiltin.rs", "raw_url": "https://github.com/rust-lang/rust/raw/ad3eda123a00b3ddf8897295ff26d97ba7c64d44/src%2Flibrustc%2Flint%2Fbuiltin.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Flint%2Fbuiltin.rs?ref=ad3eda123a00b3ddf8897295ff26d97ba7c64d44", "patch": "@@ -1114,7 +1114,7 @@ impl UnusedMut {\n                 match mode {\n                     ast::BindByValue(ast::MutMutable) => {\n                         if !token::get_ident(ident).get().starts_with(\"_\") {\n-                            mutables.insert_or_update_with(ident.name as uint,\n+                            mutables.insert_or_update_with(ident.name.uint(),\n                                 vec!(id), |_, old| { old.push(id); });\n                         }\n                     }"}, {"sha": "cc41223688ee0b9eab8f8ff7f3c6caaff54d262d", "filename": "src/librustc/metadata/decoder.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/ad3eda123a00b3ddf8897295ff26d97ba7c64d44/src%2Flibrustc%2Fmetadata%2Fdecoder.rs", "raw_url": "https://github.com/rust-lang/rust/raw/ad3eda123a00b3ddf8897295ff26d97ba7c64d44/src%2Flibrustc%2Fmetadata%2Fdecoder.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Fmetadata%2Fdecoder.rs?ref=ad3eda123a00b3ddf8897295ff26d97ba7c64d44", "patch": "@@ -323,7 +323,7 @@ fn item_name(intr: &IdentInterner, item: ebml::Doc) -> ast::Ident {\n     let string = name.as_str_slice();\n     match intr.find_equiv(&string) {\n         None => token::str_to_ident(string),\n-        Some(val) => ast::Ident::new(val as ast::Name),\n+        Some(val) => ast::Ident::new(val),\n     }\n }\n "}, {"sha": "fb2b4951ea3d68f62d45b2e86e9c293b45605eb2", "filename": "src/librustc/middle/astencode.rs", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "blob_url": "https://github.com/rust-lang/rust/blob/ad3eda123a00b3ddf8897295ff26d97ba7c64d44/src%2Flibrustc%2Fmiddle%2Fastencode.rs", "raw_url": "https://github.com/rust-lang/rust/raw/ad3eda123a00b3ddf8897295ff26d97ba7c64d44/src%2Flibrustc%2Fmiddle%2Fastencode.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Fmiddle%2Fastencode.rs?ref=ad3eda123a00b3ddf8897295ff26d97ba7c64d44", "patch": "@@ -1523,14 +1523,15 @@ fn test_basic() {\n         fn foo() {}\n     ));\n }\n-\n+/* NOTE: When there's a snapshot, update this (yay quasiquoter!)\n #[test]\n fn test_smalltalk() {\n     let cx = mk_ctxt();\n     roundtrip(quote_item!(cx,\n         fn foo() -> int { 3 + 4 } // first smalltalk program ever executed.\n     ));\n }\n+*/\n \n #[test]\n fn test_more() {"}, {"sha": "11a8207f8c43ea22608be19a96bc33590910554a", "filename": "src/librustc/middle/trans/consts.rs", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "blob_url": "https://github.com/rust-lang/rust/blob/ad3eda123a00b3ddf8897295ff26d97ba7c64d44/src%2Flibrustc%2Fmiddle%2Ftrans%2Fconsts.rs", "raw_url": "https://github.com/rust-lang/rust/raw/ad3eda123a00b3ddf8897295ff26d97ba7c64d44/src%2Flibrustc%2Fmiddle%2Ftrans%2Fconsts.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Fmiddle%2Ftrans%2Fconsts.rs?ref=ad3eda123a00b3ddf8897295ff26d97ba7c64d44", "patch": "@@ -42,6 +42,7 @@ use syntax::{ast, ast_util};\n pub fn const_lit(cx: &CrateContext, e: &ast::Expr, lit: ast::Lit)\n     -> ValueRef {\n     let _icx = push_ctxt(\"trans_lit\");\n+    debug!(\"const_lit: {}\", lit);\n     match lit.node {\n         ast::LitByte(b) => C_integral(Type::uint_from_ty(cx, ast::TyU8), b as u64, false),\n         ast::LitChar(i) => C_integral(Type::char(cx), i as u64, false),"}, {"sha": "3cb5cdc04396227677bc8cdfd9e09636e5f8f133", "filename": "src/librustdoc/html/highlight.rs", "status": "modified", "additions": 17, "deletions": 27, "changes": 44, "blob_url": "https://github.com/rust-lang/rust/blob/ad3eda123a00b3ddf8897295ff26d97ba7c64d44/src%2Flibrustdoc%2Fhtml%2Fhighlight.rs", "raw_url": "https://github.com/rust-lang/rust/raw/ad3eda123a00b3ddf8897295ff26d97ba7c64d44/src%2Flibrustdoc%2Fhtml%2Fhighlight.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustdoc%2Fhtml%2Fhighlight.rs?ref=ad3eda123a00b3ddf8897295ff26d97ba7c64d44", "patch": "@@ -18,7 +18,6 @@ use std::io;\n \n use syntax::parse;\n use syntax::parse::lexer;\n-use syntax::codemap::{BytePos, Span};\n \n use html::escape::Escape;\n \n@@ -59,38 +58,30 @@ fn doit(sess: &parse::ParseSess, mut lexer: lexer::StringReader,\n         None => {}\n     }\n     try!(write!(out, \"class='rust {}'>\\n\", class.unwrap_or(\"\")));\n-    let mut last = BytePos(0);\n     let mut is_attribute = false;\n     let mut is_macro = false;\n     let mut is_macro_nonterminal = false;\n     loop {\n         let next = lexer.next_token();\n-        let test = if next.tok == t::EOF {lexer.pos} else {next.sp.lo};\n-\n-        // The lexer consumes all whitespace and non-doc-comments when iterating\n-        // between tokens. If this token isn't directly adjacent to our last\n-        // token, then we need to emit the whitespace/comment.\n-        //\n-        // If the gap has any '/' characters then we consider the whole thing a\n-        // comment. This will classify some whitespace as a comment, but that\n-        // doesn't matter too much for syntax highlighting purposes.\n-        if test > last {\n-            let snip = sess.span_diagnostic.cm.span_to_snippet(Span {\n-                lo: last,\n-                hi: test,\n-                expn_info: None,\n-            }).unwrap();\n-            if snip.as_slice().contains(\"/\") {\n-                try!(write!(out, \"<span class='comment'>{}</span>\",\n-                              Escape(snip.as_slice())));\n-            } else {\n-                try!(write!(out, \"{}\", Escape(snip.as_slice())));\n-            }\n-        }\n-        last = next.sp.hi;\n+\n+        let snip = |sp| sess.span_diagnostic.cm.span_to_snippet(sp).unwrap();\n+\n         if next.tok == t::EOF { break }\n \n         let klass = match next.tok {\n+            t::WS => {\n+                try!(write!(out, \"{}\", Escape(snip(next.sp).as_slice())));\n+                continue\n+            },\n+            t::COMMENT => {\n+                try!(write!(out, \"<span class='comment'>{}</span>\",\n+                            Escape(snip(next.sp).as_slice())));\n+                continue\n+            },\n+            t::SHEBANG(s) => {\n+                try!(write!(out, \"{}\", Escape(s.as_str())));\n+                continue\n+            },\n             // If this '&' token is directly adjacent to another token, assume\n             // that it's the address-of operator instead of the and-operator.\n             // This allows us to give all pointers their own class (`Box` and\n@@ -144,8 +135,7 @@ fn doit(sess: &parse::ParseSess, mut lexer: lexer::StringReader,\n                 t::LIT_CHAR(..) | t::LIT_STR(..) | t::LIT_STR_RAW(..) => \"string\",\n \n             // number literals\n-            t::LIT_INT(..) | t::LIT_UINT(..) | t::LIT_INT_UNSUFFIXED(..) |\n-                t::LIT_FLOAT(..) | t::LIT_FLOAT_UNSUFFIXED(..) => \"number\",\n+            t::LIT_INTEGER(..) | t::LIT_FLOAT(..) => \"number\",\n \n             // keywords are also included in the identifier set\n             t::IDENT(ident, _is_mod_sep) => {"}, {"sha": "5aaf7ed3dba5df1953da27414c34a52120a74c24", "filename": "src/libsyntax/abi.rs", "status": "modified", "additions": 8, "deletions": 13, "changes": 21, "blob_url": "https://github.com/rust-lang/rust/blob/ad3eda123a00b3ddf8897295ff26d97ba7c64d44/src%2Flibsyntax%2Fabi.rs", "raw_url": "https://github.com/rust-lang/rust/raw/ad3eda123a00b3ddf8897295ff26d97ba7c64d44/src%2Flibsyntax%2Fabi.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fabi.rs?ref=ad3eda123a00b3ddf8897295ff26d97ba7c64d44", "patch": "@@ -60,9 +60,12 @@ pub struct AbiData {\n }\n \n pub enum AbiArchitecture {\n-    RustArch,   // Not a real ABI (e.g., intrinsic)\n-    AllArch,    // An ABI that specifies cross-platform defaults (e.g., \"C\")\n-    Archs(u32)  // Multiple architectures (bitset)\n+    /// Not a real ABI (e.g., intrinsic)\n+    RustArch,\n+    /// An ABI that specifies cross-platform defaults (e.g., \"C\")\n+    AllArch,\n+    /// Multiple architectures (bitset)\n+    Archs(u32)\n }\n \n static AbiDatas: &'static [AbiData] = &[\n@@ -84,21 +87,13 @@ static AbiDatas: &'static [AbiData] = &[\n     AbiData {abi: RustIntrinsic, name: \"rust-intrinsic\", abi_arch: RustArch},\n ];\n \n+/// Iterates through each of the defined ABIs.\n fn each_abi(op: |abi: Abi| -> bool) -> bool {\n-    /*!\n-     *\n-     * Iterates through each of the defined ABIs.\n-     */\n-\n     AbiDatas.iter().advance(|abi_data| op(abi_data.abi))\n }\n \n+/// Returns the ABI with the given name (if any).\n pub fn lookup(name: &str) -> Option<Abi> {\n-    /*!\n-     *\n-     * Returns the ABI with the given name (if any).\n-     */\n-\n     let mut res = None;\n \n     each_abi(|abi| {"}, {"sha": "ebfc45d22cee9da33124293ab4c0efb6c23197f5", "filename": "src/libsyntax/ast.rs", "status": "modified", "additions": 249, "deletions": 182, "changes": 431, "blob_url": "https://github.com/rust-lang/rust/blob/ad3eda123a00b3ddf8897295ff26d97ba7c64d44/src%2Flibsyntax%2Fast.rs", "raw_url": "https://github.com/rust-lang/rust/raw/ad3eda123a00b3ddf8897295ff26d97ba7c64d44/src%2Flibsyntax%2Fast.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fast.rs?ref=ad3eda123a00b3ddf8897295ff26d97ba7c64d44", "patch": "@@ -24,7 +24,8 @@ use std::rc::Rc;\n use std::gc::{Gc, GC};\n use serialize::{Encodable, Decodable, Encoder, Decoder};\n \n-/// A pointer abstraction. FIXME(eddyb) #10676 use Rc<T> in the future.\n+/// A pointer abstraction.\n+// FIXME(eddyb) #10676 use Rc<T> in the future.\n pub type P<T> = Gc<T>;\n \n #[allow(non_snake_case_functions)]\n@@ -36,11 +37,11 @@ pub fn P<T: 'static>(value: T) -> P<T> {\n // FIXME #6993: in librustc, uses of \"ident\" should be replaced\n // by just \"Name\".\n \n-// an identifier contains a Name (index into the interner\n-// table) and a SyntaxContext to track renaming and\n-// macro expansion per Flatt et al., \"Macros\n-// That Work Together\"\n-#[deriving(Clone, Hash, PartialOrd, Eq, Ord, Show)]\n+/// An identifier contains a Name (index into the interner\n+/// table) and a SyntaxContext to track renaming and\n+/// macro expansion per Flatt et al., \"Macros\n+/// That Work Together\"\n+#[deriving(Clone, Hash, PartialOrd, Eq, Ord)]\n pub struct Ident {\n     pub name: Name,\n     pub ctxt: SyntaxContext\n@@ -49,6 +50,16 @@ pub struct Ident {\n impl Ident {\n     /// Construct an identifier with the given name and an empty context:\n     pub fn new(name: Name) -> Ident { Ident {name: name, ctxt: EMPTY_CTXT}}\n+\n+    pub fn as_str<'a>(&'a self) -> &'a str {\n+        self.name.as_str()\n+    }\n+}\n+\n+impl Show for Ident {\n+    fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {\n+        write!(f, \"\\\"{}\\\"#{}\", token::get_ident(*self).get(), self.ctxt)\n+    }\n }\n \n impl PartialEq for Ident {\n@@ -95,7 +106,26 @@ pub static ILLEGAL_CTXT : SyntaxContext = 1;\n \n /// A name is a part of an identifier, representing a string or gensym. It's\n /// the result of interning.\n-pub type Name = u32;\n+#[deriving(Eq, Ord, PartialEq, PartialOrd, Hash, Encodable, Decodable, Clone, Show)]\n+pub struct Name(pub u32);\n+\n+impl Name {\n+    pub fn as_str<'a>(&'a self) -> &'a str {\n+        unsafe {\n+            // FIXME #12938: can't use copy_lifetime since &str isn't a &T\n+            ::std::mem::transmute(token::get_name(*self).get())\n+        }\n+    }\n+\n+    pub fn uint(&self) -> uint {\n+        let Name(nm) = *self;\n+        nm as uint\n+    }\n+\n+    pub fn ident(&self) -> Ident {\n+        Ident { name: *self, ctxt: 0 }\n+    }\n+}\n \n /// A mark represents a unique id associated with a macro expansion\n pub type Mrk = u32;\n@@ -122,10 +152,9 @@ pub struct Lifetime {\n     pub name: Name\n }\n \n-// a \"Path\" is essentially Rust's notion of a name;\n-// for instance: std::cmp::PartialEq  .  It's represented\n-// as a sequence of identifiers, along with a bunch\n-// of supporting information.\n+/// A \"Path\" is essentially Rust's notion of a name; for instance:\n+/// std::cmp::PartialEq  .  It's represented as a sequence of identifiers,\n+/// along with a bunch of supporting information.\n #[deriving(Clone, PartialEq, Eq, Encodable, Decodable, Hash)]\n pub struct Path {\n     pub span: Span,\n@@ -163,15 +192,15 @@ pub struct DefId {\n pub static LOCAL_CRATE: CrateNum = 0;\n pub static CRATE_NODE_ID: NodeId = 0;\n \n-// When parsing and doing expansions, we initially give all AST nodes this AST\n-// node value. Then later, in the renumber pass, we renumber them to have\n-// small, positive ids.\n+/// When parsing and doing expansions, we initially give all AST nodes this AST\n+/// node value. Then later, in the renumber pass, we renumber them to have\n+/// small, positive ids.\n pub static DUMMY_NODE_ID: NodeId = -1;\n \n-// The AST represents all type param bounds as types.\n-// typeck::collect::compute_bounds matches these against\n-// the \"special\" built-in traits (see middle::lang_items) and\n-// detects Copy, Send and Share.\n+/// The AST represents all type param bounds as types.\n+/// typeck::collect::compute_bounds matches these against\n+/// the \"special\" built-in traits (see middle::lang_items) and\n+/// detects Copy, Send and Share.\n #[deriving(Clone, PartialEq, Eq, Encodable, Decodable, Hash)]\n pub enum TyParamBound {\n     TraitTyParamBound(TraitRef),\n@@ -210,9 +239,9 @@ impl Generics {\n     }\n }\n \n-// The set of MetaItems that define the compilation environment of the crate,\n-// used to drive conditional compilation\n-pub type CrateConfig = Vec<Gc<MetaItem>>;\n+/// The set of MetaItems that define the compilation environment of the crate,\n+/// used to drive conditional compilation\n+pub type CrateConfig = Vec<Gc<MetaItem>> ;\n \n #[deriving(Clone, PartialEq, Eq, Encodable, Decodable, Hash)]\n pub struct Crate {\n@@ -289,13 +318,13 @@ pub enum BindingMode {\n pub enum Pat_ {\n     PatWild,\n     PatWildMulti,\n-    // A PatIdent may either be a new bound variable,\n-    // or a nullary enum (in which case the third field\n-    // is None).\n-    // In the nullary enum case, the parser can't determine\n-    // which it is. The resolver determines this, and\n-    // records this pattern's NodeId in an auxiliary\n-    // set (of \"PatIdents that refer to nullary enums\")\n+    /// A PatIdent may either be a new bound variable,\n+    /// or a nullary enum (in which case the third field\n+    /// is None).\n+    /// In the nullary enum case, the parser can't determine\n+    /// which it is. The resolver determines this, and\n+    /// records this pattern's NodeId in an auxiliary\n+    /// set (of \"PatIdents that refer to nullary enums\")\n     PatIdent(BindingMode, SpannedIdent, Option<Gc<Pat>>),\n     PatEnum(Path, Option<Vec<Gc<Pat>>>), /* \"none\" means a * pattern where\n                                      * we don't bind the fields to names */\n@@ -305,8 +334,8 @@ pub enum Pat_ {\n     PatRegion(Gc<Pat>), // reference pattern\n     PatLit(Gc<Expr>),\n     PatRange(Gc<Expr>, Gc<Expr>),\n-    // [a, b, ..i, y, z] is represented as\n-    // PatVec(~[a, b], Some(i), ~[y, z])\n+    /// [a, b, ..i, y, z] is represented as:\n+    ///     PatVec(~[a, b], Some(i), ~[y, z])\n     PatVec(Vec<Gc<Pat>>, Option<Gc<Pat>>, Vec<Gc<Pat>>),\n     PatMac(Mac),\n }\n@@ -319,9 +348,12 @@ pub enum Mutability {\n \n #[deriving(Clone, PartialEq, Eq, Encodable, Decodable, Hash)]\n pub enum ExprVstore {\n-    ExprVstoreUniq,                 // ~[1,2,3,4]\n-    ExprVstoreSlice,                // &[1,2,3,4]\n-    ExprVstoreMutSlice,             // &mut [1,2,3,4]\n+    /// ~[1, 2, 3, 4]\n+    ExprVstoreUniq,\n+    /// &[1, 2, 3, 4]\n+    ExprVstoreSlice,\n+    /// &mut [1, 2, 3, 4]\n+    ExprVstoreMutSlice,\n }\n \n #[deriving(Clone, PartialEq, Eq, Encodable, Decodable, Hash)]\n@@ -359,16 +391,16 @@ pub type Stmt = Spanned<Stmt_>;\n \n #[deriving(Clone, PartialEq, Eq, Encodable, Decodable, Hash)]\n pub enum Stmt_ {\n-    // could be an item or a local (let) binding:\n+    /// Could be an item or a local (let) binding:\n     StmtDecl(Gc<Decl>, NodeId),\n \n-    // expr without trailing semi-colon (must have unit type):\n+    /// Expr without trailing semi-colon (must have unit type):\n     StmtExpr(Gc<Expr>, NodeId),\n \n-    // expr with trailing semi-colon (may have any type):\n+    /// Expr with trailing semi-colon (may have any type):\n     StmtSemi(Gc<Expr>, NodeId),\n \n-    // bool: is there a trailing sem-colon?\n+    /// bool: is there a trailing sem-colon?\n     StmtMac(Mac, bool),\n }\n \n@@ -397,9 +429,9 @@ pub type Decl = Spanned<Decl_>;\n \n #[deriving(PartialEq, Eq, Encodable, Decodable, Hash)]\n pub enum Decl_ {\n-    // a local (let) binding:\n+    /// A local (let) binding:\n     DeclLocal(Gc<Local>),\n-    // an item binding:\n+    /// An item binding:\n     DeclItem(Gc<Item>),\n }\n \n@@ -443,7 +475,7 @@ pub struct Expr {\n #[deriving(Clone, PartialEq, Eq, Encodable, Decodable, Hash)]\n pub enum Expr_ {\n     ExprVstore(Gc<Expr>, ExprVstore),\n-    // First expr is the place; second expr is the value.\n+    /// First expr is the place; second expr is the value.\n     ExprBox(Gc<Expr>, Gc<Expr>),\n     ExprVec(Vec<Gc<Expr>>),\n     ExprCall(Gc<Expr>, Vec<Gc<Expr>>),\n@@ -483,138 +515,135 @@ pub enum Expr_ {\n \n     ExprMac(Mac),\n \n-    // A struct literal expression.\n+    /// A struct literal expression.\n     ExprStruct(Path, Vec<Field> , Option<Gc<Expr>> /* base */),\n \n-    // A vector literal constructed from one repeated element.\n+    /// A vector literal constructed from one repeated element.\n     ExprRepeat(Gc<Expr> /* element */, Gc<Expr> /* count */),\n \n-    // No-op: used solely so we can pretty-print faithfully\n+    /// No-op: used solely so we can pretty-print faithfully\n     ExprParen(Gc<Expr>)\n }\n \n-// When the main rust parser encounters a syntax-extension invocation, it\n-// parses the arguments to the invocation as a token-tree. This is a very\n-// loose structure, such that all sorts of different AST-fragments can\n-// be passed to syntax extensions using a uniform type.\n-//\n-// If the syntax extension is an MBE macro, it will attempt to match its\n-// LHS \"matchers\" against the provided token tree, and if it finds a\n-// match, will transcribe the RHS token tree, splicing in any captured\n-// macro_parser::matched_nonterminals into the TTNonterminals it finds.\n-//\n-// The RHS of an MBE macro is the only place a TTNonterminal or TTSeq\n-// makes any real sense. You could write them elsewhere but nothing\n-// else knows what to do with them, so you'll probably get a syntax\n-// error.\n-//\n+/// When the main rust parser encounters a syntax-extension invocation, it\n+/// parses the arguments to the invocation as a token-tree. This is a very\n+/// loose structure, such that all sorts of different AST-fragments can\n+/// be passed to syntax extensions using a uniform type.\n+///\n+/// If the syntax extension is an MBE macro, it will attempt to match its\n+/// LHS \"matchers\" against the provided token tree, and if it finds a\n+/// match, will transcribe the RHS token tree, splicing in any captured\n+/// macro_parser::matched_nonterminals into the TTNonterminals it finds.\n+///\n+/// The RHS of an MBE macro is the only place a TTNonterminal or TTSeq\n+/// makes any real sense. You could write them elsewhere but nothing\n+/// else knows what to do with them, so you'll probably get a syntax\n+/// error.\n #[deriving(Clone, PartialEq, Eq, Encodable, Decodable, Hash)]\n #[doc=\"For macro invocations; parsing is delegated to the macro\"]\n pub enum TokenTree {\n-    // a single token\n+    /// A single token\n     TTTok(Span, ::parse::token::Token),\n-    // a delimited sequence (the delimiters appear as the first\n-    // and last elements of the vector)\n+    /// A delimited sequence (the delimiters appear as the first\n+    /// and last elements of the vector)\n     // FIXME(eddyb) #6308 Use Rc<[TokenTree]> after DST.\n     TTDelim(Rc<Vec<TokenTree>>),\n \n     // These only make sense for right-hand-sides of MBE macros:\n \n-    // a kleene-style repetition sequence with a span, a TTForest,\n-    // an optional separator, and a boolean where true indicates\n-    // zero or more (..), and false indicates one or more (+).\n+    /// A kleene-style repetition sequence with a span, a TTForest,\n+    /// an optional separator, and a boolean where true indicates\n+    /// zero or more (..), and false indicates one or more (+).\n     // FIXME(eddyb) #6308 Use Rc<[TokenTree]> after DST.\n     TTSeq(Span, Rc<Vec<TokenTree>>, Option<::parse::token::Token>, bool),\n \n-    // a syntactic variable that will be filled in by macro expansion.\n+    /// A syntactic variable that will be filled in by macro expansion.\n     TTNonterminal(Span, Ident)\n }\n \n-//\n-// Matchers are nodes defined-by and recognized-by the main rust parser and\n-// language, but they're only ever found inside syntax-extension invocations;\n-// indeed, the only thing that ever _activates_ the rules in the rust parser\n-// for parsing a matcher is a matcher looking for the 'matchers' nonterminal\n-// itself. Matchers represent a small sub-language for pattern-matching\n-// token-trees, and are thus primarily used by the macro-defining extension\n-// itself.\n-//\n-// MatchTok\n-// --------\n-//\n-//     A matcher that matches a single token, denoted by the token itself. So\n-//     long as there's no $ involved.\n-//\n-//\n-// MatchSeq\n-// --------\n-//\n-//     A matcher that matches a sequence of sub-matchers, denoted various\n-//     possible ways:\n-//\n-//             $(M)*       zero or more Ms\n-//             $(M)+       one or more Ms\n-//             $(M),+      one or more comma-separated Ms\n-//             $(A B C);*  zero or more semi-separated 'A B C' seqs\n-//\n-//\n-// MatchNonterminal\n-// -----------------\n-//\n-//     A matcher that matches one of a few interesting named rust\n-//     nonterminals, such as types, expressions, items, or raw token-trees. A\n-//     black-box matcher on expr, for example, binds an expr to a given ident,\n-//     and that ident can re-occur as an interpolation in the RHS of a\n-//     macro-by-example rule. For example:\n-//\n-//        $foo:expr   =>     1 + $foo    // interpolate an expr\n-//        $foo:tt     =>     $foo        // interpolate a token-tree\n-//        $foo:tt     =>     bar! $foo   // only other valid interpolation\n-//                                       // is in arg position for another\n-//                                       // macro\n-//\n-// As a final, horrifying aside, note that macro-by-example's input is\n-// also matched by one of these matchers. Holy self-referential! It is matched\n-// by a MatchSeq, specifically this one:\n-//\n-//                   $( $lhs:matchers => $rhs:tt );+\n-//\n-// If you understand that, you have closed to loop and understand the whole\n-// macro system. Congratulations.\n-//\n+/// Matchers are nodes defined-by and recognized-by the main rust parser and\n+/// language, but they're only ever found inside syntax-extension invocations;\n+/// indeed, the only thing that ever _activates_ the rules in the rust parser\n+/// for parsing a matcher is a matcher looking for the 'matchers' nonterminal\n+/// itself. Matchers represent a small sub-language for pattern-matching\n+/// token-trees, and are thus primarily used by the macro-defining extension\n+/// itself.\n+///\n+/// MatchTok\n+/// --------\n+///\n+///     A matcher that matches a single token, denoted by the token itself. So\n+///     long as there's no $ involved.\n+///\n+///\n+/// MatchSeq\n+/// --------\n+///\n+///     A matcher that matches a sequence of sub-matchers, denoted various\n+///     possible ways:\n+///\n+///             $(M)*       zero or more Ms\n+///             $(M)+       one or more Ms\n+///             $(M),+      one or more comma-separated Ms\n+///             $(A B C);*  zero or more semi-separated 'A B C' seqs\n+///\n+///\n+/// MatchNonterminal\n+/// -----------------\n+///\n+///     A matcher that matches one of a few interesting named rust\n+///     nonterminals, such as types, expressions, items, or raw token-trees. A\n+///     black-box matcher on expr, for example, binds an expr to a given ident,\n+///     and that ident can re-occur as an interpolation in the RHS of a\n+///     macro-by-example rule. For example:\n+///\n+///        $foo:expr   =>     1 + $foo    // interpolate an expr\n+///        $foo:tt     =>     $foo        // interpolate a token-tree\n+///        $foo:tt     =>     bar! $foo   // only other valid interpolation\n+///                                       // is in arg position for another\n+///                                       // macro\n+///\n+/// As a final, horrifying aside, note that macro-by-example's input is\n+/// also matched by one of these matchers. Holy self-referential! It is matched\n+/// by a MatchSeq, specifically this one:\n+///\n+///                   $( $lhs:matchers => $rhs:tt );+\n+///\n+/// If you understand that, you have closed the loop and understand the whole\n+/// macro system. Congratulations.\n pub type Matcher = Spanned<Matcher_>;\n \n #[deriving(Clone, PartialEq, Eq, Encodable, Decodable, Hash)]\n pub enum Matcher_ {\n-    // match one token\n+    /// Match one token\n     MatchTok(::parse::token::Token),\n-    // match repetitions of a sequence: body, separator, zero ok?,\n-    // lo, hi position-in-match-array used:\n+    /// Match repetitions of a sequence: body, separator, zero ok?,\n+    /// lo, hi position-in-match-array used:\n     MatchSeq(Vec<Matcher> , Option<::parse::token::Token>, bool, uint, uint),\n-    // parse a Rust NT: name to bind, name of NT, position in match array:\n+    /// Parse a Rust NT: name to bind, name of NT, position in match array:\n     MatchNonterminal(Ident, Ident, uint)\n }\n \n pub type Mac = Spanned<Mac_>;\n \n-// represents a macro invocation. The Path indicates which macro\n-// is being invoked, and the vector of token-trees contains the source\n-// of the macro invocation.\n-// There's only one flavor, now, so this could presumably be simplified.\n+/// Represents a macro invocation. The Path indicates which macro\n+/// is being invoked, and the vector of token-trees contains the source\n+/// of the macro invocation.\n+/// There's only one flavor, now, so this could presumably be simplified.\n #[deriving(Clone, PartialEq, Eq, Encodable, Decodable, Hash)]\n pub enum Mac_ {\n     MacInvocTT(Path, Vec<TokenTree> , SyntaxContext),   // new macro-invocation\n }\n \n-#[deriving(Clone, PartialEq, Eq, Encodable, Decodable, Hash)]\n+#[deriving(Clone, PartialEq, Eq, Encodable, Decodable, Hash, Show)]\n pub enum StrStyle {\n     CookedStr,\n     RawStr(uint)\n }\n \n pub type Lit = Spanned<Lit_>;\n \n-#[deriving(Clone, PartialEq, Eq, Encodable, Decodable, Hash)]\n+#[deriving(Clone, PartialEq, Eq, Encodable, Decodable, Hash, Show)]\n pub enum Lit_ {\n     LitStr(InternedString, StrStyle),\n     LitBinary(Rc<Vec<u8> >),\n@@ -659,11 +688,10 @@ pub struct TypeMethod {\n     pub vis: Visibility,\n }\n \n-/// Represents a method declaration in a trait declaration, possibly\n-/// including a default implementation\n-// A trait method is either required (meaning it doesn't have an\n-// implementation, just a signature) or provided (meaning it has a default\n-// implementation).\n+/// Represents a method declaration in a trait declaration, possibly including\n+/// a default implementation A trait method is either required (meaning it\n+/// doesn't have an implementation, just a signature) or provided (meaning it\n+/// has a default implementation).\n #[deriving(Clone, PartialEq, Eq, Encodable, Decodable, Hash)]\n pub enum TraitMethod {\n     Required(TypeMethod),\n@@ -685,6 +713,16 @@ impl fmt::Show for IntTy {\n     }\n }\n \n+impl IntTy {\n+    pub fn suffix_len(&self) -> uint {\n+        match *self {\n+            TyI => 1,\n+            TyI8 => 2,\n+            TyI16 | TyI32 | TyI64  => 3,\n+        }\n+    }\n+}\n+\n #[deriving(Clone, PartialEq, Eq, Encodable, Decodable, Hash)]\n pub enum UintTy {\n     TyU,\n@@ -694,6 +732,16 @@ pub enum UintTy {\n     TyU64,\n }\n \n+impl UintTy {\n+    pub fn suffix_len(&self) -> uint {\n+        match *self {\n+            TyU => 1,\n+            TyU8 => 2,\n+            TyU16 | TyU32 | TyU64  => 3,\n+        }\n+    }\n+}\n+\n impl fmt::Show for UintTy {\n     fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {\n         write!(f, \"{}\", ast_util::uint_ty_to_string(*self, None))\n@@ -712,6 +760,14 @@ impl fmt::Show for FloatTy {\n     }\n }\n \n+impl FloatTy {\n+    pub fn suffix_len(&self) -> uint {\n+        match *self {\n+            TyF32 | TyF64 => 3, // add F128 handling here\n+        }\n+    }\n+}\n+\n // NB PartialEq method appears below.\n #[deriving(Clone, PartialEq, Eq, Encodable, Decodable, Hash)]\n pub struct Ty {\n@@ -720,7 +776,7 @@ pub struct Ty {\n     pub span: Span,\n }\n \n-// Not represented directly in the AST, referred to by name through a ty_path.\n+/// Not represented directly in the AST, referred to by name through a ty_path.\n #[deriving(Clone, PartialEq, Eq, Encodable, Decodable, Hash)]\n pub enum PrimTy {\n     TyInt(IntTy),\n@@ -753,10 +809,10 @@ pub struct ClosureTy {\n     pub fn_style: FnStyle,\n     pub onceness: Onceness,\n     pub decl: P<FnDecl>,\n-    // Optional optvec distinguishes between \"fn()\" and \"fn:()\" so we can\n-    // implement issue #7264. None means \"fn()\", which means infer a default\n-    // bound based on pointer sigil during typeck. Some(Empty) means \"fn:()\",\n-    // which means use no bounds (e.g., not even Owned on a ~fn()).\n+    /// Optional optvec distinguishes between \"fn()\" and \"fn:()\" so we can\n+    /// implement issue #7264. None means \"fn()\", which means infer a default\n+    /// bound based on pointer sigil during typeck. Some(Empty) means \"fn:()\",\n+    /// which means use no bounds (e.g., not even Owned on a ~fn()).\n     pub bounds: Option<OwnedSlice<TyParamBound>>,\n }\n \n@@ -789,11 +845,11 @@ pub enum Ty_ {\n     TyUnboxedFn(Gc<UnboxedFnTy>),\n     TyTup(Vec<P<Ty>> ),\n     TyPath(Path, Option<OwnedSlice<TyParamBound>>, NodeId), // for #7264; see above\n-    // No-op; kept solely so that we can pretty-print faithfully\n+    /// No-op; kept solely so that we can pretty-print faithfully\n     TyParen(P<Ty>),\n     TyTypeof(Gc<Expr>),\n-    // TyInfer means the type should be inferred instead of it having been\n-    // specified. This can appear anywhere in a type.\n+    /// TyInfer means the type should be inferred instead of it having been\n+    /// specified. This can appear anywhere in a type.\n     TyInfer,\n }\n \n@@ -854,8 +910,10 @@ pub struct FnDecl {\n \n #[deriving(Clone, PartialEq, Eq, Encodable, Decodable, Hash)]\n pub enum FnStyle {\n-    UnsafeFn, // declared with \"unsafe fn\"\n-    NormalFn, // declared with \"fn\"\n+    /// Declared with \"unsafe fn\"\n+    UnsafeFn,\n+    /// Declared with \"fn\"\n+    NormalFn,\n }\n \n impl fmt::Show for FnStyle {\n@@ -869,18 +927,24 @@ impl fmt::Show for FnStyle {\n \n #[deriving(Clone, PartialEq, Eq, Encodable, Decodable, Hash)]\n pub enum RetStyle {\n-    NoReturn, // functions with return type _|_ that always\n-              // raise an error or exit (i.e. never return to the caller)\n-    Return, // everything else\n+    /// Functions with return type ! that always\n+    /// raise an error or exit (i.e. never return to the caller)\n+    NoReturn,\n+    /// Everything else\n+    Return,\n }\n \n /// Represents the kind of 'self' associated with a method\n #[deriving(Clone, PartialEq, Eq, Encodable, Decodable, Hash)]\n pub enum ExplicitSelf_ {\n-    SelfStatic,                                       // no self\n-    SelfValue(Ident),                                 // `self`\n-    SelfRegion(Option<Lifetime>, Mutability, Ident),  // `&'lt self`, `&'lt mut self`\n-    SelfUniq(Ident),                                  // `~self`\n+    /// No self\n+    SelfStatic,\n+    /// `self\n+    SelfValue(Ident),\n+    /// `&'lt self`, `&'lt mut self`\n+    SelfRegion(Option<Lifetime>, Mutability, Ident),\n+    /// `~self`\n+    SelfUniq(Ident)\n }\n \n pub type ExplicitSelf = Spanned<ExplicitSelf_>;\n@@ -959,17 +1023,17 @@ pub type ViewPath = Spanned<ViewPath_>;\n #[deriving(PartialEq, Eq, Encodable, Decodable, Hash)]\n pub enum ViewPath_ {\n \n-    // quux = foo::bar::baz\n-    //\n-    // or just\n-    //\n-    // foo::bar::baz  (with 'baz =' implicitly on the left)\n+    /// `quux = foo::bar::baz`\n+    ///\n+    /// or just\n+    ///\n+    /// `foo::bar::baz ` (with 'baz =' implicitly on the left)\n     ViewPathSimple(Ident, Path, NodeId),\n \n-    // foo::bar::*\n+    /// `foo::bar::*`\n     ViewPathGlob(Path, NodeId),\n \n-    // foo::bar::{a,b,c}\n+    /// `foo::bar::{a,b,c}`\n     ViewPathList(Path, Vec<PathListIdent> , NodeId)\n }\n \n@@ -983,20 +1047,20 @@ pub struct ViewItem {\n \n #[deriving(Clone, PartialEq, Eq, Encodable, Decodable, Hash)]\n pub enum ViewItem_ {\n-    // ident: name used to refer to this crate in the code\n-    // optional (InternedString,StrStyle): if present, this is a location\n-    // (containing arbitrary characters) from which to fetch the crate sources\n-    // For example, extern crate whatever = \"github.com/rust-lang/rust\"\n+    /// Ident: name used to refer to this crate in the code\n+    /// optional (InternedString,StrStyle): if present, this is a location\n+    /// (containing arbitrary characters) from which to fetch the crate sources\n+    /// For example, extern crate whatever = \"github.com/rust-lang/rust\"\n     ViewItemExternCrate(Ident, Option<(InternedString,StrStyle)>, NodeId),\n     ViewItemUse(Gc<ViewPath>),\n }\n \n-// Meta-data associated with an item\n+/// Meta-data associated with an item\n pub type Attribute = Spanned<Attribute_>;\n \n-// Distinguishes between Attributes that decorate items and Attributes that\n-// are contained as statements within items. These two cases need to be\n-// distinguished for pretty-printing.\n+/// Distinguishes between Attributes that decorate items and Attributes that\n+/// are contained as statements within items. These two cases need to be\n+/// distinguished for pretty-printing.\n #[deriving(Clone, PartialEq, Eq, Encodable, Decodable, Hash)]\n pub enum AttrStyle {\n     AttrOuter,\n@@ -1006,7 +1070,7 @@ pub enum AttrStyle {\n #[deriving(Clone, PartialEq, Eq, Encodable, Decodable, Hash)]\n pub struct AttrId(pub uint);\n \n-// doc-comments are promoted to attributes that have is_sugared_doc = true\n+/// Doc-comments are promoted to attributes that have is_sugared_doc = true\n #[deriving(Clone, PartialEq, Eq, Encodable, Decodable, Hash)]\n pub struct Attribute_ {\n     pub id: AttrId,\n@@ -1015,13 +1079,12 @@ pub struct Attribute_ {\n     pub is_sugared_doc: bool,\n }\n \n-/*\n-  TraitRef's appear in impls.\n-  resolve maps each TraitRef's ref_id to its defining trait; that's all\n-  that the ref_id is for. The impl_id maps to the \"self type\" of this impl.\n-  If this impl is an ItemImpl, the impl_id is redundant (it could be the\n-  same as the impl's node id).\n- */\n+\n+/// TraitRef's appear in impls.\n+/// resolve maps each TraitRef's ref_id to its defining trait; that's all\n+/// that the ref_id is for. The impl_id maps to the \"self type\" of this impl.\n+/// If this impl is an ItemImpl, the impl_id is redundant (it could be the\n+/// same as the impl's node id).\n #[deriving(Clone, PartialEq, Eq, Encodable, Decodable, Hash)]\n pub struct TraitRef {\n     pub path: Path,\n@@ -1065,7 +1128,8 @@ pub type StructField = Spanned<StructField_>;\n #[deriving(Clone, PartialEq, Eq, Encodable, Decodable, Hash)]\n pub enum StructFieldKind {\n     NamedField(Ident, Visibility),\n-    UnnamedField(Visibility), // element of a tuple-like struct\n+    /// Element of a tuple-like struct\n+    UnnamedField(Visibility),\n }\n \n impl StructFieldKind {\n@@ -1079,12 +1143,15 @@ impl StructFieldKind {\n \n #[deriving(PartialEq, Eq, Encodable, Decodable, Hash)]\n pub struct StructDef {\n-    pub fields: Vec<StructField>, /* fields, not including ctor */\n-    /* ID of the constructor. This is only used for tuple- or enum-like\n-     * structs. */\n+    /// Fields, not including ctor\n+    pub fields: Vec<StructField>,\n+    /// ID of the constructor. This is only used for tuple- or enum-like\n+    /// structs.\n     pub ctor_id: Option<NodeId>,\n-    pub super_struct: Option<P<Ty>>, // Super struct, if specified.\n-    pub is_virtual: bool,            // True iff the struct may be inherited from.\n+    /// Super struct, if specified.\n+    pub super_struct: Option<P<Ty>>,\n+    /// True iff the struct may be inherited from.\n+    pub is_virtual: bool,\n }\n \n /*\n@@ -1120,7 +1187,7 @@ pub enum Item_ {\n              Option<TraitRef>, // (optional) trait this impl implements\n              P<Ty>, // self\n              Vec<Gc<Method>>),\n-    // a macro invocation (which includes macro definition)\n+    /// A macro invocation (which includes macro definition)\n     ItemMac(Mac),\n }\n \n@@ -1140,9 +1207,9 @@ pub enum ForeignItem_ {\n     ForeignItemStatic(P<Ty>, /* is_mutbl */ bool),\n }\n \n-// The data we save and restore about an inlined item or method.  This is not\n-// part of the AST that we parse from a file, but it becomes part of the tree\n-// that we trans.\n+/// The data we save and restore about an inlined item or method.  This is not\n+/// part of the AST that we parse from a file, but it becomes part of the tree\n+/// that we trans.\n #[deriving(PartialEq, Eq, Encodable, Decodable, Hash)]\n pub enum InlinedItem {\n     IIItem(Gc<Item>),"}, {"sha": "25c8e81bdbc91397bb5e5bb8436eb69be026289a", "filename": "src/libsyntax/ast_map.rs", "status": "modified", "additions": 11, "deletions": 11, "changes": 22, "blob_url": "https://github.com/rust-lang/rust/blob/ad3eda123a00b3ddf8897295ff26d97ba7c64d44/src%2Flibsyntax%2Fast_map.rs", "raw_url": "https://github.com/rust-lang/rust/raw/ad3eda123a00b3ddf8897295ff26d97ba7c64d44/src%2Flibsyntax%2Fast_map.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fast_map.rs?ref=ad3eda123a00b3ddf8897295ff26d97ba7c64d44", "patch": "@@ -112,13 +112,13 @@ pub enum Node {\n     NodeLifetime(Gc<Lifetime>),\n }\n \n-// The odd layout is to bring down the total size.\n+/// The odd layout is to bring down the total size.\n #[deriving(Clone)]\n enum MapEntry {\n-    // Placeholder for holes in the map.\n+    /// Placeholder for holes in the map.\n     NotPresent,\n \n-    // All the node types, with a parent ID.\n+    /// All the node types, with a parent ID.\n     EntryItem(NodeId, Gc<Item>),\n     EntryForeignItem(NodeId, Gc<ForeignItem>),\n     EntryTraitMethod(NodeId, Gc<TraitMethod>),\n@@ -133,14 +133,14 @@ enum MapEntry {\n     EntryStructCtor(NodeId, Gc<StructDef>),\n     EntryLifetime(NodeId, Gc<Lifetime>),\n \n-    // Roots for node trees.\n+    /// Roots for node trees.\n     RootCrate,\n     RootInlinedParent(P<InlinedParent>)\n }\n \n struct InlinedParent {\n     path: Vec<PathElem> ,\n-    // Required by NodeTraitMethod and NodeMethod.\n+    /// Required by NodeTraitMethod and NodeMethod.\n     def_id: DefId\n }\n \n@@ -243,7 +243,7 @@ impl Map {\n                 ItemForeignMod(ref nm) => Some(nm.abi),\n                 _ => None\n             },\n-            // Wrong but OK, because the only inlined foreign items are intrinsics.\n+            /// Wrong but OK, because the only inlined foreign items are intrinsics.\n             Some(RootInlinedParent(_)) => Some(abi::RustIntrinsic),\n             _ => None\n         };\n@@ -432,8 +432,8 @@ pub trait FoldOps {\n \n pub struct Ctx<'a, F> {\n     map: &'a Map,\n-    // The node in which we are currently mapping (an item or a method).\n-    // When equal to DUMMY_NODE_ID, the next mapped node becomes the parent.\n+    /// The node in which we are currently mapping (an item or a method).\n+    /// When equal to DUMMY_NODE_ID, the next mapped node becomes the parent.\n     parent: NodeId,\n     fold_ops: F\n }\n@@ -618,9 +618,9 @@ pub fn map_crate<F: FoldOps>(krate: Crate, fold_ops: F) -> (Crate, Map) {\n     (krate, map)\n }\n \n-// Used for items loaded from external crate that are being inlined into this\n-// crate.  The `path` should be the path to the item but should not include\n-// the item itself.\n+/// Used for items loaded from external crate that are being inlined into this\n+/// crate.  The `path` should be the path to the item but should not include\n+/// the item itself.\n pub fn map_decoded_item<F: FoldOps>(map: &Map,\n                                     path: Vec<PathElem> ,\n                                     fold_ops: F,"}, {"sha": "13fe8a150645952334ba3ea1f779919d3ee49417", "filename": "src/libsyntax/ast_util.rs", "status": "modified", "additions": 11, "deletions": 11, "changes": 22, "blob_url": "https://github.com/rust-lang/rust/blob/ad3eda123a00b3ddf8897295ff26d97ba7c64d44/src%2Flibsyntax%2Fast_util.rs", "raw_url": "https://github.com/rust-lang/rust/raw/ad3eda123a00b3ddf8897295ff26d97ba7c64d44/src%2Flibsyntax%2Fast_util.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fast_util.rs?ref=ad3eda123a00b3ddf8897295ff26d97ba7c64d44", "patch": "@@ -101,8 +101,8 @@ pub fn is_path(e: Gc<Expr>) -> bool {\n     return match e.node { ExprPath(_) => true, _ => false };\n }\n \n-// Get a string representation of a signed int type, with its value.\n-// We want to avoid \"45int\" and \"-3int\" in favor of \"45\" and \"-3\"\n+/// Get a string representation of a signed int type, with its value.\n+/// We want to avoid \"45int\" and \"-3int\" in favor of \"45\" and \"-3\"\n pub fn int_ty_to_string(t: IntTy, val: Option<i64>) -> String {\n     let s = match t {\n         TyI if val.is_some() => \"i\",\n@@ -131,8 +131,8 @@ pub fn int_ty_max(t: IntTy) -> u64 {\n     }\n }\n \n-// Get a string representation of an unsigned int type, with its value.\n-// We want to avoid \"42uint\" in favor of \"42u\"\n+/// Get a string representation of an unsigned int type, with its value.\n+/// We want to avoid \"42uint\" in favor of \"42u\"\n pub fn uint_ty_to_string(t: UintTy, val: Option<u64>) -> String {\n     let s = match t {\n         TyU if val.is_some() => \"u\",\n@@ -249,8 +249,8 @@ pub fn public_methods(ms: Vec<Gc<Method>> ) -> Vec<Gc<Method>> {\n     }).collect()\n }\n \n-// extract a TypeMethod from a TraitMethod. if the TraitMethod is\n-// a default, pull out the useful fields to make a TypeMethod\n+/// extract a TypeMethod from a TraitMethod. if the TraitMethod is\n+/// a default, pull out the useful fields to make a TypeMethod\n pub fn trait_method_to_ty_method(method: &TraitMethod) -> TypeMethod {\n     match *method {\n         Required(ref m) => (*m).clone(),\n@@ -705,7 +705,7 @@ pub fn segments_name_eq(a : &[ast::PathSegment], b : &[ast::PathSegment]) -> boo\n     }\n }\n \n-// Returns true if this literal is a string and false otherwise.\n+/// Returns true if this literal is a string and false otherwise.\n pub fn lit_is_str(lit: Gc<Lit>) -> bool {\n     match lit.node {\n         LitStr(..) => true,\n@@ -754,14 +754,14 @@ mod test {\n \n     #[test] fn idents_name_eq_test() {\n         assert!(segments_name_eq(\n-            [Ident{name:3,ctxt:4}, Ident{name:78,ctxt:82}]\n+            [Ident{name:Name(3),ctxt:4}, Ident{name:Name(78),ctxt:82}]\n                 .iter().map(ident_to_segment).collect::<Vec<PathSegment>>().as_slice(),\n-            [Ident{name:3,ctxt:104}, Ident{name:78,ctxt:182}]\n+            [Ident{name:Name(3),ctxt:104}, Ident{name:Name(78),ctxt:182}]\n                 .iter().map(ident_to_segment).collect::<Vec<PathSegment>>().as_slice()));\n         assert!(!segments_name_eq(\n-            [Ident{name:3,ctxt:4}, Ident{name:78,ctxt:82}]\n+            [Ident{name:Name(3),ctxt:4}, Ident{name:Name(78),ctxt:82}]\n                 .iter().map(ident_to_segment).collect::<Vec<PathSegment>>().as_slice(),\n-            [Ident{name:3,ctxt:104}, Ident{name:77,ctxt:182}]\n+            [Ident{name:Name(3),ctxt:104}, Ident{name:Name(77),ctxt:182}]\n                 .iter().map(ident_to_segment).collect::<Vec<PathSegment>>().as_slice()));\n     }\n }"}, {"sha": "e8b9ec9628f7deba896a0b87283faf22dd264842", "filename": "src/libsyntax/attr.rs", "status": "modified", "additions": 12, "deletions": 16, "changes": 28, "blob_url": "https://github.com/rust-lang/rust/blob/ad3eda123a00b3ddf8897295ff26d97ba7c64d44/src%2Flibsyntax%2Fattr.rs", "raw_url": "https://github.com/rust-lang/rust/raw/ad3eda123a00b3ddf8897295ff26d97ba7c64d44/src%2Flibsyntax%2Fattr.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fattr.rs?ref=ad3eda123a00b3ddf8897295ff26d97ba7c64d44", "patch": "@@ -46,10 +46,8 @@ pub trait AttrMetaMethods {\n     /// #[foo=\"bar\"] and #[foo(bar)]\n     fn name(&self) -> InternedString;\n \n-    /**\n-     * Gets the string value if self is a MetaNameValue variant\n-     * containing a string, otherwise None.\n-     */\n+    /// Gets the string value if self is a MetaNameValue variant\n+    /// containing a string, otherwise None.\n     fn value_str(&self) -> Option<InternedString>;\n     /// Gets a list of inner meta items from a list MetaItem type.\n     fn meta_item_list<'a>(&'a self) -> Option<&'a [Gc<MetaItem>]>;\n@@ -420,18 +418,16 @@ pub fn require_unique_names(diagnostic: &SpanHandler, metas: &[Gc<MetaItem>]) {\n }\n \n \n-/**\n- * Fold this over attributes to parse #[repr(...)] forms.\n- *\n- * Valid repr contents: any of the primitive integral type names (see\n- * `int_type_of_word`, below) to specify the discriminant type; and `C`, to use\n- * the same discriminant size that the corresponding C enum would.  These are\n- * not allowed on univariant or zero-variant enums, which have no discriminant.\n- *\n- * If a discriminant type is so specified, then the discriminant will be\n- * present (before fields, if any) with that type; reprensentation\n- * optimizations which would remove it will not be done.\n- */\n+/// Fold this over attributes to parse #[repr(...)] forms.\n+///\n+/// Valid repr contents: any of the primitive integral type names (see\n+/// `int_type_of_word`, below) to specify the discriminant type; and `C`, to use\n+/// the same discriminant size that the corresponding C enum would.  These are\n+/// not allowed on univariant or zero-variant enums, which have no discriminant.\n+///\n+/// If a discriminant type is so specified, then the discriminant will be\n+/// present (before fields, if any) with that type; reprensentation\n+/// optimizations which would remove it will not be done.\n pub fn find_repr_attr(diagnostic: &SpanHandler, attr: &Attribute, acc: ReprAttr)\n     -> ReprAttr {\n     let mut acc = acc;"}, {"sha": "ef4024a8f83fe0cabb79fda2d0250998c747db17", "filename": "src/libsyntax/codemap.rs", "status": "modified", "additions": 16, "deletions": 17, "changes": 33, "blob_url": "https://github.com/rust-lang/rust/blob/ad3eda123a00b3ddf8897295ff26d97ba7c64d44/src%2Flibsyntax%2Fcodemap.rs", "raw_url": "https://github.com/rust-lang/rust/raw/ad3eda123a00b3ddf8897295ff26d97ba7c64d44/src%2Flibsyntax%2Fcodemap.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fcodemap.rs?ref=ad3eda123a00b3ddf8897295ff26d97ba7c64d44", "patch": "@@ -96,7 +96,7 @@ pub struct Span {\n \n pub static DUMMY_SP: Span = Span { lo: BytePos(0), hi: BytePos(0), expn_info: None };\n \n-#[deriving(Clone, PartialEq, Eq, Encodable, Decodable, Hash)]\n+#[deriving(Clone, PartialEq, Eq, Encodable, Decodable, Hash, Show)]\n pub struct Spanned<T> {\n     pub node: T,\n     pub span: Span,\n@@ -252,15 +252,15 @@ pub struct FileMap {\n }\n \n impl FileMap {\n-    // EFFECT: register a start-of-line offset in the\n-    // table of line-beginnings.\n-    // UNCHECKED INVARIANT: these offsets must be added in the right\n-    // order and must be in the right places; there is shared knowledge\n-    // about what ends a line between this file and parse.rs\n-    // WARNING: pos param here is the offset relative to start of CodeMap,\n-    // and CodeMap will append a newline when adding a filemap without a newline at the end,\n-    // so the safe way to call this is with value calculated as\n-    // filemap.start_pos + newline_offset_relative_to_the_start_of_filemap.\n+    /// EFFECT: register a start-of-line offset in the\n+    /// table of line-beginnings.\n+    /// UNCHECKED INVARIANT: these offsets must be added in the right\n+    /// order and must be in the right places; there is shared knowledge\n+    /// about what ends a line between this file and parse.rs\n+    /// WARNING: pos param here is the offset relative to start of CodeMap,\n+    /// and CodeMap will append a newline when adding a filemap without a newline at the end,\n+    /// so the safe way to call this is with value calculated as\n+    /// filemap.start_pos + newline_offset_relative_to_the_start_of_filemap.\n     pub fn next_line(&self, pos: BytePos) {\n         // the new charpos must be > the last one (or it's the first one).\n         let mut lines = self.lines.borrow_mut();;\n@@ -269,7 +269,7 @@ impl FileMap {\n         lines.push(pos);\n     }\n \n-    // get a line from the list of pre-computed line-beginnings\n+    /// get a line from the list of pre-computed line-beginnings\n     pub fn get_line(&self, line: int) -> String {\n         let mut lines = self.lines.borrow_mut();\n         let begin: BytePos = *lines.get(line as uint) - self.start_pos;\n@@ -428,9 +428,8 @@ impl CodeMap {\n         FileMapAndBytePos {fm: fm, pos: offset}\n     }\n \n-    // Converts an absolute BytePos to a CharPos relative to the filemap and above.\n+    /// Converts an absolute BytePos to a CharPos relative to the filemap and above.\n     pub fn bytepos_to_file_charpos(&self, bpos: BytePos) -> CharPos {\n-        debug!(\"codemap: converting {:?} to char pos\", bpos);\n         let idx = self.lookup_filemap_idx(bpos);\n         let files = self.files.borrow();\n         let map = files.get(idx);\n@@ -439,7 +438,7 @@ impl CodeMap {\n         let mut total_extra_bytes = 0;\n \n         for mbc in map.multibyte_chars.borrow().iter() {\n-            debug!(\"codemap: {:?}-byte char at {:?}\", mbc.bytes, mbc.pos);\n+            debug!(\"{}-byte char at {}\", mbc.bytes, mbc.pos);\n             if mbc.pos < bpos {\n                 // every character is at least one byte, so we only\n                 // count the actual extra bytes.\n@@ -514,11 +513,11 @@ impl CodeMap {\n         let chpos = self.bytepos_to_file_charpos(pos);\n         let linebpos = *f.lines.borrow().get(a);\n         let linechpos = self.bytepos_to_file_charpos(linebpos);\n-        debug!(\"codemap: byte pos {:?} is on the line at byte pos {:?}\",\n+        debug!(\"byte pos {} is on the line at byte pos {}\",\n                pos, linebpos);\n-        debug!(\"codemap: char pos {:?} is on the line at char pos {:?}\",\n+        debug!(\"char pos {} is on the line at char pos {}\",\n                chpos, linechpos);\n-        debug!(\"codemap: byte is on line: {:?}\", line);\n+        debug!(\"byte is on line: {}\", line);\n         assert!(chpos >= linechpos);\n         Loc {\n             file: f,"}, {"sha": "e469f327ae8ba403a3620ad65d0189978095f4a0", "filename": "src/libsyntax/diagnostic.rs", "status": "modified", "additions": 13, "deletions": 13, "changes": 26, "blob_url": "https://github.com/rust-lang/rust/blob/ad3eda123a00b3ddf8897295ff26d97ba7c64d44/src%2Flibsyntax%2Fdiagnostic.rs", "raw_url": "https://github.com/rust-lang/rust/raw/ad3eda123a00b3ddf8897295ff26d97ba7c64d44/src%2Flibsyntax%2Fdiagnostic.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fdiagnostic.rs?ref=ad3eda123a00b3ddf8897295ff26d97ba7c64d44", "patch": "@@ -21,7 +21,7 @@ use std::string::String;\n use term::WriterWrapper;\n use term;\n \n-// maximum number of lines we will print for each error; arbitrary.\n+/// maximum number of lines we will print for each error; arbitrary.\n static MAX_LINES: uint = 6u;\n \n #[deriving(Clone)]\n@@ -73,9 +73,9 @@ pub struct FatalError;\n /// or `.span_bug` rather than a failed assertion, etc.\n pub struct ExplicitBug;\n \n-// a span-handler is like a handler but also\n-// accepts span information for source-location\n-// reporting.\n+/// A span-handler is like a handler but also\n+/// accepts span information for source-location\n+/// reporting.\n pub struct SpanHandler {\n     pub handler: Handler,\n     pub cm: codemap::CodeMap,\n@@ -114,9 +114,9 @@ impl SpanHandler {\n     }\n }\n \n-// a handler deals with errors; certain errors\n-// (fatal, bug, unimpl) may cause immediate exit,\n-// others log errors for later reporting.\n+/// A handler deals with errors; certain errors\n+/// (fatal, bug, unimpl) may cause immediate exit,\n+/// others log errors for later reporting.\n pub struct Handler {\n     err_count: Cell<uint>,\n     emit: RefCell<Box<Emitter + Send>>,\n@@ -442,12 +442,12 @@ fn highlight_lines(err: &mut EmitterWriter,\n     Ok(())\n }\n \n-// Here are the differences between this and the normal `highlight_lines`:\n-// `custom_highlight_lines` will always put arrow on the last byte of the\n-// span (instead of the first byte). Also, when the span is too long (more\n-// than 6 lines), `custom_highlight_lines` will print the first line, then\n-// dot dot dot, then last line, whereas `highlight_lines` prints the first\n-// six lines.\n+/// Here are the differences between this and the normal `highlight_lines`:\n+/// `custom_highlight_lines` will always put arrow on the last byte of the\n+/// span (instead of the first byte). Also, when the span is too long (more\n+/// than 6 lines), `custom_highlight_lines` will print the first line, then\n+/// dot dot dot, then last line, whereas `highlight_lines` prints the first\n+/// six lines.\n fn custom_highlight_lines(w: &mut EmitterWriter,\n                           cm: &codemap::CodeMap,\n                           sp: Span,"}, {"sha": "9a5c7e86d21c6df035f0df0fa52f617eed43e8c8", "filename": "src/libsyntax/ext/base.rs", "status": "modified", "additions": 17, "deletions": 15, "changes": 32, "blob_url": "https://github.com/rust-lang/rust/blob/ad3eda123a00b3ddf8897295ff26d97ba7c64d44/src%2Flibsyntax%2Fext%2Fbase.rs", "raw_url": "https://github.com/rust-lang/rust/raw/ad3eda123a00b3ddf8897295ff26d97ba7c64d44/src%2Flibsyntax%2Fext%2Fbase.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fext%2Fbase.rs?ref=ad3eda123a00b3ddf8897295ff26d97ba7c64d44", "patch": "@@ -278,9 +278,9 @@ pub enum SyntaxExtension {\n pub type NamedSyntaxExtension = (Name, SyntaxExtension);\n \n pub struct BlockInfo {\n-    // should macros escape from this scope?\n+    /// Should macros escape from this scope?\n     pub macros_escape: bool,\n-    // what are the pending renames?\n+    /// What are the pending renames?\n     pub pending_renames: mtwt::RenameList,\n }\n \n@@ -293,8 +293,8 @@ impl BlockInfo {\n     }\n }\n \n-// The base map of methods for expanding syntax extension\n-// AST nodes into full ASTs\n+/// The base map of methods for expanding syntax extension\n+/// AST nodes into full ASTs\n pub fn syntax_expander_table() -> SyntaxEnv {\n     // utility function to simplify creating NormalTT syntax extensions\n     fn builtin_normal_expander(f: MacroExpanderFn) -> SyntaxExtension {\n@@ -398,9 +398,9 @@ pub fn syntax_expander_table() -> SyntaxEnv {\n     syntax_expanders\n }\n \n-// One of these is made during expansion and incrementally updated as we go;\n-// when a macro expansion occurs, the resulting nodes have the backtrace()\n-// -> expn_info of their expansion context stored into their span.\n+/// One of these is made during expansion and incrementally updated as we go;\n+/// when a macro expansion occurs, the resulting nodes have the backtrace()\n+/// -> expn_info of their expansion context stored into their span.\n pub struct ExtCtxt<'a> {\n     pub parse_sess: &'a parse::ParseSess,\n     pub cfg: ast::CrateConfig,\n@@ -535,6 +535,9 @@ impl<'a> ExtCtxt<'a> {\n     pub fn ident_of(&self, st: &str) -> ast::Ident {\n         str_to_ident(st)\n     }\n+    pub fn name_of(&self, st: &str) -> ast::Name {\n+        token::intern(st)\n+    }\n }\n \n /// Extract a string literal from the macro expanded version of `expr`,\n@@ -579,9 +582,9 @@ pub fn get_single_str_from_tts(cx: &ExtCtxt,\n         cx.span_err(sp, format!(\"{} takes 1 argument.\", name).as_slice());\n     } else {\n         match tts[0] {\n-            ast::TTTok(_, token::LIT_STR(ident))\n-            | ast::TTTok(_, token::LIT_STR_RAW(ident, _)) => {\n-                return Some(token::get_ident(ident).get().to_string())\n+            ast::TTTok(_, token::LIT_STR(ident)) => return Some(parse::str_lit(ident.as_str())),\n+            ast::TTTok(_, token::LIT_STR_RAW(ident, _)) => {\n+                return Some(parse::raw_str_lit(ident.as_str()))\n             }\n             _ => {\n                 cx.span_err(sp,\n@@ -612,11 +615,11 @@ pub fn get_exprs_from_tts(cx: &mut ExtCtxt,\n     Some(es)\n }\n \n-// in order to have some notion of scoping for macros,\n-// we want to implement the notion of a transformation\n-// environment.\n+/// In order to have some notion of scoping for macros,\n+/// we want to implement the notion of a transformation\n+/// environment.\n \n-// This environment maps Names to SyntaxExtensions.\n+/// This environment maps Names to SyntaxExtensions.\n \n //impl question: how to implement it? Initially, the\n // env will contain only macros, so it might be painful\n@@ -633,7 +636,6 @@ struct MapChainFrame {\n     map: HashMap<Name, SyntaxExtension>,\n }\n \n-// Only generic to make it easy to test\n pub struct SyntaxEnv {\n     chain: Vec<MapChainFrame> ,\n }"}, {"sha": "3b34407edfeaa364b0d98a667e6cfdb1aa593f42", "filename": "src/libsyntax/ext/deriving/encodable.rs", "status": "modified", "additions": 70, "deletions": 73, "changes": 143, "blob_url": "https://github.com/rust-lang/rust/blob/ad3eda123a00b3ddf8897295ff26d97ba7c64d44/src%2Flibsyntax%2Fext%2Fderiving%2Fencodable.rs", "raw_url": "https://github.com/rust-lang/rust/raw/ad3eda123a00b3ddf8897295ff26d97ba7c64d44/src%2Flibsyntax%2Fext%2Fderiving%2Fencodable.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fext%2Fderiving%2Fencodable.rs?ref=ad3eda123a00b3ddf8897295ff26d97ba7c64d44", "patch": "@@ -8,79 +8,76 @@\n // option. This file may not be copied, modified, or distributed\n // except according to those terms.\n \n-/*!\n-\n-The compiler code necessary to implement the `#[deriving(Encodable)]`\n-(and `Decodable`, in decodable.rs) extension.  The idea here is that\n-type-defining items may be tagged with `#[deriving(Encodable, Decodable)]`.\n-\n-For example, a type like:\n-\n-```ignore\n-#[deriving(Encodable, Decodable)]\n-struct Node { id: uint }\n-```\n-\n-would generate two implementations like:\n-\n-```ignore\n-impl<S:serialize::Encoder> Encodable<S> for Node {\n-    fn encode(&self, s: &S) {\n-        s.emit_struct(\"Node\", 1, || {\n-            s.emit_field(\"id\", 0, || s.emit_uint(self.id))\n-        })\n-    }\n-}\n-\n-impl<D:Decoder> Decodable for node_id {\n-    fn decode(d: &D) -> Node {\n-        d.read_struct(\"Node\", 1, || {\n-            Node {\n-                id: d.read_field(\"x\".to_string(), 0, || decode(d))\n-            }\n-        })\n-    }\n-}\n-```\n-\n-Other interesting scenarios are whe the item has type parameters or\n-references other non-built-in types.  A type definition like:\n-\n-```ignore\n-#[deriving(Encodable, Decodable)]\n-struct spanned<T> { node: T, span: Span }\n-```\n-\n-would yield functions like:\n-\n-```ignore\n-    impl<\n-        S: Encoder,\n-        T: Encodable<S>\n-    > spanned<T>: Encodable<S> {\n-        fn encode<S:Encoder>(s: &S) {\n-            s.emit_rec(|| {\n-                s.emit_field(\"node\", 0, || self.node.encode(s));\n-                s.emit_field(\"span\", 1, || self.span.encode(s));\n-            })\n-        }\n-    }\n-\n-    impl<\n-        D: Decoder,\n-        T: Decodable<D>\n-    > spanned<T>: Decodable<D> {\n-        fn decode(d: &D) -> spanned<T> {\n-            d.read_rec(|| {\n-                {\n-                    node: d.read_field(\"node\".to_string(), 0, || decode(d)),\n-                    span: d.read_field(\"span\".to_string(), 1, || decode(d)),\n-                }\n-            })\n-        }\n-    }\n-```\n-*/\n+//! The compiler code necessary to implement the `#[deriving(Encodable)]`\n+//! (and `Decodable`, in decodable.rs) extension.  The idea here is that\n+//! type-defining items may be tagged with `#[deriving(Encodable, Decodable)]`.\n+//!\n+//! For example, a type like:\n+//!\n+//! ```ignore\n+//! #[deriving(Encodable, Decodable)]\n+//! struct Node { id: uint }\n+//! ```\n+//!\n+//! would generate two implementations like:\n+//!\n+//! ```ignore\n+//! impl<S:serialize::Encoder> Encodable<S> for Node {\n+//!     fn encode(&self, s: &S) {\n+//!         s.emit_struct(\"Node\", 1, || {\n+//!             s.emit_field(\"id\", 0, || s.emit_uint(self.id))\n+//!         })\n+//!     }\n+//! }\n+//!\n+//! impl<D:Decoder> Decodable for node_id {\n+//!     fn decode(d: &D) -> Node {\n+//!         d.read_struct(\"Node\", 1, || {\n+//!             Node {\n+//!                 id: d.read_field(\"x\".to_string(), 0, || decode(d))\n+//!             }\n+//!         })\n+//!     }\n+//! }\n+//! ```\n+//!\n+//! Other interesting scenarios are whe the item has type parameters or\n+//! references other non-built-in types.  A type definition like:\n+//!\n+//! ```ignore\n+//! #[deriving(Encodable, Decodable)]\n+//! struct spanned<T> { node: T, span: Span }\n+//! ```\n+//!\n+//! would yield functions like:\n+//!\n+//! ```ignore\n+//!     impl<\n+//!         S: Encoder,\n+//!         T: Encodable<S>\n+//!     > spanned<T>: Encodable<S> {\n+//!         fn encode<S:Encoder>(s: &S) {\n+//!             s.emit_rec(|| {\n+//!                 s.emit_field(\"node\", 0, || self.node.encode(s));\n+//!                 s.emit_field(\"span\", 1, || self.span.encode(s));\n+//!             })\n+//!         }\n+//!     }\n+//!\n+//!     impl<\n+//!         D: Decoder,\n+//!         T: Decodable<D>\n+//!     > spanned<T>: Decodable<D> {\n+//!         fn decode(d: &D) -> spanned<T> {\n+//!             d.read_rec(|| {\n+//!                 {\n+//!                     node: d.read_field(\"node\".to_string(), 0, || decode(d)),\n+//!                     span: d.read_field(\"span\".to_string(), 1, || decode(d)),\n+//!                 }\n+//!             })\n+//!         }\n+//!     }\n+//! ```\n \n use ast::{MetaItem, Item, Expr, ExprRet, MutMutable, LitNil};\n use codemap::Span;"}, {"sha": "c9f5936a9bb0532cc94deabae2a2a46380c75e5c", "filename": "src/libsyntax/ext/deriving/generic/mod.rs", "status": "modified", "additions": 164, "deletions": 168, "changes": 332, "blob_url": "https://github.com/rust-lang/rust/blob/ad3eda123a00b3ddf8897295ff26d97ba7c64d44/src%2Flibsyntax%2Fext%2Fderiving%2Fgeneric%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/ad3eda123a00b3ddf8897295ff26d97ba7c64d44/src%2Flibsyntax%2Fext%2Fderiving%2Fgeneric%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fext%2Fderiving%2Fgeneric%2Fmod.rs?ref=ad3eda123a00b3ddf8897295ff26d97ba7c64d44", "patch": "@@ -8,174 +8,170 @@\n // option. This file may not be copied, modified, or distributed\n // except according to those terms.\n \n-/*!\n-\n-Some code that abstracts away much of the boilerplate of writing\n-`deriving` instances for traits. Among other things it manages getting\n-access to the fields of the 4 different sorts of structs and enum\n-variants, as well as creating the method and impl ast instances.\n-\n-Supported features (fairly exhaustive):\n-\n-- Methods taking any number of parameters of any type, and returning\n-  any type, other than vectors, bottom and closures.\n-- Generating `impl`s for types with type parameters and lifetimes\n-  (e.g. `Option<T>`), the parameters are automatically given the\n-  current trait as a bound. (This includes separate type parameters\n-  and lifetimes for methods.)\n-- Additional bounds on the type parameters, e.g. the `Ord` instance\n-  requires an explicit `PartialEq` bound at the\n-  moment. (`TraitDef.additional_bounds`)\n-\n-Unsupported: FIXME #6257: calling methods on reference fields,\n-e.g. deriving Eq/Ord/Clone don't work on `struct A(&int)`,\n-because of how the auto-dereferencing happens.\n-\n-The most important thing for implementers is the `Substructure` and\n-`SubstructureFields` objects. The latter groups 5 possibilities of the\n-arguments:\n-\n-- `Struct`, when `Self` is a struct (including tuple structs, e.g\n-  `struct T(int, char)`).\n-- `EnumMatching`, when `Self` is an enum and all the arguments are the\n-  same variant of the enum (e.g. `Some(1)`, `Some(3)` and `Some(4)`)\n-- `EnumNonMatching` when `Self` is an enum and the arguments are not\n-  the same variant (e.g. `None`, `Some(1)` and `None`). If\n-  `const_nonmatching` is true, this will contain an empty list.\n-- `StaticEnum` and `StaticStruct` for static methods, where the type\n-  being derived upon is either an enum or struct respectively. (Any\n-  argument with type Self is just grouped among the non-self\n-  arguments.)\n-\n-In the first two cases, the values from the corresponding fields in\n-all the arguments are grouped together. In the `EnumNonMatching` case\n-this isn't possible (different variants have different fields), so the\n-fields are grouped by which argument they come from. There are no\n-fields with values in the static cases, so these are treated entirely\n-differently.\n-\n-The non-static cases have `Option<ident>` in several places associated\n-with field `expr`s. This represents the name of the field it is\n-associated with. It is only not `None` when the associated field has\n-an identifier in the source code. For example, the `x`s in the\n-following snippet\n-\n-```rust\n-struct A { x : int }\n-\n-struct B(int);\n-\n-enum C {\n-    C0(int),\n-    C1 { x: int }\n-}\n-```\n-\n-The `int`s in `B` and `C0` don't have an identifier, so the\n-`Option<ident>`s would be `None` for them.\n-\n-In the static cases, the structure is summarised, either into the just\n-spans of the fields or a list of spans and the field idents (for tuple\n-structs and record structs, respectively), or a list of these, for\n-enums (one for each variant). For empty struct and empty enum\n-variants, it is represented as a count of 0.\n-\n-# Examples\n-\n-The following simplified `PartialEq` is used for in-code examples:\n-\n-```rust\n-trait PartialEq {\n-    fn eq(&self, other: &Self);\n-}\n-impl PartialEq for int {\n-    fn eq(&self, other: &int) -> bool {\n-        *self == *other\n-    }\n-}\n-```\n-\n-Some examples of the values of `SubstructureFields` follow, using the\n-above `PartialEq`, `A`, `B` and `C`.\n-\n-## Structs\n-\n-When generating the `expr` for the `A` impl, the `SubstructureFields` is\n-\n-~~~text\n-Struct(~[FieldInfo {\n-           span: <span of x>\n-           name: Some(<ident of x>),\n-           self_: <expr for &self.x>,\n-           other: ~[<expr for &other.x]\n-         }])\n-~~~\n-\n-For the `B` impl, called with `B(a)` and `B(b)`,\n-\n-~~~text\n-Struct(~[FieldInfo {\n-          span: <span of `int`>,\n-          name: None,\n-          <expr for &a>\n-          ~[<expr for &b>]\n-         }])\n-~~~\n-\n-## Enums\n-\n-When generating the `expr` for a call with `self == C0(a)` and `other\n-== C0(b)`, the SubstructureFields is\n-\n-~~~text\n-EnumMatching(0, <ast::Variant for C0>,\n-             ~[FieldInfo {\n-                span: <span of int>\n-                name: None,\n-                self_: <expr for &a>,\n-                other: ~[<expr for &b>]\n-              }])\n-~~~\n-\n-For `C1 {x}` and `C1 {x}`,\n-\n-~~~text\n-EnumMatching(1, <ast::Variant for C1>,\n-             ~[FieldInfo {\n-                span: <span of x>\n-                name: Some(<ident of x>),\n-                self_: <expr for &self.x>,\n-                other: ~[<expr for &other.x>]\n-               }])\n-~~~\n-\n-For `C0(a)` and `C1 {x}` ,\n-\n-~~~text\n-EnumNonMatching(~[(0, <ast::Variant for B0>,\n-                   ~[(<span of int>, None, <expr for &a>)]),\n-                  (1, <ast::Variant for B1>,\n-                   ~[(<span of x>, Some(<ident of x>),\n-                      <expr for &other.x>)])])\n-~~~\n-\n-(and vice versa, but with the order of the outermost list flipped.)\n-\n-## Static\n-\n-A static method on the above would result in,\n-\n-~~~text\n-StaticStruct(<ast::StructDef of A>, Named(~[(<ident of x>, <span of x>)]))\n-\n-StaticStruct(<ast::StructDef of B>, Unnamed(~[<span of x>]))\n-\n-StaticEnum(<ast::EnumDef of C>, ~[(<ident of C0>, <span of C0>, Unnamed(~[<span of int>])),\n-                                  (<ident of C1>, <span of C1>,\n-                                   Named(~[(<ident of x>, <span of x>)]))])\n-~~~\n-\n-*/\n+//! Some code that abstracts away much of the boilerplate of writing\n+//! `deriving` instances for traits. Among other things it manages getting\n+//! access to the fields of the 4 different sorts of structs and enum\n+//! variants, as well as creating the method and impl ast instances.\n+//!\n+//! Supported features (fairly exhaustive):\n+//!\n+//! - Methods taking any number of parameters of any type, and returning\n+//!   any type, other than vectors, bottom and closures.\n+//! - Generating `impl`s for types with type parameters and lifetimes\n+//!   (e.g. `Option<T>`), the parameters are automatically given the\n+//!   current trait as a bound. (This includes separate type parameters\n+//!   and lifetimes for methods.)\n+//! - Additional bounds on the type parameters, e.g. the `Ord` instance\n+//!   requires an explicit `PartialEq` bound at the\n+//!   moment. (`TraitDef.additional_bounds`)\n+//!\n+//! Unsupported: FIXME #6257: calling methods on reference fields,\n+//! e.g. deriving Eq/Ord/Clone don't work on `struct A(&int)`,\n+//! because of how the auto-dereferencing happens.\n+//!\n+//! The most important thing for implementers is the `Substructure` and\n+//! `SubstructureFields` objects. The latter groups 5 possibilities of the\n+//! arguments:\n+//!\n+//! - `Struct`, when `Self` is a struct (including tuple structs, e.g\n+//!   `struct T(int, char)`).\n+//! - `EnumMatching`, when `Self` is an enum and all the arguments are the\n+//!   same variant of the enum (e.g. `Some(1)`, `Some(3)` and `Some(4)`)\n+//! - `EnumNonMatching` when `Self` is an enum and the arguments are not\n+//!   the same variant (e.g. `None`, `Some(1)` and `None`). If\n+//!   `const_nonmatching` is true, this will contain an empty list.\n+//! - `StaticEnum` and `StaticStruct` for static methods, where the type\n+//!   being derived upon is either an enum or struct respectively. (Any\n+//!   argument with type Self is just grouped among the non-self\n+//!   arguments.)\n+//!\n+//! In the first two cases, the values from the corresponding fields in\n+//! all the arguments are grouped together. In the `EnumNonMatching` case\n+//! this isn't possible (different variants have different fields), so the\n+//! fields are grouped by which argument they come from. There are no\n+//! fields with values in the static cases, so these are treated entirely\n+//! differently.\n+//!\n+//! The non-static cases have `Option<ident>` in several places associated\n+//! with field `expr`s. This represents the name of the field it is\n+//! associated with. It is only not `None` when the associated field has\n+//! an identifier in the source code. For example, the `x`s in the\n+//! following snippet\n+//!\n+//! ```rust\n+//! struct A { x : int }\n+//!\n+//! struct B(int);\n+//!\n+//! enum C {\n+//!     C0(int),\n+//!     C1 { x: int }\n+//! }\n+//! ```\n+//!\n+//! The `int`s in `B` and `C0` don't have an identifier, so the\n+//! `Option<ident>`s would be `None` for them.\n+//!\n+//! In the static cases, the structure is summarised, either into the just\n+//! spans of the fields or a list of spans and the field idents (for tuple\n+//! structs and record structs, respectively), or a list of these, for\n+//! enums (one for each variant). For empty struct and empty enum\n+//! variants, it is represented as a count of 0.\n+//!\n+//! # Examples\n+//!\n+//! The following simplified `PartialEq` is used for in-code examples:\n+//!\n+//! ```rust\n+//! trait PartialEq {\n+//!     fn eq(&self, other: &Self);\n+//! }\n+//! impl PartialEq for int {\n+//!     fn eq(&self, other: &int) -> bool {\n+//!         *self == *other\n+//!     }\n+//! }\n+//! ```\n+//!\n+//! Some examples of the values of `SubstructureFields` follow, using the\n+//! above `PartialEq`, `A`, `B` and `C`.\n+//!\n+//! ## Structs\n+//!\n+//! When generating the `expr` for the `A` impl, the `SubstructureFields` is\n+//!\n+//! ~~~text\n+//! Struct(~[FieldInfo {\n+//!            span: <span of x>\n+//!            name: Some(<ident of x>),\n+//!            self_: <expr for &self.x>,\n+//!            other: ~[<expr for &other.x]\n+//!          }])\n+//! ~~~\n+//!\n+//! For the `B` impl, called with `B(a)` and `B(b)`,\n+//!\n+//! ~~~text\n+//! Struct(~[FieldInfo {\n+//!           span: <span of `int`>,\n+//!           name: None,\n+//!           <expr for &a>\n+//!           ~[<expr for &b>]\n+//!          }])\n+//! ~~~\n+//!\n+//! ## Enums\n+//!\n+//! When generating the `expr` for a call with `self == C0(a)` and `other\n+//! == C0(b)`, the SubstructureFields is\n+//!\n+//! ~~~text\n+//! EnumMatching(0, <ast::Variant for C0>,\n+//!              ~[FieldInfo {\n+//!                 span: <span of int>\n+//!                 name: None,\n+//!                 self_: <expr for &a>,\n+//!                 other: ~[<expr for &b>]\n+//!               }])\n+//! ~~~\n+//!\n+//! For `C1 {x}` and `C1 {x}`,\n+//!\n+//! ~~~text\n+//! EnumMatching(1, <ast::Variant for C1>,\n+//!              ~[FieldInfo {\n+//!                 span: <span of x>\n+//!                 name: Some(<ident of x>),\n+//!                 self_: <expr for &self.x>,\n+//!                 other: ~[<expr for &other.x>]\n+//!                }])\n+//! ~~~\n+//!\n+//! For `C0(a)` and `C1 {x}` ,\n+//!\n+//! ~~~text\n+//! EnumNonMatching(~[(0, <ast::Variant for B0>,\n+//!                    ~[(<span of int>, None, <expr for &a>)]),\n+//!                   (1, <ast::Variant for B1>,\n+//!                    ~[(<span of x>, Some(<ident of x>),\n+//!                       <expr for &other.x>)])])\n+//! ~~~\n+//!\n+//! (and vice versa, but with the order of the outermost list flipped.)\n+//!\n+//! ## Static\n+//!\n+//! A static method on the above would result in,\n+//!\n+//! ~~~text\n+//! StaticStruct(<ast::StructDef of A>, Named(~[(<ident of x>, <span of x>)]))\n+//!\n+//! StaticStruct(<ast::StructDef of B>, Unnamed(~[<span of x>]))\n+//!\n+//! StaticEnum(<ast::EnumDef of C>, ~[(<ident of C0>, <span of C0>, Unnamed(~[<span of int>])),\n+//!                                   (<ident of C1>, <span of C1>,\n+//!                                    Named(~[(<ident of x>, <span of x>)]))])\n+//! ~~~\n \n use std::cell::RefCell;\n use std::gc::{Gc, GC};"}, {"sha": "f6a39d7b2e6c1b6dda6c57356f7de62592c1a98e", "filename": "src/libsyntax/ext/deriving/generic/ty.rs", "status": "modified", "additions": 8, "deletions": 6, "changes": 14, "blob_url": "https://github.com/rust-lang/rust/blob/ad3eda123a00b3ddf8897295ff26d97ba7c64d44/src%2Flibsyntax%2Fext%2Fderiving%2Fgeneric%2Fty.rs", "raw_url": "https://github.com/rust-lang/rust/raw/ad3eda123a00b3ddf8897295ff26d97ba7c64d44/src%2Flibsyntax%2Fext%2Fderiving%2Fgeneric%2Fty.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fext%2Fderiving%2Fgeneric%2Fty.rs?ref=ad3eda123a00b3ddf8897295ff26d97ba7c64d44", "patch": "@@ -25,8 +25,10 @@ use std::gc::Gc;\n \n /// The types of pointers\n pub enum PtrTy<'a> {\n-    Send, // ~\n-    Borrowed(Option<&'a str>, ast::Mutability), // &['lifetime] [mut]\n+    /// ~\n+    Send,\n+    /// &'lifetime mut\n+    Borrowed(Option<&'a str>, ast::Mutability),\n }\n \n /// A path, e.g. `::std::option::Option::<int>` (global). Has support\n@@ -83,12 +85,12 @@ impl<'a> Path<'a> {\n /// A type. Supports pointers (except for *), Self, and literals\n pub enum Ty<'a> {\n     Self,\n-    // &/Box/ Ty\n+    /// &/Box/ Ty\n     Ptr(Box<Ty<'a>>, PtrTy<'a>),\n-    // mod::mod::Type<[lifetime], [Params...]>, including a plain type\n-    // parameter, and things like `int`\n+    /// mod::mod::Type<[lifetime], [Params...]>, including a plain type\n+    /// parameter, and things like `int`\n     Literal(Path<'a>),\n-    // includes nil\n+    /// includes unit\n     Tuple(Vec<Ty<'a>> )\n }\n "}, {"sha": "05b5131d7e4d332d6843e089e39ae192233bb425", "filename": "src/libsyntax/ext/deriving/show.rs", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/rust-lang/rust/blob/ad3eda123a00b3ddf8897295ff26d97ba7c64d44/src%2Flibsyntax%2Fext%2Fderiving%2Fshow.rs", "raw_url": "https://github.com/rust-lang/rust/raw/ad3eda123a00b3ddf8897295ff26d97ba7c64d44/src%2Flibsyntax%2Fext%2Fderiving%2Fshow.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fext%2Fderiving%2Fshow.rs?ref=ad3eda123a00b3ddf8897295ff26d97ba7c64d44", "patch": "@@ -55,8 +55,8 @@ pub fn expand_deriving_show(cx: &mut ExtCtxt,\n     trait_def.expand(cx, mitem, item, push)\n }\n \n-// we construct a format string and then defer to std::fmt, since that\n-// knows what's up with formatting at so on.\n+/// We construct a format string and then defer to std::fmt, since that\n+/// knows what's up with formatting and so on.\n fn show_substructure(cx: &mut ExtCtxt, span: Span,\n                      substr: &Substructure) -> Gc<Expr> {\n     // build `<name>`, `<name>({}, {}, ...)` or `<name> { <field>: {},"}, {"sha": "b7d72ae4debc130342fe3d5dc3e2a8577b4a4d0d", "filename": "src/libsyntax/ext/expand.rs", "status": "modified", "additions": 10, "deletions": 10, "changes": 20, "blob_url": "https://github.com/rust-lang/rust/blob/ad3eda123a00b3ddf8897295ff26d97ba7c64d44/src%2Flibsyntax%2Fext%2Fexpand.rs", "raw_url": "https://github.com/rust-lang/rust/raw/ad3eda123a00b3ddf8897295ff26d97ba7c64d44/src%2Flibsyntax%2Fext%2Fexpand.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fext%2Fexpand.rs?ref=ad3eda123a00b3ddf8897295ff26d97ba7c64d44", "patch": "@@ -246,11 +246,11 @@ pub fn expand_expr(e: Gc<ast::Expr>, fld: &mut MacroExpander) -> Gc<ast::Expr> {\n     }\n }\n \n-// Rename loop label and expand its loop body\n-//\n-// The renaming procedure for loop is different in the sense that the loop\n-// body is in a block enclosed by loop head so the renaming of loop label\n-// must be propagated to the enclosed context.\n+/// Rename loop label and expand its loop body\n+///\n+/// The renaming procedure for loop is different in the sense that the loop\n+/// body is in a block enclosed by loop head so the renaming of loop label\n+/// must be propagated to the enclosed context.\n fn expand_loop_block(loop_block: P<Block>,\n                      opt_ident: Option<Ident>,\n                      fld: &mut MacroExpander) -> (P<Block>, Option<Ident>) {\n@@ -1150,7 +1150,7 @@ mod test {\n     use super::{pattern_bindings, expand_crate, contains_macro_escape};\n     use super::{PatIdentFinder, IdentRenamer, PatIdentRenamer};\n     use ast;\n-    use ast::{Attribute_, AttrOuter, MetaWord};\n+    use ast::{Attribute_, AttrOuter, MetaWord, Name};\n     use attr;\n     use codemap;\n     use codemap::Spanned;\n@@ -1665,12 +1665,12 @@ foo_module!()\n         let f_ident = token::str_to_ident(\"f\");\n         let x_ident = token::str_to_ident(\"x\");\n         let int_ident = token::str_to_ident(\"int\");\n-        let renames = vec!((x_ident,16));\n+        let renames = vec!((x_ident,Name(16)));\n         let mut renamer = IdentRenamer{renames: &renames};\n         let renamed_crate = renamer.fold_crate(the_crate);\n         let idents = crate_idents(&renamed_crate);\n         let resolved : Vec<ast::Name> = idents.iter().map(|id| mtwt::resolve(*id)).collect();\n-        assert_eq!(resolved,vec!(f_ident.name,16,int_ident.name,16,16,16));\n+        assert_eq!(resolved,vec!(f_ident.name,Name(16),int_ident.name,Name(16),Name(16),Name(16)));\n     }\n \n     // test the PatIdentRenamer; only PatIdents get renamed\n@@ -1680,13 +1680,13 @@ foo_module!()\n         let f_ident = token::str_to_ident(\"f\");\n         let x_ident = token::str_to_ident(\"x\");\n         let int_ident = token::str_to_ident(\"int\");\n-        let renames = vec!((x_ident,16));\n+        let renames = vec!((x_ident,Name(16)));\n         let mut renamer = PatIdentRenamer{renames: &renames};\n         let renamed_crate = renamer.fold_crate(the_crate);\n         let idents = crate_idents(&renamed_crate);\n         let resolved : Vec<ast::Name> = idents.iter().map(|id| mtwt::resolve(*id)).collect();\n         let x_name = x_ident.name;\n-        assert_eq!(resolved,vec!(f_ident.name,16,int_ident.name,16,x_name,x_name));\n+        assert_eq!(resolved,vec!(f_ident.name,Name(16),int_ident.name,Name(16),x_name,x_name));\n     }\n \n "}, {"sha": "786fd953f8901ca9ccfe5d7799f73882fe9bda81", "filename": "src/libsyntax/ext/format.rs", "status": "modified", "additions": 8, "deletions": 8, "changes": 16, "blob_url": "https://github.com/rust-lang/rust/blob/ad3eda123a00b3ddf8897295ff26d97ba7c64d44/src%2Flibsyntax%2Fext%2Fformat.rs", "raw_url": "https://github.com/rust-lang/rust/raw/ad3eda123a00b3ddf8897295ff26d97ba7c64d44/src%2Flibsyntax%2Fext%2Fformat.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fext%2Fformat.rs?ref=ad3eda123a00b3ddf8897295ff26d97ba7c64d44", "patch": "@@ -37,24 +37,24 @@ struct Context<'a, 'b> {\n     ecx: &'a mut ExtCtxt<'b>,\n     fmtsp: Span,\n \n-    // Parsed argument expressions and the types that we've found so far for\n-    // them.\n+    /// Parsed argument expressions and the types that we've found so far for\n+    /// them.\n     args: Vec<Gc<ast::Expr>>,\n     arg_types: Vec<Option<ArgumentType>>,\n-    // Parsed named expressions and the types that we've found for them so far.\n-    // Note that we keep a side-array of the ordering of the named arguments\n-    // found to be sure that we can translate them in the same order that they\n-    // were declared in.\n+    /// Parsed named expressions and the types that we've found for them so far.\n+    /// Note that we keep a side-array of the ordering of the named arguments\n+    /// found to be sure that we can translate them in the same order that they\n+    /// were declared in.\n     names: HashMap<String, Gc<ast::Expr>>,\n     name_types: HashMap<String, ArgumentType>,\n     name_ordering: Vec<String>,\n \n-    // Collection of the compiled `rt::Piece` structures\n+    /// Collection of the compiled `rt::Piece` structures\n     pieces: Vec<Gc<ast::Expr>>,\n     name_positions: HashMap<String, uint>,\n     method_statics: Vec<Gc<ast::Item>>,\n \n-    // Updated as arguments are consumed or methods are entered\n+    /// Updated as arguments are consumed or methods are entered\n     nest_level: uint,\n     next_arg: uint,\n }"}, {"sha": "2c94db5296750523c48277670ec6a44d6ff5f77c", "filename": "src/libsyntax/ext/mtwt.rs", "status": "modified", "additions": 62, "deletions": 62, "changes": 124, "blob_url": "https://github.com/rust-lang/rust/blob/ad3eda123a00b3ddf8897295ff26d97ba7c64d44/src%2Flibsyntax%2Fext%2Fmtwt.rs", "raw_url": "https://github.com/rust-lang/rust/raw/ad3eda123a00b3ddf8897295ff26d97ba7c64d44/src%2Flibsyntax%2Fext%2Fmtwt.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fext%2Fmtwt.rs?ref=ad3eda123a00b3ddf8897295ff26d97ba7c64d44", "patch": "@@ -21,16 +21,16 @@ use std::cell::RefCell;\n use std::rc::Rc;\n use std::collections::HashMap;\n \n-// the SCTable contains a table of SyntaxContext_'s. It\n-// represents a flattened tree structure, to avoid having\n-// managed pointers everywhere (that caused an ICE).\n-// the mark_memo and rename_memo fields are side-tables\n-// that ensure that adding the same mark to the same context\n-// gives you back the same context as before. This shouldn't\n-// change the semantics--everything here is immutable--but\n-// it should cut down on memory use *a lot*; applying a mark\n-// to a tree containing 50 identifiers would otherwise generate\n-// 50 new contexts\n+/// The SCTable contains a table of SyntaxContext_'s. It\n+/// represents a flattened tree structure, to avoid having\n+/// managed pointers everywhere (that caused an ICE).\n+/// the mark_memo and rename_memo fields are side-tables\n+/// that ensure that adding the same mark to the same context\n+/// gives you back the same context as before. This shouldn't\n+/// change the semantics--everything here is immutable--but\n+/// it should cut down on memory use *a lot*; applying a mark\n+/// to a tree containing 50 identifiers would otherwise generate\n+/// 50 new contexts\n pub struct SCTable {\n     table: RefCell<Vec<SyntaxContext_>>,\n     mark_memo: RefCell<HashMap<(SyntaxContext,Mrk),SyntaxContext>>,\n@@ -41,16 +41,16 @@ pub struct SCTable {\n pub enum SyntaxContext_ {\n     EmptyCtxt,\n     Mark (Mrk,SyntaxContext),\n-    // flattening the name and syntaxcontext into the rename...\n-    // HIDDEN INVARIANTS:\n-    // 1) the first name in a Rename node\n-    // can only be a programmer-supplied name.\n-    // 2) Every Rename node with a given Name in the\n-    // \"to\" slot must have the same name and context\n-    // in the \"from\" slot. In essence, they're all\n-    // pointers to a single \"rename\" event node.\n+    /// flattening the name and syntaxcontext into the rename...\n+    /// HIDDEN INVARIANTS:\n+    /// 1) the first name in a Rename node\n+    /// can only be a programmer-supplied name.\n+    /// 2) Every Rename node with a given Name in the\n+    /// \"to\" slot must have the same name and context\n+    /// in the \"from\" slot. In essence, they're all\n+    /// pointers to a single \"rename\" event node.\n     Rename (Ident,Name,SyntaxContext),\n-    // actually, IllegalCtxt may not be necessary.\n+    /// actually, IllegalCtxt may not be necessary.\n     IllegalCtxt\n }\n \n@@ -62,7 +62,7 @@ pub fn apply_mark(m: Mrk, ctxt: SyntaxContext) -> SyntaxContext {\n     with_sctable(|table| apply_mark_internal(m, ctxt, table))\n }\n \n-// Extend a syntax context with a given mark and sctable (explicit memoization)\n+/// Extend a syntax context with a given mark and sctable (explicit memoization)\n fn apply_mark_internal(m: Mrk, ctxt: SyntaxContext, table: &SCTable) -> SyntaxContext {\n     let key = (ctxt, m);\n     let new_ctxt = |_: &(SyntaxContext, Mrk)|\n@@ -77,13 +77,13 @@ pub fn apply_rename(id: Ident, to:Name,\n     with_sctable(|table| apply_rename_internal(id, to, ctxt, table))\n }\n \n-// Extend a syntax context with a given rename and sctable (explicit memoization)\n+/// Extend a syntax context with a given rename and sctable (explicit memoization)\n fn apply_rename_internal(id: Ident,\n                        to: Name,\n                        ctxt: SyntaxContext,\n                        table: &SCTable) -> SyntaxContext {\n-    let key = (ctxt,id,to);\n-    let new_ctxt = |_: &(SyntaxContext, Ident, Mrk)|\n+    let key = (ctxt, id, to);\n+    let new_ctxt = |_: &(SyntaxContext, Ident, Name)|\n                    idx_push(&mut *table.table.borrow_mut(), Rename(id, to, ctxt));\n \n     *table.rename_memo.borrow_mut().find_or_insert_with(key, new_ctxt)\n@@ -141,8 +141,8 @@ pub fn clear_tables() {\n     with_resolve_table_mut(|table| *table = HashMap::new());\n }\n \n-// Add a value to the end of a vec, return its index\n-fn idx_push<T>(vec: &mut Vec<T> , val: T) -> u32 {\n+/// Add a value to the end of a vec, return its index\n+fn idx_push<T>(vec: &mut Vec<T>, val: T) -> u32 {\n     vec.push(val);\n     (vec.len() - 1) as u32\n }\n@@ -173,8 +173,8 @@ fn with_resolve_table_mut<T>(op: |&mut ResolveTable| -> T) -> T {\n     }\n }\n \n-// Resolve a syntax object to a name, per MTWT.\n-// adding memoization to resolve 500+ seconds in resolve for librustc (!)\n+/// Resolve a syntax object to a name, per MTWT.\n+/// adding memoization to resolve 500+ seconds in resolve for librustc (!)\n fn resolve_internal(id: Ident,\n                     table: &SCTable,\n                     resolve_table: &mut ResolveTable) -> Name {\n@@ -264,8 +264,8 @@ pub fn outer_mark(ctxt: SyntaxContext) -> Mrk {\n     })\n }\n \n-// Push a name... unless it matches the one on top, in which\n-// case pop and discard (so two of the same marks cancel)\n+/// Push a name... unless it matches the one on top, in which\n+/// case pop and discard (so two of the same marks cancel)\n fn xor_push(marks: &mut Vec<Mrk>, mark: Mrk) {\n     if (marks.len() > 0) && (*marks.last().unwrap() == mark) {\n         marks.pop().unwrap();\n@@ -301,8 +301,8 @@ mod tests {\n         assert_eq!(s.clone(), vec!(14));\n     }\n \n-    fn id(n: Name, s: SyntaxContext) -> Ident {\n-        Ident {name: n, ctxt: s}\n+    fn id(n: u32, s: SyntaxContext) -> Ident {\n+        Ident {name: Name(n), ctxt: s}\n     }\n \n     // because of the SCTable, I now need a tidy way of\n@@ -349,12 +349,12 @@ mod tests {\n     fn test_unfold_refold(){\n         let mut t = new_sctable_internal();\n \n-        let test_sc = vec!(M(3),R(id(101,0),14),M(9));\n+        let test_sc = vec!(M(3),R(id(101,0),Name(14)),M(9));\n         assert_eq!(unfold_test_sc(test_sc.clone(),EMPTY_CTXT,&mut t),4);\n         {\n             let table = t.table.borrow();\n             assert!(*table.get(2) == Mark(9,0));\n-            assert!(*table.get(3) == Rename(id(101,0),14,2));\n+            assert!(*table.get(3) == Rename(id(101,0),Name(14),2));\n             assert!(*table.get(4) == Mark(3,3));\n         }\n         assert_eq!(refold_test_sc(4,&t),test_sc);\n@@ -381,8 +381,8 @@ mod tests {\n \n     #[test]\n     fn test_marksof () {\n-        let stopname = 242;\n-        let name1 = 243;\n+        let stopname = Name(242);\n+        let name1 = Name(243);\n         let mut t = new_sctable_internal();\n         assert_eq!(marksof_internal (EMPTY_CTXT,stopname,&t),Vec::new());\n         // FIXME #5074: ANF'd to dodge nested calls\n@@ -396,16 +396,16 @@ mod tests {\n          assert_eq! (marksof_internal (ans, stopname,&t), vec!(16));}\n         // rename where stop doesn't match:\n         { let chain = vec!(M(9),\n-                        R(id(name1,\n+                        R(id(name1.uint() as u32,\n                              apply_mark_internal (4, EMPTY_CTXT,&mut t)),\n-                          100101102),\n+                          Name(100101102)),\n                         M(14));\n          let ans = unfold_test_sc(chain,EMPTY_CTXT,&mut t);\n          assert_eq! (marksof_internal (ans, stopname, &t), vec!(9,14));}\n         // rename where stop does match\n         { let name1sc = apply_mark_internal(4, EMPTY_CTXT, &mut t);\n          let chain = vec!(M(9),\n-                       R(id(name1, name1sc),\n+                       R(id(name1.uint() as u32, name1sc),\n                          stopname),\n                        M(14));\n          let ans = unfold_test_sc(chain,EMPTY_CTXT,&mut t);\n@@ -419,55 +419,55 @@ mod tests {\n         let mut t = new_sctable_internal();\n         let mut rt = HashMap::new();\n         // - ctxt is MT\n-        assert_eq!(resolve_internal(id(a,EMPTY_CTXT),&mut t, &mut rt),a);\n+        assert_eq!(resolve_internal(id(a,EMPTY_CTXT),&mut t, &mut rt),Name(a));\n         // - simple ignored marks\n         { let sc = unfold_marks(vec!(1,2,3),EMPTY_CTXT,&mut t);\n-         assert_eq!(resolve_internal(id(a,sc),&mut t, &mut rt),a);}\n+         assert_eq!(resolve_internal(id(a,sc),&mut t, &mut rt),Name(a));}\n         // - orthogonal rename where names don't match\n-        { let sc = unfold_test_sc(vec!(R(id(50,EMPTY_CTXT),51),M(12)),EMPTY_CTXT,&mut t);\n-         assert_eq!(resolve_internal(id(a,sc),&mut t, &mut rt),a);}\n+        { let sc = unfold_test_sc(vec!(R(id(50,EMPTY_CTXT),Name(51)),M(12)),EMPTY_CTXT,&mut t);\n+         assert_eq!(resolve_internal(id(a,sc),&mut t, &mut rt),Name(a));}\n         // - rename where names do match, but marks don't\n         { let sc1 = apply_mark_internal(1,EMPTY_CTXT,&mut t);\n-         let sc = unfold_test_sc(vec!(R(id(a,sc1),50),\n+         let sc = unfold_test_sc(vec!(R(id(a,sc1),Name(50)),\n                                    M(1),\n                                    M(2)),\n                                  EMPTY_CTXT,&mut t);\n-        assert_eq!(resolve_internal(id(a,sc),&mut t, &mut rt), a);}\n+        assert_eq!(resolve_internal(id(a,sc),&mut t, &mut rt), Name(a));}\n         // - rename where names and marks match\n         { let sc1 = unfold_test_sc(vec!(M(1),M(2)),EMPTY_CTXT,&mut t);\n-         let sc = unfold_test_sc(vec!(R(id(a,sc1),50),M(1),M(2)),EMPTY_CTXT,&mut t);\n-         assert_eq!(resolve_internal(id(a,sc),&mut t, &mut rt), 50); }\n+         let sc = unfold_test_sc(vec!(R(id(a,sc1),Name(50)),M(1),M(2)),EMPTY_CTXT,&mut t);\n+         assert_eq!(resolve_internal(id(a,sc),&mut t, &mut rt), Name(50)); }\n         // - rename where names and marks match by literal sharing\n         { let sc1 = unfold_test_sc(vec!(M(1),M(2)),EMPTY_CTXT,&mut t);\n-         let sc = unfold_test_sc(vec!(R(id(a,sc1),50)),sc1,&mut t);\n-         assert_eq!(resolve_internal(id(a,sc),&mut t, &mut rt), 50); }\n+         let sc = unfold_test_sc(vec!(R(id(a,sc1),Name(50))),sc1,&mut t);\n+         assert_eq!(resolve_internal(id(a,sc),&mut t, &mut rt), Name(50)); }\n         // - two renames of the same var.. can only happen if you use\n         // local-expand to prevent the inner binding from being renamed\n         // during the rename-pass caused by the first:\n         println!(\"about to run bad test\");\n-        { let sc = unfold_test_sc(vec!(R(id(a,EMPTY_CTXT),50),\n-                                    R(id(a,EMPTY_CTXT),51)),\n+        { let sc = unfold_test_sc(vec!(R(id(a,EMPTY_CTXT),Name(50)),\n+                                    R(id(a,EMPTY_CTXT),Name(51))),\n                                   EMPTY_CTXT,&mut t);\n-         assert_eq!(resolve_internal(id(a,sc),&mut t, &mut rt), 51); }\n+         assert_eq!(resolve_internal(id(a,sc),&mut t, &mut rt), Name(51)); }\n         // the simplest double-rename:\n-        { let a_to_a50 = apply_rename_internal(id(a,EMPTY_CTXT),50,EMPTY_CTXT,&mut t);\n-         let a50_to_a51 = apply_rename_internal(id(a,a_to_a50),51,a_to_a50,&mut t);\n-         assert_eq!(resolve_internal(id(a,a50_to_a51),&mut t, &mut rt),51);\n+        { let a_to_a50 = apply_rename_internal(id(a,EMPTY_CTXT),Name(50),EMPTY_CTXT,&mut t);\n+         let a50_to_a51 = apply_rename_internal(id(a,a_to_a50),Name(51),a_to_a50,&mut t);\n+         assert_eq!(resolve_internal(id(a,a50_to_a51),&mut t, &mut rt),Name(51));\n          // mark on the outside doesn't stop rename:\n          let sc = apply_mark_internal(9,a50_to_a51,&mut t);\n-         assert_eq!(resolve_internal(id(a,sc),&mut t, &mut rt),51);\n+         assert_eq!(resolve_internal(id(a,sc),&mut t, &mut rt),Name(51));\n          // but mark on the inside does:\n-         let a50_to_a51_b = unfold_test_sc(vec!(R(id(a,a_to_a50),51),\n+         let a50_to_a51_b = unfold_test_sc(vec!(R(id(a,a_to_a50),Name(51)),\n                                               M(9)),\n                                            a_to_a50,\n                                            &mut t);\n-         assert_eq!(resolve_internal(id(a,a50_to_a51_b),&mut t, &mut rt),50);}\n+         assert_eq!(resolve_internal(id(a,a50_to_a51_b),&mut t, &mut rt),Name(50));}\n     }\n \n     #[test]\n     fn mtwt_resolve_test(){\n         let a = 40;\n-        assert_eq!(resolve(id(a,EMPTY_CTXT)),a);\n+        assert_eq!(resolve(id(a,EMPTY_CTXT)),Name(a));\n     }\n \n \n@@ -496,10 +496,10 @@ mod tests {\n \n     #[test]\n     fn new_resolves_test() {\n-        let renames = vec!((Ident{name:23,ctxt:EMPTY_CTXT},24),\n-                           (Ident{name:29,ctxt:EMPTY_CTXT},29));\n+        let renames = vec!((Ident{name:Name(23),ctxt:EMPTY_CTXT},Name(24)),\n+                           (Ident{name:Name(29),ctxt:EMPTY_CTXT},Name(29)));\n         let new_ctxt1 = apply_renames(&renames,EMPTY_CTXT);\n-        assert_eq!(resolve(Ident{name:23,ctxt:new_ctxt1}),24);\n-        assert_eq!(resolve(Ident{name:29,ctxt:new_ctxt1}),29);\n+        assert_eq!(resolve(Ident{name:Name(23),ctxt:new_ctxt1}),Name(24));\n+        assert_eq!(resolve(Ident{name:Name(29),ctxt:new_ctxt1}),Name(29));\n     }\n }"}, {"sha": "696d62838ba799150a68a998929e8277099a9e8e", "filename": "src/libsyntax/ext/quote.rs", "status": "modified", "additions": 20, "deletions": 42, "changes": 62, "blob_url": "https://github.com/rust-lang/rust/blob/ad3eda123a00b3ddf8897295ff26d97ba7c64d44/src%2Flibsyntax%2Fext%2Fquote.rs", "raw_url": "https://github.com/rust-lang/rust/raw/ad3eda123a00b3ddf8897295ff26d97ba7c64d44/src%2Flibsyntax%2Fext%2Fquote.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fext%2Fquote.rs?ref=ad3eda123a00b3ddf8897295ff26d97ba7c64d44", "patch": "@@ -363,6 +363,15 @@ fn mk_ident(cx: &ExtCtxt, sp: Span, ident: ast::Ident) -> Gc<ast::Expr> {\n                         vec!(e_str))\n }\n \n+// Lift a name to the expr that evaluates to that name\n+fn mk_name(cx: &ExtCtxt, sp: Span, ident: ast::Ident) -> Gc<ast::Expr> {\n+    let e_str = cx.expr_str(sp, token::get_ident(ident));\n+    cx.expr_method_call(sp,\n+                        cx.expr_ident(sp, id_ext(\"ext_cx\")),\n+                        id_ext(\"name_of\"),\n+                        vec!(e_str))\n+}\n+\n fn mk_ast_path(cx: &ExtCtxt, sp: Span, name: &str) -> Gc<ast::Expr> {\n     let idents = vec!(id_ext(\"syntax\"), id_ext(\"ast\"), id_ext(name));\n     cx.expr_path(cx.path_global(sp, idents))\n@@ -401,68 +410,37 @@ fn mk_token(cx: &ExtCtxt, sp: Span, tok: &token::Token) -> Gc<ast::Expr> {\n         }\n \n         LIT_BYTE(i) => {\n-            let e_byte = cx.expr_lit(sp, ast::LitByte(i));\n+            let e_byte = mk_name(cx, sp, i.ident());\n \n             return cx.expr_call(sp, mk_token_path(cx, sp, \"LIT_BYTE\"), vec!(e_byte));\n         }\n \n         LIT_CHAR(i) => {\n-            let e_char = cx.expr_lit(sp, ast::LitChar(i));\n+            let e_char = mk_name(cx, sp, i.ident());\n \n             return cx.expr_call(sp, mk_token_path(cx, sp, \"LIT_CHAR\"), vec!(e_char));\n         }\n \n-        LIT_INT(i, ity) => {\n-            let s_ity = match ity {\n-                ast::TyI => \"TyI\",\n-                ast::TyI8 => \"TyI8\",\n-                ast::TyI16 => \"TyI16\",\n-                ast::TyI32 => \"TyI32\",\n-                ast::TyI64 => \"TyI64\"\n-            };\n-            let e_ity = mk_ast_path(cx, sp, s_ity);\n-            let e_i64 = cx.expr_lit(sp, ast::LitInt(i, ast::TyI64));\n-            return cx.expr_call(sp, mk_token_path(cx, sp, \"LIT_INT\"), vec!(e_i64, e_ity));\n+        LIT_INTEGER(i) => {\n+            let e_int = mk_name(cx, sp, i.ident());\n+            return cx.expr_call(sp, mk_token_path(cx, sp, \"LIT_INTEGER\"), vec!(e_int));\n         }\n \n-        LIT_UINT(u, uty) => {\n-            let s_uty = match uty {\n-                ast::TyU => \"TyU\",\n-                ast::TyU8 => \"TyU8\",\n-                ast::TyU16 => \"TyU16\",\n-                ast::TyU32 => \"TyU32\",\n-                ast::TyU64 => \"TyU64\"\n-            };\n-            let e_uty = mk_ast_path(cx, sp, s_uty);\n-            let e_u64 = cx.expr_lit(sp, ast::LitUint(u, ast::TyU64));\n-            return cx.expr_call(sp, mk_token_path(cx, sp, \"LIT_UINT\"), vec!(e_u64, e_uty));\n-        }\n-\n-        LIT_INT_UNSUFFIXED(i) => {\n-            let e_i64 = cx.expr_lit(sp, ast::LitInt(i, ast::TyI64));\n-            return cx.expr_call(sp, mk_token_path(cx, sp, \"LIT_INT_UNSUFFIXED\"), vec!(e_i64));\n-        }\n-\n-        LIT_FLOAT(fident, fty) => {\n-            let s_fty = match fty {\n-                ast::TyF32 => \"TyF32\",\n-                ast::TyF64 => \"TyF64\",\n-            };\n-            let e_fty = mk_ast_path(cx, sp, s_fty);\n-            let e_fident = mk_ident(cx, sp, fident);\n-            return cx.expr_call(sp, mk_token_path(cx, sp, \"LIT_FLOAT\"), vec!(e_fident, e_fty));\n+        LIT_FLOAT(fident) => {\n+            let e_fident = mk_name(cx, sp, fident.ident());\n+            return cx.expr_call(sp, mk_token_path(cx, sp, \"LIT_FLOAT\"), vec!(e_fident));\n         }\n \n         LIT_STR(ident) => {\n             return cx.expr_call(sp,\n                                 mk_token_path(cx, sp, \"LIT_STR\"),\n-                                vec!(mk_ident(cx, sp, ident)));\n+                                vec!(mk_name(cx, sp, ident.ident())));\n         }\n \n         LIT_STR_RAW(ident, n) => {\n             return cx.expr_call(sp,\n                                 mk_token_path(cx, sp, \"LIT_STR_RAW\"),\n-                                vec!(mk_ident(cx, sp, ident), cx.expr_uint(sp, n)));\n+                                vec!(mk_name(cx, sp, ident.ident()), cx.expr_uint(sp, n)));\n         }\n \n         IDENT(ident, b) => {\n@@ -480,7 +458,7 @@ fn mk_token(cx: &ExtCtxt, sp: Span, tok: &token::Token) -> Gc<ast::Expr> {\n         DOC_COMMENT(ident) => {\n             return cx.expr_call(sp,\n                                 mk_token_path(cx, sp, \"DOC_COMMENT\"),\n-                                vec!(mk_ident(cx, sp, ident)));\n+                                vec!(mk_name(cx, sp, ident.ident())));\n         }\n \n         INTERPOLATED(_) => fail!(\"quote! with interpolated token\"),"}, {"sha": "5ac9dc86fcec2017e648743ff443165f581667bb", "filename": "src/libsyntax/ext/source_util.rs", "status": "modified", "additions": 7, "deletions": 7, "changes": 14, "blob_url": "https://github.com/rust-lang/rust/blob/ad3eda123a00b3ddf8897295ff26d97ba7c64d44/src%2Flibsyntax%2Fext%2Fsource_util.rs", "raw_url": "https://github.com/rust-lang/rust/raw/ad3eda123a00b3ddf8897295ff26d97ba7c64d44/src%2Flibsyntax%2Fext%2Fsource_util.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fext%2Fsource_util.rs?ref=ad3eda123a00b3ddf8897295ff26d97ba7c64d44", "patch": "@@ -28,7 +28,7 @@ use std::str;\n // the column/row/filename of the expression, or they include\n // a given file into the current one.\n \n-/* line!(): expands to the current line number */\n+/// line!(): expands to the current line number\n pub fn expand_line(cx: &mut ExtCtxt, sp: Span, tts: &[ast::TokenTree])\n                    -> Box<base::MacResult> {\n     base::check_zero_tts(cx, sp, tts, \"line!\");\n@@ -49,9 +49,9 @@ pub fn expand_col(cx: &mut ExtCtxt, sp: Span, tts: &[ast::TokenTree])\n     base::MacExpr::new(cx.expr_uint(topmost.call_site, loc.col.to_uint()))\n }\n \n-/* file!(): expands to the current filename */\n-/* The filemap (`loc.file`) contains a bunch more information we could spit\n- * out if we wanted. */\n+/// file!(): expands to the current filename */\n+/// The filemap (`loc.file`) contains a bunch more information we could spit\n+/// out if we wanted.\n pub fn expand_file(cx: &mut ExtCtxt, sp: Span, tts: &[ast::TokenTree])\n                    -> Box<base::MacResult> {\n     base::check_zero_tts(cx, sp, tts, \"file!\");\n@@ -82,9 +82,9 @@ pub fn expand_mod(cx: &mut ExtCtxt, sp: Span, tts: &[ast::TokenTree])\n             token::intern_and_get_ident(string.as_slice())))\n }\n \n-// include! : parse the given file as an expr\n-// This is generally a bad idea because it's going to behave\n-// unhygienically.\n+/// include! : parse the given file as an expr\n+/// This is generally a bad idea because it's going to behave\n+/// unhygienically.\n pub fn expand_include(cx: &mut ExtCtxt, sp: Span, tts: &[ast::TokenTree])\n                       -> Box<base::MacResult> {\n     let file = match get_single_str_from_tts(cx, sp, tts, \"include!\") {"}, {"sha": "bdf1f6eb6007e3bc2454069b26b905ad2a11b846", "filename": "src/libsyntax/ext/tt/macro_parser.rs", "status": "modified", "additions": 86, "deletions": 89, "changes": 175, "blob_url": "https://github.com/rust-lang/rust/blob/ad3eda123a00b3ddf8897295ff26d97ba7c64d44/src%2Flibsyntax%2Fext%2Ftt%2Fmacro_parser.rs", "raw_url": "https://github.com/rust-lang/rust/raw/ad3eda123a00b3ddf8897295ff26d97ba7c64d44/src%2Flibsyntax%2Fext%2Ftt%2Fmacro_parser.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fext%2Ftt%2Fmacro_parser.rs?ref=ad3eda123a00b3ddf8897295ff26d97ba7c64d44", "patch": "@@ -8,7 +8,72 @@\n // option. This file may not be copied, modified, or distributed\n // except according to those terms.\n \n-// Earley-like parser for macros.\n+//! This is an Earley-like parser, without support for in-grammar nonterminals,\n+//! only by calling out to the main rust parser for named nonterminals (which it\n+//! commits to fully when it hits one in a grammar). This means that there are no\n+//! completer or predictor rules, and therefore no need to store one column per\n+//! token: instead, there's a set of current Earley items and a set of next\n+//! ones. Instead of NTs, we have a special case for Kleene star. The big-O, in\n+//! pathological cases, is worse than traditional Earley parsing, but it's an\n+//! easier fit for Macro-by-Example-style rules, and I think the overhead is\n+//! lower. (In order to prevent the pathological case, we'd need to lazily\n+//! construct the resulting `NamedMatch`es at the very end. It'd be a pain,\n+//! and require more memory to keep around old items, but it would also save\n+//! overhead)\n+//!\n+//! Quick intro to how the parser works:\n+//!\n+//! A 'position' is a dot in the middle of a matcher, usually represented as a\n+//! dot. For example `\u00b7 a $( a )* a b` is a position, as is `a $( \u00b7 a )* a b`.\n+//!\n+//! The parser walks through the input a character at a time, maintaining a list\n+//! of items consistent with the current position in the input string: `cur_eis`.\n+//!\n+//! As it processes them, it fills up `eof_eis` with items that would be valid if\n+//! the macro invocation is now over, `bb_eis` with items that are waiting on\n+//! a Rust nonterminal like `$e:expr`, and `next_eis` with items that are waiting\n+//! on the a particular token. Most of the logic concerns moving the \u00b7 through the\n+//! repetitions indicated by Kleene stars. It only advances or calls out to the\n+//! real Rust parser when no `cur_eis` items remain\n+//!\n+//! Example: Start parsing `a a a a b` against [\u00b7 a $( a )* a b].\n+//!\n+//! Remaining input: `a a a a b`\n+//! next_eis: [\u00b7 a $( a )* a b]\n+//!\n+//! - - - Advance over an `a`. - - -\n+//!\n+//! Remaining input: `a a a b`\n+//! cur: [a \u00b7 $( a )* a b]\n+//! Descend/Skip (first item).\n+//! next: [a $( \u00b7 a )* a b]  [a $( a )* \u00b7 a b].\n+//!\n+//! - - - Advance over an `a`. - - -\n+//!\n+//! Remaining input: `a a b`\n+//! cur: [a $( a \u00b7 )* a b]  next: [a $( a )* a \u00b7 b]\n+//! Finish/Repeat (first item)\n+//! next: [a $( a )* \u00b7 a b]  [a $( \u00b7 a )* a b]  [a $( a )* a \u00b7 b]\n+//!\n+//! - - - Advance over an `a`. - - - (this looks exactly like the last step)\n+//!\n+//! Remaining input: `a b`\n+//! cur: [a $( a \u00b7 )* a b]  next: [a $( a )* a \u00b7 b]\n+//! Finish/Repeat (first item)\n+//! next: [a $( a )* \u00b7 a b]  [a $( \u00b7 a )* a b]  [a $( a )* a \u00b7 b]\n+//!\n+//! - - - Advance over an `a`. - - - (this looks exactly like the last step)\n+//!\n+//! Remaining input: `b`\n+//! cur: [a $( a \u00b7 )* a b]  next: [a $( a )* a \u00b7 b]\n+//! Finish/Repeat (first item)\n+//! next: [a $( a )* \u00b7 a b]  [a $( \u00b7 a )* a b]\n+//!\n+//! - - - Advance over a `b`. - - -\n+//!\n+//! Remaining input: ``\n+//! eof: [a $( a )* a b \u00b7]\n+\n \n use ast;\n use ast::{Matcher, MatchTok, MatchSeq, MatchNonterminal, Ident};\n@@ -25,75 +90,6 @@ use std::rc::Rc;\n use std::gc::GC;\n use std::collections::HashMap;\n \n-/* This is an Earley-like parser, without support for in-grammar nonterminals,\n-only by calling out to the main rust parser for named nonterminals (which it\n-commits to fully when it hits one in a grammar). This means that there are no\n-completer or predictor rules, and therefore no need to store one column per\n-token: instead, there's a set of current Earley items and a set of next\n-ones. Instead of NTs, we have a special case for Kleene star. The big-O, in\n-pathological cases, is worse than traditional Earley parsing, but it's an\n-easier fit for Macro-by-Example-style rules, and I think the overhead is\n-lower. (In order to prevent the pathological case, we'd need to lazily\n-construct the resulting `NamedMatch`es at the very end. It'd be a pain,\n-and require more memory to keep around old items, but it would also save\n-overhead)*/\n-\n-/* Quick intro to how the parser works:\n-\n-A 'position' is a dot in the middle of a matcher, usually represented as a\n-dot. For example `\u00b7 a $( a )* a b` is a position, as is `a $( \u00b7 a )* a b`.\n-\n-The parser walks through the input a character at a time, maintaining a list\n-of items consistent with the current position in the input string: `cur_eis`.\n-\n-As it processes them, it fills up `eof_eis` with items that would be valid if\n-the macro invocation is now over, `bb_eis` with items that are waiting on\n-a Rust nonterminal like `$e:expr`, and `next_eis` with items that are waiting\n-on the a particular token. Most of the logic concerns moving the \u00b7 through the\n-repetitions indicated by Kleene stars. It only advances or calls out to the\n-real Rust parser when no `cur_eis` items remain\n-\n-Example: Start parsing `a a a a b` against [\u00b7 a $( a )* a b].\n-\n-Remaining input: `a a a a b`\n-next_eis: [\u00b7 a $( a )* a b]\n-\n-- - - Advance over an `a`. - - -\n-\n-Remaining input: `a a a b`\n-cur: [a \u00b7 $( a )* a b]\n-Descend/Skip (first item).\n-next: [a $( \u00b7 a )* a b]  [a $( a )* \u00b7 a b].\n-\n-- - - Advance over an `a`. - - -\n-\n-Remaining input: `a a b`\n-cur: [a $( a \u00b7 )* a b]  next: [a $( a )* a \u00b7 b]\n-Finish/Repeat (first item)\n-next: [a $( a )* \u00b7 a b]  [a $( \u00b7 a )* a b]  [a $( a )* a \u00b7 b]\n-\n-- - - Advance over an `a`. - - - (this looks exactly like the last step)\n-\n-Remaining input: `a b`\n-cur: [a $( a \u00b7 )* a b]  next: [a $( a )* a \u00b7 b]\n-Finish/Repeat (first item)\n-next: [a $( a )* \u00b7 a b]  [a $( \u00b7 a )* a b]  [a $( a )* a \u00b7 b]\n-\n-- - - Advance over an `a`. - - - (this looks exactly like the last step)\n-\n-Remaining input: `b`\n-cur: [a $( a \u00b7 )* a b]  next: [a $( a )* a \u00b7 b]\n-Finish/Repeat (first item)\n-next: [a $( a )* \u00b7 a b]  [a $( \u00b7 a )* a b]\n-\n-- - - Advance over a `b`. - - -\n-\n-Remaining input: ``\n-eof: [a $( a )* a b \u00b7]\n-\n- */\n-\n-\n /* to avoid costly uniqueness checks, we require that `MatchSeq` always has a\n nonempty body. */\n \n@@ -147,24 +143,24 @@ pub fn initial_matcher_pos(ms: Vec<Matcher> , sep: Option<Token>, lo: BytePos)\n     }\n }\n \n-// NamedMatch is a pattern-match result for a single ast::MatchNonterminal:\n-// so it is associated with a single ident in a parse, and all\n-// MatchedNonterminal's in the NamedMatch have the same nonterminal type\n-// (expr, item, etc). All the leaves in a single NamedMatch correspond to a\n-// single matcher_nonterminal in the ast::Matcher that produced it.\n-//\n-// It should probably be renamed, it has more or less exact correspondence to\n-// ast::match nodes, and the in-memory structure of a particular NamedMatch\n-// represents the match that occurred when a particular subset of an\n-// ast::match -- those ast::Matcher nodes leading to a single\n-// MatchNonterminal -- was applied to a particular token tree.\n-//\n-// The width of each MatchedSeq in the NamedMatch, and the identity of the\n-// MatchedNonterminal's, will depend on the token tree it was applied to: each\n-// MatchedSeq corresponds to a single MatchSeq in the originating\n-// ast::Matcher. The depth of the NamedMatch structure will therefore depend\n-// only on the nesting depth of ast::MatchSeq's in the originating\n-// ast::Matcher it was derived from.\n+/// NamedMatch is a pattern-match result for a single ast::MatchNonterminal:\n+/// so it is associated with a single ident in a parse, and all\n+/// MatchedNonterminal's in the NamedMatch have the same nonterminal type\n+/// (expr, item, etc). All the leaves in a single NamedMatch correspond to a\n+/// single matcher_nonterminal in the ast::Matcher that produced it.\n+///\n+/// It should probably be renamed, it has more or less exact correspondence to\n+/// ast::match nodes, and the in-memory structure of a particular NamedMatch\n+/// represents the match that occurred when a particular subset of an\n+/// ast::match -- those ast::Matcher nodes leading to a single\n+/// MatchNonterminal -- was applied to a particular token tree.\n+///\n+/// The width of each MatchedSeq in the NamedMatch, and the identity of the\n+/// MatchedNonterminal's, will depend on the token tree it was applied to: each\n+/// MatchedSeq corresponds to a single MatchSeq in the originating\n+/// ast::Matcher. The depth of the NamedMatch structure will therefore depend\n+/// only on the nesting depth of ast::MatchSeq's in the originating\n+/// ast::Matcher it was derived from.\n \n pub enum NamedMatch {\n     MatchedSeq(Vec<Rc<NamedMatch>>, codemap::Span),\n@@ -224,7 +220,8 @@ pub fn parse_or_else(sess: &ParseSess,\n     }\n }\n \n-// perform a token equality check, ignoring syntax context (that is, an unhygienic comparison)\n+/// Perform a token equality check, ignoring syntax context (that is, an\n+/// unhygienic comparison)\n pub fn token_name_eq(t1 : &Token, t2 : &Token) -> bool {\n     match (t1,t2) {\n         (&token::IDENT(id1,_),&token::IDENT(id2,_))"}, {"sha": "249e9305150d665acf4da8d348ea4c45b0359461", "filename": "src/libsyntax/ext/tt/macro_rules.rs", "status": "modified", "additions": 4, "deletions": 4, "changes": 8, "blob_url": "https://github.com/rust-lang/rust/blob/ad3eda123a00b3ddf8897295ff26d97ba7c64d44/src%2Flibsyntax%2Fext%2Ftt%2Fmacro_rules.rs", "raw_url": "https://github.com/rust-lang/rust/raw/ad3eda123a00b3ddf8897295ff26d97ba7c64d44/src%2Flibsyntax%2Fext%2Ftt%2Fmacro_rules.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fext%2Ftt%2Fmacro_rules.rs?ref=ad3eda123a00b3ddf8897295ff26d97ba7c64d44", "patch": "@@ -119,7 +119,7 @@ impl MacResult for MacroRulesDefiner {\n     }\n }\n \n-// Given `lhses` and `rhses`, this is the new macro we create\n+/// Given `lhses` and `rhses`, this is the new macro we create\n fn generic_extension(cx: &ExtCtxt,\n                      sp: Span,\n                      name: Ident,\n@@ -193,9 +193,9 @@ fn generic_extension(cx: &ExtCtxt,\n     cx.span_fatal(best_fail_spot, best_fail_msg.as_slice());\n }\n \n-// this procedure performs the expansion of the\n-// macro_rules! macro. It parses the RHS and adds\n-// an extension to the current context.\n+/// This procedure performs the expansion of the\n+/// macro_rules! macro. It parses the RHS and adds\n+/// an extension to the current context.\n pub fn add_new_extension(cx: &mut ExtCtxt,\n                          sp: Span,\n                          name: Ident,"}, {"sha": "726a7315f69913c387b89c814edac51c863fa4ba", "filename": "src/libsyntax/ext/tt/transcribe.rs", "status": "modified", "additions": 6, "deletions": 6, "changes": 12, "blob_url": "https://github.com/rust-lang/rust/blob/ad3eda123a00b3ddf8897295ff26d97ba7c64d44/src%2Flibsyntax%2Fext%2Ftt%2Ftranscribe.rs", "raw_url": "https://github.com/rust-lang/rust/raw/ad3eda123a00b3ddf8897295ff26d97ba7c64d44/src%2Flibsyntax%2Fext%2Ftt%2Ftranscribe.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fext%2Ftt%2Ftranscribe.rs?ref=ad3eda123a00b3ddf8897295ff26d97ba7c64d44", "patch": "@@ -32,7 +32,7 @@ struct TtFrame {\n #[deriving(Clone)]\n pub struct TtReader<'a> {\n     pub sp_diag: &'a SpanHandler,\n-    // the unzipped tree:\n+    /// the unzipped tree:\n     stack: Vec<TtFrame>,\n     /* for MBE-style macro transcription */\n     interpolations: HashMap<Ident, Rc<NamedMatch>>,\n@@ -43,9 +43,9 @@ pub struct TtReader<'a> {\n     pub cur_span: Span,\n }\n \n-/** This can do Macro-By-Example transcription. On the other hand, if\n- *  `src` contains no `TTSeq`s and `TTNonterminal`s, `interp` can (and\n- *  should) be none. */\n+/// This can do Macro-By-Example transcription. On the other hand, if\n+/// `src` contains no `TTSeq`s and `TTNonterminal`s, `interp` can (and\n+/// should) be none.\n pub fn new_tt_reader<'a>(sp_diag: &'a SpanHandler,\n                          interp: Option<HashMap<Ident, Rc<NamedMatch>>>,\n                          src: Vec<ast::TokenTree> )\n@@ -138,8 +138,8 @@ fn lockstep_iter_size(t: &TokenTree, r: &TtReader) -> LockstepIterSize {\n     }\n }\n \n-// return the next token from the TtReader.\n-// EFFECT: advances the reader's token field\n+/// Return the next token from the TtReader.\n+/// EFFECT: advances the reader's token field\n pub fn tt_next_token(r: &mut TtReader) -> TokenAndSpan {\n     // FIXME(pcwalton): Bad copy?\n     let ret_val = TokenAndSpan {"}, {"sha": "53ee991385ae3aafca4f50650b252d159f2afe85", "filename": "src/libsyntax/lib.rs", "status": "modified", "additions": 5, "deletions": 9, "changes": 14, "blob_url": "https://github.com/rust-lang/rust/blob/ad3eda123a00b3ddf8897295ff26d97ba7c64d44/src%2Flibsyntax%2Flib.rs", "raw_url": "https://github.com/rust-lang/rust/raw/ad3eda123a00b3ddf8897295ff26d97ba7c64d44/src%2Flibsyntax%2Flib.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Flib.rs?ref=ad3eda123a00b3ddf8897295ff26d97ba7c64d44", "patch": "@@ -8,15 +8,11 @@\n // option. This file may not be copied, modified, or distributed\n // except according to those terms.\n \n-/*!\n-\n-The Rust parser and macro expander.\n-\n-# Note\n-\n-This API is completely unstable and subject to change.\n-\n-*/\n+//! The Rust parser and macro expander.\n+//!\n+//! # Note\n+//!\n+//! This API is completely unstable and subject to change.\n \n #![crate_id = \"syntax#0.11.0\"] // NOTE: remove after stage0\n #![crate_name = \"syntax\"]"}, {"sha": "55ad1b771231011d9c1168103af456be83fffcdb", "filename": "src/libsyntax/parse/attr.rs", "status": "modified", "additions": 24, "deletions": 24, "changes": 48, "blob_url": "https://github.com/rust-lang/rust/blob/ad3eda123a00b3ddf8897295ff26d97ba7c64d44/src%2Flibsyntax%2Fparse%2Fattr.rs", "raw_url": "https://github.com/rust-lang/rust/raw/ad3eda123a00b3ddf8897295ff26d97ba7c64d44/src%2Flibsyntax%2Fparse%2Fattr.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Fattr.rs?ref=ad3eda123a00b3ddf8897295ff26d97ba7c64d44", "patch": "@@ -18,7 +18,7 @@ use parse::token::INTERPOLATED;\n \n use std::gc::{Gc, GC};\n \n-// a parser that can parse attributes.\n+/// A parser that can parse attributes.\n pub trait ParserAttr {\n     fn parse_outer_attributes(&mut self) -> Vec<ast::Attribute>;\n     fn parse_attribute(&mut self, permit_inner: bool) -> ast::Attribute;\n@@ -30,11 +30,11 @@ pub trait ParserAttr {\n }\n \n impl<'a> ParserAttr for Parser<'a> {\n-    // Parse attributes that appear before an item\n+    /// Parse attributes that appear before an item\n     fn parse_outer_attributes(&mut self) -> Vec<ast::Attribute> {\n         let mut attrs: Vec<ast::Attribute> = Vec::new();\n         loop {\n-            debug!(\"parse_outer_attributes: self.token={:?}\",\n+            debug!(\"parse_outer_attributes: self.token={}\",\n                    self.token);\n             match self.token {\n               token::POUND => {\n@@ -43,7 +43,7 @@ impl<'a> ParserAttr for Parser<'a> {\n               token::DOC_COMMENT(s) => {\n                 let attr = ::attr::mk_sugared_doc_attr(\n                     attr::mk_attr_id(),\n-                    self.id_to_interned_str(s),\n+                    self.id_to_interned_str(s.ident()),\n                     self.span.lo,\n                     self.span.hi\n                 );\n@@ -59,10 +59,10 @@ impl<'a> ParserAttr for Parser<'a> {\n         return attrs;\n     }\n \n-    // matches attribute = # ! [ meta_item ]\n-    //\n-    // if permit_inner is true, then a leading `!` indicates an inner\n-    // attribute\n+    /// Matches `attribute = # ! [ meta_item ]`\n+    ///\n+    /// If permit_inner is true, then a leading `!` indicates an inner\n+    /// attribute\n     fn parse_attribute(&mut self, permit_inner: bool) -> ast::Attribute {\n         debug!(\"parse_attributes: permit_inner={:?} self.token={:?}\",\n                permit_inner, self.token);\n@@ -114,17 +114,17 @@ impl<'a> ParserAttr for Parser<'a> {\n         };\n     }\n \n-    // Parse attributes that appear after the opening of an item. These should\n-    // be preceded by an exclamation mark, but we accept and warn about one\n-    // terminated by a semicolon. In addition to a vector of inner attributes,\n-    // this function also returns a vector that may contain the first outer\n-    // attribute of the next item (since we can't know whether the attribute\n-    // is an inner attribute of the containing item or an outer attribute of\n-    // the first contained item until we see the semi).\n-\n-    // matches inner_attrs* outer_attr?\n-    // you can make the 'next' field an Option, but the result is going to be\n-    // more useful as a vector.\n+    /// Parse attributes that appear after the opening of an item. These should\n+    /// be preceded by an exclamation mark, but we accept and warn about one\n+    /// terminated by a semicolon. In addition to a vector of inner attributes,\n+    /// this function also returns a vector that may contain the first outer\n+    /// attribute of the next item (since we can't know whether the attribute\n+    /// is an inner attribute of the containing item or an outer attribute of\n+    /// the first contained item until we see the semi).\n+\n+    /// matches inner_attrs* outer_attr?\n+    /// you can make the 'next' field an Option, but the result is going to be\n+    /// more useful as a vector.\n     fn parse_inner_attrs_and_next(&mut self)\n                                   -> (Vec<ast::Attribute> , Vec<ast::Attribute> ) {\n         let mut inner_attrs: Vec<ast::Attribute> = Vec::new();\n@@ -139,7 +139,7 @@ impl<'a> ParserAttr for Parser<'a> {\n                     let Span { lo, hi, .. } = self.span;\n                     self.bump();\n                     attr::mk_sugared_doc_attr(attr::mk_attr_id(),\n-                                              self.id_to_interned_str(s),\n+                                              self.id_to_interned_str(s.ident()),\n                                               lo,\n                                               hi)\n                 }\n@@ -157,9 +157,9 @@ impl<'a> ParserAttr for Parser<'a> {\n         (inner_attrs, next_outer_attrs)\n     }\n \n-    // matches meta_item = IDENT\n-    // | IDENT = lit\n-    // | IDENT meta_seq\n+    /// matches meta_item = IDENT\n+    /// | IDENT = lit\n+    /// | IDENT meta_seq\n     fn parse_meta_item(&mut self) -> Gc<ast::MetaItem> {\n         match self.token {\n             token::INTERPOLATED(token::NtMeta(e)) => {\n@@ -201,7 +201,7 @@ impl<'a> ParserAttr for Parser<'a> {\n         }\n     }\n \n-    // matches meta_seq = ( COMMASEP(meta_item) )\n+    /// matches meta_seq = ( COMMASEP(meta_item) )\n     fn parse_meta_seq(&mut self) -> Vec<Gc<ast::MetaItem>> {\n         self.parse_seq(&token::LPAREN,\n                        &token::RPAREN,"}, {"sha": "516f22cdf4d606abab30e64661184aa4714a36f1", "filename": "src/libsyntax/parse/classify.rs", "status": "modified", "additions": 10, "deletions": 10, "changes": 20, "blob_url": "https://github.com/rust-lang/rust/blob/ad3eda123a00b3ddf8897295ff26d97ba7c64d44/src%2Flibsyntax%2Fparse%2Fclassify.rs", "raw_url": "https://github.com/rust-lang/rust/raw/ad3eda123a00b3ddf8897295ff26d97ba7c64d44/src%2Flibsyntax%2Fparse%2Fclassify.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Fclassify.rs?ref=ad3eda123a00b3ddf8897295ff26d97ba7c64d44", "patch": "@@ -15,13 +15,13 @@\n use ast;\n use std::gc::Gc;\n \n-// does this expression require a semicolon to be treated\n-// as a statement? The negation of this: 'can this expression\n-// be used as a statement without a semicolon' -- is used\n-// as an early-bail-out in the parser so that, for instance,\n-// 'if true {...} else {...}\n-//  |x| 5 '\n-// isn't parsed as (if true {...} else {...} | x) | 5\n+/// Does this expression require a semicolon to be treated\n+/// as a statement? The negation of this: 'can this expression\n+/// be used as a statement without a semicolon' -- is used\n+/// as an early-bail-out in the parser so that, for instance,\n+///     if true {...} else {...}\n+///      |x| 5\n+/// isn't parsed as (if true {...} else {...} | x) | 5\n pub fn expr_requires_semi_to_be_stmt(e: Gc<ast::Expr>) -> bool {\n     match e.node {\n         ast::ExprIf(..)\n@@ -41,9 +41,9 @@ pub fn expr_is_simple_block(e: Gc<ast::Expr>) -> bool {\n     }\n }\n \n-// this statement requires a semicolon after it.\n-// note that in one case (stmt_semi), we've already\n-// seen the semicolon, and thus don't need another.\n+/// this statement requires a semicolon after it.\n+/// note that in one case (stmt_semi), we've already\n+/// seen the semicolon, and thus don't need another.\n pub fn stmt_ends_with_semi(stmt: &ast::Stmt) -> bool {\n     return match stmt.node {\n         ast::StmtDecl(d, _) => {"}, {"sha": "3842170d67777ac039b12ca3536ca1888f29cbb9", "filename": "src/libsyntax/parse/common.rs", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/rust-lang/rust/blob/ad3eda123a00b3ddf8897295ff26d97ba7c64d44/src%2Flibsyntax%2Fparse%2Fcommon.rs", "raw_url": "https://github.com/rust-lang/rust/raw/ad3eda123a00b3ddf8897295ff26d97ba7c64d44/src%2Flibsyntax%2Fparse%2Fcommon.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Fcommon.rs?ref=ad3eda123a00b3ddf8897295ff26d97ba7c64d44", "patch": "@@ -12,8 +12,8 @@\n \n use parse::token;\n \n-// SeqSep : a sequence separator (token)\n-// and whether a trailing separator is allowed.\n+/// SeqSep : a sequence separator (token)\n+/// and whether a trailing separator is allowed.\n pub struct SeqSep {\n     pub sep: Option<token::Token>,\n     pub trailing_sep_allowed: bool"}, {"sha": "3f3a8a723f10c40d13c9ee7ca4112870247fb3dd", "filename": "src/libsyntax/parse/lexer/comments.rs", "status": "modified", "additions": 15, "deletions": 11, "changes": 26, "blob_url": "https://github.com/rust-lang/rust/blob/ad3eda123a00b3ddf8897295ff26d97ba7c64d44/src%2Flibsyntax%2Fparse%2Flexer%2Fcomments.rs", "raw_url": "https://github.com/rust-lang/rust/raw/ad3eda123a00b3ddf8897295ff26d97ba7c64d44/src%2Flibsyntax%2Fparse%2Flexer%2Fcomments.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Flexer%2Fcomments.rs?ref=ad3eda123a00b3ddf8897295ff26d97ba7c64d44", "patch": "@@ -13,7 +13,7 @@ use codemap::{BytePos, CharPos, CodeMap, Pos};\n use diagnostic;\n use parse::lexer::{is_whitespace, Reader};\n use parse::lexer::{StringReader, TokenAndSpan};\n-use parse::lexer::{is_line_non_doc_comment, is_block_non_doc_comment};\n+use parse::lexer::is_block_doc_comment;\n use parse::lexer;\n use parse::token;\n \n@@ -24,10 +24,14 @@ use std::uint;\n \n #[deriving(Clone, PartialEq)]\n pub enum CommentStyle {\n-    Isolated, // No code on either side of each line of the comment\n-    Trailing, // Code exists to the left of the comment\n-    Mixed, // Code before /* foo */ and after the comment\n-    BlankLine, // Just a manual blank line \"\\n\\n\", for layout\n+    /// No code on either side of each line of the comment\n+    Isolated,\n+    /// Code exists to the left of the comment\n+    Trailing,\n+    /// Code before /* foo */ and after the comment\n+    Mixed,\n+    /// Just a manual blank line \"\\n\\n\", for layout\n+    BlankLine,\n }\n \n #[deriving(Clone)]\n@@ -38,9 +42,9 @@ pub struct Comment {\n }\n \n pub fn is_doc_comment(s: &str) -> bool {\n-    (s.starts_with(\"///\") && !is_line_non_doc_comment(s)) ||\n+    (s.starts_with(\"///\") && super::is_doc_comment(s)) ||\n     s.starts_with(\"//!\") ||\n-    (s.starts_with(\"/**\") && !is_block_non_doc_comment(s)) ||\n+    (s.starts_with(\"/**\") && is_block_doc_comment(s)) ||\n     s.starts_with(\"/*!\")\n }\n \n@@ -198,9 +202,9 @@ fn read_line_comments(rdr: &mut StringReader, code_to_the_left: bool,\n     }\n }\n \n-// Returns None if the first col chars of s contain a non-whitespace char.\n-// Otherwise returns Some(k) where k is first char offset after that leading\n-// whitespace.  Note k may be outside bounds of s.\n+/// Returns None if the first col chars of s contain a non-whitespace char.\n+/// Otherwise returns Some(k) where k is first char offset after that leading\n+/// whitespace.  Note k may be outside bounds of s.\n fn all_whitespace(s: &str, col: CharPos) -> Option<uint> {\n     let len = s.len();\n     let mut col = col.to_uint();\n@@ -256,7 +260,7 @@ fn read_block_comment(rdr: &mut StringReader,\n             rdr.bump();\n             rdr.bump();\n         }\n-        if !is_block_non_doc_comment(curr_line.as_slice()) {\n+        if is_block_doc_comment(curr_line.as_slice()) {\n             return\n         }\n         assert!(!curr_line.as_slice().contains_char('\\n'));"}, {"sha": "0aaddacfab624783179153e274e848263b6ac381", "filename": "src/libsyntax/parse/lexer/mod.rs", "status": "modified", "additions": 422, "deletions": 381, "changes": 803, "blob_url": "https://github.com/rust-lang/rust/blob/ad3eda123a00b3ddf8897295ff26d97ba7c64d44/src%2Flibsyntax%2Fparse%2Flexer%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/ad3eda123a00b3ddf8897295ff26d97ba7c64d44/src%2Flibsyntax%2Fparse%2Flexer%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Flexer%2Fmod.rs?ref=ad3eda123a00b3ddf8897295ff26d97ba7c64d44", "patch": "@@ -18,7 +18,6 @@ use parse::token::{str_to_ident};\n \n use std::char;\n use std::mem::replace;\n-use std::num::from_str_radix;\n use std::rc::Rc;\n use std::str;\n \n@@ -44,13 +43,13 @@ pub struct TokenAndSpan {\n \n pub struct StringReader<'a> {\n     pub span_diagnostic: &'a SpanHandler,\n-    // The absolute offset within the codemap of the next character to read\n+    /// The absolute offset within the codemap of the next character to read\n     pub pos: BytePos,\n-    // The absolute offset within the codemap of the last character read(curr)\n+    /// The absolute offset within the codemap of the last character read(curr)\n     pub last_pos: BytePos,\n-    // The column of the next character to read\n+    /// The column of the next character to read\n     pub col: CharPos,\n-    // The last character to be read\n+    /// The last character to be read\n     pub curr: Option<char>,\n     pub filemap: Rc<codemap::FileMap>,\n     /* cached: */\n@@ -60,7 +59,7 @@ pub struct StringReader<'a> {\n \n impl<'a> Reader for StringReader<'a> {\n     fn is_eof(&self) -> bool { self.curr.is_none() }\n-    // return the next token. EFFECT: advances the string_reader.\n+    /// Return the next token. EFFECT: advances the string_reader.\n     fn next_token(&mut self) -> TokenAndSpan {\n         let ret_val = TokenAndSpan {\n             tok: replace(&mut self.peek_tok, token::UNDERSCORE),\n@@ -90,7 +89,7 @@ impl<'a> Reader for TtReader<'a> {\n     }\n     fn next_token(&mut self) -> TokenAndSpan {\n         let r = tt_next_token(self);\n-        debug!(\"TtReader: r={:?}\", r);\n+        debug!(\"TtReader: r={}\", r);\n         r\n     }\n     fn fatal(&self, m: &str) -> ! {\n@@ -188,7 +187,7 @@ impl<'a> StringReader<'a> {\n     /// Advance peek_tok and peek_span to refer to the next token, and\n     /// possibly update the interner.\n     fn advance_token(&mut self) {\n-        match self.consume_whitespace_and_comments() {\n+        match self.scan_whitespace_or_comment() {\n             Some(comment) => {\n                 self.peek_span = comment.sp;\n                 self.peek_tok = comment.tok;\n@@ -217,6 +216,20 @@ impl<'a> StringReader<'a> {\n         self.with_str_from_to(start, self.last_pos, f)\n     }\n \n+    /// Create a Name from a given offset to the current offset, each\n+    /// adjusted 1 towards each other (assumes that on either side there is a\n+    /// single-byte delimiter).\n+    pub fn name_from(&self, start: BytePos) -> ast::Name {\n+        debug!(\"taking an ident from {} to {}\", start, self.last_pos);\n+        self.with_str_from(start, token::intern)\n+    }\n+\n+    /// As name_from, with an explicit endpoint.\n+    pub fn name_from_to(&self, start: BytePos, end: BytePos) -> ast::Name {\n+        debug!(\"taking an ident from {} to {}\", start, end);\n+        self.with_str_from_to(start, end, token::intern)\n+    }\n+\n     /// Calls `f` with a string slice of the source text spanning from `start`\n     /// up to but excluding `end`.\n     fn with_str_from_to<T>(&self, start: BytePos, end: BytePos, f: |s: &str| -> T) -> T {\n@@ -326,8 +339,7 @@ impl<'a> StringReader<'a> {\n \n     /// PRECONDITION: self.curr is not whitespace\n     /// Eats any kind of comment.\n-    /// Returns a Some(sugared-doc-attr) if one exists, None otherwise\n-    fn consume_any_line_comment(&mut self) -> Option<TokenAndSpan> {\n+    fn scan_comment(&mut self) -> Option<TokenAndSpan> {\n         match self.curr {\n             Some(c) => {\n                 if c.is_whitespace() {\n@@ -362,28 +374,32 @@ impl<'a> StringReader<'a> {\n                             }\n                             self.bump();\n                         }\n-                        let ret = self.with_str_from(start_bpos, |string| {\n+                        return self.with_str_from(start_bpos, |string| {\n                             // but comments with only more \"/\"s are not\n-                            if !is_line_non_doc_comment(string) {\n-                                Some(TokenAndSpan{\n-                                    tok: token::DOC_COMMENT(str_to_ident(string)),\n-                                    sp: codemap::mk_sp(start_bpos, self.last_pos)\n-                                })\n+                            let tok = if is_doc_comment(string) {\n+                                token::DOC_COMMENT(token::intern(string))\n                             } else {\n-                                None\n-                            }\n-                        });\n+                                token::COMMENT\n+                            };\n \n-                        if ret.is_some() {\n-                            return ret;\n-                        }\n+                            return Some(TokenAndSpan{\n+                                tok: tok,\n+                                sp: codemap::mk_sp(start_bpos, self.last_pos)\n+                            });\n+                        });\n                     } else {\n+                        let start_bpos = self.last_pos - BytePos(2);\n                         while !self.curr_is('\\n') && !self.is_eof() { self.bump(); }\n+                        return Some(TokenAndSpan {\n+                            tok: token::COMMENT,\n+                            sp: codemap::mk_sp(start_bpos, self.last_pos)\n+                        });\n                     }\n-                    // Restart whitespace munch.\n-                    self.consume_whitespace_and_comments()\n                 }\n-                Some('*') => { self.bump(); self.bump(); self.consume_block_comment() }\n+                Some('*') => {\n+                    self.bump(); self.bump();\n+                    self.scan_block_comment()\n+                }\n                 _ => None\n             }\n         } else if self.curr_is('#') {\n@@ -399,9 +415,15 @@ impl<'a> StringReader<'a> {\n                 let cmap = CodeMap::new();\n                 cmap.files.borrow_mut().push(self.filemap.clone());\n                 let loc = cmap.lookup_char_pos_adj(self.last_pos);\n+                debug!(\"Skipping a shebang\");\n                 if loc.line == 1u && loc.col == CharPos(0u) {\n+                    // FIXME: Add shebang \"token\", return it\n+                    let start = self.last_pos;\n                     while !self.curr_is('\\n') && !self.is_eof() { self.bump(); }\n-                    return self.consume_whitespace_and_comments();\n+                    return Some(TokenAndSpan {\n+                        tok: token::SHEBANG(self.name_from(start)),\n+                        sp: codemap::mk_sp(start, self.last_pos)\n+                    });\n                 }\n             }\n             None\n@@ -410,15 +432,33 @@ impl<'a> StringReader<'a> {\n         }\n     }\n \n-    /// EFFECT: eats whitespace and comments.\n-    /// Returns a Some(sugared-doc-attr) if one exists, None otherwise.\n-    fn consume_whitespace_and_comments(&mut self) -> Option<TokenAndSpan> {\n-        while is_whitespace(self.curr) { self.bump(); }\n-        return self.consume_any_line_comment();\n+    /// If there is whitespace, shebang, or a comment, scan it. Otherwise,\n+    /// return None.\n+    fn scan_whitespace_or_comment(&mut self) -> Option<TokenAndSpan> {\n+        match self.curr.unwrap_or('\\0') {\n+            // # to handle shebang at start of file -- this is the entry point\n+            // for skipping over all \"junk\"\n+            '/' | '#' => {\n+                let c = self.scan_comment();\n+                debug!(\"scanning a comment {}\", c);\n+                c\n+            },\n+            c if is_whitespace(Some(c)) => {\n+                let start_bpos = self.last_pos;\n+                while is_whitespace(self.curr) { self.bump(); }\n+                let c = Some(TokenAndSpan {\n+                    tok: token::WS,\n+                    sp: codemap::mk_sp(start_bpos, self.last_pos)\n+                });\n+                debug!(\"scanning whitespace: {}\", c);\n+                c\n+            },\n+            _ => None\n+        }\n     }\n \n-    // might return a sugared-doc-attr\n-    fn consume_block_comment(&mut self) -> Option<TokenAndSpan> {\n+    /// Might return a sugared-doc-attr\n+    fn scan_block_comment(&mut self) -> Option<TokenAndSpan> {\n         // block comments starting with \"/**\" or \"/*!\" are doc-comments\n         let is_doc_comment = self.curr_is('*') || self.curr_is('!');\n         let start_bpos = self.last_pos - BytePos(2);\n@@ -453,228 +493,132 @@ impl<'a> StringReader<'a> {\n             self.bump();\n         }\n \n-        let res = if is_doc_comment {\n-            self.with_str_from(start_bpos, |string| {\n-                // but comments with only \"*\"s between two \"/\"s are not\n-                if !is_block_non_doc_comment(string) {\n-                    let string = if has_cr {\n-                        self.translate_crlf(start_bpos, string,\n-                                            \"bare CR not allowed in block doc-comment\")\n-                    } else { string.into_maybe_owned() };\n-                    Some(TokenAndSpan{\n-                            tok: token::DOC_COMMENT(str_to_ident(string.as_slice())),\n-                            sp: codemap::mk_sp(start_bpos, self.last_pos)\n-                        })\n-                } else {\n-                    None\n-                }\n-            })\n-        } else {\n-            None\n-        };\n-\n-        // restart whitespace munch.\n-        if res.is_some() { res } else { self.consume_whitespace_and_comments() }\n-    }\n-\n-    fn scan_exponent(&mut self, start_bpos: BytePos) -> Option<String> {\n-        // \\x00 hits the `return None` case immediately, so this is fine.\n-        let mut c = self.curr.unwrap_or('\\x00');\n-        let mut rslt = String::new();\n-        if c == 'e' || c == 'E' {\n-            rslt.push_char(c);\n-            self.bump();\n-            c = self.curr.unwrap_or('\\x00');\n-            if c == '-' || c == '+' {\n-                rslt.push_char(c);\n-                self.bump();\n-            }\n-            let exponent = self.scan_digits(10u);\n-            if exponent.len() > 0u {\n-                rslt.push_str(exponent.as_slice());\n-                return Some(rslt);\n+        self.with_str_from(start_bpos, |string| {\n+            // but comments with only \"*\"s between two \"/\"s are not\n+            let tok = if is_block_doc_comment(string) {\n+                let string = if has_cr {\n+                    self.translate_crlf(start_bpos, string,\n+                                        \"bare CR not allowed in block doc-comment\")\n+                } else { string.into_maybe_owned() };\n+                token::DOC_COMMENT(token::intern(string.as_slice()))\n             } else {\n-                let last_bpos = self.last_pos;\n-                self.err_span_(start_bpos, last_bpos, \"scan_exponent: bad fp literal\");\n-                rslt.push_str(\"1\"); // arbitrary placeholder exponent\n-                return Some(rslt);\n-            }\n-        } else {\n-            return None::<String>;\n-        }\n+                token::COMMENT\n+            };\n+\n+            Some(TokenAndSpan{\n+                tok: tok,\n+                sp: codemap::mk_sp(start_bpos, self.last_pos)\n+            })\n+        })\n     }\n \n-    fn scan_digits(&mut self, radix: uint) -> String {\n-        let mut rslt = String::new();\n+    /// Scan through any digits (base `radix`) or underscores, and return how\n+    /// many digits there were.\n+    fn scan_digits(&mut self, radix: uint) -> uint {\n+        let mut len = 0u;\n         loop {\n             let c = self.curr;\n-            if c == Some('_') { self.bump(); continue; }\n+            if c == Some('_') { debug!(\"skipping a _\"); self.bump(); continue; }\n             match c.and_then(|cc| char::to_digit(cc, radix)) {\n-              Some(_) => {\n-                rslt.push_char(c.unwrap());\n-                self.bump();\n-              }\n-              _ => return rslt\n+                Some(_) => {\n+                    debug!(\"{} in scan_digits\", c);\n+                    len += 1;\n+                    self.bump();\n+                }\n+                _ => return len\n             }\n         };\n     }\n \n-    fn check_float_base(&mut self, start_bpos: BytePos, last_bpos: BytePos, base: uint) {\n-        match base {\n-          16u => self.err_span_(start_bpos, last_bpos,\n-                                \"hexadecimal float literal is not supported\"),\n-          8u => self.err_span_(start_bpos, last_bpos, \"octal float literal is not supported\"),\n-          2u => self.err_span_(start_bpos, last_bpos, \"binary float literal is not supported\"),\n-          _ => ()\n-        }\n-    }\n-\n+    /// Lex a LIT_INTEGER or a LIT_FLOAT\n     fn scan_number(&mut self, c: char) -> token::Token {\n-        let mut num_str;\n-        let mut base = 10u;\n-        let mut c = c;\n-        let mut n = self.nextch().unwrap_or('\\x00');\n+        let mut num_digits;\n+        let mut base = 10;\n         let start_bpos = self.last_pos;\n-        if c == '0' && n == 'x' {\n-            self.bump();\n-            self.bump();\n-            base = 16u;\n-        } else if c == '0' && n == 'o' {\n-            self.bump();\n-            self.bump();\n-            base = 8u;\n-        } else if c == '0' && n == 'b' {\n-            self.bump();\n-            self.bump();\n-            base = 2u;\n-        }\n-        num_str = self.scan_digits(base);\n-        c = self.curr.unwrap_or('\\x00');\n-        self.nextch();\n-        if c == 'u' || c == 'i' {\n-            enum Result { Signed(ast::IntTy), Unsigned(ast::UintTy) }\n-            let signed = c == 'i';\n-            let mut tp = {\n-                if signed { Signed(ast::TyI) }\n-                else { Unsigned(ast::TyU) }\n-            };\n-            self.bump();\n-            c = self.curr.unwrap_or('\\x00');\n-            if c == '8' {\n-                self.bump();\n-                tp = if signed { Signed(ast::TyI8) }\n-                          else { Unsigned(ast::TyU8) };\n-            }\n-            n = self.nextch().unwrap_or('\\x00');\n-            if c == '1' && n == '6' {\n-                self.bump();\n-                self.bump();\n-                tp = if signed { Signed(ast::TyI16) }\n-                          else { Unsigned(ast::TyU16) };\n-            } else if c == '3' && n == '2' {\n-                self.bump();\n-                self.bump();\n-                tp = if signed { Signed(ast::TyI32) }\n-                          else { Unsigned(ast::TyU32) };\n-            } else if c == '6' && n == '4' {\n-                self.bump();\n-                self.bump();\n-                tp = if signed { Signed(ast::TyI64) }\n-                          else { Unsigned(ast::TyU64) };\n-            }\n-            if num_str.len() == 0u {\n-                let last_bpos = self.last_pos;\n-                self.err_span_(start_bpos, last_bpos, \"no valid digits found for number\");\n-                num_str = \"1\".to_string();\n-            }\n-            let parsed = match from_str_radix::<u64>(num_str.as_slice(),\n-                                                     base as uint) {\n-                Some(p) => p,\n-                None => {\n-                    let last_bpos = self.last_pos;\n-                    self.err_span_(start_bpos, last_bpos, \"int literal is too large\");\n-                    1\n-                }\n-            };\n \n-            match tp {\n-              Signed(t) => return token::LIT_INT(parsed as i64, t),\n-              Unsigned(t) => return token::LIT_UINT(parsed, t)\n+        self.bump();\n+\n+        if c == '0' {\n+            match self.curr.unwrap_or('\\0') {\n+                'b' => { self.bump(); base = 2; num_digits = self.scan_digits(2); }\n+                'o' => { self.bump(); base = 8; num_digits = self.scan_digits(8); }\n+                'x' => { self.bump(); base = 16; num_digits = self.scan_digits(16); }\n+                '0'..'9' | '_' | '.' => {\n+                    num_digits = self.scan_digits(10) + 1;\n+                }\n+                'u' | 'i' => {\n+                    self.scan_int_suffix();\n+                    return token::LIT_INTEGER(self.name_from(start_bpos));\n+                },\n+                'f' => {\n+                    let last_pos = self.last_pos;\n+                    self.scan_float_suffix();\n+                    self.check_float_base(start_bpos, last_pos, base);\n+                    return token::LIT_FLOAT(self.name_from(start_bpos));\n+                }\n+                _ => {\n+                    // just a 0\n+                    return token::LIT_INTEGER(self.name_from(start_bpos));\n+                }\n             }\n+        } else if c.is_digit_radix(10) {\n+            num_digits = self.scan_digits(10) + 1;\n+        } else {\n+            num_digits = 0;\n         }\n-        let mut is_float = false;\n-        if self.curr_is('.') && !(ident_start(self.nextch()) || self.nextch_is('.')) {\n-            is_float = true;\n-            self.bump();\n-            let dec_part = self.scan_digits(10u);\n-            num_str.push_char('.');\n-            num_str.push_str(dec_part.as_slice());\n-        }\n-        match self.scan_exponent(start_bpos) {\n-          Some(ref s) => {\n-            is_float = true;\n-            num_str.push_str(s.as_slice());\n-          }\n-          None => ()\n+\n+        if num_digits == 0 {\n+            self.err_span_(start_bpos, self.last_pos, \"no valid digits found for number\");\n+            // eat any suffix\n+            self.scan_int_suffix();\n+            return token::LIT_INTEGER(token::intern(\"0\"));\n         }\n \n-        if self.curr_is('f') {\n+        // might be a float, but don't be greedy if this is actually an\n+        // integer literal followed by field/method access or a range pattern\n+        // (`0..2` and `12.foo()`)\n+        if self.curr_is('.') && !self.nextch_is('.') && !self.nextch().unwrap_or('\\0')\n+                                                             .is_XID_start() {\n+            // might have stuff after the ., and if it does, it needs to start\n+            // with a number\n             self.bump();\n-            c = self.curr.unwrap_or('\\x00');\n-            n = self.nextch().unwrap_or('\\x00');\n-            if c == '3' && n == '2' {\n-                self.bump();\n-                self.bump();\n-                let last_bpos = self.last_pos;\n-                self.check_float_base(start_bpos, last_bpos, base);\n-                return token::LIT_FLOAT(str_to_ident(num_str.as_slice()),\n-                                        ast::TyF32);\n-            } else if c == '6' && n == '4' {\n-                self.bump();\n-                self.bump();\n-                let last_bpos = self.last_pos;\n-                self.check_float_base(start_bpos, last_bpos, base);\n-                return token::LIT_FLOAT(str_to_ident(num_str.as_slice()),\n-                                        ast::TyF64);\n-                /* FIXME (#2252): if this is out of range for either a\n-                32-bit or 64-bit float, it won't be noticed till the\n-                back-end.  */\n+            if self.curr.unwrap_or('\\0').is_digit_radix(10) {\n+                self.scan_digits(10);\n+                self.scan_float_exponent();\n+                self.scan_float_suffix();\n             }\n-            let last_bpos = self.last_pos;\n-            self.err_span_(start_bpos, last_bpos, \"expected `f32` or `f64` suffix\");\n-        }\n-        if is_float {\n-            let last_bpos = self.last_pos;\n-            self.check_float_base(start_bpos, last_bpos, base);\n-            return token::LIT_FLOAT_UNSUFFIXED(str_to_ident(\n-                    num_str.as_slice()));\n+            let last_pos = self.last_pos;\n+            self.check_float_base(start_bpos, last_pos, base);\n+            return token::LIT_FLOAT(self.name_from(start_bpos));\n+        } else if self.curr_is('f') {\n+            // or it might be an integer literal suffixed as a float\n+            self.scan_float_suffix();\n+            let last_pos = self.last_pos;\n+            self.check_float_base(start_bpos, last_pos, base);\n+            return token::LIT_FLOAT(self.name_from(start_bpos));\n         } else {\n-            if num_str.len() == 0u {\n-                let last_bpos = self.last_pos;\n-                self.err_span_(start_bpos, last_bpos, \"no valid digits found for number\");\n-                num_str = \"1\".to_string();\n+            // it might be a float if it has an exponent\n+            if self.curr_is('e') || self.curr_is('E') {\n+                self.scan_float_exponent();\n+                self.scan_float_suffix();\n+                let last_pos = self.last_pos;\n+                self.check_float_base(start_bpos, last_pos, base);\n+                return token::LIT_FLOAT(self.name_from(start_bpos));\n             }\n-            let parsed = match from_str_radix::<u64>(num_str.as_slice(),\n-                                                     base as uint) {\n-                Some(p) => p,\n-                None => {\n-                    let last_bpos = self.last_pos;\n-                    self.err_span_(start_bpos, last_bpos, \"int literal is too large\");\n-                    1\n-                }\n-            };\n-\n-            debug!(\"lexing {} as an unsuffixed integer literal\",\n-                   num_str.as_slice());\n-            return token::LIT_INT_UNSUFFIXED(parsed as i64);\n+            // but we certainly have an integer!\n+            self.scan_int_suffix();\n+            return token::LIT_INTEGER(self.name_from(start_bpos));\n         }\n     }\n \n-\n-    fn scan_numeric_escape(&mut self, n_hex_digits: uint, delim: char) -> char {\n-        let mut accum_int = 0u32;\n+    /// Scan over `n_digits` hex digits, stopping at `delim`, reporting an\n+    /// error if too many or too few digits are encountered.\n+    fn scan_hex_digits(&mut self, n_digits: uint, delim: char) -> bool {\n+        debug!(\"scanning {} digits until {}\", n_digits, delim);\n         let start_bpos = self.last_pos;\n-        for _ in range(0, n_hex_digits) {\n+        let mut accum_int = 0;\n+\n+        for _ in range(0, n_digits) {\n             if self.is_eof() {\n                 let last_bpos = self.last_pos;\n                 self.fatal_span_(start_bpos, last_bpos, \"unterminated numeric character escape\");\n@@ -695,20 +639,22 @@ impl<'a> StringReader<'a> {\n         }\n \n         match char::from_u32(accum_int) {\n-            Some(x) => x,\n+            Some(_) => true,\n             None => {\n                 let last_bpos = self.last_pos;\n                 self.err_span_(start_bpos, last_bpos, \"illegal numeric character escape\");\n-                '?'\n+                false\n             }\n         }\n     }\n \n     /// Scan for a single (possibly escaped) byte or char\n     /// in a byte, (non-raw) byte string, char, or (non-raw) string literal.\n     /// `start` is the position of `first_source_char`, which is already consumed.\n+    ///\n+    /// Returns true if there was a valid char/byte, false otherwise.\n     fn scan_char_or_byte(&mut self, start: BytePos, first_source_char: char,\n-                         ascii_only: bool, delim: char) -> Option<char> {\n+                         ascii_only: bool, delim: char) -> bool {\n         match first_source_char {\n             '\\\\' => {\n                 // '\\X' for some X must be a character constant:\n@@ -718,24 +664,18 @@ impl<'a> StringReader<'a> {\n                 match escaped {\n                     None => {},  // EOF here is an error that will be checked later.\n                     Some(e) => {\n-                        return Some(match e {\n-                            'n' => '\\n',\n-                            'r' => '\\r',\n-                            't' => '\\t',\n-                            '\\\\' => '\\\\',\n-                            '\\'' => '\\'',\n-                            '\"' => '\"',\n-                            '0' => '\\x00',\n-                            'x' => self.scan_numeric_escape(2u, delim),\n-                            'u' if !ascii_only => self.scan_numeric_escape(4u, delim),\n-                            'U' if !ascii_only => self.scan_numeric_escape(8u, delim),\n+                        return match e {\n+                            'n' | 'r' | 't' | '\\\\' | '\\'' | '\"' | '0' => true,\n+                            'x' => self.scan_hex_digits(2u, delim),\n+                            'u' if !ascii_only => self.scan_hex_digits(4u, delim),\n+                            'U' if !ascii_only => self.scan_hex_digits(8u, delim),\n                             '\\n' if delim == '\"' => {\n                                 self.consume_whitespace();\n-                                return None\n+                                true\n                             },\n                             '\\r' if delim == '\"' && self.curr_is('\\n') => {\n                                 self.consume_whitespace();\n-                                return None\n+                                true\n                             }\n                             c => {\n                                 let last_pos = self.last_pos;\n@@ -744,9 +684,9 @@ impl<'a> StringReader<'a> {\n                                     if ascii_only { \"unknown byte escape\" }\n                                     else { \"unknown character escape\" },\n                                     c);\n-                                c\n+                                false\n                             }\n-                        })\n+                        }\n                     }\n                 }\n             }\n@@ -757,14 +697,16 @@ impl<'a> StringReader<'a> {\n                     if ascii_only { \"byte constant must be escaped\" }\n                     else { \"character constant must be escaped\" },\n                     first_source_char);\n+                return false;\n             }\n             '\\r' => {\n                 if self.curr_is('\\n') {\n                     self.bump();\n-                    return Some('\\n');\n+                    return true;\n                 } else {\n                     self.err_span_(start, self.last_pos,\n                                    \"bare CR not allowed in string, use \\\\r instead\");\n+                    return false;\n                 }\n             }\n             _ => if ascii_only && first_source_char > '\\x7F' {\n@@ -773,9 +715,84 @@ impl<'a> StringReader<'a> {\n                     start, last_pos,\n                     \"byte constant must be ASCII. \\\n                      Use a \\\\xHH escape for a non-ASCII byte\", first_source_char);\n+                return false;\n+            }\n+        }\n+        true\n+    }\n+\n+    /// Scan over an int literal suffix.\n+    fn scan_int_suffix(&mut self) {\n+        match self.curr {\n+            Some('i') | Some('u') => {\n+                self.bump();\n+\n+                if self.curr_is('8') {\n+                    self.bump();\n+                } else if self.curr_is('1') {\n+                    if !self.nextch_is('6') {\n+                        self.err_span_(self.last_pos, self.pos,\n+                                      \"illegal int suffix\");\n+                    } else {\n+                        self.bump(); self.bump();\n+                    }\n+                } else if self.curr_is('3') {\n+                    if !self.nextch_is('2') {\n+                        self.err_span_(self.last_pos, self.pos,\n+                                      \"illegal int suffix\");\n+                    } else {\n+                        self.bump(); self.bump();\n+                    }\n+                } else if self.curr_is('6') {\n+                    if !self.nextch_is('4') {\n+                        self.err_span_(self.last_pos, self.pos,\n+                                      \"illegal int suffix\");\n+                    } else {\n+                        self.bump(); self.bump();\n+                    }\n+                }\n+            },\n+            _ => { }\n+        }\n+    }\n+\n+    /// Scan over a float literal suffix\n+    fn scan_float_suffix(&mut self) {\n+        if self.curr_is('f') {\n+            if (self.nextch_is('3') && self.nextnextch_is('2'))\n+            || (self.nextch_is('6') && self.nextnextch_is('4')) {\n+                self.bump();\n+                self.bump();\n+                self.bump();\n+            } else {\n+                self.err_span_(self.last_pos, self.pos, \"illegal float suffix\");\n             }\n         }\n-        Some(first_source_char)\n+    }\n+\n+    /// Scan over a float exponent.\n+    fn scan_float_exponent(&mut self) {\n+        if self.curr_is('e') || self.curr_is('E') {\n+            self.bump();\n+            if self.curr_is('-') || self.curr_is('+') {\n+                self.bump();\n+            }\n+            if self.scan_digits(10) == 0 {\n+                self.err_span_(self.last_pos, self.pos, \"expected at least one digit in exponent\")\n+            }\n+        }\n+    }\n+\n+    /// Check that a base is valid for a floating literal, emitting a nice\n+    /// error if it isn't.\n+    fn check_float_base(&mut self, start_bpos: BytePos, last_bpos: BytePos, base: uint) {\n+        match base {\n+            16u => self.err_span_(start_bpos, last_bpos, \"hexadecimal float literal is not \\\n+                                 supported\"),\n+            8u => self.err_span_(start_bpos, last_bpos, \"octal float literal is not supported\"),\n+            2u => self.err_span_(start_bpos, last_bpos, \"binary float literal is not supported\"),\n+            _ => ()\n+        }\n     }\n \n     fn binop(&mut self, op: token::BinOp) -> token::Token {\n@@ -910,7 +927,7 @@ impl<'a> StringReader<'a> {\n             let start = self.last_pos;\n \n             // the eof will be picked up by the final `'` check below\n-            let mut c2 = self.curr.unwrap_or('\\x00');\n+            let c2 = self.curr.unwrap_or('\\x00');\n             self.bump();\n \n             // If the character is an ident start not followed by another single\n@@ -953,7 +970,7 @@ impl<'a> StringReader<'a> {\n             }\n \n             // Otherwise it is a character constant:\n-            c2 = self.scan_char_or_byte(start, c2, /* ascii_only = */ false, '\\'').unwrap();\n+            let valid = self.scan_char_or_byte(start, c2, /* ascii_only = */ false, '\\'');\n             if !self.curr_is('\\'') {\n                 let last_bpos = self.last_pos;\n                 self.fatal_span_verbose(\n@@ -963,118 +980,23 @@ impl<'a> StringReader<'a> {\n                                    start - BytePos(1), last_bpos,\n                                    \"unterminated character constant\".to_string());\n             }\n+            let id = if valid { self.name_from(start) } else { token::intern(\"0\") };\n             self.bump(); // advance curr past token\n-            return token::LIT_CHAR(c2);\n+            return token::LIT_CHAR(id);\n           }\n           'b' => {\n             self.bump();\n             return match self.curr {\n-                Some('\\'') => parse_byte(self),\n-                Some('\"') => parse_byte_string(self),\n-                Some('r') => parse_raw_byte_string(self),\n+                Some('\\'') => self.scan_byte(),\n+                Some('\"') => self.scan_byte_string(),\n+                Some('r') => self.scan_raw_byte_string(),\n                 _ => unreachable!()  // Should have been a token::IDENT above.\n             };\n \n-            fn parse_byte(self_: &mut StringReader) -> token::Token {\n-                self_.bump();\n-                let start = self_.last_pos;\n-\n-                // the eof will be picked up by the final `'` check below\n-                let mut c2 = self_.curr.unwrap_or('\\x00');\n-                self_.bump();\n-\n-                c2 = self_.scan_char_or_byte(start, c2, /* ascii_only = */ true, '\\'').unwrap();\n-                if !self_.curr_is('\\'') {\n-                    // Byte offsetting here is okay because the\n-                    // character before position `start` are an\n-                    // ascii single quote and ascii 'b'.\n-                    let last_pos = self_.last_pos;\n-                    self_.fatal_span_verbose(\n-                        start - BytePos(2), last_pos,\n-                        \"unterminated byte constant\".to_string());\n-                }\n-                self_.bump(); // advance curr past token\n-                return token::LIT_BYTE(c2 as u8);\n-            }\n-\n-            fn parse_byte_string(self_: &mut StringReader) -> token::Token {\n-                self_.bump();\n-                let start = self_.last_pos;\n-                let mut value = Vec::new();\n-                while !self_.curr_is('\"') {\n-                    if self_.is_eof() {\n-                        let last_pos = self_.last_pos;\n-                        self_.fatal_span_(start, last_pos,\n-                                          \"unterminated double quote byte string\");\n-                    }\n-\n-                    let ch_start = self_.last_pos;\n-                    let ch = self_.curr.unwrap();\n-                    self_.bump();\n-                    self_.scan_char_or_byte(ch_start, ch, /* ascii_only = */ true, '\"')\n-                        .map(|ch| value.push(ch as u8));\n-                }\n-                self_.bump();\n-                return token::LIT_BINARY(Rc::new(value));\n-            }\n-\n-            fn parse_raw_byte_string(self_: &mut StringReader) -> token::Token {\n-                let start_bpos = self_.last_pos;\n-                self_.bump();\n-                let mut hash_count = 0u;\n-                while self_.curr_is('#') {\n-                    self_.bump();\n-                    hash_count += 1;\n-                }\n-\n-                if self_.is_eof() {\n-                    let last_pos = self_.last_pos;\n-                    self_.fatal_span_(start_bpos, last_pos, \"unterminated raw string\");\n-                } else if !self_.curr_is('\"') {\n-                    let last_pos = self_.last_pos;\n-                    let ch = self_.curr.unwrap();\n-                    self_.fatal_span_char(start_bpos, last_pos,\n-                                    \"only `#` is allowed in raw string delimitation; \\\n-                                     found illegal character\",\n-                                    ch);\n-                }\n-                self_.bump();\n-                let content_start_bpos = self_.last_pos;\n-                let mut content_end_bpos;\n-                'outer: loop {\n-                    match self_.curr {\n-                        None => {\n-                            let last_pos = self_.last_pos;\n-                            self_.fatal_span_(start_bpos, last_pos, \"unterminated raw string\")\n-                        },\n-                        Some('\"') => {\n-                            content_end_bpos = self_.last_pos;\n-                            for _ in range(0, hash_count) {\n-                                self_.bump();\n-                                if !self_.curr_is('#') {\n-                                    continue 'outer;\n-                                }\n-                            }\n-                            break;\n-                        },\n-                        Some(c) => if c > '\\x7F' {\n-                            let last_pos = self_.last_pos;\n-                            self_.err_span_char(\n-                                last_pos, last_pos, \"raw byte string must be ASCII\", c);\n-                        }\n-                    }\n-                    self_.bump();\n-                }\n-                self_.bump();\n-                let bytes = self_.with_str_from_to(content_start_bpos,\n-                                                   content_end_bpos,\n-                                                   |s| s.as_bytes().to_owned());\n-                return token::LIT_BINARY_RAW(Rc::new(bytes), hash_count);\n-            }\n           }\n           '\"' => {\n-            let mut accum_str = String::new();\n             let start_bpos = self.last_pos;\n+            let mut valid = true;\n             self.bump();\n             while !self.curr_is('\"') {\n                 if self.is_eof() {\n@@ -1085,11 +1007,13 @@ impl<'a> StringReader<'a> {\n                 let ch_start = self.last_pos;\n                 let ch = self.curr.unwrap();\n                 self.bump();\n-                self.scan_char_or_byte(ch_start, ch, /* ascii_only = */ false, '\"')\n-                    .map(|ch| accum_str.push_char(ch));\n+                valid &= self.scan_char_or_byte(ch_start, ch, /* ascii_only = */ false, '\"');\n             }\n+            // adjust for the ACSII \" at the start of the literal\n+            let id = if valid { self.name_from(start_bpos + BytePos(1)) }\n+                     else { token::intern(\"??\") };\n             self.bump();\n-            return token::LIT_STR(str_to_ident(accum_str.as_slice()));\n+            return token::LIT_STR(id);\n           }\n           'r' => {\n             let start_bpos = self.last_pos;\n@@ -1114,7 +1038,7 @@ impl<'a> StringReader<'a> {\n             self.bump();\n             let content_start_bpos = self.last_pos;\n             let mut content_end_bpos;\n-            let mut has_cr = false;\n+            let mut valid = true;\n             'outer: loop {\n                 if self.is_eof() {\n                     let last_bpos = self.last_pos;\n@@ -1137,23 +1061,26 @@ impl<'a> StringReader<'a> {\n                             }\n                         }\n                         break;\n-                    }\n+                    },\n                     '\\r' => {\n-                        has_cr = true;\n+                        if !self.nextch_is('\\n') {\n+                            let last_bpos = self.last_pos;\n+                            self.err_span_(start_bpos, last_bpos, \"bare CR not allowed in raw \\\n+                                           string, use \\\\r instead\");\n+                            valid = false;\n+                        }\n                     }\n                     _ => ()\n                 }\n                 self.bump();\n             }\n             self.bump();\n-            let str_content = self.with_str_from_to(content_start_bpos, content_end_bpos, |string| {\n-                let string = if has_cr {\n-                    self.translate_crlf(content_start_bpos, string,\n-                                        \"bare CR not allowed in raw string\")\n-                } else { string.into_maybe_owned() };\n-                str_to_ident(string.as_slice())\n-            });\n-            return token::LIT_STR_RAW(str_content, hash_count);\n+            let id = if valid {\n+                self.name_from_to(content_start_bpos, content_end_bpos)\n+            } else {\n+                token::intern(\"??\")\n+            };\n+            return token::LIT_STR_RAW(id, hash_count);\n           }\n           '-' => {\n             if self.nextch_is('>') {\n@@ -1221,6 +1148,104 @@ impl<'a> StringReader<'a> {\n      // consider shebangs comments, but not inner attributes\n      || (self.curr_is('#') && self.nextch_is('!') && !self.nextnextch_is('['))\n     }\n+\n+    fn scan_byte(&mut self) -> token::Token {\n+        self.bump();\n+        let start = self.last_pos;\n+\n+        // the eof will be picked up by the final `'` check below\n+        let c2 = self.curr.unwrap_or('\\x00');\n+        self.bump();\n+\n+        let valid = self.scan_char_or_byte(start, c2, /* ascii_only = */ true, '\\'');\n+        if !self.curr_is('\\'') {\n+            // Byte offsetting here is okay because the\n+            // character before position `start` are an\n+            // ascii single quote and ascii 'b'.\n+            let last_pos = self.last_pos;\n+            self.fatal_span_verbose(\n+                start - BytePos(2), last_pos,\n+                \"unterminated byte constant\".to_string());\n+        }\n+\n+        let id = if valid { self.name_from(start) } else { token::intern(\"??\") };\n+        self.bump(); // advance curr past token\n+        return token::LIT_BYTE(id);\n+    }\n+\n+    fn scan_byte_string(&mut self) -> token::Token {\n+        self.bump();\n+        let start = self.last_pos;\n+        let mut valid = true;\n+\n+        while !self.curr_is('\"') {\n+            if self.is_eof() {\n+                let last_pos = self.last_pos;\n+                self.fatal_span_(start, last_pos,\n+                                  \"unterminated double quote byte string\");\n+            }\n+\n+            let ch_start = self.last_pos;\n+            let ch = self.curr.unwrap();\n+            self.bump();\n+            valid &= self.scan_char_or_byte(ch_start, ch, /* ascii_only = */ true, '\"');\n+        }\n+        let id = if valid { self.name_from(start) } else { token::intern(\"??\") };\n+        self.bump();\n+        return token::LIT_BINARY(id);\n+    }\n+\n+    fn scan_raw_byte_string(&mut self) -> token::Token {\n+        let start_bpos = self.last_pos;\n+        self.bump();\n+        let mut hash_count = 0u;\n+        while self.curr_is('#') {\n+            self.bump();\n+            hash_count += 1;\n+        }\n+\n+        if self.is_eof() {\n+            let last_pos = self.last_pos;\n+            self.fatal_span_(start_bpos, last_pos, \"unterminated raw string\");\n+        } else if !self.curr_is('\"') {\n+            let last_pos = self.last_pos;\n+            let ch = self.curr.unwrap();\n+            self.fatal_span_char(start_bpos, last_pos,\n+                            \"only `#` is allowed in raw string delimitation; \\\n+                             found illegal character\",\n+                            ch);\n+        }\n+        self.bump();\n+        let content_start_bpos = self.last_pos;\n+        let mut content_end_bpos;\n+        'outer: loop {\n+            match self.curr {\n+                None => {\n+                    let last_pos = self.last_pos;\n+                    self.fatal_span_(start_bpos, last_pos, \"unterminated raw string\")\n+                },\n+                Some('\"') => {\n+                    content_end_bpos = self.last_pos;\n+                    for _ in range(0, hash_count) {\n+                        self.bump();\n+                        if !self.curr_is('#') {\n+                            continue 'outer;\n+                        }\n+                    }\n+                    break;\n+                },\n+                Some(c) => if c > '\\x7F' {\n+                    let last_pos = self.last_pos;\n+                    self.err_span_char(\n+                        last_pos, last_pos, \"raw byte string must be ASCII\", c);\n+                }\n+            }\n+            self.bump();\n+        }\n+        self.bump();\n+        return token::LIT_BINARY_RAW(self.name_from_to(content_start_bpos, content_end_bpos),\n+                                     hash_count);\n+    }\n }\n \n pub fn is_whitespace(c: Option<char>) -> bool {\n@@ -1239,12 +1264,18 @@ fn in_range(c: Option<char>, lo: char, hi: char) -> bool {\n \n fn is_dec_digit(c: Option<char>) -> bool { return in_range(c, '0', '9'); }\n \n-pub fn is_line_non_doc_comment(s: &str) -> bool {\n-    s.starts_with(\"////\")\n+pub fn is_doc_comment(s: &str) -> bool {\n+    let res = (s.starts_with(\"///\") && *s.as_bytes().get(3).unwrap_or(&b' ') != b'/')\n+              || s.starts_with(\"//!\");\n+    debug!(\"is `{}` a doc comment? {}\", s, res);\n+    res\n }\n \n-pub fn is_block_non_doc_comment(s: &str) -> bool {\n-    s.starts_with(\"/***\")\n+pub fn is_block_doc_comment(s: &str) -> bool {\n+    let res = (s.starts_with(\"/**\") && *s.as_bytes().get(3).unwrap_or(&b' ') != b'*')\n+              || s.starts_with(\"/*!\");\n+    debug!(\"is `{}` a doc comment? {}\", s, res);\n+    res\n }\n \n fn ident_start(c: Option<char>) -> bool {\n@@ -1295,11 +1326,14 @@ mod test {\n             \"/* my source file */ \\\n              fn main() { println!(\\\"zebra\\\"); }\\n\".to_string());\n         let id = str_to_ident(\"fn\");\n+        assert_eq!(string_reader.next_token().tok, token::COMMENT);\n+        assert_eq!(string_reader.next_token().tok, token::WS);\n         let tok1 = string_reader.next_token();\n         let tok2 = TokenAndSpan{\n             tok:token::IDENT(id, false),\n             sp:Span {lo:BytePos(21),hi:BytePos(23),expn_info: None}};\n         assert_eq!(tok1,tok2);\n+        assert_eq!(string_reader.next_token().tok, token::WS);\n         // the 'main' id is already read:\n         assert_eq!(string_reader.last_pos.clone(), BytePos(28));\n         // read another token:\n@@ -1328,6 +1362,7 @@ mod test {\n     #[test] fn doublecolonparsing () {\n         check_tokenization(setup(&mk_sh(), \"a b\".to_string()),\n                            vec!(mk_ident(\"a\",false),\n+                            token::WS,\n                              mk_ident(\"b\",false)));\n     }\n \n@@ -1341,6 +1376,7 @@ mod test {\n     #[test] fn dcparsing_3 () {\n         check_tokenization(setup(&mk_sh(), \"a ::b\".to_string()),\n                            vec!(mk_ident(\"a\",false),\n+                             token::WS,\n                              token::MOD_SEP,\n                              mk_ident(\"b\",false)));\n     }\n@@ -1349,22 +1385,23 @@ mod test {\n         check_tokenization(setup(&mk_sh(), \"a:: b\".to_string()),\n                            vec!(mk_ident(\"a\",true),\n                              token::MOD_SEP,\n+                             token::WS,\n                              mk_ident(\"b\",false)));\n     }\n \n     #[test] fn character_a() {\n         assert_eq!(setup(&mk_sh(), \"'a'\".to_string()).next_token().tok,\n-                   token::LIT_CHAR('a'));\n+                   token::LIT_CHAR(token::intern(\"a\")));\n     }\n \n     #[test] fn character_space() {\n         assert_eq!(setup(&mk_sh(), \"' '\".to_string()).next_token().tok,\n-                   token::LIT_CHAR(' '));\n+                   token::LIT_CHAR(token::intern(\" \")));\n     }\n \n     #[test] fn character_escaped() {\n         assert_eq!(setup(&mk_sh(), \"'\\\\n'\".to_string()).next_token().tok,\n-                   token::LIT_CHAR('\\n'));\n+                   token::LIT_CHAR(token::intern(\"\\\\n\")));\n     }\n \n     #[test] fn lifetime_name() {\n@@ -1376,19 +1413,23 @@ mod test {\n         assert_eq!(setup(&mk_sh(),\n                          \"r###\\\"\\\"#a\\\\b\\x00c\\\"\\\"###\".to_string()).next_token()\n                                                                  .tok,\n-                   token::LIT_STR_RAW(token::str_to_ident(\"\\\"#a\\\\b\\x00c\\\"\"), 3));\n+                   token::LIT_STR_RAW(token::intern(\"\\\"#a\\\\b\\x00c\\\"\"), 3));\n     }\n \n     #[test] fn line_doc_comments() {\n-        assert!(!is_line_non_doc_comment(\"///\"));\n-        assert!(!is_line_non_doc_comment(\"/// blah\"));\n-        assert!(is_line_non_doc_comment(\"////\"));\n+        assert!(is_doc_comment(\"///\"));\n+        assert!(is_doc_comment(\"/// blah\"));\n+        assert!(!is_doc_comment(\"////\"));\n     }\n \n     #[test] fn nested_block_comments() {\n-        assert_eq!(setup(&mk_sh(),\n-                         \"/* /* */ */'a'\".to_string()).next_token().tok,\n-                   token::LIT_CHAR('a'));\n+        let sh = mk_sh();\n+        let mut lexer = setup(&sh, \"/* /* */ */'a'\".to_string());\n+        match lexer.next_token().tok {\n+            token::COMMENT => { },\n+            _ => fail!(\"expected a comment!\")\n+        }\n+        assert_eq!(lexer.next_token().tok, token::LIT_CHAR(token::intern(\"a\")));\n     }\n \n }"}, {"sha": "37c84c95af654e3f98ce63060c94b1bd7161c4a1", "filename": "src/libsyntax/parse/mod.rs", "status": "modified", "additions": 347, "deletions": 7, "changes": 354, "blob_url": "https://github.com/rust-lang/rust/blob/ad3eda123a00b3ddf8897295ff26d97ba7c64d44/src%2Flibsyntax%2Fparse%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/ad3eda123a00b3ddf8897295ff26d97ba7c64d44/src%2Flibsyntax%2Fparse%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Fmod.rs?ref=ad3eda123a00b3ddf8897295ff26d97ba7c64d44", "patch": "@@ -10,7 +10,6 @@\n \n //! The main parser interface\n \n-\n use ast;\n use codemap::{Span, CodeMap, FileMap};\n use diagnostic::{SpanHandler, mk_span_handler, default_handler, Auto};\n@@ -32,7 +31,7 @@ pub mod common;\n pub mod classify;\n pub mod obsolete;\n \n-// info about a parsing session.\n+/// Info about a parsing session.\n pub struct ParseSess {\n     pub span_diagnostic: SpanHandler, // better be the same as the one in the reader!\n     /// Used to determine and report recursive mod inclusions\n@@ -241,14 +240,14 @@ pub fn file_to_filemap(sess: &ParseSess, path: &Path, spanopt: Option<Span>)\n     unreachable!()\n }\n \n-// given a session and a string, add the string to\n-// the session's codemap and return the new filemap\n+/// Given a session and a string, add the string to\n+/// the session's codemap and return the new filemap\n pub fn string_to_filemap(sess: &ParseSess, source: String, path: String)\n                          -> Rc<FileMap> {\n     sess.span_diagnostic.cm.new_filemap(path, source)\n }\n \n-// given a filemap, produce a sequence of token-trees\n+/// Given a filemap, produce a sequence of token-trees\n pub fn filemap_to_tts(sess: &ParseSess, filemap: Rc<FileMap>)\n     -> Vec<ast::TokenTree> {\n     // it appears to me that the cfg doesn't matter here... indeed,\n@@ -259,21 +258,362 @@ pub fn filemap_to_tts(sess: &ParseSess, filemap: Rc<FileMap>)\n     p1.parse_all_token_trees()\n }\n \n-// given tts and cfg, produce a parser\n+/// Given tts and cfg, produce a parser\n pub fn tts_to_parser<'a>(sess: &'a ParseSess,\n                          tts: Vec<ast::TokenTree>,\n                          cfg: ast::CrateConfig) -> Parser<'a> {\n     let trdr = lexer::new_tt_reader(&sess.span_diagnostic, None, tts);\n     Parser::new(sess, cfg, box trdr)\n }\n \n-// abort if necessary\n+/// Abort if necessary\n pub fn maybe_aborted<T>(result: T, mut p: Parser) -> T {\n     p.abort_if_errors();\n     result\n }\n \n+/// Parse a string representing a character literal into its final form.\n+/// Rather than just accepting/rejecting a given literal, unescapes it as\n+/// well. Can take any slice prefixed by a character escape. Returns the\n+/// character and the number of characters consumed.\n+pub fn char_lit(lit: &str) -> (char, int) {\n+    use std::{num, char};\n+\n+    let mut chars = lit.chars();\n+    let c = match (chars.next(), chars.next()) {\n+        (Some(c), None) if c != '\\\\' => return (c, 1),\n+        (Some('\\\\'), Some(c)) => match c {\n+            '\"' => Some('\"'),\n+            'n' => Some('\\n'),\n+            'r' => Some('\\r'),\n+            't' => Some('\\t'),\n+            '\\\\' => Some('\\\\'),\n+            '\\'' => Some('\\''),\n+            '0' => Some('\\0'),\n+            _ => { None }\n+        },\n+        _ => fail!(\"lexer accepted invalid char escape `{}`\", lit)\n+    };\n+\n+    match c {\n+        Some(x) => return (x, 2),\n+        None => { }\n+    }\n+\n+    let msg = format!(\"lexer should have rejected a bad character escape {}\", lit);\n+    let msg2 = msg.as_slice();\n+\n+    let esc: |uint| -> Option<(char, int)> = |len|\n+        num::from_str_radix(lit.slice(2, len), 16)\n+        .and_then(char::from_u32)\n+        .map(|x| (x, len as int));\n+\n+    // Unicode escapes\n+    return match lit.as_bytes()[1] as char {\n+        'x' | 'X' => esc(4),\n+        'u' => esc(6),\n+        'U' => esc(10),\n+        _ => None,\n+    }.expect(msg2);\n+}\n+\n+/// Parse a string representing a string literal into its final form. Does\n+/// unescaping.\n+pub fn str_lit(lit: &str) -> String {\n+    debug!(\"parse_str_lit: given {}\", lit.escape_default());\n+    let mut res = String::with_capacity(lit.len());\n+\n+    // FIXME #8372: This could be a for-loop if it didn't borrow the iterator\n+    let error = |i| format!(\"lexer should have rejected {} at {}\", lit, i);\n+\n+    /// Eat everything up to a non-whitespace\n+    fn eat<'a>(it: &mut ::std::iter::Peekable<(uint, char), ::std::str::CharOffsets<'a>>) {\n+        loop {\n+            match it.peek().map(|x| x.val1()) {\n+                Some(' ') | Some('\\n') | Some('\\r') | Some('\\t') => {\n+                    it.next();\n+                },\n+                _ => { break; }\n+            }\n+        }\n+    }\n+\n+    let mut chars = lit.char_indices().peekable();\n+    loop {\n+        match chars.next() {\n+            Some((i, c)) => {\n+                let em = error(i);\n+                match c {\n+                    '\\\\' => {\n+                        if chars.peek().expect(em.as_slice()).val1() == '\\n' {\n+                            eat(&mut chars);\n+                        } else if chars.peek().expect(em.as_slice()).val1() == '\\r' {\n+                            chars.next();\n+                            if chars.peek().expect(em.as_slice()).val1() != '\\n' {\n+                                fail!(\"lexer accepted bare CR\");\n+                            }\n+                            eat(&mut chars);\n+                        } else {\n+                            // otherwise, a normal escape\n+                            let (c, n) = char_lit(lit.slice_from(i));\n+                            for _ in range(0, n - 1) { // we don't need to move past the first \\\n+                                chars.next();\n+                            }\n+                            res.push_char(c);\n+                        }\n+                    },\n+                    '\\r' => {\n+                        if chars.peek().expect(em.as_slice()).val1() != '\\n' {\n+                            fail!(\"lexer accepted bare CR\");\n+                        }\n+                        chars.next();\n+                        res.push_char('\\n');\n+                    }\n+                    c => res.push_char(c),\n+                }\n+            },\n+            None => break\n+        }\n+    }\n+\n+    res.shrink_to_fit(); // probably not going to do anything, unless there was an escape.\n+    debug!(\"parse_str_lit: returning {}\", res);\n+    res\n+}\n+\n+/// Parse a string representing a raw string literal into its final form. The\n+/// only operation this does is convert embedded CRLF into a single LF.\n+pub fn raw_str_lit(lit: &str) -> String {\n+    debug!(\"raw_str_lit: given {}\", lit.escape_default());\n+    let mut res = String::with_capacity(lit.len());\n+\n+    // FIXME #8372: This could be a for-loop if it didn't borrow the iterator\n+    let mut chars = lit.chars().peekable();\n+    loop {\n+        match chars.next() {\n+            Some(c) => {\n+                if c == '\\r' {\n+                    if *chars.peek().unwrap() != '\\n' {\n+                        fail!(\"lexer accepted bare CR\");\n+                    }\n+                    chars.next();\n+                    res.push_char('\\n');\n+                } else {\n+                    res.push_char(c);\n+                }\n+            },\n+            None => break\n+        }\n+    }\n+\n+    res.shrink_to_fit();\n+    res\n+}\n+\n+pub fn float_lit(s: &str) -> ast::Lit_ {\n+    debug!(\"float_lit: {}\", s);\n+    // FIXME #2252: bounds checking float literals is defered until trans\n+    let s2 = s.chars().filter(|&c| c != '_').collect::<String>();\n+    let s = s2.as_slice();\n+\n+    let mut ty = None;\n+\n+    if s.ends_with(\"f32\") {\n+        ty = Some(ast::TyF32);\n+    } else if s.ends_with(\"f64\") {\n+        ty = Some(ast::TyF64);\n+    }\n+\n+\n+    match ty {\n+        Some(t) => {\n+            ast::LitFloat(token::intern_and_get_ident(s.slice_to(s.len() - t.suffix_len())), t)\n+        },\n+        None => ast::LitFloatUnsuffixed(token::intern_and_get_ident(s))\n+    }\n+}\n+\n+/// Parse a string representing a byte literal into its final form. Similar to `char_lit`\n+pub fn byte_lit(lit: &str) -> (u8, uint) {\n+    let err = |i| format!(\"lexer accepted invalid byte literal {} step {}\", lit, i);\n+\n+    if lit.len() == 1 {\n+        (lit.as_bytes()[0], 1)\n+    } else {\n+        assert!(lit.as_bytes()[0] == b'\\\\', err(0i));\n+        let b = match lit.as_bytes()[1] {\n+            b'\"' => b'\"',\n+            b'n' => b'\\n',\n+            b'r' => b'\\r',\n+            b't' => b'\\t',\n+            b'\\\\' => b'\\\\',\n+            b'\\'' => b'\\'',\n+            b'0' => b'\\0',\n+            _ => {\n+                match ::std::num::from_str_radix::<u64>(lit.slice(2, 4), 16) {\n+                    Some(c) =>\n+                        if c > 0xFF {\n+                            fail!(err(2))\n+                        } else {\n+                            return (c as u8, 4)\n+                        },\n+                    None => fail!(err(3))\n+                }\n+            }\n+        };\n+        return (b, 2);\n+    }\n+}\n+\n+pub fn binary_lit(lit: &str) -> Rc<Vec<u8>> {\n+    let mut res = Vec::with_capacity(lit.len());\n+\n+    // FIXME #8372: This could be a for-loop if it didn't borrow the iterator\n+    let error = |i| format!(\"lexer should have rejected {} at {}\", lit, i);\n+\n+    // binary literals *must* be ASCII, but the escapes don't have to be\n+    let mut chars = lit.as_bytes().iter().enumerate().peekable();\n+    loop {\n+        match chars.next() {\n+            Some((i, &c)) => {\n+                if c == b'\\\\' {\n+                    if *chars.peek().expect(error(i).as_slice()).val1() == b'\\n' {\n+                        loop {\n+                            // eat everything up to a non-whitespace\n+                            match chars.peek().map(|x| *x.val1()) {\n+                                Some(b' ') | Some(b'\\n') | Some(b'\\r') | Some(b'\\t') => {\n+                                    chars.next();\n+                                },\n+                                _ => { break; }\n+                            }\n+                        }\n+                    } else {\n+                        // otherwise, a normal escape\n+                        let (c, n) = byte_lit(lit.slice_from(i));\n+                        for _ in range(0, n - 1) { // we don't need to move past the first \\\n+                            chars.next();\n+                        }\n+                        res.push(c);\n+                    }\n+                } else {\n+                    res.push(c);\n+                }\n+            },\n+            None => { break; }\n+        }\n+    }\n+\n+    Rc::new(res)\n+}\n+\n+pub fn integer_lit(s: &str, sd: &SpanHandler, sp: Span) -> ast::Lit_ {\n+    // s can only be ascii, byte indexing is fine\n+\n+    let s2 = s.chars().filter(|&c| c != '_').collect::<String>();\n+    let mut s = s2.as_slice();\n+\n+    debug!(\"parse_integer_lit: {}\", s);\n+\n+    if s.len() == 1 {\n+        return ast::LitIntUnsuffixed((s.char_at(0)).to_digit(10).unwrap() as i64);\n+    }\n \n+    let mut base = 10;\n+    let orig = s;\n+\n+    #[deriving(Show)]\n+    enum Result {\n+        Nothing,\n+        Signed(ast::IntTy),\n+        Unsigned(ast::UintTy)\n+    }\n+\n+    impl Result {\n+        fn suffix_len(&self) -> uint {\n+            match *self {\n+                Nothing => 0,\n+                Signed(s) => s.suffix_len(),\n+                Unsigned(u) => u.suffix_len()\n+            }\n+        }\n+    }\n+\n+    let mut ty = Nothing;\n+\n+\n+    if s.char_at(0) == '0' {\n+        match s.char_at(1) {\n+            'x' => base = 16,\n+            'o' => base = 8,\n+            'b' => base = 2,\n+            _ => { }\n+        }\n+    }\n+\n+    if base != 10 {\n+        s = s.slice_from(2);\n+    }\n+\n+    let last = s.len() - 1;\n+    match s.char_at(last) {\n+        'i' => ty = Signed(ast::TyI),\n+        'u' => ty = Unsigned(ast::TyU),\n+        '8' => {\n+            if s.len() > 2 {\n+                match s.char_at(last - 1) {\n+                    'i' => ty = Signed(ast::TyI8),\n+                    'u' => ty = Unsigned(ast::TyU8),\n+                    _ => { }\n+                }\n+            }\n+        },\n+        '6' => {\n+            if s.len() > 3 && s.char_at(last - 1) == '1' {\n+                match s.char_at(last - 2) {\n+                    'i' => ty = Signed(ast::TyI16),\n+                    'u' => ty = Unsigned(ast::TyU16),\n+                    _ => { }\n+                }\n+            }\n+        },\n+        '2' => {\n+            if s.len() > 3 && s.char_at(last - 1) == '3' {\n+                match s.char_at(last - 2) {\n+                    'i' => ty = Signed(ast::TyI32),\n+                    'u' => ty = Unsigned(ast::TyU32),\n+                    _ => { }\n+                }\n+            }\n+        },\n+        '4' => {\n+            if s.len() > 3 && s.char_at(last - 1) == '6' {\n+                match s.char_at(last - 2) {\n+                    'i' => ty = Signed(ast::TyI64),\n+                    'u' => ty = Unsigned(ast::TyU64),\n+                    _ => { }\n+                }\n+            }\n+        },\n+        _ => { }\n+    }\n+\n+\n+    s = s.slice_to(s.len() - ty.suffix_len());\n+\n+    debug!(\"The suffix is {}, base {}, the new string is {}, the original \\\n+           string was {}\", ty, base, s, orig);\n+\n+    let res: u64 = match ::std::num::from_str_radix(s, base) {\n+        Some(r) => r,\n+        None => { sd.span_err(sp, \"int literal is too large\"); 0 }\n+    };\n+\n+    match ty {\n+        Nothing => ast::LitIntUnsuffixed(res as i64),\n+        Signed(t) => ast::LitInt(res as i64, t),\n+        Unsigned(t) => ast::LitUint(res, t)\n+    }\n+}\n \n #[cfg(test)]\n mod test {"}, {"sha": "cadae7ef12f8078b38dfc67a6dde7aed99d7000f", "filename": "src/libsyntax/parse/obsolete.rs", "status": "modified", "additions": 4, "deletions": 4, "changes": 8, "blob_url": "https://github.com/rust-lang/rust/blob/ad3eda123a00b3ddf8897295ff26d97ba7c64d44/src%2Flibsyntax%2Fparse%2Fobsolete.rs", "raw_url": "https://github.com/rust-lang/rust/raw/ad3eda123a00b3ddf8897295ff26d97ba7c64d44/src%2Flibsyntax%2Fparse%2Fobsolete.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Fobsolete.rs?ref=ad3eda123a00b3ddf8897295ff26d97ba7c64d44", "patch": "@@ -38,8 +38,8 @@ pub enum ObsoleteSyntax {\n pub trait ParserObsoleteMethods {\n     /// Reports an obsolete syntax non-fatal error.\n     fn obsolete(&mut self, sp: Span, kind: ObsoleteSyntax);\n-    // Reports an obsolete syntax non-fatal error, and returns\n-    // a placeholder expression\n+    /// Reports an obsolete syntax non-fatal error, and returns\n+    /// a placeholder expression\n     fn obsolete_expr(&mut self, sp: Span, kind: ObsoleteSyntax) -> Gc<Expr>;\n     fn report(&mut self,\n               sp: Span,\n@@ -83,8 +83,8 @@ impl<'a> ParserObsoleteMethods for parser::Parser<'a> {\n         self.report(sp, kind, kind_str, desc);\n     }\n \n-    // Reports an obsolete syntax non-fatal error, and returns\n-    // a placeholder expression\n+    /// Reports an obsolete syntax non-fatal error, and returns\n+    /// a placeholder expression\n     fn obsolete_expr(&mut self, sp: Span, kind: ObsoleteSyntax) -> Gc<Expr> {\n         self.obsolete(sp, kind);\n         self.mk_expr(sp.lo, sp.hi, ExprLit(box(GC) respan(sp, LitNil)))"}, {"sha": "743eeed9da5e247c4dc8fe77a6e0f94e8b993180", "filename": "src/libsyntax/parse/parser.rs", "status": "modified", "additions": 268, "deletions": 249, "changes": 517, "blob_url": "https://github.com/rust-lang/rust/blob/ad3eda123a00b3ddf8897295ff26d97ba7c64d44/src%2Flibsyntax%2Fparse%2Fparser.rs", "raw_url": "https://github.com/rust-lang/rust/raw/ad3eda123a00b3ddf8897295ff26d97ba7c64d44/src%2Flibsyntax%2Fparse%2Fparser.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Fparser.rs?ref=ad3eda123a00b3ddf8897295ff26d97ba7c64d44", "patch": "@@ -33,8 +33,8 @@ use ast::{ForeignItem, ForeignItemStatic, ForeignItemFn, ForeignMod};\n use ast::{Ident, NormalFn, Inherited, Item, Item_, ItemStatic};\n use ast::{ItemEnum, ItemFn, ItemForeignMod, ItemImpl};\n use ast::{ItemMac, ItemMod, ItemStruct, ItemTrait, ItemTy, Lit, Lit_};\n-use ast::{LitBool, LitFloat, LitFloatUnsuffixed, LitInt, LitChar, LitByte, LitBinary};\n-use ast::{LitIntUnsuffixed, LitNil, LitStr, LitUint, Local, LocalLet};\n+use ast::{LitBool, LitChar, LitByte, LitBinary};\n+use ast::{LitNil, LitStr, LitUint, Local, LocalLet};\n use ast::{MutImmutable, MutMutable, Mac_, MacInvocTT, Matcher, MatchNonterminal};\n use ast::{MatchSeq, MatchTok, Method, MutTy, BiMul, Mutability};\n use ast::{NamedField, UnNeg, NoReturn, UnNot, P, Pat, PatEnum};\n@@ -61,6 +61,7 @@ use ast_util::{as_prec, ident_to_path, lit_is_str, operator_prec};\n use ast_util;\n use codemap::{Span, BytePos, Spanned, spanned, mk_sp};\n use codemap;\n+use parse;\n use parse::attr::ParserAttr;\n use parse::classify;\n use parse::common::{SeqSep, seq_sep_none};\n@@ -117,21 +118,21 @@ pub struct PathAndBounds {\n }\n \n enum ItemOrViewItem {\n-    // Indicates a failure to parse any kind of item. The attributes are\n-    // returned.\n+    /// Indicates a failure to parse any kind of item. The attributes are\n+    /// returned.\n     IoviNone(Vec<Attribute>),\n     IoviItem(Gc<Item>),\n     IoviForeignItem(Gc<ForeignItem>),\n     IoviViewItem(ViewItem)\n }\n \n \n-// Possibly accept an `INTERPOLATED` expression (a pre-parsed expression\n-// dropped into the token stream, which happens while parsing the\n-// result of macro expansion)\n-/* Placement of these is not as complex as I feared it would be.\n-The important thing is to make sure that lookahead doesn't balk\n-at INTERPOLATED tokens */\n+/// Possibly accept an `INTERPOLATED` expression (a pre-parsed expression\n+/// dropped into the token stream, which happens while parsing the\n+/// result of macro expansion)\n+/// Placement of these is not as complex as I feared it would be.\n+/// The important thing is to make sure that lookahead doesn't balk\n+/// at INTERPOLATED tokens\n macro_rules! maybe_whole_expr (\n     ($p:expr) => (\n         {\n@@ -166,7 +167,7 @@ macro_rules! maybe_whole_expr (\n     )\n )\n \n-// As above, but for things other than expressions\n+/// As maybe_whole_expr, but for things other than expressions\n macro_rules! maybe_whole (\n     ($p:expr, $constructor:ident) => (\n         {\n@@ -287,14 +288,14 @@ struct ParsedItemsAndViewItems {\n \n pub struct Parser<'a> {\n     pub sess: &'a ParseSess,\n-    // the current token:\n+    /// the current token:\n     pub token: token::Token,\n-    // the span of the current token:\n+    /// the span of the current token:\n     pub span: Span,\n-    // the span of the prior token:\n+    /// the span of the prior token:\n     pub last_span: Span,\n     pub cfg: CrateConfig,\n-    // the previous token or None (only stashed sometimes).\n+    /// the previous token or None (only stashed sometimes).\n     pub last_token: Option<Box<token::Token>>,\n     pub buffer: [TokenAndSpan, ..4],\n     pub buffer_start: int,\n@@ -324,10 +325,24 @@ fn is_plain_ident_or_underscore(t: &token::Token) -> bool {\n     is_plain_ident(t) || *t == token::UNDERSCORE\n }\n \n+/// Get a token the parser cares about\n+fn real_token(rdr: &mut Reader) -> TokenAndSpan {\n+    let mut t = rdr.next_token();\n+    loop {\n+        match t.tok {\n+            token::WS | token::COMMENT | token::SHEBANG(_) => {\n+                t = rdr.next_token();\n+            },\n+            _ => break\n+        }\n+    }\n+    t\n+}\n+\n impl<'a> Parser<'a> {\n     pub fn new(sess: &'a ParseSess, cfg: ast::CrateConfig,\n                mut rdr: Box<Reader>) -> Parser<'a> {\n-        let tok0 = rdr.next_token();\n+        let tok0 = real_token(rdr);\n         let span = tok0.sp;\n         let placeholder = TokenAndSpan {\n             tok: token::UNDERSCORE,\n@@ -361,12 +376,13 @@ impl<'a> Parser<'a> {\n             root_module_name: None,\n         }\n     }\n-    // convert a token to a string using self's reader\n+\n+    /// Convert a token to a string using self's reader\n     pub fn token_to_string(token: &token::Token) -> String {\n         token::to_string(token)\n     }\n \n-    // convert the current token to a string using self's reader\n+    /// Convert the current token to a string using self's reader\n     pub fn this_token_to_string(&mut self) -> String {\n         Parser::token_to_string(&self.token)\n     }\n@@ -383,8 +399,8 @@ impl<'a> Parser<'a> {\n         self.fatal(format!(\"unexpected token: `{}`\", this_token).as_slice());\n     }\n \n-    // expect and consume the token t. Signal an error if\n-    // the next token is not t.\n+    /// Expect and consume the token t. Signal an error if\n+    /// the next token is not t.\n     pub fn expect(&mut self, t: &token::Token) {\n         if self.token == *t {\n             self.bump();\n@@ -397,9 +413,9 @@ impl<'a> Parser<'a> {\n         }\n     }\n \n-    // Expect next token to be edible or inedible token.  If edible,\n-    // then consume it; if inedible, then return without consuming\n-    // anything.  Signal a fatal error if next token is unexpected.\n+    /// Expect next token to be edible or inedible token.  If edible,\n+    /// then consume it; if inedible, then return without consuming\n+    /// anything.  Signal a fatal error if next token is unexpected.\n     pub fn expect_one_of(&mut self,\n                          edible: &[token::Token],\n                          inedible: &[token::Token]) {\n@@ -437,9 +453,9 @@ impl<'a> Parser<'a> {\n         }\n     }\n \n-    // Check for erroneous `ident { }`; if matches, signal error and\n-    // recover (without consuming any expected input token).  Returns\n-    // true if and only if input was consumed for recovery.\n+    /// Check for erroneous `ident { }`; if matches, signal error and\n+    /// recover (without consuming any expected input token).  Returns\n+    /// true if and only if input was consumed for recovery.\n     pub fn check_for_erroneous_unit_struct_expecting(&mut self, expected: &[token::Token]) -> bool {\n         if self.token == token::LBRACE\n             && expected.iter().all(|t| *t != token::LBRACE)\n@@ -456,9 +472,9 @@ impl<'a> Parser<'a> {\n         }\n     }\n \n-    // Commit to parsing a complete expression `e` expected to be\n-    // followed by some token from the set edible + inedible.  Recover\n-    // from anticipated input errors, discarding erroneous characters.\n+    /// Commit to parsing a complete expression `e` expected to be\n+    /// followed by some token from the set edible + inedible.  Recover\n+    /// from anticipated input errors, discarding erroneous characters.\n     pub fn commit_expr(&mut self, e: Gc<Expr>, edible: &[token::Token],\n                        inedible: &[token::Token]) {\n         debug!(\"commit_expr {:?}\", e);\n@@ -479,9 +495,9 @@ impl<'a> Parser<'a> {\n         self.commit_expr(e, &[edible], &[])\n     }\n \n-    // Commit to parsing a complete statement `s`, which expects to be\n-    // followed by some token from the set edible + inedible.  Check\n-    // for recoverable input errors, discarding erroneous characters.\n+    /// Commit to parsing a complete statement `s`, which expects to be\n+    /// followed by some token from the set edible + inedible.  Check\n+    /// for recoverable input errors, discarding erroneous characters.\n     pub fn commit_stmt(&mut self, s: Gc<Stmt>, edible: &[token::Token],\n                        inedible: &[token::Token]) {\n         debug!(\"commit_stmt {:?}\", s);\n@@ -526,8 +542,8 @@ impl<'a> Parser<'a> {\n                                               id: ast::DUMMY_NODE_ID })\n     }\n \n-    // consume token 'tok' if it exists. Returns true if the given\n-    // token was present, false otherwise.\n+    /// Consume token 'tok' if it exists. Returns true if the given\n+    /// token was present, false otherwise.\n     pub fn eat(&mut self, tok: &token::Token) -> bool {\n         let is_present = self.token == *tok;\n         if is_present { self.bump() }\n@@ -538,8 +554,8 @@ impl<'a> Parser<'a> {\n         token::is_keyword(kw, &self.token)\n     }\n \n-    // if the next token is the given keyword, eat it and return\n-    // true. Otherwise, return false.\n+    /// If the next token is the given keyword, eat it and return\n+    /// true. Otherwise, return false.\n     pub fn eat_keyword(&mut self, kw: keywords::Keyword) -> bool {\n         match self.token {\n             token::IDENT(sid, false) if kw.to_name() == sid.name => {\n@@ -550,9 +566,9 @@ impl<'a> Parser<'a> {\n         }\n     }\n \n-    // if the given word is not a keyword, signal an error.\n-    // if the next token is not the given word, signal an error.\n-    // otherwise, eat it.\n+    /// If the given word is not a keyword, signal an error.\n+    /// If the next token is not the given word, signal an error.\n+    /// Otherwise, eat it.\n     pub fn expect_keyword(&mut self, kw: keywords::Keyword) {\n         if !self.eat_keyword(kw) {\n             let id_interned_str = token::get_name(kw.to_name());\n@@ -562,7 +578,7 @@ impl<'a> Parser<'a> {\n         }\n     }\n \n-    // signal an error if the given string is a strict keyword\n+    /// Signal an error if the given string is a strict keyword\n     pub fn check_strict_keywords(&mut self) {\n         if token::is_strict_keyword(&self.token) {\n             let token_str = self.this_token_to_string();\n@@ -573,7 +589,7 @@ impl<'a> Parser<'a> {\n         }\n     }\n \n-    // signal an error if the current token is a reserved keyword\n+    /// Signal an error if the current token is a reserved keyword\n     pub fn check_reserved_keywords(&mut self) {\n         if token::is_reserved_keyword(&self.token) {\n             let token_str = self.this_token_to_string();\n@@ -582,8 +598,8 @@ impl<'a> Parser<'a> {\n         }\n     }\n \n-    // Expect and consume an `&`. If `&&` is seen, replace it with a single\n-    // `&` and continue. If an `&` is not seen, signal an error.\n+    /// Expect and consume an `&`. If `&&` is seen, replace it with a single\n+    /// `&` and continue. If an `&` is not seen, signal an error.\n     fn expect_and(&mut self) {\n         match self.token {\n             token::BINOP(token::AND) => self.bump(),\n@@ -603,8 +619,8 @@ impl<'a> Parser<'a> {\n         }\n     }\n \n-    // Expect and consume a `|`. If `||` is seen, replace it with a single\n-    // `|` and continue. If a `|` is not seen, signal an error.\n+    /// Expect and consume a `|`. If `||` is seen, replace it with a single\n+    /// `|` and continue. If a `|` is not seen, signal an error.\n     fn expect_or(&mut self) {\n         match self.token {\n             token::BINOP(token::OR) => self.bump(),\n@@ -624,26 +640,26 @@ impl<'a> Parser<'a> {\n         }\n     }\n \n-    // Attempt to consume a `<`. If `<<` is seen, replace it with a single\n-    // `<` and continue. If a `<` is not seen, return false.\n-    //\n-    // This is meant to be used when parsing generics on a path to get the\n-    // starting token. The `force` parameter is used to forcefully break up a\n-    // `<<` token. If `force` is false, then `<<` is only broken when a lifetime\n-    // shows up next. For example, consider the expression:\n-    //\n-    //      foo as bar << test\n-    //\n-    // The parser needs to know if `bar <<` is the start of a generic path or if\n-    // it's a left-shift token. If `test` were a lifetime, then it's impossible\n-    // for the token to be a left-shift, but if it's not a lifetime, then it's\n-    // considered a left-shift.\n-    //\n-    // The reason for this is that the only current ambiguity with `<<` is when\n-    // parsing closure types:\n-    //\n-    //      foo::<<'a> ||>();\n-    //      impl Foo<<'a> ||>() { ... }\n+    /// Attempt to consume a `<`. If `<<` is seen, replace it with a single\n+    /// `<` and continue. If a `<` is not seen, return false.\n+    ///\n+    /// This is meant to be used when parsing generics on a path to get the\n+    /// starting token. The `force` parameter is used to forcefully break up a\n+    /// `<<` token. If `force` is false, then `<<` is only broken when a lifetime\n+    /// shows up next. For example, consider the expression:\n+    ///\n+    ///      foo as bar << test\n+    ///\n+    /// The parser needs to know if `bar <<` is the start of a generic path or if\n+    /// it's a left-shift token. If `test` were a lifetime, then it's impossible\n+    /// for the token to be a left-shift, but if it's not a lifetime, then it's\n+    /// considered a left-shift.\n+    ///\n+    /// The reason for this is that the only current ambiguity with `<<` is when\n+    /// parsing closure types:\n+    ///\n+    ///      foo::<<'a> ||>();\n+    ///      impl Foo<<'a> ||>() { ... }\n     fn eat_lt(&mut self, force: bool) -> bool {\n         match self.token {\n             token::LT => { self.bump(); true }\n@@ -675,7 +691,7 @@ impl<'a> Parser<'a> {\n         }\n     }\n \n-    // Parse a sequence bracketed by `|` and `|`, stopping before the `|`.\n+    /// Parse a sequence bracketed by `|` and `|`, stopping before the `|`.\n     fn parse_seq_to_before_or<T>(\n                               &mut self,\n                               sep: &token::Token,\n@@ -696,9 +712,9 @@ impl<'a> Parser<'a> {\n         vector\n     }\n \n-    // expect and consume a GT. if a >> is seen, replace it\n-    // with a single > and continue. If a GT is not seen,\n-    // signal an error.\n+    /// Expect and consume a GT. if a >> is seen, replace it\n+    /// with a single > and continue. If a GT is not seen,\n+    /// signal an error.\n     pub fn expect_gt(&mut self) {\n         match self.token {\n             token::GT => self.bump(),\n@@ -727,8 +743,8 @@ impl<'a> Parser<'a> {\n         }\n     }\n \n-    // parse a sequence bracketed by '<' and '>', stopping\n-    // before the '>'.\n+    /// Parse a sequence bracketed by '<' and '>', stopping\n+    /// before the '>'.\n     pub fn parse_seq_to_before_gt<T>(\n                                   &mut self,\n                                   sep: Option<token::Token>,\n@@ -762,9 +778,9 @@ impl<'a> Parser<'a> {\n         return v;\n     }\n \n-    // parse a sequence, including the closing delimiter. The function\n-    // f must consume tokens until reaching the next separator or\n-    // closing bracket.\n+    /// Parse a sequence, including the closing delimiter. The function\n+    /// f must consume tokens until reaching the next separator or\n+    /// closing bracket.\n     pub fn parse_seq_to_end<T>(\n                             &mut self,\n                             ket: &token::Token,\n@@ -776,9 +792,9 @@ impl<'a> Parser<'a> {\n         val\n     }\n \n-    // parse a sequence, not including the closing delimiter. The function\n-    // f must consume tokens until reaching the next separator or\n-    // closing bracket.\n+    /// Parse a sequence, not including the closing delimiter. The function\n+    /// f must consume tokens until reaching the next separator or\n+    /// closing bracket.\n     pub fn parse_seq_to_before_end<T>(\n                                    &mut self,\n                                    ket: &token::Token,\n@@ -801,9 +817,9 @@ impl<'a> Parser<'a> {\n         return v;\n     }\n \n-    // parse a sequence, including the closing delimiter. The function\n-    // f must consume tokens until reaching the next separator or\n-    // closing bracket.\n+    /// Parse a sequence, including the closing delimiter. The function\n+    /// f must consume tokens until reaching the next separator or\n+    /// closing bracket.\n     pub fn parse_unspanned_seq<T>(\n                                &mut self,\n                                bra: &token::Token,\n@@ -817,8 +833,8 @@ impl<'a> Parser<'a> {\n         result\n     }\n \n-    // parse a sequence parameter of enum variant. For consistency purposes,\n-    // these should not be empty.\n+    /// Parse a sequence parameter of enum variant. For consistency purposes,\n+    /// these should not be empty.\n     pub fn parse_enum_variant_seq<T>(\n                                &mut self,\n                                bra: &token::Token,\n@@ -852,7 +868,7 @@ impl<'a> Parser<'a> {\n         spanned(lo, hi, result)\n     }\n \n-    // advance the parser by one token\n+    /// Advance the parser by one token\n     pub fn bump(&mut self) {\n         self.last_span = self.span;\n         // Stash token for error recovery (sometimes; clone is not necessarily cheap).\n@@ -862,7 +878,7 @@ impl<'a> Parser<'a> {\n             None\n         };\n         let next = if self.buffer_start == self.buffer_end {\n-            self.reader.next_token()\n+            real_token(self.reader)\n         } else {\n             // Avoid token copies with `replace`.\n             let buffer_start = self.buffer_start as uint;\n@@ -880,14 +896,14 @@ impl<'a> Parser<'a> {\n         self.tokens_consumed += 1u;\n     }\n \n-    // Advance the parser by one token and return the bumped token.\n+    /// Advance the parser by one token and return the bumped token.\n     pub fn bump_and_get(&mut self) -> token::Token {\n         let old_token = replace(&mut self.token, token::UNDERSCORE);\n         self.bump();\n         old_token\n     }\n \n-    // EFFECT: replace the current token and span with the given one\n+    /// EFFECT: replace the current token and span with the given one\n     pub fn replace_token(&mut self,\n                          next: token::Token,\n                          lo: BytePos,\n@@ -906,7 +922,7 @@ impl<'a> Parser<'a> {\n                       -> R {\n         let dist = distance as int;\n         while self.buffer_length() < dist {\n-            self.buffer[self.buffer_end as uint] = self.reader.next_token();\n+            self.buffer[self.buffer_end as uint] = real_token(self.reader);\n             self.buffer_end = (self.buffer_end + 1) & 3;\n         }\n         f(&self.buffer[((self.buffer_start + dist - 1) & 3) as uint].tok)\n@@ -940,8 +956,8 @@ impl<'a> Parser<'a> {\n         token::get_ident(id)\n     }\n \n-    // Is the current token one of the keywords that signals a bare function\n-    // type?\n+    /// Is the current token one of the keywords that signals a bare function\n+    /// type?\n     pub fn token_is_bare_fn_keyword(&mut self) -> bool {\n         if token::is_keyword(keywords::Fn, &self.token) {\n             return true\n@@ -955,14 +971,14 @@ impl<'a> Parser<'a> {\n         false\n     }\n \n-    // Is the current token one of the keywords that signals a closure type?\n+    /// Is the current token one of the keywords that signals a closure type?\n     pub fn token_is_closure_keyword(&mut self) -> bool {\n         token::is_keyword(keywords::Unsafe, &self.token) ||\n             token::is_keyword(keywords::Once, &self.token)\n     }\n \n-    // Is the current token one of the keywords that signals an old-style\n-    // closure type (with explicit sigil)?\n+    /// Is the current token one of the keywords that signals an old-style\n+    /// closure type (with explicit sigil)?\n     pub fn token_is_old_style_closure_keyword(&mut self) -> bool {\n         token::is_keyword(keywords::Unsafe, &self.token) ||\n             token::is_keyword(keywords::Once, &self.token) ||\n@@ -983,7 +999,7 @@ impl<'a> Parser<'a> {\n         }\n     }\n \n-    // parse a TyBareFn type:\n+    /// parse a TyBareFn type:\n     pub fn parse_ty_bare_fn(&mut self) -> Ty_ {\n         /*\n \n@@ -1014,8 +1030,8 @@ impl<'a> Parser<'a> {\n         });\n     }\n \n-    // Parses a procedure type (`proc`). The initial `proc` keyword must\n-    // already have been parsed.\n+    /// Parses a procedure type (`proc`). The initial `proc` keyword must\n+    /// already have been parsed.\n     pub fn parse_proc_type(&mut self) -> Ty_ {\n         /*\n \n@@ -1063,7 +1079,7 @@ impl<'a> Parser<'a> {\n         })\n     }\n \n-    // parse a TyClosure type\n+    /// Parse a TyClosure type\n     pub fn parse_ty_closure(&mut self) -> Ty_ {\n         /*\n \n@@ -1154,7 +1170,7 @@ impl<'a> Parser<'a> {\n         }\n     }\n \n-    // parse a function type (following the 'fn')\n+    /// Parse a function type (following the 'fn')\n     pub fn parse_ty_fn_decl(&mut self, allow_variadic: bool)\n                             -> (P<FnDecl>, Vec<ast::Lifetime>) {\n         /*\n@@ -1186,7 +1202,7 @@ impl<'a> Parser<'a> {\n         (decl, lifetimes)\n     }\n \n-    // parse the methods in a trait declaration\n+    /// Parse the methods in a trait declaration\n     pub fn parse_trait_methods(&mut self) -> Vec<TraitMethod> {\n         self.parse_unspanned_seq(\n             &token::LBRACE,\n@@ -1255,15 +1271,15 @@ impl<'a> Parser<'a> {\n         })\n     }\n \n-    // parse a possibly mutable type\n+    /// Parse a possibly mutable type\n     pub fn parse_mt(&mut self) -> MutTy {\n         let mutbl = self.parse_mutability();\n         let t = self.parse_ty(true);\n         MutTy { ty: t, mutbl: mutbl }\n     }\n \n-    // parse [mut/const/imm] ID : TY\n-    // now used only by obsolete record syntax parser...\n+    /// Parse [mut/const/imm] ID : TY\n+    /// now used only by obsolete record syntax parser...\n     pub fn parse_ty_field(&mut self) -> TypeField {\n         let lo = self.span.lo;\n         let mutbl = self.parse_mutability();\n@@ -1278,7 +1294,7 @@ impl<'a> Parser<'a> {\n         }\n     }\n \n-    // parse optional return type [ -> TY ] in function decl\n+    /// Parse optional return type [ -> TY ] in function decl\n     pub fn parse_ret_ty(&mut self) -> (RetStyle, P<Ty>) {\n         return if self.eat(&token::RARROW) {\n             let lo = self.span.lo;\n@@ -1478,8 +1494,8 @@ impl<'a> Parser<'a> {\n         }\n     }\n \n-    // This version of parse arg doesn't necessarily require\n-    // identifier names.\n+    /// This version of parse arg doesn't necessarily require\n+    /// identifier names.\n     pub fn parse_arg_general(&mut self, require_name: bool) -> Arg {\n         let pat = if require_name || self.is_named_argument() {\n             debug!(\"parse_arg_general parse_pat (require_name:{:?})\",\n@@ -1504,12 +1520,12 @@ impl<'a> Parser<'a> {\n         }\n     }\n \n-    // parse a single function argument\n+    /// Parse a single function argument\n     pub fn parse_arg(&mut self) -> Arg {\n         self.parse_arg_general(true)\n     }\n \n-    // parse an argument in a lambda header e.g. |arg, arg|\n+    /// Parse an argument in a lambda header e.g. |arg, arg|\n     pub fn parse_fn_block_arg(&mut self) -> Arg {\n         let pat = self.parse_pat();\n         let t = if self.eat(&token::COLON) {\n@@ -1539,34 +1555,32 @@ impl<'a> Parser<'a> {\n         }\n     }\n \n-    // matches token_lit = LIT_INT | ...\n+    /// Matches token_lit = LIT_INTEGER | ...\n     pub fn lit_from_token(&mut self, tok: &token::Token) -> Lit_ {\n         match *tok {\n-            token::LIT_BYTE(i) => LitByte(i),\n-            token::LIT_CHAR(i) => LitChar(i),\n-            token::LIT_INT(i, it) => LitInt(i, it),\n-            token::LIT_UINT(u, ut) => LitUint(u, ut),\n-            token::LIT_INT_UNSUFFIXED(i) => LitIntUnsuffixed(i),\n-            token::LIT_FLOAT(s, ft) => {\n-                LitFloat(self.id_to_interned_str(s), ft)\n-            }\n-            token::LIT_FLOAT_UNSUFFIXED(s) => {\n-                LitFloatUnsuffixed(self.id_to_interned_str(s))\n-            }\n+            token::LIT_BYTE(i) => LitByte(parse::byte_lit(i.as_str()).val0()),\n+            token::LIT_CHAR(i) => LitChar(parse::char_lit(i.as_str()).val0()),\n+            token::LIT_INTEGER(s) => parse::integer_lit(s.as_str(),\n+                                                        &self.sess.span_diagnostic, self.span),\n+            token::LIT_FLOAT(s) => parse::float_lit(s.as_str()),\n             token::LIT_STR(s) => {\n-                LitStr(self.id_to_interned_str(s), ast::CookedStr)\n+                LitStr(token::intern_and_get_ident(parse::str_lit(s.as_str()).as_slice()),\n+                       ast::CookedStr)\n             }\n             token::LIT_STR_RAW(s, n) => {\n-                LitStr(self.id_to_interned_str(s), ast::RawStr(n))\n+                LitStr(token::intern_and_get_ident(parse::raw_str_lit(s.as_str()).as_slice()),\n+                       ast::RawStr(n))\n             }\n-            token::LIT_BINARY_RAW(ref v, _) |\n-            token::LIT_BINARY(ref v) => LitBinary(v.clone()),\n+            token::LIT_BINARY(i) =>\n+                LitBinary(parse::binary_lit(i.as_str())),\n+            token::LIT_BINARY_RAW(i, _) =>\n+                LitBinary(Rc::new(i.as_str().as_bytes().iter().map(|&x| x).collect())),\n             token::LPAREN => { self.expect(&token::RPAREN); LitNil },\n             _ => { self.unexpected_last(tok); }\n         }\n     }\n \n-    // matches lit = true | false | token_lit\n+    /// Matches lit = true | false | token_lit\n     pub fn parse_lit(&mut self) -> Lit {\n         let lo = self.span.lo;\n         let lit = if self.eat_keyword(keywords::True) {\n@@ -1581,7 +1595,7 @@ impl<'a> Parser<'a> {\n         codemap::Spanned { node: lit, span: mk_sp(lo, self.last_span.hi) }\n     }\n \n-    // matches '-' lit | lit\n+    /// matches '-' lit | lit\n     pub fn parse_literal_maybe_minus(&mut self) -> Gc<Expr> {\n         let minus_lo = self.span.lo;\n         let minus_present = self.eat(&token::BINOP(token::MINUS));\n@@ -1719,7 +1733,7 @@ impl<'a> Parser<'a> {\n     }\n \n     /// Parses a single lifetime\n-    // matches lifetime = LIFETIME\n+    /// Matches lifetime = LIFETIME\n     pub fn parse_lifetime(&mut self) -> ast::Lifetime {\n         match self.token {\n             token::LIFETIME(i) => {\n@@ -1779,7 +1793,7 @@ impl<'a> Parser<'a> {\n         token::is_keyword(keywords::Const, tok)\n     }\n \n-    // parse mutability declaration (mut/const/imm)\n+    /// Parse mutability declaration (mut/const/imm)\n     pub fn parse_mutability(&mut self) -> Mutability {\n         if self.eat_keyword(keywords::Mut) {\n             MutMutable\n@@ -1788,7 +1802,7 @@ impl<'a> Parser<'a> {\n         }\n     }\n \n-    // parse ident COLON expr\n+    /// Parse ident COLON expr\n     pub fn parse_field(&mut self) -> Field {\n         let lo = self.span.lo;\n         let i = self.parse_ident();\n@@ -1867,9 +1881,9 @@ impl<'a> Parser<'a> {\n         }\n     }\n \n-    // at the bottom (top?) of the precedence hierarchy,\n-    // parse things like parenthesized exprs,\n-    // macros, return, etc.\n+    /// At the bottom (top?) of the precedence hierarchy,\n+    /// parse things like parenthesized exprs,\n+    /// macros, return, etc.\n     pub fn parse_bottom_expr(&mut self) -> Gc<Expr> {\n         maybe_whole_expr!(self);\n \n@@ -1934,7 +1948,12 @@ impl<'a> Parser<'a> {\n                     });\n                 return self.mk_expr(lo, body.span.hi, ExprProc(decl, fakeblock));\n             },\n-            token::IDENT(id @ ast::Ident{name:token::SELF_KEYWORD_NAME,ctxt:_},false) => {\n+            // FIXME #13626: Should be able to stick in\n+            // token::SELF_KEYWORD_NAME\n+            token::IDENT(id @ ast::Ident{\n+                        name: ast::Name(token::SELF_KEYWORD_NAME_NUM),\n+                        ctxt: _\n+                    } ,false) => {\n                 self.bump();\n                 let path = ast_util::ident_to_path(mk_sp(lo, hi), id);\n                 ex = ExprPath(path);\n@@ -2107,15 +2126,15 @@ impl<'a> Parser<'a> {\n         return self.mk_expr(lo, hi, ex);\n     }\n \n-    // parse a block or unsafe block\n+    /// Parse a block or unsafe block\n     pub fn parse_block_expr(&mut self, lo: BytePos, blk_mode: BlockCheckMode)\n                             -> Gc<Expr> {\n         self.expect(&token::LBRACE);\n         let blk = self.parse_block_tail(lo, blk_mode);\n         return self.mk_expr(blk.span.lo, blk.span.hi, ExprBlock(blk));\n     }\n \n-    // parse a.b or a(13) or a[4] or just a\n+    /// parse a.b or a(13) or a[4] or just a\n     pub fn parse_dot_or_call_expr(&mut self) -> Gc<Expr> {\n         let b = self.parse_bottom_expr();\n         self.parse_dot_or_call_expr_with(b)\n@@ -2199,8 +2218,8 @@ impl<'a> Parser<'a> {\n         return e;\n     }\n \n-    // parse an optional separator followed by a kleene-style\n-    // repetition token (+ or *).\n+    /// Parse an optional separator followed by a kleene-style\n+    /// repetition token (+ or *).\n     pub fn parse_sep_and_zerok(&mut self) -> (Option<token::Token>, bool) {\n         fn parse_zerok(parser: &mut Parser) -> Option<bool> {\n             match parser.token {\n@@ -2225,7 +2244,7 @@ impl<'a> Parser<'a> {\n         }\n     }\n \n-    // parse a single token tree from the input.\n+    /// parse a single token tree from the input.\n     pub fn parse_token_tree(&mut self) -> TokenTree {\n         // FIXME #6994: currently, this is too eager. It\n         // parses token trees but also identifies TTSeq's\n@@ -2341,9 +2360,9 @@ impl<'a> Parser<'a> {\n         }\n     }\n \n-    // This goofy function is necessary to correctly match parens in Matcher's.\n-    // Otherwise, `$( ( )` would be a valid Matcher, and `$( () )` would be\n-    // invalid. It's similar to common::parse_seq.\n+    /// This goofy function is necessary to correctly match parens in Matcher's.\n+    /// Otherwise, `$( ( )` would be a valid Matcher, and `$( () )` would be\n+    /// invalid. It's similar to common::parse_seq.\n     pub fn parse_matcher_subseq_upto(&mut self,\n                                      name_idx: &mut uint,\n                                      ket: &token::Token)\n@@ -2392,7 +2411,7 @@ impl<'a> Parser<'a> {\n         return spanned(lo, self.span.hi, m);\n     }\n \n-    // parse a prefix-operator expr\n+    /// Parse a prefix-operator expr\n     pub fn parse_prefix_expr(&mut self) -> Gc<Expr> {\n         let lo = self.span.lo;\n         let hi;\n@@ -2500,13 +2519,13 @@ impl<'a> Parser<'a> {\n         return self.mk_expr(lo, hi, ex);\n     }\n \n-    // parse an expression of binops\n+    /// Parse an expression of binops\n     pub fn parse_binops(&mut self) -> Gc<Expr> {\n         let prefix_expr = self.parse_prefix_expr();\n         self.parse_more_binops(prefix_expr, 0)\n     }\n \n-    // parse an expression of binops of at least min_prec precedence\n+    /// Parse an expression of binops of at least min_prec precedence\n     pub fn parse_more_binops(&mut self, lhs: Gc<Expr>,\n                              min_prec: uint) -> Gc<Expr> {\n         if self.expr_is_complete(lhs) { return lhs; }\n@@ -2554,9 +2573,9 @@ impl<'a> Parser<'a> {\n         }\n     }\n \n-    // parse an assignment expression....\n-    // actually, this seems to be the main entry point for\n-    // parsing an arbitrary expression.\n+    /// Parse an assignment expression....\n+    /// actually, this seems to be the main entry point for\n+    /// parsing an arbitrary expression.\n     pub fn parse_assign_expr(&mut self) -> Gc<Expr> {\n         let lo = self.span.lo;\n         let lhs = self.parse_binops();\n@@ -2590,7 +2609,7 @@ impl<'a> Parser<'a> {\n         }\n     }\n \n-    // parse an 'if' expression ('if' token already eaten)\n+    /// Parse an 'if' expression ('if' token already eaten)\n     pub fn parse_if_expr(&mut self) -> Gc<Expr> {\n         let lo = self.last_span.lo;\n         let cond = self.parse_expr_res(RESTRICT_NO_STRUCT_LITERAL);\n@@ -2605,7 +2624,7 @@ impl<'a> Parser<'a> {\n         self.mk_expr(lo, hi, ExprIf(cond, thn, els))\n     }\n \n-    // `|args| { ... }` or `{ ...}` like in `do` expressions\n+    /// `|args| { ... }` or `{ ...}` like in `do` expressions\n     pub fn parse_lambda_block_expr(&mut self) -> Gc<Expr> {\n         self.parse_lambda_expr_(\n             |p| {\n@@ -2634,15 +2653,15 @@ impl<'a> Parser<'a> {\n             })\n     }\n \n-    // `|args| expr`\n+    /// `|args| expr`\n     pub fn parse_lambda_expr(&mut self) -> Gc<Expr> {\n         self.parse_lambda_expr_(|p| p.parse_fn_block_decl(),\n                                 |p| p.parse_expr())\n     }\n \n-    // parse something of the form |args| expr\n-    // this is used both in parsing a lambda expr\n-    // and in parsing a block expr as e.g. in for...\n+    /// parse something of the form |args| expr\n+    /// this is used both in parsing a lambda expr\n+    /// and in parsing a block expr as e.g. in for...\n     pub fn parse_lambda_expr_(&mut self,\n                               parse_decl: |&mut Parser| -> P<FnDecl>,\n                               parse_body: |&mut Parser| -> Gc<Expr>)\n@@ -2671,7 +2690,7 @@ impl<'a> Parser<'a> {\n         }\n     }\n \n-    // parse a 'for' .. 'in' expression ('for' token already eaten)\n+    /// Parse a 'for' .. 'in' expression ('for' token already eaten)\n     pub fn parse_for_expr(&mut self, opt_ident: Option<ast::Ident>) -> Gc<Expr> {\n         // Parse: `for <src_pat> in <src_expr> <src_loop_block>`\n \n@@ -2737,12 +2756,12 @@ impl<'a> Parser<'a> {\n         return self.mk_expr(lo, hi, ExprMatch(discriminant, arms));\n     }\n \n-    // parse an expression\n+    /// Parse an expression\n     pub fn parse_expr(&mut self) -> Gc<Expr> {\n         return self.parse_expr_res(UNRESTRICTED);\n     }\n \n-    // parse an expression, subject to the given restriction\n+    /// Parse an expression, subject to the given restriction\n     pub fn parse_expr_res(&mut self, r: restriction) -> Gc<Expr> {\n         let old = self.restriction;\n         self.restriction = r;\n@@ -2751,7 +2770,7 @@ impl<'a> Parser<'a> {\n         return e;\n     }\n \n-    // parse the RHS of a local variable declaration (e.g. '= 14;')\n+    /// Parse the RHS of a local variable declaration (e.g. '= 14;')\n     fn parse_initializer(&mut self) -> Option<Gc<Expr>> {\n         if self.token == token::EQ {\n             self.bump();\n@@ -2761,7 +2780,7 @@ impl<'a> Parser<'a> {\n         }\n     }\n \n-    // parse patterns, separated by '|' s\n+    /// Parse patterns, separated by '|' s\n     fn parse_pats(&mut self) -> Vec<Gc<Pat>> {\n         let mut pats = Vec::new();\n         loop {\n@@ -2824,7 +2843,7 @@ impl<'a> Parser<'a> {\n         (before, slice, after)\n     }\n \n-    // parse the fields of a struct-like pattern\n+    /// Parse the fields of a struct-like pattern\n     fn parse_pat_fields(&mut self) -> (Vec<ast::FieldPat> , bool) {\n         let mut fields = Vec::new();\n         let mut etc = false;\n@@ -2884,7 +2903,7 @@ impl<'a> Parser<'a> {\n         return (fields, etc);\n     }\n \n-    // parse a pattern.\n+    /// Parse a pattern.\n     pub fn parse_pat(&mut self) -> Gc<Pat> {\n         maybe_whole!(self, NtPat);\n \n@@ -3126,9 +3145,9 @@ impl<'a> Parser<'a> {\n         }\n     }\n \n-    // parse ident or ident @ pat\n-    // used by the copy foo and ref foo patterns to give a good\n-    // error message when parsing mistakes like ref foo(a,b)\n+    /// Parse ident or ident @ pat\n+    /// used by the copy foo and ref foo patterns to give a good\n+    /// error message when parsing mistakes like ref foo(a,b)\n     fn parse_pat_ident(&mut self,\n                        binding_mode: ast::BindingMode)\n                        -> ast::Pat_ {\n@@ -3162,7 +3181,7 @@ impl<'a> Parser<'a> {\n         PatIdent(binding_mode, name, sub)\n     }\n \n-    // parse a local variable declaration\n+    /// Parse a local variable declaration\n     fn parse_local(&mut self) -> Gc<Local> {\n         let lo = self.span.lo;\n         let pat = self.parse_pat();\n@@ -3186,14 +3205,14 @@ impl<'a> Parser<'a> {\n         }\n     }\n \n-    // parse a \"let\" stmt\n+    /// Parse a \"let\" stmt\n     fn parse_let(&mut self) -> Gc<Decl> {\n         let lo = self.span.lo;\n         let local = self.parse_local();\n         box(GC) spanned(lo, self.last_span.hi, DeclLocal(local))\n     }\n \n-    // parse a structure field\n+    /// Parse a structure field\n     fn parse_name_and_ty(&mut self, pr: Visibility,\n                          attrs: Vec<Attribute> ) -> StructField {\n         let lo = self.span.lo;\n@@ -3211,8 +3230,8 @@ impl<'a> Parser<'a> {\n         })\n     }\n \n-    // parse a statement. may include decl.\n-    // precondition: any attributes are parsed already\n+    /// Parse a statement. may include decl.\n+    /// Precondition: any attributes are parsed already\n     pub fn parse_stmt(&mut self, item_attrs: Vec<Attribute>) -> Gc<Stmt> {\n         maybe_whole!(self, NtStmt);\n \n@@ -3315,13 +3334,13 @@ impl<'a> Parser<'a> {\n         }\n     }\n \n-    // is this expression a successfully-parsed statement?\n+    /// Is this expression a successfully-parsed statement?\n     fn expr_is_complete(&mut self, e: Gc<Expr>) -> bool {\n         return self.restriction == RESTRICT_STMT_EXPR &&\n             !classify::expr_requires_semi_to_be_stmt(e);\n     }\n \n-    // parse a block. No inner attrs are allowed.\n+    /// Parse a block. No inner attrs are allowed.\n     pub fn parse_block(&mut self) -> P<Block> {\n         maybe_whole!(no_clone self, NtBlock);\n \n@@ -3331,7 +3350,7 @@ impl<'a> Parser<'a> {\n         return self.parse_block_tail_(lo, DefaultBlock, Vec::new());\n     }\n \n-    // parse a block. Inner attrs are allowed.\n+    /// Parse a block. Inner attrs are allowed.\n     fn parse_inner_attrs_and_block(&mut self)\n         -> (Vec<Attribute> , P<Block>) {\n \n@@ -3344,15 +3363,15 @@ impl<'a> Parser<'a> {\n         (inner, self.parse_block_tail_(lo, DefaultBlock, next))\n     }\n \n-    // Precondition: already parsed the '{' or '#{'\n-    // I guess that also means \"already parsed the 'impure'\" if\n-    // necessary, and this should take a qualifier.\n-    // some blocks start with \"#{\"...\n+    /// Precondition: already parsed the '{' or '#{'\n+    /// I guess that also means \"already parsed the 'impure'\" if\n+    /// necessary, and this should take a qualifier.\n+    /// Some blocks start with \"#{\"...\n     fn parse_block_tail(&mut self, lo: BytePos, s: BlockCheckMode) -> P<Block> {\n         self.parse_block_tail_(lo, s, Vec::new())\n     }\n \n-    // parse the rest of a block expression or function body\n+    /// Parse the rest of a block expression or function body\n     fn parse_block_tail_(&mut self, lo: BytePos, s: BlockCheckMode,\n                          first_item_attrs: Vec<Attribute> ) -> P<Block> {\n         let mut stmts = Vec::new();\n@@ -3510,18 +3529,18 @@ impl<'a> Parser<'a> {\n         }\n     }\n \n-    // matches bounds    = ( boundseq )?\n-    // where   boundseq  = ( bound + boundseq ) | bound\n-    // and     bound     = 'static | ty\n-    // Returns \"None\" if there's no colon (e.g. \"T\");\n-    // Returns \"Some(Empty)\" if there's a colon but nothing after (e.g. \"T:\")\n-    // Returns \"Some(stuff)\" otherwise (e.g. \"T:stuff\").\n-    // NB: The None/Some distinction is important for issue #7264.\n-    //\n-    // Note that the `allow_any_lifetime` argument is a hack for now while the\n-    // AST doesn't support arbitrary lifetimes in bounds on type parameters. In\n-    // the future, this flag should be removed, and the return value of this\n-    // function should be Option<~[TyParamBound]>\n+    /// matches optbounds = ( ( : ( boundseq )? )? )\n+    /// where   boundseq  = ( bound + boundseq ) | bound\n+    /// and     bound     = 'static | ty\n+    /// Returns \"None\" if there's no colon (e.g. \"T\");\n+    /// Returns \"Some(Empty)\" if there's a colon but nothing after (e.g. \"T:\")\n+    /// Returns \"Some(stuff)\" otherwise (e.g. \"T:stuff\").\n+    /// NB: The None/Some distinction is important for issue #7264.\n+    ///\n+    /// Note that the `allow_any_lifetime` argument is a hack for now while the\n+    /// AST doesn't support arbitrary lifetimes in bounds on type parameters. In\n+    /// the future, this flag should be removed, and the return value of this\n+    /// function should be Option<~[TyParamBound]>\n     fn parse_ty_param_bounds(&mut self, allow_any_lifetime: bool)\n                              -> (Option<ast::Lifetime>,\n                                  OwnedSlice<TyParamBound>) {\n@@ -3588,7 +3607,7 @@ impl<'a> Parser<'a> {\n         }\n     }\n \n-    // matches typaram = (unbound`?`)? IDENT optbounds ( EQ ty )?\n+    /// Matches typaram = (unbound`?`)? IDENT optbounds ( EQ ty )?\n     fn parse_ty_param(&mut self) -> TyParam {\n         // This is a bit hacky. Currently we are only interested in a single\n         // unbound, and it may only be `Sized`. To avoid backtracking and other\n@@ -3632,10 +3651,10 @@ impl<'a> Parser<'a> {\n         }\n     }\n \n-    // parse a set of optional generic type parameter declarations\n-    // matches generics = ( ) | ( < > ) | ( < typaramseq ( , )? > ) | ( < lifetimes ( , )? > )\n-    //                  | ( < lifetimes , typaramseq ( , )? > )\n-    // where   typaramseq = ( typaram ) | ( typaram , typaramseq )\n+    /// Parse a set of optional generic type parameter declarations\n+    /// matches generics = ( ) | ( < > ) | ( < typaramseq ( , )? > ) | ( < lifetimes ( , )? > )\n+    ///                  | ( < lifetimes , typaramseq ( , )? > )\n+    /// where   typaramseq = ( typaram ) | ( typaram , typaramseq )\n     pub fn parse_generics(&mut self) -> ast::Generics {\n         if self.eat(&token::LT) {\n             let lifetimes = self.parse_lifetimes();\n@@ -3727,7 +3746,7 @@ impl<'a> Parser<'a> {\n         (args, variadic)\n     }\n \n-    // parse the argument list and result type of a function declaration\n+    /// Parse the argument list and result type of a function declaration\n     pub fn parse_fn_decl(&mut self, allow_variadic: bool) -> P<FnDecl> {\n \n         let (args, variadic) = self.parse_fn_args(true, allow_variadic);\n@@ -3762,8 +3781,8 @@ impl<'a> Parser<'a> {\n         }\n     }\n \n-    // parse the argument list and result type of a function\n-    // that may have a self type.\n+    /// Parse the argument list and result type of a function\n+    /// that may have a self type.\n     fn parse_fn_decl_with_self(&mut self, parse_arg_fn: |&mut Parser| -> Arg)\n                                -> (ExplicitSelf, P<FnDecl>) {\n         fn maybe_parse_borrowed_explicit_self(this: &mut Parser)\n@@ -3921,7 +3940,7 @@ impl<'a> Parser<'a> {\n         (spanned(lo, hi, explicit_self), fn_decl)\n     }\n \n-    // parse the |arg, arg| header on a lambda\n+    /// Parse the |arg, arg| header on a lambda\n     fn parse_fn_block_decl(&mut self) -> P<FnDecl> {\n         let inputs_captures = {\n             if self.eat(&token::OROR) {\n@@ -3953,7 +3972,7 @@ impl<'a> Parser<'a> {\n         })\n     }\n \n-    // Parses the `(arg, arg) -> return_type` header on a procedure.\n+    /// Parses the `(arg, arg) -> return_type` header on a procedure.\n     fn parse_proc_decl(&mut self) -> P<FnDecl> {\n         let inputs =\n             self.parse_unspanned_seq(&token::LPAREN,\n@@ -3979,7 +3998,7 @@ impl<'a> Parser<'a> {\n         })\n     }\n \n-    // parse the name and optional generic types of a function header.\n+    /// Parse the name and optional generic types of a function header.\n     fn parse_fn_header(&mut self) -> (Ident, ast::Generics) {\n         let id = self.parse_ident();\n         let generics = self.parse_generics();\n@@ -3999,15 +4018,15 @@ impl<'a> Parser<'a> {\n         }\n     }\n \n-    // parse an item-position function declaration.\n+    /// Parse an item-position function declaration.\n     fn parse_item_fn(&mut self, fn_style: FnStyle, abi: abi::Abi) -> ItemInfo {\n         let (ident, generics) = self.parse_fn_header();\n         let decl = self.parse_fn_decl(false);\n         let (inner_attrs, body) = self.parse_inner_attrs_and_block();\n         (ident, ItemFn(decl, fn_style, abi, generics, body), Some(inner_attrs))\n     }\n \n-    // parse a method in a trait impl, starting with `attrs` attributes.\n+    /// Parse a method in a trait impl, starting with `attrs` attributes.\n     fn parse_method(&mut self,\n                     already_parsed_attrs: Option<Vec<Attribute>>) -> Gc<Method> {\n         let next_attrs = self.parse_outer_attributes();\n@@ -4043,7 +4062,7 @@ impl<'a> Parser<'a> {\n         }\n     }\n \n-    // parse trait Foo { ... }\n+    /// Parse trait Foo { ... }\n     fn parse_item_trait(&mut self) -> ItemInfo {\n         let ident = self.parse_ident();\n         let tps = self.parse_generics();\n@@ -4062,9 +4081,9 @@ impl<'a> Parser<'a> {\n         (ident, ItemTrait(tps, sized, traits, meths), None)\n     }\n \n-    // Parses two variants (with the region/type params always optional):\n-    //    impl<T> Foo { ... }\n-    //    impl<T> ToString for ~[T] { ... }\n+    /// Parses two variants (with the region/type params always optional):\n+    ///    impl<T> Foo { ... }\n+    ///    impl<T> ToString for ~[T] { ... }\n     fn parse_item_impl(&mut self) -> ItemInfo {\n         // First, parse type parameters if necessary.\n         let generics = self.parse_generics();\n@@ -4117,15 +4136,15 @@ impl<'a> Parser<'a> {\n         (ident, ItemImpl(generics, opt_trait, ty, meths), Some(inner_attrs))\n     }\n \n-    // parse a::B<String,int>\n+    /// Parse a::B<String,int>\n     fn parse_trait_ref(&mut self) -> TraitRef {\n         ast::TraitRef {\n             path: self.parse_path(LifetimeAndTypesWithoutColons).path,\n             ref_id: ast::DUMMY_NODE_ID,\n         }\n     }\n \n-    // parse B + C<String,int> + D\n+    /// Parse B + C<String,int> + D\n     fn parse_trait_ref_list(&mut self, ket: &token::Token) -> Vec<TraitRef> {\n         self.parse_seq_to_before_end(\n             ket,\n@@ -4134,7 +4153,7 @@ impl<'a> Parser<'a> {\n         )\n     }\n \n-    // parse struct Foo { ... }\n+    /// Parse struct Foo { ... }\n     fn parse_item_struct(&mut self, is_virtual: bool) -> ItemInfo {\n         let class_name = self.parse_ident();\n         let generics = self.parse_generics();\n@@ -4217,7 +4236,7 @@ impl<'a> Parser<'a> {\n          None)\n     }\n \n-    // parse a structure field declaration\n+    /// Parse a structure field declaration\n     pub fn parse_single_struct_field(&mut self,\n                                      vis: Visibility,\n                                      attrs: Vec<Attribute> )\n@@ -4239,7 +4258,7 @@ impl<'a> Parser<'a> {\n         a_var\n     }\n \n-    // parse an element of a struct definition\n+    /// Parse an element of a struct definition\n     fn parse_struct_decl_field(&mut self) -> StructField {\n \n         let attrs = self.parse_outer_attributes();\n@@ -4251,7 +4270,7 @@ impl<'a> Parser<'a> {\n         return self.parse_single_struct_field(Inherited, attrs);\n     }\n \n-    // parse visiility: PUB, PRIV, or nothing\n+    /// Parse visiility: PUB, PRIV, or nothing\n     fn parse_visibility(&mut self) -> Visibility {\n         if self.eat_keyword(keywords::Pub) { Public }\n         else { Inherited }\n@@ -4273,8 +4292,8 @@ impl<'a> Parser<'a> {\n         }\n     }\n \n-    // given a termination token and a vector of already-parsed\n-    // attributes (of length 0 or 1), parse all of the items in a module\n+    /// Given a termination token and a vector of already-parsed\n+    /// attributes (of length 0 or 1), parse all of the items in a module\n     fn parse_mod_items(&mut self,\n                        term: token::Token,\n                        first_item_attrs: Vec<Attribute>,\n@@ -4342,7 +4361,7 @@ impl<'a> Parser<'a> {\n         (id, ItemStatic(ty, m, e), None)\n     }\n \n-    // parse a `mod <foo> { ... }` or `mod <foo>;` item\n+    /// Parse a `mod <foo> { ... }` or `mod <foo>;` item\n     fn parse_item_mod(&mut self, outer_attrs: &[Attribute]) -> ItemInfo {\n         let id_span = self.span;\n         let id = self.parse_ident();\n@@ -4380,7 +4399,7 @@ impl<'a> Parser<'a> {\n         self.mod_path_stack.pop().unwrap();\n     }\n \n-    // read a module from a source file.\n+    /// Read a module from a source file.\n     fn eval_src_mod(&mut self,\n                     id: ast::Ident,\n                     outer_attrs: &[ast::Attribute],\n@@ -4488,7 +4507,7 @@ impl<'a> Parser<'a> {\n         return (ast::ItemMod(m0), mod_attrs);\n     }\n \n-    // parse a function declaration from a foreign module\n+    /// Parse a function declaration from a foreign module\n     fn parse_item_foreign_fn(&mut self, vis: ast::Visibility,\n                              attrs: Vec<Attribute>) -> Gc<ForeignItem> {\n         let lo = self.span.lo;\n@@ -4506,7 +4525,7 @@ impl<'a> Parser<'a> {\n                                    vis: vis }\n     }\n \n-    // parse a static item from a foreign module\n+    /// Parse a static item from a foreign module\n     fn parse_item_foreign_static(&mut self, vis: ast::Visibility,\n                                  attrs: Vec<Attribute> ) -> Gc<ForeignItem> {\n         let lo = self.span.lo;\n@@ -4529,7 +4548,7 @@ impl<'a> Parser<'a> {\n         }\n     }\n \n-    // parse safe/unsafe and fn\n+    /// Parse safe/unsafe and fn\n     fn parse_fn_style(&mut self) -> FnStyle {\n         if self.eat_keyword(keywords::Fn) { NormalFn }\n         else if self.eat_keyword(keywords::Unsafe) {\n@@ -4540,8 +4559,8 @@ impl<'a> Parser<'a> {\n     }\n \n \n-    // at this point, this is essentially a wrapper for\n-    // parse_foreign_items.\n+    /// At this point, this is essentially a wrapper for\n+    /// parse_foreign_items.\n     fn parse_foreign_mod_items(&mut self,\n                                abi: abi::Abi,\n                                first_item_attrs: Vec<Attribute> )\n@@ -4642,7 +4661,7 @@ impl<'a> Parser<'a> {\n         return IoviItem(item);\n     }\n \n-    // parse type Foo = Bar;\n+    /// Parse type Foo = Bar;\n     fn parse_item_type(&mut self) -> ItemInfo {\n         let ident = self.parse_ident();\n         let tps = self.parse_generics();\n@@ -4652,8 +4671,8 @@ impl<'a> Parser<'a> {\n         (ident, ItemTy(ty, tps), None)\n     }\n \n-    // parse a structure-like enum variant definition\n-    // this should probably be renamed or refactored...\n+    /// Parse a structure-like enum variant definition\n+    /// this should probably be renamed or refactored...\n     fn parse_struct_def(&mut self) -> Gc<StructDef> {\n         let mut fields: Vec<StructField> = Vec::new();\n         while self.token != token::RBRACE {\n@@ -4669,7 +4688,7 @@ impl<'a> Parser<'a> {\n         };\n     }\n \n-    // parse the part of an \"enum\" decl following the '{'\n+    /// Parse the part of an \"enum\" decl following the '{'\n     fn parse_enum_def(&mut self, _generics: &ast::Generics) -> EnumDef {\n         let mut variants = Vec::new();\n         let mut all_nullary = true;\n@@ -4733,7 +4752,7 @@ impl<'a> Parser<'a> {\n         ast::EnumDef { variants: variants }\n     }\n \n-    // parse an \"enum\" declaration\n+    /// Parse an \"enum\" declaration\n     fn parse_item_enum(&mut self) -> ItemInfo {\n         let id = self.parse_ident();\n         let generics = self.parse_generics();\n@@ -4750,14 +4769,13 @@ impl<'a> Parser<'a> {\n         }\n     }\n \n-    // Parses a string as an ABI spec on an extern type or module. Consumes\n-    // the `extern` keyword, if one is found.\n+    /// Parses a string as an ABI spec on an extern type or module. Consumes\n+    /// the `extern` keyword, if one is found.\n     fn parse_opt_abi(&mut self) -> Option<abi::Abi> {\n         match self.token {\n             token::LIT_STR(s) | token::LIT_STR_RAW(s, _) => {\n                 self.bump();\n-                let identifier_string = token::get_ident(s);\n-                let the_string = identifier_string.get();\n+                let the_string = s.as_str();\n                 match abi::lookup(the_string) {\n                     Some(abi) => Some(abi),\n                     None => {\n@@ -4777,10 +4795,10 @@ impl<'a> Parser<'a> {\n         }\n     }\n \n-    // parse one of the items or view items allowed by the\n-    // flags; on failure, return IoviNone.\n-    // NB: this function no longer parses the items inside an\n-    // extern crate.\n+    /// Parse one of the items or view items allowed by the\n+    /// flags; on failure, return IoviNone.\n+    /// NB: this function no longer parses the items inside an\n+    /// extern crate.\n     fn parse_item_or_view_item(&mut self,\n                                attrs: Vec<Attribute> ,\n                                macros_allowed: bool)\n@@ -4988,7 +5006,7 @@ impl<'a> Parser<'a> {\n         self.parse_macro_use_or_failure(attrs,macros_allowed,lo,visibility)\n     }\n \n-    // parse a foreign item; on failure, return IoviNone.\n+    /// Parse a foreign item; on failure, return IoviNone.\n     fn parse_foreign_item(&mut self,\n                           attrs: Vec<Attribute> ,\n                           macros_allowed: bool)\n@@ -5011,7 +5029,7 @@ impl<'a> Parser<'a> {\n         self.parse_macro_use_or_failure(attrs,macros_allowed,lo,visibility)\n     }\n \n-    // this is the fall-through for parsing items.\n+    /// This is the fall-through for parsing items.\n     fn parse_macro_use_or_failure(\n         &mut self,\n         attrs: Vec<Attribute> ,\n@@ -5095,17 +5113,17 @@ impl<'a> Parser<'a> {\n         }\n     }\n \n-    // parse, e.g., \"use a::b::{z,y}\"\n+    /// Parse, e.g., \"use a::b::{z,y}\"\n     fn parse_use(&mut self) -> ViewItem_ {\n         return ViewItemUse(self.parse_view_path());\n     }\n \n \n-    // matches view_path : MOD? IDENT EQ non_global_path\n-    // | MOD? non_global_path MOD_SEP LBRACE RBRACE\n-    // | MOD? non_global_path MOD_SEP LBRACE ident_seq RBRACE\n-    // | MOD? non_global_path MOD_SEP STAR\n-    // | MOD? non_global_path\n+    /// Matches view_path : MOD? IDENT EQ non_global_path\n+    /// | MOD? non_global_path MOD_SEP LBRACE RBRACE\n+    /// | MOD? non_global_path MOD_SEP LBRACE ident_seq RBRACE\n+    /// | MOD? non_global_path MOD_SEP STAR\n+    /// | MOD? non_global_path\n     fn parse_view_path(&mut self) -> Gc<ViewPath> {\n         let lo = self.span.lo;\n \n@@ -5228,10 +5246,10 @@ impl<'a> Parser<'a> {\n                         ViewPathSimple(last, path, ast::DUMMY_NODE_ID));\n     }\n \n-    // Parses a sequence of items. Stops when it finds program\n-    // text that can't be parsed as an item\n-    // - mod_items uses extern_mod_allowed = true\n-    // - block_tail_ uses extern_mod_allowed = false\n+    /// Parses a sequence of items. Stops when it finds program\n+    /// text that can't be parsed as an item\n+    /// - mod_items uses extern_mod_allowed = true\n+    /// - block_tail_ uses extern_mod_allowed = false\n     fn parse_items_and_view_items(&mut self,\n                                   first_item_attrs: Vec<Attribute> ,\n                                   mut extern_mod_allowed: bool,\n@@ -5313,8 +5331,8 @@ impl<'a> Parser<'a> {\n         }\n     }\n \n-    // Parses a sequence of foreign items. Stops when it finds program\n-    // text that can't be parsed as an item\n+    /// Parses a sequence of foreign items. Stops when it finds program\n+    /// text that can't be parsed as an item\n     fn parse_foreign_items(&mut self, first_item_attrs: Vec<Attribute> ,\n                            macros_allowed: bool)\n         -> ParsedItemsAndViewItems {\n@@ -5353,8 +5371,8 @@ impl<'a> Parser<'a> {\n         }\n     }\n \n-    // Parses a source module as a crate. This is the main\n-    // entry point for the parser.\n+    /// Parses a source module as a crate. This is the main\n+    /// entry point for the parser.\n     pub fn parse_crate_mod(&mut self) -> Crate {\n         let lo = self.span.lo;\n         // parse the crate's inner attrs, maybe (oops) one\n@@ -5375,9 +5393,9 @@ impl<'a> Parser<'a> {\n     pub fn parse_optional_str(&mut self)\n                               -> Option<(InternedString, ast::StrStyle)> {\n         let (s, style) = match self.token {\n-            token::LIT_STR(s) => (self.id_to_interned_str(s), ast::CookedStr),\n+            token::LIT_STR(s) => (self.id_to_interned_str(s.ident()), ast::CookedStr),\n             token::LIT_STR_RAW(s, n) => {\n-                (self.id_to_interned_str(s), ast::RawStr(n))\n+                (self.id_to_interned_str(s.ident()), ast::RawStr(n))\n             }\n             _ => return None\n         };\n@@ -5392,3 +5410,4 @@ impl<'a> Parser<'a> {\n         }\n     }\n }\n+"}, {"sha": "5839df6702245d0a8a89e241b45a435262ae5b07", "filename": "src/libsyntax/parse/token.rs", "status": "modified", "additions": 93, "deletions": 103, "changes": 196, "blob_url": "https://github.com/rust-lang/rust/blob/ad3eda123a00b3ddf8897295ff26d97ba7c64d44/src%2Flibsyntax%2Fparse%2Ftoken.rs", "raw_url": "https://github.com/rust-lang/rust/raw/ad3eda123a00b3ddf8897295ff26d97ba7c64d44/src%2Flibsyntax%2Fparse%2Ftoken.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Ftoken.rs?ref=ad3eda123a00b3ddf8897295ff26d97ba7c64d44", "patch": "@@ -10,7 +10,6 @@\n \n use ast;\n use ast::{P, Ident, Name, Mrk};\n-use ast_util;\n use ext::mtwt;\n use parse::token;\n use util::interner::{RcStr, StrInterner};\n@@ -79,30 +78,37 @@ pub enum Token {\n     QUESTION,\n \n     /* Literals */\n-    LIT_BYTE(u8),\n-    LIT_CHAR(char),\n-    LIT_INT(i64, ast::IntTy),\n-    LIT_UINT(u64, ast::UintTy),\n-    LIT_INT_UNSUFFIXED(i64),\n-    LIT_FLOAT(ast::Ident, ast::FloatTy),\n-    LIT_FLOAT_UNSUFFIXED(ast::Ident),\n-    LIT_STR(ast::Ident),\n-    LIT_STR_RAW(ast::Ident, uint), /* raw str delimited by n hash symbols */\n-    LIT_BINARY(Rc<Vec<u8>>),\n-    LIT_BINARY_RAW(Rc<Vec<u8>>, uint), /* raw binary str delimited by n hash symbols */\n+    LIT_BYTE(Name),\n+    LIT_CHAR(Name),\n+    LIT_INTEGER(Name),\n+    LIT_FLOAT(Name),\n+    LIT_STR(Name),\n+    LIT_STR_RAW(Name, uint), /* raw str delimited by n hash symbols */\n+    LIT_BINARY(Name),\n+    LIT_BINARY_RAW(Name, uint), /* raw binary str delimited by n hash symbols */\n \n     /* Name components */\n-    // an identifier contains an \"is_mod_name\" boolean,\n-    // indicating whether :: follows this token with no\n-    // whitespace in between.\n-    IDENT(ast::Ident, bool),\n+    /// An identifier contains an \"is_mod_name\" boolean,\n+    /// indicating whether :: follows this token with no\n+    /// whitespace in between.\n+    IDENT(Ident, bool),\n     UNDERSCORE,\n-    LIFETIME(ast::Ident),\n+    LIFETIME(Ident),\n \n     /* For interpolation */\n     INTERPOLATED(Nonterminal),\n+    DOC_COMMENT(Name),\n+\n+    // Junk. These carry no data because we don't really care about the data\n+    // they *would* carry, and don't really want to allocate a new ident for\n+    // them. Instead, users could extract that from the associated span.\n+\n+    /// Whitespace\n+    WS,\n+    /// Comment\n+    COMMENT,\n+    SHEBANG(Name),\n \n-    DOC_COMMENT(ast::Ident),\n     EOF,\n }\n \n@@ -115,11 +121,12 @@ pub enum Nonterminal {\n     NtPat( Gc<ast::Pat>),\n     NtExpr(Gc<ast::Expr>),\n     NtTy(  P<ast::Ty>),\n-    // see IDENT, above, for meaning of bool in NtIdent:\n-    NtIdent(Box<ast::Ident>, bool),\n-    NtMeta(Gc<ast::MetaItem>), // stuff inside brackets for attributes\n+    /// See IDENT, above, for meaning of bool in NtIdent:\n+    NtIdent(Box<Ident>, bool),\n+    /// Stuff inside brackets for attributes\n+    NtMeta(Gc<ast::MetaItem>),\n     NtPath(Box<ast::Path>),\n-    NtTT(  Gc<ast::TokenTree>), // needs @ed to break a circularity\n+    NtTT(  Gc<ast::TokenTree>), // needs Gc'd to break a circularity\n     NtMatchers(Vec<ast::Matcher> )\n }\n \n@@ -200,54 +207,28 @@ pub fn to_string(t: &Token) -> String {\n \n       /* Literals */\n       LIT_BYTE(b) => {\n-          let mut res = String::from_str(\"b'\");\n-          (b as char).escape_default(|c| {\n-              res.push_char(c);\n-          });\n-          res.push_char('\\'');\n-          res\n+          format!(\"b'{}'\", b.as_str())\n       }\n       LIT_CHAR(c) => {\n-          let mut res = String::from_str(\"'\");\n-          c.escape_default(|c| {\n-              res.push_char(c);\n-          });\n-          res.push_char('\\'');\n-          res\n-      }\n-      LIT_INT(i, t) => ast_util::int_ty_to_string(t, Some(i)),\n-      LIT_UINT(u, t) => ast_util::uint_ty_to_string(t, Some(u)),\n-      LIT_INT_UNSUFFIXED(i) => { (i as u64).to_string() }\n-      LIT_FLOAT(s, t) => {\n-        let mut body = String::from_str(get_ident(s).get());\n-        if body.as_slice().ends_with(\".\") {\n-            body.push_char('0');  // `10.f` is not a float literal\n-        }\n-        body.push_str(ast_util::float_ty_to_string(t).as_slice());\n-        body\n+          format!(\"'{}'\", c.as_str())\n       }\n-      LIT_FLOAT_UNSUFFIXED(s) => {\n-        let mut body = String::from_str(get_ident(s).get());\n-        if body.as_slice().ends_with(\".\") {\n-            body.push_char('0');  // `10.f` is not a float literal\n-        }\n-        body\n+      LIT_INTEGER(c) | LIT_FLOAT(c) => {\n+          c.as_str().to_string()\n       }\n+\n       LIT_STR(s) => {\n-          format!(\"\\\"{}\\\"\", get_ident(s).get().escape_default())\n+          format!(\"\\\"{}\\\"\", s.as_str())\n       }\n       LIT_STR_RAW(s, n) => {\n         format!(\"r{delim}\\\"{string}\\\"{delim}\",\n-                 delim=\"#\".repeat(n), string=get_ident(s))\n+                 delim=\"#\".repeat(n), string=s.as_str())\n       }\n-      LIT_BINARY(ref v) => {\n-          format!(\n-            \"b\\\"{}\\\"\",\n-            v.iter().map(|&b| b as char).collect::<String>().escape_default())\n+      LIT_BINARY(v) => {\n+          format!(\"b\\\"{}\\\"\", v.as_str())\n       }\n-      LIT_BINARY_RAW(ref s, n) => {\n+      LIT_BINARY_RAW(s, n) => {\n         format!(\"br{delim}\\\"{string}\\\"{delim}\",\n-                 delim=\"#\".repeat(n), string=s.as_slice().to_ascii().as_str_ascii())\n+                 delim=\"#\".repeat(n), string=s.as_str())\n       }\n \n       /* Name components */\n@@ -258,8 +239,12 @@ pub fn to_string(t: &Token) -> String {\n       UNDERSCORE => \"_\".to_string(),\n \n       /* Other */\n-      DOC_COMMENT(s) => get_ident(s).get().to_string(),\n+      DOC_COMMENT(s) => s.as_str().to_string(),\n       EOF => \"<eof>\".to_string(),\n+      WS => \" \".to_string(),\n+      COMMENT => \"/* */\".to_string(),\n+      SHEBANG(s) => format!(\"/* shebang: {}*/\", s.as_str()),\n+\n       INTERPOLATED(ref nt) => {\n         match nt {\n             &NtExpr(ref e) => ::print::pprust::expr_to_string(&**e),\n@@ -296,11 +281,8 @@ pub fn can_begin_expr(t: &Token) -> bool {\n       TILDE => true,\n       LIT_BYTE(_) => true,\n       LIT_CHAR(_) => true,\n-      LIT_INT(_, _) => true,\n-      LIT_UINT(_, _) => true,\n-      LIT_INT_UNSUFFIXED(_) => true,\n-      LIT_FLOAT(_, _) => true,\n-      LIT_FLOAT_UNSUFFIXED(_) => true,\n+      LIT_INTEGER(_) => true,\n+      LIT_FLOAT(_) => true,\n       LIT_STR(_) => true,\n       LIT_STR_RAW(_, _) => true,\n       LIT_BINARY(_) => true,\n@@ -337,11 +319,8 @@ pub fn is_lit(t: &Token) -> bool {\n     match *t {\n       LIT_BYTE(_) => true,\n       LIT_CHAR(_) => true,\n-      LIT_INT(_, _) => true,\n-      LIT_UINT(_, _) => true,\n-      LIT_INT_UNSUFFIXED(_) => true,\n-      LIT_FLOAT(_, _) => true,\n-      LIT_FLOAT_UNSUFFIXED(_) => true,\n+      LIT_INTEGER(_) => true,\n+      LIT_FLOAT(_) => true,\n       LIT_STR(_) => true,\n       LIT_STR_RAW(_, _) => true,\n       LIT_BINARY(_) => true,\n@@ -395,19 +374,19 @@ macro_rules! declare_special_idents_and_keywords {(\n         $( ($rk_name:expr, $rk_variant:ident, $rk_str:expr); )*\n     }\n ) => {\n-    static STRICT_KEYWORD_START: Name = first!($( $sk_name, )*);\n-    static STRICT_KEYWORD_FINAL: Name = last!($( $sk_name, )*);\n-    static RESERVED_KEYWORD_START: Name = first!($( $rk_name, )*);\n-    static RESERVED_KEYWORD_FINAL: Name = last!($( $rk_name, )*);\n+    static STRICT_KEYWORD_START: Name = first!($( Name($sk_name), )*);\n+    static STRICT_KEYWORD_FINAL: Name = last!($( Name($sk_name), )*);\n+    static RESERVED_KEYWORD_START: Name = first!($( Name($rk_name), )*);\n+    static RESERVED_KEYWORD_FINAL: Name = last!($( Name($rk_name), )*);\n \n     pub mod special_idents {\n-        use ast::Ident;\n-        $( pub static $si_static: Ident = Ident { name: $si_name, ctxt: 0 }; )*\n+        use ast::{Ident, Name};\n+        $( pub static $si_static: Ident = Ident { name: Name($si_name), ctxt: 0 }; )*\n     }\n \n     pub mod special_names {\n         use ast::Name;\n-        $( pub static $si_static: Name =  $si_name; )*\n+        $( pub static $si_static: Name =  Name($si_name); )*\n     }\n \n     /**\n@@ -428,8 +407,8 @@ macro_rules! declare_special_idents_and_keywords {(\n         impl Keyword {\n             pub fn to_name(&self) -> Name {\n                 match *self {\n-                    $( $sk_variant => $sk_name, )*\n-                    $( $rk_variant => $rk_name, )*\n+                    $( $sk_variant => Name($sk_name), )*\n+                    $( $rk_variant => Name($rk_name), )*\n                 }\n             }\n         }\n@@ -448,8 +427,11 @@ macro_rules! declare_special_idents_and_keywords {(\n }}\n \n // If the special idents get renumbered, remember to modify these two as appropriate\n-pub static SELF_KEYWORD_NAME: Name = 1;\n-static STATIC_KEYWORD_NAME: Name = 2;\n+pub static SELF_KEYWORD_NAME: Name = Name(SELF_KEYWORD_NAME_NUM);\n+static STATIC_KEYWORD_NAME: Name = Name(STATIC_KEYWORD_NAME_NUM);\n+\n+pub static SELF_KEYWORD_NAME_NUM: u32 = 1;\n+static STATIC_KEYWORD_NAME_NUM: u32 = 2;\n \n // NB: leaving holes in the ident table is bad! a different ident will get\n // interned with the id from the hole, but it will be between the min and max\n@@ -459,8 +441,8 @@ declare_special_idents_and_keywords! {\n     pub mod special_idents {\n         // These ones are statics\n         (0,                          invalid,                \"\");\n-        (super::SELF_KEYWORD_NAME,   self_,                  \"self\");\n-        (super::STATIC_KEYWORD_NAME, statik,                 \"static\");\n+        (super::SELF_KEYWORD_NAME_NUM,   self_,                  \"self\");\n+        (super::STATIC_KEYWORD_NAME_NUM, statik,                 \"static\");\n         (3,                          static_lifetime,        \"'static\");\n \n         // for matcher NTs\n@@ -500,8 +482,8 @@ declare_special_idents_and_keywords! {\n         (29,                         Ref,        \"ref\");\n         (30,                         Return,     \"return\");\n         // Static and Self are also special idents (prefill de-dupes)\n-        (super::STATIC_KEYWORD_NAME, Static,     \"static\");\n-        (super::SELF_KEYWORD_NAME,   Self,       \"self\");\n+        (super::STATIC_KEYWORD_NAME_NUM, Static,     \"static\");\n+        (super::SELF_KEYWORD_NAME_NUM,   Self,       \"self\");\n         (31,                         Struct,     \"struct\");\n         (32,                         Super,      \"super\");\n         (33,                         True,       \"true\");\n@@ -683,20 +665,20 @@ pub fn gensym(s: &str) -> Name {\n \n /// Maps a string to an identifier with an empty syntax context.\n #[inline]\n-pub fn str_to_ident(s: &str) -> ast::Ident {\n-    ast::Ident::new(intern(s))\n+pub fn str_to_ident(s: &str) -> Ident {\n+    Ident::new(intern(s))\n }\n \n /// Maps a string to a gensym'ed identifier.\n #[inline]\n-pub fn gensym_ident(s: &str) -> ast::Ident {\n-    ast::Ident::new(gensym(s))\n+pub fn gensym_ident(s: &str) -> Ident {\n+    Ident::new(gensym(s))\n }\n \n // create a fresh name that maps to the same string as the old one.\n // note that this guarantees that str_ptr_eq(ident_to_string(src),interner_get(fresh_name(src)));\n // that is, that the new name and the old one are connected to ptr_eq strings.\n-pub fn fresh_name(src: &ast::Ident) -> Name {\n+pub fn fresh_name(src: &Ident) -> Name {\n     let interner = get_ident_interner();\n     interner.gensym_copy(src.name)\n     // following: debug version. Could work in final except that it's incompatible with\n@@ -708,7 +690,7 @@ pub fn fresh_name(src: &ast::Ident) -> Name {\n \n // create a fresh mark.\n pub fn fresh_mark() -> Mrk {\n-    gensym(\"mark\")\n+    gensym(\"mark\").uint() as u32\n }\n \n // See the macro above about the types of keywords\n@@ -722,31 +704,39 @@ pub fn is_keyword(kw: keywords::Keyword, tok: &Token) -> bool {\n \n pub fn is_any_keyword(tok: &Token) -> bool {\n     match *tok {\n-        token::IDENT(sid, false) => match sid.name {\n-            SELF_KEYWORD_NAME | STATIC_KEYWORD_NAME |\n-            STRICT_KEYWORD_START .. RESERVED_KEYWORD_FINAL => true,\n-            _ => false,\n+        token::IDENT(sid, false) => {\n+            let n = sid.name;\n+\n+               n == SELF_KEYWORD_NAME\n+            || n == STATIC_KEYWORD_NAME\n+            || STRICT_KEYWORD_START <= n\n+            && n <= RESERVED_KEYWORD_FINAL\n         },\n         _ => false\n     }\n }\n \n pub fn is_strict_keyword(tok: &Token) -> bool {\n     match *tok {\n-        token::IDENT(sid, false) => match sid.name {\n-            SELF_KEYWORD_NAME | STATIC_KEYWORD_NAME |\n-            STRICT_KEYWORD_START .. STRICT_KEYWORD_FINAL => true,\n-            _ => false,\n+        token::IDENT(sid, false) => {\n+            let n = sid.name;\n+\n+               n == SELF_KEYWORD_NAME\n+            || n == STATIC_KEYWORD_NAME\n+            || STRICT_KEYWORD_START <= n\n+            && n <= STRICT_KEYWORD_FINAL\n         },\n         _ => false,\n     }\n }\n \n pub fn is_reserved_keyword(tok: &Token) -> bool {\n     match *tok {\n-        token::IDENT(sid, false) => match sid.name {\n-            RESERVED_KEYWORD_START .. RESERVED_KEYWORD_FINAL => true,\n-            _ => false,\n+        token::IDENT(sid, false) => {\n+            let n = sid.name;\n+\n+               RESERVED_KEYWORD_START <= n\n+            && n <= RESERVED_KEYWORD_FINAL\n         },\n         _ => false,\n     }\n@@ -768,7 +758,7 @@ mod test {\n     use ext::mtwt;\n \n     fn mark_ident(id : ast::Ident, m : ast::Mrk) -> ast::Ident {\n-        ast::Ident{name:id.name,ctxt:mtwt::apply_mark(m,id.ctxt)}\n+        ast::Ident { name: id.name, ctxt:mtwt::apply_mark(m, id.ctxt) }\n     }\n \n     #[test] fn mtwt_token_eq_test() {"}, {"sha": "fe84eeff4f87faa4b1248903a9eb26583a1e52db", "filename": "src/libsyntax/print/pp.rs", "status": "modified", "additions": 155, "deletions": 148, "changes": 303, "blob_url": "https://github.com/rust-lang/rust/blob/ad3eda123a00b3ddf8897295ff26d97ba7c64d44/src%2Flibsyntax%2Fprint%2Fpp.rs", "raw_url": "https://github.com/rust-lang/rust/raw/ad3eda123a00b3ddf8897295ff26d97ba7c64d44/src%2Flibsyntax%2Fprint%2Fpp.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fprint%2Fpp.rs?ref=ad3eda123a00b3ddf8897295ff26d97ba7c64d44", "patch": "@@ -8,58 +8,56 @@\n // option. This file may not be copied, modified, or distributed\n // except according to those terms.\n \n-/*\n- * This pretty-printer is a direct reimplementation of Philip Karlton's\n- * Mesa pretty-printer, as described in appendix A of\n- *\n- *     STAN-CS-79-770: \"Pretty Printing\", by Derek C. Oppen.\n- *     Stanford Department of Computer Science, 1979.\n- *\n- * The algorithm's aim is to break a stream into as few lines as possible\n- * while respecting the indentation-consistency requirements of the enclosing\n- * block, and avoiding breaking at silly places on block boundaries, for\n- * example, between \"x\" and \")\" in \"x)\".\n- *\n- * I am implementing this algorithm because it comes with 20 pages of\n- * documentation explaining its theory, and because it addresses the set of\n- * concerns I've seen other pretty-printers fall down on. Weirdly. Even though\n- * it's 32 years old. What can I say?\n- *\n- * Despite some redundancies and quirks in the way it's implemented in that\n- * paper, I've opted to keep the implementation here as similar as I can,\n- * changing only what was blatantly wrong, a typo, or sufficiently\n- * non-idiomatic rust that it really stuck out.\n- *\n- * In particular you'll see a certain amount of churn related to INTEGER vs.\n- * CARDINAL in the Mesa implementation. Mesa apparently interconverts the two\n- * somewhat readily? In any case, I've used uint for indices-in-buffers and\n- * ints for character-sizes-and-indentation-offsets. This respects the need\n- * for ints to \"go negative\" while carrying a pending-calculation balance, and\n- * helps differentiate all the numbers flying around internally (slightly).\n- *\n- * I also inverted the indentation arithmetic used in the print stack, since\n- * the Mesa implementation (somewhat randomly) stores the offset on the print\n- * stack in terms of margin-col rather than col itself. I store col.\n- *\n- * I also implemented a small change in the String token, in that I store an\n- * explicit length for the string. For most tokens this is just the length of\n- * the accompanying string. But it's necessary to permit it to differ, for\n- * encoding things that are supposed to \"go on their own line\" -- certain\n- * classes of comment and blank-line -- where relying on adjacent\n- * hardbreak-like Break tokens with long blankness indication doesn't actually\n- * work. To see why, consider when there is a \"thing that should be on its own\n- * line\" between two long blocks, say functions. If you put a hardbreak after\n- * each function (or before each) and the breaking algorithm decides to break\n- * there anyways (because the functions themselves are long) you wind up with\n- * extra blank lines. If you don't put hardbreaks you can wind up with the\n- * \"thing which should be on its own line\" not getting its own line in the\n- * rare case of \"really small functions\" or such. This re-occurs with comments\n- * and explicit blank lines. So in those cases we use a string with a payload\n- * we want isolated to a line and an explicit length that's huge, surrounded\n- * by two zero-length breaks. The algorithm will try its best to fit it on a\n- * line (which it can't) and so naturally place the content on its own line to\n- * avoid combining it with other lines and making matters even worse.\n- */\n+//! This pretty-printer is a direct reimplementation of Philip Karlton's\n+//! Mesa pretty-printer, as described in appendix A of\n+//!\n+//!     STAN-CS-79-770: \"Pretty Printing\", by Derek C. Oppen.\n+//!     Stanford Department of Computer Science, 1979.\n+//!\n+//! The algorithm's aim is to break a stream into as few lines as possible\n+//! while respecting the indentation-consistency requirements of the enclosing\n+//! block, and avoiding breaking at silly places on block boundaries, for\n+//! example, between \"x\" and \")\" in \"x)\".\n+//!\n+//! I am implementing this algorithm because it comes with 20 pages of\n+//! documentation explaining its theory, and because it addresses the set of\n+//! concerns I've seen other pretty-printers fall down on. Weirdly. Even though\n+//! it's 32 years old. What can I say?\n+//!\n+//! Despite some redundancies and quirks in the way it's implemented in that\n+//! paper, I've opted to keep the implementation here as similar as I can,\n+//! changing only what was blatantly wrong, a typo, or sufficiently\n+//! non-idiomatic rust that it really stuck out.\n+//!\n+//! In particular you'll see a certain amount of churn related to INTEGER vs.\n+//! CARDINAL in the Mesa implementation. Mesa apparently interconverts the two\n+//! somewhat readily? In any case, I've used uint for indices-in-buffers and\n+//! ints for character-sizes-and-indentation-offsets. This respects the need\n+//! for ints to \"go negative\" while carrying a pending-calculation balance, and\n+//! helps differentiate all the numbers flying around internally (slightly).\n+//!\n+//! I also inverted the indentation arithmetic used in the print stack, since\n+//! the Mesa implementation (somewhat randomly) stores the offset on the print\n+//! stack in terms of margin-col rather than col itself. I store col.\n+//!\n+//! I also implemented a small change in the String token, in that I store an\n+//! explicit length for the string. For most tokens this is just the length of\n+//! the accompanying string. But it's necessary to permit it to differ, for\n+//! encoding things that are supposed to \"go on their own line\" -- certain\n+//! classes of comment and blank-line -- where relying on adjacent\n+//! hardbreak-like Break tokens with long blankness indication doesn't actually\n+//! work. To see why, consider when there is a \"thing that should be on its own\n+//! line\" between two long blocks, say functions. If you put a hardbreak after\n+//! each function (or before each) and the breaking algorithm decides to break\n+//! there anyways (because the functions themselves are long) you wind up with\n+//! extra blank lines. If you don't put hardbreaks you can wind up with the\n+//! \"thing which should be on its own line\" not getting its own line in the\n+//! rare case of \"really small functions\" or such. This re-occurs with comments\n+//! and explicit blank lines. So in those cases we use a string with a payload\n+//! we want isolated to a line and an explicit length that's huge, surrounded\n+//! by two zero-length breaks. The algorithm will try its best to fit it on a\n+//! line (which it can't) and so naturally place the content on its own line to\n+//! avoid combining it with other lines and making matters even worse.\n \n use std::io;\n use std::string::String;\n@@ -186,107 +184,116 @@ pub fn mk_printer(out: Box<io::Writer>, linewidth: uint) -> Printer {\n }\n \n \n-/*\n- * In case you do not have the paper, here is an explanation of what's going\n- * on.\n- *\n- * There is a stream of input tokens flowing through this printer.\n- *\n- * The printer buffers up to 3N tokens inside itself, where N is linewidth.\n- * Yes, linewidth is chars and tokens are multi-char, but in the worst\n- * case every token worth buffering is 1 char long, so it's ok.\n- *\n- * Tokens are String, Break, and Begin/End to delimit blocks.\n- *\n- * Begin tokens can carry an offset, saying \"how far to indent when you break\n- * inside here\", as well as a flag indicating \"consistent\" or \"inconsistent\"\n- * breaking. Consistent breaking means that after the first break, no attempt\n- * will be made to flow subsequent breaks together onto lines. Inconsistent\n- * is the opposite. Inconsistent breaking example would be, say:\n- *\n- *  foo(hello, there, good, friends)\n- *\n- * breaking inconsistently to become\n- *\n- *  foo(hello, there\n- *      good, friends);\n- *\n- * whereas a consistent breaking would yield:\n- *\n- *  foo(hello,\n- *      there\n- *      good,\n- *      friends);\n- *\n- * That is, in the consistent-break blocks we value vertical alignment\n- * more than the ability to cram stuff onto a line. But in all cases if it\n- * can make a block a one-liner, it'll do so.\n- *\n- * Carrying on with high-level logic:\n- *\n- * The buffered tokens go through a ring-buffer, 'tokens'. The 'left' and\n- * 'right' indices denote the active portion of the ring buffer as well as\n- * describing hypothetical points-in-the-infinite-stream at most 3N tokens\n- * apart (i.e. \"not wrapped to ring-buffer boundaries\"). The paper will switch\n- * between using 'left' and 'right' terms to denote the wrapepd-to-ring-buffer\n- * and point-in-infinite-stream senses freely.\n- *\n- * There is a parallel ring buffer, 'size', that holds the calculated size of\n- * each token. Why calculated? Because for Begin/End pairs, the \"size\"\n- * includes everything between the pair. That is, the \"size\" of Begin is\n- * actually the sum of the sizes of everything between Begin and the paired\n- * End that follows. Since that is arbitrarily far in the future, 'size' is\n- * being rewritten regularly while the printer runs; in fact most of the\n- * machinery is here to work out 'size' entries on the fly (and give up when\n- * they're so obviously over-long that \"infinity\" is a good enough\n- * approximation for purposes of line breaking).\n- *\n- * The \"input side\" of the printer is managed as an abstract process called\n- * SCAN, which uses 'scan_stack', 'scan_stack_empty', 'top' and 'bottom', to\n- * manage calculating 'size'. SCAN is, in other words, the process of\n- * calculating 'size' entries.\n- *\n- * The \"output side\" of the printer is managed by an abstract process called\n- * PRINT, which uses 'print_stack', 'margin' and 'space' to figure out what to\n- * do with each token/size pair it consumes as it goes. It's trying to consume\n- * the entire buffered window, but can't output anything until the size is >=\n- * 0 (sizes are set to negative while they're pending calculation).\n- *\n- * So SCAN takes input and buffers tokens and pending calculations, while\n- * PRINT gobbles up completed calculations and tokens from the buffer. The\n- * theory is that the two can never get more than 3N tokens apart, because\n- * once there's \"obviously\" too much data to fit on a line, in a size\n- * calculation, SCAN will write \"infinity\" to the size and let PRINT consume\n- * it.\n- *\n- * In this implementation (following the paper, again) the SCAN process is\n- * the method called 'pretty_print', and the 'PRINT' process is the method\n- * called 'print'.\n- */\n+/// In case you do not have the paper, here is an explanation of what's going\n+/// on.\n+///\n+/// There is a stream of input tokens flowing through this printer.\n+///\n+/// The printer buffers up to 3N tokens inside itself, where N is linewidth.\n+/// Yes, linewidth is chars and tokens are multi-char, but in the worst\n+/// case every token worth buffering is 1 char long, so it's ok.\n+///\n+/// Tokens are String, Break, and Begin/End to delimit blocks.\n+///\n+/// Begin tokens can carry an offset, saying \"how far to indent when you break\n+/// inside here\", as well as a flag indicating \"consistent\" or \"inconsistent\"\n+/// breaking. Consistent breaking means that after the first break, no attempt\n+/// will be made to flow subsequent breaks together onto lines. Inconsistent\n+/// is the opposite. Inconsistent breaking example would be, say:\n+///\n+///  foo(hello, there, good, friends)\n+///\n+/// breaking inconsistently to become\n+///\n+///  foo(hello, there\n+///      good, friends);\n+///\n+/// whereas a consistent breaking would yield:\n+///\n+///  foo(hello,\n+///      there\n+///      good,\n+///      friends);\n+///\n+/// That is, in the consistent-break blocks we value vertical alignment\n+/// more than the ability to cram stuff onto a line. But in all cases if it\n+/// can make a block a one-liner, it'll do so.\n+///\n+/// Carrying on with high-level logic:\n+///\n+/// The buffered tokens go through a ring-buffer, 'tokens'. The 'left' and\n+/// 'right' indices denote the active portion of the ring buffer as well as\n+/// describing hypothetical points-in-the-infinite-stream at most 3N tokens\n+/// apart (i.e. \"not wrapped to ring-buffer boundaries\"). The paper will switch\n+/// between using 'left' and 'right' terms to denote the wrapepd-to-ring-buffer\n+/// and point-in-infinite-stream senses freely.\n+///\n+/// There is a parallel ring buffer, 'size', that holds the calculated size of\n+/// each token. Why calculated? Because for Begin/End pairs, the \"size\"\n+/// includes everything betwen the pair. That is, the \"size\" of Begin is\n+/// actually the sum of the sizes of everything between Begin and the paired\n+/// End that follows. Since that is arbitrarily far in the future, 'size' is\n+/// being rewritten regularly while the printer runs; in fact most of the\n+/// machinery is here to work out 'size' entries on the fly (and give up when\n+/// they're so obviously over-long that \"infinity\" is a good enough\n+/// approximation for purposes of line breaking).\n+///\n+/// The \"input side\" of the printer is managed as an abstract process called\n+/// SCAN, which uses 'scan_stack', 'scan_stack_empty', 'top' and 'bottom', to\n+/// manage calculating 'size'. SCAN is, in other words, the process of\n+/// calculating 'size' entries.\n+///\n+/// The \"output side\" of the printer is managed by an abstract process called\n+/// PRINT, which uses 'print_stack', 'margin' and 'space' to figure out what to\n+/// do with each token/size pair it consumes as it goes. It's trying to consume\n+/// the entire buffered window, but can't output anything until the size is >=\n+/// 0 (sizes are set to negative while they're pending calculation).\n+///\n+/// So SCAN takes input and buffers tokens and pending calculations, while\n+/// PRINT gobbles up completed calculations and tokens from the buffer. The\n+/// theory is that the two can never get more than 3N tokens apart, because\n+/// once there's \"obviously\" too much data to fit on a line, in a size\n+/// calculation, SCAN will write \"infinity\" to the size and let PRINT consume\n+/// it.\n+///\n+/// In this implementation (following the paper, again) the SCAN process is\n+/// the method called 'pretty_print', and the 'PRINT' process is the method\n+/// called 'print'.\n pub struct Printer {\n     pub out: Box<io::Writer>,\n     buf_len: uint,\n-    margin: int, // width of lines we're constrained to\n-    space: int, // number of spaces left on line\n-    left: uint, // index of left side of input stream\n-    right: uint, // index of right side of input stream\n-    token: Vec<Token> , // ring-buffr stream goes through\n-    size: Vec<int> , // ring-buffer of calculated sizes\n-    left_total: int, // running size of stream \"...left\"\n-    right_total: int, // running size of stream \"...right\"\n-    // pseudo-stack, really a ring too. Holds the\n-    // primary-ring-buffers index of the Begin that started the\n-    // current block, possibly with the most recent Break after that\n-    // Begin (if there is any) on top of it. Stuff is flushed off the\n-    // bottom as it becomes irrelevant due to the primary ring-buffer\n-    // advancing.\n+    /// Width of lines we're constrained to\n+    margin: int,\n+    /// Number of spaces left on line\n+    space: int,\n+    /// Index of left side of input stream\n+    left: uint,\n+    /// Index of right side of input stream\n+    right: uint,\n+    /// Ring-buffr stream goes through\n+    token: Vec<Token> ,\n+    /// Ring-buffer of calculated sizes\n+    size: Vec<int> ,\n+    /// Running size of stream \"...left\"\n+    left_total: int,\n+    /// Running size of stream \"...right\"\n+    right_total: int,\n+    /// Pseudo-stack, really a ring too. Holds the\n+    /// primary-ring-buffers index of the Begin that started the\n+    /// current block, possibly with the most recent Break after that\n+    /// Begin (if there is any) on top of it. Stuff is flushed off the\n+    /// bottom as it becomes irrelevant due to the primary ring-buffer\n+    /// advancing.\n     scan_stack: Vec<uint> ,\n-    scan_stack_empty: bool, // top==bottom disambiguator\n-    top: uint, // index of top of scan_stack\n-    bottom: uint, // index of bottom of scan_stack\n-    // stack of blocks-in-progress being flushed by print\n+    /// Top==bottom disambiguator\n+    scan_stack_empty: bool,\n+    /// Index of top of scan_stack\n+    top: uint,\n+    /// Index of bottom of scan_stack\n+    bottom: uint,\n+    /// Stack of blocks-in-progress being flushed by print\n     print_stack: Vec<PrintStackElem> ,\n-    // buffered indentation to avoid writing trailing whitespace\n+    /// Buffered indentation to avoid writing trailing whitespace\n     pending_indentation: int,\n }\n "}, {"sha": "170cb7a249c4b697c72990d3b04ff0c62772404f", "filename": "src/libsyntax/print/pprust.rs", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "blob_url": "https://github.com/rust-lang/rust/blob/ad3eda123a00b3ddf8897295ff26d97ba7c64d44/src%2Flibsyntax%2Fprint%2Fpprust.rs", "raw_url": "https://github.com/rust-lang/rust/raw/ad3eda123a00b3ddf8897295ff26d97ba7c64d44/src%2Flibsyntax%2Fprint%2Fpprust.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fprint%2Fpprust.rs?ref=ad3eda123a00b3ddf8897295ff26d97ba7c64d44", "patch": "@@ -88,9 +88,9 @@ pub static indent_unit: uint = 4u;\n \n pub static default_columns: uint = 78u;\n \n-// Requires you to pass an input filename and reader so that\n-// it can scan the input text for comments and literals to\n-// copy forward.\n+/// Requires you to pass an input filename and reader so that\n+/// it can scan the input text for comments and literals to\n+/// copy forward.\n pub fn print_crate<'a>(cm: &'a CodeMap,\n                        span_diagnostic: &diagnostic::SpanHandler,\n                        krate: &ast::Crate,"}, {"sha": "452b5a5251222ca1a22b1c092c0c52848094e0f9", "filename": "src/libsyntax/util/interner.rs", "status": "modified", "additions": 49, "deletions": 47, "changes": 96, "blob_url": "https://github.com/rust-lang/rust/blob/ad3eda123a00b3ddf8897295ff26d97ba7c64d44/src%2Flibsyntax%2Futil%2Finterner.rs", "raw_url": "https://github.com/rust-lang/rust/raw/ad3eda123a00b3ddf8897295ff26d97ba7c64d44/src%2Flibsyntax%2Futil%2Finterner.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Futil%2Finterner.rs?ref=ad3eda123a00b3ddf8897295ff26d97ba7c64d44", "patch": "@@ -8,9 +8,9 @@\n // option. This file may not be copied, modified, or distributed\n // except according to those terms.\n \n-// An \"interner\" is a data structure that associates values with uint tags and\n-// allows bidirectional lookup; i.e. given a value, one can easily find the\n-// type, and vice versa.\n+//! An \"interner\" is a data structure that associates values with uint tags and\n+//! allows bidirectional lookup; i.e. given a value, one can easily find the\n+//! type, and vice versa.\n \n use ast::Name;\n \n@@ -52,23 +52,23 @@ impl<T: Eq + Hash + Clone + 'static> Interner<T> {\n         }\n \n         let mut vect = self.vect.borrow_mut();\n-        let new_idx = (*vect).len() as Name;\n+        let new_idx = Name((*vect).len() as u32);\n         (*map).insert(val.clone(), new_idx);\n         (*vect).push(val);\n         new_idx\n     }\n \n     pub fn gensym(&self, val: T) -> Name {\n         let mut vect = self.vect.borrow_mut();\n-        let new_idx = (*vect).len() as Name;\n+        let new_idx = Name((*vect).len() as u32);\n         // leave out of .map to avoid colliding\n         (*vect).push(val);\n         new_idx\n     }\n \n     pub fn get(&self, idx: Name) -> T {\n         let vect = self.vect.borrow();\n-        (*(*vect).get(idx as uint)).clone()\n+        (*(*vect).get(idx.uint())).clone()\n     }\n \n     pub fn len(&self) -> uint {\n@@ -155,15 +155,15 @@ impl StrInterner {\n             None => (),\n         }\n \n-        let new_idx = self.len() as Name;\n+        let new_idx = Name(self.len() as u32);\n         let val = RcStr::new(val);\n         map.insert(val.clone(), new_idx);\n         self.vect.borrow_mut().push(val);\n         new_idx\n     }\n \n     pub fn gensym(&self, val: &str) -> Name {\n-        let new_idx = self.len() as Name;\n+        let new_idx = Name(self.len() as u32);\n         // leave out of .map to avoid colliding\n         self.vect.borrow_mut().push(RcStr::new(val));\n         new_idx\n@@ -180,23 +180,23 @@ impl StrInterner {\n     /// Create a gensym with the same name as an existing\n     /// entry.\n     pub fn gensym_copy(&self, idx : Name) -> Name {\n-        let new_idx = self.len() as Name;\n+        let new_idx = Name(self.len() as u32);\n         // leave out of map to avoid colliding\n         let mut vect = self.vect.borrow_mut();\n-        let existing = (*vect.get(idx as uint)).clone();\n+        let existing = (*vect.get(idx.uint())).clone();\n         vect.push(existing);\n         new_idx\n     }\n \n     pub fn get(&self, idx: Name) -> RcStr {\n-        (*self.vect.borrow().get(idx as uint)).clone()\n+        (*self.vect.borrow().get(idx.uint())).clone()\n     }\n \n     /// Returns this string with lifetime tied to the interner. Since\n     /// strings may never be removed from the interner, this is safe.\n     pub fn get_ref<'a>(&'a self, idx: Name) -> &'a str {\n         let vect = self.vect.borrow();\n-        let s: &str = vect.get(idx as uint).as_slice();\n+        let s: &str = vect.get(idx.uint()).as_slice();\n         unsafe {\n             mem::transmute(s)\n         }\n@@ -222,36 +222,38 @@ impl StrInterner {\n #[cfg(test)]\n mod tests {\n     use super::*;\n+    use ast::Name;\n+\n     #[test]\n     #[should_fail]\n     fn i1 () {\n         let i : Interner<RcStr> = Interner::new();\n-        i.get(13);\n+        i.get(Name(13));\n     }\n \n     #[test]\n     fn interner_tests () {\n         let i : Interner<RcStr> = Interner::new();\n         // first one is zero:\n-        assert_eq!(i.intern(RcStr::new(\"dog\")), 0);\n+        assert_eq!(i.intern(RcStr::new(\"dog\")), Name(0));\n         // re-use gets the same entry:\n-        assert_eq!(i.intern(RcStr::new(\"dog\")), 0);\n+        assert_eq!(i.intern(RcStr::new(\"dog\")), Name(0));\n         // different string gets a different #:\n-        assert_eq!(i.intern(RcStr::new(\"cat\")), 1);\n-        assert_eq!(i.intern(RcStr::new(\"cat\")), 1);\n+        assert_eq!(i.intern(RcStr::new(\"cat\")), Name(1));\n+        assert_eq!(i.intern(RcStr::new(\"cat\")), Name(1));\n         // dog is still at zero\n-        assert_eq!(i.intern(RcStr::new(\"dog\")), 0);\n+        assert_eq!(i.intern(RcStr::new(\"dog\")), Name(0));\n         // gensym gets 3\n-        assert_eq!(i.gensym(RcStr::new(\"zebra\") ), 2);\n+        assert_eq!(i.gensym(RcStr::new(\"zebra\") ), Name(2));\n         // gensym of same string gets new number :\n-        assert_eq!(i.gensym (RcStr::new(\"zebra\") ), 3);\n+        assert_eq!(i.gensym (RcStr::new(\"zebra\") ), Name(3));\n         // gensym of *existing* string gets new number:\n-        assert_eq!(i.gensym(RcStr::new(\"dog\")), 4);\n-        assert_eq!(i.get(0), RcStr::new(\"dog\"));\n-        assert_eq!(i.get(1), RcStr::new(\"cat\"));\n-        assert_eq!(i.get(2), RcStr::new(\"zebra\"));\n-        assert_eq!(i.get(3), RcStr::new(\"zebra\"));\n-        assert_eq!(i.get(4), RcStr::new(\"dog\"));\n+        assert_eq!(i.gensym(RcStr::new(\"dog\")), Name(4));\n+        assert_eq!(i.get(Name(0)), RcStr::new(\"dog\"));\n+        assert_eq!(i.get(Name(1)), RcStr::new(\"cat\"));\n+        assert_eq!(i.get(Name(2)), RcStr::new(\"zebra\"));\n+        assert_eq!(i.get(Name(3)), RcStr::new(\"zebra\"));\n+        assert_eq!(i.get(Name(4)), RcStr::new(\"dog\"));\n     }\n \n     #[test]\n@@ -261,39 +263,39 @@ mod tests {\n             RcStr::new(\"Bob\"),\n             RcStr::new(\"Carol\")\n         ]);\n-        assert_eq!(i.get(0), RcStr::new(\"Alan\"));\n-        assert_eq!(i.get(1), RcStr::new(\"Bob\"));\n-        assert_eq!(i.get(2), RcStr::new(\"Carol\"));\n-        assert_eq!(i.intern(RcStr::new(\"Bob\")), 1);\n+        assert_eq!(i.get(Name(0)), RcStr::new(\"Alan\"));\n+        assert_eq!(i.get(Name(1)), RcStr::new(\"Bob\"));\n+        assert_eq!(i.get(Name(2)), RcStr::new(\"Carol\"));\n+        assert_eq!(i.intern(RcStr::new(\"Bob\")), Name(1));\n     }\n \n     #[test]\n     fn string_interner_tests() {\n         let i : StrInterner = StrInterner::new();\n         // first one is zero:\n-        assert_eq!(i.intern(\"dog\"), 0);\n+        assert_eq!(i.intern(\"dog\"), Name(0));\n         // re-use gets the same entry:\n-        assert_eq!(i.intern (\"dog\"), 0);\n+        assert_eq!(i.intern (\"dog\"), Name(0));\n         // different string gets a different #:\n-        assert_eq!(i.intern(\"cat\"), 1);\n-        assert_eq!(i.intern(\"cat\"), 1);\n+        assert_eq!(i.intern(\"cat\"), Name(1));\n+        assert_eq!(i.intern(\"cat\"), Name(1));\n         // dog is still at zero\n-        assert_eq!(i.intern(\"dog\"), 0);\n+        assert_eq!(i.intern(\"dog\"), Name(0));\n         // gensym gets 3\n-        assert_eq!(i.gensym(\"zebra\"), 2);\n+        assert_eq!(i.gensym(\"zebra\"), Name(2));\n         // gensym of same string gets new number :\n-        assert_eq!(i.gensym(\"zebra\"), 3);\n+        assert_eq!(i.gensym(\"zebra\"), Name(3));\n         // gensym of *existing* string gets new number:\n-        assert_eq!(i.gensym(\"dog\"), 4);\n+        assert_eq!(i.gensym(\"dog\"), Name(4));\n         // gensym tests again with gensym_copy:\n-        assert_eq!(i.gensym_copy(2), 5);\n-        assert_eq!(i.get(5), RcStr::new(\"zebra\"));\n-        assert_eq!(i.gensym_copy(2), 6);\n-        assert_eq!(i.get(6), RcStr::new(\"zebra\"));\n-        assert_eq!(i.get(0), RcStr::new(\"dog\"));\n-        assert_eq!(i.get(1), RcStr::new(\"cat\"));\n-        assert_eq!(i.get(2), RcStr::new(\"zebra\"));\n-        assert_eq!(i.get(3), RcStr::new(\"zebra\"));\n-        assert_eq!(i.get(4), RcStr::new(\"dog\"));\n+        assert_eq!(i.gensym_copy(Name(2)), Name(5));\n+        assert_eq!(i.get(Name(5)), RcStr::new(\"zebra\"));\n+        assert_eq!(i.gensym_copy(Name(2)), Name(6));\n+        assert_eq!(i.get(Name(6)), RcStr::new(\"zebra\"));\n+        assert_eq!(i.get(Name(0)), RcStr::new(\"dog\"));\n+        assert_eq!(i.get(Name(1)), RcStr::new(\"cat\"));\n+        assert_eq!(i.get(Name(2)), RcStr::new(\"zebra\"));\n+        assert_eq!(i.get(Name(3)), RcStr::new(\"zebra\"));\n+        assert_eq!(i.get(Name(4)), RcStr::new(\"dog\"));\n     }\n }"}, {"sha": "f50739a7069e05a61d18496e35027ea242a77ef0", "filename": "src/libsyntax/util/parser_testing.rs", "status": "modified", "additions": 19, "deletions": 19, "changes": 38, "blob_url": "https://github.com/rust-lang/rust/blob/ad3eda123a00b3ddf8897295ff26d97ba7c64d44/src%2Flibsyntax%2Futil%2Fparser_testing.rs", "raw_url": "https://github.com/rust-lang/rust/raw/ad3eda123a00b3ddf8897295ff26d97ba7c64d44/src%2Flibsyntax%2Futil%2Fparser_testing.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Futil%2Fparser_testing.rs?ref=ad3eda123a00b3ddf8897295ff26d97ba7c64d44", "patch": "@@ -17,14 +17,14 @@ use parse::token;\n \n use std::gc::Gc;\n \n-// map a string to tts, using a made-up filename:\n+/// Map a string to tts, using a made-up filename:\n pub fn string_to_tts(source_str: String) -> Vec<ast::TokenTree> {\n     let ps = new_parse_sess();\n     filemap_to_tts(&ps,\n                    string_to_filemap(&ps, source_str, \"bogofile\".to_string()))\n }\n \n-// map string to parser (via tts)\n+/// Map string to parser (via tts)\n pub fn string_to_parser<'a>(ps: &'a ParseSess, source_str: String) -> Parser<'a> {\n     new_parser_from_source_str(ps,\n                                Vec::new(),\n@@ -40,51 +40,51 @@ fn with_error_checking_parse<T>(s: String, f: |&mut Parser| -> T) -> T {\n     x\n }\n \n-// parse a string, return a crate.\n+/// Parse a string, return a crate.\n pub fn string_to_crate (source_str : String) -> ast::Crate {\n     with_error_checking_parse(source_str, |p| {\n         p.parse_crate_mod()\n     })\n }\n \n-// parse a string, return an expr\n+/// Parse a string, return an expr\n pub fn string_to_expr (source_str : String) -> Gc<ast::Expr> {\n     with_error_checking_parse(source_str, |p| {\n         p.parse_expr()\n     })\n }\n \n-// parse a string, return an item\n+/// Parse a string, return an item\n pub fn string_to_item (source_str : String) -> Option<Gc<ast::Item>> {\n     with_error_checking_parse(source_str, |p| {\n         p.parse_item(Vec::new())\n     })\n }\n \n-// parse a string, return a stmt\n+/// Parse a string, return a stmt\n pub fn string_to_stmt(source_str : String) -> Gc<ast::Stmt> {\n     with_error_checking_parse(source_str, |p| {\n         p.parse_stmt(Vec::new())\n     })\n }\n \n-// parse a string, return a pat. Uses \"irrefutable\"... which doesn't\n-// (currently) affect parsing.\n+/// Parse a string, return a pat. Uses \"irrefutable\"... which doesn't\n+/// (currently) affect parsing.\n pub fn string_to_pat(source_str: String) -> Gc<ast::Pat> {\n     string_to_parser(&new_parse_sess(), source_str).parse_pat()\n }\n \n-// convert a vector of strings to a vector of ast::Ident's\n+/// Convert a vector of strings to a vector of ast::Ident's\n pub fn strs_to_idents(ids: Vec<&str> ) -> Vec<ast::Ident> {\n     ids.iter().map(|u| token::str_to_ident(*u)).collect()\n }\n \n-// does the given string match the pattern? whitespace in the first string\n-// may be deleted or replaced with other whitespace to match the pattern.\n-// this function is unicode-ignorant; fortunately, the careful design of\n-// UTF-8 mitigates this ignorance.  In particular, this function only collapses\n-// sequences of \\n, \\r, ' ', and \\t, but it should otherwise tolerate unicode\n-// chars. Unsurprisingly, it doesn't do NKF-normalization(?).\n+/// Does the given string match the pattern? whitespace in the first string\n+/// may be deleted or replaced with other whitespace to match the pattern.\n+/// this function is unicode-ignorant; fortunately, the careful design of\n+/// UTF-8 mitigates this ignorance.  In particular, this function only collapses\n+/// sequences of \\n, \\r, ' ', and \\t, but it should otherwise tolerate unicode\n+/// chars. Unsurprisingly, it doesn't do NKF-normalization(?).\n pub fn matches_codepattern(a : &str, b : &str) -> bool {\n     let mut idx_a = 0;\n     let mut idx_b = 0;\n@@ -122,9 +122,9 @@ pub fn matches_codepattern(a : &str, b : &str) -> bool {\n     }\n }\n \n-// given a string and an index, return the first uint >= idx\n-// that is a non-ws-char or is outside of the legal range of\n-// the string.\n+/// Given a string and an index, return the first uint >= idx\n+/// that is a non-ws-char or is outside of the legal range of\n+/// the string.\n fn scan_for_non_ws_or_end(a : &str, idx: uint) -> uint {\n     let mut i = idx;\n     let len = a.len();\n@@ -134,7 +134,7 @@ fn scan_for_non_ws_or_end(a : &str, idx: uint) -> uint {\n     i\n }\n \n-// copied from lexer.\n+/// Copied from lexer.\n pub fn is_whitespace(c: char) -> bool {\n     return c == ' ' || c == '\\t' || c == '\\r' || c == '\\n';\n }"}, {"sha": "9298b58c4267d9c2c3493f6b403e82946224f2a3", "filename": "src/libsyntax/visit.rs", "status": "modified", "additions": 16, "deletions": 16, "changes": 32, "blob_url": "https://github.com/rust-lang/rust/blob/ad3eda123a00b3ddf8897295ff26d97ba7c64d44/src%2Flibsyntax%2Fvisit.rs", "raw_url": "https://github.com/rust-lang/rust/raw/ad3eda123a00b3ddf8897295ff26d97ba7c64d44/src%2Flibsyntax%2Fvisit.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fvisit.rs?ref=ad3eda123a00b3ddf8897295ff26d97ba7c64d44", "patch": "@@ -8,6 +8,18 @@\n // option. This file may not be copied, modified, or distributed\n // except according to those terms.\n \n+//! Context-passing AST walker. Each overridden visit method has full control\n+//! over what happens with its node, it can do its own traversal of the node's\n+//! children (potentially passing in different contexts to each), call\n+//! `visit::visit_*` to apply the default traversal algorithm (again, it can\n+//! override the context), or prevent deeper traversal by doing nothing.\n+//!\n+//! Note: it is an important invariant that the default visitor walks the body\n+//! of a function in \"execution order\" (more concretely, reverse post-order\n+//! with respect to the CFG implied by the AST), meaning that if AST node A may\n+//! execute before AST node B, then A is visited first.  The borrow checker in\n+//! particular relies on this property.\n+//!\n use abi::Abi;\n use ast::*;\n use ast;\n@@ -17,27 +29,15 @@ use owned_slice::OwnedSlice;\n \n use std::gc::Gc;\n \n-// Context-passing AST walker. Each overridden visit method has full control\n-// over what happens with its node, it can do its own traversal of the node's\n-// children (potentially passing in different contexts to each), call\n-// visit::visit_* to apply the default traversal algorithm (again, it can\n-// override the context), or prevent deeper traversal by doing nothing.\n-//\n-// Note: it is an important invariant that the default visitor walks the body\n-// of a function in \"execution order\" (more concretely, reverse post-order\n-// with respect to the CFG implied by the AST), meaning that if AST node A may\n-// execute before AST node B, then A is visited first.  The borrow checker in\n-// particular relies on this property.\n-\n pub enum FnKind<'a> {\n-    // fn foo() or extern \"Abi\" fn foo()\n+    /// fn foo() or extern \"Abi\" fn foo()\n     FkItemFn(Ident, &'a Generics, FnStyle, Abi),\n \n-    // fn foo(&self)\n+    /// fn foo(&self)\n     FkMethod(Ident, &'a Generics, &'a Method),\n \n-    // |x, y| ...\n-    // proc(x, y) ...\n+    /// |x, y| ...\n+    /// proc(x, y) ...\n     FkFnBlock,\n }\n "}, {"sha": "0eaa81bd6ab95cf99c71f800812188ed6e4bf269", "filename": "src/test/compile-fail/lex-bad-char-literals.rs", "status": "renamed", "additions": 15, "deletions": 1, "changes": 16, "blob_url": "https://github.com/rust-lang/rust/blob/ad3eda123a00b3ddf8897295ff26d97ba7c64d44/src%2Ftest%2Fcompile-fail%2Flex-bad-char-literals.rs", "raw_url": "https://github.com/rust-lang/rust/raw/ad3eda123a00b3ddf8897295ff26d97ba7c64d44/src%2Ftest%2Fcompile-fail%2Flex-bad-char-literals.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ftest%2Fcompile-fail%2Flex-bad-char-literals.rs?ref=ad3eda123a00b3ddf8897295ff26d97ba7c64d44", "patch": "@@ -31,5 +31,19 @@ static s: &'static str =\n \n static s2: &'static str =\n     \"\\u23q\" //~ ERROR: illegal character in numeric character escape\n+    //~^ ERROR: numeric character escape is too short\n+;\n+\n+static c: char =\n+    '\\\u25cf' //~ ERROR: unknown character escape\n+;\n+\n+static s: &'static str =\n+    \"\\\u25cf\" //~ ERROR: unknown character escape\n+;\n+\n+// THIS MUST BE LAST, since unterminated character constants kill the lexer\n+\n+static c: char =\n+    '\u25cf  //~ ERROR: unterminated character constant\n ;\n-//~^^ ERROR: numeric character escape is too short", "previous_filename": "src/test/compile-fail/lex-illegal-num-char-escape.rs"}, {"sha": "79c42360adb2f81c7a495e10627e092972618d06", "filename": "src/test/compile-fail/lex-bad-fp-base-3.rs", "status": "removed", "additions": 0, "deletions": 13, "changes": 13, "blob_url": "https://github.com/rust-lang/rust/blob/5716abe3f019ab7d9c8cdde9879332040191cf88/src%2Ftest%2Fcompile-fail%2Flex-bad-fp-base-3.rs", "raw_url": "https://github.com/rust-lang/rust/raw/5716abe3f019ab7d9c8cdde9879332040191cf88/src%2Ftest%2Fcompile-fail%2Flex-bad-fp-base-3.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ftest%2Fcompile-fail%2Flex-bad-fp-base-3.rs?ref=5716abe3f019ab7d9c8cdde9879332040191cf88", "patch": "@@ -1,13 +0,0 @@\n-// Copyright 2014 The Rust Project Developers. See the COPYRIGHT\n-// file at the top-level directory of this distribution and at\n-// http://rust-lang.org/COPYRIGHT.\n-//\n-// Licensed under the Apache License, Version 2.0 <LICENSE-APACHE or\n-// http://www.apache.org/licenses/LICENSE-2.0> or the MIT license\n-// <LICENSE-MIT or http://opensource.org/licenses/MIT>, at your\n-// option. This file may not be copied, modified, or distributed\n-// except according to those terms.\n-\n-fn main() {\n-    let c = 0o3.0f32; //~ ERROR: octal float literal is not supported\n-}"}, {"sha": "eaea61b0089afa6fdbc9f42a31c12be560546679", "filename": "src/test/compile-fail/lex-bad-fp-base-4.rs", "status": "removed", "additions": 0, "deletions": 13, "changes": 13, "blob_url": "https://github.com/rust-lang/rust/blob/5716abe3f019ab7d9c8cdde9879332040191cf88/src%2Ftest%2Fcompile-fail%2Flex-bad-fp-base-4.rs", "raw_url": "https://github.com/rust-lang/rust/raw/5716abe3f019ab7d9c8cdde9879332040191cf88/src%2Ftest%2Fcompile-fail%2Flex-bad-fp-base-4.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ftest%2Fcompile-fail%2Flex-bad-fp-base-4.rs?ref=5716abe3f019ab7d9c8cdde9879332040191cf88", "patch": "@@ -1,13 +0,0 @@\n-// Copyright 2014 The Rust Project Developers. See the COPYRIGHT\n-// file at the top-level directory of this distribution and at\n-// http://rust-lang.org/COPYRIGHT.\n-//\n-// Licensed under the Apache License, Version 2.0 <LICENSE-APACHE or\n-// http://www.apache.org/licenses/LICENSE-2.0> or the MIT license\n-// <LICENSE-MIT or http://opensource.org/licenses/MIT>, at your\n-// option. This file may not be copied, modified, or distributed\n-// except according to those terms.\n-\n-fn main() {\n-    let d = 0o4e4; //~ ERROR: octal float literal is not supported\n-}"}, {"sha": "ee25ed95639e265b7ec4b0a4f59951e4b33ea64e", "filename": "src/test/compile-fail/lex-bad-fp-base-5.rs", "status": "removed", "additions": 0, "deletions": 13, "changes": 13, "blob_url": "https://github.com/rust-lang/rust/blob/5716abe3f019ab7d9c8cdde9879332040191cf88/src%2Ftest%2Fcompile-fail%2Flex-bad-fp-base-5.rs", "raw_url": "https://github.com/rust-lang/rust/raw/5716abe3f019ab7d9c8cdde9879332040191cf88/src%2Ftest%2Fcompile-fail%2Flex-bad-fp-base-5.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ftest%2Fcompile-fail%2Flex-bad-fp-base-5.rs?ref=5716abe3f019ab7d9c8cdde9879332040191cf88", "patch": "@@ -1,13 +0,0 @@\n-// Copyright 2014 The Rust Project Developers. See the COPYRIGHT\n-// file at the top-level directory of this distribution and at\n-// http://rust-lang.org/COPYRIGHT.\n-//\n-// Licensed under the Apache License, Version 2.0 <LICENSE-APACHE or\n-// http://www.apache.org/licenses/LICENSE-2.0> or the MIT license\n-// <LICENSE-MIT or http://opensource.org/licenses/MIT>, at your\n-// option. This file may not be copied, modified, or distributed\n-// except according to those terms.\n-\n-fn main() {\n-    let e = 0o5.0e5; //~ ERROR: octal float literal is not supported\n-}"}, {"sha": "bf08ec1eae5fe85f656958574ec665c6c03a277f", "filename": "src/test/compile-fail/lex-bad-fp-base-6.rs", "status": "removed", "additions": 0, "deletions": 13, "changes": 13, "blob_url": "https://github.com/rust-lang/rust/blob/5716abe3f019ab7d9c8cdde9879332040191cf88/src%2Ftest%2Fcompile-fail%2Flex-bad-fp-base-6.rs", "raw_url": "https://github.com/rust-lang/rust/raw/5716abe3f019ab7d9c8cdde9879332040191cf88/src%2Ftest%2Fcompile-fail%2Flex-bad-fp-base-6.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ftest%2Fcompile-fail%2Flex-bad-fp-base-6.rs?ref=5716abe3f019ab7d9c8cdde9879332040191cf88", "patch": "@@ -1,13 +0,0 @@\n-// Copyright 2014 The Rust Project Developers. See the COPYRIGHT\n-// file at the top-level directory of this distribution and at\n-// http://rust-lang.org/COPYRIGHT.\n-//\n-// Licensed under the Apache License, Version 2.0 <LICENSE-APACHE or\n-// http://www.apache.org/licenses/LICENSE-2.0> or the MIT license\n-// <LICENSE-MIT or http://opensource.org/licenses/MIT>, at your\n-// option. This file may not be copied, modified, or distributed\n-// except according to those terms.\n-\n-fn main() {\n-    let f = 0o6e6f32; //~ ERROR: octal float literal is not supported\n-}"}, {"sha": "921ed8f1b69e8893a5bead316419f02d8ad74112", "filename": "src/test/compile-fail/lex-bad-fp-base-7.rs", "status": "removed", "additions": 0, "deletions": 13, "changes": 13, "blob_url": "https://github.com/rust-lang/rust/blob/5716abe3f019ab7d9c8cdde9879332040191cf88/src%2Ftest%2Fcompile-fail%2Flex-bad-fp-base-7.rs", "raw_url": "https://github.com/rust-lang/rust/raw/5716abe3f019ab7d9c8cdde9879332040191cf88/src%2Ftest%2Fcompile-fail%2Flex-bad-fp-base-7.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ftest%2Fcompile-fail%2Flex-bad-fp-base-7.rs?ref=5716abe3f019ab7d9c8cdde9879332040191cf88", "patch": "@@ -1,13 +0,0 @@\n-// Copyright 2014 The Rust Project Developers. See the COPYRIGHT\n-// file at the top-level directory of this distribution and at\n-// http://rust-lang.org/COPYRIGHT.\n-//\n-// Licensed under the Apache License, Version 2.0 <LICENSE-APACHE or\n-// http://www.apache.org/licenses/LICENSE-2.0> or the MIT license\n-// <LICENSE-MIT or http://opensource.org/licenses/MIT>, at your\n-// option. This file may not be copied, modified, or distributed\n-// except according to those terms.\n-\n-fn main() {\n-    let g = 0o7.0e7f64; //~ ERROR: octal float literal is not supported\n-}"}, {"sha": "10e334ede01c2326b2f4a06aa650fa786163c9d0", "filename": "src/test/compile-fail/lex-bad-fp-base-8.rs", "status": "removed", "additions": 0, "deletions": 13, "changes": 13, "blob_url": "https://github.com/rust-lang/rust/blob/5716abe3f019ab7d9c8cdde9879332040191cf88/src%2Ftest%2Fcompile-fail%2Flex-bad-fp-base-8.rs", "raw_url": "https://github.com/rust-lang/rust/raw/5716abe3f019ab7d9c8cdde9879332040191cf88/src%2Ftest%2Fcompile-fail%2Flex-bad-fp-base-8.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ftest%2Fcompile-fail%2Flex-bad-fp-base-8.rs?ref=5716abe3f019ab7d9c8cdde9879332040191cf88", "patch": "@@ -1,13 +0,0 @@\n-// Copyright 2014 The Rust Project Developers. See the COPYRIGHT\n-// file at the top-level directory of this distribution and at\n-// http://rust-lang.org/COPYRIGHT.\n-//\n-// Licensed under the Apache License, Version 2.0 <LICENSE-APACHE or\n-// http://www.apache.org/licenses/LICENSE-2.0> or the MIT license\n-// <LICENSE-MIT or http://opensource.org/licenses/MIT>, at your\n-// option. This file may not be copied, modified, or distributed\n-// except according to those terms.\n-\n-fn main() {\n-    let h = 0x8.0e+9; //~ ERROR: hexadecimal float literal is not supported\n-}"}, {"sha": "3ea151cb9826a2b6258b25dee09cd7298885a801", "filename": "src/test/compile-fail/lex-bad-fp-base-9.rs", "status": "removed", "additions": 0, "deletions": 13, "changes": 13, "blob_url": "https://github.com/rust-lang/rust/blob/5716abe3f019ab7d9c8cdde9879332040191cf88/src%2Ftest%2Fcompile-fail%2Flex-bad-fp-base-9.rs", "raw_url": "https://github.com/rust-lang/rust/raw/5716abe3f019ab7d9c8cdde9879332040191cf88/src%2Ftest%2Fcompile-fail%2Flex-bad-fp-base-9.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ftest%2Fcompile-fail%2Flex-bad-fp-base-9.rs?ref=5716abe3f019ab7d9c8cdde9879332040191cf88", "patch": "@@ -1,13 +0,0 @@\n-// Copyright 2014 The Rust Project Developers. See the COPYRIGHT\n-// file at the top-level directory of this distribution and at\n-// http://rust-lang.org/COPYRIGHT.\n-//\n-// Licensed under the Apache License, Version 2.0 <LICENSE-APACHE or\n-// http://www.apache.org/licenses/LICENSE-2.0> or the MIT license\n-// <LICENSE-MIT or http://opensource.org/licenses/MIT>, at your\n-// option. This file may not be copied, modified, or distributed\n-// except according to those terms.\n-\n-fn main() {\n-    let i = 0x9.0e-9; //~ ERROR: hexadecimal float literal is not supported\n-}"}, {"sha": "5a5e9d7d8f23876a792d72ed5ca623b8ea52014d", "filename": "src/test/compile-fail/lex-bad-fp-lit.rs", "status": "removed", "additions": 0, "deletions": 13, "changes": 13, "blob_url": "https://github.com/rust-lang/rust/blob/5716abe3f019ab7d9c8cdde9879332040191cf88/src%2Ftest%2Fcompile-fail%2Flex-bad-fp-lit.rs", "raw_url": "https://github.com/rust-lang/rust/raw/5716abe3f019ab7d9c8cdde9879332040191cf88/src%2Ftest%2Fcompile-fail%2Flex-bad-fp-lit.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ftest%2Fcompile-fail%2Flex-bad-fp-lit.rs?ref=5716abe3f019ab7d9c8cdde9879332040191cf88", "patch": "@@ -1,13 +0,0 @@\n-// Copyright 2013 The Rust Project Developers. See the COPYRIGHT\n-// file at the top-level directory of this distribution and at\n-// http://rust-lang.org/COPYRIGHT.\n-//\n-// Licensed under the Apache License, Version 2.0 <LICENSE-APACHE or\n-// http://www.apache.org/licenses/LICENSE-2.0> or the MIT license\n-// <LICENSE-MIT or http://opensource.org/licenses/MIT>, at your\n-// option. This file may not be copied, modified, or distributed\n-// except according to those terms.\n-\n-static f: float =\n-    1e+ //~ ERROR: scan_exponent: bad fp literal\n-;"}, {"sha": "9a490be6a0169963e5d4b8c7ace243f4911b8fa9", "filename": "src/test/compile-fail/lex-bad-numeric-literals.rs", "status": "added", "additions": 35, "deletions": 0, "changes": 35, "blob_url": "https://github.com/rust-lang/rust/blob/ad3eda123a00b3ddf8897295ff26d97ba7c64d44/src%2Ftest%2Fcompile-fail%2Flex-bad-numeric-literals.rs", "raw_url": "https://github.com/rust-lang/rust/raw/ad3eda123a00b3ddf8897295ff26d97ba7c64d44/src%2Ftest%2Fcompile-fail%2Flex-bad-numeric-literals.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ftest%2Fcompile-fail%2Flex-bad-numeric-literals.rs?ref=ad3eda123a00b3ddf8897295ff26d97ba7c64d44", "patch": "@@ -0,0 +1,35 @@\n+// Copyright 2014 The Rust Project Developers. See the COPYRIGHT\n+// file at the top-level directory of this distribution and at\n+// http://rust-lang.org/COPYRIGHT.\n+//\n+// Licensed under the Apache License, Version 2.0 <LICENSE-APACHE or\n+// http://www.apache.org/licenses/LICENSE-2.0> or the MIT license\n+// <LICENSE-MIT or http://opensource.org/licenses/MIT>, at your\n+// option. This file may not be copied, modified, or distributed\n+// except according to those terms.\n+\n+fn main() {\n+    0o1.0; //~ ERROR: octal float literal is not supported\n+    0o2f32; //~ ERROR: octal float literal is not supported\n+    0o3.0f32; //~ ERROR: octal float literal is not supported\n+    0o4e4; //~ ERROR: octal float literal is not supported\n+    0o5.0e5; //~ ERROR: octal float literal is not supported\n+    0o6e6f32; //~ ERROR: octal float literal is not supported\n+    0o7.0e7f64; //~ ERROR: octal float literal is not supported\n+    0x8.0e+9; //~ ERROR: hexadecimal float literal is not supported\n+    0x9.0e-9; //~ ERROR: hexadecimal float literal is not supported\n+    0o; //~ ERROR: no valid digits\n+    1e+; //~ ERROR: expected at least one digit in exponent\n+    0x539.0; //~ ERROR: hexadecimal float literal is not supported\n+    99999999999999999999999999999999; //~ ERROR: int literal is too large\n+    99999999999999999999999999999999u32; //~ ERROR: int literal is too large\n+    0x; //~ ERROR: no valid digits\n+    0xu32; //~ ERROR: no valid digits\n+    0ou32; //~ ERROR: no valid digits\n+    0bu32; //~ ERROR: no valid digits\n+    0b; //~ ERROR: no valid digits\n+    0o123f64; //~ ERROR: octal float literal is not supported\n+    0o123.456; //~ ERROR: octal float literal is not supported\n+    0b101f64; //~ ERROR: binary float literal is not supported\n+    0b111.101; //~ ERROR: binary float literal is not supported\n+}"}, {"sha": "d28d9a20c6eedf3b644f494d1697b85ed88acefb", "filename": "src/test/compile-fail/lex-bad-token.rs", "status": "renamed", "additions": 1, "deletions": 3, "changes": 4, "blob_url": "https://github.com/rust-lang/rust/blob/ad3eda123a00b3ddf8897295ff26d97ba7c64d44/src%2Ftest%2Fcompile-fail%2Flex-bad-token.rs", "raw_url": "https://github.com/rust-lang/rust/raw/ad3eda123a00b3ddf8897295ff26d97ba7c64d44/src%2Ftest%2Fcompile-fail%2Flex-bad-token.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ftest%2Fcompile-fail%2Flex-bad-token.rs?ref=ad3eda123a00b3ddf8897295ff26d97ba7c64d44", "patch": "@@ -8,6 +8,4 @@\n // option. This file may not be copied, modified, or distributed\n // except according to those terms.\n \n-fn main() {\n-    let a = 0o1.0; //~ ERROR: octal float literal is not supported\n-}\n+\u25cf //~ ERROR: unknown start of token", "previous_filename": "src/test/compile-fail/lex-bad-fp-base-1.rs"}, {"sha": "457c6126c44a5959f0433e955fe573ed7efaaecf", "filename": "src/test/compile-fail/lex-hex-float-lit.rs", "status": "removed", "additions": 0, "deletions": 13, "changes": 13, "blob_url": "https://github.com/rust-lang/rust/blob/5716abe3f019ab7d9c8cdde9879332040191cf88/src%2Ftest%2Fcompile-fail%2Flex-hex-float-lit.rs", "raw_url": "https://github.com/rust-lang/rust/raw/5716abe3f019ab7d9c8cdde9879332040191cf88/src%2Ftest%2Fcompile-fail%2Flex-hex-float-lit.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ftest%2Fcompile-fail%2Flex-hex-float-lit.rs?ref=5716abe3f019ab7d9c8cdde9879332040191cf88", "patch": "@@ -1,13 +0,0 @@\n-// Copyright 2013 The Rust Project Developers. See the COPYRIGHT\n-// file at the top-level directory of this distribution and at\n-// http://rust-lang.org/COPYRIGHT.\n-//\n-// Licensed under the Apache License, Version 2.0 <LICENSE-APACHE or\n-// http://www.apache.org/licenses/LICENSE-2.0> or the MIT license\n-// <LICENSE-MIT or http://opensource.org/licenses/MIT>, at your\n-// option. This file may not be copied, modified, or distributed\n-// except according to those terms.\n-\n-static f: float =\n-    0x539.0 //~ ERROR: hexadecimal float literal is not supported\n-;"}, {"sha": "39d1cba64b08bce68eff5c8dfb0f41fbb03ae688", "filename": "src/test/compile-fail/lex-int-lit-too-large-2.rs", "status": "removed", "additions": 0, "deletions": 13, "changes": 13, "blob_url": "https://github.com/rust-lang/rust/blob/5716abe3f019ab7d9c8cdde9879332040191cf88/src%2Ftest%2Fcompile-fail%2Flex-int-lit-too-large-2.rs", "raw_url": "https://github.com/rust-lang/rust/raw/5716abe3f019ab7d9c8cdde9879332040191cf88/src%2Ftest%2Fcompile-fail%2Flex-int-lit-too-large-2.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ftest%2Fcompile-fail%2Flex-int-lit-too-large-2.rs?ref=5716abe3f019ab7d9c8cdde9879332040191cf88", "patch": "@@ -1,13 +0,0 @@\n-// Copyright 2013 The Rust Project Developers. See the COPYRIGHT\n-// file at the top-level directory of this distribution and at\n-// http://rust-lang.org/COPYRIGHT.\n-//\n-// Licensed under the Apache License, Version 2.0 <LICENSE-APACHE or\n-// http://www.apache.org/licenses/LICENSE-2.0> or the MIT license\n-// <LICENSE-MIT or http://opensource.org/licenses/MIT>, at your\n-// option. This file may not be copied, modified, or distributed\n-// except according to those terms.\n-\n-static i: int =\n-    99999999999999999999999999999999u32 //~ ERROR: int literal is too large\n-;"}, {"sha": "6343be651fa5961d3ece0c02ad1067a008786a3a", "filename": "src/test/compile-fail/lex-int-lit-too-large.rs", "status": "removed", "additions": 0, "deletions": 13, "changes": 13, "blob_url": "https://github.com/rust-lang/rust/blob/5716abe3f019ab7d9c8cdde9879332040191cf88/src%2Ftest%2Fcompile-fail%2Flex-int-lit-too-large.rs", "raw_url": "https://github.com/rust-lang/rust/raw/5716abe3f019ab7d9c8cdde9879332040191cf88/src%2Ftest%2Fcompile-fail%2Flex-int-lit-too-large.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ftest%2Fcompile-fail%2Flex-int-lit-too-large.rs?ref=5716abe3f019ab7d9c8cdde9879332040191cf88", "patch": "@@ -1,13 +0,0 @@\n-// Copyright 2013 The Rust Project Developers. See the COPYRIGHT\n-// file at the top-level directory of this distribution and at\n-// http://rust-lang.org/COPYRIGHT.\n-//\n-// Licensed under the Apache License, Version 2.0 <LICENSE-APACHE or\n-// http://www.apache.org/licenses/LICENSE-2.0> or the MIT license\n-// <LICENSE-MIT or http://opensource.org/licenses/MIT>, at your\n-// option. This file may not be copied, modified, or distributed\n-// except according to those terms.\n-\n-static i: int =\n-    99999999999999999999999999999999 //~ ERROR: int literal is too large\n-;"}, {"sha": "549dbf5bc8c6c0f1ff8bdd830010c642272f124d", "filename": "src/test/compile-fail/lex-no-valid-digits-2.rs", "status": "removed", "additions": 0, "deletions": 13, "changes": 13, "blob_url": "https://github.com/rust-lang/rust/blob/5716abe3f019ab7d9c8cdde9879332040191cf88/src%2Ftest%2Fcompile-fail%2Flex-no-valid-digits-2.rs", "raw_url": "https://github.com/rust-lang/rust/raw/5716abe3f019ab7d9c8cdde9879332040191cf88/src%2Ftest%2Fcompile-fail%2Flex-no-valid-digits-2.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ftest%2Fcompile-fail%2Flex-no-valid-digits-2.rs?ref=5716abe3f019ab7d9c8cdde9879332040191cf88", "patch": "@@ -1,13 +0,0 @@\n-// Copyright 2013 The Rust Project Developers. See the COPYRIGHT\n-// file at the top-level directory of this distribution and at\n-// http://rust-lang.org/COPYRIGHT.\n-//\n-// Licensed under the Apache License, Version 2.0 <LICENSE-APACHE or\n-// http://www.apache.org/licenses/LICENSE-2.0> or the MIT license\n-// <LICENSE-MIT or http://opensource.org/licenses/MIT>, at your\n-// option. This file may not be copied, modified, or distributed\n-// except according to those terms.\n-\n-static i: int =\n-    0xu32 //~ ERROR: no valid digits\n-;"}, {"sha": "6a5b8e93f010a5f6b1ab21a7b3b2f40360c8c613", "filename": "src/test/compile-fail/lex-no-valid-digits.rs", "status": "removed", "additions": 0, "deletions": 13, "changes": 13, "blob_url": "https://github.com/rust-lang/rust/blob/5716abe3f019ab7d9c8cdde9879332040191cf88/src%2Ftest%2Fcompile-fail%2Flex-no-valid-digits.rs", "raw_url": "https://github.com/rust-lang/rust/raw/5716abe3f019ab7d9c8cdde9879332040191cf88/src%2Ftest%2Fcompile-fail%2Flex-no-valid-digits.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ftest%2Fcompile-fail%2Flex-no-valid-digits.rs?ref=5716abe3f019ab7d9c8cdde9879332040191cf88", "patch": "@@ -1,13 +0,0 @@\n-// Copyright 2013 The Rust Project Developers. See the COPYRIGHT\n-// file at the top-level directory of this distribution and at\n-// http://rust-lang.org/COPYRIGHT.\n-//\n-// Licensed under the Apache License, Version 2.0 <LICENSE-APACHE or\n-// http://www.apache.org/licenses/LICENSE-2.0> or the MIT license\n-// <LICENSE-MIT or http://opensource.org/licenses/MIT>, at your\n-// option. This file may not be copied, modified, or distributed\n-// except according to those terms.\n-\n-static i: int =\n-    0x //~ ERROR: no valid digits\n-;"}, {"sha": "f2445c2b60ebaf7c100605e7e833fcb75bedfb25", "filename": "src/test/compile-fail/lex-unknown-char-escape.rs", "status": "removed", "additions": 0, "deletions": 13, "changes": 13, "blob_url": "https://github.com/rust-lang/rust/blob/5716abe3f019ab7d9c8cdde9879332040191cf88/src%2Ftest%2Fcompile-fail%2Flex-unknown-char-escape.rs", "raw_url": "https://github.com/rust-lang/rust/raw/5716abe3f019ab7d9c8cdde9879332040191cf88/src%2Ftest%2Fcompile-fail%2Flex-unknown-char-escape.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ftest%2Fcompile-fail%2Flex-unknown-char-escape.rs?ref=5716abe3f019ab7d9c8cdde9879332040191cf88", "patch": "@@ -1,13 +0,0 @@\n-// Copyright 2013 The Rust Project Developers. See the COPYRIGHT\n-// file at the top-level directory of this distribution and at\n-// http://rust-lang.org/COPYRIGHT.\n-//\n-// Licensed under the Apache License, Version 2.0 <LICENSE-APACHE or\n-// http://www.apache.org/licenses/LICENSE-2.0> or the MIT license\n-// <LICENSE-MIT or http://opensource.org/licenses/MIT>, at your\n-// option. This file may not be copied, modified, or distributed\n-// except according to those terms.\n-\n-static c: char =\n-    '\\\u25cf' //~ ERROR: unknown character escape\n-;"}, {"sha": "1bb682303451bd30c36864458cabb741ed275b55", "filename": "src/test/compile-fail/lex-unknown-start-tok.rs", "status": "removed", "additions": 0, "deletions": 13, "changes": 13, "blob_url": "https://github.com/rust-lang/rust/blob/5716abe3f019ab7d9c8cdde9879332040191cf88/src%2Ftest%2Fcompile-fail%2Flex-unknown-start-tok.rs", "raw_url": "https://github.com/rust-lang/rust/raw/5716abe3f019ab7d9c8cdde9879332040191cf88/src%2Ftest%2Fcompile-fail%2Flex-unknown-start-tok.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ftest%2Fcompile-fail%2Flex-unknown-start-tok.rs?ref=5716abe3f019ab7d9c8cdde9879332040191cf88", "patch": "@@ -1,13 +0,0 @@\n-// Copyright 2013 The Rust Project Developers. See the COPYRIGHT\n-// file at the top-level directory of this distribution and at\n-// http://rust-lang.org/COPYRIGHT.\n-//\n-// Licensed under the Apache License, Version 2.0 <LICENSE-APACHE or\n-// http://www.apache.org/licenses/LICENSE-2.0> or the MIT license\n-// <LICENSE-MIT or http://opensource.org/licenses/MIT>, at your\n-// option. This file may not be copied, modified, or distributed\n-// except according to those terms.\n-\n-fn main() {\n-    \u25cf //~ ERROR: unknown start of token\n-}"}, {"sha": "9a59c4227114b3a18f5b8f75763e3a4eb8392165", "filename": "src/test/compile-fail/lex-unknown-str-escape.rs", "status": "removed", "additions": 0, "deletions": 13, "changes": 13, "blob_url": "https://github.com/rust-lang/rust/blob/5716abe3f019ab7d9c8cdde9879332040191cf88/src%2Ftest%2Fcompile-fail%2Flex-unknown-str-escape.rs", "raw_url": "https://github.com/rust-lang/rust/raw/5716abe3f019ab7d9c8cdde9879332040191cf88/src%2Ftest%2Fcompile-fail%2Flex-unknown-str-escape.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ftest%2Fcompile-fail%2Flex-unknown-str-escape.rs?ref=5716abe3f019ab7d9c8cdde9879332040191cf88", "patch": "@@ -1,13 +0,0 @@\n-// Copyright 2013 The Rust Project Developers. See the COPYRIGHT\n-// file at the top-level directory of this distribution and at\n-// http://rust-lang.org/COPYRIGHT.\n-//\n-// Licensed under the Apache License, Version 2.0 <LICENSE-APACHE or\n-// http://www.apache.org/licenses/LICENSE-2.0> or the MIT license\n-// <LICENSE-MIT or http://opensource.org/licenses/MIT>, at your\n-// option. This file may not be copied, modified, or distributed\n-// except according to those terms.\n-\n-static s: &'static str =\n-    \"\\\u25cf\" //~ ERROR: unknown character escape\n-;"}, {"sha": "551360ff9e095b425b94f748100f3f6092b65f23", "filename": "src/test/compile-fail/lex-unterminated-char-const.rs", "status": "removed", "additions": 0, "deletions": 13, "changes": 13, "blob_url": "https://github.com/rust-lang/rust/blob/5716abe3f019ab7d9c8cdde9879332040191cf88/src%2Ftest%2Fcompile-fail%2Flex-unterminated-char-const.rs", "raw_url": "https://github.com/rust-lang/rust/raw/5716abe3f019ab7d9c8cdde9879332040191cf88/src%2Ftest%2Fcompile-fail%2Flex-unterminated-char-const.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ftest%2Fcompile-fail%2Flex-unterminated-char-const.rs?ref=5716abe3f019ab7d9c8cdde9879332040191cf88", "patch": "@@ -1,13 +0,0 @@\n-// Copyright 2013 The Rust Project Developers. See the COPYRIGHT\n-// file at the top-level directory of this distribution and at\n-// http://rust-lang.org/COPYRIGHT.\n-//\n-// Licensed under the Apache License, Version 2.0 <LICENSE-APACHE or\n-// http://www.apache.org/licenses/LICENSE-2.0> or the MIT license\n-// <LICENSE-MIT or http://opensource.org/licenses/MIT>, at your\n-// option. This file may not be copied, modified, or distributed\n-// except according to those terms.\n-\n-static c: char =\n-    '\u25cf  //~ ERROR: unterminated character constant\n-;"}, {"sha": "511116b1c559c9ce7ff217f5aca9f2017777d60a", "filename": "src/test/compile-fail/no-oct-float-literal.rs", "status": "removed", "additions": 0, "deletions": 17, "changes": 17, "blob_url": "https://github.com/rust-lang/rust/blob/5716abe3f019ab7d9c8cdde9879332040191cf88/src%2Ftest%2Fcompile-fail%2Fno-oct-float-literal.rs", "raw_url": "https://github.com/rust-lang/rust/raw/5716abe3f019ab7d9c8cdde9879332040191cf88/src%2Ftest%2Fcompile-fail%2Fno-oct-float-literal.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ftest%2Fcompile-fail%2Fno-oct-float-literal.rs?ref=5716abe3f019ab7d9c8cdde9879332040191cf88", "patch": "@@ -1,17 +0,0 @@\n-// Copyright 2013 The Rust Project Developers. See the COPYRIGHT\n-// file at the top-level directory of this distribution and at\n-// http://rust-lang.org/COPYRIGHT.\n-//\n-// Licensed under the Apache License, Version 2.0 <LICENSE-APACHE or\n-// http://www.apache.org/licenses/LICENSE-2.0> or the MIT license\n-// <LICENSE-MIT or http://opensource.org/licenses/MIT>, at your\n-// option. This file may not be copied, modified, or distributed\n-// except according to those terms.\n-\n-// error-pattern:octal float literal is not supported\n-\n-fn main() {\n-    0o123f64;\n-    0o123.456;\n-    0o123p4f;\n-}"}, {"sha": "7abe8276a97823b67e314c2b8b304b9fa2d5f294", "filename": "src/test/run-pass/string-escapes.rs", "status": "renamed", "additions": 3, "deletions": 1, "changes": 4, "blob_url": "https://github.com/rust-lang/rust/blob/ad3eda123a00b3ddf8897295ff26d97ba7c64d44/src%2Ftest%2Frun-pass%2Fstring-escapes.rs", "raw_url": "https://github.com/rust-lang/rust/raw/ad3eda123a00b3ddf8897295ff26d97ba7c64d44/src%2Ftest%2Frun-pass%2Fstring-escapes.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ftest%2Frun-pass%2Fstring-escapes.rs?ref=ad3eda123a00b3ddf8897295ff26d97ba7c64d44", "patch": "@@ -9,5 +9,7 @@\n // except according to those terms.\n \n fn main() {\n-    let b = 0o2f32; //~ ERROR: octal float literal is not supported\n+    let x = \"\\\\\\\\\\\n+    \";\n+    assert!(x == r\"\\\\\"); // extraneous whitespace stripped\n }", "previous_filename": "src/test/compile-fail/lex-bad-fp-base-2.rs"}]}