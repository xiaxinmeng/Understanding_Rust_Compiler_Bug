{"sha": "77ab0d091e32ac9ec4154bba1727fc3937975d64", "node_id": "MDY6Q29tbWl0NzI0NzEyOjc3YWIwZDA5MWUzMmFjOWVjNDE1NGJiYTE3MjdmYzM5Mzc5NzVkNjQ=", "commit": {"author": {"name": "John K\u00e5re Alsaker", "email": "john.kare.alsaker@gmail.com", "date": "2020-01-31T03:00:03Z"}, "committer": {"name": "John K\u00e5re Alsaker", "email": "john.kare.alsaker@gmail.com", "date": "2020-02-12T09:30:23Z"}, "message": "Construct query job latches on-demand", "tree": {"sha": "6d6e572673c68b8b6935367ac97aca90a18e1dea", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/6d6e572673c68b8b6935367ac97aca90a18e1dea"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/77ab0d091e32ac9ec4154bba1727fc3937975d64", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/77ab0d091e32ac9ec4154bba1727fc3937975d64", "html_url": "https://github.com/rust-lang/rust/commit/77ab0d091e32ac9ec4154bba1727fc3937975d64", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/77ab0d091e32ac9ec4154bba1727fc3937975d64/comments", "author": {"login": "Zoxc", "id": 25784, "node_id": "MDQ6VXNlcjI1Nzg0", "avatar_url": "https://avatars.githubusercontent.com/u/25784?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Zoxc", "html_url": "https://github.com/Zoxc", "followers_url": "https://api.github.com/users/Zoxc/followers", "following_url": "https://api.github.com/users/Zoxc/following{/other_user}", "gists_url": "https://api.github.com/users/Zoxc/gists{/gist_id}", "starred_url": "https://api.github.com/users/Zoxc/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Zoxc/subscriptions", "organizations_url": "https://api.github.com/users/Zoxc/orgs", "repos_url": "https://api.github.com/users/Zoxc/repos", "events_url": "https://api.github.com/users/Zoxc/events{/privacy}", "received_events_url": "https://api.github.com/users/Zoxc/received_events", "type": "User", "site_admin": false}, "committer": {"login": "Zoxc", "id": 25784, "node_id": "MDQ6VXNlcjI1Nzg0", "avatar_url": "https://avatars.githubusercontent.com/u/25784?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Zoxc", "html_url": "https://github.com/Zoxc", "followers_url": "https://api.github.com/users/Zoxc/followers", "following_url": "https://api.github.com/users/Zoxc/following{/other_user}", "gists_url": "https://api.github.com/users/Zoxc/gists{/gist_id}", "starred_url": "https://api.github.com/users/Zoxc/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Zoxc/subscriptions", "organizations_url": "https://api.github.com/users/Zoxc/orgs", "repos_url": "https://api.github.com/users/Zoxc/repos", "events_url": "https://api.github.com/users/Zoxc/events{/privacy}", "received_events_url": "https://api.github.com/users/Zoxc/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "e6db6697fa84271695a96570370f3f194785699a", "url": "https://api.github.com/repos/rust-lang/rust/commits/e6db6697fa84271695a96570370f3f194785699a", "html_url": "https://github.com/rust-lang/rust/commit/e6db6697fa84271695a96570370f3f194785699a"}], "stats": {"total": 426, "additions": 275, "deletions": 151}, "files": [{"sha": "21640996d5d4ca4a7ef6cebeefec4f8b425db1b1", "filename": "src/librustc/ty/context.rs", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/rust-lang/rust/blob/77ab0d091e32ac9ec4154bba1727fc3937975d64/src%2Flibrustc%2Fty%2Fcontext.rs", "raw_url": "https://github.com/rust-lang/rust/raw/77ab0d091e32ac9ec4154bba1727fc3937975d64/src%2Flibrustc%2Fty%2Fcontext.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Fty%2Fcontext.rs?ref=77ab0d091e32ac9ec4154bba1727fc3937975d64", "patch": "@@ -1608,7 +1608,7 @@ pub mod tls {\n \n     use crate::dep_graph::TaskDeps;\n     use crate::ty::query;\n-    use rustc_data_structures::sync::{self, Lock, Lrc};\n+    use rustc_data_structures::sync::{self, Lock};\n     use rustc_data_structures::thin_vec::ThinVec;\n     use rustc_data_structures::OnDrop;\n     use rustc_errors::Diagnostic;\n@@ -1633,7 +1633,7 @@ pub mod tls {\n \n         /// The current query job, if any. This is updated by `JobOwner::start` in\n         /// `ty::query::plumbing` when executing a query.\n-        pub query: Option<Lrc<query::QueryJob<'tcx>>>,\n+        pub query: Option<query::QueryToken>,\n \n         /// Where to store diagnostics for the current query job, if any.\n         /// This is updated by `JobOwner::start` in `ty::query::plumbing` when executing a query."}, {"sha": "d440b5150b075689017d63902e27412f2232b978", "filename": "src/librustc/ty/query/job.rs", "status": "modified", "additions": 190, "deletions": 102, "changes": 292, "blob_url": "https://github.com/rust-lang/rust/blob/77ab0d091e32ac9ec4154bba1727fc3937975d64/src%2Flibrustc%2Fty%2Fquery%2Fjob.rs", "raw_url": "https://github.com/rust-lang/rust/raw/77ab0d091e32ac9ec4154bba1727fc3937975d64/src%2Flibrustc%2Fty%2Fquery%2Fjob.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Fty%2Fquery%2Fjob.rs?ref=77ab0d091e32ac9ec4154bba1727fc3937975d64", "patch": "@@ -3,18 +3,19 @@ use crate::ty::query::plumbing::CycleError;\n use crate::ty::query::Query;\n use crate::ty::tls;\n \n-use rustc_data_structures::sync::Lrc;\n+use rustc_data_structures::fx::FxHashMap;\n use rustc_span::Span;\n \n-#[cfg(not(parallel_compiler))]\n-use std::ptr;\n+use std::marker::PhantomData;\n+use std::num::NonZeroUsize;\n \n #[cfg(parallel_compiler)]\n use {\n     parking_lot::{Condvar, Mutex},\n     rustc_data_structures::fx::FxHashSet,\n     rustc_data_structures::stable_hasher::{HashStable, StableHasher},\n     rustc_data_structures::sync::Lock,\n+    rustc_data_structures::sync::Lrc,\n     rustc_data_structures::{jobserver, OnDrop},\n     rustc_rayon_core as rayon_core,\n     rustc_span::DUMMY_SP,\n@@ -30,61 +31,119 @@ pub struct QueryInfo<'tcx> {\n     pub query: Query<'tcx>,\n }\n \n-/// Representss an object representing an active query job.\n-pub struct QueryJob<'tcx> {\n+type QueryMap<'tcx> = FxHashMap<QueryToken, QueryJobInfo<'tcx>>;\n+\n+/// A value uniquely identifiying an active query job.\n+/// This value is created from a stack pointer in `get_query` and `force_query`\n+/// which is alive while the query executes.\n+#[derive(Copy, Clone, Eq, PartialEq, Hash)]\n+pub struct QueryToken(NonZeroUsize);\n+\n+impl QueryToken {\n+    pub fn from<T>(v: &T) -> Self {\n+        QueryToken(NonZeroUsize::new(v as *const T as usize).unwrap())\n+    }\n+\n+    fn query<'tcx>(self, map: &QueryMap<'tcx>) -> Query<'tcx> {\n+        map.get(&self).unwrap().info.query.clone()\n+    }\n+\n+    #[cfg(parallel_compiler)]\n+    fn span(self, map: &QueryMap<'_>) -> Span {\n+        map.get(&self).unwrap().job.span\n+    }\n+\n+    #[cfg(parallel_compiler)]\n+    fn parent(self, map: &QueryMap<'_>) -> Option<QueryToken> {\n+        map.get(&self).unwrap().job.parent\n+    }\n+\n+    #[cfg(parallel_compiler)]\n+    fn latch<'a, 'tcx>(self, map: &'a QueryMap<'tcx>) -> Option<&'a QueryLatch<'tcx>> {\n+        map.get(&self).unwrap().job.latch.as_ref()\n+    }\n+}\n+\n+pub struct QueryJobInfo<'tcx> {\n     pub info: QueryInfo<'tcx>,\n+    pub job: QueryJob<'tcx>,\n+}\n+\n+/// Represents an active query job.\n+#[derive(Clone)]\n+pub struct QueryJob<'tcx> {\n+    pub token: QueryToken,\n+\n+    /// The span corresponding to the reason for which this query was required.\n+    pub span: Span,\n \n     /// The parent query job which created this job and is implicitly waiting on it.\n-    pub parent: Option<Lrc<QueryJob<'tcx>>>,\n+    pub parent: Option<QueryToken>,\n \n     /// The latch that is used to wait on this job.\n     #[cfg(parallel_compiler)]\n-    latch: QueryLatch<'tcx>,\n+    latch: Option<QueryLatch<'tcx>>,\n+\n+    dummy: PhantomData<QueryLatch<'tcx>>,\n }\n \n impl<'tcx> QueryJob<'tcx> {\n     /// Creates a new query job.\n-    pub fn new(info: QueryInfo<'tcx>, parent: Option<Lrc<QueryJob<'tcx>>>) -> Self {\n+    pub fn new(token: QueryToken, span: Span, parent: Option<QueryToken>) -> Self {\n         QueryJob {\n-            info,\n+            token,\n+            span,\n             parent,\n             #[cfg(parallel_compiler)]\n-            latch: QueryLatch::new(),\n+            latch: None,\n+            dummy: PhantomData,\n         }\n     }\n \n-    /// Awaits for the query job to complete.\n     #[cfg(parallel_compiler)]\n-    pub(super) fn r#await(&self, tcx: TyCtxt<'tcx>, span: Span) -> Result<(), CycleError<'tcx>> {\n-        tls::with_related_context(tcx, move |icx| {\n-            let waiter = Lrc::new(QueryWaiter {\n-                query: icx.query.clone(),\n-                span,\n-                cycle: Lock::new(None),\n-                condvar: Condvar::new(),\n-            });\n-            self.latch.r#await(&waiter);\n-            // FIXME: Get rid of this lock. We have ownership of the QueryWaiter\n-            // although another thread may still have a Lrc reference so we cannot\n-            // use Lrc::get_mut\n-            let mut cycle = waiter.cycle.lock();\n-            match cycle.take() {\n-                None => Ok(()),\n-                Some(cycle) => Err(cycle),\n-            }\n-        })\n+    pub(super) fn latch(&mut self) -> QueryLatch<'tcx> {\n+        if self.latch.is_none() {\n+            self.latch = Some(QueryLatch::new());\n+        }\n+        self.latch.as_ref().unwrap().clone()\n     }\n \n     #[cfg(not(parallel_compiler))]\n+    pub(super) fn latch(&mut self) -> QueryLatch<'tcx> {\n+        QueryLatch { token: self.token, dummy: PhantomData }\n+    }\n+\n+    /// Signals to waiters that the query is complete.\n+    ///\n+    /// This does nothing for single threaded rustc,\n+    /// as there are no concurrent jobs which could be waiting on us\n+    pub fn signal_complete(self) {\n+        #[cfg(parallel_compiler)]\n+        self.latch.map(|latch| latch.set());\n+    }\n+}\n+\n+#[cfg(not(parallel_compiler))]\n+#[derive(Clone)]\n+pub(super) struct QueryLatch<'tcx> {\n+    token: QueryToken,\n+    dummy: PhantomData<&'tcx ()>,\n+}\n+\n+#[cfg(not(parallel_compiler))]\n+impl<'tcx> QueryLatch<'tcx> {\n     pub(super) fn find_cycle_in_stack(&self, tcx: TyCtxt<'tcx>, span: Span) -> CycleError<'tcx> {\n+        let query_map = tcx.queries.try_collect_active_jobs().unwrap();\n+\n         // Get the current executing query (waiter) and find the waitee amongst its parents\n-        let mut current_job = tls::with_related_context(tcx, |icx| icx.query.clone());\n+        let mut current_job = tls::with_related_context(tcx, |icx| icx.query);\n         let mut cycle = Vec::new();\n \n         while let Some(job) = current_job {\n-            cycle.push(job.info.clone());\n+            let info = query_map.get(&job).unwrap();\n+            cycle.push(info.info.clone());\n \n-            if ptr::eq(&*job, self) {\n+            if job == self.token {\n                 cycle.reverse();\n \n                 // This is the end of the cycle\n@@ -93,35 +152,24 @@ impl<'tcx> QueryJob<'tcx> {\n                 // Replace it with the span which caused the cycle to form\n                 cycle[0].span = span;\n                 // Find out why the cycle itself was used\n-                let usage =\n-                    job.parent.as_ref().map(|parent| (job.info.span, parent.info.query.clone()));\n+                let usage = info\n+                    .job\n+                    .parent\n+                    .as_ref()\n+                    .map(|parent| (info.info.span, parent.query(&query_map)));\n                 return CycleError { usage, cycle };\n             }\n \n-            current_job = job.parent.clone();\n+            current_job = info.job.parent.clone();\n         }\n \n         panic!(\"did not find a cycle\")\n     }\n-\n-    /// Signals to waiters that the query is complete.\n-    ///\n-    /// This does nothing for single threaded rustc,\n-    /// as there are no concurrent jobs which could be waiting on us\n-    pub fn signal_complete(&self) {\n-        #[cfg(parallel_compiler)]\n-        self.latch.set();\n-    }\n-\n-    #[cfg(parallel_compiler)]\n-    fn as_ptr(&self) -> *const QueryJob<'tcx> {\n-        self as *const _\n-    }\n }\n \n #[cfg(parallel_compiler)]\n struct QueryWaiter<'tcx> {\n-    query: Option<Lrc<QueryJob<'tcx>>>,\n+    query: Option<QueryToken>,\n     condvar: Condvar,\n     span: Span,\n     cycle: Lock<Option<CycleError<'tcx>>>,\n@@ -142,18 +190,43 @@ struct QueryLatchInfo<'tcx> {\n }\n \n #[cfg(parallel_compiler)]\n-struct QueryLatch<'tcx> {\n-    info: Mutex<QueryLatchInfo<'tcx>>,\n+#[derive(Clone)]\n+pub(super) struct QueryLatch<'tcx> {\n+    info: Lrc<Mutex<QueryLatchInfo<'tcx>>>,\n }\n \n #[cfg(parallel_compiler)]\n impl<'tcx> QueryLatch<'tcx> {\n     fn new() -> Self {\n-        QueryLatch { info: Mutex::new(QueryLatchInfo { complete: false, waiters: Vec::new() }) }\n+        QueryLatch {\n+            info: Lrc::new(Mutex::new(QueryLatchInfo { complete: false, waiters: Vec::new() })),\n+        }\n+    }\n+\n+    /// Awaits for the query job to complete.\n+    #[cfg(parallel_compiler)]\n+    pub(super) fn wait_on(&self, tcx: TyCtxt<'tcx>, span: Span) -> Result<(), CycleError<'tcx>> {\n+        tls::with_related_context(tcx, move |icx| {\n+            let waiter = Lrc::new(QueryWaiter {\n+                query: icx.query,\n+                span,\n+                cycle: Lock::new(None),\n+                condvar: Condvar::new(),\n+            });\n+            self.wait_on_inner(&waiter);\n+            // FIXME: Get rid of this lock. We have ownership of the QueryWaiter\n+            // although another thread may still have a Lrc reference so we cannot\n+            // use Lrc::get_mut\n+            let mut cycle = waiter.cycle.lock();\n+            match cycle.take() {\n+                None => Ok(()),\n+                Some(cycle) => Err(cycle),\n+            }\n+        })\n     }\n \n     /// Awaits the caller on this latch by blocking the current thread.\n-    fn r#await(&self, waiter: &Lrc<QueryWaiter<'tcx>>) {\n+    fn wait_on_inner(&self, waiter: &Lrc<QueryWaiter<'tcx>>) {\n         let mut info = self.info.lock();\n         if !info.complete {\n             // We push the waiter on to the `waiters` list. It can be accessed inside\n@@ -197,7 +270,7 @@ impl<'tcx> QueryLatch<'tcx> {\n \n /// A resumable waiter of a query. The usize is the index into waiters in the query's latch\n #[cfg(parallel_compiler)]\n-type Waiter<'tcx> = (Lrc<QueryJob<'tcx>>, usize);\n+type Waiter = (QueryToken, usize);\n \n /// Visits all the non-resumable and resumable waiters of a query.\n /// Only waiters in a query are visited.\n@@ -209,26 +282,33 @@ type Waiter<'tcx> = (Lrc<QueryJob<'tcx>>, usize);\n /// required information to resume the waiter.\n /// If all `visit` calls returns None, this function also returns None.\n #[cfg(parallel_compiler)]\n-fn visit_waiters<'tcx, F>(query: Lrc<QueryJob<'tcx>>, mut visit: F) -> Option<Option<Waiter<'tcx>>>\n+fn visit_waiters<'tcx, F>(\n+    query_map: &QueryMap<'tcx>,\n+    query: QueryToken,\n+    mut visit: F,\n+) -> Option<Option<Waiter>>\n where\n-    F: FnMut(Span, Lrc<QueryJob<'tcx>>) -> Option<Option<Waiter<'tcx>>>,\n+    F: FnMut(Span, QueryToken) -> Option<Option<Waiter>>,\n {\n     // Visit the parent query which is a non-resumable waiter since it's on the same stack\n-    if let Some(ref parent) = query.parent {\n-        if let Some(cycle) = visit(query.info.span, parent.clone()) {\n+    if let Some(parent) = query.parent(query_map) {\n+        if let Some(cycle) = visit(query.span(query_map), parent) {\n             return Some(cycle);\n         }\n     }\n \n     // Visit the explicit waiters which use condvars and are resumable\n-    for (i, waiter) in query.latch.info.lock().waiters.iter().enumerate() {\n-        if let Some(ref waiter_query) = waiter.query {\n-            if visit(waiter.span, waiter_query.clone()).is_some() {\n-                // Return a value which indicates that this waiter can be resumed\n-                return Some(Some((query.clone(), i)));\n+    if let Some(latch) = query.latch(query_map) {\n+        for (i, waiter) in latch.info.lock().waiters.iter().enumerate() {\n+            if let Some(waiter_query) = waiter.query {\n+                if visit(waiter.span, waiter_query).is_some() {\n+                    // Return a value which indicates that this waiter can be resumed\n+                    return Some(Some((query, i)));\n+                }\n             }\n         }\n     }\n+\n     None\n }\n \n@@ -238,13 +318,14 @@ where\n /// the cycle.\n #[cfg(parallel_compiler)]\n fn cycle_check<'tcx>(\n-    query: Lrc<QueryJob<'tcx>>,\n+    query_map: &QueryMap<'tcx>,\n+    query: QueryToken,\n     span: Span,\n-    stack: &mut Vec<(Span, Lrc<QueryJob<'tcx>>)>,\n-    visited: &mut FxHashSet<*const QueryJob<'tcx>>,\n-) -> Option<Option<Waiter<'tcx>>> {\n-    if !visited.insert(query.as_ptr()) {\n-        return if let Some(p) = stack.iter().position(|q| q.1.as_ptr() == query.as_ptr()) {\n+    stack: &mut Vec<(Span, QueryToken)>,\n+    visited: &mut FxHashSet<QueryToken>,\n+) -> Option<Option<Waiter>> {\n+    if !visited.insert(query) {\n+        return if let Some(p) = stack.iter().position(|q| q.1 == query) {\n             // We detected a query cycle, fix up the initial span and return Some\n \n             // Remove previous stack entries\n@@ -258,10 +339,12 @@ fn cycle_check<'tcx>(\n     }\n \n     // Query marked as visited is added it to the stack\n-    stack.push((span, query.clone()));\n+    stack.push((span, query));\n \n     // Visit all the waiters\n-    let r = visit_waiters(query, |span, successor| cycle_check(successor, span, stack, visited));\n+    let r = visit_waiters(query_map, query, |span, successor| {\n+        cycle_check(query_map, successor, span, stack, visited)\n+    });\n \n     // Remove the entry in our stack if we didn't find a cycle\n     if r.is_none() {\n@@ -276,26 +359,30 @@ fn cycle_check<'tcx>(\n /// This is achieved with a depth first search.\n #[cfg(parallel_compiler)]\n fn connected_to_root<'tcx>(\n-    query: Lrc<QueryJob<'tcx>>,\n-    visited: &mut FxHashSet<*const QueryJob<'tcx>>,\n+    query_map: &QueryMap<'tcx>,\n+    query: QueryToken,\n+    visited: &mut FxHashSet<QueryToken>,\n ) -> bool {\n     // We already visited this or we're deliberately ignoring it\n-    if !visited.insert(query.as_ptr()) {\n+    if !visited.insert(query) {\n         return false;\n     }\n \n     // This query is connected to the root (it has no query parent), return true\n-    if query.parent.is_none() {\n+    if query.parent(query_map).is_none() {\n         return true;\n     }\n \n-    visit_waiters(query, |_, successor| connected_to_root(successor, visited).then_some(None))\n-        .is_some()\n+    visit_waiters(query_map, query, |_, successor| {\n+        connected_to_root(query_map, successor, visited).then_some(None)\n+    })\n+    .is_some()\n }\n \n // Deterministically pick an query from a list\n #[cfg(parallel_compiler)]\n-fn pick_query<'a, 'tcx, T, F: Fn(&T) -> (Span, Lrc<QueryJob<'tcx>>)>(\n+fn pick_query<'a, 'tcx, T, F: Fn(&T) -> (Span, QueryToken)>(\n+    query_map: &QueryMap<'tcx>,\n     tcx: TyCtxt<'tcx>,\n     queries: &'a [T],\n     f: F,\n@@ -308,7 +395,7 @@ fn pick_query<'a, 'tcx, T, F: Fn(&T) -> (Span, Lrc<QueryJob<'tcx>>)>(\n         .min_by_key(|v| {\n             let (span, query) = f(v);\n             let mut stable_hasher = StableHasher::new();\n-            query.info.query.hash_stable(&mut hcx, &mut stable_hasher);\n+            query.query(query_map).hash_stable(&mut hcx, &mut stable_hasher);\n             // Prefer entry points which have valid spans for nicer error messages\n             // We add an integer to the tuple ensuring that entry points\n             // with valid spans are picked first\n@@ -325,14 +412,17 @@ fn pick_query<'a, 'tcx, T, F: Fn(&T) -> (Span, Lrc<QueryJob<'tcx>>)>(\n /// the function returns false.\n #[cfg(parallel_compiler)]\n fn remove_cycle<'tcx>(\n-    jobs: &mut Vec<Lrc<QueryJob<'tcx>>>,\n+    query_map: &QueryMap<'tcx>,\n+    jobs: &mut Vec<QueryToken>,\n     wakelist: &mut Vec<Lrc<QueryWaiter<'tcx>>>,\n     tcx: TyCtxt<'tcx>,\n ) -> bool {\n     let mut visited = FxHashSet::default();\n     let mut stack = Vec::new();\n     // Look for a cycle starting with the last query in `jobs`\n-    if let Some(waiter) = cycle_check(jobs.pop().unwrap(), DUMMY_SP, &mut stack, &mut visited) {\n+    if let Some(waiter) =\n+        cycle_check(query_map, jobs.pop().unwrap(), DUMMY_SP, &mut stack, &mut visited)\n+    {\n         // The stack is a vector of pairs of spans and queries; reverse it so that\n         // the earlier entries require later entries\n         let (mut spans, queries): (Vec<_>, Vec<_>) = stack.into_iter().rev().unzip();\n@@ -345,27 +435,25 @@ fn remove_cycle<'tcx>(\n \n         // Remove the queries in our cycle from the list of jobs to look at\n         for r in &stack {\n-            if let Some(pos) = jobs.iter().position(|j| j.as_ptr() == r.1.as_ptr()) {\n-                jobs.remove(pos);\n-            }\n+            jobs.remove_item(&r.1);\n         }\n \n         // Find the queries in the cycle which are\n         // connected to queries outside the cycle\n         let entry_points = stack\n             .iter()\n-            .filter_map(|(span, query)| {\n-                if query.parent.is_none() {\n+            .filter_map(|&(span, query)| {\n+                if query.parent(query_map).is_none() {\n                     // This query is connected to the root (it has no query parent)\n-                    Some((*span, query.clone(), None))\n+                    Some((span, query, None))\n                 } else {\n                     let mut waiters = Vec::new();\n                     // Find all the direct waiters who lead to the root\n-                    visit_waiters(query.clone(), |span, waiter| {\n+                    visit_waiters(query_map, query, |span, waiter| {\n                         // Mark all the other queries in the cycle as already visited\n-                        let mut visited = FxHashSet::from_iter(stack.iter().map(|q| q.1.as_ptr()));\n+                        let mut visited = FxHashSet::from_iter(stack.iter().map(|q| q.1));\n \n-                        if connected_to_root(waiter.clone(), &mut visited) {\n+                        if connected_to_root(query_map, waiter, &mut visited) {\n                             waiters.push((span, waiter));\n                         }\n \n@@ -375,31 +463,30 @@ fn remove_cycle<'tcx>(\n                         None\n                     } else {\n                         // Deterministically pick one of the waiters to show to the user\n-                        let waiter = pick_query(tcx, &waiters, |s| s.clone()).clone();\n-                        Some((*span, query.clone(), Some(waiter)))\n+                        let waiter = *pick_query(query_map, tcx, &waiters, |s| *s);\n+                        Some((span, query, Some(waiter)))\n                     }\n                 }\n             })\n-            .collect::<Vec<(Span, Lrc<QueryJob<'tcx>>, Option<(Span, Lrc<QueryJob<'tcx>>)>)>>();\n+            .collect::<Vec<(Span, QueryToken, Option<(Span, QueryToken)>)>>();\n \n         // Deterministically pick an entry point\n-        let (_, entry_point, usage) = pick_query(tcx, &entry_points, |e| (e.0, e.1.clone()));\n+        let (_, entry_point, usage) = pick_query(query_map, tcx, &entry_points, |e| (e.0, e.1));\n \n         // Shift the stack so that our entry point is first\n-        let entry_point_pos =\n-            stack.iter().position(|(_, query)| query.as_ptr() == entry_point.as_ptr());\n+        let entry_point_pos = stack.iter().position(|(_, query)| query == entry_point);\n         if let Some(pos) = entry_point_pos {\n             stack.rotate_left(pos);\n         }\n \n-        let usage = usage.as_ref().map(|(span, query)| (*span, query.info.query.clone()));\n+        let usage = usage.as_ref().map(|(span, query)| (*span, query.query(query_map)));\n \n         // Create the cycle error\n         let error = CycleError {\n             usage,\n             cycle: stack\n                 .iter()\n-                .map(|&(s, ref q)| QueryInfo { span: s, query: q.info.query.clone() })\n+                .map(|&(s, ref q)| QueryInfo { span: s, query: q.query(query_map) })\n                 .collect(),\n         };\n \n@@ -408,7 +495,7 @@ fn remove_cycle<'tcx>(\n         let (waitee_query, waiter_idx) = waiter.unwrap();\n \n         // Extract the waiter we want to resume\n-        let waiter = waitee_query.latch.extract_waiter(waiter_idx);\n+        let waiter = waitee_query.latch(query_map).unwrap().extract_waiter(waiter_idx);\n \n         // Set the cycle error so it will be picked up when resumed\n         *waiter.cycle.lock() = Some(error);\n@@ -460,12 +547,13 @@ fn deadlock(tcx: TyCtxt<'_>, registry: &rayon_core::Registry) {\n     });\n \n     let mut wakelist = Vec::new();\n-    let mut jobs: Vec<_> = tcx.queries.collect_active_jobs();\n+    let query_map = tcx.queries.try_collect_active_jobs().unwrap();\n+    let mut jobs: Vec<QueryToken> = query_map.keys().cloned().collect();\n \n     let mut found_cycle = false;\n \n     while jobs.len() > 0 {\n-        if remove_cycle(&mut jobs, &mut wakelist, tcx) {\n+        if remove_cycle(&query_map, &mut jobs, &mut wakelist, tcx) {\n             found_cycle = true;\n         }\n     }"}, {"sha": "ade8cdbb3e5bacc2535379b974cbc57da2234093", "filename": "src/librustc/ty/query/mod.rs", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "blob_url": "https://github.com/rust-lang/rust/blob/77ab0d091e32ac9ec4154bba1727fc3937975d64/src%2Flibrustc%2Fty%2Fquery%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/77ab0d091e32ac9ec4154bba1727fc3937975d64/src%2Flibrustc%2Fty%2Fquery%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Fty%2Fquery%2Fmod.rs?ref=77ab0d091e32ac9ec4154bba1727fc3937975d64", "patch": "@@ -66,7 +66,8 @@ pub use self::plumbing::{force_from_dep_node, CycleError};\n mod job;\n #[cfg(parallel_compiler)]\n pub use self::job::handle_deadlock;\n-pub use self::job::{QueryInfo, QueryJob};\n+use self::job::QueryJobInfo;\n+pub use self::job::{QueryInfo, QueryJob, QueryToken};\n \n mod keys;\n use self::keys::Key;"}, {"sha": "fdb86c0bede0743372e92cb00733673006c501ae", "filename": "src/librustc/ty/query/plumbing.rs", "status": "modified", "additions": 81, "deletions": 46, "changes": 127, "blob_url": "https://github.com/rust-lang/rust/blob/77ab0d091e32ac9ec4154bba1727fc3937975d64/src%2Flibrustc%2Fty%2Fquery%2Fplumbing.rs", "raw_url": "https://github.com/rust-lang/rust/raw/77ab0d091e32ac9ec4154bba1727fc3937975d64/src%2Flibrustc%2Fty%2Fquery%2Fplumbing.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Fty%2Fquery%2Fplumbing.rs?ref=77ab0d091e32ac9ec4154bba1727fc3937975d64", "patch": "@@ -4,7 +4,7 @@\n \n use crate::dep_graph::{DepKind, DepNode, DepNodeIndex, SerializedDepNodeIndex};\n use crate::ty::query::config::{QueryConfig, QueryDescription};\n-use crate::ty::query::job::{QueryInfo, QueryJob};\n+use crate::ty::query::job::{QueryInfo, QueryJob, QueryToken};\n use crate::ty::query::Query;\n use crate::ty::tls;\n use crate::ty::{self, TyCtxt};\n@@ -15,7 +15,7 @@ use rustc_data_structures::fx::{FxHashMap, FxHasher};\n #[cfg(parallel_compiler)]\n use rustc_data_structures::profiling::TimingGuard;\n use rustc_data_structures::sharded::Sharded;\n-use rustc_data_structures::sync::{Lock, Lrc};\n+use rustc_data_structures::sync::Lock;\n use rustc_data_structures::thin_vec::ThinVec;\n use rustc_errors::{struct_span_err, Diagnostic, DiagnosticBuilder, FatalError, Handler, Level};\n use rustc_span::source_map::DUMMY_SP;\n@@ -46,9 +46,9 @@ impl<T> QueryValue<T> {\n /// Indicates the state of a query for a given key in a query map.\n pub(super) enum QueryResult<'tcx> {\n     /// An already executing query. The query job can be used to await for its completion.\n-    Started(Lrc<QueryJob<'tcx>>),\n+    Started(QueryJob<'tcx>),\n \n-    /// The query panicked. Queries trying to wait on this will raise a fatal error or\n+    /// The query panicked. Queries trying to wait on this will raise a fatal error which will\n     /// silently panic.\n     Poisoned,\n }\n@@ -69,7 +69,7 @@ impl<'tcx, M: QueryConfig<'tcx>> Default for QueryCache<'tcx, M> {\n pub(super) struct JobOwner<'a, 'tcx, Q: QueryDescription<'tcx>> {\n     cache: &'a Sharded<QueryCache<'tcx, Q>>,\n     key: Q::Key,\n-    job: Lrc<QueryJob<'tcx>>,\n+    token: QueryToken,\n }\n \n impl<'a, 'tcx, Q: QueryDescription<'tcx>> JobOwner<'a, 'tcx, Q> {\n@@ -81,7 +81,12 @@ impl<'a, 'tcx, Q: QueryDescription<'tcx>> JobOwner<'a, 'tcx, Q> {\n     /// This function is inlined because that results in a noticeable speed-up\n     /// for some compile-time benchmarks.\n     #[inline(always)]\n-    pub(super) fn try_get(tcx: TyCtxt<'tcx>, span: Span, key: &Q::Key) -> TryGetJob<'a, 'tcx, Q> {\n+    pub(super) fn try_get(\n+        tcx: TyCtxt<'tcx>,\n+        span: Span,\n+        key: &Q::Key,\n+        token: QueryToken,\n+    ) -> TryGetJob<'a, 'tcx, Q> {\n         // Handling the `query_blocked_prof_timer` is a bit weird because of the\n         // control flow in this function: Blocking is implemented by\n         // awaiting a running job and, once that is done, entering the loop below\n@@ -127,10 +132,10 @@ impl<'a, 'tcx, Q: QueryDescription<'tcx>> JobOwner<'a, 'tcx, Q> {\n                 return TryGetJob::JobCompleted(result);\n             }\n \n-            let job = match lock.active.entry((*key).clone()) {\n-                Entry::Occupied(entry) => {\n-                    match *entry.get() {\n-                        QueryResult::Started(ref job) => {\n+            let latch = match lock.active.entry((*key).clone()) {\n+                Entry::Occupied(mut entry) => {\n+                    match entry.get_mut() {\n+                        QueryResult::Started(job) => {\n                             // For parallel queries, we'll block and wait until the query running\n                             // in another thread has completed. Record how long we wait in the\n                             // self-profiler.\n@@ -139,20 +144,16 @@ impl<'a, 'tcx, Q: QueryDescription<'tcx>> JobOwner<'a, 'tcx, Q> {\n                                 query_blocked_prof_timer = Some(tcx.prof.query_blocked());\n                             }\n \n-                            job.clone()\n+                            job.latch()\n                         }\n                         QueryResult::Poisoned => FatalError.raise(),\n                     }\n                 }\n                 Entry::Vacant(entry) => {\n                     // No job entry for this query. Return a new one to be started later.\n                     return tls::with_related_context(tcx, |icx| {\n-                        // Create the `parent` variable before `info`. This allows LLVM\n-                        // to elide the move of `info`\n-                        let parent = icx.query.clone();\n-                        let info = QueryInfo { span, query: Q::query(key.clone()) };\n-                        let job = Lrc::new(QueryJob::new(info, parent));\n-                        let owner = JobOwner { cache, job: job.clone(), key: (*key).clone() };\n+                        let job = QueryJob::new(token, span, icx.query);\n+                        let owner = JobOwner { cache, token, key: (*key).clone() };\n                         entry.insert(QueryResult::Started(job));\n                         TryGetJob::NotYetStarted(owner)\n                     });\n@@ -164,14 +165,14 @@ impl<'a, 'tcx, Q: QueryDescription<'tcx>> JobOwner<'a, 'tcx, Q> {\n             // so we just return the error.\n             #[cfg(not(parallel_compiler))]\n             return TryGetJob::Cycle(cold_path(|| {\n-                Q::handle_cycle_error(tcx, job.find_cycle_in_stack(tcx, span))\n+                Q::handle_cycle_error(tcx, latch.find_cycle_in_stack(tcx, span))\n             }));\n \n             // With parallel queries we might just have to wait on some other\n             // thread.\n             #[cfg(parallel_compiler)]\n             {\n-                let result = job.r#await(tcx, span);\n+                let result = latch.wait_on(tcx, span);\n \n                 if let Err(cycle) = result {\n                     return TryGetJob::Cycle(Q::handle_cycle_error(tcx, cycle));\n@@ -186,18 +187,21 @@ impl<'a, 'tcx, Q: QueryDescription<'tcx>> JobOwner<'a, 'tcx, Q> {\n     pub(super) fn complete(self, result: &Q::Value, dep_node_index: DepNodeIndex) {\n         // We can move out of `self` here because we `mem::forget` it below\n         let key = unsafe { ptr::read(&self.key) };\n-        let job = unsafe { ptr::read(&self.job) };\n         let cache = self.cache;\n \n         // Forget ourself so our destructor won't poison the query\n         mem::forget(self);\n \n         let value = QueryValue::new(result.clone(), dep_node_index);\n-        {\n+        let job = {\n             let mut lock = cache.get_shard_by_value(&key).lock();\n-            lock.active.remove(&key);\n+            let job = match lock.active.remove(&key).unwrap() {\n+                QueryResult::Started(job) => job,\n+                QueryResult::Poisoned => panic!(),\n+            };\n             lock.results.insert(key, value);\n-        }\n+            job\n+        };\n \n         job.signal_complete();\n     }\n@@ -219,10 +223,18 @@ impl<'a, 'tcx, Q: QueryDescription<'tcx>> Drop for JobOwner<'a, 'tcx, Q> {\n     fn drop(&mut self) {\n         // Poison the query so jobs waiting on it panic.\n         let shard = self.cache.get_shard_by_value(&self.key);\n-        shard.lock().active.insert(self.key.clone(), QueryResult::Poisoned);\n+        let job = {\n+            let mut shard = shard.lock();\n+            let job = match shard.active.remove(&self.key).unwrap() {\n+                QueryResult::Started(job) => job,\n+                QueryResult::Poisoned => panic!(),\n+            };\n+            shard.active.insert(self.key.clone(), QueryResult::Poisoned);\n+            job\n+        };\n         // Also signal the completion of the job, so waiters\n         // will continue execution.\n-        self.job.signal_complete();\n+        job.signal_complete();\n     }\n }\n \n@@ -254,7 +266,7 @@ impl<'tcx> TyCtxt<'tcx> {\n     #[inline(always)]\n     pub(super) fn start_query<F, R>(\n         self,\n-        job: Lrc<QueryJob<'tcx>>,\n+        token: QueryToken,\n         diagnostics: Option<&Lock<ThinVec<Diagnostic>>>,\n         compute: F,\n     ) -> R\n@@ -268,7 +280,7 @@ impl<'tcx> TyCtxt<'tcx> {\n             // Update the `ImplicitCtxt` to point to our new query job.\n             let new_icx = tls::ImplicitCtxt {\n                 tcx: self,\n-                query: Some(job),\n+                query: Some(token),\n                 diagnostics,\n                 layout_depth: current_icx.layout_depth,\n                 task_deps: current_icx.task_deps,\n@@ -335,23 +347,31 @@ impl<'tcx> TyCtxt<'tcx> {\n         // state if it was responsible for triggering the panic.\n         tls::with_context_opt(|icx| {\n             if let Some(icx) = icx {\n-                let mut current_query = icx.query.clone();\n+                let query_map = icx.tcx.queries.try_collect_active_jobs();\n+\n+                let mut current_query = icx.query;\n                 let mut i = 0;\n \n                 while let Some(query) = current_query {\n+                    let query_info =\n+                        if let Some(info) = query_map.as_ref().and_then(|map| map.get(&query)) {\n+                            info\n+                        } else {\n+                            break;\n+                        };\n                     let mut diag = Diagnostic::new(\n                         Level::FailureNote,\n                         &format!(\n                             \"#{} [{}] {}\",\n                             i,\n-                            query.info.query.name(),\n-                            query.info.query.describe(icx.tcx)\n+                            query_info.info.query.name(),\n+                            query_info.info.query.describe(icx.tcx)\n                         ),\n                     );\n-                    diag.span = icx.tcx.sess.source_map().def_span(query.info.span).into();\n+                    diag.span = icx.tcx.sess.source_map().def_span(query_info.info.span).into();\n                     handler.force_print_diagnostic(diag);\n \n-                    current_query = query.parent.clone();\n+                    current_query = query_info.job.parent;\n                     i += 1;\n                 }\n             }\n@@ -364,7 +384,12 @@ impl<'tcx> TyCtxt<'tcx> {\n     pub(super) fn get_query<Q: QueryDescription<'tcx>>(self, span: Span, key: Q::Key) -> Q::Value {\n         debug!(\"ty::query::get_query<{}>(key={:?}, span={:?})\", Q::NAME, key, span);\n \n-        let job = match JobOwner::try_get(self, span, &key) {\n+        // Create a token which uniquely identifies this query amongst the executing queries\n+        // by using a pointer to `key`. `key` is alive until the query completes execution\n+        // which will prevent reuse of the token value.\n+        let token = QueryToken::from(&key);\n+\n+        let job = match JobOwner::try_get(self, span, &key, token) {\n             TryGetJob::NotYetStarted(job) => job,\n             TryGetJob::Cycle(result) => return result,\n             TryGetJob::JobCompleted((v, index)) => {\n@@ -384,7 +409,7 @@ impl<'tcx> TyCtxt<'tcx> {\n             let prof_timer = self.prof.query_provider();\n \n             let ((result, dep_node_index), diagnostics) = with_diagnostics(|diagnostics| {\n-                self.start_query(job.job.clone(), diagnostics, |tcx| {\n+                self.start_query(job.token, diagnostics, |tcx| {\n                     tcx.dep_graph.with_anon_task(Q::dep_kind(), || Q::compute(tcx, key))\n                 })\n             });\n@@ -410,7 +435,7 @@ impl<'tcx> TyCtxt<'tcx> {\n             // The diagnostics for this query will be\n             // promoted to the current session during\n             // `try_mark_green()`, so we can ignore them here.\n-            let loaded = self.start_query(job.job.clone(), None, |tcx| {\n+            let loaded = self.start_query(job.token, None, |tcx| {\n                 let marked = tcx.dep_graph.try_mark_green_and_read(tcx, &dep_node);\n                 marked.map(|(prev_dep_node_index, dep_node_index)| {\n                     (\n@@ -544,7 +569,7 @@ impl<'tcx> TyCtxt<'tcx> {\n         let prof_timer = self.prof.query_provider();\n \n         let ((result, dep_node_index), diagnostics) = with_diagnostics(|diagnostics| {\n-            self.start_query(job.job.clone(), diagnostics, |tcx| {\n+            self.start_query(job.token, diagnostics, |tcx| {\n                 if Q::EVAL_ALWAYS {\n                     tcx.dep_graph.with_eval_always_task(\n                         dep_node,\n@@ -608,9 +633,14 @@ impl<'tcx> TyCtxt<'tcx> {\n \n     #[allow(dead_code)]\n     fn force_query<Q: QueryDescription<'tcx>>(self, key: Q::Key, span: Span, dep_node: DepNode) {\n+        // Create a token which uniquely identifies this query amongst the executing queries\n+        // by using a pointer to `key`. `key` is alive until the query completes execution\n+        // which will prevent reuse of the token value.\n+        let token = QueryToken::from(&key);\n+\n         // We may be concurrently trying both execute and force a query.\n         // Ensure that only one of them runs the query.\n-        let job = match JobOwner::try_get(self, span, &key) {\n+        let job = match JobOwner::try_get(self, span, &key, token) {\n             TryGetJob::NotYetStarted(job) => job,\n             TryGetJob::Cycle(_) | TryGetJob::JobCompleted(_) => return,\n         };\n@@ -716,24 +746,29 @@ macro_rules! define_queries_inner {\n                 }\n             }\n \n-            #[cfg(parallel_compiler)]\n-            pub fn collect_active_jobs(&self) -> Vec<Lrc<QueryJob<$tcx>>> {\n-                let mut jobs = Vec::new();\n+            pub fn try_collect_active_jobs(\n+                &self\n+            ) -> Option<FxHashMap<QueryToken, QueryJobInfo<'tcx>>> {\n+                let mut jobs = FxHashMap::default();\n \n-                // We use try_lock_shards here since we are only called from the\n-                // deadlock handler, and this shouldn't be locked.\n                 $(\n-                    let shards = self.$name.try_lock_shards().unwrap();\n-                    jobs.extend(shards.iter().flat_map(|shard| shard.active.values().filter_map(|v|\n+                    // We use try_lock_shards here since we are called from the\n+                    // deadlock handler, and this shouldn't be locked.\n+                    let shards = self.$name.try_lock_shards()?;\n+                    jobs.extend(shards.iter().flat_map(|shard| shard.active.iter().filter_map(|(k, v)|\n                         if let QueryResult::Started(ref job) = *v {\n-                            Some(job.clone())\n+                            let info = QueryInfo {\n+                                span: job.span,\n+                                query: queries::$name::query(k.clone())\n+                            };\n+                            Some((job.token, QueryJobInfo { info,  job: job.clone() }))\n                         } else {\n                             None\n                         }\n                     )));\n                 )*\n \n-                jobs\n+                Some(jobs)\n             }\n \n             pub fn print_stats(&self) {"}]}