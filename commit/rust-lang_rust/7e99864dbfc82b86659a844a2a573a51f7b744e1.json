{"sha": "7e99864dbfc82b86659a844a2a573a51f7b744e1", "node_id": "C_kwDOAAsO6NoAKDdlOTk4NjRkYmZjODJiODY2NTlhODQ0YTJhNTczYTUxZjdiNzQ0ZTE", "commit": {"author": {"name": "Aleksey Kladov", "email": "aleksey.kladov@gmail.com", "date": "2021-12-12T18:32:58Z"}, "committer": {"name": "Aleksey Kladov", "email": "aleksey.kladov@gmail.com", "date": "2021-12-18T11:55:20Z"}, "message": "move lexing to the parser crate", "tree": {"sha": "382abc9e07819f68ab81d4681efd24c5250d938c", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/382abc9e07819f68ab81d4681efd24c5250d938c"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/7e99864dbfc82b86659a844a2a573a51f7b744e1", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/7e99864dbfc82b86659a844a2a573a51f7b744e1", "html_url": "https://github.com/rust-lang/rust/commit/7e99864dbfc82b86659a844a2a573a51f7b744e1", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/7e99864dbfc82b86659a844a2a573a51f7b744e1/comments", "author": {"login": "matklad", "id": 1711539, "node_id": "MDQ6VXNlcjE3MTE1Mzk=", "avatar_url": "https://avatars.githubusercontent.com/u/1711539?v=4", "gravatar_id": "", "url": "https://api.github.com/users/matklad", "html_url": "https://github.com/matklad", "followers_url": "https://api.github.com/users/matklad/followers", "following_url": "https://api.github.com/users/matklad/following{/other_user}", "gists_url": "https://api.github.com/users/matklad/gists{/gist_id}", "starred_url": "https://api.github.com/users/matklad/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/matklad/subscriptions", "organizations_url": "https://api.github.com/users/matklad/orgs", "repos_url": "https://api.github.com/users/matklad/repos", "events_url": "https://api.github.com/users/matklad/events{/privacy}", "received_events_url": "https://api.github.com/users/matklad/received_events", "type": "User", "site_admin": false}, "committer": {"login": "matklad", "id": 1711539, "node_id": "MDQ6VXNlcjE3MTE1Mzk=", "avatar_url": "https://avatars.githubusercontent.com/u/1711539?v=4", "gravatar_id": "", "url": "https://api.github.com/users/matklad", "html_url": "https://github.com/matklad", "followers_url": "https://api.github.com/users/matklad/followers", "following_url": "https://api.github.com/users/matklad/following{/other_user}", "gists_url": "https://api.github.com/users/matklad/gists{/gist_id}", "starred_url": "https://api.github.com/users/matklad/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/matklad/subscriptions", "organizations_url": "https://api.github.com/users/matklad/orgs", "repos_url": "https://api.github.com/users/matklad/repos", "events_url": "https://api.github.com/users/matklad/events{/privacy}", "received_events_url": "https://api.github.com/users/matklad/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "958f20ff843fb14e79eb6f684fe3ed24f67e8c34", "url": "https://api.github.com/repos/rust-lang/rust/commits/958f20ff843fb14e79eb6f684fe3ed24f67e8c34", "html_url": "https://github.com/rust-lang/rust/commit/958f20ff843fb14e79eb6f684fe3ed24f67e8c34"}], "stats": {"total": 291, "additions": 289, "deletions": 2}, "files": [{"sha": "2ca5899c77c7fae9ec1676397e90a8e4efa65d07", "filename": "Cargo.lock", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/7e99864dbfc82b86659a844a2a573a51f7b744e1/Cargo.lock", "raw_url": "https://github.com/rust-lang/rust/raw/7e99864dbfc82b86659a844a2a573a51f7b744e1/Cargo.lock", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/Cargo.lock?ref=7e99864dbfc82b86659a844a2a573a51f7b744e1", "patch": "@@ -1066,7 +1066,9 @@ name = \"parser\"\n version = \"0.0.0\"\n dependencies = [\n  \"drop_bomb\",\n+ \"expect-test\",\n  \"limit\",\n+ \"rustc-ap-rustc_lexer\",\n ]\n \n [[package]]"}, {"sha": "4028082d0885f1459edd19324fb890340c0444e0", "filename": "crates/parser/Cargo.toml", "status": "modified", "additions": 4, "deletions": 1, "changes": 5, "blob_url": "https://github.com/rust-lang/rust/blob/7e99864dbfc82b86659a844a2a573a51f7b744e1/crates%2Fparser%2FCargo.toml", "raw_url": "https://github.com/rust-lang/rust/raw/7e99864dbfc82b86659a844a2a573a51f7b744e1/crates%2Fparser%2FCargo.toml", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fparser%2FCargo.toml?ref=7e99864dbfc82b86659a844a2a573a51f7b744e1", "patch": "@@ -11,5 +11,8 @@ doctest = false\n \n [dependencies]\n drop_bomb = \"0.1.4\"\n-\n+rustc_lexer = { version = \"725.0.0\", package = \"rustc-ap-rustc_lexer\" }\n limit = { path = \"../limit\", version = \"0.0.0\" }\n+\n+[dev-dependencies]\n+expect-test = \"1.2\""}, {"sha": "a9134639d27855dd5a066940ffab85dc1cb54d3e", "filename": "crates/parser/src/lexer_token.rs", "status": "added", "additions": 210, "deletions": 0, "changes": 210, "blob_url": "https://github.com/rust-lang/rust/blob/7e99864dbfc82b86659a844a2a573a51f7b744e1/crates%2Fparser%2Fsrc%2Flexer_token.rs", "raw_url": "https://github.com/rust-lang/rust/raw/7e99864dbfc82b86659a844a2a573a51f7b744e1/crates%2Fparser%2Fsrc%2Flexer_token.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fparser%2Fsrc%2Flexer_token.rs?ref=7e99864dbfc82b86659a844a2a573a51f7b744e1", "patch": "@@ -0,0 +1,210 @@\n+//! Lexing `&str` into a sequence of Rust tokens.\n+//!\n+//! Note that strictly speaking the parser in this crate is not required to work\n+//! on tokens which originated from text. Macros, eg, can synthesize tokes out\n+//! of thin air. So, ideally, lexer should be an orthogonal crate. It is however\n+//! convenient to include a text-based lexer here!\n+\n+use crate::{\n+    SyntaxKind::{self, *},\n+    T,\n+};\n+\n+#[derive(Debug, Clone, PartialEq, Eq, PartialOrd, Ord, Hash)]\n+pub struct LexerToken {\n+    pub kind: SyntaxKind,\n+    pub len: usize,\n+    pub error: Option<String>,\n+}\n+\n+impl LexerToken {\n+    pub fn new(kind: SyntaxKind, len: usize) -> Self {\n+        Self { kind, len, error: None }\n+    }\n+\n+    /// Lexes text as a sequence of tokens.\n+    pub fn tokenize(text: &str) -> Vec<LexerToken> {\n+        let mut res = Vec::new();\n+        let mut offset = 0;\n+\n+        if let Some(shebang_len) = rustc_lexer::strip_shebang(text) {\n+            res.push(LexerToken::new(SHEBANG, shebang_len));\n+            offset = shebang_len\n+        };\n+\n+        for token in rustc_lexer::tokenize(&text[offset..]) {\n+            let token_text = &text[offset..][..token.len];\n+            offset += token.len;\n+\n+            let (kind, err) = from_rustc(&token.kind, token_text);\n+            let mut token = LexerToken::new(kind, token.len);\n+            token.error = err.map(|it| it.to_string());\n+            res.push(token);\n+        }\n+\n+        res\n+    }\n+    /// Lexes text as a single token. Returns `None` if there's leftover text.\n+    pub fn from_str(text: &str) -> Option<LexerToken> {\n+        if text.is_empty() {\n+            return None;\n+        }\n+\n+        let token = rustc_lexer::first_token(text);\n+        if token.len != text.len() {\n+            return None;\n+        }\n+\n+        let (kind, err) = from_rustc(&token.kind, text);\n+\n+        let mut token = LexerToken::new(kind, token.len);\n+        token.error = err.map(|it| it.to_string());\n+        Some(token)\n+    }\n+}\n+\n+/// Returns `SyntaxKind` and an optional tokenize error message.\n+fn from_rustc(\n+    kind: &rustc_lexer::TokenKind,\n+    token_text: &str,\n+) -> (SyntaxKind, Option<&'static str>) {\n+    // A note on an intended tradeoff:\n+    // We drop some useful information here (see patterns with double dots `..`)\n+    // Storing that info in `SyntaxKind` is not possible due to its layout requirements of\n+    // being `u16` that come from `rowan::SyntaxKind`.\n+    let mut err = \"\";\n+\n+    let syntax_kind = {\n+        match kind {\n+            rustc_lexer::TokenKind::LineComment { doc_style: _ } => COMMENT,\n+            rustc_lexer::TokenKind::BlockComment { doc_style: _, terminated } => {\n+                if !terminated {\n+                    err = \"Missing trailing `*/` symbols to terminate the block comment\";\n+                }\n+                COMMENT\n+            }\n+\n+            rustc_lexer::TokenKind::Whitespace => WHITESPACE,\n+\n+            rustc_lexer::TokenKind::Ident if token_text == \"_\" => UNDERSCORE,\n+            rustc_lexer::TokenKind::Ident => SyntaxKind::from_keyword(token_text).unwrap_or(IDENT),\n+\n+            rustc_lexer::TokenKind::RawIdent => IDENT,\n+            rustc_lexer::TokenKind::Literal { kind, .. } => return from_rustc_literal(kind),\n+\n+            rustc_lexer::TokenKind::Lifetime { starts_with_number } => {\n+                if *starts_with_number {\n+                    err = \"Lifetime name cannot start with a number\";\n+                }\n+                LIFETIME_IDENT\n+            }\n+\n+            rustc_lexer::TokenKind::Semi => T![;],\n+            rustc_lexer::TokenKind::Comma => T![,],\n+            rustc_lexer::TokenKind::Dot => T![.],\n+            rustc_lexer::TokenKind::OpenParen => T!['('],\n+            rustc_lexer::TokenKind::CloseParen => T![')'],\n+            rustc_lexer::TokenKind::OpenBrace => T!['{'],\n+            rustc_lexer::TokenKind::CloseBrace => T!['}'],\n+            rustc_lexer::TokenKind::OpenBracket => T!['['],\n+            rustc_lexer::TokenKind::CloseBracket => T![']'],\n+            rustc_lexer::TokenKind::At => T![@],\n+            rustc_lexer::TokenKind::Pound => T![#],\n+            rustc_lexer::TokenKind::Tilde => T![~],\n+            rustc_lexer::TokenKind::Question => T![?],\n+            rustc_lexer::TokenKind::Colon => T![:],\n+            rustc_lexer::TokenKind::Dollar => T![$],\n+            rustc_lexer::TokenKind::Eq => T![=],\n+            rustc_lexer::TokenKind::Bang => T![!],\n+            rustc_lexer::TokenKind::Lt => T![<],\n+            rustc_lexer::TokenKind::Gt => T![>],\n+            rustc_lexer::TokenKind::Minus => T![-],\n+            rustc_lexer::TokenKind::And => T![&],\n+            rustc_lexer::TokenKind::Or => T![|],\n+            rustc_lexer::TokenKind::Plus => T![+],\n+            rustc_lexer::TokenKind::Star => T![*],\n+            rustc_lexer::TokenKind::Slash => T![/],\n+            rustc_lexer::TokenKind::Caret => T![^],\n+            rustc_lexer::TokenKind::Percent => T![%],\n+            rustc_lexer::TokenKind::Unknown => ERROR,\n+        }\n+    };\n+\n+    let err = if err.is_empty() { None } else { Some(err) };\n+    (syntax_kind, err)\n+}\n+\n+fn from_rustc_literal(kind: &rustc_lexer::LiteralKind) -> (SyntaxKind, Option<&'static str>) {\n+    let mut err = \"\";\n+\n+    let syntax_kind = match *kind {\n+        rustc_lexer::LiteralKind::Int { empty_int, base: _ } => {\n+            if empty_int {\n+                err = \"Missing digits after the integer base prefix\";\n+            }\n+            INT_NUMBER\n+        }\n+        rustc_lexer::LiteralKind::Float { empty_exponent, base: _ } => {\n+            if empty_exponent {\n+                err = \"Missing digits after the exponent symbol\";\n+            }\n+            FLOAT_NUMBER\n+        }\n+        rustc_lexer::LiteralKind::Char { terminated } => {\n+            if !terminated {\n+                err = \"Missing trailing `'` symbol to terminate the character literal\";\n+            }\n+            CHAR\n+        }\n+        rustc_lexer::LiteralKind::Byte { terminated } => {\n+            if !terminated {\n+                err = \"Missing trailing `'` symbol to terminate the byte literal\";\n+            }\n+            BYTE\n+        }\n+        rustc_lexer::LiteralKind::Str { terminated } => {\n+            if !terminated {\n+                err = \"Missing trailing `\\\"` symbol to terminate the string literal\";\n+            }\n+            STRING\n+        }\n+        rustc_lexer::LiteralKind::ByteStr { terminated } => {\n+            if !terminated {\n+                err = \"Missing trailing `\\\"` symbol to terminate the byte string literal\";\n+            }\n+            BYTE_STRING\n+        }\n+        rustc_lexer::LiteralKind::RawStr { err: raw_str_err, .. } => {\n+            if let Some(raw_str_err) = raw_str_err {\n+                err = match raw_str_err {\n+                    rustc_lexer::RawStrError::InvalidStarter { .. } => \"Missing `\\\"` symbol after `#` symbols to begin the raw string literal\",\n+                    rustc_lexer::RawStrError::NoTerminator { expected, found, .. } => if expected == found {\n+                        \"Missing trailing `\\\"` to terminate the raw string literal\"\n+                    } else {\n+                        \"Missing trailing `\\\"` with `#` symbols to terminate the raw string literal\"\n+                    },\n+                    rustc_lexer::RawStrError::TooManyDelimiters { .. } => \"Too many `#` symbols: raw strings may be delimited by up to 65535 `#` symbols\",\n+                };\n+            };\n+            STRING\n+        }\n+        rustc_lexer::LiteralKind::RawByteStr { err: raw_str_err, .. } => {\n+            if let Some(raw_str_err) = raw_str_err {\n+                err = match raw_str_err {\n+                    rustc_lexer::RawStrError::InvalidStarter { .. } => \"Missing `\\\"` symbol after `#` symbols to begin the raw byte string literal\",\n+                    rustc_lexer::RawStrError::NoTerminator { expected, found, .. } => if expected == found {\n+                        \"Missing trailing `\\\"` to terminate the raw byte string literal\"\n+                    } else {\n+                        \"Missing trailing `\\\"` with `#` symbols to terminate the raw byte string literal\"\n+                    },\n+                    rustc_lexer::RawStrError::TooManyDelimiters { .. } => \"Too many `#` symbols: raw byte strings may be delimited by up to 65535 `#` symbols\",\n+                };\n+            };\n+\n+            BYTE_STRING\n+        }\n+    };\n+\n+    let err = if err.is_empty() { None } else { Some(err) };\n+    (syntax_kind, err)\n+}"}, {"sha": "448f22185d07fa19c12456c1bc5fd51e08670e4f", "filename": "crates/parser/src/lib.rs", "status": "modified", "additions": 5, "deletions": 1, "changes": 6, "blob_url": "https://github.com/rust-lang/rust/blob/7e99864dbfc82b86659a844a2a573a51f7b744e1/crates%2Fparser%2Fsrc%2Flib.rs", "raw_url": "https://github.com/rust-lang/rust/raw/7e99864dbfc82b86659a844a2a573a51f7b744e1/crates%2Fparser%2Fsrc%2Flib.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fparser%2Fsrc%2Flib.rs?ref=7e99864dbfc82b86659a844a2a573a51f7b744e1", "patch": "@@ -18,16 +18,20 @@\n //! [`Parser`]: crate::parser::Parser\n #![allow(rustdoc::private_intra_doc_links)]\n \n+mod lexer_token;\n mod token_set;\n mod syntax_kind;\n mod event;\n mod parser;\n mod grammar;\n mod tokens;\n \n+#[cfg(test)]\n+mod tests;\n+\n pub(crate) use token_set::TokenSet;\n \n-pub use crate::{syntax_kind::SyntaxKind, tokens::Tokens};\n+pub use crate::{lexer_token::LexerToken, syntax_kind::SyntaxKind, tokens::Tokens};\n \n #[derive(Debug, Clone, PartialEq, Eq, Hash)]\n pub struct ParseError(pub Box<String>);"}, {"sha": "f323eba5e4355df45cc643ba659237c4fd40cc33", "filename": "crates/parser/src/tests.rs", "status": "added", "additions": 68, "deletions": 0, "changes": 68, "blob_url": "https://github.com/rust-lang/rust/blob/7e99864dbfc82b86659a844a2a573a51f7b744e1/crates%2Fparser%2Fsrc%2Ftests.rs", "raw_url": "https://github.com/rust-lang/rust/raw/7e99864dbfc82b86659a844a2a573a51f7b744e1/crates%2Fparser%2Fsrc%2Ftests.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fparser%2Fsrc%2Ftests.rs?ref=7e99864dbfc82b86659a844a2a573a51f7b744e1", "patch": "@@ -0,0 +1,68 @@\n+use std::{\n+    fmt::Write,\n+    fs,\n+    path::{Path, PathBuf},\n+};\n+\n+use expect_test::expect_file;\n+\n+use crate::LexerToken;\n+\n+#[test]\n+fn valid_lexes_input() {\n+    for case in TestCase::list(\"lexer/ok\") {\n+        let actual = lex(&case.text);\n+        expect_file![case.txt].assert_eq(&actual)\n+    }\n+}\n+\n+#[test]\n+fn invalid_lexes_input() {\n+    for case in TestCase::list(\"lexer/err\") {\n+        let actual = lex(&case.text);\n+        expect_file![case.txt].assert_eq(&actual)\n+    }\n+}\n+\n+fn lex(text: &str) -> String {\n+    let mut res = String::new();\n+    let mut offset = 0;\n+    for token in LexerToken::tokenize(text) {\n+        let token_text = &text[offset..][..token.len];\n+        offset += token.len;\n+        let err = token.error.map(|err| format!(\" error: {}\", err)).unwrap_or_default();\n+        writeln!(res, \"{:?} {:?}{}\", token.kind, token_text, err).unwrap();\n+    }\n+    res\n+}\n+\n+#[derive(PartialEq, Eq, PartialOrd, Ord)]\n+struct TestCase {\n+    rs: PathBuf,\n+    txt: PathBuf,\n+    text: String,\n+}\n+\n+impl TestCase {\n+    fn list(path: &'static str) -> Vec<TestCase> {\n+        let crate_root_dir = Path::new(env!(\"CARGO_MANIFEST_DIR\"));\n+        let test_data_dir = crate_root_dir.join(\"test_data\");\n+        let dir = test_data_dir.join(path);\n+\n+        let mut res = Vec::new();\n+        let read_dir = fs::read_dir(&dir)\n+            .unwrap_or_else(|err| panic!(\"can't `read_dir` {}: {}\", dir.display(), err));\n+        for file in read_dir {\n+            let file = file.unwrap();\n+            let path = file.path();\n+            if path.extension().unwrap_or_default() == \"rs\" {\n+                let rs = path;\n+                let txt = rs.with_extension(\"txt\");\n+                let text = fs::read_to_string(&rs).unwrap();\n+                res.push(TestCase { rs, txt, text });\n+            }\n+        }\n+        res.sort();\n+        res\n+    }\n+}"}]}