{"sha": "db2a7087b994e20f264f26ad6db75184282ad120", "node_id": "C_kwDOAAsO6NoAKGRiMmE3MDg3Yjk5NGUyMGYyNjRmMjZhZDZkYjc1MTg0MjgyYWQxMjA", "commit": {"author": {"name": "bors[bot]", "email": "26634292+bors[bot]@users.noreply.github.com", "date": "2021-12-12T16:38:56Z"}, "committer": {"name": "GitHub", "email": "noreply@github.com", "date": "2021-12-12T16:38:56Z"}, "message": "Merge #10995\n\n10995: internal: switch from trait-based TokenSource to simple struct of arrays r=matklad a=matklad\n\ncc #10765 \r\n\r\nThe idea here is to try to simplify the interface as best as we can. The original trait-based approach is a bit over-engineered and hard to debug. Here, we replace callback with just data. The next PR in series will replace the output `TreeSink` trait with data as well. \r\n\r\n\r\nThe biggest drawback here is that we now require to materialize all parser's input up-front. This is a bad fit for macro by example: when you parse `$e:expr`, you might consume only part of the input. However, today's trait-based solution doesn't really help -- we were already materializing the whole thing! So, let's keep it simple!\n\nCo-authored-by: Aleksey Kladov <aleksey.kladov@gmail.com>", "tree": {"sha": "dc2644c96f3cea0e29539a236112211d3b82ee6c", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/dc2644c96f3cea0e29539a236112211d3b82ee6c"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/db2a7087b994e20f264f26ad6db75184282ad120", "comment_count": 0, "verification": {"verified": true, "reason": "valid", "signature": "-----BEGIN PGP SIGNATURE-----\n\nwsBcBAABCAAQBQJhtiWgCRBK7hj4Ov3rIwAA8ZQIABUbqfD3Omupf0lUlxVGxmHS\n9JKdCG3TEIMT5choZu8TTedSH0LYSu7g64kEas/cXsjTtDlpwhbBTb1JpYrGy9VH\n6IpwdvGkGkkEJbn+7FpVSGTWGS/d7g5oOQ8HRGlpyU0ns8IXLQHw9MJN5WQbDT6N\nKgtmUucJ9MqdXS0MRt0CFkeuZW3zwCGvQkDDGBuKgWhxmcRR01BZCTQBNBAH7d07\nVloSuU6qT5Bh8AhRmDfXjuC5YsbbKT+640aYhc6EX1SFD8lvNjsjeJKZyWlvFoRw\nqkb2BVoRND6CldRa8NGP6VRmp101SbhVW1uQCw6drNcfTMJviIj2lPqCCjul4Bg=\n=KSqU\n-----END PGP SIGNATURE-----\n", "payload": "tree dc2644c96f3cea0e29539a236112211d3b82ee6c\nparent fc628cfc8952faa370e650d055c06205e923e7e6\nparent 3b5b988526b9cec74422f46e20ab1b2f9826d39c\nauthor bors[bot] <26634292+bors[bot]@users.noreply.github.com> 1639327136 +0000\ncommitter GitHub <noreply@github.com> 1639327136 +0000\n\nMerge #10995\n\n10995: internal: switch from trait-based TokenSource to simple struct of arrays r=matklad a=matklad\n\ncc #10765 \r\n\r\nThe idea here is to try to simplify the interface as best as we can. The original trait-based approach is a bit over-engineered and hard to debug. Here, we replace callback with just data. The next PR in series will replace the output `TreeSink` trait with data as well. \r\n\r\n\r\nThe biggest drawback here is that we now require to materialize all parser's input up-front. This is a bad fit for macro by example: when you parse `$e:expr`, you might consume only part of the input. However, today's trait-based solution doesn't really help -- we were already materializing the whole thing! So, let's keep it simple!\n\nCo-authored-by: Aleksey Kladov <aleksey.kladov@gmail.com>\n"}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/db2a7087b994e20f264f26ad6db75184282ad120", "html_url": "https://github.com/rust-lang/rust/commit/db2a7087b994e20f264f26ad6db75184282ad120", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/db2a7087b994e20f264f26ad6db75184282ad120/comments", "author": {"login": "bors[bot]", "id": 26634292, "node_id": "MDM6Qm90MjY2MzQyOTI=", "avatar_url": "https://avatars.githubusercontent.com/in/1847?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bors%5Bbot%5D", "html_url": "https://github.com/apps/bors", "followers_url": "https://api.github.com/users/bors%5Bbot%5D/followers", "following_url": "https://api.github.com/users/bors%5Bbot%5D/following{/other_user}", "gists_url": "https://api.github.com/users/bors%5Bbot%5D/gists{/gist_id}", "starred_url": "https://api.github.com/users/bors%5Bbot%5D/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bors%5Bbot%5D/subscriptions", "organizations_url": "https://api.github.com/users/bors%5Bbot%5D/orgs", "repos_url": "https://api.github.com/users/bors%5Bbot%5D/repos", "events_url": "https://api.github.com/users/bors%5Bbot%5D/events{/privacy}", "received_events_url": "https://api.github.com/users/bors%5Bbot%5D/received_events", "type": "Bot", "site_admin": false}, "committer": {"login": "web-flow", "id": 19864447, "node_id": "MDQ6VXNlcjE5ODY0NDQ3", "avatar_url": "https://avatars.githubusercontent.com/u/19864447?v=4", "gravatar_id": "", "url": "https://api.github.com/users/web-flow", "html_url": "https://github.com/web-flow", "followers_url": "https://api.github.com/users/web-flow/followers", "following_url": "https://api.github.com/users/web-flow/following{/other_user}", "gists_url": "https://api.github.com/users/web-flow/gists{/gist_id}", "starred_url": "https://api.github.com/users/web-flow/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/web-flow/subscriptions", "organizations_url": "https://api.github.com/users/web-flow/orgs", "repos_url": "https://api.github.com/users/web-flow/repos", "events_url": "https://api.github.com/users/web-flow/events{/privacy}", "received_events_url": "https://api.github.com/users/web-flow/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "fc628cfc8952faa370e650d055c06205e923e7e6", "url": "https://api.github.com/repos/rust-lang/rust/commits/fc628cfc8952faa370e650d055c06205e923e7e6", "html_url": "https://github.com/rust-lang/rust/commit/fc628cfc8952faa370e650d055c06205e923e7e6"}, {"sha": "3b5b988526b9cec74422f46e20ab1b2f9826d39c", "url": "https://api.github.com/repos/rust-lang/rust/commits/3b5b988526b9cec74422f46e20ab1b2f9826d39c", "html_url": "https://github.com/rust-lang/rust/commit/3b5b988526b9cec74422f46e20ab1b2f9826d39c"}], "stats": {"total": 672, "additions": 310, "deletions": 362}, "files": [{"sha": "1a56878fdb55b93701993dcf7557708f2505ce8e", "filename": "crates/mbe/src/lib.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/db2a7087b994e20f264f26ad6db75184282ad120/crates%2Fmbe%2Fsrc%2Flib.rs", "raw_url": "https://github.com/rust-lang/rust/raw/db2a7087b994e20f264f26ad6db75184282ad120/crates%2Fmbe%2Fsrc%2Flib.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fmbe%2Fsrc%2Flib.rs?ref=db2a7087b994e20f264f26ad6db75184282ad120", "patch": "@@ -10,7 +10,7 @@ mod parser;\n mod expander;\n mod syntax_bridge;\n mod tt_iter;\n-mod subtree_source;\n+mod to_parser_tokens;\n \n #[cfg(test)]\n mod benchmark;"}, {"sha": "6bdd787e301adca61bf26bc0e1f3cc06f77707d2", "filename": "crates/mbe/src/subtree_source.rs", "status": "removed", "additions": 0, "deletions": 174, "changes": 174, "blob_url": "https://github.com/rust-lang/rust/blob/fc628cfc8952faa370e650d055c06205e923e7e6/crates%2Fmbe%2Fsrc%2Fsubtree_source.rs", "raw_url": "https://github.com/rust-lang/rust/raw/fc628cfc8952faa370e650d055c06205e923e7e6/crates%2Fmbe%2Fsrc%2Fsubtree_source.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fmbe%2Fsrc%2Fsubtree_source.rs?ref=fc628cfc8952faa370e650d055c06205e923e7e6", "patch": "@@ -1,174 +0,0 @@\n-//! Our parser is generic over the source of tokens it parses.\n-//!\n-//! This module defines tokens sourced from declarative macros.\n-\n-use parser::{Token, TokenSource};\n-use syntax::{lex_single_syntax_kind, SmolStr, SyntaxKind, SyntaxKind::*, T};\n-use tt::buffer::TokenBuffer;\n-\n-#[derive(Debug, Clone, Eq, PartialEq)]\n-struct TtToken {\n-    tt: Token,\n-    text: SmolStr,\n-}\n-\n-pub(crate) struct SubtreeTokenSource {\n-    cached: Vec<TtToken>,\n-    curr: (Token, usize),\n-}\n-\n-impl<'a> SubtreeTokenSource {\n-    pub(crate) fn new(buffer: &TokenBuffer) -> SubtreeTokenSource {\n-        let mut current = buffer.begin();\n-        let mut cached = Vec::with_capacity(100);\n-\n-        while !current.eof() {\n-            let cursor = current;\n-            let tt = cursor.token_tree();\n-\n-            // Check if it is lifetime\n-            if let Some(tt::buffer::TokenTreeRef::Leaf(tt::Leaf::Punct(punct), _)) = tt {\n-                if punct.char == '\\'' {\n-                    let next = cursor.bump();\n-                    if let Some(tt::buffer::TokenTreeRef::Leaf(tt::Leaf::Ident(ident), _)) =\n-                        next.token_tree()\n-                    {\n-                        let text = SmolStr::new(\"'\".to_string() + &ident.text);\n-                        cached.push(TtToken {\n-                            tt: Token { kind: LIFETIME_IDENT, is_jointed_to_next: false },\n-                            text,\n-                        });\n-                        current = next.bump();\n-                        continue;\n-                    } else {\n-                        panic!(\"Next token must be ident : {:#?}\", next.token_tree());\n-                    }\n-                }\n-            }\n-\n-            current = match tt {\n-                Some(tt::buffer::TokenTreeRef::Leaf(leaf, _)) => {\n-                    cached.push(convert_leaf(leaf));\n-                    cursor.bump()\n-                }\n-                Some(tt::buffer::TokenTreeRef::Subtree(subtree, _)) => {\n-                    if let Some(d) = subtree.delimiter_kind() {\n-                        cached.push(convert_delim(d, false));\n-                    }\n-                    cursor.subtree().unwrap()\n-                }\n-                None => match cursor.end() {\n-                    Some(subtree) => {\n-                        if let Some(d) = subtree.delimiter_kind() {\n-                            cached.push(convert_delim(d, true));\n-                        }\n-                        cursor.bump()\n-                    }\n-                    None => continue,\n-                },\n-            };\n-        }\n-\n-        let mut res = SubtreeTokenSource {\n-            curr: (Token { kind: EOF, is_jointed_to_next: false }, 0),\n-            cached,\n-        };\n-        res.curr = (res.token(0), 0);\n-        res\n-    }\n-\n-    fn token(&self, pos: usize) -> Token {\n-        match self.cached.get(pos) {\n-            Some(it) => it.tt,\n-            None => Token { kind: EOF, is_jointed_to_next: false },\n-        }\n-    }\n-}\n-\n-impl<'a> TokenSource for SubtreeTokenSource {\n-    fn current(&self) -> Token {\n-        self.curr.0\n-    }\n-\n-    /// Lookahead n token\n-    fn lookahead_nth(&self, n: usize) -> Token {\n-        self.token(self.curr.1 + n)\n-    }\n-\n-    /// bump cursor to next token\n-    fn bump(&mut self) {\n-        if self.current().kind == EOF {\n-            return;\n-        }\n-        self.curr = (self.token(self.curr.1 + 1), self.curr.1 + 1);\n-    }\n-\n-    /// Is the current token a specified keyword?\n-    fn is_keyword(&self, kw: &str) -> bool {\n-        match self.cached.get(self.curr.1) {\n-            Some(t) => t.text == *kw,\n-            None => false,\n-        }\n-    }\n-}\n-\n-fn convert_delim(d: tt::DelimiterKind, closing: bool) -> TtToken {\n-    let (kinds, texts) = match d {\n-        tt::DelimiterKind::Parenthesis => ([T!['('], T![')']], \"()\"),\n-        tt::DelimiterKind::Brace => ([T!['{'], T!['}']], \"{}\"),\n-        tt::DelimiterKind::Bracket => ([T!['['], T![']']], \"[]\"),\n-    };\n-\n-    let idx = closing as usize;\n-    let kind = kinds[idx];\n-    let text = &texts[idx..texts.len() - (1 - idx)];\n-    TtToken { tt: Token { kind, is_jointed_to_next: false }, text: SmolStr::new(text) }\n-}\n-\n-fn convert_literal(l: &tt::Literal) -> TtToken {\n-    let is_negated = l.text.starts_with('-');\n-    let inner_text = &l.text[if is_negated { 1 } else { 0 }..];\n-\n-    let kind = lex_single_syntax_kind(inner_text)\n-        .map(|(kind, _error)| kind)\n-        .filter(|kind| {\n-            kind.is_literal() && (!is_negated || matches!(kind, FLOAT_NUMBER | INT_NUMBER))\n-        })\n-        .unwrap_or_else(|| panic!(\"Fail to convert given literal {:#?}\", &l));\n-\n-    TtToken { tt: Token { kind, is_jointed_to_next: false }, text: l.text.clone() }\n-}\n-\n-fn convert_ident(ident: &tt::Ident) -> TtToken {\n-    let kind = match ident.text.as_ref() {\n-        \"true\" => T![true],\n-        \"false\" => T![false],\n-        \"_\" => UNDERSCORE,\n-        i if i.starts_with('\\'') => LIFETIME_IDENT,\n-        _ => SyntaxKind::from_keyword(ident.text.as_str()).unwrap_or(IDENT),\n-    };\n-\n-    TtToken { tt: Token { kind, is_jointed_to_next: false }, text: ident.text.clone() }\n-}\n-\n-fn convert_punct(p: tt::Punct) -> TtToken {\n-    let kind = match SyntaxKind::from_char(p.char) {\n-        None => panic!(\"{:#?} is not a valid punct\", p),\n-        Some(kind) => kind,\n-    };\n-\n-    let text = {\n-        let mut buf = [0u8; 4];\n-        let s: &str = p.char.encode_utf8(&mut buf);\n-        SmolStr::new(s)\n-    };\n-    TtToken { tt: Token { kind, is_jointed_to_next: p.spacing == tt::Spacing::Joint }, text }\n-}\n-\n-fn convert_leaf(leaf: &tt::Leaf) -> TtToken {\n-    match leaf {\n-        tt::Leaf::Literal(l) => convert_literal(l),\n-        tt::Leaf::Ident(ident) => convert_ident(ident),\n-        tt::Leaf::Punct(punct) => convert_punct(*punct),\n-    }\n-}"}, {"sha": "28a23f6be2c5978066e1c6c39512562761f4aa44", "filename": "crates/mbe/src/syntax_bridge.rs", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "blob_url": "https://github.com/rust-lang/rust/blob/db2a7087b994e20f264f26ad6db75184282ad120/crates%2Fmbe%2Fsrc%2Fsyntax_bridge.rs", "raw_url": "https://github.com/rust-lang/rust/raw/db2a7087b994e20f264f26ad6db75184282ad120/crates%2Fmbe%2Fsrc%2Fsyntax_bridge.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fmbe%2Fsrc%2Fsyntax_bridge.rs?ref=db2a7087b994e20f264f26ad6db75184282ad120", "patch": "@@ -12,7 +12,7 @@ use syntax::{\n use tt::buffer::{Cursor, TokenBuffer};\n \n use crate::{\n-    subtree_source::SubtreeTokenSource, tt_iter::TtIter, ExpandError, ParserEntryPoint, TokenMap,\n+    to_parser_tokens::to_parser_tokens, tt_iter::TtIter, ExpandError, ParserEntryPoint, TokenMap,\n };\n \n /// Convert the syntax node to a `TokenTree` (what macro\n@@ -56,9 +56,9 @@ pub fn token_tree_to_syntax_node(\n         }\n         _ => TokenBuffer::from_subtree(tt),\n     };\n-    let mut token_source = SubtreeTokenSource::new(&buffer);\n+    let parser_tokens = to_parser_tokens(&buffer);\n     let mut tree_sink = TtTreeSink::new(buffer.begin());\n-    parser::parse(&mut token_source, &mut tree_sink, entry_point);\n+    parser::parse(&parser_tokens, &mut tree_sink, entry_point);\n     if tree_sink.roots.len() != 1 {\n         return Err(ExpandError::ConversionError);\n     }"}, {"sha": "644689f432a2adb8e4da8a5ea1c90be2c7bc4275", "filename": "crates/mbe/src/to_parser_tokens.rs", "status": "added", "additions": 99, "deletions": 0, "changes": 99, "blob_url": "https://github.com/rust-lang/rust/blob/db2a7087b994e20f264f26ad6db75184282ad120/crates%2Fmbe%2Fsrc%2Fto_parser_tokens.rs", "raw_url": "https://github.com/rust-lang/rust/raw/db2a7087b994e20f264f26ad6db75184282ad120/crates%2Fmbe%2Fsrc%2Fto_parser_tokens.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fmbe%2Fsrc%2Fto_parser_tokens.rs?ref=db2a7087b994e20f264f26ad6db75184282ad120", "patch": "@@ -0,0 +1,99 @@\n+//! Convert macro-by-example tokens which are specific to macro expansion into a\n+//! format that works for our parser.\n+\n+use syntax::{lex_single_syntax_kind, SyntaxKind, SyntaxKind::*, T};\n+use tt::buffer::TokenBuffer;\n+\n+pub(crate) fn to_parser_tokens(buffer: &TokenBuffer) -> parser::Tokens {\n+    let mut res = parser::Tokens::default();\n+\n+    let mut current = buffer.begin();\n+\n+    while !current.eof() {\n+        let cursor = current;\n+        let tt = cursor.token_tree();\n+\n+        // Check if it is lifetime\n+        if let Some(tt::buffer::TokenTreeRef::Leaf(tt::Leaf::Punct(punct), _)) = tt {\n+            if punct.char == '\\'' {\n+                let next = cursor.bump();\n+                match next.token_tree() {\n+                    Some(tt::buffer::TokenTreeRef::Leaf(tt::Leaf::Ident(_ident), _)) => {\n+                        res.push(LIFETIME_IDENT);\n+                        current = next.bump();\n+                        continue;\n+                    }\n+                    _ => panic!(\"Next token must be ident : {:#?}\", next.token_tree()),\n+                }\n+            }\n+        }\n+\n+        current = match tt {\n+            Some(tt::buffer::TokenTreeRef::Leaf(leaf, _)) => {\n+                match leaf {\n+                    tt::Leaf::Literal(lit) => {\n+                        let is_negated = lit.text.starts_with('-');\n+                        let inner_text = &lit.text[if is_negated { 1 } else { 0 }..];\n+\n+                        let kind = lex_single_syntax_kind(inner_text)\n+                            .map(|(kind, _error)| kind)\n+                            .filter(|kind| {\n+                                kind.is_literal()\n+                                    && (!is_negated || matches!(kind, FLOAT_NUMBER | INT_NUMBER))\n+                            })\n+                            .unwrap_or_else(|| panic!(\"Fail to convert given literal {:#?}\", &lit));\n+\n+                        res.push(kind);\n+                    }\n+                    tt::Leaf::Ident(ident) => match ident.text.as_ref() {\n+                        \"_\" => res.push(T![_]),\n+                        i if i.starts_with('\\'') => res.push(LIFETIME_IDENT),\n+                        _ => match SyntaxKind::from_keyword(&ident.text) {\n+                            Some(kind) => res.push(kind),\n+                            None => {\n+                                let contextual_keyword =\n+                                    SyntaxKind::from_contextual_keyword(&ident.text)\n+                                        .unwrap_or(SyntaxKind::IDENT);\n+                                res.push_ident(contextual_keyword);\n+                            }\n+                        },\n+                    },\n+                    tt::Leaf::Punct(punct) => {\n+                        let kind = SyntaxKind::from_char(punct.char)\n+                            .unwrap_or_else(|| panic!(\"{:#?} is not a valid punct\", punct));\n+                        res.push(kind);\n+                        if punct.spacing == tt::Spacing::Joint {\n+                            res.was_joint();\n+                        }\n+                    }\n+                }\n+                cursor.bump()\n+            }\n+            Some(tt::buffer::TokenTreeRef::Subtree(subtree, _)) => {\n+                if let Some(d) = subtree.delimiter_kind() {\n+                    res.push(match d {\n+                        tt::DelimiterKind::Parenthesis => T!['('],\n+                        tt::DelimiterKind::Brace => T!['{'],\n+                        tt::DelimiterKind::Bracket => T!['['],\n+                    });\n+                }\n+                cursor.subtree().unwrap()\n+            }\n+            None => match cursor.end() {\n+                Some(subtree) => {\n+                    if let Some(d) = subtree.delimiter_kind() {\n+                        res.push(match d {\n+                            tt::DelimiterKind::Parenthesis => T![')'],\n+                            tt::DelimiterKind::Brace => T!['}'],\n+                            tt::DelimiterKind::Bracket => T![']'],\n+                        })\n+                    }\n+                    cursor.bump()\n+                }\n+                None => continue,\n+            },\n+        };\n+    }\n+\n+    res\n+}"}, {"sha": "d05e84b0f027b9918304ea0734958a85746eb0ef", "filename": "crates/mbe/src/tt_iter.rs", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "blob_url": "https://github.com/rust-lang/rust/blob/db2a7087b994e20f264f26ad6db75184282ad120/crates%2Fmbe%2Fsrc%2Ftt_iter.rs", "raw_url": "https://github.com/rust-lang/rust/raw/db2a7087b994e20f264f26ad6db75184282ad120/crates%2Fmbe%2Fsrc%2Ftt_iter.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fmbe%2Fsrc%2Ftt_iter.rs?ref=db2a7087b994e20f264f26ad6db75184282ad120", "patch": "@@ -1,7 +1,7 @@\n //! A \"Parser\" structure for token trees. We use this when parsing a declarative\n //! macro definition into a list of patterns and templates.\n \n-use crate::{subtree_source::SubtreeTokenSource, ExpandError, ExpandResult, ParserEntryPoint};\n+use crate::{to_parser_tokens::to_parser_tokens, ExpandError, ExpandResult, ParserEntryPoint};\n \n use parser::TreeSink;\n use syntax::SyntaxKind;\n@@ -116,10 +116,10 @@ impl<'a> TtIter<'a> {\n         }\n \n         let buffer = TokenBuffer::from_tokens(self.inner.as_slice());\n-        let mut src = SubtreeTokenSource::new(&buffer);\n+        let parser_tokens = to_parser_tokens(&buffer);\n         let mut sink = OffsetTokenSink { cursor: buffer.begin(), error: false };\n \n-        parser::parse(&mut src, &mut sink, entry_point);\n+        parser::parse(&parser_tokens, &mut sink, entry_point);\n \n         let mut err = if !sink.cursor.is_root() || sink.error {\n             Some(err!(\"expected {:?}\", entry_point))"}, {"sha": "4b9c579a0521230ed9c4314f4133fa8ed68a0850", "filename": "crates/parser/src/grammar/expressions.rs", "status": "modified", "additions": 1, "deletions": 4, "changes": 5, "blob_url": "https://github.com/rust-lang/rust/blob/db2a7087b994e20f264f26ad6db75184282ad120/crates%2Fparser%2Fsrc%2Fgrammar%2Fexpressions.rs", "raw_url": "https://github.com/rust-lang/rust/raw/db2a7087b994e20f264f26ad6db75184282ad120/crates%2Fparser%2Fsrc%2Fgrammar%2Fexpressions.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fparser%2Fsrc%2Fgrammar%2Fexpressions.rs?ref=db2a7087b994e20f264f26ad6db75184282ad120", "patch": "@@ -296,10 +296,7 @@ fn lhs(p: &mut Parser, r: Restrictions) -> Option<(CompletedMarker, BlockLike)>\n         T![&] => {\n             m = p.start();\n             p.bump(T![&]);\n-            if p.at(IDENT)\n-                && p.at_contextual_kw(\"raw\")\n-                && (p.nth_at(1, T![mut]) || p.nth_at(1, T![const]))\n-            {\n+            if p.at_contextual_kw(T![raw]) && (p.nth_at(1, T![mut]) || p.nth_at(1, T![const])) {\n                 p.bump_remap(T![raw]);\n                 p.bump_any();\n             } else {"}, {"sha": "896efaf37572dd36d1b175c06156048a729aabe1", "filename": "crates/parser/src/grammar/items.rs", "status": "modified", "additions": 6, "deletions": 6, "changes": 12, "blob_url": "https://github.com/rust-lang/rust/blob/db2a7087b994e20f264f26ad6db75184282ad120/crates%2Fparser%2Fsrc%2Fgrammar%2Fitems.rs", "raw_url": "https://github.com/rust-lang/rust/raw/db2a7087b994e20f264f26ad6db75184282ad120/crates%2Fparser%2Fsrc%2Fgrammar%2Fitems.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fparser%2Fsrc%2Fgrammar%2Fitems.rs?ref=db2a7087b994e20f264f26ad6db75184282ad120", "patch": "@@ -122,14 +122,14 @@ pub(super) fn opt_item(p: &mut Parser, m: Marker) -> Result<(), Marker> {\n         has_mods = true;\n         abi(p);\n     }\n-    if p.at(IDENT) && p.at_contextual_kw(\"auto\") && p.nth(1) == T![trait] {\n+    if p.at_contextual_kw(T![auto]) && p.nth(1) == T![trait] {\n         p.bump_remap(T![auto]);\n         has_mods = true;\n     }\n \n     // test default_item\n     // default impl T for Foo {}\n-    if p.at(IDENT) && p.at_contextual_kw(\"default\") {\n+    if p.at_contextual_kw(T![default]) {\n         match p.nth(1) {\n             T![fn] | T![type] | T![const] | T![impl] => {\n                 p.bump_remap(T![default]);\n@@ -176,7 +176,7 @@ pub(super) fn opt_item(p: &mut Parser, m: Marker) -> Result<(), Marker> {\n \n     // test existential_type\n     // existential type Foo: Fn() -> usize;\n-    if p.at(IDENT) && p.at_contextual_kw(\"existential\") && p.nth(1) == T![type] {\n+    if p.at_contextual_kw(T![existential]) && p.nth(1) == T![type] {\n         p.bump_remap(T![existential]);\n         has_mods = true;\n     }\n@@ -224,10 +224,10 @@ fn opt_item_without_modifiers(p: &mut Parser, m: Marker) -> Result<(), Marker> {\n         T![type] => type_alias(p, m),\n         T![struct] => adt::strukt(p, m),\n         T![enum] => adt::enum_(p, m),\n-        IDENT if p.at_contextual_kw(\"union\") && p.nth(1) == IDENT => adt::union(p, m),\n+        IDENT if p.at_contextual_kw(T![union]) && p.nth(1) == IDENT => adt::union(p, m),\n \n         T![macro] => macro_def(p, m),\n-        IDENT if p.at_contextual_kw(\"macro_rules\") && p.nth(1) == BANG => macro_rules(p, m),\n+        IDENT if p.at_contextual_kw(T![macro_rules]) && p.nth(1) == BANG => macro_rules(p, m),\n \n         T![const] if (la == IDENT || la == T![_] || la == T![mut]) => consts::konst(p, m),\n         T![static] => consts::static_(p, m),\n@@ -319,7 +319,7 @@ pub(crate) fn extern_item_list(p: &mut Parser) {\n }\n \n fn macro_rules(p: &mut Parser, m: Marker) {\n-    assert!(p.at_contextual_kw(\"macro_rules\"));\n+    assert!(p.at_contextual_kw(T![macro_rules]));\n     p.bump_remap(T![macro_rules]);\n     p.expect(T![!]);\n "}, {"sha": "83b7ff05786693085344866925c7cd7fe51464ac", "filename": "crates/parser/src/grammar/items/adt.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/db2a7087b994e20f264f26ad6db75184282ad120/crates%2Fparser%2Fsrc%2Fgrammar%2Fitems%2Fadt.rs", "raw_url": "https://github.com/rust-lang/rust/raw/db2a7087b994e20f264f26ad6db75184282ad120/crates%2Fparser%2Fsrc%2Fgrammar%2Fitems%2Fadt.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fparser%2Fsrc%2Fgrammar%2Fitems%2Fadt.rs?ref=db2a7087b994e20f264f26ad6db75184282ad120", "patch": "@@ -10,7 +10,7 @@ pub(super) fn strukt(p: &mut Parser, m: Marker) {\n // test union_item\n // struct U { i: i32, f: f32 }\n pub(super) fn union(p: &mut Parser, m: Marker) {\n-    assert!(p.at_contextual_kw(\"union\"));\n+    assert!(p.at_contextual_kw(T![union]));\n     p.bump_remap(T![union]);\n     struct_or_union(p, m, false);\n }"}, {"sha": "2e2d96d02759fae99a5ebb9549b422ff689338bc", "filename": "crates/parser/src/lib.rs", "status": "modified", "additions": 13, "deletions": 39, "changes": 52, "blob_url": "https://github.com/rust-lang/rust/blob/db2a7087b994e20f264f26ad6db75184282ad120/crates%2Fparser%2Fsrc%2Flib.rs", "raw_url": "https://github.com/rust-lang/rust/raw/db2a7087b994e20f264f26ad6db75184282ad120/crates%2Fparser%2Fsrc%2Flib.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fparser%2Fsrc%2Flib.rs?ref=db2a7087b994e20f264f26ad6db75184282ad120", "patch": "@@ -1,8 +1,11 @@\n //! The Rust parser.\n //!\n+//! NOTE: The crate is undergoing refactors, don't believe everything the docs\n+//! say :-)\n+//!\n //! The parser doesn't know about concrete representation of tokens and syntax\n-//! trees. Abstract [`TokenSource`] and [`TreeSink`] traits are used instead.\n-//! As a consequence, this crate does not contain a lexer.\n+//! trees. Abstract [`TokenSource`] and [`TreeSink`] traits are used instead. As\n+//! a consequence, this crate does not contain a lexer.\n //!\n //! The [`Parser`] struct from the [`parser`] module is a cursor into the\n //! sequence of tokens.  Parsing routines use [`Parser`] to inspect current\n@@ -20,40 +23,15 @@ mod syntax_kind;\n mod event;\n mod parser;\n mod grammar;\n+mod tokens;\n \n pub(crate) use token_set::TokenSet;\n \n-pub use syntax_kind::SyntaxKind;\n+pub use crate::{syntax_kind::SyntaxKind, tokens::Tokens};\n \n #[derive(Debug, Clone, PartialEq, Eq, Hash)]\n pub struct ParseError(pub Box<String>);\n \n-/// `TokenSource` abstracts the source of the tokens parser operates on.\n-///\n-/// Hopefully this will allow us to treat text and token trees in the same way!\n-pub trait TokenSource {\n-    fn current(&self) -> Token;\n-\n-    /// Lookahead n token\n-    fn lookahead_nth(&self, n: usize) -> Token;\n-\n-    /// bump cursor to next token\n-    fn bump(&mut self);\n-\n-    /// Is the current token a specified keyword?\n-    fn is_keyword(&self, kw: &str) -> bool;\n-}\n-\n-/// `Token` abstracts the cursor of `TokenSource` operates on.\n-#[derive(Debug, Copy, Clone, Eq, PartialEq)]\n-pub struct Token {\n-    /// What is the current token?\n-    pub kind: SyntaxKind,\n-\n-    /// Is the current token joined to the next one (`> >` vs `>>`).\n-    pub is_jointed_to_next: bool,\n-}\n-\n /// `TreeSink` abstracts details of a particular syntax tree implementation.\n pub trait TreeSink {\n     /// Adds new token to the current branch.\n@@ -92,15 +70,11 @@ pub enum ParserEntryPoint {\n }\n \n /// Parse given tokens into the given sink as a rust file.\n-pub fn parse_source_file(token_source: &mut dyn TokenSource, tree_sink: &mut dyn TreeSink) {\n-    parse(token_source, tree_sink, ParserEntryPoint::SourceFile);\n+pub fn parse_source_file(tokens: &Tokens, tree_sink: &mut dyn TreeSink) {\n+    parse(tokens, tree_sink, ParserEntryPoint::SourceFile);\n }\n \n-pub fn parse(\n-    token_source: &mut dyn TokenSource,\n-    tree_sink: &mut dyn TreeSink,\n-    entry_point: ParserEntryPoint,\n-) {\n+pub fn parse(tokens: &Tokens, tree_sink: &mut dyn TreeSink, entry_point: ParserEntryPoint) {\n     let entry_point: fn(&'_ mut parser::Parser) = match entry_point {\n         ParserEntryPoint::SourceFile => grammar::entry_points::source_file,\n         ParserEntryPoint::Path => grammar::entry_points::path,\n@@ -118,7 +92,7 @@ pub fn parse(\n         ParserEntryPoint::Attr => grammar::entry_points::attr,\n     };\n \n-    let mut p = parser::Parser::new(token_source);\n+    let mut p = parser::Parser::new(tokens);\n     entry_point(&mut p);\n     let events = p.finish();\n     event::process(tree_sink, events);\n@@ -141,9 +115,9 @@ impl Reparser {\n     ///\n     /// Tokens must start with `{`, end with `}` and form a valid brace\n     /// sequence.\n-    pub fn parse(self, token_source: &mut dyn TokenSource, tree_sink: &mut dyn TreeSink) {\n+    pub fn parse(self, tokens: &Tokens, tree_sink: &mut dyn TreeSink) {\n         let Reparser(r) = self;\n-        let mut p = parser::Parser::new(token_source);\n+        let mut p = parser::Parser::new(tokens);\n         r(&mut p);\n         let events = p.finish();\n         event::process(tree_sink, events);"}, {"sha": "4c891108a60ea0863793e38ff606e8cd9062de6b", "filename": "crates/parser/src/parser.rs", "status": "modified", "additions": 19, "deletions": 28, "changes": 47, "blob_url": "https://github.com/rust-lang/rust/blob/db2a7087b994e20f264f26ad6db75184282ad120/crates%2Fparser%2Fsrc%2Fparser.rs", "raw_url": "https://github.com/rust-lang/rust/raw/db2a7087b994e20f264f26ad6db75184282ad120/crates%2Fparser%2Fsrc%2Fparser.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fparser%2Fsrc%2Fparser.rs?ref=db2a7087b994e20f264f26ad6db75184282ad120", "patch": "@@ -7,9 +7,10 @@ use limit::Limit;\n \n use crate::{\n     event::Event,\n+    tokens::Tokens,\n     ParseError,\n     SyntaxKind::{self, EOF, ERROR, TOMBSTONE},\n-    TokenSet, TokenSource, T,\n+    TokenSet, T,\n };\n \n /// `Parser` struct provides the low-level API for\n@@ -22,16 +23,17 @@ use crate::{\n /// \"start expression, consume number literal,\n /// finish expression\". See `Event` docs for more.\n pub(crate) struct Parser<'t> {\n-    token_source: &'t mut dyn TokenSource,\n+    tokens: &'t Tokens,\n+    pos: usize,\n     events: Vec<Event>,\n     steps: Cell<u32>,\n }\n \n static PARSER_STEP_LIMIT: Limit = Limit::new(15_000_000);\n \n impl<'t> Parser<'t> {\n-    pub(super) fn new(token_source: &'t mut dyn TokenSource) -> Parser<'t> {\n-        Parser { token_source, events: Vec::new(), steps: Cell::new(0) }\n+    pub(super) fn new(tokens: &'t Tokens) -> Parser<'t> {\n+        Parser { tokens, pos: 0, events: Vec::new(), steps: Cell::new(0) }\n     }\n \n     pub(crate) fn finish(self) -> Vec<Event> {\n@@ -54,7 +56,7 @@ impl<'t> Parser<'t> {\n         assert!(PARSER_STEP_LIMIT.check(steps as usize).is_ok(), \"the parser seems stuck\");\n         self.steps.set(steps + 1);\n \n-        self.token_source.lookahead_nth(n).kind\n+        self.tokens.kind(self.pos + n)\n     }\n \n     /// Checks if the current token is `kind`.\n@@ -90,7 +92,7 @@ impl<'t> Parser<'t> {\n             T![<<=] => self.at_composite3(n, T![<], T![<], T![=]),\n             T![>>=] => self.at_composite3(n, T![>], T![>], T![=]),\n \n-            _ => self.token_source.lookahead_nth(n).kind == kind,\n+            _ => self.tokens.kind(self.pos + n) == kind,\n         }\n     }\n \n@@ -129,25 +131,17 @@ impl<'t> Parser<'t> {\n     }\n \n     fn at_composite2(&self, n: usize, k1: SyntaxKind, k2: SyntaxKind) -> bool {\n-        let t1 = self.token_source.lookahead_nth(n);\n-        if t1.kind != k1 || !t1.is_jointed_to_next {\n-            return false;\n-        }\n-        let t2 = self.token_source.lookahead_nth(n + 1);\n-        t2.kind == k2\n+        self.tokens.kind(self.pos + n) == k1\n+            && self.tokens.kind(self.pos + n + 1) == k2\n+            && self.tokens.is_joint(self.pos + n)\n     }\n \n     fn at_composite3(&self, n: usize, k1: SyntaxKind, k2: SyntaxKind, k3: SyntaxKind) -> bool {\n-        let t1 = self.token_source.lookahead_nth(n);\n-        if t1.kind != k1 || !t1.is_jointed_to_next {\n-            return false;\n-        }\n-        let t2 = self.token_source.lookahead_nth(n + 1);\n-        if t2.kind != k2 || !t2.is_jointed_to_next {\n-            return false;\n-        }\n-        let t3 = self.token_source.lookahead_nth(n + 2);\n-        t3.kind == k3\n+        self.tokens.kind(self.pos + n) == k1\n+            && self.tokens.kind(self.pos + n + 1) == k2\n+            && self.tokens.kind(self.pos + n + 2) == k3\n+            && self.tokens.is_joint(self.pos + n)\n+            && self.tokens.is_joint(self.pos + n + 1)\n     }\n \n     /// Checks if the current token is in `kinds`.\n@@ -156,8 +150,8 @@ impl<'t> Parser<'t> {\n     }\n \n     /// Checks if the current token is contextual keyword with text `t`.\n-    pub(crate) fn at_contextual_kw(&self, kw: &str) -> bool {\n-        self.token_source.is_keyword(kw)\n+    pub(crate) fn at_contextual_kw(&self, kw: SyntaxKind) -> bool {\n+        self.tokens.contextual_kind(self.pos) == kw\n     }\n \n     /// Starts a new node in the syntax tree. All nodes and tokens\n@@ -243,10 +237,7 @@ impl<'t> Parser<'t> {\n     }\n \n     fn do_bump(&mut self, kind: SyntaxKind, n_raw_tokens: u8) {\n-        for _ in 0..n_raw_tokens {\n-            self.token_source.bump();\n-        }\n-\n+        self.pos += n_raw_tokens as usize;\n         self.push_event(Event::Token { kind, n_raw_tokens });\n     }\n "}, {"sha": "601a5792afde7f6de675533cc5544c383fd65ea3", "filename": "crates/parser/src/syntax_kind/generated.rs", "status": "modified", "additions": 12, "deletions": 0, "changes": 12, "blob_url": "https://github.com/rust-lang/rust/blob/db2a7087b994e20f264f26ad6db75184282ad120/crates%2Fparser%2Fsrc%2Fsyntax_kind%2Fgenerated.rs", "raw_url": "https://github.com/rust-lang/rust/raw/db2a7087b994e20f264f26ad6db75184282ad120/crates%2Fparser%2Fsrc%2Fsyntax_kind%2Fgenerated.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fparser%2Fsrc%2Fsyntax_kind%2Fgenerated.rs?ref=db2a7087b994e20f264f26ad6db75184282ad120", "patch": "@@ -334,6 +334,18 @@ impl SyntaxKind {\n         };\n         Some(kw)\n     }\n+    pub fn from_contextual_keyword(ident: &str) -> Option<SyntaxKind> {\n+        let kw = match ident {\n+            \"auto\" => AUTO_KW,\n+            \"default\" => DEFAULT_KW,\n+            \"existential\" => EXISTENTIAL_KW,\n+            \"union\" => UNION_KW,\n+            \"raw\" => RAW_KW,\n+            \"macro_rules\" => MACRO_RULES_KW,\n+            _ => return None,\n+        };\n+        Some(kw)\n+    }\n     pub fn from_char(c: char) -> Option<SyntaxKind> {\n         let tok = match c {\n             ';' => SEMICOLON,"}, {"sha": "1c0672492dab80c705b710cf502e2e1990a28f6a", "filename": "crates/parser/src/tokens.rs", "status": "added", "additions": 87, "deletions": 0, "changes": 87, "blob_url": "https://github.com/rust-lang/rust/blob/db2a7087b994e20f264f26ad6db75184282ad120/crates%2Fparser%2Fsrc%2Ftokens.rs", "raw_url": "https://github.com/rust-lang/rust/raw/db2a7087b994e20f264f26ad6db75184282ad120/crates%2Fparser%2Fsrc%2Ftokens.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fparser%2Fsrc%2Ftokens.rs?ref=db2a7087b994e20f264f26ad6db75184282ad120", "patch": "@@ -0,0 +1,87 @@\n+//! Input for the parser -- a sequence of tokens.\n+//!\n+//! As of now, parser doesn't have access to the *text* of the tokens, and makes\n+//! decisions based solely on their classification.\n+\n+use crate::SyntaxKind;\n+\n+#[allow(non_camel_case_types)]\n+type bits = u64;\n+\n+/// Main input to the parser.\n+///\n+/// A sequence of tokens represented internally as a struct of arrays.\n+#[derive(Default)]\n+pub struct Tokens {\n+    kind: Vec<SyntaxKind>,\n+    joint: Vec<bits>,\n+    contextual_kind: Vec<SyntaxKind>,\n+}\n+\n+/// `pub` impl used by callers to create `Tokens`.\n+impl Tokens {\n+    #[inline]\n+    pub fn push(&mut self, kind: SyntaxKind) {\n+        self.push_impl(kind, SyntaxKind::EOF)\n+    }\n+    #[inline]\n+    pub fn push_ident(&mut self, contextual_kind: SyntaxKind) {\n+        self.push_impl(SyntaxKind::IDENT, contextual_kind)\n+    }\n+    /// Sets jointness for the last token we've pushed.\n+    ///\n+    /// This is a separate API rather than an argument to the `push` to make it\n+    /// convenient both for textual and mbe tokens. With text, you know whether\n+    /// the *previous* token was joint, with mbe, you know whether the *current*\n+    /// one is joint. This API allows for styles of usage:\n+    ///\n+    /// ```\n+    /// // In text:\n+    /// tokens.was_joint(prev_joint);\n+    /// tokens.push(curr);\n+    ///\n+    /// // In MBE:\n+    /// token.push(curr);\n+    /// tokens.push(curr_joint)\n+    /// ```\n+    #[inline]\n+    pub fn was_joint(&mut self) {\n+        let n = self.len() - 1;\n+        let (idx, b_idx) = self.bit_index(n);\n+        self.joint[idx] |= 1 << b_idx;\n+    }\n+    #[inline]\n+    fn push_impl(&mut self, kind: SyntaxKind, contextual_kind: SyntaxKind) {\n+        let idx = self.len();\n+        if idx % (bits::BITS as usize) == 0 {\n+            self.joint.push(0);\n+        }\n+        self.kind.push(kind);\n+        self.contextual_kind.push(contextual_kind);\n+    }\n+}\n+\n+/// pub(crate) impl used by the parser to consume `Tokens`.\n+impl Tokens {\n+    pub(crate) fn kind(&self, idx: usize) -> SyntaxKind {\n+        self.kind.get(idx).copied().unwrap_or(SyntaxKind::EOF)\n+    }\n+    pub(crate) fn contextual_kind(&self, idx: usize) -> SyntaxKind {\n+        self.contextual_kind.get(idx).copied().unwrap_or(SyntaxKind::EOF)\n+    }\n+    pub(crate) fn is_joint(&self, n: usize) -> bool {\n+        let (idx, b_idx) = self.bit_index(n);\n+        self.joint[idx] & 1 << b_idx != 0\n+    }\n+}\n+\n+impl Tokens {\n+    fn bit_index(&self, n: usize) -> (usize, usize) {\n+        let idx = n / (bits::BITS as usize);\n+        let b_idx = n % (bits::BITS as usize);\n+        (idx, b_idx)\n+    }\n+    fn len(&self) -> usize {\n+        self.kind.len()\n+    }\n+}"}, {"sha": "865e146482caf528553b21f298a174744d5620fa", "filename": "crates/syntax/src/parsing.rs", "status": "modified", "additions": 37, "deletions": 13, "changes": 50, "blob_url": "https://github.com/rust-lang/rust/blob/db2a7087b994e20f264f26ad6db75184282ad120/crates%2Fsyntax%2Fsrc%2Fparsing.rs", "raw_url": "https://github.com/rust-lang/rust/raw/db2a7087b994e20f264f26ad6db75184282ad120/crates%2Fsyntax%2Fsrc%2Fparsing.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fsyntax%2Fsrc%2Fparsing.rs?ref=db2a7087b994e20f264f26ad6db75184282ad120", "patch": "@@ -2,25 +2,23 @@\n //! incremental reparsing.\n \n pub(crate) mod lexer;\n-mod text_token_source;\n mod text_tree_sink;\n mod reparsing;\n \n use parser::SyntaxKind;\n-use text_token_source::TextTokenSource;\n use text_tree_sink::TextTreeSink;\n \n use crate::{syntax_node::GreenNode, AstNode, SyntaxError, SyntaxNode};\n \n pub(crate) use crate::parsing::{lexer::*, reparsing::incremental_reparse};\n \n pub(crate) fn parse_text(text: &str) -> (GreenNode, Vec<SyntaxError>) {\n-    let (tokens, lexer_errors) = tokenize(text);\n+    let (lexer_tokens, lexer_errors) = tokenize(text);\n+    let parser_tokens = to_parser_tokens(text, &lexer_tokens);\n \n-    let mut token_source = TextTokenSource::new(text, &tokens);\n-    let mut tree_sink = TextTreeSink::new(text, &tokens);\n+    let mut tree_sink = TextTreeSink::new(text, &lexer_tokens);\n \n-    parser::parse_source_file(&mut token_source, &mut tree_sink);\n+    parser::parse_source_file(&parser_tokens, &mut tree_sink);\n \n     let (tree, mut parser_errors) = tree_sink.finish();\n     parser_errors.extend(lexer_errors);\n@@ -33,26 +31,52 @@ pub(crate) fn parse_text_as<T: AstNode>(\n     text: &str,\n     entry_point: parser::ParserEntryPoint,\n ) -> Result<T, ()> {\n-    let (tokens, lexer_errors) = tokenize(text);\n+    let (lexer_tokens, lexer_errors) = tokenize(text);\n     if !lexer_errors.is_empty() {\n         return Err(());\n     }\n \n-    let mut token_source = TextTokenSource::new(text, &tokens);\n-    let mut tree_sink = TextTreeSink::new(text, &tokens);\n+    let parser_tokens = to_parser_tokens(text, &lexer_tokens);\n+\n+    let mut tree_sink = TextTreeSink::new(text, &lexer_tokens);\n \n     // TextTreeSink assumes that there's at least some root node to which it can attach errors and\n     // tokens. We arbitrarily give it a SourceFile.\n     use parser::TreeSink;\n     tree_sink.start_node(SyntaxKind::SOURCE_FILE);\n-    parser::parse(&mut token_source, &mut tree_sink, entry_point);\n+    parser::parse(&parser_tokens, &mut tree_sink, entry_point);\n     tree_sink.finish_node();\n \n-    let (tree, parser_errors) = tree_sink.finish();\n-    use parser::TokenSource;\n-    if !parser_errors.is_empty() || token_source.current().kind != SyntaxKind::EOF {\n+    let (tree, parser_errors, eof) = tree_sink.finish_eof();\n+    if !parser_errors.is_empty() || !eof {\n         return Err(());\n     }\n \n     SyntaxNode::new_root(tree).first_child().and_then(T::cast).ok_or(())\n }\n+\n+pub(crate) fn to_parser_tokens(text: &str, lexer_tokens: &[lexer::Token]) -> ::parser::Tokens {\n+    let mut off = 0;\n+    let mut res = parser::Tokens::default();\n+    let mut was_joint = false;\n+    for t in lexer_tokens {\n+        if t.kind.is_trivia() {\n+            was_joint = false;\n+        } else {\n+            if t.kind == SyntaxKind::IDENT {\n+                let token_text = &text[off..][..usize::from(t.len)];\n+                let contextual_kw =\n+                    SyntaxKind::from_contextual_keyword(token_text).unwrap_or(SyntaxKind::IDENT);\n+                res.push_ident(contextual_kw);\n+            } else {\n+                if was_joint {\n+                    res.was_joint();\n+                }\n+                res.push(t.kind);\n+            }\n+            was_joint = true;\n+        }\n+        off += usize::from(t.len);\n+    }\n+    res\n+}"}, {"sha": "62f39a934724287f398ca222df463e80c95861a3", "filename": "crates/syntax/src/parsing/reparsing.rs", "status": "modified", "additions": 6, "deletions": 6, "changes": 12, "blob_url": "https://github.com/rust-lang/rust/blob/db2a7087b994e20f264f26ad6db75184282ad120/crates%2Fsyntax%2Fsrc%2Fparsing%2Freparsing.rs", "raw_url": "https://github.com/rust-lang/rust/raw/db2a7087b994e20f264f26ad6db75184282ad120/crates%2Fsyntax%2Fsrc%2Fparsing%2Freparsing.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fsyntax%2Fsrc%2Fparsing%2Freparsing.rs?ref=db2a7087b994e20f264f26ad6db75184282ad120", "patch": "@@ -12,8 +12,8 @@ use text_edit::Indel;\n use crate::{\n     parsing::{\n         lexer::{lex_single_syntax_kind, tokenize, Token},\n-        text_token_source::TextTokenSource,\n         text_tree_sink::TextTreeSink,\n+        to_parser_tokens,\n     },\n     syntax_node::{GreenNode, GreenToken, NodeOrToken, SyntaxElement, SyntaxNode},\n     SyntaxError,\n@@ -91,14 +91,14 @@ fn reparse_block(\n     let (node, reparser) = find_reparsable_node(root, edit.delete)?;\n     let text = get_text_after_edit(node.clone().into(), edit);\n \n-    let (tokens, new_lexer_errors) = tokenize(&text);\n-    if !is_balanced(&tokens) {\n+    let (lexer_tokens, new_lexer_errors) = tokenize(&text);\n+    if !is_balanced(&lexer_tokens) {\n         return None;\n     }\n+    let parser_tokens = to_parser_tokens(&text, &lexer_tokens);\n \n-    let mut token_source = TextTokenSource::new(&text, &tokens);\n-    let mut tree_sink = TextTreeSink::new(&text, &tokens);\n-    reparser.parse(&mut token_source, &mut tree_sink);\n+    let mut tree_sink = TextTreeSink::new(&text, &lexer_tokens);\n+    reparser.parse(&parser_tokens, &mut tree_sink);\n \n     let (green, mut new_parser_errors) = tree_sink.finish();\n     new_parser_errors.extend(new_lexer_errors);"}, {"sha": "11dfc63a65bb16f89e5d5e7d66ac38940dfd60d9", "filename": "crates/syntax/src/parsing/text_token_source.rs", "status": "removed", "additions": 0, "deletions": 82, "changes": 82, "blob_url": "https://github.com/rust-lang/rust/blob/fc628cfc8952faa370e650d055c06205e923e7e6/crates%2Fsyntax%2Fsrc%2Fparsing%2Ftext_token_source.rs", "raw_url": "https://github.com/rust-lang/rust/raw/fc628cfc8952faa370e650d055c06205e923e7e6/crates%2Fsyntax%2Fsrc%2Fparsing%2Ftext_token_source.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fsyntax%2Fsrc%2Fparsing%2Ftext_token_source.rs?ref=fc628cfc8952faa370e650d055c06205e923e7e6", "patch": "@@ -1,82 +0,0 @@\n-//! See `TextTokenSource` docs.\n-\n-use parser::TokenSource;\n-\n-use crate::{parsing::lexer::Token, SyntaxKind::EOF, TextRange, TextSize};\n-\n-/// Implementation of `parser::TokenSource` that takes tokens from source code text.\n-pub(crate) struct TextTokenSource<'t> {\n-    text: &'t str,\n-    /// token and its start position (non-whitespace/comment tokens)\n-    /// ```non-rust\n-    ///  struct Foo;\n-    ///  ^------^--^-\n-    ///  |      |    \\________\n-    ///  |      \\____         \\\n-    ///  |           \\         |\n-    ///  (struct, 0) (Foo, 7) (;, 10)\n-    /// ```\n-    /// `[(struct, 0), (Foo, 7), (;, 10)]`\n-    token_offset_pairs: Vec<(Token, TextSize)>,\n-\n-    /// Current token and position\n-    curr: (parser::Token, usize),\n-}\n-\n-impl<'t> TokenSource for TextTokenSource<'t> {\n-    fn current(&self) -> parser::Token {\n-        self.curr.0\n-    }\n-\n-    fn lookahead_nth(&self, n: usize) -> parser::Token {\n-        mk_token(self.curr.1 + n, &self.token_offset_pairs)\n-    }\n-\n-    fn bump(&mut self) {\n-        if self.curr.0.kind == EOF {\n-            return;\n-        }\n-\n-        let pos = self.curr.1 + 1;\n-        self.curr = (mk_token(pos, &self.token_offset_pairs), pos);\n-    }\n-\n-    fn is_keyword(&self, kw: &str) -> bool {\n-        self.token_offset_pairs\n-            .get(self.curr.1)\n-            .map_or(false, |(token, offset)| &self.text[TextRange::at(*offset, token.len)] == kw)\n-    }\n-}\n-\n-fn mk_token(pos: usize, token_offset_pairs: &[(Token, TextSize)]) -> parser::Token {\n-    let (kind, is_jointed_to_next) = match token_offset_pairs.get(pos) {\n-        Some((token, offset)) => (\n-            token.kind,\n-            token_offset_pairs\n-                .get(pos + 1)\n-                .map_or(false, |(_, next_offset)| offset + token.len == *next_offset),\n-        ),\n-        None => (EOF, false),\n-    };\n-    parser::Token { kind, is_jointed_to_next }\n-}\n-\n-impl<'t> TextTokenSource<'t> {\n-    /// Generate input from tokens(expect comment and whitespace).\n-    pub(crate) fn new(text: &'t str, raw_tokens: &'t [Token]) -> TextTokenSource<'t> {\n-        let token_offset_pairs: Vec<_> = raw_tokens\n-            .iter()\n-            .filter_map({\n-                let mut len = 0.into();\n-                move |token| {\n-                    let pair = if token.kind.is_trivia() { None } else { Some((*token, len)) };\n-                    len += token.len;\n-                    pair\n-                }\n-            })\n-            .collect();\n-\n-        let first = mk_token(0, &token_offset_pairs);\n-        TextTokenSource { text, token_offset_pairs, curr: (first, 0) }\n-    }\n-}"}, {"sha": "c1792199fdc85f852c999c638810a7c99e0553fc", "filename": "crates/syntax/src/parsing/text_tree_sink.rs", "status": "modified", "additions": 10, "deletions": 2, "changes": 12, "blob_url": "https://github.com/rust-lang/rust/blob/db2a7087b994e20f264f26ad6db75184282ad120/crates%2Fsyntax%2Fsrc%2Fparsing%2Ftext_tree_sink.rs", "raw_url": "https://github.com/rust-lang/rust/raw/db2a7087b994e20f264f26ad6db75184282ad120/crates%2Fsyntax%2Fsrc%2Fparsing%2Ftext_tree_sink.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fsyntax%2Fsrc%2Fparsing%2Ftext_tree_sink.rs?ref=db2a7087b994e20f264f26ad6db75184282ad120", "patch": "@@ -104,7 +104,7 @@ impl<'a> TextTreeSink<'a> {\n         }\n     }\n \n-    pub(super) fn finish(mut self) -> (GreenNode, Vec<SyntaxError>) {\n+    pub(super) fn finish_eof(mut self) -> (GreenNode, Vec<SyntaxError>, bool) {\n         match mem::replace(&mut self.state, State::Normal) {\n             State::PendingFinish => {\n                 self.eat_trivias();\n@@ -113,7 +113,15 @@ impl<'a> TextTreeSink<'a> {\n             State::PendingStart | State::Normal => unreachable!(),\n         }\n \n-        self.inner.finish_raw()\n+        let (node, errors) = self.inner.finish_raw();\n+        let is_eof = self.token_pos == self.tokens.len();\n+\n+        (node, errors, is_eof)\n+    }\n+\n+    pub(super) fn finish(self) -> (GreenNode, Vec<SyntaxError>) {\n+        let (node, errors, _eof) = self.finish_eof();\n+        (node, errors)\n     }\n \n     fn eat_trivias(&mut self) {"}, {"sha": "c66edadc3ce768ccf1a7de07a050182da6bb98b4", "filename": "crates/syntax/src/tests/sourcegen_ast.rs", "status": "modified", "additions": 12, "deletions": 0, "changes": 12, "blob_url": "https://github.com/rust-lang/rust/blob/db2a7087b994e20f264f26ad6db75184282ad120/crates%2Fsyntax%2Fsrc%2Ftests%2Fsourcegen_ast.rs", "raw_url": "https://github.com/rust-lang/rust/raw/db2a7087b994e20f264f26ad6db75184282ad120/crates%2Fsyntax%2Fsrc%2Ftests%2Fsourcegen_ast.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fsyntax%2Fsrc%2Ftests%2Fsourcegen_ast.rs?ref=db2a7087b994e20f264f26ad6db75184282ad120", "patch": "@@ -359,6 +359,10 @@ fn generate_syntax_kinds(grammar: KindsSrc<'_>) -> String {\n     let full_keywords =\n         full_keywords_values.iter().map(|kw| format_ident!(\"{}_KW\", to_upper_snake_case(kw)));\n \n+    let contextual_keywords_values = &grammar.contextual_keywords;\n+    let contextual_keywords =\n+        contextual_keywords_values.iter().map(|kw| format_ident!(\"{}_KW\", to_upper_snake_case(kw)));\n+\n     let all_keywords_values =\n         grammar.keywords.iter().chain(grammar.contextual_keywords.iter()).collect::<Vec<_>>();\n     let all_keywords_idents = all_keywords_values.iter().map(|kw| format_ident!(\"{}\", kw));\n@@ -428,6 +432,14 @@ fn generate_syntax_kinds(grammar: KindsSrc<'_>) -> String {\n                 Some(kw)\n             }\n \n+            pub fn from_contextual_keyword(ident: &str) -> Option<SyntaxKind> {\n+                let kw = match ident {\n+                    #(#contextual_keywords_values => #contextual_keywords,)*\n+                    _ => return None,\n+                };\n+                Some(kw)\n+            }\n+\n             pub fn from_char(c: char) -> Option<SyntaxKind> {\n                 let tok = match c {\n                     #(#single_byte_tokens_values => #single_byte_tokens,)*"}]}