{"sha": "b3bc6bf31265ac10946a0832092dbcedf9b26805", "node_id": "C_kwDOAAsO6NoAKGIzYmM2YmYzMTI2NWFjMTA5NDZhMDgzMjA5MmRiY2VkZjliMjY4MDU", "commit": {"author": {"name": "bors", "email": "bors@rust-lang.org", "date": "2022-11-24T20:29:13Z"}, "committer": {"name": "bors", "email": "bors@rust-lang.org", "date": "2022-11-24T20:29:13Z"}, "message": "Auto merge of #103693 - HKalbasi:master, r=oli-obk\n\nMake rustc_target usable outside of rustc\n\nI'm working on showing type size in rust-analyzer (https://github.com/rust-lang/rust-analyzer/pull/13490) and I currently copied rustc code inside rust-analyzer, which works, but is bad. With this change, I would become able to use `rustc_target` and `rustc_index` directly in r-a, reducing the amount of copy needed.\n\nThis PR contains some feature flag to put nightly features behind them to make crates buildable on the stable compiler + makes layout related types generic over index type + removes interning of nested layouts.", "tree": {"sha": "5d839466b9602bf1ae6eb4e42883b68a2aa05545", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/5d839466b9602bf1ae6eb4e42883b68a2aa05545"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/b3bc6bf31265ac10946a0832092dbcedf9b26805", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/b3bc6bf31265ac10946a0832092dbcedf9b26805", "html_url": "https://github.com/rust-lang/rust/commit/b3bc6bf31265ac10946a0832092dbcedf9b26805", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/b3bc6bf31265ac10946a0832092dbcedf9b26805/comments", "author": {"login": "bors", "id": 3372342, "node_id": "MDQ6VXNlcjMzNzIzNDI=", "avatar_url": "https://avatars.githubusercontent.com/u/3372342?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bors", "html_url": "https://github.com/bors", "followers_url": "https://api.github.com/users/bors/followers", "following_url": "https://api.github.com/users/bors/following{/other_user}", "gists_url": "https://api.github.com/users/bors/gists{/gist_id}", "starred_url": "https://api.github.com/users/bors/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bors/subscriptions", "organizations_url": "https://api.github.com/users/bors/orgs", "repos_url": "https://api.github.com/users/bors/repos", "events_url": "https://api.github.com/users/bors/events{/privacy}", "received_events_url": "https://api.github.com/users/bors/received_events", "type": "User", "site_admin": false}, "committer": {"login": "bors", "id": 3372342, "node_id": "MDQ6VXNlcjMzNzIzNDI=", "avatar_url": "https://avatars.githubusercontent.com/u/3372342?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bors", "html_url": "https://github.com/bors", "followers_url": "https://api.github.com/users/bors/followers", "following_url": "https://api.github.com/users/bors/following{/other_user}", "gists_url": "https://api.github.com/users/bors/gists{/gist_id}", "starred_url": "https://api.github.com/users/bors/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bors/subscriptions", "organizations_url": "https://api.github.com/users/bors/orgs", "repos_url": "https://api.github.com/users/bors/repos", "events_url": "https://api.github.com/users/bors/events{/privacy}", "received_events_url": "https://api.github.com/users/bors/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "5dfb4b0afaf6acace0845d00e85a934fb4289d83", "url": "https://api.github.com/repos/rust-lang/rust/commits/5dfb4b0afaf6acace0845d00e85a934fb4289d83", "html_url": "https://github.com/rust-lang/rust/commit/5dfb4b0afaf6acace0845d00e85a934fb4289d83"}, {"sha": "390a637e296ccfaac4c6abd1291b0523e8a8e00b", "url": "https://api.github.com/repos/rust-lang/rust/commits/390a637e296ccfaac4c6abd1291b0523e8a8e00b", "html_url": "https://github.com/rust-lang/rust/commit/390a637e296ccfaac4c6abd1291b0523e8a8e00b"}], "stats": {"total": 5263, "additions": 2725, "deletions": 2538}, "files": [{"sha": "d8612b3a2561b62989edaf3a72b6f8a580c1e2d7", "filename": "Cargo.lock", "status": "modified", "additions": 16, "deletions": 2, "changes": 18, "blob_url": "https://github.com/rust-lang/rust/blob/b3bc6bf31265ac10946a0832092dbcedf9b26805/Cargo.lock", "raw_url": "https://github.com/rust-lang/rust/raw/b3bc6bf31265ac10946a0832092dbcedf9b26805/Cargo.lock", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/Cargo.lock?ref=b3bc6bf31265ac10946a0832092dbcedf9b26805", "patch": "@@ -3202,6 +3202,20 @@ dependencies = [\n  \"winapi\",\n ]\n \n+[[package]]\n+name = \"rustc_abi\"\n+version = \"0.0.0\"\n+dependencies = [\n+ \"bitflags\",\n+ \"rand 0.8.5\",\n+ \"rand_xoshiro\",\n+ \"rustc_data_structures\",\n+ \"rustc_index\",\n+ \"rustc_macros\",\n+ \"rustc_serialize\",\n+ \"tracing\",\n+]\n+\n [[package]]\n name = \"rustc_apfloat\"\n version = \"0.0.0\"\n@@ -4281,6 +4295,7 @@ name = \"rustc_target\"\n version = \"0.0.0\"\n dependencies = [\n  \"bitflags\",\n+ \"rustc_abi\",\n  \"rustc_data_structures\",\n  \"rustc_feature\",\n  \"rustc_index\",\n@@ -4336,6 +4351,7 @@ dependencies = [\n  \"rustc_infer\",\n  \"rustc_middle\",\n  \"rustc_span\",\n+ \"rustc_target\",\n  \"rustc_trait_selection\",\n  \"smallvec\",\n  \"tracing\",\n@@ -4360,8 +4376,6 @@ dependencies = [\n name = \"rustc_ty_utils\"\n version = \"0.0.0\"\n dependencies = [\n- \"rand 0.8.5\",\n- \"rand_xoshiro\",\n  \"rustc_data_structures\",\n  \"rustc_errors\",\n  \"rustc_hir\","}, {"sha": "48b199cb8eed9dad92c33e20ffb6390b2062992b", "filename": "compiler/rustc_abi/Cargo.toml", "status": "added", "additions": 24, "deletions": 0, "changes": 24, "blob_url": "https://github.com/rust-lang/rust/blob/b3bc6bf31265ac10946a0832092dbcedf9b26805/compiler%2Frustc_abi%2FCargo.toml", "raw_url": "https://github.com/rust-lang/rust/raw/b3bc6bf31265ac10946a0832092dbcedf9b26805/compiler%2Frustc_abi%2FCargo.toml", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_abi%2FCargo.toml?ref=b3bc6bf31265ac10946a0832092dbcedf9b26805", "patch": "@@ -0,0 +1,24 @@\n+[package]\n+name = \"rustc_abi\"\n+version = \"0.0.0\"\n+edition = \"2021\"\n+\n+[dependencies]\n+bitflags = \"1.2.1\"\n+tracing = \"0.1\"\n+rand = { version = \"0.8.4\", default-features = false, optional = true }\n+rand_xoshiro = { version = \"0.6.0\", optional = true }\n+rustc_data_structures = { path = \"../rustc_data_structures\", optional = true  }\n+rustc_index = { path = \"../rustc_index\", default-features = false }\n+rustc_macros = { path = \"../rustc_macros\", optional = true }\n+rustc_serialize = { path = \"../rustc_serialize\", optional = true  }\n+\n+[features]\n+default = [\"nightly\", \"randomize\"]\n+randomize = [\"rand\", \"rand_xoshiro\"]\n+nightly = [\n+    \"rustc_data_structures\",\n+    \"rustc_index/nightly\",\n+    \"rustc_macros\",\n+    \"rustc_serialize\",\n+]"}, {"sha": "39ea7a85be652b74d61ad9ac40f293b508856e5c", "filename": "compiler/rustc_abi/src/layout.rs", "status": "added", "additions": 947, "deletions": 0, "changes": 947, "blob_url": "https://github.com/rust-lang/rust/blob/b3bc6bf31265ac10946a0832092dbcedf9b26805/compiler%2Frustc_abi%2Fsrc%2Flayout.rs", "raw_url": "https://github.com/rust-lang/rust/raw/b3bc6bf31265ac10946a0832092dbcedf9b26805/compiler%2Frustc_abi%2Fsrc%2Flayout.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_abi%2Fsrc%2Flayout.rs?ref=b3bc6bf31265ac10946a0832092dbcedf9b26805", "patch": "@@ -0,0 +1,947 @@\n+use super::*;\n+use std::{\n+    borrow::Borrow,\n+    cmp,\n+    fmt::Debug,\n+    iter,\n+    ops::{Bound, Deref},\n+};\n+\n+#[cfg(feature = \"randomize\")]\n+use rand::{seq::SliceRandom, SeedableRng};\n+#[cfg(feature = \"randomize\")]\n+use rand_xoshiro::Xoshiro128StarStar;\n+\n+use tracing::debug;\n+\n+// Invert a bijective mapping, i.e. `invert(map)[y] = x` if `map[x] = y`.\n+// This is used to go between `memory_index` (source field order to memory order)\n+// and `inverse_memory_index` (memory order to source field order).\n+// See also `FieldsShape::Arbitrary::memory_index` for more details.\n+// FIXME(eddyb) build a better abstraction for permutations, if possible.\n+fn invert_mapping(map: &[u32]) -> Vec<u32> {\n+    let mut inverse = vec![0; map.len()];\n+    for i in 0..map.len() {\n+        inverse[map[i] as usize] = i as u32;\n+    }\n+    inverse\n+}\n+\n+pub trait LayoutCalculator {\n+    type TargetDataLayoutRef: Borrow<TargetDataLayout>;\n+\n+    fn delay_bug(&self, txt: &str);\n+    fn current_data_layout(&self) -> Self::TargetDataLayoutRef;\n+\n+    fn scalar_pair<V: Idx>(&self, a: Scalar, b: Scalar) -> LayoutS<V> {\n+        let dl = self.current_data_layout();\n+        let dl = dl.borrow();\n+        let b_align = b.align(dl);\n+        let align = a.align(dl).max(b_align).max(dl.aggregate_align);\n+        let b_offset = a.size(dl).align_to(b_align.abi);\n+        let size = (b_offset + b.size(dl)).align_to(align.abi);\n+\n+        // HACK(nox): We iter on `b` and then `a` because `max_by_key`\n+        // returns the last maximum.\n+        let largest_niche = Niche::from_scalar(dl, b_offset, b)\n+            .into_iter()\n+            .chain(Niche::from_scalar(dl, Size::ZERO, a))\n+            .max_by_key(|niche| niche.available(dl));\n+\n+        LayoutS {\n+            variants: Variants::Single { index: V::new(0) },\n+            fields: FieldsShape::Arbitrary {\n+                offsets: vec![Size::ZERO, b_offset],\n+                memory_index: vec![0, 1],\n+            },\n+            abi: Abi::ScalarPair(a, b),\n+            largest_niche,\n+            align,\n+            size,\n+        }\n+    }\n+\n+    fn univariant<'a, V: Idx, F: Deref<Target = &'a LayoutS<V>> + Debug>(\n+        &self,\n+        dl: &TargetDataLayout,\n+        fields: &[F],\n+        repr: &ReprOptions,\n+        kind: StructKind,\n+    ) -> Option<LayoutS<V>> {\n+        let pack = repr.pack;\n+        let mut align = if pack.is_some() { dl.i8_align } else { dl.aggregate_align };\n+        let mut inverse_memory_index: Vec<u32> = (0..fields.len() as u32).collect();\n+        let optimize = !repr.inhibit_struct_field_reordering_opt();\n+        if optimize {\n+            let end =\n+                if let StructKind::MaybeUnsized = kind { fields.len() - 1 } else { fields.len() };\n+            let optimizing = &mut inverse_memory_index[..end];\n+            let effective_field_align = |f: &F| {\n+                if let Some(pack) = pack {\n+                    // return the packed alignment in bytes\n+                    f.align.abi.min(pack).bytes()\n+                } else {\n+                    // returns log2(effective-align).\n+                    // This is ok since `pack` applies to all fields equally.\n+                    // The calculation assumes that size is an integer multiple of align, except for ZSTs.\n+                    //\n+                    // group [u8; 4] with align-4 or [u8; 6] with align-2 fields\n+                    f.align.abi.bytes().max(f.size.bytes()).trailing_zeros() as u64\n+                }\n+            };\n+\n+            // If `-Z randomize-layout` was enabled for the type definition we can shuffle\n+            // the field ordering to try and catch some code making assumptions about layouts\n+            // we don't guarantee\n+            if repr.can_randomize_type_layout() && cfg!(feature = \"randomize\") {\n+                #[cfg(feature = \"randomize\")]\n+                {\n+                    // `ReprOptions.layout_seed` is a deterministic seed that we can use to\n+                    // randomize field ordering with\n+                    let mut rng = Xoshiro128StarStar::seed_from_u64(repr.field_shuffle_seed);\n+\n+                    // Shuffle the ordering of the fields\n+                    optimizing.shuffle(&mut rng);\n+                }\n+                // Otherwise we just leave things alone and actually optimize the type's fields\n+            } else {\n+                match kind {\n+                    StructKind::AlwaysSized | StructKind::MaybeUnsized => {\n+                        optimizing.sort_by_key(|&x| {\n+                            // Place ZSTs first to avoid \"interesting offsets\",\n+                            // especially with only one or two non-ZST fields.\n+                            // Then place largest alignments first, largest niches within an alignment group last\n+                            let f = &fields[x as usize];\n+                            let niche_size = f.largest_niche.map_or(0, |n| n.available(dl));\n+                            (!f.is_zst(), cmp::Reverse(effective_field_align(f)), niche_size)\n+                        });\n+                    }\n+\n+                    StructKind::Prefixed(..) => {\n+                        // Sort in ascending alignment so that the layout stays optimal\n+                        // regardless of the prefix.\n+                        // And put the largest niche in an alignment group at the end\n+                        // so it can be used as discriminant in jagged enums\n+                        optimizing.sort_by_key(|&x| {\n+                            let f = &fields[x as usize];\n+                            let niche_size = f.largest_niche.map_or(0, |n| n.available(dl));\n+                            (effective_field_align(f), niche_size)\n+                        });\n+                    }\n+                }\n+\n+                // FIXME(Kixiron): We can always shuffle fields within a given alignment class\n+                //                 regardless of the status of `-Z randomize-layout`\n+            }\n+        }\n+        // inverse_memory_index holds field indices by increasing memory offset.\n+        // That is, if field 5 has offset 0, the first element of inverse_memory_index is 5.\n+        // We now write field offsets to the corresponding offset slot;\n+        // field 5 with offset 0 puts 0 in offsets[5].\n+        // At the bottom of this function, we invert `inverse_memory_index` to\n+        // produce `memory_index` (see `invert_mapping`).\n+        let mut sized = true;\n+        let mut offsets = vec![Size::ZERO; fields.len()];\n+        let mut offset = Size::ZERO;\n+        let mut largest_niche = None;\n+        let mut largest_niche_available = 0;\n+        if let StructKind::Prefixed(prefix_size, prefix_align) = kind {\n+            let prefix_align =\n+                if let Some(pack) = pack { prefix_align.min(pack) } else { prefix_align };\n+            align = align.max(AbiAndPrefAlign::new(prefix_align));\n+            offset = prefix_size.align_to(prefix_align);\n+        }\n+        for &i in &inverse_memory_index {\n+            let field = &fields[i as usize];\n+            if !sized {\n+                self.delay_bug(&format!(\n+                    \"univariant: field #{} comes after unsized field\",\n+                    offsets.len(),\n+                ));\n+            }\n+\n+            if field.is_unsized() {\n+                sized = false;\n+            }\n+\n+            // Invariant: offset < dl.obj_size_bound() <= 1<<61\n+            let field_align = if let Some(pack) = pack {\n+                field.align.min(AbiAndPrefAlign::new(pack))\n+            } else {\n+                field.align\n+            };\n+            offset = offset.align_to(field_align.abi);\n+            align = align.max(field_align);\n+\n+            debug!(\"univariant offset: {:?} field: {:#?}\", offset, field);\n+            offsets[i as usize] = offset;\n+\n+            if let Some(mut niche) = field.largest_niche {\n+                let available = niche.available(dl);\n+                if available > largest_niche_available {\n+                    largest_niche_available = available;\n+                    niche.offset += offset;\n+                    largest_niche = Some(niche);\n+                }\n+            }\n+\n+            offset = offset.checked_add(field.size, dl)?;\n+        }\n+        if let Some(repr_align) = repr.align {\n+            align = align.max(AbiAndPrefAlign::new(repr_align));\n+        }\n+        debug!(\"univariant min_size: {:?}\", offset);\n+        let min_size = offset;\n+        // As stated above, inverse_memory_index holds field indices by increasing offset.\n+        // This makes it an already-sorted view of the offsets vec.\n+        // To invert it, consider:\n+        // If field 5 has offset 0, offsets[0] is 5, and memory_index[5] should be 0.\n+        // Field 5 would be the first element, so memory_index is i:\n+        // Note: if we didn't optimize, it's already right.\n+        let memory_index =\n+            if optimize { invert_mapping(&inverse_memory_index) } else { inverse_memory_index };\n+        let size = min_size.align_to(align.abi);\n+        let mut abi = Abi::Aggregate { sized };\n+        // Unpack newtype ABIs and find scalar pairs.\n+        if sized && size.bytes() > 0 {\n+            // All other fields must be ZSTs.\n+            let mut non_zst_fields = fields.iter().enumerate().filter(|&(_, f)| !f.is_zst());\n+\n+            match (non_zst_fields.next(), non_zst_fields.next(), non_zst_fields.next()) {\n+                // We have exactly one non-ZST field.\n+                (Some((i, field)), None, None) => {\n+                    // Field fills the struct and it has a scalar or scalar pair ABI.\n+                    if offsets[i].bytes() == 0 && align.abi == field.align.abi && size == field.size\n+                    {\n+                        match field.abi {\n+                            // For plain scalars, or vectors of them, we can't unpack\n+                            // newtypes for `#[repr(C)]`, as that affects C ABIs.\n+                            Abi::Scalar(_) | Abi::Vector { .. } if optimize => {\n+                                abi = field.abi;\n+                            }\n+                            // But scalar pairs are Rust-specific and get\n+                            // treated as aggregates by C ABIs anyway.\n+                            Abi::ScalarPair(..) => {\n+                                abi = field.abi;\n+                            }\n+                            _ => {}\n+                        }\n+                    }\n+                }\n+\n+                // Two non-ZST fields, and they're both scalars.\n+                (Some((i, a)), Some((j, b)), None) => {\n+                    match (a.abi, b.abi) {\n+                        (Abi::Scalar(a), Abi::Scalar(b)) => {\n+                            // Order by the memory placement, not source order.\n+                            let ((i, a), (j, b)) = if offsets[i] < offsets[j] {\n+                                ((i, a), (j, b))\n+                            } else {\n+                                ((j, b), (i, a))\n+                            };\n+                            let pair = self.scalar_pair::<V>(a, b);\n+                            let pair_offsets = match pair.fields {\n+                                FieldsShape::Arbitrary { ref offsets, ref memory_index } => {\n+                                    assert_eq!(memory_index, &[0, 1]);\n+                                    offsets\n+                                }\n+                                _ => panic!(),\n+                            };\n+                            if offsets[i] == pair_offsets[0]\n+                                && offsets[j] == pair_offsets[1]\n+                                && align == pair.align\n+                                && size == pair.size\n+                            {\n+                                // We can use `ScalarPair` only when it matches our\n+                                // already computed layout (including `#[repr(C)]`).\n+                                abi = pair.abi;\n+                            }\n+                        }\n+                        _ => {}\n+                    }\n+                }\n+\n+                _ => {}\n+            }\n+        }\n+        if fields.iter().any(|f| f.abi.is_uninhabited()) {\n+            abi = Abi::Uninhabited;\n+        }\n+        Some(LayoutS {\n+            variants: Variants::Single { index: V::new(0) },\n+            fields: FieldsShape::Arbitrary { offsets, memory_index },\n+            abi,\n+            largest_niche,\n+            align,\n+            size,\n+        })\n+    }\n+\n+    fn layout_of_never_type<V: Idx>(&self) -> LayoutS<V> {\n+        let dl = self.current_data_layout();\n+        let dl = dl.borrow();\n+        LayoutS {\n+            variants: Variants::Single { index: V::new(0) },\n+            fields: FieldsShape::Primitive,\n+            abi: Abi::Uninhabited,\n+            largest_niche: None,\n+            align: dl.i8_align,\n+            size: Size::ZERO,\n+        }\n+    }\n+\n+    fn layout_of_struct_or_enum<'a, V: Idx, F: Deref<Target = &'a LayoutS<V>> + Debug>(\n+        &self,\n+        repr: &ReprOptions,\n+        variants: &IndexVec<V, Vec<F>>,\n+        is_enum: bool,\n+        is_unsafe_cell: bool,\n+        scalar_valid_range: (Bound<u128>, Bound<u128>),\n+        discr_range_of_repr: impl Fn(i128, i128) -> (Integer, bool),\n+        discriminants: impl Iterator<Item = (V, i128)>,\n+        niche_optimize_enum: bool,\n+        always_sized: bool,\n+    ) -> Option<LayoutS<V>> {\n+        let dl = self.current_data_layout();\n+        let dl = dl.borrow();\n+\n+        let scalar_unit = |value: Primitive| {\n+            let size = value.size(dl);\n+            assert!(size.bits() <= 128);\n+            Scalar::Initialized { value, valid_range: WrappingRange::full(size) }\n+        };\n+\n+        // A variant is absent if it's uninhabited and only has ZST fields.\n+        // Present uninhabited variants only require space for their fields,\n+        // but *not* an encoding of the discriminant (e.g., a tag value).\n+        // See issue #49298 for more details on the need to leave space\n+        // for non-ZST uninhabited data (mostly partial initialization).\n+        let absent = |fields: &[F]| {\n+            let uninhabited = fields.iter().any(|f| f.abi.is_uninhabited());\n+            let is_zst = fields.iter().all(|f| f.is_zst());\n+            uninhabited && is_zst\n+        };\n+        let (present_first, present_second) = {\n+            let mut present_variants = variants\n+                .iter_enumerated()\n+                .filter_map(|(i, v)| if absent(v) { None } else { Some(i) });\n+            (present_variants.next(), present_variants.next())\n+        };\n+        let present_first = match present_first {\n+            Some(present_first) => present_first,\n+            // Uninhabited because it has no variants, or only absent ones.\n+            None if is_enum => {\n+                return Some(self.layout_of_never_type());\n+            }\n+            // If it's a struct, still compute a layout so that we can still compute the\n+            // field offsets.\n+            None => V::new(0),\n+        };\n+\n+        let is_struct = !is_enum ||\n+                    // Only one variant is present.\n+                    (present_second.is_none() &&\n+                        // Representation optimizations are allowed.\n+                        !repr.inhibit_enum_layout_opt());\n+        if is_struct {\n+            // Struct, or univariant enum equivalent to a struct.\n+            // (Typechecking will reject discriminant-sizing attrs.)\n+\n+            let v = present_first;\n+            let kind = if is_enum || variants[v].is_empty() {\n+                StructKind::AlwaysSized\n+            } else {\n+                if !always_sized { StructKind::MaybeUnsized } else { StructKind::AlwaysSized }\n+            };\n+\n+            let mut st = self.univariant(dl, &variants[v], &repr, kind)?;\n+            st.variants = Variants::Single { index: v };\n+\n+            if is_unsafe_cell {\n+                let hide_niches = |scalar: &mut _| match scalar {\n+                    Scalar::Initialized { value, valid_range } => {\n+                        *valid_range = WrappingRange::full(value.size(dl))\n+                    }\n+                    // Already doesn't have any niches\n+                    Scalar::Union { .. } => {}\n+                };\n+                match &mut st.abi {\n+                    Abi::Uninhabited => {}\n+                    Abi::Scalar(scalar) => hide_niches(scalar),\n+                    Abi::ScalarPair(a, b) => {\n+                        hide_niches(a);\n+                        hide_niches(b);\n+                    }\n+                    Abi::Vector { element, count: _ } => hide_niches(element),\n+                    Abi::Aggregate { sized: _ } => {}\n+                }\n+                st.largest_niche = None;\n+                return Some(st);\n+            }\n+\n+            let (start, end) = scalar_valid_range;\n+            match st.abi {\n+                Abi::Scalar(ref mut scalar) | Abi::ScalarPair(ref mut scalar, _) => {\n+                    // the asserts ensure that we are not using the\n+                    // `#[rustc_layout_scalar_valid_range(n)]`\n+                    // attribute to widen the range of anything as that would probably\n+                    // result in UB somewhere\n+                    // FIXME(eddyb) the asserts are probably not needed,\n+                    // as larger validity ranges would result in missed\n+                    // optimizations, *not* wrongly assuming the inner\n+                    // value is valid. e.g. unions enlarge validity ranges,\n+                    // because the values may be uninitialized.\n+                    if let Bound::Included(start) = start {\n+                        // FIXME(eddyb) this might be incorrect - it doesn't\n+                        // account for wrap-around (end < start) ranges.\n+                        let valid_range = scalar.valid_range_mut();\n+                        assert!(valid_range.start <= start);\n+                        valid_range.start = start;\n+                    }\n+                    if let Bound::Included(end) = end {\n+                        // FIXME(eddyb) this might be incorrect - it doesn't\n+                        // account for wrap-around (end < start) ranges.\n+                        let valid_range = scalar.valid_range_mut();\n+                        assert!(valid_range.end >= end);\n+                        valid_range.end = end;\n+                    }\n+\n+                    // Update `largest_niche` if we have introduced a larger niche.\n+                    let niche = Niche::from_scalar(dl, Size::ZERO, *scalar);\n+                    if let Some(niche) = niche {\n+                        match st.largest_niche {\n+                            Some(largest_niche) => {\n+                                // Replace the existing niche even if they're equal,\n+                                // because this one is at a lower offset.\n+                                if largest_niche.available(dl) <= niche.available(dl) {\n+                                    st.largest_niche = Some(niche);\n+                                }\n+                            }\n+                            None => st.largest_niche = Some(niche),\n+                        }\n+                    }\n+                }\n+                _ => assert!(\n+                    start == Bound::Unbounded && end == Bound::Unbounded,\n+                    \"nonscalar layout for layout_scalar_valid_range type: {:#?}\",\n+                    st,\n+                ),\n+            }\n+\n+            return Some(st);\n+        }\n+\n+        // At this point, we have handled all unions and\n+        // structs. (We have also handled univariant enums\n+        // that allow representation optimization.)\n+        assert!(is_enum);\n+\n+        // Until we've decided whether to use the tagged or\n+        // niche filling LayoutS, we don't want to intern the\n+        // variant layouts, so we can't store them in the\n+        // overall LayoutS. Store the overall LayoutS\n+        // and the variant LayoutSs here until then.\n+        struct TmpLayout<V: Idx> {\n+            layout: LayoutS<V>,\n+            variants: IndexVec<V, LayoutS<V>>,\n+        }\n+\n+        let calculate_niche_filling_layout = || -> Option<TmpLayout<V>> {\n+            if niche_optimize_enum {\n+                return None;\n+            }\n+\n+            if variants.len() < 2 {\n+                return None;\n+            }\n+\n+            let mut align = dl.aggregate_align;\n+            let mut variant_layouts = variants\n+                .iter_enumerated()\n+                .map(|(j, v)| {\n+                    let mut st = self.univariant(dl, v, &repr, StructKind::AlwaysSized)?;\n+                    st.variants = Variants::Single { index: j };\n+\n+                    align = align.max(st.align);\n+\n+                    Some(st)\n+                })\n+                .collect::<Option<IndexVec<V, _>>>()?;\n+\n+            let largest_variant_index = variant_layouts\n+                .iter_enumerated()\n+                .max_by_key(|(_i, layout)| layout.size.bytes())\n+                .map(|(i, _layout)| i)?;\n+\n+            let all_indices = (0..=variants.len() - 1).map(V::new);\n+            let needs_disc = |index: V| index != largest_variant_index && !absent(&variants[index]);\n+            let niche_variants = all_indices.clone().find(|v| needs_disc(*v)).unwrap().index()\n+                ..=all_indices.rev().find(|v| needs_disc(*v)).unwrap().index();\n+\n+            let count = niche_variants.size_hint().1.unwrap() as u128;\n+\n+            // Find the field with the largest niche\n+            let (field_index, niche, (niche_start, niche_scalar)) = variants[largest_variant_index]\n+                .iter()\n+                .enumerate()\n+                .filter_map(|(j, field)| Some((j, field.largest_niche?)))\n+                .max_by_key(|(_, niche)| niche.available(dl))\n+                .and_then(|(j, niche)| Some((j, niche, niche.reserve(dl, count)?)))?;\n+            let niche_offset =\n+                niche.offset + variant_layouts[largest_variant_index].fields.offset(field_index);\n+            let niche_size = niche.value.size(dl);\n+            let size = variant_layouts[largest_variant_index].size.align_to(align.abi);\n+\n+            let all_variants_fit = variant_layouts.iter_enumerated_mut().all(|(i, layout)| {\n+                if i == largest_variant_index {\n+                    return true;\n+                }\n+\n+                layout.largest_niche = None;\n+\n+                if layout.size <= niche_offset {\n+                    // This variant will fit before the niche.\n+                    return true;\n+                }\n+\n+                // Determine if it'll fit after the niche.\n+                let this_align = layout.align.abi;\n+                let this_offset = (niche_offset + niche_size).align_to(this_align);\n+\n+                if this_offset + layout.size > size {\n+                    return false;\n+                }\n+\n+                // It'll fit, but we need to make some adjustments.\n+                match layout.fields {\n+                    FieldsShape::Arbitrary { ref mut offsets, .. } => {\n+                        for (j, offset) in offsets.iter_mut().enumerate() {\n+                            if !variants[i][j].is_zst() {\n+                                *offset += this_offset;\n+                            }\n+                        }\n+                    }\n+                    _ => {\n+                        panic!(\"Layout of fields should be Arbitrary for variants\")\n+                    }\n+                }\n+\n+                // It can't be a Scalar or ScalarPair because the offset isn't 0.\n+                if !layout.abi.is_uninhabited() {\n+                    layout.abi = Abi::Aggregate { sized: true };\n+                }\n+                layout.size += this_offset;\n+\n+                true\n+            });\n+\n+            if !all_variants_fit {\n+                return None;\n+            }\n+\n+            let largest_niche = Niche::from_scalar(dl, niche_offset, niche_scalar);\n+\n+            let others_zst = variant_layouts\n+                .iter_enumerated()\n+                .all(|(i, layout)| i == largest_variant_index || layout.size == Size::ZERO);\n+            let same_size = size == variant_layouts[largest_variant_index].size;\n+            let same_align = align == variant_layouts[largest_variant_index].align;\n+\n+            let abi = if variant_layouts.iter().all(|v| v.abi.is_uninhabited()) {\n+                Abi::Uninhabited\n+            } else if same_size && same_align && others_zst {\n+                match variant_layouts[largest_variant_index].abi {\n+                    // When the total alignment and size match, we can use the\n+                    // same ABI as the scalar variant with the reserved niche.\n+                    Abi::Scalar(_) => Abi::Scalar(niche_scalar),\n+                    Abi::ScalarPair(first, second) => {\n+                        // Only the niche is guaranteed to be initialised,\n+                        // so use union layouts for the other primitive.\n+                        if niche_offset == Size::ZERO {\n+                            Abi::ScalarPair(niche_scalar, second.to_union())\n+                        } else {\n+                            Abi::ScalarPair(first.to_union(), niche_scalar)\n+                        }\n+                    }\n+                    _ => Abi::Aggregate { sized: true },\n+                }\n+            } else {\n+                Abi::Aggregate { sized: true }\n+            };\n+\n+            let layout = LayoutS {\n+                variants: Variants::Multiple {\n+                    tag: niche_scalar,\n+                    tag_encoding: TagEncoding::Niche {\n+                        untagged_variant: largest_variant_index,\n+                        niche_variants: (V::new(*niche_variants.start())\n+                            ..=V::new(*niche_variants.end())),\n+                        niche_start,\n+                    },\n+                    tag_field: 0,\n+                    variants: IndexVec::new(),\n+                },\n+                fields: FieldsShape::Arbitrary {\n+                    offsets: vec![niche_offset],\n+                    memory_index: vec![0],\n+                },\n+                abi,\n+                largest_niche,\n+                size,\n+                align,\n+            };\n+\n+            Some(TmpLayout { layout, variants: variant_layouts })\n+        };\n+\n+        let niche_filling_layout = calculate_niche_filling_layout();\n+\n+        let (mut min, mut max) = (i128::MAX, i128::MIN);\n+        let discr_type = repr.discr_type();\n+        let bits = Integer::from_attr(dl, discr_type).size().bits();\n+        for (i, mut val) in discriminants {\n+            if variants[i].iter().any(|f| f.abi.is_uninhabited()) {\n+                continue;\n+            }\n+            if discr_type.is_signed() {\n+                // sign extend the raw representation to be an i128\n+                val = (val << (128 - bits)) >> (128 - bits);\n+            }\n+            if val < min {\n+                min = val;\n+            }\n+            if val > max {\n+                max = val;\n+            }\n+        }\n+        // We might have no inhabited variants, so pretend there's at least one.\n+        if (min, max) == (i128::MAX, i128::MIN) {\n+            min = 0;\n+            max = 0;\n+        }\n+        assert!(min <= max, \"discriminant range is {}...{}\", min, max);\n+        let (min_ity, signed) = discr_range_of_repr(min, max); //Integer::repr_discr(tcx, ty, &repr, min, max);\n+\n+        let mut align = dl.aggregate_align;\n+        let mut size = Size::ZERO;\n+\n+        // We're interested in the smallest alignment, so start large.\n+        let mut start_align = Align::from_bytes(256).unwrap();\n+        assert_eq!(Integer::for_align(dl, start_align), None);\n+\n+        // repr(C) on an enum tells us to make a (tag, union) layout,\n+        // so we need to grow the prefix alignment to be at least\n+        // the alignment of the union. (This value is used both for\n+        // determining the alignment of the overall enum, and the\n+        // determining the alignment of the payload after the tag.)\n+        let mut prefix_align = min_ity.align(dl).abi;\n+        if repr.c() {\n+            for fields in variants {\n+                for field in fields {\n+                    prefix_align = prefix_align.max(field.align.abi);\n+                }\n+            }\n+        }\n+\n+        // Create the set of structs that represent each variant.\n+        let mut layout_variants = variants\n+            .iter_enumerated()\n+            .map(|(i, field_layouts)| {\n+                let mut st = self.univariant(\n+                    dl,\n+                    &field_layouts,\n+                    &repr,\n+                    StructKind::Prefixed(min_ity.size(), prefix_align),\n+                )?;\n+                st.variants = Variants::Single { index: i };\n+                // Find the first field we can't move later\n+                // to make room for a larger discriminant.\n+                for field in st.fields.index_by_increasing_offset().map(|j| &field_layouts[j]) {\n+                    if !field.is_zst() || field.align.abi.bytes() != 1 {\n+                        start_align = start_align.min(field.align.abi);\n+                        break;\n+                    }\n+                }\n+                size = cmp::max(size, st.size);\n+                align = align.max(st.align);\n+                Some(st)\n+            })\n+            .collect::<Option<IndexVec<V, _>>>()?;\n+\n+        // Align the maximum variant size to the largest alignment.\n+        size = size.align_to(align.abi);\n+\n+        if size.bytes() >= dl.obj_size_bound() {\n+            return None;\n+        }\n+\n+        let typeck_ity = Integer::from_attr(dl, repr.discr_type());\n+        if typeck_ity < min_ity {\n+            // It is a bug if Layout decided on a greater discriminant size than typeck for\n+            // some reason at this point (based on values discriminant can take on). Mostly\n+            // because this discriminant will be loaded, and then stored into variable of\n+            // type calculated by typeck. Consider such case (a bug): typeck decided on\n+            // byte-sized discriminant, but layout thinks we need a 16-bit to store all\n+            // discriminant values. That would be a bug, because then, in codegen, in order\n+            // to store this 16-bit discriminant into 8-bit sized temporary some of the\n+            // space necessary to represent would have to be discarded (or layout is wrong\n+            // on thinking it needs 16 bits)\n+            panic!(\n+                \"layout decided on a larger discriminant type ({:?}) than typeck ({:?})\",\n+                min_ity, typeck_ity\n+            );\n+            // However, it is fine to make discr type however large (as an optimisation)\n+            // after this point \u2013 we\u2019ll just truncate the value we load in codegen.\n+        }\n+\n+        // Check to see if we should use a different type for the\n+        // discriminant. We can safely use a type with the same size\n+        // as the alignment of the first field of each variant.\n+        // We increase the size of the discriminant to avoid LLVM copying\n+        // padding when it doesn't need to. This normally causes unaligned\n+        // load/stores and excessive memcpy/memset operations. By using a\n+        // bigger integer size, LLVM can be sure about its contents and\n+        // won't be so conservative.\n+\n+        // Use the initial field alignment\n+        let mut ity = if repr.c() || repr.int.is_some() {\n+            min_ity\n+        } else {\n+            Integer::for_align(dl, start_align).unwrap_or(min_ity)\n+        };\n+\n+        // If the alignment is not larger than the chosen discriminant size,\n+        // don't use the alignment as the final size.\n+        if ity <= min_ity {\n+            ity = min_ity;\n+        } else {\n+            // Patch up the variants' first few fields.\n+            let old_ity_size = min_ity.size();\n+            let new_ity_size = ity.size();\n+            for variant in &mut layout_variants {\n+                match variant.fields {\n+                    FieldsShape::Arbitrary { ref mut offsets, .. } => {\n+                        for i in offsets {\n+                            if *i <= old_ity_size {\n+                                assert_eq!(*i, old_ity_size);\n+                                *i = new_ity_size;\n+                            }\n+                        }\n+                        // We might be making the struct larger.\n+                        if variant.size <= old_ity_size {\n+                            variant.size = new_ity_size;\n+                        }\n+                    }\n+                    _ => panic!(),\n+                }\n+            }\n+        }\n+\n+        let tag_mask = ity.size().unsigned_int_max();\n+        let tag = Scalar::Initialized {\n+            value: Int(ity, signed),\n+            valid_range: WrappingRange {\n+                start: (min as u128 & tag_mask),\n+                end: (max as u128 & tag_mask),\n+            },\n+        };\n+        let mut abi = Abi::Aggregate { sized: true };\n+\n+        if layout_variants.iter().all(|v| v.abi.is_uninhabited()) {\n+            abi = Abi::Uninhabited;\n+        } else if tag.size(dl) == size {\n+            // Make sure we only use scalar layout when the enum is entirely its\n+            // own tag (i.e. it has no padding nor any non-ZST variant fields).\n+            abi = Abi::Scalar(tag);\n+        } else {\n+            // Try to use a ScalarPair for all tagged enums.\n+            let mut common_prim = None;\n+            let mut common_prim_initialized_in_all_variants = true;\n+            for (field_layouts, layout_variant) in iter::zip(&*variants, &layout_variants) {\n+                let FieldsShape::Arbitrary { ref offsets, .. } = layout_variant.fields else {\n+                    panic!();\n+                };\n+                let mut fields = iter::zip(field_layouts, offsets).filter(|p| !p.0.is_zst());\n+                let (field, offset) = match (fields.next(), fields.next()) {\n+                    (None, None) => {\n+                        common_prim_initialized_in_all_variants = false;\n+                        continue;\n+                    }\n+                    (Some(pair), None) => pair,\n+                    _ => {\n+                        common_prim = None;\n+                        break;\n+                    }\n+                };\n+                let prim = match field.abi {\n+                    Abi::Scalar(scalar) => {\n+                        common_prim_initialized_in_all_variants &=\n+                            matches!(scalar, Scalar::Initialized { .. });\n+                        scalar.primitive()\n+                    }\n+                    _ => {\n+                        common_prim = None;\n+                        break;\n+                    }\n+                };\n+                if let Some(pair) = common_prim {\n+                    // This is pretty conservative. We could go fancier\n+                    // by conflating things like i32 and u32, or even\n+                    // realising that (u8, u8) could just cohabit with\n+                    // u16 or even u32.\n+                    if pair != (prim, offset) {\n+                        common_prim = None;\n+                        break;\n+                    }\n+                } else {\n+                    common_prim = Some((prim, offset));\n+                }\n+            }\n+            if let Some((prim, offset)) = common_prim {\n+                let prim_scalar = if common_prim_initialized_in_all_variants {\n+                    scalar_unit(prim)\n+                } else {\n+                    // Common prim might be uninit.\n+                    Scalar::Union { value: prim }\n+                };\n+                let pair = self.scalar_pair::<V>(tag, prim_scalar);\n+                let pair_offsets = match pair.fields {\n+                    FieldsShape::Arbitrary { ref offsets, ref memory_index } => {\n+                        assert_eq!(memory_index, &[0, 1]);\n+                        offsets\n+                    }\n+                    _ => panic!(),\n+                };\n+                if pair_offsets[0] == Size::ZERO\n+                    && pair_offsets[1] == *offset\n+                    && align == pair.align\n+                    && size == pair.size\n+                {\n+                    // We can use `ScalarPair` only when it matches our\n+                    // already computed layout (including `#[repr(C)]`).\n+                    abi = pair.abi;\n+                }\n+            }\n+        }\n+\n+        // If we pick a \"clever\" (by-value) ABI, we might have to adjust the ABI of the\n+        // variants to ensure they are consistent. This is because a downcast is\n+        // semantically a NOP, and thus should not affect layout.\n+        if matches!(abi, Abi::Scalar(..) | Abi::ScalarPair(..)) {\n+            for variant in &mut layout_variants {\n+                // We only do this for variants with fields; the others are not accessed anyway.\n+                // Also do not overwrite any already existing \"clever\" ABIs.\n+                if variant.fields.count() > 0 && matches!(variant.abi, Abi::Aggregate { .. }) {\n+                    variant.abi = abi;\n+                    // Also need to bump up the size and alignment, so that the entire value fits in here.\n+                    variant.size = cmp::max(variant.size, size);\n+                    variant.align.abi = cmp::max(variant.align.abi, align.abi);\n+                }\n+            }\n+        }\n+\n+        let largest_niche = Niche::from_scalar(dl, Size::ZERO, tag);\n+\n+        let tagged_layout = LayoutS {\n+            variants: Variants::Multiple {\n+                tag,\n+                tag_encoding: TagEncoding::Direct,\n+                tag_field: 0,\n+                variants: IndexVec::new(),\n+            },\n+            fields: FieldsShape::Arbitrary { offsets: vec![Size::ZERO], memory_index: vec![0] },\n+            largest_niche,\n+            abi,\n+            align,\n+            size,\n+        };\n+\n+        let tagged_layout = TmpLayout { layout: tagged_layout, variants: layout_variants };\n+\n+        let mut best_layout = match (tagged_layout, niche_filling_layout) {\n+            (tl, Some(nl)) => {\n+                // Pick the smaller layout; otherwise,\n+                // pick the layout with the larger niche; otherwise,\n+                // pick tagged as it has simpler codegen.\n+                use cmp::Ordering::*;\n+                let niche_size = |tmp_l: &TmpLayout<V>| {\n+                    tmp_l.layout.largest_niche.map_or(0, |n| n.available(dl))\n+                };\n+                match (tl.layout.size.cmp(&nl.layout.size), niche_size(&tl).cmp(&niche_size(&nl))) {\n+                    (Greater, _) => nl,\n+                    (Equal, Less) => nl,\n+                    _ => tl,\n+                }\n+            }\n+            (tl, None) => tl,\n+        };\n+\n+        // Now we can intern the variant layouts and store them in the enum layout.\n+        best_layout.layout.variants = match best_layout.layout.variants {\n+            Variants::Multiple { tag, tag_encoding, tag_field, .. } => {\n+                Variants::Multiple { tag, tag_encoding, tag_field, variants: best_layout.variants }\n+            }\n+            _ => panic!(),\n+        };\n+        Some(best_layout.layout)\n+    }\n+\n+    fn layout_of_union<'a, V: Idx, F: Deref<Target = &'a LayoutS<V>> + Debug>(\n+        &self,\n+        repr: &ReprOptions,\n+        variants: &IndexVec<V, Vec<F>>,\n+    ) -> Option<LayoutS<V>> {\n+        let dl = self.current_data_layout();\n+        let dl = dl.borrow();\n+        let mut align = if repr.pack.is_some() { dl.i8_align } else { dl.aggregate_align };\n+\n+        if let Some(repr_align) = repr.align {\n+            align = align.max(AbiAndPrefAlign::new(repr_align));\n+        }\n+\n+        let optimize = !repr.inhibit_union_abi_opt();\n+        let mut size = Size::ZERO;\n+        let mut abi = Abi::Aggregate { sized: true };\n+        let index = V::new(0);\n+        for field in &variants[index] {\n+            assert!(field.is_sized());\n+            align = align.max(field.align);\n+\n+            // If all non-ZST fields have the same ABI, forward this ABI\n+            if optimize && !field.is_zst() {\n+                // Discard valid range information and allow undef\n+                let field_abi = match field.abi {\n+                    Abi::Scalar(x) => Abi::Scalar(x.to_union()),\n+                    Abi::ScalarPair(x, y) => Abi::ScalarPair(x.to_union(), y.to_union()),\n+                    Abi::Vector { element: x, count } => {\n+                        Abi::Vector { element: x.to_union(), count }\n+                    }\n+                    Abi::Uninhabited | Abi::Aggregate { .. } => Abi::Aggregate { sized: true },\n+                };\n+\n+                if size == Size::ZERO {\n+                    // first non ZST: initialize 'abi'\n+                    abi = field_abi;\n+                } else if abi != field_abi {\n+                    // different fields have different ABI: reset to Aggregate\n+                    abi = Abi::Aggregate { sized: true };\n+                }\n+            }\n+\n+            size = cmp::max(size, field.size);\n+        }\n+\n+        if let Some(pack) = repr.pack {\n+            align = align.min(AbiAndPrefAlign::new(pack));\n+        }\n+\n+        Some(LayoutS {\n+            variants: Variants::Single { index },\n+            fields: FieldsShape::Union(NonZeroUsize::new(variants[index].len())?),\n+            abi,\n+            largest_niche: None,\n+            align,\n+            size: size.align_to(align.abi),\n+        })\n+    }\n+}"}, {"sha": "4f4a4bf314f14f76fa5422dbe9e6baaa9c7c92a8", "filename": "compiler/rustc_abi/src/lib.rs", "status": "added", "additions": 1399, "deletions": 0, "changes": 1399, "blob_url": "https://github.com/rust-lang/rust/blob/b3bc6bf31265ac10946a0832092dbcedf9b26805/compiler%2Frustc_abi%2Fsrc%2Flib.rs", "raw_url": "https://github.com/rust-lang/rust/raw/b3bc6bf31265ac10946a0832092dbcedf9b26805/compiler%2Frustc_abi%2Fsrc%2Flib.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_abi%2Fsrc%2Flib.rs?ref=b3bc6bf31265ac10946a0832092dbcedf9b26805", "patch": "@@ -0,0 +1,1399 @@\n+#![cfg_attr(feature = \"nightly\", feature(step_trait, rustc_attrs, min_specialization))]\n+\n+use std::convert::{TryFrom, TryInto};\n+use std::fmt;\n+#[cfg(feature = \"nightly\")]\n+use std::iter::Step;\n+use std::num::{NonZeroUsize, ParseIntError};\n+use std::ops::{Add, AddAssign, Mul, RangeInclusive, Sub};\n+use std::str::FromStr;\n+\n+use bitflags::bitflags;\n+use rustc_index::vec::{Idx, IndexVec};\n+#[cfg(feature = \"nightly\")]\n+use rustc_macros::HashStable_Generic;\n+#[cfg(feature = \"nightly\")]\n+use rustc_macros::{Decodable, Encodable};\n+\n+mod layout;\n+\n+pub use layout::LayoutCalculator;\n+\n+/// Requirements for a `StableHashingContext` to be used in this crate.\n+/// This is a hack to allow using the `HashStable_Generic` derive macro\n+/// instead of implementing everything in `rustc_middle`.\n+pub trait HashStableContext {}\n+\n+use Integer::*;\n+use Primitive::*;\n+\n+bitflags! {\n+    #[derive(Default)]\n+    #[cfg_attr(feature = \"nightly\", derive(Encodable, Decodable, HashStable_Generic))]\n+    pub struct ReprFlags: u8 {\n+        const IS_C               = 1 << 0;\n+        const IS_SIMD            = 1 << 1;\n+        const IS_TRANSPARENT     = 1 << 2;\n+        // Internal only for now. If true, don't reorder fields.\n+        const IS_LINEAR          = 1 << 3;\n+        // If true, the type's layout can be randomized using\n+        // the seed stored in `ReprOptions.layout_seed`\n+        const RANDOMIZE_LAYOUT   = 1 << 4;\n+        // Any of these flags being set prevent field reordering optimisation.\n+        const IS_UNOPTIMISABLE   = ReprFlags::IS_C.bits\n+                                 | ReprFlags::IS_SIMD.bits\n+                                 | ReprFlags::IS_LINEAR.bits;\n+    }\n+}\n+\n+#[derive(Copy, Clone, Debug, Eq, PartialEq)]\n+#[cfg_attr(feature = \"nightly\", derive(Encodable, Decodable, HashStable_Generic))]\n+pub enum IntegerType {\n+    /// Pointer sized integer type, i.e. isize and usize. The field shows signedness, that\n+    /// is, `Pointer(true)` is isize.\n+    Pointer(bool),\n+    /// Fix sized integer type, e.g. i8, u32, i128 The bool field shows signedness, `Fixed(I8, false)` means `u8`\n+    Fixed(Integer, bool),\n+}\n+\n+impl IntegerType {\n+    pub fn is_signed(&self) -> bool {\n+        match self {\n+            IntegerType::Pointer(b) => *b,\n+            IntegerType::Fixed(_, b) => *b,\n+        }\n+    }\n+}\n+\n+/// Represents the repr options provided by the user,\n+#[derive(Copy, Clone, Debug, Eq, PartialEq, Default)]\n+#[cfg_attr(feature = \"nightly\", derive(Encodable, Decodable, HashStable_Generic))]\n+pub struct ReprOptions {\n+    pub int: Option<IntegerType>,\n+    pub align: Option<Align>,\n+    pub pack: Option<Align>,\n+    pub flags: ReprFlags,\n+    /// The seed to be used for randomizing a type's layout\n+    ///\n+    /// Note: This could technically be a `[u8; 16]` (a `u128`) which would\n+    /// be the \"most accurate\" hash as it'd encompass the item and crate\n+    /// hash without loss, but it does pay the price of being larger.\n+    /// Everything's a tradeoff, a `u64` seed should be sufficient for our\n+    /// purposes (primarily `-Z randomize-layout`)\n+    pub field_shuffle_seed: u64,\n+}\n+\n+impl ReprOptions {\n+    #[inline]\n+    pub fn simd(&self) -> bool {\n+        self.flags.contains(ReprFlags::IS_SIMD)\n+    }\n+\n+    #[inline]\n+    pub fn c(&self) -> bool {\n+        self.flags.contains(ReprFlags::IS_C)\n+    }\n+\n+    #[inline]\n+    pub fn packed(&self) -> bool {\n+        self.pack.is_some()\n+    }\n+\n+    #[inline]\n+    pub fn transparent(&self) -> bool {\n+        self.flags.contains(ReprFlags::IS_TRANSPARENT)\n+    }\n+\n+    #[inline]\n+    pub fn linear(&self) -> bool {\n+        self.flags.contains(ReprFlags::IS_LINEAR)\n+    }\n+\n+    /// Returns the discriminant type, given these `repr` options.\n+    /// This must only be called on enums!\n+    pub fn discr_type(&self) -> IntegerType {\n+        self.int.unwrap_or(IntegerType::Pointer(true))\n+    }\n+\n+    /// Returns `true` if this `#[repr()]` should inhabit \"smart enum\n+    /// layout\" optimizations, such as representing `Foo<&T>` as a\n+    /// single pointer.\n+    pub fn inhibit_enum_layout_opt(&self) -> bool {\n+        self.c() || self.int.is_some()\n+    }\n+\n+    /// Returns `true` if this `#[repr()]` should inhibit struct field reordering\n+    /// optimizations, such as with `repr(C)`, `repr(packed(1))`, or `repr(<int>)`.\n+    pub fn inhibit_struct_field_reordering_opt(&self) -> bool {\n+        if let Some(pack) = self.pack {\n+            if pack.bytes() == 1 {\n+                return true;\n+            }\n+        }\n+\n+        self.flags.intersects(ReprFlags::IS_UNOPTIMISABLE) || self.int.is_some()\n+    }\n+\n+    /// Returns `true` if this type is valid for reordering and `-Z randomize-layout`\n+    /// was enabled for its declaration crate\n+    pub fn can_randomize_type_layout(&self) -> bool {\n+        !self.inhibit_struct_field_reordering_opt()\n+            && self.flags.contains(ReprFlags::RANDOMIZE_LAYOUT)\n+    }\n+\n+    /// Returns `true` if this `#[repr()]` should inhibit union ABI optimisations.\n+    pub fn inhibit_union_abi_opt(&self) -> bool {\n+        self.c()\n+    }\n+}\n+\n+/// Parsed [Data layout](https://llvm.org/docs/LangRef.html#data-layout)\n+/// for a target, which contains everything needed to compute layouts.\n+#[derive(Debug, PartialEq, Eq)]\n+pub struct TargetDataLayout {\n+    pub endian: Endian,\n+    pub i1_align: AbiAndPrefAlign,\n+    pub i8_align: AbiAndPrefAlign,\n+    pub i16_align: AbiAndPrefAlign,\n+    pub i32_align: AbiAndPrefAlign,\n+    pub i64_align: AbiAndPrefAlign,\n+    pub i128_align: AbiAndPrefAlign,\n+    pub f32_align: AbiAndPrefAlign,\n+    pub f64_align: AbiAndPrefAlign,\n+    pub pointer_size: Size,\n+    pub pointer_align: AbiAndPrefAlign,\n+    pub aggregate_align: AbiAndPrefAlign,\n+\n+    /// Alignments for vector types.\n+    pub vector_align: Vec<(Size, AbiAndPrefAlign)>,\n+\n+    pub instruction_address_space: AddressSpace,\n+\n+    /// Minimum size of #[repr(C)] enums (default I32 bits)\n+    pub c_enum_min_size: Integer,\n+}\n+\n+impl Default for TargetDataLayout {\n+    /// Creates an instance of `TargetDataLayout`.\n+    fn default() -> TargetDataLayout {\n+        let align = |bits| Align::from_bits(bits).unwrap();\n+        TargetDataLayout {\n+            endian: Endian::Big,\n+            i1_align: AbiAndPrefAlign::new(align(8)),\n+            i8_align: AbiAndPrefAlign::new(align(8)),\n+            i16_align: AbiAndPrefAlign::new(align(16)),\n+            i32_align: AbiAndPrefAlign::new(align(32)),\n+            i64_align: AbiAndPrefAlign { abi: align(32), pref: align(64) },\n+            i128_align: AbiAndPrefAlign { abi: align(32), pref: align(64) },\n+            f32_align: AbiAndPrefAlign::new(align(32)),\n+            f64_align: AbiAndPrefAlign::new(align(64)),\n+            pointer_size: Size::from_bits(64),\n+            pointer_align: AbiAndPrefAlign::new(align(64)),\n+            aggregate_align: AbiAndPrefAlign { abi: align(0), pref: align(64) },\n+            vector_align: vec![\n+                (Size::from_bits(64), AbiAndPrefAlign::new(align(64))),\n+                (Size::from_bits(128), AbiAndPrefAlign::new(align(128))),\n+            ],\n+            instruction_address_space: AddressSpace::DATA,\n+            c_enum_min_size: Integer::I32,\n+        }\n+    }\n+}\n+\n+pub enum TargetDataLayoutErrors<'a> {\n+    InvalidAddressSpace { addr_space: &'a str, cause: &'a str, err: ParseIntError },\n+    InvalidBits { kind: &'a str, bit: &'a str, cause: &'a str, err: ParseIntError },\n+    MissingAlignment { cause: &'a str },\n+    InvalidAlignment { cause: &'a str, err: String },\n+    InconsistentTargetArchitecture { dl: &'a str, target: &'a str },\n+    InconsistentTargetPointerWidth { pointer_size: u64, target: u32 },\n+    InvalidBitsSize { err: String },\n+}\n+\n+impl TargetDataLayout {\n+    /// Returns exclusive upper bound on object size.\n+    ///\n+    /// The theoretical maximum object size is defined as the maximum positive `isize` value.\n+    /// This ensures that the `offset` semantics remain well-defined by allowing it to correctly\n+    /// index every address within an object along with one byte past the end, along with allowing\n+    /// `isize` to store the difference between any two pointers into an object.\n+    ///\n+    /// The upper bound on 64-bit currently needs to be lower because LLVM uses a 64-bit integer\n+    /// to represent object size in bits. It would need to be 1 << 61 to account for this, but is\n+    /// currently conservatively bounded to 1 << 47 as that is enough to cover the current usable\n+    /// address space on 64-bit ARMv8 and x86_64.\n+    #[inline]\n+    pub fn obj_size_bound(&self) -> u64 {\n+        match self.pointer_size.bits() {\n+            16 => 1 << 15,\n+            32 => 1 << 31,\n+            64 => 1 << 47,\n+            bits => panic!(\"obj_size_bound: unknown pointer bit size {}\", bits),\n+        }\n+    }\n+\n+    #[inline]\n+    pub fn ptr_sized_integer(&self) -> Integer {\n+        match self.pointer_size.bits() {\n+            16 => I16,\n+            32 => I32,\n+            64 => I64,\n+            bits => panic!(\"ptr_sized_integer: unknown pointer bit size {}\", bits),\n+        }\n+    }\n+\n+    #[inline]\n+    pub fn vector_align(&self, vec_size: Size) -> AbiAndPrefAlign {\n+        for &(size, align) in &self.vector_align {\n+            if size == vec_size {\n+                return align;\n+            }\n+        }\n+        // Default to natural alignment, which is what LLVM does.\n+        // That is, use the size, rounded up to a power of 2.\n+        AbiAndPrefAlign::new(Align::from_bytes(vec_size.bytes().next_power_of_two()).unwrap())\n+    }\n+}\n+\n+pub trait HasDataLayout {\n+    fn data_layout(&self) -> &TargetDataLayout;\n+}\n+\n+impl HasDataLayout for TargetDataLayout {\n+    #[inline]\n+    fn data_layout(&self) -> &TargetDataLayout {\n+        self\n+    }\n+}\n+\n+/// Endianness of the target, which must match cfg(target-endian).\n+#[derive(Copy, Clone, PartialEq, Eq)]\n+pub enum Endian {\n+    Little,\n+    Big,\n+}\n+\n+impl Endian {\n+    pub fn as_str(&self) -> &'static str {\n+        match self {\n+            Self::Little => \"little\",\n+            Self::Big => \"big\",\n+        }\n+    }\n+}\n+\n+impl fmt::Debug for Endian {\n+    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {\n+        f.write_str(self.as_str())\n+    }\n+}\n+\n+impl FromStr for Endian {\n+    type Err = String;\n+\n+    fn from_str(s: &str) -> Result<Self, Self::Err> {\n+        match s {\n+            \"little\" => Ok(Self::Little),\n+            \"big\" => Ok(Self::Big),\n+            _ => Err(format!(r#\"unknown endian: \"{}\"\"#, s)),\n+        }\n+    }\n+}\n+\n+/// Size of a type in bytes.\n+#[derive(Copy, Clone, PartialEq, Eq, PartialOrd, Ord, Hash)]\n+#[cfg_attr(feature = \"nightly\", derive(Encodable, Decodable, HashStable_Generic))]\n+pub struct Size {\n+    raw: u64,\n+}\n+\n+// This is debug-printed a lot in larger structs, don't waste too much space there\n+impl fmt::Debug for Size {\n+    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {\n+        write!(f, \"Size({} bytes)\", self.bytes())\n+    }\n+}\n+\n+impl Size {\n+    pub const ZERO: Size = Size { raw: 0 };\n+\n+    /// Rounds `bits` up to the next-higher byte boundary, if `bits` is\n+    /// not a multiple of 8.\n+    pub fn from_bits(bits: impl TryInto<u64>) -> Size {\n+        let bits = bits.try_into().ok().unwrap();\n+        // Avoid potential overflow from `bits + 7`.\n+        Size { raw: bits / 8 + ((bits % 8) + 7) / 8 }\n+    }\n+\n+    #[inline]\n+    pub fn from_bytes(bytes: impl TryInto<u64>) -> Size {\n+        let bytes: u64 = bytes.try_into().ok().unwrap();\n+        Size { raw: bytes }\n+    }\n+\n+    #[inline]\n+    pub fn bytes(self) -> u64 {\n+        self.raw\n+    }\n+\n+    #[inline]\n+    pub fn bytes_usize(self) -> usize {\n+        self.bytes().try_into().unwrap()\n+    }\n+\n+    #[inline]\n+    pub fn bits(self) -> u64 {\n+        #[cold]\n+        fn overflow(bytes: u64) -> ! {\n+            panic!(\"Size::bits: {} bytes in bits doesn't fit in u64\", bytes)\n+        }\n+\n+        self.bytes().checked_mul(8).unwrap_or_else(|| overflow(self.bytes()))\n+    }\n+\n+    #[inline]\n+    pub fn bits_usize(self) -> usize {\n+        self.bits().try_into().unwrap()\n+    }\n+\n+    #[inline]\n+    pub fn align_to(self, align: Align) -> Size {\n+        let mask = align.bytes() - 1;\n+        Size::from_bytes((self.bytes() + mask) & !mask)\n+    }\n+\n+    #[inline]\n+    pub fn is_aligned(self, align: Align) -> bool {\n+        let mask = align.bytes() - 1;\n+        self.bytes() & mask == 0\n+    }\n+\n+    #[inline]\n+    pub fn checked_add<C: HasDataLayout>(self, offset: Size, cx: &C) -> Option<Size> {\n+        let dl = cx.data_layout();\n+\n+        let bytes = self.bytes().checked_add(offset.bytes())?;\n+\n+        if bytes < dl.obj_size_bound() { Some(Size::from_bytes(bytes)) } else { None }\n+    }\n+\n+    #[inline]\n+    pub fn checked_mul<C: HasDataLayout>(self, count: u64, cx: &C) -> Option<Size> {\n+        let dl = cx.data_layout();\n+\n+        let bytes = self.bytes().checked_mul(count)?;\n+        if bytes < dl.obj_size_bound() { Some(Size::from_bytes(bytes)) } else { None }\n+    }\n+\n+    /// Truncates `value` to `self` bits and then sign-extends it to 128 bits\n+    /// (i.e., if it is negative, fill with 1's on the left).\n+    #[inline]\n+    pub fn sign_extend(self, value: u128) -> u128 {\n+        let size = self.bits();\n+        if size == 0 {\n+            // Truncated until nothing is left.\n+            return 0;\n+        }\n+        // Sign-extend it.\n+        let shift = 128 - size;\n+        // Shift the unsigned value to the left, then shift back to the right as signed\n+        // (essentially fills with sign bit on the left).\n+        (((value << shift) as i128) >> shift) as u128\n+    }\n+\n+    /// Truncates `value` to `self` bits.\n+    #[inline]\n+    pub fn truncate(self, value: u128) -> u128 {\n+        let size = self.bits();\n+        if size == 0 {\n+            // Truncated until nothing is left.\n+            return 0;\n+        }\n+        let shift = 128 - size;\n+        // Truncate (shift left to drop out leftover values, shift right to fill with zeroes).\n+        (value << shift) >> shift\n+    }\n+\n+    #[inline]\n+    pub fn signed_int_min(&self) -> i128 {\n+        self.sign_extend(1_u128 << (self.bits() - 1)) as i128\n+    }\n+\n+    #[inline]\n+    pub fn signed_int_max(&self) -> i128 {\n+        i128::MAX >> (128 - self.bits())\n+    }\n+\n+    #[inline]\n+    pub fn unsigned_int_max(&self) -> u128 {\n+        u128::MAX >> (128 - self.bits())\n+    }\n+}\n+\n+// Panicking addition, subtraction and multiplication for convenience.\n+// Avoid during layout computation, return `LayoutError` instead.\n+\n+impl Add for Size {\n+    type Output = Size;\n+    #[inline]\n+    fn add(self, other: Size) -> Size {\n+        Size::from_bytes(self.bytes().checked_add(other.bytes()).unwrap_or_else(|| {\n+            panic!(\"Size::add: {} + {} doesn't fit in u64\", self.bytes(), other.bytes())\n+        }))\n+    }\n+}\n+\n+impl Sub for Size {\n+    type Output = Size;\n+    #[inline]\n+    fn sub(self, other: Size) -> Size {\n+        Size::from_bytes(self.bytes().checked_sub(other.bytes()).unwrap_or_else(|| {\n+            panic!(\"Size::sub: {} - {} would result in negative size\", self.bytes(), other.bytes())\n+        }))\n+    }\n+}\n+\n+impl Mul<Size> for u64 {\n+    type Output = Size;\n+    #[inline]\n+    fn mul(self, size: Size) -> Size {\n+        size * self\n+    }\n+}\n+\n+impl Mul<u64> for Size {\n+    type Output = Size;\n+    #[inline]\n+    fn mul(self, count: u64) -> Size {\n+        match self.bytes().checked_mul(count) {\n+            Some(bytes) => Size::from_bytes(bytes),\n+            None => panic!(\"Size::mul: {} * {} doesn't fit in u64\", self.bytes(), count),\n+        }\n+    }\n+}\n+\n+impl AddAssign for Size {\n+    #[inline]\n+    fn add_assign(&mut self, other: Size) {\n+        *self = *self + other;\n+    }\n+}\n+\n+#[cfg(feature = \"nightly\")]\n+impl Step for Size {\n+    #[inline]\n+    fn steps_between(start: &Self, end: &Self) -> Option<usize> {\n+        u64::steps_between(&start.bytes(), &end.bytes())\n+    }\n+\n+    #[inline]\n+    fn forward_checked(start: Self, count: usize) -> Option<Self> {\n+        u64::forward_checked(start.bytes(), count).map(Self::from_bytes)\n+    }\n+\n+    #[inline]\n+    fn forward(start: Self, count: usize) -> Self {\n+        Self::from_bytes(u64::forward(start.bytes(), count))\n+    }\n+\n+    #[inline]\n+    unsafe fn forward_unchecked(start: Self, count: usize) -> Self {\n+        Self::from_bytes(u64::forward_unchecked(start.bytes(), count))\n+    }\n+\n+    #[inline]\n+    fn backward_checked(start: Self, count: usize) -> Option<Self> {\n+        u64::backward_checked(start.bytes(), count).map(Self::from_bytes)\n+    }\n+\n+    #[inline]\n+    fn backward(start: Self, count: usize) -> Self {\n+        Self::from_bytes(u64::backward(start.bytes(), count))\n+    }\n+\n+    #[inline]\n+    unsafe fn backward_unchecked(start: Self, count: usize) -> Self {\n+        Self::from_bytes(u64::backward_unchecked(start.bytes(), count))\n+    }\n+}\n+\n+/// Alignment of a type in bytes (always a power of two).\n+#[derive(Copy, Clone, PartialEq, Eq, PartialOrd, Ord, Hash)]\n+#[cfg_attr(feature = \"nightly\", derive(Encodable, Decodable, HashStable_Generic))]\n+pub struct Align {\n+    pow2: u8,\n+}\n+\n+// This is debug-printed a lot in larger structs, don't waste too much space there\n+impl fmt::Debug for Align {\n+    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {\n+        write!(f, \"Align({} bytes)\", self.bytes())\n+    }\n+}\n+\n+impl Align {\n+    pub const ONE: Align = Align { pow2: 0 };\n+    pub const MAX: Align = Align { pow2: 29 };\n+\n+    #[inline]\n+    pub fn from_bits(bits: u64) -> Result<Align, String> {\n+        Align::from_bytes(Size::from_bits(bits).bytes())\n+    }\n+\n+    #[inline]\n+    pub fn from_bytes(align: u64) -> Result<Align, String> {\n+        // Treat an alignment of 0 bytes like 1-byte alignment.\n+        if align == 0 {\n+            return Ok(Align::ONE);\n+        }\n+\n+        #[cold]\n+        fn not_power_of_2(align: u64) -> String {\n+            format!(\"`{}` is not a power of 2\", align)\n+        }\n+\n+        #[cold]\n+        fn too_large(align: u64) -> String {\n+            format!(\"`{}` is too large\", align)\n+        }\n+\n+        let mut bytes = align;\n+        let mut pow2: u8 = 0;\n+        while (bytes & 1) == 0 {\n+            pow2 += 1;\n+            bytes >>= 1;\n+        }\n+        if bytes != 1 {\n+            return Err(not_power_of_2(align));\n+        }\n+        if pow2 > Self::MAX.pow2 {\n+            return Err(too_large(align));\n+        }\n+\n+        Ok(Align { pow2 })\n+    }\n+\n+    #[inline]\n+    pub fn bytes(self) -> u64 {\n+        1 << self.pow2\n+    }\n+\n+    #[inline]\n+    pub fn bits(self) -> u64 {\n+        self.bytes() * 8\n+    }\n+\n+    /// Computes the best alignment possible for the given offset\n+    /// (the largest power of two that the offset is a multiple of).\n+    ///\n+    /// N.B., for an offset of `0`, this happens to return `2^64`.\n+    #[inline]\n+    pub fn max_for_offset(offset: Size) -> Align {\n+        Align { pow2: offset.bytes().trailing_zeros() as u8 }\n+    }\n+\n+    /// Lower the alignment, if necessary, such that the given offset\n+    /// is aligned to it (the offset is a multiple of the alignment).\n+    #[inline]\n+    pub fn restrict_for_offset(self, offset: Size) -> Align {\n+        self.min(Align::max_for_offset(offset))\n+    }\n+}\n+\n+/// A pair of alignments, ABI-mandated and preferred.\n+#[derive(Copy, Clone, PartialEq, Eq, Hash, Debug)]\n+#[cfg_attr(feature = \"nightly\", derive(HashStable_Generic))]\n+\n+pub struct AbiAndPrefAlign {\n+    pub abi: Align,\n+    pub pref: Align,\n+}\n+\n+impl AbiAndPrefAlign {\n+    #[inline]\n+    pub fn new(align: Align) -> AbiAndPrefAlign {\n+        AbiAndPrefAlign { abi: align, pref: align }\n+    }\n+\n+    #[inline]\n+    pub fn min(self, other: AbiAndPrefAlign) -> AbiAndPrefAlign {\n+        AbiAndPrefAlign { abi: self.abi.min(other.abi), pref: self.pref.min(other.pref) }\n+    }\n+\n+    #[inline]\n+    pub fn max(self, other: AbiAndPrefAlign) -> AbiAndPrefAlign {\n+        AbiAndPrefAlign { abi: self.abi.max(other.abi), pref: self.pref.max(other.pref) }\n+    }\n+}\n+\n+/// Integers, also used for enum discriminants.\n+#[derive(Copy, Clone, PartialEq, Eq, PartialOrd, Ord, Hash, Debug)]\n+#[cfg_attr(feature = \"nightly\", derive(Encodable, Decodable, HashStable_Generic))]\n+\n+pub enum Integer {\n+    I8,\n+    I16,\n+    I32,\n+    I64,\n+    I128,\n+}\n+\n+impl Integer {\n+    #[inline]\n+    pub fn size(self) -> Size {\n+        match self {\n+            I8 => Size::from_bytes(1),\n+            I16 => Size::from_bytes(2),\n+            I32 => Size::from_bytes(4),\n+            I64 => Size::from_bytes(8),\n+            I128 => Size::from_bytes(16),\n+        }\n+    }\n+\n+    /// Gets the Integer type from an IntegerType.\n+    pub fn from_attr<C: HasDataLayout>(cx: &C, ity: IntegerType) -> Integer {\n+        let dl = cx.data_layout();\n+\n+        match ity {\n+            IntegerType::Pointer(_) => dl.ptr_sized_integer(),\n+            IntegerType::Fixed(x, _) => x,\n+        }\n+    }\n+\n+    pub fn align<C: HasDataLayout>(self, cx: &C) -> AbiAndPrefAlign {\n+        let dl = cx.data_layout();\n+\n+        match self {\n+            I8 => dl.i8_align,\n+            I16 => dl.i16_align,\n+            I32 => dl.i32_align,\n+            I64 => dl.i64_align,\n+            I128 => dl.i128_align,\n+        }\n+    }\n+\n+    /// Finds the smallest Integer type which can represent the signed value.\n+    #[inline]\n+    pub fn fit_signed(x: i128) -> Integer {\n+        match x {\n+            -0x0000_0000_0000_0080..=0x0000_0000_0000_007f => I8,\n+            -0x0000_0000_0000_8000..=0x0000_0000_0000_7fff => I16,\n+            -0x0000_0000_8000_0000..=0x0000_0000_7fff_ffff => I32,\n+            -0x8000_0000_0000_0000..=0x7fff_ffff_ffff_ffff => I64,\n+            _ => I128,\n+        }\n+    }\n+\n+    /// Finds the smallest Integer type which can represent the unsigned value.\n+    #[inline]\n+    pub fn fit_unsigned(x: u128) -> Integer {\n+        match x {\n+            0..=0x0000_0000_0000_00ff => I8,\n+            0..=0x0000_0000_0000_ffff => I16,\n+            0..=0x0000_0000_ffff_ffff => I32,\n+            0..=0xffff_ffff_ffff_ffff => I64,\n+            _ => I128,\n+        }\n+    }\n+\n+    /// Finds the smallest integer with the given alignment.\n+    pub fn for_align<C: HasDataLayout>(cx: &C, wanted: Align) -> Option<Integer> {\n+        let dl = cx.data_layout();\n+\n+        for candidate in [I8, I16, I32, I64, I128] {\n+            if wanted == candidate.align(dl).abi && wanted.bytes() == candidate.size().bytes() {\n+                return Some(candidate);\n+            }\n+        }\n+        None\n+    }\n+\n+    /// Find the largest integer with the given alignment or less.\n+    pub fn approximate_align<C: HasDataLayout>(cx: &C, wanted: Align) -> Integer {\n+        let dl = cx.data_layout();\n+\n+        // FIXME(eddyb) maybe include I128 in the future, when it works everywhere.\n+        for candidate in [I64, I32, I16] {\n+            if wanted >= candidate.align(dl).abi && wanted.bytes() >= candidate.size().bytes() {\n+                return candidate;\n+            }\n+        }\n+        I8\n+    }\n+\n+    // FIXME(eddyb) consolidate this and other methods that find the appropriate\n+    // `Integer` given some requirements.\n+    #[inline]\n+    pub fn from_size(size: Size) -> Result<Self, String> {\n+        match size.bits() {\n+            8 => Ok(Integer::I8),\n+            16 => Ok(Integer::I16),\n+            32 => Ok(Integer::I32),\n+            64 => Ok(Integer::I64),\n+            128 => Ok(Integer::I128),\n+            _ => Err(format!(\"rust does not support integers with {} bits\", size.bits())),\n+        }\n+    }\n+}\n+\n+/// Fundamental unit of memory access and layout.\n+#[derive(Copy, Clone, PartialEq, Eq, Hash, Debug)]\n+#[cfg_attr(feature = \"nightly\", derive(HashStable_Generic))]\n+pub enum Primitive {\n+    /// The `bool` is the signedness of the `Integer` type.\n+    ///\n+    /// One would think we would not care about such details this low down,\n+    /// but some ABIs are described in terms of C types and ISAs where the\n+    /// integer arithmetic is done on {sign,zero}-extended registers, e.g.\n+    /// a negative integer passed by zero-extension will appear positive in\n+    /// the callee, and most operations on it will produce the wrong values.\n+    Int(Integer, bool),\n+    F32,\n+    F64,\n+    Pointer,\n+}\n+\n+impl Primitive {\n+    pub fn size<C: HasDataLayout>(self, cx: &C) -> Size {\n+        let dl = cx.data_layout();\n+\n+        match self {\n+            Int(i, _) => i.size(),\n+            F32 => Size::from_bits(32),\n+            F64 => Size::from_bits(64),\n+            Pointer => dl.pointer_size,\n+        }\n+    }\n+\n+    pub fn align<C: HasDataLayout>(self, cx: &C) -> AbiAndPrefAlign {\n+        let dl = cx.data_layout();\n+\n+        match self {\n+            Int(i, _) => i.align(dl),\n+            F32 => dl.f32_align,\n+            F64 => dl.f64_align,\n+            Pointer => dl.pointer_align,\n+        }\n+    }\n+\n+    // FIXME(eddyb) remove, it's trivial thanks to `matches!`.\n+    #[inline]\n+    pub fn is_float(self) -> bool {\n+        matches!(self, F32 | F64)\n+    }\n+\n+    // FIXME(eddyb) remove, it's completely unused.\n+    #[inline]\n+    pub fn is_int(self) -> bool {\n+        matches!(self, Int(..))\n+    }\n+\n+    #[inline]\n+    pub fn is_ptr(self) -> bool {\n+        matches!(self, Pointer)\n+    }\n+}\n+\n+/// Inclusive wrap-around range of valid values, that is, if\n+/// start > end, it represents `start..=MAX`,\n+/// followed by `0..=end`.\n+///\n+/// That is, for an i8 primitive, a range of `254..=2` means following\n+/// sequence:\n+///\n+///    254 (-2), 255 (-1), 0, 1, 2\n+///\n+/// This is intended specifically to mirror LLVM\u2019s `!range` metadata semantics.\n+#[derive(Clone, Copy, PartialEq, Eq, Hash)]\n+#[cfg_attr(feature = \"nightly\", derive(HashStable_Generic))]\n+pub struct WrappingRange {\n+    pub start: u128,\n+    pub end: u128,\n+}\n+\n+impl WrappingRange {\n+    pub fn full(size: Size) -> Self {\n+        Self { start: 0, end: size.unsigned_int_max() }\n+    }\n+\n+    /// Returns `true` if `v` is contained in the range.\n+    #[inline(always)]\n+    pub fn contains(&self, v: u128) -> bool {\n+        if self.start <= self.end {\n+            self.start <= v && v <= self.end\n+        } else {\n+            self.start <= v || v <= self.end\n+        }\n+    }\n+\n+    /// Returns `self` with replaced `start`\n+    #[inline(always)]\n+    pub fn with_start(mut self, start: u128) -> Self {\n+        self.start = start;\n+        self\n+    }\n+\n+    /// Returns `self` with replaced `end`\n+    #[inline(always)]\n+    pub fn with_end(mut self, end: u128) -> Self {\n+        self.end = end;\n+        self\n+    }\n+\n+    /// Returns `true` if `size` completely fills the range.\n+    #[inline]\n+    pub fn is_full_for(&self, size: Size) -> bool {\n+        let max_value = size.unsigned_int_max();\n+        debug_assert!(self.start <= max_value && self.end <= max_value);\n+        self.start == (self.end.wrapping_add(1) & max_value)\n+    }\n+}\n+\n+impl fmt::Debug for WrappingRange {\n+    fn fmt(&self, fmt: &mut fmt::Formatter<'_>) -> fmt::Result {\n+        if self.start > self.end {\n+            write!(fmt, \"(..={}) | ({}..)\", self.end, self.start)?;\n+        } else {\n+            write!(fmt, \"{}..={}\", self.start, self.end)?;\n+        }\n+        Ok(())\n+    }\n+}\n+\n+/// Information about one scalar component of a Rust type.\n+#[derive(Clone, Copy, PartialEq, Eq, Hash, Debug)]\n+#[cfg_attr(feature = \"nightly\", derive(HashStable_Generic))]\n+pub enum Scalar {\n+    Initialized {\n+        value: Primitive,\n+\n+        // FIXME(eddyb) always use the shortest range, e.g., by finding\n+        // the largest space between two consecutive valid values and\n+        // taking everything else as the (shortest) valid range.\n+        valid_range: WrappingRange,\n+    },\n+    Union {\n+        /// Even for unions, we need to use the correct registers for the kind of\n+        /// values inside the union, so we keep the `Primitive` type around. We\n+        /// also use it to compute the size of the scalar.\n+        /// However, unions never have niches and even allow undef,\n+        /// so there is no `valid_range`.\n+        value: Primitive,\n+    },\n+}\n+\n+impl Scalar {\n+    #[inline]\n+    pub fn is_bool(&self) -> bool {\n+        matches!(\n+            self,\n+            Scalar::Initialized {\n+                value: Int(I8, false),\n+                valid_range: WrappingRange { start: 0, end: 1 }\n+            }\n+        )\n+    }\n+\n+    /// Get the primitive representation of this type, ignoring the valid range and whether the\n+    /// value is allowed to be undefined (due to being a union).\n+    pub fn primitive(&self) -> Primitive {\n+        match *self {\n+            Scalar::Initialized { value, .. } | Scalar::Union { value } => value,\n+        }\n+    }\n+\n+    pub fn align(self, cx: &impl HasDataLayout) -> AbiAndPrefAlign {\n+        self.primitive().align(cx)\n+    }\n+\n+    pub fn size(self, cx: &impl HasDataLayout) -> Size {\n+        self.primitive().size(cx)\n+    }\n+\n+    #[inline]\n+    pub fn to_union(&self) -> Self {\n+        Self::Union { value: self.primitive() }\n+    }\n+\n+    #[inline]\n+    pub fn valid_range(&self, cx: &impl HasDataLayout) -> WrappingRange {\n+        match *self {\n+            Scalar::Initialized { valid_range, .. } => valid_range,\n+            Scalar::Union { value } => WrappingRange::full(value.size(cx)),\n+        }\n+    }\n+\n+    #[inline]\n+    /// Allows the caller to mutate the valid range. This operation will panic if attempted on a union.\n+    pub fn valid_range_mut(&mut self) -> &mut WrappingRange {\n+        match self {\n+            Scalar::Initialized { valid_range, .. } => valid_range,\n+            Scalar::Union { .. } => panic!(\"cannot change the valid range of a union\"),\n+        }\n+    }\n+\n+    /// Returns `true` if all possible numbers are valid, i.e `valid_range` covers the whole layout\n+    #[inline]\n+    pub fn is_always_valid<C: HasDataLayout>(&self, cx: &C) -> bool {\n+        match *self {\n+            Scalar::Initialized { valid_range, .. } => valid_range.is_full_for(self.size(cx)),\n+            Scalar::Union { .. } => true,\n+        }\n+    }\n+\n+    /// Returns `true` if this type can be left uninit.\n+    #[inline]\n+    pub fn is_uninit_valid(&self) -> bool {\n+        match *self {\n+            Scalar::Initialized { .. } => false,\n+            Scalar::Union { .. } => true,\n+        }\n+    }\n+}\n+\n+/// Describes how the fields of a type are located in memory.\n+#[derive(PartialEq, Eq, Hash, Clone, Debug)]\n+#[cfg_attr(feature = \"nightly\", derive(HashStable_Generic))]\n+pub enum FieldsShape {\n+    /// Scalar primitives and `!`, which never have fields.\n+    Primitive,\n+\n+    /// All fields start at no offset. The `usize` is the field count.\n+    Union(NonZeroUsize),\n+\n+    /// Array/vector-like placement, with all fields of identical types.\n+    Array { stride: Size, count: u64 },\n+\n+    /// Struct-like placement, with precomputed offsets.\n+    ///\n+    /// Fields are guaranteed to not overlap, but note that gaps\n+    /// before, between and after all the fields are NOT always\n+    /// padding, and as such their contents may not be discarded.\n+    /// For example, enum variants leave a gap at the start,\n+    /// where the discriminant field in the enum layout goes.\n+    Arbitrary {\n+        /// Offsets for the first byte of each field,\n+        /// ordered to match the source definition order.\n+        /// This vector does not go in increasing order.\n+        // FIXME(eddyb) use small vector optimization for the common case.\n+        offsets: Vec<Size>,\n+\n+        /// Maps source order field indices to memory order indices,\n+        /// depending on how the fields were reordered (if at all).\n+        /// This is a permutation, with both the source order and the\n+        /// memory order using the same (0..n) index ranges.\n+        ///\n+        /// Note that during computation of `memory_index`, sometimes\n+        /// it is easier to operate on the inverse mapping (that is,\n+        /// from memory order to source order), and that is usually\n+        /// named `inverse_memory_index`.\n+        ///\n+        // FIXME(eddyb) build a better abstraction for permutations, if possible.\n+        // FIXME(camlorn) also consider small vector  optimization here.\n+        memory_index: Vec<u32>,\n+    },\n+}\n+\n+impl FieldsShape {\n+    #[inline]\n+    pub fn count(&self) -> usize {\n+        match *self {\n+            FieldsShape::Primitive => 0,\n+            FieldsShape::Union(count) => count.get(),\n+            FieldsShape::Array { count, .. } => count.try_into().unwrap(),\n+            FieldsShape::Arbitrary { ref offsets, .. } => offsets.len(),\n+        }\n+    }\n+\n+    #[inline]\n+    pub fn offset(&self, i: usize) -> Size {\n+        match *self {\n+            FieldsShape::Primitive => {\n+                unreachable!(\"FieldsShape::offset: `Primitive`s have no fields\")\n+            }\n+            FieldsShape::Union(count) => {\n+                assert!(\n+                    i < count.get(),\n+                    \"tried to access field {} of union with {} fields\",\n+                    i,\n+                    count\n+                );\n+                Size::ZERO\n+            }\n+            FieldsShape::Array { stride, count } => {\n+                let i = u64::try_from(i).unwrap();\n+                assert!(i < count);\n+                stride * i\n+            }\n+            FieldsShape::Arbitrary { ref offsets, .. } => offsets[i],\n+        }\n+    }\n+\n+    #[inline]\n+    pub fn memory_index(&self, i: usize) -> usize {\n+        match *self {\n+            FieldsShape::Primitive => {\n+                unreachable!(\"FieldsShape::memory_index: `Primitive`s have no fields\")\n+            }\n+            FieldsShape::Union(_) | FieldsShape::Array { .. } => i,\n+            FieldsShape::Arbitrary { ref memory_index, .. } => memory_index[i].try_into().unwrap(),\n+        }\n+    }\n+\n+    /// Gets source indices of the fields by increasing offsets.\n+    #[inline]\n+    pub fn index_by_increasing_offset<'a>(&'a self) -> impl Iterator<Item = usize> + 'a {\n+        let mut inverse_small = [0u8; 64];\n+        let mut inverse_big = vec![];\n+        let use_small = self.count() <= inverse_small.len();\n+\n+        // We have to write this logic twice in order to keep the array small.\n+        if let FieldsShape::Arbitrary { ref memory_index, .. } = *self {\n+            if use_small {\n+                for i in 0..self.count() {\n+                    inverse_small[memory_index[i] as usize] = i as u8;\n+                }\n+            } else {\n+                inverse_big = vec![0; self.count()];\n+                for i in 0..self.count() {\n+                    inverse_big[memory_index[i] as usize] = i as u32;\n+                }\n+            }\n+        }\n+\n+        (0..self.count()).map(move |i| match *self {\n+            FieldsShape::Primitive | FieldsShape::Union(_) | FieldsShape::Array { .. } => i,\n+            FieldsShape::Arbitrary { .. } => {\n+                if use_small {\n+                    inverse_small[i] as usize\n+                } else {\n+                    inverse_big[i] as usize\n+                }\n+            }\n+        })\n+    }\n+}\n+\n+/// An identifier that specifies the address space that some operation\n+/// should operate on. Special address spaces have an effect on code generation,\n+/// depending on the target and the address spaces it implements.\n+#[derive(Copy, Clone, Debug, PartialEq, Eq, PartialOrd, Ord)]\n+pub struct AddressSpace(pub u32);\n+\n+impl AddressSpace {\n+    /// The default address space, corresponding to data space.\n+    pub const DATA: Self = AddressSpace(0);\n+}\n+\n+/// Describes how values of the type are passed by target ABIs,\n+/// in terms of categories of C types there are ABI rules for.\n+#[derive(Clone, Copy, PartialEq, Eq, Hash, Debug)]\n+#[cfg_attr(feature = \"nightly\", derive(HashStable_Generic))]\n+\n+pub enum Abi {\n+    Uninhabited,\n+    Scalar(Scalar),\n+    ScalarPair(Scalar, Scalar),\n+    Vector {\n+        element: Scalar,\n+        count: u64,\n+    },\n+    Aggregate {\n+        /// If true, the size is exact, otherwise it's only a lower bound.\n+        sized: bool,\n+    },\n+}\n+\n+impl Abi {\n+    /// Returns `true` if the layout corresponds to an unsized type.\n+    #[inline]\n+    pub fn is_unsized(&self) -> bool {\n+        match *self {\n+            Abi::Uninhabited | Abi::Scalar(_) | Abi::ScalarPair(..) | Abi::Vector { .. } => false,\n+            Abi::Aggregate { sized } => !sized,\n+        }\n+    }\n+\n+    #[inline]\n+    pub fn is_sized(&self) -> bool {\n+        !self.is_unsized()\n+    }\n+\n+    /// Returns `true` if this is a single signed integer scalar\n+    #[inline]\n+    pub fn is_signed(&self) -> bool {\n+        match self {\n+            Abi::Scalar(scal) => match scal.primitive() {\n+                Primitive::Int(_, signed) => signed,\n+                _ => false,\n+            },\n+            _ => panic!(\"`is_signed` on non-scalar ABI {:?}\", self),\n+        }\n+    }\n+\n+    /// Returns `true` if this is an uninhabited type\n+    #[inline]\n+    pub fn is_uninhabited(&self) -> bool {\n+        matches!(*self, Abi::Uninhabited)\n+    }\n+\n+    /// Returns `true` is this is a scalar type\n+    #[inline]\n+    pub fn is_scalar(&self) -> bool {\n+        matches!(*self, Abi::Scalar(_))\n+    }\n+}\n+\n+#[derive(PartialEq, Eq, Hash, Clone, Debug)]\n+#[cfg_attr(feature = \"nightly\", derive(HashStable_Generic))]\n+pub enum Variants<V: Idx> {\n+    /// Single enum variants, structs/tuples, unions, and all non-ADTs.\n+    Single { index: V },\n+\n+    /// Enum-likes with more than one inhabited variant: each variant comes with\n+    /// a *discriminant* (usually the same as the variant index but the user can\n+    /// assign explicit discriminant values).  That discriminant is encoded\n+    /// as a *tag* on the machine.  The layout of each variant is\n+    /// a struct, and they all have space reserved for the tag.\n+    /// For enums, the tag is the sole field of the layout.\n+    Multiple {\n+        tag: Scalar,\n+        tag_encoding: TagEncoding<V>,\n+        tag_field: usize,\n+        variants: IndexVec<V, LayoutS<V>>,\n+    },\n+}\n+\n+#[derive(PartialEq, Eq, Hash, Clone, Debug)]\n+#[cfg_attr(feature = \"nightly\", derive(HashStable_Generic))]\n+pub enum TagEncoding<V: Idx> {\n+    /// The tag directly stores the discriminant, but possibly with a smaller layout\n+    /// (so converting the tag to the discriminant can require sign extension).\n+    Direct,\n+\n+    /// Niche (values invalid for a type) encoding the discriminant:\n+    /// Discriminant and variant index coincide.\n+    /// The variant `untagged_variant` contains a niche at an arbitrary\n+    /// offset (field `tag_field` of the enum), which for a variant with\n+    /// discriminant `d` is set to\n+    /// `(d - niche_variants.start).wrapping_add(niche_start)`.\n+    ///\n+    /// For example, `Option<(usize, &T)>`  is represented such that\n+    /// `None` has a null pointer for the second tuple field, and\n+    /// `Some` is the identity function (with a non-null reference).\n+    Niche { untagged_variant: V, niche_variants: RangeInclusive<V>, niche_start: u128 },\n+}\n+\n+#[derive(Clone, Copy, PartialEq, Eq, Hash, Debug)]\n+#[cfg_attr(feature = \"nightly\", derive(HashStable_Generic))]\n+pub struct Niche {\n+    pub offset: Size,\n+    pub value: Primitive,\n+    pub valid_range: WrappingRange,\n+}\n+\n+impl Niche {\n+    pub fn from_scalar<C: HasDataLayout>(cx: &C, offset: Size, scalar: Scalar) -> Option<Self> {\n+        let Scalar::Initialized { value, valid_range } = scalar else { return None };\n+        let niche = Niche { offset, value, valid_range };\n+        if niche.available(cx) > 0 { Some(niche) } else { None }\n+    }\n+\n+    pub fn available<C: HasDataLayout>(&self, cx: &C) -> u128 {\n+        let Self { value, valid_range: v, .. } = *self;\n+        let size = value.size(cx);\n+        assert!(size.bits() <= 128);\n+        let max_value = size.unsigned_int_max();\n+\n+        // Find out how many values are outside the valid range.\n+        let niche = v.end.wrapping_add(1)..v.start;\n+        niche.end.wrapping_sub(niche.start) & max_value\n+    }\n+\n+    pub fn reserve<C: HasDataLayout>(&self, cx: &C, count: u128) -> Option<(u128, Scalar)> {\n+        assert!(count > 0);\n+\n+        let Self { value, valid_range: v, .. } = *self;\n+        let size = value.size(cx);\n+        assert!(size.bits() <= 128);\n+        let max_value = size.unsigned_int_max();\n+\n+        let niche = v.end.wrapping_add(1)..v.start;\n+        let available = niche.end.wrapping_sub(niche.start) & max_value;\n+        if count > available {\n+            return None;\n+        }\n+\n+        // Extend the range of valid values being reserved by moving either `v.start` or `v.end` bound.\n+        // Given an eventual `Option<T>`, we try to maximize the chance for `None` to occupy the niche of zero.\n+        // This is accomplished by preferring enums with 2 variants(`count==1`) and always taking the shortest path to niche zero.\n+        // Having `None` in niche zero can enable some special optimizations.\n+        //\n+        // Bound selection criteria:\n+        // 1. Select closest to zero given wrapping semantics.\n+        // 2. Avoid moving past zero if possible.\n+        //\n+        // In practice this means that enums with `count > 1` are unlikely to claim niche zero, since they have to fit perfectly.\n+        // If niche zero is already reserved, the selection of bounds are of little interest.\n+        let move_start = |v: WrappingRange| {\n+            let start = v.start.wrapping_sub(count) & max_value;\n+            Some((start, Scalar::Initialized { value, valid_range: v.with_start(start) }))\n+        };\n+        let move_end = |v: WrappingRange| {\n+            let start = v.end.wrapping_add(1) & max_value;\n+            let end = v.end.wrapping_add(count) & max_value;\n+            Some((start, Scalar::Initialized { value, valid_range: v.with_end(end) }))\n+        };\n+        let distance_end_zero = max_value - v.end;\n+        if v.start > v.end {\n+            // zero is unavailable because wrapping occurs\n+            move_end(v)\n+        } else if v.start <= distance_end_zero {\n+            if count <= v.start {\n+                move_start(v)\n+            } else {\n+                // moved past zero, use other bound\n+                move_end(v)\n+            }\n+        } else {\n+            let end = v.end.wrapping_add(count) & max_value;\n+            let overshot_zero = (1..=v.end).contains(&end);\n+            if overshot_zero {\n+                // moved past zero, use other bound\n+                move_start(v)\n+            } else {\n+                move_end(v)\n+            }\n+        }\n+    }\n+}\n+\n+#[derive(PartialEq, Eq, Hash, Clone)]\n+#[cfg_attr(feature = \"nightly\", derive(HashStable_Generic))]\n+pub struct LayoutS<V: Idx> {\n+    /// Says where the fields are located within the layout.\n+    pub fields: FieldsShape,\n+\n+    /// Encodes information about multi-variant layouts.\n+    /// Even with `Multiple` variants, a layout still has its own fields! Those are then\n+    /// shared between all variants. One of them will be the discriminant,\n+    /// but e.g. generators can have more.\n+    ///\n+    /// To access all fields of this layout, both `fields` and the fields of the active variant\n+    /// must be taken into account.\n+    pub variants: Variants<V>,\n+\n+    /// The `abi` defines how this data is passed between functions, and it defines\n+    /// value restrictions via `valid_range`.\n+    ///\n+    /// Note that this is entirely orthogonal to the recursive structure defined by\n+    /// `variants` and `fields`; for example, `ManuallyDrop<Result<isize, isize>>` has\n+    /// `Abi::ScalarPair`! So, even with non-`Aggregate` `abi`, `fields` and `variants`\n+    /// have to be taken into account to find all fields of this layout.\n+    pub abi: Abi,\n+\n+    /// The leaf scalar with the largest number of invalid values\n+    /// (i.e. outside of its `valid_range`), if it exists.\n+    pub largest_niche: Option<Niche>,\n+\n+    pub align: AbiAndPrefAlign,\n+    pub size: Size,\n+}\n+\n+impl<V: Idx> LayoutS<V> {\n+    pub fn scalar<C: HasDataLayout>(cx: &C, scalar: Scalar) -> Self {\n+        let largest_niche = Niche::from_scalar(cx, Size::ZERO, scalar);\n+        let size = scalar.size(cx);\n+        let align = scalar.align(cx);\n+        LayoutS {\n+            variants: Variants::Single { index: V::new(0) },\n+            fields: FieldsShape::Primitive,\n+            abi: Abi::Scalar(scalar),\n+            largest_niche,\n+            size,\n+            align,\n+        }\n+    }\n+}\n+\n+impl<V: Idx> fmt::Debug for LayoutS<V> {\n+    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {\n+        // This is how `Layout` used to print before it become\n+        // `Interned<LayoutS>`. We print it like this to avoid having to update\n+        // expected output in a lot of tests.\n+        let LayoutS { size, align, abi, fields, largest_niche, variants } = self;\n+        f.debug_struct(\"Layout\")\n+            .field(\"size\", size)\n+            .field(\"align\", align)\n+            .field(\"abi\", abi)\n+            .field(\"fields\", fields)\n+            .field(\"largest_niche\", largest_niche)\n+            .field(\"variants\", variants)\n+            .finish()\n+    }\n+}\n+\n+#[derive(Copy, Clone, PartialEq, Eq, Debug)]\n+pub enum PointerKind {\n+    /// Most general case, we know no restrictions to tell LLVM.\n+    SharedMutable,\n+\n+    /// `&T` where `T` contains no `UnsafeCell`, is `dereferenceable`, `noalias` and `readonly`.\n+    Frozen,\n+\n+    /// `&mut T` which is `dereferenceable` and `noalias` but not `readonly`.\n+    UniqueBorrowed,\n+\n+    /// `&mut !Unpin`, which is `dereferenceable` but neither `noalias` nor `readonly`.\n+    UniqueBorrowedPinned,\n+\n+    /// `Box<T>`, which is `noalias` (even on return types, unlike the above) but neither `readonly`\n+    /// nor `dereferenceable`.\n+    UniqueOwned,\n+}\n+\n+#[derive(Copy, Clone, Debug)]\n+pub struct PointeeInfo {\n+    pub size: Size,\n+    pub align: Align,\n+    pub safe: Option<PointerKind>,\n+    pub address_space: AddressSpace,\n+}\n+\n+/// Used in `might_permit_raw_init` to indicate the kind of initialisation\n+/// that is checked to be valid\n+#[derive(Copy, Clone, Debug, PartialEq, Eq)]\n+pub enum InitKind {\n+    Zero,\n+    UninitMitigated0x01Fill,\n+}\n+\n+impl<V: Idx> LayoutS<V> {\n+    /// Returns `true` if the layout corresponds to an unsized type.\n+    pub fn is_unsized(&self) -> bool {\n+        self.abi.is_unsized()\n+    }\n+\n+    pub fn is_sized(&self) -> bool {\n+        self.abi.is_sized()\n+    }\n+\n+    /// Returns `true` if the type is a ZST and not unsized.\n+    pub fn is_zst(&self) -> bool {\n+        match self.abi {\n+            Abi::Scalar(_) | Abi::ScalarPair(..) | Abi::Vector { .. } => false,\n+            Abi::Uninhabited => self.size.bytes() == 0,\n+            Abi::Aggregate { sized } => sized && self.size.bytes() == 0,\n+        }\n+    }\n+}\n+\n+#[derive(Copy, Clone, Debug)]\n+pub enum StructKind {\n+    /// A tuple, closure, or univariant which cannot be coerced to unsized.\n+    AlwaysSized,\n+    /// A univariant, the last field of which may be coerced to unsized.\n+    MaybeUnsized,\n+    /// A univariant, but with a prefix of an arbitrary size & alignment (e.g., enum tag).\n+    Prefixed(Size, Align),\n+}"}, {"sha": "4c1d95a452d5e571c94229e4e11b064f56c4769c", "filename": "compiler/rustc_hir_analysis/src/collect.rs", "status": "modified", "additions": 1, "deletions": 2, "changes": 3, "blob_url": "https://github.com/rust-lang/rust/blob/b3bc6bf31265ac10946a0832092dbcedf9b26805/compiler%2Frustc_hir_analysis%2Fsrc%2Fcollect.rs", "raw_url": "https://github.com/rust-lang/rust/raw/b3bc6bf31265ac10946a0832092dbcedf9b26805/compiler%2Frustc_hir_analysis%2Fsrc%2Fcollect.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_hir_analysis%2Fsrc%2Fcollect.rs?ref=b3bc6bf31265ac10946a0832092dbcedf9b26805", "patch": "@@ -33,7 +33,6 @@ use rustc_middle::middle::codegen_fn_attrs::{CodegenFnAttrFlags, CodegenFnAttrs}\n use rustc_middle::mir::mono::Linkage;\n use rustc_middle::ty::query::Providers;\n use rustc_middle::ty::util::{Discr, IntTypeExt};\n-use rustc_middle::ty::ReprOptions;\n use rustc_middle::ty::{self, AdtKind, Const, DefIdTree, IsSuggestable, Ty, TyCtxt};\n use rustc_session::lint;\n use rustc_session::parse::feature_err;\n@@ -860,7 +859,7 @@ fn adt_def<'tcx>(tcx: TyCtxt<'tcx>, def_id: DefId) -> ty::AdtDef<'tcx> {\n         bug!();\n     };\n \n-    let repr = ReprOptions::new(tcx, def_id.to_def_id());\n+    let repr = tcx.repr_options_of_def(def_id.to_def_id());\n     let (kind, variants) = match item.kind {\n         ItemKind::Enum(ref def, _) => {\n             let mut distance_from_explicit = 0;"}, {"sha": "e1cda5a9edda30f8b8d4a3154bbe7b467650c66f", "filename": "compiler/rustc_index/Cargo.toml", "status": "modified", "additions": 6, "deletions": 2, "changes": 8, "blob_url": "https://github.com/rust-lang/rust/blob/b3bc6bf31265ac10946a0832092dbcedf9b26805/compiler%2Frustc_index%2FCargo.toml", "raw_url": "https://github.com/rust-lang/rust/raw/b3bc6bf31265ac10946a0832092dbcedf9b26805/compiler%2Frustc_index%2FCargo.toml", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_index%2FCargo.toml?ref=b3bc6bf31265ac10946a0832092dbcedf9b26805", "patch": "@@ -7,6 +7,10 @@ edition = \"2021\"\n \n [dependencies]\n arrayvec = { version = \"0.7\", default-features = false }\n-rustc_serialize = { path = \"../rustc_serialize\" }\n-rustc_macros = { path = \"../rustc_macros\" }\n+rustc_serialize = { path = \"../rustc_serialize\", optional = true }\n+rustc_macros = { path = \"../rustc_macros\", optional = true }\n smallvec = \"1.8.1\"\n+\n+[features]\n+default = [\"nightly\"]\n+nightly = [\"rustc_serialize\", \"rustc_macros\"]"}, {"sha": "db2c791525645ba156e3e9b3eff440c5d0eacbcd", "filename": "compiler/rustc_index/src/lib.rs", "status": "modified", "additions": 15, "deletions": 7, "changes": 22, "blob_url": "https://github.com/rust-lang/rust/blob/b3bc6bf31265ac10946a0832092dbcedf9b26805/compiler%2Frustc_index%2Fsrc%2Flib.rs", "raw_url": "https://github.com/rust-lang/rust/raw/b3bc6bf31265ac10946a0832092dbcedf9b26805/compiler%2Frustc_index%2Fsrc%2Flib.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_index%2Fsrc%2Flib.rs?ref=b3bc6bf31265ac10946a0832092dbcedf9b26805", "patch": "@@ -1,17 +1,25 @@\n #![deny(rustc::untranslatable_diagnostic)]\n #![deny(rustc::diagnostic_outside_of_impl)]\n-#![feature(allow_internal_unstable)]\n-#![feature(extend_one)]\n-#![feature(min_specialization)]\n-#![feature(new_uninit)]\n-#![feature(step_trait)]\n-#![feature(stmt_expr_attributes)]\n-#![feature(test)]\n+#![cfg_attr(\n+    feature = \"nightly\",\n+    feature(\n+        allow_internal_unstable,\n+        extend_one,\n+        min_specialization,\n+        new_uninit,\n+        step_trait,\n+        stmt_expr_attributes,\n+        test\n+    )\n+)]\n \n+#[cfg(feature = \"nightly\")]\n pub mod bit_set;\n+#[cfg(feature = \"nightly\")]\n pub mod interval;\n pub mod vec;\n \n+#[cfg(feature = \"rustc_macros\")]\n pub use rustc_macros::newtype_index;\n \n /// Type size assertion. The first argument is a type and the second argument is its expected size."}, {"sha": "39aa27a23c1d2ceeb62825d123893566f57729be", "filename": "compiler/rustc_index/src/vec.rs", "status": "modified", "additions": 5, "deletions": 0, "changes": 5, "blob_url": "https://github.com/rust-lang/rust/blob/b3bc6bf31265ac10946a0832092dbcedf9b26805/compiler%2Frustc_index%2Fsrc%2Fvec.rs", "raw_url": "https://github.com/rust-lang/rust/raw/b3bc6bf31265ac10946a0832092dbcedf9b26805/compiler%2Frustc_index%2Fsrc%2Fvec.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_index%2Fsrc%2Fvec.rs?ref=b3bc6bf31265ac10946a0832092dbcedf9b26805", "patch": "@@ -1,3 +1,4 @@\n+#[cfg(feature = \"rustc_serialize\")]\n use rustc_serialize::{Decodable, Decoder, Encodable, Encoder};\n \n use std::fmt;\n@@ -61,12 +62,14 @@ pub struct IndexVec<I: Idx, T> {\n // not the phantom data.\n unsafe impl<I: Idx, T> Send for IndexVec<I, T> where T: Send {}\n \n+#[cfg(feature = \"rustc_serialize\")]\n impl<S: Encoder, I: Idx, T: Encodable<S>> Encodable<S> for IndexVec<I, T> {\n     fn encode(&self, s: &mut S) {\n         Encodable::encode(&self.raw, s);\n     }\n }\n \n+#[cfg(feature = \"rustc_serialize\")]\n impl<D: Decoder, I: Idx, T: Decodable<D>> Decodable<D> for IndexVec<I, T> {\n     fn decode(d: &mut D) -> Self {\n         IndexVec { raw: Decodable::decode(d), _marker: PhantomData }\n@@ -359,11 +362,13 @@ impl<I: Idx, T> Extend<T> for IndexVec<I, T> {\n     }\n \n     #[inline]\n+    #[cfg(feature = \"nightly\")]\n     fn extend_one(&mut self, item: T) {\n         self.raw.push(item);\n     }\n \n     #[inline]\n+    #[cfg(feature = \"nightly\")]\n     fn extend_reserve(&mut self, additional: usize) {\n         self.raw.reserve(additional);\n     }"}, {"sha": "297b509d4023d40138c47233316a38c2e41270e6", "filename": "compiler/rustc_lint/src/types.rs", "status": "modified", "additions": 5, "deletions": 3, "changes": 8, "blob_url": "https://github.com/rust-lang/rust/blob/b3bc6bf31265ac10946a0832092dbcedf9b26805/compiler%2Frustc_lint%2Fsrc%2Ftypes.rs", "raw_url": "https://github.com/rust-lang/rust/raw/b3bc6bf31265ac10946a0832092dbcedf9b26805/compiler%2Frustc_lint%2Fsrc%2Ftypes.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_lint%2Fsrc%2Ftypes.rs?ref=b3bc6bf31265ac10946a0832092dbcedf9b26805", "patch": "@@ -12,7 +12,7 @@ use rustc_middle::ty::{self, AdtKind, DefIdTree, Ty, TyCtxt, TypeSuperVisitable,\n use rustc_span::source_map;\n use rustc_span::symbol::sym;\n use rustc_span::{Span, Symbol};\n-use rustc_target::abi::{Abi, WrappingRange};\n+use rustc_target::abi::{Abi, Size, WrappingRange};\n use rustc_target::abi::{Integer, TagEncoding, Variants};\n use rustc_target::spec::abi::Abi as SpecAbi;\n \n@@ -225,11 +225,11 @@ fn report_bin_hex_error(\n     cx: &LateContext<'_>,\n     expr: &hir::Expr<'_>,\n     ty: attr::IntType,\n+    size: Size,\n     repr_str: String,\n     val: u128,\n     negative: bool,\n ) {\n-    let size = Integer::from_attr(&cx.tcx, ty).size();\n     cx.struct_span_lint(\n         OVERFLOWING_LITERALS,\n         expr.span,\n@@ -352,6 +352,7 @@ fn lint_int_literal<'tcx>(\n                 cx,\n                 e,\n                 attr::IntType::SignedInt(ty::ast_int_ty(t)),\n+                Integer::from_int_ty(cx, t).size(),\n                 repr_str,\n                 v,\n                 negative,\n@@ -437,6 +438,7 @@ fn lint_uint_literal<'tcx>(\n                 cx,\n                 e,\n                 attr::IntType::UnsignedInt(ty::ast_uint_ty(t)),\n+                Integer::from_uint_ty(cx, t).size(),\n                 repr_str,\n                 lit_val,\n                 false,\n@@ -1376,7 +1378,7 @@ impl<'tcx> LateLintPass<'tcx> for VariantSizeDifferences {\n             let (largest, slargest, largest_index) = iter::zip(enum_definition.variants, variants)\n                 .map(|(variant, variant_layout)| {\n                     // Subtract the size of the enum tag.\n-                    let bytes = variant_layout.size().bytes().saturating_sub(tag_size);\n+                    let bytes = variant_layout.size.bytes().saturating_sub(tag_size);\n \n                     debug!(\"- variant `{}` is {} bytes large\", variant.ident, bytes);\n                     bytes"}, {"sha": "7bd4b6c0c2767b88da187bceef23f07bc02fe241", "filename": "compiler/rustc_middle/src/arena.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/b3bc6bf31265ac10946a0832092dbcedf9b26805/compiler%2Frustc_middle%2Fsrc%2Farena.rs", "raw_url": "https://github.com/rust-lang/rust/raw/b3bc6bf31265ac10946a0832092dbcedf9b26805/compiler%2Frustc_middle%2Fsrc%2Farena.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_middle%2Fsrc%2Farena.rs?ref=b3bc6bf31265ac10946a0832092dbcedf9b26805", "patch": "@@ -6,7 +6,7 @@\n macro_rules! arena_types {\n     ($macro:path) => (\n         $macro!([\n-            [] layout: rustc_target::abi::LayoutS<'tcx>,\n+            [] layout: rustc_target::abi::LayoutS<rustc_target::abi::VariantIdx>,\n             [] fn_abi: rustc_target::abi::call::FnAbi<'tcx, rustc_middle::ty::Ty<'tcx>>,\n             // AdtDef are interned and compared by address\n             [decode] adt_def: rustc_middle::ty::AdtDefData,"}, {"sha": "d3d667f68407fde123e0407e22ba4caa1d0a1095", "filename": "compiler/rustc_middle/src/ty/adt.rs", "status": "modified", "additions": 2, "deletions": 4, "changes": 6, "blob_url": "https://github.com/rust-lang/rust/blob/b3bc6bf31265ac10946a0832092dbcedf9b26805/compiler%2Frustc_middle%2Fsrc%2Fty%2Fadt.rs", "raw_url": "https://github.com/rust-lang/rust/raw/b3bc6bf31265ac10946a0832092dbcedf9b26805/compiler%2Frustc_middle%2Fsrc%2Fty%2Fadt.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_middle%2Fsrc%2Fty%2Fadt.rs?ref=b3bc6bf31265ac10946a0832092dbcedf9b26805", "patch": "@@ -14,17 +14,15 @@ use rustc_index::vec::{Idx, IndexVec};\n use rustc_query_system::ich::StableHashingContext;\n use rustc_session::DataTypeKind;\n use rustc_span::symbol::sym;\n-use rustc_target::abi::VariantIdx;\n+use rustc_target::abi::{ReprOptions, VariantIdx};\n \n use std::cell::RefCell;\n use std::cmp::Ordering;\n use std::hash::{Hash, Hasher};\n use std::ops::Range;\n use std::str;\n \n-use super::{\n-    Destructor, FieldDef, GenericPredicates, ReprOptions, Ty, TyCtxt, VariantDef, VariantDiscr,\n-};\n+use super::{Destructor, FieldDef, GenericPredicates, Ty, TyCtxt, VariantDef, VariantDiscr};\n \n bitflags! {\n     #[derive(HashStable, TyEncodable, TyDecodable)]"}, {"sha": "b5327ad0cecc634e699a351e8aba4405d08b8c1c", "filename": "compiler/rustc_middle/src/ty/context.rs", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "blob_url": "https://github.com/rust-lang/rust/blob/b3bc6bf31265ac10946a0832092dbcedf9b26805/compiler%2Frustc_middle%2Fsrc%2Fty%2Fcontext.rs", "raw_url": "https://github.com/rust-lang/rust/raw/b3bc6bf31265ac10946a0832092dbcedf9b26805/compiler%2Frustc_middle%2Fsrc%2Fty%2Fcontext.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_middle%2Fsrc%2Fty%2Fcontext.rs?ref=b3bc6bf31265ac10946a0832092dbcedf9b26805", "patch": "@@ -148,7 +148,7 @@ pub struct CtxtInterners<'tcx> {\n     const_: InternedSet<'tcx, ConstS<'tcx>>,\n     const_allocation: InternedSet<'tcx, Allocation>,\n     bound_variable_kinds: InternedSet<'tcx, List<ty::BoundVariableKind>>,\n-    layout: InternedSet<'tcx, LayoutS<'tcx>>,\n+    layout: InternedSet<'tcx, LayoutS<VariantIdx>>,\n     adt_def: InternedSet<'tcx, AdtDefData>,\n }\n \n@@ -1233,7 +1233,7 @@ impl<'tcx> TyCtxt<'tcx> {\n             global_ctxt: untracked_resolutions,\n             ast_lowering: untracked_resolver_for_lowering,\n         } = resolver_outputs;\n-        let data_layout = TargetDataLayout::parse(&s.target).unwrap_or_else(|err| {\n+        let data_layout = s.target.parse_data_layout().unwrap_or_else(|err| {\n             s.emit_fatal(err);\n         });\n         let interners = CtxtInterners::new(arena);\n@@ -2244,7 +2244,7 @@ direct_interners! {\n     region: mk_region(RegionKind<'tcx>): Region -> Region<'tcx>,\n     const_: mk_const_internal(ConstS<'tcx>): Const -> Const<'tcx>,\n     const_allocation: intern_const_alloc(Allocation): ConstAllocation -> ConstAllocation<'tcx>,\n-    layout: intern_layout(LayoutS<'tcx>): Layout -> Layout<'tcx>,\n+    layout: intern_layout(LayoutS<VariantIdx>): Layout -> Layout<'tcx>,\n     adt_def: intern_adt_def(AdtDefData): AdtDef -> AdtDef<'tcx>,\n }\n "}, {"sha": "488fd567846a3a850f973c7a7f3328ae21f28694", "filename": "compiler/rustc_middle/src/ty/layout.rs", "status": "modified", "additions": 13, "deletions": 20, "changes": 33, "blob_url": "https://github.com/rust-lang/rust/blob/b3bc6bf31265ac10946a0832092dbcedf9b26805/compiler%2Frustc_middle%2Fsrc%2Fty%2Flayout.rs", "raw_url": "https://github.com/rust-lang/rust/raw/b3bc6bf31265ac10946a0832092dbcedf9b26805/compiler%2Frustc_middle%2Fsrc%2Fty%2Flayout.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_middle%2Fsrc%2Fty%2Flayout.rs?ref=b3bc6bf31265ac10946a0832092dbcedf9b26805", "patch": "@@ -1,8 +1,6 @@\n use crate::middle::codegen_fn_attrs::CodegenFnAttrFlags;\n use crate::ty::normalize_erasing_regions::NormalizationError;\n use crate::ty::{self, ReprOptions, Ty, TyCtxt, TypeVisitable};\n-use rustc_ast as ast;\n-use rustc_attr as attr;\n use rustc_errors::{DiagnosticBuilder, Handler, IntoDiagnostic};\n use rustc_hir as hir;\n use rustc_hir::def_id::DefId;\n@@ -20,7 +18,6 @@ use std::ops::Bound;\n \n pub trait IntegerExt {\n     fn to_ty<'tcx>(&self, tcx: TyCtxt<'tcx>, signed: bool) -> Ty<'tcx>;\n-    fn from_attr<C: HasDataLayout>(cx: &C, ity: attr::IntType) -> Integer;\n     fn from_int_ty<C: HasDataLayout>(cx: &C, ity: ty::IntTy) -> Integer;\n     fn from_uint_ty<C: HasDataLayout>(cx: &C, uty: ty::UintTy) -> Integer;\n     fn repr_discr<'tcx>(\n@@ -49,22 +46,6 @@ impl IntegerExt for Integer {\n         }\n     }\n \n-    /// Gets the Integer type from an attr::IntType.\n-    fn from_attr<C: HasDataLayout>(cx: &C, ity: attr::IntType) -> Integer {\n-        let dl = cx.data_layout();\n-\n-        match ity {\n-            attr::SignedInt(ast::IntTy::I8) | attr::UnsignedInt(ast::UintTy::U8) => I8,\n-            attr::SignedInt(ast::IntTy::I16) | attr::UnsignedInt(ast::UintTy::U16) => I16,\n-            attr::SignedInt(ast::IntTy::I32) | attr::UnsignedInt(ast::UintTy::U32) => I32,\n-            attr::SignedInt(ast::IntTy::I64) | attr::UnsignedInt(ast::UintTy::U64) => I64,\n-            attr::SignedInt(ast::IntTy::I128) | attr::UnsignedInt(ast::UintTy::U128) => I128,\n-            attr::SignedInt(ast::IntTy::Isize) | attr::UnsignedInt(ast::UintTy::Usize) => {\n-                dl.ptr_sized_integer()\n-            }\n-        }\n-    }\n-\n     fn from_int_ty<C: HasDataLayout>(cx: &C, ity: ty::IntTy) -> Integer {\n         match ity {\n             ty::IntTy::I8 => I8,\n@@ -237,6 +218,18 @@ pub struct LayoutCx<'tcx, C> {\n     pub param_env: ty::ParamEnv<'tcx>,\n }\n \n+impl<'tcx> LayoutCalculator for LayoutCx<'tcx, TyCtxt<'tcx>> {\n+    type TargetDataLayoutRef = &'tcx TargetDataLayout;\n+\n+    fn delay_bug(&self, txt: &str) {\n+        self.tcx.sess.delay_span_bug(DUMMY_SP, txt);\n+    }\n+\n+    fn current_data_layout(&self) -> Self::TargetDataLayoutRef {\n+        &self.tcx.data_layout\n+    }\n+}\n+\n /// Type size \"skeleton\", i.e., the only information determining a type's size.\n /// While this is conservative, (aside from constant sizes, only pointers,\n /// newtypes thereof and null pointer optimized enums are allowed), it is\n@@ -610,7 +603,7 @@ where\n                 })\n             }\n \n-            Variants::Multiple { ref variants, .. } => variants[variant_index],\n+            Variants::Multiple { ref variants, .. } => cx.tcx().intern_layout(variants[variant_index].clone()),\n         };\n \n         assert_eq!(*layout.variants(), Variants::Single { index: variant_index });"}, {"sha": "9d778ff2fb6e3a2dd2907dcb81e2b26a54c0e0db", "filename": "compiler/rustc_middle/src/ty/mod.rs", "status": "modified", "additions": 77, "deletions": 158, "changes": 235, "blob_url": "https://github.com/rust-lang/rust/blob/b3bc6bf31265ac10946a0832092dbcedf9b26805/compiler%2Frustc_middle%2Fsrc%2Fty%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/b3bc6bf31265ac10946a0832092dbcedf9b26805/compiler%2Frustc_middle%2Fsrc%2Fty%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_middle%2Fsrc%2Fty%2Fmod.rs?ref=b3bc6bf31265ac10946a0832092dbcedf9b26805", "patch": "@@ -48,7 +48,8 @@ use rustc_session::cstore::CrateStoreDyn;\n use rustc_span::hygiene::MacroKind;\n use rustc_span::symbol::{kw, sym, Ident, Symbol};\n use rustc_span::{ExpnId, Span};\n-use rustc_target::abi::{Align, VariantIdx};\n+use rustc_target::abi::{Align, Integer, IntegerType, VariantIdx};\n+pub use rustc_target::abi::{ReprFlags, ReprOptions};\n pub use subst::*;\n pub use vtable::*;\n \n@@ -1994,163 +1995,6 @@ impl Hash for FieldDef {\n     }\n }\n \n-bitflags! {\n-    #[derive(TyEncodable, TyDecodable, Default, HashStable)]\n-    pub struct ReprFlags: u8 {\n-        const IS_C               = 1 << 0;\n-        const IS_SIMD            = 1 << 1;\n-        const IS_TRANSPARENT     = 1 << 2;\n-        // Internal only for now. If true, don't reorder fields.\n-        const IS_LINEAR          = 1 << 3;\n-        // If true, the type's layout can be randomized using\n-        // the seed stored in `ReprOptions.layout_seed`\n-        const RANDOMIZE_LAYOUT   = 1 << 4;\n-        // Any of these flags being set prevent field reordering optimisation.\n-        const IS_UNOPTIMISABLE   = ReprFlags::IS_C.bits\n-                                 | ReprFlags::IS_SIMD.bits\n-                                 | ReprFlags::IS_LINEAR.bits;\n-    }\n-}\n-\n-/// Represents the repr options provided by the user,\n-#[derive(Copy, Clone, Debug, Eq, PartialEq, TyEncodable, TyDecodable, Default, HashStable)]\n-pub struct ReprOptions {\n-    pub int: Option<attr::IntType>,\n-    pub align: Option<Align>,\n-    pub pack: Option<Align>,\n-    pub flags: ReprFlags,\n-    /// The seed to be used for randomizing a type's layout\n-    ///\n-    /// Note: This could technically be a `[u8; 16]` (a `u128`) which would\n-    /// be the \"most accurate\" hash as it'd encompass the item and crate\n-    /// hash without loss, but it does pay the price of being larger.\n-    /// Everything's a tradeoff, a `u64` seed should be sufficient for our\n-    /// purposes (primarily `-Z randomize-layout`)\n-    pub field_shuffle_seed: u64,\n-}\n-\n-impl ReprOptions {\n-    pub fn new(tcx: TyCtxt<'_>, did: DefId) -> ReprOptions {\n-        let mut flags = ReprFlags::empty();\n-        let mut size = None;\n-        let mut max_align: Option<Align> = None;\n-        let mut min_pack: Option<Align> = None;\n-\n-        // Generate a deterministically-derived seed from the item's path hash\n-        // to allow for cross-crate compilation to actually work\n-        let mut field_shuffle_seed = tcx.def_path_hash(did).0.to_smaller_hash();\n-\n-        // If the user defined a custom seed for layout randomization, xor the item's\n-        // path hash with the user defined seed, this will allowing determinism while\n-        // still allowing users to further randomize layout generation for e.g. fuzzing\n-        if let Some(user_seed) = tcx.sess.opts.unstable_opts.layout_seed {\n-            field_shuffle_seed ^= user_seed;\n-        }\n-\n-        for attr in tcx.get_attrs(did, sym::repr) {\n-            for r in attr::parse_repr_attr(&tcx.sess, attr) {\n-                flags.insert(match r {\n-                    attr::ReprC => ReprFlags::IS_C,\n-                    attr::ReprPacked(pack) => {\n-                        let pack = Align::from_bytes(pack as u64).unwrap();\n-                        min_pack = Some(if let Some(min_pack) = min_pack {\n-                            min_pack.min(pack)\n-                        } else {\n-                            pack\n-                        });\n-                        ReprFlags::empty()\n-                    }\n-                    attr::ReprTransparent => ReprFlags::IS_TRANSPARENT,\n-                    attr::ReprSimd => ReprFlags::IS_SIMD,\n-                    attr::ReprInt(i) => {\n-                        size = Some(i);\n-                        ReprFlags::empty()\n-                    }\n-                    attr::ReprAlign(align) => {\n-                        max_align = max_align.max(Some(Align::from_bytes(align as u64).unwrap()));\n-                        ReprFlags::empty()\n-                    }\n-                });\n-            }\n-        }\n-\n-        // If `-Z randomize-layout` was enabled for the type definition then we can\n-        // consider performing layout randomization\n-        if tcx.sess.opts.unstable_opts.randomize_layout {\n-            flags.insert(ReprFlags::RANDOMIZE_LAYOUT);\n-        }\n-\n-        // This is here instead of layout because the choice must make it into metadata.\n-        if !tcx.consider_optimizing(|| format!(\"Reorder fields of {:?}\", tcx.def_path_str(did))) {\n-            flags.insert(ReprFlags::IS_LINEAR);\n-        }\n-\n-        Self { int: size, align: max_align, pack: min_pack, flags, field_shuffle_seed }\n-    }\n-\n-    #[inline]\n-    pub fn simd(&self) -> bool {\n-        self.flags.contains(ReprFlags::IS_SIMD)\n-    }\n-\n-    #[inline]\n-    pub fn c(&self) -> bool {\n-        self.flags.contains(ReprFlags::IS_C)\n-    }\n-\n-    #[inline]\n-    pub fn packed(&self) -> bool {\n-        self.pack.is_some()\n-    }\n-\n-    #[inline]\n-    pub fn transparent(&self) -> bool {\n-        self.flags.contains(ReprFlags::IS_TRANSPARENT)\n-    }\n-\n-    #[inline]\n-    pub fn linear(&self) -> bool {\n-        self.flags.contains(ReprFlags::IS_LINEAR)\n-    }\n-\n-    /// Returns the discriminant type, given these `repr` options.\n-    /// This must only be called on enums!\n-    pub fn discr_type(&self) -> attr::IntType {\n-        self.int.unwrap_or(attr::SignedInt(ast::IntTy::Isize))\n-    }\n-\n-    /// Returns `true` if this `#[repr()]` should inhabit \"smart enum\n-    /// layout\" optimizations, such as representing `Foo<&T>` as a\n-    /// single pointer.\n-    pub fn inhibit_enum_layout_opt(&self) -> bool {\n-        self.c() || self.int.is_some()\n-    }\n-\n-    /// Returns `true` if this `#[repr()]` should inhibit struct field reordering\n-    /// optimizations, such as with `repr(C)`, `repr(packed(1))`, or `repr(<int>)`.\n-    pub fn inhibit_struct_field_reordering_opt(&self) -> bool {\n-        if let Some(pack) = self.pack {\n-            if pack.bytes() == 1 {\n-                return true;\n-            }\n-        }\n-\n-        self.flags.intersects(ReprFlags::IS_UNOPTIMISABLE) || self.int.is_some()\n-    }\n-\n-    /// Returns `true` if this type is valid for reordering and `-Z randomize-layout`\n-    /// was enabled for its declaration crate\n-    pub fn can_randomize_type_layout(&self) -> bool {\n-        !self.inhibit_struct_field_reordering_opt()\n-            && self.flags.contains(ReprFlags::RANDOMIZE_LAYOUT)\n-    }\n-\n-    /// Returns `true` if this `#[repr()]` should inhibit union ABI optimisations.\n-    pub fn inhibit_union_abi_opt(&self) -> bool {\n-        self.c()\n-    }\n-}\n-\n impl<'tcx> FieldDef {\n     /// Returns the type of this field. The resulting type is not normalized. The `subst` is\n     /// typically obtained via the second field of [`TyKind::Adt`].\n@@ -2218,6 +2062,81 @@ impl<'tcx> TyCtxt<'tcx> {\n             .filter(move |item| item.kind == AssocKind::Fn && item.defaultness(self).has_value())\n     }\n \n+    pub fn repr_options_of_def(self, did: DefId) -> ReprOptions {\n+        let mut flags = ReprFlags::empty();\n+        let mut size = None;\n+        let mut max_align: Option<Align> = None;\n+        let mut min_pack: Option<Align> = None;\n+\n+        // Generate a deterministically-derived seed from the item's path hash\n+        // to allow for cross-crate compilation to actually work\n+        let mut field_shuffle_seed = self.def_path_hash(did).0.to_smaller_hash();\n+\n+        // If the user defined a custom seed for layout randomization, xor the item's\n+        // path hash with the user defined seed, this will allowing determinism while\n+        // still allowing users to further randomize layout generation for e.g. fuzzing\n+        if let Some(user_seed) = self.sess.opts.unstable_opts.layout_seed {\n+            field_shuffle_seed ^= user_seed;\n+        }\n+\n+        for attr in self.get_attrs(did, sym::repr) {\n+            for r in attr::parse_repr_attr(&self.sess, attr) {\n+                flags.insert(match r {\n+                    attr::ReprC => ReprFlags::IS_C,\n+                    attr::ReprPacked(pack) => {\n+                        let pack = Align::from_bytes(pack as u64).unwrap();\n+                        min_pack = Some(if let Some(min_pack) = min_pack {\n+                            min_pack.min(pack)\n+                        } else {\n+                            pack\n+                        });\n+                        ReprFlags::empty()\n+                    }\n+                    attr::ReprTransparent => ReprFlags::IS_TRANSPARENT,\n+                    attr::ReprSimd => ReprFlags::IS_SIMD,\n+                    attr::ReprInt(i) => {\n+                        size = Some(match i {\n+                            attr::IntType::SignedInt(x) => match x {\n+                                ast::IntTy::Isize => IntegerType::Pointer(true),\n+                                ast::IntTy::I8 => IntegerType::Fixed(Integer::I8, true),\n+                                ast::IntTy::I16 => IntegerType::Fixed(Integer::I16, true),\n+                                ast::IntTy::I32 => IntegerType::Fixed(Integer::I32, true),\n+                                ast::IntTy::I64 => IntegerType::Fixed(Integer::I64, true),\n+                                ast::IntTy::I128 => IntegerType::Fixed(Integer::I128, true),\n+                            },\n+                            attr::IntType::UnsignedInt(x) => match x {\n+                                ast::UintTy::Usize => IntegerType::Pointer(false),\n+                                ast::UintTy::U8 => IntegerType::Fixed(Integer::I8, false),\n+                                ast::UintTy::U16 => IntegerType::Fixed(Integer::I16, false),\n+                                ast::UintTy::U32 => IntegerType::Fixed(Integer::I32, false),\n+                                ast::UintTy::U64 => IntegerType::Fixed(Integer::I64, false),\n+                                ast::UintTy::U128 => IntegerType::Fixed(Integer::I128, false),\n+                            },\n+                        });\n+                        ReprFlags::empty()\n+                    }\n+                    attr::ReprAlign(align) => {\n+                        max_align = max_align.max(Some(Align::from_bytes(align as u64).unwrap()));\n+                        ReprFlags::empty()\n+                    }\n+                });\n+            }\n+        }\n+\n+        // If `-Z randomize-layout` was enabled for the type definition then we can\n+        // consider performing layout randomization\n+        if self.sess.opts.unstable_opts.randomize_layout {\n+            flags.insert(ReprFlags::RANDOMIZE_LAYOUT);\n+        }\n+\n+        // This is here instead of layout because the choice must make it into metadata.\n+        if !self.consider_optimizing(|| format!(\"Reorder fields of {:?}\", self.def_path_str(did))) {\n+            flags.insert(ReprFlags::IS_LINEAR);\n+        }\n+\n+        ReprOptions { int: size, align: max_align, pack: min_pack, flags, field_shuffle_seed }\n+    }\n+\n     /// Look up the name of a definition across crates. This does not look at HIR.\n     pub fn opt_item_name(self, def_id: DefId) -> Option<Symbol> {\n         if let Some(cnum) = def_id.as_crate_root() {"}, {"sha": "6561c4c278d0ee1032560d1ba7ae50341dfe7444", "filename": "compiler/rustc_middle/src/ty/util.rs", "status": "modified", "additions": 6, "deletions": 17, "changes": 23, "blob_url": "https://github.com/rust-lang/rust/blob/b3bc6bf31265ac10946a0832092dbcedf9b26805/compiler%2Frustc_middle%2Fsrc%2Fty%2Futil.rs", "raw_url": "https://github.com/rust-lang/rust/raw/b3bc6bf31265ac10946a0832092dbcedf9b26805/compiler%2Frustc_middle%2Fsrc%2Fty%2Futil.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_middle%2Fsrc%2Fty%2Futil.rs?ref=b3bc6bf31265ac10946a0832092dbcedf9b26805", "patch": "@@ -8,8 +8,6 @@ use crate::ty::{\n };\n use crate::ty::{GenericArgKind, SubstsRef};\n use rustc_apfloat::Float as _;\n-use rustc_ast as ast;\n-use rustc_attr::{self as attr, SignedInt, UnsignedInt};\n use rustc_data_structures::fx::{FxHashMap, FxHashSet};\n use rustc_data_structures::stable_hasher::{HashStable, StableHasher};\n use rustc_errors::ErrorGuaranteed;\n@@ -19,7 +17,7 @@ use rustc_hir::def_id::DefId;\n use rustc_index::bit_set::GrowableBitSet;\n use rustc_macros::HashStable;\n use rustc_span::{sym, DUMMY_SP};\n-use rustc_target::abi::{Integer, Size, TargetDataLayout};\n+use rustc_target::abi::{Integer, IntegerType, Size, TargetDataLayout};\n use rustc_target::spec::abi::Abi;\n use smallvec::SmallVec;\n use std::{fmt, iter};\n@@ -104,21 +102,12 @@ pub trait IntTypeExt {\n     fn initial_discriminant<'tcx>(&self, tcx: TyCtxt<'tcx>) -> Discr<'tcx>;\n }\n \n-impl IntTypeExt for attr::IntType {\n+impl IntTypeExt for IntegerType {\n     fn to_ty<'tcx>(&self, tcx: TyCtxt<'tcx>) -> Ty<'tcx> {\n-        match *self {\n-            SignedInt(ast::IntTy::I8) => tcx.types.i8,\n-            SignedInt(ast::IntTy::I16) => tcx.types.i16,\n-            SignedInt(ast::IntTy::I32) => tcx.types.i32,\n-            SignedInt(ast::IntTy::I64) => tcx.types.i64,\n-            SignedInt(ast::IntTy::I128) => tcx.types.i128,\n-            SignedInt(ast::IntTy::Isize) => tcx.types.isize,\n-            UnsignedInt(ast::UintTy::U8) => tcx.types.u8,\n-            UnsignedInt(ast::UintTy::U16) => tcx.types.u16,\n-            UnsignedInt(ast::UintTy::U32) => tcx.types.u32,\n-            UnsignedInt(ast::UintTy::U64) => tcx.types.u64,\n-            UnsignedInt(ast::UintTy::U128) => tcx.types.u128,\n-            UnsignedInt(ast::UintTy::Usize) => tcx.types.usize,\n+        match self {\n+            IntegerType::Pointer(true) => tcx.types.isize,\n+            IntegerType::Pointer(false) => tcx.types.usize,\n+            IntegerType::Fixed(i, s) => i.to_ty(tcx, *s),\n         }\n     }\n "}, {"sha": "be0aa0fc4c1d05fcf9f3b5e9751c41e2bb254322", "filename": "compiler/rustc_mir_transform/src/uninhabited_enum_branching.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/b3bc6bf31265ac10946a0832092dbcedf9b26805/compiler%2Frustc_mir_transform%2Fsrc%2Funinhabited_enum_branching.rs", "raw_url": "https://github.com/rust-lang/rust/raw/b3bc6bf31265ac10946a0832092dbcedf9b26805/compiler%2Frustc_mir_transform%2Fsrc%2Funinhabited_enum_branching.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_mir_transform%2Fsrc%2Funinhabited_enum_branching.rs?ref=b3bc6bf31265ac10946a0832092dbcedf9b26805", "patch": "@@ -65,7 +65,7 @@ fn variant_discriminants<'tcx>(\n         Variants::Multiple { variants, .. } => variants\n             .iter_enumerated()\n             .filter_map(|(idx, layout)| {\n-                (layout.abi() != Abi::Uninhabited)\n+                (layout.abi != Abi::Uninhabited)\n                     .then(|| ty.discriminant_for_variant(tcx, idx).unwrap().val)\n             })\n             .collect(),"}, {"sha": "3b1b33aa095a1a3b7f9c526e888d8c078864d74c", "filename": "compiler/rustc_session/src/config.rs", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/rust-lang/rust/blob/b3bc6bf31265ac10946a0832092dbcedf9b26805/compiler%2Frustc_session%2Fsrc%2Fconfig.rs", "raw_url": "https://github.com/rust-lang/rust/raw/b3bc6bf31265ac10946a0832092dbcedf9b26805/compiler%2Frustc_session%2Fsrc%2Fconfig.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_session%2Fsrc%2Fconfig.rs?ref=b3bc6bf31265ac10946a0832092dbcedf9b26805", "patch": "@@ -11,7 +11,7 @@ use crate::{lint, HashStableContext};\n use rustc_data_structures::fx::{FxHashMap, FxHashSet};\n \n use rustc_data_structures::stable_hasher::ToStableHashKey;\n-use rustc_target::abi::{Align, TargetDataLayout};\n+use rustc_target::abi::Align;\n use rustc_target::spec::{PanicStrategy, SanitizerSet, SplitDebuginfo};\n use rustc_target::spec::{Target, TargetTriple, TargetWarnings, TARGETS};\n \n@@ -900,7 +900,7 @@ fn default_configuration(sess: &Session) -> CrateConfig {\n     let min_atomic_width = sess.target.min_atomic_width();\n     let max_atomic_width = sess.target.max_atomic_width();\n     let atomic_cas = sess.target.atomic_cas;\n-    let layout = TargetDataLayout::parse(&sess.target).unwrap_or_else(|err| {\n+    let layout = sess.target.parse_data_layout().unwrap_or_else(|err| {\n         sess.emit_fatal(err);\n     });\n "}, {"sha": "568c916a163284b3db17645d62338c55c20cef12", "filename": "compiler/rustc_target/Cargo.toml", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "blob_url": "https://github.com/rust-lang/rust/blob/b3bc6bf31265ac10946a0832092dbcedf9b26805/compiler%2Frustc_target%2FCargo.toml", "raw_url": "https://github.com/rust-lang/rust/raw/b3bc6bf31265ac10946a0832092dbcedf9b26805/compiler%2Frustc_target%2FCargo.toml", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_target%2FCargo.toml?ref=b3bc6bf31265ac10946a0832092dbcedf9b26805", "patch": "@@ -7,6 +7,7 @@ edition = \"2021\"\n bitflags = \"1.2.1\"\n tracing = \"0.1\"\n serde_json = \"1.0.59\"\n+rustc_abi = { path = \"../rustc_abi\" }\n rustc_data_structures = { path = \"../rustc_data_structures\" }\n rustc_feature = { path = \"../rustc_feature\" }\n rustc_index = { path = \"../rustc_index\" }"}, {"sha": "a5ffaebea0b98cb76986ac4a1bfc2530d2035d3c", "filename": "compiler/rustc_target/src/abi/call/mod.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/b3bc6bf31265ac10946a0832092dbcedf9b26805/compiler%2Frustc_target%2Fsrc%2Fabi%2Fcall%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/b3bc6bf31265ac10946a0832092dbcedf9b26805/compiler%2Frustc_target%2Fsrc%2Fabi%2Fcall%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_target%2Fsrc%2Fabi%2Fcall%2Fmod.rs?ref=b3bc6bf31265ac10946a0832092dbcedf9b26805", "patch": "@@ -262,7 +262,7 @@ impl CastTarget {\n         let mut size = self.rest.total;\n         for i in 0..self.prefix.iter().count() {\n             match self.prefix[i] {\n-                Some(v) => size += Size { raw: v.size.bytes() },\n+                Some(v) => size += v.size,\n                 None => {}\n             }\n         }"}, {"sha": "ec8f20fe692166fe30c5d7fa23e0d1015ffd4802", "filename": "compiler/rustc_target/src/abi/call/sparc64.rs", "status": "modified", "additions": 7, "deletions": 7, "changes": 14, "blob_url": "https://github.com/rust-lang/rust/blob/b3bc6bf31265ac10946a0832092dbcedf9b26805/compiler%2Frustc_target%2Fsrc%2Fabi%2Fcall%2Fsparc64.rs", "raw_url": "https://github.com/rust-lang/rust/raw/b3bc6bf31265ac10946a0832092dbcedf9b26805/compiler%2Frustc_target%2Fsrc%2Fabi%2Fcall%2Fsparc64.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_target%2Fsrc%2Fabi%2Fcall%2Fsparc64.rs?ref=b3bc6bf31265ac10946a0832092dbcedf9b26805", "patch": "@@ -87,8 +87,8 @@ where\n         _ => {}\n     }\n \n-    if (offset.raw % 4) != 0 && scalar2.primitive().is_float() {\n-        offset.raw += 4 - (offset.raw % 4);\n+    if (offset.bytes() % 4) != 0 && scalar2.primitive().is_float() {\n+        offset += Size::from_bytes(4 - (offset.bytes() % 4));\n     }\n     data = arg_scalar(cx, &scalar2, offset, data);\n     return data;\n@@ -169,14 +169,14 @@ where\n                     has_float: false,\n                     arg_attribute: ArgAttribute::default(),\n                 },\n-                Size { raw: 0 },\n+                Size::ZERO,\n             );\n \n             if data.has_float {\n                 // Structure { float, int, int } doesn't like to be handled like\n                 // { float, long int }. Other way around it doesn't mind.\n                 if data.last_offset < arg.layout.size\n-                    && (data.last_offset.raw % 8) != 0\n+                    && (data.last_offset.bytes() % 8) != 0\n                     && data.prefix_index < data.prefix.len()\n                 {\n                     data.prefix[data.prefix_index] = Some(Reg::i32());\n@@ -185,7 +185,7 @@ where\n                 }\n \n                 let mut rest_size = arg.layout.size - data.last_offset;\n-                if (rest_size.raw % 8) != 0 && data.prefix_index < data.prefix.len() {\n+                if (rest_size.bytes() % 8) != 0 && data.prefix_index < data.prefix.len() {\n                     data.prefix[data.prefix_index] = Some(Reg::i32());\n                     rest_size = rest_size - Reg::i32().size;\n                 }\n@@ -214,13 +214,13 @@ where\n     C: HasDataLayout,\n {\n     if !fn_abi.ret.is_ignore() {\n-        classify_arg(cx, &mut fn_abi.ret, Size { raw: 32 });\n+        classify_arg(cx, &mut fn_abi.ret, Size::from_bytes(32));\n     }\n \n     for arg in fn_abi.args.iter_mut() {\n         if arg.is_ignore() {\n             continue;\n         }\n-        classify_arg(cx, arg, Size { raw: 16 });\n+        classify_arg(cx, arg, Size::from_bytes(16));\n     }\n }"}, {"sha": "53c9878ab8740be4c8187d1eb90a07e9f350dbf0", "filename": "compiler/rustc_target/src/abi/mod.rs", "status": "modified", "additions": 6, "deletions": 1324, "changes": 1330, "blob_url": "https://github.com/rust-lang/rust/blob/b3bc6bf31265ac10946a0832092dbcedf9b26805/compiler%2Frustc_target%2Fsrc%2Fabi%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/b3bc6bf31265ac10946a0832092dbcedf9b26805/compiler%2Frustc_target%2Fsrc%2Fabi%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_target%2Fsrc%2Fabi%2Fmod.rs?ref=b3bc6bf31265ac10946a0832092dbcedf9b26805", "patch": "@@ -2,1315 +2,32 @@ pub use Integer::*;\n pub use Primitive::*;\n \n use crate::json::{Json, ToJson};\n-use crate::spec::Target;\n \n-use std::convert::{TryFrom, TryInto};\n use std::fmt;\n-use std::iter::Step;\n-use std::num::{NonZeroUsize, ParseIntError};\n-use std::ops::{Add, AddAssign, Deref, Mul, RangeInclusive, Sub};\n-use std::str::FromStr;\n+use std::ops::Deref;\n \n use rustc_data_structures::intern::Interned;\n-use rustc_index::vec::{Idx, IndexVec};\n use rustc_macros::HashStable_Generic;\n \n pub mod call;\n \n-/// Parsed [Data layout](https://llvm.org/docs/LangRef.html#data-layout)\n-/// for a target, which contains everything needed to compute layouts.\n-pub struct TargetDataLayout {\n-    pub endian: Endian,\n-    pub i1_align: AbiAndPrefAlign,\n-    pub i8_align: AbiAndPrefAlign,\n-    pub i16_align: AbiAndPrefAlign,\n-    pub i32_align: AbiAndPrefAlign,\n-    pub i64_align: AbiAndPrefAlign,\n-    pub i128_align: AbiAndPrefAlign,\n-    pub f32_align: AbiAndPrefAlign,\n-    pub f64_align: AbiAndPrefAlign,\n-    pub pointer_size: Size,\n-    pub pointer_align: AbiAndPrefAlign,\n-    pub aggregate_align: AbiAndPrefAlign,\n-\n-    /// Alignments for vector types.\n-    pub vector_align: Vec<(Size, AbiAndPrefAlign)>,\n-\n-    pub instruction_address_space: AddressSpace,\n-\n-    /// Minimum size of #[repr(C)] enums (default I32 bits)\n-    pub c_enum_min_size: Integer,\n-}\n-\n-impl Default for TargetDataLayout {\n-    /// Creates an instance of `TargetDataLayout`.\n-    fn default() -> TargetDataLayout {\n-        let align = |bits| Align::from_bits(bits).unwrap();\n-        TargetDataLayout {\n-            endian: Endian::Big,\n-            i1_align: AbiAndPrefAlign::new(align(8)),\n-            i8_align: AbiAndPrefAlign::new(align(8)),\n-            i16_align: AbiAndPrefAlign::new(align(16)),\n-            i32_align: AbiAndPrefAlign::new(align(32)),\n-            i64_align: AbiAndPrefAlign { abi: align(32), pref: align(64) },\n-            i128_align: AbiAndPrefAlign { abi: align(32), pref: align(64) },\n-            f32_align: AbiAndPrefAlign::new(align(32)),\n-            f64_align: AbiAndPrefAlign::new(align(64)),\n-            pointer_size: Size::from_bits(64),\n-            pointer_align: AbiAndPrefAlign::new(align(64)),\n-            aggregate_align: AbiAndPrefAlign { abi: align(0), pref: align(64) },\n-            vector_align: vec![\n-                (Size::from_bits(64), AbiAndPrefAlign::new(align(64))),\n-                (Size::from_bits(128), AbiAndPrefAlign::new(align(128))),\n-            ],\n-            instruction_address_space: AddressSpace::DATA,\n-            c_enum_min_size: Integer::I32,\n-        }\n-    }\n-}\n-\n-pub enum TargetDataLayoutErrors<'a> {\n-    InvalidAddressSpace { addr_space: &'a str, cause: &'a str, err: ParseIntError },\n-    InvalidBits { kind: &'a str, bit: &'a str, cause: &'a str, err: ParseIntError },\n-    MissingAlignment { cause: &'a str },\n-    InvalidAlignment { cause: &'a str, err: String },\n-    InconsistentTargetArchitecture { dl: &'a str, target: &'a str },\n-    InconsistentTargetPointerWidth { pointer_size: u64, target: u32 },\n-    InvalidBitsSize { err: String },\n-}\n-\n-impl TargetDataLayout {\n-    pub fn parse<'a>(target: &'a Target) -> Result<TargetDataLayout, TargetDataLayoutErrors<'a>> {\n-        // Parse an address space index from a string.\n-        let parse_address_space = |s: &'a str, cause: &'a str| {\n-            s.parse::<u32>().map(AddressSpace).map_err(|err| {\n-                TargetDataLayoutErrors::InvalidAddressSpace { addr_space: s, cause, err }\n-            })\n-        };\n-\n-        // Parse a bit count from a string.\n-        let parse_bits = |s: &'a str, kind: &'a str, cause: &'a str| {\n-            s.parse::<u64>().map_err(|err| TargetDataLayoutErrors::InvalidBits {\n-                kind,\n-                bit: s,\n-                cause,\n-                err,\n-            })\n-        };\n-\n-        // Parse a size string.\n-        let size = |s: &'a str, cause: &'a str| parse_bits(s, \"size\", cause).map(Size::from_bits);\n-\n-        // Parse an alignment string.\n-        let align = |s: &[&'a str], cause: &'a str| {\n-            if s.is_empty() {\n-                return Err(TargetDataLayoutErrors::MissingAlignment { cause });\n-            }\n-            let align_from_bits = |bits| {\n-                Align::from_bits(bits)\n-                    .map_err(|err| TargetDataLayoutErrors::InvalidAlignment { cause, err })\n-            };\n-            let abi = parse_bits(s[0], \"alignment\", cause)?;\n-            let pref = s.get(1).map_or(Ok(abi), |pref| parse_bits(pref, \"alignment\", cause))?;\n-            Ok(AbiAndPrefAlign { abi: align_from_bits(abi)?, pref: align_from_bits(pref)? })\n-        };\n-\n-        let mut dl = TargetDataLayout::default();\n-        let mut i128_align_src = 64;\n-        for spec in target.data_layout.split('-') {\n-            let spec_parts = spec.split(':').collect::<Vec<_>>();\n-\n-            match &*spec_parts {\n-                [\"e\"] => dl.endian = Endian::Little,\n-                [\"E\"] => dl.endian = Endian::Big,\n-                [p] if p.starts_with('P') => {\n-                    dl.instruction_address_space = parse_address_space(&p[1..], \"P\")?\n-                }\n-                [\"a\", ref a @ ..] => dl.aggregate_align = align(a, \"a\")?,\n-                [\"f32\", ref a @ ..] => dl.f32_align = align(a, \"f32\")?,\n-                [\"f64\", ref a @ ..] => dl.f64_align = align(a, \"f64\")?,\n-                [p @ \"p\", s, ref a @ ..] | [p @ \"p0\", s, ref a @ ..] => {\n-                    dl.pointer_size = size(s, p)?;\n-                    dl.pointer_align = align(a, p)?;\n-                }\n-                [s, ref a @ ..] if s.starts_with('i') => {\n-                    let Ok(bits) = s[1..].parse::<u64>() else {\n-                        size(&s[1..], \"i\")?; // For the user error.\n-                        continue;\n-                    };\n-                    let a = align(a, s)?;\n-                    match bits {\n-                        1 => dl.i1_align = a,\n-                        8 => dl.i8_align = a,\n-                        16 => dl.i16_align = a,\n-                        32 => dl.i32_align = a,\n-                        64 => dl.i64_align = a,\n-                        _ => {}\n-                    }\n-                    if bits >= i128_align_src && bits <= 128 {\n-                        // Default alignment for i128 is decided by taking the alignment of\n-                        // largest-sized i{64..=128}.\n-                        i128_align_src = bits;\n-                        dl.i128_align = a;\n-                    }\n-                }\n-                [s, ref a @ ..] if s.starts_with('v') => {\n-                    let v_size = size(&s[1..], \"v\")?;\n-                    let a = align(a, s)?;\n-                    if let Some(v) = dl.vector_align.iter_mut().find(|v| v.0 == v_size) {\n-                        v.1 = a;\n-                        continue;\n-                    }\n-                    // No existing entry, add a new one.\n-                    dl.vector_align.push((v_size, a));\n-                }\n-                _ => {} // Ignore everything else.\n-            }\n-        }\n-\n-        // Perform consistency checks against the Target information.\n-        if dl.endian != target.endian {\n-            return Err(TargetDataLayoutErrors::InconsistentTargetArchitecture {\n-                dl: dl.endian.as_str(),\n-                target: target.endian.as_str(),\n-            });\n-        }\n-\n-        let target_pointer_width: u64 = target.pointer_width.into();\n-        if dl.pointer_size.bits() != target_pointer_width {\n-            return Err(TargetDataLayoutErrors::InconsistentTargetPointerWidth {\n-                pointer_size: dl.pointer_size.bits(),\n-                target: target.pointer_width,\n-            });\n-        }\n-\n-        dl.c_enum_min_size = match Integer::from_size(Size::from_bits(target.c_enum_min_bits)) {\n-            Ok(bits) => bits,\n-            Err(err) => return Err(TargetDataLayoutErrors::InvalidBitsSize { err }),\n-        };\n-\n-        Ok(dl)\n-    }\n-\n-    /// Returns exclusive upper bound on object size.\n-    ///\n-    /// The theoretical maximum object size is defined as the maximum positive `isize` value.\n-    /// This ensures that the `offset` semantics remain well-defined by allowing it to correctly\n-    /// index every address within an object along with one byte past the end, along with allowing\n-    /// `isize` to store the difference between any two pointers into an object.\n-    ///\n-    /// The upper bound on 64-bit currently needs to be lower because LLVM uses a 64-bit integer\n-    /// to represent object size in bits. It would need to be 1 << 61 to account for this, but is\n-    /// currently conservatively bounded to 1 << 47 as that is enough to cover the current usable\n-    /// address space on 64-bit ARMv8 and x86_64.\n-    #[inline]\n-    pub fn obj_size_bound(&self) -> u64 {\n-        match self.pointer_size.bits() {\n-            16 => 1 << 15,\n-            32 => 1 << 31,\n-            64 => 1 << 47,\n-            bits => panic!(\"obj_size_bound: unknown pointer bit size {}\", bits),\n-        }\n-    }\n-\n-    #[inline]\n-    pub fn ptr_sized_integer(&self) -> Integer {\n-        match self.pointer_size.bits() {\n-            16 => I16,\n-            32 => I32,\n-            64 => I64,\n-            bits => panic!(\"ptr_sized_integer: unknown pointer bit size {}\", bits),\n-        }\n-    }\n-\n-    #[inline]\n-    pub fn vector_align(&self, vec_size: Size) -> AbiAndPrefAlign {\n-        for &(size, align) in &self.vector_align {\n-            if size == vec_size {\n-                return align;\n-            }\n-        }\n-        // Default to natural alignment, which is what LLVM does.\n-        // That is, use the size, rounded up to a power of 2.\n-        AbiAndPrefAlign::new(Align::from_bytes(vec_size.bytes().next_power_of_two()).unwrap())\n-    }\n-}\n-\n-pub trait HasDataLayout {\n-    fn data_layout(&self) -> &TargetDataLayout;\n-}\n-\n-impl HasDataLayout for TargetDataLayout {\n-    #[inline]\n-    fn data_layout(&self) -> &TargetDataLayout {\n-        self\n-    }\n-}\n-\n-/// Endianness of the target, which must match cfg(target-endian).\n-#[derive(Copy, Clone, PartialEq)]\n-pub enum Endian {\n-    Little,\n-    Big,\n-}\n-\n-impl Endian {\n-    pub fn as_str(&self) -> &'static str {\n-        match self {\n-            Self::Little => \"little\",\n-            Self::Big => \"big\",\n-        }\n-    }\n-}\n-\n-impl fmt::Debug for Endian {\n-    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {\n-        f.write_str(self.as_str())\n-    }\n-}\n-\n-impl FromStr for Endian {\n-    type Err = String;\n-\n-    fn from_str(s: &str) -> Result<Self, Self::Err> {\n-        match s {\n-            \"little\" => Ok(Self::Little),\n-            \"big\" => Ok(Self::Big),\n-            _ => Err(format!(r#\"unknown endian: \"{}\"\"#, s)),\n-        }\n-    }\n-}\n+pub use rustc_abi::*;\n \n impl ToJson for Endian {\n     fn to_json(&self) -> Json {\n         self.as_str().to_json()\n     }\n }\n \n-/// Size of a type in bytes.\n-#[derive(Copy, Clone, PartialEq, Eq, PartialOrd, Ord, Hash, Encodable, Decodable)]\n-#[derive(HashStable_Generic)]\n-pub struct Size {\n-    raw: u64,\n-}\n-\n-// This is debug-printed a lot in larger structs, don't waste too much space there\n-impl fmt::Debug for Size {\n-    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {\n-        write!(f, \"Size({} bytes)\", self.bytes())\n-    }\n-}\n-\n-impl Size {\n-    pub const ZERO: Size = Size { raw: 0 };\n-\n-    /// Rounds `bits` up to the next-higher byte boundary, if `bits` is\n-    /// not a multiple of 8.\n-    pub fn from_bits(bits: impl TryInto<u64>) -> Size {\n-        let bits = bits.try_into().ok().unwrap();\n-        // Avoid potential overflow from `bits + 7`.\n-        Size { raw: bits / 8 + ((bits % 8) + 7) / 8 }\n-    }\n-\n-    #[inline]\n-    pub fn from_bytes(bytes: impl TryInto<u64>) -> Size {\n-        let bytes: u64 = bytes.try_into().ok().unwrap();\n-        Size { raw: bytes }\n-    }\n-\n-    #[inline]\n-    pub fn bytes(self) -> u64 {\n-        self.raw\n-    }\n-\n-    #[inline]\n-    pub fn bytes_usize(self) -> usize {\n-        self.bytes().try_into().unwrap()\n-    }\n-\n-    #[inline]\n-    pub fn bits(self) -> u64 {\n-        #[cold]\n-        fn overflow(bytes: u64) -> ! {\n-            panic!(\"Size::bits: {} bytes in bits doesn't fit in u64\", bytes)\n-        }\n-\n-        self.bytes().checked_mul(8).unwrap_or_else(|| overflow(self.bytes()))\n-    }\n-\n-    #[inline]\n-    pub fn bits_usize(self) -> usize {\n-        self.bits().try_into().unwrap()\n-    }\n-\n-    #[inline]\n-    pub fn align_to(self, align: Align) -> Size {\n-        let mask = align.bytes() - 1;\n-        Size::from_bytes((self.bytes() + mask) & !mask)\n-    }\n-\n-    #[inline]\n-    pub fn is_aligned(self, align: Align) -> bool {\n-        let mask = align.bytes() - 1;\n-        self.bytes() & mask == 0\n-    }\n-\n-    #[inline]\n-    pub fn checked_add<C: HasDataLayout>(self, offset: Size, cx: &C) -> Option<Size> {\n-        let dl = cx.data_layout();\n-\n-        let bytes = self.bytes().checked_add(offset.bytes())?;\n-\n-        if bytes < dl.obj_size_bound() { Some(Size::from_bytes(bytes)) } else { None }\n-    }\n-\n-    #[inline]\n-    pub fn checked_mul<C: HasDataLayout>(self, count: u64, cx: &C) -> Option<Size> {\n-        let dl = cx.data_layout();\n-\n-        let bytes = self.bytes().checked_mul(count)?;\n-        if bytes < dl.obj_size_bound() { Some(Size::from_bytes(bytes)) } else { None }\n-    }\n-\n-    /// Truncates `value` to `self` bits and then sign-extends it to 128 bits\n-    /// (i.e., if it is negative, fill with 1's on the left).\n-    #[inline]\n-    pub fn sign_extend(self, value: u128) -> u128 {\n-        let size = self.bits();\n-        if size == 0 {\n-            // Truncated until nothing is left.\n-            return 0;\n-        }\n-        // Sign-extend it.\n-        let shift = 128 - size;\n-        // Shift the unsigned value to the left, then shift back to the right as signed\n-        // (essentially fills with sign bit on the left).\n-        (((value << shift) as i128) >> shift) as u128\n-    }\n-\n-    /// Truncates `value` to `self` bits.\n-    #[inline]\n-    pub fn truncate(self, value: u128) -> u128 {\n-        let size = self.bits();\n-        if size == 0 {\n-            // Truncated until nothing is left.\n-            return 0;\n-        }\n-        let shift = 128 - size;\n-        // Truncate (shift left to drop out leftover values, shift right to fill with zeroes).\n-        (value << shift) >> shift\n-    }\n-\n-    #[inline]\n-    pub fn signed_int_min(&self) -> i128 {\n-        self.sign_extend(1_u128 << (self.bits() - 1)) as i128\n-    }\n-\n-    #[inline]\n-    pub fn signed_int_max(&self) -> i128 {\n-        i128::MAX >> (128 - self.bits())\n-    }\n-\n-    #[inline]\n-    pub fn unsigned_int_max(&self) -> u128 {\n-        u128::MAX >> (128 - self.bits())\n-    }\n-}\n-\n-// Panicking addition, subtraction and multiplication for convenience.\n-// Avoid during layout computation, return `LayoutError` instead.\n-\n-impl Add for Size {\n-    type Output = Size;\n-    #[inline]\n-    fn add(self, other: Size) -> Size {\n-        Size::from_bytes(self.bytes().checked_add(other.bytes()).unwrap_or_else(|| {\n-            panic!(\"Size::add: {} + {} doesn't fit in u64\", self.bytes(), other.bytes())\n-        }))\n-    }\n-}\n-\n-impl Sub for Size {\n-    type Output = Size;\n-    #[inline]\n-    fn sub(self, other: Size) -> Size {\n-        Size::from_bytes(self.bytes().checked_sub(other.bytes()).unwrap_or_else(|| {\n-            panic!(\"Size::sub: {} - {} would result in negative size\", self.bytes(), other.bytes())\n-        }))\n-    }\n-}\n-\n-impl Mul<Size> for u64 {\n-    type Output = Size;\n-    #[inline]\n-    fn mul(self, size: Size) -> Size {\n-        size * self\n-    }\n-}\n-\n-impl Mul<u64> for Size {\n-    type Output = Size;\n-    #[inline]\n-    fn mul(self, count: u64) -> Size {\n-        match self.bytes().checked_mul(count) {\n-            Some(bytes) => Size::from_bytes(bytes),\n-            None => panic!(\"Size::mul: {} * {} doesn't fit in u64\", self.bytes(), count),\n-        }\n-    }\n-}\n-\n-impl AddAssign for Size {\n-    #[inline]\n-    fn add_assign(&mut self, other: Size) {\n-        *self = *self + other;\n-    }\n-}\n-\n-impl Step for Size {\n-    #[inline]\n-    fn steps_between(start: &Self, end: &Self) -> Option<usize> {\n-        u64::steps_between(&start.bytes(), &end.bytes())\n-    }\n-\n-    #[inline]\n-    fn forward_checked(start: Self, count: usize) -> Option<Self> {\n-        u64::forward_checked(start.bytes(), count).map(Self::from_bytes)\n-    }\n-\n-    #[inline]\n-    fn forward(start: Self, count: usize) -> Self {\n-        Self::from_bytes(u64::forward(start.bytes(), count))\n-    }\n-\n-    #[inline]\n-    unsafe fn forward_unchecked(start: Self, count: usize) -> Self {\n-        Self::from_bytes(u64::forward_unchecked(start.bytes(), count))\n-    }\n-\n-    #[inline]\n-    fn backward_checked(start: Self, count: usize) -> Option<Self> {\n-        u64::backward_checked(start.bytes(), count).map(Self::from_bytes)\n-    }\n-\n-    #[inline]\n-    fn backward(start: Self, count: usize) -> Self {\n-        Self::from_bytes(u64::backward(start.bytes(), count))\n-    }\n-\n-    #[inline]\n-    unsafe fn backward_unchecked(start: Self, count: usize) -> Self {\n-        Self::from_bytes(u64::backward_unchecked(start.bytes(), count))\n-    }\n-}\n-\n-/// Alignment of a type in bytes (always a power of two).\n-#[derive(Copy, Clone, PartialEq, Eq, PartialOrd, Ord, Hash, Encodable, Decodable)]\n-#[derive(HashStable_Generic)]\n-pub struct Align {\n-    pow2: u8,\n-}\n-\n-// This is debug-printed a lot in larger structs, don't waste too much space there\n-impl fmt::Debug for Align {\n-    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {\n-        write!(f, \"Align({} bytes)\", self.bytes())\n-    }\n-}\n-\n-impl Align {\n-    pub const ONE: Align = Align { pow2: 0 };\n-    pub const MAX: Align = Align { pow2: 29 };\n-\n-    #[inline]\n-    pub fn from_bits(bits: u64) -> Result<Align, String> {\n-        Align::from_bytes(Size::from_bits(bits).bytes())\n-    }\n-\n-    #[inline]\n-    pub fn from_bytes(align: u64) -> Result<Align, String> {\n-        // Treat an alignment of 0 bytes like 1-byte alignment.\n-        if align == 0 {\n-            return Ok(Align::ONE);\n-        }\n-\n-        #[cold]\n-        fn not_power_of_2(align: u64) -> String {\n-            format!(\"`{}` is not a power of 2\", align)\n-        }\n-\n-        #[cold]\n-        fn too_large(align: u64) -> String {\n-            format!(\"`{}` is too large\", align)\n-        }\n-\n-        let mut bytes = align;\n-        let mut pow2: u8 = 0;\n-        while (bytes & 1) == 0 {\n-            pow2 += 1;\n-            bytes >>= 1;\n-        }\n-        if bytes != 1 {\n-            return Err(not_power_of_2(align));\n-        }\n-        if pow2 > Self::MAX.pow2 {\n-            return Err(too_large(align));\n-        }\n-\n-        Ok(Align { pow2 })\n-    }\n-\n-    #[inline]\n-    pub fn bytes(self) -> u64 {\n-        1 << self.pow2\n-    }\n-\n-    #[inline]\n-    pub fn bits(self) -> u64 {\n-        self.bytes() * 8\n-    }\n-\n-    /// Computes the best alignment possible for the given offset\n-    /// (the largest power of two that the offset is a multiple of).\n-    ///\n-    /// N.B., for an offset of `0`, this happens to return `2^64`.\n-    #[inline]\n-    pub fn max_for_offset(offset: Size) -> Align {\n-        Align { pow2: offset.bytes().trailing_zeros() as u8 }\n-    }\n-\n-    /// Lower the alignment, if necessary, such that the given offset\n-    /// is aligned to it (the offset is a multiple of the alignment).\n-    #[inline]\n-    pub fn restrict_for_offset(self, offset: Size) -> Align {\n-        self.min(Align::max_for_offset(offset))\n-    }\n-}\n-\n-/// A pair of alignments, ABI-mandated and preferred.\n-#[derive(Copy, Clone, PartialEq, Eq, Hash, Debug)]\n-#[derive(HashStable_Generic)]\n-pub struct AbiAndPrefAlign {\n-    pub abi: Align,\n-    pub pref: Align,\n-}\n-\n-impl AbiAndPrefAlign {\n-    #[inline]\n-    pub fn new(align: Align) -> AbiAndPrefAlign {\n-        AbiAndPrefAlign { abi: align, pref: align }\n-    }\n-\n-    #[inline]\n-    pub fn min(self, other: AbiAndPrefAlign) -> AbiAndPrefAlign {\n-        AbiAndPrefAlign { abi: self.abi.min(other.abi), pref: self.pref.min(other.pref) }\n-    }\n-\n-    #[inline]\n-    pub fn max(self, other: AbiAndPrefAlign) -> AbiAndPrefAlign {\n-        AbiAndPrefAlign { abi: self.abi.max(other.abi), pref: self.pref.max(other.pref) }\n-    }\n-}\n-\n-/// Integers, also used for enum discriminants.\n-#[derive(Copy, Clone, PartialEq, Eq, PartialOrd, Ord, Hash, Debug, HashStable_Generic)]\n-pub enum Integer {\n-    I8,\n-    I16,\n-    I32,\n-    I64,\n-    I128,\n-}\n-\n-impl Integer {\n-    #[inline]\n-    pub fn size(self) -> Size {\n-        match self {\n-            I8 => Size::from_bytes(1),\n-            I16 => Size::from_bytes(2),\n-            I32 => Size::from_bytes(4),\n-            I64 => Size::from_bytes(8),\n-            I128 => Size::from_bytes(16),\n-        }\n-    }\n-\n-    pub fn align<C: HasDataLayout>(self, cx: &C) -> AbiAndPrefAlign {\n-        let dl = cx.data_layout();\n-\n-        match self {\n-            I8 => dl.i8_align,\n-            I16 => dl.i16_align,\n-            I32 => dl.i32_align,\n-            I64 => dl.i64_align,\n-            I128 => dl.i128_align,\n-        }\n-    }\n-\n-    /// Finds the smallest Integer type which can represent the signed value.\n-    #[inline]\n-    pub fn fit_signed(x: i128) -> Integer {\n-        match x {\n-            -0x0000_0000_0000_0080..=0x0000_0000_0000_007f => I8,\n-            -0x0000_0000_0000_8000..=0x0000_0000_0000_7fff => I16,\n-            -0x0000_0000_8000_0000..=0x0000_0000_7fff_ffff => I32,\n-            -0x8000_0000_0000_0000..=0x7fff_ffff_ffff_ffff => I64,\n-            _ => I128,\n-        }\n-    }\n-\n-    /// Finds the smallest Integer type which can represent the unsigned value.\n-    #[inline]\n-    pub fn fit_unsigned(x: u128) -> Integer {\n-        match x {\n-            0..=0x0000_0000_0000_00ff => I8,\n-            0..=0x0000_0000_0000_ffff => I16,\n-            0..=0x0000_0000_ffff_ffff => I32,\n-            0..=0xffff_ffff_ffff_ffff => I64,\n-            _ => I128,\n-        }\n-    }\n-\n-    /// Finds the smallest integer with the given alignment.\n-    pub fn for_align<C: HasDataLayout>(cx: &C, wanted: Align) -> Option<Integer> {\n-        let dl = cx.data_layout();\n-\n-        for candidate in [I8, I16, I32, I64, I128] {\n-            if wanted == candidate.align(dl).abi && wanted.bytes() == candidate.size().bytes() {\n-                return Some(candidate);\n-            }\n-        }\n-        None\n-    }\n-\n-    /// Find the largest integer with the given alignment or less.\n-    pub fn approximate_align<C: HasDataLayout>(cx: &C, wanted: Align) -> Integer {\n-        let dl = cx.data_layout();\n-\n-        // FIXME(eddyb) maybe include I128 in the future, when it works everywhere.\n-        for candidate in [I64, I32, I16] {\n-            if wanted >= candidate.align(dl).abi && wanted.bytes() >= candidate.size().bytes() {\n-                return candidate;\n-            }\n-        }\n-        I8\n-    }\n-\n-    // FIXME(eddyb) consolidate this and other methods that find the appropriate\n-    // `Integer` given some requirements.\n-    #[inline]\n-    fn from_size(size: Size) -> Result<Self, String> {\n-        match size.bits() {\n-            8 => Ok(Integer::I8),\n-            16 => Ok(Integer::I16),\n-            32 => Ok(Integer::I32),\n-            64 => Ok(Integer::I64),\n-            128 => Ok(Integer::I128),\n-            _ => Err(format!(\"rust does not support integers with {} bits\", size.bits())),\n-        }\n-    }\n-}\n-\n-/// Fundamental unit of memory access and layout.\n-#[derive(Copy, Clone, PartialEq, Eq, Hash, Debug, HashStable_Generic)]\n-pub enum Primitive {\n-    /// The `bool` is the signedness of the `Integer` type.\n-    ///\n-    /// One would think we would not care about such details this low down,\n-    /// but some ABIs are described in terms of C types and ISAs where the\n-    /// integer arithmetic is done on {sign,zero}-extended registers, e.g.\n-    /// a negative integer passed by zero-extension will appear positive in\n-    /// the callee, and most operations on it will produce the wrong values.\n-    Int(Integer, bool),\n-    F32,\n-    F64,\n-    Pointer,\n-}\n-\n-impl Primitive {\n-    pub fn size<C: HasDataLayout>(self, cx: &C) -> Size {\n-        let dl = cx.data_layout();\n-\n-        match self {\n-            Int(i, _) => i.size(),\n-            F32 => Size::from_bits(32),\n-            F64 => Size::from_bits(64),\n-            Pointer => dl.pointer_size,\n-        }\n-    }\n-\n-    pub fn align<C: HasDataLayout>(self, cx: &C) -> AbiAndPrefAlign {\n-        let dl = cx.data_layout();\n-\n-        match self {\n-            Int(i, _) => i.align(dl),\n-            F32 => dl.f32_align,\n-            F64 => dl.f64_align,\n-            Pointer => dl.pointer_align,\n-        }\n-    }\n-\n-    // FIXME(eddyb) remove, it's trivial thanks to `matches!`.\n-    #[inline]\n-    pub fn is_float(self) -> bool {\n-        matches!(self, F32 | F64)\n-    }\n-\n-    // FIXME(eddyb) remove, it's completely unused.\n-    #[inline]\n-    pub fn is_int(self) -> bool {\n-        matches!(self, Int(..))\n-    }\n-\n-    #[inline]\n-    pub fn is_ptr(self) -> bool {\n-        matches!(self, Pointer)\n-    }\n-}\n-\n-/// Inclusive wrap-around range of valid values, that is, if\n-/// start > end, it represents `start..=MAX`,\n-/// followed by `0..=end`.\n-///\n-/// That is, for an i8 primitive, a range of `254..=2` means following\n-/// sequence:\n-///\n-///    254 (-2), 255 (-1), 0, 1, 2\n-///\n-/// This is intended specifically to mirror LLVM\u2019s `!range` metadata semantics.\n-#[derive(Clone, Copy, PartialEq, Eq, Hash)]\n-#[derive(HashStable_Generic)]\n-pub struct WrappingRange {\n-    pub start: u128,\n-    pub end: u128,\n-}\n-\n-impl WrappingRange {\n-    pub fn full(size: Size) -> Self {\n-        Self { start: 0, end: size.unsigned_int_max() }\n-    }\n-\n-    /// Returns `true` if `v` is contained in the range.\n-    #[inline(always)]\n-    pub fn contains(&self, v: u128) -> bool {\n-        if self.start <= self.end {\n-            self.start <= v && v <= self.end\n-        } else {\n-            self.start <= v || v <= self.end\n-        }\n-    }\n-\n-    /// Returns `self` with replaced `start`\n-    #[inline(always)]\n-    pub fn with_start(mut self, start: u128) -> Self {\n-        self.start = start;\n-        self\n-    }\n-\n-    /// Returns `self` with replaced `end`\n-    #[inline(always)]\n-    pub fn with_end(mut self, end: u128) -> Self {\n-        self.end = end;\n-        self\n-    }\n-\n-    /// Returns `true` if `size` completely fills the range.\n-    #[inline]\n-    pub fn is_full_for(&self, size: Size) -> bool {\n-        let max_value = size.unsigned_int_max();\n-        debug_assert!(self.start <= max_value && self.end <= max_value);\n-        self.start == (self.end.wrapping_add(1) & max_value)\n-    }\n-}\n-\n-impl fmt::Debug for WrappingRange {\n-    fn fmt(&self, fmt: &mut fmt::Formatter<'_>) -> fmt::Result {\n-        if self.start > self.end {\n-            write!(fmt, \"(..={}) | ({}..)\", self.end, self.start)?;\n-        } else {\n-            write!(fmt, \"{}..={}\", self.start, self.end)?;\n-        }\n-        Ok(())\n-    }\n-}\n-\n-/// Information about one scalar component of a Rust type.\n-#[derive(Clone, Copy, PartialEq, Eq, Hash, Debug)]\n-#[derive(HashStable_Generic)]\n-pub enum Scalar {\n-    Initialized {\n-        value: Primitive,\n-\n-        // FIXME(eddyb) always use the shortest range, e.g., by finding\n-        // the largest space between two consecutive valid values and\n-        // taking everything else as the (shortest) valid range.\n-        valid_range: WrappingRange,\n-    },\n-    Union {\n-        /// Even for unions, we need to use the correct registers for the kind of\n-        /// values inside the union, so we keep the `Primitive` type around. We\n-        /// also use it to compute the size of the scalar.\n-        /// However, unions never have niches and even allow undef,\n-        /// so there is no `valid_range`.\n-        value: Primitive,\n-    },\n-}\n-\n-impl Scalar {\n-    #[inline]\n-    pub fn is_bool(&self) -> bool {\n-        matches!(\n-            self,\n-            Scalar::Initialized {\n-                value: Int(I8, false),\n-                valid_range: WrappingRange { start: 0, end: 1 }\n-            }\n-        )\n-    }\n-\n-    /// Get the primitive representation of this type, ignoring the valid range and whether the\n-    /// value is allowed to be undefined (due to being a union).\n-    pub fn primitive(&self) -> Primitive {\n-        match *self {\n-            Scalar::Initialized { value, .. } | Scalar::Union { value } => value,\n-        }\n-    }\n-\n-    pub fn align(self, cx: &impl HasDataLayout) -> AbiAndPrefAlign {\n-        self.primitive().align(cx)\n-    }\n-\n-    pub fn size(self, cx: &impl HasDataLayout) -> Size {\n-        self.primitive().size(cx)\n-    }\n-\n-    #[inline]\n-    pub fn to_union(&self) -> Self {\n-        Self::Union { value: self.primitive() }\n-    }\n-\n-    #[inline]\n-    pub fn valid_range(&self, cx: &impl HasDataLayout) -> WrappingRange {\n-        match *self {\n-            Scalar::Initialized { valid_range, .. } => valid_range,\n-            Scalar::Union { value } => WrappingRange::full(value.size(cx)),\n-        }\n-    }\n-\n-    #[inline]\n-    /// Allows the caller to mutate the valid range. This operation will panic if attempted on a union.\n-    pub fn valid_range_mut(&mut self) -> &mut WrappingRange {\n-        match self {\n-            Scalar::Initialized { valid_range, .. } => valid_range,\n-            Scalar::Union { .. } => panic!(\"cannot change the valid range of a union\"),\n-        }\n-    }\n-\n-    /// Returns `true` if all possible numbers are valid, i.e `valid_range` covers the whole layout\n-    #[inline]\n-    pub fn is_always_valid<C: HasDataLayout>(&self, cx: &C) -> bool {\n-        match *self {\n-            Scalar::Initialized { valid_range, .. } => valid_range.is_full_for(self.size(cx)),\n-            Scalar::Union { .. } => true,\n-        }\n-    }\n-\n-    /// Returns `true` if this type can be left uninit.\n-    #[inline]\n-    pub fn is_uninit_valid(&self) -> bool {\n-        match *self {\n-            Scalar::Initialized { .. } => false,\n-            Scalar::Union { .. } => true,\n-        }\n-    }\n-}\n-\n-/// Describes how the fields of a type are located in memory.\n-#[derive(PartialEq, Eq, Hash, Debug, HashStable_Generic)]\n-pub enum FieldsShape {\n-    /// Scalar primitives and `!`, which never have fields.\n-    Primitive,\n-\n-    /// All fields start at no offset. The `usize` is the field count.\n-    Union(NonZeroUsize),\n-\n-    /// Array/vector-like placement, with all fields of identical types.\n-    Array { stride: Size, count: u64 },\n-\n-    /// Struct-like placement, with precomputed offsets.\n-    ///\n-    /// Fields are guaranteed to not overlap, but note that gaps\n-    /// before, between and after all the fields are NOT always\n-    /// padding, and as such their contents may not be discarded.\n-    /// For example, enum variants leave a gap at the start,\n-    /// where the discriminant field in the enum layout goes.\n-    Arbitrary {\n-        /// Offsets for the first byte of each field,\n-        /// ordered to match the source definition order.\n-        /// This vector does not go in increasing order.\n-        // FIXME(eddyb) use small vector optimization for the common case.\n-        offsets: Vec<Size>,\n-\n-        /// Maps source order field indices to memory order indices,\n-        /// depending on how the fields were reordered (if at all).\n-        /// This is a permutation, with both the source order and the\n-        /// memory order using the same (0..n) index ranges.\n-        ///\n-        /// Note that during computation of `memory_index`, sometimes\n-        /// it is easier to operate on the inverse mapping (that is,\n-        /// from memory order to source order), and that is usually\n-        /// named `inverse_memory_index`.\n-        ///\n-        // FIXME(eddyb) build a better abstraction for permutations, if possible.\n-        // FIXME(camlorn) also consider small vector  optimization here.\n-        memory_index: Vec<u32>,\n-    },\n-}\n-\n-impl FieldsShape {\n-    #[inline]\n-    pub fn count(&self) -> usize {\n-        match *self {\n-            FieldsShape::Primitive => 0,\n-            FieldsShape::Union(count) => count.get(),\n-            FieldsShape::Array { count, .. } => count.try_into().unwrap(),\n-            FieldsShape::Arbitrary { ref offsets, .. } => offsets.len(),\n-        }\n-    }\n-\n-    #[inline]\n-    pub fn offset(&self, i: usize) -> Size {\n-        match *self {\n-            FieldsShape::Primitive => {\n-                unreachable!(\"FieldsShape::offset: `Primitive`s have no fields\")\n-            }\n-            FieldsShape::Union(count) => {\n-                assert!(\n-                    i < count.get(),\n-                    \"tried to access field {} of union with {} fields\",\n-                    i,\n-                    count\n-                );\n-                Size::ZERO\n-            }\n-            FieldsShape::Array { stride, count } => {\n-                let i = u64::try_from(i).unwrap();\n-                assert!(i < count);\n-                stride * i\n-            }\n-            FieldsShape::Arbitrary { ref offsets, .. } => offsets[i],\n-        }\n-    }\n-\n-    #[inline]\n-    pub fn memory_index(&self, i: usize) -> usize {\n-        match *self {\n-            FieldsShape::Primitive => {\n-                unreachable!(\"FieldsShape::memory_index: `Primitive`s have no fields\")\n-            }\n-            FieldsShape::Union(_) | FieldsShape::Array { .. } => i,\n-            FieldsShape::Arbitrary { ref memory_index, .. } => memory_index[i].try_into().unwrap(),\n-        }\n-    }\n-\n-    /// Gets source indices of the fields by increasing offsets.\n-    #[inline]\n-    pub fn index_by_increasing_offset<'a>(&'a self) -> impl Iterator<Item = usize> + 'a {\n-        let mut inverse_small = [0u8; 64];\n-        let mut inverse_big = vec![];\n-        let use_small = self.count() <= inverse_small.len();\n-\n-        // We have to write this logic twice in order to keep the array small.\n-        if let FieldsShape::Arbitrary { ref memory_index, .. } = *self {\n-            if use_small {\n-                for i in 0..self.count() {\n-                    inverse_small[memory_index[i] as usize] = i as u8;\n-                }\n-            } else {\n-                inverse_big = vec![0; self.count()];\n-                for i in 0..self.count() {\n-                    inverse_big[memory_index[i] as usize] = i as u32;\n-                }\n-            }\n-        }\n-\n-        (0..self.count()).map(move |i| match *self {\n-            FieldsShape::Primitive | FieldsShape::Union(_) | FieldsShape::Array { .. } => i,\n-            FieldsShape::Arbitrary { .. } => {\n-                if use_small {\n-                    inverse_small[i] as usize\n-                } else {\n-                    inverse_big[i] as usize\n-                }\n-            }\n-        })\n-    }\n-}\n-\n-/// An identifier that specifies the address space that some operation\n-/// should operate on. Special address spaces have an effect on code generation,\n-/// depending on the target and the address spaces it implements.\n-#[derive(Copy, Clone, Debug, PartialEq, Eq, PartialOrd, Ord)]\n-pub struct AddressSpace(pub u32);\n-\n-impl AddressSpace {\n-    /// The default address space, corresponding to data space.\n-    pub const DATA: Self = AddressSpace(0);\n-}\n-\n-/// Describes how values of the type are passed by target ABIs,\n-/// in terms of categories of C types there are ABI rules for.\n-#[derive(Clone, Copy, PartialEq, Eq, Hash, Debug, HashStable_Generic)]\n-pub enum Abi {\n-    Uninhabited,\n-    Scalar(Scalar),\n-    ScalarPair(Scalar, Scalar),\n-    Vector {\n-        element: Scalar,\n-        count: u64,\n-    },\n-    Aggregate {\n-        /// If true, the size is exact, otherwise it's only a lower bound.\n-        sized: bool,\n-    },\n-}\n-\n-impl Abi {\n-    /// Returns `true` if the layout corresponds to an unsized type.\n-    #[inline]\n-    pub fn is_unsized(&self) -> bool {\n-        match *self {\n-            Abi::Uninhabited | Abi::Scalar(_) | Abi::ScalarPair(..) | Abi::Vector { .. } => false,\n-            Abi::Aggregate { sized } => !sized,\n-        }\n-    }\n-\n-    #[inline]\n-    pub fn is_sized(&self) -> bool {\n-        !self.is_unsized()\n-    }\n-\n-    /// Returns `true` if this is a single signed integer scalar\n-    #[inline]\n-    pub fn is_signed(&self) -> bool {\n-        match self {\n-            Abi::Scalar(scal) => match scal.primitive() {\n-                Primitive::Int(_, signed) => signed,\n-                _ => false,\n-            },\n-            _ => panic!(\"`is_signed` on non-scalar ABI {:?}\", self),\n-        }\n-    }\n-\n-    /// Returns `true` if this is an uninhabited type\n-    #[inline]\n-    pub fn is_uninhabited(&self) -> bool {\n-        matches!(*self, Abi::Uninhabited)\n-    }\n-\n-    /// Returns `true` is this is a scalar type\n-    #[inline]\n-    pub fn is_scalar(&self) -> bool {\n-        matches!(*self, Abi::Scalar(_))\n-    }\n-}\n-\n rustc_index::newtype_index! {\n     pub struct VariantIdx {\n         derive [HashStable_Generic]\n     }\n }\n \n-#[derive(PartialEq, Eq, Hash, Debug, HashStable_Generic)]\n-pub enum Variants<'a> {\n-    /// Single enum variants, structs/tuples, unions, and all non-ADTs.\n-    Single { index: VariantIdx },\n-\n-    /// Enum-likes with more than one inhabited variant: each variant comes with\n-    /// a *discriminant* (usually the same as the variant index but the user can\n-    /// assign explicit discriminant values).  That discriminant is encoded\n-    /// as a *tag* on the machine.  The layout of each variant is\n-    /// a struct, and they all have space reserved for the tag.\n-    /// For enums, the tag is the sole field of the layout.\n-    Multiple {\n-        tag: Scalar,\n-        tag_encoding: TagEncoding,\n-        tag_field: usize,\n-        variants: IndexVec<VariantIdx, Layout<'a>>,\n-    },\n-}\n-\n-#[derive(PartialEq, Eq, Hash, Debug, HashStable_Generic)]\n-pub enum TagEncoding {\n-    /// The tag directly stores the discriminant, but possibly with a smaller layout\n-    /// (so converting the tag to the discriminant can require sign extension).\n-    Direct,\n-\n-    /// Niche (values invalid for a type) encoding the discriminant:\n-    /// Discriminant and variant index coincide.\n-    /// The variant `untagged_variant` contains a niche at an arbitrary\n-    /// offset (field `tag_field` of the enum), which for a variant with\n-    /// discriminant `d` is set to\n-    /// `(d - niche_variants.start).wrapping_add(niche_start)`.\n-    ///\n-    /// For example, `Option<(usize, &T)>`  is represented such that\n-    /// `None` has a null pointer for the second tuple field, and\n-    /// `Some` is the identity function (with a non-null reference).\n-    Niche {\n-        untagged_variant: VariantIdx,\n-        niche_variants: RangeInclusive<VariantIdx>,\n-        niche_start: u128,\n-    },\n-}\n-\n-#[derive(Clone, Copy, PartialEq, Eq, Hash, Debug, HashStable_Generic)]\n-pub struct Niche {\n-    pub offset: Size,\n-    pub value: Primitive,\n-    pub valid_range: WrappingRange,\n-}\n-\n-impl Niche {\n-    pub fn from_scalar<C: HasDataLayout>(cx: &C, offset: Size, scalar: Scalar) -> Option<Self> {\n-        let Scalar::Initialized { value, valid_range } = scalar else { return None };\n-        let niche = Niche { offset, value, valid_range };\n-        if niche.available(cx) > 0 { Some(niche) } else { None }\n-    }\n-\n-    pub fn available<C: HasDataLayout>(&self, cx: &C) -> u128 {\n-        let Self { value, valid_range: v, .. } = *self;\n-        let size = value.size(cx);\n-        assert!(size.bits() <= 128);\n-        let max_value = size.unsigned_int_max();\n-\n-        // Find out how many values are outside the valid range.\n-        let niche = v.end.wrapping_add(1)..v.start;\n-        niche.end.wrapping_sub(niche.start) & max_value\n-    }\n-\n-    pub fn reserve<C: HasDataLayout>(&self, cx: &C, count: u128) -> Option<(u128, Scalar)> {\n-        assert!(count > 0);\n-\n-        let Self { value, valid_range: v, .. } = *self;\n-        let size = value.size(cx);\n-        assert!(size.bits() <= 128);\n-        let max_value = size.unsigned_int_max();\n-\n-        let niche = v.end.wrapping_add(1)..v.start;\n-        let available = niche.end.wrapping_sub(niche.start) & max_value;\n-        if count > available {\n-            return None;\n-        }\n-\n-        // Extend the range of valid values being reserved by moving either `v.start` or `v.end` bound.\n-        // Given an eventual `Option<T>`, we try to maximize the chance for `None` to occupy the niche of zero.\n-        // This is accomplished by preferring enums with 2 variants(`count==1`) and always taking the shortest path to niche zero.\n-        // Having `None` in niche zero can enable some special optimizations.\n-        //\n-        // Bound selection criteria:\n-        // 1. Select closest to zero given wrapping semantics.\n-        // 2. Avoid moving past zero if possible.\n-        //\n-        // In practice this means that enums with `count > 1` are unlikely to claim niche zero, since they have to fit perfectly.\n-        // If niche zero is already reserved, the selection of bounds are of little interest.\n-        let move_start = |v: WrappingRange| {\n-            let start = v.start.wrapping_sub(count) & max_value;\n-            Some((start, Scalar::Initialized { value, valid_range: v.with_start(start) }))\n-        };\n-        let move_end = |v: WrappingRange| {\n-            let start = v.end.wrapping_add(1) & max_value;\n-            let end = v.end.wrapping_add(count) & max_value;\n-            Some((start, Scalar::Initialized { value, valid_range: v.with_end(end) }))\n-        };\n-        let distance_end_zero = max_value - v.end;\n-        if v.start > v.end {\n-            // zero is unavailable because wrapping occurs\n-            move_end(v)\n-        } else if v.start <= distance_end_zero {\n-            if count <= v.start {\n-                move_start(v)\n-            } else {\n-                // moved past zero, use other bound\n-                move_end(v)\n-            }\n-        } else {\n-            let end = v.end.wrapping_add(count) & max_value;\n-            let overshot_zero = (1..=v.end).contains(&end);\n-            if overshot_zero {\n-                // moved past zero, use other bound\n-                move_start(v)\n-            } else {\n-                move_end(v)\n-            }\n-        }\n-    }\n-}\n-\n-#[derive(PartialEq, Eq, Hash, HashStable_Generic)]\n-pub struct LayoutS<'a> {\n-    /// Says where the fields are located within the layout.\n-    pub fields: FieldsShape,\n-\n-    /// Encodes information about multi-variant layouts.\n-    /// Even with `Multiple` variants, a layout still has its own fields! Those are then\n-    /// shared between all variants. One of them will be the discriminant,\n-    /// but e.g. generators can have more.\n-    ///\n-    /// To access all fields of this layout, both `fields` and the fields of the active variant\n-    /// must be taken into account.\n-    pub variants: Variants<'a>,\n-\n-    /// The `abi` defines how this data is passed between functions, and it defines\n-    /// value restrictions via `valid_range`.\n-    ///\n-    /// Note that this is entirely orthogonal to the recursive structure defined by\n-    /// `variants` and `fields`; for example, `ManuallyDrop<Result<isize, isize>>` has\n-    /// `Abi::ScalarPair`! So, even with non-`Aggregate` `abi`, `fields` and `variants`\n-    /// have to be taken into account to find all fields of this layout.\n-    pub abi: Abi,\n-\n-    /// The leaf scalar with the largest number of invalid values\n-    /// (i.e. outside of its `valid_range`), if it exists.\n-    pub largest_niche: Option<Niche>,\n-\n-    pub align: AbiAndPrefAlign,\n-    pub size: Size,\n-}\n-\n-impl<'a> LayoutS<'a> {\n-    pub fn scalar<C: HasDataLayout>(cx: &C, scalar: Scalar) -> Self {\n-        let largest_niche = Niche::from_scalar(cx, Size::ZERO, scalar);\n-        let size = scalar.size(cx);\n-        let align = scalar.align(cx);\n-        LayoutS {\n-            variants: Variants::Single { index: VariantIdx::new(0) },\n-            fields: FieldsShape::Primitive,\n-            abi: Abi::Scalar(scalar),\n-            largest_niche,\n-            size,\n-            align,\n-        }\n-    }\n-}\n-\n-impl<'a> fmt::Debug for LayoutS<'a> {\n-    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {\n-        // This is how `Layout` used to print before it become\n-        // `Interned<LayoutS>`. We print it like this to avoid having to update\n-        // expected output in a lot of tests.\n-        let LayoutS { size, align, abi, fields, largest_niche, variants } = self;\n-        f.debug_struct(\"Layout\")\n-            .field(\"size\", size)\n-            .field(\"align\", align)\n-            .field(\"abi\", abi)\n-            .field(\"fields\", fields)\n-            .field(\"largest_niche\", largest_niche)\n-            .field(\"variants\", variants)\n-            .finish()\n-    }\n-}\n-\n #[derive(Copy, Clone, PartialEq, Eq, Hash, HashStable_Generic)]\n #[rustc_pass_by_value]\n-pub struct Layout<'a>(pub Interned<'a, LayoutS<'a>>);\n+pub struct Layout<'a>(pub Interned<'a, LayoutS<VariantIdx>>);\n \n impl<'a> fmt::Debug for Layout<'a> {\n     fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {\n@@ -1324,7 +41,7 @@ impl<'a> Layout<'a> {\n         &self.0.0.fields\n     }\n \n-    pub fn variants(self) -> &'a Variants<'a> {\n+    pub fn variants(self) -> &'a Variants<VariantIdx> {\n         &self.0.0.variants\n     }\n \n@@ -1359,47 +76,12 @@ pub struct TyAndLayout<'a, Ty> {\n }\n \n impl<'a, Ty> Deref for TyAndLayout<'a, Ty> {\n-    type Target = &'a LayoutS<'a>;\n-    fn deref(&self) -> &&'a LayoutS<'a> {\n+    type Target = &'a LayoutS<VariantIdx>;\n+    fn deref(&self) -> &&'a LayoutS<VariantIdx> {\n         &self.layout.0.0\n     }\n }\n \n-#[derive(Copy, Clone, PartialEq, Eq, Debug)]\n-pub enum PointerKind {\n-    /// Most general case, we know no restrictions to tell LLVM.\n-    SharedMutable,\n-\n-    /// `&T` where `T` contains no `UnsafeCell`, is `dereferenceable`, `noalias` and `readonly`.\n-    Frozen,\n-\n-    /// `&mut T` which is `dereferenceable` and `noalias` but not `readonly`.\n-    UniqueBorrowed,\n-\n-    /// `&mut !Unpin`, which is `dereferenceable` but neither `noalias` nor `readonly`.\n-    UniqueBorrowedPinned,\n-\n-    /// `Box<T>`, which is `noalias` (even on return types, unlike the above) but neither `readonly`\n-    /// nor `dereferenceable`.\n-    UniqueOwned,\n-}\n-\n-#[derive(Copy, Clone, Debug)]\n-pub struct PointeeInfo {\n-    pub size: Size,\n-    pub align: Align,\n-    pub safe: Option<PointerKind>,\n-    pub address_space: AddressSpace,\n-}\n-\n-/// Used in `might_permit_raw_init` to indicate the kind of initialisation\n-/// that is checked to be valid\n-#[derive(Copy, Clone, Debug, PartialEq, Eq)]\n-pub enum InitKind {\n-    Zero,\n-    UninitMitigated0x01Fill,\n-}\n-\n /// Trait that needs to be implemented by the higher-level type representation\n /// (e.g. `rustc_middle::ty::Ty`), to provide `rustc_target::abi` functionality.\n pub trait TyAbiInterface<'a, C>: Sized {"}, {"sha": "b69a0a645a415913909e0dd127bff1485cfbb770", "filename": "compiler/rustc_target/src/lib.rs", "status": "modified", "additions": 1, "deletions": 4, "changes": 5, "blob_url": "https://github.com/rust-lang/rust/blob/b3bc6bf31265ac10946a0832092dbcedf9b26805/compiler%2Frustc_target%2Fsrc%2Flib.rs", "raw_url": "https://github.com/rust-lang/rust/raw/b3bc6bf31265ac10946a0832092dbcedf9b26805/compiler%2Frustc_target%2Fsrc%2Flib.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_target%2Fsrc%2Flib.rs?ref=b3bc6bf31265ac10946a0832092dbcedf9b26805", "patch": "@@ -35,10 +35,7 @@ pub mod spec;\n #[cfg(test)]\n mod tests;\n \n-/// Requirements for a `StableHashingContext` to be used in this crate.\n-/// This is a hack to allow using the `HashStable_Generic` derive macro\n-/// instead of implementing everything in `rustc_middle`.\n-pub trait HashStableContext {}\n+pub use rustc_abi::HashStableContext;\n \n /// The name of rustc's own place to organize libraries.\n ///"}, {"sha": "bd5b10d6aa7c36d5093f775dce6800b2ba71c634", "filename": "compiler/rustc_target/src/spec/mod.rs", "status": "modified", "additions": 118, "deletions": 1, "changes": 119, "blob_url": "https://github.com/rust-lang/rust/blob/b3bc6bf31265ac10946a0832092dbcedf9b26805/compiler%2Frustc_target%2Fsrc%2Fspec%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/b3bc6bf31265ac10946a0832092dbcedf9b26805/compiler%2Frustc_target%2Fsrc%2Fspec%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_target%2Fsrc%2Fspec%2Fmod.rs?ref=b3bc6bf31265ac10946a0832092dbcedf9b26805", "patch": "@@ -35,7 +35,10 @@\n //! to the list specified by the target, rather than replace.\n \n use crate::abi::call::Conv;\n-use crate::abi::Endian;\n+use crate::abi::{\n+    AbiAndPrefAlign, AddressSpace, Align, Endian, Integer, Size, TargetDataLayout,\n+    TargetDataLayoutErrors,\n+};\n use crate::json::{Json, ToJson};\n use crate::spec::abi::{lookup as lookup_abi, Abi};\n use crate::spec::crt_objects::{CrtObjects, LinkSelfContainedDefault};\n@@ -1317,6 +1320,120 @@ pub struct Target {\n     pub options: TargetOptions,\n }\n \n+impl Target {\n+    pub fn parse_data_layout<'a>(&'a self) -> Result<TargetDataLayout, TargetDataLayoutErrors<'a>> {\n+        // Parse an address space index from a string.\n+        let parse_address_space = |s: &'a str, cause: &'a str| {\n+            s.parse::<u32>().map(AddressSpace).map_err(|err| {\n+                TargetDataLayoutErrors::InvalidAddressSpace { addr_space: s, cause, err }\n+            })\n+        };\n+\n+        // Parse a bit count from a string.\n+        let parse_bits = |s: &'a str, kind: &'a str, cause: &'a str| {\n+            s.parse::<u64>().map_err(|err| TargetDataLayoutErrors::InvalidBits {\n+                kind,\n+                bit: s,\n+                cause,\n+                err,\n+            })\n+        };\n+\n+        // Parse a size string.\n+        let size = |s: &'a str, cause: &'a str| parse_bits(s, \"size\", cause).map(Size::from_bits);\n+\n+        // Parse an alignment string.\n+        let align = |s: &[&'a str], cause: &'a str| {\n+            if s.is_empty() {\n+                return Err(TargetDataLayoutErrors::MissingAlignment { cause });\n+            }\n+            let align_from_bits = |bits| {\n+                Align::from_bits(bits)\n+                    .map_err(|err| TargetDataLayoutErrors::InvalidAlignment { cause, err })\n+            };\n+            let abi = parse_bits(s[0], \"alignment\", cause)?;\n+            let pref = s.get(1).map_or(Ok(abi), |pref| parse_bits(pref, \"alignment\", cause))?;\n+            Ok(AbiAndPrefAlign { abi: align_from_bits(abi)?, pref: align_from_bits(pref)? })\n+        };\n+\n+        let mut dl = TargetDataLayout::default();\n+        let mut i128_align_src = 64;\n+        for spec in self.data_layout.split('-') {\n+            let spec_parts = spec.split(':').collect::<Vec<_>>();\n+\n+            match &*spec_parts {\n+                [\"e\"] => dl.endian = Endian::Little,\n+                [\"E\"] => dl.endian = Endian::Big,\n+                [p] if p.starts_with('P') => {\n+                    dl.instruction_address_space = parse_address_space(&p[1..], \"P\")?\n+                }\n+                [\"a\", ref a @ ..] => dl.aggregate_align = align(a, \"a\")?,\n+                [\"f32\", ref a @ ..] => dl.f32_align = align(a, \"f32\")?,\n+                [\"f64\", ref a @ ..] => dl.f64_align = align(a, \"f64\")?,\n+                [p @ \"p\", s, ref a @ ..] | [p @ \"p0\", s, ref a @ ..] => {\n+                    dl.pointer_size = size(s, p)?;\n+                    dl.pointer_align = align(a, p)?;\n+                }\n+                [s, ref a @ ..] if s.starts_with('i') => {\n+                    let Ok(bits) = s[1..].parse::<u64>() else {\n+                        size(&s[1..], \"i\")?; // For the user error.\n+                        continue;\n+                    };\n+                    let a = align(a, s)?;\n+                    match bits {\n+                        1 => dl.i1_align = a,\n+                        8 => dl.i8_align = a,\n+                        16 => dl.i16_align = a,\n+                        32 => dl.i32_align = a,\n+                        64 => dl.i64_align = a,\n+                        _ => {}\n+                    }\n+                    if bits >= i128_align_src && bits <= 128 {\n+                        // Default alignment for i128 is decided by taking the alignment of\n+                        // largest-sized i{64..=128}.\n+                        i128_align_src = bits;\n+                        dl.i128_align = a;\n+                    }\n+                }\n+                [s, ref a @ ..] if s.starts_with('v') => {\n+                    let v_size = size(&s[1..], \"v\")?;\n+                    let a = align(a, s)?;\n+                    if let Some(v) = dl.vector_align.iter_mut().find(|v| v.0 == v_size) {\n+                        v.1 = a;\n+                        continue;\n+                    }\n+                    // No existing entry, add a new one.\n+                    dl.vector_align.push((v_size, a));\n+                }\n+                _ => {} // Ignore everything else.\n+            }\n+        }\n+\n+        // Perform consistency checks against the Target information.\n+        if dl.endian != self.endian {\n+            return Err(TargetDataLayoutErrors::InconsistentTargetArchitecture {\n+                dl: dl.endian.as_str(),\n+                target: self.endian.as_str(),\n+            });\n+        }\n+\n+        let target_pointer_width: u64 = self.pointer_width.into();\n+        if dl.pointer_size.bits() != target_pointer_width {\n+            return Err(TargetDataLayoutErrors::InconsistentTargetPointerWidth {\n+                pointer_size: dl.pointer_size.bits(),\n+                target: self.pointer_width,\n+            });\n+        }\n+\n+        dl.c_enum_min_size = match Integer::from_size(Size::from_bits(self.c_enum_min_bits)) {\n+            Ok(bits) => bits,\n+            Err(err) => return Err(TargetDataLayoutErrors::InvalidBitsSize { err }),\n+        };\n+\n+        Ok(dl)\n+    }\n+}\n+\n pub trait HasTargetSpec {\n     fn target_spec(&self) -> &Target;\n }"}, {"sha": "a432498abcca4001a5498a150796de84300a4644", "filename": "compiler/rustc_traits/Cargo.toml", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "blob_url": "https://github.com/rust-lang/rust/blob/b3bc6bf31265ac10946a0832092dbcedf9b26805/compiler%2Frustc_traits%2FCargo.toml", "raw_url": "https://github.com/rust-lang/rust/raw/b3bc6bf31265ac10946a0832092dbcedf9b26805/compiler%2Frustc_traits%2FCargo.toml", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_traits%2FCargo.toml?ref=b3bc6bf31265ac10946a0832092dbcedf9b26805", "patch": "@@ -12,6 +12,7 @@ rustc_hir = { path = \"../rustc_hir\" }\n rustc_index = { path = \"../rustc_index\" }\n rustc_ast = { path = \"../rustc_ast\" }\n rustc_span = { path = \"../rustc_span\" }\n+rustc_target = { path = \"../rustc_target\" }\n chalk-ir = \"0.87.0\"\n chalk-engine = \"0.87.0\"\n chalk-solve = \"0.87.0\""}, {"sha": "344c8b93c170489dfb726ff7b12eabc1cb47025b", "filename": "compiler/rustc_traits/src/chalk/db.rs", "status": "modified", "additions": 15, "deletions": 15, "changes": 30, "blob_url": "https://github.com/rust-lang/rust/blob/b3bc6bf31265ac10946a0832092dbcedf9b26805/compiler%2Frustc_traits%2Fsrc%2Fchalk%2Fdb.rs", "raw_url": "https://github.com/rust-lang/rust/raw/b3bc6bf31265ac10946a0832092dbcedf9b26805/compiler%2Frustc_traits%2Fsrc%2Fchalk%2Fdb.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_traits%2Fsrc%2Fchalk%2Fdb.rs?ref=b3bc6bf31265ac10946a0832092dbcedf9b26805", "patch": "@@ -9,9 +9,9 @@\n use rustc_middle::traits::ChalkRustInterner as RustInterner;\n use rustc_middle::ty::{self, AssocKind, EarlyBinder, Ty, TyCtxt, TypeFoldable, TypeSuperFoldable};\n use rustc_middle::ty::{InternalSubsts, SubstsRef};\n+use rustc_target::abi::{Integer, IntegerType};\n \n use rustc_ast::ast;\n-use rustc_attr as attr;\n \n use rustc_hir::def_id::DefId;\n \n@@ -218,21 +218,21 @@ impl<'tcx> chalk_solve::RustIrDatabase<RustInterner<'tcx>> for RustIrDatabase<'t\n             c: adt_def.repr().c(),\n             packed: adt_def.repr().packed(),\n             int: adt_def.repr().int.map(|i| match i {\n-                attr::IntType::SignedInt(ty) => match ty {\n-                    ast::IntTy::Isize => int(chalk_ir::IntTy::Isize),\n-                    ast::IntTy::I8 => int(chalk_ir::IntTy::I8),\n-                    ast::IntTy::I16 => int(chalk_ir::IntTy::I16),\n-                    ast::IntTy::I32 => int(chalk_ir::IntTy::I32),\n-                    ast::IntTy::I64 => int(chalk_ir::IntTy::I64),\n-                    ast::IntTy::I128 => int(chalk_ir::IntTy::I128),\n+                IntegerType::Pointer(true) => int(chalk_ir::IntTy::Isize),\n+                IntegerType::Pointer(false) => uint(chalk_ir::UintTy::Usize),\n+                IntegerType::Fixed(i, true) => match i {\n+                    Integer::I8 => int(chalk_ir::IntTy::I8),\n+                    Integer::I16 => int(chalk_ir::IntTy::I16),\n+                    Integer::I32 => int(chalk_ir::IntTy::I32),\n+                    Integer::I64 => int(chalk_ir::IntTy::I64),\n+                    Integer::I128 => int(chalk_ir::IntTy::I128),\n                 },\n-                attr::IntType::UnsignedInt(ty) => match ty {\n-                    ast::UintTy::Usize => uint(chalk_ir::UintTy::Usize),\n-                    ast::UintTy::U8 => uint(chalk_ir::UintTy::U8),\n-                    ast::UintTy::U16 => uint(chalk_ir::UintTy::U16),\n-                    ast::UintTy::U32 => uint(chalk_ir::UintTy::U32),\n-                    ast::UintTy::U64 => uint(chalk_ir::UintTy::U64),\n-                    ast::UintTy::U128 => uint(chalk_ir::UintTy::U128),\n+                IntegerType::Fixed(i, false) => match i {\n+                    Integer::I8 => uint(chalk_ir::UintTy::U8),\n+                    Integer::I16 => uint(chalk_ir::UintTy::U16),\n+                    Integer::I32 => uint(chalk_ir::UintTy::U32),\n+                    Integer::I64 => uint(chalk_ir::UintTy::U64),\n+                    Integer::I128 => uint(chalk_ir::UintTy::U128),\n                 },\n             }),\n         })"}, {"sha": "52fbd3ae047732c18b504514e4c210baec942559", "filename": "compiler/rustc_ty_utils/Cargo.toml", "status": "modified", "additions": 0, "deletions": 2, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/b3bc6bf31265ac10946a0832092dbcedf9b26805/compiler%2Frustc_ty_utils%2FCargo.toml", "raw_url": "https://github.com/rust-lang/rust/raw/b3bc6bf31265ac10946a0832092dbcedf9b26805/compiler%2Frustc_ty_utils%2FCargo.toml", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_ty_utils%2FCargo.toml?ref=b3bc6bf31265ac10946a0832092dbcedf9b26805", "patch": "@@ -4,8 +4,6 @@ version = \"0.0.0\"\n edition = \"2021\"\n \n [dependencies]\n-rand = \"0.8.4\"\n-rand_xoshiro = \"0.6.0\"\n tracing = \"0.1\"\n rustc_middle = { path = \"../rustc_middle\" }\n rustc_data_structures = { path = \"../rustc_data_structures\" }"}, {"sha": "7a1cc1e9e6dedfd4d26be986f3aa463281865898", "filename": "compiler/rustc_ty_utils/src/layout.rs", "status": "modified", "additions": 35, "deletions": 943, "changes": 978, "blob_url": "https://github.com/rust-lang/rust/blob/b3bc6bf31265ac10946a0832092dbcedf9b26805/compiler%2Frustc_ty_utils%2Fsrc%2Flayout.rs", "raw_url": "https://github.com/rust-lang/rust/raw/b3bc6bf31265ac10946a0832092dbcedf9b26805/compiler%2Frustc_ty_utils%2Fsrc%2Flayout.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_ty_utils%2Fsrc%2Flayout.rs?ref=b3bc6bf31265ac10946a0832092dbcedf9b26805", "patch": "@@ -13,13 +13,8 @@ use rustc_span::symbol::Symbol;\n use rustc_span::DUMMY_SP;\n use rustc_target::abi::*;\n \n-use std::cmp::{self, Ordering};\n+use std::fmt::Debug;\n use std::iter;\n-use std::num::NonZeroUsize;\n-use std::ops::Bound;\n-\n-use rand::{seq::SliceRandom, SeedableRng};\n-use rand_xoshiro::Xoshiro128StarStar;\n \n use crate::layout_sanity_check::sanity_check_layout;\n \n@@ -66,16 +61,6 @@ fn layout_of<'tcx>(\n     Ok(layout)\n }\n \n-#[derive(Copy, Clone, Debug)]\n-enum StructKind {\n-    /// A tuple, closure, or univariant which cannot be coerced to unsized.\n-    AlwaysSized,\n-    /// A univariant, the last field of which may be coerced to unsized.\n-    MaybeUnsized,\n-    /// A univariant, but with a prefix of an arbitrary size & alignment (e.g., enum tag).\n-    Prefixed(Size, Align),\n-}\n-\n // Invert a bijective mapping, i.e. `invert(map)[y] = x` if `map[x] = y`.\n // This is used to go between `memory_index` (source field order to memory order)\n // and `inverse_memory_index` (memory order to source field order).\n@@ -89,267 +74,21 @@ fn invert_mapping(map: &[u32]) -> Vec<u32> {\n     inverse\n }\n \n-fn scalar_pair<'tcx>(cx: &LayoutCx<'tcx, TyCtxt<'tcx>>, a: Scalar, b: Scalar) -> LayoutS<'tcx> {\n-    let dl = cx.data_layout();\n-    let b_align = b.align(dl);\n-    let align = a.align(dl).max(b_align).max(dl.aggregate_align);\n-    let b_offset = a.size(dl).align_to(b_align.abi);\n-    let size = (b_offset + b.size(dl)).align_to(align.abi);\n-\n-    // HACK(nox): We iter on `b` and then `a` because `max_by_key`\n-    // returns the last maximum.\n-    let largest_niche = Niche::from_scalar(dl, b_offset, b)\n-        .into_iter()\n-        .chain(Niche::from_scalar(dl, Size::ZERO, a))\n-        .max_by_key(|niche| niche.available(dl));\n-\n-    LayoutS {\n-        variants: Variants::Single { index: VariantIdx::new(0) },\n-        fields: FieldsShape::Arbitrary {\n-            offsets: vec![Size::ZERO, b_offset],\n-            memory_index: vec![0, 1],\n-        },\n-        abi: Abi::ScalarPair(a, b),\n-        largest_niche,\n-        align,\n-        size,\n-    }\n-}\n-\n fn univariant_uninterned<'tcx>(\n     cx: &LayoutCx<'tcx, TyCtxt<'tcx>>,\n     ty: Ty<'tcx>,\n     fields: &[TyAndLayout<'_>],\n     repr: &ReprOptions,\n     kind: StructKind,\n-) -> Result<LayoutS<'tcx>, LayoutError<'tcx>> {\n+) -> Result<LayoutS<VariantIdx>, LayoutError<'tcx>> {\n     let dl = cx.data_layout();\n     let pack = repr.pack;\n     if pack.is_some() && repr.align.is_some() {\n         cx.tcx.sess.delay_span_bug(DUMMY_SP, \"struct cannot be packed and aligned\");\n         return Err(LayoutError::Unknown(ty));\n     }\n \n-    let mut align = if pack.is_some() { dl.i8_align } else { dl.aggregate_align };\n-\n-    let mut inverse_memory_index: Vec<u32> = (0..fields.len() as u32).collect();\n-\n-    let optimize = !repr.inhibit_struct_field_reordering_opt();\n-    if optimize {\n-        let end = if let StructKind::MaybeUnsized = kind { fields.len() - 1 } else { fields.len() };\n-        let optimizing = &mut inverse_memory_index[..end];\n-        let effective_field_align = |f: &TyAndLayout<'_>| {\n-            if let Some(pack) = pack {\n-                // return the packed alignment in bytes\n-                f.align.abi.min(pack).bytes()\n-            } else {\n-                // returns log2(effective-align).\n-                // This is ok since `pack` applies to all fields equally.\n-                // The calculation assumes that size is an integer multiple of align, except for ZSTs.\n-                //\n-                // group [u8; 4] with align-4 or [u8; 6] with align-2 fields\n-                f.align.abi.bytes().max(f.size.bytes()).trailing_zeros() as u64\n-            }\n-        };\n-\n-        // If `-Z randomize-layout` was enabled for the type definition we can shuffle\n-        // the field ordering to try and catch some code making assumptions about layouts\n-        // we don't guarantee\n-        if repr.can_randomize_type_layout() {\n-            // `ReprOptions.layout_seed` is a deterministic seed that we can use to\n-            // randomize field ordering with\n-            let mut rng = Xoshiro128StarStar::seed_from_u64(repr.field_shuffle_seed);\n-\n-            // Shuffle the ordering of the fields\n-            optimizing.shuffle(&mut rng);\n-\n-            // Otherwise we just leave things alone and actually optimize the type's fields\n-        } else {\n-            match kind {\n-                StructKind::AlwaysSized | StructKind::MaybeUnsized => {\n-                    optimizing.sort_by_key(|&x| {\n-                        // Place ZSTs first to avoid \"interesting offsets\",\n-                        // especially with only one or two non-ZST fields.\n-                        // Then place largest alignments first, largest niches within an alignment group last\n-                        let f = &fields[x as usize];\n-                        let niche_size = f.largest_niche.map_or(0, |n| n.available(cx));\n-                        (!f.is_zst(), cmp::Reverse(effective_field_align(f)), niche_size)\n-                    });\n-                }\n-\n-                StructKind::Prefixed(..) => {\n-                    // Sort in ascending alignment so that the layout stays optimal\n-                    // regardless of the prefix.\n-                    // And put the largest niche in an alignment group at the end\n-                    // so it can be used as discriminant in jagged enums\n-                    optimizing.sort_by_key(|&x| {\n-                        let f = &fields[x as usize];\n-                        let niche_size = f.largest_niche.map_or(0, |n| n.available(cx));\n-                        (effective_field_align(f), niche_size)\n-                    });\n-                }\n-            }\n-\n-            // FIXME(Kixiron): We can always shuffle fields within a given alignment class\n-            //                 regardless of the status of `-Z randomize-layout`\n-        }\n-    }\n-\n-    // inverse_memory_index holds field indices by increasing memory offset.\n-    // That is, if field 5 has offset 0, the first element of inverse_memory_index is 5.\n-    // We now write field offsets to the corresponding offset slot;\n-    // field 5 with offset 0 puts 0 in offsets[5].\n-    // At the bottom of this function, we invert `inverse_memory_index` to\n-    // produce `memory_index` (see `invert_mapping`).\n-\n-    let mut sized = true;\n-    let mut offsets = vec![Size::ZERO; fields.len()];\n-    let mut offset = Size::ZERO;\n-    let mut largest_niche = None;\n-    let mut largest_niche_available = 0;\n-\n-    if let StructKind::Prefixed(prefix_size, prefix_align) = kind {\n-        let prefix_align =\n-            if let Some(pack) = pack { prefix_align.min(pack) } else { prefix_align };\n-        align = align.max(AbiAndPrefAlign::new(prefix_align));\n-        offset = prefix_size.align_to(prefix_align);\n-    }\n-\n-    for &i in &inverse_memory_index {\n-        let field = fields[i as usize];\n-        if !sized {\n-            cx.tcx.sess.delay_span_bug(\n-                DUMMY_SP,\n-                &format!(\n-                    \"univariant: field #{} of `{}` comes after unsized field\",\n-                    offsets.len(),\n-                    ty\n-                ),\n-            );\n-        }\n-\n-        if field.is_unsized() {\n-            sized = false;\n-        }\n-\n-        // Invariant: offset < dl.obj_size_bound() <= 1<<61\n-        let field_align = if let Some(pack) = pack {\n-            field.align.min(AbiAndPrefAlign::new(pack))\n-        } else {\n-            field.align\n-        };\n-        offset = offset.align_to(field_align.abi);\n-        align = align.max(field_align);\n-\n-        debug!(\"univariant offset: {:?} field: {:#?}\", offset, field);\n-        offsets[i as usize] = offset;\n-\n-        if let Some(mut niche) = field.largest_niche {\n-            let available = niche.available(dl);\n-            if available > largest_niche_available {\n-                largest_niche_available = available;\n-                niche.offset += offset;\n-                largest_niche = Some(niche);\n-            }\n-        }\n-\n-        offset = offset.checked_add(field.size, dl).ok_or(LayoutError::SizeOverflow(ty))?;\n-    }\n-\n-    if let Some(repr_align) = repr.align {\n-        align = align.max(AbiAndPrefAlign::new(repr_align));\n-    }\n-\n-    debug!(\"univariant min_size: {:?}\", offset);\n-    let min_size = offset;\n-\n-    // As stated above, inverse_memory_index holds field indices by increasing offset.\n-    // This makes it an already-sorted view of the offsets vec.\n-    // To invert it, consider:\n-    // If field 5 has offset 0, offsets[0] is 5, and memory_index[5] should be 0.\n-    // Field 5 would be the first element, so memory_index is i:\n-    // Note: if we didn't optimize, it's already right.\n-\n-    let memory_index =\n-        if optimize { invert_mapping(&inverse_memory_index) } else { inverse_memory_index };\n-\n-    let size = min_size.align_to(align.abi);\n-    let mut abi = Abi::Aggregate { sized };\n-\n-    // Unpack newtype ABIs and find scalar pairs.\n-    if sized && size.bytes() > 0 {\n-        // All other fields must be ZSTs.\n-        let mut non_zst_fields = fields.iter().enumerate().filter(|&(_, f)| !f.is_zst());\n-\n-        match (non_zst_fields.next(), non_zst_fields.next(), non_zst_fields.next()) {\n-            // We have exactly one non-ZST field.\n-            (Some((i, field)), None, None) => {\n-                // Field fills the struct and it has a scalar or scalar pair ABI.\n-                if offsets[i].bytes() == 0 && align.abi == field.align.abi && size == field.size {\n-                    match field.abi {\n-                        // For plain scalars, or vectors of them, we can't unpack\n-                        // newtypes for `#[repr(C)]`, as that affects C ABIs.\n-                        Abi::Scalar(_) | Abi::Vector { .. } if optimize => {\n-                            abi = field.abi;\n-                        }\n-                        // But scalar pairs are Rust-specific and get\n-                        // treated as aggregates by C ABIs anyway.\n-                        Abi::ScalarPair(..) => {\n-                            abi = field.abi;\n-                        }\n-                        _ => {}\n-                    }\n-                }\n-            }\n-\n-            // Two non-ZST fields, and they're both scalars.\n-            (Some((i, a)), Some((j, b)), None) => {\n-                match (a.abi, b.abi) {\n-                    (Abi::Scalar(a), Abi::Scalar(b)) => {\n-                        // Order by the memory placement, not source order.\n-                        let ((i, a), (j, b)) = if offsets[i] < offsets[j] {\n-                            ((i, a), (j, b))\n-                        } else {\n-                            ((j, b), (i, a))\n-                        };\n-                        let pair = scalar_pair(cx, a, b);\n-                        let pair_offsets = match pair.fields {\n-                            FieldsShape::Arbitrary { ref offsets, ref memory_index } => {\n-                                assert_eq!(memory_index, &[0, 1]);\n-                                offsets\n-                            }\n-                            _ => bug!(),\n-                        };\n-                        if offsets[i] == pair_offsets[0]\n-                            && offsets[j] == pair_offsets[1]\n-                            && align == pair.align\n-                            && size == pair.size\n-                        {\n-                            // We can use `ScalarPair` only when it matches our\n-                            // already computed layout (including `#[repr(C)]`).\n-                            abi = pair.abi;\n-                        }\n-                    }\n-                    _ => {}\n-                }\n-            }\n-\n-            _ => {}\n-        }\n-    }\n-\n-    if fields.iter().any(|f| f.abi.is_uninhabited()) {\n-        abi = Abi::Uninhabited;\n-    }\n-\n-    Ok(LayoutS {\n-        variants: Variants::Single { index: VariantIdx::new(0) },\n-        fields: FieldsShape::Arbitrary { offsets, memory_index },\n-        abi,\n-        largest_niche,\n-        align,\n-        size,\n-    })\n+    cx.univariant(dl, fields, repr, kind).ok_or(LayoutError::SizeOverflow(ty))\n }\n \n fn layout_of_uncached<'tcx>(\n@@ -400,14 +139,7 @@ fn layout_of_uncached<'tcx>(\n         }\n \n         // The never type.\n-        ty::Never => tcx.intern_layout(LayoutS {\n-            variants: Variants::Single { index: VariantIdx::new(0) },\n-            fields: FieldsShape::Primitive,\n-            abi: Abi::Uninhabited,\n-            largest_niche: None,\n-            align: dl.i8_align,\n-            size: Size::ZERO,\n-        }),\n+        ty::Never => tcx.intern_layout(cx.layout_of_never_type()),\n \n         // Potentially-wide pointers.\n         ty::Ref(_, pointee, _) | ty::RawPtr(ty::TypeAndMut { ty: pointee, .. }) => {\n@@ -436,15 +168,15 @@ fn layout_of_uncached<'tcx>(\n             };\n \n             // Effectively a (ptr, meta) tuple.\n-            tcx.intern_layout(scalar_pair(cx, data_ptr, metadata))\n+            tcx.intern_layout(cx.scalar_pair(data_ptr, metadata))\n         }\n \n         ty::Dynamic(_, _, ty::DynStar) => {\n             let mut data = scalar_unit(Int(dl.ptr_sized_integer(), false));\n             data.valid_range_mut().start = 0;\n             let mut vtable = scalar_unit(Pointer);\n             vtable.valid_range_mut().start = 1;\n-            tcx.intern_layout(scalar_pair(cx, data, vtable))\n+            tcx.intern_layout(cx.scalar_pair(data, vtable))\n         }\n \n         // Arrays and slices.\n@@ -673,681 +405,41 @@ fn layout_of_uncached<'tcx>(\n                     return Err(LayoutError::Unknown(ty));\n                 }\n \n-                let mut align =\n-                    if def.repr().pack.is_some() { dl.i8_align } else { dl.aggregate_align };\n-\n-                if let Some(repr_align) = def.repr().align {\n-                    align = align.max(AbiAndPrefAlign::new(repr_align));\n-                }\n-\n-                let optimize = !def.repr().inhibit_union_abi_opt();\n-                let mut size = Size::ZERO;\n-                let mut abi = Abi::Aggregate { sized: true };\n-                let index = VariantIdx::new(0);\n-                for field in &variants[index] {\n-                    assert!(field.is_sized());\n-                    align = align.max(field.align);\n-\n-                    // If all non-ZST fields have the same ABI, forward this ABI\n-                    if optimize && !field.is_zst() {\n-                        // Discard valid range information and allow undef\n-                        let field_abi = match field.abi {\n-                            Abi::Scalar(x) => Abi::Scalar(x.to_union()),\n-                            Abi::ScalarPair(x, y) => Abi::ScalarPair(x.to_union(), y.to_union()),\n-                            Abi::Vector { element: x, count } => {\n-                                Abi::Vector { element: x.to_union(), count }\n-                            }\n-                            Abi::Uninhabited | Abi::Aggregate { .. } => {\n-                                Abi::Aggregate { sized: true }\n-                            }\n-                        };\n-\n-                        if size == Size::ZERO {\n-                            // first non ZST: initialize 'abi'\n-                            abi = field_abi;\n-                        } else if abi != field_abi {\n-                            // different fields have different ABI: reset to Aggregate\n-                            abi = Abi::Aggregate { sized: true };\n-                        }\n-                    }\n-\n-                    size = cmp::max(size, field.size);\n-                }\n-\n-                if let Some(pack) = def.repr().pack {\n-                    align = align.min(AbiAndPrefAlign::new(pack));\n-                }\n-\n-                return Ok(tcx.intern_layout(LayoutS {\n-                    variants: Variants::Single { index },\n-                    fields: FieldsShape::Union(\n-                        NonZeroUsize::new(variants[index].len()).ok_or(LayoutError::Unknown(ty))?,\n-                    ),\n-                    abi,\n-                    largest_niche: None,\n-                    align,\n-                    size: size.align_to(align.abi),\n-                }));\n-            }\n-\n-            // A variant is absent if it's uninhabited and only has ZST fields.\n-            // Present uninhabited variants only require space for their fields,\n-            // but *not* an encoding of the discriminant (e.g., a tag value).\n-            // See issue #49298 for more details on the need to leave space\n-            // for non-ZST uninhabited data (mostly partial initialization).\n-            let absent = |fields: &[TyAndLayout<'_>]| {\n-                let uninhabited = fields.iter().any(|f| f.abi.is_uninhabited());\n-                let is_zst = fields.iter().all(|f| f.is_zst());\n-                uninhabited && is_zst\n-            };\n-            let (present_first, present_second) = {\n-                let mut present_variants = variants\n-                    .iter_enumerated()\n-                    .filter_map(|(i, v)| if absent(v) { None } else { Some(i) });\n-                (present_variants.next(), present_variants.next())\n-            };\n-            let present_first = match present_first {\n-                Some(present_first) => present_first,\n-                // Uninhabited because it has no variants, or only absent ones.\n-                None if def.is_enum() => {\n-                    return Ok(tcx.layout_of(param_env.and(tcx.types.never))?.layout);\n-                }\n-                // If it's a struct, still compute a layout so that we can still compute the\n-                // field offsets.\n-                None => VariantIdx::new(0),\n-            };\n-\n-            let is_struct = !def.is_enum() ||\n-                    // Only one variant is present.\n-                    (present_second.is_none() &&\n-                        // Representation optimizations are allowed.\n-                        !def.repr().inhibit_enum_layout_opt());\n-            if is_struct {\n-                // Struct, or univariant enum equivalent to a struct.\n-                // (Typechecking will reject discriminant-sizing attrs.)\n-\n-                let v = present_first;\n-                let kind = if def.is_enum() || variants[v].is_empty() {\n-                    StructKind::AlwaysSized\n-                } else {\n-                    let param_env = tcx.param_env(def.did());\n-                    let last_field = def.variant(v).fields.last().unwrap();\n-                    let always_sized = tcx.type_of(last_field.did).is_sized(tcx, param_env);\n-                    if !always_sized { StructKind::MaybeUnsized } else { StructKind::AlwaysSized }\n-                };\n-\n-                let mut st = univariant_uninterned(cx, ty, &variants[v], &def.repr(), kind)?;\n-                st.variants = Variants::Single { index: v };\n-\n-                if def.is_unsafe_cell() {\n-                    let hide_niches = |scalar: &mut _| match scalar {\n-                        Scalar::Initialized { value, valid_range } => {\n-                            *valid_range = WrappingRange::full(value.size(dl))\n-                        }\n-                        // Already doesn't have any niches\n-                        Scalar::Union { .. } => {}\n-                    };\n-                    match &mut st.abi {\n-                        Abi::Uninhabited => {}\n-                        Abi::Scalar(scalar) => hide_niches(scalar),\n-                        Abi::ScalarPair(a, b) => {\n-                            hide_niches(a);\n-                            hide_niches(b);\n-                        }\n-                        Abi::Vector { element, count: _ } => hide_niches(element),\n-                        Abi::Aggregate { sized: _ } => {}\n-                    }\n-                    st.largest_niche = None;\n-                    return Ok(tcx.intern_layout(st));\n-                }\n-\n-                let (start, end) = cx.tcx.layout_scalar_valid_range(def.did());\n-                match st.abi {\n-                    Abi::Scalar(ref mut scalar) | Abi::ScalarPair(ref mut scalar, _) => {\n-                        // the asserts ensure that we are not using the\n-                        // `#[rustc_layout_scalar_valid_range(n)]`\n-                        // attribute to widen the range of anything as that would probably\n-                        // result in UB somewhere\n-                        // FIXME(eddyb) the asserts are probably not needed,\n-                        // as larger validity ranges would result in missed\n-                        // optimizations, *not* wrongly assuming the inner\n-                        // value is valid. e.g. unions enlarge validity ranges,\n-                        // because the values may be uninitialized.\n-                        if let Bound::Included(start) = start {\n-                            // FIXME(eddyb) this might be incorrect - it doesn't\n-                            // account for wrap-around (end < start) ranges.\n-                            let valid_range = scalar.valid_range_mut();\n-                            assert!(valid_range.start <= start);\n-                            valid_range.start = start;\n-                        }\n-                        if let Bound::Included(end) = end {\n-                            // FIXME(eddyb) this might be incorrect - it doesn't\n-                            // account for wrap-around (end < start) ranges.\n-                            let valid_range = scalar.valid_range_mut();\n-                            assert!(valid_range.end >= end);\n-                            valid_range.end = end;\n-                        }\n-\n-                        // Update `largest_niche` if we have introduced a larger niche.\n-                        let niche = Niche::from_scalar(dl, Size::ZERO, *scalar);\n-                        if let Some(niche) = niche {\n-                            match st.largest_niche {\n-                                Some(largest_niche) => {\n-                                    // Replace the existing niche even if they're equal,\n-                                    // because this one is at a lower offset.\n-                                    if largest_niche.available(dl) <= niche.available(dl) {\n-                                        st.largest_niche = Some(niche);\n-                                    }\n-                                }\n-                                None => st.largest_niche = Some(niche),\n-                            }\n-                        }\n-                    }\n-                    _ => assert!(\n-                        start == Bound::Unbounded && end == Bound::Unbounded,\n-                        \"nonscalar layout for layout_scalar_valid_range type {:?}: {:#?}\",\n-                        def,\n-                        st,\n-                    ),\n-                }\n-\n-                return Ok(tcx.intern_layout(st));\n-            }\n-\n-            // At this point, we have handled all unions and\n-            // structs. (We have also handled univariant enums\n-            // that allow representation optimization.)\n-            assert!(def.is_enum());\n-\n-            // Until we've decided whether to use the tagged or\n-            // niche filling LayoutS, we don't want to intern the\n-            // variant layouts, so we can't store them in the\n-            // overall LayoutS. Store the overall LayoutS\n-            // and the variant LayoutSs here until then.\n-            struct TmpLayout<'tcx> {\n-                layout: LayoutS<'tcx>,\n-                variants: IndexVec<VariantIdx, LayoutS<'tcx>>,\n+                return Ok(tcx.intern_layout(\n+                    cx.layout_of_union(&def.repr(), &variants).ok_or(LayoutError::Unknown(ty))?,\n+                ));\n             }\n \n-            let calculate_niche_filling_layout =\n-                || -> Result<Option<TmpLayout<'tcx>>, LayoutError<'tcx>> {\n-                    // The current code for niche-filling relies on variant indices\n-                    // instead of actual discriminants, so enums with\n-                    // explicit discriminants (RFC #2363) would misbehave.\n-                    if def.repr().inhibit_enum_layout_opt()\n+            tcx.intern_layout(\n+                cx.layout_of_struct_or_enum(\n+                    &def.repr(),\n+                    &variants,\n+                    def.is_enum(),\n+                    def.is_unsafe_cell(),\n+                    tcx.layout_scalar_valid_range(def.did()),\n+                    |min, max| Integer::repr_discr(tcx, ty, &def.repr(), min, max),\n+                    def.is_enum()\n+                        .then(|| def.discriminants(tcx).map(|(v, d)| (v, d.val as i128)))\n+                        .into_iter()\n+                        .flatten(),\n+                    def.repr().inhibit_enum_layout_opt()\n                         || def\n                             .variants()\n                             .iter_enumerated()\n-                            .any(|(i, v)| v.discr != ty::VariantDiscr::Relative(i.as_u32()))\n-                    {\n-                        return Ok(None);\n-                    }\n-\n-                    if variants.len() < 2 {\n-                        return Ok(None);\n-                    }\n-\n-                    let mut align = dl.aggregate_align;\n-                    let mut variant_layouts = variants\n-                        .iter_enumerated()\n-                        .map(|(j, v)| {\n-                            let mut st = univariant_uninterned(\n-                                cx,\n-                                ty,\n-                                v,\n-                                &def.repr(),\n-                                StructKind::AlwaysSized,\n-                            )?;\n-                            st.variants = Variants::Single { index: j };\n-\n-                            align = align.max(st.align);\n-\n-                            Ok(st)\n-                        })\n-                        .collect::<Result<IndexVec<VariantIdx, _>, _>>()?;\n-\n-                    let largest_variant_index = match variant_layouts\n-                        .iter_enumerated()\n-                        .max_by_key(|(_i, layout)| layout.size.bytes())\n-                        .map(|(i, _layout)| i)\n+                            .any(|(i, v)| v.discr != ty::VariantDiscr::Relative(i.as_u32())),\n                     {\n-                        None => return Ok(None),\n-                        Some(i) => i,\n-                    };\n-\n-                    let all_indices = VariantIdx::new(0)..=VariantIdx::new(variants.len() - 1);\n-                    let needs_disc = |index: VariantIdx| {\n-                        index != largest_variant_index && !absent(&variants[index])\n-                    };\n-                    let niche_variants = all_indices.clone().find(|v| needs_disc(*v)).unwrap()\n-                        ..=all_indices.rev().find(|v| needs_disc(*v)).unwrap();\n-\n-                    let count = niche_variants.size_hint().1.unwrap() as u128;\n-\n-                    // Find the field with the largest niche\n-                    let (field_index, niche, (niche_start, niche_scalar)) = match variants\n-                        [largest_variant_index]\n-                        .iter()\n-                        .enumerate()\n-                        .filter_map(|(j, field)| Some((j, field.largest_niche?)))\n-                        .max_by_key(|(_, niche)| niche.available(dl))\n-                        .and_then(|(j, niche)| Some((j, niche, niche.reserve(cx, count)?)))\n-                    {\n-                        None => return Ok(None),\n-                        Some(x) => x,\n-                    };\n-\n-                    let niche_offset = niche.offset\n-                        + variant_layouts[largest_variant_index].fields.offset(field_index);\n-                    let niche_size = niche.value.size(dl);\n-                    let size = variant_layouts[largest_variant_index].size.align_to(align.abi);\n-\n-                    let all_variants_fit =\n-                        variant_layouts.iter_enumerated_mut().all(|(i, layout)| {\n-                            if i == largest_variant_index {\n-                                return true;\n-                            }\n-\n-                            layout.largest_niche = None;\n-\n-                            if layout.size <= niche_offset {\n-                                // This variant will fit before the niche.\n-                                return true;\n-                            }\n-\n-                            // Determine if it'll fit after the niche.\n-                            let this_align = layout.align.abi;\n-                            let this_offset = (niche_offset + niche_size).align_to(this_align);\n-\n-                            if this_offset + layout.size > size {\n-                                return false;\n-                            }\n-\n-                            // It'll fit, but we need to make some adjustments.\n-                            match layout.fields {\n-                                FieldsShape::Arbitrary { ref mut offsets, .. } => {\n-                                    for (j, offset) in offsets.iter_mut().enumerate() {\n-                                        if !variants[i][j].is_zst() {\n-                                            *offset += this_offset;\n-                                        }\n-                                    }\n-                                }\n-                                _ => {\n-                                    panic!(\"Layout of fields should be Arbitrary for variants\")\n+                        let param_env = tcx.param_env(def.did());\n+                        def.is_struct()\n+                            && match def.variants().iter().next().and_then(|x| x.fields.last()) {\n+                                Some(last_field) => {\n+                                    tcx.type_of(last_field.did).is_sized(tcx, param_env)\n                                 }\n+                                None => false,\n                             }\n-\n-                            // It can't be a Scalar or ScalarPair because the offset isn't 0.\n-                            if !layout.abi.is_uninhabited() {\n-                                layout.abi = Abi::Aggregate { sized: true };\n-                            }\n-                            layout.size += this_offset;\n-\n-                            true\n-                        });\n-\n-                    if !all_variants_fit {\n-                        return Ok(None);\n-                    }\n-\n-                    let largest_niche = Niche::from_scalar(dl, niche_offset, niche_scalar);\n-\n-                    let others_zst = variant_layouts\n-                        .iter_enumerated()\n-                        .all(|(i, layout)| i == largest_variant_index || layout.size == Size::ZERO);\n-                    let same_size = size == variant_layouts[largest_variant_index].size;\n-                    let same_align = align == variant_layouts[largest_variant_index].align;\n-\n-                    let abi = if variant_layouts.iter().all(|v| v.abi.is_uninhabited()) {\n-                        Abi::Uninhabited\n-                    } else if same_size && same_align && others_zst {\n-                        match variant_layouts[largest_variant_index].abi {\n-                            // When the total alignment and size match, we can use the\n-                            // same ABI as the scalar variant with the reserved niche.\n-                            Abi::Scalar(_) => Abi::Scalar(niche_scalar),\n-                            Abi::ScalarPair(first, second) => {\n-                                // Only the niche is guaranteed to be initialised,\n-                                // so use union layouts for the other primitive.\n-                                if niche_offset == Size::ZERO {\n-                                    Abi::ScalarPair(niche_scalar, second.to_union())\n-                                } else {\n-                                    Abi::ScalarPair(first.to_union(), niche_scalar)\n-                                }\n-                            }\n-                            _ => Abi::Aggregate { sized: true },\n-                        }\n-                    } else {\n-                        Abi::Aggregate { sized: true }\n-                    };\n-\n-                    let layout = LayoutS {\n-                        variants: Variants::Multiple {\n-                            tag: niche_scalar,\n-                            tag_encoding: TagEncoding::Niche {\n-                                untagged_variant: largest_variant_index,\n-                                niche_variants,\n-                                niche_start,\n-                            },\n-                            tag_field: 0,\n-                            variants: IndexVec::new(),\n-                        },\n-                        fields: FieldsShape::Arbitrary {\n-                            offsets: vec![niche_offset],\n-                            memory_index: vec![0],\n-                        },\n-                        abi,\n-                        largest_niche,\n-                        size,\n-                        align,\n-                    };\n-\n-                    Ok(Some(TmpLayout { layout, variants: variant_layouts }))\n-                };\n-\n-            let niche_filling_layout = calculate_niche_filling_layout()?;\n-\n-            let (mut min, mut max) = (i128::MAX, i128::MIN);\n-            let discr_type = def.repr().discr_type();\n-            let bits = Integer::from_attr(cx, discr_type).size().bits();\n-            for (i, discr) in def.discriminants(tcx) {\n-                if variants[i].iter().any(|f| f.abi.is_uninhabited()) {\n-                    continue;\n-                }\n-                let mut x = discr.val as i128;\n-                if discr_type.is_signed() {\n-                    // sign extend the raw representation to be an i128\n-                    x = (x << (128 - bits)) >> (128 - bits);\n-                }\n-                if x < min {\n-                    min = x;\n-                }\n-                if x > max {\n-                    max = x;\n-                }\n-            }\n-            // We might have no inhabited variants, so pretend there's at least one.\n-            if (min, max) == (i128::MAX, i128::MIN) {\n-                min = 0;\n-                max = 0;\n-            }\n-            assert!(min <= max, \"discriminant range is {}...{}\", min, max);\n-            let (min_ity, signed) = Integer::repr_discr(tcx, ty, &def.repr(), min, max);\n-\n-            let mut align = dl.aggregate_align;\n-            let mut size = Size::ZERO;\n-\n-            // We're interested in the smallest alignment, so start large.\n-            let mut start_align = Align::from_bytes(256).unwrap();\n-            assert_eq!(Integer::for_align(dl, start_align), None);\n-\n-            // repr(C) on an enum tells us to make a (tag, union) layout,\n-            // so we need to grow the prefix alignment to be at least\n-            // the alignment of the union. (This value is used both for\n-            // determining the alignment of the overall enum, and the\n-            // determining the alignment of the payload after the tag.)\n-            let mut prefix_align = min_ity.align(dl).abi;\n-            if def.repr().c() {\n-                for fields in &variants {\n-                    for field in fields {\n-                        prefix_align = prefix_align.max(field.align.abi);\n-                    }\n-                }\n-            }\n-\n-            // Create the set of structs that represent each variant.\n-            let mut layout_variants = variants\n-                .iter_enumerated()\n-                .map(|(i, field_layouts)| {\n-                    let mut st = univariant_uninterned(\n-                        cx,\n-                        ty,\n-                        &field_layouts,\n-                        &def.repr(),\n-                        StructKind::Prefixed(min_ity.size(), prefix_align),\n-                    )?;\n-                    st.variants = Variants::Single { index: i };\n-                    // Find the first field we can't move later\n-                    // to make room for a larger discriminant.\n-                    for field in st.fields.index_by_increasing_offset().map(|j| field_layouts[j]) {\n-                        if !field.is_zst() || field.align.abi.bytes() != 1 {\n-                            start_align = start_align.min(field.align.abi);\n-                            break;\n-                        }\n-                    }\n-                    size = cmp::max(size, st.size);\n-                    align = align.max(st.align);\n-                    Ok(st)\n-                })\n-                .collect::<Result<IndexVec<VariantIdx, _>, _>>()?;\n-\n-            // Align the maximum variant size to the largest alignment.\n-            size = size.align_to(align.abi);\n-\n-            if size.bytes() >= dl.obj_size_bound() {\n-                return Err(LayoutError::SizeOverflow(ty));\n-            }\n-\n-            let typeck_ity = Integer::from_attr(dl, def.repr().discr_type());\n-            if typeck_ity < min_ity {\n-                // It is a bug if Layout decided on a greater discriminant size than typeck for\n-                // some reason at this point (based on values discriminant can take on). Mostly\n-                // because this discriminant will be loaded, and then stored into variable of\n-                // type calculated by typeck. Consider such case (a bug): typeck decided on\n-                // byte-sized discriminant, but layout thinks we need a 16-bit to store all\n-                // discriminant values. That would be a bug, because then, in codegen, in order\n-                // to store this 16-bit discriminant into 8-bit sized temporary some of the\n-                // space necessary to represent would have to be discarded (or layout is wrong\n-                // on thinking it needs 16 bits)\n-                bug!(\n-                    \"layout decided on a larger discriminant type ({:?}) than typeck ({:?})\",\n-                    min_ity,\n-                    typeck_ity\n-                );\n-                // However, it is fine to make discr type however large (as an optimisation)\n-                // after this point \u2013 we\u2019ll just truncate the value we load in codegen.\n-            }\n-\n-            // Check to see if we should use a different type for the\n-            // discriminant. We can safely use a type with the same size\n-            // as the alignment of the first field of each variant.\n-            // We increase the size of the discriminant to avoid LLVM copying\n-            // padding when it doesn't need to. This normally causes unaligned\n-            // load/stores and excessive memcpy/memset operations. By using a\n-            // bigger integer size, LLVM can be sure about its contents and\n-            // won't be so conservative.\n-\n-            // Use the initial field alignment\n-            let mut ity = if def.repr().c() || def.repr().int.is_some() {\n-                min_ity\n-            } else {\n-                Integer::for_align(dl, start_align).unwrap_or(min_ity)\n-            };\n-\n-            // If the alignment is not larger than the chosen discriminant size,\n-            // don't use the alignment as the final size.\n-            if ity <= min_ity {\n-                ity = min_ity;\n-            } else {\n-                // Patch up the variants' first few fields.\n-                let old_ity_size = min_ity.size();\n-                let new_ity_size = ity.size();\n-                for variant in &mut layout_variants {\n-                    match variant.fields {\n-                        FieldsShape::Arbitrary { ref mut offsets, .. } => {\n-                            for i in offsets {\n-                                if *i <= old_ity_size {\n-                                    assert_eq!(*i, old_ity_size);\n-                                    *i = new_ity_size;\n-                                }\n-                            }\n-                            // We might be making the struct larger.\n-                            if variant.size <= old_ity_size {\n-                                variant.size = new_ity_size;\n-                            }\n-                        }\n-                        _ => bug!(),\n-                    }\n-                }\n-            }\n-\n-            let tag_mask = ity.size().unsigned_int_max();\n-            let tag = Scalar::Initialized {\n-                value: Int(ity, signed),\n-                valid_range: WrappingRange {\n-                    start: (min as u128 & tag_mask),\n-                    end: (max as u128 & tag_mask),\n-                },\n-            };\n-            let mut abi = Abi::Aggregate { sized: true };\n-\n-            if layout_variants.iter().all(|v| v.abi.is_uninhabited()) {\n-                abi = Abi::Uninhabited;\n-            } else if tag.size(dl) == size {\n-                // Make sure we only use scalar layout when the enum is entirely its\n-                // own tag (i.e. it has no padding nor any non-ZST variant fields).\n-                abi = Abi::Scalar(tag);\n-            } else {\n-                // Try to use a ScalarPair for all tagged enums.\n-                let mut common_prim = None;\n-                let mut common_prim_initialized_in_all_variants = true;\n-                for (field_layouts, layout_variant) in iter::zip(&variants, &layout_variants) {\n-                    let FieldsShape::Arbitrary { ref offsets, .. } = layout_variant.fields else {\n-                            bug!();\n-                        };\n-                    let mut fields = iter::zip(field_layouts, offsets).filter(|p| !p.0.is_zst());\n-                    let (field, offset) = match (fields.next(), fields.next()) {\n-                        (None, None) => {\n-                            common_prim_initialized_in_all_variants = false;\n-                            continue;\n-                        }\n-                        (Some(pair), None) => pair,\n-                        _ => {\n-                            common_prim = None;\n-                            break;\n-                        }\n-                    };\n-                    let prim = match field.abi {\n-                        Abi::Scalar(scalar) => {\n-                            common_prim_initialized_in_all_variants &=\n-                                matches!(scalar, Scalar::Initialized { .. });\n-                            scalar.primitive()\n-                        }\n-                        _ => {\n-                            common_prim = None;\n-                            break;\n-                        }\n-                    };\n-                    if let Some(pair) = common_prim {\n-                        // This is pretty conservative. We could go fancier\n-                        // by conflating things like i32 and u32, or even\n-                        // realising that (u8, u8) could just cohabit with\n-                        // u16 or even u32.\n-                        if pair != (prim, offset) {\n-                            common_prim = None;\n-                            break;\n-                        }\n-                    } else {\n-                        common_prim = Some((prim, offset));\n-                    }\n-                }\n-                if let Some((prim, offset)) = common_prim {\n-                    let prim_scalar = if common_prim_initialized_in_all_variants {\n-                        scalar_unit(prim)\n-                    } else {\n-                        // Common prim might be uninit.\n-                        Scalar::Union { value: prim }\n-                    };\n-                    let pair = scalar_pair(cx, tag, prim_scalar);\n-                    let pair_offsets = match pair.fields {\n-                        FieldsShape::Arbitrary { ref offsets, ref memory_index } => {\n-                            assert_eq!(memory_index, &[0, 1]);\n-                            offsets\n-                        }\n-                        _ => bug!(),\n-                    };\n-                    if pair_offsets[0] == Size::ZERO\n-                        && pair_offsets[1] == *offset\n-                        && align == pair.align\n-                        && size == pair.size\n-                    {\n-                        // We can use `ScalarPair` only when it matches our\n-                        // already computed layout (including `#[repr(C)]`).\n-                        abi = pair.abi;\n-                    }\n-                }\n-            }\n-\n-            // If we pick a \"clever\" (by-value) ABI, we might have to adjust the ABI of the\n-            // variants to ensure they are consistent. This is because a downcast is\n-            // semantically a NOP, and thus should not affect layout.\n-            if matches!(abi, Abi::Scalar(..) | Abi::ScalarPair(..)) {\n-                for variant in &mut layout_variants {\n-                    // We only do this for variants with fields; the others are not accessed anyway.\n-                    // Also do not overwrite any already existing \"clever\" ABIs.\n-                    if variant.fields.count() > 0 && matches!(variant.abi, Abi::Aggregate { .. }) {\n-                        variant.abi = abi;\n-                        // Also need to bump up the size and alignment, so that the entire value fits in here.\n-                        variant.size = cmp::max(variant.size, size);\n-                        variant.align.abi = cmp::max(variant.align.abi, align.abi);\n-                    }\n-                }\n-            }\n-\n-            let largest_niche = Niche::from_scalar(dl, Size::ZERO, tag);\n-\n-            let tagged_layout = LayoutS {\n-                variants: Variants::Multiple {\n-                    tag,\n-                    tag_encoding: TagEncoding::Direct,\n-                    tag_field: 0,\n-                    variants: IndexVec::new(),\n-                },\n-                fields: FieldsShape::Arbitrary { offsets: vec![Size::ZERO], memory_index: vec![0] },\n-                largest_niche,\n-                abi,\n-                align,\n-                size,\n-            };\n-\n-            let tagged_layout = TmpLayout { layout: tagged_layout, variants: layout_variants };\n-\n-            let mut best_layout = match (tagged_layout, niche_filling_layout) {\n-                (tl, Some(nl)) => {\n-                    // Pick the smaller layout; otherwise,\n-                    // pick the layout with the larger niche; otherwise,\n-                    // pick tagged as it has simpler codegen.\n-                    use Ordering::*;\n-                    let niche_size = |tmp_l: &TmpLayout<'_>| {\n-                        tmp_l.layout.largest_niche.map_or(0, |n| n.available(dl))\n-                    };\n-                    match (\n-                        tl.layout.size.cmp(&nl.layout.size),\n-                        niche_size(&tl).cmp(&niche_size(&nl)),\n-                    ) {\n-                        (Greater, _) => nl,\n-                        (Equal, Less) => nl,\n-                        _ => tl,\n-                    }\n-                }\n-                (tl, None) => tl,\n-            };\n-\n-            // Now we can intern the variant layouts and store them in the enum layout.\n-            best_layout.layout.variants = match best_layout.layout.variants {\n-                Variants::Multiple { tag, tag_encoding, tag_field, .. } => Variants::Multiple {\n-                    tag,\n-                    tag_encoding,\n-                    tag_field,\n-                    variants: best_layout\n-                        .variants\n-                        .into_iter()\n-                        .map(|layout| tcx.intern_layout(layout))\n-                        .collect(),\n-                },\n-                _ => bug!(),\n-            };\n-\n-            tcx.intern_layout(best_layout.layout)\n+                    },\n+                )\n+                .ok_or(LayoutError::SizeOverflow(ty))?,\n+            )\n         }\n \n         // Types with no meaningful known layout.\n@@ -1657,13 +749,13 @@ fn generator_layout<'tcx>(\n \n             size = size.max(variant.size);\n             align = align.max(variant.align);\n-            Ok(tcx.intern_layout(variant))\n+            Ok(variant)\n         })\n         .collect::<Result<IndexVec<VariantIdx, _>, _>>()?;\n \n     size = size.align_to(align.abi);\n \n-    let abi = if prefix.abi.is_uninhabited() || variants.iter().all(|v| v.abi().is_uninhabited()) {\n+    let abi = if prefix.abi.is_uninhabited() || variants.iter().all(|v| v.abi.is_uninhabited()) {\n         Abi::Uninhabited\n     } else {\n         Abi::Aggregate { sized: true }"}, {"sha": "9eb8f684bdb59fc2b81bcaca1e3a09f2d8b166d6", "filename": "compiler/rustc_ty_utils/src/layout_sanity_check.rs", "status": "modified", "additions": 9, "deletions": 9, "changes": 18, "blob_url": "https://github.com/rust-lang/rust/blob/b3bc6bf31265ac10946a0832092dbcedf9b26805/compiler%2Frustc_ty_utils%2Fsrc%2Flayout_sanity_check.rs", "raw_url": "https://github.com/rust-lang/rust/raw/b3bc6bf31265ac10946a0832092dbcedf9b26805/compiler%2Frustc_ty_utils%2Fsrc%2Flayout_sanity_check.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_ty_utils%2Fsrc%2Flayout_sanity_check.rs?ref=b3bc6bf31265ac10946a0832092dbcedf9b26805", "patch": "@@ -249,27 +249,27 @@ pub(super) fn sanity_check_layout<'tcx>(\n         if let Variants::Multiple { variants, .. } = &layout.variants {\n             for variant in variants.iter() {\n                 // No nested \"multiple\".\n-                assert!(matches!(variant.variants(), Variants::Single { .. }));\n+                assert!(matches!(variant.variants, Variants::Single { .. }));\n                 // Variants should have the same or a smaller size as the full thing,\n                 // and same for alignment.\n-                if variant.size() > layout.size {\n+                if variant.size > layout.size {\n                     bug!(\n                         \"Type with size {} bytes has variant with size {} bytes: {layout:#?}\",\n                         layout.size.bytes(),\n-                        variant.size().bytes(),\n+                        variant.size.bytes(),\n                     )\n                 }\n-                if variant.align().abi > layout.align.abi {\n+                if variant.align.abi > layout.align.abi {\n                     bug!(\n                         \"Type with alignment {} bytes has variant with alignment {} bytes: {layout:#?}\",\n                         layout.align.abi.bytes(),\n-                        variant.align().abi.bytes(),\n+                        variant.align.abi.bytes(),\n                     )\n                 }\n                 // Skip empty variants.\n-                if variant.size() == Size::ZERO\n-                    || variant.fields().count() == 0\n-                    || variant.abi().is_uninhabited()\n+                if variant.size == Size::ZERO\n+                    || variant.fields.count() == 0\n+                    || variant.abi.is_uninhabited()\n                 {\n                     // These are never actually accessed anyway, so we can skip the coherence check\n                     // for them. They also fail that check, since they have\n@@ -282,7 +282,7 @@ pub(super) fn sanity_check_layout<'tcx>(\n                 let scalar_coherent = |s1: Scalar, s2: Scalar| {\n                     s1.size(cx) == s2.size(cx) && s1.align(cx) == s2.align(cx)\n                 };\n-                let abi_coherent = match (layout.abi, variant.abi()) {\n+                let abi_coherent = match (layout.abi, variant.abi) {\n                     (Abi::Scalar(s1), Abi::Scalar(s2)) => scalar_coherent(s1, s2),\n                     (Abi::ScalarPair(a1, b1), Abi::ScalarPair(a2, b2)) => {\n                         scalar_coherent(a1, a2) && scalar_coherent(b1, b2)"}, {"sha": "acbe3f22889cc03d10cf4f541477c8f6a2b9d8f7", "filename": "src/librustdoc/html/render/print_item.rs", "status": "modified", "additions": 6, "deletions": 6, "changes": 12, "blob_url": "https://github.com/rust-lang/rust/blob/b3bc6bf31265ac10946a0832092dbcedf9b26805/src%2Flibrustdoc%2Fhtml%2Frender%2Fprint_item.rs", "raw_url": "https://github.com/rust-lang/rust/raw/b3bc6bf31265ac10946a0832092dbcedf9b26805/src%2Flibrustdoc%2Fhtml%2Frender%2Fprint_item.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustdoc%2Fhtml%2Frender%2Fprint_item.rs?ref=b3bc6bf31265ac10946a0832092dbcedf9b26805", "patch": "@@ -10,7 +10,7 @@ use rustc_middle::ty::layout::LayoutError;\n use rustc_middle::ty::{self, Adt, TyCtxt};\n use rustc_span::hygiene::MacroKind;\n use rustc_span::symbol::{kw, sym, Symbol};\n-use rustc_target::abi::{Layout, Primitive, TagEncoding, Variants};\n+use rustc_target::abi::{LayoutS, Primitive, TagEncoding, VariantIdx, Variants};\n use std::cmp::Ordering;\n use std::fmt;\n use std::rc::Rc;\n@@ -1892,11 +1892,11 @@ fn document_non_exhaustive(w: &mut Buffer, item: &clean::Item) {\n }\n \n fn document_type_layout(w: &mut Buffer, cx: &Context<'_>, ty_def_id: DefId) {\n-    fn write_size_of_layout(w: &mut Buffer, layout: Layout<'_>, tag_size: u64) {\n-        if layout.abi().is_unsized() {\n+    fn write_size_of_layout(w: &mut Buffer, layout: &LayoutS<VariantIdx>, tag_size: u64) {\n+        if layout.abi.is_unsized() {\n             write!(w, \"(unsized)\");\n         } else {\n-            let bytes = layout.size().bytes() - tag_size;\n+            let bytes = layout.size.bytes() - tag_size;\n             write!(w, \"{size} byte{pl}\", size = bytes, pl = if bytes == 1 { \"\" } else { \"s\" },);\n         }\n     }\n@@ -1927,7 +1927,7 @@ fn document_type_layout(w: &mut Buffer, cx: &Context<'_>, ty_def_id: DefId) {\n                  chapter for details on type layout guarantees.</p></div>\"\n             );\n             w.write_str(\"<p><strong>Size:</strong> \");\n-            write_size_of_layout(w, ty_layout.layout, 0);\n+            write_size_of_layout(w, &ty_layout.layout.0, 0);\n             writeln!(w, \"</p>\");\n             if let Variants::Multiple { variants, tag, tag_encoding, .. } =\n                 &ty_layout.layout.variants()\n@@ -1953,7 +1953,7 @@ fn document_type_layout(w: &mut Buffer, cx: &Context<'_>, ty_def_id: DefId) {\n                     for (index, layout) in variants.iter_enumerated() {\n                         let name = adt.variant(index).name;\n                         write!(w, \"<li><code>{name}</code>: \", name = name);\n-                        write_size_of_layout(w, *layout, tag_size);\n+                        write_size_of_layout(w, layout, tag_size);\n                         writeln!(w, \"</li>\");\n                     }\n                     w.write_str(\"</ul>\");"}, {"sha": "adbcfd3189b76d0fad22532f0e767fe7e63a7932", "filename": "src/tools/clippy/clippy_lints/src/casts/cast_possible_truncation.rs", "status": "modified", "additions": 2, "deletions": 3, "changes": 5, "blob_url": "https://github.com/rust-lang/rust/blob/b3bc6bf31265ac10946a0832092dbcedf9b26805/src%2Ftools%2Fclippy%2Fclippy_lints%2Fsrc%2Fcasts%2Fcast_possible_truncation.rs", "raw_url": "https://github.com/rust-lang/rust/raw/b3bc6bf31265ac10946a0832092dbcedf9b26805/src%2Ftools%2Fclippy%2Fclippy_lints%2Fsrc%2Fcasts%2Fcast_possible_truncation.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ftools%2Fclippy%2Fclippy_lints%2Fsrc%2Fcasts%2Fcast_possible_truncation.rs?ref=b3bc6bf31265ac10946a0832092dbcedf9b26805", "patch": "@@ -2,12 +2,11 @@ use clippy_utils::consts::{constant, Constant};\n use clippy_utils::diagnostics::span_lint;\n use clippy_utils::expr_or_init;\n use clippy_utils::ty::{get_discriminant_value, is_isize_or_usize};\n-use rustc_ast::ast;\n-use rustc_attr::IntType;\n use rustc_hir::def::{DefKind, Res};\n use rustc_hir::{BinOpKind, Expr, ExprKind};\n use rustc_lint::LateContext;\n use rustc_middle::ty::{self, FloatTy, Ty};\n+use rustc_target::abi::IntegerType;\n \n use super::{utils, CAST_ENUM_TRUNCATION, CAST_POSSIBLE_TRUNCATION};\n \n@@ -122,7 +121,7 @@ pub(super) fn check(cx: &LateContext<'_>, expr: &Expr<'_>, cast_expr: &Expr<'_>,\n             let cast_from_ptr_size = def.repr().int.map_or(true, |ty| {\n                 matches!(\n                     ty,\n-                    IntType::SignedInt(ast::IntTy::Isize) | IntType::UnsignedInt(ast::UintTy::Usize)\n+                    IntegerType::Pointer(_),\n                 )\n             });\n             let suffix = match (cast_from_ptr_size, is_isize_or_usize(cast_to)) {"}, {"sha": "601990cd6a3169776a4e3f0cd8005d07196d18d0", "filename": "src/tools/clippy/clippy_lints/src/lib.rs", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "blob_url": "https://github.com/rust-lang/rust/blob/b3bc6bf31265ac10946a0832092dbcedf9b26805/src%2Ftools%2Fclippy%2Fclippy_lints%2Fsrc%2Flib.rs", "raw_url": "https://github.com/rust-lang/rust/raw/b3bc6bf31265ac10946a0832092dbcedf9b26805/src%2Ftools%2Fclippy%2Fclippy_lints%2Fsrc%2Flib.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ftools%2Fclippy%2Fclippy_lints%2Fsrc%2Flib.rs?ref=b3bc6bf31265ac10946a0832092dbcedf9b26805", "patch": "@@ -26,7 +26,6 @@\n extern crate rustc_arena;\n extern crate rustc_ast;\n extern crate rustc_ast_pretty;\n-extern crate rustc_attr;\n extern crate rustc_data_structures;\n extern crate rustc_driver;\n extern crate rustc_errors;"}]}