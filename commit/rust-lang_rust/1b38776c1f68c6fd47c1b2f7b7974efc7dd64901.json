{"sha": "1b38776c1f68c6fd47c1b2f7b7974efc7dd64901", "node_id": "MDY6Q29tbWl0NzI0NzEyOjFiMzg3NzZjMWY2OGM2ZmQ0N2MxYjJmN2I3OTc0ZWZjN2RkNjQ5MDE=", "commit": {"author": {"name": "bors", "email": "bors@rust-lang.org", "date": "2016-12-21T10:38:22Z"}, "committer": {"name": "bors", "email": "bors@rust-lang.org", "date": "2016-12-21T10:38:22Z"}, "message": "Auto merge of #38302 - Mark-Simulacrum:trans-cleanup, r=eddyb\n\nCleanup old trans\n\nThis is a cleanup of old trans, with the following main points:\n - Remove the `build.rs` API (prefer using `Builder` directly, which is now passed where needed through `BlockAndBuilder`).\n - Remove `Block` (inlining it into `BlockAndBuilder`)\n - Remove `Callee::call`, primarily through inlining and simplification of code.\n - Thinned `FunctionContext`:\n   - `mir`, `debug_scopes`, `scopes`, and `fn_ty`\u00a0are moved to `MirContext`.\n   - `param_env` is moved to `SharedCrateContext` and renamed to `empty_param_env`.\n   - `llretslotptr` is removed, replaced with more careful management of the return values in calls.\n   - `landingpad_alloca` is inlined into cleanup.\n   - `param_substs` are moved to `MirContext`.\n   - `span` is removed, it was never set to anything but `None`.\n   - `block_arena` and `lpad_arena` are\u00a0removed, since neither was necessary (landing pads and block are quite small, and neither needs arena allocation).\n - Fixed `drop_in_place` not running other destructors in the same function.\n\nFixes #35566 (thanks to @est31 for confirming).", "tree": {"sha": "ab3fa350432db81e6a2491419e99fd9704cb458e", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/ab3fa350432db81e6a2491419e99fd9704cb458e"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/1b38776c1f68c6fd47c1b2f7b7974efc7dd64901", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/1b38776c1f68c6fd47c1b2f7b7974efc7dd64901", "html_url": "https://github.com/rust-lang/rust/commit/1b38776c1f68c6fd47c1b2f7b7974efc7dd64901", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/1b38776c1f68c6fd47c1b2f7b7974efc7dd64901/comments", "author": {"login": "bors", "id": 3372342, "node_id": "MDQ6VXNlcjMzNzIzNDI=", "avatar_url": "https://avatars.githubusercontent.com/u/3372342?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bors", "html_url": "https://github.com/bors", "followers_url": "https://api.github.com/users/bors/followers", "following_url": "https://api.github.com/users/bors/following{/other_user}", "gists_url": "https://api.github.com/users/bors/gists{/gist_id}", "starred_url": "https://api.github.com/users/bors/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bors/subscriptions", "organizations_url": "https://api.github.com/users/bors/orgs", "repos_url": "https://api.github.com/users/bors/repos", "events_url": "https://api.github.com/users/bors/events{/privacy}", "received_events_url": "https://api.github.com/users/bors/received_events", "type": "User", "site_admin": false}, "committer": {"login": "bors", "id": 3372342, "node_id": "MDQ6VXNlcjMzNzIzNDI=", "avatar_url": "https://avatars.githubusercontent.com/u/3372342?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bors", "html_url": "https://github.com/bors", "followers_url": "https://api.github.com/users/bors/followers", "following_url": "https://api.github.com/users/bors/following{/other_user}", "gists_url": "https://api.github.com/users/bors/gists{/gist_id}", "starred_url": "https://api.github.com/users/bors/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bors/subscriptions", "organizations_url": "https://api.github.com/users/bors/orgs", "repos_url": "https://api.github.com/users/bors/repos", "events_url": "https://api.github.com/users/bors/events{/privacy}", "received_events_url": "https://api.github.com/users/bors/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "439c3128d740af372dae163310f7292e999098e1", "url": "https://api.github.com/repos/rust-lang/rust/commits/439c3128d740af372dae163310f7292e999098e1", "html_url": "https://github.com/rust-lang/rust/commit/439c3128d740af372dae163310f7292e999098e1"}, {"sha": "0013d4cdf61a61abab79789c9ad5320bd1e2d56a", "url": "https://api.github.com/repos/rust-lang/rust/commits/0013d4cdf61a61abab79789c9ad5320bd1e2d56a", "html_url": "https://github.com/rust-lang/rust/commit/0013d4cdf61a61abab79789c9ad5320bd1e2d56a"}], "stats": {"total": 6924, "additions": 1904, "deletions": 5020}, "files": [{"sha": "a1e32636980812b8ba7cd802ee5936cd71d3e0eb", "filename": "src/liballoc/heap.rs", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "blob_url": "https://github.com/rust-lang/rust/blob/1b38776c1f68c6fd47c1b2f7b7974efc7dd64901/src%2Fliballoc%2Fheap.rs", "raw_url": "https://github.com/rust-lang/rust/raw/1b38776c1f68c6fd47c1b2f7b7974efc7dd64901/src%2Fliballoc%2Fheap.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Fliballoc%2Fheap.rs?ref=1b38776c1f68c6fd47c1b2f7b7974efc7dd64901", "patch": "@@ -127,6 +127,7 @@ pub fn usable_size(size: usize, align: usize) -> usize {\n pub const EMPTY: *mut () = 0x1 as *mut ();\n \n /// The allocator for unique pointers.\n+// This function must not unwind. If it does, MIR trans will fail.\n #[cfg(not(test))]\n #[lang = \"exchange_malloc\"]\n #[inline]"}, {"sha": "d2b86ade7a2ab26fb283d5396321453224b5d1db", "filename": "src/librustc_llvm/ffi.rs", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "blob_url": "https://github.com/rust-lang/rust/blob/1b38776c1f68c6fd47c1b2f7b7974efc7dd64901/src%2Flibrustc_llvm%2Fffi.rs", "raw_url": "https://github.com/rust-lang/rust/raw/1b38776c1f68c6fd47c1b2f7b7974efc7dd64901/src%2Flibrustc_llvm%2Fffi.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_llvm%2Fffi.rs?ref=1b38776c1f68c6fd47c1b2f7b7974efc7dd64901", "patch": "@@ -710,6 +710,7 @@ extern \"C\" {\n \n     // Operations on instructions\n     pub fn LLVMGetInstructionParent(Inst: ValueRef) -> BasicBlockRef;\n+    pub fn LLVMGetFirstBasicBlock(Fn: ValueRef) -> BasicBlockRef;\n     pub fn LLVMGetFirstInstruction(BB: BasicBlockRef) -> ValueRef;\n     pub fn LLVMInstructionEraseFromParent(Inst: ValueRef);\n "}, {"sha": "8b4343af1990f53f0f8ee6901f08104d78343057", "filename": "src/librustc_trans/abi.rs", "status": "modified", "additions": 5, "deletions": 16, "changes": 21, "blob_url": "https://github.com/rust-lang/rust/blob/1b38776c1f68c6fd47c1b2f7b7974efc7dd64901/src%2Flibrustc_trans%2Fabi.rs", "raw_url": "https://github.com/rust-lang/rust/raw/1b38776c1f68c6fd47c1b2f7b7974efc7dd64901/src%2Flibrustc_trans%2Fabi.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_trans%2Fabi.rs?ref=1b38776c1f68c6fd47c1b2f7b7974efc7dd64901", "patch": "@@ -10,7 +10,6 @@\n \n use llvm::{self, ValueRef, Integer, Pointer, Float, Double, Struct, Array, Vector, AttributePlace};\n use base;\n-use build::AllocaFcx;\n use common::{type_is_fat_ptr, BlockAndBuilder, C_uint};\n use context::CrateContext;\n use cabi_x86;\n@@ -99,21 +98,11 @@ impl ArgAttributes {\n         self\n     }\n \n-    pub fn unset(&mut self, attr: ArgAttribute) -> &mut Self {\n-        self.regular = self.regular - attr;\n-        self\n-    }\n-\n     pub fn set_dereferenceable(&mut self, bytes: u64) -> &mut Self {\n         self.dereferenceable_bytes = bytes;\n         self\n     }\n \n-    pub fn unset_dereferenceable(&mut self) -> &mut Self {\n-        self.dereferenceable_bytes = 0;\n-        self\n-    }\n-\n     pub fn apply_llfn(&self, idx: AttributePlace, llfn: ValueRef) {\n         unsafe {\n             self.regular.for_each_kind(|attr| attr.apply_llfn(idx, llfn));\n@@ -246,7 +235,7 @@ impl ArgType {\n         if self.is_ignore() {\n             return;\n         }\n-        let ccx = bcx.ccx();\n+        let ccx = bcx.ccx;\n         if self.is_indirect() {\n             let llsz = llsize_of(ccx, self.ty);\n             let llalign = llalign_of_min(ccx, self.ty);\n@@ -278,7 +267,7 @@ impl ArgType {\n                 //   bitcasting to the struct type yields invalid cast errors.\n \n                 // We instead thus allocate some scratch space...\n-                let llscratch = AllocaFcx(bcx.fcx(), ty, \"abi_cast\");\n+                let llscratch = bcx.fcx().alloca(ty, \"abi_cast\");\n                 base::Lifetime::Start.call(bcx, llscratch);\n \n                 // ...where we first store the value...\n@@ -431,7 +420,7 @@ impl FnType {\n         let ret_ty = sig.output();\n         let mut ret = arg_of(ret_ty, true);\n \n-        if !type_is_fat_ptr(ccx.tcx(), ret_ty) {\n+        if !type_is_fat_ptr(ccx, ret_ty) {\n             // The `noalias` attribute on the return value is useful to a\n             // function ptr caller.\n             if let ty::TyBox(_) = ret_ty.sty {\n@@ -496,7 +485,7 @@ impl FnType {\n         for ty in inputs.iter().chain(extra_args.iter()) {\n             let mut arg = arg_of(ty, false);\n \n-            if type_is_fat_ptr(ccx.tcx(), ty) {\n+            if type_is_fat_ptr(ccx, ty) {\n                 let original_tys = arg.original_ty.field_types();\n                 let sizing_tys = arg.ty.field_types();\n                 assert_eq!((original_tys.len(), sizing_tys.len()), (2, 2));\n@@ -569,7 +558,7 @@ impl FnType {\n             };\n             // Fat pointers are returned by-value.\n             if !self.ret.is_ignore() {\n-                if !type_is_fat_ptr(ccx.tcx(), sig.output()) {\n+                if !type_is_fat_ptr(ccx, sig.output()) {\n                     fixup(&mut self.ret);\n                 }\n             }"}, {"sha": "31a5538a3c1179f3126eb279fffbc59ae9427d37", "filename": "src/librustc_trans/adt.rs", "status": "modified", "additions": 92, "deletions": 97, "changes": 189, "blob_url": "https://github.com/rust-lang/rust/blob/1b38776c1f68c6fd47c1b2f7b7974efc7dd64901/src%2Flibrustc_trans%2Fadt.rs", "raw_url": "https://github.com/rust-lang/rust/raw/1b38776c1f68c6fd47c1b2f7b7974efc7dd64901/src%2Flibrustc_trans%2Fadt.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_trans%2Fadt.rs?ref=1b38776c1f68c6fd47c1b2f7b7974efc7dd64901", "patch": "@@ -48,9 +48,7 @@ use std;\n use llvm::{ValueRef, True, IntEQ, IntNE};\n use rustc::ty::layout;\n use rustc::ty::{self, Ty, AdtKind};\n-use build::*;\n use common::*;\n-use debuginfo::DebugLoc;\n use glue;\n use base;\n use machine;\n@@ -295,7 +293,7 @@ fn struct_llfields<'a, 'tcx>(cx: &CrateContext<'a, 'tcx>, fields: &Vec<Ty<'tcx>>\n                              sizing: bool, dst: bool) -> Vec<Type> {\n     let fields = variant.field_index_by_increasing_offset().map(|i| fields[i as usize]);\n     if sizing {\n-        fields.filter(|ty| !dst || type_is_sized(cx.tcx(), *ty))\n+        fields.filter(|ty| !dst || cx.shared().type_is_sized(*ty))\n             .map(|ty| type_of::sizing_type_of(cx, ty)).collect()\n     } else {\n         fields.map(|ty| type_of::in_memory_type_of(cx, ty)).collect()\n@@ -304,12 +302,13 @@ fn struct_llfields<'a, 'tcx>(cx: &CrateContext<'a, 'tcx>, fields: &Vec<Ty<'tcx>>\n \n /// Obtain a representation of the discriminant sufficient to translate\n /// destructuring; this may or may not involve the actual discriminant.\n-pub fn trans_switch<'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n-                                t: Ty<'tcx>,\n-                                scrutinee: ValueRef,\n-                                range_assert: bool)\n-                                -> (BranchKind, Option<ValueRef>) {\n-    let l = bcx.ccx().layout_of(t);\n+pub fn trans_switch<'a, 'tcx>(\n+    bcx: &BlockAndBuilder<'a, 'tcx>,\n+    t: Ty<'tcx>,\n+    scrutinee: ValueRef,\n+    range_assert: bool\n+) -> (BranchKind, Option<ValueRef>) {\n+    let l = bcx.ccx.layout_of(t);\n     match *l {\n         layout::CEnum { .. } | layout::General { .. } |\n         layout::RawNullablePointer { .. } | layout::StructWrappedNullablePointer { .. } => {\n@@ -331,34 +330,37 @@ pub fn is_discr_signed<'tcx>(l: &layout::Layout) -> bool {\n }\n \n /// Obtain the actual discriminant of a value.\n-pub fn trans_get_discr<'blk, 'tcx>(bcx: Block<'blk, 'tcx>, t: Ty<'tcx>,\n-                                   scrutinee: ValueRef, cast_to: Option<Type>,\n-                                   range_assert: bool)\n-    -> ValueRef {\n+pub fn trans_get_discr<'a, 'tcx>(\n+    bcx: &BlockAndBuilder<'a, 'tcx>,\n+    t: Ty<'tcx>,\n+    scrutinee: ValueRef,\n+    cast_to: Option<Type>,\n+    range_assert: bool\n+) -> ValueRef {\n     let (def, substs) = match t.sty {\n         ty::TyAdt(ref def, substs) if def.adt_kind() == AdtKind::Enum => (def, substs),\n         _ => bug!(\"{} is not an enum\", t)\n     };\n \n     debug!(\"trans_get_discr t: {:?}\", t);\n-    let l = bcx.ccx().layout_of(t);\n+    let l = bcx.ccx.layout_of(t);\n \n     let val = match *l {\n         layout::CEnum { discr, min, max, .. } => {\n             load_discr(bcx, discr, scrutinee, min, max, range_assert)\n         }\n         layout::General { discr, .. } => {\n-            let ptr = StructGEP(bcx, scrutinee, 0);\n+            let ptr = bcx.struct_gep(scrutinee, 0);\n             load_discr(bcx, discr, ptr, 0, def.variants.len() as u64 - 1,\n                        range_assert)\n         }\n-        layout::Univariant { .. } | layout::UntaggedUnion { .. } => C_u8(bcx.ccx(), 0),\n+        layout::Univariant { .. } | layout::UntaggedUnion { .. } => C_u8(bcx.ccx, 0),\n         layout::RawNullablePointer { nndiscr, .. } => {\n             let cmp = if nndiscr == 0 { IntEQ } else { IntNE };\n-            let llptrty = type_of::sizing_type_of(bcx.ccx(),\n-                monomorphize::field_ty(bcx.ccx().tcx(), substs,\n+            let llptrty = type_of::sizing_type_of(bcx.ccx,\n+                monomorphize::field_ty(bcx.ccx.tcx(), substs,\n                 &def.variants[nndiscr as usize].fields[0]));\n-            ICmp(bcx, cmp, Load(bcx, scrutinee), C_null(llptrty), DebugLoc::None)\n+            bcx.icmp(cmp, bcx.load(scrutinee), C_null(llptrty))\n         }\n         layout::StructWrappedNullablePointer { nndiscr, ref discrfield, .. } => {\n             struct_wrapped_nullable_bitdiscr(bcx, nndiscr, discrfield, scrutinee)\n@@ -367,24 +369,28 @@ pub fn trans_get_discr<'blk, 'tcx>(bcx: Block<'blk, 'tcx>, t: Ty<'tcx>,\n     };\n     match cast_to {\n         None => val,\n-        Some(llty) => if is_discr_signed(&l) { SExt(bcx, val, llty) } else { ZExt(bcx, val, llty) }\n+        Some(llty) => if is_discr_signed(&l) { bcx.sext(val, llty) } else { bcx.zext(val, llty) }\n     }\n }\n \n-fn struct_wrapped_nullable_bitdiscr(bcx: Block, nndiscr: u64, discrfield: &layout::FieldPath,\n-                                    scrutinee: ValueRef) -> ValueRef {\n-    let llptrptr = GEPi(bcx, scrutinee,\n+fn struct_wrapped_nullable_bitdiscr(\n+    bcx: &BlockAndBuilder,\n+    nndiscr: u64,\n+    discrfield: &layout::FieldPath,\n+    scrutinee: ValueRef\n+) -> ValueRef {\n+    let llptrptr = bcx.gepi(scrutinee,\n         &discrfield.iter().map(|f| *f as usize).collect::<Vec<_>>()[..]);\n-    let llptr = Load(bcx, llptrptr);\n+    let llptr = bcx.load(llptrptr);\n     let cmp = if nndiscr == 0 { IntEQ } else { IntNE };\n-    ICmp(bcx, cmp, llptr, C_null(val_ty(llptr)), DebugLoc::None)\n+    bcx.icmp(cmp, llptr, C_null(val_ty(llptr)))\n }\n \n /// Helper for cases where the discriminant is simply loaded.\n-fn load_discr(bcx: Block, ity: layout::Integer, ptr: ValueRef, min: u64, max: u64,\n+fn load_discr(bcx: &BlockAndBuilder, ity: layout::Integer, ptr: ValueRef, min: u64, max: u64,\n               range_assert: bool)\n     -> ValueRef {\n-    let llty = Type::from_integer(bcx.ccx(), ity);\n+    let llty = Type::from_integer(bcx.ccx, ity);\n     assert_eq!(val_ty(ptr), llty.ptr_to());\n     let bits = ity.size().bits();\n     assert!(bits <= 64);\n@@ -397,30 +403,29 @@ fn load_discr(bcx: Block, ity: layout::Integer, ptr: ValueRef, min: u64, max: u6\n         // rejected by the LLVM verifier (it would mean either an\n         // empty set, which is impossible, or the entire range of the\n         // type, which is pointless).\n-        Load(bcx, ptr)\n+        bcx.load(ptr)\n     } else {\n         // llvm::ConstantRange can deal with ranges that wrap around,\n         // so an overflow on (max + 1) is fine.\n-        LoadRangeAssert(bcx, ptr, min, max.wrapping_add(1), /* signed: */ True)\n+        bcx.load_range_assert(ptr, min, max.wrapping_add(1), /* signed: */ True)\n     }\n }\n \n /// Yield information about how to dispatch a case of the\n /// discriminant-like value returned by `trans_switch`.\n ///\n /// This should ideally be less tightly tied to `_match`.\n-pub fn trans_case<'blk, 'tcx>(bcx: Block<'blk, 'tcx>, t: Ty<'tcx>, value: Disr)\n-                              -> ValueRef {\n-    let l = bcx.ccx().layout_of(t);\n+pub fn trans_case<'a, 'tcx>(bcx: &BlockAndBuilder<'a, 'tcx>, t: Ty<'tcx>, value: Disr) -> ValueRef {\n+    let l = bcx.ccx.layout_of(t);\n     match *l {\n         layout::CEnum { discr, .. }\n         | layout::General { discr, .. }=> {\n-            C_integral(Type::from_integer(bcx.ccx(), discr), value.0, true)\n+            C_integral(Type::from_integer(bcx.ccx, discr), value.0, true)\n         }\n         layout::RawNullablePointer { .. } |\n         layout::StructWrappedNullablePointer { .. } => {\n             assert!(value == Disr(0) || value == Disr(1));\n-            C_bool(bcx.ccx(), value != Disr(0))\n+            C_bool(bcx.ccx, value != Disr(0))\n         }\n         _ => {\n             bug!(\"{} does not have a discriminant. Represented as {:#?}\", t, l);\n@@ -430,29 +435,30 @@ pub fn trans_case<'blk, 'tcx>(bcx: Block<'blk, 'tcx>, t: Ty<'tcx>, value: Disr)\n \n /// Set the discriminant for a new value of the given case of the given\n /// representation.\n-pub fn trans_set_discr<'blk, 'tcx>(bcx: Block<'blk, 'tcx>, t: Ty<'tcx>,\n-                                   val: ValueRef, to: Disr) {\n-    let l = bcx.ccx().layout_of(t);\n+pub fn trans_set_discr<'a, 'tcx>(\n+    bcx: &BlockAndBuilder<'a, 'tcx>, t: Ty<'tcx>, val: ValueRef, to: Disr\n+) {\n+    let l = bcx.ccx.layout_of(t);\n     match *l {\n         layout::CEnum{ discr, min, max, .. } => {\n             assert_discr_in_range(Disr(min), Disr(max), to);\n-            Store(bcx, C_integral(Type::from_integer(bcx.ccx(), discr), to.0, true),\n+            bcx.store(C_integral(Type::from_integer(bcx.ccx, discr), to.0, true),\n                   val);\n         }\n         layout::General{ discr, .. } => {\n-            Store(bcx, C_integral(Type::from_integer(bcx.ccx(), discr), to.0, true),\n-                  StructGEP(bcx, val, 0));\n+            bcx.store(C_integral(Type::from_integer(bcx.ccx, discr), to.0, true),\n+                  bcx.struct_gep(val, 0));\n         }\n         layout::Univariant { .. }\n         | layout::UntaggedUnion { .. }\n         | layout::Vector { .. } => {\n             assert_eq!(to, Disr(0));\n         }\n         layout::RawNullablePointer { nndiscr, .. } => {\n-            let nnty = compute_fields(bcx.ccx(), t, nndiscr as usize, false)[0];\n+            let nnty = compute_fields(bcx.ccx, t, nndiscr as usize, false)[0];\n             if to.0 != nndiscr {\n-                let llptrty = type_of::sizing_type_of(bcx.ccx(), nnty);\n-                Store(bcx, C_null(llptrty), val);\n+                let llptrty = type_of::sizing_type_of(bcx.ccx, nnty);\n+                bcx.store(C_null(llptrty), val);\n             }\n         }\n         layout::StructWrappedNullablePointer { nndiscr, ref discrfield, ref nonnull, .. } => {\n@@ -461,25 +467,24 @@ pub fn trans_set_discr<'blk, 'tcx>(bcx: Block<'blk, 'tcx>, t: Ty<'tcx>,\n                     // Issue #34427: As workaround for LLVM bug on\n                     // ARM, use memset of 0 on whole struct rather\n                     // than storing null to single target field.\n-                    let b = B(bcx);\n-                    let llptr = b.pointercast(val, Type::i8(b.ccx).ptr_to());\n-                    let fill_byte = C_u8(b.ccx, 0);\n-                    let size = C_uint(b.ccx, nonnull.stride().bytes());\n-                    let align = C_i32(b.ccx, nonnull.align.abi() as i32);\n-                    base::call_memset(&b, llptr, fill_byte, size, align, false);\n+                    let llptr = bcx.pointercast(val, Type::i8(bcx.ccx).ptr_to());\n+                    let fill_byte = C_u8(bcx.ccx, 0);\n+                    let size = C_uint(bcx.ccx, nonnull.stride().bytes());\n+                    let align = C_i32(bcx.ccx, nonnull.align.abi() as i32);\n+                    base::call_memset(bcx, llptr, fill_byte, size, align, false);\n                 } else {\n                     let path = discrfield.iter().map(|&i| i as usize).collect::<Vec<_>>();\n-                    let llptrptr = GEPi(bcx, val, &path[..]);\n+                    let llptrptr = bcx.gepi(val, &path[..]);\n                     let llptrty = val_ty(llptrptr).element_type();\n-                    Store(bcx, C_null(llptrty), llptrptr);\n+                    bcx.store(C_null(llptrty), llptrptr);\n                 }\n             }\n         }\n         _ => bug!(\"Cannot handle {} represented as {:#?}\", t, l)\n     }\n }\n \n-fn target_sets_discr_via_memset<'blk, 'tcx>(bcx: Block<'blk, 'tcx>) -> bool {\n+fn target_sets_discr_via_memset<'a, 'tcx>(bcx: &BlockAndBuilder<'a, 'tcx>) -> bool {\n     bcx.sess().target.target.arch == \"arm\" || bcx.sess().target.target.arch == \"aarch64\"\n }\n \n@@ -492,27 +497,23 @@ fn assert_discr_in_range(min: Disr, max: Disr, discr: Disr) {\n }\n \n /// Access a field, at a point when the value's case is known.\n-pub fn trans_field_ptr<'blk, 'tcx>(bcx: Block<'blk, 'tcx>, t: Ty<'tcx>,\n-                                   val: MaybeSizedValue, discr: Disr, ix: usize) -> ValueRef {\n-    trans_field_ptr_builder(&bcx.build(), t, val, discr, ix)\n-}\n-\n-/// Access a field, at a point when the value's case is known.\n-pub fn trans_field_ptr_builder<'blk, 'tcx>(bcx: &BlockAndBuilder<'blk, 'tcx>,\n-                                           t: Ty<'tcx>,\n-                                           val: MaybeSizedValue,\n-                                           discr: Disr, ix: usize)\n-                                           -> ValueRef {\n-    let l = bcx.ccx().layout_of(t);\n-    debug!(\"trans_field_ptr_builder on {} represented as {:#?}\", t, l);\n+pub fn trans_field_ptr<'a, 'tcx>(\n+    bcx: &BlockAndBuilder<'a, 'tcx>,\n+    t: Ty<'tcx>,\n+    val: MaybeSizedValue,\n+    discr: Disr,\n+    ix: usize\n+) -> ValueRef {\n+    let l = bcx.ccx.layout_of(t);\n+    debug!(\"trans_field_ptr on {} represented as {:#?}\", t, l);\n     // Note: if this ever needs to generate conditionals (e.g., if we\n     // decide to do some kind of cdr-coding-like non-unique repr\n     // someday), it will need to return a possibly-new bcx as well.\n     match *l {\n         layout::Univariant { ref variant, .. } => {\n             assert_eq!(discr, Disr(0));\n             struct_field_ptr(bcx, &variant,\n-             &compute_fields(bcx.ccx(), t, 0, false),\n+             &compute_fields(bcx.ccx, t, 0, false),\n              val, ix, false)\n         }\n         layout::Vector { count, .. } => {\n@@ -521,57 +522,53 @@ pub fn trans_field_ptr_builder<'blk, 'tcx>(bcx: &BlockAndBuilder<'blk, 'tcx>,\n             bcx.struct_gep(val.value, ix)\n         }\n         layout::General { discr: d, ref variants, .. } => {\n-            let mut fields = compute_fields(bcx.ccx(), t, discr.0 as usize, false);\n-            fields.insert(0, d.to_ty(&bcx.ccx().tcx(), false));\n+            let mut fields = compute_fields(bcx.ccx, t, discr.0 as usize, false);\n+            fields.insert(0, d.to_ty(&bcx.ccx.tcx(), false));\n             struct_field_ptr(bcx, &variants[discr.0 as usize],\n              &fields,\n              val, ix + 1, true)\n         }\n         layout::UntaggedUnion { .. } => {\n-            let fields = compute_fields(bcx.ccx(), t, 0, false);\n-            let ty = type_of::in_memory_type_of(bcx.ccx(), fields[ix]);\n-            if bcx.is_unreachable() { return C_undef(ty.ptr_to()); }\n+            let fields = compute_fields(bcx.ccx, t, 0, false);\n+            let ty = type_of::in_memory_type_of(bcx.ccx, fields[ix]);\n             bcx.pointercast(val.value, ty.ptr_to())\n         }\n         layout::RawNullablePointer { nndiscr, .. } |\n         layout::StructWrappedNullablePointer { nndiscr,  .. } if discr.0 != nndiscr => {\n-            let nullfields = compute_fields(bcx.ccx(), t, (1-nndiscr) as usize, false);\n+            let nullfields = compute_fields(bcx.ccx, t, (1-nndiscr) as usize, false);\n             // The unit-like case might have a nonzero number of unit-like fields.\n             // (e.d., Result of Either with (), as one side.)\n-            let ty = type_of::type_of(bcx.ccx(), nullfields[ix]);\n-            assert_eq!(machine::llsize_of_alloc(bcx.ccx(), ty), 0);\n-            // The contents of memory at this pointer can't matter, but use\n-            // the value that's \"reasonable\" in case of pointer comparison.\n-            if bcx.is_unreachable() { return C_undef(ty.ptr_to()); }\n+            let ty = type_of::type_of(bcx.ccx, nullfields[ix]);\n+            assert_eq!(machine::llsize_of_alloc(bcx.ccx, ty), 0);\n             bcx.pointercast(val.value, ty.ptr_to())\n         }\n         layout::RawNullablePointer { nndiscr, .. } => {\n-            let nnty = compute_fields(bcx.ccx(), t, nndiscr as usize, false)[0];\n+            let nnty = compute_fields(bcx.ccx, t, nndiscr as usize, false)[0];\n             assert_eq!(ix, 0);\n             assert_eq!(discr.0, nndiscr);\n-            let ty = type_of::type_of(bcx.ccx(), nnty);\n-            if bcx.is_unreachable() { return C_undef(ty.ptr_to()); }\n+            let ty = type_of::type_of(bcx.ccx, nnty);\n             bcx.pointercast(val.value, ty.ptr_to())\n         }\n         layout::StructWrappedNullablePointer { ref nonnull, nndiscr, .. } => {\n             assert_eq!(discr.0, nndiscr);\n             struct_field_ptr(bcx, &nonnull,\n-             &compute_fields(bcx.ccx(), t, discr.0 as usize, false),\n+             &compute_fields(bcx.ccx, t, discr.0 as usize, false),\n              val, ix, false)\n         }\n         _ => bug!(\"element access in type without elements: {} represented as {:#?}\", t, l)\n     }\n }\n \n-fn struct_field_ptr<'blk, 'tcx>(bcx: &BlockAndBuilder<'blk, 'tcx>,\n-                                st: &layout::Struct, fields: &Vec<Ty<'tcx>>, val: MaybeSizedValue,\n-                                ix: usize, needs_cast: bool) -> ValueRef {\n+fn struct_field_ptr<'a, 'tcx>(\n+    bcx: &BlockAndBuilder<'a, 'tcx>,\n+    st: &layout::Struct,\n+    fields: &Vec<Ty<'tcx>>,\n+    val: MaybeSizedValue,\n+    ix: usize,\n+    needs_cast: bool\n+) -> ValueRef {\n     let fty = fields[ix];\n-    let ccx = bcx.ccx();\n-    let ll_fty = type_of::in_memory_type_of(bcx.ccx(), fty);\n-    if bcx.is_unreachable() {\n-        return C_undef(ll_fty.ptr_to());\n-    }\n+    let ccx = bcx.ccx;\n \n     let ptr_val = if needs_cast {\n         let fields = st.field_index_by_increasing_offset().map(|i| {\n@@ -587,7 +584,8 @@ fn struct_field_ptr<'blk, 'tcx>(bcx: &BlockAndBuilder<'blk, 'tcx>,\n     //   * First field - Always aligned properly\n     //   * Packed struct - There is no alignment padding\n     //   * Field is sized - pointer is properly aligned already\n-    if st.offsets[ix] == layout::Size::from_bytes(0) || st.packed || type_is_sized(bcx.tcx(), fty) {\n+    if st.offsets[ix] == layout::Size::from_bytes(0) || st.packed ||\n+        bcx.ccx.shared().type_is_sized(fty) {\n         return bcx.struct_gep(ptr_val, st.memory_index[ix] as usize);\n     }\n \n@@ -607,8 +605,6 @@ fn struct_field_ptr<'blk, 'tcx>(bcx: &BlockAndBuilder<'blk, 'tcx>,\n         return bcx.struct_gep(ptr_val, ix);\n     }\n \n-    let dbloc = DebugLoc::None;\n-\n     // We need to get the pointer manually now.\n     // We do this by casting to a *i8, then offsetting it by the appropriate amount.\n     // We do this instead of, say, simply adjusting the pointer from the result of a GEP\n@@ -628,7 +624,7 @@ fn struct_field_ptr<'blk, 'tcx>(bcx: &BlockAndBuilder<'blk, 'tcx>,\n \n \n     let offset = st.offsets[ix].bytes();\n-    let unaligned_offset = C_uint(bcx.ccx(), offset);\n+    let unaligned_offset = C_uint(bcx.ccx, offset);\n \n     // Get the alignment of the field\n     let (_, align) = glue::size_and_align_of_dst(bcx, fty, meta);\n@@ -639,19 +635,18 @@ fn struct_field_ptr<'blk, 'tcx>(bcx: &BlockAndBuilder<'blk, 'tcx>,\n     //   (unaligned offset + (align - 1)) & -align\n \n     // Calculate offset\n-    dbloc.apply(bcx.fcx());\n-    let align_sub_1 = bcx.sub(align, C_uint(bcx.ccx(), 1u64));\n+    let align_sub_1 = bcx.sub(align, C_uint(bcx.ccx, 1u64));\n     let offset = bcx.and(bcx.add(unaligned_offset, align_sub_1),\n                          bcx.neg(align));\n \n     debug!(\"struct_field_ptr: DST field offset: {:?}\", Value(offset));\n \n     // Cast and adjust pointer\n-    let byte_ptr = bcx.pointercast(ptr_val, Type::i8p(bcx.ccx()));\n+    let byte_ptr = bcx.pointercast(ptr_val, Type::i8p(bcx.ccx));\n     let byte_ptr = bcx.gep(byte_ptr, &[offset]);\n \n     // Finally, cast back to the type expected\n-    let ll_fty = type_of::in_memory_type_of(bcx.ccx(), fty);\n+    let ll_fty = type_of::in_memory_type_of(bcx.ccx, fty);\n     debug!(\"struct_field_ptr: Field type is {:?}\", ll_fty);\n     bcx.pointercast(byte_ptr, ll_fty.ptr_to())\n }"}, {"sha": "d6385e1ca156263994ec0ef4d35c030b13a23416", "filename": "src/librustc_trans/asm.rs", "status": "modified", "additions": 23, "deletions": 21, "changes": 44, "blob_url": "https://github.com/rust-lang/rust/blob/1b38776c1f68c6fd47c1b2f7b7974efc7dd64901/src%2Flibrustc_trans%2Fasm.rs", "raw_url": "https://github.com/rust-lang/rust/raw/1b38776c1f68c6fd47c1b2f7b7974efc7dd64901/src%2Flibrustc_trans%2Fasm.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_trans%2Fasm.rs?ref=1b38776c1f68c6fd47c1b2f7b7974efc7dd64901", "patch": "@@ -12,7 +12,6 @@\n \n use llvm::{self, ValueRef};\n use base;\n-use build::*;\n use common::*;\n use type_of;\n use type_::Type;\n@@ -25,10 +24,12 @@ use syntax::ast::AsmDialect;\n use libc::{c_uint, c_char};\n \n // Take an inline assembly expression and splat it out via LLVM\n-pub fn trans_inline_asm<'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n-                                    ia: &hir::InlineAsm,\n-                                    outputs: Vec<(ValueRef, Ty<'tcx>)>,\n-                                    mut inputs: Vec<ValueRef>) {\n+pub fn trans_inline_asm<'a, 'tcx>(\n+    bcx: &BlockAndBuilder<'a, 'tcx>,\n+    ia: &hir::InlineAsm,\n+    outputs: Vec<(ValueRef, Ty<'tcx>)>,\n+    mut inputs: Vec<ValueRef>\n+) {\n     let mut ext_constraints = vec![];\n     let mut output_types = vec![];\n \n@@ -47,7 +48,7 @@ pub fn trans_inline_asm<'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n         if out.is_indirect {\n             indirect_outputs.push(val.unwrap());\n         } else {\n-            output_types.push(type_of::type_of(bcx.ccx(), ty));\n+            output_types.push(type_of::type_of(bcx.ccx, ty));\n         }\n     }\n     if !indirect_outputs.is_empty() {\n@@ -78,9 +79,9 @@ pub fn trans_inline_asm<'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n     // Depending on how many outputs we have, the return type is different\n     let num_outputs = output_types.len();\n     let output_type = match num_outputs {\n-        0 => Type::void(bcx.ccx()),\n+        0 => Type::void(bcx.ccx),\n         1 => output_types[0],\n-        _ => Type::struct_(bcx.ccx(), &output_types[..], false)\n+        _ => Type::struct_(bcx.ccx, &output_types[..], false)\n     };\n \n     let dialect = match ia.dialect {\n@@ -90,32 +91,33 @@ pub fn trans_inline_asm<'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n \n     let asm = CString::new(ia.asm.as_str().as_bytes()).unwrap();\n     let constraint_cstr = CString::new(all_constraints).unwrap();\n-    let r = InlineAsmCall(bcx,\n-                          asm.as_ptr(),\n-                          constraint_cstr.as_ptr(),\n-                          &inputs,\n-                          output_type,\n-                          ia.volatile,\n-                          ia.alignstack,\n-                          dialect);\n+    let r = bcx.inline_asm_call(\n+        asm.as_ptr(),\n+        constraint_cstr.as_ptr(),\n+        &inputs,\n+        output_type,\n+        ia.volatile,\n+        ia.alignstack,\n+        dialect\n+    );\n \n     // Again, based on how many outputs we have\n     let outputs = ia.outputs.iter().zip(&outputs).filter(|&(ref o, _)| !o.is_indirect);\n     for (i, (_, &(val, _))) in outputs.enumerate() {\n-        let v = if num_outputs == 1 { r } else { ExtractValue(bcx, r, i) };\n-        Store(bcx, v, val);\n+        let v = if num_outputs == 1 { r } else { bcx.extract_value(r, i) };\n+        bcx.store(v, val);\n     }\n \n     // Store expn_id in a metadata node so we can map LLVM errors\n     // back to source locations.  See #17552.\n     unsafe {\n         let key = \"srcloc\";\n-        let kind = llvm::LLVMGetMDKindIDInContext(bcx.ccx().llcx(),\n+        let kind = llvm::LLVMGetMDKindIDInContext(bcx.ccx.llcx(),\n             key.as_ptr() as *const c_char, key.len() as c_uint);\n \n-        let val: llvm::ValueRef = C_i32(bcx.ccx(), ia.expn_id.into_u32() as i32);\n+        let val: llvm::ValueRef = C_i32(bcx.ccx, ia.expn_id.into_u32() as i32);\n \n         llvm::LLVMSetMetadata(r, kind,\n-            llvm::LLVMMDNodeInContext(bcx.ccx().llcx(), &val, 1));\n+            llvm::LLVMMDNodeInContext(bcx.ccx.llcx(), &val, 1));\n     }\n }"}, {"sha": "76bb1c56af3818aa155be237b4bb5da7292aded4", "filename": "src/librustc_trans/base.rs", "status": "modified", "additions": 164, "deletions": 617, "changes": 781, "blob_url": "https://github.com/rust-lang/rust/blob/1b38776c1f68c6fd47c1b2f7b7974efc7dd64901/src%2Flibrustc_trans%2Fbase.rs", "raw_url": "https://github.com/rust-lang/rust/raw/1b38776c1f68c6fd47c1b2f7b7974efc7dd64901/src%2Flibrustc_trans%2Fbase.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_trans%2Fbase.rs?ref=1b38776c1f68c6fd47c1b2f7b7974efc7dd64901", "patch": "@@ -23,8 +23,6 @@\n //!     but one TypeRef corresponds to many `Ty`s; for instance, tup(int, int,\n //!     int) and rec(x=int, y=int, z=int) will have the same TypeRef.\n \n-#![allow(non_camel_case_types)]\n-\n use super::CrateTranslation;\n use super::ModuleLlvm;\n use super::ModuleSource;\n@@ -37,7 +35,7 @@ use back::symbol_export::{self, ExportedSymbols};\n use llvm::{Linkage, ValueRef, Vector, get_param};\n use llvm;\n use rustc::hir::def_id::{DefId, LOCAL_CRATE};\n-use middle::lang_items::{LangItem, ExchangeMallocFnLangItem, StartFnLangItem};\n+use middle::lang_items::StartFnLangItem;\n use rustc::ty::subst::Substs;\n use rustc::traits;\n use rustc::ty::{self, Ty, TyCtxt, TypeFoldable};\n@@ -51,20 +49,18 @@ use session::{self, DataTypeKind, Session};\n use abi::{self, Abi, FnType};\n use adt;\n use attributes;\n-use build::*;\n-use builder::{Builder, noname};\n+use builder::Builder;\n use callee::{Callee};\n-use common::{Block, C_bool, C_bytes_in_context, C_i32, C_uint};\n+use common::{BlockAndBuilder, C_bool, C_bytes_in_context, C_i32, C_uint};\n use collector::{self, TransItemCollectionMode};\n-use common::{C_null, C_struct_in_context, C_u64, C_u8, C_undef};\n+use common::{C_struct_in_context, C_u64, C_undef};\n use common::{CrateContext, FunctionContext};\n-use common::{Result};\n use common::{fulfill_obligation};\n use common::{type_is_zero_size, val_ty};\n use common;\n use consts;\n use context::{SharedCrateContext, CrateContextList};\n-use debuginfo::{self, DebugLoc};\n+use debuginfo;\n use declare;\n use machine;\n use machine::{llalign_of_min, llsize_of};\n@@ -81,11 +77,8 @@ use value::Value;\n use Disr;\n use util::nodemap::{NodeSet, FxHashMap, FxHashSet};\n \n-use arena::TypedArena;\n use libc::c_uint;\n use std::ffi::{CStr, CString};\n-use std::cell::{Cell, RefCell};\n-use std::ptr;\n use std::rc::Rc;\n use std::str;\n use std::i32;\n@@ -95,52 +88,6 @@ use rustc::hir;\n use rustc::ty::layout::{self, Layout};\n use syntax::ast;\n \n-thread_local! {\n-    static TASK_LOCAL_INSN_KEY: RefCell<Option<Vec<&'static str>>> = {\n-        RefCell::new(None)\n-    }\n-}\n-\n-pub fn with_insn_ctxt<F>(blk: F)\n-    where F: FnOnce(&[&'static str])\n-{\n-    TASK_LOCAL_INSN_KEY.with(move |slot| {\n-        slot.borrow().as_ref().map(move |s| blk(s));\n-    })\n-}\n-\n-pub fn init_insn_ctxt() {\n-    TASK_LOCAL_INSN_KEY.with(|slot| {\n-        *slot.borrow_mut() = Some(Vec::new());\n-    });\n-}\n-\n-pub struct _InsnCtxt {\n-    _cannot_construct_outside_of_this_module: (),\n-}\n-\n-impl Drop for _InsnCtxt {\n-    fn drop(&mut self) {\n-        TASK_LOCAL_INSN_KEY.with(|slot| {\n-            if let Some(ctx) = slot.borrow_mut().as_mut() {\n-                ctx.pop();\n-            }\n-        })\n-    }\n-}\n-\n-pub fn push_ctxt(s: &'static str) -> _InsnCtxt {\n-    debug!(\"new InsnCtxt: {}\", s);\n-    TASK_LOCAL_INSN_KEY.with(|slot| {\n-        if let Some(ctx) = slot.borrow_mut().as_mut() {\n-            ctx.push(s)\n-        }\n-    });\n-    _InsnCtxt {\n-        _cannot_construct_outside_of_this_module: (),\n-    }\n-}\n-\n pub struct StatRecorder<'a, 'tcx: 'a> {\n     ccx: &'a CrateContext<'a, 'tcx>,\n     name: Option<String>,\n@@ -162,10 +109,7 @@ impl<'a, 'tcx> Drop for StatRecorder<'a, 'tcx> {\n     fn drop(&mut self) {\n         if self.ccx.sess().trans_stats() {\n             let iend = self.ccx.stats().n_llvm_insns.get();\n-            self.ccx\n-                .stats()\n-                .fn_stats\n-                .borrow_mut()\n+            self.ccx.stats().fn_stats.borrow_mut()\n                 .push((self.name.take().unwrap(), iend - self.istart));\n             self.ccx.stats().n_fns.set(self.ccx.stats().n_fns.get() + 1);\n             // Reset LLVM insn count to avoid compound costs.\n@@ -174,52 +118,14 @@ impl<'a, 'tcx> Drop for StatRecorder<'a, 'tcx> {\n     }\n }\n \n-pub fn get_meta(bcx: Block, fat_ptr: ValueRef) -> ValueRef {\n-    StructGEP(bcx, fat_ptr, abi::FAT_PTR_EXTRA)\n-}\n-\n-pub fn get_dataptr(bcx: Block, fat_ptr: ValueRef) -> ValueRef {\n-    StructGEP(bcx, fat_ptr, abi::FAT_PTR_ADDR)\n+pub fn get_meta(bcx: &Builder, fat_ptr: ValueRef) -> ValueRef {\n+    bcx.struct_gep(fat_ptr, abi::FAT_PTR_EXTRA)\n }\n \n-pub fn get_meta_builder(b: &Builder, fat_ptr: ValueRef) -> ValueRef {\n-    b.struct_gep(fat_ptr, abi::FAT_PTR_EXTRA)\n+pub fn get_dataptr(bcx: &Builder, fat_ptr: ValueRef) -> ValueRef {\n+    bcx.struct_gep(fat_ptr, abi::FAT_PTR_ADDR)\n }\n \n-pub fn get_dataptr_builder(b: &Builder, fat_ptr: ValueRef) -> ValueRef {\n-    b.struct_gep(fat_ptr, abi::FAT_PTR_ADDR)\n-}\n-\n-fn require_alloc_fn<'blk, 'tcx>(bcx: Block<'blk, 'tcx>, info_ty: Ty<'tcx>, it: LangItem) -> DefId {\n-    match bcx.tcx().lang_items.require(it) {\n-        Ok(id) => id,\n-        Err(s) => {\n-            bcx.sess().fatal(&format!(\"allocation of `{}` {}\", info_ty, s));\n-        }\n-    }\n-}\n-\n-// The following malloc_raw_dyn* functions allocate a box to contain\n-// a given type, but with a potentially dynamic size.\n-\n-pub fn malloc_raw_dyn<'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n-                                  llty_ptr: Type,\n-                                  info_ty: Ty<'tcx>,\n-                                  size: ValueRef,\n-                                  align: ValueRef,\n-                                  debug_loc: DebugLoc)\n-                                  -> Result<'blk, 'tcx> {\n-    let _icx = push_ctxt(\"malloc_raw_exchange\");\n-\n-    // Allocate space:\n-    let def_id = require_alloc_fn(bcx, info_ty, ExchangeMallocFnLangItem);\n-    let r = Callee::def(bcx.ccx(), def_id, bcx.tcx().intern_substs(&[]))\n-        .call(bcx, debug_loc, &[size, align], None);\n-\n-    Result::new(r.bcx, PointerCast(r.bcx, r.val, llty_ptr))\n-}\n-\n-\n pub fn bin_op_to_icmp_predicate(op: hir::BinOp_,\n                                 signed: bool)\n                                 -> llvm::IntPredicate {\n@@ -254,18 +160,18 @@ pub fn bin_op_to_fcmp_predicate(op: hir::BinOp_) -> llvm::RealPredicate {\n     }\n }\n \n-pub fn compare_simd_types<'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n-                                      lhs: ValueRef,\n-                                      rhs: ValueRef,\n-                                      t: Ty<'tcx>,\n-                                      ret_ty: Type,\n-                                      op: hir::BinOp_,\n-                                      debug_loc: DebugLoc)\n-                                      -> ValueRef {\n+pub fn compare_simd_types<'a, 'tcx>(\n+    bcx: &BlockAndBuilder<'a, 'tcx>,\n+    lhs: ValueRef,\n+    rhs: ValueRef,\n+    t: Ty<'tcx>,\n+    ret_ty: Type,\n+    op: hir::BinOp_\n+) -> ValueRef {\n     let signed = match t.sty {\n         ty::TyFloat(_) => {\n             let cmp = bin_op_to_fcmp_predicate(op);\n-            return SExt(bcx, FCmp(bcx, cmp, lhs, rhs, debug_loc), ret_ty);\n+            return bcx.sext(bcx.fcmp(cmp, lhs, rhs), ret_ty);\n         },\n         ty::TyUint(_) => false,\n         ty::TyInt(_) => true,\n@@ -277,7 +183,7 @@ pub fn compare_simd_types<'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n     // to get the correctly sized type. This will compile to a single instruction\n     // once the IR is converted to assembly if the SIMD instruction is supported\n     // by the target architecture.\n-    SExt(bcx, ICmp(bcx, cmp, lhs, rhs, debug_loc), ret_ty)\n+    bcx.sext(bcx.icmp(cmp, lhs, rhs), ret_ty)\n }\n \n /// Retrieve the information we are losing (making dynamic) in an unsizing\n@@ -311,11 +217,12 @@ pub fn unsized_info<'ccx, 'tcx>(ccx: &CrateContext<'ccx, 'tcx>,\n }\n \n /// Coerce `src` to `dst_ty`. `src_ty` must be a thin pointer.\n-pub fn unsize_thin_ptr<'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n-                                   src: ValueRef,\n-                                   src_ty: Ty<'tcx>,\n-                                   dst_ty: Ty<'tcx>)\n-                                   -> (ValueRef, ValueRef) {\n+pub fn unsize_thin_ptr<'a, 'tcx>(\n+    bcx: &BlockAndBuilder<'a, 'tcx>,\n+    src: ValueRef,\n+    src_ty: Ty<'tcx>,\n+    dst_ty: Ty<'tcx>\n+) -> (ValueRef, ValueRef) {\n     debug!(\"unsize_thin_ptr: {:?} => {:?}\", src_ty, dst_ty);\n     match (&src_ty.sty, &dst_ty.sty) {\n         (&ty::TyBox(a), &ty::TyBox(b)) |\n@@ -325,35 +232,34 @@ pub fn unsize_thin_ptr<'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n          &ty::TyRawPtr(ty::TypeAndMut { ty: b, .. })) |\n         (&ty::TyRawPtr(ty::TypeAndMut { ty: a, .. }),\n          &ty::TyRawPtr(ty::TypeAndMut { ty: b, .. })) => {\n-            assert!(common::type_is_sized(bcx.tcx(), a));\n-            let ptr_ty = type_of::in_memory_type_of(bcx.ccx(), b).ptr_to();\n-            (PointerCast(bcx, src, ptr_ty),\n-             unsized_info(bcx.ccx(), a, b, None))\n+            assert!(bcx.ccx.shared().type_is_sized(a));\n+            let ptr_ty = type_of::in_memory_type_of(bcx.ccx, b).ptr_to();\n+            (bcx.pointercast(src, ptr_ty), unsized_info(bcx.ccx, a, b, None))\n         }\n         _ => bug!(\"unsize_thin_ptr: called on bad types\"),\n     }\n }\n \n /// Coerce `src`, which is a reference to a value of type `src_ty`,\n /// to a value of type `dst_ty` and store the result in `dst`\n-pub fn coerce_unsized_into<'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n-                                       src: ValueRef,\n-                                       src_ty: Ty<'tcx>,\n-                                       dst: ValueRef,\n-                                       dst_ty: Ty<'tcx>) {\n+pub fn coerce_unsized_into<'a, 'tcx>(bcx: &BlockAndBuilder<'a, 'tcx>,\n+                                     src: ValueRef,\n+                                     src_ty: Ty<'tcx>,\n+                                     dst: ValueRef,\n+                                     dst_ty: Ty<'tcx>) {\n     match (&src_ty.sty, &dst_ty.sty) {\n         (&ty::TyBox(..), &ty::TyBox(..)) |\n         (&ty::TyRef(..), &ty::TyRef(..)) |\n         (&ty::TyRef(..), &ty::TyRawPtr(..)) |\n         (&ty::TyRawPtr(..), &ty::TyRawPtr(..)) => {\n-            let (base, info) = if common::type_is_fat_ptr(bcx.tcx(), src_ty) {\n+            let (base, info) = if common::type_is_fat_ptr(bcx.ccx, src_ty) {\n                 // fat-ptr to fat-ptr unsize preserves the vtable\n                 // i.e. &'a fmt::Debug+Send => &'a fmt::Debug\n                 // So we need to pointercast the base to ensure\n                 // the types match up.\n                 let (base, info) = load_fat_ptr(bcx, src, src_ty);\n-                let llcast_ty = type_of::fat_ptr_base_ty(bcx.ccx(), dst_ty);\n-                let base = PointerCast(bcx, base, llcast_ty);\n+                let llcast_ty = type_of::fat_ptr_base_ty(bcx.ccx, dst_ty);\n+                let base = bcx.pointercast(base, llcast_ty);\n                 (base, info)\n             } else {\n                 let base = load_ty(bcx, src, src_ty);\n@@ -377,7 +283,7 @@ pub fn coerce_unsized_into<'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n \n             let iter = src_fields.zip(dst_fields).enumerate();\n             for (i, (src_fty, dst_fty)) in iter {\n-                if type_is_zero_size(bcx.ccx(), dst_fty) {\n+                if type_is_zero_size(bcx.ccx, dst_fty) {\n                     continue;\n                 }\n \n@@ -415,8 +321,10 @@ pub fn custom_coerce_unsize_info<'scx, 'tcx>(scx: &SharedCrateContext<'scx, 'tcx\n     }\n }\n \n-pub fn cast_shift_expr_rhs(cx: Block, op: hir::BinOp_, lhs: ValueRef, rhs: ValueRef) -> ValueRef {\n-    cast_shift_rhs(op, lhs, rhs, |a, b| Trunc(cx, a, b), |a, b| ZExt(cx, a, b))\n+pub fn cast_shift_expr_rhs(\n+    cx: &BlockAndBuilder, op: hir::BinOp_, lhs: ValueRef, rhs: ValueRef\n+) -> ValueRef {\n+    cast_shift_rhs(op, lhs, rhs, |a, b| cx.trunc(a, b), |a, b| cx.zext(a, b))\n }\n \n pub fn cast_shift_const_rhs(op: hir::BinOp_, lhs: ValueRef, rhs: ValueRef) -> ValueRef {\n@@ -462,42 +370,6 @@ fn cast_shift_rhs<F, G>(op: hir::BinOp_,\n     }\n }\n \n-pub fn invoke<'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n-                          llfn: ValueRef,\n-                          llargs: &[ValueRef],\n-                          debug_loc: DebugLoc)\n-                          -> (ValueRef, Block<'blk, 'tcx>) {\n-    let _icx = push_ctxt(\"invoke_\");\n-    if bcx.unreachable.get() {\n-        return (C_null(Type::i8(bcx.ccx())), bcx);\n-    }\n-\n-    if need_invoke(bcx) {\n-        debug!(\"invoking {:?} at {:?}\", Value(llfn), bcx.llbb);\n-        for &llarg in llargs {\n-            debug!(\"arg: {:?}\", Value(llarg));\n-        }\n-        let normal_bcx = bcx.fcx.new_block(\"normal-return\");\n-        let landing_pad = bcx.fcx.get_landing_pad();\n-\n-        let llresult = Invoke(bcx,\n-                              llfn,\n-                              &llargs[..],\n-                              normal_bcx.llbb,\n-                              landing_pad,\n-                              debug_loc);\n-        return (llresult, normal_bcx);\n-    } else {\n-        debug!(\"calling {:?} at {:?}\", Value(llfn), bcx.llbb);\n-        for &llarg in llargs {\n-            debug!(\"arg: {:?}\", Value(llarg));\n-        }\n-\n-        let llresult = Call(bcx, llfn, &llargs[..], debug_loc);\n-        return (llresult, bcx);\n-    }\n-}\n-\n /// Returns whether this session's target will use SEH-based unwinding.\n ///\n /// This is only true for MSVC targets, and even then the 64-bit MSVC target\n@@ -507,18 +379,6 @@ pub fn wants_msvc_seh(sess: &Session) -> bool {\n     sess.target.target.options.is_like_msvc\n }\n \n-pub fn avoid_invoke(bcx: Block) -> bool {\n-    bcx.sess().no_landing_pads() || bcx.lpad().is_some()\n-}\n-\n-pub fn need_invoke(bcx: Block) -> bool {\n-    if avoid_invoke(bcx) {\n-        false\n-    } else {\n-        bcx.fcx.needs_invoke()\n-    }\n-}\n-\n pub fn call_assume<'a, 'tcx>(b: &Builder<'a, 'tcx>, val: ValueRef) {\n     let assume_intrinsic = b.ccx.get_intrinsic(\"llvm.assume\");\n     b.call(assume_intrinsic, &[val], None);\n@@ -527,14 +387,7 @@ pub fn call_assume<'a, 'tcx>(b: &Builder<'a, 'tcx>, val: ValueRef) {\n /// Helper for loading values from memory. Does the necessary conversion if the in-memory type\n /// differs from the type used for SSA values. Also handles various special cases where the type\n /// gives us better information about what we are loading.\n-pub fn load_ty<'blk, 'tcx>(cx: Block<'blk, 'tcx>, ptr: ValueRef, t: Ty<'tcx>) -> ValueRef {\n-    if cx.unreachable.get() {\n-        return C_undef(type_of::type_of(cx.ccx(), t));\n-    }\n-    load_ty_builder(&B(cx), ptr, t)\n-}\n-\n-pub fn load_ty_builder<'a, 'tcx>(b: &Builder<'a, 'tcx>, ptr: ValueRef, t: Ty<'tcx>) -> ValueRef {\n+pub fn load_ty<'a, 'tcx>(b: &Builder<'a, 'tcx>, ptr: ValueRef, t: Ty<'tcx>) -> ValueRef {\n     let ccx = b.ccx;\n     if type_is_zero_size(ccx, t) {\n         return C_undef(type_of::type_of(ccx, t));\n@@ -559,8 +412,7 @@ pub fn load_ty_builder<'a, 'tcx>(b: &Builder<'a, 'tcx>, ptr: ValueRef, t: Ty<'tc\n         // a char is a Unicode codepoint, and so takes values from 0\n         // to 0x10FFFF inclusive only.\n         b.load_range_assert(ptr, 0, 0x10FFFF + 1, llvm::False)\n-    } else if (t.is_region_ptr() || t.is_unique()) &&\n-              !common::type_is_fat_ptr(ccx.tcx(), t) {\n+    } else if (t.is_region_ptr() || t.is_unique()) && !common::type_is_fat_ptr(ccx, t) {\n         b.load_nonnull(ptr)\n     } else {\n         b.load(ptr)\n@@ -569,177 +421,96 @@ pub fn load_ty_builder<'a, 'tcx>(b: &Builder<'a, 'tcx>, ptr: ValueRef, t: Ty<'tc\n \n /// Helper for storing values in memory. Does the necessary conversion if the in-memory type\n /// differs from the type used for SSA values.\n-pub fn store_ty<'blk, 'tcx>(cx: Block<'blk, 'tcx>, v: ValueRef, dst: ValueRef, t: Ty<'tcx>) {\n-    if cx.unreachable.get() {\n-        return;\n-    }\n-\n+pub fn store_ty<'a, 'tcx>(cx: &BlockAndBuilder<'a, 'tcx>, v: ValueRef, dst: ValueRef, t: Ty<'tcx>) {\n     debug!(\"store_ty: {:?} : {:?} <- {:?}\", Value(dst), t, Value(v));\n \n-    if common::type_is_fat_ptr(cx.tcx(), t) {\n-        let lladdr = ExtractValue(cx, v, abi::FAT_PTR_ADDR);\n-        let llextra = ExtractValue(cx, v, abi::FAT_PTR_EXTRA);\n+    if common::type_is_fat_ptr(cx.ccx, t) {\n+        let lladdr = cx.extract_value(v, abi::FAT_PTR_ADDR);\n+        let llextra = cx.extract_value(v, abi::FAT_PTR_EXTRA);\n         store_fat_ptr(cx, lladdr, llextra, dst, t);\n     } else {\n-        Store(cx, from_immediate(cx, v), dst);\n+        cx.store(from_immediate(cx, v), dst);\n     }\n }\n \n-pub fn store_fat_ptr<'blk, 'tcx>(cx: Block<'blk, 'tcx>,\n-                                 data: ValueRef,\n-                                 extra: ValueRef,\n-                                 dst: ValueRef,\n-                                 _ty: Ty<'tcx>) {\n+pub fn store_fat_ptr<'a, 'tcx>(cx: &BlockAndBuilder<'a, 'tcx>,\n+                               data: ValueRef,\n+                               extra: ValueRef,\n+                               dst: ValueRef,\n+                               _ty: Ty<'tcx>) {\n     // FIXME: emit metadata\n-    Store(cx, data, get_dataptr(cx, dst));\n-    Store(cx, extra, get_meta(cx, dst));\n+    cx.store(data, get_dataptr(cx, dst));\n+    cx.store(extra, get_meta(cx, dst));\n }\n \n-pub fn load_fat_ptr<'blk, 'tcx>(cx: Block<'blk, 'tcx>,\n-                                src: ValueRef,\n-                                ty: Ty<'tcx>)\n-                                -> (ValueRef, ValueRef)\n-{\n-    if cx.unreachable.get() {\n-        // FIXME: remove me\n-        return (Load(cx, get_dataptr(cx, src)),\n-                Load(cx, get_meta(cx, src)));\n-    }\n-\n-    load_fat_ptr_builder(&B(cx), src, ty)\n-}\n-\n-pub fn load_fat_ptr_builder<'a, 'tcx>(\n-    b: &Builder<'a, 'tcx>,\n-    src: ValueRef,\n-    t: Ty<'tcx>)\n-    -> (ValueRef, ValueRef)\n-{\n-\n-    let ptr = get_dataptr_builder(b, src);\n+pub fn load_fat_ptr<'a, 'tcx>(\n+    b: &Builder<'a, 'tcx>, src: ValueRef, t: Ty<'tcx>\n+) -> (ValueRef, ValueRef) {\n+    let ptr = get_dataptr(b, src);\n     let ptr = if t.is_region_ptr() || t.is_unique() {\n         b.load_nonnull(ptr)\n     } else {\n         b.load(ptr)\n     };\n \n     // FIXME: emit metadata on `meta`.\n-    let meta = b.load(get_meta_builder(b, src));\n+    let meta = b.load(get_meta(b, src));\n \n     (ptr, meta)\n }\n \n-pub fn from_immediate(bcx: Block, val: ValueRef) -> ValueRef {\n-    if val_ty(val) == Type::i1(bcx.ccx()) {\n-        ZExt(bcx, val, Type::i8(bcx.ccx()))\n+pub fn from_immediate(bcx: &BlockAndBuilder, val: ValueRef) -> ValueRef {\n+    if val_ty(val) == Type::i1(bcx.ccx) {\n+        bcx.zext(val, Type::i8(bcx.ccx))\n     } else {\n         val\n     }\n }\n \n-pub fn to_immediate(bcx: Block, val: ValueRef, ty: Ty) -> ValueRef {\n+pub fn to_immediate(bcx: &BlockAndBuilder, val: ValueRef, ty: Ty) -> ValueRef {\n     if ty.is_bool() {\n-        Trunc(bcx, val, Type::i1(bcx.ccx()))\n+        bcx.trunc(val, Type::i1(bcx.ccx))\n     } else {\n         val\n     }\n }\n \n-pub fn with_cond<'blk, 'tcx, F>(bcx: Block<'blk, 'tcx>, val: ValueRef, f: F) -> Block<'blk, 'tcx>\n-    where F: FnOnce(Block<'blk, 'tcx>) -> Block<'blk, 'tcx>\n-{\n-    let _icx = push_ctxt(\"with_cond\");\n-\n-    if bcx.unreachable.get() || common::const_to_opt_uint(val) == Some(0) {\n-        return bcx;\n-    }\n-\n-    let fcx = bcx.fcx;\n-    let next_cx = fcx.new_block(\"next\");\n-    let cond_cx = fcx.new_block(\"cond\");\n-    CondBr(bcx, val, cond_cx.llbb, next_cx.llbb, DebugLoc::None);\n-    let after_cx = f(cond_cx);\n-    if !after_cx.terminated.get() {\n-        Br(after_cx, next_cx.llbb, DebugLoc::None);\n-    }\n-    next_cx\n-}\n-\n pub enum Lifetime { Start, End }\n \n-// If LLVM lifetime intrinsic support is enabled (i.e. optimizations\n-// on), and `ptr` is nonzero-sized, then extracts the size of `ptr`\n-// and the intrinsic for `lt` and passes them to `emit`, which is in\n-// charge of generating code to call the passed intrinsic on whatever\n-// block of generated code is targetted for the intrinsic.\n-//\n-// If LLVM lifetime intrinsic support is disabled (i.e.  optimizations\n-// off) or `ptr` is zero-sized, then no-op (does not call `emit`).\n-fn core_lifetime_emit<'blk, 'tcx, F>(ccx: &'blk CrateContext<'blk, 'tcx>,\n-                                     ptr: ValueRef,\n-                                     lt: Lifetime,\n-                                     emit: F)\n-    where F: FnOnce(&'blk CrateContext<'blk, 'tcx>, machine::llsize, ValueRef)\n-{\n-    if ccx.sess().opts.optimize == config::OptLevel::No {\n-        return;\n-    }\n-\n-    let _icx = push_ctxt(match lt {\n-        Lifetime::Start => \"lifetime_start\",\n-        Lifetime::End => \"lifetime_end\"\n-    });\n-\n-    let size = machine::llsize_of_alloc(ccx, val_ty(ptr).element_type());\n-    if size == 0 {\n-        return;\n-    }\n-\n-    let lifetime_intrinsic = ccx.get_intrinsic(match lt {\n-        Lifetime::Start => \"llvm.lifetime.start\",\n-        Lifetime::End => \"llvm.lifetime.end\"\n-    });\n-    emit(ccx, size, lifetime_intrinsic)\n-}\n-\n impl Lifetime {\n+    // If LLVM lifetime intrinsic support is enabled (i.e. optimizations\n+    // on), and `ptr` is nonzero-sized, then extracts the size of `ptr`\n+    // and the intrinsic for `lt` and passes them to `emit`, which is in\n+    // charge of generating code to call the passed intrinsic on whatever\n+    // block of generated code is targetted for the intrinsic.\n+    //\n+    // If LLVM lifetime intrinsic support is disabled (i.e.  optimizations\n+    // off) or `ptr` is zero-sized, then no-op (does not call `emit`).\n     pub fn call(self, b: &Builder, ptr: ValueRef) {\n-        core_lifetime_emit(b.ccx, ptr, self, |ccx, size, lifetime_intrinsic| {\n-            let ptr = b.pointercast(ptr, Type::i8p(ccx));\n-            b.call(lifetime_intrinsic, &[C_u64(ccx, size), ptr], None);\n-        });\n-    }\n-}\n+        if b.ccx.sess().opts.optimize == config::OptLevel::No {\n+            return;\n+        }\n \n-pub fn call_lifetime_start(bcx: Block, ptr: ValueRef) {\n-    if !bcx.unreachable.get() {\n-        Lifetime::Start.call(&bcx.build(), ptr);\n-    }\n-}\n+        let size = machine::llsize_of_alloc(b.ccx, val_ty(ptr).element_type());\n+        if size == 0 {\n+            return;\n+        }\n \n-pub fn call_lifetime_end(bcx: Block, ptr: ValueRef) {\n-    if !bcx.unreachable.get() {\n-        Lifetime::End.call(&bcx.build(), ptr);\n-    }\n-}\n+        let lifetime_intrinsic = b.ccx.get_intrinsic(match self {\n+            Lifetime::Start => \"llvm.lifetime.start\",\n+            Lifetime::End => \"llvm.lifetime.end\"\n+        });\n \n-// Generates code for resumption of unwind at the end of a landing pad.\n-pub fn trans_unwind_resume(bcx: Block, lpval: ValueRef) {\n-    if !bcx.sess().target.target.options.custom_unwind_resume {\n-        Resume(bcx, lpval);\n-    } else {\n-        let exc_ptr = ExtractValue(bcx, lpval, 0);\n-        bcx.fcx.eh_unwind_resume()\n-            .call(bcx, DebugLoc::None, &[exc_ptr], None);\n+        let ptr = b.pointercast(ptr, Type::i8p(b.ccx));\n+        b.call(lifetime_intrinsic, &[C_u64(b.ccx, size), ptr], None);\n     }\n }\n \n-pub fn call_memcpy<'bcx, 'tcx>(b: &Builder<'bcx, 'tcx>,\n+pub fn call_memcpy<'a, 'tcx>(b: &Builder<'a, 'tcx>,\n                                dst: ValueRef,\n                                src: ValueRef,\n                                n_bytes: ValueRef,\n                                align: u32) {\n-    let _icx = push_ctxt(\"call_memcpy\");\n     let ccx = b.ccx;\n     let ptr_width = &ccx.sess().target.target.target_pointer_width[..];\n     let key = format!(\"llvm.memcpy.p0i8.p0i8.i{}\", ptr_width);\n@@ -752,255 +523,44 @@ pub fn call_memcpy<'bcx, 'tcx>(b: &Builder<'bcx, 'tcx>,\n     b.call(memcpy, &[dst_ptr, src_ptr, size, align, volatile], None);\n }\n \n-pub fn memcpy_ty<'blk, 'tcx>(bcx: Block<'blk, 'tcx>, dst: ValueRef, src: ValueRef, t: Ty<'tcx>) {\n-    let _icx = push_ctxt(\"memcpy_ty\");\n-    let ccx = bcx.ccx();\n+pub fn memcpy_ty<'a, 'tcx>(\n+    bcx: &BlockAndBuilder<'a, 'tcx>, dst: ValueRef, src: ValueRef, t: Ty<'tcx>\n+) {\n+    let ccx = bcx.ccx;\n \n-    if type_is_zero_size(ccx, t) || bcx.unreachable.get() {\n+    if type_is_zero_size(ccx, t) {\n         return;\n     }\n \n     if t.is_structural() {\n         let llty = type_of::type_of(ccx, t);\n         let llsz = llsize_of(ccx, llty);\n         let llalign = type_of::align_of(ccx, t);\n-        call_memcpy(&B(bcx), dst, src, llsz, llalign as u32);\n-    } else if common::type_is_fat_ptr(bcx.tcx(), t) {\n+        call_memcpy(bcx, dst, src, llsz, llalign as u32);\n+    } else if common::type_is_fat_ptr(bcx.ccx, t) {\n         let (data, extra) = load_fat_ptr(bcx, src, t);\n         store_fat_ptr(bcx, data, extra, dst, t);\n     } else {\n         store_ty(bcx, load_ty(bcx, src, t), dst, t);\n     }\n }\n \n-pub fn init_zero_mem<'blk, 'tcx>(cx: Block<'blk, 'tcx>, llptr: ValueRef, t: Ty<'tcx>) {\n-    if cx.unreachable.get() {\n-        return;\n-    }\n-    let _icx = push_ctxt(\"init_zero_mem\");\n-    let bcx = cx;\n-    memfill(&B(bcx), llptr, t, 0);\n-}\n-\n-// Always use this function instead of storing a constant byte to the memory\n-// in question. e.g. if you store a zero constant, LLVM will drown in vreg\n-// allocation for large data structures, and the generated code will be\n-// awful. (A telltale sign of this is large quantities of\n-// `mov [byte ptr foo],0` in the generated code.)\n-fn memfill<'a, 'tcx>(b: &Builder<'a, 'tcx>, llptr: ValueRef, ty: Ty<'tcx>, byte: u8) {\n-    let _icx = push_ctxt(\"memfill\");\n-    let ccx = b.ccx;\n-    let llty = type_of::type_of(ccx, ty);\n-    let llptr = b.pointercast(llptr, Type::i8(ccx).ptr_to());\n-    let llzeroval = C_u8(ccx, byte);\n-    let size = machine::llsize_of(ccx, llty);\n-    let align = C_i32(ccx, type_of::align_of(ccx, ty) as i32);\n-    call_memset(b, llptr, llzeroval, size, align, false);\n-}\n-\n-pub fn call_memset<'bcx, 'tcx>(b: &Builder<'bcx, 'tcx>,\n+pub fn call_memset<'a, 'tcx>(b: &Builder<'a, 'tcx>,\n                                ptr: ValueRef,\n                                fill_byte: ValueRef,\n                                size: ValueRef,\n                                align: ValueRef,\n-                               volatile: bool) {\n-    let ccx = b.ccx;\n-    let ptr_width = &ccx.sess().target.target.target_pointer_width[..];\n+                               volatile: bool) -> ValueRef {\n+    let ptr_width = &b.ccx.sess().target.target.target_pointer_width[..];\n     let intrinsic_key = format!(\"llvm.memset.p0i8.i{}\", ptr_width);\n-    let llintrinsicfn = ccx.get_intrinsic(&intrinsic_key);\n-    let volatile = C_bool(ccx, volatile);\n-    b.call(llintrinsicfn, &[ptr, fill_byte, size, align, volatile], None);\n+    let llintrinsicfn = b.ccx.get_intrinsic(&intrinsic_key);\n+    let volatile = C_bool(b.ccx, volatile);\n+    b.call(llintrinsicfn, &[ptr, fill_byte, size, align, volatile], None)\n }\n \n-pub fn alloc_ty<'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n-                            ty: Ty<'tcx>,\n-                            name: &str) -> ValueRef {\n+pub fn alloc_ty<'a, 'tcx>(bcx: &BlockAndBuilder<'a, 'tcx>, ty: Ty<'tcx>, name: &str) -> ValueRef {\n     assert!(!ty.has_param_types());\n-    alloca(bcx, type_of::type_of(bcx.ccx(), ty), name)\n-}\n-\n-pub fn alloca(cx: Block, ty: Type, name: &str) -> ValueRef {\n-    let _icx = push_ctxt(\"alloca\");\n-    if cx.unreachable.get() {\n-        unsafe {\n-            return llvm::LLVMGetUndef(ty.ptr_to().to_ref());\n-        }\n-    }\n-    DebugLoc::None.apply(cx.fcx);\n-    let result = Alloca(cx, ty, name);\n-    debug!(\"alloca({:?}) = {:?}\", name, result);\n-    result\n-}\n-\n-impl<'blk, 'tcx> FunctionContext<'blk, 'tcx> {\n-    /// Create a function context for the given function.\n-    /// Beware that you must call `fcx.init` or `fcx.bind_args`\n-    /// before doing anything with the returned function context.\n-    pub fn new(ccx: &'blk CrateContext<'blk, 'tcx>,\n-               llfndecl: ValueRef,\n-               fn_ty: FnType,\n-               definition: Option<(Instance<'tcx>, &ty::FnSig<'tcx>, Abi)>,\n-               block_arena: &'blk TypedArena<common::BlockS<'blk, 'tcx>>)\n-               -> FunctionContext<'blk, 'tcx> {\n-        let (param_substs, def_id) = match definition {\n-            Some((instance, ..)) => {\n-                common::validate_substs(instance.substs);\n-                (instance.substs, Some(instance.def))\n-            }\n-            None => (ccx.tcx().intern_substs(&[]), None)\n-        };\n-\n-        let local_id = def_id.and_then(|id| ccx.tcx().map.as_local_node_id(id));\n-\n-        debug!(\"FunctionContext::new({})\",\n-               definition.map_or(String::new(), |d| d.0.to_string()));\n-\n-        let no_debug = if let Some(id) = local_id {\n-            ccx.tcx().map.attrs(id)\n-               .iter().any(|item| item.check_name(\"no_debug\"))\n-        } else if let Some(def_id) = def_id {\n-            ccx.sess().cstore.item_attrs(def_id)\n-               .iter().any(|item| item.check_name(\"no_debug\"))\n-        } else {\n-            false\n-        };\n-\n-        let mir = def_id.map(|id| ccx.tcx().item_mir(id));\n-\n-        let debug_context = if let (false, Some((instance, sig, abi)), &Some(ref mir)) =\n-                (no_debug, definition, &mir) {\n-            debuginfo::create_function_debug_context(ccx, instance, sig, abi, llfndecl, mir)\n-        } else {\n-            debuginfo::empty_function_debug_context(ccx)\n-        };\n-\n-        FunctionContext {\n-            mir: mir,\n-            llfn: llfndecl,\n-            llretslotptr: Cell::new(None),\n-            param_env: ccx.tcx().empty_parameter_environment(),\n-            alloca_insert_pt: Cell::new(None),\n-            landingpad_alloca: Cell::new(None),\n-            fn_ty: fn_ty,\n-            param_substs: param_substs,\n-            span: None,\n-            block_arena: block_arena,\n-            lpad_arena: TypedArena::new(),\n-            ccx: ccx,\n-            debug_context: debug_context,\n-            scopes: RefCell::new(Vec::new()),\n-        }\n-    }\n-\n-    /// Performs setup on a newly created function, creating the entry\n-    /// scope block and allocating space for the return pointer.\n-    pub fn init(&'blk self, skip_retptr: bool) -> Block<'blk, 'tcx> {\n-        let entry_bcx = self.new_block(\"entry-block\");\n-\n-        // Use a dummy instruction as the insertion point for all allocas.\n-        // This is later removed in FunctionContext::cleanup.\n-        self.alloca_insert_pt.set(Some(unsafe {\n-            Load(entry_bcx, C_null(Type::i8p(self.ccx)));\n-            llvm::LLVMGetFirstInstruction(entry_bcx.llbb)\n-        }));\n-\n-        if !self.fn_ty.ret.is_ignore() && !skip_retptr {\n-            // We normally allocate the llretslotptr, unless we\n-            // have been instructed to skip it for immediate return\n-            // values, or there is nothing to return at all.\n-\n-            // We create an alloca to hold a pointer of type `ret.original_ty`\n-            // which will hold the pointer to the right alloca which has the\n-            // final ret value\n-            let llty = self.fn_ty.ret.memory_ty(self.ccx);\n-            // But if there are no nested returns, we skip the indirection\n-            // and have a single retslot\n-            let slot = if self.fn_ty.ret.is_indirect() {\n-                get_param(self.llfn, 0)\n-            } else {\n-                AllocaFcx(self, llty, \"sret_slot\")\n-            };\n-\n-            self.llretslotptr.set(Some(slot));\n-        }\n-\n-        entry_bcx\n-    }\n-\n-    /// Ties up the llstaticallocas -> llloadenv -> lltop edges,\n-    /// and builds the return block.\n-    pub fn finish(&'blk self, ret_cx: Block<'blk, 'tcx>,\n-                  ret_debug_loc: DebugLoc) {\n-        let _icx = push_ctxt(\"FunctionContext::finish\");\n-\n-        self.build_return_block(ret_cx, ret_debug_loc);\n-\n-        DebugLoc::None.apply(self);\n-        self.cleanup();\n-    }\n-\n-    // Builds the return block for a function.\n-    pub fn build_return_block(&self, ret_cx: Block<'blk, 'tcx>,\n-                              ret_debug_location: DebugLoc) {\n-        if self.llretslotptr.get().is_none() ||\n-           ret_cx.unreachable.get() ||\n-           self.fn_ty.ret.is_indirect() {\n-            return RetVoid(ret_cx, ret_debug_location);\n-        }\n-\n-        let retslot = self.llretslotptr.get().unwrap();\n-        let retptr = Value(retslot);\n-        let llty = self.fn_ty.ret.original_ty;\n-        match (retptr.get_dominating_store(ret_cx), self.fn_ty.ret.cast) {\n-            // If there's only a single store to the ret slot, we can directly return\n-            // the value that was stored and omit the store and the alloca.\n-            // However, we only want to do this when there is no cast needed.\n-            (Some(s), None) => {\n-                let mut retval = s.get_operand(0).unwrap().get();\n-                s.erase_from_parent();\n-\n-                if retptr.has_no_uses() {\n-                    retptr.erase_from_parent();\n-                }\n-\n-                if self.fn_ty.ret.is_indirect() {\n-                    Store(ret_cx, retval, get_param(self.llfn, 0));\n-                    RetVoid(ret_cx, ret_debug_location)\n-                } else {\n-                    if llty == Type::i1(self.ccx) {\n-                        retval = Trunc(ret_cx, retval, llty);\n-                    }\n-                    Ret(ret_cx, retval, ret_debug_location)\n-                }\n-            }\n-            (_, cast_ty) if self.fn_ty.ret.is_indirect() => {\n-                // Otherwise, copy the return value to the ret slot.\n-                assert_eq!(cast_ty, None);\n-                let llsz = llsize_of(self.ccx, self.fn_ty.ret.ty);\n-                let llalign = llalign_of_min(self.ccx, self.fn_ty.ret.ty);\n-                call_memcpy(&B(ret_cx), get_param(self.llfn, 0),\n-                            retslot, llsz, llalign as u32);\n-                RetVoid(ret_cx, ret_debug_location)\n-            }\n-            (_, Some(cast_ty)) => {\n-                let load = Load(ret_cx, PointerCast(ret_cx, retslot, cast_ty.ptr_to()));\n-                let llalign = llalign_of_min(self.ccx, self.fn_ty.ret.ty);\n-                unsafe {\n-                    llvm::LLVMSetAlignment(load, llalign);\n-                }\n-                Ret(ret_cx, load, ret_debug_location)\n-            }\n-            (_, None) => {\n-                let retval = if llty == Type::i1(self.ccx) {\n-                    let val = LoadRangeAssert(ret_cx, retslot, 0, 2, llvm::False);\n-                    Trunc(ret_cx, val, llty)\n-                } else {\n-                    Load(ret_cx, retslot)\n-                };\n-                Ret(ret_cx, retval, ret_debug_location)\n-            }\n-        }\n-    }\n+    bcx.fcx().alloca(type_of::type_of(bcx.ccx, ty), name)\n }\n \n pub fn trans_instance<'a, 'tcx>(ccx: &CrateContext<'a, 'tcx>, instance: Instance<'tcx>) {\n@@ -1018,8 +578,6 @@ pub fn trans_instance<'a, 'tcx>(ccx: &CrateContext<'a, 'tcx>, instance: Instance\n     // release builds.\n     info!(\"trans_instance({})\", instance);\n \n-    let _icx = push_ctxt(\"trans_instance\");\n-\n     let fn_ty = ccx.tcx().item_type(instance.def);\n     let fn_ty = ccx.tcx().erase_regions(&fn_ty);\n     let fn_ty = monomorphize::apply_param_substs(ccx.shared(), instance.substs, &fn_ty);\n@@ -1040,19 +598,9 @@ pub fn trans_instance<'a, 'tcx>(ccx: &CrateContext<'a, 'tcx>, instance: Instance\n \n     let fn_ty = FnType::new(ccx, abi, &sig, &[]);\n \n-    let (arena, fcx): (TypedArena<_>, FunctionContext);\n-    arena = TypedArena::new();\n-    fcx = FunctionContext::new(ccx,\n-                               lldecl,\n-                               fn_ty,\n-                               Some((instance, &sig, abi)),\n-                               &arena);\n-\n-    if fcx.mir.is_none() {\n-        bug!(\"attempted translation of `{}` w/o MIR\", instance);\n-    }\n-\n-    mir::trans_mir(&fcx);\n+    let fcx = FunctionContext::new(ccx, lldecl);\n+    let mir = ccx.tcx().item_mir(instance.def);\n+    mir::trans_mir(&fcx, fn_ty, &mir, instance, &sig, abi);\n }\n \n pub fn trans_ctor_shim<'a, 'tcx>(ccx: &CrateContext<'a, 'tcx>,\n@@ -1069,34 +617,55 @@ pub fn trans_ctor_shim<'a, 'tcx>(ccx: &CrateContext<'a, 'tcx>,\n     let sig = ccx.tcx().erase_late_bound_regions_and_normalize(&ctor_ty.fn_sig());\n     let fn_ty = FnType::new(ccx, Abi::Rust, &sig, &[]);\n \n-    let (arena, fcx): (TypedArena<_>, FunctionContext);\n-    arena = TypedArena::new();\n-    fcx = FunctionContext::new(ccx, llfndecl, fn_ty, None, &arena);\n-    let bcx = fcx.init(false);\n-\n-    if !fcx.fn_ty.ret.is_ignore() {\n-        let dest = fcx.llretslotptr.get().unwrap();\n+    let fcx = FunctionContext::new(ccx, llfndecl);\n+    let bcx = fcx.get_entry_block();\n+    if !fn_ty.ret.is_ignore() {\n+        // But if there are no nested returns, we skip the indirection\n+        // and have a single retslot\n+        let dest = if fn_ty.ret.is_indirect() {\n+            get_param(fcx.llfn, 0)\n+        } else {\n+            // We create an alloca to hold a pointer of type `ret.original_ty`\n+            // which will hold the pointer to the right alloca which has the\n+            // final ret value\n+            fcx.alloca(fn_ty.ret.memory_ty(ccx), \"sret_slot\")\n+        };\n         let dest_val = adt::MaybeSizedValue::sized(dest); // Can return unsized value\n-        let mut llarg_idx = fcx.fn_ty.ret.is_indirect() as usize;\n+        let mut llarg_idx = fn_ty.ret.is_indirect() as usize;\n         let mut arg_idx = 0;\n         for (i, arg_ty) in sig.inputs().iter().enumerate() {\n-            let lldestptr = adt::trans_field_ptr(bcx, sig.output(), dest_val, Disr::from(disr), i);\n-            let arg = &fcx.fn_ty.args[arg_idx];\n+            let lldestptr = adt::trans_field_ptr(&bcx, sig.output(), dest_val, Disr::from(disr), i);\n+            let arg = &fn_ty.args[arg_idx];\n             arg_idx += 1;\n-            let b = &bcx.build();\n-            if common::type_is_fat_ptr(bcx.tcx(), arg_ty) {\n-                let meta = &fcx.fn_ty.args[arg_idx];\n+            if common::type_is_fat_ptr(bcx.ccx, arg_ty) {\n+                let meta = &fn_ty.args[arg_idx];\n                 arg_idx += 1;\n-                arg.store_fn_arg(b, &mut llarg_idx, get_dataptr(bcx, lldestptr));\n-                meta.store_fn_arg(b, &mut llarg_idx, get_meta(bcx, lldestptr));\n+                arg.store_fn_arg(&bcx, &mut llarg_idx, get_dataptr(&bcx, lldestptr));\n+                meta.store_fn_arg(&bcx, &mut llarg_idx, get_meta(&bcx, lldestptr));\n             } else {\n-                arg.store_fn_arg(b, &mut llarg_idx, lldestptr);\n+                arg.store_fn_arg(&bcx, &mut llarg_idx, lldestptr);\n             }\n         }\n-        adt::trans_set_discr(bcx, sig.output(), dest, disr);\n-    }\n+        adt::trans_set_discr(&bcx, sig.output(), dest, disr);\n \n-    fcx.finish(bcx, DebugLoc::None);\n+        if fn_ty.ret.is_indirect() {\n+            bcx.ret_void();\n+            return;\n+        }\n+\n+        if let Some(cast_ty) = fn_ty.ret.cast {\n+            let load = bcx.load(bcx.pointercast(dest, cast_ty.ptr_to()));\n+            let llalign = llalign_of_min(ccx, fn_ty.ret.ty);\n+            unsafe {\n+                llvm::LLVMSetAlignment(load, llalign);\n+            }\n+            bcx.ret(load)\n+        } else {\n+            bcx.ret(bcx.load(dest))\n+        }\n+    } else {\n+        bcx.ret_void();\n+    }\n }\n \n pub fn llvm_linkage_by_name(name: &str) -> Option<Linkage> {\n@@ -1168,9 +737,7 @@ pub fn maybe_create_entry_wrapper(ccx: &CrateContext) {\n \n     let et = ccx.sess().entry_type.get().unwrap();\n     match et {\n-        config::EntryMain => {\n-            create_entry_fn(ccx, span, main_llfn, true);\n-        }\n+        config::EntryMain => create_entry_fn(ccx, span, main_llfn, true),\n         config::EntryStart => create_entry_fn(ccx, span, main_llfn, false),\n         config::EntryNone => {}    // Do nothing.\n     }\n@@ -1195,47 +762,27 @@ pub fn maybe_create_entry_wrapper(ccx: &CrateContext) {\n         attributes::set_frame_pointer_elimination(ccx, llfn);\n \n         let llbb = unsafe {\n-            llvm::LLVMAppendBasicBlockInContext(ccx.llcx(), llfn, \"top\\0\".as_ptr() as *const _)\n+            let name = CString::new(\"top\").unwrap();\n+            llvm::LLVMAppendBasicBlockInContext(ccx.llcx(), llfn, name.as_ptr())\n         };\n-        let bld = ccx.raw_builder();\n-        unsafe {\n-            llvm::LLVMPositionBuilderAtEnd(bld, llbb);\n-\n-            debuginfo::gdb::insert_reference_to_gdb_debug_scripts_section_global(ccx);\n-\n-            let (start_fn, args) = if use_start_lang_item {\n-                let start_def_id = match ccx.tcx().lang_items.require(StartFnLangItem) {\n-                    Ok(id) => id,\n-                    Err(s) => ccx.sess().fatal(&s)\n-                };\n-                let empty_substs = ccx.tcx().intern_substs(&[]);\n-                let start_fn = Callee::def(ccx, start_def_id, empty_substs).reify(ccx);\n-                let args = {\n-                    let opaque_rust_main =\n-                        llvm::LLVMBuildPointerCast(bld,\n-                                                   rust_main,\n-                                                   Type::i8p(ccx).to_ref(),\n-                                                   \"rust_main\\0\".as_ptr() as *const _);\n-\n-                    vec![opaque_rust_main, get_param(llfn, 0), get_param(llfn, 1)]\n-                };\n-                (start_fn, args)\n-            } else {\n-                debug!(\"using user-defined start fn\");\n-                let args = vec![get_param(llfn, 0 as c_uint), get_param(llfn, 1 as c_uint)];\n+        let bld = Builder::with_ccx(ccx);\n+        bld.position_at_end(llbb);\n \n-                (rust_main, args)\n-            };\n+        debuginfo::gdb::insert_reference_to_gdb_debug_scripts_section_global(ccx, &bld);\n \n-            let result = llvm::LLVMRustBuildCall(bld,\n-                                                 start_fn,\n-                                                 args.as_ptr(),\n-                                                 args.len() as c_uint,\n-                                                 ptr::null_mut(),\n-                                                 noname());\n+        let (start_fn, args) = if use_start_lang_item {\n+            let start_def_id = ccx.tcx().require_lang_item(StartFnLangItem);\n+            let empty_substs = ccx.tcx().intern_substs(&[]);\n+            let start_fn = Callee::def(ccx, start_def_id, empty_substs).reify(ccx);\n+            (start_fn, vec![bld.pointercast(rust_main, Type::i8p(ccx).ptr_to()), get_param(llfn, 0),\n+                get_param(llfn, 1)])\n+        } else {\n+            debug!(\"using user-defined start fn\");\n+            (rust_main, vec![get_param(llfn, 0 as c_uint), get_param(llfn, 1 as c_uint)])\n+        };\n \n-            llvm::LLVMBuildRet(bld, result);\n-        }\n+        let result = bld.call(start_fn, &args, None);\n+        bld.ret(result);\n     }\n }\n "}, {"sha": "60bd3fb8ef1b8db10ed2417a2a6db484d5e8e73a", "filename": "src/librustc_trans/basic_block.rs", "status": "removed", "additions": 0, "deletions": 58, "changes": 58, "blob_url": "https://github.com/rust-lang/rust/blob/439c3128d740af372dae163310f7292e999098e1/src%2Flibrustc_trans%2Fbasic_block.rs", "raw_url": "https://github.com/rust-lang/rust/raw/439c3128d740af372dae163310f7292e999098e1/src%2Flibrustc_trans%2Fbasic_block.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_trans%2Fbasic_block.rs?ref=439c3128d740af372dae163310f7292e999098e1", "patch": "@@ -1,58 +0,0 @@\n-// Copyright 2013 The Rust Project Developers. See the COPYRIGHT\n-// file at the top-level directory of this distribution and at\n-// http://rust-lang.org/COPYRIGHT.\n-//\n-// Licensed under the Apache License, Version 2.0 <LICENSE-APACHE or\n-// http://www.apache.org/licenses/LICENSE-2.0> or the MIT license\n-// <LICENSE-MIT or http://opensource.org/licenses/MIT>, at your\n-// option. This file may not be copied, modified, or distributed\n-// except according to those terms.\n-\n-use llvm;\n-use llvm::BasicBlockRef;\n-use value::{Users, Value};\n-use std::iter::{Filter, Map};\n-\n-#[derive(Copy, Clone)]\n-pub struct BasicBlock(pub BasicBlockRef);\n-\n-pub type Preds = Map<Filter<Users, fn(&Value) -> bool>, fn(Value) -> BasicBlock>;\n-\n-/// Wrapper for LLVM BasicBlockRef\n-impl BasicBlock {\n-    pub fn get(&self) -> BasicBlockRef {\n-        let BasicBlock(v) = *self; v\n-    }\n-\n-    pub fn as_value(self) -> Value {\n-        unsafe {\n-            Value(llvm::LLVMBasicBlockAsValue(self.get()))\n-        }\n-    }\n-\n-    pub fn pred_iter(self) -> Preds {\n-        fn is_a_terminator_inst(user: &Value) -> bool { user.is_a_terminator_inst() }\n-        let is_a_terminator_inst: fn(&Value) -> bool = is_a_terminator_inst;\n-\n-        fn get_parent(user: Value) -> BasicBlock { user.get_parent().unwrap() }\n-        let get_parent: fn(Value) -> BasicBlock = get_parent;\n-\n-        self.as_value().user_iter()\n-            .filter(is_a_terminator_inst)\n-            .map(get_parent)\n-    }\n-\n-    pub fn get_single_predecessor(self) -> Option<BasicBlock> {\n-        let mut iter = self.pred_iter();\n-        match (iter.next(), iter.next()) {\n-            (Some(first), None) => Some(first),\n-            _ => None\n-        }\n-    }\n-\n-    pub fn delete(self) {\n-        unsafe {\n-            llvm::LLVMDeleteBasicBlock(self.0);\n-        }\n-    }\n-}"}, {"sha": "8cd47bd148d0cf597e5241a65bc6b0b511a7635f", "filename": "src/librustc_trans/build.rs", "status": "removed", "additions": 0, "deletions": 1167, "changes": 1167, "blob_url": "https://github.com/rust-lang/rust/blob/439c3128d740af372dae163310f7292e999098e1/src%2Flibrustc_trans%2Fbuild.rs", "raw_url": "https://github.com/rust-lang/rust/raw/439c3128d740af372dae163310f7292e999098e1/src%2Flibrustc_trans%2Fbuild.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_trans%2Fbuild.rs?ref=439c3128d740af372dae163310f7292e999098e1", "patch": "@@ -1,1167 +0,0 @@\n-// Copyright 2012-2014 The Rust Project Developers. See the COPYRIGHT\n-// file at the top-level directory of this distribution and at\n-// http://rust-lang.org/COPYRIGHT.\n-//\n-// Licensed under the Apache License, Version 2.0 <LICENSE-APACHE or\n-// http://www.apache.org/licenses/LICENSE-2.0> or the MIT license\n-// <LICENSE-MIT or http://opensource.org/licenses/MIT>, at your\n-// option. This file may not be copied, modified, or distributed\n-// except according to those terms.\n-\n-#![allow(dead_code)] // FFI wrappers\n-#![allow(non_snake_case)]\n-\n-use llvm;\n-use llvm::{AtomicRmwBinOp, AtomicOrdering, SynchronizationScope, AsmDialect};\n-use llvm::{Opcode, IntPredicate, RealPredicate};\n-use llvm::{ValueRef, BasicBlockRef};\n-use common::*;\n-use syntax_pos::Span;\n-\n-use builder::Builder;\n-use type_::Type;\n-use value::Value;\n-use debuginfo::DebugLoc;\n-\n-use libc::{c_uint, c_char};\n-\n-pub fn terminate(cx: Block, _: &str) {\n-    debug!(\"terminate({})\", cx.to_str());\n-    cx.terminated.set(true);\n-}\n-\n-pub fn check_not_terminated(cx: Block) {\n-    if cx.terminated.get() {\n-        bug!(\"already terminated!\");\n-    }\n-}\n-\n-pub fn B<'blk, 'tcx>(cx: Block<'blk, 'tcx>) -> Builder<'blk, 'tcx> {\n-    let b = cx.fcx.ccx.builder();\n-    b.position_at_end(cx.llbb);\n-    b\n-}\n-\n-// The difference between a block being unreachable and being terminated is\n-// somewhat obscure, and has to do with error checking. When a block is\n-// terminated, we're saying that trying to add any further statements in the\n-// block is an error. On the other hand, if something is unreachable, that\n-// means that the block was terminated in some way that we don't want to check\n-// for (panic/break/return statements, call to diverging functions, etc), and\n-// further instructions to the block should simply be ignored.\n-\n-pub fn RetVoid(cx: Block, debug_loc: DebugLoc) {\n-    if cx.unreachable.get() {\n-        return;\n-    }\n-    check_not_terminated(cx);\n-    terminate(cx, \"RetVoid\");\n-    debug_loc.apply(cx.fcx);\n-    B(cx).ret_void();\n-}\n-\n-pub fn Ret(cx: Block, v: ValueRef, debug_loc: DebugLoc) {\n-    if cx.unreachable.get() {\n-        return;\n-    }\n-    check_not_terminated(cx);\n-    terminate(cx, \"Ret\");\n-    debug_loc.apply(cx.fcx);\n-    B(cx).ret(v);\n-}\n-\n-pub fn AggregateRet(cx: Block,\n-                    ret_vals: &[ValueRef],\n-                    debug_loc: DebugLoc) {\n-    if cx.unreachable.get() {\n-        return;\n-    }\n-    check_not_terminated(cx);\n-    terminate(cx, \"AggregateRet\");\n-    debug_loc.apply(cx.fcx);\n-    B(cx).aggregate_ret(ret_vals);\n-}\n-\n-pub fn Br(cx: Block, dest: BasicBlockRef, debug_loc: DebugLoc) {\n-    if cx.unreachable.get() {\n-        return;\n-    }\n-    check_not_terminated(cx);\n-    terminate(cx, \"Br\");\n-    debug_loc.apply(cx.fcx);\n-    B(cx).br(dest);\n-}\n-\n-pub fn CondBr(cx: Block,\n-              if_: ValueRef,\n-              then: BasicBlockRef,\n-              else_: BasicBlockRef,\n-              debug_loc: DebugLoc) {\n-    if cx.unreachable.get() {\n-        return;\n-    }\n-    check_not_terminated(cx);\n-    terminate(cx, \"CondBr\");\n-    debug_loc.apply(cx.fcx);\n-    B(cx).cond_br(if_, then, else_);\n-}\n-\n-pub fn Switch(cx: Block, v: ValueRef, else_: BasicBlockRef, num_cases: usize)\n-    -> ValueRef {\n-    if cx.unreachable.get() { return _Undef(v); }\n-    check_not_terminated(cx);\n-    terminate(cx, \"Switch\");\n-    B(cx).switch(v, else_, num_cases)\n-}\n-\n-pub fn AddCase(s: ValueRef, on_val: ValueRef, dest: BasicBlockRef) {\n-    unsafe {\n-        if llvm::LLVMIsUndef(s) == llvm::True { return; }\n-        llvm::LLVMAddCase(s, on_val, dest);\n-    }\n-}\n-\n-pub fn IndirectBr(cx: Block,\n-                  addr: ValueRef,\n-                  num_dests: usize,\n-                  debug_loc: DebugLoc) {\n-    if cx.unreachable.get() {\n-        return;\n-    }\n-    check_not_terminated(cx);\n-    terminate(cx, \"IndirectBr\");\n-    debug_loc.apply(cx.fcx);\n-    B(cx).indirect_br(addr, num_dests);\n-}\n-\n-pub fn Invoke(cx: Block,\n-              fn_: ValueRef,\n-              args: &[ValueRef],\n-              then: BasicBlockRef,\n-              catch: BasicBlockRef,\n-              debug_loc: DebugLoc)\n-              -> ValueRef {\n-    if cx.unreachable.get() {\n-        return C_null(Type::i8(cx.ccx()));\n-    }\n-    check_not_terminated(cx);\n-    terminate(cx, \"Invoke\");\n-    debug!(\"Invoke({:?} with arguments ({}))\",\n-           Value(fn_),\n-           args.iter().map(|a| {\n-                format!(\"{:?}\", Value(*a))\n-           }).collect::<Vec<String>>().join(\", \"));\n-    debug_loc.apply(cx.fcx);\n-    let bundle = cx.lpad().and_then(|b| b.bundle());\n-    B(cx).invoke(fn_, args, then, catch, bundle)\n-}\n-\n-pub fn Unreachable(cx: Block) {\n-    if cx.unreachable.get() {\n-        return\n-    }\n-    cx.unreachable.set(true);\n-    if !cx.terminated.get() {\n-        B(cx).unreachable();\n-    }\n-}\n-\n-pub fn _Undef(val: ValueRef) -> ValueRef {\n-    unsafe {\n-        return llvm::LLVMGetUndef(val_ty(val).to_ref());\n-    }\n-}\n-\n-/* Arithmetic */\n-pub fn Add(cx: Block,\n-           lhs: ValueRef,\n-           rhs: ValueRef,\n-           debug_loc: DebugLoc)\n-           -> ValueRef {\n-    if cx.unreachable.get() {\n-        return _Undef(lhs);\n-    }\n-    debug_loc.apply(cx.fcx);\n-    B(cx).add(lhs, rhs)\n-}\n-\n-pub fn NSWAdd(cx: Block,\n-              lhs: ValueRef,\n-              rhs: ValueRef,\n-              debug_loc: DebugLoc)\n-              -> ValueRef {\n-    if cx.unreachable.get() {\n-        return _Undef(lhs);\n-    }\n-    debug_loc.apply(cx.fcx);\n-    B(cx).nswadd(lhs, rhs)\n-}\n-\n-pub fn NUWAdd(cx: Block,\n-              lhs: ValueRef,\n-              rhs: ValueRef,\n-              debug_loc: DebugLoc)\n-              -> ValueRef {\n-    if cx.unreachable.get() {\n-        return _Undef(lhs);\n-    }\n-    debug_loc.apply(cx.fcx);\n-    B(cx).nuwadd(lhs, rhs)\n-}\n-\n-pub fn FAdd(cx: Block,\n-            lhs: ValueRef,\n-            rhs: ValueRef,\n-            debug_loc: DebugLoc)\n-            -> ValueRef {\n-    if cx.unreachable.get() {\n-        return _Undef(lhs);\n-    }\n-    debug_loc.apply(cx.fcx);\n-    B(cx).fadd(lhs, rhs)\n-}\n-\n-pub fn FAddFast(cx: Block,\n-                lhs: ValueRef,\n-                rhs: ValueRef,\n-                debug_loc: DebugLoc)\n-            -> ValueRef {\n-    if cx.unreachable.get() {\n-        return _Undef(lhs);\n-    }\n-    debug_loc.apply(cx.fcx);\n-    B(cx).fadd_fast(lhs, rhs)\n-}\n-\n-pub fn Sub(cx: Block,\n-           lhs: ValueRef,\n-           rhs: ValueRef,\n-           debug_loc: DebugLoc)\n-           -> ValueRef {\n-    if cx.unreachable.get() {\n-        return _Undef(lhs);\n-    }\n-    debug_loc.apply(cx.fcx);\n-    B(cx).sub(lhs, rhs)\n-}\n-\n-pub fn NSWSub(cx: Block,\n-              lhs: ValueRef,\n-              rhs: ValueRef,\n-              debug_loc: DebugLoc)\n-              -> ValueRef {\n-    if cx.unreachable.get() {\n-        return _Undef(lhs);\n-    }\n-    debug_loc.apply(cx.fcx);\n-    B(cx).nswsub(lhs, rhs)\n-}\n-\n-pub fn NUWSub(cx: Block,\n-              lhs: ValueRef,\n-              rhs: ValueRef,\n-              debug_loc: DebugLoc)\n-              -> ValueRef {\n-    if cx.unreachable.get() {\n-        return _Undef(lhs);\n-    }\n-    debug_loc.apply(cx.fcx);\n-    B(cx).nuwsub(lhs, rhs)\n-}\n-\n-pub fn FSub(cx: Block,\n-            lhs: ValueRef,\n-            rhs: ValueRef,\n-            debug_loc: DebugLoc)\n-            -> ValueRef {\n-    if cx.unreachable.get() {\n-        return _Undef(lhs);\n-    }\n-    debug_loc.apply(cx.fcx);\n-    B(cx).fsub(lhs, rhs)\n-}\n-\n-pub fn FSubFast(cx: Block,\n-                lhs: ValueRef,\n-                rhs: ValueRef,\n-                debug_loc: DebugLoc)\n-            -> ValueRef {\n-    if cx.unreachable.get() {\n-        return _Undef(lhs);\n-    }\n-    debug_loc.apply(cx.fcx);\n-    B(cx).fsub_fast(lhs, rhs)\n-}\n-\n-pub fn Mul(cx: Block,\n-           lhs: ValueRef,\n-           rhs: ValueRef,\n-           debug_loc: DebugLoc)\n-           -> ValueRef {\n-    if cx.unreachable.get() {\n-        return _Undef(lhs);\n-    }\n-    debug_loc.apply(cx.fcx);\n-    B(cx).mul(lhs, rhs)\n-}\n-\n-pub fn NSWMul(cx: Block,\n-              lhs: ValueRef,\n-              rhs: ValueRef,\n-              debug_loc: DebugLoc)\n-              -> ValueRef {\n-    if cx.unreachable.get() {\n-        return _Undef(lhs);\n-    }\n-    debug_loc.apply(cx.fcx);\n-    B(cx).nswmul(lhs, rhs)\n-}\n-\n-pub fn NUWMul(cx: Block,\n-              lhs: ValueRef,\n-              rhs: ValueRef,\n-              debug_loc: DebugLoc)\n-              -> ValueRef {\n-    if cx.unreachable.get() {\n-        return _Undef(lhs);\n-    }\n-    debug_loc.apply(cx.fcx);\n-    B(cx).nuwmul(lhs, rhs)\n-}\n-\n-pub fn FMul(cx: Block,\n-            lhs: ValueRef,\n-            rhs: ValueRef,\n-            debug_loc: DebugLoc)\n-            -> ValueRef {\n-    if cx.unreachable.get() {\n-        return _Undef(lhs);\n-    }\n-    debug_loc.apply(cx.fcx);\n-    B(cx).fmul(lhs, rhs)\n-}\n-\n-pub fn FMulFast(cx: Block,\n-                lhs: ValueRef,\n-                rhs: ValueRef,\n-                debug_loc: DebugLoc)\n-            -> ValueRef {\n-    if cx.unreachable.get() {\n-        return _Undef(lhs);\n-    }\n-    debug_loc.apply(cx.fcx);\n-    B(cx).fmul_fast(lhs, rhs)\n-}\n-\n-pub fn UDiv(cx: Block,\n-            lhs: ValueRef,\n-            rhs: ValueRef,\n-            debug_loc: DebugLoc)\n-            -> ValueRef {\n-    if cx.unreachable.get() {\n-        return _Undef(lhs);\n-    }\n-    debug_loc.apply(cx.fcx);\n-    B(cx).udiv(lhs, rhs)\n-}\n-\n-pub fn SDiv(cx: Block,\n-            lhs: ValueRef,\n-            rhs: ValueRef,\n-            debug_loc: DebugLoc)\n-            -> ValueRef {\n-    if cx.unreachable.get() {\n-        return _Undef(lhs);\n-    }\n-    debug_loc.apply(cx.fcx);\n-    B(cx).sdiv(lhs, rhs)\n-}\n-\n-pub fn ExactSDiv(cx: Block,\n-                 lhs: ValueRef,\n-                 rhs: ValueRef,\n-                 debug_loc: DebugLoc)\n-                 -> ValueRef {\n-    if cx.unreachable.get() {\n-        return _Undef(lhs);\n-    }\n-    debug_loc.apply(cx.fcx);\n-    B(cx).exactsdiv(lhs, rhs)\n-}\n-\n-pub fn FDiv(cx: Block,\n-            lhs: ValueRef,\n-            rhs: ValueRef,\n-            debug_loc: DebugLoc)\n-            -> ValueRef {\n-    if cx.unreachable.get() {\n-        return _Undef(lhs);\n-    }\n-    debug_loc.apply(cx.fcx);\n-    B(cx).fdiv(lhs, rhs)\n-}\n-\n-pub fn FDivFast(cx: Block,\n-                lhs: ValueRef,\n-                rhs: ValueRef,\n-                debug_loc: DebugLoc)\n-            -> ValueRef {\n-    if cx.unreachable.get() {\n-        return _Undef(lhs);\n-    }\n-    debug_loc.apply(cx.fcx);\n-    B(cx).fdiv_fast(lhs, rhs)\n-}\n-\n-pub fn URem(cx: Block,\n-            lhs: ValueRef,\n-            rhs: ValueRef,\n-            debug_loc: DebugLoc)\n-            -> ValueRef {\n-    if cx.unreachable.get() {\n-        return _Undef(lhs);\n-    }\n-    debug_loc.apply(cx.fcx);\n-    B(cx).urem(lhs, rhs)\n-}\n-\n-pub fn SRem(cx: Block,\n-            lhs: ValueRef,\n-            rhs: ValueRef,\n-            debug_loc: DebugLoc)\n-            -> ValueRef {\n-    if cx.unreachable.get() {\n-        return _Undef(lhs);\n-    }\n-    debug_loc.apply(cx.fcx);\n-    B(cx).srem(lhs, rhs)\n-}\n-\n-pub fn FRem(cx: Block,\n-            lhs: ValueRef,\n-            rhs: ValueRef,\n-            debug_loc: DebugLoc)\n-            -> ValueRef {\n-    if cx.unreachable.get() {\n-        return _Undef(lhs);\n-    }\n-    debug_loc.apply(cx.fcx);\n-    B(cx).frem(lhs, rhs)\n-}\n-\n-pub fn FRemFast(cx: Block,\n-                lhs: ValueRef,\n-                rhs: ValueRef,\n-                debug_loc: DebugLoc)\n-            -> ValueRef {\n-    if cx.unreachable.get() {\n-        return _Undef(lhs);\n-    }\n-    debug_loc.apply(cx.fcx);\n-    B(cx).frem_fast(lhs, rhs)\n-}\n-\n-pub fn Shl(cx: Block,\n-           lhs: ValueRef,\n-           rhs: ValueRef,\n-           debug_loc: DebugLoc)\n-           -> ValueRef {\n-    if cx.unreachable.get() {\n-        return _Undef(lhs);\n-    }\n-    debug_loc.apply(cx.fcx);\n-    B(cx).shl(lhs, rhs)\n-}\n-\n-pub fn LShr(cx: Block,\n-            lhs: ValueRef,\n-            rhs: ValueRef,\n-            debug_loc: DebugLoc)\n-            -> ValueRef {\n-    if cx.unreachable.get() {\n-        return _Undef(lhs);\n-    }\n-    debug_loc.apply(cx.fcx);\n-    B(cx).lshr(lhs, rhs)\n-}\n-\n-pub fn AShr(cx: Block,\n-            lhs: ValueRef,\n-            rhs: ValueRef,\n-            debug_loc: DebugLoc)\n-            -> ValueRef {\n-    if cx.unreachable.get() {\n-        return _Undef(lhs);\n-    }\n-    debug_loc.apply(cx.fcx);\n-    B(cx).ashr(lhs, rhs)\n-}\n-\n-pub fn And(cx: Block,\n-           lhs: ValueRef,\n-           rhs: ValueRef,\n-           debug_loc: DebugLoc)\n-           -> ValueRef {\n-    if cx.unreachable.get() {\n-        return _Undef(lhs);\n-    }\n-    debug_loc.apply(cx.fcx);\n-    B(cx).and(lhs, rhs)\n-}\n-\n-pub fn Or(cx: Block,\n-          lhs: ValueRef,\n-          rhs: ValueRef,\n-          debug_loc: DebugLoc)\n-          -> ValueRef {\n-    if cx.unreachable.get() {\n-        return _Undef(lhs);\n-    }\n-    debug_loc.apply(cx.fcx);\n-    B(cx).or(lhs, rhs)\n-}\n-\n-pub fn Xor(cx: Block,\n-           lhs: ValueRef,\n-           rhs: ValueRef,\n-           debug_loc: DebugLoc)\n-           -> ValueRef {\n-    if cx.unreachable.get() {\n-        return _Undef(lhs);\n-    }\n-    debug_loc.apply(cx.fcx);\n-    B(cx).xor(lhs, rhs)\n-}\n-\n-pub fn BinOp(cx: Block,\n-             op: Opcode,\n-             lhs: ValueRef,\n-             rhs: ValueRef,\n-             debug_loc: DebugLoc)\n-          -> ValueRef {\n-    if cx.unreachable.get() {\n-        return _Undef(lhs);\n-    }\n-    debug_loc.apply(cx.fcx);\n-    B(cx).binop(op, lhs, rhs)\n-}\n-\n-pub fn Neg(cx: Block, v: ValueRef, debug_loc: DebugLoc) -> ValueRef {\n-    if cx.unreachable.get() {\n-        return _Undef(v);\n-    }\n-    debug_loc.apply(cx.fcx);\n-    B(cx).neg(v)\n-}\n-\n-pub fn NSWNeg(cx: Block, v: ValueRef, debug_loc: DebugLoc) -> ValueRef {\n-    if cx.unreachable.get() {\n-        return _Undef(v);\n-    }\n-    debug_loc.apply(cx.fcx);\n-    B(cx).nswneg(v)\n-}\n-\n-pub fn NUWNeg(cx: Block, v: ValueRef, debug_loc: DebugLoc) -> ValueRef {\n-    if cx.unreachable.get() {\n-        return _Undef(v);\n-    }\n-    debug_loc.apply(cx.fcx);\n-    B(cx).nuwneg(v)\n-}\n-pub fn FNeg(cx: Block, v: ValueRef, debug_loc: DebugLoc) -> ValueRef {\n-    if cx.unreachable.get() {\n-        return _Undef(v);\n-    }\n-    debug_loc.apply(cx.fcx);\n-    B(cx).fneg(v)\n-}\n-\n-pub fn Not(cx: Block, v: ValueRef, debug_loc: DebugLoc) -> ValueRef {\n-    if cx.unreachable.get() {\n-        return _Undef(v);\n-    }\n-    debug_loc.apply(cx.fcx);\n-    B(cx).not(v)\n-}\n-\n-pub fn Alloca(cx: Block, ty: Type, name: &str) -> ValueRef {\n-    unsafe {\n-        if cx.unreachable.get() { return llvm::LLVMGetUndef(ty.ptr_to().to_ref()); }\n-        AllocaFcx(cx.fcx, ty, name)\n-    }\n-}\n-\n-pub fn AllocaFcx(fcx: &FunctionContext, ty: Type, name: &str) -> ValueRef {\n-    let b = fcx.ccx.builder();\n-    b.position_before(fcx.alloca_insert_pt.get().unwrap());\n-    DebugLoc::None.apply(fcx);\n-    b.alloca(ty, name)\n-}\n-\n-pub fn Free(cx: Block, pointer_val: ValueRef) {\n-    if cx.unreachable.get() { return; }\n-    B(cx).free(pointer_val)\n-}\n-\n-pub fn Load(cx: Block, pointer_val: ValueRef) -> ValueRef {\n-    unsafe {\n-        let ccx = cx.fcx.ccx;\n-        if cx.unreachable.get() {\n-            let ty = val_ty(pointer_val);\n-            let eltty = if ty.kind() == llvm::Array {\n-                ty.element_type()\n-            } else {\n-                ccx.int_type()\n-            };\n-            return llvm::LLVMGetUndef(eltty.to_ref());\n-        }\n-        B(cx).load(pointer_val)\n-    }\n-}\n-\n-pub fn VolatileLoad(cx: Block, pointer_val: ValueRef) -> ValueRef {\n-    unsafe {\n-        if cx.unreachable.get() {\n-            return llvm::LLVMGetUndef(Type::nil(cx.ccx()).to_ref());\n-        }\n-        B(cx).volatile_load(pointer_val)\n-    }\n-}\n-\n-pub fn AtomicLoad(cx: Block, pointer_val: ValueRef, order: AtomicOrdering) -> ValueRef {\n-    unsafe {\n-        let ccx = cx.fcx.ccx;\n-        if cx.unreachable.get() {\n-            return llvm::LLVMGetUndef(ccx.int_type().to_ref());\n-        }\n-        B(cx).atomic_load(pointer_val, order)\n-    }\n-}\n-\n-\n-pub fn LoadRangeAssert(cx: Block, pointer_val: ValueRef, lo: u64,\n-                       hi: u64, signed: llvm::Bool) -> ValueRef {\n-    if cx.unreachable.get() {\n-        let ccx = cx.fcx.ccx;\n-        let ty = val_ty(pointer_val);\n-        let eltty = if ty.kind() == llvm::Array {\n-            ty.element_type()\n-        } else {\n-            ccx.int_type()\n-        };\n-        unsafe {\n-            llvm::LLVMGetUndef(eltty.to_ref())\n-        }\n-    } else {\n-        B(cx).load_range_assert(pointer_val, lo, hi, signed)\n-    }\n-}\n-\n-pub fn LoadNonNull(cx: Block, ptr: ValueRef) -> ValueRef {\n-    if cx.unreachable.get() {\n-        let ccx = cx.fcx.ccx;\n-        let ty = val_ty(ptr);\n-        let eltty = if ty.kind() == llvm::Array {\n-            ty.element_type()\n-        } else {\n-            ccx.int_type()\n-        };\n-        unsafe {\n-            llvm::LLVMGetUndef(eltty.to_ref())\n-        }\n-    } else {\n-        B(cx).load_nonnull(ptr)\n-    }\n-}\n-\n-pub fn Store(cx: Block, val: ValueRef, ptr: ValueRef) -> ValueRef {\n-    if cx.unreachable.get() { return C_nil(cx.ccx()); }\n-    B(cx).store(val, ptr)\n-}\n-\n-pub fn VolatileStore(cx: Block, val: ValueRef, ptr: ValueRef) -> ValueRef {\n-    if cx.unreachable.get() { return C_nil(cx.ccx()); }\n-    B(cx).volatile_store(val, ptr)\n-}\n-\n-pub fn AtomicStore(cx: Block, val: ValueRef, ptr: ValueRef, order: AtomicOrdering) {\n-    if cx.unreachable.get() { return; }\n-    B(cx).atomic_store(val, ptr, order)\n-}\n-\n-pub fn GEP(cx: Block, pointer: ValueRef, indices: &[ValueRef]) -> ValueRef {\n-    unsafe {\n-        if cx.unreachable.get() {\n-            return llvm::LLVMGetUndef(Type::nil(cx.ccx()).ptr_to().to_ref());\n-        }\n-        B(cx).gep(pointer, indices)\n-    }\n-}\n-\n-// Simple wrapper around GEP that takes an array of ints and wraps them\n-// in C_i32()\n-#[inline]\n-pub fn GEPi(cx: Block, base: ValueRef, ixs: &[usize]) -> ValueRef {\n-    unsafe {\n-        if cx.unreachable.get() {\n-            return llvm::LLVMGetUndef(Type::nil(cx.ccx()).ptr_to().to_ref());\n-        }\n-        B(cx).gepi(base, ixs)\n-    }\n-}\n-\n-pub fn InBoundsGEP(cx: Block, pointer: ValueRef, indices: &[ValueRef]) -> ValueRef {\n-    unsafe {\n-        if cx.unreachable.get() {\n-            return llvm::LLVMGetUndef(Type::nil(cx.ccx()).ptr_to().to_ref());\n-        }\n-        B(cx).inbounds_gep(pointer, indices)\n-    }\n-}\n-\n-pub fn StructGEP(cx: Block, pointer: ValueRef, idx: usize) -> ValueRef {\n-    unsafe {\n-        if cx.unreachable.get() {\n-            return llvm::LLVMGetUndef(Type::nil(cx.ccx()).ptr_to().to_ref());\n-        }\n-        B(cx).struct_gep(pointer, idx)\n-    }\n-}\n-\n-pub fn GlobalString(cx: Block, _str: *const c_char) -> ValueRef {\n-    unsafe {\n-        if cx.unreachable.get() {\n-            return llvm::LLVMGetUndef(Type::i8p(cx.ccx()).to_ref());\n-        }\n-        B(cx).global_string(_str)\n-    }\n-}\n-\n-pub fn GlobalStringPtr(cx: Block, _str: *const c_char) -> ValueRef {\n-    unsafe {\n-        if cx.unreachable.get() {\n-            return llvm::LLVMGetUndef(Type::i8p(cx.ccx()).to_ref());\n-        }\n-        B(cx).global_string_ptr(_str)\n-    }\n-}\n-\n-/* Casts */\n-pub fn Trunc(cx: Block, val: ValueRef, dest_ty: Type) -> ValueRef {\n-    unsafe {\n-        if cx.unreachable.get() { return llvm::LLVMGetUndef(dest_ty.to_ref()); }\n-        B(cx).trunc(val, dest_ty)\n-    }\n-}\n-\n-pub fn ZExt(cx: Block, val: ValueRef, dest_ty: Type) -> ValueRef {\n-    unsafe {\n-        if cx.unreachable.get() { return llvm::LLVMGetUndef(dest_ty.to_ref()); }\n-        B(cx).zext(val, dest_ty)\n-    }\n-}\n-\n-pub fn SExt(cx: Block, val: ValueRef, dest_ty: Type) -> ValueRef {\n-    unsafe {\n-        if cx.unreachable.get() { return llvm::LLVMGetUndef(dest_ty.to_ref()); }\n-        B(cx).sext(val, dest_ty)\n-    }\n-}\n-\n-pub fn FPToUI(cx: Block, val: ValueRef, dest_ty: Type) -> ValueRef {\n-    unsafe {\n-        if cx.unreachable.get() { return llvm::LLVMGetUndef(dest_ty.to_ref()); }\n-        B(cx).fptoui(val, dest_ty)\n-    }\n-}\n-\n-pub fn FPToSI(cx: Block, val: ValueRef, dest_ty: Type) -> ValueRef {\n-    unsafe {\n-        if cx.unreachable.get() { return llvm::LLVMGetUndef(dest_ty.to_ref()); }\n-        B(cx).fptosi(val, dest_ty)\n-    }\n-}\n-\n-pub fn UIToFP(cx: Block, val: ValueRef, dest_ty: Type) -> ValueRef {\n-    unsafe {\n-        if cx.unreachable.get() { return llvm::LLVMGetUndef(dest_ty.to_ref()); }\n-        B(cx).uitofp(val, dest_ty)\n-    }\n-}\n-\n-pub fn SIToFP(cx: Block, val: ValueRef, dest_ty: Type) -> ValueRef {\n-    unsafe {\n-        if cx.unreachable.get() { return llvm::LLVMGetUndef(dest_ty.to_ref()); }\n-        B(cx).sitofp(val, dest_ty)\n-    }\n-}\n-\n-pub fn FPTrunc(cx: Block, val: ValueRef, dest_ty: Type) -> ValueRef {\n-    unsafe {\n-        if cx.unreachable.get() { return llvm::LLVMGetUndef(dest_ty.to_ref()); }\n-        B(cx).fptrunc(val, dest_ty)\n-    }\n-}\n-\n-pub fn FPExt(cx: Block, val: ValueRef, dest_ty: Type) -> ValueRef {\n-    unsafe {\n-        if cx.unreachable.get() { return llvm::LLVMGetUndef(dest_ty.to_ref()); }\n-        B(cx).fpext(val, dest_ty)\n-    }\n-}\n-\n-pub fn PtrToInt(cx: Block, val: ValueRef, dest_ty: Type) -> ValueRef {\n-    unsafe {\n-        if cx.unreachable.get() { return llvm::LLVMGetUndef(dest_ty.to_ref()); }\n-        B(cx).ptrtoint(val, dest_ty)\n-    }\n-}\n-\n-pub fn IntToPtr(cx: Block, val: ValueRef, dest_ty: Type) -> ValueRef {\n-    unsafe {\n-        if cx.unreachable.get() { return llvm::LLVMGetUndef(dest_ty.to_ref()); }\n-        B(cx).inttoptr(val, dest_ty)\n-    }\n-}\n-\n-pub fn BitCast(cx: Block, val: ValueRef, dest_ty: Type) -> ValueRef {\n-    unsafe {\n-        if cx.unreachable.get() { return llvm::LLVMGetUndef(dest_ty.to_ref()); }\n-        B(cx).bitcast(val, dest_ty)\n-    }\n-}\n-\n-pub fn ZExtOrBitCast(cx: Block, val: ValueRef, dest_ty: Type) -> ValueRef {\n-    unsafe {\n-        if cx.unreachable.get() { return llvm::LLVMGetUndef(dest_ty.to_ref()); }\n-        B(cx).zext_or_bitcast(val, dest_ty)\n-    }\n-}\n-\n-pub fn SExtOrBitCast(cx: Block, val: ValueRef, dest_ty: Type) -> ValueRef {\n-    unsafe {\n-        if cx.unreachable.get() { return llvm::LLVMGetUndef(dest_ty.to_ref()); }\n-        B(cx).sext_or_bitcast(val, dest_ty)\n-    }\n-}\n-\n-pub fn TruncOrBitCast(cx: Block, val: ValueRef, dest_ty: Type) -> ValueRef {\n-    unsafe {\n-        if cx.unreachable.get() { return llvm::LLVMGetUndef(dest_ty.to_ref()); }\n-        B(cx).trunc_or_bitcast(val, dest_ty)\n-    }\n-}\n-\n-pub fn Cast(cx: Block, op: Opcode, val: ValueRef, dest_ty: Type,\n-            _: *const u8)\n-     -> ValueRef {\n-    unsafe {\n-        if cx.unreachable.get() { return llvm::LLVMGetUndef(dest_ty.to_ref()); }\n-        B(cx).cast(op, val, dest_ty)\n-    }\n-}\n-\n-pub fn PointerCast(cx: Block, val: ValueRef, dest_ty: Type) -> ValueRef {\n-    unsafe {\n-        if cx.unreachable.get() { return llvm::LLVMGetUndef(dest_ty.to_ref()); }\n-        B(cx).pointercast(val, dest_ty)\n-    }\n-}\n-\n-pub fn IntCast(cx: Block, val: ValueRef, dest_ty: Type) -> ValueRef {\n-    unsafe {\n-        if cx.unreachable.get() { return llvm::LLVMGetUndef(dest_ty.to_ref()); }\n-        B(cx).intcast(val, dest_ty)\n-    }\n-}\n-\n-pub fn FPCast(cx: Block, val: ValueRef, dest_ty: Type) -> ValueRef {\n-    unsafe {\n-        if cx.unreachable.get() { return llvm::LLVMGetUndef(dest_ty.to_ref()); }\n-        B(cx).fpcast(val, dest_ty)\n-    }\n-}\n-\n-\n-/* Comparisons */\n-pub fn ICmp(cx: Block,\n-            op: IntPredicate,\n-            lhs: ValueRef,\n-            rhs: ValueRef,\n-            debug_loc: DebugLoc)\n-            -> ValueRef {\n-    unsafe {\n-        if cx.unreachable.get() {\n-            return llvm::LLVMGetUndef(Type::i1(cx.ccx()).to_ref());\n-        }\n-        debug_loc.apply(cx.fcx);\n-        B(cx).icmp(op, lhs, rhs)\n-    }\n-}\n-\n-pub fn FCmp(cx: Block,\n-            op: RealPredicate,\n-            lhs: ValueRef,\n-            rhs: ValueRef,\n-            debug_loc: DebugLoc)\n-            -> ValueRef {\n-    unsafe {\n-        if cx.unreachable.get() {\n-            return llvm::LLVMGetUndef(Type::i1(cx.ccx()).to_ref());\n-        }\n-        debug_loc.apply(cx.fcx);\n-        B(cx).fcmp(op, lhs, rhs)\n-    }\n-}\n-\n-/* Miscellaneous instructions */\n-pub fn EmptyPhi(cx: Block, ty: Type) -> ValueRef {\n-    unsafe {\n-        if cx.unreachable.get() { return llvm::LLVMGetUndef(ty.to_ref()); }\n-        B(cx).empty_phi(ty)\n-    }\n-}\n-\n-pub fn Phi(cx: Block, ty: Type, vals: &[ValueRef],\n-           bbs: &[BasicBlockRef]) -> ValueRef {\n-    unsafe {\n-        if cx.unreachable.get() { return llvm::LLVMGetUndef(ty.to_ref()); }\n-        B(cx).phi(ty, vals, bbs)\n-    }\n-}\n-\n-pub fn AddIncomingToPhi(phi: ValueRef, val: ValueRef, bb: BasicBlockRef) {\n-    unsafe {\n-        if llvm::LLVMIsUndef(phi) == llvm::True { return; }\n-        llvm::LLVMAddIncoming(phi, &val, &bb, 1 as c_uint);\n-    }\n-}\n-\n-pub fn _UndefReturn(cx: Block, fn_: ValueRef) -> ValueRef {\n-    unsafe {\n-        let ccx = cx.fcx.ccx;\n-        let ty = val_ty(fn_);\n-        let retty = if ty.kind() == llvm::Function {\n-            ty.return_type()\n-        } else {\n-            ccx.int_type()\n-        };\n-        B(cx).count_insn(\"ret_undef\");\n-        llvm::LLVMGetUndef(retty.to_ref())\n-    }\n-}\n-\n-pub fn add_span_comment(cx: Block, sp: Span, text: &str) {\n-    B(cx).add_span_comment(sp, text)\n-}\n-\n-pub fn add_comment(cx: Block, text: &str) {\n-    B(cx).add_comment(text)\n-}\n-\n-pub fn InlineAsmCall(cx: Block, asm: *const c_char, cons: *const c_char,\n-                     inputs: &[ValueRef], output: Type,\n-                     volatile: bool, alignstack: bool,\n-                     dia: AsmDialect) -> ValueRef {\n-    B(cx).inline_asm_call(asm, cons, inputs, output, volatile, alignstack, dia)\n-}\n-\n-pub fn Call(cx: Block,\n-            fn_: ValueRef,\n-            args: &[ValueRef],\n-            debug_loc: DebugLoc)\n-            -> ValueRef {\n-    if cx.unreachable.get() {\n-        return _UndefReturn(cx, fn_);\n-    }\n-    debug_loc.apply(cx.fcx);\n-    let bundle = cx.lpad.get().and_then(|b| b.bundle());\n-    B(cx).call(fn_, args, bundle)\n-}\n-\n-pub fn AtomicFence(cx: Block, order: AtomicOrdering, scope: SynchronizationScope) {\n-    if cx.unreachable.get() { return; }\n-    B(cx).atomic_fence(order, scope)\n-}\n-\n-pub fn Select(cx: Block, if_: ValueRef, then: ValueRef, else_: ValueRef) -> ValueRef {\n-    if cx.unreachable.get() { return _Undef(then); }\n-    B(cx).select(if_, then, else_)\n-}\n-\n-pub fn VAArg(cx: Block, list: ValueRef, ty: Type) -> ValueRef {\n-    unsafe {\n-        if cx.unreachable.get() { return llvm::LLVMGetUndef(ty.to_ref()); }\n-        B(cx).va_arg(list, ty)\n-    }\n-}\n-\n-pub fn ExtractElement(cx: Block, vec_val: ValueRef, index: ValueRef) -> ValueRef {\n-    unsafe {\n-        if cx.unreachable.get() {\n-            return llvm::LLVMGetUndef(Type::nil(cx.ccx()).to_ref());\n-        }\n-        B(cx).extract_element(vec_val, index)\n-    }\n-}\n-\n-pub fn InsertElement(cx: Block, vec_val: ValueRef, elt_val: ValueRef,\n-                     index: ValueRef) -> ValueRef {\n-    unsafe {\n-        if cx.unreachable.get() {\n-            return llvm::LLVMGetUndef(Type::nil(cx.ccx()).to_ref());\n-        }\n-        B(cx).insert_element(vec_val, elt_val, index)\n-    }\n-}\n-\n-pub fn ShuffleVector(cx: Block, v1: ValueRef, v2: ValueRef,\n-                     mask: ValueRef) -> ValueRef {\n-    unsafe {\n-        if cx.unreachable.get() {\n-            return llvm::LLVMGetUndef(Type::nil(cx.ccx()).to_ref());\n-        }\n-        B(cx).shuffle_vector(v1, v2, mask)\n-    }\n-}\n-\n-pub fn VectorSplat(cx: Block, num_elts: usize, elt_val: ValueRef) -> ValueRef {\n-    unsafe {\n-        if cx.unreachable.get() {\n-            return llvm::LLVMGetUndef(Type::nil(cx.ccx()).to_ref());\n-        }\n-        B(cx).vector_splat(num_elts, elt_val)\n-    }\n-}\n-\n-pub fn ExtractValue(cx: Block, agg_val: ValueRef, index: usize) -> ValueRef {\n-    unsafe {\n-        if cx.unreachable.get() {\n-            return llvm::LLVMGetUndef(Type::nil(cx.ccx()).to_ref());\n-        }\n-        B(cx).extract_value(agg_val, index)\n-    }\n-}\n-\n-pub fn InsertValue(cx: Block, agg_val: ValueRef, elt_val: ValueRef, index: usize) -> ValueRef {\n-    unsafe {\n-        if cx.unreachable.get() {\n-            return llvm::LLVMGetUndef(Type::nil(cx.ccx()).to_ref());\n-        }\n-        B(cx).insert_value(agg_val, elt_val, index)\n-    }\n-}\n-\n-pub fn IsNull(cx: Block, val: ValueRef) -> ValueRef {\n-    unsafe {\n-        if cx.unreachable.get() {\n-            return llvm::LLVMGetUndef(Type::i1(cx.ccx()).to_ref());\n-        }\n-        B(cx).is_null(val)\n-    }\n-}\n-\n-pub fn IsNotNull(cx: Block, val: ValueRef) -> ValueRef {\n-    unsafe {\n-        if cx.unreachable.get() {\n-            return llvm::LLVMGetUndef(Type::i1(cx.ccx()).to_ref());\n-        }\n-        B(cx).is_not_null(val)\n-    }\n-}\n-\n-pub fn PtrDiff(cx: Block, lhs: ValueRef, rhs: ValueRef) -> ValueRef {\n-    unsafe {\n-        let ccx = cx.fcx.ccx;\n-        if cx.unreachable.get() { return llvm::LLVMGetUndef(ccx.int_type().to_ref()); }\n-        B(cx).ptrdiff(lhs, rhs)\n-    }\n-}\n-\n-pub fn Trap(cx: Block) {\n-    if cx.unreachable.get() { return; }\n-    B(cx).trap();\n-}\n-\n-pub fn LandingPad(cx: Block, ty: Type, pers_fn: ValueRef,\n-                  num_clauses: usize) -> ValueRef {\n-    check_not_terminated(cx);\n-    assert!(!cx.unreachable.get());\n-    B(cx).landing_pad(ty, pers_fn, num_clauses, cx.fcx.llfn)\n-}\n-\n-pub fn AddClause(cx: Block, landing_pad: ValueRef, clause: ValueRef) {\n-    B(cx).add_clause(landing_pad, clause)\n-}\n-\n-pub fn SetCleanup(cx: Block, landing_pad: ValueRef) {\n-    B(cx).set_cleanup(landing_pad)\n-}\n-\n-pub fn SetPersonalityFn(cx: Block, f: ValueRef) {\n-    B(cx).set_personality_fn(f)\n-}\n-\n-pub fn Resume(cx: Block, exn: ValueRef) -> ValueRef {\n-    check_not_terminated(cx);\n-    terminate(cx, \"Resume\");\n-    B(cx).resume(exn)\n-}\n-\n-// Atomic Operations\n-pub fn AtomicCmpXchg(cx: Block, dst: ValueRef,\n-                     cmp: ValueRef, src: ValueRef,\n-                     order: AtomicOrdering,\n-                     failure_order: AtomicOrdering,\n-                     weak: llvm::Bool) -> ValueRef {\n-    B(cx).atomic_cmpxchg(dst, cmp, src, order, failure_order, weak)\n-}\n-pub fn AtomicRMW(cx: Block, op: AtomicRmwBinOp,\n-                 dst: ValueRef, src: ValueRef,\n-                 order: AtomicOrdering) -> ValueRef {\n-    B(cx).atomic_rmw(op, dst, src, order)\n-}\n-\n-pub fn CleanupPad(cx: Block,\n-                  parent: Option<ValueRef>,\n-                  args: &[ValueRef]) -> ValueRef {\n-    check_not_terminated(cx);\n-    assert!(!cx.unreachable.get());\n-    B(cx).cleanup_pad(parent, args)\n-}\n-\n-pub fn CleanupRet(cx: Block,\n-                  cleanup: ValueRef,\n-                  unwind: Option<BasicBlockRef>) -> ValueRef {\n-    check_not_terminated(cx);\n-    terminate(cx, \"CleanupRet\");\n-    B(cx).cleanup_ret(cleanup, unwind)\n-}\n-\n-pub fn CatchPad(cx: Block,\n-                parent: ValueRef,\n-                args: &[ValueRef]) -> ValueRef {\n-    check_not_terminated(cx);\n-    assert!(!cx.unreachable.get());\n-    B(cx).catch_pad(parent, args)\n-}\n-\n-pub fn CatchRet(cx: Block, pad: ValueRef, unwind: BasicBlockRef) -> ValueRef {\n-    check_not_terminated(cx);\n-    terminate(cx, \"CatchRet\");\n-    B(cx).catch_ret(pad, unwind)\n-}\n-\n-pub fn CatchSwitch(cx: Block,\n-                   parent: Option<ValueRef>,\n-                   unwind: Option<BasicBlockRef>,\n-                   num_handlers: usize) -> ValueRef {\n-    check_not_terminated(cx);\n-    terminate(cx, \"CatchSwitch\");\n-    B(cx).catch_switch(parent, unwind, num_handlers)\n-}\n-\n-pub fn AddHandler(cx: Block, catch_switch: ValueRef, handler: BasicBlockRef) {\n-    B(cx).add_handler(catch_switch, handler)\n-}"}, {"sha": "136d1aad31a03965077995779028848e8b3ae033", "filename": "src/librustc_trans/builder.rs", "status": "modified", "additions": 34, "deletions": 47, "changes": 81, "blob_url": "https://github.com/rust-lang/rust/blob/1b38776c1f68c6fd47c1b2f7b7974efc7dd64901/src%2Flibrustc_trans%2Fbuilder.rs", "raw_url": "https://github.com/rust-lang/rust/raw/1b38776c1f68c6fd47c1b2f7b7974efc7dd64901/src%2Flibrustc_trans%2Fbuilder.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_trans%2Fbuilder.rs?ref=1b38776c1f68c6fd47c1b2f7b7974efc7dd64901", "patch": "@@ -14,12 +14,10 @@ use llvm;\n use llvm::{AtomicRmwBinOp, AtomicOrdering, SynchronizationScope, AsmDialect};\n use llvm::{Opcode, IntPredicate, RealPredicate, False, OperandBundleDef};\n use llvm::{ValueRef, BasicBlockRef, BuilderRef, ModuleRef};\n-use base;\n use common::*;\n use machine::llalign_of_pref;\n use type_::Type;\n use value::Value;\n-use util::nodemap::FxHashMap;\n use libc::{c_uint, c_char};\n \n use std::borrow::Cow;\n@@ -32,65 +30,40 @@ pub struct Builder<'a, 'tcx: 'a> {\n     pub ccx: &'a CrateContext<'a, 'tcx>,\n }\n \n+impl<'a, 'tcx> Drop for Builder<'a, 'tcx> {\n+    fn drop(&mut self) {\n+        unsafe {\n+            llvm::LLVMDisposeBuilder(self.llbuilder);\n+        }\n+    }\n+}\n+\n // This is a really awful way to get a zero-length c-string, but better (and a\n // lot more efficient) than doing str::as_c_str(\"\", ...) every time.\n-pub fn noname() -> *const c_char {\n+fn noname() -> *const c_char {\n     static CNULL: c_char = 0;\n     &CNULL\n }\n \n impl<'a, 'tcx> Builder<'a, 'tcx> {\n-    pub fn new(ccx: &'a CrateContext<'a, 'tcx>) -> Builder<'a, 'tcx> {\n+    pub fn with_ccx(ccx: &'a CrateContext<'a, 'tcx>) -> Self {\n+        // Create a fresh builder from the crate context.\n+        let llbuilder = unsafe {\n+            llvm::LLVMCreateBuilderInContext(ccx.llcx())\n+        };\n         Builder {\n-            llbuilder: ccx.raw_builder(),\n+            llbuilder: llbuilder,\n             ccx: ccx,\n         }\n     }\n \n-    pub fn count_insn(&self, category: &str) {\n+    fn count_insn(&self, category: &str) {\n         if self.ccx.sess().trans_stats() {\n-            self.ccx.stats().n_llvm_insns.set(self.ccx\n-                                                .stats()\n-                                                .n_llvm_insns\n-                                                .get() + 1);\n+            self.ccx.stats().n_llvm_insns.set(self.ccx.stats().n_llvm_insns.get() + 1);\n         }\n-        self.ccx.count_llvm_insn();\n         if self.ccx.sess().count_llvm_insns() {\n-            base::with_insn_ctxt(|v| {\n-                let mut h = self.ccx.stats().llvm_insns.borrow_mut();\n-\n-                // Build version of path with cycles removed.\n-\n-                // Pass 1: scan table mapping str -> rightmost pos.\n-                let mut mm = FxHashMap();\n-                let len = v.len();\n-                let mut i = 0;\n-                while i < len {\n-                    mm.insert(v[i], i);\n-                    i += 1;\n-                }\n-\n-                // Pass 2: concat strings for each elt, skipping\n-                // forwards over any cycles by advancing to rightmost\n-                // occurrence of each element in path.\n-                let mut s = String::from(\".\");\n-                i = 0;\n-                while i < len {\n-                    i = mm[v[i]];\n-                    s.push('/');\n-                    s.push_str(v[i]);\n-                    i += 1;\n-                }\n-\n-                s.push('/');\n-                s.push_str(category);\n-\n-                let n = match h.get(&s) {\n-                    Some(&n) => n,\n-                    _ => 0\n-                };\n-                h.insert(s, n+1);\n-            })\n+            let mut h = self.ccx.stats().llvm_insns.borrow_mut();\n+            *h.entry(category.to_string()).or_insert(0) += 1;\n         }\n     }\n \n@@ -462,7 +435,7 @@ impl<'a, 'tcx> Builder<'a, 'tcx> {\n         }\n     }\n \n-    pub fn alloca(&self, ty: Type, name: &str) -> ValueRef {\n+    pub fn dynamic_alloca(&self, ty: Type, name: &str) -> ValueRef {\n         self.count_insn(\"alloca\");\n         unsafe {\n             if name.is_empty() {\n@@ -1103,6 +1076,20 @@ impl<'a, 'tcx> Builder<'a, 'tcx> {\n         }\n     }\n \n+    pub fn add_case(&self, s: ValueRef, on_val: ValueRef, dest: BasicBlockRef) {\n+        unsafe {\n+            if llvm::LLVMIsUndef(s) == llvm::True { return; }\n+            llvm::LLVMAddCase(s, on_val, dest)\n+        }\n+    }\n+\n+    pub fn add_incoming_to_phi(&self, phi: ValueRef, val: ValueRef, bb: BasicBlockRef) {\n+        unsafe {\n+            if llvm::LLVMIsUndef(phi) == llvm::True { return; }\n+            llvm::LLVMAddIncoming(phi, &val, &bb, 1 as c_uint);\n+        }\n+    }\n+\n     /// Returns the ptr value that should be used for storing `val`.\n     fn check_store<'b>(&self,\n                        val: ValueRef,"}, {"sha": "85b26074bae6d4358758eaea18d7bd0f3a5ba427", "filename": "src/librustc_trans/cabi_arm.rs", "status": "modified", "additions": 0, "deletions": 2, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/1b38776c1f68c6fd47c1b2f7b7974efc7dd64901/src%2Flibrustc_trans%2Fcabi_arm.rs", "raw_url": "https://github.com/rust-lang/rust/raw/1b38776c1f68c6fd47c1b2f7b7974efc7dd64901/src%2Flibrustc_trans%2Fcabi_arm.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_trans%2Fcabi_arm.rs?ref=1b38776c1f68c6fd47c1b2f7b7974efc7dd64901", "patch": "@@ -8,8 +8,6 @@\n // option. This file may not be copied, modified, or distributed\n // except according to those terms.\n \n-#![allow(non_upper_case_globals)]\n-\n use llvm::{Integer, Pointer, Float, Double, Struct, Array, Vector};\n use abi::{self, align_up_to, FnType, ArgType};\n use context::CrateContext;"}, {"sha": "ac832b6f746fd7262a037623af2e99e97800da46", "filename": "src/librustc_trans/callee.rs", "status": "modified", "additions": 59, "deletions": 167, "changes": 226, "blob_url": "https://github.com/rust-lang/rust/blob/1b38776c1f68c6fd47c1b2f7b7974efc7dd64901/src%2Flibrustc_trans%2Fcallee.rs", "raw_url": "https://github.com/rust-lang/rust/raw/1b38776c1f68c6fd47c1b2f7b7974efc7dd64901/src%2Flibrustc_trans%2Fcallee.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_trans%2Fcallee.rs?ref=1b38776c1f68c6fd47c1b2f7b7974efc7dd64901", "patch": "@@ -16,7 +16,6 @@\n \n pub use self::CalleeData::*;\n \n-use arena::TypedArena;\n use llvm::{self, ValueRef, get_params};\n use rustc::hir::def_id::DefId;\n use rustc::ty::subst::Substs;\n@@ -25,10 +24,10 @@ use abi::{Abi, FnType};\n use attributes;\n use base;\n use base::*;\n-use build::*;\n-use common::{self, Block, Result, CrateContext, FunctionContext, SharedCrateContext};\n+use common::{\n+    self, CrateContext, FunctionContext, SharedCrateContext\n+};\n use consts;\n-use debuginfo::DebugLoc;\n use declare;\n use value::Value;\n use meth;\n@@ -71,25 +70,8 @@ impl<'tcx> Callee<'tcx> {\n         }\n     }\n \n-    /// Trait or impl method call.\n-    pub fn method_call<'blk>(bcx: Block<'blk, 'tcx>,\n-                             method_call: ty::MethodCall)\n-                             -> Callee<'tcx> {\n-        let method = bcx.tcx().tables().method_map[&method_call];\n-        Callee::method(bcx, method)\n-    }\n-\n-    /// Trait or impl method.\n-    pub fn method<'blk>(bcx: Block<'blk, 'tcx>,\n-                        method: ty::MethodCallee<'tcx>) -> Callee<'tcx> {\n-        let substs = bcx.fcx.monomorphize(&method.substs);\n-        Callee::def(bcx.ccx(), method.def_id, substs)\n-    }\n-\n     /// Function or method definition.\n-    pub fn def<'a>(ccx: &CrateContext<'a, 'tcx>,\n-                   def_id: DefId,\n-                   substs: &'tcx Substs<'tcx>)\n+    pub fn def<'a>(ccx: &CrateContext<'a, 'tcx>, def_id: DefId, substs: &'tcx Substs<'tcx>)\n                    -> Callee<'tcx> {\n         let tcx = ccx.tcx();\n \n@@ -196,25 +178,6 @@ impl<'tcx> Callee<'tcx> {\n         fn_ty\n     }\n \n-    /// This behemoth of a function translates function calls. Unfortunately, in\n-    /// order to generate more efficient LLVM output at -O0, it has quite a complex\n-    /// signature (refactoring this into two functions seems like a good idea).\n-    ///\n-    /// In particular, for lang items, it is invoked with a dest of None, and in\n-    /// that case the return value contains the result of the fn. The lang item must\n-    /// not return a structural type or else all heck breaks loose.\n-    ///\n-    /// For non-lang items, `dest` is always Some, and hence the result is written\n-    /// into memory somewhere. Nonetheless we return the actual return value of the\n-    /// function.\n-    pub fn call<'a, 'blk>(self, bcx: Block<'blk, 'tcx>,\n-                          debug_loc: DebugLoc,\n-                          args: &[ValueRef],\n-                          dest: Option<ValueRef>)\n-                          -> Result<'blk, 'tcx> {\n-        trans_call_inner(bcx, debug_loc, self, args, dest)\n-    }\n-\n     /// Turn the callee into a function pointer.\n     pub fn reify<'a>(self, ccx: &CrateContext<'a, 'tcx>) -> ValueRef {\n         match self.data {\n@@ -267,8 +230,6 @@ fn trans_closure_method<'a, 'tcx>(ccx: &'a CrateContext<'a, 'tcx>,\n     // then adapt the self type\n     let llfn_closure_kind = ccx.tcx().closure_kind(def_id);\n \n-    let _icx = push_ctxt(\"trans_closure_adapter_shim\");\n-\n     debug!(\"trans_closure_adapter_shim(llfn_closure_kind={:?}, \\\n            trait_closure_kind={:?}, llfn={:?})\",\n            llfn_closure_kind, trait_closure_kind, Value(llfn));\n@@ -367,57 +328,66 @@ fn trans_fn_once_adapter_shim<'a, 'tcx>(\n     let lloncefn = declare::define_internal_fn(ccx, &function_name, llonce_fn_ty);\n     attributes::set_frame_pointer_elimination(ccx, lloncefn);\n \n-    let (block_arena, fcx): (TypedArena<_>, FunctionContext);\n-    block_arena = TypedArena::new();\n-    fcx = FunctionContext::new(ccx, lloncefn, fn_ty, None, &block_arena);\n-    let mut bcx = fcx.init(false);\n+    let orig_fn_ty = fn_ty;\n+    let fcx = FunctionContext::new(ccx, lloncefn);\n+    let mut bcx = fcx.get_entry_block();\n \n+    let callee = Callee {\n+        data: Fn(llreffn),\n+        ty: llref_fn_ty\n+    };\n \n     // the first argument (`self`) will be the (by value) closure env.\n \n     let mut llargs = get_params(fcx.llfn);\n-    let mut self_idx = fcx.fn_ty.ret.is_indirect() as usize;\n-    let env_arg = &fcx.fn_ty.args[0];\n+    let fn_ret = callee.ty.fn_ret();\n+    let fn_ty = callee.direct_fn_type(bcx.ccx, &[]);\n+    let self_idx = fn_ty.ret.is_indirect() as usize;\n+    let env_arg = &orig_fn_ty.args[0];\n     let llenv = if env_arg.is_indirect() {\n         llargs[self_idx]\n     } else {\n-        let scratch = alloc_ty(bcx, closure_ty, \"self\");\n+        let scratch = alloc_ty(&bcx, closure_ty, \"self\");\n         let mut llarg_idx = self_idx;\n-        env_arg.store_fn_arg(&bcx.build(), &mut llarg_idx, scratch);\n+        env_arg.store_fn_arg(&bcx, &mut llarg_idx, scratch);\n         scratch\n     };\n \n     debug!(\"trans_fn_once_adapter_shim: env={:?}\", Value(llenv));\n     // Adjust llargs such that llargs[self_idx..] has the call arguments.\n     // For zero-sized closures that means sneaking in a new argument.\n     if env_arg.is_ignore() {\n-        if self_idx > 0 {\n-            self_idx -= 1;\n-            llargs[self_idx] = llenv;\n-        } else {\n-            llargs.insert(0, llenv);\n-        }\n+        llargs.insert(self_idx, llenv);\n     } else {\n         llargs[self_idx] = llenv;\n     }\n \n-    let dest = fcx.llretslotptr.get();\n-\n-    let callee = Callee {\n-        data: Fn(llreffn),\n-        ty: llref_fn_ty\n-    };\n-\n     // Call the by-ref closure body with `self` in a cleanup scope,\n     // to drop `self` when the body returns, or in case it unwinds.\n-    let self_scope = fcx.push_custom_cleanup_scope();\n-    fcx.schedule_drop_mem(self_scope, llenv, closure_ty);\n-\n-    bcx = callee.call(bcx, DebugLoc::None, &llargs[self_idx..], dest).bcx;\n+    let self_scope = fcx.schedule_drop_mem(llenv, closure_ty);\n+\n+    let llfn = callee.reify(bcx.ccx);\n+    let llret;\n+    if let Some(landing_pad) = self_scope.landing_pad {\n+        let normal_bcx = bcx.fcx().build_new_block(\"normal-return\");\n+        llret = bcx.invoke(llfn, &llargs[..], normal_bcx.llbb(), landing_pad, None);\n+        bcx = normal_bcx;\n+    } else {\n+        llret = bcx.call(llfn, &llargs[..], None);\n+    }\n+    fn_ty.apply_attrs_callsite(llret);\n \n-    fcx.pop_and_trans_custom_cleanup_scope(bcx, self_scope);\n+    if fn_ret.0.is_never() {\n+        bcx.unreachable();\n+    } else {\n+        self_scope.trans(&bcx);\n \n-    fcx.finish(bcx, DebugLoc::None);\n+        if fn_ty.ret.is_indirect() || fn_ty.ret.is_ignore() {\n+            bcx.ret_void();\n+        } else {\n+            bcx.ret(llret);\n+        }\n+    }\n \n     ccx.instances().borrow_mut().insert(method_instance, lloncefn);\n \n@@ -443,7 +413,6 @@ fn trans_fn_pointer_shim<'a, 'tcx>(\n     bare_fn_ty: Ty<'tcx>)\n     -> ValueRef\n {\n-    let _icx = push_ctxt(\"trans_fn_pointer_shim\");\n     let tcx = ccx.tcx();\n \n     // Normalize the type for better caching.\n@@ -519,32 +488,39 @@ fn trans_fn_pointer_shim<'a, 'tcx>(\n     let llfn = declare::define_internal_fn(ccx, &function_name, tuple_fn_ty);\n     attributes::set_frame_pointer_elimination(ccx, llfn);\n     //\n-    let (block_arena, fcx): (TypedArena<_>, FunctionContext);\n-    block_arena = TypedArena::new();\n-    fcx = FunctionContext::new(ccx, llfn, fn_ty, None, &block_arena);\n-    let mut bcx = fcx.init(false);\n+    let fcx = FunctionContext::new(ccx, llfn);\n+    let bcx = fcx.get_entry_block();\n \n-    let llargs = get_params(fcx.llfn);\n+    let mut llargs = get_params(fcx.llfn);\n \n-    let self_idx = fcx.fn_ty.ret.is_indirect() as usize;\n+    let self_arg = llargs.remove(fn_ty.ret.is_indirect() as usize);\n     let llfnpointer = llfnpointer.unwrap_or_else(|| {\n         // the first argument (`self`) will be ptr to the fn pointer\n         if is_by_ref {\n-            Load(bcx, llargs[self_idx])\n+            bcx.load(self_arg)\n         } else {\n-            llargs[self_idx]\n+            self_arg\n         }\n     });\n \n-    let dest = fcx.llretslotptr.get();\n-\n     let callee = Callee {\n         data: Fn(llfnpointer),\n         ty: bare_fn_ty\n     };\n-    bcx = callee.call(bcx, DebugLoc::None, &llargs[(self_idx + 1)..], dest).bcx;\n+    let fn_ret = callee.ty.fn_ret();\n+    let fn_ty = callee.direct_fn_type(ccx, &[]);\n+    let llret = bcx.call(llfnpointer, &llargs, None);\n+    fn_ty.apply_attrs_callsite(llret);\n \n-    fcx.finish(bcx, DebugLoc::None);\n+    if fn_ret.0.is_never() {\n+        bcx.unreachable();\n+    } else {\n+        if fn_ty.ret.is_indirect() || fn_ty.ret.is_ignore() {\n+            bcx.ret_void();\n+        } else {\n+            bcx.ret(llret);\n+        }\n+    }\n \n     ccx.fn_pointer_shims().borrow_mut().insert(bare_fn_ty_maybe_ref, llfn);\n \n@@ -649,87 +625,3 @@ fn get_fn<'a, 'tcx>(ccx: &CrateContext<'a, 'tcx>,\n \n     (llfn, fn_ty)\n }\n-\n-// ______________________________________________________________________\n-// Translating calls\n-\n-fn trans_call_inner<'a, 'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n-                                    debug_loc: DebugLoc,\n-                                    callee: Callee<'tcx>,\n-                                    args: &[ValueRef],\n-                                    opt_llretslot: Option<ValueRef>)\n-                                    -> Result<'blk, 'tcx> {\n-    // Introduce a temporary cleanup scope that will contain cleanups\n-    // for the arguments while they are being evaluated. The purpose\n-    // this cleanup is to ensure that, should a panic occur while\n-    // evaluating argument N, the values for arguments 0...N-1 are all\n-    // cleaned up. If no panic occurs, the values are handed off to\n-    // the callee, and hence none of the cleanups in this temporary\n-    // scope will ever execute.\n-    let fcx = bcx.fcx;\n-    let ccx = fcx.ccx;\n-\n-    let fn_ret = callee.ty.fn_ret();\n-    let fn_ty = callee.direct_fn_type(ccx, &[]);\n-\n-    let mut callee = match callee.data {\n-        NamedTupleConstructor(_) | Intrinsic => {\n-            bug!(\"{:?} calls should not go through Callee::call\", callee);\n-        }\n-        f => f\n-    };\n-\n-    // If there no destination, return must be direct, with no cast.\n-    if opt_llretslot.is_none() {\n-        assert!(!fn_ty.ret.is_indirect() && fn_ty.ret.cast.is_none());\n-    }\n-\n-    let mut llargs = Vec::new();\n-\n-    if fn_ty.ret.is_indirect() {\n-        let mut llretslot = opt_llretslot.unwrap();\n-        if let Some(ty) = fn_ty.ret.cast {\n-            llretslot = PointerCast(bcx, llretslot, ty.ptr_to());\n-        }\n-        llargs.push(llretslot);\n-    }\n-\n-    match callee {\n-        Virtual(idx) => {\n-            llargs.push(args[0]);\n-\n-            let fn_ptr = meth::get_virtual_method(bcx, args[1], idx);\n-            let llty = fn_ty.llvm_type(bcx.ccx()).ptr_to();\n-            callee = Fn(PointerCast(bcx, fn_ptr, llty));\n-            llargs.extend_from_slice(&args[2..]);\n-        }\n-        _ => llargs.extend_from_slice(args)\n-    }\n-\n-    let llfn = match callee {\n-        Fn(f) => f,\n-        _ => bug!(\"expected fn pointer callee, found {:?}\", callee)\n-    };\n-\n-    let (llret, bcx) = base::invoke(bcx, llfn, &llargs, debug_loc);\n-    if !bcx.unreachable.get() {\n-        fn_ty.apply_attrs_callsite(llret);\n-\n-        // If the function we just called does not use an outpointer,\n-        // store the result into the rust outpointer. Cast the outpointer\n-        // type to match because some ABIs will use a different type than\n-        // the Rust type. e.g., a {u32,u32} struct could be returned as\n-        // u64.\n-        if !fn_ty.ret.is_indirect() {\n-            if let Some(llretslot) = opt_llretslot {\n-                fn_ty.ret.store(&bcx.build(), llret, llretslot);\n-            }\n-        }\n-    }\n-\n-    if fn_ret.0.is_never() {\n-        Unreachable(bcx);\n-    }\n-\n-    Result::new(bcx, llret)\n-}"}, {"sha": "add820748acfcd5325107a8292a73e5d9b3671d2", "filename": "src/librustc_trans/cleanup.rs", "status": "modified", "additions": 80, "deletions": 627, "changes": 707, "blob_url": "https://github.com/rust-lang/rust/blob/1b38776c1f68c6fd47c1b2f7b7974efc7dd64901/src%2Flibrustc_trans%2Fcleanup.rs", "raw_url": "https://github.com/rust-lang/rust/raw/1b38776c1f68c6fd47c1b2f7b7974efc7dd64901/src%2Flibrustc_trans%2Fcleanup.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_trans%2Fcleanup.rs?ref=1b38776c1f68c6fd47c1b2f7b7974efc7dd64901", "patch": "@@ -11,694 +11,147 @@\n //! ## The Cleanup module\n //!\n //! The cleanup module tracks what values need to be cleaned up as scopes\n-//! are exited, either via panic or just normal control flow. The basic\n-//! idea is that the function context maintains a stack of cleanup scopes\n-//! that are pushed/popped as we traverse the AST tree. There is typically\n-//! at least one cleanup scope per AST node; some AST nodes may introduce\n-//! additional temporary scopes.\n+//! are exited, either via panic or just normal control flow.\n //!\n //! Cleanup items can be scheduled into any of the scopes on the stack.\n-//! Typically, when a scope is popped, we will also generate the code for\n-//! each of its cleanups at that time. This corresponds to a normal exit\n-//! from a block (for example, an expression completing evaluation\n-//! successfully without panic). However, it is also possible to pop a\n-//! block *without* executing its cleanups; this is typically used to\n-//! guard intermediate values that must be cleaned up on panic, but not\n-//! if everything goes right. See the section on custom scopes below for\n-//! more details.\n-//!\n-//! Cleanup scopes come in three kinds:\n-//!\n-//! - **AST scopes:** each AST node in a function body has a corresponding\n-//!   AST scope. We push the AST scope when we start generate code for an AST\n-//!   node and pop it once the AST node has been fully generated.\n-//! - **Loop scopes:** loops have an additional cleanup scope. Cleanups are\n-//!   never scheduled into loop scopes; instead, they are used to record the\n-//!   basic blocks that we should branch to when a `continue` or `break` statement\n-//!   is encountered.\n-//! - **Custom scopes:** custom scopes are typically used to ensure cleanup\n-//!   of intermediate values.\n-//!\n-//! ### When to schedule cleanup\n-//!\n-//! Although the cleanup system is intended to *feel* fairly declarative,\n-//! it's still important to time calls to `schedule_clean()` correctly.\n-//! Basically, you should not schedule cleanup for memory until it has\n-//! been initialized, because if an unwind should occur before the memory\n-//! is fully initialized, then the cleanup will run and try to free or\n-//! drop uninitialized memory. If the initialization itself produces\n-//! byproducts that need to be freed, then you should use temporary custom\n-//! scopes to ensure that those byproducts will get freed on unwind.  For\n-//! example, an expression like `box foo()` will first allocate a box in the\n-//! heap and then call `foo()` -- if `foo()` should panic, this box needs\n-//! to be *shallowly* freed.\n-//!\n-//! ### Long-distance jumps\n-//!\n-//! In addition to popping a scope, which corresponds to normal control\n-//! flow exiting the scope, we may also *jump out* of a scope into some\n-//! earlier scope on the stack. This can occur in response to a `return`,\n-//! `break`, or `continue` statement, but also in response to panic. In\n-//! any of these cases, we will generate a series of cleanup blocks for\n-//! each of the scopes that is exited. So, if the stack contains scopes A\n-//! ... Z, and we break out of a loop whose corresponding cleanup scope is\n-//! X, we would generate cleanup blocks for the cleanups in X, Y, and Z.\n-//! After cleanup is done we would branch to the exit point for scope X.\n-//! But if panic should occur, we would generate cleanups for all the\n-//! scopes from A to Z and then resume the unwind process afterwards.\n-//!\n-//! To avoid generating tons of code, we cache the cleanup blocks that we\n-//! create for breaks, returns, unwinds, and other jumps. Whenever a new\n-//! cleanup is scheduled, though, we must clear these cached blocks. A\n-//! possible improvement would be to keep the cached blocks but simply\n-//! generate a new block which performs the additional cleanup and then\n-//! branches to the existing cached blocks.\n-//!\n-//! ### AST and loop cleanup scopes\n-//!\n-//! AST cleanup scopes are pushed when we begin and end processing an AST\n-//! node. They are used to house cleanups related to rvalue temporary that\n-//! get referenced (e.g., due to an expression like `&Foo()`). Whenever an\n-//! AST scope is popped, we always trans all the cleanups, adding the cleanup\n-//! code after the postdominator of the AST node.\n-//!\n-//! AST nodes that represent breakable loops also push a loop scope; the\n-//! loop scope never has any actual cleanups, it's just used to point to\n-//! the basic blocks where control should flow after a \"continue\" or\n-//! \"break\" statement. Popping a loop scope never generates code.\n-//!\n-//! ### Custom cleanup scopes\n-//!\n-//! Custom cleanup scopes are used for a variety of purposes. The most\n-//! common though is to handle temporary byproducts, where cleanup only\n-//! needs to occur on panic. The general strategy is to push a custom\n-//! cleanup scope, schedule *shallow* cleanups into the custom scope, and\n-//! then pop the custom scope (without transing the cleanups) when\n-//! execution succeeds normally. This way the cleanups are only trans'd on\n-//! unwind, and only up until the point where execution succeeded, at\n-//! which time the complete value should be stored in an lvalue or some\n-//! other place where normal cleanup applies.\n-//!\n-//! To spell it out, here is an example. Imagine an expression `box expr`.\n-//! We would basically:\n-//!\n-//! 1. Push a custom cleanup scope C.\n-//! 2. Allocate the box.\n-//! 3. Schedule a shallow free in the scope C.\n-//! 4. Trans `expr` into the box.\n-//! 5. Pop the scope C.\n-//! 6. Return the box as an rvalue.\n-//!\n-//! This way, if a panic occurs while transing `expr`, the custom\n-//! cleanup scope C is pushed and hence the box will be freed. The trans\n-//! code for `expr` itself is responsible for freeing any other byproducts\n-//! that may be in play.\n-\n-pub use self::EarlyExitLabel::*;\n+//! Typically, when a scope is finished, we generate the cleanup code. This\n+//! corresponds to a normal exit from a block (for example, an expression\n+//! completing evaluation successfully without panic).\n \n use llvm::{BasicBlockRef, ValueRef};\n use base;\n-use build;\n-use common;\n-use common::{Block, FunctionContext, LandingPad};\n-use debuginfo::{DebugLoc};\n+use common::{BlockAndBuilder, FunctionContext, Funclet};\n use glue;\n use type_::Type;\n use value::Value;\n use rustc::ty::Ty;\n \n pub struct CleanupScope<'tcx> {\n-    // Cleanups to run upon scope exit.\n-    cleanups: Vec<DropValue<'tcx>>,\n-\n-    // The debug location any drop calls generated for this scope will be\n-    // associated with.\n-    debug_loc: DebugLoc,\n+    // Cleanup to run upon scope exit.\n+    cleanup: Option<DropValue<'tcx>>,\n \n-    cached_early_exits: Vec<CachedEarlyExit>,\n-    cached_landing_pad: Option<BasicBlockRef>,\n+    // Computed on creation if compiling with landing pads (!sess.no_landing_pads)\n+    pub landing_pad: Option<BasicBlockRef>,\n }\n \n-#[derive(Copy, Clone, Debug)]\n-pub struct CustomScopeIndex {\n-    index: usize\n+#[derive(Copy, Clone)]\n+pub struct DropValue<'tcx> {\n+    val: ValueRef,\n+    ty: Ty<'tcx>,\n+    skip_dtor: bool,\n }\n \n-#[derive(Copy, Clone, PartialEq, Debug)]\n-pub enum EarlyExitLabel {\n-    UnwindExit(UnwindKind),\n-}\n+impl<'tcx> DropValue<'tcx> {\n+    fn trans<'a>(&self, funclet: Option<&'a Funclet>, bcx: &BlockAndBuilder<'a, 'tcx>) {\n+        glue::call_drop_glue(bcx, self.val, self.ty, self.skip_dtor, funclet)\n+    }\n \n-#[derive(Copy, Clone, Debug)]\n-pub enum UnwindKind {\n-    LandingPad,\n-    CleanupPad(ValueRef),\n-}\n+    /// Creates a landing pad for the top scope. The landing pad will perform all cleanups necessary\n+    /// for an unwind and then `resume` to continue error propagation:\n+    ///\n+    ///     landing_pad -> ... cleanups ... -> [resume]\n+    ///\n+    /// This should only be called once per function, as it creates an alloca for the landingpad.\n+    fn get_landing_pad<'a>(&self, fcx: &FunctionContext<'a, 'tcx>) -> BasicBlockRef {\n+        debug!(\"get_landing_pad\");\n+        let bcx = fcx.build_new_block(\"cleanup_unwind\");\n+        let llpersonality = bcx.ccx.eh_personality();\n+        bcx.set_personality_fn(llpersonality);\n \n-#[derive(Copy, Clone)]\n-pub struct CachedEarlyExit {\n-    label: EarlyExitLabel,\n-    cleanup_block: BasicBlockRef,\n-    last_cleanup: usize,\n-}\n+        if base::wants_msvc_seh(fcx.ccx.sess()) {\n+            let pad = bcx.cleanup_pad(None, &[]);\n+            let funclet = Some(Funclet::new(pad));\n+            self.trans(funclet.as_ref(), &bcx);\n \n-impl<'blk, 'tcx> FunctionContext<'blk, 'tcx> {\n-    pub fn push_custom_cleanup_scope(&self) -> CustomScopeIndex {\n-        let index = self.scopes_len();\n-        debug!(\"push_custom_cleanup_scope(): {}\", index);\n+            bcx.cleanup_ret(pad, None);\n+        } else {\n+            // The landing pad return type (the type being propagated). Not sure\n+            // what this represents but it's determined by the personality\n+            // function and this is what the EH proposal example uses.\n+            let llretty = Type::struct_(fcx.ccx, &[Type::i8p(fcx.ccx), Type::i32(fcx.ccx)], false);\n \n-        // Just copy the debuginfo source location from the enclosing scope\n-        let debug_loc = self.scopes\n-                            .borrow()\n-                            .last()\n-                            .map(|opt_scope| opt_scope.debug_loc)\n-                            .unwrap_or(DebugLoc::None);\n+            // The only landing pad clause will be 'cleanup'\n+            let llretval = bcx.landing_pad(llretty, llpersonality, 1, bcx.fcx().llfn);\n \n-        self.push_scope(CleanupScope::new(debug_loc));\n-        CustomScopeIndex { index: index }\n-    }\n+            // The landing pad block is a cleanup\n+            bcx.set_cleanup(llretval);\n \n-    /// Removes the top cleanup scope from the stack without executing its cleanups. The top\n-    /// cleanup scope must be the temporary scope `custom_scope`.\n-    pub fn pop_custom_cleanup_scope(&self,\n-                                    custom_scope: CustomScopeIndex) {\n-        debug!(\"pop_custom_cleanup_scope({})\", custom_scope.index);\n-        assert!(self.is_valid_to_pop_custom_scope(custom_scope));\n-        let _ = self.pop_scope();\n-    }\n+            // Insert cleanup instructions into the cleanup block\n+            self.trans(None, &bcx);\n \n-    /// Removes the top cleanup scope from the stack, which must be a temporary scope, and\n-    /// generates the code to do its cleanups for normal exit.\n-    pub fn pop_and_trans_custom_cleanup_scope(&self,\n-                                              bcx: Block<'blk, 'tcx>,\n-                                              custom_scope: CustomScopeIndex)\n-                                              -> Block<'blk, 'tcx> {\n-        debug!(\"pop_and_trans_custom_cleanup_scope({:?})\", custom_scope);\n-        assert!(self.is_valid_to_pop_custom_scope(custom_scope));\n+            if !bcx.sess().target.target.options.custom_unwind_resume {\n+                bcx.resume(llretval);\n+            } else {\n+                let exc_ptr = bcx.extract_value(llretval, 0);\n+                bcx.call(bcx.ccx.eh_unwind_resume(), &[exc_ptr], None);\n+                bcx.unreachable();\n+            }\n+        }\n \n-        let scope = self.pop_scope();\n-        self.trans_scope_cleanups(bcx, &scope)\n+        bcx.llbb()\n     }\n+}\n \n-    /// Schedules a (deep) drop of `val`, which is a pointer to an instance of\n-    /// `ty`\n-    pub fn schedule_drop_mem(&self,\n-                             cleanup_scope: CustomScopeIndex,\n-                             val: ValueRef,\n-                             ty: Ty<'tcx>) {\n-        if !self.type_needs_drop(ty) { return; }\n+impl<'a, 'tcx> FunctionContext<'a, 'tcx> {\n+    /// Schedules a (deep) drop of `val`, which is a pointer to an instance of `ty`\n+    pub fn schedule_drop_mem(&self, val: ValueRef, ty: Ty<'tcx>) -> CleanupScope<'tcx> {\n+        if !self.ccx.shared().type_needs_drop(ty) { return CleanupScope::noop(); }\n         let drop = DropValue {\n-            is_immediate: false,\n             val: val,\n             ty: ty,\n             skip_dtor: false,\n         };\n \n-        debug!(\"schedule_drop_mem({:?}, val={:?}, ty={:?}) skip_dtor={}\",\n-               cleanup_scope,\n-               Value(val),\n-               ty,\n-               drop.skip_dtor);\n+        debug!(\"schedule_drop_mem(val={:?}, ty={:?}) skip_dtor={}\", Value(val), ty, drop.skip_dtor);\n \n-        self.schedule_clean(cleanup_scope, drop);\n+        CleanupScope::new(self, drop)\n     }\n \n     /// Issue #23611: Schedules a (deep) drop of the contents of\n     /// `val`, which is a pointer to an instance of struct/enum type\n     /// `ty`. The scheduled code handles extracting the discriminant\n     /// and dropping the contents associated with that variant\n     /// *without* executing any associated drop implementation.\n-    pub fn schedule_drop_adt_contents(&self,\n-                                      cleanup_scope: CustomScopeIndex,\n-                                      val: ValueRef,\n-                                      ty: Ty<'tcx>) {\n+    pub fn schedule_drop_adt_contents(&self, val: ValueRef, ty: Ty<'tcx>) -> CleanupScope<'tcx> {\n         // `if` below could be \"!contents_needs_drop\"; skipping drop\n         // is just an optimization, so sound to be conservative.\n-        if !self.type_needs_drop(ty) { return; }\n+        if !self.ccx.shared().type_needs_drop(ty) { return CleanupScope::noop(); }\n \n         let drop = DropValue {\n-            is_immediate: false,\n             val: val,\n             ty: ty,\n             skip_dtor: true,\n         };\n \n-        debug!(\"schedule_drop_adt_contents({:?}, val={:?}, ty={:?}) skip_dtor={}\",\n-               cleanup_scope,\n-               Value(val),\n-               ty,\n-               drop.skip_dtor);\n+        debug!(\"schedule_drop_adt_contents(val={:?}, ty={:?}) skip_dtor={}\",\n+               Value(val), ty, drop.skip_dtor);\n \n-        self.schedule_clean(cleanup_scope, drop);\n-    }\n-\n-    /// Schedules a (deep) drop of `val`, which is an instance of `ty`\n-    pub fn schedule_drop_immediate(&self,\n-                                   cleanup_scope: CustomScopeIndex,\n-                                   val: ValueRef,\n-                                   ty: Ty<'tcx>) {\n-\n-        if !self.type_needs_drop(ty) { return; }\n-        let drop = DropValue {\n-            is_immediate: true,\n-            val: val,\n-            ty: ty,\n-            skip_dtor: false,\n-        };\n-\n-        debug!(\"schedule_drop_immediate({:?}, val={:?}, ty={:?}) skip_dtor={}\",\n-               cleanup_scope,\n-               Value(val),\n-               ty,\n-               drop.skip_dtor);\n-\n-        self.schedule_clean(cleanup_scope, drop);\n-    }\n-\n-    /// Schedules a cleanup to occur in the top-most scope, which must be a temporary scope.\n-    fn schedule_clean(&self, custom_scope: CustomScopeIndex, cleanup: DropValue<'tcx>) {\n-        debug!(\"schedule_clean_in_custom_scope(custom_scope={})\",\n-               custom_scope.index);\n-\n-        assert!(self.is_valid_custom_scope(custom_scope));\n-\n-        let mut scopes = self.scopes.borrow_mut();\n-        let scope = &mut (*scopes)[custom_scope.index];\n-        scope.cleanups.push(cleanup);\n-        scope.cached_landing_pad = None;\n-    }\n-\n-    /// Returns true if there are pending cleanups that should execute on panic.\n-    pub fn needs_invoke(&self) -> bool {\n-        self.scopes.borrow().iter().rev().any(|s| s.needs_invoke())\n-    }\n-\n-    /// Returns a basic block to branch to in the event of a panic. This block\n-    /// will run the panic cleanups and eventually resume the exception that\n-    /// caused the landing pad to be run.\n-    pub fn get_landing_pad(&'blk self) -> BasicBlockRef {\n-        let _icx = base::push_ctxt(\"get_landing_pad\");\n-\n-        debug!(\"get_landing_pad\");\n-\n-        let orig_scopes_len = self.scopes_len();\n-        assert!(orig_scopes_len > 0);\n-\n-        // Remove any scopes that do not have cleanups on panic:\n-        let mut popped_scopes = vec![];\n-        while !self.top_scope(|s| s.needs_invoke()) {\n-            debug!(\"top scope does not need invoke\");\n-            popped_scopes.push(self.pop_scope());\n-        }\n-\n-        // Check for an existing landing pad in the new topmost scope:\n-        let llbb = self.get_or_create_landing_pad();\n-\n-        // Push the scopes we removed back on:\n-        loop {\n-            match popped_scopes.pop() {\n-                Some(scope) => self.push_scope(scope),\n-                None => break\n-            }\n-        }\n-\n-        assert_eq!(self.scopes_len(), orig_scopes_len);\n-\n-        return llbb;\n-    }\n-\n-    fn is_valid_to_pop_custom_scope(&self, custom_scope: CustomScopeIndex) -> bool {\n-        self.is_valid_custom_scope(custom_scope) &&\n-            custom_scope.index == self.scopes.borrow().len() - 1\n-    }\n-\n-    fn is_valid_custom_scope(&self, custom_scope: CustomScopeIndex) -> bool {\n-        let scopes = self.scopes.borrow();\n-        custom_scope.index < scopes.len()\n-    }\n-\n-    /// Generates the cleanups for `scope` into `bcx`\n-    fn trans_scope_cleanups(&self, // cannot borrow self, will recurse\n-                            bcx: Block<'blk, 'tcx>,\n-                            scope: &CleanupScope<'tcx>) -> Block<'blk, 'tcx> {\n-\n-        let mut bcx = bcx;\n-        if !bcx.unreachable.get() {\n-            for cleanup in scope.cleanups.iter().rev() {\n-                bcx = cleanup.trans(bcx, scope.debug_loc);\n-            }\n-        }\n-        bcx\n-    }\n-\n-    fn scopes_len(&self) -> usize {\n-        self.scopes.borrow().len()\n-    }\n-\n-    fn push_scope(&self, scope: CleanupScope<'tcx>) {\n-        self.scopes.borrow_mut().push(scope)\n-    }\n-\n-    fn pop_scope(&self) -> CleanupScope<'tcx> {\n-        debug!(\"popping cleanup scope {}, {} scopes remaining\",\n-               self.top_scope(|s| s.block_name(\"\")),\n-               self.scopes_len() - 1);\n-\n-        self.scopes.borrow_mut().pop().unwrap()\n-    }\n-\n-    fn top_scope<R, F>(&self, f: F) -> R where F: FnOnce(&CleanupScope<'tcx>) -> R {\n-        f(self.scopes.borrow().last().unwrap())\n-    }\n-\n-    /// Used when the caller wishes to jump to an early exit, such as a return,\n-    /// break, continue, or unwind. This function will generate all cleanups\n-    /// between the top of the stack and the exit `label` and return a basic\n-    /// block that the caller can branch to.\n-    ///\n-    /// For example, if the current stack of cleanups were as follows:\n-    ///\n-    ///      AST 22\n-    ///      Custom 1\n-    ///      AST 23\n-    ///      Loop 23\n-    ///      Custom 2\n-    ///      AST 24\n-    ///\n-    /// and the `label` specifies a break from `Loop 23`, then this function\n-    /// would generate a series of basic blocks as follows:\n-    ///\n-    ///      Cleanup(AST 24) -> Cleanup(Custom 2) -> break_blk\n-    ///\n-    /// where `break_blk` is the block specified in `Loop 23` as the target for\n-    /// breaks. The return value would be the first basic block in that sequence\n-    /// (`Cleanup(AST 24)`). The caller could then branch to `Cleanup(AST 24)`\n-    /// and it will perform all cleanups and finally branch to the `break_blk`.\n-    fn trans_cleanups_to_exit_scope(&'blk self,\n-                                    label: EarlyExitLabel)\n-                                    -> BasicBlockRef {\n-        debug!(\"trans_cleanups_to_exit_scope label={:?} scopes={}\",\n-               label, self.scopes_len());\n-\n-        let orig_scopes_len = self.scopes_len();\n-        let mut prev_llbb;\n-        let mut popped_scopes = vec![];\n-        let mut skip = 0;\n-\n-        // First we pop off all the cleanup stacks that are\n-        // traversed until the exit is reached, pushing them\n-        // onto the side vector `popped_scopes`. No code is\n-        // generated at this time.\n-        //\n-        // So, continuing the example from above, we would wind up\n-        // with a `popped_scopes` vector of `[AST 24, Custom 2]`.\n-        // (Presuming that there are no cached exits)\n-        loop {\n-            if self.scopes_len() == 0 {\n-                match label {\n-                    UnwindExit(val) => {\n-                        // Generate a block that will resume unwinding to the\n-                        // calling function\n-                        let bcx = self.new_block(\"resume\");\n-                        match val {\n-                            UnwindKind::LandingPad => {\n-                                let addr = self.landingpad_alloca.get()\n-                                               .unwrap();\n-                                let lp = build::Load(bcx, addr);\n-                                base::call_lifetime_end(bcx, addr);\n-                                base::trans_unwind_resume(bcx, lp);\n-                            }\n-                            UnwindKind::CleanupPad(_) => {\n-                                let pad = build::CleanupPad(bcx, None, &[]);\n-                                build::CleanupRet(bcx, pad, None);\n-                            }\n-                        }\n-                        prev_llbb = bcx.llbb;\n-                        break;\n-                    }\n-                }\n-            }\n-\n-            // Pop off the scope, since we may be generating\n-            // unwinding code for it.\n-            let top_scope = self.pop_scope();\n-            let cached_exit = top_scope.cached_early_exit(label);\n-            popped_scopes.push(top_scope);\n-\n-            // Check if we have already cached the unwinding of this\n-            // scope for this label. If so, we can stop popping scopes\n-            // and branch to the cached label, since it contains the\n-            // cleanups for any subsequent scopes.\n-            if let Some((exit, last_cleanup)) = cached_exit {\n-                prev_llbb = exit;\n-                skip = last_cleanup;\n-                break;\n-            }\n-        }\n-\n-        debug!(\"trans_cleanups_to_exit_scope: popped {} scopes\",\n-               popped_scopes.len());\n-\n-        // Now push the popped scopes back on. As we go,\n-        // we track in `prev_llbb` the exit to which this scope\n-        // should branch when it's done.\n-        //\n-        // So, continuing with our example, we will start out with\n-        // `prev_llbb` being set to `break_blk` (or possibly a cached\n-        // early exit). We will then pop the scopes from `popped_scopes`\n-        // and generate a basic block for each one, prepending it in the\n-        // series and updating `prev_llbb`. So we begin by popping `Custom 2`\n-        // and generating `Cleanup(Custom 2)`. We make `Cleanup(Custom 2)`\n-        // branch to `prev_llbb == break_blk`, giving us a sequence like:\n-        //\n-        //     Cleanup(Custom 2) -> prev_llbb\n-        //\n-        // We then pop `AST 24` and repeat the process, giving us the sequence:\n-        //\n-        //     Cleanup(AST 24) -> Cleanup(Custom 2) -> prev_llbb\n-        //\n-        // At this point, `popped_scopes` is empty, and so the final block\n-        // that we return to the user is `Cleanup(AST 24)`.\n-        while let Some(mut scope) = popped_scopes.pop() {\n-            if !scope.cleanups.is_empty() {\n-                let name = scope.block_name(\"clean\");\n-                debug!(\"generating cleanups for {}\", name);\n-\n-                let bcx_in = self.new_block(&name[..]);\n-                let exit_label = label.start(bcx_in);\n-                let mut bcx_out = bcx_in;\n-                let len = scope.cleanups.len();\n-                for cleanup in scope.cleanups.iter().rev().take(len - skip) {\n-                    bcx_out = cleanup.trans(bcx_out, scope.debug_loc);\n-                }\n-                skip = 0;\n-                exit_label.branch(bcx_out, prev_llbb);\n-                prev_llbb = bcx_in.llbb;\n-\n-                scope.add_cached_early_exit(exit_label, prev_llbb, len);\n-            }\n-            self.push_scope(scope);\n-        }\n-\n-        debug!(\"trans_cleanups_to_exit_scope: prev_llbb={:?}\", prev_llbb);\n-\n-        assert_eq!(self.scopes_len(), orig_scopes_len);\n-        prev_llbb\n-    }\n-\n-    /// Creates a landing pad for the top scope, if one does not exist.  The\n-    /// landing pad will perform all cleanups necessary for an unwind and then\n-    /// `resume` to continue error propagation:\n-    ///\n-    ///     landing_pad -> ... cleanups ... -> [resume]\n-    ///\n-    /// (The cleanups and resume instruction are created by\n-    /// `trans_cleanups_to_exit_scope()`, not in this function itself.)\n-    fn get_or_create_landing_pad(&'blk self) -> BasicBlockRef {\n-        let pad_bcx;\n-\n-        debug!(\"get_or_create_landing_pad\");\n-\n-        // Check if a landing pad block exists; if not, create one.\n-        {\n-            let mut scopes = self.scopes.borrow_mut();\n-            let last_scope = scopes.last_mut().unwrap();\n-            match last_scope.cached_landing_pad {\n-                Some(llbb) => return llbb,\n-                None => {\n-                    let name = last_scope.block_name(\"unwind\");\n-                    pad_bcx = self.new_block(&name[..]);\n-                    last_scope.cached_landing_pad = Some(pad_bcx.llbb);\n-                }\n-            }\n-        };\n-\n-        let llpersonality = pad_bcx.fcx.eh_personality();\n-\n-        let val = if base::wants_msvc_seh(self.ccx.sess()) {\n-            // A cleanup pad requires a personality function to be specified, so\n-            // we do that here explicitly (happens implicitly below through\n-            // creation of the landingpad instruction). We then create a\n-            // cleanuppad instruction which has no filters to run cleanup on all\n-            // exceptions.\n-            build::SetPersonalityFn(pad_bcx, llpersonality);\n-            let llretval = build::CleanupPad(pad_bcx, None, &[]);\n-            UnwindKind::CleanupPad(llretval)\n-        } else {\n-            // The landing pad return type (the type being propagated). Not sure\n-            // what this represents but it's determined by the personality\n-            // function and this is what the EH proposal example uses.\n-            let llretty = Type::struct_(self.ccx,\n-                                        &[Type::i8p(self.ccx), Type::i32(self.ccx)],\n-                                        false);\n-\n-            // The only landing pad clause will be 'cleanup'\n-            let llretval = build::LandingPad(pad_bcx, llretty, llpersonality, 1);\n-\n-            // The landing pad block is a cleanup\n-            build::SetCleanup(pad_bcx, llretval);\n-\n-            let addr = match self.landingpad_alloca.get() {\n-                Some(addr) => addr,\n-                None => {\n-                    let addr = base::alloca(pad_bcx, common::val_ty(llretval),\n-                                            \"\");\n-                    base::call_lifetime_start(pad_bcx, addr);\n-                    self.landingpad_alloca.set(Some(addr));\n-                    addr\n-                }\n-            };\n-            build::Store(pad_bcx, llretval, addr);\n-            UnwindKind::LandingPad\n-        };\n-\n-        // Generate the cleanup block and branch to it.\n-        let label = UnwindExit(val);\n-        let cleanup_llbb = self.trans_cleanups_to_exit_scope(label);\n-        label.branch(pad_bcx, cleanup_llbb);\n-\n-        return pad_bcx.llbb;\n+        CleanupScope::new(self, drop)\n     }\n }\n \n impl<'tcx> CleanupScope<'tcx> {\n-    fn new(debug_loc: DebugLoc) -> CleanupScope<'tcx> {\n+    fn new<'a>(fcx: &FunctionContext<'a, 'tcx>, drop_val: DropValue<'tcx>) -> CleanupScope<'tcx> {\n         CleanupScope {\n-            debug_loc: debug_loc,\n-            cleanups: vec![],\n-            cached_early_exits: vec![],\n-            cached_landing_pad: None,\n+            cleanup: Some(drop_val),\n+            landing_pad: if !fcx.ccx.sess().no_landing_pads() {\n+                Some(drop_val.get_landing_pad(fcx))\n+            } else {\n+                None\n+            },\n         }\n     }\n \n-    fn cached_early_exit(&self,\n-                         label: EarlyExitLabel)\n-                         -> Option<(BasicBlockRef, usize)> {\n-        self.cached_early_exits.iter().rev().\n-            find(|e| e.label == label).\n-            map(|e| (e.cleanup_block, e.last_cleanup))\n-    }\n-\n-    fn add_cached_early_exit(&mut self,\n-                             label: EarlyExitLabel,\n-                             blk: BasicBlockRef,\n-                             last_cleanup: usize) {\n-        self.cached_early_exits.push(\n-            CachedEarlyExit { label: label,\n-                              cleanup_block: blk,\n-                              last_cleanup: last_cleanup});\n-    }\n-\n-    /// True if this scope has cleanups that need unwinding\n-    fn needs_invoke(&self) -> bool {\n-        self.cached_landing_pad.is_some() ||\n-            !self.cleanups.is_empty()\n-    }\n-\n-    /// Returns a suitable name to use for the basic block that handles this cleanup scope\n-    fn block_name(&self, prefix: &str) -> String {\n-        format!(\"{}_custom_\", prefix)\n-    }\n-}\n-\n-impl EarlyExitLabel {\n-    /// Generates a branch going from `from_bcx` to `to_llbb` where `self` is\n-    /// the exit label attached to the start of `from_bcx`.\n-    ///\n-    /// Transitions from an exit label to other exit labels depend on the type\n-    /// of label. For example with MSVC exceptions unwind exit labels will use\n-    /// the `cleanupret` instruction instead of the `br` instruction.\n-    fn branch(&self, from_bcx: Block, to_llbb: BasicBlockRef) {\n-        if let UnwindExit(UnwindKind::CleanupPad(pad)) = *self {\n-            build::CleanupRet(from_bcx, pad, Some(to_llbb));\n-        } else {\n-            build::Br(from_bcx, to_llbb, DebugLoc::None);\n-        }\n-    }\n-\n-    /// Generates the necessary instructions at the start of `bcx` to prepare\n-    /// for the same kind of early exit label that `self` is.\n-    ///\n-    /// This function will appropriately configure `bcx` based on the kind of\n-    /// label this is. For UnwindExit labels, the `lpad` field of the block will\n-    /// be set to `Some`, and for MSVC exceptions this function will generate a\n-    /// `cleanuppad` instruction at the start of the block so it may be jumped\n-    /// to in the future (e.g. so this block can be cached as an early exit).\n-    ///\n-    /// Returns a new label which will can be used to cache `bcx` in the list of\n-    /// early exits.\n-    fn start(&self, bcx: Block) -> EarlyExitLabel {\n-        match *self {\n-            UnwindExit(UnwindKind::CleanupPad(..)) => {\n-                let pad = build::CleanupPad(bcx, None, &[]);\n-                bcx.lpad.set(Some(bcx.fcx.lpad_arena.alloc(LandingPad::msvc(pad))));\n-                UnwindExit(UnwindKind::CleanupPad(pad))\n-            }\n-            UnwindExit(UnwindKind::LandingPad) => {\n-                bcx.lpad.set(Some(bcx.fcx.lpad_arena.alloc(LandingPad::gnu())));\n-                *self\n-            }\n+    pub fn noop() -> CleanupScope<'tcx> {\n+        CleanupScope {\n+            cleanup: None,\n+            landing_pad: None,\n         }\n     }\n-}\n \n-impl PartialEq for UnwindKind {\n-    fn eq(&self, val: &UnwindKind) -> bool {\n-        match (*self, *val) {\n-            (UnwindKind::LandingPad, UnwindKind::LandingPad) |\n-            (UnwindKind::CleanupPad(..), UnwindKind::CleanupPad(..)) => true,\n-            _ => false,\n+    pub fn trans<'a>(self, bcx: &'a BlockAndBuilder<'a, 'tcx>) {\n+        if let Some(cleanup) = self.cleanup {\n+            cleanup.trans(None, &bcx);\n         }\n     }\n }\n-\n-///////////////////////////////////////////////////////////////////////////\n-// Cleanup types\n-\n-#[derive(Copy, Clone)]\n-pub struct DropValue<'tcx> {\n-    is_immediate: bool,\n-    val: ValueRef,\n-    ty: Ty<'tcx>,\n-    skip_dtor: bool,\n-}\n-\n-impl<'tcx> DropValue<'tcx> {\n-    fn trans<'blk>(&self,\n-                   bcx: Block<'blk, 'tcx>,\n-                   debug_loc: DebugLoc)\n-                   -> Block<'blk, 'tcx> {\n-        let skip_dtor = self.skip_dtor;\n-        let _icx = if skip_dtor {\n-            base::push_ctxt(\"<DropValue as Cleanup>::trans skip_dtor=true\")\n-        } else {\n-            base::push_ctxt(\"<DropValue as Cleanup>::trans skip_dtor=false\")\n-        };\n-        let bcx = if self.is_immediate {\n-            glue::drop_ty_immediate(bcx, self.val, self.ty, debug_loc, self.skip_dtor)\n-        } else {\n-            glue::drop_ty_core(bcx, self.val, self.ty, debug_loc, self.skip_dtor)\n-        };\n-        bcx\n-    }\n-}"}, {"sha": "d8c212745376d69225bd2ff424dd1c15f4731f62", "filename": "src/librustc_trans/collector.rs", "status": "modified", "additions": 15, "deletions": 15, "changes": 30, "blob_url": "https://github.com/rust-lang/rust/blob/1b38776c1f68c6fd47c1b2f7b7974efc7dd64901/src%2Flibrustc_trans%2Fcollector.rs", "raw_url": "https://github.com/rust-lang/rust/raw/1b38776c1f68c6fd47c1b2f7b7974efc7dd64901/src%2Flibrustc_trans%2Fcollector.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_trans%2Fcollector.rs?ref=1b38776c1f68c6fd47c1b2f7b7974efc7dd64901", "patch": "@@ -208,7 +208,7 @@ use syntax::abi::Abi;\n use syntax_pos::DUMMY_SP;\n use base::custom_coerce_unsize_info;\n use context::SharedCrateContext;\n-use common::{fulfill_obligation, type_is_sized};\n+use common::fulfill_obligation;\n use glue::{self, DropGlueKind};\n use monomorphize::{self, Instance};\n use util::nodemap::{FxHashSet, FxHashMap, DefIdMap};\n@@ -337,7 +337,7 @@ fn collect_items_rec<'a, 'tcx: 'a>(scx: &SharedCrateContext<'a, 'tcx>,\n         TransItem::Static(node_id) => {\n             let def_id = scx.tcx().map.local_def_id(node_id);\n             let ty = scx.tcx().item_type(def_id);\n-            let ty = glue::get_drop_glue_type(scx.tcx(), ty);\n+            let ty = glue::get_drop_glue_type(scx, ty);\n             neighbors.push(TransItem::DropGlue(DropGlueKind::Ty(ty)));\n \n             recursion_depth_reset = None;\n@@ -542,7 +542,7 @@ impl<'a, 'tcx> MirVisitor<'tcx> for MirNeighborCollector<'a, 'tcx> {\n                                                       self.param_substs,\n                                                       &ty);\n             assert!(ty.is_normalized_for_trans());\n-            let ty = glue::get_drop_glue_type(self.scx.tcx(), ty);\n+            let ty = glue::get_drop_glue_type(self.scx, ty);\n             self.output.push(TransItem::DropGlue(DropGlueKind::Ty(ty)));\n         }\n \n@@ -678,7 +678,7 @@ impl<'a, 'tcx> MirVisitor<'tcx> for MirNeighborCollector<'a, 'tcx> {\n                             let operand_ty = monomorphize::apply_param_substs(self.scx,\n                                                                               self.param_substs,\n                                                                               &mt.ty);\n-                            let ty = glue::get_drop_glue_type(tcx, operand_ty);\n+                            let ty = glue::get_drop_glue_type(self.scx, operand_ty);\n                             self.output.push(TransItem::DropGlue(DropGlueKind::Ty(ty)));\n                         } else {\n                             bug!(\"Has the drop_in_place() intrinsic's signature changed?\")\n@@ -804,33 +804,33 @@ fn find_drop_glue_neighbors<'a, 'tcx>(scx: &SharedCrateContext<'a, 'tcx>,\n                 let field_type = monomorphize::apply_param_substs(scx,\n                                                                   substs,\n                                                                   &field_type);\n-                let field_type = glue::get_drop_glue_type(scx.tcx(), field_type);\n+                let field_type = glue::get_drop_glue_type(scx, field_type);\n \n-                if glue::type_needs_drop(scx.tcx(), field_type) {\n+                if scx.type_needs_drop(field_type) {\n                     output.push(TransItem::DropGlue(DropGlueKind::Ty(field_type)));\n                 }\n             }\n         }\n         ty::TyClosure(def_id, substs) => {\n             for upvar_ty in substs.upvar_tys(def_id, scx.tcx()) {\n-                let upvar_ty = glue::get_drop_glue_type(scx.tcx(), upvar_ty);\n-                if glue::type_needs_drop(scx.tcx(), upvar_ty) {\n+                let upvar_ty = glue::get_drop_glue_type(scx, upvar_ty);\n+                if scx.type_needs_drop(upvar_ty) {\n                     output.push(TransItem::DropGlue(DropGlueKind::Ty(upvar_ty)));\n                 }\n             }\n         }\n         ty::TyBox(inner_type)      |\n         ty::TySlice(inner_type)    |\n         ty::TyArray(inner_type, _) => {\n-            let inner_type = glue::get_drop_glue_type(scx.tcx(), inner_type);\n-            if glue::type_needs_drop(scx.tcx(), inner_type) {\n+            let inner_type = glue::get_drop_glue_type(scx, inner_type);\n+            if scx.type_needs_drop(inner_type) {\n                 output.push(TransItem::DropGlue(DropGlueKind::Ty(inner_type)));\n             }\n         }\n         ty::TyTuple(args) => {\n             for arg in args {\n-                let arg = glue::get_drop_glue_type(scx.tcx(), arg);\n-                if glue::type_needs_drop(scx.tcx(), arg) {\n+                let arg = glue::get_drop_glue_type(scx, arg);\n+                if scx.type_needs_drop(arg) {\n                     output.push(TransItem::DropGlue(DropGlueKind::Ty(arg)));\n                 }\n             }\n@@ -969,7 +969,7 @@ fn find_vtable_types_for_unsizing<'a, 'tcx>(scx: &SharedCrateContext<'a, 'tcx>,\n          &ty::TyRawPtr(ty::TypeAndMut { ty: b, .. })) => {\n             let (inner_source, inner_target) = (a, b);\n \n-            if !type_is_sized(scx.tcx(), inner_source) {\n+            if !scx.type_is_sized(inner_source) {\n                 (inner_source, inner_target)\n             } else {\n                 scx.tcx().struct_lockstep_tails(inner_source, inner_target)\n@@ -1051,7 +1051,7 @@ fn create_trans_items_for_vtable_methods<'a, 'tcx>(scx: &SharedCrateContext<'a,\n             output.extend(methods);\n         }\n         // Also add the destructor\n-        let dg_type = glue::get_drop_glue_type(scx.tcx(), impl_ty);\n+        let dg_type = glue::get_drop_glue_type(scx, impl_ty);\n         output.push(TransItem::DropGlue(DropGlueKind::Ty(dg_type)));\n     }\n }\n@@ -1097,7 +1097,7 @@ impl<'b, 'a, 'v> ItemLikeVisitor<'v> for RootCollector<'b, 'a, 'v> {\n                                def_id_to_string(self.scx.tcx(), def_id));\n \n                         let ty = self.scx.tcx().item_type(def_id);\n-                        let ty = glue::get_drop_glue_type(self.scx.tcx(), ty);\n+                        let ty = glue::get_drop_glue_type(self.scx, ty);\n                         self.output.push(TransItem::DropGlue(DropGlueKind::Ty(ty)));\n                     }\n                 }"}, {"sha": "71e17f1ea74051376c83ab4df19be3b06b1e9fac", "filename": "src/librustc_trans/common.rs", "status": "modified", "additions": 113, "deletions": 428, "changes": 541, "blob_url": "https://github.com/rust-lang/rust/blob/1b38776c1f68c6fd47c1b2f7b7974efc7dd64901/src%2Flibrustc_trans%2Fcommon.rs", "raw_url": "https://github.com/rust-lang/rust/raw/1b38776c1f68c6fd47c1b2f7b7974efc7dd64901/src%2Flibrustc_trans%2Fcommon.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_trans%2Fcommon.rs?ref=1b38776c1f68c6fd47c1b2f7b7974efc7dd64901", "patch": "@@ -14,24 +14,16 @@\n \n use session::Session;\n use llvm;\n-use llvm::{ValueRef, BasicBlockRef, BuilderRef, ContextRef, TypeKind};\n+use llvm::{ValueRef, BasicBlockRef, ContextRef, TypeKind};\n use llvm::{True, False, Bool, OperandBundleDef};\n use rustc::hir::def::Def;\n use rustc::hir::def_id::DefId;\n use rustc::hir::map::DefPathData;\n-use rustc::infer::TransNormalize;\n-use rustc::mir::Mir;\n use rustc::util::common::MemoizationMap;\n use middle::lang_items::LangItem;\n-use rustc::ty::subst::Substs;\n-use abi::{Abi, FnType};\n use base;\n-use build;\n use builder::Builder;\n-use callee::Callee;\n-use cleanup;\n use consts;\n-use debuginfo::{self, DebugLoc};\n use declare;\n use machine;\n use monomorphize;\n@@ -40,34 +32,26 @@ use value::Value;\n use rustc::ty::{self, Ty, TyCtxt};\n use rustc::ty::layout::Layout;\n use rustc::traits::{self, SelectionContext, Reveal};\n-use rustc::ty::fold::TypeFoldable;\n use rustc::hir;\n \n-use arena::TypedArena;\n use libc::{c_uint, c_char};\n use std::borrow::Cow;\n use std::iter;\n use std::ops::Deref;\n use std::ffi::CString;\n-use std::cell::{Cell, RefCell, Ref};\n \n use syntax::ast;\n use syntax::symbol::{Symbol, InternedString};\n-use syntax_pos::{DUMMY_SP, Span};\n+use syntax_pos::Span;\n \n pub use context::{CrateContext, SharedCrateContext};\n \n-/// Is the type's representation size known at compile time?\n-pub fn type_is_sized<'a, 'tcx>(tcx: TyCtxt<'a, 'tcx, 'tcx>, ty: Ty<'tcx>) -> bool {\n-    ty.is_sized(tcx, &tcx.empty_parameter_environment(), DUMMY_SP)\n-}\n-\n-pub fn type_is_fat_ptr<'a, 'tcx>(tcx: TyCtxt<'a, 'tcx, 'tcx>, ty: Ty<'tcx>) -> bool {\n+pub fn type_is_fat_ptr<'a, 'tcx>(ccx: &CrateContext<'a, 'tcx>, ty: Ty<'tcx>) -> bool {\n     match ty.sty {\n         ty::TyRawPtr(ty::TypeAndMut{ty, ..}) |\n         ty::TyRef(_, ty::TypeAndMut{ty, ..}) |\n         ty::TyBox(ty) => {\n-            !type_is_sized(tcx, ty)\n+            !ccx.shared().type_is_sized(ty)\n         }\n         _ => {\n             false\n@@ -79,14 +63,13 @@ pub fn type_is_immediate<'a, 'tcx>(ccx: &CrateContext<'a, 'tcx>, ty: Ty<'tcx>) -\n     use machine::llsize_of_alloc;\n     use type_of::sizing_type_of;\n \n-    let tcx = ccx.tcx();\n     let simple = ty.is_scalar() ||\n         ty.is_unique() || ty.is_region_ptr() ||\n         ty.is_simd();\n-    if simple && !type_is_fat_ptr(tcx, ty) {\n+    if simple && !type_is_fat_ptr(ccx, ty) {\n         return true;\n     }\n-    if !type_is_sized(tcx, ty) {\n+    if !ccx.shared().type_is_sized(ty) {\n         return false;\n     }\n     match ty.sty {\n@@ -236,416 +219,139 @@ impl<'a, 'tcx> VariantInfo<'tcx> {\n     }\n }\n \n-pub struct BuilderRef_res {\n-    pub b: BuilderRef,\n-}\n-\n-impl Drop for BuilderRef_res {\n-    fn drop(&mut self) {\n-        unsafe {\n-            llvm::LLVMDisposeBuilder(self.b);\n-        }\n-    }\n-}\n-\n-pub fn BuilderRef_res(b: BuilderRef) -> BuilderRef_res {\n-    BuilderRef_res {\n-        b: b\n-    }\n-}\n-\n-pub fn validate_substs(substs: &Substs) {\n-    assert!(!substs.needs_infer());\n-}\n-\n-// Function context.  Every LLVM function we create will have one of\n-// these.\n+// Function context. Every LLVM function we create will have one of these.\n pub struct FunctionContext<'a, 'tcx: 'a> {\n-    // The MIR for this function.\n-    pub mir: Option<Ref<'tcx, Mir<'tcx>>>,\n-\n     // The ValueRef returned from a call to llvm::LLVMAddFunction; the\n     // address of the first instruction in the sequence of\n     // instructions for this function that will go in the .text\n     // section of the executable we're generating.\n     pub llfn: ValueRef,\n \n-    // always an empty parameter-environment NOTE: @jroesch another use of ParamEnv\n-    pub param_env: ty::ParameterEnvironment<'tcx>,\n-\n-    // A pointer to where to store the return value. If the return type is\n-    // immediate, this points to an alloca in the function. Otherwise, it's a\n-    // pointer to the hidden first parameter of the function. After function\n-    // construction, this should always be Some.\n-    pub llretslotptr: Cell<Option<ValueRef>>,\n-\n-    // These pub elements: \"hoisted basic blocks\" containing\n-    // administrative activities that have to happen in only one place in\n-    // the function, due to LLVM's quirks.\n     // A marker for the place where we want to insert the function's static\n     // allocas, so that LLVM will coalesce them into a single alloca call.\n-    pub alloca_insert_pt: Cell<Option<ValueRef>>,\n-\n-    // When working with landingpad-based exceptions this value is alloca'd and\n-    // later loaded when using the resume instruction. This ends up being\n-    // critical to chaining landing pads and resuing already-translated\n-    // cleanups.\n-    //\n-    // Note that for cleanuppad-based exceptions this is not used.\n-    pub landingpad_alloca: Cell<Option<ValueRef>>,\n-\n-    // Describes the return/argument LLVM types and their ABI handling.\n-    pub fn_ty: FnType,\n-\n-    // If this function is being monomorphized, this contains the type\n-    // substitutions used.\n-    pub param_substs: &'tcx Substs<'tcx>,\n-\n-    // The source span and nesting context where this function comes from, for\n-    // error reporting and symbol generation.\n-    pub span: Option<Span>,\n-\n-    // The arena that blocks are allocated from.\n-    pub block_arena: &'a TypedArena<BlockS<'a, 'tcx>>,\n-\n-    // The arena that landing pads are allocated from.\n-    pub lpad_arena: TypedArena<LandingPad>,\n+    alloca_insert_pt: Option<ValueRef>,\n \n     // This function's enclosing crate context.\n     pub ccx: &'a CrateContext<'a, 'tcx>,\n \n-    // Used and maintained by the debuginfo module.\n-    pub debug_context: debuginfo::FunctionDebugContext,\n-\n-    // Cleanup scopes.\n-    pub scopes: RefCell<Vec<cleanup::CleanupScope<'tcx>>>,\n+    alloca_builder: Builder<'a, 'tcx>,\n }\n \n impl<'a, 'tcx> FunctionContext<'a, 'tcx> {\n-    pub fn mir(&self) -> Ref<'tcx, Mir<'tcx>> {\n-        self.mir.as_ref().map(Ref::clone).expect(\"fcx.mir was empty\")\n+    /// Create a function context for the given function.\n+    /// Call FunctionContext::get_entry_block for the first entry block.\n+    pub fn new(ccx: &'a CrateContext<'a, 'tcx>, llfndecl: ValueRef) -> FunctionContext<'a, 'tcx> {\n+        let mut fcx = FunctionContext {\n+            llfn: llfndecl,\n+            alloca_insert_pt: None,\n+            ccx: ccx,\n+            alloca_builder: Builder::with_ccx(ccx),\n+        };\n+\n+        let val = {\n+            let entry_bcx = fcx.build_new_block(\"entry-block\");\n+            let val = entry_bcx.load(C_null(Type::i8p(ccx)));\n+            fcx.alloca_builder.position_at_start(entry_bcx.llbb());\n+            val\n+        };\n+\n+        // Use a dummy instruction as the insertion point for all allocas.\n+        // This is later removed in the drop of FunctionContext.\n+        fcx.alloca_insert_pt = Some(val);\n+\n+        fcx\n     }\n \n-    pub fn cleanup(&self) {\n-        unsafe {\n-            llvm::LLVMInstructionEraseFromParent(self.alloca_insert_pt\n-                                                     .get()\n-                                                     .unwrap());\n-        }\n+    pub fn get_entry_block(&'a self) -> BlockAndBuilder<'a, 'tcx> {\n+        BlockAndBuilder::new(unsafe {\n+            llvm::LLVMGetFirstBasicBlock(self.llfn)\n+        }, self)\n     }\n \n-    pub fn new_block(&'a self,\n-                     name: &str)\n-                     -> Block<'a, 'tcx> {\n+    pub fn new_block(&'a self, name: &str) -> BasicBlockRef {\n         unsafe {\n             let name = CString::new(name).unwrap();\n-            let llbb = llvm::LLVMAppendBasicBlockInContext(self.ccx.llcx(),\n-                                                           self.llfn,\n-                                                           name.as_ptr());\n-            BlockS::new(llbb, self)\n+            llvm::LLVMAppendBasicBlockInContext(\n+                self.ccx.llcx(),\n+                self.llfn,\n+                name.as_ptr()\n+            )\n         }\n     }\n \n-    pub fn monomorphize<T>(&self, value: &T) -> T\n-        where T: TransNormalize<'tcx>\n-    {\n-        monomorphize::apply_param_substs(self.ccx.shared(),\n-                                         self.param_substs,\n-                                         value)\n-    }\n-\n-    /// This is the same as `common::type_needs_drop`, except that it\n-    /// may use or update caches within this `FunctionContext`.\n-    pub fn type_needs_drop(&self, ty: Ty<'tcx>) -> bool {\n-        self.ccx.tcx().type_needs_drop_given_env(ty, &self.param_env)\n+    pub fn build_new_block(&'a self, name: &str) -> BlockAndBuilder<'a, 'tcx> {\n+        BlockAndBuilder::new(self.new_block(name), self)\n     }\n \n-    pub fn eh_personality(&self) -> ValueRef {\n-        // The exception handling personality function.\n-        //\n-        // If our compilation unit has the `eh_personality` lang item somewhere\n-        // within it, then we just need to translate that. Otherwise, we're\n-        // building an rlib which will depend on some upstream implementation of\n-        // this function, so we just codegen a generic reference to it. We don't\n-        // specify any of the types for the function, we just make it a symbol\n-        // that LLVM can later use.\n-        //\n-        // Note that MSVC is a little special here in that we don't use the\n-        // `eh_personality` lang item at all. Currently LLVM has support for\n-        // both Dwarf and SEH unwind mechanisms for MSVC targets and uses the\n-        // *name of the personality function* to decide what kind of unwind side\n-        // tables/landing pads to emit. It looks like Dwarf is used by default,\n-        // injecting a dependency on the `_Unwind_Resume` symbol for resuming\n-        // an \"exception\", but for MSVC we want to force SEH. This means that we\n-        // can't actually have the personality function be our standard\n-        // `rust_eh_personality` function, but rather we wired it up to the\n-        // CRT's custom personality function, which forces LLVM to consider\n-        // landing pads as \"landing pads for SEH\".\n-        let ccx = self.ccx;\n-        let tcx = ccx.tcx();\n-        match tcx.lang_items.eh_personality() {\n-            Some(def_id) if !base::wants_msvc_seh(ccx.sess()) => {\n-                Callee::def(ccx, def_id, tcx.intern_substs(&[])).reify(ccx)\n-            }\n-            _ => {\n-                if let Some(llpersonality) = ccx.eh_personality().get() {\n-                    return llpersonality\n-                }\n-                let name = if base::wants_msvc_seh(ccx.sess()) {\n-                    \"__CxxFrameHandler3\"\n-                } else {\n-                    \"rust_eh_personality\"\n-                };\n-                let fty = Type::variadic_func(&[], &Type::i32(ccx));\n-                let f = declare::declare_cfn(ccx, name, fty);\n-                ccx.eh_personality().set(Some(f));\n-                f\n-            }\n-        }\n+    pub fn alloca(&self, ty: Type, name: &str) -> ValueRef {\n+        self.alloca_builder.dynamic_alloca(ty, name)\n     }\n+}\n \n-    // Returns a ValueRef of the \"eh_unwind_resume\" lang item if one is defined,\n-    // otherwise declares it as an external function.\n-    pub fn eh_unwind_resume(&self) -> Callee<'tcx> {\n-        use attributes;\n-        let ccx = self.ccx;\n-        let tcx = ccx.tcx();\n-        assert!(ccx.sess().target.target.options.custom_unwind_resume);\n-        if let Some(def_id) = tcx.lang_items.eh_unwind_resume() {\n-            return Callee::def(ccx, def_id, tcx.intern_substs(&[]));\n-        }\n-\n-        let ty = tcx.mk_fn_ptr(tcx.mk_bare_fn(ty::BareFnTy {\n-            unsafety: hir::Unsafety::Unsafe,\n-            abi: Abi::C,\n-            sig: ty::Binder(tcx.mk_fn_sig(\n-                iter::once(tcx.mk_mut_ptr(tcx.types.u8)),\n-                tcx.types.never,\n-                false\n-            )),\n-        }));\n-\n-        let unwresume = ccx.eh_unwind_resume();\n-        if let Some(llfn) = unwresume.get() {\n-            return Callee::ptr(llfn, ty);\n+impl<'a, 'tcx> Drop for FunctionContext<'a, 'tcx> {\n+    fn drop(&mut self) {\n+        unsafe {\n+            llvm::LLVMInstructionEraseFromParent(self.alloca_insert_pt.unwrap());\n         }\n-        let llfn = declare::declare_fn(ccx, \"rust_eh_unwind_resume\", ty);\n-        attributes::unwind(llfn, true);\n-        unwresume.set(Some(llfn));\n-        Callee::ptr(llfn, ty)\n     }\n }\n \n-// Basic block context.  We create a block context for each basic block\n-// (single-entry, single-exit sequence of instructions) we generate from Rust\n-// code.  Each basic block we generate is attached to a function, typically\n-// with many basic blocks per function.  All the basic blocks attached to a\n-// function are organized as a directed graph.\n-pub struct BlockS<'blk, 'tcx: 'blk> {\n+#[must_use]\n+pub struct BlockAndBuilder<'a, 'tcx: 'a> {\n     // The BasicBlockRef returned from a call to\n     // llvm::LLVMAppendBasicBlock(llfn, name), which adds a basic\n     // block to the function pointed to by llfn.  We insert\n     // instructions into that block by way of this block context.\n     // The block pointing to this one in the function's digraph.\n-    pub llbb: BasicBlockRef,\n-    pub terminated: Cell<bool>,\n-    pub unreachable: Cell<bool>,\n-\n-    // If this block part of a landing pad, then this is `Some` indicating what\n-    // kind of landing pad its in, otherwise this is none.\n-    pub lpad: Cell<Option<&'blk LandingPad>>,\n+    llbb: BasicBlockRef,\n \n     // The function context for the function to which this block is\n     // attached.\n-    pub fcx: &'blk FunctionContext<'blk, 'tcx>,\n-}\n-\n-pub type Block<'blk, 'tcx> = &'blk BlockS<'blk, 'tcx>;\n-\n-impl<'blk, 'tcx> BlockS<'blk, 'tcx> {\n-    pub fn new(llbb: BasicBlockRef,\n-               fcx: &'blk FunctionContext<'blk, 'tcx>)\n-               -> Block<'blk, 'tcx> {\n-        fcx.block_arena.alloc(BlockS {\n-            llbb: llbb,\n-            terminated: Cell::new(false),\n-            unreachable: Cell::new(false),\n-            lpad: Cell::new(None),\n-            fcx: fcx\n-        })\n-    }\n-\n-    pub fn ccx(&self) -> &'blk CrateContext<'blk, 'tcx> {\n-        self.fcx.ccx\n-    }\n-    pub fn fcx(&self) -> &'blk FunctionContext<'blk, 'tcx> {\n-        self.fcx\n-    }\n-    pub fn tcx(&self) -> TyCtxt<'blk, 'tcx, 'tcx> {\n-        self.fcx.ccx.tcx()\n-    }\n-    pub fn sess(&self) -> &'blk Session { self.fcx.ccx.sess() }\n-\n-    pub fn lpad(&self) -> Option<&'blk LandingPad> {\n-        self.lpad.get()\n-    }\n-\n-    pub fn set_lpad_ref(&self, lpad: Option<&'blk LandingPad>) {\n-        // FIXME: use an IVar?\n-        self.lpad.set(lpad);\n-    }\n+    fcx: &'a FunctionContext<'a, 'tcx>,\n \n-    pub fn set_lpad(&self, lpad: Option<LandingPad>) {\n-        self.set_lpad_ref(lpad.map(|p| &*self.fcx().lpad_arena.alloc(p)))\n-    }\n-\n-    pub fn mir(&self) -> Ref<'tcx, Mir<'tcx>> {\n-        self.fcx.mir()\n-    }\n-\n-    pub fn name(&self, name: ast::Name) -> String {\n-        name.to_string()\n-    }\n-\n-    pub fn node_id_to_string(&self, id: ast::NodeId) -> String {\n-        self.tcx().map.node_to_string(id).to_string()\n-    }\n-\n-    pub fn to_str(&self) -> String {\n-        format!(\"[block {:p}]\", self)\n-    }\n-\n-    pub fn monomorphize<T>(&self, value: &T) -> T\n-        where T: TransNormalize<'tcx>\n-    {\n-        monomorphize::apply_param_substs(self.fcx.ccx.shared(),\n-                                         self.fcx.param_substs,\n-                                         value)\n-    }\n-\n-    pub fn build(&'blk self) -> BlockAndBuilder<'blk, 'tcx> {\n-        BlockAndBuilder::new(self, OwnedBuilder::new_with_ccx(self.ccx()))\n-    }\n-}\n-\n-pub struct OwnedBuilder<'blk, 'tcx: 'blk> {\n-    builder: Builder<'blk, 'tcx>\n+    builder: Builder<'a, 'tcx>,\n }\n \n-impl<'blk, 'tcx> OwnedBuilder<'blk, 'tcx> {\n-    pub fn new_with_ccx(ccx: &'blk CrateContext<'blk, 'tcx>) -> Self {\n-        // Create a fresh builder from the crate context.\n-        let llbuilder = unsafe {\n-            llvm::LLVMCreateBuilderInContext(ccx.llcx())\n-        };\n-        OwnedBuilder {\n-            builder: Builder {\n-                llbuilder: llbuilder,\n-                ccx: ccx,\n-            }\n-        }\n-    }\n-}\n-\n-impl<'blk, 'tcx> Drop for OwnedBuilder<'blk, 'tcx> {\n-    fn drop(&mut self) {\n-        unsafe {\n-            llvm::LLVMDisposeBuilder(self.builder.llbuilder);\n-        }\n-    }\n-}\n-\n-pub struct BlockAndBuilder<'blk, 'tcx: 'blk> {\n-    bcx: Block<'blk, 'tcx>,\n-    owned_builder: OwnedBuilder<'blk, 'tcx>,\n-}\n-\n-impl<'blk, 'tcx> BlockAndBuilder<'blk, 'tcx> {\n-    pub fn new(bcx: Block<'blk, 'tcx>, owned_builder: OwnedBuilder<'blk, 'tcx>) -> Self {\n+impl<'a, 'tcx> BlockAndBuilder<'a, 'tcx> {\n+    pub fn new(llbb: BasicBlockRef, fcx: &'a FunctionContext<'a, 'tcx>) -> Self {\n+        let builder = Builder::with_ccx(fcx.ccx);\n         // Set the builder's position to this block's end.\n-        owned_builder.builder.position_at_end(bcx.llbb);\n+        builder.position_at_end(llbb);\n         BlockAndBuilder {\n-            bcx: bcx,\n-            owned_builder: owned_builder,\n+            llbb: llbb,\n+            fcx: fcx,\n+            builder: builder,\n         }\n     }\n \n-    pub fn with_block<F, R>(&self, f: F) -> R\n-        where F: FnOnce(Block<'blk, 'tcx>) -> R\n-    {\n-        let result = f(self.bcx);\n-        self.position_at_end(self.bcx.llbb);\n-        result\n-    }\n-\n-    pub fn map_block<F>(self, f: F) -> Self\n-        where F: FnOnce(Block<'blk, 'tcx>) -> Block<'blk, 'tcx>\n-    {\n-        let BlockAndBuilder { bcx, owned_builder } = self;\n-        let bcx = f(bcx);\n-        BlockAndBuilder::new(bcx, owned_builder)\n-    }\n-\n     pub fn at_start<F, R>(&self, f: F) -> R\n-        where F: FnOnce(&BlockAndBuilder<'blk, 'tcx>) -> R\n+        where F: FnOnce(&BlockAndBuilder<'a, 'tcx>) -> R\n     {\n-        self.position_at_start(self.bcx.llbb);\n+        self.position_at_start(self.llbb);\n         let r = f(self);\n-        self.position_at_end(self.bcx.llbb);\n+        self.position_at_end(self.llbb);\n         r\n     }\n \n-    // Methods delegated to bcx\n-\n-    pub fn is_unreachable(&self) -> bool {\n-        self.bcx.unreachable.get()\n-    }\n-\n-    pub fn ccx(&self) -> &'blk CrateContext<'blk, 'tcx> {\n-        self.bcx.ccx()\n-    }\n-    pub fn fcx(&self) -> &'blk FunctionContext<'blk, 'tcx> {\n-        self.bcx.fcx()\n+    pub fn fcx(&self) -> &'a FunctionContext<'a, 'tcx> {\n+        self.fcx\n     }\n-    pub fn tcx(&self) -> TyCtxt<'blk, 'tcx, 'tcx> {\n-        self.bcx.tcx()\n+    pub fn tcx(&self) -> TyCtxt<'a, 'tcx, 'tcx> {\n+        self.ccx.tcx()\n     }\n-    pub fn sess(&self) -> &'blk Session {\n-        self.bcx.sess()\n+    pub fn sess(&self) -> &'a Session {\n+        self.ccx.sess()\n     }\n \n     pub fn llbb(&self) -> BasicBlockRef {\n-        self.bcx.llbb\n-    }\n-\n-    pub fn mir(&self) -> Ref<'tcx, Mir<'tcx>> {\n-        self.bcx.mir()\n-    }\n-\n-    pub fn monomorphize<T>(&self, value: &T) -> T\n-        where T: TransNormalize<'tcx>\n-    {\n-        self.bcx.monomorphize(value)\n-    }\n-\n-    pub fn set_lpad(&self, lpad: Option<LandingPad>) {\n-        self.bcx.set_lpad(lpad)\n-    }\n-\n-    pub fn set_lpad_ref(&self, lpad: Option<&'blk LandingPad>) {\n-        // FIXME: use an IVar?\n-        self.bcx.set_lpad_ref(lpad);\n-    }\n-\n-    pub fn lpad(&self) -> Option<&'blk LandingPad> {\n-        self.bcx.lpad()\n+        self.llbb\n     }\n }\n \n-impl<'blk, 'tcx> Deref for BlockAndBuilder<'blk, 'tcx> {\n-    type Target = Builder<'blk, 'tcx>;\n+impl<'a, 'tcx> Deref for BlockAndBuilder<'a, 'tcx> {\n+    type Target = Builder<'a, 'tcx>;\n     fn deref(&self) -> &Self::Target {\n-        &self.owned_builder.builder\n+        &self.builder\n     }\n }\n \n@@ -663,53 +369,33 @@ impl<'blk, 'tcx> Deref for BlockAndBuilder<'blk, 'tcx> {\n /// When inside of a landing pad, each function call in LLVM IR needs to be\n /// annotated with which landing pad it's a part of. This is accomplished via\n /// the `OperandBundleDef` value created for MSVC landing pads.\n-pub struct LandingPad {\n-    cleanuppad: Option<ValueRef>,\n-    operand: Option<OperandBundleDef>,\n+pub struct Funclet {\n+    cleanuppad: ValueRef,\n+    operand: OperandBundleDef,\n }\n \n-impl LandingPad {\n-    pub fn gnu() -> LandingPad {\n-        LandingPad { cleanuppad: None, operand: None }\n-    }\n-\n-    pub fn msvc(cleanuppad: ValueRef) -> LandingPad {\n-        LandingPad {\n-            cleanuppad: Some(cleanuppad),\n-            operand: Some(OperandBundleDef::new(\"funclet\", &[cleanuppad])),\n+impl Funclet {\n+    pub fn new(cleanuppad: ValueRef) -> Funclet {\n+        Funclet {\n+            cleanuppad: cleanuppad,\n+            operand: OperandBundleDef::new(\"funclet\", &[cleanuppad]),\n         }\n     }\n \n-    pub fn bundle(&self) -> Option<&OperandBundleDef> {\n-        self.operand.as_ref()\n-    }\n-\n-    pub fn cleanuppad(&self) -> Option<ValueRef> {\n+    pub fn cleanuppad(&self) -> ValueRef {\n         self.cleanuppad\n     }\n-}\n \n-impl Clone for LandingPad {\n-    fn clone(&self) -> LandingPad {\n-        LandingPad {\n-            cleanuppad: self.cleanuppad,\n-            operand: self.cleanuppad.map(|p| {\n-                OperandBundleDef::new(\"funclet\", &[p])\n-            }),\n-        }\n+    pub fn bundle(&self) -> &OperandBundleDef {\n+        &self.operand\n     }\n }\n \n-pub struct Result<'blk, 'tcx: 'blk> {\n-    pub bcx: Block<'blk, 'tcx>,\n-    pub val: ValueRef\n-}\n-\n-impl<'b, 'tcx> Result<'b, 'tcx> {\n-    pub fn new(bcx: Block<'b, 'tcx>, val: ValueRef) -> Result<'b, 'tcx> {\n-        Result {\n-            bcx: bcx,\n-            val: val,\n+impl Clone for Funclet {\n+    fn clone(&self) -> Funclet {\n+        Funclet {\n+            cleanuppad: self.cleanuppad,\n+            operand: OperandBundleDef::new(\"funclet\", &[self.cleanuppad]),\n         }\n     }\n }\n@@ -1016,43 +702,42 @@ pub fn langcall(tcx: TyCtxt,\n // all shifts). For 32- and 64-bit types, this matches the semantics\n // of Java. (See related discussion on #1877 and #10183.)\n \n-pub fn build_unchecked_lshift<'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n-                                          lhs: ValueRef,\n-                                          rhs: ValueRef,\n-                                          binop_debug_loc: DebugLoc) -> ValueRef {\n+pub fn build_unchecked_lshift<'a, 'tcx>(\n+    bcx: &BlockAndBuilder<'a, 'tcx>,\n+    lhs: ValueRef,\n+    rhs: ValueRef\n+) -> ValueRef {\n     let rhs = base::cast_shift_expr_rhs(bcx, hir::BinOp_::BiShl, lhs, rhs);\n     // #1877, #10183: Ensure that input is always valid\n-    let rhs = shift_mask_rhs(bcx, rhs, binop_debug_loc);\n-    build::Shl(bcx, lhs, rhs, binop_debug_loc)\n+    let rhs = shift_mask_rhs(bcx, rhs);\n+    bcx.shl(lhs, rhs)\n }\n \n-pub fn build_unchecked_rshift<'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n-                                          lhs_t: Ty<'tcx>,\n-                                          lhs: ValueRef,\n-                                          rhs: ValueRef,\n-                                          binop_debug_loc: DebugLoc) -> ValueRef {\n+pub fn build_unchecked_rshift<'a, 'tcx>(\n+    bcx: &BlockAndBuilder<'a, 'tcx>, lhs_t: Ty<'tcx>, lhs: ValueRef, rhs: ValueRef\n+) -> ValueRef {\n     let rhs = base::cast_shift_expr_rhs(bcx, hir::BinOp_::BiShr, lhs, rhs);\n     // #1877, #10183: Ensure that input is always valid\n-    let rhs = shift_mask_rhs(bcx, rhs, binop_debug_loc);\n+    let rhs = shift_mask_rhs(bcx, rhs);\n     let is_signed = lhs_t.is_signed();\n     if is_signed {\n-        build::AShr(bcx, lhs, rhs, binop_debug_loc)\n+        bcx.ashr(lhs, rhs)\n     } else {\n-        build::LShr(bcx, lhs, rhs, binop_debug_loc)\n+        bcx.lshr(lhs, rhs)\n     }\n }\n \n-fn shift_mask_rhs<'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n-                              rhs: ValueRef,\n-                              debug_loc: DebugLoc) -> ValueRef {\n+fn shift_mask_rhs<'a, 'tcx>(bcx: &BlockAndBuilder<'a, 'tcx>, rhs: ValueRef) -> ValueRef {\n     let rhs_llty = val_ty(rhs);\n-    build::And(bcx, rhs, shift_mask_val(bcx, rhs_llty, rhs_llty, false), debug_loc)\n+    bcx.and(rhs, shift_mask_val(bcx, rhs_llty, rhs_llty, false))\n }\n \n-pub fn shift_mask_val<'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n-                              llty: Type,\n-                              mask_llty: Type,\n-                              invert: bool) -> ValueRef {\n+pub fn shift_mask_val<'a, 'tcx>(\n+    bcx: &BlockAndBuilder<'a, 'tcx>,\n+    llty: Type,\n+    mask_llty: Type,\n+    invert: bool\n+) -> ValueRef {\n     let kind = llty.kind();\n     match kind {\n         TypeKind::Integer => {\n@@ -1066,7 +751,7 @@ pub fn shift_mask_val<'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n         },\n         TypeKind::Vector => {\n             let mask = shift_mask_val(bcx, llty.element_type(), mask_llty.element_type(), invert);\n-            build::VectorSplat(bcx, mask_llty.vector_length(), mask)\n+            bcx.vector_splat(mask_llty.vector_length(), mask)\n         },\n         _ => bug!(\"shift_mask_val: expected Integer or Vector, found {:?}\", kind),\n     }"}, {"sha": "2e2644d91bb6c3584aadeec23a53e7b52cbd6b53", "filename": "src/librustc_trans/consts.rs", "status": "modified", "additions": 1, "deletions": 2, "changes": 3, "blob_url": "https://github.com/rust-lang/rust/blob/1b38776c1f68c6fd47c1b2f7b7974efc7dd64901/src%2Flibrustc_trans%2Fconsts.rs", "raw_url": "https://github.com/rust-lang/rust/raw/1b38776c1f68c6fd47c1b2f7b7974efc7dd64901/src%2Flibrustc_trans%2Fconsts.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_trans%2Fconsts.rs?ref=1b38776c1f68c6fd47c1b2f7b7974efc7dd64901", "patch": "@@ -16,7 +16,7 @@ use rustc_const_eval::ConstEvalErr;\n use rustc::hir::def_id::DefId;\n use rustc::hir::map as hir_map;\n use {debuginfo, machine};\n-use base::{self, push_ctxt};\n+use base;\n use trans_item::TransItem;\n use common::{CrateContext, val_ty};\n use declare;\n@@ -221,7 +221,6 @@ pub fn trans_static(ccx: &CrateContext,\n                     attrs: &[ast::Attribute])\n                     -> Result<ValueRef, ConstEvalErr> {\n     unsafe {\n-        let _icx = push_ctxt(\"trans_static\");\n         let def_id = ccx.tcx().map.local_def_id(id);\n         let g = get_static(ccx, def_id);\n "}, {"sha": "f292a70965004c0a2c3964b043d1842cc1889e5b", "filename": "src/librustc_trans/context.rs", "status": "modified", "additions": 95, "deletions": 88, "changes": 183, "blob_url": "https://github.com/rust-lang/rust/blob/1b38776c1f68c6fd47c1b2f7b7974efc7dd64901/src%2Flibrustc_trans%2Fcontext.rs", "raw_url": "https://github.com/rust-lang/rust/raw/1b38776c1f68c6fd47c1b2f7b7974efc7dd64901/src%2Flibrustc_trans%2Fcontext.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_trans%2Fcontext.rs?ref=1b38776c1f68c6fd47c1b2f7b7974efc7dd64901", "patch": "@@ -9,17 +9,16 @@\n // except according to those terms.\n \n use llvm;\n-use llvm::{ContextRef, ModuleRef, ValueRef, BuilderRef};\n-use rustc::dep_graph::{DepGraph, DepNode, DepTrackingMap, DepTrackingMapConfig,\n-                       WorkProduct};\n+use llvm::{ContextRef, ModuleRef, ValueRef};\n+use rustc::dep_graph::{DepGraph, DepNode, DepTrackingMap, DepTrackingMapConfig, WorkProduct};\n use middle::cstore::LinkMeta;\n+use rustc::hir;\n use rustc::hir::def::ExportMap;\n use rustc::hir::def_id::DefId;\n use rustc::traits;\n-use base;\n-use builder::Builder;\n-use common::BuilderRef_res;\n use debuginfo;\n+use callee::Callee;\n+use base;\n use declare;\n use glue::DropGlueKind;\n use monomorphize::Instance;\n@@ -40,11 +39,13 @@ use std::ffi::{CStr, CString};\n use std::cell::{Cell, RefCell};\n use std::marker::PhantomData;\n use std::ptr;\n+use std::iter;\n use std::rc::Rc;\n use std::str;\n use syntax::ast;\n use syntax::symbol::InternedString;\n-use abi::FnType;\n+use syntax_pos::DUMMY_SP;\n+use abi::{Abi, FnType};\n \n pub struct Stats {\n     pub n_glues_created: Cell<usize>,\n@@ -71,6 +72,7 @@ pub struct SharedCrateContext<'a, 'tcx: 'a> {\n     exported_symbols: NodeSet,\n     link_meta: LinkMeta,\n     tcx: TyCtxt<'a, 'tcx, 'tcx>,\n+    empty_param_env: ty::ParameterEnvironment<'tcx>,\n     stats: Stats,\n     check_overflow: bool,\n \n@@ -140,7 +142,6 @@ pub struct LocalCrateContext<'tcx> {\n     int_type: Type,\n     opaque_vec_type: Type,\n     str_slice_type: Type,\n-    builder: BuilderRef_res,\n \n     /// Holds the LLVM values for closure IDs.\n     closure_vals: RefCell<FxHashMap<Instance<'tcx>, ValueRef>>,\n@@ -153,11 +154,6 @@ pub struct LocalCrateContext<'tcx> {\n \n     intrinsics: RefCell<FxHashMap<&'static str, ValueRef>>,\n \n-    /// Number of LLVM instructions translated into this `LocalCrateContext`.\n-    /// This is used to perform some basic load-balancing to keep all LLVM\n-    /// contexts around the same size.\n-    n_llvm_insns: Cell<usize>,\n-\n     /// Depth of the current type-of computation - used to bail out\n     type_of_depth: Cell<usize>,\n \n@@ -316,38 +312,6 @@ impl<'a, 'tcx> Iterator for CrateContextIterator<'a,'tcx> {\n     }\n }\n \n-/// The iterator produced by `CrateContext::maybe_iter`.\n-pub struct CrateContextMaybeIterator<'a, 'tcx: 'a> {\n-    shared: &'a SharedCrateContext<'a, 'tcx>,\n-    local_ccxs: &'a [LocalCrateContext<'tcx>],\n-    index: usize,\n-    single: bool,\n-    origin: usize,\n-}\n-\n-impl<'a, 'tcx> Iterator for CrateContextMaybeIterator<'a, 'tcx> {\n-    type Item = (CrateContext<'a, 'tcx>, bool);\n-\n-    fn next(&mut self) -> Option<(CrateContext<'a, 'tcx>, bool)> {\n-        if self.index >= self.local_ccxs.len() {\n-            return None;\n-        }\n-\n-        let index = self.index;\n-        self.index += 1;\n-        if self.single {\n-            self.index = self.local_ccxs.len();\n-        }\n-\n-        let ccx = CrateContext {\n-            shared: self.shared,\n-            index: index,\n-            local_ccxs: self.local_ccxs\n-        };\n-        Some((ccx, index == self.origin))\n-    }\n-}\n-\n pub fn get_reloc_model(sess: &Session) -> llvm::RelocMode {\n     let reloc_model_arg = match sess.opts.cg.relocation_model {\n         Some(ref s) => &s[..],\n@@ -496,6 +460,7 @@ impl<'b, 'tcx> SharedCrateContext<'b, 'tcx> {\n             export_map: export_map,\n             exported_symbols: exported_symbols,\n             link_meta: link_meta,\n+            empty_param_env: tcx.empty_parameter_environment(),\n             tcx: tcx,\n             stats: Stats {\n                 n_glues_created: Cell::new(0),\n@@ -516,6 +481,14 @@ impl<'b, 'tcx> SharedCrateContext<'b, 'tcx> {\n         }\n     }\n \n+    pub fn type_needs_drop(&self, ty: Ty<'tcx>) -> bool {\n+        self.tcx.type_needs_drop_given_env(ty, &self.empty_param_env)\n+    }\n+\n+    pub fn type_is_sized(&self, ty: Ty<'tcx>) -> bool {\n+        ty.is_sized(self.tcx, &self.empty_param_env, DUMMY_SP)\n+    }\n+\n     pub fn metadata_llmod(&self) -> ModuleRef {\n         self.metadata_llmod\n     }\n@@ -638,14 +611,12 @@ impl<'tcx> LocalCrateContext<'tcx> {\n                 int_type: Type::from_ref(ptr::null_mut()),\n                 opaque_vec_type: Type::from_ref(ptr::null_mut()),\n                 str_slice_type: Type::from_ref(ptr::null_mut()),\n-                builder: BuilderRef_res(llvm::LLVMCreateBuilderInContext(llcx)),\n                 closure_vals: RefCell::new(FxHashMap()),\n                 dbg_cx: dbg_cx,\n                 eh_personality: Cell::new(None),\n                 eh_unwind_resume: Cell::new(None),\n                 rust_try_fn: Cell::new(None),\n                 intrinsics: RefCell::new(FxHashMap()),\n-                n_llvm_insns: Cell::new(0),\n                 type_of_depth: Cell::new(0),\n                 symbol_map: symbol_map,\n                 local_gen_sym_counter: Cell::new(0),\n@@ -671,10 +642,6 @@ impl<'tcx> LocalCrateContext<'tcx> {\n             local_ccx.opaque_vec_type = opaque_vec_type;\n             local_ccx.str_slice_type = str_slice_ty;\n \n-            if shared.tcx.sess.count_llvm_insns() {\n-                base::init_insn_ctxt()\n-            }\n-\n             local_ccx\n         }\n     }\n@@ -703,26 +670,10 @@ impl<'b, 'tcx> CrateContext<'b, 'tcx> {\n         self.shared\n     }\n \n-    pub fn local(&self) -> &'b LocalCrateContext<'tcx> {\n+    fn local(&self) -> &'b LocalCrateContext<'tcx> {\n         &self.local_ccxs[self.index]\n     }\n \n-    /// Either iterate over only `self`, or iterate over all `CrateContext`s in\n-    /// the `SharedCrateContext`.  The iterator produces `(ccx, is_origin)`\n-    /// pairs, where `is_origin` is `true` if `ccx` is `self` and `false`\n-    /// otherwise.  This method is useful for avoiding code duplication in\n-    /// cases where it may or may not be necessary to translate code into every\n-    /// context.\n-    pub fn maybe_iter(&self, iter_all: bool) -> CrateContextMaybeIterator<'b, 'tcx> {\n-        CrateContextMaybeIterator {\n-            shared: self.shared,\n-            index: if iter_all { 0 } else { self.index },\n-            single: !iter_all,\n-            origin: self.index,\n-            local_ccxs: self.local_ccxs,\n-        }\n-    }\n-\n     pub fn tcx<'a>(&'a self) -> TyCtxt<'a, 'tcx, 'tcx> {\n         self.shared.tcx\n     }\n@@ -731,14 +682,6 @@ impl<'b, 'tcx> CrateContext<'b, 'tcx> {\n         &self.shared.tcx.sess\n     }\n \n-    pub fn builder<'a>(&'a self) -> Builder<'a, 'tcx> {\n-        Builder::new(self)\n-    }\n-\n-    pub fn raw_builder<'a>(&'a self) -> BuilderRef {\n-        self.local().builder.b\n-    }\n-\n     pub fn get_intrinsic(&self, key: &str) -> ValueRef {\n         if let Some(v) = self.intrinsics().borrow().get(key).cloned() {\n             return v;\n@@ -886,14 +829,6 @@ impl<'b, 'tcx> CrateContext<'b, 'tcx> {\n         &self.local().dbg_cx\n     }\n \n-    pub fn eh_personality<'a>(&'a self) -> &'a Cell<Option<ValueRef>> {\n-        &self.local().eh_personality\n-    }\n-\n-    pub fn eh_unwind_resume<'a>(&'a self) -> &'a Cell<Option<ValueRef>> {\n-        &self.local().eh_unwind_resume\n-    }\n-\n     pub fn rust_try_fn<'a>(&'a self) -> &'a Cell<Option<ValueRef>> {\n         &self.local().rust_try_fn\n     }\n@@ -902,10 +837,6 @@ impl<'b, 'tcx> CrateContext<'b, 'tcx> {\n         &self.local().intrinsics\n     }\n \n-    pub fn count_llvm_insn(&self) {\n-        self.local().n_llvm_insns.set(self.local().n_llvm_insns.get() + 1);\n-    }\n-\n     pub fn obj_size_bound(&self) -> u64 {\n         self.tcx().data_layout.obj_size_bound()\n     }\n@@ -974,6 +905,82 @@ impl<'b, 'tcx> CrateContext<'b, 'tcx> {\n         base_n::push_str(idx as u64, base_n::ALPHANUMERIC_ONLY, &mut name);\n         name\n     }\n+\n+    pub fn eh_personality(&self) -> ValueRef {\n+        // The exception handling personality function.\n+        //\n+        // If our compilation unit has the `eh_personality` lang item somewhere\n+        // within it, then we just need to translate that. Otherwise, we're\n+        // building an rlib which will depend on some upstream implementation of\n+        // this function, so we just codegen a generic reference to it. We don't\n+        // specify any of the types for the function, we just make it a symbol\n+        // that LLVM can later use.\n+        //\n+        // Note that MSVC is a little special here in that we don't use the\n+        // `eh_personality` lang item at all. Currently LLVM has support for\n+        // both Dwarf and SEH unwind mechanisms for MSVC targets and uses the\n+        // *name of the personality function* to decide what kind of unwind side\n+        // tables/landing pads to emit. It looks like Dwarf is used by default,\n+        // injecting a dependency on the `_Unwind_Resume` symbol for resuming\n+        // an \"exception\", but for MSVC we want to force SEH. This means that we\n+        // can't actually have the personality function be our standard\n+        // `rust_eh_personality` function, but rather we wired it up to the\n+        // CRT's custom personality function, which forces LLVM to consider\n+        // landing pads as \"landing pads for SEH\".\n+        if let Some(llpersonality) = self.local().eh_personality.get() {\n+            return llpersonality\n+        }\n+        let tcx = self.tcx();\n+        let llfn = match tcx.lang_items.eh_personality() {\n+            Some(def_id) if !base::wants_msvc_seh(self.sess()) => {\n+                Callee::def(self, def_id, tcx.intern_substs(&[])).reify(self)\n+            }\n+            _ => {\n+                let name = if base::wants_msvc_seh(self.sess()) {\n+                    \"__CxxFrameHandler3\"\n+                } else {\n+                    \"rust_eh_personality\"\n+                };\n+                let fty = Type::variadic_func(&[], &Type::i32(self));\n+                declare::declare_cfn(self, name, fty)\n+            }\n+        };\n+        self.local().eh_personality.set(Some(llfn));\n+        llfn\n+    }\n+\n+    // Returns a ValueRef of the \"eh_unwind_resume\" lang item if one is defined,\n+    // otherwise declares it as an external function.\n+    pub fn eh_unwind_resume(&self) -> ValueRef {\n+        use attributes;\n+        let unwresume = &self.local().eh_unwind_resume;\n+        if let Some(llfn) = unwresume.get() {\n+            return llfn;\n+        }\n+\n+        let tcx = self.tcx();\n+        assert!(self.sess().target.target.options.custom_unwind_resume);\n+        if let Some(def_id) = tcx.lang_items.eh_unwind_resume() {\n+            let llfn = Callee::def(self, def_id, tcx.intern_substs(&[])).reify(self);\n+            unwresume.set(Some(llfn));\n+            return llfn;\n+        }\n+\n+        let ty = tcx.mk_fn_ptr(tcx.mk_bare_fn(ty::BareFnTy {\n+            unsafety: hir::Unsafety::Unsafe,\n+            abi: Abi::C,\n+            sig: ty::Binder(tcx.mk_fn_sig(\n+                iter::once(tcx.mk_mut_ptr(tcx.types.u8)),\n+                tcx.types.never,\n+                false\n+            )),\n+        }));\n+\n+        let llfn = declare::declare_fn(self, \"rust_eh_unwind_resume\", ty);\n+        attributes::unwind(llfn, true);\n+        unwresume.set(Some(llfn));\n+        llfn\n+    }\n }\n \n pub struct TypeOfDepthLock<'a, 'tcx: 'a>(&'a LocalCrateContext<'tcx>);"}, {"sha": "f5a8eeacf38adba37051666d798402945cb92170", "filename": "src/librustc_trans/debuginfo/create_scope_map.rs", "status": "modified", "additions": 4, "deletions": 4, "changes": 8, "blob_url": "https://github.com/rust-lang/rust/blob/1b38776c1f68c6fd47c1b2f7b7974efc7dd64901/src%2Flibrustc_trans%2Fdebuginfo%2Fcreate_scope_map.rs", "raw_url": "https://github.com/rust-lang/rust/raw/1b38776c1f68c6fd47c1b2f7b7974efc7dd64901/src%2Flibrustc_trans%2Fdebuginfo%2Fcreate_scope_map.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_trans%2Fdebuginfo%2Fcreate_scope_map.rs?ref=1b38776c1f68c6fd47c1b2f7b7974efc7dd64901", "patch": "@@ -44,17 +44,17 @@ impl MirDebugScope {\n \n /// Produce DIScope DIEs for each MIR Scope which has variables defined in it.\n /// If debuginfo is disabled, the returned vector is empty.\n-pub fn create_mir_scopes(fcx: &FunctionContext) -> IndexVec<VisibilityScope, MirDebugScope> {\n-    let mir = fcx.mir();\n+pub fn create_mir_scopes(fcx: &FunctionContext, mir: &Mir, debug_context: &FunctionDebugContext)\n+    -> IndexVec<VisibilityScope, MirDebugScope> {\n     let null_scope = MirDebugScope {\n         scope_metadata: ptr::null_mut(),\n         file_start_pos: BytePos(0),\n         file_end_pos: BytePos(0)\n     };\n     let mut scopes = IndexVec::from_elem(null_scope, &mir.visibility_scopes);\n \n-    let fn_metadata = match fcx.debug_context {\n-        FunctionDebugContext::RegularContext(box ref data) => data.fn_metadata,\n+    let fn_metadata = match *debug_context {\n+        FunctionDebugContext::RegularContext(ref data) => data.fn_metadata,\n         FunctionDebugContext::DebugInfoDisabled |\n         FunctionDebugContext::FunctionWithoutDebugInfo => {\n             return scopes;"}, {"sha": "e8728a39993081bc3bcc45966b43537922ea2b37", "filename": "src/librustc_trans/debuginfo/gdb.rs", "status": "modified", "additions": 8, "deletions": 19, "changes": 27, "blob_url": "https://github.com/rust-lang/rust/blob/1b38776c1f68c6fd47c1b2f7b7974efc7dd64901/src%2Flibrustc_trans%2Fdebuginfo%2Fgdb.rs", "raw_url": "https://github.com/rust-lang/rust/raw/1b38776c1f68c6fd47c1b2f7b7974efc7dd64901/src%2Flibrustc_trans%2Fdebuginfo%2Fgdb.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_trans%2Fdebuginfo%2Fgdb.rs?ref=1b38776c1f68c6fd47c1b2f7b7974efc7dd64901", "patch": "@@ -13,37 +13,26 @@\n use llvm;\n \n use common::{C_bytes, CrateContext, C_i32};\n+use builder::Builder;\n use declare;\n use type_::Type;\n use session::config::NoDebugInfo;\n \n-use std::ffi::CString;\n use std::ptr;\n use syntax::attr;\n \n \n /// Inserts a side-effect free instruction sequence that makes sure that the\n /// .debug_gdb_scripts global is referenced, so it isn't removed by the linker.\n-pub fn insert_reference_to_gdb_debug_scripts_section_global(ccx: &CrateContext) {\n+pub fn insert_reference_to_gdb_debug_scripts_section_global(ccx: &CrateContext, builder: &Builder) {\n     if needs_gdb_debug_scripts_section(ccx) {\n-        let empty = CString::new(\"\").unwrap();\n-        let gdb_debug_scripts_section_global =\n-            get_or_insert_gdb_debug_scripts_section_global(ccx);\n+        let gdb_debug_scripts_section_global = get_or_insert_gdb_debug_scripts_section_global(ccx);\n+        // Load just the first byte as that's all that's necessary to force\n+        // LLVM to keep around the reference to the global.\n+        let indices = [C_i32(ccx, 0), C_i32(ccx, 0)];\n+        let element = builder.inbounds_gep(gdb_debug_scripts_section_global, &indices);\n+        let volative_load_instruction = builder.volatile_load(element);\n         unsafe {\n-            // Load just the first byte as that's all that's necessary to force\n-            // LLVM to keep around the reference to the global.\n-            let indices = [C_i32(ccx, 0), C_i32(ccx, 0)];\n-            let element =\n-                llvm::LLVMBuildInBoundsGEP(ccx.raw_builder(),\n-                                           gdb_debug_scripts_section_global,\n-                                           indices.as_ptr(),\n-                                           indices.len() as ::libc::c_uint,\n-                                           empty.as_ptr());\n-            let volative_load_instruction =\n-                llvm::LLVMBuildLoad(ccx.raw_builder(),\n-                                    element,\n-                                    empty.as_ptr());\n-            llvm::LLVMSetVolatile(volative_load_instruction, llvm::True);\n             llvm::LLVMSetAlignment(volative_load_instruction, 1);\n         }\n     }"}, {"sha": "86099d241df686bec35b2594f87cb3ef78e990ab", "filename": "src/librustc_trans/debuginfo/mod.rs", "status": "modified", "additions": 31, "deletions": 68, "changes": 99, "blob_url": "https://github.com/rust-lang/rust/blob/1b38776c1f68c6fd47c1b2f7b7974efc7dd64901/src%2Flibrustc_trans%2Fdebuginfo%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/1b38776c1f68c6fd47c1b2f7b7974efc7dd64901/src%2Flibrustc_trans%2Fdebuginfo%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_trans%2Fdebuginfo%2Fmod.rs?ref=1b38776c1f68c6fd47c1b2f7b7974efc7dd64901", "patch": "@@ -27,7 +27,7 @@ use rustc::hir::def_id::DefId;\n use rustc::ty::subst::Substs;\n \n use abi::Abi;\n-use common::{CrateContext, FunctionContext, Block, BlockAndBuilder};\n+use common::{CrateContext, BlockAndBuilder};\n use monomorphize::{self, Instance};\n use rustc::ty::{self, Ty};\n use rustc::mir;\n@@ -55,6 +55,7 @@ pub use self::create_scope_map::{create_mir_scopes, MirDebugScope};\n pub use self::source_loc::start_emitting_source_locations;\n pub use self::metadata::create_global_var_metadata;\n pub use self::metadata::extend_scope_to_file;\n+pub use self::source_loc::set_source_location;\n \n #[allow(non_upper_case_globals)]\n const DW_TAG_auto_variable: c_uint = 0x100;\n@@ -65,7 +66,6 @@ const DW_TAG_arg_variable: c_uint = 0x101;\n pub struct CrateDebugContext<'tcx> {\n     llcontext: ContextRef,\n     builder: DIBuilderRef,\n-    current_debug_location: Cell<InternalDebugLocation>,\n     created_files: RefCell<FxHashMap<String, DIFile>>,\n     created_enum_disr_types: RefCell<FxHashMap<(DefId, layout::Integer), DIType>>,\n \n@@ -83,40 +83,33 @@ impl<'tcx> CrateDebugContext<'tcx> {\n         let builder = unsafe { llvm::LLVMRustDIBuilderCreate(llmod) };\n         // DIBuilder inherits context from the module, so we'd better use the same one\n         let llcontext = unsafe { llvm::LLVMGetModuleContext(llmod) };\n-        return CrateDebugContext {\n+        CrateDebugContext {\n             llcontext: llcontext,\n             builder: builder,\n-            current_debug_location: Cell::new(InternalDebugLocation::UnknownLocation),\n             created_files: RefCell::new(FxHashMap()),\n             created_enum_disr_types: RefCell::new(FxHashMap()),\n             type_map: RefCell::new(TypeMap::new()),\n             namespace_map: RefCell::new(DefIdMap()),\n             composite_types_completed: RefCell::new(FxHashSet()),\n-        };\n+        }\n     }\n }\n \n pub enum FunctionDebugContext {\n-    RegularContext(Box<FunctionDebugContextData>),\n+    RegularContext(FunctionDebugContextData),\n     DebugInfoDisabled,\n     FunctionWithoutDebugInfo,\n }\n \n impl FunctionDebugContext {\n-    fn get_ref<'a>(&'a self,\n-                   span: Span)\n-                   -> &'a FunctionDebugContextData {\n+    fn get_ref<'a>(&'a self, span: Span) -> &'a FunctionDebugContextData {\n         match *self {\n-            FunctionDebugContext::RegularContext(box ref data) => data,\n+            FunctionDebugContext::RegularContext(ref data) => data,\n             FunctionDebugContext::DebugInfoDisabled => {\n-                span_bug!(span,\n-                          \"{}\",\n-                          FunctionDebugContext::debuginfo_disabled_message());\n+                span_bug!(span, \"{}\", FunctionDebugContext::debuginfo_disabled_message());\n             }\n             FunctionDebugContext::FunctionWithoutDebugInfo => {\n-                span_bug!(span,\n-                          \"{}\",\n-                          FunctionDebugContext::should_be_ignored_message());\n+                span_bug!(span, \"{}\", FunctionDebugContext::should_be_ignored_message());\n             }\n         }\n     }\n@@ -134,7 +127,6 @@ impl FunctionDebugContext {\n pub struct FunctionDebugContextData {\n     fn_metadata: DISubprogram,\n     source_locations_enabled: Cell<bool>,\n-    source_location_override: Cell<bool>,\n }\n \n pub enum VariableAccess<'a> {\n@@ -197,18 +189,6 @@ pub fn finalize(cx: &CrateContext) {\n     };\n }\n \n-/// Creates a function-specific debug context for a function w/o debuginfo.\n-pub fn empty_function_debug_context<'a, 'tcx>(cx: &CrateContext<'a, 'tcx>)\n-                                              -> FunctionDebugContext {\n-    if cx.sess().opts.debuginfo == NoDebugInfo {\n-        return FunctionDebugContext::DebugInfoDisabled;\n-    }\n-\n-    // Clear the debug location so we don't assign them in the function prelude.\n-    source_loc::set_debug_location(cx, None, UnknownLocation);\n-    FunctionDebugContext::FunctionWithoutDebugInfo\n-}\n-\n /// Creates the function-specific debug context.\n ///\n /// Returns the FunctionDebugContext for the function which holds state needed\n@@ -225,15 +205,18 @@ pub fn create_function_debug_context<'a, 'tcx>(cx: &CrateContext<'a, 'tcx>,\n         return FunctionDebugContext::DebugInfoDisabled;\n     }\n \n-    // Clear the debug location so we don't assign them in the function prelude.\n-    // Do this here already, in case we do an early exit from this function.\n-    source_loc::set_debug_location(cx, None, UnknownLocation);\n+    for attr in cx.tcx().get_attrs(instance.def).iter() {\n+        if attr.check_name(\"no_debug\") {\n+            return FunctionDebugContext::FunctionWithoutDebugInfo;\n+        }\n+    }\n \n     let containing_scope = get_containing_scope(cx, instance);\n     let span = mir.span;\n \n     // This can be the case for functions inlined from another crate\n     if span == syntax_pos::DUMMY_SP {\n+        // FIXME(simulacrum): Probably can't happen; remove.\n         return FunctionDebugContext::FunctionWithoutDebugInfo;\n     }\n \n@@ -293,10 +276,9 @@ pub fn create_function_debug_context<'a, 'tcx>(cx: &CrateContext<'a, 'tcx>,\n     };\n \n     // Initialize fn debug context (including scope map and namespace map)\n-    let fn_debug_context = box FunctionDebugContextData {\n+    let fn_debug_context = FunctionDebugContextData {\n         fn_metadata: fn_metadata,\n         source_locations_enabled: Cell::new(false),\n-        source_location_override: Cell::new(false),\n     };\n \n     return FunctionDebugContext::RegularContext(fn_debug_context);\n@@ -441,14 +423,15 @@ pub fn create_function_debug_context<'a, 'tcx>(cx: &CrateContext<'a, 'tcx>,\n     }\n }\n \n-pub fn declare_local<'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n-                                 variable_name: ast::Name,\n-                                 variable_type: Ty<'tcx>,\n-                                 scope_metadata: DIScope,\n-                                 variable_access: VariableAccess,\n-                                 variable_kind: VariableKind,\n-                                 span: Span) {\n-    let cx: &CrateContext = bcx.ccx();\n+pub fn declare_local<'a, 'tcx>(bcx: &BlockAndBuilder<'a, 'tcx>,\n+                               dbg_context: &FunctionDebugContext,\n+                               variable_name: ast::Name,\n+                               variable_type: Ty<'tcx>,\n+                               scope_metadata: DIScope,\n+                               variable_access: VariableAccess,\n+                               variable_kind: VariableKind,\n+                               span: Span) {\n+    let cx = bcx.ccx;\n \n     let file = span_start(cx, span).file;\n     let filename = file.name.clone();\n@@ -483,49 +466,29 @@ pub fn declare_local<'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n                     align as u64,\n                 )\n             };\n-            source_loc::set_debug_location(cx, None,\n+            source_loc::set_debug_location(bcx,\n                 InternalDebugLocation::new(scope_metadata, loc.line, loc.col.to_usize()));\n             unsafe {\n-                let debug_loc = llvm::LLVMGetCurrentDebugLocation(cx.raw_builder());\n+                let debug_loc = llvm::LLVMGetCurrentDebugLocation(bcx.llbuilder);\n                 let instr = llvm::LLVMRustDIBuilderInsertDeclareAtEnd(\n                     DIB(cx),\n                     alloca,\n                     metadata,\n                     address_operations.as_ptr(),\n                     address_operations.len() as c_uint,\n                     debug_loc,\n-                    bcx.llbb);\n+                    bcx.llbb());\n \n-                llvm::LLVMSetInstDebugLocation(::build::B(bcx).llbuilder, instr);\n+                llvm::LLVMSetInstDebugLocation(bcx.llbuilder, instr);\n             }\n         }\n     }\n \n     match variable_kind {\n         ArgumentVariable(_) | CapturedVariable => {\n-            assert!(!bcx.fcx\n-                        .debug_context\n-                        .get_ref(span)\n-                        .source_locations_enabled\n-                        .get());\n-            source_loc::set_debug_location(cx, None, UnknownLocation);\n+            assert!(!dbg_context.get_ref(span).source_locations_enabled.get());\n+            source_loc::set_debug_location(bcx, UnknownLocation);\n         }\n         _ => { /* nothing to do */ }\n     }\n }\n-\n-#[derive(Copy, Clone, PartialEq, Eq, Debug)]\n-pub enum DebugLoc {\n-    ScopeAt(DIScope, Span),\n-    None\n-}\n-\n-impl DebugLoc {\n-    pub fn apply(self, fcx: &FunctionContext) {\n-        source_loc::set_source_location(fcx, None, self);\n-    }\n-\n-    pub fn apply_to_bcx(self, bcx: &BlockAndBuilder) {\n-        source_loc::set_source_location(bcx.fcx(), Some(bcx), self);\n-    }\n-}"}, {"sha": "e02c8be19a2f477315c2e1cdf671ee205c7f6796", "filename": "src/librustc_trans/debuginfo/source_loc.rs", "status": "modified", "additions": 18, "deletions": 48, "changes": 66, "blob_url": "https://github.com/rust-lang/rust/blob/1b38776c1f68c6fd47c1b2f7b7974efc7dd64901/src%2Flibrustc_trans%2Fdebuginfo%2Fsource_loc.rs", "raw_url": "https://github.com/rust-lang/rust/raw/1b38776c1f68c6fd47c1b2f7b7974efc7dd64901/src%2Flibrustc_trans%2Fdebuginfo%2Fsource_loc.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_trans%2Fdebuginfo%2Fsource_loc.rs?ref=1b38776c1f68c6fd47c1b2f7b7974efc7dd64901", "patch": "@@ -11,57 +11,40 @@\n use self::InternalDebugLocation::*;\n \n use super::utils::{debug_context, span_start};\n-use super::metadata::{UNKNOWN_COLUMN_NUMBER};\n-use super::{FunctionDebugContext, DebugLoc};\n+use super::metadata::UNKNOWN_COLUMN_NUMBER;\n+use super::FunctionDebugContext;\n \n use llvm;\n use llvm::debuginfo::DIScope;\n use builder::Builder;\n-use common::{CrateContext, FunctionContext};\n \n use libc::c_uint;\n use std::ptr;\n-use syntax_pos::Pos;\n+use syntax_pos::{Span, Pos};\n \n /// Sets the current debug location at the beginning of the span.\n ///\n /// Maps to a call to llvm::LLVMSetCurrentDebugLocation(...).\n-pub fn set_source_location(fcx: &FunctionContext,\n-                           builder: Option<&Builder>,\n-                           debug_loc: DebugLoc) {\n-    let builder = builder.map(|b| b.llbuilder);\n-    let function_debug_context = match fcx.debug_context {\n+pub fn set_source_location(\n+    debug_context: &FunctionDebugContext, builder: &Builder, scope: DIScope, span: Span\n+) {\n+    let function_debug_context = match *debug_context {\n         FunctionDebugContext::DebugInfoDisabled => return,\n         FunctionDebugContext::FunctionWithoutDebugInfo => {\n-            set_debug_location(fcx.ccx, builder, UnknownLocation);\n+            set_debug_location(builder, UnknownLocation);\n             return;\n         }\n-        FunctionDebugContext::RegularContext(box ref data) => data\n+        FunctionDebugContext::RegularContext(ref data) => data\n     };\n \n-    if function_debug_context.source_location_override.get() {\n-        // Just ignore any attempts to set a new debug location while\n-        // the override is active.\n-        return;\n-    }\n-\n     let dbg_loc = if function_debug_context.source_locations_enabled.get() {\n-        let (scope, span) = match debug_loc {\n-            DebugLoc::ScopeAt(scope, span) => (scope, span),\n-            DebugLoc::None => {\n-                set_debug_location(fcx.ccx, builder, UnknownLocation);\n-                return;\n-            }\n-        };\n-\n-        debug!(\"set_source_location: {}\",\n-               fcx.ccx.sess().codemap().span_to_string(span));\n-        let loc = span_start(fcx.ccx, span);\n+        debug!(\"set_source_location: {}\", builder.ccx.sess().codemap().span_to_string(span));\n+        let loc = span_start(builder.ccx, span);\n         InternalDebugLocation::new(scope, loc.line, loc.col.to_usize())\n     } else {\n         UnknownLocation\n     };\n-    set_debug_location(fcx.ccx, builder, dbg_loc);\n+    set_debug_location(builder, dbg_loc);\n }\n \n /// Enables emitting source locations for the given functions.\n@@ -70,9 +53,9 @@ pub fn set_source_location(fcx: &FunctionContext,\n /// they are disabled when beginning to translate a new function. This functions\n /// switches source location emitting on and must therefore be called before the\n /// first real statement/expression of the function is translated.\n-pub fn start_emitting_source_locations(fcx: &FunctionContext) {\n-    match fcx.debug_context {\n-        FunctionDebugContext::RegularContext(box ref data) => {\n+pub fn start_emitting_source_locations(dbg_context: &FunctionDebugContext) {\n+    match *dbg_context {\n+        FunctionDebugContext::RegularContext(ref data) => {\n             data.source_locations_enabled.set(true)\n         },\n         _ => { /* safe to ignore */ }\n@@ -96,15 +79,7 @@ impl InternalDebugLocation {\n     }\n }\n \n-pub fn set_debug_location(cx: &CrateContext,\n-                          builder: Option<llvm::BuilderRef>,\n-                          debug_location: InternalDebugLocation) {\n-    if builder.is_none() {\n-        if debug_location == debug_context(cx).current_debug_location.get() {\n-            return;\n-        }\n-    }\n-\n+pub fn set_debug_location(builder: &Builder, debug_location: InternalDebugLocation) {\n     let metadata_node = match debug_location {\n         KnownLocation { scope, line, .. } => {\n             // Always set the column to zero like Clang and GCC\n@@ -113,7 +88,7 @@ pub fn set_debug_location(cx: &CrateContext,\n \n             unsafe {\n                 llvm::LLVMRustDIBuilderCreateDebugLocation(\n-                    debug_context(cx).llcontext,\n+                    debug_context(builder.ccx).llcontext,\n                     line as c_uint,\n                     col as c_uint,\n                     scope,\n@@ -126,12 +101,7 @@ pub fn set_debug_location(cx: &CrateContext,\n         }\n     };\n \n-    if builder.is_none() {\n-        debug_context(cx).current_debug_location.set(debug_location);\n-    }\n-\n-    let builder = builder.unwrap_or_else(|| cx.raw_builder());\n     unsafe {\n-        llvm::LLVMSetCurrentDebugLocation(builder, metadata_node);\n+        llvm::LLVMSetCurrentDebugLocation(builder.llbuilder, metadata_node);\n     }\n }"}, {"sha": "5fb4a0e088f6293c26390662f45501519c89fdec", "filename": "src/librustc_trans/glue.rs", "status": "modified", "additions": 220, "deletions": 282, "changes": 502, "blob_url": "https://github.com/rust-lang/rust/blob/1b38776c1f68c6fd47c1b2f7b7974efc7dd64901/src%2Flibrustc_trans%2Fglue.rs", "raw_url": "https://github.com/rust-lang/rust/raw/1b38776c1f68c6fd47c1b2f7b7974efc7dd64901/src%2Flibrustc_trans%2Fglue.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_trans%2Fglue.rs?ref=1b38776c1f68c6fd47c1b2f7b7974efc7dd64901", "patch": "@@ -19,13 +19,11 @@ use llvm::{ValueRef, get_param};\n use middle::lang_items::ExchangeFreeFnLangItem;\n use rustc::ty::subst::{Substs};\n use rustc::traits;\n-use rustc::ty::{self, AdtKind, Ty, TyCtxt, TypeFoldable};\n+use rustc::ty::{self, AdtKind, Ty, TypeFoldable};\n use adt;\n use base::*;\n-use build::*;\n-use callee::{Callee};\n+use callee::Callee;\n use common::*;\n-use debuginfo::DebugLoc;\n use machine::*;\n use monomorphize;\n use trans_item::TransItem;\n@@ -34,69 +32,50 @@ use type_of::{type_of, sizing_type_of, align_of};\n use type_::Type;\n use value::Value;\n use Disr;\n+use cleanup::CleanupScope;\n \n-use arena::TypedArena;\n use syntax_pos::DUMMY_SP;\n \n-pub fn trans_exchange_free_dyn<'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n-                                           v: ValueRef,\n-                                           size: ValueRef,\n-                                           align: ValueRef,\n-                                           debug_loc: DebugLoc)\n-                                           -> Block<'blk, 'tcx> {\n-    let _icx = push_ctxt(\"trans_exchange_free\");\n-\n+pub fn trans_exchange_free_dyn<'a, 'tcx>(\n+    bcx: &BlockAndBuilder<'a, 'tcx>,\n+    v: ValueRef,\n+    size: ValueRef,\n+    align: ValueRef\n+) {\n     let def_id = langcall(bcx.tcx(), None, \"\", ExchangeFreeFnLangItem);\n-    let args = [PointerCast(bcx, v, Type::i8p(bcx.ccx())), size, align];\n-    Callee::def(bcx.ccx(), def_id, bcx.tcx().intern_substs(&[]))\n-        .call(bcx, debug_loc, &args, None).bcx\n-}\n+    let args = [bcx.pointercast(v, Type::i8p(bcx.ccx)), size, align];\n+    let callee = Callee::def(bcx.ccx, def_id, bcx.tcx().intern_substs(&[]));\n \n-pub fn trans_exchange_free<'blk, 'tcx>(cx: Block<'blk, 'tcx>,\n-                                       v: ValueRef,\n-                                       size: u64,\n-                                       align: u32,\n-                                       debug_loc: DebugLoc)\n-                                       -> Block<'blk, 'tcx> {\n-    trans_exchange_free_dyn(cx,\n-                            v,\n-                            C_uint(cx.ccx(), size),\n-                            C_uint(cx.ccx(), align),\n-                            debug_loc)\n+    let ccx = bcx.ccx;\n+    let fn_ty = callee.direct_fn_type(ccx, &[]);\n+\n+    let llret = bcx.call(callee.reify(ccx), &args[..], None);\n+    fn_ty.apply_attrs_callsite(llret);\n }\n \n-pub fn trans_exchange_free_ty<'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n-                                          ptr: ValueRef,\n-                                          content_ty: Ty<'tcx>,\n-                                          debug_loc: DebugLoc)\n-                                          -> Block<'blk, 'tcx> {\n-    assert!(type_is_sized(bcx.ccx().tcx(), content_ty));\n-    let sizing_type = sizing_type_of(bcx.ccx(), content_ty);\n-    let content_size = llsize_of_alloc(bcx.ccx(), sizing_type);\n+pub fn trans_exchange_free_ty<'a, 'tcx>(\n+    bcx: &BlockAndBuilder<'a, 'tcx>, ptr: ValueRef, content_ty: Ty<'tcx>\n+) {\n+    assert!(bcx.ccx.shared().type_is_sized(content_ty));\n+    let sizing_type = sizing_type_of(bcx.ccx, content_ty);\n+    let content_size = llsize_of_alloc(bcx.ccx, sizing_type);\n \n     // `Box<ZeroSizeType>` does not allocate.\n     if content_size != 0 {\n-        let content_align = align_of(bcx.ccx(), content_ty);\n-        trans_exchange_free(bcx, ptr, content_size, content_align, debug_loc)\n-    } else {\n-        bcx\n+        let content_align = align_of(bcx.ccx, content_ty);\n+        let ccx = bcx.ccx;\n+        trans_exchange_free_dyn(bcx, ptr, C_uint(ccx, content_size), C_uint(ccx, content_align));\n     }\n }\n \n-pub fn type_needs_drop<'a, 'tcx>(tcx: TyCtxt<'a, 'tcx, 'tcx>,\n-                                 ty: Ty<'tcx>) -> bool {\n-    tcx.type_needs_drop_given_env(ty, &tcx.empty_parameter_environment())\n-}\n-\n-pub fn get_drop_glue_type<'a, 'tcx>(tcx: TyCtxt<'a, 'tcx, 'tcx>,\n-                                    t: Ty<'tcx>) -> Ty<'tcx> {\n+pub fn get_drop_glue_type<'a, 'tcx>(scx: &SharedCrateContext<'a, 'tcx>, t: Ty<'tcx>) -> Ty<'tcx> {\n     assert!(t.is_normalized_for_trans());\n \n-    let t = tcx.erase_regions(&t);\n+    let t = scx.tcx().erase_regions(&t);\n \n     // Even if there is no dtor for t, there might be one deeper down and we\n     // might need to pass in the vtable ptr.\n-    if !type_is_sized(tcx, t) {\n+    if !scx.type_is_sized(t) {\n         return t;\n     }\n \n@@ -109,17 +88,16 @@ pub fn get_drop_glue_type<'a, 'tcx>(tcx: TyCtxt<'a, 'tcx, 'tcx>,\n     // returned `tcx.types.i8` does not appear unsound. The impact on\n     // code quality is unknown at this time.)\n \n-    if !type_needs_drop(tcx, t) {\n-        return tcx.types.i8;\n+    if !scx.type_needs_drop(t) {\n+        return scx.tcx().types.i8;\n     }\n     match t.sty {\n-        ty::TyBox(typ) if !type_needs_drop(tcx, typ)\n-                         && type_is_sized(tcx, typ) => {\n-            tcx.infer_ctxt(None, None, traits::Reveal::All).enter(|infcx| {\n+        ty::TyBox(typ) if !scx.type_needs_drop(typ) && scx.type_is_sized(typ) => {\n+            scx.tcx().infer_ctxt(None, None, traits::Reveal::All).enter(|infcx| {\n                 let layout = t.layout(&infcx).unwrap();\n-                if layout.size(&tcx.data_layout).bytes() == 0 {\n+                if layout.size(&scx.tcx().data_layout).bytes() == 0 {\n                     // `Box<ZeroSizeType>` does not allocate.\n-                    tcx.types.i8\n+                    scx.tcx().types.i8\n                 } else {\n                     t\n                 }\n@@ -129,56 +107,37 @@ pub fn get_drop_glue_type<'a, 'tcx>(tcx: TyCtxt<'a, 'tcx, 'tcx>,\n     }\n }\n \n-pub fn drop_ty<'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n-                           v: ValueRef,\n-                           t: Ty<'tcx>,\n-                           debug_loc: DebugLoc) -> Block<'blk, 'tcx> {\n-    drop_ty_core(bcx, v, t, debug_loc, false)\n+fn drop_ty<'a, 'tcx>(bcx: &BlockAndBuilder<'a, 'tcx>, v: ValueRef, t: Ty<'tcx>) {\n+    call_drop_glue(bcx, v, t, false, None)\n }\n \n-pub fn drop_ty_core<'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n-                                v: ValueRef,\n-                                t: Ty<'tcx>,\n-                                debug_loc: DebugLoc,\n-                                skip_dtor: bool)\n-                                -> Block<'blk, 'tcx> {\n+pub fn call_drop_glue<'a, 'tcx>(\n+    bcx: &BlockAndBuilder<'a, 'tcx>,\n+    v: ValueRef,\n+    t: Ty<'tcx>,\n+    skip_dtor: bool,\n+    funclet: Option<&'a Funclet>,\n+) {\n     // NB: v is an *alias* of type t here, not a direct value.\n-    debug!(\"drop_ty_core(t={:?}, skip_dtor={})\", t, skip_dtor);\n-    let _icx = push_ctxt(\"drop_ty\");\n-    if bcx.fcx.type_needs_drop(t) {\n-        let ccx = bcx.ccx();\n+    debug!(\"call_drop_glue(t={:?}, skip_dtor={})\", t, skip_dtor);\n+    if bcx.ccx.shared().type_needs_drop(t) {\n+        let ccx = bcx.ccx;\n         let g = if skip_dtor {\n             DropGlueKind::TyContents(t)\n         } else {\n             DropGlueKind::Ty(t)\n         };\n         let glue = get_drop_glue_core(ccx, g);\n-        let glue_type = get_drop_glue_type(ccx.tcx(), t);\n+        let glue_type = get_drop_glue_type(ccx.shared(), t);\n         let ptr = if glue_type != t {\n-            PointerCast(bcx, v, type_of(ccx, glue_type).ptr_to())\n+            bcx.pointercast(v, type_of(ccx, glue_type).ptr_to())\n         } else {\n             v\n         };\n \n         // No drop-hint ==> call standard drop glue\n-        Call(bcx, glue, &[ptr], debug_loc);\n+        bcx.call(glue, &[ptr], funclet.map(|b| b.bundle()));\n     }\n-    bcx\n-}\n-\n-pub fn drop_ty_immediate<'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n-                                     v: ValueRef,\n-                                     t: Ty<'tcx>,\n-                                     debug_loc: DebugLoc,\n-                                     skip_dtor: bool)\n-                                     -> Block<'blk, 'tcx> {\n-    let _icx = push_ctxt(\"drop_ty_immediate\");\n-    let vp = alloc_ty(bcx, t, \"\");\n-    call_lifetime_start(bcx, vp);\n-    store_ty(bcx, v, vp, t);\n-    let bcx = drop_ty_core(bcx, vp, t, debug_loc, skip_dtor);\n-    call_lifetime_end(bcx, vp);\n-    bcx\n }\n \n pub fn get_drop_glue<'a, 'tcx>(ccx: &CrateContext<'a, 'tcx>, t: Ty<'tcx>) -> ValueRef {\n@@ -212,9 +171,8 @@ impl<'tcx> DropGlueKind<'tcx> {\n     }\n }\n \n-fn get_drop_glue_core<'a, 'tcx>(ccx: &CrateContext<'a, 'tcx>,\n-                                g: DropGlueKind<'tcx>) -> ValueRef {\n-    let g = g.map_ty(|t| get_drop_glue_type(ccx.tcx(), t));\n+fn get_drop_glue_core<'a, 'tcx>(ccx: &CrateContext<'a, 'tcx>, g: DropGlueKind<'tcx>) -> ValueRef {\n+    let g = g.map_ty(|t| get_drop_glue_type(ccx.shared(), t));\n     match ccx.drop_glues().borrow().get(&g) {\n         Some(&(glue, _)) => glue,\n         None => {\n@@ -226,17 +184,12 @@ fn get_drop_glue_core<'a, 'tcx>(ccx: &CrateContext<'a, 'tcx>,\n     }\n }\n \n-pub fn implement_drop_glue<'a, 'tcx>(ccx: &CrateContext<'a, 'tcx>,\n-                                     g: DropGlueKind<'tcx>) {\n-    let tcx = ccx.tcx();\n-    assert_eq!(g.ty(), get_drop_glue_type(tcx, g.ty()));\n-    let (llfn, fn_ty) = ccx.drop_glues().borrow().get(&g).unwrap().clone();\n-\n-    let (arena, fcx): (TypedArena<_>, FunctionContext);\n-    arena = TypedArena::new();\n-    fcx = FunctionContext::new(ccx, llfn, fn_ty, None, &arena);\n+pub fn implement_drop_glue<'a, 'tcx>(ccx: &CrateContext<'a, 'tcx>, g: DropGlueKind<'tcx>) {\n+    assert_eq!(g.ty(), get_drop_glue_type(ccx.shared(), g.ty()));\n+    let (llfn, _) = ccx.drop_glues().borrow().get(&g).unwrap().clone();\n \n-    let bcx = fcx.init(false);\n+    let fcx = FunctionContext::new(ccx, llfn);\n+    let bcx = fcx.get_entry_block();\n \n     ccx.stats().n_glues_created.set(ccx.stats().n_glues_created.get() + 1);\n     // All glue functions take values passed *by alias*; this is a\n@@ -247,19 +200,91 @@ pub fn implement_drop_glue<'a, 'tcx>(ccx: &CrateContext<'a, 'tcx>,\n     // llfn is expected be declared to take a parameter of the appropriate\n     // type, so we don't need to explicitly cast the function parameter.\n \n-    let bcx = make_drop_glue(bcx, get_param(llfn, 0), g);\n-    fcx.finish(bcx, DebugLoc::None);\n+    // NB: v0 is an *alias* of type t here, not a direct value.\n+    // Only drop the value when it ... well, we used to check for\n+    // non-null, (and maybe we need to continue doing so), but we now\n+    // must definitely check for special bit-patterns corresponding to\n+    // the special dtor markings.\n+    let v0 = get_param(llfn, 0);\n+    let t = g.ty();\n+\n+    let skip_dtor = match g {\n+        DropGlueKind::Ty(_) => false,\n+        DropGlueKind::TyContents(_) => true\n+    };\n+\n+    let bcx = match t.sty {\n+        ty::TyBox(content_ty) => {\n+            // Support for TyBox is built-in and its drop glue is\n+            // special. It may move to library and have Drop impl. As\n+            // a safe-guard, assert TyBox not used with TyContents.\n+            assert!(!skip_dtor);\n+            if !bcx.ccx.shared().type_is_sized(content_ty) {\n+                let llval = get_dataptr(&bcx, v0);\n+                let llbox = bcx.load(llval);\n+                drop_ty(&bcx, v0, content_ty);\n+                // FIXME(#36457) -- we should pass unsized values to drop glue as two arguments\n+                let info = get_meta(&bcx, v0);\n+                let info = bcx.load(info);\n+                let (llsize, llalign) = size_and_align_of_dst(&bcx, content_ty, info);\n+\n+                // `Box<ZeroSizeType>` does not allocate.\n+                let needs_free = bcx.icmp(llvm::IntNE, llsize, C_uint(bcx.ccx, 0u64));\n+                if const_to_opt_uint(needs_free) == Some(0) {\n+                    bcx\n+                } else {\n+                    let next_cx = bcx.fcx().build_new_block(\"next\");\n+                    let cond_cx = bcx.fcx().build_new_block(\"cond\");\n+                    bcx.cond_br(needs_free, cond_cx.llbb(), next_cx.llbb());\n+                    trans_exchange_free_dyn(&cond_cx, llbox, llsize, llalign);\n+                    cond_cx.br(next_cx.llbb());\n+                    next_cx\n+                }\n+            } else {\n+                let llval = v0;\n+                let llbox = bcx.load(llval);\n+                drop_ty(&bcx, llbox, content_ty);\n+                trans_exchange_free_ty(&bcx, llbox, content_ty);\n+                bcx\n+            }\n+        }\n+        ty::TyDynamic(..) => {\n+            // No support in vtable for distinguishing destroying with\n+            // versus without calling Drop::drop. Assert caller is\n+            // okay with always calling the Drop impl, if any.\n+            // FIXME(#36457) -- we should pass unsized values to drop glue as two arguments\n+            assert!(!skip_dtor);\n+            let data_ptr = get_dataptr(&bcx, v0);\n+            let vtable_ptr = bcx.load(get_meta(&bcx, v0));\n+            let dtor = bcx.load(vtable_ptr);\n+            bcx.call(dtor, &[bcx.pointercast(bcx.load(data_ptr), Type::i8p(bcx.ccx))], None);\n+            bcx\n+        }\n+        ty::TyAdt(def, ..) if def.dtor_kind().is_present() && !skip_dtor => {\n+            trans_custom_dtor(bcx, t, v0, def.is_union())\n+        }\n+        ty::TyAdt(def, ..) if def.is_union() => {\n+            bcx\n+        }\n+        _ => {\n+            if bcx.ccx.shared().type_needs_drop(t) {\n+                drop_structural_ty(bcx, v0, t)\n+            } else {\n+                bcx\n+            }\n+        }\n+    };\n+    bcx.ret_void();\n }\n \n-fn trans_custom_dtor<'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n-                                 t: Ty<'tcx>,\n-                                 v0: ValueRef,\n-                                 shallow_drop: bool)\n-                                 -> Block<'blk, 'tcx>\n+fn trans_custom_dtor<'a, 'tcx>(mut bcx: BlockAndBuilder<'a, 'tcx>,\n+                               t: Ty<'tcx>,\n+                               v0: ValueRef,\n+                               shallow_drop: bool)\n+                               -> BlockAndBuilder<'a, 'tcx>\n {\n     debug!(\"trans_custom_dtor t: {}\", t);\n     let tcx = bcx.tcx();\n-    let mut bcx = bcx;\n \n     let def = t.ty_adt_def().unwrap();\n \n@@ -269,23 +294,23 @@ fn trans_custom_dtor<'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n     //\n     // FIXME (#14875) panic-in-drop semantics might be unsupported; we\n     // might well consider changing below to more direct code.\n-    let contents_scope = bcx.fcx.push_custom_cleanup_scope();\n-\n     // Issue #23611: schedule cleanup of contents, re-inspecting the\n     // discriminant (if any) in case of variant swap in drop code.\n-    if !shallow_drop {\n-        bcx.fcx.schedule_drop_adt_contents(contents_scope, v0, t);\n-    }\n+    let contents_scope = if !shallow_drop {\n+        bcx.fcx().schedule_drop_adt_contents(v0, t)\n+    } else {\n+        CleanupScope::noop()\n+    };\n \n     let (sized_args, unsized_args);\n-    let args: &[ValueRef] = if type_is_sized(tcx, t) {\n+    let args: &[ValueRef] = if bcx.ccx.shared().type_is_sized(t) {\n         sized_args = [v0];\n         &sized_args\n     } else {\n         // FIXME(#36457) -- we should pass unsized values to drop glue as two arguments\n         unsized_args = [\n-            Load(bcx, get_dataptr(bcx, v0)),\n-            Load(bcx, get_meta(bcx, v0))\n+            bcx.load(get_dataptr(&bcx, v0)),\n+            bcx.load(get_meta(&bcx, v0))\n         ];\n         &unsized_args\n     };\n@@ -294,39 +319,44 @@ fn trans_custom_dtor<'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n         def_id: tcx.lang_items.drop_trait().unwrap(),\n         substs: tcx.mk_substs_trait(t, &[])\n     });\n-    let vtbl = match fulfill_obligation(bcx.ccx().shared(), DUMMY_SP, trait_ref) {\n+    let vtbl = match fulfill_obligation(bcx.ccx.shared(), DUMMY_SP, trait_ref) {\n         traits::VtableImpl(data) => data,\n         _ => bug!(\"dtor for {:?} is not an impl???\", t)\n     };\n     let dtor_did = def.destructor().unwrap();\n-    bcx = Callee::def(bcx.ccx(), dtor_did, vtbl.substs)\n-        .call(bcx, DebugLoc::None, args, None).bcx;\n-\n-    bcx.fcx.pop_and_trans_custom_cleanup_scope(bcx, contents_scope)\n+    let callee = Callee::def(bcx.ccx, dtor_did, vtbl.substs);\n+    let fn_ty = callee.direct_fn_type(bcx.ccx, &[]);\n+    let llret;\n+    if let Some(landing_pad) = contents_scope.landing_pad {\n+        let normal_bcx = bcx.fcx().build_new_block(\"normal-return\");\n+        llret = bcx.invoke(callee.reify(bcx.ccx), args, normal_bcx.llbb(), landing_pad, None);\n+        bcx = normal_bcx;\n+    } else {\n+        llret = bcx.call(callee.reify(bcx.ccx), args, None);\n+    }\n+    fn_ty.apply_attrs_callsite(llret);\n+    contents_scope.trans(&bcx);\n+    bcx\n }\n \n-pub fn size_and_align_of_dst<'blk, 'tcx>(bcx: &BlockAndBuilder<'blk, 'tcx>,\n-                                         t: Ty<'tcx>, info: ValueRef)\n-                                         -> (ValueRef, ValueRef) {\n+pub fn size_and_align_of_dst<'a, 'tcx>(bcx: &BlockAndBuilder<'a, 'tcx>,\n+                                       t: Ty<'tcx>, info: ValueRef)\n+                                       -> (ValueRef, ValueRef) {\n     debug!(\"calculate size of DST: {}; with lost info: {:?}\",\n            t, Value(info));\n-    if type_is_sized(bcx.tcx(), t) {\n-        let sizing_type = sizing_type_of(bcx.ccx(), t);\n-        let size = llsize_of_alloc(bcx.ccx(), sizing_type);\n-        let align = align_of(bcx.ccx(), t);\n+    if bcx.ccx.shared().type_is_sized(t) {\n+        let sizing_type = sizing_type_of(bcx.ccx, t);\n+        let size = llsize_of_alloc(bcx.ccx, sizing_type);\n+        let align = align_of(bcx.ccx, t);\n         debug!(\"size_and_align_of_dst t={} info={:?} size: {} align: {}\",\n                t, Value(info), size, align);\n-        let size = C_uint(bcx.ccx(), size);\n-        let align = C_uint(bcx.ccx(), align);\n+        let size = C_uint(bcx.ccx, size);\n+        let align = C_uint(bcx.ccx, align);\n         return (size, align);\n     }\n-    if bcx.is_unreachable() {\n-        let llty = Type::int(bcx.ccx());\n-        return (C_undef(llty), C_undef(llty));\n-    }\n     match t.sty {\n         ty::TyAdt(def, substs) => {\n-            let ccx = bcx.ccx();\n+            let ccx = bcx.ccx;\n             // First get the size of all statically known fields.\n             // Don't use type_of::sizing_type_of because that expects t to be sized,\n             // and it also rounds up to alignment, which we want to avoid,\n@@ -389,15 +419,15 @@ pub fn size_and_align_of_dst<'blk, 'tcx>(bcx: &BlockAndBuilder<'blk, 'tcx>,\n             //\n             //   `(size + (align-1)) & -align`\n \n-            let addend = bcx.sub(align, C_uint(bcx.ccx(), 1_u64));\n+            let addend = bcx.sub(align, C_uint(bcx.ccx, 1_u64));\n             let size = bcx.and(bcx.add(size, addend), bcx.neg(align));\n \n             (size, align)\n         }\n         ty::TyDynamic(..) => {\n             // info points to the vtable and the second entry in the vtable is the\n             // dynamic size of the object.\n-            let info = bcx.pointercast(info, Type::int(bcx.ccx()).ptr_to());\n+            let info = bcx.pointercast(info, Type::int(bcx.ccx).ptr_to());\n             let size_ptr = bcx.gepi(info, &[1]);\n             let align_ptr = bcx.gepi(info, &[2]);\n             (bcx.load(size_ptr), bcx.load(align_ptr))\n@@ -406,194 +436,106 @@ pub fn size_and_align_of_dst<'blk, 'tcx>(bcx: &BlockAndBuilder<'blk, 'tcx>,\n             let unit_ty = t.sequence_element_type(bcx.tcx());\n             // The info in this case is the length of the str, so the size is that\n             // times the unit size.\n-            let llunit_ty = sizing_type_of(bcx.ccx(), unit_ty);\n-            let unit_align = llalign_of_min(bcx.ccx(), llunit_ty);\n-            let unit_size = llsize_of_alloc(bcx.ccx(), llunit_ty);\n-            (bcx.mul(info, C_uint(bcx.ccx(), unit_size)),\n-             C_uint(bcx.ccx(), unit_align))\n+            let llunit_ty = sizing_type_of(bcx.ccx, unit_ty);\n+            let unit_align = llalign_of_min(bcx.ccx, llunit_ty);\n+            let unit_size = llsize_of_alloc(bcx.ccx, llunit_ty);\n+            (bcx.mul(info, C_uint(bcx.ccx, unit_size)),\n+             C_uint(bcx.ccx, unit_align))\n         }\n         _ => bug!(\"Unexpected unsized type, found {}\", t)\n     }\n }\n \n-fn make_drop_glue<'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n-                              v0: ValueRef,\n-                              g: DropGlueKind<'tcx>)\n-                              -> Block<'blk, 'tcx> {\n-    let t = g.ty();\n-\n-    let skip_dtor = match g { DropGlueKind::Ty(_) => false, DropGlueKind::TyContents(_) => true };\n-    // NB: v0 is an *alias* of type t here, not a direct value.\n-    let _icx = push_ctxt(\"make_drop_glue\");\n-\n-    // Only drop the value when it ... well, we used to check for\n-    // non-null, (and maybe we need to continue doing so), but we now\n-    // must definitely check for special bit-patterns corresponding to\n-    // the special dtor markings.\n-\n-    match t.sty {\n-        ty::TyBox(content_ty) => {\n-            // Support for TyBox is built-in and its drop glue is\n-            // special. It may move to library and have Drop impl. As\n-            // a safe-guard, assert TyBox not used with TyContents.\n-            assert!(!skip_dtor);\n-            if !type_is_sized(bcx.tcx(), content_ty) {\n-                let llval = get_dataptr(bcx, v0);\n-                let llbox = Load(bcx, llval);\n-                let bcx = drop_ty(bcx, v0, content_ty, DebugLoc::None);\n-                // FIXME(#36457) -- we should pass unsized values to drop glue as two arguments\n-                let info = get_meta(bcx, v0);\n-                let info = Load(bcx, info);\n-                let (llsize, llalign) =\n-                    size_and_align_of_dst(&bcx.build(), content_ty, info);\n-\n-                // `Box<ZeroSizeType>` does not allocate.\n-                let needs_free = ICmp(bcx,\n-                                        llvm::IntNE,\n-                                        llsize,\n-                                        C_uint(bcx.ccx(), 0u64),\n-                                        DebugLoc::None);\n-                with_cond(bcx, needs_free, |bcx| {\n-                    trans_exchange_free_dyn(bcx, llbox, llsize, llalign, DebugLoc::None)\n-                })\n-            } else {\n-                let llval = v0;\n-                let llbox = Load(bcx, llval);\n-                let bcx = drop_ty(bcx, llbox, content_ty, DebugLoc::None);\n-                trans_exchange_free_ty(bcx, llbox, content_ty, DebugLoc::None)\n-            }\n-        }\n-        ty::TyDynamic(..) => {\n-            // No support in vtable for distinguishing destroying with\n-            // versus without calling Drop::drop. Assert caller is\n-            // okay with always calling the Drop impl, if any.\n-            // FIXME(#36457) -- we should pass unsized values to drop glue as two arguments\n-            assert!(!skip_dtor);\n-            let data_ptr = get_dataptr(bcx, v0);\n-            let vtable_ptr = Load(bcx, get_meta(bcx, v0));\n-            let dtor = Load(bcx, vtable_ptr);\n-            Call(bcx,\n-                 dtor,\n-                 &[PointerCast(bcx, Load(bcx, data_ptr), Type::i8p(bcx.ccx()))],\n-                 DebugLoc::None);\n-            bcx\n-        }\n-        ty::TyAdt(def, ..) if def.dtor_kind().is_present() && !skip_dtor => {\n-            trans_custom_dtor(bcx, t, v0, def.is_union())\n-        }\n-        ty::TyAdt(def, ..) if def.is_union() => {\n-            bcx\n-        }\n-        _ => {\n-            if bcx.fcx.type_needs_drop(t) {\n-                drop_structural_ty(bcx, v0, t)\n-            } else {\n-                bcx\n-            }\n-        }\n-    }\n-}\n-\n // Iterates through the elements of a structural type, dropping them.\n-fn drop_structural_ty<'blk, 'tcx>(cx: Block<'blk, 'tcx>,\n-                                  av: ValueRef,\n-                                  t: Ty<'tcx>)\n-                                  -> Block<'blk, 'tcx> {\n-    let _icx = push_ctxt(\"drop_structural_ty\");\n-\n-    fn iter_variant<'blk, 'tcx>(cx: Block<'blk, 'tcx>,\n-                                t: Ty<'tcx>,\n-                                av: adt::MaybeSizedValue,\n-                                variant: &'tcx ty::VariantDef,\n-                                substs: &Substs<'tcx>)\n-                                -> Block<'blk, 'tcx> {\n-        let _icx = push_ctxt(\"iter_variant\");\n+fn drop_structural_ty<'a, 'tcx>(cx: BlockAndBuilder<'a, 'tcx>,\n+                                av: ValueRef,\n+                                t: Ty<'tcx>)\n+                                -> BlockAndBuilder<'a, 'tcx> {\n+    fn iter_variant<'a, 'tcx>(cx: &BlockAndBuilder<'a, 'tcx>,\n+                              t: Ty<'tcx>,\n+                              av: adt::MaybeSizedValue,\n+                              variant: &'tcx ty::VariantDef,\n+                              substs: &Substs<'tcx>) {\n         let tcx = cx.tcx();\n-        let mut cx = cx;\n-\n         for (i, field) in variant.fields.iter().enumerate() {\n             let arg = monomorphize::field_ty(tcx, substs, field);\n-            cx = drop_ty(cx,\n-                         adt::trans_field_ptr(cx, t, av, Disr::from(variant.disr_val), i),\n-                         arg, DebugLoc::None);\n+            let field_ptr = adt::trans_field_ptr(&cx, t, av, Disr::from(variant.disr_val), i);\n+            drop_ty(&cx, field_ptr, arg);\n         }\n-        return cx;\n     }\n \n-    let value = if type_is_sized(cx.tcx(), t) {\n+    let value = if cx.ccx.shared().type_is_sized(t) {\n         adt::MaybeSizedValue::sized(av)\n     } else {\n         // FIXME(#36457) -- we should pass unsized values as two arguments\n-        let data = Load(cx, get_dataptr(cx, av));\n-        let info = Load(cx, get_meta(cx, av));\n+        let data = cx.load(get_dataptr(&cx, av));\n+        let info = cx.load(get_meta(&cx, av));\n         adt::MaybeSizedValue::unsized_(data, info)\n     };\n \n     let mut cx = cx;\n     match t.sty {\n         ty::TyClosure(def_id, substs) => {\n             for (i, upvar_ty) in substs.upvar_tys(def_id, cx.tcx()).enumerate() {\n-                let llupvar = adt::trans_field_ptr(cx, t, value, Disr(0), i);\n-                cx = drop_ty(cx, llupvar, upvar_ty, DebugLoc::None);\n+                let llupvar = adt::trans_field_ptr(&cx, t, value, Disr(0), i);\n+                drop_ty(&cx, llupvar, upvar_ty);\n             }\n         }\n         ty::TyArray(_, n) => {\n-            let base = get_dataptr(cx, value.value);\n-            let len = C_uint(cx.ccx(), n);\n+            let base = get_dataptr(&cx, value.value);\n+            let len = C_uint(cx.ccx, n);\n             let unit_ty = t.sequence_element_type(cx.tcx());\n-            cx = tvec::slice_for_each(cx, base, unit_ty, len,\n-                |bb, vv| drop_ty(bb, vv, unit_ty, DebugLoc::None));\n+            cx = tvec::slice_for_each(&cx, base, unit_ty, len, |bb, vv| drop_ty(bb, vv, unit_ty));\n         }\n         ty::TySlice(_) | ty::TyStr => {\n             let unit_ty = t.sequence_element_type(cx.tcx());\n-            cx = tvec::slice_for_each(cx, value.value, unit_ty, value.meta,\n-                |bb, vv| drop_ty(bb, vv, unit_ty, DebugLoc::None));\n+            cx = tvec::slice_for_each(&cx, value.value, unit_ty, value.meta,\n+                |bb, vv| drop_ty(bb, vv, unit_ty));\n         }\n         ty::TyTuple(ref args) => {\n             for (i, arg) in args.iter().enumerate() {\n-                let llfld_a = adt::trans_field_ptr(cx, t, value, Disr(0), i);\n-                cx = drop_ty(cx, llfld_a, *arg, DebugLoc::None);\n+                let llfld_a = adt::trans_field_ptr(&cx, t, value, Disr(0), i);\n+                drop_ty(&cx, llfld_a, *arg);\n             }\n         }\n         ty::TyAdt(adt, substs) => match adt.adt_kind() {\n             AdtKind::Struct => {\n                 let VariantInfo { fields, discr } = VariantInfo::from_ty(cx.tcx(), t, None);\n                 for (i, &Field(_, field_ty)) in fields.iter().enumerate() {\n-                    let llfld_a = adt::trans_field_ptr(cx, t, value, Disr::from(discr), i);\n+                    let llfld_a = adt::trans_field_ptr(&cx, t, value, Disr::from(discr), i);\n \n-                    let val = if type_is_sized(cx.tcx(), field_ty) {\n+                    let val = if cx.ccx.shared().type_is_sized(field_ty) {\n                         llfld_a\n                     } else {\n                         // FIXME(#36457) -- we should pass unsized values as two arguments\n-                        let scratch = alloc_ty(cx, field_ty, \"__fat_ptr_iter\");\n-                        Store(cx, llfld_a, get_dataptr(cx, scratch));\n-                        Store(cx, value.meta, get_meta(cx, scratch));\n+                        let scratch = alloc_ty(&cx, field_ty, \"__fat_ptr_iter\");\n+                        cx.store(llfld_a, get_dataptr(&cx, scratch));\n+                        cx.store(value.meta, get_meta(&cx, scratch));\n                         scratch\n                     };\n-                    cx = drop_ty(cx, val, field_ty, DebugLoc::None);\n+                    drop_ty(&cx, val, field_ty);\n                 }\n             }\n             AdtKind::Union => {\n                 bug!(\"Union in `glue::drop_structural_ty`\");\n             }\n             AdtKind::Enum => {\n-                let fcx = cx.fcx;\n-                let ccx = fcx.ccx;\n                 let n_variants = adt.variants.len();\n \n                 // NB: we must hit the discriminant first so that structural\n                 // comparison know not to proceed when the discriminants differ.\n \n-                match adt::trans_switch(cx, t, av, false) {\n+                match adt::trans_switch(&cx, t, av, false) {\n                     (adt::BranchKind::Single, None) => {\n                         if n_variants != 0 {\n                             assert!(n_variants == 1);\n-                            cx = iter_variant(cx, t, adt::MaybeSizedValue::sized(av),\n+                            iter_variant(&cx, t, adt::MaybeSizedValue::sized(av),\n                                             &adt.variants[0], substs);\n                         }\n                     }\n                     (adt::BranchKind::Switch, Some(lldiscrim_a)) => {\n-                        cx = drop_ty(cx, lldiscrim_a, cx.tcx().types.isize, DebugLoc::None);\n+                        let tcx = cx.tcx();\n+                        drop_ty(&cx, lldiscrim_a, tcx.types.isize);\n \n                         // Create a fall-through basic block for the \"else\" case of\n                         // the switch instruction we're about to generate. Note that\n@@ -608,27 +550,23 @@ fn drop_structural_ty<'blk, 'tcx>(cx: Block<'blk, 'tcx>,\n                         // from the outer function, and any other use case will only\n                         // call this for an already-valid enum in which case the `ret\n                         // void` will never be hit.\n-                        let ret_void_cx = fcx.new_block(\"enum-iter-ret-void\");\n-                        RetVoid(ret_void_cx, DebugLoc::None);\n-                        let llswitch = Switch(cx, lldiscrim_a, ret_void_cx.llbb, n_variants);\n-                        let next_cx = fcx.new_block(\"enum-iter-next\");\n+                        let ret_void_cx = cx.fcx().build_new_block(\"enum-iter-ret-void\");\n+                        ret_void_cx.ret_void();\n+                        let llswitch = cx.switch(lldiscrim_a, ret_void_cx.llbb(), n_variants);\n+                        let next_cx = cx.fcx().build_new_block(\"enum-iter-next\");\n \n                         for variant in &adt.variants {\n-                            let variant_cx = fcx.new_block(&format!(\"enum-iter-variant-{}\",\n-                                                                        &variant.disr_val\n-                                                                                .to_string()));\n-                            let case_val = adt::trans_case(cx, t, Disr::from(variant.disr_val));\n-                            AddCase(llswitch, case_val, variant_cx.llbb);\n-                            let variant_cx = iter_variant(variant_cx,\n-                                                        t,\n-                                                        value,\n-                                                        variant,\n-                                                        substs);\n-                            Br(variant_cx, next_cx.llbb, DebugLoc::None);\n+                            let variant_cx_name = format!(\"enum-iter-variant-{}\",\n+                                &variant.disr_val.to_string());\n+                            let variant_cx = cx.fcx().build_new_block(&variant_cx_name);\n+                            let case_val = adt::trans_case(&cx, t, Disr::from(variant.disr_val));\n+                            variant_cx.add_case(llswitch, case_val, variant_cx.llbb());\n+                            iter_variant(&variant_cx, t, value, variant, substs);\n+                            variant_cx.br(next_cx.llbb());\n                         }\n                         cx = next_cx;\n                     }\n-                    _ => ccx.sess().unimpl(\"value from adt::trans_switch in drop_structural_ty\"),\n+                    _ => cx.ccx.sess().unimpl(\"value from adt::trans_switch in drop_structural_ty\"),\n                 }\n             }\n         },"}, {"sha": "b7116ba1f338baadec8f4abebeabe3e0ea588945", "filename": "src/librustc_trans/intrinsic.rs", "status": "modified", "additions": 290, "deletions": 432, "changes": 722, "blob_url": "https://github.com/rust-lang/rust/blob/1b38776c1f68c6fd47c1b2f7b7974efc7dd64901/src%2Flibrustc_trans%2Fintrinsic.rs", "raw_url": "https://github.com/rust-lang/rust/raw/1b38776c1f68c6fd47c1b2f7b7974efc7dd64901/src%2Flibrustc_trans%2Fintrinsic.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_trans%2Fintrinsic.rs?ref=1b38776c1f68c6fd47c1b2f7b7974efc7dd64901", "patch": "@@ -10,17 +10,14 @@\n \n #![allow(non_upper_case_globals)]\n \n-use arena::TypedArena;\n use intrinsics::{self, Intrinsic};\n use libc;\n use llvm;\n use llvm::{ValueRef};\n use abi::{Abi, FnType};\n use adt;\n use base::*;\n-use build::*;\n use common::*;\n-use debuginfo::DebugLoc;\n use declare;\n use glue;\n use type_of;\n@@ -33,7 +30,7 @@ use syntax::ast;\n use syntax::symbol::Symbol;\n \n use rustc::session::Session;\n-use syntax_pos::{Span, DUMMY_SP};\n+use syntax_pos::Span;\n \n use std::cmp::Ordering;\n use std::iter;\n@@ -79,6 +76,7 @@ fn get_simple_intrinsic(ccx: &CrateContext, name: &str) -> Option<ValueRef> {\n         \"roundf32\" => \"llvm.round.f32\",\n         \"roundf64\" => \"llvm.round.f64\",\n         \"assume\" => \"llvm.assume\",\n+        \"abort\" => \"llvm.trap\",\n         _ => return None\n     };\n     Some(ccx.get_intrinsic(&llvm_name))\n@@ -87,19 +85,15 @@ fn get_simple_intrinsic(ccx: &CrateContext, name: &str) -> Option<ValueRef> {\n /// Remember to add all intrinsics here, in librustc_typeck/check/mod.rs,\n /// and in libcore/intrinsics.rs; if you need access to any llvm intrinsics,\n /// add them to librustc_trans/trans/context.rs\n-pub fn trans_intrinsic_call<'a, 'blk, 'tcx>(mut bcx: Block<'blk, 'tcx>,\n-                                            callee_ty: Ty<'tcx>,\n-                                            fn_ty: &FnType,\n-                                            llargs: &[ValueRef],\n-                                            llresult: ValueRef,\n-                                            call_debug_location: DebugLoc)\n-                                            -> Result<'blk, 'tcx> {\n-    let fcx = bcx.fcx;\n-    let ccx = fcx.ccx;\n+pub fn trans_intrinsic_call<'a, 'tcx>(bcx: &BlockAndBuilder<'a, 'tcx>,\n+                                      callee_ty: Ty<'tcx>,\n+                                      fn_ty: &FnType,\n+                                      llargs: &[ValueRef],\n+                                      llresult: ValueRef,\n+                                      span: Span) {\n+    let ccx = bcx.ccx;\n     let tcx = bcx.tcx();\n \n-    let _icx = push_ctxt(\"trans_intrinsic_call\");\n-\n     let (def_id, substs, fty) = match callee_ty.sty {\n         ty::TyFnDef(def_id, substs, ref fty) => (def_id, substs, fty),\n         _ => bug!(\"expected fn item type, found {}\", callee_ty)\n@@ -110,273 +104,209 @@ pub fn trans_intrinsic_call<'a, 'blk, 'tcx>(mut bcx: Block<'blk, 'tcx>,\n     let ret_ty = sig.output();\n     let name = &*tcx.item_name(def_id).as_str();\n \n-    let span = match call_debug_location {\n-        DebugLoc::ScopeAt(_, span) => span,\n-        DebugLoc::None => {\n-            span_bug!(fcx.span.unwrap_or(DUMMY_SP),\n-                      \"intrinsic `{}` called with missing span\", name);\n-        }\n-    };\n-\n-    // These are the only intrinsic functions that diverge.\n-    if name == \"abort\" {\n-        let llfn = ccx.get_intrinsic(&(\"llvm.trap\"));\n-        Call(bcx, llfn, &[], call_debug_location);\n-        Unreachable(bcx);\n-        return Result::new(bcx, C_undef(Type::nil(ccx).ptr_to()));\n-    } else if name == \"unreachable\" {\n-        Unreachable(bcx);\n-        return Result::new(bcx, C_nil(ccx));\n-    }\n-\n     let llret_ty = type_of::type_of(ccx, ret_ty);\n \n     let simple = get_simple_intrinsic(ccx, name);\n-    let llval = match (simple, name) {\n-        (Some(llfn), _) => {\n-            Call(bcx, llfn, &llargs, call_debug_location)\n+    let llval = match name {\n+        _ if simple.is_some() => {\n+            bcx.call(simple.unwrap(), &llargs, None)\n         }\n-        (_, \"likely\") => {\n+        \"unreachable\" => {\n+            return;\n+        },\n+        \"likely\" => {\n             let expect = ccx.get_intrinsic(&(\"llvm.expect.i1\"));\n-            Call(bcx, expect, &[llargs[0], C_bool(ccx, true)], call_debug_location)\n+            bcx.call(expect, &[llargs[0], C_bool(ccx, true)], None)\n         }\n-        (_, \"unlikely\") => {\n+        \"unlikely\" => {\n             let expect = ccx.get_intrinsic(&(\"llvm.expect.i1\"));\n-            Call(bcx, expect, &[llargs[0], C_bool(ccx, false)], call_debug_location)\n+            bcx.call(expect, &[llargs[0], C_bool(ccx, false)], None)\n         }\n-        (_, \"try\") => {\n-            bcx = try_intrinsic(bcx, llargs[0], llargs[1], llargs[2], llresult,\n-                                call_debug_location);\n+        \"try\" => {\n+            try_intrinsic(bcx, llargs[0], llargs[1], llargs[2], llresult);\n             C_nil(ccx)\n         }\n-        (_, \"breakpoint\") => {\n+        \"breakpoint\" => {\n             let llfn = ccx.get_intrinsic(&(\"llvm.debugtrap\"));\n-            Call(bcx, llfn, &[], call_debug_location)\n+            bcx.call(llfn, &[], None)\n         }\n-        (_, \"size_of\") => {\n+        \"size_of\" => {\n             let tp_ty = substs.type_at(0);\n             let lltp_ty = type_of::type_of(ccx, tp_ty);\n             C_uint(ccx, machine::llsize_of_alloc(ccx, lltp_ty))\n         }\n-        (_, \"size_of_val\") => {\n+        \"size_of_val\" => {\n             let tp_ty = substs.type_at(0);\n-            if !type_is_sized(tcx, tp_ty) {\n+            if !bcx.ccx.shared().type_is_sized(tp_ty) {\n                 let (llsize, _) =\n-                    glue::size_and_align_of_dst(&bcx.build(), tp_ty, llargs[1]);\n+                    glue::size_and_align_of_dst(bcx, tp_ty, llargs[1]);\n                 llsize\n             } else {\n                 let lltp_ty = type_of::type_of(ccx, tp_ty);\n                 C_uint(ccx, machine::llsize_of_alloc(ccx, lltp_ty))\n             }\n         }\n-        (_, \"min_align_of\") => {\n+        \"min_align_of\" => {\n             let tp_ty = substs.type_at(0);\n             C_uint(ccx, type_of::align_of(ccx, tp_ty))\n         }\n-        (_, \"min_align_of_val\") => {\n+        \"min_align_of_val\" => {\n             let tp_ty = substs.type_at(0);\n-            if !type_is_sized(tcx, tp_ty) {\n+            if !bcx.ccx.shared().type_is_sized(tp_ty) {\n                 let (_, llalign) =\n-                    glue::size_and_align_of_dst(&bcx.build(), tp_ty, llargs[1]);\n+                    glue::size_and_align_of_dst(bcx, tp_ty, llargs[1]);\n                 llalign\n             } else {\n                 C_uint(ccx, type_of::align_of(ccx, tp_ty))\n             }\n         }\n-        (_, \"pref_align_of\") => {\n+        \"pref_align_of\" => {\n             let tp_ty = substs.type_at(0);\n             let lltp_ty = type_of::type_of(ccx, tp_ty);\n             C_uint(ccx, machine::llalign_of_pref(ccx, lltp_ty))\n         }\n-        (_, \"drop_in_place\") => {\n-            let tp_ty = substs.type_at(0);\n-            let is_sized = type_is_sized(tcx, tp_ty);\n-            let ptr = if is_sized {\n-                llargs[0]\n-            } else {\n-                // FIXME(#36457) -- we should pass unsized values as two arguments\n-                let scratch = alloc_ty(bcx, tp_ty, \"drop\");\n-                call_lifetime_start(bcx, scratch);\n-                Store(bcx, llargs[0], get_dataptr(bcx, scratch));\n-                Store(bcx, llargs[1], get_meta(bcx, scratch));\n-                scratch\n-            };\n-            glue::drop_ty(bcx, ptr, tp_ty, call_debug_location);\n-            if !is_sized {\n-                call_lifetime_end(bcx, ptr);\n-            }\n-            C_nil(ccx)\n-        }\n-        (_, \"type_name\") => {\n+        \"type_name\" => {\n             let tp_ty = substs.type_at(0);\n             let ty_name = Symbol::intern(&tp_ty.to_string()).as_str();\n             C_str_slice(ccx, ty_name)\n         }\n-        (_, \"type_id\") => {\n+        \"type_id\" => {\n             C_u64(ccx, ccx.tcx().type_id_hash(substs.type_at(0)))\n         }\n-        (_, \"init\") => {\n-            let tp_ty = substs.type_at(0);\n-            if !type_is_zero_size(ccx, tp_ty) {\n-                // Just zero out the stack slot. (See comment on base::memzero for explanation)\n-                init_zero_mem(bcx, llresult, tp_ty);\n+        \"init\" => {\n+            let ty = substs.type_at(0);\n+            if !type_is_zero_size(ccx, ty) {\n+                // Just zero out the stack slot.\n+                // If we store a zero constant, LLVM will drown in vreg allocation for large data\n+                // structures, and the generated code will be awful. (A telltale sign of this is\n+                // large quantities of `mov [byte ptr foo],0` in the generated code.)\n+                memset_intrinsic(bcx, false, ty, llresult, C_u8(ccx, 0), C_uint(ccx, 1usize));\n             }\n             C_nil(ccx)\n         }\n         // Effectively no-ops\n-        (_, \"uninit\") | (_, \"forget\") => {\n+        \"uninit\" | \"forget\" => {\n             C_nil(ccx)\n         }\n-        (_, \"needs_drop\") => {\n+        \"needs_drop\" => {\n             let tp_ty = substs.type_at(0);\n \n-            C_bool(ccx, bcx.fcx.type_needs_drop(tp_ty))\n+            C_bool(ccx, bcx.ccx.shared().type_needs_drop(tp_ty))\n         }\n-        (_, \"offset\") => {\n+        \"offset\" => {\n             let ptr = llargs[0];\n             let offset = llargs[1];\n-            InBoundsGEP(bcx, ptr, &[offset])\n+            bcx.inbounds_gep(ptr, &[offset])\n         }\n-        (_, \"arith_offset\") => {\n+        \"arith_offset\" => {\n             let ptr = llargs[0];\n             let offset = llargs[1];\n-            GEP(bcx, ptr, &[offset])\n+            bcx.gep(ptr, &[offset])\n         }\n \n-        (_, \"copy_nonoverlapping\") => {\n-            copy_intrinsic(bcx,\n-                           false,\n-                           false,\n-                           substs.type_at(0),\n-                           llargs[1],\n-                           llargs[0],\n-                           llargs[2],\n-                           call_debug_location)\n+        \"copy_nonoverlapping\" => {\n+            copy_intrinsic(bcx, false, false, substs.type_at(0), llargs[1], llargs[0], llargs[2])\n         }\n-        (_, \"copy\") => {\n-            copy_intrinsic(bcx,\n-                           true,\n-                           false,\n-                           substs.type_at(0),\n-                           llargs[1],\n-                           llargs[0],\n-                           llargs[2],\n-                           call_debug_location)\n+        \"copy\" => {\n+            copy_intrinsic(bcx, true, false, substs.type_at(0), llargs[1], llargs[0], llargs[2])\n         }\n-        (_, \"write_bytes\") => {\n-            memset_intrinsic(bcx,\n-                             false,\n-                             substs.type_at(0),\n-                             llargs[0],\n-                             llargs[1],\n-                             llargs[2],\n-                             call_debug_location)\n+        \"write_bytes\" => {\n+            memset_intrinsic(bcx, false, substs.type_at(0), llargs[0], llargs[1], llargs[2])\n         }\n \n-        (_, \"volatile_copy_nonoverlapping_memory\") => {\n-            copy_intrinsic(bcx,\n-                           false,\n-                           true,\n-                           substs.type_at(0),\n-                           llargs[0],\n-                           llargs[1],\n-                           llargs[2],\n-                           call_debug_location)\n+        \"volatile_copy_nonoverlapping_memory\" => {\n+            copy_intrinsic(bcx, false, true, substs.type_at(0), llargs[0], llargs[1], llargs[2])\n         }\n-        (_, \"volatile_copy_memory\") => {\n-            copy_intrinsic(bcx,\n-                           true,\n-                           true,\n-                           substs.type_at(0),\n-                           llargs[0],\n-                           llargs[1],\n-                           llargs[2],\n-                           call_debug_location)\n+        \"volatile_copy_memory\" => {\n+            copy_intrinsic(bcx, true, true, substs.type_at(0), llargs[0], llargs[1], llargs[2])\n         }\n-        (_, \"volatile_set_memory\") => {\n-            memset_intrinsic(bcx,\n-                             true,\n-                             substs.type_at(0),\n-                             llargs[0],\n-                             llargs[1],\n-                             llargs[2],\n-                             call_debug_location)\n+        \"volatile_set_memory\" => {\n+            memset_intrinsic(bcx, true, substs.type_at(0), llargs[0], llargs[1], llargs[2])\n         }\n-        (_, \"volatile_load\") => {\n+        \"volatile_load\" => {\n             let tp_ty = substs.type_at(0);\n             let mut ptr = llargs[0];\n             if let Some(ty) = fn_ty.ret.cast {\n-                ptr = PointerCast(bcx, ptr, ty.ptr_to());\n+                ptr = bcx.pointercast(ptr, ty.ptr_to());\n             }\n-            let load = VolatileLoad(bcx, ptr);\n+            let load = bcx.volatile_load(ptr);\n             unsafe {\n                 llvm::LLVMSetAlignment(load, type_of::align_of(ccx, tp_ty));\n             }\n             to_immediate(bcx, load, tp_ty)\n         },\n-        (_, \"volatile_store\") => {\n+        \"volatile_store\" => {\n             let tp_ty = substs.type_at(0);\n-            if type_is_fat_ptr(bcx.tcx(), tp_ty) {\n-                VolatileStore(bcx, llargs[1], get_dataptr(bcx, llargs[0]));\n-                VolatileStore(bcx, llargs[2], get_meta(bcx, llargs[0]));\n+            if type_is_fat_ptr(bcx.ccx, tp_ty) {\n+                bcx.volatile_store(llargs[1], get_dataptr(bcx, llargs[0]));\n+                bcx.volatile_store(llargs[2], get_meta(bcx, llargs[0]));\n             } else {\n                 let val = if fn_ty.args[1].is_indirect() {\n-                    Load(bcx, llargs[1])\n+                    bcx.load(llargs[1])\n                 } else {\n                     from_immediate(bcx, llargs[1])\n                 };\n-                let ptr = PointerCast(bcx, llargs[0], val_ty(val).ptr_to());\n-                let store = VolatileStore(bcx, val, ptr);\n+                let ptr = bcx.pointercast(llargs[0], val_ty(val).ptr_to());\n+                let store = bcx.volatile_store(val, ptr);\n                 unsafe {\n                     llvm::LLVMSetAlignment(store, type_of::align_of(ccx, tp_ty));\n                 }\n             }\n             C_nil(ccx)\n         },\n \n-        (_, \"ctlz\") | (_, \"cttz\") | (_, \"ctpop\") | (_, \"bswap\") |\n-        (_, \"add_with_overflow\") | (_, \"sub_with_overflow\") | (_, \"mul_with_overflow\") |\n-        (_, \"overflowing_add\") | (_, \"overflowing_sub\") | (_, \"overflowing_mul\") |\n-        (_, \"unchecked_div\") | (_, \"unchecked_rem\") => {\n+        \"ctlz\" | \"cttz\" | \"ctpop\" | \"bswap\" |\n+        \"add_with_overflow\" | \"sub_with_overflow\" | \"mul_with_overflow\" |\n+        \"overflowing_add\" | \"overflowing_sub\" | \"overflowing_mul\" |\n+        \"unchecked_div\" | \"unchecked_rem\" => {\n             let sty = &arg_tys[0].sty;\n             match int_type_width_signed(sty, ccx) {\n                 Some((width, signed)) =>\n                     match name {\n-                        \"ctlz\" => count_zeros_intrinsic(bcx, &format!(\"llvm.ctlz.i{}\", width),\n-                                                        llargs[0], call_debug_location),\n-                        \"cttz\" => count_zeros_intrinsic(bcx, &format!(\"llvm.cttz.i{}\", width),\n-                                                        llargs[0], call_debug_location),\n-                        \"ctpop\" => Call(bcx, ccx.get_intrinsic(&format!(\"llvm.ctpop.i{}\", width)),\n-                                        &llargs, call_debug_location),\n+                        \"ctlz\" | \"cttz\" => {\n+                            let y = C_bool(bcx.ccx, false);\n+                            let llfn = ccx.get_intrinsic(&format!(\"llvm.{}.i{}\", name, width));\n+                            bcx.call(llfn, &[llargs[0], y], None)\n+                        }\n+                        \"ctpop\" => bcx.call(ccx.get_intrinsic(&format!(\"llvm.ctpop.i{}\", width)),\n+                                        &llargs, None),\n                         \"bswap\" => {\n                             if width == 8 {\n                                 llargs[0] // byte swap a u8/i8 is just a no-op\n                             } else {\n-                                Call(bcx, ccx.get_intrinsic(&format!(\"llvm.bswap.i{}\", width)),\n-                                        &llargs, call_debug_location)\n+                                bcx.call(ccx.get_intrinsic(&format!(\"llvm.bswap.i{}\", width)),\n+                                        &llargs, None)\n                             }\n                         }\n                         \"add_with_overflow\" | \"sub_with_overflow\" | \"mul_with_overflow\" => {\n                             let intrinsic = format!(\"llvm.{}{}.with.overflow.i{}\",\n                                                     if signed { 's' } else { 'u' },\n                                                     &name[..3], width);\n-                            with_overflow_intrinsic(bcx, &intrinsic, llargs[0], llargs[1], llresult,\n-                                                    call_debug_location)\n+                            let llfn = bcx.ccx.get_intrinsic(&intrinsic);\n+\n+                            // Convert `i1` to a `bool`, and write it to the out parameter\n+                            let val = bcx.call(llfn, &[llargs[0], llargs[1]], None);\n+                            let result = bcx.extract_value(val, 0);\n+                            let overflow = bcx.zext(bcx.extract_value(val, 1), Type::bool(ccx));\n+                            bcx.store(result, bcx.struct_gep(llresult, 0));\n+                            bcx.store(overflow, bcx.struct_gep(llresult, 1));\n+\n+                            C_nil(bcx.ccx)\n                         },\n-                        \"overflowing_add\" => Add(bcx, llargs[0], llargs[1], call_debug_location),\n-                        \"overflowing_sub\" => Sub(bcx, llargs[0], llargs[1], call_debug_location),\n-                        \"overflowing_mul\" => Mul(bcx, llargs[0], llargs[1], call_debug_location),\n+                        \"overflowing_add\" => bcx.add(llargs[0], llargs[1]),\n+                        \"overflowing_sub\" => bcx.sub(llargs[0], llargs[1]),\n+                        \"overflowing_mul\" => bcx.mul(llargs[0], llargs[1]),\n                         \"unchecked_div\" =>\n                             if signed {\n-                                SDiv(bcx, llargs[0], llargs[1], call_debug_location)\n+                                bcx.sdiv(llargs[0], llargs[1])\n                             } else {\n-                                UDiv(bcx, llargs[0], llargs[1], call_debug_location)\n+                                bcx.udiv(llargs[0], llargs[1])\n                             },\n                         \"unchecked_rem\" =>\n                             if signed {\n-                                SRem(bcx, llargs[0], llargs[1], call_debug_location)\n+                                bcx.srem(llargs[0], llargs[1])\n                             } else {\n-                                URem(bcx, llargs[0], llargs[1], call_debug_location)\n+                                bcx.urem(llargs[0], llargs[1])\n                             },\n                         _ => bug!(),\n                     },\n@@ -390,17 +320,16 @@ pub fn trans_intrinsic_call<'a, 'blk, 'tcx>(mut bcx: Block<'blk, 'tcx>,\n             }\n \n         },\n-        (_, \"fadd_fast\") | (_, \"fsub_fast\") | (_, \"fmul_fast\") | (_, \"fdiv_fast\") |\n-        (_, \"frem_fast\") => {\n+        \"fadd_fast\" | \"fsub_fast\" | \"fmul_fast\" | \"fdiv_fast\" | \"frem_fast\" => {\n             let sty = &arg_tys[0].sty;\n             match float_type_width(sty) {\n                 Some(_width) =>\n                     match name {\n-                        \"fadd_fast\" => FAddFast(bcx, llargs[0], llargs[1], call_debug_location),\n-                        \"fsub_fast\" => FSubFast(bcx, llargs[0], llargs[1], call_debug_location),\n-                        \"fmul_fast\" => FMulFast(bcx, llargs[0], llargs[1], call_debug_location),\n-                        \"fdiv_fast\" => FDivFast(bcx, llargs[0], llargs[1], call_debug_location),\n-                        \"frem_fast\" => FRemFast(bcx, llargs[0], llargs[1], call_debug_location),\n+                        \"fadd_fast\" => bcx.fadd_fast(llargs[0], llargs[1]),\n+                        \"fsub_fast\" => bcx.fsub_fast(llargs[0], llargs[1]),\n+                        \"fmul_fast\" => bcx.fmul_fast(llargs[0], llargs[1]),\n+                        \"fdiv_fast\" => bcx.fdiv_fast(llargs[0], llargs[1]),\n+                        \"frem_fast\" => bcx.frem_fast(llargs[0], llargs[1]),\n                         _ => bug!(),\n                     },\n                 None => {\n@@ -414,7 +343,7 @@ pub fn trans_intrinsic_call<'a, 'blk, 'tcx>(mut bcx: Block<'blk, 'tcx>,\n \n         },\n \n-        (_, \"discriminant_value\") => {\n+        \"discriminant_value\" => {\n             let val_ty = substs.type_at(0);\n             match val_ty.sty {\n                 ty::TyAdt(adt, ..) if adt.is_enum() => {\n@@ -424,17 +353,16 @@ pub fn trans_intrinsic_call<'a, 'blk, 'tcx>(mut bcx: Block<'blk, 'tcx>,\n                 _ => C_null(llret_ty)\n             }\n         }\n-        (_, name) if name.starts_with(\"simd_\") => {\n+        name if name.starts_with(\"simd_\") => {\n             generic_simd_intrinsic(bcx, name,\n                                    callee_ty,\n                                    &llargs,\n                                    ret_ty, llret_ty,\n-                                   call_debug_location,\n                                    span)\n         }\n         // This requires that atomic intrinsics follow a specific naming pattern:\n         // \"atomic_<operation>[_<ordering>]\", and no ordering means SeqCst\n-        (_, name) if name.starts_with(\"atomic_\") => {\n+        name if name.starts_with(\"atomic_\") => {\n             use llvm::AtomicOrdering::*;\n \n             let split: Vec<&str> = name.split('_').collect();\n@@ -464,59 +392,56 @@ pub fn trans_intrinsic_call<'a, 'blk, 'tcx>(mut bcx: Block<'blk, 'tcx>,\n                 _ => ccx.sess().fatal(\"Atomic intrinsic not in correct format\"),\n             };\n \n+            let invalid_monomorphization = |sty| {\n+                span_invalid_monomorphization_error(tcx.sess, span,\n+                    &format!(\"invalid monomorphization of `{}` intrinsic: \\\n+                              expected basic integer type, found `{}`\", name, sty));\n+            };\n+\n             match split[1] {\n                 \"cxchg\" | \"cxchgweak\" => {\n                     let sty = &substs.type_at(0).sty;\n                     if int_type_width_signed(sty, ccx).is_some() {\n                         let weak = if split[1] == \"cxchgweak\" { llvm::True } else { llvm::False };\n-                        let val = AtomicCmpXchg(bcx, llargs[0], llargs[1], llargs[2],\n-                                                order, failorder, weak);\n-                        let result = ExtractValue(bcx, val, 0);\n-                        let success = ZExt(bcx, ExtractValue(bcx, val, 1), Type::bool(bcx.ccx()));\n-                        Store(bcx, result, StructGEP(bcx, llresult, 0));\n-                        Store(bcx, success, StructGEP(bcx, llresult, 1));\n+                        let val = bcx.atomic_cmpxchg(llargs[0], llargs[1], llargs[2], order,\n+                            failorder, weak);\n+                        let result = bcx.extract_value(val, 0);\n+                        let success = bcx.zext(bcx.extract_value(val, 1), Type::bool(bcx.ccx));\n+                        bcx.store(result, bcx.struct_gep(llresult, 0));\n+                        bcx.store(success, bcx.struct_gep(llresult, 1));\n                     } else {\n-                        span_invalid_monomorphization_error(\n-                            tcx.sess, span,\n-                            &format!(\"invalid monomorphization of `{}` intrinsic: \\\n-                                      expected basic integer type, found `{}`\", name, sty));\n+                        invalid_monomorphization(sty);\n                     }\n                     C_nil(ccx)\n                 }\n \n                 \"load\" => {\n                     let sty = &substs.type_at(0).sty;\n                     if int_type_width_signed(sty, ccx).is_some() {\n-                        AtomicLoad(bcx, llargs[0], order)\n+                        bcx.atomic_load(llargs[0], order)\n                     } else {\n-                        span_invalid_monomorphization_error(\n-                            tcx.sess, span,\n-                            &format!(\"invalid monomorphization of `{}` intrinsic: \\\n-                                      expected basic integer type, found `{}`\", name, sty));\n+                        invalid_monomorphization(sty);\n                         C_nil(ccx)\n                     }\n                 }\n \n                 \"store\" => {\n                     let sty = &substs.type_at(0).sty;\n                     if int_type_width_signed(sty, ccx).is_some() {\n-                        AtomicStore(bcx, llargs[1], llargs[0], order);\n+                        bcx.atomic_store(llargs[1], llargs[0], order);\n                     } else {\n-                        span_invalid_monomorphization_error(\n-                            tcx.sess, span,\n-                            &format!(\"invalid monomorphization of `{}` intrinsic: \\\n-                                      expected basic integer type, found `{}`\", name, sty));\n+                        invalid_monomorphization(sty);\n                     }\n                     C_nil(ccx)\n                 }\n \n                 \"fence\" => {\n-                    AtomicFence(bcx, order, llvm::SynchronizationScope::CrossThread);\n+                    bcx.atomic_fence(order, llvm::SynchronizationScope::CrossThread);\n                     C_nil(ccx)\n                 }\n \n                 \"singlethreadfence\" => {\n-                    AtomicFence(bcx, order, llvm::SynchronizationScope::SingleThread);\n+                    bcx.atomic_fence(order, llvm::SynchronizationScope::SingleThread);\n                     C_nil(ccx)\n                 }\n \n@@ -539,20 +464,16 @@ pub fn trans_intrinsic_call<'a, 'blk, 'tcx>(mut bcx: Block<'blk, 'tcx>,\n \n                     let sty = &substs.type_at(0).sty;\n                     if int_type_width_signed(sty, ccx).is_some() {\n-                        AtomicRMW(bcx, atom_op, llargs[0], llargs[1], order)\n+                        bcx.atomic_rmw(atom_op, llargs[0], llargs[1], order)\n                     } else {\n-                        span_invalid_monomorphization_error(\n-                            tcx.sess, span,\n-                            &format!(\"invalid monomorphization of `{}` intrinsic: \\\n-                                      expected basic integer type, found `{}`\", name, sty));\n+                        invalid_monomorphization(sty);\n                         C_nil(ccx)\n                     }\n                 }\n             }\n-\n         }\n \n-        (..) => {\n+        _ => {\n             let intr = match Intrinsic::find(&name) {\n                 Some(intr) => intr,\n                 None => bug!(\"unknown intrinsic '{}'\", name),\n@@ -581,18 +502,15 @@ pub fn trans_intrinsic_call<'a, 'blk, 'tcx>(mut bcx: Block<'blk, 'tcx>,\n                         *any_changes_needed |= llvm_elem.is_some();\n \n                         let t = llvm_elem.as_ref().unwrap_or(t);\n-                        let elem = one(ty_to_type(ccx, t,\n-                                                  any_changes_needed));\n+                        let elem = one(ty_to_type(ccx, t, any_changes_needed));\n                         vec![elem.ptr_to()]\n                     }\n                     Vector(ref t, ref llvm_elem, length) => {\n                         *any_changes_needed |= llvm_elem.is_some();\n \n                         let t = llvm_elem.as_ref().unwrap_or(t);\n-                        let elem = one(ty_to_type(ccx, t,\n-                                                  any_changes_needed));\n-                        vec![Type::vector(&elem,\n-                                          length as u64)]\n+                        let elem = one(ty_to_type(ccx, t, any_changes_needed));\n+                        vec![Type::vector(&elem, length as u64)]\n                     }\n                     Aggregate(false, ref contents) => {\n                         let elems = contents.iter()\n@@ -613,11 +531,11 @@ pub fn trans_intrinsic_call<'a, 'blk, 'tcx>(mut bcx: Block<'blk, 'tcx>,\n             // qux` to be converted into `foo, bar, baz, qux`, integer\n             // arguments to be truncated as needed and pointers to be\n             // cast.\n-            fn modify_as_needed<'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n-                                            t: &intrinsics::Type,\n-                                            arg_type: Ty<'tcx>,\n-                                            llarg: ValueRef)\n-                                            -> Vec<ValueRef>\n+            fn modify_as_needed<'a, 'tcx>(bcx: &BlockAndBuilder<'a, 'tcx>,\n+                                          t: &intrinsics::Type,\n+                                          arg_type: Ty<'tcx>,\n+                                          llarg: ValueRef)\n+                                          -> Vec<ValueRef>\n             {\n                 match *t {\n                     intrinsics::Type::Aggregate(true, ref contents) => {\n@@ -627,29 +545,27 @@ pub fn trans_intrinsic_call<'a, 'blk, 'tcx>(mut bcx: Block<'blk, 'tcx>,\n                         // This assumes the type is \"simple\", i.e. no\n                         // destructors, and the contents are SIMD\n                         // etc.\n-                        assert!(!bcx.fcx.type_needs_drop(arg_type));\n+                        assert!(!bcx.ccx.shared().type_needs_drop(arg_type));\n                         let arg = adt::MaybeSizedValue::sized(llarg);\n                         (0..contents.len())\n                             .map(|i| {\n-                                Load(bcx, adt::trans_field_ptr(bcx, arg_type, arg, Disr(0), i))\n+                                bcx.load(adt::trans_field_ptr(bcx, arg_type, arg, Disr(0), i))\n                             })\n                             .collect()\n                     }\n                     intrinsics::Type::Pointer(_, Some(ref llvm_elem), _) => {\n-                        let llvm_elem = one(ty_to_type(bcx.ccx(), llvm_elem, &mut false));\n-                        vec![PointerCast(bcx, llarg,\n-                                         llvm_elem.ptr_to())]\n+                        let llvm_elem = one(ty_to_type(bcx.ccx, llvm_elem, &mut false));\n+                        vec![bcx.pointercast(llarg, llvm_elem.ptr_to())]\n                     }\n                     intrinsics::Type::Vector(_, Some(ref llvm_elem), length) => {\n-                        let llvm_elem = one(ty_to_type(bcx.ccx(), llvm_elem, &mut false));\n-                        vec![BitCast(bcx, llarg,\n-                                     Type::vector(&llvm_elem, length as u64))]\n+                        let llvm_elem = one(ty_to_type(bcx.ccx, llvm_elem, &mut false));\n+                        vec![bcx.bitcast(llarg, Type::vector(&llvm_elem, length as u64))]\n                     }\n                     intrinsics::Type::Integer(_, width, llvm_width) if width != llvm_width => {\n                         // the LLVM intrinsic uses a smaller integer\n                         // size than the C intrinsic's signature, so\n                         // we have to trim it down here.\n-                        vec![Trunc(bcx, llarg, Type::ix(bcx.ccx(), llvm_width as u64))]\n+                        vec![bcx.trunc(llarg, Type::ix(bcx.ccx, llvm_width as u64))]\n                     }\n                     _ => vec![llarg],\n                 }\n@@ -686,7 +602,7 @@ pub fn trans_intrinsic_call<'a, 'blk, 'tcx>(mut bcx: Block<'blk, 'tcx>,\n                     let f = declare::declare_cfn(ccx,\n                                                  name,\n                                                  Type::func(&inputs, &outputs));\n-                    Call(bcx, f, &llargs, call_debug_location)\n+                    bcx.call(f, &llargs, None)\n                 }\n             };\n \n@@ -696,8 +612,8 @@ pub fn trans_intrinsic_call<'a, 'blk, 'tcx>(mut bcx: Block<'blk, 'tcx>,\n                     assert!(!flatten);\n \n                     for i in 0..elems.len() {\n-                        let val = ExtractValue(bcx, val, i);\n-                        Store(bcx, val, StructGEP(bcx, llresult, i));\n+                        let val = bcx.extract_value(val, i);\n+                        bcx.store(val, bcx.struct_gep(llresult, i));\n                     }\n                     C_nil(ccx)\n                 }\n@@ -706,32 +622,28 @@ pub fn trans_intrinsic_call<'a, 'blk, 'tcx>(mut bcx: Block<'blk, 'tcx>,\n         }\n     };\n \n-    if val_ty(llval) != Type::void(ccx) &&\n-       machine::llsize_of_alloc(ccx, val_ty(llval)) != 0 {\n+    if val_ty(llval) != Type::void(ccx) && machine::llsize_of_alloc(ccx, val_ty(llval)) != 0 {\n         if let Some(ty) = fn_ty.ret.cast {\n-            let ptr = PointerCast(bcx, llresult, ty.ptr_to());\n-            let store = Store(bcx, llval, ptr);\n+            let ptr = bcx.pointercast(llresult, ty.ptr_to());\n+            let store = bcx.store(llval, ptr);\n             unsafe {\n                 llvm::LLVMSetAlignment(store, type_of::align_of(ccx, ret_ty));\n             }\n         } else {\n             store_ty(bcx, llval, llresult, ret_ty);\n         }\n     }\n-\n-    Result::new(bcx, llresult)\n }\n \n-fn copy_intrinsic<'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n-                              allow_overlap: bool,\n-                              volatile: bool,\n-                              tp_ty: Ty<'tcx>,\n-                              dst: ValueRef,\n-                              src: ValueRef,\n-                              count: ValueRef,\n-                              call_debug_location: DebugLoc)\n-                              -> ValueRef {\n-    let ccx = bcx.ccx();\n+fn copy_intrinsic<'a, 'tcx>(bcx: &BlockAndBuilder<'a, 'tcx>,\n+                            allow_overlap: bool,\n+                            volatile: bool,\n+                            tp_ty: Ty<'tcx>,\n+                            dst: ValueRef,\n+                            src: ValueRef,\n+                            count: ValueRef)\n+                            -> ValueRef {\n+    let ccx = bcx.ccx;\n     let lltp_ty = type_of::type_of(ccx, tp_ty);\n     let align = C_i32(ccx, type_of::align_of(ccx, tp_ty) as i32);\n     let size = machine::llsize_of(ccx, lltp_ty);\n@@ -745,92 +657,49 @@ fn copy_intrinsic<'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n \n     let name = format!(\"llvm.{}.p0i8.p0i8.i{}\", operation, int_size);\n \n-    let dst_ptr = PointerCast(bcx, dst, Type::i8p(ccx));\n-    let src_ptr = PointerCast(bcx, src, Type::i8p(ccx));\n+    let dst_ptr = bcx.pointercast(dst, Type::i8p(ccx));\n+    let src_ptr = bcx.pointercast(src, Type::i8p(ccx));\n     let llfn = ccx.get_intrinsic(&name);\n \n-    Call(bcx,\n-         llfn,\n-         &[dst_ptr,\n-           src_ptr,\n-           Mul(bcx, size, count, DebugLoc::None),\n-           align,\n-           C_bool(ccx, volatile)],\n-         call_debug_location)\n+    bcx.call(llfn,\n+        &[dst_ptr,\n+        src_ptr,\n+        bcx.mul(size, count),\n+        align,\n+        C_bool(ccx, volatile)],\n+        None)\n }\n \n-fn memset_intrinsic<'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n-                                volatile: bool,\n-                                tp_ty: Ty<'tcx>,\n-                                dst: ValueRef,\n-                                val: ValueRef,\n-                                count: ValueRef,\n-                                call_debug_location: DebugLoc)\n-                                -> ValueRef {\n-    let ccx = bcx.ccx();\n-    let lltp_ty = type_of::type_of(ccx, tp_ty);\n-    let align = C_i32(ccx, type_of::align_of(ccx, tp_ty) as i32);\n+fn memset_intrinsic<'a, 'tcx>(\n+    bcx: &BlockAndBuilder<'a, 'tcx>,\n+    volatile: bool,\n+    ty: Ty<'tcx>,\n+    dst: ValueRef,\n+    val: ValueRef,\n+    count: ValueRef\n+) -> ValueRef {\n+    let ccx = bcx.ccx;\n+    let align = C_i32(ccx, type_of::align_of(ccx, ty) as i32);\n+    let lltp_ty = type_of::type_of(ccx, ty);\n     let size = machine::llsize_of(ccx, lltp_ty);\n-    let int_size = machine::llbitsize_of_real(ccx, ccx.int_type());\n-\n-    let name = format!(\"llvm.memset.p0i8.i{}\", int_size);\n-\n-    let dst_ptr = PointerCast(bcx, dst, Type::i8p(ccx));\n-    let llfn = ccx.get_intrinsic(&name);\n-\n-    Call(bcx,\n-         llfn,\n-         &[dst_ptr,\n-           val,\n-           Mul(bcx, size, count, DebugLoc::None),\n-           align,\n-           C_bool(ccx, volatile)],\n-         call_debug_location)\n+    let dst = bcx.pointercast(dst, Type::i8p(ccx));\n+    call_memset(bcx, dst, val, bcx.mul(size, count), align, volatile)\n }\n \n-fn count_zeros_intrinsic(bcx: Block,\n-                         name: &str,\n-                         val: ValueRef,\n-                         call_debug_location: DebugLoc)\n-                         -> ValueRef {\n-    let y = C_bool(bcx.ccx(), false);\n-    let llfn = bcx.ccx().get_intrinsic(&name);\n-    Call(bcx, llfn, &[val, y], call_debug_location)\n-}\n-\n-fn with_overflow_intrinsic<'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n-                                       name: &str,\n-                                       a: ValueRef,\n-                                       b: ValueRef,\n-                                       out: ValueRef,\n-                                       call_debug_location: DebugLoc)\n-                                       -> ValueRef {\n-    let llfn = bcx.ccx().get_intrinsic(&name);\n-\n-    // Convert `i1` to a `bool`, and write it to the out parameter\n-    let val = Call(bcx, llfn, &[a, b], call_debug_location);\n-    let result = ExtractValue(bcx, val, 0);\n-    let overflow = ZExt(bcx, ExtractValue(bcx, val, 1), Type::bool(bcx.ccx()));\n-    Store(bcx, result, StructGEP(bcx, out, 0));\n-    Store(bcx, overflow, StructGEP(bcx, out, 1));\n-\n-    C_nil(bcx.ccx())\n-}\n-\n-fn try_intrinsic<'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n-                             func: ValueRef,\n-                             data: ValueRef,\n-                             local_ptr: ValueRef,\n-                             dest: ValueRef,\n-                             dloc: DebugLoc) -> Block<'blk, 'tcx> {\n+fn try_intrinsic<'a, 'tcx>(\n+    bcx: &BlockAndBuilder<'a, 'tcx>,\n+    func: ValueRef,\n+    data: ValueRef,\n+    local_ptr: ValueRef,\n+    dest: ValueRef,\n+) {\n     if bcx.sess().no_landing_pads() {\n-        Call(bcx, func, &[data], dloc);\n-        Store(bcx, C_null(Type::i8p(bcx.ccx())), dest);\n-        bcx\n+        bcx.call(func, &[data], None);\n+        bcx.store(C_null(Type::i8p(&bcx.ccx)), dest);\n     } else if wants_msvc_seh(bcx.sess()) {\n-        trans_msvc_try(bcx, func, data, local_ptr, dest, dloc)\n+        trans_msvc_try(bcx, func, data, local_ptr, dest);\n     } else {\n-        trans_gnu_try(bcx, func, data, local_ptr, dest, dloc)\n+        trans_gnu_try(bcx, func, data, local_ptr, dest);\n     }\n }\n \n@@ -841,26 +710,24 @@ fn try_intrinsic<'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n // instructions are meant to work for all targets, as of the time of this\n // writing, however, LLVM does not recommend the usage of these new instructions\n // as the old ones are still more optimized.\n-fn trans_msvc_try<'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n-                              func: ValueRef,\n-                              data: ValueRef,\n-                              local_ptr: ValueRef,\n-                              dest: ValueRef,\n-                              dloc: DebugLoc) -> Block<'blk, 'tcx> {\n-    let llfn = get_rust_try_fn(bcx.fcx, &mut |bcx| {\n-        let ccx = bcx.ccx();\n-        let dloc = DebugLoc::None;\n-\n-        SetPersonalityFn(bcx, bcx.fcx.eh_personality());\n-\n-        let normal = bcx.fcx.new_block(\"normal\");\n-        let catchswitch = bcx.fcx.new_block(\"catchswitch\");\n-        let catchpad = bcx.fcx.new_block(\"catchpad\");\n-        let caught = bcx.fcx.new_block(\"caught\");\n-\n-        let func = llvm::get_param(bcx.fcx.llfn, 0);\n-        let data = llvm::get_param(bcx.fcx.llfn, 1);\n-        let local_ptr = llvm::get_param(bcx.fcx.llfn, 2);\n+fn trans_msvc_try<'a, 'tcx>(bcx: &BlockAndBuilder<'a, 'tcx>,\n+                            func: ValueRef,\n+                            data: ValueRef,\n+                            local_ptr: ValueRef,\n+                            dest: ValueRef) {\n+    let llfn = get_rust_try_fn(bcx.fcx(), &mut |bcx| {\n+        let ccx = bcx.ccx;\n+\n+        bcx.set_personality_fn(bcx.ccx.eh_personality());\n+\n+        let normal = bcx.fcx().build_new_block(\"normal\");\n+        let catchswitch = bcx.fcx().build_new_block(\"catchswitch\");\n+        let catchpad = bcx.fcx().build_new_block(\"catchpad\");\n+        let caught = bcx.fcx().build_new_block(\"caught\");\n+\n+        let func = llvm::get_param(bcx.fcx().llfn, 0);\n+        let data = llvm::get_param(bcx.fcx().llfn, 1);\n+        let local_ptr = llvm::get_param(bcx.fcx().llfn, 2);\n \n         // We're generating an IR snippet that looks like:\n         //\n@@ -902,37 +769,37 @@ fn trans_msvc_try<'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n         //\n         // More information can be found in libstd's seh.rs implementation.\n         let i64p = Type::i64(ccx).ptr_to();\n-        let slot = Alloca(bcx, i64p, \"slot\");\n-        Invoke(bcx, func, &[data], normal.llbb, catchswitch.llbb, dloc);\n+        let slot = bcx.fcx().alloca(i64p, \"slot\");\n+        bcx.invoke(func, &[data], normal.llbb(), catchswitch.llbb(),\n+            None);\n \n-        Ret(normal, C_i32(ccx, 0), dloc);\n+        normal.ret(C_i32(ccx, 0));\n \n-        let cs = CatchSwitch(catchswitch, None, None, 1);\n-        AddHandler(catchswitch, cs, catchpad.llbb);\n+        let cs = catchswitch.catch_switch(None, None, 1);\n+        catchswitch.add_handler(cs, catchpad.llbb());\n \n         let tcx = ccx.tcx();\n         let tydesc = match tcx.lang_items.msvc_try_filter() {\n             Some(did) => ::consts::get_static(ccx, did),\n             None => bug!(\"msvc_try_filter not defined\"),\n         };\n-        let tok = CatchPad(catchpad, cs, &[tydesc, C_i32(ccx, 0), slot]);\n-        let addr = Load(catchpad, slot);\n-        let arg1 = Load(catchpad, addr);\n+        let tok = catchpad.catch_pad(cs, &[tydesc, C_i32(ccx, 0), slot]);\n+        let addr = catchpad.load(slot);\n+        let arg1 = catchpad.load(addr);\n         let val1 = C_i32(ccx, 1);\n-        let arg2 = Load(catchpad, InBoundsGEP(catchpad, addr, &[val1]));\n-        let local_ptr = BitCast(catchpad, local_ptr, i64p);\n-        Store(catchpad, arg1, local_ptr);\n-        Store(catchpad, arg2, InBoundsGEP(catchpad, local_ptr, &[val1]));\n-        CatchRet(catchpad, tok, caught.llbb);\n+        let arg2 = catchpad.load(catchpad.inbounds_gep(addr, &[val1]));\n+        let local_ptr = catchpad.bitcast(local_ptr, i64p);\n+        catchpad.store(arg1, local_ptr);\n+        catchpad.store(arg2, catchpad.inbounds_gep(local_ptr, &[val1]));\n+        catchpad.catch_ret(tok, caught.llbb());\n \n-        Ret(caught, C_i32(ccx, 1), dloc);\n+        caught.ret(C_i32(ccx, 1));\n     });\n \n     // Note that no invoke is used here because by definition this function\n     // can't panic (that's what it's catching).\n-    let ret = Call(bcx, llfn, &[func, data, local_ptr], dloc);\n-    Store(bcx, ret, dest);\n-    return bcx\n+    let ret = bcx.call(llfn, &[func, data, local_ptr], None);\n+    bcx.store(ret, dest);\n }\n \n // Definition of the standard \"try\" function for Rust using the GNU-like model\n@@ -946,15 +813,13 @@ fn trans_msvc_try<'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n // function calling it, and that function may already have other personality\n // functions in play. By calling a shim we're guaranteed that our shim will have\n // the right personality function.\n-fn trans_gnu_try<'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n-                             func: ValueRef,\n-                             data: ValueRef,\n-                             local_ptr: ValueRef,\n-                             dest: ValueRef,\n-                             dloc: DebugLoc) -> Block<'blk, 'tcx> {\n-    let llfn = get_rust_try_fn(bcx.fcx, &mut |bcx| {\n-        let ccx = bcx.ccx();\n-        let dloc = DebugLoc::None;\n+fn trans_gnu_try<'a, 'tcx>(bcx: &BlockAndBuilder<'a, 'tcx>,\n+                           func: ValueRef,\n+                           data: ValueRef,\n+                           local_ptr: ValueRef,\n+                           dest: ValueRef) {\n+    let llfn = get_rust_try_fn(bcx.fcx(), &mut |bcx| {\n+        let ccx = bcx.ccx;\n \n         // Translates the shims described above:\n         //\n@@ -973,14 +838,14 @@ fn trans_gnu_try<'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n         // expected to be `*mut *mut u8` for this to actually work, but that's\n         // managed by the standard library.\n \n-        let then = bcx.fcx.new_block(\"then\");\n-        let catch = bcx.fcx.new_block(\"catch\");\n+        let then = bcx.fcx().build_new_block(\"then\");\n+        let catch = bcx.fcx().build_new_block(\"catch\");\n \n-        let func = llvm::get_param(bcx.fcx.llfn, 0);\n-        let data = llvm::get_param(bcx.fcx.llfn, 1);\n-        let local_ptr = llvm::get_param(bcx.fcx.llfn, 2);\n-        Invoke(bcx, func, &[data], then.llbb, catch.llbb, dloc);\n-        Ret(then, C_i32(ccx, 0), dloc);\n+        let func = llvm::get_param(bcx.fcx().llfn, 0);\n+        let data = llvm::get_param(bcx.fcx().llfn, 1);\n+        let local_ptr = llvm::get_param(bcx.fcx().llfn, 2);\n+        bcx.invoke(func, &[data], then.llbb(), catch.llbb(), None);\n+        then.ret(C_i32(ccx, 0));\n \n         // Type indicator for the exception being thrown.\n         //\n@@ -990,18 +855,17 @@ fn trans_gnu_try<'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n         // rust_try ignores the selector.\n         let lpad_ty = Type::struct_(ccx, &[Type::i8p(ccx), Type::i32(ccx)],\n                                     false);\n-        let vals = LandingPad(catch, lpad_ty, bcx.fcx.eh_personality(), 1);\n-        AddClause(catch, vals, C_null(Type::i8p(ccx)));\n-        let ptr = ExtractValue(catch, vals, 0);\n-        Store(catch, ptr, BitCast(catch, local_ptr, Type::i8p(ccx).ptr_to()));\n-        Ret(catch, C_i32(ccx, 1), dloc);\n+        let vals = catch.landing_pad(lpad_ty, bcx.ccx.eh_personality(), 1, catch.fcx().llfn);\n+        catch.add_clause(vals, C_null(Type::i8p(ccx)));\n+        let ptr = catch.extract_value(vals, 0);\n+        catch.store(ptr, catch.bitcast(local_ptr, Type::i8p(ccx).ptr_to()));\n+        catch.ret(C_i32(ccx, 1));\n     });\n \n     // Note that no invoke is used here because by definition this function\n     // can't panic (that's what it's catching).\n-    let ret = Call(bcx, llfn, &[func, data, local_ptr], dloc);\n-    Store(bcx, ret, dest);\n-    return bcx;\n+    let ret = bcx.call(llfn, &[func, data, local_ptr], None);\n+    bcx.store(ret, dest);\n }\n \n // Helper function to give a Block to a closure to translate a shim function.\n@@ -1010,23 +874,19 @@ fn gen_fn<'a, 'tcx>(fcx: &FunctionContext<'a, 'tcx>,\n                     name: &str,\n                     inputs: Vec<Ty<'tcx>>,\n                     output: Ty<'tcx>,\n-                    trans: &mut for<'b> FnMut(Block<'b, 'tcx>))\n+                    trans: &mut for<'b> FnMut(BlockAndBuilder<'b, 'tcx>))\n                     -> ValueRef {\n     let ccx = fcx.ccx;\n     let sig = ccx.tcx().mk_fn_sig(inputs.into_iter(), output, false);\n-    let fn_ty = FnType::new(ccx, Abi::Rust, &sig, &[]);\n \n     let rust_fn_ty = ccx.tcx().mk_fn_ptr(ccx.tcx().mk_bare_fn(ty::BareFnTy {\n         unsafety: hir::Unsafety::Unsafe,\n         abi: Abi::Rust,\n         sig: ty::Binder(sig)\n     }));\n     let llfn = declare::define_internal_fn(ccx, name, rust_fn_ty);\n-    let (fcx, block_arena);\n-    block_arena = TypedArena::new();\n-    fcx = FunctionContext::new(ccx, llfn, fn_ty, None, &block_arena);\n-    trans(fcx.init(true));\n-    fcx.cleanup();\n+    let fcx = FunctionContext::new(ccx, llfn);\n+    trans(fcx.get_entry_block());\n     llfn\n }\n \n@@ -1035,7 +895,7 @@ fn gen_fn<'a, 'tcx>(fcx: &FunctionContext<'a, 'tcx>,\n //\n // This function is only generated once and is then cached.\n fn get_rust_try_fn<'a, 'tcx>(fcx: &FunctionContext<'a, 'tcx>,\n-                             trans: &mut for<'b> FnMut(Block<'b, 'tcx>))\n+                             trans: &mut for<'b> FnMut(BlockAndBuilder<'b, 'tcx>))\n                              -> ValueRef {\n     let ccx = fcx.ccx;\n     if let Some(llfn) = ccx.rust_try_fn().get() {\n@@ -1060,16 +920,15 @@ fn span_invalid_monomorphization_error(a: &Session, b: Span, c: &str) {\n     span_err!(a, b, E0511, \"{}\", c);\n }\n \n-fn generic_simd_intrinsic<'blk, 'tcx, 'a>\n-    (bcx: Block<'blk, 'tcx>,\n-     name: &str,\n-     callee_ty: Ty<'tcx>,\n-     llargs: &[ValueRef],\n-     ret_ty: Ty<'tcx>,\n-     llret_ty: Type,\n-     call_debug_location: DebugLoc,\n-     span: Span) -> ValueRef\n-{\n+fn generic_simd_intrinsic<'a, 'tcx>(\n+    bcx: &BlockAndBuilder<'a, 'tcx>,\n+    name: &str,\n+    callee_ty: Ty<'tcx>,\n+    llargs: &[ValueRef],\n+    ret_ty: Ty<'tcx>,\n+    llret_ty: Type,\n+    span: Span\n+) -> ValueRef {\n     // macros for error handling:\n     macro_rules! emit_error {\n         ($msg: tt) => {\n@@ -1087,7 +946,7 @@ fn generic_simd_intrinsic<'blk, 'tcx, 'a>\n         ($cond: expr, $($fmt: tt)*) => {\n             if !$cond {\n                 emit_error!($($fmt)*);\n-                return C_nil(bcx.ccx())\n+                return C_nil(bcx.ccx)\n             }\n         }\n     }\n@@ -1138,8 +997,7 @@ fn generic_simd_intrinsic<'blk, 'tcx, 'a>\n                                   llargs[1],\n                                   in_elem,\n                                   llret_ty,\n-                                  cmp_op,\n-                                  call_debug_location)\n+                                  cmp_op)\n     }\n \n     if name.starts_with(\"simd_shuffle\") {\n@@ -1179,7 +1037,7 @@ fn generic_simd_intrinsic<'blk, 'tcx, 'a>\n                                     arg_idx, total_len);\n                         None\n                     }\n-                    Some(idx) => Some(C_i32(bcx.ccx(), idx as i32)),\n+                    Some(idx) => Some(C_i32(bcx.ccx, idx as i32)),\n                 }\n             })\n             .collect();\n@@ -1188,20 +1046,20 @@ fn generic_simd_intrinsic<'blk, 'tcx, 'a>\n             None => return C_null(llret_ty)\n         };\n \n-        return ShuffleVector(bcx, llargs[0], llargs[1], C_vector(&indices))\n+        return bcx.shuffle_vector(llargs[0], llargs[1], C_vector(&indices))\n     }\n \n     if name == \"simd_insert\" {\n         require!(in_elem == arg_tys[2],\n                  \"expected inserted type `{}` (element of input `{}`), found `{}`\",\n                  in_elem, in_ty, arg_tys[2]);\n-        return InsertElement(bcx, llargs[0], llargs[2], llargs[1])\n+        return bcx.insert_element(llargs[0], llargs[2], llargs[1])\n     }\n     if name == \"simd_extract\" {\n         require!(ret_ty == in_elem,\n                  \"expected return type `{}` (element of input `{}`), found `{}`\",\n                  in_elem, in_ty, ret_ty);\n-        return ExtractElement(bcx, llargs[0], llargs[1])\n+        return bcx.extract_element(llargs[0], llargs[1])\n     }\n \n     if name == \"simd_cast\" {\n@@ -1237,34 +1095,34 @@ fn generic_simd_intrinsic<'blk, 'tcx, 'a>\n         match (in_style, out_style) {\n             (Style::Int(in_is_signed), Style::Int(_)) => {\n                 return match in_width.cmp(&out_width) {\n-                    Ordering::Greater => Trunc(bcx, llargs[0], llret_ty),\n+                    Ordering::Greater => bcx.trunc(llargs[0], llret_ty),\n                     Ordering::Equal => llargs[0],\n                     Ordering::Less => if in_is_signed {\n-                        SExt(bcx, llargs[0], llret_ty)\n+                        bcx.sext(llargs[0], llret_ty)\n                     } else {\n-                        ZExt(bcx, llargs[0], llret_ty)\n+                        bcx.zext(llargs[0], llret_ty)\n                     }\n                 }\n             }\n             (Style::Int(in_is_signed), Style::Float) => {\n                 return if in_is_signed {\n-                    SIToFP(bcx, llargs[0], llret_ty)\n+                    bcx.sitofp(llargs[0], llret_ty)\n                 } else {\n-                    UIToFP(bcx, llargs[0], llret_ty)\n+                    bcx.uitofp(llargs[0], llret_ty)\n                 }\n             }\n             (Style::Float, Style::Int(out_is_signed)) => {\n                 return if out_is_signed {\n-                    FPToSI(bcx, llargs[0], llret_ty)\n+                    bcx.fptosi(llargs[0], llret_ty)\n                 } else {\n-                    FPToUI(bcx, llargs[0], llret_ty)\n+                    bcx.fptoui(llargs[0], llret_ty)\n                 }\n             }\n             (Style::Float, Style::Float) => {\n                 return match in_width.cmp(&out_width) {\n-                    Ordering::Greater => FPTrunc(bcx, llargs[0], llret_ty),\n+                    Ordering::Greater => bcx.fptrunc(llargs[0], llret_ty),\n                     Ordering::Equal => llargs[0],\n-                    Ordering::Less => FPExt(bcx, llargs[0], llret_ty)\n+                    Ordering::Less => bcx.fpext(llargs[0], llret_ty)\n                 }\n             }\n             _ => {/* Unsupported. Fallthrough. */}\n@@ -1275,13 +1133,13 @@ fn generic_simd_intrinsic<'blk, 'tcx, 'a>\n                  ret_ty, out_elem);\n     }\n     macro_rules! arith {\n-        ($($name: ident: $($($p: ident),* => $call: expr),*;)*) => {\n+        ($($name: ident: $($($p: ident),* => $call: ident),*;)*) => {\n             $(\n                 if name == stringify!($name) {\n                     match in_elem.sty {\n                         $(\n                             $(ty::$p(_))|* => {\n-                                return $call(bcx, llargs[0], llargs[1], call_debug_location)\n+                                return bcx.$call(llargs[0], llargs[1])\n                             }\n                             )*\n                         _ => {},\n@@ -1294,15 +1152,15 @@ fn generic_simd_intrinsic<'blk, 'tcx, 'a>\n         }\n     }\n     arith! {\n-        simd_add: TyUint, TyInt => Add, TyFloat => FAdd;\n-        simd_sub: TyUint, TyInt => Sub, TyFloat => FSub;\n-        simd_mul: TyUint, TyInt => Mul, TyFloat => FMul;\n-        simd_div: TyFloat => FDiv;\n-        simd_shl: TyUint, TyInt => Shl;\n-        simd_shr: TyUint => LShr, TyInt => AShr;\n-        simd_and: TyUint, TyInt => And;\n-        simd_or: TyUint, TyInt => Or;\n-        simd_xor: TyUint, TyInt => Xor;\n+        simd_add: TyUint, TyInt => add, TyFloat => fadd;\n+        simd_sub: TyUint, TyInt => sub, TyFloat => fsub;\n+        simd_mul: TyUint, TyInt => mul, TyFloat => fmul;\n+        simd_div: TyFloat => fdiv;\n+        simd_shl: TyUint, TyInt => shl;\n+        simd_shr: TyUint => lshr, TyInt => ashr;\n+        simd_and: TyUint, TyInt => and;\n+        simd_or: TyUint, TyInt => or;\n+        simd_xor: TyUint, TyInt => xor;\n     }\n     span_bug!(span, \"unknown SIMD intrinsic\");\n }"}, {"sha": "2fb0e8c24c540de4200c1c78074b316772837125", "filename": "src/librustc_trans/lib.rs", "status": "modified", "additions": 1, "deletions": 2, "changes": 3, "blob_url": "https://github.com/rust-lang/rust/blob/1b38776c1f68c6fd47c1b2f7b7974efc7dd64901/src%2Flibrustc_trans%2Flib.rs", "raw_url": "https://github.com/rust-lang/rust/raw/1b38776c1f68c6fd47c1b2f7b7974efc7dd64901/src%2Flibrustc_trans%2Flib.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_trans%2Flib.rs?ref=1b38776c1f68c6fd47c1b2f7b7974efc7dd64901", "patch": "@@ -36,6 +36,7 @@\n #![feature(slice_patterns)]\n #![feature(staged_api)]\n #![feature(unicode)]\n+#![feature(conservative_impl_trait)]\n \n use rustc::dep_graph::WorkProduct;\n \n@@ -95,8 +96,6 @@ mod asm;\n mod assert_module_sources;\n mod attributes;\n mod base;\n-mod basic_block;\n-mod build;\n mod builder;\n mod cabi_aarch64;\n mod cabi_arm;"}, {"sha": "cf50e7be2afb5c1692b3532ec84b0f33d5d1f3ae", "filename": "src/librustc_trans/meth.rs", "status": "modified", "additions": 37, "deletions": 32, "changes": 69, "blob_url": "https://github.com/rust-lang/rust/blob/1b38776c1f68c6fd47c1b2f7b7974efc7dd64901/src%2Flibrustc_trans%2Fmeth.rs", "raw_url": "https://github.com/rust-lang/rust/raw/1b38776c1f68c6fd47c1b2f7b7974efc7dd64901/src%2Flibrustc_trans%2Fmeth.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_trans%2Fmeth.rs?ref=1b38776c1f68c6fd47c1b2f7b7974efc7dd64901", "patch": "@@ -9,16 +9,11 @@\n // except according to those terms.\n \n use attributes;\n-use arena::TypedArena;\n use llvm::{ValueRef, get_params};\n use rustc::traits;\n-use abi::FnType;\n-use base::*;\n-use build::*;\n-use callee::Callee;\n+use callee::{Callee, CalleeData};\n use common::*;\n use consts;\n-use debuginfo::DebugLoc;\n use declare;\n use glue;\n use machine;\n@@ -32,15 +27,15 @@ use rustc::ty;\n const VTABLE_OFFSET: usize = 3;\n \n /// Extracts a method from a trait object's vtable, at the specified index.\n-pub fn get_virtual_method<'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n-                                      llvtable: ValueRef,\n-                                      vtable_index: usize)\n-                                      -> ValueRef {\n+pub fn get_virtual_method<'a, 'tcx>(bcx: &BlockAndBuilder<'a, 'tcx>,\n+                                    llvtable: ValueRef,\n+                                    vtable_index: usize)\n+                                    -> ValueRef {\n     // Load the data pointer from the object.\n     debug!(\"get_virtual_method(vtable_index={}, llvtable={:?})\",\n            vtable_index, Value(llvtable));\n \n-    Load(bcx, GEPi(bcx, llvtable, &[vtable_index + VTABLE_OFFSET]))\n+    bcx.load(bcx.gepi(llvtable, &[vtable_index + VTABLE_OFFSET]))\n }\n \n /// Generate a shim function that allows an object type like `SomeTrait` to\n@@ -67,36 +62,47 @@ pub fn get_virtual_method<'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n pub fn trans_object_shim<'a, 'tcx>(ccx: &'a CrateContext<'a, 'tcx>,\n                                    callee: Callee<'tcx>)\n                                    -> ValueRef {\n-    let _icx = push_ctxt(\"trans_object_shim\");\n-    let tcx = ccx.tcx();\n-\n     debug!(\"trans_object_shim({:?})\", callee);\n \n-    let (sig, abi, function_name) = match callee.ty.sty {\n-        ty::TyFnDef(def_id, substs, f) => {\n+    let function_name = match callee.ty.sty {\n+        ty::TyFnDef(def_id, substs, _) => {\n             let instance = Instance::new(def_id, substs);\n-            (&f.sig, f.abi, instance.symbol_name(ccx.shared()))\n+            instance.symbol_name(ccx.shared())\n         }\n         _ => bug!()\n     };\n \n-    let sig = tcx.erase_late_bound_regions_and_normalize(sig);\n-    let fn_ty = FnType::new(ccx, abi, &sig, &[]);\n-\n     let llfn = declare::define_internal_fn(ccx, &function_name, callee.ty);\n     attributes::set_frame_pointer_elimination(ccx, llfn);\n \n-    let (block_arena, fcx): (TypedArena<_>, FunctionContext);\n-    block_arena = TypedArena::new();\n-    fcx = FunctionContext::new(ccx, llfn, fn_ty, None, &block_arena);\n-    let mut bcx = fcx.init(false);\n-\n-    let dest = fcx.llretslotptr.get();\n-    let llargs = get_params(fcx.llfn);\n-    bcx = callee.call(bcx, DebugLoc::None,\n-                      &llargs[fcx.fn_ty.ret.is_indirect() as usize..], dest).bcx;\n-\n-    fcx.finish(bcx, DebugLoc::None);\n+    let fcx = FunctionContext::new(ccx, llfn);\n+    let bcx = fcx.get_entry_block();\n+\n+    let mut llargs = get_params(fcx.llfn);\n+    let fn_ret = callee.ty.fn_ret();\n+    let fn_ty = callee.direct_fn_type(ccx, &[]);\n+\n+    let fn_ptr = match callee.data {\n+        CalleeData::Virtual(idx) => {\n+            let fn_ptr = get_virtual_method(&bcx,\n+                llargs.remove(fn_ty.ret.is_indirect() as usize + 1), idx);\n+            let llty = fn_ty.llvm_type(bcx.ccx).ptr_to();\n+            bcx.pointercast(fn_ptr, llty)\n+        },\n+        _ => bug!(\"trans_object_shim called with non-virtual callee\"),\n+    };\n+    let llret = bcx.call(fn_ptr, &llargs, None);\n+    fn_ty.apply_attrs_callsite(llret);\n+\n+    if fn_ret.0.is_never() {\n+        bcx.unreachable();\n+    } else {\n+        if fn_ty.ret.is_indirect() || fn_ty.ret.is_ignore() {\n+            bcx.ret_void();\n+        } else {\n+            bcx.ret(llret);\n+        }\n+    }\n \n     llfn\n }\n@@ -115,7 +121,6 @@ pub fn get_vtable<'a, 'tcx>(ccx: &CrateContext<'a, 'tcx>,\n                             -> ValueRef\n {\n     let tcx = ccx.tcx();\n-    let _icx = push_ctxt(\"meth::get_vtable\");\n \n     debug!(\"get_vtable(ty={:?}, trait_ref={:?})\", ty, trait_ref);\n "}, {"sha": "8df24da7135887e4ecdea99dfcf1467ce2d7bc5b", "filename": "src/librustc_trans/mir/analyze.rs", "status": "modified", "additions": 27, "deletions": 35, "changes": 62, "blob_url": "https://github.com/rust-lang/rust/blob/1b38776c1f68c6fd47c1b2f7b7974efc7dd64901/src%2Flibrustc_trans%2Fmir%2Fanalyze.rs", "raw_url": "https://github.com/rust-lang/rust/raw/1b38776c1f68c6fd47c1b2f7b7974efc7dd64901/src%2Flibrustc_trans%2Fmir%2Fanalyze.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_trans%2Fmir%2Fanalyze.rs?ref=1b38776c1f68c6fd47c1b2f7b7974efc7dd64901", "patch": "@@ -16,31 +16,30 @@ use rustc_data_structures::indexed_vec::{Idx, IndexVec};\n use rustc::mir::{self, Location, TerminatorKind};\n use rustc::mir::visit::{Visitor, LvalueContext};\n use rustc::mir::traversal;\n-use common::{self, Block, BlockAndBuilder};\n-use glue;\n+use common;\n+use super::MirContext;\n use super::rvalue;\n \n-pub fn lvalue_locals<'bcx, 'tcx>(bcx: Block<'bcx,'tcx>,\n-                                 mir: &mir::Mir<'tcx>) -> BitVector {\n-    let bcx = bcx.build();\n-    let mut analyzer = LocalAnalyzer::new(mir, &bcx);\n+pub fn lvalue_locals<'a, 'tcx>(mircx: &MirContext<'a, 'tcx>) -> BitVector {\n+    let mir = mircx.mir;\n+    let mut analyzer = LocalAnalyzer::new(mircx);\n \n     analyzer.visit_mir(mir);\n \n     for (index, ty) in mir.local_decls.iter().map(|l| l.ty).enumerate() {\n-        let ty = bcx.monomorphize(&ty);\n+        let ty = mircx.monomorphize(&ty);\n         debug!(\"local {} has type {:?}\", index, ty);\n         if ty.is_scalar() ||\n             ty.is_unique() ||\n             ty.is_region_ptr() ||\n             ty.is_simd() ||\n-            common::type_is_zero_size(bcx.ccx(), ty)\n+            common::type_is_zero_size(mircx.ccx, ty)\n         {\n             // These sorts of types are immediates that we can store\n             // in an ValueRef without an alloca.\n-            assert!(common::type_is_immediate(bcx.ccx(), ty) ||\n-                    common::type_is_fat_ptr(bcx.tcx(), ty));\n-        } else if common::type_is_imm_pair(bcx.ccx(), ty) {\n+            assert!(common::type_is_immediate(mircx.ccx, ty) ||\n+                    common::type_is_fat_ptr(mircx.ccx, ty));\n+        } else if common::type_is_imm_pair(mircx.ccx, ty) {\n             // We allow pairs and uses of any of their 2 fields.\n         } else {\n             // These sorts of types require an alloca. Note that\n@@ -56,22 +55,18 @@ pub fn lvalue_locals<'bcx, 'tcx>(bcx: Block<'bcx,'tcx>,\n     analyzer.lvalue_locals\n }\n \n-struct LocalAnalyzer<'mir, 'bcx: 'mir, 'tcx: 'bcx> {\n-    mir: &'mir mir::Mir<'tcx>,\n-    bcx: &'mir BlockAndBuilder<'bcx, 'tcx>,\n+struct LocalAnalyzer<'mir, 'a: 'mir, 'tcx: 'a> {\n+    cx: &'mir MirContext<'a, 'tcx>,\n     lvalue_locals: BitVector,\n     seen_assigned: BitVector\n }\n \n-impl<'mir, 'bcx, 'tcx> LocalAnalyzer<'mir, 'bcx, 'tcx> {\n-    fn new(mir: &'mir mir::Mir<'tcx>,\n-           bcx: &'mir BlockAndBuilder<'bcx, 'tcx>)\n-           -> LocalAnalyzer<'mir, 'bcx, 'tcx> {\n+impl<'mir, 'a, 'tcx> LocalAnalyzer<'mir, 'a, 'tcx> {\n+    fn new(mircx: &'mir MirContext<'a, 'tcx>) -> LocalAnalyzer<'mir, 'a, 'tcx> {\n         LocalAnalyzer {\n-            mir: mir,\n-            bcx: bcx,\n-            lvalue_locals: BitVector::new(mir.local_decls.len()),\n-            seen_assigned: BitVector::new(mir.local_decls.len())\n+            cx: mircx,\n+            lvalue_locals: BitVector::new(mircx.mir.local_decls.len()),\n+            seen_assigned: BitVector::new(mircx.mir.local_decls.len())\n         }\n     }\n \n@@ -87,7 +82,7 @@ impl<'mir, 'bcx, 'tcx> LocalAnalyzer<'mir, 'bcx, 'tcx> {\n     }\n }\n \n-impl<'mir, 'bcx, 'tcx> Visitor<'tcx> for LocalAnalyzer<'mir, 'bcx, 'tcx> {\n+impl<'mir, 'a, 'tcx> Visitor<'tcx> for LocalAnalyzer<'mir, 'a, 'tcx> {\n     fn visit_assign(&mut self,\n                     block: mir::BasicBlock,\n                     lvalue: &mir::Lvalue<'tcx>,\n@@ -97,7 +92,7 @@ impl<'mir, 'bcx, 'tcx> Visitor<'tcx> for LocalAnalyzer<'mir, 'bcx, 'tcx> {\n \n         if let mir::Lvalue::Local(index) = *lvalue {\n             self.mark_assigned(index);\n-            if !rvalue::rvalue_creates_operand(self.mir, self.bcx, rvalue) {\n+            if !rvalue::rvalue_creates_operand(rvalue) {\n                 self.mark_as_lvalue(index);\n             }\n         } else {\n@@ -117,7 +112,7 @@ impl<'mir, 'bcx, 'tcx> Visitor<'tcx> for LocalAnalyzer<'mir, 'bcx, 'tcx> {\n                     literal: mir::Literal::Item { def_id, .. }, ..\n                 }),\n                 ref args, ..\n-            } if Some(def_id) == self.bcx.tcx().lang_items.box_free_fn() => {\n+            } if Some(def_id) == self.cx.ccx.tcx().lang_items.box_free_fn() => {\n                 // box_free(x) shares with `drop x` the property that it\n                 // is not guaranteed to be statically dominated by the\n                 // definition of x, so x must always be in an alloca.\n@@ -140,10 +135,10 @@ impl<'mir, 'bcx, 'tcx> Visitor<'tcx> for LocalAnalyzer<'mir, 'bcx, 'tcx> {\n         // Allow uses of projections of immediate pair fields.\n         if let mir::Lvalue::Projection(ref proj) = *lvalue {\n             if let mir::Lvalue::Local(_) = proj.base {\n-                let ty = proj.base.ty(self.mir, self.bcx.tcx());\n+                let ty = proj.base.ty(self.cx.mir, self.cx.ccx.tcx());\n \n-                let ty = self.bcx.monomorphize(&ty.to_ty(self.bcx.tcx()));\n-                if common::type_is_imm_pair(self.bcx.ccx(), ty) {\n+                let ty = self.cx.monomorphize(&ty.to_ty(self.cx.ccx.tcx()));\n+                if common::type_is_imm_pair(self.cx.ccx, ty) {\n                     if let mir::ProjectionElem::Field(..) = proj.elem {\n                         if let LvalueContext::Consume = context {\n                             return;\n@@ -171,11 +166,11 @@ impl<'mir, 'bcx, 'tcx> Visitor<'tcx> for LocalAnalyzer<'mir, 'bcx, 'tcx> {\n                 }\n \n                 LvalueContext::Drop => {\n-                    let ty = lvalue.ty(self.mir, self.bcx.tcx());\n-                    let ty = self.bcx.monomorphize(&ty.to_ty(self.bcx.tcx()));\n+                    let ty = lvalue.ty(self.cx.mir, self.cx.ccx.tcx());\n+                    let ty = self.cx.monomorphize(&ty.to_ty(self.cx.ccx.tcx()));\n \n                     // Only need the lvalue if we're actually dropping it.\n-                    if glue::type_needs_drop(self.bcx.tcx(), ty) {\n+                    if self.cx.ccx.shared().type_needs_drop(ty) {\n                         self.mark_as_lvalue(index);\n                     }\n                 }\n@@ -200,10 +195,7 @@ pub enum CleanupKind {\n     Internal { funclet: mir::BasicBlock }\n }\n \n-pub fn cleanup_kinds<'bcx,'tcx>(_bcx: Block<'bcx,'tcx>,\n-                                mir: &mir::Mir<'tcx>)\n-                                -> IndexVec<mir::BasicBlock, CleanupKind>\n-{\n+pub fn cleanup_kinds<'a, 'tcx>(mir: &mir::Mir<'tcx>) -> IndexVec<mir::BasicBlock, CleanupKind> {\n     fn discover_masters<'tcx>(result: &mut IndexVec<mir::BasicBlock, CleanupKind>,\n                               mir: &mir::Mir<'tcx>) {\n         for (bb, data) in mir.basic_blocks().iter_enumerated() {"}, {"sha": "5ad52b3d252cb1c29089560b4f651257f850c7d5", "filename": "src/librustc_trans/mir/block.rs", "status": "modified", "additions": 191, "deletions": 175, "changes": 366, "blob_url": "https://github.com/rust-lang/rust/blob/1b38776c1f68c6fd47c1b2f7b7974efc7dd64901/src%2Flibrustc_trans%2Fmir%2Fblock.rs", "raw_url": "https://github.com/rust-lang/rust/raw/1b38776c1f68c6fd47c1b2f7b7974efc7dd64901/src%2Flibrustc_trans%2Fmir%2Fblock.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_trans%2Fmir%2Fblock.rs?ref=1b38776c1f68c6fd47c1b2f7b7974efc7dd64901", "patch": "@@ -8,27 +8,26 @@\n // option. This file may not be copied, modified, or distributed\n // except according to those terms.\n \n-use llvm::{self, ValueRef};\n+use llvm::{self, ValueRef, BasicBlockRef};\n use rustc_const_eval::{ErrKind, ConstEvalErr, note_const_eval_err};\n use rustc::middle::lang_items;\n use rustc::ty::{self, layout};\n use rustc::mir;\n use abi::{Abi, FnType, ArgType};\n use adt;\n-use base;\n-use build;\n+use base::{self, Lifetime};\n use callee::{Callee, CalleeData, Fn, Intrinsic, NamedTupleConstructor, Virtual};\n-use common::{self, Block, BlockAndBuilder, LandingPad};\n+use common::{self, BlockAndBuilder, Funclet};\n use common::{C_bool, C_str_slice, C_struct, C_u32, C_undef};\n use consts;\n-use debuginfo::DebugLoc;\n use Disr;\n use machine::{llalign_of_min, llbitsize_of_real};\n use meth;\n use type_of;\n use glue;\n use type_::Type;\n \n+use rustc_data_structures::indexed_vec::IndexVec;\n use rustc_data_structures::fx::FxHashMap;\n use syntax::symbol::Symbol;\n \n@@ -39,21 +38,27 @@ use super::lvalue::{LvalueRef};\n use super::operand::OperandRef;\n use super::operand::OperandValue::{Pair, Ref, Immediate};\n \n-use std::cell::Ref as CellRef;\n+use std::ptr;\n \n-impl<'bcx, 'tcx> MirContext<'bcx, 'tcx> {\n-    pub fn trans_block(&mut self, bb: mir::BasicBlock) {\n-        let mut bcx = self.bcx(bb);\n-        let data = &CellRef::clone(&self.mir)[bb];\n+impl<'a, 'tcx> MirContext<'a, 'tcx> {\n+    pub fn trans_block(&mut self, bb: mir::BasicBlock,\n+        funclets: &IndexVec<mir::BasicBlock, Option<Funclet>>) {\n+        let mut bcx = self.build_block(bb);\n+        let data = &self.mir[bb];\n \n         debug!(\"trans_block({:?}={:?})\", bb, data);\n \n+        let funclet = match self.cleanup_kinds[bb] {\n+            CleanupKind::Internal { funclet } => funclets[funclet].as_ref(),\n+            _ => funclets[bb].as_ref(),\n+        };\n+\n         // Create the cleanup bundle, if needed.\n-        let cleanup_pad = bcx.lpad().and_then(|lp| lp.cleanuppad());\n-        let cleanup_bundle = bcx.lpad().and_then(|l| l.bundle());\n+        let cleanup_pad = funclet.map(|lp| lp.cleanuppad());\n+        let cleanup_bundle = funclet.map(|l| l.bundle());\n \n         let funclet_br = |this: &Self, bcx: BlockAndBuilder, bb: mir::BasicBlock| {\n-            let lltarget = this.blocks[bb].llbb;\n+            let lltarget = this.blocks[bb];\n             if let Some(cp) = cleanup_pad {\n                 match this.cleanup_kinds[bb] {\n                     CleanupKind::Funclet => {\n@@ -70,7 +75,7 @@ impl<'bcx, 'tcx> MirContext<'bcx, 'tcx> {\n         };\n \n         let llblock = |this: &mut Self, target: mir::BasicBlock| {\n-            let lltarget = this.blocks[target].llbb;\n+            let lltarget = this.blocks[target];\n \n             if let Some(cp) = cleanup_pad {\n                 match this.cleanup_kinds[target] {\n@@ -79,8 +84,7 @@ impl<'bcx, 'tcx> MirContext<'bcx, 'tcx> {\n \n                         debug!(\"llblock: creating cleanup trampoline for {:?}\", target);\n                         let name = &format!(\"{:?}_cleanup_trampoline_{:?}\", bb, target);\n-                        let trampoline = this.fcx.new_block(name).build();\n-                        trampoline.set_personality_fn(this.fcx.eh_personality());\n+                        let trampoline = this.fcx.build_new_block(name);\n                         trampoline.cleanup_ret(cp, Some(lltarget));\n                         trampoline.llbb()\n                     }\n@@ -93,7 +97,7 @@ impl<'bcx, 'tcx> MirContext<'bcx, 'tcx> {\n                     (this.cleanup_kinds[bb], this.cleanup_kinds[target])\n                 {\n                     // jump *into* cleanup - need a landing pad if GNU\n-                    this.landing_pad_to(target).llbb\n+                    this.landing_pad_to(target)\n                 } else {\n                     lltarget\n                 }\n@@ -108,23 +112,22 @@ impl<'bcx, 'tcx> MirContext<'bcx, 'tcx> {\n         debug!(\"trans_block: terminator: {:?}\", terminator);\n \n         let span = terminator.source_info.span;\n-        let debug_loc = self.debug_loc(terminator.source_info);\n-        debug_loc.apply_to_bcx(&bcx);\n-        debug_loc.apply(bcx.fcx());\n+        self.set_debug_loc(&bcx, terminator.source_info);\n         match terminator.kind {\n             mir::TerminatorKind::Resume => {\n                 if let Some(cleanup_pad) = cleanup_pad {\n                     bcx.cleanup_ret(cleanup_pad, None);\n                 } else {\n-                    let llpersonality = bcx.fcx().eh_personality();\n-                    bcx.set_personality_fn(llpersonality);\n-\n                     let ps = self.get_personality_slot(&bcx);\n                     let lp = bcx.load(ps);\n-                    bcx.with_block(|bcx| {\n-                        base::call_lifetime_end(bcx, ps);\n-                        base::trans_unwind_resume(bcx, lp);\n-                    });\n+                    Lifetime::End.call(&bcx, ps);\n+                    if !bcx.sess().target.target.options.custom_unwind_resume {\n+                        bcx.resume(lp);\n+                    } else {\n+                        let exc_ptr = bcx.extract_value(lp, 0);\n+                        bcx.call(bcx.ccx.eh_unwind_resume(), &[exc_ptr], cleanup_bundle);\n+                        bcx.unreachable();\n+                    }\n                 }\n             }\n \n@@ -143,9 +146,7 @@ impl<'bcx, 'tcx> MirContext<'bcx, 'tcx> {\n             mir::TerminatorKind::Switch { ref discr, ref adt_def, ref targets } => {\n                 let discr_lvalue = self.trans_lvalue(&bcx, discr);\n                 let ty = discr_lvalue.ty.to_ty(bcx.tcx());\n-                let discr = bcx.with_block(|bcx|\n-                    adt::trans_get_discr(bcx, ty, discr_lvalue.llval, None, true)\n-                );\n+                let discr = adt::trans_get_discr(&bcx, ty, discr_lvalue.llval, None, true);\n \n                 let mut bb_hist = FxHashMap();\n                 for target in targets {\n@@ -162,34 +163,33 @@ impl<'bcx, 'tcx> MirContext<'bcx, 'tcx> {\n                     // We're generating an exhaustive switch, so the else branch\n                     // can't be hit.  Branching to an unreachable instruction\n                     // lets LLVM know this\n-                    _ => (None, self.unreachable_block().llbb)\n+                    _ => (None, self.unreachable_block())\n                 };\n                 let switch = bcx.switch(discr, default_blk, targets.len());\n                 assert_eq!(adt_def.variants.len(), targets.len());\n                 for (adt_variant, &target) in adt_def.variants.iter().zip(targets) {\n                     if default_bb != Some(target) {\n                         let llbb = llblock(self, target);\n-                        let llval = bcx.with_block(|bcx| adt::trans_case(\n-                                bcx, ty, Disr::from(adt_variant.disr_val)));\n-                        build::AddCase(switch, llval, llbb)\n+                        let llval = adt::trans_case(&bcx, ty, Disr::from(adt_variant.disr_val));\n+                        bcx.add_case(switch, llval, llbb)\n                     }\n                 }\n             }\n \n             mir::TerminatorKind::SwitchInt { ref discr, switch_ty, ref values, ref targets } => {\n                 let (otherwise, targets) = targets.split_last().unwrap();\n                 let discr = bcx.load(self.trans_lvalue(&bcx, discr).llval);\n-                let discr = bcx.with_block(|bcx| base::to_immediate(bcx, discr, switch_ty));\n+                let discr = base::to_immediate(&bcx, discr, switch_ty);\n                 let switch = bcx.switch(discr, llblock(self, *otherwise), values.len());\n                 for (value, target) in values.iter().zip(targets) {\n-                    let val = Const::from_constval(bcx.ccx(), value.clone(), switch_ty);\n+                    let val = Const::from_constval(bcx.ccx, value.clone(), switch_ty);\n                     let llbb = llblock(self, *target);\n-                    build::AddCase(switch, val.llval, llbb)\n+                    bcx.add_case(switch, val.llval, llbb)\n                 }\n             }\n \n             mir::TerminatorKind::Return => {\n-                let ret = bcx.fcx().fn_ty.ret;\n+                let ret = self.fn_ty.ret;\n                 if ret.is_ignore() || ret.is_indirect() {\n                     bcx.ret_void();\n                     return;\n@@ -208,14 +208,14 @@ impl<'bcx, 'tcx> MirContext<'bcx, 'tcx> {\n                     };\n                     let llslot = match op.val {\n                         Immediate(_) | Pair(..) => {\n-                            let llscratch = build::AllocaFcx(bcx.fcx(), ret.original_ty, \"ret\");\n+                            let llscratch = bcx.fcx().alloca(ret.original_ty, \"ret\");\n                             self.store_operand(&bcx, llscratch, op);\n                             llscratch\n                         }\n                         Ref(llval) => llval\n                     };\n                     let load = bcx.load(bcx.pointercast(llslot, cast_ty.ptr_to()));\n-                    let llalign = llalign_of_min(bcx.ccx(), ret.ty);\n+                    let llalign = llalign_of_min(bcx.ccx, ret.ty);\n                     unsafe {\n                         llvm::LLVMSetAlignment(load, llalign);\n                     }\n@@ -233,21 +233,21 @@ impl<'bcx, 'tcx> MirContext<'bcx, 'tcx> {\n \n             mir::TerminatorKind::Drop { ref location, target, unwind } => {\n                 let ty = location.ty(&self.mir, bcx.tcx()).to_ty(bcx.tcx());\n-                let ty = bcx.monomorphize(&ty);\n+                let ty = self.monomorphize(&ty);\n \n                 // Double check for necessity to drop\n-                if !glue::type_needs_drop(bcx.tcx(), ty) {\n+                if !bcx.ccx.shared().type_needs_drop(ty) {\n                     funclet_br(self, bcx, target);\n                     return;\n                 }\n \n                 let lvalue = self.trans_lvalue(&bcx, location);\n-                let drop_fn = glue::get_drop_glue(bcx.ccx(), ty);\n-                let drop_ty = glue::get_drop_glue_type(bcx.tcx(), ty);\n-                let is_sized = common::type_is_sized(bcx.tcx(), ty);\n+                let drop_fn = glue::get_drop_glue(bcx.ccx, ty);\n+                let drop_ty = glue::get_drop_glue_type(bcx.ccx.shared(), ty);\n+                let is_sized = bcx.ccx.shared().type_is_sized(ty);\n                 let llvalue = if is_sized {\n                     if drop_ty != ty {\n-                        bcx.pointercast(lvalue.llval, type_of::type_of(bcx.ccx(), drop_ty).ptr_to())\n+                        bcx.pointercast(lvalue.llval, type_of::type_of(bcx.ccx, drop_ty).ptr_to())\n                     } else {\n                         lvalue.llval\n                     }\n@@ -259,18 +259,16 @@ impl<'bcx, 'tcx> MirContext<'bcx, 'tcx> {\n                     // but I am shooting for a quick fix to #35546\n                     // here that can be cleanly backported to beta, so\n                     // I want to avoid touching all of trans.\n-                    bcx.with_block(|bcx| {\n-                        let scratch = base::alloc_ty(bcx, ty, \"drop\");\n-                        base::call_lifetime_start(bcx, scratch);\n-                        build::Store(bcx, lvalue.llval, base::get_dataptr(bcx, scratch));\n-                        build::Store(bcx, lvalue.llextra, base::get_meta(bcx, scratch));\n-                        scratch\n-                    })\n+                    let scratch = base::alloc_ty(&bcx, ty, \"drop\");\n+                    Lifetime::Start.call(&bcx, scratch);\n+                    bcx.store(lvalue.llval, base::get_dataptr(&bcx, scratch));\n+                    bcx.store(lvalue.llextra, base::get_meta(&bcx, scratch));\n+                    scratch\n                 };\n                 if let Some(unwind) = unwind {\n                     bcx.invoke(drop_fn,\n                                &[llvalue],\n-                               self.blocks[target].llbb,\n+                               self.blocks[target],\n                                llblock(self, unwind),\n                                cleanup_bundle);\n                 } else {\n@@ -290,7 +288,7 @@ impl<'bcx, 'tcx> MirContext<'bcx, 'tcx> {\n                 // NOTE: Unlike binops, negation doesn't have its own\n                 // checked operation, just a comparison with the minimum\n                 // value, so we have to check for the assert message.\n-                if !bcx.ccx().check_overflow() {\n+                if !bcx.ccx.check_overflow() {\n                     use rustc_const_math::ConstMathErr::Overflow;\n                     use rustc_const_math::Op::Neg;\n \n@@ -306,27 +304,27 @@ impl<'bcx, 'tcx> MirContext<'bcx, 'tcx> {\n                 }\n \n                 // Pass the condition through llvm.expect for branch hinting.\n-                let expect = bcx.ccx().get_intrinsic(&\"llvm.expect.i1\");\n-                let cond = bcx.call(expect, &[cond, C_bool(bcx.ccx(), expected)], None);\n+                let expect = bcx.ccx.get_intrinsic(&\"llvm.expect.i1\");\n+                let cond = bcx.call(expect, &[cond, C_bool(bcx.ccx, expected)], None);\n \n                 // Create the failure block and the conditional branch to it.\n                 let lltarget = llblock(self, target);\n-                let panic_block = self.fcx.new_block(\"panic\");\n+                let panic_block = self.fcx.build_new_block(\"panic\");\n                 if expected {\n-                    bcx.cond_br(cond, lltarget, panic_block.llbb);\n+                    bcx.cond_br(cond, lltarget, panic_block.llbb());\n                 } else {\n-                    bcx.cond_br(cond, panic_block.llbb, lltarget);\n+                    bcx.cond_br(cond, panic_block.llbb(), lltarget);\n                 }\n \n                 // After this point, bcx is the block for the call to panic.\n-                bcx = panic_block.build();\n-                debug_loc.apply_to_bcx(&bcx);\n+                bcx = panic_block;\n+                self.set_debug_loc(&bcx, terminator.source_info);\n \n                 // Get the location information.\n                 let loc = bcx.sess().codemap().lookup_char_pos(span.lo);\n                 let filename = Symbol::intern(&loc.file.name).as_str();\n-                let filename = C_str_slice(bcx.ccx(), filename);\n-                let line = C_u32(bcx.ccx(), loc.line as u32);\n+                let filename = C_str_slice(bcx.ccx, filename);\n+                let line = C_u32(bcx.ccx, loc.line as u32);\n \n                 // Put together the arguments to the panic entry point.\n                 let (lang_item, args, const_err) = match *msg {\n@@ -343,9 +341,9 @@ impl<'bcx, 'tcx> MirContext<'bcx, 'tcx> {\n                             })\n                         });\n \n-                        let file_line = C_struct(bcx.ccx(), &[filename, line], false);\n-                        let align = llalign_of_min(bcx.ccx(), common::val_ty(file_line));\n-                        let file_line = consts::addr_of(bcx.ccx(),\n+                        let file_line = C_struct(bcx.ccx, &[filename, line], false);\n+                        let align = llalign_of_min(bcx.ccx, common::val_ty(file_line));\n+                        let file_line = consts::addr_of(bcx.ccx,\n                                                         file_line,\n                                                         align,\n                                                         \"panic_bounds_check_loc\");\n@@ -355,12 +353,12 @@ impl<'bcx, 'tcx> MirContext<'bcx, 'tcx> {\n                     }\n                     mir::AssertMessage::Math(ref err) => {\n                         let msg_str = Symbol::intern(err.description()).as_str();\n-                        let msg_str = C_str_slice(bcx.ccx(), msg_str);\n-                        let msg_file_line = C_struct(bcx.ccx(),\n+                        let msg_str = C_str_slice(bcx.ccx, msg_str);\n+                        let msg_file_line = C_struct(bcx.ccx,\n                                                      &[msg_str, filename, line],\n                                                      false);\n-                        let align = llalign_of_min(bcx.ccx(), common::val_ty(msg_file_line));\n-                        let msg_file_line = consts::addr_of(bcx.ccx(),\n+                        let align = llalign_of_min(bcx.ccx, common::val_ty(msg_file_line));\n+                        let msg_file_line = consts::addr_of(bcx.ccx,\n                                                             msg_file_line,\n                                                             align,\n                                                             \"panic_loc\");\n@@ -384,15 +382,15 @@ impl<'bcx, 'tcx> MirContext<'bcx, 'tcx> {\n \n                 // Obtain the panic entry point.\n                 let def_id = common::langcall(bcx.tcx(), Some(span), \"\", lang_item);\n-                let callee = Callee::def(bcx.ccx(), def_id,\n-                    bcx.ccx().empty_substs_for_def_id(def_id));\n-                let llfn = callee.reify(bcx.ccx());\n+                let callee = Callee::def(bcx.ccx, def_id,\n+                    bcx.ccx.empty_substs_for_def_id(def_id));\n+                let llfn = callee.reify(bcx.ccx);\n \n                 // Translate the actual panic invoke/call.\n                 if let Some(unwind) = cleanup {\n                     bcx.invoke(llfn,\n                                &args,\n-                               self.unreachable_block().llbb,\n+                               self.unreachable_block(),\n                                llblock(self, unwind),\n                                cleanup_bundle);\n                 } else {\n@@ -411,7 +409,7 @@ impl<'bcx, 'tcx> MirContext<'bcx, 'tcx> {\n \n                 let (mut callee, abi, sig) = match callee.ty.sty {\n                     ty::TyFnDef(def_id, substs, f) => {\n-                        (Callee::def(bcx.ccx(), def_id, substs), f.abi, &f.sig)\n+                        (Callee::def(bcx.ccx, def_id, substs), f.abi, &f.sig)\n                     }\n                     ty::TyFnPtr(f) => {\n                         (Callee {\n@@ -443,6 +441,65 @@ impl<'bcx, 'tcx> MirContext<'bcx, 'tcx> {\n                     return;\n                 }\n \n+                // FIXME: This should proxy to the drop glue in the future when the ABI matches;\n+                // most of the below code was copied from the match arm for TerminatorKind::Drop.\n+                if intrinsic == Some(\"drop_in_place\") {\n+                    let &(_, target) = destination.as_ref().unwrap();\n+                    let ty = if let ty::TyFnDef(_, substs, _) = callee.ty.sty {\n+                        substs.type_at(0)\n+                    } else {\n+                        bug!(\"Unexpected ty: {}\", callee.ty);\n+                    };\n+\n+                    // Double check for necessity to drop\n+                    if !bcx.ccx.shared().type_needs_drop(ty) {\n+                        funclet_br(self, bcx, target);\n+                        return;\n+                    }\n+\n+                    let ptr = self.trans_operand(&bcx, &args[0]);\n+                    let (llval, llextra) = match ptr.val {\n+                        Immediate(llptr) => (llptr, ptr::null_mut()),\n+                        Pair(llptr, llextra) => (llptr, llextra),\n+                        Ref(_) => bug!(\"Deref of by-Ref type {:?}\", ptr.ty)\n+                    };\n+\n+                    let drop_fn = glue::get_drop_glue(bcx.ccx, ty);\n+                    let drop_ty = glue::get_drop_glue_type(bcx.ccx.shared(), ty);\n+                    let is_sized = bcx.ccx.shared().type_is_sized(ty);\n+                    let llvalue = if is_sized {\n+                        if drop_ty != ty {\n+                            bcx.pointercast(llval, type_of::type_of(bcx.ccx, drop_ty).ptr_to())\n+                        } else {\n+                            llval\n+                        }\n+                    } else {\n+                        // FIXME(#36457) Currently drop glue takes sized\n+                        // values as a `*(data, meta)`, but elsewhere in\n+                        // MIR we pass `(data, meta)` as two separate\n+                        // arguments. It would be better to fix drop glue,\n+                        // but I am shooting for a quick fix to #35546\n+                        // here that can be cleanly backported to beta, so\n+                        // I want to avoid touching all of trans.\n+                        let scratch = base::alloc_ty(&bcx, ty, \"drop\");\n+                        Lifetime::Start.call(&bcx, scratch);\n+                        bcx.store(llval, base::get_dataptr(&bcx, scratch));\n+                        bcx.store(llextra, base::get_meta(&bcx, scratch));\n+                        scratch\n+                    };\n+                    if let Some(unwind) = *cleanup {\n+                        bcx.invoke(drop_fn,\n+                            &[llvalue],\n+                            self.blocks[target],\n+                            llblock(self, unwind),\n+                            cleanup_bundle);\n+                    } else {\n+                        bcx.call(drop_fn, &[llvalue], cleanup_bundle);\n+                        funclet_br(self, bcx, target);\n+                    }\n+                    return;\n+                }\n+\n                 if intrinsic == Some(\"transmute\") {\n                     let &(ref dest, target) = destination.as_ref().unwrap();\n                     self.with_lvalue_ref(&bcx, dest, |this, dest| {\n@@ -456,9 +513,9 @@ impl<'bcx, 'tcx> MirContext<'bcx, 'tcx> {\n                 let extra_args = &args[sig.inputs().len()..];\n                 let extra_args = extra_args.iter().map(|op_arg| {\n                     let op_ty = op_arg.ty(&self.mir, bcx.tcx());\n-                    bcx.monomorphize(&op_ty)\n+                    self.monomorphize(&op_ty)\n                 }).collect::<Vec<_>>();\n-                let fn_ty = callee.direct_fn_type(bcx.ccx(), &extra_args);\n+                let fn_ty = callee.direct_fn_type(bcx.ccx, &extra_args);\n \n                 // The arguments we'll be passing. Plus one to account for outptr, if used.\n                 let arg_count = fn_ty.args.len() + fn_ty.ret.is_indirect() as usize;\n@@ -519,7 +576,7 @@ impl<'bcx, 'tcx> MirContext<'bcx, 'tcx> {\n                 let fn_ptr = match callee.data {\n                     NamedTupleConstructor(_) => {\n                         // FIXME translate this like mir::Rvalue::Aggregate.\n-                        callee.reify(bcx.ccx())\n+                        callee.reify(bcx.ccx)\n                     }\n                     Intrinsic => {\n                         use intrinsic::trans_intrinsic_call;\n@@ -537,10 +594,8 @@ impl<'bcx, 'tcx> MirContext<'bcx, 'tcx> {\n                                 bug!(\"Cannot use direct operand with an intrinsic call\")\n                         };\n \n-                        bcx.with_block(|bcx| {\n-                            trans_intrinsic_call(bcx, callee.ty, &fn_ty,\n-                                                 &llargs, dest, debug_loc);\n-                        });\n+                        trans_intrinsic_call(&bcx, callee.ty, &fn_ty, &llargs, dest,\n+                            terminator.source_info.span);\n \n                         if let ReturnDest::IndirectOperand(dst, _) = ret_dest {\n                             // Make a fake operand for store_return\n@@ -554,8 +609,7 @@ impl<'bcx, 'tcx> MirContext<'bcx, 'tcx> {\n                         if let Some((_, target)) = *destination {\n                             funclet_br(self, bcx, target);\n                         } else {\n-                            // trans_intrinsic_call already used Unreachable.\n-                            // bcx.unreachable();\n+                            bcx.unreachable();\n                         }\n \n                         return;\n@@ -573,15 +627,15 @@ impl<'bcx, 'tcx> MirContext<'bcx, 'tcx> {\n                     };\n                     let invokeret = bcx.invoke(fn_ptr,\n                                                &llargs,\n-                                               ret_bcx.llbb,\n+                                               ret_bcx,\n                                                llblock(self, cleanup),\n                                                cleanup_bundle);\n                     fn_ty.apply_attrs_callsite(invokeret);\n \n-                    if destination.is_some() {\n-                        let ret_bcx = ret_bcx.build();\n+                    if let Some((_, target)) = *destination {\n+                        let ret_bcx = self.build_block(target);\n                         ret_bcx.at_start(|ret_bcx| {\n-                            debug_loc.apply_to_bcx(ret_bcx);\n+                            self.set_debug_loc(&ret_bcx, terminator.source_info);\n                             let op = OperandRef {\n                                 val: Immediate(invokeret),\n                                 ty: sig.output(),\n@@ -608,22 +662,20 @@ impl<'bcx, 'tcx> MirContext<'bcx, 'tcx> {\n     }\n \n     fn trans_argument(&mut self,\n-                      bcx: &BlockAndBuilder<'bcx, 'tcx>,\n+                      bcx: &BlockAndBuilder<'a, 'tcx>,\n                       op: OperandRef<'tcx>,\n                       llargs: &mut Vec<ValueRef>,\n                       fn_ty: &FnType,\n                       next_idx: &mut usize,\n                       callee: &mut CalleeData) {\n         if let Pair(a, b) = op.val {\n             // Treat the values in a fat pointer separately.\n-            if common::type_is_fat_ptr(bcx.tcx(), op.ty) {\n+            if common::type_is_fat_ptr(bcx.ccx, op.ty) {\n                 let (ptr, meta) = (a, b);\n                 if *next_idx == 0 {\n                     if let Virtual(idx) = *callee {\n-                        let llfn = bcx.with_block(|bcx| {\n-                            meth::get_virtual_method(bcx, meta, idx)\n-                        });\n-                        let llty = fn_ty.llvm_type(bcx.ccx()).ptr_to();\n+                        let llfn = meth::get_virtual_method(bcx, meta, idx);\n+                        let llty = fn_ty.llvm_type(bcx.ccx).ptr_to();\n                         *callee = Fn(bcx.pointercast(llfn, llty));\n                     }\n                 }\n@@ -655,7 +707,7 @@ impl<'bcx, 'tcx> MirContext<'bcx, 'tcx> {\n         let (mut llval, by_ref) = match op.val {\n             Immediate(_) | Pair(..) => {\n                 if arg.is_indirect() || arg.cast.is_some() {\n-                    let llscratch = build::AllocaFcx(bcx.fcx(), arg.original_ty, \"arg\");\n+                    let llscratch = bcx.fcx().alloca(arg.original_ty, \"arg\");\n                     self.store_operand(bcx, llscratch, op);\n                     (llscratch, true)\n                 } else {\n@@ -667,13 +719,13 @@ impl<'bcx, 'tcx> MirContext<'bcx, 'tcx> {\n \n         if by_ref && !arg.is_indirect() {\n             // Have to load the argument, maybe while casting it.\n-            if arg.original_ty == Type::i1(bcx.ccx()) {\n+            if arg.original_ty == Type::i1(bcx.ccx) {\n                 // We store bools as i8 so we need to truncate to i1.\n                 llval = bcx.load_range_assert(llval, 0, 2, llvm::False);\n                 llval = bcx.trunc(llval, arg.original_ty);\n             } else if let Some(ty) = arg.cast {\n                 llval = bcx.load(bcx.pointercast(llval, ty.ptr_to()));\n-                let llalign = llalign_of_min(bcx.ccx(), arg.ty);\n+                let llalign = llalign_of_min(bcx.ccx, arg.ty);\n                 unsafe {\n                     llvm::LLVMSetAlignment(llval, llalign);\n                 }\n@@ -686,7 +738,7 @@ impl<'bcx, 'tcx> MirContext<'bcx, 'tcx> {\n     }\n \n     fn trans_arguments_untupled(&mut self,\n-                                bcx: &BlockAndBuilder<'bcx, 'tcx>,\n+                                bcx: &BlockAndBuilder<'a, 'tcx>,\n                                 operand: &mir::Operand<'tcx>,\n                                 llargs: &mut Vec<ValueRef>,\n                                 fn_ty: &FnType,\n@@ -705,9 +757,9 @@ impl<'bcx, 'tcx> MirContext<'bcx, 'tcx> {\n             Ref(llval) => {\n                 let base = adt::MaybeSizedValue::sized(llval);\n                 for (n, &ty) in arg_types.iter().enumerate() {\n-                    let ptr = adt::trans_field_ptr_builder(bcx, tuple.ty, base, Disr(0), n);\n-                    let val = if common::type_is_fat_ptr(bcx.tcx(), ty) {\n-                        let (lldata, llextra) = base::load_fat_ptr_builder(bcx, ptr, ty);\n+                    let ptr = adt::trans_field_ptr(bcx, tuple.ty, base, Disr(0), n);\n+                    let val = if common::type_is_fat_ptr(bcx.ccx, ty) {\n+                        let (lldata, llextra) = base::load_fat_ptr(bcx, ptr, ty);\n                         Pair(lldata, llextra)\n                     } else {\n                         // trans_argument will load this if it needs to\n@@ -722,7 +774,7 @@ impl<'bcx, 'tcx> MirContext<'bcx, 'tcx> {\n \n             }\n             Immediate(llval) => {\n-                let l = bcx.ccx().layout_of(tuple.ty);\n+                let l = bcx.ccx.layout_of(tuple.ty);\n                 let v = if let layout::Univariant { ref variant, .. } = *l {\n                     variant\n                 } else {\n@@ -731,8 +783,8 @@ impl<'bcx, 'tcx> MirContext<'bcx, 'tcx> {\n                 for (n, &ty) in arg_types.iter().enumerate() {\n                     let mut elem = bcx.extract_value(llval, v.memory_index[n] as usize);\n                     // Truncate bools to i1, if needed\n-                    if ty.is_bool() && common::val_ty(elem) != Type::i1(bcx.ccx()) {\n-                        elem = bcx.trunc(elem, Type::i1(bcx.ccx()));\n+                    if ty.is_bool() && common::val_ty(elem) != Type::i1(bcx.ccx) {\n+                        elem = bcx.trunc(elem, Type::i1(bcx.ccx));\n                     }\n                     // If the tuple is immediate, the elements are as well\n                     let op = OperandRef {\n@@ -747,8 +799,8 @@ impl<'bcx, 'tcx> MirContext<'bcx, 'tcx> {\n                 for (n, &ty) in arg_types.iter().enumerate() {\n                     let mut elem = elems[n];\n                     // Truncate bools to i1, if needed\n-                    if ty.is_bool() && common::val_ty(elem) != Type::i1(bcx.ccx()) {\n-                        elem = bcx.trunc(elem, Type::i1(bcx.ccx()));\n+                    if ty.is_bool() && common::val_ty(elem) != Type::i1(bcx.ccx) {\n+                        elem = bcx.trunc(elem, Type::i1(bcx.ccx));\n                     }\n                     // Pair is always made up of immediates\n                     let op = OperandRef {\n@@ -762,91 +814,61 @@ impl<'bcx, 'tcx> MirContext<'bcx, 'tcx> {\n \n     }\n \n-    fn get_personality_slot(&mut self, bcx: &BlockAndBuilder<'bcx, 'tcx>) -> ValueRef {\n-        let ccx = bcx.ccx();\n+    fn get_personality_slot(&mut self, bcx: &BlockAndBuilder<'a, 'tcx>) -> ValueRef {\n+        let ccx = bcx.ccx;\n         if let Some(slot) = self.llpersonalityslot {\n             slot\n         } else {\n             let llretty = Type::struct_(ccx, &[Type::i8p(ccx), Type::i32(ccx)], false);\n-            bcx.with_block(|bcx| {\n-                let slot = base::alloca(bcx, llretty, \"personalityslot\");\n-                self.llpersonalityslot = Some(slot);\n-                base::call_lifetime_start(bcx, slot);\n-                slot\n-            })\n+            let slot = bcx.fcx().alloca(llretty, \"personalityslot\");\n+            self.llpersonalityslot = Some(slot);\n+            Lifetime::Start.call(bcx, slot);\n+            slot\n         }\n     }\n \n     /// Return the landingpad wrapper around the given basic block\n     ///\n     /// No-op in MSVC SEH scheme.\n-    fn landing_pad_to(&mut self, target_bb: mir::BasicBlock) -> Block<'bcx, 'tcx>\n-    {\n+    fn landing_pad_to(&mut self, target_bb: mir::BasicBlock) -> BasicBlockRef {\n         if let Some(block) = self.landing_pads[target_bb] {\n             return block;\n         }\n \n-        if base::wants_msvc_seh(self.fcx.ccx.sess()) {\n+        if base::wants_msvc_seh(self.ccx.sess()) {\n             return self.blocks[target_bb];\n         }\n \n-        let target = self.bcx(target_bb);\n+        let target = self.build_block(target_bb);\n \n-        let block = self.fcx.new_block(\"cleanup\");\n-        self.landing_pads[target_bb] = Some(block);\n+        let bcx = self.fcx.build_new_block(\"cleanup\");\n+        self.landing_pads[target_bb] = Some(bcx.llbb());\n \n-        let bcx = block.build();\n-        let ccx = bcx.ccx();\n-        let llpersonality = self.fcx.eh_personality();\n+        let ccx = bcx.ccx;\n+        let llpersonality = self.ccx.eh_personality();\n         let llretty = Type::struct_(ccx, &[Type::i8p(ccx), Type::i32(ccx)], false);\n         let llretval = bcx.landing_pad(llretty, llpersonality, 1, self.fcx.llfn);\n         bcx.set_cleanup(llretval);\n         let slot = self.get_personality_slot(&bcx);\n         bcx.store(llretval, slot);\n         bcx.br(target.llbb());\n-        block\n+        bcx.llbb()\n     }\n \n-    pub fn init_cpad(&mut self, bb: mir::BasicBlock) {\n-        let bcx = self.bcx(bb);\n-        let data = &self.mir[bb];\n-        debug!(\"init_cpad({:?})\", data);\n-\n-        match self.cleanup_kinds[bb] {\n-            CleanupKind::NotCleanup => {\n-                bcx.set_lpad(None)\n-            }\n-            _ if !base::wants_msvc_seh(bcx.sess()) => {\n-                bcx.set_lpad(Some(LandingPad::gnu()))\n-            }\n-            CleanupKind::Internal { funclet } => {\n-                // FIXME: is this needed?\n-                bcx.set_personality_fn(self.fcx.eh_personality());\n-                bcx.set_lpad_ref(self.bcx(funclet).lpad());\n-            }\n-            CleanupKind::Funclet => {\n-                bcx.set_personality_fn(self.fcx.eh_personality());\n-                DebugLoc::None.apply_to_bcx(&bcx);\n-                let cleanup_pad = bcx.cleanup_pad(None, &[]);\n-                bcx.set_lpad(Some(LandingPad::msvc(cleanup_pad)));\n-            }\n-        };\n-    }\n-\n-    fn unreachable_block(&mut self) -> Block<'bcx, 'tcx> {\n+    fn unreachable_block(&mut self) -> BasicBlockRef {\n         self.unreachable_block.unwrap_or_else(|| {\n-            let bl = self.fcx.new_block(\"unreachable\");\n-            bl.build().unreachable();\n-            self.unreachable_block = Some(bl);\n-            bl\n+            let bl = self.fcx.build_new_block(\"unreachable\");\n+            bl.unreachable();\n+            self.unreachable_block = Some(bl.llbb());\n+            bl.llbb()\n         })\n     }\n \n-    fn bcx(&self, bb: mir::BasicBlock) -> BlockAndBuilder<'bcx, 'tcx> {\n-        self.blocks[bb].build()\n+    pub fn build_block(&self, bb: mir::BasicBlock) -> BlockAndBuilder<'a, 'tcx> {\n+        BlockAndBuilder::new(self.blocks[bb], self.fcx)\n     }\n \n-    fn make_return_dest(&mut self, bcx: &BlockAndBuilder<'bcx, 'tcx>,\n+    fn make_return_dest(&mut self, bcx: &BlockAndBuilder<'a, 'tcx>,\n                         dest: &mir::Lvalue<'tcx>, fn_ret_ty: &ArgType,\n                         llargs: &mut Vec<ValueRef>, is_intrinsic: bool) -> ReturnDest {\n         // If the return is ignored, we can just return a do-nothing ReturnDest\n@@ -863,18 +885,14 @@ impl<'bcx, 'tcx> MirContext<'bcx, 'tcx> {\n                     return if fn_ret_ty.is_indirect() {\n                         // Odd, but possible, case, we have an operand temporary,\n                         // but the calling convention has an indirect return.\n-                        let tmp = bcx.with_block(|bcx| {\n-                            base::alloc_ty(bcx, ret_ty, \"tmp_ret\")\n-                        });\n+                        let tmp = base::alloc_ty(bcx, ret_ty, \"tmp_ret\");\n                         llargs.push(tmp);\n                         ReturnDest::IndirectOperand(tmp, index)\n                     } else if is_intrinsic {\n                         // Currently, intrinsics always need a location to store\n                         // the result. so we create a temporary alloca for the\n                         // result\n-                        let tmp = bcx.with_block(|bcx| {\n-                            base::alloc_ty(bcx, ret_ty, \"tmp_ret\")\n-                        });\n+                        let tmp = base::alloc_ty(bcx, ret_ty, \"tmp_ret\");\n                         ReturnDest::IndirectOperand(tmp, index)\n                     } else {\n                         ReturnDest::DirectOperand(index)\n@@ -895,35 +913,35 @@ impl<'bcx, 'tcx> MirContext<'bcx, 'tcx> {\n         }\n     }\n \n-    fn trans_transmute(&mut self, bcx: &BlockAndBuilder<'bcx, 'tcx>,\n+    fn trans_transmute(&mut self, bcx: &BlockAndBuilder<'a, 'tcx>,\n                        src: &mir::Operand<'tcx>, dst: LvalueRef<'tcx>) {\n         let mut val = self.trans_operand(bcx, src);\n         if let ty::TyFnDef(def_id, substs, _) = val.ty.sty {\n-            let llouttype = type_of::type_of(bcx.ccx(), dst.ty.to_ty(bcx.tcx()));\n-            let out_type_size = llbitsize_of_real(bcx.ccx(), llouttype);\n+            let llouttype = type_of::type_of(bcx.ccx, dst.ty.to_ty(bcx.tcx()));\n+            let out_type_size = llbitsize_of_real(bcx.ccx, llouttype);\n             if out_type_size != 0 {\n                 // FIXME #19925 Remove this hack after a release cycle.\n-                let f = Callee::def(bcx.ccx(), def_id, substs);\n+                let f = Callee::def(bcx.ccx, def_id, substs);\n                 let ty = match f.ty.sty {\n                     ty::TyFnDef(.., f) => bcx.tcx().mk_fn_ptr(f),\n                     _ => f.ty\n                 };\n                 val = OperandRef {\n-                    val: Immediate(f.reify(bcx.ccx())),\n+                    val: Immediate(f.reify(bcx.ccx)),\n                     ty: ty\n                 };\n             }\n         }\n \n-        let llty = type_of::type_of(bcx.ccx(), val.ty);\n+        let llty = type_of::type_of(bcx.ccx, val.ty);\n         let cast_ptr = bcx.pointercast(dst.llval, llty.ptr_to());\n         self.store_operand(bcx, cast_ptr, val);\n     }\n \n \n     // Stores the return value of a function call into it's final location.\n     fn store_return(&mut self,\n-                    bcx: &BlockAndBuilder<'bcx, 'tcx>,\n+                    bcx: &BlockAndBuilder<'a, 'tcx>,\n                     dest: ReturnDest,\n                     ret_ty: ArgType,\n                     op: OperandRef<'tcx>) {\n@@ -939,9 +957,7 @@ impl<'bcx, 'tcx> MirContext<'bcx, 'tcx> {\n             DirectOperand(index) => {\n                 // If there is a cast, we have to store and reload.\n                 let op = if ret_ty.cast.is_some() {\n-                    let tmp = bcx.with_block(|bcx| {\n-                        base::alloc_ty(bcx, op.ty, \"tmp_ret\")\n-                    });\n+                    let tmp = base::alloc_ty(bcx, op.ty, \"tmp_ret\");\n                     ret_ty.store(bcx, op.immediate(), tmp);\n                     self.trans_load(bcx, tmp, op.ty)\n                 } else {"}, {"sha": "08f68f8d49c78a8465ff7c5ec56d239a10e0b729", "filename": "src/librustc_trans/mir/constant.rs", "status": "modified", "additions": 18, "deletions": 19, "changes": 37, "blob_url": "https://github.com/rust-lang/rust/blob/1b38776c1f68c6fd47c1b2f7b7974efc7dd64901/src%2Flibrustc_trans%2Fmir%2Fconstant.rs", "raw_url": "https://github.com/rust-lang/rust/raw/1b38776c1f68c6fd47c1b2f7b7974efc7dd64901/src%2Flibrustc_trans%2Fmir%2Fconstant.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_trans%2Fmir%2Fconstant.rs?ref=1b38776c1f68c6fd47c1b2f7b7974efc7dd64901", "patch": "@@ -25,7 +25,7 @@ use rustc::ty::subst::Substs;\n use rustc_data_structures::indexed_vec::{Idx, IndexVec};\n use {abi, adt, base, Disr, machine};\n use callee::Callee;\n-use common::{self, BlockAndBuilder, CrateContext, const_get_elt, val_ty, type_is_sized};\n+use common::{self, BlockAndBuilder, CrateContext, const_get_elt, val_ty};\n use common::{C_array, C_bool, C_bytes, C_floating_f64, C_integral};\n use common::{C_null, C_struct, C_str_slice, C_undef, C_uint};\n use common::{const_to_opt_int, const_to_opt_uint};\n@@ -401,7 +401,7 @@ impl<'a, 'tcx> MirConstContext<'a, 'tcx> {\n                     .projection_ty(tcx, &projection.elem);\n                 let base = tr_base.to_const(span);\n                 let projected_ty = self.monomorphize(&projected_ty).to_ty(tcx);\n-                let is_sized = common::type_is_sized(tcx, projected_ty);\n+                let is_sized = self.ccx.shared().type_is_sized(projected_ty);\n \n                 let (projected, llextra) = match projection.elem {\n                     mir::ProjectionElem::Deref => {\n@@ -598,11 +598,11 @@ impl<'a, 'tcx> MirConstContext<'a, 'tcx> {\n                     mir::CastKind::Unsize => {\n                         // unsize targets other than to a fat pointer currently\n                         // can't be in constants.\n-                        assert!(common::type_is_fat_ptr(tcx, cast_ty));\n+                        assert!(common::type_is_fat_ptr(self.ccx, cast_ty));\n \n                         let pointee_ty = operand.ty.builtin_deref(true, ty::NoPreference)\n                             .expect(\"consts: unsizing got non-pointer type\").ty;\n-                        let (base, old_info) = if !common::type_is_sized(tcx, pointee_ty) {\n+                        let (base, old_info) = if !self.ccx.shared().type_is_sized(pointee_ty) {\n                             // Normally, the source is a thin pointer and we are\n                             // adding extra info to make a fat pointer. The exception\n                             // is when we are upcasting an existing object fat pointer\n@@ -685,9 +685,9 @@ impl<'a, 'tcx> MirConstContext<'a, 'tcx> {\n                     mir::CastKind::Misc => { // Casts from a fat-ptr.\n                         let ll_cast_ty = type_of::immediate_type_of(self.ccx, cast_ty);\n                         let ll_from_ty = type_of::immediate_type_of(self.ccx, operand.ty);\n-                        if common::type_is_fat_ptr(tcx, operand.ty) {\n+                        if common::type_is_fat_ptr(self.ccx, operand.ty) {\n                             let (data_ptr, meta_ptr) = operand.get_fat_ptr();\n-                            if common::type_is_fat_ptr(tcx, cast_ty) {\n+                            if common::type_is_fat_ptr(self.ccx, cast_ty) {\n                                 let ll_cft = ll_cast_ty.field_types();\n                                 let ll_fft = ll_from_ty.field_types();\n                                 let data_cast = consts::ptrcast(data_ptr, ll_cft[0]);\n@@ -716,7 +716,7 @@ impl<'a, 'tcx> MirConstContext<'a, 'tcx> {\n                 let base = match tr_lvalue.base {\n                     Base::Value(llval) => {\n                         // FIXME: may be wrong for &*(&simd_vec as &fmt::Debug)\n-                        let align = if type_is_sized(self.ccx.tcx(), ty) {\n+                        let align = if self.ccx.shared().type_is_sized(ty) {\n                             type_of::align_of(self.ccx, ty)\n                         } else {\n                             self.ccx.tcx().data_layout.pointer_align.abi() as machine::llalign\n@@ -731,7 +731,7 @@ impl<'a, 'tcx> MirConstContext<'a, 'tcx> {\n                     Base::Static(llval) => llval\n                 };\n \n-                let ptr = if common::type_is_sized(tcx, ty) {\n+                let ptr = if self.ccx.shared().type_is_sized(ty) {\n                     base\n                 } else {\n                     C_struct(self.ccx, &[base, tr_lvalue.llextra], false)\n@@ -945,40 +945,39 @@ pub fn const_scalar_checked_binop<'a, 'tcx>(tcx: TyCtxt<'a, 'tcx, 'tcx>,\n     }\n }\n \n-impl<'bcx, 'tcx> MirContext<'bcx, 'tcx> {\n+impl<'a, 'tcx> MirContext<'a, 'tcx> {\n     pub fn trans_constant(&mut self,\n-                          bcx: &BlockAndBuilder<'bcx, 'tcx>,\n+                          bcx: &BlockAndBuilder<'a, 'tcx>,\n                           constant: &mir::Constant<'tcx>)\n                           -> Const<'tcx>\n     {\n         debug!(\"trans_constant({:?})\", constant);\n-        let ty = bcx.monomorphize(&constant.ty);\n+        let ty = self.monomorphize(&constant.ty);\n         let result = match constant.literal.clone() {\n             mir::Literal::Item { def_id, substs } => {\n                 // Shortcut for zero-sized types, including function item\n                 // types, which would not work with MirConstContext.\n-                if common::type_is_zero_size(bcx.ccx(), ty) {\n-                    let llty = type_of::type_of(bcx.ccx(), ty);\n+                if common::type_is_zero_size(bcx.ccx, ty) {\n+                    let llty = type_of::type_of(bcx.ccx, ty);\n                     return Const::new(C_null(llty), ty);\n                 }\n \n-                let substs = bcx.monomorphize(&substs);\n+                let substs = self.monomorphize(&substs);\n                 let instance = Instance::new(def_id, substs);\n-                MirConstContext::trans_def(bcx.ccx(), instance, IndexVec::new())\n+                MirConstContext::trans_def(bcx.ccx, instance, IndexVec::new())\n             }\n             mir::Literal::Promoted { index } => {\n                 let mir = &self.mir.promoted[index];\n-                MirConstContext::new(bcx.ccx(), mir, bcx.fcx().param_substs,\n-                                     IndexVec::new()).trans()\n+                MirConstContext::new(bcx.ccx, mir, self.param_substs, IndexVec::new()).trans()\n             }\n             mir::Literal::Value { value } => {\n-                Ok(Const::from_constval(bcx.ccx(), value, ty))\n+                Ok(Const::from_constval(bcx.ccx, value, ty))\n             }\n         };\n \n         let result = result.unwrap_or_else(|_| {\n             // We've errored, so we don't have to produce working code.\n-            let llty = type_of::type_of(bcx.ccx(), ty);\n+            let llty = type_of::type_of(bcx.ccx, ty);\n             Const::new(C_undef(llty), ty)\n         });\n "}, {"sha": "0cd7f007c5df92dda45b67fee48d100b808ac524", "filename": "src/librustc_trans/mir/lvalue.rs", "status": "modified", "additions": 26, "deletions": 26, "changes": 52, "blob_url": "https://github.com/rust-lang/rust/blob/1b38776c1f68c6fd47c1b2f7b7974efc7dd64901/src%2Flibrustc_trans%2Fmir%2Flvalue.rs", "raw_url": "https://github.com/rust-lang/rust/raw/1b38776c1f68c6fd47c1b2f7b7974efc7dd64901/src%2Flibrustc_trans%2Fmir%2Flvalue.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_trans%2Fmir%2Flvalue.rs?ref=1b38776c1f68c6fd47c1b2f7b7974efc7dd64901", "patch": "@@ -44,13 +44,13 @@ impl<'tcx> LvalueRef<'tcx> {\n         LvalueRef { llval: llval, llextra: ptr::null_mut(), ty: lvalue_ty }\n     }\n \n-    pub fn alloca<'bcx>(bcx: &BlockAndBuilder<'bcx, 'tcx>,\n+    pub fn alloca<'a>(bcx: &BlockAndBuilder<'a, 'tcx>,\n                         ty: Ty<'tcx>,\n                         name: &str)\n                         -> LvalueRef<'tcx>\n     {\n         assert!(!ty.has_erasable_regions());\n-        let lltemp = bcx.with_block(|bcx| base::alloc_ty(bcx, ty, name));\n+        let lltemp = base::alloc_ty(bcx, ty, name);\n         LvalueRef::new_sized(lltemp, LvalueTy::from_ty(ty))\n     }\n \n@@ -67,14 +67,14 @@ impl<'tcx> LvalueRef<'tcx> {\n     }\n }\n \n-impl<'bcx, 'tcx> MirContext<'bcx, 'tcx> {\n+impl<'a, 'tcx> MirContext<'a, 'tcx> {\n     pub fn trans_lvalue(&mut self,\n-                        bcx: &BlockAndBuilder<'bcx, 'tcx>,\n+                        bcx: &BlockAndBuilder<'a, 'tcx>,\n                         lvalue: &mir::Lvalue<'tcx>)\n                         -> LvalueRef<'tcx> {\n         debug!(\"trans_lvalue(lvalue={:?})\", lvalue);\n \n-        let ccx = bcx.ccx();\n+        let ccx = bcx.ccx;\n         let tcx = bcx.tcx();\n \n         if let mir::Lvalue::Local(index) = *lvalue {\n@@ -103,7 +103,7 @@ impl<'bcx, 'tcx> MirContext<'bcx, 'tcx> {\n                 let ptr = self.trans_consume(bcx, base);\n                 let projected_ty = LvalueTy::from_ty(ptr.ty)\n                     .projection_ty(tcx, &mir::ProjectionElem::Deref);\n-                let projected_ty = bcx.monomorphize(&projected_ty);\n+                let projected_ty = self.monomorphize(&projected_ty);\n                 let (llptr, llextra) = match ptr.val {\n                     OperandValue::Immediate(llptr) => (llptr, ptr::null_mut()),\n                     OperandValue::Pair(llptr, llextra) => (llptr, llextra),\n@@ -118,14 +118,14 @@ impl<'bcx, 'tcx> MirContext<'bcx, 'tcx> {\n             mir::Lvalue::Projection(ref projection) => {\n                 let tr_base = self.trans_lvalue(bcx, &projection.base);\n                 let projected_ty = tr_base.ty.projection_ty(tcx, &projection.elem);\n-                let projected_ty = bcx.monomorphize(&projected_ty);\n+                let projected_ty = self.monomorphize(&projected_ty);\n \n                 let project_index = |llindex| {\n                     let element = if let ty::TySlice(_) = tr_base.ty.to_ty(tcx).sty {\n                         // Slices already point to the array element type.\n                         bcx.inbounds_gep(tr_base.llval, &[llindex])\n                     } else {\n-                        let zero = common::C_uint(bcx.ccx(), 0u64);\n+                        let zero = common::C_uint(bcx.ccx, 0u64);\n                         bcx.inbounds_gep(tr_base.llval, &[zero, llindex])\n                     };\n                     element\n@@ -140,14 +140,14 @@ impl<'bcx, 'tcx> MirContext<'bcx, 'tcx> {\n                             LvalueTy::Downcast { adt_def: _, substs: _, variant_index: v } => v,\n                         };\n                         let discr = discr as u64;\n-                        let is_sized = common::type_is_sized(tcx, projected_ty.to_ty(tcx));\n+                        let is_sized = self.ccx.shared().type_is_sized(projected_ty.to_ty(tcx));\n                         let base = if is_sized {\n                             adt::MaybeSizedValue::sized(tr_base.llval)\n                         } else {\n                             adt::MaybeSizedValue::unsized_(tr_base.llval, tr_base.llextra)\n                         };\n-                        let llprojected = adt::trans_field_ptr_builder(bcx, base_ty, base,\n-                                                                       Disr(discr), field.index());\n+                        let llprojected = adt::trans_field_ptr(bcx, base_ty, base, Disr(discr),\n+                            field.index());\n                         let llextra = if is_sized {\n                             ptr::null_mut()\n                         } else {\n@@ -162,19 +162,19 @@ impl<'bcx, 'tcx> MirContext<'bcx, 'tcx> {\n                     mir::ProjectionElem::ConstantIndex { offset,\n                                                          from_end: false,\n                                                          min_length: _ } => {\n-                        let lloffset = C_uint(bcx.ccx(), offset);\n+                        let lloffset = C_uint(bcx.ccx, offset);\n                         (project_index(lloffset), ptr::null_mut())\n                     }\n                     mir::ProjectionElem::ConstantIndex { offset,\n                                                          from_end: true,\n                                                          min_length: _ } => {\n-                        let lloffset = C_uint(bcx.ccx(), offset);\n-                        let lllen = tr_base.len(bcx.ccx());\n+                        let lloffset = C_uint(bcx.ccx, offset);\n+                        let lllen = tr_base.len(bcx.ccx);\n                         let llindex = bcx.sub(lllen, lloffset);\n                         (project_index(llindex), ptr::null_mut())\n                     }\n                     mir::ProjectionElem::Subslice { from, to } => {\n-                        let llindex = C_uint(bcx.ccx(), from);\n+                        let llindex = C_uint(bcx.ccx, from);\n                         let llbase = project_index(llindex);\n \n                         let base_ty = tr_base.ty.to_ty(bcx.tcx());\n@@ -183,14 +183,14 @@ impl<'bcx, 'tcx> MirContext<'bcx, 'tcx> {\n                                 // must cast the lvalue pointer type to the new\n                                 // array type (*[%_; new_len]).\n                                 let base_ty = self.monomorphized_lvalue_ty(lvalue);\n-                                let llbasety = type_of::type_of(bcx.ccx(), base_ty).ptr_to();\n+                                let llbasety = type_of::type_of(bcx.ccx, base_ty).ptr_to();\n                                 let llbase = bcx.pointercast(llbase, llbasety);\n                                 (llbase, ptr::null_mut())\n                             }\n                             ty::TySlice(..) => {\n                                 assert!(tr_base.llextra != ptr::null_mut());\n                                 let lllen = bcx.sub(tr_base.llextra,\n-                                                    C_uint(bcx.ccx(), from+to));\n+                                                    C_uint(bcx.ccx, from+to));\n                                 (llbase, lllen)\n                             }\n                             _ => bug!(\"unexpected type {:?} in Subslice\", base_ty)\n@@ -214,7 +214,7 @@ impl<'bcx, 'tcx> MirContext<'bcx, 'tcx> {\n     // Perform an action using the given Lvalue.\n     // If the Lvalue is an empty LocalRef::Operand, then a temporary stack slot\n     // is created first, then used as an operand to update the Lvalue.\n-    pub fn with_lvalue_ref<F, U>(&mut self, bcx: &BlockAndBuilder<'bcx, 'tcx>,\n+    pub fn with_lvalue_ref<F, U>(&mut self, bcx: &BlockAndBuilder<'a, 'tcx>,\n                                  lvalue: &mir::Lvalue<'tcx>, f: F) -> U\n     where F: FnOnce(&mut Self, LvalueRef<'tcx>) -> U\n     {\n@@ -235,9 +235,9 @@ impl<'bcx, 'tcx> MirContext<'bcx, 'tcx> {\n                     // See comments in LocalRef::new_operand as to why\n                     // we always have Some in a ZST LocalRef::Operand.\n                     let ty = self.monomorphized_lvalue_ty(lvalue);\n-                    if common::type_is_zero_size(bcx.ccx(), ty) {\n+                    if common::type_is_zero_size(bcx.ccx, ty) {\n                         // Pass an undef pointer as no stores can actually occur.\n-                        let llptr = C_undef(type_of(bcx.ccx(), ty).ptr_to());\n+                        let llptr = C_undef(type_of(bcx.ccx, ty).ptr_to());\n                         f(self, LvalueRef::new_sized(llptr, LvalueTy::from_ty(ty)))\n                     } else {\n                         bug!(\"Lvalue local already set\");\n@@ -255,13 +255,13 @@ impl<'bcx, 'tcx> MirContext<'bcx, 'tcx> {\n     ///\n     /// nmatsakis: is this still necessary? Not sure.\n     fn prepare_index(&mut self,\n-                     bcx: &BlockAndBuilder<'bcx, 'tcx>,\n+                     bcx: &BlockAndBuilder<'a, 'tcx>,\n                      llindex: ValueRef)\n                      -> ValueRef\n     {\n-        let ccx = bcx.ccx();\n-        let index_size = machine::llbitsize_of_real(bcx.ccx(), common::val_ty(llindex));\n-        let int_size = machine::llbitsize_of_real(bcx.ccx(), ccx.int_type());\n+        let ccx = bcx.ccx;\n+        let index_size = machine::llbitsize_of_real(bcx.ccx, common::val_ty(llindex));\n+        let int_size = machine::llbitsize_of_real(bcx.ccx, ccx.int_type());\n         if index_size < int_size {\n             bcx.zext(llindex, ccx.int_type())\n         } else if index_size > int_size {\n@@ -272,8 +272,8 @@ impl<'bcx, 'tcx> MirContext<'bcx, 'tcx> {\n     }\n \n     pub fn monomorphized_lvalue_ty(&self, lvalue: &mir::Lvalue<'tcx>) -> Ty<'tcx> {\n-        let tcx = self.fcx.ccx.tcx();\n+        let tcx = self.ccx.tcx();\n         let lvalue_ty = lvalue.ty(&self.mir, tcx);\n-        self.fcx.monomorphize(&lvalue_ty.to_ty(tcx))\n+        self.monomorphize(&lvalue_ty.to_ty(tcx))\n     }\n }"}, {"sha": "7a50e5cbe8c79a4d8bef09bd7fc76e718e8e4321", "filename": "src/librustc_trans/mir/mod.rs", "status": "modified", "additions": 163, "deletions": 130, "changes": 293, "blob_url": "https://github.com/rust-lang/rust/blob/1b38776c1f68c6fd47c1b2f7b7974efc7dd64901/src%2Flibrustc_trans%2Fmir%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/1b38776c1f68c6fd47c1b2f7b7974efc7dd64901/src%2Flibrustc_trans%2Fmir%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_trans%2Fmir%2Fmod.rs?ref=1b38776c1f68c6fd47c1b2f7b7974efc7dd64901", "patch": "@@ -9,40 +9,50 @@\n // except according to those terms.\n \n use libc::c_uint;\n-use llvm::{self, ValueRef};\n+use llvm::{self, ValueRef, BasicBlockRef};\n+use llvm::debuginfo::DIScope;\n use rustc::ty::{self, layout};\n-use rustc::mir;\n+use rustc::mir::{self, Mir};\n use rustc::mir::tcx::LvalueTy;\n+use rustc::ty::subst::Substs;\n+use rustc::infer::TransNormalize;\n+use rustc::ty::TypeFoldable;\n use session::config::FullDebugInfo;\n use base;\n-use common::{self, Block, BlockAndBuilder, CrateContext, FunctionContext, C_null};\n-use debuginfo::{self, declare_local, DebugLoc, VariableAccess, VariableKind, FunctionDebugContext};\n+use common::{self, BlockAndBuilder, CrateContext, FunctionContext, C_null, Funclet};\n+use debuginfo::{self, declare_local, VariableAccess, VariableKind, FunctionDebugContext};\n+use monomorphize::{self, Instance};\n+use abi::FnType;\n use type_of;\n \n-use syntax_pos::{DUMMY_SP, NO_EXPANSION, COMMAND_LINE_EXPN, BytePos};\n+use syntax_pos::{DUMMY_SP, NO_EXPANSION, COMMAND_LINE_EXPN, BytePos, Span};\n use syntax::symbol::keywords;\n+use syntax::abi::Abi;\n \n-use std::cell::Ref;\n use std::iter;\n \n-use basic_block::BasicBlock;\n-\n use rustc_data_structures::bitvec::BitVector;\n use rustc_data_structures::indexed_vec::{IndexVec, Idx};\n \n pub use self::constant::trans_static_initializer;\n \n+use self::analyze::CleanupKind;\n use self::lvalue::{LvalueRef};\n use rustc::mir::traversal;\n \n use self::operand::{OperandRef, OperandValue};\n \n /// Master context for translating MIR.\n-pub struct MirContext<'bcx, 'tcx:'bcx> {\n-    mir: Ref<'tcx, mir::Mir<'tcx>>,\n+pub struct MirContext<'a, 'tcx:'a> {\n+    mir: &'a mir::Mir<'tcx>,\n+\n+    debug_context: debuginfo::FunctionDebugContext,\n+\n+    fcx: &'a common::FunctionContext<'a, 'tcx>,\n \n-    /// Function context\n-    fcx: &'bcx common::FunctionContext<'bcx, 'tcx>,\n+    ccx: &'a CrateContext<'a, 'tcx>,\n+\n+    fn_ty: FnType,\n \n     /// When unwinding is initiated, we have to store this personality\n     /// value somewhere so that we can load it and re-use it in the\n@@ -54,17 +64,17 @@ pub struct MirContext<'bcx, 'tcx:'bcx> {\n     llpersonalityslot: Option<ValueRef>,\n \n     /// A `Block` for each MIR `BasicBlock`\n-    blocks: IndexVec<mir::BasicBlock, Block<'bcx, 'tcx>>,\n+    blocks: IndexVec<mir::BasicBlock, BasicBlockRef>,\n \n     /// The funclet status of each basic block\n     cleanup_kinds: IndexVec<mir::BasicBlock, analyze::CleanupKind>,\n \n     /// This stores the landing-pad block for a given BB, computed lazily on GNU\n     /// and eagerly on MSVC.\n-    landing_pads: IndexVec<mir::BasicBlock, Option<Block<'bcx, 'tcx>>>,\n+    landing_pads: IndexVec<mir::BasicBlock, Option<BasicBlockRef>>,\n \n     /// Cached unreachable block\n-    unreachable_block: Option<Block<'bcx, 'tcx>>,\n+    unreachable_block: Option<BasicBlockRef>,\n \n     /// The location where each MIR arg/var/tmp/ret is stored. This is\n     /// usually an `LvalueRef` representing an alloca, but not always:\n@@ -85,18 +95,28 @@ pub struct MirContext<'bcx, 'tcx:'bcx> {\n \n     /// Debug information for MIR scopes.\n     scopes: IndexVec<mir::VisibilityScope, debuginfo::MirDebugScope>,\n+\n+    /// If this function is being monomorphized, this contains the type substitutions used.\n+    param_substs: &'tcx Substs<'tcx>,\n }\n \n-impl<'blk, 'tcx> MirContext<'blk, 'tcx> {\n-    pub fn debug_loc(&mut self, source_info: mir::SourceInfo) -> DebugLoc {\n+impl<'a, 'tcx> MirContext<'a, 'tcx> {\n+    pub fn monomorphize<T>(&self, value: &T) -> T\n+        where T: TransNormalize<'tcx> {\n+        monomorphize::apply_param_substs(self.ccx.shared(), self.param_substs, value)\n+    }\n+\n+    pub fn set_debug_loc(&mut self, bcx: &BlockAndBuilder, source_info: mir::SourceInfo) {\n+        let (scope, span) = self.debug_loc(source_info);\n+        debuginfo::set_source_location(&self.debug_context, bcx, scope, span);\n+    }\n+\n+    pub fn debug_loc(&mut self, source_info: mir::SourceInfo) -> (DIScope, Span) {\n         // Bail out if debug info emission is not enabled.\n-        match self.fcx.debug_context {\n+        match self.debug_context {\n             FunctionDebugContext::DebugInfoDisabled |\n             FunctionDebugContext::FunctionWithoutDebugInfo => {\n-                // Can't return DebugLoc::None here because intrinsic::trans_intrinsic_call()\n-                // relies on debug location to obtain span of the call site.\n-                return DebugLoc::ScopeAt(self.scopes[source_info.scope].scope_metadata,\n-                                         source_info.span);\n+                return (self.scopes[source_info.scope].scope_metadata, source_info.span);\n             }\n             FunctionDebugContext::RegularContext(_) =>{}\n         }\n@@ -106,13 +126,12 @@ impl<'blk, 'tcx> MirContext<'blk, 'tcx> {\n         // (unless the crate is being compiled with `-Z debug-macros`).\n         if source_info.span.expn_id == NO_EXPANSION ||\n             source_info.span.expn_id == COMMAND_LINE_EXPN ||\n-            self.fcx.ccx.sess().opts.debugging_opts.debug_macros {\n+            self.ccx.sess().opts.debugging_opts.debug_macros {\n \n-            let scope_metadata = self.scope_metadata_for_loc(source_info.scope,\n-                                                             source_info.span.lo);\n-            DebugLoc::ScopeAt(scope_metadata, source_info.span)\n+            let scope = self.scope_metadata_for_loc(source_info.scope, source_info.span.lo);\n+            (scope, source_info.span)\n         } else {\n-            let cm = self.fcx.ccx.sess().codemap();\n+            let cm = self.ccx.sess().codemap();\n             // Walk up the macro expansion chain until we reach a non-expanded span.\n             let mut span = source_info.span;\n             while span.expn_id != NO_EXPANSION && span.expn_id != COMMAND_LINE_EXPN {\n@@ -123,9 +142,9 @@ impl<'blk, 'tcx> MirContext<'blk, 'tcx> {\n                     break;\n                 }\n             }\n-            let scope_metadata = self.scope_metadata_for_loc(source_info.scope, span.lo);\n+            let scope = self.scope_metadata_for_loc(source_info.scope, span.lo);\n             // Use span of the outermost call site, while keeping the original lexical scope\n-            DebugLoc::ScopeAt(scope_metadata, span)\n+            (scope, span)\n         }\n     }\n \n@@ -138,10 +157,8 @@ impl<'blk, 'tcx> MirContext<'blk, 'tcx> {\n         let scope_metadata = self.scopes[scope_id].scope_metadata;\n         if pos < self.scopes[scope_id].file_start_pos ||\n            pos >= self.scopes[scope_id].file_end_pos {\n-            let cm = self.fcx.ccx.sess().codemap();\n-            debuginfo::extend_scope_to_file(self.fcx.ccx,\n-                                            scope_metadata,\n-                                            &cm.lookup_char_pos(pos).file)\n+            let cm = self.ccx.sess().codemap();\n+            debuginfo::extend_scope_to_file(self.ccx, scope_metadata, &cm.lookup_char_pos(pos).file)\n         } else {\n             scope_metadata\n         }\n@@ -154,7 +171,7 @@ enum LocalRef<'tcx> {\n }\n \n impl<'tcx> LocalRef<'tcx> {\n-    fn new_operand<'bcx>(ccx: &CrateContext<'bcx, 'tcx>,\n+    fn new_operand<'a>(ccx: &CrateContext<'a, 'tcx>,\n                          ty: ty::Ty<'tcx>) -> LocalRef<'tcx> {\n         if common::type_is_zero_size(ccx, ty) {\n             // Zero-size temporaries aren't always initialized, which\n@@ -180,19 +197,22 @@ impl<'tcx> LocalRef<'tcx> {\n \n ///////////////////////////////////////////////////////////////////////////\n \n-pub fn trans_mir<'blk, 'tcx: 'blk>(fcx: &'blk FunctionContext<'blk, 'tcx>) {\n-    let bcx = fcx.init(true).build();\n-    let mir = bcx.mir();\n+pub fn trans_mir<'a, 'tcx: 'a>(\n+    fcx: &'a FunctionContext<'a, 'tcx>,\n+    fn_ty: FnType,\n+    mir: &'a Mir<'tcx>,\n+    instance: Instance<'tcx>,\n+    sig: &ty::FnSig<'tcx>,\n+    abi: Abi,\n+) {\n+    let debug_context =\n+        debuginfo::create_function_debug_context(fcx.ccx, instance, sig, abi, fcx.llfn, mir);\n+    let bcx = fcx.get_entry_block();\n \n-    // Analyze the temps to determine which must be lvalues\n-    // FIXME\n-    let (lvalue_locals, cleanup_kinds) = bcx.with_block(|bcx| {\n-        (analyze::lvalue_locals(bcx, &mir),\n-         analyze::cleanup_kinds(bcx, &mir))\n-    });\n+    let cleanup_kinds = analyze::cleanup_kinds(&mir);\n \n     // Allocate a `Block` for every basic block\n-    let block_bcxs: IndexVec<mir::BasicBlock, Block<'blk,'tcx>> =\n+    let block_bcxs: IndexVec<mir::BasicBlock, BasicBlockRef> =\n         mir.basic_blocks().indices().map(|bb| {\n             if bb == mir::START_BLOCK {\n                 fcx.new_block(\"start\")\n@@ -202,27 +222,36 @@ pub fn trans_mir<'blk, 'tcx: 'blk>(fcx: &'blk FunctionContext<'blk, 'tcx>) {\n         }).collect();\n \n     // Compute debuginfo scopes from MIR scopes.\n-    let scopes = debuginfo::create_mir_scopes(fcx);\n+    let scopes = debuginfo::create_mir_scopes(fcx, mir, &debug_context);\n \n     let mut mircx = MirContext {\n-        mir: Ref::clone(&mir),\n+        mir: mir,\n         fcx: fcx,\n+        fn_ty: fn_ty,\n+        ccx: fcx.ccx,\n         llpersonalityslot: None,\n         blocks: block_bcxs,\n         unreachable_block: None,\n         cleanup_kinds: cleanup_kinds,\n         landing_pads: IndexVec::from_elem(None, mir.basic_blocks()),\n         scopes: scopes,\n         locals: IndexVec::new(),\n+        debug_context: debug_context,\n+        param_substs: {\n+            assert!(!instance.substs.needs_infer());\n+            instance.substs\n+        },\n     };\n \n+    let lvalue_locals = analyze::lvalue_locals(&mircx);\n+\n     // Allocate variable and temp allocas\n     mircx.locals = {\n-        let args = arg_local_refs(&bcx, &mir, &mircx.scopes, &lvalue_locals);\n+        let args = arg_local_refs(&bcx, &mircx, &mircx.scopes, &lvalue_locals);\n \n         let mut allocate_local = |local| {\n             let decl = &mir.local_decls[local];\n-            let ty = bcx.monomorphize(&decl.ty);\n+            let ty = mircx.monomorphize(&decl.ty);\n \n             if let Some(name) = decl.name {\n                 // User variable\n@@ -232,27 +261,21 @@ pub fn trans_mir<'blk, 'tcx: 'blk>(fcx: &'blk FunctionContext<'blk, 'tcx>) {\n \n                 if !lvalue_locals.contains(local.index()) && !dbg {\n                     debug!(\"alloc: {:?} ({}) -> operand\", local, name);\n-                    return LocalRef::new_operand(bcx.ccx(), ty);\n+                    return LocalRef::new_operand(bcx.ccx, ty);\n                 }\n \n                 debug!(\"alloc: {:?} ({}) -> lvalue\", local, name);\n                 let lvalue = LvalueRef::alloca(&bcx, ty, &name.as_str());\n                 if dbg {\n-                    let dbg_loc = mircx.debug_loc(source_info);\n-                    if let DebugLoc::ScopeAt(scope, span) = dbg_loc {\n-                        bcx.with_block(|bcx| {\n-                            declare_local(bcx, name, ty, scope,\n-                                        VariableAccess::DirectVariable { alloca: lvalue.llval },\n-                                        VariableKind::LocalVariable, span);\n-                        });\n-                    } else {\n-                        panic!(\"Unexpected\");\n-                    }\n+                    let (scope, span) = mircx.debug_loc(source_info);\n+                    declare_local(&bcx, &mircx.debug_context, name, ty, scope,\n+                        VariableAccess::DirectVariable { alloca: lvalue.llval },\n+                        VariableKind::LocalVariable, span);\n                 }\n                 LocalRef::Lvalue(lvalue)\n             } else {\n                 // Temporary or return pointer\n-                if local == mir::RETURN_POINTER && fcx.fn_ty.ret.is_indirect() {\n+                if local == mir::RETURN_POINTER && mircx.fn_ty.ret.is_indirect() {\n                     debug!(\"alloc: {:?} (return pointer) -> lvalue\", local);\n                     let llretptr = llvm::get_param(fcx.llfn, 0);\n                     LocalRef::Lvalue(LvalueRef::new_sized(llretptr, LvalueTy::from_ty(ty)))\n@@ -264,7 +287,7 @@ pub fn trans_mir<'blk, 'tcx: 'blk>(fcx: &'blk FunctionContext<'blk, 'tcx>) {\n                     // alloca in advance. Instead we wait until we see the\n                     // definition and update the operand there.\n                     debug!(\"alloc: {:?} -> operand\", local);\n-                    LocalRef::new_operand(bcx.ccx(), ty)\n+                    LocalRef::new_operand(bcx.ccx, ty)\n                 }\n             }\n         };\n@@ -278,57 +301,61 @@ pub fn trans_mir<'blk, 'tcx: 'blk>(fcx: &'blk FunctionContext<'blk, 'tcx>) {\n \n     // Branch to the START block\n     let start_bcx = mircx.blocks[mir::START_BLOCK];\n-    bcx.br(start_bcx.llbb);\n+    bcx.br(start_bcx);\n \n     // Up until here, IR instructions for this function have explicitly not been annotated with\n     // source code location, so we don't step into call setup code. From here on, source location\n     // emitting should be enabled.\n-    debuginfo::start_emitting_source_locations(fcx);\n-\n-    let mut visited = BitVector::new(mir.basic_blocks().len());\n+    debuginfo::start_emitting_source_locations(&mircx.debug_context);\n+\n+    let funclets: IndexVec<mir::BasicBlock, Option<Funclet>> =\n+    mircx.cleanup_kinds.iter_enumerated().map(|(bb, cleanup_kind)| {\n+        if let CleanupKind::Funclet = *cleanup_kind {\n+            let bcx = mircx.build_block(bb);\n+            bcx.set_personality_fn(mircx.ccx.eh_personality());\n+            if base::wants_msvc_seh(fcx.ccx.sess()) {\n+                return Some(Funclet::new(bcx.cleanup_pad(None, &[])));\n+            }\n+        }\n \n-    let mut rpo = traversal::reverse_postorder(&mir);\n+        None\n+    }).collect();\n \n-    // Prepare each block for translation.\n-    for (bb, _) in rpo.by_ref() {\n-        mircx.init_cpad(bb);\n-    }\n-    rpo.reset();\n+    let rpo = traversal::reverse_postorder(&mir);\n+    let mut visited = BitVector::new(mir.basic_blocks().len());\n \n     // Translate the body of each block using reverse postorder\n     for (bb, _) in rpo {\n         visited.insert(bb.index());\n-        mircx.trans_block(bb);\n+        mircx.trans_block(bb, &funclets);\n     }\n \n     // Remove blocks that haven't been visited, or have no\n     // predecessors.\n     for bb in mir.basic_blocks().indices() {\n-        let block = mircx.blocks[bb];\n-        let block = BasicBlock(block.llbb);\n         // Unreachable block\n         if !visited.contains(bb.index()) {\n             debug!(\"trans_mir: block {:?} was not visited\", bb);\n-            block.delete();\n+            unsafe {\n+                llvm::LLVMDeleteBasicBlock(mircx.blocks[bb]);\n+            }\n         }\n     }\n-\n-    DebugLoc::None.apply(fcx);\n-    fcx.cleanup();\n }\n \n /// Produce, for each argument, a `ValueRef` pointing at the\n /// argument's value. As arguments are lvalues, these are always\n /// indirect.\n-fn arg_local_refs<'bcx, 'tcx>(bcx: &BlockAndBuilder<'bcx, 'tcx>,\n-                              mir: &mir::Mir<'tcx>,\n-                              scopes: &IndexVec<mir::VisibilityScope, debuginfo::MirDebugScope>,\n-                              lvalue_locals: &BitVector)\n-                              -> Vec<LocalRef<'tcx>> {\n+fn arg_local_refs<'a, 'tcx>(bcx: &BlockAndBuilder<'a, 'tcx>,\n+                            mircx: &MirContext<'a, 'tcx>,\n+                            scopes: &IndexVec<mir::VisibilityScope, debuginfo::MirDebugScope>,\n+                            lvalue_locals: &BitVector)\n+                            -> Vec<LocalRef<'tcx>> {\n+    let mir = mircx.mir;\n     let fcx = bcx.fcx();\n     let tcx = bcx.tcx();\n     let mut idx = 0;\n-    let mut llarg_idx = fcx.fn_ty.ret.is_indirect() as usize;\n+    let mut llarg_idx = mircx.fn_ty.ret.is_indirect() as usize;\n \n     // Get the argument scope, if it exists and if we need it.\n     let arg_scope = scopes[mir::ARGUMENT_VISIBILITY_SCOPE];\n@@ -340,7 +367,7 @@ fn arg_local_refs<'bcx, 'tcx>(bcx: &BlockAndBuilder<'bcx, 'tcx>,\n \n     mir.args_iter().enumerate().map(|(arg_index, local)| {\n         let arg_decl = &mir.local_decls[local];\n-        let arg_ty = bcx.monomorphize(&arg_decl.ty);\n+        let arg_ty = mircx.monomorphize(&arg_decl.ty);\n \n         if Some(local) == mir.spread_arg {\n             // This argument (e.g. the last argument in the \"rust-call\" ABI)\n@@ -353,43 +380,44 @@ fn arg_local_refs<'bcx, 'tcx>(bcx: &BlockAndBuilder<'bcx, 'tcx>,\n                 _ => bug!(\"spread argument isn't a tuple?!\")\n             };\n \n-            let lltemp = bcx.with_block(|bcx| {\n-                base::alloc_ty(bcx, arg_ty, &format!(\"arg{}\", arg_index))\n-            });\n+            let lltemp = base::alloc_ty(&bcx, arg_ty, &format!(\"arg{}\", arg_index));\n             for (i, &tupled_arg_ty) in tupled_arg_tys.iter().enumerate() {\n                 let dst = bcx.struct_gep(lltemp, i);\n-                let arg = &fcx.fn_ty.args[idx];\n+                let arg = &mircx.fn_ty.args[idx];\n                 idx += 1;\n-                if common::type_is_fat_ptr(tcx, tupled_arg_ty) {\n+                if common::type_is_fat_ptr(bcx.ccx, tupled_arg_ty) {\n                     // We pass fat pointers as two words, but inside the tuple\n                     // they are the two sub-fields of a single aggregate field.\n-                    let meta = &fcx.fn_ty.args[idx];\n+                    let meta = &mircx.fn_ty.args[idx];\n                     idx += 1;\n-                    arg.store_fn_arg(bcx, &mut llarg_idx,\n-                                     base::get_dataptr_builder(bcx, dst));\n-                    meta.store_fn_arg(bcx, &mut llarg_idx,\n-                                      base::get_meta_builder(bcx, dst));\n+                    arg.store_fn_arg(bcx, &mut llarg_idx, base::get_dataptr(bcx, dst));\n+                    meta.store_fn_arg(bcx, &mut llarg_idx, base::get_meta(bcx, dst));\n                 } else {\n                     arg.store_fn_arg(bcx, &mut llarg_idx, dst);\n                 }\n             }\n \n             // Now that we have one alloca that contains the aggregate value,\n             // we can create one debuginfo entry for the argument.\n-            bcx.with_block(|bcx| arg_scope.map(|scope| {\n+            arg_scope.map(|scope| {\n                 let variable_access = VariableAccess::DirectVariable {\n                     alloca: lltemp\n                 };\n-                declare_local(bcx, arg_decl.name.unwrap_or(keywords::Invalid.name()),\n-                              arg_ty, scope, variable_access,\n-                              VariableKind::ArgumentVariable(arg_index + 1),\n-                              bcx.fcx().span.unwrap_or(DUMMY_SP));\n-            }));\n+                declare_local(\n+                    bcx,\n+                    &mircx.debug_context,\n+                    arg_decl.name.unwrap_or(keywords::Invalid.name()),\n+                    arg_ty, scope,\n+                    variable_access,\n+                    VariableKind::ArgumentVariable(arg_index + 1),\n+                    DUMMY_SP\n+                );\n+            });\n \n             return LocalRef::Lvalue(LvalueRef::new_sized(lltemp, LvalueTy::from_ty(arg_ty)));\n         }\n \n-        let arg = &fcx.fn_ty.args[idx];\n+        let arg = &mircx.fn_ty.args[idx];\n         idx += 1;\n         let llval = if arg.is_indirect() && bcx.sess().opts.debuginfo != FullDebugInfo {\n             // Don't copy an indirect argument to an alloca, the caller\n@@ -406,7 +434,7 @@ fn arg_local_refs<'bcx, 'tcx>(bcx: &BlockAndBuilder<'bcx, 'tcx>,\n                   !arg.is_indirect() && arg.cast.is_none() &&\n                   arg_scope.is_none() {\n             if arg.is_ignore() {\n-                return LocalRef::new_operand(bcx.ccx(), arg_ty);\n+                return LocalRef::new_operand(bcx.ccx, arg_ty);\n             }\n \n             // We don't have to cast or keep the argument in the alloca.\n@@ -417,8 +445,8 @@ fn arg_local_refs<'bcx, 'tcx>(bcx: &BlockAndBuilder<'bcx, 'tcx>,\n             }\n             let llarg = llvm::get_param(fcx.llfn, llarg_idx as c_uint);\n             llarg_idx += 1;\n-            let val = if common::type_is_fat_ptr(tcx, arg_ty) {\n-                let meta = &fcx.fn_ty.args[idx];\n+            let val = if common::type_is_fat_ptr(bcx.ccx, arg_ty) {\n+                let meta = &mircx.fn_ty.args[idx];\n                 idx += 1;\n                 assert_eq!((meta.cast, meta.pad), (None, None));\n                 let llmeta = llvm::get_param(fcx.llfn, llarg_idx as c_uint);\n@@ -433,33 +461,35 @@ fn arg_local_refs<'bcx, 'tcx>(bcx: &BlockAndBuilder<'bcx, 'tcx>,\n             };\n             return LocalRef::Operand(Some(operand.unpack_if_pair(bcx)));\n         } else {\n-            let lltemp = bcx.with_block(|bcx| {\n-                base::alloc_ty(bcx, arg_ty, &format!(\"arg{}\", arg_index))\n-            });\n-            if common::type_is_fat_ptr(tcx, arg_ty) {\n+            let lltemp = base::alloc_ty(&bcx, arg_ty, &format!(\"arg{}\", arg_index));\n+            if common::type_is_fat_ptr(bcx.ccx, arg_ty) {\n                 // we pass fat pointers as two words, but we want to\n                 // represent them internally as a pointer to two words,\n                 // so make an alloca to store them in.\n-                let meta = &fcx.fn_ty.args[idx];\n+                let meta = &mircx.fn_ty.args[idx];\n                 idx += 1;\n-                arg.store_fn_arg(bcx, &mut llarg_idx,\n-                                 base::get_dataptr_builder(bcx, lltemp));\n-                meta.store_fn_arg(bcx, &mut llarg_idx,\n-                                  base::get_meta_builder(bcx, lltemp));\n+                arg.store_fn_arg(bcx, &mut llarg_idx, base::get_dataptr(bcx, lltemp));\n+                meta.store_fn_arg(bcx, &mut llarg_idx, base::get_meta(bcx, lltemp));\n             } else  {\n                 // otherwise, arg is passed by value, so make a\n                 // temporary and store it there\n                 arg.store_fn_arg(bcx, &mut llarg_idx, lltemp);\n             }\n             lltemp\n         };\n-        bcx.with_block(|bcx| arg_scope.map(|scope| {\n+        arg_scope.map(|scope| {\n             // Is this a regular argument?\n             if arg_index > 0 || mir.upvar_decls.is_empty() {\n-                declare_local(bcx, arg_decl.name.unwrap_or(keywords::Invalid.name()), arg_ty,\n-                              scope, VariableAccess::DirectVariable { alloca: llval },\n-                              VariableKind::ArgumentVariable(arg_index + 1),\n-                              bcx.fcx().span.unwrap_or(DUMMY_SP));\n+                declare_local(\n+                    bcx,\n+                    &mircx.debug_context,\n+                    arg_decl.name.unwrap_or(keywords::Invalid.name()),\n+                    arg_ty,\n+                    scope,\n+                    VariableAccess::DirectVariable { alloca: llval },\n+                    VariableKind::ArgumentVariable(arg_index + 1),\n+                    DUMMY_SP\n+                );\n                 return;\n             }\n \n@@ -483,17 +513,14 @@ fn arg_local_refs<'bcx, 'tcx>(bcx: &BlockAndBuilder<'bcx, 'tcx>,\n             // doesn't actually strip the offset when splitting the closure\n             // environment into its components so it ends up out of bounds.\n             let env_ptr = if !env_ref {\n-                use base::*;\n-                use build::*;\n-                use common::*;\n-                let alloc = alloca(bcx, val_ty(llval), \"__debuginfo_env_ptr\");\n-                Store(bcx, llval, alloc);\n+                let alloc = bcx.fcx().alloca(common::val_ty(llval), \"__debuginfo_env_ptr\");\n+                bcx.store(llval, alloc);\n                 alloc\n             } else {\n                 llval\n             };\n \n-            let layout = bcx.ccx().layout_of(closure_ty);\n+            let layout = bcx.ccx.layout_of(closure_ty);\n             let offsets = match *layout {\n                 layout::Univariant { ref variant, .. } => &variant.offsets[..],\n                 _ => bug!(\"Closures are only supposed to be Univariant\")\n@@ -502,7 +529,6 @@ fn arg_local_refs<'bcx, 'tcx>(bcx: &BlockAndBuilder<'bcx, 'tcx>,\n             for (i, (decl, ty)) in mir.upvar_decls.iter().zip(upvar_tys).enumerate() {\n                 let byte_offset_of_var_in_env = offsets[i].bytes();\n \n-\n                 let ops = unsafe {\n                     [llvm::LLVMRustDIBuilderCreateOpDeref(),\n                      llvm::LLVMRustDIBuilderCreateOpPlus(),\n@@ -527,11 +553,18 @@ fn arg_local_refs<'bcx, 'tcx>(bcx: &BlockAndBuilder<'bcx, 'tcx>,\n                     alloca: env_ptr,\n                     address_operations: &ops\n                 };\n-                declare_local(bcx, decl.debug_name, ty, scope, variable_access,\n-                              VariableKind::CapturedVariable,\n-                              bcx.fcx().span.unwrap_or(DUMMY_SP));\n+                declare_local(\n+                    bcx,\n+                    &mircx.debug_context,\n+                    decl.debug_name,\n+                    ty,\n+                    scope,\n+                    variable_access,\n+                    VariableKind::CapturedVariable,\n+                    DUMMY_SP\n+                );\n             }\n-        }));\n+        });\n         LocalRef::Lvalue(LvalueRef::new_sized(llval, LvalueTy::from_ty(arg_ty)))\n     }).collect()\n }"}, {"sha": "a15d51d9da64dcc2850bb2d03bcd87950f06df0a", "filename": "src/librustc_trans/mir/operand.rs", "status": "modified", "additions": 31, "deletions": 41, "changes": 72, "blob_url": "https://github.com/rust-lang/rust/blob/1b38776c1f68c6fd47c1b2f7b7974efc7dd64901/src%2Flibrustc_trans%2Fmir%2Foperand.rs", "raw_url": "https://github.com/rust-lang/rust/raw/1b38776c1f68c6fd47c1b2f7b7974efc7dd64901/src%2Flibrustc_trans%2Fmir%2Foperand.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_trans%2Fmir%2Foperand.rs?ref=1b38776c1f68c6fd47c1b2f7b7974efc7dd64901", "patch": "@@ -14,7 +14,7 @@ use rustc::mir;\n use rustc_data_structures::indexed_vec::Idx;\n \n use base;\n-use common::{self, Block, BlockAndBuilder};\n+use common::{self, BlockAndBuilder};\n use value::Value;\n use type_of;\n use type_::Type;\n@@ -73,7 +73,7 @@ impl<'tcx> fmt::Debug for OperandRef<'tcx> {\n     }\n }\n \n-impl<'bcx, 'tcx> OperandRef<'tcx> {\n+impl<'a, 'tcx> OperandRef<'tcx> {\n     /// Asserts that this operand refers to a scalar and returns\n     /// a reference to its value.\n     pub fn immediate(self) -> ValueRef {\n@@ -85,18 +85,18 @@ impl<'bcx, 'tcx> OperandRef<'tcx> {\n \n     /// If this operand is a Pair, we return an\n     /// Immediate aggregate with the two values.\n-    pub fn pack_if_pair(mut self, bcx: &BlockAndBuilder<'bcx, 'tcx>)\n+    pub fn pack_if_pair(mut self, bcx: &BlockAndBuilder<'a, 'tcx>)\n                         -> OperandRef<'tcx> {\n         if let OperandValue::Pair(a, b) = self.val {\n             // Reconstruct the immediate aggregate.\n-            let llty = type_of::type_of(bcx.ccx(), self.ty);\n+            let llty = type_of::type_of(bcx.ccx, self.ty);\n             let mut llpair = common::C_undef(llty);\n             let elems = [a, b];\n             for i in 0..2 {\n                 let mut elem = elems[i];\n                 // Extend boolean i1's to i8.\n-                if common::val_ty(elem) == Type::i1(bcx.ccx()) {\n-                    elem = bcx.zext(elem, Type::i8(bcx.ccx()));\n+                if common::val_ty(elem) == Type::i1(bcx.ccx) {\n+                    elem = bcx.zext(elem, Type::i8(bcx.ccx));\n                 }\n                 llpair = bcx.insert_value(llpair, elem, i);\n             }\n@@ -107,23 +107,23 @@ impl<'bcx, 'tcx> OperandRef<'tcx> {\n \n     /// If this operand is a pair in an Immediate,\n     /// we return a Pair with the two halves.\n-    pub fn unpack_if_pair(mut self, bcx: &BlockAndBuilder<'bcx, 'tcx>)\n+    pub fn unpack_if_pair(mut self, bcx: &BlockAndBuilder<'a, 'tcx>)\n                           -> OperandRef<'tcx> {\n         if let OperandValue::Immediate(llval) = self.val {\n             // Deconstruct the immediate aggregate.\n-            if common::type_is_imm_pair(bcx.ccx(), self.ty) {\n+            if common::type_is_imm_pair(bcx.ccx, self.ty) {\n                 debug!(\"Operand::unpack_if_pair: unpacking {:?}\", self);\n \n                 let mut a = bcx.extract_value(llval, 0);\n                 let mut b = bcx.extract_value(llval, 1);\n \n-                let pair_fields = common::type_pair_fields(bcx.ccx(), self.ty);\n+                let pair_fields = common::type_pair_fields(bcx.ccx, self.ty);\n                 if let Some([a_ty, b_ty]) = pair_fields {\n                     if a_ty.is_bool() {\n-                        a = bcx.trunc(a, Type::i1(bcx.ccx()));\n+                        a = bcx.trunc(a, Type::i1(bcx.ccx));\n                     }\n                     if b_ty.is_bool() {\n-                        b = bcx.trunc(b, Type::i1(bcx.ccx()));\n+                        b = bcx.trunc(b, Type::i1(bcx.ccx));\n                     }\n                 }\n \n@@ -134,29 +134,29 @@ impl<'bcx, 'tcx> OperandRef<'tcx> {\n     }\n }\n \n-impl<'bcx, 'tcx> MirContext<'bcx, 'tcx> {\n+impl<'a, 'tcx> MirContext<'a, 'tcx> {\n     pub fn trans_load(&mut self,\n-                      bcx: &BlockAndBuilder<'bcx, 'tcx>,\n+                      bcx: &BlockAndBuilder<'a, 'tcx>,\n                       llval: ValueRef,\n                       ty: Ty<'tcx>)\n                       -> OperandRef<'tcx>\n     {\n         debug!(\"trans_load: {:?} @ {:?}\", Value(llval), ty);\n \n-        let val = if common::type_is_fat_ptr(bcx.tcx(), ty) {\n-            let (lldata, llextra) = base::load_fat_ptr_builder(bcx, llval, ty);\n+        let val = if common::type_is_fat_ptr(bcx.ccx, ty) {\n+            let (lldata, llextra) = base::load_fat_ptr(bcx, llval, ty);\n             OperandValue::Pair(lldata, llextra)\n-        } else if common::type_is_imm_pair(bcx.ccx(), ty) {\n-            let [a_ty, b_ty] = common::type_pair_fields(bcx.ccx(), ty).unwrap();\n+        } else if common::type_is_imm_pair(bcx.ccx, ty) {\n+            let [a_ty, b_ty] = common::type_pair_fields(bcx.ccx, ty).unwrap();\n             let a_ptr = bcx.struct_gep(llval, 0);\n             let b_ptr = bcx.struct_gep(llval, 1);\n \n             OperandValue::Pair(\n-                base::load_ty_builder(bcx, a_ptr, a_ty),\n-                base::load_ty_builder(bcx, b_ptr, b_ty)\n+                base::load_ty(bcx, a_ptr, a_ty),\n+                base::load_ty(bcx, b_ptr, b_ty)\n             )\n-        } else if common::type_is_immediate(bcx.ccx(), ty) {\n-            OperandValue::Immediate(base::load_ty_builder(bcx, llval, ty))\n+        } else if common::type_is_immediate(bcx.ccx, ty) {\n+            OperandValue::Immediate(base::load_ty(bcx, llval, ty))\n         } else {\n             OperandValue::Ref(llval)\n         };\n@@ -165,7 +165,7 @@ impl<'bcx, 'tcx> MirContext<'bcx, 'tcx> {\n     }\n \n     pub fn trans_consume(&mut self,\n-                         bcx: &BlockAndBuilder<'bcx, 'tcx>,\n+                         bcx: &BlockAndBuilder<'a, 'tcx>,\n                          lvalue: &mir::Lvalue<'tcx>)\n                          -> OperandRef<'tcx>\n     {\n@@ -197,7 +197,7 @@ impl<'bcx, 'tcx> MirContext<'bcx, 'tcx> {\n                             let llval = [a, b][f.index()];\n                             let op = OperandRef {\n                                 val: OperandValue::Immediate(llval),\n-                                ty: bcx.monomorphize(&ty)\n+                                ty: self.monomorphize(&ty)\n                             };\n \n                             // Handle nested pairs.\n@@ -217,7 +217,7 @@ impl<'bcx, 'tcx> MirContext<'bcx, 'tcx> {\n     }\n \n     pub fn trans_operand(&mut self,\n-                         bcx: &BlockAndBuilder<'bcx, 'tcx>,\n+                         bcx: &BlockAndBuilder<'a, 'tcx>,\n                          operand: &mir::Operand<'tcx>)\n                          -> OperandRef<'tcx>\n     {\n@@ -230,7 +230,7 @@ impl<'bcx, 'tcx> MirContext<'bcx, 'tcx> {\n \n             mir::Operand::Constant(ref constant) => {\n                 let val = self.trans_constant(bcx, constant);\n-                let operand = val.to_operand(bcx.ccx());\n+                let operand = val.to_operand(bcx.ccx);\n                 if let OperandValue::Ref(ptr) = operand.val {\n                     // If this is a OperandValue::Ref to an immediate constant, load it.\n                     self.trans_load(bcx, ptr, operand.ty)\n@@ -242,33 +242,23 @@ impl<'bcx, 'tcx> MirContext<'bcx, 'tcx> {\n     }\n \n     pub fn store_operand(&mut self,\n-                         bcx: &BlockAndBuilder<'bcx, 'tcx>,\n+                         bcx: &BlockAndBuilder<'a, 'tcx>,\n                          lldest: ValueRef,\n-                         operand: OperandRef<'tcx>)\n-    {\n-        debug!(\"store_operand: operand={:?} lldest={:?}\", operand, lldest);\n-        bcx.with_block(|bcx| self.store_operand_direct(bcx, lldest, operand))\n-    }\n-\n-    pub fn store_operand_direct(&mut self,\n-                                bcx: Block<'bcx, 'tcx>,\n-                                lldest: ValueRef,\n-                                operand: OperandRef<'tcx>)\n-    {\n+                         operand: OperandRef<'tcx>) {\n+        debug!(\"store_operand: operand={:?}\", operand);\n         // Avoid generating stores of zero-sized values, because the only way to have a zero-sized\n         // value is through `undef`, and store itself is useless.\n-        if common::type_is_zero_size(bcx.ccx(), operand.ty) {\n+        if common::type_is_zero_size(bcx.ccx, operand.ty) {\n             return;\n         }\n         match operand.val {\n             OperandValue::Ref(r) => base::memcpy_ty(bcx, lldest, r, operand.ty),\n             OperandValue::Immediate(s) => base::store_ty(bcx, s, lldest, operand.ty),\n             OperandValue::Pair(a, b) => {\n-                use build::*;\n                 let a = base::from_immediate(bcx, a);\n                 let b = base::from_immediate(bcx, b);\n-                Store(bcx, a, StructGEP(bcx, lldest, 0));\n-                Store(bcx, b, StructGEP(bcx, lldest, 1));\n+                bcx.store(a, bcx.struct_gep(lldest, 0));\n+                bcx.store(b, bcx.struct_gep(lldest, 1));\n             }\n         }\n     }"}, {"sha": "b17550087edf77c837870561b2f5dba8c0d333e2", "filename": "src/librustc_trans/mir/rvalue.rs", "status": "modified", "additions": 95, "deletions": 137, "changes": 232, "blob_url": "https://github.com/rust-lang/rust/blob/1b38776c1f68c6fd47c1b2f7b7974efc7dd64901/src%2Flibrustc_trans%2Fmir%2Frvalue.rs", "raw_url": "https://github.com/rust-lang/rust/raw/1b38776c1f68c6fd47c1b2f7b7974efc7dd64901/src%2Flibrustc_trans%2Fmir%2Frvalue.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_trans%2Fmir%2Frvalue.rs?ref=1b38776c1f68c6fd47c1b2f7b7974efc7dd64901", "patch": "@@ -13,13 +13,13 @@ use rustc::ty::{self, Ty};\n use rustc::ty::cast::{CastTy, IntTy};\n use rustc::ty::layout::Layout;\n use rustc::mir;\n+use middle::lang_items::ExchangeMallocFnLangItem;\n \n use asm;\n use base;\n use callee::Callee;\n-use common::{self, val_ty, C_bool, C_null, C_uint, BlockAndBuilder, Result};\n+use common::{self, val_ty, C_bool, C_null, C_uint, BlockAndBuilder};\n use common::{C_integral};\n-use debuginfo::DebugLoc;\n use adt;\n use machine;\n use type_::Type;\n@@ -33,13 +33,12 @@ use super::constant::const_scalar_checked_binop;\n use super::operand::{OperandRef, OperandValue};\n use super::lvalue::{LvalueRef};\n \n-impl<'bcx, 'tcx> MirContext<'bcx, 'tcx> {\n+impl<'a, 'tcx> MirContext<'a, 'tcx> {\n     pub fn trans_rvalue(&mut self,\n-                        bcx: BlockAndBuilder<'bcx, 'tcx>,\n+                        bcx: BlockAndBuilder<'a, 'tcx>,\n                         dest: LvalueRef<'tcx>,\n-                        rvalue: &mir::Rvalue<'tcx>,\n-                        debug_loc: DebugLoc)\n-                        -> BlockAndBuilder<'bcx, 'tcx>\n+                        rvalue: &mir::Rvalue<'tcx>)\n+                        -> BlockAndBuilder<'a, 'tcx>\n     {\n         debug!(\"trans_rvalue(dest.llval={:?}, rvalue={:?})\",\n                Value(dest.llval), rvalue);\n@@ -54,12 +53,12 @@ impl<'bcx, 'tcx> MirContext<'bcx, 'tcx> {\n            }\n \n             mir::Rvalue::Cast(mir::CastKind::Unsize, ref source, cast_ty) => {\n-                let cast_ty = bcx.monomorphize(&cast_ty);\n+                let cast_ty = self.monomorphize(&cast_ty);\n \n-                if common::type_is_fat_ptr(bcx.tcx(), cast_ty) {\n+                if common::type_is_fat_ptr(bcx.ccx, cast_ty) {\n                     // into-coerce of a thin pointer to a fat pointer - just\n                     // use the operand path.\n-                    let (bcx, temp) = self.trans_rvalue_operand(bcx, rvalue, debug_loc);\n+                    let (bcx, temp) = self.trans_rvalue_operand(bcx, rvalue);\n                     self.store_operand(&bcx, dest.llval, temp);\n                     return bcx;\n                 }\n@@ -70,71 +69,57 @@ impl<'bcx, 'tcx> MirContext<'bcx, 'tcx> {\n                 // so the (generic) MIR may not be able to expand it.\n                 let operand = self.trans_operand(&bcx, source);\n                 let operand = operand.pack_if_pair(&bcx);\n-                bcx.with_block(|bcx| {\n-                    match operand.val {\n-                        OperandValue::Pair(..) => bug!(),\n-                        OperandValue::Immediate(llval) => {\n-                            // unsize from an immediate structure. We don't\n-                            // really need a temporary alloca here, but\n-                            // avoiding it would require us to have\n-                            // `coerce_unsized_into` use extractvalue to\n-                            // index into the struct, and this case isn't\n-                            // important enough for it.\n-                            debug!(\"trans_rvalue: creating ugly alloca\");\n-                            let lltemp = base::alloc_ty(bcx, operand.ty, \"__unsize_temp\");\n-                            base::store_ty(bcx, llval, lltemp, operand.ty);\n-                            base::coerce_unsized_into(bcx,\n-                                                      lltemp, operand.ty,\n-                                                      dest.llval, cast_ty);\n-                        }\n-                        OperandValue::Ref(llref) => {\n-                            base::coerce_unsized_into(bcx,\n-                                                      llref, operand.ty,\n-                                                      dest.llval, cast_ty);\n-                        }\n+                let llref = match operand.val {\n+                    OperandValue::Pair(..) => bug!(),\n+                    OperandValue::Immediate(llval) => {\n+                        // unsize from an immediate structure. We don't\n+                        // really need a temporary alloca here, but\n+                        // avoiding it would require us to have\n+                        // `coerce_unsized_into` use extractvalue to\n+                        // index into the struct, and this case isn't\n+                        // important enough for it.\n+                        debug!(\"trans_rvalue: creating ugly alloca\");\n+                        let lltemp = base::alloc_ty(&bcx, operand.ty, \"__unsize_temp\");\n+                        base::store_ty(&bcx, llval, lltemp, operand.ty);\n+                        lltemp\n                     }\n-                });\n+                    OperandValue::Ref(llref) => llref\n+                };\n+                base::coerce_unsized_into(&bcx, llref, operand.ty, dest.llval, cast_ty);\n                 bcx\n             }\n \n             mir::Rvalue::Repeat(ref elem, ref count) => {\n                 let tr_elem = self.trans_operand(&bcx, elem);\n                 let size = count.value.as_u64(bcx.tcx().sess.target.uint_type);\n-                let size = C_uint(bcx.ccx(), size);\n-                let base = base::get_dataptr_builder(&bcx, dest.llval);\n-                let bcx = bcx.map_block(|block| {\n-                    tvec::slice_for_each(block, base, tr_elem.ty, size, |block, llslot| {\n-                        self.store_operand_direct(block, llslot, tr_elem);\n-                        block\n-                    })\n-                });\n-                bcx\n+                let size = C_uint(bcx.ccx, size);\n+                let base = base::get_dataptr(&bcx, dest.llval);\n+                tvec::slice_for_each(&bcx, base, tr_elem.ty, size, |bcx, llslot| {\n+                    self.store_operand(bcx, llslot, tr_elem);\n+                })\n             }\n \n             mir::Rvalue::Aggregate(ref kind, ref operands) => {\n                 match *kind {\n                     mir::AggregateKind::Adt(adt_def, variant_index, _, active_field_index) => {\n                         let disr = Disr::from(adt_def.variants[variant_index].disr_val);\n-                        bcx.with_block(|bcx| {\n-                            adt::trans_set_discr(bcx,\n-                                dest.ty.to_ty(bcx.tcx()), dest.llval, Disr::from(disr));\n-                        });\n+                        let dest_ty = dest.ty.to_ty(bcx.tcx());\n+                        adt::trans_set_discr(&bcx, dest_ty, dest.llval, Disr::from(disr));\n                         for (i, operand) in operands.iter().enumerate() {\n                             let op = self.trans_operand(&bcx, operand);\n                             // Do not generate stores and GEPis for zero-sized fields.\n-                            if !common::type_is_zero_size(bcx.ccx(), op.ty) {\n+                            if !common::type_is_zero_size(bcx.ccx, op.ty) {\n                                 let val = adt::MaybeSizedValue::sized(dest.llval);\n                                 let field_index = active_field_index.unwrap_or(i);\n-                                let lldest_i = adt::trans_field_ptr_builder(&bcx,\n-                                    dest.ty.to_ty(bcx.tcx()),\n-                                    val, disr, field_index);\n+                                let lldest_i = adt::trans_field_ptr(&bcx, dest_ty, val, disr,\n+                                    field_index);\n                                 self.store_operand(&bcx, lldest_i, op);\n                             }\n                         }\n                     },\n                     _ => {\n                         // If this is a tuple or closure, we need to translate GEP indices.\n-                        let layout = bcx.ccx().layout_of(dest.ty.to_ty(bcx.tcx()));\n+                        let layout = bcx.ccx.layout_of(dest.ty.to_ty(bcx.tcx()));\n                         let translation = if let Layout::Univariant { ref variant, .. } = *layout {\n                             Some(&variant.memory_index)\n                         } else {\n@@ -143,7 +128,7 @@ impl<'bcx, 'tcx> MirContext<'bcx, 'tcx> {\n                         for (i, operand) in operands.iter().enumerate() {\n                             let op = self.trans_operand(&bcx, operand);\n                             // Do not generate stores and GEPis for zero-sized fields.\n-                            if !common::type_is_zero_size(bcx.ccx(), op.ty) {\n+                            if !common::type_is_zero_size(bcx.ccx, op.ty) {\n                                 // Note: perhaps this should be StructGep, but\n                                 // note that in some cases the values here will\n                                 // not be structs but arrays.\n@@ -171,44 +156,39 @@ impl<'bcx, 'tcx> MirContext<'bcx, 'tcx> {\n                     self.trans_operand(&bcx, input).immediate()\n                 }).collect();\n \n-                bcx.with_block(|bcx| {\n-                    asm::trans_inline_asm(bcx, asm, outputs, input_vals);\n-                });\n-\n+                asm::trans_inline_asm(&bcx, asm, outputs, input_vals);\n                 bcx\n             }\n \n             _ => {\n-                assert!(rvalue_creates_operand(&self.mir, &bcx, rvalue));\n-                let (bcx, temp) = self.trans_rvalue_operand(bcx, rvalue, debug_loc);\n+                assert!(rvalue_creates_operand(rvalue));\n+                let (bcx, temp) = self.trans_rvalue_operand(bcx, rvalue);\n                 self.store_operand(&bcx, dest.llval, temp);\n                 bcx\n             }\n         }\n     }\n \n     pub fn trans_rvalue_operand(&mut self,\n-                                bcx: BlockAndBuilder<'bcx, 'tcx>,\n-                                rvalue: &mir::Rvalue<'tcx>,\n-                                debug_loc: DebugLoc)\n-                                -> (BlockAndBuilder<'bcx, 'tcx>, OperandRef<'tcx>)\n+                                bcx: BlockAndBuilder<'a, 'tcx>,\n+                                rvalue: &mir::Rvalue<'tcx>)\n+                                -> (BlockAndBuilder<'a, 'tcx>, OperandRef<'tcx>)\n     {\n-        assert!(rvalue_creates_operand(&self.mir, &bcx, rvalue),\n-                \"cannot trans {:?} to operand\", rvalue);\n+        assert!(rvalue_creates_operand(rvalue), \"cannot trans {:?} to operand\", rvalue);\n \n         match *rvalue {\n             mir::Rvalue::Cast(ref kind, ref source, cast_ty) => {\n                 let operand = self.trans_operand(&bcx, source);\n                 debug!(\"cast operand is {:?}\", operand);\n-                let cast_ty = bcx.monomorphize(&cast_ty);\n+                let cast_ty = self.monomorphize(&cast_ty);\n \n                 let val = match *kind {\n                     mir::CastKind::ReifyFnPointer => {\n                         match operand.ty.sty {\n                             ty::TyFnDef(def_id, substs, _) => {\n                                 OperandValue::Immediate(\n-                                    Callee::def(bcx.ccx(), def_id, substs)\n-                                        .reify(bcx.ccx()))\n+                                    Callee::def(bcx.ccx, def_id, substs)\n+                                        .reify(bcx.ccx))\n                             }\n                             _ => {\n                                 bug!(\"{} cannot be reified to a fn ptr\", operand.ty)\n@@ -222,7 +202,7 @@ impl<'bcx, 'tcx> MirContext<'bcx, 'tcx> {\n                     mir::CastKind::Unsize => {\n                         // unsize targets other than to a fat pointer currently\n                         // can't be operands.\n-                        assert!(common::type_is_fat_ptr(bcx.tcx(), cast_ty));\n+                        assert!(common::type_is_fat_ptr(bcx.ccx, cast_ty));\n \n                         match operand.val {\n                             OperandValue::Pair(lldata, llextra) => {\n@@ -232,16 +212,14 @@ impl<'bcx, 'tcx> MirContext<'bcx, 'tcx> {\n                                 //   &'a fmt::Debug+Send => &'a fmt::Debug,\n                                 // So we need to pointercast the base to ensure\n                                 // the types match up.\n-                                let llcast_ty = type_of::fat_ptr_base_ty(bcx.ccx(), cast_ty);\n+                                let llcast_ty = type_of::fat_ptr_base_ty(bcx.ccx, cast_ty);\n                                 let lldata = bcx.pointercast(lldata, llcast_ty);\n                                 OperandValue::Pair(lldata, llextra)\n                             }\n                             OperandValue::Immediate(lldata) => {\n                                 // \"standard\" unsize\n-                                let (lldata, llextra) = bcx.with_block(|bcx| {\n-                                    base::unsize_thin_ptr(bcx, lldata,\n-                                                          operand.ty, cast_ty)\n-                                });\n+                                let (lldata, llextra) = base::unsize_thin_ptr(&bcx, lldata,\n+                                    operand.ty, cast_ty);\n                                 OperandValue::Pair(lldata, llextra)\n                             }\n                             OperandValue::Ref(_) => {\n@@ -250,11 +228,11 @@ impl<'bcx, 'tcx> MirContext<'bcx, 'tcx> {\n                             }\n                         }\n                     }\n-                    mir::CastKind::Misc if common::type_is_fat_ptr(bcx.tcx(), operand.ty) => {\n-                        let ll_cast_ty = type_of::immediate_type_of(bcx.ccx(), cast_ty);\n-                        let ll_from_ty = type_of::immediate_type_of(bcx.ccx(), operand.ty);\n+                    mir::CastKind::Misc if common::type_is_fat_ptr(bcx.ccx, operand.ty) => {\n+                        let ll_cast_ty = type_of::immediate_type_of(bcx.ccx, cast_ty);\n+                        let ll_from_ty = type_of::immediate_type_of(bcx.ccx, operand.ty);\n                         if let OperandValue::Pair(data_ptr, meta_ptr) = operand.val {\n-                            if common::type_is_fat_ptr(bcx.tcx(), cast_ty) {\n+                            if common::type_is_fat_ptr(bcx.ccx, cast_ty) {\n                                 let ll_cft = ll_cast_ty.field_types();\n                                 let ll_fft = ll_from_ty.field_types();\n                                 let data_cast = bcx.pointercast(data_ptr, ll_cft[0]);\n@@ -271,19 +249,17 @@ impl<'bcx, 'tcx> MirContext<'bcx, 'tcx> {\n                         }\n                     }\n                     mir::CastKind::Misc => {\n-                        debug_assert!(common::type_is_immediate(bcx.ccx(), cast_ty));\n+                        debug_assert!(common::type_is_immediate(bcx.ccx, cast_ty));\n                         let r_t_in = CastTy::from_ty(operand.ty).expect(\"bad input type for cast\");\n                         let r_t_out = CastTy::from_ty(cast_ty).expect(\"bad output type for cast\");\n-                        let ll_t_in = type_of::immediate_type_of(bcx.ccx(), operand.ty);\n-                        let ll_t_out = type_of::immediate_type_of(bcx.ccx(), cast_ty);\n+                        let ll_t_in = type_of::immediate_type_of(bcx.ccx, operand.ty);\n+                        let ll_t_out = type_of::immediate_type_of(bcx.ccx, cast_ty);\n                         let (llval, signed) = if let CastTy::Int(IntTy::CEnum) = r_t_in {\n-                            let l = bcx.ccx().layout_of(operand.ty);\n+                            let l = bcx.ccx.layout_of(operand.ty);\n                             let discr = match operand.val {\n                                 OperandValue::Immediate(llval) => llval,\n                                 OperandValue::Ref(llptr) => {\n-                                    bcx.with_block(|bcx| {\n-                                        adt::trans_get_discr(bcx, operand.ty, llptr, None, true)\n-                                    })\n+                                    adt::trans_get_discr(&bcx, operand.ty, llptr, None, true)\n                                 }\n                                 OperandValue::Pair(..) => bug!(\"Unexpected Pair operand\")\n                             };\n@@ -376,7 +352,7 @@ impl<'bcx, 'tcx> MirContext<'bcx, 'tcx> {\n \n                 // Note: lvalues are indirect, so storing the `llval` into the\n                 // destination effectively creates a reference.\n-                let operand = if common::type_is_sized(bcx.tcx(), ty) {\n+                let operand = if bcx.ccx.shared().type_is_sized(ty) {\n                     OperandRef {\n                         val: OperandValue::Immediate(tr_lvalue.llval),\n                         ty: ref_ty,\n@@ -394,7 +370,7 @@ impl<'bcx, 'tcx> MirContext<'bcx, 'tcx> {\n             mir::Rvalue::Len(ref lvalue) => {\n                 let tr_lvalue = self.trans_lvalue(&bcx, lvalue);\n                 let operand = OperandRef {\n-                    val: OperandValue::Immediate(tr_lvalue.len(bcx.ccx())),\n+                    val: OperandValue::Immediate(tr_lvalue.len(bcx.ccx)),\n                     ty: bcx.tcx().types.usize,\n                 };\n                 (bcx, operand)\n@@ -403,7 +379,7 @@ impl<'bcx, 'tcx> MirContext<'bcx, 'tcx> {\n             mir::Rvalue::BinaryOp(op, ref lhs, ref rhs) => {\n                 let lhs = self.trans_operand(&bcx, lhs);\n                 let rhs = self.trans_operand(&bcx, rhs);\n-                let llresult = if common::type_is_fat_ptr(bcx.tcx(), lhs.ty) {\n+                let llresult = if common::type_is_fat_ptr(bcx.ccx, lhs.ty) {\n                     match (lhs.val, rhs.val) {\n                         (OperandValue::Pair(lhs_addr, lhs_extra),\n                          OperandValue::Pair(rhs_addr, rhs_extra)) => {\n@@ -461,26 +437,27 @@ impl<'bcx, 'tcx> MirContext<'bcx, 'tcx> {\n             }\n \n             mir::Rvalue::Box(content_ty) => {\n-                let content_ty: Ty<'tcx> = bcx.monomorphize(&content_ty);\n-                let llty = type_of::type_of(bcx.ccx(), content_ty);\n-                let llsize = machine::llsize_of(bcx.ccx(), llty);\n-                let align = type_of::align_of(bcx.ccx(), content_ty);\n-                let llalign = C_uint(bcx.ccx(), align);\n+                let content_ty: Ty<'tcx> = self.monomorphize(&content_ty);\n+                let llty = type_of::type_of(bcx.ccx, content_ty);\n+                let llsize = machine::llsize_of(bcx.ccx, llty);\n+                let align = type_of::align_of(bcx.ccx, content_ty);\n+                let llalign = C_uint(bcx.ccx, align);\n                 let llty_ptr = llty.ptr_to();\n                 let box_ty = bcx.tcx().mk_box(content_ty);\n-                let mut llval = None;\n-                let bcx = bcx.map_block(|bcx| {\n-                    let Result { bcx, val } = base::malloc_raw_dyn(bcx,\n-                                                                   llty_ptr,\n-                                                                   box_ty,\n-                                                                   llsize,\n-                                                                   llalign,\n-                                                                   debug_loc);\n-                    llval = Some(val);\n-                    bcx\n-                });\n+\n+                // Allocate space:\n+                let def_id = match bcx.tcx().lang_items.require(ExchangeMallocFnLangItem) {\n+                    Ok(id) => id,\n+                    Err(s) => {\n+                        bcx.sess().fatal(&format!(\"allocation of `{}` {}\", box_ty, s));\n+                    }\n+                };\n+                let r = Callee::def(bcx.ccx, def_id, bcx.tcx().intern_substs(&[]))\n+                    .reify(bcx.ccx);\n+                let val = bcx.pointercast(bcx.call(r, &[llsize, llalign], None), llty_ptr);\n+\n                 let operand = OperandRef {\n-                    val: OperandValue::Immediate(llval.unwrap()),\n+                    val: OperandValue::Immediate(val),\n                     ty: box_ty,\n                 };\n                 (bcx, operand)\n@@ -500,7 +477,7 @@ impl<'bcx, 'tcx> MirContext<'bcx, 'tcx> {\n     }\n \n     pub fn trans_scalar_binop(&mut self,\n-                              bcx: &BlockAndBuilder<'bcx, 'tcx>,\n+                              bcx: &BlockAndBuilder<'a, 'tcx>,\n                               op: mir::BinOp,\n                               lhs: ValueRef,\n                               rhs: ValueRef,\n@@ -542,26 +519,11 @@ impl<'bcx, 'tcx> MirContext<'bcx, 'tcx> {\n             mir::BinOp::BitOr => bcx.or(lhs, rhs),\n             mir::BinOp::BitAnd => bcx.and(lhs, rhs),\n             mir::BinOp::BitXor => bcx.xor(lhs, rhs),\n-            mir::BinOp::Shl => {\n-                bcx.with_block(|bcx| {\n-                    common::build_unchecked_lshift(bcx,\n-                                                   lhs,\n-                                                   rhs,\n-                                                   DebugLoc::None)\n-                })\n-            }\n-            mir::BinOp::Shr => {\n-                bcx.with_block(|bcx| {\n-                    common::build_unchecked_rshift(bcx,\n-                                                   input_ty,\n-                                                   lhs,\n-                                                   rhs,\n-                                                   DebugLoc::None)\n-                })\n-            }\n+            mir::BinOp::Shl => common::build_unchecked_lshift(bcx, lhs, rhs),\n+            mir::BinOp::Shr => common::build_unchecked_rshift(bcx, input_ty, lhs, rhs),\n             mir::BinOp::Ne | mir::BinOp::Lt | mir::BinOp::Gt |\n             mir::BinOp::Eq | mir::BinOp::Le | mir::BinOp::Ge => if is_nil {\n-                C_bool(bcx.ccx(), match op {\n+                C_bool(bcx.ccx, match op {\n                     mir::BinOp::Ne | mir::BinOp::Lt | mir::BinOp::Gt => false,\n                     mir::BinOp::Eq | mir::BinOp::Le | mir::BinOp::Ge => true,\n                     _ => unreachable!()\n@@ -575,8 +537,8 @@ impl<'bcx, 'tcx> MirContext<'bcx, 'tcx> {\n                 let (lhs, rhs) = if is_bool {\n                     // FIXME(#36856) -- extend the bools into `i8` because\n                     // LLVM's i1 comparisons are broken.\n-                    (bcx.zext(lhs, Type::i8(bcx.ccx())),\n-                     bcx.zext(rhs, Type::i8(bcx.ccx())))\n+                    (bcx.zext(lhs, Type::i8(bcx.ccx)),\n+                     bcx.zext(rhs, Type::i8(bcx.ccx)))\n                 } else {\n                     (lhs, rhs)\n                 };\n@@ -590,7 +552,7 @@ impl<'bcx, 'tcx> MirContext<'bcx, 'tcx> {\n     }\n \n     pub fn trans_fat_ptr_binop(&mut self,\n-                               bcx: &BlockAndBuilder<'bcx, 'tcx>,\n+                               bcx: &BlockAndBuilder<'a, 'tcx>,\n                                op: mir::BinOp,\n                                lhs_addr: ValueRef,\n                                lhs_extra: ValueRef,\n@@ -637,7 +599,7 @@ impl<'bcx, 'tcx> MirContext<'bcx, 'tcx> {\n     }\n \n     pub fn trans_scalar_checked_binop(&mut self,\n-                                      bcx: &BlockAndBuilder<'bcx, 'tcx>,\n+                                      bcx: &BlockAndBuilder<'a, 'tcx>,\n                                       op: mir::BinOp,\n                                       lhs: ValueRef,\n                                       rhs: ValueRef,\n@@ -646,17 +608,17 @@ impl<'bcx, 'tcx> MirContext<'bcx, 'tcx> {\n         // with #[rustc_inherit_overflow_checks] and inlined from\n         // another crate (mostly core::num generic/#[inline] fns),\n         // while the current crate doesn't use overflow checks.\n-        if !bcx.ccx().check_overflow() {\n+        if !bcx.ccx.check_overflow() {\n             let val = self.trans_scalar_binop(bcx, op, lhs, rhs, input_ty);\n-            return OperandValue::Pair(val, C_bool(bcx.ccx(), false));\n+            return OperandValue::Pair(val, C_bool(bcx.ccx, false));\n         }\n \n         // First try performing the operation on constants, which\n         // will only succeed if both operands are constant.\n         // This is necessary to determine when an overflow Assert\n         // will always panic at runtime, and produce a warning.\n         if let Some((val, of)) = const_scalar_checked_binop(bcx.tcx(), op, lhs, rhs, input_ty) {\n-            return OperandValue::Pair(val, C_bool(bcx.ccx(), of));\n+            return OperandValue::Pair(val, C_bool(bcx.ccx, of));\n         }\n \n         let (val, of) = match op {\n@@ -677,9 +639,7 @@ impl<'bcx, 'tcx> MirContext<'bcx, 'tcx> {\n             mir::BinOp::Shl | mir::BinOp::Shr => {\n                 let lhs_llty = val_ty(lhs);\n                 let rhs_llty = val_ty(rhs);\n-                let invert_mask = bcx.with_block(|bcx| {\n-                    common::shift_mask_val(bcx, lhs_llty, rhs_llty, true)\n-                });\n+                let invert_mask = common::shift_mask_val(&bcx, lhs_llty, rhs_llty, true);\n                 let outer_bits = bcx.and(rhs, invert_mask);\n \n                 let of = bcx.icmp(llvm::IntNE, outer_bits, C_null(rhs_llty));\n@@ -696,9 +656,7 @@ impl<'bcx, 'tcx> MirContext<'bcx, 'tcx> {\n     }\n }\n \n-pub fn rvalue_creates_operand<'bcx, 'tcx>(_mir: &mir::Mir<'tcx>,\n-                                          _bcx: &BlockAndBuilder<'bcx, 'tcx>,\n-                                          rvalue: &mir::Rvalue<'tcx>) -> bool {\n+pub fn rvalue_creates_operand(rvalue: &mir::Rvalue) -> bool {\n     match *rvalue {\n         mir::Rvalue::Ref(..) |\n         mir::Rvalue::Len(..) |\n@@ -789,5 +747,5 @@ fn get_overflow_intrinsic(oop: OverflowOp, bcx: &BlockAndBuilder, ty: Ty) -> Val\n         },\n     };\n \n-    bcx.ccx().get_intrinsic(&name)\n+    bcx.ccx.get_intrinsic(&name)\n }"}, {"sha": "cc85f68c197ec34551df74a1742b7d171ddf5bc5", "filename": "src/librustc_trans/mir/statement.rs", "status": "modified", "additions": 15, "deletions": 20, "changes": 35, "blob_url": "https://github.com/rust-lang/rust/blob/1b38776c1f68c6fd47c1b2f7b7974efc7dd64901/src%2Flibrustc_trans%2Fmir%2Fstatement.rs", "raw_url": "https://github.com/rust-lang/rust/raw/1b38776c1f68c6fd47c1b2f7b7974efc7dd64901/src%2Flibrustc_trans%2Fmir%2Fstatement.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_trans%2Fmir%2Fstatement.rs?ref=1b38776c1f68c6fd47c1b2f7b7974efc7dd64901", "patch": "@@ -18,57 +18,52 @@ use super::LocalRef;\n use super::super::adt;\n use super::super::disr::Disr;\n \n-impl<'bcx, 'tcx> MirContext<'bcx, 'tcx> {\n+impl<'a, 'tcx> MirContext<'a, 'tcx> {\n     pub fn trans_statement(&mut self,\n-                           bcx: BlockAndBuilder<'bcx, 'tcx>,\n+                           bcx: BlockAndBuilder<'a, 'tcx>,\n                            statement: &mir::Statement<'tcx>)\n-                           -> BlockAndBuilder<'bcx, 'tcx> {\n+                           -> BlockAndBuilder<'a, 'tcx> {\n         debug!(\"trans_statement(statement={:?})\", statement);\n \n-        let debug_loc = self.debug_loc(statement.source_info);\n-        debug_loc.apply_to_bcx(&bcx);\n-        debug_loc.apply(bcx.fcx());\n+        self.set_debug_loc(&bcx, statement.source_info);\n         match statement.kind {\n             mir::StatementKind::Assign(ref lvalue, ref rvalue) => {\n                 if let mir::Lvalue::Local(index) = *lvalue {\n                     match self.locals[index] {\n                         LocalRef::Lvalue(tr_dest) => {\n-                            self.trans_rvalue(bcx, tr_dest, rvalue, debug_loc)\n+                            self.trans_rvalue(bcx, tr_dest, rvalue)\n                         }\n                         LocalRef::Operand(None) => {\n-                            let (bcx, operand) = self.trans_rvalue_operand(bcx, rvalue,\n-                                                                           debug_loc);\n+                            let (bcx, operand) = self.trans_rvalue_operand(bcx, rvalue);\n                             self.locals[index] = LocalRef::Operand(Some(operand));\n                             bcx\n                         }\n                         LocalRef::Operand(Some(_)) => {\n                             let ty = self.monomorphized_lvalue_ty(lvalue);\n \n-                            if !common::type_is_zero_size(bcx.ccx(), ty) {\n+                            if !common::type_is_zero_size(bcx.ccx, ty) {\n                                 span_bug!(statement.source_info.span,\n                                           \"operand {:?} already assigned\",\n                                           rvalue);\n                             } else {\n                                 // If the type is zero-sized, it's already been set here,\n                                 // but we still need to make sure we translate the operand\n-                                self.trans_rvalue_operand(bcx, rvalue, debug_loc).0\n+                                self.trans_rvalue_operand(bcx, rvalue).0\n                             }\n                         }\n                     }\n                 } else {\n                     let tr_dest = self.trans_lvalue(&bcx, lvalue);\n-                    self.trans_rvalue(bcx, tr_dest, rvalue, debug_loc)\n+                    self.trans_rvalue(bcx, tr_dest, rvalue)\n                 }\n             }\n             mir::StatementKind::SetDiscriminant{ref lvalue, variant_index} => {\n                 let ty = self.monomorphized_lvalue_ty(lvalue);\n                 let lvalue_transed = self.trans_lvalue(&bcx, lvalue);\n-                bcx.with_block(|bcx|\n-                    adt::trans_set_discr(bcx,\n-                                         ty,\n-                                        lvalue_transed.llval,\n-                                        Disr::from(variant_index))\n-                );\n+                adt::trans_set_discr(&bcx,\n+                    ty,\n+                    lvalue_transed.llval,\n+                    Disr::from(variant_index));\n                 bcx\n             }\n             mir::StatementKind::StorageLive(ref lvalue) => {\n@@ -82,10 +77,10 @@ impl<'bcx, 'tcx> MirContext<'bcx, 'tcx> {\n     }\n \n     fn trans_storage_liveness(&self,\n-                              bcx: BlockAndBuilder<'bcx, 'tcx>,\n+                              bcx: BlockAndBuilder<'a, 'tcx>,\n                               lvalue: &mir::Lvalue<'tcx>,\n                               intrinsic: base::Lifetime)\n-                              -> BlockAndBuilder<'bcx, 'tcx> {\n+                              -> BlockAndBuilder<'a, 'tcx> {\n         if let mir::Lvalue::Local(index) = *lvalue {\n             if let LocalRef::Lvalue(tr_lval) = self.locals[index] {\n                 intrinsic.call(&bcx, tr_lval.llval);"}, {"sha": "527bee832956a7c25d2e76d04a21abc8b1ff2aa0", "filename": "src/librustc_trans/trans_item.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/1b38776c1f68c6fd47c1b2f7b7974efc7dd64901/src%2Flibrustc_trans%2Ftrans_item.rs", "raw_url": "https://github.com/rust-lang/rust/raw/1b38776c1f68c6fd47c1b2f7b7974efc7dd64901/src%2Flibrustc_trans%2Ftrans_item.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_trans%2Ftrans_item.rs?ref=1b38776c1f68c6fd47c1b2f7b7974efc7dd64901", "patch": "@@ -184,7 +184,7 @@ impl<'a, 'tcx> TransItem<'tcx> {\n                            linkage: llvm::Linkage,\n                            symbol_name: &str) {\n         let tcx = ccx.tcx();\n-        assert_eq!(dg.ty(), glue::get_drop_glue_type(tcx, dg.ty()));\n+        assert_eq!(dg.ty(), glue::get_drop_glue_type(ccx.shared(), dg.ty()));\n         let t = dg.ty();\n \n         let sig = tcx.mk_fn_sig(iter::once(tcx.mk_mut_ptr(tcx.types.i8)), tcx.mk_nil(), false);"}, {"sha": "c09726fda081028c6d48b71d8f9cdecdcf6ad624", "filename": "src/librustc_trans/tvec.rs", "status": "modified", "additions": 24, "deletions": 34, "changes": 58, "blob_url": "https://github.com/rust-lang/rust/blob/1b38776c1f68c6fd47c1b2f7b7974efc7dd64901/src%2Flibrustc_trans%2Ftvec.rs", "raw_url": "https://github.com/rust-lang/rust/raw/1b38776c1f68c6fd47c1b2f7b7974efc7dd64901/src%2Flibrustc_trans%2Ftvec.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_trans%2Ftvec.rs?ref=1b38776c1f68c6fd47c1b2f7b7974efc7dd64901", "patch": "@@ -8,56 +8,46 @@\n // option. This file may not be copied, modified, or distributed\n // except according to those terms.\n \n-#![allow(non_camel_case_types)]\n-\n use llvm;\n use llvm::ValueRef;\n-use base::*;\n-use build::*;\n use common::*;\n-use debuginfo::DebugLoc;\n use rustc::ty::Ty;\n \n-pub fn slice_for_each<'blk, 'tcx, F>(bcx: Block<'blk, 'tcx>,\n-                                     data_ptr: ValueRef,\n-                                     unit_ty: Ty<'tcx>,\n-                                     len: ValueRef,\n-                                     f: F)\n-                                     -> Block<'blk, 'tcx> where\n-    F: FnOnce(Block<'blk, 'tcx>, ValueRef) -> Block<'blk, 'tcx>,\n-{\n-    let _icx = push_ctxt(\"tvec::slice_for_each\");\n-    let fcx = bcx.fcx;\n-\n+pub fn slice_for_each<'a, 'tcx, F>(\n+    bcx: &BlockAndBuilder<'a, 'tcx>,\n+    data_ptr: ValueRef,\n+    unit_ty: Ty<'tcx>,\n+    len: ValueRef,\n+    f: F\n+) -> BlockAndBuilder<'a, 'tcx> where F: FnOnce(&BlockAndBuilder<'a, 'tcx>, ValueRef) {\n     // Special-case vectors with elements of size 0  so they don't go out of bounds (#9890)\n-    let zst = type_is_zero_size(bcx.ccx(), unit_ty);\n-    let add = |bcx, a, b| if zst {\n-        Add(bcx, a, b, DebugLoc::None)\n+    let zst = type_is_zero_size(bcx.ccx, unit_ty);\n+    let add = |bcx: &BlockAndBuilder, a, b| if zst {\n+        bcx.add(a, b)\n     } else {\n-        InBoundsGEP(bcx, a, &[b])\n+        bcx.inbounds_gep(a, &[b])\n     };\n \n-    let header_bcx = fcx.new_block(\"slice_loop_header\");\n-    let body_bcx = fcx.new_block(\"slice_loop_body\");\n-    let next_bcx = fcx.new_block(\"slice_loop_next\");\n+    let body_bcx = bcx.fcx().build_new_block(\"slice_loop_body\");\n+    let next_bcx = bcx.fcx().build_new_block(\"slice_loop_next\");\n+    let header_bcx = bcx.fcx().build_new_block(\"slice_loop_header\");\n \n     let start = if zst {\n-        C_uint(bcx.ccx(), 0 as usize)\n+        C_uint(bcx.ccx, 0usize)\n     } else {\n         data_ptr\n     };\n-    let end = add(bcx, start, len);\n+    let end = add(&bcx, start, len);\n \n-    Br(bcx, header_bcx.llbb, DebugLoc::None);\n-    let current = Phi(header_bcx, val_ty(start), &[start], &[bcx.llbb]);\n+    bcx.br(header_bcx.llbb());\n+    let current = header_bcx.phi(val_ty(start), &[start], &[bcx.llbb()]);\n \n-    let keep_going =\n-        ICmp(header_bcx, llvm::IntNE, current, end, DebugLoc::None);\n-    CondBr(header_bcx, keep_going, body_bcx.llbb, next_bcx.llbb, DebugLoc::None);\n+    let keep_going = header_bcx.icmp(llvm::IntNE, current, end);\n+    header_bcx.cond_br(keep_going, body_bcx.llbb(), next_bcx.llbb());\n \n-    let body_bcx = f(body_bcx, if zst { data_ptr } else { current });\n-    let next = add(body_bcx, current, C_uint(bcx.ccx(), 1usize));\n-    AddIncomingToPhi(current, next, body_bcx.llbb);\n-    Br(body_bcx, header_bcx.llbb, DebugLoc::None);\n+    f(&body_bcx, if zst { data_ptr } else { current });\n+    let next = add(&body_bcx, current, C_uint(bcx.ccx, 1usize));\n+    header_bcx.add_incoming_to_phi(current, next, body_bcx.llbb());\n+    body_bcx.br(header_bcx.llbb());\n     next_bcx\n }"}, {"sha": "469214b466e1ae7b9c3ffeffaa5857703b4c63b2", "filename": "src/librustc_trans/type_of.rs", "status": "modified", "additions": 6, "deletions": 8, "changes": 14, "blob_url": "https://github.com/rust-lang/rust/blob/1b38776c1f68c6fd47c1b2f7b7974efc7dd64901/src%2Flibrustc_trans%2Ftype_of.rs", "raw_url": "https://github.com/rust-lang/rust/raw/1b38776c1f68c6fd47c1b2f7b7974efc7dd64901/src%2Flibrustc_trans%2Ftype_of.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_trans%2Ftype_of.rs?ref=1b38776c1f68c6fd47c1b2f7b7974efc7dd64901", "patch": "@@ -8,8 +8,6 @@\n // option. This file may not be copied, modified, or distributed\n // except according to those terms.\n \n-#![allow(non_camel_case_types)]\n-\n use abi::FnType;\n use adt;\n use common::*;\n@@ -41,7 +39,7 @@ pub fn sizing_type_of<'a, 'tcx>(cx: &CrateContext<'a, 'tcx>, t: Ty<'tcx>) -> Typ\n     let _recursion_lock = cx.enter_type_of(t);\n \n     let llsizingty = match t.sty {\n-        _ if !type_is_sized(cx.tcx(), t) => {\n+        _ if !cx.shared().type_is_sized(t) => {\n             Type::struct_(cx, &[Type::i8p(cx), unsized_info_ty(cx, t)], false)\n         }\n \n@@ -55,7 +53,7 @@ pub fn sizing_type_of<'a, 'tcx>(cx: &CrateContext<'a, 'tcx>, t: Ty<'tcx>) -> Typ\n         ty::TyBox(ty) |\n         ty::TyRef(_, ty::TypeAndMut{ty, ..}) |\n         ty::TyRawPtr(ty::TypeAndMut{ty, ..}) => {\n-            if type_is_sized(cx.tcx(), ty) {\n+            if cx.shared().type_is_sized(ty) {\n                 Type::i8p(cx)\n             } else {\n                 Type::struct_(cx, &[Type::i8p(cx), unsized_info_ty(cx, ty)], false)\n@@ -104,7 +102,7 @@ pub fn sizing_type_of<'a, 'tcx>(cx: &CrateContext<'a, 'tcx>, t: Ty<'tcx>) -> Typ\n \n     // FIXME(eddyb) Temporary sanity check for ty::layout.\n     let layout = cx.layout_of(t);\n-    if !type_is_sized(cx.tcx(), t) {\n+    if !cx.shared().type_is_sized(t) {\n         if !layout.is_unsized() {\n             bug!(\"layout should be unsized for type `{}` / {:#?}\",\n                  t, layout);\n@@ -135,7 +133,7 @@ pub fn fat_ptr_base_ty<'a, 'tcx>(ccx: &CrateContext<'a, 'tcx>, ty: Ty<'tcx>) ->\n     match ty.sty {\n         ty::TyBox(t) |\n         ty::TyRef(_, ty::TypeAndMut { ty: t, .. }) |\n-        ty::TyRawPtr(ty::TypeAndMut { ty: t, .. }) if !type_is_sized(ccx.tcx(), t) => {\n+        ty::TyRawPtr(ty::TypeAndMut { ty: t, .. }) if !ccx.shared().type_is_sized(t) => {\n             in_memory_type_of(ccx, t).ptr_to()\n         }\n         _ => bug!(\"expected fat ptr ty but got {:?}\", ty)\n@@ -172,7 +170,7 @@ pub fn immediate_type_of<'a, 'tcx>(cx: &CrateContext<'a, 'tcx>, t: Ty<'tcx>) ->\n /// is too large for it to be placed in SSA value (by our rules).\n /// For the raw type without far pointer indirection, see `in_memory_type_of`.\n pub fn type_of<'a, 'tcx>(cx: &CrateContext<'a, 'tcx>, ty: Ty<'tcx>) -> Type {\n-    let ty = if !type_is_sized(cx.tcx(), ty) {\n+    let ty = if !cx.shared().type_is_sized(ty) {\n         cx.tcx().mk_imm_ptr(ty)\n     } else {\n         ty\n@@ -232,7 +230,7 @@ pub fn in_memory_type_of<'a, 'tcx>(cx: &CrateContext<'a, 'tcx>, t: Ty<'tcx>) ->\n       ty::TyBox(ty) |\n       ty::TyRef(_, ty::TypeAndMut{ty, ..}) |\n       ty::TyRawPtr(ty::TypeAndMut{ty, ..}) => {\n-          if !type_is_sized(cx.tcx(), ty) {\n+          if !cx.shared().type_is_sized(ty) {\n               if let ty::TyStr = ty.sty {\n                   // This means we get a nicer name in the output (str is always\n                   // unsized)."}, {"sha": "287ad87caacf94e6398c9e6bc8e1ebd9abb063e7", "filename": "src/librustc_trans/value.rs", "status": "modified", "additions": 1, "deletions": 155, "changes": 156, "blob_url": "https://github.com/rust-lang/rust/blob/1b38776c1f68c6fd47c1b2f7b7974efc7dd64901/src%2Flibrustc_trans%2Fvalue.rs", "raw_url": "https://github.com/rust-lang/rust/raw/1b38776c1f68c6fd47c1b2f7b7974efc7dd64901/src%2Flibrustc_trans%2Fvalue.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_trans%2Fvalue.rs?ref=1b38776c1f68c6fd47c1b2f7b7974efc7dd64901", "patch": "@@ -9,16 +9,11 @@\n // except according to those terms.\n \n use llvm;\n-use llvm::{UseRef, ValueRef};\n-use basic_block::BasicBlock;\n-use common::Block;\n \n use std::fmt;\n \n-use libc::c_uint;\n-\n #[derive(Copy, Clone, PartialEq)]\n-pub struct Value(pub ValueRef);\n+pub struct Value(pub llvm::ValueRef);\n \n impl fmt::Debug for Value {\n     fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {\n@@ -27,152 +22,3 @@ impl fmt::Debug for Value {\n         }).expect(\"nun-UTF8 value description from LLVM\"))\n     }\n }\n-\n-macro_rules! opt_val { ($e:expr) => (\n-    unsafe {\n-        match $e {\n-            p if !p.is_null() => Some(Value(p)),\n-            _ => None\n-        }\n-    }\n-) }\n-\n-/// Wrapper for LLVM ValueRef\n-impl Value {\n-    /// Returns the native ValueRef\n-    pub fn get(&self) -> ValueRef {\n-        let Value(v) = *self; v\n-    }\n-\n-    /// Returns the BasicBlock that contains this value\n-    pub fn get_parent(self) -> Option<BasicBlock> {\n-        unsafe {\n-            match llvm::LLVMGetInstructionParent(self.get()) {\n-                p if !p.is_null() => Some(BasicBlock(p)),\n-                _ => None\n-            }\n-        }\n-    }\n-\n-    /// Removes this value from its containing BasicBlock\n-    pub fn erase_from_parent(self) {\n-        unsafe {\n-            llvm::LLVMInstructionEraseFromParent(self.get());\n-        }\n-    }\n-\n-    /// Returns the single dominating store to this value, if any\n-    /// This only performs a search for a trivially dominating store. The store\n-    /// must be the only user of this value, and there must not be any conditional\n-    /// branches between the store and the given block.\n-    pub fn get_dominating_store(self, bcx: Block) -> Option<Value> {\n-        match self.get_single_user().and_then(|user| user.as_store_inst()) {\n-            Some(store) => {\n-                store.get_parent().and_then(|store_bb| {\n-                    let mut bb = BasicBlock(bcx.llbb);\n-                    let mut ret = Some(store);\n-                    while bb.get() != store_bb.get() {\n-                        match bb.get_single_predecessor() {\n-                            Some(pred) => bb = pred,\n-                            None => { ret = None; break }\n-                        }\n-                    }\n-                    ret\n-                })\n-            }\n-            _ => None\n-        }\n-    }\n-\n-    /// Returns the first use of this value, if any\n-    pub fn get_first_use(self) -> Option<Use> {\n-        unsafe {\n-            match llvm::LLVMGetFirstUse(self.get()) {\n-                u if !u.is_null() => Some(Use(u)),\n-                _ => None\n-            }\n-        }\n-    }\n-\n-    /// Tests if there are no uses of this value\n-    pub fn has_no_uses(self) -> bool {\n-        self.get_first_use().is_none()\n-    }\n-\n-    /// Returns the single user of this value\n-    /// If there are no users or multiple users, this returns None\n-    pub fn get_single_user(self) -> Option<Value> {\n-        let mut iter = self.user_iter();\n-        match (iter.next(), iter.next()) {\n-            (Some(first), None) => Some(first),\n-            _ => None\n-        }\n-    }\n-\n-    /// Returns an iterator for the users of this value\n-    pub fn user_iter(self) -> Users {\n-        Users {\n-            next: self.get_first_use()\n-        }\n-    }\n-\n-    /// Returns the requested operand of this instruction\n-    /// Returns None, if there's no operand at the given index\n-    pub fn get_operand(self, i: usize) -> Option<Value> {\n-        opt_val!(llvm::LLVMGetOperand(self.get(), i as c_uint))\n-    }\n-\n-    /// Returns the Store represent by this value, if any\n-    pub fn as_store_inst(self) -> Option<Value> {\n-        opt_val!(llvm::LLVMIsAStoreInst(self.get()))\n-    }\n-\n-    /// Tests if this value is a terminator instruction\n-    pub fn is_a_terminator_inst(self) -> bool {\n-        unsafe {\n-            !llvm::LLVMIsATerminatorInst(self.get()).is_null()\n-        }\n-    }\n-}\n-\n-/// Wrapper for LLVM UseRef\n-#[derive(Copy, Clone)]\n-pub struct Use(UseRef);\n-\n-impl Use {\n-    pub fn get(&self) -> UseRef {\n-        let Use(v) = *self; v\n-    }\n-\n-    pub fn get_user(self) -> Value {\n-        unsafe {\n-            Value(llvm::LLVMGetUser(self.get()))\n-        }\n-    }\n-\n-    pub fn get_next_use(self) -> Option<Use> {\n-        unsafe {\n-            match llvm::LLVMGetNextUse(self.get()) {\n-                u if !u.is_null() => Some(Use(u)),\n-                _ => None\n-            }\n-        }\n-    }\n-}\n-\n-/// Iterator for the users of a value\n-pub struct Users {\n-    next: Option<Use>\n-}\n-\n-impl Iterator for Users {\n-    type Item = Value;\n-\n-    fn next(&mut self) -> Option<Value> {\n-        let current = self.next;\n-\n-        self.next = current.and_then(|u| u.get_next_use());\n-\n-        current.map(|u| u.get_user())\n-    }\n-}"}, {"sha": "5fbfef05e10d4f321b14f8f891c31d58e15faa24", "filename": "src/test/run-pass/trans-object-shim.rs", "status": "added", "additions": 14, "deletions": 0, "changes": 14, "blob_url": "https://github.com/rust-lang/rust/blob/1b38776c1f68c6fd47c1b2f7b7974efc7dd64901/src%2Ftest%2Frun-pass%2Ftrans-object-shim.rs", "raw_url": "https://github.com/rust-lang/rust/raw/1b38776c1f68c6fd47c1b2f7b7974efc7dd64901/src%2Ftest%2Frun-pass%2Ftrans-object-shim.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ftest%2Frun-pass%2Ftrans-object-shim.rs?ref=1b38776c1f68c6fd47c1b2f7b7974efc7dd64901", "patch": "@@ -0,0 +1,14 @@\n+// Copyright 2016 The Rust Project Developers. See the COPYRIGHT\n+// file at the top-level directory of this distribution and at\n+// http://rust-lang.org/COPYRIGHT.\n+//\n+// Licensed under the Apache License, Version 2.0 <LICENSE-APACHE or\n+// http://www.apache.org/licenses/LICENSE-2.0> or the MIT license\n+// <LICENSE-MIT or http://opensource.org/licenses/MIT>, at your\n+// option. This file may not be copied, modified, or distributed\n+// except according to those terms.\n+\n+fn main() {\n+    assert_eq!((ToString::to_string as fn(&(ToString+'static)) -> String)(&\"foo\"),\n+        String::from(\"foo\"));\n+}"}]}