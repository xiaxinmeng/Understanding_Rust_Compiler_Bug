{"sha": "47fa016193a729091aef9c531df9385548ad46ab", "node_id": "MDY6Q29tbWl0NzI0NzEyOjQ3ZmEwMTYxOTNhNzI5MDkxYWVmOWM1MzFkZjkzODU1NDhhZDQ2YWI=", "commit": {"author": {"name": "Scott McMurray", "email": "scottmcm@users.noreply.github.com", "date": "2017-06-21T06:48:15Z"}, "committer": {"name": "Scott McMurray", "email": "scottmcm@users.noreply.github.com", "date": "2017-06-22T05:58:45Z"}, "message": "Reuse the mem::swap optimizations to speed up slice::rotate\n\nExposes the swapping logic from PR 40454 as `pub unsafe fn ptr::swap_nonoverlapping` under feature swap_nonoverlapping\n\nThis is most helpful for compound types where LLVM didn't vectorize the loop.  Highlight: bench slice::rotate_medium_by727_strings gets 38% faster.", "tree": {"sha": "9c2fdcbe55503be8d89c553a807ecc215cf2e567", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/9c2fdcbe55503be8d89c553a807ecc215cf2e567"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/47fa016193a729091aef9c531df9385548ad46ab", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/47fa016193a729091aef9c531df9385548ad46ab", "html_url": "https://github.com/rust-lang/rust/commit/47fa016193a729091aef9c531df9385548ad46ab", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/47fa016193a729091aef9c531df9385548ad46ab/comments", "author": {"login": "scottmcm", "id": 18526288, "node_id": "MDQ6VXNlcjE4NTI2Mjg4", "avatar_url": "https://avatars.githubusercontent.com/u/18526288?v=4", "gravatar_id": "", "url": "https://api.github.com/users/scottmcm", "html_url": "https://github.com/scottmcm", "followers_url": "https://api.github.com/users/scottmcm/followers", "following_url": "https://api.github.com/users/scottmcm/following{/other_user}", "gists_url": "https://api.github.com/users/scottmcm/gists{/gist_id}", "starred_url": "https://api.github.com/users/scottmcm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/scottmcm/subscriptions", "organizations_url": "https://api.github.com/users/scottmcm/orgs", "repos_url": "https://api.github.com/users/scottmcm/repos", "events_url": "https://api.github.com/users/scottmcm/events{/privacy}", "received_events_url": "https://api.github.com/users/scottmcm/received_events", "type": "User", "site_admin": false}, "committer": {"login": "scottmcm", "id": 18526288, "node_id": "MDQ6VXNlcjE4NTI2Mjg4", "avatar_url": "https://avatars.githubusercontent.com/u/18526288?v=4", "gravatar_id": "", "url": "https://api.github.com/users/scottmcm", "html_url": "https://github.com/scottmcm", "followers_url": "https://api.github.com/users/scottmcm/followers", "following_url": "https://api.github.com/users/scottmcm/following{/other_user}", "gists_url": "https://api.github.com/users/scottmcm/gists{/gist_id}", "starred_url": "https://api.github.com/users/scottmcm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/scottmcm/subscriptions", "organizations_url": "https://api.github.com/users/scottmcm/orgs", "repos_url": "https://api.github.com/users/scottmcm/repos", "events_url": "https://api.github.com/users/scottmcm/events{/privacy}", "received_events_url": "https://api.github.com/users/scottmcm/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "6de26f42deab53b1a8cbd57e4c696626e6650708", "url": "https://api.github.com/repos/rust-lang/rust/commits/6de26f42deab53b1a8cbd57e4c696626e6650708", "html_url": "https://github.com/rust-lang/rust/commit/6de26f42deab53b1a8cbd57e4c696626e6650708"}], "stats": {"total": 147, "additions": 86, "deletions": 61}, "files": [{"sha": "62dff93d85cb1860b03c1a20573a59dd393b25e3", "filename": "src/libcore/mem.rs", "status": "modified", "additions": 1, "deletions": 53, "changes": 54, "blob_url": "https://github.com/rust-lang/rust/blob/47fa016193a729091aef9c531df9385548ad46ab/src%2Flibcore%2Fmem.rs", "raw_url": "https://github.com/rust-lang/rust/raw/47fa016193a729091aef9c531df9385548ad46ab/src%2Flibcore%2Fmem.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibcore%2Fmem.rs?ref=47fa016193a729091aef9c531df9385548ad46ab", "patch": "@@ -499,59 +499,7 @@ pub unsafe fn uninitialized<T>() -> T {\n #[stable(feature = \"rust1\", since = \"1.0.0\")]\n pub fn swap<T>(x: &mut T, y: &mut T) {\n     unsafe {\n-        // The approach here is to utilize simd to swap x & y efficiently. Testing reveals\n-        // that swapping either 32 bytes or 64 bytes at a time is most efficient for intel\n-        // Haswell E processors. LLVM is more able to optimize if we give a struct a\n-        // #[repr(simd)], even if we don't actually use this struct directly.\n-        //\n-        // FIXME repr(simd) broken on emscripten and redox\n-        #[cfg_attr(not(any(target_os = \"emscripten\", target_os = \"redox\")), repr(simd))]\n-        struct Block(u64, u64, u64, u64);\n-        struct UnalignedBlock(u64, u64, u64, u64);\n-\n-        let block_size = size_of::<Block>();\n-\n-        // Get raw pointers to the bytes of x & y for easier manipulation\n-        let x = x as *mut T as *mut u8;\n-        let y = y as *mut T as *mut u8;\n-\n-        // Loop through x & y, copying them `Block` at a time\n-        // The optimizer should unroll the loop fully for most types\n-        // N.B. We can't use a for loop as the `range` impl calls `mem::swap` recursively\n-        let len = size_of::<T>();\n-        let mut i = 0;\n-        while i + block_size <= len {\n-            // Create some uninitialized memory as scratch space\n-            // Declaring `t` here avoids aligning the stack when this loop is unused\n-            let mut t: Block = uninitialized();\n-            let t = &mut t as *mut _ as *mut u8;\n-            let x = x.offset(i as isize);\n-            let y = y.offset(i as isize);\n-\n-            // Swap a block of bytes of x & y, using t as a temporary buffer\n-            // This should be optimized into efficient SIMD operations where available\n-            ptr::copy_nonoverlapping(x, t, block_size);\n-            ptr::copy_nonoverlapping(y, x, block_size);\n-            ptr::copy_nonoverlapping(t, y, block_size);\n-            i += block_size;\n-        }\n-\n-\n-        if i < len {\n-            // Swap any remaining bytes, using aligned types to copy\n-            // where appropriate (this information is lost by conversion\n-            // to *mut u8, so restore it manually here)\n-            let mut t: UnalignedBlock = uninitialized();\n-            let rem = len - i;\n-\n-            let t = &mut t as *mut _ as *mut u8;\n-            let x = x.offset(i as isize);\n-            let y = y.offset(i as isize);\n-\n-            ptr::copy_nonoverlapping(x, t, rem);\n-            ptr::copy_nonoverlapping(y, x, rem);\n-            ptr::copy_nonoverlapping(t, y, rem);\n-        }\n+        ptr::swap_nonoverlapping(x, y, 1);\n     }\n }\n "}, {"sha": "bd311743bebfdf1742bf4ea8c5985824f7b70668", "filename": "src/libcore/ptr.rs", "status": "modified", "additions": 84, "deletions": 0, "changes": 84, "blob_url": "https://github.com/rust-lang/rust/blob/47fa016193a729091aef9c531df9385548ad46ab/src%2Flibcore%2Fptr.rs", "raw_url": "https://github.com/rust-lang/rust/raw/47fa016193a729091aef9c531df9385548ad46ab/src%2Flibcore%2Fptr.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibcore%2Fptr.rs?ref=47fa016193a729091aef9c531df9385548ad46ab", "patch": "@@ -117,6 +117,90 @@ pub unsafe fn swap<T>(x: *mut T, y: *mut T) {\n     mem::forget(tmp);\n }\n \n+/// Swaps a sequence of values at two mutable locations of the same type.\n+///\n+/// # Safety\n+///\n+/// The two arguments must each point to the beginning of `count` locations\n+/// of valid memory, and the two memory ranges must not overlap.\n+///\n+/// # Examples\n+///\n+/// Basic usage:\n+///\n+/// ```\n+/// #![feature(swap_nonoverlapping)]\n+///\n+/// use std::ptr;\n+///\n+/// let mut x = [1, 2, 3, 4];\n+/// let mut y = [7, 8, 9];\n+///\n+/// unsafe {\n+///     ptr::swap_nonoverlapping(x.as_mut_ptr(), y.as_mut_ptr(), 2);\n+/// }\n+///\n+/// assert_eq!(x, [7, 8, 3, 4]);\n+/// assert_eq!(y, [1, 2, 9]);\n+/// ```\n+#[inline]\n+#[unstable(feature = \"swap_nonoverlapping\", issue = \"42818\")]\n+pub unsafe fn swap_nonoverlapping<T>(x: *mut T, y: *mut T, count: usize) {\n+    let x = x as *mut u8;\n+    let y = y as *mut u8;\n+    let len = mem::size_of::<T>() * count;\n+    swap_nonoverlapping_bytes(x, y, len)\n+}\n+\n+#[inline]\n+unsafe fn swap_nonoverlapping_bytes(x: *mut u8, y: *mut u8, len: usize) {\n+    // The approach here is to utilize simd to swap x & y efficiently. Testing reveals\n+    // that swapping either 32 bytes or 64 bytes at a time is most efficient for intel\n+    // Haswell E processors. LLVM is more able to optimize if we give a struct a\n+    // #[repr(simd)], even if we don't actually use this struct directly.\n+    //\n+    // FIXME repr(simd) broken on emscripten and redox\n+    #[cfg_attr(not(any(target_os = \"emscripten\", target_os = \"redox\")), repr(simd))]\n+    struct Block(u64, u64, u64, u64);\n+    struct UnalignedBlock(u64, u64, u64, u64);\n+\n+    let block_size = mem::size_of::<Block>();\n+\n+    // Loop through x & y, copying them `Block` at a time\n+    // The optimizer should unroll the loop fully for most types\n+    // N.B. We can't use a for loop as the `range` impl calls `mem::swap` recursively\n+    let mut i = 0;\n+    while i + block_size <= len {\n+        // Create some uninitialized memory as scratch space\n+        // Declaring `t` here avoids aligning the stack when this loop is unused\n+        let mut t: Block = mem::uninitialized();\n+        let t = &mut t as *mut _ as *mut u8;\n+        let x = x.offset(i as isize);\n+        let y = y.offset(i as isize);\n+\n+        // Swap a block of bytes of x & y, using t as a temporary buffer\n+        // This should be optimized into efficient SIMD operations where available\n+        copy_nonoverlapping(x, t, block_size);\n+        copy_nonoverlapping(y, x, block_size);\n+        copy_nonoverlapping(t, y, block_size);\n+        i += block_size;\n+    }\n+\n+    if i < len {\n+        // Swap any remaining bytes\n+        let mut t: UnalignedBlock = mem::uninitialized();\n+        let rem = len - i;\n+\n+        let t = &mut t as *mut _ as *mut u8;\n+        let x = x.offset(i as isize);\n+        let y = y.offset(i as isize);\n+\n+        copy_nonoverlapping(x, t, rem);\n+        copy_nonoverlapping(y, x, rem);\n+        copy_nonoverlapping(t, y, rem);\n+    }\n+}\n+\n /// Replaces the value at `dest` with `src`, returning the old\n /// value, without dropping either.\n ///"}, {"sha": "e4a4e33c1729efb94bd924b552915d048480d6c5", "filename": "src/libcore/slice/rotate.rs", "status": "modified", "additions": 1, "deletions": 8, "changes": 9, "blob_url": "https://github.com/rust-lang/rust/blob/47fa016193a729091aef9c531df9385548ad46ab/src%2Flibcore%2Fslice%2Frotate.rs", "raw_url": "https://github.com/rust-lang/rust/raw/47fa016193a729091aef9c531df9385548ad46ab/src%2Flibcore%2Fslice%2Frotate.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibcore%2Fslice%2Frotate.rs?ref=47fa016193a729091aef9c531df9385548ad46ab", "patch": "@@ -76,7 +76,7 @@ pub unsafe fn ptr_rotate<T>(mut left: usize, mid: *mut T, mut right: usize) {\n             break;\n         }\n \n-        ptr_swap_n(\n+        ptr::swap_nonoverlapping(\n             mid.offset(-(left as isize)),\n             mid.offset((right-delta) as isize),\n             delta);\n@@ -103,10 +103,3 @@ pub unsafe fn ptr_rotate<T>(mut left: usize, mid: *mut T, mut right: usize) {\n         ptr::copy_nonoverlapping(buf, mid.offset(-(left as isize)), right);\n     }\n }\n-\n-unsafe fn ptr_swap_n<T>(a: *mut T, b: *mut T, n: usize) {\n-    for i in 0..n {\n-        // These are nonoverlapping, so use mem::swap instead of ptr::swap\n-        mem::swap(&mut *a.offset(i as isize), &mut *b.offset(i as isize));\n-    }\n-}"}]}