{"sha": "95c99b2044a75f27e691308ebbb7ed0d4e2cbf3b", "node_id": "MDY6Q29tbWl0NzI0NzEyOjk1Yzk5YjIwNDRhNzVmMjdlNjkxMzA4ZWJiYjdlZDBkNGUyY2JmM2I=", "commit": {"author": {"name": "JCTyBlaidd", "email": "JCTyblaidd@users.noreply.github.com", "date": "2020-11-04T21:35:48Z"}, "committer": {"name": "JCTyBlaidd", "email": "JCTyblaidd@users.noreply.github.com", "date": "2020-11-04T21:35:48Z"}, "message": "Detect races between atomic and non-atomic accesses of a variable,\n previously only data races between two non-atomic accesses were\n detected.", "tree": {"sha": "6ca54a3359b400c9a7fc5ed65bf331059871d3e3", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/6ca54a3359b400c9a7fc5ed65bf331059871d3e3"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/95c99b2044a75f27e691308ebbb7ed0d4e2cbf3b", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/95c99b2044a75f27e691308ebbb7ed0d4e2cbf3b", "html_url": "https://github.com/rust-lang/rust/commit/95c99b2044a75f27e691308ebbb7ed0d4e2cbf3b", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/95c99b2044a75f27e691308ebbb7ed0d4e2cbf3b/comments", "author": {"login": "JCTyblaidd", "id": 8288600, "node_id": "MDQ6VXNlcjgyODg2MDA=", "avatar_url": "https://avatars.githubusercontent.com/u/8288600?v=4", "gravatar_id": "", "url": "https://api.github.com/users/JCTyblaidd", "html_url": "https://github.com/JCTyblaidd", "followers_url": "https://api.github.com/users/JCTyblaidd/followers", "following_url": "https://api.github.com/users/JCTyblaidd/following{/other_user}", "gists_url": "https://api.github.com/users/JCTyblaidd/gists{/gist_id}", "starred_url": "https://api.github.com/users/JCTyblaidd/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/JCTyblaidd/subscriptions", "organizations_url": "https://api.github.com/users/JCTyblaidd/orgs", "repos_url": "https://api.github.com/users/JCTyblaidd/repos", "events_url": "https://api.github.com/users/JCTyblaidd/events{/privacy}", "received_events_url": "https://api.github.com/users/JCTyblaidd/received_events", "type": "User", "site_admin": false}, "committer": {"login": "JCTyblaidd", "id": 8288600, "node_id": "MDQ6VXNlcjgyODg2MDA=", "avatar_url": "https://avatars.githubusercontent.com/u/8288600?v=4", "gravatar_id": "", "url": "https://api.github.com/users/JCTyblaidd", "html_url": "https://github.com/JCTyblaidd", "followers_url": "https://api.github.com/users/JCTyblaidd/followers", "following_url": "https://api.github.com/users/JCTyblaidd/following{/other_user}", "gists_url": "https://api.github.com/users/JCTyblaidd/gists{/gist_id}", "starred_url": "https://api.github.com/users/JCTyblaidd/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/JCTyblaidd/subscriptions", "organizations_url": "https://api.github.com/users/JCTyblaidd/orgs", "repos_url": "https://api.github.com/users/JCTyblaidd/repos", "events_url": "https://api.github.com/users/JCTyblaidd/events{/privacy}", "received_events_url": "https://api.github.com/users/JCTyblaidd/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "fe2e857cc3744a69b1d1dc2fe77f94da10978091", "url": "https://api.github.com/repos/rust-lang/rust/commits/fe2e857cc3744a69b1d1dc2fe77f94da10978091", "html_url": "https://github.com/rust-lang/rust/commit/fe2e857cc3744a69b1d1dc2fe77f94da10978091"}], "stats": {"total": 541, "additions": 463, "deletions": 78}, "files": [{"sha": "8e7a3548f5cf007ea6a0e6e043113a7bf87af243", "filename": "src/data_race.rs", "status": "modified", "additions": 273, "deletions": 77, "changes": 350, "blob_url": "https://github.com/rust-lang/rust/blob/95c99b2044a75f27e691308ebbb7ed0d4e2cbf3b/src%2Fdata_race.rs", "raw_url": "https://github.com/rust-lang/rust/raw/95c99b2044a75f27e691308ebbb7ed0d4e2cbf3b/src%2Fdata_race.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Fdata_race.rs?ref=95c99b2044a75f27e691308ebbb7ed0d4e2cbf3b", "patch": "@@ -6,8 +6,16 @@\n //!  and RMW operations\n //! This does not explore weak memory orders and so can still miss data-races\n //!  but should not report false-positives\n+//! Data-race definiton from(https://en.cppreference.com/w/cpp/language/memory_model#Threads_and_data_races):\n+//!  - if a memory location is accessed by twice is a data-race unless:\n+//!    - both operations execute on the same thread/signal-handler\n+//!    - both conflicting operations are atomic operations (1 atomic and 1 non-atomic race)\n+//!    - 1 of the operations happens-before the other operation (see link for definition)\n \n-use std::{fmt::{self, Debug}, cmp::Ordering, rc::Rc, cell::{Cell, RefCell, Ref, RefMut}, ops::Index};\n+use std::{\n+    fmt::{self, Debug}, cmp::Ordering, rc::Rc,\n+    cell::{Cell, RefCell, Ref, RefMut}, ops::Index, mem\n+};\n \n use rustc_index::vec::{Idx, IndexVec};\n use rustc_target::abi::Size;\n@@ -16,7 +24,11 @@ use rustc_data_structures::fx::FxHashMap;\n \n use smallvec::SmallVec;\n \n-use crate::*;\n+use crate::{\n+    MiriEvalContext, ThreadId, Tag, MiriEvalContextExt, RangeMap,\n+    MPlaceTy, ImmTy, InterpResult, Pointer, ScalarMaybeUninit,\n+    OpTy, Immediate, MemPlaceMeta\n+};\n \n pub type AllocExtra = VClockAlloc;\n pub type MemoryExtra = Rc<GlobalState>;\n@@ -58,8 +70,8 @@ pub enum AtomicFenceOp {\n }\n \n /// Evaluation context extensions\n-impl<'mir, 'tcx: 'mir> EvalContextExt<'mir, 'tcx> for crate::MiriEvalContext<'mir, 'tcx> {}\n-pub trait EvalContextExt<'mir, 'tcx: 'mir>: crate::MiriEvalContextExt<'mir, 'tcx> {\n+impl<'mir, 'tcx: 'mir> EvalContextExt<'mir, 'tcx> for MiriEvalContext<'mir, 'tcx> {}\n+pub trait EvalContextExt<'mir, 'tcx: 'mir>: MiriEvalContextExt<'mir, 'tcx> {\n \n     /// Variant of `read_immediate` that does not perform `data-race` checks.\n     fn read_immediate_racy(&self, op: MPlaceTy<'tcx, Tag>) -> InterpResult<'tcx, ImmTy<'tcx, Tag>> {\n@@ -119,6 +131,26 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: crate::MiriEvalContextExt<'mir, 'tcx\n         this.read_scalar_racy(value_place.into())\n     }\n \n+    /// Variant of `write_scalar_at_offfset` helper function that performs\n+    ///  an atomic load operation with verification instead\n+    fn read_scalar_at_offset_atomic(\n+        &mut self,\n+        op: OpTy<'tcx, Tag>,\n+        offset: u64,\n+        layout: TyAndLayout<'tcx>,\n+        atomic: AtomicReadOp\n+    ) -> InterpResult<'tcx, ScalarMaybeUninit<Tag>> {\n+        let this = self.eval_context_mut();\n+        let op_place = this.deref_operand(op)?;\n+        let offset = Size::from_bytes(offset);\n+        // Ensure that the following read at an offset is within bounds\n+        assert!(op_place.layout.size >= offset + layout.size);\n+        let value_place = op_place.offset(offset, MemPlaceMeta::None, layout, this)?;\n+        let res = this.read_scalar_racy(value_place.into())?;\n+        this.validate_atomic_load(value_place, atomic)?;\n+        Ok(res)\n+    }\n+\n     /// Variant of `write_scalar_at_offfset` helper function that does not perform\n     ///  data-race checks.\n     fn write_scalar_at_offset_racy(\n@@ -137,10 +169,28 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: crate::MiriEvalContextExt<'mir, 'tcx\n         this.write_scalar_racy(value.into(), value_place.into())\n     }\n \n+    /// Load the data race allocation state for a given memory place\n+    ///  also returns the size and offset of the result in the allocation\n+    ///  metadata\n+    /// This is used for atomic loads since unconditionally requesteing\n+    ///  mutable access causes issues for read-only memory, which will\n+    ///  fail validation on mutable access\n+    fn load_data_race_state_ref<'a>(\n+        &'a self, place: MPlaceTy<'tcx, Tag>\n+    ) -> InterpResult<'tcx, (&'a VClockAlloc, Size, Size)> where 'mir: 'a {\n+        let this = self.eval_context_ref();\n+\n+        let ptr = place.ptr.assert_ptr();\n+        let size = place.layout.size;\n+        let data_race = &this.memory.get_raw(ptr.alloc_id)?.extra.data_race;\n+\n+        Ok((data_race, size, ptr.offset))\n+    }\n+\n     /// Load the data race allocation state for a given memory place\n     ///  also returns the size and the offset of the result in the allocation\n     ///  metadata\n-    fn load_data_race_state<'a>(\n+    fn load_data_race_state_mut<'a>(\n         &'a mut self, place: MPlaceTy<'tcx, Tag>\n     ) -> InterpResult<'tcx, (&'a mut VClockAlloc, Size, Size)> where 'mir: 'a {\n         let this = self.eval_context_mut();\n@@ -164,29 +214,42 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: crate::MiriEvalContextExt<'mir, 'tcx\n \n             let (\n                 alloc, size, offset\n-            ) = this.load_data_race_state(place)?;\n+            ) = this.load_data_race_state_ref(place)?;\n             log::trace!(\n                 \"Atomic load on {:?} with ordering {:?}, in memory({:?}, offset={}, size={})\",\n                 alloc.global.current_thread(), atomic,\n                 place.ptr.assert_ptr().alloc_id, offset.bytes(), size.bytes()\n             );\n \n+            let current_thread = alloc.global.current_thread();\n             let mut current_state = alloc.global.current_thread_state_mut();\n             if atomic == AtomicReadOp::Relaxed {\n                 // Perform relaxed atomic load\n-                for (_,range) in alloc.alloc_ranges.get_mut().iter_mut(offset, size) {\n-                    range.load_relaxed(&mut *current_state);\n+                for (_,range) in alloc.alloc_ranges.borrow_mut().iter_mut(offset, size) {\n+                    if range.load_relaxed(&mut *current_state, current_thread) == Err(DataRace) {\n+                        mem::drop(current_state);\n+                        return VClockAlloc::report_data_race(\n+                            &alloc.global, range, \"ATOMIC_LOAD\", true,\n+                            place.ptr.assert_ptr(), size\n+                        );\n+                    }\n                 }\n             }else{\n                 // Perform acquire(or seq-cst) atomic load\n-                for (_,range) in alloc.alloc_ranges.get_mut().iter_mut(offset, size) {\n-                    range.acquire(&mut *current_state);\n+                for (_,range) in alloc.alloc_ranges.borrow_mut().iter_mut(offset, size) {\n+                    if range.acquire(&mut *current_state, current_thread) == Err(DataRace) {\n+                        mem::drop(current_state);\n+                        return VClockAlloc::report_data_race(\n+                            &alloc.global, range, \"ATOMIC_LOAD\", true,\n+                            place.ptr.assert_ptr(), size\n+                        );\n+                    }\n                 }\n             }\n \n             // Log changes to atomic memory\n             if log::log_enabled!(log::Level::Trace) {\n-                for (_,range) in alloc.alloc_ranges.get_mut().iter(offset, size) {\n+                for (_,range) in alloc.alloc_ranges.borrow_mut().iter(offset, size) {\n                     log::trace!(\n                         \"  updated atomic memory({:?}, offset={}, size={}) to {:#?}\",\n                         place.ptr.assert_ptr().alloc_id, offset.bytes(), size.bytes(),\n@@ -195,7 +258,7 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: crate::MiriEvalContextExt<'mir, 'tcx\n                 }\n             }\n \n-            std::mem::drop(current_state);\n+            mem::drop(current_state);\n             let data_race = &*this.memory.extra.data_race;\n             data_race.advance_vector_clock();\n         }\n@@ -214,7 +277,7 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: crate::MiriEvalContextExt<'mir, 'tcx\n \n             let (\n                 alloc, size, offset\n-            ) = this.load_data_race_state(place)?;\n+            ) = this.load_data_race_state_mut(place)?;\n             let current_thread = alloc.global.current_thread();\n             let mut current_state = alloc.global.current_thread_state_mut();\n             log::trace!(\n@@ -226,12 +289,24 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: crate::MiriEvalContextExt<'mir, 'tcx\n             if atomic == AtomicWriteOp::Relaxed {\n                 // Perform relaxed atomic store\n                 for (_,range) in alloc.alloc_ranges.get_mut().iter_mut(offset, size) {\n-                    range.store_relaxed(&mut *current_state, current_thread);\n+                    if range.store_relaxed(&mut *current_state, current_thread) == Err(DataRace) {\n+                        mem::drop(current_state);\n+                        return VClockAlloc::report_data_race(\n+                            &alloc.global, range, \"ATOMIC_STORE\", true,\n+                            place.ptr.assert_ptr(), size\n+                        );\n+                    }\n                 }\n             }else{\n                 // Perform release(or seq-cst) atomic store\n                 for (_,range) in alloc.alloc_ranges.get_mut().iter_mut(offset, size) {\n-                    range.release(&mut *current_state, current_thread);\n+                    if range.release(&mut *current_state, current_thread) == Err(DataRace) {\n+                        mem::drop(current_state);\n+                        return VClockAlloc::report_data_race(\n+                            &alloc.global, range, \"ATOMIC_STORE\", true,\n+                            place.ptr.assert_ptr(), size\n+                        );\n+                    }\n                 }\n             }\n \n@@ -246,7 +321,7 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: crate::MiriEvalContextExt<'mir, 'tcx\n                 }\n             }\n \n-            std::mem::drop(current_state);\n+            mem::drop(current_state);\n             let data_race = &*this.memory.extra.data_race;\n             data_race.advance_vector_clock();\n         }\n@@ -266,7 +341,7 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: crate::MiriEvalContextExt<'mir, 'tcx\n \n             let (\n                 alloc, size, offset\n-            ) = this.load_data_race_state(place)?;\n+            ) = this.load_data_race_state_mut(place)?;\n             let current_thread = alloc.global.current_thread();\n             let mut current_state = alloc.global.current_thread_state_mut();\n             log::trace!(\n@@ -280,17 +355,31 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: crate::MiriEvalContextExt<'mir, 'tcx\n             for (_,range) in alloc.alloc_ranges.get_mut().iter_mut(offset, size) {\n                 //FIXME: this is probably still slightly wrong due to the quirks\n                 // in the c++11 memory model\n-                if acquire {\n+                let maybe_race = if acquire {\n                     // Atomic RW-Op acquire\n-                    range.acquire(&mut *current_state);\n+                    range.acquire(&mut *current_state, current_thread)\n                 }else{\n-                    range.load_relaxed(&mut *current_state);\n+                    range.load_relaxed(&mut *current_state, current_thread) \n+                };\n+                if maybe_race == Err(DataRace) {\n+                    mem::drop(current_state);\n+                    return VClockAlloc::report_data_race(\n+                        &alloc.global, range, \"ATOMIC_RMW(LOAD)\", true,\n+                        place.ptr.assert_ptr(), size\n+                    );\n                 }\n-                if release {\n+                let maybe_race = if release {\n                     // Atomic RW-Op release\n-                    range.rmw_release(&mut *current_state, current_thread);\n+                    range.rmw_release(&mut *current_state, current_thread)\n                 }else{\n-                    range.rmw_relaxed(&mut *current_state);\n+                    range.rmw_relaxed(&mut *current_state, current_thread)\n+                };\n+                if maybe_race == Err(DataRace) {\n+                    mem::drop(current_state);\n+                    return VClockAlloc::report_data_race(\n+                        &alloc.global, range, \"ATOMIC_RMW(STORE)\", true,\n+                        place.ptr.assert_ptr(), size\n+                    );\n                 }\n             }\n \n@@ -305,7 +394,7 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: crate::MiriEvalContextExt<'mir, 'tcx\n                 }\n             }\n \n-            std::mem::drop(current_state);\n+            mem::drop(current_state);\n             let data_race = &*this.memory.extra.data_race;\n             data_race.advance_vector_clock();\n         }\n@@ -478,18 +567,38 @@ impl Debug for AtomicReleaseSequences {\n     }\n }\n \n+/// Error returned by finding a data race\n+///  should be elaborated upon\n+#[derive(Copy, Clone, PartialEq, Eq, PartialOrd, Ord, Hash, Debug)]\n+pub struct DataRace;\n+\n /// Externally stored memory cell clocks\n ///  explicitly to reduce memory usage for the\n ///  common case where no atomic operations\n ///  exists on the memory cell\n #[derive(Clone, PartialEq, Eq, Debug)]\n struct AtomicMemoryCellClocks {\n \n+    /// The clock-vector for the set of atomic read operations\n+    ///  used for detecting data-races with non-atomic write\n+    ///  operations\n+    read_vector: VClock,\n+\n+    /// The clock-vector for the set of atomic write operations\n+    ///  used for detecting data-races with non-atomic read or\n+    ///  write operations\n+    write_vector: VClock,\n+\n     /// Synchronization vector for acquire-release semantics\n+    ///   contains the vector of timestamps that will\n+    ///   happen-before a thread if an acquire-load is \n+    ///   performed on the data\n     sync_vector: VClock,\n \n     /// The Hash-Map of all threads for which a release\n-    ///  sequence exists in the memory cell \n+    ///  sequence exists in the memory cell, required\n+    ///  since read-modify-write operations do not\n+    ///  invalidate existing release sequences \n     release_sequences: AtomicReleaseSequences,\n }\n \n@@ -498,10 +607,12 @@ struct AtomicMemoryCellClocks {\n #[derive(Clone, PartialEq, Eq, Debug)]\n struct MemoryCellClocks {\n \n-    /// The vector-clock of the last write\n+    /// The vector-clock of the last write, only one value is stored\n+    ///  since all previous writes happened-before the current write\n     write: Timestamp,\n \n-    /// The id of the thread that performed the last write to this memory location\n+    /// The identifier of the thread that performed the last write\n+    ///  operation\n     write_thread: ThreadId,\n \n     /// The vector-clock of the set of previous reads\n@@ -532,7 +643,7 @@ impl MemoryCellClocks {\n \n     /// Load the internal atomic memory cells if they exist\n     #[inline]\n-    fn atomic(&mut self) -> Option<&AtomicMemoryCellClocks> {\n+    fn atomic(&self) -> Option<&AtomicMemoryCellClocks> {\n         match &self.atomic_ops {\n             Some(op) => Some(&*op),\n             None => None\n@@ -545,6 +656,8 @@ impl MemoryCellClocks {\n     fn atomic_mut(&mut self) -> &mut AtomicMemoryCellClocks {\n         self.atomic_ops.get_or_insert_with(|| {\n             Box::new(AtomicMemoryCellClocks {\n+                read_vector: VClock::default(),\n+                write_vector: VClock::default(),\n                 sync_vector: VClock::default(),\n                 release_sequences: AtomicReleaseSequences::new()\n             })\n@@ -554,75 +667,131 @@ impl MemoryCellClocks {\n     /// Update memory cell data-race tracking for atomic\n     ///  load acquire semantics, is a no-op if this memory was\n     ///  not used previously as atomic memory\n-    fn acquire(&mut self, clocks: &mut ThreadClockSet) {\n+    fn acquire(&mut self, clocks: &mut ThreadClockSet, thread: ThreadId) -> Result<(), DataRace> {\n+        self.atomic_read_detect(clocks, thread)?;\n         if let Some(atomic) = self.atomic() {\n             clocks.clock.join(&atomic.sync_vector);\n         }\n+        Ok(())\n     }\n     /// Update memory cell data-race tracking for atomic\n     ///  load relaxed semantics, is a no-op if this memory was\n     ///  not used previously as atomic memory\n-    fn load_relaxed(&mut self, clocks: &mut ThreadClockSet) {\n+    fn load_relaxed(&mut self, clocks: &mut ThreadClockSet, thread: ThreadId) -> Result<(), DataRace> {\n+        self.atomic_read_detect(clocks, thread)?;\n         if let Some(atomic) = self.atomic() {\n             clocks.fence_acquire.join(&atomic.sync_vector);\n         }\n+        Ok(())\n     }\n \n \n     /// Update the memory cell data-race tracking for atomic\n     ///  store release semantics\n-    fn release(&mut self, clocks: &ThreadClockSet, thread: ThreadId) {\n+    fn release(&mut self, clocks: &ThreadClockSet, thread: ThreadId) -> Result<(), DataRace> {\n+        self.atomic_write_detect(clocks, thread)?;\n         let atomic = self.atomic_mut();\n         atomic.sync_vector.set_values(&clocks.clock);\n         atomic.release_sequences.clear_and_set(thread, &clocks.clock);\n+        Ok(())\n     }\n     /// Update the memory cell data-race tracking for atomic\n     ///  store relaxed semantics\n-    fn store_relaxed(&mut self, clocks: &ThreadClockSet, thread: ThreadId) {\n+    fn store_relaxed(&mut self, clocks: &ThreadClockSet, thread: ThreadId) -> Result<(), DataRace> {\n+        self.atomic_write_detect(clocks, thread)?;\n         let atomic = self.atomic_mut();\n         atomic.sync_vector.set_values(&clocks.fence_release);\n         if let Some(release) = atomic.release_sequences.load(thread) {\n             atomic.sync_vector.join(release);\n         }\n         atomic.release_sequences.clear_and_retain(thread);\n+        Ok(())\n     }\n     /// Update the memory cell data-race tracking for atomic\n     ///  store release semantics for RMW operations\n-    fn rmw_release(&mut self, clocks: &ThreadClockSet, thread: ThreadId) {\n+    fn rmw_release(&mut self, clocks: &ThreadClockSet, thread: ThreadId) -> Result<(), DataRace> {\n+        self.atomic_write_detect(clocks, thread)?;\n         let atomic = self.atomic_mut();\n         atomic.sync_vector.join(&clocks.clock);\n         atomic.release_sequences.insert(thread, &clocks.clock);\n+        Ok(())\n     }\n     /// Update the memory cell data-race tracking for atomic\n     ///  store relaxed semantics for RMW operations\n-    fn rmw_relaxed(&mut self, clocks: &ThreadClockSet) {\n+    fn rmw_relaxed(&mut self, clocks: &ThreadClockSet, thread: ThreadId) -> Result<(), DataRace> {\n+        self.atomic_write_detect(clocks, thread)?;\n         let atomic = self.atomic_mut();\n         atomic.sync_vector.join(&clocks.fence_release);\n+        Ok(())\n     }\n     \n-    \n+    /// Detect data-races with an atomic read, caused by a non-atomic write that does\n+    ///  not happen-before the atomic-read\n+    fn atomic_read_detect(&mut self, clocks: &ThreadClockSet, thread: ThreadId) -> Result<(), DataRace> {\n+        log::trace!(\"Atomic read with vectors: {:#?} :: {:#?}\", self, clocks);\n+        if self.write <= clocks.clock[self.write_thread] {\n+            let atomic = self.atomic_mut();\n+            atomic.read_vector.set_at_thread(&clocks.clock, thread);\n+            Ok(())\n+        }else{\n+            Err(DataRace)\n+        }\n+    }\n+\n+    /// Detect data-races with an atomic write, either with a non-atomic read or with\n+    ///  a non-atomic write:\n+    fn atomic_write_detect(&mut self, clocks: &ThreadClockSet, thread: ThreadId) -> Result<(), DataRace> {\n+        log::trace!(\"Atomic write with vectors: {:#?} :: {:#?}\", self, clocks);\n+        if self.write <= clocks.clock[self.write_thread] && self.read <= clocks.clock {\n+            let atomic = self.atomic_mut();\n+            atomic.write_vector.set_at_thread(&clocks.clock, thread);\n+            Ok(())\n+        }else{\n+            Err(DataRace)\n+        }\n+    }\n \n     /// Detect races for non-atomic read operations at the current memory cell\n     ///  returns true if a data-race is detected\n-    fn read_race_detect(&mut self, clocks: &ThreadClockSet, thread: ThreadId) -> bool {\n+    fn read_race_detect(&mut self, clocks: &ThreadClockSet, thread: ThreadId) -> Result<(), DataRace> {\n+        log::trace!(\"Unsynchronized read with vectors: {:#?} :: {:#?}\", self, clocks);\n         if self.write <= clocks.clock[self.write_thread] {\n-            self.read.set_at_thread(&clocks.clock, thread);\n-            false\n+            let race_free = if let Some(atomic) = self.atomic() {\n+                atomic.write_vector <= clocks.clock\n+            }else{\n+                true\n+            };\n+            if race_free {\n+                self.read.set_at_thread(&clocks.clock, thread);\n+                Ok(())\n+            }else{\n+                Err(DataRace)\n+            }\n         }else{\n-            true\n+            Err(DataRace)\n         }\n     }\n \n     /// Detect races for non-atomic write operations at the current memory cell\n     ///  returns true if a data-race is detected\n-    fn write_race_detect(&mut self, clocks: &ThreadClockSet, thread: ThreadId) -> bool {\n+    fn write_race_detect(&mut self, clocks: &ThreadClockSet, thread: ThreadId)  -> Result<(), DataRace> {\n+        log::trace!(\"Unsynchronized write with vectors: {:#?} :: {:#?}\", self, clocks);\n         if self.write <= clocks.clock[self.write_thread] && self.read <= clocks.clock {\n-            self.write = clocks.clock[thread];\n-            self.write_thread = thread;\n-            self.read.set_zero_vector();\n-            false\n+            let race_free = if let Some(atomic) = self.atomic() {\n+                atomic.write_vector <= clocks.clock && atomic.read_vector <= clocks.clock\n+            }else{\n+                true\n+            };\n+            if race_free {\n+                self.write = clocks.clock[thread];\n+                self.write_thread = thread;\n+                self.read.set_zero_vector();\n+                Ok(())\n+            }else{\n+                Err(DataRace)\n+            }\n         }else{\n-            true\n+            Err(DataRace)\n         }\n     }\n }\n@@ -651,6 +820,33 @@ impl VClockAlloc {\n         }\n     }\n \n+    // Find an index, if one exists where the value\n+    //  in `l` is greater than the value in `r`\n+    fn find_gt_index(l: &VClock, r: &VClock) -> Option<usize> {\n+        let l_slice = l.as_slice();\n+        let r_slice = r.as_slice();\n+        l_slice.iter().zip(r_slice.iter())\n+            .enumerate()\n+            .find_map(|(idx, (&l, &r))| {\n+                if l > r { Some(idx) } else { None }\n+            }).or_else(|| {\n+                if l_slice.len() > r_slice.len() {\n+                    // By invariant, if l_slice is longer\n+                    //  then one element must be larger\n+                    // This just validates that this is true\n+                    //  and reports earlier elements first\n+                    let l_remainder_slice = &l_slice[r_slice.len()..];\n+                    let idx = l_remainder_slice.iter().enumerate()\n+                        .find_map(|(idx, &r)| {\n+                            if r == 0 { None } else { Some(idx) }\n+                        }).expect(\"Invalid VClock Invariant\");\n+                    Some(idx)\n+                }else{\n+                    None\n+                }\n+            })\n+    }\n+\n     /// Report a data-race found in the program\n     ///  this finds the two racing threads and the type\n     ///  of data-race that occured, this will also\n@@ -659,7 +855,8 @@ impl VClockAlloc {\n     #[cold]\n     #[inline(never)]\n     fn report_data_race<'tcx>(\n-        global: &MemoryExtra, range: &MemoryCellClocks, action: &str,\n+        global: &MemoryExtra, range: &MemoryCellClocks,\n+        action: &str, is_atomic: bool,\n         pointer: Pointer<Tag>, len: Size\n     ) -> InterpResult<'tcx> {\n         let current_thread = global.current_thread();\n@@ -669,40 +866,39 @@ impl VClockAlloc {\n             other_action, other_thread, other_clock\n         ) = if range.write > current_state.clock[range.write_thread] {\n \n-            // Create effective write-clock that the data-race occured with\n+            // Convert the write action into the vector clock it\n+            //  represents for diagnostic purposes\n             let wclock = write_clock.get_mut_with_min_len(\n                 current_state.clock.as_slice().len()\n                 .max(range.write_thread.to_u32() as usize + 1)\n             );\n             wclock[range.write_thread.to_u32() as usize] = range.write;\n             (\"WRITE\", range.write_thread, write_clock.as_slice())\n+        }else if let Some(idx) = Self::find_gt_index(\n+            &range.read, &current_state.clock\n+        ){\n+            (\"READ\", ThreadId::new(idx), range.read.as_slice())\n+        }else if !is_atomic {\n+            if let Some(atomic) = range.atomic() {\n+                if let Some(idx) = Self::find_gt_index(\n+                    &atomic.write_vector, &current_state.clock\n+                ) {\n+                    (\"ATOMIC_STORE\", ThreadId::new(idx), atomic.write_vector.as_slice())\n+                }else if let Some(idx) = Self::find_gt_index(\n+                    &atomic.read_vector, &current_state.clock\n+                ) {\n+                    (\"ATOMIC_LOAD\", ThreadId::new(idx), atomic.read_vector.as_slice())\n+                }else{\n+                    unreachable!(\"Failed to find report data-race for non-atomic operation: no race found\")\n+                }\n+            }else{\n+                unreachable!(\"Failed to report data-race for non-atomic operation: no atomic component\")\n+            }\n         }else{\n-\n-            // Find index in the read-clock that the data-race occured with\n-            let read_slice = range.read.as_slice();\n-            let clock_slice = current_state.clock.as_slice();\n-            let conflicting_index = read_slice.iter()\n-                .zip(clock_slice.iter())\n-                .enumerate().find_map(|(idx,(&read, &clock))| {\n-                    if read > clock {\n-                        Some(idx)\n-                    }else{\n-                        None\n-                    }\n-            }).unwrap_or_else(|| {\n-                assert!(read_slice.len() > clock_slice.len(), \"BUG: cannot find read race yet reported data-race\");\n-                let rest_read = &read_slice[clock_slice.len()..];\n-                rest_read.iter().enumerate().find_map(|(idx, &val)| {\n-                    if val > 0 {\n-                        Some(idx + clock_slice.len())\n-                    }else{\n-                        None\n-                    }\n-                }).expect(\"Invariant broken for read-slice, no 0 element at the tail\")\n-            });\n-            (\"READ\", ThreadId::new(conflicting_index), range.read.as_slice())\n+            unreachable!(\"Failed to report data-race for atomic operation\")\n         };\n \n+        // Load elaborated thread information about the racing thread actions\n         let current_thread_info = global.print_thread_metadata(current_thread);\n         let other_thread_info = global.print_thread_metadata(other_thread);\n         \n@@ -732,10 +928,10 @@ impl VClockAlloc {\n             //  to the ranges being tested, so this is ok\n             let mut alloc_ranges = self.alloc_ranges.borrow_mut();\n             for (_,range) in alloc_ranges.iter_mut(pointer.offset, len) {\n-                if range.read_race_detect(&*current_state, current_thread) {\n+                if range.read_race_detect(&*current_state, current_thread) == Err(DataRace) {\n                     // Report data-race\n                     return Self::report_data_race(\n-                        &self.global,range, \"READ\", pointer, len\n+                        &self.global,range, \"READ\", false, pointer, len\n                     );\n                 }\n             }\n@@ -753,10 +949,10 @@ impl VClockAlloc {\n             let current_thread = self.global.current_thread();\n             let current_state = self.global.current_thread_state();\n             for (_,range) in self.alloc_ranges.get_mut().iter_mut(pointer.offset, len) {\n-                if range.write_race_detect(&*current_state, current_thread) {\n+                if range.write_race_detect(&*current_state, current_thread) == Err(DataRace) {\n                     // Report data-race\n                     return Self::report_data_race(\n-                        &self.global, range, \"WRITE\", pointer, len\n+                        &self.global, range, \"WRITE\", false, pointer, len\n                     );\n                 }\n             }\n@@ -774,10 +970,10 @@ impl VClockAlloc {\n             let current_thread = self.global.current_thread();\n             let current_state = self.global.current_thread_state();\n             for (_,range) in self.alloc_ranges.get_mut().iter_mut(pointer.offset, len) {\n-                if range.write_race_detect(&*current_state, current_thread) {\n+                if range.write_race_detect(&*current_state, current_thread) == Err(DataRace) {\n                     // Report data-race\n                     return Self::report_data_race(\n-                        &self.global, range, \"DEALLOCATE\", pointer, len\n+                        &self.global, range, \"DEALLOCATE\", false, pointer, len\n                     );\n                 }\n             }"}, {"sha": "67cea55077376d79b2362812598f97d687eb5489", "filename": "src/shims/posix/linux/sync.rs", "status": "modified", "additions": 4, "deletions": 1, "changes": 5, "blob_url": "https://github.com/rust-lang/rust/blob/95c99b2044a75f27e691308ebbb7ed0d4e2cbf3b/src%2Fshims%2Fposix%2Flinux%2Fsync.rs", "raw_url": "https://github.com/rust-lang/rust/raw/95c99b2044a75f27e691308ebbb7ed0d4e2cbf3b/src%2Fshims%2Fposix%2Flinux%2Fsync.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Fshims%2Fposix%2Flinux%2Fsync.rs?ref=95c99b2044a75f27e691308ebbb7ed0d4e2cbf3b", "patch": "@@ -78,7 +78,10 @@ pub fn futex<'tcx>(\n             // Read an `i32` through the pointer, regardless of any wrapper types.\n             // It's not uncommon for `addr` to be passed as another type than `*mut i32`, such as `*const AtomicI32`.\n             // FIXME: this fails if `addr` is not a pointer type.\n-            let futex_val = this.read_scalar_at_offset(addr.into(), 0, this.machine.layouts.i32)?.to_i32()?;\n+            // FIXME: what form of atomic operation should the `futex` use to load the value?\n+            let futex_val = this.read_scalar_at_offset_atomic(\n+                addr.into(), 0, this.machine.layouts.i32, AtomicReadOp::Acquire\n+            )?.to_i32()?;\n             if val == futex_val {\n                 // The value still matches, so we block the trait make it wait for FUTEX_WAKE.\n                 this.block_thread(thread);"}, {"sha": "0b9610edc64b3e58d90278e68e27b93b91aca7dc", "filename": "tests/compile-fail/data_race/atomic_read_write_race.rs", "status": "added", "additions": 31, "deletions": 0, "changes": 31, "blob_url": "https://github.com/rust-lang/rust/blob/95c99b2044a75f27e691308ebbb7ed0d4e2cbf3b/tests%2Fcompile-fail%2Fdata_race%2Fatomic_read_write_race.rs", "raw_url": "https://github.com/rust-lang/rust/raw/95c99b2044a75f27e691308ebbb7ed0d4e2cbf3b/tests%2Fcompile-fail%2Fdata_race%2Fatomic_read_write_race.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/tests%2Fcompile-fail%2Fdata_race%2Fatomic_read_write_race.rs?ref=95c99b2044a75f27e691308ebbb7ed0d4e2cbf3b", "patch": "@@ -0,0 +1,31 @@\n+// ignore-windows: Concurrency on Windows is not supported yet.\n+#![feature(core_intrinsics)]\n+\n+use std::thread::spawn;\n+use std::sync::atomic::AtomicUsize;\n+use std::intrinsics::atomic_load;\n+\n+#[derive(Copy, Clone)]\n+struct EvilSend<T>(pub T);\n+\n+unsafe impl<T> Send for EvilSend<T> {}\n+unsafe impl<T> Sync for EvilSend<T> {}\n+\n+pub fn main() {\n+    let mut a = AtomicUsize::new(0);\n+    let b = &mut a as *mut AtomicUsize;\n+    let c = EvilSend(b);\n+    unsafe {\n+        let j1 = spawn(move || {\n+            *(c.0 as *mut usize) = 32;\n+        });\n+\n+        let j2 = spawn(move || {\n+            //Equivalent to: (&*c.0).load(Ordering::SeqCst)\n+            atomic_load(c.0 as *mut usize) //~ ERROR Data race\n+        });\n+\n+        j1.join().unwrap();\n+        j2.join().unwrap();\n+    }\n+}"}, {"sha": "779babefd8e602339dffa3e675179e65cf9da531", "filename": "tests/compile-fail/data_race/atomic_read_write_race_alt.rs", "status": "added", "additions": 31, "deletions": 0, "changes": 31, "blob_url": "https://github.com/rust-lang/rust/blob/95c99b2044a75f27e691308ebbb7ed0d4e2cbf3b/tests%2Fcompile-fail%2Fdata_race%2Fatomic_read_write_race_alt.rs", "raw_url": "https://github.com/rust-lang/rust/raw/95c99b2044a75f27e691308ebbb7ed0d4e2cbf3b/tests%2Fcompile-fail%2Fdata_race%2Fatomic_read_write_race_alt.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/tests%2Fcompile-fail%2Fdata_race%2Fatomic_read_write_race_alt.rs?ref=95c99b2044a75f27e691308ebbb7ed0d4e2cbf3b", "patch": "@@ -0,0 +1,31 @@\n+// ignore-windows: Concurrency on Windows is not supported yet.\n+\n+use std::thread::spawn;\n+use std::sync::atomic::AtomicUsize;\n+use std::sync::atomic::Ordering;\n+\n+#[derive(Copy, Clone)]\n+struct EvilSend<T>(pub T);\n+\n+unsafe impl<T> Send for EvilSend<T> {}\n+unsafe impl<T> Sync for EvilSend<T> {}\n+\n+pub fn main() {\n+    let mut a = AtomicUsize::new(0);\n+    let b = &mut a as *mut AtomicUsize;\n+    let c = EvilSend(b);\n+    unsafe {\n+        let j1 = spawn(move || {\n+            let atomic_ref = &mut *c.0;\n+            atomic_ref.load(Ordering::SeqCst)\n+        });\n+\n+        let j2 = spawn(move || {\n+            let atomic_ref = &mut *c.0;\n+            *atomic_ref.get_mut() = 32; //~ ERROR Data race\n+        });\n+\n+        j1.join().unwrap();\n+        j2.join().unwrap();\n+    }\n+}"}, {"sha": "3211a5ae53442c168b6e2f37db64af930da9885b", "filename": "tests/compile-fail/data_race/atomic_write_read_race.rs", "status": "added", "additions": 31, "deletions": 0, "changes": 31, "blob_url": "https://github.com/rust-lang/rust/blob/95c99b2044a75f27e691308ebbb7ed0d4e2cbf3b/tests%2Fcompile-fail%2Fdata_race%2Fatomic_write_read_race.rs", "raw_url": "https://github.com/rust-lang/rust/raw/95c99b2044a75f27e691308ebbb7ed0d4e2cbf3b/tests%2Fcompile-fail%2Fdata_race%2Fatomic_write_read_race.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/tests%2Fcompile-fail%2Fdata_race%2Fatomic_write_read_race.rs?ref=95c99b2044a75f27e691308ebbb7ed0d4e2cbf3b", "patch": "@@ -0,0 +1,31 @@\n+// ignore-windows: Concurrency on Windows is not supported yet.\n+\n+use std::thread::spawn;\n+use std::sync::atomic::AtomicUsize;\n+use std::sync::atomic::Ordering;\n+\n+#[derive(Copy, Clone)]\n+struct EvilSend<T>(pub T);\n+\n+unsafe impl<T> Send for EvilSend<T> {}\n+unsafe impl<T> Sync for EvilSend<T> {}\n+\n+pub fn main() {\n+    let mut a = AtomicUsize::new(0);\n+    let b = &mut a as *mut AtomicUsize;\n+    let c = EvilSend(b);\n+    unsafe {\n+        let j1 = spawn(move || {\n+            let atomic_ref = &mut *c.0;\n+            atomic_ref.store(32, Ordering::SeqCst)\n+        });\n+\n+        let j2 = spawn(move || {\n+            let atomic_ref = &mut *c.0;\n+            *atomic_ref.get_mut() //~ ERROR Data race\n+        });\n+\n+        j1.join().unwrap();\n+        j2.join().unwrap();\n+    }\n+}"}, {"sha": "131d4e07b829fb1831f0fbb4bb567ca8de7a527d", "filename": "tests/compile-fail/data_race/atomic_write_read_race_alt.rs", "status": "added", "additions": 31, "deletions": 0, "changes": 31, "blob_url": "https://github.com/rust-lang/rust/blob/95c99b2044a75f27e691308ebbb7ed0d4e2cbf3b/tests%2Fcompile-fail%2Fdata_race%2Fatomic_write_read_race_alt.rs", "raw_url": "https://github.com/rust-lang/rust/raw/95c99b2044a75f27e691308ebbb7ed0d4e2cbf3b/tests%2Fcompile-fail%2Fdata_race%2Fatomic_write_read_race_alt.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/tests%2Fcompile-fail%2Fdata_race%2Fatomic_write_read_race_alt.rs?ref=95c99b2044a75f27e691308ebbb7ed0d4e2cbf3b", "patch": "@@ -0,0 +1,31 @@\n+// ignore-windows: Concurrency on Windows is not supported yet.\n+#![feature(core_intrinsics)]\n+\n+use std::thread::spawn;\n+use std::sync::atomic::AtomicUsize;\n+use std::intrinsics::atomic_store;\n+\n+#[derive(Copy, Clone)]\n+struct EvilSend<T>(pub T);\n+\n+unsafe impl<T> Send for EvilSend<T> {}\n+unsafe impl<T> Sync for EvilSend<T> {}\n+\n+pub fn main() {\n+    let mut a = AtomicUsize::new(0);\n+    let b = &mut a as *mut AtomicUsize;\n+    let c = EvilSend(b);\n+    unsafe {\n+        let j1 = spawn(move || {\n+            *(c.0 as *mut usize)\n+        });\n+\n+        let j2 = spawn(move || {\n+            //Equivalent to: (&*c.0).store(32, Ordering::SeqCst)\n+            atomic_store(c.0 as *mut usize, 32); //~ ERROR Data race\n+        });\n+\n+        j1.join().unwrap();\n+        j2.join().unwrap();\n+    }\n+}"}, {"sha": "74adf7ae4b8d28cb461c4122681099541042e07e", "filename": "tests/compile-fail/data_race/atomic_write_write_race.rs", "status": "added", "additions": 31, "deletions": 0, "changes": 31, "blob_url": "https://github.com/rust-lang/rust/blob/95c99b2044a75f27e691308ebbb7ed0d4e2cbf3b/tests%2Fcompile-fail%2Fdata_race%2Fatomic_write_write_race.rs", "raw_url": "https://github.com/rust-lang/rust/raw/95c99b2044a75f27e691308ebbb7ed0d4e2cbf3b/tests%2Fcompile-fail%2Fdata_race%2Fatomic_write_write_race.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/tests%2Fcompile-fail%2Fdata_race%2Fatomic_write_write_race.rs?ref=95c99b2044a75f27e691308ebbb7ed0d4e2cbf3b", "patch": "@@ -0,0 +1,31 @@\n+// ignore-windows: Concurrency on Windows is not supported yet.\n+#![feature(core_intrinsics)]\n+\n+use std::thread::spawn;\n+use std::sync::atomic::AtomicUsize;\n+use std::intrinsics::atomic_store;\n+\n+#[derive(Copy, Clone)]\n+struct EvilSend<T>(pub T);\n+\n+unsafe impl<T> Send for EvilSend<T> {}\n+unsafe impl<T> Sync for EvilSend<T> {}\n+\n+pub fn main() {\n+    let mut a = AtomicUsize::new(0);\n+    let b = &mut a as *mut AtomicUsize;\n+    let c = EvilSend(b);\n+    unsafe {\n+        let j1 = spawn(move || {\n+            *(c.0 as *mut usize) = 32;\n+        });\n+\n+        let j2 = spawn(move || {\n+            //Equivalent to: (&*c.0).store(64, Ordering::SeqCst)\n+            atomic_store(c.0 as *mut usize, 64); //~ ERROR Data race\n+        });\n+\n+        j1.join().unwrap();\n+        j2.join().unwrap();\n+    }\n+}"}, {"sha": "75ad755fbd2c3c728048b3b5e4a0c3c673ef601a", "filename": "tests/compile-fail/data_race/atomic_write_write_race_alt.rs", "status": "added", "additions": 31, "deletions": 0, "changes": 31, "blob_url": "https://github.com/rust-lang/rust/blob/95c99b2044a75f27e691308ebbb7ed0d4e2cbf3b/tests%2Fcompile-fail%2Fdata_race%2Fatomic_write_write_race_alt.rs", "raw_url": "https://github.com/rust-lang/rust/raw/95c99b2044a75f27e691308ebbb7ed0d4e2cbf3b/tests%2Fcompile-fail%2Fdata_race%2Fatomic_write_write_race_alt.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/tests%2Fcompile-fail%2Fdata_race%2Fatomic_write_write_race_alt.rs?ref=95c99b2044a75f27e691308ebbb7ed0d4e2cbf3b", "patch": "@@ -0,0 +1,31 @@\n+// ignore-windows: Concurrency on Windows is not supported yet.\n+\n+use std::thread::spawn;\n+use std::sync::atomic::AtomicUsize;\n+use std::sync::atomic::Ordering;\n+\n+#[derive(Copy, Clone)]\n+struct EvilSend<T>(pub T);\n+\n+unsafe impl<T> Send for EvilSend<T> {}\n+unsafe impl<T> Sync for EvilSend<T> {}\n+\n+pub fn main() {\n+    let mut a = AtomicUsize::new(0);\n+    let b = &mut a as *mut AtomicUsize;\n+    let c = EvilSend(b);\n+    unsafe {\n+        let j1 = spawn(move || {\n+            let atomic_ref = &mut *c.0;\n+            atomic_ref.store(64, Ordering::SeqCst);\n+        });\n+\n+        let j2 = spawn(move || {\n+            let atomic_ref = &mut *c.0;\n+            *atomic_ref.get_mut() = 32; //~ ERROR Data race\n+        });\n+\n+        j1.join().unwrap();\n+        j2.join().unwrap();\n+    }\n+}"}]}