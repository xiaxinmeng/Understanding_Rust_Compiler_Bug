{"sha": "d8b1fa0ae0c27e54d3539190683c01e194d36fbd", "node_id": "MDY6Q29tbWl0NzI0NzEyOmQ4YjFmYTBhZTBjMjdlNTRkMzUzOTE5MDY4M2MwMWUxOTRkMzZmYmQ=", "commit": {"author": {"name": "Brendan Zabarauskas", "email": "bjzaba@yahoo.com.au", "date": "2014-10-27T08:22:52Z"}, "committer": {"name": "Brendan Zabarauskas", "email": "bjzaba@yahoo.com.au", "date": "2014-10-28T04:55:37Z"}, "message": "Use PascalCase for token variants", "tree": {"sha": "0a03a2995fdbc9d616be18904b2311be854617cf", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/0a03a2995fdbc9d616be18904b2311be854617cf"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/d8b1fa0ae0c27e54d3539190683c01e194d36fbd", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/d8b1fa0ae0c27e54d3539190683c01e194d36fbd", "html_url": "https://github.com/rust-lang/rust/commit/d8b1fa0ae0c27e54d3539190683c01e194d36fbd", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/d8b1fa0ae0c27e54d3539190683c01e194d36fbd/comments", "author": {"login": "brendanzab", "id": 695077, "node_id": "MDQ6VXNlcjY5NTA3Nw==", "avatar_url": "https://avatars.githubusercontent.com/u/695077?v=4", "gravatar_id": "", "url": "https://api.github.com/users/brendanzab", "html_url": "https://github.com/brendanzab", "followers_url": "https://api.github.com/users/brendanzab/followers", "following_url": "https://api.github.com/users/brendanzab/following{/other_user}", "gists_url": "https://api.github.com/users/brendanzab/gists{/gist_id}", "starred_url": "https://api.github.com/users/brendanzab/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/brendanzab/subscriptions", "organizations_url": "https://api.github.com/users/brendanzab/orgs", "repos_url": "https://api.github.com/users/brendanzab/repos", "events_url": "https://api.github.com/users/brendanzab/events{/privacy}", "received_events_url": "https://api.github.com/users/brendanzab/received_events", "type": "User", "site_admin": false}, "committer": {"login": "brendanzab", "id": 695077, "node_id": "MDQ6VXNlcjY5NTA3Nw==", "avatar_url": "https://avatars.githubusercontent.com/u/695077?v=4", "gravatar_id": "", "url": "https://api.github.com/users/brendanzab", "html_url": "https://github.com/brendanzab", "followers_url": "https://api.github.com/users/brendanzab/followers", "following_url": "https://api.github.com/users/brendanzab/following{/other_user}", "gists_url": "https://api.github.com/users/brendanzab/gists{/gist_id}", "starred_url": "https://api.github.com/users/brendanzab/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/brendanzab/subscriptions", "organizations_url": "https://api.github.com/users/brendanzab/orgs", "repos_url": "https://api.github.com/users/brendanzab/repos", "events_url": "https://api.github.com/users/brendanzab/events{/privacy}", "received_events_url": "https://api.github.com/users/brendanzab/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "bd7138dd698dde29fb4d7fd34529a863b85d947e", "url": "https://api.github.com/repos/rust-lang/rust/commits/bd7138dd698dde29fb4d7fd34529a863b85d947e", "html_url": "https://github.com/rust-lang/rust/commit/bd7138dd698dde29fb4d7fd34529a863b85d947e"}], "stats": {"total": 2351, "additions": 1198, "deletions": 1153}, "files": [{"sha": "eb3e4ce75c4708b98e2d7907c24c721e5e40d579", "filename": "src/doc/guide-plugin.md", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/rust-lang/rust/blob/d8b1fa0ae0c27e54d3539190683c01e194d36fbd/src%2Fdoc%2Fguide-plugin.md", "raw_url": "https://github.com/rust-lang/rust/raw/d8b1fa0ae0c27e54d3539190683c01e194d36fbd/src%2Fdoc%2Fguide-plugin.md", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Fdoc%2Fguide-plugin.md?ref=d8b1fa0ae0c27e54d3539190683c01e194d36fbd", "patch": "@@ -55,7 +55,7 @@ extern crate syntax;\n extern crate rustc;\n \n use syntax::codemap::Span;\n-use syntax::parse::token::{IDENT, get_ident};\n+use syntax::parse::token;\n use syntax::ast::{TokenTree, TtToken};\n use syntax::ext::base::{ExtCtxt, MacResult, DummyResult, MacExpr};\n use syntax::ext::build::AstBuilder;  // trait for expr_uint\n@@ -71,7 +71,7 @@ fn expand_rn(cx: &mut ExtCtxt, sp: Span, args: &[TokenTree])\n         (\"I\",    1)];\n \n     let text = match args {\n-        [TtToken(_, IDENT(s, _))] => get_ident(s).to_string(),\n+        [TtToken(_, token::Ident(s, _))] => token::get_ident(s).to_string(),\n         _ => {\n             cx.span_err(sp, \"argument should be a single identifier\");\n             return DummyResult::any(sp);"}, {"sha": "16abf5160fa5babb7fd7d48cb2323729bce5f342", "filename": "src/grammar/verify.rs", "status": "modified", "additions": 102, "deletions": 100, "changes": 202, "blob_url": "https://github.com/rust-lang/rust/blob/d8b1fa0ae0c27e54d3539190683c01e194d36fbd/src%2Fgrammar%2Fverify.rs", "raw_url": "https://github.com/rust-lang/rust/raw/d8b1fa0ae0c27e54d3539190683c01e194d36fbd/src%2Fgrammar%2Fverify.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Fgrammar%2Fverify.rs?ref=d8b1fa0ae0c27e54d3539190683c01e194d36fbd", "patch": "@@ -30,12 +30,12 @@ use rustc::driver::{session, config};\n \n use syntax::ast;\n use syntax::ast::Name;\n-use syntax::parse::token::*;\n+use syntax::parse::token;\n use syntax::parse::lexer::TokenAndSpan;\n \n fn parse_token_list(file: &str) -> HashMap<String, Token> {\n     fn id() -> Token {\n-        IDENT(ast::Ident { name: Name(0), ctxt: 0, }, false)\n+        token::Ident(ast::Ident { name: Name(0), ctxt: 0, }, false)\n     }\n \n     let mut res = HashMap::new();\n@@ -52,64 +52,64 @@ fn parse_token_list(file: &str) -> HashMap<String, Token> {\n         let num = line.slice_from(eq + 1);\n \n         let tok = match val {\n-            \"SHR\" => BINOP(SHR),\n-            \"DOLLAR\" => DOLLAR,\n-            \"LT\" => LT,\n-            \"STAR\" => BINOP(STAR),\n-            \"FLOAT_SUFFIX\" => id(),\n-            \"INT_SUFFIX\" => id(),\n-            \"SHL\" => BINOP(SHL),\n-            \"LBRACE\" => LBRACE,\n-            \"RARROW\" => RARROW,\n-            \"LIT_STR\" => LIT_STR(Name(0)),\n-            \"DOTDOT\" => DOTDOT,\n-            \"MOD_SEP\" => MOD_SEP,\n-            \"DOTDOTDOT\" => DOTDOTDOT,\n-            \"NOT\" => NOT,\n-            \"AND\" => BINOP(AND),\n-            \"LPAREN\" => LPAREN,\n-            \"ANDAND\" => ANDAND,\n-            \"AT\" => AT,\n-            \"LBRACKET\" => LBRACKET,\n-            \"LIT_STR_RAW\" => LIT_STR_RAW(Name(0), 0),\n-            \"RPAREN\" => RPAREN,\n-            \"SLASH\" => BINOP(SLASH),\n-            \"COMMA\" => COMMA,\n-            \"LIFETIME\" => LIFETIME(ast::Ident { name: Name(0), ctxt: 0 }),\n-            \"CARET\" => BINOP(CARET),\n-            \"TILDE\" => TILDE,\n-            \"IDENT\" => id(),\n-            \"PLUS\" => BINOP(PLUS),\n-            \"LIT_CHAR\" => LIT_CHAR(Name(0)),\n-            \"LIT_BYTE\" => LIT_BYTE(Name(0)),\n-            \"EQ\" => EQ,\n-            \"RBRACKET\" => RBRACKET,\n-            \"COMMENT\" => COMMENT,\n-            \"DOC_COMMENT\" => DOC_COMMENT(Name(0)),\n-            \"DOT\" => DOT,\n-            \"EQEQ\" => EQEQ,\n-            \"NE\" => NE,\n-            \"GE\" => GE,\n-            \"PERCENT\" => BINOP(PERCENT),\n-            \"RBRACE\" => RBRACE,\n-            \"BINOP\" => BINOP(PLUS),\n-            \"POUND\" => POUND,\n-            \"OROR\" => OROR,\n-            \"LIT_INTEGER\" => LIT_INTEGER(Name(0)),\n-            \"BINOPEQ\" => BINOPEQ(PLUS),\n-            \"LIT_FLOAT\" => LIT_FLOAT(Name(0)),\n-            \"WHITESPACE\" => WS,\n-            \"UNDERSCORE\" => UNDERSCORE,\n-            \"MINUS\" => BINOP(MINUS),\n-            \"SEMI\" => SEMI,\n-            \"COLON\" => COLON,\n-            \"FAT_ARROW\" => FAT_ARROW,\n-            \"OR\" => BINOP(OR),\n-            \"GT\" => GT,\n-            \"LE\" => LE,\n-            \"LIT_BINARY\" => LIT_BINARY(Name(0)),\n-            \"LIT_BINARY_RAW\" => LIT_BINARY_RAW(Name(0), 0),\n-            _ => continue\n+            \"SHR\"               => token::BinOp(token::Shr),\n+            \"DOLLAR\"            => token::Dollar,\n+            \"LT\"                => token::Lt,\n+            \"STAR\"              => token::BinOp(token::Star),\n+            \"FLOAT_SUFFIX\"      => id(),\n+            \"INT_SUFFIX\"        => id(),\n+            \"SHL\"               => token::BinOp(token::Shl),\n+            \"LBRACE\"            => token::LBrace,\n+            \"RARROW\"            => token::Rarrow,\n+            \"LIT_STR\"           => token::LitStr(Name(0)),\n+            \"DOTDOT\"            => token::DotDot,\n+            \"MOD_SEP\"           => token::ModSep,\n+            \"DOTDOTDOT\"         => token::DotDotDot,\n+            \"NOT\"               => token::Not,\n+            \"AND\"               => token::BinOp(token::And),\n+            \"LPAREN\"            => token::LParen,\n+            \"ANDAND\"            => token::AndAnd,\n+            \"AT\"                => token::At,\n+            \"LBRACKET\"          => token::LBracket,\n+            \"LIT_STR_RAW\"       => token::LitStrRaw(Name(0), 0),\n+            \"RPAREN\"            => token::RParen,\n+            \"SLASH\"             => token::BinOp(token::Slash),\n+            \"COMMA\"             => token::Comma,\n+            \"LIFETIME\"          => token::Lifetime(ast::Ident { name: Name(0), ctxt: 0 }),\n+            \"CARET\"             => token::BinOp(token::Caret),\n+            \"TILDE\"             => token::Tilde,\n+            \"IDENT\"             => token::Id(),\n+            \"PLUS\"              => token::BinOp(token::Plus),\n+            \"LIT_CHAR\"          => token::LitChar(Name(0)),\n+            \"LIT_BYTE\"          => token::LitByte(Name(0)),\n+            \"EQ\"                => token::Eq,\n+            \"RBRACKET\"          => token::RBracket,\n+            \"COMMENT\"           => token::Comment,\n+            \"DOC_COMMENT\"       => token::DocComment(Name(0)),\n+            \"DOT\"               => token::Dot,\n+            \"EQEQ\"              => token::EqEq,\n+            \"NE\"                => token::Ne,\n+            \"GE\"                => token::Ge,\n+            \"PERCENT\"           => token::BinOp(token::Percent),\n+            \"RBRACE\"            => token::RBrace,\n+            \"BINOP\"             => token::BinOp(token::Plus),\n+            \"POUND\"             => token::Pound,\n+            \"OROR\"              => token::OrOr,\n+            \"LIT_INTEGER\"       => token::LitInteger(Name(0)),\n+            \"BINOPEQ\"           => token::BinOpEq(token::Plus),\n+            \"LIT_FLOAT\"         => token::LitFloat(Name(0)),\n+            \"WHITESPACE\"        => token::Whitespace,\n+            \"UNDERSCORE\"        => token::Underscore,\n+            \"MINUS\"             => token::BinOp(token::Minus),\n+            \"SEMI\"              => token::Semi,\n+            \"COLON\"             => token::Colon,\n+            \"FAT_ARROW\"         => token::FatArrow,\n+            \"OR\"                => token::BinOp(token::Or),\n+            \"GT\"                => token::Gt,\n+            \"LE\"                => token::Le,\n+            \"LIT_BINARY\"        => token::LitBinary(Name(0)),\n+            \"LIT_BINARY_RAW\"    => token::LitBinaryRaw(Name(0), 0),\n+            _                   => continue,\n         };\n \n         res.insert(num.to_string(), tok);\n@@ -119,19 +119,19 @@ fn parse_token_list(file: &str) -> HashMap<String, Token> {\n     res\n }\n \n-fn str_to_binop(s: &str) -> BinOp {\n+fn str_to_binop(s: &str) -> BinOpToken {\n     match s {\n-        \"+\" => PLUS,\n-        \"/\" => SLASH,\n-        \"-\" => MINUS,\n-        \"*\" => STAR,\n-        \"%\" => PERCENT,\n-        \"^\" => CARET,\n-        \"&\" => AND,\n-        \"|\" => OR,\n-        \"<<\" => SHL,\n-        \">>\" => SHR,\n-        _ => fail!(\"Bad binop str `{}`\", s)\n+        \"+\"     => token::Plus,\n+        \"/\"     => token::Slash,\n+        \"-\"     => token::Minus,\n+        \"*\"     => token::Star,\n+        \"%\"     => token::Percent,\n+        \"^\"     => token::Caret,\n+        \"&\"     => token::And,\n+        \"|\"     => token::Or,\n+        \"<<\"    => token::Shl,\n+        \">>\"    => token::Shr,\n+        _       => fail!(\"Bad binop str `{}`\", s),\n     }\n }\n \n@@ -186,19 +186,20 @@ fn parse_antlr_token(s: &str, tokens: &HashMap<String, Token>) -> TokenAndSpan {\n     debug!(\"What we got: content (`{}`), proto: {}\", content, proto_tok);\n \n     let real_tok = match *proto_tok {\n-        BINOP(..) => BINOP(str_to_binop(content)),\n-        BINOPEQ(..) => BINOPEQ(str_to_binop(content.slice_to(content.len() - 1))),\n-        LIT_STR(..) => LIT_STR(fix(content)),\n-        LIT_STR_RAW(..) => LIT_STR_RAW(fix(content), count(content)),\n-        LIT_CHAR(..) => LIT_CHAR(fixchar(content)),\n-        LIT_BYTE(..) => LIT_BYTE(fixchar(content)),\n-        DOC_COMMENT(..) => DOC_COMMENT(nm),\n-        LIT_INTEGER(..) => LIT_INTEGER(nm),\n-        LIT_FLOAT(..) => LIT_FLOAT(nm),\n-        LIT_BINARY(..) => LIT_BINARY(nm),\n-        LIT_BINARY_RAW(..) => LIT_BINARY_RAW(fix(content), count(content)),\n-        IDENT(..) => IDENT(ast::Ident { name: nm, ctxt: 0 }, true),\n-        LIFETIME(..) => LIFETIME(ast::Ident { name: nm, ctxt: 0 }),\n+        token::BinOp(..)           => token::BinOp(str_to_binop(content)),\n+        token::BinOpEq(..)         => token::BinOpEq(str_to_binop(content.slice_to(\n+                                                                    content.len() - 1))),\n+        token::LitStr(..)          => token::LitStr(fix(content)),\n+        token::LitStrRaw(..)       => token::LitStrRaw(fix(content), count(content)),\n+        token::LitChar(..)         => token::LitChar(fixchar(content)),\n+        token::LitByte(..)         => token::LitByte(fixchar(content)),\n+        token::DocComment(..)      => token::DocComment(nm),\n+        token::LitInteger(..)      => token::LitInteger(nm),\n+        token::LitFloat(..)        => token::LitFloat(nm),\n+        token::LitBinary(..)       => token::LitBinary(nm),\n+        token::LitBinaryRaw(..)    => token::LitBinaryRaw(fix(content), count(content)),\n+        token::Ident(..)           => token::Ident(ast::Ident { name: nm, ctxt: 0 }, true),\n+        token::Lifetime(..)        => token::Lifetime(ast::Ident { name: nm, ctxt: 0 }),\n         ref t => t.clone()\n     };\n \n@@ -222,8 +223,8 @@ fn parse_antlr_token(s: &str, tokens: &HashMap<String, Token>) -> TokenAndSpan {\n \n fn tok_cmp(a: &Token, b: &Token) -> bool {\n     match a {\n-        &IDENT(id, _) => match b {\n-                &IDENT(id2, _) => id == id2,\n+        &token::Ident(id, _) => match b {\n+                &token::Ident(id2, _) => id == id2,\n                 _ => false\n         },\n         _ => a == b\n@@ -281,19 +282,20 @@ fn main() {\n             )\n         )\n \n-        matches!(LIT_BYTE(..),\n-            LIT_CHAR(..),\n-            LIT_INTEGER(..),\n-            LIT_FLOAT(..),\n-            LIT_STR(..),\n-            LIT_STR_RAW(..),\n-            LIT_BINARY(..),\n-            LIT_BINARY_RAW(..),\n-            IDENT(..),\n-            LIFETIME(..),\n-            INTERPOLATED(..),\n-            DOC_COMMENT(..),\n-            SHEBANG(..)\n+        matches!(\n+            LitByte(..),\n+            LitChar(..),\n+            LitInteger(..),\n+            LitFloat(..),\n+            LitStr(..),\n+            LitStrRaw(..),\n+            LitBinary(..),\n+            LitBinaryRaw(..),\n+            Ident(..),\n+            Lifetime(..),\n+            Interpolated(..),\n+            DocComment(..),\n+            Shebang(..)\n         );\n     }\n }"}, {"sha": "28b18ef0bf901f73066213ba1e3a1dfc300d282c", "filename": "src/libregex_macros/lib.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/d8b1fa0ae0c27e54d3539190683c01e194d36fbd/src%2Flibregex_macros%2Flib.rs", "raw_url": "https://github.com/rust-lang/rust/raw/d8b1fa0ae0c27e54d3539190683c01e194d36fbd/src%2Flibregex_macros%2Flib.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibregex_macros%2Flib.rs?ref=d8b1fa0ae0c27e54d3539190683c01e194d36fbd", "patch": "@@ -634,7 +634,7 @@ fn parse(cx: &mut ExtCtxt, tts: &[ast::TokenTree]) -> Option<String> {\n             return None\n         }\n     };\n-    if !parser.eat(&token::EOF) {\n+    if !parser.eat(&token::Eof) {\n         cx.span_err(parser.span, \"only one string literal allowed\");\n         return None;\n     }"}, {"sha": "4748de01240cfb210a4b307009df6a01243bdea4", "filename": "src/librustc/middle/save/mod.rs", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/rust-lang/rust/blob/d8b1fa0ae0c27e54d3539190683c01e194d36fbd/src%2Flibrustc%2Fmiddle%2Fsave%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/d8b1fa0ae0c27e54d3539190683c01e194d36fbd/src%2Flibrustc%2Fmiddle%2Fsave%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Fmiddle%2Fsave%2Fmod.rs?ref=d8b1fa0ae0c27e54d3539190683c01e194d36fbd", "patch": "@@ -428,7 +428,7 @@ impl <'l, 'tcx> DxrVisitor<'l, 'tcx> {\n                 let qualname = format!(\"{}::{}\", qualname, name);\n                 let typ = ppaux::ty_to_string(&self.analysis.ty_cx,\n                     (*self.analysis.ty_cx.node_types.borrow())[field.node.id as uint]);\n-                match self.span.sub_span_before_token(field.span, token::COLON) {\n+                match self.span.sub_span_before_token(field.span, token::Colon) {\n                     Some(sub_span) => self.fmt.field_str(field.span,\n                                                          Some(sub_span),\n                                                          field.node.id,\n@@ -1175,7 +1175,7 @@ impl<'l, 'tcx, 'v> Visitor<'v> for DxrVisitor<'l, 'tcx> {\n                         // 'use' always introduces an alias, if there is not an explicit\n                         // one, there is an implicit one.\n                         let sub_span =\n-                            match self.span.sub_span_before_token(path.span, token::EQ) {\n+                            match self.span.sub_span_before_token(path.span, token::Eq) {\n                                 Some(sub_span) => Some(sub_span),\n                                 None => sub_span,\n                             };"}, {"sha": "08567dba3a4fa3a777a17c209360958f811fb0c6", "filename": "src/librustc/middle/save/span_utils.rs", "status": "modified", "additions": 30, "deletions": 30, "changes": 60, "blob_url": "https://github.com/rust-lang/rust/blob/d8b1fa0ae0c27e54d3539190683c01e194d36fbd/src%2Flibrustc%2Fmiddle%2Fsave%2Fspan_utils.rs", "raw_url": "https://github.com/rust-lang/rust/raw/d8b1fa0ae0c27e54d3539190683c01e194d36fbd/src%2Flibrustc%2Fmiddle%2Fsave%2Fspan_utils.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Fmiddle%2Fsave%2Fspan_utils.rs?ref=d8b1fa0ae0c27e54d3539190683c01e194d36fbd", "patch": "@@ -93,7 +93,7 @@ impl<'a> SpanUtils<'a> {\n         let mut bracket_count = 0u;\n         loop {\n             let ts = toks.next_token();\n-            if ts.tok == token::EOF {\n+            if ts.tok == token::Eof {\n                 return self.make_sub_span(span, result)\n             }\n             if bracket_count == 0 &&\n@@ -102,9 +102,9 @@ impl<'a> SpanUtils<'a> {\n             }\n \n             bracket_count += match ts.tok {\n-                token::LT => 1,\n-                token::GT => -1,\n-                token::BINOP(token::SHR) => -2,\n+                token::Lt => 1,\n+                token::Gt => -1,\n+                token::BinOp(token::Shr) => -2,\n                 _ => 0\n             }\n         }\n@@ -116,7 +116,7 @@ impl<'a> SpanUtils<'a> {\n         let mut bracket_count = 0u;\n         loop {\n             let ts = toks.next_token();\n-            if ts.tok == token::EOF {\n+            if ts.tok == token::Eof {\n                 return None;\n             }\n             if bracket_count == 0 &&\n@@ -125,9 +125,9 @@ impl<'a> SpanUtils<'a> {\n             }\n \n             bracket_count += match ts.tok {\n-                token::LT => 1,\n-                token::GT => -1,\n-                token::BINOP(token::SHR) => -2,\n+                token::Lt => 1,\n+                token::Gt => -1,\n+                token::BinOp(token::Shr) => -2,\n                 _ => 0\n             }\n         }\n@@ -141,32 +141,32 @@ impl<'a> SpanUtils<'a> {\n         let mut result = None;\n         let mut bracket_count = 0u;\n         let mut last_span = None;\n-        while prev.tok != token::EOF {\n+        while prev.tok != token::Eof {\n             last_span = None;\n             let mut next = toks.next_token();\n \n-            if (next.tok == token::LPAREN ||\n-                next.tok == token::LT) &&\n+            if (next.tok == token::LParen ||\n+                next.tok == token::Lt) &&\n                bracket_count == 0 &&\n                is_ident(&prev.tok) {\n                 result = Some(prev.sp);\n             }\n \n             if bracket_count == 0 &&\n-                next.tok == token::MOD_SEP {\n+                next.tok == token::ModSep {\n                 let old = prev;\n                 prev = next;\n                 next = toks.next_token();\n-                if next.tok == token::LT &&\n+                if next.tok == token::Lt &&\n                    is_ident(&old.tok) {\n                     result = Some(old.sp);\n                 }\n             }\n \n             bracket_count += match prev.tok {\n-                token::LPAREN | token::LT => 1,\n-                token::RPAREN | token::GT => -1,\n-                token::BINOP(token::SHR) => -2,\n+                token::LParen | token::Lt => 1,\n+                token::RParen | token::Gt => -1,\n+                token::BinOp(token::Shr) => -2,\n                 _ => 0\n             };\n \n@@ -191,21 +191,21 @@ impl<'a> SpanUtils<'a> {\n         loop {\n             let next = toks.next_token();\n \n-            if (next.tok == token::LT ||\n-                next.tok == token::COLON) &&\n+            if (next.tok == token::Lt ||\n+                next.tok == token::Colon) &&\n                bracket_count == 0 &&\n                is_ident(&prev.tok) {\n                 result = Some(prev.sp);\n             }\n \n             bracket_count += match prev.tok {\n-                token::LT => 1,\n-                token::GT => -1,\n-                token::BINOP(token::SHR) => -2,\n+                token::Lt => 1,\n+                token::Gt => -1,\n+                token::BinOp(token::Shr) => -2,\n                 _ => 0\n             };\n \n-            if next.tok == token::EOF {\n+            if next.tok == token::Eof {\n                 break;\n             }\n             prev = next;\n@@ -235,7 +235,7 @@ impl<'a> SpanUtils<'a> {\n         let mut bracket_count = 0i;\n         loop {\n             let ts = toks.next_token();\n-            if ts.tok == token::EOF {\n+            if ts.tok == token::Eof {\n                 if bracket_count != 0 {\n                     let loc = self.sess.codemap().lookup_char_pos(span.lo);\n                     self.sess.span_bug(span, format!(\n@@ -248,10 +248,10 @@ impl<'a> SpanUtils<'a> {\n                 return result;\n             }\n             bracket_count += match ts.tok {\n-                token::LT => 1,\n-                token::GT => -1,\n-                token::BINOP(token::SHL) => 2,\n-                token::BINOP(token::SHR) => -2,\n+                token::Lt => 1,\n+                token::Gt => -1,\n+                token::BinOp(token::Shl) => 2,\n+                token::BinOp(token::Shr) => -2,\n                 _ => 0\n             };\n             if is_ident(&ts.tok) &&\n@@ -265,7 +265,7 @@ impl<'a> SpanUtils<'a> {\n         let mut toks = self.retokenise_span(span);\n         let mut prev = toks.next_token();\n         loop {\n-            if prev.tok == token::EOF {\n+            if prev.tok == token::Eof {\n                 return None;\n             }\n             let next = toks.next_token();\n@@ -282,12 +282,12 @@ impl<'a> SpanUtils<'a> {\n         let mut toks = self.retokenise_span(span);\n         loop {\n             let ts = toks.next_token();\n-            if ts.tok == token::EOF {\n+            if ts.tok == token::Eof {\n                 return None;\n             }\n             if is_keyword(keyword, &ts.tok) {\n                 let ts = toks.next_token();\n-                if ts.tok == token::EOF {\n+                if ts.tok == token::Eof {\n                     return None\n                 } else {\n                     return self.make_sub_span(span, Some(ts.sp));"}, {"sha": "481cd19739477a23ffcf21d21209164f1122a66d", "filename": "src/librustdoc/html/highlight.rs", "status": "modified", "additions": 29, "deletions": 28, "changes": 57, "blob_url": "https://github.com/rust-lang/rust/blob/d8b1fa0ae0c27e54d3539190683c01e194d36fbd/src%2Flibrustdoc%2Fhtml%2Fhighlight.rs", "raw_url": "https://github.com/rust-lang/rust/raw/d8b1fa0ae0c27e54d3539190683c01e194d36fbd/src%2Flibrustdoc%2Fhtml%2Fhighlight.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustdoc%2Fhtml%2Fhighlight.rs?ref=d8b1fa0ae0c27e54d3539190683c01e194d36fbd", "patch": "@@ -17,7 +17,7 @@ use html::escape::Escape;\n \n use std::io;\n use syntax::parse::lexer;\n-use syntax::parse::token as t;\n+use syntax::parse::token;\n use syntax::parse;\n \n /// Highlights some source code, returning the HTML output.\n@@ -63,44 +63,45 @@ fn doit(sess: &parse::ParseSess, mut lexer: lexer::StringReader,\n \n         let snip = |sp| sess.span_diagnostic.cm.span_to_snippet(sp).unwrap();\n \n-        if next.tok == t::EOF { break }\n+        if next.tok == token::Eof { break }\n \n         let klass = match next.tok {\n-            t::WS => {\n+            token::Whitespace => {\n                 try!(write!(out, \"{}\", Escape(snip(next.sp).as_slice())));\n                 continue\n             },\n-            t::COMMENT => {\n+            token::Comment => {\n                 try!(write!(out, \"<span class='comment'>{}</span>\",\n                             Escape(snip(next.sp).as_slice())));\n                 continue\n             },\n-            t::SHEBANG(s) => {\n+            token::Shebang(s) => {\n                 try!(write!(out, \"{}\", Escape(s.as_str())));\n                 continue\n             },\n             // If this '&' token is directly adjacent to another token, assume\n             // that it's the address-of operator instead of the and-operator.\n             // This allows us to give all pointers their own class (`Box` and\n             // `@` are below).\n-            t::BINOP(t::AND) if lexer.peek().sp.lo == next.sp.hi => \"kw-2\",\n-            t::AT | t::TILDE => \"kw-2\",\n+            token::BinOp(token::And) if lexer.peek().sp.lo == next.sp.hi => \"kw-2\",\n+            token::At | token::Tilde => \"kw-2\",\n \n             // consider this as part of a macro invocation if there was a\n             // leading identifier\n-            t::NOT if is_macro => { is_macro = false; \"macro\" }\n+            token::Not if is_macro => { is_macro = false; \"macro\" }\n \n             // operators\n-            t::EQ | t::LT | t::LE | t::EQEQ | t::NE | t::GE | t::GT |\n-                t::ANDAND | t::OROR | t::NOT | t::BINOP(..) | t::RARROW |\n-                t::BINOPEQ(..) | t::FAT_ARROW => \"op\",\n+            token::Eq | token::Lt | token::Le | token::EqEq | token::Ne | token::Ge | token::Gt |\n+                token::AndAnd | token::OrOr | token::Not | token::BinOp(..) | token::RArrow |\n+                token::BinOpEq(..) | token::FatArrow => \"op\",\n \n             // miscellaneous, no highlighting\n-            t::DOT | t::DOTDOT | t::DOTDOTDOT | t::COMMA | t::SEMI |\n-                t::COLON | t::MOD_SEP | t::LARROW | t::LPAREN |\n-                t::RPAREN | t::LBRACKET | t::LBRACE | t::RBRACE | t::QUESTION => \"\",\n-            t::DOLLAR => {\n-                if t::is_ident(&lexer.peek().tok) {\n+            token::Dot | token::DotDot | token::DotDotDot | token::Comma | token::Semi |\n+                token::Colon | token::ModSep | token::LArrow | token::LParen |\n+                token::RParen | token::LBracket | token::LBrace | token::RBrace |\n+                token::Question => \"\",\n+            token::Dollar => {\n+                if token::is_ident(&lexer.peek().tok) {\n                     is_macro_nonterminal = true;\n                     \"macro-nonterminal\"\n                 } else {\n@@ -112,12 +113,12 @@ fn doit(sess: &parse::ParseSess, mut lexer: lexer::StringReader,\n             // continue highlighting it as an attribute until the ending ']' is\n             // seen, so skip out early. Down below we terminate the attribute\n             // span when we see the ']'.\n-            t::POUND => {\n+            token::Pound => {\n                 is_attribute = true;\n                 try!(write!(out, r\"<span class='attribute'>#\"));\n                 continue\n             }\n-            t::RBRACKET => {\n+            token::RBracket => {\n                 if is_attribute {\n                     is_attribute = false;\n                     try!(write!(out, \"]</span>\"));\n@@ -128,15 +129,15 @@ fn doit(sess: &parse::ParseSess, mut lexer: lexer::StringReader,\n             }\n \n             // text literals\n-            t::LIT_BYTE(..) | t::LIT_BINARY(..) | t::LIT_BINARY_RAW(..) |\n-                t::LIT_CHAR(..) | t::LIT_STR(..) | t::LIT_STR_RAW(..) => \"string\",\n+            token::LitByte(..) | token::LitBinary(..) | token::LitBinaryRaw(..) |\n+                token::LitChar(..) | token::LitStr(..) | token::LitStrRaw(..) => \"string\",\n \n             // number literals\n-            t::LIT_INTEGER(..) | t::LIT_FLOAT(..) => \"number\",\n+            token::LitInteger(..) | token::LitFloat(..) => \"number\",\n \n             // keywords are also included in the identifier set\n-            t::IDENT(ident, _is_mod_sep) => {\n-                match t::get_ident(ident).get() {\n+            token::Ident(ident, _is_mod_sep) => {\n+                match token::get_ident(ident).get() {\n                     \"ref\" | \"mut\" => \"kw-2\",\n \n                     \"self\" => \"self\",\n@@ -145,12 +146,12 @@ fn doit(sess: &parse::ParseSess, mut lexer: lexer::StringReader,\n                     \"Option\" | \"Result\" => \"prelude-ty\",\n                     \"Some\" | \"None\" | \"Ok\" | \"Err\" => \"prelude-val\",\n \n-                    _ if t::is_any_keyword(&next.tok) => \"kw\",\n+                    _ if token::is_any_keyword(&next.tok) => \"kw\",\n                     _ => {\n                         if is_macro_nonterminal {\n                             is_macro_nonterminal = false;\n                             \"macro-nonterminal\"\n-                        } else if lexer.peek().tok == t::NOT {\n+                        } else if lexer.peek().tok == token::Not {\n                             is_macro = true;\n                             \"macro\"\n                         } else {\n@@ -160,9 +161,9 @@ fn doit(sess: &parse::ParseSess, mut lexer: lexer::StringReader,\n                 }\n             }\n \n-            t::LIFETIME(..) => \"lifetime\",\n-            t::DOC_COMMENT(..) => \"doccomment\",\n-            t::UNDERSCORE | t::EOF | t::INTERPOLATED(..) => \"\",\n+            token::Lifetime(..) => \"lifetime\",\n+            token::DocComment(..) => \"doccomment\",\n+            token::Underscore | token::Eof | token::Interpolated(..) => \"\",\n         };\n \n         // as mentioned above, use the original source code instead of"}, {"sha": "d9d549f6841252da37ef21ac3ec76e2565f0bfe1", "filename": "src/libsyntax/diagnostics/plugin.rs", "status": "modified", "additions": 6, "deletions": 6, "changes": 12, "blob_url": "https://github.com/rust-lang/rust/blob/d8b1fa0ae0c27e54d3539190683c01e194d36fbd/src%2Flibsyntax%2Fdiagnostics%2Fplugin.rs", "raw_url": "https://github.com/rust-lang/rust/raw/d8b1fa0ae0c27e54d3539190683c01e194d36fbd/src%2Flibsyntax%2Fdiagnostics%2Fplugin.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fdiagnostics%2Fplugin.rs?ref=d8b1fa0ae0c27e54d3539190683c01e194d36fbd", "patch": "@@ -50,7 +50,7 @@ pub fn expand_diagnostic_used<'cx>(ecx: &'cx mut ExtCtxt,\n                                    token_tree: &[TokenTree])\n                                    -> Box<MacResult+'cx> {\n     let code = match token_tree {\n-        [ast::TtToken(_, token::IDENT(code, _))] => code,\n+        [ast::TtToken(_, token::Ident(code, _))] => code,\n         _ => unreachable!()\n     };\n     with_registered_diagnostics(|diagnostics| {\n@@ -82,12 +82,12 @@ pub fn expand_register_diagnostic<'cx>(ecx: &'cx mut ExtCtxt,\n                                        token_tree: &[TokenTree])\n                                        -> Box<MacResult+'cx> {\n     let (code, description) = match token_tree {\n-        [ast::TtToken(_, token::IDENT(ref code, _))] => {\n+        [ast::TtToken(_, token::Ident(ref code, _))] => {\n             (code, None)\n         },\n-        [ast::TtToken(_, token::IDENT(ref code, _)),\n-         ast::TtToken(_, token::COMMA),\n-         ast::TtToken(_, token::LIT_STR_RAW(description, _))] => {\n+        [ast::TtToken(_, token::Ident(ref code, _)),\n+         ast::TtToken(_, token::Comma),\n+         ast::TtToken(_, token::LitStrRaw(description, _))] => {\n             (code, Some(description))\n         }\n         _ => unreachable!()\n@@ -110,7 +110,7 @@ pub fn expand_build_diagnostic_array<'cx>(ecx: &'cx mut ExtCtxt,\n                                           token_tree: &[TokenTree])\n                                           -> Box<MacResult+'cx> {\n     let name = match token_tree {\n-        [ast::TtToken(_, token::IDENT(ref name, _))] => name,\n+        [ast::TtToken(_, token::Ident(ref name, _))] => name,\n         _ => unreachable!()\n     };\n "}, {"sha": "2b52b7feaccd0c5487b9148d15f818c569946042", "filename": "src/libsyntax/ext/asm.rs", "status": "modified", "additions": 23, "deletions": 23, "changes": 46, "blob_url": "https://github.com/rust-lang/rust/blob/d8b1fa0ae0c27e54d3539190683c01e194d36fbd/src%2Flibsyntax%2Fext%2Fasm.rs", "raw_url": "https://github.com/rust-lang/rust/raw/d8b1fa0ae0c27e54d3539190683c01e194d36fbd/src%2Flibsyntax%2Fext%2Fasm.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fext%2Fasm.rs?ref=d8b1fa0ae0c27e54d3539190683c01e194d36fbd", "patch": "@@ -72,21 +72,21 @@ pub fn expand_asm<'cx>(cx: &'cx mut ExtCtxt, sp: Span, tts: &[ast::TokenTree])\n                 asm_str_style = Some(style);\n             }\n             Outputs => {\n-                while p.token != token::EOF &&\n-                      p.token != token::COLON &&\n-                      p.token != token::MOD_SEP {\n+                while p.token != token::Eof &&\n+                      p.token != token::Colon &&\n+                      p.token != token::ModSep {\n \n                     if outputs.len() != 0 {\n-                        p.eat(&token::COMMA);\n+                        p.eat(&token::Comma);\n                     }\n \n                     let (constraint, _str_style) = p.parse_str();\n \n                     let span = p.last_span;\n \n-                    p.expect(&token::LPAREN);\n+                    p.expect(&token::LParen);\n                     let out = p.parse_expr();\n-                    p.expect(&token::RPAREN);\n+                    p.expect(&token::RParen);\n \n                     // Expands a read+write operand into two operands.\n                     //\n@@ -113,12 +113,12 @@ pub fn expand_asm<'cx>(cx: &'cx mut ExtCtxt, sp: Span, tts: &[ast::TokenTree])\n                 }\n             }\n             Inputs => {\n-                while p.token != token::EOF &&\n-                      p.token != token::COLON &&\n-                      p.token != token::MOD_SEP {\n+                while p.token != token::Eof &&\n+                      p.token != token::Colon &&\n+                      p.token != token::ModSep {\n \n                     if inputs.len() != 0 {\n-                        p.eat(&token::COMMA);\n+                        p.eat(&token::Comma);\n                     }\n \n                     let (constraint, _str_style) = p.parse_str();\n@@ -129,21 +129,21 @@ pub fn expand_asm<'cx>(cx: &'cx mut ExtCtxt, sp: Span, tts: &[ast::TokenTree])\n                         cx.span_err(p.last_span, \"input operand constraint contains '+'\");\n                     }\n \n-                    p.expect(&token::LPAREN);\n+                    p.expect(&token::LParen);\n                     let input = p.parse_expr();\n-                    p.expect(&token::RPAREN);\n+                    p.expect(&token::RParen);\n \n                     inputs.push((constraint, input));\n                 }\n             }\n             Clobbers => {\n                 let mut clobs = Vec::new();\n-                while p.token != token::EOF &&\n-                      p.token != token::COLON &&\n-                      p.token != token::MOD_SEP {\n+                while p.token != token::Eof &&\n+                      p.token != token::Colon &&\n+                      p.token != token::ModSep {\n \n                     if clobs.len() != 0 {\n-                        p.eat(&token::COMMA);\n+                        p.eat(&token::Comma);\n                     }\n \n                     let (s, _str_style) = p.parse_str();\n@@ -172,8 +172,8 @@ pub fn expand_asm<'cx>(cx: &'cx mut ExtCtxt, sp: Span, tts: &[ast::TokenTree])\n                     cx.span_warn(p.last_span, \"unrecognized option\");\n                 }\n \n-                if p.token == token::COMMA {\n-                    p.eat(&token::COMMA);\n+                if p.token == token::Comma {\n+                    p.eat(&token::Comma);\n                 }\n             }\n             StateNone => ()\n@@ -183,17 +183,17 @@ pub fn expand_asm<'cx>(cx: &'cx mut ExtCtxt, sp: Span, tts: &[ast::TokenTree])\n             // MOD_SEP is a double colon '::' without space in between.\n             // When encountered, the state must be advanced twice.\n             match (&p.token, state.next(), state.next().next()) {\n-                (&token::COLON, StateNone, _)   |\n-                (&token::MOD_SEP, _, StateNone) => {\n+                (&token::Colon, StateNone, _)   |\n+                (&token::ModSep, _, StateNone) => {\n                     p.bump();\n                     break 'statement;\n                 }\n-                (&token::COLON, st, _)   |\n-                (&token::MOD_SEP, _, st) => {\n+                (&token::Colon, st, _)   |\n+                (&token::ModSep, _, st) => {\n                     p.bump();\n                     state = st;\n                 }\n-                (&token::EOF, _, _) => break 'statement,\n+                (&token::Eof, _, _) => break 'statement,\n                 _ => break\n             }\n         }"}, {"sha": "a8326e79ef368eac5506a72684c0c7eb98e5c171", "filename": "src/libsyntax/ext/base.rs", "status": "modified", "additions": 5, "deletions": 5, "changes": 10, "blob_url": "https://github.com/rust-lang/rust/blob/d8b1fa0ae0c27e54d3539190683c01e194d36fbd/src%2Flibsyntax%2Fext%2Fbase.rs", "raw_url": "https://github.com/rust-lang/rust/raw/d8b1fa0ae0c27e54d3539190683c01e194d36fbd/src%2Flibsyntax%2Fext%2Fbase.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fext%2Fbase.rs?ref=d8b1fa0ae0c27e54d3539190683c01e194d36fbd", "patch": "@@ -684,8 +684,8 @@ pub fn get_single_str_from_tts(cx: &ExtCtxt,\n         cx.span_err(sp, format!(\"{} takes 1 argument.\", name).as_slice());\n     } else {\n         match tts[0] {\n-            ast::TtToken(_, token::LIT_STR(ident)) => return Some(parse::str_lit(ident.as_str())),\n-            ast::TtToken(_, token::LIT_STR_RAW(ident, _)) => {\n+            ast::TtToken(_, token::LitStr(ident)) => return Some(parse::str_lit(ident.as_str())),\n+            ast::TtToken(_, token::LitStrRaw(ident, _)) => {\n                 return Some(parse::raw_str_lit(ident.as_str()))\n             }\n             _ => {\n@@ -704,12 +704,12 @@ pub fn get_exprs_from_tts(cx: &mut ExtCtxt,\n                           tts: &[ast::TokenTree]) -> Option<Vec<P<ast::Expr>>> {\n     let mut p = cx.new_parser_from_tts(tts);\n     let mut es = Vec::new();\n-    while p.token != token::EOF {\n+    while p.token != token::Eof {\n         es.push(cx.expander().fold_expr(p.parse_expr()));\n-        if p.eat(&token::COMMA) {\n+        if p.eat(&token::Comma) {\n             continue;\n         }\n-        if p.token != token::EOF {\n+        if p.token != token::Eof {\n             cx.span_err(sp, \"expected token: `,`\");\n             return None;\n         }"}, {"sha": "72da60ffe0941f827a97fb6723de25ddf27fc1b3", "filename": "src/libsyntax/ext/cfg.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/d8b1fa0ae0c27e54d3539190683c01e194d36fbd/src%2Flibsyntax%2Fext%2Fcfg.rs", "raw_url": "https://github.com/rust-lang/rust/raw/d8b1fa0ae0c27e54d3539190683c01e194d36fbd/src%2Flibsyntax%2Fext%2Fcfg.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fext%2Fcfg.rs?ref=d8b1fa0ae0c27e54d3539190683c01e194d36fbd", "patch": "@@ -29,7 +29,7 @@ pub fn expand_cfg<'cx>(cx: &mut ExtCtxt,\n     let mut p = cx.new_parser_from_tts(tts);\n     let cfg = p.parse_meta_item();\n \n-    if !p.eat(&token::EOF) {\n+    if !p.eat(&token::Eof) {\n         cx.span_err(sp, \"expected 1 cfg-pattern\");\n         return DummyResult::expr(sp);\n     }"}, {"sha": "e5e93a7d8b3bb3c761e2e7ddf643fb9aaf266859", "filename": "src/libsyntax/ext/concat_idents.rs", "status": "modified", "additions": 5, "deletions": 5, "changes": 10, "blob_url": "https://github.com/rust-lang/rust/blob/d8b1fa0ae0c27e54d3539190683c01e194d36fbd/src%2Flibsyntax%2Fext%2Fconcat_idents.rs", "raw_url": "https://github.com/rust-lang/rust/raw/d8b1fa0ae0c27e54d3539190683c01e194d36fbd/src%2Flibsyntax%2Fext%2Fconcat_idents.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fext%2Fconcat_idents.rs?ref=d8b1fa0ae0c27e54d3539190683c01e194d36fbd", "patch": "@@ -23,21 +23,21 @@ pub fn expand_syntax_ext<'cx>(cx: &mut ExtCtxt, sp: Span, tts: &[ast::TokenTree]\n     for (i, e) in tts.iter().enumerate() {\n         if i & 1 == 1 {\n             match *e {\n-                ast::TtToken(_, token::COMMA) => (),\n+                ast::TtToken(_, token::Comma) => {},\n                 _ => {\n                     cx.span_err(sp, \"concat_idents! expecting comma.\");\n                     return DummyResult::expr(sp);\n-                }\n+                },\n             }\n         } else {\n             match *e {\n-                ast::TtToken(_, token::IDENT(ident,_)) => {\n+                ast::TtToken(_, token::Ident(ident, _)) => {\n                     res_str.push_str(token::get_ident(ident).get())\n-                }\n+                },\n                 _ => {\n                     cx.span_err(sp, \"concat_idents! requires ident args.\");\n                     return DummyResult::expr(sp);\n-                }\n+                },\n             }\n         }\n     }"}, {"sha": "fdf61d4abd9dab3bf6d77dbc28a9a118cc0bd138", "filename": "src/libsyntax/ext/format.rs", "status": "modified", "additions": 9, "deletions": 9, "changes": 18, "blob_url": "https://github.com/rust-lang/rust/blob/d8b1fa0ae0c27e54d3539190683c01e194d36fbd/src%2Flibsyntax%2Fext%2Fformat.rs", "raw_url": "https://github.com/rust-lang/rust/raw/d8b1fa0ae0c27e54d3539190683c01e194d36fbd/src%2Flibsyntax%2Fext%2Fformat.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fext%2Fformat.rs?ref=d8b1fa0ae0c27e54d3539190683c01e194d36fbd", "patch": "@@ -91,36 +91,36 @@ fn parse_args(ecx: &mut ExtCtxt, sp: Span, allow_method: bool,\n     // Parse the leading function expression (maybe a block, maybe a path)\n     let invocation = if allow_method {\n         let e = p.parse_expr();\n-        if !p.eat(&token::COMMA) {\n+        if !p.eat(&token::Comma) {\n             ecx.span_err(sp, \"expected token: `,`\");\n             return (Call(e), None);\n         }\n         MethodCall(e, p.parse_ident())\n     } else {\n         Call(p.parse_expr())\n     };\n-    if !p.eat(&token::COMMA) {\n+    if !p.eat(&token::Comma) {\n         ecx.span_err(sp, \"expected token: `,`\");\n         return (invocation, None);\n     }\n \n-    if p.token == token::EOF {\n+    if p.token == token::Eof {\n         ecx.span_err(sp, \"requires at least a format string argument\");\n         return (invocation, None);\n     }\n     let fmtstr = p.parse_expr();\n     let mut named = false;\n-    while p.token != token::EOF {\n-        if !p.eat(&token::COMMA) {\n+    while p.token != token::Eof {\n+        if !p.eat(&token::Comma) {\n             ecx.span_err(sp, \"expected token: `,`\");\n             return (invocation, None);\n         }\n-        if p.token == token::EOF { break } // accept trailing commas\n+        if p.token == token::Eof { break } // accept trailing commas\n         if named || (token::is_ident(&p.token) &&\n-                     p.look_ahead(1, |t| *t == token::EQ)) {\n+                     p.look_ahead(1, |t| *t == token::Eq)) {\n             named = true;\n             let ident = match p.token {\n-                token::IDENT(i, _) => {\n+                token::Ident(i, _) => {\n                     p.bump();\n                     i\n                 }\n@@ -139,7 +139,7 @@ fn parse_args(ecx: &mut ExtCtxt, sp: Span, allow_method: bool,\n             };\n             let interned_name = token::get_ident(ident);\n             let name = interned_name.get();\n-            p.expect(&token::EQ);\n+            p.expect(&token::Eq);\n             let e = p.parse_expr();\n             match names.find_equiv(&name) {\n                 None => {}"}, {"sha": "39a538f917b009894e36c2043de625631f46bc98", "filename": "src/libsyntax/ext/quote.rs", "status": "modified", "additions": 67, "deletions": 68, "changes": 135, "blob_url": "https://github.com/rust-lang/rust/blob/d8b1fa0ae0c27e54d3539190683c01e194d36fbd/src%2Flibsyntax%2Fext%2Fquote.rs", "raw_url": "https://github.com/rust-lang/rust/raw/d8b1fa0ae0c27e54d3539190683c01e194d36fbd/src%2Flibsyntax%2Fext%2Fquote.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fext%2Fquote.rs?ref=d8b1fa0ae0c27e54d3539190683c01e194d36fbd", "patch": "@@ -515,123 +515,122 @@ fn mk_token_path(cx: &ExtCtxt, sp: Span, name: &str) -> P<ast::Expr> {\n     cx.expr_path(cx.path_global(sp, idents))\n }\n \n-fn mk_binop(cx: &ExtCtxt, sp: Span, bop: token::BinOp) -> P<ast::Expr> {\n+fn mk_binop(cx: &ExtCtxt, sp: Span, bop: token::BinOpToken) -> P<ast::Expr> {\n     let name = match bop {\n-        PLUS => \"PLUS\",\n-        MINUS => \"MINUS\",\n-        STAR => \"STAR\",\n-        SLASH => \"SLASH\",\n-        PERCENT => \"PERCENT\",\n-        CARET => \"CARET\",\n-        AND => \"AND\",\n-        OR => \"OR\",\n-        SHL => \"SHL\",\n-        SHR => \"SHR\"\n+        token::Plus     => \"Plus\",\n+        token::Minus    => \"Minus\",\n+        token::Star     => \"Star\",\n+        token::Slash    => \"Slash\",\n+        token::Percent  => \"Percent\",\n+        token::Caret    => \"Caret\",\n+        token::And      => \"And\",\n+        token::Or       => \"Or\",\n+        token::Shl      => \"Shl\",\n+        token::Shr      => \"Shr\"\n     };\n     mk_token_path(cx, sp, name)\n }\n \n fn mk_token(cx: &ExtCtxt, sp: Span, tok: &token::Token) -> P<ast::Expr> {\n-\n     match *tok {\n-        BINOP(binop) => {\n-            return cx.expr_call(sp, mk_token_path(cx, sp, \"BINOP\"), vec!(mk_binop(cx, sp, binop)));\n+        token::BinOp(binop) => {\n+            return cx.expr_call(sp, mk_token_path(cx, sp, \"BinOp\"), vec!(mk_binop(cx, sp, binop)));\n         }\n-        BINOPEQ(binop) => {\n-            return cx.expr_call(sp, mk_token_path(cx, sp, \"BINOPEQ\"),\n+        token::BinOpEq(binop) => {\n+            return cx.expr_call(sp, mk_token_path(cx, sp, \"BinOpEq\"),\n                                 vec!(mk_binop(cx, sp, binop)));\n         }\n \n-        LIT_BYTE(i) => {\n+        token::LitByte(i) => {\n             let e_byte = mk_name(cx, sp, i.ident());\n \n-            return cx.expr_call(sp, mk_token_path(cx, sp, \"LIT_BYTE\"), vec!(e_byte));\n+            return cx.expr_call(sp, mk_token_path(cx, sp, \"LitByte\"), vec!(e_byte));\n         }\n \n-        LIT_CHAR(i) => {\n+        token::LitChar(i) => {\n             let e_char = mk_name(cx, sp, i.ident());\n \n-            return cx.expr_call(sp, mk_token_path(cx, sp, \"LIT_CHAR\"), vec!(e_char));\n+            return cx.expr_call(sp, mk_token_path(cx, sp, \"LitChar\"), vec!(e_char));\n         }\n \n-        LIT_INTEGER(i) => {\n+        token::LitInteger(i) => {\n             let e_int = mk_name(cx, sp, i.ident());\n-            return cx.expr_call(sp, mk_token_path(cx, sp, \"LIT_INTEGER\"), vec!(e_int));\n+            return cx.expr_call(sp, mk_token_path(cx, sp, \"LitInteger\"), vec!(e_int));\n         }\n \n-        LIT_FLOAT(fident) => {\n+        token::LitFloat(fident) => {\n             let e_fident = mk_name(cx, sp, fident.ident());\n-            return cx.expr_call(sp, mk_token_path(cx, sp, \"LIT_FLOAT\"), vec!(e_fident));\n+            return cx.expr_call(sp, mk_token_path(cx, sp, \"LitFloat\"), vec!(e_fident));\n         }\n \n-        LIT_STR(ident) => {\n+        token::LitStr(ident) => {\n             return cx.expr_call(sp,\n-                                mk_token_path(cx, sp, \"LIT_STR\"),\n+                                mk_token_path(cx, sp, \"LitStr\"),\n                                 vec!(mk_name(cx, sp, ident.ident())));\n         }\n \n-        LIT_STR_RAW(ident, n) => {\n+        token::LitStrRaw(ident, n) => {\n             return cx.expr_call(sp,\n-                                mk_token_path(cx, sp, \"LIT_STR_RAW\"),\n+                                mk_token_path(cx, sp, \"LitStrRaw\"),\n                                 vec!(mk_name(cx, sp, ident.ident()), cx.expr_uint(sp, n)));\n         }\n \n-        IDENT(ident, b) => {\n+        token::Ident(ident, b) => {\n             return cx.expr_call(sp,\n-                                mk_token_path(cx, sp, \"IDENT\"),\n+                                mk_token_path(cx, sp, \"Ident\"),\n                                 vec!(mk_ident(cx, sp, ident), cx.expr_bool(sp, b)));\n         }\n \n-        LIFETIME(ident) => {\n+        token::Lifetime(ident) => {\n             return cx.expr_call(sp,\n-                                mk_token_path(cx, sp, \"LIFETIME\"),\n+                                mk_token_path(cx, sp, \"Lifetime\"),\n                                 vec!(mk_ident(cx, sp, ident)));\n         }\n \n-        DOC_COMMENT(ident) => {\n+        token::DocComment(ident) => {\n             return cx.expr_call(sp,\n-                                mk_token_path(cx, sp, \"DOC_COMMENT\"),\n+                                mk_token_path(cx, sp, \"DocComment\"),\n                                 vec!(mk_name(cx, sp, ident.ident())));\n         }\n \n-        INTERPOLATED(_) => fail!(\"quote! with interpolated token\"),\n+        token::Interpolated(_) => fail!(\"quote! with interpolated token\"),\n \n         _ => ()\n     }\n \n     let name = match *tok {\n-        EQ => \"EQ\",\n-        LT => \"LT\",\n-        LE => \"LE\",\n-        EQEQ => \"EQEQ\",\n-        NE => \"NE\",\n-        GE => \"GE\",\n-        GT => \"GT\",\n-        ANDAND => \"ANDAND\",\n-        OROR => \"OROR\",\n-        NOT => \"NOT\",\n-        TILDE => \"TILDE\",\n-        AT => \"AT\",\n-        DOT => \"DOT\",\n-        DOTDOT => \"DOTDOT\",\n-        COMMA => \"COMMA\",\n-        SEMI => \"SEMI\",\n-        COLON => \"COLON\",\n-        MOD_SEP => \"MOD_SEP\",\n-        RARROW => \"RARROW\",\n-        LARROW => \"LARROW\",\n-        FAT_ARROW => \"FAT_ARROW\",\n-        LPAREN => \"LPAREN\",\n-        RPAREN => \"RPAREN\",\n-        LBRACKET => \"LBRACKET\",\n-        RBRACKET => \"RBRACKET\",\n-        LBRACE => \"LBRACE\",\n-        RBRACE => \"RBRACE\",\n-        POUND => \"POUND\",\n-        DOLLAR => \"DOLLAR\",\n-        UNDERSCORE => \"UNDERSCORE\",\n-        EOF => \"EOF\",\n-        _ => fail!()\n+        token::Eq           => \"Eq\",\n+        token::Lt           => \"Lt\",\n+        token::Le           => \"Le\",\n+        token::EqEq         => \"EqEq\",\n+        token::Ne           => \"Ne\",\n+        token::Ge           => \"Ge\",\n+        token::Gt           => \"Gt\",\n+        token::AndAnd       => \"AndAnd\",\n+        token::OrOr         => \"OrOr\",\n+        token::Not          => \"Not\",\n+        token::Tilde        => \"Tilde\",\n+        token::At           => \"At\",\n+        token::Dot          => \"Dot\",\n+        token::DotDot       => \"DotDot\",\n+        token::Comma        => \"Comma\",\n+        token::Semi         => \"Semi\",\n+        token::Colon        => \"Colon\",\n+        token::ModSep       => \"ModSep\",\n+        token::RArrow       => \"RArrow\",\n+        token::LArrow       => \"LArrow\",\n+        token::FatArrow     => \"FatArrow\",\n+        token::LParen       => \"LParen\",\n+        token::RParen       => \"RParen\",\n+        token::LBracket     => \"LBracket\",\n+        token::RBracket     => \"RBracket\",\n+        token::LBrace       => \"LBrace\",\n+        token::RBrace       => \"RBrace\",\n+        token::Pound        => \"Pound\",\n+        token::Dollar       => \"Dollar\",\n+        token::Underscore   => \"Underscore\",\n+        token::Eof          => \"Eof\",\n+        _                   => fail!(),\n     };\n     mk_token_path(cx, sp, name)\n }\n@@ -702,7 +701,7 @@ fn expand_tts(cx: &ExtCtxt, sp: Span, tts: &[ast::TokenTree])\n     p.quote_depth += 1u;\n \n     let cx_expr = p.parse_expr();\n-    if !p.eat(&token::COMMA) {\n+    if !p.eat(&token::Comma) {\n         p.fatal(\"expected token `,`\");\n     }\n "}, {"sha": "6d30de96a3c37d76c8cbdab37388bac8d2d156d5", "filename": "src/libsyntax/ext/tt/macro_parser.rs", "status": "modified", "additions": 8, "deletions": 8, "changes": 16, "blob_url": "https://github.com/rust-lang/rust/blob/d8b1fa0ae0c27e54d3539190683c01e194d36fbd/src%2Flibsyntax%2Fext%2Ftt%2Fmacro_parser.rs", "raw_url": "https://github.com/rust-lang/rust/raw/d8b1fa0ae0c27e54d3539190683c01e194d36fbd/src%2Flibsyntax%2Fext%2Ftt%2Fmacro_parser.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fext%2Ftt%2Fmacro_parser.rs?ref=d8b1fa0ae0c27e54d3539190683c01e194d36fbd", "patch": "@@ -85,7 +85,7 @@ use parse::lexer::*; //resolve bug?\n use parse::ParseSess;\n use parse::attr::ParserAttr;\n use parse::parser::{LifetimeAndTypesWithoutColons, Parser};\n-use parse::token::{Token, EOF, Nonterminal};\n+use parse::token::{Token, Nonterminal};\n use parse::token;\n use ptr::P;\n \n@@ -226,8 +226,8 @@ pub fn parse_or_else(sess: &ParseSess,\n /// unhygienic comparison)\n pub fn token_name_eq(t1 : &Token, t2 : &Token) -> bool {\n     match (t1,t2) {\n-        (&token::IDENT(id1,_),&token::IDENT(id2,_))\n-        | (&token::LIFETIME(id1),&token::LIFETIME(id2)) =>\n+        (&token::Ident(id1,_),&token::Ident(id2,_))\n+        | (&token::Lifetime(id1),&token::Lifetime(id2)) =>\n             id1.name == id2.name,\n         _ => *t1 == *t2\n     }\n@@ -354,9 +354,9 @@ pub fn parse(sess: &ParseSess,\n                     // Built-in nonterminals never start with these tokens,\n                     // so we can eliminate them from consideration.\n                     match tok {\n-                        token::RPAREN |\n-                        token::RBRACE |\n-                        token::RBRACKET => {},\n+                        token::RParen |\n+                        token::RBrace |\n+                        token::RBracket => {},\n                         _ => bb_eis.push(ei)\n                     }\n                   }\n@@ -372,7 +372,7 @@ pub fn parse(sess: &ParseSess,\n         }\n \n         /* error messages here could be improved with links to orig. rules */\n-        if token_name_eq(&tok, &EOF) {\n+        if token_name_eq(&tok, &token::Eof) {\n             if eof_eis.len() == 1u {\n                 let mut v = Vec::new();\n                 for dv in eof_eis.get_mut(0).matches.iter_mut() {\n@@ -447,7 +447,7 @@ pub fn parse_nt(p: &mut Parser, name: &str) -> Nonterminal {\n       \"ty\" => token::NtTy(p.parse_ty(false /* no need to disambiguate*/)),\n       // this could be handled like a token, since it is one\n       \"ident\" => match p.token {\n-        token::IDENT(sn,b) => { p.bump(); token::NtIdent(box sn,b) }\n+        token::Ident(sn,b) => { p.bump(); token::NtIdent(box sn,b) }\n         _ => {\n             let token_str = token::to_string(&p.token);\n             p.fatal((format!(\"expected ident, found {}\","}, {"sha": "20428e50c7f1f50c9c6fa9b79426890e00e3d518", "filename": "src/libsyntax/ext/tt/macro_rules.rs", "status": "modified", "additions": 9, "deletions": 8, "changes": 17, "blob_url": "https://github.com/rust-lang/rust/blob/d8b1fa0ae0c27e54d3539190683c01e194d36fbd/src%2Flibsyntax%2Fext%2Ftt%2Fmacro_rules.rs", "raw_url": "https://github.com/rust-lang/rust/raw/d8b1fa0ae0c27e54d3539190683c01e194d36fbd/src%2Flibsyntax%2Fext%2Ftt%2Fmacro_rules.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fext%2Ftt%2Fmacro_rules.rs?ref=d8b1fa0ae0c27e54d3539190683c01e194d36fbd", "patch": "@@ -20,7 +20,7 @@ use parse::lexer::new_tt_reader;\n use parse::parser::Parser;\n use parse::attr::ParserAttr;\n use parse::token::{special_idents, gensym_ident};\n-use parse::token::{FAT_ARROW, SEMI, NtMatchers, NtTT, EOF};\n+use parse::token::{NtMatchers, NtTT};\n use parse::token;\n use print;\n use ptr::P;\n@@ -43,10 +43,10 @@ impl<'a> ParserAnyMacro<'a> {\n     /// allowed to be there.\n     fn ensure_complete_parse(&self, allow_semi: bool) {\n         let mut parser = self.parser.borrow_mut();\n-        if allow_semi && parser.token == SEMI {\n+        if allow_semi && parser.token == token::Semi {\n             parser.bump()\n         }\n-        if parser.token != EOF {\n+        if parser.token != token::Eof {\n             let token_str = parser.this_token_to_string();\n             let msg = format!(\"macro expansion ignores token `{}` and any \\\n                                following\",\n@@ -89,7 +89,7 @@ impl<'a> MacResult for ParserAnyMacro<'a> {\n         loop {\n             let mut parser = self.parser.borrow_mut();\n             match parser.token {\n-                EOF => break,\n+                token::Eof => break,\n                 _ => {\n                     let attrs = parser.parse_outer_attributes();\n                     ret.push(parser.parse_method(attrs, ast::Inherited))\n@@ -231,12 +231,13 @@ pub fn add_new_extension<'cx>(cx: &'cx mut ExtCtxt,\n     let argument_gram = vec!(\n         ms(MatchSeq(vec!(\n             ms(MatchNonterminal(lhs_nm, special_idents::matchers, 0u)),\n-            ms(MatchTok(FAT_ARROW)),\n-            ms(MatchNonterminal(rhs_nm, special_idents::tt, 1u))), Some(SEMI),\n-                                ast::OneOrMore, 0u, 2u)),\n+            ms(MatchTok(token::FatArrow)),\n+            ms(MatchNonterminal(rhs_nm, special_idents::tt, 1u))),\n+                                Some(token::Semi), ast::OneOrMore, 0u, 2u)),\n         //to phase into semicolon-termination instead of\n         //semicolon-separation\n-        ms(MatchSeq(vec!(ms(MatchTok(SEMI))), None, ast::ZeroOrMore, 2u, 2u)));\n+        ms(MatchSeq(vec!(ms(MatchTok(token::Semi))), None,\n+                            ast::ZeroOrMore, 2u, 2u)));\n \n \n     // Parse the macro_rules! invocation (`none` is for no interpolations):"}, {"sha": "2c7b583d46021c1f1967d95600562539c0286b6b", "filename": "src/libsyntax/ext/tt/transcribe.rs", "status": "modified", "additions": 6, "deletions": 6, "changes": 12, "blob_url": "https://github.com/rust-lang/rust/blob/d8b1fa0ae0c27e54d3539190683c01e194d36fbd/src%2Flibsyntax%2Fext%2Ftt%2Ftranscribe.rs", "raw_url": "https://github.com/rust-lang/rust/raw/d8b1fa0ae0c27e54d3539190683c01e194d36fbd/src%2Flibsyntax%2Fext%2Ftt%2Ftranscribe.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fext%2Ftt%2Ftranscribe.rs?ref=d8b1fa0ae0c27e54d3539190683c01e194d36fbd", "patch": "@@ -13,7 +13,7 @@ use ast::{TokenTree, TtDelimited, TtToken, TtSequence, TtNonterminal, Ident};\n use codemap::{Span, DUMMY_SP};\n use diagnostic::SpanHandler;\n use ext::tt::macro_parser::{NamedMatch, MatchedSeq, MatchedNonterminal};\n-use parse::token::{EOF, INTERPOLATED, IDENT, Token, NtIdent};\n+use parse::token::{Token, NtIdent};\n use parse::token;\n use parse::lexer::TokenAndSpan;\n \n@@ -66,7 +66,7 @@ pub fn new_tt_reader<'a>(sp_diag: &'a SpanHandler,\n         repeat_idx: Vec::new(),\n         repeat_len: Vec::new(),\n         /* dummy values, never read: */\n-        cur_tok: EOF,\n+        cur_tok: token::Eof,\n         cur_span: DUMMY_SP,\n     };\n     tt_next_token(&mut r); /* get cur_tok and cur_span set up */\n@@ -158,7 +158,7 @@ pub fn tt_next_token(r: &mut TtReader) -> TokenAndSpan {\n     loop {\n         let should_pop = match r.stack.last() {\n             None => {\n-                assert_eq!(ret_val.tok, EOF);\n+                assert_eq!(ret_val.tok, token::Eof);\n                 return ret_val;\n             }\n             Some(frame) => {\n@@ -175,7 +175,7 @@ pub fn tt_next_token(r: &mut TtReader) -> TokenAndSpan {\n             let prev = r.stack.pop().unwrap();\n             match r.stack.last_mut() {\n                 None => {\n-                    r.cur_tok = EOF;\n+                    r.cur_tok = token::Eof;\n                     return ret_val;\n                 }\n                 Some(frame) => {\n@@ -272,13 +272,13 @@ pub fn tt_next_token(r: &mut TtReader) -> TokenAndSpan {\n                        (b) we actually can, since it's a token. */\n                     MatchedNonterminal(NtIdent(box sn, b)) => {\n                         r.cur_span = sp;\n-                        r.cur_tok = IDENT(sn,b);\n+                        r.cur_tok = token::Ident(sn,b);\n                         return ret_val;\n                     }\n                     MatchedNonterminal(ref other_whole_nt) => {\n                         // FIXME(pcwalton): Bad copy.\n                         r.cur_span = sp;\n-                        r.cur_tok = INTERPOLATED((*other_whole_nt).clone());\n+                        r.cur_tok = token::Interpolated((*other_whole_nt).clone());\n                         return ret_val;\n                     }\n                     MatchedSeq(..) => {"}, {"sha": "967ad3a897cdeac1a1b0576889f8fcf622b30b8a", "filename": "src/libsyntax/fold.rs", "status": "modified", "additions": 4, "deletions": 4, "changes": 8, "blob_url": "https://github.com/rust-lang/rust/blob/d8b1fa0ae0c27e54d3539190683c01e194d36fbd/src%2Flibsyntax%2Ffold.rs", "raw_url": "https://github.com/rust-lang/rust/raw/d8b1fa0ae0c27e54d3539190683c01e194d36fbd/src%2Flibsyntax%2Ffold.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Ffold.rs?ref=d8b1fa0ae0c27e54d3539190683c01e194d36fbd", "patch": "@@ -602,11 +602,11 @@ pub fn noop_fold_tts<T: Folder>(tts: &[TokenTree], fld: &mut T) -> Vec<TokenTree\n // apply ident folder if it's an ident, apply other folds to interpolated nodes\n pub fn noop_fold_token<T: Folder>(t: token::Token, fld: &mut T) -> token::Token {\n     match t {\n-        token::IDENT(id, followed_by_colons) => {\n-            token::IDENT(fld.fold_ident(id), followed_by_colons)\n+        token::Ident(id, followed_by_colons) => {\n+            token::Ident(fld.fold_ident(id), followed_by_colons)\n         }\n-        token::LIFETIME(id) => token::LIFETIME(fld.fold_ident(id)),\n-        token::INTERPOLATED(nt) => token::INTERPOLATED(fld.fold_interpolated(nt)),\n+        token::Lifetime(id) => token::Lifetime(fld.fold_ident(id)),\n+        token::Interpolated(nt) => token::Interpolated(fld.fold_interpolated(nt)),\n         _ => t\n     }\n }"}, {"sha": "458a5042a7e234df1828f29caa799ca3c6adbbca", "filename": "src/libsyntax/parse/attr.rs", "status": "modified", "additions": 16, "deletions": 17, "changes": 33, "blob_url": "https://github.com/rust-lang/rust/blob/d8b1fa0ae0c27e54d3539190683c01e194d36fbd/src%2Flibsyntax%2Fparse%2Fattr.rs", "raw_url": "https://github.com/rust-lang/rust/raw/d8b1fa0ae0c27e54d3539190683c01e194d36fbd/src%2Flibsyntax%2Fparse%2Fattr.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Fattr.rs?ref=d8b1fa0ae0c27e54d3539190683c01e194d36fbd", "patch": "@@ -14,7 +14,6 @@ use codemap::{spanned, Spanned, mk_sp, Span};\n use parse::common::*; //resolve bug?\n use parse::token;\n use parse::parser::Parser;\n-use parse::token::INTERPOLATED;\n use ptr::P;\n \n /// A parser that can parse attributes.\n@@ -36,10 +35,10 @@ impl<'a> ParserAttr for Parser<'a> {\n             debug!(\"parse_outer_attributes: self.token={}\",\n                    self.token);\n             match self.token {\n-              token::POUND => {\n+              token::Pound => {\n                 attrs.push(self.parse_attribute(false));\n               }\n-              token::DOC_COMMENT(s) => {\n+              token::DocComment(s) => {\n                 let attr = ::attr::mk_sugared_doc_attr(\n                     attr::mk_attr_id(),\n                     self.id_to_interned_str(s.ident()),\n@@ -66,11 +65,11 @@ impl<'a> ParserAttr for Parser<'a> {\n         debug!(\"parse_attributes: permit_inner={} self.token={}\",\n                permit_inner, self.token);\n         let (span, value, mut style) = match self.token {\n-            token::POUND => {\n+            token::Pound => {\n                 let lo = self.span.lo;\n                 self.bump();\n \n-                let style = if self.eat(&token::NOT) {\n+                let style = if self.eat(&token::Not) {\n                     if !permit_inner {\n                         let span = self.span;\n                         self.span_err(span,\n@@ -82,10 +81,10 @@ impl<'a> ParserAttr for Parser<'a> {\n                     ast::AttrOuter\n                 };\n \n-                self.expect(&token::LBRACKET);\n+                self.expect(&token::LBracket);\n                 let meta_item = self.parse_meta_item();\n                 let hi = self.span.hi;\n-                self.expect(&token::RBRACKET);\n+                self.expect(&token::RBracket);\n \n                 (mk_sp(lo, hi), meta_item, style)\n             }\n@@ -96,7 +95,7 @@ impl<'a> ParserAttr for Parser<'a> {\n             }\n         };\n \n-        if permit_inner && self.eat(&token::SEMI) {\n+        if permit_inner && self.eat(&token::Semi) {\n             self.span_warn(span, \"this inner attribute syntax is deprecated. \\\n                            The new syntax is `#![foo]`, with a bang and no semicolon.\");\n             style = ast::AttrInner;\n@@ -130,10 +129,10 @@ impl<'a> ParserAttr for Parser<'a> {\n         let mut next_outer_attrs: Vec<ast::Attribute> = Vec::new();\n         loop {\n             let attr = match self.token {\n-                token::POUND => {\n+                token::Pound => {\n                     self.parse_attribute(true)\n                 }\n-                token::DOC_COMMENT(s) => {\n+                token::DocComment(s) => {\n                     // we need to get the position of this token before we bump.\n                     let Span { lo, hi, .. } = self.span;\n                     self.bump();\n@@ -161,7 +160,7 @@ impl<'a> ParserAttr for Parser<'a> {\n     /// | IDENT meta_seq\n     fn parse_meta_item(&mut self) -> P<ast::MetaItem> {\n         let nt_meta = match self.token {\n-            token::INTERPOLATED(token::NtMeta(ref e)) => {\n+            token::Interpolated(token::NtMeta(ref e)) => {\n                 Some(e.clone())\n             }\n             _ => None\n@@ -179,7 +178,7 @@ impl<'a> ParserAttr for Parser<'a> {\n         let ident = self.parse_ident();\n         let name = self.id_to_interned_str(ident);\n         match self.token {\n-            token::EQ => {\n+            token::Eq => {\n                 self.bump();\n                 let lit = self.parse_lit();\n                 // FIXME #623 Non-string meta items are not serialized correctly;\n@@ -195,7 +194,7 @@ impl<'a> ParserAttr for Parser<'a> {\n                 let hi = self.span.hi;\n                 P(spanned(lo, hi, ast::MetaNameValue(name, lit)))\n             }\n-            token::LPAREN => {\n+            token::LParen => {\n                 let inner_items = self.parse_meta_seq();\n                 let hi = self.span.hi;\n                 P(spanned(lo, hi, ast::MetaList(name, inner_items)))\n@@ -209,15 +208,15 @@ impl<'a> ParserAttr for Parser<'a> {\n \n     /// matches meta_seq = ( COMMASEP(meta_item) )\n     fn parse_meta_seq(&mut self) -> Vec<P<ast::MetaItem>> {\n-        self.parse_seq(&token::LPAREN,\n-                       &token::RPAREN,\n-                       seq_sep_trailing_disallowed(token::COMMA),\n+        self.parse_seq(&token::LParen,\n+                       &token::RParen,\n+                       seq_sep_trailing_disallowed(token::Comma),\n                        |p| p.parse_meta_item()).node\n     }\n \n     fn parse_optional_meta(&mut self) -> Vec<P<ast::MetaItem>> {\n         match self.token {\n-            token::LPAREN => self.parse_meta_seq(),\n+            token::LParen => self.parse_meta_seq(),\n             _ => Vec::new()\n         }\n     }"}, {"sha": "4226c3ce3a4f9c4796ed67d41844f6487076234c", "filename": "src/libsyntax/parse/lexer/mod.rs", "status": "modified", "additions": 94, "deletions": 94, "changes": 188, "blob_url": "https://github.com/rust-lang/rust/blob/d8b1fa0ae0c27e54d3539190683c01e194d36fbd/src%2Flibsyntax%2Fparse%2Flexer%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/d8b1fa0ae0c27e54d3539190683c01e194d36fbd/src%2Flibsyntax%2Fparse%2Flexer%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Flexer%2Fmod.rs?ref=d8b1fa0ae0c27e54d3539190683c01e194d36fbd", "patch": "@@ -69,7 +69,7 @@ impl<'a> Reader for StringReader<'a> {\n     /// Return the next token. EFFECT: advances the string_reader.\n     fn next_token(&mut self) -> TokenAndSpan {\n         let ret_val = TokenAndSpan {\n-            tok: replace(&mut self.peek_tok, token::UNDERSCORE),\n+            tok: replace(&mut self.peek_tok, token::Underscore),\n             sp: self.peek_span,\n         };\n         self.advance_token();\n@@ -92,7 +92,7 @@ impl<'a> Reader for StringReader<'a> {\n \n impl<'a> Reader for TtReader<'a> {\n     fn is_eof(&self) -> bool {\n-        self.cur_tok == token::EOF\n+        self.cur_tok == token::Eof\n     }\n     fn next_token(&mut self) -> TokenAndSpan {\n         let r = tt_next_token(self);\n@@ -136,7 +136,7 @@ impl<'a> StringReader<'a> {\n             curr: Some('\\n'),\n             filemap: filemap,\n             /* dummy values; not read */\n-            peek_tok: token::EOF,\n+            peek_tok: token::Eof,\n             peek_span: codemap::DUMMY_SP,\n             read_embedded_ident: false,\n         };\n@@ -213,7 +213,7 @@ impl<'a> StringReader<'a> {\n             },\n             None => {\n                 if self.is_eof() {\n-                    self.peek_tok = token::EOF;\n+                    self.peek_tok = token::Eof;\n                 } else {\n                     let start_bytepos = self.last_pos;\n                     self.peek_tok = self.next_token_inner();\n@@ -396,9 +396,9 @@ impl<'a> StringReader<'a> {\n                         return self.with_str_from(start_bpos, |string| {\n                             // but comments with only more \"/\"s are not\n                             let tok = if is_doc_comment(string) {\n-                                token::DOC_COMMENT(token::intern(string))\n+                                token::DocComment(token::intern(string))\n                             } else {\n-                                token::COMMENT\n+                                token::Comment\n                             };\n \n                             return Some(TokenAndSpan{\n@@ -410,7 +410,7 @@ impl<'a> StringReader<'a> {\n                         let start_bpos = self.last_pos - BytePos(2);\n                         while !self.curr_is('\\n') && !self.is_eof() { self.bump(); }\n                         return Some(TokenAndSpan {\n-                            tok: token::COMMENT,\n+                            tok: token::Comment,\n                             sp: codemap::mk_sp(start_bpos, self.last_pos)\n                         });\n                     }\n@@ -440,7 +440,7 @@ impl<'a> StringReader<'a> {\n                     let start = self.last_pos;\n                     while !self.curr_is('\\n') && !self.is_eof() { self.bump(); }\n                     return Some(TokenAndSpan {\n-                        tok: token::SHEBANG(self.name_from(start)),\n+                        tok: token::Shebang(self.name_from(start)),\n                         sp: codemap::mk_sp(start, self.last_pos)\n                     });\n                 }\n@@ -466,7 +466,7 @@ impl<'a> StringReader<'a> {\n                 let start_bpos = self.last_pos;\n                 while is_whitespace(self.curr) { self.bump(); }\n                 let c = Some(TokenAndSpan {\n-                    tok: token::WS,\n+                    tok: token::Whitespace,\n                     sp: codemap::mk_sp(start_bpos, self.last_pos)\n                 });\n                 debug!(\"scanning whitespace: {}\", c);\n@@ -519,9 +519,9 @@ impl<'a> StringReader<'a> {\n                     self.translate_crlf(start_bpos, string,\n                                         \"bare CR not allowed in block doc-comment\")\n                 } else { string.into_maybe_owned() };\n-                token::DOC_COMMENT(token::intern(string.as_slice()))\n+                token::DocComment(token::intern(string.as_slice()))\n             } else {\n-                token::COMMENT\n+                token::Comment\n             };\n \n             Some(TokenAndSpan{\n@@ -642,17 +642,17 @@ impl<'a> StringReader<'a> {\n                 }\n                 'u' | 'i' => {\n                     self.scan_int_suffix();\n-                    return token::LIT_INTEGER(self.name_from(start_bpos));\n+                    return token::LitInteger(self.name_from(start_bpos));\n                 },\n                 'f' => {\n                     let last_pos = self.last_pos;\n                     self.scan_float_suffix();\n                     self.check_float_base(start_bpos, last_pos, base);\n-                    return token::LIT_FLOAT(self.name_from(start_bpos));\n+                    return token::LitFloat(self.name_from(start_bpos));\n                 }\n                 _ => {\n                     // just a 0\n-                    return token::LIT_INTEGER(self.name_from(start_bpos));\n+                    return token::LitInteger(self.name_from(start_bpos));\n                 }\n             }\n         } else if c.is_digit_radix(10) {\n@@ -665,7 +665,7 @@ impl<'a> StringReader<'a> {\n             self.err_span_(start_bpos, self.last_pos, \"no valid digits found for number\");\n             // eat any suffix\n             self.scan_int_suffix();\n-            return token::LIT_INTEGER(token::intern(\"0\"));\n+            return token::LitInteger(token::intern(\"0\"));\n         }\n \n         // might be a float, but don't be greedy if this is actually an\n@@ -683,25 +683,25 @@ impl<'a> StringReader<'a> {\n             }\n             let last_pos = self.last_pos;\n             self.check_float_base(start_bpos, last_pos, base);\n-            return token::LIT_FLOAT(self.name_from(start_bpos));\n+            return token::LitFloat(self.name_from(start_bpos));\n         } else if self.curr_is('f') {\n             // or it might be an integer literal suffixed as a float\n             self.scan_float_suffix();\n             let last_pos = self.last_pos;\n             self.check_float_base(start_bpos, last_pos, base);\n-            return token::LIT_FLOAT(self.name_from(start_bpos));\n+            return token::LitFloat(self.name_from(start_bpos));\n         } else {\n             // it might be a float if it has an exponent\n             if self.curr_is('e') || self.curr_is('E') {\n                 self.scan_float_exponent();\n                 self.scan_float_suffix();\n                 let last_pos = self.last_pos;\n                 self.check_float_base(start_bpos, last_pos, base);\n-                return token::LIT_FLOAT(self.name_from(start_bpos));\n+                return token::LitFloat(self.name_from(start_bpos));\n             }\n             // but we certainly have an integer!\n             self.scan_int_suffix();\n-            return token::LIT_INTEGER(self.name_from(start_bpos));\n+            return token::LitInteger(self.name_from(start_bpos));\n         }\n     }\n \n@@ -889,13 +889,13 @@ impl<'a> StringReader<'a> {\n         }\n     }\n \n-    fn binop(&mut self, op: token::BinOp) -> token::Token {\n+    fn binop(&mut self, op: token::BinOpToken) -> token::Token {\n         self.bump();\n         if self.curr_is('=') {\n             self.bump();\n-            return token::BINOPEQ(op);\n+            return token::BinOpEq(op);\n         } else {\n-            return token::BINOP(op);\n+            return token::BinOp(op);\n         }\n     }\n \n@@ -919,12 +919,12 @@ impl<'a> StringReader<'a> {\n \n             return self.with_str_from(start, |string| {\n                 if string == \"_\" {\n-                    token::UNDERSCORE\n+                    token::Underscore\n                 } else {\n                     let is_mod_name = self.curr_is(':') && self.nextch_is(':');\n \n                     // FIXME: perform NFKC normalization here. (Issue #2253)\n-                    token::IDENT(str_to_ident(string), is_mod_name)\n+                    token::Ident(str_to_ident(string), is_mod_name)\n                 }\n             })\n         }\n@@ -938,92 +938,92 @@ impl<'a> StringReader<'a> {\n                 ('\\x00', Some('n'), Some('a')) => {\n                     let ast_ident = self.scan_embedded_hygienic_ident();\n                     let is_mod_name = self.curr_is(':') && self.nextch_is(':');\n-                    return token::IDENT(ast_ident, is_mod_name);\n+                    return token::Ident(ast_ident, is_mod_name);\n                 }\n                 _ => {}\n             }\n         }\n \n         match c.expect(\"next_token_inner called at EOF\") {\n           // One-byte tokens.\n-          ';' => { self.bump(); return token::SEMI; }\n-          ',' => { self.bump(); return token::COMMA; }\n+          ';' => { self.bump(); return token::Semi; }\n+          ',' => { self.bump(); return token::Comma; }\n           '.' => {\n               self.bump();\n               return if self.curr_is('.') {\n                   self.bump();\n                   if self.curr_is('.') {\n                       self.bump();\n-                      token::DOTDOTDOT\n+                      token::DotDotDot\n                   } else {\n-                      token::DOTDOT\n+                      token::DotDot\n                   }\n               } else {\n-                  token::DOT\n+                  token::Dot\n               };\n           }\n-          '(' => { self.bump(); return token::LPAREN; }\n-          ')' => { self.bump(); return token::RPAREN; }\n-          '{' => { self.bump(); return token::LBRACE; }\n-          '}' => { self.bump(); return token::RBRACE; }\n-          '[' => { self.bump(); return token::LBRACKET; }\n-          ']' => { self.bump(); return token::RBRACKET; }\n-          '@' => { self.bump(); return token::AT; }\n-          '#' => { self.bump(); return token::POUND; }\n-          '~' => { self.bump(); return token::TILDE; }\n-          '?' => { self.bump(); return token::QUESTION; }\n+          '(' => { self.bump(); return token::LParen; }\n+          ')' => { self.bump(); return token::RParen; }\n+          '{' => { self.bump(); return token::LBrace; }\n+          '}' => { self.bump(); return token::RBrace; }\n+          '[' => { self.bump(); return token::LBracket; }\n+          ']' => { self.bump(); return token::RBracket; }\n+          '@' => { self.bump(); return token::At; }\n+          '#' => { self.bump(); return token::Pound; }\n+          '~' => { self.bump(); return token::Tilde; }\n+          '?' => { self.bump(); return token::Question; }\n           ':' => {\n             self.bump();\n             if self.curr_is(':') {\n                 self.bump();\n-                return token::MOD_SEP;\n+                return token::ModSep;\n             } else {\n-                return token::COLON;\n+                return token::Colon;\n             }\n           }\n \n-          '$' => { self.bump(); return token::DOLLAR; }\n+          '$' => { self.bump(); return token::Dollar; }\n \n           // Multi-byte tokens.\n           '=' => {\n             self.bump();\n             if self.curr_is('=') {\n                 self.bump();\n-                return token::EQEQ;\n+                return token::EqEq;\n             } else if self.curr_is('>') {\n                 self.bump();\n-                return token::FAT_ARROW;\n+                return token::FatArrow;\n             } else {\n-                return token::EQ;\n+                return token::Eq;\n             }\n           }\n           '!' => {\n             self.bump();\n             if self.curr_is('=') {\n                 self.bump();\n-                return token::NE;\n-            } else { return token::NOT; }\n+                return token::Ne;\n+            } else { return token::Not; }\n           }\n           '<' => {\n             self.bump();\n             match self.curr.unwrap_or('\\x00') {\n-              '=' => { self.bump(); return token::LE; }\n-              '<' => { return self.binop(token::SHL); }\n+              '=' => { self.bump(); return token::Le; }\n+              '<' => { return self.binop(token::Shl); }\n               '-' => {\n                 self.bump();\n                 match self.curr.unwrap_or('\\x00') {\n-                  _ => { return token::LARROW; }\n+                  _ => { return token::LArrow; }\n                 }\n               }\n-              _ => { return token::LT; }\n+              _ => { return token::Lt; }\n             }\n           }\n           '>' => {\n             self.bump();\n             match self.curr.unwrap_or('\\x00') {\n-              '=' => { self.bump(); return token::GE; }\n-              '>' => { return self.binop(token::SHR); }\n-              _ => { return token::GT; }\n+              '=' => { self.bump(); return token::Ge; }\n+              '>' => { return self.binop(token::Shr); }\n+              _ => { return token::Gt; }\n             }\n           }\n           '\\'' => {\n@@ -1056,7 +1056,7 @@ impl<'a> StringReader<'a> {\n                         str_to_ident(lifetime_name)\n                     });\n                 let keyword_checking_token =\n-                    &token::IDENT(keyword_checking_ident, false);\n+                    &token::Ident(keyword_checking_ident, false);\n                 let last_bpos = self.last_pos;\n                 if token::is_keyword(token::keywords::Self,\n                                      keyword_checking_token) {\n@@ -1071,7 +1071,7 @@ impl<'a> StringReader<'a> {\n                                    last_bpos,\n                                    \"invalid lifetime name\");\n                 }\n-                return token::LIFETIME(ident);\n+                return token::Lifetime(ident);\n             }\n \n             // Otherwise it is a character constant:\n@@ -1087,15 +1087,15 @@ impl<'a> StringReader<'a> {\n             }\n             let id = if valid { self.name_from(start) } else { token::intern(\"0\") };\n             self.bump(); // advance curr past token\n-            return token::LIT_CHAR(id);\n+            return token::LitChar(id);\n           }\n           'b' => {\n             self.bump();\n             return match self.curr {\n                 Some('\\'') => self.scan_byte(),\n                 Some('\"') => self.scan_byte_string(),\n                 Some('r') => self.scan_raw_byte_string(),\n-                _ => unreachable!()  // Should have been a token::IDENT above.\n+                _ => unreachable!()  // Should have been a token::Ident above.\n             };\n \n           }\n@@ -1118,7 +1118,7 @@ impl<'a> StringReader<'a> {\n             let id = if valid { self.name_from(start_bpos + BytePos(1)) }\n                      else { token::intern(\"??\") };\n             self.bump();\n-            return token::LIT_STR(id);\n+            return token::LitStr(id);\n           }\n           'r' => {\n             let start_bpos = self.last_pos;\n@@ -1185,33 +1185,33 @@ impl<'a> StringReader<'a> {\n             } else {\n                 token::intern(\"??\")\n             };\n-            return token::LIT_STR_RAW(id, hash_count);\n+            return token::LitStrRaw(id, hash_count);\n           }\n           '-' => {\n             if self.nextch_is('>') {\n                 self.bump();\n                 self.bump();\n-                return token::RARROW;\n-            } else { return self.binop(token::MINUS); }\n+                return token::RArrow;\n+            } else { return self.binop(token::Minus); }\n           }\n           '&' => {\n             if self.nextch_is('&') {\n                 self.bump();\n                 self.bump();\n-                return token::ANDAND;\n-            } else { return self.binop(token::AND); }\n+                return token::AndAnd;\n+            } else { return self.binop(token::And); }\n           }\n           '|' => {\n             match self.nextch() {\n-              Some('|') => { self.bump(); self.bump(); return token::OROR; }\n-              _ => { return self.binop(token::OR); }\n+              Some('|') => { self.bump(); self.bump(); return token::OrOr; }\n+              _ => { return self.binop(token::Or); }\n             }\n           }\n-          '+' => { return self.binop(token::PLUS); }\n-          '*' => { return self.binop(token::STAR); }\n-          '/' => { return self.binop(token::SLASH); }\n-          '^' => { return self.binop(token::CARET); }\n-          '%' => { return self.binop(token::PERCENT); }\n+          '+' => { return self.binop(token::Plus); }\n+          '*' => { return self.binop(token::Star); }\n+          '/' => { return self.binop(token::Slash); }\n+          '^' => { return self.binop(token::Caret); }\n+          '%' => { return self.binop(token::Percent); }\n           c => {\n               let last_bpos = self.last_pos;\n               let bpos = self.pos;\n@@ -1275,7 +1275,7 @@ impl<'a> StringReader<'a> {\n \n         let id = if valid { self.name_from(start) } else { token::intern(\"??\") };\n         self.bump(); // advance curr past token\n-        return token::LIT_BYTE(id);\n+        return token::LitByte(id);\n     }\n \n     fn scan_byte_string(&mut self) -> token::Token {\n@@ -1297,7 +1297,7 @@ impl<'a> StringReader<'a> {\n         }\n         let id = if valid { self.name_from(start) } else { token::intern(\"??\") };\n         self.bump();\n-        return token::LIT_BINARY(id);\n+        return token::LitBinary(id);\n     }\n \n     fn scan_raw_byte_string(&mut self) -> token::Token {\n@@ -1348,7 +1348,7 @@ impl<'a> StringReader<'a> {\n             self.bump();\n         }\n         self.bump();\n-        return token::LIT_BINARY_RAW(self.name_from_to(content_start_bpos, content_end_bpos),\n+        return token::LitBinaryRaw(self.name_from_to(content_start_bpos, content_end_bpos),\n                                      hash_count);\n     }\n }\n@@ -1431,20 +1431,20 @@ mod test {\n             \"/* my source file */ \\\n              fn main() { println!(\\\"zebra\\\"); }\\n\".to_string());\n         let id = str_to_ident(\"fn\");\n-        assert_eq!(string_reader.next_token().tok, token::COMMENT);\n-        assert_eq!(string_reader.next_token().tok, token::WS);\n+        assert_eq!(string_reader.next_token().tok, token::Comment);\n+        assert_eq!(string_reader.next_token().tok, token::Whitespace);\n         let tok1 = string_reader.next_token();\n         let tok2 = TokenAndSpan{\n-            tok:token::IDENT(id, false),\n+            tok:token::Ident(id, false),\n             sp:Span {lo:BytePos(21),hi:BytePos(23),expn_id: NO_EXPANSION}};\n         assert_eq!(tok1,tok2);\n-        assert_eq!(string_reader.next_token().tok, token::WS);\n+        assert_eq!(string_reader.next_token().tok, token::Whitespace);\n         // the 'main' id is already read:\n         assert_eq!(string_reader.last_pos.clone(), BytePos(28));\n         // read another token:\n         let tok3 = string_reader.next_token();\n         let tok4 = TokenAndSpan{\n-            tok:token::IDENT(str_to_ident(\"main\"), false),\n+            tok:token::Ident(str_to_ident(\"main\"), false),\n             sp:Span {lo:BytePos(24),hi:BytePos(28),expn_id: NO_EXPANSION}};\n         assert_eq!(tok3,tok4);\n         // the lparen is already read:\n@@ -1461,64 +1461,64 @@ mod test {\n \n     // make the identifier by looking up the string in the interner\n     fn mk_ident (id: &str, is_mod_name: bool) -> token::Token {\n-        token::IDENT (str_to_ident(id),is_mod_name)\n+        token::Ident (str_to_ident(id),is_mod_name)\n     }\n \n     #[test] fn doublecolonparsing () {\n         check_tokenization(setup(&mk_sh(), \"a b\".to_string()),\n                            vec!(mk_ident(\"a\",false),\n-                            token::WS,\n+                            token::Whitespace,\n                              mk_ident(\"b\",false)));\n     }\n \n     #[test] fn dcparsing_2 () {\n         check_tokenization(setup(&mk_sh(), \"a::b\".to_string()),\n                            vec!(mk_ident(\"a\",true),\n-                             token::MOD_SEP,\n+                             token::ModSep,\n                              mk_ident(\"b\",false)));\n     }\n \n     #[test] fn dcparsing_3 () {\n         check_tokenization(setup(&mk_sh(), \"a ::b\".to_string()),\n                            vec!(mk_ident(\"a\",false),\n-                             token::WS,\n-                             token::MOD_SEP,\n+                             token::Whitespace,\n+                             token::ModSep,\n                              mk_ident(\"b\",false)));\n     }\n \n     #[test] fn dcparsing_4 () {\n         check_tokenization(setup(&mk_sh(), \"a:: b\".to_string()),\n                            vec!(mk_ident(\"a\",true),\n-                             token::MOD_SEP,\n-                             token::WS,\n+                             token::ModSep,\n+                             token::Whitespace,\n                              mk_ident(\"b\",false)));\n     }\n \n     #[test] fn character_a() {\n         assert_eq!(setup(&mk_sh(), \"'a'\".to_string()).next_token().tok,\n-                   token::LIT_CHAR(token::intern(\"a\")));\n+                   token::LitChar(token::intern(\"a\")));\n     }\n \n     #[test] fn character_space() {\n         assert_eq!(setup(&mk_sh(), \"' '\".to_string()).next_token().tok,\n-                   token::LIT_CHAR(token::intern(\" \")));\n+                   token::LitChar(token::intern(\" \")));\n     }\n \n     #[test] fn character_escaped() {\n         assert_eq!(setup(&mk_sh(), \"'\\\\n'\".to_string()).next_token().tok,\n-                   token::LIT_CHAR(token::intern(\"\\\\n\")));\n+                   token::LitChar(token::intern(\"\\\\n\")));\n     }\n \n     #[test] fn lifetime_name() {\n         assert_eq!(setup(&mk_sh(), \"'abc\".to_string()).next_token().tok,\n-                   token::LIFETIME(token::str_to_ident(\"'abc\")));\n+                   token::Lifetime(token::str_to_ident(\"'abc\")));\n     }\n \n     #[test] fn raw_string() {\n         assert_eq!(setup(&mk_sh(),\n                          \"r###\\\"\\\"#a\\\\b\\x00c\\\"\\\"###\".to_string()).next_token()\n                                                                  .tok,\n-                   token::LIT_STR_RAW(token::intern(\"\\\"#a\\\\b\\x00c\\\"\"), 3));\n+                   token::LitStrRaw(token::intern(\"\\\"#a\\\\b\\x00c\\\"\"), 3));\n     }\n \n     #[test] fn line_doc_comments() {\n@@ -1531,10 +1531,10 @@ mod test {\n         let sh = mk_sh();\n         let mut lexer = setup(&sh, \"/* /* */ */'a'\".to_string());\n         match lexer.next_token().tok {\n-            token::COMMENT => { },\n+            token::Comment => { },\n             _ => fail!(\"expected a comment!\")\n         }\n-        assert_eq!(lexer.next_token().tok, token::LIT_CHAR(token::intern(\"a\")));\n+        assert_eq!(lexer.next_token().tok, token::LitChar(token::intern(\"a\")));\n     }\n \n }"}, {"sha": "6c0df39daebba7fa1a8f86540e42886aa975d86f", "filename": "src/libsyntax/parse/mod.rs", "status": "modified", "additions": 26, "deletions": 26, "changes": 52, "blob_url": "https://github.com/rust-lang/rust/blob/d8b1fa0ae0c27e54d3539190683c01e194d36fbd/src%2Flibsyntax%2Fparse%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/d8b1fa0ae0c27e54d3539190683c01e194d36fbd/src%2Flibsyntax%2Fparse%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Fmod.rs?ref=d8b1fa0ae0c27e54d3539190683c01e194d36fbd", "patch": "@@ -793,34 +793,34 @@ mod test {\n         let tts = string_to_tts(\"macro_rules! zip (($a)=>($a))\".to_string());\n         let tts: &[ast::TokenTree] = tts.as_slice();\n         match tts {\n-            [ast::TtToken(_, token::IDENT(name_macro_rules, false)),\n-             ast::TtToken(_, token::NOT),\n-             ast::TtToken(_, token::IDENT(name_zip, false)),\n+            [ast::TtToken(_, token::Ident(name_macro_rules, false)),\n+             ast::TtToken(_, token::Not),\n+             ast::TtToken(_, token::Ident(name_zip, false)),\n              ast::TtDelimited(_, ref macro_delimed)]\n             if name_macro_rules.as_str() == \"macro_rules\"\n             && name_zip.as_str() == \"zip\" => {\n                 let (ref macro_open, ref macro_tts, ref macro_close) = **macro_delimed;\n                 match (macro_open, macro_tts.as_slice(), macro_close) {\n-                    (&ast::Delimiter { token: token::LPAREN, .. },\n+                    (&ast::Delimiter { token: token::LParen, .. },\n                      [ast::TtDelimited(_, ref first_delimed),\n-                      ast::TtToken(_, token::FAT_ARROW),\n+                      ast::TtToken(_, token::FatArrow),\n                       ast::TtDelimited(_, ref second_delimed)],\n-                     &ast::Delimiter { token: token::RPAREN, .. }) => {\n+                     &ast::Delimiter { token: token::RParen, .. }) => {\n                         let (ref first_open, ref first_tts, ref first_close) = **first_delimed;\n                         match (first_open, first_tts.as_slice(), first_close) {\n-                            (&ast::Delimiter { token: token::LPAREN, .. },\n-                             [ast::TtToken(_, token::DOLLAR),\n-                              ast::TtToken(_, token::IDENT(name, false))],\n-                             &ast::Delimiter { token: token::RPAREN, .. })\n+                            (&ast::Delimiter { token: token::LParen, .. },\n+                             [ast::TtToken(_, token::Dollar),\n+                              ast::TtToken(_, token::Ident(name, false))],\n+                             &ast::Delimiter { token: token::RParen, .. })\n                             if name.as_str() == \"a\" => {},\n                             _ => fail!(\"value 3: {}\", **first_delimed),\n                         }\n                         let (ref second_open, ref second_tts, ref second_close) = **second_delimed;\n                         match (second_open, second_tts.as_slice(), second_close) {\n-                            (&ast::Delimiter { token: token::LPAREN, .. },\n-                             [ast::TtToken(_, token::DOLLAR),\n-                              ast::TtToken(_, token::IDENT(name, false))],\n-                             &ast::Delimiter { token: token::RPAREN, .. })\n+                            (&ast::Delimiter { token: token::LParen, .. },\n+                             [ast::TtToken(_, token::Dollar),\n+                              ast::TtToken(_, token::Ident(name, false))],\n+                             &ast::Delimiter { token: token::RParen, .. })\n                             if name.as_str() == \"a\" => {},\n                             _ => fail!(\"value 4: {}\", **second_delimed),\n                         }\n@@ -842,7 +842,7 @@ mod test {\n         \\\"fields\\\":[\\\n             null,\\\n             {\\\n-                \\\"variant\\\":\\\"IDENT\\\",\\\n+                \\\"variant\\\":\\\"Ident\\\",\\\n                 \\\"fields\\\":[\\\n                     \\\"fn\\\",\\\n                     false\\\n@@ -855,7 +855,7 @@ mod test {\n         \\\"fields\\\":[\\\n             null,\\\n             {\\\n-                \\\"variant\\\":\\\"IDENT\\\",\\\n+                \\\"variant\\\":\\\"Ident\\\",\\\n                 \\\"fields\\\":[\\\n                     \\\"a\\\",\\\n                     false\\\n@@ -870,15 +870,15 @@ mod test {\n             [\\\n                 {\\\n                     \\\"span\\\":null,\\\n-                    \\\"token\\\":\\\"LPAREN\\\"\\\n+                    \\\"token\\\":\\\"LParen\\\"\\\n                 },\\\n                 [\\\n                     {\\\n                         \\\"variant\\\":\\\"TtToken\\\",\\\n                         \\\"fields\\\":[\\\n                             null,\\\n                             {\\\n-                                \\\"variant\\\":\\\"IDENT\\\",\\\n+                                \\\"variant\\\":\\\"Ident\\\",\\\n                                 \\\"fields\\\":[\\\n                                     \\\"b\\\",\\\n                                     false\\\n@@ -890,15 +890,15 @@ mod test {\n                         \\\"variant\\\":\\\"TtToken\\\",\\\n                         \\\"fields\\\":[\\\n                             null,\\\n-                            \\\"COLON\\\"\\\n+                            \\\"Colon\\\"\\\n                         ]\\\n                     },\\\n                     {\\\n                         \\\"variant\\\":\\\"TtToken\\\",\\\n                         \\\"fields\\\":[\\\n                             null,\\\n                             {\\\n-                                \\\"variant\\\":\\\"IDENT\\\",\\\n+                                \\\"variant\\\":\\\"Ident\\\",\\\n                                 \\\"fields\\\":[\\\n                                     \\\"int\\\",\\\n                                     false\\\n@@ -909,7 +909,7 @@ mod test {\n                 ],\\\n                 {\\\n                     \\\"span\\\":null,\\\n-                    \\\"token\\\":\\\"RPAREN\\\"\\\n+                    \\\"token\\\":\\\"RParen\\\"\\\n                 }\\\n             ]\\\n         ]\\\n@@ -921,15 +921,15 @@ mod test {\n             [\\\n                 {\\\n                     \\\"span\\\":null,\\\n-                    \\\"token\\\":\\\"LBRACE\\\"\\\n+                    \\\"token\\\":\\\"LBrace\\\"\\\n                 },\\\n                 [\\\n                     {\\\n                         \\\"variant\\\":\\\"TtToken\\\",\\\n                         \\\"fields\\\":[\\\n                             null,\\\n                             {\\\n-                                \\\"variant\\\":\\\"IDENT\\\",\\\n+                                \\\"variant\\\":\\\"Ident\\\",\\\n                                 \\\"fields\\\":[\\\n                                     \\\"b\\\",\\\n                                     false\\\n@@ -941,13 +941,13 @@ mod test {\n                         \\\"variant\\\":\\\"TtToken\\\",\\\n                         \\\"fields\\\":[\\\n                             null,\\\n-                            \\\"SEMI\\\"\\\n+                            \\\"Semi\\\"\\\n                         ]\\\n                     }\\\n                 ],\\\n                 {\\\n                     \\\"span\\\":null,\\\n-                    \\\"token\\\":\\\"RBRACE\\\"\\\n+                    \\\"token\\\":\\\"RBrace\\\"\\\n                 }\\\n             ]\\\n         ]\\\n@@ -1002,7 +1002,7 @@ mod test {\n     }\n \n     fn parser_done(p: Parser){\n-        assert_eq!(p.token.clone(), token::EOF);\n+        assert_eq!(p.token.clone(), token::Eof);\n     }\n \n     #[test] fn parse_ident_pat () {"}, {"sha": "73787763c8b58a7e9f8a1e153c60dbccf25de7e7", "filename": "src/libsyntax/parse/obsolete.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/d8b1fa0ae0c27e54d3539190683c01e194d36fbd/src%2Flibsyntax%2Fparse%2Fobsolete.rs", "raw_url": "https://github.com/rust-lang/rust/raw/d8b1fa0ae0c27e54d3539190683c01e194d36fbd/src%2Flibsyntax%2Fparse%2Fobsolete.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Fobsolete.rs?ref=d8b1fa0ae0c27e54d3539190683c01e194d36fbd", "patch": "@@ -118,7 +118,7 @@ impl<'a> ParserObsoleteMethods for parser::Parser<'a> {\n \n     fn is_obsolete_ident(&mut self, ident: &str) -> bool {\n         match self.token {\n-            token::IDENT(sid, _) => {\n+            token::Ident(sid, _) => {\n                 token::get_ident(sid).equiv(&ident)\n             }\n             _ => false"}, {"sha": "bd977962e91a1e464ecfb2f7df6b93900edd42b6", "filename": "src/libsyntax/parse/parser.rs", "status": "modified", "additions": 426, "deletions": 427, "changes": 853, "blob_url": "https://github.com/rust-lang/rust/blob/d8b1fa0ae0c27e54d3539190683c01e194d36fbd/src%2Flibsyntax%2Fparse%2Fparser.rs", "raw_url": "https://github.com/rust-lang/rust/raw/d8b1fa0ae0c27e54d3539190683c01e194d36fbd/src%2Flibsyntax%2Fparse%2Fparser.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Fparser.rs?ref=d8b1fa0ae0c27e54d3539190683c01e194d36fbd"}, {"sha": "0cc56b2ab2b320dcf61eaeaa6107aedd05b683d9", "filename": "src/libsyntax/parse/token.rs", "status": "modified", "additions": 322, "deletions": 278, "changes": 600, "blob_url": "https://github.com/rust-lang/rust/blob/d8b1fa0ae0c27e54d3539190683c01e194d36fbd/src%2Flibsyntax%2Fparse%2Ftoken.rs", "raw_url": "https://github.com/rust-lang/rust/raw/d8b1fa0ae0c27e54d3539190683c01e194d36fbd/src%2Flibsyntax%2Fparse%2Ftoken.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Ftoken.rs?ref=d8b1fa0ae0c27e54d3539190683c01e194d36fbd", "patch": "@@ -9,9 +9,7 @@\n // except according to those terms.\n \n use ast;\n-use ast::{Ident, Name, Mrk};\n use ext::mtwt;\n-use parse::token;\n use ptr::P;\n use util::interner::{RcStr, StrInterner};\n use util::interner;\n@@ -22,94 +20,157 @@ use std::mem;\n use std::path::BytesContainer;\n use std::rc::Rc;\n \n+// NOTE(stage0): remove these re-exports after the next snapshot\n+// (needed to allow quotations to pass stage0)\n+#[cfg(stage0)] pub use self::Plus           as PLUS;\n+#[cfg(stage0)] pub use self::Minus          as MINUS;\n+#[cfg(stage0)] pub use self::Star           as STAR;\n+#[cfg(stage0)] pub use self::Slash          as SLASH;\n+#[cfg(stage0)] pub use self::Percent        as PERCENT;\n+#[cfg(stage0)] pub use self::Caret          as CARET;\n+#[cfg(stage0)] pub use self::And            as AND;\n+#[cfg(stage0)] pub use self::Or             as OR;\n+#[cfg(stage0)] pub use self::Shl            as SHL;\n+#[cfg(stage0)] pub use self::Shr            as SHR;\n+#[cfg(stage0)] pub use self::Eq             as EQ;\n+#[cfg(stage0)] pub use self::Lt             as LT;\n+#[cfg(stage0)] pub use self::Le             as LE;\n+#[cfg(stage0)] pub use self::EqEq           as EQEQ;\n+#[cfg(stage0)] pub use self::Ne             as NE;\n+#[cfg(stage0)] pub use self::Ge             as GE;\n+#[cfg(stage0)] pub use self::Gt             as GT;\n+#[cfg(stage0)] pub use self::AndAnd         as ANDAND;\n+#[cfg(stage0)] pub use self::OrOr           as OROR;\n+#[cfg(stage0)] pub use self::Not            as NOT;\n+#[cfg(stage0)] pub use self::Tilde          as TILDE;\n+#[cfg(stage0)] pub use self::BinOp          as BINOP;\n+#[cfg(stage0)] pub use self::BinOpEq        as BINOPEQ;\n+#[cfg(stage0)] pub use self::At             as AT;\n+#[cfg(stage0)] pub use self::Dot            as DOT;\n+#[cfg(stage0)] pub use self::DotDot         as DOTDOT;\n+#[cfg(stage0)] pub use self::DotDotDot      as DOTDOTDOT;\n+#[cfg(stage0)] pub use self::Comma          as COMMA;\n+#[cfg(stage0)] pub use self::Semi           as SEMI;\n+#[cfg(stage0)] pub use self::Colon          as COLON;\n+#[cfg(stage0)] pub use self::ModSep         as MOD_SEP;\n+#[cfg(stage0)] pub use self::RArrow         as RARROW;\n+#[cfg(stage0)] pub use self::LArrow         as LARROW;\n+#[cfg(stage0)] pub use self::FatArrow       as FAT_ARROW;\n+#[cfg(stage0)] pub use self::LParen         as LPAREN;\n+#[cfg(stage0)] pub use self::RParen         as RPAREN;\n+#[cfg(stage0)] pub use self::LBracket       as LBRACKET;\n+#[cfg(stage0)] pub use self::RBracket       as RBRACKET;\n+#[cfg(stage0)] pub use self::LBrace         as LBRACE;\n+#[cfg(stage0)] pub use self::RBrace         as RBRACE;\n+#[cfg(stage0)] pub use self::Pound          as POUND;\n+#[cfg(stage0)] pub use self::Dollar         as DOLLAR;\n+#[cfg(stage0)] pub use self::Question       as QUESTION;\n+#[cfg(stage0)] pub use self::LitByte        as LIT_BYTE;\n+#[cfg(stage0)] pub use self::LitChar        as LIT_CHAR;\n+#[cfg(stage0)] pub use self::LitInteger     as LIT_INTEGER;\n+#[cfg(stage0)] pub use self::LitFloat       as LIT_FLOAT;\n+#[cfg(stage0)] pub use self::LitStr         as LIT_STR;\n+#[cfg(stage0)] pub use self::LitStrRaw      as LIT_STR_RAW;\n+#[cfg(stage0)] pub use self::LitBinary      as LIT_BINARY;\n+#[cfg(stage0)] pub use self::LitBinaryRaw   as LIT_BINARY_RAW;\n+#[cfg(stage0)] pub use self::Ident          as IDENT;\n+#[cfg(stage0)] pub use self::Underscore     as UNDERSCORE;\n+#[cfg(stage0)] pub use self::Lifetime       as LIFETIME;\n+#[cfg(stage0)] pub use self::Interpolated   as INTERPOLATED;\n+#[cfg(stage0)] pub use self::DocComment     as DOC_COMMENT;\n+#[cfg(stage0)] pub use self::Whitespace     as WS;\n+#[cfg(stage0)] pub use self::Comment        as COMMENT;\n+#[cfg(stage0)] pub use self::Shebang        as SHEBANG;\n+#[cfg(stage0)] pub use self::Eof            as EOF;\n+\n #[allow(non_camel_case_types)]\n #[deriving(Clone, Encodable, Decodable, PartialEq, Eq, Hash, Show)]\n-pub enum BinOp {\n-    PLUS,\n-    MINUS,\n-    STAR,\n-    SLASH,\n-    PERCENT,\n-    CARET,\n-    AND,\n-    OR,\n-    SHL,\n-    SHR,\n+pub enum BinOpToken {\n+    Plus,\n+    Minus,\n+    Star,\n+    Slash,\n+    Percent,\n+    Caret,\n+    And,\n+    Or,\n+    Shl,\n+    Shr,\n }\n \n #[allow(non_camel_case_types)]\n #[deriving(Clone, Encodable, Decodable, PartialEq, Eq, Hash, Show)]\n pub enum Token {\n     /* Expression-operator symbols. */\n-    EQ,\n-    LT,\n-    LE,\n-    EQEQ,\n-    NE,\n-    GE,\n-    GT,\n-    ANDAND,\n-    OROR,\n-    NOT,\n-    TILDE,\n-    BINOP(BinOp),\n-    BINOPEQ(BinOp),\n+    Eq,\n+    Lt,\n+    Le,\n+    EqEq,\n+    Ne,\n+    Ge,\n+    Gt,\n+    AndAnd,\n+    OrOr,\n+    Not,\n+    Tilde,\n+    BinOp(BinOpToken),\n+    BinOpEq(BinOpToken),\n \n     /* Structural symbols */\n-    AT,\n-    DOT,\n-    DOTDOT,\n-    DOTDOTDOT,\n-    COMMA,\n-    SEMI,\n-    COLON,\n-    MOD_SEP,\n-    RARROW,\n-    LARROW,\n-    FAT_ARROW,\n-    LPAREN,\n-    RPAREN,\n-    LBRACKET,\n-    RBRACKET,\n-    LBRACE,\n-    RBRACE,\n-    POUND,\n-    DOLLAR,\n-    QUESTION,\n+    At,\n+    Dot,\n+    DotDot,\n+    DotDotDot,\n+    Comma,\n+    Semi,\n+    Colon,\n+    ModSep,\n+    RArrow,\n+    LArrow,\n+    FatArrow,\n+    LParen,\n+    RParen,\n+    LBracket,\n+    RBracket,\n+    LBrace,\n+    RBrace,\n+    Pound,\n+    Dollar,\n+    Question,\n \n     /* Literals */\n-    LIT_BYTE(Name),\n-    LIT_CHAR(Name),\n-    LIT_INTEGER(Name),\n-    LIT_FLOAT(Name),\n-    LIT_STR(Name),\n-    LIT_STR_RAW(Name, uint), /* raw str delimited by n hash symbols */\n-    LIT_BINARY(Name),\n-    LIT_BINARY_RAW(Name, uint), /* raw binary str delimited by n hash symbols */\n+    LitByte(ast::Name),\n+    LitChar(ast::Name),\n+    LitInteger(ast::Name),\n+    LitFloat(ast::Name),\n+    LitStr(ast::Name),\n+    LitStrRaw(ast::Name, uint), /* raw str delimited by n hash symbols */\n+    LitBinary(ast::Name),\n+    LitBinaryRaw(ast::Name, uint), /* raw binary str delimited by n hash symbols */\n \n     /* Name components */\n     /// An identifier contains an \"is_mod_name\" boolean,\n     /// indicating whether :: follows this token with no\n     /// whitespace in between.\n-    IDENT(Ident, bool),\n-    UNDERSCORE,\n-    LIFETIME(Ident),\n+    Ident(ast::Ident, bool),\n+    Underscore,\n+    Lifetime(ast::Ident),\n \n     /* For interpolation */\n-    INTERPOLATED(Nonterminal),\n-    DOC_COMMENT(Name),\n+    Interpolated(Nonterminal),\n+    DocComment(ast::Name),\n \n     // Junk. These carry no data because we don't really care about the data\n     // they *would* carry, and don't really want to allocate a new ident for\n     // them. Instead, users could extract that from the associated span.\n \n     /// Whitespace\n-    WS,\n+    Whitespace,\n     /// Comment\n-    COMMENT,\n-    SHEBANG(Name),\n+    Comment,\n+    Shebang(ast::Name),\n \n-    EOF,\n+    Eof,\n }\n \n #[deriving(Clone, Encodable, Decodable, PartialEq, Eq, Hash)]\n@@ -122,7 +183,7 @@ pub enum Nonterminal {\n     NtExpr( P<ast::Expr>),\n     NtTy(   P<ast::Ty>),\n     /// See IDENT, above, for meaning of bool in NtIdent:\n-    NtIdent(Box<Ident>, bool),\n+    NtIdent(Box<ast::Ident>, bool),\n     /// Stuff inside brackets for attributes\n     NtMeta( P<ast::MetaItem>),\n     NtPath(Box<ast::Path>),\n@@ -148,202 +209,179 @@ impl fmt::Show for Nonterminal {\n     }\n }\n \n-pub fn binop_to_string(o: BinOp) -> &'static str {\n+pub fn binop_to_string(o: BinOpToken) -> &'static str {\n     match o {\n-      PLUS => \"+\",\n-      MINUS => \"-\",\n-      STAR => \"*\",\n-      SLASH => \"/\",\n-      PERCENT => \"%\",\n-      CARET => \"^\",\n-      AND => \"&\",\n-      OR => \"|\",\n-      SHL => \"<<\",\n-      SHR => \">>\"\n+        Plus      => \"+\",\n+        Minus     => \"-\",\n+        Star      => \"*\",\n+        Slash     => \"/\",\n+        Percent   => \"%\",\n+        Caret     => \"^\",\n+        And       => \"&\",\n+        Or        => \"|\",\n+        Shl       => \"<<\",\n+        Shr       => \">>\",\n     }\n }\n \n pub fn to_string(t: &Token) -> String {\n     match *t {\n-      EQ => \"=\".into_string(),\n-      LT => \"<\".into_string(),\n-      LE => \"<=\".into_string(),\n-      EQEQ => \"==\".into_string(),\n-      NE => \"!=\".into_string(),\n-      GE => \">=\".into_string(),\n-      GT => \">\".into_string(),\n-      NOT => \"!\".into_string(),\n-      TILDE => \"~\".into_string(),\n-      OROR => \"||\".into_string(),\n-      ANDAND => \"&&\".into_string(),\n-      BINOP(op) => binop_to_string(op).into_string(),\n-      BINOPEQ(op) => {\n-          let mut s = binop_to_string(op).into_string();\n-          s.push_str(\"=\");\n-          s\n-      }\n-\n-      /* Structural symbols */\n-      AT => \"@\".into_string(),\n-      DOT => \".\".into_string(),\n-      DOTDOT => \"..\".into_string(),\n-      DOTDOTDOT => \"...\".into_string(),\n-      COMMA => \",\".into_string(),\n-      SEMI => \";\".into_string(),\n-      COLON => \":\".into_string(),\n-      MOD_SEP => \"::\".into_string(),\n-      RARROW => \"->\".into_string(),\n-      LARROW => \"<-\".into_string(),\n-      FAT_ARROW => \"=>\".into_string(),\n-      LPAREN => \"(\".into_string(),\n-      RPAREN => \")\".into_string(),\n-      LBRACKET => \"[\".into_string(),\n-      RBRACKET => \"]\".into_string(),\n-      LBRACE => \"{\".into_string(),\n-      RBRACE => \"}\".into_string(),\n-      POUND => \"#\".into_string(),\n-      DOLLAR => \"$\".into_string(),\n-      QUESTION => \"?\".into_string(),\n-\n-      /* Literals */\n-      LIT_BYTE(b) => {\n-          format!(\"b'{}'\", b.as_str())\n-      }\n-      LIT_CHAR(c) => {\n-          format!(\"'{}'\", c.as_str())\n-      }\n-      LIT_INTEGER(c) | LIT_FLOAT(c) => {\n-          c.as_str().into_string()\n-      }\n-\n-      LIT_STR(s) => {\n-          format!(\"\\\"{}\\\"\", s.as_str())\n-      }\n-      LIT_STR_RAW(s, n) => {\n-        format!(\"r{delim}\\\"{string}\\\"{delim}\",\n-                 delim=\"#\".repeat(n), string=s.as_str())\n-      }\n-      LIT_BINARY(v) => {\n-          format!(\"b\\\"{}\\\"\", v.as_str())\n-      }\n-      LIT_BINARY_RAW(s, n) => {\n-        format!(\"br{delim}\\\"{string}\\\"{delim}\",\n-                 delim=\"#\".repeat(n), string=s.as_str())\n-      }\n-\n-      /* Name components */\n-      IDENT(s, _) => get_ident(s).get().into_string(),\n-      LIFETIME(s) => {\n-          format!(\"{}\", get_ident(s))\n-      }\n-      UNDERSCORE => \"_\".into_string(),\n-\n-      /* Other */\n-      DOC_COMMENT(s) => s.as_str().into_string(),\n-      EOF => \"<eof>\".into_string(),\n-      WS => \" \".into_string(),\n-      COMMENT => \"/* */\".into_string(),\n-      SHEBANG(s) => format!(\"/* shebang: {}*/\", s.as_str()),\n-\n-      INTERPOLATED(ref nt) => {\n-        match nt {\n-            &NtExpr(ref e) => ::print::pprust::expr_to_string(&**e),\n-            &NtMeta(ref e) => ::print::pprust::meta_item_to_string(&**e),\n-            &NtTy(ref e) => ::print::pprust::ty_to_string(&**e),\n-            &NtPath(ref e) => ::print::pprust::path_to_string(&**e),\n-            _ => {\n-                let mut s = \"an interpolated \".into_string();\n-                match *nt {\n-                    NtItem(..) => s.push_str(\"item\"),\n-                    NtBlock(..) => s.push_str(\"block\"),\n-                    NtStmt(..) => s.push_str(\"statement\"),\n-                    NtPat(..) => s.push_str(\"pattern\"),\n-                    NtMeta(..) => fail!(\"should have been handled\"),\n-                    NtExpr(..) => fail!(\"should have been handled\"),\n-                    NtTy(..) => fail!(\"should have been handled\"),\n-                    NtIdent(..) => s.push_str(\"identifier\"),\n-                    NtPath(..) => fail!(\"should have been handled\"),\n-                    NtTT(..) => s.push_str(\"tt\"),\n-                    NtMatchers(..) => s.push_str(\"matcher sequence\")\n-                };\n-                s\n-            }\n+        Eq                  => \"=\".into_string(),\n+        Lt                  => \"<\".into_string(),\n+        Le                  => \"<=\".into_string(),\n+        EqEq                => \"==\".into_string(),\n+        Ne                  => \"!=\".into_string(),\n+        Ge                  => \">=\".into_string(),\n+        Gt                  => \">\".into_string(),\n+        Not                 => \"!\".into_string(),\n+        Tilde               => \"~\".into_string(),\n+        OrOr                => \"||\".into_string(),\n+        AndAnd              => \"&&\".into_string(),\n+        BinOp(op)           => binop_to_string(op).into_string(),\n+        BinOpEq(op)         => format!(\"{}=\", binop_to_string(op)),\n+\n+        /* Structural symbols */\n+        At                  => \"@\".into_string(),\n+        Dot                 => \".\".into_string(),\n+        DotDot              => \"..\".into_string(),\n+        DotDotDot           => \"...\".into_string(),\n+        Comma               => \",\".into_string(),\n+        Semi                => \";\".into_string(),\n+        Colon               => \":\".into_string(),\n+        ModSep              => \"::\".into_string(),\n+        RArrow              => \"->\".into_string(),\n+        LArrow              => \"<-\".into_string(),\n+        FatArrow            => \"=>\".into_string(),\n+        LParen              => \"(\".into_string(),\n+        RParen              => \")\".into_string(),\n+        LBracket            => \"[\".into_string(),\n+        RBracket            => \"]\".into_string(),\n+        LBrace              => \"{\".into_string(),\n+        RBrace              => \"}\".into_string(),\n+        Pound               => \"#\".into_string(),\n+        Dollar              => \"$\".into_string(),\n+        Question            => \"?\".into_string(),\n+\n+        /* Literals */\n+        LitByte(b)          => format!(\"b'{}'\", b.as_str()),\n+        LitChar(c)          => format!(\"'{}'\", c.as_str()),\n+        LitFloat(c)         => c.as_str().into_string(),\n+        LitInteger(c)       => c.as_str().into_string(),\n+        LitStr(s)           => format!(\"\\\"{}\\\"\", s.as_str()),\n+        LitStrRaw(s, n)     => format!(\"r{delim}\\\"{string}\\\"{delim}\",\n+                                       delim=\"#\".repeat(n),\n+                                       string=s.as_str()),\n+        LitBinary(v)        => format!(\"b\\\"{}\\\"\", v.as_str()),\n+        LitBinaryRaw(s, n)  => format!(\"br{delim}\\\"{string}\\\"{delim}\",\n+                                       delim=\"#\".repeat(n),\n+                                       string=s.as_str()),\n+\n+        /* Name components */\n+        Ident(s, _)         => get_ident(s).get().into_string(),\n+        Lifetime(s)         => format!(\"{}\", get_ident(s)),\n+        Underscore          => \"_\".into_string(),\n+\n+        /* Other */\n+        DocComment(s)       => s.as_str().into_string(),\n+        Eof                 => \"<eof>\".into_string(),\n+        Whitespace          => \" \".into_string(),\n+        Comment             => \"/* */\".into_string(),\n+        Shebang(s)          => format!(\"/* shebang: {}*/\", s.as_str()),\n+\n+        Interpolated(ref nt) => match *nt {\n+            NtExpr(ref e)  => ::print::pprust::expr_to_string(&**e),\n+            NtMeta(ref e)  => ::print::pprust::meta_item_to_string(&**e),\n+            NtTy(ref e)    => ::print::pprust::ty_to_string(&**e),\n+            NtPath(ref e)  => ::print::pprust::path_to_string(&**e),\n+            NtItem(..)     => \"an interpolated item\".into_string(),\n+            NtBlock(..)    => \"an interpolated block\".into_string(),\n+            NtStmt(..)     => \"an interpolated statement\".into_string(),\n+            NtPat(..)      => \"an interpolated pattern\".into_string(),\n+            NtIdent(..)    => \"an interpolated identifier\".into_string(),\n+            NtTT(..)       => \"an interpolated tt\".into_string(),\n+            NtMatchers(..) => \"an interpolated matcher sequence\".into_string(),\n         }\n-      }\n     }\n }\n \n pub fn can_begin_expr(t: &Token) -> bool {\n     match *t {\n-      LPAREN => true,\n-      LBRACE => true,\n-      LBRACKET => true,\n-      IDENT(_, _) => true,\n-      UNDERSCORE => true,\n-      TILDE => true,\n-      LIT_BYTE(_) => true,\n-      LIT_CHAR(_) => true,\n-      LIT_INTEGER(_) => true,\n-      LIT_FLOAT(_) => true,\n-      LIT_STR(_) => true,\n-      LIT_STR_RAW(_, _) => true,\n-      LIT_BINARY(_) => true,\n-      LIT_BINARY_RAW(_, _) => true,\n-      POUND => true,\n-      AT => true,\n-      NOT => true,\n-      BINOP(MINUS) => true,\n-      BINOP(STAR) => true,\n-      BINOP(AND) => true,\n-      BINOP(OR) => true, // in lambda syntax\n-      OROR => true, // in lambda syntax\n-      MOD_SEP => true,\n-      INTERPOLATED(NtExpr(..))\n-      | INTERPOLATED(NtIdent(..))\n-      | INTERPOLATED(NtBlock(..))\n-      | INTERPOLATED(NtPath(..)) => true,\n-      _ => false\n+        LParen              => true,\n+        LBrace              => true,\n+        LBracket            => true,\n+        Ident(_, _)         => true,\n+        Underscore          => true,\n+        Tilde               => true,\n+        LitByte(_)          => true,\n+        LitChar(_)          => true,\n+        LitInteger(_)       => true,\n+        LitFloat(_)         => true,\n+        LitStr(_)           => true,\n+        LitStrRaw(_, _)     => true,\n+        LitBinary(_)        => true,\n+        LitBinaryRaw(_, _)  => true,\n+        Pound               => true,\n+        At                  => true,\n+        Not                 => true,\n+        BinOp(Minus)        => true,\n+        BinOp(Star)         => true,\n+        BinOp(And)          => true,\n+        BinOp(Or)           => true, // in lambda syntax\n+        OrOr                => true, // in lambda syntax\n+        ModSep              => true,\n+        Interpolated(NtExpr(..))    => true,\n+        Interpolated(NtIdent(..))   => true,\n+        Interpolated(NtBlock(..))   => true,\n+        Interpolated(NtPath(..))    => true,\n+        _                   => false,\n     }\n }\n \n /// Returns the matching close delimiter if this is an open delimiter,\n /// otherwise `None`.\n pub fn close_delimiter_for(t: &Token) -> Option<Token> {\n     match *t {\n-        LPAREN   => Some(RPAREN),\n-        LBRACE   => Some(RBRACE),\n-        LBRACKET => Some(RBRACKET),\n-        _        => None\n+        LParen   => Some(RParen),\n+        LBrace   => Some(RBrace),\n+        LBracket => Some(RBracket),\n+        _        => None,\n     }\n }\n \n pub fn is_lit(t: &Token) -> bool {\n     match *t {\n-      LIT_BYTE(_) => true,\n-      LIT_CHAR(_) => true,\n-      LIT_INTEGER(_) => true,\n-      LIT_FLOAT(_) => true,\n-      LIT_STR(_) => true,\n-      LIT_STR_RAW(_, _) => true,\n-      LIT_BINARY(_) => true,\n-      LIT_BINARY_RAW(_, _) => true,\n-      _ => false\n+        LitByte(_)          => true,\n+        LitChar(_)          => true,\n+        LitInteger(_)       => true,\n+        LitFloat(_)         => true,\n+        LitStr(_)           => true,\n+        LitStrRaw(_, _)     => true,\n+        LitBinary(_)        => true,\n+        LitBinaryRaw(_, _)  => true,\n+        _ => false,\n     }\n }\n \n pub fn is_ident(t: &Token) -> bool {\n-    match *t { IDENT(_, _) => true, _ => false }\n+    match *t {\n+        Ident(_, _) => true,\n+        _           => false,\n+    }\n }\n \n pub fn is_ident_or_path(t: &Token) -> bool {\n     match *t {\n-      IDENT(_, _) | INTERPOLATED(NtPath(..)) => true,\n-      _ => false\n+        Ident(_, _)                 => true,\n+        Interpolated(NtPath(..))    => true,\n+        _                           => false,\n     }\n }\n \n pub fn is_plain_ident(t: &Token) -> bool {\n-    match *t { IDENT(_, false) => true, _ => false }\n+    match *t {\n+        Ident(_, false) => true,\n+        _               => false,\n+    }\n }\n \n // Get the first \"argument\"\n@@ -376,22 +414,28 @@ macro_rules! declare_special_idents_and_keywords {(\n         $( ($rk_name:expr, $rk_variant:ident, $rk_str:expr); )*\n     }\n ) => {\n-    static STRICT_KEYWORD_START: Name = first!($( Name($sk_name), )*);\n-    static STRICT_KEYWORD_FINAL: Name = last!($( Name($sk_name), )*);\n-    static RESERVED_KEYWORD_START: Name = first!($( Name($rk_name), )*);\n-    static RESERVED_KEYWORD_FINAL: Name = last!($( Name($rk_name), )*);\n+    static STRICT_KEYWORD_START: ast::Name = first!($( ast::Name($sk_name), )*);\n+    static STRICT_KEYWORD_FINAL: ast::Name = last!($( ast::Name($sk_name), )*);\n+    static RESERVED_KEYWORD_START: ast::Name = first!($( ast::Name($rk_name), )*);\n+    static RESERVED_KEYWORD_FINAL: ast::Name = last!($( ast::Name($rk_name), )*);\n \n     pub mod special_idents {\n-        use ast::{Ident, Name};\n+        use ast;\n         $(\n             #[allow(non_uppercase_statics)]\n-            pub const $si_static: Ident = Ident { name: Name($si_name), ctxt: 0 };\n+            pub const $si_static: ast::Ident = ast::Ident {\n+                name: ast::Name($si_name),\n+                ctxt: 0,\n+            };\n          )*\n     }\n \n     pub mod special_names {\n-        use ast::Name;\n-        $( #[allow(non_uppercase_statics)] pub const $si_static: Name =  Name($si_name); )*\n+        use ast;\n+        $(\n+            #[allow(non_uppercase_statics)]\n+            pub const $si_static: ast::Name =  ast::Name($si_name);\n+        )*\n     }\n \n     /**\n@@ -402,18 +446,18 @@ macro_rules! declare_special_idents_and_keywords {(\n      * the language and may not appear as identifiers.\n      */\n     pub mod keywords {\n-        use ast::Name;\n+        use ast;\n \n         pub enum Keyword {\n             $( $sk_variant, )*\n             $( $rk_variant, )*\n         }\n \n         impl Keyword {\n-            pub fn to_name(&self) -> Name {\n+            pub fn to_name(&self) -> ast::Name {\n                 match *self {\n-                    $( $sk_variant => Name($sk_name), )*\n-                    $( $rk_variant => Name($rk_name), )*\n+                    $( $sk_variant => ast::Name($sk_name), )*\n+                    $( $rk_variant => ast::Name($rk_name), )*\n                 }\n             }\n         }\n@@ -432,9 +476,9 @@ macro_rules! declare_special_idents_and_keywords {(\n }}\n \n // If the special idents get renumbered, remember to modify these two as appropriate\n-pub const SELF_KEYWORD_NAME: Name = Name(SELF_KEYWORD_NAME_NUM);\n-const STATIC_KEYWORD_NAME: Name = Name(STATIC_KEYWORD_NAME_NUM);\n-const SUPER_KEYWORD_NAME: Name = Name(SUPER_KEYWORD_NAME_NUM);\n+pub const SELF_KEYWORD_NAME: ast::Name = ast::Name(SELF_KEYWORD_NAME_NUM);\n+const STATIC_KEYWORD_NAME: ast::Name = ast::Name(STATIC_KEYWORD_NAME_NUM);\n+const SUPER_KEYWORD_NAME: ast::Name = ast::Name(SUPER_KEYWORD_NAME_NUM);\n \n pub const SELF_KEYWORD_NAME_NUM: u32 = 1;\n const STATIC_KEYWORD_NAME_NUM: u32 = 2;\n@@ -531,27 +575,27 @@ declare_special_idents_and_keywords! {\n  * operator\n  */\n pub fn token_to_binop(tok: &Token) -> Option<ast::BinOp> {\n-  match *tok {\n-      BINOP(STAR)    => Some(ast::BiMul),\n-      BINOP(SLASH)   => Some(ast::BiDiv),\n-      BINOP(PERCENT) => Some(ast::BiRem),\n-      BINOP(PLUS)    => Some(ast::BiAdd),\n-      BINOP(MINUS)   => Some(ast::BiSub),\n-      BINOP(SHL)     => Some(ast::BiShl),\n-      BINOP(SHR)     => Some(ast::BiShr),\n-      BINOP(AND)     => Some(ast::BiBitAnd),\n-      BINOP(CARET)   => Some(ast::BiBitXor),\n-      BINOP(OR)      => Some(ast::BiBitOr),\n-      LT             => Some(ast::BiLt),\n-      LE             => Some(ast::BiLe),\n-      GE             => Some(ast::BiGe),\n-      GT             => Some(ast::BiGt),\n-      EQEQ           => Some(ast::BiEq),\n-      NE             => Some(ast::BiNe),\n-      ANDAND         => Some(ast::BiAnd),\n-      OROR           => Some(ast::BiOr),\n-      _              => None\n-  }\n+    match *tok {\n+        BinOp(Star)     => Some(ast::BiMul),\n+        BinOp(Slash)    => Some(ast::BiDiv),\n+        BinOp(Percent)  => Some(ast::BiRem),\n+        BinOp(Plus)     => Some(ast::BiAdd),\n+        BinOp(Minus)    => Some(ast::BiSub),\n+        BinOp(Shl)      => Some(ast::BiShl),\n+        BinOp(Shr)      => Some(ast::BiShr),\n+        BinOp(And)      => Some(ast::BiBitAnd),\n+        BinOp(Caret)    => Some(ast::BiBitXor),\n+        BinOp(Or)       => Some(ast::BiBitOr),\n+        Lt              => Some(ast::BiLt),\n+        Le              => Some(ast::BiLe),\n+        Ge              => Some(ast::BiGe),\n+        Gt              => Some(ast::BiGt),\n+        EqEq            => Some(ast::BiEq),\n+        Ne              => Some(ast::BiNe),\n+        AndAnd          => Some(ast::BiAnd),\n+        OrOr            => Some(ast::BiOr),\n+        _               => None\n+    }\n }\n \n // looks like we can get rid of this completely...\n@@ -646,15 +690,15 @@ impl<S:Encoder<E>, E> Encodable<S, E> for InternedString {\n \n /// Returns the string contents of a name, using the task-local interner.\n #[inline]\n-pub fn get_name(name: Name) -> InternedString {\n+pub fn get_name(name: ast::Name) -> InternedString {\n     let interner = get_ident_interner();\n     InternedString::new_from_rc_str(interner.get(name))\n }\n \n /// Returns the string contents of an identifier, using the task-local\n /// interner.\n #[inline]\n-pub fn get_ident(ident: Ident) -> InternedString {\n+pub fn get_ident(ident: ast::Ident) -> InternedString {\n     get_name(ident.name)\n }\n \n@@ -667,32 +711,32 @@ pub fn intern_and_get_ident(s: &str) -> InternedString {\n \n /// Maps a string to its interned representation.\n #[inline]\n-pub fn intern(s: &str) -> Name {\n+pub fn intern(s: &str) -> ast::Name {\n     get_ident_interner().intern(s)\n }\n \n /// gensym's a new uint, using the current interner.\n #[inline]\n-pub fn gensym(s: &str) -> Name {\n+pub fn gensym(s: &str) -> ast::Name {\n     get_ident_interner().gensym(s)\n }\n \n /// Maps a string to an identifier with an empty syntax context.\n #[inline]\n-pub fn str_to_ident(s: &str) -> Ident {\n-    Ident::new(intern(s))\n+pub fn str_to_ident(s: &str) -> ast::Ident {\n+    ast::Ident::new(intern(s))\n }\n \n /// Maps a string to a gensym'ed identifier.\n #[inline]\n-pub fn gensym_ident(s: &str) -> Ident {\n-    Ident::new(gensym(s))\n+pub fn gensym_ident(s: &str) -> ast::Ident {\n+    ast::Ident::new(gensym(s))\n }\n \n // create a fresh name that maps to the same string as the old one.\n // note that this guarantees that str_ptr_eq(ident_to_string(src),interner_get(fresh_name(src)));\n // that is, that the new name and the old one are connected to ptr_eq strings.\n-pub fn fresh_name(src: &Ident) -> Name {\n+pub fn fresh_name(src: &ast::Ident) -> ast::Name {\n     let interner = get_ident_interner();\n     interner.gensym_copy(src.name)\n     // following: debug version. Could work in final except that it's incompatible with\n@@ -703,22 +747,22 @@ pub fn fresh_name(src: &Ident) -> Name {\n }\n \n // create a fresh mark.\n-pub fn fresh_mark() -> Mrk {\n+pub fn fresh_mark() -> ast::Mrk {\n     gensym(\"mark\").uint() as u32\n }\n \n // See the macro above about the types of keywords\n \n pub fn is_keyword(kw: keywords::Keyword, tok: &Token) -> bool {\n     match *tok {\n-        token::IDENT(sid, false) => { kw.to_name() == sid.name }\n+        Ident(sid, false) => { kw.to_name() == sid.name }\n         _ => { false }\n     }\n }\n \n pub fn is_any_keyword(tok: &Token) -> bool {\n     match *tok {\n-        token::IDENT(sid, false) => {\n+        Ident(sid, false) => {\n             let n = sid.name;\n \n                n == SELF_KEYWORD_NAME\n@@ -733,7 +777,7 @@ pub fn is_any_keyword(tok: &Token) -> bool {\n \n pub fn is_strict_keyword(tok: &Token) -> bool {\n     match *tok {\n-        token::IDENT(sid, false) => {\n+        Ident(sid, false) => {\n             let n = sid.name;\n \n                n == SELF_KEYWORD_NAME\n@@ -742,7 +786,7 @@ pub fn is_strict_keyword(tok: &Token) -> bool {\n             || STRICT_KEYWORD_START <= n\n             && n <= STRICT_KEYWORD_FINAL\n         },\n-        token::IDENT(sid, true) => {\n+        Ident(sid, true) => {\n             let n = sid.name;\n \n                n != SELF_KEYWORD_NAME\n@@ -756,7 +800,7 @@ pub fn is_strict_keyword(tok: &Token) -> bool {\n \n pub fn is_reserved_keyword(tok: &Token) -> bool {\n     match *tok {\n-        token::IDENT(sid, false) => {\n+        Ident(sid, false) => {\n             let n = sid.name;\n \n                RESERVED_KEYWORD_START <= n\n@@ -768,7 +812,7 @@ pub fn is_reserved_keyword(tok: &Token) -> bool {\n \n pub fn mtwt_token_eq(t1 : &Token, t2 : &Token) -> bool {\n     match (t1,t2) {\n-        (&IDENT(id1,_),&IDENT(id2,_)) | (&LIFETIME(id1),&LIFETIME(id2)) =>\n+        (&Ident(id1,_),&Ident(id2,_)) | (&Lifetime(id1),&Lifetime(id2)) =>\n             mtwt::resolve(id1) == mtwt::resolve(id2),\n         _ => *t1 == *t2\n     }\n@@ -786,9 +830,9 @@ mod test {\n     }\n \n     #[test] fn mtwt_token_eq_test() {\n-        assert!(mtwt_token_eq(&GT,&GT));\n+        assert!(mtwt_token_eq(&Gt,&Gt));\n         let a = str_to_ident(\"bac\");\n         let a1 = mark_ident(a,92);\n-        assert!(mtwt_token_eq(&IDENT(a,true),&IDENT(a1,false)));\n+        assert!(mtwt_token_eq(&Ident(a,true),&Ident(a1,false)));\n     }\n }"}, {"sha": "25ef8700ed005ae5b6f848ee7499350f626da2aa", "filename": "src/libsyntax/print/pprust.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/d8b1fa0ae0c27e54d3539190683c01e194d36fbd/src%2Flibsyntax%2Fprint%2Fpprust.rs", "raw_url": "https://github.com/rust-lang/rust/raw/d8b1fa0ae0c27e54d3539190683c01e194d36fbd/src%2Flibsyntax%2Fprint%2Fpprust.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fprint%2Fpprust.rs?ref=d8b1fa0ae0c27e54d3539190683c01e194d36fbd", "patch": "@@ -1035,7 +1035,7 @@ impl<'a> State<'a> {\n             ast::TtToken(_, ref tk) => {\n                 try!(word(&mut self.s, parse::token::to_string(tk).as_slice()));\n                 match *tk {\n-                    parse::token::DOC_COMMENT(..) => {\n+                    parse::token::DocComment(..) => {\n                         hardbreak(&mut self.s)\n                     }\n                     _ => Ok(())"}, {"sha": "519f32fc248bd325bc7805edb1e86357f8fa359b", "filename": "src/test/auxiliary/roman_numerals.rs", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/rust-lang/rust/blob/d8b1fa0ae0c27e54d3539190683c01e194d36fbd/src%2Ftest%2Fauxiliary%2Froman_numerals.rs", "raw_url": "https://github.com/rust-lang/rust/raw/d8b1fa0ae0c27e54d3539190683c01e194d36fbd/src%2Ftest%2Fauxiliary%2Froman_numerals.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ftest%2Fauxiliary%2Froman_numerals.rs?ref=d8b1fa0ae0c27e54d3539190683c01e194d36fbd", "patch": "@@ -17,7 +17,7 @@ extern crate syntax;\n extern crate rustc;\n \n use syntax::codemap::Span;\n-use syntax::parse::token::{IDENT, get_ident};\n+use syntax::parse::token;\n use syntax::ast::{TokenTree, TtToken};\n use syntax::ext::base::{ExtCtxt, MacResult, DummyResult, MacExpr};\n use syntax::ext::build::AstBuilder;  // trait for expr_uint\n@@ -39,7 +39,7 @@ fn expand_rn(cx: &mut ExtCtxt, sp: Span, args: &[TokenTree])\n         (\"I\",    1)];\n \n     let text = match args {\n-        [TtToken(_, IDENT(s, _))] => get_ident(s).to_string(),\n+        [TtToken(_, token::Ident(s, _))] => token::get_ident(s).to_string(),\n         _ => {\n             cx.span_err(sp, \"argument should be a single identifier\");\n             return DummyResult::any(sp);"}, {"sha": "b3fa04d8025bdf137aa6e5beaf53576af108ef5c", "filename": "src/test/compile-fail/removed-syntax-record.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/d8b1fa0ae0c27e54d3539190683c01e194d36fbd/src%2Ftest%2Fcompile-fail%2Fremoved-syntax-record.rs", "raw_url": "https://github.com/rust-lang/rust/raw/d8b1fa0ae0c27e54d3539190683c01e194d36fbd/src%2Ftest%2Fcompile-fail%2Fremoved-syntax-record.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ftest%2Fcompile-fail%2Fremoved-syntax-record.rs?ref=d8b1fa0ae0c27e54d3539190683c01e194d36fbd", "patch": "@@ -8,4 +8,4 @@\n // option. This file may not be copied, modified, or distributed\n // except according to those terms.\n \n-type t = { f: () }; //~ ERROR expected type, found token LBRACE\n+type t = { f: () }; //~ ERROR expected type, found token LBrace"}]}