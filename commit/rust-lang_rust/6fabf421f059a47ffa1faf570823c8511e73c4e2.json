{"sha": "6fabf421f059a47ffa1faf570823c8511e73c4e2", "node_id": "MDY6Q29tbWl0NzI0NzEyOjZmYWJmNDIxZjA1OWE0N2ZmYTFmYWY1NzA4MjNjODUxMWU3M2M0ZTI=", "commit": {"author": {"name": "Alex Crichton", "email": "alex@alexcrichton.com", "date": "2014-12-30T00:36:18Z"}, "committer": {"name": "Alex Crichton", "email": "alex@alexcrichton.com", "date": "2014-12-30T00:36:18Z"}, "message": "rollup merge of #20245: fhahn/make-lexer-tests-runable-again\n\nI would like to look into some issues related to the model lexer  #15883.\n\nI stumbled upon 2 minor problems when I tried running the lexer tests:\n\n* antlr did not put the generated files in the correct directory\n* grammer/verify.rs did not work with the most recent version of rust\n\nWith these changes (and setting CLASSPATH=/usr/share/java/antlr-4.4-complete.jar:$CLASSPATH) I was able to execute the tests.\n\nNote that I just fixed the syntax errors and added `None` as 2. argument of `Literal`. I am not sure if this is correct however. I still have to take a closer look at what verify.rs actually does. Are there any helpful pointers?", "tree": {"sha": "6f1ac8105ad2ae59631fa4b43127c4b91b9bda9b", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/6f1ac8105ad2ae59631fa4b43127c4b91b9bda9b"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/6fabf421f059a47ffa1faf570823c8511e73c4e2", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/6fabf421f059a47ffa1faf570823c8511e73c4e2", "html_url": "https://github.com/rust-lang/rust/commit/6fabf421f059a47ffa1faf570823c8511e73c4e2", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/6fabf421f059a47ffa1faf570823c8511e73c4e2/comments", "author": {"login": "alexcrichton", "id": 64996, "node_id": "MDQ6VXNlcjY0OTk2", "avatar_url": "https://avatars.githubusercontent.com/u/64996?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alexcrichton", "html_url": "https://github.com/alexcrichton", "followers_url": "https://api.github.com/users/alexcrichton/followers", "following_url": "https://api.github.com/users/alexcrichton/following{/other_user}", "gists_url": "https://api.github.com/users/alexcrichton/gists{/gist_id}", "starred_url": "https://api.github.com/users/alexcrichton/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alexcrichton/subscriptions", "organizations_url": "https://api.github.com/users/alexcrichton/orgs", "repos_url": "https://api.github.com/users/alexcrichton/repos", "events_url": "https://api.github.com/users/alexcrichton/events{/privacy}", "received_events_url": "https://api.github.com/users/alexcrichton/received_events", "type": "User", "site_admin": false}, "committer": {"login": "alexcrichton", "id": 64996, "node_id": "MDQ6VXNlcjY0OTk2", "avatar_url": "https://avatars.githubusercontent.com/u/64996?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alexcrichton", "html_url": "https://github.com/alexcrichton", "followers_url": "https://api.github.com/users/alexcrichton/followers", "following_url": "https://api.github.com/users/alexcrichton/following{/other_user}", "gists_url": "https://api.github.com/users/alexcrichton/gists{/gist_id}", "starred_url": "https://api.github.com/users/alexcrichton/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alexcrichton/subscriptions", "organizations_url": "https://api.github.com/users/alexcrichton/orgs", "repos_url": "https://api.github.com/users/alexcrichton/repos", "events_url": "https://api.github.com/users/alexcrichton/events{/privacy}", "received_events_url": "https://api.github.com/users/alexcrichton/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "b31926d1156b6010497b75858d4e4f518de0006e", "url": "https://api.github.com/repos/rust-lang/rust/commits/b31926d1156b6010497b75858d4e4f518de0006e", "html_url": "https://github.com/rust-lang/rust/commit/b31926d1156b6010497b75858d4e4f518de0006e"}, {"sha": "adda8997b17f8d3d9d49384e3c5a9cb9e2640345", "url": "https://api.github.com/repos/rust-lang/rust/commits/adda8997b17f8d3d9d49384e3c5a9cb9e2640345", "html_url": "https://github.com/rust-lang/rust/commit/adda8997b17f8d3d9d49384e3c5a9cb9e2640345"}], "stats": {"total": 64, "additions": 33, "deletions": 31}, "files": [{"sha": "08461f9dcc2b760c895a92d6a35a659ebeca782d", "filename": "mk/grammar.mk", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/6fabf421f059a47ffa1faf570823c8511e73c4e2/mk%2Fgrammar.mk", "raw_url": "https://github.com/rust-lang/rust/raw/6fabf421f059a47ffa1faf570823c8511e73c4e2/mk%2Fgrammar.mk", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/mk%2Fgrammar.mk?ref=6fabf421f059a47ffa1faf570823c8511e73c4e2", "patch": "@@ -31,7 +31,7 @@ $(BG):\n \t$(Q)mkdir -p $(BG)\n \n $(BG)RustLexer.class: $(BG) $(SG)RustLexer.g4\n-\t$(Q)$(CFG_ANTLR4) -o $(B)grammar $(SG)RustLexer.g4\n+\t$(Q)$(CFG_ANTLR4) -o $(BG) $(SG)RustLexer.g4\n \t$(Q)$(CFG_JAVAC) -d $(BG) $(BG)RustLexer.java\n \n check-build-lexer-verifier: $(BG)verify"}, {"sha": "bdb616fcc99b83d744c28afed4ce5c8dcd250a36", "filename": "src/grammar/verify.rs", "status": "modified", "additions": 32, "deletions": 30, "changes": 62, "blob_url": "https://github.com/rust-lang/rust/blob/6fabf421f059a47ffa1faf570823c8511e73c4e2/src%2Fgrammar%2Fverify.rs", "raw_url": "https://github.com/rust-lang/rust/raw/6fabf421f059a47ffa1faf570823c8511e73c4e2/src%2Fgrammar%2Fverify.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Fgrammar%2Fverify.rs?ref=6fabf421f059a47ffa1faf570823c8511e73c4e2", "patch": "@@ -61,7 +61,7 @@ fn parse_token_list(file: &str) -> HashMap<String, token::Token> {\n             \"SHL\"               => token::BinOp(token::Shl),\n             \"LBRACE\"            => token::OpenDelim(token::Brace),\n             \"RARROW\"            => token::RArrow,\n-            \"LIT_STR\"           => token::Literal(token::Str_(Name(0))),\n+            \"LIT_STR\"           => token::Literal(token::Str_(Name(0)), None),\n             \"DOTDOT\"            => token::DotDot,\n             \"MOD_SEP\"           => token::ModSep,\n             \"DOTDOTDOT\"         => token::DotDotDot,\n@@ -71,7 +71,7 @@ fn parse_token_list(file: &str) -> HashMap<String, token::Token> {\n             \"ANDAND\"            => token::AndAnd,\n             \"AT\"                => token::At,\n             \"LBRACKET\"          => token::OpenDelim(token::Bracket),\n-            \"LIT_STR_RAW\"       => token::Literal(token::StrRaw(Name(0), 0)),\n+            \"LIT_STR_RAW\"       => token::Literal(token::StrRaw(Name(0), 0), None),\n             \"RPAREN\"            => token::CloseDelim(token::Paren),\n             \"SLASH\"             => token::BinOp(token::Slash),\n             \"COMMA\"             => token::Comma,\n@@ -80,8 +80,8 @@ fn parse_token_list(file: &str) -> HashMap<String, token::Token> {\n             \"TILDE\"             => token::Tilde,\n             \"IDENT\"             => id(),\n             \"PLUS\"              => token::BinOp(token::Plus),\n-            \"LIT_CHAR\"          => token::Literal(token::Char(Name(0))),\n-            \"LIT_BYTE\"          => token::Literal(token::Byte(Name(0))),\n+            \"LIT_CHAR\"          => token::Literal(token::Char(Name(0)), None),\n+            \"LIT_BYTE\"          => token::Literal(token::Byte(Name(0)), None),\n             \"EQ\"                => token::Eq,\n             \"RBRACKET\"          => token::CloseDelim(token::Bracket),\n             \"COMMENT\"           => token::Comment,\n@@ -95,9 +95,9 @@ fn parse_token_list(file: &str) -> HashMap<String, token::Token> {\n             \"BINOP\"             => token::BinOp(token::Plus),\n             \"POUND\"             => token::Pound,\n             \"OROR\"              => token::OrOr,\n-            \"LIT_INTEGER\"       => token::Literal(token::Integer(Name(0))),\n+            \"LIT_INTEGER\"       => token::Literal(token::Integer(Name(0)), None),\n             \"BINOPEQ\"           => token::BinOpEq(token::Plus),\n-            \"LIT_FLOAT\"         => token::Literal(token::Float(Name(0))),\n+            \"LIT_FLOAT\"         => token::Literal(token::Float(Name(0)), None),\n             \"WHITESPACE\"        => token::Whitespace,\n             \"UNDERSCORE\"        => token::Underscore,\n             \"MINUS\"             => token::BinOp(token::Minus),\n@@ -107,8 +107,8 @@ fn parse_token_list(file: &str) -> HashMap<String, token::Token> {\n             \"OR\"                => token::BinOp(token::Or),\n             \"GT\"                => token::Gt,\n             \"LE\"                => token::Le,\n-            \"LIT_BINARY\"        => token::Literal(token::Binary(Name(0))),\n-            \"LIT_BINARY_RAW\"    => token::Literal(token::BinaryRaw(Name(0), 0)),\n+            \"LIT_BINARY\"        => token::Literal(token::Binary(Name(0)), None),\n+            \"LIT_BINARY_RAW\"    => token::Literal(token::BinaryRaw(Name(0), 0), None),\n             _                   => continue,\n         };\n \n@@ -189,17 +189,17 @@ fn parse_antlr_token(s: &str, tokens: &HashMap<String, token::Token>) -> TokenAn\n         token::BinOp(..)           => token::BinOp(str_to_binop(content)),\n         token::BinOpEq(..)         => token::BinOpEq(str_to_binop(content.slice_to(\n                                                                     content.len() - 1))),\n-        token::Literal(token::Str_(..))      => token::Literal(token::Str_(fix(content))),\n-        token::Literal(token::StrRaw(..))    => token::Literal(token::StrRaw(fix(content),\n-                                                                             count(content))),\n-        token::Literal(token::Char(..))      => token::Literal(token::Char(fixchar(content))),\n-        token::Literal(token::Byte(..))      => token::Literal(token::Byte(fixchar(content))),\n+        token::Literal(token::Str_(..), n)      => token::Literal(token::Str_(fix(content)), n),\n+        token::Literal(token::StrRaw(..), n)    => token::Literal(token::StrRaw(fix(content),\n+                                                                             count(content)), n),\n+        token::Literal(token::Char(..), n)      => token::Literal(token::Char(fixchar(content)), n),\n+        token::Literal(token::Byte(..), n)      => token::Literal(token::Byte(fixchar(content)), n),\n         token::DocComment(..)      => token::DocComment(nm),\n-        token::Literal(token::Integer(..))   => token::Literal(token::Integer(nm)),\n-        token::Literal(token::Float(..))     => token::Literal(token::Float(nm)),\n-        token::Literal(token::Binary(..))    => token::Literal(token::Binary(nm)),\n-        token::Literal(token::BinaryRaw(..)) => token::Literal(token::BinaryRaw(fix(content),\n-                                                                                count(content))),\n+        token::Literal(token::Integer(..), n)   => token::Literal(token::Integer(nm), n),\n+        token::Literal(token::Float(..), n)     => token::Literal(token::Float(nm), n),\n+        token::Literal(token::Binary(..), n)    => token::Literal(token::Binary(nm), n),\n+        token::Literal(token::BinaryRaw(..), n) => token::Literal(token::BinaryRaw(fix(content),\n+                                                                                count(content)), n),\n         token::Ident(..)           => token::Ident(ast::Ident { name: nm, ctxt: 0 },\n                                                    token::ModName),\n         token::Lifetime(..)        => token::Lifetime(ast::Ident { name: nm, ctxt: 0 }),\n@@ -214,8 +214,8 @@ fn parse_antlr_token(s: &str, tokens: &HashMap<String, token::Token>) -> TokenAn\n     };\n \n     let sp = syntax::codemap::Span {\n-        lo: syntax::codemap::BytePos(from_str::<u32>(start).unwrap() - offset),\n-        hi: syntax::codemap::BytePos(from_str::<u32>(end).unwrap() + 1),\n+        lo: syntax::codemap::BytePos(start.parse::<u32>().unwrap() - offset),\n+        hi: syntax::codemap::BytePos(end.parse::<u32>().unwrap() + 1),\n         expn_id: syntax::codemap::NO_EXPANSION\n     };\n \n@@ -247,7 +247,9 @@ fn main() {\n     let token_map = parse_token_list(token_file.read_to_string().unwrap().as_slice());\n \n     let mut stdin = std::io::stdin();\n-    let mut antlr_tokens = stdin.lines().map(|l| parse_antlr_token(l.unwrap().as_slice().trim(),\n+    let mut lock = stdin.lock();\n+    let lines = lock.lines();\n+    let mut antlr_tokens = lines.map(|l| parse_antlr_token(l.unwrap().as_slice().trim(),\n                                                                    &token_map));\n \n     let code = File::open(&Path::new(args[1].as_slice())).unwrap().read_to_string().unwrap();\n@@ -284,17 +286,17 @@ fn main() {\n                     ref c => assert!(c == &antlr_tok.tok, \"{} is not {}\", rustc_tok, antlr_tok)\n                 }\n             )\n-        )\n+        );\n \n         matches!(\n-            token::Literal(token::Byte(..)),\n-            token::Literal(token::Char(..)),\n-            token::Literal(token::Integer(..)),\n-            token::Literal(token::Float(..)),\n-            token::Literal(token::Str_(..)),\n-            token::Literal(token::StrRaw(..)),\n-            token::Literal(token::Binary(..)),\n-            token::Literal(token::BinaryRaw(..)),\n+            token::Literal(token::Byte(..), _),\n+            token::Literal(token::Char(..), _),\n+            token::Literal(token::Integer(..), _),\n+            token::Literal(token::Float(..), _),\n+            token::Literal(token::Str_(..), _),\n+            token::Literal(token::StrRaw(..), _),\n+            token::Literal(token::Binary(..), _),\n+            token::Literal(token::BinaryRaw(..), _),\n             token::Ident(..),\n             token::Lifetime(..),\n             token::Interpolated(..),"}]}