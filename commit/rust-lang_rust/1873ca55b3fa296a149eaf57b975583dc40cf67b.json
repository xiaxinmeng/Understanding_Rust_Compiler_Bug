{"sha": "1873ca55b3fa296a149eaf57b975583dc40cf67b", "node_id": "MDY6Q29tbWl0NzI0NzEyOjE4NzNjYTU1YjNmYTI5NmExNDllYWY1N2I5NzU1ODNkYzQwY2Y2N2I=", "commit": {"author": {"name": "Mara Bos", "email": "m-ou.se@m-ou.se", "date": "2020-10-31T08:49:41Z"}, "committer": {"name": "GitHub", "email": "noreply@github.com", "date": "2020-10-31T08:49:41Z"}, "message": "Rollup merge of #78587 - petrochenkov:lazytok, r=Aaron1011\n\nparser: Cleanup `LazyTokenStream` and avoid some clones\n\nby using a named struct instead of a closure.\n\nr? @Aaron1011", "tree": {"sha": "314e6ed97dbc2ace70df03241836406f6558989d", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/314e6ed97dbc2ace70df03241836406f6558989d"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/1873ca55b3fa296a149eaf57b975583dc40cf67b", "comment_count": 0, "verification": {"verified": true, "reason": "valid", "signature": "-----BEGIN PGP SIGNATURE-----\n\nwsBcBAABCAAQBQJfnSUlCRBK7hj4Ov3rIwAAdHIIAIHSLm9rp8sXd5hxHUbRLXeH\nXMSxHQ1IXU0G4c3zZG91LJWLY7W/4V4iPy6Oa2c6Pb9wno/7CTWdw7G4H7mbl4TK\nNfomASfWkDCjh1qk8rA2lVqZ8P1sYuojxBA8sa2Xw168FRGi2zCUKY0MOxKhS6J5\ni7fh0ZrKgYGjNtca8kxriHIcLpaSk96uN+2ubcbf/sSjXOntZhx+8/8XRtCVTd6Q\nw6E9KUhdy7t5Uzpzrs88S5spdqaHvFjTrwU7z5064C3f++cocoCS5RHClQDkcAxx\n2ZuFiKrUErOmFUKe6t/yZpkZABWX78ucvKBGH3kKkZfBHrpj3/elNeeJiNTnKg4=\n=8BHf\n-----END PGP SIGNATURE-----\n", "payload": "tree 314e6ed97dbc2ace70df03241836406f6558989d\nparent 3601f9d40bdcd8bb7e3057e1d66c0b187e40ed9e\nparent d0c63bccc5f5214fb0defb974dfe75a2ea3ef6cb\nauthor Mara Bos <m-ou.se@m-ou.se> 1604134181 +0100\ncommitter GitHub <noreply@github.com> 1604134181 +0100\n\nRollup merge of #78587 - petrochenkov:lazytok, r=Aaron1011\n\nparser: Cleanup `LazyTokenStream` and avoid some clones\n\nby using a named struct instead of a closure.\n\nr? @Aaron1011\n"}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/1873ca55b3fa296a149eaf57b975583dc40cf67b", "html_url": "https://github.com/rust-lang/rust/commit/1873ca55b3fa296a149eaf57b975583dc40cf67b", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/1873ca55b3fa296a149eaf57b975583dc40cf67b/comments", "author": {"login": "m-ou-se", "id": 783247, "node_id": "MDQ6VXNlcjc4MzI0Nw==", "avatar_url": "https://avatars.githubusercontent.com/u/783247?v=4", "gravatar_id": "", "url": "https://api.github.com/users/m-ou-se", "html_url": "https://github.com/m-ou-se", "followers_url": "https://api.github.com/users/m-ou-se/followers", "following_url": "https://api.github.com/users/m-ou-se/following{/other_user}", "gists_url": "https://api.github.com/users/m-ou-se/gists{/gist_id}", "starred_url": "https://api.github.com/users/m-ou-se/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/m-ou-se/subscriptions", "organizations_url": "https://api.github.com/users/m-ou-se/orgs", "repos_url": "https://api.github.com/users/m-ou-se/repos", "events_url": "https://api.github.com/users/m-ou-se/events{/privacy}", "received_events_url": "https://api.github.com/users/m-ou-se/received_events", "type": "User", "site_admin": false}, "committer": {"login": "web-flow", "id": 19864447, "node_id": "MDQ6VXNlcjE5ODY0NDQ3", "avatar_url": "https://avatars.githubusercontent.com/u/19864447?v=4", "gravatar_id": "", "url": "https://api.github.com/users/web-flow", "html_url": "https://github.com/web-flow", "followers_url": "https://api.github.com/users/web-flow/followers", "following_url": "https://api.github.com/users/web-flow/following{/other_user}", "gists_url": "https://api.github.com/users/web-flow/gists{/gist_id}", "starred_url": "https://api.github.com/users/web-flow/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/web-flow/subscriptions", "organizations_url": "https://api.github.com/users/web-flow/orgs", "repos_url": "https://api.github.com/users/web-flow/repos", "events_url": "https://api.github.com/users/web-flow/events{/privacy}", "received_events_url": "https://api.github.com/users/web-flow/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "3601f9d40bdcd8bb7e3057e1d66c0b187e40ed9e", "url": "https://api.github.com/repos/rust-lang/rust/commits/3601f9d40bdcd8bb7e3057e1d66c0b187e40ed9e", "html_url": "https://github.com/rust-lang/rust/commit/3601f9d40bdcd8bb7e3057e1d66c0b187e40ed9e"}, {"sha": "d0c63bccc5f5214fb0defb974dfe75a2ea3ef6cb", "url": "https://api.github.com/repos/rust-lang/rust/commits/d0c63bccc5f5214fb0defb974dfe75a2ea3ef6cb", "html_url": "https://github.com/rust-lang/rust/commit/d0c63bccc5f5214fb0defb974dfe75a2ea3ef6cb"}], "stats": {"total": 164, "additions": 77, "deletions": 87}, "files": [{"sha": "b53acb97aeb9e6596601cdef63713a8d839c99b9", "filename": "compiler/rustc_ast/src/tokenstream.rs", "status": "modified", "additions": 22, "deletions": 43, "changes": 65, "blob_url": "https://github.com/rust-lang/rust/blob/1873ca55b3fa296a149eaf57b975583dc40cf67b/compiler%2Frustc_ast%2Fsrc%2Ftokenstream.rs", "raw_url": "https://github.com/rust-lang/rust/raw/1873ca55b3fa296a149eaf57b975583dc40cf67b/compiler%2Frustc_ast%2Fsrc%2Ftokenstream.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_ast%2Fsrc%2Ftokenstream.rs?ref=1873ca55b3fa296a149eaf57b975583dc40cf67b", "patch": "@@ -22,7 +22,7 @@ use rustc_serialize::{Decodable, Decoder, Encodable, Encoder};\n use rustc_span::{Span, DUMMY_SP};\n use smallvec::{smallvec, SmallVec};\n \n-use std::{iter, mem};\n+use std::{fmt, iter, mem};\n \n /// When the main rust parser encounters a syntax-extension invocation, it\n /// parses the arguments to the invocation as a token-tree. This is a very\n@@ -120,72 +120,51 @@ where\n     }\n }\n \n-// A cloneable callback which produces a `TokenStream`. Each clone\n-// of this should produce the same `TokenStream`\n-pub trait CreateTokenStream: sync::Send + sync::Sync + FnOnce() -> TokenStream {\n-    // Workaround for the fact that `Clone` is not object-safe\n-    fn clone_it(&self) -> Box<dyn CreateTokenStream>;\n+pub trait CreateTokenStream: sync::Send + sync::Sync {\n+    fn create_token_stream(&self) -> TokenStream;\n }\n \n-impl<F: 'static + Clone + sync::Send + sync::Sync + FnOnce() -> TokenStream> CreateTokenStream\n-    for F\n-{\n-    fn clone_it(&self) -> Box<dyn CreateTokenStream> {\n-        Box::new(self.clone())\n-    }\n-}\n-\n-impl Clone for Box<dyn CreateTokenStream> {\n-    fn clone(&self) -> Self {\n-        let val: &(dyn CreateTokenStream) = &**self;\n-        val.clone_it()\n+impl CreateTokenStream for TokenStream {\n+    fn create_token_stream(&self) -> TokenStream {\n+        self.clone()\n     }\n }\n \n-/// A lazy version of `TokenStream`, which may defer creation\n+/// A lazy version of `TokenStream`, which defers creation\n /// of an actual `TokenStream` until it is needed.\n-pub type LazyTokenStream = Lrc<LazyTokenStreamInner>;\n-\n+/// `Box` is here only to reduce the structure size.\n #[derive(Clone)]\n-pub enum LazyTokenStreamInner {\n-    Lazy(Box<dyn CreateTokenStream>),\n-    Ready(TokenStream),\n-}\n+pub struct LazyTokenStream(Lrc<Box<dyn CreateTokenStream>>);\n \n-impl std::fmt::Debug for LazyTokenStreamInner {\n-    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {\n-        match self {\n-            LazyTokenStreamInner::Lazy(..) => f.debug_struct(\"LazyTokenStream::Lazy\").finish(),\n-            LazyTokenStreamInner::Ready(..) => f.debug_struct(\"LazyTokenStream::Ready\").finish(),\n-        }\n+impl LazyTokenStream {\n+    pub fn new(inner: impl CreateTokenStream + 'static) -> LazyTokenStream {\n+        LazyTokenStream(Lrc::new(Box::new(inner)))\n+    }\n+\n+    pub fn create_token_stream(&self) -> TokenStream {\n+        self.0.create_token_stream()\n     }\n }\n \n-impl LazyTokenStreamInner {\n-    pub fn into_token_stream(&self) -> TokenStream {\n-        match self {\n-            // Note that we do not cache this. If this ever becomes a performance\n-            // problem, we should investigate wrapping `LazyTokenStreamInner`\n-            // in a lock\n-            LazyTokenStreamInner::Lazy(cb) => (cb.clone())(),\n-            LazyTokenStreamInner::Ready(stream) => stream.clone(),\n-        }\n+impl fmt::Debug for LazyTokenStream {\n+    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {\n+        fmt::Debug::fmt(\"LazyTokenStream\", f)\n     }\n }\n \n-impl<S: Encoder> Encodable<S> for LazyTokenStreamInner {\n+impl<S: Encoder> Encodable<S> for LazyTokenStream {\n     fn encode(&self, _s: &mut S) -> Result<(), S::Error> {\n         panic!(\"Attempted to encode LazyTokenStream\");\n     }\n }\n \n-impl<D: Decoder> Decodable<D> for LazyTokenStreamInner {\n+impl<D: Decoder> Decodable<D> for LazyTokenStream {\n     fn decode(_d: &mut D) -> Result<Self, D::Error> {\n         panic!(\"Attempted to decode LazyTokenStream\");\n     }\n }\n \n-impl<CTX> HashStable<CTX> for LazyTokenStreamInner {\n+impl<CTX> HashStable<CTX> for LazyTokenStream {\n     fn hash_stable(&self, _hcx: &mut CTX, _hasher: &mut StableHasher) {\n         panic!(\"Attempted to compute stable hash for LazyTokenStream\");\n     }"}, {"sha": "c124ab64218620600bd93b11c33790a9bff22bd0", "filename": "compiler/rustc_expand/src/config.rs", "status": "modified", "additions": 6, "deletions": 7, "changes": 13, "blob_url": "https://github.com/rust-lang/rust/blob/1873ca55b3fa296a149eaf57b975583dc40cf67b/compiler%2Frustc_expand%2Fsrc%2Fconfig.rs", "raw_url": "https://github.com/rust-lang/rust/raw/1873ca55b3fa296a149eaf57b975583dc40cf67b/compiler%2Frustc_expand%2Fsrc%2Fconfig.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_expand%2Fsrc%2Fconfig.rs?ref=1873ca55b3fa296a149eaf57b975583dc40cf67b", "patch": "@@ -4,12 +4,11 @@ use rustc_ast::attr::HasAttrs;\n use rustc_ast::mut_visit::*;\n use rustc_ast::ptr::P;\n use rustc_ast::token::{DelimToken, Token, TokenKind};\n-use rustc_ast::tokenstream::{DelimSpan, LazyTokenStreamInner, Spacing, TokenStream, TokenTree};\n+use rustc_ast::tokenstream::{DelimSpan, LazyTokenStream, Spacing, TokenStream, TokenTree};\n use rustc_ast::{self as ast, AttrItem, Attribute, MetaItem};\n use rustc_attr as attr;\n use rustc_data_structures::fx::FxHashMap;\n use rustc_data_structures::map_in_place::MapInPlace;\n-use rustc_data_structures::sync::Lrc;\n use rustc_errors::{error_code, struct_span_err, Applicability, Handler};\n use rustc_feature::{Feature, Features, State as FeatureState};\n use rustc_feature::{\n@@ -303,7 +302,7 @@ impl<'a> StripUnconfigured<'a> {\n \n                 // Use the `#` in `#[cfg_attr(pred, attr)]` as the `#` token\n                 // for `attr` when we expand it to `#[attr]`\n-                let pound_token = orig_tokens.into_token_stream().trees().next().unwrap();\n+                let pound_token = orig_tokens.create_token_stream().trees().next().unwrap();\n                 if !matches!(pound_token, TokenTree::Token(Token { kind: TokenKind::Pound, .. })) {\n                     panic!(\"Bad tokens for attribute {:?}\", attr);\n                 }\n@@ -313,16 +312,16 @@ impl<'a> StripUnconfigured<'a> {\n                     DelimSpan::from_single(pound_token.span()),\n                     DelimToken::Bracket,\n                     item.tokens\n-                        .clone()\n+                        .as_ref()\n                         .unwrap_or_else(|| panic!(\"Missing tokens for {:?}\", item))\n-                        .into_token_stream(),\n+                        .create_token_stream(),\n                 );\n \n                 let mut attr = attr::mk_attr_from_item(attr.style, item, span);\n-                attr.tokens = Some(Lrc::new(LazyTokenStreamInner::Ready(TokenStream::new(vec![\n+                attr.tokens = Some(LazyTokenStream::new(TokenStream::new(vec![\n                     (pound_token, Spacing::Alone),\n                     (bracket_group, Spacing::Alone),\n-                ]))));\n+                ])));\n                 self.process_cfg_attr(attr)\n             })\n             .collect()"}, {"sha": "e851451269e324809e349a23f249abe4b84028a9", "filename": "compiler/rustc_parse/src/lib.rs", "status": "modified", "additions": 12, "deletions": 11, "changes": 23, "blob_url": "https://github.com/rust-lang/rust/blob/1873ca55b3fa296a149eaf57b975583dc40cf67b/compiler%2Frustc_parse%2Fsrc%2Flib.rs", "raw_url": "https://github.com/rust-lang/rust/raw/1873ca55b3fa296a149eaf57b975583dc40cf67b/compiler%2Frustc_parse%2Fsrc%2Flib.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_parse%2Fsrc%2Flib.rs?ref=1873ca55b3fa296a149eaf57b975583dc40cf67b", "patch": "@@ -249,29 +249,30 @@ pub fn nt_to_tokenstream(nt: &Nonterminal, sess: &ParseSess, span: Span) -> Toke\n     // came from. Here we attempt to extract these lossless token streams\n     // before we fall back to the stringification.\n \n-    let convert_tokens = |tokens: Option<LazyTokenStream>| tokens.map(|t| t.into_token_stream());\n+    let convert_tokens =\n+        |tokens: &Option<LazyTokenStream>| tokens.as_ref().map(|t| t.create_token_stream());\n \n     let tokens = match *nt {\n         Nonterminal::NtItem(ref item) => prepend_attrs(&item.attrs, item.tokens.as_ref()),\n-        Nonterminal::NtBlock(ref block) => convert_tokens(block.tokens.clone()),\n+        Nonterminal::NtBlock(ref block) => convert_tokens(&block.tokens),\n         Nonterminal::NtStmt(ref stmt) => {\n             // FIXME: We currently only collect tokens for `:stmt`\n             // matchers in `macro_rules!` macros. When we start collecting\n             // tokens for attributes on statements, we will need to prepend\n             // attributes here\n-            convert_tokens(stmt.tokens.clone())\n+            convert_tokens(&stmt.tokens)\n         }\n-        Nonterminal::NtPat(ref pat) => convert_tokens(pat.tokens.clone()),\n-        Nonterminal::NtTy(ref ty) => convert_tokens(ty.tokens.clone()),\n+        Nonterminal::NtPat(ref pat) => convert_tokens(&pat.tokens),\n+        Nonterminal::NtTy(ref ty) => convert_tokens(&ty.tokens),\n         Nonterminal::NtIdent(ident, is_raw) => {\n             Some(tokenstream::TokenTree::token(token::Ident(ident.name, is_raw), ident.span).into())\n         }\n         Nonterminal::NtLifetime(ident) => {\n             Some(tokenstream::TokenTree::token(token::Lifetime(ident.name), ident.span).into())\n         }\n-        Nonterminal::NtMeta(ref attr) => convert_tokens(attr.tokens.clone()),\n-        Nonterminal::NtPath(ref path) => convert_tokens(path.tokens.clone()),\n-        Nonterminal::NtVis(ref vis) => convert_tokens(vis.tokens.clone()),\n+        Nonterminal::NtMeta(ref attr) => convert_tokens(&attr.tokens),\n+        Nonterminal::NtPath(ref path) => convert_tokens(&path.tokens),\n+        Nonterminal::NtVis(ref vis) => convert_tokens(&vis.tokens),\n         Nonterminal::NtTT(ref tt) => Some(tt.clone().into()),\n         Nonterminal::NtExpr(ref expr) | Nonterminal::NtLiteral(ref expr) => {\n             if expr.tokens.is_none() {\n@@ -604,7 +605,7 @@ fn prepend_attrs(\n     attrs: &[ast::Attribute],\n     tokens: Option<&tokenstream::LazyTokenStream>,\n ) -> Option<tokenstream::TokenStream> {\n-    let tokens = tokens?.clone().into_token_stream();\n+    let tokens = tokens?.create_token_stream();\n     if attrs.is_empty() {\n         return Some(tokens);\n     }\n@@ -617,9 +618,9 @@ fn prepend_attrs(\n         );\n         builder.push(\n             attr.tokens\n-                .clone()\n+                .as_ref()\n                 .unwrap_or_else(|| panic!(\"Attribute {:?} is missing tokens!\", attr))\n-                .into_token_stream(),\n+                .create_token_stream(),\n         );\n     }\n     builder.push(tokens);"}, {"sha": "da1c54e88b5e20ea997054a67c34485e23d3ac56", "filename": "compiler/rustc_parse/src/parser/mod.rs", "status": "modified", "additions": 37, "deletions": 26, "changes": 63, "blob_url": "https://github.com/rust-lang/rust/blob/1873ca55b3fa296a149eaf57b975583dc40cf67b/compiler%2Frustc_parse%2Fsrc%2Fparser%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/1873ca55b3fa296a149eaf57b975583dc40cf67b/compiler%2Frustc_parse%2Fsrc%2Fparser%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_parse%2Fsrc%2Fparser%2Fmod.rs?ref=1873ca55b3fa296a149eaf57b975583dc40cf67b", "patch": "@@ -16,8 +16,8 @@ pub use path::PathStyle;\n \n use rustc_ast::ptr::P;\n use rustc_ast::token::{self, DelimToken, Token, TokenKind};\n-use rustc_ast::tokenstream::{self, DelimSpan, LazyTokenStream, LazyTokenStreamInner, Spacing};\n-use rustc_ast::tokenstream::{TokenStream, TokenTree};\n+use rustc_ast::tokenstream::{self, DelimSpan, LazyTokenStream, Spacing};\n+use rustc_ast::tokenstream::{CreateTokenStream, TokenStream, TokenTree};\n use rustc_ast::DUMMY_NODE_ID;\n use rustc_ast::{self as ast, AnonConst, AttrStyle, AttrVec, Const, CrateSugar, Extern, Unsafe};\n use rustc_ast::{Async, Expr, ExprKind, MacArgs, MacDelimiter, Mutability, StrLit};\n@@ -1199,15 +1199,12 @@ impl<'a> Parser<'a> {\n         f: impl FnOnce(&mut Self) -> PResult<'a, R>,\n     ) -> PResult<'a, (R, Option<LazyTokenStream>)> {\n         let start_token = (self.token.clone(), self.token_spacing);\n-        let mut cursor_snapshot = self.token_cursor.clone();\n+        let cursor_snapshot = self.token_cursor.clone();\n \n         let ret = f(self)?;\n \n-        let new_calls = self.token_cursor.num_next_calls;\n-        let num_calls = new_calls - cursor_snapshot.num_next_calls;\n-        let desugar_doc_comments = self.desugar_doc_comments;\n-\n         // We didn't capture any tokens\n+        let num_calls = self.token_cursor.num_next_calls - cursor_snapshot.num_next_calls;\n         if num_calls == 0 {\n             return Ok((ret, None));\n         }\n@@ -1220,27 +1217,41 @@ impl<'a> Parser<'a> {\n         //\n         // This also makes `Parser` very cheap to clone, since\n         // there is no intermediate collection buffer to clone.\n-        let lazy_cb = move || {\n-            // The token produced by the final call to `next` or `next_desugared`\n-            // was not actually consumed by the callback. The combination\n-            // of chaining the initial token and using `take` produces the desired\n-            // result - we produce an empty `TokenStream` if no calls were made,\n-            // and omit the final token otherwise.\n-            let tokens = std::iter::once(start_token)\n-                .chain((0..num_calls).map(|_| {\n-                    if desugar_doc_comments {\n-                        cursor_snapshot.next_desugared()\n-                    } else {\n-                        cursor_snapshot.next()\n-                    }\n-                }))\n-                .take(num_calls);\n+        struct LazyTokenStreamImpl {\n+            start_token: (Token, Spacing),\n+            cursor_snapshot: TokenCursor,\n+            num_calls: usize,\n+            desugar_doc_comments: bool,\n+        }\n+        impl CreateTokenStream for LazyTokenStreamImpl {\n+            fn create_token_stream(&self) -> TokenStream {\n+                // The token produced by the final call to `next` or `next_desugared`\n+                // was not actually consumed by the callback. The combination\n+                // of chaining the initial token and using `take` produces the desired\n+                // result - we produce an empty `TokenStream` if no calls were made,\n+                // and omit the final token otherwise.\n+                let mut cursor_snapshot = self.cursor_snapshot.clone();\n+                let tokens = std::iter::once(self.start_token.clone())\n+                    .chain((0..self.num_calls).map(|_| {\n+                        if self.desugar_doc_comments {\n+                            cursor_snapshot.next_desugared()\n+                        } else {\n+                            cursor_snapshot.next()\n+                        }\n+                    }))\n+                    .take(self.num_calls);\n \n-            make_token_stream(tokens)\n-        };\n-        let stream = LazyTokenStream::new(LazyTokenStreamInner::Lazy(Box::new(lazy_cb)));\n+                make_token_stream(tokens)\n+            }\n+        }\n \n-        Ok((ret, Some(stream)))\n+        let lazy_impl = LazyTokenStreamImpl {\n+            start_token,\n+            cursor_snapshot,\n+            num_calls,\n+            desugar_doc_comments: self.desugar_doc_comments,\n+        };\n+        Ok((ret, Some(LazyTokenStream::new(lazy_impl))))\n     }\n \n     /// `::{` or `::*`"}]}