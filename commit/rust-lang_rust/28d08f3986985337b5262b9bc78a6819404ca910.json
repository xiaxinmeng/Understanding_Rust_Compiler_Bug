{"sha": "28d08f3986985337b5262b9bc78a6819404ca910", "node_id": "MDY6Q29tbWl0NzI0NzEyOjI4ZDA4ZjM5ODY5ODUzMzdiNTI2MmI5YmM3OGE2ODE5NDA0Y2E5MTA=", "commit": {"author": {"name": "Mazdak Farrokhzad", "email": "twingoow@gmail.com", "date": "2019-10-14T05:36:59Z"}, "committer": {"name": "GitHub", "email": "noreply@github.com", "date": "2019-10-14T05:36:59Z"}, "message": "Rollup merge of #65392 - Centril:nt-to-tt, r=Mark-Simulacrum\n\nMove `Nonterminal::to_tokenstream` to parser & don't rely directly on parser in lowering\n\nSplit out from https://github.com/rust-lang/rust/pull/65324.\n\nr? @petrochenkov", "tree": {"sha": "288b7850100b56a907f3384e0062f7729c641c59", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/288b7850100b56a907f3384e0062f7729c641c59"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/28d08f3986985337b5262b9bc78a6819404ca910", "comment_count": 0, "verification": {"verified": true, "reason": "valid", "signature": "-----BEGIN PGP SIGNATURE-----\n\nwsBcBAABCAAQBQJdpAl7CRBK7hj4Ov3rIwAAdHIIAFG8hUVzR5WtYEsMSb4X+9Yp\nYRBFsusdX/XnrPkEIFIMBSuaDlyWvSZtANwpVoZxd3GFnOCZIVEFmtUKG6pqBLd3\n438tVn+8AtalUn4t9XtOxaoufl/YkQWW6NFUIGSRes95tGqug9mbaFZueGCsfrTV\nNCbxJQVWUKl6Q7olIhVsEOx0OLHWDoMOlsWQDEedzjsHPzNS0vj5Ja0v5XZh22Js\nxiDR9f/XXdpIgm/v9EbMFVUh7py+zMtjTfakb6qc/7p/PB0Bw7w1FsINDlOlmSKT\nYn9rgCBbaULdVUok9/NhGvzzQzop9VLVIUjQ6MFKf4Z6Q9npc3y8DbSCFQsojgA=\n=7qTR\n-----END PGP SIGNATURE-----\n", "payload": "tree 288b7850100b56a907f3384e0062f7729c641c59\nparent e29a6fcfa2bab56728fd3882ded01220ad7421b6\nparent 07e946caf7a4b9e5781b15f75b927b9e695b4f7c\nauthor Mazdak Farrokhzad <twingoow@gmail.com> 1571031419 +0200\ncommitter GitHub <noreply@github.com> 1571031419 +0200\n\nRollup merge of #65392 - Centril:nt-to-tt, r=Mark-Simulacrum\n\nMove `Nonterminal::to_tokenstream` to parser & don't rely directly on parser in lowering\n\nSplit out from https://github.com/rust-lang/rust/pull/65324.\n\nr? @petrochenkov\n"}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/28d08f3986985337b5262b9bc78a6819404ca910", "html_url": "https://github.com/rust-lang/rust/commit/28d08f3986985337b5262b9bc78a6819404ca910", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/28d08f3986985337b5262b9bc78a6819404ca910/comments", "author": {"login": "Centril", "id": 855702, "node_id": "MDQ6VXNlcjg1NTcwMg==", "avatar_url": "https://avatars.githubusercontent.com/u/855702?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Centril", "html_url": "https://github.com/Centril", "followers_url": "https://api.github.com/users/Centril/followers", "following_url": "https://api.github.com/users/Centril/following{/other_user}", "gists_url": "https://api.github.com/users/Centril/gists{/gist_id}", "starred_url": "https://api.github.com/users/Centril/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Centril/subscriptions", "organizations_url": "https://api.github.com/users/Centril/orgs", "repos_url": "https://api.github.com/users/Centril/repos", "events_url": "https://api.github.com/users/Centril/events{/privacy}", "received_events_url": "https://api.github.com/users/Centril/received_events", "type": "User", "site_admin": false}, "committer": {"login": "web-flow", "id": 19864447, "node_id": "MDQ6VXNlcjE5ODY0NDQ3", "avatar_url": "https://avatars.githubusercontent.com/u/19864447?v=4", "gravatar_id": "", "url": "https://api.github.com/users/web-flow", "html_url": "https://github.com/web-flow", "followers_url": "https://api.github.com/users/web-flow/followers", "following_url": "https://api.github.com/users/web-flow/following{/other_user}", "gists_url": "https://api.github.com/users/web-flow/gists{/gist_id}", "starred_url": "https://api.github.com/users/web-flow/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/web-flow/subscriptions", "organizations_url": "https://api.github.com/users/web-flow/orgs", "repos_url": "https://api.github.com/users/web-flow/repos", "events_url": "https://api.github.com/users/web-flow/events{/privacy}", "received_events_url": "https://api.github.com/users/web-flow/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "e29a6fcfa2bab56728fd3882ded01220ad7421b6", "url": "https://api.github.com/repos/rust-lang/rust/commits/e29a6fcfa2bab56728fd3882ded01220ad7421b6", "html_url": "https://github.com/rust-lang/rust/commit/e29a6fcfa2bab56728fd3882ded01220ad7421b6"}, {"sha": "07e946caf7a4b9e5781b15f75b927b9e695b4f7c", "url": "https://api.github.com/repos/rust-lang/rust/commits/07e946caf7a4b9e5781b15f75b927b9e695b4f7c", "html_url": "https://github.com/rust-lang/rust/commit/07e946caf7a4b9e5781b15f75b927b9e695b4f7c"}], "stats": {"total": 294, "additions": 152, "deletions": 142}, "files": [{"sha": "747848edc7a8d24c24f8037558fd21b122bbaaab", "filename": "src/librustc/hir/lowering.rs", "status": "modified", "additions": 12, "deletions": 2, "changes": 14, "blob_url": "https://github.com/rust-lang/rust/blob/28d08f3986985337b5262b9bc78a6819404ca910/src%2Flibrustc%2Fhir%2Flowering.rs", "raw_url": "https://github.com/rust-lang/rust/raw/28d08f3986985337b5262b9bc78a6819404ca910/src%2Flibrustc%2Fhir%2Flowering.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Fhir%2Flowering.rs?ref=28d08f3986985337b5262b9bc78a6819404ca910", "patch": "@@ -70,7 +70,8 @@ use syntax::print::pprust;\n use syntax::source_map::{respan, ExpnData, ExpnKind, DesugaringKind, Spanned};\n use syntax::symbol::{kw, sym, Symbol};\n use syntax::tokenstream::{TokenStream, TokenTree};\n-use syntax::parse::token::{self, Token};\n+use syntax::parse::token::{self, Nonterminal, Token};\n+use syntax::parse::ParseSess;\n use syntax::visit::{self, Visitor};\n use syntax_pos::Span;\n \n@@ -86,6 +87,11 @@ pub struct LoweringContext<'a> {\n \n     resolver: &'a mut dyn Resolver,\n \n+    /// HACK(Centril): there is a cyclic dependency between the parser and lowering\n+    /// if we don't have this function pointer. To avoid that dependency so that\n+    /// librustc is independent of the parser, we use dynamic dispatch here.\n+    nt_to_tokenstream: NtToTokenstream,\n+\n     /// The items being lowered are collected here.\n     items: BTreeMap<hir::HirId, hir::Item>,\n \n@@ -180,6 +186,8 @@ pub trait Resolver {\n     fn has_derives(&self, node_id: NodeId, derives: SpecialDerives) -> bool;\n }\n \n+type NtToTokenstream = fn(&Nonterminal, &ParseSess, Span) -> TokenStream;\n+\n /// Context of `impl Trait` in code, which determines whether it is allowed in an HIR subtree,\n /// and if so, what meaning it has.\n #[derive(Debug)]\n@@ -236,6 +244,7 @@ pub fn lower_crate(\n     dep_graph: &DepGraph,\n     krate: &Crate,\n     resolver: &mut dyn Resolver,\n+    nt_to_tokenstream: NtToTokenstream,\n ) -> hir::Crate {\n     // We're constructing the HIR here; we don't care what we will\n     // read, since we haven't even constructed the *input* to\n@@ -249,6 +258,7 @@ pub fn lower_crate(\n         sess,\n         cstore,\n         resolver,\n+        nt_to_tokenstream,\n         items: BTreeMap::new(),\n         trait_items: BTreeMap::new(),\n         impl_items: BTreeMap::new(),\n@@ -1022,7 +1032,7 @@ impl<'a> LoweringContext<'a> {\n     fn lower_token(&mut self, token: Token) -> TokenStream {\n         match token.kind {\n             token::Interpolated(nt) => {\n-                let tts = nt.to_tokenstream(&self.sess.parse_sess, token.span);\n+                let tts = (self.nt_to_tokenstream)(&nt, &self.sess.parse_sess, token.span);\n                 self.lower_token_stream(tts)\n             }\n             _ => TokenTree::Token(token).into(),"}, {"sha": "328798862b864b9ab971d0b2c78280f13ea18f3e", "filename": "src/librustc_interface/passes.rs", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "blob_url": "https://github.com/rust-lang/rust/blob/28d08f3986985337b5262b9bc78a6819404ca910/src%2Flibrustc_interface%2Fpasses.rs", "raw_url": "https://github.com/rust-lang/rust/raw/28d08f3986985337b5262b9bc78a6819404ca910/src%2Flibrustc_interface%2Fpasses.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_interface%2Fpasses.rs?ref=28d08f3986985337b5262b9bc78a6819404ca910", "patch": "@@ -541,7 +541,8 @@ pub fn lower_to_hir(\n ) -> Result<hir::map::Forest> {\n     // Lower AST to HIR.\n     let hir_forest = time(sess, \"lowering AST -> HIR\", || {\n-        let hir_crate = lower_crate(sess, cstore, &dep_graph, &krate, resolver);\n+        let nt_to_tokenstream = syntax::parse::nt_to_tokenstream;\n+        let hir_crate = lower_crate(sess, cstore, &dep_graph, &krate, resolver, nt_to_tokenstream);\n \n         if sess.opts.debugging_opts.hir_stats {\n             hir_stats::print_hir_stats(&hir_crate);"}, {"sha": "65037c4a0b8baef06bce86c1360d62c8cbc96455", "filename": "src/libsyntax/ext/proc_macro_server.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/28d08f3986985337b5262b9bc78a6819404ca910/src%2Flibsyntax%2Fext%2Fproc_macro_server.rs", "raw_url": "https://github.com/rust-lang/rust/raw/28d08f3986985337b5262b9bc78a6819404ca910/src%2Flibsyntax%2Fext%2Fproc_macro_server.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fext%2Fproc_macro_server.rs?ref=28d08f3986985337b5262b9bc78a6819404ca910", "patch": "@@ -175,7 +175,7 @@ impl FromInternal<(TreeAndJoint, &'_ ParseSess, &'_ mut Vec<Self>)>\n             }\n \n             Interpolated(nt) => {\n-                let stream = nt.to_tokenstream(sess, span);\n+                let stream = parse::nt_to_tokenstream(&nt, sess, span);\n                 TokenTree::Group(Group {\n                     delimiter: Delimiter::None,\n                     stream,"}, {"sha": "0df695d37b94f10810164165c67ed80797d1d17b", "filename": "src/libsyntax/parse/mod.rs", "status": "modified", "additions": 134, "deletions": 4, "changes": 138, "blob_url": "https://github.com/rust-lang/rust/blob/28d08f3986985337b5262b9bc78a6819404ca910/src%2Flibsyntax%2Fparse%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/28d08f3986985337b5262b9bc78a6819404ca910/src%2Flibsyntax%2Fparse%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Fmod.rs?ref=28d08f3986985337b5262b9bc78a6819404ca910", "patch": "@@ -4,10 +4,9 @@ use crate::ast::{self, CrateConfig, NodeId};\n use crate::early_buffered_lints::{BufferedEarlyLint, BufferedEarlyLintId};\n use crate::source_map::{SourceMap, FilePathMapping};\n use crate::feature_gate::UnstableFeatures;\n-use crate::parse::parser::Parser;\n-use crate::parse::parser::emit_unclosed_delims;\n-use crate::parse::token::TokenKind;\n-use crate::tokenstream::{TokenStream, TokenTree};\n+use crate::parse::parser::{Parser, emit_unclosed_delims};\n+use crate::parse::token::{Nonterminal, TokenKind};\n+use crate::tokenstream::{self, TokenStream, TokenTree};\n use crate::print::pprust;\n use crate::symbol::Symbol;\n \n@@ -24,6 +23,8 @@ use std::borrow::Cow;\n use std::path::{Path, PathBuf};\n use std::str;\n \n+use log::info;\n+\n #[cfg(test)]\n mod tests;\n \n@@ -407,3 +408,132 @@ impl SeqSep {\n         }\n     }\n }\n+\n+// NOTE(Centril): The following probably shouldn't be here but it acknowledges the\n+// fact that architecturally, we are using parsing (read on below to understand why).\n+\n+pub fn nt_to_tokenstream(nt: &Nonterminal, sess: &ParseSess, span: Span) -> TokenStream {\n+    // A `Nonterminal` is often a parsed AST item. At this point we now\n+    // need to convert the parsed AST to an actual token stream, e.g.\n+    // un-parse it basically.\n+    //\n+    // Unfortunately there's not really a great way to do that in a\n+    // guaranteed lossless fashion right now. The fallback here is to just\n+    // stringify the AST node and reparse it, but this loses all span\n+    // information.\n+    //\n+    // As a result, some AST nodes are annotated with the token stream they\n+    // came from. Here we attempt to extract these lossless token streams\n+    // before we fall back to the stringification.\n+    let tokens = match *nt {\n+        Nonterminal::NtItem(ref item) => {\n+            prepend_attrs(sess, &item.attrs, item.tokens.as_ref(), span)\n+        }\n+        Nonterminal::NtTraitItem(ref item) => {\n+            prepend_attrs(sess, &item.attrs, item.tokens.as_ref(), span)\n+        }\n+        Nonterminal::NtImplItem(ref item) => {\n+            prepend_attrs(sess, &item.attrs, item.tokens.as_ref(), span)\n+        }\n+        Nonterminal::NtIdent(ident, is_raw) => {\n+            Some(tokenstream::TokenTree::token(token::Ident(ident.name, is_raw), ident.span).into())\n+        }\n+        Nonterminal::NtLifetime(ident) => {\n+            Some(tokenstream::TokenTree::token(token::Lifetime(ident.name), ident.span).into())\n+        }\n+        Nonterminal::NtTT(ref tt) => {\n+            Some(tt.clone().into())\n+        }\n+        _ => None,\n+    };\n+\n+    // FIXME(#43081): Avoid this pretty-print + reparse hack\n+    let source = pprust::nonterminal_to_string(nt);\n+    let filename = FileName::macro_expansion_source_code(&source);\n+    let tokens_for_real = parse_stream_from_source_str(filename, source, sess, Some(span));\n+\n+    // During early phases of the compiler the AST could get modified\n+    // directly (e.g., attributes added or removed) and the internal cache\n+    // of tokens my not be invalidated or updated. Consequently if the\n+    // \"lossless\" token stream disagrees with our actual stringification\n+    // (which has historically been much more battle-tested) then we go\n+    // with the lossy stream anyway (losing span information).\n+    //\n+    // Note that the comparison isn't `==` here to avoid comparing spans,\n+    // but it *also* is a \"probable\" equality which is a pretty weird\n+    // definition. We mostly want to catch actual changes to the AST\n+    // like a `#[cfg]` being processed or some weird `macro_rules!`\n+    // expansion.\n+    //\n+    // What we *don't* want to catch is the fact that a user-defined\n+    // literal like `0xf` is stringified as `15`, causing the cached token\n+    // stream to not be literal `==` token-wise (ignoring spans) to the\n+    // token stream we got from stringification.\n+    //\n+    // Instead the \"probably equal\" check here is \"does each token\n+    // recursively have the same discriminant?\" We basically don't look at\n+    // the token values here and assume that such fine grained token stream\n+    // modifications, including adding/removing typically non-semantic\n+    // tokens such as extra braces and commas, don't happen.\n+    if let Some(tokens) = tokens {\n+        if tokens.probably_equal_for_proc_macro(&tokens_for_real) {\n+            return tokens\n+        }\n+        info!(\"cached tokens found, but they're not \\\"probably equal\\\", \\\n+                going with stringified version\");\n+    }\n+    return tokens_for_real\n+}\n+\n+fn prepend_attrs(\n+    sess: &ParseSess,\n+    attrs: &[ast::Attribute],\n+    tokens: Option<&tokenstream::TokenStream>,\n+    span: syntax_pos::Span\n+) -> Option<tokenstream::TokenStream> {\n+    let tokens = tokens?;\n+    if attrs.len() == 0 {\n+        return Some(tokens.clone())\n+    }\n+    let mut builder = tokenstream::TokenStreamBuilder::new();\n+    for attr in attrs {\n+        assert_eq!(attr.style, ast::AttrStyle::Outer,\n+                   \"inner attributes should prevent cached tokens from existing\");\n+\n+        let source = pprust::attribute_to_string(attr);\n+        let macro_filename = FileName::macro_expansion_source_code(&source);\n+        if attr.is_sugared_doc {\n+            let stream = parse_stream_from_source_str(macro_filename, source, sess, Some(span));\n+            builder.push(stream);\n+            continue\n+        }\n+\n+        // synthesize # [ $path $tokens ] manually here\n+        let mut brackets = tokenstream::TokenStreamBuilder::new();\n+\n+        // For simple paths, push the identifier directly\n+        if attr.path.segments.len() == 1 && attr.path.segments[0].args.is_none() {\n+            let ident = attr.path.segments[0].ident;\n+            let token = token::Ident(ident.name, ident.as_str().starts_with(\"r#\"));\n+            brackets.push(tokenstream::TokenTree::token(token, ident.span));\n+\n+        // ... and for more complicated paths, fall back to a reparse hack that\n+        // should eventually be removed.\n+        } else {\n+            let stream = parse_stream_from_source_str(macro_filename, source, sess, Some(span));\n+            brackets.push(stream);\n+        }\n+\n+        brackets.push(attr.tokens.clone());\n+\n+        // The span we list here for `#` and for `[ ... ]` are both wrong in\n+        // that it encompasses more than each token, but it hopefully is \"good\n+        // enough\" for now at least.\n+        builder.push(tokenstream::TokenTree::token(token::Pound, attr.span));\n+        let delim_span = tokenstream::DelimSpan::from_single(attr.span);\n+        builder.push(tokenstream::TokenTree::Delimited(\n+            delim_span, token::DelimToken::Bracket, brackets.build().into()));\n+    }\n+    builder.push(tokens.clone());\n+    Some(builder.build())\n+}"}, {"sha": "eb74ab2b9192d8b73af9edae549d368f90dbbf09", "filename": "src/libsyntax/parse/token.rs", "status": "modified", "additions": 3, "deletions": 134, "changes": 137, "blob_url": "https://github.com/rust-lang/rust/blob/28d08f3986985337b5262b9bc78a6819404ca910/src%2Flibsyntax%2Fparse%2Ftoken.rs", "raw_url": "https://github.com/rust-lang/rust/raw/28d08f3986985337b5262b9bc78a6819404ca910/src%2Flibsyntax%2Fparse%2Ftoken.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Ftoken.rs?ref=28d08f3986985337b5262b9bc78a6819404ca910", "patch": "@@ -4,16 +4,13 @@ pub use DelimToken::*;\n pub use LitKind::*;\n pub use TokenKind::*;\n \n-use crate::ast::{self};\n-use crate::parse::{parse_stream_from_source_str, ParseSess};\n-use crate::print::pprust;\n+use crate::ast;\n use crate::ptr::P;\n use crate::symbol::kw;\n-use crate::tokenstream::{self, DelimSpan, TokenStream, TokenTree};\n+use crate::tokenstream::TokenTree;\n \n use syntax_pos::symbol::Symbol;\n-use syntax_pos::{self, Span, FileName, DUMMY_SP};\n-use log::info;\n+use syntax_pos::{self, Span, DUMMY_SP};\n \n use std::fmt;\n use std::mem;\n@@ -737,131 +734,3 @@ impl fmt::Debug for Nonterminal {\n         }\n     }\n }\n-\n-impl Nonterminal {\n-    pub fn to_tokenstream(&self, sess: &ParseSess, span: Span) -> TokenStream {\n-        // A `Nonterminal` is often a parsed AST item. At this point we now\n-        // need to convert the parsed AST to an actual token stream, e.g.\n-        // un-parse it basically.\n-        //\n-        // Unfortunately there's not really a great way to do that in a\n-        // guaranteed lossless fashion right now. The fallback here is to just\n-        // stringify the AST node and reparse it, but this loses all span\n-        // information.\n-        //\n-        // As a result, some AST nodes are annotated with the token stream they\n-        // came from. Here we attempt to extract these lossless token streams\n-        // before we fall back to the stringification.\n-        let tokens = match *self {\n-            Nonterminal::NtItem(ref item) => {\n-                prepend_attrs(sess, &item.attrs, item.tokens.as_ref(), span)\n-            }\n-            Nonterminal::NtTraitItem(ref item) => {\n-                prepend_attrs(sess, &item.attrs, item.tokens.as_ref(), span)\n-            }\n-            Nonterminal::NtImplItem(ref item) => {\n-                prepend_attrs(sess, &item.attrs, item.tokens.as_ref(), span)\n-            }\n-            Nonterminal::NtIdent(ident, is_raw) => {\n-                Some(TokenTree::token(Ident(ident.name, is_raw), ident.span).into())\n-            }\n-            Nonterminal::NtLifetime(ident) => {\n-                Some(TokenTree::token(Lifetime(ident.name), ident.span).into())\n-            }\n-            Nonterminal::NtTT(ref tt) => {\n-                Some(tt.clone().into())\n-            }\n-            _ => None,\n-        };\n-\n-        // FIXME(#43081): Avoid this pretty-print + reparse hack\n-        let source = pprust::nonterminal_to_string(self);\n-        let filename = FileName::macro_expansion_source_code(&source);\n-        let tokens_for_real = parse_stream_from_source_str(filename, source, sess, Some(span));\n-\n-        // During early phases of the compiler the AST could get modified\n-        // directly (e.g., attributes added or removed) and the internal cache\n-        // of tokens my not be invalidated or updated. Consequently if the\n-        // \"lossless\" token stream disagrees with our actual stringification\n-        // (which has historically been much more battle-tested) then we go\n-        // with the lossy stream anyway (losing span information).\n-        //\n-        // Note that the comparison isn't `==` here to avoid comparing spans,\n-        // but it *also* is a \"probable\" equality which is a pretty weird\n-        // definition. We mostly want to catch actual changes to the AST\n-        // like a `#[cfg]` being processed or some weird `macro_rules!`\n-        // expansion.\n-        //\n-        // What we *don't* want to catch is the fact that a user-defined\n-        // literal like `0xf` is stringified as `15`, causing the cached token\n-        // stream to not be literal `==` token-wise (ignoring spans) to the\n-        // token stream we got from stringification.\n-        //\n-        // Instead the \"probably equal\" check here is \"does each token\n-        // recursively have the same discriminant?\" We basically don't look at\n-        // the token values here and assume that such fine grained token stream\n-        // modifications, including adding/removing typically non-semantic\n-        // tokens such as extra braces and commas, don't happen.\n-        if let Some(tokens) = tokens {\n-            if tokens.probably_equal_for_proc_macro(&tokens_for_real) {\n-                return tokens\n-            }\n-            info!(\"cached tokens found, but they're not \\\"probably equal\\\", \\\n-                   going with stringified version\");\n-        }\n-        return tokens_for_real\n-    }\n-}\n-\n-fn prepend_attrs(sess: &ParseSess,\n-                 attrs: &[ast::Attribute],\n-                 tokens: Option<&tokenstream::TokenStream>,\n-                 span: syntax_pos::Span)\n-    -> Option<tokenstream::TokenStream>\n-{\n-    let tokens = tokens?;\n-    if attrs.len() == 0 {\n-        return Some(tokens.clone())\n-    }\n-    let mut builder = tokenstream::TokenStreamBuilder::new();\n-    for attr in attrs {\n-        assert_eq!(attr.style, ast::AttrStyle::Outer,\n-                   \"inner attributes should prevent cached tokens from existing\");\n-\n-        let source = pprust::attribute_to_string(attr);\n-        let macro_filename = FileName::macro_expansion_source_code(&source);\n-        if attr.is_sugared_doc {\n-            let stream = parse_stream_from_source_str(macro_filename, source, sess, Some(span));\n-            builder.push(stream);\n-            continue\n-        }\n-\n-        // synthesize # [ $path $tokens ] manually here\n-        let mut brackets = tokenstream::TokenStreamBuilder::new();\n-\n-        // For simple paths, push the identifier directly\n-        if attr.path.segments.len() == 1 && attr.path.segments[0].args.is_none() {\n-            let ident = attr.path.segments[0].ident;\n-            let token = Ident(ident.name, ident.as_str().starts_with(\"r#\"));\n-            brackets.push(tokenstream::TokenTree::token(token, ident.span));\n-\n-        // ... and for more complicated paths, fall back to a reparse hack that\n-        // should eventually be removed.\n-        } else {\n-            let stream = parse_stream_from_source_str(macro_filename, source, sess, Some(span));\n-            brackets.push(stream);\n-        }\n-\n-        brackets.push(attr.tokens.clone());\n-\n-        // The span we list here for `#` and for `[ ... ]` are both wrong in\n-        // that it encompasses more than each token, but it hopefully is \"good\n-        // enough\" for now at least.\n-        builder.push(tokenstream::TokenTree::token(Pound, attr.span));\n-        let delim_span = DelimSpan::from_single(attr.span);\n-        builder.push(tokenstream::TokenTree::Delimited(\n-            delim_span, DelimToken::Bracket, brackets.build().into()));\n-    }\n-    builder.push(tokens.clone());\n-    Some(builder.build())\n-}"}]}