{"sha": "3ddc72f69be4d0a2027ff598ad262ea2b2ca3812", "node_id": "MDY6Q29tbWl0NzI0NzEyOjNkZGM3MmY2OWJlNGQwYTIwMjdmZjU5OGFkMjYyZWEyYjJjYTM4MTI=", "commit": {"author": {"name": "bors", "email": "bors@rust-lang.org", "date": "2013-08-02T21:55:54Z"}, "committer": {"name": "bors", "email": "bors@rust-lang.org", "date": "2013-08-02T21:55:54Z"}, "message": "auto merge of #8234 : bblum/rust/assorted-fixes, r=brson\n\nThis fixes 4 bugs that prevented the extra::arc and extra::sync tests from passing on the new runtime.\r\n\r\n* In ```Add SendDeferred trait``` I add a non-rescheduling ```send_deferred``` method to our various channel types. The ```extra::sync``` concurrency primitives need this guarantee so they can send while inside of an exclusive. (This fixes deterministic deadlocks seen with ```RUST_THREADS=1```.)\r\n* In \"Fix nasty double-free bug\" I make sure that a ```ChanOne``` suppresses_finalize *before* rescheduling away to the receiver, so in case it gets a kill signal upon coming back, the destructor is inhibited as desired. (This is pretty uncommon on multiple CPUs but showed up always with ```RUST_THREADS=1```.)\r\n* In ```Fix embarrassing bug where 'unkillable' would unwind improperly``` I make sure the task's unkillable counter stays consistent when a kill signal is received right at the start of an unkillable section. (This is a very uncommon race and can only occur with multiple CPUs.)\r\n* In ```Don't fail from kill signals if already unwinding``` I do pretty much what it says on the tin. Surprising that it took the whole suite of sync/arc tests to expose this.\r\n\r\nThe other two commits are cleanup.\r\n\r\nr @brson", "tree": {"sha": "5942120bc4dba4d4b2da56457b425145030daf29", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/5942120bc4dba4d4b2da56457b425145030daf29"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/3ddc72f69be4d0a2027ff598ad262ea2b2ca3812", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/3ddc72f69be4d0a2027ff598ad262ea2b2ca3812", "html_url": "https://github.com/rust-lang/rust/commit/3ddc72f69be4d0a2027ff598ad262ea2b2ca3812", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/3ddc72f69be4d0a2027ff598ad262ea2b2ca3812/comments", "author": {"login": "bors", "id": 3372342, "node_id": "MDQ6VXNlcjMzNzIzNDI=", "avatar_url": "https://avatars.githubusercontent.com/u/3372342?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bors", "html_url": "https://github.com/bors", "followers_url": "https://api.github.com/users/bors/followers", "following_url": "https://api.github.com/users/bors/following{/other_user}", "gists_url": "https://api.github.com/users/bors/gists{/gist_id}", "starred_url": "https://api.github.com/users/bors/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bors/subscriptions", "organizations_url": "https://api.github.com/users/bors/orgs", "repos_url": "https://api.github.com/users/bors/repos", "events_url": "https://api.github.com/users/bors/events{/privacy}", "received_events_url": "https://api.github.com/users/bors/received_events", "type": "User", "site_admin": false}, "committer": {"login": "bors", "id": 3372342, "node_id": "MDQ6VXNlcjMzNzIzNDI=", "avatar_url": "https://avatars.githubusercontent.com/u/3372342?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bors", "html_url": "https://github.com/bors", "followers_url": "https://api.github.com/users/bors/followers", "following_url": "https://api.github.com/users/bors/following{/other_user}", "gists_url": "https://api.github.com/users/bors/gists{/gist_id}", "starred_url": "https://api.github.com/users/bors/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bors/subscriptions", "organizations_url": "https://api.github.com/users/bors/orgs", "repos_url": "https://api.github.com/users/bors/repos", "events_url": "https://api.github.com/users/bors/events{/privacy}", "received_events_url": "https://api.github.com/users/bors/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "f1c1f92d0c555d6e38ad1cac55926d6d9c9b090f", "url": "https://api.github.com/repos/rust-lang/rust/commits/f1c1f92d0c555d6e38ad1cac55926d6d9c9b090f", "html_url": "https://github.com/rust-lang/rust/commit/f1c1f92d0c555d6e38ad1cac55926d6d9c9b090f"}, {"sha": "43fecf3556b47305320221586f48f89fe2f6c505", "url": "https://api.github.com/repos/rust-lang/rust/commits/43fecf3556b47305320221586f48f89fe2f6c505", "html_url": "https://github.com/rust-lang/rust/commit/43fecf3556b47305320221586f48f89fe2f6c505"}], "stats": {"total": 570, "additions": 328, "deletions": 242}, "files": [{"sha": "276f9cad7c6d0ac8dfcf8a4559d3dd03ea8d5102", "filename": "src/libextra/sync.rs", "status": "modified", "additions": 107, "deletions": 211, "changes": 318, "blob_url": "https://github.com/rust-lang/rust/blob/3ddc72f69be4d0a2027ff598ad262ea2b2ca3812/src%2Flibextra%2Fsync.rs", "raw_url": "https://github.com/rust-lang/rust/raw/3ddc72f69be4d0a2027ff598ad262ea2b2ca3812/src%2Flibextra%2Fsync.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibextra%2Fsync.rs?ref=3ddc72f69be4d0a2027ff598ad262ea2b2ca3812", "patch": "@@ -18,10 +18,13 @@\n \n use std::borrow;\n use std::comm;\n+use std::comm::SendDeferred;\n use std::task;\n use std::unstable::sync::{Exclusive, UnsafeAtomicRcBox};\n use std::unstable::atomics;\n+use std::unstable::finally::Finally;\n use std::util;\n+use std::util::NonCopyable;\n \n /****************************************************************************\n  * Internals\n@@ -49,7 +52,7 @@ impl WaitQueue {\n         if self.head.peek() {\n             // Pop and send a wakeup signal. If the waiter was killed, its port\n             // will have closed. Keep trying until we get a live task.\n-            if comm::try_send_one(self.head.recv(), ()) {\n+            if self.head.recv().try_send_deferred(()) {\n                 true\n             } else {\n                 self.signal()\n@@ -62,7 +65,7 @@ impl WaitQueue {\n     fn broadcast(&self) -> uint {\n         let mut count = 0;\n         while self.head.peek() {\n-            if comm::try_send_one(self.head.recv(), ()) {\n+            if self.head.recv().try_send_deferred(()) {\n                 count += 1;\n             }\n         }\n@@ -83,7 +86,6 @@ struct SemInner<Q> {\n #[doc(hidden)]\n struct Sem<Q>(Exclusive<SemInner<Q>>);\n \n-\n #[doc(hidden)]\n impl<Q:Send> Sem<Q> {\n     fn new(count: int, q: Q) -> Sem<Q> {\n@@ -102,7 +104,7 @@ impl<Q:Send> Sem<Q> {\n                     // Tell outer scope we need to block.\n                     waiter_nobe = Some(WaitEnd);\n                     // Enqueue ourself.\n-                    state.waiters.tail.send(SignalEnd);\n+                    state.waiters.tail.send_deferred(SignalEnd);\n                 }\n             }\n             // Uncomment if you wish to test for sem races. Not valgrind-friendly.\n@@ -124,17 +126,18 @@ impl<Q:Send> Sem<Q> {\n             }\n         }\n     }\n-}\n-// FIXME(#3154) move both copies of this into Sem<Q>, and unify the 2 structs\n-#[doc(hidden)]\n-impl Sem<()> {\n+\n     pub fn access<U>(&self, blk: &fn() -> U) -> U {\n-        let mut release = None;\n         do task::unkillable {\n-            self.acquire();\n-            release = Some(SemRelease(self));\n+            do (|| {\n+                self.acquire();\n+                unsafe {\n+                    do task::rekillable { blk() }\n+                }\n+            }).finally {\n+                self.release();\n+            }\n         }\n-        blk()\n     }\n }\n \n@@ -148,46 +151,6 @@ impl Sem<~[WaitQueue]> {\n         }\n         Sem::new(count, queues)\n     }\n-\n-    pub fn access_waitqueue<U>(&self, blk: &fn() -> U) -> U {\n-        let mut release = None;\n-        do task::unkillable {\n-            self.acquire();\n-            release = Some(SemAndSignalRelease(self));\n-        }\n-        blk()\n-    }\n-}\n-\n-// FIXME(#3588) should go inside of access()\n-#[doc(hidden)]\n-type SemRelease<'self> = SemReleaseGeneric<'self, ()>;\n-#[doc(hidden)]\n-type SemAndSignalRelease<'self> = SemReleaseGeneric<'self, ~[WaitQueue]>;\n-#[doc(hidden)]\n-struct SemReleaseGeneric<'self, Q> { sem: &'self Sem<Q> }\n-\n-#[doc(hidden)]\n-#[unsafe_destructor]\n-impl<'self, Q:Send> Drop for SemReleaseGeneric<'self, Q> {\n-    fn drop(&self) {\n-        self.sem.release();\n-    }\n-}\n-\n-#[doc(hidden)]\n-fn SemRelease<'r>(sem: &'r Sem<()>) -> SemRelease<'r> {\n-    SemReleaseGeneric {\n-        sem: sem\n-    }\n-}\n-\n-#[doc(hidden)]\n-fn SemAndSignalRelease<'r>(sem: &'r Sem<~[WaitQueue]>)\n-                        -> SemAndSignalRelease<'r> {\n-    SemReleaseGeneric {\n-        sem: sem\n-    }\n }\n \n // FIXME(#3598): Want to use an Option down below, but we need a custom enum\n@@ -210,11 +173,10 @@ pub struct Condvar<'self> {\n     // writer waking up from a cvar wait can't race with a reader to steal it,\n     // See the comment in write_cond for more detail.\n     priv order: ReacquireOrderLock<'self>,\n+    // Make sure condvars are non-copyable.\n+    priv token: util::NonCopyable,\n }\n \n-#[unsafe_destructor]\n-impl<'self> Drop for Condvar<'self> { fn drop(&self) {} }\n-\n impl<'self> Condvar<'self> {\n     /**\n      * Atomically drop the associated lock, and block until a signal is sent.\n@@ -242,11 +204,10 @@ impl<'self> Condvar<'self> {\n         let (WaitEnd, SignalEnd) = comm::oneshot();\n         let mut WaitEnd   = Some(WaitEnd);\n         let mut SignalEnd = Some(SignalEnd);\n-        let mut reacquire = None;\n         let mut out_of_bounds = None;\n-        unsafe {\n-            do task::unkillable {\n-                // Release lock, 'atomically' enqueuing ourselves in so doing.\n+        do task::unkillable {\n+            // Release lock, 'atomically' enqueuing ourselves in so doing.\n+            unsafe {\n                 do (**self.sem).with |state| {\n                     if condvar_id < state.blocked.len() {\n                         // Drop the lock.\n@@ -256,42 +217,30 @@ impl<'self> Condvar<'self> {\n                         }\n                         // Enqueue ourself to be woken up by a signaller.\n                         let SignalEnd = SignalEnd.take_unwrap();\n-                        state.blocked[condvar_id].tail.send(SignalEnd);\n+                        state.blocked[condvar_id].tail.send_deferred(SignalEnd);\n                     } else {\n                         out_of_bounds = Some(state.blocked.len());\n                     }\n                 }\n-\n-                // If yield checks start getting inserted anywhere, we can be\n-                // killed before or after enqueueing. Deciding whether to\n-                // unkillably reacquire the lock needs to happen atomically\n-                // wrt enqueuing.\n-                if out_of_bounds.is_none() {\n-                    reacquire = Some(CondvarReacquire { sem:   self.sem,\n-                                                        order: self.order });\n-                }\n             }\n-        }\n-        do check_cvar_bounds(out_of_bounds, condvar_id, \"cond.wait_on()\") {\n-            // Unconditionally \"block\". (Might not actually block if a\n-            // signaller already sent -- I mean 'unconditionally' in contrast\n-            // with acquire().)\n-            let _ = comm::recv_one(WaitEnd.take_unwrap());\n-        }\n \n-        // This is needed for a failing condition variable to reacquire the\n-        // mutex during unwinding. As long as the wrapper (mutex, etc) is\n-        // bounded in when it gets released, this shouldn't hang forever.\n-        struct CondvarReacquire<'self> {\n-            sem: &'self Sem<~[WaitQueue]>,\n-            order: ReacquireOrderLock<'self>,\n-        }\n-\n-        #[unsafe_destructor]\n-        impl<'self> Drop for CondvarReacquire<'self> {\n-            fn drop(&self) {\n-                // Needs to succeed, instead of itself dying.\n-                do task::unkillable {\n+            // If yield checks start getting inserted anywhere, we can be\n+            // killed before or after enqueueing. Deciding whether to\n+            // unkillably reacquire the lock needs to happen atomically\n+            // wrt enqueuing.\n+            do check_cvar_bounds(out_of_bounds, condvar_id, \"cond.wait_on()\") {\n+                // Unconditionally \"block\". (Might not actually block if a\n+                // signaller already sent -- I mean 'unconditionally' in contrast\n+                // with acquire().)\n+                do (|| {\n+                    unsafe {\n+                        do task::rekillable {\n+                            let _ = comm::recv_one(WaitEnd.take_unwrap());\n+                        }\n+                    }\n+                }).finally {\n+                    // Reacquire the condvar. Note this is back in the unkillable\n+                    // section; it needs to succeed, instead of itself dying.\n                     match self.order {\n                         Just(lock) => do lock.access {\n                             self.sem.acquire();\n@@ -373,8 +322,8 @@ impl Sem<~[WaitQueue]> {\n     // The only other places that condvars get built are rwlock.write_cond()\n     // and rwlock_write_mode.\n     pub fn access_cond<U>(&self, blk: &fn(c: &Condvar) -> U) -> U {\n-        do self.access_waitqueue {\n-            blk(&Condvar { sem: self, order: Nothing })\n+        do self.access {\n+            blk(&Condvar { sem: self, order: Nothing, token: NonCopyable::new() })\n         }\n     }\n }\n@@ -452,7 +401,7 @@ impl Mutex {\n \n     /// Run a function with ownership of the mutex.\n     pub fn lock<U>(&self, blk: &fn() -> U) -> U {\n-        (&self.sem).access_waitqueue(blk)\n+        (&self.sem).access(blk)\n     }\n \n     /// Run a function with ownership of the mutex and a handle to a condvar.\n@@ -531,7 +480,6 @@ impl RWLock {\n      * tasks may run concurrently with this one.\n      */\n     pub fn read<U>(&self, blk: &fn() -> U) -> U {\n-        let mut release = None;\n         unsafe {\n             do task::unkillable {\n                 do (&self.order_lock).access {\n@@ -542,10 +490,24 @@ impl RWLock {\n                         state.read_mode = true;\n                     }\n                 }\n-                release = Some(RWLockReleaseRead(self));\n+                do (|| {\n+                    do task::rekillable { blk() }\n+                }).finally {\n+                    let state = &mut *self.state.get();\n+                    assert!(state.read_mode);\n+                    let old_count = state.read_count.fetch_sub(1, atomics::Release);\n+                    assert!(old_count > 0);\n+                    if old_count == 1 {\n+                        state.read_mode = false;\n+                        // Note: this release used to be outside of a locked access\n+                        // to exclusive-protected state. If this code is ever\n+                        // converted back to such (instead of using atomic ops),\n+                        // this access MUST NOT go inside the exclusive access.\n+                        (&self.access_lock).release();\n+                    }\n+                }\n             }\n         }\n-        blk()\n     }\n \n     /**\n@@ -556,7 +518,7 @@ impl RWLock {\n         unsafe {\n             do task::unkillable {\n                 (&self.order_lock).acquire();\n-                do (&self.access_lock).access_waitqueue {\n+                do (&self.access_lock).access {\n                     (&self.order_lock).release();\n                     do task::rekillable {\n                         blk()\n@@ -606,7 +568,8 @@ impl RWLock {\n                     (&self.order_lock).release();\n                     do task::rekillable {\n                         let opt_lock = Just(&self.order_lock);\n-                        blk(&Condvar { order: opt_lock, ..*cond })\n+                        blk(&Condvar { sem: cond.sem, order: opt_lock,\n+                                       token: NonCopyable::new() })\n                     }\n                 }\n             }\n@@ -637,14 +600,43 @@ impl RWLock {\n     pub fn write_downgrade<U>(&self, blk: &fn(v: RWLockWriteMode) -> U) -> U {\n         // Implementation slightly different from the slicker 'write's above.\n         // The exit path is conditional on whether the caller downgrades.\n-        let mut _release = None;\n         do task::unkillable {\n             (&self.order_lock).acquire();\n             (&self.access_lock).acquire();\n             (&self.order_lock).release();\n+            do (|| {\n+                unsafe {\n+                    do task::rekillable {\n+                        blk(RWLockWriteMode { lock: self, token: NonCopyable::new() })\n+                    }\n+                }\n+            }).finally {\n+                let writer_or_last_reader;\n+                // Check if we're releasing from read mode or from write mode.\n+                let state = unsafe { &mut *self.state.get() };\n+                if state.read_mode {\n+                    // Releasing from read mode.\n+                    let old_count = state.read_count.fetch_sub(1, atomics::Release);\n+                    assert!(old_count > 0);\n+                    // Check if other readers remain.\n+                    if old_count == 1 {\n+                        // Case 1: Writer downgraded & was the last reader\n+                        writer_or_last_reader = true;\n+                        state.read_mode = false;\n+                    } else {\n+                        // Case 2: Writer downgraded & was not the last reader\n+                        writer_or_last_reader = false;\n+                    }\n+                } else {\n+                    // Case 3: Writer did not downgrade\n+                    writer_or_last_reader = true;\n+                }\n+                if writer_or_last_reader {\n+                    // Nobody left inside; release the \"reader cloud\" lock.\n+                    (&self.access_lock).release();\n+                }\n+            }\n         }\n-        _release = Some(RWLockReleaseDowngrade(self));\n-        blk(RWLockWriteMode { lock: self })\n     }\n \n     /// To be called inside of the write_downgrade block.\n@@ -673,105 +665,16 @@ impl RWLock {\n                 }\n             }\n         }\n-        RWLockReadMode { lock: token.lock }\n-    }\n-}\n-\n-// FIXME(#3588) should go inside of read()\n-#[doc(hidden)]\n-struct RWLockReleaseRead<'self> {\n-    lock: &'self RWLock,\n-}\n-\n-#[doc(hidden)]\n-#[unsafe_destructor]\n-impl<'self> Drop for RWLockReleaseRead<'self> {\n-    fn drop(&self) {\n-        unsafe {\n-            do task::unkillable {\n-                let state = &mut *self.lock.state.get();\n-                assert!(state.read_mode);\n-                let old_count = state.read_count.fetch_sub(1, atomics::Release);\n-                assert!(old_count > 0);\n-                if old_count == 1 {\n-                    state.read_mode = false;\n-                    // Note: this release used to be outside of a locked access\n-                    // to exclusive-protected state. If this code is ever\n-                    // converted back to such (instead of using atomic ops),\n-                    // this access MUST NOT go inside the exclusive access.\n-                    (&self.lock.access_lock).release();\n-                }\n-            }\n-        }\n-    }\n-}\n-\n-#[doc(hidden)]\n-fn RWLockReleaseRead<'r>(lock: &'r RWLock) -> RWLockReleaseRead<'r> {\n-    RWLockReleaseRead {\n-        lock: lock\n-    }\n-}\n-\n-// FIXME(#3588) should go inside of downgrade()\n-#[doc(hidden)]\n-#[unsafe_destructor]\n-struct RWLockReleaseDowngrade<'self> {\n-    lock: &'self RWLock,\n-}\n-\n-#[doc(hidden)]\n-#[unsafe_destructor]\n-impl<'self> Drop for RWLockReleaseDowngrade<'self> {\n-    fn drop(&self) {\n-        unsafe {\n-            do task::unkillable {\n-                let writer_or_last_reader;\n-                // Check if we're releasing from read mode or from write mode.\n-                let state = &mut *self.lock.state.get();\n-                if state.read_mode {\n-                    // Releasing from read mode.\n-                    let old_count = state.read_count.fetch_sub(1, atomics::Release);\n-                    assert!(old_count > 0);\n-                    // Check if other readers remain.\n-                    if old_count == 1 {\n-                        // Case 1: Writer downgraded & was the last reader\n-                        writer_or_last_reader = true;\n-                        state.read_mode = false;\n-                    } else {\n-                        // Case 2: Writer downgraded & was not the last reader\n-                        writer_or_last_reader = false;\n-                    }\n-                } else {\n-                    // Case 3: Writer did not downgrade\n-                    writer_or_last_reader = true;\n-                }\n-                if writer_or_last_reader {\n-                    // Nobody left inside; release the \"reader cloud\" lock.\n-                    (&self.lock.access_lock).release();\n-                }\n-            }\n-        }\n-    }\n-}\n-\n-#[doc(hidden)]\n-fn RWLockReleaseDowngrade<'r>(lock: &'r RWLock)\n-                           -> RWLockReleaseDowngrade<'r> {\n-    RWLockReleaseDowngrade {\n-        lock: lock\n+        RWLockReadMode { lock: token.lock, token: NonCopyable::new() }\n     }\n }\n \n /// The \"write permission\" token used for rwlock.write_downgrade().\n-pub struct RWLockWriteMode<'self> { priv lock: &'self RWLock }\n-#[unsafe_destructor]\n-impl<'self> Drop for RWLockWriteMode<'self> { fn drop(&self) {} }\n+pub struct RWLockWriteMode<'self> { priv lock: &'self RWLock, priv token: NonCopyable }\n \n /// The \"read permission\" token used for rwlock.write_downgrade().\n-pub struct RWLockReadMode<'self> { priv lock: &'self RWLock }\n-#[unsafe_destructor]\n-impl<'self> Drop for RWLockReadMode<'self> { fn drop(&self) {} }\n+pub struct RWLockReadMode<'self> { priv lock: &'self RWLock,\n+                                   priv token: NonCopyable }\n \n impl<'self> RWLockWriteMode<'self> {\n     /// Access the pre-downgrade rwlock in write mode.\n@@ -781,7 +684,8 @@ impl<'self> RWLockWriteMode<'self> {\n         // Need to make the condvar use the order lock when reacquiring the\n         // access lock. See comment in RWLock::write_cond for why.\n         blk(&Condvar { sem:        &self.lock.access_lock,\n-                       order: Just(&self.lock.order_lock), })\n+                       order: Just(&self.lock.order_lock),\n+                       token: NonCopyable::new() })\n     }\n }\n \n@@ -1059,6 +963,8 @@ mod tests {\n     }\n     #[test] #[ignore(cfg(windows))]\n     fn test_mutex_killed_broadcast() {\n+        use std::unstable::finally::Finally;\n+\n         let m = ~Mutex::new();\n         let m2 = ~m.clone();\n         let (p,c) = comm::stream();\n@@ -1075,8 +981,13 @@ mod tests {\n                     do mi.lock_cond |cond| {\n                         let c = c.take();\n                         c.send(()); // tell sibling to go ahead\n-                        let _z = SendOnFailure(c);\n-                        cond.wait(); // block forever\n+                        do (|| {\n+                            cond.wait(); // block forever\n+                        }).finally {\n+                            error!(\"task unwinding and sending\");\n+                            c.send(());\n+                            error!(\"task unwinding and done sending\");\n+                        }\n                     }\n                 }\n             }\n@@ -1095,21 +1006,6 @@ mod tests {\n             let woken = cond.broadcast();\n             assert_eq!(woken, 0);\n         }\n-        struct SendOnFailure {\n-            c: comm::Chan<()>,\n-        }\n-\n-        impl Drop for SendOnFailure {\n-            fn drop(&self) {\n-                self.c.send(());\n-            }\n-        }\n-\n-        fn SendOnFailure(c: comm::Chan<()>) -> SendOnFailure {\n-            SendOnFailure {\n-                c: c\n-            }\n-        }\n     }\n     #[test]\n     fn test_mutex_cond_signal_on_0() {"}, {"sha": "a0731dc3494c20d617daa4005be8bf55befdd9a5", "filename": "src/libstd/comm.rs", "status": "modified", "additions": 30, "deletions": 0, "changes": 30, "blob_url": "https://github.com/rust-lang/rust/blob/3ddc72f69be4d0a2027ff598ad262ea2b2ca3812/src%2Flibstd%2Fcomm.rs", "raw_url": "https://github.com/rust-lang/rust/raw/3ddc72f69be4d0a2027ff598ad262ea2b2ca3812/src%2Flibstd%2Fcomm.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibstd%2Fcomm.rs?ref=3ddc72f69be4d0a2027ff598ad262ea2b2ca3812", "patch": "@@ -19,6 +19,7 @@ use either::{Either, Left, Right};\n use kinds::Send;\n use option::{Option, Some};\n use unstable::sync::Exclusive;\n+pub use rt::comm::SendDeferred;\n use rtcomm = rt::comm;\n use rt;\n \n@@ -105,6 +106,21 @@ impl<T: Send> GenericSmartChan<T> for Chan<T> {\n     }\n }\n \n+impl<T: Send> SendDeferred<T> for Chan<T> {\n+    fn send_deferred(&self, x: T) {\n+        match self.inner {\n+            Left(ref chan) => chan.send(x),\n+            Right(ref chan) => chan.send_deferred(x)\n+        }\n+    }\n+    fn try_send_deferred(&self, x: T) -> bool {\n+        match self.inner {\n+            Left(ref chan) => chan.try_send(x),\n+            Right(ref chan) => chan.try_send_deferred(x)\n+        }\n+    }\n+}\n+\n impl<T: Send> GenericPort<T> for Port<T> {\n     fn recv(&self) -> T {\n         match self.inner {\n@@ -250,6 +266,20 @@ impl<T: Send> ChanOne<T> {\n             Right(p) => p.try_send(data)\n         }\n     }\n+    pub fn send_deferred(self, data: T) {\n+        let ChanOne { inner } = self;\n+        match inner {\n+            Left(p) => p.send(data),\n+            Right(p) => p.send_deferred(data)\n+        }\n+    }\n+    pub fn try_send_deferred(self, data: T) -> bool {\n+        let ChanOne { inner } = self;\n+        match inner {\n+            Left(p) => p.try_send(data),\n+            Right(p) => p.try_send_deferred(data)\n+        }\n+    }\n }\n \n pub fn recv_one<T: Send>(port: PortOne<T>) -> T {"}, {"sha": "a060059f5fc9339b9ba2af58288d8a5a461c6358", "filename": "src/libstd/rt/comm.rs", "status": "modified", "additions": 134, "deletions": 26, "changes": 160, "blob_url": "https://github.com/rust-lang/rust/blob/3ddc72f69be4d0a2027ff598ad262ea2b2ca3812/src%2Flibstd%2Frt%2Fcomm.rs", "raw_url": "https://github.com/rust-lang/rust/raw/3ddc72f69be4d0a2027ff598ad262ea2b2ca3812/src%2Flibstd%2Frt%2Fcomm.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibstd%2Frt%2Fcomm.rs?ref=3ddc72f69be4d0a2027ff598ad262ea2b2ca3812", "patch": "@@ -25,6 +25,7 @@ use comm::{GenericChan, GenericSmartChan, GenericPort, Peekable};\n use cell::Cell;\n use clone::Clone;\n use rt::{context, SchedulerContext};\n+use tuple::ImmutableTuple;\n \n /// A combined refcount / BlockedTask-as-uint pointer.\n ///\n@@ -86,12 +87,32 @@ impl<T> ChanOne<T> {\n         }\n     }\n \n+    /// Send a message on the one-shot channel. If a receiver task is blocked\n+    /// waiting for the message, will wake it up and reschedule to it.\n     pub fn send(self, val: T) {\n         self.try_send(val);\n     }\n \n+    /// As `send`, but also returns whether or not the receiver endpoint is still open.\n     pub fn try_send(self, val: T) -> bool {\n+        self.try_send_inner(val, true)\n+    }\n+\n+    /// Send a message without immediately rescheduling to a blocked receiver.\n+    /// This can be useful in contexts where rescheduling is forbidden, or to\n+    /// optimize for when the sender expects to still have useful work to do.\n+    pub fn send_deferred(self, val: T) {\n+        self.try_send_deferred(val);\n+    }\n+\n+    /// As `send_deferred` and `try_send` together.\n+    pub fn try_send_deferred(self, val: T) -> bool {\n+        self.try_send_inner(val, false)\n+    }\n \n+    // 'do_resched' configures whether the scheduler immediately switches to\n+    // the receiving task, or leaves the sending task still running.\n+    fn try_send_inner(self, val: T, do_resched: bool) -> bool {\n         rtassert!(context() != SchedulerContext);\n \n         let mut this = self;\n@@ -110,6 +131,13 @@ impl<T> ChanOne<T> {\n             // acquire barrier that keeps the subsequent access of the\n             // ~Task pointer from being reordered.\n             let oldstate = (*packet).state.swap(STATE_ONE, SeqCst);\n+\n+            // Suppress the synchronizing actions in the finalizer. We're\n+            // done with the packet. NB: In case of do_resched, this *must*\n+            // happen before waking up a blocked task (or be unkillable),\n+            // because we might get a kill signal during the reschedule.\n+            this.suppress_finalize = true;\n+\n             match oldstate {\n                 STATE_BOTH => {\n                     // Port is not waiting yet. Nothing to do\n@@ -130,15 +158,20 @@ impl<T> ChanOne<T> {\n                 task_as_state => {\n                     // Port is blocked. Wake it up.\n                     let recvr = BlockedTask::cast_from_uint(task_as_state);\n-                    do recvr.wake().map_consume |woken_task| {\n-                        Scheduler::run_task(woken_task);\n-                    };\n+                    if do_resched {\n+                        do recvr.wake().map_consume |woken_task| {\n+                            Scheduler::run_task(woken_task);\n+                        };\n+                    } else {\n+                        let recvr = Cell::new(recvr);\n+                        do Local::borrow::<Scheduler, ()> |sched| {\n+                            sched.enqueue_blocked_task(recvr.take());\n+                        }\n+                    }\n                 }\n             }\n         }\n \n-        // Suppress the synchronizing actions in the finalizer. We're done with the packet.\n-        this.suppress_finalize = true;\n         return recvr_active;\n     }\n }\n@@ -152,6 +185,7 @@ impl<T> PortOne<T> {\n         }\n     }\n \n+    /// Wait for a message on the one-shot port. Fails if the send end is closed.\n     pub fn recv(self) -> T {\n         match self.try_recv() {\n             Some(val) => val,\n@@ -161,6 +195,7 @@ impl<T> PortOne<T> {\n         }\n     }\n \n+    /// As `recv`, but returns `None` if the send end is closed rather than failing.\n     pub fn try_recv(self) -> Option<T> {\n         let mut this = self;\n \n@@ -382,6 +417,12 @@ impl<T> Drop for PortOne<T> {\n     }\n }\n \n+/// Trait for non-rescheduling send operations, similar to `send_deferred` on ChanOne.\n+pub trait SendDeferred<T> {\n+    fn send_deferred(&self, val: T);\n+    fn try_send_deferred(&self, val: T) -> bool;\n+}\n+\n struct StreamPayload<T> {\n     val: T,\n     next: PortOne<StreamPayload<T>>\n@@ -409,6 +450,15 @@ pub fn stream<T: Send>() -> (Port<T>, Chan<T>) {\n     return (port, chan);\n }\n \n+impl<T: Send> Chan<T> {\n+    fn try_send_inner(&self, val: T, do_resched: bool) -> bool {\n+        let (next_pone, next_cone) = oneshot();\n+        let cone = self.next.take();\n+        self.next.put_back(next_cone);\n+        cone.try_send_inner(StreamPayload { val: val, next: next_pone }, do_resched)\n+    }\n+}\n+\n impl<T: Send> GenericChan<T> for Chan<T> {\n     fn send(&self, val: T) {\n         self.try_send(val);\n@@ -417,10 +467,16 @@ impl<T: Send> GenericChan<T> for Chan<T> {\n \n impl<T: Send> GenericSmartChan<T> for Chan<T> {\n     fn try_send(&self, val: T) -> bool {\n-        let (next_pone, next_cone) = oneshot();\n-        let cone = self.next.take();\n-        self.next.put_back(next_cone);\n-        cone.try_send(StreamPayload { val: val, next: next_pone })\n+        self.try_send_inner(val, true)\n+    }\n+}\n+\n+impl<T: Send> SendDeferred<T> for Chan<T> {\n+    fn send_deferred(&self, val: T) {\n+        self.try_send_deferred(val);\n+    }\n+    fn try_send_deferred(&self, val: T) -> bool {\n+        self.try_send_inner(val, false)\n     }\n }\n \n@@ -495,6 +551,17 @@ impl<T> SharedChan<T> {\n     }\n }\n \n+impl<T: Send> SharedChan<T> {\n+    fn try_send_inner(&self, val: T, do_resched: bool) -> bool {\n+        unsafe {\n+            let (next_pone, next_cone) = oneshot();\n+            let cone = (*self.next.get()).swap(~next_cone, SeqCst);\n+            cone.unwrap().try_send_inner(StreamPayload { val: val, next: next_pone },\n+                                         do_resched)\n+        }\n+    }\n+}\n+\n impl<T: Send> GenericChan<T> for SharedChan<T> {\n     fn send(&self, val: T) {\n         self.try_send(val);\n@@ -503,11 +570,16 @@ impl<T: Send> GenericChan<T> for SharedChan<T> {\n \n impl<T: Send> GenericSmartChan<T> for SharedChan<T> {\n     fn try_send(&self, val: T) -> bool {\n-        unsafe {\n-            let (next_pone, next_cone) = oneshot();\n-            let cone = (*self.next.get()).swap(~next_cone, SeqCst);\n-            cone.unwrap().try_send(StreamPayload { val: val, next: next_pone })\n-        }\n+        self.try_send_inner(val, true)\n+    }\n+}\n+\n+impl<T: Send> SendDeferred<T> for SharedChan<T> {\n+    fn send_deferred(&self, val: T) {\n+        self.try_send_deferred(val);\n+    }\n+    fn try_send_deferred(&self, val: T) -> bool {\n+        self.try_send_inner(val, false)\n     }\n }\n \n@@ -584,31 +656,32 @@ pub fn megapipe<T: Send>() -> MegaPipe<T> {\n \n impl<T: Send> GenericChan<T> for MegaPipe<T> {\n     fn send(&self, val: T) {\n-        match *self {\n-            (_, ref c) => c.send(val)\n-        }\n+        self.second_ref().send(val)\n     }\n }\n \n impl<T: Send> GenericSmartChan<T> for MegaPipe<T> {\n     fn try_send(&self, val: T) -> bool {\n-        match *self {\n-            (_, ref c) => c.try_send(val)\n-        }\n+        self.second_ref().try_send(val)\n     }\n }\n \n impl<T: Send> GenericPort<T> for MegaPipe<T> {\n     fn recv(&self) -> T {\n-        match *self {\n-            (ref p, _) => p.recv()\n-        }\n+        self.first_ref().recv()\n     }\n \n     fn try_recv(&self) -> Option<T> {\n-        match *self {\n-            (ref p, _) => p.try_recv()\n-        }\n+        self.first_ref().try_recv()\n+    }\n+}\n+\n+impl<T: Send> SendDeferred<T> for MegaPipe<T> {\n+    fn send_deferred(&self, val: T) {\n+        self.second_ref().send_deferred(val)\n+    }\n+    fn try_send_deferred(&self, val: T) -> bool {\n+        self.second_ref().try_send_deferred(val)\n     }\n }\n \n@@ -1017,4 +1090,39 @@ mod test {\n             }\n         }\n     }\n+\n+    #[test]\n+    fn send_deferred() {\n+        use unstable::sync::atomically;\n+\n+        // Tests no-rescheduling of send_deferred on all types of channels.\n+        do run_in_newsched_task {\n+            let (pone, cone) = oneshot();\n+            let (pstream, cstream) = stream();\n+            let (pshared, cshared) = stream();\n+            let cshared = SharedChan::new(cshared);\n+            let mp = megapipe();\n+\n+            let pone = Cell::new(pone);\n+            do spawntask { pone.take().recv(); }\n+            let pstream = Cell::new(pstream);\n+            do spawntask { pstream.take().recv(); }\n+            let pshared = Cell::new(pshared);\n+            do spawntask { pshared.take().recv(); }\n+            let p_mp = Cell::new(mp.clone());\n+            do spawntask { p_mp.take().recv(); }\n+\n+            let cs = Cell::new((cone, cstream, cshared, mp));\n+            unsafe {\n+                do atomically {\n+                    let (cone, cstream, cshared, mp) = cs.take();\n+                    cone.send_deferred(());\n+                    cstream.send_deferred(());\n+                    cshared.send_deferred(());\n+                    mp.send_deferred(());\n+                }\n+            }\n+        }\n+    }\n+\n }"}, {"sha": "deec8dd37a600d1a484f02f909ae6fe0a9753c1e", "filename": "src/libstd/rt/kill.rs", "status": "modified", "additions": 5, "deletions": 4, "changes": 9, "blob_url": "https://github.com/rust-lang/rust/blob/3ddc72f69be4d0a2027ff598ad262ea2b2ca3812/src%2Flibstd%2Frt%2Fkill.rs", "raw_url": "https://github.com/rust-lang/rust/raw/3ddc72f69be4d0a2027ff598ad262ea2b2ca3812/src%2Flibstd%2Frt%2Fkill.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibstd%2Frt%2Fkill.rs?ref=3ddc72f69be4d0a2027ff598ad262ea2b2ca3812", "patch": "@@ -530,13 +530,13 @@ impl Death {\n \n     /// Fails if a kill signal was received.\n     #[inline]\n-    pub fn check_killed(&self) {\n+    pub fn check_killed(&self, already_failing: bool) {\n         match self.kill_handle {\n             Some(ref kill_handle) =>\n                 // The task may be both unkillable and killed if it does some\n                 // synchronization during unwinding or cleanup (for example,\n                 // sending on a notify port). In that case failing won't help.\n-                if self.unkillable == 0 && kill_handle.killed() {\n+                if self.unkillable == 0 && (!already_failing) && kill_handle.killed() {\n                     fail!(KILLED_MSG);\n                 },\n             // This may happen during task death (see comments in collect_failure).\n@@ -548,11 +548,12 @@ impl Death {\n     /// All calls must be paired with a subsequent call to allow_kill.\n     #[inline]\n     pub fn inhibit_kill(&mut self, already_failing: bool) {\n-        if self.unkillable == 0 {\n+        self.unkillable += 1;\n+        // May fail, hence must happen *after* incrementing the counter\n+        if self.unkillable == 1 {\n             rtassert!(self.kill_handle.is_some());\n             self.kill_handle.get_mut_ref().inhibit_kill(already_failing);\n         }\n-        self.unkillable += 1;\n     }\n \n     /// Exit a possibly-nested unkillable section of code."}, {"sha": "dfe003253c2cf2e6cd656f2da62011108a9da140", "filename": "src/libstd/rt/sched.rs", "status": "modified", "additions": 5, "deletions": 1, "changes": 6, "blob_url": "https://github.com/rust-lang/rust/blob/3ddc72f69be4d0a2027ff598ad262ea2b2ca3812/src%2Flibstd%2Frt%2Fsched.rs", "raw_url": "https://github.com/rust-lang/rust/raw/3ddc72f69be4d0a2027ff598ad262ea2b2ca3812/src%2Flibstd%2Frt%2Fsched.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibstd%2Frt%2Fsched.rs?ref=3ddc72f69be4d0a2027ff598ad262ea2b2ca3812", "patch": "@@ -540,6 +540,10 @@ impl Scheduler {\n         // The current task is grabbed from TLS, not taken as an input.\n         let current_task: ~Task = Local::take::<Task>();\n \n+        // Check that the task is not in an atomically() section (e.g.,\n+        // holding a pthread mutex, which could deadlock the scheduler).\n+        current_task.death.assert_may_sleep();\n+\n         // These transmutes do something fishy with a closure.\n         let f_fake_region = unsafe {\n             transmute::<&fn(&mut Scheduler, ~Task),\n@@ -600,7 +604,7 @@ impl Scheduler {\n \n             // Must happen after running the cleanup job (of course).\n             let task = Local::unsafe_borrow::<Task>();\n-            (*task).death.check_killed();\n+            (*task).death.check_killed((*task).unwinder.unwinding);\n         }\n     }\n "}, {"sha": "e08297a142516cb968d6610854c0768b3cf98b6c", "filename": "src/libstd/task/mod.rs", "status": "modified", "additions": 41, "deletions": 0, "changes": 41, "blob_url": "https://github.com/rust-lang/rust/blob/3ddc72f69be4d0a2027ff598ad262ea2b2ca3812/src%2Flibstd%2Ftask%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/3ddc72f69be4d0a2027ff598ad262ea2b2ca3812/src%2Flibstd%2Ftask%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibstd%2Ftask%2Fmod.rs?ref=3ddc72f69be4d0a2027ff598ad262ea2b2ca3812", "patch": "@@ -655,6 +655,47 @@ pub unsafe fn rekillable<U>(f: &fn() -> U) -> U {\n     }\n }\n \n+#[test] #[ignore(cfg(windows))]\n+fn test_kill_unkillable_task() {\n+    use rt::test::*;\n+\n+    // Attempt to test that when a kill signal is received at the start of an\n+    // unkillable section, 'unkillable' unwinds correctly. This is actually\n+    // quite a difficult race to expose, as the kill has to happen on a second\n+    // CPU, *after* the spawner is already switched-back-to (and passes the\n+    // killed check at the start of its timeslice). As far as I know, it's not\n+    // possible to make this race deterministic, or even more likely to happen.\n+    do run_in_newsched_task {\n+        do task::try {\n+            do task::spawn {\n+                fail!();\n+            }\n+            do task::unkillable { }\n+        };\n+    }\n+}\n+\n+#[test] #[ignore(cfg(windows))]\n+fn test_kill_rekillable_task() {\n+    use rt::test::*;\n+\n+    // Tests that when a kill signal is received, 'rekillable' and\n+    // 'unkillable' unwind correctly in conjunction with each other.\n+    do run_in_newsched_task {\n+        do task::try {\n+            do task::unkillable {\n+                unsafe {\n+                    do task::rekillable {\n+                        do task::spawn {\n+                            fail!();\n+                        }\n+                    }\n+                }\n+            }\n+        };\n+    }\n+}\n+\n #[test] #[should_fail] #[ignore(cfg(windows))]\n fn test_cant_dup_task_builder() {\n     let mut builder = task();"}, {"sha": "b46876ad3fe44db96389a84194f9be1ac2009680", "filename": "src/libstd/util.rs", "status": "modified", "additions": 6, "deletions": 0, "changes": 6, "blob_url": "https://github.com/rust-lang/rust/blob/3ddc72f69be4d0a2027ff598ad262ea2b2ca3812/src%2Flibstd%2Futil.rs", "raw_url": "https://github.com/rust-lang/rust/raw/3ddc72f69be4d0a2027ff598ad262ea2b2ca3812/src%2Flibstd%2Futil.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibstd%2Futil.rs?ref=3ddc72f69be4d0a2027ff598ad262ea2b2ca3812", "patch": "@@ -79,6 +79,12 @@ pub fn replace<T>(dest: &mut T, mut src: T) -> T {\n #[unsafe_no_drop_flag]\n pub struct NonCopyable;\n \n+impl NonCopyable {\n+    // FIXME(#8233) should not be necessary\n+    /// Create a new noncopyable token.\n+    pub fn new() -> NonCopyable { NonCopyable }\n+}\n+\n impl Drop for NonCopyable {\n     fn drop(&self) { }\n }"}]}