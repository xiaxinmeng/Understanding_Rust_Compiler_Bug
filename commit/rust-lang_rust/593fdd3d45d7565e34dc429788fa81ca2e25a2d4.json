{"sha": "593fdd3d45d7565e34dc429788fa81ca2e25a2d4", "node_id": "MDY6Q29tbWl0NzI0NzEyOjU5M2ZkZDNkNDVkNzU2NWUzNGRjNDI5Nzg4ZmE4MWNhMmUyNWEyZDQ=", "commit": {"author": {"name": "Aaron Hill", "email": "aa1ronham@gmail.com", "date": "2020-09-27T01:56:29Z"}, "committer": {"name": "Aaron Hill", "email": "aa1ronham@gmail.com", "date": "2020-10-19T17:59:18Z"}, "message": "Rewrite `collect_tokens` implementations to use a flattened buffer\n\nInstead of trying to collect tokens at each depth, we 'flatten' the\nstream as we go allong, pushing open/close delimiters to our buffer\njust like regular tokens. One capturing is complete, we reconstruct a\nnested `TokenTree::Delimited` structure, producing a normal\n`TokenStream`.\n\nThe reconstructed `TokenStream` is not created immediately - instead, it is\nproduced on-demand by a closure (wrapped in a new `LazyTokenStream` type). This\nclosure stores a clone of the original `TokenCursor`, plus a record of the\nnumber of calls to `next()/next_desugared()`. This is sufficient to reconstruct\nthe tokenstream seen by the callback without storing any additional state. If\nthe tokenstream is never used (e.g. when a captured `macro_rules!` argument is\nnever passed to a proc macro), we never actually create a `TokenStream`.\n\nThis implementation has a number of advantages over the previous one:\n\n* It is significantly simpler, with no edge cases around capturing the\n  start/end of a delimited group.\n\n* It can be easily extended to allow replacing tokens an an arbitrary\n  'depth' by just using `Vec::splice` at the proper position. This is\n  important for PR #76130, which requires us to track information about\n  attributes along with tokens.\n\n* The lazy approach to `TokenStream` construction allows us to easily\n  parse an AST struct, and then decide after the fact whether we need a\n  `TokenStream`. This will be useful when we start collecting tokens for\n  `Attribute` - we can discard the `LazyTokenStream` if the parsed\n  attribute doesn't need tokens (e.g. is a builtin attribute).\n\nThe performance impact seems to be neglibile (see\nhttps://github.com/rust-lang/rust/pull/77250#issuecomment-703960604). There is a\nsmall slowdown on a few benchmarks, but it only rises above 1% for incremental\nbuilds, where it represents a larger fraction of the much smaller instruction\ncount. There a ~1% speedup on a few other incremental benchmarks - my guess is\nthat the speedups and slowdowns will usually cancel out in practice.", "tree": {"sha": "2d3ae4c7a1cb800273757906d1464db7333ee977", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/2d3ae4c7a1cb800273757906d1464db7333ee977"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/593fdd3d45d7565e34dc429788fa81ca2e25a2d4", "comment_count": 0, "verification": {"verified": true, "reason": "valid", "signature": "-----BEGIN PGP SIGNATURE-----\n\niQIzBAABCAAdFiEE7J9Gc3TfBwj2K399tAh+UQ6YsWQFAl+N1FIACgkQtAh+UQ6Y\nsWRSAw//R96ZjxU/dRefFJlRF64BjmDGTGweGjb5u8NnnJE8QaK25bZHyz9c2f/P\neIO+H4wqBIZFwvJNjuxpm3cNVlpkApGjblwC4/ghij9hCLGba9pU8JZUFGzEV7Vm\nuY79hKJFVX3VLJv0lQK6KybbiO7a9TINn3yFzhuOc+ddXVEtDfKosITeRDsfgw+f\nMx6lPFeqkJ5oERKVkUbfdL3j8vvjduuE7rH9apFixKuxdOK0qo+uGe59/KWnlbw1\ne8QcREpmdj62HYnvEi4ZIrtDBR8z1qcGtHry36QpSX5M/y9GY2RxHGfv5LSrf8w8\n/GGo33HASYGk0HHQuYDPydR6p7nvHoECQr62osF1UnRGZmJ2PwR1jHBkgwcOEGOM\nkfBpE7/mkSPu9nfRgBAZJQX8cxZD++CdgPcpF1Nx8Ct+tX351/zSLhPHWY9gb55A\nzy/7fVVWV9bTaSIS/Tadbchuj8N1T1w7h4rjG2eK5yfAtKxXNLNYrtf/E3InYodF\nBEIseo0f14ey60XDPjG/fup1NrptHRuHJMexWycztHgnQHNMWrGFQU4DIfop+/ln\nmsTghb5Wyqk9gZW7osyqZfFTGHNZ9Eyu46lzq/0cavChEoHpScgQbTwHV93G/bze\nTX2PoMtRX9A/hQlrA1HJe5wz34xBrEJJbXHeAqTZ/jmvt6wcRfE=\n=kr6F\n-----END PGP SIGNATURE-----", "payload": "tree 2d3ae4c7a1cb800273757906d1464db7333ee977\nparent cb2462c53f2cc3f140c0f1ea0976261cab968a34\nauthor Aaron Hill <aa1ronham@gmail.com> 1601171789 -0400\ncommitter Aaron Hill <aa1ronham@gmail.com> 1603130358 -0400\n\nRewrite `collect_tokens` implementations to use a flattened buffer\n\nInstead of trying to collect tokens at each depth, we 'flatten' the\nstream as we go allong, pushing open/close delimiters to our buffer\njust like regular tokens. One capturing is complete, we reconstruct a\nnested `TokenTree::Delimited` structure, producing a normal\n`TokenStream`.\n\nThe reconstructed `TokenStream` is not created immediately - instead, it is\nproduced on-demand by a closure (wrapped in a new `LazyTokenStream` type). This\nclosure stores a clone of the original `TokenCursor`, plus a record of the\nnumber of calls to `next()/next_desugared()`. This is sufficient to reconstruct\nthe tokenstream seen by the callback without storing any additional state. If\nthe tokenstream is never used (e.g. when a captured `macro_rules!` argument is\nnever passed to a proc macro), we never actually create a `TokenStream`.\n\nThis implementation has a number of advantages over the previous one:\n\n* It is significantly simpler, with no edge cases around capturing the\n  start/end of a delimited group.\n\n* It can be easily extended to allow replacing tokens an an arbitrary\n  'depth' by just using `Vec::splice` at the proper position. This is\n  important for PR #76130, which requires us to track information about\n  attributes along with tokens.\n\n* The lazy approach to `TokenStream` construction allows us to easily\n  parse an AST struct, and then decide after the fact whether we need a\n  `TokenStream`. This will be useful when we start collecting tokens for\n  `Attribute` - we can discard the `LazyTokenStream` if the parsed\n  attribute doesn't need tokens (e.g. is a builtin attribute).\n\nThe performance impact seems to be neglibile (see\nhttps://github.com/rust-lang/rust/pull/77250#issuecomment-703960604). There is a\nsmall slowdown on a few benchmarks, but it only rises above 1% for incremental\nbuilds, where it represents a larger fraction of the much smaller instruction\ncount. There a ~1% speedup on a few other incremental benchmarks - my guess is\nthat the speedups and slowdowns will usually cancel out in practice.\n"}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/593fdd3d45d7565e34dc429788fa81ca2e25a2d4", "html_url": "https://github.com/rust-lang/rust/commit/593fdd3d45d7565e34dc429788fa81ca2e25a2d4", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/593fdd3d45d7565e34dc429788fa81ca2e25a2d4/comments", "author": {"login": "Aaron1011", "id": 1408859, "node_id": "MDQ6VXNlcjE0MDg4NTk=", "avatar_url": "https://avatars.githubusercontent.com/u/1408859?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Aaron1011", "html_url": "https://github.com/Aaron1011", "followers_url": "https://api.github.com/users/Aaron1011/followers", "following_url": "https://api.github.com/users/Aaron1011/following{/other_user}", "gists_url": "https://api.github.com/users/Aaron1011/gists{/gist_id}", "starred_url": "https://api.github.com/users/Aaron1011/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Aaron1011/subscriptions", "organizations_url": "https://api.github.com/users/Aaron1011/orgs", "repos_url": "https://api.github.com/users/Aaron1011/repos", "events_url": "https://api.github.com/users/Aaron1011/events{/privacy}", "received_events_url": "https://api.github.com/users/Aaron1011/received_events", "type": "User", "site_admin": false}, "committer": {"login": "Aaron1011", "id": 1408859, "node_id": "MDQ6VXNlcjE0MDg4NTk=", "avatar_url": "https://avatars.githubusercontent.com/u/1408859?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Aaron1011", "html_url": "https://github.com/Aaron1011", "followers_url": "https://api.github.com/users/Aaron1011/followers", "following_url": "https://api.github.com/users/Aaron1011/following{/other_user}", "gists_url": "https://api.github.com/users/Aaron1011/gists{/gist_id}", "starred_url": "https://api.github.com/users/Aaron1011/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Aaron1011/subscriptions", "organizations_url": "https://api.github.com/users/Aaron1011/orgs", "repos_url": "https://api.github.com/users/Aaron1011/repos", "events_url": "https://api.github.com/users/Aaron1011/events{/privacy}", "received_events_url": "https://api.github.com/users/Aaron1011/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "cb2462c53f2cc3f140c0f1ea0976261cab968a34", "url": "https://api.github.com/repos/rust-lang/rust/commits/cb2462c53f2cc3f140c0f1ea0976261cab968a34", "html_url": "https://github.com/rust-lang/rust/commit/cb2462c53f2cc3f140c0f1ea0976261cab968a34"}], "stats": {"total": 421, "additions": 254, "deletions": 167}, "files": [{"sha": "9eb934c0c9e74aa6905d4f5d33a13ed813639329", "filename": "compiler/rustc_ast/src/ast.rs", "status": "modified", "additions": 10, "deletions": 10, "changes": 20, "blob_url": "https://github.com/rust-lang/rust/blob/593fdd3d45d7565e34dc429788fa81ca2e25a2d4/compiler%2Frustc_ast%2Fsrc%2Fast.rs", "raw_url": "https://github.com/rust-lang/rust/raw/593fdd3d45d7565e34dc429788fa81ca2e25a2d4/compiler%2Frustc_ast%2Fsrc%2Fast.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_ast%2Fsrc%2Fast.rs?ref=593fdd3d45d7565e34dc429788fa81ca2e25a2d4", "patch": "@@ -24,7 +24,7 @@ pub use UnsafeSource::*;\n \n use crate::ptr::P;\n use crate::token::{self, CommentKind, DelimToken};\n-use crate::tokenstream::{DelimSpan, TokenStream, TokenTree};\n+use crate::tokenstream::{DelimSpan, LazyTokenStream, TokenStream, TokenTree};\n \n use rustc_data_structures::stable_hasher::{HashStable, StableHasher};\n use rustc_data_structures::stack::ensure_sufficient_stack;\n@@ -97,7 +97,7 @@ pub struct Path {\n     /// The segments in the path: the things separated by `::`.\n     /// Global paths begin with `kw::PathRoot`.\n     pub segments: Vec<PathSegment>,\n-    pub tokens: Option<TokenStream>,\n+    pub tokens: Option<LazyTokenStream>,\n }\n \n impl PartialEq<Symbol> for Path {\n@@ -535,7 +535,7 @@ pub struct Block {\n     /// Distinguishes between `unsafe { ... }` and `{ ... }`.\n     pub rules: BlockCheckMode,\n     pub span: Span,\n-    pub tokens: Option<TokenStream>,\n+    pub tokens: Option<LazyTokenStream>,\n }\n \n /// A match pattern.\n@@ -546,7 +546,7 @@ pub struct Pat {\n     pub id: NodeId,\n     pub kind: PatKind,\n     pub span: Span,\n-    pub tokens: Option<TokenStream>,\n+    pub tokens: Option<LazyTokenStream>,\n }\n \n impl Pat {\n@@ -892,7 +892,7 @@ pub struct Stmt {\n     pub id: NodeId,\n     pub kind: StmtKind,\n     pub span: Span,\n-    pub tokens: Option<TokenStream>,\n+    pub tokens: Option<LazyTokenStream>,\n }\n \n impl Stmt {\n@@ -1040,7 +1040,7 @@ pub struct Expr {\n     pub kind: ExprKind,\n     pub span: Span,\n     pub attrs: AttrVec,\n-    pub tokens: Option<TokenStream>,\n+    pub tokens: Option<LazyTokenStream>,\n }\n \n // `Expr` is used a lot. Make sure it doesn't unintentionally get bigger.\n@@ -1835,7 +1835,7 @@ pub struct Ty {\n     pub id: NodeId,\n     pub kind: TyKind,\n     pub span: Span,\n-    pub tokens: Option<TokenStream>,\n+    pub tokens: Option<LazyTokenStream>,\n }\n \n impl Clone for Ty {\n@@ -2408,7 +2408,7 @@ impl<D: Decoder> rustc_serialize::Decodable<D> for AttrId {\n pub struct AttrItem {\n     pub path: Path,\n     pub args: MacArgs,\n-    pub tokens: Option<TokenStream>,\n+    pub tokens: Option<LazyTokenStream>,\n }\n \n /// A list of attributes.\n@@ -2482,7 +2482,7 @@ pub enum CrateSugar {\n pub struct Visibility {\n     pub kind: VisibilityKind,\n     pub span: Span,\n-    pub tokens: Option<TokenStream>,\n+    pub tokens: Option<LazyTokenStream>,\n }\n \n #[derive(Clone, Encodable, Decodable, Debug)]\n@@ -2569,7 +2569,7 @@ pub struct Item<K = ItemKind> {\n     ///\n     /// Note that the tokens here do not include the outer attributes, but will\n     /// include inner attributes.\n-    pub tokens: Option<TokenStream>,\n+    pub tokens: Option<LazyTokenStream>,\n }\n \n impl Item {"}, {"sha": "8a7277fa7d956c540f2f4c38e3699e5db8a91372", "filename": "compiler/rustc_ast/src/tokenstream.rs", "status": "modified", "additions": 73, "deletions": 1, "changes": 74, "blob_url": "https://github.com/rust-lang/rust/blob/593fdd3d45d7565e34dc429788fa81ca2e25a2d4/compiler%2Frustc_ast%2Fsrc%2Ftokenstream.rs", "raw_url": "https://github.com/rust-lang/rust/raw/593fdd3d45d7565e34dc429788fa81ca2e25a2d4/compiler%2Frustc_ast%2Fsrc%2Ftokenstream.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_ast%2Fsrc%2Ftokenstream.rs?ref=593fdd3d45d7565e34dc429788fa81ca2e25a2d4", "patch": "@@ -16,8 +16,9 @@\n use crate::token::{self, DelimToken, Token, TokenKind};\n \n use rustc_data_structures::stable_hasher::{HashStable, StableHasher};\n-use rustc_data_structures::sync::Lrc;\n+use rustc_data_structures::sync::{self, Lrc};\n use rustc_macros::HashStable_Generic;\n+use rustc_serialize::{Decodable, Decoder, Encodable, Encoder};\n use rustc_span::{Span, DUMMY_SP};\n use smallvec::{smallvec, SmallVec};\n \n@@ -119,6 +120,77 @@ where\n     }\n }\n \n+// A cloneable callback which produces a `TokenStream`. Each clone\n+// of this should produce the same `TokenStream`\n+pub trait CreateTokenStream: sync::Send + sync::Sync + FnOnce() -> TokenStream {\n+    // Workaround for the fact that `Clone` is not object-safe\n+    fn clone_it(&self) -> Box<dyn CreateTokenStream>;\n+}\n+\n+impl<F: 'static + Clone + sync::Send + sync::Sync + FnOnce() -> TokenStream> CreateTokenStream\n+    for F\n+{\n+    fn clone_it(&self) -> Box<dyn CreateTokenStream> {\n+        Box::new(self.clone())\n+    }\n+}\n+\n+impl Clone for Box<dyn CreateTokenStream> {\n+    fn clone(&self) -> Self {\n+        let val: &(dyn CreateTokenStream) = &**self;\n+        val.clone_it()\n+    }\n+}\n+\n+/// A lazy version of `TokenStream`, which may defer creation\n+/// of an actual `TokenStream` until it is needed.\n+pub type LazyTokenStream = Lrc<LazyTokenStreamInner>;\n+\n+#[derive(Clone)]\n+pub enum LazyTokenStreamInner {\n+    Lazy(Box<dyn CreateTokenStream>),\n+    Ready(TokenStream),\n+}\n+\n+impl std::fmt::Debug for LazyTokenStreamInner {\n+    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {\n+        match self {\n+            LazyTokenStreamInner::Lazy(..) => f.debug_struct(\"LazyTokenStream::Lazy\").finish(),\n+            LazyTokenStreamInner::Ready(..) => f.debug_struct(\"LazyTokenStream::Ready\").finish(),\n+        }\n+    }\n+}\n+\n+impl LazyTokenStreamInner {\n+    pub fn into_token_stream(&self) -> TokenStream {\n+        match self {\n+            // Note that we do not cache this. If this ever becomes a performance\n+            // problem, we should investigate wrapping `LazyTokenStreamInner`\n+            // in a lock\n+            LazyTokenStreamInner::Lazy(cb) => (cb.clone())(),\n+            LazyTokenStreamInner::Ready(stream) => stream.clone(),\n+        }\n+    }\n+}\n+\n+impl<S: Encoder> Encodable<S> for LazyTokenStreamInner {\n+    fn encode(&self, _s: &mut S) -> Result<(), S::Error> {\n+        panic!(\"Attempted to encode LazyTokenStream\");\n+    }\n+}\n+\n+impl<D: Decoder> Decodable<D> for LazyTokenStreamInner {\n+    fn decode(_d: &mut D) -> Result<Self, D::Error> {\n+        panic!(\"Attempted to decode LazyTokenStream\");\n+    }\n+}\n+\n+impl<CTX> HashStable<CTX> for LazyTokenStreamInner {\n+    fn hash_stable(&self, _hcx: &mut CTX, _hasher: &mut StableHasher) {\n+        panic!(\"Attempted to compute stable hash for LazyTokenStream\");\n+    }\n+}\n+\n /// A `TokenStream` is an abstract sequence of tokens, organized into `TokenTree`s.\n ///\n /// The goal is for procedural macros to work with `TokenStream`s and `TokenTree`s"}, {"sha": "e073f57108838e0fadcf1e9a9b520908b09fd01f", "filename": "compiler/rustc_parse/src/lib.rs", "status": "modified", "additions": 13, "deletions": 10, "changes": 23, "blob_url": "https://github.com/rust-lang/rust/blob/593fdd3d45d7565e34dc429788fa81ca2e25a2d4/compiler%2Frustc_parse%2Fsrc%2Flib.rs", "raw_url": "https://github.com/rust-lang/rust/raw/593fdd3d45d7565e34dc429788fa81ca2e25a2d4/compiler%2Frustc_parse%2Fsrc%2Flib.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_parse%2Fsrc%2Flib.rs?ref=593fdd3d45d7565e34dc429788fa81ca2e25a2d4", "patch": "@@ -8,7 +8,7 @@\n \n use rustc_ast as ast;\n use rustc_ast::token::{self, DelimToken, Nonterminal, Token, TokenKind};\n-use rustc_ast::tokenstream::{self, TokenStream, TokenTree};\n+use rustc_ast::tokenstream::{self, LazyTokenStream, TokenStream, TokenTree};\n use rustc_ast_pretty::pprust;\n use rustc_data_structures::sync::Lrc;\n use rustc_errors::{Diagnostic, FatalError, Level, PResult};\n@@ -248,29 +248,32 @@ pub fn nt_to_tokenstream(nt: &Nonterminal, sess: &ParseSess, span: Span) -> Toke\n     // As a result, some AST nodes are annotated with the token stream they\n     // came from. Here we attempt to extract these lossless token streams\n     // before we fall back to the stringification.\n+\n+    let convert_tokens = |tokens: Option<LazyTokenStream>| tokens.map(|t| t.into_token_stream());\n+\n     let tokens = match *nt {\n         Nonterminal::NtItem(ref item) => {\n             prepend_attrs(sess, &item.attrs, item.tokens.as_ref(), span)\n         }\n-        Nonterminal::NtBlock(ref block) => block.tokens.clone(),\n+        Nonterminal::NtBlock(ref block) => convert_tokens(block.tokens.clone()),\n         Nonterminal::NtStmt(ref stmt) => {\n             // FIXME: We currently only collect tokens for `:stmt`\n             // matchers in `macro_rules!` macros. When we start collecting\n             // tokens for attributes on statements, we will need to prepend\n             // attributes here\n-            stmt.tokens.clone()\n+            convert_tokens(stmt.tokens.clone())\n         }\n-        Nonterminal::NtPat(ref pat) => pat.tokens.clone(),\n-        Nonterminal::NtTy(ref ty) => ty.tokens.clone(),\n+        Nonterminal::NtPat(ref pat) => convert_tokens(pat.tokens.clone()),\n+        Nonterminal::NtTy(ref ty) => convert_tokens(ty.tokens.clone()),\n         Nonterminal::NtIdent(ident, is_raw) => {\n             Some(tokenstream::TokenTree::token(token::Ident(ident.name, is_raw), ident.span).into())\n         }\n         Nonterminal::NtLifetime(ident) => {\n             Some(tokenstream::TokenTree::token(token::Lifetime(ident.name), ident.span).into())\n         }\n-        Nonterminal::NtMeta(ref attr) => attr.tokens.clone(),\n-        Nonterminal::NtPath(ref path) => path.tokens.clone(),\n-        Nonterminal::NtVis(ref vis) => vis.tokens.clone(),\n+        Nonterminal::NtMeta(ref attr) => convert_tokens(attr.tokens.clone()),\n+        Nonterminal::NtPath(ref path) => convert_tokens(path.tokens.clone()),\n+        Nonterminal::NtVis(ref vis) => convert_tokens(vis.tokens.clone()),\n         Nonterminal::NtTT(ref tt) => Some(tt.clone().into()),\n         Nonterminal::NtExpr(ref expr) | Nonterminal::NtLiteral(ref expr) => {\n             if expr.tokens.is_none() {\n@@ -602,10 +605,10 @@ fn token_probably_equal_for_proc_macro(first: &Token, other: &Token) -> bool {\n fn prepend_attrs(\n     sess: &ParseSess,\n     attrs: &[ast::Attribute],\n-    tokens: Option<&tokenstream::TokenStream>,\n+    tokens: Option<&tokenstream::LazyTokenStream>,\n     span: rustc_span::Span,\n ) -> Option<tokenstream::TokenStream> {\n-    let tokens = tokens?;\n+    let tokens = tokens?.clone().into_token_stream();\n     if attrs.is_empty() {\n         return Some(tokens.clone());\n     }"}, {"sha": "73439643d69b9e031d8e9f693a76c9287802e37e", "filename": "compiler/rustc_parse/src/parser/attr.rs", "status": "modified", "additions": 14, "deletions": 1, "changes": 15, "blob_url": "https://github.com/rust-lang/rust/blob/593fdd3d45d7565e34dc429788fa81ca2e25a2d4/compiler%2Frustc_parse%2Fsrc%2Fparser%2Fattr.rs", "raw_url": "https://github.com/rust-lang/rust/raw/593fdd3d45d7565e34dc429788fa81ca2e25a2d4/compiler%2Frustc_parse%2Fsrc%2Fparser%2Fattr.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_parse%2Fsrc%2Fparser%2Fattr.rs?ref=593fdd3d45d7565e34dc429788fa81ca2e25a2d4", "patch": "@@ -4,7 +4,7 @@ use rustc_ast::attr;\n use rustc_ast::token::{self, Nonterminal};\n use rustc_ast_pretty::pprust;\n use rustc_errors::{error_code, PResult};\n-use rustc_span::Span;\n+use rustc_span::{sym, Span};\n \n use tracing::debug;\n \n@@ -302,3 +302,16 @@ impl<'a> Parser<'a> {\n         Err(self.struct_span_err(self.token.span, &msg))\n     }\n }\n+\n+pub fn maybe_needs_tokens(attrs: &[ast::Attribute]) -> bool {\n+    attrs.iter().any(|attr| {\n+        if let Some(ident) = attr.ident() {\n+            ident.name == sym::derive\n+            // This might apply a custom attribute/derive\n+            || ident.name == sym::cfg_attr\n+            || !rustc_feature::is_builtin_attr_name(ident.name)\n+        } else {\n+            true\n+        }\n+    })\n+}"}, {"sha": "698a7e7d9cde80b5e014b23035836684ffd65f61", "filename": "compiler/rustc_parse/src/parser/expr.rs", "status": "modified", "additions": 10, "deletions": 9, "changes": 19, "blob_url": "https://github.com/rust-lang/rust/blob/593fdd3d45d7565e34dc429788fa81ca2e25a2d4/compiler%2Frustc_parse%2Fsrc%2Fparser%2Fexpr.rs", "raw_url": "https://github.com/rust-lang/rust/raw/593fdd3d45d7565e34dc429788fa81ca2e25a2d4/compiler%2Frustc_parse%2Fsrc%2Fparser%2Fexpr.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_parse%2Fsrc%2Fparser%2Fexpr.rs?ref=593fdd3d45d7565e34dc429788fa81ca2e25a2d4", "patch": "@@ -6,6 +6,7 @@ use crate::maybe_recover_from_interpolated_ty_qpath;\n \n use rustc_ast::ptr::P;\n use rustc_ast::token::{self, Token, TokenKind};\n+use rustc_ast::tokenstream::Spacing;\n use rustc_ast::util::classify;\n use rustc_ast::util::literal::LitError;\n use rustc_ast::util::parser::{prec_let_scrutinee_needs_par, AssocOp, Fixity};\n@@ -18,7 +19,6 @@ use rustc_span::source_map::{self, Span, Spanned};\n use rustc_span::symbol::{kw, sym, Ident, Symbol};\n use rustc_span::{BytePos, Pos};\n use std::mem;\n-use tracing::debug;\n \n /// Possibly accepts an `token::Interpolated` expression (a pre-parsed expression\n /// dropped into the token stream, which happens while parsing the result of\n@@ -459,7 +459,7 @@ impl<'a> Parser<'a> {\n     /// Parses a prefix-unary-operator expr.\n     fn parse_prefix_expr(&mut self, attrs: Option<AttrVec>) -> PResult<'a, P<Expr>> {\n         let attrs = self.parse_or_use_outer_attributes(attrs)?;\n-        self.maybe_collect_tokens(!attrs.is_empty(), |this| {\n+        self.maybe_collect_tokens(super::attr::maybe_needs_tokens(&attrs), |this| {\n             let lo = this.token.span;\n             // Note: when adding new unary operators, don't forget to adjust TokenKind::can_begin_expr()\n             let (hi, ex) = match this.token.uninterpolate().kind {\n@@ -884,7 +884,7 @@ impl<'a> Parser<'a> {\n                 assert!(suffix.is_none());\n                 let symbol = Symbol::intern(&i);\n                 self.token = Token::new(token::Ident(symbol, false), ident_span);\n-                let next_token = Token::new(token::Dot, dot_span);\n+                let next_token = (Token::new(token::Dot, dot_span), self.token_spacing);\n                 self.parse_tuple_field_access_expr(lo, base, symbol, None, Some(next_token))\n             }\n             // 1.2 | 1.2e3\n@@ -902,12 +902,14 @@ impl<'a> Parser<'a> {\n                 };\n                 let symbol1 = Symbol::intern(&i1);\n                 self.token = Token::new(token::Ident(symbol1, false), ident1_span);\n-                let next_token1 = Token::new(token::Dot, dot_span);\n+                // This needs to be `Spacing::Alone` to prevent regressions.\n+                // See issue #76399 and PR #76285 for more details\n+                let next_token1 = (Token::new(token::Dot, dot_span), Spacing::Alone);\n                 let base1 =\n                     self.parse_tuple_field_access_expr(lo, base, symbol1, None, Some(next_token1));\n                 let symbol2 = Symbol::intern(&i2);\n                 let next_token2 = Token::new(token::Ident(symbol2, false), ident2_span);\n-                self.bump_with(next_token2); // `.`\n+                self.bump_with((next_token2, self.token_spacing)); // `.`\n                 self.parse_tuple_field_access_expr(lo, base1, symbol2, suffix, None)\n             }\n             // 1e+ | 1e- (recovered)\n@@ -930,7 +932,7 @@ impl<'a> Parser<'a> {\n         base: P<Expr>,\n         field: Symbol,\n         suffix: Option<Symbol>,\n-        next_token: Option<Token>,\n+        next_token: Option<(Token, Spacing)>,\n     ) -> P<Expr> {\n         match next_token {\n             Some(next_token) => self.bump_with(next_token),\n@@ -1109,12 +1111,11 @@ impl<'a> Parser<'a> {\n \n     fn maybe_collect_tokens(\n         &mut self,\n-        has_outer_attrs: bool,\n+        needs_tokens: bool,\n         f: impl FnOnce(&mut Self) -> PResult<'a, P<Expr>>,\n     ) -> PResult<'a, P<Expr>> {\n-        if has_outer_attrs {\n+        if needs_tokens {\n             let (mut expr, tokens) = self.collect_tokens(f)?;\n-            debug!(\"maybe_collect_tokens: Collected tokens for {:?} (tokens {:?}\", expr, tokens);\n             expr.tokens = Some(tokens);\n             Ok(expr)\n         } else {"}, {"sha": "4ad259715bd980dd108affff65cfc0bcde5cfb2d", "filename": "compiler/rustc_parse/src/parser/item.rs", "status": "modified", "additions": 3, "deletions": 2, "changes": 5, "blob_url": "https://github.com/rust-lang/rust/blob/593fdd3d45d7565e34dc429788fa81ca2e25a2d4/compiler%2Frustc_parse%2Fsrc%2Fparser%2Fitem.rs", "raw_url": "https://github.com/rust-lang/rust/raw/593fdd3d45d7565e34dc429788fa81ca2e25a2d4/compiler%2Frustc_parse%2Fsrc%2Fparser%2Fitem.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_parse%2Fsrc%2Fparser%2Fitem.rs?ref=593fdd3d45d7565e34dc429788fa81ca2e25a2d4", "patch": "@@ -116,15 +116,16 @@ impl<'a> Parser<'a> {\n             Some(item.into_inner())\n         });\n \n+        let needs_tokens = super::attr::maybe_needs_tokens(&attrs);\n+\n         let mut unclosed_delims = vec![];\n-        let has_attrs = !attrs.is_empty();\n         let parse_item = |this: &mut Self| {\n             let item = this.parse_item_common_(attrs, mac_allowed, attrs_allowed, req_name);\n             unclosed_delims.append(&mut this.unclosed_delims);\n             item\n         };\n \n-        let (mut item, tokens) = if has_attrs {\n+        let (mut item, tokens) = if needs_tokens {\n             let (item, tokens) = self.collect_tokens(parse_item)?;\n             (item, Some(tokens))\n         } else {"}, {"sha": "f726abf9df39bd500953e7920a4f25635899bcef", "filename": "compiler/rustc_parse/src/parser/mod.rs", "status": "modified", "additions": 131, "deletions": 134, "changes": 265, "blob_url": "https://github.com/rust-lang/rust/blob/593fdd3d45d7565e34dc429788fa81ca2e25a2d4/compiler%2Frustc_parse%2Fsrc%2Fparser%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/593fdd3d45d7565e34dc429788fa81ca2e25a2d4/compiler%2Frustc_parse%2Fsrc%2Fparser%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_parse%2Fsrc%2Fparser%2Fmod.rs?ref=593fdd3d45d7565e34dc429788fa81ca2e25a2d4", "patch": "@@ -16,13 +16,15 @@ pub use path::PathStyle;\n \n use rustc_ast::ptr::P;\n use rustc_ast::token::{self, DelimToken, Token, TokenKind};\n-use rustc_ast::tokenstream::{self, DelimSpan, TokenStream, TokenTree, TreeAndSpacing};\n+use rustc_ast::tokenstream::{self, DelimSpan, LazyTokenStream, LazyTokenStreamInner, Spacing};\n+use rustc_ast::tokenstream::{TokenStream, TokenTree};\n use rustc_ast::DUMMY_NODE_ID;\n use rustc_ast::{self as ast, AnonConst, AttrStyle, AttrVec, Const, CrateSugar, Extern, Unsafe};\n use rustc_ast::{Async, Expr, ExprKind, MacArgs, MacDelimiter, Mutability, StrLit};\n use rustc_ast::{Visibility, VisibilityKind};\n use rustc_ast_pretty::pprust;\n-use rustc_errors::{struct_span_err, Applicability, DiagnosticBuilder, FatalError, PResult};\n+use rustc_errors::PResult;\n+use rustc_errors::{struct_span_err, Applicability, DiagnosticBuilder, FatalError};\n use rustc_session::parse::ParseSess;\n use rustc_span::source_map::{Span, DUMMY_SP};\n use rustc_span::symbol::{kw, sym, Ident, Symbol};\n@@ -85,10 +87,14 @@ pub struct Parser<'a> {\n     pub sess: &'a ParseSess,\n     /// The current token.\n     pub token: Token,\n+    /// The spacing for the current token\n+    pub token_spacing: Spacing,\n     /// The previous token.\n     pub prev_token: Token,\n     restrictions: Restrictions,\n     expected_tokens: Vec<TokenType>,\n+    // Important: This must only be advanced from `next_tok`\n+    // to ensure that `token_cursor.num_next_calls` is updated properly\n     token_cursor: TokenCursor,\n     desugar_doc_comments: bool,\n     /// This field is used to keep track of how many left angle brackets we have seen. This is\n@@ -120,8 +126,10 @@ impl<'a> Drop for Parser<'a> {\n struct TokenCursor {\n     frame: TokenCursorFrame,\n     stack: Vec<TokenCursorFrame>,\n-    cur_token: Option<TreeAndSpacing>,\n-    collecting: Option<Collecting>,\n+    desugar_doc_comments: bool,\n+    // Counts the number of calls to `next` or `next_desugared`,\n+    // depending on whether `desugar_doc_comments` is set.\n+    num_next_calls: usize,\n }\n \n #[derive(Clone)]\n@@ -133,40 +141,22 @@ struct TokenCursorFrame {\n     close_delim: bool,\n }\n \n-/// Used to track additional state needed by `collect_tokens`\n-#[derive(Clone, Debug)]\n-struct Collecting {\n-    /// Holds the current tokens captured during the most\n-    /// recent call to `collect_tokens`\n-    buf: Vec<TreeAndSpacing>,\n-    /// The depth of the `TokenCursor` stack at the time\n-    /// collection was started. When we encounter a `TokenTree::Delimited`,\n-    /// we want to record the `TokenTree::Delimited` itself,\n-    /// but *not* any of the inner tokens while we are inside\n-    /// the new frame (this would cause us to record duplicate tokens).\n-    ///\n-    /// This `depth` fields tracks stack depth we are recording tokens.\n-    /// Only tokens encountered at this depth will be recorded. See\n-    /// `TokenCursor::next` for more details.\n-    depth: usize,\n-}\n-\n impl TokenCursorFrame {\n-    fn new(span: DelimSpan, delim: DelimToken, tts: &TokenStream) -> Self {\n+    fn new(span: DelimSpan, delim: DelimToken, tts: TokenStream) -> Self {\n         TokenCursorFrame {\n             delim,\n             span,\n             open_delim: delim == token::NoDelim,\n-            tree_cursor: tts.clone().into_trees(),\n+            tree_cursor: tts.into_trees(),\n             close_delim: delim == token::NoDelim,\n         }\n     }\n }\n \n impl TokenCursor {\n-    fn next(&mut self) -> Token {\n+    fn next(&mut self) -> (Token, Spacing) {\n         loop {\n-            let tree = if !self.frame.open_delim {\n+            let (tree, spacing) = if !self.frame.open_delim {\n                 self.frame.open_delim = true;\n                 TokenTree::open_tt(self.frame.span, self.frame.delim).into()\n             } else if let Some(tree) = self.frame.tree_cursor.next_with_spacing() {\n@@ -178,40 +168,24 @@ impl TokenCursor {\n                 self.frame = frame;\n                 continue;\n             } else {\n-                return Token::new(token::Eof, DUMMY_SP);\n+                (TokenTree::Token(Token::new(token::Eof, DUMMY_SP)), Spacing::Alone)\n             };\n \n-            // Don't set an open delimiter as our current token - we want\n-            // to leave it as the full `TokenTree::Delimited` from the previous\n-            // iteration of this loop\n-            if !matches!(tree.0, TokenTree::Token(Token { kind: TokenKind::OpenDelim(_), .. })) {\n-                self.cur_token = Some(tree.clone());\n-            }\n-\n-            if let Some(collecting) = &mut self.collecting {\n-                if collecting.depth == self.stack.len() {\n-                    debug!(\n-                        \"TokenCursor::next():  collected {:?} at depth {:?}\",\n-                        tree,\n-                        self.stack.len()\n-                    );\n-                    collecting.buf.push(tree.clone())\n+            match tree {\n+                TokenTree::Token(token) => {\n+                    return (token, spacing);\n                 }\n-            }\n-\n-            match tree.0 {\n-                TokenTree::Token(token) => return token,\n                 TokenTree::Delimited(sp, delim, tts) => {\n-                    let frame = TokenCursorFrame::new(sp, delim, &tts);\n+                    let frame = TokenCursorFrame::new(sp, delim, tts);\n                     self.stack.push(mem::replace(&mut self.frame, frame));\n                 }\n             }\n         }\n     }\n \n-    fn next_desugared(&mut self) -> Token {\n+    fn next_desugared(&mut self) -> (Token, Spacing) {\n         let (data, attr_style, sp) = match self.next() {\n-            Token { kind: token::DocComment(_, attr_style, data), span } => {\n+            (Token { kind: token::DocComment(_, attr_style, data), span }, _) => {\n                 (data, attr_style, span)\n             }\n             tok => return tok,\n@@ -249,7 +223,7 @@ impl TokenCursor {\n             TokenCursorFrame::new(\n                 delim_span,\n                 token::NoDelim,\n-                &if attr_style == AttrStyle::Inner {\n+                if attr_style == AttrStyle::Inner {\n                     [TokenTree::token(token::Pound, sp), TokenTree::token(token::Not, sp), body]\n                         .iter()\n                         .cloned()\n@@ -351,14 +325,15 @@ impl<'a> Parser<'a> {\n         let mut parser = Parser {\n             sess,\n             token: Token::dummy(),\n+            token_spacing: Spacing::Alone,\n             prev_token: Token::dummy(),\n             restrictions: Restrictions::empty(),\n             expected_tokens: Vec::new(),\n             token_cursor: TokenCursor {\n-                frame: TokenCursorFrame::new(DelimSpan::dummy(), token::NoDelim, &tokens),\n+                frame: TokenCursorFrame::new(DelimSpan::dummy(), token::NoDelim, tokens),\n                 stack: Vec::new(),\n-                cur_token: None,\n-                collecting: None,\n+                num_next_calls: 0,\n+                desugar_doc_comments,\n             },\n             desugar_doc_comments,\n             unmatched_angle_bracket_count: 0,\n@@ -375,17 +350,18 @@ impl<'a> Parser<'a> {\n         parser\n     }\n \n-    fn next_tok(&mut self, fallback_span: Span) -> Token {\n-        let mut next = if self.desugar_doc_comments {\n+    fn next_tok(&mut self, fallback_span: Span) -> (Token, Spacing) {\n+        let (mut next, spacing) = if self.desugar_doc_comments {\n             self.token_cursor.next_desugared()\n         } else {\n             self.token_cursor.next()\n         };\n+        self.token_cursor.num_next_calls += 1;\n         if next.span.is_dummy() {\n             // Tweak the location for better diagnostics, but keep syntactic context intact.\n             next.span = fallback_span.with_ctxt(next.span.ctxt());\n         }\n-        next\n+        (next, spacing)\n     }\n \n     pub fn unexpected<T>(&mut self) -> PResult<'a, T> {\n@@ -573,7 +549,9 @@ impl<'a> Parser<'a> {\n                 let first_span = self.sess.source_map().start_point(self.token.span);\n                 let second_span = self.token.span.with_lo(first_span.hi());\n                 self.token = Token::new(first, first_span);\n-                self.bump_with(Token::new(second, second_span));\n+                // Use the spacing of the glued token as the spacing\n+                // of the unglued second token.\n+                self.bump_with((Token::new(second, second_span), self.token_spacing));\n                 true\n             }\n             _ => {\n@@ -805,7 +783,7 @@ impl<'a> Parser<'a> {\n     }\n \n     /// Advance the parser by one token using provided token as the next one.\n-    fn bump_with(&mut self, next_token: Token) {\n+    fn bump_with(&mut self, (next_token, next_spacing): (Token, Spacing)) {\n         // Bumping after EOF is a bad sign, usually an infinite loop.\n         if self.prev_token.kind == TokenKind::Eof {\n             let msg = \"attempted to bump the parser past EOF (may be stuck in a loop)\";\n@@ -814,6 +792,7 @@ impl<'a> Parser<'a> {\n \n         // Update the current and previous tokens.\n         self.prev_token = mem::replace(&mut self.token, next_token);\n+        self.token_spacing = next_spacing;\n \n         // Diagnostics.\n         self.expected_tokens.clear();\n@@ -984,13 +963,27 @@ impl<'a> Parser<'a> {\n     pub(crate) fn parse_token_tree(&mut self) -> TokenTree {\n         match self.token.kind {\n             token::OpenDelim(..) => {\n-                let frame = mem::replace(\n-                    &mut self.token_cursor.frame,\n-                    self.token_cursor.stack.pop().unwrap(),\n-                );\n-                self.token = Token::new(TokenKind::CloseDelim(frame.delim), frame.span.close);\n+                let depth = self.token_cursor.stack.len();\n+\n+                // We keep advancing the token cursor until we hit\n+                // the matching `CloseDelim` token.\n+                while !(depth == self.token_cursor.stack.len()\n+                    && matches!(self.token.kind, token::CloseDelim(_)))\n+                {\n+                    // Advance one token at a time, so `TokenCursor::next()`\n+                    // can capture these tokens if necessary.\n+                    self.bump();\n+                }\n+                // We are still inside the frame corresponding\n+                // to the delimited stream we captured, so grab\n+                // the tokens from this frame.\n+                let frame = &self.token_cursor.frame;\n+                let stream = frame.tree_cursor.stream.clone();\n+                let span = frame.span;\n+                let delim = frame.delim;\n+                // Consume close delimiter\n                 self.bump();\n-                TokenTree::Delimited(frame.span, frame.delim, frame.tree_cursor.stream)\n+                TokenTree::Delimited(span, delim, stream)\n             }\n             token::CloseDelim(_) | token::Eof => unreachable!(),\n             _ => {\n@@ -1198,79 +1191,45 @@ impl<'a> Parser<'a> {\n     pub fn collect_tokens<R>(\n         &mut self,\n         f: impl FnOnce(&mut Self) -> PResult<'a, R>,\n-    ) -> PResult<'a, (R, TokenStream)> {\n-        // Record all tokens we parse when parsing this item.\n-        let tokens: Vec<TreeAndSpacing> = self.token_cursor.cur_token.clone().into_iter().collect();\n-        debug!(\"collect_tokens: starting with {:?}\", tokens);\n-\n-        // We need special handling for the case where `collect_tokens` is called\n-        // on an opening delimeter (e.g. '('). At this point, we have already pushed\n-        // a new frame - however, we want to record the original `TokenTree::Delimited`,\n-        // for consistency with the case where we start recording one token earlier.\n-        // See `TokenCursor::next` to see how `cur_token` is set up.\n-        let prev_depth =\n-            if matches!(self.token_cursor.cur_token, Some((TokenTree::Delimited(..), _))) {\n-                if self.token_cursor.stack.is_empty() {\n-                    // There is nothing below us in the stack that\n-                    // the function could consume, so the only thing it can legally\n-                    // capture is the entire contents of the current frame.\n-                    return Ok((f(self)?, TokenStream::new(tokens)));\n-                }\n-                // We have already recorded the full `TokenTree::Delimited` when we created\n-                // our `tokens` vector at the start of this function. We are now inside\n-                // a new frame corresponding to the `TokenTree::Delimited` we already recoreded.\n-                // We don't want to record any of the tokens inside this frame, since they\n-                // will be duplicates of the tokens nested inside the `TokenTree::Delimited`.\n-                // Therefore, we set our recording depth to the *previous* frame. This allows\n-                // us to record a sequence like: `(foo).bar()`: the `(foo)` will be recored\n-                // as our initial `cur_token`, while the `.bar()` will be recored after we\n-                // pop the `(foo)` frame.\n-                self.token_cursor.stack.len() - 1\n-            } else {\n-                self.token_cursor.stack.len()\n-            };\n-        let prev_collecting =\n-            self.token_cursor.collecting.replace(Collecting { buf: tokens, depth: prev_depth });\n-\n-        let ret = f(self);\n+    ) -> PResult<'a, (R, LazyTokenStream)> {\n+        let start_token = (self.token.clone(), self.token_spacing);\n+        let mut cursor_snapshot = self.token_cursor.clone();\n+\n+        let ret = f(self)?;\n+\n+        let new_calls = self.token_cursor.num_next_calls;\n+        let num_calls = new_calls - cursor_snapshot.num_next_calls;\n+        let desugar_doc_comments = self.desugar_doc_comments;\n+\n+        // Produces a `TokenStream` on-demand. Using `cursor_snapshot`\n+        // and `num_calls`, we can reconstruct the `TokenStream` seen\n+        // by the callback. This allows us to avoid producing a `TokenStream`\n+        // if it is never needed - for example, a captured `macro_rules!`\n+        // argument that is never passed to a proc macro.\n+        //\n+        // This also makes `Parser` very cheap to clone, since\n+        // there is no intermediate collection buffer to clone.\n+        let lazy_cb = move || {\n+            // The token produced by the final call to `next` or `next_desugared`\n+            // was not actually consumed by the callback. The combination\n+            // of chaining the initial token and using `take` produces the desired\n+            // result - we produce an empty `TokenStream` if no calls were made,\n+            // and omit the final token otherwise.\n+            let tokens = std::iter::once(start_token)\n+                .chain((0..num_calls).map(|_| {\n+                    if desugar_doc_comments {\n+                        cursor_snapshot.next_desugared()\n+                    } else {\n+                        cursor_snapshot.next()\n+                    }\n+                }))\n+                .take(num_calls);\n \n-        let mut collected_tokens = if let Some(collecting) = self.token_cursor.collecting.take() {\n-            collecting.buf\n-        } else {\n-            let msg = \"our vector went away?\";\n-            debug!(\"collect_tokens: {}\", msg);\n-            self.sess.span_diagnostic.delay_span_bug(self.token.span, &msg);\n-            // This can happen due to a bad interaction of two unrelated recovery mechanisms\n-            // with mismatched delimiters *and* recovery lookahead on the likely typo\n-            // `pub ident(` (#62895, different but similar to the case above).\n-            return Ok((ret?, TokenStream::default()));\n+            make_token_stream(tokens)\n         };\n+        let stream = LazyTokenStream::new(LazyTokenStreamInner::Lazy(Box::new(lazy_cb)));\n \n-        debug!(\"collect_tokens: got raw tokens {:?}\", collected_tokens);\n-\n-        // If we're not at EOF our current token wasn't actually consumed by\n-        // `f`, but it'll still be in our list that we pulled out. In that case\n-        // put it back.\n-        let extra_token = if self.token != token::Eof { collected_tokens.pop() } else { None };\n-\n-        if let Some(mut collecting) = prev_collecting {\n-            // If we were previously collecting at the same depth,\n-            // then the previous call to `collect_tokens` needs to see\n-            // the tokens we just recorded.\n-            //\n-            // If we were previously recording at an lower `depth`,\n-            // then the previous `collect_tokens` call already recorded\n-            // this entire frame in the form of a `TokenTree::Delimited`,\n-            // so there is nothing else for us to do.\n-            if collecting.depth == prev_depth {\n-                collecting.buf.extend(collected_tokens.iter().cloned());\n-                collecting.buf.extend(extra_token);\n-                debug!(\"collect_tokens: updating previous buf to {:?}\", collecting);\n-            }\n-            self.token_cursor.collecting = Some(collecting)\n-        }\n-\n-        Ok((ret?, TokenStream::new(collected_tokens)))\n+        Ok((ret, stream))\n     }\n \n     /// `::{` or `::*`\n@@ -1319,3 +1278,41 @@ pub fn emit_unclosed_delims(unclosed_delims: &mut Vec<UnmatchedBrace>, sess: &Pa\n         }\n     }\n }\n+\n+/// Converts a flattened iterator of tokens (including open and close delimiter tokens)\n+/// into a `TokenStream`, creating a `TokenTree::Delimited` for each matching pair\n+/// of open and close delims.\n+fn make_token_stream(tokens: impl Iterator<Item = (Token, Spacing)>) -> TokenStream {\n+    #[derive(Debug)]\n+    struct FrameData {\n+        open: Span,\n+        inner: Vec<(TokenTree, Spacing)>,\n+    }\n+    let mut stack = vec![FrameData { open: DUMMY_SP, inner: vec![] }];\n+    for (token, spacing) in tokens {\n+        match token {\n+            Token { kind: TokenKind::OpenDelim(_), span } => {\n+                stack.push(FrameData { open: span, inner: vec![] });\n+            }\n+            Token { kind: TokenKind::CloseDelim(delim), span } => {\n+                let frame_data = stack.pop().expect(\"Token stack was empty!\");\n+                let dspan = DelimSpan::from_pair(frame_data.open, span);\n+                let stream = TokenStream::new(frame_data.inner);\n+                let delimited = TokenTree::Delimited(dspan, delim, stream);\n+                stack\n+                    .last_mut()\n+                    .unwrap_or_else(|| panic!(\"Bottom token frame is missing for tokens!\"))\n+                    .inner\n+                    .push((delimited, Spacing::Alone));\n+            }\n+            token => stack\n+                .last_mut()\n+                .expect(\"Bottom token frame is missing!\")\n+                .inner\n+                .push((TokenTree::Token(token), spacing)),\n+        }\n+    }\n+    let final_buf = stack.pop().expect(\"Missing final buf!\");\n+    assert!(stack.is_empty(), \"Stack should be empty: final_buf={:?} stack={:?}\", final_buf, stack);\n+    TokenStream::new(final_buf.inner)\n+}"}]}