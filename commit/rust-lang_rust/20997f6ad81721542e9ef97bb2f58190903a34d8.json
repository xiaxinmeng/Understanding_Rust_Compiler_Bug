{"sha": "20997f6ad81721542e9ef97bb2f58190903a34d8", "node_id": "MDY6Q29tbWl0NzI0NzEyOjIwOTk3ZjZhZDgxNzIxNTQyZTllZjk3YmIyZjU4MTkwOTAzYTM0ZDg=", "commit": {"author": {"name": "bors", "email": "bors@rust-lang.org", "date": "2021-08-26T10:49:25Z"}, "committer": {"name": "bors", "email": "bors@rust-lang.org", "date": "2021-08-26T10:49:25Z"}, "message": "Auto merge of #83698 - erikdesjardins:undefconst, r=RalfJung,oli-obk\n\nUse undef for uninitialized bytes in constants\n\nFixes #83657\n\nThis generates good code when the const is fully uninit, e.g.\n\n```rust\n#[no_mangle]\npub const fn fully_uninit() -> MaybeUninit<[u8; 10]> {\n    const M: MaybeUninit<[u8; 10]> = MaybeUninit::uninit();\n    M\n}\n```\ngenerates\n```asm\nfully_uninit:\n\tret\n```\n\nas you would expect.\n\nThere is no improvement, however, when it's partially uninit, e.g.\n\n```rust\npub struct PartiallyUninit {\n    x: u64,\n    y: MaybeUninit<[u8; 10]>\n}\n\n#[no_mangle]\npub const fn partially_uninit() -> PartiallyUninit {\n    const X: PartiallyUninit = PartiallyUninit { x: 0xdeadbeefcafe, y: MaybeUninit::uninit() };\n    X\n}\n```\ngenerates\n```asm\npartially_uninit:\n\tmov\trax, rdi\n\tmov\trcx, qword ptr [rip + .L__unnamed_1+16]\n\tmov\tqword ptr [rdi + 16], rcx\n\tmovups\txmm0, xmmword ptr [rip + .L__unnamed_1]\n\tmovups\txmmword ptr [rdi], xmm0\n\tret\n\n.L__unnamed_1:\n\t.asciz\t\"\\376\\312\\357\\276\\255\\336\\000\"\n\t.zero\t16\n\t.size\t.L__unnamed_1, 24\n```\nwhich copies a bunch of zeros in place of the undef bytes, the same as before this change.\n\nEdit: generating partially-undef constants isn't viable at the moment anyways due to #84565, so it's disabled", "tree": {"sha": "de8dfb8c6e55afc212f24c99cdad28fc0cd1f43d", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/de8dfb8c6e55afc212f24c99cdad28fc0cd1f43d"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/20997f6ad81721542e9ef97bb2f58190903a34d8", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/20997f6ad81721542e9ef97bb2f58190903a34d8", "html_url": "https://github.com/rust-lang/rust/commit/20997f6ad81721542e9ef97bb2f58190903a34d8", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/20997f6ad81721542e9ef97bb2f58190903a34d8/comments", "author": {"login": "bors", "id": 3372342, "node_id": "MDQ6VXNlcjMzNzIzNDI=", "avatar_url": "https://avatars.githubusercontent.com/u/3372342?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bors", "html_url": "https://github.com/bors", "followers_url": "https://api.github.com/users/bors/followers", "following_url": "https://api.github.com/users/bors/following{/other_user}", "gists_url": "https://api.github.com/users/bors/gists{/gist_id}", "starred_url": "https://api.github.com/users/bors/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bors/subscriptions", "organizations_url": "https://api.github.com/users/bors/orgs", "repos_url": "https://api.github.com/users/bors/repos", "events_url": "https://api.github.com/users/bors/events{/privacy}", "received_events_url": "https://api.github.com/users/bors/received_events", "type": "User", "site_admin": false}, "committer": {"login": "bors", "id": 3372342, "node_id": "MDQ6VXNlcjMzNzIzNDI=", "avatar_url": "https://avatars.githubusercontent.com/u/3372342?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bors", "html_url": "https://github.com/bors", "followers_url": "https://api.github.com/users/bors/followers", "following_url": "https://api.github.com/users/bors/following{/other_user}", "gists_url": "https://api.github.com/users/bors/gists{/gist_id}", "starred_url": "https://api.github.com/users/bors/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bors/subscriptions", "organizations_url": "https://api.github.com/users/bors/orgs", "repos_url": "https://api.github.com/users/bors/repos", "events_url": "https://api.github.com/users/bors/events{/privacy}", "received_events_url": "https://api.github.com/users/bors/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "3b3ce374d203445eb1d0dce50f4211f4aceb7db6", "url": "https://api.github.com/repos/rust-lang/rust/commits/3b3ce374d203445eb1d0dce50f4211f4aceb7db6", "html_url": "https://github.com/rust-lang/rust/commit/3b3ce374d203445eb1d0dce50f4211f4aceb7db6"}, {"sha": "adf3b013c8b51e7d6ceea33ef3005896cc2cd030", "url": "https://api.github.com/repos/rust-lang/rust/commits/adf3b013c8b51e7d6ceea33ef3005896cc2cd030", "html_url": "https://github.com/rust-lang/rust/commit/adf3b013c8b51e7d6ceea33ef3005896cc2cd030"}], "stats": {"total": 788, "additions": 615, "deletions": 173}, "files": [{"sha": "a4e4fc4fffb8f500ea14c6599e10383de80fd4f6", "filename": "compiler/rustc_codegen_llvm/src/consts.rs", "status": "modified", "additions": 58, "deletions": 13, "changes": 71, "blob_url": "https://github.com/rust-lang/rust/blob/20997f6ad81721542e9ef97bb2f58190903a34d8/compiler%2Frustc_codegen_llvm%2Fsrc%2Fconsts.rs", "raw_url": "https://github.com/rust-lang/rust/raw/20997f6ad81721542e9ef97bb2f58190903a34d8/compiler%2Frustc_codegen_llvm%2Fsrc%2Fconsts.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_codegen_llvm%2Fsrc%2Fconsts.rs?ref=20997f6ad81721542e9ef97bb2f58190903a34d8", "patch": "@@ -11,21 +11,74 @@ use rustc_codegen_ssa::traits::*;\n use rustc_hir::def_id::DefId;\n use rustc_middle::middle::codegen_fn_attrs::{CodegenFnAttrFlags, CodegenFnAttrs};\n use rustc_middle::mir::interpret::{\n-    read_target_uint, Allocation, ErrorHandled, GlobalAlloc, Pointer, Scalar as InterpScalar,\n+    read_target_uint, Allocation, ErrorHandled, GlobalAlloc, InitChunk, Pointer,\n+    Scalar as InterpScalar,\n };\n use rustc_middle::mir::mono::MonoItem;\n use rustc_middle::ty::{self, Instance, Ty};\n use rustc_middle::{bug, span_bug};\n use rustc_target::abi::{\n     AddressSpace, Align, HasDataLayout, LayoutOf, Primitive, Scalar, Size, WrappingRange,\n };\n+use std::ops::Range;\n use tracing::debug;\n \n pub fn const_alloc_to_llvm(cx: &CodegenCx<'ll, '_>, alloc: &Allocation) -> &'ll Value {\n     let mut llvals = Vec::with_capacity(alloc.relocations().len() + 1);\n     let dl = cx.data_layout();\n     let pointer_size = dl.pointer_size.bytes() as usize;\n \n+    // Note: this function may call `inspect_with_uninit_and_ptr_outside_interpreter`,\n+    // so `range` must be within the bounds of `alloc` and not contain or overlap a relocation.\n+    fn append_chunks_of_init_and_uninit_bytes<'ll, 'a, 'b>(\n+        llvals: &mut Vec<&'ll Value>,\n+        cx: &'a CodegenCx<'ll, 'b>,\n+        alloc: &'a Allocation,\n+        range: Range<usize>,\n+    ) {\n+        let mut chunks = alloc\n+            .init_mask()\n+            .range_as_init_chunks(Size::from_bytes(range.start), Size::from_bytes(range.end));\n+\n+        let chunk_to_llval = move |chunk| match chunk {\n+            InitChunk::Init(range) => {\n+                let range = (range.start.bytes() as usize)..(range.end.bytes() as usize);\n+                let bytes = alloc.inspect_with_uninit_and_ptr_outside_interpreter(range);\n+                cx.const_bytes(bytes)\n+            }\n+            InitChunk::Uninit(range) => {\n+                let len = range.end.bytes() - range.start.bytes();\n+                cx.const_undef(cx.type_array(cx.type_i8(), len))\n+            }\n+        };\n+\n+        // Generating partially-uninit consts inhibits optimizations, so it is disabled by default.\n+        // See https://github.com/rust-lang/rust/issues/84565.\n+        let allow_partially_uninit =\n+            match cx.sess().opts.debugging_opts.partially_uninit_const_threshold {\n+                Some(max) => range.len() <= max,\n+                None => false,\n+            };\n+\n+        if allow_partially_uninit {\n+            llvals.extend(chunks.map(chunk_to_llval));\n+        } else {\n+            let llval = match (chunks.next(), chunks.next()) {\n+                (Some(chunk), None) => {\n+                    // exactly one chunk, either fully init or fully uninit\n+                    chunk_to_llval(chunk)\n+                }\n+                _ => {\n+                    // partially uninit, codegen as if it was initialized\n+                    // (using some arbitrary value for uninit bytes)\n+                    let bytes = alloc.inspect_with_uninit_and_ptr_outside_interpreter(range);\n+                    cx.const_bytes(bytes)\n+                }\n+            };\n+            llvals.push(llval);\n+        }\n+    }\n+\n     let mut next_offset = 0;\n     for &(offset, alloc_id) in alloc.relocations().iter() {\n         let offset = offset.bytes();\n@@ -34,12 +87,8 @@ pub fn const_alloc_to_llvm(cx: &CodegenCx<'ll, '_>, alloc: &Allocation) -> &'ll\n         if offset > next_offset {\n             // This `inspect` is okay since we have checked that it is not within a relocation, it\n             // is within the bounds of the allocation, and it doesn't affect interpreter execution\n-            // (we inspect the result after interpreter execution). Any undef byte is replaced with\n-            // some arbitrary byte value.\n-            //\n-            // FIXME: relay undef bytes to codegen as undef const bytes\n-            let bytes = alloc.inspect_with_uninit_and_ptr_outside_interpreter(next_offset..offset);\n-            llvals.push(cx.const_bytes(bytes));\n+            // (we inspect the result after interpreter execution).\n+            append_chunks_of_init_and_uninit_bytes(&mut llvals, cx, alloc, next_offset..offset);\n         }\n         let ptr_offset = read_target_uint(\n             dl.endian,\n@@ -70,12 +119,8 @@ pub fn const_alloc_to_llvm(cx: &CodegenCx<'ll, '_>, alloc: &Allocation) -> &'ll\n         let range = next_offset..alloc.len();\n         // This `inspect` is okay since we have check that it is after all relocations, it is\n         // within the bounds of the allocation, and it doesn't affect interpreter execution (we\n-        // inspect the result after interpreter execution). Any undef byte is replaced with some\n-        // arbitrary byte value.\n-        //\n-        // FIXME: relay undef bytes to codegen as undef const bytes\n-        let bytes = alloc.inspect_with_uninit_and_ptr_outside_interpreter(range);\n-        llvals.push(cx.const_bytes(bytes));\n+        // inspect the result after interpreter execution).\n+        append_chunks_of_init_and_uninit_bytes(&mut llvals, cx, alloc, range);\n     }\n \n     cx.const_struct(&llvals, true)"}, {"sha": "afab919bc3c2cff582ac90614c5f6d3a7bc9fc51", "filename": "compiler/rustc_interface/src/tests.rs", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "blob_url": "https://github.com/rust-lang/rust/blob/20997f6ad81721542e9ef97bb2f58190903a34d8/compiler%2Frustc_interface%2Fsrc%2Ftests.rs", "raw_url": "https://github.com/rust-lang/rust/raw/20997f6ad81721542e9ef97bb2f58190903a34d8/compiler%2Frustc_interface%2Fsrc%2Ftests.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_interface%2Fsrc%2Ftests.rs?ref=20997f6ad81721542e9ef97bb2f58190903a34d8", "patch": "@@ -743,6 +743,7 @@ fn test_debugging_options_tracking_hash() {\n     tracked!(no_profiler_runtime, true);\n     tracked!(osx_rpath_install_name, true);\n     tracked!(panic_abort_tests, true);\n+    tracked!(partially_uninit_const_threshold, Some(123));\n     tracked!(plt, Some(true));\n     tracked!(polonius, true);\n     tracked!(precise_enum_drop_elaboration, false);"}, {"sha": "b6358f9929448cc22b1baa98ec7123d2da0f6617", "filename": "compiler/rustc_middle/src/mir/interpret/allocation.rs", "status": "modified", "additions": 435, "deletions": 159, "changes": 594, "blob_url": "https://github.com/rust-lang/rust/blob/20997f6ad81721542e9ef97bb2f58190903a34d8/compiler%2Frustc_middle%2Fsrc%2Fmir%2Finterpret%2Fallocation.rs", "raw_url": "https://github.com/rust-lang/rust/raw/20997f6ad81721542e9ef97bb2f58190903a34d8/compiler%2Frustc_middle%2Fsrc%2Fmir%2Finterpret%2Fallocation.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_middle%2Fsrc%2Fmir%2Finterpret%2Fallocation.rs?ref=20997f6ad81721542e9ef97bb2f58190903a34d8", "patch": "@@ -1,7 +1,7 @@\n //! The virtual memory representation of the MIR interpreter.\n \n use std::borrow::Cow;\n-use std::convert::TryFrom;\n+use std::convert::{TryFrom, TryInto};\n use std::iter;\n use std::ops::{Deref, Range};\n use std::ptr;\n@@ -495,129 +495,6 @@ impl<Tag: Copy, Extra> Allocation<Tag, Extra> {\n     }\n }\n \n-/// Uninitialized bytes.\n-impl<Tag: Copy, Extra> Allocation<Tag, Extra> {\n-    /// Checks whether the given range  is entirely initialized.\n-    ///\n-    /// Returns `Ok(())` if it's initialized. Otherwise returns the range of byte\n-    /// indexes of the first contiguous uninitialized access.\n-    fn is_init(&self, range: AllocRange) -> Result<(), Range<Size>> {\n-        self.init_mask.is_range_initialized(range.start, range.end()) // `Size` addition\n-    }\n-\n-    /// Checks that a range of bytes is initialized. If not, returns the `InvalidUninitBytes`\n-    /// error which will report the first range of bytes which is uninitialized.\n-    fn check_init(&self, range: AllocRange) -> AllocResult {\n-        self.is_init(range).or_else(|idx_range| {\n-            Err(AllocError::InvalidUninitBytes(Some(UninitBytesAccess {\n-                access_offset: range.start,\n-                access_size: range.size,\n-                uninit_offset: idx_range.start,\n-                uninit_size: idx_range.end - idx_range.start, // `Size` subtraction\n-            })))\n-        })\n-    }\n-\n-    pub fn mark_init(&mut self, range: AllocRange, is_init: bool) {\n-        if range.size.bytes() == 0 {\n-            return;\n-        }\n-        assert!(self.mutability == Mutability::Mut);\n-        self.init_mask.set_range(range.start, range.end(), is_init);\n-    }\n-}\n-\n-/// Run-length encoding of the uninit mask.\n-/// Used to copy parts of a mask multiple times to another allocation.\n-pub struct InitMaskCompressed {\n-    /// Whether the first range is initialized.\n-    initial: bool,\n-    /// The lengths of ranges that are run-length encoded.\n-    /// The initialization state of the ranges alternate starting with `initial`.\n-    ranges: smallvec::SmallVec<[u64; 1]>,\n-}\n-\n-impl InitMaskCompressed {\n-    pub fn no_bytes_init(&self) -> bool {\n-        // The `ranges` are run-length encoded and of alternating initialization state.\n-        // So if `ranges.len() > 1` then the second block is an initialized range.\n-        !self.initial && self.ranges.len() == 1\n-    }\n-}\n-\n-/// Transferring the initialization mask to other allocations.\n-impl<Tag, Extra> Allocation<Tag, Extra> {\n-    /// Creates a run-length encoding of the initialization mask.\n-    pub fn compress_uninit_range(&self, range: AllocRange) -> InitMaskCompressed {\n-        // Since we are copying `size` bytes from `src` to `dest + i * size` (`for i in 0..repeat`),\n-        // a naive initialization mask copying algorithm would repeatedly have to read the initialization mask from\n-        // the source and write it to the destination. Even if we optimized the memory accesses,\n-        // we'd be doing all of this `repeat` times.\n-        // Therefore we precompute a compressed version of the initialization mask of the source value and\n-        // then write it back `repeat` times without computing any more information from the source.\n-\n-        // A precomputed cache for ranges of initialized / uninitialized bits\n-        // 0000010010001110 will become\n-        // `[5, 1, 2, 1, 3, 3, 1]`,\n-        // where each element toggles the state.\n-\n-        let mut ranges = smallvec::SmallVec::<[u64; 1]>::new();\n-        let initial = self.init_mask.get(range.start);\n-        let mut cur_len = 1;\n-        let mut cur = initial;\n-\n-        for i in 1..range.size.bytes() {\n-            // FIXME: optimize to bitshift the current uninitialized block's bits and read the top bit.\n-            if self.init_mask.get(range.start + Size::from_bytes(i)) == cur {\n-                cur_len += 1;\n-            } else {\n-                ranges.push(cur_len);\n-                cur_len = 1;\n-                cur = !cur;\n-            }\n-        }\n-\n-        ranges.push(cur_len);\n-\n-        InitMaskCompressed { ranges, initial }\n-    }\n-\n-    /// Applies multiple instances of the run-length encoding to the initialization mask.\n-    pub fn mark_compressed_init_range(\n-        &mut self,\n-        defined: &InitMaskCompressed,\n-        range: AllocRange,\n-        repeat: u64,\n-    ) {\n-        // An optimization where we can just overwrite an entire range of initialization\n-        // bits if they are going to be uniformly `1` or `0`.\n-        if defined.ranges.len() <= 1 {\n-            self.init_mask.set_range_inbounds(\n-                range.start,\n-                range.start + range.size * repeat, // `Size` operations\n-                defined.initial,\n-            );\n-            return;\n-        }\n-\n-        for mut j in 0..repeat {\n-            j *= range.size.bytes();\n-            j += range.start.bytes();\n-            let mut cur = defined.initial;\n-            for range in &defined.ranges {\n-                let old_j = j;\n-                j += range;\n-                self.init_mask.set_range_inbounds(\n-                    Size::from_bytes(old_j),\n-                    Size::from_bytes(j),\n-                    cur,\n-                );\n-                cur = !cur;\n-            }\n-        }\n-    }\n-}\n-\n /// \"Relocations\" stores the provenance information of pointers stored in memory.\n #[derive(Clone, PartialEq, Eq, PartialOrd, Ord, Hash, Debug, TyEncodable, TyDecodable)]\n pub struct Relocations<Tag = AllocId>(SortedMap<Size, Tag>);\n@@ -704,35 +581,28 @@ pub struct InitMask {\n impl InitMask {\n     pub const BLOCK_SIZE: u64 = 64;\n \n-    pub fn new(size: Size, state: bool) -> Self {\n-        let mut m = InitMask { blocks: vec![], len: Size::ZERO };\n-        m.grow(size, state);\n-        m\n+    #[inline]\n+    fn bit_index(bits: Size) -> (usize, usize) {\n+        // BLOCK_SIZE is the number of bits that can fit in a `Block`.\n+        // Each bit in a `Block` represents the initialization state of one byte of an allocation,\n+        // so we use `.bytes()` here.\n+        let bits = bits.bytes();\n+        let a = bits / InitMask::BLOCK_SIZE;\n+        let b = bits % InitMask::BLOCK_SIZE;\n+        (usize::try_from(a).unwrap(), usize::try_from(b).unwrap())\n     }\n \n-    /// Checks whether the range `start..end` (end-exclusive) is entirely initialized.\n-    ///\n-    /// Returns `Ok(())` if it's initialized. Otherwise returns a range of byte\n-    /// indexes for the first contiguous span of the uninitialized access.\n     #[inline]\n-    pub fn is_range_initialized(&self, start: Size, end: Size) -> Result<(), Range<Size>> {\n-        if end > self.len {\n-            return Err(self.len..end);\n-        }\n-\n-        // FIXME(oli-obk): optimize this for allocations larger than a block.\n-        let idx = (start.bytes()..end.bytes()).map(Size::from_bytes).find(|&i| !self.get(i));\n+    fn size_from_bit_index(block: impl TryInto<u64>, bit: impl TryInto<u64>) -> Size {\n+        let block = block.try_into().ok().unwrap();\n+        let bit = bit.try_into().ok().unwrap();\n+        Size::from_bytes(block * InitMask::BLOCK_SIZE + bit)\n+    }\n \n-        match idx {\n-            Some(idx) => {\n-                let uninit_end = (idx.bytes()..end.bytes())\n-                    .map(Size::from_bytes)\n-                    .find(|&i| self.get(i))\n-                    .unwrap_or(end);\n-                Err(idx..uninit_end)\n-            }\n-            None => Ok(()),\n-        }\n+    pub fn new(size: Size, state: bool) -> Self {\n+        let mut m = InitMask { blocks: vec![], len: Size::ZERO };\n+        m.grow(size, state);\n+        m\n     }\n \n     pub fn set_range(&mut self, start: Size, end: Size, new_state: bool) {\n@@ -744,8 +614,8 @@ impl InitMask {\n     }\n \n     pub fn set_range_inbounds(&mut self, start: Size, end: Size, new_state: bool) {\n-        let (blocka, bita) = bit_index(start);\n-        let (blockb, bitb) = bit_index(end);\n+        let (blocka, bita) = Self::bit_index(start);\n+        let (blockb, bitb) = Self::bit_index(end);\n         if blocka == blockb {\n             // First set all bits except the first `bita`,\n             // then unset the last `64 - bitb` bits.\n@@ -789,13 +659,13 @@ impl InitMask {\n \n     #[inline]\n     pub fn get(&self, i: Size) -> bool {\n-        let (block, bit) = bit_index(i);\n+        let (block, bit) = Self::bit_index(i);\n         (self.blocks[block] & (1 << bit)) != 0\n     }\n \n     #[inline]\n     pub fn set(&mut self, i: Size, new_state: bool) {\n-        let (block, bit) = bit_index(i);\n+        let (block, bit) = Self::bit_index(i);\n         self.set_bit(block, bit, new_state);\n     }\n \n@@ -825,12 +695,418 @@ impl InitMask {\n         self.len += amount;\n         self.set_range_inbounds(start, start + amount, new_state); // `Size` operation\n     }\n+\n+    /// Returns the index of the first bit in `start..end` (end-exclusive) that is equal to is_init.\n+    fn find_bit(&self, start: Size, end: Size, is_init: bool) -> Option<Size> {\n+        /// A fast implementation of `find_bit`,\n+        /// which skips over an entire block at a time if it's all 0s (resp. 1s),\n+        /// and finds the first 1 (resp. 0) bit inside a block using `trailing_zeros` instead of a loop.\n+        ///\n+        /// Note that all examples below are written with 8 (instead of 64) bit blocks for simplicity,\n+        /// and with the least significant bit (and lowest block) first:\n+        ///\n+        ///          00000000|00000000\n+        ///          ^      ^ ^      ^\n+        ///   index: 0      7 8      15\n+        ///\n+        /// Also, if not stated, assume that `is_init = true`, that is, we are searching for the first 1 bit.\n+        fn find_bit_fast(\n+            init_mask: &InitMask,\n+            start: Size,\n+            end: Size,\n+            is_init: bool,\n+        ) -> Option<Size> {\n+            /// Search one block, returning the index of the first bit equal to `is_init`.\n+            fn search_block(\n+                bits: Block,\n+                block: usize,\n+                start_bit: usize,\n+                is_init: bool,\n+            ) -> Option<Size> {\n+                // For the following examples, assume this function was called with:\n+                //   bits = 0b00111011\n+                //   start_bit = 3\n+                //   is_init = false\n+                // Note that, for the examples in this function, the most significant bit is written first,\n+                // which is backwards compared to the comments in `find_bit`/`find_bit_fast`.\n+\n+                // Invert bits so we're always looking for the first set bit.\n+                //        ! 0b00111011\n+                //   bits = 0b11000100\n+                let bits = if is_init { bits } else { !bits };\n+                // Mask off unused start bits.\n+                //          0b11000100\n+                //        & 0b11111000\n+                //   bits = 0b11000000\n+                let bits = bits & (!0 << start_bit);\n+                // Find set bit, if any.\n+                //   bit = trailing_zeros(0b11000000)\n+                //   bit = 6\n+                if bits == 0 {\n+                    None\n+                } else {\n+                    let bit = bits.trailing_zeros();\n+                    Some(InitMask::size_from_bit_index(block, bit))\n+                }\n+            }\n+\n+            if start >= end {\n+                return None;\n+            }\n+\n+            // Convert `start` and `end` to block indexes and bit indexes within each block.\n+            // We must convert `end` to an inclusive bound to handle block boundaries correctly.\n+            //\n+            // For example:\n+            //\n+            //   (a) 00000000|00000000    (b) 00000000|\n+            //       ^~~~~~~~~~~^             ^~~~~~~~~^\n+            //     start       end          start     end\n+            //\n+            // In both cases, the block index of `end` is 1.\n+            // But we do want to search block 1 in (a), and we don't in (b).\n+            //\n+            // We subtract 1 from both end positions to make them inclusive:\n+            //\n+            //   (a) 00000000|00000000    (b) 00000000|\n+            //       ^~~~~~~~~~^              ^~~~~~~^\n+            //     start    end_inclusive   start end_inclusive\n+            //\n+            // For (a), the block index of `end_inclusive` is 1, and for (b), it's 0.\n+            // This provides the desired behavior of searching blocks 0 and 1 for (a),\n+            // and searching only block 0 for (b).\n+            // There is no concern of overflows since we checked for `start >= end` above.\n+            let (start_block, start_bit) = InitMask::bit_index(start);\n+            let end_inclusive = Size::from_bytes(end.bytes() - 1);\n+            let (end_block_inclusive, _) = InitMask::bit_index(end_inclusive);\n+\n+            // Handle first block: need to skip `start_bit` bits.\n+            //\n+            // We need to handle the first block separately,\n+            // because there may be bits earlier in the block that should be ignored,\n+            // such as the bit marked (1) in this example:\n+            //\n+            //       (1)\n+            //       -|------\n+            //   (c) 01000000|00000000|00000001\n+            //          ^~~~~~~~~~~~~~~~~~^\n+            //        start              end\n+            if let Some(i) =\n+                search_block(init_mask.blocks[start_block], start_block, start_bit, is_init)\n+            {\n+                // If the range is less than a block, we may find a matching bit after `end`.\n+                //\n+                // For example, we shouldn't successfully find bit (2), because it's after `end`:\n+                //\n+                //             (2)\n+                //       -------|\n+                //   (d) 00000001|00000000|00000001\n+                //        ^~~~~^\n+                //      start end\n+                //\n+                // An alternative would be to mask off end bits in the same way as we do for start bits,\n+                // but performing this check afterwards is faster and simpler to implement.\n+                if i < end {\n+                    return Some(i);\n+                } else {\n+                    return None;\n+                }\n+            }\n+\n+            // Handle remaining blocks.\n+            //\n+            // We can skip over an entire block at once if it's all 0s (resp. 1s).\n+            // The block marked (3) in this example is the first block that will be handled by this loop,\n+            // and it will be skipped for that reason:\n+            //\n+            //                   (3)\n+            //                --------\n+            //   (e) 01000000|00000000|00000001\n+            //          ^~~~~~~~~~~~~~~~~~^\n+            //        start              end\n+            if start_block < end_block_inclusive {\n+                // This loop is written in a specific way for performance.\n+                // Notably: `..end_block_inclusive + 1` is used for an inclusive range instead of `..=end_block_inclusive`,\n+                // and `.zip(start_block + 1..)` is used to track the index instead of `.enumerate().skip().take()`,\n+                // because both alternatives result in significantly worse codegen.\n+                // `end_block_inclusive + 1` is guaranteed not to wrap, because `end_block_inclusive <= end / BLOCK_SIZE`,\n+                // and `BLOCK_SIZE` (the number of bits per block) will always be at least 8 (1 byte).\n+                for (&bits, block) in init_mask.blocks[start_block + 1..end_block_inclusive + 1]\n+                    .iter()\n+                    .zip(start_block + 1..)\n+                {\n+                    if let Some(i) = search_block(bits, block, 0, is_init) {\n+                        // If this is the last block, we may find a matching bit after `end`.\n+                        //\n+                        // For example, we shouldn't successfully find bit (4), because it's after `end`:\n+                        //\n+                        //                               (4)\n+                        //                         -------|\n+                        //   (f) 00000001|00000000|00000001\n+                        //          ^~~~~~~~~~~~~~~~~~^\n+                        //        start              end\n+                        //\n+                        // As above with example (d), we could handle the end block separately and mask off end bits,\n+                        // but unconditionally searching an entire block at once and performing this check afterwards\n+                        // is faster and much simpler to implement.\n+                        if i < end {\n+                            return Some(i);\n+                        } else {\n+                            return None;\n+                        }\n+                    }\n+                }\n+            }\n+\n+            None\n+        }\n+\n+        #[cfg_attr(not(debug_assertions), allow(dead_code))]\n+        fn find_bit_slow(\n+            init_mask: &InitMask,\n+            start: Size,\n+            end: Size,\n+            is_init: bool,\n+        ) -> Option<Size> {\n+            (start..end).find(|&i| init_mask.get(i) == is_init)\n+        }\n+\n+        let result = find_bit_fast(self, start, end, is_init);\n+\n+        debug_assert_eq!(\n+            result,\n+            find_bit_slow(self, start, end, is_init),\n+            \"optimized implementation of find_bit is wrong for start={:?} end={:?} is_init={} init_mask={:#?}\",\n+            start,\n+            end,\n+            is_init,\n+            self\n+        );\n+\n+        result\n+    }\n+}\n+\n+/// A contiguous chunk of initialized or uninitialized memory.\n+pub enum InitChunk {\n+    Init(Range<Size>),\n+    Uninit(Range<Size>),\n }\n \n-#[inline]\n-fn bit_index(bits: Size) -> (usize, usize) {\n-    let bits = bits.bytes();\n-    let a = bits / InitMask::BLOCK_SIZE;\n-    let b = bits % InitMask::BLOCK_SIZE;\n-    (usize::try_from(a).unwrap(), usize::try_from(b).unwrap())\n+impl InitChunk {\n+    #[inline]\n+    pub fn is_init(&self) -> bool {\n+        match self {\n+            Self::Init(_) => true,\n+            Self::Uninit(_) => false,\n+        }\n+    }\n+\n+    #[inline]\n+    pub fn range(&self) -> Range<Size> {\n+        match self {\n+            Self::Init(r) => r.clone(),\n+            Self::Uninit(r) => r.clone(),\n+        }\n+    }\n+}\n+\n+impl InitMask {\n+    /// Checks whether the range `start..end` (end-exclusive) is entirely initialized.\n+    ///\n+    /// Returns `Ok(())` if it's initialized. Otherwise returns a range of byte\n+    /// indexes for the first contiguous span of the uninitialized access.\n+    #[inline]\n+    pub fn is_range_initialized(&self, start: Size, end: Size) -> Result<(), Range<Size>> {\n+        if end > self.len {\n+            return Err(self.len..end);\n+        }\n+\n+        let uninit_start = self.find_bit(start, end, false);\n+\n+        match uninit_start {\n+            Some(uninit_start) => {\n+                let uninit_end = self.find_bit(uninit_start, end, true).unwrap_or(end);\n+                Err(uninit_start..uninit_end)\n+            }\n+            None => Ok(()),\n+        }\n+    }\n+\n+    /// Returns an iterator, yielding a range of byte indexes for each contiguous region\n+    /// of initialized or uninitialized bytes inside the range `start..end` (end-exclusive).\n+    ///\n+    /// The iterator guarantees the following:\n+    /// - Chunks are nonempty.\n+    /// - Chunks are adjacent (each range's start is equal to the previous range's end).\n+    /// - Chunks span exactly `start..end` (the first starts at `start`, the last ends at `end`).\n+    /// - Chunks alternate between [`InitChunk::Init`] and [`InitChunk::Uninit`].\n+    #[inline]\n+    pub fn range_as_init_chunks(&self, start: Size, end: Size) -> InitChunkIter<'_> {\n+        assert!(end <= self.len);\n+\n+        let is_init = if start < end {\n+            self.get(start)\n+        } else {\n+            // `start..end` is empty: there are no chunks, so use some arbitrary value\n+            false\n+        };\n+\n+        InitChunkIter { init_mask: self, is_init, start, end }\n+    }\n+}\n+\n+/// Yields [`InitChunk`]s. See [`InitMask::range_as_init_chunks`].\n+pub struct InitChunkIter<'a> {\n+    init_mask: &'a InitMask,\n+    /// Whether the next chunk we will return is initialized.\n+    /// If there are no more chunks, contains some arbitrary value.\n+    is_init: bool,\n+    /// The current byte index into `init_mask`.\n+    start: Size,\n+    /// The end byte index into `init_mask`.\n+    end: Size,\n+}\n+\n+impl<'a> Iterator for InitChunkIter<'a> {\n+    type Item = InitChunk;\n+\n+    #[inline]\n+    fn next(&mut self) -> Option<Self::Item> {\n+        if self.start >= self.end {\n+            return None;\n+        }\n+\n+        let end_of_chunk =\n+            self.init_mask.find_bit(self.start, self.end, !self.is_init).unwrap_or(self.end);\n+        let range = self.start..end_of_chunk;\n+\n+        let ret =\n+            Some(if self.is_init { InitChunk::Init(range) } else { InitChunk::Uninit(range) });\n+\n+        self.is_init = !self.is_init;\n+        self.start = end_of_chunk;\n+\n+        ret\n+    }\n+}\n+\n+/// Uninitialized bytes.\n+impl<Tag: Copy, Extra> Allocation<Tag, Extra> {\n+    /// Checks whether the given range  is entirely initialized.\n+    ///\n+    /// Returns `Ok(())` if it's initialized. Otherwise returns the range of byte\n+    /// indexes of the first contiguous uninitialized access.\n+    fn is_init(&self, range: AllocRange) -> Result<(), Range<Size>> {\n+        self.init_mask.is_range_initialized(range.start, range.end()) // `Size` addition\n+    }\n+\n+    /// Checks that a range of bytes is initialized. If not, returns the `InvalidUninitBytes`\n+    /// error which will report the first range of bytes which is uninitialized.\n+    fn check_init(&self, range: AllocRange) -> AllocResult {\n+        self.is_init(range).or_else(|idx_range| {\n+            Err(AllocError::InvalidUninitBytes(Some(UninitBytesAccess {\n+                access_offset: range.start,\n+                access_size: range.size,\n+                uninit_offset: idx_range.start,\n+                uninit_size: idx_range.end - idx_range.start, // `Size` subtraction\n+            })))\n+        })\n+    }\n+\n+    pub fn mark_init(&mut self, range: AllocRange, is_init: bool) {\n+        if range.size.bytes() == 0 {\n+            return;\n+        }\n+        assert!(self.mutability == Mutability::Mut);\n+        self.init_mask.set_range(range.start, range.end(), is_init);\n+    }\n+}\n+\n+/// Run-length encoding of the uninit mask.\n+/// Used to copy parts of a mask multiple times to another allocation.\n+pub struct InitMaskCompressed {\n+    /// Whether the first range is initialized.\n+    initial: bool,\n+    /// The lengths of ranges that are run-length encoded.\n+    /// The initialization state of the ranges alternate starting with `initial`.\n+    ranges: smallvec::SmallVec<[u64; 1]>,\n+}\n+\n+impl InitMaskCompressed {\n+    pub fn no_bytes_init(&self) -> bool {\n+        // The `ranges` are run-length encoded and of alternating initialization state.\n+        // So if `ranges.len() > 1` then the second block is an initialized range.\n+        !self.initial && self.ranges.len() == 1\n+    }\n+}\n+\n+/// Transferring the initialization mask to other allocations.\n+impl<Tag, Extra> Allocation<Tag, Extra> {\n+    /// Creates a run-length encoding of the initialization mask; panics if range is empty.\n+    ///\n+    /// This is essentially a more space-efficient version of\n+    /// `InitMask::range_as_init_chunks(...).collect::<Vec<_>>()`.\n+    pub fn compress_uninit_range(&self, range: AllocRange) -> InitMaskCompressed {\n+        // Since we are copying `size` bytes from `src` to `dest + i * size` (`for i in 0..repeat`),\n+        // a naive initialization mask copying algorithm would repeatedly have to read the initialization mask from\n+        // the source and write it to the destination. Even if we optimized the memory accesses,\n+        // we'd be doing all of this `repeat` times.\n+        // Therefore we precompute a compressed version of the initialization mask of the source value and\n+        // then write it back `repeat` times without computing any more information from the source.\n+\n+        // A precomputed cache for ranges of initialized / uninitialized bits\n+        // 0000010010001110 will become\n+        // `[5, 1, 2, 1, 3, 3, 1]`,\n+        // where each element toggles the state.\n+\n+        let mut ranges = smallvec::SmallVec::<[u64; 1]>::new();\n+\n+        let mut chunks = self.init_mask.range_as_init_chunks(range.start, range.end()).peekable();\n+\n+        let initial = chunks.peek().expect(\"range should be nonempty\").is_init();\n+\n+        // Here we rely on `range_as_init_chunks` to yield alternating init/uninit chunks.\n+        for chunk in chunks {\n+            let len = chunk.range().end.bytes() - chunk.range().start.bytes();\n+            ranges.push(len);\n+        }\n+\n+        InitMaskCompressed { ranges, initial }\n+    }\n+\n+    /// Applies multiple instances of the run-length encoding to the initialization mask.\n+    pub fn mark_compressed_init_range(\n+        &mut self,\n+        defined: &InitMaskCompressed,\n+        range: AllocRange,\n+        repeat: u64,\n+    ) {\n+        // An optimization where we can just overwrite an entire range of initialization\n+        // bits if they are going to be uniformly `1` or `0`.\n+        if defined.ranges.len() <= 1 {\n+            self.init_mask.set_range_inbounds(\n+                range.start,\n+                range.start + range.size * repeat, // `Size` operations\n+                defined.initial,\n+            );\n+            return;\n+        }\n+\n+        for mut j in 0..repeat {\n+            j *= range.size.bytes();\n+            j += range.start.bytes();\n+            let mut cur = defined.initial;\n+            for range in &defined.ranges {\n+                let old_j = j;\n+                j += range;\n+                self.init_mask.set_range_inbounds(\n+                    Size::from_bytes(old_j),\n+                    Size::from_bytes(j),\n+                    cur,\n+                );\n+                cur = !cur;\n+            }\n+        }\n+    }\n }"}, {"sha": "4628c24292f02a168454b25f54a5ce3ccce306ea", "filename": "compiler/rustc_middle/src/mir/interpret/mod.rs", "status": "modified", "additions": 3, "deletions": 1, "changes": 4, "blob_url": "https://github.com/rust-lang/rust/blob/20997f6ad81721542e9ef97bb2f58190903a34d8/compiler%2Frustc_middle%2Fsrc%2Fmir%2Finterpret%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/20997f6ad81721542e9ef97bb2f58190903a34d8/compiler%2Frustc_middle%2Fsrc%2Fmir%2Finterpret%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_middle%2Fsrc%2Fmir%2Finterpret%2Fmod.rs?ref=20997f6ad81721542e9ef97bb2f58190903a34d8", "patch": "@@ -125,7 +125,9 @@ pub use self::error::{\n \n pub use self::value::{get_slice_bytes, ConstAlloc, ConstValue, Scalar, ScalarMaybeUninit};\n \n-pub use self::allocation::{alloc_range, AllocRange, Allocation, InitMask, Relocations};\n+pub use self::allocation::{\n+    alloc_range, AllocRange, Allocation, InitChunk, InitChunkIter, InitMask, Relocations,\n+};\n \n pub use self::pointer::{Pointer, PointerArithmetic, Provenance};\n "}, {"sha": "9a1be40558ccb3ca03796f97f6381065301b455e", "filename": "compiler/rustc_session/src/options.rs", "status": "modified", "additions": 3, "deletions": 0, "changes": 3, "blob_url": "https://github.com/rust-lang/rust/blob/20997f6ad81721542e9ef97bb2f58190903a34d8/compiler%2Frustc_session%2Fsrc%2Foptions.rs", "raw_url": "https://github.com/rust-lang/rust/raw/20997f6ad81721542e9ef97bb2f58190903a34d8/compiler%2Frustc_session%2Fsrc%2Foptions.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_session%2Fsrc%2Foptions.rs?ref=20997f6ad81721542e9ef97bb2f58190903a34d8", "patch": "@@ -1186,6 +1186,9 @@ options! {\n         \"support compiling tests with panic=abort (default: no)\"),\n     parse_only: bool = (false, parse_bool, [UNTRACKED],\n         \"parse only; do not compile, assemble, or link (default: no)\"),\n+    partially_uninit_const_threshold: Option<usize> = (None, parse_opt_number, [TRACKED],\n+        \"allow generating const initializers with mixed init/uninit bytes, \\\n+        and set the maximum total size of a const allocation for which this is allowed (default: never)\"),\n     perf_stats: bool = (false, parse_bool, [UNTRACKED],\n         \"print some performance-related statistics (default: no)\"),\n     plt: Option<bool> = (None, parse_opt_bool, [TRACKED],"}, {"sha": "88f1b1c320c1e3137722f2cc318ddd5a6b280df4", "filename": "compiler/rustc_target/src/abi/mod.rs", "status": "modified", "additions": 38, "deletions": 0, "changes": 38, "blob_url": "https://github.com/rust-lang/rust/blob/20997f6ad81721542e9ef97bb2f58190903a34d8/compiler%2Frustc_target%2Fsrc%2Fabi%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/20997f6ad81721542e9ef97bb2f58190903a34d8/compiler%2Frustc_target%2Fsrc%2Fabi%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_target%2Fsrc%2Fabi%2Fmod.rs?ref=20997f6ad81721542e9ef97bb2f58190903a34d8", "patch": "@@ -5,6 +5,7 @@ use crate::spec::Target;\n \n use std::convert::{TryFrom, TryInto};\n use std::fmt;\n+use std::iter::Step;\n use std::num::NonZeroUsize;\n use std::ops::{Add, AddAssign, Deref, Mul, Range, RangeInclusive, Sub};\n use std::str::FromStr;\n@@ -440,6 +441,43 @@ impl AddAssign for Size {\n     }\n }\n \n+impl Step for Size {\n+    #[inline]\n+    fn steps_between(start: &Self, end: &Self) -> Option<usize> {\n+        u64::steps_between(&start.bytes(), &end.bytes())\n+    }\n+\n+    #[inline]\n+    fn forward_checked(start: Self, count: usize) -> Option<Self> {\n+        u64::forward_checked(start.bytes(), count).map(Self::from_bytes)\n+    }\n+\n+    #[inline]\n+    fn forward(start: Self, count: usize) -> Self {\n+        Self::from_bytes(u64::forward(start.bytes(), count))\n+    }\n+\n+    #[inline]\n+    unsafe fn forward_unchecked(start: Self, count: usize) -> Self {\n+        Self::from_bytes(u64::forward_unchecked(start.bytes(), count))\n+    }\n+\n+    #[inline]\n+    fn backward_checked(start: Self, count: usize) -> Option<Self> {\n+        u64::backward_checked(start.bytes(), count).map(Self::from_bytes)\n+    }\n+\n+    #[inline]\n+    fn backward(start: Self, count: usize) -> Self {\n+        Self::from_bytes(u64::backward(start.bytes(), count))\n+    }\n+\n+    #[inline]\n+    unsafe fn backward_unchecked(start: Self, count: usize) -> Self {\n+        Self::from_bytes(u64::backward_unchecked(start.bytes(), count))\n+    }\n+}\n+\n /// Alignment of a type in bytes (always a power of two).\n #[derive(Copy, Clone, PartialEq, Eq, PartialOrd, Ord, Hash, Debug, Encodable, Decodable)]\n #[derive(HashStable_Generic)]"}, {"sha": "e75c52555b90347418bdb8255f7b7fe1d3b2bf72", "filename": "compiler/rustc_target/src/lib.rs", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/20997f6ad81721542e9ef97bb2f58190903a34d8/compiler%2Frustc_target%2Fsrc%2Flib.rs", "raw_url": "https://github.com/rust-lang/rust/raw/20997f6ad81721542e9ef97bb2f58190903a34d8/compiler%2Frustc_target%2Fsrc%2Flib.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_target%2Fsrc%2Flib.rs?ref=20997f6ad81721542e9ef97bb2f58190903a34d8", "patch": "@@ -14,6 +14,8 @@\n #![feature(associated_type_bounds)]\n #![feature(exhaustive_patterns)]\n #![feature(min_specialization)]\n+#![feature(step_trait)]\n+#![feature(unchecked_math)]\n \n use std::path::{Path, PathBuf};\n "}, {"sha": "f7420e4126ed0999d40a263f300bdb93e0d940cb", "filename": "src/test/codegen/uninit-consts-allow-partially-uninit.rs", "status": "added", "additions": 35, "deletions": 0, "changes": 35, "blob_url": "https://github.com/rust-lang/rust/blob/20997f6ad81721542e9ef97bb2f58190903a34d8/src%2Ftest%2Fcodegen%2Funinit-consts-allow-partially-uninit.rs", "raw_url": "https://github.com/rust-lang/rust/raw/20997f6ad81721542e9ef97bb2f58190903a34d8/src%2Ftest%2Fcodegen%2Funinit-consts-allow-partially-uninit.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ftest%2Fcodegen%2Funinit-consts-allow-partially-uninit.rs?ref=20997f6ad81721542e9ef97bb2f58190903a34d8", "patch": "@@ -0,0 +1,35 @@\n+// compile-flags: -C no-prepopulate-passes -Z partially_uninit_const_threshold=1024\n+\n+// Like uninit-consts.rs, but tests that we correctly generate partially-uninit consts\n+// when the (disabled by default) partially_uninit_const_threshold flag is used.\n+\n+#![crate_type = \"lib\"]\n+\n+use std::mem::MaybeUninit;\n+\n+pub struct PartiallyUninit {\n+    x: u32,\n+    y: MaybeUninit<[u8; 10]>\n+}\n+\n+// This should be partially undef.\n+// CHECK: [[PARTIALLY_UNINIT:@[0-9]+]] = private unnamed_addr constant <{ [4 x i8], [12 x i8] }> <{ [4 x i8] c\"\\EF\\BE\\AD\\DE\", [12 x i8] undef }>, align 4\n+\n+// This shouldn't contain undef, since it's larger than the 1024 byte limit.\n+// CHECK: [[UNINIT_PADDING_HUGE:@[0-9]+]] = private unnamed_addr constant <{ [32768 x i8] }> <{ [32768 x i8] c\"{{.+}}\" }>, align 4\n+\n+// CHECK-LABEL: @partially_uninit\n+#[no_mangle]\n+pub const fn partially_uninit() -> PartiallyUninit {\n+    const X: PartiallyUninit = PartiallyUninit { x: 0xdeadbeef, y: MaybeUninit::uninit() };\n+    // CHECK: call void @llvm.memcpy.p0i8.p0i8.i{{(32|64)}}(i8* align 4 %1, i8* align 4 getelementptr inbounds (<{ [4 x i8], [12 x i8] }>, <{ [4 x i8], [12 x i8] }>* [[PARTIALLY_UNINIT]], i32 0, i32 0, i32 0), i{{(32|64)}} 16, i1 false)\n+    X\n+}\n+\n+// CHECK-LABEL: @uninit_padding_huge\n+#[no_mangle]\n+pub const fn uninit_padding_huge() -> [(u32, u8); 4096] {\n+    const X: [(u32, u8); 4096] = [(123, 45); 4096];\n+    // CHECK: call void @llvm.memcpy.p0i8.p0i8.i{{(32|64)}}(i8* align 4 %1, i8* align 4 getelementptr inbounds (<{ [32768 x i8] }>, <{ [32768 x i8] }>* [[UNINIT_PADDING_HUGE]], i32 0, i32 0, i32 0), i{{(32|64)}} 32768, i1 false)\n+    X\n+}"}, {"sha": "c4c21e03f16767e2aa93538cbb70a4f161f958b9", "filename": "src/test/codegen/uninit-consts.rs", "status": "added", "additions": 40, "deletions": 0, "changes": 40, "blob_url": "https://github.com/rust-lang/rust/blob/20997f6ad81721542e9ef97bb2f58190903a34d8/src%2Ftest%2Fcodegen%2Funinit-consts.rs", "raw_url": "https://github.com/rust-lang/rust/raw/20997f6ad81721542e9ef97bb2f58190903a34d8/src%2Ftest%2Fcodegen%2Funinit-consts.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ftest%2Fcodegen%2Funinit-consts.rs?ref=20997f6ad81721542e9ef97bb2f58190903a34d8", "patch": "@@ -0,0 +1,40 @@\n+// compile-flags: -C no-prepopulate-passes\n+\n+// Check that we use undef (and not zero) for uninitialized bytes in constants.\n+\n+#![crate_type = \"lib\"]\n+\n+use std::mem::MaybeUninit;\n+\n+pub struct PartiallyUninit {\n+    x: u32,\n+    y: MaybeUninit<[u8; 10]>\n+}\n+\n+// CHECK: [[FULLY_UNINIT:@[0-9]+]] = private unnamed_addr constant <{ [10 x i8] }> undef\n+// CHECK: [[PARTIALLY_UNINIT:@[0-9]+]] = private unnamed_addr constant <{ [16 x i8] }> <{ [16 x i8] c\"\\EF\\BE\\AD\\DE\\00\\00\\00\\00\\00\\00\\00\\00\\00\\00\\00\\00\" }>, align 4\n+// CHECK: [[FULLY_UNINIT_HUGE:@[0-9]+]] = private unnamed_addr constant <{ [16384 x i8] }> undef\n+\n+// CHECK-LABEL: @fully_uninit\n+#[no_mangle]\n+pub const fn fully_uninit() -> MaybeUninit<[u8; 10]> {\n+    const M: MaybeUninit<[u8; 10]> = MaybeUninit::uninit();\n+    // CHECK: call void @llvm.memcpy.p0i8.p0i8.i{{(32|64)}}(i8* align 1 %1, i8* align 1 getelementptr inbounds (<{ [10 x i8] }>, <{ [10 x i8] }>* [[FULLY_UNINIT]], i32 0, i32 0, i32 0), i{{(32|64)}} 10, i1 false)\n+    M\n+}\n+\n+// CHECK-LABEL: @partially_uninit\n+#[no_mangle]\n+pub const fn partially_uninit() -> PartiallyUninit {\n+    const X: PartiallyUninit = PartiallyUninit { x: 0xdeadbeef, y: MaybeUninit::uninit() };\n+    // CHECK: call void @llvm.memcpy.p0i8.p0i8.i{{(32|64)}}(i8* align 4 %1, i8* align 4 getelementptr inbounds (<{ [16 x i8] }>, <{ [16 x i8] }>* [[PARTIALLY_UNINIT]], i32 0, i32 0, i32 0), i{{(32|64)}} 16, i1 false)\n+    X\n+}\n+\n+// CHECK-LABEL: @fully_uninit_huge\n+#[no_mangle]\n+pub const fn fully_uninit_huge() -> MaybeUninit<[u32; 4096]> {\n+    const F: MaybeUninit<[u32; 4096]> = MaybeUninit::uninit();\n+    // CHECK: call void @llvm.memcpy.p0i8.p0i8.i{{(32|64)}}(i8* align 4 %1, i8* align 4 getelementptr inbounds (<{ [16384 x i8] }>, <{ [16384 x i8] }>* [[FULLY_UNINIT_HUGE]], i32 0, i32 0, i32 0), i{{(32|64)}} 16384, i1 false)\n+    F\n+}"}]}