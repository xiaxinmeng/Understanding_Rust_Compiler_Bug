{"sha": "a6a97a9bb19c03a2c7bf8e6d7f2da962ee9a2efa", "node_id": "MDY6Q29tbWl0NzI0NzEyOmE2YTk3YTliYjE5YzAzYTJjN2JmOGU2ZDdmMmRhOTYyZWU5YTJlZmE=", "commit": {"author": {"name": "Niko Matsakis", "email": "niko@alum.mit.edu", "date": "2016-08-05T13:48:22Z"}, "committer": {"name": "Niko Matsakis", "email": "niko@alum.mit.edu", "date": "2016-08-05T13:48:22Z"}, "message": "rustfmt save.rs", "tree": {"sha": "c5db0b5db504a0531489a209c3aa11f4f40e48c4", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/c5db0b5db504a0531489a209c3aa11f4f40e48c4"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/a6a97a9bb19c03a2c7bf8e6d7f2da962ee9a2efa", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/a6a97a9bb19c03a2c7bf8e6d7f2da962ee9a2efa", "html_url": "https://github.com/rust-lang/rust/commit/a6a97a9bb19c03a2c7bf8e6d7f2da962ee9a2efa", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/a6a97a9bb19c03a2c7bf8e6d7f2da962ee9a2efa/comments", "author": {"login": "nikomatsakis", "id": 155238, "node_id": "MDQ6VXNlcjE1NTIzOA==", "avatar_url": "https://avatars.githubusercontent.com/u/155238?v=4", "gravatar_id": "", "url": "https://api.github.com/users/nikomatsakis", "html_url": "https://github.com/nikomatsakis", "followers_url": "https://api.github.com/users/nikomatsakis/followers", "following_url": "https://api.github.com/users/nikomatsakis/following{/other_user}", "gists_url": "https://api.github.com/users/nikomatsakis/gists{/gist_id}", "starred_url": "https://api.github.com/users/nikomatsakis/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/nikomatsakis/subscriptions", "organizations_url": "https://api.github.com/users/nikomatsakis/orgs", "repos_url": "https://api.github.com/users/nikomatsakis/repos", "events_url": "https://api.github.com/users/nikomatsakis/events{/privacy}", "received_events_url": "https://api.github.com/users/nikomatsakis/received_events", "type": "User", "site_admin": false}, "committer": {"login": "nikomatsakis", "id": 155238, "node_id": "MDQ6VXNlcjE1NTIzOA==", "avatar_url": "https://avatars.githubusercontent.com/u/155238?v=4", "gravatar_id": "", "url": "https://api.github.com/users/nikomatsakis", "html_url": "https://github.com/nikomatsakis", "followers_url": "https://api.github.com/users/nikomatsakis/followers", "following_url": "https://api.github.com/users/nikomatsakis/following{/other_user}", "gists_url": "https://api.github.com/users/nikomatsakis/gists{/gist_id}", "starred_url": "https://api.github.com/users/nikomatsakis/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/nikomatsakis/subscriptions", "organizations_url": "https://api.github.com/users/nikomatsakis/orgs", "repos_url": "https://api.github.com/users/nikomatsakis/repos", "events_url": "https://api.github.com/users/nikomatsakis/events{/privacy}", "received_events_url": "https://api.github.com/users/nikomatsakis/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "bfbfe639b1c00afe5fd939d3a0b46751ab69cc55", "url": "https://api.github.com/repos/rust-lang/rust/commits/bfbfe639b1c00afe5fd939d3a0b46751ab69cc55", "html_url": "https://github.com/rust-lang/rust/commit/bfbfe639b1c00afe5fd939d3a0b46751ab69cc55"}], "stats": {"total": 201, "additions": 95, "deletions": 106}, "files": [{"sha": "e71204dd8399cf1afb55e440710684c89e563028", "filename": "src/librustc_incremental/persist/save.rs", "status": "modified", "additions": 95, "deletions": 106, "changes": 201, "blob_url": "https://github.com/rust-lang/rust/blob/a6a97a9bb19c03a2c7bf8e6d7f2da962ee9a2efa/src%2Flibrustc_incremental%2Fpersist%2Fsave.rs", "raw_url": "https://github.com/rust-lang/rust/raw/a6a97a9bb19c03a2c7bf8e6d7f2da962ee9a2efa/src%2Flibrustc_incremental%2Fpersist%2Fsave.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_incremental%2Fpersist%2Fsave.rs?ref=a6a97a9bb19c03a2c7bf8e6d7f2da962ee9a2efa", "patch": "@@ -14,7 +14,7 @@ use rustc::hir::def_id::DefId;\n use rustc::middle::cstore::LOCAL_CRATE;\n use rustc::session::Session;\n use rustc::ty::TyCtxt;\n-use rustc_serialize::{Encodable as RustcEncodable};\n+use rustc_serialize::Encodable as RustcEncodable;\n use std::hash::{Hash, Hasher, SipHasher};\n use std::io::{self, Cursor, Write};\n use std::fs::{self, File};\n@@ -35,8 +35,12 @@ pub fn save_dep_graph<'a, 'tcx>(tcx: TyCtxt<'a, 'tcx, 'tcx>) {\n     let mut hcx = HashContext::new(tcx);\n     let mut builder = DefIdDirectoryBuilder::new(tcx);\n     let query = tcx.dep_graph.query();\n-    save_in(sess, dep_graph_path(tcx), |e| encode_dep_graph(&mut hcx, &mut builder, &query, e));\n-    save_in(sess, metadata_hash_path(tcx, LOCAL_CRATE), |e| encode_metadata_hashes(&mut hcx, &mut builder, &query, e));\n+    save_in(sess,\n+            dep_graph_path(tcx),\n+            |e| encode_dep_graph(&mut hcx, &mut builder, &query, e));\n+    save_in(sess,\n+            metadata_hash_path(tcx, LOCAL_CRATE),\n+            |e| encode_metadata_hashes(&mut hcx, &mut builder, &query, e));\n }\n \n pub fn save_work_products(sess: &Session, local_crate_name: &str) {\n@@ -46,26 +50,24 @@ pub fn save_work_products(sess: &Session, local_crate_name: &str) {\n     save_in(sess, path, |e| encode_work_products(sess, e));\n }\n \n-fn save_in<F>(sess: &Session,\n-              opt_path_buf: Option<PathBuf>,\n-              encode: F)\n+fn save_in<F>(sess: &Session, opt_path_buf: Option<PathBuf>, encode: F)\n     where F: FnOnce(&mut Encoder) -> io::Result<()>\n {\n     let path_buf = match opt_path_buf {\n         Some(p) => p,\n-        None => return\n+        None => return,\n     };\n \n     // FIXME(#32754) lock file?\n \n     // delete the old dep-graph, if any\n     if path_buf.exists() {\n         match fs::remove_file(&path_buf) {\n-            Ok(()) => { }\n+            Ok(()) => {}\n             Err(err) => {\n-                sess.err(\n-                    &format!(\"unable to delete old dep-graph at `{}`: {}\",\n-                             path_buf.display(), err));\n+                sess.err(&format!(\"unable to delete old dep-graph at `{}`: {}\",\n+                                  path_buf.display(),\n+                                  err));\n                 return;\n             }\n         }\n@@ -74,26 +76,23 @@ fn save_in<F>(sess: &Session,\n     // generate the data in a memory buffer\n     let mut wr = Cursor::new(Vec::new());\n     match encode(&mut Encoder::new(&mut wr)) {\n-        Ok(()) => { }\n+        Ok(()) => {}\n         Err(err) => {\n-            sess.err(\n-                &format!(\"could not encode dep-graph to `{}`: {}\",\n-                         path_buf.display(), err));\n+            sess.err(&format!(\"could not encode dep-graph to `{}`: {}\",\n+                              path_buf.display(),\n+                              err));\n             return;\n         }\n     }\n \n     // write the data out\n     let data = wr.into_inner();\n-    match\n-        File::create(&path_buf)\n-        .and_then(|mut file| file.write_all(&data))\n-    {\n-        Ok(_) => { }\n+    match File::create(&path_buf).and_then(|mut file| file.write_all(&data)) {\n+        Ok(_) => {}\n         Err(err) => {\n-            sess.err(\n-                &format!(\"failed to write dep-graph to `{}`: {}\",\n-                         path_buf.display(), err));\n+            sess.err(&format!(\"failed to write dep-graph to `{}`: {}\",\n+                              path_buf.display(),\n+                              err));\n             return;\n         }\n     }\n@@ -103,32 +102,33 @@ pub fn encode_dep_graph<'a, 'tcx>(hcx: &mut HashContext<'a, 'tcx>,\n                                   builder: &mut DefIdDirectoryBuilder,\n                                   query: &DepGraphQuery<DefId>,\n                                   encoder: &mut Encoder)\n-                                  -> io::Result<()>\n-{\n+                                  -> io::Result<()> {\n     let (nodes, edges) = (query.nodes(), query.edges());\n \n     // Create hashes for inputs.\n-    let hashes =\n-        nodes.iter()\n-             .filter_map(|dep_node| {\n-                 hcx.hash(dep_node)\n-                    .map(|(_, hash)| {\n-                        let node = builder.map(dep_node);\n-                        SerializedHash { node: node, hash: hash }\n-                    })\n-             })\n-             .collect();\n+    let hashes = nodes.iter()\n+        .filter_map(|dep_node| {\n+            hcx.hash(dep_node)\n+                .map(|(_, hash)| {\n+                    let node = builder.map(dep_node);\n+                    SerializedHash {\n+                        node: node,\n+                        hash: hash,\n+                    }\n+                })\n+        })\n+        .collect();\n \n     // Create the serialized dep-graph.\n     let graph = SerializedDepGraph {\n         nodes: nodes.iter().map(|node| builder.map(node)).collect(),\n         edges: edges.iter()\n-                    .map(|&(ref source_node, ref target_node)| {\n-                        let source = builder.map(source_node);\n-                        let target = builder.map(target_node);\n-                        (source, target)\n-                    })\n-                    .collect(),\n+            .map(|&(ref source_node, ref target_node)| {\n+                let source = builder.map(source_node);\n+                let target = builder.map(target_node);\n+                (source, target)\n+            })\n+            .collect(),\n         hashes: hashes,\n     };\n \n@@ -145,72 +145,65 @@ pub fn encode_metadata_hashes<'a, 'tcx>(hcx: &mut HashContext<'a, 'tcx>,\n                                         builder: &mut DefIdDirectoryBuilder,\n                                         query: &DepGraphQuery<DefId>,\n                                         encoder: &mut Encoder)\n-                                        -> io::Result<()>\n-{\n+                                        -> io::Result<()> {\n     let tcx = hcx.tcx;\n \n     let serialized_hashes = {\n         // Identify the `MetaData(X)` nodes where `X` is local. These are\n         // the metadata items we export. Downstream crates will want to\n         // see a hash that tells them whether we might have changed the\n         // metadata for a given item since they last compiled.\n-        let meta_data_def_ids =\n-            query.nodes()\n-                 .into_iter()\n-                 .filter_map(|dep_node| match *dep_node {\n-                     DepNode::MetaData(def_id) if def_id.is_local() => Some(def_id),\n-                     _ => None,\n-                 });\n+        let meta_data_def_ids = query.nodes()\n+            .into_iter()\n+            .filter_map(|dep_node| match *dep_node {\n+                DepNode::MetaData(def_id) if def_id.is_local() => Some(def_id),\n+                _ => None,\n+            });\n \n         // To create the hash for each item `X`, we don't hash the raw\n         // bytes of the metadata (though in principle we\n         // could). Instead, we walk the predecessors of `MetaData(X)`\n         // from the dep-graph. This corresponds to all the inputs that\n         // were read to construct the metadata. To create the hash for\n         // the metadata, we hash (the hash of) all of those inputs.\n-        let hashes =\n-            meta_data_def_ids\n-            .map(|def_id| {\n-                assert!(def_id.is_local());\n-                let dep_node = DepNode::MetaData(def_id);\n-                let mut state = SipHasher::new();\n-                debug!(\"save: computing metadata hash for {:?}\", dep_node);\n-\n-                let predecessors = query.transitive_predecessors(&dep_node);\n-                let mut hashes: Vec<_> =\n-                    predecessors.iter()\n-                                .filter_map(|node| hcx.hash(&node))\n-                                .map(|(def_id, hash)| {\n-                                    let index = builder.add(def_id);\n-                                    let path = builder.lookup_def_path(index);\n-                                    (path.to_string(tcx), hash) // (*)\n-                                })\n-                                .collect();\n-\n-                // (*) creating a `String` from each def-path is a bit inefficient,\n-                // but it's the easiest way to get a deterministic ord/hash.\n-\n-                hashes.sort();\n-                state.write_usize(hashes.len());\n-                for (path, hash) in hashes {\n-                    debug!(\"save: predecessor {:?} has hash {}\", path, hash);\n-                    path.hash(&mut state);\n-                    state.write_u64(hash.to_le());\n-                }\n-\n-                let hash = state.finish();\n-                debug!(\"save: metadata hash for {:?} is {}\", dep_node, hash);\n-\n-                SerializedMetadataHash {\n-                    def_index: def_id.index,\n-                    hash: hash,\n-                }\n-            });\n+        let hashes = meta_data_def_ids.map(|def_id| {\n+            assert!(def_id.is_local());\n+            let dep_node = DepNode::MetaData(def_id);\n+            let mut state = SipHasher::new();\n+            debug!(\"save: computing metadata hash for {:?}\", dep_node);\n+\n+            let predecessors = query.transitive_predecessors(&dep_node);\n+            let mut hashes: Vec<_> = predecessors.iter()\n+                .filter_map(|node| hcx.hash(&node))\n+                .map(|(def_id, hash)| {\n+                    let index = builder.add(def_id);\n+                    let path = builder.lookup_def_path(index);\n+                    (path.to_string(tcx), hash) // (*)\n+                })\n+                .collect();\n+\n+            // (*) creating a `String` from each def-path is a bit inefficient,\n+            // but it's the easiest way to get a deterministic ord/hash.\n+\n+            hashes.sort();\n+            state.write_usize(hashes.len());\n+            for (path, hash) in hashes {\n+                debug!(\"save: predecessor {:?} has hash {}\", path, hash);\n+                path.hash(&mut state);\n+                state.write_u64(hash.to_le());\n+            }\n+\n+            let hash = state.finish();\n+            debug!(\"save: metadata hash for {:?} is {}\", dep_node, hash);\n+\n+            SerializedMetadataHash {\n+                def_index: def_id.index,\n+                hash: hash,\n+            }\n+        });\n \n         // Collect these up into a vector.\n-        SerializedMetadataHashes {\n-            hashes: hashes.collect()\n-        }\n+        SerializedMetadataHashes { hashes: hashes.collect() }\n     };\n \n     // Encode everything.\n@@ -219,21 +212,17 @@ pub fn encode_metadata_hashes<'a, 'tcx>(hcx: &mut HashContext<'a, 'tcx>,\n     Ok(())\n }\n \n-pub fn encode_work_products(sess: &Session,\n-                            encoder: &mut Encoder)\n-                            -> io::Result<()>\n-{\n-    let work_products: Vec<_> =\n-        sess.dep_graph.work_products()\n-                     .iter()\n-                     .map(|(id, work_product)| {\n-                         SerializedWorkProduct {\n-                             id: id.clone(),\n-                             work_product: work_product.clone(),\n-                         }\n-                     })\n-                     .collect();\n+pub fn encode_work_products(sess: &Session, encoder: &mut Encoder) -> io::Result<()> {\n+    let work_products: Vec<_> = sess.dep_graph\n+        .work_products()\n+        .iter()\n+        .map(|(id, work_product)| {\n+            SerializedWorkProduct {\n+                id: id.clone(),\n+                work_product: work_product.clone(),\n+            }\n+        })\n+        .collect();\n \n     work_products.encode(encoder)\n }\n-"}]}