{"sha": "bd36df84a57f2719e99c691e7ed23d0264836d41", "node_id": "MDY6Q29tbWl0NzI0NzEyOmJkMzZkZjg0YTU3ZjI3MTllOTljNjkxZTdlZDIzZDAyNjQ4MzZkNDE=", "commit": {"author": {"name": "Michael Woerister", "email": "michaelwoerister@posteo.net", "date": "2017-07-28T12:28:08Z"}, "committer": {"name": "Michael Woerister", "email": "michaelwoerister@posteo.net", "date": "2017-07-31T13:15:44Z"}, "message": "async-llvm(24): Improve scheduling and documentation.", "tree": {"sha": "1c88837257cd12da14072dbb23e0044d8f9ac3ad", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/1c88837257cd12da14072dbb23e0044d8f9ac3ad"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/bd36df84a57f2719e99c691e7ed23d0264836d41", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/bd36df84a57f2719e99c691e7ed23d0264836d41", "html_url": "https://github.com/rust-lang/rust/commit/bd36df84a57f2719e99c691e7ed23d0264836d41", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/bd36df84a57f2719e99c691e7ed23d0264836d41/comments", "author": {"login": "michaelwoerister", "id": 1825894, "node_id": "MDQ6VXNlcjE4MjU4OTQ=", "avatar_url": "https://avatars.githubusercontent.com/u/1825894?v=4", "gravatar_id": "", "url": "https://api.github.com/users/michaelwoerister", "html_url": "https://github.com/michaelwoerister", "followers_url": "https://api.github.com/users/michaelwoerister/followers", "following_url": "https://api.github.com/users/michaelwoerister/following{/other_user}", "gists_url": "https://api.github.com/users/michaelwoerister/gists{/gist_id}", "starred_url": "https://api.github.com/users/michaelwoerister/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/michaelwoerister/subscriptions", "organizations_url": "https://api.github.com/users/michaelwoerister/orgs", "repos_url": "https://api.github.com/users/michaelwoerister/repos", "events_url": "https://api.github.com/users/michaelwoerister/events{/privacy}", "received_events_url": "https://api.github.com/users/michaelwoerister/received_events", "type": "User", "site_admin": false}, "committer": {"login": "michaelwoerister", "id": 1825894, "node_id": "MDQ6VXNlcjE4MjU4OTQ=", "avatar_url": "https://avatars.githubusercontent.com/u/1825894?v=4", "gravatar_id": "", "url": "https://api.github.com/users/michaelwoerister", "html_url": "https://github.com/michaelwoerister", "followers_url": "https://api.github.com/users/michaelwoerister/followers", "following_url": "https://api.github.com/users/michaelwoerister/following{/other_user}", "gists_url": "https://api.github.com/users/michaelwoerister/gists{/gist_id}", "starred_url": "https://api.github.com/users/michaelwoerister/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/michaelwoerister/subscriptions", "organizations_url": "https://api.github.com/users/michaelwoerister/orgs", "repos_url": "https://api.github.com/users/michaelwoerister/repos", "events_url": "https://api.github.com/users/michaelwoerister/events{/privacy}", "received_events_url": "https://api.github.com/users/michaelwoerister/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "f5acc392e0b28295ccaff6135e12fab219b0b006", "url": "https://api.github.com/repos/rust-lang/rust/commits/f5acc392e0b28295ccaff6135e12fab219b0b006", "html_url": "https://github.com/rust-lang/rust/commit/f5acc392e0b28295ccaff6135e12fab219b0b006"}], "stats": {"total": 306, "additions": 215, "deletions": 91}, "files": [{"sha": "31742023d46f03e171fe8ccfe5f89938ccc95225", "filename": "src/Cargo.lock", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/bd36df84a57f2719e99c691e7ed23d0264836d41/src%2FCargo.lock", "raw_url": "https://github.com/rust-lang/rust/raw/bd36df84a57f2719e99c691e7ed23d0264836d41/src%2FCargo.lock", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2FCargo.lock?ref=bd36df84a57f2719e99c691e7ed23d0264836d41", "patch": "@@ -1517,11 +1517,11 @@ dependencies = [\n name = \"rustc_trans\"\n version = \"0.0.0\"\n dependencies = [\n- \"crossbeam 0.2.10 (registry+https://github.com/rust-lang/crates.io-index)\",\n  \"flate2 0.2.19 (registry+https://github.com/rust-lang/crates.io-index)\",\n  \"gcc 0.3.51 (registry+https://github.com/rust-lang/crates.io-index)\",\n  \"jobserver 0.1.6 (registry+https://github.com/rust-lang/crates.io-index)\",\n  \"log 0.3.8 (registry+https://github.com/rust-lang/crates.io-index)\",\n+ \"num_cpus 1.6.2 (registry+https://github.com/rust-lang/crates.io-index)\",\n  \"owning_ref 0.3.3 (registry+https://github.com/rust-lang/crates.io-index)\",\n  \"rustc 0.0.0\",\n  \"rustc-demangle 0.1.4 (registry+https://github.com/rust-lang/crates.io-index)\","}, {"sha": "ed9321cc3f3a1197dd97f45703cdfd53ebd50080", "filename": "src/librustc_trans/Cargo.toml", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/bd36df84a57f2719e99c691e7ed23d0264836d41/src%2Flibrustc_trans%2FCargo.toml", "raw_url": "https://github.com/rust-lang/rust/raw/bd36df84a57f2719e99c691e7ed23d0264836d41/src%2Flibrustc_trans%2FCargo.toml", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_trans%2FCargo.toml?ref=bd36df84a57f2719e99c691e7ed23d0264836d41", "patch": "@@ -10,7 +10,7 @@ crate-type = [\"dylib\"]\n test = false\n \n [dependencies]\n-crossbeam = \"0.2\"\n+num_cpus = \"1.0\"\n flate2 = \"0.2\"\n jobserver = \"0.1.5\"\n log = \"0.3\""}, {"sha": "4e68fa8ce40c36c59027855111889b262df28f48", "filename": "src/librustc_trans/back/write.rs", "status": "modified", "additions": 191, "deletions": 88, "changes": 279, "blob_url": "https://github.com/rust-lang/rust/blob/bd36df84a57f2719e99c691e7ed23d0264836d41/src%2Flibrustc_trans%2Fback%2Fwrite.rs", "raw_url": "https://github.com/rust-lang/rust/raw/bd36df84a57f2719e99c691e7ed23d0264836d41/src%2Flibrustc_trans%2Fback%2Fwrite.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_trans%2Fback%2Fwrite.rs?ref=bd36df84a57f2719e99c691e7ed23d0264836d41", "patch": "@@ -1077,7 +1077,8 @@ enum Message {\n     },\n     TranslationDone {\n         llvm_work_item: WorkItem,\n-        is_last: bool\n+        cost: u64,\n+        is_last: bool,\n     },\n     TranslateItem,\n }\n@@ -1089,7 +1090,7 @@ struct Diagnostic {\n }\n \n #[derive(PartialEq, Clone, Copy, Debug)]\n-enum TransWorkerState {\n+enum MainThreadWorkerState {\n     Idle,\n     Translating,\n     LLVMing,\n@@ -1148,16 +1149,110 @@ fn start_executing_work(sess: &Session,\n     // It's here that we manage parallelism, schedule work, and work with\n     // messages coming from clients.\n     //\n-    // Our channel `rx` created above is a channel of messages coming from our\n-    // various worker threads. This includes the jobserver helper thread above\n-    // as well as the work we'll spawn off here. Each turn of this loop starts\n-    // off by trying to spawn as much work as possible. After we've done that we\n-    // then wait for an event and dispatch accordingly once the event is\n-    // received. We're only done once all our work items have been drained and\n-    // nothing is running, at which point we return back up the stack.\n+    // There are a few environmental pre-conditions that shape how the system\n+    // is set up:\n     //\n-    // ## Parallelism management\n+    // - Error reporting only can happen on the main thread because that's the\n+    //   only place where we have access to the compiler `Session`.\n+    // - LLVM work can be done on any thread.\n+    // - Translation can only happen on the main thread.\n+    // - Each thread doing substantial work most be in possession of a `Token`\n+    //   from the `Jobserver`.\n+    // - The compiler process always holds one `Token`. Any additional `Tokens`\n+    //   have to be requested from the `Jobserver`.\n     //\n+    // Error Reporting\n+    // ===============\n+    // The error reporting restriction is handled separately from the rest: We\n+    // set up a `SharedEmitter` the holds an open channel to the main thread.\n+    // When an error occurs on any thread, the shared emitter will send the\n+    // error message to the receiver main thread (`SharedEmitterMain`). The\n+    // main thread will periodically query this error message queue and emit\n+    // any error messages it has received. It might even abort compilation if\n+    // has received a fatal error. In this case we rely on all other threads\n+    // being torn down automatically with the main thread.\n+    // Since the main thread will often be busy doing translation work, error\n+    // reporting will be somewhat delayed, since the message queue can only be\n+    // checked in between to work packages.\n+    //\n+    // Work Processing Infrastructure\n+    // ==============================\n+    // The work processing infrastructure knows three major actors:\n+    //\n+    // - the coordinator thread,\n+    // - the main thread, and\n+    // - LLVM worker threads\n+    //\n+    // The coordinator thread is running a message loop. It instructs the main\n+    // thread about what work to do when, and it will spawn off LLVM worker\n+    // threads as open LLVM WorkItems become available.\n+    //\n+    // The job of the main thread is to translate CGUs into LLVM work package\n+    // (since the main thread is the only thread that can do this). The main\n+    // thread will block until it receives a message from the coordinator, upon\n+    // which it will translate one CGU, send it to the coordinator and block\n+    // again. This way the coordinator can control what the main thread is\n+    // doing.\n+    //\n+    // The coordinator keeps a queue of LLVM WorkItems, and when a `Token` is\n+    // available, it will spawn off a new LLVM worker thread and let it process\n+    // that a WorkItem. When a LLVM worker thread is done with its WorkItem,\n+    // it will just shut down, which also frees all resources associated with\n+    // the given LLVM module, and sends a message to the coordinator that the\n+    // has been completed.\n+    //\n+    // Work Scheduling\n+    // ===============\n+    // The scheduler's goal is to minimize the time it takes to complete all\n+    // work there is, however, we also want to keep memory consumption low\n+    // if possible. These two goals are at odds with each other: If memory\n+    // consumption were not an issue, we could just let the main thread produce\n+    // LLVM WorkItems at full speed, assuring maximal utilization of\n+    // Tokens/LLVM worker threads. However, since translation usual is faster\n+    // than LLVM processing, the queue of LLVM WorkItems would fill up and each\n+    // WorkItem potentially holds on to a substantial amount of memory.\n+    //\n+    // So the actual goal is to always produce just enough LLVM WorkItems as\n+    // not to starve our LLVM worker threads. That means, once we have enough\n+    // WorkItems in our queue, we can block the main thread, so it does not\n+    // produce more until we need them.\n+    //\n+    // Doing LLVM Work on the Main Thread\n+    // ----------------------------------\n+    // Since the main thread owns the compiler processes implicit `Token`, it is\n+    // wasteful to keep it blocked without doing any work. Therefore, what we do\n+    // in this case is: We spawn off an additional LLVM worker thread that helps\n+    // reduce the queue. The work it is doing corresponds to the implicit\n+    // `Token`. The coordinator will mark the main thread as being busy with\n+    // LLVM work. (The actual work happens on another OS thread but we just care\n+    // about `Tokens`, not actual threads).\n+    //\n+    // When any LLVM worker thread finishes while the main thread is marked as\n+    // \"busy with LLVM work\", we can do a little switcheroo: We give the Token\n+    // of the just finished thread to the LLVM worker thread that is working on\n+    // behalf of the main thread's implicit Token, thus freeing up the main\n+    // thread again. The coordinator can then again decide what the main thread\n+    // should do. This allows the coordinator to make decisions at more points\n+    // in time.\n+    //\n+    // Striking a Balance between Throughput and Memory Consumption\n+    // ------------------------------------------------------------\n+    // Since our two goals, (1) use as many Tokens as possible and (2) keep\n+    // memory consumption as low as possible, are in conflict with each other,\n+    // we have to find a trade off between them. Right now, the goal is to keep\n+    // all workers busy, which means that no worker should find the queue empty\n+    // when it is ready to start.\n+    // How do we do achieve this? Good question :) We actually never know how\n+    // many `Tokens` are potentially available so it's hard to say how much to\n+    // fill up the queue before switching the main thread to LLVM work. Also we\n+    // currently don't have a means to estimate how long a running LLVM worker\n+    // will still be busy with it's current WorkItem. However, we know the\n+    // maximal count of available Tokens that makes sense (=the number of CPU\n+    // cores), so we can take a conservative guess. The heuristic we use here\n+    // is implemented in the `queue_full_enough()` function.\n+    //\n+    // Some Background on Jobservers\n+    // -----------------------------\n     // It's worth also touching on the management of parallelism here. We don't\n     // want to just spawn a thread per work item because while that's optimal\n     // parallelism it may overload a system with too many threads or violate our\n@@ -1170,36 +1265,8 @@ fn start_executing_work(sess: &Session,\n     // and whenever we're done with that work we release the semaphore. In this\n     // manner we can ensure that the maximum number of parallel workers is\n     // capped at any one point in time.\n-    //\n-    // The jobserver protocol is a little unique, however. We, as a running\n-    // process, already have an ephemeral token assigned to us. We're not going\n-    // to be doing any productive work in this thread though so we're going to\n-    // give this token to a worker thread (there's no actual token to give, this\n-    // is just conceptually). As a result you'll see a few `+1` and `-1`\n-    // instances below, and it's about working with this ephemeral token.\n-    //\n-    // To acquire tokens we have our `helper` thread above which is just in a\n-    // loop acquiring tokens and sending them to us. We then store all tokens\n-    // locally in a `tokens` vector once they're acquired. Currently we don't\n-    // literally send a token to a worker thread to assist with management of\n-    // our \"ephemeral token\".\n-    //\n-    // As a result, our \"spawn as much work as possible\" basically means that we\n-    // fill up the `running` counter up to the limit of the `tokens` list.\n-    // Whenever we get a new token this'll mean a new unit of work is spawned,\n-    // and then whenever a unit of work finishes we relinquish a token, if we\n-    // had one, to maybe get re-acquired later.\n-    //\n-    // Note that there's a race which may mean that we acquire more tokens than\n-    // we originally anticipated. For example let's say we have 2 units of work.\n-    // First we request one token from the helper thread and then we\n-    // immediately spawn one unit of work with our ephemeral token after. We may\n-    // then finish the first piece of work before the token is acquired, but we\n-    // can continue to spawn the second piece of work with our ephemeral token.\n-    // Before that work finishes, however, we may acquire a token. In that case\n-    // we actually wastefully acquired the token, so we relinquish it back to\n-    // the jobserver.\n-    thread::spawn(move || {\n+    return thread::spawn(move || {\n+        let max_workers = ::num_cpus::get();\n         let mut worker_id_counter = 0;\n         let mut free_worker_ids = Vec::new();\n         let mut get_worker_id = |free_worker_ids: &mut Vec<usize>| {\n@@ -1212,74 +1279,75 @@ fn start_executing_work(sess: &Session,\n             }\n         };\n \n+        // This is where we collect codegen units that have gone all the way\n+        // through translation and LLVM.\n         let mut compiled_modules = vec![];\n         let mut compiled_metadata_module = None;\n         let mut compiled_allocator_module = None;\n \n+        // This flag tracks whether all items have gone through translations\n         let mut translation_done = false;\n+\n+        // This is the queue of LLVM work items that still need processing.\n         let mut work_items = Vec::new();\n+\n+        // This are the Jobserver Tokens we currently hold. Does not include\n+        // the implicit Token the compiler process owns no matter what.\n         let mut tokens = Vec::new();\n \n-        let mut trans_worker_state = TransWorkerState::Idle;\n+        let mut main_thread_worker_state = MainThreadWorkerState::Idle;\n         let mut running = 0;\n \n+        // Run the message loop while there's still anything that needs message\n+        // processing:\n         while !translation_done ||\n               work_items.len() > 0 ||\n               running > 0 ||\n-              trans_worker_state != TransWorkerState::Idle {\n+              main_thread_worker_state != MainThreadWorkerState::Idle {\n \n+            // While there are still CGUs to be translated, the coordinator has\n+            // to decide how to utilize the compiler processes implicit Token:\n+            // For translating more CGU or for running them through LLVM.\n             if !translation_done {\n-                if trans_worker_state == TransWorkerState::Idle {\n-                    // Translation is not done yet, so there are two things the\n-                    // translation worker could do:\n-                    //\n-                    // (1) Translate another CGU\n-                    // (2) Run an already translated CGU through LLVM\n-                    //\n-                    // Option (2) makes sense if there's already enough work for\n-                    // all the other workers. In that case it's better to run\n-                    // a CGU through LLVM, so its resources can be freed.\n-                    //\n-                    // However, it's not trivial to determines what \"enough work\n-                    // for all the other workers\" means because:\n-                    //\n-                    // (1) We don't know how long the currently working workers\n-                    //     will need to finish their work package, and\n-                    // (2) we don't know how many idle workers would be available\n-                    //     because that is dynamically decided by the jobserver.\n-                    //\n-                    // TODO: Come up with a useful heuristic.\n-                    if work_items.len() <= 4 {\n+                if main_thread_worker_state == MainThreadWorkerState::Idle {\n+                    if !queue_full_enough(work_items.len(), running, max_workers) {\n+                        // The queue is not full enough, translate more items:\n                         trans_worker_send.send(Message::TranslateItem).unwrap();\n-                        trans_worker_state = TransWorkerState::Translating;\n+                        main_thread_worker_state = MainThreadWorkerState::Translating;\n                     } else {\n-                        let item = work_items.pop().unwrap();\n+                        // The queue is full enough to not let the worker\n+                        // threads starve. Use the implicit Token to do some\n+                        // LLVM work too.\n+                        let (item, _) = work_items.pop().unwrap();\n                         let cgcx = CodegenContext {\n-                            worker: TRANS_WORKER_ID,\n+                            worker: get_worker_id(&mut free_worker_ids),\n                             .. cgcx.clone()\n                         };\n-                        trans_worker_state = TransWorkerState::LLVMing;\n+                        main_thread_worker_state = MainThreadWorkerState::LLVMing;\n                         spawn_work(cgcx, item);\n                     }\n                 }\n             } else {\n-                match trans_worker_state {\n-                    TransWorkerState::Idle => {\n-                        if let Some(item) = work_items.pop() {\n+                // In this branch, we know that everything has been translated,\n+                // so it's just a matter of determining whether the implicit\n+                // Token is free to use for LLVM work.\n+                match main_thread_worker_state {\n+                    MainThreadWorkerState::Idle => {\n+                        if let Some((item, _)) = work_items.pop() {\n                             let cgcx = CodegenContext {\n-                                worker: TRANS_WORKER_ID,\n+                                worker: get_worker_id(&mut free_worker_ids),\n                                 .. cgcx.clone()\n                             };\n \n-                            trans_worker_state = TransWorkerState::LLVMing;\n+                            main_thread_worker_state = MainThreadWorkerState::LLVMing;\n                             spawn_work(cgcx, item);\n                         }\n                     }\n-                    TransWorkerState::Translating => {\n+                    MainThreadWorkerState::Translating => {\n                         bug!(\"trans worker should not be translating after \\\n                               translation was already completed\")\n                     }\n-                    TransWorkerState::LLVMing => {\n+                    MainThreadWorkerState::LLVMing => {\n                         // Already making good use of that token\n                     }\n                 }\n@@ -1288,11 +1356,10 @@ fn start_executing_work(sess: &Session,\n             // Spin up what work we can, only doing this while we've got available\n             // parallelism slots and work left to spawn.\n             while work_items.len() > 0 && running < tokens.len() {\n-                let item = work_items.pop().unwrap();\n-                let worker_id = get_worker_id(&mut free_worker_ids);\n+                let (item, _) = work_items.pop().unwrap();\n \n                 let cgcx = CodegenContext {\n-                    worker: worker_id,\n+                    worker: get_worker_id(&mut free_worker_ids),\n                     .. cgcx.clone()\n                 };\n \n@@ -1310,15 +1377,37 @@ fn start_executing_work(sess: &Session,\n                 Message::Token(token) => {\n                     if let Ok(token) = token {\n                         tokens.push(token);\n+\n+                        if main_thread_worker_state == MainThreadWorkerState::LLVMing {\n+                            // If the main thread token is used for LLVM work\n+                            // at the moment, we turn that thread into a regular\n+                            // LLVM worker thread, so the main thread is free\n+                            // to react to translation demand.\n+                            main_thread_worker_state = MainThreadWorkerState::Idle;\n+                            running += 1;\n+                        }\n                     } else {\n                         shared_emitter.fatal(\"failed to acquire jobserver token\");\n                         // Exit the coordinator thread\n                         panic!()\n                     }\n                 }\n \n-                Message::TranslationDone { llvm_work_item, is_last } => {\n-                    work_items.insert(0, llvm_work_item);\n+                Message::TranslationDone { llvm_work_item, cost, is_last } => {\n+                    // We keep the queue sorted by estimated processing cost,\n+                    // so that more expensive items are processed earlier. This\n+                    // is good for throughput as it gives the main thread more\n+                    // time to fill up the queue and it avoids scheduling\n+                    // expensive items to the end.\n+                    // Note, however, that this is not ideal for memory\n+                    // consumption, as LLVM module sizes are not evenly\n+                    // distributed.\n+                    let insertion_index =\n+                        work_items.binary_search_by_key(&cost, |&(_, cost)| cost);\n+                    let insertion_index = match insertion_index {\n+                        Ok(idx) | Err(idx) => idx\n+                    };\n+                    work_items.insert(insertion_index, (llvm_work_item, cost));\n \n                     if is_last {\n                         // If this is the last, don't request a token because\n@@ -1329,8 +1418,9 @@ fn start_executing_work(sess: &Session,\n                         helper.request_token();\n                     }\n \n-                    assert_eq!(trans_worker_state, TransWorkerState::Translating);\n-                    trans_worker_state = TransWorkerState::Idle;\n+                    assert_eq!(main_thread_worker_state,\n+                               MainThreadWorkerState::Translating);\n+                    main_thread_worker_state = MainThreadWorkerState::Idle;\n                 }\n \n                 // If a thread exits successfully then we drop a token associated\n@@ -1342,15 +1432,14 @@ fn start_executing_work(sess: &Session,\n                 // Note that if the thread failed that means it panicked, so we\n                 // abort immediately.\n                 Message::Done { result: Ok(compiled_module), worker_id } => {\n-                    if worker_id == TRANS_WORKER_ID {\n-                        assert_eq!(trans_worker_state, TransWorkerState::LLVMing);\n-                        trans_worker_state = TransWorkerState::Idle;\n+                    if main_thread_worker_state == MainThreadWorkerState::LLVMing {\n+                        main_thread_worker_state = MainThreadWorkerState::Idle;\n                     } else {\n-                        drop(tokens.pop());\n                         running -= 1;\n-                        free_worker_ids.push(worker_id);\n                     }\n \n+                    free_worker_ids.push(worker_id);\n+\n                     match compiled_module.kind {\n                         ModuleKind::Regular => {\n                             compiled_modules.push(compiled_module);\n@@ -1381,7 +1470,16 @@ fn start_executing_work(sess: &Session,\n             metadata_module: compiled_metadata_module.unwrap(),\n             allocator_module: compiled_allocator_module,\n         }\n-    })\n+    });\n+\n+    // A heuristic that determines if we have enough LLVM WorkItems in the\n+    // queue so that the main thread can do LLVM work instead of translation\n+    fn queue_full_enough(items_in_queue: usize,\n+                         workers_running: usize,\n+                         max_workers: usize) -> bool {\n+        // Tune me, plz.\n+        items_in_queue >= max_workers.saturating_sub(workers_running / 2)\n+    }\n }\n \n pub const TRANS_WORKER_ID: usize = ::std::usize::MAX;\n@@ -1729,6 +1827,7 @@ impl OngoingCrateTranslation {\n     pub fn submit_translated_module_to_llvm(&self,\n                                             sess: &Session,\n                                             mtrans: ModuleTranslation,\n+                                            cost: u64,\n                                             is_last: bool) {\n         let module_config = match mtrans.kind {\n             ModuleKind::Regular => self.regular_module_config.clone(sess),\n@@ -1742,6 +1841,7 @@ impl OngoingCrateTranslation {\n \n         drop(self.coordinator_send.send(Message::TranslationDone {\n             llvm_work_item,\n+            cost,\n             is_last\n         }));\n     }\n@@ -1752,7 +1852,10 @@ impl OngoingCrateTranslation {\n                                                 is_last: bool) {\n         self.wait_for_signal_to_translate_item();\n         self.check_for_errors(sess);\n-        self.submit_translated_module_to_llvm(sess, mtrans, is_last);\n+\n+        // These are generally cheap and won't through off scheduling.\n+        let cost = 0;\n+        self.submit_translated_module_to_llvm(sess, mtrans, cost, is_last);\n     }\n \n     pub fn check_for_errors(&self, sess: &Session) {"}, {"sha": "e4a7634552843ec92f96bc9625ca830a81899be5", "filename": "src/librustc_trans/base.rs", "status": "modified", "additions": 21, "deletions": 0, "changes": 21, "blob_url": "https://github.com/rust-lang/rust/blob/bd36df84a57f2719e99c691e7ed23d0264836d41/src%2Flibrustc_trans%2Fbase.rs", "raw_url": "https://github.com/rust-lang/rust/raw/bd36df84a57f2719e99c691e7ed23d0264836d41/src%2Flibrustc_trans%2Fbase.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_trans%2Fbase.rs?ref=bd36df84a57f2719e99c691e7ed23d0264836d41", "patch": "@@ -80,6 +80,7 @@ use libc::c_uint;\n use std::ffi::{CStr, CString};\n use std::str;\n use std::sync::Arc;\n+use std::time::Instant;\n use std::i32;\n use syntax_pos::Span;\n use syntax::attr;\n@@ -1082,10 +1083,22 @@ pub fn trans_crate<'a, 'tcx>(tcx: TyCtxt<'a, 'tcx, 'tcx>,\n     let mut all_stats = Stats::default();\n     let mut module_dispositions = tcx.sess.opts.incremental.as_ref().map(|_| Vec::new());\n \n+    // We sort the codegen units by size. This way we can schedule work for LLVM\n+    // a bit more efficiently. Note that \"size\" is defined rather crudely at the\n+    // moment as it is just the number of TransItems in the CGU, not taking into\n+    // account the size of each TransItem.\n+    let codegen_units = {\n+        let mut codegen_units = codegen_units;\n+        codegen_units.sort_by_key(|cgu| -(cgu.items().len() as isize));\n+        codegen_units\n+    };\n+\n     for (cgu_index, cgu) in codegen_units.into_iter().enumerate() {\n         ongoing_translation.wait_for_signal_to_translate_item();\n         ongoing_translation.check_for_errors(tcx.sess);\n \n+        let start_time = Instant::now();\n+\n         let module = {\n             let _timing_guard = time_graph\n                 .as_ref()\n@@ -1108,10 +1121,18 @@ pub fn trans_crate<'a, 'tcx>(tcx: TyCtxt<'a, 'tcx, 'tcx>,\n             module\n         };\n \n+        let time_to_translate = Instant::now().duration_since(start_time);\n+\n+        // We assume that the cost to run LLVM on a CGU is proportional to\n+        // the time we needed for translating it.\n+        let cost = time_to_translate.as_secs() * 1_000_000_000 +\n+                   time_to_translate.subsec_nanos() as u64;\n+\n         let is_last_cgu = (cgu_index + 1) == codegen_unit_count;\n \n         ongoing_translation.submit_translated_module_to_llvm(tcx.sess,\n                                                              module,\n+                                                             cost,\n                                                              is_last_cgu);\n         ongoing_translation.check_for_errors(tcx.sess);\n     }"}, {"sha": "5a4a5b95cf90a8abbdffca2c6c35b865945ccedb", "filename": "src/librustc_trans/lib.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/bd36df84a57f2719e99c691e7ed23d0264836d41/src%2Flibrustc_trans%2Flib.rs", "raw_url": "https://github.com/rust-lang/rust/raw/bd36df84a57f2719e99c691e7ed23d0264836d41/src%2Flibrustc_trans%2Flib.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_trans%2Flib.rs?ref=bd36df84a57f2719e99c691e7ed23d0264836d41", "patch": "@@ -39,7 +39,6 @@ use syntax_pos::symbol::Symbol;\n use std::sync::Arc;\n \n extern crate flate2;\n-extern crate crossbeam;\n extern crate libc;\n extern crate owning_ref;\n #[macro_use] extern crate rustc;\n@@ -55,6 +54,7 @@ extern crate rustc_const_math;\n extern crate rustc_bitflags;\n extern crate rustc_demangle;\n extern crate jobserver;\n+extern crate num_cpus;\n \n #[macro_use] extern crate log;\n #[macro_use] extern crate syntax;"}]}