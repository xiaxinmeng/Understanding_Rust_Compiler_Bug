{"sha": "cd5c8235c5448a7234548c772468c8d2e8f150d9", "node_id": "MDY6Q29tbWl0NzI0NzEyOmNkNWM4MjM1YzU0NDhhNzIzNDU0OGM3NzI0NjhjOGQyZThmMTUwZDk=", "commit": {"author": {"name": "Steve Klabnik", "email": "steve@steveklabnik.com", "date": "2014-11-26T02:17:11Z"}, "committer": {"name": "Alex Crichton", "email": "alex@alexcrichton.com", "date": "2014-11-27T00:50:14Z"}, "message": "/*! -> //!\n\nSister pull request of https://github.com/rust-lang/rust/pull/19288, but\nfor the other style of block doc comment.", "tree": {"sha": "0eee3d02c3bae381cf2a18296241ddd9ad04fac6", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/0eee3d02c3bae381cf2a18296241ddd9ad04fac6"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/cd5c8235c5448a7234548c772468c8d2e8f150d9", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/cd5c8235c5448a7234548c772468c8d2e8f150d9", "html_url": "https://github.com/rust-lang/rust/commit/cd5c8235c5448a7234548c772468c8d2e8f150d9", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/cd5c8235c5448a7234548c772468c8d2e8f150d9/comments", "author": {"login": "steveklabnik", "id": 27786, "node_id": "MDQ6VXNlcjI3Nzg2", "avatar_url": "https://avatars.githubusercontent.com/u/27786?v=4", "gravatar_id": "", "url": "https://api.github.com/users/steveklabnik", "html_url": "https://github.com/steveklabnik", "followers_url": "https://api.github.com/users/steveklabnik/followers", "following_url": "https://api.github.com/users/steveklabnik/following{/other_user}", "gists_url": "https://api.github.com/users/steveklabnik/gists{/gist_id}", "starred_url": "https://api.github.com/users/steveklabnik/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/steveklabnik/subscriptions", "organizations_url": "https://api.github.com/users/steveklabnik/orgs", "repos_url": "https://api.github.com/users/steveklabnik/repos", "events_url": "https://api.github.com/users/steveklabnik/events{/privacy}", "received_events_url": "https://api.github.com/users/steveklabnik/received_events", "type": "User", "site_admin": false}, "committer": {"login": "alexcrichton", "id": 64996, "node_id": "MDQ6VXNlcjY0OTk2", "avatar_url": "https://avatars.githubusercontent.com/u/64996?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alexcrichton", "html_url": "https://github.com/alexcrichton", "followers_url": "https://api.github.com/users/alexcrichton/followers", "following_url": "https://api.github.com/users/alexcrichton/following{/other_user}", "gists_url": "https://api.github.com/users/alexcrichton/gists{/gist_id}", "starred_url": "https://api.github.com/users/alexcrichton/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alexcrichton/subscriptions", "organizations_url": "https://api.github.com/users/alexcrichton/orgs", "repos_url": "https://api.github.com/users/alexcrichton/repos", "events_url": "https://api.github.com/users/alexcrichton/events{/privacy}", "received_events_url": "https://api.github.com/users/alexcrichton/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "fac5a07679cac21a580badc84b755b8df0f975cf", "url": "https://api.github.com/repos/rust-lang/rust/commits/fac5a07679cac21a580badc84b755b8df0f975cf", "html_url": "https://github.com/rust-lang/rust/commit/fac5a07679cac21a580badc84b755b8df0f975cf"}], "stats": {"total": 14940, "additions": 6860, "deletions": 8080}, "files": [{"sha": "1dc2539c592e9d68564aaa26055ff39f4b750437", "filename": "src/libcollections/hash/mod.rs", "status": "modified", "additions": 50, "deletions": 52, "changes": 102, "blob_url": "https://github.com/rust-lang/rust/blob/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibcollections%2Fhash%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibcollections%2Fhash%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibcollections%2Fhash%2Fmod.rs?ref=cd5c8235c5448a7234548c772468c8d2e8f150d9", "patch": "@@ -8,58 +8,56 @@\n // option. This file may not be copied, modified, or distributed\n // except according to those terms.\n \n-/*!\n- * Generic hashing support.\n- *\n- * This module provides a generic way to compute the hash of a value. The\n- * simplest way to make a type hashable is to use `#[deriving(Hash)]`:\n- *\n- * # Example\n- *\n- * ```rust\n- * use std::hash;\n- * use std::hash::Hash;\n- *\n- * #[deriving(Hash)]\n- * struct Person {\n- *     id: uint,\n- *     name: String,\n- *     phone: u64,\n- * }\n- *\n- * let person1 = Person { id: 5, name: \"Janet\".to_string(), phone: 555_666_7777 };\n- * let person2 = Person { id: 5, name: \"Bob\".to_string(), phone: 555_666_7777 };\n- *\n- * assert!(hash::hash(&person1) != hash::hash(&person2));\n- * ```\n- *\n- * If you need more control over how a value is hashed, you need to implement\n- * the trait `Hash`:\n- *\n- * ```rust\n- * use std::hash;\n- * use std::hash::Hash;\n- * use std::hash::sip::SipState;\n- *\n- * struct Person {\n- *     id: uint,\n- *     name: String,\n- *     phone: u64,\n- * }\n- *\n- * impl Hash for Person {\n- *     fn hash(&self, state: &mut SipState) {\n- *         self.id.hash(state);\n- *         self.phone.hash(state);\n- *     }\n- * }\n- *\n- * let person1 = Person { id: 5, name: \"Janet\".to_string(), phone: 555_666_7777 };\n- * let person2 = Person { id: 5, name: \"Bob\".to_string(), phone: 555_666_7777 };\n- *\n- * assert!(hash::hash(&person1) == hash::hash(&person2));\n- * ```\n- */\n+//! Generic hashing support.\n+//!\n+//! This module provides a generic way to compute the hash of a value. The\n+//! simplest way to make a type hashable is to use `#[deriving(Hash)]`:\n+//!\n+//! # Example\n+//!\n+//! ```rust\n+//! use std::hash;\n+//! use std::hash::Hash;\n+//!\n+//! #[deriving(Hash)]\n+//! struct Person {\n+//!     id: uint,\n+//!     name: String,\n+//!     phone: u64,\n+//! }\n+//!\n+//! let person1 = Person { id: 5, name: \"Janet\".to_string(), phone: 555_666_7777 };\n+//! let person2 = Person { id: 5, name: \"Bob\".to_string(), phone: 555_666_7777 };\n+//!\n+//! assert!(hash::hash(&person1) != hash::hash(&person2));\n+//! ```\n+//!\n+//! If you need more control over how a value is hashed, you need to implement\n+//! the trait `Hash`:\n+//!\n+//! ```rust\n+//! use std::hash;\n+//! use std::hash::Hash;\n+//! use std::hash::sip::SipState;\n+//!\n+//! struct Person {\n+//!     id: uint,\n+//!     name: String,\n+//!     phone: u64,\n+//! }\n+//!\n+//! impl Hash for Person {\n+//!     fn hash(&self, state: &mut SipState) {\n+//!         self.id.hash(state);\n+//!         self.phone.hash(state);\n+//!     }\n+//! }\n+//!\n+//! let person1 = Person { id: 5, name: \"Janet\".to_string(), phone: 555_666_7777 };\n+//! let person2 = Person { id: 5, name: \"Bob\".to_string(), phone: 555_666_7777 };\n+//!\n+//! assert!(hash::hash(&person1) == hash::hash(&person2));\n+//! ```\n \n #![allow(unused_must_use)]\n "}, {"sha": "9f928f57e9e400c856c27b81511c8141c8d81557", "filename": "src/libcore/clone.rs", "status": "modified", "additions": 10, "deletions": 12, "changes": 22, "blob_url": "https://github.com/rust-lang/rust/blob/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibcore%2Fclone.rs", "raw_url": "https://github.com/rust-lang/rust/raw/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibcore%2Fclone.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibcore%2Fclone.rs?ref=cd5c8235c5448a7234548c772468c8d2e8f150d9", "patch": "@@ -8,18 +8,16 @@\n // option. This file may not be copied, modified, or distributed\n // except according to those terms.\n \n-/*! The `Clone` trait for types that cannot be 'implicitly copied'\n-\n-In Rust, some simple types are \"implicitly copyable\" and when you\n-assign them or pass them as arguments, the receiver will get a copy,\n-leaving the original value in place. These types do not require\n-allocation to copy and do not have finalizers (i.e. they do not\n-contain owned boxes or implement `Drop`), so the compiler considers\n-them cheap and safe to copy. For other types copies must be made\n-explicitly, by convention implementing the `Clone` trait and calling\n-the `clone` method.\n-\n-*/\n+//! The `Clone` trait for types that cannot be 'implicitly copied'\n+//!\n+//! In Rust, some simple types are \"implicitly copyable\" and when you\n+//! assign them or pass them as arguments, the receiver will get a copy,\n+//! leaving the original value in place. These types do not require\n+//! allocation to copy and do not have finalizers (i.e. they do not\n+//! contain owned boxes or implement `Drop`), so the compiler considers\n+//! them cheap and safe to copy. For other types copies must be made\n+//! explicitly, by convention implementing the `Clone` trait and calling\n+//! the `clone` method.\n \n #![unstable]\n "}, {"sha": "8bfdd934477239adafb2a6c56dfdd6b3eb3d5563", "filename": "src/libcore/finally.rs", "status": "modified", "additions": 19, "deletions": 21, "changes": 40, "blob_url": "https://github.com/rust-lang/rust/blob/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibcore%2Ffinally.rs", "raw_url": "https://github.com/rust-lang/rust/raw/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibcore%2Ffinally.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibcore%2Ffinally.rs?ref=cd5c8235c5448a7234548c772468c8d2e8f150d9", "patch": "@@ -8,27 +8,25 @@\n // option. This file may not be copied, modified, or distributed\n // except according to those terms.\n \n-/*!\n-The Finally trait provides a method, `finally` on\n-stack closures that emulates Java-style try/finally blocks.\n-\n-Using the `finally` method is sometimes convenient, but the type rules\n-prohibit any shared, mutable state between the \"try\" case and the\n-\"finally\" case. For advanced cases, the `try_finally` function can\n-also be used. See that function for more details.\n-\n-# Example\n-\n-```\n-use std::finally::Finally;\n-\n-(|| {\n-    // ...\n-}).finally(|| {\n-    // this code is always run\n-})\n-```\n-*/\n+//! The Finally trait provides a method, `finally` on\n+//! stack closures that emulates Java-style try/finally blocks.\n+//!\n+//! Using the `finally` method is sometimes convenient, but the type rules\n+//! prohibit any shared, mutable state between the \"try\" case and the\n+//! \"finally\" case. For advanced cases, the `try_finally` function can\n+//! also be used. See that function for more details.\n+//!\n+//! # Example\n+//!\n+//! ```\n+//! use std::finally::Finally;\n+//!\n+//! (|| {\n+//!     // ...\n+//! }).finally(|| {\n+//!     // this code is always run\n+//! })\n+//! ```\n \n #![experimental]\n "}, {"sha": "78c74075d4867107131380631481c8d2ac0f0197", "filename": "src/libcore/intrinsics.rs", "status": "modified", "additions": 30, "deletions": 32, "changes": 62, "blob_url": "https://github.com/rust-lang/rust/blob/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibcore%2Fintrinsics.rs", "raw_url": "https://github.com/rust-lang/rust/raw/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibcore%2Fintrinsics.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibcore%2Fintrinsics.rs?ref=cd5c8235c5448a7234548c772468c8d2e8f150d9", "patch": "@@ -8,38 +8,36 @@\n // option. This file may not be copied, modified, or distributed\n // except according to those terms.\n \n-/*! rustc compiler intrinsics.\n-\n-The corresponding definitions are in librustc/middle/trans/foreign.rs.\n-\n-# Volatiles\n-\n-The volatile intrinsics provide operations intended to act on I/O\n-memory, which are guaranteed to not be reordered by the compiler\n-across other volatile intrinsics. See the LLVM documentation on\n-[[volatile]].\n-\n-[volatile]: http://llvm.org/docs/LangRef.html#volatile-memory-accesses\n-\n-# Atomics\n-\n-The atomic intrinsics provide common atomic operations on machine\n-words, with multiple possible memory orderings. They obey the same\n-semantics as C++11. See the LLVM documentation on [[atomics]].\n-\n-[atomics]: http://llvm.org/docs/Atomics.html\n-\n-A quick refresher on memory ordering:\n-\n-* Acquire - a barrier for acquiring a lock. Subsequent reads and writes\n-  take place after the barrier.\n-* Release - a barrier for releasing a lock. Preceding reads and writes\n-  take place before the barrier.\n-* Sequentially consistent - sequentially consistent operations are\n-  guaranteed to happen in order. This is the standard mode for working\n-  with atomic types and is equivalent to Java's `volatile`.\n-\n-*/\n+//! rustc compiler intrinsics.\n+//!\n+//! The corresponding definitions are in librustc/middle/trans/foreign.rs.\n+//!\n+//! # Volatiles\n+//!\n+//! The volatile intrinsics provide operations intended to act on I/O\n+//! memory, which are guaranteed to not be reordered by the compiler\n+//! across other volatile intrinsics. See the LLVM documentation on\n+//! [[volatile]].\n+//!\n+//! [volatile]: http://llvm.org/docs/LangRef.html#volatile-memory-accesses\n+//!\n+//! # Atomics\n+//!\n+//! The atomic intrinsics provide common atomic operations on machine\n+//! words, with multiple possible memory orderings. They obey the same\n+//! semantics as C++11. See the LLVM documentation on [[atomics]].\n+//!\n+//! [atomics]: http://llvm.org/docs/Atomics.html\n+//!\n+//! A quick refresher on memory ordering:\n+//!\n+//! * Acquire - a barrier for acquiring a lock. Subsequent reads and writes\n+//!   take place after the barrier.\n+//! * Release - a barrier for releasing a lock. Preceding reads and writes\n+//!   take place before the barrier.\n+//! * Sequentially consistent - sequentially consistent operations are\n+//!   guaranteed to happen in order. This is the standard mode for working\n+//!   with atomic types and is equivalent to Java's `volatile`.\n \n #![experimental]\n #![allow(missing_docs)]"}, {"sha": "2d488a4b15563196d0f2a5526d3171941e00f68c", "filename": "src/libcore/iter.rs", "status": "modified", "additions": 45, "deletions": 49, "changes": 94, "blob_url": "https://github.com/rust-lang/rust/blob/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibcore%2Fiter.rs", "raw_url": "https://github.com/rust-lang/rust/raw/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibcore%2Fiter.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibcore%2Fiter.rs?ref=cd5c8235c5448a7234548c772468c8d2e8f150d9", "patch": "@@ -8,55 +8,51 @@\n // option. This file may not be copied, modified, or distributed\n // except according to those terms.\n \n-/*!\n-\n-Composable external iterators\n-\n-# The `Iterator` trait\n-\n-This module defines Rust's core iteration trait. The `Iterator` trait has one\n-unimplemented method, `next`. All other methods are derived through default\n-methods to perform operations such as `zip`, `chain`, `enumerate`, and `fold`.\n-\n-The goal of this module is to unify iteration across all containers in Rust.\n-An iterator can be considered as a state machine which is used to track which\n-element will be yielded next.\n-\n-There are various extensions also defined in this module to assist with various\n-types of iteration, such as the `DoubleEndedIterator` for iterating in reverse,\n-the `FromIterator` trait for creating a container from an iterator, and much\n-more.\n-\n-## Rust's `for` loop\n-\n-The special syntax used by rust's `for` loop is based around the `Iterator`\n-trait defined in this module. For loops can be viewed as a syntactical expansion\n-into a `loop`, for example, the `for` loop in this example is essentially\n-translated to the `loop` below.\n-\n-```rust\n-let values = vec![1i, 2, 3];\n-\n-// \"Syntactical sugar\" taking advantage of an iterator\n-for &x in values.iter() {\n-    println!(\"{}\", x);\n-}\n-\n-// Rough translation of the iteration without a `for` iterator.\n-let mut it = values.iter();\n-loop {\n-    match it.next() {\n-        Some(&x) => {\n-            println!(\"{}\", x);\n-        }\n-        None => { break }\n-    }\n-}\n-```\n-\n-This `for` loop syntax can be applied to any iterator over any type.\n-\n-*/\n+//! Composable external iterators\n+//!\n+//! # The `Iterator` trait\n+//!\n+//! This module defines Rust's core iteration trait. The `Iterator` trait has one\n+//! unimplemented method, `next`. All other methods are derived through default\n+//! methods to perform operations such as `zip`, `chain`, `enumerate`, and `fold`.\n+//!\n+//! The goal of this module is to unify iteration across all containers in Rust.\n+//! An iterator can be considered as a state machine which is used to track which\n+//! element will be yielded next.\n+//!\n+//! There are various extensions also defined in this module to assist with various\n+//! types of iteration, such as the `DoubleEndedIterator` for iterating in reverse,\n+//! the `FromIterator` trait for creating a container from an iterator, and much\n+//! more.\n+//!\n+//! ## Rust's `for` loop\n+//!\n+//! The special syntax used by rust's `for` loop is based around the `Iterator`\n+//! trait defined in this module. For loops can be viewed as a syntactical expansion\n+//! into a `loop`, for example, the `for` loop in this example is essentially\n+//! translated to the `loop` below.\n+//!\n+//! ```rust\n+//! let values = vec![1i, 2, 3];\n+//!\n+//! // \"Syntactical sugar\" taking advantage of an iterator\n+//! for &x in values.iter() {\n+//!     println!(\"{}\", x);\n+//! }\n+//!\n+//! // Rough translation of the iteration without a `for` iterator.\n+//! let mut it = values.iter();\n+//! loop {\n+//!     match it.next() {\n+//!         Some(&x) => {\n+//!             println!(\"{}\", x);\n+//!         }\n+//!         None => { break }\n+//!     }\n+//! }\n+//! ```\n+//!\n+//! This `for` loop syntax can be applied to any iterator over any type.\n \n pub use self::MinMaxResult::*;\n "}, {"sha": "0c2cb9d5910056cd385c6ab84d0de9f2493b96aa", "filename": "src/libcore/kinds.rs", "status": "modified", "additions": 8, "deletions": 11, "changes": 19, "blob_url": "https://github.com/rust-lang/rust/blob/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibcore%2Fkinds.rs", "raw_url": "https://github.com/rust-lang/rust/raw/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibcore%2Fkinds.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibcore%2Fkinds.rs?ref=cd5c8235c5448a7234548c772468c8d2e8f150d9", "patch": "@@ -8,17 +8,14 @@\n // option. This file may not be copied, modified, or distributed\n // except according to those terms.\n \n-/*!\n-Primitive traits representing basic 'kinds' of types\n-\n-Rust types can be classified in various useful ways according to\n-intrinsic properties of the type. These classifications, often called\n-'kinds', are represented as traits.\n-\n-They cannot be implemented by user code, but are instead implemented\n-by the compiler automatically for the types to which they apply.\n-\n-*/\n+//! Primitive traits representing basic 'kinds' of types\n+//!\n+//! Rust types can be classified in various useful ways according to\n+//! intrinsic properties of the type. These classifications, often called\n+//! 'kinds', are represented as traits.\n+//!\n+//! They cannot be implemented by user code, but are instead implemented\n+//! by the compiler automatically for the types to which they apply.\n \n /// Types able to be transferred across task boundaries.\n #[lang=\"send\"]"}, {"sha": "519dfd47fd8e33d651907d478887015814ef29fc", "filename": "src/libcore/ops.rs", "status": "modified", "additions": 42, "deletions": 46, "changes": 88, "blob_url": "https://github.com/rust-lang/rust/blob/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibcore%2Fops.rs", "raw_url": "https://github.com/rust-lang/rust/raw/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibcore%2Fops.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibcore%2Fops.rs?ref=cd5c8235c5448a7234548c772468c8d2e8f150d9", "patch": "@@ -8,52 +8,48 @@\n // option. This file may not be copied, modified, or distributed\n // except according to those terms.\n \n-/*!\n- *\n- * Overloadable operators\n- *\n- * Implementing these traits allows you to get an effect similar to\n- * overloading operators.\n- *\n- * The values for the right hand side of an operator are automatically\n- * borrowed, so `a + b` is sugar for `a.add(&b)`.\n- *\n- * All of these traits are imported by the prelude, so they are available in\n- * every Rust program.\n- *\n- * # Example\n- *\n- * This example creates a `Point` struct that implements `Add` and `Sub`, and then\n- * demonstrates adding and subtracting two `Point`s.\n- *\n- * ```rust\n- * #[deriving(Show)]\n- * struct Point {\n- *     x: int,\n- *     y: int\n- * }\n- *\n- * impl Add<Point, Point> for Point {\n- *     fn add(&self, other: &Point) -> Point {\n- *         Point {x: self.x + other.x, y: self.y + other.y}\n- *     }\n- * }\n- *\n- * impl Sub<Point, Point> for Point {\n- *     fn sub(&self, other: &Point) -> Point {\n- *         Point {x: self.x - other.x, y: self.y - other.y}\n- *     }\n- * }\n- * fn main() {\n- *     println!(\"{}\", Point {x: 1, y: 0} + Point {x: 2, y: 3});\n- *     println!(\"{}\", Point {x: 1, y: 0} - Point {x: 2, y: 3});\n- * }\n- * ```\n- *\n- * See the documentation for each trait for a minimum implementation that prints\n- * something to the screen.\n- *\n- */\n+//! Overloadable operators\n+//!\n+//! Implementing these traits allows you to get an effect similar to\n+//! overloading operators.\n+//!\n+//! The values for the right hand side of an operator are automatically\n+//! borrowed, so `a + b` is sugar for `a.add(&b)`.\n+//!\n+//! All of these traits are imported by the prelude, so they are available in\n+//! every Rust program.\n+//!\n+//! # Example\n+//!\n+//! This example creates a `Point` struct that implements `Add` and `Sub`, and then\n+//! demonstrates adding and subtracting two `Point`s.\n+//!\n+//! ```rust\n+//! #[deriving(Show)]\n+//! struct Point {\n+//!     x: int,\n+//!     y: int\n+//! }\n+//!\n+//! impl Add<Point, Point> for Point {\n+//!     fn add(&self, other: &Point) -> Point {\n+//!         Point {x: self.x + other.x, y: self.y + other.y}\n+//!     }\n+//! }\n+//!\n+//! impl Sub<Point, Point> for Point {\n+//!     fn sub(&self, other: &Point) -> Point {\n+//!         Point {x: self.x - other.x, y: self.y - other.y}\n+//!     }\n+//! }\n+//! fn main() {\n+//!     println!(\"{}\", Point {x: 1, y: 0} + Point {x: 2, y: 3});\n+//!     println!(\"{}\", Point {x: 1, y: 0} - Point {x: 2, y: 3});\n+//! }\n+//! ```\n+//!\n+//! See the documentation for each trait for a minimum implementation that prints\n+//! something to the screen.\n \n use kinds::Sized;\n "}, {"sha": "36a04392c36f3028619b000b719cc263ff720c31", "filename": "src/libflate/lib.rs", "status": "modified", "additions": 5, "deletions": 9, "changes": 14, "blob_url": "https://github.com/rust-lang/rust/blob/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibflate%2Flib.rs", "raw_url": "https://github.com/rust-lang/rust/raw/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibflate%2Flib.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibflate%2Flib.rs?ref=cd5c8235c5448a7234548c772468c8d2e8f150d9", "patch": "@@ -8,15 +8,11 @@\n // option. This file may not be copied, modified, or distributed\n // except according to those terms.\n \n-/*!\n-\n-Simple [DEFLATE][def]-based compression. This is a wrapper around the\n-[`miniz`][mz] library, which is a one-file pure-C implementation of zlib.\n-\n-[def]: https://en.wikipedia.org/wiki/DEFLATE\n-[mz]: https://code.google.com/p/miniz/\n-\n-*/\n+//! Simple [DEFLATE][def]-based compression. This is a wrapper around the\n+//! [`miniz`][mz] library, which is a one-file pure-C implementation of zlib.\n+//!\n+//! [def]: https://en.wikipedia.org/wiki/DEFLATE\n+//! [mz]: https://code.google.com/p/miniz/\n \n #![crate_name = \"flate\"]\n #![experimental]"}, {"sha": "04eeeb62e1d359cb31fa193aaafd7840589ddcd5", "filename": "src/libgraphviz/lib.rs", "status": "modified", "additions": 252, "deletions": 254, "changes": 506, "blob_url": "https://github.com/rust-lang/rust/blob/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibgraphviz%2Flib.rs", "raw_url": "https://github.com/rust-lang/rust/raw/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibgraphviz%2Flib.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibgraphviz%2Flib.rs?ref=cd5c8235c5448a7234548c772468c8d2e8f150d9", "patch": "@@ -8,260 +8,258 @@\n // option. This file may not be copied, modified, or distributed\n // except according to those terms.\n \n-/*! Generate files suitable for use with [Graphviz](http://www.graphviz.org/)\n-\n-The `render` function generates output (e.g. an `output.dot` file) for\n-use with [Graphviz](http://www.graphviz.org/) by walking a labelled\n-graph. (Graphviz can then automatically lay out the nodes and edges\n-of the graph, and also optionally render the graph as an image or\n-other [output formats](\n-http://www.graphviz.org/content/output-formats), such as SVG.)\n-\n-Rather than impose some particular graph data structure on clients,\n-this library exposes two traits that clients can implement on their\n-own structs before handing them over to the rendering function.\n-\n-Note: This library does not yet provide access to the full\n-expressiveness of the [DOT language](\n-http://www.graphviz.org/doc/info/lang.html). For example, there are\n-many [attributes](http://www.graphviz.org/content/attrs) related to\n-providing layout hints (e.g. left-to-right versus top-down, which\n-algorithm to use, etc). The current intention of this library is to\n-emit a human-readable .dot file with very regular structure suitable\n-for easy post-processing.\n-\n-# Examples\n-\n-The first example uses a very simple graph representation: a list of\n-pairs of ints, representing the edges (the node set is implicit).\n-Each node label is derived directly from the int representing the node,\n-while the edge labels are all empty strings.\n-\n-This example also illustrates how to use `CowVec` to return\n-an owned vector or a borrowed slice as appropriate: we construct the\n-node vector from scratch, but borrow the edge list (rather than\n-constructing a copy of all the edges from scratch).\n-\n-The output from this example renders five nodes, with the first four\n-forming a diamond-shaped acyclic graph and then pointing to the fifth\n-which is cyclic.\n-\n-```rust\n-use graphviz as dot;\n-\n-type Nd = int;\n-type Ed = (int,int);\n-struct Edges(Vec<Ed>);\n-\n-pub fn render_to<W:Writer>(output: &mut W) {\n-    let edges = Edges(vec!((0,1), (0,2), (1,3), (2,3), (3,4), (4,4)));\n-    dot::render(&edges, output).unwrap()\n-}\n-\n-impl<'a> dot::Labeller<'a, Nd, Ed> for Edges {\n-    fn graph_id(&'a self) -> dot::Id<'a> { dot::Id::new(\"example1\").unwrap() }\n-\n-    fn node_id(&'a self, n: &Nd) -> dot::Id<'a> {\n-        dot::Id::new(format!(\"N{}\", *n)).unwrap()\n-    }\n-}\n-\n-impl<'a> dot::GraphWalk<'a, Nd, Ed> for Edges {\n-    fn nodes(&self) -> dot::Nodes<'a,Nd> {\n-        // (assumes that |N| \\approxeq |E|)\n-        let &Edges(ref v) = self;\n-        let mut nodes = Vec::with_capacity(v.len());\n-        for &(s,t) in v.iter() {\n-            nodes.push(s); nodes.push(t);\n-        }\n-        nodes.sort();\n-        nodes.dedup();\n-        nodes.into_cow()\n-    }\n-\n-    fn edges(&'a self) -> dot::Edges<'a,Ed> {\n-        let &Edges(ref edges) = self;\n-        edges.as_slice().into_cow()\n-    }\n-\n-    fn source(&self, e: &Ed) -> Nd { let &(s,_) = e; s }\n-\n-    fn target(&self, e: &Ed) -> Nd { let &(_,t) = e; t }\n-}\n-\n-# pub fn main() { render_to(&mut Vec::new()) }\n-```\n-\n-```no_run\n-# pub fn render_to<W:Writer>(output: &mut W) { unimplemented!() }\n-pub fn main() {\n-    use std::io::File;\n-    let mut f = File::create(&Path::new(\"example1.dot\"));\n-    render_to(&mut f)\n-}\n-```\n-\n-Output from first example (in `example1.dot`):\n-\n-```ignore\n-digraph example1 {\n-    N0[label=\"N0\"];\n-    N1[label=\"N1\"];\n-    N2[label=\"N2\"];\n-    N3[label=\"N3\"];\n-    N4[label=\"N4\"];\n-    N0 -> N1[label=\"\"];\n-    N0 -> N2[label=\"\"];\n-    N1 -> N3[label=\"\"];\n-    N2 -> N3[label=\"\"];\n-    N3 -> N4[label=\"\"];\n-    N4 -> N4[label=\"\"];\n-}\n-```\n-\n-The second example illustrates using `node_label` and `edge_label` to\n-add labels to the nodes and edges in the rendered graph. The graph\n-here carries both `nodes` (the label text to use for rendering a\n-particular node), and `edges` (again a list of `(source,target)`\n-indices).\n-\n-This example also illustrates how to use a type (in this case the edge\n-type) that shares substructure with the graph: the edge type here is a\n-direct reference to the `(source,target)` pair stored in the graph's\n-internal vector (rather than passing around a copy of the pair\n-itself). Note that this implies that `fn edges(&'a self)` must\n-construct a fresh `Vec<&'a (uint,uint)>` from the `Vec<(uint,uint)>`\n-edges stored in `self`.\n-\n-Since both the set of nodes and the set of edges are always\n-constructed from scratch via iterators, we use the `collect()` method\n-from the `Iterator` trait to collect the nodes and edges into freshly\n-constructed growable `Vec` values (rather use the `into_cow`\n-from the `IntoCow` trait as was used in the first example\n-above).\n-\n-The output from this example renders four nodes that make up the\n-Hasse-diagram for the subsets of the set `{x, y}`. Each edge is\n-labelled with the &sube; character (specified using the HTML character\n-entity `&sube`).\n-\n-```rust\n-use graphviz as dot;\n-\n-type Nd = uint;\n-type Ed<'a> = &'a (uint, uint);\n-struct Graph { nodes: Vec<&'static str>, edges: Vec<(uint,uint)> }\n-\n-pub fn render_to<W:Writer>(output: &mut W) {\n-    let nodes = vec!(\"{x,y}\",\"{x}\",\"{y}\",\"{}\");\n-    let edges = vec!((0,1), (0,2), (1,3), (2,3));\n-    let graph = Graph { nodes: nodes, edges: edges };\n-\n-    dot::render(&graph, output).unwrap()\n-}\n-\n-impl<'a> dot::Labeller<'a, Nd, Ed<'a>> for Graph {\n-    fn graph_id(&'a self) -> dot::Id<'a> { dot::Id::new(\"example2\").unwrap() }\n-    fn node_id(&'a self, n: &Nd) -> dot::Id<'a> {\n-        dot::Id::new(format!(\"N{}\", n)).unwrap()\n-    }\n-    fn node_label<'a>(&'a self, n: &Nd) -> dot::LabelText<'a> {\n-        dot::LabelStr(self.nodes[*n].as_slice().into_cow())\n-    }\n-    fn edge_label<'a>(&'a self, _: &Ed) -> dot::LabelText<'a> {\n-        dot::LabelStr(\"&sube;\".into_cow())\n-    }\n-}\n-\n-impl<'a> dot::GraphWalk<'a, Nd, Ed<'a>> for Graph {\n-    fn nodes(&self) -> dot::Nodes<'a,Nd> { range(0,self.nodes.len()).collect() }\n-    fn edges(&'a self) -> dot::Edges<'a,Ed<'a>> { self.edges.iter().collect() }\n-    fn source(&self, e: &Ed) -> Nd { let & &(s,_) = e; s }\n-    fn target(&self, e: &Ed) -> Nd { let & &(_,t) = e; t }\n-}\n-\n-# pub fn main() { render_to(&mut Vec::new()) }\n-```\n-\n-```no_run\n-# pub fn render_to<W:Writer>(output: &mut W) { unimplemented!() }\n-pub fn main() {\n-    use std::io::File;\n-    let mut f = File::create(&Path::new(\"example2.dot\"));\n-    render_to(&mut f)\n-}\n-```\n-\n-The third example is similar to the second, except now each node and\n-edge now carries a reference to the string label for each node as well\n-as that node's index. (This is another illustration of how to share\n-structure with the graph itself, and why one might want to do so.)\n-\n-The output from this example is the same as the second example: the\n-Hasse-diagram for the subsets of the set `{x, y}`.\n-\n-```rust\n-use graphviz as dot;\n-\n-type Nd<'a> = (uint, &'a str);\n-type Ed<'a> = (Nd<'a>, Nd<'a>);\n-struct Graph { nodes: Vec<&'static str>, edges: Vec<(uint,uint)> }\n-\n-pub fn render_to<W:Writer>(output: &mut W) {\n-    let nodes = vec!(\"{x,y}\",\"{x}\",\"{y}\",\"{}\");\n-    let edges = vec!((0,1), (0,2), (1,3), (2,3));\n-    let graph = Graph { nodes: nodes, edges: edges };\n-\n-    dot::render(&graph, output).unwrap()\n-}\n-\n-impl<'a> dot::Labeller<'a, Nd<'a>, Ed<'a>> for Graph {\n-    fn graph_id(&'a self) -> dot::Id<'a> { dot::Id::new(\"example3\").unwrap() }\n-    fn node_id(&'a self, n: &Nd<'a>) -> dot::Id<'a> {\n-        dot::Id::new(format!(\"N{}\", n.val0())).unwrap()\n-    }\n-    fn node_label<'a>(&'a self, n: &Nd<'a>) -> dot::LabelText<'a> {\n-        let &(i, _) = n;\n-        dot::LabelStr(self.nodes[i].as_slice().into_cow())\n-    }\n-    fn edge_label<'a>(&'a self, _: &Ed<'a>) -> dot::LabelText<'a> {\n-        dot::LabelStr(\"&sube;\".into_cow())\n-    }\n-}\n-\n-impl<'a> dot::GraphWalk<'a, Nd<'a>, Ed<'a>> for Graph {\n-    fn nodes(&'a self) -> dot::Nodes<'a,Nd<'a>> {\n-        self.nodes.iter().map(|s|s.as_slice()).enumerate().collect()\n-    }\n-    fn edges(&'a self) -> dot::Edges<'a,Ed<'a>> {\n-        self.edges.iter()\n-            .map(|&(i,j)|((i, self.nodes[i].as_slice()),\n-                          (j, self.nodes[j].as_slice())))\n-            .collect()\n-    }\n-    fn source(&self, e: &Ed<'a>) -> Nd<'a> { let &(s,_) = e; s }\n-    fn target(&self, e: &Ed<'a>) -> Nd<'a> { let &(_,t) = e; t }\n-}\n-\n-# pub fn main() { render_to(&mut Vec::new()) }\n-```\n-\n-```no_run\n-# pub fn render_to<W:Writer>(output: &mut W) { unimplemented!() }\n-pub fn main() {\n-    use std::io::File;\n-    let mut f = File::create(&Path::new(\"example3.dot\"));\n-    render_to(&mut f)\n-}\n-```\n-\n-# References\n-\n-* [Graphviz](http://www.graphviz.org/)\n-\n-* [DOT language](http://www.graphviz.org/doc/info/lang.html)\n-\n-*/\n+//! Generate files suitable for use with [Graphviz](http://www.graphviz.org/)\n+//!\n+//! The `render` function generates output (e.g. an `output.dot` file) for\n+//! use with [Graphviz](http://www.graphviz.org/) by walking a labelled\n+//! graph. (Graphviz can then automatically lay out the nodes and edges\n+//! of the graph, and also optionally render the graph as an image or\n+//! other [output formats](\n+//! http://www.graphviz.org/content/output-formats), such as SVG.)\n+//!\n+//! Rather than impose some particular graph data structure on clients,\n+//! this library exposes two traits that clients can implement on their\n+//! own structs before handing them over to the rendering function.\n+//!\n+//! Note: This library does not yet provide access to the full\n+//! expressiveness of the [DOT language](\n+//! http://www.graphviz.org/doc/info/lang.html). For example, there are\n+//! many [attributes](http://www.graphviz.org/content/attrs) related to\n+//! providing layout hints (e.g. left-to-right versus top-down, which\n+//! algorithm to use, etc). The current intention of this library is to\n+//! emit a human-readable .dot file with very regular structure suitable\n+//! for easy post-processing.\n+//!\n+//! # Examples\n+//!\n+//! The first example uses a very simple graph representation: a list of\n+//! pairs of ints, representing the edges (the node set is implicit).\n+//! Each node label is derived directly from the int representing the node,\n+//! while the edge labels are all empty strings.\n+//!\n+//! This example also illustrates how to use `CowVec` to return\n+//! an owned vector or a borrowed slice as appropriate: we construct the\n+//! node vector from scratch, but borrow the edge list (rather than\n+//! constructing a copy of all the edges from scratch).\n+//!\n+//! The output from this example renders five nodes, with the first four\n+//! forming a diamond-shaped acyclic graph and then pointing to the fifth\n+//! which is cyclic.\n+//!\n+//! ```rust\n+//! use graphviz as dot;\n+//!\n+//! type Nd = int;\n+//! type Ed = (int,int);\n+//! struct Edges(Vec<Ed>);\n+//!\n+//! pub fn render_to<W:Writer>(output: &mut W) {\n+//!     let edges = Edges(vec!((0,1), (0,2), (1,3), (2,3), (3,4), (4,4)));\n+//!     dot::render(&edges, output).unwrap()\n+//! }\n+//!\n+//! impl<'a> dot::Labeller<'a, Nd, Ed> for Edges {\n+//!     fn graph_id(&'a self) -> dot::Id<'a> { dot::Id::new(\"example1\").unwrap() }\n+//!\n+//!     fn node_id(&'a self, n: &Nd) -> dot::Id<'a> {\n+//!         dot::Id::new(format!(\"N{}\", *n)).unwrap()\n+//!     }\n+//! }\n+//!\n+//! impl<'a> dot::GraphWalk<'a, Nd, Ed> for Edges {\n+//!     fn nodes(&self) -> dot::Nodes<'a,Nd> {\n+//!         // (assumes that |N| \\approxeq |E|)\n+//!         let &Edges(ref v) = self;\n+//!         let mut nodes = Vec::with_capacity(v.len());\n+//!         for &(s,t) in v.iter() {\n+//!             nodes.push(s); nodes.push(t);\n+//!         }\n+//!         nodes.sort();\n+//!         nodes.dedup();\n+//!         nodes.into_cow()\n+//!     }\n+//!\n+//!     fn edges(&'a self) -> dot::Edges<'a,Ed> {\n+//!         let &Edges(ref edges) = self;\n+//!         edges.as_slice().into_cow()\n+//!     }\n+//!\n+//!     fn source(&self, e: &Ed) -> Nd { let &(s,_) = e; s }\n+//!\n+//!     fn target(&self, e: &Ed) -> Nd { let &(_,t) = e; t }\n+//! }\n+//!\n+//! # pub fn main() { render_to(&mut Vec::new()) }\n+//! ```\n+//!\n+//! ```no_run\n+//! # pub fn render_to<W:Writer>(output: &mut W) { unimplemented!() }\n+//! pub fn main() {\n+//!     use std::io::File;\n+//!     let mut f = File::create(&Path::new(\"example1.dot\"));\n+//!     render_to(&mut f)\n+//! }\n+//! ```\n+//!\n+//! Output from first example (in `example1.dot`):\n+//!\n+//! ```ignore\n+//! digraph example1 {\n+//!     N0[label=\"N0\"];\n+//!     N1[label=\"N1\"];\n+//!     N2[label=\"N2\"];\n+//!     N3[label=\"N3\"];\n+//!     N4[label=\"N4\"];\n+//!     N0 -> N1[label=\"\"];\n+//!     N0 -> N2[label=\"\"];\n+//!     N1 -> N3[label=\"\"];\n+//!     N2 -> N3[label=\"\"];\n+//!     N3 -> N4[label=\"\"];\n+//!     N4 -> N4[label=\"\"];\n+//! }\n+//! ```\n+//!\n+//! The second example illustrates using `node_label` and `edge_label` to\n+//! add labels to the nodes and edges in the rendered graph. The graph\n+//! here carries both `nodes` (the label text to use for rendering a\n+//! particular node), and `edges` (again a list of `(source,target)`\n+//! indices).\n+//!\n+//! This example also illustrates how to use a type (in this case the edge\n+//! type) that shares substructure with the graph: the edge type here is a\n+//! direct reference to the `(source,target)` pair stored in the graph's\n+//! internal vector (rather than passing around a copy of the pair\n+//! itself). Note that this implies that `fn edges(&'a self)` must\n+//! construct a fresh `Vec<&'a (uint,uint)>` from the `Vec<(uint,uint)>`\n+//! edges stored in `self`.\n+//!\n+//! Since both the set of nodes and the set of edges are always\n+//! constructed from scratch via iterators, we use the `collect()` method\n+//! from the `Iterator` trait to collect the nodes and edges into freshly\n+//! constructed growable `Vec` values (rather use the `into_cow`\n+//! from the `IntoCow` trait as was used in the first example\n+//! above).\n+//!\n+//! The output from this example renders four nodes that make up the\n+//! Hasse-diagram for the subsets of the set `{x, y}`. Each edge is\n+//! labelled with the &sube; character (specified using the HTML character\n+//! entity `&sube`).\n+//!\n+//! ```rust\n+//! use graphviz as dot;\n+//!\n+//! type Nd = uint;\n+//! type Ed<'a> = &'a (uint, uint);\n+//! struct Graph { nodes: Vec<&'static str>, edges: Vec<(uint,uint)> }\n+//!\n+//! pub fn render_to<W:Writer>(output: &mut W) {\n+//!     let nodes = vec!(\"{x,y}\",\"{x}\",\"{y}\",\"{}\");\n+//!     let edges = vec!((0,1), (0,2), (1,3), (2,3));\n+//!     let graph = Graph { nodes: nodes, edges: edges };\n+//!\n+//!     dot::render(&graph, output).unwrap()\n+//! }\n+//!\n+//! impl<'a> dot::Labeller<'a, Nd, Ed<'a>> for Graph {\n+//!     fn graph_id(&'a self) -> dot::Id<'a> { dot::Id::new(\"example2\").unwrap() }\n+//!     fn node_id(&'a self, n: &Nd) -> dot::Id<'a> {\n+//!         dot::Id::new(format!(\"N{}\", n)).unwrap()\n+//!     }\n+//!     fn node_label<'a>(&'a self, n: &Nd) -> dot::LabelText<'a> {\n+//!         dot::LabelStr(self.nodes[*n].as_slice().into_cow())\n+//!     }\n+//!     fn edge_label<'a>(&'a self, _: &Ed) -> dot::LabelText<'a> {\n+//!         dot::LabelStr(\"&sube;\".into_cow())\n+//!     }\n+//! }\n+//!\n+//! impl<'a> dot::GraphWalk<'a, Nd, Ed<'a>> for Graph {\n+//!     fn nodes(&self) -> dot::Nodes<'a,Nd> { range(0,self.nodes.len()).collect() }\n+//!     fn edges(&'a self) -> dot::Edges<'a,Ed<'a>> { self.edges.iter().collect() }\n+//!     fn source(&self, e: &Ed) -> Nd { let & &(s,_) = e; s }\n+//!     fn target(&self, e: &Ed) -> Nd { let & &(_,t) = e; t }\n+//! }\n+//!\n+//! # pub fn main() { render_to(&mut Vec::new()) }\n+//! ```\n+//!\n+//! ```no_run\n+//! # pub fn render_to<W:Writer>(output: &mut W) { unimplemented!() }\n+//! pub fn main() {\n+//!     use std::io::File;\n+//!     let mut f = File::create(&Path::new(\"example2.dot\"));\n+//!     render_to(&mut f)\n+//! }\n+//! ```\n+//!\n+//! The third example is similar to the second, except now each node and\n+//! edge now carries a reference to the string label for each node as well\n+//! as that node's index. (This is another illustration of how to share\n+//! structure with the graph itself, and why one might want to do so.)\n+//!\n+//! The output from this example is the same as the second example: the\n+//! Hasse-diagram for the subsets of the set `{x, y}`.\n+//!\n+//! ```rust\n+//! use graphviz as dot;\n+//!\n+//! type Nd<'a> = (uint, &'a str);\n+//! type Ed<'a> = (Nd<'a>, Nd<'a>);\n+//! struct Graph { nodes: Vec<&'static str>, edges: Vec<(uint,uint)> }\n+//!\n+//! pub fn render_to<W:Writer>(output: &mut W) {\n+//!     let nodes = vec!(\"{x,y}\",\"{x}\",\"{y}\",\"{}\");\n+//!     let edges = vec!((0,1), (0,2), (1,3), (2,3));\n+//!     let graph = Graph { nodes: nodes, edges: edges };\n+//!\n+//!     dot::render(&graph, output).unwrap()\n+//! }\n+//!\n+//! impl<'a> dot::Labeller<'a, Nd<'a>, Ed<'a>> for Graph {\n+//!     fn graph_id(&'a self) -> dot::Id<'a> { dot::Id::new(\"example3\").unwrap() }\n+//!     fn node_id(&'a self, n: &Nd<'a>) -> dot::Id<'a> {\n+//!         dot::Id::new(format!(\"N{}\", n.val0())).unwrap()\n+//!     }\n+//!     fn node_label<'a>(&'a self, n: &Nd<'a>) -> dot::LabelText<'a> {\n+//!         let &(i, _) = n;\n+//!         dot::LabelStr(self.nodes[i].as_slice().into_cow())\n+//!     }\n+//!     fn edge_label<'a>(&'a self, _: &Ed<'a>) -> dot::LabelText<'a> {\n+//!         dot::LabelStr(\"&sube;\".into_cow())\n+//!     }\n+//! }\n+//!\n+//! impl<'a> dot::GraphWalk<'a, Nd<'a>, Ed<'a>> for Graph {\n+//!     fn nodes(&'a self) -> dot::Nodes<'a,Nd<'a>> {\n+//!         self.nodes.iter().map(|s|s.as_slice()).enumerate().collect()\n+//!     }\n+//!     fn edges(&'a self) -> dot::Edges<'a,Ed<'a>> {\n+//!         self.edges.iter()\n+//!             .map(|&(i,j)|((i, self.nodes[i].as_slice()),\n+//!                           (j, self.nodes[j].as_slice())))\n+//!             .collect()\n+//!     }\n+//!     fn source(&self, e: &Ed<'a>) -> Nd<'a> { let &(s,_) = e; s }\n+//!     fn target(&self, e: &Ed<'a>) -> Nd<'a> { let &(_,t) = e; t }\n+//! }\n+//!\n+//! # pub fn main() { render_to(&mut Vec::new()) }\n+//! ```\n+//!\n+//! ```no_run\n+//! # pub fn render_to<W:Writer>(output: &mut W) { unimplemented!() }\n+//! pub fn main() {\n+//!     use std::io::File;\n+//!     let mut f = File::create(&Path::new(\"example3.dot\"));\n+//!     render_to(&mut f)\n+//! }\n+//! ```\n+//!\n+//! # References\n+//!\n+//! * [Graphviz](http://www.graphviz.org/)\n+//!\n+//! * [DOT language](http://www.graphviz.org/doc/info/lang.html)\n \n #![crate_name = \"graphviz\"]\n #![experimental]"}, {"sha": "0014a3e3941d75875bd29349d1188335ddad571f", "filename": "src/liblibc/lib.rs", "status": "modified", "additions": 51, "deletions": 53, "changes": 104, "blob_url": "https://github.com/rust-lang/rust/blob/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Fliblibc%2Flib.rs", "raw_url": "https://github.com/rust-lang/rust/raw/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Fliblibc%2Flib.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Fliblibc%2Flib.rs?ref=cd5c8235c5448a7234548c772468c8d2e8f150d9", "patch": "@@ -19,59 +19,57 @@\n        html_root_url = \"http://doc.rust-lang.org/nightly/\",\n        html_playground_url = \"http://play.rust-lang.org/\")]\n \n-/*!\n-* Bindings for the C standard library and other platform libraries\n-*\n-* **NOTE:** These are *architecture and libc* specific. On Linux, these\n-* bindings are only correct for glibc.\n-*\n-* This module contains bindings to the C standard library, organized into\n-* modules by their defining standard.  Additionally, it contains some assorted\n-* platform-specific definitions.  For convenience, most functions and types\n-* are reexported, so `use libc::*` will import the available C bindings as\n-* appropriate for the target platform. The exact set of functions available\n-* are platform specific.\n-*\n-* *Note:* Because these definitions are platform-specific, some may not appear\n-* in the generated documentation.\n-*\n-* We consider the following specs reasonably normative with respect to\n-* interoperating with the C standard library (libc/msvcrt):\n-*\n-* * ISO 9899:1990 ('C95', 'ANSI C', 'Standard C'), NA1, 1995.\n-* * ISO 9899:1999 ('C99' or 'C9x').\n-* * ISO 9945:1988 / IEEE 1003.1-1988 ('POSIX.1').\n-* * ISO 9945:2001 / IEEE 1003.1-2001 ('POSIX:2001', 'SUSv3').\n-* * ISO 9945:2008 / IEEE 1003.1-2008 ('POSIX:2008', 'SUSv4').\n-*\n-* Note that any reference to the 1996 revision of POSIX, or any revs between\n-* 1990 (when '88 was approved at ISO) and 2001 (when the next actual\n-* revision-revision happened), are merely additions of other chapters (1b and\n-* 1c) outside the core interfaces.\n-*\n-* Despite having several names each, these are *reasonably* coherent\n-* point-in-time, list-of-definition sorts of specs. You can get each under a\n-* variety of names but will wind up with the same definition in each case.\n-*\n-* See standards(7) in linux-manpages for more details.\n-*\n-* Our interface to these libraries is complicated by the non-universality of\n-* conformance to any of them. About the only thing universally supported is\n-* the first (C95), beyond that definitions quickly become absent on various\n-* platforms.\n-*\n-* We therefore wind up dividing our module-space up (mostly for the sake of\n-* sanity while editing, filling-in-details and eliminating duplication) into\n-* definitions common-to-all (held in modules named c95, c99, posix88, posix01\n-* and posix08) and definitions that appear only on *some* platforms (named\n-* 'extra'). This would be things like significant OSX foundation kit, or Windows\n-* library kernel32.dll, or various fancy glibc, Linux or BSD extensions.\n-*\n-* In addition to the per-platform 'extra' modules, we define a module of\n-* 'common BSD' libc routines that never quite made it into POSIX but show up\n-* in multiple derived systems. This is the 4.4BSD r2 / 1995 release, the final\n-* one from Berkeley after the lawsuits died down and the CSRG dissolved.\n-*/\n+//! Bindings for the C standard library and other platform libraries\n+//!\n+//! **NOTE:** These are *architecture and libc* specific. On Linux, these\n+//! bindings are only correct for glibc.\n+//!\n+//! This module contains bindings to the C standard library, organized into\n+//! modules by their defining standard.  Additionally, it contains some assorted\n+//! platform-specific definitions.  For convenience, most functions and types\n+//! are reexported, so `use libc::*` will import the available C bindings as\n+//! appropriate for the target platform. The exact set of functions available\n+//! are platform specific.\n+//!\n+//! *Note:* Because these definitions are platform-specific, some may not appear\n+//! in the generated documentation.\n+//!\n+//! We consider the following specs reasonably normative with respect to\n+//! interoperating with the C standard library (libc/msvcrt):\n+//!\n+//! * ISO 9899:1990 ('C95', 'ANSI C', 'Standard C'), NA1, 1995.\n+//! * ISO 9899:1999 ('C99' or 'C9x').\n+//! * ISO 9945:1988 / IEEE 1003.1-1988 ('POSIX.1').\n+//! * ISO 9945:2001 / IEEE 1003.1-2001 ('POSIX:2001', 'SUSv3').\n+//! * ISO 9945:2008 / IEEE 1003.1-2008 ('POSIX:2008', 'SUSv4').\n+//!\n+//! Note that any reference to the 1996 revision of POSIX, or any revs between\n+//! 1990 (when '88 was approved at ISO) and 2001 (when the next actual\n+//! revision-revision happened), are merely additions of other chapters (1b and\n+//! 1c) outside the core interfaces.\n+//!\n+//! Despite having several names each, these are *reasonably* coherent\n+//! point-in-time, list-of-definition sorts of specs. You can get each under a\n+//! variety of names but will wind up with the same definition in each case.\n+//!\n+//! See standards(7) in linux-manpages for more details.\n+//!\n+//! Our interface to these libraries is complicated by the non-universality of\n+//! conformance to any of them. About the only thing universally supported is\n+//! the first (C95), beyond that definitions quickly become absent on various\n+//! platforms.\n+//!\n+//! We therefore wind up dividing our module-space up (mostly for the sake of\n+//! sanity while editing, filling-in-details and eliminating duplication) into\n+//! definitions common-to-all (held in modules named c95, c99, posix88, posix01\n+//! and posix08) and definitions that appear only on *some* platforms (named\n+//! 'extra'). This would be things like significant OSX foundation kit, or Windows\n+//! library kernel32.dll, or various fancy glibc, Linux or BSD extensions.\n+//!\n+//! In addition to the per-platform 'extra' modules, we define a module of\n+//! 'common BSD' libc routines that never quite made it into POSIX but show up\n+//! in multiple derived systems. This is the 4.4BSD r2 / 1995 release, the final\n+//! one from Berkeley after the lawsuits died down and the CSRG dissolved.\n \n #![allow(non_camel_case_types)]\n #![allow(non_snake_case)]"}, {"sha": "0fa989bf0b2b9b4e8713bc1cc2581e5fba7a31bd", "filename": "src/librand/distributions/mod.rs", "status": "modified", "additions": 8, "deletions": 11, "changes": 19, "blob_url": "https://github.com/rust-lang/rust/blob/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrand%2Fdistributions%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrand%2Fdistributions%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrand%2Fdistributions%2Fmod.rs?ref=cd5c8235c5448a7234548c772468c8d2e8f150d9", "patch": "@@ -8,17 +8,14 @@\n // option. This file may not be copied, modified, or distributed\n // except according to those terms.\n \n-/*!\n-Sampling from random distributions.\n-\n-This is a generalization of `Rand` to allow parameters to control the\n-exact properties of the generated values, e.g. the mean and standard\n-deviation of a normal distribution. The `Sample` trait is the most\n-general, and allows for generating values that change some state\n-internally. The `IndependentSample` trait is for generating values\n-that do not need to record state.\n-\n-*/\n+//! Sampling from random distributions.\n+//!\n+//! This is a generalization of `Rand` to allow parameters to control the\n+//! exact properties of the generated values, e.g. the mean and standard\n+//! deviation of a normal distribution. The `Sample` trait is the most\n+//! general, and allows for generating values that change some state\n+//! internally. The `IndependentSample` trait is for generating values\n+//! that do not need to record state.\n \n #![experimental]\n "}, {"sha": "c599a0f2daf7f194249e0780867a745b8a3d8486", "filename": "src/librustc/lib.rs", "status": "modified", "additions": 5, "deletions": 9, "changes": 14, "blob_url": "https://github.com/rust-lang/rust/blob/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc%2Flib.rs", "raw_url": "https://github.com/rust-lang/rust/raw/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc%2Flib.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Flib.rs?ref=cd5c8235c5448a7234548c772468c8d2e8f150d9", "patch": "@@ -8,15 +8,11 @@\n // option. This file may not be copied, modified, or distributed\n // except according to those terms.\n \n-/*!\n-\n-The Rust compiler.\n-\n-# Note\n-\n-This API is completely unstable and subject to change.\n-\n-*/\n+//! The Rust compiler.\n+//!\n+//! # Note\n+//!\n+//! This API is completely unstable and subject to change.\n \n #![crate_name = \"rustc\"]\n #![experimental]"}, {"sha": "523e997a8deec149995e228cf531ddc7719841b9", "filename": "src/librustc/middle/astencode.rs", "status": "modified", "additions": 51, "deletions": 69, "changes": 120, "blob_url": "https://github.com/rust-lang/rust/blob/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc%2Fmiddle%2Fastencode.rs", "raw_url": "https://github.com/rust-lang/rust/raw/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc%2Fmiddle%2Fastencode.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Fmiddle%2Fastencode.rs?ref=cd5c8235c5448a7234548c772468c8d2e8f150d9", "patch": "@@ -196,53 +196,38 @@ fn reserve_id_range(sess: &Session,\n }\n \n impl<'a, 'b, 'tcx> DecodeContext<'a, 'b, 'tcx> {\n+    /// Translates an internal id, meaning a node id that is known to refer to some part of the\n+    /// item currently being inlined, such as a local variable or argument.  All naked node-ids\n+    /// that appear in types have this property, since if something might refer to an external item\n+    /// we would use a def-id to allow for the possibility that the item resides in another crate.\n     pub fn tr_id(&self, id: ast::NodeId) -> ast::NodeId {\n-        /*!\n-         * Translates an internal id, meaning a node id that is known\n-         * to refer to some part of the item currently being inlined,\n-         * such as a local variable or argument.  All naked node-ids\n-         * that appear in types have this property, since if something\n-         * might refer to an external item we would use a def-id to\n-         * allow for the possibility that the item resides in another\n-         * crate.\n-         */\n-\n         // from_id_range should be non-empty\n         assert!(!self.from_id_range.empty());\n         (id - self.from_id_range.min + self.to_id_range.min)\n     }\n+\n+    /// Translates an EXTERNAL def-id, converting the crate number from the one used in the encoded\n+    /// data to the current crate numbers..  By external, I mean that it be translated to a\n+    /// reference to the item in its original crate, as opposed to being translated to a reference\n+    /// to the inlined version of the item.  This is typically, but not always, what you want,\n+    /// because most def-ids refer to external things like types or other fns that may or may not\n+    /// be inlined.  Note that even when the inlined function is referencing itself recursively, we\n+    /// would want `tr_def_id` for that reference--- conceptually the function calls the original,\n+    /// non-inlined version, and trans deals with linking that recursive call to the inlined copy.\n+    ///\n+    /// However, there are a *few* cases where def-ids are used but we know that the thing being\n+    /// referenced is in fact *internal* to the item being inlined.  In those cases, you should use\n+    /// `tr_intern_def_id()` below.\n     pub fn tr_def_id(&self, did: ast::DefId) -> ast::DefId {\n-        /*!\n-         * Translates an EXTERNAL def-id, converting the crate number\n-         * from the one used in the encoded data to the current crate\n-         * numbers..  By external, I mean that it be translated to a\n-         * reference to the item in its original crate, as opposed to\n-         * being translated to a reference to the inlined version of\n-         * the item.  This is typically, but not always, what you\n-         * want, because most def-ids refer to external things like\n-         * types or other fns that may or may not be inlined.  Note\n-         * that even when the inlined function is referencing itself\n-         * recursively, we would want `tr_def_id` for that\n-         * reference--- conceptually the function calls the original,\n-         * non-inlined version, and trans deals with linking that\n-         * recursive call to the inlined copy.\n-         *\n-         * However, there are a *few* cases where def-ids are used but\n-         * we know that the thing being referenced is in fact *internal*\n-         * to the item being inlined.  In those cases, you should use\n-         * `tr_intern_def_id()` below.\n-         */\n \n         decoder::translate_def_id(self.cdata, did)\n     }\n-    pub fn tr_intern_def_id(&self, did: ast::DefId) -> ast::DefId {\n-        /*!\n-         * Translates an INTERNAL def-id, meaning a def-id that is\n-         * known to refer to some part of the item currently being\n-         * inlined.  In that case, we want to convert the def-id to\n-         * refer to the current crate and to the new, inlined node-id.\n-         */\n \n+    /// Translates an INTERNAL def-id, meaning a def-id that is\n+    /// known to refer to some part of the item currently being\n+    /// inlined.  In that case, we want to convert the def-id to\n+    /// refer to the current crate and to the new, inlined node-id.\n+    pub fn tr_intern_def_id(&self, did: ast::DefId) -> ast::DefId {\n         assert_eq!(did.krate, ast::LOCAL_CRATE);\n         ast::DefId { krate: ast::LOCAL_CRATE, node: self.tr_id(did.node) }\n     }\n@@ -1780,43 +1765,40 @@ impl<'a, 'tcx> rbml_decoder_decoder_helpers<'tcx> for reader::Decoder<'a> {\n         }\n     }\n \n+    /// Converts a def-id that appears in a type.  The correct\n+    /// translation will depend on what kind of def-id this is.\n+    /// This is a subtle point: type definitions are not\n+    /// inlined into the current crate, so if the def-id names\n+    /// a nominal type or type alias, then it should be\n+    /// translated to refer to the source crate.\n+    ///\n+    /// However, *type parameters* are cloned along with the function\n+    /// they are attached to.  So we should translate those def-ids\n+    /// to refer to the new, cloned copy of the type parameter.\n+    /// We only see references to free type parameters in the body of\n+    /// an inlined function. In such cases, we need the def-id to\n+    /// be a local id so that the TypeContents code is able to lookup\n+    /// the relevant info in the ty_param_defs table.\n+    ///\n+    /// *Region parameters*, unfortunately, are another kettle of fish.\n+    /// In such cases, def_id's can appear in types to distinguish\n+    /// shadowed bound regions and so forth. It doesn't actually\n+    /// matter so much what we do to these, since regions are erased\n+    /// at trans time, but it's good to keep them consistent just in\n+    /// case. We translate them with `tr_def_id()` which will map\n+    /// the crate numbers back to the original source crate.\n+    ///\n+    /// Unboxed closures are cloned along with the function being\n+    /// inlined, and all side tables use interned node IDs, so we\n+    /// translate their def IDs accordingly.\n+    ///\n+    /// It'd be really nice to refactor the type repr to not include\n+    /// def-ids so that all these distinctions were unnecessary.\n     fn convert_def_id(&mut self,\n                       dcx: &DecodeContext,\n                       source: tydecode::DefIdSource,\n                       did: ast::DefId)\n                       -> ast::DefId {\n-        /*!\n-         * Converts a def-id that appears in a type.  The correct\n-         * translation will depend on what kind of def-id this is.\n-         * This is a subtle point: type definitions are not\n-         * inlined into the current crate, so if the def-id names\n-         * a nominal type or type alias, then it should be\n-         * translated to refer to the source crate.\n-         *\n-         * However, *type parameters* are cloned along with the function\n-         * they are attached to.  So we should translate those def-ids\n-         * to refer to the new, cloned copy of the type parameter.\n-         * We only see references to free type parameters in the body of\n-         * an inlined function. In such cases, we need the def-id to\n-         * be a local id so that the TypeContents code is able to lookup\n-         * the relevant info in the ty_param_defs table.\n-         *\n-         * *Region parameters*, unfortunately, are another kettle of fish.\n-         * In such cases, def_id's can appear in types to distinguish\n-         * shadowed bound regions and so forth. It doesn't actually\n-         * matter so much what we do to these, since regions are erased\n-         * at trans time, but it's good to keep them consistent just in\n-         * case. We translate them with `tr_def_id()` which will map\n-         * the crate numbers back to the original source crate.\n-         *\n-         * Unboxed closures are cloned along with the function being\n-         * inlined, and all side tables use interned node IDs, so we\n-         * translate their def IDs accordingly.\n-         *\n-         * It'd be really nice to refactor the type repr to not include\n-         * def-ids so that all these distinctions were unnecessary.\n-         */\n-\n         let r = match source {\n             NominalType | TypeWithId | RegionParameter => dcx.tr_def_id(did),\n             TypeParameter | UnboxedClosureSource => dcx.tr_intern_def_id(did)"}, {"sha": "9a27abbe8322dc107a3a90f4eb4c785d48509ce8", "filename": "src/librustc/middle/borrowck/check_loans.rs", "status": "modified", "additions": 19, "deletions": 23, "changes": 42, "blob_url": "https://github.com/rust-lang/rust/blob/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc%2Fmiddle%2Fborrowck%2Fcheck_loans.rs", "raw_url": "https://github.com/rust-lang/rust/raw/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc%2Fmiddle%2Fborrowck%2Fcheck_loans.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Fmiddle%2Fborrowck%2Fcheck_loans.rs?ref=cd5c8235c5448a7234548c772468c8d2e8f150d9", "patch": "@@ -684,16 +684,13 @@ impl<'a, 'tcx> CheckLoanCtxt<'a, 'tcx> {\n         return ret;\n     }\n \n+    /// Reports an error if `expr` (which should be a path)\n+    /// is using a moved/uninitialized value\n     fn check_if_path_is_moved(&self,\n                               id: ast::NodeId,\n                               span: Span,\n                               use_kind: MovedValueUseKind,\n                               lp: &Rc<LoanPath<'tcx>>) {\n-        /*!\n-         * Reports an error if `expr` (which should be a path)\n-         * is using a moved/uninitialized value\n-         */\n-\n         debug!(\"check_if_path_is_moved(id={}, use_kind={}, lp={})\",\n                id, use_kind, lp.repr(self.bccx.tcx));\n         let base_lp = owned_ptr_base_path_rc(lp);\n@@ -708,30 +705,29 @@ impl<'a, 'tcx> CheckLoanCtxt<'a, 'tcx> {\n         });\n     }\n \n+    /// Reports an error if assigning to `lp` will use a\n+    /// moved/uninitialized value. Mainly this is concerned with\n+    /// detecting derefs of uninitialized pointers.\n+    ///\n+    /// For example:\n+    ///\n+    /// ```\n+    /// let a: int;\n+    /// a = 10; // ok, even though a is uninitialized\n+    ///\n+    /// struct Point { x: uint, y: uint }\n+    /// let p: Point;\n+    /// p.x = 22; // ok, even though `p` is uninitialized\n+    ///\n+    /// let p: ~Point;\n+    /// (*p).x = 22; // not ok, p is uninitialized, can't deref\n+    /// ```\n     fn check_if_assigned_path_is_moved(&self,\n                                        id: ast::NodeId,\n                                        span: Span,\n                                        use_kind: MovedValueUseKind,\n                                        lp: &Rc<LoanPath<'tcx>>)\n     {\n-        /*!\n-         * Reports an error if assigning to `lp` will use a\n-         * moved/uninitialized value. Mainly this is concerned with\n-         * detecting derefs of uninitialized pointers.\n-         *\n-         * For example:\n-         *\n-         *     let a: int;\n-         *     a = 10; // ok, even though a is uninitialized\n-         *\n-         *     struct Point { x: uint, y: uint }\n-         *     let p: Point;\n-         *     p.x = 22; // ok, even though `p` is uninitialized\n-         *\n-         *     let p: ~Point;\n-         *     (*p).x = 22; // not ok, p is uninitialized, can't deref\n-         */\n-\n         match lp.kind {\n             LpVar(_) | LpUpvar(_) => {\n                 // assigning to `x` does not require that `x` is initialized"}, {"sha": "c6db5340f0f511d9e6865516368908a347b68185", "filename": "src/librustc/middle/borrowck/doc.rs", "status": "modified", "additions": 1212, "deletions": 1216, "changes": 2428, "blob_url": "https://github.com/rust-lang/rust/blob/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc%2Fmiddle%2Fborrowck%2Fdoc.rs", "raw_url": "https://github.com/rust-lang/rust/raw/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc%2Fmiddle%2Fborrowck%2Fdoc.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Fmiddle%2Fborrowck%2Fdoc.rs?ref=cd5c8235c5448a7234548c772468c8d2e8f150d9", "patch": "@@ -8,1219 +8,1215 @@\n // option. This file may not be copied, modified, or distributed\n // except according to those terms.\n \n-/*!\n-\n-# The Borrow Checker\n-\n-This pass has the job of enforcing memory safety. This is a subtle\n-topic. This docs aim to explain both the practice and the theory\n-behind the borrow checker. They start with a high-level overview of\n-how it works, and then proceed to dive into the theoretical\n-background. Finally, they go into detail on some of the more subtle\n-aspects.\n-\n-# Table of contents\n-\n-These docs are long. Search for the section you are interested in.\n-\n-- Overview\n-- Formal model\n-- Borrowing and loans\n-- Moves and initialization\n-- Drop flags and structural fragments\n-- Future work\n-\n-# Overview\n-\n-The borrow checker checks one function at a time. It operates in two\n-passes. The first pass, called `gather_loans`, walks over the function\n-and identifies all of the places where borrows (e.g., `&` expressions\n-and `ref` bindings) and moves (copies or captures of a linear value)\n-occur. It also tracks initialization sites. For each borrow and move,\n-it checks various basic safety conditions at this time (for example,\n-that the lifetime of the borrow doesn't exceed the lifetime of the\n-value being borrowed, or that there is no move out of an `&T`\n-referent).\n-\n-It then uses the dataflow module to propagate which of those borrows\n-may be in scope at each point in the procedure. A loan is considered\n-to come into scope at the expression that caused it and to go out of\n-scope when the lifetime of the resulting reference expires.\n-\n-Once the in-scope loans are known for each point in the program, the\n-borrow checker walks the IR again in a second pass called\n-`check_loans`. This pass examines each statement and makes sure that\n-it is safe with respect to the in-scope loans.\n-\n-# Formal model\n-\n-Throughout the docs we'll consider a simple subset of Rust in which\n-you can only borrow from lvalues, defined like so:\n-\n-```text\n-LV = x | LV.f | *LV\n-```\n-\n-Here `x` represents some variable, `LV.f` is a field reference,\n-and `*LV` is a pointer dereference. There is no auto-deref or other\n-niceties. This means that if you have a type like:\n-\n-```text\n-struct S { f: uint }\n-```\n-\n-and a variable `a: Box<S>`, then the rust expression `a.f` would correspond\n-to an `LV` of `(*a).f`.\n-\n-Here is the formal grammar for the types we'll consider:\n-\n-```text\n-TY = () | S<'LT...> | Box<TY> | & 'LT MQ TY\n-MQ = mut | imm | const\n-```\n-\n-Most of these types should be pretty self explanatory. Here `S` is a\n-struct name and we assume structs are declared like so:\n-\n-```text\n-SD = struct S<'LT...> { (f: TY)... }\n-```\n-\n-# Borrowing and loans\n-\n-## An intuitive explanation\n-\n-### Issuing loans\n-\n-Now, imagine we had a program like this:\n-\n-```text\n-struct Foo { f: uint, g: uint }\n-...\n-'a: {\n-  let mut x: Box<Foo> = ...;\n-  let y = &mut (*x).f;\n-  x = ...;\n-}\n-```\n-\n-This is of course dangerous because mutating `x` will free the old\n-value and hence invalidate `y`. The borrow checker aims to prevent\n-this sort of thing.\n-\n-#### Loans and restrictions\n-\n-The way the borrow checker works is that it analyzes each borrow\n-expression (in our simple model, that's stuff like `&LV`, though in\n-real life there are a few other cases to consider). For each borrow\n-expression, it computes a `Loan`, which is a data structure that\n-records (1) the value being borrowed, (2) the mutability and scope of\n-the borrow, and (3) a set of restrictions. In the code, `Loan` is a\n-struct defined in `middle::borrowck`. Formally, we define `LOAN` as\n-follows:\n-\n-```text\n-LOAN = (LV, LT, MQ, RESTRICTION*)\n-RESTRICTION = (LV, ACTION*)\n-ACTION = MUTATE | CLAIM | FREEZE\n-```\n-\n-Here the `LOAN` tuple defines the lvalue `LV` being borrowed; the\n-lifetime `LT` of that borrow; the mutability `MQ` of the borrow; and a\n-list of restrictions. The restrictions indicate actions which, if\n-taken, could invalidate the loan and lead to type safety violations.\n-\n-Each `RESTRICTION` is a pair of a restrictive lvalue `LV` (which will\n-either be the path that was borrowed or some prefix of the path that\n-was borrowed) and a set of restricted actions.  There are three kinds\n-of actions that may be restricted for the path `LV`:\n-\n-- `MUTATE` means that `LV` cannot be assigned to;\n-- `CLAIM` means that the `LV` cannot be borrowed mutably;\n-- `FREEZE` means that the `LV` cannot be borrowed immutably;\n-\n-Finally, it is never possible to move from an lvalue that appears in a\n-restriction. This implies that the \"empty restriction\" `(LV, [])`,\n-which contains an empty set of actions, still has a purpose---it\n-prevents moves from `LV`. I chose not to make `MOVE` a fourth kind of\n-action because that would imply that sometimes moves are permitted\n-from restrictived values, which is not the case.\n-\n-#### Example\n-\n-To give you a better feeling for what kind of restrictions derived\n-from a loan, let's look at the loan `L` that would be issued as a\n-result of the borrow `&mut (*x).f` in the example above:\n-\n-```text\n-L = ((*x).f, 'a, mut, RS) where\n-    RS = [((*x).f, [MUTATE, CLAIM, FREEZE]),\n-          (*x, [MUTATE, CLAIM, FREEZE]),\n-          (x, [MUTATE, CLAIM, FREEZE])]\n-```\n-\n-The loan states that the expression `(*x).f` has been loaned as\n-mutable for the lifetime `'a`. Because the loan is mutable, that means\n-that the value `(*x).f` may be mutated via the newly created reference\n-(and *only* via that pointer). This is reflected in the\n-restrictions `RS` that accompany the loan.\n-\n-The first restriction `((*x).f, [MUTATE, CLAIM, FREEZE])` states that\n-the lender may not mutate, freeze, nor alias `(*x).f`. Mutation is\n-illegal because `(*x).f` is only supposed to be mutated via the new\n-reference, not by mutating the original path `(*x).f`. Freezing is\n-illegal because the path now has an `&mut` alias; so even if we the\n-lender were to consider `(*x).f` to be immutable, it might be mutated\n-via this alias. They will be enforced for the lifetime `'a` of the\n-loan. After the loan expires, the restrictions no longer apply.\n-\n-The second restriction on `*x` is interesting because it does not\n-apply to the path that was lent (`(*x).f`) but rather to a prefix of\n-the borrowed path. This is due to the rules of inherited mutability:\n-if the user were to assign to (or freeze) `*x`, they would indirectly\n-overwrite (or freeze) `(*x).f`, and thus invalidate the reference\n-that was created. In general it holds that when a path is\n-lent, restrictions are issued for all the owning prefixes of that\n-path. In this case, the path `*x` owns the path `(*x).f` and,\n-because `x` is an owned pointer, the path `x` owns the path `*x`.\n-Therefore, borrowing `(*x).f` yields restrictions on both\n-`*x` and `x`.\n-\n-### Checking for illegal assignments, moves, and reborrows\n-\n-Once we have computed the loans introduced by each borrow, the borrow\n-checker uses a data flow propagation to compute the full set of loans\n-in scope at each expression and then uses that set to decide whether\n-that expression is legal.  Remember that the scope of loan is defined\n-by its lifetime LT.  We sometimes say that a loan which is in-scope at\n-a particular point is an \"outstanding loan\", and the set of\n-restrictions included in those loans as the \"outstanding\n-restrictions\".\n-\n-The kinds of expressions which in-scope loans can render illegal are:\n-- *assignments* (`lv = v`): illegal if there is an in-scope restriction\n-  against mutating `lv`;\n-- *moves*: illegal if there is any in-scope restriction on `lv` at all;\n-- *mutable borrows* (`&mut lv`): illegal there is an in-scope restriction\n-  against claiming `lv`;\n-- *immutable borrows* (`&lv`): illegal there is an in-scope restriction\n-  against freezing `lv`.\n-\n-## Formal rules\n-\n-Now that we hopefully have some kind of intuitive feeling for how the\n-borrow checker works, let's look a bit more closely now at the precise\n-conditions that it uses. For simplicity I will ignore const loans.\n-\n-I will present the rules in a modified form of standard inference\n-rules, which looks as follows:\n-\n-```text\n-PREDICATE(X, Y, Z)                  // Rule-Name\n-  Condition 1\n-  Condition 2\n-  Condition 3\n-```\n-\n-The initial line states the predicate that is to be satisfied.  The\n-indented lines indicate the conditions that must be met for the\n-predicate to be satisfied. The right-justified comment states the name\n-of this rule: there are comments in the borrowck source referencing\n-these names, so that you can cross reference to find the actual code\n-that corresponds to the formal rule.\n-\n-### Invariants\n-\n-I want to collect, at a high-level, the invariants the borrow checker\n-maintains. I will give them names and refer to them throughout the\n-text. Together these invariants are crucial for the overall soundness\n-of the system.\n-\n-**Mutability requires uniqueness.** To mutate a path\n-\n-**Unique mutability.** There is only one *usable* mutable path to any\n-given memory at any given time. This implies that when claiming memory\n-with an expression like `p = &mut x`, the compiler must guarantee that\n-the borrowed value `x` can no longer be mutated so long as `p` is\n-live. (This is done via restrictions, read on.)\n-\n-**.**\n-\n-\n-### The `gather_loans` pass\n-\n-We start with the `gather_loans` pass, which walks the AST looking for\n-borrows.  For each borrow, there are three bits of information: the\n-lvalue `LV` being borrowed and the mutability `MQ` and lifetime `LT`\n-of the resulting pointer. Given those, `gather_loans` applies four\n-validity tests:\n-\n-1. `MUTABILITY(LV, MQ)`: The mutability of the reference is\n-compatible with the mutability of `LV` (i.e., not borrowing immutable\n-data as mutable).\n-\n-2. `ALIASABLE(LV, MQ)`: The aliasability of the reference is\n-compatible with the aliasability of `LV`. The goal is to prevent\n-`&mut` borrows of aliasability data.\n-\n-3. `LIFETIME(LV, LT, MQ)`: The lifetime of the borrow does not exceed\n-the lifetime of the value being borrowed.\n-\n-4. `RESTRICTIONS(LV, LT, ACTIONS) = RS`: This pass checks and computes the\n-restrictions to maintain memory safety. These are the restrictions\n-that will go into the final loan. We'll discuss in more detail below.\n-\n-## Checking mutability\n-\n-Checking mutability is fairly straightforward. We just want to prevent\n-immutable data from being borrowed as mutable. Note that it is ok to\n-borrow mutable data as immutable, since that is simply a\n-freeze. Formally we define a predicate `MUTABLE(LV, MQ)` which, if\n-defined, means that \"borrowing `LV` with mutability `MQ` is ok. The\n-Rust code corresponding to this predicate is the function\n-`check_mutability` in `middle::borrowck::gather_loans`.\n-\n-### Checking mutability of variables\n-\n-*Code pointer:* Function `check_mutability()` in `gather_loans/mod.rs`,\n-but also the code in `mem_categorization`.\n-\n-Let's begin with the rules for variables, which state that if a\n-variable is declared as mutable, it may be borrowed any which way, but\n-otherwise the variable must be borrowed as immutable or const:\n-\n-```text\n-MUTABILITY(X, MQ)                   // M-Var-Mut\n-  DECL(X) = mut\n-\n-MUTABILITY(X, MQ)                   // M-Var-Imm\n-  DECL(X) = imm\n-  MQ = imm | const\n-```\n-\n-### Checking mutability of owned content\n-\n-Fields and owned pointers inherit their mutability from\n-their base expressions, so both of their rules basically\n-delegate the check to the base expression `LV`:\n-\n-```text\n-MUTABILITY(LV.f, MQ)                // M-Field\n-  MUTABILITY(LV, MQ)\n-\n-MUTABILITY(*LV, MQ)                 // M-Deref-Unique\n-  TYPE(LV) = Box<Ty>\n-  MUTABILITY(LV, MQ)\n-```\n-\n-### Checking mutability of immutable pointer types\n-\n-Immutable pointer types like `&T` can only\n-be borrowed if MQ is immutable or const:\n-\n-```text\n-MUTABILITY(*LV, MQ)                // M-Deref-Borrowed-Imm\n-  TYPE(LV) = &Ty\n-  MQ == imm | const\n-```\n-\n-### Checking mutability of mutable pointer types\n-\n-`&mut T` can be frozen, so it is acceptable to borrow it as either imm or mut:\n-\n-```text\n-MUTABILITY(*LV, MQ)                 // M-Deref-Borrowed-Mut\n-  TYPE(LV) = &mut Ty\n-```\n-\n-## Checking aliasability\n-\n-The goal of the aliasability check is to ensure that we never permit\n-`&mut` borrows of aliasable data. Formally we define a predicate\n-`ALIASABLE(LV, MQ)` which if defined means that\n-\"borrowing `LV` with mutability `MQ` is ok\". The\n-Rust code corresponding to this predicate is the function\n-`check_aliasability()` in `middle::borrowck::gather_loans`.\n-\n-### Checking aliasability of variables\n-\n-Local variables are never aliasable as they are accessible only within\n-the stack frame.\n-\n-```text\n-    ALIASABLE(X, MQ)                   // M-Var-Mut\n-```\n-\n-### Checking aliasable of owned content\n-\n-Owned content is aliasable if it is found in an aliasable location:\n-\n-```text\n-ALIASABLE(LV.f, MQ)                // M-Field\n-  ALIASABLE(LV, MQ)\n-\n-ALIASABLE(*LV, MQ)                 // M-Deref-Unique\n-  ALIASABLE(LV, MQ)\n-```\n-\n-### Checking mutability of immutable pointer types\n-\n-Immutable pointer types like `&T` are aliasable, and hence can only be\n-borrowed immutably:\n-\n-```text\n-ALIASABLE(*LV, imm)                // M-Deref-Borrowed-Imm\n-  TYPE(LV) = &Ty\n-```\n-\n-### Checking mutability of mutable pointer types\n-\n-`&mut T` can be frozen, so it is acceptable to borrow it as either imm or mut:\n-\n-```text\n-ALIASABLE(*LV, MQ)                 // M-Deref-Borrowed-Mut\n-  TYPE(LV) = &mut Ty\n-```\n-\n-## Checking lifetime\n-\n-These rules aim to ensure that no data is borrowed for a scope that exceeds\n-its lifetime. These two computations wind up being intimately related.\n-Formally, we define a predicate `LIFETIME(LV, LT, MQ)`, which states that\n-\"the lvalue `LV` can be safely borrowed for the lifetime `LT` with mutability\n-`MQ`\". The Rust code corresponding to this predicate is the module\n-`middle::borrowck::gather_loans::lifetime`.\n-\n-### The Scope function\n-\n-Several of the rules refer to a helper function `SCOPE(LV)=LT`.  The\n-`SCOPE(LV)` yields the lifetime `LT` for which the lvalue `LV` is\n-guaranteed to exist, presuming that no mutations occur.\n-\n-The scope of a local variable is the block where it is declared:\n-\n-```text\n-  SCOPE(X) = block where X is declared\n-```\n-\n-The scope of a field is the scope of the struct:\n-\n-```text\n-  SCOPE(LV.f) = SCOPE(LV)\n-```\n-\n-The scope of a unique referent is the scope of the pointer, since\n-(barring mutation or moves) the pointer will not be freed until\n-the pointer itself `LV` goes out of scope:\n-\n-```text\n-  SCOPE(*LV) = SCOPE(LV) if LV has type Box<T>\n-```\n-\n-The scope of a borrowed referent is the scope associated with the\n-pointer.  This is a conservative approximation, since the data that\n-the pointer points at may actually live longer:\n-\n-```text\n-  SCOPE(*LV) = LT if LV has type &'LT T or &'LT mut T\n-```\n-\n-### Checking lifetime of variables\n-\n-The rule for variables states that a variable can only be borrowed a\n-lifetime `LT` that is a subregion of the variable's scope:\n-\n-```text\n-LIFETIME(X, LT, MQ)                 // L-Local\n-  LT <= SCOPE(X)\n-```\n-\n-### Checking lifetime for owned content\n-\n-The lifetime of a field or owned pointer is the same as the lifetime\n-of its owner:\n-\n-```text\n-LIFETIME(LV.f, LT, MQ)              // L-Field\n-  LIFETIME(LV, LT, MQ)\n-\n-LIFETIME(*LV, LT, MQ)               // L-Deref-Send\n-  TYPE(LV) = Box<Ty>\n-  LIFETIME(LV, LT, MQ)\n-```\n-\n-### Checking lifetime for derefs of references\n-\n-References have a lifetime `LT'` associated with them.  The\n-data they point at has been guaranteed to be valid for at least this\n-lifetime. Therefore, the borrow is valid so long as the lifetime `LT`\n-of the borrow is shorter than the lifetime `LT'` of the pointer\n-itself:\n-\n-```text\n-LIFETIME(*LV, LT, MQ)               // L-Deref-Borrowed\n-  TYPE(LV) = &LT' Ty OR &LT' mut Ty\n-  LT <= LT'\n-```\n-\n-## Computing the restrictions\n-\n-The final rules govern the computation of *restrictions*, meaning that\n-we compute the set of actions that will be illegal for the life of the\n-loan. The predicate is written `RESTRICTIONS(LV, LT, ACTIONS) =\n-RESTRICTION*`, which can be read \"in order to prevent `ACTIONS` from\n-occurring on `LV`, the restrictions `RESTRICTION*` must be respected\n-for the lifetime of the loan\".\n-\n-Note that there is an initial set of restrictions: these restrictions\n-are computed based on the kind of borrow:\n-\n-```text\n-&mut LV =>   RESTRICTIONS(LV, LT, MUTATE|CLAIM|FREEZE)\n-&LV =>       RESTRICTIONS(LV, LT, MUTATE|CLAIM)\n-&const LV => RESTRICTIONS(LV, LT, [])\n-```\n-\n-The reasoning here is that a mutable borrow must be the only writer,\n-therefore it prevents other writes (`MUTATE`), mutable borrows\n-(`CLAIM`), and immutable borrows (`FREEZE`). An immutable borrow\n-permits other immutable borrows but forbids writes and mutable borrows.\n-Finally, a const borrow just wants to be sure that the value is not\n-moved out from under it, so no actions are forbidden.\n-\n-### Restrictions for loans of a local variable\n-\n-The simplest case is a borrow of a local variable `X`:\n-\n-```text\n-RESTRICTIONS(X, LT, ACTIONS) = (X, ACTIONS)            // R-Variable\n-```\n-\n-In such cases we just record the actions that are not permitted.\n-\n-### Restrictions for loans of fields\n-\n-Restricting a field is the same as restricting the owner of that\n-field:\n-\n-```text\n-RESTRICTIONS(LV.f, LT, ACTIONS) = RS, (LV.f, ACTIONS)  // R-Field\n-  RESTRICTIONS(LV, LT, ACTIONS) = RS\n-```\n-\n-The reasoning here is as follows. If the field must not be mutated,\n-then you must not mutate the owner of the field either, since that\n-would indirectly modify the field. Similarly, if the field cannot be\n-frozen or aliased, we cannot allow the owner to be frozen or aliased,\n-since doing so indirectly freezes/aliases the field. This is the\n-origin of inherited mutability.\n-\n-### Restrictions for loans of owned referents\n-\n-Because the mutability of owned referents is inherited, restricting an\n-owned referent is similar to restricting a field, in that it implies\n-restrictions on the pointer. However, owned pointers have an important\n-twist: if the owner `LV` is mutated, that causes the owned referent\n-`*LV` to be freed! So whenever an owned referent `*LV` is borrowed, we\n-must prevent the owned pointer `LV` from being mutated, which means\n-that we always add `MUTATE` and `CLAIM` to the restriction set imposed\n-on `LV`:\n-\n-```text\n-RESTRICTIONS(*LV, LT, ACTIONS) = RS, (*LV, ACTIONS)    // R-Deref-Send-Pointer\n-  TYPE(LV) = Box<Ty>\n-  RESTRICTIONS(LV, LT, ACTIONS|MUTATE|CLAIM) = RS\n-```\n-\n-### Restrictions for loans of immutable borrowed referents\n-\n-Immutable borrowed referents are freely aliasable, meaning that\n-the compiler does not prevent you from copying the pointer.  This\n-implies that issuing restrictions is useless. We might prevent the\n-user from acting on `*LV` itself, but there could be another path\n-`*LV1` that refers to the exact same memory, and we would not be\n-restricting that path. Therefore, the rule for `&Ty` pointers\n-always returns an empty set of restrictions, and it only permits\n-restricting `MUTATE` and `CLAIM` actions:\n-\n-```text\n-RESTRICTIONS(*LV, LT, ACTIONS) = []                    // R-Deref-Imm-Borrowed\n-  TYPE(LV) = &LT' Ty\n-  LT <= LT'                                            // (1)\n-  ACTIONS subset of [MUTATE, CLAIM]\n-```\n-\n-The reason that we can restrict `MUTATE` and `CLAIM` actions even\n-without a restrictions list is that it is never legal to mutate nor to\n-borrow mutably the contents of a `&Ty` pointer. In other words,\n-those restrictions are already inherent in the type.\n-\n-Clause (1) in the rule for `&Ty` deserves mention. Here I\n-specify that the lifetime of the loan must be less than the lifetime\n-of the `&Ty` pointer. In simple cases, this clause is redundant, since\n-the `LIFETIME()` function will already enforce the required rule:\n-\n-```\n-fn foo(point: &'a Point) -> &'static f32 {\n-    &point.x // Error\n-}\n-```\n-\n-The above example fails to compile both because of clause (1) above\n-but also by the basic `LIFETIME()` check. However, in more advanced\n-examples involving multiple nested pointers, clause (1) is needed:\n-\n-```\n-fn foo(point: &'a &'b mut Point) -> &'b f32 {\n-    &point.x // Error\n-}\n-```\n-\n-The `LIFETIME` rule here would accept `'b` because, in fact, the\n-*memory is* guaranteed to remain valid (i.e., not be freed) for the\n-lifetime `'b`, since the `&mut` pointer is valid for `'b`. However, we\n-are returning an immutable reference, so we need the memory to be both\n-valid and immutable. Even though `point.x` is referenced by an `&mut`\n-pointer, it can still be considered immutable so long as that `&mut`\n-pointer is found in an aliased location. That means the memory is\n-guaranteed to be *immutable* for the lifetime of the `&` pointer,\n-which is only `'a`, not `'b`. Hence this example yields an error.\n-\n-As a final twist, consider the case of two nested *immutable*\n-pointers, rather than a mutable pointer within an immutable one:\n-\n-```\n-fn foo(point: &'a &'b Point) -> &'b f32 {\n-    &point.x // OK\n-}\n-```\n-\n-This function is legal. The reason for this is that the inner pointer\n-(`*point : &'b Point`) is enough to guarantee the memory is immutable\n-and valid for the lifetime `'b`.  This is reflected in\n-`RESTRICTIONS()` by the fact that we do not recurse (i.e., we impose\n-no restrictions on `LV`, which in this particular case is the pointer\n-`point : &'a &'b Point`).\n-\n-#### Why both `LIFETIME()` and `RESTRICTIONS()`?\n-\n-Given the previous text, it might seem that `LIFETIME` and\n-`RESTRICTIONS` should be folded together into one check, but there is\n-a reason that they are separated. They answer separate concerns.\n-The rules pertaining to `LIFETIME` exist to ensure that we don't\n-create a borrowed pointer that outlives the memory it points at. So\n-`LIFETIME` prevents a function like this:\n-\n-```\n-fn get_1<'a>() -> &'a int {\n-    let x = 1;\n-    &x\n-}\n-```\n-\n-Here we would be returning a pointer into the stack. Clearly bad.\n-\n-However, the `RESTRICTIONS` rules are more concerned with how memory\n-is used. The example above doesn't generate an error according to\n-`RESTRICTIONS` because, for local variables, we don't require that the\n-loan lifetime be a subset of the local variable lifetime. The idea\n-here is that we *can* guarantee that `x` is not (e.g.) mutated for the\n-lifetime `'a`, even though `'a` exceeds the function body and thus\n-involves unknown code in the caller -- after all, `x` ceases to exist\n-after we return and hence the remaining code in `'a` cannot possibly\n-mutate it. This distinction is important for type checking functions\n-like this one:\n-\n-```\n-fn inc_and_get<'a>(p: &'a mut Point) -> &'a int {\n-    p.x += 1;\n-    &p.x\n-}\n-```\n-\n-In this case, we take in a `&mut` and return a frozen borrowed pointer\n-with the same lifetime. So long as the lifetime of the returned value\n-doesn't exceed the lifetime of the `&mut` we receive as input, this is\n-fine, though it may seem surprising at first (it surprised me when I\n-first worked it through). After all, we're guaranteeing that `*p`\n-won't be mutated for the lifetime `'a`, even though we can't \"see\" the\n-entirety of the code during that lifetime, since some of it occurs in\n-our caller. But we *do* know that nobody can mutate `*p` except\n-through `p`. So if we don't mutate `*p` and we don't return `p`, then\n-we know that the right to mutate `*p` has been lost to our caller --\n-in terms of capability, the caller passed in the ability to mutate\n-`*p`, and we never gave it back. (Note that we can't return `p` while\n-`*p` is borrowed since that would be a move of `p`, as `&mut` pointers\n-are affine.)\n-\n-### Restrictions for loans of const aliasable referents\n-\n-Freeze pointers are read-only. There may be `&mut` or `&` aliases, and\n-we can not prevent *anything* but moves in that case. So the\n-`RESTRICTIONS` function is only defined if `ACTIONS` is the empty set.\n-Because moves from a `&const` lvalue are never legal, it is not\n-necessary to add any restrictions at all to the final result.\n-\n-```text\n-    RESTRICTIONS(*LV, LT, []) = []                         // R-Deref-Freeze-Borrowed\n-      TYPE(LV) = &const Ty\n-```\n-\n-### Restrictions for loans of mutable borrowed referents\n-\n-Mutable borrowed pointers are guaranteed to be the only way to mutate\n-their referent. This permits us to take greater license with them; for\n-example, the referent can be frozen simply be ensuring that we do not\n-use the original pointer to perform mutate. Similarly, we can allow\n-the referent to be claimed, so long as the original pointer is unused\n-while the new claimant is live.\n-\n-The rule for mutable borrowed pointers is as follows:\n-\n-```text\n-RESTRICTIONS(*LV, LT, ACTIONS) = RS, (*LV, ACTIONS)    // R-Deref-Mut-Borrowed\n-  TYPE(LV) = &LT' mut Ty\n-  LT <= LT'                                            // (1)\n-  RESTRICTIONS(LV, LT, ACTIONS) = RS                   // (2)\n-```\n-\n-Let's examine the two numbered clauses:\n-\n-Clause (1) specifies that the lifetime of the loan (`LT`) cannot\n-exceed the lifetime of the `&mut` pointer (`LT'`). The reason for this\n-is that the `&mut` pointer is guaranteed to be the only legal way to\n-mutate its referent -- but only for the lifetime `LT'`.  After that\n-lifetime, the loan on the referent expires and hence the data may be\n-modified by its owner again. This implies that we are only able to\n-guarantee that the referent will not be modified or aliased for a\n-maximum of `LT'`.\n-\n-Here is a concrete example of a bug this rule prevents:\n-\n-```\n-// Test region-reborrow-from-shorter-mut-ref.rs:\n-fn copy_pointer<'a,'b,T>(x: &'a mut &'b mut T) -> &'b mut T {\n-    &mut **p // ERROR due to clause (1)\n-}\n-fn main() {\n-    let mut x = 1;\n-    let mut y = &mut x; // <-'b-----------------------------+\n-    //      +-'a--------------------+                       |\n-    //      v                       v                       |\n-    let z = copy_borrowed_ptr(&mut y); // y is lent         |\n-    *y += 1; // Here y==z, so both should not be usable...  |\n-    *z += 1; // ...and yet they would be, but for clause 1. |\n-} // <------------------------------------------------------+\n-```\n-\n-Clause (2) propagates the restrictions on the referent to the pointer\n-itself. This is the same as with an owned pointer, though the\n-reasoning is mildly different. The basic goal in all cases is to\n-prevent the user from establishing another route to the same data. To\n-see what I mean, let's examine various cases of what can go wrong and\n-show how it is prevented.\n-\n-**Example danger 1: Moving the base pointer.** One of the simplest\n-ways to violate the rules is to move the base pointer to a new name\n-and access it via that new name, thus bypassing the restrictions on\n-the old name. Here is an example:\n-\n-```\n-// src/test/compile-fail/borrowck-move-mut-base-ptr.rs\n-fn foo(t0: &mut int) {\n-    let p: &int = &*t0; // Freezes `*t0`\n-    let t1 = t0;        //~ ERROR cannot move out of `t0`\n-    *t1 = 22;           // OK, not a write through `*t0`\n-}\n-```\n-\n-Remember that `&mut` pointers are linear, and hence `let t1 = t0` is a\n-move of `t0` -- or would be, if it were legal. Instead, we get an\n-error, because clause (2) imposes restrictions on `LV` (`t0`, here),\n-and any restrictions on a path make it impossible to move from that\n-path.\n-\n-**Example danger 2: Claiming the base pointer.** Another possible\n-danger is to mutably borrow the base path. This can lead to two bad\n-scenarios. The most obvious is that the mutable borrow itself becomes\n-another path to access the same data, as shown here:\n-\n-```\n-// src/test/compile-fail/borrowck-mut-borrow-of-mut-base-ptr.rs\n-fn foo<'a>(mut t0: &'a mut int,\n-           mut t1: &'a mut int) {\n-    let p: &int = &*t0;     // Freezes `*t0`\n-    let mut t2 = &mut t0;   //~ ERROR cannot borrow `t0`\n-    **t2 += 1;              // Mutates `*t0`\n-}\n-```\n-\n-In this example, `**t2` is the same memory as `*t0`. Because `t2` is\n-an `&mut` pointer, `**t2` is a unique path and hence it would be\n-possible to mutate `**t2` even though that memory was supposed to be\n-frozen by the creation of `p`. However, an error is reported -- the\n-reason is that the freeze `&*t0` will restrict claims and mutation\n-against `*t0` which, by clause 2, in turn prevents claims and mutation\n-of `t0`. Hence the claim `&mut t0` is illegal.\n-\n-Another danger with an `&mut` pointer is that we could swap the `t0`\n-value away to create a new path:\n-\n-```\n-// src/test/compile-fail/borrowck-swap-mut-base-ptr.rs\n-fn foo<'a>(mut t0: &'a mut int,\n-           mut t1: &'a mut int) {\n-    let p: &int = &*t0;     // Freezes `*t0`\n-    swap(&mut t0, &mut t1); //~ ERROR cannot borrow `t0`\n-    *t1 = 22;\n-}\n-```\n-\n-This is illegal for the same reason as above. Note that if we added\n-back a swap operator -- as we used to have -- we would want to be very\n-careful to ensure this example is still illegal.\n-\n-**Example danger 3: Freeze the base pointer.** In the case where the\n-referent is claimed, even freezing the base pointer can be dangerous,\n-as shown in the following example:\n-\n-```\n-// src/test/compile-fail/borrowck-borrow-of-mut-base-ptr.rs\n-fn foo<'a>(mut t0: &'a mut int,\n-           mut t1: &'a mut int) {\n-    let p: &mut int = &mut *t0; // Claims `*t0`\n-    let mut t2 = &t0;           //~ ERROR cannot borrow `t0`\n-    let q: &int = &*t2;         // Freezes `*t0` but not through `*p`\n-    *p += 1;                    // violates type of `*q`\n-}\n-```\n-\n-Here the problem is that `*t0` is claimed by `p`, and hence `p` wants\n-to be the controlling pointer through which mutation or freezes occur.\n-But `t2` would -- if it were legal -- have the type `& &mut int`, and\n-hence would be a mutable pointer in an aliasable location, which is\n-considered frozen (since no one can write to `**t2` as it is not a\n-unique path). Therefore, we could reasonably create a frozen `&int`\n-pointer pointing at `*t0` that coexists with the mutable pointer `p`,\n-which is clearly unsound.\n-\n-However, it is not always unsafe to freeze the base pointer. In\n-particular, if the referent is frozen, there is no harm in it:\n-\n-```\n-// src/test/run-pass/borrowck-borrow-of-mut-base-ptr-safe.rs\n-fn foo<'a>(mut t0: &'a mut int,\n-           mut t1: &'a mut int) {\n-    let p: &int = &*t0; // Freezes `*t0`\n-    let mut t2 = &t0;\n-    let q: &int = &*t2; // Freezes `*t0`, but that's ok...\n-    let r: &int = &*t0; // ...after all, could do same thing directly.\n-}\n-```\n-\n-In this case, creating the alias `t2` of `t0` is safe because the only\n-thing `t2` can be used for is to further freeze `*t0`, which is\n-already frozen. In particular, we cannot assign to `*t0` through the\n-new alias `t2`, as demonstrated in this test case:\n-\n-```\n-// src/test/run-pass/borrowck-borrow-mut-base-ptr-in-aliasable-loc.rs\n-fn foo(t0: & &mut int) {\n-    let t1 = t0;\n-    let p: &int = &**t0;\n-    **t1 = 22; //~ ERROR cannot assign\n-}\n-```\n-\n-This distinction is reflected in the rules. When doing an `&mut`\n-borrow -- as in the first example -- the set `ACTIONS` will be\n-`CLAIM|MUTATE|FREEZE`, because claiming the referent implies that it\n-cannot be claimed, mutated, or frozen by anyone else. These\n-restrictions are propagated back to the base path and hence the base\n-path is considered unfreezable.\n-\n-In contrast, when the referent is merely frozen -- as in the second\n-example -- the set `ACTIONS` will be `CLAIM|MUTATE`, because freezing\n-the referent implies that it cannot be claimed or mutated but permits\n-others to freeze. Hence when these restrictions are propagated back to\n-the base path, it will still be considered freezable.\n-\n-\n-\n-**FIXME #10520: Restrictions against mutating the base pointer.** When\n-an `&mut` pointer is frozen or claimed, we currently pass along the\n-restriction against MUTATE to the base pointer. I do not believe this\n-restriction is needed. It dates from the days when we had a way to\n-mutate that preserved the value being mutated (i.e., swap). Nowadays\n-the only form of mutation is assignment, which destroys the pointer\n-being mutated -- therefore, a mutation cannot create a new path to the\n-same data. Rather, it removes an existing path. This implies that not\n-only can we permit mutation, we can have mutation kill restrictions in\n-the dataflow sense.\n-\n-**WARNING:** We do not currently have `const` borrows in the\n-language. If they are added back in, we must ensure that they are\n-consistent with all of these examples. The crucial question will be\n-what sorts of actions are permitted with a `&const &mut` pointer. I\n-would suggest that an `&mut` referent found in an `&const` location be\n-prohibited from both freezes and claims. This would avoid the need to\n-prevent `const` borrows of the base pointer when the referent is\n-borrowed.\n-\n-# Moves and initialization\n-\n-The borrow checker is also in charge of ensuring that:\n-\n-- all memory which is accessed is initialized\n-- immutable local variables are assigned at most once.\n-\n-These are two separate dataflow analyses built on the same\n-framework. Let's look at checking that memory is initialized first;\n-the checking of immutable local variable assignments works in a very\n-similar way.\n-\n-To track the initialization of memory, we actually track all the\n-points in the program that *create uninitialized memory*, meaning\n-moves and the declaration of uninitialized variables. For each of\n-these points, we create a bit in the dataflow set. Assignments to a\n-variable `x` or path `a.b.c` kill the move/uninitialization bits for\n-those paths and any subpaths (e.g., `x`, `x.y`, `a.b.c`, `*a.b.c`).\n-Bits are unioned when two control-flow paths join. Thus, the\n-presence of a bit indicates that the move may have occurred without an\n-intervening assignment to the same memory. At each use of a variable,\n-we examine the bits in scope, and check that none of them are\n-moves/uninitializations of the variable that is being used.\n-\n-Let's look at a simple example:\n-\n-```\n-fn foo(a: Box<int>) {\n-    let b: Box<int>;   // Gen bit 0.\n-\n-    if cond {          // Bits: 0\n-        use(&*a);\n-        b = a;         // Gen bit 1, kill bit 0.\n-        use(&*b);\n-    } else {\n-                       // Bits: 0\n-    }\n-                       // Bits: 0,1\n-    use(&*a);          // Error.\n-    use(&*b);          // Error.\n-}\n-\n-fn use(a: &int) { }\n-```\n-\n-In this example, the variable `b` is created uninitialized. In one\n-branch of an `if`, we then move the variable `a` into `b`. Once we\n-exit the `if`, therefore, it is an error to use `a` or `b` since both\n-are only conditionally initialized. I have annotated the dataflow\n-state using comments. There are two dataflow bits, with bit 0\n-corresponding to the creation of `b` without an initializer, and bit 1\n-corresponding to the move of `a`. The assignment `b = a` both\n-generates bit 1, because it is a move of `a`, and kills bit 0, because\n-`b` is now initialized. On the else branch, though, `b` is never\n-initialized, and so bit 0 remains untouched. When the two flows of\n-control join, we union the bits from both sides, resulting in both\n-bits 0 and 1 being set. Thus any attempt to use `a` uncovers the bit 1\n-from the \"then\" branch, showing that `a` may be moved, and any attempt\n-to use `b` uncovers bit 0, from the \"else\" branch, showing that `b`\n-may not be initialized.\n-\n-## Initialization of immutable variables\n-\n-Initialization of immutable variables works in a very similar way,\n-except that:\n-\n-1. we generate bits for each assignment to a variable;\n-2. the bits are never killed except when the variable goes out of scope.\n-\n-Thus the presence of an assignment bit indicates that the assignment\n-may have occurred. Note that assignments are only killed when the\n-variable goes out of scope, as it is not relevant whether or not there\n-has been a move in the meantime. Using these bits, we can declare that\n-an assignment to an immutable variable is legal iff there is no other\n-assignment bit to that same variable in scope.\n-\n-## Why is the design made this way?\n-\n-It may seem surprising that we assign dataflow bits to *each move*\n-rather than *each path being moved*. This is somewhat less efficient,\n-since on each use, we must iterate through all moves and check whether\n-any of them correspond to the path in question. Similar concerns apply\n-to the analysis for double assignments to immutable variables. The\n-main reason to do it this way is that it allows us to print better\n-error messages, because when a use occurs, we can print out the\n-precise move that may be in scope, rather than simply having to say\n-\"the variable may not be initialized\".\n-\n-## Data structures used in the move analysis\n-\n-The move analysis maintains several data structures that enable it to\n-cross-reference moves and assignments to determine when they may be\n-moving/assigning the same memory. These are all collected into the\n-`MoveData` and `FlowedMoveData` structs. The former represents the set\n-of move paths, moves, and assignments, and the latter adds in the\n-results of a dataflow computation.\n-\n-### Move paths\n-\n-The `MovePath` tree tracks every path that is moved or assigned to.\n-These paths have the same form as the `LoanPath` data structure, which\n-in turn is the \"real world version of the lvalues `LV` that we\n-introduced earlier. The difference between a `MovePath` and a `LoanPath`\n-is that move paths are:\n-\n-1. Canonicalized, so that we have exactly one copy of each, and\n-   we can refer to move paths by index;\n-2. Cross-referenced with other paths into a tree, so that given a move\n-   path we can efficiently find all parent move paths and all\n-   extensions (e.g., given the `a.b` move path, we can easily find the\n-   move path `a` and also the move paths `a.b.c`)\n-3. Cross-referenced with moves and assignments, so that we can\n-   easily find all moves and assignments to a given path.\n-\n-The mechanism that we use is to create a `MovePath` record for each\n-move path. These are arranged in an array and are referenced using\n-`MovePathIndex` values, which are newtype'd indices. The `MovePath`\n-structs are arranged into a tree, representing using the standard\n-Knuth representation where each node has a child 'pointer' and a \"next\n-sibling\" 'pointer'. In addition, each `MovePath` has a parent\n-'pointer'.  In this case, the 'pointers' are just `MovePathIndex`\n-values.\n-\n-In this way, if we want to find all base paths of a given move path,\n-we can just iterate up the parent pointers (see `each_base_path()` in\n-the `move_data` module). If we want to find all extensions, we can\n-iterate through the subtree (see `each_extending_path()`).\n-\n-### Moves and assignments\n-\n-There are structs to represent moves (`Move`) and assignments\n-(`Assignment`), and these are also placed into arrays and referenced\n-by index. All moves of a particular path are arranged into a linked\n-lists, beginning with `MovePath.first_move` and continuing through\n-`Move.next_move`.\n-\n-We distinguish between \"var\" assignments, which are assignments to a\n-variable like `x = foo`, and \"path\" assignments (`x.f = foo`).  This\n-is because we need to assign dataflows to the former, but not the\n-latter, so as to check for double initialization of immutable\n-variables.\n-\n-### Gathering and checking moves\n-\n-Like loans, we distinguish two phases. The first, gathering, is where\n-we uncover all the moves and assignments. As with loans, we do some\n-basic sanity checking in this phase, so we'll report errors if you\n-attempt to move out of a borrowed pointer etc. Then we do the dataflow\n-(see `FlowedMoveData::new`). Finally, in the `check_loans.rs` code, we\n-walk back over, identify all uses, assignments, and captures, and\n-check that they are legal given the set of dataflow bits we have\n-computed for that program point.\n-\n-# Drop flags and structural fragments\n-\n-In addition to the job of enforcing memory safety, the borrow checker\n-code is also responsible for identifying the *structural fragments* of\n-data in the function, to support out-of-band dynamic drop flags\n-allocated on the stack. (For background, see [RFC PR #320].)\n-\n-[RFC PR #320]: https://github.com/rust-lang/rfcs/pull/320\n-\n-Semantically, each piece of data that has a destructor may need a\n-boolean flag to indicate whether or not its destructor has been run\n-yet. However, in many cases there is no need to actually maintain such\n-a flag: It can be apparent from the code itself that a given path is\n-always initialized (or always deinitialized) when control reaches the\n-end of its owner's scope, and thus we can unconditionally emit (or\n-not) the destructor invocation for that path.\n-\n-A simple example of this is the following:\n-\n-```rust\n-struct D { p: int }\n-impl D { fn new(x: int) -> D { ... }\n-impl Drop for D { ... }\n-\n-fn foo(a: D, b: D, t: || -> bool) {\n-    let c: D;\n-    let d: D;\n-    if t() { c = b; }\n-}\n-```\n-\n-At the end of the body of `foo`, the compiler knows that `a` is\n-initialized, introducing a drop obligation (deallocating the boxed\n-integer) for the end of `a`'s scope that is run unconditionally.\n-Likewise the compiler knows that `d` is not initialized, and thus it\n-leave out the drop code for `d`.\n-\n-The compiler cannot statically know the drop-state of `b` nor `c` at\n-the end of their scope, since that depends on the value of\n-`t`. Therefore, we need to insert boolean flags to track whether we\n-need to drop `b` and `c`.\n-\n-However, the matter is not as simple as just mapping local variables\n-to their corresponding drop flags when necessary. In particular, in\n-addition to being able to move data out of local variables, Rust\n-allows one to move values in and out of structured data.\n-\n-Consider the following:\n-\n-```rust\n-struct S { x: D, y: D, z: D }\n-\n-fn foo(a: S, mut b: S, t: || -> bool) {\n-    let mut c: S;\n-    let d: S;\n-    let e: S = a.clone();\n-    if t() {\n-        c = b;\n-        b.x = e.y;\n-    }\n-    if t() { c.y = D::new(4); }\n-}\n-```\n-\n-As before, the drop obligations of `a` and `d` can be statically\n-determined, and again the state of `b` and `c` depend on dynamic\n-state. But additionally, the dynamic drop obligations introduced by\n-`b` and `c` are not just per-local boolean flags. For example, if the\n-first call to `t` returns `false` and the second call `true`, then at\n-the end of their scope, `b` will be completely initialized, but only\n-`c.y` in `c` will be initialized.  If both calls to `t` return `true`,\n-then at the end of their scope, `c` will be completely initialized,\n-but only `b.x` will be initialized in `b`, and only `e.x` and `e.z`\n-will be initialized in `e`.\n-\n-Note that we need to cover the `z` field in each case in some way,\n-since it may (or may not) need to be dropped, even though `z` is never\n-directly mentioned in the body of the `foo` function. We call a path\n-like `b.z` a *fragment sibling* of `b.x`, since the field `z` comes\n-from the same structure `S` that declared the field `x` in `b.x`.\n-\n-In general we need to maintain boolean flags that match the\n-`S`-structure of both `b` and `c`.  In addition, we need to consult\n-such a flag when doing an assignment (such as `c.y = D::new(4);`\n-above), in order to know whether or not there is a previous value that\n-needs to be dropped before we do the assignment.\n-\n-So for any given function, we need to determine what flags are needed\n-to track its drop obligations. Our strategy for determining the set of\n-flags is to represent the fragmentation of the structure explicitly:\n-by starting initially from the paths that are explicitly mentioned in\n-moves and assignments (such as `b.x` and `c.y` above), and then\n-traversing the structure of the path's type to identify leftover\n-*unmoved fragments*: assigning into `c.y` means that `c.x` and `c.z`\n-are leftover unmoved fragments. Each fragment represents a drop\n-obligation that may need to be tracked. Paths that are only moved or\n-assigned in their entirety (like `a` and `d`) are treated as a single\n-drop obligation.\n-\n-The fragment construction process works by piggy-backing on the\n-existing `move_data` module. We already have callbacks that visit each\n-direct move and assignment; these form the basis for the sets of\n-moved_leaf_paths and assigned_leaf_paths. From these leaves, we can\n-walk up their parent chain to identify all of their parent paths.\n-We need to identify the parents because of cases like the following:\n-\n-```rust\n-struct Pair<X,Y>{ x: X, y: Y }\n-fn foo(dd_d_d: Pair<Pair<Pair<D, D>, D>, D>) {\n-    other_function(dd_d_d.x.y);\n-}\n-```\n-\n-In this code, the move of the path `dd_d.x.y` leaves behind not only\n-the fragment drop-obligation `dd_d.x.x` but also `dd_d.y` as well.\n-\n-Once we have identified the directly-referenced leaves and their\n-parents, we compute the left-over fragments, in the function\n-`fragments::add_fragment_siblings`. As of this writing this works by\n-looking at each directly-moved or assigned path P, and blindly\n-gathering all sibling fields of P (as well as siblings for the parents\n-of P, etc). After accumulating all such siblings, we filter out the\n-entries added as siblings of P that turned out to be\n-directly-referenced paths (or parents of directly referenced paths)\n-themselves, thus leaving the never-referenced \"left-overs\" as the only\n-thing left from the gathering step.\n-\n-## Array structural fragments\n-\n-A special case of the structural fragments discussed above are\n-the elements of an array that has been passed by value, such as\n-the following:\n-\n-```rust\n-fn foo(a: [D, ..10], i: uint) -> D {\n-    a[i]\n-}\n-```\n-\n-The above code moves a single element out of the input array `a`.\n-The remainder of the array still needs to be dropped; i.e., it\n-is a structural fragment. Note that after performing such a move,\n-it is not legal to read from the array `a`. There are a number of\n-ways to deal with this, but the important thing to note is that\n-the semantics needs to distinguish in some manner between a\n-fragment that is the *entire* array versus a fragment that represents\n-all-but-one element of the array.  A place where that distinction\n-would arise is the following:\n-\n-```rust\n-fn foo(a: [D, ..10], b: [D, ..10], i: uint, t: bool) -> D {\n-    if t {\n-        a[i]\n-    } else {\n-        b[i]\n-    }\n-\n-    // When control exits, we will need either to drop all of `a`\n-    // and all-but-one of `b`, or to drop all of `b` and all-but-one\n-    // of `a`.\n-}\n-```\n-\n-There are a number of ways that the trans backend could choose to\n-compile this (e.g. a `[bool, ..10]` array for each such moved array;\n-or an `Option<uint>` for each moved array).  From the viewpoint of the\n-borrow-checker, the important thing is to record what kind of fragment\n-is implied by the relevant moves.\n-\n-# Future work\n-\n-While writing up these docs, I encountered some rules I believe to be\n-stricter than necessary:\n-\n-- I think restricting the `&mut` LV against moves and `ALIAS` is sufficient,\n-  `MUTATE` and `CLAIM` are overkill. `MUTATE` was necessary when swap was\n-  a built-in operator, but as it is not, it is implied by `CLAIM`,\n-  and `CLAIM` is implied by `ALIAS`. The only net effect of this is an\n-  extra error message in some cases, though.\n-- I have not described how closures interact. Current code is unsound.\n-  I am working on describing and implementing the fix.\n-- If we wish, we can easily extend the move checking to allow finer-grained\n-  tracking of what is initialized and what is not, enabling code like\n-  this:\n-\n-      a = x.f.g; // x.f.g is now uninitialized\n-      // here, x and x.f are not usable, but x.f.h *is*\n-      x.f.g = b; // x.f.g is not initialized\n-      // now x, x.f, x.f.g, x.f.h are all usable\n-\n-  What needs to change here, most likely, is that the `moves` module\n-  should record not only what paths are moved, but what expressions\n-  are actual *uses*. For example, the reference to `x` in `x.f.g = b`\n-  is not a true *use* in the sense that it requires `x` to be fully\n-  initialized. This is in fact why the above code produces an error\n-  today: the reference to `x` in `x.f.g = b` is considered illegal\n-  because `x` is not fully initialized.\n-\n-There are also some possible refactorings:\n-\n-- It might be nice to replace all loan paths with the MovePath mechanism,\n-  since they allow lightweight comparison using an integer.\n-\n-*/\n+//! # The Borrow Checker\n+//!\n+//! This pass has the job of enforcing memory safety. This is a subtle\n+//! topic. This docs aim to explain both the practice and the theory\n+//! behind the borrow checker. They start with a high-level overview of\n+//! how it works, and then proceed to dive into the theoretical\n+//! background. Finally, they go into detail on some of the more subtle\n+//! aspects.\n+//!\n+//! # Table of contents\n+//!\n+//! These docs are long. Search for the section you are interested in.\n+//!\n+//! - Overview\n+//! - Formal model\n+//! - Borrowing and loans\n+//! - Moves and initialization\n+//! - Drop flags and structural fragments\n+//! - Future work\n+//!\n+//! # Overview\n+//!\n+//! The borrow checker checks one function at a time. It operates in two\n+//! passes. The first pass, called `gather_loans`, walks over the function\n+//! and identifies all of the places where borrows (e.g., `&` expressions\n+//! and `ref` bindings) and moves (copies or captures of a linear value)\n+//! occur. It also tracks initialization sites. For each borrow and move,\n+//! it checks various basic safety conditions at this time (for example,\n+//! that the lifetime of the borrow doesn't exceed the lifetime of the\n+//! value being borrowed, or that there is no move out of an `&T`\n+//! referent).\n+//!\n+//! It then uses the dataflow module to propagate which of those borrows\n+//! may be in scope at each point in the procedure. A loan is considered\n+//! to come into scope at the expression that caused it and to go out of\n+//! scope when the lifetime of the resulting reference expires.\n+//!\n+//! Once the in-scope loans are known for each point in the program, the\n+//! borrow checker walks the IR again in a second pass called\n+//! `check_loans`. This pass examines each statement and makes sure that\n+//! it is safe with respect to the in-scope loans.\n+//!\n+//! # Formal model\n+//!\n+//! Throughout the docs we'll consider a simple subset of Rust in which\n+//! you can only borrow from lvalues, defined like so:\n+//!\n+//! ```text\n+//! LV = x | LV.f | *LV\n+//! ```\n+//!\n+//! Here `x` represents some variable, `LV.f` is a field reference,\n+//! and `*LV` is a pointer dereference. There is no auto-deref or other\n+//! niceties. This means that if you have a type like:\n+//!\n+//! ```text\n+//! struct S { f: uint }\n+//! ```\n+//!\n+//! and a variable `a: Box<S>`, then the rust expression `a.f` would correspond\n+//! to an `LV` of `(*a).f`.\n+//!\n+//! Here is the formal grammar for the types we'll consider:\n+//!\n+//! ```text\n+//! TY = () | S<'LT...> | Box<TY> | & 'LT MQ TY\n+//! MQ = mut | imm | const\n+//! ```\n+//!\n+//! Most of these types should be pretty self explanatory. Here `S` is a\n+//! struct name and we assume structs are declared like so:\n+//!\n+//! ```text\n+//! SD = struct S<'LT...> { (f: TY)... }\n+//! ```\n+//!\n+//! # Borrowing and loans\n+//!\n+//! ## An intuitive explanation\n+//!\n+//! ### Issuing loans\n+//!\n+//! Now, imagine we had a program like this:\n+//!\n+//! ```text\n+//! struct Foo { f: uint, g: uint }\n+//! ...\n+//! 'a: {\n+//!   let mut x: Box<Foo> = ...;\n+//!   let y = &mut (*x).f;\n+//!   x = ...;\n+//! }\n+//! ```\n+//!\n+//! This is of course dangerous because mutating `x` will free the old\n+//! value and hence invalidate `y`. The borrow checker aims to prevent\n+//! this sort of thing.\n+//!\n+//! #### Loans and restrictions\n+//!\n+//! The way the borrow checker works is that it analyzes each borrow\n+//! expression (in our simple model, that's stuff like `&LV`, though in\n+//! real life there are a few other cases to consider). For each borrow\n+//! expression, it computes a `Loan`, which is a data structure that\n+//! records (1) the value being borrowed, (2) the mutability and scope of\n+//! the borrow, and (3) a set of restrictions. In the code, `Loan` is a\n+//! struct defined in `middle::borrowck`. Formally, we define `LOAN` as\n+//! follows:\n+//!\n+//! ```text\n+//! LOAN = (LV, LT, MQ, RESTRICTION*)\n+//! RESTRICTION = (LV, ACTION*)\n+//! ACTION = MUTATE | CLAIM | FREEZE\n+//! ```\n+//!\n+//! Here the `LOAN` tuple defines the lvalue `LV` being borrowed; the\n+//! lifetime `LT` of that borrow; the mutability `MQ` of the borrow; and a\n+//! list of restrictions. The restrictions indicate actions which, if\n+//! taken, could invalidate the loan and lead to type safety violations.\n+//!\n+//! Each `RESTRICTION` is a pair of a restrictive lvalue `LV` (which will\n+//! either be the path that was borrowed or some prefix of the path that\n+//! was borrowed) and a set of restricted actions.  There are three kinds\n+//! of actions that may be restricted for the path `LV`:\n+//!\n+//! - `MUTATE` means that `LV` cannot be assigned to;\n+//! - `CLAIM` means that the `LV` cannot be borrowed mutably;\n+//! - `FREEZE` means that the `LV` cannot be borrowed immutably;\n+//!\n+//! Finally, it is never possible to move from an lvalue that appears in a\n+//! restriction. This implies that the \"empty restriction\" `(LV, [])`,\n+//! which contains an empty set of actions, still has a purpose---it\n+//! prevents moves from `LV`. I chose not to make `MOVE` a fourth kind of\n+//! action because that would imply that sometimes moves are permitted\n+//! from restrictived values, which is not the case.\n+//!\n+//! #### Example\n+//!\n+//! To give you a better feeling for what kind of restrictions derived\n+//! from a loan, let's look at the loan `L` that would be issued as a\n+//! result of the borrow `&mut (*x).f` in the example above:\n+//!\n+//! ```text\n+//! L = ((*x).f, 'a, mut, RS) where\n+//!     RS = [((*x).f, [MUTATE, CLAIM, FREEZE]),\n+//!           (*x, [MUTATE, CLAIM, FREEZE]),\n+//!           (x, [MUTATE, CLAIM, FREEZE])]\n+//! ```\n+//!\n+//! The loan states that the expression `(*x).f` has been loaned as\n+//! mutable for the lifetime `'a`. Because the loan is mutable, that means\n+//! that the value `(*x).f` may be mutated via the newly created reference\n+//! (and *only* via that pointer). This is reflected in the\n+//! restrictions `RS` that accompany the loan.\n+//!\n+//! The first restriction `((*x).f, [MUTATE, CLAIM, FREEZE])` states that\n+//! the lender may not mutate, freeze, nor alias `(*x).f`. Mutation is\n+//! illegal because `(*x).f` is only supposed to be mutated via the new\n+//! reference, not by mutating the original path `(*x).f`. Freezing is\n+//! illegal because the path now has an `&mut` alias; so even if we the\n+//! lender were to consider `(*x).f` to be immutable, it might be mutated\n+//! via this alias. They will be enforced for the lifetime `'a` of the\n+//! loan. After the loan expires, the restrictions no longer apply.\n+//!\n+//! The second restriction on `*x` is interesting because it does not\n+//! apply to the path that was lent (`(*x).f`) but rather to a prefix of\n+//! the borrowed path. This is due to the rules of inherited mutability:\n+//! if the user were to assign to (or freeze) `*x`, they would indirectly\n+//! overwrite (or freeze) `(*x).f`, and thus invalidate the reference\n+//! that was created. In general it holds that when a path is\n+//! lent, restrictions are issued for all the owning prefixes of that\n+//! path. In this case, the path `*x` owns the path `(*x).f` and,\n+//! because `x` is an owned pointer, the path `x` owns the path `*x`.\n+//! Therefore, borrowing `(*x).f` yields restrictions on both\n+//! `*x` and `x`.\n+//!\n+//! ### Checking for illegal assignments, moves, and reborrows\n+//!\n+//! Once we have computed the loans introduced by each borrow, the borrow\n+//! checker uses a data flow propagation to compute the full set of loans\n+//! in scope at each expression and then uses that set to decide whether\n+//! that expression is legal.  Remember that the scope of loan is defined\n+//! by its lifetime LT.  We sometimes say that a loan which is in-scope at\n+//! a particular point is an \"outstanding loan\", and the set of\n+//! restrictions included in those loans as the \"outstanding\n+//! restrictions\".\n+//!\n+//! The kinds of expressions which in-scope loans can render illegal are:\n+//! - *assignments* (`lv = v`): illegal if there is an in-scope restriction\n+//!   against mutating `lv`;\n+//! - *moves*: illegal if there is any in-scope restriction on `lv` at all;\n+//! - *mutable borrows* (`&mut lv`): illegal there is an in-scope restriction\n+//!   against claiming `lv`;\n+//! - *immutable borrows* (`&lv`): illegal there is an in-scope restriction\n+//!   against freezing `lv`.\n+//!\n+//! ## Formal rules\n+//!\n+//! Now that we hopefully have some kind of intuitive feeling for how the\n+//! borrow checker works, let's look a bit more closely now at the precise\n+//! conditions that it uses. For simplicity I will ignore const loans.\n+//!\n+//! I will present the rules in a modified form of standard inference\n+//! rules, which looks as follows:\n+//!\n+//! ```text\n+//! PREDICATE(X, Y, Z)                  // Rule-Name\n+//!   Condition 1\n+//!   Condition 2\n+//!   Condition 3\n+//! ```\n+//!\n+//! The initial line states the predicate that is to be satisfied.  The\n+//! indented lines indicate the conditions that must be met for the\n+//! predicate to be satisfied. The right-justified comment states the name\n+//! of this rule: there are comments in the borrowck source referencing\n+//! these names, so that you can cross reference to find the actual code\n+//! that corresponds to the formal rule.\n+//!\n+//! ### Invariants\n+//!\n+//! I want to collect, at a high-level, the invariants the borrow checker\n+//! maintains. I will give them names and refer to them throughout the\n+//! text. Together these invariants are crucial for the overall soundness\n+//! of the system.\n+//!\n+//! **Mutability requires uniqueness.** To mutate a path\n+//!\n+//! **Unique mutability.** There is only one *usable* mutable path to any\n+//! given memory at any given time. This implies that when claiming memory\n+//! with an expression like `p = &mut x`, the compiler must guarantee that\n+//! the borrowed value `x` can no longer be mutated so long as `p` is\n+//! live. (This is done via restrictions, read on.)\n+//!\n+//! **.**\n+//!\n+//!\n+//! ### The `gather_loans` pass\n+//!\n+//! We start with the `gather_loans` pass, which walks the AST looking for\n+//! borrows.  For each borrow, there are three bits of information: the\n+//! lvalue `LV` being borrowed and the mutability `MQ` and lifetime `LT`\n+//! of the resulting pointer. Given those, `gather_loans` applies four\n+//! validity tests:\n+//!\n+//! 1. `MUTABILITY(LV, MQ)`: The mutability of the reference is\n+//! compatible with the mutability of `LV` (i.e., not borrowing immutable\n+//! data as mutable).\n+//!\n+//! 2. `ALIASABLE(LV, MQ)`: The aliasability of the reference is\n+//! compatible with the aliasability of `LV`. The goal is to prevent\n+//! `&mut` borrows of aliasability data.\n+//!\n+//! 3. `LIFETIME(LV, LT, MQ)`: The lifetime of the borrow does not exceed\n+//! the lifetime of the value being borrowed.\n+//!\n+//! 4. `RESTRICTIONS(LV, LT, ACTIONS) = RS`: This pass checks and computes the\n+//! restrictions to maintain memory safety. These are the restrictions\n+//! that will go into the final loan. We'll discuss in more detail below.\n+//!\n+//! ## Checking mutability\n+//!\n+//! Checking mutability is fairly straightforward. We just want to prevent\n+//! immutable data from being borrowed as mutable. Note that it is ok to\n+//! borrow mutable data as immutable, since that is simply a\n+//! freeze. Formally we define a predicate `MUTABLE(LV, MQ)` which, if\n+//! defined, means that \"borrowing `LV` with mutability `MQ` is ok. The\n+//! Rust code corresponding to this predicate is the function\n+//! `check_mutability` in `middle::borrowck::gather_loans`.\n+//!\n+//! ### Checking mutability of variables\n+//!\n+//! *Code pointer:* Function `check_mutability()` in `gather_loans/mod.rs`,\n+//! but also the code in `mem_categorization`.\n+//!\n+//! Let's begin with the rules for variables, which state that if a\n+//! variable is declared as mutable, it may be borrowed any which way, but\n+//! otherwise the variable must be borrowed as immutable or const:\n+//!\n+//! ```text\n+//! MUTABILITY(X, MQ)                   // M-Var-Mut\n+//!   DECL(X) = mut\n+//!\n+//! MUTABILITY(X, MQ)                   // M-Var-Imm\n+//!   DECL(X) = imm\n+//!   MQ = imm | const\n+//! ```\n+//!\n+//! ### Checking mutability of owned content\n+//!\n+//! Fields and owned pointers inherit their mutability from\n+//! their base expressions, so both of their rules basically\n+//! delegate the check to the base expression `LV`:\n+//!\n+//! ```text\n+//! MUTABILITY(LV.f, MQ)                // M-Field\n+//!   MUTABILITY(LV, MQ)\n+//!\n+//! MUTABILITY(*LV, MQ)                 // M-Deref-Unique\n+//!   TYPE(LV) = Box<Ty>\n+//!   MUTABILITY(LV, MQ)\n+//! ```\n+//!\n+//! ### Checking mutability of immutable pointer types\n+//!\n+//! Immutable pointer types like `&T` can only\n+//! be borrowed if MQ is immutable or const:\n+//!\n+//! ```text\n+//! MUTABILITY(*LV, MQ)                // M-Deref-Borrowed-Imm\n+//!   TYPE(LV) = &Ty\n+//!   MQ == imm | const\n+//! ```\n+//!\n+//! ### Checking mutability of mutable pointer types\n+//!\n+//! `&mut T` can be frozen, so it is acceptable to borrow it as either imm or mut:\n+//!\n+//! ```text\n+//! MUTABILITY(*LV, MQ)                 // M-Deref-Borrowed-Mut\n+//!   TYPE(LV) = &mut Ty\n+//! ```\n+//!\n+//! ## Checking aliasability\n+//!\n+//! The goal of the aliasability check is to ensure that we never permit\n+//! `&mut` borrows of aliasable data. Formally we define a predicate\n+//! `ALIASABLE(LV, MQ)` which if defined means that\n+//! \"borrowing `LV` with mutability `MQ` is ok\". The\n+//! Rust code corresponding to this predicate is the function\n+//! `check_aliasability()` in `middle::borrowck::gather_loans`.\n+//!\n+//! ### Checking aliasability of variables\n+//!\n+//! Local variables are never aliasable as they are accessible only within\n+//! the stack frame.\n+//!\n+//! ```text\n+//!     ALIASABLE(X, MQ)                   // M-Var-Mut\n+//! ```\n+//!\n+//! ### Checking aliasable of owned content\n+//!\n+//! Owned content is aliasable if it is found in an aliasable location:\n+//!\n+//! ```text\n+//! ALIASABLE(LV.f, MQ)                // M-Field\n+//!   ALIASABLE(LV, MQ)\n+//!\n+//! ALIASABLE(*LV, MQ)                 // M-Deref-Unique\n+//!   ALIASABLE(LV, MQ)\n+//! ```\n+//!\n+//! ### Checking mutability of immutable pointer types\n+//!\n+//! Immutable pointer types like `&T` are aliasable, and hence can only be\n+//! borrowed immutably:\n+//!\n+//! ```text\n+//! ALIASABLE(*LV, imm)                // M-Deref-Borrowed-Imm\n+//!   TYPE(LV) = &Ty\n+//! ```\n+//!\n+//! ### Checking mutability of mutable pointer types\n+//!\n+//! `&mut T` can be frozen, so it is acceptable to borrow it as either imm or mut:\n+//!\n+//! ```text\n+//! ALIASABLE(*LV, MQ)                 // M-Deref-Borrowed-Mut\n+//!   TYPE(LV) = &mut Ty\n+//! ```\n+//!\n+//! ## Checking lifetime\n+//!\n+//! These rules aim to ensure that no data is borrowed for a scope that exceeds\n+//! its lifetime. These two computations wind up being intimately related.\n+//! Formally, we define a predicate `LIFETIME(LV, LT, MQ)`, which states that\n+//! \"the lvalue `LV` can be safely borrowed for the lifetime `LT` with mutability\n+//! `MQ`\". The Rust code corresponding to this predicate is the module\n+//! `middle::borrowck::gather_loans::lifetime`.\n+//!\n+//! ### The Scope function\n+//!\n+//! Several of the rules refer to a helper function `SCOPE(LV)=LT`.  The\n+//! `SCOPE(LV)` yields the lifetime `LT` for which the lvalue `LV` is\n+//! guaranteed to exist, presuming that no mutations occur.\n+//!\n+//! The scope of a local variable is the block where it is declared:\n+//!\n+//! ```text\n+//!   SCOPE(X) = block where X is declared\n+//! ```\n+//!\n+//! The scope of a field is the scope of the struct:\n+//!\n+//! ```text\n+//!   SCOPE(LV.f) = SCOPE(LV)\n+//! ```\n+//!\n+//! The scope of a unique referent is the scope of the pointer, since\n+//! (barring mutation or moves) the pointer will not be freed until\n+//! the pointer itself `LV` goes out of scope:\n+//!\n+//! ```text\n+//!   SCOPE(*LV) = SCOPE(LV) if LV has type Box<T>\n+//! ```\n+//!\n+//! The scope of a borrowed referent is the scope associated with the\n+//! pointer.  This is a conservative approximation, since the data that\n+//! the pointer points at may actually live longer:\n+//!\n+//! ```text\n+//!   SCOPE(*LV) = LT if LV has type &'LT T or &'LT mut T\n+//! ```\n+//!\n+//! ### Checking lifetime of variables\n+//!\n+//! The rule for variables states that a variable can only be borrowed a\n+//! lifetime `LT` that is a subregion of the variable's scope:\n+//!\n+//! ```text\n+//! LIFETIME(X, LT, MQ)                 // L-Local\n+//!   LT <= SCOPE(X)\n+//! ```\n+//!\n+//! ### Checking lifetime for owned content\n+//!\n+//! The lifetime of a field or owned pointer is the same as the lifetime\n+//! of its owner:\n+//!\n+//! ```text\n+//! LIFETIME(LV.f, LT, MQ)              // L-Field\n+//!   LIFETIME(LV, LT, MQ)\n+//!\n+//! LIFETIME(*LV, LT, MQ)               // L-Deref-Send\n+//!   TYPE(LV) = Box<Ty>\n+//!   LIFETIME(LV, LT, MQ)\n+//! ```\n+//!\n+//! ### Checking lifetime for derefs of references\n+//!\n+//! References have a lifetime `LT'` associated with them.  The\n+//! data they point at has been guaranteed to be valid for at least this\n+//! lifetime. Therefore, the borrow is valid so long as the lifetime `LT`\n+//! of the borrow is shorter than the lifetime `LT'` of the pointer\n+//! itself:\n+//!\n+//! ```text\n+//! LIFETIME(*LV, LT, MQ)               // L-Deref-Borrowed\n+//!   TYPE(LV) = &LT' Ty OR &LT' mut Ty\n+//!   LT <= LT'\n+//! ```\n+//!\n+//! ## Computing the restrictions\n+//!\n+//! The final rules govern the computation of *restrictions*, meaning that\n+//! we compute the set of actions that will be illegal for the life of the\n+//! loan. The predicate is written `RESTRICTIONS(LV, LT, ACTIONS) =\n+//! RESTRICTION*`, which can be read \"in order to prevent `ACTIONS` from\n+//! occurring on `LV`, the restrictions `RESTRICTION*` must be respected\n+//! for the lifetime of the loan\".\n+//!\n+//! Note that there is an initial set of restrictions: these restrictions\n+//! are computed based on the kind of borrow:\n+//!\n+//! ```text\n+//! &mut LV =>   RESTRICTIONS(LV, LT, MUTATE|CLAIM|FREEZE)\n+//! &LV =>       RESTRICTIONS(LV, LT, MUTATE|CLAIM)\n+//! &const LV => RESTRICTIONS(LV, LT, [])\n+//! ```\n+//!\n+//! The reasoning here is that a mutable borrow must be the only writer,\n+//! therefore it prevents other writes (`MUTATE`), mutable borrows\n+//! (`CLAIM`), and immutable borrows (`FREEZE`). An immutable borrow\n+//! permits other immutable borrows but forbids writes and mutable borrows.\n+//! Finally, a const borrow just wants to be sure that the value is not\n+//! moved out from under it, so no actions are forbidden.\n+//!\n+//! ### Restrictions for loans of a local variable\n+//!\n+//! The simplest case is a borrow of a local variable `X`:\n+//!\n+//! ```text\n+//! RESTRICTIONS(X, LT, ACTIONS) = (X, ACTIONS)            // R-Variable\n+//! ```\n+//!\n+//! In such cases we just record the actions that are not permitted.\n+//!\n+//! ### Restrictions for loans of fields\n+//!\n+//! Restricting a field is the same as restricting the owner of that\n+//! field:\n+//!\n+//! ```text\n+//! RESTRICTIONS(LV.f, LT, ACTIONS) = RS, (LV.f, ACTIONS)  // R-Field\n+//!   RESTRICTIONS(LV, LT, ACTIONS) = RS\n+//! ```\n+//!\n+//! The reasoning here is as follows. If the field must not be mutated,\n+//! then you must not mutate the owner of the field either, since that\n+//! would indirectly modify the field. Similarly, if the field cannot be\n+//! frozen or aliased, we cannot allow the owner to be frozen or aliased,\n+//! since doing so indirectly freezes/aliases the field. This is the\n+//! origin of inherited mutability.\n+//!\n+//! ### Restrictions for loans of owned referents\n+//!\n+//! Because the mutability of owned referents is inherited, restricting an\n+//! owned referent is similar to restricting a field, in that it implies\n+//! restrictions on the pointer. However, owned pointers have an important\n+//! twist: if the owner `LV` is mutated, that causes the owned referent\n+//! `*LV` to be freed! So whenever an owned referent `*LV` is borrowed, we\n+//! must prevent the owned pointer `LV` from being mutated, which means\n+//! that we always add `MUTATE` and `CLAIM` to the restriction set imposed\n+//! on `LV`:\n+//!\n+//! ```text\n+//! RESTRICTIONS(*LV, LT, ACTIONS) = RS, (*LV, ACTIONS)    // R-Deref-Send-Pointer\n+//!   TYPE(LV) = Box<Ty>\n+//!   RESTRICTIONS(LV, LT, ACTIONS|MUTATE|CLAIM) = RS\n+//! ```\n+//!\n+//! ### Restrictions for loans of immutable borrowed referents\n+//!\n+//! Immutable borrowed referents are freely aliasable, meaning that\n+//! the compiler does not prevent you from copying the pointer.  This\n+//! implies that issuing restrictions is useless. We might prevent the\n+//! user from acting on `*LV` itself, but there could be another path\n+//! `*LV1` that refers to the exact same memory, and we would not be\n+//! restricting that path. Therefore, the rule for `&Ty` pointers\n+//! always returns an empty set of restrictions, and it only permits\n+//! restricting `MUTATE` and `CLAIM` actions:\n+//!\n+//! ```text\n+//! RESTRICTIONS(*LV, LT, ACTIONS) = []                    // R-Deref-Imm-Borrowed\n+//!   TYPE(LV) = &LT' Ty\n+//!   LT <= LT'                                            // (1)\n+//!   ACTIONS subset of [MUTATE, CLAIM]\n+//! ```\n+//!\n+//! The reason that we can restrict `MUTATE` and `CLAIM` actions even\n+//! without a restrictions list is that it is never legal to mutate nor to\n+//! borrow mutably the contents of a `&Ty` pointer. In other words,\n+//! those restrictions are already inherent in the type.\n+//!\n+//! Clause (1) in the rule for `&Ty` deserves mention. Here I\n+//! specify that the lifetime of the loan must be less than the lifetime\n+//! of the `&Ty` pointer. In simple cases, this clause is redundant, since\n+//! the `LIFETIME()` function will already enforce the required rule:\n+//!\n+//! ```\n+//! fn foo(point: &'a Point) -> &'static f32 {\n+//!     &point.x // Error\n+//! }\n+//! ```\n+//!\n+//! The above example fails to compile both because of clause (1) above\n+//! but also by the basic `LIFETIME()` check. However, in more advanced\n+//! examples involving multiple nested pointers, clause (1) is needed:\n+//!\n+//! ```\n+//! fn foo(point: &'a &'b mut Point) -> &'b f32 {\n+//!     &point.x // Error\n+//! }\n+//! ```\n+//!\n+//! The `LIFETIME` rule here would accept `'b` because, in fact, the\n+//! *memory is* guaranteed to remain valid (i.e., not be freed) for the\n+//! lifetime `'b`, since the `&mut` pointer is valid for `'b`. However, we\n+//! are returning an immutable reference, so we need the memory to be both\n+//! valid and immutable. Even though `point.x` is referenced by an `&mut`\n+//! pointer, it can still be considered immutable so long as that `&mut`\n+//! pointer is found in an aliased location. That means the memory is\n+//! guaranteed to be *immutable* for the lifetime of the `&` pointer,\n+//! which is only `'a`, not `'b`. Hence this example yields an error.\n+//!\n+//! As a final twist, consider the case of two nested *immutable*\n+//! pointers, rather than a mutable pointer within an immutable one:\n+//!\n+//! ```\n+//! fn foo(point: &'a &'b Point) -> &'b f32 {\n+//!     &point.x // OK\n+//! }\n+//! ```\n+//!\n+//! This function is legal. The reason for this is that the inner pointer\n+//! (`*point : &'b Point`) is enough to guarantee the memory is immutable\n+//! and valid for the lifetime `'b`.  This is reflected in\n+//! `RESTRICTIONS()` by the fact that we do not recurse (i.e., we impose\n+//! no restrictions on `LV`, which in this particular case is the pointer\n+//! `point : &'a &'b Point`).\n+//!\n+//! #### Why both `LIFETIME()` and `RESTRICTIONS()`?\n+//!\n+//! Given the previous text, it might seem that `LIFETIME` and\n+//! `RESTRICTIONS` should be folded together into one check, but there is\n+//! a reason that they are separated. They answer separate concerns.\n+//! The rules pertaining to `LIFETIME` exist to ensure that we don't\n+//! create a borrowed pointer that outlives the memory it points at. So\n+//! `LIFETIME` prevents a function like this:\n+//!\n+//! ```\n+//! fn get_1<'a>() -> &'a int {\n+//!     let x = 1;\n+//!     &x\n+//! }\n+//! ```\n+//!\n+//! Here we would be returning a pointer into the stack. Clearly bad.\n+//!\n+//! However, the `RESTRICTIONS` rules are more concerned with how memory\n+//! is used. The example above doesn't generate an error according to\n+//! `RESTRICTIONS` because, for local variables, we don't require that the\n+//! loan lifetime be a subset of the local variable lifetime. The idea\n+//! here is that we *can* guarantee that `x` is not (e.g.) mutated for the\n+//! lifetime `'a`, even though `'a` exceeds the function body and thus\n+//! involves unknown code in the caller -- after all, `x` ceases to exist\n+//! after we return and hence the remaining code in `'a` cannot possibly\n+//! mutate it. This distinction is important for type checking functions\n+//! like this one:\n+//!\n+//! ```\n+//! fn inc_and_get<'a>(p: &'a mut Point) -> &'a int {\n+//!     p.x += 1;\n+//!     &p.x\n+//! }\n+//! ```\n+//!\n+//! In this case, we take in a `&mut` and return a frozen borrowed pointer\n+//! with the same lifetime. So long as the lifetime of the returned value\n+//! doesn't exceed the lifetime of the `&mut` we receive as input, this is\n+//! fine, though it may seem surprising at first (it surprised me when I\n+//! first worked it through). After all, we're guaranteeing that `*p`\n+//! won't be mutated for the lifetime `'a`, even though we can't \"see\" the\n+//! entirety of the code during that lifetime, since some of it occurs in\n+//! our caller. But we *do* know that nobody can mutate `*p` except\n+//! through `p`. So if we don't mutate `*p` and we don't return `p`, then\n+//! we know that the right to mutate `*p` has been lost to our caller --\n+//! in terms of capability, the caller passed in the ability to mutate\n+//! `*p`, and we never gave it back. (Note that we can't return `p` while\n+//! `*p` is borrowed since that would be a move of `p`, as `&mut` pointers\n+//! are affine.)\n+//!\n+//! ### Restrictions for loans of const aliasable referents\n+//!\n+//! Freeze pointers are read-only. There may be `&mut` or `&` aliases, and\n+//! we can not prevent *anything* but moves in that case. So the\n+//! `RESTRICTIONS` function is only defined if `ACTIONS` is the empty set.\n+//! Because moves from a `&const` lvalue are never legal, it is not\n+//! necessary to add any restrictions at all to the final result.\n+//!\n+//! ```text\n+//!     RESTRICTIONS(*LV, LT, []) = []                         // R-Deref-Freeze-Borrowed\n+//!       TYPE(LV) = &const Ty\n+//! ```\n+//!\n+//! ### Restrictions for loans of mutable borrowed referents\n+//!\n+//! Mutable borrowed pointers are guaranteed to be the only way to mutate\n+//! their referent. This permits us to take greater license with them; for\n+//! example, the referent can be frozen simply be ensuring that we do not\n+//! use the original pointer to perform mutate. Similarly, we can allow\n+//! the referent to be claimed, so long as the original pointer is unused\n+//! while the new claimant is live.\n+//!\n+//! The rule for mutable borrowed pointers is as follows:\n+//!\n+//! ```text\n+//! RESTRICTIONS(*LV, LT, ACTIONS) = RS, (*LV, ACTIONS)    // R-Deref-Mut-Borrowed\n+//!   TYPE(LV) = &LT' mut Ty\n+//!   LT <= LT'                                            // (1)\n+//!   RESTRICTIONS(LV, LT, ACTIONS) = RS                   // (2)\n+//! ```\n+//!\n+//! Let's examine the two numbered clauses:\n+//!\n+//! Clause (1) specifies that the lifetime of the loan (`LT`) cannot\n+//! exceed the lifetime of the `&mut` pointer (`LT'`). The reason for this\n+//! is that the `&mut` pointer is guaranteed to be the only legal way to\n+//! mutate its referent -- but only for the lifetime `LT'`.  After that\n+//! lifetime, the loan on the referent expires and hence the data may be\n+//! modified by its owner again. This implies that we are only able to\n+//! guarantee that the referent will not be modified or aliased for a\n+//! maximum of `LT'`.\n+//!\n+//! Here is a concrete example of a bug this rule prevents:\n+//!\n+//! ```\n+//! // Test region-reborrow-from-shorter-mut-ref.rs:\n+//! fn copy_pointer<'a,'b,T>(x: &'a mut &'b mut T) -> &'b mut T {\n+//!     &mut **p // ERROR due to clause (1)\n+//! }\n+//! fn main() {\n+//!     let mut x = 1;\n+//!     let mut y = &mut x; // <-'b-----------------------------+\n+//!     //      +-'a--------------------+                       |\n+//!     //      v                       v                       |\n+//!     let z = copy_borrowed_ptr(&mut y); // y is lent         |\n+//!     *y += 1; // Here y==z, so both should not be usable...  |\n+//!     *z += 1; // ...and yet they would be, but for clause 1. |\n+//! } // <------------------------------------------------------+\n+//! ```\n+//!\n+//! Clause (2) propagates the restrictions on the referent to the pointer\n+//! itself. This is the same as with an owned pointer, though the\n+//! reasoning is mildly different. The basic goal in all cases is to\n+//! prevent the user from establishing another route to the same data. To\n+//! see what I mean, let's examine various cases of what can go wrong and\n+//! show how it is prevented.\n+//!\n+//! **Example danger 1: Moving the base pointer.** One of the simplest\n+//! ways to violate the rules is to move the base pointer to a new name\n+//! and access it via that new name, thus bypassing the restrictions on\n+//! the old name. Here is an example:\n+//!\n+//! ```\n+//! // src/test/compile-fail/borrowck-move-mut-base-ptr.rs\n+//! fn foo(t0: &mut int) {\n+//!     let p: &int = &*t0; // Freezes `*t0`\n+//!     let t1 = t0;        //~ ERROR cannot move out of `t0`\n+//!     *t1 = 22;           // OK, not a write through `*t0`\n+//! }\n+//! ```\n+//!\n+//! Remember that `&mut` pointers are linear, and hence `let t1 = t0` is a\n+//! move of `t0` -- or would be, if it were legal. Instead, we get an\n+//! error, because clause (2) imposes restrictions on `LV` (`t0`, here),\n+//! and any restrictions on a path make it impossible to move from that\n+//! path.\n+//!\n+//! **Example danger 2: Claiming the base pointer.** Another possible\n+//! danger is to mutably borrow the base path. This can lead to two bad\n+//! scenarios. The most obvious is that the mutable borrow itself becomes\n+//! another path to access the same data, as shown here:\n+//!\n+//! ```\n+//! // src/test/compile-fail/borrowck-mut-borrow-of-mut-base-ptr.rs\n+//! fn foo<'a>(mut t0: &'a mut int,\n+//!            mut t1: &'a mut int) {\n+//!     let p: &int = &*t0;     // Freezes `*t0`\n+//!     let mut t2 = &mut t0;   //~ ERROR cannot borrow `t0`\n+//!     **t2 += 1;              // Mutates `*t0`\n+//! }\n+//! ```\n+//!\n+//! In this example, `**t2` is the same memory as `*t0`. Because `t2` is\n+//! an `&mut` pointer, `**t2` is a unique path and hence it would be\n+//! possible to mutate `**t2` even though that memory was supposed to be\n+//! frozen by the creation of `p`. However, an error is reported -- the\n+//! reason is that the freeze `&*t0` will restrict claims and mutation\n+//! against `*t0` which, by clause 2, in turn prevents claims and mutation\n+//! of `t0`. Hence the claim `&mut t0` is illegal.\n+//!\n+//! Another danger with an `&mut` pointer is that we could swap the `t0`\n+//! value away to create a new path:\n+//!\n+//! ```\n+//! // src/test/compile-fail/borrowck-swap-mut-base-ptr.rs\n+//! fn foo<'a>(mut t0: &'a mut int,\n+//!            mut t1: &'a mut int) {\n+//!     let p: &int = &*t0;     // Freezes `*t0`\n+//!     swap(&mut t0, &mut t1); //~ ERROR cannot borrow `t0`\n+//!     *t1 = 22;\n+//! }\n+//! ```\n+//!\n+//! This is illegal for the same reason as above. Note that if we added\n+//! back a swap operator -- as we used to have -- we would want to be very\n+//! careful to ensure this example is still illegal.\n+//!\n+//! **Example danger 3: Freeze the base pointer.** In the case where the\n+//! referent is claimed, even freezing the base pointer can be dangerous,\n+//! as shown in the following example:\n+//!\n+//! ```\n+//! // src/test/compile-fail/borrowck-borrow-of-mut-base-ptr.rs\n+//! fn foo<'a>(mut t0: &'a mut int,\n+//!            mut t1: &'a mut int) {\n+//!     let p: &mut int = &mut *t0; // Claims `*t0`\n+//!     let mut t2 = &t0;           //~ ERROR cannot borrow `t0`\n+//!     let q: &int = &*t2;         // Freezes `*t0` but not through `*p`\n+//!     *p += 1;                    // violates type of `*q`\n+//! }\n+//! ```\n+//!\n+//! Here the problem is that `*t0` is claimed by `p`, and hence `p` wants\n+//! to be the controlling pointer through which mutation or freezes occur.\n+//! But `t2` would -- if it were legal -- have the type `& &mut int`, and\n+//! hence would be a mutable pointer in an aliasable location, which is\n+//! considered frozen (since no one can write to `**t2` as it is not a\n+//! unique path). Therefore, we could reasonably create a frozen `&int`\n+//! pointer pointing at `*t0` that coexists with the mutable pointer `p`,\n+//! which is clearly unsound.\n+//!\n+//! However, it is not always unsafe to freeze the base pointer. In\n+//! particular, if the referent is frozen, there is no harm in it:\n+//!\n+//! ```\n+//! // src/test/run-pass/borrowck-borrow-of-mut-base-ptr-safe.rs\n+//! fn foo<'a>(mut t0: &'a mut int,\n+//!            mut t1: &'a mut int) {\n+//!     let p: &int = &*t0; // Freezes `*t0`\n+//!     let mut t2 = &t0;\n+//!     let q: &int = &*t2; // Freezes `*t0`, but that's ok...\n+//!     let r: &int = &*t0; // ...after all, could do same thing directly.\n+//! }\n+//! ```\n+//!\n+//! In this case, creating the alias `t2` of `t0` is safe because the only\n+//! thing `t2` can be used for is to further freeze `*t0`, which is\n+//! already frozen. In particular, we cannot assign to `*t0` through the\n+//! new alias `t2`, as demonstrated in this test case:\n+//!\n+//! ```\n+//! // src/test/run-pass/borrowck-borrow-mut-base-ptr-in-aliasable-loc.rs\n+//! fn foo(t0: & &mut int) {\n+//!     let t1 = t0;\n+//!     let p: &int = &**t0;\n+//!     **t1 = 22; //~ ERROR cannot assign\n+//! }\n+//! ```\n+//!\n+//! This distinction is reflected in the rules. When doing an `&mut`\n+//! borrow -- as in the first example -- the set `ACTIONS` will be\n+//! `CLAIM|MUTATE|FREEZE`, because claiming the referent implies that it\n+//! cannot be claimed, mutated, or frozen by anyone else. These\n+//! restrictions are propagated back to the base path and hence the base\n+//! path is considered unfreezable.\n+//!\n+//! In contrast, when the referent is merely frozen -- as in the second\n+//! example -- the set `ACTIONS` will be `CLAIM|MUTATE`, because freezing\n+//! the referent implies that it cannot be claimed or mutated but permits\n+//! others to freeze. Hence when these restrictions are propagated back to\n+//! the base path, it will still be considered freezable.\n+//!\n+//!\n+//!\n+//! **FIXME #10520: Restrictions against mutating the base pointer.** When\n+//! an `&mut` pointer is frozen or claimed, we currently pass along the\n+//! restriction against MUTATE to the base pointer. I do not believe this\n+//! restriction is needed. It dates from the days when we had a way to\n+//! mutate that preserved the value being mutated (i.e., swap). Nowadays\n+//! the only form of mutation is assignment, which destroys the pointer\n+//! being mutated -- therefore, a mutation cannot create a new path to the\n+//! same data. Rather, it removes an existing path. This implies that not\n+//! only can we permit mutation, we can have mutation kill restrictions in\n+//! the dataflow sense.\n+//!\n+//! **WARNING:** We do not currently have `const` borrows in the\n+//! language. If they are added back in, we must ensure that they are\n+//! consistent with all of these examples. The crucial question will be\n+//! what sorts of actions are permitted with a `&const &mut` pointer. I\n+//! would suggest that an `&mut` referent found in an `&const` location be\n+//! prohibited from both freezes and claims. This would avoid the need to\n+//! prevent `const` borrows of the base pointer when the referent is\n+//! borrowed.\n+//!\n+//! # Moves and initialization\n+//!\n+//! The borrow checker is also in charge of ensuring that:\n+//!\n+//! - all memory which is accessed is initialized\n+//! - immutable local variables are assigned at most once.\n+//!\n+//! These are two separate dataflow analyses built on the same\n+//! framework. Let's look at checking that memory is initialized first;\n+//! the checking of immutable local variable assignments works in a very\n+//! similar way.\n+//!\n+//! To track the initialization of memory, we actually track all the\n+//! points in the program that *create uninitialized memory*, meaning\n+//! moves and the declaration of uninitialized variables. For each of\n+//! these points, we create a bit in the dataflow set. Assignments to a\n+//! variable `x` or path `a.b.c` kill the move/uninitialization bits for\n+//! those paths and any subpaths (e.g., `x`, `x.y`, `a.b.c`, `*a.b.c`).\n+//! Bits are unioned when two control-flow paths join. Thus, the\n+//! presence of a bit indicates that the move may have occurred without an\n+//! intervening assignment to the same memory. At each use of a variable,\n+//! we examine the bits in scope, and check that none of them are\n+//! moves/uninitializations of the variable that is being used.\n+//!\n+//! Let's look at a simple example:\n+//!\n+//! ```\n+//! fn foo(a: Box<int>) {\n+//!     let b: Box<int>;   // Gen bit 0.\n+//!\n+//!     if cond {          // Bits: 0\n+//!         use(&*a);\n+//!         b = a;         // Gen bit 1, kill bit 0.\n+//!         use(&*b);\n+//!     } else {\n+//!                        // Bits: 0\n+//!     }\n+//!                        // Bits: 0,1\n+//!     use(&*a);          // Error.\n+//!     use(&*b);          // Error.\n+//! }\n+//!\n+//! fn use(a: &int) { }\n+//! ```\n+//!\n+//! In this example, the variable `b` is created uninitialized. In one\n+//! branch of an `if`, we then move the variable `a` into `b`. Once we\n+//! exit the `if`, therefore, it is an error to use `a` or `b` since both\n+//! are only conditionally initialized. I have annotated the dataflow\n+//! state using comments. There are two dataflow bits, with bit 0\n+//! corresponding to the creation of `b` without an initializer, and bit 1\n+//! corresponding to the move of `a`. The assignment `b = a` both\n+//! generates bit 1, because it is a move of `a`, and kills bit 0, because\n+//! `b` is now initialized. On the else branch, though, `b` is never\n+//! initialized, and so bit 0 remains untouched. When the two flows of\n+//! control join, we union the bits from both sides, resulting in both\n+//! bits 0 and 1 being set. Thus any attempt to use `a` uncovers the bit 1\n+//! from the \"then\" branch, showing that `a` may be moved, and any attempt\n+//! to use `b` uncovers bit 0, from the \"else\" branch, showing that `b`\n+//! may not be initialized.\n+//!\n+//! ## Initialization of immutable variables\n+//!\n+//! Initialization of immutable variables works in a very similar way,\n+//! except that:\n+//!\n+//! 1. we generate bits for each assignment to a variable;\n+//! 2. the bits are never killed except when the variable goes out of scope.\n+//!\n+//! Thus the presence of an assignment bit indicates that the assignment\n+//! may have occurred. Note that assignments are only killed when the\n+//! variable goes out of scope, as it is not relevant whether or not there\n+//! has been a move in the meantime. Using these bits, we can declare that\n+//! an assignment to an immutable variable is legal iff there is no other\n+//! assignment bit to that same variable in scope.\n+//!\n+//! ## Why is the design made this way?\n+//!\n+//! It may seem surprising that we assign dataflow bits to *each move*\n+//! rather than *each path being moved*. This is somewhat less efficient,\n+//! since on each use, we must iterate through all moves and check whether\n+//! any of them correspond to the path in question. Similar concerns apply\n+//! to the analysis for double assignments to immutable variables. The\n+//! main reason to do it this way is that it allows us to print better\n+//! error messages, because when a use occurs, we can print out the\n+//! precise move that may be in scope, rather than simply having to say\n+//! \"the variable may not be initialized\".\n+//!\n+//! ## Data structures used in the move analysis\n+//!\n+//! The move analysis maintains several data structures that enable it to\n+//! cross-reference moves and assignments to determine when they may be\n+//! moving/assigning the same memory. These are all collected into the\n+//! `MoveData` and `FlowedMoveData` structs. The former represents the set\n+//! of move paths, moves, and assignments, and the latter adds in the\n+//! results of a dataflow computation.\n+//!\n+//! ### Move paths\n+//!\n+//! The `MovePath` tree tracks every path that is moved or assigned to.\n+//! These paths have the same form as the `LoanPath` data structure, which\n+//! in turn is the \"real world version of the lvalues `LV` that we\n+//! introduced earlier. The difference between a `MovePath` and a `LoanPath`\n+//! is that move paths are:\n+//!\n+//! 1. Canonicalized, so that we have exactly one copy of each, and\n+//!    we can refer to move paths by index;\n+//! 2. Cross-referenced with other paths into a tree, so that given a move\n+//!    path we can efficiently find all parent move paths and all\n+//!    extensions (e.g., given the `a.b` move path, we can easily find the\n+//!    move path `a` and also the move paths `a.b.c`)\n+//! 3. Cross-referenced with moves and assignments, so that we can\n+//!    easily find all moves and assignments to a given path.\n+//!\n+//! The mechanism that we use is to create a `MovePath` record for each\n+//! move path. These are arranged in an array and are referenced using\n+//! `MovePathIndex` values, which are newtype'd indices. The `MovePath`\n+//! structs are arranged into a tree, representing using the standard\n+//! Knuth representation where each node has a child 'pointer' and a \"next\n+//! sibling\" 'pointer'. In addition, each `MovePath` has a parent\n+//! 'pointer'.  In this case, the 'pointers' are just `MovePathIndex`\n+//! values.\n+//!\n+//! In this way, if we want to find all base paths of a given move path,\n+//! we can just iterate up the parent pointers (see `each_base_path()` in\n+//! the `move_data` module). If we want to find all extensions, we can\n+//! iterate through the subtree (see `each_extending_path()`).\n+//!\n+//! ### Moves and assignments\n+//!\n+//! There are structs to represent moves (`Move`) and assignments\n+//! (`Assignment`), and these are also placed into arrays and referenced\n+//! by index. All moves of a particular path are arranged into a linked\n+//! lists, beginning with `MovePath.first_move` and continuing through\n+//! `Move.next_move`.\n+//!\n+//! We distinguish between \"var\" assignments, which are assignments to a\n+//! variable like `x = foo`, and \"path\" assignments (`x.f = foo`).  This\n+//! is because we need to assign dataflows to the former, but not the\n+//! latter, so as to check for double initialization of immutable\n+//! variables.\n+//!\n+//! ### Gathering and checking moves\n+//!\n+//! Like loans, we distinguish two phases. The first, gathering, is where\n+//! we uncover all the moves and assignments. As with loans, we do some\n+//! basic sanity checking in this phase, so we'll report errors if you\n+//! attempt to move out of a borrowed pointer etc. Then we do the dataflow\n+//! (see `FlowedMoveData::new`). Finally, in the `check_loans.rs` code, we\n+//! walk back over, identify all uses, assignments, and captures, and\n+//! check that they are legal given the set of dataflow bits we have\n+//! computed for that program point.\n+//!\n+//! # Drop flags and structural fragments\n+//!\n+//! In addition to the job of enforcing memory safety, the borrow checker\n+//! code is also responsible for identifying the *structural fragments* of\n+//! data in the function, to support out-of-band dynamic drop flags\n+//! allocated on the stack. (For background, see [RFC PR #320].)\n+//!\n+//! [RFC PR #320]: https://github.com/rust-lang/rfcs/pull/320\n+//!\n+//! Semantically, each piece of data that has a destructor may need a\n+//! boolean flag to indicate whether or not its destructor has been run\n+//! yet. However, in many cases there is no need to actually maintain such\n+//! a flag: It can be apparent from the code itself that a given path is\n+//! always initialized (or always deinitialized) when control reaches the\n+//! end of its owner's scope, and thus we can unconditionally emit (or\n+//! not) the destructor invocation for that path.\n+//!\n+//! A simple example of this is the following:\n+//!\n+//! ```rust\n+//! struct D { p: int }\n+//! impl D { fn new(x: int) -> D { ... }\n+//! impl Drop for D { ... }\n+//!\n+//! fn foo(a: D, b: D, t: || -> bool) {\n+//!     let c: D;\n+//!     let d: D;\n+//!     if t() { c = b; }\n+//! }\n+//! ```\n+//!\n+//! At the end of the body of `foo`, the compiler knows that `a` is\n+//! initialized, introducing a drop obligation (deallocating the boxed\n+//! integer) for the end of `a`'s scope that is run unconditionally.\n+//! Likewise the compiler knows that `d` is not initialized, and thus it\n+//! leave out the drop code for `d`.\n+//!\n+//! The compiler cannot statically know the drop-state of `b` nor `c` at\n+//! the end of their scope, since that depends on the value of\n+//! `t`. Therefore, we need to insert boolean flags to track whether we\n+//! need to drop `b` and `c`.\n+//!\n+//! However, the matter is not as simple as just mapping local variables\n+//! to their corresponding drop flags when necessary. In particular, in\n+//! addition to being able to move data out of local variables, Rust\n+//! allows one to move values in and out of structured data.\n+//!\n+//! Consider the following:\n+//!\n+//! ```rust\n+//! struct S { x: D, y: D, z: D }\n+//!\n+//! fn foo(a: S, mut b: S, t: || -> bool) {\n+//!     let mut c: S;\n+//!     let d: S;\n+//!     let e: S = a.clone();\n+//!     if t() {\n+//!         c = b;\n+//!         b.x = e.y;\n+//!     }\n+//!     if t() { c.y = D::new(4); }\n+//! }\n+//! ```\n+//!\n+//! As before, the drop obligations of `a` and `d` can be statically\n+//! determined, and again the state of `b` and `c` depend on dynamic\n+//! state. But additionally, the dynamic drop obligations introduced by\n+//! `b` and `c` are not just per-local boolean flags. For example, if the\n+//! first call to `t` returns `false` and the second call `true`, then at\n+//! the end of their scope, `b` will be completely initialized, but only\n+//! `c.y` in `c` will be initialized.  If both calls to `t` return `true`,\n+//! then at the end of their scope, `c` will be completely initialized,\n+//! but only `b.x` will be initialized in `b`, and only `e.x` and `e.z`\n+//! will be initialized in `e`.\n+//!\n+//! Note that we need to cover the `z` field in each case in some way,\n+//! since it may (or may not) need to be dropped, even though `z` is never\n+//! directly mentioned in the body of the `foo` function. We call a path\n+//! like `b.z` a *fragment sibling* of `b.x`, since the field `z` comes\n+//! from the same structure `S` that declared the field `x` in `b.x`.\n+//!\n+//! In general we need to maintain boolean flags that match the\n+//! `S`-structure of both `b` and `c`.  In addition, we need to consult\n+//! such a flag when doing an assignment (such as `c.y = D::new(4);`\n+//! above), in order to know whether or not there is a previous value that\n+//! needs to be dropped before we do the assignment.\n+//!\n+//! So for any given function, we need to determine what flags are needed\n+//! to track its drop obligations. Our strategy for determining the set of\n+//! flags is to represent the fragmentation of the structure explicitly:\n+//! by starting initially from the paths that are explicitly mentioned in\n+//! moves and assignments (such as `b.x` and `c.y` above), and then\n+//! traversing the structure of the path's type to identify leftover\n+//! *unmoved fragments*: assigning into `c.y` means that `c.x` and `c.z`\n+//! are leftover unmoved fragments. Each fragment represents a drop\n+//! obligation that may need to be tracked. Paths that are only moved or\n+//! assigned in their entirety (like `a` and `d`) are treated as a single\n+//! drop obligation.\n+//!\n+//! The fragment construction process works by piggy-backing on the\n+//! existing `move_data` module. We already have callbacks that visit each\n+//! direct move and assignment; these form the basis for the sets of\n+//! moved_leaf_paths and assigned_leaf_paths. From these leaves, we can\n+//! walk up their parent chain to identify all of their parent paths.\n+//! We need to identify the parents because of cases like the following:\n+//!\n+//! ```rust\n+//! struct Pair<X,Y>{ x: X, y: Y }\n+//! fn foo(dd_d_d: Pair<Pair<Pair<D, D>, D>, D>) {\n+//!     other_function(dd_d_d.x.y);\n+//! }\n+//! ```\n+//!\n+//! In this code, the move of the path `dd_d.x.y` leaves behind not only\n+//! the fragment drop-obligation `dd_d.x.x` but also `dd_d.y` as well.\n+//!\n+//! Once we have identified the directly-referenced leaves and their\n+//! parents, we compute the left-over fragments, in the function\n+//! `fragments::add_fragment_siblings`. As of this writing this works by\n+//! looking at each directly-moved or assigned path P, and blindly\n+//! gathering all sibling fields of P (as well as siblings for the parents\n+//! of P, etc). After accumulating all such siblings, we filter out the\n+//! entries added as siblings of P that turned out to be\n+//! directly-referenced paths (or parents of directly referenced paths)\n+//! themselves, thus leaving the never-referenced \"left-overs\" as the only\n+//! thing left from the gathering step.\n+//!\n+//! ## Array structural fragments\n+//!\n+//! A special case of the structural fragments discussed above are\n+//! the elements of an array that has been passed by value, such as\n+//! the following:\n+//!\n+//! ```rust\n+//! fn foo(a: [D, ..10], i: uint) -> D {\n+//!     a[i]\n+//! }\n+//! ```\n+//!\n+//! The above code moves a single element out of the input array `a`.\n+//! The remainder of the array still needs to be dropped; i.e., it\n+//! is a structural fragment. Note that after performing such a move,\n+//! it is not legal to read from the array `a`. There are a number of\n+//! ways to deal with this, but the important thing to note is that\n+//! the semantics needs to distinguish in some manner between a\n+//! fragment that is the *entire* array versus a fragment that represents\n+//! all-but-one element of the array.  A place where that distinction\n+//! would arise is the following:\n+//!\n+//! ```rust\n+//! fn foo(a: [D, ..10], b: [D, ..10], i: uint, t: bool) -> D {\n+//!     if t {\n+//!         a[i]\n+//!     } else {\n+//!         b[i]\n+//!     }\n+//!\n+//!     // When control exits, we will need either to drop all of `a`\n+//!     // and all-but-one of `b`, or to drop all of `b` and all-but-one\n+//!     // of `a`.\n+//! }\n+//! ```\n+//!\n+//! There are a number of ways that the trans backend could choose to\n+//! compile this (e.g. a `[bool, ..10]` array for each such moved array;\n+//! or an `Option<uint>` for each moved array).  From the viewpoint of the\n+//! borrow-checker, the important thing is to record what kind of fragment\n+//! is implied by the relevant moves.\n+//!\n+//! # Future work\n+//!\n+//! While writing up these docs, I encountered some rules I believe to be\n+//! stricter than necessary:\n+//!\n+//! - I think restricting the `&mut` LV against moves and `ALIAS` is sufficient,\n+//!   `MUTATE` and `CLAIM` are overkill. `MUTATE` was necessary when swap was\n+//!   a built-in operator, but as it is not, it is implied by `CLAIM`,\n+//!   and `CLAIM` is implied by `ALIAS`. The only net effect of this is an\n+//!   extra error message in some cases, though.\n+//! - I have not described how closures interact. Current code is unsound.\n+//!   I am working on describing and implementing the fix.\n+//! - If we wish, we can easily extend the move checking to allow finer-grained\n+//!   tracking of what is initialized and what is not, enabling code like\n+//!   this:\n+//!\n+//!       a = x.f.g; // x.f.g is now uninitialized\n+//!       // here, x and x.f are not usable, but x.f.h *is*\n+//!       x.f.g = b; // x.f.g is not initialized\n+//!       // now x, x.f, x.f.g, x.f.h are all usable\n+//!\n+//!   What needs to change here, most likely, is that the `moves` module\n+//!   should record not only what paths are moved, but what expressions\n+//!   are actual *uses*. For example, the reference to `x` in `x.f.g = b`\n+//!   is not a true *use* in the sense that it requires `x` to be fully\n+//!   initialized. This is in fact why the above code produces an error\n+//!   today: the reference to `x` in `x.f.g = b` is considered illegal\n+//!   because `x` is not fully initialized.\n+//!\n+//! There are also some possible refactorings:\n+//!\n+//! - It might be nice to replace all loan paths with the MovePath mechanism,\n+//!   since they allow lightweight comparison using an integer."}, {"sha": "dddc326df35724ca70ce4ab9a977e399a1197e48", "filename": "src/librustc/middle/borrowck/fragments.rs", "status": "modified", "additions": 15, "deletions": 32, "changes": 47, "blob_url": "https://github.com/rust-lang/rust/blob/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc%2Fmiddle%2Fborrowck%2Ffragments.rs", "raw_url": "https://github.com/rust-lang/rust/raw/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc%2Fmiddle%2Fborrowck%2Ffragments.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Fmiddle%2Fborrowck%2Ffragments.rs?ref=cd5c8235c5448a7234548c772468c8d2e8f150d9", "patch": "@@ -8,13 +8,10 @@\n // option. This file may not be copied, modified, or distributed\n // except according to those terms.\n \n-/*!\n+//! Helper routines used for fragmenting structural paths due to moves for\n+//! tracking drop obligations. Please see the extensive comments in the\n+//! section \"Structural fragments\" in `doc.rs`.\n \n-Helper routines used for fragmenting structural paths due to moves for\n-tracking drop obligations. Please see the extensive comments in the\n-section \"Structural fragments\" in `doc.rs`.\n-\n-*/\n use self::Fragment::*;\n \n use session::config;\n@@ -176,16 +173,12 @@ pub fn instrument_move_fragments<'tcx>(this: &MoveData<'tcx>,\n     instrument_all_paths(\"assigned_leaf_path\", &fragments.assigned_leaf_paths);\n }\n \n+/// Normalizes the fragment sets in `this`; i.e., removes duplicate entries, constructs the set of\n+/// parents, and constructs the left-over fragments.\n+///\n+/// Note: \"left-over fragments\" means paths that were not directly referenced in moves nor\n+/// assignments, but must nonetheless be tracked as potential drop obligations.\n pub fn fixup_fragment_sets<'tcx>(this: &MoveData<'tcx>, tcx: &ty::ctxt<'tcx>) {\n-    /*!\n-     * Normalizes the fragment sets in `this`; i.e., removes\n-     * duplicate entries, constructs the set of parents, and\n-     * constructs the left-over fragments.\n-     *\n-     * Note: \"left-over fragments\" means paths that were not\n-     * directly referenced in moves nor assignments, but must\n-     * nonetheless be tracked as potential drop obligations.\n-     */\n \n     let mut fragments = this.fragments.borrow_mut();\n \n@@ -283,18 +276,14 @@ pub fn fixup_fragment_sets<'tcx>(this: &MoveData<'tcx>, tcx: &ty::ctxt<'tcx>) {\n     }\n }\n \n+/// Adds all of the precisely-tracked siblings of `lp` as potential move paths of interest. For\n+/// example, if `lp` represents `s.x.j`, then adds moves paths for `s.x.i` and `s.x.k`, the\n+/// siblings of `s.x.j`.\n fn add_fragment_siblings<'tcx>(this: &MoveData<'tcx>,\n                                tcx: &ty::ctxt<'tcx>,\n                                gathered_fragments: &mut Vec<Fragment>,\n                                lp: Rc<LoanPath<'tcx>>,\n                                origin_id: Option<ast::NodeId>) {\n-    /*!\n-     * Adds all of the precisely-tracked siblings of `lp` as\n-     * potential move paths of interest. For example, if `lp`\n-     * represents `s.x.j`, then adds moves paths for `s.x.i` and\n-     * `s.x.k`, the siblings of `s.x.j`.\n-     */\n-\n     match lp.kind {\n         LpVar(_) | LpUpvar(..) => {} // Local variables have no siblings.\n \n@@ -343,6 +332,8 @@ fn add_fragment_siblings<'tcx>(this: &MoveData<'tcx>,\n     }\n }\n \n+/// We have determined that `origin_lp` destructures to LpExtend(parent, original_field_name).\n+/// Based on this, add move paths for all of the siblings of `origin_lp`.\n fn add_fragment_siblings_for_extension<'tcx>(this: &MoveData<'tcx>,\n                                              tcx: &ty::ctxt<'tcx>,\n                                              gathered_fragments: &mut Vec<Fragment>,\n@@ -353,12 +344,6 @@ fn add_fragment_siblings_for_extension<'tcx>(this: &MoveData<'tcx>,\n                                              origin_id: Option<ast::NodeId>,\n                                              enum_variant_info: Option<(ast::DefId,\n                                                                         Rc<LoanPath<'tcx>>)>) {\n-    /*!\n-     * We have determined that `origin_lp` destructures to\n-     * LpExtend(parent, original_field_name). Based on this,\n-     * add move paths for all of the siblings of `origin_lp`.\n-     */\n-\n     let parent_ty = parent_lp.to_type();\n \n     let add_fragment_sibling_local = |field_name| {\n@@ -454,17 +439,15 @@ fn add_fragment_siblings_for_extension<'tcx>(this: &MoveData<'tcx>,\n     }\n }\n \n+/// Adds the single sibling `LpExtend(parent, new_field_name)` of `origin_lp` (the original\n+/// loan-path).\n fn add_fragment_sibling_core<'tcx>(this: &MoveData<'tcx>,\n                                    tcx: &ty::ctxt<'tcx>,\n                                    gathered_fragments: &mut Vec<Fragment>,\n                                    parent: Rc<LoanPath<'tcx>>,\n                                    mc: mc::MutabilityCategory,\n                                    new_field_name: mc::FieldName,\n                                    origin_lp: &Rc<LoanPath<'tcx>>) -> MovePathIndex {\n-    /*!\n-     * Adds the single sibling `LpExtend(parent, new_field_name)`\n-     * of `origin_lp` (the original loan-path).\n-     */\n     let opt_variant_did = match parent.kind {\n         LpDowncast(_, variant_did) => Some(variant_did),\n         LpVar(..) | LpUpvar(..) | LpExtend(..) => None,"}, {"sha": "651141605042782ff943c3bbfa8181264216276e", "filename": "src/librustc/middle/borrowck/gather_loans/gather_moves.rs", "status": "modified", "additions": 1, "deletions": 3, "changes": 4, "blob_url": "https://github.com/rust-lang/rust/blob/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc%2Fmiddle%2Fborrowck%2Fgather_loans%2Fgather_moves.rs", "raw_url": "https://github.com/rust-lang/rust/raw/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc%2Fmiddle%2Fborrowck%2Fgather_loans%2Fgather_moves.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Fmiddle%2Fborrowck%2Fgather_loans%2Fgather_moves.rs?ref=cd5c8235c5448a7234548c772468c8d2e8f150d9", "patch": "@@ -8,9 +8,7 @@\n // option. This file may not be copied, modified, or distributed\n // except according to those terms.\n \n-/*!\n- * Computes moves.\n- */\n+//! Computes moves.\n \n use middle::borrowck::*;\n use middle::borrowck::LoanPathKind::*;"}, {"sha": "e6a7c150df8f41d82ae460e1ad8f447a12e33d86", "filename": "src/librustc/middle/borrowck/gather_loans/lifetime.rs", "status": "modified", "additions": 2, "deletions": 4, "changes": 6, "blob_url": "https://github.com/rust-lang/rust/blob/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc%2Fmiddle%2Fborrowck%2Fgather_loans%2Flifetime.rs", "raw_url": "https://github.com/rust-lang/rust/raw/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc%2Fmiddle%2Fborrowck%2Fgather_loans%2Flifetime.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Fmiddle%2Fborrowck%2Fgather_loans%2Flifetime.rs?ref=cd5c8235c5448a7234548c772468c8d2e8f150d9", "patch": "@@ -8,10 +8,8 @@\n // option. This file may not be copied, modified, or distributed\n // except according to those terms.\n \n-/*!\n- * This module implements the check that the lifetime of a borrow\n- * does not exceed the lifetime of the value being borrowed.\n- */\n+//! This module implements the check that the lifetime of a borrow\n+//! does not exceed the lifetime of the value being borrowed.\n \n use middle::borrowck::*;\n use middle::expr_use_visitor as euv;"}, {"sha": "4f7ecc99c8938e22d54d6485e95d46efdbd3a533", "filename": "src/librustc/middle/borrowck/gather_loans/mod.rs", "status": "modified", "additions": 3, "deletions": 6, "changes": 9, "blob_url": "https://github.com/rust-lang/rust/blob/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc%2Fmiddle%2Fborrowck%2Fgather_loans%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc%2Fmiddle%2Fborrowck%2Fgather_loans%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Fmiddle%2Fborrowck%2Fgather_loans%2Fmod.rs?ref=cd5c8235c5448a7234548c772468c8d2e8f150d9", "patch": "@@ -225,19 +225,16 @@ fn check_aliasability<'a, 'tcx>(bccx: &BorrowckCtxt<'a, 'tcx>,\n impl<'a, 'tcx> GatherLoanCtxt<'a, 'tcx> {\n     pub fn tcx(&self) -> &'a ty::ctxt<'tcx> { self.bccx.tcx }\n \n+    /// Guarantees that `addr_of(cmt)` will be valid for the duration of `static_scope_r`, or\n+    /// reports an error.  This may entail taking out loans, which will be added to the\n+    /// `req_loan_map`.\n     fn guarantee_valid(&mut self,\n                        borrow_id: ast::NodeId,\n                        borrow_span: Span,\n                        cmt: mc::cmt<'tcx>,\n                        req_kind: ty::BorrowKind,\n                        loan_region: ty::Region,\n                        cause: euv::LoanCause) {\n-        /*!\n-         * Guarantees that `addr_of(cmt)` will be valid for the duration of\n-         * `static_scope_r`, or reports an error.  This may entail taking\n-         * out loans, which will be added to the `req_loan_map`.\n-         */\n-\n         debug!(\"guarantee_valid(borrow_id={}, cmt={}, \\\n                 req_mutbl={}, loan_region={})\",\n                borrow_id,"}, {"sha": "bd9cf8f84b6431a1913291628956d748a1d20514", "filename": "src/librustc/middle/borrowck/gather_loans/restrictions.rs", "status": "modified", "additions": 1, "deletions": 3, "changes": 4, "blob_url": "https://github.com/rust-lang/rust/blob/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc%2Fmiddle%2Fborrowck%2Fgather_loans%2Frestrictions.rs", "raw_url": "https://github.com/rust-lang/rust/raw/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc%2Fmiddle%2Fborrowck%2Fgather_loans%2Frestrictions.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Fmiddle%2Fborrowck%2Fgather_loans%2Frestrictions.rs?ref=cd5c8235c5448a7234548c772468c8d2e8f150d9", "patch": "@@ -8,9 +8,7 @@\n // option. This file may not be copied, modified, or distributed\n // except according to those terms.\n \n-/*!\n- * Computes the restrictions that result from a borrow.\n- */\n+//! Computes the restrictions that result from a borrow.\n \n pub use self::RestrictionResult::*;\n "}, {"sha": "0bbcdfe61bb46c86d5688ad0f81c0e16c177ecab", "filename": "src/librustc/middle/borrowck/mod.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc%2Fmiddle%2Fborrowck%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc%2Fmiddle%2Fborrowck%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Fmiddle%2Fborrowck%2Fmod.rs?ref=cd5c8235c5448a7234548c772468c8d2e8f150d9", "patch": "@@ -8,7 +8,7 @@\n // option. This file may not be copied, modified, or distributed\n // except according to those terms.\n \n-/*! See doc.rs for a thorough explanation of the borrow checker */\n+//! See doc.rs for a thorough explanation of the borrow checker\n \n #![allow(non_camel_case_types)]\n "}, {"sha": "7bf3458f0ae3da3613273ea5e9ca6afa1c791574", "filename": "src/librustc/middle/borrowck/move_data.rs", "status": "modified", "additions": 23, "deletions": 54, "changes": 77, "blob_url": "https://github.com/rust-lang/rust/blob/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc%2Fmiddle%2Fborrowck%2Fmove_data.rs", "raw_url": "https://github.com/rust-lang/rust/raw/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc%2Fmiddle%2Fborrowck%2Fmove_data.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Fmiddle%2Fborrowck%2Fmove_data.rs?ref=cd5c8235c5448a7234548c772468c8d2e8f150d9", "patch": "@@ -8,12 +8,8 @@\n // option. This file may not be copied, modified, or distributed\n // except according to those terms.\n \n-/*!\n-\n-Data structures used for tracking moves. Please see the extensive\n-comments in the section \"Moves and initialization\" in `doc.rs`.\n-\n-*/\n+//! Data structures used for tracking moves. Please see the extensive\n+//! comments in the section \"Moves and initialization\" in `doc.rs`.\n \n pub use self::MoveKind::*;\n \n@@ -297,15 +293,11 @@ impl<'tcx> MoveData<'tcx> {\n         self.path_parent(index) == InvalidMovePathIndex\n     }\n \n+    /// Returns the existing move path index for `lp`, if any, and otherwise adds a new index for\n+    /// `lp` and any of its base paths that do not yet have an index.\n     pub fn move_path(&self,\n                      tcx: &ty::ctxt<'tcx>,\n                      lp: Rc<LoanPath<'tcx>>) -> MovePathIndex {\n-        /*!\n-         * Returns the existing move path index for `lp`, if any,\n-         * and otherwise adds a new index for `lp` and any of its\n-         * base paths that do not yet have an index.\n-         */\n-\n         match self.path_map.borrow().get(&lp) {\n             Some(&index) => {\n                 return index;\n@@ -370,13 +362,10 @@ impl<'tcx> MoveData<'tcx> {\n         result\n     }\n \n+    /// Adds any existing move path indices for `lp` and any base paths of `lp` to `result`, but\n+    /// does not add new move paths\n     fn add_existing_base_paths(&self, lp: &Rc<LoanPath<'tcx>>,\n                                result: &mut Vec<MovePathIndex>) {\n-        /*!\n-         * Adds any existing move path indices for `lp` and any base\n-         * paths of `lp` to `result`, but does not add new move paths\n-         */\n-\n         match self.path_map.borrow().get(lp).cloned() {\n             Some(index) => {\n                 self.each_base_path(index, |p| {\n@@ -397,16 +386,12 @@ impl<'tcx> MoveData<'tcx> {\n \n     }\n \n+    /// Adds a new move entry for a move of `lp` that occurs at location `id` with kind `kind`.\n     pub fn add_move(&self,\n                     tcx: &ty::ctxt<'tcx>,\n                     lp: Rc<LoanPath<'tcx>>,\n                     id: ast::NodeId,\n                     kind: MoveKind) {\n-        /*!\n-         * Adds a new move entry for a move of `lp` that occurs at\n-         * location `id` with kind `kind`.\n-         */\n-\n         debug!(\"add_move(lp={}, id={}, kind={})\",\n                lp.repr(tcx),\n                id,\n@@ -428,18 +413,15 @@ impl<'tcx> MoveData<'tcx> {\n         });\n     }\n \n+    /// Adds a new record for an assignment to `lp` that occurs at location `id` with the given\n+    /// `span`.\n     pub fn add_assignment(&self,\n                           tcx: &ty::ctxt<'tcx>,\n                           lp: Rc<LoanPath<'tcx>>,\n                           assign_id: ast::NodeId,\n                           span: Span,\n                           assignee_id: ast::NodeId,\n                           mode: euv::MutateMode) {\n-        /*!\n-         * Adds a new record for an assignment to `lp` that occurs at\n-         * location `id` with the given `span`.\n-         */\n-\n         debug!(\"add_assignment(lp={}, assign_id={}, assignee_id={}\",\n                lp.repr(tcx), assign_id, assignee_id);\n \n@@ -473,18 +455,16 @@ impl<'tcx> MoveData<'tcx> {\n         }\n     }\n \n+    /// Adds a new record for a match of `base_lp`, downcast to\n+    /// variant `lp`, that occurs at location `pattern_id`.  (One\n+    /// should be able to recover the span info from the\n+    /// `pattern_id` and the ast_map, I think.)\n     pub fn add_variant_match(&self,\n                              tcx: &ty::ctxt<'tcx>,\n                              lp: Rc<LoanPath<'tcx>>,\n                              pattern_id: ast::NodeId,\n                              base_lp: Rc<LoanPath<'tcx>>,\n                              mode: euv::MatchMode) {\n-        /*!\n-         * Adds a new record for a match of `base_lp`, downcast to\n-         * variant `lp`, that occurs at location `pattern_id`.  (One\n-         * should be able to recover the span info from the\n-         * `pattern_id` and the ast_map, I think.)\n-         */\n         debug!(\"add_variant_match(lp={}, pattern_id={})\",\n                lp.repr(tcx), pattern_id);\n \n@@ -507,18 +487,15 @@ impl<'tcx> MoveData<'tcx> {\n         fragments::fixup_fragment_sets(self, tcx)\n     }\n \n+    /// Adds the gen/kills for the various moves and\n+    /// assignments into the provided data flow contexts.\n+    /// Moves are generated by moves and killed by assignments and\n+    /// scoping. Assignments are generated by assignment to variables and\n+    /// killed by scoping. See `doc.rs` for more details.\n     fn add_gen_kills(&self,\n                      tcx: &ty::ctxt<'tcx>,\n                      dfcx_moves: &mut MoveDataFlow,\n                      dfcx_assign: &mut AssignDataFlow) {\n-        /*!\n-         * Adds the gen/kills for the various moves and\n-         * assignments into the provided data flow contexts.\n-         * Moves are generated by moves and killed by assignments and\n-         * scoping. Assignments are generated by assignment to variables and\n-         * killed by scoping. See `doc.rs` for more details.\n-         */\n-\n         for (i, the_move) in self.moves.borrow().iter().enumerate() {\n             dfcx_moves.add_gen(the_move.id, i);\n         }\n@@ -695,18 +672,14 @@ impl<'a, 'tcx> FlowedMoveData<'a, 'tcx> {\n         ret\n     }\n \n+    /// Iterates through each move of `loan_path` (or some base path of `loan_path`) that *may*\n+    /// have occurred on entry to `id` without an intervening assignment. In other words, any moves\n+    /// that would invalidate a reference to `loan_path` at location `id`.\n     pub fn each_move_of(&self,\n                         id: ast::NodeId,\n                         loan_path: &Rc<LoanPath<'tcx>>,\n                         f: |&Move, &LoanPath<'tcx>| -> bool)\n                         -> bool {\n-        /*!\n-         * Iterates through each move of `loan_path` (or some base path\n-         * of `loan_path`) that *may* have occurred on entry to `id` without\n-         * an intervening assignment. In other words, any moves that\n-         * would invalidate a reference to `loan_path` at location `id`.\n-         */\n-\n         // Bad scenarios:\n         //\n         // 1. Move of `a.b.c`, use of `a.b.c`\n@@ -755,17 +728,13 @@ impl<'a, 'tcx> FlowedMoveData<'a, 'tcx> {\n         })\n     }\n \n+    /// Iterates through every assignment to `loan_path` that may have occurred on entry to `id`.\n+    /// `loan_path` must be a single variable.\n     pub fn each_assignment_of(&self,\n                               id: ast::NodeId,\n                               loan_path: &Rc<LoanPath<'tcx>>,\n                               f: |&Assignment| -> bool)\n                               -> bool {\n-        /*!\n-         * Iterates through every assignment to `loan_path` that\n-         * may have occurred on entry to `id`. `loan_path` must be\n-         * a single variable.\n-         */\n-\n         let loan_path_index = {\n             match self.move_data.existing_move_path(loan_path) {\n                 Some(i) => i,"}, {"sha": "a2e8ba8d65c3321e782a147b6512973a74c8c71d", "filename": "src/librustc/middle/cfg/mod.rs", "status": "modified", "additions": 2, "deletions": 6, "changes": 8, "blob_url": "https://github.com/rust-lang/rust/blob/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc%2Fmiddle%2Fcfg%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc%2Fmiddle%2Fcfg%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Fmiddle%2Fcfg%2Fmod.rs?ref=cd5c8235c5448a7234548c772468c8d2e8f150d9", "patch": "@@ -8,12 +8,8 @@\n // option. This file may not be copied, modified, or distributed\n // except according to those terms.\n \n-/*!\n-\n-Module that constructs a control-flow graph representing an item.\n-Uses `Graph` as the underlying representation.\n-\n-*/\n+//! Module that constructs a control-flow graph representing an item.\n+//! Uses `Graph` as the underlying representation.\n \n use middle::graph;\n use middle::ty;"}, {"sha": "53fea8ffc86c65e1f7519d5880fbf11cf0ed7f58", "filename": "src/librustc/middle/dataflow.rs", "status": "modified", "additions": 4, "deletions": 6, "changes": 10, "blob_url": "https://github.com/rust-lang/rust/blob/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc%2Fmiddle%2Fdataflow.rs", "raw_url": "https://github.com/rust-lang/rust/raw/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc%2Fmiddle%2Fdataflow.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Fmiddle%2Fdataflow.rs?ref=cd5c8235c5448a7234548c772468c8d2e8f150d9", "patch": "@@ -9,12 +9,10 @@\n // except according to those terms.\n \n \n-/*!\n- * A module for propagating forward dataflow information. The analysis\n- * assumes that the items to be propagated can be represented as bits\n- * and thus uses bitvectors. Your job is simply to specify the so-called\n- * GEN and KILL bits for each expression.\n- */\n+//! A module for propagating forward dataflow information. The analysis\n+//! assumes that the items to be propagated can be represented as bits\n+//! and thus uses bitvectors. Your job is simply to specify the so-called\n+//! GEN and KILL bits for each expression.\n \n pub use self::EntryOrExit::*;\n "}, {"sha": "9bb5a6f9a2447da225bdcfac599742feeb6e4b6e", "filename": "src/librustc/middle/expr_use_visitor.rs", "status": "modified", "additions": 8, "deletions": 17, "changes": 25, "blob_url": "https://github.com/rust-lang/rust/blob/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc%2Fmiddle%2Fexpr_use_visitor.rs", "raw_url": "https://github.com/rust-lang/rust/raw/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc%2Fmiddle%2Fexpr_use_visitor.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Fmiddle%2Fexpr_use_visitor.rs?ref=cd5c8235c5448a7234548c772468c8d2e8f150d9", "patch": "@@ -8,11 +8,9 @@\n // option. This file may not be copied, modified, or distributed\n // except according to those terms.\n \n-/*!\n- * A different sort of visitor for walking fn bodies.  Unlike the\n- * normal visitor, which just walks the entire body in one shot, the\n- * `ExprUseVisitor` determines how expressions are being used.\n- */\n+//! A different sort of visitor for walking fn bodies.  Unlike the\n+//! normal visitor, which just walks the entire body in one shot, the\n+//! `ExprUseVisitor` determines how expressions are being used.\n \n pub use self::MutateMode::*;\n pub use self::LoanCause::*;\n@@ -716,12 +714,9 @@ impl<'d,'t,'tcx,TYPER:mc::Typer<'tcx>> ExprUseVisitor<'d,'t,'tcx,TYPER> {\n         }\n     }\n \n+    /// Indicates that the value of `blk` will be consumed, meaning either copied or moved\n+    /// depending on its type.\n     fn walk_block(&mut self, blk: &ast::Block) {\n-        /*!\n-         * Indicates that the value of `blk` will be consumed,\n-         * meaning either copied or moved depending on its type.\n-         */\n-\n         debug!(\"walk_block(blk.id={})\", blk.id);\n \n         for stmt in blk.stmts.iter() {\n@@ -821,16 +816,12 @@ impl<'d,'t,'tcx,TYPER:mc::Typer<'tcx>> ExprUseVisitor<'d,'t,'tcx,TYPER> {\n         }\n     }\n \n+    /// Autoderefs for overloaded Deref calls in fact reference their receiver. That is, if we have\n+    /// `(*x)` where `x` is of type `Rc<T>`, then this in fact is equivalent to `x.deref()`. Since\n+    /// `deref()` is declared with `&self`, this is an autoref of `x`.\n     fn walk_autoderefs(&mut self,\n                        expr: &ast::Expr,\n                        autoderefs: uint) {\n-        /*!\n-         * Autoderefs for overloaded Deref calls in fact reference\n-         * their receiver. That is, if we have `(*x)` where `x` is of\n-         * type `Rc<T>`, then this in fact is equivalent to\n-         * `x.deref()`. Since `deref()` is declared with `&self`, this\n-         * is an autoref of `x`.\n-         */\n         debug!(\"walk_autoderefs expr={} autoderefs={}\", expr.repr(self.tcx()), autoderefs);\n \n         for i in range(0, autoderefs) {"}, {"sha": "da467c3d0d555a3736fd97ea77e66585ae8d47ba", "filename": "src/librustc/middle/fast_reject.rs", "status": "modified", "additions": 9, "deletions": 15, "changes": 24, "blob_url": "https://github.com/rust-lang/rust/blob/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc%2Fmiddle%2Ffast_reject.rs", "raw_url": "https://github.com/rust-lang/rust/raw/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc%2Fmiddle%2Ffast_reject.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Fmiddle%2Ffast_reject.rs?ref=cd5c8235c5448a7234548c772468c8d2e8f150d9", "patch": "@@ -33,26 +33,20 @@ pub enum SimplifiedType {\n     ParameterSimplifiedType,\n }\n \n+/// Tries to simplify a type by dropping type parameters, deref'ing away any reference types, etc.\n+/// The idea is to get something simple that we can use to quickly decide if two types could unify\n+/// during method lookup.\n+///\n+/// If `can_simplify_params` is false, then we will fail to simplify type parameters entirely. This\n+/// is useful when those type parameters would be instantiated with fresh type variables, since\n+/// then we can't say much about whether two types would unify. Put another way,\n+/// `can_simplify_params` should be true if type parameters appear free in `ty` and `false` if they\n+/// are to be considered bound.\n pub fn simplify_type(tcx: &ty::ctxt,\n                      ty: Ty,\n                      can_simplify_params: bool)\n                      -> Option<SimplifiedType>\n {\n-    /*!\n-     * Tries to simplify a type by dropping type parameters, deref'ing\n-     * away any reference types, etc. The idea is to get something\n-     * simple that we can use to quickly decide if two types could\n-     * unify during method lookup.\n-     *\n-     * If `can_simplify_params` is false, then we will fail to\n-     * simplify type parameters entirely. This is useful when those\n-     * type parameters would be instantiated with fresh type\n-     * variables, since then we can't say much about whether two types\n-     * would unify. Put another way, `can_simplify_params` should be\n-     * true if type parameters appear free in `ty` and `false` if they\n-     * are to be considered bound.\n-     */\n-\n     match ty.sty {\n         ty::ty_bool => Some(BoolSimplifiedType),\n         ty::ty_char => Some(CharSimplifiedType),"}, {"sha": "2f50a96402302a5354aa540095006dcde08c53f2", "filename": "src/librustc/middle/graph.rs", "status": "modified", "additions": 21, "deletions": 25, "changes": 46, "blob_url": "https://github.com/rust-lang/rust/blob/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc%2Fmiddle%2Fgraph.rs", "raw_url": "https://github.com/rust-lang/rust/raw/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc%2Fmiddle%2Fgraph.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Fmiddle%2Fgraph.rs?ref=cd5c8235c5448a7234548c772468c8d2e8f150d9", "patch": "@@ -8,31 +8,27 @@\n // option. This file may not be copied, modified, or distributed\n // except according to those terms.\n \n-/*!\n-\n-A graph module for use in dataflow, region resolution, and elsewhere.\n-\n-# Interface details\n-\n-You customize the graph by specifying a \"node data\" type `N` and an\n-\"edge data\" type `E`. You can then later gain access (mutable or\n-immutable) to these \"user-data\" bits. Currently, you can only add\n-nodes or edges to the graph. You cannot remove or modify them once\n-added. This could be changed if we have a need.\n-\n-# Implementation details\n-\n-The main tricky thing about this code is the way that edges are\n-stored. The edges are stored in a central array, but they are also\n-threaded onto two linked lists for each node, one for incoming edges\n-and one for outgoing edges. Note that every edge is a member of some\n-incoming list and some outgoing list.  Basically you can load the\n-first index of the linked list from the node data structures (the\n-field `first_edge`) and then, for each edge, load the next index from\n-the field `next_edge`). Each of those fields is an array that should\n-be indexed by the direction (see the type `Direction`).\n-\n-*/\n+//! A graph module for use in dataflow, region resolution, and elsewhere.\n+//!\n+//! # Interface details\n+//!\n+//! You customize the graph by specifying a \"node data\" type `N` and an\n+//! \"edge data\" type `E`. You can then later gain access (mutable or\n+//! immutable) to these \"user-data\" bits. Currently, you can only add\n+//! nodes or edges to the graph. You cannot remove or modify them once\n+//! added. This could be changed if we have a need.\n+//!\n+//! # Implementation details\n+//!\n+//! The main tricky thing about this code is the way that edges are\n+//! stored. The edges are stored in a central array, but they are also\n+//! threaded onto two linked lists for each node, one for incoming edges\n+//! and one for outgoing edges. Note that every edge is a member of some\n+//! incoming list and some outgoing list.  Basically you can load the\n+//! first index of the linked list from the node data structures (the\n+//! field `first_edge`) and then, for each edge, load the next index from\n+//! the field `next_edge`). Each of those fields is an array that should\n+//! be indexed by the direction (see the type `Direction`).\n \n #![allow(dead_code)] // still WIP\n "}, {"sha": "a09ceac11a53dae7513e0b37e30fb2dd353f237a", "filename": "src/librustc/middle/liveness.rs", "status": "modified", "additions": 97, "deletions": 99, "changes": 196, "blob_url": "https://github.com/rust-lang/rust/blob/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc%2Fmiddle%2Fliveness.rs", "raw_url": "https://github.com/rust-lang/rust/raw/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc%2Fmiddle%2Fliveness.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Fmiddle%2Fliveness.rs?ref=cd5c8235c5448a7234548c772468c8d2e8f150d9", "patch": "@@ -8,105 +8,103 @@\n // option. This file may not be copied, modified, or distributed\n // except according to those terms.\n \n-/*!\n- * A classic liveness analysis based on dataflow over the AST.  Computes,\n- * for each local variable in a function, whether that variable is live\n- * at a given point.  Program execution points are identified by their\n- * id.\n- *\n- * # Basic idea\n- *\n- * The basic model is that each local variable is assigned an index.  We\n- * represent sets of local variables using a vector indexed by this\n- * index.  The value in the vector is either 0, indicating the variable\n- * is dead, or the id of an expression that uses the variable.\n- *\n- * We conceptually walk over the AST in reverse execution order.  If we\n- * find a use of a variable, we add it to the set of live variables.  If\n- * we find an assignment to a variable, we remove it from the set of live\n- * variables.  When we have to merge two flows, we take the union of\n- * those two flows---if the variable is live on both paths, we simply\n- * pick one id.  In the event of loops, we continue doing this until a\n- * fixed point is reached.\n- *\n- * ## Checking initialization\n- *\n- * At the function entry point, all variables must be dead.  If this is\n- * not the case, we can report an error using the id found in the set of\n- * live variables, which identifies a use of the variable which is not\n- * dominated by an assignment.\n- *\n- * ## Checking moves\n- *\n- * After each explicit move, the variable must be dead.\n- *\n- * ## Computing last uses\n- *\n- * Any use of the variable where the variable is dead afterwards is a\n- * last use.\n- *\n- * # Implementation details\n- *\n- * The actual implementation contains two (nested) walks over the AST.\n- * The outer walk has the job of building up the ir_maps instance for the\n- * enclosing function.  On the way down the tree, it identifies those AST\n- * nodes and variable IDs that will be needed for the liveness analysis\n- * and assigns them contiguous IDs.  The liveness id for an AST node is\n- * called a `live_node` (it's a newtype'd uint) and the id for a variable\n- * is called a `variable` (another newtype'd uint).\n- *\n- * On the way back up the tree, as we are about to exit from a function\n- * declaration we allocate a `liveness` instance.  Now that we know\n- * precisely how many nodes and variables we need, we can allocate all\n- * the various arrays that we will need to precisely the right size.  We then\n- * perform the actual propagation on the `liveness` instance.\n- *\n- * This propagation is encoded in the various `propagate_through_*()`\n- * methods.  It effectively does a reverse walk of the AST; whenever we\n- * reach a loop node, we iterate until a fixed point is reached.\n- *\n- * ## The `Users` struct\n- *\n- * At each live node `N`, we track three pieces of information for each\n- * variable `V` (these are encapsulated in the `Users` struct):\n- *\n- * - `reader`: the `LiveNode` ID of some node which will read the value\n- *    that `V` holds on entry to `N`.  Formally: a node `M` such\n- *    that there exists a path `P` from `N` to `M` where `P` does not\n- *    write `V`.  If the `reader` is `invalid_node()`, then the current\n- *    value will never be read (the variable is dead, essentially).\n- *\n- * - `writer`: the `LiveNode` ID of some node which will write the\n- *    variable `V` and which is reachable from `N`.  Formally: a node `M`\n- *    such that there exists a path `P` from `N` to `M` and `M` writes\n- *    `V`.  If the `writer` is `invalid_node()`, then there is no writer\n- *    of `V` that follows `N`.\n- *\n- * - `used`: a boolean value indicating whether `V` is *used*.  We\n- *   distinguish a *read* from a *use* in that a *use* is some read that\n- *   is not just used to generate a new value.  For example, `x += 1` is\n- *   a read but not a use.  This is used to generate better warnings.\n- *\n- * ## Special Variables\n- *\n- * We generate various special variables for various, well, special purposes.\n- * These are described in the `specials` struct:\n- *\n- * - `exit_ln`: a live node that is generated to represent every 'exit' from\n- *   the function, whether it be by explicit return, panic, or other means.\n- *\n- * - `fallthrough_ln`: a live node that represents a fallthrough\n- *\n- * - `no_ret_var`: a synthetic variable that is only 'read' from, the\n- *   fallthrough node.  This allows us to detect functions where we fail\n- *   to return explicitly.\n- * - `clean_exit_var`: a synthetic variable that is only 'read' from the\n- *   fallthrough node.  It is only live if the function could converge\n- *   via means other than an explicit `return` expression. That is, it is\n- *   only dead if the end of the function's block can never be reached.\n- *   It is the responsibility of typeck to ensure that there are no\n- *   `return` expressions in a function declared as diverging.\n- */\n+//! A classic liveness analysis based on dataflow over the AST.  Computes,\n+//! for each local variable in a function, whether that variable is live\n+//! at a given point.  Program execution points are identified by their\n+//! id.\n+//!\n+//! # Basic idea\n+//!\n+//! The basic model is that each local variable is assigned an index.  We\n+//! represent sets of local variables using a vector indexed by this\n+//! index.  The value in the vector is either 0, indicating the variable\n+//! is dead, or the id of an expression that uses the variable.\n+//!\n+//! We conceptually walk over the AST in reverse execution order.  If we\n+//! find a use of a variable, we add it to the set of live variables.  If\n+//! we find an assignment to a variable, we remove it from the set of live\n+//! variables.  When we have to merge two flows, we take the union of\n+//! those two flows---if the variable is live on both paths, we simply\n+//! pick one id.  In the event of loops, we continue doing this until a\n+//! fixed point is reached.\n+//!\n+//! ## Checking initialization\n+//!\n+//! At the function entry point, all variables must be dead.  If this is\n+//! not the case, we can report an error using the id found in the set of\n+//! live variables, which identifies a use of the variable which is not\n+//! dominated by an assignment.\n+//!\n+//! ## Checking moves\n+//!\n+//! After each explicit move, the variable must be dead.\n+//!\n+//! ## Computing last uses\n+//!\n+//! Any use of the variable where the variable is dead afterwards is a\n+//! last use.\n+//!\n+//! # Implementation details\n+//!\n+//! The actual implementation contains two (nested) walks over the AST.\n+//! The outer walk has the job of building up the ir_maps instance for the\n+//! enclosing function.  On the way down the tree, it identifies those AST\n+//! nodes and variable IDs that will be needed for the liveness analysis\n+//! and assigns them contiguous IDs.  The liveness id for an AST node is\n+//! called a `live_node` (it's a newtype'd uint) and the id for a variable\n+//! is called a `variable` (another newtype'd uint).\n+//!\n+//! On the way back up the tree, as we are about to exit from a function\n+//! declaration we allocate a `liveness` instance.  Now that we know\n+//! precisely how many nodes and variables we need, we can allocate all\n+//! the various arrays that we will need to precisely the right size.  We then\n+//! perform the actual propagation on the `liveness` instance.\n+//!\n+//! This propagation is encoded in the various `propagate_through_*()`\n+//! methods.  It effectively does a reverse walk of the AST; whenever we\n+//! reach a loop node, we iterate until a fixed point is reached.\n+//!\n+//! ## The `Users` struct\n+//!\n+//! At each live node `N`, we track three pieces of information for each\n+//! variable `V` (these are encapsulated in the `Users` struct):\n+//!\n+//! - `reader`: the `LiveNode` ID of some node which will read the value\n+//!    that `V` holds on entry to `N`.  Formally: a node `M` such\n+//!    that there exists a path `P` from `N` to `M` where `P` does not\n+//!    write `V`.  If the `reader` is `invalid_node()`, then the current\n+//!    value will never be read (the variable is dead, essentially).\n+//!\n+//! - `writer`: the `LiveNode` ID of some node which will write the\n+//!    variable `V` and which is reachable from `N`.  Formally: a node `M`\n+//!    such that there exists a path `P` from `N` to `M` and `M` writes\n+//!    `V`.  If the `writer` is `invalid_node()`, then there is no writer\n+//!    of `V` that follows `N`.\n+//!\n+//! - `used`: a boolean value indicating whether `V` is *used*.  We\n+//!   distinguish a *read* from a *use* in that a *use* is some read that\n+//!   is not just used to generate a new value.  For example, `x += 1` is\n+//!   a read but not a use.  This is used to generate better warnings.\n+//!\n+//! ## Special Variables\n+//!\n+//! We generate various special variables for various, well, special purposes.\n+//! These are described in the `specials` struct:\n+//!\n+//! - `exit_ln`: a live node that is generated to represent every 'exit' from\n+//!   the function, whether it be by explicit return, panic, or other means.\n+//!\n+//! - `fallthrough_ln`: a live node that represents a fallthrough\n+//!\n+//! - `no_ret_var`: a synthetic variable that is only 'read' from, the\n+//!   fallthrough node.  This allows us to detect functions where we fail\n+//!   to return explicitly.\n+//! - `clean_exit_var`: a synthetic variable that is only 'read' from the\n+//!   fallthrough node.  It is only live if the function could converge\n+//!   via means other than an explicit `return` expression. That is, it is\n+//!   only dead if the end of the function's block can never be reached.\n+//!   It is the responsibility of typeck to ensure that there are no\n+//!   `return` expressions in a function declared as diverging.\n use self::LoopKind::*;\n use self::LiveNodeKind::*;\n use self::VarKind::*;"}, {"sha": "53a5ac7a09342f7f1fd8204e07f365ca3829d4b2", "filename": "src/librustc/middle/mem_categorization.rs", "status": "modified", "additions": 60, "deletions": 73, "changes": 133, "blob_url": "https://github.com/rust-lang/rust/blob/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc%2Fmiddle%2Fmem_categorization.rs", "raw_url": "https://github.com/rust-lang/rust/raw/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc%2Fmiddle%2Fmem_categorization.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Fmiddle%2Fmem_categorization.rs?ref=cd5c8235c5448a7234548c772468c8d2e8f150d9", "patch": "@@ -8,57 +8,55 @@\n // option. This file may not be copied, modified, or distributed\n // except according to those terms.\n \n-/*!\n- * # Categorization\n- *\n- * The job of the categorization module is to analyze an expression to\n- * determine what kind of memory is used in evaluating it (for example,\n- * where dereferences occur and what kind of pointer is dereferenced;\n- * whether the memory is mutable; etc)\n- *\n- * Categorization effectively transforms all of our expressions into\n- * expressions of the following forms (the actual enum has many more\n- * possibilities, naturally, but they are all variants of these base\n- * forms):\n- *\n- *     E = rvalue    // some computed rvalue\n- *       | x         // address of a local variable or argument\n- *       | *E        // deref of a ptr\n- *       | E.comp    // access to an interior component\n- *\n- * Imagine a routine ToAddr(Expr) that evaluates an expression and returns an\n- * address where the result is to be found.  If Expr is an lvalue, then this\n- * is the address of the lvalue.  If Expr is an rvalue, this is the address of\n- * some temporary spot in memory where the result is stored.\n- *\n- * Now, cat_expr() classifies the expression Expr and the address A=ToAddr(Expr)\n- * as follows:\n- *\n- * - cat: what kind of expression was this?  This is a subset of the\n- *   full expression forms which only includes those that we care about\n- *   for the purpose of the analysis.\n- * - mutbl: mutability of the address A\n- * - ty: the type of data found at the address A\n- *\n- * The resulting categorization tree differs somewhat from the expressions\n- * themselves.  For example, auto-derefs are explicit.  Also, an index a[b] is\n- * decomposed into two operations: a dereference to reach the array data and\n- * then an index to jump forward to the relevant item.\n- *\n- * ## By-reference upvars\n- *\n- * One part of the translation which may be non-obvious is that we translate\n- * closure upvars into the dereference of a borrowed pointer; this more closely\n- * resembles the runtime translation. So, for example, if we had:\n- *\n- *     let mut x = 3;\n- *     let y = 5;\n- *     let inc = || x += y;\n- *\n- * Then when we categorize `x` (*within* the closure) we would yield a\n- * result of `*x'`, effectively, where `x'` is a `cat_upvar` reference\n- * tied to `x`. The type of `x'` will be a borrowed pointer.\n- */\n+//! # Categorization\n+//!\n+//! The job of the categorization module is to analyze an expression to\n+//! determine what kind of memory is used in evaluating it (for example,\n+//! where dereferences occur and what kind of pointer is dereferenced;\n+//! whether the memory is mutable; etc)\n+//!\n+//! Categorization effectively transforms all of our expressions into\n+//! expressions of the following forms (the actual enum has many more\n+//! possibilities, naturally, but they are all variants of these base\n+//! forms):\n+//!\n+//!     E = rvalue    // some computed rvalue\n+//!       | x         // address of a local variable or argument\n+//!       | *E        // deref of a ptr\n+//!       | E.comp    // access to an interior component\n+//!\n+//! Imagine a routine ToAddr(Expr) that evaluates an expression and returns an\n+//! address where the result is to be found.  If Expr is an lvalue, then this\n+//! is the address of the lvalue.  If Expr is an rvalue, this is the address of\n+//! some temporary spot in memory where the result is stored.\n+//!\n+//! Now, cat_expr() classifies the expression Expr and the address A=ToAddr(Expr)\n+//! as follows:\n+//!\n+//! - cat: what kind of expression was this?  This is a subset of the\n+//!   full expression forms which only includes those that we care about\n+//!   for the purpose of the analysis.\n+//! - mutbl: mutability of the address A\n+//! - ty: the type of data found at the address A\n+//!\n+//! The resulting categorization tree differs somewhat from the expressions\n+//! themselves.  For example, auto-derefs are explicit.  Also, an index a[b] is\n+//! decomposed into two operations: a dereference to reach the array data and\n+//! then an index to jump forward to the relevant item.\n+//!\n+//! ## By-reference upvars\n+//!\n+//! One part of the translation which may be non-obvious is that we translate\n+//! closure upvars into the dereference of a borrowed pointer; this more closely\n+//! resembles the runtime translation. So, for example, if we had:\n+//!\n+//!     let mut x = 3;\n+//!     let y = 5;\n+//!     let inc = || x += y;\n+//!\n+//! Then when we categorize `x` (*within* the closure) we would yield a\n+//! result of `*x'`, effectively, where `x'` is a `cat_upvar` reference\n+//! tied to `x`. The type of `x'` will be a borrowed pointer.\n \n #![allow(non_camel_case_types)]\n \n@@ -1058,38 +1056,31 @@ impl<'t,'tcx,TYPER:Typer<'tcx>> MemCategorizationContext<'t,TYPER> {\n         }\n     }\n \n+    /// Given a pattern P like: `[_, ..Q, _]`, where `vec_cmt` is the cmt for `P`, `slice_pat` is\n+    /// the pattern `Q`, returns:\n+    ///\n+    /// * a cmt for `Q`\n+    /// * the mutability and region of the slice `Q`\n+    ///\n+    /// These last two bits of info happen to be things that borrowck needs.\n     pub fn cat_slice_pattern(&self,\n                              vec_cmt: cmt<'tcx>,\n                              slice_pat: &ast::Pat)\n                              -> McResult<(cmt<'tcx>, ast::Mutability, ty::Region)> {\n-        /*!\n-         * Given a pattern P like: `[_, ..Q, _]`, where `vec_cmt` is\n-         * the cmt for `P`, `slice_pat` is the pattern `Q`, returns:\n-         * - a cmt for `Q`\n-         * - the mutability and region of the slice `Q`\n-         *\n-         * These last two bits of info happen to be things that\n-         * borrowck needs.\n-         */\n-\n         let slice_ty = if_ok!(self.node_ty(slice_pat.id));\n         let (slice_mutbl, slice_r) = vec_slice_info(self.tcx(),\n                                                     slice_pat,\n                                                     slice_ty);\n         let cmt_slice = self.cat_index(slice_pat, self.deref_vec(slice_pat, vec_cmt));\n         return Ok((cmt_slice, slice_mutbl, slice_r));\n \n+        /// In a pattern like [a, b, ..c], normally `c` has slice type, but if you have [a, b,\n+        /// ..ref c], then the type of `ref c` will be `&&[]`, so to extract the slice details we\n+        /// have to recurse through rptrs.\n         fn vec_slice_info(tcx: &ty::ctxt,\n                           pat: &ast::Pat,\n                           slice_ty: Ty)\n                           -> (ast::Mutability, ty::Region) {\n-            /*!\n-             * In a pattern like [a, b, ..c], normally `c` has slice type,\n-             * but if you have [a, b, ..ref c], then the type of `ref c`\n-             * will be `&&[]`, so to extract the slice details we have\n-             * to recurse through rptrs.\n-             */\n-\n             match slice_ty.sty {\n                 ty::ty_rptr(r, ref mt) => match mt.ty.sty {\n                     ty::ty_vec(_, None) => (mt.mutbl, r),\n@@ -1428,13 +1419,9 @@ impl<'tcx> cmt_<'tcx> {\n         }\n     }\n \n+    /// Returns `Some(_)` if this lvalue represents a freely aliasable pointer type.\n     pub fn freely_aliasable(&self, ctxt: &ty::ctxt<'tcx>)\n                             -> Option<AliasableReason> {\n-        /*!\n-         * Returns `Some(_)` if this lvalue represents a freely aliasable\n-         * pointer type.\n-         */\n-\n         // Maybe non-obvious: copied upvars can only be considered\n         // non-aliasable in once closures, since any other kind can be\n         // aliased and eventually recused."}, {"sha": "20be98ca977d1025ee344cf08a02c2c2567461ea", "filename": "src/librustc/middle/region.rs", "status": "modified", "additions": 55, "deletions": 96, "changes": 151, "blob_url": "https://github.com/rust-lang/rust/blob/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc%2Fmiddle%2Fregion.rs", "raw_url": "https://github.com/rust-lang/rust/raw/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc%2Fmiddle%2Fregion.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Fmiddle%2Fregion.rs?ref=cd5c8235c5448a7234548c772468c8d2e8f150d9", "patch": "@@ -8,18 +8,13 @@\n // option. This file may not be copied, modified, or distributed\n // except according to those terms.\n \n-/*!\n-\n-This file actually contains two passes related to regions.  The first\n-pass builds up the `scope_map`, which describes the parent links in\n-the region hierarchy.  The second pass infers which types must be\n-region parameterized.\n-\n-Most of the documentation on regions can be found in\n-`middle/typeck/infer/region_inference.rs`\n-\n-*/\n-\n+//! This file actually contains two passes related to regions.  The first\n+//! pass builds up the `scope_map`, which describes the parent links in\n+//! the region hierarchy.  The second pass infers which types must be\n+//! region parameterized.\n+//!\n+//! Most of the documentation on regions can be found in\n+//! `middle/typeck/infer/region_inference.rs`\n \n use session::Session;\n use middle::ty::{mod, Ty, FreeRegion};\n@@ -171,14 +166,10 @@ impl RegionMaps {\n         self.rvalue_scopes.borrow_mut().insert(var, lifetime);\n     }\n \n+    /// Records that a scope is a TERMINATING SCOPE. Whenever we create automatic temporaries --\n+    /// e.g. by an expression like `a().f` -- they will be freed within the innermost terminating\n+    /// scope.\n     pub fn mark_as_terminating_scope(&self, scope_id: CodeExtent) {\n-        /*!\n-         * Records that a scope is a TERMINATING SCOPE. Whenever we\n-         * create automatic temporaries -- e.g. by an\n-         * expression like `a().f` -- they will be freed within\n-         * the innermost terminating scope.\n-         */\n-\n         debug!(\"record_terminating_scope(scope_id={})\", scope_id);\n         self.terminating_scopes.borrow_mut().insert(scope_id);\n     }\n@@ -197,10 +188,8 @@ impl RegionMaps {\n         }\n     }\n \n+    /// Returns the lifetime of the local variable `var_id`\n     pub fn var_scope(&self, var_id: ast::NodeId) -> CodeExtent {\n-        /*!\n-         * Returns the lifetime of the local variable `var_id`\n-         */\n         match self.var_map.borrow().get(&var_id) {\n             Some(&r) => r,\n             None => { panic!(\"no enclosing scope for id {}\", var_id); }\n@@ -257,15 +246,12 @@ impl RegionMaps {\n         self.is_subscope_of(scope2, scope1)\n     }\n \n+    /// Returns true if `subscope` is equal to or is lexically nested inside `superscope` and false\n+    /// otherwise.\n     pub fn is_subscope_of(&self,\n                           subscope: CodeExtent,\n                           superscope: CodeExtent)\n                           -> bool {\n-        /*!\n-         * Returns true if `subscope` is equal to or is lexically\n-         * nested inside `superscope` and false otherwise.\n-         */\n-\n         let mut s = subscope;\n         while superscope != s {\n             match self.scope_map.borrow().get(&s) {\n@@ -285,27 +271,20 @@ impl RegionMaps {\n         return true;\n     }\n \n+    /// Determines whether two free regions have a subregion relationship\n+    /// by walking the graph encoded in `free_region_map`.  Note that\n+    /// it is possible that `sub != sup` and `sub <= sup` and `sup <= sub`\n+    /// (that is, the user can give two different names to the same lifetime).\n     pub fn sub_free_region(&self, sub: FreeRegion, sup: FreeRegion) -> bool {\n-        /*!\n-         * Determines whether two free regions have a subregion relationship\n-         * by walking the graph encoded in `free_region_map`.  Note that\n-         * it is possible that `sub != sup` and `sub <= sup` and `sup <= sub`\n-         * (that is, the user can give two different names to the same lifetime).\n-         */\n-\n         can_reach(&*self.free_region_map.borrow(), sub, sup)\n     }\n \n+    /// Determines whether one region is a subregion of another.  This is intended to run *after\n+    /// inference* and sadly the logic is somewhat duplicated with the code in infer.rs.\n     pub fn is_subregion_of(&self,\n                            sub_region: ty::Region,\n                            super_region: ty::Region)\n                            -> bool {\n-        /*!\n-         * Determines whether one region is a subregion of another.  This is\n-         * intended to run *after inference* and sadly the logic is somewhat\n-         * duplicated with the code in infer.rs.\n-         */\n-\n         debug!(\"is_subregion_of(sub_region={}, super_region={})\",\n                sub_region, super_region);\n \n@@ -345,16 +324,12 @@ impl RegionMaps {\n         }\n     }\n \n+    /// Finds the nearest common ancestor (if any) of two scopes.  That is, finds the smallest\n+    /// scope which is greater than or equal to both `scope_a` and `scope_b`.\n     pub fn nearest_common_ancestor(&self,\n                                    scope_a: CodeExtent,\n                                    scope_b: CodeExtent)\n                                    -> Option<CodeExtent> {\n-        /*!\n-         * Finds the nearest common ancestor (if any) of two scopes.  That\n-         * is, finds the smallest scope which is greater than or equal to\n-         * both `scope_a` and `scope_b`.\n-         */\n-\n         if scope_a == scope_b { return Some(scope_a); }\n \n         let a_ancestors = ancestors_of(self, scope_a);\n@@ -681,18 +656,15 @@ fn resolve_local(visitor: &mut RegionResolutionVisitor, local: &ast::Local) {\n \n     visit::walk_local(visitor, local);\n \n+    /// True if `pat` match the `P&` nonterminal:\n+    ///\n+    ///     P& = ref X\n+    ///        | StructName { ..., P&, ... }\n+    ///        | VariantName(..., P&, ...)\n+    ///        | [ ..., P&, ... ]\n+    ///        | ( ..., P&, ... )\n+    ///        | box P&\n     fn is_binding_pat(pat: &ast::Pat) -> bool {\n-        /*!\n-         * True if `pat` match the `P&` nonterminal:\n-         *\n-         *     P& = ref X\n-         *        | StructName { ..., P&, ... }\n-         *        | VariantName(..., P&, ...)\n-         *        | [ ..., P&, ... ]\n-         *        | ( ..., P&, ... )\n-         *        | box P&\n-         */\n-\n         match pat.node {\n             ast::PatIdent(ast::BindByRef(_), _, _) => true,\n \n@@ -719,35 +691,27 @@ fn resolve_local(visitor: &mut RegionResolutionVisitor, local: &ast::Local) {\n         }\n     }\n \n+    /// True if `ty` is a borrowed pointer type like `&int` or `&[...]`.\n     fn is_borrowed_ty(ty: &ast::Ty) -> bool {\n-        /*!\n-         * True if `ty` is a borrowed pointer type\n-         * like `&int` or `&[...]`.\n-         */\n-\n         match ty.node {\n             ast::TyRptr(..) => true,\n             _ => false\n         }\n     }\n \n+    /// If `expr` matches the `E&` grammar, then records an extended rvalue scope as appropriate:\n+    ///\n+    ///     E& = & ET\n+    ///        | StructName { ..., f: E&, ... }\n+    ///        | [ ..., E&, ... ]\n+    ///        | ( ..., E&, ... )\n+    ///        | {...; E&}\n+    ///        | box E&\n+    ///        | E& as ...\n+    ///        | ( E& )\n     fn record_rvalue_scope_if_borrow_expr(visitor: &mut RegionResolutionVisitor,\n                                           expr: &ast::Expr,\n                                           blk_id: CodeExtent) {\n-        /*!\n-         * If `expr` matches the `E&` grammar, then records an extended\n-         * rvalue scope as appropriate:\n-         *\n-         *     E& = & ET\n-         *        | StructName { ..., f: E&, ... }\n-         *        | [ ..., E&, ... ]\n-         *        | ( ..., E&, ... )\n-         *        | {...; E&}\n-         *        | box E&\n-         *        | E& as ...\n-         *        | ( E& )\n-         */\n-\n         match expr.node {\n             ast::ExprAddrOf(_, ref subexpr) => {\n                 record_rvalue_scope_if_borrow_expr(visitor, &**subexpr, blk_id);\n@@ -787,29 +751,24 @@ fn resolve_local(visitor: &mut RegionResolutionVisitor, local: &ast::Local) {\n         }\n     }\n \n+    /// Applied to an expression `expr` if `expr` -- or something owned or partially owned by\n+    /// `expr` -- is going to be indirectly referenced by a variable in a let statement. In that\n+    /// case, the \"temporary lifetime\" or `expr` is extended to be the block enclosing the `let`\n+    /// statement.\n+    ///\n+    /// More formally, if `expr` matches the grammar `ET`, record the rvalue scope of the matching\n+    /// `<rvalue>` as `blk_id`:\n+    ///\n+    ///     ET = *ET\n+    ///        | ET[...]\n+    ///        | ET.f\n+    ///        | (ET)\n+    ///        | <rvalue>\n+    ///\n+    /// Note: ET is intended to match \"rvalues or lvalues based on rvalues\".\n     fn record_rvalue_scope<'a>(visitor: &mut RegionResolutionVisitor,\n                                expr: &'a ast::Expr,\n                                blk_scope: CodeExtent) {\n-        /*!\n-         * Applied to an expression `expr` if `expr` -- or something\n-         * owned or partially owned by `expr` -- is going to be\n-         * indirectly referenced by a variable in a let statement. In\n-         * that case, the \"temporary lifetime\" or `expr` is extended\n-         * to be the block enclosing the `let` statement.\n-         *\n-         * More formally, if `expr` matches the grammar `ET`, record\n-         * the rvalue scope of the matching `<rvalue>` as `blk_id`:\n-         *\n-         *     ET = *ET\n-         *        | ET[...]\n-         *        | ET.f\n-         *        | (ET)\n-         *        | <rvalue>\n-         *\n-         * Note: ET is intended to match \"rvalues or\n-         * lvalues based on rvalues\".\n-         */\n-\n         let mut expr = expr;\n         loop {\n             // Note: give all the expressions matching `ET` with the"}, {"sha": "9c32410ecbfaf187ccb2d0ae4eee0868127293fe", "filename": "src/librustc/middle/resolve_lifetime.rs", "status": "modified", "additions": 25, "deletions": 38, "changes": 63, "blob_url": "https://github.com/rust-lang/rust/blob/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc%2Fmiddle%2Fresolve_lifetime.rs", "raw_url": "https://github.com/rust-lang/rust/raw/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc%2Fmiddle%2Fresolve_lifetime.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Fmiddle%2Fresolve_lifetime.rs?ref=cd5c8235c5448a7234548c772468c8d2e8f150d9", "patch": "@@ -8,14 +8,12 @@\n // option. This file may not be copied, modified, or distributed\n // except according to those terms.\n \n-/*!\n- * Name resolution for lifetimes.\n- *\n- * Name resolution for lifetimes follows MUCH simpler rules than the\n- * full resolve. For example, lifetime names are never exported or\n- * used between functions, and they operate in a purely top-down\n- * way. Therefore we break lifetime name resolution into a separate pass.\n- */\n+//! Name resolution for lifetimes.\n+//!\n+//! Name resolution for lifetimes follows MUCH simpler rules than the\n+//! full resolve. For example, lifetime names are never exported or\n+//! used between functions, and they operate in a purely top-down\n+//! way. Therefore we break lifetime name resolution into a separate pass.\n \n pub use self::DefRegion::*;\n use self::ScopeChain::*;\n@@ -254,34 +252,27 @@ impl<'a> LifetimeContext<'a> {\n     }\n \n     /// Visits self by adding a scope and handling recursive walk over the contents with `walk`.\n+    ///\n+    /// Handles visiting fns and methods. These are a bit complicated because we must distinguish\n+    /// early- vs late-bound lifetime parameters. We do this by checking which lifetimes appear\n+    /// within type bounds; those are early bound lifetimes, and the rest are late bound.\n+    ///\n+    /// For example:\n+    ///\n+    ///    fn foo<'a,'b,'c,T:Trait<'b>>(...)\n+    ///\n+    /// Here `'a` and `'c` are late bound but `'b` is early bound. Note that early- and late-bound\n+    /// lifetimes may be interspersed together.\n+    ///\n+    /// If early bound lifetimes are present, we separate them into their own list (and likewise\n+    /// for late bound). They will be numbered sequentially, starting from the lowest index that is\n+    /// already in scope (for a fn item, that will be 0, but for a method it might not be). Late\n+    /// bound lifetimes are resolved by name and associated with a binder id (`binder_id`), so the\n+    /// ordering is not important there.\n     fn visit_early_late(&mut self,\n                         early_space: subst::ParamSpace,\n                         generics: &ast::Generics,\n                         walk: |&mut LifetimeContext|) {\n-        /*!\n-         * Handles visiting fns and methods. These are a bit\n-         * complicated because we must distinguish early- vs late-bound\n-         * lifetime parameters. We do this by checking which lifetimes\n-         * appear within type bounds; those are early bound lifetimes,\n-         * and the rest are late bound.\n-         *\n-         * For example:\n-         *\n-         *    fn foo<'a,'b,'c,T:Trait<'b>>(...)\n-         *\n-         * Here `'a` and `'c` are late bound but `'b` is early\n-         * bound. Note that early- and late-bound lifetimes may be\n-         * interspersed together.\n-         *\n-         * If early bound lifetimes are present, we separate them into\n-         * their own list (and likewise for late bound). They will be\n-         * numbered sequentially, starting from the lowest index that\n-         * is already in scope (for a fn item, that will be 0, but for\n-         * a method it might not be). Late bound lifetimes are\n-         * resolved by name and associated with a binder id (`binder_id`), so\n-         * the ordering is not important there.\n-         */\n-\n         let referenced_idents = early_bound_lifetime_names(generics);\n \n         debug!(\"visit_early_late: referenced_idents={}\",\n@@ -479,13 +470,9 @@ pub fn early_bound_lifetimes<'a>(generics: &'a ast::Generics) -> Vec<ast::Lifeti\n         .collect()\n }\n \n+/// Given a set of generic declarations, returns a list of names containing all early bound\n+/// lifetime names for those generics. (In fact, this list may also contain other names.)\n fn early_bound_lifetime_names(generics: &ast::Generics) -> Vec<ast::Name> {\n-    /*!\n-     * Given a set of generic declarations, returns a list of names\n-     * containing all early bound lifetime names for those\n-     * generics. (In fact, this list may also contain other names.)\n-     */\n-\n     // Create two lists, dividing the lifetimes into early/late bound.\n     // Initially, all of them are considered late, but we will move\n     // things from late into early as we go if we find references to"}, {"sha": "365c2ed39dbc006c9925f4183dec3e1ce5a74a45", "filename": "src/librustc/middle/subst.rs", "status": "modified", "additions": 46, "deletions": 64, "changes": 110, "blob_url": "https://github.com/rust-lang/rust/blob/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc%2Fmiddle%2Fsubst.rs", "raw_url": "https://github.com/rust-lang/rust/raw/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc%2Fmiddle%2Fsubst.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Fmiddle%2Fsubst.rs?ref=cd5c8235c5448a7234548c772468c8d2e8f150d9", "patch": "@@ -131,26 +131,18 @@ pub fn self_ty(&self) -> Option<Ty<'tcx>> {\n         Substs { types: types, regions: ErasedRegions }\n     }\n \n+    /// Since ErasedRegions are only to be used in trans, most of the compiler can use this method\n+    /// to easily access the set of region substitutions.\n     pub fn regions<'a>(&'a self) -> &'a VecPerParamSpace<ty::Region> {\n-        /*!\n-         * Since ErasedRegions are only to be used in trans, most of\n-         * the compiler can use this method to easily access the set\n-         * of region substitutions.\n-         */\n-\n         match self.regions {\n             ErasedRegions => panic!(\"Erased regions only expected in trans\"),\n             NonerasedRegions(ref r) => r\n         }\n     }\n \n+    /// Since ErasedRegions are only to be used in trans, most of the compiler can use this method\n+    /// to easily access the set of region substitutions.\n     pub fn mut_regions<'a>(&'a mut self) -> &'a mut VecPerParamSpace<ty::Region> {\n-        /*!\n-         * Since ErasedRegions are only to be used in trans, most of\n-         * the compiler can use this method to easily access the set\n-         * of region substitutions.\n-         */\n-\n         match self.regions {\n             ErasedRegions => panic!(\"Erased regions only expected in trans\"),\n             NonerasedRegions(ref mut r) => r\n@@ -688,59 +680,49 @@ impl<'a,'tcx> SubstFolder<'a,'tcx> {\n         self.shift_regions_through_binders(ty)\n     }\n \n+    /// It is sometimes necessary to adjust the debruijn indices during substitution. This occurs\n+    /// when we are substituting a type with escaping regions into a context where we have passed\n+    /// through region binders. That's quite a mouthful. Let's see an example:\n+    ///\n+    /// ```\n+    /// type Func<A> = fn(A);\n+    /// type MetaFunc = for<'a> fn(Func<&'a int>)\n+    /// ```\n+    ///\n+    /// The type `MetaFunc`, when fully expanded, will be\n+    ///\n+    ///     for<'a> fn(fn(&'a int))\n+    ///             ^~ ^~ ^~~\n+    ///             |  |  |\n+    ///             |  |  DebruijnIndex of 2\n+    ///             Binders\n+    ///\n+    /// Here the `'a` lifetime is bound in the outer function, but appears as an argument of the\n+    /// inner one. Therefore, that appearance will have a DebruijnIndex of 2, because we must skip\n+    /// over the inner binder (remember that we count Debruijn indices from 1). However, in the\n+    /// definition of `MetaFunc`, the binder is not visible, so the type `&'a int` will have a\n+    /// debruijn index of 1. It's only during the substitution that we can see we must increase the\n+    /// depth by 1 to account for the binder that we passed through.\n+    ///\n+    /// As a second example, consider this twist:\n+    ///\n+    /// ```\n+    /// type FuncTuple<A> = (A,fn(A));\n+    /// type MetaFuncTuple = for<'a> fn(FuncTuple<&'a int>)\n+    /// ```\n+    ///\n+    /// Here the final type will be:\n+    ///\n+    ///     for<'a> fn((&'a int, fn(&'a int)))\n+    ///                 ^~~         ^~~\n+    ///                 |           |\n+    ///          DebruijnIndex of 1 |\n+    ///                      DebruijnIndex of 2\n+    ///\n+    /// As indicated in the diagram, here the same type `&'a int` is substituted once, but in the\n+    /// first case we do not increase the Debruijn index and in the second case we do. The reason\n+    /// is that only in the second case have we passed through a fn binder.\n     fn shift_regions_through_binders(&self, ty: Ty<'tcx>) -> Ty<'tcx> {\n-        /*!\n-         * It is sometimes necessary to adjust the debruijn indices\n-         * during substitution. This occurs when we are substituting a\n-         * type with escaping regions into a context where we have\n-         * passed through region binders. That's quite a\n-         * mouthful. Let's see an example:\n-         *\n-         * ```\n-         * type Func<A> = fn(A);\n-         * type MetaFunc = for<'a> fn(Func<&'a int>)\n-         * ```\n-         *\n-         * The type `MetaFunc`, when fully expanded, will be\n-         *\n-         *     for<'a> fn(fn(&'a int))\n-         *             ^~ ^~ ^~~\n-         *             |  |  |\n-         *             |  |  DebruijnIndex of 2\n-         *             Binders\n-         *\n-         * Here the `'a` lifetime is bound in the outer function, but\n-         * appears as an argument of the inner one. Therefore, that\n-         * appearance will have a DebruijnIndex of 2, because we must\n-         * skip over the inner binder (remember that we count Debruijn\n-         * indices from 1). However, in the definition of `MetaFunc`,\n-         * the binder is not visible, so the type `&'a int` will have\n-         * a debruijn index of 1. It's only during the substitution\n-         * that we can see we must increase the depth by 1 to account\n-         * for the binder that we passed through.\n-         *\n-         * As a second example, consider this twist:\n-         *\n-         * ```\n-         * type FuncTuple<A> = (A,fn(A));\n-         * type MetaFuncTuple = for<'a> fn(FuncTuple<&'a int>)\n-         * ```\n-         *\n-         * Here the final type will be:\n-         *\n-         *     for<'a> fn((&'a int, fn(&'a int)))\n-         *                 ^~~         ^~~\n-         *                 |           |\n-         *          DebruijnIndex of 1 |\n-         *                      DebruijnIndex of 2\n-         *\n-         * As indicated in the diagram, here the same type `&'a int`\n-         * is substituted once, but in the first case we do not\n-         * increase the Debruijn index and in the second case we\n-         * do. The reason is that only in the second case have we\n-         * passed through a fn binder.\n-         */\n-\n         debug!(\"shift_regions(ty={}, region_binders_passed={}, type_has_escaping_regions={})\",\n                ty.repr(self.tcx()), self.region_binders_passed, ty::type_has_escaping_regions(ty));\n "}, {"sha": "048f394224cf0b1d2e8e2133a63d4083b72c5145", "filename": "src/librustc/middle/traits/coherence.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc%2Fmiddle%2Ftraits%2Fcoherence.rs", "raw_url": "https://github.com/rust-lang/rust/raw/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc%2Fmiddle%2Ftraits%2Fcoherence.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Fmiddle%2Ftraits%2Fcoherence.rs?ref=cd5c8235c5448a7234548c772468c8d2e8f150d9", "patch": "@@ -8,7 +8,7 @@\n // option. This file may not be copied, modified, or distributed\n // except according to those terms.\n \n-/*! See `doc.rs` for high-level documentation */\n+//! See `doc.rs` for high-level documentation\n \n use super::SelectionContext;\n use super::Obligation;"}, {"sha": "62246b77ee9409b0c168806403ec4b1b85e8e71e", "filename": "src/librustc/middle/traits/doc.rs", "status": "modified", "additions": 396, "deletions": 400, "changes": 796, "blob_url": "https://github.com/rust-lang/rust/blob/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc%2Fmiddle%2Ftraits%2Fdoc.rs", "raw_url": "https://github.com/rust-lang/rust/raw/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc%2Fmiddle%2Ftraits%2Fdoc.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Fmiddle%2Ftraits%2Fdoc.rs?ref=cd5c8235c5448a7234548c772468c8d2e8f150d9", "patch": "@@ -8,403 +8,399 @@\n // option. This file may not be copied, modified, or distributed\n // except according to those terms.\n \n-/*!\n-\n-# TRAIT RESOLUTION\n-\n-This document describes the general process and points out some non-obvious\n-things.\n-\n-## Major concepts\n-\n-Trait resolution is the process of pairing up an impl with each\n-reference to a trait. So, for example, if there is a generic function like:\n-\n-    fn clone_slice<T:Clone>(x: &[T]) -> Vec<T> { ... }\n-\n-and then a call to that function:\n-\n-    let v: Vec<int> = clone_slice([1, 2, 3].as_slice())\n-\n-it is the job of trait resolution to figure out (in which case)\n-whether there exists an impl of `int : Clone`\n-\n-Note that in some cases, like generic functions, we may not be able to\n-find a specific impl, but we can figure out that the caller must\n-provide an impl. To see what I mean, consider the body of `clone_slice`:\n-\n-    fn clone_slice<T:Clone>(x: &[T]) -> Vec<T> {\n-        let mut v = Vec::new();\n-        for e in x.iter() {\n-            v.push((*e).clone()); // (*)\n-        }\n-    }\n-\n-The line marked `(*)` is only legal if `T` (the type of `*e`)\n-implements the `Clone` trait. Naturally, since we don't know what `T`\n-is, we can't find the specific impl; but based on the bound `T:Clone`,\n-we can say that there exists an impl which the caller must provide.\n-\n-We use the term *obligation* to refer to a trait reference in need of\n-an impl.\n-\n-## Overview\n-\n-Trait resolution consists of three major parts:\n-\n-- SELECTION: Deciding how to resolve a specific obligation. For\n-  example, selection might decide that a specific obligation can be\n-  resolved by employing an impl which matches the self type, or by\n-  using a parameter bound. In the case of an impl, Selecting one\n-  obligation can create *nested obligations* because of where clauses\n-  on the impl itself. It may also require evaluating those nested\n-  obligations to resolve ambiguities.\n-\n-- FULFILLMENT: The fulfillment code is what tracks that obligations\n-  are completely fulfilled. Basically it is a worklist of obligations\n-  to be selected: once selection is successful, the obligation is\n-  removed from the worklist and any nested obligations are enqueued.\n-\n-- COHERENCE: The coherence checks are intended to ensure that there\n-  are never overlapping impls, where two impls could be used with\n-  equal precedence.\n-\n-## Selection\n-\n-Selection is the process of deciding whether an obligation can be\n-resolved and, if so, how it is to be resolved (via impl, where clause, etc).\n-The main interface is the `select()` function, which takes an obligation\n-and returns a `SelectionResult`. There are three possible outcomes:\n-\n-- `Ok(Some(selection))` -- yes, the obligation can be resolved, and\n-  `selection` indicates how. If the impl was resolved via an impl,\n-  then `selection` may also indicate nested obligations that are required\n-  by the impl.\n-\n-- `Ok(None)` -- we are not yet sure whether the obligation can be\n-  resolved or not. This happens most commonly when the obligation\n-  contains unbound type variables.\n-\n-- `Err(err)` -- the obligation definitely cannot be resolved due to a\n-  type error, or because there are no impls that could possibly apply,\n-  etc.\n-\n-The basic algorithm for selection is broken into two big phases:\n-candidate assembly and confirmation.\n-\n-### Candidate assembly\n-\n-Searches for impls/where-clauses/etc that might\n-possibly be used to satisfy the obligation. Each of those is called\n-a candidate. To avoid ambiguity, we want to find exactly one\n-candidate that is definitively applicable. In some cases, we may not\n-know whether an impl/where-clause applies or not -- this occurs when\n-the obligation contains unbound inference variables.\n-\n-The basic idea for candidate assembly is to do a first pass in which\n-we identify all possible candidates. During this pass, all that we do\n-is try and unify the type parameters. (In particular, we ignore any\n-nested where clauses.) Presuming that this unification succeeds, the\n-impl is added as a candidate.\n-\n-Once this first pass is done, we can examine the set of candidates. If\n-it is a singleton set, then we are done: this is the only impl in\n-scope that could possibly apply. Otherwise, we can winnow down the set\n-of candidates by using where clauses and other conditions. If this\n-reduced set yields a single, unambiguous entry, we're good to go,\n-otherwise the result is considered ambiguous.\n-\n-#### The basic process: Inferring based on the impls we see\n-\n-This process is easier if we work through some examples. Consider\n-the following trait:\n-\n-```\n-trait Convert<Target> {\n-    fn convert(&self) -> Target;\n-}\n-```\n-\n-This trait just has one method. It's about as simple as it gets. It\n-converts from the (implicit) `Self` type to the `Target` type. If we\n-wanted to permit conversion between `int` and `uint`, we might\n-implement `Convert` like so:\n-\n-```rust\n-impl Convert<uint> for int { ... } // int -> uint\n-impl Convert<int> for uint { ... } // uint -> uint\n-```\n-\n-Now imagine there is some code like the following:\n-\n-```rust\n-let x: int = ...;\n-let y = x.convert();\n-```\n-\n-The call to convert will generate a trait reference `Convert<$Y> for\n-int`, where `$Y` is the type variable representing the type of\n-`y`. When we match this against the two impls we can see, we will find\n-that only one remains: `Convert<uint> for int`. Therefore, we can\n-select this impl, which will cause the type of `$Y` to be unified to\n-`uint`. (Note that while assembling candidates, we do the initial\n-unifications in a transaction, so that they don't affect one another.)\n-\n-There are tests to this effect in src/test/run-pass:\n-\n-   traits-multidispatch-infer-convert-source-and-target.rs\n-   traits-multidispatch-infer-convert-target.rs\n-\n-#### Winnowing: Resolving ambiguities\n-\n-But what happens if there are multiple impls where all the types\n-unify? Consider this example:\n-\n-```rust\n-trait Get {\n-    fn get(&self) -> Self;\n-}\n-\n-impl<T:Copy> Get for T {\n-    fn get(&self) -> T { *self }\n-}\n-\n-impl<T:Get> Get for Box<T> {\n-    fn get(&self) -> Box<T> { box get_it(&**self) }\n-}\n-```\n-\n-What happens when we invoke `get_it(&box 1_u16)`, for example? In this\n-case, the `Self` type is `Box<u16>` -- that unifies with both impls,\n-because the first applies to all types, and the second to all\n-boxes. In the olden days we'd have called this ambiguous. But what we\n-do now is do a second *winnowing* pass that considers where clauses\n-and attempts to remove candidates -- in this case, the first impl only\n-applies if `Box<u16> : Copy`, which doesn't hold. After winnowing,\n-then, we are left with just one candidate, so we can proceed. There is\n-a test of this in `src/test/run-pass/traits-conditional-dispatch.rs`.\n-\n-#### Matching\n-\n-The subroutines that decide whether a particular impl/where-clause/etc\n-applies to a particular obligation. At the moment, this amounts to\n-unifying the self types, but in the future we may also recursively\n-consider some of the nested obligations, in the case of an impl.\n-\n-#### Lifetimes and selection\n-\n-Because of how that lifetime inference works, it is not possible to\n-give back immediate feedback as to whether a unification or subtype\n-relationship between lifetimes holds or not. Therefore, lifetime\n-matching is *not* considered during selection. This is reflected in\n-the fact that subregion assignment is infallible. This may yield\n-lifetime constraints that will later be found to be in error (in\n-contrast, the non-lifetime-constraints have already been checked\n-during selection and can never cause an error, though naturally they\n-may lead to other errors downstream).\n-\n-#### Where clauses\n-\n-Besides an impl, the other major way to resolve an obligation is via a\n-where clause. The selection process is always given a *parameter\n-environment* which contains a list of where clauses, which are\n-basically obligations that can assume are satisfiable. We will iterate\n-over that list and check whether our current obligation can be found\n-in that list, and if so it is considered satisfied. More precisely, we\n-want to check whether there is a where-clause obligation that is for\n-the same trait (or some subtrait) and for which the self types match,\n-using the definition of *matching* given above.\n-\n-Consider this simple example:\n-\n-     trait A1 { ... }\n-     trait A2 : A1 { ... }\n-\n-     trait B { ... }\n-\n-     fn foo<X:A2+B> { ... }\n-\n-Clearly we can use methods offered by `A1`, `A2`, or `B` within the\n-body of `foo`. In each case, that will incur an obligation like `X :\n-A1` or `X : A2`. The parameter environment will contain two\n-where-clauses, `X : A2` and `X : B`. For each obligation, then, we\n-search this list of where-clauses.  To resolve an obligation `X:A1`,\n-we would note that `X:A2` implies that `X:A1`.\n-\n-### Confirmation\n-\n-Confirmation unifies the output type parameters of the trait with the\n-values found in the obligation, possibly yielding a type error.  If we\n-return to our example of the `Convert` trait from the previous\n-section, confirmation is where an error would be reported, because the\n-impl specified that `T` would be `uint`, but the obligation reported\n-`char`. Hence the result of selection would be an error.\n-\n-### Selection during translation\n-\n-During type checking, we do not store the results of trait selection.\n-We simply wish to verify that trait selection will succeed. Then\n-later, at trans time, when we have all concrete types available, we\n-can repeat the trait selection.  In this case, we do not consider any\n-where-clauses to be in scope. We know that therefore each resolution\n-will resolve to a particular impl.\n-\n-One interesting twist has to do with nested obligations. In general, in trans,\n-we only need to do a \"shallow\" selection for an obligation. That is, we wish to\n-identify which impl applies, but we do not (yet) need to decide how to select\n-any nested obligations. Nonetheless, we *do* currently do a complete resolution,\n-and that is because it can sometimes inform the results of type inference. That is,\n-we do not have the full substitutions in terms of the type varibales of the impl available\n-to us, so we must run trait selection to figure everything out.\n-\n-Here is an example:\n-\n-    trait Foo { ... }\n-    impl<U,T:Bar<U>> Foo for Vec<T> { ... }\n-\n-    impl Bar<uint> for int { ... }\n-\n-After one shallow round of selection for an obligation like `Vec<int>\n-: Foo`, we would know which impl we want, and we would know that\n-`T=int`, but we do not know the type of `U`.  We must select the\n-nested obligation `int : Bar<U>` to find out that `U=uint`.\n-\n-It would be good to only do *just as much* nested resolution as\n-necessary. Currently, though, we just do a full resolution.\n-\n-## Method matching\n-\n-Method dispach follows a slightly different path than normal trait\n-selection. This is because it must account for the transformed self\n-type of the receiver and various other complications. The procedure is\n-described in `select.rs` in the \"METHOD MATCHING\" section.\n-\n-# Caching and subtle considerations therewith\n-\n-In general we attempt to cache the results of trait selection.  This\n-is a somewhat complex process. Part of the reason for this is that we\n-want to be able to cache results even when all the types in the trait\n-reference are not fully known. In that case, it may happen that the\n-trait selection process is also influencing type variables, so we have\n-to be able to not only cache the *result* of the selection process,\n-but *replay* its effects on the type variables.\n-\n-## An example\n-\n-The high-level idea of how the cache works is that we first replace\n-all unbound inference variables with skolemized versions. Therefore,\n-if we had a trait reference `uint : Foo<$1>`, where `$n` is an unbound\n-inference variable, we might replace it with `uint : Foo<%0>`, where\n-`%n` is a skolemized type. We would then look this up in the cache.\n-If we found a hit, the hit would tell us the immediate next step to\n-take in the selection process: i.e., apply impl #22, or apply where\n-clause `X : Foo<Y>`. Let's say in this case there is no hit.\n-Therefore, we search through impls and where clauses and so forth, and\n-we come to the conclusion that the only possible impl is this one,\n-with def-id 22:\n-\n-    impl Foo<int> for uint { ... } // Impl #22\n-\n-We would then record in the cache `uint : Foo<%0> ==>\n-ImplCandidate(22)`. Next we would confirm `ImplCandidate(22)`, which\n-would (as a side-effect) unify `$1` with `int`.\n-\n-Now, at some later time, we might come along and see a `uint :\n-Foo<$3>`.  When skolemized, this would yield `uint : Foo<%0>`, just as\n-before, and hence the cache lookup would succeed, yielding\n-`ImplCandidate(22)`. We would confirm `ImplCandidate(22)` which would\n-(as a side-effect) unify `$3` with `int`.\n-\n-## Where clauses and the local vs global cache\n-\n-One subtle interaction is that the results of trait lookup will vary\n-depending on what where clauses are in scope. Therefore, we actually\n-have *two* caches, a local and a global cache. The local cache is\n-attached to the `ParameterEnvironment` and the global cache attached\n-to the `tcx`. We use the local cache whenever the result might depend\n-on the where clauses that are in scope. The determination of which\n-cache to use is done by the method `pick_candidate_cache` in\n-`select.rs`.\n-\n-There are two cases where we currently use the local cache. The\n-current rules are probably more conservative than necessary.\n-\n-### Trait references that involve parameter types\n-\n-The most obvious case where you need the local environment is\n-when the trait reference includes parameter types. For example,\n-consider the following function:\n-\n-    impl<T> Vec<T> {\n-        fn foo(x: T)\n-            where T : Foo\n-        { ... }\n-\n-        fn bar(x: T)\n-        { ... }\n-    }\n-\n-If there is an obligation `T : Foo`, or `int : Bar<T>`, or whatever,\n-clearly the results from `foo` and `bar` are potentially different,\n-since the set of where clauses in scope are different.\n-\n-### Trait references with unbound variables when where clauses are in scope\n-\n-There is another less obvious interaction which involves unbound variables\n-where *only* where clauses are in scope (no impls). This manifested as\n-issue #18209 (`run-pass/trait-cache-issue-18209.rs`). Consider\n-this snippet:\n-\n-```\n-pub trait Foo {\n-    fn load_from() -> Box<Self>;\n-    fn load() -> Box<Self> {\n-        Foo::load_from()\n-    }\n-}\n-```\n-\n-The default method will incur an obligation `$0 : Foo` from the call\n-to `load_from`. If there are no impls, this can be eagerly resolved to\n-`VtableParam(Self : Foo)` and cached. Because the trait reference\n-doesn't involve any parameters types (only the resolution does), this\n-result was stored in the global cache, causing later calls to\n-`Foo::load_from()` to get nonsense.\n-\n-To fix this, we always use the local cache if there are unbound\n-variables and where clauses in scope. This is more conservative than\n-necessary as far as I can tell. However, it still seems to be a simple\n-rule and I observe ~99% hit rate on rustc, so it doesn't seem to hurt\n-us in particular.\n-\n-Here is an example of the kind of subtle case that I would be worried\n-about with a more complex rule (although this particular case works\n-out ok). Imagine the trait reference doesn't directly reference a\n-where clause, but the where clause plays a role in the winnowing\n-phase. Something like this:\n-\n-```\n-pub trait Foo<T> { ... }\n-pub trait Bar { ... }\n-impl<U,T:Bar> Foo<U> for T { ... } // Impl A\n-impl Foo<char> for uint { ... }    // Impl B\n-```\n-\n-Now, in some function, we have no where clauses in scope, and we have\n-an obligation `$1 : Foo<$0>`. We might then conclude that `$0=char`\n-and `$1=uint`: this is because for impl A to apply, `uint:Bar` would\n-have to hold, and we know it does not or else the coherence check\n-would have failed.  So we might enter into our global cache: `$1 :\n-Foo<$0> => Impl B`.  Then we come along in a different scope, where a\n-generic type `A` is around with the bound `A:Bar`. Now suddenly the\n-impl is viable.\n-\n-The flaw in this imaginary DOOMSDAY SCENARIO is that we would not\n-currently conclude that `$1 : Foo<$0>` implies that `$0 == uint` and\n-`$1 == char`, even though it is true that (absent type parameters)\n-there is no other type the user could enter. However, it is not\n-*completely* implausible that we *could* draw this conclusion in the\n-future; we wouldn't have to guess types, in particular, we could be\n-led by the impls.\n-\n-*/\n+//! # TRAIT RESOLUTION\n+//!\n+//! This document describes the general process and points out some non-obvious\n+//! things.\n+//!\n+//! ## Major concepts\n+//!\n+//! Trait resolution is the process of pairing up an impl with each\n+//! reference to a trait. So, for example, if there is a generic function like:\n+//!\n+//!     fn clone_slice<T:Clone>(x: &[T]) -> Vec<T> { ... }\n+//!\n+//! and then a call to that function:\n+//!\n+//!     let v: Vec<int> = clone_slice([1, 2, 3].as_slice())\n+//!\n+//! it is the job of trait resolution to figure out (in which case)\n+//! whether there exists an impl of `int : Clone`\n+//!\n+//! Note that in some cases, like generic functions, we may not be able to\n+//! find a specific impl, but we can figure out that the caller must\n+//! provide an impl. To see what I mean, consider the body of `clone_slice`:\n+//!\n+//!     fn clone_slice<T:Clone>(x: &[T]) -> Vec<T> {\n+//!         let mut v = Vec::new();\n+//!         for e in x.iter() {\n+//!             v.push((*e).clone()); // (*)\n+//!         }\n+//!     }\n+//!\n+//! The line marked `(*)` is only legal if `T` (the type of `*e`)\n+//! implements the `Clone` trait. Naturally, since we don't know what `T`\n+//! is, we can't find the specific impl; but based on the bound `T:Clone`,\n+//! we can say that there exists an impl which the caller must provide.\n+//!\n+//! We use the term *obligation* to refer to a trait reference in need of\n+//! an impl.\n+//!\n+//! ## Overview\n+//!\n+//! Trait resolution consists of three major parts:\n+//!\n+//! - SELECTION: Deciding how to resolve a specific obligation. For\n+//!   example, selection might decide that a specific obligation can be\n+//!   resolved by employing an impl which matches the self type, or by\n+//!   using a parameter bound. In the case of an impl, Selecting one\n+//!   obligation can create *nested obligations* because of where clauses\n+//!   on the impl itself. It may also require evaluating those nested\n+//!   obligations to resolve ambiguities.\n+//!\n+//! - FULFILLMENT: The fulfillment code is what tracks that obligations\n+//!   are completely fulfilled. Basically it is a worklist of obligations\n+//!   to be selected: once selection is successful, the obligation is\n+//!   removed from the worklist and any nested obligations are enqueued.\n+//!\n+//! - COHERENCE: The coherence checks are intended to ensure that there\n+//!   are never overlapping impls, where two impls could be used with\n+//!   equal precedence.\n+//!\n+//! ## Selection\n+//!\n+//! Selection is the process of deciding whether an obligation can be\n+//! resolved and, if so, how it is to be resolved (via impl, where clause, etc).\n+//! The main interface is the `select()` function, which takes an obligation\n+//! and returns a `SelectionResult`. There are three possible outcomes:\n+//!\n+//! - `Ok(Some(selection))` -- yes, the obligation can be resolved, and\n+//!   `selection` indicates how. If the impl was resolved via an impl,\n+//!   then `selection` may also indicate nested obligations that are required\n+//!   by the impl.\n+//!\n+//! - `Ok(None)` -- we are not yet sure whether the obligation can be\n+//!   resolved or not. This happens most commonly when the obligation\n+//!   contains unbound type variables.\n+//!\n+//! - `Err(err)` -- the obligation definitely cannot be resolved due to a\n+//!   type error, or because there are no impls that could possibly apply,\n+//!   etc.\n+//!\n+//! The basic algorithm for selection is broken into two big phases:\n+//! candidate assembly and confirmation.\n+//!\n+//! ### Candidate assembly\n+//!\n+//! Searches for impls/where-clauses/etc that might\n+//! possibly be used to satisfy the obligation. Each of those is called\n+//! a candidate. To avoid ambiguity, we want to find exactly one\n+//! candidate that is definitively applicable. In some cases, we may not\n+//! know whether an impl/where-clause applies or not -- this occurs when\n+//! the obligation contains unbound inference variables.\n+//!\n+//! The basic idea for candidate assembly is to do a first pass in which\n+//! we identify all possible candidates. During this pass, all that we do\n+//! is try and unify the type parameters. (In particular, we ignore any\n+//! nested where clauses.) Presuming that this unification succeeds, the\n+//! impl is added as a candidate.\n+//!\n+//! Once this first pass is done, we can examine the set of candidates. If\n+//! it is a singleton set, then we are done: this is the only impl in\n+//! scope that could possibly apply. Otherwise, we can winnow down the set\n+//! of candidates by using where clauses and other conditions. If this\n+//! reduced set yields a single, unambiguous entry, we're good to go,\n+//! otherwise the result is considered ambiguous.\n+//!\n+//! #### The basic process: Inferring based on the impls we see\n+//!\n+//! This process is easier if we work through some examples. Consider\n+//! the following trait:\n+//!\n+//! ```\n+//! trait Convert<Target> {\n+//!     fn convert(&self) -> Target;\n+//! }\n+//! ```\n+//!\n+//! This trait just has one method. It's about as simple as it gets. It\n+//! converts from the (implicit) `Self` type to the `Target` type. If we\n+//! wanted to permit conversion between `int` and `uint`, we might\n+//! implement `Convert` like so:\n+//!\n+//! ```rust\n+//! impl Convert<uint> for int { ... } // int -> uint\n+//! impl Convert<int> for uint { ... } // uint -> uint\n+//! ```\n+//!\n+//! Now imagine there is some code like the following:\n+//!\n+//! ```rust\n+//! let x: int = ...;\n+//! let y = x.convert();\n+//! ```\n+//!\n+//! The call to convert will generate a trait reference `Convert<$Y> for\n+//! int`, where `$Y` is the type variable representing the type of\n+//! `y`. When we match this against the two impls we can see, we will find\n+//! that only one remains: `Convert<uint> for int`. Therefore, we can\n+//! select this impl, which will cause the type of `$Y` to be unified to\n+//! `uint`. (Note that while assembling candidates, we do the initial\n+//! unifications in a transaction, so that they don't affect one another.)\n+//!\n+//! There are tests to this effect in src/test/run-pass:\n+//!\n+//!    traits-multidispatch-infer-convert-source-and-target.rs\n+//!    traits-multidispatch-infer-convert-target.rs\n+//!\n+//! #### Winnowing: Resolving ambiguities\n+//!\n+//! But what happens if there are multiple impls where all the types\n+//! unify? Consider this example:\n+//!\n+//! ```rust\n+//! trait Get {\n+//!     fn get(&self) -> Self;\n+//! }\n+//!\n+//! impl<T:Copy> Get for T {\n+//!     fn get(&self) -> T { *self }\n+//! }\n+//!\n+//! impl<T:Get> Get for Box<T> {\n+//!     fn get(&self) -> Box<T> { box get_it(&**self) }\n+//! }\n+//! ```\n+//!\n+//! What happens when we invoke `get_it(&box 1_u16)`, for example? In this\n+//! case, the `Self` type is `Box<u16>` -- that unifies with both impls,\n+//! because the first applies to all types, and the second to all\n+//! boxes. In the olden days we'd have called this ambiguous. But what we\n+//! do now is do a second *winnowing* pass that considers where clauses\n+//! and attempts to remove candidates -- in this case, the first impl only\n+//! applies if `Box<u16> : Copy`, which doesn't hold. After winnowing,\n+//! then, we are left with just one candidate, so we can proceed. There is\n+//! a test of this in `src/test/run-pass/traits-conditional-dispatch.rs`.\n+//!\n+//! #### Matching\n+//!\n+//! The subroutines that decide whether a particular impl/where-clause/etc\n+//! applies to a particular obligation. At the moment, this amounts to\n+//! unifying the self types, but in the future we may also recursively\n+//! consider some of the nested obligations, in the case of an impl.\n+//!\n+//! #### Lifetimes and selection\n+//!\n+//! Because of how that lifetime inference works, it is not possible to\n+//! give back immediate feedback as to whether a unification or subtype\n+//! relationship between lifetimes holds or not. Therefore, lifetime\n+//! matching is *not* considered during selection. This is reflected in\n+//! the fact that subregion assignment is infallible. This may yield\n+//! lifetime constraints that will later be found to be in error (in\n+//! contrast, the non-lifetime-constraints have already been checked\n+//! during selection and can never cause an error, though naturally they\n+//! may lead to other errors downstream).\n+//!\n+//! #### Where clauses\n+//!\n+//! Besides an impl, the other major way to resolve an obligation is via a\n+//! where clause. The selection process is always given a *parameter\n+//! environment* which contains a list of where clauses, which are\n+//! basically obligations that can assume are satisfiable. We will iterate\n+//! over that list and check whether our current obligation can be found\n+//! in that list, and if so it is considered satisfied. More precisely, we\n+//! want to check whether there is a where-clause obligation that is for\n+//! the same trait (or some subtrait) and for which the self types match,\n+//! using the definition of *matching* given above.\n+//!\n+//! Consider this simple example:\n+//!\n+//!      trait A1 { ... }\n+//!      trait A2 : A1 { ... }\n+//!\n+//!      trait B { ... }\n+//!\n+//!      fn foo<X:A2+B> { ... }\n+//!\n+//! Clearly we can use methods offered by `A1`, `A2`, or `B` within the\n+//! body of `foo`. In each case, that will incur an obligation like `X :\n+//! A1` or `X : A2`. The parameter environment will contain two\n+//! where-clauses, `X : A2` and `X : B`. For each obligation, then, we\n+//! search this list of where-clauses.  To resolve an obligation `X:A1`,\n+//! we would note that `X:A2` implies that `X:A1`.\n+//!\n+//! ### Confirmation\n+//!\n+//! Confirmation unifies the output type parameters of the trait with the\n+//! values found in the obligation, possibly yielding a type error.  If we\n+//! return to our example of the `Convert` trait from the previous\n+//! section, confirmation is where an error would be reported, because the\n+//! impl specified that `T` would be `uint`, but the obligation reported\n+//! `char`. Hence the result of selection would be an error.\n+//!\n+//! ### Selection during translation\n+//!\n+//! During type checking, we do not store the results of trait selection.\n+//! We simply wish to verify that trait selection will succeed. Then\n+//! later, at trans time, when we have all concrete types available, we\n+//! can repeat the trait selection.  In this case, we do not consider any\n+//! where-clauses to be in scope. We know that therefore each resolution\n+//! will resolve to a particular impl.\n+//!\n+//! One interesting twist has to do with nested obligations. In general, in trans,\n+//! we only need to do a \"shallow\" selection for an obligation. That is, we wish to\n+//! identify which impl applies, but we do not (yet) need to decide how to select\n+//! any nested obligations. Nonetheless, we *do* currently do a complete resolution,\n+//! and that is because it can sometimes inform the results of type inference. That is,\n+//! we do not have the full substitutions in terms of the type varibales of the impl available\n+//! to us, so we must run trait selection to figure everything out.\n+//!\n+//! Here is an example:\n+//!\n+//!     trait Foo { ... }\n+//!     impl<U,T:Bar<U>> Foo for Vec<T> { ... }\n+//!\n+//!     impl Bar<uint> for int { ... }\n+//!\n+//! After one shallow round of selection for an obligation like `Vec<int>\n+//! : Foo`, we would know which impl we want, and we would know that\n+//! `T=int`, but we do not know the type of `U`.  We must select the\n+//! nested obligation `int : Bar<U>` to find out that `U=uint`.\n+//!\n+//! It would be good to only do *just as much* nested resolution as\n+//! necessary. Currently, though, we just do a full resolution.\n+//!\n+//! ## Method matching\n+//!\n+//! Method dispach follows a slightly different path than normal trait\n+//! selection. This is because it must account for the transformed self\n+//! type of the receiver and various other complications. The procedure is\n+//! described in `select.rs` in the \"METHOD MATCHING\" section.\n+//!\n+//! # Caching and subtle considerations therewith\n+//!\n+//! In general we attempt to cache the results of trait selection.  This\n+//! is a somewhat complex process. Part of the reason for this is that we\n+//! want to be able to cache results even when all the types in the trait\n+//! reference are not fully known. In that case, it may happen that the\n+//! trait selection process is also influencing type variables, so we have\n+//! to be able to not only cache the *result* of the selection process,\n+//! but *replay* its effects on the type variables.\n+//!\n+//! ## An example\n+//!\n+//! The high-level idea of how the cache works is that we first replace\n+//! all unbound inference variables with skolemized versions. Therefore,\n+//! if we had a trait reference `uint : Foo<$1>`, where `$n` is an unbound\n+//! inference variable, we might replace it with `uint : Foo<%0>`, where\n+//! `%n` is a skolemized type. We would then look this up in the cache.\n+//! If we found a hit, the hit would tell us the immediate next step to\n+//! take in the selection process: i.e., apply impl #22, or apply where\n+//! clause `X : Foo<Y>`. Let's say in this case there is no hit.\n+//! Therefore, we search through impls and where clauses and so forth, and\n+//! we come to the conclusion that the only possible impl is this one,\n+//! with def-id 22:\n+//!\n+//!     impl Foo<int> for uint { ... } // Impl #22\n+//!\n+//! We would then record in the cache `uint : Foo<%0> ==>\n+//! ImplCandidate(22)`. Next we would confirm `ImplCandidate(22)`, which\n+//! would (as a side-effect) unify `$1` with `int`.\n+//!\n+//! Now, at some later time, we might come along and see a `uint :\n+//! Foo<$3>`.  When skolemized, this would yield `uint : Foo<%0>`, just as\n+//! before, and hence the cache lookup would succeed, yielding\n+//! `ImplCandidate(22)`. We would confirm `ImplCandidate(22)` which would\n+//! (as a side-effect) unify `$3` with `int`.\n+//!\n+//! ## Where clauses and the local vs global cache\n+//!\n+//! One subtle interaction is that the results of trait lookup will vary\n+//! depending on what where clauses are in scope. Therefore, we actually\n+//! have *two* caches, a local and a global cache. The local cache is\n+//! attached to the `ParameterEnvironment` and the global cache attached\n+//! to the `tcx`. We use the local cache whenever the result might depend\n+//! on the where clauses that are in scope. The determination of which\n+//! cache to use is done by the method `pick_candidate_cache` in\n+//! `select.rs`.\n+//!\n+//! There are two cases where we currently use the local cache. The\n+//! current rules are probably more conservative than necessary.\n+//!\n+//! ### Trait references that involve parameter types\n+//!\n+//! The most obvious case where you need the local environment is\n+//! when the trait reference includes parameter types. For example,\n+//! consider the following function:\n+//!\n+//!     impl<T> Vec<T> {\n+//!         fn foo(x: T)\n+//!             where T : Foo\n+//!         { ... }\n+//!\n+//!         fn bar(x: T)\n+//!         { ... }\n+//!     }\n+//!\n+//! If there is an obligation `T : Foo`, or `int : Bar<T>`, or whatever,\n+//! clearly the results from `foo` and `bar` are potentially different,\n+//! since the set of where clauses in scope are different.\n+//!\n+//! ### Trait references with unbound variables when where clauses are in scope\n+//!\n+//! There is another less obvious interaction which involves unbound variables\n+//! where *only* where clauses are in scope (no impls). This manifested as\n+//! issue #18209 (`run-pass/trait-cache-issue-18209.rs`). Consider\n+//! this snippet:\n+//!\n+//! ```\n+//! pub trait Foo {\n+//!     fn load_from() -> Box<Self>;\n+//!     fn load() -> Box<Self> {\n+//!         Foo::load_from()\n+//!     }\n+//! }\n+//! ```\n+//!\n+//! The default method will incur an obligation `$0 : Foo` from the call\n+//! to `load_from`. If there are no impls, this can be eagerly resolved to\n+//! `VtableParam(Self : Foo)` and cached. Because the trait reference\n+//! doesn't involve any parameters types (only the resolution does), this\n+//! result was stored in the global cache, causing later calls to\n+//! `Foo::load_from()` to get nonsense.\n+//!\n+//! To fix this, we always use the local cache if there are unbound\n+//! variables and where clauses in scope. This is more conservative than\n+//! necessary as far as I can tell. However, it still seems to be a simple\n+//! rule and I observe ~99% hit rate on rustc, so it doesn't seem to hurt\n+//! us in particular.\n+//!\n+//! Here is an example of the kind of subtle case that I would be worried\n+//! about with a more complex rule (although this particular case works\n+//! out ok). Imagine the trait reference doesn't directly reference a\n+//! where clause, but the where clause plays a role in the winnowing\n+//! phase. Something like this:\n+//!\n+//! ```\n+//! pub trait Foo<T> { ... }\n+//! pub trait Bar { ... }\n+//! impl<U,T:Bar> Foo<U> for T { ... } // Impl A\n+//! impl Foo<char> for uint { ... }    // Impl B\n+//! ```\n+//!\n+//! Now, in some function, we have no where clauses in scope, and we have\n+//! an obligation `$1 : Foo<$0>`. We might then conclude that `$0=char`\n+//! and `$1=uint`: this is because for impl A to apply, `uint:Bar` would\n+//! have to hold, and we know it does not or else the coherence check\n+//! would have failed.  So we might enter into our global cache: `$1 :\n+//! Foo<$0> => Impl B`.  Then we come along in a different scope, where a\n+//! generic type `A` is around with the bound `A:Bar`. Now suddenly the\n+//! impl is viable.\n+//!\n+//! The flaw in this imaginary DOOMSDAY SCENARIO is that we would not\n+//! currently conclude that `$1 : Foo<$0>` implies that `$0 == uint` and\n+//! `$1 == char`, even though it is true that (absent type parameters)\n+//! there is no other type the user could enter. However, it is not\n+//! *completely* implausible that we *could* draw this conclusion in the\n+//! future; we wouldn't have to guess types, in particular, we could be\n+//! led by the impls."}, {"sha": "a22eba486e8b9f649f15bbc235d24096b5fe984a", "filename": "src/librustc/middle/traits/fulfill.rs", "status": "modified", "additions": 6, "deletions": 13, "changes": 19, "blob_url": "https://github.com/rust-lang/rust/blob/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc%2Fmiddle%2Ftraits%2Ffulfill.rs", "raw_url": "https://github.com/rust-lang/rust/raw/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc%2Fmiddle%2Ftraits%2Ffulfill.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Fmiddle%2Ftraits%2Ffulfill.rs?ref=cd5c8235c5448a7234548c772468c8d2e8f150d9", "patch": "@@ -81,20 +81,16 @@ impl<'tcx> FulfillmentContext<'tcx> {\n         }\n     }\n \n+    /// Attempts to select obligations that were registered since the call to a selection routine.\n+    /// This is used by the type checker to eagerly attempt to resolve obligations in hopes of\n+    /// gaining type information. It'd be equally valid to use `select_where_possible` but it\n+    /// results in `O(n^2)` performance (#18208).\n     pub fn select_new_obligations<'a>(&mut self,\n                                       infcx: &InferCtxt<'a,'tcx>,\n                                       param_env: &ty::ParameterEnvironment<'tcx>,\n                                       typer: &Typer<'tcx>)\n                                       -> Result<(),Vec<FulfillmentError<'tcx>>>\n     {\n-        /*!\n-         * Attempts to select obligations that were registered since\n-         * the call to a selection routine. This is used by the type checker\n-         * to eagerly attempt to resolve obligations in hopes of gaining\n-         * type information. It'd be equally valid to use `select_where_possible`\n-         * but it results in `O(n^2)` performance (#18208).\n-         */\n-\n         let mut selcx = SelectionContext::new(infcx, param_env, typer);\n         self.select(&mut selcx, true)\n     }\n@@ -113,16 +109,13 @@ impl<'tcx> FulfillmentContext<'tcx> {\n         self.trait_obligations[]\n     }\n \n+    /// Attempts to select obligations using `selcx`. If `only_new_obligations` is true, then it\n+    /// only attempts to select obligations that haven't been seen before.\n     fn select<'a>(&mut self,\n                   selcx: &mut SelectionContext<'a, 'tcx>,\n                   only_new_obligations: bool)\n                   -> Result<(),Vec<FulfillmentError<'tcx>>>\n     {\n-        /*!\n-         * Attempts to select obligations using `selcx`. If\n-         * `only_new_obligations` is true, then it only attempts to\n-         * select obligations that haven't been seen before.\n-         */\n         debug!(\"select({} obligations, only_new_obligations={}) start\",\n                self.trait_obligations.len(),\n                only_new_obligations);"}, {"sha": "c4eeff8caf64aeca0fdfbd3361da04d87bd3c990", "filename": "src/librustc/middle/traits/mod.rs", "status": "modified", "additions": 18, "deletions": 35, "changes": 53, "blob_url": "https://github.com/rust-lang/rust/blob/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc%2Fmiddle%2Ftraits%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc%2Fmiddle%2Ftraits%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Fmiddle%2Ftraits%2Fmod.rs?ref=cd5c8235c5448a7234548c772468c8d2e8f150d9", "patch": "@@ -8,9 +8,7 @@\n // option. This file may not be copied, modified, or distributed\n // except according to those terms.\n \n-/*!\n- * Trait Resolution. See doc.rs.\n- */\n+//! Trait Resolution. See doc.rs.\n \n pub use self::SelectionError::*;\n pub use self::FulfillmentErrorCode::*;\n@@ -226,6 +224,10 @@ pub struct VtableParamData<'tcx> {\n     pub bound: Rc<ty::TraitRef<'tcx>>,\n }\n \n+/// Matches the self type of the inherent impl `impl_def_id`\n+/// against `self_ty` and returns the resulting resolution.  This\n+/// routine may modify the surrounding type context (for example,\n+/// it may unify variables).\n pub fn select_inherent_impl<'a,'tcx>(infcx: &InferCtxt<'a,'tcx>,\n                                      param_env: &ty::ParameterEnvironment<'tcx>,\n                                      typer: &Typer<'tcx>,\n@@ -235,13 +237,6 @@ pub fn select_inherent_impl<'a,'tcx>(infcx: &InferCtxt<'a,'tcx>,\n                                      -> SelectionResult<'tcx,\n                                             VtableImplData<'tcx, Obligation<'tcx>>>\n {\n-    /*!\n-     * Matches the self type of the inherent impl `impl_def_id`\n-     * against `self_ty` and returns the resulting resolution.  This\n-     * routine may modify the surrounding type context (for example,\n-     * it may unify variables).\n-     */\n-\n     // This routine is only suitable for inherent impls. This is\n     // because it does not attempt to unify the output type parameters\n     // from the trait ref against the values from the obligation.\n@@ -256,53 +251,41 @@ pub fn select_inherent_impl<'a,'tcx>(infcx: &InferCtxt<'a,'tcx>,\n     selcx.select_inherent_impl(impl_def_id, cause, self_ty)\n }\n \n+/// True if neither the trait nor self type is local. Note that `impl_def_id` must refer to an impl\n+/// of a trait, not an inherent impl.\n pub fn is_orphan_impl(tcx: &ty::ctxt,\n                       impl_def_id: ast::DefId)\n                       -> bool\n {\n-    /*!\n-     * True if neither the trait nor self type is local. Note that\n-     * `impl_def_id` must refer to an impl of a trait, not an inherent\n-     * impl.\n-     */\n-\n     !coherence::impl_is_local(tcx, impl_def_id)\n }\n \n+/// True if there exist types that satisfy both of the two given impls.\n pub fn overlapping_impls(infcx: &InferCtxt,\n                          impl1_def_id: ast::DefId,\n                          impl2_def_id: ast::DefId)\n                          -> bool\n {\n-    /*!\n-     * True if there exist types that satisfy both of the two given impls.\n-     */\n-\n     coherence::impl_can_satisfy(infcx, impl1_def_id, impl2_def_id) &&\n     coherence::impl_can_satisfy(infcx, impl2_def_id, impl1_def_id)\n }\n \n+/// Given generic bounds from an impl like:\n+///\n+///    impl<A:Foo, B:Bar+Qux> ...\n+///\n+/// along with the bindings for the types `A` and `B` (e.g., `<A=A0, B=B0>`), yields a result like\n+///\n+///    [[Foo for A0, Bar for B0, Qux for B0], [], []]\n+///\n+/// Expects that `generic_bounds` have already been fully substituted, late-bound regions liberated\n+/// and so forth, so that they are in the same namespace as `type_substs`.\n pub fn obligations_for_generics<'tcx>(tcx: &ty::ctxt<'tcx>,\n                                       cause: ObligationCause<'tcx>,\n                                       generic_bounds: &ty::GenericBounds<'tcx>,\n                                       type_substs: &subst::VecPerParamSpace<Ty<'tcx>>)\n                                       -> subst::VecPerParamSpace<Obligation<'tcx>>\n {\n-    /*!\n-     * Given generic bounds from an impl like:\n-     *\n-     *    impl<A:Foo, B:Bar+Qux> ...\n-     *\n-     * along with the bindings for the types `A` and `B` (e.g.,\n-     * `<A=A0, B=B0>`), yields a result like\n-     *\n-     *    [[Foo for A0, Bar for B0, Qux for B0], [], []]\n-     *\n-     * Expects that `generic_bounds` have already been fully\n-     * substituted, late-bound regions liberated and so forth,\n-     * so that they are in the same namespace as `type_substs`.\n-     */\n-\n     util::obligations_for_generics(tcx, cause, 0, generic_bounds, type_substs)\n }\n "}, {"sha": "f49cd2dd19f7f0692f0c05f7f50f4df7d8b3284e", "filename": "src/librustc/middle/traits/select.rs", "status": "modified", "additions": 106, "deletions": 174, "changes": 280, "blob_url": "https://github.com/rust-lang/rust/blob/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc%2Fmiddle%2Ftraits%2Fselect.rs", "raw_url": "https://github.com/rust-lang/rust/raw/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc%2Fmiddle%2Ftraits%2Fselect.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Fmiddle%2Ftraits%2Fselect.rs?ref=cd5c8235c5448a7234548c772468c8d2e8f150d9", "patch": "@@ -8,7 +8,7 @@\n // option. This file may not be copied, modified, or distributed\n // except according to those terms.\n \n-/*! See `doc.rs` for high-level documentation */\n+//! See `doc.rs` for high-level documentation\n #![allow(dead_code)] // FIXME -- just temporarily\n \n pub use self::MethodMatchResult::*;\n@@ -201,15 +201,11 @@ impl<'cx, 'tcx> SelectionContext<'cx, 'tcx> {\n     //    is `Vec<Foo>:Iterable<Bar>`, but the impl specifies\n     //    `impl<T> Iterable<T> for Vec<T>`, than an error would result.\n \n+    /// Evaluates whether the obligation can be satisfied. Returns an indication of whether the\n+    /// obligation can be satisfied and, if so, by what means. Never affects surrounding typing\n+    /// environment.\n     pub fn select(&mut self, obligation: &Obligation<'tcx>)\n                   -> SelectionResult<'tcx, Selection<'tcx>> {\n-        /*!\n-         * Evaluates whether the obligation can be satisfied. Returns\n-         * an indication of whether the obligation can be satisfied\n-         * and, if so, by what means. Never affects surrounding typing\n-         * environment.\n-         */\n-\n         debug!(\"select({})\", obligation.repr(self.tcx()));\n         assert!(!obligation.trait_ref.has_escaping_regions());\n \n@@ -253,15 +249,11 @@ impl<'cx, 'tcx> SelectionContext<'cx, 'tcx> {\n     // The result is \"true\" if the obligation *may* hold and \"false\" if\n     // we can be sure it does not.\n \n+    /// Evaluates whether the obligation `obligation` can be satisfied (by any means).\n     pub fn evaluate_obligation(&mut self,\n                                obligation: &Obligation<'tcx>)\n                                -> bool\n     {\n-        /*!\n-         * Evaluates whether the obligation `obligation` can be\n-         * satisfied (by any means).\n-         */\n-\n         debug!(\"evaluate_obligation({})\",\n                obligation.repr(self.tcx()));\n         assert!(!obligation.trait_ref.has_escaping_regions());\n@@ -387,17 +379,13 @@ impl<'cx, 'tcx> SelectionContext<'cx, 'tcx> {\n         }\n     }\n \n+    /// Evaluates whether the impl with id `impl_def_id` could be applied to the self type\n+    /// `obligation_self_ty`. This can be used either for trait or inherent impls.\n     pub fn evaluate_impl(&mut self,\n                          impl_def_id: ast::DefId,\n                          obligation: &Obligation<'tcx>)\n                          -> bool\n     {\n-        /*!\n-         * Evaluates whether the impl with id `impl_def_id` could be\n-         * applied to the self type `obligation_self_ty`. This can be\n-         * used either for trait or inherent impls.\n-         */\n-\n         debug!(\"evaluate_impl(impl_def_id={}, obligation={})\",\n                impl_def_id.repr(self.tcx()),\n                obligation.repr(self.tcx()));\n@@ -435,23 +423,20 @@ impl<'cx, 'tcx> SelectionContext<'cx, 'tcx> {\n     // the body of `evaluate_method_obligation()` for more details on\n     // the algorithm.\n \n+    /// Determine whether a trait-method is applicable to a receiver of\n+    /// type `rcvr_ty`. *Does not affect the inference state.*\n+    ///\n+    /// - `rcvr_ty` -- type of the receiver\n+    /// - `xform_self_ty` -- transformed self type declared on the method, with `Self`\n+    ///   to a fresh type variable\n+    /// - `obligation` -- a reference to the trait where the method is declared, with\n+    ///   the input types on the trait replaced with fresh type variables\n     pub fn evaluate_method_obligation(&mut self,\n                                       rcvr_ty: Ty<'tcx>,\n                                       xform_self_ty: Ty<'tcx>,\n                                       obligation: &Obligation<'tcx>)\n                                       -> MethodMatchResult\n     {\n-        /*!\n-         * Determine whether a trait-method is applicable to a receiver of\n-         * type `rcvr_ty`. *Does not affect the inference state.*\n-         *\n-         * - `rcvr_ty` -- type of the receiver\n-         * - `xform_self_ty` -- transformed self type declared on the method, with `Self`\n-         *   to a fresh type variable\n-         * - `obligation` -- a reference to the trait where the method is declared, with\n-         *   the input types on the trait replaced with fresh type variables\n-         */\n-\n         // Here is the situation. We have a trait method declared (say) like so:\n         //\n         //     trait TheTrait {\n@@ -563,19 +548,15 @@ impl<'cx, 'tcx> SelectionContext<'cx, 'tcx> {\n         }\n     }\n \n+    /// Given the successful result of a method match, this function \"confirms\" the result, which\n+    /// basically repeats the various matching operations, but outside of any snapshot so that\n+    /// their effects are committed into the inference state.\n     pub fn confirm_method_match(&mut self,\n                                 rcvr_ty: Ty<'tcx>,\n                                 xform_self_ty: Ty<'tcx>,\n                                 obligation: &Obligation<'tcx>,\n                                 data: MethodMatchedData)\n     {\n-        /*!\n-         * Given the successful result of a method match, this\n-         * function \"confirms\" the result, which basically repeats the\n-         * various matching operations, but outside of any snapshot so\n-         * that their effects are committed into the inference state.\n-         */\n-\n         let is_ok = match data {\n             PreciseMethodMatch => {\n                 self.match_method_precise(rcvr_ty, xform_self_ty, obligation).is_ok()\n@@ -597,17 +578,14 @@ impl<'cx, 'tcx> SelectionContext<'cx, 'tcx> {\n         }\n     }\n \n+    /// Implements the *precise method match* procedure described in\n+    /// `evaluate_method_obligation()`.\n     fn match_method_precise(&mut self,\n                             rcvr_ty: Ty<'tcx>,\n                             xform_self_ty: Ty<'tcx>,\n                             obligation: &Obligation<'tcx>)\n                             -> Result<(),()>\n     {\n-        /*!\n-         * Implements the *precise method match* procedure described in\n-         * `evaluate_method_obligation()`.\n-         */\n-\n         self.infcx.commit_if_ok(|| {\n             match self.infcx.sub_types(false, infer::RelateSelfType(obligation.cause.span),\n                                        rcvr_ty, xform_self_ty) {\n@@ -623,18 +601,14 @@ impl<'cx, 'tcx> SelectionContext<'cx, 'tcx> {\n         })\n     }\n \n+    /// Assembles a list of potentially applicable impls using the *coercive match* procedure\n+    /// described in `evaluate_method_obligation()`.\n     fn assemble_method_candidates_from_impls(&mut self,\n                                              rcvr_ty: Ty<'tcx>,\n                                              xform_self_ty: Ty<'tcx>,\n                                              obligation: &Obligation<'tcx>)\n                                              -> Vec<ast::DefId>\n     {\n-        /*!\n-         * Assembles a list of potentially applicable impls using the\n-         * *coercive match* procedure described in\n-         * `evaluate_method_obligation()`.\n-         */\n-\n         let mut candidates = Vec::new();\n \n         let all_impls = self.all_impls(obligation.trait_ref.def_id);\n@@ -650,18 +624,15 @@ impl<'cx, 'tcx> SelectionContext<'cx, 'tcx> {\n         candidates\n     }\n \n+    /// Applies the *coercive match* procedure described in `evaluate_method_obligation()` to a\n+    /// particular impl.\n     fn match_method_coerce(&mut self,\n                            impl_def_id: ast::DefId,\n                            rcvr_ty: Ty<'tcx>,\n                            xform_self_ty: Ty<'tcx>,\n                            obligation: &Obligation<'tcx>)\n                            -> Result<Substs<'tcx>, ()>\n     {\n-        /*!\n-         * Applies the *coercive match* procedure described in\n-         * `evaluate_method_obligation()` to a particular impl.\n-         */\n-\n         // This is almost always expected to succeed. It\n         // causes the impl's self-type etc to be unified with\n         // the type variable that is shared between\n@@ -683,20 +654,15 @@ impl<'cx, 'tcx> SelectionContext<'cx, 'tcx> {\n         Ok(substs)\n     }\n \n+    /// A version of `winnow_impl` applicable to coerice method matching.  This is basically the\n+    /// same as `winnow_impl` but it uses the method matching procedure and is specific to impls.\n     fn winnow_method_impl(&mut self,\n                           impl_def_id: ast::DefId,\n                           rcvr_ty: Ty<'tcx>,\n                           xform_self_ty: Ty<'tcx>,\n                           obligation: &Obligation<'tcx>)\n                           -> bool\n     {\n-        /*!\n-         * A version of `winnow_impl` applicable to coerice method\n-         * matching.  This is basically the same as `winnow_impl` but\n-         * it uses the method matching procedure and is specific to\n-         * impls.\n-         */\n-\n         debug!(\"winnow_method_impl: impl_def_id={} rcvr_ty={} xform_self_ty={} obligation={}\",\n                impl_def_id.repr(self.tcx()),\n                rcvr_ty.repr(self.tcx()),\n@@ -962,19 +928,15 @@ impl<'cx, 'tcx> SelectionContext<'cx, 'tcx> {\n         Ok(candidates)\n     }\n \n+    /// Given an obligation like `<SomeTrait for T>`, search the obligations that the caller\n+    /// supplied to find out whether it is listed among them.\n+    ///\n+    /// Never affects inference environment.\n     fn assemble_candidates_from_caller_bounds(&mut self,\n                                               obligation: &Obligation<'tcx>,\n                                               candidates: &mut CandidateSet<'tcx>)\n                                               -> Result<(),SelectionError<'tcx>>\n     {\n-        /*!\n-         * Given an obligation like `<SomeTrait for T>`, search the obligations\n-         * that the caller supplied to find out whether it is listed among\n-         * them.\n-         *\n-         * Never affects inference environment.\n-         */\n-\n         debug!(\"assemble_candidates_from_caller_bounds({})\",\n                obligation.repr(self.tcx()));\n \n@@ -1002,22 +964,17 @@ impl<'cx, 'tcx> SelectionContext<'cx, 'tcx> {\n         Ok(())\n     }\n \n+    /// Check for the artificial impl that the compiler will create for an obligation like `X :\n+    /// FnMut<..>` where `X` is an unboxed closure type.\n+    ///\n+    /// Note: the type parameters on an unboxed closure candidate are modeled as *output* type\n+    /// parameters and hence do not affect whether this trait is a match or not. They will be\n+    /// unified during the confirmation step.\n     fn assemble_unboxed_candidates(&mut self,\n                                    obligation: &Obligation<'tcx>,\n                                    candidates: &mut CandidateSet<'tcx>)\n                                    -> Result<(),SelectionError<'tcx>>\n     {\n-        /*!\n-         * Check for the artificial impl that the compiler will create\n-         * for an obligation like `X : FnMut<..>` where `X` is an\n-         * unboxed closure type.\n-         *\n-         * Note: the type parameters on an unboxed closure candidate\n-         * are modeled as *output* type parameters and hence do not\n-         * affect whether this trait is a match or not. They will be\n-         * unified during the confirmation step.\n-         */\n-\n         let tcx = self.tcx();\n         let kind = if Some(obligation.trait_ref.def_id) == tcx.lang_items.fn_trait() {\n             ty::FnUnboxedClosureKind\n@@ -1060,15 +1017,12 @@ impl<'cx, 'tcx> SelectionContext<'cx, 'tcx> {\n         Ok(())\n     }\n \n+    /// Search for impls that might apply to `obligation`.\n     fn assemble_candidates_from_impls(&mut self,\n                                       obligation: &Obligation<'tcx>,\n                                       candidates: &mut CandidateSet<'tcx>)\n                                       -> Result<(), SelectionError<'tcx>>\n     {\n-        /*!\n-         * Search for impls that might apply to `obligation`.\n-         */\n-\n         let all_impls = self.all_impls(obligation.trait_ref.def_id);\n         for &impl_def_id in all_impls.iter() {\n             self.infcx.probe(|| {\n@@ -1092,17 +1046,14 @@ impl<'cx, 'tcx> SelectionContext<'cx, 'tcx> {\n     // attempt to evaluate recursive bounds to see if they are\n     // satisfied.\n \n+    /// Further evaluate `candidate` to decide whether all type parameters match and whether nested\n+    /// obligations are met. Returns true if `candidate` remains viable after this further\n+    /// scrutiny.\n     fn winnow_candidate<'o>(&mut self,\n                             stack: &ObligationStack<'o, 'tcx>,\n                             candidate: &Candidate<'tcx>)\n                             -> EvaluationResult\n     {\n-        /*!\n-         * Further evaluate `candidate` to decide whether all type parameters match\n-         * and whether nested obligations are met. Returns true if `candidate` remains\n-         * viable after this further scrutiny.\n-         */\n-\n         debug!(\"winnow_candidate: candidate={}\", candidate.repr(self.tcx()));\n         self.infcx.probe(|| {\n             let candidate = (*candidate).clone();\n@@ -1129,37 +1080,35 @@ impl<'cx, 'tcx> SelectionContext<'cx, 'tcx> {\n         result\n     }\n \n+    /// Returns true if `candidate_i` should be dropped in favor of `candidate_j`.\n+    ///\n+    /// This is generally true if either:\n+    /// - candidate i and candidate j are equivalent; or,\n+    /// - candidate i is a conrete impl and candidate j is a where clause bound,\n+    ///   and the concrete impl is applicable to the types in the where clause bound.\n+    ///\n+    /// The last case refers to cases where there are blanket impls (often conditional\n+    /// blanket impls) as well as a where clause. This can come down to one of two cases:\n+    ///\n+    /// - The impl is truly unconditional (it has no where clauses\n+    ///   of its own), in which case the where clause is\n+    ///   unnecessary, because coherence requires that we would\n+    ///   pick that particular impl anyhow (at least so long as we\n+    ///   don't have specialization).\n+    ///\n+    /// - The impl is conditional, in which case we may not have winnowed it out\n+    ///   because we don't know if the conditions apply, but the where clause is basically\n+    ///   telling us taht there is some impl, though not necessarily the one we see.\n+    ///\n+    /// In both cases we prefer to take the where clause, which is\n+    /// essentially harmless.  See issue #18453 for more details of\n+    /// a case where doing the opposite caused us harm.\n     fn candidate_should_be_dropped_in_favor_of<'o>(&mut self,\n                                                    stack: &ObligationStack<'o, 'tcx>,\n                                                    candidate_i: &Candidate<'tcx>,\n                                                    candidate_j: &Candidate<'tcx>)\n                                                    -> bool\n     {\n-        /*!\n-         * Returns true if `candidate_i` should be dropped in favor of `candidate_j`.\n-         * This is generally true if either:\n-         * - candidate i and candidate j are equivalent; or,\n-         * - candidate i is a conrete impl and candidate j is a where clause bound,\n-         *   and the concrete impl is applicable to the types in the where clause bound.\n-         *\n-         * The last case refers to cases where there are blanket impls (often conditional\n-         * blanket impls) as well as a where clause. This can come down to one of two cases:\n-         *\n-         * - The impl is truly unconditional (it has no where clauses\n-         *   of its own), in which case the where clause is\n-         *   unnecessary, because coherence requires that we would\n-         *   pick that particular impl anyhow (at least so long as we\n-         *   don't have specialization).\n-         *\n-         * - The impl is conditional, in which case we may not have winnowed it out\n-         *   because we don't know if the conditions apply, but the where clause is basically\n-         *   telling us taht there is some impl, though not necessarily the one we see.\n-         *\n-         * In both cases we prefer to take the where clause, which is\n-         * essentially harmless.  See issue #18453 for more details of\n-         * a case where doing the opposite caused us harm.\n-         */\n-\n         match (candidate_i, candidate_j) {\n             (&ImplCandidate(impl_def_id), &ParamCandidate(ref vt)) => {\n                 debug!(\"Considering whether to drop param {} in favor of impl {}\",\n@@ -1848,26 +1797,23 @@ impl<'cx, 'tcx> SelectionContext<'cx, 'tcx> {\n         }\n     }\n \n+    /// Determines whether the self type declared against\n+    /// `impl_def_id` matches `obligation_self_ty`. If successful,\n+    /// returns the substitutions used to make them match. See\n+    /// `match_impl()`. For example, if `impl_def_id` is declared\n+    /// as:\n+    ///\n+    ///    impl<T:Copy> Foo for ~T { ... }\n+    ///\n+    /// and `obligation_self_ty` is `int`, we'd back an `Err(_)`\n+    /// result. But if `obligation_self_ty` were `~int`, we'd get\n+    /// back `Ok(T=int)`.\n     fn match_inherent_impl(&mut self,\n                            impl_def_id: ast::DefId,\n                            obligation_cause: ObligationCause,\n                            obligation_self_ty: Ty<'tcx>)\n                            -> Result<Substs<'tcx>,()>\n     {\n-        /*!\n-         * Determines whether the self type declared against\n-         * `impl_def_id` matches `obligation_self_ty`. If successful,\n-         * returns the substitutions used to make them match. See\n-         * `match_impl()`.  For example, if `impl_def_id` is declared\n-         * as:\n-         *\n-         *    impl<T:Copy> Foo for ~T { ... }\n-         *\n-         * and `obligation_self_ty` is `int`, we'd back an `Err(_)`\n-         * result. But if `obligation_self_ty` were `~int`, we'd get\n-         * back `Ok(T=int)`.\n-         */\n-\n         // Create fresh type variables for each type parameter declared\n         // on the impl etc.\n         let impl_substs = util::fresh_substs_for_impl(self.infcx,\n@@ -1928,68 +1874,57 @@ impl<'cx, 'tcx> SelectionContext<'cx, 'tcx> {\n     // the output type parameters from the obligation with those found\n     // on the impl/bound, which may yield type errors.\n \n+    /// Relates the output type parameters from an impl to the\n+    /// trait.  This may lead to type errors. The confirmation step\n+    /// is separated from the main match procedure because these\n+    /// type errors do not cause us to select another impl.\n+    ///\n+    /// As an example, consider matching the obligation\n+    /// `Iterator<char> for Elems<int>` using the following impl:\n+    ///\n+    ///    impl<T> Iterator<T> for Elems<T> { ... }\n+    ///\n+    /// The match phase will succeed with substitution `T=int`.\n+    /// The confirm step will then try to unify `int` and `char`\n+    /// and yield an error.\n     fn confirm_impl_vtable(&mut self,\n                            impl_def_id: ast::DefId,\n                            obligation_cause: ObligationCause<'tcx>,\n                            obligation_trait_ref: Rc<ty::TraitRef<'tcx>>,\n                            substs: &Substs<'tcx>)\n                            -> Result<(), SelectionError<'tcx>>\n     {\n-        /*!\n-         * Relates the output type parameters from an impl to the\n-         * trait.  This may lead to type errors. The confirmation step\n-         * is separated from the main match procedure because these\n-         * type errors do not cause us to select another impl.\n-         *\n-         * As an example, consider matching the obligation\n-         * `Iterator<char> for Elems<int>` using the following impl:\n-         *\n-         *    impl<T> Iterator<T> for Elems<T> { ... }\n-         *\n-         * The match phase will succeed with substitution `T=int`.\n-         * The confirm step will then try to unify `int` and `char`\n-         * and yield an error.\n-         */\n-\n         let impl_trait_ref = ty::impl_trait_ref(self.tcx(),\n                                                 impl_def_id).unwrap();\n         let impl_trait_ref = impl_trait_ref.subst(self.tcx(),\n                                                   substs);\n         self.confirm(obligation_cause, obligation_trait_ref, impl_trait_ref)\n     }\n \n+    /// After we have determined which impl applies, and with what substitutions, there is one last\n+    /// step. We have to go back and relate the \"output\" type parameters from the obligation to the\n+    /// types that are specified in the impl.\n+    ///\n+    /// For example, imagine we have:\n+    ///\n+    ///     impl<T> Iterator<T> for Vec<T> { ... }\n+    ///\n+    /// and our obligation is `Iterator<Foo> for Vec<int>` (note the mismatch in the obligation\n+    /// types). Up until this step, no error would be reported: the self type is `Vec<int>`, and\n+    /// that matches `Vec<T>` with the substitution `T=int`. At this stage, we could then go and\n+    /// check that the type parameters to the `Iterator` trait match. (In terms of the parameters,\n+    /// the `expected_trait_ref` here would be `Iterator<int> for Vec<int>`, and the\n+    /// `obligation_trait_ref` would be `Iterator<Foo> for Vec<int>`.\n+    ///\n+    /// Note that this checking occurs *after* the impl has selected, because these output type\n+    /// parameters should not affect the selection of the impl. Therefore, if there is a mismatch,\n+    /// we report an error to the user.\n     fn confirm(&mut self,\n                obligation_cause: ObligationCause,\n                obligation_trait_ref: Rc<ty::TraitRef<'tcx>>,\n                expected_trait_ref: Rc<ty::TraitRef<'tcx>>)\n                -> Result<(), SelectionError<'tcx>>\n     {\n-        /*!\n-         * After we have determined which impl applies, and with what\n-         * substitutions, there is one last step. We have to go back\n-         * and relate the \"output\" type parameters from the obligation\n-         * to the types that are specified in the impl.\n-         *\n-         * For example, imagine we have:\n-         *\n-         *     impl<T> Iterator<T> for Vec<T> { ... }\n-         *\n-         * and our obligation is `Iterator<Foo> for Vec<int>` (note\n-         * the mismatch in the obligation types). Up until this step,\n-         * no error would be reported: the self type is `Vec<int>`,\n-         * and that matches `Vec<T>` with the substitution `T=int`.\n-         * At this stage, we could then go and check that the type\n-         * parameters to the `Iterator` trait match.\n-         * (In terms of the parameters, the `expected_trait_ref`\n-         * here would be `Iterator<int> for Vec<int>`, and the\n-         * `obligation_trait_ref` would be `Iterator<Foo> for Vec<int>`.\n-         *\n-         * Note that this checking occurs *after* the impl has\n-         * selected, because these output type parameters should not\n-         * affect the selection of the impl. Therefore, if there is a\n-         * mismatch, we report an error to the user.\n-         */\n-\n         let origin = infer::RelateOutputImplTypes(obligation_cause.span);\n \n         let obligation_trait_ref = obligation_trait_ref.clone();\n@@ -2019,11 +1954,8 @@ impl<'cx, 'tcx> SelectionContext<'cx, 'tcx> {\n         }\n     }\n \n+    /// Returns set of all impls for a given trait.\n     fn all_impls(&self, trait_def_id: ast::DefId) -> Vec<ast::DefId> {\n-        /*!\n-         * Returns set of all impls for a given trait.\n-         */\n-\n         ty::populate_implementations_for_trait_if_necessary(self.tcx(),\n                                                             trait_def_id);\n         match self.tcx().trait_impls.borrow().get(&trait_def_id) {"}, {"sha": "b9e694ff4e2b2780ee11a64d7f5df732139ddf72", "filename": "src/librustc/middle/traits/util.rs", "status": "modified", "additions": 18, "deletions": 31, "changes": 49, "blob_url": "https://github.com/rust-lang/rust/blob/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc%2Fmiddle%2Ftraits%2Futil.rs", "raw_url": "https://github.com/rust-lang/rust/raw/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc%2Fmiddle%2Ftraits%2Futil.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Fmiddle%2Ftraits%2Futil.rs?ref=cd5c8235c5448a7234548c772468c8d2e8f150d9", "patch": "@@ -42,22 +42,18 @@ pub fn supertraits<'cx, 'tcx>(tcx: &'cx ty::ctxt<'tcx>,\n                               trait_ref: Rc<ty::TraitRef<'tcx>>)\n                               -> Supertraits<'cx, 'tcx>\n {\n-    /*!\n-     * Returns an iterator over the trait reference `T` and all of its\n-     * supertrait references. May contain duplicates. In general\n-     * the ordering is not defined.\n-     *\n-     * Example:\n-     *\n-     * ```\n-     * trait Foo { ... }\n-     * trait Bar : Foo { ... }\n-     * trait Baz : Bar+Foo { ... }\n-     * ```\n-     *\n-     * `supertraits(Baz)` yields `[Baz, Bar, Foo, Foo]` in some order.\n-     */\n-\n+    /// Returns an iterator over the trait reference `T` and all of its supertrait references. May\n+    /// contain duplicates. In general the ordering is not defined.\n+    ///\n+    /// Example:\n+    ///\n+    /// ```\n+    /// trait Foo { ... }\n+    /// trait Bar : Foo { ... }\n+    /// trait Baz : Bar+Foo { ... }\n+    /// ```\n+    ///\n+    /// `supertraits(Baz)` yields `[Baz, Bar, Foo, Foo]` in some order.\n     transitive_bounds(tcx, &[trait_ref])\n }\n \n@@ -97,12 +93,8 @@ impl<'cx, 'tcx> Supertraits<'cx, 'tcx> {\n         self.stack.push(entry);\n     }\n \n+    /// Returns the path taken through the trait supertraits to reach the current point.\n     pub fn indices(&self) -> Vec<uint> {\n-        /*!\n-         * Returns the path taken through the trait supertraits to\n-         * reach the current point.\n-         */\n-\n         self.stack.iter().map(|e| e.position).collect()\n     }\n }\n@@ -171,14 +163,14 @@ impl<'tcx> fmt::Show for VtableParamData<'tcx> {\n     }\n }\n \n+/// See `super::obligations_for_generics`\n pub fn obligations_for_generics<'tcx>(tcx: &ty::ctxt<'tcx>,\n                                       cause: ObligationCause<'tcx>,\n                                       recursion_depth: uint,\n                                       generic_bounds: &ty::GenericBounds<'tcx>,\n                                       type_substs: &VecPerParamSpace<Ty<'tcx>>)\n                                       -> VecPerParamSpace<Obligation<'tcx>>\n {\n-    /*! See `super::obligations_for_generics` */\n \n     debug!(\"obligations_for_generics(generic_bounds={}, type_substs={})\",\n            generic_bounds.repr(tcx), type_substs.repr(tcx));\n@@ -272,20 +264,15 @@ pub fn obligation_for_builtin_bound<'tcx>(\n     }\n }\n \n+/// Starting from a caller obligation `caller_bound` (which has coordinates `space`/`i` in the list\n+/// of caller obligations), search through the trait and supertraits to find one where `test(d)` is\n+/// true, where `d` is the def-id of the trait/supertrait. If any is found, return `Some(p)` where\n+/// `p` is the path to that trait/supertrait. Else `None`.\n pub fn search_trait_and_supertraits_from_bound<'tcx>(tcx: &ty::ctxt<'tcx>,\n                                                      caller_bound: Rc<ty::TraitRef<'tcx>>,\n                                                      test: |ast::DefId| -> bool)\n                                                      -> Option<VtableParamData<'tcx>>\n {\n-    /*!\n-     * Starting from a caller obligation `caller_bound` (which has\n-     * coordinates `space`/`i` in the list of caller obligations),\n-     * search through the trait and supertraits to find one where\n-     * `test(d)` is true, where `d` is the def-id of the\n-     * trait/supertrait.  If any is found, return `Some(p)` where `p`\n-     * is the path to that trait/supertrait. Else `None`.\n-     */\n-\n     for bound in transitive_bounds(tcx, &[caller_bound]) {\n         if test(bound.def_id) {\n             let vtable_param = VtableParamData { bound: bound };"}, {"sha": "b79bce62f0b77ed3a0fe182b69d1eb607e897a33", "filename": "src/librustc/middle/ty.rs", "status": "modified", "additions": 65, "deletions": 132, "changes": 197, "blob_url": "https://github.com/rust-lang/rust/blob/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc%2Fmiddle%2Fty.rs", "raw_url": "https://github.com/rust-lang/rust/raw/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc%2Fmiddle%2Fty.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Fmiddle%2Fty.rs?ref=cd5c8235c5448a7234548c772468c8d2e8f150d9", "patch": "@@ -671,39 +671,29 @@ pub fn type_has_late_bound_regions(ty: Ty) -> bool {\n     ty.flags.intersects(HAS_RE_LATE_BOUND)\n }\n \n+/// An \"escaping region\" is a bound region whose binder is not part of `t`.\n+///\n+/// So, for example, consider a type like the following, which has two binders:\n+///\n+///    for<'a> fn(x: for<'b> fn(&'a int, &'b int))\n+///    ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ outer scope\n+///                  ^~~~~~~~~~~~~~~~~~~~~~~~~~~~  inner scope\n+///\n+/// This type has *bound regions* (`'a`, `'b`), but it does not have escaping regions, because the\n+/// binders of both `'a` and `'b` are part of the type itself. However, if we consider the *inner\n+/// fn type*, that type has an escaping region: `'a`.\n+///\n+/// Note that what I'm calling an \"escaping region\" is often just called a \"free region\". However,\n+/// we already use the term \"free region\". It refers to the regions that we use to represent bound\n+/// regions on a fn definition while we are typechecking its body.\n+///\n+/// To clarify, conceptually there is no particular difference between an \"escaping\" region and a\n+/// \"free\" region. However, there is a big difference in practice. Basically, when \"entering\" a\n+/// binding level, one is generally required to do some sort of processing to a bound region, such\n+/// as replacing it with a fresh/skolemized region, or making an entry in the environment to\n+/// represent the scope to which it is attached, etc. An escaping region represents a bound region\n+/// for which this processing has not yet been done.\n pub fn type_has_escaping_regions(ty: Ty) -> bool {\n-    /*!\n-     * An \"escaping region\" is a bound region whose binder is not part of `t`.\n-     *\n-     * So, for example, consider a type like the following, which has two\n-     * binders:\n-     *\n-     *    for<'a> fn(x: for<'b> fn(&'a int, &'b int))\n-     *    ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ outer scope\n-     *                  ^~~~~~~~~~~~~~~~~~~~~~~~~~~~  inner scope\n-     *\n-     * This type has *bound regions* (`'a`, `'b`), but it does not\n-     * have escaping regions, because the binders of both `'a` and\n-     * `'b` are part of the type itself. However, if we consider the\n-     * *inner fn type*, that type has an escaping region: `'a`.\n-     *\n-     * Note that what I'm calling an \"escaping region\" is often just\n-     * called a \"free region\". However, we already use the term \"free\n-     * region\". It refers to the regions that we use to represent\n-     * bound regions on a fn definition while we are typechecking its\n-     * body.\n-     *\n-     * To clarify, conceptually there is no particular difference\n-     * between an \"escaping\" region and a \"free\" region. However,\n-     * there is a big difference in practice. Basically, when\n-     * \"entering\" a binding level, one is generally required to do\n-     * some sort of processing to a bound region, such as replacing it\n-     * with a fresh/skolemized region, or making an entry in the\n-     * environment to represent the scope to which it is attached,\n-     * etc. An escaping region represents a bound region for which\n-     * this processing has not yet been done.\n-     */\n-\n     type_escapes_depth(ty, 0)\n }\n \n@@ -1248,11 +1238,8 @@ pub fn all_builtin_bounds() -> BuiltinBounds {\n     set\n }\n \n+/// An existential bound that does not implement any traits.\n pub fn region_existential_bound(r: ty::Region) -> ExistentialBounds {\n-    /*!\n-     * An existential bound that does not implement any traits.\n-     */\n-\n     ty::ExistentialBounds { region_bound: r,\n                             builtin_bounds: empty_builtin_bounds() }\n }\n@@ -1834,12 +1821,9 @@ impl FlagComputation {\n         }\n     }\n \n+    /// Adds the flags/depth from a set of types that appear within the current type, but within a\n+    /// region binder.\n     fn add_bound_computation(&mut self, computation: &FlagComputation) {\n-        /*!\n-         * Adds the flags/depth from a set of types that appear within\n-         * the current type, but within a region binder.\n-         */\n-\n         self.add_flags(computation.flags);\n \n         // The types that contributed to `computation` occured within\n@@ -2575,38 +2559,26 @@ impl TypeContents {\n         self.intersects(TC::NeedsDrop)\n     }\n \n+    /// Includes only those bits that still apply when indirected through a `Box` pointer\n     pub fn owned_pointer(&self) -> TypeContents {\n-        /*!\n-         * Includes only those bits that still apply\n-         * when indirected through a `Box` pointer\n-         */\n         TC::OwnsOwned | (\n             *self & (TC::OwnsAll | TC::ReachesAll))\n     }\n \n+    /// Includes only those bits that still apply when indirected through a reference (`&`)\n     pub fn reference(&self, bits: TypeContents) -> TypeContents {\n-        /*!\n-         * Includes only those bits that still apply\n-         * when indirected through a reference (`&`)\n-         */\n         bits | (\n             *self & TC::ReachesAll)\n     }\n \n+    /// Includes only those bits that still apply when indirected through a managed pointer (`@`)\n     pub fn managed_pointer(&self) -> TypeContents {\n-        /*!\n-         * Includes only those bits that still apply\n-         * when indirected through a managed pointer (`@`)\n-         */\n         TC::Managed | (\n             *self & TC::ReachesAll)\n     }\n \n+    /// Includes only those bits that still apply when indirected through an unsafe pointer (`*`)\n     pub fn unsafe_pointer(&self) -> TypeContents {\n-        /*!\n-         * Includes only those bits that still apply\n-         * when indirected through an unsafe pointer (`*`)\n-         */\n         *self & TC::ReachesAll\n     }\n \n@@ -2883,14 +2855,10 @@ pub fn type_contents<'tcx>(cx: &ctxt<'tcx>, ty: Ty<'tcx>) -> TypeContents {\n         }\n     }\n \n+    /// Type contents due to containing a reference with the region `region` and borrow kind `bk`\n     fn borrowed_contents(region: ty::Region,\n                          mutbl: ast::Mutability)\n                          -> TypeContents {\n-        /*!\n-         * Type contents due to containing a reference\n-         * with the region `region` and borrow kind `bk`\n-         */\n-\n         let b = match mutbl {\n             ast::MutMutable => TC::ReachesMutable | TC::OwnsAffine,\n             ast::MutImmutable => TC::None,\n@@ -3648,20 +3616,16 @@ pub fn expr_ty_opt<'tcx>(cx: &ctxt<'tcx>, expr: &ast::Expr) -> Option<Ty<'tcx>>\n     return node_id_to_type_opt(cx, expr.id);\n }\n \n+/// Returns the type of `expr`, considering any `AutoAdjustment`\n+/// entry recorded for that expression.\n+///\n+/// It would almost certainly be better to store the adjusted ty in with\n+/// the `AutoAdjustment`, but I opted not to do this because it would\n+/// require serializing and deserializing the type and, although that's not\n+/// hard to do, I just hate that code so much I didn't want to touch it\n+/// unless it was to fix it properly, which seemed a distraction from the\n+/// task at hand! -nmatsakis\n pub fn expr_ty_adjusted<'tcx>(cx: &ctxt<'tcx>, expr: &ast::Expr) -> Ty<'tcx> {\n-    /*!\n-     *\n-     * Returns the type of `expr`, considering any `AutoAdjustment`\n-     * entry recorded for that expression.\n-     *\n-     * It would almost certainly be better to store the adjusted ty in with\n-     * the `AutoAdjustment`, but I opted not to do this because it would\n-     * require serializing and deserializing the type and, although that's not\n-     * hard to do, I just hate that code so much I didn't want to touch it\n-     * unless it was to fix it properly, which seemed a distraction from the\n-     * task at hand! -nmatsakis\n-     */\n-\n     adjust_ty(cx, expr.span, expr.id, expr_ty(cx, expr),\n               cx.adjustments.borrow().get(&expr.id),\n               |method_call| cx.method_map.borrow().get(&method_call).map(|method| method.ty))\n@@ -3707,14 +3671,14 @@ pub fn local_var_name_str(cx: &ctxt, id: NodeId) -> InternedString {\n     }\n }\n \n+/// See `expr_ty_adjusted`\n pub fn adjust_ty<'tcx>(cx: &ctxt<'tcx>,\n                        span: Span,\n                        expr_id: ast::NodeId,\n                        unadjusted_ty: Ty<'tcx>,\n                        adjustment: Option<&AutoAdjustment<'tcx>>,\n                        method_type: |typeck::MethodCall| -> Option<Ty<'tcx>>)\n                        -> Ty<'tcx> {\n-    /*! See `expr_ty_adjusted` */\n \n     match unadjusted_ty.sty {\n         ty_err => return unadjusted_ty,\n@@ -4128,16 +4092,11 @@ pub fn ty_sort_string<'tcx>(cx: &ctxt<'tcx>, ty: Ty<'tcx>) -> String {\n     }\n }\n \n+/// Explains the source of a type err in a short, human readable way. This is meant to be placed\n+/// in parentheses after some larger message. You should also invoke `note_and_explain_type_err()`\n+/// afterwards to present additional details, particularly when it comes to lifetime-related\n+/// errors.\n pub fn type_err_to_str<'tcx>(cx: &ctxt<'tcx>, err: &type_err<'tcx>) -> String {\n-    /*!\n-     *\n-     * Explains the source of a type err in a short,\n-     * human readable way.  This is meant to be placed in\n-     * parentheses after some larger message.  You should\n-     * also invoke `note_and_explain_type_err()` afterwards\n-     * to present additional details, particularly when\n-     * it comes to lifetime-related errors. */\n-\n     fn tstore_to_closure(s: &TraitStore) -> String {\n         match s {\n             &UniqTraitStore => \"proc\".to_string(),\n@@ -4352,21 +4311,16 @@ pub fn provided_trait_methods<'tcx>(cx: &ctxt<'tcx>, id: ast::DefId)\n     }\n }\n \n+/// Helper for looking things up in the various maps that are populated during typeck::collect\n+/// (e.g., `cx.impl_or_trait_items`, `cx.tcache`, etc).  All of these share the pattern that if the\n+/// id is local, it should have been loaded into the map by the `typeck::collect` phase.  If the\n+/// def-id is external, then we have to go consult the crate loading code (and cache the result for\n+/// the future).\n fn lookup_locally_or_in_crate_store<V:Clone>(\n                                     descr: &str,\n                                     def_id: ast::DefId,\n                                     map: &mut DefIdMap<V>,\n                                     load_external: || -> V) -> V {\n-    /*!\n-     * Helper for looking things up in the various maps\n-     * that are populated during typeck::collect (e.g.,\n-     * `cx.impl_or_trait_items`, `cx.tcache`, etc).  All of these share\n-     * the pattern that if the id is local, it should have\n-     * been loaded into the map by the `typeck::collect` phase.\n-     * If the def-id is external, then we have to go consult\n-     * the crate loading code (and cache the result for the future).\n-     */\n-\n     match map.get(&def_id).cloned() {\n         Some(v) => { return v; }\n         None => { }\n@@ -5238,19 +5192,16 @@ pub fn each_bound_trait_and_supertraits<'tcx>(tcx: &ctxt<'tcx>,\n     return true;\n }\n \n+/// Given a type which must meet the builtin bounds and trait bounds, returns a set of lifetimes\n+/// which the type must outlive.\n+///\n+/// Requires that trait definitions have been processed.\n pub fn required_region_bounds<'tcx>(tcx: &ctxt<'tcx>,\n                                     region_bounds: &[ty::Region],\n                                     builtin_bounds: BuiltinBounds,\n                                     trait_bounds: &[Rc<TraitRef<'tcx>>])\n                                     -> Vec<ty::Region>\n {\n-    /*!\n-     * Given a type which must meet the builtin bounds and trait\n-     * bounds, returns a set of lifetimes which the type must outlive.\n-     *\n-     * Requires that trait definitions have been processed.\n-     */\n-\n     let mut all_bounds = Vec::new();\n \n     debug!(\"required_region_bounds(builtin_bounds={}, trait_bounds={})\",\n@@ -5636,28 +5587,24 @@ impl Variance {\n     }\n }\n \n+/// Construct a parameter environment suitable for static contexts or other contexts where there\n+/// are no free type/lifetime parameters in scope.\n pub fn empty_parameter_environment<'tcx>() -> ParameterEnvironment<'tcx> {\n-    /*!\n-     * Construct a parameter environment suitable for static contexts\n-     * or other contexts where there are no free type/lifetime\n-     * parameters in scope.\n-     */\n-\n     ty::ParameterEnvironment { free_substs: Substs::empty(),\n                                bounds: VecPerParamSpace::empty(),\n                                caller_obligations: VecPerParamSpace::empty(),\n                                implicit_region_bound: ty::ReEmpty,\n                                selection_cache: traits::SelectionCache::new(), }\n }\n \n+/// See `ParameterEnvironment` struct def'n for details\n pub fn construct_parameter_environment<'tcx>(\n     tcx: &ctxt<'tcx>,\n     span: Span,\n     generics: &ty::Generics<'tcx>,\n     free_id: ast::NodeId)\n     -> ParameterEnvironment<'tcx>\n {\n-    /*! See `ParameterEnvironment` struct def'n for details */\n \n     //\n     // Construct the free substs.\n@@ -5786,15 +5733,11 @@ impl BorrowKind {\n         }\n     }\n \n+    /// Returns a mutability `m` such that an `&m T` pointer could be used to obtain this borrow\n+    /// kind. Because borrow kinds are richer than mutabilities, we sometimes have to pick a\n+    /// mutability that is stronger than necessary so that it at least *would permit* the borrow in\n+    /// question.\n     pub fn to_mutbl_lossy(self) -> ast::Mutability {\n-        /*!\n-         * Returns a mutability `m` such that an `&m T` pointer could\n-         * be used to obtain this borrow kind. Because borrow kinds\n-         * are richer than mutabilities, we sometimes have to pick a\n-         * mutability that is stronger than necessary so that it at\n-         * least *would permit* the borrow in question.\n-         */\n-\n         match self {\n             MutBorrow => ast::MutMutable,\n             ImmBorrow => ast::MutImmutable,\n@@ -5959,49 +5902,39 @@ impl<'tcx> AutoDerefRef<'tcx> {\n     }\n }\n \n+/// Replace any late-bound regions bound in `value` with free variants attached to scope-id\n+/// `scope_id`.\n pub fn liberate_late_bound_regions<'tcx, HR>(\n     tcx: &ty::ctxt<'tcx>,\n     scope: region::CodeExtent,\n     value: &HR)\n     -> HR\n     where HR : HigherRankedFoldable<'tcx>\n {\n-    /*!\n-     * Replace any late-bound regions bound in `value` with free variants\n-     * attached to scope-id `scope_id`.\n-     */\n-\n     replace_late_bound_regions(\n         tcx, value,\n         |br, _| ty::ReFree(ty::FreeRegion{scope: scope, bound_region: br})).0\n }\n \n+/// Replace any late-bound regions bound in `value` with `'static`. Useful in trans but also\n+/// method lookup and a few other places where precise region relationships are not required.\n pub fn erase_late_bound_regions<'tcx, HR>(\n     tcx: &ty::ctxt<'tcx>,\n     value: &HR)\n     -> HR\n     where HR : HigherRankedFoldable<'tcx>\n {\n-    /*!\n-     * Replace any late-bound regions bound in `value` with `'static`.\n-     * Useful in trans but also method lookup and a few other places\n-     * where precise region relationships are not required.\n-     */\n-\n     replace_late_bound_regions(tcx, value, |_, _| ty::ReStatic).0\n }\n \n+/// Replaces the late-bound-regions in `value` that are bound by `value`.\n pub fn replace_late_bound_regions<'tcx, HR>(\n     tcx: &ty::ctxt<'tcx>,\n     value: &HR,\n     mapf: |BoundRegion, DebruijnIndex| -> ty::Region)\n     -> (HR, FnvHashMap<ty::BoundRegion,ty::Region>)\n     where HR : HigherRankedFoldable<'tcx>\n {\n-    /*!\n-     * Replaces the late-bound-regions in `value` that are bound by `value`.\n-     */\n-\n     debug!(\"replace_late_bound_regions({})\", value.repr(tcx));\n \n     let mut map = FnvHashMap::new();"}, {"sha": "0d7b9b99c57e6fea1990f4da780095b03d6b948f", "filename": "src/librustc/middle/ty_fold.rs", "status": "modified", "additions": 25, "deletions": 27, "changes": 52, "blob_url": "https://github.com/rust-lang/rust/blob/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc%2Fmiddle%2Fty_fold.rs", "raw_url": "https://github.com/rust-lang/rust/raw/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc%2Fmiddle%2Fty_fold.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Fmiddle%2Fty_fold.rs?ref=cd5c8235c5448a7234548c772468c8d2e8f150d9", "patch": "@@ -8,33 +8,31 @@\n // option. This file may not be copied, modified, or distributed\n // except according to those terms.\n \n-/*!\n- * Generalized type folding mechanism. The setup is a bit convoluted\n- * but allows for convenient usage. Let T be an instance of some\n- * \"foldable type\" (one which implements `TypeFoldable`) and F be an\n- * instance of a \"folder\" (a type which implements `TypeFolder`). Then\n- * the setup is intended to be:\n- *\n- *     T.fold_with(F) --calls--> F.fold_T(T) --calls--> super_fold_T(F, T)\n- *\n- * This way, when you define a new folder F, you can override\n- * `fold_T()` to customize the behavior, and invoke `super_fold_T()`\n- * to get the original behavior. Meanwhile, to actually fold\n- * something, you can just write `T.fold_with(F)`, which is\n- * convenient. (Note that `fold_with` will also transparently handle\n- * things like a `Vec<T>` where T is foldable and so on.)\n- *\n- * In this ideal setup, the only function that actually *does*\n- * anything is `super_fold_T`, which traverses the type `T`. Moreover,\n- * `super_fold_T` should only ever call `T.fold_with()`.\n- *\n- * In some cases, we follow a degenerate pattern where we do not have\n- * a `fold_T` nor `super_fold_T` method. Instead, `T.fold_with`\n- * traverses the structure directly. This is suboptimal because the\n- * behavior cannot be overriden, but it's much less work to implement.\n- * If you ever *do* need an override that doesn't exist, it's not hard\n- * to convert the degenerate pattern into the proper thing.\n- */\n+//! Generalized type folding mechanism. The setup is a bit convoluted\n+//! but allows for convenient usage. Let T be an instance of some\n+//! \"foldable type\" (one which implements `TypeFoldable`) and F be an\n+//! instance of a \"folder\" (a type which implements `TypeFolder`). Then\n+//! the setup is intended to be:\n+//!\n+//!     T.fold_with(F) --calls--> F.fold_T(T) --calls--> super_fold_T(F, T)\n+//!\n+//! This way, when you define a new folder F, you can override\n+//! `fold_T()` to customize the behavior, and invoke `super_fold_T()`\n+//! to get the original behavior. Meanwhile, to actually fold\n+//! something, you can just write `T.fold_with(F)`, which is\n+//! convenient. (Note that `fold_with` will also transparently handle\n+//! things like a `Vec<T>` where T is foldable and so on.)\n+//!\n+//! In this ideal setup, the only function that actually *does*\n+//! anything is `super_fold_T`, which traverses the type `T`. Moreover,\n+//! `super_fold_T` should only ever call `T.fold_with()`.\n+//!\n+//! In some cases, we follow a degenerate pattern where we do not have\n+//! a `fold_T` nor `super_fold_T` method. Instead, `T.fold_with`\n+//! traverses the structure directly. This is suboptimal because the\n+//! behavior cannot be overriden, but it's much less work to implement.\n+//! If you ever *do* need an override that doesn't exist, it's not hard\n+//! to convert the degenerate pattern into the proper thing.\n \n use middle::subst;\n use middle::subst::VecPerParamSpace;"}, {"sha": "5dfe3fc3a58cd921aaadf4fffdfc8f3b867b1f50", "filename": "src/librustc/middle/typeck/astconv.rs", "status": "modified", "additions": 56, "deletions": 85, "changes": 141, "blob_url": "https://github.com/rust-lang/rust/blob/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc%2Fmiddle%2Ftypeck%2Fastconv.rs", "raw_url": "https://github.com/rust-lang/rust/raw/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc%2Fmiddle%2Ftypeck%2Fastconv.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Fmiddle%2Ftypeck%2Fastconv.rs?ref=cd5c8235c5448a7234548c772468c8d2e8f150d9", "patch": "@@ -8,46 +8,44 @@\n // option. This file may not be copied, modified, or distributed\n // except according to those terms.\n \n-/*!\n- * Conversion from AST representation of types to the ty.rs\n- * representation.  The main routine here is `ast_ty_to_ty()`: each use\n- * is parameterized by an instance of `AstConv` and a `RegionScope`.\n- *\n- * The parameterization of `ast_ty_to_ty()` is because it behaves\n- * somewhat differently during the collect and check phases,\n- * particularly with respect to looking up the types of top-level\n- * items.  In the collect phase, the crate context is used as the\n- * `AstConv` instance; in this phase, the `get_item_ty()` function\n- * triggers a recursive call to `ty_of_item()`  (note that\n- * `ast_ty_to_ty()` will detect recursive types and report an error).\n- * In the check phase, when the FnCtxt is used as the `AstConv`,\n- * `get_item_ty()` just looks up the item type in `tcx.tcache`.\n- *\n- * The `RegionScope` trait controls what happens when the user does\n- * not specify a region in some location where a region is required\n- * (e.g., if the user writes `&Foo` as a type rather than `&'a Foo`).\n- * See the `rscope` module for more details.\n- *\n- * Unlike the `AstConv` trait, the region scope can change as we descend\n- * the type.  This is to accommodate the fact that (a) fn types are binding\n- * scopes and (b) the default region may change.  To understand case (a),\n- * consider something like:\n- *\n- *   type foo = { x: &a.int, y: |&a.int| }\n- *\n- * The type of `x` is an error because there is no region `a` in scope.\n- * In the type of `y`, however, region `a` is considered a bound region\n- * as it does not already appear in scope.\n- *\n- * Case (b) says that if you have a type:\n- *   type foo<'a> = ...;\n- *   type bar = fn(&foo, &a.foo)\n- * The fully expanded version of type bar is:\n- *   type bar = fn(&'foo &, &a.foo<'a>)\n- * Note that the self region for the `foo` defaulted to `&` in the first\n- * case but `&a` in the second.  Basically, defaults that appear inside\n- * an rptr (`&r.T`) use the region `r` that appears in the rptr.\n- */\n+//! Conversion from AST representation of types to the ty.rs\n+//! representation.  The main routine here is `ast_ty_to_ty()`: each use\n+//! is parameterized by an instance of `AstConv` and a `RegionScope`.\n+//!\n+//! The parameterization of `ast_ty_to_ty()` is because it behaves\n+//! somewhat differently during the collect and check phases,\n+//! particularly with respect to looking up the types of top-level\n+//! items.  In the collect phase, the crate context is used as the\n+//! `AstConv` instance; in this phase, the `get_item_ty()` function\n+//! triggers a recursive call to `ty_of_item()`  (note that\n+//! `ast_ty_to_ty()` will detect recursive types and report an error).\n+//! In the check phase, when the FnCtxt is used as the `AstConv`,\n+//! `get_item_ty()` just looks up the item type in `tcx.tcache`.\n+//!\n+//! The `RegionScope` trait controls what happens when the user does\n+//! not specify a region in some location where a region is required\n+//! (e.g., if the user writes `&Foo` as a type rather than `&'a Foo`).\n+//! See the `rscope` module for more details.\n+//!\n+//! Unlike the `AstConv` trait, the region scope can change as we descend\n+//! the type.  This is to accommodate the fact that (a) fn types are binding\n+//! scopes and (b) the default region may change.  To understand case (a),\n+//! consider something like:\n+//!\n+//!   type foo = { x: &a.int, y: |&a.int| }\n+//!\n+//! The type of `x` is an error because there is no region `a` in scope.\n+//! In the type of `y`, however, region `a` is considered a bound region\n+//! as it does not already appear in scope.\n+//!\n+//! Case (b) says that if you have a type:\n+//!   type foo<'a> = ...;\n+//!   type bar = fn(&foo, &a.foo)\n+//! The fully expanded version of type bar is:\n+//!   type bar = fn(&'foo &, &a.foo<'a>)\n+//! Note that the self region for the `foo` defaulted to `&` in the first\n+//! case but `&a` in the second.  Basically, defaults that appear inside\n+//! an rptr (`&r.T`) use the region `r` that appears in the rptr.\n use middle::const_eval;\n use middle::def;\n use middle::resolve_lifetime as rl;\n@@ -201,6 +199,8 @@ pub fn opt_ast_region_to_region<'tcx, AC: AstConv<'tcx>, RS: RegionScope>(\n     r\n }\n \n+/// Given a path `path` that refers to an item `I` with the declared generics `decl_generics`,\n+/// returns an appropriate set of substitutions for this particular reference to `I`.\n fn ast_path_substs_for_ty<'tcx,AC,RS>(\n     this: &AC,\n     rscope: &RS,\n@@ -211,12 +211,6 @@ fn ast_path_substs_for_ty<'tcx,AC,RS>(\n     -> Substs<'tcx>\n     where AC: AstConv<'tcx>, RS: RegionScope\n {\n-    /*!\n-     * Given a path `path` that refers to an item `I` with the\n-     * declared generics `decl_generics`, returns an appropriate\n-     * set of substitutions for this particular reference to `I`.\n-     */\n-\n     let tcx = this.tcx();\n \n     // ast_path_substs() is only called to convert paths that are\n@@ -422,6 +416,9 @@ pub fn instantiate_poly_trait_ref<'tcx,AC,RS>(\n     instantiate_trait_ref(this, rscope, &ast_trait_ref.trait_ref, self_ty)\n }\n \n+/// Instantiates the path for the given trait reference, assuming that it's bound to a valid trait\n+/// type. Returns the def_id for the defining trait. Fails if the type is a type other than a trait\n+/// type.\n pub fn instantiate_trait_ref<'tcx,AC,RS>(this: &AC,\n                                          rscope: &RS,\n                                          ast_trait_ref: &ast::TraitRef,\n@@ -430,12 +427,6 @@ pub fn instantiate_trait_ref<'tcx,AC,RS>(this: &AC,\n                                          where AC: AstConv<'tcx>,\n                                                RS: RegionScope\n {\n-    /*!\n-     * Instantiates the path for the given trait reference, assuming that\n-     * it's bound to a valid trait type. Returns the def_id for the defining\n-     * trait. Fails if the type is a type other than a trait type.\n-     */\n-\n     match lookup_def_tcx(this.tcx(),\n                          ast_trait_ref.path.span,\n                          ast_trait_ref.ref_id) {\n@@ -1318,6 +1309,10 @@ pub fn ty_of_closure<'tcx, AC: AstConv<'tcx>>(\n     }\n }\n \n+/// Given an existential type like `Foo+'a+Bar`, this routine converts the `'a` and `Bar` intos an\n+/// `ExistentialBounds` struct. The `main_trait_refs` argument specifies the `Foo` -- it is absent\n+/// for closures. Eventually this should all be normalized, I think, so that there is no \"main\n+/// trait ref\" and instead we just have a flat list of bounds as the existential type.\n pub fn conv_existential_bounds<'tcx, AC: AstConv<'tcx>, RS:RegionScope>(\n     this: &AC,\n     rscope: &RS,\n@@ -1326,16 +1321,6 @@ pub fn conv_existential_bounds<'tcx, AC: AstConv<'tcx>, RS:RegionScope>(\n     ast_bounds: &[ast::TyParamBound])\n     -> ty::ExistentialBounds\n {\n-    /*!\n-     * Given an existential type like `Foo+'a+Bar`, this routine\n-     * converts the `'a` and `Bar` intos an `ExistentialBounds`\n-     * struct. The `main_trait_refs` argument specifies the `Foo` --\n-     * it is absent for closures. Eventually this should all be\n-     * normalized, I think, so that there is no \"main trait ref\" and\n-     * instead we just have a flat list of bounds as the existential\n-     * type.\n-     */\n-\n     let ast_bound_refs: Vec<&ast::TyParamBound> =\n         ast_bounds.iter().collect();\n \n@@ -1432,23 +1417,17 @@ pub fn conv_existential_bounds_from_partitioned_bounds<'tcx, AC, RS>(\n     }\n }\n \n+/// Given the bounds on a type parameter / existential type, determines what single region bound\n+/// (if any) we can use to summarize this type. The basic idea is that we will use the bound the\n+/// user provided, if they provided one, and otherwise search the supertypes of trait bounds for\n+/// region bounds. It may be that we can derive no bound at all, in which case we return `None`.\n pub fn compute_opt_region_bound<'tcx>(tcx: &ty::ctxt<'tcx>,\n                                       span: Span,\n                                       builtin_bounds: ty::BuiltinBounds,\n                                       region_bounds: &[&ast::Lifetime],\n                                       trait_bounds: &[Rc<ty::TraitRef<'tcx>>])\n                                       -> Option<ty::Region>\n {\n-    /*!\n-     * Given the bounds on a type parameter / existential type,\n-     * determines what single region bound (if any) we can use to\n-     * summarize this type. The basic idea is that we will use the\n-     * bound the user provided, if they provided one, and otherwise\n-     * search the supertypes of trait bounds for region bounds. It may\n-     * be that we can derive no bound at all, in which case we return\n-     * `None`.\n-     */\n-\n     if region_bounds.len() > 1 {\n         tcx.sess.span_err(\n             region_bounds[1].span,\n@@ -1495,6 +1474,9 @@ pub fn compute_opt_region_bound<'tcx>(tcx: &ty::ctxt<'tcx>,\n     return Some(r);\n }\n \n+/// A version of `compute_opt_region_bound` for use where some region bound is required\n+/// (existential types, basically). Reports an error if no region bound can be derived and we are\n+/// in an `rscope` that does not provide a default.\n fn compute_region_bound<'tcx, AC: AstConv<'tcx>, RS:RegionScope>(\n     this: &AC,\n     rscope: &RS,\n@@ -1504,13 +1486,6 @@ fn compute_region_bound<'tcx, AC: AstConv<'tcx>, RS:RegionScope>(\n     trait_bounds: &[Rc<ty::TraitRef<'tcx>>])\n     -> ty::Region\n {\n-    /*!\n-     * A version of `compute_opt_region_bound` for use where some\n-     * region bound is required (existential types,\n-     * basically). Reports an error if no region bound can be derived\n-     * and we are in an `rscope` that does not provide a default.\n-     */\n-\n     match compute_opt_region_bound(this.tcx(), span, builtin_bounds,\n                                    region_bounds, trait_bounds) {\n         Some(r) => r,\n@@ -1534,17 +1509,13 @@ pub struct PartitionedBounds<'a> {\n     pub region_bounds: Vec<&'a ast::Lifetime>,\n }\n \n+/// Divides a list of bounds from the AST into three groups: builtin bounds (Copy, Sized etc),\n+/// general trait bounds, and region bounds.\n pub fn partition_bounds<'a>(tcx: &ty::ctxt,\n                             _span: Span,\n                             ast_bounds: &'a [&ast::TyParamBound])\n                             -> PartitionedBounds<'a>\n {\n-    /*!\n-     * Divides a list of bounds from the AST into three groups:\n-     * builtin bounds (Copy, Sized etc), general trait bounds,\n-     * and region bounds.\n-     */\n-\n     let mut builtin_bounds = ty::empty_builtin_bounds();\n     let mut region_bounds = Vec::new();\n     let mut trait_bounds = Vec::new();"}, {"sha": "0a93b3a5ec7dc93326698f4b733542f9c8ac7881", "filename": "src/librustc/middle/typeck/check/closure.rs", "status": "modified", "additions": 1, "deletions": 3, "changes": 4, "blob_url": "https://github.com/rust-lang/rust/blob/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc%2Fmiddle%2Ftypeck%2Fcheck%2Fclosure.rs", "raw_url": "https://github.com/rust-lang/rust/raw/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc%2Fmiddle%2Ftypeck%2Fcheck%2Fclosure.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Fmiddle%2Ftypeck%2Fcheck%2Fclosure.rs?ref=cd5c8235c5448a7234548c772468c8d2e8f150d9", "patch": "@@ -8,9 +8,7 @@\n // option. This file may not be copied, modified, or distributed\n // except according to those terms.\n \n-/*!\n- * Code for type-checking closure expressions.\n- */\n+//! Code for type-checking closure expressions.\n \n use super::check_fn;\n use super::{Expectation, ExpectCastableToType, ExpectHasType, NoExpectation};"}, {"sha": "e866627be3d29ad1537423dc03d6057fa55a5d12", "filename": "src/librustc/middle/typeck/check/method/confirm.rs", "status": "modified", "additions": 9, "deletions": 17, "changes": 26, "blob_url": "https://github.com/rust-lang/rust/blob/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc%2Fmiddle%2Ftypeck%2Fcheck%2Fmethod%2Fconfirm.rs", "raw_url": "https://github.com/rust-lang/rust/raw/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc%2Fmiddle%2Ftypeck%2Fcheck%2Fmethod%2Fconfirm.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Fmiddle%2Ftypeck%2Fcheck%2Fmethod%2Fconfirm.rs?ref=cd5c8235c5448a7234548c772468c8d2e8f150d9", "patch": "@@ -189,22 +189,17 @@ impl<'a,'tcx> ConfirmContext<'a,'tcx> {\n     ///////////////////////////////////////////////////////////////////////////\n     //\n \n+    /// Returns a set of substitutions for the method *receiver* where all type and region\n+    /// parameters are instantiated with fresh variables. This substitution does not include any\n+    /// parameters declared on the method itself.\n+    ///\n+    /// Note that this substitution may include late-bound regions from the impl level. If so,\n+    /// these are instantiated later in the `instantiate_method_sig` routine.\n     fn fresh_receiver_substs(&mut self,\n                              self_ty: Ty<'tcx>,\n                              pick: &probe::Pick<'tcx>)\n                              -> (subst::Substs<'tcx>, MethodOrigin<'tcx>)\n     {\n-        /*!\n-         * Returns a set of substitutions for the method *receiver*\n-         * where all type and region parameters are instantiated with\n-         * fresh variables. This substitution does not include any\n-         * parameters declared on the method itself.\n-         *\n-         * Note that this substitution may include late-bound regions\n-         * from the impl level. If so, these are instantiated later in\n-         * the `instantiate_method_sig` routine.\n-         */\n-\n         match pick.kind {\n             probe::InherentImplPick(impl_def_id) => {\n                 assert!(ty::impl_trait_ref(self.tcx(), impl_def_id).is_none(),\n@@ -478,14 +473,11 @@ impl<'a,'tcx> ConfirmContext<'a,'tcx> {\n     ///////////////////////////////////////////////////////////////////////////\n     // RECONCILIATION\n \n+    /// When we select a method with an `&mut self` receiver, we have to go convert any\n+    /// auto-derefs, indices, etc from `Deref` and `Index` into `DerefMut` and `IndexMut`\n+    /// respectively.\n     fn fixup_derefs_on_method_receiver_if_necessary(&self,\n                                                     method_callee: &MethodCallee) {\n-        /*!\n-         * When we select a method with an `&mut self` receiver, we have to go\n-         * convert any auto-derefs, indices, etc from `Deref` and `Index` into\n-         * `DerefMut` and `IndexMut` respectively.\n-         */\n-\n         let sig = match method_callee.ty.sty {\n             ty::ty_bare_fn(ref f) => f.sig.clone(),\n             ty::ty_closure(ref f) => f.sig.clone(),"}, {"sha": "6129e38e39c12f9e6e0810dfd0101fcd98605996", "filename": "src/librustc/middle/typeck/check/method/doc.rs", "status": "modified", "additions": 111, "deletions": 116, "changes": 227, "blob_url": "https://github.com/rust-lang/rust/blob/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc%2Fmiddle%2Ftypeck%2Fcheck%2Fmethod%2Fdoc.rs", "raw_url": "https://github.com/rust-lang/rust/raw/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc%2Fmiddle%2Ftypeck%2Fcheck%2Fmethod%2Fdoc.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Fmiddle%2Ftypeck%2Fcheck%2Fmethod%2Fdoc.rs?ref=cd5c8235c5448a7234548c772468c8d2e8f150d9", "patch": "@@ -8,119 +8,114 @@\n // option. This file may not be copied, modified, or distributed\n // except according to those terms.\n \n-/*!\n-\n-# Method lookup\n-\n-Method lookup can be rather complex due to the interaction of a number\n-of factors, such as self types, autoderef, trait lookup, etc. This\n-file provides an overview of the process. More detailed notes are in\n-the code itself, naturally.\n-\n-One way to think of method lookup is that we convert an expression of\n-the form:\n-\n-    receiver.method(...)\n-\n-into a more explicit UFCS form:\n-\n-    Trait::method(ADJ(receiver), ...) // for a trait call\n-    ReceiverType::method(ADJ(receiver), ...) // for an inherent method call\n-\n-Here `ADJ` is some kind of adjustment, which is typically a series of\n-autoderefs and then possibly an autoref (e.g., `&**receiver`). However\n-we sometimes do other adjustments and coercions along the way, in\n-particular unsizing (e.g., converting from `[T, ..n]` to `[T]`).\n-\n-## The Two Phases\n-\n-Method lookup is divided into two major phases: probing (`probe.rs`)\n-and confirmation (`confirm.rs`). The probe phase is when we decide\n-what method to call and how to adjust the receiver. The confirmation\n-phase \"applies\" this selection, updating the side-tables, unifying\n-type variables, and otherwise doing side-effectful things.\n-\n-One reason for this division is to be more amenable to caching.  The\n-probe phase produces a \"pick\" (`probe::Pick`), which is designed to be\n-cacheable across method-call sites. Therefore, it does not include\n-inference variables or other information.\n-\n-## Probe phase\n-\n-The probe phase (`probe.rs`) decides what method is being called and\n-how to adjust the receiver.\n-\n-### Steps\n-\n-The first thing that the probe phase does is to create a series of\n-*steps*. This is done by progressively dereferencing the receiver type\n-until it cannot be deref'd anymore, as well as applying an optional\n-\"unsize\" step. So if the receiver has type `Rc<Box<[T, ..3]>>`, this\n-might yield:\n-\n-    Rc<Box<[T, ..3]>>\n-    Box<[T, ..3]>\n-    [T, ..3]\n-    [T]\n-\n-### Candidate assembly\n-\n-We then search along those steps to create a list of *candidates*. A\n-`Candidate` is a method item that might plausibly be the method being\n-invoked. For each candidate, we'll derive a \"transformed self type\"\n-that takes into account explicit self.\n-\n-Candidates are grouped into two kinds, inherent and extension.\n-\n-**Inherent candidates** are those that are derived from the\n-type of the receiver itself.  So, if you have a receiver of some\n-nominal type `Foo` (e.g., a struct), any methods defined within an\n-impl like `impl Foo` are inherent methods.  Nothing needs to be\n-imported to use an inherent method, they are associated with the type\n-itself (note that inherent impls can only be defined in the same\n-module as the type itself).\n-\n-FIXME: Inherent candidates are not always derived from impls.  If you\n-have a trait object, such as a value of type `Box<ToString>`, then the\n-trait methods (`to_string()`, in this case) are inherently associated\n-with it. Another case is type parameters, in which case the methods of\n-their bounds are inherent. However, this part of the rules is subject\n-to change: when DST's \"impl Trait for Trait\" is complete, trait object\n-dispatch could be subsumed into trait matching, and the type parameter\n-behavior should be reconsidered in light of where clauses.\n-\n-**Extension candidates** are derived from imported traits.  If I have\n-the trait `ToString` imported, and I call `to_string()` on a value of\n-type `T`, then we will go off to find out whether there is an impl of\n-`ToString` for `T`.  These kinds of method calls are called \"extension\n-methods\".  They can be defined in any module, not only the one that\n-defined `T`.  Furthermore, you must import the trait to call such a\n-method.\n-\n-So, let's continue our example. Imagine that we were calling a method\n-`foo` with the receiver `Rc<Box<[T, ..3]>>` and there is a trait `Foo`\n-that defines it with `&self` for the type `Rc<U>` as well as a method\n-on the type `Box` that defines `Foo` but with `&mut self`. Then we\n-might have two candidates:\n-\n-    &Rc<Box<[T, ..3]>> from the impl of `Foo` for `Rc<U>` where `U=Box<T, ..3]>\n-    &mut Box<[T, ..3]>> from the inherent impl on `Box<U>` where `U=[T, ..3]`\n-\n-### Candidate search\n-\n-Finally, to actually pick the method, we will search down the steps,\n-trying to match the receiver type against the candidate types. At\n-each step, we also consider an auto-ref and auto-mut-ref to see whether\n-that makes any of the candidates match. We pick the first step where\n-we find a match.\n-\n-In the case of our example, the first step is `Rc<Box<[T, ..3]>>`,\n-which does not itself match any candidate. But when we autoref it, we\n-get the type `&Rc<Box<[T, ..3]>>` which does match. We would then\n-recursively consider all where-clauses that appear on the impl: if\n-those match (or we cannot rule out that they do), then this is the\n-method we would pick. Otherwise, we would continue down the series of\n-steps.\n-\n-*/\n-\n+//! # Method lookup\n+//!\n+//! Method lookup can be rather complex due to the interaction of a number\n+//! of factors, such as self types, autoderef, trait lookup, etc. This\n+//! file provides an overview of the process. More detailed notes are in\n+//! the code itself, naturally.\n+//!\n+//! One way to think of method lookup is that we convert an expression of\n+//! the form:\n+//!\n+//!     receiver.method(...)\n+//!\n+//! into a more explicit UFCS form:\n+//!\n+//!     Trait::method(ADJ(receiver), ...) // for a trait call\n+//!     ReceiverType::method(ADJ(receiver), ...) // for an inherent method call\n+//!\n+//! Here `ADJ` is some kind of adjustment, which is typically a series of\n+//! autoderefs and then possibly an autoref (e.g., `&**receiver`). However\n+//! we sometimes do other adjustments and coercions along the way, in\n+//! particular unsizing (e.g., converting from `[T, ..n]` to `[T]`).\n+//!\n+//! ## The Two Phases\n+//!\n+//! Method lookup is divided into two major phases: probing (`probe.rs`)\n+//! and confirmation (`confirm.rs`). The probe phase is when we decide\n+//! what method to call and how to adjust the receiver. The confirmation\n+//! phase \"applies\" this selection, updating the side-tables, unifying\n+//! type variables, and otherwise doing side-effectful things.\n+//!\n+//! One reason for this division is to be more amenable to caching.  The\n+//! probe phase produces a \"pick\" (`probe::Pick`), which is designed to be\n+//! cacheable across method-call sites. Therefore, it does not include\n+//! inference variables or other information.\n+//!\n+//! ## Probe phase\n+//!\n+//! The probe phase (`probe.rs`) decides what method is being called and\n+//! how to adjust the receiver.\n+//!\n+//! ### Steps\n+//!\n+//! The first thing that the probe phase does is to create a series of\n+//! *steps*. This is done by progressively dereferencing the receiver type\n+//! until it cannot be deref'd anymore, as well as applying an optional\n+//! \"unsize\" step. So if the receiver has type `Rc<Box<[T, ..3]>>`, this\n+//! might yield:\n+//!\n+//!     Rc<Box<[T, ..3]>>\n+//!     Box<[T, ..3]>\n+//!     [T, ..3]\n+//!     [T]\n+//!\n+//! ### Candidate assembly\n+//!\n+//! We then search along those steps to create a list of *candidates*. A\n+//! `Candidate` is a method item that might plausibly be the method being\n+//! invoked. For each candidate, we'll derive a \"transformed self type\"\n+//! that takes into account explicit self.\n+//!\n+//! Candidates are grouped into two kinds, inherent and extension.\n+//!\n+//! **Inherent candidates** are those that are derived from the\n+//! type of the receiver itself.  So, if you have a receiver of some\n+//! nominal type `Foo` (e.g., a struct), any methods defined within an\n+//! impl like `impl Foo` are inherent methods.  Nothing needs to be\n+//! imported to use an inherent method, they are associated with the type\n+//! itself (note that inherent impls can only be defined in the same\n+//! module as the type itself).\n+//!\n+//! FIXME: Inherent candidates are not always derived from impls.  If you\n+//! have a trait object, such as a value of type `Box<ToString>`, then the\n+//! trait methods (`to_string()`, in this case) are inherently associated\n+//! with it. Another case is type parameters, in which case the methods of\n+//! their bounds are inherent. However, this part of the rules is subject\n+//! to change: when DST's \"impl Trait for Trait\" is complete, trait object\n+//! dispatch could be subsumed into trait matching, and the type parameter\n+//! behavior should be reconsidered in light of where clauses.\n+//!\n+//! **Extension candidates** are derived from imported traits.  If I have\n+//! the trait `ToString` imported, and I call `to_string()` on a value of\n+//! type `T`, then we will go off to find out whether there is an impl of\n+//! `ToString` for `T`.  These kinds of method calls are called \"extension\n+//! methods\".  They can be defined in any module, not only the one that\n+//! defined `T`.  Furthermore, you must import the trait to call such a\n+//! method.\n+//!\n+//! So, let's continue our example. Imagine that we were calling a method\n+//! `foo` with the receiver `Rc<Box<[T, ..3]>>` and there is a trait `Foo`\n+//! that defines it with `&self` for the type `Rc<U>` as well as a method\n+//! on the type `Box` that defines `Foo` but with `&mut self`. Then we\n+//! might have two candidates:\n+//!\n+//!     &Rc<Box<[T, ..3]>> from the impl of `Foo` for `Rc<U>` where `U=Box<T, ..3]>\n+//!     &mut Box<[T, ..3]>> from the inherent impl on `Box<U>` where `U=[T, ..3]`\n+//!\n+//! ### Candidate search\n+//!\n+//! Finally, to actually pick the method, we will search down the steps,\n+//! trying to match the receiver type against the candidate types. At\n+//! each step, we also consider an auto-ref and auto-mut-ref to see whether\n+//! that makes any of the candidates match. We pick the first step where\n+//! we find a match.\n+//!\n+//! In the case of our example, the first step is `Rc<Box<[T, ..3]>>`,\n+//! which does not itself match any candidate. But when we autoref it, we\n+//! get the type `&Rc<Box<[T, ..3]>>` which does match. We would then\n+//! recursively consider all where-clauses that appear on the impl: if\n+//! those match (or we cannot rule out that they do), then this is the\n+//! method we would pick. Otherwise, we would continue down the series of\n+//! steps."}, {"sha": "34c3292f8cd69b60e515228bb1283a3729113440", "filename": "src/librustc/middle/typeck/check/method/mod.rs", "status": "modified", "additions": 27, "deletions": 42, "changes": 69, "blob_url": "https://github.com/rust-lang/rust/blob/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc%2Fmiddle%2Ftypeck%2Fcheck%2Fmethod%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc%2Fmiddle%2Ftypeck%2Fcheck%2Fmethod%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Fmiddle%2Ftypeck%2Fcheck%2Fmethod%2Fmod.rs?ref=cd5c8235c5448a7234548c772468c8d2e8f150d9", "patch": "@@ -8,7 +8,7 @@\n // option. This file may not be copied, modified, or distributed\n // except according to those terms.\n \n-/*! Method lookup: the secret sauce of Rust. See `doc.rs`. */\n+//! Method lookup: the secret sauce of Rust. See `doc.rs`.\n \n use middle::subst;\n use middle::subst::{Subst};\n@@ -56,24 +56,35 @@ pub enum CandidateSource {\n \n type MethodIndex = uint; // just for doc purposes\n \n+/// Determines whether the type `self_ty` supports a method name `method_name` or not.\n pub fn exists<'a, 'tcx>(fcx: &FnCtxt<'a, 'tcx>,\n                         span: Span,\n                         method_name: ast::Name,\n                         self_ty: Ty<'tcx>,\n                         call_expr_id: ast::NodeId)\n                         -> bool\n {\n-    /*!\n-     * Determines whether the type `self_ty` supports a method name `method_name` or not.\n-     */\n-\n     match probe::probe(fcx, span, method_name, self_ty, call_expr_id) {\n         Ok(_) => true,\n         Err(NoMatch(_)) => false,\n         Err(Ambiguity(_)) => true,\n     }\n }\n \n+/// Performs method lookup. If lookup is successful, it will return the callee and store an\n+/// appropriate adjustment for the self-expr. In some cases it may report an error (e.g., invoking\n+/// the `drop` method).\n+///\n+/// # Arguments\n+///\n+/// Given a method call like `foo.bar::<T1,...Tn>(...)`:\n+///\n+/// * `fcx`:                   the surrounding `FnCtxt` (!)\n+/// * `span`:                  the span for the method call\n+/// * `method_name`:           the name of the method being called (`bar`)\n+/// * `self_ty`:               the (unadjusted) type of the self expression (`foo`)\n+/// * `supplied_method_types`: the explicit method type parameters, if any (`T1..Tn`)\n+/// * `self_expr`:             the self expression (`foo`)\n pub fn lookup<'a, 'tcx>(fcx: &FnCtxt<'a, 'tcx>,\n                         span: Span,\n                         method_name: ast::Name,\n@@ -83,23 +94,6 @@ pub fn lookup<'a, 'tcx>(fcx: &FnCtxt<'a, 'tcx>,\n                         self_expr: &ast::Expr)\n                         -> Result<MethodCallee<'tcx>, MethodError>\n {\n-    /*!\n-     * Performs method lookup. If lookup is successful, it will return the callee\n-     * and store an appropriate adjustment for the self-expr. In some cases it may\n-     * report an error (e.g., invoking the `drop` method).\n-     *\n-     * # Arguments\n-     *\n-     * Given a method call like `foo.bar::<T1,...Tn>(...)`:\n-     *\n-     * - `fcx`:                   the surrounding `FnCtxt` (!)\n-     * - `span`:                  the span for the method call\n-     * - `method_name`:           the name of the method being called (`bar`)\n-     * - `self_ty`:               the (unadjusted) type of the self expression (`foo`)\n-     * - `supplied_method_types`: the explicit method type parameters, if any (`T1..Tn`)\n-     * - `self_expr`:             the self expression (`foo`)\n-     */\n-\n     debug!(\"lookup(method_name={}, self_ty={}, call_expr={}, self_expr={})\",\n            method_name.repr(fcx.tcx()),\n            self_ty.repr(fcx.tcx()),\n@@ -124,6 +118,15 @@ pub fn lookup_in_trait<'a, 'tcx>(fcx: &'a FnCtxt<'a, 'tcx>,\n                              self_ty, opt_input_types)\n }\n \n+/// `lookup_in_trait_adjusted` is used for overloaded operators. It does a very narrow slice of\n+/// what the normal probe/confirm path does. In particular, it doesn't really do any probing: it\n+/// simply constructs an obligation for a particular trait with the given self-type and checks\n+/// whether that trait is implemented.\n+///\n+/// FIXME(#18741) -- It seems likely that we can consolidate some of this code with the other\n+/// method-lookup code. In particular, autoderef on index is basically identical to autoderef with\n+/// normal probes, except that the test also looks for built-in indexing. Also, the second half of\n+/// this method is basically the same as confirmation.\n pub fn lookup_in_trait_adjusted<'a, 'tcx>(fcx: &'a FnCtxt<'a, 'tcx>,\n                                           span: Span,\n                                           self_expr: Option<&'a ast::Expr>,\n@@ -134,21 +137,6 @@ pub fn lookup_in_trait_adjusted<'a, 'tcx>(fcx: &'a FnCtxt<'a, 'tcx>,\n                                           opt_input_types: Option<Vec<Ty<'tcx>>>)\n                                           -> Option<MethodCallee<'tcx>>\n {\n-    /*!\n-     * `lookup_in_trait_adjusted` is used for overloaded operators. It\n-     * does a very narrow slice of what the normal probe/confirm path\n-     * does. In particular, it doesn't really do any probing: it\n-     * simply constructs an obligation for a particular trait with the\n-     * given self-type and checks whether that trait is implemented.\n-     *\n-     * FIXME(#18741) -- It seems likely that we can consolidate some of this\n-     * code with the other method-lookup code. In particular,\n-     * autoderef on index is basically identical to autoderef with\n-     * normal probes, except that the test also looks for built-in\n-     * indexing. Also, the second half of this method is basically\n-     * the same as confirmation.\n-     */\n-\n     debug!(\"lookup_in_trait_adjusted(self_ty={}, self_expr={}, m_name={}, trait_def_id={})\",\n            self_ty.repr(fcx.tcx()),\n            self_expr.repr(fcx.tcx()),\n@@ -408,16 +396,13 @@ pub fn report_error<'a, 'tcx>(fcx: &FnCtxt<'a, 'tcx>,\n     }\n }\n \n+/// Find method with name `method_name` defined in `trait_def_id` and return it, along with its\n+/// index (or `None`, if no such method).\n fn trait_method<'tcx>(tcx: &ty::ctxt<'tcx>,\n                       trait_def_id: ast::DefId,\n                       method_name: ast::Name)\n                       -> Option<(uint, Rc<ty::Method<'tcx>>)>\n {\n-    /*!\n-     * Find method with name `method_name` defined in `trait_def_id` and return it,\n-     * along with its index (or `None`, if no such method).\n-     */\n-\n     let trait_items = ty::trait_items(tcx, trait_def_id);\n     trait_items\n         .iter()"}, {"sha": "484d72130e61d94182df0c71e57e88a4677db01f", "filename": "src/librustc/middle/typeck/check/method/probe.rs", "status": "modified", "additions": 37, "deletions": 56, "changes": 93, "blob_url": "https://github.com/rust-lang/rust/blob/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc%2Fmiddle%2Ftypeck%2Fcheck%2Fmethod%2Fprobe.rs", "raw_url": "https://github.com/rust-lang/rust/raw/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc%2Fmiddle%2Ftypeck%2Fcheck%2Fmethod%2Fprobe.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Fmiddle%2Ftypeck%2Fcheck%2Fmethod%2Fprobe.rs?ref=cd5c8235c5448a7234548c772468c8d2e8f150d9", "patch": "@@ -807,33 +807,26 @@ impl<'a,'tcx> ProbeContext<'a,'tcx> {\n         })\n     }\n \n+    /// Sometimes we get in a situation where we have multiple probes that are all impls of the\n+    /// same trait, but we don't know which impl to use. In this case, since in all cases the\n+    /// external interface of the method can be determined from the trait, it's ok not to decide.\n+    /// We can basically just collapse all of the probes for various impls into one where-clause\n+    /// probe. This will result in a pending obligation so when more type-info is available we can\n+    /// make the final decision.\n+    ///\n+    /// Example (`src/test/run-pass/method-two-trait-defer-resolution-1.rs`):\n+    ///\n+    /// ```\n+    /// trait Foo { ... }\n+    /// impl Foo for Vec<int> { ... }\n+    /// impl Foo for Vec<uint> { ... }\n+    /// ```\n+    ///\n+    /// Now imagine the receiver is `Vec<_>`. It doesn't really matter at this time which impl we\n+    /// use, so it's ok to just commit to \"using the method from the trait Foo\".\n     fn collapse_candidates_to_trait_pick(&self,\n                                          probes: &[&Candidate<'tcx>])\n                                          -> Option<Pick<'tcx>> {\n-        /*!\n-         * Sometimes we get in a situation where we have multiple\n-         * probes that are all impls of the same trait, but we don't\n-         * know which impl to use. In this case, since in all cases\n-         * the external interface of the method can be determined from\n-         * the trait, it's ok not to decide.  We can basically just\n-         * collapse all of the probes for various impls into one\n-         * where-clause probe. This will result in a pending\n-         * obligation so when more type-info is available we can make\n-         * the final decision.\n-         *\n-         * Example (`src/test/run-pass/method-two-trait-defer-resolution-1.rs`):\n-         *\n-         * ```\n-         * trait Foo { ... }\n-         * impl Foo for Vec<int> { ... }\n-         * impl Foo for Vec<uint> { ... }\n-         * ```\n-         *\n-         * Now imagine the receiver is `Vec<_>`. It doesn't really\n-         * matter at this time which impl we use, so it's ok to just\n-         * commit to \"using the method from the trait Foo\".\n-         */\n-\n         // Do all probes correspond to the same trait?\n         let trait_data = match probes[0].to_trait_data() {\n             Some(data) => data,\n@@ -952,36 +945,27 @@ impl<'a,'tcx> ProbeContext<'a,'tcx> {\n         subst::Substs::new(type_vars, region_placeholders)\n     }\n \n+    /// Replace late-bound-regions bound by `value` with `'static` using\n+    /// `ty::erase_late_bound_regions`.\n+    ///\n+    /// This is only a reasonable thing to do during the *probe* phase, not the *confirm* phase, of\n+    /// method matching. It is reasonable during the probe phase because we don't consider region\n+    /// relationships at all. Therefore, we can just replace all the region variables with 'static\n+    /// rather than creating fresh region variables. This is nice for two reasons:\n+    ///\n+    /// 1. Because the numbers of the region variables would otherwise be fairly unique to this\n+    ///    particular method call, it winds up creating fewer types overall, which helps for memory\n+    ///    usage. (Admittedly, this is a rather small effect, though measureable.)\n+    ///\n+    /// 2. It makes it easier to deal with higher-ranked trait bounds, because we can replace any\n+    ///    late-bound regions with 'static. Otherwise, if we were going to replace late-bound\n+    ///    regions with actual region variables as is proper, we'd have to ensure that the same\n+    ///    region got replaced with the same variable, which requires a bit more coordination\n+    ///    and/or tracking the substitution and\n+    ///    so forth.\n     fn erase_late_bound_regions<T>(&self, value: &T) -> T\n         where T : HigherRankedFoldable<'tcx>\n     {\n-        /*!\n-         * Replace late-bound-regions bound by `value` with `'static`\n-         * using `ty::erase_late_bound_regions`.\n-         *\n-         * This is only a reasonable thing to do during the *probe*\n-         * phase, not the *confirm* phase, of method matching. It is\n-         * reasonable during the probe phase because we don't consider\n-         * region relationships at all. Therefore, we can just replace\n-         * all the region variables with 'static rather than creating\n-         * fresh region variables. This is nice for two reasons:\n-         *\n-         * 1. Because the numbers of the region variables would\n-         *    otherwise be fairly unique to this particular method\n-         *    call, it winds up creating fewer types overall, which\n-         *    helps for memory usage. (Admittedly, this is a rather\n-         *    small effect, though measureable.)\n-         *\n-         * 2. It makes it easier to deal with higher-ranked trait\n-         *    bounds, because we can replace any late-bound regions\n-         *    with 'static. Otherwise, if we were going to replace\n-         *    late-bound regions with actual region variables as is\n-         *    proper, we'd have to ensure that the same region got\n-         *    replaced with the same variable, which requires a bit\n-         *    more coordination and/or tracking the substitution and\n-         *    so forth.\n-         */\n-\n         ty::erase_late_bound_regions(self.tcx(), value)\n     }\n }\n@@ -1000,16 +984,13 @@ fn impl_method<'tcx>(tcx: &ty::ctxt<'tcx>,\n         .and_then(|item| item.as_opt_method())\n }\n \n+/// Find method with name `method_name` defined in `trait_def_id` and return it, along with its\n+/// index (or `None`, if no such method).\n fn trait_method<'tcx>(tcx: &ty::ctxt<'tcx>,\n                       trait_def_id: ast::DefId,\n                       method_name: ast::Name)\n                       -> Option<(uint, Rc<ty::Method<'tcx>>)>\n {\n-    /*!\n-     * Find method with name `method_name` defined in `trait_def_id` and return it,\n-     * along with its index (or `None`, if no such method).\n-     */\n-\n     let trait_items = ty::trait_items(tcx, trait_def_id);\n     trait_items\n         .iter()"}, {"sha": "b33ce04f5ebe74b96ba1df1347298dc04058cdac", "filename": "src/librustc/middle/typeck/check/mod.rs", "status": "modified", "additions": 102, "deletions": 166, "changes": 268, "blob_url": "https://github.com/rust-lang/rust/blob/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc%2Fmiddle%2Ftypeck%2Fcheck%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc%2Fmiddle%2Ftypeck%2Fcheck%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Fmiddle%2Ftypeck%2Fcheck%2Fmod.rs?ref=cd5c8235c5448a7234548c772468c8d2e8f150d9", "patch": "@@ -486,6 +486,12 @@ impl<'a, 'tcx, 'v> Visitor<'v> for GatherLocalsVisitor<'a, 'tcx> {\n \n }\n \n+/// Helper used by check_bare_fn and check_expr_fn. Does the grungy work of checking a function\n+/// body and returns the function context used for that purpose, since in the case of a fn item\n+/// there is still a bit more to do.\n+///\n+/// * ...\n+/// * inherited: other fields inherited from the enclosing fn (if any)\n fn check_fn<'a, 'tcx>(ccx: &'a CrateCtxt<'a, 'tcx>,\n                       fn_style: ast::FnStyle,\n                       fn_style_id: ast::NodeId,\n@@ -495,16 +501,6 @@ fn check_fn<'a, 'tcx>(ccx: &'a CrateCtxt<'a, 'tcx>,\n                       body: &ast::Block,\n                       inherited: &'a Inherited<'a, 'tcx>)\n                       -> FnCtxt<'a, 'tcx> {\n-    /*!\n-     * Helper used by check_bare_fn and check_expr_fn.  Does the\n-     * grungy work of checking a function body and returns the\n-     * function context used for that purpose, since in the case of a\n-     * fn item there is still a bit more to do.\n-     *\n-     * - ...\n-     * - inherited: other fields inherited from the enclosing fn (if any)\n-     */\n-\n     let tcx = ccx.tcx;\n     let err_count_on_creation = tcx.sess.err_count();\n \n@@ -701,19 +697,17 @@ pub fn check_item(ccx: &CrateCtxt, it: &ast::Item) {\n     }\n }\n \n+/// Type checks a method body.\n+///\n+/// # Parameters\n+///\n+/// * `item_generics`: generics defined on the impl/trait that contains\n+///   the method\n+/// * `self_bound`: bound for the `Self` type parameter, if any\n+/// * `method`: the method definition\n fn check_method_body<'a, 'tcx>(ccx: &CrateCtxt<'a, 'tcx>,\n                                item_generics: &ty::Generics<'tcx>,\n                                method: &ast::Method) {\n-    /*!\n-     * Type checks a method body.\n-     *\n-     * # Parameters\n-     * - `item_generics`: generics defined on the impl/trait that contains\n-     *   the method\n-     * - `self_bound`: bound for the `Self` type parameter, if any\n-     * - `method`: the method definition\n-     */\n-\n     debug!(\"check_method_body(item_generics={}, method.id={})\",\n             item_generics.repr(ccx.tcx),\n             method.id);\n@@ -1222,6 +1216,33 @@ fn compare_impl_method<'tcx>(tcx: &ty::ctxt<'tcx>,\n     // parameters.\n     infcx.resolve_regions_and_report_errors();\n \n+    /// Check that region bounds on impl method are the same as those on the trait. In principle,\n+    /// it could be ok for there to be fewer region bounds on the impl method, but this leads to an\n+    /// annoying corner case that is painful to handle (described below), so for now we can just\n+    /// forbid it.\n+    ///\n+    /// Example (see `src/test/compile-fail/regions-bound-missing-bound-in-impl.rs`):\n+    ///\n+    /// ```\n+    /// trait Foo<'a> {\n+    ///     fn method1<'b>();\n+    ///     fn method2<'b:'a>();\n+    /// }\n+    ///\n+    /// impl<'a> Foo<'a> for ... {\n+    ///     fn method1<'b:'a>() { .. case 1, definitely bad .. }\n+    ///     fn method2<'b>() { .. case 2, could be ok .. }\n+    /// }\n+    /// ```\n+    ///\n+    /// The \"definitely bad\" case is case #1. Here, the impl adds an extra constraint not present\n+    /// in the trait.\n+    ///\n+    /// The \"maybe bad\" case is case #2. Here, the impl adds an extra constraint not present in the\n+    /// trait. We could in principle allow this, but it interacts in a complex way with early/late\n+    /// bound resolution of lifetimes. Basically the presence or absence of a lifetime bound\n+    /// affects whether the lifetime is early/late bound, and right now the code breaks if the\n+    /// trait has an early bound lifetime parameter and the method does not.\n     fn check_region_bounds_on_impl_method<'tcx>(tcx: &ty::ctxt<'tcx>,\n                                                 span: Span,\n                                                 impl_m: &ty::Method<'tcx>,\n@@ -1232,37 +1253,6 @@ fn compare_impl_method<'tcx>(tcx: &ty::ctxt<'tcx>,\n                                                 impl_to_skol_substs: &Substs<'tcx>)\n                                                 -> bool\n     {\n-        /*!\n-\n-        Check that region bounds on impl method are the same as those\n-        on the trait. In principle, it could be ok for there to be\n-        fewer region bounds on the impl method, but this leads to an\n-        annoying corner case that is painful to handle (described\n-        below), so for now we can just forbid it.\n-\n-        Example (see\n-        `src/test/compile-fail/regions-bound-missing-bound-in-impl.rs`):\n-\n-            trait Foo<'a> {\n-                fn method1<'b>();\n-                fn method2<'b:'a>();\n-            }\n-\n-            impl<'a> Foo<'a> for ... {\n-                fn method1<'b:'a>() { .. case 1, definitely bad .. }\n-                fn method2<'b>() { .. case 2, could be ok .. }\n-            }\n-\n-        The \"definitely bad\" case is case #1. Here, the impl adds an\n-        extra constraint not present in the trait.\n-\n-        The \"maybe bad\" case is case #2. Here, the impl adds an extra\n-        constraint not present in the trait. We could in principle\n-        allow this, but it interacts in a complex way with early/late\n-        bound resolution of lifetimes. Basically the presence or\n-        absence of a lifetime bound affects whether the lifetime is\n-        early/late bound, and right now the code breaks if the trait\n-        has an early bound lifetime parameter and the method does not.\n \n         */\n \n@@ -1770,23 +1760,17 @@ impl<'a, 'tcx> FnCtxt<'a, 'tcx> {\n         }\n     }\n \n+    /// Returns the type of `def_id` with all generics replaced by by fresh type/region variables.\n+    /// Also returns the substitution from the type parameters on `def_id` to the fresh variables.\n+    /// Registers any trait obligations specified on `def_id` at the same time.\n+    ///\n+    /// Note that function is only intended to be used with types (notably, not impls). This is\n+    /// because it doesn't do any instantiation of late-bound regions.\n     pub fn instantiate_type(&self,\n                             span: Span,\n                             def_id: ast::DefId)\n                             -> TypeAndSubsts<'tcx>\n     {\n-        /*!\n-         * Returns the type of `def_id` with all generics replaced by\n-         * by fresh type/region variables. Also returns the\n-         * substitution from the type parameters on `def_id` to the\n-         * fresh variables. Registers any trait obligations specified\n-         * on `def_id` at the same time.\n-         *\n-         * Note that function is only intended to be used with types\n-         * (notably, not impls). This is because it doesn't do any\n-         * instantiation of late-bound regions.\n-         */\n-\n         let polytype =\n             ty::lookup_item_type(self.tcx(), def_id);\n         let substs =\n@@ -1886,26 +1870,19 @@ impl<'a, 'tcx> FnCtxt<'a, 'tcx> {\n         }\n     }\n \n+    /// Fetch type of `expr` after applying adjustments that have been recorded in the fcx.\n     pub fn expr_ty_adjusted(&self, expr: &ast::Expr) -> Ty<'tcx> {\n-        /*!\n-         * Fetch type of `expr` after applying adjustments that\n-         * have been recorded in the fcx.\n-         */\n-\n         let adjustments = self.inh.adjustments.borrow();\n         let adjustment = adjustments.get(&expr.id);\n         self.adjust_expr_ty(expr, adjustment)\n     }\n \n+    /// Apply `adjustment` to the type of `expr`\n     pub fn adjust_expr_ty(&self,\n                           expr: &ast::Expr,\n                           adjustment: Option<&ty::AutoAdjustment<'tcx>>)\n                           -> Ty<'tcx>\n     {\n-        /*!\n-         * Apply `adjustment` to the type of `expr`\n-         */\n-\n         let raw_ty = self.expr_ty(expr);\n         let raw_ty = self.infcx().shallow_resolve(raw_ty);\n         ty::adjust_ty(self.tcx(),\n@@ -2013,16 +1990,13 @@ impl<'a, 'tcx> FnCtxt<'a, 'tcx> {\n         self.infcx().report_mismatched_types(sp, e, a, err)\n     }\n \n+    /// Registers an obligation for checking later, during regionck, that the type `ty` must\n+    /// outlive the region `r`.\n     pub fn register_region_obligation(&self,\n                                       origin: infer::SubregionOrigin<'tcx>,\n                                       ty: Ty<'tcx>,\n                                       r: ty::Region)\n     {\n-        /*!\n-         * Registers an obligation for checking later, during\n-         * regionck, that the type `ty` must outlive the region `r`.\n-         */\n-\n         let mut region_obligations = self.inh.region_obligations.borrow_mut();\n         let region_obligation = RegionObligation { sub_region: r,\n                                                    sup_type: ty,\n@@ -2045,31 +2019,29 @@ impl<'a, 'tcx> FnCtxt<'a, 'tcx> {\n         }\n     }\n \n+    /// Given a fully substituted set of bounds (`generic_bounds`), and the values with which each\n+    /// type/region parameter was instantiated (`substs`), creates and registers suitable\n+    /// trait/region obligations.\n+    ///\n+    /// For example, if there is a function:\n+    ///\n+    /// ```\n+    /// fn foo<'a,T:'a>(...)\n+    /// ```\n+    ///\n+    /// and a reference:\n+    ///\n+    /// ```\n+    /// let f = foo;\n+    /// ```\n+    ///\n+    /// Then we will create a fresh region variable `'$0` and a fresh type variable `$1` for `'a`\n+    /// and `T`. This routine will add a region obligation `$1:'$0` and register it locally.\n     pub fn add_obligations_for_parameters(&self,\n                                           cause: traits::ObligationCause<'tcx>,\n                                           substs: &Substs<'tcx>,\n                                           generic_bounds: &ty::GenericBounds<'tcx>)\n     {\n-        /*!\n-         * Given a fully substituted set of bounds (`generic_bounds`),\n-         * and the values with which each type/region parameter was\n-         * instantiated (`substs`), creates and registers suitable\n-         * trait/region obligations.\n-         *\n-         * For example, if there is a function:\n-         *\n-         *    fn foo<'a,T:'a>(...)\n-         *\n-         * and a reference:\n-         *\n-         *    let f = foo;\n-         *\n-         * Then we will create a fresh region variable `'$0` and a\n-         * fresh type variable `$1` for `'a` and `T`. This routine\n-         * will add a region obligation `$1:'$0` and register it\n-         * locally.\n-         */\n-\n         assert!(!generic_bounds.has_escaping_regions());\n \n         debug!(\"add_obligations_for_parameters(substs={}, generic_bounds={})\",\n@@ -2160,22 +2132,17 @@ pub enum LvaluePreference {\n     NoPreference\n }\n \n+/// Executes an autoderef loop for the type `t`. At each step, invokes `should_stop` to decide\n+/// whether to terminate the loop. Returns the final type and number of derefs that it performed.\n+///\n+/// Note: this method does not modify the adjustments table. The caller is responsible for\n+/// inserting an AutoAdjustment record into the `fcx` using one of the suitable methods.\n pub fn autoderef<'a, 'tcx, T>(fcx: &FnCtxt<'a, 'tcx>, sp: Span,\n                               base_ty: Ty<'tcx>,\n                               expr_id: Option<ast::NodeId>,\n                               mut lvalue_pref: LvaluePreference,\n                               should_stop: |Ty<'tcx>, uint| -> Option<T>)\n                               -> (Ty<'tcx>, uint, Option<T>) {\n-    /*!\n-     * Executes an autoderef loop for the type `t`. At each step, invokes\n-     * `should_stop` to decide whether to terminate the loop. Returns\n-     * the final type and number of derefs that it performed.\n-     *\n-     * Note: this method does not modify the adjustments table. The caller is\n-     * responsible for inserting an AutoAdjustment record into the `fcx`\n-     * using one of the suitable methods.\n-     */\n-\n     let mut t = base_ty;\n     for autoderefs in range(0, fcx.tcx().sess.recursion_limit.get()) {\n         let resolved_t = structurally_resolved_type(fcx, sp, t);\n@@ -2306,19 +2273,14 @@ fn try_overloaded_deref<'a, 'tcx>(fcx: &FnCtxt<'a, 'tcx>,\n     make_overloaded_lvalue_return_type(fcx, method_call, method)\n }\n \n+/// For the overloaded lvalue expressions (`*x`, `x[3]`), the trait returns a type of `&T`, but the\n+/// actual type we assign to the *expression* is `T`. So this function just peels off the return\n+/// type by one layer to yield `T`. It also inserts the `method-callee` into the method map.\n fn make_overloaded_lvalue_return_type<'a, 'tcx>(fcx: &FnCtxt<'a, 'tcx>,\n                                                 method_call: Option<MethodCall>,\n                                                 method: Option<MethodCallee<'tcx>>)\n                                                 -> Option<ty::mt<'tcx>>\n {\n-    /*!\n-     * For the overloaded lvalue expressions (`*x`, `x[3]`), the trait\n-     * returns a type of `&T`, but the actual type we assign to the\n-     * *expression* is `T`. So this function just peels off the return\n-     * type by one layer to yield `T`. It also inserts the\n-     * `method-callee` into the method map.\n-     */\n-\n     match method {\n         Some(method) => {\n             let ref_ty = ty::ty_fn_ret(method.ty);\n@@ -2380,6 +2342,8 @@ fn autoderef_for_index<'a, 'tcx, T>(fcx: &FnCtxt<'a, 'tcx>,\n     }\n }\n \n+/// Autoderefs `base_expr`, looking for a `Slice` impl. If it finds one, installs the relevant\n+/// method info and returns the result type (else None).\n fn try_overloaded_slice<'a, 'tcx>(fcx: &FnCtxt<'a, 'tcx>,\n                                   method_call: MethodCall,\n                                   expr: &ast::Expr,\n@@ -2390,12 +2354,6 @@ fn try_overloaded_slice<'a, 'tcx>(fcx: &FnCtxt<'a, 'tcx>,\n                                   mutbl: ast::Mutability)\n                                   -> Option<Ty<'tcx>> // return type is result of slice\n {\n-    /*!\n-     * Autoderefs `base_expr`, looking for a `Slice` impl. If it\n-     * finds one, installs the relevant method info and returns the\n-     * result type (else None).\n-     */\n-\n     let lvalue_pref = match mutbl {\n         ast::MutMutable => PreferMutLvalue,\n         ast::MutImmutable => NoPreference\n@@ -2436,6 +2394,8 @@ fn try_overloaded_slice<'a, 'tcx>(fcx: &FnCtxt<'a, 'tcx>,\n     })\n }\n \n+/// Checks for a `Slice` (or `SliceMut`) impl at the relevant level of autoderef. If it finds one,\n+/// installs method info and returns type of method (else None).\n fn try_overloaded_slice_step<'a, 'tcx>(fcx: &FnCtxt<'a, 'tcx>,\n                                        method_call: MethodCall,\n                                        expr: &ast::Expr,\n@@ -2448,12 +2408,6 @@ fn try_overloaded_slice_step<'a, 'tcx>(fcx: &FnCtxt<'a, 'tcx>,\n                                        // result type is type of method being called\n                                        -> Option<Ty<'tcx>>\n {\n-    /*!\n-     * Checks for a `Slice` (or `SliceMut`) impl at the relevant level\n-     * of autoderef. If it finds one, installs method info and returns\n-     * type of method (else None).\n-     */\n-\n     let method = if mutbl == ast::MutMutable {\n         // Try `SliceMut` first, if preferred.\n         match fcx.tcx().lang_items.slice_mut_trait() {\n@@ -2510,6 +2464,10 @@ fn try_overloaded_slice_step<'a, 'tcx>(fcx: &FnCtxt<'a, 'tcx>,\n     })\n }\n \n+/// To type-check `base_expr[index_expr]`, we progressively autoderef (and otherwise adjust)\n+/// `base_expr`, looking for a type which either supports builtin indexing or overloaded indexing.\n+/// This loop implements one step in that search; the autoderef loop is implemented by\n+/// `autoderef_for_index`.\n fn try_index_step<'a, 'tcx>(fcx: &FnCtxt<'a, 'tcx>,\n                             method_call: MethodCall,\n                             expr: &ast::Expr,\n@@ -2519,13 +2477,6 @@ fn try_index_step<'a, 'tcx>(fcx: &FnCtxt<'a, 'tcx>,\n                             lvalue_pref: LvaluePreference)\n                             -> Option<(/*index type*/ Ty<'tcx>, /*element type*/ Ty<'tcx>)>\n {\n-    /*!\n-     * To type-check `base_expr[index_expr]`, we progressively autoderef (and otherwise adjust)\n-     * `base_expr`, looking for a type which either supports builtin indexing or overloaded\n-     * indexing. This loop implements one step in that search; the autoderef loop is implemented\n-     * by `autoderef_for_index`.\n-     */\n-\n     debug!(\"try_index_step(expr={}, base_expr.id={}, adjusted_ty={}, adjustment={})\",\n            expr.repr(fcx.tcx()),\n            base_expr.repr(fcx.tcx()),\n@@ -2712,6 +2663,8 @@ fn check_method_argument_types<'a, 'tcx>(fcx: &FnCtxt<'a, 'tcx>,\n     }\n }\n \n+/// Generic function that factors out common logic from function calls, method calls and overloaded\n+/// operators.\n fn check_argument_types<'a, 'tcx>(fcx: &FnCtxt<'a, 'tcx>,\n                                   sp: Span,\n                                   fn_inputs: &[Ty<'tcx>],\n@@ -2720,12 +2673,6 @@ fn check_argument_types<'a, 'tcx>(fcx: &FnCtxt<'a, 'tcx>,\n                                   deref_args: DerefArgs,\n                                   variadic: bool,\n                                   tuple_arguments: TupleArgumentsFlag) {\n-    /*!\n-     *\n-     * Generic function that factors out common logic from\n-     * function calls, method calls and overloaded operators.\n-     */\n-\n     let tcx = fcx.ccx.tcx;\n \n     // Grab the argument types, supplying fresh type variables\n@@ -5289,6 +5236,15 @@ pub fn instantiate_path<'a, 'tcx>(fcx: &FnCtxt<'a, 'tcx>,\n         }\n     }\n \n+    /// Finds the parameters that the user provided and adds them to `substs`. If too many\n+    /// parameters are provided, then reports an error and clears the output vector.\n+    ///\n+    /// We clear the output vector because that will cause the `adjust_XXX_parameters()` later to\n+    /// use inference variables. This seems less likely to lead to derived errors.\n+    ///\n+    /// Note that we *do not* check for *too few* parameters here. Due to the presence of defaults\n+    /// etc that is more complicated. I wanted however to do the reporting of *too many* parameters\n+    /// here because we can easily use the precise span of the N+1'th parameter.\n     fn push_explicit_parameters_from_segment_to_substs<'a, 'tcx>(\n         fcx: &FnCtxt<'a, 'tcx>,\n         space: subst::ParamSpace,\n@@ -5298,23 +5254,6 @@ pub fn instantiate_path<'a, 'tcx>(fcx: &FnCtxt<'a, 'tcx>,\n         segment: &ast::PathSegment,\n         substs: &mut Substs<'tcx>)\n     {\n-        /*!\n-         * Finds the parameters that the user provided and adds them\n-         * to `substs`. If too many parameters are provided, then\n-         * reports an error and clears the output vector.\n-         *\n-         * We clear the output vector because that will cause the\n-         * `adjust_XXX_parameters()` later to use inference\n-         * variables. This seems less likely to lead to derived\n-         * errors.\n-         *\n-         * Note that we *do not* check for *too few* parameters here.\n-         * Due to the presence of defaults etc that is more\n-         * complicated. I wanted however to do the reporting of *too\n-         * many* parameters here because we can easily use the precise\n-         * span of the N+1'th parameter.\n-         */\n-\n         match segment.parameters {\n             ast::AngleBracketedParameters(ref data) => {\n                 push_explicit_angle_bracketed_parameters_from_segment_to_substs(\n@@ -5373,6 +5312,12 @@ pub fn instantiate_path<'a, 'tcx>(fcx: &FnCtxt<'a, 'tcx>,\n         }\n     }\n \n+    /// As with\n+    /// `push_explicit_angle_bracketed_parameters_from_segment_to_substs`,\n+    /// but intended for `Foo(A,B) -> C` form. This expands to\n+    /// roughly the same thing as `Foo<(A,B),C>`. One important\n+    /// difference has to do with the treatment of anonymous\n+    /// regions, which are translated into bound regions (NYI).\n     fn push_explicit_parenthesized_parameters_from_segment_to_substs<'a, 'tcx>(\n         fcx: &FnCtxt<'a, 'tcx>,\n         space: subst::ParamSpace,\n@@ -5381,15 +5326,6 @@ pub fn instantiate_path<'a, 'tcx>(fcx: &FnCtxt<'a, 'tcx>,\n         data: &ast::ParenthesizedParameterData,\n         substs: &mut Substs<'tcx>)\n     {\n-        /*!\n-         * As with\n-         * `push_explicit_angle_bracketed_parameters_from_segment_to_substs`,\n-         * but intended for `Foo(A,B) -> C` form. This expands to\n-         * roughly the same thing as `Foo<(A,B),C>`. One important\n-         * difference has to do with the treatment of anonymous\n-         * regions, which are translated into bound regions (NYI).\n-         */\n-\n         let type_count = type_defs.len(space);\n         if type_count < 2 {\n             span_err!(fcx.tcx().sess, span, E0167,"}, {"sha": "bc6e7d9d87ffed20635b46cf075e3a000b15aa23", "filename": "src/librustc/middle/typeck/check/regionck.rs", "status": "modified", "additions": 223, "deletions": 314, "changes": 537, "blob_url": "https://github.com/rust-lang/rust/blob/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc%2Fmiddle%2Ftypeck%2Fcheck%2Fregionck.rs", "raw_url": "https://github.com/rust-lang/rust/raw/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc%2Fmiddle%2Ftypeck%2Fcheck%2Fregionck.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Fmiddle%2Ftypeck%2Fcheck%2Fregionck.rs?ref=cd5c8235c5448a7234548c772468c8d2e8f150d9", "patch": "@@ -8,115 +8,111 @@\n // option. This file may not be copied, modified, or distributed\n // except according to those terms.\n \n-/*!\n-\n-The region check is a final pass that runs over the AST after we have\n-inferred the type constraints but before we have actually finalized\n-the types.  Its purpose is to embed a variety of region constraints.\n-Inserting these constraints as a separate pass is good because (1) it\n-localizes the code that has to do with region inference and (2) often\n-we cannot know what constraints are needed until the basic types have\n-been inferred.\n-\n-### Interaction with the borrow checker\n-\n-In general, the job of the borrowck module (which runs later) is to\n-check that all soundness criteria are met, given a particular set of\n-regions. The job of *this* module is to anticipate the needs of the\n-borrow checker and infer regions that will satisfy its requirements.\n-It is generally true that the inference doesn't need to be sound,\n-meaning that if there is a bug and we inferred bad regions, the borrow\n-checker should catch it. This is not entirely true though; for\n-example, the borrow checker doesn't check subtyping, and it doesn't\n-check that region pointers are always live when they are used. It\n-might be worthwhile to fix this so that borrowck serves as a kind of\n-verification step -- that would add confidence in the overall\n-correctness of the compiler, at the cost of duplicating some type\n-checks and effort.\n-\n-### Inferring the duration of borrows, automatic and otherwise\n-\n-Whenever we introduce a borrowed pointer, for example as the result of\n-a borrow expression `let x = &data`, the lifetime of the pointer `x`\n-is always specified as a region inference variable. `regionck` has the\n-job of adding constraints such that this inference variable is as\n-narrow as possible while still accommodating all uses (that is, every\n-dereference of the resulting pointer must be within the lifetime).\n-\n-#### Reborrows\n-\n-Generally speaking, `regionck` does NOT try to ensure that the data\n-`data` will outlive the pointer `x`. That is the job of borrowck.  The\n-one exception is when \"re-borrowing\" the contents of another borrowed\n-pointer. For example, imagine you have a borrowed pointer `b` with\n-lifetime L1 and you have an expression `&*b`. The result of this\n-expression will be another borrowed pointer with lifetime L2 (which is\n-an inference variable). The borrow checker is going to enforce the\n-constraint that L2 < L1, because otherwise you are re-borrowing data\n-for a lifetime larger than the original loan.  However, without the\n-routines in this module, the region inferencer would not know of this\n-dependency and thus it might infer the lifetime of L2 to be greater\n-than L1 (issue #3148).\n-\n-There are a number of troublesome scenarios in the tests\n-`region-dependent-*.rs`, but here is one example:\n-\n-    struct Foo { i: int }\n-    struct Bar { foo: Foo  }\n-    fn get_i(x: &'a Bar) -> &'a int {\n-       let foo = &x.foo; // Lifetime L1\n-       &foo.i            // Lifetime L2\n-    }\n-\n-Note that this comes up either with `&` expressions, `ref`\n-bindings, and `autorefs`, which are the three ways to introduce\n-a borrow.\n-\n-The key point here is that when you are borrowing a value that\n-is \"guaranteed\" by a borrowed pointer, you must link the\n-lifetime of that borrowed pointer (L1, here) to the lifetime of\n-the borrow itself (L2).  What do I mean by \"guaranteed\" by a\n-borrowed pointer? I mean any data that is reached by first\n-dereferencing a borrowed pointer and then either traversing\n-interior offsets or owned pointers.  We say that the guarantor\n-of such data it the region of the borrowed pointer that was\n-traversed.  This is essentially the same as the ownership\n-relation, except that a borrowed pointer never owns its\n-contents.\n-\n-### Inferring borrow kinds for upvars\n-\n-Whenever there is a closure expression, we need to determine how each\n-upvar is used. We do this by initially assigning each upvar an\n-immutable \"borrow kind\" (see `ty::BorrowKind` for details) and then\n-\"escalating\" the kind as needed. The borrow kind proceeds according to\n-the following lattice:\n-\n-    ty::ImmBorrow -> ty::UniqueImmBorrow -> ty::MutBorrow\n-\n-So, for example, if we see an assignment `x = 5` to an upvar `x`, we\n-will promote its borrow kind to mutable borrow. If we see an `&mut x`\n-we'll do the same. Naturally, this applies not just to the upvar, but\n-to everything owned by `x`, so the result is the same for something\n-like `x.f = 5` and so on (presuming `x` is not a borrowed pointer to a\n-struct). These adjustments are performed in\n-`adjust_upvar_borrow_kind()` (you can trace backwards through the code\n-from there).\n-\n-The fact that we are inferring borrow kinds as we go results in a\n-semi-hacky interaction with mem-categorization. In particular,\n-mem-categorization will query the current borrow kind as it\n-categorizes, and we'll return the *current* value, but this may get\n-adjusted later. Therefore, in this module, we generally ignore the\n-borrow kind (and derived mutabilities) that are returned from\n-mem-categorization, since they may be inaccurate. (Another option\n-would be to use a unification scheme, where instead of returning a\n-concrete borrow kind like `ty::ImmBorrow`, we return a\n-`ty::InferBorrow(upvar_id)` or something like that, but this would\n-then mean that all later passes would have to check for these figments\n-and report an error, and it just seems like more mess in the end.)\n-\n-*/\n+//! The region check is a final pass that runs over the AST after we have\n+//! inferred the type constraints but before we have actually finalized\n+//! the types.  Its purpose is to embed a variety of region constraints.\n+//! Inserting these constraints as a separate pass is good because (1) it\n+//! localizes the code that has to do with region inference and (2) often\n+//! we cannot know what constraints are needed until the basic types have\n+//! been inferred.\n+//!\n+//! ### Interaction with the borrow checker\n+//!\n+//! In general, the job of the borrowck module (which runs later) is to\n+//! check that all soundness criteria are met, given a particular set of\n+//! regions. The job of *this* module is to anticipate the needs of the\n+//! borrow checker and infer regions that will satisfy its requirements.\n+//! It is generally true that the inference doesn't need to be sound,\n+//! meaning that if there is a bug and we inferred bad regions, the borrow\n+//! checker should catch it. This is not entirely true though; for\n+//! example, the borrow checker doesn't check subtyping, and it doesn't\n+//! check that region pointers are always live when they are used. It\n+//! might be worthwhile to fix this so that borrowck serves as a kind of\n+//! verification step -- that would add confidence in the overall\n+//! correctness of the compiler, at the cost of duplicating some type\n+//! checks and effort.\n+//!\n+//! ### Inferring the duration of borrows, automatic and otherwise\n+//!\n+//! Whenever we introduce a borrowed pointer, for example as the result of\n+//! a borrow expression `let x = &data`, the lifetime of the pointer `x`\n+//! is always specified as a region inference variable. `regionck` has the\n+//! job of adding constraints such that this inference variable is as\n+//! narrow as possible while still accommodating all uses (that is, every\n+//! dereference of the resulting pointer must be within the lifetime).\n+//!\n+//! #### Reborrows\n+//!\n+//! Generally speaking, `regionck` does NOT try to ensure that the data\n+//! `data` will outlive the pointer `x`. That is the job of borrowck.  The\n+//! one exception is when \"re-borrowing\" the contents of another borrowed\n+//! pointer. For example, imagine you have a borrowed pointer `b` with\n+//! lifetime L1 and you have an expression `&*b`. The result of this\n+//! expression will be another borrowed pointer with lifetime L2 (which is\n+//! an inference variable). The borrow checker is going to enforce the\n+//! constraint that L2 < L1, because otherwise you are re-borrowing data\n+//! for a lifetime larger than the original loan.  However, without the\n+//! routines in this module, the region inferencer would not know of this\n+//! dependency and thus it might infer the lifetime of L2 to be greater\n+//! than L1 (issue #3148).\n+//!\n+//! There are a number of troublesome scenarios in the tests\n+//! `region-dependent-*.rs`, but here is one example:\n+//!\n+//!     struct Foo { i: int }\n+//!     struct Bar { foo: Foo  }\n+//!     fn get_i(x: &'a Bar) -> &'a int {\n+//!        let foo = &x.foo; // Lifetime L1\n+//!        &foo.i            // Lifetime L2\n+//!     }\n+//!\n+//! Note that this comes up either with `&` expressions, `ref`\n+//! bindings, and `autorefs`, which are the three ways to introduce\n+//! a borrow.\n+//!\n+//! The key point here is that when you are borrowing a value that\n+//! is \"guaranteed\" by a borrowed pointer, you must link the\n+//! lifetime of that borrowed pointer (L1, here) to the lifetime of\n+//! the borrow itself (L2).  What do I mean by \"guaranteed\" by a\n+//! borrowed pointer? I mean any data that is reached by first\n+//! dereferencing a borrowed pointer and then either traversing\n+//! interior offsets or owned pointers.  We say that the guarantor\n+//! of such data it the region of the borrowed pointer that was\n+//! traversed.  This is essentially the same as the ownership\n+//! relation, except that a borrowed pointer never owns its\n+//! contents.\n+//!\n+//! ### Inferring borrow kinds for upvars\n+//!\n+//! Whenever there is a closure expression, we need to determine how each\n+//! upvar is used. We do this by initially assigning each upvar an\n+//! immutable \"borrow kind\" (see `ty::BorrowKind` for details) and then\n+//! \"escalating\" the kind as needed. The borrow kind proceeds according to\n+//! the following lattice:\n+//!\n+//!     ty::ImmBorrow -> ty::UniqueImmBorrow -> ty::MutBorrow\n+//!\n+//! So, for example, if we see an assignment `x = 5` to an upvar `x`, we\n+//! will promote its borrow kind to mutable borrow. If we see an `&mut x`\n+//! we'll do the same. Naturally, this applies not just to the upvar, but\n+//! to everything owned by `x`, so the result is the same for something\n+//! like `x.f = 5` and so on (presuming `x` is not a borrowed pointer to a\n+//! struct). These adjustments are performed in\n+//! `adjust_upvar_borrow_kind()` (you can trace backwards through the code\n+//! from there).\n+//!\n+//! The fact that we are inferring borrow kinds as we go results in a\n+//! semi-hacky interaction with mem-categorization. In particular,\n+//! mem-categorization will query the current borrow kind as it\n+//! categorizes, and we'll return the *current* value, but this may get\n+//! adjusted later. Therefore, in this module, we generally ignore the\n+//! borrow kind (and derived mutabilities) that are returned from\n+//! mem-categorization, since they may be inaccurate. (Another option\n+//! would be to use a unification scheme, where instead of returning a\n+//! concrete borrow kind like `ty::ImmBorrow`, we return a\n+//! `ty::InferBorrow(upvar_id)` or something like that, but this would\n+//! then mean that all later passes would have to check for these figments\n+//! and report an error, and it just seems like more mess in the end.)\n \n use middle::def;\n use middle::mem_categorization as mc;\n@@ -177,15 +173,11 @@ pub fn regionck_fn(fcx: &FnCtxt, id: ast::NodeId, blk: &ast::Block) {\n     fcx.infcx().resolve_regions_and_report_errors();\n }\n \n+/// Checks that the types in `component_tys` are well-formed. This will add constraints into the\n+/// region graph. Does *not* run `resolve_regions_and_report_errors` and so forth.\n pub fn regionck_ensure_component_tys_wf<'a, 'tcx>(fcx: &FnCtxt<'a, 'tcx>,\n                                                   span: Span,\n                                                   component_tys: &[Ty<'tcx>]) {\n-    /*!\n-     * Checks that the types in `component_tys` are well-formed.\n-     * This will add constraints into the region graph.\n-     * Does *not* run `resolve_regions_and_report_errors` and so forth.\n-     */\n-\n     let mut rcx = Rcx::new(fcx, 0);\n     for &component_ty in component_tys.iter() {\n         // Check that each type outlives the empty region. Since the\n@@ -239,12 +231,8 @@ pub struct Rcx<'a, 'tcx: 'a> {\n     maybe_links: MaybeLinkMap<'tcx>\n }\n \n+/// Returns the validity region of `def` -- that is, how long is `def` valid?\n fn region_of_def(fcx: &FnCtxt, def: def::Def) -> ty::Region {\n-    /*!\n-     * Returns the validity region of `def` -- that is, how long\n-     * is `def` valid?\n-     */\n-\n     let tcx = fcx.tcx();\n     match def {\n         def::DefLocal(node_id) => {\n@@ -283,35 +271,30 @@ impl<'a, 'tcx> Rcx<'a, 'tcx> {\n         old_scope\n     }\n \n+    /// Try to resolve the type for the given node, returning t_err if an error results.  Note that\n+    /// we never care about the details of the error, the same error will be detected and reported\n+    /// in the writeback phase.\n+    ///\n+    /// Note one important point: we do not attempt to resolve *region variables* here.  This is\n+    /// because regionck is essentially adding constraints to those region variables and so may yet\n+    /// influence how they are resolved.\n+    ///\n+    /// Consider this silly example:\n+    ///\n+    /// ```\n+    /// fn borrow(x: &int) -> &int {x}\n+    /// fn foo(x: @int) -> int {  // block: B\n+    ///     let b = borrow(x);    // region: <R0>\n+    ///     *b\n+    /// }\n+    /// ```\n+    ///\n+    /// Here, the region of `b` will be `<R0>`.  `<R0>` is constrainted to be some subregion of the\n+    /// block B and some superregion of the call.  If we forced it now, we'd choose the smaller\n+    /// region (the call).  But that would make the *b illegal.  Since we don't resolve, the type\n+    /// of b will be `&<R0>.int` and then `*b` will require that `<R0>` be bigger than the let and\n+    /// the `*b` expression, so we will effectively resolve `<R0>` to be the block B.\n     pub fn resolve_type(&self, unresolved_ty: Ty<'tcx>) -> Ty<'tcx> {\n-        /*!\n-         * Try to resolve the type for the given node, returning\n-         * t_err if an error results.  Note that we never care\n-         * about the details of the error, the same error will be\n-         * detected and reported in the writeback phase.\n-         *\n-         * Note one important point: we do not attempt to resolve\n-         * *region variables* here.  This is because regionck is\n-         * essentially adding constraints to those region variables\n-         * and so may yet influence how they are resolved.\n-         *\n-         * Consider this silly example:\n-         *\n-         *     fn borrow(x: &int) -> &int {x}\n-         *     fn foo(x: @int) -> int {  // block: B\n-         *         let b = borrow(x);    // region: <R0>\n-         *         *b\n-         *     }\n-         *\n-         * Here, the region of `b` will be `<R0>`.  `<R0>` is\n-         * constrainted to be some subregion of the block B and some\n-         * superregion of the call.  If we forced it now, we'd choose\n-         * the smaller region (the call).  But that would make the *b\n-         * illegal.  Since we don't resolve, the type of b will be\n-         * `&<R0>.int` and then `*b` will require that `<R0>` be\n-         * bigger than the let and the `*b` expression, so we will\n-         * effectively resolve `<R0>` to be the block B.\n-         */\n         match resolve_type(self.fcx.infcx(), None, unresolved_ty,\n                            resolve_and_force_all_but_regions) {\n             Ok(t) => t,\n@@ -384,25 +367,19 @@ impl<'a, 'tcx> Rcx<'a, 'tcx> {\n         }\n     }\n \n+    /// This method populates the region map's `free_region_map`. It walks over the transformed\n+    /// argument and return types for each function just before we check the body of that function,\n+    /// looking for types where you have a borrowed pointer to other borrowed data (e.g., `&'a &'b\n+    /// [uint]`.  We do not allow references to outlive the things they point at, so we can assume\n+    /// that `'a <= 'b`. This holds for both the argument and return types, basically because, on\n+    /// the caller side, the caller is responsible for checking that the type of every expression\n+    /// (including the actual values for the arguments, as well as the return type of the fn call)\n+    /// is well-formed.\n+    ///\n+    /// Tests: `src/test/compile-fail/regions-free-region-ordering-*.rs`\n     fn relate_free_regions(&mut self,\n                            fn_sig_tys: &[Ty<'tcx>],\n                            body_id: ast::NodeId) {\n-        /*!\n-         * This method populates the region map's `free_region_map`.\n-         * It walks over the transformed argument and return types for\n-         * each function just before we check the body of that\n-         * function, looking for types where you have a borrowed\n-         * pointer to other borrowed data (e.g., `&'a &'b [uint]`.  We\n-         * do not allow references to outlive the things they point\n-         * at, so we can assume that `'a <= 'b`. This holds for both\n-         * the argument and return types, basically because, on the caller\n-         * side, the caller is responsible for checking that the type of\n-         * every expression (including the actual values for the arguments,\n-         * as well as the return type of the fn call) is well-formed.\n-         *\n-         * Tests: `src/test/compile-fail/regions-free-region-ordering-*.rs`\n-         */\n-\n         debug!(\"relate_free_regions >>\");\n         let tcx = self.tcx();\n \n@@ -921,19 +898,15 @@ fn check_expr_fn_block(rcx: &mut Rcx,\n         _ => {}\n     }\n \n+    /// Make sure that the type of all free variables referenced inside a closure/proc outlive the\n+    /// closure/proc's lifetime bound. This is just a special case of the usual rules about closed\n+    /// over values outliving the object's lifetime bound.\n     fn ensure_free_variable_types_outlive_closure_bound(\n         rcx: &mut Rcx,\n         bounds: ty::ExistentialBounds,\n         expr: &ast::Expr,\n         freevars: &[ty::Freevar])\n     {\n-        /*!\n-         * Make sure that the type of all free variables referenced\n-         * inside a closure/proc outlive the closure/proc's lifetime\n-         * bound. This is just a special case of the usual rules about\n-         * closed over values outliving the object's lifetime bound.\n-         */\n-\n         let tcx = rcx.fcx.ccx.tcx;\n \n         debug!(\"ensure_free_variable_types_outlive_closure_bound({}, {})\",\n@@ -984,18 +957,14 @@ fn check_expr_fn_block(rcx: &mut Rcx,\n         }\n     }\n \n+    /// Make sure that all free variables referenced inside the closure outlive the closure's\n+    /// lifetime bound. Also, create an entry in the upvar_borrows map with a region.\n     fn constrain_free_variables_in_by_ref_closure(\n         rcx: &mut Rcx,\n         region_bound: ty::Region,\n         expr: &ast::Expr,\n         freevars: &[ty::Freevar])\n     {\n-        /*!\n-         * Make sure that all free variables referenced inside the\n-         * closure outlive the closure's lifetime bound. Also, create\n-         * an entry in the upvar_borrows map with a region.\n-         */\n-\n         let tcx = rcx.fcx.ccx.tcx;\n         let infcx = rcx.fcx.infcx();\n         debug!(\"constrain_free_variables({}, {})\",\n@@ -1183,15 +1152,12 @@ fn constrain_call<'a, I: Iterator<&'a ast::Expr>>(rcx: &mut Rcx,\n     }\n }\n \n+/// Invoked on any auto-dereference that occurs. Checks that if this is a region pointer being\n+/// dereferenced, the lifetime of the pointer includes the deref expr.\n fn constrain_autoderefs<'a, 'tcx>(rcx: &mut Rcx<'a, 'tcx>,\n                                   deref_expr: &ast::Expr,\n                                   derefs: uint,\n                                   mut derefd_ty: Ty<'tcx>) {\n-    /*!\n-     * Invoked on any auto-dereference that occurs.  Checks that if\n-     * this is a region pointer being dereferenced, the lifetime of\n-     * the pointer includes the deref expr.\n-     */\n     let r_deref_expr = ty::ReScope(CodeExtent::from_node_id(deref_expr.id));\n     for i in range(0u, derefs) {\n         debug!(\"constrain_autoderefs(deref_expr=?, derefd_ty={}, derefs={}/{}\",\n@@ -1259,16 +1225,12 @@ pub fn mk_subregion_due_to_dereference(rcx: &mut Rcx,\n }\n \n \n+/// Invoked on any index expression that occurs. Checks that if this is a slice being indexed, the\n+/// lifetime of the pointer includes the deref expr.\n fn constrain_index<'a, 'tcx>(rcx: &mut Rcx<'a, 'tcx>,\n                              index_expr: &ast::Expr,\n                              indexed_ty: Ty<'tcx>)\n {\n-    /*!\n-     * Invoked on any index expression that occurs.  Checks that if\n-     * this is a slice being indexed, the lifetime of the pointer\n-     * includes the deref expr.\n-     */\n-\n     debug!(\"constrain_index(index_expr=?, indexed_ty={}\",\n            rcx.fcx.infcx().ty_to_string(indexed_ty));\n \n@@ -1286,18 +1248,14 @@ fn constrain_index<'a, 'tcx>(rcx: &mut Rcx<'a, 'tcx>,\n     }\n }\n \n+/// Guarantees that any lifetimes which appear in the type of the node `id` (after applying\n+/// adjustments) are valid for at least `minimum_lifetime`\n fn type_of_node_must_outlive<'a, 'tcx>(\n     rcx: &mut Rcx<'a, 'tcx>,\n     origin: infer::SubregionOrigin<'tcx>,\n     id: ast::NodeId,\n     minimum_lifetime: ty::Region)\n {\n-    /*!\n-     * Guarantees that any lifetimes which appear in the type of\n-     * the node `id` (after applying adjustments) are valid for at\n-     * least `minimum_lifetime`\n-     */\n-\n     let tcx = rcx.fcx.tcx();\n \n     // Try to resolve the type.  If we encounter an error, then typeck\n@@ -1314,14 +1272,10 @@ fn type_of_node_must_outlive<'a, 'tcx>(\n     type_must_outlive(rcx, origin, ty, minimum_lifetime);\n }\n \n+/// Computes the guarantor for an expression `&base` and then ensures that the lifetime of the\n+/// resulting pointer is linked to the lifetime of its guarantor (if any).\n fn link_addr_of(rcx: &mut Rcx, expr: &ast::Expr,\n                mutability: ast::Mutability, base: &ast::Expr) {\n-    /*!\n-     * Computes the guarantor for an expression `&base` and then\n-     * ensures that the lifetime of the resulting pointer is linked\n-     * to the lifetime of its guarantor (if any).\n-     */\n-\n     debug!(\"link_addr_of(base=?)\");\n \n     let cmt = {\n@@ -1331,13 +1285,10 @@ fn link_addr_of(rcx: &mut Rcx, expr: &ast::Expr,\n     link_region_from_node_type(rcx, expr.span, expr.id, mutability, cmt);\n }\n \n+/// Computes the guarantors for any ref bindings in a `let` and\n+/// then ensures that the lifetime of the resulting pointer is\n+/// linked to the lifetime of the initialization expression.\n fn link_local(rcx: &Rcx, local: &ast::Local) {\n-    /*!\n-     * Computes the guarantors for any ref bindings in a `let` and\n-     * then ensures that the lifetime of the resulting pointer is\n-     * linked to the lifetime of the initialization expression.\n-     */\n-\n     debug!(\"regionck::for_local()\");\n     let init_expr = match local.init {\n         None => { return; }\n@@ -1348,12 +1299,10 @@ fn link_local(rcx: &Rcx, local: &ast::Local) {\n     link_pattern(rcx, mc, discr_cmt, &*local.pat);\n }\n \n+/// Computes the guarantors for any ref bindings in a match and\n+/// then ensures that the lifetime of the resulting pointer is\n+/// linked to the lifetime of its guarantor (if any).\n fn link_match(rcx: &Rcx, discr: &ast::Expr, arms: &[ast::Arm]) {\n-    /*!\n-     * Computes the guarantors for any ref bindings in a match and\n-     * then ensures that the lifetime of the resulting pointer is\n-     * linked to the lifetime of its guarantor (if any).\n-     */\n \n     debug!(\"regionck::for_match()\");\n     let mc = mc::MemCategorizationContext::new(rcx);\n@@ -1366,15 +1315,12 @@ fn link_match(rcx: &Rcx, discr: &ast::Expr, arms: &[ast::Arm]) {\n     }\n }\n \n+/// Link lifetimes of any ref bindings in `root_pat` to the pointers found in the discriminant, if\n+/// needed.\n fn link_pattern<'a, 'tcx>(rcx: &Rcx<'a, 'tcx>,\n                           mc: mc::MemCategorizationContext<Rcx<'a, 'tcx>>,\n                           discr_cmt: mc::cmt<'tcx>,\n                           root_pat: &ast::Pat) {\n-    /*!\n-     * Link lifetimes of any ref bindings in `root_pat` to\n-     * the pointers found in the discriminant, if needed.\n-     */\n-\n     let _ = mc.cat_pattern(discr_cmt, root_pat, |mc, sub_cmt, sub_pat| {\n             match sub_pat.node {\n                 // `ref x` pattern\n@@ -1400,14 +1346,12 @@ fn link_pattern<'a, 'tcx>(rcx: &Rcx<'a, 'tcx>,\n         });\n }\n \n+/// Link lifetime of borrowed pointer resulting from autoref to lifetimes in the value being\n+/// autoref'd.\n fn link_autoref(rcx: &Rcx,\n                 expr: &ast::Expr,\n                 autoderefs: uint,\n                 autoref: &ty::AutoRef) {\n-    /*!\n-     * Link lifetime of borrowed pointer resulting from autoref\n-     * to lifetimes in the value being autoref'd.\n-     */\n \n     debug!(\"link_autoref(autoref={})\", autoref);\n     let mc = mc::MemCategorizationContext::new(rcx);\n@@ -1424,15 +1368,11 @@ fn link_autoref(rcx: &Rcx,\n     }\n }\n \n+/// Computes the guarantor for cases where the `expr` is being passed by implicit reference and\n+/// must outlive `callee_scope`.\n fn link_by_ref(rcx: &Rcx,\n                expr: &ast::Expr,\n                callee_scope: CodeExtent) {\n-    /*!\n-     * Computes the guarantor for cases where the `expr` is\n-     * being passed by implicit reference and must outlive\n-     * `callee_scope`.\n-     */\n-\n     let tcx = rcx.tcx();\n     debug!(\"link_by_ref(expr={}, callee_scope={})\",\n            expr.repr(tcx), callee_scope);\n@@ -1442,17 +1382,13 @@ fn link_by_ref(rcx: &Rcx,\n     link_region(rcx, expr.span, borrow_region, ty::ImmBorrow, expr_cmt);\n }\n \n+/// Like `link_region()`, except that the region is extracted from the type of `id`, which must be\n+/// some reference (`&T`, `&str`, etc).\n fn link_region_from_node_type<'a, 'tcx>(rcx: &Rcx<'a, 'tcx>,\n                                         span: Span,\n                                         id: ast::NodeId,\n                                         mutbl: ast::Mutability,\n                                         cmt_borrowed: mc::cmt<'tcx>) {\n-    /*!\n-     * Like `link_region()`, except that the region is\n-     * extracted from the type of `id`, which must be some\n-     * reference (`&T`, `&str`, etc).\n-     */\n-\n     let rptr_ty = rcx.resolve_node_type(id);\n     if !ty::type_is_error(rptr_ty) {\n         let tcx = rcx.fcx.ccx.tcx;\n@@ -1463,19 +1399,14 @@ fn link_region_from_node_type<'a, 'tcx>(rcx: &Rcx<'a, 'tcx>,\n     }\n }\n \n+/// Informs the inference engine that `borrow_cmt` is being borrowed with kind `borrow_kind` and\n+/// lifetime `borrow_region`. In order to ensure borrowck is satisfied, this may create constraints\n+/// between regions, as explained in `link_reborrowed_region()`.\n fn link_region<'a, 'tcx>(rcx: &Rcx<'a, 'tcx>,\n                          span: Span,\n                          borrow_region: ty::Region,\n                          borrow_kind: ty::BorrowKind,\n                          borrow_cmt: mc::cmt<'tcx>) {\n-    /*!\n-     * Informs the inference engine that `borrow_cmt` is being\n-     * borrowed with kind `borrow_kind` and lifetime `borrow_region`.\n-     * In order to ensure borrowck is satisfied, this may create\n-     * constraints between regions, as explained in\n-     * `link_reborrowed_region()`.\n-     */\n-\n     let mut borrow_cmt = borrow_cmt;\n     let mut borrow_kind = borrow_kind;\n \n@@ -1525,6 +1456,46 @@ fn link_region<'a, 'tcx>(rcx: &Rcx<'a, 'tcx>,\n     }\n }\n \n+/// This is the most complicated case: the path being borrowed is\n+/// itself the referent of a borrowed pointer. Let me give an\n+/// example fragment of code to make clear(er) the situation:\n+///\n+///    let r: &'a mut T = ...;  // the original reference \"r\" has lifetime 'a\n+///    ...\n+///    &'z *r                   // the reborrow has lifetime 'z\n+///\n+/// Now, in this case, our primary job is to add the inference\n+/// constraint that `'z <= 'a`. Given this setup, let's clarify the\n+/// parameters in (roughly) terms of the example:\n+///\n+///     A borrow of: `& 'z bk * r` where `r` has type `& 'a bk T`\n+///     borrow_region   ^~                 ref_region    ^~\n+///     borrow_kind        ^~               ref_kind        ^~\n+///     ref_cmt                 ^\n+///\n+/// Here `bk` stands for some borrow-kind (e.g., `mut`, `uniq`, etc).\n+///\n+/// Unfortunately, there are some complications beyond the simple\n+/// scenario I just painted:\n+///\n+/// 1. The reference `r` might in fact be a \"by-ref\" upvar. In that\n+///    case, we have two jobs. First, we are inferring whether this reference\n+///    should be an `&T`, `&mut T`, or `&uniq T` reference, and we must\n+///    adjust that based on this borrow (e.g., if this is an `&mut` borrow,\n+///    then `r` must be an `&mut` reference). Second, whenever we link\n+///    two regions (here, `'z <= 'a`), we supply a *cause*, and in this\n+///    case we adjust the cause to indicate that the reference being\n+///    \"reborrowed\" is itself an upvar. This provides a nicer error message\n+///    should something go wrong.\n+///\n+/// 2. There may in fact be more levels of reborrowing. In the\n+///    example, I said the borrow was like `&'z *r`, but it might\n+///    in fact be a borrow like `&'z **q` where `q` has type `&'a\n+///    &'b mut T`. In that case, we want to ensure that `'z <= 'a`\n+///    and `'z <= 'b`. This is explained more below.\n+///\n+/// The return value of this function indicates whether we need to\n+/// recurse and process `ref_cmt` (see case 2 above).\n fn link_reborrowed_region<'a, 'tcx>(rcx: &Rcx<'a, 'tcx>,\n                                     span: Span,\n                                     borrow_region: ty::Region,\n@@ -1535,49 +1506,6 @@ fn link_reborrowed_region<'a, 'tcx>(rcx: &Rcx<'a, 'tcx>,\n                                     note: mc::Note)\n                                     -> Option<(mc::cmt<'tcx>, ty::BorrowKind)>\n {\n-    /*!\n-     * This is the most complicated case: the path being borrowed is\n-     * itself the referent of a borrowed pointer. Let me give an\n-     * example fragment of code to make clear(er) the situation:\n-     *\n-     *    let r: &'a mut T = ...;  // the original reference \"r\" has lifetime 'a\n-     *    ...\n-     *    &'z *r                   // the reborrow has lifetime 'z\n-     *\n-     * Now, in this case, our primary job is to add the inference\n-     * constraint that `'z <= 'a`. Given this setup, let's clarify the\n-     * parameters in (roughly) terms of the example:\n-     *\n-     *     A borrow of: `& 'z bk * r` where `r` has type `& 'a bk T`\n-     *     borrow_region   ^~                 ref_region    ^~\n-     *     borrow_kind        ^~               ref_kind        ^~\n-     *     ref_cmt                 ^\n-     *\n-     * Here `bk` stands for some borrow-kind (e.g., `mut`, `uniq`, etc).\n-     *\n-     * Unfortunately, there are some complications beyond the simple\n-     * scenario I just painted:\n-     *\n-     * 1. The reference `r` might in fact be a \"by-ref\" upvar. In that\n-     *    case, we have two jobs. First, we are inferring whether this reference\n-     *    should be an `&T`, `&mut T`, or `&uniq T` reference, and we must\n-     *    adjust that based on this borrow (e.g., if this is an `&mut` borrow,\n-     *    then `r` must be an `&mut` reference). Second, whenever we link\n-     *    two regions (here, `'z <= 'a`), we supply a *cause*, and in this\n-     *    case we adjust the cause to indicate that the reference being\n-     *    \"reborrowed\" is itself an upvar. This provides a nicer error message\n-     *    should something go wrong.\n-     *\n-     * 2. There may in fact be more levels of reborrowing. In the\n-     *    example, I said the borrow was like `&'z *r`, but it might\n-     *    in fact be a borrow like `&'z **q` where `q` has type `&'a\n-     *    &'b mut T`. In that case, we want to ensure that `'z <= 'a`\n-     *    and `'z <= 'b`. This is explained more below.\n-     *\n-     * The return value of this function indicates whether we need to\n-     * recurse and process `ref_cmt` (see case 2 above).\n-     */\n-\n     // Possible upvar ID we may need later to create an entry in the\n     // maybe link map.\n \n@@ -1715,27 +1643,19 @@ fn link_reborrowed_region<'a, 'tcx>(rcx: &Rcx<'a, 'tcx>,\n     }\n }\n \n+/// Adjusts the inferred borrow_kind as needed to account for upvars that are assigned to in an\n+/// assignment expression.\n fn adjust_borrow_kind_for_assignment_lhs(rcx: &Rcx,\n                                          lhs: &ast::Expr) {\n-    /*!\n-     * Adjusts the inferred borrow_kind as needed to account\n-     * for upvars that are assigned to in an assignment\n-     * expression.\n-     */\n-\n     let mc = mc::MemCategorizationContext::new(rcx);\n     let cmt = ignore_err!(mc.cat_expr(lhs));\n     adjust_upvar_borrow_kind_for_mut(rcx, cmt);\n }\n \n+/// Indicates that `cmt` is being directly mutated (e.g., assigned to). If cmt contains any by-ref\n+/// upvars, this implies that those upvars must be borrowed using an `&mut` borow.\n fn adjust_upvar_borrow_kind_for_mut<'a, 'tcx>(rcx: &Rcx<'a, 'tcx>,\n                                               cmt: mc::cmt<'tcx>) {\n-    /*!\n-     * Indicates that `cmt` is being directly mutated (e.g., assigned\n-     * to).  If cmt contains any by-ref upvars, this implies that\n-     * those upvars must be borrowed using an `&mut` borow.\n-     */\n-\n     let mut cmt = cmt;\n     loop {\n         debug!(\"adjust_upvar_borrow_kind_for_mut(cmt={})\",\n@@ -1834,16 +1754,12 @@ fn adjust_upvar_borrow_kind_for_unique<'a, 'tcx>(rcx: &Rcx<'a, 'tcx>, cmt: mc::c\n     }\n }\n \n+/// Indicates that the borrow_kind of `outer_upvar_id` must permit a reborrowing with the\n+/// borrow_kind of `inner_upvar_id`. This occurs in nested closures, see comment above at the call\n+/// to this function.\n fn link_upvar_borrow_kind_for_nested_closures(rcx: &mut Rcx,\n                                               inner_upvar_id: ty::UpvarId,\n                                               outer_upvar_id: ty::UpvarId) {\n-    /*!\n-     * Indicates that the borrow_kind of `outer_upvar_id` must\n-     * permit a reborrowing with the borrow_kind of `inner_upvar_id`.\n-     * This occurs in nested closures, see comment above at the call to\n-     * this function.\n-     */\n-\n     debug!(\"link_upvar_borrow_kind: inner_upvar_id={} outer_upvar_id={}\",\n            inner_upvar_id, outer_upvar_id);\n \n@@ -1867,18 +1783,14 @@ fn adjust_upvar_borrow_kind_for_loan(rcx: &Rcx,\n     adjust_upvar_borrow_kind(rcx, upvar_id, upvar_borrow, kind)\n }\n \n+/// We infer the borrow_kind with which to borrow upvars in a stack closure. The borrow_kind\n+/// basically follows a lattice of `imm < unique-imm < mut`, moving from left to right as needed\n+/// (but never right to left). Here the argument `mutbl` is the borrow_kind that is required by\n+/// some particular use.\n fn adjust_upvar_borrow_kind(rcx: &Rcx,\n                             upvar_id: ty::UpvarId,\n                             upvar_borrow: &mut ty::UpvarBorrow,\n                             kind: ty::BorrowKind) {\n-    /*!\n-     * We infer the borrow_kind with which to borrow upvars in a stack\n-     * closure. The borrow_kind basically follows a lattice of\n-     * `imm < unique-imm < mut`, moving from left to right as needed (but never\n-     * right to left). Here the argument `mutbl` is the borrow_kind that\n-     * is required by some particular use.\n-     */\n-\n     debug!(\"adjust_upvar_borrow_kind: id={} kind=({} -> {})\",\n            upvar_id, upvar_borrow.kind, kind);\n \n@@ -1911,15 +1823,12 @@ fn adjust_upvar_borrow_kind(rcx: &Rcx,\n     }\n }\n \n+/// Ensures that all borrowed data reachable via `ty` outlives `region`.\n fn type_must_outlive<'a, 'tcx>(rcx: &mut Rcx<'a, 'tcx>,\n                                origin: infer::SubregionOrigin<'tcx>,\n                                ty: Ty<'tcx>,\n                                region: ty::Region)\n {\n-    /*!\n-     * Ensures that all borrowed data reachable via `ty` outlives `region`.\n-     */\n-\n     debug!(\"type_must_outlive(ty={}, region={})\",\n            ty.repr(rcx.tcx()),\n            region.repr(rcx.tcx()));"}, {"sha": "55214618aa90b5856bb8968487e07e45ae44617a", "filename": "src/librustc/middle/typeck/check/regionmanip.rs", "status": "modified", "additions": 6, "deletions": 17, "changes": 23, "blob_url": "https://github.com/rust-lang/rust/blob/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc%2Fmiddle%2Ftypeck%2Fcheck%2Fregionmanip.rs", "raw_url": "https://github.com/rust-lang/rust/raw/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc%2Fmiddle%2Ftypeck%2Fcheck%2Fregionmanip.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Fmiddle%2Ftypeck%2Fcheck%2Fregionmanip.rs?ref=cd5c8235c5448a7234548c772468c8d2e8f150d9", "patch": "@@ -33,18 +33,14 @@ struct Wf<'a, 'tcx: 'a> {\n     out: Vec<WfConstraint<'tcx>>,\n }\n \n+/// This routine computes the well-formedness constraints that must hold for the type `ty` to\n+/// appear in a context with lifetime `outer_region`\n pub fn region_wf_constraints<'tcx>(\n     tcx: &ty::ctxt<'tcx>,\n     ty: Ty<'tcx>,\n     outer_region: ty::Region)\n     -> Vec<WfConstraint<'tcx>>\n {\n-    /*!\n-     * This routine computes the well-formedness constraints that must\n-     * hold for the type `ty` to appear in a context with lifetime\n-     * `outer_region`\n-     */\n-\n     let mut stack = Vec::new();\n     stack.push((outer_region, None));\n     let mut wf = Wf { tcx: tcx,\n@@ -168,12 +164,9 @@ impl<'a, 'tcx> Wf<'a, 'tcx> {\n         self.stack.pop().unwrap();\n     }\n \n+    /// Pushes a constraint that `r_b` must outlive the top region on the stack.\n     fn push_region_constraint_from_top(&mut self,\n                                        r_b: ty::Region) {\n-        /*!\n-         * Pushes a constraint that `r_b` must outlive the\n-         * top region on the stack.\n-         */\n \n         // Indicates that we have found borrowed content with a lifetime\n         // of at least `r_b`. This adds a constraint that `r_b` must\n@@ -192,30 +185,26 @@ impl<'a, 'tcx> Wf<'a, 'tcx> {\n         self.push_sub_region_constraint(opt_ty, r_a, r_b);\n     }\n \n+    /// Pushes a constraint that `r_a <= r_b`, due to `opt_ty`\n     fn push_sub_region_constraint(&mut self,\n                                   opt_ty: Option<Ty<'tcx>>,\n                                   r_a: ty::Region,\n                                   r_b: ty::Region) {\n-        /*! Pushes a constraint that `r_a <= r_b`, due to `opt_ty` */\n         self.out.push(RegionSubRegionConstraint(opt_ty, r_a, r_b));\n     }\n \n+    /// Pushes a constraint that `param_ty` must outlive the top region on the stack.\n     fn push_param_constraint_from_top(&mut self,\n                                       param_ty: ty::ParamTy) {\n-        /*!\n-         * Pushes a constraint that `param_ty` must outlive the\n-         * top region on the stack.\n-         */\n-\n         let &(region, opt_ty) = self.stack.last().unwrap();\n         self.push_param_constraint(region, opt_ty, param_ty);\n     }\n \n+    /// Pushes a constraint that `region <= param_ty`, due to `opt_ty`\n     fn push_param_constraint(&mut self,\n                              region: ty::Region,\n                              opt_ty: Option<Ty<'tcx>>,\n                              param_ty: ty::ParamTy) {\n-        /*! Pushes a constraint that `region <= param_ty`, due to `opt_ty` */\n         self.out.push(RegionSubParamConstraint(opt_ty, region, param_ty));\n     }\n "}, {"sha": "51978a01f712410bb0f8857f3205a376e863c81c", "filename": "src/librustc/middle/typeck/check/vtable.rs", "status": "modified", "additions": 9, "deletions": 16, "changes": 25, "blob_url": "https://github.com/rust-lang/rust/blob/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc%2Fmiddle%2Ftypeck%2Fcheck%2Fvtable.rs", "raw_url": "https://github.com/rust-lang/rust/raw/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc%2Fmiddle%2Ftypeck%2Fcheck%2Fvtable.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Fmiddle%2Ftypeck%2Fcheck%2Fvtable.rs?ref=cd5c8235c5448a7234548c772468c8d2e8f150d9", "patch": "@@ -168,17 +168,14 @@ pub fn check_object_safety<'tcx>(tcx: &ty::ctxt<'tcx>,\n         }\n     }\n \n-    // Returns a vec of error messages. If hte vec is empty - no errors!\n+    /// Returns a vec of error messages. If hte vec is empty - no errors!\n+    ///\n+    /// There are some limitations to calling functions through an object, because (a) the self\n+    /// type is not known (that's the whole point of a trait instance, after all, to obscure the\n+    /// self type) and (b) the call must go through a vtable and hence cannot be monomorphized.\n     fn check_object_safety_of_method<'tcx>(tcx: &ty::ctxt<'tcx>,\n                                            method: &ty::Method<'tcx>)\n                                            -> Vec<String> {\n-        /*!\n-         * There are some limitations to calling functions through an\n-         * object, because (a) the self type is not known\n-         * (that's the whole point of a trait instance, after all, to\n-         * obscure the self type) and (b) the call must go through a\n-         * vtable and hence cannot be monomorphized.\n-         */\n         let mut msgs = Vec::new();\n \n         let method_name = method.name.repr(tcx);\n@@ -455,8 +452,8 @@ pub fn maybe_report_ambiguity<'a, 'tcx>(fcx: &FnCtxt<'a, 'tcx>,\n     }\n }\n \n+/// Select as many obligations as we can at present.\n pub fn select_fcx_obligations_where_possible(fcx: &FnCtxt) {\n-    /*! Select as many obligations as we can at present. */\n \n     match\n         fcx.inh.fulfillment_cx\n@@ -468,14 +465,10 @@ pub fn select_fcx_obligations_where_possible(fcx: &FnCtxt) {\n     }\n }\n \n+/// Try to select any fcx obligation that we haven't tried yet, in an effort to improve inference.\n+/// You could just call `select_fcx_obligations_where_possible` except that it leads to repeated\n+/// work.\n pub fn select_new_fcx_obligations(fcx: &FnCtxt) {\n-    /*!\n-     * Try to select any fcx obligation that we haven't tried yet,\n-     * in an effort to improve inference. You could just call\n-     * `select_fcx_obligations_where_possible` except that it leads\n-     * to repeated work.\n-     */\n-\n     match\n         fcx.inh.fulfillment_cx\n         .borrow_mut()"}, {"sha": "502e37aa9f37040fcea2e16f4403a127015519d3", "filename": "src/librustc/middle/typeck/check/wf.rs", "status": "modified", "additions": 21, "deletions": 37, "changes": 58, "blob_url": "https://github.com/rust-lang/rust/blob/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc%2Fmiddle%2Ftypeck%2Fcheck%2Fwf.rs", "raw_url": "https://github.com/rust-lang/rust/raw/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc%2Fmiddle%2Ftypeck%2Fcheck%2Fwf.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Fmiddle%2Ftypeck%2Fcheck%2Fwf.rs?ref=cd5c8235c5448a7234548c772468c8d2e8f150d9", "patch": "@@ -38,24 +38,18 @@ impl<'ccx, 'tcx> CheckTypeWellFormedVisitor<'ccx, 'tcx> {\n         CheckTypeWellFormedVisitor { ccx: ccx, cache: HashSet::new() }\n     }\n \n+    /// Checks that the field types (in a struct def'n) or argument types (in an enum def'n) are\n+    /// well-formed, meaning that they do not require any constraints not declared in the struct\n+    /// definition itself. For example, this definition would be illegal:\n+    ///\n+    ///     struct Ref<'a, T> { x: &'a T }\n+    ///\n+    /// because the type did not declare that `T:'a`.\n+    ///\n+    /// We do this check as a pre-pass before checking fn bodies because if these constraints are\n+    /// not included it frequently leads to confusing errors in fn bodies. So it's better to check\n+    /// the types first.\n     fn check_item_well_formed(&mut self, item: &ast::Item) {\n-        /*!\n-         * Checks that the field types (in a struct def'n) or\n-         * argument types (in an enum def'n) are well-formed,\n-         * meaning that they do not require any constraints not\n-         * declared in the struct definition itself.\n-         * For example, this definition would be illegal:\n-         *\n-         *     struct Ref<'a, T> { x: &'a T }\n-         *\n-         * because the type did not declare that `T:'a`.\n-         *\n-         * We do this check as a pre-pass before checking fn bodies\n-         * because if these constraints are not included it frequently\n-         * leads to confusing errors in fn bodies. So it's better to check\n-         * the types first.\n-         */\n-\n         let ccx = self.ccx;\n         debug!(\"check_item_well_formed(it.id={}, it.ident={})\",\n                item.id,\n@@ -107,16 +101,12 @@ impl<'ccx, 'tcx> CheckTypeWellFormedVisitor<'ccx, 'tcx> {\n         regionck::regionck_item(&fcx, item);\n     }\n \n+    /// In a type definition, we check that to ensure that the types of the fields are well-formed.\n     fn check_type_defn(&mut self,\n                        item: &ast::Item,\n                        lookup_fields: for<'fcx> |&FnCtxt<'fcx, 'tcx>|\n                                                  -> Vec<AdtVariant<'tcx>>)\n     {\n-        /*!\n-         * In a type definition, we check that to ensure that the types of the fields are\n-         * well-formed.\n-         */\n-\n         self.with_fcx(item, |this, fcx| {\n             let variants = lookup_fields(fcx);\n             let mut bounds_checker = BoundsChecker::new(fcx,\n@@ -282,22 +272,16 @@ impl<'cx,'tcx> BoundsChecker<'cx,'tcx> {\n                         cache: cache, binding_count: 0 }\n     }\n \n+    /// Given a trait ref like `A : Trait<B>`, where `Trait` is defined as (say):\n+    ///\n+    ///     trait Trait<B:OtherTrait> : Copy { ... }\n+    ///\n+    /// This routine will check that `B : OtherTrait` and `A : Trait<B>`. It will also recursively\n+    /// check that the types `A` and `B` are well-formed.\n+    ///\n+    /// Note that it does not (currently, at least) check that `A : Copy` (that check is delegated\n+    /// to the point where impl `A : Trait<B>` is implemented).\n     pub fn check_trait_ref(&mut self, trait_ref: &ty::TraitRef<'tcx>) {\n-        /*!\n-         * Given a trait ref like `A : Trait<B>`, where `Trait` is\n-         * defined as (say):\n-         *\n-         *     trait Trait<B:OtherTrait> : Copy { ... }\n-         *\n-         * This routine will check that `B : OtherTrait` and `A :\n-         * Trait<B>`. It will also recursively check that the types\n-         * `A` and `B` are well-formed.\n-         *\n-         * Note that it does not (currently, at least)\n-         * check that `A : Copy` (that check is delegated to the point\n-         * where impl `A : Trait<B>` is implemented).\n-         */\n-\n         let trait_def = ty::lookup_trait_def(self.fcx.tcx(), trait_ref.def_id);\n \n         let bounds = trait_def.generics.to_bounds(self.tcx(), &trait_ref.substs);"}, {"sha": "758608b79c2cb226fefbce0c650d261bd93e784d", "filename": "src/librustc/middle/typeck/coherence/mod.rs", "status": "modified", "additions": 2, "deletions": 6, "changes": 8, "blob_url": "https://github.com/rust-lang/rust/blob/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc%2Fmiddle%2Ftypeck%2Fcoherence%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc%2Fmiddle%2Ftypeck%2Fcoherence%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Fmiddle%2Ftypeck%2Fcoherence%2Fmod.rs?ref=cd5c8235c5448a7234548c772468c8d2e8f150d9", "patch": "@@ -477,17 +477,13 @@ impl<'a, 'tcx> CoherenceChecker<'a, 'tcx> {\n     }\n }\n \n+/// Substitutes the values for the receiver's type parameters that are found in method, leaving the\n+/// method's type parameters intact.\n pub fn make_substs_for_receiver_types<'tcx>(tcx: &ty::ctxt<'tcx>,\n                                             trait_ref: &ty::TraitRef<'tcx>,\n                                             method: &ty::Method<'tcx>)\n                                             -> subst::Substs<'tcx>\n {\n-    /*!\n-     * Substitutes the values for the receiver's type parameters\n-     * that are found in method, leaving the method's type parameters\n-     * intact.\n-     */\n-\n     let meth_tps: Vec<Ty> =\n         method.generics.types.get_slice(subst::FnSpace)\n               .iter()"}, {"sha": "dc3afaae35f615800056ddbfef49787cadf128c6", "filename": "src/librustc/middle/typeck/coherence/orphan.rs", "status": "modified", "additions": 2, "deletions": 4, "changes": 6, "blob_url": "https://github.com/rust-lang/rust/blob/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc%2Fmiddle%2Ftypeck%2Fcoherence%2Forphan.rs", "raw_url": "https://github.com/rust-lang/rust/raw/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc%2Fmiddle%2Ftypeck%2Fcoherence%2Forphan.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Fmiddle%2Ftypeck%2Fcoherence%2Forphan.rs?ref=cd5c8235c5448a7234548c772468c8d2e8f150d9", "patch": "@@ -8,10 +8,8 @@\n // option. This file may not be copied, modified, or distributed\n // except according to those terms.\n \n-/*!\n- * Orphan checker: every impl either implements a trait defined in this\n- * crate or pertains to a type defined in this crate.\n- */\n+//! Orphan checker: every impl either implements a trait defined in this\n+//! crate or pertains to a type defined in this crate.\n \n use middle::traits;\n use middle::ty;"}, {"sha": "9f10a58f45852eb81c76c28f789a151639c8b9cb", "filename": "src/librustc/middle/typeck/coherence/overlap.rs", "status": "modified", "additions": 2, "deletions": 4, "changes": 6, "blob_url": "https://github.com/rust-lang/rust/blob/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc%2Fmiddle%2Ftypeck%2Fcoherence%2Foverlap.rs", "raw_url": "https://github.com/rust-lang/rust/raw/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc%2Fmiddle%2Ftypeck%2Fcoherence%2Foverlap.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Fmiddle%2Ftypeck%2Fcoherence%2Foverlap.rs?ref=cd5c8235c5448a7234548c772468c8d2e8f150d9", "patch": "@@ -8,10 +8,8 @@\n // option. This file may not be copied, modified, or distributed\n // except according to those terms.\n \n-/*!\n- * Overlap: No two impls for the same trait are implemented for the\n- * same type.\n- */\n+//! Overlap: No two impls for the same trait are implemented for the\n+//! same type.\n \n use middle::traits;\n use middle::ty;"}, {"sha": "3a62978ed007a37ba900505219159153eb26ddb6", "filename": "src/librustc/middle/typeck/collect.rs", "status": "modified", "additions": 5, "deletions": 12, "changes": 17, "blob_url": "https://github.com/rust-lang/rust/blob/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc%2Fmiddle%2Ftypeck%2Fcollect.rs", "raw_url": "https://github.com/rust-lang/rust/raw/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc%2Fmiddle%2Ftypeck%2Fcollect.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Fmiddle%2Ftypeck%2Fcollect.rs?ref=cd5c8235c5448a7234548c772468c8d2e8f150d9", "patch": "@@ -1944,6 +1944,9 @@ fn get_or_create_type_parameter_def<'tcx,AC>(this: &AC,\n     def\n }\n \n+/// Translate the AST's notion of ty param bounds (which are an enum consisting of a newtyped Ty or\n+/// a region) to ty's notion of ty param bounds, which can either be user-defined traits, or the\n+/// built-in trait (formerly known as kind): Send.\n fn compute_bounds<'tcx,AC>(this: &AC,\n                            name_of_bounded_thing: ast::Name,\n                            param_ty: ty::ParamTy,\n@@ -1953,13 +1956,6 @@ fn compute_bounds<'tcx,AC>(this: &AC,\n                            where_clause: &ast::WhereClause)\n                            -> ty::ParamBounds<'tcx>\n                            where AC: AstConv<'tcx> {\n-    /*!\n-     * Translate the AST's notion of ty param bounds (which are an\n-     * enum consisting of a newtyped Ty or a region) to ty's\n-     * notion of ty param bounds, which can either be user-defined\n-     * traits, or the built-in trait (formerly known as kind): Send.\n-     */\n-\n     let mut param_bounds = conv_param_bounds(this,\n                                              span,\n                                              param_ty,\n@@ -2040,16 +2036,13 @@ fn conv_param_bounds<'tcx,AC>(this: &AC,\n     }\n }\n \n+/// Merges the bounds declared on a type parameter with those found from where clauses into a\n+/// single list.\n fn merge_param_bounds<'a>(tcx: &ty::ctxt,\n                           param_ty: ty::ParamTy,\n                           ast_bounds: &'a [ast::TyParamBound],\n                           where_clause: &'a ast::WhereClause)\n                           -> Vec<&'a ast::TyParamBound> {\n-    /*!\n-     * Merges the bounds declared on a type parameter with those\n-     * found from where clauses into a single list.\n-     */\n-\n     let mut result = Vec::new();\n \n     for ast_bound in ast_bounds.iter() {"}, {"sha": "51f8668692ea71dab909949f7c5196a6c28d7851", "filename": "src/librustc/middle/typeck/infer/coercion.rs", "status": "modified", "additions": 53, "deletions": 61, "changes": 114, "blob_url": "https://github.com/rust-lang/rust/blob/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc%2Fmiddle%2Ftypeck%2Finfer%2Fcoercion.rs", "raw_url": "https://github.com/rust-lang/rust/raw/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc%2Fmiddle%2Ftypeck%2Finfer%2Fcoercion.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Fmiddle%2Ftypeck%2Finfer%2Fcoercion.rs?ref=cd5c8235c5448a7234548c772468c8d2e8f150d9", "patch": "@@ -8,61 +8,57 @@\n // option. This file may not be copied, modified, or distributed\n // except according to those terms.\n \n-/*!\n-\n-# Type Coercion\n-\n-Under certain circumstances we will coerce from one type to another,\n-for example by auto-borrowing.  This occurs in situations where the\n-compiler has a firm 'expected type' that was supplied from the user,\n-and where the actual type is similar to that expected type in purpose\n-but not in representation (so actual subtyping is inappropriate).\n-\n-## Reborrowing\n-\n-Note that if we are expecting a reference, we will *reborrow*\n-even if the argument provided was already a reference.  This is\n-useful for freezing mut/const things (that is, when the expected is &T\n-but you have &const T or &mut T) and also for avoiding the linearity\n-of mut things (when the expected is &mut T and you have &mut T).  See\n-the various `src/test/run-pass/coerce-reborrow-*.rs` tests for\n-examples of where this is useful.\n-\n-## Subtle note\n-\n-When deciding what type coercions to consider, we do not attempt to\n-resolve any type variables we may encounter.  This is because `b`\n-represents the expected type \"as the user wrote it\", meaning that if\n-the user defined a generic function like\n-\n-   fn foo<A>(a: A, b: A) { ... }\n-\n-and then we wrote `foo(&1, @2)`, we will not auto-borrow\n-either argument.  In older code we went to some lengths to\n-resolve the `b` variable, which could mean that we'd\n-auto-borrow later arguments but not earlier ones, which\n-seems very confusing.\n-\n-## Subtler note\n-\n-However, right now, if the user manually specifies the\n-values for the type variables, as so:\n-\n-   foo::<&int>(@1, @2)\n-\n-then we *will* auto-borrow, because we can't distinguish this from a\n-function that declared `&int`.  This is inconsistent but it's easiest\n-at the moment. The right thing to do, I think, is to consider the\n-*unsubstituted* type when deciding whether to auto-borrow, but the\n-*substituted* type when considering the bounds and so forth. But most\n-of our methods don't give access to the unsubstituted type, and\n-rightly so because they'd be error-prone.  So maybe the thing to do is\n-to actually determine the kind of coercions that should occur\n-separately and pass them in.  Or maybe it's ok as is.  Anyway, it's\n-sort of a minor point so I've opted to leave it for later---after all\n-we may want to adjust precisely when coercions occur.\n-\n-*/\n+//! # Type Coercion\n+//!\n+//! Under certain circumstances we will coerce from one type to another,\n+//! for example by auto-borrowing.  This occurs in situations where the\n+//! compiler has a firm 'expected type' that was supplied from the user,\n+//! and where the actual type is similar to that expected type in purpose\n+//! but not in representation (so actual subtyping is inappropriate).\n+//!\n+//! ## Reborrowing\n+//!\n+//! Note that if we are expecting a reference, we will *reborrow*\n+//! even if the argument provided was already a reference.  This is\n+//! useful for freezing mut/const things (that is, when the expected is &T\n+//! but you have &const T or &mut T) and also for avoiding the linearity\n+//! of mut things (when the expected is &mut T and you have &mut T).  See\n+//! the various `src/test/run-pass/coerce-reborrow-*.rs` tests for\n+//! examples of where this is useful.\n+//!\n+//! ## Subtle note\n+//!\n+//! When deciding what type coercions to consider, we do not attempt to\n+//! resolve any type variables we may encounter.  This is because `b`\n+//! represents the expected type \"as the user wrote it\", meaning that if\n+//! the user defined a generic function like\n+//!\n+//!    fn foo<A>(a: A, b: A) { ... }\n+//!\n+//! and then we wrote `foo(&1, @2)`, we will not auto-borrow\n+//! either argument.  In older code we went to some lengths to\n+//! resolve the `b` variable, which could mean that we'd\n+//! auto-borrow later arguments but not earlier ones, which\n+//! seems very confusing.\n+//!\n+//! ## Subtler note\n+//!\n+//! However, right now, if the user manually specifies the\n+//! values for the type variables, as so:\n+//!\n+//!    foo::<&int>(@1, @2)\n+//!\n+//! then we *will* auto-borrow, because we can't distinguish this from a\n+//! function that declared `&int`.  This is inconsistent but it's easiest\n+//! at the moment. The right thing to do, I think, is to consider the\n+//! *unsubstituted* type when deciding whether to auto-borrow, but the\n+//! *substituted* type when considering the bounds and so forth. But most\n+//! of our methods don't give access to the unsubstituted type, and\n+//! rightly so because they'd be error-prone.  So maybe the thing to do is\n+//! to actually determine the kind of coercions that should occur\n+//! separately and pass them in.  Or maybe it's ok as is.  Anyway, it's\n+//! sort of a minor point so I've opted to leave it for later---after all\n+//! we may want to adjust precisely when coercions occur.\n \n use middle::subst;\n use middle::ty::{AutoPtr, AutoDerefRef, AdjustDerefRef, AutoUnsize, AutoUnsafe};\n@@ -512,14 +508,10 @@ impl<'f, 'tcx> Coerce<'f, 'tcx> {\n         }\n     }\n \n+    ///  Attempts to coerce from a bare Rust function (`extern \"Rust\" fn`) into a closure or a\n+    ///  `proc`.\n     fn coerce_from_bare_fn(&self, a: Ty<'tcx>, fn_ty_a: &ty::BareFnTy<'tcx>, b: Ty<'tcx>)\n                            -> CoerceResult<'tcx> {\n-        /*!\n-         *\n-         * Attempts to coerce from a bare Rust function (`extern\n-         * \"Rust\" fn`) into a closure or a `proc`.\n-         */\n-\n         self.unpack_actual_value(b, |sty_b| {\n \n             debug!(\"coerce_from_bare_fn(a={}, b={})\","}, {"sha": "ba6ae00b6671f197cbb85ec9b97dcd927fc44f29", "filename": "src/librustc/middle/typeck/infer/combine.rs", "status": "modified", "additions": 4, "deletions": 9, "changes": 13, "blob_url": "https://github.com/rust-lang/rust/blob/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc%2Fmiddle%2Ftypeck%2Finfer%2Fcombine.rs", "raw_url": "https://github.com/rust-lang/rust/raw/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc%2Fmiddle%2Ftypeck%2Finfer%2Fcombine.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Fmiddle%2Ftypeck%2Finfer%2Fcombine.rs?ref=cd5c8235c5448a7234548c772468c8d2e8f150d9", "patch": "@@ -642,21 +642,16 @@ impl<'f, 'tcx> CombineFields<'f, 'tcx> {\n         Ok(())\n     }\n \n+    /// Attempts to generalize `ty` for the type variable `for_vid`.  This checks for cycle -- that\n+    /// is, whether the type `ty` references `for_vid`. If `make_region_vars` is true, it will also\n+    /// replace all regions with fresh variables. Returns `ty_err` in the case of a cycle, `Ok`\n+    /// otherwise.\n     fn generalize(&self,\n                   ty: Ty<'tcx>,\n                   for_vid: ty::TyVid,\n                   make_region_vars: bool)\n                   -> cres<'tcx, Ty<'tcx>>\n     {\n-        /*!\n-         * Attempts to generalize `ty` for the type variable\n-         * `for_vid`.  This checks for cycle -- that is, whether the\n-         * type `ty` references `for_vid`. If `make_region_vars` is\n-         * true, it will also replace all regions with fresh\n-         * variables. Returns `ty_err` in the case of a cycle, `Ok`\n-         * otherwise.\n-         */\n-\n         let mut generalize = Generalizer { infcx: self.infcx,\n                                            span: self.trace.origin.span(),\n                                            for_vid: for_vid,"}, {"sha": "0e3cc5f68c868c86b00e12cb906a0da5bfc5dc71", "filename": "src/librustc/middle/typeck/infer/doc.rs", "status": "modified", "additions": 237, "deletions": 241, "changes": 478, "blob_url": "https://github.com/rust-lang/rust/blob/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc%2Fmiddle%2Ftypeck%2Finfer%2Fdoc.rs", "raw_url": "https://github.com/rust-lang/rust/raw/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc%2Fmiddle%2Ftypeck%2Finfer%2Fdoc.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Fmiddle%2Ftypeck%2Finfer%2Fdoc.rs?ref=cd5c8235c5448a7234548c772468c8d2e8f150d9", "patch": "@@ -8,244 +8,240 @@\n // option. This file may not be copied, modified, or distributed\n // except according to those terms.\n \n-/*!\n-\n-# Type inference engine\n-\n-This is loosely based on standard HM-type inference, but with an\n-extension to try and accommodate subtyping.  There is nothing\n-principled about this extension; it's sound---I hope!---but it's a\n-heuristic, ultimately, and does not guarantee that it finds a valid\n-typing even if one exists (in fact, there are known scenarios where it\n-fails, some of which may eventually become problematic).\n-\n-## Key idea\n-\n-The main change is that each type variable T is associated with a\n-lower-bound L and an upper-bound U.  L and U begin as bottom and top,\n-respectively, but gradually narrow in response to new constraints\n-being introduced.  When a variable is finally resolved to a concrete\n-type, it can (theoretically) select any type that is a supertype of L\n-and a subtype of U.\n-\n-There are several critical invariants which we maintain:\n-\n-- the upper-bound of a variable only becomes lower and the lower-bound\n-  only becomes higher over time;\n-- the lower-bound L is always a subtype of the upper bound U;\n-- the lower-bound L and upper-bound U never refer to other type variables,\n-  but only to types (though those types may contain type variables).\n-\n-> An aside: if the terms upper- and lower-bound confuse you, think of\n-> \"supertype\" and \"subtype\".  The upper-bound is a \"supertype\"\n-> (super=upper in Latin, or something like that anyway) and the lower-bound\n-> is a \"subtype\" (sub=lower in Latin).  I find it helps to visualize\n-> a simple class hierarchy, like Java minus interfaces and\n-> primitive types.  The class Object is at the root (top) and other\n-> types lie in between.  The bottom type is then the Null type.\n-> So the tree looks like:\n->\n-> ```text\n->         Object\n->         /    \\\n->     String   Other\n->         \\    /\n->         (null)\n-> ```\n->\n-> So the upper bound type is the \"supertype\" and the lower bound is the\n-> \"subtype\" (also, super and sub mean upper and lower in Latin, or something\n-> like that anyway).\n-\n-## Satisfying constraints\n-\n-At a primitive level, there is only one form of constraint that the\n-inference understands: a subtype relation.  So the outside world can\n-say \"make type A a subtype of type B\".  If there are variables\n-involved, the inferencer will adjust their upper- and lower-bounds as\n-needed to ensure that this relation is satisfied. (We also allow \"make\n-type A equal to type B\", but this is translated into \"A <: B\" and \"B\n-<: A\")\n-\n-As stated above, we always maintain the invariant that type bounds\n-never refer to other variables.  This keeps the inference relatively\n-simple, avoiding the scenario of having a kind of graph where we have\n-to pump constraints along and reach a fixed point, but it does impose\n-some heuristics in the case where the user is relating two type\n-variables A <: B.\n-\n-Combining two variables such that variable A will forever be a subtype\n-of variable B is the trickiest part of the algorithm because there is\n-often no right choice---that is, the right choice will depend on\n-future constraints which we do not yet know. The problem comes about\n-because both A and B have bounds that can be adjusted in the future.\n-Let's look at some of the cases that can come up.\n-\n-Imagine, to start, the best case, where both A and B have an upper and\n-lower bound (that is, the bounds are not top nor bot respectively). In\n-that case, if we're lucky, A.ub <: B.lb, and so we know that whatever\n-A and B should become, they will forever have the desired subtyping\n-relation.  We can just leave things as they are.\n-\n-### Option 1: Unify\n-\n-However, suppose that A.ub is *not* a subtype of B.lb.  In\n-that case, we must make a decision.  One option is to unify A\n-and B so that they are one variable whose bounds are:\n-\n-    UB = GLB(A.ub, B.ub)\n-    LB = LUB(A.lb, B.lb)\n-\n-(Note that we will have to verify that LB <: UB; if it does not, the\n-types are not intersecting and there is an error) In that case, A <: B\n-holds trivially because A==B.  However, we have now lost some\n-flexibility, because perhaps the user intended for A and B to end up\n-as different types and not the same type.\n-\n-Pictorally, what this does is to take two distinct variables with\n-(hopefully not completely) distinct type ranges and produce one with\n-the intersection.\n-\n-```text\n-                  B.ub                  B.ub\n-                   /\\                    /\n-           A.ub   /  \\           A.ub   /\n-           /   \\ /    \\              \\ /\n-          /     X      \\              UB\n-         /     / \\      \\            / \\\n-        /     /   /      \\          /   /\n-        \\     \\  /       /          \\  /\n-         \\      X       /             LB\n-          \\    / \\     /             / \\\n-           \\  /   \\   /             /   \\\n-           A.lb    B.lb          A.lb    B.lb\n-```\n-\n-\n-### Option 2: Relate UB/LB\n-\n-Another option is to keep A and B as distinct variables but set their\n-bounds in such a way that, whatever happens, we know that A <: B will hold.\n-This can be achieved by ensuring that A.ub <: B.lb.  In practice there\n-are two ways to do that, depicted pictorally here:\n-\n-```text\n-    Before                Option #1            Option #2\n-\n-             B.ub                B.ub                B.ub\n-              /\\                 /  \\                /  \\\n-      A.ub   /  \\        A.ub   /(B')\\       A.ub   /(B')\\\n-      /   \\ /    \\           \\ /     /           \\ /     /\n-     /     X      \\         __UB____/             UB    /\n-    /     / \\      \\       /  |                   |    /\n-   /     /   /      \\     /   |                   |   /\n-   \\     \\  /       /    /(A')|                   |  /\n-    \\      X       /    /     LB            ______LB/\n-     \\    / \\     /    /     / \\           / (A')/ \\\n-      \\  /   \\   /     \\    /   \\          \\    /   \\\n-      A.lb    B.lb       A.lb    B.lb        A.lb    B.lb\n-```\n-\n-In these diagrams, UB and LB are defined as before.  As you can see,\n-the new ranges `A'` and `B'` are quite different from the range that\n-would be produced by unifying the variables.\n-\n-### What we do now\n-\n-Our current technique is to *try* (transactionally) to relate the\n-existing bounds of A and B, if there are any (i.e., if `UB(A) != top\n-&& LB(B) != bot`).  If that succeeds, we're done.  If it fails, then\n-we merge A and B into same variable.\n-\n-This is not clearly the correct course.  For example, if `UB(A) !=\n-top` but `LB(B) == bot`, we could conceivably set `LB(B)` to `UB(A)`\n-and leave the variables unmerged.  This is sometimes the better\n-course, it depends on the program.\n-\n-The main case which fails today that I would like to support is:\n-\n-```text\n-fn foo<T>(x: T, y: T) { ... }\n-\n-fn bar() {\n-    let x: @mut int = @mut 3;\n-    let y: @int = @3;\n-    foo(x, y);\n-}\n-```\n-\n-In principle, the inferencer ought to find that the parameter `T` to\n-`foo(x, y)` is `@const int`.  Today, however, it does not; this is\n-because the type variable `T` is merged with the type variable for\n-`X`, and thus inherits its UB/LB of `@mut int`.  This leaves no\n-flexibility for `T` to later adjust to accommodate `@int`.\n-\n-### What to do when not all bounds are present\n-\n-In the prior discussion we assumed that A.ub was not top and B.lb was\n-not bot.  Unfortunately this is rarely the case.  Often type variables\n-have \"lopsided\" bounds.  For example, if a variable in the program has\n-been initialized but has not been used, then its corresponding type\n-variable will have a lower bound but no upper bound.  When that\n-variable is then used, we would like to know its upper bound---but we\n-don't have one!  In this case we'll do different things depending on\n-how the variable is being used.\n-\n-## Transactional support\n-\n-Whenever we adjust merge variables or adjust their bounds, we always\n-keep a record of the old value.  This allows the changes to be undone.\n-\n-## Regions\n-\n-I've only talked about type variables here, but region variables\n-follow the same principle.  They have upper- and lower-bounds.  A\n-region A is a subregion of a region B if A being valid implies that B\n-is valid.  This basically corresponds to the block nesting structure:\n-the regions for outer block scopes are superregions of those for inner\n-block scopes.\n-\n-## Integral and floating-point type variables\n-\n-There is a third variety of type variable that we use only for\n-inferring the types of unsuffixed integer literals.  Integral type\n-variables differ from general-purpose type variables in that there's\n-no subtyping relationship among the various integral types, so instead\n-of associating each variable with an upper and lower bound, we just\n-use simple unification.  Each integer variable is associated with at\n-most one integer type.  Floating point types are handled similarly to\n-integral types.\n-\n-## GLB/LUB\n-\n-Computing the greatest-lower-bound and least-upper-bound of two\n-types/regions is generally straightforward except when type variables\n-are involved. In that case, we follow a similar \"try to use the bounds\n-when possible but otherwise merge the variables\" strategy.  In other\n-words, `GLB(A, B)` where `A` and `B` are variables will often result\n-in `A` and `B` being merged and the result being `A`.\n-\n-## Type coercion\n-\n-We have a notion of assignability which differs somewhat from\n-subtyping; in particular it may cause region borrowing to occur.  See\n-the big comment later in this file on Type Coercion for specifics.\n-\n-### In conclusion\n-\n-I showed you three ways to relate `A` and `B`.  There are also more,\n-of course, though I'm not sure if there are any more sensible options.\n-The main point is that there are various options, each of which\n-produce a distinct range of types for `A` and `B`.  Depending on what\n-the correct values for A and B are, one of these options will be the\n-right choice: but of course we don't know the right values for A and B\n-yet, that's what we're trying to find!  In our code, we opt to unify\n-(Option #1).\n-\n-# Implementation details\n-\n-We make use of a trait-like implementation strategy to consolidate\n-duplicated code between subtypes, GLB, and LUB computations.  See the\n-section on \"Type Combining\" below for details.\n-\n-*/\n+//! # Type inference engine\n+//!\n+//! This is loosely based on standard HM-type inference, but with an\n+//! extension to try and accommodate subtyping.  There is nothing\n+//! principled about this extension; it's sound---I hope!---but it's a\n+//! heuristic, ultimately, and does not guarantee that it finds a valid\n+//! typing even if one exists (in fact, there are known scenarios where it\n+//! fails, some of which may eventually become problematic).\n+//!\n+//! ## Key idea\n+//!\n+//! The main change is that each type variable T is associated with a\n+//! lower-bound L and an upper-bound U.  L and U begin as bottom and top,\n+//! respectively, but gradually narrow in response to new constraints\n+//! being introduced.  When a variable is finally resolved to a concrete\n+//! type, it can (theoretically) select any type that is a supertype of L\n+//! and a subtype of U.\n+//!\n+//! There are several critical invariants which we maintain:\n+//!\n+//! - the upper-bound of a variable only becomes lower and the lower-bound\n+//!   only becomes higher over time;\n+//! - the lower-bound L is always a subtype of the upper bound U;\n+//! - the lower-bound L and upper-bound U never refer to other type variables,\n+//!   but only to types (though those types may contain type variables).\n+//!\n+//! > An aside: if the terms upper- and lower-bound confuse you, think of\n+//! > \"supertype\" and \"subtype\".  The upper-bound is a \"supertype\"\n+//! > (super=upper in Latin, or something like that anyway) and the lower-bound\n+//! > is a \"subtype\" (sub=lower in Latin).  I find it helps to visualize\n+//! > a simple class hierarchy, like Java minus interfaces and\n+//! > primitive types.  The class Object is at the root (top) and other\n+//! > types lie in between.  The bottom type is then the Null type.\n+//! > So the tree looks like:\n+//! >\n+//! > ```text\n+//! >         Object\n+//! >         /    \\\n+//! >     String   Other\n+//! >         \\    /\n+//! >         (null)\n+//! > ```\n+//! >\n+//! > So the upper bound type is the \"supertype\" and the lower bound is the\n+//! > \"subtype\" (also, super and sub mean upper and lower in Latin, or something\n+//! > like that anyway).\n+//!\n+//! ## Satisfying constraints\n+//!\n+//! At a primitive level, there is only one form of constraint that the\n+//! inference understands: a subtype relation.  So the outside world can\n+//! say \"make type A a subtype of type B\".  If there are variables\n+//! involved, the inferencer will adjust their upper- and lower-bounds as\n+//! needed to ensure that this relation is satisfied. (We also allow \"make\n+//! type A equal to type B\", but this is translated into \"A <: B\" and \"B\n+//! <: A\")\n+//!\n+//! As stated above, we always maintain the invariant that type bounds\n+//! never refer to other variables.  This keeps the inference relatively\n+//! simple, avoiding the scenario of having a kind of graph where we have\n+//! to pump constraints along and reach a fixed point, but it does impose\n+//! some heuristics in the case where the user is relating two type\n+//! variables A <: B.\n+//!\n+//! Combining two variables such that variable A will forever be a subtype\n+//! of variable B is the trickiest part of the algorithm because there is\n+//! often no right choice---that is, the right choice will depend on\n+//! future constraints which we do not yet know. The problem comes about\n+//! because both A and B have bounds that can be adjusted in the future.\n+//! Let's look at some of the cases that can come up.\n+//!\n+//! Imagine, to start, the best case, where both A and B have an upper and\n+//! lower bound (that is, the bounds are not top nor bot respectively). In\n+//! that case, if we're lucky, A.ub <: B.lb, and so we know that whatever\n+//! A and B should become, they will forever have the desired subtyping\n+//! relation.  We can just leave things as they are.\n+//!\n+//! ### Option 1: Unify\n+//!\n+//! However, suppose that A.ub is *not* a subtype of B.lb.  In\n+//! that case, we must make a decision.  One option is to unify A\n+//! and B so that they are one variable whose bounds are:\n+//!\n+//!     UB = GLB(A.ub, B.ub)\n+//!     LB = LUB(A.lb, B.lb)\n+//!\n+//! (Note that we will have to verify that LB <: UB; if it does not, the\n+//! types are not intersecting and there is an error) In that case, A <: B\n+//! holds trivially because A==B.  However, we have now lost some\n+//! flexibility, because perhaps the user intended for A and B to end up\n+//! as different types and not the same type.\n+//!\n+//! Pictorally, what this does is to take two distinct variables with\n+//! (hopefully not completely) distinct type ranges and produce one with\n+//! the intersection.\n+//!\n+//! ```text\n+//!                   B.ub                  B.ub\n+//!                    /\\                    /\n+//!            A.ub   /  \\           A.ub   /\n+//!            /   \\ /    \\              \\ /\n+//!           /     X      \\              UB\n+//!          /     / \\      \\            / \\\n+//!         /     /   /      \\          /   /\n+//!         \\     \\  /       /          \\  /\n+//!          \\      X       /             LB\n+//!           \\    / \\     /             / \\\n+//!            \\  /   \\   /             /   \\\n+//!            A.lb    B.lb          A.lb    B.lb\n+//! ```\n+//!\n+//!\n+//! ### Option 2: Relate UB/LB\n+//!\n+//! Another option is to keep A and B as distinct variables but set their\n+//! bounds in such a way that, whatever happens, we know that A <: B will hold.\n+//! This can be achieved by ensuring that A.ub <: B.lb.  In practice there\n+//! are two ways to do that, depicted pictorally here:\n+//!\n+//! ```text\n+//!     Before                Option #1            Option #2\n+//!\n+//!              B.ub                B.ub                B.ub\n+//!               /\\                 /  \\                /  \\\n+//!       A.ub   /  \\        A.ub   /(B')\\       A.ub   /(B')\\\n+//!       /   \\ /    \\           \\ /     /           \\ /     /\n+//!      /     X      \\         __UB____/             UB    /\n+//!     /     / \\      \\       /  |                   |    /\n+//!    /     /   /      \\     /   |                   |   /\n+//!    \\     \\  /       /    /(A')|                   |  /\n+//!     \\      X       /    /     LB            ______LB/\n+//!      \\    / \\     /    /     / \\           / (A')/ \\\n+//!       \\  /   \\   /     \\    /   \\          \\    /   \\\n+//!       A.lb    B.lb       A.lb    B.lb        A.lb    B.lb\n+//! ```\n+//!\n+//! In these diagrams, UB and LB are defined as before.  As you can see,\n+//! the new ranges `A'` and `B'` are quite different from the range that\n+//! would be produced by unifying the variables.\n+//!\n+//! ### What we do now\n+//!\n+//! Our current technique is to *try* (transactionally) to relate the\n+//! existing bounds of A and B, if there are any (i.e., if `UB(A) != top\n+//! && LB(B) != bot`).  If that succeeds, we're done.  If it fails, then\n+//! we merge A and B into same variable.\n+//!\n+//! This is not clearly the correct course.  For example, if `UB(A) !=\n+//! top` but `LB(B) == bot`, we could conceivably set `LB(B)` to `UB(A)`\n+//! and leave the variables unmerged.  This is sometimes the better\n+//! course, it depends on the program.\n+//!\n+//! The main case which fails today that I would like to support is:\n+//!\n+//! ```text\n+//! fn foo<T>(x: T, y: T) { ... }\n+//!\n+//! fn bar() {\n+//!     let x: @mut int = @mut 3;\n+//!     let y: @int = @3;\n+//!     foo(x, y);\n+//! }\n+//! ```\n+//!\n+//! In principle, the inferencer ought to find that the parameter `T` to\n+//! `foo(x, y)` is `@const int`.  Today, however, it does not; this is\n+//! because the type variable `T` is merged with the type variable for\n+//! `X`, and thus inherits its UB/LB of `@mut int`.  This leaves no\n+//! flexibility for `T` to later adjust to accommodate `@int`.\n+//!\n+//! ### What to do when not all bounds are present\n+//!\n+//! In the prior discussion we assumed that A.ub was not top and B.lb was\n+//! not bot.  Unfortunately this is rarely the case.  Often type variables\n+//! have \"lopsided\" bounds.  For example, if a variable in the program has\n+//! been initialized but has not been used, then its corresponding type\n+//! variable will have a lower bound but no upper bound.  When that\n+//! variable is then used, we would like to know its upper bound---but we\n+//! don't have one!  In this case we'll do different things depending on\n+//! how the variable is being used.\n+//!\n+//! ## Transactional support\n+//!\n+//! Whenever we adjust merge variables or adjust their bounds, we always\n+//! keep a record of the old value.  This allows the changes to be undone.\n+//!\n+//! ## Regions\n+//!\n+//! I've only talked about type variables here, but region variables\n+//! follow the same principle.  They have upper- and lower-bounds.  A\n+//! region A is a subregion of a region B if A being valid implies that B\n+//! is valid.  This basically corresponds to the block nesting structure:\n+//! the regions for outer block scopes are superregions of those for inner\n+//! block scopes.\n+//!\n+//! ## Integral and floating-point type variables\n+//!\n+//! There is a third variety of type variable that we use only for\n+//! inferring the types of unsuffixed integer literals.  Integral type\n+//! variables differ from general-purpose type variables in that there's\n+//! no subtyping relationship among the various integral types, so instead\n+//! of associating each variable with an upper and lower bound, we just\n+//! use simple unification.  Each integer variable is associated with at\n+//! most one integer type.  Floating point types are handled similarly to\n+//! integral types.\n+//!\n+//! ## GLB/LUB\n+//!\n+//! Computing the greatest-lower-bound and least-upper-bound of two\n+//! types/regions is generally straightforward except when type variables\n+//! are involved. In that case, we follow a similar \"try to use the bounds\n+//! when possible but otherwise merge the variables\" strategy.  In other\n+//! words, `GLB(A, B)` where `A` and `B` are variables will often result\n+//! in `A` and `B` being merged and the result being `A`.\n+//!\n+//! ## Type coercion\n+//!\n+//! We have a notion of assignability which differs somewhat from\n+//! subtyping; in particular it may cause region borrowing to occur.  See\n+//! the big comment later in this file on Type Coercion for specifics.\n+//!\n+//! ### In conclusion\n+//!\n+//! I showed you three ways to relate `A` and `B`.  There are also more,\n+//! of course, though I'm not sure if there are any more sensible options.\n+//! The main point is that there are various options, each of which\n+//! produce a distinct range of types for `A` and `B`.  Depending on what\n+//! the correct values for A and B are, one of these options will be the\n+//! right choice: but of course we don't know the right values for A and B\n+//! yet, that's what we're trying to find!  In our code, we opt to unify\n+//! (Option #1).\n+//!\n+//! # Implementation details\n+//!\n+//! We make use of a trait-like implementation strategy to consolidate\n+//! duplicated code between subtypes, GLB, and LUB computations.  See the\n+//! section on \"Type Combining\" below for details."}, {"sha": "abc68852f4bdf5515982c0113c6a06b0e1bab82c", "filename": "src/librustc/middle/typeck/infer/error_reporting.rs", "status": "modified", "additions": 49, "deletions": 54, "changes": 103, "blob_url": "https://github.com/rust-lang/rust/blob/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc%2Fmiddle%2Ftypeck%2Finfer%2Ferror_reporting.rs", "raw_url": "https://github.com/rust-lang/rust/raw/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc%2Fmiddle%2Ftypeck%2Finfer%2Ferror_reporting.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Fmiddle%2Ftypeck%2Finfer%2Ferror_reporting.rs?ref=cd5c8235c5448a7234548c772468c8d2e8f150d9", "patch": "@@ -8,56 +8,53 @@\n // option. This file may not be copied, modified, or distributed\n // except according to those terms.\n \n-/*!\n-\n-Error Reporting Code for the inference engine\n-\n-Because of the way inference, and in particular region inference,\n-works, it often happens that errors are not detected until far after\n-the relevant line of code has been type-checked. Therefore, there is\n-an elaborate system to track why a particular constraint in the\n-inference graph arose so that we can explain to the user what gave\n-rise to a particular error.\n-\n-The basis of the system are the \"origin\" types. An \"origin\" is the\n-reason that a constraint or inference variable arose. There are\n-different \"origin\" enums for different kinds of constraints/variables\n-(e.g., `TypeOrigin`, `RegionVariableOrigin`). An origin always has\n-a span, but also more information so that we can generate a meaningful\n-error message.\n-\n-Having a catalogue of all the different reasons an error can arise is\n-also useful for other reasons, like cross-referencing FAQs etc, though\n-we are not really taking advantage of this yet.\n-\n-# Region Inference\n-\n-Region inference is particularly tricky because it always succeeds \"in\n-the moment\" and simply registers a constraint. Then, at the end, we\n-can compute the full graph and report errors, so we need to be able to\n-store and later report what gave rise to the conflicting constraints.\n-\n-# Subtype Trace\n-\n-Determing whether `T1 <: T2` often involves a number of subtypes and\n-subconstraints along the way. A \"TypeTrace\" is an extended version\n-of an origin that traces the types and other values that were being\n-compared. It is not necessarily comprehensive (in fact, at the time of\n-this writing it only tracks the root values being compared) but I'd\n-like to extend it to include significant \"waypoints\". For example, if\n-you are comparing `(T1, T2) <: (T3, T4)`, and the problem is that `T2\n-<: T4` fails, I'd like the trace to include enough information to say\n-\"in the 2nd element of the tuple\". Similarly, failures when comparing\n-arguments or return types in fn types should be able to cite the\n-specific position, etc.\n-\n-# Reality vs plan\n-\n-Of course, there is still a LOT of code in typeck that has yet to be\n-ported to this system, and which relies on string concatenation at the\n-time of error detection.\n-\n-*/\n+//! Error Reporting Code for the inference engine\n+//!\n+//! Because of the way inference, and in particular region inference,\n+//! works, it often happens that errors are not detected until far after\n+//! the relevant line of code has been type-checked. Therefore, there is\n+//! an elaborate system to track why a particular constraint in the\n+//! inference graph arose so that we can explain to the user what gave\n+//! rise to a particular error.\n+//!\n+//! The basis of the system are the \"origin\" types. An \"origin\" is the\n+//! reason that a constraint or inference variable arose. There are\n+//! different \"origin\" enums for different kinds of constraints/variables\n+//! (e.g., `TypeOrigin`, `RegionVariableOrigin`). An origin always has\n+//! a span, but also more information so that we can generate a meaningful\n+//! error message.\n+//!\n+//! Having a catalogue of all the different reasons an error can arise is\n+//! also useful for other reasons, like cross-referencing FAQs etc, though\n+//! we are not really taking advantage of this yet.\n+//!\n+//! # Region Inference\n+//!\n+//! Region inference is particularly tricky because it always succeeds \"in\n+//! the moment\" and simply registers a constraint. Then, at the end, we\n+//! can compute the full graph and report errors, so we need to be able to\n+//! store and later report what gave rise to the conflicting constraints.\n+//!\n+//! # Subtype Trace\n+//!\n+//! Determing whether `T1 <: T2` often involves a number of subtypes and\n+//! subconstraints along the way. A \"TypeTrace\" is an extended version\n+//! of an origin that traces the types and other values that were being\n+//! compared. It is not necessarily comprehensive (in fact, at the time of\n+//! this writing it only tracks the root values being compared) but I'd\n+//! like to extend it to include significant \"waypoints\". For example, if\n+//! you are comparing `(T1, T2) <: (T3, T4)`, and the problem is that `T2\n+//! <: T4` fails, I'd like the trace to include enough information to say\n+//! \"in the 2nd element of the tuple\". Similarly, failures when comparing\n+//! arguments or return types in fn types should be able to cite the\n+//! specific position, etc.\n+//!\n+//! # Reality vs plan\n+//!\n+//! Of course, there is still a LOT of code in typeck that has yet to be\n+//! ported to this system, and which relies on string concatenation at the\n+//! time of error detection.\n+\n use self::FreshOrKept::*;\n \n use std::collections::HashSet;\n@@ -391,11 +388,9 @@ impl<'a, 'tcx> ErrorReporting<'tcx> for InferCtxt<'a, 'tcx> {\n         ty::note_and_explain_type_err(self.tcx, terr);\n     }\n \n+    /// Returns a string of the form \"expected `{}`, found `{}`\", or None if this is a derived\n+    /// error.\n     fn values_str(&self, values: &ValuePairs<'tcx>) -> Option<String> {\n-        /*!\n-         * Returns a string of the form \"expected `{}`, found `{}`\",\n-         * or None if this is a derived error.\n-         */\n         match *values {\n             infer::Types(ref exp_found) => self.expected_found_str(exp_found),\n             infer::TraitRefs(ref exp_found) => self.expected_found_str(exp_found)"}, {"sha": "2bad3616a05d19fd92f2bc67820f5cdc7a8dd7fd", "filename": "src/librustc/middle/typeck/infer/higher_ranked/doc.rs", "status": "modified", "additions": 401, "deletions": 405, "changes": 806, "blob_url": "https://github.com/rust-lang/rust/blob/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc%2Fmiddle%2Ftypeck%2Finfer%2Fhigher_ranked%2Fdoc.rs", "raw_url": "https://github.com/rust-lang/rust/raw/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc%2Fmiddle%2Ftypeck%2Finfer%2Fhigher_ranked%2Fdoc.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Fmiddle%2Ftypeck%2Finfer%2Fhigher_ranked%2Fdoc.rs?ref=cd5c8235c5448a7234548c772468c8d2e8f150d9", "patch": "@@ -8,408 +8,404 @@\n // option. This file may not be copied, modified, or distributed\n // except according to those terms.\n \n-/*!\n-\n-# Skolemization and functions\n-\n-One of the trickiest and most subtle aspects of regions is dealing\n-with higher-ranked things which include bound region variables, such\n-as function types. I strongly suggest that if you want to understand\n-the situation, you read this paper (which is, admittedly, very long,\n-but you don't have to read the whole thing):\n-\n-http://research.microsoft.com/en-us/um/people/simonpj/papers/higher-rank/\n-\n-Although my explanation will never compete with SPJ's (for one thing,\n-his is approximately 100 pages), I will attempt to explain the basic\n-problem and also how we solve it. Note that the paper only discusses\n-subtyping, not the computation of LUB/GLB.\n-\n-The problem we are addressing is that there is a kind of subtyping\n-between functions with bound region parameters. Consider, for\n-example, whether the following relation holds:\n-\n-    for<'a> fn(&'a int) <: for<'b> fn(&'b int)? (Yes, a => b)\n-\n-The answer is that of course it does. These two types are basically\n-the same, except that in one we used the name `a` and one we used\n-the name `b`.\n-\n-In the examples that follow, it becomes very important to know whether\n-a lifetime is bound in a function type (that is, is a lifetime\n-parameter) or appears free (is defined in some outer scope).\n-Therefore, from now on I will always write the bindings explicitly,\n-using the Rust syntax `for<'a> fn(&'a int)` to indicate that `a` is a\n-lifetime parameter.\n-\n-Now let's consider two more function types. Here, we assume that the\n-`'b` lifetime is defined somewhere outside and hence is not a lifetime\n-parameter bound by the function type (it \"appears free\"):\n-\n-    for<'a> fn(&'a int) <: fn(&'b int)? (Yes, a => b)\n-\n-This subtyping relation does in fact hold. To see why, you have to\n-consider what subtyping means. One way to look at `T1 <: T2` is to\n-say that it means that it is always ok to treat an instance of `T1` as\n-if it had the type `T2`. So, with our functions, it is always ok to\n-treat a function that can take pointers with any lifetime as if it\n-were a function that can only take a pointer with the specific\n-lifetime `'b`. After all, `'b` is a lifetime, after all, and\n-the function can take values of any lifetime.\n-\n-You can also look at subtyping as the *is a* relationship. This amounts\n-to the same thing: a function that accepts pointers with any lifetime\n-*is a* function that accepts pointers with some specific lifetime.\n-\n-So, what if we reverse the order of the two function types, like this:\n-\n-    fn(&'b int) <: for<'a> fn(&'a int)? (No)\n-\n-Does the subtyping relationship still hold?  The answer of course is\n-no. In this case, the function accepts *only the lifetime `'b`*,\n-so it is not reasonable to treat it as if it were a function that\n-accepted any lifetime.\n-\n-What about these two examples:\n-\n-    for<'a,'b> fn(&'a int, &'b int) <: for<'a>    fn(&'a int, &'a int)? (Yes)\n-    for<'a>    fn(&'a int, &'a int) <: for<'a,'b> fn(&'a int, &'b int)? (No)\n-\n-Here, it is true that functions which take two pointers with any two\n-lifetimes can be treated as if they only accepted two pointers with\n-the same lifetime, but not the reverse.\n-\n-## The algorithm\n-\n-Here is the algorithm we use to perform the subtyping check:\n-\n-1. Replace all bound regions in the subtype with new variables\n-2. Replace all bound regions in the supertype with skolemized\n-   equivalents. A \"skolemized\" region is just a new fresh region\n-   name.\n-3. Check that the parameter and return types match as normal\n-4. Ensure that no skolemized regions 'leak' into region variables\n-   visible from \"the outside\"\n-\n-Let's walk through some examples and see how this algorithm plays out.\n-\n-#### First example\n-\n-We'll start with the first example, which was:\n-\n-    1. for<'a> fn(&'a T) <: for<'b> fn(&'b T)?        Yes: a -> b\n-\n-After steps 1 and 2 of the algorithm we will have replaced the types\n-like so:\n-\n-    1. fn(&'A T) <: fn(&'x T)?\n-\n-Here the upper case `&A` indicates a *region variable*, that is, a\n-region whose value is being inferred by the system. I also replaced\n-`&b` with `&x`---I'll use letters late in the alphabet (`x`, `y`, `z`)\n-to indicate skolemized region names. We can assume they don't appear\n-elsewhere. Note that neither the sub- nor the supertype bind any\n-region names anymore (as indicated by the absence of `<` and `>`).\n-\n-The next step is to check that the parameter types match. Because\n-parameters are contravariant, this means that we check whether:\n-\n-    &'x T <: &'A T\n-\n-Region pointers are contravariant so this implies that\n-\n-    &A <= &x\n-\n-must hold, where `<=` is the subregion relationship. Processing\n-*this* constrain simply adds a constraint into our graph that `&A <=\n-&x` and is considered successful (it can, for example, be satisfied by\n-choosing the value `&x` for `&A`).\n-\n-So far we have encountered no error, so the subtype check succeeds.\n-\n-#### The third example\n-\n-Now let's look first at the third example, which was:\n-\n-    3. fn(&'a T)    <: for<'b> fn(&'b T)?        No!\n-\n-After steps 1 and 2 of the algorithm we will have replaced the types\n-like so:\n-\n-    3. fn(&'a T) <: fn(&'x T)?\n-\n-This looks pretty much the same as before, except that on the LHS\n-`'a` was not bound, and hence was left as-is and not replaced with\n-a variable. The next step is again to check that the parameter types\n-match. This will ultimately require (as before) that `'a` <= `&x`\n-must hold: but this does not hold. `self` and `x` are both distinct\n-free regions. So the subtype check fails.\n-\n-#### Checking for skolemization leaks\n-\n-You may be wondering about that mysterious last step in the algorithm.\n-So far it has not been relevant. The purpose of that last step is to\n-catch something like *this*:\n-\n-    for<'a> fn() -> fn(&'a T) <: fn() -> for<'b> fn(&'b T)?   No.\n-\n-Here the function types are the same but for where the binding occurs.\n-The subtype returns a function that expects a value in precisely one\n-region. The supertype returns a function that expects a value in any\n-region. If we allow an instance of the subtype to be used where the\n-supertype is expected, then, someone could call the fn and think that\n-the return value has type `fn<b>(&'b T)` when it really has type\n-`fn(&'a T)` (this is case #3, above). Bad.\n-\n-So let's step through what happens when we perform this subtype check.\n-We first replace the bound regions in the subtype (the supertype has\n-no bound regions). This gives us:\n-\n-    fn() -> fn(&'A T) <: fn() -> for<'b> fn(&'b T)?\n-\n-Now we compare the return types, which are covariant, and hence we have:\n-\n-    fn(&'A T) <: for<'b> fn(&'b T)?\n-\n-Here we skolemize the bound region in the supertype to yield:\n-\n-    fn(&'A T) <: fn(&'x T)?\n-\n-And then proceed to compare the argument types:\n-\n-    &'x T <: &'A T\n-    'A <= 'x\n-\n-Finally, this is where it gets interesting!  This is where an error\n-*should* be reported. But in fact this will not happen. The reason why\n-is that `A` is a variable: we will infer that its value is the fresh\n-region `x` and think that everything is happy. In fact, this behavior\n-is *necessary*, it was key to the first example we walked through.\n-\n-The difference between this example and the first one is that the variable\n-`A` already existed at the point where the skolemization occurred. In\n-the first example, you had two functions:\n-\n-    for<'a> fn(&'a T) <: for<'b> fn(&'b T)\n-\n-and hence `&A` and `&x` were created \"together\". In general, the\n-intention of the skolemized names is that they are supposed to be\n-fresh names that could never be equal to anything from the outside.\n-But when inference comes into play, we might not be respecting this\n-rule.\n-\n-So the way we solve this is to add a fourth step that examines the\n-constraints that refer to skolemized names. Basically, consider a\n-non-directed verison of the constraint graph. Let `Tainted(x)` be the\n-set of all things reachable from a skolemized variable `x`.\n-`Tainted(x)` should not contain any regions that existed before the\n-step at which the skolemization was performed. So this case here\n-would fail because `&x` was created alone, but is relatable to `&A`.\n-\n-## Computing the LUB and GLB\n-\n-The paper I pointed you at is written for Haskell. It does not\n-therefore considering subtyping and in particular does not consider\n-LUB or GLB computation. We have to consider this. Here is the\n-algorithm I implemented.\n-\n-First though, let's discuss what we are trying to compute in more\n-detail. The LUB is basically the \"common supertype\" and the GLB is\n-\"common subtype\"; one catch is that the LUB should be the\n-*most-specific* common supertype and the GLB should be *most general*\n-common subtype (as opposed to any common supertype or any common\n-subtype).\n-\n-Anyway, to help clarify, here is a table containing some function\n-pairs and their LUB/GLB (for conciseness, in this table, I'm just\n-including the lifetimes here, not the rest of the types, and I'm\n-writing `fn<>` instead of `for<> fn`):\n-\n-```\n-Type 1                Type 2                LUB                    GLB\n-fn<'a>('a)            fn('X)                fn('X)                 fn<'a>('a)\n-fn('a)                fn('X)                --                     fn<'a>('a)\n-fn<'a,'b>('a, 'b)     fn<'x>('x, 'x)        fn<'a>('a, 'a)         fn<'a,'b>('a, 'b)\n-fn<'a,'b>('a, 'b, 'a) fn<'x,'y>('x, 'y, 'y) fn<'a>('a, 'a, 'a)     fn<'a,'b,'c>('a,'b,'c)\n-```\n-\n-### Conventions\n-\n-I use lower-case letters (e.g., `&a`) for bound regions and upper-case\n-letters for free regions (`&A`).  Region variables written with a\n-dollar-sign (e.g., `$a`).  I will try to remember to enumerate the\n-bound-regions on the fn type as well (e.g., `for<'a> fn(&a)`).\n-\n-### High-level summary\n-\n-Both the LUB and the GLB algorithms work in a similar fashion.  They\n-begin by replacing all bound regions (on both sides) with fresh region\n-inference variables.  Therefore, both functions are converted to types\n-that contain only free regions.  We can then compute the LUB/GLB in a\n-straightforward way, as described in `combine.rs`.  This results in an\n-interim type T.  The algorithms then examine the regions that appear\n-in T and try to, in some cases, replace them with bound regions to\n-yield the final result.\n-\n-To decide whether to replace a region `R` that appears in `T` with a\n-bound region, the algorithms make use of two bits of information.\n-First is a set `V` that contains all region variables created as part\n-of the LUB/GLB computation. `V` will contain the region variables\n-created to replace the bound regions in the input types, but it also\n-contains 'intermediate' variables created to represent the LUB/GLB of\n-individual regions.  Basically, when asked to compute the LUB/GLB of a\n-region variable with another region, the inferencer cannot oblige\n-immediately since the values of that variables are not known.\n-Therefore, it creates a new variable that is related to the two\n-regions.  For example, the LUB of two variables `$x` and `$y` is a\n-fresh variable `$z` that is constrained such that `$x <= $z` and `$y\n-<= $z`.  So `V` will contain these intermediate variables as well.\n-\n-The other important factor in deciding how to replace a region in T is\n-the function `Tainted($r)` which, for a region variable, identifies\n-all regions that the region variable is related to in some way\n-(`Tainted()` made an appearance in the subtype computation as well).\n-\n-### LUB\n-\n-The LUB algorithm proceeds in three steps:\n-\n-1. Replace all bound regions (on both sides) with fresh region\n-   inference variables.\n-2. Compute the LUB \"as normal\", meaning compute the GLB of each\n-   pair of argument types and the LUB of the return types and\n-   so forth.  Combine those to a new function type `F`.\n-3. Replace each region `R` that appears in `F` as follows:\n-   - Let `V` be the set of variables created during the LUB\n-     computational steps 1 and 2, as described in the previous section.\n-   - If `R` is not in `V`, replace `R` with itself.\n-   - If `Tainted(R)` contains a region that is not in `V`,\n-     replace `R` with itself.\n-   - Otherwise, select the earliest variable in `Tainted(R)` that originates\n-     from the left-hand side and replace `R` with the bound region that\n-     this variable was a replacement for.\n-\n-So, let's work through the simplest example: `fn(&A)` and `for<'a> fn(&a)`.\n-In this case, `&a` will be replaced with `$a` and the interim LUB type\n-`fn($b)` will be computed, where `$b=GLB(&A,$a)`.  Therefore, `V =\n-{$a, $b}` and `Tainted($b) = { $b, $a, &A }`.  When we go to replace\n-`$b`, we find that since `&A \\in Tainted($b)` is not a member of `V`,\n-we leave `$b` as is.  When region inference happens, `$b` will be\n-resolved to `&A`, as we wanted.\n-\n-Let's look at a more complex one: `fn(&a, &b)` and `fn(&x, &x)`.  In\n-this case, we'll end up with a (pre-replacement) LUB type of `fn(&g,\n-&h)` and a graph that looks like:\n-\n-```\n-     $a        $b     *--$x\n-       \\        \\    /  /\n-        \\        $h-*  /\n-         $g-----------*\n-```\n-\n-Here `$g` and `$h` are fresh variables that are created to represent\n-the LUB/GLB of things requiring inference.  This means that `V` and\n-`Tainted` will look like:\n-\n-```\n-V = {$a, $b, $g, $h, $x}\n-Tainted($g) = Tainted($h) = { $a, $b, $h, $g, $x }\n-```\n-\n-Therefore we replace both `$g` and `$h` with `$a`, and end up\n-with the type `fn(&a, &a)`.\n-\n-### GLB\n-\n-The procedure for computing the GLB is similar.  The difference lies\n-in computing the replacements for the various variables. For each\n-region `R` that appears in the type `F`, we again compute `Tainted(R)`\n-and examine the results:\n-\n-1. If `R` is not in `V`, it is not replaced.\n-2. Else, if `Tainted(R)` contains only variables in `V`, and it\n-   contains exactly one variable from the LHS and one variable from\n-   the RHS, then `R` can be mapped to the bound version of the\n-   variable from the LHS.\n-3. Else, if `Tainted(R)` contains no variable from the LHS and no\n-   variable from the RHS, then `R` can be mapped to itself.\n-4. Else, `R` is mapped to a fresh bound variable.\n-\n-These rules are pretty complex.  Let's look at some examples to see\n-how they play out.\n-\n-Out first example was `fn(&a)` and `fn(&X)`.  In this case, `&a` will\n-be replaced with `$a` and we will ultimately compute a\n-(pre-replacement) GLB type of `fn($g)` where `$g=LUB($a,&X)`.\n-Therefore, `V={$a,$g}` and `Tainted($g)={$g,$a,&X}.  To find the\n-replacement for `$g` we consult the rules above:\n-- Rule (1) does not apply because `$g \\in V`\n-- Rule (2) does not apply because `&X \\in Tainted($g)`\n-- Rule (3) does not apply because `$a \\in Tainted($g)`\n-- Hence, by rule (4), we replace `$g` with a fresh bound variable `&z`.\n-So our final result is `fn(&z)`, which is correct.\n-\n-The next example is `fn(&A)` and `fn(&Z)`. In this case, we will again\n-have a (pre-replacement) GLB of `fn(&g)`, where `$g = LUB(&A,&Z)`.\n-Therefore, `V={$g}` and `Tainted($g) = {$g, &A, &Z}`.  In this case,\n-by rule (3), `$g` is mapped to itself, and hence the result is\n-`fn($g)`.  This result is correct (in this case, at least), but it is\n-indicative of a case that *can* lead us into concluding that there is\n-no GLB when in fact a GLB does exist.  See the section \"Questionable\n-Results\" below for more details.\n-\n-The next example is `fn(&a, &b)` and `fn(&c, &c)`. In this case, as\n-before, we'll end up with `F=fn($g, $h)` where `Tainted($g) =\n-Tainted($h) = {$g, $h, $a, $b, $c}`.  Only rule (4) applies and hence\n-we'll select fresh bound variables `y` and `z` and wind up with\n-`fn(&y, &z)`.\n-\n-For the last example, let's consider what may seem trivial, but is\n-not: `fn(&a, &a)` and `fn(&b, &b)`.  In this case, we'll get `F=fn($g,\n-$h)` where `Tainted($g) = {$g, $a, $x}` and `Tainted($h) = {$h, $a,\n-$x}`.  Both of these sets contain exactly one bound variable from each\n-side, so we'll map them both to `&a`, resulting in `fn(&a, &a)`, which\n-is the desired result.\n-\n-### Shortcomings and correctness\n-\n-You may be wondering whether this algorithm is correct.  The answer is\n-\"sort of\".  There are definitely cases where they fail to compute a\n-result even though a correct result exists.  I believe, though, that\n-if they succeed, then the result is valid, and I will attempt to\n-convince you.  The basic argument is that the \"pre-replacement\" step\n-computes a set of constraints.  The replacements, then, attempt to\n-satisfy those constraints, using bound identifiers where needed.\n-\n-For now I will briefly go over the cases for LUB/GLB and identify\n-their intent:\n-\n-- LUB:\n-  - The region variables that are substituted in place of bound regions\n-    are intended to collect constraints on those bound regions.\n-  - If Tainted(R) contains only values in V, then this region is unconstrained\n-    and can therefore be generalized, otherwise it cannot.\n-- GLB:\n-  - The region variables that are substituted in place of bound regions\n-    are intended to collect constraints on those bound regions.\n-  - If Tainted(R) contains exactly one variable from each side, and\n-    only variables in V, that indicates that those two bound regions\n-    must be equated.\n-  - Otherwise, if Tainted(R) references any variables from left or right\n-    side, then it is trying to combine a bound region with a free one or\n-    multiple bound regions, so we need to select fresh bound regions.\n-\n-Sorry this is more of a shorthand to myself.  I will try to write up something\n-more convincing in the future.\n-\n-#### Where are the algorithms wrong?\n-\n-- The pre-replacement computation can fail even though using a\n-  bound-region would have succeeded.\n-- We will compute GLB(fn(fn($a)), fn(fn($b))) as fn($c) where $c is the\n-  GLB of $a and $b.  But if inference finds that $a and $b must be mapped\n-  to regions without a GLB, then this is effectively a failure to compute\n-  the GLB.  However, the result `fn<$c>(fn($c))` is a valid GLB.\n-\n- */\n+//! # Skolemization and functions\n+//!\n+//! One of the trickiest and most subtle aspects of regions is dealing\n+//! with higher-ranked things which include bound region variables, such\n+//! as function types. I strongly suggest that if you want to understand\n+//! the situation, you read this paper (which is, admittedly, very long,\n+//! but you don't have to read the whole thing):\n+//!\n+//! http://research.microsoft.com/en-us/um/people/simonpj/papers/higher-rank/\n+//!\n+//! Although my explanation will never compete with SPJ's (for one thing,\n+//! his is approximately 100 pages), I will attempt to explain the basic\n+//! problem and also how we solve it. Note that the paper only discusses\n+//! subtyping, not the computation of LUB/GLB.\n+//!\n+//! The problem we are addressing is that there is a kind of subtyping\n+//! between functions with bound region parameters. Consider, for\n+//! example, whether the following relation holds:\n+//!\n+//!     for<'a> fn(&'a int) <: for<'b> fn(&'b int)? (Yes, a => b)\n+//!\n+//! The answer is that of course it does. These two types are basically\n+//! the same, except that in one we used the name `a` and one we used\n+//! the name `b`.\n+//!\n+//! In the examples that follow, it becomes very important to know whether\n+//! a lifetime is bound in a function type (that is, is a lifetime\n+//! parameter) or appears free (is defined in some outer scope).\n+//! Therefore, from now on I will always write the bindings explicitly,\n+//! using the Rust syntax `for<'a> fn(&'a int)` to indicate that `a` is a\n+//! lifetime parameter.\n+//!\n+//! Now let's consider two more function types. Here, we assume that the\n+//! `'b` lifetime is defined somewhere outside and hence is not a lifetime\n+//! parameter bound by the function type (it \"appears free\"):\n+//!\n+//!     for<'a> fn(&'a int) <: fn(&'b int)? (Yes, a => b)\n+//!\n+//! This subtyping relation does in fact hold. To see why, you have to\n+//! consider what subtyping means. One way to look at `T1 <: T2` is to\n+//! say that it means that it is always ok to treat an instance of `T1` as\n+//! if it had the type `T2`. So, with our functions, it is always ok to\n+//! treat a function that can take pointers with any lifetime as if it\n+//! were a function that can only take a pointer with the specific\n+//! lifetime `'b`. After all, `'b` is a lifetime, after all, and\n+//! the function can take values of any lifetime.\n+//!\n+//! You can also look at subtyping as the *is a* relationship. This amounts\n+//! to the same thing: a function that accepts pointers with any lifetime\n+//! *is a* function that accepts pointers with some specific lifetime.\n+//!\n+//! So, what if we reverse the order of the two function types, like this:\n+//!\n+//!     fn(&'b int) <: for<'a> fn(&'a int)? (No)\n+//!\n+//! Does the subtyping relationship still hold?  The answer of course is\n+//! no. In this case, the function accepts *only the lifetime `'b`*,\n+//! so it is not reasonable to treat it as if it were a function that\n+//! accepted any lifetime.\n+//!\n+//! What about these two examples:\n+//!\n+//!     for<'a,'b> fn(&'a int, &'b int) <: for<'a>    fn(&'a int, &'a int)? (Yes)\n+//!     for<'a>    fn(&'a int, &'a int) <: for<'a,'b> fn(&'a int, &'b int)? (No)\n+//!\n+//! Here, it is true that functions which take two pointers with any two\n+//! lifetimes can be treated as if they only accepted two pointers with\n+//! the same lifetime, but not the reverse.\n+//!\n+//! ## The algorithm\n+//!\n+//! Here is the algorithm we use to perform the subtyping check:\n+//!\n+//! 1. Replace all bound regions in the subtype with new variables\n+//! 2. Replace all bound regions in the supertype with skolemized\n+//!    equivalents. A \"skolemized\" region is just a new fresh region\n+//!    name.\n+//! 3. Check that the parameter and return types match as normal\n+//! 4. Ensure that no skolemized regions 'leak' into region variables\n+//!    visible from \"the outside\"\n+//!\n+//! Let's walk through some examples and see how this algorithm plays out.\n+//!\n+//! #### First example\n+//!\n+//! We'll start with the first example, which was:\n+//!\n+//!     1. for<'a> fn(&'a T) <: for<'b> fn(&'b T)?        Yes: a -> b\n+//!\n+//! After steps 1 and 2 of the algorithm we will have replaced the types\n+//! like so:\n+//!\n+//!     1. fn(&'A T) <: fn(&'x T)?\n+//!\n+//! Here the upper case `&A` indicates a *region variable*, that is, a\n+//! region whose value is being inferred by the system. I also replaced\n+//! `&b` with `&x`---I'll use letters late in the alphabet (`x`, `y`, `z`)\n+//! to indicate skolemized region names. We can assume they don't appear\n+//! elsewhere. Note that neither the sub- nor the supertype bind any\n+//! region names anymore (as indicated by the absence of `<` and `>`).\n+//!\n+//! The next step is to check that the parameter types match. Because\n+//! parameters are contravariant, this means that we check whether:\n+//!\n+//!     &'x T <: &'A T\n+//!\n+//! Region pointers are contravariant so this implies that\n+//!\n+//!     &A <= &x\n+//!\n+//! must hold, where `<=` is the subregion relationship. Processing\n+//! *this* constrain simply adds a constraint into our graph that `&A <=\n+//! &x` and is considered successful (it can, for example, be satisfied by\n+//! choosing the value `&x` for `&A`).\n+//!\n+//! So far we have encountered no error, so the subtype check succeeds.\n+//!\n+//! #### The third example\n+//!\n+//! Now let's look first at the third example, which was:\n+//!\n+//!     3. fn(&'a T)    <: for<'b> fn(&'b T)?        No!\n+//!\n+//! After steps 1 and 2 of the algorithm we will have replaced the types\n+//! like so:\n+//!\n+//!     3. fn(&'a T) <: fn(&'x T)?\n+//!\n+//! This looks pretty much the same as before, except that on the LHS\n+//! `'a` was not bound, and hence was left as-is and not replaced with\n+//! a variable. The next step is again to check that the parameter types\n+//! match. This will ultimately require (as before) that `'a` <= `&x`\n+//! must hold: but this does not hold. `self` and `x` are both distinct\n+//! free regions. So the subtype check fails.\n+//!\n+//! #### Checking for skolemization leaks\n+//!\n+//! You may be wondering about that mysterious last step in the algorithm.\n+//! So far it has not been relevant. The purpose of that last step is to\n+//! catch something like *this*:\n+//!\n+//!     for<'a> fn() -> fn(&'a T) <: fn() -> for<'b> fn(&'b T)?   No.\n+//!\n+//! Here the function types are the same but for where the binding occurs.\n+//! The subtype returns a function that expects a value in precisely one\n+//! region. The supertype returns a function that expects a value in any\n+//! region. If we allow an instance of the subtype to be used where the\n+//! supertype is expected, then, someone could call the fn and think that\n+//! the return value has type `fn<b>(&'b T)` when it really has type\n+//! `fn(&'a T)` (this is case #3, above). Bad.\n+//!\n+//! So let's step through what happens when we perform this subtype check.\n+//! We first replace the bound regions in the subtype (the supertype has\n+//! no bound regions). This gives us:\n+//!\n+//!     fn() -> fn(&'A T) <: fn() -> for<'b> fn(&'b T)?\n+//!\n+//! Now we compare the return types, which are covariant, and hence we have:\n+//!\n+//!     fn(&'A T) <: for<'b> fn(&'b T)?\n+//!\n+//! Here we skolemize the bound region in the supertype to yield:\n+//!\n+//!     fn(&'A T) <: fn(&'x T)?\n+//!\n+//! And then proceed to compare the argument types:\n+//!\n+//!     &'x T <: &'A T\n+//!     'A <= 'x\n+//!\n+//! Finally, this is where it gets interesting!  This is where an error\n+//! *should* be reported. But in fact this will not happen. The reason why\n+//! is that `A` is a variable: we will infer that its value is the fresh\n+//! region `x` and think that everything is happy. In fact, this behavior\n+//! is *necessary*, it was key to the first example we walked through.\n+//!\n+//! The difference between this example and the first one is that the variable\n+//! `A` already existed at the point where the skolemization occurred. In\n+//! the first example, you had two functions:\n+//!\n+//!     for<'a> fn(&'a T) <: for<'b> fn(&'b T)\n+//!\n+//! and hence `&A` and `&x` were created \"together\". In general, the\n+//! intention of the skolemized names is that they are supposed to be\n+//! fresh names that could never be equal to anything from the outside.\n+//! But when inference comes into play, we might not be respecting this\n+//! rule.\n+//!\n+//! So the way we solve this is to add a fourth step that examines the\n+//! constraints that refer to skolemized names. Basically, consider a\n+//! non-directed verison of the constraint graph. Let `Tainted(x)` be the\n+//! set of all things reachable from a skolemized variable `x`.\n+//! `Tainted(x)` should not contain any regions that existed before the\n+//! step at which the skolemization was performed. So this case here\n+//! would fail because `&x` was created alone, but is relatable to `&A`.\n+//!\n+//! ## Computing the LUB and GLB\n+//!\n+//! The paper I pointed you at is written for Haskell. It does not\n+//! therefore considering subtyping and in particular does not consider\n+//! LUB or GLB computation. We have to consider this. Here is the\n+//! algorithm I implemented.\n+//!\n+//! First though, let's discuss what we are trying to compute in more\n+//! detail. The LUB is basically the \"common supertype\" and the GLB is\n+//! \"common subtype\"; one catch is that the LUB should be the\n+//! *most-specific* common supertype and the GLB should be *most general*\n+//! common subtype (as opposed to any common supertype or any common\n+//! subtype).\n+//!\n+//! Anyway, to help clarify, here is a table containing some function\n+//! pairs and their LUB/GLB (for conciseness, in this table, I'm just\n+//! including the lifetimes here, not the rest of the types, and I'm\n+//! writing `fn<>` instead of `for<> fn`):\n+//!\n+//! ```\n+//! Type 1                Type 2                LUB                    GLB\n+//! fn<'a>('a)            fn('X)                fn('X)                 fn<'a>('a)\n+//! fn('a)                fn('X)                --                     fn<'a>('a)\n+//! fn<'a,'b>('a, 'b)     fn<'x>('x, 'x)        fn<'a>('a, 'a)         fn<'a,'b>('a, 'b)\n+//! fn<'a,'b>('a, 'b, 'a) fn<'x,'y>('x, 'y, 'y) fn<'a>('a, 'a, 'a)     fn<'a,'b,'c>('a,'b,'c)\n+//! ```\n+//!\n+//! ### Conventions\n+//!\n+//! I use lower-case letters (e.g., `&a`) for bound regions and upper-case\n+//! letters for free regions (`&A`).  Region variables written with a\n+//! dollar-sign (e.g., `$a`).  I will try to remember to enumerate the\n+//! bound-regions on the fn type as well (e.g., `for<'a> fn(&a)`).\n+//!\n+//! ### High-level summary\n+//!\n+//! Both the LUB and the GLB algorithms work in a similar fashion.  They\n+//! begin by replacing all bound regions (on both sides) with fresh region\n+//! inference variables.  Therefore, both functions are converted to types\n+//! that contain only free regions.  We can then compute the LUB/GLB in a\n+//! straightforward way, as described in `combine.rs`.  This results in an\n+//! interim type T.  The algorithms then examine the regions that appear\n+//! in T and try to, in some cases, replace them with bound regions to\n+//! yield the final result.\n+//!\n+//! To decide whether to replace a region `R` that appears in `T` with a\n+//! bound region, the algorithms make use of two bits of information.\n+//! First is a set `V` that contains all region variables created as part\n+//! of the LUB/GLB computation. `V` will contain the region variables\n+//! created to replace the bound regions in the input types, but it also\n+//! contains 'intermediate' variables created to represent the LUB/GLB of\n+//! individual regions.  Basically, when asked to compute the LUB/GLB of a\n+//! region variable with another region, the inferencer cannot oblige\n+//! immediately since the values of that variables are not known.\n+//! Therefore, it creates a new variable that is related to the two\n+//! regions.  For example, the LUB of two variables `$x` and `$y` is a\n+//! fresh variable `$z` that is constrained such that `$x <= $z` and `$y\n+//! <= $z`.  So `V` will contain these intermediate variables as well.\n+//!\n+//! The other important factor in deciding how to replace a region in T is\n+//! the function `Tainted($r)` which, for a region variable, identifies\n+//! all regions that the region variable is related to in some way\n+//! (`Tainted()` made an appearance in the subtype computation as well).\n+//!\n+//! ### LUB\n+//!\n+//! The LUB algorithm proceeds in three steps:\n+//!\n+//! 1. Replace all bound regions (on both sides) with fresh region\n+//!    inference variables.\n+//! 2. Compute the LUB \"as normal\", meaning compute the GLB of each\n+//!    pair of argument types and the LUB of the return types and\n+//!    so forth.  Combine those to a new function type `F`.\n+//! 3. Replace each region `R` that appears in `F` as follows:\n+//!    - Let `V` be the set of variables created during the LUB\n+//!      computational steps 1 and 2, as described in the previous section.\n+//!    - If `R` is not in `V`, replace `R` with itself.\n+//!    - If `Tainted(R)` contains a region that is not in `V`,\n+//!      replace `R` with itself.\n+//!    - Otherwise, select the earliest variable in `Tainted(R)` that originates\n+//!      from the left-hand side and replace `R` with the bound region that\n+//!      this variable was a replacement for.\n+//!\n+//! So, let's work through the simplest example: `fn(&A)` and `for<'a> fn(&a)`.\n+//! In this case, `&a` will be replaced with `$a` and the interim LUB type\n+//! `fn($b)` will be computed, where `$b=GLB(&A,$a)`.  Therefore, `V =\n+//! {$a, $b}` and `Tainted($b) = { $b, $a, &A }`.  When we go to replace\n+//! `$b`, we find that since `&A \\in Tainted($b)` is not a member of `V`,\n+//! we leave `$b` as is.  When region inference happens, `$b` will be\n+//! resolved to `&A`, as we wanted.\n+//!\n+//! Let's look at a more complex one: `fn(&a, &b)` and `fn(&x, &x)`.  In\n+//! this case, we'll end up with a (pre-replacement) LUB type of `fn(&g,\n+//! &h)` and a graph that looks like:\n+//!\n+//! ```\n+//!      $a        $b     *--$x\n+//!        \\        \\    /  /\n+//!         \\        $h-*  /\n+//!          $g-----------*\n+//! ```\n+//!\n+//! Here `$g` and `$h` are fresh variables that are created to represent\n+//! the LUB/GLB of things requiring inference.  This means that `V` and\n+//! `Tainted` will look like:\n+//!\n+//! ```\n+//! V = {$a, $b, $g, $h, $x}\n+//! Tainted($g) = Tainted($h) = { $a, $b, $h, $g, $x }\n+//! ```\n+//!\n+//! Therefore we replace both `$g` and `$h` with `$a`, and end up\n+//! with the type `fn(&a, &a)`.\n+//!\n+//! ### GLB\n+//!\n+//! The procedure for computing the GLB is similar.  The difference lies\n+//! in computing the replacements for the various variables. For each\n+//! region `R` that appears in the type `F`, we again compute `Tainted(R)`\n+//! and examine the results:\n+//!\n+//! 1. If `R` is not in `V`, it is not replaced.\n+//! 2. Else, if `Tainted(R)` contains only variables in `V`, and it\n+//!    contains exactly one variable from the LHS and one variable from\n+//!    the RHS, then `R` can be mapped to the bound version of the\n+//!    variable from the LHS.\n+//! 3. Else, if `Tainted(R)` contains no variable from the LHS and no\n+//!    variable from the RHS, then `R` can be mapped to itself.\n+//! 4. Else, `R` is mapped to a fresh bound variable.\n+//!\n+//! These rules are pretty complex.  Let's look at some examples to see\n+//! how they play out.\n+//!\n+//! Out first example was `fn(&a)` and `fn(&X)`.  In this case, `&a` will\n+//! be replaced with `$a` and we will ultimately compute a\n+//! (pre-replacement) GLB type of `fn($g)` where `$g=LUB($a,&X)`.\n+//! Therefore, `V={$a,$g}` and `Tainted($g)={$g,$a,&X}.  To find the\n+//! replacement for `$g` we consult the rules above:\n+//! - Rule (1) does not apply because `$g \\in V`\n+//! - Rule (2) does not apply because `&X \\in Tainted($g)`\n+//! - Rule (3) does not apply because `$a \\in Tainted($g)`\n+//! - Hence, by rule (4), we replace `$g` with a fresh bound variable `&z`.\n+//! So our final result is `fn(&z)`, which is correct.\n+//!\n+//! The next example is `fn(&A)` and `fn(&Z)`. In this case, we will again\n+//! have a (pre-replacement) GLB of `fn(&g)`, where `$g = LUB(&A,&Z)`.\n+//! Therefore, `V={$g}` and `Tainted($g) = {$g, &A, &Z}`.  In this case,\n+//! by rule (3), `$g` is mapped to itself, and hence the result is\n+//! `fn($g)`.  This result is correct (in this case, at least), but it is\n+//! indicative of a case that *can* lead us into concluding that there is\n+//! no GLB when in fact a GLB does exist.  See the section \"Questionable\n+//! Results\" below for more details.\n+//!\n+//! The next example is `fn(&a, &b)` and `fn(&c, &c)`. In this case, as\n+//! before, we'll end up with `F=fn($g, $h)` where `Tainted($g) =\n+//! Tainted($h) = {$g, $h, $a, $b, $c}`.  Only rule (4) applies and hence\n+//! we'll select fresh bound variables `y` and `z` and wind up with\n+//! `fn(&y, &z)`.\n+//!\n+//! For the last example, let's consider what may seem trivial, but is\n+//! not: `fn(&a, &a)` and `fn(&b, &b)`.  In this case, we'll get `F=fn($g,\n+//! $h)` where `Tainted($g) = {$g, $a, $x}` and `Tainted($h) = {$h, $a,\n+//! $x}`.  Both of these sets contain exactly one bound variable from each\n+//! side, so we'll map them both to `&a`, resulting in `fn(&a, &a)`, which\n+//! is the desired result.\n+//!\n+//! ### Shortcomings and correctness\n+//!\n+//! You may be wondering whether this algorithm is correct.  The answer is\n+//! \"sort of\".  There are definitely cases where they fail to compute a\n+//! result even though a correct result exists.  I believe, though, that\n+//! if they succeed, then the result is valid, and I will attempt to\n+//! convince you.  The basic argument is that the \"pre-replacement\" step\n+//! computes a set of constraints.  The replacements, then, attempt to\n+//! satisfy those constraints, using bound identifiers where needed.\n+//!\n+//! For now I will briefly go over the cases for LUB/GLB and identify\n+//! their intent:\n+//!\n+//! - LUB:\n+//!   - The region variables that are substituted in place of bound regions\n+//!     are intended to collect constraints on those bound regions.\n+//!   - If Tainted(R) contains only values in V, then this region is unconstrained\n+//!     and can therefore be generalized, otherwise it cannot.\n+//! - GLB:\n+//!   - The region variables that are substituted in place of bound regions\n+//!     are intended to collect constraints on those bound regions.\n+//!   - If Tainted(R) contains exactly one variable from each side, and\n+//!     only variables in V, that indicates that those two bound regions\n+//!     must be equated.\n+//!   - Otherwise, if Tainted(R) references any variables from left or right\n+//!     side, then it is trying to combine a bound region with a free one or\n+//!     multiple bound regions, so we need to select fresh bound regions.\n+//!\n+//! Sorry this is more of a shorthand to myself.  I will try to write up something\n+//! more convincing in the future.\n+//!\n+//! #### Where are the algorithms wrong?\n+//!\n+//! - The pre-replacement computation can fail even though using a\n+//!   bound-region would have succeeded.\n+//! - We will compute GLB(fn(fn($a)), fn(fn($b))) as fn($c) where $c is the\n+//!   GLB of $a and $b.  But if inference finds that $a and $b must be mapped\n+//!   to regions without a GLB, then this is effectively a failure to compute\n+//!   the GLB.  However, the result `fn<$c>(fn($c))` is a valid GLB."}, {"sha": "2f80a574bb18bde49fafca54e0c41ca905364a62", "filename": "src/librustc/middle/typeck/infer/higher_ranked/mod.rs", "status": "modified", "additions": 2, "deletions": 4, "changes": 6, "blob_url": "https://github.com/rust-lang/rust/blob/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc%2Fmiddle%2Ftypeck%2Finfer%2Fhigher_ranked%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc%2Fmiddle%2Ftypeck%2Finfer%2Fhigher_ranked%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Fmiddle%2Ftypeck%2Finfer%2Fhigher_ranked%2Fmod.rs?ref=cd5c8235c5448a7234548c772468c8d2e8f150d9", "patch": "@@ -8,10 +8,8 @@\n // option. This file may not be copied, modified, or distributed\n // except according to those terms.\n \n-/*!\n- * Helper routines for higher-ranked things. See the `doc` module at\n- * the end of the file for details.\n- */\n+//! Helper routines for higher-ranked things. See the `doc` module at\n+//! the end of the file for details.\n \n use middle::ty::{mod, Ty, replace_late_bound_regions};\n use middle::typeck::infer::{mod, combine, cres, InferCtxt};"}, {"sha": "daec959d11cd322efdadef224fa1c95c53a3b643", "filename": "src/librustc/middle/typeck/infer/lattice.rs", "status": "modified", "additions": 20, "deletions": 22, "changes": 42, "blob_url": "https://github.com/rust-lang/rust/blob/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc%2Fmiddle%2Ftypeck%2Finfer%2Flattice.rs", "raw_url": "https://github.com/rust-lang/rust/raw/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc%2Fmiddle%2Ftypeck%2Finfer%2Flattice.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Fmiddle%2Ftypeck%2Finfer%2Flattice.rs?ref=cd5c8235c5448a7234548c772468c8d2e8f150d9", "patch": "@@ -8,28 +8,26 @@\n // option. This file may not be copied, modified, or distributed\n // except according to those terms.\n \n-/*!\n- * # Lattice Variables\n- *\n- * This file contains generic code for operating on inference variables\n- * that are characterized by an upper- and lower-bound.  The logic and\n- * reasoning is explained in detail in the large comment in `infer.rs`.\n- *\n- * The code in here is defined quite generically so that it can be\n- * applied both to type variables, which represent types being inferred,\n- * and fn variables, which represent function types being inferred.\n- * It may eventually be applied to their types as well, who knows.\n- * In some cases, the functions are also generic with respect to the\n- * operation on the lattice (GLB vs LUB).\n- *\n- * Although all the functions are generic, we generally write the\n- * comments in a way that is specific to type variables and the LUB\n- * operation.  It's just easier that way.\n- *\n- * In general all of the functions are defined parametrically\n- * over a `LatticeValue`, which is a value defined with respect to\n- * a lattice.\n- */\n+//! # Lattice Variables\n+//!\n+//! This file contains generic code for operating on inference variables\n+//! that are characterized by an upper- and lower-bound.  The logic and\n+//! reasoning is explained in detail in the large comment in `infer.rs`.\n+//!\n+//! The code in here is defined quite generically so that it can be\n+//! applied both to type variables, which represent types being inferred,\n+//! and fn variables, which represent function types being inferred.\n+//! It may eventually be applied to their types as well, who knows.\n+//! In some cases, the functions are also generic with respect to the\n+//! operation on the lattice (GLB vs LUB).\n+//!\n+//! Although all the functions are generic, we generally write the\n+//! comments in a way that is specific to type variables and the LUB\n+//! operation.  It's just easier that way.\n+//!\n+//! In general all of the functions are defined parametrically\n+//! over a `LatticeValue`, which is a value defined with respect to\n+//! a lattice.\n \n use middle::ty::{TyVar};\n use middle::ty::{mod, Ty};"}, {"sha": "c5845b143af89aca4da0d3ee6b563e498d094543", "filename": "src/librustc/middle/typeck/infer/mod.rs", "status": "modified", "additions": 8, "deletions": 18, "changes": 26, "blob_url": "https://github.com/rust-lang/rust/blob/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc%2Fmiddle%2Ftypeck%2Finfer%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc%2Fmiddle%2Ftypeck%2Finfer%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Fmiddle%2Ftypeck%2Finfer%2Fmod.rs?ref=cd5c8235c5448a7234548c772468c8d2e8f150d9", "patch": "@@ -8,7 +8,7 @@\n // option. This file may not be copied, modified, or distributed\n // except according to those terms.\n \n-/*! See doc.rs for documentation */\n+//! See doc.rs for documentation\n \n #![allow(non_camel_case_types)]\n \n@@ -305,18 +305,15 @@ pub fn new_infer_ctxt<'a, 'tcx>(tcx: &'a ty::ctxt<'tcx>)\n     }\n }\n \n+/// Computes the least upper-bound of `a` and `b`. If this is not possible, reports an error and\n+/// returns ty::err.\n pub fn common_supertype<'a, 'tcx>(cx: &InferCtxt<'a, 'tcx>,\n                                   origin: TypeOrigin,\n                                   a_is_expected: bool,\n                                   a: Ty<'tcx>,\n                                   b: Ty<'tcx>)\n                                   -> Ty<'tcx>\n {\n-    /*!\n-     * Computes the least upper-bound of `a` and `b`. If this is\n-     * not possible, reports an error and returns ty::err.\n-     */\n-\n     debug!(\"common_supertype({}, {})\",\n            a.repr(cx.tcx), b.repr(cx.tcx));\n \n@@ -754,17 +751,13 @@ impl<'a, 'tcx> InferCtxt<'a, 'tcx> {\n             .collect()\n     }\n \n+    /// Given a set of generics defined on a type or impl, returns a substitution mapping each\n+    /// type/region parameter to a fresh inference variable.\n     pub fn fresh_substs_for_generics(&self,\n                                      span: Span,\n                                      generics: &ty::Generics<'tcx>)\n                                      -> subst::Substs<'tcx>\n     {\n-        /*!\n-         * Given a set of generics defined on a type or impl, returns\n-         * a substitution mapping each type/region parameter to a\n-         * fresh inference variable.\n-         */\n-\n         let type_params =\n             generics.types.map(\n                 |_| self.next_ty_var());\n@@ -774,18 +767,15 @@ impl<'a, 'tcx> InferCtxt<'a, 'tcx> {\n         subst::Substs::new(type_params, region_params)\n     }\n \n+    /// Given a set of generics defined on a trait, returns a substitution mapping each output\n+    /// type/region parameter to a fresh inference variable, and mapping the self type to\n+    /// `self_ty`.\n     pub fn fresh_substs_for_trait(&self,\n                                   span: Span,\n                                   generics: &ty::Generics<'tcx>,\n                                   self_ty: Ty<'tcx>)\n                                   -> subst::Substs<'tcx>\n     {\n-        /*!\n-         * Given a set of generics defined on a trait, returns a\n-         * substitution mapping each output type/region parameter to a\n-         * fresh inference variable, and mapping the self type to\n-         * `self_ty`.\n-         */\n \n         assert!(generics.types.len(subst::SelfSpace) == 1);\n         assert!(generics.types.len(subst::FnSpace) == 0);"}, {"sha": "b4eac4c002677cdd87695a8d78ef60c0caf4ec20", "filename": "src/librustc/middle/typeck/infer/region_inference/doc.rs", "status": "modified", "additions": 364, "deletions": 368, "changes": 732, "blob_url": "https://github.com/rust-lang/rust/blob/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc%2Fmiddle%2Ftypeck%2Finfer%2Fregion_inference%2Fdoc.rs", "raw_url": "https://github.com/rust-lang/rust/raw/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc%2Fmiddle%2Ftypeck%2Finfer%2Fregion_inference%2Fdoc.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Fmiddle%2Ftypeck%2Finfer%2Fregion_inference%2Fdoc.rs?ref=cd5c8235c5448a7234548c772468c8d2e8f150d9", "patch": "@@ -8,371 +8,367 @@\n // option. This file may not be copied, modified, or distributed\n // except according to those terms.\n \n-/*!\n-\n-Region inference module.\n-\n-# Terminology\n-\n-Note that we use the terms region and lifetime interchangeably,\n-though the term `lifetime` is preferred.\n-\n-# Introduction\n-\n-Region inference uses a somewhat more involved algorithm than type\n-inference.  It is not the most efficient thing ever written though it\n-seems to work well enough in practice (famous last words).  The reason\n-that we use a different algorithm is because, unlike with types, it is\n-impractical to hand-annotate with regions (in some cases, there aren't\n-even the requisite syntactic forms).  So we have to get it right, and\n-it's worth spending more time on a more involved analysis.  Moreover,\n-regions are a simpler case than types: they don't have aggregate\n-structure, for example.\n-\n-Unlike normal type inference, which is similar in spirit to H-M and thus\n-works progressively, the region type inference works by accumulating\n-constraints over the course of a function.  Finally, at the end of\n-processing a function, we process and solve the constraints all at\n-once.\n-\n-The constraints are always of one of three possible forms:\n-\n-- ConstrainVarSubVar(R_i, R_j) states that region variable R_i\n-  must be a subregion of R_j\n-- ConstrainRegSubVar(R, R_i) states that the concrete region R\n-  (which must not be a variable) must be a subregion of the varibale R_i\n-- ConstrainVarSubReg(R_i, R) is the inverse\n-\n-# Building up the constraints\n-\n-Variables and constraints are created using the following methods:\n-\n-- `new_region_var()` creates a new, unconstrained region variable;\n-- `make_subregion(R_i, R_j)` states that R_i is a subregion of R_j\n-- `lub_regions(R_i, R_j) -> R_k` returns a region R_k which is\n-  the smallest region that is greater than both R_i and R_j\n-- `glb_regions(R_i, R_j) -> R_k` returns a region R_k which is\n-  the greatest region that is smaller than both R_i and R_j\n-\n-The actual region resolution algorithm is not entirely\n-obvious, though it is also not overly complex.\n-\n-## Snapshotting\n-\n-It is also permitted to try (and rollback) changes to the graph.  This\n-is done by invoking `start_snapshot()`, which returns a value.  Then\n-later you can call `rollback_to()` which undoes the work.\n-Alternatively, you can call `commit()` which ends all snapshots.\n-Snapshots can be recursive---so you can start a snapshot when another\n-is in progress, but only the root snapshot can \"commit\".\n-\n-# Resolving constraints\n-\n-The constraint resolution algorithm is not super complex but also not\n-entirely obvious.  Here I describe the problem somewhat abstractly,\n-then describe how the current code works.  There may be other, smarter\n-ways of doing this with which I am unfamiliar and can't be bothered to\n-research at the moment. - NDM\n-\n-## The problem\n-\n-Basically our input is a directed graph where nodes can be divided\n-into two categories: region variables and concrete regions.  Each edge\n-`R -> S` in the graph represents a constraint that the region `R` is a\n-subregion of the region `S`.\n-\n-Region variable nodes can have arbitrary degree.  There is one region\n-variable node per region variable.\n-\n-Each concrete region node is associated with some, well, concrete\n-region: e.g., a free lifetime, or the region for a particular scope.\n-Note that there may be more than one concrete region node for a\n-particular region value.  Moreover, because of how the graph is built,\n-we know that all concrete region nodes have either in-degree 1 or\n-out-degree 1.\n-\n-Before resolution begins, we build up the constraints in a hashmap\n-that maps `Constraint` keys to spans.  During resolution, we construct\n-the actual `Graph` structure that we describe here.\n-\n-## Our current algorithm\n-\n-We divide region variables into two groups: Expanding and Contracting.\n-Expanding region variables are those that have a concrete region\n-predecessor (direct or indirect).  Contracting region variables are\n-all others.\n-\n-We first resolve the values of Expanding region variables and then\n-process Contracting ones.  We currently use an iterative, fixed-point\n-procedure (but read on, I believe this could be replaced with a linear\n-walk).  Basically we iterate over the edges in the graph, ensuring\n-that, if the source of the edge has a value, then this value is a\n-subregion of the target value.  If the target does not yet have a\n-value, it takes the value from the source.  If the target already had\n-a value, then the resulting value is Least Upper Bound of the old and\n-new values. When we are done, each Expanding node will have the\n-smallest region that it could possibly have and still satisfy the\n-constraints.\n-\n-We next process the Contracting nodes.  Here we again iterate over the\n-edges, only this time we move values from target to source (if the\n-source is a Contracting node).  For each contracting node, we compute\n-its value as the GLB of all its successors.  Basically contracting\n-nodes ensure that there is overlap between their successors; we will\n-ultimately infer the largest overlap possible.\n-\n-# The Region Hierarchy\n-\n-## Without closures\n-\n-Let's first consider the region hierarchy without thinking about\n-closures, because they add a lot of complications. The region\n-hierarchy *basically* mirrors the lexical structure of the code.\n-There is a region for every piece of 'evaluation' that occurs, meaning\n-every expression, block, and pattern (patterns are considered to\n-\"execute\" by testing the value they are applied to and creating any\n-relevant bindings).  So, for example:\n-\n-    fn foo(x: int, y: int) { // -+\n-    //  +------------+       //  |\n-    //  |      +-----+       //  |\n-    //  |  +-+ +-+ +-+       //  |\n-    //  |  | | | | | |       //  |\n-    //  v  v v v v v v       //  |\n-        let z = x + y;       //  |\n-        ...                  //  |\n-    }                        // -+\n-\n-    fn bar() { ... }\n-\n-In this example, there is a region for the fn body block as a whole,\n-and then a subregion for the declaration of the local variable.\n-Within that, there are sublifetimes for the assignment pattern and\n-also the expression `x + y`. The expression itself has sublifetimes\n-for evaluating `x` and `y`.\n-\n-## Function calls\n-\n-Function calls are a bit tricky. I will describe how we handle them\n-*now* and then a bit about how we can improve them (Issue #6268).\n-\n-Consider a function call like `func(expr1, expr2)`, where `func`,\n-`arg1`, and `arg2` are all arbitrary expressions. Currently,\n-we construct a region hierarchy like:\n-\n-    +----------------+\n-    |                |\n-    +--+ +---+  +---+|\n-    v  v v   v  v   vv\n-    func(expr1, expr2)\n-\n-Here you can see that the call as a whole has a region and the\n-function plus arguments are subregions of that. As a side-effect of\n-this, we get a lot of spurious errors around nested calls, in\n-particular when combined with `&mut` functions. For example, a call\n-like this one\n-\n-    self.foo(self.bar())\n-\n-where both `foo` and `bar` are `&mut self` functions will always yield\n-an error.\n-\n-Here is a more involved example (which is safe) so we can see what's\n-going on:\n-\n-    struct Foo { f: uint, g: uint }\n-    ...\n-    fn add(p: &mut uint, v: uint) {\n-        *p += v;\n-    }\n-    ...\n-    fn inc(p: &mut uint) -> uint {\n-        *p += 1; *p\n-    }\n-    fn weird() {\n-        let mut x: Box<Foo> = box Foo { ... };\n-        'a: add(&mut (*x).f,\n-                'b: inc(&mut (*x).f)) // (..)\n-    }\n-\n-The important part is the line marked `(..)` which contains a call to\n-`add()`. The first argument is a mutable borrow of the field `f`.  The\n-second argument also borrows the field `f`. Now, in the current borrow\n-checker, the first borrow is given the lifetime of the call to\n-`add()`, `'a`.  The second borrow is given the lifetime of `'b` of the\n-call to `inc()`. Because `'b` is considered to be a sublifetime of\n-`'a`, an error is reported since there are two co-existing mutable\n-borrows of the same data.\n-\n-However, if we were to examine the lifetimes a bit more carefully, we\n-can see that this error is unnecessary. Let's examine the lifetimes\n-involved with `'a` in detail. We'll break apart all the steps involved\n-in a call expression:\n-\n-    'a: {\n-        'a_arg1: let a_temp1: ... = add;\n-        'a_arg2: let a_temp2: &'a mut uint = &'a mut (*x).f;\n-        'a_arg3: let a_temp3: uint = {\n-            let b_temp1: ... = inc;\n-            let b_temp2: &'b = &'b mut (*x).f;\n-            'b_call: b_temp1(b_temp2)\n-        };\n-        'a_call: a_temp1(a_temp2, a_temp3) // (**)\n-    }\n-\n-Here we see that the lifetime `'a` includes a number of substatements.\n-In particular, there is this lifetime I've called `'a_call` that\n-corresponds to the *actual execution of the function `add()`*, after\n-all arguments have been evaluated. There is a corresponding lifetime\n-`'b_call` for the execution of `inc()`. If we wanted to be precise\n-about it, the lifetime of the two borrows should be `'a_call` and\n-`'b_call` respectively, since the references that were created\n-will not be dereferenced except during the execution itself.\n-\n-However, this model by itself is not sound. The reason is that\n-while the two references that are created will never be used\n-simultaneously, it is still true that the first reference is\n-*created* before the second argument is evaluated, and so even though\n-it will not be *dereferenced* during the evaluation of the second\n-argument, it can still be *invalidated* by that evaluation. Consider\n-this similar but unsound example:\n-\n-    struct Foo { f: uint, g: uint }\n-    ...\n-    fn add(p: &mut uint, v: uint) {\n-        *p += v;\n-    }\n-    ...\n-    fn consume(x: Box<Foo>) -> uint {\n-        x.f + x.g\n-    }\n-    fn weird() {\n-        let mut x: Box<Foo> = box Foo { ... };\n-        'a: add(&mut (*x).f, consume(x)) // (..)\n-    }\n-\n-In this case, the second argument to `add` actually consumes `x`, thus\n-invalidating the first argument.\n-\n-So, for now, we exclude the `call` lifetimes from our model.\n-Eventually I would like to include them, but we will have to make the\n-borrow checker handle this situation correctly. In particular, if\n-there is a reference created whose lifetime does not enclose\n-the borrow expression, we must issue sufficient restrictions to ensure\n-that the pointee remains valid.\n-\n-## Adding closures\n-\n-The other significant complication to the region hierarchy is\n-closures. I will describe here how closures should work, though some\n-of the work to implement this model is ongoing at the time of this\n-writing.\n-\n-The body of closures are type-checked along with the function that\n-creates them. However, unlike other expressions that appear within the\n-function body, it is not entirely obvious when a closure body executes\n-with respect to the other expressions. This is because the closure\n-body will execute whenever the closure is called; however, we can\n-never know precisely when the closure will be called, especially\n-without some sort of alias analysis.\n-\n-However, we can place some sort of limits on when the closure\n-executes.  In particular, the type of every closure `fn:'r K` includes\n-a region bound `'r`. This bound indicates the maximum lifetime of that\n-closure; once we exit that region, the closure cannot be called\n-anymore. Therefore, we say that the lifetime of the closure body is a\n-sublifetime of the closure bound, but the closure body itself is unordered\n-with respect to other parts of the code.\n-\n-For example, consider the following fragment of code:\n-\n-    'a: {\n-         let closure: fn:'a() = || 'b: {\n-             'c: ...\n-         };\n-         'd: ...\n-    }\n-\n-Here we have four lifetimes, `'a`, `'b`, `'c`, and `'d`. The closure\n-`closure` is bounded by the lifetime `'a`. The lifetime `'b` is the\n-lifetime of the closure body, and `'c` is some statement within the\n-closure body. Finally, `'d` is a statement within the outer block that\n-created the closure.\n-\n-We can say that the closure body `'b` is a sublifetime of `'a` due to\n-the closure bound. By the usual lexical scoping conventions, the\n-statement `'c` is clearly a sublifetime of `'b`, and `'d` is a\n-sublifetime of `'d`. However, there is no ordering between `'c` and\n-`'d` per se (this kind of ordering between statements is actually only\n-an issue for dataflow; passes like the borrow checker must assume that\n-closures could execute at any time from the moment they are created\n-until they go out of scope).\n-\n-### Complications due to closure bound inference\n-\n-There is only one problem with the above model: in general, we do not\n-actually *know* the closure bounds during region inference! In fact,\n-closure bounds are almost always region variables! This is very tricky\n-because the inference system implicitly assumes that we can do things\n-like compute the LUB of two scoped lifetimes without needing to know\n-the values of any variables.\n-\n-Here is an example to illustrate the problem:\n-\n-    fn identify<T>(x: T) -> T { x }\n-\n-    fn foo() { // 'foo is the function body\n-      'a: {\n-           let closure = identity(|| 'b: {\n-               'c: ...\n-           });\n-           'd: closure();\n-      }\n-      'e: ...;\n-    }\n-\n-In this example, the closure bound is not explicit. At compile time,\n-we will create a region variable (let's call it `V0`) to represent the\n-closure bound.\n-\n-The primary difficulty arises during the constraint propagation phase.\n-Imagine there is some variable with incoming edges from `'c` and `'d`.\n-This means that the value of the variable must be `LUB('c,\n-'d)`. However, without knowing what the closure bound `V0` is, we\n-can't compute the LUB of `'c` and `'d`! Any we don't know the closure\n-bound until inference is done.\n-\n-The solution is to rely on the fixed point nature of inference.\n-Basically, when we must compute `LUB('c, 'd)`, we just use the current\n-value for `V0` as the closure's bound. If `V0`'s binding should\n-change, then we will do another round of inference, and the result of\n-`LUB('c, 'd)` will change.\n-\n-One minor implication of this is that the graph does not in fact track\n-the full set of dependencies between edges. We cannot easily know\n-whether the result of a LUB computation will change, since there may\n-be indirect dependencies on other variables that are not reflected on\n-the graph. Therefore, we must *always* iterate over all edges when\n-doing the fixed point calculation, not just those adjacent to nodes\n-whose values have changed.\n-\n-Were it not for this requirement, we could in fact avoid fixed-point\n-iteration altogether. In that universe, we could instead first\n-identify and remove strongly connected components (SCC) in the graph.\n-Note that such components must consist solely of region variables; all\n-of these variables can effectively be unified into a single variable.\n-Once SCCs are removed, we are left with a DAG.  At this point, we\n-could walk the DAG in topological order once to compute the expanding\n-nodes, and again in reverse topological order to compute the\n-contracting nodes. However, as I said, this does not work given the\n-current treatment of closure bounds, but perhaps in the future we can\n-address this problem somehow and make region inference somewhat more\n-efficient. Note that this is solely a matter of performance, not\n-expressiveness.\n-\n-### Skolemization\n-\n-For a discussion on skolemization and higher-ranked subtyping, please\n-see the module `middle::typeck::infer::higher_ranked::doc`.\n-\n-*/\n+//! Region inference module.\n+//!\n+//! # Terminology\n+//!\n+//! Note that we use the terms region and lifetime interchangeably,\n+//! though the term `lifetime` is preferred.\n+//!\n+//! # Introduction\n+//!\n+//! Region inference uses a somewhat more involved algorithm than type\n+//! inference.  It is not the most efficient thing ever written though it\n+//! seems to work well enough in practice (famous last words).  The reason\n+//! that we use a different algorithm is because, unlike with types, it is\n+//! impractical to hand-annotate with regions (in some cases, there aren't\n+//! even the requisite syntactic forms).  So we have to get it right, and\n+//! it's worth spending more time on a more involved analysis.  Moreover,\n+//! regions are a simpler case than types: they don't have aggregate\n+//! structure, for example.\n+//!\n+//! Unlike normal type inference, which is similar in spirit to H-M and thus\n+//! works progressively, the region type inference works by accumulating\n+//! constraints over the course of a function.  Finally, at the end of\n+//! processing a function, we process and solve the constraints all at\n+//! once.\n+//!\n+//! The constraints are always of one of three possible forms:\n+//!\n+//! - ConstrainVarSubVar(R_i, R_j) states that region variable R_i\n+//!   must be a subregion of R_j\n+//! - ConstrainRegSubVar(R, R_i) states that the concrete region R\n+//!   (which must not be a variable) must be a subregion of the varibale R_i\n+//! - ConstrainVarSubReg(R_i, R) is the inverse\n+//!\n+//! # Building up the constraints\n+//!\n+//! Variables and constraints are created using the following methods:\n+//!\n+//! - `new_region_var()` creates a new, unconstrained region variable;\n+//! - `make_subregion(R_i, R_j)` states that R_i is a subregion of R_j\n+//! - `lub_regions(R_i, R_j) -> R_k` returns a region R_k which is\n+//!   the smallest region that is greater than both R_i and R_j\n+//! - `glb_regions(R_i, R_j) -> R_k` returns a region R_k which is\n+//!   the greatest region that is smaller than both R_i and R_j\n+//!\n+//! The actual region resolution algorithm is not entirely\n+//! obvious, though it is also not overly complex.\n+//!\n+//! ## Snapshotting\n+//!\n+//! It is also permitted to try (and rollback) changes to the graph.  This\n+//! is done by invoking `start_snapshot()`, which returns a value.  Then\n+//! later you can call `rollback_to()` which undoes the work.\n+//! Alternatively, you can call `commit()` which ends all snapshots.\n+//! Snapshots can be recursive---so you can start a snapshot when another\n+//! is in progress, but only the root snapshot can \"commit\".\n+//!\n+//! # Resolving constraints\n+//!\n+//! The constraint resolution algorithm is not super complex but also not\n+//! entirely obvious.  Here I describe the problem somewhat abstractly,\n+//! then describe how the current code works.  There may be other, smarter\n+//! ways of doing this with which I am unfamiliar and can't be bothered to\n+//! research at the moment. - NDM\n+//!\n+//! ## The problem\n+//!\n+//! Basically our input is a directed graph where nodes can be divided\n+//! into two categories: region variables and concrete regions.  Each edge\n+//! `R -> S` in the graph represents a constraint that the region `R` is a\n+//! subregion of the region `S`.\n+//!\n+//! Region variable nodes can have arbitrary degree.  There is one region\n+//! variable node per region variable.\n+//!\n+//! Each concrete region node is associated with some, well, concrete\n+//! region: e.g., a free lifetime, or the region for a particular scope.\n+//! Note that there may be more than one concrete region node for a\n+//! particular region value.  Moreover, because of how the graph is built,\n+//! we know that all concrete region nodes have either in-degree 1 or\n+//! out-degree 1.\n+//!\n+//! Before resolution begins, we build up the constraints in a hashmap\n+//! that maps `Constraint` keys to spans.  During resolution, we construct\n+//! the actual `Graph` structure that we describe here.\n+//!\n+//! ## Our current algorithm\n+//!\n+//! We divide region variables into two groups: Expanding and Contracting.\n+//! Expanding region variables are those that have a concrete region\n+//! predecessor (direct or indirect).  Contracting region variables are\n+//! all others.\n+//!\n+//! We first resolve the values of Expanding region variables and then\n+//! process Contracting ones.  We currently use an iterative, fixed-point\n+//! procedure (but read on, I believe this could be replaced with a linear\n+//! walk).  Basically we iterate over the edges in the graph, ensuring\n+//! that, if the source of the edge has a value, then this value is a\n+//! subregion of the target value.  If the target does not yet have a\n+//! value, it takes the value from the source.  If the target already had\n+//! a value, then the resulting value is Least Upper Bound of the old and\n+//! new values. When we are done, each Expanding node will have the\n+//! smallest region that it could possibly have and still satisfy the\n+//! constraints.\n+//!\n+//! We next process the Contracting nodes.  Here we again iterate over the\n+//! edges, only this time we move values from target to source (if the\n+//! source is a Contracting node).  For each contracting node, we compute\n+//! its value as the GLB of all its successors.  Basically contracting\n+//! nodes ensure that there is overlap between their successors; we will\n+//! ultimately infer the largest overlap possible.\n+//!\n+//! # The Region Hierarchy\n+//!\n+//! ## Without closures\n+//!\n+//! Let's first consider the region hierarchy without thinking about\n+//! closures, because they add a lot of complications. The region\n+//! hierarchy *basically* mirrors the lexical structure of the code.\n+//! There is a region for every piece of 'evaluation' that occurs, meaning\n+//! every expression, block, and pattern (patterns are considered to\n+//! \"execute\" by testing the value they are applied to and creating any\n+//! relevant bindings).  So, for example:\n+//!\n+//!     fn foo(x: int, y: int) { // -+\n+//!     //  +------------+       //  |\n+//!     //  |      +-----+       //  |\n+//!     //  |  +-+ +-+ +-+       //  |\n+//!     //  |  | | | | | |       //  |\n+//!     //  v  v v v v v v       //  |\n+//!         let z = x + y;       //  |\n+//!         ...                  //  |\n+//!     }                        // -+\n+//!\n+//!     fn bar() { ... }\n+//!\n+//! In this example, there is a region for the fn body block as a whole,\n+//! and then a subregion for the declaration of the local variable.\n+//! Within that, there are sublifetimes for the assignment pattern and\n+//! also the expression `x + y`. The expression itself has sublifetimes\n+//! for evaluating `x` and `y`.\n+//!\n+//! ## Function calls\n+//!\n+//! Function calls are a bit tricky. I will describe how we handle them\n+//! *now* and then a bit about how we can improve them (Issue #6268).\n+//!\n+//! Consider a function call like `func(expr1, expr2)`, where `func`,\n+//! `arg1`, and `arg2` are all arbitrary expressions. Currently,\n+//! we construct a region hierarchy like:\n+//!\n+//!     +----------------+\n+//!     |                |\n+//!     +--+ +---+  +---+|\n+//!     v  v v   v  v   vv\n+//!     func(expr1, expr2)\n+//!\n+//! Here you can see that the call as a whole has a region and the\n+//! function plus arguments are subregions of that. As a side-effect of\n+//! this, we get a lot of spurious errors around nested calls, in\n+//! particular when combined with `&mut` functions. For example, a call\n+//! like this one\n+//!\n+//!     self.foo(self.bar())\n+//!\n+//! where both `foo` and `bar` are `&mut self` functions will always yield\n+//! an error.\n+//!\n+//! Here is a more involved example (which is safe) so we can see what's\n+//! going on:\n+//!\n+//!     struct Foo { f: uint, g: uint }\n+//!     ...\n+//!     fn add(p: &mut uint, v: uint) {\n+//!         *p += v;\n+//!     }\n+//!     ...\n+//!     fn inc(p: &mut uint) -> uint {\n+//!         *p += 1; *p\n+//!     }\n+//!     fn weird() {\n+//!         let mut x: Box<Foo> = box Foo { ... };\n+//!         'a: add(&mut (*x).f,\n+//!                 'b: inc(&mut (*x).f)) // (..)\n+//!     }\n+//!\n+//! The important part is the line marked `(..)` which contains a call to\n+//! `add()`. The first argument is a mutable borrow of the field `f`.  The\n+//! second argument also borrows the field `f`. Now, in the current borrow\n+//! checker, the first borrow is given the lifetime of the call to\n+//! `add()`, `'a`.  The second borrow is given the lifetime of `'b` of the\n+//! call to `inc()`. Because `'b` is considered to be a sublifetime of\n+//! `'a`, an error is reported since there are two co-existing mutable\n+//! borrows of the same data.\n+//!\n+//! However, if we were to examine the lifetimes a bit more carefully, we\n+//! can see that this error is unnecessary. Let's examine the lifetimes\n+//! involved with `'a` in detail. We'll break apart all the steps involved\n+//! in a call expression:\n+//!\n+//!     'a: {\n+//!         'a_arg1: let a_temp1: ... = add;\n+//!         'a_arg2: let a_temp2: &'a mut uint = &'a mut (*x).f;\n+//!         'a_arg3: let a_temp3: uint = {\n+//!             let b_temp1: ... = inc;\n+//!             let b_temp2: &'b = &'b mut (*x).f;\n+//!             'b_call: b_temp1(b_temp2)\n+//!         };\n+//!         'a_call: a_temp1(a_temp2, a_temp3) // (**)\n+//!     }\n+//!\n+//! Here we see that the lifetime `'a` includes a number of substatements.\n+//! In particular, there is this lifetime I've called `'a_call` that\n+//! corresponds to the *actual execution of the function `add()`*, after\n+//! all arguments have been evaluated. There is a corresponding lifetime\n+//! `'b_call` for the execution of `inc()`. If we wanted to be precise\n+//! about it, the lifetime of the two borrows should be `'a_call` and\n+//! `'b_call` respectively, since the references that were created\n+//! will not be dereferenced except during the execution itself.\n+//!\n+//! However, this model by itself is not sound. The reason is that\n+//! while the two references that are created will never be used\n+//! simultaneously, it is still true that the first reference is\n+//! *created* before the second argument is evaluated, and so even though\n+//! it will not be *dereferenced* during the evaluation of the second\n+//! argument, it can still be *invalidated* by that evaluation. Consider\n+//! this similar but unsound example:\n+//!\n+//!     struct Foo { f: uint, g: uint }\n+//!     ...\n+//!     fn add(p: &mut uint, v: uint) {\n+//!         *p += v;\n+//!     }\n+//!     ...\n+//!     fn consume(x: Box<Foo>) -> uint {\n+//!         x.f + x.g\n+//!     }\n+//!     fn weird() {\n+//!         let mut x: Box<Foo> = box Foo { ... };\n+//!         'a: add(&mut (*x).f, consume(x)) // (..)\n+//!     }\n+//!\n+//! In this case, the second argument to `add` actually consumes `x`, thus\n+//! invalidating the first argument.\n+//!\n+//! So, for now, we exclude the `call` lifetimes from our model.\n+//! Eventually I would like to include them, but we will have to make the\n+//! borrow checker handle this situation correctly. In particular, if\n+//! there is a reference created whose lifetime does not enclose\n+//! the borrow expression, we must issue sufficient restrictions to ensure\n+//! that the pointee remains valid.\n+//!\n+//! ## Adding closures\n+//!\n+//! The other significant complication to the region hierarchy is\n+//! closures. I will describe here how closures should work, though some\n+//! of the work to implement this model is ongoing at the time of this\n+//! writing.\n+//!\n+//! The body of closures are type-checked along with the function that\n+//! creates them. However, unlike other expressions that appear within the\n+//! function body, it is not entirely obvious when a closure body executes\n+//! with respect to the other expressions. This is because the closure\n+//! body will execute whenever the closure is called; however, we can\n+//! never know precisely when the closure will be called, especially\n+//! without some sort of alias analysis.\n+//!\n+//! However, we can place some sort of limits on when the closure\n+//! executes.  In particular, the type of every closure `fn:'r K` includes\n+//! a region bound `'r`. This bound indicates the maximum lifetime of that\n+//! closure; once we exit that region, the closure cannot be called\n+//! anymore. Therefore, we say that the lifetime of the closure body is a\n+//! sublifetime of the closure bound, but the closure body itself is unordered\n+//! with respect to other parts of the code.\n+//!\n+//! For example, consider the following fragment of code:\n+//!\n+//!     'a: {\n+//!          let closure: fn:'a() = || 'b: {\n+//!              'c: ...\n+//!          };\n+//!          'd: ...\n+//!     }\n+//!\n+//! Here we have four lifetimes, `'a`, `'b`, `'c`, and `'d`. The closure\n+//! `closure` is bounded by the lifetime `'a`. The lifetime `'b` is the\n+//! lifetime of the closure body, and `'c` is some statement within the\n+//! closure body. Finally, `'d` is a statement within the outer block that\n+//! created the closure.\n+//!\n+//! We can say that the closure body `'b` is a sublifetime of `'a` due to\n+//! the closure bound. By the usual lexical scoping conventions, the\n+//! statement `'c` is clearly a sublifetime of `'b`, and `'d` is a\n+//! sublifetime of `'d`. However, there is no ordering between `'c` and\n+//! `'d` per se (this kind of ordering between statements is actually only\n+//! an issue for dataflow; passes like the borrow checker must assume that\n+//! closures could execute at any time from the moment they are created\n+//! until they go out of scope).\n+//!\n+//! ### Complications due to closure bound inference\n+//!\n+//! There is only one problem with the above model: in general, we do not\n+//! actually *know* the closure bounds during region inference! In fact,\n+//! closure bounds are almost always region variables! This is very tricky\n+//! because the inference system implicitly assumes that we can do things\n+//! like compute the LUB of two scoped lifetimes without needing to know\n+//! the values of any variables.\n+//!\n+//! Here is an example to illustrate the problem:\n+//!\n+//!     fn identify<T>(x: T) -> T { x }\n+//!\n+//!     fn foo() { // 'foo is the function body\n+//!       'a: {\n+//!            let closure = identity(|| 'b: {\n+//!                'c: ...\n+//!            });\n+//!            'd: closure();\n+//!       }\n+//!       'e: ...;\n+//!     }\n+//!\n+//! In this example, the closure bound is not explicit. At compile time,\n+//! we will create a region variable (let's call it `V0`) to represent the\n+//! closure bound.\n+//!\n+//! The primary difficulty arises during the constraint propagation phase.\n+//! Imagine there is some variable with incoming edges from `'c` and `'d`.\n+//! This means that the value of the variable must be `LUB('c,\n+//! 'd)`. However, without knowing what the closure bound `V0` is, we\n+//! can't compute the LUB of `'c` and `'d`! Any we don't know the closure\n+//! bound until inference is done.\n+//!\n+//! The solution is to rely on the fixed point nature of inference.\n+//! Basically, when we must compute `LUB('c, 'd)`, we just use the current\n+//! value for `V0` as the closure's bound. If `V0`'s binding should\n+//! change, then we will do another round of inference, and the result of\n+//! `LUB('c, 'd)` will change.\n+//!\n+//! One minor implication of this is that the graph does not in fact track\n+//! the full set of dependencies between edges. We cannot easily know\n+//! whether the result of a LUB computation will change, since there may\n+//! be indirect dependencies on other variables that are not reflected on\n+//! the graph. Therefore, we must *always* iterate over all edges when\n+//! doing the fixed point calculation, not just those adjacent to nodes\n+//! whose values have changed.\n+//!\n+//! Were it not for this requirement, we could in fact avoid fixed-point\n+//! iteration altogether. In that universe, we could instead first\n+//! identify and remove strongly connected components (SCC) in the graph.\n+//! Note that such components must consist solely of region variables; all\n+//! of these variables can effectively be unified into a single variable.\n+//! Once SCCs are removed, we are left with a DAG.  At this point, we\n+//! could walk the DAG in topological order once to compute the expanding\n+//! nodes, and again in reverse topological order to compute the\n+//! contracting nodes. However, as I said, this does not work given the\n+//! current treatment of closure bounds, but perhaps in the future we can\n+//! address this problem somehow and make region inference somewhat more\n+//! efficient. Note that this is solely a matter of performance, not\n+//! expressiveness.\n+//!\n+//! ### Skolemization\n+//!\n+//! For a discussion on skolemization and higher-ranked subtyping, please\n+//! see the module `middle::typeck::infer::higher_ranked::doc`."}, {"sha": "01533cba7ab6dcad2dd1c35523d4e07c51ab3760", "filename": "src/librustc/middle/typeck/infer/region_inference/mod.rs", "status": "modified", "additions": 9, "deletions": 21, "changes": 30, "blob_url": "https://github.com/rust-lang/rust/blob/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc%2Fmiddle%2Ftypeck%2Finfer%2Fregion_inference%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc%2Fmiddle%2Ftypeck%2Finfer%2Fregion_inference%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Fmiddle%2Ftypeck%2Finfer%2Fregion_inference%2Fmod.rs?ref=cd5c8235c5448a7234548c772468c8d2e8f150d9", "patch": "@@ -8,7 +8,7 @@\n // option. This file may not be copied, modified, or distributed\n // except according to those terms.\n \n-/*! See doc.rs */\n+//! See doc.rs\n \n pub use self::Constraint::*;\n pub use self::Verify::*;\n@@ -597,15 +597,10 @@ impl<'a, 'tcx> RegionVarBindings<'a, 'tcx> {\n             .collect()\n     }\n \n+    /// Computes all regions that have been related to `r0` in any way since the mark `mark` was\n+    /// made---`r0` itself will be the first entry. This is used when checking whether skolemized\n+    /// regions are being improperly related to other regions.\n     pub fn tainted(&self, mark: RegionMark, r0: Region) -> Vec<Region> {\n-        /*!\n-         * Computes all regions that have been related to `r0` in any\n-         * way since the mark `mark` was made---`r0` itself will be\n-         * the first entry. This is used when checking whether\n-         * skolemized regions are being improperly related to other\n-         * regions.\n-         */\n-\n         debug!(\"tainted(mark={}, r0={})\", mark, r0.repr(self.tcx));\n         let _indenter = indenter();\n \n@@ -783,16 +778,12 @@ impl<'a, 'tcx> RegionVarBindings<'a, 'tcx> {\n         }\n     }\n \n+    /// Computes a region that encloses both free region arguments. Guarantee that if the same two\n+    /// regions are given as argument, in any order, a consistent result is returned.\n     fn lub_free_regions(&self,\n                         a: &FreeRegion,\n                         b: &FreeRegion) -> ty::Region\n     {\n-        /*!\n-         * Computes a region that encloses both free region arguments.\n-         * Guarantee that if the same two regions are given as argument,\n-         * in any order, a consistent result is returned.\n-         */\n-\n         return match a.cmp(b) {\n             Less => helper(self, a, b),\n             Greater => helper(self, b, a),\n@@ -884,16 +875,13 @@ impl<'a, 'tcx> RegionVarBindings<'a, 'tcx> {\n         }\n     }\n \n+    /// Computes a region that is enclosed by both free region arguments, if any. Guarantees that\n+    /// if the same two regions are given as argument, in any order, a consistent result is\n+    /// returned.\n     fn glb_free_regions(&self,\n                         a: &FreeRegion,\n                         b: &FreeRegion) -> cres<'tcx, ty::Region>\n     {\n-        /*!\n-         * Computes a region that is enclosed by both free region arguments,\n-         * if any. Guarantees that if the same two regions are given as argument,\n-         * in any order, a consistent result is returned.\n-         */\n-\n         return match a.cmp(b) {\n             Less => helper(self, a, b),\n             Greater => helper(self, b, a),"}, {"sha": "62bf1d0126a59d156442f1260ecd98b47d21dae2", "filename": "src/librustc/middle/typeck/infer/skolemize.rs", "status": "modified", "additions": 21, "deletions": 31, "changes": 52, "blob_url": "https://github.com/rust-lang/rust/blob/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc%2Fmiddle%2Ftypeck%2Finfer%2Fskolemize.rs", "raw_url": "https://github.com/rust-lang/rust/raw/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc%2Fmiddle%2Ftypeck%2Finfer%2Fskolemize.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Fmiddle%2Ftypeck%2Finfer%2Fskolemize.rs?ref=cd5c8235c5448a7234548c772468c8d2e8f150d9", "patch": "@@ -8,37 +8,27 @@\n // option. This file may not be copied, modified, or distributed\n // except according to those terms.\n \n-/*!\n- * Skolemization is the process of replacing unknown variables with\n- * fresh types. The idea is that the type, after skolemization,\n- * contains no inference variables but instead contains either a value\n- * for each variable or fresh \"arbitrary\" types wherever a variable\n- * would have been.\n- *\n- * Skolemization is used primarily to get a good type for inserting\n- * into a cache. The result summarizes what the type inferencer knows\n- * \"so far\". The primary place it is used right now is in the trait\n- * matching algorithm, which needs to be able to cache whether an\n- * `impl` self type matches some other type X -- *without* affecting\n- * `X`. That means if that if the type `X` is in fact an unbound type\n- * variable, we want the match to be regarded as ambiguous, because\n- * depending on what type that type variable is ultimately assigned,\n- * the match may or may not succeed.\n- *\n- * Note that you should be careful not to allow the output of\n- * skolemization to leak to the user in error messages or in any other\n- * form. Skolemization is only really useful as an internal detail.\n- *\n- * __An important detail concerning regions.__ The skolemizer also\n- * replaces *all* regions with 'static. The reason behind this is\n- * that, in general, we do not take region relationships into account\n- * when making type-overloaded decisions. This is important because of\n- * the design of the region inferencer, which is not based on\n- * unification but rather on accumulating and then solving a set of\n- * constraints. In contrast, the type inferencer assigns a value to\n- * each type variable only once, and it does so as soon as it can, so\n- * it is reasonable to ask what the type inferencer knows \"so far\".\n- */\n+//! Skolemization is the process of replacing unknown variables with fresh types. The idea is that\n+//! the type, after skolemization, contains no inference variables but instead contains either a\n+//! value for each variable or fresh \"arbitrary\" types wherever a variable would have been.\n+//!\n+//! Skolemization is used primarily to get a good type for inserting into a cache. The result\n+//! summarizes what the type inferencer knows \"so far\". The primary place it is used right now is\n+//! in the trait matching algorithm, which needs to be able to cache whether an `impl` self type\n+//! matches some other type X -- *without* affecting `X`. That means if that if the type `X` is in\n+//! fact an unbound type variable, we want the match to be regarded as ambiguous, because depending\n+//! on what type that type variable is ultimately assigned, the match may or may not succeed.\n+//!\n+//! Note that you should be careful not to allow the output of skolemization to leak to the user in\n+//! error messages or in any other form. Skolemization is only really useful as an internal detail.\n+//!\n+//! __An important detail concerning regions.__ The skolemizer also replaces *all* regions with\n+//! 'static. The reason behind this is that, in general, we do not take region relationships into\n+//! account when making type-overloaded decisions. This is important because of the design of the\n+//! region inferencer, which is not based on unification but rather on accumulating and then\n+//! solving a set of constraints. In contrast, the type inferencer assigns a value to each type\n+//! variable only once, and it does so as soon as it can, so it is reasonable to ask what the type\n+//! inferencer knows \"so far\".\n \n use middle::ty::{mod, Ty};\n use middle::ty_fold;"}, {"sha": "3058f09a83a851d5006d846f6f1e3ddbf9aaed6f", "filename": "src/librustc/middle/typeck/infer/type_variable.rs", "status": "modified", "additions": 6, "deletions": 12, "changes": 18, "blob_url": "https://github.com/rust-lang/rust/blob/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc%2Fmiddle%2Ftypeck%2Finfer%2Ftype_variable.rs", "raw_url": "https://github.com/rust-lang/rust/raw/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc%2Fmiddle%2Ftypeck%2Finfer%2Ftype_variable.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Fmiddle%2Ftypeck%2Finfer%2Ftype_variable.rs?ref=cd5c8235c5448a7234548c772468c8d2e8f150d9", "patch": "@@ -72,12 +72,10 @@ impl<'tcx> TypeVariableTable<'tcx> {\n         self.values.get(vid.index).diverging\n     }\n \n+    /// Records that `a <: b`, `a :> b`, or `a == b`, depending on `dir`.\n+    ///\n+    /// Precondition: neither `a` nor `b` are known.\n     pub fn relate_vars(&mut self, a: ty::TyVid, dir: RelationDir, b: ty::TyVid) {\n-        /*!\n-         * Records that `a <: b`, `a :> b`, or `a == b`, depending on `dir`.\n-         *\n-         * Precondition: neither `a` nor `b` are known.\n-         */\n \n         if a != b {\n             self.relations(a).push((dir, b));\n@@ -86,19 +84,15 @@ impl<'tcx> TypeVariableTable<'tcx> {\n         }\n     }\n \n+    /// Instantiates `vid` with the type `ty` and then pushes an entry onto `stack` for each of the\n+    /// relations of `vid` to other variables. The relations will have the form `(ty, dir, vid1)`\n+    /// where `vid1` is some other variable id.\n     pub fn instantiate_and_push(\n         &mut self,\n         vid: ty::TyVid,\n         ty: Ty<'tcx>,\n         stack: &mut Vec<(Ty<'tcx>, RelationDir, ty::TyVid)>)\n     {\n-        /*!\n-         * Instantiates `vid` with the type `ty` and then pushes an\n-         * entry onto `stack` for each of the relations of `vid` to\n-         * other variables. The relations will have the form `(ty,\n-         * dir, vid1)` where `vid1` is some other variable id.\n-         */\n-\n         let old_value = {\n             let value_ptr = &mut self.values.get_mut(vid.index).value;\n             mem::replace(value_ptr, Known(ty))"}, {"sha": "38f55cc3f467b226242152c0eb67a50c94af6722", "filename": "src/librustc/middle/typeck/infer/unify.rs", "status": "modified", "additions": 12, "deletions": 32, "changes": 44, "blob_url": "https://github.com/rust-lang/rust/blob/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc%2Fmiddle%2Ftypeck%2Finfer%2Funify.rs", "raw_url": "https://github.com/rust-lang/rust/raw/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc%2Fmiddle%2Ftypeck%2Finfer%2Funify.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Fmiddle%2Ftypeck%2Finfer%2Funify.rs?ref=cd5c8235c5448a7234548c772468c8d2e8f150d9", "patch": "@@ -157,13 +157,9 @@ impl<'tcx, V:PartialEq+Clone+Repr<'tcx>, K:UnifyKey<'tcx, V>> UnificationTable<K\n         k\n     }\n \n+    /// Find the root node for `vid`. This uses the standard union-find algorithm with path\n+    /// compression: http://en.wikipedia.org/wiki/Disjoint-set_data_structure\n     pub fn get(&mut self, tcx: &ty::ctxt, vid: K) -> Node<K,V> {\n-        /*!\n-         * Find the root node for `vid`. This uses the standard\n-         * union-find algorithm with path compression:\n-         * http://en.wikipedia.org/wiki/Disjoint-set_data_structure\n-         */\n-\n         let index = vid.index();\n         let value = (*self.values.get(index)).clone();\n         match value {\n@@ -188,16 +184,13 @@ impl<'tcx, V:PartialEq+Clone+Repr<'tcx>, K:UnifyKey<'tcx, V>> UnificationTable<K\n         }\n     }\n \n+    /// Sets the value for `vid` to `new_value`. `vid` MUST be a root node! Also, we must be in the\n+    /// middle of a snapshot.\n     pub fn set(&mut self,\n                tcx: &ty::ctxt<'tcx>,\n                key: K,\n                new_value: VarValue<K,V>)\n     {\n-        /*!\n-         * Sets the value for `vid` to `new_value`. `vid` MUST be a\n-         * root node! Also, we must be in the middle of a snapshot.\n-         */\n-\n         assert!(self.is_root(&key));\n \n         debug!(\"Updating variable {} to {}\",\n@@ -207,19 +200,15 @@ impl<'tcx, V:PartialEq+Clone+Repr<'tcx>, K:UnifyKey<'tcx, V>> UnificationTable<K\n         self.values.set(key.index(), new_value);\n     }\n \n+    /// Either redirects node_a to node_b or vice versa, depending on the relative rank. Returns\n+    /// the new root and rank. You should then update the value of the new root to something\n+    /// suitable.\n     pub fn unify(&mut self,\n                  tcx: &ty::ctxt<'tcx>,\n                  node_a: &Node<K,V>,\n                  node_b: &Node<K,V>)\n                  -> (K, uint)\n     {\n-        /*!\n-         * Either redirects node_a to node_b or vice versa, depending\n-         * on the relative rank. Returns the new root and rank.  You\n-         * should then update the value of the new root to something\n-         * suitable.\n-         */\n-\n         debug!(\"unify(node_a(id={}, rank={}), node_b(id={}, rank={}))\",\n                node_a.key.repr(tcx),\n                node_a.rank,\n@@ -295,19 +284,15 @@ pub trait InferCtxtMethodsForSimplyUnifiableTypes<'tcx, V:SimplyUnifiable<'tcx>,\n impl<'a,'tcx,V:SimplyUnifiable<'tcx>,K:UnifyKey<'tcx, Option<V>>>\n     InferCtxtMethodsForSimplyUnifiableTypes<'tcx, V, K> for InferCtxt<'a, 'tcx>\n {\n+    /// Unifies two simple keys. Because simple keys do not have any subtyping relationships, if\n+    /// both keys have already been associated with a value, then those two values must be the\n+    /// same.\n     fn simple_vars(&self,\n                    a_is_expected: bool,\n                    a_id: K,\n                    b_id: K)\n                    -> ures<'tcx>\n     {\n-        /*!\n-         * Unifies two simple keys.  Because simple keys do\n-         * not have any subtyping relationships, if both keys\n-         * have already been associated with a value, then those two\n-         * values must be the same.\n-         */\n-\n         let tcx = self.tcx;\n         let table = UnifyKey::unification_table(self);\n         let node_a = table.borrow_mut().get(tcx, a_id);\n@@ -341,19 +326,14 @@ impl<'a,'tcx,V:SimplyUnifiable<'tcx>,K:UnifyKey<'tcx, Option<V>>>\n         return Ok(())\n     }\n \n+    /// Sets the value of the key `a_id` to `b`. Because simple keys do not have any subtyping\n+    /// relationships, if `a_id` already has a value, it must be the same as `b`.\n     fn simple_var_t(&self,\n                     a_is_expected: bool,\n                     a_id: K,\n                     b: V)\n                     -> ures<'tcx>\n     {\n-        /*!\n-         * Sets the value of the key `a_id` to `b`.  Because\n-         * simple keys do not have any subtyping relationships,\n-         * if `a_id` already has a value, it must be the same as\n-         * `b`.\n-         */\n-\n         let tcx = self.tcx;\n         let table = UnifyKey::unification_table(self);\n         let node_a = table.borrow_mut().get(tcx, a_id);"}, {"sha": "fa001f0434ffd2319190b184be82c0d36029a67d", "filename": "src/librustc/middle/typeck/variance.rs", "status": "modified", "additions": 181, "deletions": 187, "changes": 368, "blob_url": "https://github.com/rust-lang/rust/blob/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc%2Fmiddle%2Ftypeck%2Fvariance.rs", "raw_url": "https://github.com/rust-lang/rust/raw/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc%2Fmiddle%2Ftypeck%2Fvariance.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Fmiddle%2Ftypeck%2Fvariance.rs?ref=cd5c8235c5448a7234548c772468c8d2e8f150d9", "patch": "@@ -8,189 +8,186 @@\n // option. This file may not be copied, modified, or distributed\n // except according to those terms.\n \n-/*!\n-\n-This file infers the variance of type and lifetime parameters. The\n-algorithm is taken from Section 4 of the paper \"Taming the Wildcards:\n-Combining Definition- and Use-Site Variance\" published in PLDI'11 and\n-written by Altidor et al., and hereafter referred to as The Paper.\n-\n-This inference is explicitly designed *not* to consider the uses of\n-types within code. To determine the variance of type parameters\n-defined on type `X`, we only consider the definition of the type `X`\n-and the definitions of any types it references.\n-\n-We only infer variance for type parameters found on *types*: structs,\n-enums, and traits. We do not infer variance for type parameters found\n-on fns or impls. This is because those things are not type definitions\n-and variance doesn't really make sense in that context.\n-\n-It is worth covering what variance means in each case. For structs and\n-enums, I think it is fairly straightforward. The variance of the type\n-or lifetime parameters defines whether `T<A>` is a subtype of `T<B>`\n-(resp. `T<'a>` and `T<'b>`) based on the relationship of `A` and `B`\n-(resp. `'a` and `'b`). (FIXME #3598 -- we do not currently make use of\n-the variances we compute for type parameters.)\n-\n-### Variance on traits\n-\n-The meaning of variance for trait parameters is more subtle and worth\n-expanding upon. There are in fact two uses of the variance values we\n-compute.\n-\n-#### Trait variance and object types\n-\n-The first is for object types. Just as with structs and enums, we can\n-decide the subtyping relationship between two object types `&Trait<A>`\n-and `&Trait<B>` based on the relationship of `A` and `B`. Note that\n-for object types we ignore the `Self` type parameter -- it is unknown,\n-and the nature of dynamic dispatch ensures that we will always call a\n-function that is expected the appropriate `Self` type. However, we\n-must be careful with the other type parameters, or else we could end\n-up calling a function that is expecting one type but provided another.\n-\n-To see what I mean, consider a trait like so:\n-\n-    trait ConvertTo<A> {\n-        fn convertTo(&self) -> A;\n-    }\n-\n-Intuitively, If we had one object `O=&ConvertTo<Object>` and another\n-`S=&ConvertTo<String>`, then `S <: O` because `String <: Object`\n-(presuming Java-like \"string\" and \"object\" types, my go to examples\n-for subtyping). The actual algorithm would be to compare the\n-(explicit) type parameters pairwise respecting their variance: here,\n-the type parameter A is covariant (it appears only in a return\n-position), and hence we require that `String <: Object`.\n-\n-You'll note though that we did not consider the binding for the\n-(implicit) `Self` type parameter: in fact, it is unknown, so that's\n-good. The reason we can ignore that parameter is precisely because we\n-don't need to know its value until a call occurs, and at that time (as\n-you said) the dynamic nature of virtual dispatch means the code we run\n-will be correct for whatever value `Self` happens to be bound to for\n-the particular object whose method we called. `Self` is thus different\n-from `A`, because the caller requires that `A` be known in order to\n-know the return type of the method `convertTo()`. (As an aside, we\n-have rules preventing methods where `Self` appears outside of the\n-receiver position from being called via an object.)\n-\n-#### Trait variance and vtable resolution\n-\n-But traits aren't only used with objects. They're also used when\n-deciding whether a given impl satisfies a given trait bound. To set the\n-scene here, imagine I had a function:\n-\n-    fn convertAll<A,T:ConvertTo<A>>(v: &[T]) {\n-        ...\n-    }\n-\n-Now imagine that I have an implementation of `ConvertTo` for `Object`:\n-\n-    impl ConvertTo<int> for Object { ... }\n-\n-And I want to call `convertAll` on an array of strings. Suppose\n-further that for whatever reason I specifically supply the value of\n-`String` for the type parameter `T`:\n-\n-    let mut vector = ~[\"string\", ...];\n-    convertAll::<int, String>(v);\n-\n-Is this legal? To put another way, can we apply the `impl` for\n-`Object` to the type `String`? The answer is yes, but to see why\n-we have to expand out what will happen:\n-\n-- `convertAll` will create a pointer to one of the entries in the\n-  vector, which will have type `&String`\n-- It will then call the impl of `convertTo()` that is intended\n-  for use with objects. This has the type:\n-\n-      fn(self: &Object) -> int\n-\n-  It is ok to provide a value for `self` of type `&String` because\n-  `&String <: &Object`.\n-\n-OK, so intuitively we want this to be legal, so let's bring this back\n-to variance and see whether we are computing the correct result. We\n-must first figure out how to phrase the question \"is an impl for\n-`Object,int` usable where an impl for `String,int` is expected?\"\n-\n-Maybe it's helpful to think of a dictionary-passing implementation of\n-type classes. In that case, `convertAll()` takes an implicit parameter\n-representing the impl. In short, we *have* an impl of type:\n-\n-    V_O = ConvertTo<int> for Object\n-\n-and the function prototype expects an impl of type:\n-\n-    V_S = ConvertTo<int> for String\n-\n-As with any argument, this is legal if the type of the value given\n-(`V_O`) is a subtype of the type expected (`V_S`). So is `V_O <: V_S`?\n-The answer will depend on the variance of the various parameters. In\n-this case, because the `Self` parameter is contravariant and `A` is\n-covariant, it means that:\n+//! This file infers the variance of type and lifetime parameters. The\n+//! algorithm is taken from Section 4 of the paper \"Taming the Wildcards:\n+//! Combining Definition- and Use-Site Variance\" published in PLDI'11 and\n+//! written by Altidor et al., and hereafter referred to as The Paper.\n+//!\n+//! This inference is explicitly designed *not* to consider the uses of\n+//! types within code. To determine the variance of type parameters\n+//! defined on type `X`, we only consider the definition of the type `X`\n+//! and the definitions of any types it references.\n+//!\n+//! We only infer variance for type parameters found on *types*: structs,\n+//! enums, and traits. We do not infer variance for type parameters found\n+//! on fns or impls. This is because those things are not type definitions\n+//! and variance doesn't really make sense in that context.\n+//!\n+//! It is worth covering what variance means in each case. For structs and\n+//! enums, I think it is fairly straightforward. The variance of the type\n+//! or lifetime parameters defines whether `T<A>` is a subtype of `T<B>`\n+//! (resp. `T<'a>` and `T<'b>`) based on the relationship of `A` and `B`\n+//! (resp. `'a` and `'b`). (FIXME #3598 -- we do not currently make use of\n+//! the variances we compute for type parameters.)\n+//!\n+//! ### Variance on traits\n+//!\n+//! The meaning of variance for trait parameters is more subtle and worth\n+//! expanding upon. There are in fact two uses of the variance values we\n+//! compute.\n+//!\n+//! #### Trait variance and object types\n+//!\n+//! The first is for object types. Just as with structs and enums, we can\n+//! decide the subtyping relationship between two object types `&Trait<A>`\n+//! and `&Trait<B>` based on the relationship of `A` and `B`. Note that\n+//! for object types we ignore the `Self` type parameter -- it is unknown,\n+//! and the nature of dynamic dispatch ensures that we will always call a\n+//! function that is expected the appropriate `Self` type. However, we\n+//! must be careful with the other type parameters, or else we could end\n+//! up calling a function that is expecting one type but provided another.\n+//!\n+//! To see what I mean, consider a trait like so:\n+//!\n+//!     trait ConvertTo<A> {\n+//!         fn convertTo(&self) -> A;\n+//!     }\n+//!\n+//! Intuitively, If we had one object `O=&ConvertTo<Object>` and another\n+//! `S=&ConvertTo<String>`, then `S <: O` because `String <: Object`\n+//! (presuming Java-like \"string\" and \"object\" types, my go to examples\n+//! for subtyping). The actual algorithm would be to compare the\n+//! (explicit) type parameters pairwise respecting their variance: here,\n+//! the type parameter A is covariant (it appears only in a return\n+//! position), and hence we require that `String <: Object`.\n+//!\n+//! You'll note though that we did not consider the binding for the\n+//! (implicit) `Self` type parameter: in fact, it is unknown, so that's\n+//! good. The reason we can ignore that parameter is precisely because we\n+//! don't need to know its value until a call occurs, and at that time (as\n+//! you said) the dynamic nature of virtual dispatch means the code we run\n+//! will be correct for whatever value `Self` happens to be bound to for\n+//! the particular object whose method we called. `Self` is thus different\n+//! from `A`, because the caller requires that `A` be known in order to\n+//! know the return type of the method `convertTo()`. (As an aside, we\n+//! have rules preventing methods where `Self` appears outside of the\n+//! receiver position from being called via an object.)\n+//!\n+//! #### Trait variance and vtable resolution\n+//!\n+//! But traits aren't only used with objects. They're also used when\n+//! deciding whether a given impl satisfies a given trait bound. To set the\n+//! scene here, imagine I had a function:\n+//!\n+//!     fn convertAll<A,T:ConvertTo<A>>(v: &[T]) {\n+//!         ...\n+//!     }\n+//!\n+//! Now imagine that I have an implementation of `ConvertTo` for `Object`:\n+//!\n+//!     impl ConvertTo<int> for Object { ... }\n+//!\n+//! And I want to call `convertAll` on an array of strings. Suppose\n+//! further that for whatever reason I specifically supply the value of\n+//! `String` for the type parameter `T`:\n+//!\n+//!     let mut vector = ~[\"string\", ...];\n+//!     convertAll::<int, String>(v);\n+//!\n+//! Is this legal? To put another way, can we apply the `impl` for\n+//! `Object` to the type `String`? The answer is yes, but to see why\n+//! we have to expand out what will happen:\n+//!\n+//! - `convertAll` will create a pointer to one of the entries in the\n+//!   vector, which will have type `&String`\n+//! - It will then call the impl of `convertTo()` that is intended\n+//!   for use with objects. This has the type:\n+//!\n+//!       fn(self: &Object) -> int\n+//!\n+//!   It is ok to provide a value for `self` of type `&String` because\n+//!   `&String <: &Object`.\n+//!\n+//! OK, so intuitively we want this to be legal, so let's bring this back\n+//! to variance and see whether we are computing the correct result. We\n+//! must first figure out how to phrase the question \"is an impl for\n+//! `Object,int` usable where an impl for `String,int` is expected?\"\n+//!\n+//! Maybe it's helpful to think of a dictionary-passing implementation of\n+//! type classes. In that case, `convertAll()` takes an implicit parameter\n+//! representing the impl. In short, we *have* an impl of type:\n+//!\n+//!     V_O = ConvertTo<int> for Object\n+//!\n+//! and the function prototype expects an impl of type:\n+//!\n+//!     V_S = ConvertTo<int> for String\n+//!\n+//! As with any argument, this is legal if the type of the value given\n+//! (`V_O`) is a subtype of the type expected (`V_S`). So is `V_O <: V_S`?\n+//! The answer will depend on the variance of the various parameters. In\n+//! this case, because the `Self` parameter is contravariant and `A` is\n+//! covariant, it means that:\n+//!\n+//!     V_O <: V_S iff\n+//!         int <: int\n+//!         String <: Object\n+//!\n+//! These conditions are satisfied and so we are happy.\n+//!\n+//! ### The algorithm\n+//!\n+//! The basic idea is quite straightforward. We iterate over the types\n+//! defined and, for each use of a type parameter X, accumulate a\n+//! constraint indicating that the variance of X must be valid for the\n+//! variance of that use site. We then iteratively refine the variance of\n+//! X until all constraints are met. There is *always* a sol'n, because at\n+//! the limit we can declare all type parameters to be invariant and all\n+//! constraints will be satisfied.\n+//!\n+//! As a simple example, consider:\n+//!\n+//!     enum Option<A> { Some(A), None }\n+//!     enum OptionalFn<B> { Some(|B|), None }\n+//!     enum OptionalMap<C> { Some(|C| -> C), None }\n+//!\n+//! Here, we will generate the constraints:\n+//!\n+//!     1. V(A) <= +\n+//!     2. V(B) <= -\n+//!     3. V(C) <= +\n+//!     4. V(C) <= -\n+//!\n+//! These indicate that (1) the variance of A must be at most covariant;\n+//! (2) the variance of B must be at most contravariant; and (3, 4) the\n+//! variance of C must be at most covariant *and* contravariant. All of these\n+//! results are based on a variance lattice defined as follows:\n+//!\n+//!       *      Top (bivariant)\n+//!    -     +\n+//!       o      Bottom (invariant)\n+//!\n+//! Based on this lattice, the solution V(A)=+, V(B)=-, V(C)=o is the\n+//! optimal solution. Note that there is always a naive solution which\n+//! just declares all variables to be invariant.\n+//!\n+//! You may be wondering why fixed-point iteration is required. The reason\n+//! is that the variance of a use site may itself be a function of the\n+//! variance of other type parameters. In full generality, our constraints\n+//! take the form:\n+//!\n+//!     V(X) <= Term\n+//!     Term := + | - | * | o | V(X) | Term x Term\n+//!\n+//! Here the notation V(X) indicates the variance of a type/region\n+//! parameter `X` with respect to its defining class. `Term x Term`\n+//! represents the \"variance transform\" as defined in the paper:\n+//!\n+//!   If the variance of a type variable `X` in type expression `E` is `V2`\n+//!   and the definition-site variance of the [corresponding] type parameter\n+//!   of a class `C` is `V1`, then the variance of `X` in the type expression\n+//!   `C<E>` is `V3 = V1.xform(V2)`.\n \n-    V_O <: V_S iff\n-        int <: int\n-        String <: Object\n-\n-These conditions are satisfied and so we are happy.\n-\n-### The algorithm\n-\n-The basic idea is quite straightforward. We iterate over the types\n-defined and, for each use of a type parameter X, accumulate a\n-constraint indicating that the variance of X must be valid for the\n-variance of that use site. We then iteratively refine the variance of\n-X until all constraints are met. There is *always* a sol'n, because at\n-the limit we can declare all type parameters to be invariant and all\n-constraints will be satisfied.\n-\n-As a simple example, consider:\n-\n-    enum Option<A> { Some(A), None }\n-    enum OptionalFn<B> { Some(|B|), None }\n-    enum OptionalMap<C> { Some(|C| -> C), None }\n-\n-Here, we will generate the constraints:\n-\n-    1. V(A) <= +\n-    2. V(B) <= -\n-    3. V(C) <= +\n-    4. V(C) <= -\n-\n-These indicate that (1) the variance of A must be at most covariant;\n-(2) the variance of B must be at most contravariant; and (3, 4) the\n-variance of C must be at most covariant *and* contravariant. All of these\n-results are based on a variance lattice defined as follows:\n-\n-      *      Top (bivariant)\n-   -     +\n-      o      Bottom (invariant)\n-\n-Based on this lattice, the solution V(A)=+, V(B)=-, V(C)=o is the\n-optimal solution. Note that there is always a naive solution which\n-just declares all variables to be invariant.\n-\n-You may be wondering why fixed-point iteration is required. The reason\n-is that the variance of a use site may itself be a function of the\n-variance of other type parameters. In full generality, our constraints\n-take the form:\n-\n-    V(X) <= Term\n-    Term := + | - | * | o | V(X) | Term x Term\n-\n-Here the notation V(X) indicates the variance of a type/region\n-parameter `X` with respect to its defining class. `Term x Term`\n-represents the \"variance transform\" as defined in the paper:\n-\n-  If the variance of a type variable `X` in type expression `E` is `V2`\n-  and the definition-site variance of the [corresponding] type parameter\n-  of a class `C` is `V1`, then the variance of `X` in the type expression\n-  `C<E>` is `V3 = V1.xform(V2)`.\n-\n-*/\n use self::VarianceTerm::*;\n use self::ParamKind::*;\n \n@@ -632,18 +629,15 @@ impl<'a, 'tcx> ConstraintContext<'a, 'tcx> {\n         return result;\n     }\n \n+    /// Returns a variance term representing the declared variance of the type/region parameter\n+    /// with the given id.\n     fn declared_variance(&self,\n                          param_def_id: ast::DefId,\n                          item_def_id: ast::DefId,\n                          kind: ParamKind,\n                          space: ParamSpace,\n                          index: uint)\n                          -> VarianceTermPtr<'a> {\n-        /*!\n-         * Returns a variance term representing the declared variance of\n-         * the type/region parameter with the given id.\n-         */\n-\n         assert_eq!(param_def_id.krate, item_def_id.krate);\n \n         if self.invariant_lang_items[kind as uint] == Some(item_def_id) {"}, {"sha": "8dd60880cdd56022773879ed502dcce3bc4ed8f7", "filename": "src/librustc/plugin/mod.rs", "status": "modified", "additions": 46, "deletions": 48, "changes": 94, "blob_url": "https://github.com/rust-lang/rust/blob/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc%2Fplugin%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc%2Fplugin%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Fplugin%2Fmod.rs?ref=cd5c8235c5448a7234548c772468c8d2e8f150d9", "patch": "@@ -8,54 +8,52 @@\n // option. This file may not be copied, modified, or distributed\n // except according to those terms.\n \n-/*!\n- * Infrastructure for compiler plugins.\n- *\n- * Plugins are Rust libraries which extend the behavior of `rustc`\n- * in various ways.\n- *\n- * Plugin authors will use the `Registry` type re-exported by\n- * this module, along with its methods.  The rest of the module\n- * is for use by `rustc` itself.\n- *\n- * To define a plugin, build a dylib crate with a\n- * `#[plugin_registrar]` function:\n- *\n- * ```rust,ignore\n- * #![crate_name = \"myplugin\"]\n- * #![crate_type = \"dylib\"]\n- * #![feature(plugin_registrar)]\n- *\n- * extern crate rustc;\n- *\n- * use rustc::plugin::Registry;\n- *\n- * #[plugin_registrar]\n- * pub fn plugin_registrar(reg: &mut Registry) {\n- *     reg.register_macro(\"mymacro\", expand_mymacro);\n- * }\n- *\n- * fn expand_mymacro(...) {  // details elided\n- * ```\n- *\n- * WARNING: We currently don't check that the registrar function\n- * has the appropriate type!\n- *\n- * To use a plugin while compiling another crate:\n- *\n- * ```rust\n- * #![feature(phase)]\n- *\n- * #[phase(plugin)]\n- * extern crate myplugin;\n- * ```\n- *\n- * If you also need the plugin crate available at runtime, use\n- * `phase(plugin, link)`.\n- *\n- * See [the compiler plugin guide](../../guide-plugin.html)\n- * for more examples.\n- */\n+//! Infrastructure for compiler plugins.\n+//!\n+//! Plugins are Rust libraries which extend the behavior of `rustc`\n+//! in various ways.\n+//!\n+//! Plugin authors will use the `Registry` type re-exported by\n+//! this module, along with its methods.  The rest of the module\n+//! is for use by `rustc` itself.\n+//!\n+//! To define a plugin, build a dylib crate with a\n+//! `#[plugin_registrar]` function:\n+//!\n+//! ```rust,ignore\n+//! #![crate_name = \"myplugin\"]\n+//! #![crate_type = \"dylib\"]\n+//! #![feature(plugin_registrar)]\n+//!\n+//! extern crate rustc;\n+//!\n+//! use rustc::plugin::Registry;\n+//!\n+//! #[plugin_registrar]\n+//! pub fn plugin_registrar(reg: &mut Registry) {\n+//!     reg.register_macro(\"mymacro\", expand_mymacro);\n+//! }\n+//!\n+//! fn expand_mymacro(...) {  // details elided\n+//! ```\n+//!\n+//! WARNING: We currently don't check that the registrar function\n+//! has the appropriate type!\n+//!\n+//! To use a plugin while compiling another crate:\n+//!\n+//! ```rust\n+//! #![feature(phase)]\n+//!\n+//! #[phase(plugin)]\n+//! extern crate myplugin;\n+//! ```\n+//!\n+//! If you also need the plugin crate available at runtime, use\n+//! `phase(plugin, link)`.\n+//!\n+//! See [the compiler plugin guide](../../guide-plugin.html)\n+//! for more examples.\n \n pub use self::registry::Registry;\n "}, {"sha": "ea252d9fd205c99d0abac30fdc05648ee4f9c24f", "filename": "src/librustc/util/common.rs", "status": "modified", "additions": 8, "deletions": 12, "changes": 20, "blob_url": "https://github.com/rust-lang/rust/blob/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc%2Futil%2Fcommon.rs", "raw_url": "https://github.com/rust-lang/rust/raw/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc%2Futil%2Fcommon.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Futil%2Fcommon.rs?ref=cd5c8235c5448a7234548c772468c8d2e8f150d9", "patch": "@@ -122,24 +122,20 @@ pub fn block_query(b: &ast::Block, p: |&ast::Expr| -> bool) -> bool {\n     return v.flag;\n }\n \n-// K: Eq + Hash<S>, V, S, H: Hasher<S>\n+/// K: Eq + Hash<S>, V, S, H: Hasher<S>\n+///\n+/// Determines whether there exists a path from `source` to `destination`.  The graph is defined by\n+/// the `edges_map`, which maps from a node `S` to a list of its adjacent nodes `T`.\n+///\n+/// Efficiency note: This is implemented in an inefficient way because it is typically invoked on\n+/// very small graphs. If the graphs become larger, a more efficient graph representation and\n+/// algorithm would probably be advised.\n pub fn can_reach<S,H:Hasher<S>,T:Eq+Clone+Hash<S>>(\n     edges_map: &HashMap<T,Vec<T>,H>,\n     source: T,\n     destination: T)\n     -> bool\n {\n-    /*!\n-     * Determines whether there exists a path from `source` to\n-     * `destination`.  The graph is defined by the `edges_map`, which\n-     * maps from a node `S` to a list of its adjacent nodes `T`.\n-     *\n-     * Efficiency note: This is implemented in an inefficient way\n-     * because it is typically invoked on very small graphs. If the graphs\n-     * become larger, a more efficient graph representation and algorithm\n-     * would probably be advised.\n-     */\n-\n     if source == destination {\n         return true;\n     }"}, {"sha": "b739a97f734bea228db1d3f76fa2f6ca96a43da3", "filename": "src/librustc/util/ppaux.rs", "status": "modified", "additions": 2, "deletions": 5, "changes": 7, "blob_url": "https://github.com/rust-lang/rust/blob/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc%2Futil%2Fppaux.rs", "raw_url": "https://github.com/rust-lang/rust/raw/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc%2Futil%2Fppaux.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Futil%2Fppaux.rs?ref=cd5c8235c5448a7234548c772468c8d2e8f150d9", "patch": "@@ -65,12 +65,9 @@ pub fn note_and_explain_region(cx: &ctxt,\n     }\n }\n \n+/// When a free region is associated with `item`, how should we describe the item in the error\n+/// message.\n fn item_scope_tag(item: &ast::Item) -> &'static str {\n-    /*!\n-     * When a free region is associated with `item`, how should we describe\n-     * the item in the error message.\n-     */\n-\n     match item.node {\n         ast::ItemImpl(..) => \"impl\",\n         ast::ItemStruct(..) => \"struct\","}, {"sha": "64e67a1f4bf753d7ddc5c45f4dd9ae9c8371a9ef", "filename": "src/librustc/util/snapshot_vec.rs", "status": "modified", "additions": 15, "deletions": 27, "changes": 42, "blob_url": "https://github.com/rust-lang/rust/blob/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc%2Futil%2Fsnapshot_vec.rs", "raw_url": "https://github.com/rust-lang/rust/raw/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc%2Futil%2Fsnapshot_vec.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Futil%2Fsnapshot_vec.rs?ref=cd5c8235c5448a7234548c772468c8d2e8f150d9", "patch": "@@ -8,21 +8,16 @@\n // option. This file may not be copied, modified, or distributed\n // except according to those terms.\n \n-/*!\n- * A utility class for implementing \"snapshottable\" things; a\n- * snapshottable data structure permits you to take a snapshot (via\n- * `start_snapshot`) and then, after making some changes, elect either\n- * to rollback to the start of the snapshot or commit those changes.\n- *\n- * This vector is intended to be used as part of an abstraction, not\n- * serve as a complete abstraction on its own. As such, while it will\n- * roll back most changes on its own, it also supports a `get_mut`\n- * operation that gives you an abitrary mutable pointer into the\n- * vector. To ensure that any changes you make this with this pointer\n- * are rolled back, you must invoke `record` to record any changes you\n- * make and also supplying a delegate capable of reversing those\n- * changes.\n- */\n+//! A utility class for implementing \"snapshottable\" things; a snapshottable data structure permits\n+//! you to take a snapshot (via `start_snapshot`) and then, after making some changes, elect either\n+//! to rollback to the start of the snapshot or commit those changes.\n+//!\n+//! This vector is intended to be used as part of an abstraction, not serve as a complete\n+//! abstraction on its own. As such, while it will roll back most changes on its own, it also\n+//! supports a `get_mut` operation that gives you an abitrary mutable pointer into the vector. To\n+//! ensure that any changes you make this with this pointer are rolled back, you must invoke\n+//! `record` to record any changes you make and also supplying a delegate capable of reversing\n+//! those changes.\n use self::UndoLog::*;\n \n use std::kinds::marker;\n@@ -98,23 +93,16 @@ impl<T,U,D:SnapshotVecDelegate<T,U>> SnapshotVec<T,U,D> {\n         &self.values[index]\n     }\n \n+    /// Returns a mutable pointer into the vec; whatever changes you make here cannot be undone\n+    /// automatically, so you should be sure call `record()` with some sort of suitable undo\n+    /// action.\n     pub fn get_mut<'a>(&'a mut self, index: uint) -> &'a mut T {\n-        /*!\n-         * Returns a mutable pointer into the vec; whatever changes\n-         * you make here cannot be undone automatically, so you should\n-         * be sure call `record()` with some sort of suitable undo\n-         * action.\n-         */\n-\n         &mut self.values[index]\n     }\n \n+    /// Updates the element at the given index. The old value will saved (and perhaps restored) if\n+    /// a snapshot is active.\n     pub fn set(&mut self, index: uint, new_elem: T) {\n-        /*!\n-         * Updates the element at the given index. The old value will\n-         * saved (and perhaps restored) if a snapshot is active.\n-         */\n-\n         let old_elem = mem::replace(&mut self.values[index], new_elem);\n         if self.in_snapshot() {\n             self.undo_log.push(SetElem(index, old_elem));"}, {"sha": "4186f479fcce653e985524237765d325b42dac32", "filename": "src/librustc_trans/lib.rs", "status": "modified", "additions": 5, "deletions": 9, "changes": 14, "blob_url": "https://github.com/rust-lang/rust/blob/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc_trans%2Flib.rs", "raw_url": "https://github.com/rust-lang/rust/raw/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc_trans%2Flib.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_trans%2Flib.rs?ref=cd5c8235c5448a7234548c772468c8d2e8f150d9", "patch": "@@ -8,15 +8,11 @@\n // option. This file may not be copied, modified, or distributed\n // except according to those terms.\n \n-/*!\n-\n-The Rust compiler.\n-\n-# Note\n-\n-This API is completely unstable and subject to change.\n-\n-*/\n+//! The Rust compiler.\n+//!\n+//! # Note\n+//!\n+//! This API is completely unstable and subject to change.\n \n #![crate_name = \"rustc_trans\"]\n #![experimental]"}, {"sha": "41fbe855769331291f6fb5b20845a997e633a70b", "filename": "src/librustc_trans/test.rs", "status": "modified", "additions": 8, "deletions": 22, "changes": 30, "blob_url": "https://github.com/rust-lang/rust/blob/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc_trans%2Ftest.rs", "raw_url": "https://github.com/rust-lang/rust/raw/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc_trans%2Ftest.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_trans%2Ftest.rs?ref=cd5c8235c5448a7234548c772468c8d2e8f150d9", "patch": "@@ -8,11 +8,7 @@\n // option. This file may not be copied, modified, or distributed\n // except according to those terms.\n \n-/*!\n-\n-# Standalone Tests for the Inference Module\n-\n-*/\n+//! # Standalone Tests for the Inference Module\n \n use driver::diagnostic;\n use driver::diagnostic::Emitter;\n@@ -537,12 +533,10 @@ fn glb_bound_static() {\n     })\n }\n \n+/// Test substituting a bound region into a function, which introduces another level of binding.\n+/// This requires adjusting the Debruijn index.\n #[test]\n fn subst_ty_renumber_bound() {\n-    /*!\n-     * Test substituting a bound region into a function, which introduces another\n-     * level of binding. This requires adjusting the Debruijn index.\n-     */\n \n     test_env(EMPTY_SOURCE_STR, errors(&[]), |env| {\n         // Situation:\n@@ -575,13 +569,10 @@ fn subst_ty_renumber_bound() {\n     })\n }\n \n+/// Test substituting a bound region into a function, which introduces another level of binding.\n+/// This requires adjusting the Debruijn index.\n #[test]\n fn subst_ty_renumber_some_bounds() {\n-    /*!\n-     * Test substituting a bound region into a function, which introduces another\n-     * level of binding. This requires adjusting the Debruijn index.\n-     */\n-\n     test_env(EMPTY_SOURCE_STR, errors(&[]), |env| {\n         // Situation:\n         // Theta = [A -> &'a foo]\n@@ -615,12 +606,9 @@ fn subst_ty_renumber_some_bounds() {\n     })\n }\n \n+/// Test that we correctly compute whether a type has escaping regions or not.\n #[test]\n fn escaping() {\n-    /*!\n-     * Test that we correctly compute whether a type has escaping\n-     * regions or not.\n-     */\n \n     test_env(EMPTY_SOURCE_STR, errors(&[]), |env| {\n         // Situation:\n@@ -658,12 +646,10 @@ fn escaping() {\n     })\n }\n \n+/// Test applying a substitution where the value being substituted for an early-bound region is a\n+/// late-bound region.\n #[test]\n fn subst_region_renumber_region() {\n-    /*!\n-     * Test applying a substitution where the value being substituted\n-     * for an early-bound region is a late-bound region.\n-     */\n \n     test_env(EMPTY_SOURCE_STR, errors(&[]), |env| {\n         let re_bound1 = env.re_late_bound_with_debruijn(1, ty::DebruijnIndex::new(1));"}, {"sha": "d83eeadc7b96f6a47b8343940866c808274ab080", "filename": "src/librustc_trans/trans/_match.rs", "status": "modified", "additions": 200, "deletions": 219, "changes": 419, "blob_url": "https://github.com/rust-lang/rust/blob/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc_trans%2Ftrans%2F_match.rs", "raw_url": "https://github.com/rust-lang/rust/raw/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc_trans%2Ftrans%2F_match.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_trans%2Ftrans%2F_match.rs?ref=cd5c8235c5448a7234548c772468c8d2e8f150d9", "patch": "@@ -8,183 +8,179 @@\n // option. This file may not be copied, modified, or distributed\n // except according to those terms.\n \n-/*!\n- *\n- * # Compilation of match statements\n- *\n- * I will endeavor to explain the code as best I can.  I have only a loose\n- * understanding of some parts of it.\n- *\n- * ## Matching\n- *\n- * The basic state of the code is maintained in an array `m` of `Match`\n- * objects.  Each `Match` describes some list of patterns, all of which must\n- * match against the current list of values.  If those patterns match, then\n- * the arm listed in the match is the correct arm.  A given arm may have\n- * multiple corresponding match entries, one for each alternative that\n- * remains.  As we proceed these sets of matches are adjusted by the various\n- * `enter_XXX()` functions, each of which adjusts the set of options given\n- * some information about the value which has been matched.\n- *\n- * So, initially, there is one value and N matches, each of which have one\n- * constituent pattern.  N here is usually the number of arms but may be\n- * greater, if some arms have multiple alternatives.  For example, here:\n- *\n- *     enum Foo { A, B(int), C(uint, uint) }\n- *     match foo {\n- *         A => ...,\n- *         B(x) => ...,\n- *         C(1u, 2) => ...,\n- *         C(_) => ...\n- *     }\n- *\n- * The value would be `foo`.  There would be four matches, each of which\n- * contains one pattern (and, in one case, a guard).  We could collect the\n- * various options and then compile the code for the case where `foo` is an\n- * `A`, a `B`, and a `C`.  When we generate the code for `C`, we would (1)\n- * drop the two matches that do not match a `C` and (2) expand the other two\n- * into two patterns each.  In the first case, the two patterns would be `1u`\n- * and `2`, and the in the second case the _ pattern would be expanded into\n- * `_` and `_`.  The two values are of course the arguments to `C`.\n- *\n- * Here is a quick guide to the various functions:\n- *\n- * - `compile_submatch()`: The main workhouse.  It takes a list of values and\n- *   a list of matches and finds the various possibilities that could occur.\n- *\n- * - `enter_XXX()`: modifies the list of matches based on some information\n- *   about the value that has been matched.  For example,\n- *   `enter_rec_or_struct()` adjusts the values given that a record or struct\n- *   has been matched.  This is an infallible pattern, so *all* of the matches\n- *   must be either wildcards or record/struct patterns.  `enter_opt()`\n- *   handles the fallible cases, and it is correspondingly more complex.\n- *\n- * ## Bindings\n- *\n- * We store information about the bound variables for each arm as part of the\n- * per-arm `ArmData` struct.  There is a mapping from identifiers to\n- * `BindingInfo` structs.  These structs contain the mode/id/type of the\n- * binding, but they also contain an LLVM value which points at an alloca\n- * called `llmatch`. For by value bindings that are Copy, we also create\n- * an extra alloca that we copy the matched value to so that any changes\n- * we do to our copy is not reflected in the original and vice-versa.\n- * We don't do this if it's a move since the original value can't be used\n- * and thus allowing us to cheat in not creating an extra alloca.\n- *\n- * The `llmatch` binding always stores a pointer into the value being matched\n- * which points at the data for the binding.  If the value being matched has\n- * type `T`, then, `llmatch` will point at an alloca of type `T*` (and hence\n- * `llmatch` has type `T**`).  So, if you have a pattern like:\n- *\n- *    let a: A = ...;\n- *    let b: B = ...;\n- *    match (a, b) { (ref c, d) => { ... } }\n- *\n- * For `c` and `d`, we would generate allocas of type `C*` and `D*`\n- * respectively.  These are called the `llmatch`.  As we match, when we come\n- * up against an identifier, we store the current pointer into the\n- * corresponding alloca.\n- *\n- * Once a pattern is completely matched, and assuming that there is no guard\n- * pattern, we will branch to a block that leads to the body itself.  For any\n- * by-value bindings, this block will first load the ptr from `llmatch` (the\n- * one of type `D*`) and then load a second time to get the actual value (the\n- * one of type `D`). For by ref bindings, the value of the local variable is\n- * simply the first alloca.\n- *\n- * So, for the example above, we would generate a setup kind of like this:\n- *\n- *        +-------+\n- *        | Entry |\n- *        +-------+\n- *            |\n- *        +--------------------------------------------+\n- *        | llmatch_c = (addr of first half of tuple)  |\n- *        | llmatch_d = (addr of second half of tuple) |\n- *        +--------------------------------------------+\n- *            |\n- *        +--------------------------------------+\n- *        | *llbinding_d = **llmatch_d           |\n- *        +--------------------------------------+\n- *\n- * If there is a guard, the situation is slightly different, because we must\n- * execute the guard code.  Moreover, we need to do so once for each of the\n- * alternatives that lead to the arm, because if the guard fails, they may\n- * have different points from which to continue the search. Therefore, in that\n- * case, we generate code that looks more like:\n- *\n- *        +-------+\n- *        | Entry |\n- *        +-------+\n- *            |\n- *        +-------------------------------------------+\n- *        | llmatch_c = (addr of first half of tuple) |\n- *        | llmatch_d = (addr of first half of tuple) |\n- *        +-------------------------------------------+\n- *            |\n- *        +-------------------------------------------------+\n- *        | *llbinding_d = **llmatch_d                      |\n- *        | check condition                                 |\n- *        | if false { goto next case }                     |\n- *        | if true { goto body }                           |\n- *        +-------------------------------------------------+\n- *\n- * The handling for the cleanups is a bit... sensitive.  Basically, the body\n- * is the one that invokes `add_clean()` for each binding.  During the guard\n- * evaluation, we add temporary cleanups and revoke them after the guard is\n- * evaluated (it could fail, after all). Note that guards and moves are\n- * just plain incompatible.\n- *\n- * Some relevant helper functions that manage bindings:\n- * - `create_bindings_map()`\n- * - `insert_lllocals()`\n- *\n- *\n- * ## Notes on vector pattern matching.\n- *\n- * Vector pattern matching is surprisingly tricky. The problem is that\n- * the structure of the vector isn't fully known, and slice matches\n- * can be done on subparts of it.\n- *\n- * The way that vector pattern matches are dealt with, then, is as\n- * follows. First, we make the actual condition associated with a\n- * vector pattern simply a vector length comparison. So the pattern\n- * [1, .. x] gets the condition \"vec len >= 1\", and the pattern\n- * [.. x] gets the condition \"vec len >= 0\". The problem here is that\n- * having the condition \"vec len >= 1\" hold clearly does not mean that\n- * only a pattern that has exactly that condition will match. This\n- * means that it may well be the case that a condition holds, but none\n- * of the patterns matching that condition match; to deal with this,\n- * when doing vector length matches, we have match failures proceed to\n- * the next condition to check.\n- *\n- * There are a couple more subtleties to deal with. While the \"actual\"\n- * condition associated with vector length tests is simply a test on\n- * the vector length, the actual vec_len Opt entry contains more\n- * information used to restrict which matches are associated with it.\n- * So that all matches in a submatch are matching against the same\n- * values from inside the vector, they are split up by how many\n- * elements they match at the front and at the back of the vector. In\n- * order to make sure that arms are properly checked in order, even\n- * with the overmatching conditions, each vec_len Opt entry is\n- * associated with a range of matches.\n- * Consider the following:\n- *\n- *   match &[1, 2, 3] {\n- *       [1, 1, .. _] => 0,\n- *       [1, 2, 2, .. _] => 1,\n- *       [1, 2, 3, .. _] => 2,\n- *       [1, 2, .. _] => 3,\n- *       _ => 4\n- *   }\n- * The proper arm to match is arm 2, but arms 0 and 3 both have the\n- * condition \"len >= 2\". If arm 3 was lumped in with arm 0, then the\n- * wrong branch would be taken. Instead, vec_len Opts are associated\n- * with a contiguous range of matches that have the same \"shape\".\n- * This is sort of ugly and requires a bunch of special handling of\n- * vec_len options.\n- *\n- */\n+//! # Compilation of match statements\n+//!\n+//! I will endeavor to explain the code as best I can.  I have only a loose\n+//! understanding of some parts of it.\n+//!\n+//! ## Matching\n+//!\n+//! The basic state of the code is maintained in an array `m` of `Match`\n+//! objects.  Each `Match` describes some list of patterns, all of which must\n+//! match against the current list of values.  If those patterns match, then\n+//! the arm listed in the match is the correct arm.  A given arm may have\n+//! multiple corresponding match entries, one for each alternative that\n+//! remains.  As we proceed these sets of matches are adjusted by the various\n+//! `enter_XXX()` functions, each of which adjusts the set of options given\n+//! some information about the value which has been matched.\n+//!\n+//! So, initially, there is one value and N matches, each of which have one\n+//! constituent pattern.  N here is usually the number of arms but may be\n+//! greater, if some arms have multiple alternatives.  For example, here:\n+//!\n+//!     enum Foo { A, B(int), C(uint, uint) }\n+//!     match foo {\n+//!         A => ...,\n+//!         B(x) => ...,\n+//!         C(1u, 2) => ...,\n+//!         C(_) => ...\n+//!     }\n+//!\n+//! The value would be `foo`.  There would be four matches, each of which\n+//! contains one pattern (and, in one case, a guard).  We could collect the\n+//! various options and then compile the code for the case where `foo` is an\n+//! `A`, a `B`, and a `C`.  When we generate the code for `C`, we would (1)\n+//! drop the two matches that do not match a `C` and (2) expand the other two\n+//! into two patterns each.  In the first case, the two patterns would be `1u`\n+//! and `2`, and the in the second case the _ pattern would be expanded into\n+//! `_` and `_`.  The two values are of course the arguments to `C`.\n+//!\n+//! Here is a quick guide to the various functions:\n+//!\n+//! - `compile_submatch()`: The main workhouse.  It takes a list of values and\n+//!   a list of matches and finds the various possibilities that could occur.\n+//!\n+//! - `enter_XXX()`: modifies the list of matches based on some information\n+//!   about the value that has been matched.  For example,\n+//!   `enter_rec_or_struct()` adjusts the values given that a record or struct\n+//!   has been matched.  This is an infallible pattern, so *all* of the matches\n+//!   must be either wildcards or record/struct patterns.  `enter_opt()`\n+//!   handles the fallible cases, and it is correspondingly more complex.\n+//!\n+//! ## Bindings\n+//!\n+//! We store information about the bound variables for each arm as part of the\n+//! per-arm `ArmData` struct.  There is a mapping from identifiers to\n+//! `BindingInfo` structs.  These structs contain the mode/id/type of the\n+//! binding, but they also contain an LLVM value which points at an alloca\n+//! called `llmatch`. For by value bindings that are Copy, we also create\n+//! an extra alloca that we copy the matched value to so that any changes\n+//! we do to our copy is not reflected in the original and vice-versa.\n+//! We don't do this if it's a move since the original value can't be used\n+//! and thus allowing us to cheat in not creating an extra alloca.\n+//!\n+//! The `llmatch` binding always stores a pointer into the value being matched\n+//! which points at the data for the binding.  If the value being matched has\n+//! type `T`, then, `llmatch` will point at an alloca of type `T*` (and hence\n+//! `llmatch` has type `T**`).  So, if you have a pattern like:\n+//!\n+//!    let a: A = ...;\n+//!    let b: B = ...;\n+//!    match (a, b) { (ref c, d) => { ... } }\n+//!\n+//! For `c` and `d`, we would generate allocas of type `C*` and `D*`\n+//! respectively.  These are called the `llmatch`.  As we match, when we come\n+//! up against an identifier, we store the current pointer into the\n+//! corresponding alloca.\n+//!\n+//! Once a pattern is completely matched, and assuming that there is no guard\n+//! pattern, we will branch to a block that leads to the body itself.  For any\n+//! by-value bindings, this block will first load the ptr from `llmatch` (the\n+//! one of type `D*`) and then load a second time to get the actual value (the\n+//! one of type `D`). For by ref bindings, the value of the local variable is\n+//! simply the first alloca.\n+//!\n+//! So, for the example above, we would generate a setup kind of like this:\n+//!\n+//!        +-------+\n+//!        | Entry |\n+//!        +-------+\n+//!            |\n+//!        +--------------------------------------------+\n+//!        | llmatch_c = (addr of first half of tuple)  |\n+//!        | llmatch_d = (addr of second half of tuple) |\n+//!        +--------------------------------------------+\n+//!            |\n+//!        +--------------------------------------+\n+//!        | *llbinding_d = **llmatch_d           |\n+//!        +--------------------------------------+\n+//!\n+//! If there is a guard, the situation is slightly different, because we must\n+//! execute the guard code.  Moreover, we need to do so once for each of the\n+//! alternatives that lead to the arm, because if the guard fails, they may\n+//! have different points from which to continue the search. Therefore, in that\n+//! case, we generate code that looks more like:\n+//!\n+//!        +-------+\n+//!        | Entry |\n+//!        +-------+\n+//!            |\n+//!        +-------------------------------------------+\n+//!        | llmatch_c = (addr of first half of tuple) |\n+//!        | llmatch_d = (addr of first half of tuple) |\n+//!        +-------------------------------------------+\n+//!            |\n+//!        +-------------------------------------------------+\n+//!        | *llbinding_d = **llmatch_d                      |\n+//!        | check condition                                 |\n+//!        | if false { goto next case }                     |\n+//!        | if true { goto body }                           |\n+//!        +-------------------------------------------------+\n+//!\n+//! The handling for the cleanups is a bit... sensitive.  Basically, the body\n+//! is the one that invokes `add_clean()` for each binding.  During the guard\n+//! evaluation, we add temporary cleanups and revoke them after the guard is\n+//! evaluated (it could fail, after all). Note that guards and moves are\n+//! just plain incompatible.\n+//!\n+//! Some relevant helper functions that manage bindings:\n+//! - `create_bindings_map()`\n+//! - `insert_lllocals()`\n+//!\n+//!\n+//! ## Notes on vector pattern matching.\n+//!\n+//! Vector pattern matching is surprisingly tricky. The problem is that\n+//! the structure of the vector isn't fully known, and slice matches\n+//! can be done on subparts of it.\n+//!\n+//! The way that vector pattern matches are dealt with, then, is as\n+//! follows. First, we make the actual condition associated with a\n+//! vector pattern simply a vector length comparison. So the pattern\n+//! [1, .. x] gets the condition \"vec len >= 1\", and the pattern\n+//! [.. x] gets the condition \"vec len >= 0\". The problem here is that\n+//! having the condition \"vec len >= 1\" hold clearly does not mean that\n+//! only a pattern that has exactly that condition will match. This\n+//! means that it may well be the case that a condition holds, but none\n+//! of the patterns matching that condition match; to deal with this,\n+//! when doing vector length matches, we have match failures proceed to\n+//! the next condition to check.\n+//!\n+//! There are a couple more subtleties to deal with. While the \"actual\"\n+//! condition associated with vector length tests is simply a test on\n+//! the vector length, the actual vec_len Opt entry contains more\n+//! information used to restrict which matches are associated with it.\n+//! So that all matches in a submatch are matching against the same\n+//! values from inside the vector, they are split up by how many\n+//! elements they match at the front and at the back of the vector. In\n+//! order to make sure that arms are properly checked in order, even\n+//! with the overmatching conditions, each vec_len Opt entry is\n+//! associated with a range of matches.\n+//! Consider the following:\n+//!\n+//!   match &[1, 2, 3] {\n+//!       [1, 1, .. _] => 0,\n+//!       [1, 2, 2, .. _] => 1,\n+//!       [1, 2, 3, .. _] => 2,\n+//!       [1, 2, .. _] => 3,\n+//!       _ => 4\n+//!   }\n+//! The proper arm to match is arm 2, but arms 0 and 3 both have the\n+//! condition \"len >= 2\". If arm 3 was lumped in with arm 0, then the\n+//! wrong branch would be taken. Instead, vec_len Opts are associated\n+//! with a contiguous range of matches that have the same \"shape\".\n+//! This is sort of ugly and requires a bunch of special handling of\n+//! vec_len options.\n \n pub use self::BranchKind::*;\n pub use self::OptResult::*;\n@@ -620,12 +616,9 @@ fn extract_variant_args<'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n     ExtractedBlock { vals: args, bcx: bcx }\n }\n \n+/// Helper for converting from the ValueRef that we pass around in the match code, which is always\n+/// an lvalue, into a Datum. Eventually we should just pass around a Datum and be done with it.\n fn match_datum<'tcx>(val: ValueRef, left_ty: Ty<'tcx>) -> Datum<'tcx, Lvalue> {\n-    /*!\n-     * Helper for converting from the ValueRef that we pass around in\n-     * the match code, which is always an lvalue, into a Datum. Eventually\n-     * we should just pass around a Datum and be done with it.\n-     */\n     Datum::new(val, left_ty, Lvalue)\n }\n \n@@ -831,15 +824,11 @@ fn compare_values<'blk, 'tcx>(cx: Block<'blk, 'tcx>,\n     }\n }\n \n+/// For each binding in `data.bindings_map`, adds an appropriate entry into the `fcx.lllocals` map\n fn insert_lllocals<'blk, 'tcx>(mut bcx: Block<'blk, 'tcx>,\n                                bindings_map: &BindingsMap<'tcx>,\n                                cs: Option<cleanup::ScopeId>)\n                                -> Block<'blk, 'tcx> {\n-    /*!\n-     * For each binding in `data.bindings_map`, adds an appropriate entry into\n-     * the `fcx.lllocals` map\n-     */\n-\n     for (&ident, &binding_info) in bindings_map.iter() {\n         let llval = match binding_info.trmode {\n             // By value mut binding for a copy type: load from the ptr\n@@ -1416,13 +1405,11 @@ fn trans_match_inner<'blk, 'tcx>(scope_cx: Block<'blk, 'tcx>,\n     return bcx;\n }\n \n+/// Generates code for a local variable declaration like `let <pat>;` or `let <pat> =\n+/// <opt_init_expr>`.\n pub fn store_local<'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n                                local: &ast::Local)\n                                -> Block<'blk, 'tcx> {\n-    /*!\n-     * Generates code for a local variable declaration like\n-     * `let <pat>;` or `let <pat> = <opt_init_expr>`.\n-     */\n     let _icx = push_ctxt(\"match::store_local\");\n     let mut bcx = bcx;\n     let tcx = bcx.tcx();\n@@ -1482,24 +1469,21 @@ pub fn store_local<'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n     }\n }\n \n+/// Generates code for argument patterns like `fn foo(<pat>: T)`.\n+/// Creates entries in the `lllocals` map for each of the bindings\n+/// in `pat`.\n+///\n+/// # Arguments\n+///\n+/// - `pat` is the argument pattern\n+/// - `llval` is a pointer to the argument value (in other words,\n+///   if the argument type is `T`, then `llval` is a `T*`). In some\n+///   cases, this code may zero out the memory `llval` points at.\n pub fn store_arg<'blk, 'tcx>(mut bcx: Block<'blk, 'tcx>,\n                              pat: &ast::Pat,\n                              arg: Datum<'tcx, Rvalue>,\n                              arg_scope: cleanup::ScopeId)\n                              -> Block<'blk, 'tcx> {\n-    /*!\n-     * Generates code for argument patterns like `fn foo(<pat>: T)`.\n-     * Creates entries in the `lllocals` map for each of the bindings\n-     * in `pat`.\n-     *\n-     * # Arguments\n-     *\n-     * - `pat` is the argument pattern\n-     * - `llval` is a pointer to the argument value (in other words,\n-     *   if the argument type is `T`, then `llval` is a `T*`). In some\n-     *   cases, this code may zero out the memory `llval` points at.\n-     */\n-\n     let _icx = push_ctxt(\"match::store_arg\");\n \n     match simple_identifier(&*pat) {\n@@ -1583,26 +1567,23 @@ fn mk_binding_alloca<'blk, 'tcx, A>(bcx: Block<'blk, 'tcx>,\n     bcx\n }\n \n+/// A simple version of the pattern matching code that only handles\n+/// irrefutable patterns. This is used in let/argument patterns,\n+/// not in match statements. Unifying this code with the code above\n+/// sounds nice, but in practice it produces very inefficient code,\n+/// since the match code is so much more general. In most cases,\n+/// LLVM is able to optimize the code, but it causes longer compile\n+/// times and makes the generated code nigh impossible to read.\n+///\n+/// # Arguments\n+/// - bcx: starting basic block context\n+/// - pat: the irrefutable pattern being matched.\n+/// - val: the value being matched -- must be an lvalue (by ref, with cleanup)\n fn bind_irrefutable_pat<'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n                                     pat: &ast::Pat,\n                                     val: ValueRef,\n                                     cleanup_scope: cleanup::ScopeId)\n                                     -> Block<'blk, 'tcx> {\n-    /*!\n-     * A simple version of the pattern matching code that only handles\n-     * irrefutable patterns. This is used in let/argument patterns,\n-     * not in match statements. Unifying this code with the code above\n-     * sounds nice, but in practice it produces very inefficient code,\n-     * since the match code is so much more general. In most cases,\n-     * LLVM is able to optimize the code, but it causes longer compile\n-     * times and makes the generated code nigh impossible to read.\n-     *\n-     * # Arguments\n-     * - bcx: starting basic block context\n-     * - pat: the irrefutable pattern being matched.\n-     * - val: the value being matched -- must be an lvalue (by ref, with cleanup)\n-     */\n-\n     debug!(\"bind_irrefutable_pat(bcx={}, pat={})\",\n            bcx.to_str(),\n            pat.repr(bcx.tcx()));"}, {"sha": "568805bee40479f66595be7fe82a84d3acc04725", "filename": "src/librustc_trans/trans/adt.rs", "status": "modified", "additions": 32, "deletions": 34, "changes": 66, "blob_url": "https://github.com/rust-lang/rust/blob/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc_trans%2Ftrans%2Fadt.rs", "raw_url": "https://github.com/rust-lang/rust/raw/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc_trans%2Ftrans%2Fadt.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_trans%2Ftrans%2Fadt.rs?ref=cd5c8235c5448a7234548c772468c8d2e8f150d9", "patch": "@@ -8,40 +8,38 @@\n // option. This file may not be copied, modified, or distributed\n // except according to those terms.\n \n-/*!\n- * # Representation of Algebraic Data Types\n- *\n- * This module determines how to represent enums, structs, and tuples\n- * based on their monomorphized types; it is responsible both for\n- * choosing a representation and translating basic operations on\n- * values of those types.  (Note: exporting the representations for\n- * debuggers is handled in debuginfo.rs, not here.)\n- *\n- * Note that the interface treats everything as a general case of an\n- * enum, so structs/tuples/etc. have one pseudo-variant with\n- * discriminant 0; i.e., as if they were a univariant enum.\n- *\n- * Having everything in one place will enable improvements to data\n- * structure representation; possibilities include:\n- *\n- * - User-specified alignment (e.g., cacheline-aligning parts of\n- *   concurrently accessed data structures); LLVM can't represent this\n- *   directly, so we'd have to insert padding fields in any structure\n- *   that might contain one and adjust GEP indices accordingly.  See\n- *   issue #4578.\n- *\n- * - Store nested enums' discriminants in the same word.  Rather, if\n- *   some variants start with enums, and those enums representations\n- *   have unused alignment padding between discriminant and body, the\n- *   outer enum's discriminant can be stored there and those variants\n- *   can start at offset 0.  Kind of fancy, and might need work to\n- *   make copies of the inner enum type cooperate, but it could help\n- *   with `Option` or `Result` wrapped around another enum.\n- *\n- * - Tagged pointers would be neat, but given that any type can be\n- *   used unboxed and any field can have pointers (including mutable)\n- *   taken to it, implementing them for Rust seems difficult.\n- */\n+//! # Representation of Algebraic Data Types\n+//!\n+//! This module determines how to represent enums, structs, and tuples\n+//! based on their monomorphized types; it is responsible both for\n+//! choosing a representation and translating basic operations on\n+//! values of those types.  (Note: exporting the representations for\n+//! debuggers is handled in debuginfo.rs, not here.)\n+//!\n+//! Note that the interface treats everything as a general case of an\n+//! enum, so structs/tuples/etc. have one pseudo-variant with\n+//! discriminant 0; i.e., as if they were a univariant enum.\n+//!\n+//! Having everything in one place will enable improvements to data\n+//! structure representation; possibilities include:\n+//!\n+//! - User-specified alignment (e.g., cacheline-aligning parts of\n+//!   concurrently accessed data structures); LLVM can't represent this\n+//!   directly, so we'd have to insert padding fields in any structure\n+//!   that might contain one and adjust GEP indices accordingly.  See\n+//!   issue #4578.\n+//!\n+//! - Store nested enums' discriminants in the same word.  Rather, if\n+//!   some variants start with enums, and those enums representations\n+//!   have unused alignment padding between discriminant and body, the\n+//!   outer enum's discriminant can be stored there and those variants\n+//!   can start at offset 0.  Kind of fancy, and might need work to\n+//!   make copies of the inner enum type cooperate, but it could help\n+//!   with `Option` or `Result` wrapped around another enum.\n+//!\n+//! - Tagged pointers would be neat, but given that any type can be\n+//!   used unboxed and any field can have pointers (including mutable)\n+//!   taken to it, implementing them for Rust seems difficult.\n \n #![allow(unsigned_negation)]\n "}, {"sha": "024df2a63adb5c7ef92d8ee500353ef9ef96f06a", "filename": "src/librustc_trans/trans/asm.rs", "status": "modified", "additions": 1, "deletions": 3, "changes": 4, "blob_url": "https://github.com/rust-lang/rust/blob/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc_trans%2Ftrans%2Fasm.rs", "raw_url": "https://github.com/rust-lang/rust/raw/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc_trans%2Ftrans%2Fasm.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_trans%2Ftrans%2Fasm.rs?ref=cd5c8235c5448a7234548c772468c8d2e8f150d9", "patch": "@@ -8,9 +8,7 @@\n // option. This file may not be copied, modified, or distributed\n // except according to those terms.\n \n-/*!\n-# Translation of inline assembly.\n-*/\n+//! # Translation of inline assembly.\n \n use llvm;\n use trans::build::*;"}, {"sha": "52e54a4a2613a41289835ffc16e3bf545ea0741b", "filename": "src/librustc_trans/trans/base.rs", "status": "modified", "additions": 5, "deletions": 10, "changes": 15, "blob_url": "https://github.com/rust-lang/rust/blob/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc_trans%2Ftrans%2Fbase.rs", "raw_url": "https://github.com/rust-lang/rust/raw/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc_trans%2Ftrans%2Fbase.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_trans%2Ftrans%2Fbase.rs?ref=cd5c8235c5448a7234548c772468c8d2e8f150d9", "patch": "@@ -1050,14 +1050,11 @@ pub fn load_if_immediate<'blk, 'tcx>(cx: Block<'blk, 'tcx>,\n     return v;\n }\n \n+/// Helper for loading values from memory. Does the necessary conversion if the in-memory type\n+/// differs from the type used for SSA values. Also handles various special cases where the type\n+/// gives us better information about what we are loading.\n pub fn load_ty<'blk, 'tcx>(cx: Block<'blk, 'tcx>,\n                            ptr: ValueRef, t: Ty<'tcx>) -> ValueRef {\n-    /*!\n-     * Helper for loading values from memory. Does the necessary conversion if\n-     * the in-memory type differs from the type used for SSA values. Also\n-     * handles various special cases where the type gives us better information\n-     * about what we are loading.\n-     */\n     if type_is_zero_size(cx.ccx(), t) {\n         C_undef(type_of::type_of(cx.ccx(), t))\n     } else if ty::type_is_bool(t) {\n@@ -1071,11 +1068,9 @@ pub fn load_ty<'blk, 'tcx>(cx: Block<'blk, 'tcx>,\n     }\n }\n \n+/// Helper for storing values in memory. Does the necessary conversion if the in-memory type\n+/// differs from the type used for SSA values.\n pub fn store_ty(cx: Block, v: ValueRef, dst: ValueRef, t: Ty) {\n-    /*!\n-     * Helper for storing values in memory. Does the necessary conversion if\n-     * the in-memory type differs from the type used for SSA values.\n-     */\n     if ty::type_is_bool(t) {\n         Store(cx, ZExt(cx, v, Type::i8(cx.ccx())), dst);\n     } else {"}, {"sha": "5d713526a3d6aa0bdc1be0ce95413723bd7204b7", "filename": "src/librustc_trans/trans/callee.rs", "status": "modified", "additions": 28, "deletions": 43, "changes": 71, "blob_url": "https://github.com/rust-lang/rust/blob/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc_trans%2Ftrans%2Fcallee.rs", "raw_url": "https://github.com/rust-lang/rust/raw/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc_trans%2Ftrans%2Fcallee.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_trans%2Ftrans%2Fcallee.rs?ref=cd5c8235c5448a7234548c772468c8d2e8f150d9", "patch": "@@ -8,13 +8,11 @@\n // option. This file may not be copied, modified, or distributed\n // except according to those terms.\n \n-/*!\n- * Handles translation of callees as well as other call-related\n- * things.  Callees are a superset of normal rust values and sometimes\n- * have different representations.  In particular, top-level fn items\n- * and methods are represented as just a fn ptr and not a full\n- * closure.\n- */\n+//! Handles translation of callees as well as other call-related\n+//! things.  Callees are a superset of normal rust values and sometimes\n+//! have different representations.  In particular, top-level fn items\n+//! and methods are represented as just a fn ptr and not a full\n+//! closure.\n \n pub use self::AutorefArg::*;\n pub use self::CalleeData::*;\n@@ -220,13 +218,9 @@ fn trans<'blk, 'tcx>(bcx: Block<'blk, 'tcx>, expr: &ast::Expr)\n     }\n }\n \n+/// Translates a reference (with id `ref_id`) to the fn/method with id `def_id` into a function\n+/// pointer. This may require monomorphization or inlining.\n pub fn trans_fn_ref(bcx: Block, def_id: ast::DefId, node: ExprOrMethodCall) -> ValueRef {\n-    /*!\n-     * Translates a reference (with id `ref_id`) to the fn/method\n-     * with id `def_id` into a function pointer.  This may require\n-     * monomorphization or inlining.\n-     */\n-\n     let _icx = push_ctxt(\"trans_fn_ref\");\n \n     let substs = node_id_substs(bcx, node);\n@@ -398,27 +392,24 @@ pub fn trans_unboxing_shim<'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n     llfn\n }\n \n+/// Translates a reference to a fn/method item, monomorphizing and\n+/// inlining as it goes.\n+///\n+/// # Parameters\n+///\n+/// - `bcx`: the current block where the reference to the fn occurs\n+/// - `def_id`: def id of the fn or method item being referenced\n+/// - `node`: node id of the reference to the fn/method, if applicable.\n+///   This parameter may be zero; but, if so, the resulting value may not\n+///   have the right type, so it must be cast before being used.\n+/// - `substs`: values for each of the fn/method's parameters\n pub fn trans_fn_ref_with_substs<'blk, 'tcx>(\n     bcx: Block<'blk, 'tcx>,      //\n     def_id: ast::DefId,          // def id of fn\n     node: ExprOrMethodCall,      // node id of use of fn; may be zero if N/A\n     substs: subst::Substs<'tcx>) // vtables for the call\n     -> ValueRef\n {\n-    /*!\n-     * Translates a reference to a fn/method item, monomorphizing and\n-     * inlining as it goes.\n-     *\n-     * # Parameters\n-     *\n-     * - `bcx`: the current block where the reference to the fn occurs\n-     * - `def_id`: def id of the fn or method item being referenced\n-     * - `node`: node id of the reference to the fn/method, if applicable.\n-     *   This parameter may be zero; but, if so, the resulting value may not\n-     *   have the right type, so it must be cast before being used.\n-     * - `substs`: values for each of the fn/method's parameters\n-     */\n-\n     let _icx = push_ctxt(\"trans_fn_ref_with_substs\");\n     let ccx = bcx.ccx();\n     let tcx = bcx.tcx();\n@@ -668,6 +659,16 @@ pub fn trans_lang_call<'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n                              dest)\n }\n \n+/// This behemoth of a function translates function calls. Unfortunately, in order to generate more\n+/// efficient LLVM output at -O0, it has quite a complex signature (refactoring this into two\n+/// functions seems like a good idea).\n+///\n+/// In particular, for lang items, it is invoked with a dest of None, and in that case the return\n+/// value contains the result of the fn. The lang item must not return a structural type or else\n+/// all heck breaks loose.\n+///\n+/// For non-lang items, `dest` is always Some, and hence the result is written into memory\n+/// somewhere. Nonetheless we return the actual return value of the function.\n pub fn trans_call_inner<'a, 'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n                                         call_info: Option<NodeInfo>,\n                                         callee_ty: Ty<'tcx>,\n@@ -677,22 +678,6 @@ pub fn trans_call_inner<'a, 'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n                                         args: CallArgs<'a, 'tcx>,\n                                         dest: Option<expr::Dest>)\n                                         -> Result<'blk, 'tcx> {\n-    /*!\n-     * This behemoth of a function translates function calls.\n-     * Unfortunately, in order to generate more efficient LLVM\n-     * output at -O0, it has quite a complex signature (refactoring\n-     * this into two functions seems like a good idea).\n-     *\n-     * In particular, for lang items, it is invoked with a dest of\n-     * None, and in that case the return value contains the result of\n-     * the fn. The lang item must not return a structural type or else\n-     * all heck breaks loose.\n-     *\n-     * For non-lang items, `dest` is always Some, and hence the result\n-     * is written into memory somewhere. Nonetheless we return the\n-     * actual return value of the function.\n-     */\n-\n     // Introduce a temporary cleanup scope that will contain cleanups\n     // for the arguments while they are being evaluated. The purpose\n     // this cleanup is to ensure that, should a panic occur while"}, {"sha": "d7da83ddb0d04168957d6cb59fbe2700ab0680e5", "filename": "src/librustc_trans/trans/cleanup.rs", "status": "modified", "additions": 64, "deletions": 150, "changes": 214, "blob_url": "https://github.com/rust-lang/rust/blob/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc_trans%2Ftrans%2Fcleanup.rs", "raw_url": "https://github.com/rust-lang/rust/raw/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc_trans%2Ftrans%2Fcleanup.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_trans%2Ftrans%2Fcleanup.rs?ref=cd5c8235c5448a7234548c772468c8d2e8f150d9", "patch": "@@ -8,10 +8,8 @@\n // option. This file may not be copied, modified, or distributed\n // except according to those terms.\n \n-/*!\n- * Code pertaining to cleanup of temporaries as well as execution of\n- * drop glue. See discussion in `doc.rs` for a high-level summary.\n- */\n+//! Code pertaining to cleanup of temporaries as well as execution of\n+//! drop glue. See discussion in `doc.rs` for a high-level summary.\n \n pub use self::ScopeId::*;\n pub use self::CleanupScopeKind::*;\n@@ -114,12 +112,8 @@ pub enum ScopeId {\n }\n \n impl<'blk, 'tcx> CleanupMethods<'blk, 'tcx> for FunctionContext<'blk, 'tcx> {\n+    /// Invoked when we start to trans the code contained within a new cleanup scope.\n     fn push_ast_cleanup_scope(&self, debug_loc: NodeInfo) {\n-        /*!\n-         * Invoked when we start to trans the code contained\n-         * within a new cleanup scope.\n-         */\n-\n         debug!(\"push_ast_cleanup_scope({})\",\n                self.ccx.tcx().map.node_to_string(debug_loc.id));\n \n@@ -189,16 +183,12 @@ impl<'blk, 'tcx> CleanupMethods<'blk, 'tcx> for FunctionContext<'blk, 'tcx> {\n         CustomScopeIndex { index: index }\n     }\n \n+    /// Removes the cleanup scope for id `cleanup_scope`, which must be at the top of the cleanup\n+    /// stack, and generates the code to do its cleanups for normal exit.\n     fn pop_and_trans_ast_cleanup_scope(&self,\n                                        bcx: Block<'blk, 'tcx>,\n                                        cleanup_scope: ast::NodeId)\n                                        -> Block<'blk, 'tcx> {\n-        /*!\n-         * Removes the cleanup scope for id `cleanup_scope`, which\n-         * must be at the top of the cleanup stack, and generates the\n-         * code to do its cleanups for normal exit.\n-         */\n-\n         debug!(\"pop_and_trans_ast_cleanup_scope({})\",\n                self.ccx.tcx().map.node_to_string(cleanup_scope));\n \n@@ -208,15 +198,11 @@ impl<'blk, 'tcx> CleanupMethods<'blk, 'tcx> for FunctionContext<'blk, 'tcx> {\n         self.trans_scope_cleanups(bcx, &scope)\n     }\n \n+    /// Removes the loop cleanup scope for id `cleanup_scope`, which must be at the top of the\n+    /// cleanup stack. Does not generate any cleanup code, since loop scopes should exit by\n+    /// branching to a block generated by `normal_exit_block`.\n     fn pop_loop_cleanup_scope(&self,\n                               cleanup_scope: ast::NodeId) {\n-        /*!\n-         * Removes the loop cleanup scope for id `cleanup_scope`, which\n-         * must be at the top of the cleanup stack. Does not generate\n-         * any cleanup code, since loop scopes should exit by\n-         * branching to a block generated by `normal_exit_block`.\n-         */\n-\n         debug!(\"pop_loop_cleanup_scope({})\",\n                self.ccx.tcx().map.node_to_string(cleanup_scope));\n \n@@ -225,41 +211,30 @@ impl<'blk, 'tcx> CleanupMethods<'blk, 'tcx> for FunctionContext<'blk, 'tcx> {\n         let _ = self.pop_scope();\n     }\n \n+    /// Removes the top cleanup scope from the stack without executing its cleanups. The top\n+    /// cleanup scope must be the temporary scope `custom_scope`.\n     fn pop_custom_cleanup_scope(&self,\n                                 custom_scope: CustomScopeIndex) {\n-        /*!\n-         * Removes the top cleanup scope from the stack without\n-         * executing its cleanups. The top cleanup scope must\n-         * be the temporary scope `custom_scope`.\n-         */\n-\n         debug!(\"pop_custom_cleanup_scope({})\", custom_scope.index);\n         assert!(self.is_valid_to_pop_custom_scope(custom_scope));\n         let _ = self.pop_scope();\n     }\n \n+    /// Removes the top cleanup scope from the stack, which must be a temporary scope, and\n+    /// generates the code to do its cleanups for normal exit.\n     fn pop_and_trans_custom_cleanup_scope(&self,\n                                           bcx: Block<'blk, 'tcx>,\n                                           custom_scope: CustomScopeIndex)\n                                           -> Block<'blk, 'tcx> {\n-        /*!\n-         * Removes the top cleanup scope from the stack, which must be\n-         * a temporary scope, and generates the code to do its\n-         * cleanups for normal exit.\n-         */\n-\n         debug!(\"pop_and_trans_custom_cleanup_scope({})\", custom_scope);\n         assert!(self.is_valid_to_pop_custom_scope(custom_scope));\n \n         let scope = self.pop_scope();\n         self.trans_scope_cleanups(bcx, &scope)\n     }\n \n+    /// Returns the id of the top-most loop scope\n     fn top_loop_scope(&self) -> ast::NodeId {\n-        /*!\n-         * Returns the id of the top-most loop scope\n-         */\n-\n         for scope in self.scopes.borrow().iter().rev() {\n             match scope.kind {\n                 LoopScopeKind(id, _) => {\n@@ -271,24 +246,17 @@ impl<'blk, 'tcx> CleanupMethods<'blk, 'tcx> for FunctionContext<'blk, 'tcx> {\n         self.ccx.sess().bug(\"no loop scope found\");\n     }\n \n+    /// Returns a block to branch to which will perform all pending cleanups and then\n+    /// break/continue (depending on `exit`) out of the loop with id `cleanup_scope`\n     fn normal_exit_block(&'blk self,\n                          cleanup_scope: ast::NodeId,\n                          exit: uint) -> BasicBlockRef {\n-        /*!\n-         * Returns a block to branch to which will perform all pending\n-         * cleanups and then break/continue (depending on `exit`) out\n-         * of the loop with id `cleanup_scope`\n-         */\n-\n         self.trans_cleanups_to_exit_scope(LoopExit(cleanup_scope, exit))\n     }\n \n+    /// Returns a block to branch to which will perform all pending cleanups and then return from\n+    /// this function\n     fn return_exit_block(&'blk self) -> BasicBlockRef {\n-        /*!\n-         * Returns a block to branch to which will perform all pending\n-         * cleanups and then return from this function\n-         */\n-\n         self.trans_cleanups_to_exit_scope(ReturnExit)\n     }\n \n@@ -306,15 +274,11 @@ impl<'blk, 'tcx> CleanupMethods<'blk, 'tcx> for FunctionContext<'blk, 'tcx> {\n         self.schedule_clean(cleanup_scope, drop as CleanupObj);\n     }\n \n+    /// Schedules a (deep) drop of `val`, which is a pointer to an instance of `ty`\n     fn schedule_drop_mem(&self,\n                          cleanup_scope: ScopeId,\n                          val: ValueRef,\n                          ty: Ty<'tcx>) {\n-        /*!\n-         * Schedules a (deep) drop of `val`, which is a pointer to an\n-         * instance of `ty`\n-         */\n-\n         if !ty::type_needs_drop(self.ccx.tcx(), ty) { return; }\n         let drop = box DropValue {\n             is_immediate: false,\n@@ -332,15 +296,11 @@ impl<'blk, 'tcx> CleanupMethods<'blk, 'tcx> for FunctionContext<'blk, 'tcx> {\n         self.schedule_clean(cleanup_scope, drop as CleanupObj);\n     }\n \n+    /// Schedules a (deep) drop and zero-ing of `val`, which is a pointer to an instance of `ty`\n     fn schedule_drop_and_zero_mem(&self,\n                                   cleanup_scope: ScopeId,\n                                   val: ValueRef,\n                                   ty: Ty<'tcx>) {\n-        /*!\n-         * Schedules a (deep) drop and zero-ing of `val`, which is a pointer\n-         * to an instance of `ty`\n-         */\n-\n         if !ty::type_needs_drop(self.ccx.tcx(), ty) { return; }\n         let drop = box DropValue {\n             is_immediate: false,\n@@ -359,13 +319,11 @@ impl<'blk, 'tcx> CleanupMethods<'blk, 'tcx> for FunctionContext<'blk, 'tcx> {\n         self.schedule_clean(cleanup_scope, drop as CleanupObj);\n     }\n \n+    /// Schedules a (deep) drop of `val`, which is an instance of `ty`\n     fn schedule_drop_immediate(&self,\n                                cleanup_scope: ScopeId,\n                                val: ValueRef,\n                                ty: Ty<'tcx>) {\n-        /*!\n-         * Schedules a (deep) drop of `val`, which is an instance of `ty`\n-         */\n \n         if !ty::type_needs_drop(self.ccx.tcx(), ty) { return; }\n         let drop = box DropValue {\n@@ -384,16 +342,12 @@ impl<'blk, 'tcx> CleanupMethods<'blk, 'tcx> for FunctionContext<'blk, 'tcx> {\n         self.schedule_clean(cleanup_scope, drop as CleanupObj);\n     }\n \n+    /// Schedules a call to `free(val)`. Note that this is a shallow operation.\n     fn schedule_free_value(&self,\n                            cleanup_scope: ScopeId,\n                            val: ValueRef,\n                            heap: Heap,\n                            content_ty: Ty<'tcx>) {\n-        /*!\n-         * Schedules a call to `free(val)`. Note that this is a shallow\n-         * operation.\n-         */\n-\n         let drop = box FreeValue { ptr: val, heap: heap, content_ty: content_ty };\n \n         debug!(\"schedule_free_value({}, val={}, heap={})\",\n@@ -404,17 +358,13 @@ impl<'blk, 'tcx> CleanupMethods<'blk, 'tcx> for FunctionContext<'blk, 'tcx> {\n         self.schedule_clean(cleanup_scope, drop as CleanupObj);\n     }\n \n+    /// Schedules a call to `free(val)`. Note that this is a shallow operation.\n     fn schedule_free_slice(&self,\n                            cleanup_scope: ScopeId,\n                            val: ValueRef,\n                            size: ValueRef,\n                            align: ValueRef,\n                            heap: Heap) {\n-        /*!\n-         * Schedules a call to `free(val)`. Note that this is a shallow\n-         * operation.\n-         */\n-\n         let drop = box FreeSlice { ptr: val, size: size, align: align, heap: heap };\n \n         debug!(\"schedule_free_slice({}, val={}, heap={})\",\n@@ -434,15 +384,12 @@ impl<'blk, 'tcx> CleanupMethods<'blk, 'tcx> for FunctionContext<'blk, 'tcx> {\n         }\n     }\n \n+    /// Schedules a cleanup to occur upon exit from `cleanup_scope`. If `cleanup_scope` is not\n+    /// provided, then the cleanup is scheduled in the topmost scope, which must be a temporary\n+    /// scope.\n     fn schedule_clean_in_ast_scope(&self,\n                                    cleanup_scope: ast::NodeId,\n                                    cleanup: CleanupObj<'tcx>) {\n-        /*!\n-         * Schedules a cleanup to occur upon exit from `cleanup_scope`.\n-         * If `cleanup_scope` is not provided, then the cleanup is scheduled\n-         * in the topmost scope, which must be a temporary scope.\n-         */\n-\n         debug!(\"schedule_clean_in_ast_scope(cleanup_scope={})\",\n                cleanup_scope);\n \n@@ -462,14 +409,10 @@ impl<'blk, 'tcx> CleanupMethods<'blk, 'tcx> for FunctionContext<'blk, 'tcx> {\n                     self.ccx.tcx().map.node_to_string(cleanup_scope)).as_slice());\n     }\n \n+    /// Schedules a cleanup to occur in the top-most scope, which must be a temporary scope.\n     fn schedule_clean_in_custom_scope(&self,\n                                       custom_scope: CustomScopeIndex,\n                                       cleanup: CleanupObj<'tcx>) {\n-        /*!\n-         * Schedules a cleanup to occur in the top-most scope,\n-         * which must be a temporary scope.\n-         */\n-\n         debug!(\"schedule_clean_in_custom_scope(custom_scope={})\",\n                custom_scope.index);\n \n@@ -481,22 +424,14 @@ impl<'blk, 'tcx> CleanupMethods<'blk, 'tcx> for FunctionContext<'blk, 'tcx> {\n         scope.clear_cached_exits();\n     }\n \n+    /// Returns true if there are pending cleanups that should execute on panic.\n     fn needs_invoke(&self) -> bool {\n-        /*!\n-         * Returns true if there are pending cleanups that should\n-         * execute on panic.\n-         */\n-\n         self.scopes.borrow().iter().rev().any(|s| s.needs_invoke())\n     }\n \n+    /// Returns a basic block to branch to in the event of a panic. This block will run the panic\n+    /// cleanups and eventually invoke the LLVM `Resume` instruction.\n     fn get_landing_pad(&'blk self) -> BasicBlockRef {\n-        /*!\n-         * Returns a basic block to branch to in the event of a panic.\n-         * This block will run the panic cleanups and eventually\n-         * invoke the LLVM `Resume` instruction.\n-         */\n-\n         let _icx = base::push_ctxt(\"get_landing_pad\");\n \n         debug!(\"get_landing_pad\");\n@@ -529,10 +464,8 @@ impl<'blk, 'tcx> CleanupMethods<'blk, 'tcx> for FunctionContext<'blk, 'tcx> {\n }\n \n impl<'blk, 'tcx> CleanupHelperMethods<'blk, 'tcx> for FunctionContext<'blk, 'tcx> {\n+    /// Returns the id of the current top-most AST scope, if any.\n     fn top_ast_scope(&self) -> Option<ast::NodeId> {\n-        /*!\n-         * Returns the id of the current top-most AST scope, if any.\n-         */\n         for scope in self.scopes.borrow().iter().rev() {\n             match scope.kind {\n                 CustomScopeKind | LoopScopeKind(..) => {}\n@@ -559,10 +492,10 @@ impl<'blk, 'tcx> CleanupHelperMethods<'blk, 'tcx> for FunctionContext<'blk, 'tcx\n             (*scopes)[custom_scope.index].kind.is_temp()\n     }\n \n+    /// Generates the cleanups for `scope` into `bcx`\n     fn trans_scope_cleanups(&self, // cannot borrow self, will recurse\n                             bcx: Block<'blk, 'tcx>,\n                             scope: &CleanupScope<'blk, 'tcx>) -> Block<'blk, 'tcx> {\n-        /*! Generates the cleanups for `scope` into `bcx` */\n \n         let mut bcx = bcx;\n         if !bcx.unreachable.get() {\n@@ -593,37 +526,31 @@ impl<'blk, 'tcx> CleanupHelperMethods<'blk, 'tcx> for FunctionContext<'blk, 'tcx\n         f(self.scopes.borrow().last().unwrap())\n     }\n \n+    /// Used when the caller wishes to jump to an early exit, such as a return, break, continue, or\n+    /// unwind. This function will generate all cleanups between the top of the stack and the exit\n+    /// `label` and return a basic block that the caller can branch to.\n+    ///\n+    /// For example, if the current stack of cleanups were as follows:\n+    ///\n+    ///      AST 22\n+    ///      Custom 1\n+    ///      AST 23\n+    ///      Loop 23\n+    ///      Custom 2\n+    ///      AST 24\n+    ///\n+    /// and the `label` specifies a break from `Loop 23`, then this function would generate a\n+    /// series of basic blocks as follows:\n+    ///\n+    ///      Cleanup(AST 24) -> Cleanup(Custom 2) -> break_blk\n+    ///\n+    /// where `break_blk` is the block specified in `Loop 23` as the target for breaks. The return\n+    /// value would be the first basic block in that sequence (`Cleanup(AST 24)`). The caller could\n+    /// then branch to `Cleanup(AST 24)` and it will perform all cleanups and finally branch to the\n+    /// `break_blk`.\n     fn trans_cleanups_to_exit_scope(&'blk self,\n                                     label: EarlyExitLabel)\n                                     -> BasicBlockRef {\n-        /*!\n-         * Used when the caller wishes to jump to an early exit, such\n-         * as a return, break, continue, or unwind. This function will\n-         * generate all cleanups between the top of the stack and the\n-         * exit `label` and return a basic block that the caller can\n-         * branch to.\n-         *\n-         * For example, if the current stack of cleanups were as follows:\n-         *\n-         *      AST 22\n-         *      Custom 1\n-         *      AST 23\n-         *      Loop 23\n-         *      Custom 2\n-         *      AST 24\n-         *\n-         * and the `label` specifies a break from `Loop 23`, then this\n-         * function would generate a series of basic blocks as follows:\n-         *\n-         *      Cleanup(AST 24) -> Cleanup(Custom 2) -> break_blk\n-         *\n-         * where `break_blk` is the block specified in `Loop 23` as\n-         * the target for breaks. The return value would be the first\n-         * basic block in that sequence (`Cleanup(AST 24)`). The\n-         * caller could then branch to `Cleanup(AST 24)` and it will\n-         * perform all cleanups and finally branch to the `break_blk`.\n-         */\n-\n         debug!(\"trans_cleanups_to_exit_scope label={} scopes={}\",\n                label, self.scopes_len());\n \n@@ -756,20 +683,15 @@ impl<'blk, 'tcx> CleanupHelperMethods<'blk, 'tcx> for FunctionContext<'blk, 'tcx\n         prev_llbb\n     }\n \n+    /// Creates a landing pad for the top scope, if one does not exist.  The landing pad will\n+    /// perform all cleanups necessary for an unwind and then `resume` to continue error\n+    /// propagation:\n+    ///\n+    ///     landing_pad -> ... cleanups ... -> [resume]\n+    ///\n+    /// (The cleanups and resume instruction are created by `trans_cleanups_to_exit_scope()`, not\n+    /// in this function itself.)\n     fn get_or_create_landing_pad(&'blk self) -> BasicBlockRef {\n-        /*!\n-         * Creates a landing pad for the top scope, if one does not\n-         * exist.  The landing pad will perform all cleanups necessary\n-         * for an unwind and then `resume` to continue error\n-         * propagation:\n-         *\n-         *     landing_pad -> ... cleanups ... -> [resume]\n-         *\n-         * (The cleanups and resume instruction are created by\n-         * `trans_cleanups_to_exit_scope()`, not in this function\n-         * itself.)\n-         */\n-\n         let pad_bcx;\n \n         debug!(\"get_or_create_landing_pad\");\n@@ -883,19 +805,15 @@ impl<'blk, 'tcx> CleanupScope<'blk, 'tcx> {\n                               cleanup_block: blk });\n     }\n \n+    /// True if this scope has cleanups that need unwinding\n     fn needs_invoke(&self) -> bool {\n-        /*! True if this scope has cleanups that need unwinding */\n \n         self.cached_landing_pad.is_some() ||\n             self.cleanups.iter().any(|c| c.must_unwind())\n     }\n \n+    /// Returns a suitable name to use for the basic block that handles this cleanup scope\n     fn block_name(&self, prefix: &str) -> String {\n-        /*!\n-         * Returns a suitable name to use for the basic block that\n-         * handles this cleanup scope\n-         */\n-\n         match self.kind {\n             CustomScopeKind => format!(\"{}_custom_\", prefix),\n             AstScopeKind(id) => format!(\"{}_ast_{}_\", prefix, id),\n@@ -930,14 +848,10 @@ impl<'blk, 'tcx> CleanupScopeKind<'blk, 'tcx> {\n         }\n     }\n \n+    /// If this is a loop scope with id `id`, return the early exit block `exit`, else `None`\n     fn early_exit_block(&self,\n                         id: ast::NodeId,\n                         exit: uint) -> Option<BasicBlockRef> {\n-        /*!\n-         * If this is a loop scope with id `id`, return the early\n-         * exit block `exit`, else `None`\n-         */\n-\n         match *self {\n             LoopScopeKind(i, ref exits) if id == i => Some(exits[exit].llbb),\n             _ => None,"}, {"sha": "2f82b8286c2d531da021e90752b4eba7b025b56f", "filename": "src/librustc_trans/trans/closure.rs", "status": "modified", "additions": 9, "deletions": 13, "changes": 22, "blob_url": "https://github.com/rust-lang/rust/blob/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc_trans%2Ftrans%2Fclosure.rs", "raw_url": "https://github.com/rust-lang/rust/raw/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc_trans%2Ftrans%2Fclosure.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_trans%2Ftrans%2Fclosure.rs?ref=cd5c8235c5448a7234548c772468c8d2e8f150d9", "patch": "@@ -386,26 +386,22 @@ impl<'a, 'tcx> ClosureEnv<'a, 'tcx> {\n     }\n }\n \n+/// Translates the body of a closure expression.\n+///\n+/// - `store`\n+/// - `decl`\n+/// - `body`\n+/// - `id`: The id of the closure expression.\n+/// - `cap_clause`: information about captured variables, if any.\n+/// - `dest`: where to write the closure value, which must be a\n+///   (fn ptr, env) pair\n pub fn trans_expr_fn<'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n                                  store: ty::TraitStore,\n                                  decl: &ast::FnDecl,\n                                  body: &ast::Block,\n                                  id: ast::NodeId,\n                                  dest: expr::Dest)\n                                  -> Block<'blk, 'tcx> {\n-    /*!\n-     *\n-     * Translates the body of a closure expression.\n-     *\n-     * - `store`\n-     * - `decl`\n-     * - `body`\n-     * - `id`: The id of the closure expression.\n-     * - `cap_clause`: information about captured variables, if any.\n-     * - `dest`: where to write the closure value, which must be a\n-         (fn ptr, env) pair\n-     */\n-\n     let _icx = push_ctxt(\"closure::trans_expr_fn\");\n \n     let dest_addr = match dest {"}, {"sha": "febb33f6c54aff7db2476556317d18cf3705d801", "filename": "src/librustc_trans/trans/common.rs", "status": "modified", "additions": 8, "deletions": 20, "changes": 28, "blob_url": "https://github.com/rust-lang/rust/blob/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc_trans%2Ftrans%2Fcommon.rs", "raw_url": "https://github.com/rust-lang/rust/raw/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc_trans%2Ftrans%2Fcommon.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_trans%2Ftrans%2Fcommon.rs?ref=cd5c8235c5448a7234548c772468c8d2e8f150d9", "patch": "@@ -95,26 +95,19 @@ pub fn type_is_immediate<'a, 'tcx>(ccx: &CrateContext<'a, 'tcx>, ty: Ty<'tcx>) -\n     }\n }\n \n+/// Identify types which have size zero at runtime.\n pub fn type_is_zero_size<'a, 'tcx>(ccx: &CrateContext<'a, 'tcx>, ty: Ty<'tcx>) -> bool {\n-    /*!\n-     * Identify types which have size zero at runtime.\n-     */\n-\n     use trans::machine::llsize_of_alloc;\n     use trans::type_of::sizing_type_of;\n     let llty = sizing_type_of(ccx, ty);\n     llsize_of_alloc(ccx, llty) == 0\n }\n \n+/// Identifies types which we declare to be equivalent to `void` in C for the purpose of function\n+/// return types. These are `()`, bot, and uninhabited enums. Note that all such types are also\n+/// zero-size, but not all zero-size types use a `void` return type (in order to aid with C ABI\n+/// compatibility).\n pub fn return_type_is_void(ccx: &CrateContext, ty: Ty) -> bool {\n-    /*!\n-     * Identifies types which we declare to be equivalent to `void`\n-     * in C for the purpose of function return types. These are\n-     * `()`, bot, and uninhabited enums. Note that all such types\n-     * are also zero-size, but not all zero-size types use a `void`\n-     * return type (in order to aid with C ABI compatibility).\n-     */\n-\n     ty::type_is_nil(ty) || ty::type_is_empty(ccx.tcx(), ty)\n }\n \n@@ -768,19 +761,14 @@ pub fn expr_ty_adjusted<'blk, 'tcx>(bcx: Block<'blk, 'tcx>, ex: &ast::Expr) -> T\n     monomorphize_type(bcx, ty::expr_ty_adjusted(bcx.tcx(), ex))\n }\n \n+/// Attempts to resolve an obligation. The result is a shallow vtable resolution -- meaning that we\n+/// do not (necessarily) resolve all nested obligations on the impl. Note that type check should\n+/// guarantee to us that all nested obligations *could be* resolved if we wanted to.\n pub fn fulfill_obligation<'a, 'tcx>(ccx: &CrateContext<'a, 'tcx>,\n                                     span: Span,\n                                     trait_ref: Rc<ty::TraitRef<'tcx>>)\n                                     -> traits::Vtable<'tcx, ()>\n {\n-    /*!\n-     * Attempts to resolve an obligation. The result is a shallow\n-     * vtable resolution -- meaning that we do not (necessarily) resolve\n-     * all nested obligations on the impl. Note that type check should\n-     * guarantee to us that all nested obligations *could be* resolved\n-     * if we wanted to.\n-     */\n-\n     let tcx = ccx.tcx();\n \n     // Remove any references to regions; this helps improve caching."}, {"sha": "22f030be3d6530f73dff4d9a227f098b990586c1", "filename": "src/librustc_trans/trans/datum.rs", "status": "modified", "additions": 47, "deletions": 114, "changes": 161, "blob_url": "https://github.com/rust-lang/rust/blob/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc_trans%2Ftrans%2Fdatum.rs", "raw_url": "https://github.com/rust-lang/rust/raw/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc_trans%2Ftrans%2Fdatum.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_trans%2Ftrans%2Fdatum.rs?ref=cd5c8235c5448a7234548c772468c8d2e8f150d9", "patch": "@@ -8,10 +8,8 @@\n // option. This file may not be copied, modified, or distributed\n // except according to those terms.\n \n-/*!\n- * See the section on datums in `doc.rs` for an overview of what\n- * Datums are and how they are intended to be used.\n- */\n+//! See the section on datums in `doc.rs` for an overview of what Datums are and how they are\n+//! intended to be used.\n \n pub use self::Expr::*;\n pub use self::RvalueMode::*;\n@@ -107,6 +105,10 @@ pub fn immediate_rvalue_bcx<'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n }\n \n \n+/// Allocates temporary space on the stack using alloca() and returns a by-ref Datum pointing to\n+/// it. The memory will be dropped upon exit from `scope`. The callback `populate` should\n+/// initialize the memory. If `zero` is true, the space will be zeroed when it is allocated; this\n+/// is not necessary unless `bcx` does not dominate the end of `scope`.\n pub fn lvalue_scratch_datum<'blk, 'tcx, A>(bcx: Block<'blk, 'tcx>,\n                                            ty: Ty<'tcx>,\n                                            name: &str,\n@@ -116,15 +118,6 @@ pub fn lvalue_scratch_datum<'blk, 'tcx, A>(bcx: Block<'blk, 'tcx>,\n                                            populate: |A, Block<'blk, 'tcx>, ValueRef|\n                                                       -> Block<'blk, 'tcx>)\n                                           -> DatumBlock<'blk, 'tcx, Lvalue> {\n-    /*!\n-     * Allocates temporary space on the stack using alloca() and\n-     * returns a by-ref Datum pointing to it. The memory will be\n-     * dropped upon exit from `scope`. The callback `populate` should\n-     * initialize the memory. If `zero` is true, the space will be\n-     * zeroed when it is allocated; this is not necessary unless `bcx`\n-     * does not dominate the end of `scope`.\n-     */\n-\n     let scratch = if zero {\n         alloca_zeroed(bcx, ty, name)\n     } else {\n@@ -140,33 +133,24 @@ pub fn lvalue_scratch_datum<'blk, 'tcx, A>(bcx: Block<'blk, 'tcx>,\n     DatumBlock::new(bcx, Datum::new(scratch, ty, Lvalue))\n }\n \n+/// Allocates temporary space on the stack using alloca() and returns a by-ref Datum pointing to\n+/// it.  If `zero` is true, the space will be zeroed when it is allocated; this is normally not\n+/// necessary, but in the case of automatic rooting in match statements it is possible to have\n+/// temporaries that may not get initialized if a certain arm is not taken, so we must zero them.\n+/// You must arrange any cleanups etc yourself!\n pub fn rvalue_scratch_datum<'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n                                         ty: Ty<'tcx>,\n                                         name: &str)\n                                         -> Datum<'tcx, Rvalue> {\n-    /*!\n-     * Allocates temporary space on the stack using alloca() and\n-     * returns a by-ref Datum pointing to it.  If `zero` is true, the\n-     * space will be zeroed when it is allocated; this is normally not\n-     * necessary, but in the case of automatic rooting in match\n-     * statements it is possible to have temporaries that may not get\n-     * initialized if a certain arm is not taken, so we must zero\n-     * them. You must arrange any cleanups etc yourself!\n-     */\n-\n     let llty = type_of::type_of(bcx.ccx(), ty);\n     let scratch = alloca(bcx, llty, name);\n     Datum::new(scratch, ty, Rvalue::new(ByRef))\n }\n \n+/// Indicates the \"appropriate\" mode for this value, which is either by ref or by value, depending\n+/// on whether type is immediate or not.\n pub fn appropriate_rvalue_mode<'a, 'tcx>(ccx: &CrateContext<'a, 'tcx>,\n                                          ty: Ty<'tcx>) -> RvalueMode {\n-    /*!\n-     * Indicates the \"appropriate\" mode for this value,\n-     * which is either by ref or by value, depending\n-     * on whether type is immediate or not.\n-     */\n-\n     if type_is_immediate(ccx, ty) {\n         ByValue\n     } else {\n@@ -234,17 +218,13 @@ impl KindOps for Rvalue {\n }\n \n impl KindOps for Lvalue {\n+    /// If an lvalue is moved, we must zero out the memory in which it resides so as to cancel\n+    /// cleanup. If an @T lvalue is copied, we must increment the reference count.\n     fn post_store<'blk, 'tcx>(&self,\n                               bcx: Block<'blk, 'tcx>,\n                               val: ValueRef,\n                               ty: Ty<'tcx>)\n                               -> Block<'blk, 'tcx> {\n-        /*!\n-         * If an lvalue is moved, we must zero out the memory in which\n-         * it resides so as to cancel cleanup. If an @T lvalue is\n-         * copied, we must increment the reference count.\n-         */\n-\n         if ty::type_needs_drop(bcx.tcx(), ty) {\n             // cancel cleanup of affine values by zeroing out\n             let () = zero_mem(bcx, val, ty);\n@@ -288,31 +268,24 @@ impl KindOps for Expr {\n }\n \n impl<'tcx> Datum<'tcx, Rvalue> {\n+    /// Schedules a cleanup for this datum in the given scope. That means that this datum is no\n+    /// longer an rvalue datum; hence, this function consumes the datum and returns the contained\n+    /// ValueRef.\n     pub fn add_clean<'a>(self,\n                          fcx: &FunctionContext<'a, 'tcx>,\n                          scope: cleanup::ScopeId)\n                          -> ValueRef {\n-        /*!\n-         * Schedules a cleanup for this datum in the given scope.\n-         * That means that this datum is no longer an rvalue datum;\n-         * hence, this function consumes the datum and returns the\n-         * contained ValueRef.\n-         */\n-\n         add_rvalue_clean(self.kind.mode, fcx, scope, self.val, self.ty);\n         self.val\n     }\n \n+    /// Returns an lvalue datum (that is, a by ref datum with cleanup scheduled). If `self` is not\n+    /// already an lvalue, cleanup will be scheduled in the temporary scope for `expr_id`.\n     pub fn to_lvalue_datum_in_scope<'blk>(self,\n                                           bcx: Block<'blk, 'tcx>,\n                                           name: &str,\n                                           scope: cleanup::ScopeId)\n                                           -> DatumBlock<'blk, 'tcx, Lvalue> {\n-        /*!\n-         * Returns an lvalue datum (that is, a by ref datum with\n-         * cleanup scheduled). If `self` is not already an lvalue,\n-         * cleanup will be scheduled in the temporary scope for `expr_id`.\n-         */\n         let fcx = bcx.fcx;\n \n         match self.kind.mode {\n@@ -381,22 +354,16 @@ impl<'tcx> Datum<'tcx, Expr> {\n         }\n     }\n \n+    /// Asserts that this datum *is* an lvalue and returns it.\n     #[allow(dead_code)] // potentially useful\n     pub fn assert_lvalue(self, bcx: Block) -> Datum<'tcx, Lvalue> {\n-        /*!\n-         * Asserts that this datum *is* an lvalue and returns it.\n-         */\n-\n         self.match_kind(\n             |d| d,\n             |_| bcx.sess().bug(\"assert_lvalue given rvalue\"))\n     }\n \n+    /// Asserts that this datum *is* an lvalue and returns it.\n     pub fn assert_rvalue(self, bcx: Block) -> Datum<'tcx, Rvalue> {\n-        /*!\n-         * Asserts that this datum *is* an lvalue and returns it.\n-         */\n-\n         self.match_kind(\n             |_| bcx.sess().bug(\"assert_rvalue given lvalue\"),\n             |r| r)\n@@ -418,14 +385,11 @@ impl<'tcx> Datum<'tcx, Expr> {\n         }\n     }\n \n+    /// Arranges cleanup for `self` if it is an rvalue. Use when you are done working with a value\n+    /// that may need drop.\n     pub fn add_clean_if_rvalue<'blk>(self,\n                                      bcx: Block<'blk, 'tcx>,\n                                      expr_id: ast::NodeId) {\n-        /*!\n-         * Arranges cleanup for `self` if it is an rvalue. Use when\n-         * you are done working with a value that may need drop.\n-         */\n-\n         self.match_kind(\n             |_| { /* Nothing to do, cleanup already arranged */ },\n             |r| {\n@@ -434,16 +398,12 @@ impl<'tcx> Datum<'tcx, Expr> {\n             })\n     }\n \n+    /// Ensures that `self` will get cleaned up, if it is not an lvalue already.\n     pub fn clean<'blk>(self,\n                        bcx: Block<'blk, 'tcx>,\n                        name: &'static str,\n                        expr_id: ast::NodeId)\n                        -> Block<'blk, 'tcx> {\n-        /*!\n-         * Ensures that `self` will get cleaned up, if it is not an lvalue\n-         * already.\n-         */\n-\n         self.to_lvalue_datum(bcx, name, expr_id).bcx\n     }\n \n@@ -464,15 +424,11 @@ impl<'tcx> Datum<'tcx, Expr> {\n             })\n     }\n \n+    /// Ensures that we have an rvalue datum (that is, a datum with no cleanup scheduled).\n     pub fn to_rvalue_datum<'blk>(self,\n                                  bcx: Block<'blk, 'tcx>,\n                                  name: &'static str)\n                                  -> DatumBlock<'blk, 'tcx, Rvalue> {\n-        /*!\n-         * Ensures that we have an rvalue datum (that is, a datum with\n-         * no cleanup scheduled).\n-         */\n-\n         self.match_kind(\n             |l| {\n                 let mut bcx = bcx;\n@@ -501,12 +457,9 @@ impl<'tcx> Datum<'tcx, Expr> {\n  * from an array.\n  */\n impl<'tcx> Datum<'tcx, Lvalue> {\n+    /// Converts a datum into a by-ref value. The datum type must be one which is always passed by\n+    /// reference.\n     pub fn to_llref(self) -> ValueRef {\n-        /*!\n-         * Converts a datum into a by-ref value. The datum type must\n-         * be one which is always passed by reference.\n-         */\n-\n         self.val\n     }\n \n@@ -555,40 +508,30 @@ impl<'tcx, K: KindOps + fmt::Show> Datum<'tcx, K> {\n         Datum { val: val, ty: ty, kind: kind.to_expr_kind() }\n     }\n \n+    /// Moves or copies this value into a new home, as appropriate depending on the type of the\n+    /// datum. This method consumes the datum, since it would be incorrect to go on using the datum\n+    /// if the value represented is affine (and hence the value is moved).\n     pub fn store_to<'blk>(self,\n                           bcx: Block<'blk, 'tcx>,\n                           dst: ValueRef)\n                           -> Block<'blk, 'tcx> {\n-        /*!\n-         * Moves or copies this value into a new home, as appropriate\n-         * depending on the type of the datum. This method consumes\n-         * the datum, since it would be incorrect to go on using the\n-         * datum if the value represented is affine (and hence the value\n-         * is moved).\n-         */\n-\n         self.shallow_copy_raw(bcx, dst);\n \n         self.kind.post_store(bcx, self.val, self.ty)\n     }\n \n+    /// Helper function that performs a shallow copy of this value into `dst`, which should be a\n+    /// pointer to a memory location suitable for `self.ty`. `dst` should contain uninitialized\n+    /// memory (either newly allocated, zeroed, or dropped).\n+    ///\n+    /// This function is private to datums because it leaves memory in an unstable state, where the\n+    /// source value has been copied but not zeroed. Public methods are `store_to` (if you no\n+    /// longer need the source value) or `shallow_copy` (if you wish the source value to remain\n+    /// valid).\n     fn shallow_copy_raw<'blk>(&self,\n                               bcx: Block<'blk, 'tcx>,\n                               dst: ValueRef)\n                               -> Block<'blk, 'tcx> {\n-        /*!\n-         * Helper function that performs a shallow copy of this value\n-         * into `dst`, which should be a pointer to a memory location\n-         * suitable for `self.ty`. `dst` should contain uninitialized\n-         * memory (either newly allocated, zeroed, or dropped).\n-         *\n-         * This function is private to datums because it leaves memory\n-         * in an unstable state, where the source value has been\n-         * copied but not zeroed. Public methods are `store_to`\n-         * (if you no longer need the source value) or `shallow_copy`\n-         * (if you wish the source value to remain valid).\n-         */\n-\n         let _icx = push_ctxt(\"copy_to_no_check\");\n \n         if type_is_zero_size(bcx.ccx(), self.ty) {\n@@ -604,17 +547,13 @@ impl<'tcx, K: KindOps + fmt::Show> Datum<'tcx, K> {\n         return bcx;\n     }\n \n+    /// Copies the value into a new location. This function always preserves the existing datum as\n+    /// a valid value. Therefore, it does not consume `self` and, also, cannot be applied to affine\n+    /// values (since they must never be duplicated).\n     pub fn shallow_copy<'blk>(&self,\n                               bcx: Block<'blk, 'tcx>,\n                               dst: ValueRef)\n                               -> Block<'blk, 'tcx> {\n-        /*!\n-         * Copies the value into a new location. This function always\n-         * preserves the existing datum as a valid value. Therefore,\n-         * it does not consume `self` and, also, cannot be applied to\n-         * affine values (since they must never be duplicated).\n-         */\n-\n         assert!(!ty::type_moves_by_default(bcx.tcx(), self.ty));\n         self.shallow_copy_raw(bcx, dst)\n     }\n@@ -627,23 +566,17 @@ impl<'tcx, K: KindOps + fmt::Show> Datum<'tcx, K> {\n                 self.kind)\n     }\n \n+    //! See the `appropriate_rvalue_mode()` function\n     pub fn appropriate_rvalue_mode<'a>(&self, ccx: &CrateContext<'a, 'tcx>)\n                                        -> RvalueMode {\n-        /*! See the `appropriate_rvalue_mode()` function */\n-\n         appropriate_rvalue_mode(ccx, self.ty)\n     }\n \n+    /// Converts `self` into a by-value `ValueRef`. Consumes this datum (i.e., absolves you of\n+    /// responsibility to cleanup the value). For this to work, the value must be something\n+    /// scalar-ish (like an int or a pointer) which (1) does not require drop glue and (2) is\n+    /// naturally passed around by value, and not by reference.\n     pub fn to_llscalarish<'blk>(self, bcx: Block<'blk, 'tcx>) -> ValueRef {\n-        /*!\n-         * Converts `self` into a by-value `ValueRef`. Consumes this\n-         * datum (i.e., absolves you of responsibility to cleanup the\n-         * value). For this to work, the value must be something\n-         * scalar-ish (like an int or a pointer) which (1) does not\n-         * require drop glue and (2) is naturally passed around by\n-         * value, and not by reference.\n-         */\n-\n         assert!(!ty::type_needs_drop(bcx.tcx(), self.ty));\n         assert!(self.appropriate_rvalue_mode(bcx.ccx()) == ByValue);\n         if self.kind.is_by_ref() {"}, {"sha": "c35de3209c61f087566f87ab65495783bdf0c5cd", "filename": "src/librustc_trans/trans/debuginfo.rs", "status": "modified", "additions": 174, "deletions": 175, "changes": 349, "blob_url": "https://github.com/rust-lang/rust/blob/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc_trans%2Ftrans%2Fdebuginfo.rs", "raw_url": "https://github.com/rust-lang/rust/raw/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc_trans%2Ftrans%2Fdebuginfo.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_trans%2Ftrans%2Fdebuginfo.rs?ref=cd5c8235c5448a7234548c772468c8d2e8f150d9", "patch": "@@ -8,181 +8,180 @@\n // option. This file may not be copied, modified, or distributed\n // except according to those terms.\n \n-/*!\n-# Debug Info Module\n-\n-This module serves the purpose of generating debug symbols. We use LLVM's\n-[source level debugging](http://llvm.org/docs/SourceLevelDebugging.html)\n-features for generating the debug information. The general principle is this:\n-\n-Given the right metadata in the LLVM IR, the LLVM code generator is able to\n-create DWARF debug symbols for the given code. The\n-[metadata](http://llvm.org/docs/LangRef.html#metadata-type) is structured much\n-like DWARF *debugging information entries* (DIE), representing type information\n-such as datatype layout, function signatures, block layout, variable location\n-and scope information, etc. It is the purpose of this module to generate correct\n-metadata and insert it into the LLVM IR.\n-\n-As the exact format of metadata trees may change between different LLVM\n-versions, we now use LLVM\n-[DIBuilder](http://llvm.org/docs/doxygen/html/classllvm_1_1DIBuilder.html) to\n-create metadata where possible. This will hopefully ease the adaption of this\n-module to future LLVM versions.\n-\n-The public API of the module is a set of functions that will insert the correct\n-metadata into the LLVM IR when called with the right parameters. The module is\n-thus driven from an outside client with functions like\n-`debuginfo::create_local_var_metadata(bcx: block, local: &ast::local)`.\n-\n-Internally the module will try to reuse already created metadata by utilizing a\n-cache. The way to get a shared metadata node when needed is thus to just call\n-the corresponding function in this module:\n-\n-    let file_metadata = file_metadata(crate_context, path);\n-\n-The function will take care of probing the cache for an existing node for that\n-exact file path.\n-\n-All private state used by the module is stored within either the\n-CrateDebugContext struct (owned by the CrateContext) or the FunctionDebugContext\n-(owned by the FunctionContext).\n-\n-This file consists of three conceptual sections:\n-1. The public interface of the module\n-2. Module-internal metadata creation functions\n-3. Minor utility functions\n-\n-\n-## Recursive Types\n-\n-Some kinds of types, such as structs and enums can be recursive. That means that\n-the type definition of some type X refers to some other type which in turn\n-(transitively) refers to X. This introduces cycles into the type referral graph.\n-A naive algorithm doing an on-demand, depth-first traversal of this graph when\n-describing types, can get trapped in an endless loop when it reaches such a\n-cycle.\n-\n-For example, the following simple type for a singly-linked list...\n-\n-```\n-struct List {\n-    value: int,\n-    tail: Option<Box<List>>,\n-}\n-```\n-\n-will generate the following callstack with a naive DFS algorithm:\n-\n-```\n-describe(t = List)\n-  describe(t = int)\n-  describe(t = Option<Box<List>>)\n-    describe(t = Box<List>)\n-      describe(t = List) // at the beginning again...\n-      ...\n-```\n-\n-To break cycles like these, we use \"forward declarations\". That is, when the\n-algorithm encounters a possibly recursive type (any struct or enum), it\n-immediately creates a type description node and inserts it into the cache\n-*before* describing the members of the type. This type description is just a\n-stub (as type members are not described and added to it yet) but it allows the\n-algorithm to already refer to the type. After the stub is inserted into the\n-cache, the algorithm continues as before. If it now encounters a recursive\n-reference, it will hit the cache and does not try to describe the type anew.\n-\n-This behaviour is encapsulated in the 'RecursiveTypeDescription' enum, which\n-represents a kind of continuation, storing all state needed to continue\n-traversal at the type members after the type has been registered with the cache.\n-(This implementation approach might be a tad over-engineered and may change in\n-the future)\n-\n-\n-## Source Locations and Line Information\n-\n-In addition to data type descriptions the debugging information must also allow\n-to map machine code locations back to source code locations in order to be useful.\n-This functionality is also handled in this module. The following functions allow\n-to control source mappings:\n-\n-+ set_source_location()\n-+ clear_source_location()\n-+ start_emitting_source_locations()\n-\n-`set_source_location()` allows to set the current source location. All IR\n-instructions created after a call to this function will be linked to the given\n-source location, until another location is specified with\n-`set_source_location()` or the source location is cleared with\n-`clear_source_location()`. In the later case, subsequent IR instruction will not\n-be linked to any source location. As you can see, this is a stateful API\n-(mimicking the one in LLVM), so be careful with source locations set by previous\n-calls. It's probably best to not rely on any specific state being present at a\n-given point in code.\n-\n-One topic that deserves some extra attention is *function prologues*. At the\n-beginning of a function's machine code there are typically a few instructions\n-for loading argument values into allocas and checking if there's enough stack\n-space for the function to execute. This *prologue* is not visible in the source\n-code and LLVM puts a special PROLOGUE END marker into the line table at the\n-first non-prologue instruction of the function. In order to find out where the\n-prologue ends, LLVM looks for the first instruction in the function body that is\n-linked to a source location. So, when generating prologue instructions we have\n-to make sure that we don't emit source location information until the 'real'\n-function body begins. For this reason, source location emission is disabled by\n-default for any new function being translated and is only activated after a call\n-to the third function from the list above, `start_emitting_source_locations()`.\n-This function should be called right before regularly starting to translate the\n-top-level block of the given function.\n-\n-There is one exception to the above rule: `llvm.dbg.declare` instruction must be\n-linked to the source location of the variable being declared. For function\n-parameters these `llvm.dbg.declare` instructions typically occur in the middle\n-of the prologue, however, they are ignored by LLVM's prologue detection. The\n-`create_argument_metadata()` and related functions take care of linking the\n-`llvm.dbg.declare` instructions to the correct source locations even while\n-source location emission is still disabled, so there is no need to do anything\n-special with source location handling here.\n-\n-## Unique Type Identification\n-\n-In order for link-time optimization to work properly, LLVM needs a unique type\n-identifier that tells it across compilation units which types are the same as\n-others. This type identifier is created by TypeMap::get_unique_type_id_of_type()\n-using the following algorithm:\n-\n-(1) Primitive types have their name as ID\n-(2) Structs, enums and traits have a multipart identifier\n-\n-    (1) The first part is the SVH (strict version hash) of the crate they were\n-        originally defined in\n-\n-    (2) The second part is the ast::NodeId of the definition in their original\n-        crate\n-\n-    (3) The final part is a concatenation of the type IDs of their concrete type\n-        arguments if they are generic types.\n-\n-(3) Tuple-, pointer and function types are structurally identified, which means\n-    that they are equivalent if their component types are equivalent (i.e. (int,\n-    int) is the same regardless in which crate it is used).\n-\n-This algorithm also provides a stable ID for types that are defined in one crate\n-but instantiated from metadata within another crate. We just have to take care\n-to always map crate and node IDs back to the original crate context.\n-\n-As a side-effect these unique type IDs also help to solve a problem arising from\n-lifetime parameters. Since lifetime parameters are completely omitted in\n-debuginfo, more than one `Ty` instance may map to the same debuginfo type\n-metadata, that is, some struct `Struct<'a>` may have N instantiations with\n-different concrete substitutions for `'a`, and thus there will be N `Ty`\n-instances for the type `Struct<'a>` even though it is not generic otherwise.\n-Unfortunately this means that we cannot use `ty::type_id()` as cheap identifier\n-for type metadata---we have done this in the past, but it led to unnecessary\n-metadata duplication in the best case and LLVM assertions in the worst. However,\n-the unique type ID as described above *can* be used as identifier. Since it is\n-comparatively expensive to construct, though, `ty::type_id()` is still used\n-additionally as an optimization for cases where the exact same type has been\n-seen before (which is most of the time). */\n+//! # Debug Info Module\n+//!\n+//! This module serves the purpose of generating debug symbols. We use LLVM's\n+//! [source level debugging](http://llvm.org/docs/SourceLevelDebugging.html)\n+//! features for generating the debug information. The general principle is this:\n+//!\n+//! Given the right metadata in the LLVM IR, the LLVM code generator is able to\n+//! create DWARF debug symbols for the given code. The\n+//! [metadata](http://llvm.org/docs/LangRef.html#metadata-type) is structured much\n+//! like DWARF *debugging information entries* (DIE), representing type information\n+//! such as datatype layout, function signatures, block layout, variable location\n+//! and scope information, etc. It is the purpose of this module to generate correct\n+//! metadata and insert it into the LLVM IR.\n+//!\n+//! As the exact format of metadata trees may change between different LLVM\n+//! versions, we now use LLVM\n+//! [DIBuilder](http://llvm.org/docs/doxygen/html/classllvm_1_1DIBuilder.html) to\n+//! create metadata where possible. This will hopefully ease the adaption of this\n+//! module to future LLVM versions.\n+//!\n+//! The public API of the module is a set of functions that will insert the correct\n+//! metadata into the LLVM IR when called with the right parameters. The module is\n+//! thus driven from an outside client with functions like\n+//! `debuginfo::create_local_var_metadata(bcx: block, local: &ast::local)`.\n+//!\n+//! Internally the module will try to reuse already created metadata by utilizing a\n+//! cache. The way to get a shared metadata node when needed is thus to just call\n+//! the corresponding function in this module:\n+//!\n+//!     let file_metadata = file_metadata(crate_context, path);\n+//!\n+//! The function will take care of probing the cache for an existing node for that\n+//! exact file path.\n+//!\n+//! All private state used by the module is stored within either the\n+//! CrateDebugContext struct (owned by the CrateContext) or the FunctionDebugContext\n+//! (owned by the FunctionContext).\n+//!\n+//! This file consists of three conceptual sections:\n+//! 1. The public interface of the module\n+//! 2. Module-internal metadata creation functions\n+//! 3. Minor utility functions\n+//!\n+//!\n+//! ## Recursive Types\n+//!\n+//! Some kinds of types, such as structs and enums can be recursive. That means that\n+//! the type definition of some type X refers to some other type which in turn\n+//! (transitively) refers to X. This introduces cycles into the type referral graph.\n+//! A naive algorithm doing an on-demand, depth-first traversal of this graph when\n+//! describing types, can get trapped in an endless loop when it reaches such a\n+//! cycle.\n+//!\n+//! For example, the following simple type for a singly-linked list...\n+//!\n+//! ```\n+//! struct List {\n+//!     value: int,\n+//!     tail: Option<Box<List>>,\n+//! }\n+//! ```\n+//!\n+//! will generate the following callstack with a naive DFS algorithm:\n+//!\n+//! ```\n+//! describe(t = List)\n+//!   describe(t = int)\n+//!   describe(t = Option<Box<List>>)\n+//!     describe(t = Box<List>)\n+//!       describe(t = List) // at the beginning again...\n+//!       ...\n+//! ```\n+//!\n+//! To break cycles like these, we use \"forward declarations\". That is, when the\n+//! algorithm encounters a possibly recursive type (any struct or enum), it\n+//! immediately creates a type description node and inserts it into the cache\n+//! *before* describing the members of the type. This type description is just a\n+//! stub (as type members are not described and added to it yet) but it allows the\n+//! algorithm to already refer to the type. After the stub is inserted into the\n+//! cache, the algorithm continues as before. If it now encounters a recursive\n+//! reference, it will hit the cache and does not try to describe the type anew.\n+//!\n+//! This behaviour is encapsulated in the 'RecursiveTypeDescription' enum, which\n+//! represents a kind of continuation, storing all state needed to continue\n+//! traversal at the type members after the type has been registered with the cache.\n+//! (This implementation approach might be a tad over-engineered and may change in\n+//! the future)\n+//!\n+//!\n+//! ## Source Locations and Line Information\n+//!\n+//! In addition to data type descriptions the debugging information must also allow\n+//! to map machine code locations back to source code locations in order to be useful.\n+//! This functionality is also handled in this module. The following functions allow\n+//! to control source mappings:\n+//!\n+//! + set_source_location()\n+//! + clear_source_location()\n+//! + start_emitting_source_locations()\n+//!\n+//! `set_source_location()` allows to set the current source location. All IR\n+//! instructions created after a call to this function will be linked to the given\n+//! source location, until another location is specified with\n+//! `set_source_location()` or the source location is cleared with\n+//! `clear_source_location()`. In the later case, subsequent IR instruction will not\n+//! be linked to any source location. As you can see, this is a stateful API\n+//! (mimicking the one in LLVM), so be careful with source locations set by previous\n+//! calls. It's probably best to not rely on any specific state being present at a\n+//! given point in code.\n+//!\n+//! One topic that deserves some extra attention is *function prologues*. At the\n+//! beginning of a function's machine code there are typically a few instructions\n+//! for loading argument values into allocas and checking if there's enough stack\n+//! space for the function to execute. This *prologue* is not visible in the source\n+//! code and LLVM puts a special PROLOGUE END marker into the line table at the\n+//! first non-prologue instruction of the function. In order to find out where the\n+//! prologue ends, LLVM looks for the first instruction in the function body that is\n+//! linked to a source location. So, when generating prologue instructions we have\n+//! to make sure that we don't emit source location information until the 'real'\n+//! function body begins. For this reason, source location emission is disabled by\n+//! default for any new function being translated and is only activated after a call\n+//! to the third function from the list above, `start_emitting_source_locations()`.\n+//! This function should be called right before regularly starting to translate the\n+//! top-level block of the given function.\n+//!\n+//! There is one exception to the above rule: `llvm.dbg.declare` instruction must be\n+//! linked to the source location of the variable being declared. For function\n+//! parameters these `llvm.dbg.declare` instructions typically occur in the middle\n+//! of the prologue, however, they are ignored by LLVM's prologue detection. The\n+//! `create_argument_metadata()` and related functions take care of linking the\n+//! `llvm.dbg.declare` instructions to the correct source locations even while\n+//! source location emission is still disabled, so there is no need to do anything\n+//! special with source location handling here.\n+//!\n+//! ## Unique Type Identification\n+//!\n+//! In order for link-time optimization to work properly, LLVM needs a unique type\n+//! identifier that tells it across compilation units which types are the same as\n+//! others. This type identifier is created by TypeMap::get_unique_type_id_of_type()\n+//! using the following algorithm:\n+//!\n+//! (1) Primitive types have their name as ID\n+//! (2) Structs, enums and traits have a multipart identifier\n+//!\n+//!     (1) The first part is the SVH (strict version hash) of the crate they were\n+//!         originally defined in\n+//!\n+//!     (2) The second part is the ast::NodeId of the definition in their original\n+//!         crate\n+//!\n+//!     (3) The final part is a concatenation of the type IDs of their concrete type\n+//!         arguments if they are generic types.\n+//!\n+//! (3) Tuple-, pointer and function types are structurally identified, which means\n+//!     that they are equivalent if their component types are equivalent (i.e. (int,\n+//!     int) is the same regardless in which crate it is used).\n+//!\n+//! This algorithm also provides a stable ID for types that are defined in one crate\n+//! but instantiated from metadata within another crate. We just have to take care\n+//! to always map crate and node IDs back to the original crate context.\n+//!\n+//! As a side-effect these unique type IDs also help to solve a problem arising from\n+//! lifetime parameters. Since lifetime parameters are completely omitted in\n+//! debuginfo, more than one `Ty` instance may map to the same debuginfo type\n+//! metadata, that is, some struct `Struct<'a>` may have N instantiations with\n+//! different concrete substitutions for `'a`, and thus there will be N `Ty`\n+//! instances for the type `Struct<'a>` even though it is not generic otherwise.\n+//! Unfortunately this means that we cannot use `ty::type_id()` as cheap identifier\n+//! for type metadata---we have done this in the past, but it led to unnecessary\n+//! metadata duplication in the best case and LLVM assertions in the worst. However,\n+//! the unique type ID as described above *can* be used as identifier. Since it is\n+//! comparatively expensive to construct, though, `ty::type_id()` is still used\n+//! additionally as an optimization for cases where the exact same type has been\n+//! seen before (which is most of the time).\n use self::FunctionDebugContextRepr::*;\n use self::VariableAccess::*;\n use self::VariableKind::*;"}, {"sha": "c3ab8986372ad14b8d35106f599baa95cfd664d3", "filename": "src/librustc_trans/trans/doc.rs", "status": "modified", "additions": 223, "deletions": 227, "changes": 450, "blob_url": "https://github.com/rust-lang/rust/blob/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc_trans%2Ftrans%2Fdoc.rs", "raw_url": "https://github.com/rust-lang/rust/raw/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc_trans%2Ftrans%2Fdoc.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_trans%2Ftrans%2Fdoc.rs?ref=cd5c8235c5448a7234548c772468c8d2e8f150d9", "patch": "@@ -8,230 +8,226 @@\n // option. This file may not be copied, modified, or distributed\n // except according to those terms.\n \n-/*!\n-\n-# Documentation for the trans module\n-\n-This module contains high-level summaries of how the various modules\n-in trans work. It is a work in progress. For detailed comments,\n-naturally, you can refer to the individual modules themselves.\n-\n-## The Expr module\n-\n-The expr module handles translation of expressions. The most general\n-translation routine is `trans()`, which will translate an expression\n-into a datum. `trans_into()` is also available, which will translate\n-an expression and write the result directly into memory, sometimes\n-avoiding the need for a temporary stack slot. Finally,\n-`trans_to_lvalue()` is available if you'd like to ensure that the\n-result has cleanup scheduled.\n-\n-Internally, each of these functions dispatches to various other\n-expression functions depending on the kind of expression. We divide\n-up expressions into:\n-\n-- **Datum expressions:** Those that most naturally yield values.\n-  Examples would be `22`, `box x`, or `a + b` (when not overloaded).\n-- **DPS expressions:** Those that most naturally write into a location\n-  in memory. Examples would be `foo()` or `Point { x: 3, y: 4 }`.\n-- **Statement expressions:** That that do not generate a meaningful\n-  result. Examples would be `while { ... }` or `return 44`.\n-\n-## The Datum module\n-\n-A `Datum` encapsulates the result of evaluating a Rust expression.  It\n-contains a `ValueRef` indicating the result, a `Ty` describing\n-the Rust type, but also a *kind*. The kind indicates whether the datum\n-has cleanup scheduled (lvalue) or not (rvalue) and -- in the case of\n-rvalues -- whether or not the value is \"by ref\" or \"by value\".\n-\n-The datum API is designed to try and help you avoid memory errors like\n-forgetting to arrange cleanup or duplicating a value. The type of the\n-datum incorporates the kind, and thus reflects whether it has cleanup\n-scheduled:\n-\n-- `Datum<Lvalue>` -- by ref, cleanup scheduled\n-- `Datum<Rvalue>` -- by value or by ref, no cleanup scheduled\n-- `Datum<Expr>` -- either `Datum<Lvalue>` or `Datum<Rvalue>`\n-\n-Rvalue and expr datums are noncopyable, and most of the methods on\n-datums consume the datum itself (with some notable exceptions). This\n-reflects the fact that datums may represent affine values which ought\n-to be consumed exactly once, and if you were to try to (for example)\n-store an affine value multiple times, you would be duplicating it,\n-which would certainly be a bug.\n-\n-Some of the datum methods, however, are designed to work only on\n-copyable values such as ints or pointers. Those methods may borrow the\n-datum (`&self`) rather than consume it, but they always include\n-assertions on the type of the value represented to check that this\n-makes sense. An example is `shallow_copy()`, which duplicates\n-a datum value.\n-\n-Translating an expression always yields a `Datum<Expr>` result, but\n-the methods `to_[lr]value_datum()` can be used to coerce a\n-`Datum<Expr>` into a `Datum<Lvalue>` or `Datum<Rvalue>` as\n-needed. Coercing to an lvalue is fairly common, and generally occurs\n-whenever it is necessary to inspect a value and pull out its\n-subcomponents (for example, a match, or indexing expression). Coercing\n-to an rvalue is more unusual; it occurs when moving values from place\n-to place, such as in an assignment expression or parameter passing.\n-\n-### Lvalues in detail\n-\n-An lvalue datum is one for which cleanup has been scheduled. Lvalue\n-datums are always located in memory, and thus the `ValueRef` for an\n-LLVM value is always a pointer to the actual Rust value. This means\n-that if the Datum has a Rust type of `int`, then the LLVM type of the\n-`ValueRef` will be `int*` (pointer to int).\n-\n-Because lvalues already have cleanups scheduled, the memory must be\n-zeroed to prevent the cleanup from taking place (presuming that the\n-Rust type needs drop in the first place, otherwise it doesn't\n-matter). The Datum code automatically performs this zeroing when the\n-value is stored to a new location, for example.\n-\n-Lvalues usually result from evaluating lvalue expressions. For\n-example, evaluating a local variable `x` yields an lvalue, as does a\n-reference to a field like `x.f` or an index `x[i]`.\n-\n-Lvalue datums can also arise by *converting* an rvalue into an lvalue.\n-This is done with the `to_lvalue_datum` method defined on\n-`Datum<Expr>`. Basically this method just schedules cleanup if the\n-datum is an rvalue, possibly storing the value into a stack slot first\n-if needed. Converting rvalues into lvalues occurs in constructs like\n-`&foo()` or `match foo() { ref x => ... }`, where the user is\n-implicitly requesting a temporary.\n-\n-Somewhat surprisingly, not all lvalue expressions yield lvalue datums\n-when trans'd. Ultimately the reason for this is to micro-optimize\n-the resulting LLVM. For example, consider the following code:\n-\n-    fn foo() -> Box<int> { ... }\n-    let x = *foo();\n-\n-The expression `*foo()` is an lvalue, but if you invoke `expr::trans`,\n-it will return an rvalue datum. See `deref_once` in expr.rs for\n-more details.\n-\n-### Rvalues in detail\n-\n-Rvalues datums are values with no cleanup scheduled. One must be\n-careful with rvalue datums to ensure that cleanup is properly\n-arranged, usually by converting to an lvalue datum or by invoking the\n-`add_clean` method.\n-\n-### Scratch datums\n-\n-Sometimes you need some temporary scratch space.  The functions\n-`[lr]value_scratch_datum()` can be used to get temporary stack\n-space. As their name suggests, they yield lvalues and rvalues\n-respectively. That is, the slot from `lvalue_scratch_datum` will have\n-cleanup arranged, and the slot from `rvalue_scratch_datum` does not.\n-\n-## The Cleanup module\n-\n-The cleanup module tracks what values need to be cleaned up as scopes\n-are exited, either via panic or just normal control flow. The basic\n-idea is that the function context maintains a stack of cleanup scopes\n-that are pushed/popped as we traverse the AST tree. There is typically\n-at least one cleanup scope per AST node; some AST nodes may introduce\n-additional temporary scopes.\n-\n-Cleanup items can be scheduled into any of the scopes on the stack.\n-Typically, when a scope is popped, we will also generate the code for\n-each of its cleanups at that time. This corresponds to a normal exit\n-from a block (for example, an expression completing evaluation\n-successfully without panic). However, it is also possible to pop a\n-block *without* executing its cleanups; this is typically used to\n-guard intermediate values that must be cleaned up on panic, but not\n-if everything goes right. See the section on custom scopes below for\n-more details.\n-\n-Cleanup scopes come in three kinds:\n-- **AST scopes:** each AST node in a function body has a corresponding\n-  AST scope. We push the AST scope when we start generate code for an AST\n-  node and pop it once the AST node has been fully generated.\n-- **Loop scopes:** loops have an additional cleanup scope. Cleanups are\n-  never scheduled into loop scopes; instead, they are used to record the\n-  basic blocks that we should branch to when a `continue` or `break` statement\n-  is encountered.\n-- **Custom scopes:** custom scopes are typically used to ensure cleanup\n-  of intermediate values.\n-\n-### When to schedule cleanup\n-\n-Although the cleanup system is intended to *feel* fairly declarative,\n-it's still important to time calls to `schedule_clean()` correctly.\n-Basically, you should not schedule cleanup for memory until it has\n-been initialized, because if an unwind should occur before the memory\n-is fully initialized, then the cleanup will run and try to free or\n-drop uninitialized memory. If the initialization itself produces\n-byproducts that need to be freed, then you should use temporary custom\n-scopes to ensure that those byproducts will get freed on unwind.  For\n-example, an expression like `box foo()` will first allocate a box in the\n-heap and then call `foo()` -- if `foo()` should panic, this box needs\n-to be *shallowly* freed.\n-\n-### Long-distance jumps\n-\n-In addition to popping a scope, which corresponds to normal control\n-flow exiting the scope, we may also *jump out* of a scope into some\n-earlier scope on the stack. This can occur in response to a `return`,\n-`break`, or `continue` statement, but also in response to panic. In\n-any of these cases, we will generate a series of cleanup blocks for\n-each of the scopes that is exited. So, if the stack contains scopes A\n-... Z, and we break out of a loop whose corresponding cleanup scope is\n-X, we would generate cleanup blocks for the cleanups in X, Y, and Z.\n-After cleanup is done we would branch to the exit point for scope X.\n-But if panic should occur, we would generate cleanups for all the\n-scopes from A to Z and then resume the unwind process afterwards.\n-\n-To avoid generating tons of code, we cache the cleanup blocks that we\n-create for breaks, returns, unwinds, and other jumps. Whenever a new\n-cleanup is scheduled, though, we must clear these cached blocks. A\n-possible improvement would be to keep the cached blocks but simply\n-generate a new block which performs the additional cleanup and then\n-branches to the existing cached blocks.\n-\n-### AST and loop cleanup scopes\n-\n-AST cleanup scopes are pushed when we begin and end processing an AST\n-node. They are used to house cleanups related to rvalue temporary that\n-get referenced (e.g., due to an expression like `&Foo()`). Whenever an\n-AST scope is popped, we always trans all the cleanups, adding the cleanup\n-code after the postdominator of the AST node.\n-\n-AST nodes that represent breakable loops also push a loop scope; the\n-loop scope never has any actual cleanups, it's just used to point to\n-the basic blocks where control should flow after a \"continue\" or\n-\"break\" statement. Popping a loop scope never generates code.\n-\n-### Custom cleanup scopes\n-\n-Custom cleanup scopes are used for a variety of purposes. The most\n-common though is to handle temporary byproducts, where cleanup only\n-needs to occur on panic. The general strategy is to push a custom\n-cleanup scope, schedule *shallow* cleanups into the custom scope, and\n-then pop the custom scope (without transing the cleanups) when\n-execution succeeds normally. This way the cleanups are only trans'd on\n-unwind, and only up until the point where execution succeeded, at\n-which time the complete value should be stored in an lvalue or some\n-other place where normal cleanup applies.\n-\n-To spell it out, here is an example. Imagine an expression `box expr`.\n-We would basically:\n-\n-1. Push a custom cleanup scope C.\n-2. Allocate the box.\n-3. Schedule a shallow free in the scope C.\n-4. Trans `expr` into the box.\n-5. Pop the scope C.\n-6. Return the box as an rvalue.\n-\n-This way, if a panic occurs while transing `expr`, the custom\n-cleanup scope C is pushed and hence the box will be freed. The trans\n-code for `expr` itself is responsible for freeing any other byproducts\n-that may be in play.\n-\n-*/\n+//! # Documentation for the trans module\n+//!\n+//! This module contains high-level summaries of how the various modules\n+//! in trans work. It is a work in progress. For detailed comments,\n+//! naturally, you can refer to the individual modules themselves.\n+//!\n+//! ## The Expr module\n+//!\n+//! The expr module handles translation of expressions. The most general\n+//! translation routine is `trans()`, which will translate an expression\n+//! into a datum. `trans_into()` is also available, which will translate\n+//! an expression and write the result directly into memory, sometimes\n+//! avoiding the need for a temporary stack slot. Finally,\n+//! `trans_to_lvalue()` is available if you'd like to ensure that the\n+//! result has cleanup scheduled.\n+//!\n+//! Internally, each of these functions dispatches to various other\n+//! expression functions depending on the kind of expression. We divide\n+//! up expressions into:\n+//!\n+//! - **Datum expressions:** Those that most naturally yield values.\n+//!   Examples would be `22`, `box x`, or `a + b` (when not overloaded).\n+//! - **DPS expressions:** Those that most naturally write into a location\n+//!   in memory. Examples would be `foo()` or `Point { x: 3, y: 4 }`.\n+//! - **Statement expressions:** That that do not generate a meaningful\n+//!   result. Examples would be `while { ... }` or `return 44`.\n+//!\n+//! ## The Datum module\n+//!\n+//! A `Datum` encapsulates the result of evaluating a Rust expression.  It\n+//! contains a `ValueRef` indicating the result, a `Ty` describing\n+//! the Rust type, but also a *kind*. The kind indicates whether the datum\n+//! has cleanup scheduled (lvalue) or not (rvalue) and -- in the case of\n+//! rvalues -- whether or not the value is \"by ref\" or \"by value\".\n+//!\n+//! The datum API is designed to try and help you avoid memory errors like\n+//! forgetting to arrange cleanup or duplicating a value. The type of the\n+//! datum incorporates the kind, and thus reflects whether it has cleanup\n+//! scheduled:\n+//!\n+//! - `Datum<Lvalue>` -- by ref, cleanup scheduled\n+//! - `Datum<Rvalue>` -- by value or by ref, no cleanup scheduled\n+//! - `Datum<Expr>` -- either `Datum<Lvalue>` or `Datum<Rvalue>`\n+//!\n+//! Rvalue and expr datums are noncopyable, and most of the methods on\n+//! datums consume the datum itself (with some notable exceptions). This\n+//! reflects the fact that datums may represent affine values which ought\n+//! to be consumed exactly once, and if you were to try to (for example)\n+//! store an affine value multiple times, you would be duplicating it,\n+//! which would certainly be a bug.\n+//!\n+//! Some of the datum methods, however, are designed to work only on\n+//! copyable values such as ints or pointers. Those methods may borrow the\n+//! datum (`&self`) rather than consume it, but they always include\n+//! assertions on the type of the value represented to check that this\n+//! makes sense. An example is `shallow_copy()`, which duplicates\n+//! a datum value.\n+//!\n+//! Translating an expression always yields a `Datum<Expr>` result, but\n+//! the methods `to_[lr]value_datum()` can be used to coerce a\n+//! `Datum<Expr>` into a `Datum<Lvalue>` or `Datum<Rvalue>` as\n+//! needed. Coercing to an lvalue is fairly common, and generally occurs\n+//! whenever it is necessary to inspect a value and pull out its\n+//! subcomponents (for example, a match, or indexing expression). Coercing\n+//! to an rvalue is more unusual; it occurs when moving values from place\n+//! to place, such as in an assignment expression or parameter passing.\n+//!\n+//! ### Lvalues in detail\n+//!\n+//! An lvalue datum is one for which cleanup has been scheduled. Lvalue\n+//! datums are always located in memory, and thus the `ValueRef` for an\n+//! LLVM value is always a pointer to the actual Rust value. This means\n+//! that if the Datum has a Rust type of `int`, then the LLVM type of the\n+//! `ValueRef` will be `int*` (pointer to int).\n+//!\n+//! Because lvalues already have cleanups scheduled, the memory must be\n+//! zeroed to prevent the cleanup from taking place (presuming that the\n+//! Rust type needs drop in the first place, otherwise it doesn't\n+//! matter). The Datum code automatically performs this zeroing when the\n+//! value is stored to a new location, for example.\n+//!\n+//! Lvalues usually result from evaluating lvalue expressions. For\n+//! example, evaluating a local variable `x` yields an lvalue, as does a\n+//! reference to a field like `x.f` or an index `x[i]`.\n+//!\n+//! Lvalue datums can also arise by *converting* an rvalue into an lvalue.\n+//! This is done with the `to_lvalue_datum` method defined on\n+//! `Datum<Expr>`. Basically this method just schedules cleanup if the\n+//! datum is an rvalue, possibly storing the value into a stack slot first\n+//! if needed. Converting rvalues into lvalues occurs in constructs like\n+//! `&foo()` or `match foo() { ref x => ... }`, where the user is\n+//! implicitly requesting a temporary.\n+//!\n+//! Somewhat surprisingly, not all lvalue expressions yield lvalue datums\n+//! when trans'd. Ultimately the reason for this is to micro-optimize\n+//! the resulting LLVM. For example, consider the following code:\n+//!\n+//!     fn foo() -> Box<int> { ... }\n+//!     let x = *foo();\n+//!\n+//! The expression `*foo()` is an lvalue, but if you invoke `expr::trans`,\n+//! it will return an rvalue datum. See `deref_once` in expr.rs for\n+//! more details.\n+//!\n+//! ### Rvalues in detail\n+//!\n+//! Rvalues datums are values with no cleanup scheduled. One must be\n+//! careful with rvalue datums to ensure that cleanup is properly\n+//! arranged, usually by converting to an lvalue datum or by invoking the\n+//! `add_clean` method.\n+//!\n+//! ### Scratch datums\n+//!\n+//! Sometimes you need some temporary scratch space.  The functions\n+//! `[lr]value_scratch_datum()` can be used to get temporary stack\n+//! space. As their name suggests, they yield lvalues and rvalues\n+//! respectively. That is, the slot from `lvalue_scratch_datum` will have\n+//! cleanup arranged, and the slot from `rvalue_scratch_datum` does not.\n+//!\n+//! ## The Cleanup module\n+//!\n+//! The cleanup module tracks what values need to be cleaned up as scopes\n+//! are exited, either via panic or just normal control flow. The basic\n+//! idea is that the function context maintains a stack of cleanup scopes\n+//! that are pushed/popped as we traverse the AST tree. There is typically\n+//! at least one cleanup scope per AST node; some AST nodes may introduce\n+//! additional temporary scopes.\n+//!\n+//! Cleanup items can be scheduled into any of the scopes on the stack.\n+//! Typically, when a scope is popped, we will also generate the code for\n+//! each of its cleanups at that time. This corresponds to a normal exit\n+//! from a block (for example, an expression completing evaluation\n+//! successfully without panic). However, it is also possible to pop a\n+//! block *without* executing its cleanups; this is typically used to\n+//! guard intermediate values that must be cleaned up on panic, but not\n+//! if everything goes right. See the section on custom scopes below for\n+//! more details.\n+//!\n+//! Cleanup scopes come in three kinds:\n+//! - **AST scopes:** each AST node in a function body has a corresponding\n+//!   AST scope. We push the AST scope when we start generate code for an AST\n+//!   node and pop it once the AST node has been fully generated.\n+//! - **Loop scopes:** loops have an additional cleanup scope. Cleanups are\n+//!   never scheduled into loop scopes; instead, they are used to record the\n+//!   basic blocks that we should branch to when a `continue` or `break` statement\n+//!   is encountered.\n+//! - **Custom scopes:** custom scopes are typically used to ensure cleanup\n+//!   of intermediate values.\n+//!\n+//! ### When to schedule cleanup\n+//!\n+//! Although the cleanup system is intended to *feel* fairly declarative,\n+//! it's still important to time calls to `schedule_clean()` correctly.\n+//! Basically, you should not schedule cleanup for memory until it has\n+//! been initialized, because if an unwind should occur before the memory\n+//! is fully initialized, then the cleanup will run and try to free or\n+//! drop uninitialized memory. If the initialization itself produces\n+//! byproducts that need to be freed, then you should use temporary custom\n+//! scopes to ensure that those byproducts will get freed on unwind.  For\n+//! example, an expression like `box foo()` will first allocate a box in the\n+//! heap and then call `foo()` -- if `foo()` should panic, this box needs\n+//! to be *shallowly* freed.\n+//!\n+//! ### Long-distance jumps\n+//!\n+//! In addition to popping a scope, which corresponds to normal control\n+//! flow exiting the scope, we may also *jump out* of a scope into some\n+//! earlier scope on the stack. This can occur in response to a `return`,\n+//! `break`, or `continue` statement, but also in response to panic. In\n+//! any of these cases, we will generate a series of cleanup blocks for\n+//! each of the scopes that is exited. So, if the stack contains scopes A\n+//! ... Z, and we break out of a loop whose corresponding cleanup scope is\n+//! X, we would generate cleanup blocks for the cleanups in X, Y, and Z.\n+//! After cleanup is done we would branch to the exit point for scope X.\n+//! But if panic should occur, we would generate cleanups for all the\n+//! scopes from A to Z and then resume the unwind process afterwards.\n+//!\n+//! To avoid generating tons of code, we cache the cleanup blocks that we\n+//! create for breaks, returns, unwinds, and other jumps. Whenever a new\n+//! cleanup is scheduled, though, we must clear these cached blocks. A\n+//! possible improvement would be to keep the cached blocks but simply\n+//! generate a new block which performs the additional cleanup and then\n+//! branches to the existing cached blocks.\n+//!\n+//! ### AST and loop cleanup scopes\n+//!\n+//! AST cleanup scopes are pushed when we begin and end processing an AST\n+//! node. They are used to house cleanups related to rvalue temporary that\n+//! get referenced (e.g., due to an expression like `&Foo()`). Whenever an\n+//! AST scope is popped, we always trans all the cleanups, adding the cleanup\n+//! code after the postdominator of the AST node.\n+//!\n+//! AST nodes that represent breakable loops also push a loop scope; the\n+//! loop scope never has any actual cleanups, it's just used to point to\n+//! the basic blocks where control should flow after a \"continue\" or\n+//! \"break\" statement. Popping a loop scope never generates code.\n+//!\n+//! ### Custom cleanup scopes\n+//!\n+//! Custom cleanup scopes are used for a variety of purposes. The most\n+//! common though is to handle temporary byproducts, where cleanup only\n+//! needs to occur on panic. The general strategy is to push a custom\n+//! cleanup scope, schedule *shallow* cleanups into the custom scope, and\n+//! then pop the custom scope (without transing the cleanups) when\n+//! execution succeeds normally. This way the cleanups are only trans'd on\n+//! unwind, and only up until the point where execution succeeded, at\n+//! which time the complete value should be stored in an lvalue or some\n+//! other place where normal cleanup applies.\n+//!\n+//! To spell it out, here is an example. Imagine an expression `box expr`.\n+//! We would basically:\n+//!\n+//! 1. Push a custom cleanup scope C.\n+//! 2. Allocate the box.\n+//! 3. Schedule a shallow free in the scope C.\n+//! 4. Trans `expr` into the box.\n+//! 5. Pop the scope C.\n+//! 6. Return the box as an rvalue.\n+//!\n+//! This way, if a panic occurs while transing `expr`, the custom\n+//! cleanup scope C is pushed and hence the box will be freed. The trans\n+//! code for `expr` itself is responsible for freeing any other byproducts\n+//! that may be in play."}, {"sha": "60809c8644d2df3531cf30774d10c8bd580559d5", "filename": "src/librustc_trans/trans/expr.rs", "status": "modified", "additions": 47, "deletions": 80, "changes": 127, "blob_url": "https://github.com/rust-lang/rust/blob/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc_trans%2Ftrans%2Fexpr.rs", "raw_url": "https://github.com/rust-lang/rust/raw/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc_trans%2Ftrans%2Fexpr.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_trans%2Ftrans%2Fexpr.rs?ref=cd5c8235c5448a7234548c772468c8d2e8f150d9", "patch": "@@ -8,28 +8,26 @@\n // option. This file may not be copied, modified, or distributed\n // except according to those terms.\n \n-/*!\n- * # Translation of Expressions\n- *\n- * Public entry points:\n- *\n- * - `trans_into(bcx, expr, dest) -> bcx`: evaluates an expression,\n- *   storing the result into `dest`. This is the preferred form, if you\n- *   can manage it.\n- *\n- * - `trans(bcx, expr) -> DatumBlock`: evaluates an expression, yielding\n- *   `Datum` with the result. You can then store the datum, inspect\n- *   the value, etc. This may introduce temporaries if the datum is a\n- *   structural type.\n- *\n- * - `trans_to_lvalue(bcx, expr, \"...\") -> DatumBlock`: evaluates an\n- *   expression and ensures that the result has a cleanup associated with it,\n- *   creating a temporary stack slot if necessary.\n- *\n- * - `trans_local_var -> Datum`: looks up a local variable or upvar.\n- *\n- * See doc.rs for more comments.\n- */\n+//! # Translation of Expressions\n+//!\n+//! Public entry points:\n+//!\n+//! - `trans_into(bcx, expr, dest) -> bcx`: evaluates an expression,\n+//!   storing the result into `dest`. This is the preferred form, if you\n+//!   can manage it.\n+//!\n+//! - `trans(bcx, expr) -> DatumBlock`: evaluates an expression, yielding\n+//!   `Datum` with the result. You can then store the datum, inspect\n+//!   the value, etc. This may introduce temporaries if the datum is a\n+//!   structural type.\n+//!\n+//! - `trans_to_lvalue(bcx, expr, \"...\") -> DatumBlock`: evaluates an\n+//!   expression and ensures that the result has a cleanup associated with it,\n+//!   creating a temporary stack slot if necessary.\n+//!\n+//! - `trans_local_var -> Datum`: looks up a local variable or upvar.\n+//!\n+//! See doc.rs for more comments.\n \n #![allow(non_camel_case_types)]\n \n@@ -82,15 +80,12 @@ impl Dest {\n     }\n }\n \n+/// This function is equivalent to `trans(bcx, expr).store_to_dest(dest)` but it may generate\n+/// better optimized LLVM code.\n pub fn trans_into<'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n                               expr: &ast::Expr,\n                               dest: Dest)\n                               -> Block<'blk, 'tcx> {\n-    /*!\n-     * This function is equivalent to `trans(bcx, expr).store_to_dest(dest)`\n-     * but it may generate better optimized LLVM code.\n-     */\n-\n     let mut bcx = bcx;\n \n     if bcx.tcx().adjustments.borrow().contains_key(&expr.id) {\n@@ -124,16 +119,12 @@ pub fn trans_into<'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n     bcx.fcx.pop_and_trans_ast_cleanup_scope(bcx, expr.id)\n }\n \n+/// Translates an expression, returning a datum (and new block) encapsulating the result. When\n+/// possible, it is preferred to use `trans_into`, as that may avoid creating a temporary on the\n+/// stack.\n pub fn trans<'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n                          expr: &ast::Expr)\n                          -> DatumBlock<'blk, 'tcx, Expr> {\n-    /*!\n-     * Translates an expression, returning a datum (and new block)\n-     * encapsulating the result. When possible, it is preferred to\n-     * use `trans_into`, as that may avoid creating a temporary on\n-     * the stack.\n-     */\n-\n     debug!(\"trans(expr={})\", bcx.expr_to_string(expr));\n \n     let mut bcx = bcx;\n@@ -157,15 +148,12 @@ pub fn get_dataptr(bcx: Block, fat_ptr: ValueRef) -> ValueRef {\n     GEPi(bcx, fat_ptr, &[0u, abi::FAT_PTR_ADDR])\n }\n \n+/// Helper for trans that apply adjustments from `expr` to `datum`, which should be the unadjusted\n+/// translation of `expr`.\n fn apply_adjustments<'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n                                  expr: &ast::Expr,\n                                  datum: Datum<'tcx, Expr>)\n                                  -> DatumBlock<'blk, 'tcx, Expr> {\n-    /*!\n-     * Helper for trans that apply adjustments from `expr` to `datum`,\n-     * which should be the unadjusted translation of `expr`.\n-     */\n-\n     let mut bcx = bcx;\n     let mut datum = datum;\n     let adjustment = match bcx.tcx().adjustments.borrow().get(&expr.id).cloned() {\n@@ -480,34 +468,27 @@ fn apply_adjustments<'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n     }\n }\n \n+/// Translates an expression in \"lvalue\" mode -- meaning that it returns a reference to the memory\n+/// that the expr represents.\n+///\n+/// If this expression is an rvalue, this implies introducing a temporary.  In other words,\n+/// something like `x().f` is translated into roughly the equivalent of\n+///\n+///   { tmp = x(); tmp.f }\n pub fn trans_to_lvalue<'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n                                    expr: &ast::Expr,\n                                    name: &str)\n                                    -> DatumBlock<'blk, 'tcx, Lvalue> {\n-    /*!\n-     * Translates an expression in \"lvalue\" mode -- meaning that it\n-     * returns a reference to the memory that the expr represents.\n-     *\n-     * If this expression is an rvalue, this implies introducing a\n-     * temporary.  In other words, something like `x().f` is\n-     * translated into roughly the equivalent of\n-     *\n-     *   { tmp = x(); tmp.f }\n-     */\n-\n     let mut bcx = bcx;\n     let datum = unpack_datum!(bcx, trans(bcx, expr));\n     return datum.to_lvalue_datum(bcx, name, expr.id);\n }\n \n+/// A version of `trans` that ignores adjustments. You almost certainly do not want to call this\n+/// directly.\n fn trans_unadjusted<'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n                                 expr: &ast::Expr)\n                                 -> DatumBlock<'blk, 'tcx, Expr> {\n-    /*!\n-     * A version of `trans` that ignores adjustments. You almost\n-     * certainly do not want to call this directly.\n-     */\n-\n     let mut bcx = bcx;\n \n     debug!(\"trans_unadjusted(expr={})\", bcx.expr_to_string(expr));\n@@ -1218,14 +1199,10 @@ fn trans_def_fn_unadjusted<'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n     DatumBlock::new(bcx, Datum::new(llfn, fn_ty, RvalueExpr(Rvalue::new(ByValue))))\n }\n \n+/// Translates a reference to a local variable or argument. This always results in an lvalue datum.\n pub fn trans_local_var<'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n                                    def: def::Def)\n                                    -> Datum<'tcx, Lvalue> {\n-    /*!\n-     * Translates a reference to a local variable or argument.\n-     * This always results in an lvalue datum.\n-     */\n-\n     let _icx = push_ctxt(\"trans_local_var\");\n \n     match def {\n@@ -1262,18 +1239,14 @@ pub fn trans_local_var<'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n     }\n }\n \n+/// Helper for enumerating the field types of structs, enums, or records. The optional node ID here\n+/// is the node ID of the path identifying the enum variant in use. If none, this cannot possibly\n+/// an enum variant (so, if it is and `node_id_opt` is none, this function panics).\n pub fn with_field_tys<'tcx, R>(tcx: &ty::ctxt<'tcx>,\n                                ty: Ty<'tcx>,\n                                node_id_opt: Option<ast::NodeId>,\n                                op: |ty::Disr, (&[ty::field<'tcx>])| -> R)\n                                -> R {\n-    /*!\n-     * Helper for enumerating the field types of structs, enums, or records.\n-     * The optional node ID here is the node ID of the path identifying the enum\n-     * variant in use. If none, this cannot possibly an enum variant (so, if it\n-     * is and `node_id_opt` is none, this function panics).\n-     */\n-\n     match ty.sty {\n         ty::ty_struct(did, ref substs) => {\n             op(0, struct_fields(tcx, did, substs).as_slice())\n@@ -2189,24 +2162,18 @@ fn deref_once<'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n \n     return r;\n \n+    /// We microoptimize derefs of owned pointers a bit here. Basically, the idea is to make the\n+    /// deref of an rvalue result in an rvalue. This helps to avoid intermediate stack slots in the\n+    /// resulting LLVM. The idea here is that, if the `Box<T>` pointer is an rvalue, then we can\n+    /// schedule a *shallow* free of the `Box<T>` pointer, and then return a ByRef rvalue into the\n+    /// pointer. Because the free is shallow, it is legit to return an rvalue, because we know that\n+    /// the contents are not yet scheduled to be freed. The language rules ensure that the contents\n+    /// will be used (or moved) before the free occurs.\n     fn deref_owned_pointer<'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n                                        expr: &ast::Expr,\n                                        datum: Datum<'tcx, Expr>,\n                                        content_ty: Ty<'tcx>)\n                                        -> DatumBlock<'blk, 'tcx, Expr> {\n-        /*!\n-         * We microoptimize derefs of owned pointers a bit here.\n-         * Basically, the idea is to make the deref of an rvalue\n-         * result in an rvalue. This helps to avoid intermediate stack\n-         * slots in the resulting LLVM. The idea here is that, if the\n-         * `Box<T>` pointer is an rvalue, then we can schedule a *shallow*\n-         * free of the `Box<T>` pointer, and then return a ByRef rvalue\n-         * into the pointer. Because the free is shallow, it is legit\n-         * to return an rvalue, because we know that the contents are\n-         * not yet scheduled to be freed. The language rules ensure that the\n-         * contents will be used (or moved) before the free occurs.\n-         */\n-\n         match datum.kind {\n             RvalueExpr(Rvalue { mode: ByRef }) => {\n                 let scope = cleanup::temporary_scope(bcx.tcx(), expr.id);"}, {"sha": "6f97f6453fd91f5f9176c5c40326ae74153013b6", "filename": "src/librustc_trans/trans/foreign.rs", "status": "modified", "additions": 19, "deletions": 30, "changes": 49, "blob_url": "https://github.com/rust-lang/rust/blob/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc_trans%2Ftrans%2Fforeign.rs", "raw_url": "https://github.com/rust-lang/rust/raw/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc_trans%2Ftrans%2Fforeign.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_trans%2Ftrans%2Fforeign.rs?ref=cd5c8235c5448a7234548c772468c8d2e8f150d9", "patch": "@@ -161,14 +161,10 @@ pub fn register_static(ccx: &CrateContext,\n     }\n }\n \n+/// Registers a foreign function found in a library. Just adds a LLVM global.\n pub fn register_foreign_item_fn<'a, 'tcx>(ccx: &CrateContext<'a, 'tcx>,\n                                           abi: Abi, fty: Ty<'tcx>,\n                                           name: &str) -> ValueRef {\n-    /*!\n-     * Registers a foreign function found in a library.\n-     * Just adds a LLVM global.\n-     */\n-\n     debug!(\"register_foreign_item_fn(abi={}, \\\n             ty={}, \\\n             name={})\",\n@@ -201,30 +197,27 @@ pub fn register_foreign_item_fn<'a, 'tcx>(ccx: &CrateContext<'a, 'tcx>,\n     llfn\n }\n \n+/// Prepares a call to a native function. This requires adapting\n+/// from the Rust argument passing rules to the native rules.\n+///\n+/// # Parameters\n+///\n+/// - `callee_ty`: Rust type for the function we are calling\n+/// - `llfn`: the function pointer we are calling\n+/// - `llretptr`: where to store the return value of the function\n+/// - `llargs_rust`: a list of the argument values, prepared\n+///   as they would be if calling a Rust function\n+/// - `passed_arg_tys`: Rust type for the arguments. Normally we\n+///   can derive these from callee_ty but in the case of variadic\n+///   functions passed_arg_tys will include the Rust type of all\n+///   the arguments including the ones not specified in the fn's signature.\n pub fn trans_native_call<'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n                                      callee_ty: Ty<'tcx>,\n                                      llfn: ValueRef,\n                                      llretptr: ValueRef,\n                                      llargs_rust: &[ValueRef],\n                                      passed_arg_tys: Vec<Ty<'tcx>>)\n                                      -> Block<'blk, 'tcx> {\n-    /*!\n-     * Prepares a call to a native function. This requires adapting\n-     * from the Rust argument passing rules to the native rules.\n-     *\n-     * # Parameters\n-     *\n-     * - `callee_ty`: Rust type for the function we are calling\n-     * - `llfn`: the function pointer we are calling\n-     * - `llretptr`: where to store the return value of the function\n-     * - `llargs_rust`: a list of the argument values, prepared\n-     *   as they would be if calling a Rust function\n-     * - `passed_arg_tys`: Rust type for the arguments. Normally we\n-     *   can derive these from callee_ty but in the case of variadic\n-     *   functions passed_arg_tys will include the Rust type of all\n-     *   the arguments including the ones not specified in the fn's signature.\n-     */\n-\n     let ccx = bcx.ccx();\n     let tcx = bcx.tcx();\n \n@@ -832,17 +825,13 @@ pub fn link_name(i: &ast::ForeignItem) -> InternedString {\n     }\n }\n \n+/// The ForeignSignature is the LLVM types of the arguments/return type of a function. Note that\n+/// these LLVM types are not quite the same as the LLVM types would be for a native Rust function\n+/// because foreign functions just plain ignore modes. They also don't pass aggregate values by\n+/// pointer like we do.\n fn foreign_signature<'a, 'tcx>(ccx: &CrateContext<'a, 'tcx>,\n                                fn_sig: &ty::FnSig<'tcx>, arg_tys: &[Ty<'tcx>])\n                                -> LlvmSignature {\n-    /*!\n-     * The ForeignSignature is the LLVM types of the arguments/return type\n-     * of a function.  Note that these LLVM types are not quite the same\n-     * as the LLVM types would be for a native Rust function because foreign\n-     * functions just plain ignore modes.  They also don't pass aggregate\n-     * values by pointer like we do.\n-     */\n-\n     let llarg_tys = arg_tys.iter().map(|&arg| arg_type_of(ccx, arg)).collect();\n     let (llret_ty, ret_def) = match fn_sig.output {\n         ty::FnConverging(ret_ty) =>"}, {"sha": "06d916c1ea6f2d9f12df83398abfef5149d9a6a9", "filename": "src/librustc_trans/trans/meth.rs", "status": "modified", "additions": 19, "deletions": 38, "changes": 57, "blob_url": "https://github.com/rust-lang/rust/blob/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc_trans%2Ftrans%2Fmeth.rs", "raw_url": "https://github.com/rust-lang/rust/raw/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc_trans%2Ftrans%2Fmeth.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_trans%2Ftrans%2Fmeth.rs?ref=cd5c8235c5448a7234548c772468c8d2e8f150d9", "patch": "@@ -377,28 +377,21 @@ fn trans_monomorphized_callee<'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n     }\n }\n \n+ /// Creates a concatenated set of substitutions which includes those from the impl and those from\n+ /// the method.  This are some subtle complications here.  Statically, we have a list of type\n+ /// parameters like `[T0, T1, T2, M1, M2, M3]` where `Tn` are type parameters that appear on the\n+ /// receiver.  For example, if the receiver is a method parameter `A` with a bound like\n+ /// `trait<B,C,D>` then `Tn` would be `[B,C,D]`.\n+ ///\n+ /// The weird part is that the type `A` might now be bound to any other type, such as `foo<X>`.\n+ /// In that case, the vector we want is: `[X, M1, M2, M3]`.  Therefore, what we do now is to slice\n+ /// off the method type parameters and append them to the type parameters from the type that the\n+ /// receiver is mapped to.\n fn combine_impl_and_methods_tps<'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n                                             node: ExprOrMethodCall,\n                                             rcvr_substs: subst::Substs<'tcx>)\n                                             -> subst::Substs<'tcx>\n {\n-    /*!\n-     * Creates a concatenated set of substitutions which includes\n-     * those from the impl and those from the method.  This are\n-     * some subtle complications here.  Statically, we have a list\n-     * of type parameters like `[T0, T1, T2, M1, M2, M3]` where\n-     * `Tn` are type parameters that appear on the receiver.  For\n-     * example, if the receiver is a method parameter `A` with a\n-     * bound like `trait<B,C,D>` then `Tn` would be `[B,C,D]`.\n-     *\n-     * The weird part is that the type `A` might now be bound to\n-     * any other type, such as `foo<X>`.  In that case, the vector\n-     * we want is: `[X, M1, M2, M3]`.  Therefore, what we do now is\n-     * to slice off the method type parameters and append them to\n-     * the type parameters from the type that the receiver is\n-     * mapped to.\n-     */\n-\n     let ccx = bcx.ccx();\n \n     let node_substs = node_id_substs(bcx, node);\n@@ -422,21 +415,16 @@ fn combine_impl_and_methods_tps<'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n     }\n }\n \n+/// Create a method callee where the method is coming from a trait object (e.g., Box<Trait> type).\n+/// In this case, we must pull the fn pointer out of the vtable that is packaged up with the\n+/// object. Objects are represented as a pair, so we first evaluate the self expression and then\n+/// extract the self data and vtable out of the pair.\n fn trans_trait_callee<'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n                                   method_ty: Ty<'tcx>,\n                                   n_method: uint,\n                                   self_expr: &ast::Expr,\n                                   arg_cleanup_scope: cleanup::ScopeId)\n                                   -> Callee<'blk, 'tcx> {\n-    /*!\n-     * Create a method callee where the method is coming from a trait\n-     * object (e.g., Box<Trait> type).  In this case, we must pull the fn\n-     * pointer out of the vtable that is packaged up with the object.\n-     * Objects are represented as a pair, so we first evaluate the self\n-     * expression and then extract the self data and vtable out of the\n-     * pair.\n-     */\n-\n     let _icx = push_ctxt(\"meth::trans_trait_callee\");\n     let mut bcx = bcx;\n \n@@ -466,16 +454,13 @@ fn trans_trait_callee<'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n     trans_trait_callee_from_llval(bcx, method_ty, n_method, llval)\n }\n \n+/// Same as `trans_trait_callee()` above, except that it is given a by-ref pointer to the object\n+/// pair.\n pub fn trans_trait_callee_from_llval<'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n                                                  callee_ty: Ty<'tcx>,\n                                                  n_method: uint,\n                                                  llpair: ValueRef)\n                                                  -> Callee<'blk, 'tcx> {\n-    /*!\n-     * Same as `trans_trait_callee()` above, except that it is given\n-     * a by-ref pointer to the object pair.\n-     */\n-\n     let _icx = push_ctxt(\"meth::trans_trait_callee\");\n     let ccx = bcx.ccx();\n \n@@ -731,19 +716,15 @@ fn emit_vtable_methods<'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n     }).collect()\n }\n \n+/// Generates the code to convert from a pointer (`Box<T>`, `&T`, etc) into an object\n+/// (`Box<Trait>`, `&Trait`, etc). This means creating a pair where the first word is the vtable\n+/// and the second word is the pointer.\n pub fn trans_trait_cast<'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n                                     datum: Datum<'tcx, Expr>,\n                                     id: ast::NodeId,\n                                     trait_ref: Rc<ty::TraitRef<'tcx>>,\n                                     dest: expr::Dest)\n                                     -> Block<'blk, 'tcx> {\n-    /*!\n-     * Generates the code to convert from a pointer (`Box<T>`, `&T`, etc)\n-     * into an object (`Box<Trait>`, `&Trait`, etc). This means creating a\n-     * pair where the first word is the vtable and the second word is\n-     * the pointer.\n-     */\n-\n     let mut bcx = bcx;\n     let _icx = push_ctxt(\"meth::trans_trait_cast\");\n "}, {"sha": "9aeb4cdb8a30a7a8fd19b3a6ba5516a8fbf0f5a8", "filename": "src/librustc_trans/trans/tvec.rs", "status": "modified", "additions": 10, "deletions": 26, "changes": 36, "blob_url": "https://github.com/rust-lang/rust/blob/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc_trans%2Ftrans%2Ftvec.rs", "raw_url": "https://github.com/rust-lang/rust/raw/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustc_trans%2Ftrans%2Ftvec.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_trans%2Ftrans%2Ftvec.rs?ref=cd5c8235c5448a7234548c772468c8d2e8f150d9", "patch": "@@ -134,17 +134,13 @@ pub fn trans_fixed_vstore<'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n     };\n }\n \n+/// &[...] allocates memory on the stack and writes the values into it, returning the vector (the\n+/// caller must make the reference).  \"...\" is similar except that the memory can be statically\n+/// allocated and we return a reference (strings are always by-ref).\n pub fn trans_slice_vec<'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n                                    slice_expr: &ast::Expr,\n                                    content_expr: &ast::Expr)\n                                    -> DatumBlock<'blk, 'tcx, Expr> {\n-    /*!\n-     * &[...] allocates memory on the stack and writes the values into it,\n-     * returning the vector (the caller must make the reference).  \"...\" is\n-     * similar except that the memory can be statically allocated and we return\n-     * a reference (strings are always by-ref).\n-     */\n-\n     let fcx = bcx.fcx;\n     let ccx = fcx.ccx;\n     let mut bcx = bcx;\n@@ -208,17 +204,13 @@ pub fn trans_slice_vec<'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n     immediate_rvalue_bcx(bcx, llfixed, vec_ty).to_expr_datumblock()\n }\n \n+/// Literal strings translate to slices into static memory.  This is different from\n+/// trans_slice_vstore() above because it doesn't need to copy the content anywhere.\n pub fn trans_lit_str<'blk, 'tcx>(bcx: Block<'blk, 'tcx>,\n                                  lit_expr: &ast::Expr,\n                                  str_lit: InternedString,\n                                  dest: Dest)\n                                  -> Block<'blk, 'tcx> {\n-    /*!\n-     * Literal strings translate to slices into static memory.  This is\n-     * different from trans_slice_vstore() above because it doesn't need to copy\n-     * the content anywhere.\n-     */\n-\n     debug!(\"trans_lit_str(lit_expr={}, dest={})\",\n            bcx.expr_to_string(lit_expr),\n            dest.to_string(bcx.ccx()));\n@@ -382,15 +374,12 @@ pub fn elements_required(bcx: Block, content_expr: &ast::Expr) -> uint {\n     }\n }\n \n+/// Converts a fixed-length vector into the slice pair. The vector should be stored in `llval`\n+/// which should be by ref.\n pub fn get_fixed_base_and_len(bcx: Block,\n                               llval: ValueRef,\n                               vec_length: uint)\n                               -> (ValueRef, ValueRef) {\n-    /*!\n-     * Converts a fixed-length vector into the slice pair.\n-     * The vector should be stored in `llval` which should be by ref.\n-     */\n-\n     let ccx = bcx.ccx();\n \n     let base = expr::get_dataptr(bcx, llval);\n@@ -406,18 +395,13 @@ fn get_slice_base_and_len(bcx: Block,\n     (base, len)\n }\n \n+/// Converts a vector into the slice pair.  The vector should be stored in `llval` which should be\n+/// by-reference.  If you have a datum, you would probably prefer to call\n+/// `Datum::get_base_and_len()` which will handle any conversions for you.\n pub fn get_base_and_len(bcx: Block,\n                         llval: ValueRef,\n                         vec_ty: Ty)\n                         -> (ValueRef, ValueRef) {\n-    /*!\n-     * Converts a vector into the slice pair.  The vector should be\n-     * stored in `llval` which should be by-reference.  If you have a\n-     * datum, you would probably prefer to call\n-     * `Datum::get_base_and_len()` which will handle any conversions\n-     * for you.\n-     */\n-\n     let ccx = bcx.ccx();\n \n     match vec_ty.sty {"}, {"sha": "261bd1b9f8cb88367ad0b1d30180322dbccf044e", "filename": "src/librustrt/c_str.rs", "status": "modified", "additions": 58, "deletions": 62, "changes": 120, "blob_url": "https://github.com/rust-lang/rust/blob/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustrt%2Fc_str.rs", "raw_url": "https://github.com/rust-lang/rust/raw/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibrustrt%2Fc_str.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustrt%2Fc_str.rs?ref=cd5c8235c5448a7234548c772468c8d2e8f150d9", "patch": "@@ -8,68 +8,64 @@\n // option. This file may not be copied, modified, or distributed\n // except according to those terms.\n \n-/*!\n-\n-C-string manipulation and management\n-\n-This modules provides the basic methods for creating and manipulating\n-null-terminated strings for use with FFI calls (back to C). Most C APIs require\n-that the string being passed to them is null-terminated, and by default rust's\n-string types are *not* null terminated.\n-\n-The other problem with translating Rust strings to C strings is that Rust\n-strings can validly contain a null-byte in the middle of the string (0 is a\n-valid Unicode codepoint). This means that not all Rust strings can actually be\n-translated to C strings.\n-\n-# Creation of a C string\n-\n-A C string is managed through the `CString` type defined in this module. It\n-\"owns\" the internal buffer of characters and will automatically deallocate the\n-buffer when the string is dropped. The `ToCStr` trait is implemented for `&str`\n-and `&[u8]`, but the conversions can fail due to some of the limitations\n-explained above.\n-\n-This also means that currently whenever a C string is created, an allocation\n-must be performed to place the data elsewhere (the lifetime of the C string is\n-not tied to the lifetime of the original string/data buffer). If C strings are\n-heavily used in applications, then caching may be advisable to prevent\n-unnecessary amounts of allocations.\n-\n-Be carefull to remember that the memory is managed by C allocator API and not\n-by Rust allocator API.\n-That means that the CString pointers should be freed with C allocator API\n-if you intend to do that on your own, as the behaviour if you free them with\n-Rust's allocator API is not well defined\n-\n-An example of creating and using a C string would be:\n-\n-```rust\n-extern crate libc;\n-\n-extern {\n-    fn puts(s: *const libc::c_char);\n-}\n-\n-fn main() {\n-    let my_string = \"Hello, world!\";\n-\n-    // Allocate the C string with an explicit local that owns the string. The\n-    // `c_buffer` pointer will be deallocated when `my_c_string` goes out of scope.\n-    let my_c_string = my_string.to_c_str();\n-    unsafe {\n-        puts(my_c_string.as_ptr());\n-    }\n-\n-    // Don't save/return the pointer to the C string, the `c_buffer` will be\n-    // deallocated when this block returns!\n-    my_string.with_c_str(|c_buffer| {\n-        unsafe { puts(c_buffer); }\n-    });\n-}\n-```\n-\n-*/\n+//! C-string manipulation and management\n+//!\n+//! This modules provides the basic methods for creating and manipulating\n+//! null-terminated strings for use with FFI calls (back to C). Most C APIs require\n+//! that the string being passed to them is null-terminated, and by default rust's\n+//! string types are *not* null terminated.\n+//!\n+//! The other problem with translating Rust strings to C strings is that Rust\n+//! strings can validly contain a null-byte in the middle of the string (0 is a\n+//! valid Unicode codepoint). This means that not all Rust strings can actually be\n+//! translated to C strings.\n+//!\n+//! # Creation of a C string\n+//!\n+//! A C string is managed through the `CString` type defined in this module. It\n+//! \"owns\" the internal buffer of characters and will automatically deallocate the\n+//! buffer when the string is dropped. The `ToCStr` trait is implemented for `&str`\n+//! and `&[u8]`, but the conversions can fail due to some of the limitations\n+//! explained above.\n+//!\n+//! This also means that currently whenever a C string is created, an allocation\n+//! must be performed to place the data elsewhere (the lifetime of the C string is\n+//! not tied to the lifetime of the original string/data buffer). If C strings are\n+//! heavily used in applications, then caching may be advisable to prevent\n+//! unnecessary amounts of allocations.\n+//!\n+//! Be carefull to remember that the memory is managed by C allocator API and not\n+//! by Rust allocator API.\n+//! That means that the CString pointers should be freed with C allocator API\n+//! if you intend to do that on your own, as the behaviour if you free them with\n+//! Rust's allocator API is not well defined\n+//!\n+//! An example of creating and using a C string would be:\n+//!\n+//! ```rust\n+//! extern crate libc;\n+//!\n+//! extern {\n+//!     fn puts(s: *const libc::c_char);\n+//! }\n+//!\n+//! fn main() {\n+//!     let my_string = \"Hello, world!\";\n+//!\n+//!     // Allocate the C string with an explicit local that owns the string. The\n+//!     // `c_buffer` pointer will be deallocated when `my_c_string` goes out of scope.\n+//!     let my_c_string = my_string.to_c_str();\n+//!     unsafe {\n+//!         puts(my_c_string.as_ptr());\n+//!     }\n+//!\n+//!     // Don't save/return the pointer to the C string, the `c_buffer` will be\n+//!     // deallocated when this block returns!\n+//!     my_string.with_c_str(|c_buffer| {\n+//!         unsafe { puts(c_buffer); }\n+//!     });\n+//! }\n+//! ```\n \n use collections::string::String;\n use collections::hash;"}, {"sha": "3c03dc35f3b291e59a21ec763c3c346654abcdec", "filename": "src/libserialize/json.rs", "status": "modified", "additions": 176, "deletions": 179, "changes": 355, "blob_url": "https://github.com/rust-lang/rust/blob/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibserialize%2Fjson.rs", "raw_url": "https://github.com/rust-lang/rust/raw/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibserialize%2Fjson.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibserialize%2Fjson.rs?ref=cd5c8235c5448a7234548c772468c8d2e8f150d9", "patch": "@@ -14,185 +14,182 @@\n #![forbid(non_camel_case_types)]\n #![allow(missing_docs)]\n \n-/*!\n-JSON parsing and serialization\n-\n-# What is JSON?\n-\n-JSON (JavaScript Object Notation) is a way to write data in Javascript.\n-Like XML, it allows to encode structured data in a text format that can be easily read by humans.\n-Its simple syntax and native compatibility with JavaScript have made it a widely used format.\n-\n-Data types that can be encoded are JavaScript types (see the `Json` enum for more details):\n-\n-* `Boolean`: equivalent to rust's `bool`\n-* `Number`: equivalent to rust's `f64`\n-* `String`: equivalent to rust's `String`\n-* `Array`: equivalent to rust's `Vec<T>`, but also allowing objects of different types in the same\n-array\n-* `Object`: equivalent to rust's `Treemap<String, json::Json>`\n-* `Null`\n-\n-An object is a series of string keys mapping to values, in `\"key\": value` format.\n-Arrays are enclosed in square brackets ([ ... ]) and objects in curly brackets ({ ... }).\n-A simple JSON document encoding a person, his/her age, address and phone numbers could look like:\n-\n-```ignore\n-{\n-    \"FirstName\": \"John\",\n-    \"LastName\": \"Doe\",\n-    \"Age\": 43,\n-    \"Address\": {\n-        \"Street\": \"Downing Street 10\",\n-        \"City\": \"London\",\n-        \"Country\": \"Great Britain\"\n-    },\n-    \"PhoneNumbers\": [\n-        \"+44 1234567\",\n-        \"+44 2345678\"\n-    ]\n-}\n-```\n-\n-# Rust Type-based Encoding and Decoding\n-\n-Rust provides a mechanism for low boilerplate encoding & decoding of values to and from JSON via\n-the serialization API.\n-To be able to encode a piece of data, it must implement the `serialize::Encodable` trait.\n-To be able to decode a piece of data, it must implement the `serialize::Decodable` trait.\n-The Rust compiler provides an annotation to automatically generate the code for these traits:\n-`#[deriving(Decodable, Encodable)]`\n-\n-The JSON API provides an enum `json::Json` and a trait `ToJson` to encode objects.\n-The `ToJson` trait provides a `to_json` method to convert an object into a `json::Json` value.\n-A `json::Json` value can be encoded as a string or buffer using the functions described above.\n-You can also use the `json::Encoder` object, which implements the `Encoder` trait.\n-\n-When using `ToJson` the `Encodable` trait implementation is not mandatory.\n-\n-# Examples of use\n-\n-## Using Autoserialization\n-\n-Create a struct called `TestStruct` and serialize and deserialize it to and from JSON using the\n-serialization API, using the derived serialization code.\n-\n-```rust\n-extern crate serialize;\n-use serialize::json;\n-\n-// Automatically generate `Decodable` and `Encodable` trait implementations\n-#[deriving(Decodable, Encodable)]\n-pub struct TestStruct  {\n-    data_int: u8,\n-    data_str: String,\n-    data_vector: Vec<u8>,\n-}\n-\n-fn main() {\n-    let object = TestStruct {\n-        data_int: 1,\n-        data_str: \"toto\".to_string(),\n-        data_vector: vec![2,3,4,5],\n-    };\n-\n-    // Serialize using `json::encode`\n-    let encoded = json::encode(&object);\n-\n-    // Deserialize using `json::decode`\n-    let decoded: TestStruct = json::decode(encoded.as_slice()).unwrap();\n-}\n-```\n-\n-## Using the `ToJson` trait\n-\n-The examples above use the `ToJson` trait to generate the JSON string, which is required\n-for custom mappings.\n-\n-### Simple example of `ToJson` usage\n-\n-```rust\n-extern crate serialize;\n-use serialize::json::ToJson;\n-use serialize::json;\n-\n-// A custom data structure\n-struct ComplexNum {\n-    a: f64,\n-    b: f64,\n-}\n-\n-// JSON value representation\n-impl ToJson for ComplexNum {\n-    fn to_json(&self) -> json::Json {\n-        json::String(format!(\"{}+{}i\", self.a, self.b))\n-    }\n-}\n-\n-// Only generate `Encodable` trait implementation\n-#[deriving(Encodable)]\n-pub struct ComplexNumRecord {\n-    uid: u8,\n-    dsc: String,\n-    val: json::Json,\n-}\n-\n-fn main() {\n-    let num = ComplexNum { a: 0.0001, b: 12.539 };\n-    let data: String = json::encode(&ComplexNumRecord{\n-        uid: 1,\n-        dsc: \"test\".to_string(),\n-        val: num.to_json(),\n-    });\n-    println!(\"data: {}\", data);\n-    // data: {\"uid\":1,\"dsc\":\"test\",\"val\":\"0.0001+12.539j\"};\n-}\n-```\n-\n-### Verbose example of `ToJson` usage\n-\n-```rust\n-extern crate serialize;\n-use std::collections::TreeMap;\n-use serialize::json::ToJson;\n-use serialize::json;\n-\n-// Only generate `Decodable` trait implementation\n-#[deriving(Decodable)]\n-pub struct TestStruct {\n-    data_int: u8,\n-    data_str: String,\n-    data_vector: Vec<u8>,\n-}\n-\n-// Specify encoding method manually\n-impl ToJson for TestStruct {\n-    fn to_json(&self) -> json::Json {\n-        let mut d = TreeMap::new();\n-        // All standard types implement `to_json()`, so use it\n-        d.insert(\"data_int\".to_string(), self.data_int.to_json());\n-        d.insert(\"data_str\".to_string(), self.data_str.to_json());\n-        d.insert(\"data_vector\".to_string(), self.data_vector.to_json());\n-        json::Object(d)\n-    }\n-}\n-\n-fn main() {\n-    // Serialize using `ToJson`\n-    let input_data = TestStruct {\n-        data_int: 1,\n-        data_str: \"toto\".to_string(),\n-        data_vector: vec![2,3,4,5],\n-    };\n-    let json_obj: json::Json = input_data.to_json();\n-    let json_str: String = json_obj.to_string();\n-\n-    // Deserialize like before\n-    let decoded: TestStruct = json::decode(json_str.as_slice()).unwrap();\n-}\n-```\n-\n-*/\n+//! JSON parsing and serialization\n+//!\n+//! # What is JSON?\n+//!\n+//! JSON (JavaScript Object Notation) is a way to write data in Javascript.\n+//! Like XML, it allows to encode structured data in a text format that can be easily read by humans\n+//! Its simple syntax and native compatibility with JavaScript have made it a widely used format.\n+//!\n+//! Data types that can be encoded are JavaScript types (see the `Json` enum for more details):\n+//!\n+//! * `Boolean`: equivalent to rust's `bool`\n+//! * `Number`: equivalent to rust's `f64`\n+//! * `String`: equivalent to rust's `String`\n+//! * `Array`: equivalent to rust's `Vec<T>`, but also allowing objects of different types in the\n+//!   same array\n+//! * `Object`: equivalent to rust's `Treemap<String, json::Json>`\n+//! * `Null`\n+//!\n+//! An object is a series of string keys mapping to values, in `\"key\": value` format.\n+//! Arrays are enclosed in square brackets ([ ... ]) and objects in curly brackets ({ ... }).\n+//! A simple JSON document encoding a person, his/her age, address and phone numbers could look like\n+//!\n+//! ```ignore\n+//! {\n+//!     \"FirstName\": \"John\",\n+//!     \"LastName\": \"Doe\",\n+//!     \"Age\": 43,\n+//!     \"Address\": {\n+//!         \"Street\": \"Downing Street 10\",\n+//!         \"City\": \"London\",\n+//!         \"Country\": \"Great Britain\"\n+//!     },\n+//!     \"PhoneNumbers\": [\n+//!         \"+44 1234567\",\n+//!         \"+44 2345678\"\n+//!     ]\n+//! }\n+//! ```\n+//!\n+//! # Rust Type-based Encoding and Decoding\n+//!\n+//! Rust provides a mechanism for low boilerplate encoding & decoding of values to and from JSON via\n+//! the serialization API.\n+//! To be able to encode a piece of data, it must implement the `serialize::Encodable` trait.\n+//! To be able to decode a piece of data, it must implement the `serialize::Decodable` trait.\n+//! The Rust compiler provides an annotation to automatically generate the code for these traits:\n+//! `#[deriving(Decodable, Encodable)]`\n+//!\n+//! The JSON API provides an enum `json::Json` and a trait `ToJson` to encode objects.\n+//! The `ToJson` trait provides a `to_json` method to convert an object into a `json::Json` value.\n+//! A `json::Json` value can be encoded as a string or buffer using the functions described above.\n+//! You can also use the `json::Encoder` object, which implements the `Encoder` trait.\n+//!\n+//! When using `ToJson` the `Encodable` trait implementation is not mandatory.\n+//!\n+//! # Examples of use\n+//!\n+//! ## Using Autoserialization\n+//!\n+//! Create a struct called `TestStruct` and serialize and deserialize it to and from JSON using the\n+//! serialization API, using the derived serialization code.\n+//!\n+//! ```rust\n+//! extern crate serialize;\n+//! use serialize::json;\n+//!\n+//! // Automatically generate `Decodable` and `Encodable` trait implementations\n+//! #[deriving(Decodable, Encodable)]\n+//! pub struct TestStruct  {\n+//!     data_int: u8,\n+//!     data_str: String,\n+//!     data_vector: Vec<u8>,\n+//! }\n+//!\n+//! fn main() {\n+//!     let object = TestStruct {\n+//!         data_int: 1,\n+//!         data_str: \"toto\".to_string(),\n+//!         data_vector: vec![2,3,4,5],\n+//!     };\n+//!\n+//!     // Serialize using `json::encode`\n+//!     let encoded = json::encode(&object);\n+//!\n+//!     // Deserialize using `json::decode`\n+//!     let decoded: TestStruct = json::decode(encoded.as_slice()).unwrap();\n+//! }\n+//! ```\n+//!\n+//! ## Using the `ToJson` trait\n+//!\n+//! The examples above use the `ToJson` trait to generate the JSON string, which is required\n+//! for custom mappings.\n+//!\n+//! ### Simple example of `ToJson` usage\n+//!\n+//! ```rust\n+//! extern crate serialize;\n+//! use serialize::json::ToJson;\n+//! use serialize::json;\n+//!\n+//! // A custom data structure\n+//! struct ComplexNum {\n+//!     a: f64,\n+//!     b: f64,\n+//! }\n+//!\n+//! // JSON value representation\n+//! impl ToJson for ComplexNum {\n+//!     fn to_json(&self) -> json::Json {\n+//!         json::String(format!(\"{}+{}i\", self.a, self.b))\n+//!     }\n+//! }\n+//!\n+//! // Only generate `Encodable` trait implementation\n+//! #[deriving(Encodable)]\n+//! pub struct ComplexNumRecord {\n+//!     uid: u8,\n+//!     dsc: String,\n+//!     val: json::Json,\n+//! }\n+//!\n+//! fn main() {\n+//!     let num = ComplexNum { a: 0.0001, b: 12.539 };\n+//!     let data: String = json::encode(&ComplexNumRecord{\n+//!         uid: 1,\n+//!         dsc: \"test\".to_string(),\n+//!         val: num.to_json(),\n+//!     });\n+//!     println!(\"data: {}\", data);\n+//!     // data: {\"uid\":1,\"dsc\":\"test\",\"val\":\"0.0001+12.539j\"};\n+//! }\n+//! ```\n+//!\n+//! ### Verbose example of `ToJson` usage\n+//!\n+//! ```rust\n+//! extern crate serialize;\n+//! use std::collections::TreeMap;\n+//! use serialize::json::ToJson;\n+//! use serialize::json;\n+//!\n+//! // Only generate `Decodable` trait implementation\n+//! #[deriving(Decodable)]\n+//! pub struct TestStruct {\n+//!     data_int: u8,\n+//!     data_str: String,\n+//!     data_vector: Vec<u8>,\n+//! }\n+//!\n+//! // Specify encoding method manually\n+//! impl ToJson for TestStruct {\n+//!     fn to_json(&self) -> json::Json {\n+//!         let mut d = TreeMap::new();\n+//!         // All standard types implement `to_json()`, so use it\n+//!         d.insert(\"data_int\".to_string(), self.data_int.to_json());\n+//!         d.insert(\"data_str\".to_string(), self.data_str.to_json());\n+//!         d.insert(\"data_vector\".to_string(), self.data_vector.to_json());\n+//!         json::Object(d)\n+//!     }\n+//! }\n+//!\n+//! fn main() {\n+//!     // Serialize using `ToJson`\n+//!     let input_data = TestStruct {\n+//!         data_int: 1,\n+//!         data_str: \"toto\".to_string(),\n+//!         data_vector: vec![2,3,4,5],\n+//!     };\n+//!     let json_obj: json::Json = input_data.to_json();\n+//!     let json_str: String = json_obj.to_string();\n+//!\n+//!     // Deserialize like before\n+//!     let decoded: TestStruct = json::decode(json_str.as_slice()).unwrap();\n+//! }\n+//! ```\n \n pub use self::JsonEvent::*;\n pub use self::StackElement::*;"}, {"sha": "3cd0c0eeaf290b949180286e43a689b5f7dbd14e", "filename": "src/libstd/dynamic_lib.rs", "status": "modified", "additions": 3, "deletions": 7, "changes": 10, "blob_url": "https://github.com/rust-lang/rust/blob/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibstd%2Fdynamic_lib.rs", "raw_url": "https://github.com/rust-lang/rust/raw/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibstd%2Fdynamic_lib.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibstd%2Fdynamic_lib.rs?ref=cd5c8235c5448a7234548c772468c8d2e8f150d9", "patch": "@@ -8,13 +8,9 @@\n // option. This file may not be copied, modified, or distributed\n // except according to those terms.\n \n-/*!\n-\n-Dynamic library facilities.\n-\n-A simple wrapper over the platform's dynamic library facilities\n-\n-*/\n+//! Dynamic library facilities.\n+//!\n+//! A simple wrapper over the platform's dynamic library facilities\n \n #![experimental]\n #![allow(missing_docs)]"}, {"sha": "62ca3483c21b37e69c9c73cd8b6c357e53debf75", "filename": "src/libstd/fmt.rs", "status": "modified", "additions": 382, "deletions": 386, "changes": 768, "blob_url": "https://github.com/rust-lang/rust/blob/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibstd%2Ffmt.rs", "raw_url": "https://github.com/rust-lang/rust/raw/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibstd%2Ffmt.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibstd%2Ffmt.rs?ref=cd5c8235c5448a7234548c772468c8d2e8f150d9", "patch": "@@ -10,392 +10,388 @@\n //\n // ignore-lexer-test FIXME #15679\n \n-/*!\n-\n-Utilities for formatting and printing strings\n-\n-This module contains the runtime support for the `format!` syntax extension.\n-This macro is implemented in the compiler to emit calls to this module in order\n-to format arguments at runtime into strings and streams.\n-\n-The functions contained in this module should not normally be used in everyday\n-use cases of `format!`. The assumptions made by these functions are unsafe for\n-all inputs, and the compiler performs a large amount of validation on the\n-arguments to `format!` in order to ensure safety at runtime. While it is\n-possible to call these functions directly, it is not recommended to do so in the\n-general case.\n-\n-## Usage\n-\n-The `format!` macro is intended to be familiar to those coming from C's\n-printf/fprintf functions or Python's `str.format` function. In its current\n-revision, the `format!` macro returns a `String` type which is the result of\n-the formatting. In the future it will also be able to pass in a stream to\n-format arguments directly while performing minimal allocations.\n-\n-Some examples of the `format!` extension are:\n-\n-```rust\n-# fn main() {\n-format!(\"Hello\");                  // => \"Hello\"\n-format!(\"Hello, {}!\", \"world\");    // => \"Hello, world!\"\n-format!(\"The number is {}\", 1i);   // => \"The number is 1\"\n-format!(\"{}\", (3i, 4i));           // => \"(3, 4)\"\n-format!(\"{value}\", value=4i);      // => \"4\"\n-format!(\"{} {}\", 1i, 2u);          // => \"1 2\"\n-# }\n-```\n-\n-From these, you can see that the first argument is a format string. It is\n-required by the compiler for this to be a string literal; it cannot be a\n-variable passed in (in order to perform validity checking). The compiler will\n-then parse the format string and determine if the list of arguments provided is\n-suitable to pass to this format string.\n-\n-### Positional parameters\n-\n-Each formatting argument is allowed to specify which value argument it's\n-referencing, and if omitted it is assumed to be \"the next argument\". For\n-example, the format string `{} {} {}` would take three parameters, and they\n-would be formatted in the same order as they're given. The format string\n-`{2} {1} {0}`, however, would format arguments in reverse order.\n-\n-Things can get a little tricky once you start intermingling the two types of\n-positional specifiers. The \"next argument\" specifier can be thought of as an\n-iterator over the argument. Each time a \"next argument\" specifier is seen, the\n-iterator advances. This leads to behavior like this:\n-\n-```rust\n-format!(\"{1} {} {0} {}\", 1i, 2i); // => \"2 1 1 2\"\n-```\n-\n-The internal iterator over the argument has not been advanced by the time the\n-first `{}` is seen, so it prints the first argument. Then upon reaching the\n-second `{}`, the iterator has advanced forward to the second argument.\n-Essentially, parameters which explicitly name their argument do not affect\n-parameters which do not name an argument in terms of positional specifiers.\n-\n-A format string is required to use all of its arguments, otherwise it is a\n-compile-time error. You may refer to the same argument more than once in the\n-format string, although it must always be referred to with the same type.\n-\n-### Named parameters\n-\n-Rust itself does not have a Python-like equivalent of named parameters to a\n-function, but the `format!` macro is a syntax extension which allows it to\n-leverage named parameters. Named parameters are listed at the end of the\n-argument list and have the syntax:\n-\n-```text\n-identifier '=' expression\n-```\n-\n-For example, the following `format!` expressions all use named argument:\n-\n-```rust\n-# fn main() {\n-format!(\"{argument}\", argument = \"test\");   // => \"test\"\n-format!(\"{name} {}\", 1i, name = 2i);        // => \"2 1\"\n-format!(\"{a} {c} {b}\", a=\"a\", b=(), c=3i);  // => \"a 3 ()\"\n-# }\n-```\n-\n-It is illegal to put positional parameters (those without names) after arguments\n-which have names. Like with positional parameters, it is illegal to provide\n-named parameters that are unused by the format string.\n-\n-### Argument types\n-\n-Each argument's type is dictated by the format string. It is a requirement that every argument is\n-only ever referred to by one type. For example, this is an invalid format string:\n-\n-```text\n-{0:x} {0:o}\n-```\n-\n-This is invalid because the first argument is both referred to as a hexidecimal as well as an\n-octal.\n-\n-There are various parameters which do require a particular type, however. Namely if the syntax\n-`{:.*}` is used, then the number of characters to print precedes the actual object being formatted,\n-and the number of characters must have the type `uint`. Although a `uint` can be printed with\n-`{}`, it is illegal to reference an argument as such. For example this is another invalid\n-format string:\n-\n-```text\n-{:.*} {0}\n-```\n-\n-### Formatting traits\n-\n-When requesting that an argument be formatted with a particular type, you are\n-actually requesting that an argument ascribes to a particular trait. This allows\n-multiple actual types to be formatted via `{:x}` (like `i8` as well as `int`).\n-The current mapping of types to traits is:\n-\n-* *nothing* \u21d2 `Show`\n-* `o` \u21d2 `Octal`\n-* `x` \u21d2 `LowerHex`\n-* `X` \u21d2 `UpperHex`\n-* `p` \u21d2 `Pointer`\n-* `b` \u21d2 `Binary`\n-* `e` \u21d2 `LowerExp`\n-* `E` \u21d2 `UpperExp`\n-\n-What this means is that any type of argument which implements the\n-`std::fmt::Binary` trait can then be formatted with `{:b}`. Implementations are\n-provided for these traits for a number of primitive types by the standard\n-library as well. If no format is specified (as in `{}` or `{:6}`), then the\n-format trait used is the `Show` trait. This is one of the more commonly\n-implemented traits when formatting a custom type.\n-\n-When implementing a format trait for your own type, you will have to implement a\n-method of the signature:\n-\n-```rust\n-# use std::fmt;\n-# struct Foo; // our custom type\n-# impl fmt::Show for Foo {\n-fn fmt(&self, f: &mut std::fmt::Formatter) -> fmt::Result {\n-# write!(f, \"testing, testing\")\n-# } }\n-```\n-\n-Your type will be passed as `self` by-reference, and then the function should\n-emit output into the `f.buf` stream. It is up to each format trait\n-implementation to correctly adhere to the requested formatting parameters. The\n-values of these parameters will be listed in the fields of the `Formatter`\n-struct. In order to help with this, the `Formatter` struct also provides some\n-helper methods.\n-\n-Additionally, the return value of this function is `fmt::Result` which is a\n-typedef to `Result<(), IoError>` (also known as `IoResult<()>`). Formatting\n-implementations should ensure that they return errors from `write!` correctly\n-(propagating errors upward).\n-\n-An example of implementing the formatting traits would look\n-like:\n-\n-```rust\n-use std::fmt;\n-use std::f64;\n-use std::num::Float;\n-\n-struct Vector2D {\n-    x: int,\n-    y: int,\n-}\n-\n-impl fmt::Show for Vector2D {\n-    fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {\n-        // The `f` value implements the `Writer` trait, which is what the\n-        // write! macro is expecting. Note that this formatting ignores the\n-        // various flags provided to format strings.\n-        write!(f, \"({}, {})\", self.x, self.y)\n-    }\n-}\n-\n-// Different traits allow different forms of output of a type. The meaning of\n-// this format is to print the magnitude of a vector.\n-impl fmt::Binary for Vector2D {\n-    fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {\n-        let magnitude = (self.x * self.x + self.y * self.y) as f64;\n-        let magnitude = magnitude.sqrt();\n-\n-        // Respect the formatting flags by using the helper method\n-        // `pad_integral` on the Formatter object. See the method documentation\n-        // for details, and the function `pad` can be used to pad strings.\n-        let decimals = f.precision().unwrap_or(3);\n-        let string = f64::to_str_exact(magnitude, decimals);\n-        f.pad_integral(true, \"\", string.as_bytes())\n-    }\n-}\n-\n-fn main() {\n-    let myvector = Vector2D { x: 3, y: 4 };\n-\n-    println!(\"{}\", myvector);       // => \"(3, 4)\"\n-    println!(\"{:10.3b}\", myvector); // => \"     5.000\"\n-}\n-```\n-\n-### Related macros\n-\n-There are a number of related macros in the `format!` family. The ones that are\n-currently implemented are:\n-\n-```ignore\n-format!      // described above\n-write!       // first argument is a &mut io::Writer, the destination\n-writeln!     // same as write but appends a newline\n-print!       // the format string is printed to the standard output\n-println!     // same as print but appends a newline\n-format_args! // described below.\n-```\n-\n-#### `write!`\n-\n-This and `writeln` are two macros which are used to emit the format string to a\n-specified stream. This is used to prevent intermediate allocations of format\n-strings and instead directly write the output. Under the hood, this function is\n-actually invoking the `write` function defined in this module. Example usage is:\n-\n-```rust\n-# #![allow(unused_must_use)]\n-use std::io;\n-\n-let mut w = Vec::new();\n-write!(&mut w as &mut io::Writer, \"Hello {}!\", \"world\");\n-```\n-\n-#### `print!`\n-\n-This and `println` emit their output to stdout. Similarly to the `write!` macro,\n-the goal of these macros is to avoid intermediate allocations when printing\n-output. Example usage is:\n-\n-```rust\n-print!(\"Hello {}!\", \"world\");\n-println!(\"I have a newline {}\", \"character at the end\");\n-```\n-\n-#### `format_args!`\n-This is a curious macro which is used to safely pass around\n-an opaque object describing the format string. This object\n-does not require any heap allocations to create, and it only\n-references information on the stack. Under the hood, all of\n-the related macros are implemented in terms of this. First\n-off, some example usage is:\n-\n-```\n-use std::fmt;\n-use std::io;\n-\n-# #[allow(unused_must_use)]\n-# fn main() {\n-format_args!(fmt::format, \"this returns {}\", \"String\");\n-\n-let some_writer: &mut io::Writer = &mut io::stdout();\n-format_args!(|args| { write!(some_writer, \"{}\", args) }, \"print with a {}\", \"closure\");\n-\n-fn my_fmt_fn(args: &fmt::Arguments) {\n-    write!(&mut io::stdout(), \"{}\", args);\n-}\n-format_args!(my_fmt_fn, \"or a {} too\", \"function\");\n-# }\n-```\n-\n-The first argument of the `format_args!` macro is a function (or closure) which\n-takes one argument of type `&fmt::Arguments`. This structure can then be\n-passed to the `write` and `format` functions inside this module in order to\n-process the format string. The goal of this macro is to even further prevent\n-intermediate allocations when dealing formatting strings.\n-\n-For example, a logging library could use the standard formatting syntax, but it\n-would internally pass around this structure until it has been determined where\n-output should go to.\n-\n-It is unsafe to programmatically create an instance of `fmt::Arguments` because\n-the operations performed when executing a format string require the compile-time\n-checks provided by the compiler. The `format_args!` macro is the only method of\n-safely creating these structures, but they can be unsafely created with the\n-constructor provided.\n-\n-## Syntax\n-\n-The syntax for the formatting language used is drawn from other languages, so it\n-should not be too alien. Arguments are formatted with python-like syntax,\n-meaning that arguments are surrounded by `{}` instead of the C-like `%`. The\n-actual grammar for the formatting syntax is:\n-\n-```text\n-format_string := <text> [ format <text> ] *\n-format := '{' [ argument ] [ ':' format_spec ] '}'\n-argument := integer | identifier\n-\n-format_spec := [[fill]align][sign]['#'][0][width]['.' precision][type]\n-fill := character\n-align := '<' | '^' | '>'\n-sign := '+' | '-'\n-width := count\n-precision := count | '*'\n-type := identifier | ''\n-count := parameter | integer\n-parameter := integer '$'\n-```\n-\n-## Formatting Parameters\n-\n-Each argument being formatted can be transformed by a number of formatting\n-parameters (corresponding to `format_spec` in the syntax above). These\n-parameters affect the string representation of what's being formatted. This\n-syntax draws heavily from Python's, so it may seem a bit familiar.\n-\n-### Fill/Alignment\n-\n-The fill character is provided normally in conjunction with the `width`\n-parameter. This indicates that if the value being formatted is smaller than\n-`width` some extra characters will be printed around it. The extra characters\n-are specified by `fill`, and the alignment can be one of two options:\n-\n-* `<` - the argument is left-aligned in `width` columns\n-* `^` - the argument is center-aligned in `width` columns\n-* `>` - the argument is right-aligned in `width` columns\n-\n-### Sign/#/0\n-\n-These can all be interpreted as flags for a particular formatter.\n-\n-* '+' - This is intended for numeric types and indicates that the sign should\n-        always be printed. Positive signs are never printed by default, and the\n-        negative sign is only printed by default for the `Signed` trait. This\n-        flag indicates that the correct sign (+ or -) should always be printed.\n-* '-' - Currently not used\n-* '#' - This flag is indicates that the \"alternate\" form of printing should be\n-        used. By default, this only applies to the integer formatting traits and\n-        performs like:\n-    * `x` - precedes the argument with a \"0x\"\n-    * `X` - precedes the argument with a \"0x\"\n-    * `t` - precedes the argument with a \"0b\"\n-    * `o` - precedes the argument with a \"0o\"\n-* '0' - This is used to indicate for integer formats that the padding should\n-        both be done with a `0` character as well as be sign-aware. A format\n-        like `{:08d}` would yield `00000001` for the integer `1`, while the same\n-        format would yield `-0000001` for the integer `-1`. Notice that the\n-        negative version has one fewer zero than the positive version.\n-\n-### Width\n-\n-This is a parameter for the \"minimum width\" that the format should take up. If\n-the value's string does not fill up this many characters, then the padding\n-specified by fill/alignment will be used to take up the required space.\n-\n-The default fill/alignment for non-numerics is a space and left-aligned. The\n-defaults for numeric formatters is also a space but with right-alignment. If the\n-'0' flag is specified for numerics, then the implicit fill character is '0'.\n-\n-The value for the width can also be provided as a `uint` in the list of\n-parameters by using the `2$` syntax indicating that the second argument is a\n-`uint` specifying the width.\n-\n-### Precision\n-\n-For non-numeric types, this can be considered a \"maximum width\". If the\n-resulting string is longer than this width, then it is truncated down to this\n-many characters and only those are emitted.\n-\n-For integral types, this has no meaning currently.\n-\n-For floating-point types, this indicates how many digits after the decimal point\n-should be printed.\n-\n-## Escaping\n-\n-The literal characters `{` and `}` may be included in a string by preceding them\n-with the same character. For example, the `{` character is escaped with `{{` and\n-the `}` character is escaped with `}}`.\n-\n-*/\n+//! Utilities for formatting and printing strings\n+//!\n+//! This module contains the runtime support for the `format!` syntax extension.\n+//! This macro is implemented in the compiler to emit calls to this module in order\n+//! to format arguments at runtime into strings and streams.\n+//!\n+//! The functions contained in this module should not normally be used in everyday\n+//! use cases of `format!`. The assumptions made by these functions are unsafe for\n+//! all inputs, and the compiler performs a large amount of validation on the\n+//! arguments to `format!` in order to ensure safety at runtime. While it is\n+//! possible to call these functions directly, it is not recommended to do so in the\n+//! general case.\n+//!\n+//! ## Usage\n+//!\n+//! The `format!` macro is intended to be familiar to those coming from C's\n+//! printf/fprintf functions or Python's `str.format` function. In its current\n+//! revision, the `format!` macro returns a `String` type which is the result of\n+//! the formatting. In the future it will also be able to pass in a stream to\n+//! format arguments directly while performing minimal allocations.\n+//!\n+//! Some examples of the `format!` extension are:\n+//!\n+//! ```rust\n+//! # fn main() {\n+//! format!(\"Hello\");                  // => \"Hello\"\n+//! format!(\"Hello, {}!\", \"world\");    // => \"Hello, world!\"\n+//! format!(\"The number is {}\", 1i);   // => \"The number is 1\"\n+//! format!(\"{}\", (3i, 4i));           // => \"(3, 4)\"\n+//! format!(\"{value}\", value=4i);      // => \"4\"\n+//! format!(\"{} {}\", 1i, 2u);          // => \"1 2\"\n+//! # }\n+//! ```\n+//!\n+//! From these, you can see that the first argument is a format string. It is\n+//! required by the compiler for this to be a string literal; it cannot be a\n+//! variable passed in (in order to perform validity checking). The compiler will\n+//! then parse the format string and determine if the list of arguments provided is\n+//! suitable to pass to this format string.\n+//!\n+//! ### Positional parameters\n+//!\n+//! Each formatting argument is allowed to specify which value argument it's\n+//! referencing, and if omitted it is assumed to be \"the next argument\". For\n+//! example, the format string `{} {} {}` would take three parameters, and they\n+//! would be formatted in the same order as they're given. The format string\n+//! `{2} {1} {0}`, however, would format arguments in reverse order.\n+//!\n+//! Things can get a little tricky once you start intermingling the two types of\n+//! positional specifiers. The \"next argument\" specifier can be thought of as an\n+//! iterator over the argument. Each time a \"next argument\" specifier is seen, the\n+//! iterator advances. This leads to behavior like this:\n+//!\n+//! ```rust\n+//! format!(\"{1} {} {0} {}\", 1i, 2i); // => \"2 1 1 2\"\n+//! ```\n+//!\n+//! The internal iterator over the argument has not been advanced by the time the\n+//! first `{}` is seen, so it prints the first argument. Then upon reaching the\n+//! second `{}`, the iterator has advanced forward to the second argument.\n+//! Essentially, parameters which explicitly name their argument do not affect\n+//! parameters which do not name an argument in terms of positional specifiers.\n+//!\n+//! A format string is required to use all of its arguments, otherwise it is a\n+//! compile-time error. You may refer to the same argument more than once in the\n+//! format string, although it must always be referred to with the same type.\n+//!\n+//! ### Named parameters\n+//!\n+//! Rust itself does not have a Python-like equivalent of named parameters to a\n+//! function, but the `format!` macro is a syntax extension which allows it to\n+//! leverage named parameters. Named parameters are listed at the end of the\n+//! argument list and have the syntax:\n+//!\n+//! ```text\n+//! identifier '=' expression\n+//! ```\n+//!\n+//! For example, the following `format!` expressions all use named argument:\n+//!\n+//! ```rust\n+//! # fn main() {\n+//! format!(\"{argument}\", argument = \"test\");   // => \"test\"\n+//! format!(\"{name} {}\", 1i, name = 2i);        // => \"2 1\"\n+//! format!(\"{a} {c} {b}\", a=\"a\", b=(), c=3i);  // => \"a 3 ()\"\n+//! # }\n+//! ```\n+//!\n+//! It is illegal to put positional parameters (those without names) after arguments\n+//! which have names. Like with positional parameters, it is illegal to provide\n+//! named parameters that are unused by the format string.\n+//!\n+//! ### Argument types\n+//!\n+//! Each argument's type is dictated by the format string. It is a requirement that every argument is\n+//! only ever referred to by one type. For example, this is an invalid format string:\n+//!\n+//! ```text\n+//! {0:x} {0:o}\n+//! ```\n+//!\n+//! This is invalid because the first argument is both referred to as a hexidecimal as well as an\n+//! octal.\n+//!\n+//! There are various parameters which do require a particular type, however. Namely if the syntax\n+//! `{:.*}` is used, then the number of characters to print precedes the actual object being formatted,\n+//! and the number of characters must have the type `uint`. Although a `uint` can be printed with\n+//! `{}`, it is illegal to reference an argument as such. For example this is another invalid\n+//! format string:\n+//!\n+//! ```text\n+//! {:.*} {0}\n+//! ```\n+//!\n+//! ### Formatting traits\n+//!\n+//! When requesting that an argument be formatted with a particular type, you are\n+//! actually requesting that an argument ascribes to a particular trait. This allows\n+//! multiple actual types to be formatted via `{:x}` (like `i8` as well as `int`).\n+//! The current mapping of types to traits is:\n+//!\n+//! * *nothing* \u21d2 `Show`\n+//! * `o` \u21d2 `Octal`\n+//! * `x` \u21d2 `LowerHex`\n+//! * `X` \u21d2 `UpperHex`\n+//! * `p` \u21d2 `Pointer`\n+//! * `b` \u21d2 `Binary`\n+//! * `e` \u21d2 `LowerExp`\n+//! * `E` \u21d2 `UpperExp`\n+//!\n+//! What this means is that any type of argument which implements the\n+//! `std::fmt::Binary` trait can then be formatted with `{:b}`. Implementations are\n+//! provided for these traits for a number of primitive types by the standard\n+//! library as well. If no format is specified (as in `{}` or `{:6}`), then the\n+//! format trait used is the `Show` trait. This is one of the more commonly\n+//! implemented traits when formatting a custom type.\n+//!\n+//! When implementing a format trait for your own type, you will have to implement a\n+//! method of the signature:\n+//!\n+//! ```rust\n+//! # use std::fmt;\n+//! # struct Foo; // our custom type\n+//! # impl fmt::Show for Foo {\n+//! fn fmt(&self, f: &mut std::fmt::Formatter) -> fmt::Result {\n+//! # write!(f, \"testing, testing\")\n+//! # } }\n+//! ```\n+//!\n+//! Your type will be passed as `self` by-reference, and then the function should\n+//! emit output into the `f.buf` stream. It is up to each format trait\n+//! implementation to correctly adhere to the requested formatting parameters. The\n+//! values of these parameters will be listed in the fields of the `Formatter`\n+//! struct. In order to help with this, the `Formatter` struct also provides some\n+//! helper methods.\n+//!\n+//! Additionally, the return value of this function is `fmt::Result` which is a\n+//! typedef to `Result<(), IoError>` (also known as `IoResult<()>`). Formatting\n+//! implementations should ensure that they return errors from `write!` correctly\n+//! (propagating errors upward).\n+//!\n+//! An example of implementing the formatting traits would look\n+//! like:\n+//!\n+//! ```rust\n+//! use std::fmt;\n+//! use std::f64;\n+//! use std::num::Float;\n+//!\n+//! struct Vector2D {\n+//!     x: int,\n+//!     y: int,\n+//! }\n+//!\n+//! impl fmt::Show for Vector2D {\n+//!     fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {\n+//!         // The `f` value implements the `Writer` trait, which is what the\n+//!         // write! macro is expecting. Note that this formatting ignores the\n+//!         // various flags provided to format strings.\n+//!         write!(f, \"({}, {})\", self.x, self.y)\n+//!     }\n+//! }\n+//!\n+//! // Different traits allow different forms of output of a type. The meaning of\n+//! // this format is to print the magnitude of a vector.\n+//! impl fmt::Binary for Vector2D {\n+//!     fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {\n+//!         let magnitude = (self.x * self.x + self.y * self.y) as f64;\n+//!         let magnitude = magnitude.sqrt();\n+//!\n+//!         // Respect the formatting flags by using the helper method\n+//!         // `pad_integral` on the Formatter object. See the method documentation\n+//!         // for details, and the function `pad` can be used to pad strings.\n+//!         let decimals = f.precision().unwrap_or(3);\n+//!         let string = f64::to_str_exact(magnitude, decimals);\n+//!         f.pad_integral(true, \"\", string.as_bytes())\n+//!     }\n+//! }\n+//!\n+//! fn main() {\n+//!     let myvector = Vector2D { x: 3, y: 4 };\n+//!\n+//!     println!(\"{}\", myvector);       // => \"(3, 4)\"\n+//!     println!(\"{:10.3b}\", myvector); // => \"     5.000\"\n+//! }\n+//! ```\n+//!\n+//! ### Related macros\n+//!\n+//! There are a number of related macros in the `format!` family. The ones that are\n+//! currently implemented are:\n+//!\n+//! ```ignore\n+//! format!      // described above\n+//! write!       // first argument is a &mut io::Writer, the destination\n+//! writeln!     // same as write but appends a newline\n+//! print!       // the format string is printed to the standard output\n+//! println!     // same as print but appends a newline\n+//! format_args! // described below.\n+//! ```\n+//!\n+//! #### `write!`\n+//!\n+//! This and `writeln` are two macros which are used to emit the format string to a\n+//! specified stream. This is used to prevent intermediate allocations of format\n+//! strings and instead directly write the output. Under the hood, this function is\n+//! actually invoking the `write` function defined in this module. Example usage is:\n+//!\n+//! ```rust\n+//! # #![allow(unused_must_use)]\n+//! use std::io;\n+//!\n+//! let mut w = Vec::new();\n+//! write!(&mut w as &mut io::Writer, \"Hello {}!\", \"world\");\n+//! ```\n+//!\n+//! #### `print!`\n+//!\n+//! This and `println` emit their output to stdout. Similarly to the `write!` macro,\n+//! the goal of these macros is to avoid intermediate allocations when printing\n+//! output. Example usage is:\n+//!\n+//! ```rust\n+//! print!(\"Hello {}!\", \"world\");\n+//! println!(\"I have a newline {}\", \"character at the end\");\n+//! ```\n+//!\n+//! #### `format_args!`\n+//! This is a curious macro which is used to safely pass around\n+//! an opaque object describing the format string. This object\n+//! does not require any heap allocations to create, and it only\n+//! references information on the stack. Under the hood, all of\n+//! the related macros are implemented in terms of this. First\n+//! off, some example usage is:\n+//!\n+//! ```\n+//! use std::fmt;\n+//! use std::io;\n+//!\n+//! # #[allow(unused_must_use)]\n+//! # fn main() {\n+//! format_args!(fmt::format, \"this returns {}\", \"String\");\n+//!\n+//! let some_writer: &mut io::Writer = &mut io::stdout();\n+//! format_args!(|args| { write!(some_writer, \"{}\", args) }, \"print with a {}\", \"closure\");\n+//!\n+//! fn my_fmt_fn(args: &fmt::Arguments) {\n+//!     write!(&mut io::stdout(), \"{}\", args);\n+//! }\n+//! format_args!(my_fmt_fn, \"or a {} too\", \"function\");\n+//! # }\n+//! ```\n+//!\n+//! The first argument of the `format_args!` macro is a function (or closure) which\n+//! takes one argument of type `&fmt::Arguments`. This structure can then be\n+//! passed to the `write` and `format` functions inside this module in order to\n+//! process the format string. The goal of this macro is to even further prevent\n+//! intermediate allocations when dealing formatting strings.\n+//!\n+//! For example, a logging library could use the standard formatting syntax, but it\n+//! would internally pass around this structure until it has been determined where\n+//! output should go to.\n+//!\n+//! It is unsafe to programmatically create an instance of `fmt::Arguments` because\n+//! the operations performed when executing a format string require the compile-time\n+//! checks provided by the compiler. The `format_args!` macro is the only method of\n+//! safely creating these structures, but they can be unsafely created with the\n+//! constructor provided.\n+//!\n+//! ## Syntax\n+//!\n+//! The syntax for the formatting language used is drawn from other languages, so it\n+//! should not be too alien. Arguments are formatted with python-like syntax,\n+//! meaning that arguments are surrounded by `{}` instead of the C-like `%`. The\n+//! actual grammar for the formatting syntax is:\n+//!\n+//! ```text\n+//! format_string := <text> [ format <text> ] *\n+//! format := '{' [ argument ] [ ':' format_spec ] '}'\n+//! argument := integer | identifier\n+//!\n+//! format_spec := [[fill]align][sign]['#'][0][width]['.' precision][type]\n+//! fill := character\n+//! align := '<' | '^' | '>'\n+//! sign := '+' | '-'\n+//! width := count\n+//! precision := count | '*'\n+//! type := identifier | ''\n+//! count := parameter | integer\n+//! parameter := integer '$'\n+//! ```\n+//!\n+//! ## Formatting Parameters\n+//!\n+//! Each argument being formatted can be transformed by a number of formatting\n+//! parameters (corresponding to `format_spec` in the syntax above). These\n+//! parameters affect the string representation of what's being formatted. This\n+//! syntax draws heavily from Python's, so it may seem a bit familiar.\n+//!\n+//! ### Fill/Alignment\n+//!\n+//! The fill character is provided normally in conjunction with the `width`\n+//! parameter. This indicates that if the value being formatted is smaller than\n+//! `width` some extra characters will be printed around it. The extra characters\n+//! are specified by `fill`, and the alignment can be one of two options:\n+//!\n+//! * `<` - the argument is left-aligned in `width` columns\n+//! * `^` - the argument is center-aligned in `width` columns\n+//! * `>` - the argument is right-aligned in `width` columns\n+//!\n+//! ### Sign/#/0\n+//!\n+//! These can all be interpreted as flags for a particular formatter.\n+//!\n+//! * '+' - This is intended for numeric types and indicates that the sign should\n+//!         always be printed. Positive signs are never printed by default, and the\n+//!         negative sign is only printed by default for the `Signed` trait. This\n+//!         flag indicates that the correct sign (+ or -) should always be printed.\n+//! * '-' - Currently not used\n+//! * '#' - This flag is indicates that the \"alternate\" form of printing should be\n+//!         used. By default, this only applies to the integer formatting traits and\n+//!         performs like:\n+//!     * `x` - precedes the argument with a \"0x\"\n+//!     * `X` - precedes the argument with a \"0x\"\n+//!     * `t` - precedes the argument with a \"0b\"\n+//!     * `o` - precedes the argument with a \"0o\"\n+//! * '0' - This is used to indicate for integer formats that the padding should\n+//!         both be done with a `0` character as well as be sign-aware. A format\n+//!         like `{:08d}` would yield `00000001` for the integer `1`, while the same\n+//!         format would yield `-0000001` for the integer `-1`. Notice that the\n+//!         negative version has one fewer zero than the positive version.\n+//!\n+//! ### Width\n+//!\n+//! This is a parameter for the \"minimum width\" that the format should take up. If\n+//! the value's string does not fill up this many characters, then the padding\n+//! specified by fill/alignment will be used to take up the required space.\n+//!\n+//! The default fill/alignment for non-numerics is a space and left-aligned. The\n+//! defaults for numeric formatters is also a space but with right-alignment. If the\n+//! '0' flag is specified for numerics, then the implicit fill character is '0'.\n+//!\n+//! The value for the width can also be provided as a `uint` in the list of\n+//! parameters by using the `2$` syntax indicating that the second argument is a\n+//! `uint` specifying the width.\n+//!\n+//! ### Precision\n+//!\n+//! For non-numeric types, this can be considered a \"maximum width\". If the\n+//! resulting string is longer than this width, then it is truncated down to this\n+//! many characters and only those are emitted.\n+//!\n+//! For integral types, this has no meaning currently.\n+//!\n+//! For floating-point types, this indicates how many digits after the decimal point\n+//! should be printed.\n+//!\n+//! ## Escaping\n+//!\n+//! The literal characters `{` and `}` may be included in a string by preceding them\n+//! with the same character. For example, the `{` character is escaped with `{{` and\n+//! the `}` character is escaped with `}}`.\n \n #![experimental]\n "}, {"sha": "ac68e1ef121fbed040c124b42664a459b12eb472", "filename": "src/libstd/hash.rs", "status": "modified", "additions": 50, "deletions": 52, "changes": 102, "blob_url": "https://github.com/rust-lang/rust/blob/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibstd%2Fhash.rs", "raw_url": "https://github.com/rust-lang/rust/raw/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibstd%2Fhash.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibstd%2Fhash.rs?ref=cd5c8235c5448a7234548c772468c8d2e8f150d9", "patch": "@@ -8,58 +8,56 @@\n // option. This file may not be copied, modified, or distributed\n // except according to those terms.\n \n-/*!\n- * Generic hashing support.\n- *\n- * This module provides a generic way to compute the hash of a value. The\n- * simplest way to make a type hashable is to use `#[deriving(Hash)]`:\n- *\n- * # Example\n- *\n- * ```rust\n- * use std::hash;\n- * use std::hash::Hash;\n- *\n- * #[deriving(Hash)]\n- * struct Person {\n- *     id: uint,\n- *     name: String,\n- *     phone: u64,\n- * }\n- *\n- * let person1 = Person { id: 5, name: \"Janet\".to_string(), phone: 555_666_7777 };\n- * let person2 = Person { id: 5, name: \"Bob\".to_string(), phone: 555_666_7777 };\n- *\n- * assert!(hash::hash(&person1) != hash::hash(&person2));\n- * ```\n- *\n- * If you need more control over how a value is hashed, you need to implement\n- * the trait `Hash`:\n- *\n- * ```rust\n- * use std::hash;\n- * use std::hash::Hash;\n- * use std::hash::sip::SipState;\n- *\n- * struct Person {\n- *     id: uint,\n- *     name: String,\n- *     phone: u64,\n- * }\n- *\n- * impl Hash for Person {\n- *     fn hash(&self, state: &mut SipState) {\n- *         self.id.hash(state);\n- *         self.phone.hash(state);\n- *     }\n- * }\n- *\n- * let person1 = Person { id: 5, name: \"Janet\".to_string(), phone: 555_666_7777 };\n- * let person2 = Person { id: 5, name: \"Bob\".to_string(), phone: 555_666_7777 };\n- *\n- * assert!(hash::hash(&person1) == hash::hash(&person2));\n- * ```\n- */\n+//! Generic hashing support.\n+//!\n+//! This module provides a generic way to compute the hash of a value. The\n+//! simplest way to make a type hashable is to use `#[deriving(Hash)]`:\n+//!\n+//! # Example\n+//!\n+//! ```rust\n+//! use std::hash;\n+//! use std::hash::Hash;\n+//!\n+//! #[deriving(Hash)]\n+//! struct Person {\n+//!     id: uint,\n+//!     name: String,\n+//!     phone: u64,\n+//! }\n+//!\n+//! let person1 = Person { id: 5, name: \"Janet\".to_string(), phone: 555_666_7777 };\n+//! let person2 = Person { id: 5, name: \"Bob\".to_string(), phone: 555_666_7777 };\n+//!\n+//! assert!(hash::hash(&person1) != hash::hash(&person2));\n+//! ```\n+//!\n+//! If you need more control over how a value is hashed, you need to implement\n+//! the trait `Hash`:\n+//!\n+//! ```rust\n+//! use std::hash;\n+//! use std::hash::Hash;\n+//! use std::hash::sip::SipState;\n+//!\n+//! struct Person {\n+//!     id: uint,\n+//!     name: String,\n+//!     phone: u64,\n+//! }\n+//!\n+//! impl Hash for Person {\n+//!     fn hash(&self, state: &mut SipState) {\n+//!         self.id.hash(state);\n+//!         self.phone.hash(state);\n+//!     }\n+//! }\n+//!\n+//! let person1 = Person { id: 5, name: \"Janet\".to_string(), phone: 555_666_7777 };\n+//! let person2 = Person { id: 5, name: \"Bob\".to_string(), phone: 555_666_7777 };\n+//!\n+//! assert!(hash::hash(&person1) == hash::hash(&person2));\n+//! ```\n \n #![experimental]\n "}, {"sha": "da69cee69e6500160b36e5ba060c38e2205cce64", "filename": "src/libstd/io/fs.rs", "status": "modified", "additions": 39, "deletions": 41, "changes": 80, "blob_url": "https://github.com/rust-lang/rust/blob/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibstd%2Fio%2Ffs.rs", "raw_url": "https://github.com/rust-lang/rust/raw/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibstd%2Fio%2Ffs.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibstd%2Fio%2Ffs.rs?ref=cd5c8235c5448a7234548c772468c8d2e8f150d9", "patch": "@@ -10,47 +10,45 @@\n //\n // ignore-lexer-test FIXME #15679\n \n-/*! Synchronous File I/O\n-\n-This module provides a set of functions and traits for working\n-with regular files & directories on a filesystem.\n-\n-At the top-level of the module are a set of freestanding functions, associated\n-with various filesystem operations. They all operate on `Path` objects.\n-\n-All operations in this module, including those as part of `File` et al\n-block the task during execution. In the event of failure, all functions/methods\n-will return an `IoResult` type with an `Err` value.\n-\n-Also included in this module is an implementation block on the `Path` object\n-defined in `std::path::Path`. The impl adds useful methods about inspecting the\n-metadata of a file. This includes getting the `stat` information, reading off\n-particular bits of it, etc.\n-\n-# Example\n-\n-```rust\n-# #![allow(unused_must_use)]\n-use std::io::fs::PathExtensions;\n-use std::io::{File, fs};\n-\n-let path = Path::new(\"foo.txt\");\n-\n-// create the file, whether it exists or not\n-let mut file = File::create(&path);\n-file.write(b\"foobar\");\n-# drop(file);\n-\n-// open the file in read-only mode\n-let mut file = File::open(&path);\n-file.read_to_end();\n-\n-println!(\"{}\", path.stat().unwrap().size);\n-# drop(file);\n-fs::unlink(&path);\n-```\n-\n-*/\n+//! Synchronous File I/O\n+//!\n+//! This module provides a set of functions and traits for working\n+//! with regular files & directories on a filesystem.\n+//!\n+//! At the top-level of the module are a set of freestanding functions, associated\n+//! with various filesystem operations. They all operate on `Path` objects.\n+//!\n+//! All operations in this module, including those as part of `File` et al\n+//! block the task during execution. In the event of failure, all functions/methods\n+//! will return an `IoResult` type with an `Err` value.\n+//!\n+//! Also included in this module is an implementation block on the `Path` object\n+//! defined in `std::path::Path`. The impl adds useful methods about inspecting the\n+//! metadata of a file. This includes getting the `stat` information, reading off\n+//! particular bits of it, etc.\n+//!\n+//! # Example\n+//!\n+//! ```rust\n+//! # #![allow(unused_must_use)]\n+//! use std::io::fs::PathExtensions;\n+//! use std::io::{File, fs};\n+//!\n+//! let path = Path::new(\"foo.txt\");\n+//!\n+//! // create the file, whether it exists or not\n+//! let mut file = File::create(&path);\n+//! file.write(b\"foobar\");\n+//! # drop(file);\n+//!\n+//! // open the file in read-only mode\n+//! let mut file = File::open(&path);\n+//! file.read_to_end();\n+//!\n+//! println!(\"{}\", path.stat().unwrap().size);\n+//! # drop(file);\n+//! fs::unlink(&path);\n+//! ```\n \n use clone::Clone;\n use io::standard_error;"}, {"sha": "fc6ee58346dec63dd5433be70cdb0da62bcd3d24", "filename": "src/libstd/io/mod.rs", "status": "modified", "additions": 199, "deletions": 201, "changes": 400, "blob_url": "https://github.com/rust-lang/rust/blob/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibstd%2Fio%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibstd%2Fio%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibstd%2Fio%2Fmod.rs?ref=cd5c8235c5448a7234548c772468c8d2e8f150d9", "patch": "@@ -16,207 +16,205 @@\n //        error handling\n \n \n-/*! I/O, including files, networking, timers, and processes\n-\n-`std::io` provides Rust's basic I/O types,\n-for reading and writing to files, TCP, UDP,\n-and other types of sockets and pipes,\n-manipulating the file system, spawning processes.\n-\n-# Examples\n-\n-Some examples of obvious things you might want to do\n-\n-* Read lines from stdin\n-\n-    ```rust\n-    use std::io;\n-\n-    for line in io::stdin().lines() {\n-        print!(\"{}\", line.unwrap());\n-    }\n-    ```\n-\n-* Read a complete file\n-\n-    ```rust\n-    use std::io::File;\n-\n-    let contents = File::open(&Path::new(\"message.txt\")).read_to_end();\n-    ```\n-\n-* Write a line to a file\n-\n-    ```rust\n-    # #![allow(unused_must_use)]\n-    use std::io::File;\n-\n-    let mut file = File::create(&Path::new(\"message.txt\"));\n-    file.write(b\"hello, file!\\n\");\n-    # drop(file);\n-    # ::std::io::fs::unlink(&Path::new(\"message.txt\"));\n-    ```\n-\n-* Iterate over the lines of a file\n-\n-    ```rust,no_run\n-    use std::io::BufferedReader;\n-    use std::io::File;\n-\n-    let path = Path::new(\"message.txt\");\n-    let mut file = BufferedReader::new(File::open(&path));\n-    for line in file.lines() {\n-        print!(\"{}\", line.unwrap());\n-    }\n-    ```\n-\n-* Pull the lines of a file into a vector of strings\n-\n-    ```rust,no_run\n-    use std::io::BufferedReader;\n-    use std::io::File;\n-\n-    let path = Path::new(\"message.txt\");\n-    let mut file = BufferedReader::new(File::open(&path));\n-    let lines: Vec<String> = file.lines().map(|x| x.unwrap()).collect();\n-    ```\n-\n-* Make a simple TCP client connection and request\n-\n-    ```rust\n-    # #![allow(unused_must_use)]\n-    use std::io::TcpStream;\n-\n-    # // connection doesn't fail if a server is running on 8080\n-    # // locally, we still want to be type checking this code, so lets\n-    # // just stop it running (#11576)\n-    # if false {\n-    let mut socket = TcpStream::connect(\"127.0.0.1:8080\").unwrap();\n-    socket.write(b\"GET / HTTP/1.0\\n\\n\");\n-    let response = socket.read_to_end();\n-    # }\n-    ```\n-\n-* Make a simple TCP server\n-\n-    ```rust\n-    # fn main() { }\n-    # fn foo() {\n-    # #![allow(dead_code)]\n-    use std::io::{TcpListener, TcpStream};\n-    use std::io::{Acceptor, Listener};\n-\n-    let listener = TcpListener::bind(\"127.0.0.1:80\");\n-\n-    // bind the listener to the specified address\n-    let mut acceptor = listener.listen();\n-\n-    fn handle_client(mut stream: TcpStream) {\n-        // ...\n-    # &mut stream; // silence unused mutability/variable warning\n-    }\n-    // accept connections and process them, spawning a new tasks for each one\n-    for stream in acceptor.incoming() {\n-        match stream {\n-            Err(e) => { /* connection failed */ }\n-            Ok(stream) => spawn(proc() {\n-                // connection succeeded\n-                handle_client(stream)\n-            })\n-        }\n-    }\n-\n-    // close the socket server\n-    drop(acceptor);\n-    # }\n-    ```\n-\n-\n-# Error Handling\n-\n-I/O is an area where nearly every operation can result in unexpected\n-errors. Errors should be painfully visible when they happen, and handling them\n-should be easy to work with. It should be convenient to handle specific I/O\n-errors, and it should also be convenient to not deal with I/O errors.\n-\n-Rust's I/O employs a combination of techniques to reduce boilerplate\n-while still providing feedback about errors. The basic strategy:\n-\n-* All I/O operations return `IoResult<T>` which is equivalent to\n-  `Result<T, IoError>`. The `Result` type is defined in the `std::result`\n-  module.\n-* If the `Result` type goes unused, then the compiler will by default emit a\n-  warning about the unused result. This is because `Result` has the\n-  `#[must_use]` attribute.\n-* Common traits are implemented for `IoResult`, e.g.\n-  `impl<R: Reader> Reader for IoResult<R>`, so that error values do not have\n-  to be 'unwrapped' before use.\n-\n-These features combine in the API to allow for expressions like\n-`File::create(&Path::new(\"diary.txt\")).write(b\"Met a girl.\\n\")`\n-without having to worry about whether \"diary.txt\" exists or whether\n-the write succeeds. As written, if either `new` or `write_line`\n-encounters an error then the result of the entire expression will\n-be an error.\n-\n-If you wanted to handle the error though you might write:\n-\n-```rust\n-# #![allow(unused_must_use)]\n-use std::io::File;\n-\n-match File::create(&Path::new(\"diary.txt\")).write(b\"Met a girl.\\n\") {\n-    Ok(()) => (), // succeeded\n-    Err(e) => println!(\"failed to write to my diary: {}\", e),\n-}\n-\n-# ::std::io::fs::unlink(&Path::new(\"diary.txt\"));\n-```\n-\n-So what actually happens if `create` encounters an error?\n-It's important to know that what `new` returns is not a `File`\n-but an `IoResult<File>`.  If the file does not open, then `new` will simply\n-return `Err(..)`. Because there is an implementation of `Writer` (the trait\n-required ultimately required for types to implement `write_line`) there is no\n-need to inspect or unwrap the `IoResult<File>` and we simply call `write_line`\n-on it. If `new` returned an `Err(..)` then the followup call to `write_line`\n-will also return an error.\n-\n-## `try!`\n-\n-Explicit pattern matching on `IoResult`s can get quite verbose, especially\n-when performing many I/O operations. Some examples (like those above) are\n-alleviated with extra methods implemented on `IoResult`, but others have more\n-complex interdependencies among each I/O operation.\n-\n-The `try!` macro from `std::macros` is provided as a method of early-return\n-inside `Result`-returning functions. It expands to an early-return on `Err`\n-and otherwise unwraps the contained `Ok` value.\n-\n-If you wanted to read several `u32`s from a file and return their product:\n-\n-```rust\n-use std::io::{File, IoResult};\n-\n-fn file_product(p: &Path) -> IoResult<u32> {\n-    let mut f = File::open(p);\n-    let x1 = try!(f.read_le_u32());\n-    let x2 = try!(f.read_le_u32());\n-\n-    Ok(x1 * x2)\n-}\n-\n-match file_product(&Path::new(\"numbers.bin\")) {\n-    Ok(x) => println!(\"{}\", x),\n-    Err(e) => println!(\"Failed to read numbers!\")\n-}\n-```\n-\n-With `try!` in `file_product`, each `read_le_u32` need not be directly\n-concerned with error handling; instead its caller is responsible for\n-responding to errors that may occur while attempting to read the numbers.\n-\n-*/\n+//! I/O, including files, networking, timers, and processes\n+//!\n+//! `std::io` provides Rust's basic I/O types,\n+//! for reading and writing to files, TCP, UDP,\n+//! and other types of sockets and pipes,\n+//! manipulating the file system, spawning processes.\n+//!\n+//! # Examples\n+//!\n+//! Some examples of obvious things you might want to do\n+//!\n+//! * Read lines from stdin\n+//!\n+//!     ```rust\n+//!     use std::io;\n+//!\n+//!     for line in io::stdin().lines() {\n+//!         print!(\"{}\", line.unwrap());\n+//!     }\n+//!     ```\n+//!\n+//! * Read a complete file\n+//!\n+//!     ```rust\n+//!     use std::io::File;\n+//!\n+//!     let contents = File::open(&Path::new(\"message.txt\")).read_to_end();\n+//!     ```\n+//!\n+//! * Write a line to a file\n+//!\n+//!     ```rust\n+//!     # #![allow(unused_must_use)]\n+//!     use std::io::File;\n+//!\n+//!     let mut file = File::create(&Path::new(\"message.txt\"));\n+//!     file.write(b\"hello, file!\\n\");\n+//!     # drop(file);\n+//!     # ::std::io::fs::unlink(&Path::new(\"message.txt\"));\n+//!     ```\n+//!\n+//! * Iterate over the lines of a file\n+//!\n+//!     ```rust,no_run\n+//!     use std::io::BufferedReader;\n+//!     use std::io::File;\n+//!\n+//!     let path = Path::new(\"message.txt\");\n+//!     let mut file = BufferedReader::new(File::open(&path));\n+//!     for line in file.lines() {\n+//!         print!(\"{}\", line.unwrap());\n+//!     }\n+//!     ```\n+//!\n+//! * Pull the lines of a file into a vector of strings\n+//!\n+//!     ```rust,no_run\n+//!     use std::io::BufferedReader;\n+//!     use std::io::File;\n+//!\n+//!     let path = Path::new(\"message.txt\");\n+//!     let mut file = BufferedReader::new(File::open(&path));\n+//!     let lines: Vec<String> = file.lines().map(|x| x.unwrap()).collect();\n+//!     ```\n+//!\n+//! * Make a simple TCP client connection and request\n+//!\n+//!     ```rust\n+//!     # #![allow(unused_must_use)]\n+//!     use std::io::TcpStream;\n+//!\n+//!     # // connection doesn't fail if a server is running on 8080\n+//!     # // locally, we still want to be type checking this code, so lets\n+//!     # // just stop it running (#11576)\n+//!     # if false {\n+//!     let mut socket = TcpStream::connect(\"127.0.0.1:8080\").unwrap();\n+//!     socket.write(b\"GET / HTTP/1.0\\n\\n\");\n+//!     let response = socket.read_to_end();\n+//!     # }\n+//!     ```\n+//!\n+//! * Make a simple TCP server\n+//!\n+//!     ```rust\n+//!     # fn main() { }\n+//!     # fn foo() {\n+//!     # #![allow(dead_code)]\n+//!     use std::io::{TcpListener, TcpStream};\n+//!     use std::io::{Acceptor, Listener};\n+//!\n+//!     let listener = TcpListener::bind(\"127.0.0.1:80\");\n+//!\n+//!     // bind the listener to the specified address\n+//!     let mut acceptor = listener.listen();\n+//!\n+//!     fn handle_client(mut stream: TcpStream) {\n+//!         // ...\n+//!     # &mut stream; // silence unused mutability/variable warning\n+//!     }\n+//!     // accept connections and process them, spawning a new tasks for each one\n+//!     for stream in acceptor.incoming() {\n+//!         match stream {\n+//!             Err(e) => { /* connection failed */ }\n+//!             Ok(stream) => spawn(proc() {\n+//!                 // connection succeeded\n+//!                 handle_client(stream)\n+//!             })\n+//!         }\n+//!     }\n+//!\n+//!     // close the socket server\n+//!     drop(acceptor);\n+//!     # }\n+//!     ```\n+//!\n+//!\n+//! # Error Handling\n+//!\n+//! I/O is an area where nearly every operation can result in unexpected\n+//! errors. Errors should be painfully visible when they happen, and handling them\n+//! should be easy to work with. It should be convenient to handle specific I/O\n+//! errors, and it should also be convenient to not deal with I/O errors.\n+//!\n+//! Rust's I/O employs a combination of techniques to reduce boilerplate\n+//! while still providing feedback about errors. The basic strategy:\n+//!\n+//! * All I/O operations return `IoResult<T>` which is equivalent to\n+//!   `Result<T, IoError>`. The `Result` type is defined in the `std::result`\n+//!   module.\n+//! * If the `Result` type goes unused, then the compiler will by default emit a\n+//!   warning about the unused result. This is because `Result` has the\n+//!   `#[must_use]` attribute.\n+//! * Common traits are implemented for `IoResult`, e.g.\n+//!   `impl<R: Reader> Reader for IoResult<R>`, so that error values do not have\n+//!   to be 'unwrapped' before use.\n+//!\n+//! These features combine in the API to allow for expressions like\n+//! `File::create(&Path::new(\"diary.txt\")).write(b\"Met a girl.\\n\")`\n+//! without having to worry about whether \"diary.txt\" exists or whether\n+//! the write succeeds. As written, if either `new` or `write_line`\n+//! encounters an error then the result of the entire expression will\n+//! be an error.\n+//!\n+//! If you wanted to handle the error though you might write:\n+//!\n+//! ```rust\n+//! # #![allow(unused_must_use)]\n+//! use std::io::File;\n+//!\n+//! match File::create(&Path::new(\"diary.txt\")).write(b\"Met a girl.\\n\") {\n+//!     Ok(()) => (), // succeeded\n+//!     Err(e) => println!(\"failed to write to my diary: {}\", e),\n+//! }\n+//!\n+//! # ::std::io::fs::unlink(&Path::new(\"diary.txt\"));\n+//! ```\n+//!\n+//! So what actually happens if `create` encounters an error?\n+//! It's important to know that what `new` returns is not a `File`\n+//! but an `IoResult<File>`.  If the file does not open, then `new` will simply\n+//! return `Err(..)`. Because there is an implementation of `Writer` (the trait\n+//! required ultimately required for types to implement `write_line`) there is no\n+//! need to inspect or unwrap the `IoResult<File>` and we simply call `write_line`\n+//! on it. If `new` returned an `Err(..)` then the followup call to `write_line`\n+//! will also return an error.\n+//!\n+//! ## `try!`\n+//!\n+//! Explicit pattern matching on `IoResult`s can get quite verbose, especially\n+//! when performing many I/O operations. Some examples (like those above) are\n+//! alleviated with extra methods implemented on `IoResult`, but others have more\n+//! complex interdependencies among each I/O operation.\n+//!\n+//! The `try!` macro from `std::macros` is provided as a method of early-return\n+//! inside `Result`-returning functions. It expands to an early-return on `Err`\n+//! and otherwise unwraps the contained `Ok` value.\n+//!\n+//! If you wanted to read several `u32`s from a file and return their product:\n+//!\n+//! ```rust\n+//! use std::io::{File, IoResult};\n+//!\n+//! fn file_product(p: &Path) -> IoResult<u32> {\n+//!     let mut f = File::open(p);\n+//!     let x1 = try!(f.read_le_u32());\n+//!     let x2 = try!(f.read_le_u32());\n+//!\n+//!     Ok(x1 * x2)\n+//! }\n+//!\n+//! match file_product(&Path::new(\"numbers.bin\")) {\n+//!     Ok(x) => println!(\"{}\", x),\n+//!     Err(e) => println!(\"Failed to read numbers!\")\n+//! }\n+//! ```\n+//!\n+//! With `try!` in `file_product`, each `read_le_u32` need not be directly\n+//! concerned with error handling; instead its caller is responsible for\n+//! responding to errors that may occur while attempting to read the numbers.\n \n #![experimental]\n #![deny(unused_must_use)]"}, {"sha": "7de786921309153ed13eb6eb214c32980c17583a", "filename": "src/libstd/io/net/addrinfo.rs", "status": "modified", "additions": 4, "deletions": 8, "changes": 12, "blob_url": "https://github.com/rust-lang/rust/blob/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibstd%2Fio%2Fnet%2Faddrinfo.rs", "raw_url": "https://github.com/rust-lang/rust/raw/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibstd%2Fio%2Fnet%2Faddrinfo.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibstd%2Fio%2Fnet%2Faddrinfo.rs?ref=cd5c8235c5448a7234548c772468c8d2e8f150d9", "patch": "@@ -8,14 +8,10 @@\n // option. This file may not be copied, modified, or distributed\n // except according to those terms.\n \n-/*!\n-\n-Synchronous DNS Resolution\n-\n-Contains the functionality to perform DNS resolution in a style related to\n-getaddrinfo()\n-\n-*/\n+//! Synchronous DNS Resolution\n+//!\n+//! Contains the functionality to perform DNS resolution in a style related to\n+//! `getaddrinfo()`\n \n #![allow(missing_docs)]\n "}, {"sha": "ec997b71986cc4782471827e2dee86a72a5d2a66", "filename": "src/libstd/io/net/pipe.rs", "status": "modified", "additions": 9, "deletions": 13, "changes": 22, "blob_url": "https://github.com/rust-lang/rust/blob/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibstd%2Fio%2Fnet%2Fpipe.rs", "raw_url": "https://github.com/rust-lang/rust/raw/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibstd%2Fio%2Fnet%2Fpipe.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibstd%2Fio%2Fnet%2Fpipe.rs?ref=cd5c8235c5448a7234548c772468c8d2e8f150d9", "patch": "@@ -8,19 +8,15 @@\n // option. This file may not be copied, modified, or distributed\n // except according to those terms.\n \n-/*!\n-\n-Named pipes\n-\n-This module contains the ability to communicate over named pipes with\n-synchronous I/O. On windows, this corresponds to talking over a Named Pipe,\n-while on Unix it corresponds to UNIX domain sockets.\n-\n-These pipes are similar to TCP in the sense that you can have both a stream to a\n-server and a server itself. The server provided accepts other `UnixStream`\n-instances as clients.\n-\n-*/\n+//! Named pipes\n+//!\n+//! This module contains the ability to communicate over named pipes with\n+//! synchronous I/O. On windows, this corresponds to talking over a Named Pipe,\n+//! while on Unix it corresponds to UNIX domain sockets.\n+//!\n+//! These pipes are similar to TCP in the sense that you can have both a stream to a\n+//! server and a server itself. The server provided accepts other `UnixStream`\n+//! instances as clients.\n \n #![allow(missing_docs)]\n "}, {"sha": "665000eae883773ccd302a23b724502455593a06", "filename": "src/libstd/io/stdio.rs", "status": "modified", "additions": 16, "deletions": 18, "changes": 34, "blob_url": "https://github.com/rust-lang/rust/blob/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibstd%2Fio%2Fstdio.rs", "raw_url": "https://github.com/rust-lang/rust/raw/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibstd%2Fio%2Fstdio.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibstd%2Fio%2Fstdio.rs?ref=cd5c8235c5448a7234548c772468c8d2e8f150d9", "patch": "@@ -8,24 +8,22 @@\n // option. This file may not be copied, modified, or distributed\n // except according to those terms.\n \n-/*! Non-blocking access to stdin, stdout, and stderr.\n-\n-This module provides bindings to the local event loop's TTY interface, using it\n-to offer synchronous but non-blocking versions of stdio. These handles can be\n-inspected for information about terminal dimensions or for related information\n-about the stream or terminal to which it is attached.\n-\n-# Example\n-\n-```rust\n-# #![allow(unused_must_use)]\n-use std::io;\n-\n-let mut out = io::stdout();\n-out.write(b\"Hello, world!\");\n-```\n-\n-*/\n+//! Non-blocking access to stdin, stdout, and stderr.\n+//!\n+//! This module provides bindings to the local event loop's TTY interface, using it\n+//! to offer synchronous but non-blocking versions of stdio. These handles can be\n+//! inspected for information about terminal dimensions or for related information\n+//! about the stream or terminal to which it is attached.\n+//!\n+//! # Example\n+//!\n+//! ```rust\n+//! # #![allow(unused_must_use)]\n+//! use std::io;\n+//!\n+//! let mut out = io::stdout();\n+//! out.write(b\"Hello, world!\");\n+//! ```\n \n use self::StdSource::*;\n "}, {"sha": "af56735021e8646bda59f2ea981bc925817fe2fc", "filename": "src/libstd/io/test.rs", "status": "modified", "additions": 6, "deletions": 9, "changes": 15, "blob_url": "https://github.com/rust-lang/rust/blob/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibstd%2Fio%2Ftest.rs", "raw_url": "https://github.com/rust-lang/rust/raw/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibstd%2Fio%2Ftest.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibstd%2Fio%2Ftest.rs?ref=cd5c8235c5448a7234548c772468c8d2e8f150d9", "patch": "@@ -8,7 +8,7 @@\n // option. This file may not be copied, modified, or distributed\n // except according to those terms.\n \n-/*! Various utility functions useful for writing I/O tests */\n+//! Various utility functions useful for writing I/O tests\n \n #![macro_escape]\n \n@@ -95,17 +95,14 @@ pub fn raise_fd_limit() {\n     unsafe { darwin_fd_limit::raise_fd_limit() }\n }\n \n+/// darwin_fd_limit exists to work around an issue where launchctl on Mac OS X defaults the rlimit\n+/// maxfiles to 256/unlimited. The default soft limit of 256 ends up being far too low for our\n+/// multithreaded scheduler testing, depending on the number of cores available.\n+///\n+/// This fixes issue #7772.\n #[cfg(target_os=\"macos\")]\n #[allow(non_camel_case_types)]\n mod darwin_fd_limit {\n-    /*!\n-     * darwin_fd_limit exists to work around an issue where launchctl on Mac OS X defaults the\n-     * rlimit maxfiles to 256/unlimited. The default soft limit of 256 ends up being far too low\n-     * for our multithreaded scheduler testing, depending on the number of cores available.\n-     *\n-     * This fixes issue #7772.\n-     */\n-\n     use libc;\n     type rlim_t = libc::uint64_t;\n     #[repr(C)]"}, {"sha": "ad02b534d04c647de4572bc41c4748519882a906", "filename": "src/libstd/io/timer.rs", "status": "modified", "additions": 4, "deletions": 8, "changes": 12, "blob_url": "https://github.com/rust-lang/rust/blob/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibstd%2Fio%2Ftimer.rs", "raw_url": "https://github.com/rust-lang/rust/raw/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibstd%2Fio%2Ftimer.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibstd%2Fio%2Ftimer.rs?ref=cd5c8235c5448a7234548c772468c8d2e8f150d9", "patch": "@@ -8,14 +8,10 @@\n // option. This file may not be copied, modified, or distributed\n // except according to those terms.\n \n-/*!\n-\n-Synchronous Timers\n-\n-This module exposes the functionality to create timers, block the current task,\n-and create receivers which will receive notifications after a period of time.\n-\n-*/\n+//! Synchronous Timers\n+//!\n+//! This module exposes the functionality to create timers, block the current task,\n+//! and create receivers which will receive notifications after a period of time.\n \n // FIXME: These functions take Durations but only pass ms to the backend impls.\n "}, {"sha": "393283ff64c5bf079f1acc27b2ad779a3baaa7bf", "filename": "src/libstd/io/util.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibstd%2Fio%2Futil.rs", "raw_url": "https://github.com/rust-lang/rust/raw/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibstd%2Fio%2Futil.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibstd%2Fio%2Futil.rs?ref=cd5c8235c5448a7234548c772468c8d2e8f150d9", "patch": "@@ -8,7 +8,7 @@\n // option. This file may not be copied, modified, or distributed\n // except according to those terms.\n \n-/*! Utility implementations of Reader and Writer */\n+//! Utility implementations of Reader and Writer\n \n use prelude::*;\n use cmp;"}, {"sha": "b9a103d3d9b2e01897889017fdc12f90cf03a195", "filename": "src/libstd/os.rs", "status": "modified", "additions": 13, "deletions": 17, "changes": 30, "blob_url": "https://github.com/rust-lang/rust/blob/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibstd%2Fos.rs", "raw_url": "https://github.com/rust-lang/rust/raw/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibstd%2Fos.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibstd%2Fos.rs?ref=cd5c8235c5448a7234548c772468c8d2e8f150d9", "patch": "@@ -8,23 +8,19 @@\n // option. This file may not be copied, modified, or distributed\n // except according to those terms.\n \n-/*!\n- * Higher-level interfaces to libc::* functions and operating system services.\n- *\n- * In general these take and return rust types, use rust idioms (enums,\n- * closures, vectors) rather than C idioms, and do more extensive safety\n- * checks.\n- *\n- * This module is not meant to only contain 1:1 mappings to libc entries; any\n- * os-interface code that is reasonably useful and broadly applicable can go\n- * here. Including utility routines that merely build on other os code.\n- *\n- * We assume the general case is that users do not care, and do not want to\n- * be made to care, which operating system they are on. While they may want\n- * to special case various special cases -- and so we will not _hide_ the\n- * facts of which OS the user is on -- they should be given the opportunity\n- * to write OS-ignorant code by default.\n- */\n+//! Higher-level interfaces to libc::* functions and operating system services.\n+//!\n+//! In general these take and return rust types, use rust idioms (enums, closures, vectors) rather\n+//! than C idioms, and do more extensive safety checks.\n+//!\n+//! This module is not meant to only contain 1:1 mappings to libc entries; any os-interface code\n+//! that is reasonably useful and broadly applicable can go here. Including utility routines that\n+//! merely build on other os code.\n+//!\n+//! We assume the general case is that users do not care, and do not want to be made to care, which\n+//! operating system they are on. While they may want to special case various special cases -- and\n+//! so we will not _hide_ the facts of which OS the user is on -- they should be given the\n+//! opportunity to write OS-ignorant code by default.\n \n #![experimental]\n "}, {"sha": "b17106e811f622f5201baf2d6624020c2bc01cd8", "filename": "src/libstd/path/mod.rs", "status": "modified", "additions": 50, "deletions": 56, "changes": 106, "blob_url": "https://github.com/rust-lang/rust/blob/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibstd%2Fpath%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibstd%2Fpath%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibstd%2Fpath%2Fmod.rs?ref=cd5c8235c5448a7234548c772468c8d2e8f150d9", "patch": "@@ -8,62 +8,56 @@\n // option. This file may not be copied, modified, or distributed\n // except according to those terms.\n \n-/*!\n-\n-Cross-platform path support\n-\n-This module implements support for two flavors of paths. `PosixPath` represents\n-a path on any unix-like system, whereas `WindowsPath` represents a path on\n-Windows. This module also exposes a typedef `Path` which is equal to the\n-appropriate platform-specific path variant.\n-\n-Both `PosixPath` and `WindowsPath` implement a trait `GenericPath`, which\n-contains the set of methods that behave the same for both paths. They each also\n-implement some methods that could not be expressed in `GenericPath`, yet behave\n-identically for both path flavors, such as `.components()`.\n-\n-The three main design goals of this module are 1) to avoid unnecessary\n-allocation, 2) to behave the same regardless of which flavor of path is being\n-used, and 3) to support paths that cannot be represented in UTF-8 (as Linux has\n-no restriction on paths beyond disallowing NUL).\n-\n-## Usage\n-\n-Usage of this module is fairly straightforward. Unless writing platform-specific\n-code, `Path` should be used to refer to the platform-native path.\n-\n-Creation of a path is typically done with either `Path::new(some_str)` or\n-`Path::new(some_vec)`. This path can be modified with `.push()` and\n-`.pop()` (and other setters). The resulting Path can either be passed to another\n-API that expects a path, or can be turned into a `&[u8]` with `.as_vec()` or a\n-`Option<&str>` with `.as_str()`. Similarly, attributes of the path can be queried\n-with methods such as `.filename()`. There are also methods that return a new\n-path instead of modifying the receiver, such as `.join()` or `.dir_path()`.\n-\n-Paths are always kept in normalized form. This means that creating the path\n-`Path::new(\"a/b/../c\")` will return the path `a/c`. Similarly any attempt\n-to mutate the path will always leave it in normalized form.\n-\n-When rendering a path to some form of output, there is a method `.display()`\n-which is compatible with the `format!()` parameter `{}`. This will render the\n-path as a string, replacing all non-utf8 sequences with the Replacement\n-Character (U+FFFD). As such it is not suitable for passing to any API that\n-actually operates on the path; it is only intended for display.\n-\n-## Example\n-\n-```rust\n-use std::io::fs::PathExtensions;\n-\n-let mut path = Path::new(\"/tmp/path\");\n-println!(\"path: {}\", path.display());\n-path.set_filename(\"foo\");\n-path.push(\"bar\");\n-println!(\"new path: {}\", path.display());\n-println!(\"path exists: {}\", path.exists());\n-```\n-\n-*/\n+//! Cross-platform path support\n+//!\n+//! This module implements support for two flavors of paths. `PosixPath` represents a path on any\n+//! unix-like system, whereas `WindowsPath` represents a path on Windows. This module also exposes\n+//! a typedef `Path` which is equal to the appropriate platform-specific path variant.\n+//!\n+//! Both `PosixPath` and `WindowsPath` implement a trait `GenericPath`, which contains the set of\n+//! methods that behave the same for both paths. They each also implement some methods that could\n+//! not be expressed in `GenericPath`, yet behave identically for both path flavors, such as\n+//! `.components()`.\n+//!\n+//! The three main design goals of this module are 1) to avoid unnecessary allocation, 2) to behave\n+//! the same regardless of which flavor of path is being used, and 3) to support paths that cannot\n+//! be represented in UTF-8 (as Linux has no restriction on paths beyond disallowing NUL).\n+//!\n+//! ## Usage\n+//!\n+//! Usage of this module is fairly straightforward. Unless writing platform-specific code, `Path`\n+//! should be used to refer to the platform-native path.\n+//!\n+//! Creation of a path is typically done with either `Path::new(some_str)` or\n+//! `Path::new(some_vec)`. This path can be modified with `.push()` and `.pop()` (and other\n+//! setters). The resulting Path can either be passed to another API that expects a path, or can be\n+//! turned into a `&[u8]` with `.as_vec()` or a `Option<&str>` with `.as_str()`. Similarly,\n+//! attributes of the path can be queried with methods such as `.filename()`. There are also\n+//! methods that return a new path instead of modifying the receiver, such as `.join()` or\n+//! `.dir_path()`.\n+//!\n+//! Paths are always kept in normalized form. This means that creating the path\n+//! `Path::new(\"a/b/../c\")` will return the path `a/c`. Similarly any attempt to mutate the path\n+//! will always leave it in normalized form.\n+//!\n+//! When rendering a path to some form of output, there is a method `.display()` which is\n+//! compatible with the `format!()` parameter `{}`. This will render the path as a string,\n+//! replacing all non-utf8 sequences with the Replacement Character (U+FFFD). As such it is not\n+//! suitable for passing to any API that actually operates on the path; it is only intended for\n+//! display.\n+//!\n+//! ## Example\n+//!\n+//! ```rust\n+//! use std::io::fs::PathExtensions;\n+//!\n+//! let mut path = Path::new(\"/tmp/path\");\n+//! println!(\"path: {}\", path.display());\n+//! path.set_filename(\"foo\");\n+//! path.push(\"bar\");\n+//! println!(\"new path: {}\", path.display());\n+//! println!(\"path exists: {}\", path.exists());\n+//! ```\n \n #![experimental]\n "}, {"sha": "5ecd3ff04f1cd95c00ac9939f7b9ff0eb1cae64c", "filename": "src/libstd/rt/mod.rs", "status": "modified", "additions": 32, "deletions": 40, "changes": 72, "blob_url": "https://github.com/rust-lang/rust/blob/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibstd%2Frt%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibstd%2Frt%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibstd%2Frt%2Fmod.rs?ref=cd5c8235c5448a7234548c772468c8d2e8f150d9", "patch": "@@ -8,46 +8,38 @@\n // option. This file may not be copied, modified, or distributed\n // except according to those terms.\n \n-/*! Runtime services, including the task scheduler and I/O dispatcher\n-\n-The `rt` module provides the private runtime infrastructure necessary\n-to support core language features like the exchange and local heap,\n-logging, local data and unwinding. It also implements the default task\n-scheduler and task model. Initialization routines are provided for setting\n-up runtime resources in common configurations, including that used by\n-`rustc` when generating executables.\n-\n-It is intended that the features provided by `rt` can be factored in a\n-way such that the core library can be built with different 'profiles'\n-for different use cases, e.g. excluding the task scheduler. A number\n-of runtime features though are critical to the functioning of the\n-language and an implementation must be provided regardless of the\n-execution environment.\n-\n-Of foremost importance is the global exchange heap, in the module\n-`heap`. Very little practical Rust code can be written without\n-access to the global heap. Unlike most of `rt` the global heap is\n-truly a global resource and generally operates independently of the\n-rest of the runtime.\n-\n-All other runtime features are task-local, including the local heap,\n-local storage, logging and the stack unwinder.\n-\n-The relationship between `rt` and the rest of the core library is\n-not entirely clear yet and some modules will be moving into or\n-out of `rt` as development proceeds.\n-\n-Several modules in `core` are clients of `rt`:\n-\n-* `std::task` - The user-facing interface to the Rust task model.\n-* `std::local_data` - The interface to local data.\n-* `std::unstable::lang` - Miscellaneous lang items, some of which rely on `std::rt`.\n-* `std::cleanup` - Local heap destruction.\n-* `std::io` - In the future `std::io` will use an `rt` implementation.\n-* `std::logging`\n-* `std::comm`\n-\n-*/\n+//! Runtime services, including the task scheduler and I/O dispatcher\n+//!\n+//! The `rt` module provides the private runtime infrastructure necessary to support core language\n+//! features like the exchange and local heap, logging, local data and unwinding. It also\n+//! implements the default task scheduler and task model. Initialization routines are provided for\n+//! setting up runtime resources in common configurations, including that used by `rustc` when\n+//! generating executables.\n+//!\n+//! It is intended that the features provided by `rt` can be factored in a way such that the core\n+//! library can be built with different 'profiles' for different use cases, e.g. excluding the task\n+//! scheduler. A number of runtime features though are critical to the functioning of the language\n+//! and an implementation must be provided regardless of the execution environment.\n+//!\n+//! Of foremost importance is the global exchange heap, in the module `heap`. Very little practical\n+//! Rust code can be written without access to the global heap. Unlike most of `rt` the global heap\n+//! is truly a global resource and generally operates independently of the rest of the runtime.\n+//!\n+//! All other runtime features are task-local, including the local heap, local storage, logging and\n+//! the stack unwinder.\n+//!\n+//! The relationship between `rt` and the rest of the core library is not entirely clear yet and\n+//! some modules will be moving into or out of `rt` as development proceeds.\n+//!\n+//! Several modules in `core` are clients of `rt`:\n+//!\n+//! * `std::task` - The user-facing interface to the Rust task model.\n+//! * `std::local_data` - The interface to local data.\n+//! * `std::unstable::lang` - Miscellaneous lang items, some of which rely on `std::rt`.\n+//! * `std::cleanup` - Local heap destruction.\n+//! * `std::io` - In the future `std::io` will use an `rt` implementation.\n+//! * `std::logging`\n+//! * `std::comm`\n \n #![experimental]\n "}, {"sha": "f2f9351fd0d58542f2610fb820cfe17da58e49c6", "filename": "src/libstd/sync/future.rs", "status": "modified", "additions": 13, "deletions": 15, "changes": 28, "blob_url": "https://github.com/rust-lang/rust/blob/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibstd%2Fsync%2Ffuture.rs", "raw_url": "https://github.com/rust-lang/rust/raw/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibstd%2Fsync%2Ffuture.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibstd%2Fsync%2Ffuture.rs?ref=cd5c8235c5448a7234548c772468c8d2e8f150d9", "patch": "@@ -8,21 +8,19 @@\n // option. This file may not be copied, modified, or distributed\n // except according to those terms.\n \n-/*!\n- * A type representing values that may be computed concurrently and\n- * operations for working with them.\n- *\n- * # Example\n- *\n- * ```rust\n- * use std::sync::Future;\n- * # fn fib(n: uint) -> uint {42};\n- * # fn make_a_sandwich() {};\n- * let mut delayed_fib = Future::spawn(proc() { fib(5000) });\n- * make_a_sandwich();\n- * println!(\"fib(5000) = {}\", delayed_fib.get())\n- * ```\n- */\n+//! A type representing values that may be computed concurrently and operations for working with\n+//! them.\n+//!\n+//! # Example\n+//!\n+//! ```rust\n+//! use std::sync::Future;\n+//! # fn fib(n: uint) -> uint {42};\n+//! # fn make_a_sandwich() {};\n+//! let mut delayed_fib = Future::spawn(proc() { fib(5000) });\n+//! make_a_sandwich();\n+//! println!(\"fib(5000) = {}\", delayed_fib.get())\n+//! ```\n \n #![allow(missing_docs)]\n "}, {"sha": "26c049d267dfb6f955e4577f6bb9655a4de06af4", "filename": "src/libsyntax/ast.rs", "status": "modified", "additions": 2, "deletions": 4, "changes": 6, "blob_url": "https://github.com/rust-lang/rust/blob/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibsyntax%2Fast.rs", "raw_url": "https://github.com/rust-lang/rust/raw/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibsyntax%2Fast.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fast.rs?ref=cd5c8235c5448a7234548c772468c8d2e8f150d9", "patch": "@@ -276,11 +276,9 @@ impl PathParameters {\n         }\n     }\n \n+    /// Returns the types that the user wrote. Note that these do not necessarily map to the type\n+    /// parameters in the parenthesized case.\n     pub fn types(&self) -> Vec<&P<Ty>> {\n-        /*!\n-         * Returns the types that the user wrote. Note that these do not\n-         * necessarily map to the type parameters in the parenthesized case.\n-         */\n         match *self {\n             AngleBracketedParameters(ref data) => {\n                 data.types.iter().collect()"}, {"sha": "4d35fbc143723c12d7ac19e26081c1312af6b80d", "filename": "src/libsyntax/ast_util.rs", "status": "modified", "additions": 1, "deletions": 5, "changes": 6, "blob_url": "https://github.com/rust-lang/rust/blob/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibsyntax%2Fast_util.rs", "raw_url": "https://github.com/rust-lang/rust/raw/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibsyntax%2Fast_util.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fast_util.rs?ref=cd5c8235c5448a7234548c772468c8d2e8f150d9", "patch": "@@ -569,18 +569,14 @@ pub fn compute_id_range_for_inlined_item(item: &InlinedItem) -> IdRange {\n     visitor.result\n }\n \n+/// Computes the id range for a single fn body, ignoring nested items.\n pub fn compute_id_range_for_fn_body(fk: visit::FnKind,\n                                     decl: &FnDecl,\n                                     body: &Block,\n                                     sp: Span,\n                                     id: NodeId)\n                                     -> IdRange\n {\n-    /*!\n-     * Computes the id range for a single fn body,\n-     * ignoring nested items.\n-     */\n-\n     let mut visitor = IdRangeComputingVisitor {\n         result: IdRange::max()\n     };"}, {"sha": "1c1e1acab1c57ed40970484faf08882f60d515ca", "filename": "src/libsyntax/codemap.rs", "status": "modified", "additions": 6, "deletions": 12, "changes": 18, "blob_url": "https://github.com/rust-lang/rust/blob/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibsyntax%2Fcodemap.rs", "raw_url": "https://github.com/rust-lang/rust/raw/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibsyntax%2Fcodemap.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fcodemap.rs?ref=cd5c8235c5448a7234548c772468c8d2e8f150d9", "patch": "@@ -10,18 +10,12 @@\n //\n // ignore-lexer-test FIXME #15679\n \n-/*!\n-\n-The CodeMap tracks all the source code used within a single crate, mapping\n-from integer byte positions to the original source code location. Each bit of\n-source parsed during crate parsing (typically files, in-memory strings, or\n-various bits of macro expansion) cover a continuous range of bytes in the\n-CodeMap and are represented by FileMaps. Byte positions are stored in `spans`\n-and used pervasively in the compiler. They are absolute positions within the\n-CodeMap, which upon request can be converted to line and column information,\n-source code snippets, etc.\n-\n-*/\n+//! The CodeMap tracks all the source code used within a single crate, mapping from integer byte\n+//! positions to the original source code location. Each bit of source parsed during crate parsing\n+//! (typically files, in-memory strings, or various bits of macro expansion) cover a continuous\n+//! range of bytes in the CodeMap and are represented by FileMaps. Byte positions are stored in\n+//! `spans` and used pervasively in the compiler. They are absolute positions within the CodeMap,\n+//! which upon request can be converted to line and column information, source code snippets, etc.\n \n pub use self::MacroFormat::*;\n "}, {"sha": "e3cf2b68752fd78e0f000242c7742c89324b82d3", "filename": "src/libsyntax/ext/deriving/decodable.rs", "status": "modified", "additions": 1, "deletions": 4, "changes": 5, "blob_url": "https://github.com/rust-lang/rust/blob/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibsyntax%2Fext%2Fderiving%2Fdecodable.rs", "raw_url": "https://github.com/rust-lang/rust/raw/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibsyntax%2Fext%2Fderiving%2Fdecodable.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fext%2Fderiving%2Fdecodable.rs?ref=cd5c8235c5448a7234548c772468c8d2e8f150d9", "patch": "@@ -8,10 +8,7 @@\n // option. This file may not be copied, modified, or distributed\n // except according to those terms.\n \n-/*!\n-The compiler code necessary for `#[deriving(Decodable)]`. See\n-encodable.rs for more.\n-*/\n+//! The compiler code necessary for `#[deriving(Decodable)]`. See encodable.rs for more.\n \n use ast;\n use ast::{MetaItem, Item, Expr, MutMutable};"}, {"sha": "f285d2cc2ff3ab71f21b2822d99d23cfb2988bce", "filename": "src/libsyntax/ext/deriving/generic/ty.rs", "status": "modified", "additions": 2, "deletions": 4, "changes": 6, "blob_url": "https://github.com/rust-lang/rust/blob/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibsyntax%2Fext%2Fderiving%2Fgeneric%2Fty.rs", "raw_url": "https://github.com/rust-lang/rust/raw/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibsyntax%2Fext%2Fderiving%2Fgeneric%2Fty.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fext%2Fderiving%2Fgeneric%2Fty.rs?ref=cd5c8235c5448a7234548c772468c8d2e8f150d9", "patch": "@@ -8,10 +8,8 @@\n // option. This file may not be copied, modified, or distributed\n // except according to those terms.\n \n-/*!\n-A mini version of ast::Ty, which is easier to use, and features an\n-explicit `Self` type to use when specifying impls to be derived.\n-*/\n+//! A mini version of ast::Ty, which is easier to use, and features an explicit `Self` type to use\n+//! when specifying impls to be derived.\n \n pub use self::PtrTy::*;\n pub use self::Ty::*;"}, {"sha": "fccef47d1ea2c875632f172315721fd71402c827", "filename": "src/libsyntax/ext/deriving/mod.rs", "status": "modified", "additions": 4, "deletions": 9, "changes": 13, "blob_url": "https://github.com/rust-lang/rust/blob/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibsyntax%2Fext%2Fderiving%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibsyntax%2Fext%2Fderiving%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fext%2Fderiving%2Fmod.rs?ref=cd5c8235c5448a7234548c772468c8d2e8f150d9", "patch": "@@ -8,15 +8,10 @@\n // option. This file may not be copied, modified, or distributed\n // except according to those terms.\n \n-/*!\n-The compiler code necessary to implement the `#[deriving]` extensions.\n-\n-\n-FIXME (#2810): hygiene. Search for \"__\" strings (in other files too).\n-We also assume \"extra\" is the standard library, and \"std\" is the core\n-library.\n-\n-*/\n+//! The compiler code necessary to implement the `#[deriving]` extensions.\n+//!\n+//! FIXME (#2810): hygiene. Search for \"__\" strings (in other files too). We also assume \"extra\" is\n+//! the standard library, and \"std\" is the core library.\n \n use ast::{Item, MetaItem, MetaList, MetaNameValue, MetaWord};\n use ext::base::ExtCtxt;"}, {"sha": "86a96fc521642569d92922eaeb1b22b79e453bc5", "filename": "src/libsyntax/parse/obsolete.rs", "status": "modified", "additions": 4, "deletions": 8, "changes": 12, "blob_url": "https://github.com/rust-lang/rust/blob/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibsyntax%2Fparse%2Fobsolete.rs", "raw_url": "https://github.com/rust-lang/rust/raw/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibsyntax%2Fparse%2Fobsolete.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Fobsolete.rs?ref=cd5c8235c5448a7234548c772468c8d2e8f150d9", "patch": "@@ -8,14 +8,10 @@\n // option. This file may not be copied, modified, or distributed\n // except according to those terms.\n \n-/*!\n-Support for parsing unsupported, old syntaxes, for the\n-purpose of reporting errors. Parsing of these syntaxes\n-is tested by compile-test/obsolete-syntax.rs.\n-\n-Obsolete syntax that becomes too hard to parse can be\n-removed.\n-*/\n+//! Support for parsing unsupported, old syntaxes, for the purpose of reporting errors. Parsing of\n+//! these syntaxes is tested by compile-test/obsolete-syntax.rs.\n+//!\n+//! Obsolete syntax that becomes too hard to parse can be removed.\n \n pub use self::ObsoleteSyntax::*;\n "}, {"sha": "b620799cc97e802a49f0536ba27ba41197eaff00", "filename": "src/libsyntax/parse/parser.rs", "status": "modified", "additions": 8, "deletions": 13, "changes": 21, "blob_url": "https://github.com/rust-lang/rust/blob/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibsyntax%2Fparse%2Fparser.rs", "raw_url": "https://github.com/rust-lang/rust/raw/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibsyntax%2Fparse%2Fparser.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Fparser.rs?ref=cd5c8235c5448a7234548c772468c8d2e8f150d9", "patch": "@@ -1963,11 +1963,9 @@ impl<'a> Parser<'a> {\n         }\n     }\n \n+    /// Parses `lifetime_defs = [ lifetime_defs { ',' lifetime_defs } ]` where `lifetime_def  =\n+    /// lifetime [':' lifetimes]`\n     pub fn parse_lifetime_defs(&mut self) -> Vec<ast::LifetimeDef> {\n-        /*!\n-         * Parses `lifetime_defs = [ lifetime_defs { ',' lifetime_defs } ]`\n-         * where `lifetime_def  = lifetime [':' lifetimes]`\n-         */\n \n         let mut res = Vec::new();\n         loop {\n@@ -2003,16 +2001,13 @@ impl<'a> Parser<'a> {\n         }\n     }\n \n-    // matches lifetimes = ( lifetime ) | ( lifetime , lifetimes )\n-    // actually, it matches the empty one too, but putting that in there\n-    // messes up the grammar....\n+    /// matches lifetimes = ( lifetime ) | ( lifetime , lifetimes ) actually, it matches the empty\n+    /// one too, but putting that in there messes up the grammar....\n+    ///\n+    /// Parses zero or more comma separated lifetimes. Expects each lifetime to be followed by\n+    /// either a comma or `>`.  Used when parsing type parameter lists, where we expect something\n+    /// like `<'a, 'b, T>`.\n     pub fn parse_lifetimes(&mut self, sep: token::Token) -> Vec<ast::Lifetime> {\n-        /*!\n-         * Parses zero or more comma separated lifetimes.\n-         * Expects each lifetime to be followed by either\n-         * a comma or `>`.  Used when parsing type parameter\n-         * lists, where we expect something like `<'a, 'b, T>`.\n-         */\n \n         let mut res = Vec::new();\n         loop {"}, {"sha": "84afa56b07d5eb28b18d1d9bced94cc1fddafee2", "filename": "src/libsyntax/visit.rs", "status": "modified", "additions": 3, "deletions": 5, "changes": 8, "blob_url": "https://github.com/rust-lang/rust/blob/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibsyntax%2Fvisit.rs", "raw_url": "https://github.com/rust-lang/rust/raw/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibsyntax%2Fvisit.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fvisit.rs?ref=cd5c8235c5448a7234548c772468c8d2e8f150d9", "patch": "@@ -92,14 +92,12 @@ pub trait Visitor<'v> {\n     }\n     fn visit_struct_field(&mut self, s: &'v StructField) { walk_struct_field(self, s) }\n     fn visit_variant(&mut self, v: &'v Variant, g: &'v Generics) { walk_variant(self, v, g) }\n+\n+    /// Visits an optional reference to a lifetime. The `span` is the span of some surrounding\n+    /// reference should opt_lifetime be None.\n     fn visit_opt_lifetime_ref(&mut self,\n                               _span: Span,\n                               opt_lifetime: &'v Option<Lifetime>) {\n-        /*!\n-         * Visits an optional reference to a lifetime. The `span` is\n-         * the span of some surrounding reference should opt_lifetime\n-         * be None.\n-         */\n         match *opt_lifetime {\n             Some(ref l) => self.visit_lifetime_ref(l),\n             None => ()"}, {"sha": "962be3d5acdc910b5b992b116ab59573f07bee6e", "filename": "src/libunicode/normalize.rs", "status": "modified", "additions": 1, "deletions": 4, "changes": 5, "blob_url": "https://github.com/rust-lang/rust/blob/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibunicode%2Fnormalize.rs", "raw_url": "https://github.com/rust-lang/rust/raw/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibunicode%2Fnormalize.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibunicode%2Fnormalize.rs?ref=cd5c8235c5448a7234548c772468c8d2e8f150d9", "patch": "@@ -8,10 +8,7 @@\n // option. This file may not be copied, modified, or distributed\n // except according to those terms.\n \n-/*!\n-  Functions for computing canonical and compatible decompositions\n-  for Unicode characters.\n-  */\n+//! Functions for computing canonical and compatible decompositions for Unicode characters.\n \n use core::cmp::{Equal, Less, Greater};\n use core::option::{Option, Some, None};"}, {"sha": "a73dac1a6186678cb9f5562ec6e824aa3cadd8fc", "filename": "src/libunicode/u_char.rs", "status": "modified", "additions": 4, "deletions": 6, "changes": 10, "blob_url": "https://github.com/rust-lang/rust/blob/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibunicode%2Fu_char.rs", "raw_url": "https://github.com/rust-lang/rust/raw/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibunicode%2Fu_char.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibunicode%2Fu_char.rs?ref=cd5c8235c5448a7234548c772468c8d2e8f150d9", "patch": "@@ -8,12 +8,10 @@\n // option. This file may not be copied, modified, or distributed\n // except according to those terms.\n \n-/*!\n- * Unicode-intensive `char` methods.\n- *\n- * These methods implement functionality for `char` that requires knowledge of\n- * Unicode definitions, including normalization, categorization, and display information.\n- */\n+//! Unicode-intensive `char` methods.\n+//!\n+//! These methods implement functionality for `char` that requires knowledge of\n+//! Unicode definitions, including normalization, categorization, and display information.\n \n use core::option::Option;\n use tables::{derived_property, property, general_category, conversions, charwidth};"}, {"sha": "a5f761425759510a168fcf25601433ba1dd538ed", "filename": "src/libunicode/u_str.rs", "status": "modified", "additions": 4, "deletions": 6, "changes": 10, "blob_url": "https://github.com/rust-lang/rust/blob/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibunicode%2Fu_str.rs", "raw_url": "https://github.com/rust-lang/rust/raw/cd5c8235c5448a7234548c772468c8d2e8f150d9/src%2Flibunicode%2Fu_str.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibunicode%2Fu_str.rs?ref=cd5c8235c5448a7234548c772468c8d2e8f150d9", "patch": "@@ -10,12 +10,10 @@\n //\n // ignore-lexer-test FIXME #15679\n \n-/*!\n- * Unicode-intensive string manipulations.\n- *\n- * This module provides functionality to `str` that requires the Unicode\n- * methods provided by the UnicodeChar trait.\n- */\n+//! Unicode-intensive string manipulations.\n+//!\n+//! This module provides functionality to `str` that requires the Unicode methods provided by the\n+//! UnicodeChar trait.\n \n use self::GraphemeState::*;\n use core::cmp;"}]}