{"sha": "259cc6e3dccdf7dd7f9c8e2052c6dea19fe066d9", "node_id": "MDY6Q29tbWl0NzI0NzEyOjI1OWNjNmUzZGNjZGY3ZGQ3ZjljOGUyMDUyYzZkZWExOWZlMDY2ZDk=", "commit": {"author": {"name": "Ralf Jung", "email": "post@ralfj.de", "date": "2018-08-15T19:01:40Z"}, "committer": {"name": "Ralf Jung", "email": "post@ralfj.de", "date": "2018-08-16T08:30:43Z"}, "message": "rustup for big refactor; kill most of validation", "tree": {"sha": "07e95f8a8264c5b11fc6b1ed8a59cbe6baa755f8", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/07e95f8a8264c5b11fc6b1ed8a59cbe6baa755f8"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/259cc6e3dccdf7dd7f9c8e2052c6dea19fe066d9", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/259cc6e3dccdf7dd7f9c8e2052c6dea19fe066d9", "html_url": "https://github.com/rust-lang/rust/commit/259cc6e3dccdf7dd7f9c8e2052c6dea19fe066d9", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/259cc6e3dccdf7dd7f9c8e2052c6dea19fe066d9/comments", "author": {"login": "RalfJung", "id": 330628, "node_id": "MDQ6VXNlcjMzMDYyOA==", "avatar_url": "https://avatars.githubusercontent.com/u/330628?v=4", "gravatar_id": "", "url": "https://api.github.com/users/RalfJung", "html_url": "https://github.com/RalfJung", "followers_url": "https://api.github.com/users/RalfJung/followers", "following_url": "https://api.github.com/users/RalfJung/following{/other_user}", "gists_url": "https://api.github.com/users/RalfJung/gists{/gist_id}", "starred_url": "https://api.github.com/users/RalfJung/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/RalfJung/subscriptions", "organizations_url": "https://api.github.com/users/RalfJung/orgs", "repos_url": "https://api.github.com/users/RalfJung/repos", "events_url": "https://api.github.com/users/RalfJung/events{/privacy}", "received_events_url": "https://api.github.com/users/RalfJung/received_events", "type": "User", "site_admin": false}, "committer": {"login": "RalfJung", "id": 330628, "node_id": "MDQ6VXNlcjMzMDYyOA==", "avatar_url": "https://avatars.githubusercontent.com/u/330628?v=4", "gravatar_id": "", "url": "https://api.github.com/users/RalfJung", "html_url": "https://github.com/RalfJung", "followers_url": "https://api.github.com/users/RalfJung/followers", "following_url": "https://api.github.com/users/RalfJung/following{/other_user}", "gists_url": "https://api.github.com/users/RalfJung/gists{/gist_id}", "starred_url": "https://api.github.com/users/RalfJung/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/RalfJung/subscriptions", "organizations_url": "https://api.github.com/users/RalfJung/orgs", "repos_url": "https://api.github.com/users/RalfJung/repos", "events_url": "https://api.github.com/users/RalfJung/events{/privacy}", "received_events_url": "https://api.github.com/users/RalfJung/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "65357faef8b926a7fc61ceaa2e4db79d431c80fb", "url": "https://api.github.com/repos/rust-lang/rust/commits/65357faef8b926a7fc61ceaa2e4db79d431c80fb", "html_url": "https://github.com/rust-lang/rust/commit/65357faef8b926a7fc61ceaa2e4db79d431c80fb"}], "stats": {"total": 2454, "additions": 576, "deletions": 1878}, "files": [{"sha": "559f3adb90f13c47e93466e683e711d7a09698ae", "filename": "src/fn_call.rs", "status": "modified", "additions": 115, "deletions": 180, "changes": 295, "blob_url": "https://github.com/rust-lang/rust/blob/259cc6e3dccdf7dd7f9c8e2052c6dea19fe066d9/src%2Ffn_call.rs", "raw_url": "https://github.com/rust-lang/rust/raw/259cc6e3dccdf7dd7f9c8e2052c6dea19fe066d9/src%2Ffn_call.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ffn_call.rs?ref=259cc6e3dccdf7dd7f9c8e2052c6dea19fe066d9", "patch": "@@ -1,8 +1,7 @@\n-use rustc::ty::{self, Ty};\n-use rustc::ty::layout::{self, Align, LayoutOf, Size};\n+use rustc::ty;\n+use rustc::ty::layout::{Align, LayoutOf, Size};\n use rustc::hir::def_id::{DefId, CRATE_DEF_INDEX};\n use rustc::mir;\n-use rustc_data_structures::indexed_vec::Idx;\n use syntax::attr;\n use syntax::codemap::Span;\n \n@@ -14,57 +13,12 @@ use tls::MemoryExt;\n \n use super::memory::MemoryKind;\n \n-fn write_discriminant_value<'a, 'mir, 'tcx: 'a + 'mir>(\n-        ecx: &mut EvalContext<'a, 'mir, 'tcx, super::Evaluator<'tcx>>,\n-        dest_ty: Ty<'tcx>,\n-        dest: Place,\n-        variant_index: usize,\n-    ) -> EvalResult<'tcx> {\n-        let layout = ecx.layout_of(dest_ty)?;\n-\n-        match layout.variants {\n-            layout::Variants::Single { index } => {\n-                if index != variant_index {\n-                    // If the layout of an enum is `Single`, all\n-                    // other variants are necessarily uninhabited.\n-                    assert_eq!(layout.for_variant(&ecx, variant_index).abi,\n-                               layout::Abi::Uninhabited);\n-                }\n-            }\n-            layout::Variants::Tagged { .. } => {\n-                let discr_val = dest_ty.ty_adt_def().unwrap()\n-                    .discriminant_for_variant(*ecx.tcx, variant_index)\n-                    .val;\n-\n-                let (discr_dest, discr) = ecx.place_field(dest, mir::Field::new(0), layout)?;\n-                ecx.write_scalar(discr_dest, Scalar::from_uint(discr_val, discr.size), discr.ty)?;\n-            }\n-            layout::Variants::NicheFilling {\n-                dataful_variant,\n-                ref niche_variants,\n-                niche_start,\n-                ..\n-            } => {\n-                if variant_index != dataful_variant {\n-                    let (niche_dest, niche) =\n-                        ecx.place_field(dest, mir::Field::new(0), layout)?;\n-                    let niche_value = ((variant_index - niche_variants.start()) as u128)\n-                        .wrapping_add(niche_start);\n-                    ecx.write_scalar(niche_dest, Scalar::from_uint(niche_value, niche.size), niche.ty)?;\n-                }\n-            }\n-        }\n-\n-        Ok(())\n-    }\n-\n pub trait EvalContextExt<'tcx> {\n     fn call_foreign_item(\n         &mut self,\n         def_id: DefId,\n-        args: &[ValTy<'tcx>],\n-        dest: Place,\n-        dest_ty: Ty<'tcx>,\n+        args: &[OpTy<'tcx>],\n+        dest: PlaceTy<'tcx>,\n         dest_block: mir::BasicBlock,\n     ) -> EvalResult<'tcx>;\n \n@@ -73,49 +27,46 @@ pub trait EvalContextExt<'tcx> {\n     fn call_missing_fn(\n         &mut self,\n         instance: ty::Instance<'tcx>,\n-        destination: Option<(Place, mir::BasicBlock)>,\n-        args: &[ValTy<'tcx>],\n-        sig: ty::FnSig<'tcx>,\n+        destination: Option<(PlaceTy<'tcx>, mir::BasicBlock)>,\n+        args: &[OpTy<'tcx>],\n         path: String,\n     ) -> EvalResult<'tcx>;\n \n     fn eval_fn_call(\n         &mut self,\n         instance: ty::Instance<'tcx>,\n-        destination: Option<(Place, mir::BasicBlock)>,\n-        args: &[ValTy<'tcx>],\n+        destination: Option<(PlaceTy<'tcx>, mir::BasicBlock)>,\n+        args: &[OpTy<'tcx>],\n         span: Span,\n-        sig: ty::FnSig<'tcx>,\n     ) -> EvalResult<'tcx, bool>;\n \n-    fn write_null(&mut self, dest: Place, dest_layout: TyLayout<'tcx>) -> EvalResult<'tcx>;\n+    fn write_null(&mut self, dest: PlaceTy<'tcx>) -> EvalResult<'tcx>;\n }\n \n impl<'a, 'mir, 'tcx: 'mir + 'a> EvalContextExt<'tcx> for EvalContext<'a, 'mir, 'tcx, super::Evaluator<'tcx>> {\n     fn eval_fn_call(\n         &mut self,\n         instance: ty::Instance<'tcx>,\n-        destination: Option<(Place, mir::BasicBlock)>,\n-        args: &[ValTy<'tcx>],\n+        destination: Option<(PlaceTy<'tcx>, mir::BasicBlock)>,\n+        args: &[OpTy<'tcx>],\n         span: Span,\n-        sig: ty::FnSig<'tcx>,\n     ) -> EvalResult<'tcx, bool> {\n-        trace!(\"eval_fn_call: {:#?}, {:#?}\", instance, destination);\n+        trace!(\"eval_fn_call: {:#?}, {:?}\", instance, destination.map(|(place, bb)| (*place, bb)));\n \n         let def_id = instance.def_id();\n         let item_path = self.tcx.absolute_item_path_str(def_id);\n         match &*item_path {\n             \"std::sys::unix::thread::guard::init\" | \"std::sys::unix::thread::guard::current\" => {\n                 // Return None, as it doesn't make sense to return Some, because miri detects stack overflow itself.\n-                let ret_ty = sig.output();\n-                match ret_ty.sty {\n+                let (return_place, return_to_block) = destination.unwrap();\n+                match return_place.layout.ty.sty {\n                     ty::TyAdt(ref adt_def, _) => {\n                         assert!(adt_def.is_enum(), \"Unexpected return type for {}\", item_path);\n                         let none_variant_index = adt_def.variants.iter().position(|def| {\n                             def.name.as_str() == \"None\"\n                         }).expect(\"No None variant\");\n-                        let (return_place, return_to_block) = destination.unwrap();\n-                        write_discriminant_value(self, ret_ty, return_place, none_variant_index)?;\n+\n+                        self.write_discriminant_value(none_variant_index, return_place)?;\n                         self.goto_block(return_to_block);\n                         return Ok(true);\n                     }\n@@ -135,11 +86,9 @@ impl<'a, 'mir, 'tcx: 'mir + 'a> EvalContextExt<'tcx> for EvalContext<'a, 'mir, '\n             // FIXME: return a real value in case the target allocation has an\n             // alignment bigger than the one requested\n             let n = u128::max_value();\n-            let amt = 128 - self.memory.pointer_size().bytes() * 8;\n             let (dest, return_to_block) = destination.unwrap();\n-            let ty = self.tcx.types.usize;\n-            let ptr_size = self.memory.pointer_size();\n-            self.write_scalar(dest, Scalar::from_uint((n << amt) >> amt, ptr_size), ty)?;\n+            let n = self.truncate(n, dest.layout);\n+            self.write_scalar(Scalar::from_uint(n, dest.layout.size), dest)?;\n             self.goto_block(return_to_block);\n             return Ok(true);\n         }\n@@ -151,7 +100,6 @@ impl<'a, 'mir, 'tcx: 'mir + 'a> EvalContextExt<'tcx> for EvalContext<'a, 'mir, '\n                     instance,\n                     destination,\n                     args,\n-                    sig,\n                     path,\n                 )?;\n                 return Ok(true);\n@@ -160,8 +108,8 @@ impl<'a, 'mir, 'tcx: 'mir + 'a> EvalContextExt<'tcx> for EvalContext<'a, 'mir, '\n         };\n \n         let (return_place, return_to_block) = match destination {\n-            Some((place, block)) => (place, StackPopCleanup::Goto(block)),\n-            None => (Place::undef(), StackPopCleanup::None),\n+            Some((place, block)) => (*place, StackPopCleanup::Goto(block)),\n+            None => (Place::null(&self), StackPopCleanup::None),\n         };\n \n         self.push_stack_frame(\n@@ -178,32 +126,30 @@ impl<'a, 'mir, 'tcx: 'mir + 'a> EvalContextExt<'tcx> for EvalContext<'a, 'mir, '\n     fn call_foreign_item(\n         &mut self,\n         def_id: DefId,\n-        args: &[ValTy<'tcx>],\n-        dest: Place,\n-        dest_ty: Ty<'tcx>,\n+        args: &[OpTy<'tcx>],\n+        dest: PlaceTy<'tcx>,\n         dest_block: mir::BasicBlock,\n     ) -> EvalResult<'tcx> {\n         let attrs = self.tcx.get_attrs(def_id);\n         let link_name = match attr::first_attr_value_str_by_name(&attrs, \"link_name\") {\n             Some(name) => name.as_str(),\n             None => self.tcx.item_name(def_id).as_str(),\n         };\n-        let dest_layout = self.layout_of(dest_ty)?;\n \n         match &link_name[..] {\n             \"malloc\" => {\n-                let size = self.value_to_scalar(args[0])?.to_usize(self)?;\n+                let size = self.read_scalar(args[0])?.to_usize(&self)?;\n                 if size == 0 {\n-                    self.write_null(dest, dest_layout)?;\n+                    self.write_null(dest)?;\n                 } else {\n                     let align = self.tcx.data_layout.pointer_align;\n                     let ptr = self.memory.allocate(Size::from_bytes(size), align, MemoryKind::C.into())?;\n-                    self.write_scalar(dest, Scalar::Ptr(ptr), dest_ty)?;\n+                    self.write_scalar(Scalar::Ptr(ptr), dest)?;\n                 }\n             }\n \n             \"free\" => {\n-                let ptr = self.into_ptr(args[0].value)?.unwrap_or_err()?;\n+                let ptr = self.read_scalar(args[0])?.not_undef()?;\n                 if !ptr.is_null() {\n                     self.memory.deallocate(\n                         ptr.to_ptr()?,\n@@ -214,8 +160,8 @@ impl<'a, 'mir, 'tcx: 'mir + 'a> EvalContextExt<'tcx> for EvalContext<'a, 'mir, '\n             }\n \n             \"__rust_alloc\" => {\n-                let size = self.value_to_scalar(args[0])?.to_usize(self)?;\n-                let align = self.value_to_scalar(args[1])?.to_usize(self)?;\n+                let size = self.read_scalar(args[0])?.to_usize(&self)?;\n+                let align = self.read_scalar(args[1])?.to_usize(&self)?;\n                 if size == 0 {\n                     return err!(HeapAllocZeroBytes);\n                 }\n@@ -225,11 +171,11 @@ impl<'a, 'mir, 'tcx: 'mir + 'a> EvalContextExt<'tcx> for EvalContext<'a, 'mir, '\n                 let ptr = self.memory.allocate(Size::from_bytes(size),\n                                                Align::from_bytes(align, align).unwrap(),\n                                                MemoryKind::Rust.into())?;\n-                self.write_scalar(dest, Scalar::Ptr(ptr), dest_ty)?;\n+                self.write_scalar(Scalar::Ptr(ptr), dest)?;\n             }\n             \"__rust_alloc_zeroed\" => {\n-                let size = self.value_to_scalar(args[0])?.to_usize(self)?;\n-                let align = self.value_to_scalar(args[1])?.to_usize(self)?;\n+                let size = self.read_scalar(args[0])?.to_usize(&self)?;\n+                let align = self.read_scalar(args[1])?.to_usize(&self)?;\n                 if size == 0 {\n                     return err!(HeapAllocZeroBytes);\n                 }\n@@ -240,12 +186,12 @@ impl<'a, 'mir, 'tcx: 'mir + 'a> EvalContextExt<'tcx> for EvalContext<'a, 'mir, '\n                                                Align::from_bytes(align, align).unwrap(),\n                                                MemoryKind::Rust.into())?;\n                 self.memory.write_repeat(ptr.into(), 0, Size::from_bytes(size))?;\n-                self.write_scalar(dest, Scalar::Ptr(ptr), dest_ty)?;\n+                self.write_scalar(Scalar::Ptr(ptr), dest)?;\n             }\n             \"__rust_dealloc\" => {\n-                let ptr = self.into_ptr(args[0].value)?.unwrap_or_err()?.to_ptr()?;\n-                let old_size = self.value_to_scalar(args[1])?.to_usize(self)?;\n-                let align = self.value_to_scalar(args[2])?.to_usize(self)?;\n+                let ptr = self.read_scalar(args[0])?.to_ptr()?;\n+                let old_size = self.read_scalar(args[1])?.to_usize(&self)?;\n+                let align = self.read_scalar(args[2])?.to_usize(&self)?;\n                 if old_size == 0 {\n                     return err!(HeapAllocZeroBytes);\n                 }\n@@ -259,10 +205,10 @@ impl<'a, 'mir, 'tcx: 'mir + 'a> EvalContextExt<'tcx> for EvalContext<'a, 'mir, '\n                 )?;\n             }\n             \"__rust_realloc\" => {\n-                let ptr = self.into_ptr(args[0].value)?.unwrap_or_err()?.to_ptr()?;\n-                let old_size = self.value_to_scalar(args[1])?.to_usize(self)?;\n-                let align = self.value_to_scalar(args[2])?.to_usize(self)?;\n-                let new_size = self.value_to_scalar(args[3])?.to_usize(self)?;\n+                let ptr = self.read_scalar(args[0])?.to_ptr()?;\n+                let old_size = self.read_scalar(args[1])?.to_usize(&self)?;\n+                let align = self.read_scalar(args[2])?.to_usize(&self)?;\n+                let new_size = self.read_scalar(args[3])?.to_usize(&self)?;\n                 if old_size == 0 || new_size == 0 {\n                     return err!(HeapAllocZeroBytes);\n                 }\n@@ -277,7 +223,7 @@ impl<'a, 'mir, 'tcx: 'mir + 'a> EvalContextExt<'tcx> for EvalContext<'a, 'mir, '\n                     Align::from_bytes(align, align).unwrap(),\n                     MemoryKind::Rust.into(),\n                 )?;\n-                self.write_scalar(dest, Scalar::Ptr(new_ptr), dest_ty)?;\n+                self.write_scalar(Scalar::Ptr(new_ptr), dest)?;\n             }\n \n             \"syscall\" => {\n@@ -286,7 +232,7 @@ impl<'a, 'mir, 'tcx: 'mir + 'a> EvalContextExt<'tcx> for EvalContext<'a, 'mir, '\n                 //\n                 // libc::syscall(NR_GETRANDOM, buf.as_mut_ptr(), buf.len(), GRND_NONBLOCK)\n                 // is called if a `HashMap` is created the regular way.\n-                match self.value_to_scalar(args[0])?.to_usize(self)? {\n+                match self.read_scalar(args[0])?.to_usize(&self)? {\n                     318 | 511 => {\n                         return err!(Unimplemented(\n                             \"miri does not support random number generators\".to_owned(),\n@@ -301,8 +247,8 @@ impl<'a, 'mir, 'tcx: 'mir + 'a> EvalContextExt<'tcx> for EvalContext<'a, 'mir, '\n             }\n \n             \"dlsym\" => {\n-                let _handle = self.into_ptr(args[0].value)?;\n-                let symbol = self.into_ptr(args[1].value)?.unwrap_or_err()?.to_ptr()?;\n+                let _handle = self.read_scalar(args[0])?;\n+                let symbol = self.read_scalar(args[1])?.to_ptr()?;\n                 let symbol_name = self.memory.read_c_str(symbol)?;\n                 let err = format!(\"bad c unicode symbol: {:?}\", symbol_name);\n                 let symbol_name = ::std::str::from_utf8(symbol_name).unwrap_or(&err);\n@@ -315,20 +261,20 @@ impl<'a, 'mir, 'tcx: 'mir + 'a> EvalContextExt<'tcx> for EvalContext<'a, 'mir, '\n             \"__rust_maybe_catch_panic\" => {\n                 // fn __rust_maybe_catch_panic(f: fn(*mut u8), data: *mut u8, data_ptr: *mut usize, vtable_ptr: *mut usize) -> u32\n                 // We abort on panic, so not much is going on here, but we still have to call the closure\n-                let u8_ptr_ty = self.tcx.mk_mut_ptr(self.tcx.types.u8);\n-                let f = self.into_ptr(args[0].value)?.unwrap_or_err()?.to_ptr()?;\n-                let data = self.into_ptr(args[1].value)?.unwrap_or_err()?;\n+                let f = self.read_scalar(args[0])?.to_ptr()?;\n+                let data = self.read_scalar(args[1])?.not_undef()?;\n                 let f_instance = self.memory.get_fn(f)?;\n-                self.write_null(dest, dest_layout)?;\n+                self.write_null(dest)?;\n \n                 // Now we make a function call.  TODO: Consider making this re-usable?  EvalContext::step does sth. similar for the TLS dtors,\n                 // and of course eval_main.\n                 let mir = self.load_mir(f_instance.def)?;\n+                let ret = Place::null(&self);\n                 self.push_stack_frame(\n                     f_instance,\n                     mir.span,\n                     mir,\n-                    Place::undef(),\n+                    ret,\n                     StackPopCleanup::Goto(dest_block),\n                 )?;\n                 let mut args = self.frame().mir.args_iter();\n@@ -340,12 +286,12 @@ impl<'a, 'mir, 'tcx: 'mir + 'a> EvalContextExt<'tcx> for EvalContext<'a, 'mir, '\n                     ),\n                 )?;\n                 let arg_dest = self.eval_place(&mir::Place::Local(arg_local))?;\n-                self.write_ptr(arg_dest, data, u8_ptr_ty)?;\n+                self.write_scalar(data, arg_dest)?;\n \n                 assert!(args.next().is_none(), \"__rust_maybe_catch_panic argument has more arguments than expected\");\n \n                 // We ourselves return 0\n-                self.write_null(dest, dest_layout)?;\n+                self.write_null(dest)?;\n \n                 // Don't fall through\n                 return Ok(());\n@@ -356,9 +302,9 @@ impl<'a, 'mir, 'tcx: 'mir + 'a> EvalContextExt<'tcx> for EvalContext<'a, 'mir, '\n             }\n \n             \"memcmp\" => {\n-                let left = self.into_ptr(args[0].value)?.unwrap_or_err()?;\n-                let right = self.into_ptr(args[1].value)?.unwrap_or_err()?;\n-                let n = Size::from_bytes(self.value_to_scalar(args[2])?.to_usize(self)?);\n+                let left = self.read_scalar(args[0])?.not_undef()?;\n+                let right = self.read_scalar(args[1])?.not_undef()?;\n+                let n = Size::from_bytes(self.read_scalar(args[2])?.to_usize(&self)?);\n \n                 let result = {\n                     let left_bytes = self.memory.read_bytes(left, n)?;\n@@ -373,58 +319,57 @@ impl<'a, 'mir, 'tcx: 'mir + 'a> EvalContextExt<'tcx> for EvalContext<'a, 'mir, '\n                 };\n \n                 self.write_scalar(\n-                    dest,\n                     Scalar::from_i32(result),\n-                    dest_ty,\n+                    dest,\n                 )?;\n             }\n \n             \"memrchr\" => {\n-                let ptr = self.into_ptr(args[0].value)?.unwrap_or_err()?;\n-                let val = self.value_to_scalar(args[1])?.to_bytes()? as u8;\n-                let num = self.value_to_scalar(args[2])?.to_usize(self)?;\n+                let ptr = self.read_scalar(args[0])?.not_undef()?;\n+                let val = self.read_scalar(args[1])?.to_bytes()? as u8;\n+                let num = self.read_scalar(args[2])?.to_usize(&self)?;\n                 if let Some(idx) = self.memory.read_bytes(ptr, Size::from_bytes(num))?.iter().rev().position(\n                     |&c| c == val,\n                 )\n                 {\n                     let new_ptr = ptr.ptr_offset(Size::from_bytes(num - idx as u64 - 1), &self)?;\n-                    self.write_ptr(dest, new_ptr, dest_ty)?;\n+                    self.write_scalar(new_ptr, dest)?;\n                 } else {\n-                    self.write_null(dest, dest_layout)?;\n+                    self.write_null(dest)?;\n                 }\n             }\n \n             \"memchr\" => {\n-                let ptr = self.into_ptr(args[0].value)?.unwrap_or_err()?;\n-                let val = self.value_to_scalar(args[1])?.to_bytes()? as u8;\n-                let num = self.value_to_scalar(args[2])?.to_usize(self)?;\n+                let ptr = self.read_scalar(args[0])?.not_undef()?;\n+                let val = self.read_scalar(args[1])?.to_bytes()? as u8;\n+                let num = self.read_scalar(args[2])?.to_usize(&self)?;\n                 if let Some(idx) = self.memory.read_bytes(ptr, Size::from_bytes(num))?.iter().position(\n                     |&c| c == val,\n                 )\n                 {\n                     let new_ptr = ptr.ptr_offset(Size::from_bytes(idx as u64), &self)?;\n-                    self.write_ptr(dest, new_ptr, dest_ty)?;\n+                    self.write_scalar(new_ptr, dest)?;\n                 } else {\n-                    self.write_null(dest, dest_layout)?;\n+                    self.write_null(dest)?;\n                 }\n             }\n \n             \"getenv\" => {\n                 let result = {\n-                    let name_ptr = self.into_ptr(args[0].value)?.unwrap_or_err()?.to_ptr()?;\n+                    let name_ptr = self.read_scalar(args[0])?.to_ptr()?;\n                     let name = self.memory.read_c_str(name_ptr)?;\n                     match self.machine.env_vars.get(name) {\n                         Some(&var) => Scalar::Ptr(var),\n                         None => Scalar::null(self.memory.pointer_size()),\n                     }\n                 };\n-                self.write_scalar(dest, result, dest_ty)?;\n+                self.write_scalar(result, dest)?;\n             }\n \n             \"unsetenv\" => {\n                 let mut success = None;\n                 {\n-                    let name_ptr = self.into_ptr(args[0].value)?.unwrap_or_err()?;\n+                    let name_ptr = self.read_scalar(args[0])?.not_undef()?;\n                     if !name_ptr.is_null() {\n                         let name = self.memory.read_c_str(name_ptr.to_ptr()?)?;\n                         if !name.is_empty() && !name.contains(&b'=') {\n@@ -436,17 +381,17 @@ impl<'a, 'mir, 'tcx: 'mir + 'a> EvalContextExt<'tcx> for EvalContext<'a, 'mir, '\n                     if let Some(var) = old {\n                         self.memory.deallocate(var, None, MemoryKind::Env.into())?;\n                     }\n-                    self.write_null(dest, dest_layout)?;\n+                    self.write_null(dest)?;\n                 } else {\n-                    self.write_scalar(dest, Scalar::from_int(-1, dest_layout.size), dest_ty)?;\n+                    self.write_scalar(Scalar::from_int(-1, dest.layout.size), dest)?;\n                 }\n             }\n \n             \"setenv\" => {\n                 let mut new = None;\n                 {\n-                    let name_ptr = self.into_ptr(args[0].value)?.unwrap_or_err()?;\n-                    let value_ptr = self.into_ptr(args[1].value)?.unwrap_or_err()?.to_ptr()?;\n+                    let name_ptr = self.read_scalar(args[0])?.not_undef()?;\n+                    let value_ptr = self.read_scalar(args[1])?.to_ptr()?;\n                     let value = self.memory.read_c_str(value_ptr)?;\n                     if !name_ptr.is_null() {\n                         let name = self.memory.read_c_str(name_ptr.to_ptr()?)?;\n@@ -472,16 +417,16 @@ impl<'a, 'mir, 'tcx: 'mir + 'a> EvalContextExt<'tcx> for EvalContext<'a, 'mir, '\n                     {\n                         self.memory.deallocate(var, None, MemoryKind::Env.into())?;\n                     }\n-                    self.write_null(dest, dest_layout)?;\n+                    self.write_null(dest)?;\n                 } else {\n-                    self.write_scalar(dest, Scalar::from_int(-1, dest_layout.size), dest_ty)?;\n+                    self.write_scalar(Scalar::from_int(-1, dest.layout.size), dest)?;\n                 }\n             }\n \n             \"write\" => {\n-                let fd = self.value_to_scalar(args[0])?.to_bytes()?;\n-                let buf = self.into_ptr(args[1].value)?.unwrap_or_err()?;\n-                let n = self.value_to_scalar(args[2])?.to_bytes()? as u64;\n+                let fd = self.read_scalar(args[0])?.to_bytes()?;\n+                let buf = self.read_scalar(args[1])?.not_undef()?;\n+                let n = self.read_scalar(args[2])?.to_bytes()? as u64;\n                 trace!(\"Called write({:?}, {:?}, {:?})\", fd, buf, n);\n                 let result = if fd == 1 || fd == 2 {\n                     // stdout/stderr\n@@ -501,36 +446,31 @@ impl<'a, 'mir, 'tcx: 'mir + 'a> EvalContextExt<'tcx> for EvalContext<'a, 'mir, '\n                     warn!(\"Ignored output to FD {}\", fd);\n                     n as i64 // pretend it all went well\n                 }; // now result is the value we return back to the program\n-                let ptr_size = self.memory.pointer_size();\n                 self.write_scalar(\n+                    Scalar::from_int(result, dest.layout.size),\n                     dest,\n-                    Scalar::from_int(result, ptr_size),\n-                    dest_ty,\n                 )?;\n             }\n \n             \"strlen\" => {\n-                let ptr = self.into_ptr(args[0].value)?.unwrap_or_err()?.to_ptr()?;\n+                let ptr = self.read_scalar(args[0])?.to_ptr()?;\n                 let n = self.memory.read_c_str(ptr)?.len();\n-                let ptr_size = self.memory.pointer_size();\n-                self.write_scalar(dest, Scalar::from_uint(n as u64, ptr_size), dest_ty)?;\n+                self.write_scalar(Scalar::from_uint(n as u64, dest.layout.size), dest)?;\n             }\n \n             // Some things needed for sys::thread initialization to go through\n             \"signal\" | \"sigaction\" | \"sigaltstack\" => {\n-                let ptr_size = self.memory.pointer_size();\n-                self.write_scalar(dest, Scalar::null(ptr_size), dest_ty)?;\n+                self.write_scalar(Scalar::null(dest.layout.size), dest)?;\n             }\n \n             \"sysconf\" => {\n-                let name = self.value_to_scalar(args[0])?.to_usize(self)?;\n-                let ptr_size = self.memory.pointer_size();\n+                let name = self.read_scalar(args[0])?.to_usize(&self)?;\n \n                 trace!(\"sysconf() called with name {}\", name);\n                 // cache the sysconf integers via miri's global cache\n                 let paths = &[\n-                    (&[\"libc\", \"_SC_PAGESIZE\"], Scalar::from_int(4096, ptr_size)),\n-                    (&[\"libc\", \"_SC_GETPW_R_SIZE_MAX\"], Scalar::from_int(-1, ptr_size)),\n+                    (&[\"libc\", \"_SC_PAGESIZE\"], Scalar::from_int(4096, dest.layout.size)),\n+                    (&[\"libc\", \"_SC_GETPW_R_SIZE_MAX\"], Scalar::from_int(-1, dest.layout.size)),\n                 ];\n                 let mut result = None;\n                 for &(path, path_value) in paths {\n@@ -548,7 +488,7 @@ impl<'a, 'mir, 'tcx: 'mir + 'a> EvalContextExt<'tcx> for EvalContext<'a, 'mir, '\n                     }\n                 }\n                 if let Some(result) = result {\n-                    self.write_scalar(dest, result, dest_ty)?;\n+                    self.write_scalar(result, dest)?;\n                 } else {\n                     return err!(Unimplemented(\n                         format!(\"Unimplemented sysconf name: {}\", name),\n@@ -558,10 +498,10 @@ impl<'a, 'mir, 'tcx: 'mir + 'a> EvalContextExt<'tcx> for EvalContext<'a, 'mir, '\n \n             // Hook pthread calls that go to the thread-local storage memory subsystem\n             \"pthread_key_create\" => {\n-                let key_ptr = self.into_ptr(args[0].value)?.unwrap_or_err()?;\n+                let key_ptr = self.read_scalar(args[0])?.not_undef()?;\n \n                 // Extract the function type out of the signature (that seems easier than constructing it ourselves...)\n-                let dtor = match self.into_ptr(args[1].value)?.unwrap_or_err()? {\n+                let dtor = match self.read_scalar(args[1])?.not_undef()? {\n                     Scalar::Ptr(dtor_ptr) => Some(self.memory.get_fn(dtor_ptr)?),\n                     Scalar::Bits { bits: 0, size } => {\n                         assert_eq!(size as u64, self.memory.pointer_size().bytes());\n@@ -571,7 +511,7 @@ impl<'a, 'mir, 'tcx: 'mir + 'a> EvalContextExt<'tcx> for EvalContext<'a, 'mir, '\n                 };\n \n                 // Figure out how large a pthread TLS key actually is. This is libc::pthread_key_t.\n-                let key_type = args[0].ty.builtin_deref(true)\n+                let key_type = args[0].layout.ty.builtin_deref(true)\n                                    .ok_or_else(|| EvalErrorKind::AbiViolation(\"Wrong signature used for pthread_key_create: First argument must be a raw pointer.\".to_owned()))?.ty;\n                 let key_layout = self.layout_of(key_type)?;\n \n@@ -590,26 +530,26 @@ impl<'a, 'mir, 'tcx: 'mir + 'a> EvalContextExt<'tcx> for EvalContext<'a, 'mir, '\n                 )?;\n \n                 // Return success (0)\n-                self.write_null(dest, dest_layout)?;\n+                self.write_null(dest)?;\n             }\n             \"pthread_key_delete\" => {\n-                let key = self.value_to_scalar(args[0])?.to_bytes()?;\n+                let key = self.read_scalar(args[0])?.to_bytes()?;\n                 self.memory.delete_tls_key(key)?;\n                 // Return success (0)\n-                self.write_null(dest, dest_layout)?;\n+                self.write_null(dest)?;\n             }\n             \"pthread_getspecific\" => {\n-                let key = self.value_to_scalar(args[0])?.to_bytes()?;\n+                let key = self.read_scalar(args[0])?.to_bytes()?;\n                 let ptr = self.memory.load_tls(key)?;\n-                self.write_ptr(dest, ptr, dest_ty)?;\n+                self.write_scalar(ptr, dest)?;\n             }\n             \"pthread_setspecific\" => {\n-                let key = self.value_to_scalar(args[0])?.to_bytes()?;\n-                let new_ptr = self.into_ptr(args[1].value)?.unwrap_or_err()?;\n+                let key = self.read_scalar(args[0])?.to_bytes()?;\n+                let new_ptr = self.read_scalar(args[1])?.not_undef()?;\n                 self.memory.store_tls(key, new_ptr)?;\n \n                 // Return success (0)\n-                self.write_null(dest, dest_layout)?;\n+                self.write_null(dest)?;\n             }\n \n             \"_tlv_atexit\" => {\n@@ -619,20 +559,19 @@ impl<'a, 'mir, 'tcx: 'mir + 'a> EvalContextExt<'tcx> for EvalContext<'a, 'mir, '\n             // Stub out all the other pthread calls to just return 0\n             link_name if link_name.starts_with(\"pthread_\") => {\n                 debug!(\"ignoring C ABI call: {}\", link_name);\n-                self.write_null(dest, dest_layout)?;\n+                self.write_null(dest)?;\n             }\n \n             \"mmap\" => {\n                 // This is a horrible hack, but well... the guard page mechanism calls mmap and expects a particular return value, so we give it that value\n-                let addr = self.into_ptr(args[0].value)?.unwrap_or_err()?;\n-                self.write_ptr(dest, addr, dest_ty)?;\n+                let addr = self.read_scalar(args[0])?.not_undef()?;\n+                self.write_scalar(addr, dest)?;\n             }\n \n             // Windows API subs\n             \"AddVectoredExceptionHandler\" => {\n                 // any non zero value works for the stdlib. This is just used for stackoverflows anyway\n-                let ptr_size = self.memory.pointer_size();\n-                self.write_scalar(dest, Scalar::from_int(1, ptr_size), dest_ty)?;\n+                self.write_scalar(Scalar::from_int(1, dest.layout.size), dest)?;\n             },\n             \"InitializeCriticalSection\" |\n             \"EnterCriticalSection\" |\n@@ -645,11 +584,11 @@ impl<'a, 'mir, 'tcx: 'mir + 'a> EvalContextExt<'tcx> for EvalContext<'a, 'mir, '\n             \"GetProcAddress\" |\n             \"TryEnterCriticalSection\" => {\n                 // pretend these do not exist/nothing happened, by returning zero\n-                self.write_scalar(dest, Scalar::from_int(0, dest_layout.size), dest_ty)?;\n+                self.write_null(dest)?;\n             },\n             \"GetLastError\" => {\n                 // this is c::ERROR_CALL_NOT_IMPLEMENTED\n-                self.write_scalar(dest, Scalar::from_int(120, dest_layout.size), dest_ty)?;\n+                self.write_scalar(Scalar::from_int(120, dest.layout.size), dest)?;\n             },\n \n             // Windows TLS\n@@ -660,23 +599,23 @@ impl<'a, 'mir, 'tcx: 'mir + 'a> EvalContextExt<'tcx> for EvalContext<'a, 'mir, '\n                 let key = self.memory.create_tls_key(None) as u128;\n \n                 // Figure out how large a TLS key actually is. This is c::DWORD.\n-                if dest_layout.size.bits() < 128 && key >= (1u128 << dest_layout.size.bits() as u128) {\n+                if dest.layout.size.bits() < 128 && key >= (1u128 << dest.layout.size.bits() as u128) {\n                     return err!(OutOfTls);\n                 }\n-                self.write_scalar(dest, Scalar::from_uint(key, dest_layout.size), dest_layout.ty)?;\n+                self.write_scalar(Scalar::from_uint(key, dest.layout.size), dest)?;\n             }\n             \"TlsGetValue\" => {\n-                let key = self.value_to_scalar(args[0])?.to_bytes()?;\n+                let key = self.read_scalar(args[0])?.to_bytes()?;\n                 let ptr = self.memory.load_tls(key)?;\n-                self.write_ptr(dest, ptr, dest_ty)?;\n+                self.write_scalar(ptr, dest)?;\n             }\n             \"TlsSetValue\" => {\n-                let key = self.value_to_scalar(args[0])?.to_bytes()?;\n-                let new_ptr = self.into_ptr(args[1].value)?.unwrap_or_err()?;\n+                let key = self.read_scalar(args[0])?.to_bytes()?;\n+                let new_ptr = self.read_scalar(args[1])?.not_undef()?;\n                 self.memory.store_tls(key, new_ptr)?;\n \n                 // Return success (1)\n-                self.write_scalar(dest, Scalar::from_int(1, dest_layout.size), dest_ty)?;\n+                self.write_scalar(Scalar::from_int(1, dest.layout.size), dest)?;\n             }\n \n             // We can't execute anything else\n@@ -690,7 +629,7 @@ impl<'a, 'mir, 'tcx: 'mir + 'a> EvalContextExt<'tcx> for EvalContext<'a, 'mir, '\n         // Since we pushed no stack frame, the main loop will act\n         // as if the call just completed and it's returning to the\n         // current frame.\n-        self.dump_local(dest);\n+        self.dump_place(*dest);\n         self.goto_block(dest_block);\n         Ok(())\n     }\n@@ -732,9 +671,8 @@ impl<'a, 'mir, 'tcx: 'mir + 'a> EvalContextExt<'tcx> for EvalContext<'a, 'mir, '\n     fn call_missing_fn(\n         &mut self,\n         instance: ty::Instance<'tcx>,\n-        destination: Option<(Place, mir::BasicBlock)>,\n-        args: &[ValTy<'tcx>],\n-        sig: ty::FnSig<'tcx>,\n+        destination: Option<(PlaceTy<'tcx>, mir::BasicBlock)>,\n+        args: &[OpTy<'tcx>],\n         path: String,\n     ) -> EvalResult<'tcx> {\n         // In some cases in non-MIR libstd-mode, not having a destination is legit.  Handle these early.\n@@ -745,7 +683,6 @@ impl<'a, 'mir, 'tcx: 'mir + 'a> EvalContextExt<'tcx> for EvalContext<'a, 'mir, '\n             _ => {}\n         }\n \n-        let dest_ty = sig.output();\n         let (dest, dest_block) = destination.ok_or_else(\n             || EvalErrorKind::NoMirFor(path.clone()),\n         )?;\n@@ -758,7 +695,6 @@ impl<'a, 'mir, 'tcx: 'mir + 'a> EvalContextExt<'tcx> for EvalContext<'a, 'mir, '\n                 instance.def_id(),\n                 args,\n                 dest,\n-                dest_ty,\n                 dest_block,\n             )?;\n             return Ok(());\n@@ -784,8 +720,7 @@ impl<'a, 'mir, 'tcx: 'mir + 'a> EvalContextExt<'tcx> for EvalContext<'a, 'mir, '\n             \"std::panicking::panicking\" |\n             \"std::rt::panicking\" => {\n                 // we abort on panic -> `std::rt::panicking` always returns false\n-                let bool = self.tcx.types.bool;\n-                self.write_scalar(dest, Scalar::from_bool(false), bool)?;\n+                self.write_scalar(Scalar::from_bool(false), dest)?;\n             }\n \n             _ => return err!(NoMirFor(path)),\n@@ -794,12 +729,12 @@ impl<'a, 'mir, 'tcx: 'mir + 'a> EvalContextExt<'tcx> for EvalContext<'a, 'mir, '\n         // Since we pushed no stack frame, the main loop will act\n         // as if the call just completed and it's returning to the\n         // current frame.\n-        self.dump_local(dest);\n+        self.dump_place(*dest);\n         self.goto_block(dest_block);\n         Ok(())\n     }\n \n-    fn write_null(&mut self, dest: Place, dest_layout: TyLayout<'tcx>) -> EvalResult<'tcx> {\n-        self.write_scalar(dest, Scalar::null(dest_layout.size), dest_layout.ty)\n+    fn write_null(&mut self, dest: PlaceTy<'tcx>) -> EvalResult<'tcx> {\n+        self.write_scalar(Scalar::null(dest.layout.size), dest)\n     }\n }"}, {"sha": "606f1bb4ecb447e9cbcf0332b46bcfe30b1cbd23", "filename": "src/helpers.rs", "status": "modified", "additions": 110, "deletions": 117, "changes": 227, "blob_url": "https://github.com/rust-lang/rust/blob/259cc6e3dccdf7dd7f9c8e2052c6dea19fe066d9/src%2Fhelpers.rs", "raw_url": "https://github.com/rust-lang/rust/raw/259cc6e3dccdf7dd7f9c8e2052c6dea19fe066d9/src%2Fhelpers.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Fhelpers.rs?ref=259cc6e3dccdf7dd7f9c8e2052c6dea19fe066d9", "patch": "@@ -1,128 +1,121 @@\n-use mir;\n-use rustc::ty::Ty;\n-use rustc::ty::layout::{LayoutOf, Size};\n+use rustc::ty::layout::{Size, HasDataLayout};\n \n-use super::{Scalar, ScalarExt, EvalResult, EvalContext, ValTy};\n+use super::{Scalar, ScalarMaybeUndef, EvalResult};\n use rustc_mir::interpret::sign_extend;\n \n-pub trait EvalContextExt<'tcx> {\n-    fn wrapping_pointer_offset(\n-        &self,\n-        ptr: Scalar,\n-        pointee_ty: Ty<'tcx>,\n-        offset: i64,\n-    ) -> EvalResult<'tcx, Scalar>;\n-\n-    fn pointer_offset(\n-        &self,\n-        ptr: Scalar,\n-        pointee_ty: Ty<'tcx>,\n-        offset: i64,\n-    ) -> EvalResult<'tcx, Scalar>;\n-\n-    fn value_to_isize(\n-        &self,\n-        value: ValTy<'tcx>,\n-    ) -> EvalResult<'tcx, i64>;\n-\n-    fn value_to_usize(\n-        &self,\n-        value: ValTy<'tcx>,\n-    ) -> EvalResult<'tcx, u64>;\n-\n-    fn value_to_i32(\n-        &self,\n-        value: ValTy<'tcx>,\n-    ) -> EvalResult<'tcx, i32>;\n-\n-    fn value_to_u8(\n-        &self,\n-        value: ValTy<'tcx>,\n-    ) -> EvalResult<'tcx, u8>;\n+pub trait ScalarExt {\n+    fn null(size: Size) -> Self;\n+    fn from_i32(i: i32) -> Self;\n+    fn from_uint(i: impl Into<u128>, ptr_size: Size) -> Self;\n+    fn from_int(i: impl Into<i128>, ptr_size: Size) -> Self;\n+    fn from_f32(f: f32) -> Self;\n+    fn from_f64(f: f64) -> Self;\n+    fn is_null(self) -> bool;\n }\n \n-impl<'a, 'mir, 'tcx> EvalContextExt<'tcx> for EvalContext<'a, 'mir, 'tcx, super::Evaluator<'tcx>> {\n-    fn wrapping_pointer_offset(\n-        &self,\n-        ptr: Scalar,\n-        pointee_ty: Ty<'tcx>,\n-        offset: i64,\n-    ) -> EvalResult<'tcx, Scalar> {\n-        // FIXME: assuming here that type size is < i64::max_value()\n-        let pointee_size = self.layout_of(pointee_ty)?.size.bytes() as i64;\n-        let offset = offset.overflowing_mul(pointee_size).0;\n-        Ok(ptr.ptr_wrapping_signed_offset(offset, self))\n-    }\n-\n-    fn pointer_offset(\n-        &self,\n-        ptr: Scalar,\n-        pointee_ty: Ty<'tcx>,\n-        offset: i64,\n-    ) -> EvalResult<'tcx, Scalar> {\n-        // This function raises an error if the offset moves the pointer outside of its allocation.  We consider\n-        // ZSTs their own huge allocation that doesn't overlap with anything (and nothing moves in there because the size is 0).\n-        // We also consider the NULL pointer its own separate allocation, and all the remaining integers pointers their own\n-        // allocation.\n-\n-        if ptr.is_null() {\n-            // NULL pointers must only be offset by 0\n-            return if offset == 0 {\n-                Ok(ptr)\n-            } else {\n-                err!(InvalidNullPointerUsage)\n-            };\n+pub trait FalibleScalarExt {\n+    fn to_usize(self, cx: impl HasDataLayout) -> EvalResult<'static, u64>;\n+    fn to_isize(self, cx: impl HasDataLayout) -> EvalResult<'static, i64>;\n+    fn to_i32(self) -> EvalResult<'static, i32>;\n+    fn to_u8(self) -> EvalResult<'static, u8>;\n+\n+    /// HACK: this function just extracts all bits if `defined != 0`\n+    /// Mainly used for args of C-functions and we should totally correctly fetch the size\n+    /// of their arguments\n+    fn to_bytes(self) -> EvalResult<'static, u128>;\n+}\n+\n+impl ScalarExt for Scalar {\n+    fn null(size: Size) -> Self {\n+        Scalar::Bits { bits: 0, size: size.bytes() as u8 }\n+    }\n+\n+    fn from_i32(i: i32) -> Self {\n+        Scalar::Bits { bits: i as u32 as u128, size: 4 }\n+    }\n+\n+    fn from_uint(i: impl Into<u128>, size: Size) -> Self {\n+        Scalar::Bits { bits: i.into(), size: size.bytes() as u8 }\n+    }\n+\n+    fn from_int(i: impl Into<i128>, size: Size) -> Self {\n+        Scalar::Bits { bits: i.into() as u128, size: size.bytes() as u8 }\n+    }\n+\n+    fn from_f32(f: f32) -> Self {\n+        Scalar::Bits { bits: f.to_bits() as u128, size: 4 }\n+    }\n+\n+    fn from_f64(f: f64) -> Self {\n+        Scalar::Bits { bits: f.to_bits() as u128, size: 8 }\n+    }\n+\n+    fn is_null(self) -> bool {\n+        match self {\n+            Scalar::Bits { bits, .. } => bits == 0,\n+            Scalar::Ptr(_) => false\n         }\n-        // FIXME: assuming here that type size is < i64::max_value()\n-        let pointee_size = self.layout_of(pointee_ty)?.size.bytes() as i64;\n-         if let Some(offset) = offset.checked_mul(pointee_size) {\n-            let ptr = ptr.ptr_signed_offset(offset, self)?;\n-            // Do not do bounds-checking for integers; they can never alias a normal pointer anyway.\n-            if let Scalar::Ptr(ptr) = ptr {\n-                self.memory.check_bounds(ptr, false)?;\n-            } else if ptr.is_null() {\n-                // We moved *to* a NULL pointer.  That seems wrong, LLVM considers the NULL pointer its own small allocation.  Reject this, for now.\n-                return err!(InvalidNullPointerUsage);\n-            }\n-            Ok(ptr)\n-        } else {\n-            err!(Overflow(mir::BinOp::Mul))\n+    }\n+}\n+\n+impl FalibleScalarExt for Scalar {\n+    fn to_usize(self, cx: impl HasDataLayout) -> EvalResult<'static, u64> {\n+        let b = self.to_bits(cx.data_layout().pointer_size)?;\n+        assert_eq!(b as u64 as u128, b);\n+        Ok(b as u64)\n+    }\n+\n+    fn to_u8(self) -> EvalResult<'static, u8> {\n+        let sz = Size::from_bits(8);\n+        let b = self.to_bits(sz)?;\n+        assert_eq!(b as u8 as u128, b);\n+        Ok(b as u8)\n+    }\n+\n+    fn to_isize(self, cx: impl HasDataLayout) -> EvalResult<'static, i64> {\n+        let b = self.to_bits(cx.data_layout().pointer_size)?;\n+        let b = sign_extend(b, cx.data_layout().pointer_size) as i128;\n+        assert_eq!(b as i64 as i128, b);\n+        Ok(b as i64)\n+    }\n+\n+    fn to_i32(self) -> EvalResult<'static, i32> {\n+        let sz = Size::from_bits(32);\n+        let b = self.to_bits(sz)?;\n+        let b = sign_extend(b, sz) as i128;\n+        assert_eq!(b as i32 as i128, b);\n+        Ok(b as i32)\n+    }\n+\n+    fn to_bytes(self) -> EvalResult<'static, u128> {\n+        match self {\n+            Scalar::Bits { bits, size } => {\n+                assert_ne!(size, 0);\n+                Ok(bits)\n+            },\n+            Scalar::Ptr(_) => err!(ReadPointerAsBytes),\n         }\n     }\n+}\n+\n+impl FalibleScalarExt for ScalarMaybeUndef {\n+    fn to_usize(self, cx: impl HasDataLayout) -> EvalResult<'static, u64> {\n+        self.not_undef()?.to_usize(cx)\n+    }\n+\n+    fn to_u8(self) -> EvalResult<'static, u8> {\n+        self.not_undef()?.to_u8()\n+    }\n+\n+    fn to_isize(self, cx: impl HasDataLayout) -> EvalResult<'static, i64> {\n+        self.not_undef()?.to_isize(cx)\n+    }\n+\n+    fn to_i32(self) -> EvalResult<'static, i32> {\n+        self.not_undef()?.to_i32()\n+    }\n \n-    fn value_to_isize(\n-        &self,\n-        value: ValTy<'tcx>,\n-    ) -> EvalResult<'tcx, i64> {\n-        assert_eq!(value.ty, self.tcx.types.isize);\n-        let raw = self.value_to_scalar(value)?.to_bits(self.memory.pointer_size())?;\n-        let raw = sign_extend(raw, self.layout_of(self.tcx.types.isize).unwrap());\n-        Ok(raw as i64)\n-    }\n-\n-    fn value_to_usize(\n-        &self,\n-        value: ValTy<'tcx>,\n-    ) -> EvalResult<'tcx, u64> {\n-        assert_eq!(value.ty, self.tcx.types.usize);\n-        self.value_to_scalar(value)?.to_bits(self.memory.pointer_size()).map(|v| v as u64)\n-    }\n-\n-    fn value_to_i32(\n-        &self,\n-        value: ValTy<'tcx>,\n-    ) -> EvalResult<'tcx, i32> {\n-        assert_eq!(value.ty, self.tcx.types.i32);\n-        let raw = self.value_to_scalar(value)?.to_bits(Size::from_bits(32))?;\n-        let raw = sign_extend(raw, self.layout_of(self.tcx.types.i32).unwrap());\n-        Ok(raw as i32)\n-    }\n-\n-    fn value_to_u8(\n-        &self,\n-        value: ValTy<'tcx>,\n-    ) -> EvalResult<'tcx, u8> {\n-        assert_eq!(value.ty, self.tcx.types.u8);\n-        self.value_to_scalar(value)?.to_bits(Size::from_bits(8)).map(|v| v as u8)\n+    fn to_bytes(self) -> EvalResult<'static, u128> {\n+        self.not_undef()?.to_bytes()\n     }\n }"}, {"sha": "631653b97b6b601c8263598a95342ffbbd7f01f2", "filename": "src/intrinsic.rs", "status": "modified", "additions": 210, "deletions": 264, "changes": 474, "blob_url": "https://github.com/rust-lang/rust/blob/259cc6e3dccdf7dd7f9c8e2052c6dea19fe066d9/src%2Fintrinsic.rs", "raw_url": "https://github.com/rust-lang/rust/raw/259cc6e3dccdf7dd7f9c8e2052c6dea19fe066d9/src%2Fintrinsic.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Fintrinsic.rs?ref=259cc6e3dccdf7dd7f9c8e2052c6dea19fe066d9", "patch": "@@ -1,21 +1,20 @@\n use rustc::mir;\n-use rustc::ty::layout::{TyLayout, LayoutOf, Size, Primitive, Integer::*};\n+use rustc::ty::layout::{self, LayoutOf, Size, Primitive, Integer::*};\n use rustc::ty;\n \n-use rustc::mir::interpret::{EvalResult, Scalar, Value, ScalarMaybeUndef};\n-use rustc_mir::interpret::{Place, PlaceExtra, HasMemory, EvalContext, ValTy};\n+use rustc::mir::interpret::{EvalResult, Scalar, ScalarMaybeUndef};\n+use rustc_mir::interpret::{\n+    PlaceExtra, PlaceTy, EvalContext, OpTy, Value\n+};\n \n-use helpers::EvalContextExt as HelperEvalContextExt;\n-\n-use super::ScalarExt;\n+use super::{ScalarExt, FalibleScalarExt, OperatorEvalContextExt};\n \n pub trait EvalContextExt<'tcx> {\n     fn call_intrinsic(\n         &mut self,\n         instance: ty::Instance<'tcx>,\n-        args: &[ValTy<'tcx>],\n-        dest: Place,\n-        dest_layout: TyLayout<'tcx>,\n+        args: &[OpTy<'tcx>],\n+        dest: PlaceTy<'tcx>,\n         target: mir::BasicBlock,\n     ) -> EvalResult<'tcx>;\n }\n@@ -24,54 +23,60 @@ impl<'a, 'mir, 'tcx> EvalContextExt<'tcx> for EvalContext<'a, 'mir, 'tcx, super:\n     fn call_intrinsic(\n         &mut self,\n         instance: ty::Instance<'tcx>,\n-        args: &[ValTy<'tcx>],\n-        dest: Place,\n-        dest_layout: TyLayout<'tcx>,\n+        args: &[OpTy<'tcx>],\n+        dest: PlaceTy<'tcx>,\n         target: mir::BasicBlock,\n     ) -> EvalResult<'tcx> {\n         let substs = instance.substs;\n \n         let intrinsic_name = &self.tcx.item_name(instance.def_id()).as_str()[..];\n         match intrinsic_name {\n             \"add_with_overflow\" => {\n-                self.intrinsic_with_overflow(\n+                let l = self.read_value(args[0])?;\n+                let r = self.read_value(args[1])?;\n+                self.binop_with_overflow(\n                     mir::BinOp::Add,\n-                    args[0],\n-                    args[1],\n+                    l,\n+                    r,\n                     dest,\n-                    dest_layout.ty,\n                 )?\n             }\n \n             \"sub_with_overflow\" => {\n-                self.intrinsic_with_overflow(\n+                let l = self.read_value(args[0])?;\n+                let r = self.read_value(args[1])?;\n+                self.binop_with_overflow(\n                     mir::BinOp::Sub,\n-                    args[0],\n-                    args[1],\n+                    l,\n+                    r,\n                     dest,\n-                    dest_layout.ty,\n                 )?\n             }\n \n             \"mul_with_overflow\" => {\n-                self.intrinsic_with_overflow(\n+                let l = self.read_value(args[0])?;\n+                let r = self.read_value(args[1])?;\n+                self.binop_with_overflow(\n                     mir::BinOp::Mul,\n-                    args[0],\n-                    args[1],\n+                    l,\n+                    r,\n                     dest,\n-                    dest_layout.ty,\n                 )?\n             }\n \n             \"arith_offset\" => {\n-                let offset = self.value_to_isize(args[1])?;\n-                let ptr = self.into_ptr(args[0].value)?.unwrap_or_err()?;\n-                let result_ptr = self.wrapping_pointer_offset(ptr, substs.type_at(0), offset)?;\n-                self.write_ptr(dest, result_ptr, dest_layout.ty)?;\n+                let offset = self.read_scalar(args[1])?.to_isize(&self)?;\n+                let ptr = self.read_scalar(args[0])?.not_undef()?;\n+\n+                let pointee_ty = substs.type_at(0);\n+                let pointee_size = self.layout_of(pointee_ty)?.size.bytes() as i64;\n+                let offset = offset.overflowing_mul(pointee_size).0;\n+                let result_ptr = ptr.ptr_wrapping_signed_offset(offset, &self);\n+                self.write_scalar(result_ptr, dest)?;\n             }\n \n             \"assume\" => {\n-                let cond = self.value_to_scalar(args[0])?.to_bool()?;\n+                let cond = self.read_scalar(args[0])?.to_bool()?;\n                 if !cond {\n                     return err!(AssumptionNotHeld);\n                 }\n@@ -81,72 +86,45 @@ impl<'a, 'mir, 'tcx> EvalContextExt<'tcx> for EvalContext<'a, 'mir, 'tcx, super:\n             \"atomic_load_relaxed\" |\n             \"atomic_load_acq\" |\n             \"volatile_load\" => {\n-                let ptr = self.into_ptr(args[0].value)?.unwrap_or_err()?;\n-                let align = self.layout_of(args[0].ty)?.align;\n-\n-                let valty = ValTy {\n-                    value: Value::ByRef(ptr, align),\n-                    ty: substs.type_at(0),\n-                };\n-                self.write_value(valty, dest)?;\n+                let ptr = self.ref_to_mplace(self.read_value(args[0])?)?;\n+                let val = self.read_scalar(ptr.into())?; // make sure it fits into a scalar; otherwise it cannot be atomic\n+                self.write_scalar(val, dest)?;\n             }\n \n             \"atomic_store\" |\n             \"atomic_store_relaxed\" |\n             \"atomic_store_rel\" |\n             \"volatile_store\" => {\n-                let ty = substs.type_at(0);\n-                let align = self.layout_of(ty)?.align;\n-                let dest = self.into_ptr(args[0].value)?.unwrap_or_err()?;\n-                self.write_value_to_ptr(args[1].value, dest, align, ty)?;\n+                let ptr = self.ref_to_mplace(self.read_value(args[0])?)?;\n+                let val = self.read_scalar(args[1])?; // make sure it fits into a scalar; otherwise it cannot be atomic\n+                self.write_scalar(val, ptr.into())?;\n             }\n \n             \"atomic_fence_acq\" => {\n                 // we are inherently singlethreaded and singlecored, this is a nop\n             }\n \n             _ if intrinsic_name.starts_with(\"atomic_xchg\") => {\n-                let ty = substs.type_at(0);\n-                let align = self.layout_of(ty)?.align;\n-                let ptr = self.into_ptr(args[0].value)?.unwrap_or_err()?;\n-                let change = self.value_to_scalar(args[1])?;\n-                let old = self.read_value(ptr, align, ty)?;\n-                let old = match old {\n-                    Value::Scalar(val) => val,\n-                    Value::ByRef { .. } => bug!(\"just read the value, can't be byref\"),\n-                    Value::ScalarPair(..) => bug!(\"atomic_xchg doesn't work with nonprimitives\"),\n-                };\n-                self.write_scalar(dest, old, ty)?;\n-                self.write_scalar(\n-                    Place::from_scalar_ptr(ptr.into(), align),\n-                    change,\n-                    ty,\n-                )?;\n+                let ptr = self.ref_to_mplace(self.read_value(args[0])?)?;\n+                let new = self.read_scalar(args[1])?;\n+                let old = self.read_scalar(ptr.into())?;\n+                self.write_scalar(old, dest)?; // old value is returned\n+                self.write_scalar(new, ptr.into())?;\n             }\n \n             _ if intrinsic_name.starts_with(\"atomic_cxchg\") => {\n-                let ty = substs.type_at(0);\n-                let align = self.layout_of(ty)?.align;\n-                let ptr = self.into_ptr(args[0].value)?.unwrap_or_err()?;\n-                let expect_old = self.value_to_scalar(args[1])?;\n-                let change = self.value_to_scalar(args[2])?;\n-                let old = self.read_value(ptr, align, ty)?;\n-                let old = match old {\n-                    Value::Scalar(val) => val.unwrap_or_err()?,\n-                    Value::ByRef { .. } => bug!(\"just read the value, can't be byref\"),\n-                    Value::ScalarPair(..) => bug!(\"atomic_cxchg doesn't work with nonprimitives\"),\n-                };\n-                let (val, _) = self.binary_op(mir::BinOp::Eq, old, ty, expect_old, ty)?;\n-                let valty = ValTy {\n-                    value: Value::ScalarPair(old.into(), val.into()),\n-                    ty: dest_layout.ty,\n-                };\n-                self.write_value(valty, dest)?;\n-                self.write_scalar(\n-                    Place::from_scalar_ptr(ptr.into(), dest_layout.align),\n-                    change,\n-                    ty,\n-                )?;\n+                let ptr = self.ref_to_mplace(self.read_value(args[0])?)?;\n+                let expect_old = self.read_value(args[1])?; // read as value for the sake of `binary_op()`\n+                let new = self.read_scalar(args[2])?;\n+                let old = self.read_value(ptr.into())?; // read as value for the sake of `binary_op()`\n+                // binary_op will bail if either of them is not a scalar\n+                let (eq, _) = self.binary_op(mir::BinOp::Eq, old, expect_old)?;\n+                let res = Value::ScalarPair(old.to_scalar_or_undef(), eq.into());\n+                self.write_value(res, dest)?; // old value is returned\n+                // update ptr depending on comparison\n+                if eq.to_bool()? {\n+                    self.write_scalar(new, ptr.into())?;\n+                }\n             }\n \n             \"atomic_or\" |\n@@ -174,19 +152,10 @@ impl<'a, 'mir, 'tcx> EvalContextExt<'tcx> for EvalContext<'a, 'mir, 'tcx, super:\n             \"atomic_xsub_rel\" |\n             \"atomic_xsub_acqrel\" |\n             \"atomic_xsub_relaxed\" => {\n-                let ty = substs.type_at(0);\n-                let align = self.layout_of(ty)?.align;\n-                let ptr = self.into_ptr(args[0].value)?.unwrap_or_err()?;\n-                let change = self.value_to_scalar(args[1])?;\n-                let old = self.read_value(ptr, align, ty)?;\n-                let old = match old {\n-                    Value::Scalar(val) => val,\n-                    Value::ByRef { .. } => bug!(\"just read the value, can't be byref\"),\n-                    Value::ScalarPair(..) => {\n-                        bug!(\"atomic_xadd_relaxed doesn't work with nonprimitives\")\n-                    }\n-                };\n-                self.write_scalar(dest, old, ty)?;\n+                let ptr = self.ref_to_mplace(self.read_value(args[0])?)?;\n+                let rhs = self.read_value(args[1])?;\n+                let old = self.read_value(ptr.into())?;\n+                self.write_value(*old, dest)?; // old value is returned\n                 let op = match intrinsic_name.split('_').nth(1).unwrap() {\n                     \"or\" => mir::BinOp::BitOr,\n                     \"xor\" => mir::BinOp::BitXor,\n@@ -196,8 +165,8 @@ impl<'a, 'mir, 'tcx> EvalContextExt<'tcx> for EvalContext<'a, 'mir, 'tcx, super:\n                     _ => bug!(),\n                 };\n                 // FIXME: what do atomics do on overflow?\n-                let (val, _) = self.binary_op(op, old.unwrap_or_err()?, ty, change, ty)?;\n-                self.write_scalar(Place::from_scalar_ptr(ptr.into(), dest_layout.align), val, ty)?;\n+                let (val, _) = self.binary_op(op, old, rhs)?;\n+                self.write_scalar(val, ptr.into())?;\n             }\n \n             \"breakpoint\" => unimplemented!(), // halt miri\n@@ -207,13 +176,13 @@ impl<'a, 'mir, 'tcx> EvalContextExt<'tcx> for EvalContext<'a, 'mir, 'tcx, super:\n                 let elem_ty = substs.type_at(0);\n                 let elem_layout = self.layout_of(elem_ty)?;\n                 let elem_size = elem_layout.size.bytes();\n-                let count = self.value_to_usize(args[2])?;\n+                let count = self.read_scalar(args[2])?.to_usize(&self)?;\n                 if count * elem_size != 0 {\n                     // TODO: We do not even validate alignment for the 0-bytes case.  libstd relies on this in vec::IntoIter::next.\n                     // Also see the write_bytes intrinsic.\n                     let elem_align = elem_layout.align;\n-                    let src = self.into_ptr(args[0].value)?.unwrap_or_err()?;\n-                    let dest = self.into_ptr(args[1].value)?.unwrap_or_err()?;\n+                    let src = self.read_scalar(args[0])?.not_undef()?;\n+                    let dest = self.read_scalar(args[1])?.not_undef()?;\n                     self.memory.copy(\n                         src,\n                         elem_align,\n@@ -227,7 +196,7 @@ impl<'a, 'mir, 'tcx> EvalContextExt<'tcx> for EvalContext<'a, 'mir, 'tcx, super:\n \n             \"ctpop\" | \"cttz\" | \"cttz_nonzero\" | \"ctlz\" | \"ctlz_nonzero\" | \"bswap\" => {\n                 let ty = substs.type_at(0);\n-                let num = self.value_to_scalar(args[0])?.to_bytes()?;\n+                let num = self.read_scalar(args[0])?.to_bytes()?;\n                 let kind = match self.layout_of(ty)?.abi {\n                     ty::layout::Abi::Scalar(ref scalar) => scalar.value,\n                     _ => Err(::rustc::mir::interpret::EvalErrorKind::TypeNotPrimitive(ty))?,\n@@ -240,22 +209,18 @@ impl<'a, 'mir, 'tcx> EvalContextExt<'tcx> for EvalContext<'a, 'mir, 'tcx, super:\n                 } else {\n                     numeric_intrinsic(intrinsic_name, num, kind)?\n                 };\n-                self.write_scalar(dest, num, ty)?;\n+                self.write_scalar(num, dest)?;\n             }\n \n             \"discriminant_value\" => {\n-                let ty = substs.type_at(0);\n-                let layout = self.layout_of(ty)?;\n-                let adt_ptr = self.into_ptr(args[0].value)?;\n-                let adt_align = self.layout_of(args[0].ty)?.align;\n-                let place = Place::from_scalar_ptr(adt_ptr, adt_align);\n-                let discr_val = self.read_discriminant_value(place, layout)?;\n-                self.write_scalar(dest, Scalar::from_uint(discr_val, dest_layout.size), dest_layout.ty)?;\n+                let place = self.ref_to_mplace(self.read_value(args[0])?)?;\n+                let discr_val = self.read_discriminant_value(place.into())?;\n+                self.write_scalar(Scalar::from_uint(discr_val, dest.layout.size), dest)?;\n             }\n \n             \"sinf32\" | \"fabsf32\" | \"cosf32\" | \"sqrtf32\" | \"expf32\" | \"exp2f32\" | \"logf32\" |\n             \"log10f32\" | \"log2f32\" | \"floorf32\" | \"ceilf32\" | \"truncf32\" => {\n-                let f = self.value_to_scalar(args[0])?.to_bytes()?;\n+                let f = self.read_scalar(args[0])?.to_bytes()?;\n                 let f = f32::from_bits(f as u32);\n                 let f = match intrinsic_name {\n                     \"sinf32\" => f.sin(),\n@@ -272,12 +237,12 @@ impl<'a, 'mir, 'tcx> EvalContextExt<'tcx> for EvalContext<'a, 'mir, 'tcx, super:\n                     \"truncf32\" => f.trunc(),\n                     _ => bug!(),\n                 };\n-                self.write_scalar(dest, Scalar::from_f32(f), dest_layout.ty)?;\n+                self.write_scalar(Scalar::from_f32(f), dest)?;\n             }\n \n             \"sinf64\" | \"fabsf64\" | \"cosf64\" | \"sqrtf64\" | \"expf64\" | \"exp2f64\" | \"logf64\" |\n             \"log10f64\" | \"log2f64\" | \"floorf64\" | \"ceilf64\" | \"truncf64\" => {\n-                let f = self.value_to_scalar(args[0])?.to_bytes()?;\n+                let f = self.read_scalar(args[0])?.to_bytes()?;\n                 let f = f64::from_bits(f as u64);\n                 let f = match intrinsic_name {\n                     \"sinf64\" => f.sin(),\n@@ -294,13 +259,12 @@ impl<'a, 'mir, 'tcx> EvalContextExt<'tcx> for EvalContext<'a, 'mir, 'tcx, super:\n                     \"truncf64\" => f.trunc(),\n                     _ => bug!(),\n                 };\n-                self.write_scalar(dest, Scalar::from_f64(f), dest_layout.ty)?;\n+                self.write_scalar(Scalar::from_f64(f), dest)?;\n             }\n \n             \"fadd_fast\" | \"fsub_fast\" | \"fmul_fast\" | \"fdiv_fast\" | \"frem_fast\" => {\n-                let ty = substs.type_at(0);\n-                let a = self.value_to_scalar(args[0])?;\n-                let b = self.value_to_scalar(args[1])?;\n+                let a = self.read_value(args[0])?;\n+                let b = self.read_value(args[1])?;\n                 let op = match intrinsic_name {\n                     \"fadd_fast\" => mir::BinOp::Add,\n                     \"fsub_fast\" => mir::BinOp::Sub,\n@@ -309,48 +273,43 @@ impl<'a, 'mir, 'tcx> EvalContextExt<'tcx> for EvalContext<'a, 'mir, 'tcx, super:\n                     \"frem_fast\" => mir::BinOp::Rem,\n                     _ => bug!(),\n                 };\n-                let result = self.binary_op(op, a, ty, b, ty)?;\n-                self.write_scalar(dest, result.0, dest_layout.ty)?;\n+                let result = self.binary_op(op, a, b)?;\n+                self.write_scalar(result.0, dest)?;\n             }\n \n             \"exact_div\" => {\n                 // Performs an exact division, resulting in undefined behavior where\n                 // `x % y != 0` or `y == 0` or `x == T::min_value() && y == -1`\n-                let ty = substs.type_at(0);\n-                let a = self.value_to_scalar(args[0])?;\n-                let b = self.value_to_scalar(args[1])?;\n+                let a = self.read_value(args[0])?;\n+                let b = self.read_value(args[1])?;\n                 // check x % y != 0\n-                if !self.binary_op(mir::BinOp::Rem, a, ty, b, ty)?.0.is_null() {\n+                if !self.binary_op(mir::BinOp::Rem, a, b)?.0.is_null() {\n                     return err!(ValidationFailure(format!(\"exact_div: {:?} cannot be divided by {:?}\", a, b)));\n                 }\n-                let result = self.binary_op(mir::BinOp::Div, a, ty, b, ty)?;\n-                self.write_scalar(dest, result.0, dest_layout.ty)?;\n+                let result = self.binary_op(mir::BinOp::Div, a, b)?;\n+                self.write_scalar(result.0, dest)?;\n             },\n \n             \"likely\" | \"unlikely\" | \"forget\" => {}\n \n             \"init\" => {\n-                // we don't want to force an allocation in case the destination is a simple value\n-                match dest {\n-                    Place::Local { frame, local } => {\n-                        match self.stack()[frame].locals[local].access()? {\n-                            Value::ByRef(ptr, _) => {\n-                                // These writes have no alignment restriction anyway.\n-                                self.memory.write_repeat(ptr, 0, dest_layout.size)?;\n-                            }\n-                            Value::Scalar(_) => self.write_value(ValTy { value: Value::Scalar(Scalar::null(dest_layout.size).into()), ty: dest_layout.ty }, dest)?,\n-                            Value::ScalarPair(..) => {\n-                                self.write_value(ValTy { value: Value::ScalarPair(Scalar::null(dest_layout.size).into(), Scalar::null(dest_layout.size).into()), ty: dest_layout.ty }, dest)?;\n-                            }\n-                        }\n-                    },\n-                    Place::Ptr {\n-                        ptr,\n-                        align: _align,\n-                        extra: PlaceExtra::None,\n-                    } => self.memory.write_repeat(ptr.unwrap_or_err()?, 0, dest_layout.size)?,\n-                    Place::Ptr { .. } => {\n-                        bug!(\"init intrinsic tried to write to fat or unaligned ptr target\")\n+                // Check fast path: we don't want to force an allocation in case the destination is a simple value,\n+                // but we also do not want to create a new allocation with 0s and then copy that over.\n+                match dest.layout.abi {\n+                    layout::Abi::Scalar(ref s) => {\n+                        let x = Scalar::null(s.value.size(&self));\n+                        self.write_value(Value::Scalar(x.into()), dest)?;\n+                    }\n+                    layout::Abi::ScalarPair(ref s1, ref s2) => {\n+                        let x = Scalar::null(s1.value.size(&self));\n+                        let y = Scalar::null(s2.value.size(&self));\n+                        self.write_value(Value::ScalarPair(x.into(), y.into()), dest)?;\n+                    }\n+                    _ => {\n+                        // Do it in memory\n+                        let mplace = self.force_allocation(dest)?;\n+                        assert_eq!(mplace.extra, PlaceExtra::None);\n+                        self.memory.write_repeat(mplace.ptr, 0, dest.layout.size)?;\n                     }\n                 }\n             }\n@@ -360,7 +319,7 @@ impl<'a, 'mir, 'tcx> EvalContextExt<'tcx> for EvalContext<'a, 'mir, 'tcx, super:\n                 let elem_align = self.layout_of(elem_ty)?.align.abi();\n                 let ptr_size = self.memory.pointer_size();\n                 let align_val = Scalar::from_uint(elem_align as u128, ptr_size);\n-                self.write_scalar(dest, align_val, dest_layout.ty)?;\n+                self.write_scalar(align_val, dest)?;\n             }\n \n             \"pref_align_of\" => {\n@@ -369,286 +328,273 @@ impl<'a, 'mir, 'tcx> EvalContextExt<'tcx> for EvalContext<'a, 'mir, 'tcx, super:\n                 let align = layout.align.pref();\n                 let ptr_size = self.memory.pointer_size();\n                 let align_val = Scalar::from_uint(align as u128, ptr_size);\n-                self.write_scalar(dest, align_val, dest_layout.ty)?;\n+                self.write_scalar(align_val, dest)?;\n             }\n \n             \"move_val_init\" => {\n-                let ty = substs.type_at(0);\n-                let ptr = self.into_ptr(args[0].value)?.unwrap_or_err()?;\n-                let align = self.layout_of(args[0].ty)?.align;\n-                self.write_value_to_ptr(args[1].value, ptr, align, ty)?;\n+                let ptr = self.ref_to_mplace(self.read_value(args[0])?)?;\n+                self.copy_op(args[1], ptr.into())?;\n             }\n \n             \"needs_drop\" => {\n                 let ty = substs.type_at(0);\n                 let env = ty::ParamEnv::reveal_all();\n                 let needs_drop = ty.needs_drop(self.tcx.tcx, env);\n                 self.write_scalar(\n-                    dest,\n                     Scalar::from_bool(needs_drop),\n-                    dest_layout.ty,\n+                    dest,\n                 )?;\n             }\n \n             \"offset\" => {\n-                let offset = self.value_to_isize(args[1])?;\n-                let ptr = self.into_ptr(args[0].value)?.unwrap_or_err()?;\n-                let result_ptr = self.pointer_offset(ptr, substs.type_at(0), offset)?;\n-                self.write_ptr(dest, result_ptr, dest_layout.ty)?;\n+                let offset = self.read_scalar(args[1])?.to_isize(&self)?;\n+                let ptr = self.read_scalar(args[0])?.not_undef()?;\n+                let result_ptr = self.pointer_offset_inbounds(ptr, substs.type_at(0), offset)?;\n+                self.write_scalar(result_ptr, dest)?;\n             }\n \n             \"overflowing_sub\" => {\n-                self.intrinsic_overflowing(\n+                let l = self.read_value(args[0])?;\n+                let r = self.read_value(args[1])?;\n+                self.binop_ignore_overflow(\n                     mir::BinOp::Sub,\n-                    args[0],\n-                    args[1],\n+                    l,\n+                    r,\n                     dest,\n-                    dest_layout.ty,\n                 )?;\n             }\n \n             \"overflowing_mul\" => {\n-                self.intrinsic_overflowing(\n+                let l = self.read_value(args[0])?;\n+                let r = self.read_value(args[1])?;\n+                self.binop_ignore_overflow(\n                     mir::BinOp::Mul,\n-                    args[0],\n-                    args[1],\n+                    r,\n+                    l,\n                     dest,\n-                    dest_layout.ty,\n                 )?;\n             }\n \n             \"overflowing_add\" => {\n-                self.intrinsic_overflowing(\n+                let l = self.read_value(args[0])?;\n+                let r = self.read_value(args[1])?;\n+                self.binop_ignore_overflow(\n                     mir::BinOp::Add,\n-                    args[0],\n-                    args[1],\n+                    r,\n+                    l,\n                     dest,\n-                    dest_layout.ty,\n                 )?;\n             }\n \n             \"powf32\" => {\n-                let f = self.value_to_scalar(args[0])?.to_bits(Size::from_bits(32))?;\n+                let f = self.read_scalar(args[0])?.to_bits(Size::from_bits(32))?;\n                 let f = f32::from_bits(f as u32);\n-                let f2 = self.value_to_scalar(args[1])?.to_bits(Size::from_bits(32))?;\n+                let f2 = self.read_scalar(args[1])?.to_bits(Size::from_bits(32))?;\n                 let f2 = f32::from_bits(f2 as u32);\n                 self.write_scalar(\n-                    dest,\n                     Scalar::from_f32(f.powf(f2)),\n-                    dest_layout.ty,\n+                    dest,\n                 )?;\n             }\n \n             \"powf64\" => {\n-                let f = self.value_to_scalar(args[0])?.to_bits(Size::from_bits(64))?;\n+                let f = self.read_scalar(args[0])?.to_bits(Size::from_bits(64))?;\n                 let f = f64::from_bits(f as u64);\n-                let f2 = self.value_to_scalar(args[1])?.to_bits(Size::from_bits(64))?;\n+                let f2 = self.read_scalar(args[1])?.to_bits(Size::from_bits(64))?;\n                 let f2 = f64::from_bits(f2 as u64);\n                 self.write_scalar(\n-                    dest,\n                     Scalar::from_f64(f.powf(f2)),\n-                    dest_layout.ty,\n+                    dest,\n                 )?;\n             }\n \n             \"fmaf32\" => {\n-                let a = self.value_to_scalar(args[0])?.to_bits(Size::from_bits(32))?;\n+                let a = self.read_scalar(args[0])?.to_bits(Size::from_bits(32))?;\n                 let a = f32::from_bits(a as u32);\n-                let b = self.value_to_scalar(args[1])?.to_bits(Size::from_bits(32))?;\n+                let b = self.read_scalar(args[1])?.to_bits(Size::from_bits(32))?;\n                 let b = f32::from_bits(b as u32);\n-                let c = self.value_to_scalar(args[2])?.to_bits(Size::from_bits(32))?;\n+                let c = self.read_scalar(args[2])?.to_bits(Size::from_bits(32))?;\n                 let c = f32::from_bits(c as u32);\n                 self.write_scalar(\n-                    dest,\n                     Scalar::from_f32(a * b + c),\n-                    dest_layout.ty,\n+                    dest,\n                 )?;\n             }\n \n             \"fmaf64\" => {\n-                let a = self.value_to_scalar(args[0])?.to_bits(Size::from_bits(64))?;\n+                let a = self.read_scalar(args[0])?.to_bits(Size::from_bits(64))?;\n                 let a = f64::from_bits(a as u64);\n-                let b = self.value_to_scalar(args[1])?.to_bits(Size::from_bits(64))?;\n+                let b = self.read_scalar(args[1])?.to_bits(Size::from_bits(64))?;\n                 let b = f64::from_bits(b as u64);\n-                let c = self.value_to_scalar(args[2])?.to_bits(Size::from_bits(64))?;\n+                let c = self.read_scalar(args[2])?.to_bits(Size::from_bits(64))?;\n                 let c = f64::from_bits(c as u64);\n                 self.write_scalar(\n-                    dest,\n                     Scalar::from_f64(a * b + c),\n-                    dest_layout.ty,\n+                    dest,\n                 )?;\n             }\n \n             \"powif32\" => {\n-                let f = self.value_to_scalar(args[0])?.to_bits(Size::from_bits(32))?;\n+                let f = self.read_scalar(args[0])?.to_bits(Size::from_bits(32))?;\n                 let f = f32::from_bits(f as u32);\n-                let i = self.value_to_i32(args[1])?;\n+                let i = self.read_scalar(args[1])?.to_i32()?;\n                 self.write_scalar(\n-                    dest,\n                     Scalar::from_f32(f.powi(i)),\n-                    dest_layout.ty,\n+                    dest,\n                 )?;\n             }\n \n             \"powif64\" => {\n-                let f = self.value_to_scalar(args[0])?.to_bits(Size::from_bits(64))?;\n+                let f = self.read_scalar(args[0])?.to_bits(Size::from_bits(64))?;\n                 let f = f64::from_bits(f as u64);\n-                let i = self.value_to_i32(args[1])?;\n+                let i = self.read_scalar(args[1])?.to_i32()?;\n                 self.write_scalar(\n-                    dest,\n                     Scalar::from_f64(f.powi(i)),\n-                    dest_layout.ty,\n+                    dest,\n                 )?;\n             }\n \n             \"size_of\" => {\n                 let ty = substs.type_at(0);\n                 let size = self.layout_of(ty)?.size.bytes();\n                 let ptr_size = self.memory.pointer_size();\n-                self.write_scalar(dest, Scalar::from_uint(size, ptr_size), dest_layout.ty)?;\n+                self.write_scalar(Scalar::from_uint(size, ptr_size), dest)?;\n             }\n \n             \"size_of_val\" => {\n-                let ty = substs.type_at(0);\n-                let (size, _) = self.size_and_align_of_dst(ty, args[0].value)?;\n+                let mplace = self.ref_to_mplace(self.read_value(args[0])?)?;\n+                let (size, _) = self.size_and_align_of_mplace(mplace)?;\n                 let ptr_size = self.memory.pointer_size();\n                 self.write_scalar(\n-                    dest,\n                     Scalar::from_uint(size.bytes() as u128, ptr_size),\n-                    dest_layout.ty,\n+                    dest,\n                 )?;\n             }\n \n             \"min_align_of_val\" |\n             \"align_of_val\" => {\n-                let ty = substs.type_at(0);\n-                let (_, align) = self.size_and_align_of_dst(ty, args[0].value)?;\n+                let mplace = self.ref_to_mplace(self.read_value(args[0])?)?;\n+                let (_, align) = self.size_and_align_of_mplace(mplace)?;\n                 let ptr_size = self.memory.pointer_size();\n                 self.write_scalar(\n-                    dest,\n                     Scalar::from_uint(align.abi(), ptr_size),\n-                    dest_layout.ty,\n+                    dest,\n                 )?;\n             }\n \n             \"type_name\" => {\n                 let ty = substs.type_at(0);\n                 let ty_name = ty.to_string();\n                 let value = self.str_to_value(&ty_name)?;\n-                self.write_value(ValTy { value, ty: dest_layout.ty }, dest)?;\n+                self.write_value(value, dest)?;\n             }\n             \"type_id\" => {\n                 let ty = substs.type_at(0);\n                 let n = self.tcx.type_id_hash(ty);\n-                self.write_scalar(dest, Scalar::Bits { bits: n as u128, size: 8 }, dest_layout.ty)?;\n+                self.write_scalar(Scalar::Bits { bits: n as u128, size: 8 }, dest)?;\n             }\n \n             \"transmute\" => {\n-                let src_ty = substs.type_at(0);\n-                let _src_align = self.layout_of(src_ty)?.align;\n-                let ptr = self.force_allocation(dest)?.to_ptr()?;\n-                let dest_align = self.layout_of(substs.type_at(1))?.align;\n-                self.write_value_to_ptr(args[0].value, ptr.into(), dest_align, src_ty).unwrap();\n+                // Go through an allocation, to make sure the completely different layouts\n+                // do not pose a problem.  (When the user transmutes through a union,\n+                // there will not be a layout mismatch.)\n+                let dest = self.force_allocation(dest)?;\n+                self.copy_op(args[0], dest.into())?;\n             }\n \n             \"unchecked_shl\" => {\n-                let bits = dest_layout.size.bytes() as u128 * 8;\n-                let rhs = self.value_to_scalar(args[1])?\n-                    .to_bytes()?;\n-                if rhs >= bits {\n+                let bits = dest.layout.size.bytes() as u128 * 8;\n+                let l = self.read_value(args[0])?;\n+                let r = self.read_value(args[1])?;\n+                let rval = r.to_scalar()?.to_bytes()?;\n+                if rval >= bits {\n                     return err!(Intrinsic(\n-                        format!(\"Overflowing shift by {} in unchecked_shl\", rhs),\n+                        format!(\"Overflowing shift by {} in unchecked_shl\", rval),\n                     ));\n                 }\n-                self.intrinsic_overflowing(\n+                self.binop_ignore_overflow(\n                     mir::BinOp::Shl,\n-                    args[0],\n-                    args[1],\n+                    l,\n+                    r,\n                     dest,\n-                    dest_layout.ty,\n                 )?;\n             }\n \n             \"unchecked_shr\" => {\n-                let bits = dest_layout.size.bytes() as u128 * 8;\n-                let rhs = self.value_to_scalar(args[1])?\n-                    .to_bytes()?;\n-                if rhs >= bits {\n+                let bits = dest.layout.size.bytes() as u128 * 8;\n+                let l = self.read_value(args[0])?;\n+                let r = self.read_value(args[1])?;\n+                let rval = r.to_scalar()?.to_bytes()?;\n+                if rval >= bits {\n                     return err!(Intrinsic(\n-                        format!(\"Overflowing shift by {} in unchecked_shr\", rhs),\n+                        format!(\"Overflowing shift by {} in unchecked_shr\", rval),\n                     ));\n                 }\n-                self.intrinsic_overflowing(\n+                self.binop_ignore_overflow(\n                     mir::BinOp::Shr,\n-                    args[0],\n-                    args[1],\n+                    l,\n+                    r,\n                     dest,\n-                    dest_layout.ty,\n                 )?;\n             }\n \n             \"unchecked_div\" => {\n-                let rhs = self.value_to_scalar(args[1])?\n-                    .to_bytes()?;\n-                if rhs == 0 {\n+                let l = self.read_value(args[0])?;\n+                let r = self.read_value(args[1])?;\n+                let rval = r.to_scalar()?.to_bytes()?;\n+                if rval == 0 {\n                     return err!(Intrinsic(format!(\"Division by 0 in unchecked_div\")));\n                 }\n-                self.intrinsic_overflowing(\n+                self.binop_ignore_overflow(\n                     mir::BinOp::Div,\n-                    args[0],\n-                    args[1],\n+                    l,\n+                    r,\n                     dest,\n-                    dest_layout.ty,\n                 )?;\n             }\n \n             \"unchecked_rem\" => {\n-                let rhs = self.value_to_scalar(args[1])?\n-                    .to_bytes()?;\n-                if rhs == 0 {\n+                let l = self.read_value(args[0])?;\n+                let r = self.read_value(args[1])?;\n+                let rval = r.to_scalar()?.to_bytes()?;\n+                if rval == 0 {\n                     return err!(Intrinsic(format!(\"Division by 0 in unchecked_rem\")));\n                 }\n-                self.intrinsic_overflowing(\n+                self.binop_ignore_overflow(\n                     mir::BinOp::Rem,\n-                    args[0],\n-                    args[1],\n+                    l,\n+                    r,\n                     dest,\n-                    dest_layout.ty,\n                 )?;\n             }\n \n             \"uninit\" => {\n-                // we don't want to force an allocation in case the destination is a simple value\n-                match dest {\n-                    Place::Local { frame, local } => {\n-                        match self.stack()[frame].locals[local].access()? {\n-                            Value::ByRef(ptr, _) => {\n-                                // These writes have no alignment restriction anyway.\n-                                self.memory.mark_definedness(ptr, dest_layout.size, false)?;\n-                            }\n-                            Value::Scalar(_) => self.write_value(ValTy { value: Value::Scalar(ScalarMaybeUndef::Undef), ty: dest_layout.ty }, dest)?,\n-                            Value::ScalarPair(..) => {\n-                                self.write_value(ValTy { value: Value::ScalarPair(ScalarMaybeUndef::Undef, ScalarMaybeUndef::Undef), ty: dest_layout.ty }, dest)?;\n-                            }\n-                        }\n-                    },\n-                    Place::Ptr {\n-                        ptr,\n-                        align: _align,\n-                        extra: PlaceExtra::None,\n-                    } => self.memory.mark_definedness(ptr.unwrap_or_err()?, dest_layout.size, false)?,\n-                    Place::Ptr { .. } => {\n-                        bug!(\"uninit intrinsic tried to write to fat or unaligned ptr target\")\n+                // Check fast path: we don't want to force an allocation in case the destination is a simple value,\n+                // but we also do not want to create a new allocation with 0s and then copy that over.\n+                match dest.layout.abi {\n+                    layout::Abi::Scalar(..) => {\n+                        let x = ScalarMaybeUndef::Undef;\n+                        self.write_value(Value::Scalar(x), dest)?;\n+                    }\n+                    layout::Abi::ScalarPair(..) => {\n+                        let x = ScalarMaybeUndef::Undef;\n+                        self.write_value(Value::ScalarPair(x, x), dest)?;\n+                    }\n+                    _ => {\n+                        // Do it in memory\n+                        let mplace = self.force_allocation(dest)?;\n+                        assert_eq!(mplace.extra, PlaceExtra::None);\n+                        self.memory.mark_definedness(mplace.ptr, dest.layout.size, false)?;\n                     }\n                 }\n             }\n \n             \"write_bytes\" => {\n                 let ty = substs.type_at(0);\n                 let ty_layout = self.layout_of(ty)?;\n-                let val_byte = self.value_to_u8(args[1])?;\n-                let ptr = self.into_ptr(args[0].value)?.unwrap_or_err()?;\n-                let count = self.value_to_usize(args[2])?;\n+                let val_byte = self.read_scalar(args[1])?.to_u8()?;\n+                let ptr = self.read_scalar(args[0])?.not_undef()?;\n+                let count = self.read_scalar(args[2])?.to_usize(&self)?;\n                 if count > 0 {\n                     // HashMap relies on write_bytes on a NULL ptr with count == 0 to work\n                     // TODO: Should we, at least, validate the alignment? (Also see the copy intrinsic)"}, {"sha": "3680d49836fb52ca3499b99536837f7c179f946c", "filename": "src/lib.rs", "status": "modified", "additions": 58, "deletions": 168, "changes": 226, "blob_url": "https://github.com/rust-lang/rust/blob/259cc6e3dccdf7dd7f9c8e2052c6dea19fe066d9/src%2Flib.rs", "raw_url": "https://github.com/rust-lang/rust/raw/259cc6e3dccdf7dd7f9c8e2052c6dea19fe066d9/src%2Flib.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flib.rs?ref=259cc6e3dccdf7dd7f9c8e2052c6dea19fe066d9", "patch": "@@ -27,6 +27,7 @@ use rustc_data_structures::fx::FxHasher;\n use syntax::ast::Mutability;\n use syntax::codemap::Span;\n \n+use std::marker::PhantomData;\n use std::collections::{HashMap, BTreeMap};\n use std::hash::{Hash, Hasher};\n \n@@ -41,81 +42,14 @@ mod memory;\n mod tls;\n mod locks;\n mod range_map;\n-mod validation;\n \n use fn_call::EvalContextExt as MissingFnsEvalContextExt;\n use operator::EvalContextExt as OperatorEvalContextExt;\n use intrinsic::EvalContextExt as IntrinsicEvalContextExt;\n use tls::EvalContextExt as TlsEvalContextExt;\n use locks::LockInfo;\n-use locks::MemoryExt as LockMemoryExt;\n-use validation::EvalContextExt as ValidationEvalContextExt;\n use range_map::RangeMap;\n-use validation::{ValidationQuery, AbsPlace};\n-\n-pub trait ScalarExt {\n-    fn null(size: Size) -> Self;\n-    fn from_i32(i: i32) -> Self;\n-    fn from_uint(i: impl Into<u128>, ptr_size: Size) -> Self;\n-    fn from_int(i: impl Into<i128>, ptr_size: Size) -> Self;\n-    fn from_f32(f: f32) -> Self;\n-    fn from_f64(f: f64) -> Self;\n-    fn to_usize<'a, 'mir, 'tcx>(self, ecx: &rustc_mir::interpret::EvalContext<'a, 'mir, 'tcx, Evaluator<'tcx>>) -> EvalResult<'static, u64>;\n-    fn is_null(self) -> bool;\n-    /// HACK: this function just extracts all bits if `defined != 0`\n-    /// Mainly used for args of C-functions and we should totally correctly fetch the size\n-    /// of their arguments\n-    fn to_bytes(self) -> EvalResult<'static, u128>;\n-}\n-\n-impl ScalarExt for Scalar {\n-    fn null(size: Size) -> Self {\n-        Scalar::Bits { bits: 0, size: size.bytes() as u8 }\n-    }\n-\n-    fn from_i32(i: i32) -> Self {\n-        Scalar::Bits { bits: i as u32 as u128, size: 4 }\n-    }\n-\n-    fn from_uint(i: impl Into<u128>, ptr_size: Size) -> Self {\n-        Scalar::Bits { bits: i.into(), size: ptr_size.bytes() as u8 }\n-    }\n-\n-    fn from_int(i: impl Into<i128>, ptr_size: Size) -> Self {\n-        Scalar::Bits { bits: i.into() as u128, size: ptr_size.bytes() as u8 }\n-    }\n-\n-    fn from_f32(f: f32) -> Self {\n-        Scalar::Bits { bits: f.to_bits() as u128, size: 4 }\n-    }\n-\n-    fn from_f64(f: f64) -> Self {\n-        Scalar::Bits { bits: f.to_bits() as u128, size: 8 }\n-    }\n-\n-    fn to_usize<'a, 'mir, 'tcx>(self, ecx: &rustc_mir::interpret::EvalContext<'a, 'mir, 'tcx, Evaluator<'tcx>>) -> EvalResult<'static, u64> {\n-        let b = self.to_bits(ecx.memory.pointer_size())?;\n-        assert_eq!(b as u64 as u128, b);\n-        Ok(b as u64)\n-    }\n-\n-    fn is_null(self) -> bool {\n-        match self {\n-            Scalar::Bits { bits, .. } => bits == 0,\n-            Scalar::Ptr(_) => false\n-        }\n-    }\n-\n-    fn to_bytes(self) -> EvalResult<'static, u128> {\n-        match self {\n-            Scalar::Bits { bits, size } => {\n-                assert_ne!(size, 0);\n-                Ok(bits)\n-            },\n-            Scalar::Ptr(_) => err!(ReadPointerAsBytes),\n-        }\n-    }\n-}\n+use helpers::{ScalarExt, FalibleScalarExt};\n \n pub fn create_ecx<'a, 'mir: 'a, 'tcx: 'mir>(\n     tcx: TyCtxt<'a, 'tcx, 'tcx>,\n@@ -180,31 +114,22 @@ pub fn create_ecx<'a, 'mir: 'a, 'tcx: 'mir>(\n         // First argument: pointer to main()\n         let main_ptr = ecx.memory_mut().create_fn_alloc(main_instance);\n         let dest = ecx.eval_place(&mir::Place::Local(args.next().unwrap()))?;\n-        let main_ty = main_instance.ty(ecx.tcx.tcx);\n-        let main_ptr_ty = ecx.tcx.mk_fn_ptr(main_ty.fn_sig(ecx.tcx.tcx));\n-        ecx.write_value(\n-            ValTy {\n-                value: Value::Scalar(Scalar::Ptr(main_ptr).into()),\n-                ty: main_ptr_ty,\n-            },\n-            dest,\n-        )?;\n+        ecx.write_scalar(Scalar::Ptr(main_ptr), dest)?;\n \n         // Second argument (argc): 1\n         let dest = ecx.eval_place(&mir::Place::Local(args.next().unwrap()))?;\n-        let ty = ecx.tcx.types.isize;\n-        ecx.write_scalar(dest, Scalar::from_int(1, ptr_size), ty)?;\n+        ecx.write_scalar(Scalar::from_int(1, dest.layout.size), dest)?;\n \n         // FIXME: extract main source file path\n         // Third argument (argv): &[b\"foo\"]\n         let dest = ecx.eval_place(&mir::Place::Local(args.next().unwrap()))?;\n-        let ty = ecx.tcx.mk_imm_ptr(ecx.tcx.mk_imm_ptr(ecx.tcx.types.u8));\n         let foo = ecx.memory.allocate_bytes(b\"foo\\0\");\n-        let ptr_align = ecx.tcx.data_layout.pointer_align;\n-        let foo_ptr = ecx.memory.allocate(ptr_size, ptr_align, MemoryKind::Stack)?;\n-        ecx.memory.write_scalar(foo_ptr.into(), ptr_align, Scalar::Ptr(foo).into(), ptr_size, ptr_align, false)?;\n-        ecx.memory.mark_static_initialized(foo_ptr.alloc_id, Mutability::Immutable)?;\n-        ecx.write_ptr(dest, foo_ptr.into(), ty)?;\n+        let foo_ty = ecx.tcx.mk_imm_ptr(ecx.tcx.types.u8);\n+        let foo_layout = ecx.layout_of(foo_ty)?;\n+        let foo_place = ecx.allocate(foo_layout, MemoryKind::Stack)?;\n+        ecx.write_scalar(Scalar::Ptr(foo), foo_place.into())?;\n+        ecx.memory.mark_static_initialized(foo_place.to_ptr()?.alloc_id, Mutability::Immutable)?;\n+        ecx.write_scalar(foo_place.ptr, dest)?;\n \n         assert!(args.next().is_none(), \"start lang item has more arguments than expected\");\n     } else {\n@@ -293,15 +218,15 @@ pub struct Evaluator<'tcx> {\n     /// Miri does not expose env vars from the host to the emulated program\n     pub(crate) env_vars: HashMap<Vec<u8>, Pointer>,\n \n-    /// Places that were suspended by the validation subsystem, and will be recovered later\n-    pub(crate) suspended: HashMap<DynamicLifetime, Vec<ValidationQuery<'tcx>>>,\n+    /// Use the lifetime\n+    _dummy : PhantomData<&'tcx ()>,\n }\n \n impl<'tcx> Hash for Evaluator<'tcx> {\n     fn hash<H: Hasher>(&self, state: &mut H) {\n         let Evaluator {\n             env_vars,\n-            suspended: _,\n+            _dummy: _,\n         } = self;\n \n         env_vars.iter()\n@@ -373,34 +298,32 @@ impl<'mir, 'tcx: 'mir> Machine<'mir, 'tcx> for Evaluator<'tcx> {\n     fn eval_fn_call<'a>(\n         ecx: &mut EvalContext<'a, 'mir, 'tcx, Self>,\n         instance: ty::Instance<'tcx>,\n-        destination: Option<(Place, mir::BasicBlock)>,\n-        args: &[ValTy<'tcx>],\n+        destination: Option<(PlaceTy<'tcx>, mir::BasicBlock)>,\n+        args: &[OpTy<'tcx>],\n         span: Span,\n-        sig: ty::FnSig<'tcx>,\n     ) -> EvalResult<'tcx, bool> {\n-        ecx.eval_fn_call(instance, destination, args, span, sig)\n+        ecx.eval_fn_call(instance, destination, args, span)\n     }\n \n     fn call_intrinsic<'a>(\n         ecx: &mut rustc_mir::interpret::EvalContext<'a, 'mir, 'tcx, Self>,\n         instance: ty::Instance<'tcx>,\n-        args: &[ValTy<'tcx>],\n-        dest: Place,\n-        dest_layout: TyLayout<'tcx>,\n+        args: &[OpTy<'tcx>],\n+        dest: PlaceTy<'tcx>,\n         target: mir::BasicBlock,\n     ) -> EvalResult<'tcx> {\n-        ecx.call_intrinsic(instance, args, dest, dest_layout, target)\n+        ecx.call_intrinsic(instance, args, dest, target)\n     }\n \n     fn try_ptr_op<'a>(\n         ecx: &rustc_mir::interpret::EvalContext<'a, 'mir, 'tcx, Self>,\n         bin_op: mir::BinOp,\n         left: Scalar,\n-        left_ty: ty::Ty<'tcx>,\n+        left_layout: TyLayout<'tcx>,\n         right: Scalar,\n-        right_ty: ty::Ty<'tcx>,\n+        right_layout: TyLayout<'tcx>,\n     ) -> EvalResult<'tcx, Option<(Scalar, bool)>> {\n-        ecx.ptr_op(bin_op, left, left_ty, right, right_ty)\n+        ecx.ptr_op(bin_op, left, left_layout, right, right_layout)\n     }\n \n     fn mark_static_initialized<'a>(\n@@ -460,14 +383,16 @@ impl<'mir, 'tcx: 'mir> Machine<'mir, 'tcx> for Evaluator<'tcx> {\n         let call_stackframe = ecx.stack().len();\n         while ecx.step()? && ecx.stack().len() >= call_stackframe {\n             if ecx.stack().len() == call_stackframe {\n-                let frame = ecx.frame_mut();\n-                let bb = &frame.mir.basic_blocks()[frame.block];\n-                if bb.statements.len() == frame.stmt && !bb.is_cleanup {\n-                    if let ::rustc::mir::TerminatorKind::Return = bb.terminator().kind {\n-                        for (local, _local_decl) in mir.local_decls.iter_enumerated().skip(1) {\n-                            // Don't deallocate locals, because the return value might reference them\n-                            frame.storage_dead(local);\n-                        }\n+                let cleanup = {\n+                    let frame = ecx.frame();\n+                    let bb = &frame.mir.basic_blocks()[frame.block];\n+                    bb.statements.len() == frame.stmt && !bb.is_cleanup &&\n+                        if let ::rustc::mir::TerminatorKind::Return = bb.terminator().kind { true } else { false }\n+                };\n+                if cleanup {\n+                    for (local, _local_decl) in mir.local_decls.iter_enumerated().skip(1) {\n+                        // Don't deallocate locals, because the return value might reference them\n+                        ecx.storage_dead(local);\n                     }\n                 }\n             }\n@@ -481,11 +406,9 @@ impl<'mir, 'tcx: 'mir> Machine<'mir, 'tcx> for Evaluator<'tcx> {\n \n     fn box_alloc<'a>(\n         ecx: &mut EvalContext<'a, 'mir, 'tcx, Self>,\n-        ty: ty::Ty<'tcx>,\n-        dest: Place,\n+        dest: PlaceTy<'tcx>,\n     ) -> EvalResult<'tcx> {\n-        let layout = ecx.layout_of(ty)?;\n-\n+        trace!(\"box_alloc for {:?}\", dest.layout.ty);\n         // Call the `exchange_malloc` lang item\n         let malloc = ecx.tcx.lang_items().exchange_malloc_fn().unwrap();\n         let malloc = ty::Instance::mono(ecx.tcx.tcx, malloc);\n@@ -494,39 +417,26 @@ impl<'mir, 'tcx: 'mir> Machine<'mir, 'tcx> for Evaluator<'tcx> {\n             malloc,\n             malloc_mir.span,\n             malloc_mir,\n-            dest,\n+            *dest,\n             // Don't do anything when we are done.  The statement() function will increment\n             // the old stack frame's stmt counter to the next statement, which means that when\n             // exchange_malloc returns, we go on evaluating exactly where we want to be.\n             StackPopCleanup::None,\n         )?;\n \n         let mut args = ecx.frame().mir.args_iter();\n-        let usize = ecx.tcx.types.usize;\n-        let ptr_size = ecx.memory.pointer_size();\n+        let layout = ecx.layout_of(dest.layout.ty.builtin_deref(false).unwrap().ty)?;\n \n         // First argument: size\n-        let dest = ecx.eval_place(&mir::Place::Local(args.next().unwrap()))?;\n-        ecx.write_value(\n-            ValTy {\n-                value: Value::Scalar(Scalar::from_uint(match layout.size.bytes() {\n-                    0 => 1,\n-                    size => size,\n-                }, ptr_size).into()),\n-                ty: usize,\n-            },\n-            dest,\n-        )?;\n+        // (0 is allowed here, this is expected to be handled by the lang item)\n+        let arg = ecx.eval_place(&mir::Place::Local(args.next().unwrap()))?;\n+        let size = layout.size.bytes();\n+        ecx.write_scalar(Scalar::from_uint(size, arg.layout.size), arg)?;\n \n         // Second argument: align\n-        let dest = ecx.eval_place(&mir::Place::Local(args.next().unwrap()))?;\n-        ecx.write_value(\n-            ValTy {\n-                value: Value::Scalar(Scalar::from_uint(layout.align.abi(), ptr_size).into()),\n-                ty: usize,\n-            },\n-            dest,\n-        )?;\n+        let arg = ecx.eval_place(&mir::Place::Local(args.next().unwrap()))?;\n+        let align = layout.align.abi();\n+        ecx.write_scalar(Scalar::from_uint(align, arg.layout.size), arg)?;\n \n         // No more arguments\n         assert!(args.next().is_none(), \"exchange_malloc lang item has more arguments than expected\");\n@@ -542,52 +452,32 @@ impl<'mir, 'tcx: 'mir> Machine<'mir, 'tcx> for Evaluator<'tcx> {\n     }\n \n     fn check_locks<'a>(\n-        mem: &Memory<'a, 'mir, 'tcx, Self>,\n-        ptr: Pointer,\n-        size: Size,\n-        access: AccessKind,\n+        _mem: &Memory<'a, 'mir, 'tcx, Self>,\n+        _ptr: Pointer,\n+        _size: Size,\n+        _access: AccessKind,\n     ) -> EvalResult<'tcx> {\n-        mem.check_locks(ptr, size.bytes(), access)\n+        Ok(())\n     }\n \n     fn add_lock<'a>(\n-        mem: &mut Memory<'a, 'mir, 'tcx, Self>,\n-        id: AllocId,\n-    ) {\n-        mem.data.locks.insert(id, RangeMap::new());\n-    }\n+        _mem: &mut Memory<'a, 'mir, 'tcx, Self>,\n+        _id: AllocId,\n+    ) { }\n \n     fn free_lock<'a>(\n-        mem: &mut Memory<'a, 'mir, 'tcx, Self>,\n-        id: AllocId,\n-        len: u64,\n+        _mem: &mut Memory<'a, 'mir, 'tcx, Self>,\n+        _id: AllocId,\n+        _len: u64,\n     ) -> EvalResult<'tcx> {\n-        mem.data.locks\n-            .remove(&id)\n-            .expect(\"allocation has no corresponding locks\")\n-            .check(\n-                Some(mem.cur_frame),\n-                0,\n-                len,\n-                AccessKind::Read,\n-            )\n-            .map_err(|lock| {\n-                EvalErrorKind::DeallocatedLockedMemory {\n-                    //ptr, FIXME\n-                    ptr: Pointer {\n-                        alloc_id: AllocId(0),\n-                        offset: Size::from_bytes(0),\n-                    },\n-                    lock: lock.active,\n-                }.into()\n-            })\n+        Ok(())\n     }\n \n     fn end_region<'a>(\n-        ecx: &mut EvalContext<'a, 'mir, 'tcx, Self>,\n-        reg: Option<::rustc::middle::region::Scope>,\n+        _ecx: &mut EvalContext<'a, 'mir, 'tcx, Self>,\n+        _reg: Option<::rustc::middle::region::Scope>,\n     ) -> EvalResult<'tcx> {\n-        ecx.end_region(reg)\n+        Ok(())\n     }\n \n     fn validation_op<'a>("}, {"sha": "a87ff6367e3ad89c398360e87013b3d994586d22", "filename": "src/locks.rs", "status": "modified", "additions": 5, "deletions": 315, "changes": 320, "blob_url": "https://github.com/rust-lang/rust/blob/259cc6e3dccdf7dd7f9c8e2052c6dea19fe066d9/src%2Flocks.rs", "raw_url": "https://github.com/rust-lang/rust/raw/259cc6e3dccdf7dd7f9c8e2052c6dea19fe066d9/src%2Flocks.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flocks.rs?ref=259cc6e3dccdf7dd7f9c8e2052c6dea19fe066d9", "patch": "@@ -1,3 +1,5 @@\n+#![allow(unused)]\n+\n use super::*;\n use rustc::middle::region;\n use rustc::ty::layout::Size;\n@@ -6,6 +8,9 @@ use rustc::ty::layout::Size;\n // Locks\n ////////////////////////////////////////////////////////////////////////////////\n \n+// Just some dummy to keep this compiling; I think some of this will be useful later\n+type AbsPlace<'tcx> = ::rustc::ty::Ty<'tcx>;\n+\n /// Information about a lock that is currently held.\n #[derive(Clone, Debug, PartialEq, Eq)]\n pub struct LockInfo<'tcx> {\n@@ -67,321 +72,6 @@ impl<'tcx> LockInfo<'tcx> {\n     }\n }\n \n-pub trait MemoryExt<'tcx> {\n-    fn check_locks(\n-        &self,\n-        ptr: Pointer,\n-        len: u64,\n-        access: AccessKind,\n-    ) -> EvalResult<'tcx>;\n-    fn acquire_lock(\n-        &mut self,\n-        ptr: Pointer,\n-        len: u64,\n-        region: Option<region::Scope>,\n-        kind: AccessKind,\n-    ) -> EvalResult<'tcx>;\n-    fn suspend_write_lock(\n-        &mut self,\n-        ptr: Pointer,\n-        len: u64,\n-        lock_path: &AbsPlace<'tcx>,\n-        suspend: Option<region::Scope>,\n-    ) -> EvalResult<'tcx>;\n-    fn recover_write_lock(\n-        &mut self,\n-        ptr: Pointer,\n-        len: u64,\n-        lock_path: &AbsPlace<'tcx>,\n-        lock_region: Option<region::Scope>,\n-        suspended_region: region::Scope,\n-    ) -> EvalResult<'tcx>;\n-    fn locks_lifetime_ended(&mut self, ending_region: Option<region::Scope>);\n-}\n-\n-\n-impl<'a, 'mir, 'tcx: 'mir + 'a> MemoryExt<'tcx> for Memory<'a, 'mir, 'tcx, Evaluator<'tcx>> {\n-    fn check_locks(\n-        &self,\n-        ptr: Pointer,\n-        len: u64,\n-        access: AccessKind,\n-    ) -> EvalResult<'tcx> {\n-        if len == 0 {\n-            return Ok(());\n-        }\n-        let locks = match self.data.locks.get(&ptr.alloc_id) {\n-            Some(locks) => locks,\n-            // immutable static or other constant memory\n-            None => return Ok(()),\n-        };\n-        let frame = self.cur_frame;\n-        locks\n-            .check(Some(frame), ptr.offset.bytes(), len, access)\n-            .map_err(|lock| {\n-                EvalErrorKind::MemoryLockViolation {\n-                    ptr,\n-                    len,\n-                    frame,\n-                    access,\n-                    lock: lock.active,\n-                }.into()\n-            })\n-    }\n-\n-    /// Acquire the lock for the given lifetime\n-    fn acquire_lock(\n-        &mut self,\n-        ptr: Pointer,\n-        len: u64,\n-        region: Option<region::Scope>,\n-        kind: AccessKind,\n-    ) -> EvalResult<'tcx> {\n-        let frame = self.cur_frame;\n-        assert!(len > 0);\n-        trace!(\n-            \"Frame {} acquiring {:?} lock at {:?}, size {} for region {:?}\",\n-            frame,\n-            kind,\n-            ptr,\n-            len,\n-            region\n-        );\n-        self.check_bounds(ptr.offset(Size::from_bytes(len), &*self)?, true)?; // if ptr.offset is in bounds, then so is ptr (because offset checks for overflow)\n-\n-        let locks = match self.data.locks.get_mut(&ptr.alloc_id) {\n-            Some(locks) => locks,\n-            // immutable static or other constant memory\n-            None => return Ok(()),\n-        };\n-\n-        // Iterate over our range and acquire the lock.  If the range is already split into pieces,\n-        // we have to manipulate all of them.\n-        let lifetime = DynamicLifetime { frame, region };\n-        for lock in locks.iter_mut(ptr.offset.bytes(), len) {\n-            if !lock.access_permitted(None, kind) {\n-                return err!(MemoryAcquireConflict {\n-                    ptr,\n-                    len,\n-                    kind,\n-                    lock: lock.active.clone(),\n-                });\n-            }\n-            // See what we have to do\n-            match (&mut lock.active, kind) {\n-                (active @ &mut NoLock, AccessKind::Write) => {\n-                    *active = WriteLock(lifetime);\n-                }\n-                (active @ &mut NoLock, AccessKind::Read) => {\n-                    *active = ReadLock(vec![lifetime]);\n-                }\n-                (&mut ReadLock(ref mut lifetimes), AccessKind::Read) => {\n-                    lifetimes.push(lifetime);\n-                }\n-                _ => bug!(\"We already checked that there is no conflicting lock\"),\n-            }\n-        }\n-        Ok(())\n-    }\n-\n-    /// Release or suspend a write lock of the given lifetime prematurely.\n-    /// When releasing, if there is a read lock or someone else's write lock, that's an error.\n-    /// If no lock is held, that's fine.  This can happen when e.g. a local is initialized\n-    /// from a constant, and then suspended.\n-    /// When suspending, the same cases are fine; we just register an additional suspension.\n-    fn suspend_write_lock(\n-        &mut self,\n-        ptr: Pointer,\n-        len: u64,\n-        lock_path: &AbsPlace<'tcx>,\n-        suspend: Option<region::Scope>,\n-    ) -> EvalResult<'tcx> {\n-        assert!(len > 0);\n-        let cur_frame = self.cur_frame;\n-        let locks = match self.data.locks.get_mut(&ptr.alloc_id) {\n-            Some(locks) => locks,\n-            // immutable static or other constant memory\n-            None => return Ok(()),\n-        };\n-\n-        'locks: for lock in locks.iter_mut(ptr.offset.bytes(), len) {\n-            let is_our_lock = match lock.active {\n-                WriteLock(lft) =>\n-                    // Double-check that we are holding the lock.\n-                    // (Due to subtyping, checking the region would not make any sense.)\n-                    lft.frame == cur_frame,\n-                ReadLock(_) | NoLock => false,\n-            };\n-            if is_our_lock {\n-                trace!(\"Releasing {:?}\", lock.active);\n-                // Disable the lock\n-                lock.active = NoLock;\n-            } else {\n-                trace!(\n-                    \"Not touching {:?} as it is not our lock\",\n-                    lock.active,\n-                );\n-            }\n-            // Check if we want to register a suspension\n-            if let Some(suspend_region) = suspend {\n-                let lock_id = WriteLockId {\n-                    frame: cur_frame,\n-                    path: lock_path.clone(),\n-                };\n-                trace!(\"Adding suspension to {:?}\", lock_id);\n-                let mut new_suspension = false;\n-                lock.suspended\n-                    .entry(lock_id)\n-                    // Remember whether we added a new suspension or not\n-                    .or_insert_with(|| { new_suspension = true; Vec::new() })\n-                    .push(suspend_region);\n-                // If the suspension is new, we should have owned this.\n-                // If there already was a suspension, we should NOT have owned this.\n-                if new_suspension == is_our_lock {\n-                    // All is well\n-                    continue 'locks;\n-                }\n-            } else if !is_our_lock {\n-                // All is well.\n-                continue 'locks;\n-            }\n-            // If we get here, releasing this is an error except for NoLock.\n-            if lock.active != NoLock {\n-                return err!(InvalidMemoryLockRelease {\n-                    ptr,\n-                    len,\n-                    frame: cur_frame,\n-                    lock: lock.active.clone(),\n-                });\n-            }\n-        }\n-\n-        Ok(())\n-    }\n-\n-    /// Release a suspension from the write lock.  If this is the last suspension or if there is no suspension, acquire the lock.\n-    fn recover_write_lock(\n-        &mut self,\n-        ptr: Pointer,\n-        len: u64,\n-        lock_path: &AbsPlace<'tcx>,\n-        lock_region: Option<region::Scope>,\n-        suspended_region: region::Scope,\n-    ) -> EvalResult<'tcx> {\n-        assert!(len > 0);\n-        let cur_frame = self.cur_frame;\n-        let lock_id = WriteLockId {\n-            frame: cur_frame,\n-            path: lock_path.clone(),\n-        };\n-        let locks = match self.data.locks.get_mut(&ptr.alloc_id) {\n-            Some(locks) => locks,\n-            // immutable static or other constant memory\n-            None => return Ok(()),\n-        };\n-\n-        for lock in locks.iter_mut(ptr.offset.bytes(), len) {\n-            // Check if we have a suspension here\n-            let (got_the_lock, remove_suspension) = match lock.suspended.get_mut(&lock_id) {\n-                None => {\n-                    trace!(\"No suspension around, we can just acquire\");\n-                    (true, false)\n-                }\n-                Some(suspensions) => {\n-                    trace!(\"Found suspension of {:?}, removing it\", lock_id);\n-                    // That's us!  Remove suspension (it should be in there).  The same suspension can\n-                    // occur multiple times (when there are multiple shared borrows of this that have the same\n-                    // lifetime); only remove one of them.\n-                    let idx = match suspensions.iter().enumerate().find(|&(_, re)| re == &suspended_region) {\n-                        None => // TODO: Can the user trigger this?\n-                            bug!(\"We have this lock suspended, but not for the given region.\"),\n-                        Some((idx, _)) => idx\n-                    };\n-                    suspensions.remove(idx);\n-                    let got_lock = suspensions.is_empty();\n-                    if got_lock {\n-                        trace!(\"All suspensions are gone, we can have the lock again\");\n-                    }\n-                    (got_lock, got_lock)\n-                }\n-            };\n-            if remove_suspension {\n-                // with NLL, we could do that up in the match above...\n-                assert!(got_the_lock);\n-                lock.suspended.remove(&lock_id);\n-            }\n-            if got_the_lock {\n-                match lock.active {\n-                    ref mut active @ NoLock => {\n-                        *active = WriteLock(\n-                            DynamicLifetime {\n-                                frame: cur_frame,\n-                                region: lock_region,\n-                            }\n-                        );\n-                    }\n-                    _ => {\n-                        return err!(MemoryAcquireConflict {\n-                            ptr,\n-                            len,\n-                            kind: AccessKind::Write,\n-                            lock: lock.active.clone(),\n-                        })\n-                    }\n-                }\n-            }\n-        }\n-\n-        Ok(())\n-    }\n-\n-    fn locks_lifetime_ended(&mut self, ending_region: Option<region::Scope>) {\n-        let cur_frame = self.cur_frame;\n-        trace!(\n-            \"Releasing frame {} locks that expire at {:?}\",\n-            cur_frame,\n-            ending_region\n-        );\n-        let has_ended = |lifetime: &DynamicLifetime| -> bool {\n-            if lifetime.frame != cur_frame {\n-                return false;\n-            }\n-            match ending_region {\n-                None => true, // When a function ends, we end *all* its locks. It's okay for a function to still have lifetime-related locks\n-                // when it returns, that can happen e.g. with NLL when a lifetime can, but does not have to, extend beyond the\n-                // end of a function.  Same for a function still having recoveries.\n-                Some(ending_region) => lifetime.region == Some(ending_region),\n-            }\n-        };\n-\n-        for alloc_locks in self.data.locks.values_mut() {\n-            for lock in alloc_locks.iter_mut_all() {\n-                // Delete everything that ends now -- i.e., keep only all the other lifetimes.\n-                let lock_ended = match lock.active {\n-                    WriteLock(ref lft) => has_ended(lft),\n-                    ReadLock(ref mut lfts) => {\n-                        lfts.retain(|lft| !has_ended(lft));\n-                        lfts.is_empty()\n-                    }\n-                    NoLock => false,\n-                };\n-                if lock_ended {\n-                    lock.active = NoLock;\n-                }\n-                // Also clean up suspended write locks when the function returns\n-                if ending_region.is_none() {\n-                    lock.suspended.retain(|id, _suspensions| id.frame != cur_frame);\n-                }\n-            }\n-            // Clean up the map\n-            alloc_locks.retain(|lock| match lock.active {\n-                NoLock => !lock.suspended.is_empty(),\n-                _ => true,\n-            });\n-        }\n-    }\n-}\n-\n impl<'tcx> RangeMap<LockInfo<'tcx>> {\n     pub fn check(\n         &self,"}, {"sha": "3ff38008abd0bd816f2877f291820141203f5b26", "filename": "src/operator.rs", "status": "modified", "additions": 62, "deletions": 21, "changes": 83, "blob_url": "https://github.com/rust-lang/rust/blob/259cc6e3dccdf7dd7f9c8e2052c6dea19fe066d9/src%2Foperator.rs", "raw_url": "https://github.com/rust-lang/rust/raw/259cc6e3dccdf7dd7f9c8e2052c6dea19fe066d9/src%2Foperator.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Foperator.rs?ref=259cc6e3dccdf7dd7f9c8e2052c6dea19fe066d9", "patch": "@@ -1,19 +1,17 @@\n-use rustc::ty;\n-use rustc::ty::layout::Primitive;\n+use rustc::ty::{self, Ty};\n+use rustc::ty::layout::{TyLayout, Primitive};\n use rustc::mir;\n \n use super::*;\n \n-use helpers::EvalContextExt as HelperEvalContextExt;\n-\n pub trait EvalContextExt<'tcx> {\n     fn ptr_op(\n         &self,\n         bin_op: mir::BinOp,\n         left: Scalar,\n-        left_ty: ty::Ty<'tcx>,\n+        left_layout: TyLayout<'tcx>,\n         right: Scalar,\n-        right_ty: ty::Ty<'tcx>,\n+        right_layout: TyLayout<'tcx>,\n     ) -> EvalResult<'tcx, Option<(Scalar, bool)>>;\n \n     fn ptr_int_arithmetic(\n@@ -23,16 +21,23 @@ pub trait EvalContextExt<'tcx> {\n         right: i128,\n         signed: bool,\n     ) -> EvalResult<'tcx, (Scalar, bool)>;\n+\n+    fn pointer_offset_inbounds(\n+        &self,\n+        ptr: Scalar,\n+        pointee_ty: Ty<'tcx>,\n+        offset: i64,\n+    ) -> EvalResult<'tcx, Scalar>;\n }\n \n impl<'a, 'mir, 'tcx> EvalContextExt<'tcx> for EvalContext<'a, 'mir, 'tcx, super::Evaluator<'tcx>> {\n     fn ptr_op(\n         &self,\n         bin_op: mir::BinOp,\n         left: Scalar,\n-        left_ty: ty::Ty<'tcx>,\n+        left_layout: TyLayout<'tcx>,\n         right: Scalar,\n-        right_ty: ty::Ty<'tcx>,\n+        right_layout: TyLayout<'tcx>,\n     ) -> EvalResult<'tcx, Option<(Scalar, bool)>> {\n         trace!(\"ptr_op: {:?} {:?} {:?}\", left, bin_op, right);\n \n@@ -45,32 +50,31 @@ impl<'a, 'mir, 'tcx> EvalContextExt<'tcx> for EvalContext<'a, 'mir, 'tcx, super:\n             8 => I64,\n             16 => I128,\n             _ => unreachable!(),\n-        }, false);\n+        }, /*signed*/ false);\n         let isize = Primitive::Int(match self.memory.pointer_size().bytes() {\n             1 => I8,\n             2 => I16,\n             4 => I32,\n             8 => I64,\n             16 => I128,\n             _ => unreachable!(),\n-        }, true);\n-        let left_layout = self.layout_of(left_ty)?;\n+        }, /*signed*/ true);\n         let left_kind = match left_layout.abi {\n             ty::layout::Abi::Scalar(ref scalar) => scalar.value,\n-            _ => Err(EvalErrorKind::TypeNotPrimitive(left_ty))?,\n+            _ => Err(EvalErrorKind::TypeNotPrimitive(left_layout.ty))?,\n         };\n-        let right_layout = self.layout_of(right_ty)?;\n         let right_kind = match right_layout.abi {\n             ty::layout::Abi::Scalar(ref scalar) => scalar.value,\n-            _ => Err(EvalErrorKind::TypeNotPrimitive(right_ty))?,\n+            _ => Err(EvalErrorKind::TypeNotPrimitive(right_layout.ty))?,\n         };\n         match bin_op {\n-            Offset if left_kind == Primitive::Pointer && right_kind == usize => {\n-                let pointee_ty = left_ty\n+            Offset => {\n+                assert!(left_kind == Primitive::Pointer && right_kind == usize);\n+                let pointee_ty = left_layout.ty\n                     .builtin_deref(true)\n                     .expect(\"Offset called on non-ptr type\")\n                     .ty;\n-                let ptr = self.pointer_offset(\n+                let ptr = self.pointer_offset_inbounds(\n                     left,\n                     pointee_ty,\n                     right.to_bits(self.memory.pointer_size())? as i64,\n@@ -114,12 +118,13 @@ impl<'a, 'mir, 'tcx> EvalContextExt<'tcx> for EvalContext<'a, 'mir, 'tcx, super:\n                         Gt => left.offset > right.offset,\n                         Ge => left.offset >= right.offset,\n                         Sub => {\n+                            let left_offset = Scalar::from_uint(left.offset.bytes(), self.memory.pointer_size());\n+                            let right_offset = Scalar::from_uint(right.offset.bytes(), self.memory.pointer_size());\n+                            let layout = self.layout_of(self.tcx.types.usize)?;\n                             return self.binary_op(\n                                 Sub,\n-                                Scalar::Bits { bits: left.offset.bytes() as u128, size: self.memory.pointer_size().bytes() as u8 },\n-                                self.tcx.types.usize,\n-                                Scalar::Bits { bits: right.offset.bytes() as u128, size: self.memory.pointer_size().bytes() as u8 },\n-                                self.tcx.types.usize,\n+                                ValTy { value: Value::Scalar(left_offset.into()), layout },\n+                                ValTy { value: Value::Scalar(right_offset.into()), layout },\n                             ).map(Some)\n                         }\n                         _ => bug!(\"We already established it has to be one of these operators.\"),\n@@ -200,4 +205,40 @@ impl<'a, 'mir, 'tcx> EvalContextExt<'tcx> for EvalContext<'a, 'mir, 'tcx, super:\n             }\n         })\n     }\n+\n+    /// This function raises an error if the offset moves the pointer outside of its allocation.  We consider\n+    /// ZSTs their own huge allocation that doesn't overlap with anything (and nothing moves in there because the size is 0).\n+    /// We also consider the NULL pointer its own separate allocation, and all the remaining integers pointers their own\n+    /// allocation.\n+    fn pointer_offset_inbounds(\n+        &self,\n+        ptr: Scalar,\n+        pointee_ty: Ty<'tcx>,\n+        offset: i64,\n+    ) -> EvalResult<'tcx, Scalar> {\n+        if ptr.is_null() {\n+            // NULL pointers must only be offset by 0\n+            return if offset == 0 {\n+                Ok(ptr)\n+            } else {\n+                err!(InvalidNullPointerUsage)\n+            };\n+        }\n+        // FIXME: assuming here that type size is < i64::max_value()\n+        let pointee_size = self.layout_of(pointee_ty)?.size.bytes() as i64;\n+        let offset = offset.checked_mul(pointee_size).ok_or_else(|| EvalErrorKind::Overflow(mir::BinOp::Mul))?;\n+        // Now let's see what kind of pointer this is\n+        if let Scalar::Ptr(ptr) = ptr {\n+            // Both old and new pointer must be in-bounds.\n+            // (Of the same allocation, but that part is trivial with our representation.)\n+            self.memory.check_bounds(ptr, false)?;\n+            let ptr = ptr.signed_offset(offset, self)?;\n+            self.memory.check_bounds(ptr, false)?;\n+            Ok(Scalar::Ptr(ptr))\n+        } else {\n+            // An integer pointer. They can move around freely, as long as they do not overflow\n+            // (which ptr_signed_offset checks).\n+            ptr.ptr_signed_offset(offset, self)\n+        }\n+    }\n }"}, {"sha": "e55534e36fd2ed08708b077df16224a24a4f1b8e", "filename": "src/range_map.rs", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/259cc6e3dccdf7dd7f9c8e2052c6dea19fe066d9/src%2Frange_map.rs", "raw_url": "https://github.com/rust-lang/rust/raw/259cc6e3dccdf7dd7f9c8e2052c6dea19fe066d9/src%2Frange_map.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Frange_map.rs?ref=259cc6e3dccdf7dd7f9c8e2052c6dea19fe066d9", "patch": "@@ -1,3 +1,5 @@\n+#![allow(unused)]\n+\n //! Implements a map from integer indices to data.\n //! Rather than storing data for every index, internally, this maps entire ranges to the data.\n //! To this end, the APIs all work on ranges, not on individual integers. Ranges are split as"}, {"sha": "878884065bb71e0285a63fa147aabdefcd77ae66", "filename": "src/tls.rs", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "blob_url": "https://github.com/rust-lang/rust/blob/259cc6e3dccdf7dd7f9c8e2052c6dea19fe066d9/src%2Ftls.rs", "raw_url": "https://github.com/rust-lang/rust/raw/259cc6e3dccdf7dd7f9c8e2052c6dea19fe066d9/src%2Ftls.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ftls.rs?ref=259cc6e3dccdf7dd7f9c8e2052c6dea19fe066d9", "patch": "@@ -119,19 +119,19 @@ impl<'a, 'mir, 'tcx: 'mir + 'a> EvalContextExt<'tcx> for EvalContext<'a, 'mir, '\n             // TODO: Potentially, this has to support all the other possible instances?\n             // See eval_fn_call in interpret/terminator/mod.rs\n             let mir = self.load_mir(instance.def)?;\n+            let ret = Place::null(&self);\n             self.push_stack_frame(\n                 instance,\n                 mir.span,\n                 mir,\n-                Place::undef(),\n+                ret,\n                 StackPopCleanup::None,\n             )?;\n             let arg_local = self.frame().mir.args_iter().next().ok_or_else(\n                 || EvalErrorKind::AbiViolation(\"TLS dtor does not take enough arguments.\".to_owned()),\n             )?;\n             let dest = self.eval_place(&mir::Place::Local(arg_local))?;\n-            let ty = self.tcx.mk_mut_ptr(self.tcx.types.u8);\n-            self.write_ptr(dest, ptr, ty)?;\n+            self.write_scalar(ptr, dest)?;\n \n             // step until out of stackframes\n             while self.step()? {}"}, {"sha": "7f0abb9ae0bad40f78f1e6993965644c7b30c129", "filename": "src/validation.rs", "status": "removed", "additions": 0, "deletions": 803, "changes": 803, "blob_url": "https://github.com/rust-lang/rust/blob/65357faef8b926a7fc61ceaa2e4db79d431c80fb/src%2Fvalidation.rs", "raw_url": "https://github.com/rust-lang/rust/raw/65357faef8b926a7fc61ceaa2e4db79d431c80fb/src%2Fvalidation.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Fvalidation.rs?ref=65357faef8b926a7fc61ceaa2e4db79d431c80fb", "patch": "@@ -1,803 +0,0 @@\n-use rustc::hir::{self, Mutability};\n-use rustc::hir::Mutability::*;\n-use rustc::mir::{self, ValidationOp, ValidationOperand};\n-use rustc::mir::interpret::GlobalId;\n-use rustc::ty::{self, Ty, TypeFoldable, TyCtxt, Instance};\n-use rustc::ty::layout::{LayoutOf, PrimitiveExt};\n-use rustc::ty::subst::{Substs, Subst};\n-use rustc::traits::{self, TraitEngine};\n-use rustc::infer::InferCtxt;\n-use rustc::middle::region;\n-use rustc::mir::interpret::{ConstValue};\n-use rustc_data_structures::indexed_vec::Idx;\n-use rustc_mir::interpret::HasMemory;\n-\n-use super::{EvalContext, Place, PlaceExtra, ValTy, ScalarExt};\n-use rustc::mir::interpret::{DynamicLifetime, AccessKind, EvalErrorKind, Value, EvalError, EvalResult};\n-use locks::MemoryExt;\n-\n-pub type ValidationQuery<'tcx> = ValidationOperand<'tcx, (AbsPlace<'tcx>, Place)>;\n-\n-#[derive(Copy, Clone, Debug, PartialEq)]\n-pub(crate) enum ValidationMode {\n-    Acquire,\n-    /// Recover because the given region ended\n-    Recover(region::Scope),\n-    ReleaseUntil(Option<region::Scope>),\n-}\n-\n-impl ValidationMode {\n-    fn acquiring(self) -> bool {\n-        use self::ValidationMode::*;\n-        match self {\n-            Acquire | Recover(_) => true,\n-            ReleaseUntil(_) => false,\n-        }\n-    }\n-}\n-\n-// Abstract places\n-#[derive(Clone, Debug, PartialEq, Eq, Hash)]\n-pub enum AbsPlace<'tcx> {\n-    Local(mir::Local),\n-    Static(hir::def_id::DefId),\n-    Projection(Box<AbsPlaceProjection<'tcx>>),\n-}\n-\n-type AbsPlaceProjection<'tcx> = mir::Projection<'tcx, AbsPlace<'tcx>, u64, ()>;\n-type AbsPlaceElem<'tcx> = mir::ProjectionElem<'tcx, u64, ()>;\n-\n-impl<'tcx> AbsPlace<'tcx> {\n-    pub fn field(self, f: mir::Field) -> AbsPlace<'tcx> {\n-        self.elem(mir::ProjectionElem::Field(f, ()))\n-    }\n-\n-    pub fn deref(self) -> AbsPlace<'tcx> {\n-        self.elem(mir::ProjectionElem::Deref)\n-    }\n-\n-    pub fn downcast(self, adt_def: &'tcx ty::AdtDef, variant_index: usize) -> AbsPlace<'tcx> {\n-        self.elem(mir::ProjectionElem::Downcast(adt_def, variant_index))\n-    }\n-\n-    pub fn index(self, index: u64) -> AbsPlace<'tcx> {\n-        self.elem(mir::ProjectionElem::Index(index))\n-    }\n-\n-    fn elem(self, elem: AbsPlaceElem<'tcx>) -> AbsPlace<'tcx> {\n-        AbsPlace::Projection(Box::new(AbsPlaceProjection {\n-            base: self,\n-            elem,\n-        }))\n-    }\n-}\n-\n-pub(crate) trait EvalContextExt<'tcx> {\n-    fn abstract_place_projection(&self, proj: &mir::PlaceProjection<'tcx>) -> EvalResult<'tcx, AbsPlaceProjection<'tcx>>;\n-    fn abstract_place(&self, place: &mir::Place<'tcx>) -> EvalResult<'tcx, AbsPlace<'tcx>>;\n-    fn validation_op(\n-        &mut self,\n-        op: ValidationOp,\n-        operand: &ValidationOperand<'tcx, mir::Place<'tcx>>,\n-    ) -> EvalResult<'tcx>;\n-    fn end_region(&mut self, scope: Option<region::Scope>) -> EvalResult<'tcx>;\n-    fn normalize_type_unerased(&self, ty: Ty<'tcx>) -> Ty<'tcx>;\n-    fn field_with_lifetimes(\n-        &mut self,\n-        base: Place,\n-        layout: ty::layout::TyLayout<'tcx>,\n-        i: usize,\n-    ) -> EvalResult<'tcx, Ty<'tcx>>;\n-    fn validate_fields(\n-        &mut self,\n-        query: ValidationQuery<'tcx>,\n-        mode: ValidationMode,\n-    ) -> EvalResult<'tcx>;\n-    fn validate_ptr(\n-        &mut self,\n-        val: Value,\n-        abs_place: AbsPlace<'tcx>,\n-        pointee_ty: Ty<'tcx>,\n-        re: Option<region::Scope>,\n-        mutbl: Mutability,\n-        mode: ValidationMode,\n-    ) -> EvalResult<'tcx>;\n-    fn validate(\n-        &mut self,\n-        query: ValidationQuery<'tcx>,\n-        mode: ValidationMode,\n-    ) -> EvalResult<'tcx>;\n-}\n-\n-impl<'a, 'mir, 'tcx: 'mir + 'a> EvalContextExt<'tcx> for EvalContext<'a, 'mir, 'tcx, super::Evaluator<'tcx>> {\n-    fn abstract_place_projection(&self, proj: &mir::PlaceProjection<'tcx>) -> EvalResult<'tcx, AbsPlaceProjection<'tcx>> {\n-        use self::mir::ProjectionElem::*;\n-\n-        let elem = match proj.elem {\n-            Deref => Deref,\n-            Field(f, _) => Field(f, ()),\n-            Index(v) => {\n-                let value = self.frame().locals[v].access()?;\n-                let ty = self.tcx.tcx.types.usize;\n-                let n = self.value_to_scalar(ValTy { value, ty })?.to_usize(self)?;\n-                Index(n)\n-            },\n-            ConstantIndex { offset, min_length, from_end } =>\n-                ConstantIndex { offset, min_length, from_end },\n-            Subslice { from, to } =>\n-                Subslice { from, to },\n-            Downcast(adt, sz) => Downcast(adt, sz),\n-        };\n-        Ok(AbsPlaceProjection {\n-            base: self.abstract_place(&proj.base)?,\n-            elem\n-        })\n-    }\n-\n-    fn abstract_place(&self, place: &mir::Place<'tcx>) -> EvalResult<'tcx, AbsPlace<'tcx>> {\n-        Ok(match *place {\n-            mir::Place::Local(l) => AbsPlace::Local(l),\n-            mir::Place::Static(ref s) => AbsPlace::Static(s.def_id),\n-            mir::Place::Projection(ref p) =>\n-                AbsPlace::Projection(Box::new(self.abstract_place_projection(&*p)?)),\n-            _ => unimplemented!(\"validation is not currently maintained\"),\n-        })\n-    }\n-\n-    // Validity checks\n-    fn validation_op(\n-        &mut self,\n-        op: ValidationOp,\n-        operand: &ValidationOperand<'tcx, mir::Place<'tcx>>,\n-    ) -> EvalResult<'tcx> {\n-        // If mir-emit-validate is set to 0 (i.e., disabled), we may still see validation commands\n-        // because other crates may have been compiled with mir-emit-validate > 0.  Ignore those\n-        // commands.  This makes mir-emit-validate also a flag to control whether miri will do\n-        // validation or not.\n-        if self.tcx.tcx.sess.opts.debugging_opts.mir_emit_validate == 0 {\n-            return Ok(());\n-        }\n-        debug_assert!(self.memory.cur_frame == self.cur_frame());\n-\n-        // We need to monomorphize ty *without* erasing lifetimes\n-        trace!(\"validation_op1: {:?}\", operand.ty.sty);\n-        let ty = operand.ty.subst(self.tcx.tcx, self.substs());\n-        trace!(\"validation_op2: {:?}\", operand.ty.sty);\n-        let place = self.eval_place(&operand.place)?;\n-        let abs_place = self.abstract_place(&operand.place)?;\n-        let query = ValidationQuery {\n-            place: (abs_place, place),\n-            ty,\n-            re: operand.re,\n-            mutbl: operand.mutbl,\n-        };\n-\n-        // Check the mode, and also perform mode-specific operations\n-        let mode = match op {\n-            ValidationOp::Acquire => ValidationMode::Acquire,\n-            ValidationOp::Release => ValidationMode::ReleaseUntil(None),\n-            ValidationOp::Suspend(scope) => {\n-                if query.mutbl == MutMutable {\n-                    let lft = DynamicLifetime {\n-                        frame: self.cur_frame(),\n-                        region: Some(scope), // Notably, we only ever suspend things for given regions.\n-                        // Suspending for the entire function does not make any sense.\n-                    };\n-                    trace!(\"Suspending {:?} until {:?}\", query, scope);\n-                    self.machine.suspended.entry(lft).or_insert_with(Vec::new).push(\n-                        query.clone(),\n-                    );\n-                }\n-                ValidationMode::ReleaseUntil(Some(scope))\n-            }\n-        };\n-        self.validate(query, mode)\n-    }\n-\n-    /// Release locks and executes suspensions of the given region (or the entire fn, in case of None).\n-    fn end_region(&mut self, scope: Option<region::Scope>) -> EvalResult<'tcx> {\n-        debug_assert!(self.memory.cur_frame == self.cur_frame());\n-        self.memory.locks_lifetime_ended(scope);\n-        match scope {\n-            Some(scope) => {\n-                // Recover suspended places\n-                let lft = DynamicLifetime {\n-                    frame: self.cur_frame(),\n-                    region: Some(scope),\n-                };\n-                if let Some(queries) = self.machine.suspended.remove(&lft) {\n-                    for query in queries {\n-                        trace!(\"Recovering {:?} from suspension\", query);\n-                        self.validate(query, ValidationMode::Recover(scope))?;\n-                    }\n-                }\n-            }\n-            None => {\n-                // Clean suspension table of current frame\n-                let cur_frame = self.cur_frame();\n-                self.machine.suspended.retain(|lft, _| {\n-                    lft.frame != cur_frame // keep only what is in the other (lower) frames\n-                });\n-            }\n-        }\n-        Ok(())\n-    }\n-\n-    fn normalize_type_unerased(&self, ty: Ty<'tcx>) -> Ty<'tcx> {\n-        return normalize_associated_type(self.tcx.tcx, &ty);\n-\n-        use syntax::codemap::{Span, DUMMY_SP};\n-\n-        // We copy a bunch of stuff from rustc/infer/mod.rs to be able to tweak its behavior\n-        fn normalize_projections_in<'a, 'gcx, 'tcx, T>(\n-            self_: &InferCtxt<'a, 'gcx, 'tcx>,\n-            param_env: ty::ParamEnv<'tcx>,\n-            value: &T,\n-        ) -> T::Lifted\n-        where\n-            T: TypeFoldable<'tcx> + ty::Lift<'gcx>,\n-        {\n-            let mut selcx = traits::SelectionContext::new(self_);\n-            let cause = traits::ObligationCause::dummy();\n-            let traits::Normalized {\n-                value: result,\n-                obligations,\n-            } = traits::normalize(&mut selcx, param_env, cause, value);\n-\n-            let mut fulfill_cx = traits::FulfillmentContext::new();\n-\n-            for obligation in obligations {\n-                fulfill_cx.register_predicate_obligation(self_, obligation);\n-            }\n-\n-            drain_fulfillment_cx_or_panic(self_, DUMMY_SP, &mut fulfill_cx, &result)\n-        }\n-\n-        fn drain_fulfillment_cx_or_panic<'a, 'gcx, 'tcx, T>(\n-            self_: &InferCtxt<'a, 'gcx, 'tcx>,\n-            span: Span,\n-            fulfill_cx: &mut traits::FulfillmentContext<'tcx>,\n-            result: &T,\n-        ) -> T::Lifted\n-        where\n-            T: TypeFoldable<'tcx> + ty::Lift<'gcx>,\n-        {\n-            // In principle, we only need to do this so long as `result`\n-            // contains unbound type parameters. It could be a slight\n-            // optimization to stop iterating early.\n-            match fulfill_cx.select_all_or_error(self_) {\n-                Ok(()) => { }\n-                Err(errors) => {\n-                    span_bug!(\n-                        span,\n-                        \"Encountered errors `{:?}` resolving bounds after type-checking\",\n-                        errors\n-                    );\n-                }\n-            }\n-\n-            let result = self_.resolve_type_vars_if_possible(result);\n-            let result = self_.tcx.fold_regions(\n-                &result,\n-                &mut false,\n-                |r, _| match *r {\n-                    ty::ReVar(_) => self_.tcx.types.re_erased,\n-                    _ => r,\n-                },\n-            );\n-\n-            match self_.tcx.lift_to_global(&result) {\n-                Some(result) => result,\n-                None => {\n-                    span_bug!(span, \"Uninferred types/regions in `{:?}`\", result);\n-                }\n-            }\n-        }\n-\n-        trait MyTransNormalize<'gcx>: TypeFoldable<'gcx> {\n-            fn my_trans_normalize<'a, 'tcx>(\n-                &self,\n-                infcx: &InferCtxt<'a, 'gcx, 'tcx>,\n-                param_env: ty::ParamEnv<'tcx>,\n-            ) -> Self;\n-        }\n-\n-        macro_rules! items { ($($item:item)+) => ($($item)+) }\n-        macro_rules! impl_trans_normalize {\n-            ($lt_gcx:tt, $($ty:ty),+) => {\n-                items!($(impl<$lt_gcx> MyTransNormalize<$lt_gcx> for $ty {\n-                    fn my_trans_normalize<'a, 'tcx>(&self,\n-                                                infcx: &InferCtxt<'a, $lt_gcx, 'tcx>,\n-                                                param_env: ty::ParamEnv<'tcx>)\n-                                                -> Self {\n-                        normalize_projections_in(infcx, param_env, self)\n-                    }\n-                })+);\n-            }\n-        }\n-\n-        impl_trans_normalize!('gcx,\n-            Ty<'gcx>,\n-            &'gcx Substs<'gcx>,\n-            ty::FnSig<'gcx>,\n-            ty::PolyFnSig<'gcx>,\n-            ty::ClosureSubsts<'gcx>,\n-            ty::PolyTraitRef<'gcx>,\n-            ty::ExistentialTraitRef<'gcx>\n-        );\n-\n-        fn normalize_associated_type<'a, 'tcx, T>(self_: TyCtxt<'a, 'tcx, 'tcx>, value: &T) -> T\n-        where\n-            T: MyTransNormalize<'tcx>,\n-        {\n-            let param_env = ty::ParamEnv::reveal_all();\n-\n-            if !value.has_projections() {\n-                return value.clone();\n-            }\n-\n-            self_.infer_ctxt().enter(|infcx| {\n-                value.my_trans_normalize(&infcx, param_env)\n-            })\n-        }\n-    }\n-\n-    // This is a copy of `Layout::field`\n-    //\n-    // FIXME: remove once validation does not depend on lifetimes\n-    fn field_with_lifetimes(\n-        &mut self,\n-        base: Place,\n-        mut layout: ty::layout::TyLayout<'tcx>,\n-        i: usize,\n-    ) -> EvalResult<'tcx, Ty<'tcx>> {\n-        if let Place::Ptr { extra: PlaceExtra::DowncastVariant(variant_index), .. } = base {\n-            layout = layout.for_variant(&self, variant_index);\n-        }\n-        let tcx = self.tcx.tcx;\n-        Ok(match layout.ty.sty {\n-            ty::TyBool |\n-            ty::TyChar |\n-            ty::TyInt(_) |\n-            ty::TyUint(_) |\n-            ty::TyFloat(_) |\n-            ty::TyFnPtr(_) |\n-            ty::TyNever |\n-            ty::TyFnDef(..) |\n-            ty::TyGeneratorWitness(..) |\n-            ty::TyDynamic(..) |\n-            ty::TyForeign(..) => {\n-                bug!(\"TyLayout::field_type({:?}): not applicable\", layout)\n-            }\n-\n-            // Potentially-fat pointers.\n-            ty::TyRef(_, pointee, _) |\n-            ty::TyRawPtr(ty::TypeAndMut { ty: pointee, .. }) => {\n-                assert!(i < 2);\n-\n-                // Reuse the fat *T type as its own thin pointer data field.\n-                // This provides information about e.g. DST struct pointees\n-                // (which may have no non-DST form), and will work as long\n-                // as the `Abi` or `FieldPlacement` is checked by users.\n-                if i == 0 {\n-                    return Ok(layout.ty);\n-                }\n-\n-                match tcx.struct_tail(pointee).sty {\n-                    ty::TySlice(_) |\n-                    ty::TyStr => tcx.types.usize,\n-                    ty::TyDynamic(..) => {\n-                        // FIXME(eddyb) use an usize/fn() array with\n-                        // the correct number of vtables slots.\n-                        tcx.mk_imm_ref(tcx.types.re_static, tcx.mk_nil())\n-                    }\n-                    _ => bug!(\"TyLayout::field_type({:?}): not applicable\", layout)\n-                }\n-            }\n-\n-            // Arrays and slices.\n-            ty::TyArray(element, _) |\n-            ty::TySlice(element) => element,\n-            ty::TyStr => tcx.types.u8,\n-\n-            // Tuples, generators and closures.\n-            ty::TyClosure(def_id, ref substs) => {\n-                substs.upvar_tys(def_id, tcx).nth(i).unwrap()\n-            }\n-\n-            ty::TyGenerator(def_id, ref substs, _) => {\n-                substs.field_tys(def_id, tcx).nth(i).unwrap()\n-            }\n-\n-            ty::TyTuple(tys) => tys[i],\n-\n-            // SIMD vector types.\n-            ty::TyAdt(def, ..) if def.repr.simd() => {\n-                layout.ty.simd_type(tcx)\n-            }\n-\n-            // ADTs.\n-            ty::TyAdt(def, substs) => {\n-                use rustc::ty::layout::Variants;\n-                match layout.variants {\n-                    Variants::Single { index } => {\n-                        def.variants[index].fields[i].ty(tcx, substs)\n-                    }\n-\n-                    // Discriminant field for enums (where applicable).\n-                    Variants::Tagged { tag: ref discr, .. } |\n-                    Variants::NicheFilling { niche: ref discr, .. } => {\n-                        assert_eq!(i, 0);\n-                        return Ok(discr.value.to_ty(tcx))\n-                    }\n-                }\n-            }\n-\n-            ty::TyProjection(_) | ty::TyAnon(..) | ty::TyParam(_) |\n-            ty::TyInfer(_) | ty::TyError => {\n-                bug!(\"TyLayout::field_type: unexpected type `{}`\", layout.ty)\n-            }\n-        })\n-    }\n-\n-    fn validate_fields(\n-        &mut self,\n-        query: ValidationQuery<'tcx>,\n-        mode: ValidationMode,\n-    ) -> EvalResult<'tcx> {\n-        let mut layout = self.layout_of(query.ty)?;\n-        layout.ty = query.ty;\n-\n-        // TODO: Maybe take visibility/privacy into account.\n-        for idx in 0..layout.fields.count() {\n-            let field = mir::Field::new(idx);\n-            let (field_place, field_layout) =\n-                self.place_field(query.place.1, field, layout)?;\n-            // layout stuff erases lifetimes, get the field ourselves\n-            let field_ty = self.field_with_lifetimes(query.place.1, layout, idx)?;\n-            trace!(\"assuming \\n{:?}\\n == \\n{:?}\\n except for lifetimes\", field_layout.ty, field_ty);\n-            self.validate(\n-                ValidationQuery {\n-                    place: (query.place.0.clone().field(field), field_place),\n-                    ty: field_ty,\n-                    ..query\n-                },\n-                mode,\n-            )?;\n-        }\n-\n-        Ok(())\n-    }\n-\n-    fn validate_ptr(\n-        &mut self,\n-        val: Value,\n-        abs_place: AbsPlace<'tcx>,\n-        pointee_ty: Ty<'tcx>,\n-        re: Option<region::Scope>,\n-        mutbl: Mutability,\n-        mode: ValidationMode,\n-    ) -> EvalResult<'tcx> {\n-        // Check alignment and non-NULLness\n-        let (_, align) = self.size_and_align_of_dst(pointee_ty, val)?;\n-        let ptr = self.into_ptr(val)?.unwrap_or_err()?;\n-        self.memory.check_align(ptr, align)?;\n-\n-        // Recurse\n-        let pointee_place = self.val_to_place(val, pointee_ty)?;\n-        self.validate(\n-            ValidationQuery {\n-                place: (abs_place.deref(), pointee_place),\n-                ty: pointee_ty,\n-                re,\n-                mutbl,\n-            },\n-            mode,\n-        )\n-    }\n-\n-    /// Validate the place at the given type. If `acquire` is false, just do a release of all write locks\n-    fn validate(\n-        &mut self,\n-        mut query: ValidationQuery<'tcx>,\n-        mode: ValidationMode,\n-    ) -> EvalResult<'tcx> {\n-        use rustc::ty::TypeVariants::*;\n-        use rustc::ty::RegionKind::*;\n-        use rustc::ty::AdtKind;\n-\n-        // No point releasing shared stuff.\n-        if !mode.acquiring() && query.mutbl == MutImmutable {\n-            return Ok(());\n-        }\n-        // When we recover, we may see data whose validity *just* ended.  Do not acquire it.\n-        if let ValidationMode::Recover(ending_ce) = mode {\n-            if query.re == Some(ending_ce) {\n-                return Ok(());\n-            }\n-        }\n-\n-        query.ty = self.normalize_type_unerased(&query.ty);\n-        trace!(\"{:?} on {:#?}\", mode, query);\n-        trace!(\"{:#?}\", query.ty.sty);\n-\n-        // Decide whether this type *owns* the memory it covers (like integers), or whether it\n-        // just assembles pieces (that each own their memory) together to a larger whole.\n-        // TODO: Currently, we don't acquire locks for padding and discriminants. We should.\n-        let is_owning = match query.ty.sty {\n-            TyInt(_) | TyUint(_) | TyRawPtr(_) | TyBool | TyFloat(_) | TyChar | TyStr |\n-            TyRef(..) | TyFnPtr(..) | TyFnDef(..) | TyNever => true,\n-            TyAdt(adt, _) if adt.is_box() => true,\n-            TySlice(_) | TyAdt(_, _) | TyTuple(..) | TyClosure(..) | TyArray(..) |\n-            TyDynamic(..) | TyGenerator(..) | TyForeign(_) => false,\n-            TyGeneratorWitness(..) => unreachable!(\"TyGeneratorWitness in validate\"),\n-            TyParam(_) | TyInfer(_) | TyProjection(_) | TyAnon(..) | TyError => {\n-                bug!(\"I got an incomplete/unnormalized type for validation\")\n-            }\n-        };\n-        if is_owning {\n-            // We need to lock.  So we need memory.  So we have to force_acquire.\n-            // Tracking the same state for locals not backed by memory would just duplicate too\n-            // much machinery.\n-            // FIXME: We ignore alignment.\n-            let (ptr, _, extra) = self.force_allocation(query.place.1)?.to_ptr_align_extra();\n-            // Determine the size\n-            // FIXME: Can we reuse size_and_align_of_dst for Places?\n-            let layout = self.layout_of(query.ty)?;\n-            let len = if !layout.is_unsized() {\n-                assert_eq!(extra, PlaceExtra::None, \"Got a fat ptr to a sized type\");\n-                layout.size.bytes()\n-            } else {\n-                // The only unsized typ we concider \"owning\" is TyStr.\n-                assert_eq!(\n-                    query.ty.sty,\n-                    TyStr,\n-                    \"Found a surprising unsized owning type\"\n-                );\n-                // The extra must be the length, in bytes.\n-                match extra {\n-                    PlaceExtra::Length(len) => len,\n-                    _ => bug!(\"TyStr must have a length as extra\"),\n-                }\n-            };\n-            // Handle locking\n-            if len > 0 {\n-                let ptr = ptr.unwrap_or_err()?.to_ptr()?;\n-                match query.mutbl {\n-                    MutImmutable => {\n-                        if mode.acquiring() {\n-                            self.memory.acquire_lock(\n-                                ptr,\n-                                len,\n-                                query.re,\n-                                AccessKind::Read,\n-                            )?;\n-                        }\n-                    }\n-                    // No releasing of read locks, ever.\n-                    MutMutable => {\n-                        match mode {\n-                            ValidationMode::Acquire => {\n-                                self.memory.acquire_lock(\n-                                    ptr,\n-                                    len,\n-                                    query.re,\n-                                    AccessKind::Write,\n-                                )?\n-                            }\n-                            ValidationMode::Recover(ending_ce) => {\n-                                self.memory.recover_write_lock(\n-                                    ptr,\n-                                    len,\n-                                    &query.place.0,\n-                                    query.re,\n-                                    ending_ce,\n-                                )?\n-                            }\n-                            ValidationMode::ReleaseUntil(suspended_ce) => {\n-                                self.memory.suspend_write_lock(\n-                                    ptr,\n-                                    len,\n-                                    &query.place.0,\n-                                    suspended_ce,\n-                                )?\n-                            }\n-                        }\n-                    }\n-                }\n-            }\n-        }\n-\n-        let res: EvalResult<'tcx> = do catch {\n-            match query.ty.sty {\n-                TyInt(_) | TyUint(_) | TyRawPtr(_) => {\n-                    if mode.acquiring() {\n-                        // Make sure we can read this.\n-                        let val = self.read_place(query.place.1)?;\n-                        self.follow_by_ref_value(val, query.ty)?;\n-                        // FIXME: It would be great to rule out Undef here, but that doesn't actually work.\n-                        // Passing around undef data is a thing that e.g. Vec::extend_with does.\n-                    }\n-                }\n-                TyBool | TyFloat(_) | TyChar => {\n-                    if mode.acquiring() {\n-                        let val = self.read_place(query.place.1)?;\n-                        let val = self.value_to_scalar(ValTy { value: val, ty: query.ty })?;\n-                        val.to_bytes()?;\n-                        // TODO: Check if these are valid bool/float/codepoint/UTF-8\n-                    }\n-                }\n-                TyNever => return err!(ValidationFailure(format!(\"The empty type is never valid.\"))),\n-                TyRef(region, pointee_ty, mutbl) => {\n-                    let val = self.read_place(query.place.1)?;\n-                    // Sharing restricts our context\n-                    if mutbl == MutImmutable {\n-                        query.mutbl = MutImmutable;\n-                    }\n-                    // Inner lifetimes *outlive* outer ones, so only if we have no lifetime restriction yet,\n-                    // we record the region of this borrow to the context.\n-                    if query.re == None {\n-                        if let ReScope(scope) = *region {\n-                            query.re = Some(scope);\n-                        }\n-                        // It is possible for us to encounter erased lifetimes here because the lifetimes in\n-                        // this functions' Subst will be erased.\n-                    }\n-                    self.validate_ptr(val, query.place.0, pointee_ty, query.re, query.mutbl, mode)?;\n-                }\n-                TyAdt(adt, _) if adt.is_box() => {\n-                    let val = self.read_place(query.place.1)?;\n-                    self.validate_ptr(val, query.place.0, query.ty.boxed_ty(), query.re, query.mutbl, mode)?;\n-                }\n-                TyFnPtr(_sig) => {\n-                    let ptr = self.read_place(query.place.1)?;\n-                    let ptr = self.into_ptr(ptr)?.unwrap_or_err()?.to_ptr()?;\n-                    self.memory.get_fn(ptr)?;\n-                    // TODO: Check if the signature matches (should be the same check as what terminator/mod.rs already does on call?).\n-                }\n-                TyFnDef(..) => {\n-                    // This is a zero-sized type with all relevant data sitting in the type.\n-                    // There is nothing to validate.\n-                }\n-\n-                // Compound types\n-                TyStr => {\n-                    // TODO: Validate strings\n-                }\n-                TySlice(elem_ty) => {\n-                    let len = match query.place.1 {\n-                        Place::Ptr { extra: PlaceExtra::Length(len), .. } => len,\n-                        _ => {\n-                            bug!(\n-                                \"acquire_valid of a TySlice given non-slice place: {:?}\",\n-                                query.place\n-                            )\n-                        }\n-                    };\n-                    for i in 0..len {\n-                        let inner_place = self.place_index(query.place.1, query.ty, i)?;\n-                        self.validate(\n-                            ValidationQuery {\n-                                place: (query.place.0.clone().index(i), inner_place),\n-                                ty: elem_ty,\n-                                ..query\n-                            },\n-                            mode,\n-                        )?;\n-                    }\n-                }\n-                TyArray(elem_ty, len) => {\n-                    let len = match len.val {\n-                        ConstValue::Unevaluated(def_id, substs) => {\n-                            self.tcx.const_eval(self.tcx.param_env(def_id).and(GlobalId {\n-                                instance: Instance::new(def_id, substs),\n-                                promoted: None,\n-                            }))\n-                                .map_err(|_err|EvalErrorKind::MachineError(\"<already reported>\".to_string()))?\n-                        }\n-                        _ => len,\n-                    };\n-                    let len = len.unwrap_usize(self.tcx.tcx);\n-                    for i in 0..len {\n-                        let inner_place = self.place_index(query.place.1, query.ty, i as u64)?;\n-                        self.validate(\n-                            ValidationQuery {\n-                                place: (query.place.0.clone().index(i as u64), inner_place),\n-                                ty: elem_ty,\n-                                ..query\n-                            },\n-                            mode,\n-                        )?;\n-                    }\n-                }\n-                TyDynamic(_data, _region) => {\n-                    // Check that this is a valid vtable\n-                    let vtable = match query.place.1 {\n-                        Place::Ptr { extra: PlaceExtra::Vtable(vtable), .. } => vtable,\n-                        _ => {\n-                            bug!(\n-                                \"acquire_valid of a TyDynamic given non-trait-object place: {:?}\",\n-                                query.place\n-                            )\n-                        }\n-                    };\n-                    self.read_size_and_align_from_vtable(vtable)?;\n-                    // TODO: Check that the vtable contains all the function pointers we expect it to have.\n-                    // Trait objects cannot have any operations performed\n-                    // on them directly.  We cannot, in general, even acquire any locks as the trait object *could*\n-                    // contain an UnsafeCell.  If we call functions to get access to data, we will validate\n-                    // their return values.  So, it doesn't seem like there's anything else to do.\n-                }\n-                TyAdt(adt, _) => {\n-                    if Some(adt.did) == self.tcx.tcx.lang_items().unsafe_cell_type() &&\n-                        query.mutbl == MutImmutable\n-                    {\n-                        // No locks for shared unsafe cells.  Also no other validation, the only field is private anyway.\n-                        return Ok(());\n-                    }\n-\n-                    match adt.adt_kind() {\n-                        AdtKind::Enum => {\n-                            let layout = self.layout_of(query.ty)?;\n-                            let variant_idx = self.read_discriminant_as_variant_index(query.place.1, layout)?;\n-                            let variant = &adt.variants[variant_idx];\n-\n-                            if !variant.fields.is_empty() {\n-                                // Downcast to this variant, if needed\n-                                let place = if adt.is_enum() {\n-                                    (\n-                                        query.place.0.downcast(adt, variant_idx),\n-                                        self.eval_place_projection(\n-                                            query.place.1,\n-                                            query.ty,\n-                                            &mir::ProjectionElem::Downcast(adt, variant_idx),\n-                                        )?,\n-                                    )\n-                                } else {\n-                                    query.place\n-                                };\n-\n-                                // Recursively validate the fields\n-                                self.validate_fields(\n-                                    ValidationQuery { place, ..query },\n-                                    mode,\n-                                )?;\n-                            } else {\n-                                // No fields, nothing left to check.  Downcasting may fail, e.g. in case of a CEnum.\n-                            }\n-                        }\n-                        AdtKind::Struct => {\n-                            self.validate_fields(query, mode)?;\n-                        }\n-                        AdtKind::Union => {\n-                            // No guarantees are provided for union types.\n-                            // TODO: Make sure that all access to union fields is unsafe; otherwise, we may have some checking to do (but what exactly?)\n-                        }\n-                    }\n-                }\n-                TyTuple(..) |\n-                TyClosure(..) => {\n-                    // TODO: Check if the signature matches for `TyClosure`\n-                    // (should be the same check as what terminator/mod.rs already does on call?).\n-                    // Is there other things we can/should check?  Like vtable pointers?\n-                    self.validate_fields(query, mode)?;\n-                }\n-                // FIXME: generators aren't validated right now\n-                TyGenerator(..) => {},\n-                _ => bug!(\"We already established that this is a type we support. ({})\", query.ty),\n-            }\n-        };\n-        match res {\n-            // ReleaseUntil(None) of an uninitalized variable is a NOP.  This is needed because\n-            // we have to release the return value of a function; due to destination-passing-style\n-            // the callee may directly write there.\n-            // TODO: Ideally we would know whether the destination is already initialized, and only\n-            // release if it is.  But of course that can't even always be statically determined.\n-            Err(EvalError { kind: EvalErrorKind::ReadUndefBytes, .. })\n-                if mode == ValidationMode::ReleaseUntil(None) => {\n-                Ok(())\n-            }\n-            res => res,\n-        }\n-    }\n-}"}, {"sha": "ec8e16d33e42b7eba0f1137299bf651f3608b3ae", "filename": "tests/run-pass/atomic-compare_exchange.rs", "status": "modified", "additions": 11, "deletions": 7, "changes": 18, "blob_url": "https://github.com/rust-lang/rust/blob/259cc6e3dccdf7dd7f9c8e2052c6dea19fe066d9/tests%2Frun-pass%2Fatomic-compare_exchange.rs", "raw_url": "https://github.com/rust-lang/rust/raw/259cc6e3dccdf7dd7f9c8e2052c6dea19fe066d9/tests%2Frun-pass%2Fatomic-compare_exchange.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/tests%2Frun-pass%2Fatomic-compare_exchange.rs?ref=259cc6e3dccdf7dd7f9c8e2052c6dea19fe066d9", "patch": "@@ -15,18 +15,22 @@ static ATOMIC: AtomicIsize = ATOMIC_ISIZE_INIT;\n \n fn main() {\n     // Make sure trans can emit all the intrinsics correctly\n-    ATOMIC.compare_exchange(0, 1, Relaxed, Relaxed).ok();\n-    ATOMIC.compare_exchange(0, 1, Acquire, Relaxed).ok();\n-    ATOMIC.compare_exchange(0, 1, Release, Relaxed).ok();\n-    ATOMIC.compare_exchange(0, 1, AcqRel, Relaxed).ok();\n+    assert_eq!(ATOMIC.compare_exchange(0, 1, Relaxed, Relaxed), Ok(0));\n+    assert_eq!(ATOMIC.compare_exchange(0, 2, Acquire, Relaxed), Err(1));\n+    assert_eq!(ATOMIC.compare_exchange(0, 1, Release, Relaxed), Err(1));\n+    assert_eq!(ATOMIC.compare_exchange(1, 0, AcqRel, Relaxed), Ok(1));\n     ATOMIC.compare_exchange(0, 1, SeqCst, Relaxed).ok();\n     ATOMIC.compare_exchange(0, 1, Acquire, Acquire).ok();\n     ATOMIC.compare_exchange(0, 1, AcqRel, Acquire).ok();\n     ATOMIC.compare_exchange(0, 1, SeqCst, Acquire).ok();\n     ATOMIC.compare_exchange(0, 1, SeqCst, SeqCst).ok();\n-    ATOMIC.compare_exchange_weak(0, 1, Relaxed, Relaxed).ok();\n-    ATOMIC.compare_exchange_weak(0, 1, Acquire, Relaxed).ok();\n-    ATOMIC.compare_exchange_weak(0, 1, Release, Relaxed).ok();\n+\n+    ATOMIC.store(0, SeqCst);\n+\n+    assert_eq!(ATOMIC.compare_exchange_weak(0, 1, Relaxed, Relaxed), Ok(0));\n+    assert_eq!(ATOMIC.compare_exchange_weak(0, 2, Acquire, Relaxed), Err(1));\n+    assert_eq!(ATOMIC.compare_exchange_weak(0, 1, Release, Relaxed), Err(1));\n+    assert_eq!(ATOMIC.compare_exchange_weak(1, 0, AcqRel, Relaxed), Ok(1));\n     ATOMIC.compare_exchange_weak(0, 1, AcqRel, Relaxed).ok();\n     ATOMIC.compare_exchange_weak(0, 1, SeqCst, Relaxed).ok();\n     ATOMIC.compare_exchange_weak(0, 1, Acquire, Acquire).ok();"}]}