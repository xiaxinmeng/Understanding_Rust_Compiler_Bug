{"sha": "a497e9a05ef9b3d850014851a8e0875d773698a1", "node_id": "C_kwDOAAsO6NoAKGE0OTdlOWEwNWVmOWIzZDg1MDAxNDg1MWE4ZTA4NzVkNzczNjk4YTE", "commit": {"author": {"name": "Jake Heinz", "email": "jh@discordapp.com", "date": "2023-04-22T02:49:13Z"}, "committer": {"name": "Jake Heinz", "email": "jh@discordapp.com", "date": "2023-04-22T03:06:06Z"}, "message": "mbe: fix token conversion for doc comments", "tree": {"sha": "9fdf34f4a54b9416dcb5aab43a8e6b3e4ad0d77a", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/9fdf34f4a54b9416dcb5aab43a8e6b3e4ad0d77a"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/a497e9a05ef9b3d850014851a8e0875d773698a1", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/a497e9a05ef9b3d850014851a8e0875d773698a1", "html_url": "https://github.com/rust-lang/rust/commit/a497e9a05ef9b3d850014851a8e0875d773698a1", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/a497e9a05ef9b3d850014851a8e0875d773698a1/comments", "author": {"login": "jhgg", "id": 5489149, "node_id": "MDQ6VXNlcjU0ODkxNDk=", "avatar_url": "https://avatars.githubusercontent.com/u/5489149?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jhgg", "html_url": "https://github.com/jhgg", "followers_url": "https://api.github.com/users/jhgg/followers", "following_url": "https://api.github.com/users/jhgg/following{/other_user}", "gists_url": "https://api.github.com/users/jhgg/gists{/gist_id}", "starred_url": "https://api.github.com/users/jhgg/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jhgg/subscriptions", "organizations_url": "https://api.github.com/users/jhgg/orgs", "repos_url": "https://api.github.com/users/jhgg/repos", "events_url": "https://api.github.com/users/jhgg/events{/privacy}", "received_events_url": "https://api.github.com/users/jhgg/received_events", "type": "User", "site_admin": false}, "committer": {"login": "jhgg", "id": 5489149, "node_id": "MDQ6VXNlcjU0ODkxNDk=", "avatar_url": "https://avatars.githubusercontent.com/u/5489149?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jhgg", "html_url": "https://github.com/jhgg", "followers_url": "https://api.github.com/users/jhgg/followers", "following_url": "https://api.github.com/users/jhgg/following{/other_user}", "gists_url": "https://api.github.com/users/jhgg/gists{/gist_id}", "starred_url": "https://api.github.com/users/jhgg/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jhgg/subscriptions", "organizations_url": "https://api.github.com/users/jhgg/orgs", "repos_url": "https://api.github.com/users/jhgg/repos", "events_url": "https://api.github.com/users/jhgg/events{/privacy}", "received_events_url": "https://api.github.com/users/jhgg/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "af3b6a0893cc3a05b5ddc1e9d31b2c454b480426", "url": "https://api.github.com/repos/rust-lang/rust/commits/af3b6a0893cc3a05b5ddc1e9d31b2c454b480426", "html_url": "https://github.com/rust-lang/rust/commit/af3b6a0893cc3a05b5ddc1e9d31b2c454b480426"}], "stats": {"total": 100, "additions": 64, "deletions": 36}, "files": [{"sha": "1518b4951530137903ea68352c1cd21c97219a82", "filename": "crates/ide/src/goto_definition.rs", "status": "modified", "additions": 26, "deletions": 0, "changes": 26, "blob_url": "https://github.com/rust-lang/rust/blob/a497e9a05ef9b3d850014851a8e0875d773698a1/crates%2Fide%2Fsrc%2Fgoto_definition.rs", "raw_url": "https://github.com/rust-lang/rust/raw/a497e9a05ef9b3d850014851a8e0875d773698a1/crates%2Fide%2Fsrc%2Fgoto_definition.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fide%2Fsrc%2Fgoto_definition.rs?ref=a497e9a05ef9b3d850014851a8e0875d773698a1", "patch": "@@ -850,6 +850,32 @@ fn foo() {}\n         );\n     }\n \n+    #[test]\n+    fn goto_through_included_file_struct_with_doc_comment() {\n+        check(\n+            r#\"\n+//- /main.rs\n+#[rustc_builtin_macro]\n+macro_rules! include {}\n+\n+include!(\"foo.rs\");\n+\n+fn f() {\n+    let x = Foo$0;\n+}\n+\n+mod confuse_index {\n+    pub struct Foo;\n+}\n+\n+//- /foo.rs\n+/// This is a doc comment\n+pub struct Foo;\n+         //^^^\n+        \"#,\n+        );\n+    }\n+\n     #[test]\n     fn goto_for_type_param() {\n         check("}, {"sha": "8cbf0f8fc0bd4c1a23dcb92f3f35e8a8a0c56168", "filename": "crates/mbe/src/syntax_bridge.rs", "status": "modified", "additions": 38, "deletions": 36, "changes": 74, "blob_url": "https://github.com/rust-lang/rust/blob/a497e9a05ef9b3d850014851a8e0875d773698a1/crates%2Fmbe%2Fsrc%2Fsyntax_bridge.rs", "raw_url": "https://github.com/rust-lang/rust/raw/a497e9a05ef9b3d850014851a8e0875d773698a1/crates%2Fmbe%2Fsrc%2Fsyntax_bridge.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fmbe%2Fsrc%2Fsyntax_bridge.rs?ref=a497e9a05ef9b3d850014851a8e0875d773698a1", "patch": "@@ -190,20 +190,13 @@ fn convert_tokens<C: TokenConverter>(conv: &mut C) -> tt::Subtree {\n \n         let kind = token.kind(conv);\n         if kind == COMMENT {\n-            if let Some(tokens) = conv.convert_doc_comment(&token) {\n-                // FIXME: There has to be a better way to do this\n-                // Add the comments token id to the converted doc string\n+            // Since `convert_doc_comment` can fail, we need to peek the next id, so that we can\n+            // figure out which token id to use for the doc comment, if it is converted successfully.\n+            let next_id = conv.id_alloc().peek_next_id();\n+            if let Some(tokens) = conv.convert_doc_comment(&token, next_id) {\n                 let id = conv.id_alloc().alloc(range, synth_id);\n-                result.extend(tokens.into_iter().map(|mut tt| {\n-                    if let tt::TokenTree::Subtree(sub) = &mut tt {\n-                        if let Some(tt::TokenTree::Leaf(tt::Leaf::Literal(lit))) =\n-                            sub.token_trees.get_mut(2)\n-                        {\n-                            lit.span = id\n-                        }\n-                    }\n-                    tt\n-                }));\n+                debug_assert_eq!(id, next_id);\n+                result.extend(tokens);\n             }\n             continue;\n         }\n@@ -382,49 +375,46 @@ fn doc_comment_text(comment: &ast::Comment) -> SmolStr {\n     text.into()\n }\n \n-fn convert_doc_comment(token: &syntax::SyntaxToken) -> Option<Vec<tt::TokenTree>> {\n+fn convert_doc_comment(\n+    token: &syntax::SyntaxToken,\n+    span: tt::TokenId,\n+) -> Option<Vec<tt::TokenTree>> {\n     cov_mark::hit!(test_meta_doc_comments);\n     let comment = ast::Comment::cast(token.clone())?;\n     let doc = comment.kind().doc?;\n \n     // Make `doc=\"\\\" Comments\\\"\"\n-    let meta_tkns = vec![mk_ident(\"doc\"), mk_punct('='), mk_doc_literal(&comment)];\n+    let meta_tkns =\n+        vec![mk_ident(\"doc\", span), mk_punct('=', span), mk_doc_literal(&comment, span)];\n \n     // Make `#![]`\n     let mut token_trees = Vec::with_capacity(3);\n-    token_trees.push(mk_punct('#'));\n+    token_trees.push(mk_punct('#', span));\n     if let ast::CommentPlacement::Inner = doc {\n-        token_trees.push(mk_punct('!'));\n+        token_trees.push(mk_punct('!', span));\n     }\n     token_trees.push(tt::TokenTree::from(tt::Subtree {\n-        delimiter: tt::Delimiter {\n-            open: tt::TokenId::UNSPECIFIED,\n-            close: tt::TokenId::UNSPECIFIED,\n-            kind: tt::DelimiterKind::Bracket,\n-        },\n+        delimiter: tt::Delimiter { open: span, close: span, kind: tt::DelimiterKind::Bracket },\n         token_trees: meta_tkns,\n     }));\n \n     return Some(token_trees);\n \n     // Helper functions\n-    fn mk_ident(s: &str) -> tt::TokenTree {\n-        tt::TokenTree::from(tt::Leaf::from(tt::Ident {\n-            text: s.into(),\n-            span: tt::TokenId::unspecified(),\n-        }))\n+    fn mk_ident(s: &str, span: tt::TokenId) -> tt::TokenTree {\n+        tt::TokenTree::from(tt::Leaf::from(tt::Ident { text: s.into(), span }))\n     }\n \n-    fn mk_punct(c: char) -> tt::TokenTree {\n+    fn mk_punct(c: char, span: tt::TokenId) -> tt::TokenTree {\n         tt::TokenTree::from(tt::Leaf::from(tt::Punct {\n             char: c,\n             spacing: tt::Spacing::Alone,\n-            span: tt::TokenId::unspecified(),\n+            span,\n         }))\n     }\n \n-    fn mk_doc_literal(comment: &ast::Comment) -> tt::TokenTree {\n-        let lit = tt::Literal { text: doc_comment_text(comment), span: tt::TokenId::unspecified() };\n+    fn mk_doc_literal(comment: &ast::Comment, span: tt::TokenId) -> tt::TokenTree {\n+        let lit = tt::Literal { text: doc_comment_text(comment), span };\n \n         tt::TokenTree::from(tt::Leaf::from(lit))\n     }\n@@ -480,6 +470,10 @@ impl TokenIdAlloc {\n             }\n         }\n     }\n+\n+    fn peek_next_id(&self) -> tt::TokenId {\n+        tt::TokenId(self.next_id)\n+    }\n }\n \n /// A raw token (straight from lexer) converter\n@@ -502,7 +496,11 @@ trait SrcToken<Ctx>: std::fmt::Debug {\n trait TokenConverter: Sized {\n     type Token: SrcToken<Self>;\n \n-    fn convert_doc_comment(&self, token: &Self::Token) -> Option<Vec<tt::TokenTree>>;\n+    fn convert_doc_comment(\n+        &self,\n+        token: &Self::Token,\n+        span: tt::TokenId,\n+    ) -> Option<Vec<tt::TokenTree>>;\n \n     fn bump(&mut self) -> Option<(Self::Token, TextRange)>;\n \n@@ -532,9 +530,9 @@ impl<'a> SrcToken<RawConverter<'a>> for usize {\n impl<'a> TokenConverter for RawConverter<'a> {\n     type Token = usize;\n \n-    fn convert_doc_comment(&self, &token: &usize) -> Option<Vec<tt::TokenTree>> {\n+    fn convert_doc_comment(&self, &token: &usize, span: tt::TokenId) -> Option<Vec<tt::TokenTree>> {\n         let text = self.lexed.text(token);\n-        convert_doc_comment(&doc_comment(text))\n+        convert_doc_comment(&doc_comment(text), span)\n     }\n \n     fn bump(&mut self) -> Option<(Self::Token, TextRange)> {\n@@ -681,8 +679,12 @@ impl SrcToken<Converter> for SynToken {\n \n impl TokenConverter for Converter {\n     type Token = SynToken;\n-    fn convert_doc_comment(&self, token: &Self::Token) -> Option<Vec<tt::TokenTree>> {\n-        convert_doc_comment(token.token()?)\n+    fn convert_doc_comment(\n+        &self,\n+        token: &Self::Token,\n+        span: tt::TokenId,\n+    ) -> Option<Vec<tt::TokenTree>> {\n+        convert_doc_comment(token.token()?, span)\n     }\n \n     fn bump(&mut self) -> Option<(Self::Token, TextRange)> {"}]}