{"sha": "adda8997b17f8d3d9d49384e3c5a9cb9e2640345", "node_id": "MDY6Q29tbWl0NzI0NzEyOmFkZGE4OTk3YjE3ZjhkM2Q5ZDQ5Mzg0ZTNjNWE5Y2I5ZTI2NDAzNDU=", "commit": {"author": {"name": "Florian Hahn", "email": "flo@fhahn.com", "date": "2014-12-26T19:49:36Z"}, "committer": {"name": "Florian Hahn", "email": "flo@fhahn.com", "date": "2014-12-29T18:40:40Z"}, "message": "Update grammer/verify.rs to work with recent master", "tree": {"sha": "f71857fb2084fcbaebde765599058bdeee7d67f1", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/f71857fb2084fcbaebde765599058bdeee7d67f1"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/adda8997b17f8d3d9d49384e3c5a9cb9e2640345", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/adda8997b17f8d3d9d49384e3c5a9cb9e2640345", "html_url": "https://github.com/rust-lang/rust/commit/adda8997b17f8d3d9d49384e3c5a9cb9e2640345", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/adda8997b17f8d3d9d49384e3c5a9cb9e2640345/comments", "author": {"login": "fhahn", "id": 450489, "node_id": "MDQ6VXNlcjQ1MDQ4OQ==", "avatar_url": "https://avatars.githubusercontent.com/u/450489?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fhahn", "html_url": "https://github.com/fhahn", "followers_url": "https://api.github.com/users/fhahn/followers", "following_url": "https://api.github.com/users/fhahn/following{/other_user}", "gists_url": "https://api.github.com/users/fhahn/gists{/gist_id}", "starred_url": "https://api.github.com/users/fhahn/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fhahn/subscriptions", "organizations_url": "https://api.github.com/users/fhahn/orgs", "repos_url": "https://api.github.com/users/fhahn/repos", "events_url": "https://api.github.com/users/fhahn/events{/privacy}", "received_events_url": "https://api.github.com/users/fhahn/received_events", "type": "User", "site_admin": false}, "committer": {"login": "fhahn", "id": 450489, "node_id": "MDQ6VXNlcjQ1MDQ4OQ==", "avatar_url": "https://avatars.githubusercontent.com/u/450489?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fhahn", "html_url": "https://github.com/fhahn", "followers_url": "https://api.github.com/users/fhahn/followers", "following_url": "https://api.github.com/users/fhahn/following{/other_user}", "gists_url": "https://api.github.com/users/fhahn/gists{/gist_id}", "starred_url": "https://api.github.com/users/fhahn/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fhahn/subscriptions", "organizations_url": "https://api.github.com/users/fhahn/orgs", "repos_url": "https://api.github.com/users/fhahn/repos", "events_url": "https://api.github.com/users/fhahn/events{/privacy}", "received_events_url": "https://api.github.com/users/fhahn/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "288195370c0b033f0c8540b5e4b65a45adb850da", "url": "https://api.github.com/repos/rust-lang/rust/commits/288195370c0b033f0c8540b5e4b65a45adb850da", "html_url": "https://github.com/rust-lang/rust/commit/288195370c0b033f0c8540b5e4b65a45adb850da"}], "stats": {"total": 62, "additions": 32, "deletions": 30}, "files": [{"sha": "bdb616fcc99b83d744c28afed4ce5c8dcd250a36", "filename": "src/grammar/verify.rs", "status": "modified", "additions": 32, "deletions": 30, "changes": 62, "blob_url": "https://github.com/rust-lang/rust/blob/adda8997b17f8d3d9d49384e3c5a9cb9e2640345/src%2Fgrammar%2Fverify.rs", "raw_url": "https://github.com/rust-lang/rust/raw/adda8997b17f8d3d9d49384e3c5a9cb9e2640345/src%2Fgrammar%2Fverify.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Fgrammar%2Fverify.rs?ref=adda8997b17f8d3d9d49384e3c5a9cb9e2640345", "patch": "@@ -61,7 +61,7 @@ fn parse_token_list(file: &str) -> HashMap<String, token::Token> {\n             \"SHL\"               => token::BinOp(token::Shl),\n             \"LBRACE\"            => token::OpenDelim(token::Brace),\n             \"RARROW\"            => token::RArrow,\n-            \"LIT_STR\"           => token::Literal(token::Str_(Name(0))),\n+            \"LIT_STR\"           => token::Literal(token::Str_(Name(0)), None),\n             \"DOTDOT\"            => token::DotDot,\n             \"MOD_SEP\"           => token::ModSep,\n             \"DOTDOTDOT\"         => token::DotDotDot,\n@@ -71,7 +71,7 @@ fn parse_token_list(file: &str) -> HashMap<String, token::Token> {\n             \"ANDAND\"            => token::AndAnd,\n             \"AT\"                => token::At,\n             \"LBRACKET\"          => token::OpenDelim(token::Bracket),\n-            \"LIT_STR_RAW\"       => token::Literal(token::StrRaw(Name(0), 0)),\n+            \"LIT_STR_RAW\"       => token::Literal(token::StrRaw(Name(0), 0), None),\n             \"RPAREN\"            => token::CloseDelim(token::Paren),\n             \"SLASH\"             => token::BinOp(token::Slash),\n             \"COMMA\"             => token::Comma,\n@@ -80,8 +80,8 @@ fn parse_token_list(file: &str) -> HashMap<String, token::Token> {\n             \"TILDE\"             => token::Tilde,\n             \"IDENT\"             => id(),\n             \"PLUS\"              => token::BinOp(token::Plus),\n-            \"LIT_CHAR\"          => token::Literal(token::Char(Name(0))),\n-            \"LIT_BYTE\"          => token::Literal(token::Byte(Name(0))),\n+            \"LIT_CHAR\"          => token::Literal(token::Char(Name(0)), None),\n+            \"LIT_BYTE\"          => token::Literal(token::Byte(Name(0)), None),\n             \"EQ\"                => token::Eq,\n             \"RBRACKET\"          => token::CloseDelim(token::Bracket),\n             \"COMMENT\"           => token::Comment,\n@@ -95,9 +95,9 @@ fn parse_token_list(file: &str) -> HashMap<String, token::Token> {\n             \"BINOP\"             => token::BinOp(token::Plus),\n             \"POUND\"             => token::Pound,\n             \"OROR\"              => token::OrOr,\n-            \"LIT_INTEGER\"       => token::Literal(token::Integer(Name(0))),\n+            \"LIT_INTEGER\"       => token::Literal(token::Integer(Name(0)), None),\n             \"BINOPEQ\"           => token::BinOpEq(token::Plus),\n-            \"LIT_FLOAT\"         => token::Literal(token::Float(Name(0))),\n+            \"LIT_FLOAT\"         => token::Literal(token::Float(Name(0)), None),\n             \"WHITESPACE\"        => token::Whitespace,\n             \"UNDERSCORE\"        => token::Underscore,\n             \"MINUS\"             => token::BinOp(token::Minus),\n@@ -107,8 +107,8 @@ fn parse_token_list(file: &str) -> HashMap<String, token::Token> {\n             \"OR\"                => token::BinOp(token::Or),\n             \"GT\"                => token::Gt,\n             \"LE\"                => token::Le,\n-            \"LIT_BINARY\"        => token::Literal(token::Binary(Name(0))),\n-            \"LIT_BINARY_RAW\"    => token::Literal(token::BinaryRaw(Name(0), 0)),\n+            \"LIT_BINARY\"        => token::Literal(token::Binary(Name(0)), None),\n+            \"LIT_BINARY_RAW\"    => token::Literal(token::BinaryRaw(Name(0), 0), None),\n             _                   => continue,\n         };\n \n@@ -189,17 +189,17 @@ fn parse_antlr_token(s: &str, tokens: &HashMap<String, token::Token>) -> TokenAn\n         token::BinOp(..)           => token::BinOp(str_to_binop(content)),\n         token::BinOpEq(..)         => token::BinOpEq(str_to_binop(content.slice_to(\n                                                                     content.len() - 1))),\n-        token::Literal(token::Str_(..))      => token::Literal(token::Str_(fix(content))),\n-        token::Literal(token::StrRaw(..))    => token::Literal(token::StrRaw(fix(content),\n-                                                                             count(content))),\n-        token::Literal(token::Char(..))      => token::Literal(token::Char(fixchar(content))),\n-        token::Literal(token::Byte(..))      => token::Literal(token::Byte(fixchar(content))),\n+        token::Literal(token::Str_(..), n)      => token::Literal(token::Str_(fix(content)), n),\n+        token::Literal(token::StrRaw(..), n)    => token::Literal(token::StrRaw(fix(content),\n+                                                                             count(content)), n),\n+        token::Literal(token::Char(..), n)      => token::Literal(token::Char(fixchar(content)), n),\n+        token::Literal(token::Byte(..), n)      => token::Literal(token::Byte(fixchar(content)), n),\n         token::DocComment(..)      => token::DocComment(nm),\n-        token::Literal(token::Integer(..))   => token::Literal(token::Integer(nm)),\n-        token::Literal(token::Float(..))     => token::Literal(token::Float(nm)),\n-        token::Literal(token::Binary(..))    => token::Literal(token::Binary(nm)),\n-        token::Literal(token::BinaryRaw(..)) => token::Literal(token::BinaryRaw(fix(content),\n-                                                                                count(content))),\n+        token::Literal(token::Integer(..), n)   => token::Literal(token::Integer(nm), n),\n+        token::Literal(token::Float(..), n)     => token::Literal(token::Float(nm), n),\n+        token::Literal(token::Binary(..), n)    => token::Literal(token::Binary(nm), n),\n+        token::Literal(token::BinaryRaw(..), n) => token::Literal(token::BinaryRaw(fix(content),\n+                                                                                count(content)), n),\n         token::Ident(..)           => token::Ident(ast::Ident { name: nm, ctxt: 0 },\n                                                    token::ModName),\n         token::Lifetime(..)        => token::Lifetime(ast::Ident { name: nm, ctxt: 0 }),\n@@ -214,8 +214,8 @@ fn parse_antlr_token(s: &str, tokens: &HashMap<String, token::Token>) -> TokenAn\n     };\n \n     let sp = syntax::codemap::Span {\n-        lo: syntax::codemap::BytePos(from_str::<u32>(start).unwrap() - offset),\n-        hi: syntax::codemap::BytePos(from_str::<u32>(end).unwrap() + 1),\n+        lo: syntax::codemap::BytePos(start.parse::<u32>().unwrap() - offset),\n+        hi: syntax::codemap::BytePos(end.parse::<u32>().unwrap() + 1),\n         expn_id: syntax::codemap::NO_EXPANSION\n     };\n \n@@ -247,7 +247,9 @@ fn main() {\n     let token_map = parse_token_list(token_file.read_to_string().unwrap().as_slice());\n \n     let mut stdin = std::io::stdin();\n-    let mut antlr_tokens = stdin.lines().map(|l| parse_antlr_token(l.unwrap().as_slice().trim(),\n+    let mut lock = stdin.lock();\n+    let lines = lock.lines();\n+    let mut antlr_tokens = lines.map(|l| parse_antlr_token(l.unwrap().as_slice().trim(),\n                                                                    &token_map));\n \n     let code = File::open(&Path::new(args[1].as_slice())).unwrap().read_to_string().unwrap();\n@@ -284,17 +286,17 @@ fn main() {\n                     ref c => assert!(c == &antlr_tok.tok, \"{} is not {}\", rustc_tok, antlr_tok)\n                 }\n             )\n-        )\n+        );\n \n         matches!(\n-            token::Literal(token::Byte(..)),\n-            token::Literal(token::Char(..)),\n-            token::Literal(token::Integer(..)),\n-            token::Literal(token::Float(..)),\n-            token::Literal(token::Str_(..)),\n-            token::Literal(token::StrRaw(..)),\n-            token::Literal(token::Binary(..)),\n-            token::Literal(token::BinaryRaw(..)),\n+            token::Literal(token::Byte(..), _),\n+            token::Literal(token::Char(..), _),\n+            token::Literal(token::Integer(..), _),\n+            token::Literal(token::Float(..), _),\n+            token::Literal(token::Str_(..), _),\n+            token::Literal(token::StrRaw(..), _),\n+            token::Literal(token::Binary(..), _),\n+            token::Literal(token::BinaryRaw(..), _),\n             token::Ident(..),\n             token::Lifetime(..),\n             token::Interpolated(..),"}]}