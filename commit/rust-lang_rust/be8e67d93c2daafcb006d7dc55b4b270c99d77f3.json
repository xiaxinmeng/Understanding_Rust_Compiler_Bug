{"sha": "be8e67d93c2daafcb006d7dc55b4b270c99d77f3", "node_id": "C_kwDOAAsO6NoAKGJlOGU2N2Q5M2MyZGFhZmNiMDA2ZDdkYzU1YjRiMjcwYzk5ZDc3ZjM", "commit": {"author": {"name": "The 8472", "email": "git@infinite-source.de", "date": "2023-02-16T00:50:57Z"}, "committer": {"name": "The 8472", "email": "git@infinite-source.de", "date": "2023-04-27T20:29:03Z"}, "message": "refactor: extract function", "tree": {"sha": "b12dbae1b55815340867e3107fad0890448bf288", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/b12dbae1b55815340867e3107fad0890448bf288"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/be8e67d93c2daafcb006d7dc55b4b270c99d77f3", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/be8e67d93c2daafcb006d7dc55b4b270c99d77f3", "html_url": "https://github.com/rust-lang/rust/commit/be8e67d93c2daafcb006d7dc55b4b270c99d77f3", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/be8e67d93c2daafcb006d7dc55b4b270c99d77f3/comments", "author": {"login": "the8472", "id": 1065730, "node_id": "MDQ6VXNlcjEwNjU3MzA=", "avatar_url": "https://avatars.githubusercontent.com/u/1065730?v=4", "gravatar_id": "", "url": "https://api.github.com/users/the8472", "html_url": "https://github.com/the8472", "followers_url": "https://api.github.com/users/the8472/followers", "following_url": "https://api.github.com/users/the8472/following{/other_user}", "gists_url": "https://api.github.com/users/the8472/gists{/gist_id}", "starred_url": "https://api.github.com/users/the8472/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/the8472/subscriptions", "organizations_url": "https://api.github.com/users/the8472/orgs", "repos_url": "https://api.github.com/users/the8472/repos", "events_url": "https://api.github.com/users/the8472/events{/privacy}", "received_events_url": "https://api.github.com/users/the8472/received_events", "type": "User", "site_admin": false}, "committer": {"login": "the8472", "id": 1065730, "node_id": "MDQ6VXNlcjEwNjU3MzA=", "avatar_url": "https://avatars.githubusercontent.com/u/1065730?v=4", "gravatar_id": "", "url": "https://api.github.com/users/the8472", "html_url": "https://github.com/the8472", "followers_url": "https://api.github.com/users/the8472/followers", "following_url": "https://api.github.com/users/the8472/following{/other_user}", "gists_url": "https://api.github.com/users/the8472/gists{/gist_id}", "starred_url": "https://api.github.com/users/the8472/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/the8472/subscriptions", "organizations_url": "https://api.github.com/users/the8472/orgs", "repos_url": "https://api.github.com/users/the8472/repos", "events_url": "https://api.github.com/users/the8472/events{/privacy}", "received_events_url": "https://api.github.com/users/the8472/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "901fdb3b04375e3456b5cf771f86ecca8d6c1917", "url": "https://api.github.com/repos/rust-lang/rust/commits/901fdb3b04375e3456b5cf771f86ecca8d6c1917", "html_url": "https://github.com/rust-lang/rust/commit/901fdb3b04375e3456b5cf771f86ecca8d6c1917"}], "stats": {"total": 434, "additions": 220, "deletions": 214}, "files": [{"sha": "a76ac5f98e6d212781f9754ae5c40241f9d65b34", "filename": "compiler/rustc_abi/src/layout.rs", "status": "modified", "additions": 220, "deletions": 214, "changes": 434, "blob_url": "https://github.com/rust-lang/rust/blob/be8e67d93c2daafcb006d7dc55b4b270c99d77f3/compiler%2Frustc_abi%2Fsrc%2Flayout.rs", "raw_url": "https://github.com/rust-lang/rust/raw/be8e67d93c2daafcb006d7dc55b4b270c99d77f3/compiler%2Frustc_abi%2Fsrc%2Flayout.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_abi%2Fsrc%2Flayout.rs?ref=be8e67d93c2daafcb006d7dc55b4b270c99d77f3", "patch": "@@ -49,220 +49,7 @@ pub trait LayoutCalculator {\n         repr: &ReprOptions,\n         kind: StructKind,\n     ) -> Option<LayoutS> {\n-        let pack = repr.pack;\n-        let mut align = if pack.is_some() { dl.i8_align } else { dl.aggregate_align };\n-        let mut inverse_memory_index: IndexVec<u32, FieldIdx> = fields.indices().collect();\n-        let optimize = !repr.inhibit_struct_field_reordering_opt();\n-        if optimize {\n-            let end =\n-                if let StructKind::MaybeUnsized = kind { fields.len() - 1 } else { fields.len() };\n-            let optimizing = &mut inverse_memory_index.raw[..end];\n-            let effective_field_align = |layout: Layout<'_>| {\n-                if let Some(pack) = pack {\n-                    // return the packed alignment in bytes\n-                    layout.align().abi.min(pack).bytes()\n-                } else {\n-                    // returns log2(effective-align).\n-                    // This is ok since `pack` applies to all fields equally.\n-                    // The calculation assumes that size is an integer multiple of align, except for ZSTs.\n-                    //\n-                    // group [u8; 4] with align-4 or [u8; 6] with align-2 fields\n-                    layout.align().abi.bytes().max(layout.size().bytes()).trailing_zeros() as u64\n-                }\n-            };\n-\n-            // If `-Z randomize-layout` was enabled for the type definition we can shuffle\n-            // the field ordering to try and catch some code making assumptions about layouts\n-            // we don't guarantee\n-            if repr.can_randomize_type_layout() && cfg!(feature = \"randomize\") {\n-                #[cfg(feature = \"randomize\")]\n-                {\n-                    // `ReprOptions.layout_seed` is a deterministic seed that we can use to\n-                    // randomize field ordering with\n-                    let mut rng =\n-                        Xoshiro128StarStar::seed_from_u64(repr.field_shuffle_seed.as_u64());\n-\n-                    // Shuffle the ordering of the fields\n-                    optimizing.shuffle(&mut rng);\n-                }\n-                // Otherwise we just leave things alone and actually optimize the type's fields\n-            } else {\n-                match kind {\n-                    StructKind::AlwaysSized | StructKind::MaybeUnsized => {\n-                        optimizing.sort_by_key(|&x| {\n-                            // Place ZSTs first to avoid \"interesting offsets\",\n-                            // especially with only one or two non-ZST fields.\n-                            // Then place largest alignments first, largest niches within an alignment group last\n-                            let f = fields[x];\n-                            let niche_size = f.largest_niche().map_or(0, |n| n.available(dl));\n-                            (!f.0.is_zst(), cmp::Reverse(effective_field_align(f)), niche_size)\n-                        });\n-                    }\n-\n-                    StructKind::Prefixed(..) => {\n-                        // Sort in ascending alignment so that the layout stays optimal\n-                        // regardless of the prefix.\n-                        // And put the largest niche in an alignment group at the end\n-                        // so it can be used as discriminant in jagged enums\n-                        optimizing.sort_by_key(|&x| {\n-                            let f = fields[x];\n-                            let niche_size = f.largest_niche().map_or(0, |n| n.available(dl));\n-                            (effective_field_align(f), niche_size)\n-                        });\n-                    }\n-                }\n-\n-                // FIXME(Kixiron): We can always shuffle fields within a given alignment class\n-                //                 regardless of the status of `-Z randomize-layout`\n-            }\n-        }\n-        // inverse_memory_index holds field indices by increasing memory offset.\n-        // That is, if field 5 has offset 0, the first element of inverse_memory_index is 5.\n-        // We now write field offsets to the corresponding offset slot;\n-        // field 5 with offset 0 puts 0 in offsets[5].\n-        // At the bottom of this function, we invert `inverse_memory_index` to\n-        // produce `memory_index` (see `invert_mapping`).\n-        let mut sized = true;\n-        let mut offsets = IndexVec::from_elem(Size::ZERO, &fields);\n-        let mut offset = Size::ZERO;\n-        let mut largest_niche = None;\n-        let mut largest_niche_available = 0;\n-        if let StructKind::Prefixed(prefix_size, prefix_align) = kind {\n-            let prefix_align =\n-                if let Some(pack) = pack { prefix_align.min(pack) } else { prefix_align };\n-            align = align.max(AbiAndPrefAlign::new(prefix_align));\n-            offset = prefix_size.align_to(prefix_align);\n-        }\n-        for &i in &inverse_memory_index {\n-            let field = &fields[i];\n-            if !sized {\n-                self.delay_bug(&format!(\n-                    \"univariant: field #{} comes after unsized field\",\n-                    offsets.len(),\n-                ));\n-            }\n-\n-            if field.0.is_unsized() {\n-                sized = false;\n-            }\n-\n-            // Invariant: offset < dl.obj_size_bound() <= 1<<61\n-            let field_align = if let Some(pack) = pack {\n-                field.align().min(AbiAndPrefAlign::new(pack))\n-            } else {\n-                field.align()\n-            };\n-            offset = offset.align_to(field_align.abi);\n-            align = align.max(field_align);\n-\n-            debug!(\"univariant offset: {:?} field: {:#?}\", offset, field);\n-            offsets[i] = offset;\n-\n-            if let Some(mut niche) = field.largest_niche() {\n-                let available = niche.available(dl);\n-                if available > largest_niche_available {\n-                    largest_niche_available = available;\n-                    niche.offset += offset;\n-                    largest_niche = Some(niche);\n-                }\n-            }\n-\n-            offset = offset.checked_add(field.size(), dl)?;\n-        }\n-        if let Some(repr_align) = repr.align {\n-            align = align.max(AbiAndPrefAlign::new(repr_align));\n-        }\n-        debug!(\"univariant min_size: {:?}\", offset);\n-        let min_size = offset;\n-        // As stated above, inverse_memory_index holds field indices by increasing offset.\n-        // This makes it an already-sorted view of the offsets vec.\n-        // To invert it, consider:\n-        // If field 5 has offset 0, offsets[0] is 5, and memory_index[5] should be 0.\n-        // Field 5 would be the first element, so memory_index is i:\n-        // Note: if we didn't optimize, it's already right.\n-        let memory_index = if optimize {\n-            inverse_memory_index.invert_bijective_mapping()\n-        } else {\n-            debug_assert!(inverse_memory_index.iter().copied().eq(fields.indices()));\n-            inverse_memory_index.into_iter().map(FieldIdx::as_u32).collect()\n-        };\n-        let size = min_size.align_to(align.abi);\n-        let mut abi = Abi::Aggregate { sized };\n-        // Unpack newtype ABIs and find scalar pairs.\n-        if sized && size.bytes() > 0 {\n-            // All other fields must be ZSTs.\n-            let mut non_zst_fields = fields.iter_enumerated().filter(|&(_, f)| !f.0.is_zst());\n-\n-            match (non_zst_fields.next(), non_zst_fields.next(), non_zst_fields.next()) {\n-                // We have exactly one non-ZST field.\n-                (Some((i, field)), None, None) => {\n-                    // Field fills the struct and it has a scalar or scalar pair ABI.\n-                    if offsets[i].bytes() == 0\n-                        && align.abi == field.align().abi\n-                        && size == field.size()\n-                    {\n-                        match field.abi() {\n-                            // For plain scalars, or vectors of them, we can't unpack\n-                            // newtypes for `#[repr(C)]`, as that affects C ABIs.\n-                            Abi::Scalar(_) | Abi::Vector { .. } if optimize => {\n-                                abi = field.abi();\n-                            }\n-                            // But scalar pairs are Rust-specific and get\n-                            // treated as aggregates by C ABIs anyway.\n-                            Abi::ScalarPair(..) => {\n-                                abi = field.abi();\n-                            }\n-                            _ => {}\n-                        }\n-                    }\n-                }\n-\n-                // Two non-ZST fields, and they're both scalars.\n-                (Some((i, a)), Some((j, b)), None) => {\n-                    match (a.abi(), b.abi()) {\n-                        (Abi::Scalar(a), Abi::Scalar(b)) => {\n-                            // Order by the memory placement, not source order.\n-                            let ((i, a), (j, b)) = if offsets[i] < offsets[j] {\n-                                ((i, a), (j, b))\n-                            } else {\n-                                ((j, b), (i, a))\n-                            };\n-                            let pair = self.scalar_pair(a, b);\n-                            let pair_offsets = match pair.fields {\n-                                FieldsShape::Arbitrary { ref offsets, ref memory_index } => {\n-                                    assert_eq!(memory_index.raw, [0, 1]);\n-                                    offsets\n-                                }\n-                                _ => panic!(),\n-                            };\n-                            if offsets[i] == pair_offsets[FieldIdx::from_usize(0)]\n-                                && offsets[j] == pair_offsets[FieldIdx::from_usize(1)]\n-                                && align == pair.align\n-                                && size == pair.size\n-                            {\n-                                // We can use `ScalarPair` only when it matches our\n-                                // already computed layout (including `#[repr(C)]`).\n-                                abi = pair.abi;\n-                            }\n-                        }\n-                        _ => {}\n-                    }\n-                }\n-\n-                _ => {}\n-            }\n-        }\n-        if fields.iter().any(|f| f.abi().is_uninhabited()) {\n-            abi = Abi::Uninhabited;\n-        }\n-        Some(LayoutS {\n-            variants: Variants::Single { index: FIRST_VARIANT },\n-            fields: FieldsShape::Arbitrary { offsets, memory_index },\n-            abi,\n-            largest_niche,\n-            align,\n-            size,\n-        })\n+        univariant(self, dl, fields, repr, kind)\n     }\n \n     fn layout_of_never_type(&self) -> LayoutS {\n@@ -934,3 +721,222 @@ pub trait LayoutCalculator {\n         })\n     }\n }\n+\n+fn univariant(\n+    this: &(impl LayoutCalculator + ?Sized),\n+    dl: &TargetDataLayout,\n+    fields: &IndexSlice<FieldIdx, Layout<'_>>,\n+    repr: &ReprOptions,\n+    kind: StructKind,\n+) -> Option<LayoutS> {\n+    let pack = repr.pack;\n+    let mut align = if pack.is_some() { dl.i8_align } else { dl.aggregate_align };\n+    let mut inverse_memory_index: IndexVec<u32, FieldIdx> = fields.indices().collect();\n+    let optimize = !repr.inhibit_struct_field_reordering_opt();\n+    if optimize {\n+        let end = if let StructKind::MaybeUnsized = kind { fields.len() - 1 } else { fields.len() };\n+        let optimizing = &mut inverse_memory_index.raw[..end];\n+        let effective_field_align = |layout: Layout<'_>| {\n+            if let Some(pack) = pack {\n+                // return the packed alignment in bytes\n+                layout.align().abi.min(pack).bytes()\n+            } else {\n+                // returns log2(effective-align).\n+                // This is ok since `pack` applies to all fields equally.\n+                // The calculation assumes that size is an integer multiple of align, except for ZSTs.\n+                //\n+                // group [u8; 4] with align-4 or [u8; 6] with align-2 fields\n+                layout.align().abi.bytes().max(layout.size().bytes()).trailing_zeros() as u64\n+            }\n+        };\n+\n+        // If `-Z randomize-layout` was enabled for the type definition we can shuffle\n+        // the field ordering to try and catch some code making assumptions about layouts\n+        // we don't guarantee\n+        if repr.can_randomize_type_layout() && cfg!(feature = \"randomize\") {\n+            #[cfg(feature = \"randomize\")]\n+            {\n+                // `ReprOptions.layout_seed` is a deterministic seed that we can use to\n+                // randomize field ordering with\n+                let mut rng = Xoshiro128StarStar::seed_from_u64(repr.field_shuffle_seed.as_u64());\n+\n+                // Shuffle the ordering of the fields\n+                optimizing.shuffle(&mut rng);\n+            }\n+            // Otherwise we just leave things alone and actually optimize the type's fields\n+        } else {\n+            match kind {\n+                StructKind::AlwaysSized | StructKind::MaybeUnsized => {\n+                    optimizing.sort_by_key(|&x| {\n+                        // Place ZSTs first to avoid \"interesting offsets\",\n+                        // especially with only one or two non-ZST fields.\n+                        // Then place largest alignments first, largest niches within an alignment group last\n+                        let f = fields[x];\n+                        let niche_size = f.largest_niche().map_or(0, |n| n.available(dl));\n+                        (!f.0.is_zst(), cmp::Reverse(effective_field_align(f)), niche_size)\n+                    });\n+                }\n+\n+                StructKind::Prefixed(..) => {\n+                    // Sort in ascending alignment so that the layout stays optimal\n+                    // regardless of the prefix.\n+                    // And put the largest niche in an alignment group at the end\n+                    // so it can be used as discriminant in jagged enums\n+                    optimizing.sort_by_key(|&x| {\n+                        let f = fields[x];\n+                        let niche_size = f.largest_niche().map_or(0, |n| n.available(dl));\n+                        (effective_field_align(f), niche_size)\n+                    });\n+                }\n+            }\n+\n+            // FIXME(Kixiron): We can always shuffle fields within a given alignment class\n+            //                 regardless of the status of `-Z randomize-layout`\n+        }\n+    }\n+    // inverse_memory_index holds field indices by increasing memory offset.\n+    // That is, if field 5 has offset 0, the first element of inverse_memory_index is 5.\n+    // We now write field offsets to the corresponding offset slot;\n+    // field 5 with offset 0 puts 0 in offsets[5].\n+    // At the bottom of this function, we invert `inverse_memory_index` to\n+    // produce `memory_index` (see `invert_mapping`).\n+    let mut sized = true;\n+    let mut offsets = IndexVec::from_elem(Size::ZERO, &fields);\n+    let mut offset = Size::ZERO;\n+    let mut largest_niche = None;\n+    let mut largest_niche_available = 0;\n+    if let StructKind::Prefixed(prefix_size, prefix_align) = kind {\n+        let prefix_align =\n+            if let Some(pack) = pack { prefix_align.min(pack) } else { prefix_align };\n+        align = align.max(AbiAndPrefAlign::new(prefix_align));\n+        offset = prefix_size.align_to(prefix_align);\n+    }\n+    for &i in &inverse_memory_index {\n+        let field = &fields[i];\n+        if !sized {\n+            this.delay_bug(&format!(\n+                \"univariant: field #{} comes after unsized field\",\n+                offsets.len(),\n+            ));\n+        }\n+\n+        if field.0.is_unsized() {\n+            sized = false;\n+        }\n+\n+        // Invariant: offset < dl.obj_size_bound() <= 1<<61\n+        let field_align = if let Some(pack) = pack {\n+            field.align().min(AbiAndPrefAlign::new(pack))\n+        } else {\n+            field.align()\n+        };\n+        offset = offset.align_to(field_align.abi);\n+        align = align.max(field_align);\n+\n+        debug!(\"univariant offset: {:?} field: {:#?}\", offset, field);\n+        offsets[i] = offset;\n+\n+        if let Some(mut niche) = field.largest_niche() {\n+            let available = niche.available(dl);\n+            if available > largest_niche_available {\n+                largest_niche_available = available;\n+                niche.offset += offset;\n+                largest_niche = Some(niche);\n+            }\n+        }\n+\n+        offset = offset.checked_add(field.size(), dl)?;\n+    }\n+    if let Some(repr_align) = repr.align {\n+        align = align.max(AbiAndPrefAlign::new(repr_align));\n+    }\n+    debug!(\"univariant min_size: {:?}\", offset);\n+    let min_size = offset;\n+    // As stated above, inverse_memory_index holds field indices by increasing offset.\n+    // This makes it an already-sorted view of the offsets vec.\n+    // To invert it, consider:\n+    // If field 5 has offset 0, offsets[0] is 5, and memory_index[5] should be 0.\n+    // Field 5 would be the first element, so memory_index is i:\n+    // Note: if we didn't optimize, it's already right.\n+    let memory_index = if optimize {\n+        inverse_memory_index.invert_bijective_mapping()\n+    } else {\n+        debug_assert!(inverse_memory_index.iter().copied().eq(fields.indices()));\n+        inverse_memory_index.into_iter().map(FieldIdx::as_u32).collect()\n+    };\n+    let size = min_size.align_to(align.abi);\n+    let mut abi = Abi::Aggregate { sized };\n+    // Unpack newtype ABIs and find scalar pairs.\n+    if sized && size.bytes() > 0 {\n+        // All other fields must be ZSTs.\n+        let mut non_zst_fields = fields.iter_enumerated().filter(|&(_, f)| !f.0.is_zst());\n+\n+        match (non_zst_fields.next(), non_zst_fields.next(), non_zst_fields.next()) {\n+            // We have exactly one non-ZST field.\n+            (Some((i, field)), None, None) => {\n+                // Field fills the struct and it has a scalar or scalar pair ABI.\n+                if offsets[i].bytes() == 0 && align.abi == field.align().abi && size == field.size()\n+                {\n+                    match field.abi() {\n+                        // For plain scalars, or vectors of them, we can't unpack\n+                        // newtypes for `#[repr(C)]`, as that affects C ABIs.\n+                        Abi::Scalar(_) | Abi::Vector { .. } if optimize => {\n+                            abi = field.abi();\n+                        }\n+                        // But scalar pairs are Rust-specific and get\n+                        // treated as aggregates by C ABIs anyway.\n+                        Abi::ScalarPair(..) => {\n+                            abi = field.abi();\n+                        }\n+                        _ => {}\n+                    }\n+                }\n+            }\n+\n+            // Two non-ZST fields, and they're both scalars.\n+            (Some((i, a)), Some((j, b)), None) => {\n+                match (a.abi(), b.abi()) {\n+                    (Abi::Scalar(a), Abi::Scalar(b)) => {\n+                        // Order by the memory placement, not source order.\n+                        let ((i, a), (j, b)) = if offsets[i] < offsets[j] {\n+                            ((i, a), (j, b))\n+                        } else {\n+                            ((j, b), (i, a))\n+                        };\n+                        let pair = this.scalar_pair(a, b);\n+                        let pair_offsets = match pair.fields {\n+                            FieldsShape::Arbitrary { ref offsets, ref memory_index } => {\n+                                assert_eq!(memory_index.raw, [0, 1]);\n+                                offsets\n+                            }\n+                            _ => panic!(),\n+                        };\n+                        if offsets[i] == pair_offsets[FieldIdx::from_usize(0)]\n+                            && offsets[j] == pair_offsets[FieldIdx::from_usize(1)]\n+                            && align == pair.align\n+                            && size == pair.size\n+                        {\n+                            // We can use `ScalarPair` only when it matches our\n+                            // already computed layout (including `#[repr(C)]`).\n+                            abi = pair.abi;\n+                        }\n+                    }\n+                    _ => {}\n+                }\n+            }\n+\n+            _ => {}\n+        }\n+    }\n+    if fields.iter().any(|f| f.abi().is_uninhabited()) {\n+        abi = Abi::Uninhabited;\n+    }\n+    Some(LayoutS {\n+        variants: Variants::Single { index: FIRST_VARIANT },\n+        fields: FieldsShape::Arbitrary { offsets, memory_index },\n+        abi,\n+        largest_niche,\n+        align,\n+        size,\n+    })\n+}"}]}