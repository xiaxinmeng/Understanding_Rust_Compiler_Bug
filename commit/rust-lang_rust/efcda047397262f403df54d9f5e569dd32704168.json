{"sha": "efcda047397262f403df54d9f5e569dd32704168", "node_id": "MDY6Q29tbWl0NzI0NzEyOmVmY2RhMDQ3Mzk3MjYyZjQwM2RmNTRkOWY1ZTU2OWRkMzI3MDQxNjg=", "commit": {"author": {"name": "Mark Rousskov", "email": "mark.simulacrum@gmail.com", "date": "2020-01-13T21:59:33Z"}, "committer": {"name": "Mark Rousskov", "email": "mark.simulacrum@gmail.com", "date": "2020-01-15T00:11:28Z"}, "message": "Replace old tables with new unicode data", "tree": {"sha": "57c2681d118208dd8c8f467ec1ee6b1752ccf343", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/57c2681d118208dd8c8f467ec1ee6b1752ccf343"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/efcda047397262f403df54d9f5e569dd32704168", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/efcda047397262f403df54d9f5e569dd32704168", "html_url": "https://github.com/rust-lang/rust/commit/efcda047397262f403df54d9f5e569dd32704168", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/efcda047397262f403df54d9f5e569dd32704168/comments", "author": {"login": "Mark-Simulacrum", "id": 5047365, "node_id": "MDQ6VXNlcjUwNDczNjU=", "avatar_url": "https://avatars.githubusercontent.com/u/5047365?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Mark-Simulacrum", "html_url": "https://github.com/Mark-Simulacrum", "followers_url": "https://api.github.com/users/Mark-Simulacrum/followers", "following_url": "https://api.github.com/users/Mark-Simulacrum/following{/other_user}", "gists_url": "https://api.github.com/users/Mark-Simulacrum/gists{/gist_id}", "starred_url": "https://api.github.com/users/Mark-Simulacrum/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Mark-Simulacrum/subscriptions", "organizations_url": "https://api.github.com/users/Mark-Simulacrum/orgs", "repos_url": "https://api.github.com/users/Mark-Simulacrum/repos", "events_url": "https://api.github.com/users/Mark-Simulacrum/events{/privacy}", "received_events_url": "https://api.github.com/users/Mark-Simulacrum/received_events", "type": "User", "site_admin": false}, "committer": {"login": "Mark-Simulacrum", "id": 5047365, "node_id": "MDQ6VXNlcjUwNDczNjU=", "avatar_url": "https://avatars.githubusercontent.com/u/5047365?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Mark-Simulacrum", "html_url": "https://github.com/Mark-Simulacrum", "followers_url": "https://api.github.com/users/Mark-Simulacrum/followers", "following_url": "https://api.github.com/users/Mark-Simulacrum/following{/other_user}", "gists_url": "https://api.github.com/users/Mark-Simulacrum/gists{/gist_id}", "starred_url": "https://api.github.com/users/Mark-Simulacrum/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Mark-Simulacrum/subscriptions", "organizations_url": "https://api.github.com/users/Mark-Simulacrum/orgs", "repos_url": "https://api.github.com/users/Mark-Simulacrum/repos", "events_url": "https://api.github.com/users/Mark-Simulacrum/events{/privacy}", "received_events_url": "https://api.github.com/users/Mark-Simulacrum/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "40ad8778513185c3597e99170371181d1e1d694c", "url": "https://api.github.com/repos/rust-lang/rust/commits/40ad8778513185c3597e99170371181d1e1d694c", "html_url": "https://github.com/rust-lang/rust/commit/40ad8778513185c3597e99170371181d1e1d694c"}], "stats": {"total": 5542, "additions": 2353, "deletions": 3189}, "files": [{"sha": "c341bb552a1eaf4c4dc89b4a1246abfaad2bed66", "filename": "src/libcore/char/methods.rs", "status": "modified", "additions": 8, "deletions": 8, "changes": 16, "blob_url": "https://github.com/rust-lang/rust/blob/efcda047397262f403df54d9f5e569dd32704168/src%2Flibcore%2Fchar%2Fmethods.rs", "raw_url": "https://github.com/rust-lang/rust/raw/efcda047397262f403df54d9f5e569dd32704168/src%2Flibcore%2Fchar%2Fmethods.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibcore%2Fchar%2Fmethods.rs?ref=efcda047397262f403df54d9f5e569dd32704168", "patch": "@@ -3,7 +3,7 @@\n use crate::slice;\n use crate::str::from_utf8_unchecked_mut;\n use crate::unicode::printable::is_printable;\n-use crate::unicode::tables::{conversions, derived_property, general_category, property};\n+use crate::unicode::{self, conversions};\n \n use super::*;\n \n@@ -552,7 +552,7 @@ impl char {\n     pub fn is_alphabetic(self) -> bool {\n         match self {\n             'a'..='z' | 'A'..='Z' => true,\n-            c => c > '\\x7f' && derived_property::Alphabetic(c),\n+            c => c > '\\x7f' && unicode::Alphabetic(c),\n         }\n     }\n \n@@ -583,7 +583,7 @@ impl char {\n     pub fn is_lowercase(self) -> bool {\n         match self {\n             'a'..='z' => true,\n-            c => c > '\\x7f' && derived_property::Lowercase(c),\n+            c => c > '\\x7f' && unicode::Lowercase(c),\n         }\n     }\n \n@@ -614,7 +614,7 @@ impl char {\n     pub fn is_uppercase(self) -> bool {\n         match self {\n             'A'..='Z' => true,\n-            c => c > '\\x7f' && derived_property::Uppercase(c),\n+            c => c > '\\x7f' && unicode::Uppercase(c),\n         }\n     }\n \n@@ -642,7 +642,7 @@ impl char {\n     pub fn is_whitespace(self) -> bool {\n         match self {\n             ' ' | '\\x09'..='\\x0d' => true,\n-            c => c > '\\x7f' && property::White_Space(c),\n+            c => c > '\\x7f' && unicode::White_Space(c),\n         }\n     }\n \n@@ -693,7 +693,7 @@ impl char {\n     #[stable(feature = \"rust1\", since = \"1.0.0\")]\n     #[inline]\n     pub fn is_control(self) -> bool {\n-        general_category::Cc(self)\n+        unicode::Cc(self)\n     }\n \n     /// Returns `true` if this `char` has the `Grapheme_Extend` property.\n@@ -707,7 +707,7 @@ impl char {\n     /// [`DerivedCoreProperties.txt`]: https://www.unicode.org/Public/UCD/latest/ucd/DerivedCoreProperties.txt\n     #[inline]\n     pub(crate) fn is_grapheme_extended(self) -> bool {\n-        derived_property::Grapheme_Extend(self)\n+        unicode::Grapheme_Extend(self)\n     }\n \n     /// Returns `true` if this `char` has one of the general categories for numbers.\n@@ -739,7 +739,7 @@ impl char {\n     pub fn is_numeric(self) -> bool {\n         match self {\n             '0'..='9' => true,\n-            c => c > '\\x7f' && general_category::N(c),\n+            c => c > '\\x7f' && unicode::N(c),\n         }\n     }\n "}, {"sha": "cf5576e549cdfa043ad0b9ea24b238178f19272d", "filename": "src/libcore/char/mod.rs", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/rust-lang/rust/blob/efcda047397262f403df54d9f5e569dd32704168/src%2Flibcore%2Fchar%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/efcda047397262f403df54d9f5e569dd32704168/src%2Flibcore%2Fchar%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibcore%2Fchar%2Fmod.rs?ref=efcda047397262f403df54d9f5e569dd32704168", "patch": "@@ -37,9 +37,9 @@ pub use self::decode::{decode_utf16, DecodeUtf16, DecodeUtf16Error};\n \n // unstable re-exports\n #[unstable(feature = \"unicode_version\", issue = \"49726\")]\n-pub use crate::unicode::tables::UNICODE_VERSION;\n-#[unstable(feature = \"unicode_version\", issue = \"49726\")]\n pub use crate::unicode::version::UnicodeVersion;\n+#[unstable(feature = \"unicode_version\", issue = \"49726\")]\n+pub use crate::unicode::UNICODE_VERSION;\n \n use crate::fmt::{self, Write};\n use crate::iter::FusedIterator;"}, {"sha": "b7fba88a540f9012dbc01efd9b5758d86e30709d", "filename": "src/libcore/unicode/bool_trie.rs", "status": "removed", "additions": 0, "deletions": 66, "changes": 66, "blob_url": "https://github.com/rust-lang/rust/blob/40ad8778513185c3597e99170371181d1e1d694c/src%2Flibcore%2Funicode%2Fbool_trie.rs", "raw_url": "https://github.com/rust-lang/rust/raw/40ad8778513185c3597e99170371181d1e1d694c/src%2Flibcore%2Funicode%2Fbool_trie.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibcore%2Funicode%2Fbool_trie.rs?ref=40ad8778513185c3597e99170371181d1e1d694c", "patch": "@@ -1,66 +0,0 @@\n-/// BoolTrie is a trie for representing a set of Unicode codepoints. It is\n-/// implemented with postfix compression (sharing of identical child nodes),\n-/// which gives both compact size and fast lookup.\n-///\n-/// The space of Unicode codepoints is divided into 3 subareas, each\n-/// represented by a trie with different depth. In the first (0..0x800), there\n-/// is no trie structure at all; each u64 entry corresponds to a bitvector\n-/// effectively holding 64 bool values.\n-///\n-/// In the second (0x800..0x10000), each child of the root node represents a\n-/// 64-wide subrange, but instead of storing the full 64-bit value of the leaf,\n-/// the trie stores an 8-bit index into a shared table of leaf values. This\n-/// exploits the fact that in reasonable sets, many such leaves can be shared.\n-///\n-/// In the third (0x10000..0x110000), each child of the root node represents a\n-/// 4096-wide subrange, and the trie stores an 8-bit index into a 64-byte slice\n-/// of a child tree. Each of these 64 bytes represents an index into the table\n-/// of shared 64-bit leaf values. This exploits the sparse structure in the\n-/// non-BMP range of most Unicode sets.\n-pub struct BoolTrie {\n-    // 0..0x800 (corresponding to 1 and 2 byte utf-8 sequences)\n-    pub r1: [u64; 32], // leaves\n-\n-    // 0x800..0x10000 (corresponding to 3 byte utf-8 sequences)\n-    pub r2: [u8; 992],      // first level\n-    pub r3: &'static [u64], // leaves\n-\n-    // 0x10000..0x110000 (corresponding to 4 byte utf-8 sequences)\n-    pub r4: [u8; 256],      // first level\n-    pub r5: &'static [u8],  // second level\n-    pub r6: &'static [u64], // leaves\n-}\n-impl BoolTrie {\n-    pub fn lookup(&self, c: char) -> bool {\n-        let c = c as u32;\n-        if c < 0x800 {\n-            trie_range_leaf(c, self.r1[(c >> 6) as usize])\n-        } else if c < 0x10000 {\n-            let child = self.r2[(c >> 6) as usize - 0x20];\n-            trie_range_leaf(c, self.r3[child as usize])\n-        } else {\n-            let child = self.r4[(c >> 12) as usize - 0x10];\n-            let leaf = self.r5[((child as usize) << 6) + ((c >> 6) as usize & 0x3f)];\n-            trie_range_leaf(c, self.r6[leaf as usize])\n-        }\n-    }\n-}\n-\n-pub struct SmallBoolTrie {\n-    pub(crate) r1: &'static [u8],  // first level\n-    pub(crate) r2: &'static [u64], // leaves\n-}\n-\n-impl SmallBoolTrie {\n-    pub fn lookup(&self, c: char) -> bool {\n-        let c = c as u32;\n-        match self.r1.get((c >> 6) as usize) {\n-            Some(&child) => trie_range_leaf(c, self.r2[child as usize]),\n-            None => false,\n-        }\n-    }\n-}\n-\n-fn trie_range_leaf(c: u32, bitmap_chunk: u64) -> bool {\n-    ((bitmap_chunk >> (c & 63)) & 1) != 0\n-}"}, {"sha": "3fa125e8fea15fc9921ce35af4660c76554f619c", "filename": "src/libcore/unicode/tables.rs", "status": "removed", "additions": 0, "deletions": 2235, "changes": 2235, "blob_url": "https://github.com/rust-lang/rust/blob/40ad8778513185c3597e99170371181d1e1d694c/src%2Flibcore%2Funicode%2Ftables.rs", "raw_url": "https://github.com/rust-lang/rust/raw/40ad8778513185c3597e99170371181d1e1d694c/src%2Flibcore%2Funicode%2Ftables.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibcore%2Funicode%2Ftables.rs?ref=40ad8778513185c3597e99170371181d1e1d694c"}, {"sha": "97df92a56da66ea2b0527a35d4a5eb28f6457390", "filename": "src/libcore/unicode/unicode.py", "status": "removed", "additions": 0, "deletions": 878, "changes": 878, "blob_url": "https://github.com/rust-lang/rust/blob/40ad8778513185c3597e99170371181d1e1d694c/src%2Flibcore%2Funicode%2Funicode.py", "raw_url": "https://github.com/rust-lang/rust/raw/40ad8778513185c3597e99170371181d1e1d694c/src%2Flibcore%2Funicode%2Funicode.py", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibcore%2Funicode%2Funicode.py?ref=40ad8778513185c3597e99170371181d1e1d694c", "patch": "@@ -1,878 +0,0 @@\n-#!/usr/bin/env python\n-\n-\"\"\"\n-Regenerate Unicode tables (tables.rs).\n-\"\"\"\n-\n-# This script uses the Unicode tables as defined\n-# in the UnicodeFiles class.\n-\n-# Since this should not require frequent updates, we just store this\n-# out-of-line and check the tables.rs file into git.\n-\n-# Note that the \"curl\" program is required for operation.\n-# This script is compatible with Python 2.7 and 3.x.\n-\n-import argparse\n-import datetime\n-import fileinput\n-import itertools\n-import os\n-import re\n-import textwrap\n-import subprocess\n-\n-from collections import defaultdict, namedtuple\n-\n-try:\n-    # Python 3\n-    from itertools import zip_longest\n-    from io import StringIO\n-except ImportError:\n-    # Python 2 compatibility\n-    zip_longest = itertools.izip_longest\n-    from StringIO import StringIO\n-\n-try:\n-    # Completely optional type hinting\n-    # (Python 2 compatible using comments,\n-    # see: https://mypy.readthedocs.io/en/latest/python2.html)\n-    # This is very helpful in typing-aware IDE like PyCharm.\n-    from typing import Any, Callable, Dict, Iterable, Iterator, List, Optional, Set, Tuple\n-except ImportError:\n-    pass\n-\n-\n-# We don't use enum.Enum because of Python 2.7 compatibility.\n-class UnicodeFiles(object):\n-    # ReadMe does not contain any Unicode data, we\n-    # only use it to extract versions.\n-    README = \"ReadMe.txt\"\n-\n-    DERIVED_CORE_PROPERTIES = \"DerivedCoreProperties.txt\"\n-    DERIVED_NORMALIZATION_PROPS = \"DerivedNormalizationProps.txt\"\n-    PROPS = \"PropList.txt\"\n-    SCRIPTS = \"Scripts.txt\"\n-    SPECIAL_CASING = \"SpecialCasing.txt\"\n-    UNICODE_DATA = \"UnicodeData.txt\"\n-\n-\n-# The order doesn't really matter (Python < 3.6 won't preserve it),\n-# we only want to aggregate all the file names.\n-ALL_UNICODE_FILES = tuple(\n-    value for name, value in UnicodeFiles.__dict__.items()\n-    if not name.startswith(\"_\")\n-)\n-\n-assert len(ALL_UNICODE_FILES) == 7, \"Unexpected number of unicode files\"\n-\n-# The directory this file is located in.\n-THIS_DIR = os.path.dirname(os.path.realpath(__file__))\n-\n-# Where to download the Unicode data.  The downloaded files\n-# will be placed in sub-directories named after Unicode version.\n-FETCH_DIR = os.path.join(THIS_DIR, \"downloaded\")\n-\n-FETCH_URL_LATEST = \"ftp://ftp.unicode.org/Public/UNIDATA/{filename}\"\n-FETCH_URL_VERSION = \"ftp://ftp.unicode.org/Public/{version}/ucd/{filename}\"\n-\n-PREAMBLE = \"\"\"\\\n-// NOTE: The following code was generated by \"./unicode.py\", do not edit directly\n-\n-#![allow(missing_docs, non_upper_case_globals, non_snake_case, clippy::unreadable_literal)]\n-\n-use crate::unicode::bool_trie::{{BoolTrie, SmallBoolTrie}};\n-use crate::unicode::version::UnicodeVersion;\n-\"\"\".format(year=datetime.datetime.now().year)\n-\n-# Mapping taken from Table 12 from:\n-# http://www.unicode.org/reports/tr44/#General_Category_Values\n-EXPANDED_CATEGORIES = {\n-    \"Lu\": [\"LC\", \"L\"], \"Ll\": [\"LC\", \"L\"], \"Lt\": [\"LC\", \"L\"],\n-    \"Lm\": [\"L\"], \"Lo\": [\"L\"],\n-    \"Mn\": [\"M\"], \"Mc\": [\"M\"], \"Me\": [\"M\"],\n-    \"Nd\": [\"N\"], \"Nl\": [\"N\"], \"No\": [\"N\"],\n-    \"Pc\": [\"P\"], \"Pd\": [\"P\"], \"Ps\": [\"P\"], \"Pe\": [\"P\"],\n-    \"Pi\": [\"P\"], \"Pf\": [\"P\"], \"Po\": [\"P\"],\n-    \"Sm\": [\"S\"], \"Sc\": [\"S\"], \"Sk\": [\"S\"], \"So\": [\"S\"],\n-    \"Zs\": [\"Z\"], \"Zl\": [\"Z\"], \"Zp\": [\"Z\"],\n-    \"Cc\": [\"C\"], \"Cf\": [\"C\"], \"Cs\": [\"C\"], \"Co\": [\"C\"], \"Cn\": [\"C\"],\n-}\n-\n-# This is the (inclusive) range of surrogate codepoints.\n-# These are not valid Rust characters.\n-SURROGATE_CODEPOINTS_RANGE = (0xd800, 0xdfff)\n-\n-UnicodeData = namedtuple(\n-    \"UnicodeData\", (\n-        # Conversions:\n-        \"to_upper\", \"to_lower\", \"to_title\",\n-\n-        # Decompositions: canonical decompositions, compatibility decomp\n-        \"canon_decomp\", \"compat_decomp\",\n-\n-        # Grouped: general categories and combining characters\n-        \"general_categories\", \"combines\",\n-    )\n-)\n-\n-UnicodeVersion = namedtuple(\n-    \"UnicodeVersion\", (\"major\", \"minor\", \"micro\", \"as_str\")\n-)\n-\n-\n-def fetch_files(version=None):\n-    # type: (str) -> UnicodeVersion\n-    \"\"\"\n-    Fetch all the Unicode files from unicode.org.\n-\n-    This will use cached files (stored in `FETCH_DIR`) if they exist,\n-    creating them if they don't.  In any case, the Unicode version\n-    is always returned.\n-\n-    :param version: The desired Unicode version, as string.\n-        (If None, defaults to latest final release available,\n-         querying the unicode.org service).\n-    \"\"\"\n-    have_version = check_stored_version(version)\n-    if have_version:\n-        return have_version\n-\n-    if version:\n-        # Check if the desired version exists on the server.\n-        get_fetch_url = lambda name: FETCH_URL_VERSION.format(version=version, filename=name)\n-    else:\n-        # Extract the latest version.\n-        get_fetch_url = lambda name: FETCH_URL_LATEST.format(filename=name)\n-\n-    readme_url = get_fetch_url(UnicodeFiles.README)\n-\n-    print(\"Fetching: {}\".format(readme_url))\n-    readme_content = subprocess.check_output((\"curl\", readme_url))\n-\n-    unicode_version = parse_readme_unicode_version(\n-        readme_content.decode(\"utf8\")\n-    )\n-\n-    download_dir = get_unicode_dir(unicode_version)\n-    if not os.path.exists(download_dir):\n-        # For 2.7 compat, we don't use `exist_ok=True`.\n-        os.makedirs(download_dir)\n-\n-    for filename in ALL_UNICODE_FILES:\n-        file_path = get_unicode_file_path(unicode_version, filename)\n-\n-        if os.path.exists(file_path):\n-            # Assume file on the server didn't change if it's been saved before.\n-            continue\n-\n-        if filename == UnicodeFiles.README:\n-            with open(file_path, \"wb\") as fd:\n-                fd.write(readme_content)\n-        else:\n-            url = get_fetch_url(filename)\n-            print(\"Fetching: {}\".format(url))\n-            subprocess.check_call((\"curl\", \"-o\", file_path, url))\n-\n-    return unicode_version\n-\n-\n-def check_stored_version(version):\n-    # type: (Optional[str]) -> Optional[UnicodeVersion]\n-    \"\"\"\n-    Given desired Unicode version, return the version\n-    if stored files are all present, and `None` otherwise.\n-    \"\"\"\n-    if not version:\n-        # If no desired version specified, we should check what's the latest\n-        # version, skipping stored version checks.\n-        return None\n-\n-    fetch_dir = os.path.join(FETCH_DIR, version)\n-\n-    for filename in ALL_UNICODE_FILES:\n-        file_path = os.path.join(fetch_dir, filename)\n-\n-        if not os.path.exists(file_path):\n-            return None\n-\n-    with open(os.path.join(fetch_dir, UnicodeFiles.README)) as fd:\n-        return parse_readme_unicode_version(fd.read())\n-\n-\n-def parse_readme_unicode_version(readme_content):\n-    # type: (str) -> UnicodeVersion\n-    \"\"\"\n-    Parse the Unicode version contained in their `ReadMe.txt` file.\n-    \"\"\"\n-    # \"Raw string\" is necessary for \\d not being treated as escape char\n-    # (for the sake of compat with future Python versions).\n-    # See: https://docs.python.org/3.6/whatsnew/3.6.html#deprecated-python-behavior\n-    pattern = r\"for Version (\\d+)\\.(\\d+)\\.(\\d+) of the Unicode\"\n-    groups = re.search(pattern, readme_content).groups()\n-\n-    return UnicodeVersion(*map(int, groups), as_str=\".\".join(groups))\n-\n-\n-def get_unicode_dir(unicode_version):\n-    # type: (UnicodeVersion) -> str\n-    \"\"\"\n-    Indicate in which parent dir the Unicode data files should be stored.\n-\n-    This returns a full, absolute path.\n-    \"\"\"\n-    return os.path.join(FETCH_DIR, unicode_version.as_str)\n-\n-\n-def get_unicode_file_path(unicode_version, filename):\n-    # type: (UnicodeVersion, str) -> str\n-    \"\"\"\n-    Indicate where the Unicode data file should be stored.\n-    \"\"\"\n-    return os.path.join(get_unicode_dir(unicode_version), filename)\n-\n-\n-def is_surrogate(n):\n-    # type: (int) -> bool\n-    \"\"\"\n-    Tell if given codepoint is a surrogate (not a valid Rust character).\n-    \"\"\"\n-    return SURROGATE_CODEPOINTS_RANGE[0] <= n <= SURROGATE_CODEPOINTS_RANGE[1]\n-\n-\n-def load_unicode_data(file_path):\n-    # type: (str) -> UnicodeData\n-    \"\"\"\n-    Load main Unicode data.\n-    \"\"\"\n-    # Conversions\n-    to_lower = {}   # type: Dict[int, Tuple[int, int, int]]\n-    to_upper = {}   # type: Dict[int, Tuple[int, int, int]]\n-    to_title = {}   # type: Dict[int, Tuple[int, int, int]]\n-\n-    # Decompositions\n-    compat_decomp = {}   # type: Dict[int, List[int]]\n-    canon_decomp = {}    # type: Dict[int, List[int]]\n-\n-    # Combining characters\n-    # FIXME: combines are not used\n-    combines = defaultdict(set)   # type: Dict[str, Set[int]]\n-\n-    # Categories\n-    general_categories = defaultdict(set)   # type: Dict[str, Set[int]]\n-    category_assigned_codepoints = set()    # type: Set[int]\n-\n-    all_codepoints = {}\n-\n-    range_start = -1\n-\n-    for line in fileinput.input(file_path):\n-        data = line.split(\";\")\n-        if len(data) != 15:\n-            continue\n-        codepoint = int(data[0], 16)\n-        if is_surrogate(codepoint):\n-            continue\n-        if range_start >= 0:\n-            for i in range(range_start, codepoint):\n-                all_codepoints[i] = data\n-            range_start = -1\n-        if data[1].endswith(\", First>\"):\n-            range_start = codepoint\n-            continue\n-        all_codepoints[codepoint] = data\n-\n-    for code, data in all_codepoints.items():\n-        (code_org, name, gencat, combine, bidi,\n-         decomp, deci, digit, num, mirror,\n-         old, iso, upcase, lowcase, titlecase) = data\n-\n-        # Generate char to char direct common and simple conversions:\n-\n-        # Uppercase to lowercase\n-        if lowcase != \"\" and code_org != lowcase:\n-            to_lower[code] = (int(lowcase, 16), 0, 0)\n-\n-        # Lowercase to uppercase\n-        if upcase != \"\" and code_org != upcase:\n-            to_upper[code] = (int(upcase, 16), 0, 0)\n-\n-        # Title case\n-        if titlecase.strip() != \"\" and code_org != titlecase:\n-            to_title[code] = (int(titlecase, 16), 0, 0)\n-\n-        # Store decomposition, if given\n-        if decomp:\n-            decompositions = decomp.split()[1:]\n-            decomp_code_points = [int(i, 16) for i in decompositions]\n-\n-            if decomp.startswith(\"<\"):\n-                # Compatibility decomposition\n-                compat_decomp[code] = decomp_code_points\n-            else:\n-                # Canonical decomposition\n-                canon_decomp[code] = decomp_code_points\n-\n-        # Place letter in categories as appropriate.\n-        for cat in itertools.chain((gencat, ), EXPANDED_CATEGORIES.get(gencat, [])):\n-            general_categories[cat].add(code)\n-            category_assigned_codepoints.add(code)\n-\n-        # Record combining class, if any.\n-        if combine != \"0\":\n-            combines[combine].add(code)\n-\n-    # Generate Not_Assigned from Assigned.\n-    general_categories[\"Cn\"] = get_unassigned_codepoints(category_assigned_codepoints)\n-\n-    # Other contains Not_Assigned\n-    general_categories[\"C\"].update(general_categories[\"Cn\"])\n-\n-    grouped_categories = group_categories(general_categories)\n-\n-    # FIXME: combines are not used\n-    return UnicodeData(\n-        to_lower=to_lower, to_upper=to_upper, to_title=to_title,\n-        compat_decomp=compat_decomp, canon_decomp=canon_decomp,\n-        general_categories=grouped_categories, combines=combines,\n-    )\n-\n-\n-def load_special_casing(file_path, unicode_data):\n-    # type: (str, UnicodeData) -> None\n-    \"\"\"\n-    Load special casing data and enrich given Unicode data.\n-    \"\"\"\n-    for line in fileinput.input(file_path):\n-        data = line.split(\"#\")[0].split(\";\")\n-        if len(data) == 5:\n-            code, lower, title, upper, _comment = data\n-        elif len(data) == 6:\n-            code, lower, title, upper, condition, _comment = data\n-            if condition.strip():  # Only keep unconditional mappins\n-                continue\n-        else:\n-            continue\n-        code = code.strip()\n-        lower = lower.strip()\n-        title = title.strip()\n-        upper = upper.strip()\n-        key = int(code, 16)\n-        for (map_, values) in ((unicode_data.to_lower, lower),\n-                               (unicode_data.to_upper, upper),\n-                               (unicode_data.to_title, title)):\n-            if values != code:\n-                split = values.split()\n-\n-                codepoints = list(itertools.chain(\n-                    (int(i, 16) for i in split),\n-                    (0 for _ in range(len(split), 3))\n-                ))\n-\n-                assert len(codepoints) == 3\n-                map_[key] = codepoints\n-\n-\n-def group_categories(mapping):\n-    # type: (Dict[Any, Iterable[int]]) -> Dict[str, List[Tuple[int, int]]]\n-    \"\"\"\n-    Group codepoints mapped in \"categories\".\n-    \"\"\"\n-    return {category: group_codepoints(codepoints)\n-            for category, codepoints in mapping.items()}\n-\n-\n-def group_codepoints(codepoints):\n-    # type: (Iterable[int]) -> List[Tuple[int, int]]\n-    \"\"\"\n-    Group integral values into continuous, disjoint value ranges.\n-\n-    Performs value deduplication.\n-\n-    :return: sorted list of pairs denoting start and end of codepoint\n-        group values, both ends inclusive.\n-\n-    >>> group_codepoints([1, 2, 10, 11, 12, 3, 4])\n-    [(1, 4), (10, 12)]\n-    >>> group_codepoints([1])\n-    [(1, 1)]\n-    >>> group_codepoints([1, 5, 6])\n-    [(1, 1), (5, 6)]\n-    >>> group_codepoints([])\n-    []\n-    \"\"\"\n-    sorted_codes = sorted(set(codepoints))\n-    result = []     # type: List[Tuple[int, int]]\n-\n-    if not sorted_codes:\n-        return result\n-\n-    next_codes = sorted_codes[1:]\n-    start_code = sorted_codes[0]\n-\n-    for code, next_code in zip_longest(sorted_codes, next_codes, fillvalue=None):\n-        if next_code is None or next_code - code != 1:\n-            result.append((start_code, code))\n-            start_code = next_code\n-\n-    return result\n-\n-\n-def ungroup_codepoints(codepoint_pairs):\n-    # type: (Iterable[Tuple[int, int]]) -> List[int]\n-    \"\"\"\n-    The inverse of group_codepoints -- produce a flat list of values\n-    from value range pairs.\n-\n-    >>> ungroup_codepoints([(1, 4), (10, 12)])\n-    [1, 2, 3, 4, 10, 11, 12]\n-    >>> ungroup_codepoints([(1, 1), (5, 6)])\n-    [1, 5, 6]\n-    >>> ungroup_codepoints(group_codepoints([1, 2, 7, 8]))\n-    [1, 2, 7, 8]\n-    >>> ungroup_codepoints([])\n-    []\n-    \"\"\"\n-    return list(itertools.chain.from_iterable(\n-        range(lo, hi + 1) for lo, hi in codepoint_pairs\n-    ))\n-\n-\n-def get_unassigned_codepoints(assigned_codepoints):\n-    # type: (Set[int]) -> Set[int]\n-    \"\"\"\n-    Given a set of \"assigned\" codepoints, return a set\n-    of these that are not in assigned and not surrogate.\n-    \"\"\"\n-    return {i for i in range(0, 0x110000)\n-            if i not in assigned_codepoints and not is_surrogate(i)}\n-\n-\n-def generate_table_lines(items, indent, wrap=98):\n-    # type: (Iterable[str], int, int) -> Iterator[str]\n-    \"\"\"\n-    Given table items, generate wrapped lines of text with comma-separated items.\n-\n-    This is a generator function.\n-\n-    :param wrap: soft wrap limit (characters per line), integer.\n-    \"\"\"\n-    line = \" \" * indent\n-    first = True\n-    for item in items:\n-        if len(line) + len(item) < wrap:\n-            if first:\n-                line += item\n-            else:\n-                line += \", \" + item\n-            first = False\n-        else:\n-            yield line + \",\\n\"\n-            line = \" \" * indent + item\n-\n-    yield line\n-\n-\n-def load_properties(file_path, interesting_props):\n-    # type: (str, Iterable[str]) -> Dict[str, List[Tuple[int, int]]]\n-    \"\"\"\n-    Load properties data and return in grouped form.\n-    \"\"\"\n-    props = defaultdict(list)   # type: Dict[str, List[Tuple[int, int]]]\n-    # \"Raw string\" is necessary for `\\.` and `\\w` not to be treated as escape chars\n-    # (for the sake of compat with future Python versions).\n-    # See: https://docs.python.org/3.6/whatsnew/3.6.html#deprecated-python-behavior\n-    re1 = re.compile(r\"^ *([0-9A-F]+) *; *(\\w+)\")\n-    re2 = re.compile(r\"^ *([0-9A-F]+)\\.\\.([0-9A-F]+) *; *(\\w+)\")\n-\n-    for line in fileinput.input(file_path):\n-        match = re1.match(line) or re2.match(line)\n-        if match:\n-            groups = match.groups()\n-\n-            if len(groups) == 2:\n-                # `re1` matched (2 groups).\n-                d_lo, prop = groups\n-                d_hi = d_lo\n-            else:\n-                d_lo, d_hi, prop = groups\n-        else:\n-            continue\n-\n-        if interesting_props and prop not in interesting_props:\n-            continue\n-\n-        lo_value = int(d_lo, 16)\n-        hi_value = int(d_hi, 16)\n-\n-        props[prop].append((lo_value, hi_value))\n-\n-    # Optimize if possible.\n-    for prop in props:\n-        props[prop] = group_codepoints(ungroup_codepoints(props[prop]))\n-\n-    return props\n-\n-\n-def escape_char(c):\n-    # type: (int) -> str\n-    r\"\"\"\n-    Escape a codepoint for use as Rust char literal.\n-\n-    Outputs are OK to use as Rust source code as char literals\n-    and they also include necessary quotes.\n-\n-    >>> escape_char(97)\n-    \"'\\\\u{61}'\"\n-    >>> escape_char(0)\n-    \"'\\\\0'\"\n-    \"\"\"\n-    return r\"'\\u{%x}'\" % c if c != 0 else r\"'\\0'\"\n-\n-\n-def format_char_pair(pair):\n-    # type: (Tuple[int, int]) -> str\n-    \"\"\"\n-    Format a pair of two Rust chars.\n-    \"\"\"\n-    return \"(%s,%s)\" % (escape_char(pair[0]), escape_char(pair[1]))\n-\n-\n-def generate_table(\n-    name,   # type: str\n-    items,  # type: List[Tuple[int, int]]\n-    decl_type=\"&[(char, char)]\",    # type: str\n-    is_pub=True,                    # type: bool\n-    format_item=format_char_pair,   # type: Callable[[Tuple[int, int]], str]\n-):\n-    # type: (...) -> Iterator[str]\n-    \"\"\"\n-    Generate a nicely formatted Rust constant \"table\" array.\n-\n-    This generates actual Rust code.\n-    \"\"\"\n-    pub_string = \"\"\n-    if is_pub:\n-        pub_string = \"pub \"\n-\n-    yield \"\\n\"\n-    yield \"    #[rustfmt::skip]\\n\"\n-    yield \"    %sconst %s: %s = &[\\n\" % (pub_string, name, decl_type)\n-\n-    data = []\n-    first = True\n-    for item in items:\n-        if not first:\n-            data.append(\",\")\n-        first = False\n-        data.extend(format_item(item))\n-\n-    for table_line in generate_table_lines(\"\".join(data).split(\",\"), 8):\n-        yield table_line\n-\n-    yield \"\\n    ];\\n\"\n-\n-\n-def compute_trie(raw_data, chunk_size):\n-    # type: (List[int], int) -> Tuple[List[int], List[int]]\n-    \"\"\"\n-    Compute postfix-compressed trie.\n-\n-    See: bool_trie.rs for more details.\n-\n-    >>> compute_trie([1, 2, 3, 1, 2, 3, 4, 5, 6], 3)\n-    ([0, 0, 1], [1, 2, 3, 4, 5, 6])\n-    >>> compute_trie([1, 2, 3, 1, 2, 4, 4, 5, 6], 3)\n-    ([0, 1, 2], [1, 2, 3, 1, 2, 4, 4, 5, 6])\n-    \"\"\"\n-    root = []\n-    childmap = {}       # type: Dict[Tuple[int, ...], int]\n-    child_data = []\n-\n-    assert len(raw_data) % chunk_size == 0, \"Chunks must be equally sized\"\n-\n-    for i in range(len(raw_data) // chunk_size):\n-        data = raw_data[i * chunk_size : (i + 1) * chunk_size]\n-\n-        # Postfix compression of child nodes (data chunks)\n-        # (identical child nodes are shared).\n-\n-        # Make a tuple out of the list so it's hashable.\n-        child = tuple(data)\n-        if child not in childmap:\n-            childmap[child] = len(childmap)\n-            child_data.extend(data)\n-\n-        root.append(childmap[child])\n-\n-    return root, child_data\n-\n-\n-def generate_bool_trie(name, codepoint_ranges, is_pub=False):\n-    # type: (str, List[Tuple[int, int]], bool) -> Iterator[str]\n-    \"\"\"\n-    Generate Rust code for BoolTrie struct.\n-\n-    This yields string fragments that should be joined to produce\n-    the final string.\n-\n-    See: `bool_trie.rs`.\n-    \"\"\"\n-    chunk_size = 64\n-    rawdata = [False] * 0x110000\n-    for (lo, hi) in codepoint_ranges:\n-        for cp in range(lo, hi + 1):\n-            rawdata[cp] = True\n-\n-    # Convert to bitmap chunks of `chunk_size` bits each.\n-    chunks = []\n-    for i in range(0x110000 // chunk_size):\n-        chunk = 0\n-        for j in range(chunk_size):\n-            if rawdata[i * chunk_size + j]:\n-                chunk |= 1 << j\n-        chunks.append(chunk)\n-\n-    pub_string = \"\"\n-    if is_pub:\n-        pub_string = \"pub \"\n-\n-    yield \"\\n\"\n-    yield \"    #[rustfmt::skip]\\n\"\n-    yield \"    %sconst %s: &super::BoolTrie = &super::BoolTrie {\\n\" % (pub_string, name)\n-    yield \"        r1: [\\n\"\n-    data = (\"0x%016x\" % chunk for chunk in chunks[:0x800 // chunk_size])\n-    for fragment in generate_table_lines(data, 12):\n-        yield fragment\n-    yield \"\\n        ],\\n\"\n-\n-    # 0x800..0x10000 trie\n-    (r2, r3) = compute_trie(chunks[0x800 // chunk_size : 0x10000 // chunk_size], 64 // chunk_size)\n-    yield \"        r2: [\\n\"\n-    data = map(str, r2)\n-    for fragment in generate_table_lines(data, 12):\n-        yield fragment\n-    yield \"\\n        ],\\n\"\n-\n-    yield \"        r3: &[\\n\"\n-    data = (\"0x%016x\" % node for node in r3)\n-    for fragment in generate_table_lines(data, 12):\n-        yield fragment\n-    yield \"\\n        ],\\n\"\n-\n-    # 0x10000..0x110000 trie\n-    (mid, r6) = compute_trie(chunks[0x10000 // chunk_size : 0x110000 // chunk_size],\n-                             64 // chunk_size)\n-    (r4, r5) = compute_trie(mid, 64)\n-\n-    yield \"        r4: [\\n\"\n-    data = map(str, r4)\n-    for fragment in generate_table_lines(data, 12):\n-        yield fragment\n-    yield \"\\n        ],\\n\"\n-\n-    yield \"        r5: &[\\n\"\n-    data = map(str, r5)\n-    for fragment in generate_table_lines(data, 12):\n-        yield fragment\n-    yield \"\\n        ],\\n\"\n-\n-    yield \"        r6: &[\\n\"\n-    data = (\"0x%016x\" % node for node in r6)\n-    for fragment in generate_table_lines(data, 12):\n-        yield fragment\n-    yield \"\\n        ],\\n\"\n-\n-    yield \"    };\\n\"\n-\n-\n-def generate_small_bool_trie(name, codepoint_ranges, is_pub=False):\n-    # type: (str, List[Tuple[int, int]], bool) -> Iterator[str]\n-    \"\"\"\n-    Generate Rust code for `SmallBoolTrie` struct.\n-\n-    See: `bool_trie.rs`.\n-    \"\"\"\n-    last_chunk = max(hi // 64 for (lo, hi) in codepoint_ranges)\n-    n_chunks = last_chunk + 1\n-    chunks = [0] * n_chunks\n-    for (lo, hi) in codepoint_ranges:\n-        for cp in range(lo, hi + 1):\n-            assert cp // 64 < len(chunks)\n-            chunks[cp // 64] |= 1 << (cp & 63)\n-\n-    pub_string = \"\"\n-    if is_pub:\n-        pub_string = \"pub \"\n-\n-    yield \"\\n\"\n-    yield \"    #[rustfmt::skip]\\n\"\n-    yield (\"    %sconst %s: &super::SmallBoolTrie = &super::SmallBoolTrie {\\n\"\n-           % (pub_string, name))\n-\n-    (r1, r2) = compute_trie(chunks, 1)\n-\n-    yield \"        r1: &[\\n\"\n-    data = (str(node) for node in r1)\n-    for fragment in generate_table_lines(data, 12):\n-        yield fragment\n-    yield \"\\n        ],\\n\"\n-\n-    yield \"        r2: &[\\n\"\n-    data = (\"0x%016x\" % node for node in r2)\n-    for fragment in generate_table_lines(data, 12):\n-        yield fragment\n-    yield \"\\n        ],\\n\"\n-\n-    yield \"    };\\n\"\n-\n-\n-def generate_property_module(mod, grouped_categories, category_subset):\n-    # type: (str, Dict[str, List[Tuple[int, int]]], Iterable[str]) -> Iterator[str]\n-    \"\"\"\n-    Generate Rust code for module defining properties.\n-    \"\"\"\n-\n-    yield \"pub(crate) mod %s {\" % mod\n-    for cat in sorted(category_subset):\n-        if cat in (\"Cc\", \"White_Space\"):\n-            generator = generate_small_bool_trie(\"%s_table\" % cat, grouped_categories[cat])\n-        else:\n-            generator = generate_bool_trie(\"%s_table\" % cat, grouped_categories[cat])\n-\n-        for fragment in generator:\n-            yield fragment\n-\n-        yield \"\\n\"\n-        yield \"    pub fn %s(c: char) -> bool {\\n\" % cat\n-        yield \"        %s_table.lookup(c)\\n\" % cat\n-        yield \"    }\\n\"\n-\n-    yield \"}\\n\\n\"\n-\n-\n-def generate_conversions_module(unicode_data):\n-    # type: (UnicodeData) -> Iterator[str]\n-    \"\"\"\n-    Generate Rust code for module defining conversions.\n-    \"\"\"\n-\n-    yield \"pub(crate) mod conversions {\"\n-    yield \"\"\"\n-    pub fn to_lower(c: char) -> [char; 3] {\n-        match bsearch_case_table(c, to_lowercase_table) {\n-            None => [c, '\\\\0', '\\\\0'],\n-            Some(index) => to_lowercase_table[index].1,\n-        }\n-    }\n-\n-    pub fn to_upper(c: char) -> [char; 3] {\n-        match bsearch_case_table(c, to_uppercase_table) {\n-            None => [c, '\\\\0', '\\\\0'],\n-            Some(index) => to_uppercase_table[index].1,\n-        }\n-    }\n-\n-    fn bsearch_case_table(c: char, table: &[(char, [char; 3])]) -> Option<usize> {\n-        table.binary_search_by(|&(key, _)| key.cmp(&c)).ok()\n-    }\\n\"\"\"\n-\n-    decl_type = \"&[(char, [char; 3])]\"\n-    format_conversion = lambda x: \"({},[{},{},{}])\".format(*(\n-        escape_char(c) for c in (x[0], x[1][0], x[1][1], x[1][2])\n-    ))\n-\n-    for fragment in generate_table(\n-        name=\"to_lowercase_table\",\n-        items=sorted(unicode_data.to_lower.items(), key=lambda x: x[0]),\n-        decl_type=decl_type,\n-        is_pub=False,\n-        format_item=format_conversion\n-    ):\n-        yield fragment\n-\n-    for fragment in generate_table(\n-        name=\"to_uppercase_table\",\n-        items=sorted(unicode_data.to_upper.items(), key=lambda x: x[0]),\n-        decl_type=decl_type,\n-        is_pub=False,\n-        format_item=format_conversion\n-    ):\n-        yield fragment\n-\n-    yield \"}\\n\"\n-\n-\n-def parse_args():\n-    # type: () -> argparse.Namespace\n-    \"\"\"\n-    Parse command line arguments.\n-    \"\"\"\n-    parser = argparse.ArgumentParser(description=__doc__)\n-    parser.add_argument(\"-v\", \"--version\", default=None, type=str,\n-                        help=\"Unicode version to use (if not specified,\"\n-                             \" defaults to latest release).\")\n-\n-    return parser.parse_args()\n-\n-\n-def main():\n-    # type: () -> None\n-    \"\"\"\n-    Script entry point.\n-    \"\"\"\n-    args = parse_args()\n-\n-    unicode_version = fetch_files(args.version)\n-    print(\"Using Unicode version: {}\".format(unicode_version.as_str))\n-\n-    # All the writing happens entirely in memory, we only write to file\n-    # once we have generated the file content (it's not very large, <1 MB).\n-    buf = StringIO()\n-    buf.write(PREAMBLE)\n-\n-    unicode_version_notice = textwrap.dedent(\"\"\"\n-    /// The version of [Unicode](http://www.unicode.org/) that the Unicode parts of\n-    /// `char` and `str` methods are based on.\n-    #[unstable(feature = \"unicode_version\", issue = \"49726\")]\n-    pub const UNICODE_VERSION: UnicodeVersion =\n-        UnicodeVersion {{ major: {v.major}, minor: {v.minor}, micro: {v.micro}, _priv: () }};\n-    \"\"\").format(v=unicode_version)\n-    buf.write(unicode_version_notice)\n-\n-    get_path = lambda f: get_unicode_file_path(unicode_version, f)\n-\n-    unicode_data = load_unicode_data(get_path(UnicodeFiles.UNICODE_DATA))\n-    load_special_casing(get_path(UnicodeFiles.SPECIAL_CASING), unicode_data)\n-\n-    want_derived = {\"Alphabetic\", \"Lowercase\", \"Uppercase\",\n-                    \"Cased\", \"Case_Ignorable\", \"Grapheme_Extend\"}\n-    derived = load_properties(get_path(UnicodeFiles.DERIVED_CORE_PROPERTIES), want_derived)\n-\n-    props = load_properties(get_path(UnicodeFiles.PROPS),\n-                            {\"White_Space\", \"Join_Control\", \"Noncharacter_Code_Point\"})\n-\n-    # Category tables\n-    for (name, categories, category_subset) in (\n-            (\"general_category\", unicode_data.general_categories, [\"N\", \"Cc\"]),\n-            (\"derived_property\", derived, want_derived),\n-            (\"property\", props, [\"White_Space\"])\n-    ):\n-        for fragment in generate_property_module(name, categories, category_subset):\n-            buf.write(fragment)\n-\n-    for fragment in generate_conversions_module(unicode_data):\n-        buf.write(fragment)\n-\n-    tables_rs_path = os.path.join(THIS_DIR, \"tables.rs\")\n-\n-    # Actually write out the file content.\n-    # Will overwrite the file if it exists.\n-    with open(tables_rs_path, \"w\") as fd:\n-        fd.write(buf.getvalue())\n-\n-    print(\"Regenerated tables.rs.\")\n-\n-\n-if __name__ == \"__main__\":\n-    main()"}, {"sha": "da4cd4e9b1da1900e5703b5a2cc7f3ad65a77d90", "filename": "src/libcore/unicode/unicode_data.rs", "status": "added", "additions": 2343, "deletions": 0, "changes": 2343, "blob_url": "https://github.com/rust-lang/rust/blob/efcda047397262f403df54d9f5e569dd32704168/src%2Flibcore%2Funicode%2Funicode_data.rs", "raw_url": "https://github.com/rust-lang/rust/raw/efcda047397262f403df54d9f5e569dd32704168/src%2Flibcore%2Funicode%2Funicode_data.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibcore%2Funicode%2Funicode_data.rs?ref=efcda047397262f403df54d9f5e569dd32704168"}]}