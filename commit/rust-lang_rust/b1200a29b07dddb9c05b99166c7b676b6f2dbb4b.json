{"sha": "b1200a29b07dddb9c05b99166c7b676b6f2dbb4b", "node_id": "MDY6Q29tbWl0NzI0NzEyOmIxMjAwYTI5YjA3ZGRkYjljMDViOTkxNjZjN2I2NzZiNmYyZGJiNGI=", "commit": {"author": {"name": "Mazdak Farrokhzad", "email": "twingoow@gmail.com", "date": "2019-01-13T16:21:39Z"}, "committer": {"name": "GitHub", "email": "noreply@github.com", "date": "2019-01-13T16:21:39Z"}, "message": "Rollup merge of #57004 - nnethercote:TS-change-Stream, r=petrochenkov\n\nMake `TokenStream` less recursive.\n\n`TokenStream` is currently recursive in *two* ways:\n\n- the `TokenTree` variant contains a `ThinTokenStream`, which can\n  contain a `TokenStream`;\n\n- the `TokenStream` variant contains a `Vec<TokenStream>`.\n\nThe latter is not necessary and causes significant complexity. This\ncommit replaces it with the simpler `Vec<(TokenTree, IsJoint)>`.\n\nThis reduces complexity significantly. In particular, `StreamCursor` is\neliminated, and `Cursor` becomes much simpler, consisting now of just a\n`TokenStream` and an index.\n\nThe commit also removes the `Extend` impl for `TokenStream`, because it\nis only used in tests. (The commit also removes those tests.)\n\nOverall, the commit reduces the number of lines of code by almost 200.", "tree": {"sha": "1bc3034952a0cc72a6f85a2d8218da8a7734e9cb", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/1bc3034952a0cc72a6f85a2d8218da8a7734e9cb"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/b1200a29b07dddb9c05b99166c7b676b6f2dbb4b", "comment_count": 0, "verification": {"verified": true, "reason": "valid", "signature": "-----BEGIN PGP SIGNATURE-----\n\nwsBcBAABCAAQBQJcO2WTCRBK7hj4Ov3rIwAAdHIIAB6D/Ey9ihpEL6KLeDF6CAJg\ndzz0B4J/ssas8mnbh35f52K/Inb2iHunrP2/EhInpsfSn2lDDZRrenc7RQnjWzTC\ncE/PoNqAAr+Svk+pgGm9WPNLISitrjA3p8/gcUSldzV7VbUpWuWcfAROvZkIgqol\nImA2kuHS2GM+jRrzeIlL01QQ+ILNeI4tCjPp5Od/NuHii79IewRTkyxZgNCkj0ls\nj+0j5O/o9iTwBAEYf88vukBfbtsamIK9yL2BItDjl3OhzruoOWd8PDtWhJEdofn9\ncnjZDA9v/Cx0684mUhR770INB+D2VhfWJKRqi2u5Zg1zDUyG/lIZEWdNZBLjbFU=\n=1ze5\n-----END PGP SIGNATURE-----\n", "payload": "tree 1bc3034952a0cc72a6f85a2d8218da8a7734e9cb\nparent d45bef9db62a0797c6dd3b06e21db1a0acd8cfe7\nparent e80a93040ffbbb7eb8013f1dcd3b594ce8a631cd\nauthor Mazdak Farrokhzad <twingoow@gmail.com> 1547396499 +0100\ncommitter GitHub <noreply@github.com> 1547396499 +0100\n\nRollup merge of #57004 - nnethercote:TS-change-Stream, r=petrochenkov\n\nMake `TokenStream` less recursive.\n\n`TokenStream` is currently recursive in *two* ways:\n\n- the `TokenTree` variant contains a `ThinTokenStream`, which can\n  contain a `TokenStream`;\n\n- the `TokenStream` variant contains a `Vec<TokenStream>`.\n\nThe latter is not necessary and causes significant complexity. This\ncommit replaces it with the simpler `Vec<(TokenTree, IsJoint)>`.\n\nThis reduces complexity significantly. In particular, `StreamCursor` is\neliminated, and `Cursor` becomes much simpler, consisting now of just a\n`TokenStream` and an index.\n\nThe commit also removes the `Extend` impl for `TokenStream`, because it\nis only used in tests. (The commit also removes those tests.)\n\nOverall, the commit reduces the number of lines of code by almost 200.\n"}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/b1200a29b07dddb9c05b99166c7b676b6f2dbb4b", "html_url": "https://github.com/rust-lang/rust/commit/b1200a29b07dddb9c05b99166c7b676b6f2dbb4b", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/b1200a29b07dddb9c05b99166c7b676b6f2dbb4b/comments", "author": {"login": "Centril", "id": 855702, "node_id": "MDQ6VXNlcjg1NTcwMg==", "avatar_url": "https://avatars.githubusercontent.com/u/855702?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Centril", "html_url": "https://github.com/Centril", "followers_url": "https://api.github.com/users/Centril/followers", "following_url": "https://api.github.com/users/Centril/following{/other_user}", "gists_url": "https://api.github.com/users/Centril/gists{/gist_id}", "starred_url": "https://api.github.com/users/Centril/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Centril/subscriptions", "organizations_url": "https://api.github.com/users/Centril/orgs", "repos_url": "https://api.github.com/users/Centril/repos", "events_url": "https://api.github.com/users/Centril/events{/privacy}", "received_events_url": "https://api.github.com/users/Centril/received_events", "type": "User", "site_admin": false}, "committer": {"login": "web-flow", "id": 19864447, "node_id": "MDQ6VXNlcjE5ODY0NDQ3", "avatar_url": "https://avatars.githubusercontent.com/u/19864447?v=4", "gravatar_id": "", "url": "https://api.github.com/users/web-flow", "html_url": "https://github.com/web-flow", "followers_url": "https://api.github.com/users/web-flow/followers", "following_url": "https://api.github.com/users/web-flow/following{/other_user}", "gists_url": "https://api.github.com/users/web-flow/gists{/gist_id}", "starred_url": "https://api.github.com/users/web-flow/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/web-flow/subscriptions", "organizations_url": "https://api.github.com/users/web-flow/orgs", "repos_url": "https://api.github.com/users/web-flow/repos", "events_url": "https://api.github.com/users/web-flow/events{/privacy}", "received_events_url": "https://api.github.com/users/web-flow/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "d45bef9db62a0797c6dd3b06e21db1a0acd8cfe7", "url": "https://api.github.com/repos/rust-lang/rust/commits/d45bef9db62a0797c6dd3b06e21db1a0acd8cfe7", "html_url": "https://github.com/rust-lang/rust/commit/d45bef9db62a0797c6dd3b06e21db1a0acd8cfe7"}, {"sha": "e80a93040ffbbb7eb8013f1dcd3b594ce8a631cd", "url": "https://api.github.com/repos/rust-lang/rust/commits/e80a93040ffbbb7eb8013f1dcd3b594ce8a631cd", "html_url": "https://github.com/rust-lang/rust/commit/e80a93040ffbbb7eb8013f1dcd3b594ce8a631cd"}], "stats": {"total": 489, "additions": 148, "deletions": 341}, "files": [{"sha": "d03563f8891aa56982862e1dda9ce45344a9f96d", "filename": "src/libsyntax/attr/mod.rs", "status": "modified", "additions": 5, "deletions": 3, "changes": 8, "blob_url": "https://github.com/rust-lang/rust/blob/b1200a29b07dddb9c05b99166c7b676b6f2dbb4b/src%2Flibsyntax%2Fattr%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/b1200a29b07dddb9c05b99166c7b676b6f2dbb4b/src%2Flibsyntax%2Fattr%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fattr%2Fmod.rs?ref=b1200a29b07dddb9c05b99166c7b676b6f2dbb4b", "patch": "@@ -472,7 +472,7 @@ impl MetaItem {\n                                          Token::from_ast_ident(segment.ident)).into());\n             last_pos = segment.ident.span.hi();\n         }\n-        idents.push(self.node.tokens(self.span));\n+        self.node.tokens(self.span).append_to_tree_and_joint_vec(&mut idents);\n         TokenStream::new(idents)\n     }\n \n@@ -529,15 +529,17 @@ impl MetaItemKind {\n         match *self {\n             MetaItemKind::Word => TokenStream::empty(),\n             MetaItemKind::NameValue(ref lit) => {\n-                TokenStream::new(vec![TokenTree::Token(span, Token::Eq).into(), lit.tokens()])\n+                let mut vec = vec![TokenTree::Token(span, Token::Eq).into()];\n+                lit.tokens().append_to_tree_and_joint_vec(&mut vec);\n+                TokenStream::new(vec)\n             }\n             MetaItemKind::List(ref list) => {\n                 let mut tokens = Vec::new();\n                 for (i, item) in list.iter().enumerate() {\n                     if i > 0 {\n                         tokens.push(TokenTree::Token(span, Token::Comma).into());\n                     }\n-                    tokens.push(item.node.tokens());\n+                    item.node.tokens().append_to_tree_and_joint_vec(&mut tokens);\n                 }\n                 TokenTree::Delimited(\n                     DelimSpan::from_single(span),"}, {"sha": "c3124144009ab61bfc2b7f589d8df50e59d3586c", "filename": "src/libsyntax/ext/quote.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/b1200a29b07dddb9c05b99166c7b676b6f2dbb4b/src%2Flibsyntax%2Fext%2Fquote.rs", "raw_url": "https://github.com/rust-lang/rust/raw/b1200a29b07dddb9c05b99166c7b676b6f2dbb4b/src%2Flibsyntax%2Fext%2Fquote.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fext%2Fquote.rs?ref=b1200a29b07dddb9c05b99166c7b676b6f2dbb4b", "patch": "@@ -233,7 +233,7 @@ pub mod rt {\n                     self.span, token::Token::from_ast_ident(segment.ident)\n                 ).into());\n             }\n-            inner.push(self.tokens.clone());\n+            self.tokens.clone().append_to_tree_and_joint_vec(&mut inner);\n \n             let delim_span = DelimSpan::from_single(self.span);\n             r.push(TokenTree::Delimited("}, {"sha": "0ef2d3b749d810f962e5b226fba17f94a58e0eac", "filename": "src/libsyntax/ext/tt/transcribe.rs", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "blob_url": "https://github.com/rust-lang/rust/blob/b1200a29b07dddb9c05b99166c7b676b6f2dbb4b/src%2Flibsyntax%2Fext%2Ftt%2Ftranscribe.rs", "raw_url": "https://github.com/rust-lang/rust/raw/b1200a29b07dddb9c05b99166c7b676b6f2dbb4b/src%2Flibsyntax%2Fext%2Ftt%2Ftranscribe.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fext%2Ftt%2Ftranscribe.rs?ref=b1200a29b07dddb9c05b99166c7b676b6f2dbb4b", "patch": "@@ -7,7 +7,7 @@ use fold::noop_fold_tt;\n use parse::token::{self, Token, NtTT};\n use smallvec::SmallVec;\n use syntax_pos::DUMMY_SP;\n-use tokenstream::{TokenStream, TokenTree, DelimSpan};\n+use tokenstream::{DelimSpan, TokenStream, TokenTree, TreeAndJoint};\n \n use rustc_data_structures::fx::FxHashMap;\n use rustc_data_structures::sync::Lrc;\n@@ -63,7 +63,7 @@ pub fn transcribe(cx: &ExtCtxt,\n     let mut stack: SmallVec<[Frame; 1]> = smallvec![Frame::new(src)];\n     let interpolations = interp.unwrap_or_else(FxHashMap::default); /* just a convenience */\n     let mut repeats = Vec::new();\n-    let mut result: Vec<TokenStream> = Vec::new();\n+    let mut result: Vec<TreeAndJoint> = Vec::new();\n     let mut result_stack = Vec::new();\n \n     loop {\n@@ -78,7 +78,7 @@ pub fn transcribe(cx: &ExtCtxt,\n                     if let Some(sep) = sep.clone() {\n                         // repeat same span, I guess\n                         let prev_span = match result.last() {\n-                            Some(stream) => stream.trees().next().unwrap().span(),\n+                            Some((tt, _)) => tt.span(),\n                             None => DUMMY_SP,\n                         };\n                         result.push(TokenTree::Token(prev_span, sep).into());"}, {"sha": "d219f29f06c204ba2145ec8a9a3da8195e29a23f", "filename": "src/libsyntax/parse/lexer/tokentrees.rs", "status": "modified", "additions": 4, "deletions": 4, "changes": 8, "blob_url": "https://github.com/rust-lang/rust/blob/b1200a29b07dddb9c05b99166c7b676b6f2dbb4b/src%2Flibsyntax%2Fparse%2Flexer%2Ftokentrees.rs", "raw_url": "https://github.com/rust-lang/rust/raw/b1200a29b07dddb9c05b99166c7b676b6f2dbb4b/src%2Flibsyntax%2Fparse%2Flexer%2Ftokentrees.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Flexer%2Ftokentrees.rs?ref=b1200a29b07dddb9c05b99166c7b676b6f2dbb4b", "patch": "@@ -1,7 +1,7 @@\n use print::pprust::token_to_string;\n use parse::lexer::StringReader;\n use parse::{token, PResult};\n-use tokenstream::{DelimSpan, IsJoint::*, TokenStream, TokenTree};\n+use tokenstream::{DelimSpan, IsJoint::*, TokenStream, TokenTree, TreeAndJoint};\n \n impl<'a> StringReader<'a> {\n     // Parse a stream of tokens into a list of `TokenTree`s, up to an `Eof`.\n@@ -33,7 +33,7 @@ impl<'a> StringReader<'a> {\n         }\n     }\n \n-    fn parse_token_tree(&mut self) -> PResult<'a, TokenStream> {\n+    fn parse_token_tree(&mut self) -> PResult<'a, TreeAndJoint> {\n         let sm = self.sess.source_map();\n         match self.token {\n             token::Eof => {\n@@ -156,7 +156,7 @@ impl<'a> StringReader<'a> {\n                 Ok(TokenTree::Delimited(\n                     delim_span,\n                     delim,\n-                    tts.into(),\n+                    tts.into()\n                 ).into())\n             },\n             token::CloseDelim(_) => {\n@@ -176,7 +176,7 @@ impl<'a> StringReader<'a> {\n                 let raw = self.span_src_raw;\n                 self.real_token();\n                 let is_joint = raw.hi() == self.span_src_raw.lo() && token::is_op(&self.token);\n-                Ok(TokenStream::Tree(tt, if is_joint { Joint } else { NonJoint }))\n+                Ok((tt, if is_joint { Joint } else { NonJoint }))\n             }\n         }\n     }"}, {"sha": "5c8ed94731afbb16785163a68e350cfe0714bfc1", "filename": "src/libsyntax/parse/parser.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/b1200a29b07dddb9c05b99166c7b676b6f2dbb4b/src%2Flibsyntax%2Fparse%2Fparser.rs", "raw_url": "https://github.com/rust-lang/rust/raw/b1200a29b07dddb9c05b99166c7b676b6f2dbb4b/src%2Flibsyntax%2Fparse%2Fparser.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Fparser.rs?ref=b1200a29b07dddb9c05b99166c7b676b6f2dbb4b", "patch": "@@ -2914,7 +2914,7 @@ impl<'a> Parser<'a> {\n                 TokenTree::Delimited(\n                     frame.span,\n                     frame.delim,\n-                    frame.tree_cursor.original_stream().into(),\n+                    frame.tree_cursor.stream.into(),\n                 )\n             },\n             token::CloseDelim(_) | token::Eof => unreachable!(),"}, {"sha": "fb72ef9c956ce5057041339d2645dbb56b31c1d8", "filename": "src/libsyntax/tokenstream.rs", "status": "modified", "additions": 127, "deletions": 323, "changes": 450, "blob_url": "https://github.com/rust-lang/rust/blob/b1200a29b07dddb9c05b99166c7b676b6f2dbb4b/src%2Flibsyntax%2Ftokenstream.rs", "raw_url": "https://github.com/rust-lang/rust/raw/b1200a29b07dddb9c05b99166c7b676b6f2dbb4b/src%2Flibsyntax%2Ftokenstream.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Ftokenstream.rs?ref=b1200a29b07dddb9c05b99166c7b676b6f2dbb4b", "patch": "@@ -147,9 +147,11 @@ impl TokenTree {\n pub enum TokenStream {\n     Empty,\n     Tree(TokenTree, IsJoint),\n-    Stream(Lrc<Vec<TokenStream>>),\n+    Stream(Lrc<Vec<TreeAndJoint>>),\n }\n \n+pub type TreeAndJoint = (TokenTree, IsJoint);\n+\n // `TokenStream` is used a lot. Make sure it doesn't unintentionally get bigger.\n #[cfg(target_arch = \"x86_64\")]\n static_assert!(MEM_SIZE_OF_TOKEN_STREAM: mem::size_of::<TokenStream>() == 32);\n@@ -173,16 +175,14 @@ impl TokenStream {\n             while let Some((pos, ts)) = iter.next() {\n                 if let Some((_, next)) = iter.peek() {\n                     let sp = match (&ts, &next) {\n-                        (TokenStream::Tree(TokenTree::Token(_, token::Token::Comma), NonJoint), _) |\n-                        (_, TokenStream::Tree(TokenTree::Token(_, token::Token::Comma), NonJoint))\n-                          => continue,\n-                        (TokenStream::Tree(TokenTree::Token(sp, _), NonJoint), _) => *sp,\n-                        (TokenStream::Tree(TokenTree::Delimited(sp, ..), NonJoint), _) =>\n-                            sp.entire(),\n+                        ((TokenTree::Token(_, token::Token::Comma), NonJoint), _) |\n+                        (_, (TokenTree::Token(_, token::Token::Comma), NonJoint)) => continue,\n+                        ((TokenTree::Token(sp, _), NonJoint), _) => *sp,\n+                        ((TokenTree::Delimited(sp, ..), NonJoint), _) => sp.entire(),\n                         _ => continue,\n                     };\n                     let sp = sp.shrink_to_hi();\n-                    let comma = TokenStream::Tree(TokenTree::Token(sp, token::Comma), NonJoint);\n+                    let comma = (TokenTree::Token(sp, token::Comma), NonJoint);\n                     suggestion = Some((pos, comma, sp));\n                 }\n             }\n@@ -200,8 +200,14 @@ impl TokenStream {\n }\n \n impl From<TokenTree> for TokenStream {\n-    fn from(tt: TokenTree) -> TokenStream {\n-        TokenStream::Tree(tt, NonJoint)\n+    fn from(tree: TokenTree) -> TokenStream {\n+        TokenStream::Tree(tree, NonJoint)\n+    }\n+}\n+\n+impl From<TokenTree> for TreeAndJoint {\n+    fn from(tree: TokenTree) -> TreeAndJoint {\n+        (tree, NonJoint)\n     }\n }\n \n@@ -213,56 +219,7 @@ impl From<Token> for TokenStream {\n \n impl<T: Into<TokenStream>> iter::FromIterator<T> for TokenStream {\n     fn from_iter<I: IntoIterator<Item = T>>(iter: I) -> Self {\n-        TokenStream::new(iter.into_iter().map(Into::into).collect::<Vec<_>>())\n-    }\n-}\n-\n-impl Extend<TokenStream> for TokenStream {\n-    fn extend<I: IntoIterator<Item = TokenStream>>(&mut self, iter: I) {\n-        let iter = iter.into_iter();\n-        let this = mem::replace(self, TokenStream::Empty);\n-\n-        // Vector of token streams originally in self.\n-        let tts: Vec<TokenStream> = match this {\n-            TokenStream::Empty => {\n-                let mut vec = Vec::new();\n-                vec.reserve(iter.size_hint().0);\n-                vec\n-            }\n-            TokenStream::Tree(..) => {\n-                let mut vec = Vec::new();\n-                vec.reserve(1 + iter.size_hint().0);\n-                vec.push(this);\n-                vec\n-            }\n-            TokenStream::Stream(rc_vec) => match Lrc::try_unwrap(rc_vec) {\n-                Ok(mut vec) => {\n-                    // Extend in place using the existing capacity if possible.\n-                    // This is the fast path for libraries like `quote` that\n-                    // build a token stream.\n-                    vec.reserve(iter.size_hint().0);\n-                    vec\n-                }\n-                Err(rc_vec) => {\n-                    // Self is shared so we need to copy and extend that.\n-                    let mut vec = Vec::new();\n-                    vec.reserve(rc_vec.len() + iter.size_hint().0);\n-                    vec.extend_from_slice(&rc_vec);\n-                    vec\n-                }\n-            }\n-        };\n-\n-        // Perform the extend, joining tokens as needed along the way.\n-        let mut builder = TokenStreamBuilder(tts);\n-        for stream in iter {\n-            builder.push(stream);\n-        }\n-\n-        // Build the resulting token stream. If it contains more than one token,\n-        // preserve capacity in the vector in anticipation of the caller\n-        // performing additional calls to extend.\n-        *self = TokenStream::new(builder.0);\n+        TokenStream::from_streams(iter.into_iter().map(Into::into).collect::<Vec<_>>())\n     }\n }\n \n@@ -294,14 +251,43 @@ impl TokenStream {\n         }\n     }\n \n-    pub fn new(mut streams: Vec<TokenStream>) -> TokenStream {\n+    fn from_streams(mut streams: Vec<TokenStream>) -> TokenStream {\n         match streams.len() {\n             0 => TokenStream::empty(),\n             1 => streams.pop().unwrap(),\n+            _ => {\n+                let mut vec = vec![];\n+                for stream in streams {\n+                    match stream {\n+                        TokenStream::Empty => {},\n+                        TokenStream::Tree(tree, is_joint) => vec.push((tree, is_joint)),\n+                        TokenStream::Stream(stream2) => vec.extend(stream2.iter().cloned()),\n+                    }\n+                }\n+                TokenStream::new(vec)\n+            }\n+        }\n+    }\n+\n+    pub fn new(mut streams: Vec<TreeAndJoint>) -> TokenStream {\n+        match streams.len() {\n+            0 => TokenStream::empty(),\n+            1 => {\n+                let (tree, is_joint) = streams.pop().unwrap();\n+                TokenStream::Tree(tree, is_joint)\n+            }\n             _ => TokenStream::Stream(Lrc::new(streams)),\n         }\n     }\n \n+    pub fn append_to_tree_and_joint_vec(self, vec: &mut Vec<TreeAndJoint>) {\n+        match self {\n+            TokenStream::Empty => {}\n+            TokenStream::Tree(tree, is_joint) => vec.push((tree, is_joint)),\n+            TokenStream::Stream(stream) => vec.extend(stream.iter().cloned()),\n+        }\n+    }\n+\n     pub fn trees(&self) -> Cursor {\n         self.clone().into_trees()\n     }\n@@ -362,54 +348,58 @@ impl TokenStream {\n         t1.next().is_none() && t2.next().is_none()\n     }\n \n-    /// Precondition: `self` consists of a single token tree.\n-    /// Returns true if the token tree is a joint operation w.r.t. `proc_macro::TokenNode`.\n-    pub fn as_tree(self) -> (TokenTree, bool /* joint? */) {\n-        match self {\n-            TokenStream::Tree(tree, is_joint) => (tree, is_joint == Joint),\n-            _ => unreachable!(),\n-        }\n-    }\n-\n     pub fn map_enumerated<F: FnMut(usize, TokenTree) -> TokenTree>(self, mut f: F) -> TokenStream {\n-        let mut trees = self.into_trees();\n-        let mut result = Vec::new();\n-        let mut i = 0;\n-        while let Some(stream) = trees.next_as_stream() {\n-            result.push(match stream {\n-                TokenStream::Tree(tree, is_joint) => TokenStream::Tree(f(i, tree), is_joint),\n-                _ => unreachable!()\n-            });\n-            i += 1;\n+        match self {\n+            TokenStream::Empty => TokenStream::Empty,\n+            TokenStream::Tree(tree, is_joint) => TokenStream::Tree(f(0, tree), is_joint),\n+            TokenStream::Stream(stream) => TokenStream::Stream(Lrc::new(\n+                stream\n+                    .iter()\n+                    .enumerate()\n+                    .map(|(i, (tree, is_joint))| (f(i, tree.clone()), *is_joint))\n+                    .collect()\n+            )),\n         }\n-        TokenStream::new(result)\n     }\n \n     pub fn map<F: FnMut(TokenTree) -> TokenTree>(self, mut f: F) -> TokenStream {\n-        let mut trees = self.into_trees();\n-        let mut result = Vec::new();\n-        while let Some(stream) = trees.next_as_stream() {\n-            result.push(match stream {\n-                TokenStream::Tree(tree, is_joint) => TokenStream::Tree(f(tree), is_joint),\n-                _ => unreachable!()\n-            });\n+        match self {\n+            TokenStream::Empty => TokenStream::Empty,\n+            TokenStream::Tree(tree, is_joint) => TokenStream::Tree(f(tree), is_joint),\n+            TokenStream::Stream(stream) => TokenStream::Stream(Lrc::new(\n+                stream\n+                    .iter()\n+                    .map(|(tree, is_joint)| (f(tree.clone()), *is_joint))\n+                    .collect()\n+            )),\n         }\n-        TokenStream::new(result)\n     }\n \n     fn first_tree_and_joint(&self) -> Option<(TokenTree, IsJoint)> {\n         match self {\n             TokenStream::Empty => None,\n             TokenStream::Tree(ref tree, is_joint) => Some((tree.clone(), *is_joint)),\n-            TokenStream::Stream(ref stream) => stream.first().unwrap().first_tree_and_joint(),\n+            TokenStream::Stream(ref stream) => Some(stream.first().unwrap().clone())\n         }\n     }\n \n     fn last_tree_if_joint(&self) -> Option<TokenTree> {\n         match self {\n-            TokenStream::Empty | TokenStream::Tree(_, NonJoint) => None,\n-            TokenStream::Tree(ref tree, Joint) => Some(tree.clone()),\n-            TokenStream::Stream(ref stream) => stream.last().unwrap().last_tree_if_joint(),\n+            TokenStream::Empty => None,\n+            TokenStream::Tree(ref tree, is_joint) => {\n+                if *is_joint == Joint {\n+                    Some(tree.clone())\n+                } else {\n+                    None\n+                }\n+            }\n+            TokenStream::Stream(ref stream) => {\n+                if let (tree, Joint) = stream.last().unwrap() {\n+                    Some(tree.clone())\n+                } else {\n+                    None\n+                }\n+            }\n         }\n     }\n }\n@@ -442,24 +432,18 @@ impl TokenStreamBuilder {\n         self.0.push(stream);\n     }\n \n-    pub fn add<T: Into<TokenStream>>(mut self, stream: T) -> Self {\n-        self.push(stream);\n-        self\n-    }\n-\n     pub fn build(self) -> TokenStream {\n-        TokenStream::new(self.0)\n+        TokenStream::from_streams(self.0)\n     }\n \n     fn push_all_but_last_tree(&mut self, stream: &TokenStream) {\n         if let TokenStream::Stream(ref streams) = stream {\n             let len = streams.len();\n             match len {\n                 1 => {}\n-                2 => self.0.push(streams[0].clone().into()),\n-                _ => self.0.push(TokenStream::new(streams[0 .. len - 1].to_vec())),\n+                2 => self.0.push(TokenStream::Tree(streams[0].0.clone(), streams[0].1)),\n+                _ => self.0.push(TokenStream::Stream(Lrc::new(streams[0 .. len - 1].to_vec()))),\n             }\n-            self.push_all_but_last_tree(&streams[len - 1])\n         }\n     }\n \n@@ -468,162 +452,85 @@ impl TokenStreamBuilder {\n             let len = streams.len();\n             match len {\n                 1 => {}\n-                2 => self.0.push(streams[1].clone().into()),\n-                _ => self.0.push(TokenStream::new(streams[1 .. len].to_vec())),\n+                2 => self.0.push(TokenStream::Tree(streams[1].0.clone(), streams[1].1)),\n+                _ => self.0.push(TokenStream::Stream(Lrc::new(streams[1 .. len].to_vec()))),\n             }\n-            self.push_all_but_first_tree(&streams[0])\n         }\n     }\n }\n \n #[derive(Clone)]\n-pub struct Cursor(CursorKind);\n-\n-#[derive(Clone)]\n-enum CursorKind {\n-    Empty,\n-    Tree(TokenTree, IsJoint, bool /* consumed? */),\n-    Stream(StreamCursor),\n-}\n-\n-#[derive(Clone)]\n-struct StreamCursor {\n-    stream: Lrc<Vec<TokenStream>>,\n+pub struct Cursor {\n+    pub stream: TokenStream,\n     index: usize,\n-    stack: Vec<(Lrc<Vec<TokenStream>>, usize)>,\n-}\n-\n-impl StreamCursor {\n-    fn new(stream: Lrc<Vec<TokenStream>>) -> Self {\n-        StreamCursor { stream: stream, index: 0, stack: Vec::new() }\n-    }\n-\n-    fn next_as_stream(&mut self) -> Option<TokenStream> {\n-        loop {\n-            if self.index < self.stream.len() {\n-                self.index += 1;\n-                let next = self.stream[self.index - 1].clone();\n-                match next {\n-                    TokenStream::Empty => {}\n-                    TokenStream::Tree(..) => return Some(next),\n-                    TokenStream::Stream(stream) => self.insert(stream),\n-                }\n-            } else if let Some((stream, index)) = self.stack.pop() {\n-                self.stream = stream;\n-                self.index = index;\n-            } else {\n-                return None;\n-            }\n-        }\n-    }\n-\n-    fn insert(&mut self, stream: Lrc<Vec<TokenStream>>) {\n-        self.stack.push((mem::replace(&mut self.stream, stream),\n-                         mem::replace(&mut self.index, 0)));\n-    }\n }\n \n impl Iterator for Cursor {\n     type Item = TokenTree;\n \n     fn next(&mut self) -> Option<TokenTree> {\n-        self.next_as_stream().map(|stream| match stream {\n-            TokenStream::Tree(tree, _) => tree,\n-            _ => unreachable!()\n-        })\n+        self.next_with_joint().map(|(tree, _)| tree)\n     }\n }\n \n impl Cursor {\n     fn new(stream: TokenStream) -> Self {\n-        Cursor(match stream {\n-            TokenStream::Empty => CursorKind::Empty,\n-            TokenStream::Tree(tree, is_joint) => CursorKind::Tree(tree, is_joint, false),\n-            TokenStream::Stream(stream) => CursorKind::Stream(StreamCursor::new(stream)),\n-        })\n-    }\n-\n-    pub fn next_as_stream(&mut self) -> Option<TokenStream> {\n-        let (stream, consumed) = match self.0 {\n-            CursorKind::Tree(ref tree, ref is_joint, ref mut consumed @ false) =>\n-                (TokenStream::Tree(tree.clone(), *is_joint), consumed),\n-            CursorKind::Stream(ref mut cursor) => return cursor.next_as_stream(),\n-            _ => return None,\n-        };\n-\n-        *consumed = true;\n-        Some(stream)\n+        Cursor { stream, index: 0 }\n     }\n \n-    pub fn insert(&mut self, stream: TokenStream) {\n-        match self.0 {\n-            _ if stream.is_empty() => return,\n-            CursorKind::Empty => *self = stream.trees(),\n-            CursorKind::Tree(_, _, consumed) => {\n-                *self = TokenStream::new(vec![self.original_stream(), stream]).trees();\n-                if consumed {\n-                    self.next();\n+    pub fn next_with_joint(&mut self) -> Option<TreeAndJoint> {\n+        match self.stream {\n+            TokenStream::Empty => None,\n+            TokenStream::Tree(ref tree, ref is_joint) => {\n+                if self.index == 0 {\n+                    self.index = 1;\n+                    Some((tree.clone(), *is_joint))\n+                } else {\n+                    None\n                 }\n             }\n-            CursorKind::Stream(ref mut cursor) => {\n-                cursor.insert(ThinTokenStream::from(stream).0.unwrap());\n+            TokenStream::Stream(ref stream) => {\n+                if self.index < stream.len() {\n+                    self.index += 1;\n+                    Some(stream[self.index - 1].clone())\n+                } else {\n+                    None\n+                }\n             }\n         }\n     }\n \n-    pub fn original_stream(&self) -> TokenStream {\n-        match self.0 {\n-            CursorKind::Empty => TokenStream::empty(),\n-            CursorKind::Tree(ref tree, ref is_joint, _) =>\n-                TokenStream::Tree(tree.clone(), *is_joint),\n-            CursorKind::Stream(ref cursor) => TokenStream::Stream(\n-                cursor.stack.get(0).cloned().map(|(stream, _)| stream)\n-                    .unwrap_or_else(|| cursor.stream.clone())\n-            ),\n+    pub fn append(&mut self, new_stream: TokenStream) {\n+        if new_stream.is_empty() {\n+            return;\n         }\n+        let index = self.index;\n+        let stream = mem::replace(&mut self.stream, TokenStream::Empty);\n+        *self = TokenStream::from_streams(vec![stream, new_stream]).into_trees();\n+        self.index = index;\n     }\n \n     pub fn look_ahead(&self, n: usize) -> Option<TokenTree> {\n-        fn look_ahead(streams: &[TokenStream], mut n: usize) -> Result<TokenTree, usize> {\n-            for stream in streams {\n-                n = match stream {\n-                    TokenStream::Tree(ref tree, _) if n == 0 => return Ok(tree.clone()),\n-                    TokenStream::Tree(..) => n - 1,\n-                    TokenStream::Stream(ref stream) => match look_ahead(stream, n) {\n-                        Ok(tree) => return Ok(tree),\n-                        Err(n) => n,\n-                    },\n-                    _ => n,\n-                };\n+        match self.stream {\n+            TokenStream::Empty => None,\n+            TokenStream::Tree(ref tree, _) => {\n+                if n == 0 && self.index == 0 {\n+                    Some(tree.clone())\n+                } else {\n+                    None\n+                }\n             }\n-            Err(n)\n+            TokenStream::Stream(ref stream) =>\n+                stream[self.index ..].get(n).map(|(tree, _)| tree.clone()),\n         }\n-\n-        match self.0 {\n-            CursorKind::Empty |\n-            CursorKind::Tree(_, _, true) => Err(n),\n-            CursorKind::Tree(ref tree, _, false) => look_ahead(&[tree.clone().into()], n),\n-            CursorKind::Stream(ref cursor) => {\n-                look_ahead(&cursor.stream[cursor.index ..], n).or_else(|mut n| {\n-                    for &(ref stream, index) in cursor.stack.iter().rev() {\n-                        n = match look_ahead(&stream[index..], n) {\n-                            Ok(tree) => return Ok(tree),\n-                            Err(n) => n,\n-                        }\n-                    }\n-\n-                    Err(n)\n-                })\n-            }\n-        }.ok()\n     }\n }\n \n /// The `TokenStream` type is large enough to represent a single `TokenTree` without allocation.\n /// `ThinTokenStream` is smaller, but needs to allocate to represent a single `TokenTree`.\n /// We must use `ThinTokenStream` in `TokenTree::Delimited` to avoid infinite size due to recursion.\n #[derive(Debug, Clone)]\n-pub struct ThinTokenStream(Option<Lrc<Vec<TokenStream>>>);\n+pub struct ThinTokenStream(Option<Lrc<Vec<TreeAndJoint>>>);\n \n impl ThinTokenStream {\n     pub fn stream(&self) -> TokenStream {\n@@ -635,7 +542,7 @@ impl From<TokenStream> for ThinTokenStream {\n     fn from(stream: TokenStream) -> ThinTokenStream {\n         ThinTokenStream(match stream {\n             TokenStream::Empty => None,\n-            TokenStream::Tree(..) => Some(Lrc::new(vec![stream])),\n+            TokenStream::Tree(tree, is_joint) => Some(Lrc::new(vec![(tree, is_joint)])),\n             TokenStream::Stream(stream) => Some(stream),\n         })\n     }\n@@ -742,7 +649,7 @@ mod tests {\n             let test_res = string_to_ts(\"foo::bar::baz\");\n             let test_fst = string_to_ts(\"foo::bar\");\n             let test_snd = string_to_ts(\"::baz\");\n-            let eq_res = TokenStream::new(vec![test_fst, test_snd]);\n+            let eq_res = TokenStream::from_streams(vec![test_fst, test_snd]);\n             assert_eq!(test_res.trees().count(), 5);\n             assert_eq!(eq_res.trees().count(), 5);\n             assert_eq!(test_res.eq_unspanned(&eq_res), true);\n@@ -827,107 +734,4 @@ mod tests {\n         assert!(stream.eq_unspanned(&string_to_ts(\"...\")));\n         assert_eq!(stream.trees().count(), 1);\n     }\n-\n-    #[test]\n-    fn test_extend_empty() {\n-        with_globals(|| {\n-            // Append a token onto an empty token stream.\n-            let mut stream = TokenStream::empty();\n-            stream.extend(vec![string_to_ts(\"t\")]);\n-\n-            let expected = string_to_ts(\"t\");\n-            assert!(stream.eq_unspanned(&expected));\n-        });\n-    }\n-\n-    #[test]\n-    fn test_extend_nothing() {\n-        with_globals(|| {\n-            // Append nothing onto a token stream containing one token.\n-            let mut stream = string_to_ts(\"t\");\n-            stream.extend(vec![]);\n-\n-            let expected = string_to_ts(\"t\");\n-            assert!(stream.eq_unspanned(&expected));\n-        });\n-    }\n-\n-    #[test]\n-    fn test_extend_single() {\n-        with_globals(|| {\n-            // Append a token onto token stream containing a single token.\n-            let mut stream = string_to_ts(\"t1\");\n-            stream.extend(vec![string_to_ts(\"t2\")]);\n-\n-            let expected = string_to_ts(\"t1 t2\");\n-            assert!(stream.eq_unspanned(&expected));\n-        });\n-    }\n-\n-    #[test]\n-    fn test_extend_in_place() {\n-        with_globals(|| {\n-            // Append a token onto token stream containing a reference counted\n-            // vec of tokens. The token stream has a reference count of 1 so\n-            // this can happen in place.\n-            let mut stream = string_to_ts(\"t1 t2\");\n-            stream.extend(vec![string_to_ts(\"t3\")]);\n-\n-            let expected = string_to_ts(\"t1 t2 t3\");\n-            assert!(stream.eq_unspanned(&expected));\n-        });\n-    }\n-\n-    #[test]\n-    fn test_extend_copy() {\n-        with_globals(|| {\n-            // Append a token onto token stream containing a reference counted\n-            // vec of tokens. The token stream is shared so the extend takes\n-            // place on a copy.\n-            let mut stream = string_to_ts(\"t1 t2\");\n-            let _incref = stream.clone();\n-            stream.extend(vec![string_to_ts(\"t3\")]);\n-\n-            let expected = string_to_ts(\"t1 t2 t3\");\n-            assert!(stream.eq_unspanned(&expected));\n-        });\n-    }\n-\n-    #[test]\n-    fn test_extend_no_join() {\n-        with_globals(|| {\n-            let first = TokenTree::Token(DUMMY_SP, Token::Dot);\n-            let second = TokenTree::Token(DUMMY_SP, Token::Dot);\n-\n-            // Append a dot onto a token stream containing a dot, but do not\n-            // join them.\n-            let mut stream = TokenStream::from(first);\n-            stream.extend(vec![TokenStream::from(second)]);\n-\n-            let expected = string_to_ts(\". .\");\n-            assert!(stream.eq_unspanned(&expected));\n-\n-            let unexpected = string_to_ts(\"..\");\n-            assert!(!stream.eq_unspanned(&unexpected));\n-        });\n-    }\n-\n-    #[test]\n-    fn test_extend_join() {\n-        with_globals(|| {\n-            let first = TokenTree::Token(DUMMY_SP, Token::Dot).joint();\n-            let second = TokenTree::Token(DUMMY_SP, Token::Dot);\n-\n-            // Append a dot onto a token stream containing a dot, forming a\n-            // dotdot.\n-            let mut stream = first;\n-            stream.extend(vec![TokenStream::from(second)]);\n-\n-            let expected = string_to_ts(\"..\");\n-            assert!(stream.eq_unspanned(&expected));\n-\n-            let unexpected = string_to_ts(\". .\");\n-            assert!(!stream.eq_unspanned(&unexpected));\n-        });\n-    }\n }"}, {"sha": "158cbc791ef504f30cf0d5ff6802f84769480874", "filename": "src/libsyntax_ext/proc_macro_server.rs", "status": "modified", "additions": 7, "deletions": 6, "changes": 13, "blob_url": "https://github.com/rust-lang/rust/blob/b1200a29b07dddb9c05b99166c7b676b6f2dbb4b/src%2Flibsyntax_ext%2Fproc_macro_server.rs", "raw_url": "https://github.com/rust-lang/rust/raw/b1200a29b07dddb9c05b99166c7b676b6f2dbb4b/src%2Flibsyntax_ext%2Fproc_macro_server.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax_ext%2Fproc_macro_server.rs?ref=b1200a29b07dddb9c05b99166c7b676b6f2dbb4b", "patch": "@@ -11,7 +11,7 @@ use syntax::ast;\n use syntax::ext::base::ExtCtxt;\n use syntax::parse::lexer::comments;\n use syntax::parse::{self, token, ParseSess};\n-use syntax::tokenstream::{self, DelimSpan, IsJoint::*, TokenStream};\n+use syntax::tokenstream::{self, DelimSpan, IsJoint::*, TokenStream, TreeAndJoint};\n use syntax_pos::hygiene::{SyntaxContext, Transparency};\n use syntax_pos::symbol::{keywords, Symbol};\n use syntax_pos::{BytePos, FileName, MultiSpan, Pos, SourceFile, Span};\n@@ -46,13 +46,14 @@ impl ToInternal<token::DelimToken> for Delimiter {\n     }\n }\n \n-impl FromInternal<(TokenStream, &'_ ParseSess, &'_ mut Vec<Self>)>\n+impl FromInternal<(TreeAndJoint, &'_ ParseSess, &'_ mut Vec<Self>)>\n     for TokenTree<Group, Punct, Ident, Literal>\n {\n-    fn from_internal((stream, sess, stack): (TokenStream, &ParseSess, &mut Vec<Self>)) -> Self {\n+    fn from_internal(((tree, is_joint), sess, stack): (TreeAndJoint, &ParseSess, &mut Vec<Self>))\n+                    -> Self {\n         use syntax::parse::token::*;\n \n-        let (tree, joint) = stream.as_tree();\n+        let joint = is_joint == Joint;\n         let (span, token) = match tree {\n             tokenstream::TokenTree::Delimited(span, delim, tts) => {\n                 let delimiter = Delimiter::from_internal(delim);\n@@ -450,7 +451,7 @@ impl server::TokenStreamIter for Rustc<'_> {\n     ) -> Option<TokenTree<Self::Group, Self::Punct, Self::Ident, Self::Literal>> {\n         loop {\n             let tree = iter.stack.pop().or_else(|| {\n-                let next = iter.cursor.next_as_stream()?;\n+                let next = iter.cursor.next_with_joint()?;\n                 Some(TokenTree::from_internal((next, self.sess, &mut iter.stack)))\n             })?;\n             // HACK: The condition \"dummy span + group with empty delimiter\" represents an AST\n@@ -461,7 +462,7 @@ impl server::TokenStreamIter for Rustc<'_> {\n             // and not doing the roundtrip through AST.\n             if let TokenTree::Group(ref group) = tree {\n                 if group.delimiter == Delimiter::None && group.span.entire().is_dummy() {\n-                    iter.cursor.insert(group.stream.clone());\n+                    iter.cursor.append(group.stream.clone());\n                     continue;\n                 }\n             }"}]}