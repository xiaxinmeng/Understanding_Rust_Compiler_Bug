{"sha": "5d5dcae7f15850e7668a38e5da87cf6010e87eba", "node_id": "MDY6Q29tbWl0NzI0NzEyOjVkNWRjYWU3ZjE1ODUwZTc2NjhhMzhlNWRhODdjZjYwMTBlODdlYmE=", "commit": {"author": {"name": "bors", "email": "bors@rust-lang.org", "date": "2017-09-19T08:19:45Z"}, "committer": {"name": "bors", "email": "bors@rust-lang.org", "date": "2017-09-19T08:19:45Z"}, "message": "Auto merge of #44601 - alexcrichton:lower-attributes-in-hir, r=nrc\n\nrustc: Forbid interpolated tokens in the HIR\n\nRight now the HIR contains raw `syntax::ast::Attribute` structure but nowadays\nthese can contain arbitrary tokens. One variant of the `Token` enum is an\n\"interpolated\" token which basically means to shove all the tokens for a\nnonterminal in this position. A \"nonterminal\" in this case is roughly analagous\nto a macro argument:\n\n    macro_rules! foo {\n        ($a:expr) => {\n            // $a is a nonterminal as an expression\n        }\n    }\n\nCurrently nonterminals contain namely items and expressions, and this poses a\nproblem for incremental compilation! With incremental we want a stable hash of\nall HIR items, but this means we may transitively need a stable hash *of the\nentire AST*, which is certainly not stable w/ node ids and whatnot. Hence today\nthere's a \"bug\" where the \"stable hash\" of an AST is just the raw hash value of\nthe AST, and this only arises with interpolated nonterminals. The downside of\nthis approach, however, is that a bunch of errors get spewed out during\ncompilation about how this isn't a great idea.\n\nThis PR is focused at fixing these warnings, basically deleting them from the\ncompiler. The implementation here is to alter attributes as they're lowered from\nthe AST to HIR, expanding all nonterminals in-place as we see them. This code\nfor expanding a nonterminal to a token stream already exists for the\n`proc_macro` crate, so we basically just reuse the same implementation there.\n\nAfter this PR it's considered a bug to have an `Interpolated` token and hence\nthe stable hash implementation simply uses `bug!` in this location.\n\nCloses #40946", "tree": {"sha": "b81d24fa2d8e0070957e4498c20a9c35eefc1745", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/b81d24fa2d8e0070957e4498c20a9c35eefc1745"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/5d5dcae7f15850e7668a38e5da87cf6010e87eba", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/5d5dcae7f15850e7668a38e5da87cf6010e87eba", "html_url": "https://github.com/rust-lang/rust/commit/5d5dcae7f15850e7668a38e5da87cf6010e87eba", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/5d5dcae7f15850e7668a38e5da87cf6010e87eba/comments", "author": {"login": "bors", "id": 3372342, "node_id": "MDQ6VXNlcjMzNzIzNDI=", "avatar_url": "https://avatars.githubusercontent.com/u/3372342?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bors", "html_url": "https://github.com/bors", "followers_url": "https://api.github.com/users/bors/followers", "following_url": "https://api.github.com/users/bors/following{/other_user}", "gists_url": "https://api.github.com/users/bors/gists{/gist_id}", "starred_url": "https://api.github.com/users/bors/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bors/subscriptions", "organizations_url": "https://api.github.com/users/bors/orgs", "repos_url": "https://api.github.com/users/bors/repos", "events_url": "https://api.github.com/users/bors/events{/privacy}", "received_events_url": "https://api.github.com/users/bors/received_events", "type": "User", "site_admin": false}, "committer": {"login": "bors", "id": 3372342, "node_id": "MDQ6VXNlcjMzNzIzNDI=", "avatar_url": "https://avatars.githubusercontent.com/u/3372342?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bors", "html_url": "https://github.com/bors", "followers_url": "https://api.github.com/users/bors/followers", "following_url": "https://api.github.com/users/bors/following{/other_user}", "gists_url": "https://api.github.com/users/bors/gists{/gist_id}", "starred_url": "https://api.github.com/users/bors/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bors/subscriptions", "organizations_url": "https://api.github.com/users/bors/orgs", "repos_url": "https://api.github.com/users/bors/repos", "events_url": "https://api.github.com/users/bors/events{/privacy}", "received_events_url": "https://api.github.com/users/bors/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "9a00f3cc306f2f79bfbd54f1986d8ca7a74f6661", "url": "https://api.github.com/repos/rust-lang/rust/commits/9a00f3cc306f2f79bfbd54f1986d8ca7a74f6661", "html_url": "https://github.com/rust-lang/rust/commit/9a00f3cc306f2f79bfbd54f1986d8ca7a74f6661"}, {"sha": "0694e4fde4c4abd13dffa82a571c7a9559b19fb1", "url": "https://api.github.com/repos/rust-lang/rust/commits/0694e4fde4c4abd13dffa82a571c7a9559b19fb1", "html_url": "https://github.com/rust-lang/rust/commit/0694e4fde4c4abd13dffa82a571c7a9559b19fb1"}], "stats": {"total": 234, "additions": 143, "deletions": 91}, "files": [{"sha": "07e933985a0c9ed8abeb3e085499681f103958ad", "filename": "src/libproc_macro/lib.rs", "status": "modified", "additions": 5, "deletions": 71, "changes": 76, "blob_url": "https://github.com/rust-lang/rust/blob/5d5dcae7f15850e7668a38e5da87cf6010e87eba/src%2Flibproc_macro%2Flib.rs", "raw_url": "https://github.com/rust-lang/rust/raw/5d5dcae7f15850e7668a38e5da87cf6010e87eba/src%2Flibproc_macro%2Flib.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibproc_macro%2Flib.rs?ref=5d5dcae7f15850e7668a38e5da87cf6010e87eba", "patch": "@@ -54,8 +54,7 @@ use std::str::FromStr;\n \n use syntax::ast;\n use syntax::errors::DiagnosticBuilder;\n-use syntax::parse::{self, token, parse_stream_from_source_str};\n-use syntax::print::pprust;\n+use syntax::parse::{self, token};\n use syntax::symbol::Symbol;\n use syntax::tokenstream;\n use syntax_pos::DUMMY_SP;\n@@ -525,47 +524,10 @@ impl TokenTree {\n             Ident(ident) | Lifetime(ident) => TokenNode::Term(Term(ident.name)),\n             Literal(..) | DocComment(..) => TokenNode::Literal(self::Literal(token)),\n \n-            Interpolated(ref nt) => {\n-                // An `Interpolated` token means that we have a `Nonterminal`\n-                // which is often a parsed AST item. At this point we now need\n-                // to convert the parsed AST to an actual token stream, e.g.\n-                // un-parse it basically.\n-                //\n-                // Unfortunately there's not really a great way to do that in a\n-                // guaranteed lossless fashion right now. The fallback here is\n-                // to just stringify the AST node and reparse it, but this loses\n-                // all span information.\n-                //\n-                // As a result, some AST nodes are annotated with the token\n-                // stream they came from. Attempt to extract these lossless\n-                // token streams before we fall back to the stringification.\n-                let mut tokens = None;\n-\n-                match nt.0 {\n-                    Nonterminal::NtItem(ref item) => {\n-                        tokens = prepend_attrs(&item.attrs, item.tokens.as_ref(), span);\n-                    }\n-                    Nonterminal::NtTraitItem(ref item) => {\n-                        tokens = prepend_attrs(&item.attrs, item.tokens.as_ref(), span);\n-                    }\n-                    Nonterminal::NtImplItem(ref item) => {\n-                        tokens = prepend_attrs(&item.attrs, item.tokens.as_ref(), span);\n-                    }\n-                    _ => {}\n-                }\n-\n-                tokens.map(|tokens| {\n-                    TokenNode::Group(Delimiter::None,\n-                                     TokenStream(tokens.clone()))\n-                }).unwrap_or_else(|| {\n-                    __internal::with_sess(|(sess, _)| {\n-                        TokenNode::Group(Delimiter::None, TokenStream(nt.1.force(|| {\n-                            // FIXME(jseyfried): Avoid this pretty-print + reparse hack\n-                            let name = \"<macro expansion>\".to_owned();\n-                            let source = pprust::token_to_string(&token);\n-                            parse_stream_from_source_str(name, source, sess, Some(span))\n-                        })))\n-                    })\n+            Interpolated(_) => {\n+                __internal::with_sess(|(sess, _)| {\n+                    let tts = token.interpolated_to_tokenstream(sess, span);\n+                    TokenNode::Group(Delimiter::None, TokenStream(tts))\n                 })\n             }\n \n@@ -631,34 +593,6 @@ impl TokenTree {\n     }\n }\n \n-fn prepend_attrs(attrs: &[ast::Attribute],\n-                 tokens: Option<&tokenstream::TokenStream>,\n-                 span: syntax_pos::Span)\n-    -> Option<tokenstream::TokenStream>\n-{\n-    let tokens = match tokens {\n-        Some(tokens) => tokens,\n-        None => return None,\n-    };\n-    if attrs.len() == 0 {\n-        return Some(tokens.clone())\n-    }\n-    let mut builder = tokenstream::TokenStreamBuilder::new();\n-    for attr in attrs {\n-        assert_eq!(attr.style, ast::AttrStyle::Outer,\n-                   \"inner attributes should prevent cached tokens from existing\");\n-        let stream = __internal::with_sess(|(sess, _)| {\n-            // FIXME: Avoid this pretty-print + reparse hack as bove\n-            let name = \"<macro expansion>\".to_owned();\n-            let source = pprust::attr_to_string(attr);\n-            parse_stream_from_source_str(name, source, sess, Some(span))\n-        });\n-        builder.push(stream);\n-    }\n-    builder.push(tokens.clone());\n-    Some(builder.build())\n-}\n-\n /// Permanently unstable internal implementation details of this crate. This\n /// should not be used.\n ///"}, {"sha": "c7ea9c47028694d3714398010f054c5527b56d1f", "filename": "src/librustc/hir/lowering.rs", "status": "modified", "additions": 48, "deletions": 2, "changes": 50, "blob_url": "https://github.com/rust-lang/rust/blob/5d5dcae7f15850e7668a38e5da87cf6010e87eba/src%2Flibrustc%2Fhir%2Flowering.rs", "raw_url": "https://github.com/rust-lang/rust/raw/5d5dcae7f15850e7668a38e5da87cf6010e87eba/src%2Flibrustc%2Fhir%2Flowering.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Fhir%2Flowering.rs?ref=5d5dcae7f15850e7668a38e5da87cf6010e87eba", "patch": "@@ -64,6 +64,8 @@ use syntax::ptr::P;\n use syntax::codemap::{self, respan, Spanned, CompilerDesugaringKind};\n use syntax::std_inject;\n use syntax::symbol::{Symbol, keywords};\n+use syntax::tokenstream::{TokenStream, TokenTree, Delimited};\n+use syntax::parse::token::{Token, DelimToken};\n use syntax::util::small_vector::SmallVector;\n use syntax::visit::{self, Visitor};\n use syntax_pos::Span;\n@@ -589,7 +591,50 @@ impl<'a> LoweringContext<'a> {\n     }\n \n     fn lower_attrs(&mut self, attrs: &Vec<Attribute>) -> hir::HirVec<Attribute> {\n-        attrs.clone().into()\n+        attrs.iter().map(|a| self.lower_attr(a)).collect::<Vec<_>>().into()\n+    }\n+\n+    fn lower_attr(&mut self, attr: &Attribute) -> Attribute {\n+        Attribute {\n+            id: attr.id,\n+            style: attr.style,\n+            path: attr.path.clone(),\n+            tokens: self.lower_token_stream(attr.tokens.clone()),\n+            is_sugared_doc: attr.is_sugared_doc,\n+            span: attr.span,\n+        }\n+    }\n+\n+    fn lower_token_stream(&mut self, tokens: TokenStream) -> TokenStream {\n+        tokens.into_trees().map(|tree| self.lower_token_tree(tree)).collect()\n+    }\n+\n+    fn lower_token_tree(&mut self, tree: TokenTree) -> TokenTree {\n+        match tree {\n+            TokenTree::Token(span, token) => {\n+                self.lower_token(token, span)\n+            }\n+            TokenTree::Delimited(span, delimited) => {\n+                TokenTree::Delimited(span, Delimited {\n+                    delim: delimited.delim,\n+                    tts: self.lower_token_stream(delimited.tts.into()).into(),\n+                })\n+            }\n+        }\n+    }\n+\n+    fn lower_token(&mut self, token: Token, span: Span) -> TokenTree {\n+        match token {\n+            Token::Interpolated(_) => {}\n+            other => return TokenTree::Token(span, other),\n+        }\n+\n+        let tts = token.interpolated_to_tokenstream(&self.sess.parse_sess, span);\n+        let tts = self.lower_token_stream(tts);\n+        TokenTree::Delimited(span, Delimited {\n+            delim: DelimToken::NoDelim,\n+            tts: tts.into(),\n+        })\n     }\n \n     fn lower_arm(&mut self, arm: &Arm) -> hir::Arm {\n@@ -1625,13 +1670,14 @@ impl<'a> LoweringContext<'a> {\n         let attrs = self.lower_attrs(&i.attrs);\n         if let ItemKind::MacroDef(ref def) = i.node {\n             if !def.legacy || i.attrs.iter().any(|attr| attr.path == \"macro_export\") {\n+                let body = self.lower_token_stream(def.stream());\n                 self.exported_macros.push(hir::MacroDef {\n                     name,\n                     vis,\n                     attrs,\n                     id: i.id,\n                     span: i.span,\n-                    body: def.stream(),\n+                    body,\n                     legacy: def.legacy,\n                 });\n             }"}, {"sha": "56ec6a65eb679648c07231bd68fb943430d56e37", "filename": "src/librustc/ich/impls_syntax.rs", "status": "modified", "additions": 5, "deletions": 18, "changes": 23, "blob_url": "https://github.com/rust-lang/rust/blob/5d5dcae7f15850e7668a38e5da87cf6010e87eba/src%2Flibrustc%2Fich%2Fimpls_syntax.rs", "raw_url": "https://github.com/rust-lang/rust/raw/5d5dcae7f15850e7668a38e5da87cf6010e87eba/src%2Flibrustc%2Fich%2Fimpls_syntax.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Fich%2Fimpls_syntax.rs?ref=5d5dcae7f15850e7668a38e5da87cf6010e87eba", "patch": "@@ -20,7 +20,7 @@ use syntax::ast;\n use syntax::parse::token;\n use syntax::symbol::InternedString;\n use syntax::tokenstream;\n-use syntax_pos::{Span, FileMap};\n+use syntax_pos::FileMap;\n \n use hir::def_id::{DefId, CrateNum, CRATE_DEF_INDEX};\n \n@@ -228,7 +228,7 @@ for tokenstream::TokenTree {\n         match *self {\n             tokenstream::TokenTree::Token(span, ref token) => {\n                 span.hash_stable(hcx, hasher);\n-                hash_token(token, hcx, hasher, span);\n+                hash_token(token, hcx, hasher);\n             }\n             tokenstream::TokenTree::Delimited(span, ref delimited) => {\n                 span.hash_stable(hcx, hasher);\n@@ -254,8 +254,7 @@ for tokenstream::TokenStream {\n \n fn hash_token<'gcx, W: StableHasherResult>(token: &token::Token,\n                                            hcx: &mut StableHashingContext<'gcx>,\n-                                           hasher: &mut StableHasher<W>,\n-                                           error_reporting_span: Span) {\n+                                           hasher: &mut StableHasher<W>) {\n     mem::discriminant(token).hash_stable(hcx, hasher);\n     match *token {\n         token::Token::Eq |\n@@ -318,20 +317,8 @@ fn hash_token<'gcx, W: StableHasherResult>(token: &token::Token,\n         token::Token::Ident(ident) |\n         token::Token::Lifetime(ident) => ident.name.hash_stable(hcx, hasher),\n \n-        token::Token::Interpolated(ref non_terminal) => {\n-            // FIXME(mw): This could be implemented properly. It's just a\n-            //            lot of work, since we would need to hash the AST\n-            //            in a stable way, in addition to the HIR.\n-            //            Since this is hardly used anywhere, just emit a\n-            //            warning for now.\n-            if hcx.sess().opts.debugging_opts.incremental.is_some() {\n-                let msg = format!(\"Quasi-quoting might make incremental \\\n-                                   compilation very inefficient: {:?}\",\n-                                  non_terminal);\n-                hcx.sess().span_warn(error_reporting_span, &msg[..]);\n-            }\n-\n-            std_hash::Hash::hash(non_terminal, hasher);\n+        token::Token::Interpolated(_) => {\n+            bug!(\"interpolated tokens should not be present in the HIR\")\n         }\n \n         token::Token::DocComment(val) |"}, {"sha": "a316733bdb51c4bd32d3a78939a4dfbe85fea25a", "filename": "src/libsyntax/parse/token.rs", "status": "modified", "additions": 85, "deletions": 0, "changes": 85, "blob_url": "https://github.com/rust-lang/rust/blob/5d5dcae7f15850e7668a38e5da87cf6010e87eba/src%2Flibsyntax%2Fparse%2Ftoken.rs", "raw_url": "https://github.com/rust-lang/rust/raw/5d5dcae7f15850e7668a38e5da87cf6010e87eba/src%2Flibsyntax%2Fparse%2Ftoken.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Ftoken.rs?ref=5d5dcae7f15850e7668a38e5da87cf6010e87eba", "patch": "@@ -15,10 +15,15 @@ pub use self::Lit::*;\n pub use self::Token::*;\n \n use ast::{self};\n+use parse::ParseSess;\n+use print::pprust;\n use ptr::P;\n use serialize::{Decodable, Decoder, Encodable, Encoder};\n use symbol::keywords;\n+use syntax::parse::parse_stream_from_source_str;\n+use syntax_pos::{self, Span};\n use tokenstream::{TokenStream, TokenTree};\n+use tokenstream;\n \n use std::cell::Cell;\n use std::{cmp, fmt};\n@@ -421,6 +426,59 @@ impl Token {\n     pub fn is_reserved_ident(&self) -> bool {\n         self.is_special_ident() || self.is_used_keyword() || self.is_unused_keyword()\n     }\n+\n+    pub fn interpolated_to_tokenstream(&self, sess: &ParseSess, span: Span)\n+        -> TokenStream\n+    {\n+        let nt = match *self {\n+            Token::Interpolated(ref nt) => nt,\n+            _ => panic!(\"only works on interpolated tokens\"),\n+        };\n+\n+        // An `Interpolated` token means that we have a `Nonterminal`\n+        // which is often a parsed AST item. At this point we now need\n+        // to convert the parsed AST to an actual token stream, e.g.\n+        // un-parse it basically.\n+        //\n+        // Unfortunately there's not really a great way to do that in a\n+        // guaranteed lossless fashion right now. The fallback here is\n+        // to just stringify the AST node and reparse it, but this loses\n+        // all span information.\n+        //\n+        // As a result, some AST nodes are annotated with the token\n+        // stream they came from. Attempt to extract these lossless\n+        // token streams before we fall back to the stringification.\n+        let mut tokens = None;\n+\n+        match nt.0 {\n+            Nonterminal::NtItem(ref item) => {\n+                tokens = prepend_attrs(sess, &item.attrs, item.tokens.as_ref(), span);\n+            }\n+            Nonterminal::NtTraitItem(ref item) => {\n+                tokens = prepend_attrs(sess, &item.attrs, item.tokens.as_ref(), span);\n+            }\n+            Nonterminal::NtImplItem(ref item) => {\n+                tokens = prepend_attrs(sess, &item.attrs, item.tokens.as_ref(), span);\n+            }\n+            Nonterminal::NtIdent(ident) => {\n+                let token = Token::Ident(ident.node);\n+                tokens = Some(TokenTree::Token(ident.span, token).into());\n+            }\n+            Nonterminal::NtTT(ref tt) => {\n+                tokens = Some(tt.clone().into());\n+            }\n+            _ => {}\n+        }\n+\n+        tokens.unwrap_or_else(|| {\n+            nt.1.force(|| {\n+                // FIXME(jseyfried): Avoid this pretty-print + reparse hack\n+                let name = \"<macro expansion>\".to_owned();\n+                let source = pprust::token_to_string(self);\n+                parse_stream_from_source_str(name, source, sess, Some(span))\n+            })\n+        })\n+    }\n }\n \n #[derive(Clone, RustcEncodable, RustcDecodable, PartialEq, Eq, Hash)]\n@@ -533,3 +591,30 @@ impl Decodable for LazyTokenStream {\n impl ::std::hash::Hash for LazyTokenStream {\n     fn hash<H: ::std::hash::Hasher>(&self, _hasher: &mut H) {}\n }\n+\n+fn prepend_attrs(sess: &ParseSess,\n+                 attrs: &[ast::Attribute],\n+                 tokens: Option<&tokenstream::TokenStream>,\n+                 span: syntax_pos::Span)\n+    -> Option<tokenstream::TokenStream>\n+{\n+    let tokens = match tokens {\n+        Some(tokens) => tokens,\n+        None => return None,\n+    };\n+    if attrs.len() == 0 {\n+        return Some(tokens.clone())\n+    }\n+    let mut builder = tokenstream::TokenStreamBuilder::new();\n+    for attr in attrs {\n+        assert_eq!(attr.style, ast::AttrStyle::Outer,\n+                   \"inner attributes should prevent cached tokens from existing\");\n+        // FIXME: Avoid this pretty-print + reparse hack as bove\n+        let name = \"<macro expansion>\".to_owned();\n+        let source = pprust::attr_to_string(attr);\n+        let stream = parse_stream_from_source_str(name, source, sess, Some(span));\n+        builder.push(stream);\n+    }\n+    builder.push(tokens.clone());\n+    Some(builder.build())\n+}"}]}