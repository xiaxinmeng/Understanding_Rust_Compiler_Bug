{"sha": "7d2460e5c4e5714ce72fc21bc65d94957e0bd8b6", "node_id": "MDY6Q29tbWl0NzI0NzEyOjdkMjQ2MGU1YzRlNTcxNGNlNzJmYzIxYmM2NWQ5NDk1N2UwYmQ4YjY=", "commit": {"author": {"name": "Aleksey Kladov", "email": "aleksey.kladov@gmail.com", "date": "2020-08-31T13:22:40Z"}, "committer": {"name": "GitHub", "email": "noreply@github.com", "date": "2020-08-31T13:22:40Z"}, "message": "Rollup merge of #76050 - matklad:pos, r=petrochenkov\n\nRemove unused function", "tree": {"sha": "01b1bccc1829b9767bc91da72d6038bec5fee3e6", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/01b1bccc1829b9767bc91da72d6038bec5fee3e6"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/7d2460e5c4e5714ce72fc21bc65d94957e0bd8b6", "comment_count": 0, "verification": {"verified": true, "reason": "valid", "signature": "-----BEGIN PGP SIGNATURE-----\n\nwsBcBAABCAAQBQJfTPmgCRBK7hj4Ov3rIwAAdHIIAAQHBh8qlCSTKlrGrknhIhDF\nhoToLRUXTAMqHfqn2rekzKXTU/E44uOdReGC9H6fZu5vmprEH0yla/YZQMCEjYwG\n56fc33Cmx112u5pGqA9nNK2l8zFi1tEMqPwnWVzxeoGfJvjZlwCi8XROVPhQ7Ixv\nkPBAzm2dREMfj34w3IcB8tWNVvIEiQelDovZL/QBcUpg549vOxlEYgFAzVGE5cXa\nODWEA9rblOaWQYEjwzA4sibaBDJjD0hOs2k9rdlg8T7KGQs9jx5rTXW7Pphgm1MA\nzvu6ti8Un/dEPqO13cI/oh7+QBIS5i+y6tMzQdMda3Ns25BpmwnmjUaFLlWyN4k=\n=HpgE\n-----END PGP SIGNATURE-----\n", "payload": "tree 01b1bccc1829b9767bc91da72d6038bec5fee3e6\nparent 4e2a25d759d3e014dc5e47872d8cff2a4a821b08\nparent 518cac91902d34567ac8bfea3022f426a7de53f6\nauthor Aleksey Kladov <aleksey.kladov@gmail.com> 1598880160 +0200\ncommitter GitHub <noreply@github.com> 1598880160 +0200\n\nRollup merge of #76050 - matklad:pos, r=petrochenkov\n\nRemove unused function\n"}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/7d2460e5c4e5714ce72fc21bc65d94957e0bd8b6", "html_url": "https://github.com/rust-lang/rust/commit/7d2460e5c4e5714ce72fc21bc65d94957e0bd8b6", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/7d2460e5c4e5714ce72fc21bc65d94957e0bd8b6/comments", "author": {"login": "matklad", "id": 1711539, "node_id": "MDQ6VXNlcjE3MTE1Mzk=", "avatar_url": "https://avatars.githubusercontent.com/u/1711539?v=4", "gravatar_id": "", "url": "https://api.github.com/users/matklad", "html_url": "https://github.com/matklad", "followers_url": "https://api.github.com/users/matklad/followers", "following_url": "https://api.github.com/users/matklad/following{/other_user}", "gists_url": "https://api.github.com/users/matklad/gists{/gist_id}", "starred_url": "https://api.github.com/users/matklad/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/matklad/subscriptions", "organizations_url": "https://api.github.com/users/matklad/orgs", "repos_url": "https://api.github.com/users/matklad/repos", "events_url": "https://api.github.com/users/matklad/events{/privacy}", "received_events_url": "https://api.github.com/users/matklad/received_events", "type": "User", "site_admin": false}, "committer": {"login": "web-flow", "id": 19864447, "node_id": "MDQ6VXNlcjE5ODY0NDQ3", "avatar_url": "https://avatars.githubusercontent.com/u/19864447?v=4", "gravatar_id": "", "url": "https://api.github.com/users/web-flow", "html_url": "https://github.com/web-flow", "followers_url": "https://api.github.com/users/web-flow/followers", "following_url": "https://api.github.com/users/web-flow/following{/other_user}", "gists_url": "https://api.github.com/users/web-flow/gists{/gist_id}", "starred_url": "https://api.github.com/users/web-flow/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/web-flow/subscriptions", "organizations_url": "https://api.github.com/users/web-flow/orgs", "repos_url": "https://api.github.com/users/web-flow/repos", "events_url": "https://api.github.com/users/web-flow/events{/privacy}", "received_events_url": "https://api.github.com/users/web-flow/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "4e2a25d759d3e014dc5e47872d8cff2a4a821b08", "url": "https://api.github.com/repos/rust-lang/rust/commits/4e2a25d759d3e014dc5e47872d8cff2a4a821b08", "html_url": "https://github.com/rust-lang/rust/commit/4e2a25d759d3e014dc5e47872d8cff2a4a821b08"}, {"sha": "518cac91902d34567ac8bfea3022f426a7de53f6", "url": "https://api.github.com/repos/rust-lang/rust/commits/518cac91902d34567ac8bfea3022f426a7de53f6", "html_url": "https://github.com/rust-lang/rust/commit/518cac91902d34567ac8bfea3022f426a7de53f6"}], "stats": {"total": 421, "additions": 140, "deletions": 281}, "files": [{"sha": "5436b1ef737f519ba7980bafaf62cbbf4ddb9ccc", "filename": "compiler/rustc_expand/src/lib.rs", "status": "modified", "additions": 0, "deletions": 5, "changes": 5, "blob_url": "https://github.com/rust-lang/rust/blob/7d2460e5c4e5714ce72fc21bc65d94957e0bd8b6/compiler%2Frustc_expand%2Fsrc%2Flib.rs", "raw_url": "https://github.com/rust-lang/rust/raw/7d2460e5c4e5714ce72fc21bc65d94957e0bd8b6/compiler%2Frustc_expand%2Fsrc%2Flib.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_expand%2Fsrc%2Flib.rs?ref=7d2460e5c4e5714ce72fc21bc65d94957e0bd8b6", "patch": "@@ -39,11 +39,6 @@ mod tests;\n mod parse {\n     #[cfg(test)]\n     mod tests;\n-    #[cfg(test)]\n-    mod lexer {\n-        #[cfg(test)]\n-        mod tests;\n-    }\n }\n #[cfg(test)]\n mod tokenstream {"}, {"sha": "871844442839cf7fd27f1a735de3cb7be9dbc7b8", "filename": "compiler/rustc_expand/src/parse/lexer/tests.rs", "status": "removed", "additions": 0, "deletions": 252, "changes": 252, "blob_url": "https://github.com/rust-lang/rust/blob/4e2a25d759d3e014dc5e47872d8cff2a4a821b08/compiler%2Frustc_expand%2Fsrc%2Fparse%2Flexer%2Ftests.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4e2a25d759d3e014dc5e47872d8cff2a4a821b08/compiler%2Frustc_expand%2Fsrc%2Fparse%2Flexer%2Ftests.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_expand%2Fsrc%2Fparse%2Flexer%2Ftests.rs?ref=4e2a25d759d3e014dc5e47872d8cff2a4a821b08", "patch": "@@ -1,252 +0,0 @@\n-use rustc_ast::ast::AttrStyle;\n-use rustc_ast::token::{self, CommentKind, Token, TokenKind};\n-use rustc_data_structures::sync::Lrc;\n-use rustc_errors::{emitter::EmitterWriter, Handler};\n-use rustc_parse::lexer::StringReader;\n-use rustc_session::parse::ParseSess;\n-use rustc_span::source_map::{FilePathMapping, SourceMap};\n-use rustc_span::symbol::Symbol;\n-use rustc_span::with_default_session_globals;\n-use rustc_span::{BytePos, Span};\n-\n-use std::io;\n-use std::path::PathBuf;\n-\n-fn mk_sess(sm: Lrc<SourceMap>) -> ParseSess {\n-    let emitter = EmitterWriter::new(\n-        Box::new(io::sink()),\n-        Some(sm.clone()),\n-        false,\n-        false,\n-        false,\n-        None,\n-        false,\n-    );\n-    ParseSess::with_span_handler(Handler::with_emitter(true, None, Box::new(emitter)), sm)\n-}\n-\n-// Creates a string reader for the given string.\n-fn setup<'a>(sm: &SourceMap, sess: &'a ParseSess, teststr: String) -> StringReader<'a> {\n-    let sf = sm.new_source_file(PathBuf::from(teststr.clone()).into(), teststr);\n-    StringReader::new(sess, sf, None)\n-}\n-\n-#[test]\n-fn t1() {\n-    with_default_session_globals(|| {\n-        let sm = Lrc::new(SourceMap::new(FilePathMapping::empty()));\n-        let sh = mk_sess(sm.clone());\n-        let mut string_reader = setup(\n-            &sm,\n-            &sh,\n-            \"/* my source file */ fn main() { println!(\\\"zebra\\\"); }\\n\".to_string(),\n-        );\n-        assert_eq!(string_reader.next_token(), token::Comment);\n-        assert_eq!(string_reader.next_token(), token::Whitespace);\n-        let tok1 = string_reader.next_token();\n-        let tok2 = Token::new(mk_ident(\"fn\"), Span::with_root_ctxt(BytePos(21), BytePos(23)));\n-        assert_eq!(tok1.kind, tok2.kind);\n-        assert_eq!(tok1.span, tok2.span);\n-        assert_eq!(string_reader.next_token(), token::Whitespace);\n-        // Read another token.\n-        let tok3 = string_reader.next_token();\n-        assert_eq!(string_reader.pos(), BytePos(28));\n-        let tok4 = Token::new(mk_ident(\"main\"), Span::with_root_ctxt(BytePos(24), BytePos(28)));\n-        assert_eq!(tok3.kind, tok4.kind);\n-        assert_eq!(tok3.span, tok4.span);\n-\n-        assert_eq!(string_reader.next_token(), token::OpenDelim(token::Paren));\n-        assert_eq!(string_reader.pos(), BytePos(29))\n-    })\n-}\n-\n-// Checks that the given reader produces the desired stream\n-// of tokens (stop checking after exhausting `expected`).\n-fn check_tokenization(mut string_reader: StringReader<'_>, expected: Vec<TokenKind>) {\n-    for expected_tok in &expected {\n-        assert_eq!(&string_reader.next_token(), expected_tok);\n-    }\n-}\n-\n-// Makes the identifier by looking up the string in the interner.\n-fn mk_ident(id: &str) -> TokenKind {\n-    token::Ident(Symbol::intern(id), false)\n-}\n-\n-fn mk_lit(kind: token::LitKind, symbol: &str, suffix: Option<&str>) -> TokenKind {\n-    TokenKind::lit(kind, Symbol::intern(symbol), suffix.map(Symbol::intern))\n-}\n-\n-#[test]\n-fn doublecolon_parsing() {\n-    with_default_session_globals(|| {\n-        let sm = Lrc::new(SourceMap::new(FilePathMapping::empty()));\n-        let sh = mk_sess(sm.clone());\n-        check_tokenization(\n-            setup(&sm, &sh, \"a b\".to_string()),\n-            vec![mk_ident(\"a\"), token::Whitespace, mk_ident(\"b\")],\n-        );\n-    })\n-}\n-\n-#[test]\n-fn doublecolon_parsing_2() {\n-    with_default_session_globals(|| {\n-        let sm = Lrc::new(SourceMap::new(FilePathMapping::empty()));\n-        let sh = mk_sess(sm.clone());\n-        check_tokenization(\n-            setup(&sm, &sh, \"a::b\".to_string()),\n-            vec![mk_ident(\"a\"), token::Colon, token::Colon, mk_ident(\"b\")],\n-        );\n-    })\n-}\n-\n-#[test]\n-fn doublecolon_parsing_3() {\n-    with_default_session_globals(|| {\n-        let sm = Lrc::new(SourceMap::new(FilePathMapping::empty()));\n-        let sh = mk_sess(sm.clone());\n-        check_tokenization(\n-            setup(&sm, &sh, \"a ::b\".to_string()),\n-            vec![mk_ident(\"a\"), token::Whitespace, token::Colon, token::Colon, mk_ident(\"b\")],\n-        );\n-    })\n-}\n-\n-#[test]\n-fn doublecolon_parsing_4() {\n-    with_default_session_globals(|| {\n-        let sm = Lrc::new(SourceMap::new(FilePathMapping::empty()));\n-        let sh = mk_sess(sm.clone());\n-        check_tokenization(\n-            setup(&sm, &sh, \"a:: b\".to_string()),\n-            vec![mk_ident(\"a\"), token::Colon, token::Colon, token::Whitespace, mk_ident(\"b\")],\n-        );\n-    })\n-}\n-\n-#[test]\n-fn character_a() {\n-    with_default_session_globals(|| {\n-        let sm = Lrc::new(SourceMap::new(FilePathMapping::empty()));\n-        let sh = mk_sess(sm.clone());\n-        assert_eq!(setup(&sm, &sh, \"'a'\".to_string()).next_token(), mk_lit(token::Char, \"a\", None),);\n-    })\n-}\n-\n-#[test]\n-fn character_space() {\n-    with_default_session_globals(|| {\n-        let sm = Lrc::new(SourceMap::new(FilePathMapping::empty()));\n-        let sh = mk_sess(sm.clone());\n-        assert_eq!(setup(&sm, &sh, \"' '\".to_string()).next_token(), mk_lit(token::Char, \" \", None),);\n-    })\n-}\n-\n-#[test]\n-fn character_escaped() {\n-    with_default_session_globals(|| {\n-        let sm = Lrc::new(SourceMap::new(FilePathMapping::empty()));\n-        let sh = mk_sess(sm.clone());\n-        assert_eq!(\n-            setup(&sm, &sh, \"'\\\\n'\".to_string()).next_token(),\n-            mk_lit(token::Char, \"\\\\n\", None),\n-        );\n-    })\n-}\n-\n-#[test]\n-fn lifetime_name() {\n-    with_default_session_globals(|| {\n-        let sm = Lrc::new(SourceMap::new(FilePathMapping::empty()));\n-        let sh = mk_sess(sm.clone());\n-        assert_eq!(\n-            setup(&sm, &sh, \"'abc\".to_string()).next_token(),\n-            token::Lifetime(Symbol::intern(\"'abc\")),\n-        );\n-    })\n-}\n-\n-#[test]\n-fn raw_string() {\n-    with_default_session_globals(|| {\n-        let sm = Lrc::new(SourceMap::new(FilePathMapping::empty()));\n-        let sh = mk_sess(sm.clone());\n-        assert_eq!(\n-            setup(&sm, &sh, \"r###\\\"\\\"#a\\\\b\\x00c\\\"\\\"###\".to_string()).next_token(),\n-            mk_lit(token::StrRaw(3), \"\\\"#a\\\\b\\x00c\\\"\", None),\n-        );\n-    })\n-}\n-\n-#[test]\n-fn literal_suffixes() {\n-    with_default_session_globals(|| {\n-        let sm = Lrc::new(SourceMap::new(FilePathMapping::empty()));\n-        let sh = mk_sess(sm.clone());\n-        macro_rules! test {\n-            ($input: expr, $tok_type: ident, $tok_contents: expr) => {{\n-                assert_eq!(\n-                    setup(&sm, &sh, format!(\"{}suffix\", $input)).next_token(),\n-                    mk_lit(token::$tok_type, $tok_contents, Some(\"suffix\")),\n-                );\n-                // with a whitespace separator\n-                assert_eq!(\n-                    setup(&sm, &sh, format!(\"{} suffix\", $input)).next_token(),\n-                    mk_lit(token::$tok_type, $tok_contents, None),\n-                );\n-            }};\n-        }\n-\n-        test!(\"'a'\", Char, \"a\");\n-        test!(\"b'a'\", Byte, \"a\");\n-        test!(\"\\\"a\\\"\", Str, \"a\");\n-        test!(\"b\\\"a\\\"\", ByteStr, \"a\");\n-        test!(\"1234\", Integer, \"1234\");\n-        test!(\"0b101\", Integer, \"0b101\");\n-        test!(\"0xABC\", Integer, \"0xABC\");\n-        test!(\"1.0\", Float, \"1.0\");\n-        test!(\"1.0e10\", Float, \"1.0e10\");\n-\n-        assert_eq!(\n-            setup(&sm, &sh, \"2us\".to_string()).next_token(),\n-            mk_lit(token::Integer, \"2\", Some(\"us\")),\n-        );\n-        assert_eq!(\n-            setup(&sm, &sh, \"r###\\\"raw\\\"###suffix\".to_string()).next_token(),\n-            mk_lit(token::StrRaw(3), \"raw\", Some(\"suffix\")),\n-        );\n-        assert_eq!(\n-            setup(&sm, &sh, \"br###\\\"raw\\\"###suffix\".to_string()).next_token(),\n-            mk_lit(token::ByteStrRaw(3), \"raw\", Some(\"suffix\")),\n-        );\n-    })\n-}\n-\n-#[test]\n-fn nested_block_comments() {\n-    with_default_session_globals(|| {\n-        let sm = Lrc::new(SourceMap::new(FilePathMapping::empty()));\n-        let sh = mk_sess(sm.clone());\n-        let mut lexer = setup(&sm, &sh, \"/* /* */ */'a'\".to_string());\n-        assert_eq!(lexer.next_token(), token::Comment);\n-        assert_eq!(lexer.next_token(), mk_lit(token::Char, \"a\", None));\n-    })\n-}\n-\n-#[test]\n-fn crlf_comments() {\n-    with_default_session_globals(|| {\n-        let sm = Lrc::new(SourceMap::new(FilePathMapping::empty()));\n-        let sh = mk_sess(sm.clone());\n-        let mut lexer = setup(&sm, &sh, \"// test\\r\\n/// test\\r\\n\".to_string());\n-        let comment = lexer.next_token();\n-        assert_eq!(comment.kind, token::Comment);\n-        assert_eq!((comment.span.lo(), comment.span.hi()), (BytePos(0), BytePos(7)));\n-        assert_eq!(lexer.next_token(), token::Whitespace);\n-        assert_eq!(\n-            lexer.next_token(),\n-            token::DocComment(CommentKind::Line, AttrStyle::Outer, Symbol::intern(\" test\"))\n-        );\n-    })\n-}"}, {"sha": "94017b7b286e2bd79f364ad117bf54d2259a94ee", "filename": "compiler/rustc_lexer/src/tests.rs", "status": "modified", "additions": 140, "deletions": 20, "changes": 160, "blob_url": "https://github.com/rust-lang/rust/blob/7d2460e5c4e5714ce72fc21bc65d94957e0bd8b6/compiler%2Frustc_lexer%2Fsrc%2Ftests.rs", "raw_url": "https://github.com/rust-lang/rust/raw/7d2460e5c4e5714ce72fc21bc65d94957e0bd8b6/compiler%2Frustc_lexer%2Fsrc%2Ftests.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_lexer%2Fsrc%2Ftests.rs?ref=7d2460e5c4e5714ce72fc21bc65d94957e0bd8b6", "patch": "@@ -128,6 +128,34 @@ fn check_lexing(src: &str, expect: Expect) {\n     expect.assert_eq(&actual)\n }\n \n+#[test]\n+fn smoke_test() {\n+    check_lexing(\n+        \"/* my source file */ fn main() { println!(\\\"zebra\\\"); }\\n\",\n+        expect![[r#\"\n+            Token { kind: BlockComment { doc_style: None, terminated: true }, len: 20 }\n+            Token { kind: Whitespace, len: 1 }\n+            Token { kind: Ident, len: 2 }\n+            Token { kind: Whitespace, len: 1 }\n+            Token { kind: Ident, len: 4 }\n+            Token { kind: OpenParen, len: 1 }\n+            Token { kind: CloseParen, len: 1 }\n+            Token { kind: Whitespace, len: 1 }\n+            Token { kind: OpenBrace, len: 1 }\n+            Token { kind: Whitespace, len: 1 }\n+            Token { kind: Ident, len: 7 }\n+            Token { kind: Bang, len: 1 }\n+            Token { kind: OpenParen, len: 1 }\n+            Token { kind: Literal { kind: Str { terminated: true }, suffix_start: 7 }, len: 7 }\n+            Token { kind: CloseParen, len: 1 }\n+            Token { kind: Semi, len: 1 }\n+            Token { kind: Whitespace, len: 1 }\n+            Token { kind: CloseBrace, len: 1 }\n+            Token { kind: Whitespace, len: 1 }\n+        \"#]],\n+    )\n+}\n+\n #[test]\n fn comment_flavors() {\n     check_lexing(\n@@ -143,25 +171,117 @@ fn comment_flavors() {\n /*! inner doc block */\n \",\n         expect![[r#\"\n-                Token { kind: Whitespace, len: 1 }\n-                Token { kind: LineComment { doc_style: None }, len: 7 }\n-                Token { kind: Whitespace, len: 1 }\n-                Token { kind: LineComment { doc_style: None }, len: 17 }\n-                Token { kind: Whitespace, len: 1 }\n-                Token { kind: LineComment { doc_style: Some(Outer) }, len: 18 }\n-                Token { kind: Whitespace, len: 1 }\n-                Token { kind: LineComment { doc_style: Some(Inner) }, len: 18 }\n-                Token { kind: Whitespace, len: 1 }\n-                Token { kind: BlockComment { doc_style: None, terminated: true }, len: 11 }\n-                Token { kind: Whitespace, len: 1 }\n-                Token { kind: BlockComment { doc_style: None, terminated: true }, len: 4 }\n-                Token { kind: Whitespace, len: 1 }\n-                Token { kind: BlockComment { doc_style: None, terminated: true }, len: 18 }\n-                Token { kind: Whitespace, len: 1 }\n-                Token { kind: BlockComment { doc_style: Some(Outer), terminated: true }, len: 22 }\n-                Token { kind: Whitespace, len: 1 }\n-                Token { kind: BlockComment { doc_style: Some(Inner), terminated: true }, len: 22 }\n-                Token { kind: Whitespace, len: 1 }\n-            \"#]],\n+            Token { kind: Whitespace, len: 1 }\n+            Token { kind: LineComment { doc_style: None }, len: 7 }\n+            Token { kind: Whitespace, len: 1 }\n+            Token { kind: LineComment { doc_style: None }, len: 17 }\n+            Token { kind: Whitespace, len: 1 }\n+            Token { kind: LineComment { doc_style: Some(Outer) }, len: 18 }\n+            Token { kind: Whitespace, len: 1 }\n+            Token { kind: LineComment { doc_style: Some(Inner) }, len: 18 }\n+            Token { kind: Whitespace, len: 1 }\n+            Token { kind: BlockComment { doc_style: None, terminated: true }, len: 11 }\n+            Token { kind: Whitespace, len: 1 }\n+            Token { kind: BlockComment { doc_style: None, terminated: true }, len: 4 }\n+            Token { kind: Whitespace, len: 1 }\n+            Token { kind: BlockComment { doc_style: None, terminated: true }, len: 18 }\n+            Token { kind: Whitespace, len: 1 }\n+            Token { kind: BlockComment { doc_style: Some(Outer), terminated: true }, len: 22 }\n+            Token { kind: Whitespace, len: 1 }\n+            Token { kind: BlockComment { doc_style: Some(Inner), terminated: true }, len: 22 }\n+            Token { kind: Whitespace, len: 1 }\n+        \"#]],\n+    )\n+}\n+\n+#[test]\n+fn nested_block_comments() {\n+    check_lexing(\n+        \"/* /* */ */'a'\",\n+        expect![[r#\"\n+            Token { kind: BlockComment { doc_style: None, terminated: true }, len: 11 }\n+            Token { kind: Literal { kind: Char { terminated: true }, suffix_start: 3 }, len: 3 }\n+        \"#]],\n+    )\n+}\n+\n+#[test]\n+fn characters() {\n+    check_lexing(\n+        \"'a' ' ' '\\\\n'\",\n+        expect![[r#\"\n+            Token { kind: Literal { kind: Char { terminated: true }, suffix_start: 3 }, len: 3 }\n+            Token { kind: Whitespace, len: 1 }\n+            Token { kind: Literal { kind: Char { terminated: true }, suffix_start: 3 }, len: 3 }\n+            Token { kind: Whitespace, len: 1 }\n+            Token { kind: Literal { kind: Char { terminated: true }, suffix_start: 4 }, len: 4 }\n+        \"#]],\n+    );\n+}\n+\n+#[test]\n+fn lifetime() {\n+    check_lexing(\n+        \"'abc\",\n+        expect![[r#\"\n+            Token { kind: Lifetime { starts_with_number: false }, len: 4 }\n+        \"#]],\n+    );\n+}\n+\n+#[test]\n+fn raw_string() {\n+    check_lexing(\n+        \"r###\\\"\\\"#a\\\\b\\x00c\\\"\\\"###\",\n+        expect![[r#\"\n+            Token { kind: Literal { kind: RawStr { n_hashes: 3, err: None }, suffix_start: 17 }, len: 17 }\n+        \"#]],\n+    )\n+}\n+\n+#[test]\n+fn literal_suffixes() {\n+    check_lexing(\n+        r####\"\n+'a'\n+b'a'\n+\"a\"\n+b\"a\"\n+1234\n+0b101\n+0xABC\n+1.0\n+1.0e10\n+2us\n+r###\"raw\"###suffix\n+br###\"raw\"###suffix\n+\"####,\n+        expect![[r#\"\n+            Token { kind: Whitespace, len: 1 }\n+            Token { kind: Literal { kind: Char { terminated: true }, suffix_start: 3 }, len: 3 }\n+            Token { kind: Whitespace, len: 1 }\n+            Token { kind: Literal { kind: Byte { terminated: true }, suffix_start: 4 }, len: 4 }\n+            Token { kind: Whitespace, len: 1 }\n+            Token { kind: Literal { kind: Str { terminated: true }, suffix_start: 3 }, len: 3 }\n+            Token { kind: Whitespace, len: 1 }\n+            Token { kind: Literal { kind: ByteStr { terminated: true }, suffix_start: 4 }, len: 4 }\n+            Token { kind: Whitespace, len: 1 }\n+            Token { kind: Literal { kind: Int { base: Decimal, empty_int: false }, suffix_start: 4 }, len: 4 }\n+            Token { kind: Whitespace, len: 1 }\n+            Token { kind: Literal { kind: Int { base: Binary, empty_int: false }, suffix_start: 5 }, len: 5 }\n+            Token { kind: Whitespace, len: 1 }\n+            Token { kind: Literal { kind: Int { base: Hexadecimal, empty_int: false }, suffix_start: 5 }, len: 5 }\n+            Token { kind: Whitespace, len: 1 }\n+            Token { kind: Literal { kind: Float { base: Decimal, empty_exponent: false }, suffix_start: 3 }, len: 3 }\n+            Token { kind: Whitespace, len: 1 }\n+            Token { kind: Literal { kind: Float { base: Decimal, empty_exponent: false }, suffix_start: 6 }, len: 6 }\n+            Token { kind: Whitespace, len: 1 }\n+            Token { kind: Literal { kind: Int { base: Decimal, empty_int: false }, suffix_start: 1 }, len: 3 }\n+            Token { kind: Whitespace, len: 1 }\n+            Token { kind: Literal { kind: RawStr { n_hashes: 3, err: None }, suffix_start: 12 }, len: 18 }\n+            Token { kind: Whitespace, len: 1 }\n+            Token { kind: Literal { kind: RawByteStr { n_hashes: 3, err: None }, suffix_start: 13 }, len: 19 }\n+            Token { kind: Whitespace, len: 1 }\n+        \"#]],\n     )\n }"}, {"sha": "c4ef35bc30c70928da1e1670567e173895e7381c", "filename": "compiler/rustc_parse/src/lexer/mod.rs", "status": "modified", "additions": 0, "deletions": 4, "changes": 4, "blob_url": "https://github.com/rust-lang/rust/blob/7d2460e5c4e5714ce72fc21bc65d94957e0bd8b6/compiler%2Frustc_parse%2Fsrc%2Flexer%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/7d2460e5c4e5714ce72fc21bc65d94957e0bd8b6/compiler%2Frustc_parse%2Fsrc%2Flexer%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_parse%2Fsrc%2Flexer%2Fmod.rs?ref=7d2460e5c4e5714ce72fc21bc65d94957e0bd8b6", "patch": "@@ -439,10 +439,6 @@ impl<'a> StringReader<'a> {\n         (lit_kind, id)\n     }\n \n-    pub fn pos(&self) -> BytePos {\n-        self.pos\n-    }\n-\n     #[inline]\n     fn src_index(&self, pos: BytePos) -> usize {\n         (pos - self.start_pos).to_usize()"}]}