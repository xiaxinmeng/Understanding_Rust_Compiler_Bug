{"sha": "a3425edb46dfcc7031068b8bdda868e5a3b16ae1", "node_id": "MDY6Q29tbWl0NzI0NzEyOmEzNDI1ZWRiNDZkZmNjNzAzMTA2OGI4YmRkYTg2OGU1YTNiMTZhZTE=", "commit": {"author": {"name": "Vadim Petrochenkov", "email": "vadim.petrochenkov@gmail.com", "date": "2019-06-04T15:48:40Z"}, "committer": {"name": "Vadim Petrochenkov", "email": "vadim.petrochenkov@gmail.com", "date": "2019-06-06T11:03:15Z"}, "message": "syntax: Rename `TokenAndSpan` into `Token`", "tree": {"sha": "e62b7f6bd83cc5c3676e4df1f15b7c416667ca4a", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/e62b7f6bd83cc5c3676e4df1f15b7c416667ca4a"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/a3425edb46dfcc7031068b8bdda868e5a3b16ae1", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/a3425edb46dfcc7031068b8bdda868e5a3b16ae1", "html_url": "https://github.com/rust-lang/rust/commit/a3425edb46dfcc7031068b8bdda868e5a3b16ae1", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/a3425edb46dfcc7031068b8bdda868e5a3b16ae1/comments", "author": {"login": "petrochenkov", "id": 5751617, "node_id": "MDQ6VXNlcjU3NTE2MTc=", "avatar_url": "https://avatars.githubusercontent.com/u/5751617?v=4", "gravatar_id": "", "url": "https://api.github.com/users/petrochenkov", "html_url": "https://github.com/petrochenkov", "followers_url": "https://api.github.com/users/petrochenkov/followers", "following_url": "https://api.github.com/users/petrochenkov/following{/other_user}", "gists_url": "https://api.github.com/users/petrochenkov/gists{/gist_id}", "starred_url": "https://api.github.com/users/petrochenkov/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/petrochenkov/subscriptions", "organizations_url": "https://api.github.com/users/petrochenkov/orgs", "repos_url": "https://api.github.com/users/petrochenkov/repos", "events_url": "https://api.github.com/users/petrochenkov/events{/privacy}", "received_events_url": "https://api.github.com/users/petrochenkov/received_events", "type": "User", "site_admin": false}, "committer": {"login": "petrochenkov", "id": 5751617, "node_id": "MDQ6VXNlcjU3NTE2MTc=", "avatar_url": "https://avatars.githubusercontent.com/u/5751617?v=4", "gravatar_id": "", "url": "https://api.github.com/users/petrochenkov", "html_url": "https://github.com/petrochenkov", "followers_url": "https://api.github.com/users/petrochenkov/followers", "following_url": "https://api.github.com/users/petrochenkov/following{/other_user}", "gists_url": "https://api.github.com/users/petrochenkov/gists{/gist_id}", "starred_url": "https://api.github.com/users/petrochenkov/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/petrochenkov/subscriptions", "organizations_url": "https://api.github.com/users/petrochenkov/orgs", "repos_url": "https://api.github.com/users/petrochenkov/repos", "events_url": "https://api.github.com/users/petrochenkov/events{/privacy}", "received_events_url": "https://api.github.com/users/petrochenkov/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "99b27d749c22117eccf862f5ee4eb540b65b681f", "url": "https://api.github.com/repos/rust-lang/rust/commits/99b27d749c22117eccf862f5ee4eb540b65b681f", "html_url": "https://github.com/rust-lang/rust/commit/99b27d749c22117eccf862f5ee4eb540b65b681f"}], "stats": {"total": 240, "additions": 118, "deletions": 122}, "files": [{"sha": "5831b0bcd8fa37a65e8cdb307dc13d611d227d5d", "filename": "src/librustc_save_analysis/span_utils.rs", "status": "modified", "additions": 9, "deletions": 9, "changes": 18, "blob_url": "https://github.com/rust-lang/rust/blob/a3425edb46dfcc7031068b8bdda868e5a3b16ae1/src%2Flibrustc_save_analysis%2Fspan_utils.rs", "raw_url": "https://github.com/rust-lang/rust/raw/a3425edb46dfcc7031068b8bdda868e5a3b16ae1/src%2Flibrustc_save_analysis%2Fspan_utils.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_save_analysis%2Fspan_utils.rs?ref=a3425edb46dfcc7031068b8bdda868e5a3b16ae1", "patch": "@@ -60,11 +60,11 @@ impl<'a> SpanUtils<'a> {\n         let mut toks = self.retokenise_span(span);\n         loop {\n             let next = toks.real_token();\n-            if next.tok == token::Eof {\n+            if next == token::Eof {\n                 return None;\n             }\n-            if next.tok == tok {\n-                return Some(next.sp);\n+            if next == tok {\n+                return Some(next.span);\n             }\n         }\n     }\n@@ -74,12 +74,12 @@ impl<'a> SpanUtils<'a> {\n     //     let mut toks = self.retokenise_span(span);\n     //     loop {\n     //         let ts = toks.real_token();\n-    //         if ts.tok == token::Eof {\n+    //         if ts == token::Eof {\n     //             return None;\n     //         }\n-    //         if ts.tok == token::Not {\n+    //         if ts == token::Not {\n     //             let ts = toks.real_token();\n-    //             if ts.tok.is_ident() {\n+    //             if ts.kind.is_ident() {\n     //                 return Some(ts.sp);\n     //             } else {\n     //                 return None;\n@@ -93,12 +93,12 @@ impl<'a> SpanUtils<'a> {\n     //     let mut toks = self.retokenise_span(span);\n     //     let mut prev = toks.real_token();\n     //     loop {\n-    //         if prev.tok == token::Eof {\n+    //         if prev == token::Eof {\n     //             return None;\n     //         }\n     //         let ts = toks.real_token();\n-    //         if ts.tok == token::Not {\n-    //             if prev.tok.is_ident() {\n+    //         if ts == token::Not {\n+    //             if prev.kind.is_ident() {\n     //                 return Some(prev.sp);\n     //             } else {\n     //                 return None;"}, {"sha": "3b9de761828b7387c1eeecebcab26e90005f397e", "filename": "src/librustdoc/html/highlight.rs", "status": "modified", "additions": 15, "deletions": 15, "changes": 30, "blob_url": "https://github.com/rust-lang/rust/blob/a3425edb46dfcc7031068b8bdda868e5a3b16ae1/src%2Flibrustdoc%2Fhtml%2Fhighlight.rs", "raw_url": "https://github.com/rust-lang/rust/raw/a3425edb46dfcc7031068b8bdda868e5a3b16ae1/src%2Flibrustdoc%2Fhtml%2Fhighlight.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustdoc%2Fhtml%2Fhighlight.rs?ref=a3425edb46dfcc7031068b8bdda868e5a3b16ae1", "patch": "@@ -12,8 +12,8 @@ use std::io;\n use std::io::prelude::*;\n \n use syntax::source_map::{SourceMap, FilePathMapping};\n-use syntax::parse::lexer::{self, TokenAndSpan};\n-use syntax::parse::token;\n+use syntax::parse::lexer;\n+use syntax::parse::token::{self, Token};\n use syntax::parse;\n use syntax::symbol::{kw, sym};\n use syntax_pos::{Span, FileName};\n@@ -186,9 +186,9 @@ impl<'a> Classifier<'a> {\n     }\n \n     /// Gets the next token out of the lexer.\n-    fn try_next_token(&mut self) -> Result<TokenAndSpan, HighlightError> {\n+    fn try_next_token(&mut self) -> Result<Token, HighlightError> {\n         match self.lexer.try_next_token() {\n-            Ok(tas) => Ok(tas),\n+            Ok(token) => Ok(token),\n             Err(_) => Err(HighlightError::LexError),\n         }\n     }\n@@ -205,7 +205,7 @@ impl<'a> Classifier<'a> {\n                                    -> Result<(), HighlightError> {\n         loop {\n             let next = self.try_next_token()?;\n-            if next.tok == token::Eof {\n+            if next == token::Eof {\n                 break;\n             }\n \n@@ -218,9 +218,9 @@ impl<'a> Classifier<'a> {\n     // Handles an individual token from the lexer.\n     fn write_token<W: Writer>(&mut self,\n                               out: &mut W,\n-                              tas: TokenAndSpan)\n+                              token: Token)\n                               -> Result<(), HighlightError> {\n-        let klass = match tas.tok {\n+        let klass = match token.kind {\n             token::Shebang(s) => {\n                 out.string(Escape(&s.as_str()), Class::None)?;\n                 return Ok(());\n@@ -234,7 +234,7 @@ impl<'a> Classifier<'a> {\n             // reference or dereference operator or a reference or pointer type, instead of the\n             // bit-and or multiplication operator.\n             token::BinOp(token::And) | token::BinOp(token::Star)\n-                if self.lexer.peek().tok != token::Whitespace => Class::RefKeyWord,\n+                if self.lexer.peek().kind != token::Whitespace => Class::RefKeyWord,\n \n             // Consider this as part of a macro invocation if there was a\n             // leading identifier.\n@@ -257,7 +257,7 @@ impl<'a> Classifier<'a> {\n             token::Question => Class::QuestionMark,\n \n             token::Dollar => {\n-                if self.lexer.peek().tok.is_ident() {\n+                if self.lexer.peek().kind.is_ident() {\n                     self.in_macro_nonterminal = true;\n                     Class::MacroNonTerminal\n                 } else {\n@@ -280,9 +280,9 @@ impl<'a> Classifier<'a> {\n                 // as an attribute.\n \n                 // Case 1: #![inner_attribute]\n-                if self.lexer.peek().tok == token::Not {\n+                if self.lexer.peek() == token::Not {\n                     self.try_next_token()?; // NOTE: consumes `!` token!\n-                    if self.lexer.peek().tok == token::OpenDelim(token::Bracket) {\n+                    if self.lexer.peek() == token::OpenDelim(token::Bracket) {\n                         self.in_attribute = true;\n                         out.enter_span(Class::Attribute)?;\n                     }\n@@ -292,7 +292,7 @@ impl<'a> Classifier<'a> {\n                 }\n \n                 // Case 2: #[outer_attribute]\n-                if self.lexer.peek().tok == token::OpenDelim(token::Bracket) {\n+                if self.lexer.peek() == token::OpenDelim(token::Bracket) {\n                     self.in_attribute = true;\n                     out.enter_span(Class::Attribute)?;\n                 }\n@@ -335,13 +335,13 @@ impl<'a> Classifier<'a> {\n                     sym::Option | sym::Result => Class::PreludeTy,\n                     sym::Some | sym::None | sym::Ok | sym::Err => Class::PreludeVal,\n \n-                    _ if tas.tok.is_reserved_ident() => Class::KeyWord,\n+                    _ if token.kind.is_reserved_ident() => Class::KeyWord,\n \n                     _ => {\n                         if self.in_macro_nonterminal {\n                             self.in_macro_nonterminal = false;\n                             Class::MacroNonTerminal\n-                        } else if self.lexer.peek().tok == token::Not {\n+                        } else if self.lexer.peek() == token::Not {\n                             self.in_macro = true;\n                             Class::Macro\n                         } else {\n@@ -359,7 +359,7 @@ impl<'a> Classifier<'a> {\n \n         // Anything that didn't return above is the simple case where we the\n         // class just spans a single token, so we can use the `string` method.\n-        out.string(Escape(&self.snip(tas.sp)), klass)?;\n+        out.string(Escape(&self.snip(token.span)), klass)?;\n \n         Ok(())\n     }"}, {"sha": "694843ad7f71e1accb2f1ae3ad2d62c8a2ded3c1", "filename": "src/librustdoc/passes/check_code_block_syntax.rs", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "blob_url": "https://github.com/rust-lang/rust/blob/a3425edb46dfcc7031068b8bdda868e5a3b16ae1/src%2Flibrustdoc%2Fpasses%2Fcheck_code_block_syntax.rs", "raw_url": "https://github.com/rust-lang/rust/raw/a3425edb46dfcc7031068b8bdda868e5a3b16ae1/src%2Flibrustdoc%2Fpasses%2Fcheck_code_block_syntax.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustdoc%2Fpasses%2Fcheck_code_block_syntax.rs?ref=a3425edb46dfcc7031068b8bdda868e5a3b16ae1", "patch": "@@ -1,5 +1,5 @@\n use errors::Applicability;\n-use syntax::parse::lexer::{TokenAndSpan, StringReader as Lexer};\n+use syntax::parse::lexer::{StringReader as Lexer};\n use syntax::parse::{ParseSess, token};\n use syntax::source_map::FilePathMapping;\n use syntax_pos::FileName;\n@@ -33,8 +33,8 @@ impl<'a, 'tcx> SyntaxChecker<'a, 'tcx> {\n         );\n \n         let errors = Lexer::new_or_buffered_errs(&sess, source_file, None).and_then(|mut lexer| {\n-            while let Ok(TokenAndSpan { tok, .. }) = lexer.try_next_token() {\n-                if tok == token::Eof {\n+            while let Ok(token::Token { kind, .. }) = lexer.try_next_token() {\n+                if kind == token::Eof {\n                     break;\n                 }\n             }"}, {"sha": "32d5b16dd714f7d09c3fefb933ea03e487b909b7", "filename": "src/libsyntax/parse/lexer/mod.rs", "status": "modified", "additions": 62, "deletions": 77, "changes": 139, "blob_url": "https://github.com/rust-lang/rust/blob/a3425edb46dfcc7031068b8bdda868e5a3b16ae1/src%2Flibsyntax%2Fparse%2Flexer%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/a3425edb46dfcc7031068b8bdda868e5a3b16ae1/src%2Flibsyntax%2Fparse%2Flexer%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Flexer%2Fmod.rs?ref=a3425edb46dfcc7031068b8bdda868e5a3b16ae1", "patch": "@@ -1,6 +1,6 @@\n use crate::ast::{self, Ident};\n use crate::parse::ParseSess;\n-use crate::parse::token::{self, TokenKind};\n+use crate::parse::token::{self, Token, TokenKind};\n use crate::symbol::{sym, Symbol};\n use crate::parse::unescape;\n use crate::parse::unescape_error_reporting::{emit_unescape_error, push_escaped_char};\n@@ -20,21 +20,6 @@ pub mod comments;\n mod tokentrees;\n mod unicode_chars;\n \n-#[derive(Clone, Debug)]\n-pub struct TokenAndSpan {\n-    pub tok: TokenKind,\n-    pub sp: Span,\n-}\n-\n-impl Default for TokenAndSpan {\n-    fn default() -> Self {\n-        TokenAndSpan {\n-            tok: token::Whitespace,\n-            sp: syntax_pos::DUMMY_SP,\n-        }\n-    }\n-}\n-\n #[derive(Clone, Debug)]\n pub struct UnmatchedBrace {\n     pub expected_delim: token::DelimToken,\n@@ -87,7 +72,7 @@ impl<'a> StringReader<'a> {\n         ident\n     }\n \n-    fn unwrap_or_abort(&mut self, res: Result<TokenAndSpan, ()>) -> TokenAndSpan {\n+    fn unwrap_or_abort(&mut self, res: Result<Token, ()>) -> Token {\n         match res {\n             Ok(tok) => tok,\n             Err(_) => {\n@@ -97,17 +82,17 @@ impl<'a> StringReader<'a> {\n         }\n     }\n \n-    fn next_token(&mut self) -> TokenAndSpan where Self: Sized {\n+    fn next_token(&mut self) -> Token where Self: Sized {\n         let res = self.try_next_token();\n         self.unwrap_or_abort(res)\n     }\n \n     /// Returns the next token. EFFECT: advances the string_reader.\n-    pub fn try_next_token(&mut self) -> Result<TokenAndSpan, ()> {\n+    pub fn try_next_token(&mut self) -> Result<Token, ()> {\n         assert!(self.fatal_errs.is_empty());\n-        let ret_val = TokenAndSpan {\n-            tok: replace(&mut self.peek_tok, token::Whitespace),\n-            sp: self.peek_span,\n+        let ret_val = Token {\n+            kind: replace(&mut self.peek_tok, token::Whitespace),\n+            span: self.peek_span,\n         };\n         self.advance_token()?;\n         Ok(ret_val)\n@@ -135,10 +120,10 @@ impl<'a> StringReader<'a> {\n         return None;\n     }\n \n-    fn try_real_token(&mut self) -> Result<TokenAndSpan, ()> {\n+    fn try_real_token(&mut self) -> Result<Token, ()> {\n         let mut t = self.try_next_token()?;\n         loop {\n-            match t.tok {\n+            match t.kind {\n                 token::Whitespace | token::Comment | token::Shebang(_) => {\n                     t = self.try_next_token()?;\n                 }\n@@ -149,7 +134,7 @@ impl<'a> StringReader<'a> {\n         Ok(t)\n     }\n \n-    pub fn real_token(&mut self) -> TokenAndSpan {\n+    pub fn real_token(&mut self) -> Token {\n         let res = self.try_real_token();\n         self.unwrap_or_abort(res)\n     }\n@@ -194,11 +179,11 @@ impl<'a> StringReader<'a> {\n         buffer\n     }\n \n-    pub fn peek(&self) -> TokenAndSpan {\n+    pub fn peek(&self) -> Token {\n         // FIXME(pcwalton): Bad copy!\n-        TokenAndSpan {\n-            tok: self.peek_tok.clone(),\n-            sp: self.peek_span,\n+        Token {\n+            kind: self.peek_tok.clone(),\n+            span: self.peek_span,\n         }\n     }\n \n@@ -341,9 +326,9 @@ impl<'a> StringReader<'a> {\n     fn advance_token(&mut self) -> Result<(), ()> {\n         match self.scan_whitespace_or_comment() {\n             Some(comment) => {\n-                self.peek_span_src_raw = comment.sp;\n-                self.peek_span = comment.sp;\n-                self.peek_tok = comment.tok;\n+                self.peek_span_src_raw = comment.span;\n+                self.peek_span = comment.span;\n+                self.peek_tok = comment.kind;\n             }\n             None => {\n                 if self.is_eof() {\n@@ -527,7 +512,7 @@ impl<'a> StringReader<'a> {\n \n     /// PRECONDITION: self.ch is not whitespace\n     /// Eats any kind of comment.\n-    fn scan_comment(&mut self) -> Option<TokenAndSpan> {\n+    fn scan_comment(&mut self) -> Option<Token> {\n         if let Some(c) = self.ch {\n             if c.is_whitespace() {\n                 let msg = \"called consume_any_line_comment, but there was whitespace\";\n@@ -563,14 +548,14 @@ impl<'a> StringReader<'a> {\n                         self.bump();\n                     }\n \n-                    let tok = if doc_comment {\n+                    let kind = if doc_comment {\n                         self.with_str_from(start_bpos, |string| {\n                             token::DocComment(Symbol::intern(string))\n                         })\n                     } else {\n                         token::Comment\n                     };\n-                    Some(TokenAndSpan { tok, sp: self.mk_sp(start_bpos, self.pos) })\n+                    Some(Token { kind, span: self.mk_sp(start_bpos, self.pos) })\n                 }\n                 Some('*') => {\n                     self.bump();\n@@ -594,9 +579,9 @@ impl<'a> StringReader<'a> {\n                     while !self.ch_is('\\n') && !self.is_eof() {\n                         self.bump();\n                     }\n-                    return Some(TokenAndSpan {\n-                        tok: token::Shebang(self.name_from(start)),\n-                        sp: self.mk_sp(start, self.pos),\n+                    return Some(Token {\n+                        kind: token::Shebang(self.name_from(start)),\n+                        span: self.mk_sp(start, self.pos),\n                     });\n                 }\n             }\n@@ -608,7 +593,7 @@ impl<'a> StringReader<'a> {\n \n     /// If there is whitespace, shebang, or a comment, scan it. Otherwise,\n     /// return `None`.\n-    fn scan_whitespace_or_comment(&mut self) -> Option<TokenAndSpan> {\n+    fn scan_whitespace_or_comment(&mut self) -> Option<Token> {\n         match self.ch.unwrap_or('\\0') {\n             // # to handle shebang at start of file -- this is the entry point\n             // for skipping over all \"junk\"\n@@ -622,9 +607,9 @@ impl<'a> StringReader<'a> {\n                 while is_pattern_whitespace(self.ch) {\n                     self.bump();\n                 }\n-                let c = Some(TokenAndSpan {\n-                    tok: token::Whitespace,\n-                    sp: self.mk_sp(start_bpos, self.pos),\n+                let c = Some(Token {\n+                    kind: token::Whitespace,\n+                    span: self.mk_sp(start_bpos, self.pos),\n                 });\n                 debug!(\"scanning whitespace: {:?}\", c);\n                 c\n@@ -634,7 +619,7 @@ impl<'a> StringReader<'a> {\n     }\n \n     /// Might return a sugared-doc-attr\n-    fn scan_block_comment(&mut self) -> Option<TokenAndSpan> {\n+    fn scan_block_comment(&mut self) -> Option<Token> {\n         // block comments starting with \"/**\" or \"/*!\" are doc-comments\n         let is_doc_comment = self.ch_is('*') || self.ch_is('!');\n         let start_bpos = self.pos - BytePos(2);\n@@ -671,7 +656,7 @@ impl<'a> StringReader<'a> {\n \n         self.with_str_from(start_bpos, |string| {\n             // but comments with only \"*\"s between two \"/\"s are not\n-            let tok = if is_block_doc_comment(string) {\n+            let kind = if is_block_doc_comment(string) {\n                 let string = if has_cr {\n                     self.translate_crlf(start_bpos,\n                                         string,\n@@ -684,9 +669,9 @@ impl<'a> StringReader<'a> {\n                 token::Comment\n             };\n \n-            Some(TokenAndSpan {\n-                tok,\n-                sp: self.mk_sp(start_bpos, self.pos),\n+            Some(Token {\n+                kind,\n+                span: self.mk_sp(start_bpos, self.pos),\n             })\n         })\n     }\n@@ -1611,26 +1596,26 @@ mod tests {\n                                         \"/* my source file */ fn main() { println!(\\\"zebra\\\"); }\\n\"\n                                             .to_string());\n             let id = Ident::from_str(\"fn\");\n-            assert_eq!(string_reader.next_token().tok, token::Comment);\n-            assert_eq!(string_reader.next_token().tok, token::Whitespace);\n+            assert_eq!(string_reader.next_token().kind, token::Comment);\n+            assert_eq!(string_reader.next_token().kind, token::Whitespace);\n             let tok1 = string_reader.next_token();\n-            let tok2 = TokenAndSpan {\n-                tok: token::Ident(id, false),\n-                sp: Span::new(BytePos(21), BytePos(23), NO_EXPANSION),\n+            let tok2 = Token {\n+                kind: token::Ident(id, false),\n+                span: Span::new(BytePos(21), BytePos(23), NO_EXPANSION),\n             };\n-            assert_eq!(tok1.tok, tok2.tok);\n-            assert_eq!(tok1.sp, tok2.sp);\n-            assert_eq!(string_reader.next_token().tok, token::Whitespace);\n+            assert_eq!(tok1.kind, tok2.kind);\n+            assert_eq!(tok1.span, tok2.span);\n+            assert_eq!(string_reader.next_token().kind, token::Whitespace);\n             // the 'main' id is already read:\n             assert_eq!(string_reader.pos.clone(), BytePos(28));\n             // read another token:\n             let tok3 = string_reader.next_token();\n-            let tok4 = TokenAndSpan {\n-                tok: mk_ident(\"main\"),\n-                sp: Span::new(BytePos(24), BytePos(28), NO_EXPANSION),\n+            let tok4 = Token {\n+                kind: mk_ident(\"main\"),\n+                span: Span::new(BytePos(24), BytePos(28), NO_EXPANSION),\n             };\n-            assert_eq!(tok3.tok, tok4.tok);\n-            assert_eq!(tok3.sp, tok4.sp);\n+            assert_eq!(tok3.kind, tok4.kind);\n+            assert_eq!(tok3.span, tok4.span);\n             // the lparen is already read:\n             assert_eq!(string_reader.pos.clone(), BytePos(29))\n         })\n@@ -1640,7 +1625,7 @@ mod tests {\n     // of tokens (stop checking after exhausting the expected vec)\n     fn check_tokenization(mut string_reader: StringReader<'_>, expected: Vec<TokenKind>) {\n         for expected_tok in &expected {\n-            assert_eq!(&string_reader.next_token().tok, expected_tok);\n+            assert_eq!(&string_reader.next_token().kind, expected_tok);\n         }\n     }\n \n@@ -1698,7 +1683,7 @@ mod tests {\n         with_default_globals(|| {\n             let sm = Lrc::new(SourceMap::new(FilePathMapping::empty()));\n             let sh = mk_sess(sm.clone());\n-            assert_eq!(setup(&sm, &sh, \"'a'\".to_string()).next_token().tok,\n+            assert_eq!(setup(&sm, &sh, \"'a'\".to_string()).next_token().kind,\n                        mk_lit(token::Char, \"a\", None));\n         })\n     }\n@@ -1708,7 +1693,7 @@ mod tests {\n         with_default_globals(|| {\n             let sm = Lrc::new(SourceMap::new(FilePathMapping::empty()));\n             let sh = mk_sess(sm.clone());\n-            assert_eq!(setup(&sm, &sh, \"' '\".to_string()).next_token().tok,\n+            assert_eq!(setup(&sm, &sh, \"' '\".to_string()).next_token().kind,\n                        mk_lit(token::Char, \" \", None));\n         })\n     }\n@@ -1718,7 +1703,7 @@ mod tests {\n         with_default_globals(|| {\n             let sm = Lrc::new(SourceMap::new(FilePathMapping::empty()));\n             let sh = mk_sess(sm.clone());\n-            assert_eq!(setup(&sm, &sh, \"'\\\\n'\".to_string()).next_token().tok,\n+            assert_eq!(setup(&sm, &sh, \"'\\\\n'\".to_string()).next_token().kind,\n                        mk_lit(token::Char, \"\\\\n\", None));\n         })\n     }\n@@ -1728,7 +1713,7 @@ mod tests {\n         with_default_globals(|| {\n             let sm = Lrc::new(SourceMap::new(FilePathMapping::empty()));\n             let sh = mk_sess(sm.clone());\n-            assert_eq!(setup(&sm, &sh, \"'abc\".to_string()).next_token().tok,\n+            assert_eq!(setup(&sm, &sh, \"'abc\".to_string()).next_token().kind,\n                        token::Lifetime(Ident::from_str(\"'abc\")));\n         })\n     }\n@@ -1738,7 +1723,7 @@ mod tests {\n         with_default_globals(|| {\n             let sm = Lrc::new(SourceMap::new(FilePathMapping::empty()));\n             let sh = mk_sess(sm.clone());\n-            assert_eq!(setup(&sm, &sh, \"r###\\\"\\\"#a\\\\b\\x00c\\\"\\\"###\".to_string()).next_token().tok,\n+            assert_eq!(setup(&sm, &sh, \"r###\\\"\\\"#a\\\\b\\x00c\\\"\\\"###\".to_string()).next_token().kind,\n                        mk_lit(token::StrRaw(3), \"\\\"#a\\\\b\\x00c\\\"\", None));\n         })\n     }\n@@ -1750,10 +1735,10 @@ mod tests {\n             let sh = mk_sess(sm.clone());\n             macro_rules! test {\n                 ($input: expr, $tok_type: ident, $tok_contents: expr) => {{\n-                    assert_eq!(setup(&sm, &sh, format!(\"{}suffix\", $input)).next_token().tok,\n+                    assert_eq!(setup(&sm, &sh, format!(\"{}suffix\", $input)).next_token().kind,\n                                mk_lit(token::$tok_type, $tok_contents, Some(\"suffix\")));\n                     // with a whitespace separator:\n-                    assert_eq!(setup(&sm, &sh, format!(\"{} suffix\", $input)).next_token().tok,\n+                    assert_eq!(setup(&sm, &sh, format!(\"{} suffix\", $input)).next_token().kind,\n                                mk_lit(token::$tok_type, $tok_contents, None));\n                 }}\n             }\n@@ -1768,11 +1753,11 @@ mod tests {\n             test!(\"1.0\", Float, \"1.0\");\n             test!(\"1.0e10\", Float, \"1.0e10\");\n \n-            assert_eq!(setup(&sm, &sh, \"2us\".to_string()).next_token().tok,\n+            assert_eq!(setup(&sm, &sh, \"2us\".to_string()).next_token().kind,\n                        mk_lit(token::Integer, \"2\", Some(\"us\")));\n-            assert_eq!(setup(&sm, &sh, \"r###\\\"raw\\\"###suffix\".to_string()).next_token().tok,\n+            assert_eq!(setup(&sm, &sh, \"r###\\\"raw\\\"###suffix\".to_string()).next_token().kind,\n                        mk_lit(token::StrRaw(3), \"raw\", Some(\"suffix\")));\n-            assert_eq!(setup(&sm, &sh, \"br###\\\"raw\\\"###suffix\".to_string()).next_token().tok,\n+            assert_eq!(setup(&sm, &sh, \"br###\\\"raw\\\"###suffix\".to_string()).next_token().kind,\n                        mk_lit(token::ByteStrRaw(3), \"raw\", Some(\"suffix\")));\n         })\n     }\n@@ -1790,11 +1775,11 @@ mod tests {\n             let sm = Lrc::new(SourceMap::new(FilePathMapping::empty()));\n             let sh = mk_sess(sm.clone());\n             let mut lexer = setup(&sm, &sh, \"/* /* */ */'a'\".to_string());\n-            match lexer.next_token().tok {\n+            match lexer.next_token().kind {\n                 token::Comment => {}\n                 _ => panic!(\"expected a comment!\"),\n             }\n-            assert_eq!(lexer.next_token().tok, mk_lit(token::Char, \"a\", None));\n+            assert_eq!(lexer.next_token().kind, mk_lit(token::Char, \"a\", None));\n         })\n     }\n \n@@ -1805,10 +1790,10 @@ mod tests {\n             let sh = mk_sess(sm.clone());\n             let mut lexer = setup(&sm, &sh, \"// test\\r\\n/// test\\r\\n\".to_string());\n             let comment = lexer.next_token();\n-            assert_eq!(comment.tok, token::Comment);\n-            assert_eq!((comment.sp.lo(), comment.sp.hi()), (BytePos(0), BytePos(7)));\n-            assert_eq!(lexer.next_token().tok, token::Whitespace);\n-            assert_eq!(lexer.next_token().tok,\n+            assert_eq!(comment.kind, token::Comment);\n+            assert_eq!((comment.span.lo(), comment.span.hi()), (BytePos(0), BytePos(7)));\n+            assert_eq!(lexer.next_token().kind, token::Whitespace);\n+            assert_eq!(lexer.next_token().kind,\n                     token::DocComment(Symbol::intern(\"/// test\")));\n         })\n     }"}, {"sha": "767d37016da87cecece3ad65d3fe2d0911463e71", "filename": "src/libsyntax/parse/lexer/tokentrees.rs", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/rust-lang/rust/blob/a3425edb46dfcc7031068b8bdda868e5a3b16ae1/src%2Flibsyntax%2Fparse%2Flexer%2Ftokentrees.rs", "raw_url": "https://github.com/rust-lang/rust/raw/a3425edb46dfcc7031068b8bdda868e5a3b16ae1/src%2Flibsyntax%2Fparse%2Flexer%2Ftokentrees.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Flexer%2Ftokentrees.rs?ref=a3425edb46dfcc7031068b8bdda868e5a3b16ae1", "patch": "@@ -220,7 +220,7 @@ impl<'a> TokenTreesReader<'a> {\n \n     fn real_token(&mut self) {\n         let t = self.string_reader.real_token();\n-        self.token = t.tok;\n-        self.span = t.sp;\n+        self.token = t.kind;\n+        self.span = t.span;\n     }\n }"}, {"sha": "3b7d4e14dbb40fb18282ef53a8cb54952d342dcf", "filename": "src/libsyntax/parse/parser.rs", "status": "modified", "additions": 15, "deletions": 15, "changes": 30, "blob_url": "https://github.com/rust-lang/rust/blob/a3425edb46dfcc7031068b8bdda868e5a3b16ae1/src%2Flibsyntax%2Fparse%2Fparser.rs", "raw_url": "https://github.com/rust-lang/rust/raw/a3425edb46dfcc7031068b8bdda868e5a3b16ae1/src%2Flibsyntax%2Fparse%2Fparser.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Fparser.rs?ref=a3425edb46dfcc7031068b8bdda868e5a3b16ae1", "patch": "@@ -36,9 +36,9 @@ use crate::{ast, attr};\n use crate::ext::base::DummyResult;\n use crate::source_map::{self, SourceMap, Spanned, respan};\n use crate::parse::{SeqSep, classify, literal, token};\n-use crate::parse::lexer::{TokenAndSpan, UnmatchedBrace};\n+use crate::parse::lexer::UnmatchedBrace;\n use crate::parse::lexer::comments::{doc_comment_style, strip_doc_comment_decoration};\n-use crate::parse::token::DelimToken;\n+use crate::parse::token::{Token, DelimToken};\n use crate::parse::{new_sub_parser_from_file, ParseSess, Directory, DirectoryOwnership};\n use crate::util::parser::{AssocOp, Fixity};\n use crate::print::pprust;\n@@ -295,7 +295,7 @@ impl TokenCursorFrame {\n }\n \n impl TokenCursor {\n-    fn next(&mut self) -> TokenAndSpan {\n+    fn next(&mut self) -> Token {\n         loop {\n             let tree = if !self.frame.open_delim {\n                 self.frame.open_delim = true;\n@@ -309,7 +309,7 @@ impl TokenCursor {\n                 self.frame = frame;\n                 continue\n             } else {\n-                return TokenAndSpan { tok: token::Eof, sp: DUMMY_SP }\n+                return Token { kind: token::Eof, span: DUMMY_SP }\n             };\n \n             match self.frame.last_token {\n@@ -318,7 +318,7 @@ impl TokenCursor {\n             }\n \n             match tree {\n-                TokenTree::Token(sp, tok) => return TokenAndSpan { tok: tok, sp: sp },\n+                TokenTree::Token(span, kind) => return Token { kind, span },\n                 TokenTree::Delimited(sp, delim, tts) => {\n                     let frame = TokenCursorFrame::new(sp, delim, &tts);\n                     self.stack.push(mem::replace(&mut self.frame, frame));\n@@ -327,9 +327,9 @@ impl TokenCursor {\n         }\n     }\n \n-    fn next_desugared(&mut self) -> TokenAndSpan {\n+    fn next_desugared(&mut self) -> Token {\n         let (sp, name) = match self.next() {\n-            TokenAndSpan { sp, tok: token::DocComment(name) } => (sp, name),\n+            Token { span, kind: token::DocComment(name) } => (span, name),\n             tok => return tok,\n         };\n \n@@ -499,8 +499,8 @@ impl<'a> Parser<'a> {\n         };\n \n         let tok = parser.next_tok();\n-        parser.token = tok.tok;\n-        parser.span = tok.sp;\n+        parser.token = tok.kind;\n+        parser.span = tok.span;\n \n         if let Some(directory) = directory {\n             parser.directory = directory;\n@@ -515,15 +515,15 @@ impl<'a> Parser<'a> {\n         parser\n     }\n \n-    fn next_tok(&mut self) -> TokenAndSpan {\n+    fn next_tok(&mut self) -> Token {\n         let mut next = if self.desugar_doc_comments {\n             self.token_cursor.next_desugared()\n         } else {\n             self.token_cursor.next()\n         };\n-        if next.sp.is_dummy() {\n+        if next.span.is_dummy() {\n             // Tweak the location for better diagnostics, but keep syntactic context intact.\n-            next.sp = self.prev_span.with_ctxt(next.sp.ctxt());\n+            next.span = self.prev_span.with_ctxt(next.span.ctxt());\n         }\n         next\n     }\n@@ -1023,8 +1023,8 @@ impl<'a> Parser<'a> {\n         };\n \n         let next = self.next_tok();\n-        self.span = next.sp;\n-        self.token = next.tok;\n+        self.token = next.kind;\n+        self.span = next.span;\n         self.expected_tokens.clear();\n         // check after each token\n         self.process_potential_macro_variable();\n@@ -1038,8 +1038,8 @@ impl<'a> Parser<'a> {\n         // fortunately for tokens currently using `bump_with`, the\n         // prev_token_kind will be of no use anyway.\n         self.prev_token_kind = PrevTokenKind::Other;\n-        self.span = span;\n         self.token = next;\n+        self.span = span;\n         self.expected_tokens.clear();\n     }\n "}, {"sha": "3679e4050ff4210ab18bf29f817ce3658f0843a7", "filename": "src/libsyntax/parse/token.rs", "status": "modified", "additions": 12, "deletions": 0, "changes": 12, "blob_url": "https://github.com/rust-lang/rust/blob/a3425edb46dfcc7031068b8bdda868e5a3b16ae1/src%2Flibsyntax%2Fparse%2Ftoken.rs", "raw_url": "https://github.com/rust-lang/rust/raw/a3425edb46dfcc7031068b8bdda868e5a3b16ae1/src%2Flibsyntax%2Fparse%2Ftoken.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Ftoken.rs?ref=a3425edb46dfcc7031068b8bdda868e5a3b16ae1", "patch": "@@ -235,6 +235,12 @@ pub enum TokenKind {\n #[cfg(target_arch = \"x86_64\")]\n static_assert_size!(TokenKind, 16);\n \n+#[derive(Clone, Debug)]\n+pub struct Token {\n+    pub kind: TokenKind,\n+    pub span: Span,\n+}\n+\n impl TokenKind {\n     /// Recovers a `TokenKind` from an `ast::Ident`. This creates a raw identifier if necessary.\n     pub fn from_ast_ident(ident: ast::Ident) -> TokenKind {\n@@ -602,6 +608,12 @@ impl TokenKind {\n     }\n }\n \n+impl PartialEq<TokenKind> for Token {\n+    fn eq(&self, rhs: &TokenKind) -> bool {\n+        self.kind == *rhs\n+    }\n+}\n+\n #[derive(Clone, RustcEncodable, RustcDecodable)]\n /// For interpolation during macro expansion.\n pub enum Nonterminal {"}, {"sha": "654c21fd094e907224ac7b7c5f384a03148c95b4", "filename": "src/libsyntax/tokenstream.rs", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "blob_url": "https://github.com/rust-lang/rust/blob/a3425edb46dfcc7031068b8bdda868e5a3b16ae1/src%2Flibsyntax%2Ftokenstream.rs", "raw_url": "https://github.com/rust-lang/rust/raw/a3425edb46dfcc7031068b8bdda868e5a3b16ae1/src%2Flibsyntax%2Ftokenstream.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Ftokenstream.rs?ref=a3425edb46dfcc7031068b8bdda868e5a3b16ae1", "patch": "@@ -580,7 +580,6 @@ mod tests {\n     use super::*;\n     use crate::syntax::ast::Ident;\n     use crate::with_default_globals;\n-    use crate::parse::token::TokenKind;\n     use crate::util::parser_testing::string_to_stream;\n     use syntax_pos::{Span, BytePos, NO_EXPANSION};\n "}]}