{"sha": "ad679a7f433b60e9d5a7ce5029d50600aa919fd6", "node_id": "MDY6Q29tbWl0NzI0NzEyOmFkNjc5YTdmNDMzYjYwZTlkNWE3Y2U1MDI5ZDUwNjAwYWE5MTlmZDY=", "commit": {"author": {"name": "Mark Rousskov", "email": "mark.simulacrum@gmail.com", "date": "2020-03-27T22:13:22Z"}, "committer": {"name": "Mark Rousskov", "email": "mark.simulacrum@gmail.com", "date": "2020-03-27T23:02:23Z"}, "message": "Update the documentation comment", "tree": {"sha": "5cfeec67f6e9bffbad30a36e9c1029ec34c9da76", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/5cfeec67f6e9bffbad30a36e9c1029ec34c9da76"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/ad679a7f433b60e9d5a7ce5029d50600aa919fd6", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/ad679a7f433b60e9d5a7ce5029d50600aa919fd6", "html_url": "https://github.com/rust-lang/rust/commit/ad679a7f433b60e9d5a7ce5029d50600aa919fd6", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/ad679a7f433b60e9d5a7ce5029d50600aa919fd6/comments", "author": {"login": "Mark-Simulacrum", "id": 5047365, "node_id": "MDQ6VXNlcjUwNDczNjU=", "avatar_url": "https://avatars.githubusercontent.com/u/5047365?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Mark-Simulacrum", "html_url": "https://github.com/Mark-Simulacrum", "followers_url": "https://api.github.com/users/Mark-Simulacrum/followers", "following_url": "https://api.github.com/users/Mark-Simulacrum/following{/other_user}", "gists_url": "https://api.github.com/users/Mark-Simulacrum/gists{/gist_id}", "starred_url": "https://api.github.com/users/Mark-Simulacrum/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Mark-Simulacrum/subscriptions", "organizations_url": "https://api.github.com/users/Mark-Simulacrum/orgs", "repos_url": "https://api.github.com/users/Mark-Simulacrum/repos", "events_url": "https://api.github.com/users/Mark-Simulacrum/events{/privacy}", "received_events_url": "https://api.github.com/users/Mark-Simulacrum/received_events", "type": "User", "site_admin": false}, "committer": {"login": "Mark-Simulacrum", "id": 5047365, "node_id": "MDQ6VXNlcjUwNDczNjU=", "avatar_url": "https://avatars.githubusercontent.com/u/5047365?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Mark-Simulacrum", "html_url": "https://github.com/Mark-Simulacrum", "followers_url": "https://api.github.com/users/Mark-Simulacrum/followers", "following_url": "https://api.github.com/users/Mark-Simulacrum/following{/other_user}", "gists_url": "https://api.github.com/users/Mark-Simulacrum/gists{/gist_id}", "starred_url": "https://api.github.com/users/Mark-Simulacrum/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Mark-Simulacrum/subscriptions", "organizations_url": "https://api.github.com/users/Mark-Simulacrum/orgs", "repos_url": "https://api.github.com/users/Mark-Simulacrum/repos", "events_url": "https://api.github.com/users/Mark-Simulacrum/events{/privacy}", "received_events_url": "https://api.github.com/users/Mark-Simulacrum/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "b6bc9060041bb5de18d9b31fe935d29193d9bad5", "url": "https://api.github.com/repos/rust-lang/rust/commits/b6bc9060041bb5de18d9b31fe935d29193d9bad5", "html_url": "https://github.com/rust-lang/rust/commit/b6bc9060041bb5de18d9b31fe935d29193d9bad5"}], "stats": {"total": 112, "additions": 73, "deletions": 39}, "files": [{"sha": "d5562ff91df4da66cbad05d167f652f835b9a60c", "filename": "src/tools/unicode-table-generator/src/main.rs", "status": "modified", "additions": 73, "deletions": 0, "changes": 73, "blob_url": "https://github.com/rust-lang/rust/blob/ad679a7f433b60e9d5a7ce5029d50600aa919fd6/src%2Ftools%2Funicode-table-generator%2Fsrc%2Fmain.rs", "raw_url": "https://github.com/rust-lang/rust/raw/ad679a7f433b60e9d5a7ce5029d50600aa919fd6/src%2Ftools%2Funicode-table-generator%2Fsrc%2Fmain.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ftools%2Funicode-table-generator%2Fsrc%2Fmain.rs?ref=ad679a7f433b60e9d5a7ce5029d50600aa919fd6", "patch": "@@ -1,3 +1,76 @@\n+//! This implements the core logic of the compression scheme used to compactly\n+//! encode Unicode properties.\n+//!\n+//! We have two primary goals with the encoding: we want to be compact, because\n+//! these tables often end up in ~every Rust program (especially the\n+//! grapheme_extend table, used for str debugging), including those for embedded\n+//! targets (where space is important). We also want to be relatively fast,\n+//! though this is more of a nice to have rather than a key design constraint.\n+//! It is expected that libraries/applications which are performance-sensitive\n+//! to Unicode property lookups are extremely rare, and those that care may find\n+//! the tradeoff of the raw bitsets worth it. For most applications, a\n+//! relatively fast but much smaller (and as such less cache-impacting, etc.)\n+//! data set is likely preferable.\n+//!\n+//! We have two separate encoding schemes: a skiplist-like approach, and a\n+//! compressed bitset. The datasets we consider mostly use the skiplist (it's\n+//! smaller) but the lowercase and uppercase sets are sufficiently sparse for\n+//! the bitset to be worthwhile -- for those sets the biset is a 2x size win.\n+//! Since the bitset is also faster, this seems an obvious choice. (As a\n+//! historical note, the bitset was also the prior implementation, so its\n+//! relative complexity had already been paid).\n+//!\n+//! ## The bitset\n+//!\n+//! The primary idea is that we 'flatten' the Unicode ranges into an enormous\n+//! bitset. To represent any arbitrary codepoint in a raw bitset, we would need\n+//! over 17 kilobytes of data per character set -- way too much for our\n+//! purposes.\n+//!\n+//! First, the raw bitset (one bit for every valid `char`, from 0 to 0x10FFFF,\n+//! not skipping the small 'gap') is associated into words (u64) and\n+//! deduplicated. On random data, this would be useless; on our data, this is\n+//! incredibly beneficial -- our data sets have (far) less than 256 unique\n+//! words.\n+//!\n+//! This gives us an array that maps `u8 -> word`; the current algorithm does\n+//! not handle the case of more than 256 unique words, but we are relatively far\n+//! from coming that close.\n+//!\n+//! With that scheme, we now have a single byte for every 64 codepoints.\n+//!\n+//! We further chunk these by some constant N (between 1 and 64 per group,\n+//! dynamically chosen for smallest size), and again deduplicate and store in an\n+//! array (u8 -> [u8; N]).\n+//!\n+//! The bytes of this array map into the words from the bitset above, but we\n+//! apply another trick here: some of these words are similar enough that they\n+//! can be represented by some function of another word. The particular\n+//! functions chosen are rotation, inversion, and shifting (right).\n+//!\n+//! ## The skiplist\n+//!\n+//! The skip list arose out of the desire for an even smaller encoding than the\n+//! bitset -- and was the answer to the question \"what is the smallest\n+//! representation we can imagine?\". However, it is not necessarily the\n+//! smallest, and if you have a better proposal, please do suggest it!\n+//!\n+//! This is a relatively straightforward encoding. First, we break up all the\n+//! ranges in the input data into offsets from each other, essentially a gap\n+//! encoding. In practice, most gaps are small -- less than u8::MAX -- so we\n+//! store those directly. We make use of the larger gaps (which are nicely\n+//! interspersed already) throughout the dataset to index this data set.\n+//!\n+//! In particular, each run of small gaps (terminating in a large gap) is\n+//! indexed in a separate dataset. That data set stores an index into the\n+//! primary offset list and a prefix sum of that offset list. These are packed\n+//! into a single u32 (11 bits for the offset, 21 bits for the prefix sum).\n+//!\n+//! Lookup proceeds via a binary search in the index and then a straightforward\n+//! linear scan (adding up the offsets) until we reach the needle, and then the\n+//! index of that offset is utilized as the answer to whether we're in the set\n+//! or not.\n+\n use std::collections::{BTreeMap, HashMap};\n use std::ops::Range;\n use ucd_parse::Codepoints;"}, {"sha": "95b63aca1549b45c2cb89b8a54c21aa14189a40a", "filename": "src/tools/unicode-table-generator/src/raw_emitter.rs", "status": "modified", "additions": 0, "deletions": 39, "changes": 39, "blob_url": "https://github.com/rust-lang/rust/blob/ad679a7f433b60e9d5a7ce5029d50600aa919fd6/src%2Ftools%2Funicode-table-generator%2Fsrc%2Fraw_emitter.rs", "raw_url": "https://github.com/rust-lang/rust/raw/ad679a7f433b60e9d5a7ce5029d50600aa919fd6/src%2Ftools%2Funicode-table-generator%2Fsrc%2Fraw_emitter.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ftools%2Funicode-table-generator%2Fsrc%2Fraw_emitter.rs?ref=ad679a7f433b60e9d5a7ce5029d50600aa919fd6", "patch": "@@ -1,42 +1,3 @@\n-//! This implements the core logic of the compression scheme used to compactly\n-//! encode the Unicode character classes.\n-//!\n-//! The primary idea is that we 'flatten' the Unicode ranges into an enormous\n-//! bitset. To represent any arbitrary codepoint in a raw bitset, we would need\n-//! over 17 kilobytes of data per character set -- way too much for our\n-//! purposes.\n-//!\n-//! We have two primary goals with the encoding: we want to be compact, because\n-//! these tables often end up in ~every Rust program (especially the\n-//! grapheme_extend table, used for str debugging), including those for embedded\n-//! targets (where space is important). We also want to be relatively fast,\n-//! though this is more of a nice to have rather than a key design constraint.\n-//! In practice, due to modern processor design these two are closely related.\n-//!\n-//! The encoding scheme here compresses the bitset by first deduplicating the\n-//! \"words\" (64 bits on all platforms). In practice very few words are present\n-//! in most data sets.\n-//!\n-//! This gives us an array that maps `u8 -> word` (if we ever went beyond 256\n-//! words, we could go to u16 -> word or have some dual compression scheme\n-//! mapping into two separate sets; currently this is not dealt with).\n-//!\n-//! With that scheme, we now have a single byte for every 64 codepoints. We\n-//! further group these by some constant N (between 1 and 64 per group), and\n-//! again deduplicate and store in an array (u8 -> [u8; N]). The constant is\n-//! chosen to be optimal in bytes-in-memory for the given dataset.\n-//!\n-//! The indices into this array represent ranges of 64*16 = 1024 codepoints.\n-//!\n-//! This already reduces the top-level array to at most 1,086 bytes, but in\n-//! practice we usually can encode in far fewer (the first couple Unicode planes\n-//! are dense).\n-//!\n-//! The last byte of this top-level array is pulled out to a separate static\n-//! and trailing zeros are dropped; this is simply because grapheme_extend and\n-//! case_ignorable have a single entry in the 896th entry, so this shrinks them\n-//! down considerably.\n-\n use crate::fmt_list;\n use std::collections::{BTreeMap, BTreeSet, HashMap};\n use std::convert::TryFrom;"}]}