{"sha": "159da285e9d8594a2cc4436b5cee7874b07806c9", "node_id": "MDY6Q29tbWl0NzI0NzEyOjE1OWRhMjg1ZTlkODU5NGEyY2M0NDM2YjVjZWU3ODc0YjA3ODA2Yzk=", "commit": {"author": {"name": "Edwin Cheng", "email": "edwin0cheng@gmail.com", "date": "2019-11-03T14:46:12Z"}, "committer": {"name": "Edwin Cheng", "email": "edwin0cheng@gmail.com", "date": "2019-11-04T17:38:20Z"}, "message": "Add macro_expansion_info in hir_expand", "tree": {"sha": "15630e1158cfb7d7305cedd93c1a644b28f9b776", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/15630e1158cfb7d7305cedd93c1a644b28f9b776"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/159da285e9d8594a2cc4436b5cee7874b07806c9", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/159da285e9d8594a2cc4436b5cee7874b07806c9", "html_url": "https://github.com/rust-lang/rust/commit/159da285e9d8594a2cc4436b5cee7874b07806c9", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/159da285e9d8594a2cc4436b5cee7874b07806c9/comments", "author": {"login": "edwin0cheng", "id": 11014119, "node_id": "MDQ6VXNlcjExMDE0MTE5", "avatar_url": "https://avatars.githubusercontent.com/u/11014119?v=4", "gravatar_id": "", "url": "https://api.github.com/users/edwin0cheng", "html_url": "https://github.com/edwin0cheng", "followers_url": "https://api.github.com/users/edwin0cheng/followers", "following_url": "https://api.github.com/users/edwin0cheng/following{/other_user}", "gists_url": "https://api.github.com/users/edwin0cheng/gists{/gist_id}", "starred_url": "https://api.github.com/users/edwin0cheng/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/edwin0cheng/subscriptions", "organizations_url": "https://api.github.com/users/edwin0cheng/orgs", "repos_url": "https://api.github.com/users/edwin0cheng/repos", "events_url": "https://api.github.com/users/edwin0cheng/events{/privacy}", "received_events_url": "https://api.github.com/users/edwin0cheng/received_events", "type": "User", "site_admin": false}, "committer": {"login": "edwin0cheng", "id": 11014119, "node_id": "MDQ6VXNlcjExMDE0MTE5", "avatar_url": "https://avatars.githubusercontent.com/u/11014119?v=4", "gravatar_id": "", "url": "https://api.github.com/users/edwin0cheng", "html_url": "https://github.com/edwin0cheng", "followers_url": "https://api.github.com/users/edwin0cheng/followers", "following_url": "https://api.github.com/users/edwin0cheng/following{/other_user}", "gists_url": "https://api.github.com/users/edwin0cheng/gists{/gist_id}", "starred_url": "https://api.github.com/users/edwin0cheng/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/edwin0cheng/subscriptions", "organizations_url": "https://api.github.com/users/edwin0cheng/orgs", "repos_url": "https://api.github.com/users/edwin0cheng/repos", "events_url": "https://api.github.com/users/edwin0cheng/events{/privacy}", "received_events_url": "https://api.github.com/users/edwin0cheng/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "9fd546bec23ac817a45da28889e76118969db91e", "url": "https://api.github.com/repos/rust-lang/rust/commits/9fd546bec23ac817a45da28889e76118969db91e", "html_url": "https://github.com/rust-lang/rust/commit/9fd546bec23ac817a45da28889e76118969db91e"}], "stats": {"total": 259, "additions": 212, "deletions": 47}, "files": [{"sha": "8abfbb4ffaaafb7061b3872369514e2911d69cfd", "filename": "crates/ra_hir_expand/src/db.rs", "status": "modified", "additions": 75, "deletions": 18, "changes": 93, "blob_url": "https://github.com/rust-lang/rust/blob/159da285e9d8594a2cc4436b5cee7874b07806c9/crates%2Fra_hir_expand%2Fsrc%2Fdb.rs", "raw_url": "https://github.com/rust-lang/rust/raw/159da285e9d8594a2cc4436b5cee7874b07806c9/crates%2Fra_hir_expand%2Fsrc%2Fdb.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fra_hir_expand%2Fsrc%2Fdb.rs?ref=159da285e9d8594a2cc4436b5cee7874b07806c9", "patch": "@@ -8,10 +8,16 @@ use ra_prof::profile;\n use ra_syntax::{AstNode, Parse, SyntaxNode};\n \n use crate::{\n-    ast_id_map::AstIdMap, HirFileId, HirFileIdRepr, MacroCallId, MacroCallLoc, MacroDefId,\n-    MacroFile, MacroFileKind,\n+    ast_id_map::AstIdMap, ExpansionInfo, HirFileId, HirFileIdRepr, MacroCallId, MacroCallLoc,\n+    MacroDefId, MacroFile, MacroFileKind,\n };\n \n+#[derive(Debug, PartialEq, Eq, Clone)]\n+pub struct ParseMacroWithInfo {\n+    pub parsed: Parse<SyntaxNode>,\n+    pub expansion_info: Arc<ExpansionInfo>,\n+}\n+\n // FIXME: rename to ExpandDatabase\n #[salsa::query_group(AstDatabaseStorage)]\n pub trait AstDatabase: SourceDatabase {\n@@ -22,10 +28,16 @@ pub trait AstDatabase: SourceDatabase {\n \n     #[salsa::interned]\n     fn intern_macro(&self, macro_call: MacroCallLoc) -> MacroCallId;\n-    fn macro_arg(&self, id: MacroCallId) -> Option<Arc<tt::Subtree>>;\n-    fn macro_def(&self, id: MacroDefId) -> Option<Arc<mbe::MacroRules>>;\n+    fn macro_arg(&self, id: MacroCallId) -> Option<(Arc<tt::Subtree>, Arc<mbe::TokenMap>)>;\n+    fn macro_def(&self, id: MacroDefId) -> Option<(Arc<mbe::MacroRules>, Arc<mbe::TokenMap>)>;\n     fn parse_macro(&self, macro_file: MacroFile) -> Option<Parse<SyntaxNode>>;\n-    fn macro_expand(&self, macro_call: MacroCallId) -> Result<Arc<tt::Subtree>, String>;\n+    fn parse_macro_with_info(&self, macro_file: MacroFile) -> Option<ParseMacroWithInfo>;\n+    fn macro_expand(\n+        &self,\n+        macro_call: MacroCallId,\n+    ) -> Result<(Arc<tt::Subtree>, (Arc<mbe::TokenMap>, Arc<mbe::TokenMap>)), String>;\n+\n+    fn macro_expansion_info(&self, macro_file: MacroFile) -> Option<Arc<ExpansionInfo>>;\n }\n \n pub(crate) fn ast_id_map(db: &dyn AstDatabase, file_id: HirFileId) -> Arc<AstIdMap> {\n@@ -34,43 +46,50 @@ pub(crate) fn ast_id_map(db: &dyn AstDatabase, file_id: HirFileId) -> Arc<AstIdM\n     Arc::new(map)\n }\n \n-pub(crate) fn macro_def(db: &dyn AstDatabase, id: MacroDefId) -> Option<Arc<MacroRules>> {\n+pub(crate) fn macro_def(\n+    db: &dyn AstDatabase,\n+    id: MacroDefId,\n+) -> Option<(Arc<mbe::MacroRules>, Arc<mbe::TokenMap>)> {\n     let macro_call = id.ast_id.to_node(db);\n     let arg = macro_call.token_tree()?;\n-    let (tt, _) = mbe::ast_to_token_tree(&arg).or_else(|| {\n+    let (tt, tmap) = mbe::ast_to_token_tree(&arg).or_else(|| {\n         log::warn!(\"fail on macro_def to token tree: {:#?}\", arg);\n         None\n     })?;\n     let rules = MacroRules::parse(&tt).ok().or_else(|| {\n         log::warn!(\"fail on macro_def parse: {:#?}\", tt);\n         None\n     })?;\n-    Some(Arc::new(rules))\n+    Some((Arc::new(rules), Arc::new(tmap)))\n }\n \n-pub(crate) fn macro_arg(db: &dyn AstDatabase, id: MacroCallId) -> Option<Arc<tt::Subtree>> {\n+pub(crate) fn macro_arg(\n+    db: &dyn AstDatabase,\n+    id: MacroCallId,\n+) -> Option<(Arc<tt::Subtree>, Arc<mbe::TokenMap>)> {\n     let loc = db.lookup_intern_macro(id);\n     let macro_call = loc.ast_id.to_node(db);\n     let arg = macro_call.token_tree()?;\n-    let (tt, _) = mbe::ast_to_token_tree(&arg)?;\n-    Some(Arc::new(tt))\n+    let (tt, tmap) = mbe::ast_to_token_tree(&arg)?;\n+    Some((Arc::new(tt), Arc::new(tmap)))\n }\n \n pub(crate) fn macro_expand(\n     db: &dyn AstDatabase,\n     id: MacroCallId,\n-) -> Result<Arc<tt::Subtree>, String> {\n+) -> Result<(Arc<tt::Subtree>, (Arc<mbe::TokenMap>, Arc<mbe::TokenMap>)), String> {\n     let loc = db.lookup_intern_macro(id);\n     let macro_arg = db.macro_arg(id).ok_or(\"Fail to args in to tt::TokenTree\")?;\n \n     let macro_rules = db.macro_def(loc.def).ok_or(\"Fail to find macro definition\")?;\n-    let tt = macro_rules.expand(&macro_arg).map_err(|err| format!(\"{:?}\", err))?;\n+    let tt = macro_rules.0.expand(&macro_arg.0).map_err(|err| format!(\"{:?}\", err))?;\n     // Set a hard limit for the expanded tt\n     let count = tt.count();\n     if count > 65536 {\n         return Err(format!(\"Total tokens count exceed limit : count = {}\", count));\n     }\n-    Ok(Arc::new(tt))\n+\n+    Ok((Arc::new(tt), (macro_arg.1.clone(), macro_rules.1.clone())))\n }\n \n pub(crate) fn parse_or_expand(db: &dyn AstDatabase, file_id: HirFileId) -> Option<SyntaxNode> {\n@@ -87,6 +106,13 @@ pub(crate) fn parse_macro(\n     macro_file: MacroFile,\n ) -> Option<Parse<SyntaxNode>> {\n     let _p = profile(\"parse_macro_query\");\n+    db.parse_macro_with_info(macro_file).map(|r| r.parsed)\n+}\n+\n+pub(crate) fn parse_macro_with_info(\n+    db: &dyn AstDatabase,\n+    macro_file: MacroFile,\n+) -> Option<ParseMacroWithInfo> {\n     let macro_call_id = macro_file.macro_call_id;\n     let tt = db\n         .macro_expand(macro_call_id)\n@@ -97,8 +123,39 @@ pub(crate) fn parse_macro(\n             log::warn!(\"fail on macro_parse: (reason: {})\", err,);\n         })\n         .ok()?;\n-    match macro_file.macro_file_kind {\n-        MacroFileKind::Items => mbe::token_tree_to_items(&tt).ok().map(Parse::to_syntax),\n-        MacroFileKind::Expr => mbe::token_tree_to_expr(&tt).ok().map(Parse::to_syntax),\n-    }\n+    let res = match macro_file.macro_file_kind {\n+        MacroFileKind::Items => {\n+            mbe::token_tree_to_items(&tt.0).ok().map(|(p, map)| (Parse::to_syntax(p), map))\n+        }\n+        MacroFileKind::Expr => {\n+            mbe::token_tree_to_expr(&tt.0).ok().map(|(p, map)| (Parse::to_syntax(p), map))\n+        }\n+    };\n+\n+    res.map(|(parsed, exp_map)| {\n+        let (arg_map, def_map) = tt.1;\n+        let loc: MacroCallLoc = db.lookup_intern_macro(macro_call_id);\n+\n+        let def_start =\n+            loc.def.ast_id.to_node(db).token_tree().map(|t| t.syntax().text_range().start());\n+        let arg_start =\n+            loc.ast_id.to_node(db).token_tree().map(|t| t.syntax().text_range().start());\n+\n+        let arg_map =\n+            arg_start.map(|start| exp_map.ranges(&arg_map, start)).unwrap_or_else(|| Vec::new());\n+\n+        let def_map =\n+            def_start.map(|start| exp_map.ranges(&def_map, start)).unwrap_or_else(|| Vec::new());\n+\n+        let info = ExpansionInfo { arg_map, def_map };\n+\n+        ParseMacroWithInfo { parsed, expansion_info: Arc::new(info) }\n+    })\n+}\n+\n+pub(crate) fn macro_expansion_info(\n+    db: &dyn AstDatabase,\n+    macro_file: MacroFile,\n+) -> Option<Arc<ExpansionInfo>> {\n+    db.parse_macro_with_info(macro_file).map(|res| res.expansion_info.clone())\n }"}, {"sha": "194020b45cc706dfbfba9a6f7fd828571ba125f3", "filename": "crates/ra_hir_expand/src/lib.rs", "status": "modified", "additions": 34, "deletions": 1, "changes": 35, "blob_url": "https://github.com/rust-lang/rust/blob/159da285e9d8594a2cc4436b5cee7874b07806c9/crates%2Fra_hir_expand%2Fsrc%2Flib.rs", "raw_url": "https://github.com/rust-lang/rust/raw/159da285e9d8594a2cc4436b5cee7874b07806c9/crates%2Fra_hir_expand%2Fsrc%2Flib.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fra_hir_expand%2Fsrc%2Flib.rs?ref=159da285e9d8594a2cc4436b5cee7874b07806c9", "patch": "@@ -16,7 +16,7 @@ use std::hash::{Hash, Hasher};\n use ra_db::{salsa, CrateId, FileId};\n use ra_syntax::{\n     ast::{self, AstNode},\n-    SyntaxNode,\n+    SyntaxNode, TextRange,\n };\n \n use crate::ast_id_map::FileAstId;\n@@ -112,6 +112,39 @@ impl MacroCallId {\n     }\n }\n \n+#[derive(Debug, Clone, PartialEq, Eq)]\n+/// ExpansionInfo mainly describle how to map text range between src and expaned macro\n+pub struct ExpansionInfo {\n+    pub arg_map: Vec<(TextRange, TextRange)>,\n+    pub def_map: Vec<(TextRange, TextRange)>,\n+}\n+\n+impl ExpansionInfo {\n+    pub fn find_range(\n+        &self,\n+        from: TextRange,\n+        (arg_file_id, def_file_id): (HirFileId, HirFileId),\n+    ) -> Option<(HirFileId, TextRange)> {\n+        for (src, dest) in &self.arg_map {\n+            dbg!((src, *dest, \"arg_map\"));\n+            if src.is_subrange(&from) {\n+                dbg!((arg_file_id, *dest));\n+                return Some((arg_file_id, *dest));\n+            }\n+        }\n+\n+        for (src, dest) in &self.def_map {\n+            dbg!((src, *dest, \"def_map\"));\n+            if src.is_subrange(&from) {\n+                dbg!((arg_file_id, *dest));\n+                return Some((def_file_id, *dest));\n+            }\n+        }\n+\n+        None\n+    }\n+}\n+\n /// `AstId` points to an AST node in any file.\n ///\n /// It is stable across reparses, and can be used as salsa key/value."}, {"sha": "2926b29fd0857ce3d556f8942d80fc7f1fc3785d", "filename": "crates/ra_mbe/src/lib.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/159da285e9d8594a2cc4436b5cee7874b07806c9/crates%2Fra_mbe%2Fsrc%2Flib.rs", "raw_url": "https://github.com/rust-lang/rust/raw/159da285e9d8594a2cc4436b5cee7874b07806c9/crates%2Fra_mbe%2Fsrc%2Flib.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fra_mbe%2Fsrc%2Flib.rs?ref=159da285e9d8594a2cc4436b5cee7874b07806c9", "patch": "@@ -32,7 +32,7 @@ pub enum ExpandError {\n \n pub use crate::syntax_bridge::{\n     ast_to_token_tree, syntax_node_to_token_tree, token_tree_to_expr, token_tree_to_items,\n-    token_tree_to_macro_stmts, token_tree_to_pat, token_tree_to_ty,\n+    token_tree_to_macro_stmts, token_tree_to_pat, token_tree_to_ty, TokenMap,\n };\n \n /// This struct contains AST for a single `macro_rules` definition. What might"}, {"sha": "5db6647e3a4e134db2a04556780d83812518a614", "filename": "crates/ra_mbe/src/syntax_bridge.rs", "status": "modified", "additions": 94, "deletions": 19, "changes": 113, "blob_url": "https://github.com/rust-lang/rust/blob/159da285e9d8594a2cc4436b5cee7874b07806c9/crates%2Fra_mbe%2Fsrc%2Fsyntax_bridge.rs", "raw_url": "https://github.com/rust-lang/rust/raw/159da285e9d8594a2cc4436b5cee7874b07806c9/crates%2Fra_mbe%2Fsrc%2Fsyntax_bridge.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fra_mbe%2Fsrc%2Fsyntax_bridge.rs?ref=159da285e9d8594a2cc4436b5cee7874b07806c9", "patch": "@@ -15,6 +15,7 @@ use crate::ExpandError;\n use std::sync::atomic::{AtomicU32, Ordering};\n \n /// Maps `tt::TokenId` to the relative range of the original token.\n+#[derive(Debug, PartialEq, Eq)]\n pub struct TokenMap {\n     /// Maps `tt::TokenId` to the *relative* source range.\n     tokens: Vec<TextRange>,\n@@ -34,6 +35,13 @@ impl std::default::Default for TokenMap {\n     }\n }\n \n+/// Maps Relative range of the expanded syntax node to `tt::TokenId`\n+#[derive(Debug, PartialEq, Eq, Default)]\n+pub struct ExpandedRangeMap {\n+    /// Maps `tt::TokenId` to the *relative* source range.\n+    ranges: Vec<(TextRange, tt::TokenId)>,\n+}\n+\n /// Convert the syntax tree (what user has written) to a `TokenTree` (what macro\n /// will consume).\n pub fn ast_to_token_tree(ast: &ast::TokenTree) -> Option<(tt::Subtree, TokenMap)> {\n@@ -66,7 +74,7 @@ pub fn syntax_node_to_token_tree(node: &SyntaxNode) -> Option<(tt::Subtree, Toke\n fn fragment_to_syntax_node(\n     tt: &tt::Subtree,\n     fragment_kind: FragmentKind,\n-) -> Result<Parse<SyntaxNode>, ExpandError> {\n+) -> Result<(Parse<SyntaxNode>, ExpandedRangeMap), ExpandError> {\n     let tmp;\n     let tokens = match tt {\n         tt::Subtree { delimiter: tt::Delimiter::None, token_trees } => token_trees.as_slice(),\n@@ -77,44 +85,55 @@ fn fragment_to_syntax_node(\n     };\n     let buffer = TokenBuffer::new(&tokens);\n     let mut token_source = SubtreeTokenSource::new(&buffer);\n-    let mut tree_sink = TtTreeSink::new(buffer.begin());\n+    let mut range_map = ExpandedRangeMap::default();\n+    let mut tree_sink = TtTreeSink::new(buffer.begin(), &mut range_map);\n     ra_parser::parse_fragment(&mut token_source, &mut tree_sink, fragment_kind);\n     if tree_sink.roots.len() != 1 {\n         return Err(ExpandError::ConversionError);\n     }\n     //FIXME: would be cool to report errors\n     let parse = tree_sink.inner.finish();\n-    Ok(parse)\n+    Ok((parse, range_map))\n }\n \n /// Parses the token tree (result of macro expansion) to an expression\n-pub fn token_tree_to_expr(tt: &tt::Subtree) -> Result<Parse<ast::Expr>, ExpandError> {\n-    let parse = fragment_to_syntax_node(tt, Expr)?;\n-    parse.cast().ok_or_else(|| crate::ExpandError::ConversionError)\n+pub fn token_tree_to_expr(\n+    tt: &tt::Subtree,\n+) -> Result<(Parse<ast::Expr>, ExpandedRangeMap), ExpandError> {\n+    let (parse, map) = fragment_to_syntax_node(tt, Expr)?;\n+    parse.cast().ok_or_else(|| crate::ExpandError::ConversionError).map(|p| (p, map))\n }\n \n /// Parses the token tree (result of macro expansion) to a Pattern\n-pub fn token_tree_to_pat(tt: &tt::Subtree) -> Result<Parse<ast::Pat>, ExpandError> {\n-    let parse = fragment_to_syntax_node(tt, Pattern)?;\n-    parse.cast().ok_or_else(|| crate::ExpandError::ConversionError)\n+pub fn token_tree_to_pat(\n+    tt: &tt::Subtree,\n+) -> Result<(Parse<ast::Pat>, ExpandedRangeMap), ExpandError> {\n+    let (parse, map) = fragment_to_syntax_node(tt, Pattern)?;\n+    parse.cast().ok_or_else(|| crate::ExpandError::ConversionError).map(|p| (p, map))\n }\n \n /// Parses the token tree (result of macro expansion) to a Type\n-pub fn token_tree_to_ty(tt: &tt::Subtree) -> Result<Parse<ast::TypeRef>, ExpandError> {\n-    let parse = fragment_to_syntax_node(tt, Type)?;\n-    parse.cast().ok_or_else(|| crate::ExpandError::ConversionError)\n+pub fn token_tree_to_ty(\n+    tt: &tt::Subtree,\n+) -> Result<(Parse<ast::TypeRef>, ExpandedRangeMap), ExpandError> {\n+    let (parse, map) = fragment_to_syntax_node(tt, Type)?;\n+    parse.cast().ok_or_else(|| crate::ExpandError::ConversionError).map(|p| (p, map))\n }\n \n /// Parses the token tree (result of macro expansion) as a sequence of stmts\n-pub fn token_tree_to_macro_stmts(tt: &tt::Subtree) -> Result<Parse<ast::MacroStmts>, ExpandError> {\n-    let parse = fragment_to_syntax_node(tt, Statements)?;\n-    parse.cast().ok_or_else(|| crate::ExpandError::ConversionError)\n+pub fn token_tree_to_macro_stmts(\n+    tt: &tt::Subtree,\n+) -> Result<(Parse<ast::MacroStmts>, ExpandedRangeMap), ExpandError> {\n+    let (parse, map) = fragment_to_syntax_node(tt, Statements)?;\n+    parse.cast().ok_or_else(|| crate::ExpandError::ConversionError).map(|p| (p, map))\n }\n \n /// Parses the token tree (result of macro expansion) as a sequence of items\n-pub fn token_tree_to_items(tt: &tt::Subtree) -> Result<Parse<ast::MacroItems>, ExpandError> {\n-    let parse = fragment_to_syntax_node(tt, Items)?;\n-    parse.cast().ok_or_else(|| crate::ExpandError::ConversionError)\n+pub fn token_tree_to_items(\n+    tt: &tt::Subtree,\n+) -> Result<(Parse<ast::MacroItems>, ExpandedRangeMap), ExpandError> {\n+    let (parse, map) = fragment_to_syntax_node(tt, Items)?;\n+    parse.cast().ok_or_else(|| crate::ExpandError::ConversionError).map(|p| (p, map))\n }\n \n impl TokenMap {\n@@ -133,6 +152,28 @@ impl TokenMap {\n     }\n }\n \n+impl ExpandedRangeMap {\n+    fn set(&mut self, relative_range: TextRange, token_id: &tt::TokenId) {\n+        self.ranges.push((relative_range, token_id.clone()))\n+    }\n+\n+    pub fn ranges(&self, to: &TokenMap) -> Vec<(TextRange, TextRange)> {\n+        self.ranges\n+            .iter()\n+            .filter_map(|(r, tid)| {\n+                if to.map_id == tid.map_id() {\n+                    return None;\n+                }\n+                if let Some(to_range) = to.relative_range_of(*tid) {\n+                    Some((*r, to_range))\n+                } else {\n+                    None\n+                }\n+            })\n+            .collect()\n+    }\n+}\n+\n /// Returns the textual content of a doc comment block as a quoted string\n /// That is, strips leading `///` (or `/**`, etc)\n /// and strips the ending `*/`\n@@ -279,20 +320,24 @@ struct TtTreeSink<'a> {\n     cursor: Cursor<'a>,\n     text_pos: TextUnit,\n     inner: SyntaxTreeBuilder,\n+    range_marker: Option<(TextRange, tt::TokenId)>,\n+    range_map: &'a mut ExpandedRangeMap,\n \n     // Number of roots\n     // Use for detect ill-form tree which is not single root\n     roots: smallvec::SmallVec<[usize; 1]>,\n }\n \n impl<'a> TtTreeSink<'a> {\n-    fn new(cursor: Cursor<'a>) -> Self {\n+    fn new(cursor: Cursor<'a>, range_map: &'a mut ExpandedRangeMap) -> Self {\n         TtTreeSink {\n             buf: String::new(),\n             cursor,\n             text_pos: 0.into(),\n             inner: SyntaxTreeBuilder::default(),\n             roots: smallvec::SmallVec::new(),\n+            range_map,\n+            range_marker: None,\n         }\n     }\n }\n@@ -317,6 +362,8 @@ impl<'a> TreeSink for TtTreeSink<'a> {\n             return;\n         }\n \n+        let mut last_ident = None;\n+\n         for _ in 0..n_tokens {\n             if self.cursor.eof() {\n                 break;\n@@ -326,6 +373,10 @@ impl<'a> TreeSink for TtTreeSink<'a> {\n                 Some(tt::TokenTree::Leaf(leaf)) => {\n                     self.cursor = self.cursor.bump();\n                     self.buf += &format!(\"{}\", leaf);\n+\n+                    if let tt::Leaf::Ident(ident) = leaf {\n+                        last_ident = Some(ident);\n+                    }\n                 }\n                 Some(tt::TokenTree::Subtree(subtree)) => {\n                     self.cursor = self.cursor.subtree().unwrap();\n@@ -345,6 +396,14 @@ impl<'a> TreeSink for TtTreeSink<'a> {\n         self.buf.clear();\n         self.inner.token(kind, text);\n \n+        // Mark the range if needed\n+        if let Some((range, token_id)) = self.range_marker.as_mut() {\n+            if let Some(ident) = last_ident {\n+                *range = TextRange::offset_len(range.start(), TextUnit::of_str(&ident.text));\n+                *token_id = ident.id;\n+            }\n+        }\n+\n         // Add whitespace between adjoint puncts\n         let next = self.cursor.bump();\n         if let (\n@@ -354,13 +413,23 @@ impl<'a> TreeSink for TtTreeSink<'a> {\n         {\n             if curr.spacing == tt::Spacing::Alone {\n                 self.inner.token(WHITESPACE, \" \".into());\n+                self.text_pos += TextUnit::of_char(' ');\n             }\n         }\n     }\n \n     fn start_node(&mut self, kind: SyntaxKind) {\n         self.inner.start_node(kind);\n \n+        self.range_marker = if kind == IDENT {\n+            Some((\n+                TextRange::offset_len(self.text_pos, TextUnit::from_usize(0)),\n+                tt::TokenId::unspecified(),\n+            ))\n+        } else {\n+            None\n+        };\n+\n         match self.roots.last_mut() {\n             None | Some(0) => self.roots.push(1),\n             Some(ref mut n) => **n += 1,\n@@ -370,6 +439,12 @@ impl<'a> TreeSink for TtTreeSink<'a> {\n     fn finish_node(&mut self) {\n         self.inner.finish_node();\n         *self.roots.last_mut().unwrap() -= 1;\n+\n+        if let Some(range) = self.range_marker {\n+            if range.1 != tt::TokenId::unspecified() {\n+                self.range_map.set(range.0, &range.1)\n+            }\n+        }\n     }\n \n     fn error(&mut self, error: ParseError) {"}, {"sha": "a848ea3347ac611d00bf06c7970d5e7c1deabfa5", "filename": "crates/ra_mbe/src/tests.rs", "status": "modified", "additions": 8, "deletions": 8, "changes": 16, "blob_url": "https://github.com/rust-lang/rust/blob/159da285e9d8594a2cc4436b5cee7874b07806c9/crates%2Fra_mbe%2Fsrc%2Ftests.rs", "raw_url": "https://github.com/rust-lang/rust/raw/159da285e9d8594a2cc4436b5cee7874b07806c9/crates%2Fra_mbe%2Fsrc%2Ftests.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fra_mbe%2Fsrc%2Ftests.rs?ref=159da285e9d8594a2cc4436b5cee7874b07806c9", "patch": "@@ -126,7 +126,7 @@ fn test_expr_order() {\n \"#,\n     );\n     let expanded = expand(&rules, \"foo! { 1 + 1}\");\n-    let tree = token_tree_to_items(&expanded).unwrap().tree();\n+    let tree = token_tree_to_items(&expanded).unwrap().0.tree();\n \n     let dump = format!(\"{:#?}\", tree.syntax());\n     assert_eq_text!(\n@@ -383,7 +383,7 @@ fn test_expand_to_item_list() {\n             \",\n     );\n     let expansion = expand(&rules, \"structs!(Foo, Bar);\");\n-    let tree = token_tree_to_items(&expansion).unwrap().tree();\n+    let tree = token_tree_to_items(&expansion).unwrap().0.tree();\n     assert_eq!(\n         format!(\"{:#?}\", tree.syntax()).trim(),\n         r#\"\n@@ -501,7 +501,7 @@ fn test_tt_to_stmts() {\n     );\n \n     let expanded = expand(&rules, \"foo!{}\");\n-    let stmts = token_tree_to_macro_stmts(&expanded).unwrap().tree();\n+    let stmts = token_tree_to_macro_stmts(&expanded).unwrap().0.tree();\n \n     assert_eq!(\n         format!(\"{:#?}\", stmts.syntax()).trim(),\n@@ -946,7 +946,7 @@ fn test_vec() {\n     );\n \n     let expansion = expand(&rules, r#\"vec![1u32,2];\"#);\n-    let tree = token_tree_to_expr(&expansion).unwrap().tree();\n+    let tree = token_tree_to_expr(&expansion).unwrap().0.tree();\n \n     assert_eq!(\n         format!(\"{:#?}\", tree.syntax()).trim(),\n@@ -1436,8 +1436,8 @@ pub(crate) fn assert_expansion(\n     };\n     let (expanded_tree, expected_tree) = match kind {\n         MacroKind::Items => {\n-            let expanded_tree = token_tree_to_items(&expanded).unwrap().tree();\n-            let expected_tree = token_tree_to_items(&expected).unwrap().tree();\n+            let expanded_tree = token_tree_to_items(&expanded).unwrap().0.tree();\n+            let expected_tree = token_tree_to_items(&expected).unwrap().0.tree();\n \n             (\n                 debug_dump_ignore_spaces(expanded_tree.syntax()).trim().to_string(),\n@@ -1446,8 +1446,8 @@ pub(crate) fn assert_expansion(\n         }\n \n         MacroKind::Stmts => {\n-            let expanded_tree = token_tree_to_macro_stmts(&expanded).unwrap().tree();\n-            let expected_tree = token_tree_to_macro_stmts(&expected).unwrap().tree();\n+            let expanded_tree = token_tree_to_macro_stmts(&expanded).unwrap().0.tree();\n+            let expected_tree = token_tree_to_macro_stmts(&expected).unwrap().0.tree();\n \n             (\n                 debug_dump_ignore_spaces(expanded_tree.syntax()).trim().to_string(),"}]}