{"sha": "9cb6b8da3f6ca2da1139c91754d520bf2d354f31", "node_id": "MDY6Q29tbWl0NzI0NzEyOjljYjZiOGRhM2Y2Y2EyZGExMTM5YzkxNzU0ZDUyMGJmMmQzNTRmMzE=", "commit": {"author": {"name": "JCTyBlaidd", "email": "JCTyblaidd@users.noreply.github.com", "date": "2020-11-05T03:54:39Z"}, "committer": {"name": "JCTyBlaidd", "email": "JCTyblaidd@users.noreply.github.com", "date": "2020-11-05T03:54:39Z"}, "message": "Split out vector_clock to separate file, general tidy up of some of the\n code & add support for vector index re-use for multiple threads\n after termination.", "tree": {"sha": "8b678d535b480855ce098924fe097214d80779d5", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/8b678d535b480855ce098924fe097214d80779d5"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/9cb6b8da3f6ca2da1139c91754d520bf2d354f31", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/9cb6b8da3f6ca2da1139c91754d520bf2d354f31", "html_url": "https://github.com/rust-lang/rust/commit/9cb6b8da3f6ca2da1139c91754d520bf2d354f31", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/9cb6b8da3f6ca2da1139c91754d520bf2d354f31/comments", "author": {"login": "JCTyblaidd", "id": 8288600, "node_id": "MDQ6VXNlcjgyODg2MDA=", "avatar_url": "https://avatars.githubusercontent.com/u/8288600?v=4", "gravatar_id": "", "url": "https://api.github.com/users/JCTyblaidd", "html_url": "https://github.com/JCTyblaidd", "followers_url": "https://api.github.com/users/JCTyblaidd/followers", "following_url": "https://api.github.com/users/JCTyblaidd/following{/other_user}", "gists_url": "https://api.github.com/users/JCTyblaidd/gists{/gist_id}", "starred_url": "https://api.github.com/users/JCTyblaidd/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/JCTyblaidd/subscriptions", "organizations_url": "https://api.github.com/users/JCTyblaidd/orgs", "repos_url": "https://api.github.com/users/JCTyblaidd/repos", "events_url": "https://api.github.com/users/JCTyblaidd/events{/privacy}", "received_events_url": "https://api.github.com/users/JCTyblaidd/received_events", "type": "User", "site_admin": false}, "committer": {"login": "JCTyblaidd", "id": 8288600, "node_id": "MDQ6VXNlcjgyODg2MDA=", "avatar_url": "https://avatars.githubusercontent.com/u/8288600?v=4", "gravatar_id": "", "url": "https://api.github.com/users/JCTyblaidd", "html_url": "https://github.com/JCTyblaidd", "followers_url": "https://api.github.com/users/JCTyblaidd/followers", "following_url": "https://api.github.com/users/JCTyblaidd/following{/other_user}", "gists_url": "https://api.github.com/users/JCTyblaidd/gists{/gist_id}", "starred_url": "https://api.github.com/users/JCTyblaidd/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/JCTyblaidd/subscriptions", "organizations_url": "https://api.github.com/users/JCTyblaidd/orgs", "repos_url": "https://api.github.com/users/JCTyblaidd/repos", "events_url": "https://api.github.com/users/JCTyblaidd/events{/privacy}", "received_events_url": "https://api.github.com/users/JCTyblaidd/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "95c99b2044a75f27e691308ebbb7ed0d4e2cbf3b", "url": "https://api.github.com/repos/rust-lang/rust/commits/95c99b2044a75f27e691308ebbb7ed0d4e2cbf3b", "html_url": "https://github.com/rust-lang/rust/commit/95c99b2044a75f27e691308ebbb7ed0d4e2cbf3b"}], "stats": {"total": 2013, "additions": 1084, "deletions": 929}, "files": [{"sha": "e992c5a1d5899125774d6a04da615cee32fa8f86", "filename": "src/data_race.rs", "status": "modified", "additions": 424, "deletions": 909, "changes": 1333, "blob_url": "https://github.com/rust-lang/rust/blob/9cb6b8da3f6ca2da1139c91754d520bf2d354f31/src%2Fdata_race.rs", "raw_url": "https://github.com/rust-lang/rust/raw/9cb6b8da3f6ca2da1139c91754d520bf2d354f31/src%2Fdata_race.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Fdata_race.rs?ref=9cb6b8da3f6ca2da1139c91754d520bf2d354f31", "patch": "@@ -13,21 +13,21 @@\n //!    - 1 of the operations happens-before the other operation (see link for definition)\n \n use std::{\n-    fmt::{self, Debug}, cmp::Ordering, rc::Rc,\n-    cell::{Cell, RefCell, Ref, RefMut}, ops::Index, mem\n+    fmt::Debug, rc::Rc,\n+    cell::{Cell, RefCell, Ref, RefMut}, mem\n };\n \n use rustc_index::vec::{Idx, IndexVec};\n use rustc_target::abi::Size;\n use rustc_middle::ty::layout::TyAndLayout;\n-use rustc_data_structures::fx::FxHashMap;\n-\n-use smallvec::SmallVec;\n+use rustc_data_structures::fx::FxHashSet;\n \n use crate::{\n-    MiriEvalContext, ThreadId, Tag, MiriEvalContextExt, RangeMap,\n-    MPlaceTy, ImmTy, InterpResult, Pointer, ScalarMaybeUninit,\n-    OpTy, Immediate, MemPlaceMeta\n+    MiriEvalContext, MiriEvalContextExt,\n+    ThreadId, Tag, RangeMap,\n+    InterpResult, Pointer, ScalarMaybeUninit,\n+    MPlaceTy, OpTy, MemPlaceMeta,\n+    VClock, VSmallClockSet, VectorIdx, VTimestamp\n };\n \n pub type AllocExtra = VClockAlloc;\n@@ -73,194 +73,136 @@ pub enum AtomicFenceOp {\n impl<'mir, 'tcx: 'mir> EvalContextExt<'mir, 'tcx> for MiriEvalContext<'mir, 'tcx> {}\n pub trait EvalContextExt<'mir, 'tcx: 'mir>: MiriEvalContextExt<'mir, 'tcx> {\n \n-    /// Variant of `read_immediate` that does not perform `data-race` checks.\n-    fn read_immediate_racy(&self, op: MPlaceTy<'tcx, Tag>) -> InterpResult<'tcx, ImmTy<'tcx, Tag>> {\n+    // Temporarily allow data-races to occur, this should only be\n+    //  used if either one of the appropiate `validate_atomic` functions\n+    //  will be called to treat a memory access as atomic or if the memory\n+    //  being accessed should be treated as internal state, that cannot be\n+    //  accessed by the interpreted program.\n+    #[inline]\n+    fn allow_data_races_ref<R>(&self, op: impl FnOnce(&MiriEvalContext<'mir, 'tcx>) -> R) -> R {\n         let this = self.eval_context_ref();\n         let data_race = &*this.memory.extra.data_race;\n-\n         let old = data_race.multi_threaded.replace(false);\n-        let res = this.read_immediate(op.into());\n+        let result = op(this);\n         data_race.multi_threaded.set(old);\n-\n-        res\n+        result\n     }\n-    \n-    /// Variant of `write_immediate` that does not perform `data-race` checks.\n-    fn write_immediate_racy(\n-        &mut self, src: Immediate<Tag>, dest: MPlaceTy<'tcx, Tag>\n-    ) -> InterpResult<'tcx> {\n+\n+    /// Same as `allow_data_races_ref`, this temporarily disables any data-race detection and\n+    ///  so should only be used for atomic operations or internal state that the program cannot\n+    ///  access\n+    #[inline]\n+    fn allow_data_races_mut<R>(&mut self, op: impl FnOnce(&mut MiriEvalContext<'mir, 'tcx>) -> R) -> R {\n         let this = self.eval_context_mut();\n         let data_race = &*this.memory.extra.data_race;\n         let old = data_race.multi_threaded.replace(false);\n-\n-        let imm = this.write_immediate(src, dest.into());\n-\n+        let result = op(this);\n         let data_race = &*this.memory.extra.data_race;\n         data_race.multi_threaded.set(old);\n-        imm\n+        result\n     }\n \n-    /// Variant of `read_scalar` that does not perform data-race checks.\n-    fn read_scalar_racy(\n-        &self, op: MPlaceTy<'tcx, Tag>\n-    )-> InterpResult<'tcx, ScalarMaybeUninit<Tag>> {\n-        Ok(self.read_immediate_racy(op)?.to_scalar_or_uninit())\n-    }\n \n-    /// Variant of `write_scalar` that does not perform data-race checks.\n-    fn write_scalar_racy(\n-        &mut self, val: ScalarMaybeUninit<Tag>, dest: MPlaceTy<'tcx, Tag>\n-    ) -> InterpResult<'tcx> {\n-        self.write_immediate_racy(Immediate::Scalar(val.into()), dest)\n-    }\n-\n-    /// Variant of `read_scalar_at_offset` helper function that does not perform\n-    /// `data-race checks.\n-    fn read_scalar_at_offset_racy(\n-        &self,\n-        op: OpTy<'tcx, Tag>,\n-        offset: u64,\n-        layout: TyAndLayout<'tcx>,\n-    ) -> InterpResult<'tcx, ScalarMaybeUninit<Tag>> {\n-        let this = self.eval_context_ref();\n-        let op_place = this.deref_operand(op)?;\n-        let offset = Size::from_bytes(offset);\n-        // Ensure that the following read at an offset is within bounds\n-        assert!(op_place.layout.size >= offset + layout.size);\n-        let value_place = op_place.offset(offset, MemPlaceMeta::None, layout, this)?;\n-        this.read_scalar_racy(value_place.into())\n-    }\n-\n-    /// Variant of `write_scalar_at_offfset` helper function that performs\n-    ///  an atomic load operation with verification instead\n     fn read_scalar_at_offset_atomic(\n-        &mut self,\n+        &self,\n         op: OpTy<'tcx, Tag>,\n         offset: u64,\n         layout: TyAndLayout<'tcx>,\n         atomic: AtomicReadOp\n     ) -> InterpResult<'tcx, ScalarMaybeUninit<Tag>> {\n-        let this = self.eval_context_mut();\n+        let this = self.eval_context_ref();\n         let op_place = this.deref_operand(op)?;\n         let offset = Size::from_bytes(offset);\n         // Ensure that the following read at an offset is within bounds\n         assert!(op_place.layout.size >= offset + layout.size);\n         let value_place = op_place.offset(offset, MemPlaceMeta::None, layout, this)?;\n-        let res = this.read_scalar_racy(value_place.into())?;\n-        this.validate_atomic_load(value_place, atomic)?;\n-        Ok(res)\n+        this.read_scalar_atomic(value_place, atomic)\n     }\n-\n-    /// Variant of `write_scalar_at_offfset` helper function that does not perform\n-    ///  data-race checks.\n-    fn write_scalar_at_offset_racy(\n+    fn write_scalar_at_offset_atomic(\n         &mut self,\n         op: OpTy<'tcx, Tag>,\n         offset: u64,\n         value: impl Into<ScalarMaybeUninit<Tag>>,\n         layout: TyAndLayout<'tcx>,\n-    ) -> InterpResult<'tcx, ()> {\n+        atomic: AtomicWriteOp\n+    ) -> InterpResult<'tcx> {\n         let this = self.eval_context_mut();\n         let op_place = this.deref_operand(op)?;\n         let offset = Size::from_bytes(offset);\n         // Ensure that the following read at an offset is within bounds\n         assert!(op_place.layout.size >= offset + layout.size);\n         let value_place = op_place.offset(offset, MemPlaceMeta::None, layout, this)?;\n-        this.write_scalar_racy(value.into(), value_place.into())\n+        this.write_scalar_atomic(value.into(), value_place, atomic)\n     }\n-\n-    /// Load the data race allocation state for a given memory place\n-    ///  also returns the size and offset of the result in the allocation\n-    ///  metadata\n-    /// This is used for atomic loads since unconditionally requesteing\n-    ///  mutable access causes issues for read-only memory, which will\n-    ///  fail validation on mutable access\n-    fn load_data_race_state_ref<'a>(\n-        &'a self, place: MPlaceTy<'tcx, Tag>\n-    ) -> InterpResult<'tcx, (&'a VClockAlloc, Size, Size)> where 'mir: 'a {\n-        let this = self.eval_context_ref();\n-\n-        let ptr = place.ptr.assert_ptr();\n-        let size = place.layout.size;\n-        let data_race = &this.memory.get_raw(ptr.alloc_id)?.extra.data_race;\n-\n-        Ok((data_race, size, ptr.offset))\n-    }\n-\n-    /// Load the data race allocation state for a given memory place\n-    ///  also returns the size and the offset of the result in the allocation\n-    ///  metadata\n-    fn load_data_race_state_mut<'a>(\n-        &'a mut self, place: MPlaceTy<'tcx, Tag>\n-    ) -> InterpResult<'tcx, (&'a mut VClockAlloc, Size, Size)> where 'mir: 'a {\n-        let this = self.eval_context_mut();\n-\n-        let ptr = place.ptr.assert_ptr();\n-        let size = place.layout.size;\n-        let data_race = &mut this.memory.get_raw_mut(ptr.alloc_id)?.extra.data_race;\n-\n-        Ok((data_race, size, ptr.offset))\n+    fn read_scalar_atomic(\n+        &self, place: MPlaceTy<'tcx, Tag>, atomic: AtomicReadOp\n+    ) -> InterpResult<'tcx, ScalarMaybeUninit<Tag>> {\n+        let scalar = self.allow_data_races_ref(move |this| {\n+            this.read_scalar(place.into())\n+        })?;\n+        self.validate_atomic_load(place, atomic)?;\n+        Ok(scalar)\n+    }\n+    fn write_scalar_atomic(\n+        &mut self, val: ScalarMaybeUninit<Tag>, dest: MPlaceTy<'tcx, Tag>,\n+        atomic: AtomicWriteOp\n+    ) -> InterpResult<'tcx> {\n+        self.allow_data_races_mut(move |this| {\n+            this.write_scalar(val, dest.into())\n+        })?;\n+        self.validate_atomic_store(dest, atomic)\n     }\n     \n     /// Update the data-race detector for an atomic read occuring at the\n     ///  associated memory-place and on the current thread\n     fn validate_atomic_load(\n-        &mut self, place: MPlaceTy<'tcx, Tag>, atomic: AtomicReadOp\n+        &self, place: MPlaceTy<'tcx, Tag>, atomic: AtomicReadOp\n     ) -> InterpResult<'tcx> {\n-        let this = self.eval_context_mut();\n+        let this = self.eval_context_ref();\n         let data_race = &*this.memory.extra.data_race;\n         if data_race.multi_threaded.get() {\n-            data_race.advance_vector_clock();\n \n-            let (\n-                alloc, size, offset\n-            ) = this.load_data_race_state_ref(place)?;\n+            // Load an log the atomic operation\n+            //  the memory access has to be `get_raw` since otherwise this despite only \n+            //  mutating MemoryExtra will still trigger errors on read-only memory\n+            let place_ptr = place.ptr.assert_ptr();\n+            let size = place.layout.size;\n+            let alloc_meta =  &this.memory.get_raw(place_ptr.alloc_id)?.extra.data_race;\n             log::trace!(\n-                \"Atomic load on {:?} with ordering {:?}, in memory({:?}, offset={}, size={})\",\n-                alloc.global.current_thread(), atomic,\n-                place.ptr.assert_ptr().alloc_id, offset.bytes(), size.bytes()\n+                \"Atomic op({}) with ordering {:?} on memory({:?}, offset={}, size={})\",\n+                \"Atomic load\", &atomic, place_ptr.alloc_id, place_ptr.offset.bytes(), size.bytes()\n             );\n \n-            let current_thread = alloc.global.current_thread();\n-            let mut current_state = alloc.global.current_thread_state_mut();\n-            if atomic == AtomicReadOp::Relaxed {\n-                // Perform relaxed atomic load\n-                for (_,range) in alloc.alloc_ranges.borrow_mut().iter_mut(offset, size) {\n-                    if range.load_relaxed(&mut *current_state, current_thread) == Err(DataRace) {\n-                        mem::drop(current_state);\n+            // Perform the atomic operation\n+            let data_race = &alloc_meta.global;\n+            data_race.maybe_perform_sync_operation(move |index, mut clocks| {\n+                for (_,range) in alloc_meta.alloc_ranges.borrow_mut().iter_mut(place_ptr.offset, size) {\n+                    let res = if atomic == AtomicReadOp::Relaxed {\n+                        range.load_relaxed(&mut *clocks, index)\n+                    }else{\n+                        range.acquire(&mut *clocks, index)\n+                    };\n+                    if let Err(DataRace) = res {\n+                        mem::drop(clocks);\n                         return VClockAlloc::report_data_race(\n-                            &alloc.global, range, \"ATOMIC_LOAD\", true,\n-                            place.ptr.assert_ptr(), size\n+                            &alloc_meta.global, range, \"Atomic load\", true,\n+                            place_ptr, size\n                         );\n                     }\n                 }\n-            }else{\n-                // Perform acquire(or seq-cst) atomic load\n-                for (_,range) in alloc.alloc_ranges.borrow_mut().iter_mut(offset, size) {\n-                    if range.acquire(&mut *current_state, current_thread) == Err(DataRace) {\n-                        mem::drop(current_state);\n-                        return VClockAlloc::report_data_race(\n-                            &alloc.global, range, \"ATOMIC_LOAD\", true,\n-                            place.ptr.assert_ptr(), size\n-                        );\n-                    }\n-                }\n-            }\n+                Ok(())\n+            })?;\n \n             // Log changes to atomic memory\n             if log::log_enabled!(log::Level::Trace) {\n-                for (_,range) in alloc.alloc_ranges.borrow_mut().iter(offset, size) {\n+                for (_,range) in alloc_meta.alloc_ranges.borrow().iter(place_ptr.offset, size) {\n                     log::trace!(\n-                        \"  updated atomic memory({:?}, offset={}, size={}) to {:#?}\",\n-                        place.ptr.assert_ptr().alloc_id, offset.bytes(), size.bytes(),\n+                        \"Updated atomic memory({:?}, offset={}, size={}) to {:#?}\",\n+                        place.ptr.assert_ptr().alloc_id, place_ptr.offset.bytes(), size.bytes(),\n                         range.atomic_ops\n                     );\n                 }\n             }\n-\n-            mem::drop(current_state);\n-            let data_race = &*this.memory.extra.data_race;\n-            data_race.advance_vector_clock();\n         }\n         Ok(())\n     }\n@@ -271,61 +213,16 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: MiriEvalContextExt<'mir, 'tcx> {\n         &mut self, place: MPlaceTy<'tcx, Tag>, atomic: AtomicWriteOp\n     ) -> InterpResult<'tcx> {\n         let this = self.eval_context_mut();\n-        let data_race = &*this.memory.extra.data_race;\n-        if data_race.multi_threaded.get() {\n-            data_race.advance_vector_clock();\n-\n-            let (\n-                alloc, size, offset\n-            ) = this.load_data_race_state_mut(place)?;\n-            let current_thread = alloc.global.current_thread();\n-            let mut current_state = alloc.global.current_thread_state_mut();\n-            log::trace!(\n-                \"Atomic store on {:?} with ordering {:?}, in memory({:?}, offset={}, size={})\",\n-                current_thread, atomic,\n-                place.ptr.assert_ptr().alloc_id, offset.bytes(), size.bytes()\n-            );\n-\n-            if atomic == AtomicWriteOp::Relaxed {\n-                // Perform relaxed atomic store\n-                for (_,range) in alloc.alloc_ranges.get_mut().iter_mut(offset, size) {\n-                    if range.store_relaxed(&mut *current_state, current_thread) == Err(DataRace) {\n-                        mem::drop(current_state);\n-                        return VClockAlloc::report_data_race(\n-                            &alloc.global, range, \"ATOMIC_STORE\", true,\n-                            place.ptr.assert_ptr(), size\n-                        );\n-                    }\n-                }\n-            }else{\n-                // Perform release(or seq-cst) atomic store\n-                for (_,range) in alloc.alloc_ranges.get_mut().iter_mut(offset, size) {\n-                    if range.release(&mut *current_state, current_thread) == Err(DataRace) {\n-                        mem::drop(current_state);\n-                        return VClockAlloc::report_data_race(\n-                            &alloc.global, range, \"ATOMIC_STORE\", true,\n-                            place.ptr.assert_ptr(), size\n-                        );\n-                    }\n-                }\n-            }\n-\n-            // Log changes to atomic memory\n-            if log::log_enabled!(log::Level::Trace) {\n-                for (_,range) in alloc.alloc_ranges.get_mut().iter(offset, size) {\n-                    log::trace!(\n-                        \"  updated atomic memory({:?}, offset={}, size={}) to {:#?}\",\n-                        place.ptr.assert_ptr().alloc_id, offset.bytes(), size.bytes(),\n-                        range.atomic_ops\n-                    );\n+        this.validate_atomic_op_mut(\n+            place, atomic, \"Atomic Store\",\n+            move |memory, clocks, index, atomic| {\n+                if atomic == AtomicWriteOp::Relaxed {\n+                    memory.store_relaxed(clocks, index)\n+                }else{\n+                    memory.release(clocks, index)\n                 }\n             }\n-\n-            mem::drop(current_state);\n-            let data_race = &*this.memory.extra.data_race;\n-            data_race.advance_vector_clock();\n-        }\n-        Ok(())\n+        )\n     }\n \n     /// Update the data-race detector for an atomic read-modify-write occuring\n@@ -334,97 +231,104 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: MiriEvalContextExt<'mir, 'tcx> {\n         &mut self, place: MPlaceTy<'tcx, Tag>, atomic: AtomicRWOp\n     ) -> InterpResult<'tcx> {\n         use AtomicRWOp::*;\n+        let acquire = matches!(atomic, Acquire | AcqRel | SeqCst);\n+        let release = matches!(atomic, Release | AcqRel | SeqCst);\n         let this = self.eval_context_mut();\n-        let data_race = &*this.memory.extra.data_race;\n-        if data_race.multi_threaded.get() {\n-            data_race.advance_vector_clock();\n-\n-            let (\n-                alloc, size, offset\n-            ) = this.load_data_race_state_mut(place)?;\n-            let current_thread = alloc.global.current_thread();\n-            let mut current_state = alloc.global.current_thread_state_mut();\n-            log::trace!(\n-                \"Atomic RMW on {:?} with ordering {:?}, in memory({:?}, offset={}, size={})\",\n-                current_thread, atomic,\n-                place.ptr.assert_ptr().alloc_id, offset.bytes(), size.bytes()\n-            );\n-\n-            let acquire = matches!(atomic, Acquire | AcqRel | SeqCst);\n-            let release = matches!(atomic, Release | AcqRel | SeqCst);\n-            for (_,range) in alloc.alloc_ranges.get_mut().iter_mut(offset, size) {\n-                //FIXME: this is probably still slightly wrong due to the quirks\n-                // in the c++11 memory model\n-                let maybe_race = if acquire {\n-                    // Atomic RW-Op acquire\n-                    range.acquire(&mut *current_state, current_thread)\n+        this.validate_atomic_op_mut(\n+            place, atomic, \"Atomic RMW\",\n+            move |memory, clocks, index, _| {\n+                if acquire {\n+                    memory.acquire(clocks, index)?;\n                 }else{\n-                    range.load_relaxed(&mut *current_state, current_thread) \n-                };\n-                if maybe_race == Err(DataRace) {\n-                    mem::drop(current_state);\n-                    return VClockAlloc::report_data_race(\n-                        &alloc.global, range, \"ATOMIC_RMW(LOAD)\", true,\n-                        place.ptr.assert_ptr(), size\n-                    );\n+                    memory.load_relaxed(clocks, index)?;\n                 }\n-                let maybe_race = if release {\n-                    // Atomic RW-Op release\n-                    range.rmw_release(&mut *current_state, current_thread)\n+                if release {\n+                    memory.rmw_release(clocks, index)\n                 }else{\n-                    range.rmw_relaxed(&mut *current_state, current_thread)\n-                };\n-                if maybe_race == Err(DataRace) {\n-                    mem::drop(current_state);\n-                    return VClockAlloc::report_data_race(\n-                        &alloc.global, range, \"ATOMIC_RMW(STORE)\", true,\n-                        place.ptr.assert_ptr(), size\n-                    );\n+                    memory.rmw_relaxed(clocks, index)\n                 }\n             }\n-\n-            // Log changes to atomic memory\n-            if log::log_enabled!(log::Level::Trace) {\n-                for (_,range) in alloc.alloc_ranges.get_mut().iter(offset, size) {\n-                    log::trace!(\n-                        \"  updated atomic memory({:?}, offset={}, size={}) to {:#?}\",\n-                        place.ptr.assert_ptr().alloc_id, offset.bytes(), size.bytes(),\n-                        range.atomic_ops\n-                    );\n-                }\n-            }\n-\n-            mem::drop(current_state);\n-            let data_race = &*this.memory.extra.data_race;\n-            data_race.advance_vector_clock();\n-        }\n-        Ok(())\n+        )\n     }\n \n     /// Update the data-race detector for an atomic fence on the current thread\n     fn validate_atomic_fence(&mut self, atomic: AtomicFenceOp) -> InterpResult<'tcx> {\n         let this = self.eval_context_mut();\n         let data_race = &*this.memory.extra.data_race;\n-        if data_race.multi_threaded.get() {\n-            data_race.advance_vector_clock();\n-\n-            log::trace!(\"Atomic fence on {:?} with ordering {:?}\", data_race.current_thread(), atomic);\n+        data_race.maybe_perform_sync_operation(move |index, mut clocks| {\n+            log::trace!(\"Atomic fence on {:?} with ordering {:?}\", index, atomic);\n             // Apply data-race detection for the current fences\n             //  this treats AcqRel and SeqCst as the same as a acquire\n             //  and release fence applied in the same timestamp.\n             if atomic != AtomicFenceOp::Release {\n                 // Either Acquire | AcqRel | SeqCst\n-                data_race.current_thread_state_mut().apply_acquire_fence();\n+                clocks.apply_acquire_fence();\n             }\n             if atomic != AtomicFenceOp::Acquire {\n                 // Either Release | AcqRel | SeqCst\n-                data_race.current_thread_state_mut().apply_release_fence();\n+                clocks.apply_release_fence();\n             }\n+            Ok(())\n+        })\n+    }\n+}\n \n-            data_race.advance_vector_clock();\n+impl<'mir, 'tcx: 'mir> EvalContextPrivExt<'mir, 'tcx> for MiriEvalContext<'mir, 'tcx> {}\n+trait EvalContextPrivExt<'mir, 'tcx: 'mir>: MiriEvalContextExt<'mir, 'tcx> {\n+\n+    /// Generic atomic operation implementation, this however\n+    ///  cannot be used for the atomic read operation since\n+    ///  that requires non mutable memory access to not trigger\n+    ///  the writing to read-only memory errors during `get_raw_mut`\n+    fn validate_atomic_op_mut<A: Debug + Copy>(\n+        &mut self, place: MPlaceTy<'tcx, Tag>,\n+        atomic: A, description: &str,\n+        mut op: impl FnMut(\n+            &mut MemoryCellClocks, &mut ThreadClockSet, VectorIdx, A\n+        ) -> Result<(), DataRace>\n+    ) -> InterpResult<'tcx> {\n+        let this = self.eval_context_mut();\n+        let data_race = &*this.memory.extra.data_race;\n+        if data_race.multi_threaded.get() {\n+\n+            // Load an log the atomic operation\n+            let place_ptr = place.ptr.assert_ptr();\n+            let size = place.layout.size;\n+            let alloc_meta =  &mut this.memory.get_raw_mut(place_ptr.alloc_id)?.extra.data_race;\n+            log::trace!(\n+                \"Atomic op({}) with ordering {:?} on memory({:?}, offset={}, size={})\",\n+                description, &atomic, place_ptr.alloc_id, place_ptr.offset.bytes(), size.bytes()\n+            );\n+\n+            // Perform the atomic operation\n+            let data_race = &alloc_meta.global;\n+            data_race.maybe_perform_sync_operation(|index, mut clocks| {\n+                for (_,range) in alloc_meta.alloc_ranges.borrow_mut().iter_mut(place_ptr.offset, size) {\n+                    if let Err(DataRace) = op(range, &mut *clocks, index, atomic) {\n+                        mem::drop(clocks);\n+                        return VClockAlloc::report_data_race(\n+                            &alloc_meta.global, range, description, true,\n+                            place_ptr, size\n+                        );\n+                    }\n+                }\n+                Ok(())\n+            })?;\n+\n+            // Log changes to atomic memory\n+            if log::log_enabled!(log::Level::Trace) {\n+                for (_,range) in alloc_meta.alloc_ranges.borrow().iter(place_ptr.offset, size) {\n+                    log::trace!(\n+                        \"Updated atomic memory({:?}, offset={}, size={}) to {:#?}\",\n+                        place.ptr.assert_ptr().alloc_id, place_ptr.offset.bytes(), size.bytes(),\n+                        range.atomic_ops\n+                    );\n+                }\n+            }\n         }\n         Ok(())\n     }\n+\n }\n \n /// Handle for locks to express their\n@@ -439,134 +343,14 @@ pub struct DataRaceLockHandle {\n }\n impl DataRaceLockHandle {\n     pub fn set_values(&mut self, other: &Self) {\n-        self.clock.set_values(&other.clock)\n+        self.clock.clone_from(&other.clock)\n     }\n     pub fn reset(&mut self) {\n         self.clock.set_zero_vector();\n     }\n }\n \n \n-/// Avoid an atomic allocation for the common\n-///  case with atomic operations where the number\n-///  of active release sequences is small\n-#[derive(Clone, PartialEq, Eq)]\n-enum AtomicReleaseSequences {\n-\n-    /// Contains one or no values\n-    ///  if empty: (None, reset vector clock)\n-    ///  if one:   (Some(thread), thread_clock)\n-    ReleaseOneOrEmpty(Option<ThreadId>, VClock),\n-\n-    /// Contains two or more values\n-    ///  stored in a hash-map of thread id to\n-    ///  vector clocks\n-    ReleaseMany(FxHashMap<ThreadId, VClock>)\n-}\n-impl AtomicReleaseSequences {\n-\n-    /// Return an empty set of atomic release sequences\n-    #[inline]\n-    fn new() -> AtomicReleaseSequences {\n-        Self::ReleaseOneOrEmpty(None, VClock::default())\n-    }\n-\n-    /// Remove all values except for the value stored at `thread` and set\n-    ///  the vector clock to the associated `clock` value\n-    #[inline]\n-    fn clear_and_set(&mut self, thread: ThreadId, clock: &VClock) {\n-        match self {\n-            Self::ReleaseOneOrEmpty(id, rel_clock) => {\n-                *id = Some(thread);\n-                rel_clock.set_values(clock);\n-            }\n-            Self::ReleaseMany(_) => {\n-                *self = Self::ReleaseOneOrEmpty(Some(thread), clock.clone());\n-            }\n-        }\n-    }\n-\n-    /// Remove all values except for the value stored at `thread`\n-    #[inline]\n-    fn clear_and_retain(&mut self, thread: ThreadId) {\n-        match self {\n-            Self::ReleaseOneOrEmpty(id, rel_clock) => {\n-                // If the id is the same, then reatin the value\n-                //  otherwise delete and clear the release vector clock\n-                if *id != Some(thread) {\n-                    *id = None;\n-                    rel_clock.set_zero_vector();\n-                }\n-            },\n-            Self::ReleaseMany(hash_map) => {\n-                // Retain only the thread element, so reduce to size\n-                //  of 1 or 0, and move to smaller format\n-                if let Some(clock) = hash_map.remove(&thread) {\n-                    *self = Self::ReleaseOneOrEmpty(Some(thread), clock);\n-                }else{\n-                    *self = Self::new();\n-                }\n-            }\n-        }\n-    }\n-\n-    /// Insert a release sequence at `thread` with values `clock`\n-    fn insert(&mut self, thread: ThreadId, clock: &VClock) {\n-        match self {\n-            Self::ReleaseOneOrEmpty(id, rel_clock) => {\n-                if id.map_or(true, |id| id == thread) {\n-                    *id = Some(thread);\n-                    rel_clock.set_values(clock);\n-                }else{\n-                    let mut hash_map = FxHashMap::default();\n-                    hash_map.insert(thread, clock.clone());\n-                    hash_map.insert(id.unwrap(), rel_clock.clone());\n-                    *self = Self::ReleaseMany(hash_map);\n-                }\n-            },\n-            Self::ReleaseMany(hash_map) => {\n-                hash_map.insert(thread, clock.clone());\n-            }\n-        }\n-    }\n-\n-    /// Return the release sequence at `thread` if one exists\n-    #[inline]\n-    fn load(&self, thread: ThreadId) -> Option<&VClock> {\n-        match self {\n-            Self::ReleaseOneOrEmpty(id, clock) => {\n-                if *id == Some(thread) {\n-                    Some(clock)\n-                }else{\n-                    None\n-                }\n-            },\n-            Self::ReleaseMany(hash_map) => {\n-                hash_map.get(&thread)\n-            }\n-        }\n-    }\n-}\n-\n-/// Custom debug implementation to correctly\n-///  print debug as a logical mapping from threads\n-///  to vector-clocks\n-impl Debug for AtomicReleaseSequences {\n-    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {\n-        match self {\n-            Self::ReleaseOneOrEmpty(None,_) => {\n-                f.debug_map().finish()\n-            },\n-            Self::ReleaseOneOrEmpty(Some(id), clock) => {\n-                f.debug_map().entry(&id, &clock).finish()\n-            },\n-            Self::ReleaseMany(hash_map) => {\n-                Debug::fmt(hash_map, f)\n-            }\n-        }\n-    }\n-}\n-\n /// Error returned by finding a data race\n ///  should be elaborated upon\n #[derive(Copy, Clone, PartialEq, Eq, PartialOrd, Ord, Hash, Debug)]\n@@ -576,7 +360,7 @@ pub struct DataRace;\n ///  explicitly to reduce memory usage for the\n ///  common case where no atomic operations\n ///  exists on the memory cell\n-#[derive(Clone, PartialEq, Eq, Debug)]\n+#[derive(Clone, PartialEq, Eq, Default, Debug)]\n struct AtomicMemoryCellClocks {\n \n     /// The clock-vector for the set of atomic read operations\n@@ -599,7 +383,7 @@ struct AtomicMemoryCellClocks {\n     ///  sequence exists in the memory cell, required\n     ///  since read-modify-write operations do not\n     ///  invalidate existing release sequences \n-    release_sequences: AtomicReleaseSequences,\n+    release_sequences: VSmallClockSet,\n }\n \n /// Memory Cell vector clock metadata\n@@ -609,11 +393,11 @@ struct MemoryCellClocks {\n \n     /// The vector-clock of the last write, only one value is stored\n     ///  since all previous writes happened-before the current write\n-    write: Timestamp,\n+    write: VTimestamp,\n \n     /// The identifier of the thread that performed the last write\n     ///  operation\n-    write_thread: ThreadId,\n+    write_index: VectorIdx,\n \n     /// The vector-clock of the set of previous reads\n     ///  each index is set to the timestamp that the associated\n@@ -633,7 +417,7 @@ impl Default for MemoryCellClocks {\n         MemoryCellClocks {\n             read: VClock::default(),\n             write: 0,\n-            write_thread: ThreadId::new(u32::MAX as usize),\n+            write_index: VectorIdx::MAX_INDEX,\n             atomic_ops: None\n         }\n     }\n@@ -654,21 +438,14 @@ impl MemoryCellClocks {\n     ///  if it does not exist\n     #[inline]\n     fn atomic_mut(&mut self) -> &mut AtomicMemoryCellClocks {\n-        self.atomic_ops.get_or_insert_with(|| {\n-            Box::new(AtomicMemoryCellClocks {\n-                read_vector: VClock::default(),\n-                write_vector: VClock::default(),\n-                sync_vector: VClock::default(),\n-                release_sequences: AtomicReleaseSequences::new()\n-            })\n-        })\n+        self.atomic_ops.get_or_insert_with(Default::default)\n     }\n \n     /// Update memory cell data-race tracking for atomic\n     ///  load acquire semantics, is a no-op if this memory was\n     ///  not used previously as atomic memory\n-    fn acquire(&mut self, clocks: &mut ThreadClockSet, thread: ThreadId) -> Result<(), DataRace> {\n-        self.atomic_read_detect(clocks, thread)?;\n+    fn acquire(&mut self, clocks: &mut ThreadClockSet, index: VectorIdx) -> Result<(), DataRace> {\n+        self.atomic_read_detect(clocks, index)?;\n         if let Some(atomic) = self.atomic() {\n             clocks.clock.join(&atomic.sync_vector);\n         }\n@@ -677,8 +454,8 @@ impl MemoryCellClocks {\n     /// Update memory cell data-race tracking for atomic\n     ///  load relaxed semantics, is a no-op if this memory was\n     ///  not used previously as atomic memory\n-    fn load_relaxed(&mut self, clocks: &mut ThreadClockSet, thread: ThreadId) -> Result<(), DataRace> {\n-        self.atomic_read_detect(clocks, thread)?;\n+    fn load_relaxed(&mut self, clocks: &mut ThreadClockSet, index: VectorIdx) -> Result<(), DataRace> {\n+        self.atomic_read_detect(clocks, index)?;\n         if let Some(atomic) = self.atomic() {\n             clocks.fence_acquire.join(&atomic.sync_vector);\n         }\n@@ -688,50 +465,51 @@ impl MemoryCellClocks {\n \n     /// Update the memory cell data-race tracking for atomic\n     ///  store release semantics\n-    fn release(&mut self, clocks: &ThreadClockSet, thread: ThreadId) -> Result<(), DataRace> {\n-        self.atomic_write_detect(clocks, thread)?;\n+    fn release(&mut self, clocks: &ThreadClockSet, index: VectorIdx) -> Result<(), DataRace> {\n+        self.atomic_write_detect(clocks, index)?;\n         let atomic = self.atomic_mut();\n-        atomic.sync_vector.set_values(&clocks.clock);\n-        atomic.release_sequences.clear_and_set(thread, &clocks.clock);\n+        atomic.sync_vector.clone_from(&clocks.clock);\n+        atomic.release_sequences.clear();\n+        atomic.release_sequences.insert(index, &clocks.clock);\n         Ok(())\n     }\n     /// Update the memory cell data-race tracking for atomic\n     ///  store relaxed semantics\n-    fn store_relaxed(&mut self, clocks: &ThreadClockSet, thread: ThreadId) -> Result<(), DataRace> {\n-        self.atomic_write_detect(clocks, thread)?;\n+    fn store_relaxed(&mut self, clocks: &ThreadClockSet, index: VectorIdx) -> Result<(), DataRace> {\n+        self.atomic_write_detect(clocks, index)?;\n         let atomic = self.atomic_mut();\n-        atomic.sync_vector.set_values(&clocks.fence_release);\n-        if let Some(release) = atomic.release_sequences.load(thread) {\n+        atomic.sync_vector.clone_from(&clocks.fence_release);\n+        if let Some(release) = atomic.release_sequences.get(index) {\n             atomic.sync_vector.join(release);\n         }\n-        atomic.release_sequences.clear_and_retain(thread);\n+        atomic.release_sequences.retain_index(index);\n         Ok(())\n     }\n     /// Update the memory cell data-race tracking for atomic\n     ///  store release semantics for RMW operations\n-    fn rmw_release(&mut self, clocks: &ThreadClockSet, thread: ThreadId) -> Result<(), DataRace> {\n-        self.atomic_write_detect(clocks, thread)?;\n+    fn rmw_release(&mut self, clocks: &ThreadClockSet, index: VectorIdx) -> Result<(), DataRace> {\n+        self.atomic_write_detect(clocks, index)?;\n         let atomic = self.atomic_mut();\n         atomic.sync_vector.join(&clocks.clock);\n-        atomic.release_sequences.insert(thread, &clocks.clock);\n+        atomic.release_sequences.insert(index, &clocks.clock);\n         Ok(())\n     }\n     /// Update the memory cell data-race tracking for atomic\n     ///  store relaxed semantics for RMW operations\n-    fn rmw_relaxed(&mut self, clocks: &ThreadClockSet, thread: ThreadId) -> Result<(), DataRace> {\n-        self.atomic_write_detect(clocks, thread)?;\n+    fn rmw_relaxed(&mut self, clocks: &ThreadClockSet, index: VectorIdx) -> Result<(), DataRace> {\n+        self.atomic_write_detect(clocks, index)?;\n         let atomic = self.atomic_mut();\n         atomic.sync_vector.join(&clocks.fence_release);\n         Ok(())\n     }\n     \n     /// Detect data-races with an atomic read, caused by a non-atomic write that does\n     ///  not happen-before the atomic-read\n-    fn atomic_read_detect(&mut self, clocks: &ThreadClockSet, thread: ThreadId) -> Result<(), DataRace> {\n+    fn atomic_read_detect(&mut self, clocks: &ThreadClockSet, index: VectorIdx) -> Result<(), DataRace> {\n         log::trace!(\"Atomic read with vectors: {:#?} :: {:#?}\", self, clocks);\n-        if self.write <= clocks.clock[self.write_thread] {\n+        if self.write <= clocks.clock[self.write_index] {\n             let atomic = self.atomic_mut();\n-            atomic.read_vector.set_at_thread(&clocks.clock, thread);\n+            atomic.read_vector.set_at_index(&clocks.clock, index);\n             Ok(())\n         }else{\n             Err(DataRace)\n@@ -740,11 +518,11 @@ impl MemoryCellClocks {\n \n     /// Detect data-races with an atomic write, either with a non-atomic read or with\n     ///  a non-atomic write:\n-    fn atomic_write_detect(&mut self, clocks: &ThreadClockSet, thread: ThreadId) -> Result<(), DataRace> {\n+    fn atomic_write_detect(&mut self, clocks: &ThreadClockSet, index: VectorIdx) -> Result<(), DataRace> {\n         log::trace!(\"Atomic write with vectors: {:#?} :: {:#?}\", self, clocks);\n-        if self.write <= clocks.clock[self.write_thread] && self.read <= clocks.clock {\n+        if self.write <= clocks.clock[self.write_index] && self.read <= clocks.clock {\n             let atomic = self.atomic_mut();\n-            atomic.write_vector.set_at_thread(&clocks.clock, thread);\n+            atomic.write_vector.set_at_index(&clocks.clock, index);\n             Ok(())\n         }else{\n             Err(DataRace)\n@@ -753,16 +531,16 @@ impl MemoryCellClocks {\n \n     /// Detect races for non-atomic read operations at the current memory cell\n     ///  returns true if a data-race is detected\n-    fn read_race_detect(&mut self, clocks: &ThreadClockSet, thread: ThreadId) -> Result<(), DataRace> {\n+    fn read_race_detect(&mut self, clocks: &ThreadClockSet, index: VectorIdx) -> Result<(), DataRace> {\n         log::trace!(\"Unsynchronized read with vectors: {:#?} :: {:#?}\", self, clocks);\n-        if self.write <= clocks.clock[self.write_thread] {\n+        if self.write <= clocks.clock[self.write_index] {\n             let race_free = if let Some(atomic) = self.atomic() {\n                 atomic.write_vector <= clocks.clock\n             }else{\n                 true\n             };\n             if race_free {\n-                self.read.set_at_thread(&clocks.clock, thread);\n+                self.read.set_at_index(&clocks.clock, index);\n                 Ok(())\n             }else{\n                 Err(DataRace)\n@@ -774,17 +552,17 @@ impl MemoryCellClocks {\n \n     /// Detect races for non-atomic write operations at the current memory cell\n     ///  returns true if a data-race is detected\n-    fn write_race_detect(&mut self, clocks: &ThreadClockSet, thread: ThreadId)  -> Result<(), DataRace> {\n+    fn write_race_detect(&mut self, clocks: &ThreadClockSet, index: VectorIdx)  -> Result<(), DataRace> {\n         log::trace!(\"Unsynchronized write with vectors: {:#?} :: {:#?}\", self, clocks);\n-        if self.write <= clocks.clock[self.write_thread] && self.read <= clocks.clock {\n+        if self.write <= clocks.clock[self.write_index] && self.read <= clocks.clock {\n             let race_free = if let Some(atomic) = self.atomic() {\n                 atomic.write_vector <= clocks.clock && atomic.read_vector <= clocks.clock\n             }else{\n                 true\n             };\n             if race_free {\n-                self.write = clocks.clock[thread];\n-                self.write_thread = thread;\n+                self.write = clocks.clock[index];\n+                self.write_index = index;\n                 self.read.set_zero_vector();\n                 Ok(())\n             }else{\n@@ -822,7 +600,7 @@ impl VClockAlloc {\n \n     // Find an index, if one exists where the value\n     //  in `l` is greater than the value in `r`\n-    fn find_gt_index(l: &VClock, r: &VClock) -> Option<usize> {\n+    fn find_gt_index(l: &VClock, r: &VClock) -> Option<VectorIdx> {\n         let l_slice = l.as_slice();\n         let r_slice = r.as_slice();\n         l_slice.iter().zip(r_slice.iter())\n@@ -844,7 +622,7 @@ impl VClockAlloc {\n                 }else{\n                     None\n                 }\n-            })\n+            }).map(|idx| VectorIdx::new(idx))\n     }\n \n     /// Report a data-race found in the program\n@@ -859,35 +637,29 @@ impl VClockAlloc {\n         action: &str, is_atomic: bool,\n         pointer: Pointer<Tag>, len: Size\n     ) -> InterpResult<'tcx> {\n-        let current_thread = global.current_thread();\n-        let current_state = global.current_thread_state();\n-        let mut write_clock = VClock::default();\n+        let (current_index, current_clocks) = global.current_thread_state();\n+        let write_clock;\n         let (\n             other_action, other_thread, other_clock\n-        ) = if range.write > current_state.clock[range.write_thread] {\n-\n+        ) = if range.write > current_clocks.clock[range.write_index] {\n             // Convert the write action into the vector clock it\n             //  represents for diagnostic purposes\n-            let wclock = write_clock.get_mut_with_min_len(\n-                current_state.clock.as_slice().len()\n-                .max(range.write_thread.to_u32() as usize + 1)\n-            );\n-            wclock[range.write_thread.to_u32() as usize] = range.write;\n-            (\"WRITE\", range.write_thread, write_clock.as_slice())\n+            write_clock = VClock::new_with_index(range.write_index, range.write);\n+            (\"WRITE\", range.write_index, &write_clock)\n         }else if let Some(idx) = Self::find_gt_index(\n-            &range.read, &current_state.clock\n+            &range.read, &current_clocks.clock\n         ){\n-            (\"READ\", ThreadId::new(idx), range.read.as_slice())\n+            (\"READ\", idx, &range.read)\n         }else if !is_atomic {\n             if let Some(atomic) = range.atomic() {\n                 if let Some(idx) = Self::find_gt_index(\n-                    &atomic.write_vector, &current_state.clock\n+                    &atomic.write_vector, &current_clocks.clock\n                 ) {\n-                    (\"ATOMIC_STORE\", ThreadId::new(idx), atomic.write_vector.as_slice())\n+                    (\"ATOMIC_STORE\", idx, &atomic.write_vector)\n                 }else if let Some(idx) = Self::find_gt_index(\n-                    &atomic.read_vector, &current_state.clock\n+                    &atomic.read_vector, &current_clocks.clock\n                 ) {\n-                    (\"ATOMIC_LOAD\", ThreadId::new(idx), atomic.read_vector.as_slice())\n+                    (\"ATOMIC_LOAD\", idx, &atomic.read_vector)\n                 }else{\n                     unreachable!(\"Failed to find report data-race for non-atomic operation: no race found\")\n                 }\n@@ -899,7 +671,7 @@ impl VClockAlloc {\n         };\n \n         // Load elaborated thread information about the racing thread actions\n-        let current_thread_info = global.print_thread_metadata(current_thread);\n+        let current_thread_info = global.print_thread_metadata(current_index);\n         let other_thread_info = global.print_thread_metadata(other_thread);\n         \n         // Throw the data-race detection\n@@ -910,7 +682,7 @@ impl VClockAlloc {\n             action, current_thread_info, \n             other_action, other_thread_info,\n             pointer.alloc_id, pointer.offset.bytes(), len.bytes(),\n-            current_state.clock,\n+            current_clocks.clock,\n             other_clock\n         )\n     }\n@@ -921,14 +693,10 @@ impl VClockAlloc {\n     ///  operation\n     pub fn read<'tcx>(&self, pointer: Pointer<Tag>, len: Size) -> InterpResult<'tcx> {\n         if self.global.multi_threaded.get() {\n-            let current_thread = self.global.current_thread();\n-            let current_state = self.global.current_thread_state();\n-\n-            // The alloc-ranges are not split, however changes are not going to be made\n-            //  to the ranges being tested, so this is ok\n+            let (index, clocks) = self.global.current_thread_state();\n             let mut alloc_ranges = self.alloc_ranges.borrow_mut();\n             for (_,range) in alloc_ranges.iter_mut(pointer.offset, len) {\n-                if range.read_race_detect(&*current_state, current_thread) == Err(DataRace) {\n+                if range.read_race_detect(&*clocks, index) == Err(DataRace) {\n                     // Report data-race\n                     return Self::report_data_race(\n                         &self.global,range, \"READ\", false, pointer, len\n@@ -946,10 +714,9 @@ impl VClockAlloc {\n     ///  operation\n     pub fn write<'tcx>(&mut self, pointer: Pointer<Tag>, len: Size) -> InterpResult<'tcx> {\n         if self.global.multi_threaded.get() {\n-            let current_thread = self.global.current_thread();\n-            let current_state = self.global.current_thread_state();\n+            let (index, clocks) = self.global.current_thread_state();\n             for (_,range) in self.alloc_ranges.get_mut().iter_mut(pointer.offset, len) {\n-                if range.write_race_detect(&*current_state, current_thread) == Err(DataRace) {\n+                if range.write_race_detect(&*clocks, index) == Err(DataRace) {\n                     // Report data-race\n                     return Self::report_data_race(\n                         &self.global, range, \"WRITE\", false, pointer, len\n@@ -967,10 +734,9 @@ impl VClockAlloc {\n     ///  operation\n     pub fn deallocate<'tcx>(&mut self, pointer: Pointer<Tag>, len: Size) -> InterpResult<'tcx> {\n         if self.global.multi_threaded.get() {\n-            let current_thread = self.global.current_thread();\n-            let current_state = self.global.current_thread_state();\n+            let (index, clocks) = self.global.current_thread_state();\n             for (_,range) in self.alloc_ranges.get_mut().iter_mut(pointer.offset, len) {\n-                if range.write_race_detect(&*current_state, current_thread) == Err(DataRace) {\n+                if range.write_race_detect(&*clocks, index) == Err(DataRace) {\n                     // Report data-race\n                     return Self::report_data_race(\n                         &self.global, range, \"DEALLOCATE\", false, pointer, len\n@@ -989,6 +755,7 @@ impl VClockAlloc {\n ///  additional metadata to model atomic fence operations\n #[derive(Clone, Default, Debug)]\n struct ThreadClockSet {\n+\n     /// The increasing clock representing timestamps\n     ///  that happen-before this thread.\n     clock: VClock,\n@@ -1008,7 +775,7 @@ impl ThreadClockSet {\n     ///  set of thread vector clocks\n     #[inline]\n     fn apply_release_fence(&mut self) {\n-        self.fence_release.set_values(&self.clock);\n+        self.fence_release.clone_from(&self.clock);\n     }\n \n     /// Apply the effects of a acquire fence to this\n@@ -1021,8 +788,8 @@ impl ThreadClockSet {\n     /// Increment the happens-before clock at a\n     ///  known index\n     #[inline]\n-    fn increment_clock(&mut self, thread: ThreadId) {\n-        self.clock.increment_thread(thread);\n+    fn increment_clock(&mut self, index: VectorIdx) {\n+        self.clock.increment_index(index);\n     }\n \n     /// Join the happens-before clock with that of\n@@ -1047,123 +814,230 @@ pub struct GlobalState {\n     ///  any data-races\n     multi_threaded: Cell<bool>,\n \n-    /// The current vector clock for all threads\n-    ///  this includes threads that have terminated\n-    ///  execution\n-    thread_clocks: RefCell<IndexVec<ThreadId, ThreadClockSet>>,\n+    /// Mapping of a vector index to a known set of thread\n+    ///  clocks, this is not directly mapping from a thread id\n+    ///  since it may refer to multiple threads\n+    vector_clocks: RefCell<IndexVec<VectorIdx, ThreadClockSet>>,\n+\n+    /// Mapping of a given vector index to the current thread\n+    ///  that the execution is representing, this may change\n+    ///  if a vector index is re-assigned to a new thread\n+    vector_info: RefCell<IndexVec<VectorIdx, ThreadId>>, //FIXME: make option\n+\n+    /// The mapping of a given thread to a known vector clock\n+    thread_info: RefCell<IndexVec<ThreadId, (Option<VectorIdx>, Option<Box<str>>)>>,\n \n-    /// Thread name cache for better diagnostics on the reporting\n-    ///  of a data-race\n-    thread_names: RefCell<IndexVec<ThreadId, Option<Box<str>>>>,\n+    /// The current vector index being executed\n+    current_index: Cell<VectorIdx>,\n \n-    /// The current thread being executed,\n-    ///  this is mirrored from the scheduler since\n-    ///  it is required for loading the current vector\n-    ///  clock for data-race detection\n-    current_thread_id: Cell<ThreadId>,\n+    /// Potential vector indices that could be re-used on thread creation\n+    ///  values are inserted here on thread join events, and can be\n+    ///  re-used once the vector clocks of all current threads\n+    ///  are equal to the vector clock of the joined thread\n+    reuse_candidates: RefCell<FxHashSet<VectorIdx>>,\n }\n impl GlobalState {\n \n     /// Create a new global state, setup with just thread-id=0\n     ///  advanced to timestamp = 1\n     pub fn new() -> Self {\n-        let mut vec = IndexVec::new();\n-        let thread_id = vec.push(ThreadClockSet::default());\n-        vec[thread_id].increment_clock(thread_id);\n-        GlobalState {\n+        let global_state = GlobalState {\n             multi_threaded: Cell::new(false),\n-            thread_clocks: RefCell::new(vec),\n-            thread_names: RefCell::new(IndexVec::new()),\n-            current_thread_id: Cell::new(thread_id),\n-        }\n+            vector_clocks: RefCell::new(IndexVec::new()),\n+            vector_info: RefCell::new(IndexVec::new()),\n+            thread_info: RefCell::new(IndexVec::new()),\n+            current_index: Cell::new(VectorIdx::new(0)),\n+            reuse_candidates: RefCell::new(FxHashSet::default()),\n+        };\n+\n+        // Setup the main-thread since it is not explicitly created:\n+        //  uses vector index and thread-id 0, also the rust runtime gives\n+        //  the main-thread a name of \"main\".\n+        let index = global_state.vector_clocks.borrow_mut().push(ThreadClockSet::default());\n+        global_state.vector_info.borrow_mut().push(ThreadId::new(0));\n+        global_state.thread_info.borrow_mut().push(\n+            (Some(index), Some(\"main\".to_string().into_boxed_str())\n+        ));\n+\n+        global_state\n     }\n     \n+    // Try to find vector index values that can potentially be re-used\n+    //  by a new thread instead of a new vector index being created\n+    fn find_vector_index_reuse_candidate(&self) -> Option<VectorIdx> {\n+        let mut reuse = self.reuse_candidates.borrow_mut();\n+        let vector_clocks = self.vector_clocks.borrow();\n+        for  &candidate in reuse.iter() {\n+            let target_timestamp = vector_clocks[candidate].clock[candidate];\n+            if vector_clocks.iter().all(|clock| {\n+                clock.clock[candidate] == target_timestamp\n+            }) {\n+                // All vector clocks for each vector index are equal to\n+                //  the target timestamp, therefore since the thread has\n+                //  terminated and cannot update the vector clock.\n+                // No more data-races involving this vector index are possible\n+                //  so it can be re-used\n+                assert!(reuse.remove(&candidate));\n+                return Some(candidate)\n+            }\n+        }\n+        None\n+    }\n \n     // Hook for thread creation, enabled multi-threaded execution and marks\n     //  the current thread timestamp as happening-before the current thread\n     #[inline]\n     pub fn thread_created(&self, thread: ThreadId) {\n+        let current_index = self.current_index();\n \n-        // Enable multi-threaded execution mode now that there are at least\n-        //  two threads\n+        // Enable multi-threaded execution, there are now two threads\n+        //  so data-races are now possible.\n         self.multi_threaded.set(true);\n-        let current_thread = self.current_thread_id.get();\n-        let mut vectors = self.thread_clocks.borrow_mut();\n-        vectors.ensure_contains_elem(thread, Default::default);\n-        let (current, created) = vectors.pick2_mut(current_thread, thread);\n \n-        // Pre increment clocks before atomic operation\n-        current.increment_clock(current_thread);\n+        // Load and setup the associated thread metadata\n+        let mut thread_info = self.thread_info.borrow_mut();\n+        thread_info.ensure_contains_elem(thread, Default::default);\n+\n+        // Assign a vector index for the thread, attempting to re-use an old\n+        //  vector index that can no longer report any data-races if possible\n+        let created_index = if let Some(\n+            reuse_index\n+        ) = self.find_vector_index_reuse_candidate() {\n+            // Now re-configure the re-use candidate, increment the clock\n+            //  for the new sync use of the vector\n+            let mut vector_clocks = self.vector_clocks.borrow_mut();\n+            vector_clocks[reuse_index].increment_clock(reuse_index);\n+\n+            // Locate the old thread the vector was associated with and update\n+            //  it to represent the new thread instead\n+            let mut vector_info = self.vector_info.borrow_mut();\n+            let old_thread = vector_info[reuse_index];\n+            vector_info[reuse_index] = thread;\n+\n+            // Mark the thread the vector index was associated with as no longer\n+            //  representing a thread index\n+            thread_info[old_thread].0 = None;\n+\n+            reuse_index\n+        }else{\n+            // No vector re-use candidates available, instead create\n+            //  a new vector index\n+            let mut vector_info = self.vector_info.borrow_mut();\n+            vector_info.push(thread)\n+        };\n+\n+        // Mark the chosen vector index as in use by the thread\n+        thread_info[thread].0 = Some(created_index);\n+\n+        // Create a thread clock set if applicable\n+        let mut vector_clocks = self.vector_clocks.borrow_mut();\n+        if created_index == vector_clocks.next_index() {\n+            vector_clocks.push(ThreadClockSet::default());\n+        }\n \n-        // The current thread happens-before the created thread\n-        //  so update the created vector clock\n+        // Now load the two clocks and configure the initial state\n+        let (current, created) = vector_clocks.pick2_mut(current_index, created_index);\n+\n+        // Advance the current thread before the synchronized operation\n+        current.increment_clock(current_index);\n+\n+        // Join the created with current, since the current threads\n+        //  previous actions happen-before the created thread\n         created.join_with(current);\n \n-        // Post increment clocks after atomic operation\n-        current.increment_clock(current_thread);\n-        created.increment_clock(thread);\n+        // Advance both threads after the synchronized operation\n+        current.increment_clock(current_index);\n+        created.increment_clock(created_index);\n     }\n \n     /// Hook on a thread join to update the implicit happens-before relation\n-    ///  between the joined thead and the current thread\n+    ///  between the joined thead and the current thread.\n+    /// Called after the join has occured, and hence implicitly also states\n+    ///  that the thread must have terminated as well\n     #[inline]\n     pub fn thread_joined(&self, current_thread: ThreadId, join_thread: ThreadId) {\n-        let mut vectors = self.thread_clocks.borrow_mut();\n-        let (current, join) = vectors.pick2_mut(current_thread, join_thread);\n+        let (current_index, join_index) = {\n+            let thread_info = self.thread_info.borrow();\n+            let current_index = thread_info[current_thread].0\n+                .expect(\"Joining into thread with no assigned vector\");\n+            let join_index = thread_info[join_thread].0\n+                .expect(\"Joining thread with no assigned vector\");\n+            (current_index, join_index)\n+        };\n+        let mut clocks_vec = self.vector_clocks.borrow_mut();\n+        let (current, join) = clocks_vec.pick2_mut(current_index, join_index);\n \n         // Pre increment clocks before atomic operation\n-        current.increment_clock(current_thread);\n-        join.increment_clock(join_thread);\n+        current.increment_clock(current_index);\n+        join.increment_clock(join_index);\n \n         // The join thread happens-before the current thread\n         //   so update the current vector clock\n         current.join_with(join);\n \n         // Post increment clocks after atomic operation\n-        current.increment_clock(current_thread);\n-        join.increment_clock(join_thread);\n+        current.increment_clock(current_index);\n+        join.increment_clock(join_index);\n+\n+        // The joined thread vector clock is a potential candidate\n+        //  for re-use given sufficient time, mark as available once\n+        //  threads have been created. This is because this function\n+        //  is called once join_thread has terminated and such cannot\n+        //  update any-more\n+        let mut reuse = self.reuse_candidates.borrow_mut();\n+        reuse.insert(join_index);\n     }\n \n     /// Hook for updating the local tracker of the currently\n     ///  enabled thread, should always be updated whenever\n     ///  `active_thread` in thread.rs is updated\n     #[inline]\n     pub fn thread_set_active(&self, thread: ThreadId) {\n-        self.current_thread_id.set(thread);\n+        let thread_info = self.thread_info.borrow();\n+        let vector_idx = thread_info[thread].0\n+            .expect(\"Setting thread active with no assigned vector\");\n+        self.current_index.set(vector_idx);\n     }\n \n     /// Hook for updating the local tracker of the threads name\n     ///  this should always mirror the local value in thread.rs\n     ///  the thread name is used for improved diagnostics\n     ///  during a data-race\n     #[inline]\n-    pub fn thread_set_name(&self, name: String) {\n+    pub fn thread_set_name(&self, thread: ThreadId, name: String) {\n         let name = name.into_boxed_str();\n-        let mut names = self.thread_names.borrow_mut();\n-        let thread = self.current_thread_id.get();\n-        names.ensure_contains_elem(thread, Default::default);\n-        names[thread] = Some(name);\n+        let mut thread_info = self.thread_info.borrow_mut();\n+        thread_info[thread].1 = Some(name);\n     }\n \n \n-    /// Advance the vector clock for a thread\n-    ///  this is called before and after any atomic/synchronizing operations\n-    ///  that may manipulate state\n-    #[inline]\n-    fn advance_vector_clock(&self) {\n-        let thread = self.current_thread_id.get();\n-        let mut vectors = self.thread_clocks.borrow_mut();\n-        vectors[thread].increment_clock(thread);\n-\n-        // Log the increment in the atomic vector clock\n-        log::trace!(\"Atomic vector clock increase for {:?} to {:?}\",thread, vectors[thread].clock);\n+    /// Attempt to perform a synchronized operation, this\n+    ///  will perform no operation if multi-threading is\n+    ///  not currently enabled.\n+    /// Otherwise it will increment the clock for the current\n+    ///  vector before and after the operation for data-race\n+    ///  detection between any happens-before edges the\n+    ///  operation may create\n+    fn maybe_perform_sync_operation<'tcx>(\n+        &self, op: impl FnOnce(VectorIdx, RefMut<'_,ThreadClockSet>) -> InterpResult<'tcx>,\n+    ) -> InterpResult<'tcx> {\n+        if self.multi_threaded.get() {\n+            let (index, mut clocks) = self.current_thread_state_mut();\n+            clocks.increment_clock(index);\n+            op(index, clocks)?;\n+            let (_, mut clocks) = self.current_thread_state_mut();\n+            clocks.increment_clock(index);\n+        }\n+        Ok(())\n     }\n     \n \n     /// Internal utility to identify a thread stored internally\n     ///  returns the id and the name for better diagnostics\n-    fn print_thread_metadata(&self, thread: ThreadId) -> String {\n-        if let Some(Some(name)) = self.thread_names.borrow().get(thread) {\n+    fn print_thread_metadata(&self, vector: VectorIdx) -> String {\n+        let thread = self.vector_info.borrow()[vector];\n+        let thread_name = &self.thread_info.borrow()[thread].1;\n+        if let Some(name) = thread_name {\n             let name: &str = name;\n             format!(\"Thread(id = {:?}, name = {:?})\", thread.to_u32(), &*name)\n         }else{\n@@ -1175,427 +1049,68 @@ impl GlobalState {\n     /// Acquire a lock, express that the previous call of\n     ///  `validate_lock_release` must happen before this\n     pub fn validate_lock_acquire(&self, lock: &DataRaceLockHandle, thread: ThreadId) {\n-        let mut ref_vector = self.thread_clocks.borrow_mut();\n-        ref_vector[thread].increment_clock(thread);\n-\n-        let clocks = &mut ref_vector[thread];\n+        let (index, mut clocks) = self.load_thread_state_mut(thread);\n+        clocks.increment_clock(index);\n         clocks.clock.join(&lock.clock);\n-\n-        ref_vector[thread].increment_clock(thread);\n+        clocks.increment_clock(index);\n     }\n \n     /// Release a lock handle, express that this happens-before\n     ///  any subsequent calls to `validate_lock_acquire`\n     pub fn validate_lock_release(&self, lock: &mut DataRaceLockHandle, thread: ThreadId) {\n-        let mut ref_vector = self.thread_clocks.borrow_mut();\n-        ref_vector[thread].increment_clock(thread);\n-\n-        let clocks = &ref_vector[thread];\n-        lock.clock.set_values(&clocks.clock);\n-\n-        ref_vector[thread].increment_clock(thread);\n+        let (index, mut clocks) = self.load_thread_state_mut(thread);\n+        clocks.increment_clock(index);\n+        lock.clock.clone_from(&clocks.clock);\n+        clocks.increment_clock(index);\n     }\n \n     /// Release a lock handle, express that this happens-before\n     ///  any subsequent calls to `validate_lock_acquire` as well\n     ///  as any previous calls to this function after any\n     ///  `validate_lock_release` calls\n     pub fn validate_lock_release_shared(&self, lock: &mut DataRaceLockHandle, thread: ThreadId) {\n-        let mut ref_vector = self.thread_clocks.borrow_mut();\n-        ref_vector[thread].increment_clock(thread);\n-\n-        let clocks = &ref_vector[thread];\n+        let (index, mut clocks) = self.load_thread_state_mut(thread);\n+        clocks.increment_clock(index);\n         lock.clock.join(&clocks.clock);\n-\n-        ref_vector[thread].increment_clock(thread);\n-    }\n-\n-    /// Load the thread clock set associated with the current thread\n-    #[inline]\n-    fn current_thread_state(&self) -> Ref<'_, ThreadClockSet> {\n-        let ref_vector = self.thread_clocks.borrow();\n-        let thread = self.current_thread_id.get();\n-        Ref::map(ref_vector, |vector| &vector[thread])\n-    }\n-\n-    /// Load the thread clock set associated with the current thread\n-    ///  mutably for modification\n-    #[inline]\n-    fn current_thread_state_mut(&self) -> RefMut<'_, ThreadClockSet> {\n-        let ref_vector = self.thread_clocks.borrow_mut();\n-        let thread = self.current_thread_id.get();\n-        RefMut::map(ref_vector, |vector| &mut vector[thread])\n-    }\n-\n-    /// Return the current thread, should be the same\n-    ///  as the data-race active thread\n-    #[inline]\n-    fn current_thread(&self) -> ThreadId {\n-        self.current_thread_id.get()\n+        clocks.increment_clock(index);\n     }\n-}\n-\n-\n-/// The size of the vector-clock to store inline\n-///  clock vectors larger than this will be stored on the heap\n-const SMALL_VECTOR: usize = 4;\n-\n-/// The type of the time-stamps recorded in the data-race detector\n-///  set to a type of unsigned integer\n-type Timestamp = u32;\n-\n-/// A vector clock for detecting data-races\n-///  invariants:\n-///   - the last element in a VClock must not be 0\n-///     -- this means that derive(PartialEq & Eq) is correct\n-///     --  as there is no implicit zero tail that might be equal\n-///     --  also simplifies the implementation of PartialOrd\n-#[derive(Clone, PartialEq, Eq, Default, Debug)]\n-pub struct VClock(SmallVec<[Timestamp; SMALL_VECTOR]>);\n-\n-impl VClock {\n \n-    /// Load the backing slice behind the clock vector.\n+    /// Load the vector index used by the given thread as well as the set of vector clocks\n+    ///  used by the thread\n     #[inline]\n-    fn as_slice(&self) -> &[Timestamp] {\n-        self.0.as_slice()\n+    fn load_thread_state_mut(&self, thread: ThreadId) -> (VectorIdx, RefMut<'_, ThreadClockSet>) {\n+        let index = self.thread_info.borrow()[thread].0\n+            .expect(\"Loading thread state for thread with no assigned vector\");\n+        let ref_vector = self.vector_clocks.borrow_mut();\n+        let clocks = RefMut::map(ref_vector, |vec| &mut vec[index]);\n+        (index, clocks)\n     }\n \n-    /// Get a mutable slice to the internal vector with minimum `min_len`\n-    ///  elements, to preserve invariants this vector must modify\n-    ///  the `min_len`-1 nth element to a non-zero value\n+    /// Load the current vector clock in use and the current set of thread clocks\n+    ///  in use for the vector\n     #[inline]\n-    fn get_mut_with_min_len(&mut self, min_len: usize) -> &mut [Timestamp] {\n-        if self.0.len() < min_len {\n-            self.0.resize(min_len, 0);\n-        }\n-        assert!(self.0.len() >= min_len);\n-        self.0.as_mut_slice()\n+    fn current_thread_state(&self) -> (VectorIdx, Ref<'_, ThreadClockSet>) {\n+        let index = self.current_index();\n+        let ref_vector = self.vector_clocks.borrow();\n+        let clocks = Ref::map(ref_vector, |vec| &vec[index]);\n+        (index, clocks)\n     }\n \n-    /// Increment the vector clock at a known index\n+    /// Load the current vector clock in use and the current set of thread clocks\n+    ///  in use for the vector mutably for modification\n     #[inline]\n-    fn increment_index(&mut self, idx: usize) {\n-        let mut_slice = self.get_mut_with_min_len(idx + 1);\n-        let idx_ref = &mut mut_slice[idx];\n-        *idx_ref = idx_ref.checked_add(1).expect(\"Vector clock overflow\")\n+    fn current_thread_state_mut(&self) -> (VectorIdx, RefMut<'_, ThreadClockSet>) {\n+        let index = self.current_index();\n+        let ref_vector = self.vector_clocks.borrow_mut();\n+        let clocks = RefMut::map(ref_vector, |vec| &mut vec[index]);\n+        (index, clocks)\n     }\n \n-    // Increment the vector element representing the progress\n-    //  of execution in the given thread\n-    #[inline]\n-    pub fn increment_thread(&mut self, thread: ThreadId) {\n-        self.increment_index(thread.to_u32() as usize);\n-    }\n-\n-    // Join the two vector-clocks together, this\n-    //  sets each vector-element to the maximum value\n-    //  of that element in either of the two source elements.\n-    pub fn join(&mut self, other: &Self) {\n-        let rhs_slice = other.as_slice();\n-        let lhs_slice = self.get_mut_with_min_len(rhs_slice.len());\n-\n-        // Element-wise set to maximum.\n-        for (l, &r) in lhs_slice.iter_mut().zip(rhs_slice.iter()) {\n-            *l = r.max(*l);\n-        }\n-    }\n-\n-    /// Joins with a thread at a known index\n-    fn set_at_index(&mut self, other: &Self, idx: usize){\n-        let mut_slice = self.get_mut_with_min_len(idx + 1);\n-        let slice = other.as_slice();\n-        mut_slice[idx] = slice[idx];\n-    }\n-\n-    /// Join with a threads vector clock only at the desired index\n-    ///  returns true if the value updated\n-    #[inline]\n-    pub fn set_at_thread(&mut self, other: &Self, thread: ThreadId){\n-        self.set_at_index(other, thread.to_u32() as usize);\n-    }\n-\n-    /// Clear the vector to all zeros, stored as an empty internal\n-    ///  vector\n-    #[inline]\n-    pub fn set_zero_vector(&mut self) {\n-        self.0.clear();\n-    }\n-\n-    /// Set the values stored in this vector clock\n-    ///  to the values stored in another.\n-    pub fn set_values(&mut self, new_value: &VClock) {\n-        let new_slice = new_value.as_slice();\n-        self.0.resize(new_slice.len(), 0);\n-        self.0.copy_from_slice(new_slice);\n-    }\n-}\n-\n-\n-impl PartialOrd for VClock {\n-    fn partial_cmp(&self, other: &VClock) -> Option<Ordering> {\n-\n-        // Load the values as slices\n-        let lhs_slice = self.as_slice();\n-        let rhs_slice = other.as_slice();\n-\n-        // Iterate through the combined vector slice\n-        //  keeping track of the order that is currently possible to satisfy.\n-        // If an ordering relation is detected to be impossible, then bail and\n-        //  directly return None\n-        let mut iter = lhs_slice.iter().zip(rhs_slice.iter());\n-        let mut order = match iter.next() {\n-            Some((lhs, rhs)) => lhs.cmp(rhs),\n-            None => Ordering::Equal\n-        };\n-        for (l, r) in iter {\n-            match order {\n-                Ordering::Equal => order = l.cmp(r),\n-                Ordering::Less => if l > r {\n-                    return None\n-                },\n-                Ordering::Greater => if l < r {\n-                    return None\n-                }\n-            }\n-        }\n-\n-        //Now test if either left or right have trailing elements\n-        // by the invariant the trailing elements have at least 1\n-        // non zero value, so no additional calculation is required\n-        // to determine the result of the PartialOrder\n-        let l_len = lhs_slice.len();\n-        let r_len = rhs_slice.len();\n-        match l_len.cmp(&r_len) {\n-            // Equal has no additional elements: return current order\n-            Ordering::Equal => Some(order),\n-            // Right has at least 1 element > than the implicit 0,\n-            //  so the only valid values are Ordering::Less or None\n-            Ordering::Less => match order {\n-                Ordering::Less | Ordering::Equal => Some(Ordering::Less),\n-                Ordering::Greater => None\n-            }\n-            // Left has at least 1 element > than the implicit 0,\n-            //  so the only valid values are Ordering::Greater or None\n-            Ordering::Greater => match order {\n-                Ordering::Greater | Ordering::Equal => Some(Ordering::Greater),\n-                Ordering::Less => None\n-            }\n-        }\n-    }\n-\n-    fn lt(&self, other: &VClock) -> bool {\n-        // Load the values as slices\n-        let lhs_slice = self.as_slice();\n-        let rhs_slice = other.as_slice();\n-\n-        // If l_len > r_len then at least one element\n-        //  in l_len is > than r_len, therefore the result\n-        //  is either Some(Greater) or None, so return false\n-        //  early.\n-        let l_len = lhs_slice.len();\n-        let r_len = rhs_slice.len();\n-        if l_len <= r_len {\n-            // If any elements on the left are greater than the right\n-            //  then the result is None or Some(Greater), both of which\n-            //  return false, the earlier test asserts that no elements in the\n-            //  extended tail violate this assumption. Otherwise l <= r, finally\n-            //  the case where the values are potentially equal needs to be considered\n-            //  and false returned as well\n-            let mut equal = l_len == r_len;\n-            for (&l, &r) in lhs_slice.iter().zip(rhs_slice.iter()) {\n-                if l > r {\n-                    return false\n-                }else if l < r {\n-                    equal = false;\n-                }\n-            }\n-            !equal\n-        }else{\n-            false\n-        }\n-    }\n-\n-    fn le(&self, other: &VClock) -> bool {\n-        // Load the values as slices\n-        let lhs_slice = self.as_slice();\n-        let rhs_slice = other.as_slice();\n-\n-        // If l_len > r_len then at least one element\n-        //  in l_len is > than r_len, therefore the result\n-        //  is either Some(Greater) or None, so return false\n-        //  early.\n-        let l_len = lhs_slice.len();\n-        let r_len = rhs_slice.len();\n-        if l_len <= r_len {\n-            // If any elements on the left are greater than the right\n-            //  then the result is None or Some(Greater), both of which\n-            //  return false, the earlier test asserts that no elements in the\n-            //  extended tail violate this assumption. Otherwise l <= r\n-            !lhs_slice.iter().zip(rhs_slice.iter()).any(|(&l, &r)| l > r)\n-        }else{\n-            false\n-        }\n-    }\n-\n-    fn gt(&self, other: &VClock) -> bool {\n-        // Load the values as slices\n-        let lhs_slice = self.as_slice();\n-        let rhs_slice = other.as_slice();\n-\n-        // If r_len > l_len then at least one element\n-        //  in r_len is > than l_len, therefore the result\n-        //  is either Some(Less) or None, so return false\n-        //  early.\n-        let l_len = lhs_slice.len();\n-        let r_len = rhs_slice.len();\n-        if l_len >= r_len {\n-            // If any elements on the left are less than the right\n-            //  then the result is None or Some(Less), both of which\n-            //  return false, the earlier test asserts that no elements in the\n-            //  extended tail violate this assumption. Otherwise l >=, finally\n-            //  the case where the values are potentially equal needs to be considered\n-            //  and false returned as well\n-            let mut equal = l_len == r_len;\n-            for (&l, &r) in lhs_slice.iter().zip(rhs_slice.iter()) {\n-                if l < r {\n-                    return false\n-                }else if l > r {\n-                    equal = false;\n-                }\n-            }\n-            !equal\n-        }else{\n-            false\n-        }\n-    }\n-\n-    fn ge(&self, other: &VClock) -> bool {\n-        // Load the values as slices\n-        let lhs_slice = self.as_slice();\n-        let rhs_slice = other.as_slice();\n-\n-        // If r_len > l_len then at least one element\n-        //  in r_len is > than l_len, therefore the result\n-        //  is either Some(Less) or None, so return false\n-        //  early.\n-        let l_len = lhs_slice.len();\n-        let r_len = rhs_slice.len();\n-        if l_len >= r_len {\n-            // If any elements on the left are less than the right\n-            //  then the result is None or Some(Less), both of which\n-            //  return false, the earlier test asserts that no elements in the\n-            //  extended tail violate this assumption. Otherwise l >= r\n-            !lhs_slice.iter().zip(rhs_slice.iter()).any(|(&l, &r)| l < r)\n-        }else{\n-            false\n-        }\n-    }\n-}\n-\n-impl Index<ThreadId> for VClock {\n-    type Output = Timestamp;\n-\n+    /// Return the current thread, should be the same\n+    ///  as the data-race active thread\n     #[inline]\n-    fn index(&self, index: ThreadId) -> &Timestamp {\n-       self.as_slice().get(index.to_u32() as usize).unwrap_or(&0)\n+    fn current_index(&self) -> VectorIdx {\n+        self.current_index.get()\n     }\n }\n \n-\n-/// Test vector clock ordering operations\n-///  data-race detection is tested in the external\n-///  test suite\n-#[cfg(test)]\n-mod tests {\n-    use super::{VClock, Timestamp};\n-    use std::cmp::Ordering;\n-\n-    #[test]\n-    fn test_equal() {\n-        let mut c1 = VClock::default();\n-        let mut c2 = VClock::default();\n-        assert_eq!(c1, c2);\n-        c1.increment_index(5);\n-        assert_ne!(c1, c2);\n-        c2.increment_index(53);\n-        assert_ne!(c1, c2);\n-        c1.increment_index(53);\n-        assert_ne!(c1, c2);\n-        c2.increment_index(5);\n-        assert_eq!(c1, c2);\n-    }\n-\n-    #[test]\n-    fn test_partial_order() {\n-        // Small test\n-        assert_order(&[1], &[1], Some(Ordering::Equal));\n-        assert_order(&[1], &[2], Some(Ordering::Less));\n-        assert_order(&[2], &[1], Some(Ordering::Greater));\n-        assert_order(&[1], &[1,2], Some(Ordering::Less));\n-        assert_order(&[2], &[1,2], None);\n-\n-        // Misc tests\n-        assert_order(&[400], &[0, 1], None);\n-\n-        // Large test\n-        assert_order(&[0,1,2,3,4,5,6,7,8,9,10], &[0,1,2,3,4,5,6,7,8,9,10,0,0,0], Some(Ordering::Equal));\n-        assert_order(&[0,1,2,3,4,5,6,7,8,9,10], &[0,1,2,3,4,5,6,7,8,9,10,0,1,0], Some(Ordering::Less));\n-        assert_order(&[0,1,2,3,4,5,6,7,8,9,11], &[0,1,2,3,4,5,6,7,8,9,10,0,0,0], Some(Ordering::Greater));\n-        assert_order(&[0,1,2,3,4,5,6,7,8,9,11], &[0,1,2,3,4,5,6,7,8,9,10,0,1,0], None);\n-        assert_order(&[0,1,2,3,4,5,6,7,8,9,9 ], &[0,1,2,3,4,5,6,7,8,9,10,0,0,0], Some(Ordering::Less));\n-        assert_order(&[0,1,2,3,4,5,6,7,8,9,9 ], &[0,1,2,3,4,5,6,7,8,9,10,0,1,0], Some(Ordering::Less));\n-    }\n-\n-    fn from_slice(mut slice: &[Timestamp]) -> VClock {\n-        while let Some(0) = slice.last() {\n-            slice = &slice[..slice.len() - 1]\n-        }\n-        VClock(smallvec::SmallVec::from_slice(slice))\n-    }\n-\n-    fn assert_order(l: &[Timestamp], r: &[Timestamp], o: Option<Ordering>) {\n-        let l = from_slice(l);\n-        let r = from_slice(r);\n-\n-        //Test partial_cmp\n-        let compare = l.partial_cmp(&r);\n-        assert_eq!(compare, o, \"Invalid comparison\\n l: {:?}\\n r: {:?}\",l,r);\n-        let alt_compare = r.partial_cmp(&l);\n-        assert_eq!(alt_compare, o.map(Ordering::reverse), \"Invalid alt comparison\\n l: {:?}\\n r: {:?}\",l,r);\n-\n-        //Test operatorsm with faster implementations\n-        assert_eq!(\n-            matches!(compare,Some(Ordering::Less)), l < r,\n-            \"Invalid (<):\\n l: {:?}\\n r: {:?}\",l,r\n-        );\n-        assert_eq!(\n-            matches!(compare,Some(Ordering::Less) | Some(Ordering::Equal)), l <= r,\n-            \"Invalid (<=):\\n l: {:?}\\n r: {:?}\",l,r\n-        );\n-        assert_eq!(\n-            matches!(compare,Some(Ordering::Greater)), l > r,\n-            \"Invalid (>):\\n l: {:?}\\n r: {:?}\",l,r\n-        );\n-        assert_eq!(\n-            matches!(compare,Some(Ordering::Greater) | Some(Ordering::Equal)), l >= r,\n-            \"Invalid (>=):\\n l: {:?}\\n r: {:?}\",l,r\n-        );\n-        assert_eq!(\n-            matches!(alt_compare,Some(Ordering::Less)), r < l,\n-            \"Invalid alt (<):\\n l: {:?}\\n r: {:?}\",l,r\n-        );\n-        assert_eq!(\n-            matches!(alt_compare,Some(Ordering::Less) | Some(Ordering::Equal)), r <= l,\n-            \"Invalid alt (<=):\\n l: {:?}\\n r: {:?}\",l,r\n-        );\n-        assert_eq!(\n-            matches!(alt_compare,Some(Ordering::Greater)), r > l,\n-            \"Invalid alt (>):\\n l: {:?}\\n r: {:?}\",l,r\n-        );\n-        assert_eq!(\n-            matches!(alt_compare,Some(Ordering::Greater) | Some(Ordering::Equal)), r >= l,\n-            \"Invalid alt (>=):\\n l: {:?}\\n r: {:?}\",l,r\n-        );\n-    }\n-}"}, {"sha": "c8c9e70ec3deb22ba283662bd65d6689050c7a31", "filename": "src/lib.rs", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "blob_url": "https://github.com/rust-lang/rust/blob/9cb6b8da3f6ca2da1139c91754d520bf2d354f31/src%2Flib.rs", "raw_url": "https://github.com/rust-lang/rust/raw/9cb6b8da3f6ca2da1139c91754d520bf2d354f31/src%2Flib.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flib.rs?ref=9cb6b8da3f6ca2da1139c91754d520bf2d354f31", "patch": "@@ -35,6 +35,7 @@ mod shims;\n mod stacked_borrows;\n mod sync;\n mod thread;\n+mod vector_clock;\n \n // Establish a \"crate-wide prelude\": we often import `crate::*`.\n \n@@ -79,6 +80,9 @@ pub use crate::thread::{\n pub use crate::sync::{\n     EvalContextExt as SyncEvalContextExt, CondvarId, MutexId, RwLockId\n };\n+pub use crate::vector_clock::{\n+    VClock, VSmallClockSet, VectorIdx, VTimestamp\n+};\n \n /// Insert rustc arguments at the beginning of the argument list that Miri wants to be\n /// set per default, for maximal validation power."}, {"sha": "50f97af8453e619e588635188b74cd12d725c392", "filename": "src/shims/intrinsics.rs", "status": "modified", "additions": 20, "deletions": 11, "changes": 31, "blob_url": "https://github.com/rust-lang/rust/blob/9cb6b8da3f6ca2da1139c91754d520bf2d354f31/src%2Fshims%2Fintrinsics.rs", "raw_url": "https://github.com/rust-lang/rust/raw/9cb6b8da3f6ca2da1139c91754d520bf2d354f31/src%2Fshims%2Fintrinsics.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Fshims%2Fintrinsics.rs?ref=9cb6b8da3f6ca2da1139c91754d520bf2d354f31", "patch": "@@ -469,8 +469,7 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: crate::MiriEvalContextExt<'mir, 'tcx\n         let place = this.deref_operand(place)?;\n \n         // make sure it fits into a scalar; otherwise it cannot be atomic\n-        let val = this.read_scalar_racy(place)?;\n-        this.validate_atomic_load(place, atomic)?;\n+        let val = this.read_scalar_atomic(place, atomic)?;\n \n         // Check alignment requirements. Atomics must always be aligned to their size,\n         // even if the type they wrap would be less aligned (e.g. AtomicU64 on 32bit must\n@@ -495,9 +494,7 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: crate::MiriEvalContextExt<'mir, 'tcx\n         this.memory.check_ptr_access(place.ptr, place.layout.size, align)?;\n \n         // Perform atomic store\n-        this.write_scalar_racy(val, place)?;\n-\n-        this.validate_atomic_store(place, atomic)?;\n+        this.write_scalar_atomic(val, place, atomic)?;\n         Ok(())\n     }\n \n@@ -527,7 +524,9 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: crate::MiriEvalContextExt<'mir, 'tcx\n             bug!(\"Atomic arithmetic operations only work on integer types\");\n         }\n         let rhs = this.read_immediate(rhs)?;\n-        let old = this.read_immediate_racy(place)?;\n+        let old = this.allow_data_races_mut(|this| {\n+            this.read_immediate(place. into())\n+        })?;\n \n         // Check alignment requirements. Atomics must always be aligned to their size,\n         // even if the type they wrap would be less aligned (e.g. AtomicU64 on 32bit must\n@@ -539,7 +538,9 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: crate::MiriEvalContextExt<'mir, 'tcx\n         // Atomics wrap around on overflow.\n         let val = this.binary_op(op, old, rhs)?;\n         let val = if neg { this.unary_op(mir::UnOp::Not, val)? } else { val };\n-        this.write_immediate_racy(*val, place)?;\n+        this.allow_data_races_mut(|this| {\n+            this.write_immediate(*val, place.into())\n+        })?;\n \n         this.validate_atomic_rmw(place, atomic)?;\n         Ok(())\n@@ -553,7 +554,9 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: crate::MiriEvalContextExt<'mir, 'tcx\n         let &[place, new] = check_arg_count(args)?;\n         let place = this.deref_operand(place)?;\n         let new = this.read_scalar(new)?;\n-        let old = this.read_scalar_racy(place)?;\n+        let old = this.allow_data_races_mut(|this| {\n+            this.read_scalar(place.into())\n+        })?;\n \n         // Check alignment requirements. Atomics must always be aligned to their size,\n         // even if the type they wrap would be less aligned (e.g. AtomicU64 on 32bit must\n@@ -562,7 +565,9 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: crate::MiriEvalContextExt<'mir, 'tcx\n         this.memory.check_ptr_access(place.ptr, place.layout.size, align)?;\n \n         this.write_scalar(old, dest)?; // old value is returned\n-        this.write_scalar_racy(new, place)?;\n+        this.allow_data_races_mut(|this| {\n+            this.write_scalar(new, place.into())\n+        })?;\n \n         this.validate_atomic_rmw(place, atomic)?;\n         Ok(())\n@@ -583,7 +588,9 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: crate::MiriEvalContextExt<'mir, 'tcx\n         //  to read with the failure ordering and if successfull then try again with the success\n         //  read ordering and write in the success case.\n         // Read as immediate for the sake of `binary_op()`\n-        let old = this.read_immediate_racy(place)?; \n+        let old = this.allow_data_races_mut(|this| {\n+            this.read_immediate(place.into())\n+        })?; \n \n         // Check alignment requirements. Atomics must always be aligned to their size,\n         // even if the type they wrap would be less aligned (e.g. AtomicU64 on 32bit must\n@@ -602,7 +609,9 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: crate::MiriEvalContextExt<'mir, 'tcx\n         //  if successful, perform a full rw-atomic validation\n         //  otherwise treat this as an atomic load with the fail ordering\n         if eq.to_bool()? {\n-            this.write_scalar_racy(new, place)?;\n+            this.allow_data_races_mut(|this| {\n+                this.write_scalar(new, place.into())\n+            })?;\n             this.validate_atomic_rmw(place, success)?;\n         } else {\n             this.validate_atomic_load(place, fail)?;"}, {"sha": "d741ef346e945f961c56ce90ba2d91cff9cdc5d9", "filename": "src/shims/posix/sync.rs", "status": "modified", "additions": 31, "deletions": 8, "changes": 39, "blob_url": "https://github.com/rust-lang/rust/blob/9cb6b8da3f6ca2da1139c91754d520bf2d354f31/src%2Fshims%2Fposix%2Fsync.rs", "raw_url": "https://github.com/rust-lang/rust/raw/9cb6b8da3f6ca2da1139c91754d520bf2d354f31/src%2Fshims%2Fposix%2Fsync.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Fshims%2Fposix%2Fsync.rs?ref=9cb6b8da3f6ca2da1139c91754d520bf2d354f31", "patch": "@@ -62,7 +62,10 @@ fn mutex_get_kind<'mir, 'tcx: 'mir>(\n     mutex_op: OpTy<'tcx, Tag>,\n ) -> InterpResult<'tcx, ScalarMaybeUninit<Tag>> {\n     let offset = if ecx.pointer_size().bytes() == 8 { 16 } else { 12 };\n-    ecx.read_scalar_at_offset_racy(mutex_op, offset, ecx.machine.layouts.i32)\n+    ecx.read_scalar_at_offset_atomic(\n+        mutex_op, offset, ecx.machine.layouts.i32,\n+        AtomicReadOp::SeqCst\n+    )\n }\n \n fn mutex_set_kind<'mir, 'tcx: 'mir>(\n@@ -71,22 +74,30 @@ fn mutex_set_kind<'mir, 'tcx: 'mir>(\n     kind: impl Into<ScalarMaybeUninit<Tag>>,\n ) -> InterpResult<'tcx, ()> {\n     let offset = if ecx.pointer_size().bytes() == 8 { 16 } else { 12 };\n-    ecx.write_scalar_at_offset_racy(mutex_op, offset, kind, ecx.machine.layouts.i32)\n+    ecx.write_scalar_at_offset_atomic(\n+        mutex_op, offset, kind, ecx.machine.layouts.i32, \n+        AtomicWriteOp::SeqCst\n+    )\n }\n \n fn mutex_get_id<'mir, 'tcx: 'mir>(\n     ecx: &MiriEvalContext<'mir, 'tcx>,\n     mutex_op: OpTy<'tcx, Tag>,\n ) -> InterpResult<'tcx, ScalarMaybeUninit<Tag>> {\n-    ecx.read_scalar_at_offset_racy(mutex_op, 4, ecx.machine.layouts.u32)\n+    ecx.read_scalar_at_offset_atomic(\n+        mutex_op, 4, ecx.machine.layouts.u32, AtomicReadOp::SeqCst\n+    )\n }\n \n fn mutex_set_id<'mir, 'tcx: 'mir>(\n     ecx: &mut MiriEvalContext<'mir, 'tcx>,\n     mutex_op: OpTy<'tcx, Tag>,\n     id: impl Into<ScalarMaybeUninit<Tag>>,\n ) -> InterpResult<'tcx, ()> {\n-    ecx.write_scalar_at_offset_racy(mutex_op, 4, id, ecx.machine.layouts.u32)\n+    ecx.write_scalar_at_offset_atomic(\n+        mutex_op, 4, id, ecx.machine.layouts.u32,\n+        AtomicWriteOp::SeqCst\n+    )\n }\n \n fn mutex_get_or_create_id<'mir, 'tcx: 'mir>(\n@@ -116,15 +127,21 @@ fn rwlock_get_id<'mir, 'tcx: 'mir>(\n     ecx: &MiriEvalContext<'mir, 'tcx>,\n     rwlock_op: OpTy<'tcx, Tag>,\n ) -> InterpResult<'tcx, ScalarMaybeUninit<Tag>> {\n-    ecx.read_scalar_at_offset_racy(rwlock_op, 4, ecx.machine.layouts.u32)\n+    ecx.read_scalar_at_offset_atomic(\n+        rwlock_op, 4, ecx.machine.layouts.u32,\n+        AtomicReadOp::SeqCst\n+    )\n }\n \n fn rwlock_set_id<'mir, 'tcx: 'mir>(\n     ecx: &mut MiriEvalContext<'mir, 'tcx>,\n     rwlock_op: OpTy<'tcx, Tag>,\n     id: impl Into<ScalarMaybeUninit<Tag>>,\n ) -> InterpResult<'tcx, ()> {\n-    ecx.write_scalar_at_offset_racy(rwlock_op, 4, id, ecx.machine.layouts.u32)\n+    ecx.write_scalar_at_offset_atomic(\n+        rwlock_op, 4, id, ecx.machine.layouts.u32,\n+        AtomicWriteOp::SeqCst\n+    )\n }\n \n fn rwlock_get_or_create_id<'mir, 'tcx: 'mir>(\n@@ -177,15 +194,21 @@ fn cond_get_id<'mir, 'tcx: 'mir>(\n     ecx: &MiriEvalContext<'mir, 'tcx>,\n     cond_op: OpTy<'tcx, Tag>,\n ) -> InterpResult<'tcx, ScalarMaybeUninit<Tag>> {\n-    ecx.read_scalar_at_offset_racy(cond_op, 4, ecx.machine.layouts.u32)\n+    ecx.read_scalar_at_offset_atomic(\n+        cond_op, 4, ecx.machine.layouts.u32,\n+        AtomicReadOp::SeqCst\n+    )\n }\n \n fn cond_set_id<'mir, 'tcx: 'mir>(\n     ecx: &mut MiriEvalContext<'mir, 'tcx>,\n     cond_op: OpTy<'tcx, Tag>,\n     id: impl Into<ScalarMaybeUninit<Tag>>,\n ) -> InterpResult<'tcx, ()> {\n-    ecx.write_scalar_at_offset_racy(cond_op, 4, id, ecx.machine.layouts.u32)\n+    ecx.write_scalar_at_offset_atomic(\n+        cond_op, 4, id, ecx.machine.layouts.u32,\n+        AtomicWriteOp::SeqCst\n+    )\n }\n \n fn cond_get_or_create_id<'mir, 'tcx: 'mir>("}, {"sha": "f94805ae022afc4a83c673c82410e8c4db38b9d5", "filename": "src/thread.rs", "status": "modified", "additions": 3, "deletions": 1, "changes": 4, "blob_url": "https://github.com/rust-lang/rust/blob/9cb6b8da3f6ca2da1139c91754d520bf2d354f31/src%2Fthread.rs", "raw_url": "https://github.com/rust-lang/rust/raw/9cb6b8da3f6ca2da1139c91754d520bf2d354f31/src%2Fthread.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Fthread.rs?ref=9cb6b8da3f6ca2da1139c91754d520bf2d354f31", "patch": "@@ -638,7 +638,9 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: crate::MiriEvalContextExt<'mir, 'tcx\n     fn set_active_thread_name(&mut self, new_thread_name: Vec<u8>) {\n         let this = self.eval_context_mut();\n         if let Ok(string) = String::from_utf8(new_thread_name.clone()) {\n-            this.memory.extra.data_race.thread_set_name(string);\n+            this.memory.extra.data_race.thread_set_name(\n+                this.machine.threads.active_thread, string\n+            );\n         }\n         this.machine.threads.set_thread_name(new_thread_name);\n     }"}, {"sha": "8d05eb1b992bb153dfa42a188959b2016f8c1f43", "filename": "src/vector_clock.rs", "status": "added", "additions": 602, "deletions": 0, "changes": 602, "blob_url": "https://github.com/rust-lang/rust/blob/9cb6b8da3f6ca2da1139c91754d520bf2d354f31/src%2Fvector_clock.rs", "raw_url": "https://github.com/rust-lang/rust/raw/9cb6b8da3f6ca2da1139c91754d520bf2d354f31/src%2Fvector_clock.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Fvector_clock.rs?ref=9cb6b8da3f6ca2da1139c91754d520bf2d354f31", "patch": "@@ -0,0 +1,602 @@\n+use std::{\n+    fmt::{self, Debug}, cmp::Ordering, ops::Index,\n+    num::TryFromIntError, convert::TryFrom, mem\n+};\n+use smallvec::SmallVec;\n+use rustc_index::vec::Idx;\n+use rustc_data_structures::fx::FxHashMap;\n+\n+/// A vector clock index, this is associated with a thread id\n+///  but in some cases one vector index may be shared with\n+///  multiple thread ids.\n+#[derive(Clone, Copy, Debug, PartialOrd, Ord, PartialEq, Eq, Hash)]\n+pub struct VectorIdx(u32);\n+\n+impl VectorIdx{\n+    pub fn to_u32(self) -> u32 {\n+        self.0\n+    }\n+    pub const MAX_INDEX: VectorIdx = VectorIdx(u32::MAX);\n+}\n+\n+impl Idx for VectorIdx {\n+    fn new(idx: usize) -> Self {\n+        VectorIdx(u32::try_from(idx).unwrap())\n+    }\n+\n+    fn index(self) -> usize {\n+        usize::try_from(self.0).unwrap()\n+    }\n+}\n+\n+impl TryFrom<u64> for VectorIdx {\n+    type Error = TryFromIntError;\n+    fn try_from(id: u64) -> Result<Self, Self::Error> {\n+        u32::try_from(id).map(|id_u32| Self(id_u32))\n+    }\n+}\n+\n+impl From<u32> for VectorIdx {\n+    fn from(id: u32) -> Self {\n+        Self(id)\n+    }\n+}\n+\n+\n+/// A sparse set of vector clocks, where each vector index\n+///  is associated with a vector clock.\n+/// This treats all vector clocks that have not been assigned\n+///  as equal to the all zero vector clocks\n+/// Is optimized for the common case where only 1 element is stored\n+///  in the set and the rest can be ignored, falling-back to\n+///  using an internal hash-map once more than 1 element is assigned\n+///  at any one time\n+#[derive(Clone)]\n+pub struct VSmallClockSet(VSmallClockSetInner);\n+\n+#[derive(Clone)]\n+enum VSmallClockSetInner {\n+    /// Zero or 1 vector elements, common\n+    ///  case for the sparse set.\n+    /// The all zero vector clock is treated\n+    ///  as equal to the empty element\n+    Small(VectorIdx, VClock),\n+\n+    /// Hash-map of vector clocks\n+    Large(FxHashMap<VectorIdx, VClock>)\n+}\n+\n+impl VSmallClockSet {\n+\n+    /// Remove all clock vectors from the map, setting them\n+    ///  to the zero vector\n+    pub fn clear(&mut self) {\n+        match &mut self.0 {\n+            VSmallClockSetInner::Small(_, clock) => {\n+                clock.set_zero_vector()\n+            }\n+            VSmallClockSetInner::Large(hash_map) => {\n+                hash_map.clear();\n+            }\n+        }\n+    }\n+\n+    /// Remove all clock vectors except for the clock vector\n+    ///  stored at the given index, which is retained\n+    pub fn retain_index(&mut self, index: VectorIdx) {\n+        match &mut self.0 {\n+            VSmallClockSetInner::Small(small_idx, clock) => {\n+                if index != *small_idx {\n+                    // The zero-vector is considered to equal\n+                    //  the empty element\n+                    clock.set_zero_vector()\n+                }\n+            },\n+            VSmallClockSetInner::Large(hash_map) => {\n+                hash_map.retain(|idx,_| {\n+                    *idx == index\n+                });\n+            }\n+        }\n+    }\n+\n+    /// Insert the vector clock into the associated vector\n+    ///  index\n+    pub fn insert(&mut self, index: VectorIdx, clock: &VClock) {\n+        match &mut self.0 {\n+            VSmallClockSetInner::Small(small_idx, small_clock) => {\n+                if small_clock.is_zero_vector() {\n+                    *small_idx = index;\n+                    small_clock.clone_from(clock);\n+                }else if !clock.is_zero_vector() {\n+                    let mut hash_map = FxHashMap::default();\n+                    hash_map.insert(*small_idx, mem::take(small_clock));\n+                    hash_map.insert(index, clock.clone());\n+                    self.0 = VSmallClockSetInner::Large(hash_map);\n+                }\n+            },\n+            VSmallClockSetInner::Large(hash_map) => {\n+                if !clock.is_zero_vector() {\n+                    hash_map.insert(index, clock.clone());\n+                }\n+            }\n+        }\n+    }\n+\n+    /// Try to load the vector clock associated with the current\n+    ///  vector index.\n+    pub fn get(&self, index: VectorIdx) -> Option<&VClock> {\n+        match &self.0 {\n+            VSmallClockSetInner::Small(small_idx, small_clock) => {\n+                if *small_idx == index && !small_clock.is_zero_vector() {\n+                    Some(small_clock)\n+                }else{\n+                    None\n+                }\n+            },\n+            VSmallClockSetInner::Large(hash_map) => {\n+                hash_map.get(&index)\n+            }\n+        }\n+    }\n+}\n+\n+impl Default for VSmallClockSet {\n+    #[inline]\n+    fn default() -> Self {\n+        VSmallClockSet(\n+            VSmallClockSetInner::Small(VectorIdx::new(0), VClock::default())\n+        )\n+    }\n+}\n+\n+impl Debug for VSmallClockSet {\n+    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {\n+        // Print the contents of the small vector clock set as the map\n+        //  of vector index to vector clock that they represent\n+        let mut map = f.debug_map();\n+        match &self.0 {\n+            VSmallClockSetInner::Small(small_idx, small_clock) => {\n+                if !small_clock.is_zero_vector() {\n+                    map.entry(&small_idx, &small_clock);\n+                }\n+            },\n+            VSmallClockSetInner::Large(hash_map) => {\n+                for (idx, elem) in hash_map.iter() {\n+                    map.entry(idx, elem);\n+                }\n+            }\n+        }\n+        map.finish()\n+    }\n+}\n+impl PartialEq for VSmallClockSet {\n+    fn eq(&self, other: &Self) -> bool {\n+        use VSmallClockSetInner::*;\n+        match (&self.0, &other.0) {\n+            (Small(i1, c1), Small(i2, c2)) => {\n+                if c1.is_zero_vector() {\n+                    // Either they are both zero or they are non-equal\n+                    c2.is_zero_vector()\n+                }else{\n+                    // At least one is non-zero, so the full comparison is correct\n+                    i1 == i2 && c1 == c2\n+                }\n+            }\n+            (VSmallClockSetInner::Small(idx, clock), VSmallClockSetInner::Large(hash_map)) |\n+            (VSmallClockSetInner::Large(hash_map), VSmallClockSetInner::Small(idx, clock)) => {\n+                if hash_map.len() == 0 {\n+                    // Equal to the empty hash-map\n+                    clock.is_zero_vector()\n+                }else if hash_map.len() == 1 {\n+                    // Equal to the hash-map with one element\n+                    let (hash_idx, hash_clock) = hash_map.iter().next().unwrap();\n+                    hash_idx == idx && hash_clock == clock\n+                }else{\n+                    false\n+                }\n+            }\n+            (Large(map1), Large(map2)) => {\n+                map1 == map2\n+            }\n+        }\n+    }\n+}\n+impl Eq for VSmallClockSet {}\n+\n+\n+\n+/// The size of the vector-clock to store inline\n+///  clock vectors larger than this will be stored on the heap\n+const SMALL_VECTOR: usize = 4;\n+\n+/// The type of the time-stamps recorded in the data-race detector\n+///  set to a type of unsigned integer\n+pub type VTimestamp = u32;\n+\n+/// A vector clock for detecting data-races\n+///  invariants:\n+///   - the last element in a VClock must not be 0\n+///     -- this means that derive(PartialEq & Eq) is correct\n+///     --  as there is no implicit zero tail that might be equal\n+///     --  also simplifies the implementation of PartialOrd\n+#[derive(PartialEq, Eq, Default, Debug)]\n+pub struct VClock(SmallVec<[VTimestamp; SMALL_VECTOR]>);\n+\n+impl VClock {\n+\n+    /// Create a new vector-clock containing all zeros except\n+    ///  for a value at the given index\n+    pub fn new_with_index(index: VectorIdx, timestamp: VTimestamp) -> VClock {\n+        let len = index.index() + 1;\n+        let mut vec = smallvec::smallvec![0; len];\n+        vec[index.index()] = timestamp;\n+        VClock(vec)\n+    }\n+\n+    /// Load the internal timestamp slice in the vector clock\n+    #[inline]\n+    pub fn as_slice(&self) -> &[VTimestamp] {\n+        self.0.as_slice()\n+    }\n+\n+    /// Get a mutable slice to the internal vector with minimum `min_len`\n+    ///  elements, to preserve invariants this vector must modify\n+    ///  the `min_len`-1 nth element to a non-zero value\n+    #[inline]\n+    fn get_mut_with_min_len(&mut self, min_len: usize) -> &mut [VTimestamp] {\n+        if self.0.len() < min_len {\n+            self.0.resize(min_len, 0);\n+        }\n+        assert!(self.0.len() >= min_len);\n+        self.0.as_mut_slice()\n+    }\n+\n+    /// Increment the vector clock at a known index\n+    ///  this will panic if the vector index overflows\n+    #[inline]\n+    pub fn increment_index(&mut self, idx: VectorIdx) {\n+        let idx = idx.index();\n+        let mut_slice = self.get_mut_with_min_len(idx + 1);\n+        let idx_ref = &mut mut_slice[idx];\n+        *idx_ref = idx_ref.checked_add(1).expect(\"Vector clock overflow\")\n+    }\n+\n+    // Join the two vector-clocks together, this\n+    //  sets each vector-element to the maximum value\n+    //  of that element in either of the two source elements.\n+    pub fn join(&mut self, other: &Self) {\n+        let rhs_slice = other.as_slice();\n+        let lhs_slice = self.get_mut_with_min_len(rhs_slice.len());\n+        for (l, &r) in lhs_slice.iter_mut().zip(rhs_slice.iter()) {\n+            *l = r.max(*l);\n+        }\n+    }\n+\n+    /// Set the element at the current index of the vector\n+    pub fn set_at_index(&mut self, other: &Self, idx: VectorIdx) {\n+        let idx = idx.index();\n+        let mut_slice = self.get_mut_with_min_len(idx + 1);\n+        let slice = other.as_slice();\n+        mut_slice[idx] = slice[idx];\n+    }\n+\n+    /// Set the vector to the all-zero vector\n+    #[inline]\n+    pub fn set_zero_vector(&mut self) {\n+        self.0.clear();\n+    }\n+\n+    /// Return if this vector is the all-zero vector\n+    pub fn is_zero_vector(&self) -> bool {\n+        self.0.is_empty()\n+    }\n+}\n+\n+impl Clone for VClock {\n+    fn clone(&self) -> Self {\n+        VClock(self.0.clone())\n+    }\n+    fn clone_from(&mut self, source: &Self) {\n+        let source_slice = source.as_slice();\n+        self.0.clear();\n+        self.0.extend_from_slice(source_slice);\n+    }\n+}\n+\n+impl PartialOrd for VClock {\n+    fn partial_cmp(&self, other: &VClock) -> Option<Ordering> {\n+\n+        // Load the values as slices\n+        let lhs_slice = self.as_slice();\n+        let rhs_slice = other.as_slice();\n+\n+        // Iterate through the combined vector slice\n+        //  keeping track of the order that is currently possible to satisfy.\n+        // If an ordering relation is detected to be impossible, then bail and\n+        //  directly return None\n+        let mut iter = lhs_slice.iter().zip(rhs_slice.iter());\n+        let mut order = match iter.next() {\n+            Some((lhs, rhs)) => lhs.cmp(rhs),\n+            None => Ordering::Equal\n+        };\n+        for (l, r) in iter {\n+            match order {\n+                Ordering::Equal => order = l.cmp(r),\n+                Ordering::Less => if l > r {\n+                    return None\n+                },\n+                Ordering::Greater => if l < r {\n+                    return None\n+                }\n+            }\n+        }\n+\n+        //Now test if either left or right have trailing elements\n+        // by the invariant the trailing elements have at least 1\n+        // non zero value, so no additional calculation is required\n+        // to determine the result of the PartialOrder\n+        let l_len = lhs_slice.len();\n+        let r_len = rhs_slice.len();\n+        match l_len.cmp(&r_len) {\n+            // Equal has no additional elements: return current order\n+            Ordering::Equal => Some(order),\n+            // Right has at least 1 element > than the implicit 0,\n+            //  so the only valid values are Ordering::Less or None\n+            Ordering::Less => match order {\n+                Ordering::Less | Ordering::Equal => Some(Ordering::Less),\n+                Ordering::Greater => None\n+            }\n+            // Left has at least 1 element > than the implicit 0,\n+            //  so the only valid values are Ordering::Greater or None\n+            Ordering::Greater => match order {\n+                Ordering::Greater | Ordering::Equal => Some(Ordering::Greater),\n+                Ordering::Less => None\n+            }\n+        }\n+    }\n+\n+    fn lt(&self, other: &VClock) -> bool {\n+        // Load the values as slices\n+        let lhs_slice = self.as_slice();\n+        let rhs_slice = other.as_slice();\n+\n+        // If l_len > r_len then at least one element\n+        //  in l_len is > than r_len, therefore the result\n+        //  is either Some(Greater) or None, so return false\n+        //  early.\n+        let l_len = lhs_slice.len();\n+        let r_len = rhs_slice.len();\n+        if l_len <= r_len {\n+            // If any elements on the left are greater than the right\n+            //  then the result is None or Some(Greater), both of which\n+            //  return false, the earlier test asserts that no elements in the\n+            //  extended tail violate this assumption. Otherwise l <= r, finally\n+            //  the case where the values are potentially equal needs to be considered\n+            //  and false returned as well\n+            let mut equal = l_len == r_len;\n+            for (&l, &r) in lhs_slice.iter().zip(rhs_slice.iter()) {\n+                if l > r {\n+                    return false\n+                }else if l < r {\n+                    equal = false;\n+                }\n+            }\n+            !equal\n+        }else{\n+            false\n+        }\n+    }\n+\n+    fn le(&self, other: &VClock) -> bool {\n+        // Load the values as slices\n+        let lhs_slice = self.as_slice();\n+        let rhs_slice = other.as_slice();\n+\n+        // If l_len > r_len then at least one element\n+        //  in l_len is > than r_len, therefore the result\n+        //  is either Some(Greater) or None, so return false\n+        //  early.\n+        let l_len = lhs_slice.len();\n+        let r_len = rhs_slice.len();\n+        if l_len <= r_len {\n+            // If any elements on the left are greater than the right\n+            //  then the result is None or Some(Greater), both of which\n+            //  return false, the earlier test asserts that no elements in the\n+            //  extended tail violate this assumption. Otherwise l <= r\n+            !lhs_slice.iter().zip(rhs_slice.iter()).any(|(&l, &r)| l > r)\n+        }else{\n+            false\n+        }\n+    }\n+\n+    fn gt(&self, other: &VClock) -> bool {\n+        // Load the values as slices\n+        let lhs_slice = self.as_slice();\n+        let rhs_slice = other.as_slice();\n+\n+        // If r_len > l_len then at least one element\n+        //  in r_len is > than l_len, therefore the result\n+        //  is either Some(Less) or None, so return false\n+        //  early.\n+        let l_len = lhs_slice.len();\n+        let r_len = rhs_slice.len();\n+        if l_len >= r_len {\n+            // If any elements on the left are less than the right\n+            //  then the result is None or Some(Less), both of which\n+            //  return false, the earlier test asserts that no elements in the\n+            //  extended tail violate this assumption. Otherwise l >=, finally\n+            //  the case where the values are potentially equal needs to be considered\n+            //  and false returned as well\n+            let mut equal = l_len == r_len;\n+            for (&l, &r) in lhs_slice.iter().zip(rhs_slice.iter()) {\n+                if l < r {\n+                    return false\n+                }else if l > r {\n+                    equal = false;\n+                }\n+            }\n+            !equal\n+        }else{\n+            false\n+        }\n+    }\n+\n+    fn ge(&self, other: &VClock) -> bool {\n+        // Load the values as slices\n+        let lhs_slice = self.as_slice();\n+        let rhs_slice = other.as_slice();\n+\n+        // If r_len > l_len then at least one element\n+        //  in r_len is > than l_len, therefore the result\n+        //  is either Some(Less) or None, so return false\n+        //  early.\n+        let l_len = lhs_slice.len();\n+        let r_len = rhs_slice.len();\n+        if l_len >= r_len {\n+            // If any elements on the left are less than the right\n+            //  then the result is None or Some(Less), both of which\n+            //  return false, the earlier test asserts that no elements in the\n+            //  extended tail violate this assumption. Otherwise l >= r\n+            !lhs_slice.iter().zip(rhs_slice.iter()).any(|(&l, &r)| l < r)\n+        }else{\n+            false\n+        }\n+    }\n+}\n+\n+impl Index<VectorIdx> for VClock {\n+    type Output = VTimestamp;\n+\n+    #[inline]\n+    fn index(&self, index: VectorIdx) -> &VTimestamp {\n+       self.as_slice().get(index.to_u32() as usize).unwrap_or(&0)\n+    }\n+}\n+\n+\n+/// Test vector clock ordering operations\n+///  data-race detection is tested in the external\n+///  test suite\n+#[cfg(test)]\n+mod tests {\n+    use super::{VClock, VTimestamp, VectorIdx, VSmallClockSet};\n+    use std::cmp::Ordering;\n+\n+    #[test]\n+    fn test_equal() {\n+        let mut c1 = VClock::default();\n+        let mut c2 = VClock::default();\n+        assert_eq!(c1, c2);\n+        c1.increment_index(VectorIdx(5));\n+        assert_ne!(c1, c2);\n+        c2.increment_index(VectorIdx(53));\n+        assert_ne!(c1, c2);\n+        c1.increment_index(VectorIdx(53));\n+        assert_ne!(c1, c2);\n+        c2.increment_index(VectorIdx(5));\n+        assert_eq!(c1, c2);\n+    }\n+\n+    #[test]\n+    fn test_partial_order() {\n+        // Small test\n+        assert_order(&[1], &[1], Some(Ordering::Equal));\n+        assert_order(&[1], &[2], Some(Ordering::Less));\n+        assert_order(&[2], &[1], Some(Ordering::Greater));\n+        assert_order(&[1], &[1,2], Some(Ordering::Less));\n+        assert_order(&[2], &[1,2], None);\n+\n+        // Misc tests\n+        assert_order(&[400], &[0, 1], None);\n+\n+        // Large test\n+        assert_order(&[0,1,2,3,4,5,6,7,8,9,10], &[0,1,2,3,4,5,6,7,8,9,10,0,0,0], Some(Ordering::Equal));\n+        assert_order(&[0,1,2,3,4,5,6,7,8,9,10], &[0,1,2,3,4,5,6,7,8,9,10,0,1,0], Some(Ordering::Less));\n+        assert_order(&[0,1,2,3,4,5,6,7,8,9,11], &[0,1,2,3,4,5,6,7,8,9,10,0,0,0], Some(Ordering::Greater));\n+        assert_order(&[0,1,2,3,4,5,6,7,8,9,11], &[0,1,2,3,4,5,6,7,8,9,10,0,1,0], None);\n+        assert_order(&[0,1,2,3,4,5,6,7,8,9,9 ], &[0,1,2,3,4,5,6,7,8,9,10,0,0,0], Some(Ordering::Less));\n+        assert_order(&[0,1,2,3,4,5,6,7,8,9,9 ], &[0,1,2,3,4,5,6,7,8,9,10,0,1,0], Some(Ordering::Less));\n+    }\n+\n+    fn from_slice(mut slice: &[VTimestamp]) -> VClock {\n+        while let Some(0) = slice.last() {\n+            slice = &slice[..slice.len() - 1]\n+        }\n+        VClock(smallvec::SmallVec::from_slice(slice))\n+    }\n+\n+    fn assert_order(l: &[VTimestamp], r: &[VTimestamp], o: Option<Ordering>) {\n+        let l = from_slice(l);\n+        let r = from_slice(r);\n+\n+        //Test partial_cmp\n+        let compare = l.partial_cmp(&r);\n+        assert_eq!(compare, o, \"Invalid comparison\\n l: {:?}\\n r: {:?}\",l,r);\n+        let alt_compare = r.partial_cmp(&l);\n+        assert_eq!(alt_compare, o.map(Ordering::reverse), \"Invalid alt comparison\\n l: {:?}\\n r: {:?}\",l,r);\n+\n+        //Test operatorsm with faster implementations\n+        assert_eq!(\n+            matches!(compare,Some(Ordering::Less)), l < r,\n+            \"Invalid (<):\\n l: {:?}\\n r: {:?}\",l,r\n+        );\n+        assert_eq!(\n+            matches!(compare,Some(Ordering::Less) | Some(Ordering::Equal)), l <= r,\n+            \"Invalid (<=):\\n l: {:?}\\n r: {:?}\",l,r\n+        );\n+        assert_eq!(\n+            matches!(compare,Some(Ordering::Greater)), l > r,\n+            \"Invalid (>):\\n l: {:?}\\n r: {:?}\",l,r\n+        );\n+        assert_eq!(\n+            matches!(compare,Some(Ordering::Greater) | Some(Ordering::Equal)), l >= r,\n+            \"Invalid (>=):\\n l: {:?}\\n r: {:?}\",l,r\n+        );\n+        assert_eq!(\n+            matches!(alt_compare,Some(Ordering::Less)), r < l,\n+            \"Invalid alt (<):\\n l: {:?}\\n r: {:?}\",l,r\n+        );\n+        assert_eq!(\n+            matches!(alt_compare,Some(Ordering::Less) | Some(Ordering::Equal)), r <= l,\n+            \"Invalid alt (<=):\\n l: {:?}\\n r: {:?}\",l,r\n+        );\n+        assert_eq!(\n+            matches!(alt_compare,Some(Ordering::Greater)), r > l,\n+            \"Invalid alt (>):\\n l: {:?}\\n r: {:?}\",l,r\n+        );\n+        assert_eq!(\n+            matches!(alt_compare,Some(Ordering::Greater) | Some(Ordering::Equal)), r >= l,\n+            \"Invalid alt (>=):\\n l: {:?}\\n r: {:?}\",l,r\n+        );\n+    }\n+\n+    #[test]\n+    pub fn test_vclock_set() {\n+        let mut set = VSmallClockSet::default();\n+        let v1 = from_slice(&[3,0,1]);\n+        let v2 = from_slice(&[4,2,3]);\n+        let v3 = from_slice(&[4,8,3]);\n+        set.insert(VectorIdx(0), &v1);\n+        assert_eq!(set.get(VectorIdx(0)), Some(&v1));\n+        set.insert(VectorIdx(5), &v2);\n+        assert_eq!(set.get(VectorIdx(0)), Some(&v1));\n+        assert_eq!(set.get(VectorIdx(5)), Some(&v2));\n+        set.insert(VectorIdx(53), &v3);\n+        assert_eq!(set.get(VectorIdx(0)), Some(&v1));\n+        assert_eq!(set.get(VectorIdx(5)), Some(&v2));\n+        assert_eq!(set.get(VectorIdx(53)), Some(&v3));\n+        set.retain_index(VectorIdx(53));\n+        assert_eq!(set.get(VectorIdx(0)), None);\n+        assert_eq!(set.get(VectorIdx(5)), None);\n+        assert_eq!(set.get(VectorIdx(53)), Some(&v3));\n+        set.clear();\n+        assert_eq!(set.get(VectorIdx(0)), None);\n+        assert_eq!(set.get(VectorIdx(5)), None);\n+        assert_eq!(set.get(VectorIdx(53)), None);\n+        set.insert(VectorIdx(53), &v3);\n+        assert_eq!(set.get(VectorIdx(0)), None);\n+        assert_eq!(set.get(VectorIdx(5)), None);\n+        assert_eq!(set.get(VectorIdx(53)), Some(&v3));\n+    }\n+}"}]}