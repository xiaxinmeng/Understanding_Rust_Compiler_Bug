{"sha": "141319a412ddca4a2b16f45eed459c4a735e8f8f", "node_id": "MDY6Q29tbWl0NzI0NzEyOjE0MTMxOWE0MTJkZGNhNGEyYjE2ZjQ1ZWVkNDU5YzRhNzM1ZThmOGY=", "commit": {"author": {"name": "David Cook", "email": "divergentdave@gmail.com", "date": "2020-03-22T20:18:02Z"}, "committer": {"name": "David Cook", "email": "divergentdave@gmail.com", "date": "2020-04-05T15:07:39Z"}, "message": "Refactor sync shims with setters and getters", "tree": {"sha": "d7442232532624dc7a991ecab2217c7b1c58787c", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/d7442232532624dc7a991ecab2217c7b1c58787c"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/141319a412ddca4a2b16f45eed459c4a735e8f8f", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/141319a412ddca4a2b16f45eed459c4a735e8f8f", "html_url": "https://github.com/rust-lang/rust/commit/141319a412ddca4a2b16f45eed459c4a735e8f8f", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/141319a412ddca4a2b16f45eed459c4a735e8f8f/comments", "author": {"login": "divergentdave", "id": 181772, "node_id": "MDQ6VXNlcjE4MTc3Mg==", "avatar_url": "https://avatars.githubusercontent.com/u/181772?v=4", "gravatar_id": "", "url": "https://api.github.com/users/divergentdave", "html_url": "https://github.com/divergentdave", "followers_url": "https://api.github.com/users/divergentdave/followers", "following_url": "https://api.github.com/users/divergentdave/following{/other_user}", "gists_url": "https://api.github.com/users/divergentdave/gists{/gist_id}", "starred_url": "https://api.github.com/users/divergentdave/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/divergentdave/subscriptions", "organizations_url": "https://api.github.com/users/divergentdave/orgs", "repos_url": "https://api.github.com/users/divergentdave/repos", "events_url": "https://api.github.com/users/divergentdave/events{/privacy}", "received_events_url": "https://api.github.com/users/divergentdave/received_events", "type": "User", "site_admin": false}, "committer": {"login": "divergentdave", "id": 181772, "node_id": "MDQ6VXNlcjE4MTc3Mg==", "avatar_url": "https://avatars.githubusercontent.com/u/181772?v=4", "gravatar_id": "", "url": "https://api.github.com/users/divergentdave", "html_url": "https://github.com/divergentdave", "followers_url": "https://api.github.com/users/divergentdave/followers", "following_url": "https://api.github.com/users/divergentdave/following{/other_user}", "gists_url": "https://api.github.com/users/divergentdave/gists{/gist_id}", "starred_url": "https://api.github.com/users/divergentdave/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/divergentdave/subscriptions", "organizations_url": "https://api.github.com/users/divergentdave/orgs", "repos_url": "https://api.github.com/users/divergentdave/repos", "events_url": "https://api.github.com/users/divergentdave/events{/privacy}", "received_events_url": "https://api.github.com/users/divergentdave/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "fd94255b9d4ab69b110bb5d2acef5c288fe4a0e1", "url": "https://api.github.com/repos/rust-lang/rust/commits/fd94255b9d4ab69b110bb5d2acef5c288fe4a0e1", "html_url": "https://github.com/rust-lang/rust/commit/fd94255b9d4ab69b110bb5d2acef5c288fe4a0e1"}], "stats": {"total": 400, "additions": 219, "deletions": 181}, "files": [{"sha": "b8d9a88865e67dd9098a3c2f23c7db650b0afe15", "filename": "src/shims/sync.rs", "status": "modified", "additions": 219, "deletions": 181, "changes": 400, "blob_url": "https://github.com/rust-lang/rust/blob/141319a412ddca4a2b16f45eed459c4a735e8f8f/src%2Fshims%2Fsync.rs", "raw_url": "https://github.com/rust-lang/rust/raw/141319a412ddca4a2b16f45eed459c4a735e8f8f/src%2Fshims%2Fsync.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Fshims%2Fsync.rs?ref=141319a412ddca4a2b16f45eed459c4a735e8f8f", "patch": "@@ -6,26 +6,16 @@ use crate::*;\n \n impl<'mir, 'tcx> EvalContextExt<'mir, 'tcx> for crate::MiriEvalContext<'mir, 'tcx> {}\n pub trait EvalContextExt<'mir, 'tcx: 'mir>: crate::MiriEvalContextExt<'mir, 'tcx> {\n-    // pthread_mutexattr_t is either 4 or 8 bytes, depending on the platform\n-    // memory layout: store an i32 in the first four bytes equal to the\n-    // corresponding libc mutex kind constant (i.e. PTHREAD_MUTEX_NORMAL)\n-\n     fn pthread_mutexattr_init(&mut self, attr_op: OpTy<'tcx, Tag>) -> InterpResult<'tcx, i32> {\n         let this = self.eval_context_mut();\n \n-        // Ensure that the following write at an offset to the attr pointer is within bounds\n-        assert_ptr_target_min_size(this, attr_op, 4)?;\n-\n         let attr = this.read_scalar(attr_op)?.not_undef()?;\n         if this.is_null(attr)? {\n             return this.eval_libc_i32(\"EINVAL\");\n         }\n \n-        let attr_place = this.deref_operand(attr_op)?;\n-        let i32_layout = this.layout_of(this.tcx.types.i32)?;\n-        let kind_place = attr_place.offset(Size::ZERO, MemPlaceMeta::None, i32_layout, this)?;\n         let default_kind = this.eval_libc(\"PTHREAD_MUTEX_DEFAULT\")?;\n-        this.write_scalar(default_kind, kind_place.into())?;\n+        mutexattr_set_kind(this, attr_op, default_kind)?;\n \n         Ok(0)\n     }\n@@ -37,22 +27,17 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: crate::MiriEvalContextExt<'mir, 'tcx\n     ) -> InterpResult<'tcx, i32> {\n         let this = self.eval_context_mut();\n \n-        // Ensure that the following write at an offset to the attr pointer is within bounds\n-        assert_ptr_target_min_size(this, attr_op, 4)?;\n-\n         let attr = this.read_scalar(attr_op)?.not_undef()?;\n         if this.is_null(attr)? {\n             return this.eval_libc_i32(\"EINVAL\");\n         }\n \n         let kind = this.read_scalar(kind_op)?.not_undef()?;\n-        if kind == this.eval_libc(\"PTHREAD_MUTEX_NORMAL\")? ||\n-                kind == this.eval_libc(\"PTHREAD_MUTEX_ERRORCHECK\")? ||\n-                kind == this.eval_libc(\"PTHREAD_MUTEX_RECURSIVE\")? {\n-            let attr_place = this.deref_operand(attr_op)?;\n-            let i32_layout = this.layout_of(this.tcx.types.i32)?;\n-            let kind_place = attr_place.offset(Size::ZERO, MemPlaceMeta::None, i32_layout, this)?;\n-            this.write_scalar(kind, kind_place.into())?;\n+        if kind == this.eval_libc(\"PTHREAD_MUTEX_NORMAL\")?\n+            || kind == this.eval_libc(\"PTHREAD_MUTEX_ERRORCHECK\")?\n+            || kind == this.eval_libc(\"PTHREAD_MUTEX_RECURSIVE\")?\n+        {\n+            mutexattr_set_kind(this, attr_op, kind)?;\n         } else {\n             let einval = this.eval_libc_i32(\"EINVAL\")?;\n             return Ok(einval);\n@@ -64,105 +49,68 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: crate::MiriEvalContextExt<'mir, 'tcx\n     fn pthread_mutexattr_destroy(&mut self, attr_op: OpTy<'tcx, Tag>) -> InterpResult<'tcx, i32> {\n         let this = self.eval_context_mut();\n \n-        // Ensure that the following write at an offset to the attr pointer is within bounds\n-        assert_ptr_target_min_size(this, attr_op, 4)?;\n-\n         let attr = this.read_scalar(attr_op)?.not_undef()?;\n         if this.is_null(attr)? {\n             return this.eval_libc_i32(\"EINVAL\");\n         }\n \n-        let attr_place = this.deref_operand(attr_op)?;\n-        let i32_layout = this.layout_of(this.tcx.types.i32)?;\n-        let kind_place = attr_place.offset(Size::ZERO, MemPlaceMeta::None, i32_layout, this)?;\n-        this.write_scalar(ScalarMaybeUndef::Undef, kind_place.into())?;\n+        mutexattr_set_kind(this, attr_op, ScalarMaybeUndef::Undef)?;\n \n         Ok(0)\n     }\n \n-    // pthread_mutex_t is between 24 and 48 bytes, depending on the platform\n-    // memory layout:\n-    // bytes 0-3: reserved for signature on macOS\n-    // bytes 4-7: count of how many times this mutex has been locked, as a u32\n-    // bytes 12-15: mutex kind, as an i32\n-    // (the kind should be at this offset for compatibility with the static\n-    // initializer macro)\n-\n     fn pthread_mutex_init(\n         &mut self,\n         mutex_op: OpTy<'tcx, Tag>,\n         attr_op: OpTy<'tcx, Tag>,\n     ) -> InterpResult<'tcx, i32> {\n         let this = self.eval_context_mut();\n \n-        // Ensure that the following writes at offsets to the mutex pointer are within bounds\n-        assert_ptr_target_min_size(this, mutex_op, 16)?;\n-        // Ensure that the following read at an offset to the attr pointer is within bounds\n-        assert_ptr_target_min_size(this, attr_op, 4)?;\n-\n         let mutex = this.read_scalar(mutex_op)?.not_undef()?;\n         if this.is_null(mutex)? {\n             return this.eval_libc_i32(\"EINVAL\");\n         }\n-        let mutex_place = this.deref_operand(mutex_op)?;\n-\n-        let i32_layout = this.layout_of(this.tcx.types.i32)?;\n \n         let attr = this.read_scalar(attr_op)?.not_undef()?;\n         let kind = if this.is_null(attr)? {\n             this.eval_libc(\"PTHREAD_MUTEX_DEFAULT\")?\n         } else {\n-            let attr_place = this.deref_operand(attr_op)?;\n-            let attr_kind_place = attr_place.offset(Size::ZERO, MemPlaceMeta::None, i32_layout, this)?;\n-            this.read_scalar(attr_kind_place.into())?.not_undef()?\n+            mutexattr_get_kind(this, attr_op)?.not_undef()?\n         };\n \n-        let u32_layout = this.layout_of(this.tcx.types.u32)?;\n-        let locked_count_place = mutex_place.offset(Size::from_bytes(4), MemPlaceMeta::None, u32_layout, this)?;\n-        this.write_scalar(Scalar::from_u32(0), locked_count_place.into())?;\n-\n-        let mutex_kind_place = mutex_place.offset(Size::from_bytes(12), MemPlaceMeta::None, i32_layout, &*this.tcx)?;\n-        this.write_scalar(kind, mutex_kind_place.into())?;\n+        mutex_set_locked_count(this, mutex_op, Scalar::from_u32(0))?;\n+        mutex_set_kind(this, mutex_op, kind)?;\n \n         Ok(0)\n     }\n \n     fn pthread_mutex_lock(&mut self, mutex_op: OpTy<'tcx, Tag>) -> InterpResult<'tcx, i32> {\n         let this = self.eval_context_mut();\n \n-        // Ensure that the following reads and writes at offsets to the mutex pointer are within bounds\n-        assert_ptr_target_min_size(this, mutex_op, 16)?;\n-\n         let mutex = this.read_scalar(mutex_op)?.not_undef()?;\n         if this.is_null(mutex)? {\n             return this.eval_libc_i32(\"EINVAL\");\n         }\n-        let mutex_place = this.deref_operand(mutex_op)?;\n \n-        let i32_layout = this.layout_of(this.tcx.types.i32)?;\n-        let kind_place = mutex_place.offset(Size::from_bytes(12), MemPlaceMeta::None, i32_layout, this)?;\n-        let kind = this.read_scalar(kind_place.into())?.not_undef()?;\n-\n-        let u32_layout = this.layout_of(this.tcx.types.u32)?;\n-        let locked_count_place = mutex_place.offset(Size::from_bytes(4), MemPlaceMeta::None, u32_layout, this)?;\n-        let locked_count = this.read_scalar(locked_count_place.into())?.to_u32()?;\n+        let kind = mutex_get_kind(this, mutex_op)?.not_undef()?;\n+        let locked_count = mutex_get_locked_count(this, mutex_op)?.to_u32()?;\n \n         if kind == this.eval_libc(\"PTHREAD_MUTEX_NORMAL\")? {\n             if locked_count == 0 {\n-                this.write_scalar(Scalar::from_u32(1), locked_count_place.into())?;\n+                mutex_set_locked_count(this, mutex_op, Scalar::from_u32(1))?;\n                 Ok(0)\n             } else {\n                 throw_unsup_format!(\"Deadlock due to locking a PTHREAD_MUTEX_NORMAL mutex twice\");\n             }\n         } else if kind == this.eval_libc(\"PTHREAD_MUTEX_ERRORCHECK\")? {\n             if locked_count == 0 {\n-                this.write_scalar(Scalar::from_u32(1), locked_count_place.into())?;\n+                mutex_set_locked_count(this, mutex_op, Scalar::from_u32(1))?;\n                 Ok(0)\n             } else {\n                 this.eval_libc_i32(\"EDEADLK\")\n             }\n         } else if kind == this.eval_libc(\"PTHREAD_MUTEX_RECURSIVE\")? {\n-            this.write_scalar(Scalar::from_u32(locked_count + 1), locked_count_place.into())?;\n+            mutex_set_locked_count(this, mutex_op, Scalar::from_u32(locked_count + 1))?;\n             Ok(0)\n         } else {\n             this.eval_libc_i32(\"EINVAL\")\n@@ -172,33 +120,25 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: crate::MiriEvalContextExt<'mir, 'tcx\n     fn pthread_mutex_trylock(&mut self, mutex_op: OpTy<'tcx, Tag>) -> InterpResult<'tcx, i32> {\n         let this = self.eval_context_mut();\n \n-        // Ensure that the following reads and writes at offsets to the mutex pointer are within bounds\n-        assert_ptr_target_min_size(this, mutex_op, 16)?;\n-\n         let mutex = this.read_scalar(mutex_op)?.not_undef()?;\n         if this.is_null(mutex)? {\n             return this.eval_libc_i32(\"EINVAL\");\n         }\n-        let mutex_place = this.deref_operand(mutex_op)?;\n \n-        let i32_layout = this.layout_of(this.tcx.types.i32)?;\n-        let kind_place = mutex_place.offset(Size::from_bytes(12), MemPlaceMeta::None, i32_layout, this)?;\n-        let kind = this.read_scalar(kind_place.into())?.not_undef()?;\n+        let kind = mutex_get_kind(this, mutex_op)?.not_undef()?;\n+        let locked_count = mutex_get_locked_count(this, mutex_op)?.to_u32()?;\n \n-        let u32_layout = this.layout_of(this.tcx.types.u32)?;\n-        let locked_count_place = mutex_place.offset(Size::from_bytes(4), MemPlaceMeta::None, u32_layout, this)?;\n-        let locked_count = this.read_scalar(locked_count_place.into())?.to_u32()?;\n-\n-        if kind == this.eval_libc(\"PTHREAD_MUTEX_NORMAL\")? ||\n-                kind == this.eval_libc(\"PTHREAD_MUTEX_ERRORCHECK\")? {\n+        if kind == this.eval_libc(\"PTHREAD_MUTEX_NORMAL\")?\n+            || kind == this.eval_libc(\"PTHREAD_MUTEX_ERRORCHECK\")?\n+        {\n             if locked_count == 0 {\n-                this.write_scalar(Scalar::from_u32(1), locked_count_place.into())?;\n+                mutex_set_locked_count(this, mutex_op, Scalar::from_u32(1))?;\n                 Ok(0)\n             } else {\n                 this.eval_libc_i32(\"EBUSY\")\n             }\n         } else if kind == this.eval_libc(\"PTHREAD_MUTEX_RECURSIVE\")? {\n-            this.write_scalar(Scalar::from_u32(locked_count + 1), locked_count_place.into())?;\n+            mutex_set_locked_count(this, mutex_op, Scalar::from_u32(locked_count + 1))?;\n             Ok(0)\n         } else {\n             this.eval_libc_i32(\"EINVAL\")\n@@ -208,40 +148,33 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: crate::MiriEvalContextExt<'mir, 'tcx\n     fn pthread_mutex_unlock(&mut self, mutex_op: OpTy<'tcx, Tag>) -> InterpResult<'tcx, i32> {\n         let this = self.eval_context_mut();\n \n-        // Ensure that the following reads and writes at offsets to the mutex pointer are within bounds\n-        assert_ptr_target_min_size(this, mutex_op, 16)?;\n-\n         let mutex = this.read_scalar(mutex_op)?.not_undef()?;\n         if this.is_null(mutex)? {\n             return this.eval_libc_i32(\"EINVAL\");\n         }\n-        let mutex_place = this.deref_operand(mutex_op)?;\n-\n-        let i32_layout = this.layout_of(this.tcx.types.i32)?;\n-        let kind_place = mutex_place.offset(Size::from_bytes(12), MemPlaceMeta::None, i32_layout, this)?;\n-        let kind = this.read_scalar(kind_place.into())?.not_undef()?;\n \n-        let u32_layout = this.layout_of(this.tcx.types.u32)?;\n-        let locked_count_place = mutex_place.offset(Size::from_bytes(4), MemPlaceMeta::None, u32_layout, this)?;\n-        let locked_count = this.read_scalar(locked_count_place.into())?.to_u32()?;\n+        let kind = mutex_get_kind(this, mutex_op)?.not_undef()?;\n+        let locked_count = mutex_get_locked_count(this, mutex_op)?.to_u32()?;\n \n         if kind == this.eval_libc(\"PTHREAD_MUTEX_NORMAL\")? {\n             if locked_count == 1 {\n-                this.write_scalar(Scalar::from_u32(0), locked_count_place.into())?;\n+                mutex_set_locked_count(this, mutex_op, Scalar::from_u32(0))?;\n                 Ok(0)\n             } else {\n-                throw_ub_format!(\"Attempted to unlock a PTHREAD_MUTEX_NORMAL mutex that was not locked\");\n+                throw_ub_format!(\n+                    \"Attempted to unlock a PTHREAD_MUTEX_NORMAL mutex that was not locked\"\n+                );\n             }\n         } else if kind == this.eval_libc(\"PTHREAD_MUTEX_ERRORCHECK\")? {\n             if locked_count == 1 {\n-                this.write_scalar(Scalar::from_u32(0), locked_count_place.into())?;\n+                mutex_set_locked_count(this, mutex_op, Scalar::from_u32(0))?;\n                 Ok(0)\n             } else {\n                 this.eval_libc_i32(\"EPERM\")\n             }\n         } else if kind == this.eval_libc(\"PTHREAD_MUTEX_RECURSIVE\")? {\n             if locked_count > 0 {\n-                this.write_scalar(Scalar::from_u32(locked_count - 1), locked_count_place.into())?;\n+                mutex_set_locked_count(this, mutex_op, Scalar::from_u32(locked_count - 1))?;\n                 Ok(0)\n             } else {\n                 this.eval_libc_i32(\"EPERM\")\n@@ -254,159 +187,116 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: crate::MiriEvalContextExt<'mir, 'tcx\n     fn pthread_mutex_destroy(&mut self, mutex_op: OpTy<'tcx, Tag>) -> InterpResult<'tcx, i32> {\n         let this = self.eval_context_mut();\n \n-        // Ensure that the following read and writes at offsets to the mutex pointer are within bounds\n-        assert_ptr_target_min_size(this, mutex_op, 16)?;\n-\n         let mutex = this.read_scalar(mutex_op)?.not_undef()?;\n         if this.is_null(mutex)? {\n             return this.eval_libc_i32(\"EINVAL\");\n         }\n-        let mutex_place = this.deref_operand(mutex_op)?;\n \n-        let u32_layout = this.layout_of(this.tcx.types.u32)?;\n-        let locked_count_place = mutex_place.offset(Size::from_bytes(4), MemPlaceMeta::None, u32_layout, this)?;\n-        if this.read_scalar(locked_count_place.into())?.to_u32()? != 0 {\n+        if mutex_get_locked_count(this, mutex_op)?.to_u32()? != 0 {\n             return this.eval_libc_i32(\"EBUSY\");\n         }\n \n-        let i32_layout = this.layout_of(this.tcx.types.i32)?;\n-        let kind_place = mutex_place.offset(Size::from_bytes(12), MemPlaceMeta::None, i32_layout, this)?;\n-        this.write_scalar(ScalarMaybeUndef::Undef, kind_place.into())?;\n-        this.write_scalar(ScalarMaybeUndef::Undef, locked_count_place.into())?;\n+        mutex_set_kind(this, mutex_op, ScalarMaybeUndef::Undef)?;\n+        mutex_set_locked_count(this, mutex_op, ScalarMaybeUndef::Undef)?;\n \n         Ok(0)\n     }\n \n-    // pthread_rwlock_t is between 32 and 56 bytes, depending on the platform\n-    // memory layout:\n-    // bytes 0-3: reserved for signature on macOS\n-    // bytes 4-7: reader count, as a u32\n-    // bytes 8-11: writer count, as a u32\n-\n     fn pthread_rwlock_rdlock(&mut self, rwlock_op: OpTy<'tcx, Tag>) -> InterpResult<'tcx, i32> {\n         let this = self.eval_context_mut();\n \n-        // Ensure that the following reads and write at offsets to the rwlock pointer are within bounds\n-        assert_ptr_target_min_size(this, rwlock_op, 12)?;\n-\n         let rwlock = this.read_scalar(rwlock_op)?.not_undef()?;\n         if this.is_null(rwlock)? {\n             return this.eval_libc_i32(\"EINVAL\");\n         }\n-        let rwlock_place = this.deref_operand(rwlock_op)?;\n \n-        let u32_layout = this.layout_of(this.tcx.types.u32)?;\n-        let readers_place = rwlock_place.offset(Size::from_bytes(4), MemPlaceMeta::None, u32_layout, this)?;\n-        let writers_place = rwlock_place.offset(Size::from_bytes(8), MemPlaceMeta::None, u32_layout, this)?;\n-        let readers = this.read_scalar(readers_place.into())?.to_u32()?;\n-        let writers = this.read_scalar(writers_place.into())?.to_u32()?;\n+        let readers = rwlock_get_readers(this, rwlock_op)?.to_u32()?;\n+        let writers = rwlock_get_writers(this, rwlock_op)?.to_u32()?;\n         if writers != 0 {\n-            throw_unsup_format!(\"Deadlock due to read-locking a pthreads read-write lock while it is already write-locked\");\n+            throw_unsup_format!(\n+                \"Deadlock due to read-locking a pthreads read-write lock while it is already write-locked\"\n+            );\n         } else {\n-            this.write_scalar(Scalar::from_u32(readers + 1), readers_place.into())?;\n+            rwlock_set_readers(this, rwlock_op, Scalar::from_u32(readers + 1))?;\n             Ok(0)\n         }\n     }\n \n     fn pthread_rwlock_tryrdlock(&mut self, rwlock_op: OpTy<'tcx, Tag>) -> InterpResult<'tcx, i32> {\n         let this = self.eval_context_mut();\n \n-        // Ensure that the following reads and write at offsets to the rwlock pointer are within bounds\n-        assert_ptr_target_min_size(this, rwlock_op, 12)?;\n-\n         let rwlock = this.read_scalar(rwlock_op)?.not_undef()?;\n         if this.is_null(rwlock)? {\n             return this.eval_libc_i32(\"EINVAL\");\n         }\n-        let rwlock_place = this.deref_operand(rwlock_op)?;\n \n-        let u32_layout = this.layout_of(this.tcx.types.u32)?;\n-        let readers_place = rwlock_place.offset(Size::from_bytes(4), MemPlaceMeta::None, u32_layout, this)?;\n-        let writers_place = rwlock_place.offset(Size::from_bytes(8), MemPlaceMeta::None, u32_layout, this)?;\n-        let readers = this.read_scalar(readers_place.into())?.to_u32()?;\n-        let writers = this.read_scalar(writers_place.into())?.to_u32()?;\n+        let readers = rwlock_get_readers(this, rwlock_op)?.to_u32()?;\n+        let writers = rwlock_get_writers(this, rwlock_op)?.to_u32()?;\n         if writers != 0 {\n             this.eval_libc_i32(\"EBUSY\")\n         } else {\n-            this.write_scalar(Scalar::from_u32(readers + 1), readers_place.into())?;\n+            rwlock_set_readers(this, rwlock_op, Scalar::from_u32(readers + 1))?;\n             Ok(0)\n         }\n     }\n \n     fn pthread_rwlock_wrlock(&mut self, rwlock_op: OpTy<'tcx, Tag>) -> InterpResult<'tcx, i32> {\n         let this = self.eval_context_mut();\n \n-        // Ensure that the following reads and write at offsets to the rwlock pointer are within bounds\n-        assert_ptr_target_min_size(this, rwlock_op, 12)?;\n-\n         let rwlock = this.read_scalar(rwlock_op)?.not_undef()?;\n         if this.is_null(rwlock)? {\n             return this.eval_libc_i32(\"EINVAL\");\n         }\n-        let rwlock_place = this.deref_operand(rwlock_op)?;\n \n-        let u32_layout = this.layout_of(this.tcx.types.u32)?;\n-        let readers_place = rwlock_place.offset(Size::from_bytes(4), MemPlaceMeta::None, u32_layout, this)?;\n-        let writers_place = rwlock_place.offset(Size::from_bytes(8), MemPlaceMeta::None, u32_layout, this)?;\n-        let readers = this.read_scalar(readers_place.into())?.to_u32()?;\n-        let writers = this.read_scalar(writers_place.into())?.to_u32()?;\n+        let readers = rwlock_get_readers(this, rwlock_op)?.to_u32()?;\n+        let writers = rwlock_get_writers(this, rwlock_op)?.to_u32()?;\n         if readers != 0 {\n-            throw_unsup_format!(\"Deadlock due to write-locking a pthreads read-write lock while it is already read-locked\");\n+            throw_unsup_format!(\n+                \"Deadlock due to write-locking a pthreads read-write lock while it is already read-locked\"\n+            );\n         } else if writers != 0 {\n-            throw_unsup_format!(\"Deadlock due to write-locking a pthreads read-write lock while it is already write-locked\");\n+            throw_unsup_format!(\n+                \"Deadlock due to write-locking a pthreads read-write lock while it is already write-locked\"\n+            );\n         } else {\n-            this.write_scalar(Scalar::from_u32(1), writers_place.into())?;\n+            rwlock_set_writers(this, rwlock_op, Scalar::from_u32(1))?;\n             Ok(0)\n         }\n     }\n \n     fn pthread_rwlock_trywrlock(&mut self, rwlock_op: OpTy<'tcx, Tag>) -> InterpResult<'tcx, i32> {\n         let this = self.eval_context_mut();\n \n-        // Ensure that the following reads and write at offsets to the rwlock pointer are within bounds\n-        assert_ptr_target_min_size(this, rwlock_op, 12)?;\n-\n         let rwlock = this.read_scalar(rwlock_op)?.not_undef()?;\n         if this.is_null(rwlock)? {\n             return this.eval_libc_i32(\"EINVAL\");\n         }\n-        let rwlock_place = this.deref_operand(rwlock_op)?;\n \n-        let u32_layout = this.layout_of(this.tcx.types.u32)?;\n-        let readers_place = rwlock_place.offset(Size::from_bytes(4), MemPlaceMeta::None, u32_layout, this)?;\n-        let writers_place = rwlock_place.offset(Size::from_bytes(8), MemPlaceMeta::None, u32_layout, this)?;\n-        let readers = this.read_scalar(readers_place.into())?.to_u32()?;\n-        let writers = this.read_scalar(writers_place.into())?.to_u32()?;\n+        let readers = rwlock_get_readers(this, rwlock_op)?.to_u32()?;\n+        let writers = rwlock_get_writers(this, rwlock_op)?.to_u32()?;\n         if readers != 0 || writers != 0 {\n             this.eval_libc_i32(\"EBUSY\")\n         } else {\n-            this.write_scalar(Scalar::from_u32(1), writers_place.into())?;\n+            rwlock_set_writers(this, rwlock_op, Scalar::from_u32(1))?;\n             Ok(0)\n         }\n     }\n \n     fn pthread_rwlock_unlock(&mut self, rwlock_op: OpTy<'tcx, Tag>) -> InterpResult<'tcx, i32> {\n         let this = self.eval_context_mut();\n \n-        // Ensure that the following reads and writes at offsets to the rwlock pointer are within bounds\n-        assert_ptr_target_min_size(this, rwlock_op, 12)?;\n-\n         let rwlock = this.read_scalar(rwlock_op)?.not_undef()?;\n         if this.is_null(rwlock)? {\n             return this.eval_libc_i32(\"EINVAL\");\n         }\n-        let rwlock_place = this.deref_operand(rwlock_op)?;\n \n-        let u32_layout = this.layout_of(this.tcx.types.u32)?;\n-        let readers_place = rwlock_place.offset(Size::from_bytes(4), MemPlaceMeta::None, u32_layout, this)?;\n-        let writers_place = rwlock_place.offset(Size::from_bytes(8), MemPlaceMeta::None, u32_layout, this)?;\n-        let readers = this.read_scalar(readers_place.into())?.to_u32()?;\n-        let writers = this.read_scalar(writers_place.into())?.to_u32()?;\n+        let readers = rwlock_get_readers(this, rwlock_op)?.to_u32()?;\n+        let writers = rwlock_get_writers(this, rwlock_op)?.to_u32()?;\n         if readers != 0 {\n-            this.write_scalar(Scalar::from_u32(readers - 1), readers_place.into())?;\n+            rwlock_set_readers(this, rwlock_op, Scalar::from_u32(readers - 1))?;\n             Ok(0)\n         } else if writers != 0 {\n-            this.write_scalar(Scalar::from_u32(0), writers_place.into())?;\n+            rwlock_set_writers(this, rwlock_op, Scalar::from_u32(0))?;\n             Ok(0)\n         } else {\n             this.eval_libc_i32(\"EPERM\")\n@@ -416,38 +306,186 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: crate::MiriEvalContextExt<'mir, 'tcx\n     fn pthread_rwlock_destroy(&mut self, rwlock_op: OpTy<'tcx, Tag>) -> InterpResult<'tcx, i32> {\n         let this = self.eval_context_mut();\n \n-        // Ensure that the following reads and writes at offsets to the rwlock pointer are within bounds\n-        assert_ptr_target_min_size(this, rwlock_op, 12)?;\n-\n         let rwlock = this.read_scalar(rwlock_op)?.not_undef()?;\n         if this.is_null(rwlock)? {\n             return this.eval_libc_i32(\"EINVAL\");\n         }\n-        let rwlock_place = this.deref_operand(rwlock_op)?;\n \n-        let u32_layout = this.layout_of(this.tcx.types.u32)?;\n-        let readers_place = rwlock_place.offset(Size::from_bytes(4), MemPlaceMeta::None, u32_layout, this)?;\n-        if this.read_scalar(readers_place.into())?.to_u32()? != 0 {\n+        if rwlock_get_readers(this, rwlock_op)?.to_u32()? != 0 {\n             return this.eval_libc_i32(\"EBUSY\");\n         }\n-        let writers_place = rwlock_place.offset(Size::from_bytes(8), MemPlaceMeta::None, u32_layout, this)?;\n-        if this.read_scalar(writers_place.into())?.to_u32()? != 0 {\n+        if rwlock_get_writers(this, rwlock_op)?.to_u32()? != 0 {\n             return this.eval_libc_i32(\"EBUSY\");\n         }\n \n-        this.write_scalar(ScalarMaybeUndef::Undef, readers_place.into())?;\n-        this.write_scalar(ScalarMaybeUndef::Undef, writers_place.into())?;\n+        rwlock_set_readers(this, rwlock_op, ScalarMaybeUndef::Undef)?;\n+        rwlock_set_writers(this, rwlock_op, ScalarMaybeUndef::Undef)?;\n \n         Ok(0)\n     }\n }\n \n-fn assert_ptr_target_min_size<'mir, 'tcx: 'mir>(ecx: &MiriEvalContext<'mir, 'tcx>, operand: OpTy<'tcx, Tag>, min_size: u64) -> InterpResult<'tcx, ()> {\n+fn assert_ptr_target_min_size<'mir, 'tcx: 'mir>(\n+    ecx: &MiriEvalContext<'mir, 'tcx>,\n+    operand: OpTy<'tcx, Tag>,\n+    min_size: u64,\n+) -> InterpResult<'tcx, ()> {\n     let target_ty = match operand.layout.ty.kind {\n-        TyKind::RawPtr(TypeAndMut{ ty, mutbl: _ }) => ty,\n+        TyKind::RawPtr(TypeAndMut { ty, mutbl: _ }) => ty,\n         _ => panic!(\"Argument to pthread function was not a raw pointer\"),\n     };\n     let target_layout = ecx.layout_of(target_ty)?;\n     assert!(target_layout.size.bytes() >= min_size);\n     Ok(())\n }\n+\n+// pthread_mutexattr_t is either 4 or 8 bytes, depending on the platform\n+// memory layout: store an i32 in the first four bytes equal to the\n+// corresponding libc mutex kind constant (i.e. PTHREAD_MUTEX_NORMAL)\n+\n+fn mutexattr_get_kind<'mir, 'tcx: 'mir>(\n+    ecx: &MiriEvalContext<'mir, 'tcx>,\n+    attr_op: OpTy<'tcx, Tag>,\n+) -> InterpResult<'tcx, ScalarMaybeUndef<Tag>> {\n+    // Ensure that the following read at an offset to the attr pointer is within bounds\n+    assert_ptr_target_min_size(ecx, attr_op, 4)?;\n+    let attr_place = ecx.deref_operand(attr_op)?;\n+    let i32_layout = ecx.layout_of(ecx.tcx.types.i32)?;\n+    let kind_place = attr_place.offset(Size::ZERO, MemPlaceMeta::None, i32_layout, ecx)?;\n+    ecx.read_scalar(kind_place.into())\n+}\n+\n+fn mutexattr_set_kind<'mir, 'tcx: 'mir>(\n+    ecx: &mut MiriEvalContext<'mir, 'tcx>,\n+    attr_op: OpTy<'tcx, Tag>,\n+    kind: impl Into<ScalarMaybeUndef<Tag>>,\n+) -> InterpResult<'tcx, ()> {\n+    // Ensure that the following write at an offset to the attr pointer is within bounds\n+    assert_ptr_target_min_size(ecx, attr_op, 4)?;\n+    let attr_place = ecx.deref_operand(attr_op)?;\n+    let i32_layout = ecx.layout_of(ecx.tcx.types.i32)?;\n+    let kind_place = attr_place.offset(Size::ZERO, MemPlaceMeta::None, i32_layout, ecx)?;\n+    ecx.write_scalar(kind.into(), kind_place.into())\n+}\n+\n+// pthread_mutex_t is between 24 and 48 bytes, depending on the platform\n+// memory layout:\n+// bytes 0-3: reserved for signature on macOS\n+// bytes 4-7: count of how many times this mutex has been locked, as a u32\n+// bytes 12-15: mutex kind, as an i32\n+// (the kind should be at this offset for compatibility with the static\n+// initializer macro)\n+\n+fn mutex_get_locked_count<'mir, 'tcx: 'mir>(\n+    ecx: &MiriEvalContext<'mir, 'tcx>,\n+    mutex_op: OpTy<'tcx, Tag>,\n+) -> InterpResult<'tcx, ScalarMaybeUndef<Tag>> {\n+    // Ensure that the following read at an offset to the mutex pointer is within bounds\n+    assert_ptr_target_min_size(ecx, mutex_op, 16)?;\n+    let mutex_place = ecx.deref_operand(mutex_op)?;\n+    let u32_layout = ecx.layout_of(ecx.tcx.types.u32)?;\n+    let locked_count_place =\n+        mutex_place.offset(Size::from_bytes(4), MemPlaceMeta::None, u32_layout, ecx)?;\n+    ecx.read_scalar(locked_count_place.into())\n+}\n+\n+fn mutex_set_locked_count<'mir, 'tcx: 'mir>(\n+    ecx: &mut MiriEvalContext<'mir, 'tcx>,\n+    mutex_op: OpTy<'tcx, Tag>,\n+    locked_count: impl Into<ScalarMaybeUndef<Tag>>,\n+) -> InterpResult<'tcx, ()> {\n+    // Ensure that the following write at an offset to the mutex pointer is within bounds\n+    assert_ptr_target_min_size(ecx, mutex_op, 16)?;\n+    let mutex_place = ecx.deref_operand(mutex_op)?;\n+    let u32_layout = ecx.layout_of(ecx.tcx.types.u32)?;\n+    let locked_count_place =\n+        mutex_place.offset(Size::from_bytes(4), MemPlaceMeta::None, u32_layout, ecx)?;\n+    ecx.write_scalar(locked_count.into(), locked_count_place.into())\n+}\n+\n+fn mutex_get_kind<'mir, 'tcx: 'mir>(\n+    ecx: &MiriEvalContext<'mir, 'tcx>,\n+    mutex_op: OpTy<'tcx, Tag>,\n+) -> InterpResult<'tcx, ScalarMaybeUndef<Tag>> {\n+    // Ensure that the following read at an offset to the mutex pointer is within bounds\n+    assert_ptr_target_min_size(ecx, mutex_op, 16)?;\n+    let mutex_place = ecx.deref_operand(mutex_op)?;\n+    let i32_layout = ecx.layout_of(ecx.tcx.types.i32)?;\n+    let kind_place =\n+        mutex_place.offset(Size::from_bytes(12), MemPlaceMeta::None, i32_layout, ecx)?;\n+    ecx.read_scalar(kind_place.into())\n+}\n+\n+fn mutex_set_kind<'mir, 'tcx: 'mir>(\n+    ecx: &mut MiriEvalContext<'mir, 'tcx>,\n+    mutex_op: OpTy<'tcx, Tag>,\n+    kind: impl Into<ScalarMaybeUndef<Tag>>,\n+) -> InterpResult<'tcx, ()> {\n+    // Ensure that the following write at an offset to the mutex pointer is within bounds\n+    assert_ptr_target_min_size(ecx, mutex_op, 16)?;\n+    let mutex_place = ecx.deref_operand(mutex_op)?;\n+    let i32_layout = ecx.layout_of(ecx.tcx.types.i32)?;\n+    let kind_place =\n+        mutex_place.offset(Size::from_bytes(12), MemPlaceMeta::None, i32_layout, ecx)?;\n+    ecx.write_scalar(kind.into(), kind_place.into())\n+}\n+\n+// pthread_rwlock_t is between 32 and 56 bytes, depending on the platform\n+// memory layout:\n+// bytes 0-3: reserved for signature on macOS\n+// bytes 4-7: reader count, as a u32\n+// bytes 8-11: writer count, as a u32\n+\n+fn rwlock_get_readers<'mir, 'tcx: 'mir>(\n+    ecx: &MiriEvalContext<'mir, 'tcx>,\n+    rwlock_op: OpTy<'tcx, Tag>,\n+) -> InterpResult<'tcx, ScalarMaybeUndef<Tag>> {\n+    // Ensure that the following read at an offset to the rwlock pointer is within bounds\n+    assert_ptr_target_min_size(ecx, rwlock_op, 12)?;\n+    let rwlock_place = ecx.deref_operand(rwlock_op)?;\n+    let u32_layout = ecx.layout_of(ecx.tcx.types.u32)?;\n+    let readers_place =\n+        rwlock_place.offset(Size::from_bytes(4), MemPlaceMeta::None, u32_layout, ecx)?;\n+    ecx.read_scalar(readers_place.into())\n+}\n+\n+fn rwlock_set_readers<'mir, 'tcx: 'mir>(\n+    ecx: &mut MiriEvalContext<'mir, 'tcx>,\n+    rwlock_op: OpTy<'tcx, Tag>,\n+    readers: impl Into<ScalarMaybeUndef<Tag>>,\n+) -> InterpResult<'tcx, ()> {\n+    // Ensure that the following write at an offset to the rwlock pointer is within bounds\n+    assert_ptr_target_min_size(ecx, rwlock_op, 12)?;\n+    let rwlock_place = ecx.deref_operand(rwlock_op)?;\n+    let u32_layout = ecx.layout_of(ecx.tcx.types.u32)?;\n+    let readers_place =\n+        rwlock_place.offset(Size::from_bytes(4), MemPlaceMeta::None, u32_layout, ecx)?;\n+    ecx.write_scalar(readers.into(), readers_place.into())\n+}\n+\n+fn rwlock_get_writers<'mir, 'tcx: 'mir>(\n+    ecx: &MiriEvalContext<'mir, 'tcx>,\n+    rwlock_op: OpTy<'tcx, Tag>,\n+) -> InterpResult<'tcx, ScalarMaybeUndef<Tag>> {\n+    // Ensure that the following read at an offset to the rwlock pointer is within bounds\n+    assert_ptr_target_min_size(ecx, rwlock_op, 12)?;\n+    let rwlock_place = ecx.deref_operand(rwlock_op)?;\n+    let u32_layout = ecx.layout_of(ecx.tcx.types.u32)?;\n+    let writers_place =\n+        rwlock_place.offset(Size::from_bytes(8), MemPlaceMeta::None, u32_layout, ecx)?;\n+    ecx.read_scalar(writers_place.into())\n+}\n+\n+fn rwlock_set_writers<'mir, 'tcx: 'mir>(\n+    ecx: &mut MiriEvalContext<'mir, 'tcx>,\n+    rwlock_op: OpTy<'tcx, Tag>,\n+    writers: impl Into<ScalarMaybeUndef<Tag>>,\n+) -> InterpResult<'tcx, ()> {\n+    // Ensure that the following write at an offset to the rwlock pointer is within bounds\n+    assert_ptr_target_min_size(ecx, rwlock_op, 12)?;\n+    let rwlock_place = ecx.deref_operand(rwlock_op)?;\n+    let u32_layout = ecx.layout_of(ecx.tcx.types.u32)?;\n+    let writers_place =\n+        rwlock_place.offset(Size::from_bytes(8), MemPlaceMeta::None, u32_layout, ecx)?;\n+    ecx.write_scalar(writers.into(), writers_place.into())\n+}"}]}