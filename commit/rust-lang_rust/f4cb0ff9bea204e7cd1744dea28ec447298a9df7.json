{"sha": "f4cb0ff9bea204e7cd1744dea28ec447298a9df7", "node_id": "C_kwDOAAsO6NoAKGY0Y2IwZmY5YmVhMjA0ZTdjZDE3NDRkZWEyOGVjNDQ3Mjk4YTlkZjc", "commit": {"author": {"name": "Aleksey Kladov", "email": "aleksey.kladov@gmail.com", "date": "2021-12-26T13:47:10Z"}, "committer": {"name": "Aleksey Kladov", "email": "aleksey.kladov@gmail.com", "date": "2021-12-26T13:47:10Z"}, "message": "internal: move ws attachment logic to the parser crate\n\nThis has to re-introduce the `sink` pattern, because doing this purely\nwith iterators is awkward :( Maaaybe the event vector was a false start?\n\nBut, anyway, I like the current factoring more -- it sort-of obvious\nthat we do want to keep ws-attachment business in the parser, and that\nwe also don't want that to depend on the particular tree structure. I\nthink `shortcuts` module achieves that.", "tree": {"sha": "81ebf92da3327ebf4034c85df09f35dc89abd71c", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/81ebf92da3327ebf4034c85df09f35dc89abd71c"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/f4cb0ff9bea204e7cd1744dea28ec447298a9df7", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/f4cb0ff9bea204e7cd1744dea28ec447298a9df7", "html_url": "https://github.com/rust-lang/rust/commit/f4cb0ff9bea204e7cd1744dea28ec447298a9df7", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/f4cb0ff9bea204e7cd1744dea28ec447298a9df7/comments", "author": {"login": "matklad", "id": 1711539, "node_id": "MDQ6VXNlcjE3MTE1Mzk=", "avatar_url": "https://avatars.githubusercontent.com/u/1711539?v=4", "gravatar_id": "", "url": "https://api.github.com/users/matklad", "html_url": "https://github.com/matklad", "followers_url": "https://api.github.com/users/matklad/followers", "following_url": "https://api.github.com/users/matklad/following{/other_user}", "gists_url": "https://api.github.com/users/matklad/gists{/gist_id}", "starred_url": "https://api.github.com/users/matklad/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/matklad/subscriptions", "organizations_url": "https://api.github.com/users/matklad/orgs", "repos_url": "https://api.github.com/users/matklad/repos", "events_url": "https://api.github.com/users/matklad/events{/privacy}", "received_events_url": "https://api.github.com/users/matklad/received_events", "type": "User", "site_admin": false}, "committer": {"login": "matklad", "id": 1711539, "node_id": "MDQ6VXNlcjE3MTE1Mzk=", "avatar_url": "https://avatars.githubusercontent.com/u/1711539?v=4", "gravatar_id": "", "url": "https://api.github.com/users/matklad", "html_url": "https://github.com/matklad", "followers_url": "https://api.github.com/users/matklad/followers", "following_url": "https://api.github.com/users/matklad/following{/other_user}", "gists_url": "https://api.github.com/users/matklad/gists{/gist_id}", "starred_url": "https://api.github.com/users/matklad/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/matklad/subscriptions", "organizations_url": "https://api.github.com/users/matklad/orgs", "repos_url": "https://api.github.com/users/matklad/repos", "events_url": "https://api.github.com/users/matklad/events{/privacy}", "received_events_url": "https://api.github.com/users/matklad/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "c456b217d808058d2515dd909bc4f68206de340e", "url": "https://api.github.com/repos/rust-lang/rust/commits/c456b217d808058d2515dd909bc4f68206de340e", "html_url": "https://github.com/rust-lang/rust/commit/c456b217d808058d2515dd909bc4f68206de340e"}], "stats": {"total": 476, "additions": 255, "deletions": 221}, "files": [{"sha": "5a42c53c942f128077111b6b4835773eb756f7e4", "filename": "crates/parser/src/lexed_str.rs", "status": "modified", "additions": 0, "deletions": 25, "changes": 25, "blob_url": "https://github.com/rust-lang/rust/blob/f4cb0ff9bea204e7cd1744dea28ec447298a9df7/crates%2Fparser%2Fsrc%2Flexed_str.rs", "raw_url": "https://github.com/rust-lang/rust/raw/f4cb0ff9bea204e7cd1744dea28ec447298a9df7/crates%2Fparser%2Fsrc%2Flexed_str.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fparser%2Fsrc%2Flexed_str.rs?ref=f4cb0ff9bea204e7cd1744dea28ec447298a9df7", "patch": "@@ -122,31 +122,6 @@ impl<'a> LexedStr<'a> {\n         self.error.iter().map(|it| (it.token as usize, it.msg.as_str()))\n     }\n \n-    pub fn to_input(&self) -> crate::Input {\n-        let mut res = crate::Input::default();\n-        let mut was_joint = false;\n-        for i in 0..self.len() {\n-            let kind = self.kind(i);\n-            if kind.is_trivia() {\n-                was_joint = false\n-            } else {\n-                if kind == SyntaxKind::IDENT {\n-                    let token_text = self.text(i);\n-                    let contextual_kw = SyntaxKind::from_contextual_keyword(token_text)\n-                        .unwrap_or(SyntaxKind::IDENT);\n-                    res.push_ident(contextual_kw);\n-                } else {\n-                    if was_joint {\n-                        res.was_joint();\n-                    }\n-                    res.push(kind);\n-                }\n-                was_joint = true;\n-            }\n-        }\n-        res\n-    }\n-\n     fn push(&mut self, kind: SyntaxKind, offset: usize) {\n         self.kind.push(kind);\n         self.start.push(offset as u32);"}, {"sha": "841d2aa4e946056de3b8f238ecde2727e883ccf4", "filename": "crates/parser/src/lib.rs", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/f4cb0ff9bea204e7cd1744dea28ec447298a9df7/crates%2Fparser%2Fsrc%2Flib.rs", "raw_url": "https://github.com/rust-lang/rust/raw/f4cb0ff9bea204e7cd1744dea28ec447298a9df7/crates%2Fparser%2Fsrc%2Flib.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fparser%2Fsrc%2Flib.rs?ref=f4cb0ff9bea204e7cd1744dea28ec447298a9df7", "patch": "@@ -26,6 +26,7 @@ mod parser;\n mod grammar;\n mod input;\n mod output;\n+mod shortcuts;\n \n #[cfg(test)]\n mod tests;\n@@ -36,6 +37,7 @@ pub use crate::{\n     input::Input,\n     lexed_str::LexedStr,\n     output::{Output, Step},\n+    shortcuts::StrStep,\n     syntax_kind::SyntaxKind,\n };\n "}, {"sha": "15387a85cc16bd531ae65619141e4e51ee069357", "filename": "crates/parser/src/shortcuts.rs", "status": "added", "additions": 220, "deletions": 0, "changes": 220, "blob_url": "https://github.com/rust-lang/rust/blob/f4cb0ff9bea204e7cd1744dea28ec447298a9df7/crates%2Fparser%2Fsrc%2Fshortcuts.rs", "raw_url": "https://github.com/rust-lang/rust/raw/f4cb0ff9bea204e7cd1744dea28ec447298a9df7/crates%2Fparser%2Fsrc%2Fshortcuts.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fparser%2Fsrc%2Fshortcuts.rs?ref=f4cb0ff9bea204e7cd1744dea28ec447298a9df7", "patch": "@@ -0,0 +1,220 @@\n+//! Shortcuts that span lexer/parser abstraction.\n+//!\n+//! The way Rust works, parser doesn't necessary parse text, and you might\n+//! tokenize text without parsing it further. So, it makes sense to keep\n+//! abstract token parsing, and string tokenization as completely separate\n+//! layers.\n+//!\n+//! However, often you do pares text into syntax trees and the glue code for\n+//! that needs to live somewhere. Rather than putting it to lexer or parser, we\n+//! use a separate shortcuts module for that.\n+\n+use std::mem;\n+\n+use crate::{\n+    LexedStr, Step,\n+    SyntaxKind::{self, *},\n+};\n+\n+pub enum StrStep<'a> {\n+    Token { kind: SyntaxKind, text: &'a str },\n+    Enter { kind: SyntaxKind },\n+    Exit,\n+    Error { msg: &'a str, pos: usize },\n+}\n+\n+impl<'a> LexedStr<'a> {\n+    pub fn to_input(&self) -> crate::Input {\n+        let mut res = crate::Input::default();\n+        let mut was_joint = false;\n+        for i in 0..self.len() {\n+            let kind = self.kind(i);\n+            if kind.is_trivia() {\n+                was_joint = false\n+            } else {\n+                if kind == SyntaxKind::IDENT {\n+                    let token_text = self.text(i);\n+                    let contextual_kw = SyntaxKind::from_contextual_keyword(token_text)\n+                        .unwrap_or(SyntaxKind::IDENT);\n+                    res.push_ident(contextual_kw);\n+                } else {\n+                    if was_joint {\n+                        res.was_joint();\n+                    }\n+                    res.push(kind);\n+                }\n+                was_joint = true;\n+            }\n+        }\n+        res\n+    }\n+\n+    pub fn intersperse_trivia(\n+        &self,\n+        output: &crate::Output,\n+        synthetic_root: bool,\n+        sink: &mut dyn FnMut(StrStep),\n+    ) -> bool {\n+        let mut builder = Builder { lexed: self, pos: 0, state: State::PendingEnter, sink };\n+\n+        if synthetic_root {\n+            builder.enter(SyntaxKind::SOURCE_FILE);\n+        }\n+        for event in output.iter() {\n+            match event {\n+                Step::Token { kind, n_input_tokens: n_raw_tokens } => {\n+                    builder.token(kind, n_raw_tokens)\n+                }\n+                Step::Enter { kind } => builder.enter(kind),\n+                Step::Exit => builder.exit(),\n+                Step::Error { msg } => {\n+                    let text_pos = builder.lexed.text_start(builder.pos);\n+                    (builder.sink)(StrStep::Error { msg, pos: text_pos });\n+                }\n+            }\n+        }\n+        if synthetic_root {\n+            builder.exit();\n+        }\n+\n+        match mem::replace(&mut builder.state, State::Normal) {\n+            State::PendingExit => {\n+                builder.eat_trivias();\n+                (builder.sink)(StrStep::Exit);\n+            }\n+            State::PendingEnter | State::Normal => unreachable!(),\n+        }\n+\n+        let is_eof = builder.pos == builder.lexed.len();\n+        is_eof\n+    }\n+}\n+\n+struct Builder<'a, 'b> {\n+    lexed: &'a LexedStr<'a>,\n+    pos: usize,\n+    state: State,\n+    sink: &'b mut dyn FnMut(StrStep<'_>),\n+}\n+\n+enum State {\n+    PendingEnter,\n+    Normal,\n+    PendingExit,\n+}\n+\n+impl Builder<'_, '_> {\n+    fn token(&mut self, kind: SyntaxKind, n_tokens: u8) {\n+        match mem::replace(&mut self.state, State::Normal) {\n+            State::PendingEnter => unreachable!(),\n+            State::PendingExit => (self.sink)(StrStep::Exit),\n+            State::Normal => (),\n+        }\n+        self.eat_trivias();\n+        self.do_token(kind, n_tokens as usize);\n+    }\n+\n+    fn enter(&mut self, kind: SyntaxKind) {\n+        match mem::replace(&mut self.state, State::Normal) {\n+            State::PendingEnter => {\n+                (self.sink)(StrStep::Enter { kind });\n+                // No need to attach trivias to previous node: there is no\n+                // previous node.\n+                return;\n+            }\n+            State::PendingExit => (self.sink)(StrStep::Exit),\n+            State::Normal => (),\n+        }\n+\n+        let n_trivias =\n+            (self.pos..self.lexed.len()).take_while(|&it| self.lexed.kind(it).is_trivia()).count();\n+        let leading_trivias = self.pos..self.pos + n_trivias;\n+        let n_attached_trivias = n_attached_trivias(\n+            kind,\n+            leading_trivias.rev().map(|it| (self.lexed.kind(it), self.lexed.text(it))),\n+        );\n+        self.eat_n_trivias(n_trivias - n_attached_trivias);\n+        (self.sink)(StrStep::Enter { kind });\n+        self.eat_n_trivias(n_attached_trivias);\n+    }\n+\n+    fn exit(&mut self) {\n+        match mem::replace(&mut self.state, State::PendingExit) {\n+            State::PendingEnter => unreachable!(),\n+            State::PendingExit => (self.sink)(StrStep::Exit),\n+            State::Normal => (),\n+        }\n+    }\n+\n+    fn eat_trivias(&mut self) {\n+        while self.pos < self.lexed.len() {\n+            let kind = self.lexed.kind(self.pos);\n+            if !kind.is_trivia() {\n+                break;\n+            }\n+            self.do_token(kind, 1);\n+        }\n+    }\n+\n+    fn eat_n_trivias(&mut self, n: usize) {\n+        for _ in 0..n {\n+            let kind = self.lexed.kind(self.pos);\n+            assert!(kind.is_trivia());\n+            self.do_token(kind, 1);\n+        }\n+    }\n+\n+    fn do_token(&mut self, kind: SyntaxKind, n_tokens: usize) {\n+        let text = &self.lexed.range_text(self.pos..self.pos + n_tokens);\n+        self.pos += n_tokens;\n+        (self.sink)(StrStep::Token { kind, text });\n+    }\n+}\n+\n+fn n_attached_trivias<'a>(\n+    kind: SyntaxKind,\n+    trivias: impl Iterator<Item = (SyntaxKind, &'a str)>,\n+) -> usize {\n+    match kind {\n+        CONST | ENUM | FN | IMPL | MACRO_CALL | MACRO_DEF | MACRO_RULES | MODULE | RECORD_FIELD\n+        | STATIC | STRUCT | TRAIT | TUPLE_FIELD | TYPE_ALIAS | UNION | USE | VARIANT => {\n+            let mut res = 0;\n+            let mut trivias = trivias.enumerate().peekable();\n+\n+            while let Some((i, (kind, text))) = trivias.next() {\n+                match kind {\n+                    WHITESPACE if text.contains(\"\\n\\n\") => {\n+                        // we check whether the next token is a doc-comment\n+                        // and skip the whitespace in this case\n+                        if let Some((COMMENT, peek_text)) = trivias.peek().map(|(_, pair)| pair) {\n+                            if is_outer(peek_text) {\n+                                continue;\n+                            }\n+                        }\n+                        break;\n+                    }\n+                    COMMENT => {\n+                        if is_inner(text) {\n+                            break;\n+                        }\n+                        res = i + 1;\n+                    }\n+                    _ => (),\n+                }\n+            }\n+            res\n+        }\n+        _ => 0,\n+    }\n+}\n+\n+fn is_outer(text: &str) -> bool {\n+    if text.starts_with(\"////\") || text.starts_with(\"/***\") {\n+        return false;\n+    }\n+    text.starts_with(\"///\") || text.starts_with(\"/**\")\n+}\n+\n+fn is_inner(text: &str) -> bool {\n+    text.starts_with(\"//!\") || text.starts_with(\"/*!\")\n+}"}, {"sha": "971fa2700d2f31a98db261749e039b87412d74c5", "filename": "crates/syntax/src/parsing.rs", "status": "modified", "additions": 32, "deletions": 4, "changes": 36, "blob_url": "https://github.com/rust-lang/rust/blob/f4cb0ff9bea204e7cd1744dea28ec447298a9df7/crates%2Fsyntax%2Fsrc%2Fparsing.rs", "raw_url": "https://github.com/rust-lang/rust/raw/f4cb0ff9bea204e7cd1744dea28ec447298a9df7/crates%2Fsyntax%2Fsrc%2Fparsing.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fsyntax%2Fsrc%2Fparsing.rs?ref=f4cb0ff9bea204e7cd1744dea28ec447298a9df7", "patch": "@@ -1,12 +1,11 @@\n //! Lexing, bridging to parser (which does the actual parsing) and\n //! incremental reparsing.\n \n-mod text_tree_sink;\n mod reparsing;\n \n-use crate::{\n-    parsing::text_tree_sink::build_tree, syntax_node::GreenNode, AstNode, SyntaxError, SyntaxNode,\n-};\n+use rowan::TextRange;\n+\n+use crate::{syntax_node::GreenNode, AstNode, SyntaxError, SyntaxNode, SyntaxTreeBuilder};\n \n pub(crate) use crate::parsing::reparsing::incremental_reparse;\n \n@@ -37,3 +36,32 @@ pub(crate) fn parse_text_as<T: AstNode>(\n \n     SyntaxNode::new_root(node).first_child().and_then(T::cast).ok_or(())\n }\n+\n+pub(crate) fn build_tree(\n+    lexed: parser::LexedStr<'_>,\n+    parser_output: parser::Output,\n+    synthetic_root: bool,\n+) -> (GreenNode, Vec<SyntaxError>, bool) {\n+    let mut builder = SyntaxTreeBuilder::default();\n+\n+    let is_eof = lexed.intersperse_trivia(&parser_output, synthetic_root, &mut |step| match step {\n+        parser::StrStep::Token { kind, text } => builder.token(kind, text),\n+        parser::StrStep::Enter { kind } => builder.start_node(kind),\n+        parser::StrStep::Exit => builder.finish_node(),\n+        parser::StrStep::Error { msg, pos } => {\n+            builder.error(msg.to_string(), pos.try_into().unwrap())\n+        }\n+    });\n+\n+    let (node, mut errors) = builder.finish_raw();\n+    for (i, err) in lexed.errors() {\n+        let text_range = lexed.text_range(i);\n+        let text_range = TextRange::new(\n+            text_range.start.try_into().unwrap(),\n+            text_range.end.try_into().unwrap(),\n+        );\n+        errors.push(SyntaxError::new(err, text_range))\n+    }\n+\n+    (node, errors, is_eof)\n+}"}, {"sha": "dd2e01bfc4cacbb21c8e22de0051d63a11ae38ed", "filename": "crates/syntax/src/parsing/reparsing.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/f4cb0ff9bea204e7cd1744dea28ec447298a9df7/crates%2Fsyntax%2Fsrc%2Fparsing%2Freparsing.rs", "raw_url": "https://github.com/rust-lang/rust/raw/f4cb0ff9bea204e7cd1744dea28ec447298a9df7/crates%2Fsyntax%2Fsrc%2Fparsing%2Freparsing.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fsyntax%2Fsrc%2Fparsing%2Freparsing.rs?ref=f4cb0ff9bea204e7cd1744dea28ec447298a9df7", "patch": "@@ -10,7 +10,7 @@ use parser::Reparser;\n use text_edit::Indel;\n \n use crate::{\n-    parsing::text_tree_sink::build_tree,\n+    parsing::build_tree,\n     syntax_node::{GreenNode, GreenToken, NodeOrToken, SyntaxElement, SyntaxNode},\n     SyntaxError,\n     SyntaxKind::*,"}, {"sha": "f40c549e3d7bcedb12ec44a97a5ffb7a543216c5", "filename": "crates/syntax/src/parsing/text_tree_sink.rs", "status": "removed", "additions": 0, "deletions": 191, "changes": 191, "blob_url": "https://github.com/rust-lang/rust/blob/c456b217d808058d2515dd909bc4f68206de340e/crates%2Fsyntax%2Fsrc%2Fparsing%2Ftext_tree_sink.rs", "raw_url": "https://github.com/rust-lang/rust/raw/c456b217d808058d2515dd909bc4f68206de340e/crates%2Fsyntax%2Fsrc%2Fparsing%2Ftext_tree_sink.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fsyntax%2Fsrc%2Fparsing%2Ftext_tree_sink.rs?ref=c456b217d808058d2515dd909bc4f68206de340e", "patch": "@@ -1,191 +0,0 @@\n-//! See [`TextTreeSink`].\n-\n-use std::mem;\n-\n-use parser::LexedStr;\n-\n-use crate::{\n-    ast,\n-    syntax_node::GreenNode,\n-    SyntaxError,\n-    SyntaxKind::{self, *},\n-    SyntaxTreeBuilder, TextRange,\n-};\n-\n-pub(crate) fn build_tree(\n-    lexed: LexedStr<'_>,\n-    parser_output: parser::Output,\n-    synthetic_root: bool,\n-) -> (GreenNode, Vec<SyntaxError>, bool) {\n-    let mut builder = Builder::new(lexed);\n-\n-    if synthetic_root {\n-        builder.enter(SyntaxKind::SOURCE_FILE);\n-    }\n-\n-    for event in parser_output.iter() {\n-        match event {\n-            parser::Step::Token { kind, n_input_tokens: n_raw_tokens } => {\n-                builder.token(kind, n_raw_tokens)\n-            }\n-            parser::Step::Enter { kind } => builder.enter(kind),\n-            parser::Step::Exit => builder.exit(),\n-            parser::Step::Error { msg } => {\n-                let text_pos = builder.lexed.text_start(builder.pos).try_into().unwrap();\n-                builder.inner.error(msg.to_string(), text_pos);\n-            }\n-        }\n-    }\n-    if synthetic_root {\n-        builder.exit()\n-    }\n-    builder.build()\n-}\n-\n-struct Builder<'a> {\n-    lexed: LexedStr<'a>,\n-    pos: usize,\n-    state: State,\n-    inner: SyntaxTreeBuilder,\n-}\n-\n-enum State {\n-    PendingStart,\n-    Normal,\n-    PendingFinish,\n-}\n-\n-impl<'a> Builder<'a> {\n-    fn new(lexed: parser::LexedStr<'a>) -> Self {\n-        Self { lexed, pos: 0, state: State::PendingStart, inner: SyntaxTreeBuilder::default() }\n-    }\n-\n-    fn build(mut self) -> (GreenNode, Vec<SyntaxError>, bool) {\n-        match mem::replace(&mut self.state, State::Normal) {\n-            State::PendingFinish => {\n-                self.eat_trivias();\n-                self.inner.finish_node();\n-            }\n-            State::PendingStart | State::Normal => unreachable!(),\n-        }\n-\n-        let (node, mut errors) = self.inner.finish_raw();\n-        for (i, err) in self.lexed.errors() {\n-            let text_range = self.lexed.text_range(i);\n-            let text_range = TextRange::new(\n-                text_range.start.try_into().unwrap(),\n-                text_range.end.try_into().unwrap(),\n-            );\n-            errors.push(SyntaxError::new(err, text_range))\n-        }\n-\n-        let is_eof = self.pos == self.lexed.len();\n-\n-        (node, errors, is_eof)\n-    }\n-\n-    fn token(&mut self, kind: SyntaxKind, n_tokens: u8) {\n-        match mem::replace(&mut self.state, State::Normal) {\n-            State::PendingStart => unreachable!(),\n-            State::PendingFinish => self.inner.finish_node(),\n-            State::Normal => (),\n-        }\n-        self.eat_trivias();\n-        self.do_token(kind, n_tokens as usize);\n-    }\n-\n-    fn enter(&mut self, kind: SyntaxKind) {\n-        match mem::replace(&mut self.state, State::Normal) {\n-            State::PendingStart => {\n-                self.inner.start_node(kind);\n-                // No need to attach trivias to previous node: there is no\n-                // previous node.\n-                return;\n-            }\n-            State::PendingFinish => self.inner.finish_node(),\n-            State::Normal => (),\n-        }\n-\n-        let n_trivias =\n-            (self.pos..self.lexed.len()).take_while(|&it| self.lexed.kind(it).is_trivia()).count();\n-        let leading_trivias = self.pos..self.pos + n_trivias;\n-        let n_attached_trivias = n_attached_trivias(\n-            kind,\n-            leading_trivias.rev().map(|it| (self.lexed.kind(it), self.lexed.text(it))),\n-        );\n-        self.eat_n_trivias(n_trivias - n_attached_trivias);\n-        self.inner.start_node(kind);\n-        self.eat_n_trivias(n_attached_trivias);\n-    }\n-\n-    fn exit(&mut self) {\n-        match mem::replace(&mut self.state, State::PendingFinish) {\n-            State::PendingStart => unreachable!(),\n-            State::PendingFinish => self.inner.finish_node(),\n-            State::Normal => (),\n-        }\n-    }\n-\n-    fn eat_trivias(&mut self) {\n-        while self.pos < self.lexed.len() {\n-            let kind = self.lexed.kind(self.pos);\n-            if !kind.is_trivia() {\n-                break;\n-            }\n-            self.do_token(kind, 1);\n-        }\n-    }\n-\n-    fn eat_n_trivias(&mut self, n: usize) {\n-        for _ in 0..n {\n-            let kind = self.lexed.kind(self.pos);\n-            assert!(kind.is_trivia());\n-            self.do_token(kind, 1);\n-        }\n-    }\n-\n-    fn do_token(&mut self, kind: SyntaxKind, n_tokens: usize) {\n-        let text = &self.lexed.range_text(self.pos..self.pos + n_tokens);\n-        self.pos += n_tokens;\n-        self.inner.token(kind, text);\n-    }\n-}\n-\n-fn n_attached_trivias<'a>(\n-    kind: SyntaxKind,\n-    trivias: impl Iterator<Item = (SyntaxKind, &'a str)>,\n-) -> usize {\n-    match kind {\n-        CONST | ENUM | FN | IMPL | MACRO_CALL | MACRO_DEF | MACRO_RULES | MODULE | RECORD_FIELD\n-        | STATIC | STRUCT | TRAIT | TUPLE_FIELD | TYPE_ALIAS | UNION | USE | VARIANT => {\n-            let mut res = 0;\n-            let mut trivias = trivias.enumerate().peekable();\n-\n-            while let Some((i, (kind, text))) = trivias.next() {\n-                match kind {\n-                    WHITESPACE if text.contains(\"\\n\\n\") => {\n-                        // we check whether the next token is a doc-comment\n-                        // and skip the whitespace in this case\n-                        if let Some((COMMENT, peek_text)) = trivias.peek().map(|(_, pair)| pair) {\n-                            let comment_kind = ast::CommentKind::from_text(peek_text);\n-                            if comment_kind.doc == Some(ast::CommentPlacement::Outer) {\n-                                continue;\n-                            }\n-                        }\n-                        break;\n-                    }\n-                    COMMENT => {\n-                        let comment_kind = ast::CommentKind::from_text(text);\n-                        if comment_kind.doc == Some(ast::CommentPlacement::Inner) {\n-                            break;\n-                        }\n-                        res = i + 1;\n-                    }\n-                    _ => (),\n-                }\n-            }\n-            res\n-        }\n-        _ => 0,\n-    }\n-}"}]}