{"sha": "622c843a4c598befaf459e64e9f75f31b4886a5b", "node_id": "MDY6Q29tbWl0NzI0NzEyOjYyMmM4NDNhNGM1OThiZWZhZjQ1OWU2NGU5Zjc1ZjMxYjQ4ODZhNWI=", "commit": {"author": {"name": "Edwin Cheng", "email": "edwin0cheng@gmail.com", "date": "2020-03-20T19:04:11Z"}, "committer": {"name": "Edwin Cheng", "email": "edwin0cheng@gmail.com", "date": "2020-03-20T19:08:56Z"}, "message": "Add TokenConvertor trait", "tree": {"sha": "6a46953e52aeb02f724fd87afe9fab2cec1c7691", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/6a46953e52aeb02f724fd87afe9fab2cec1c7691"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/622c843a4c598befaf459e64e9f75f31b4886a5b", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/622c843a4c598befaf459e64e9f75f31b4886a5b", "html_url": "https://github.com/rust-lang/rust/commit/622c843a4c598befaf459e64e9f75f31b4886a5b", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/622c843a4c598befaf459e64e9f75f31b4886a5b/comments", "author": {"login": "edwin0cheng", "id": 11014119, "node_id": "MDQ6VXNlcjExMDE0MTE5", "avatar_url": "https://avatars.githubusercontent.com/u/11014119?v=4", "gravatar_id": "", "url": "https://api.github.com/users/edwin0cheng", "html_url": "https://github.com/edwin0cheng", "followers_url": "https://api.github.com/users/edwin0cheng/followers", "following_url": "https://api.github.com/users/edwin0cheng/following{/other_user}", "gists_url": "https://api.github.com/users/edwin0cheng/gists{/gist_id}", "starred_url": "https://api.github.com/users/edwin0cheng/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/edwin0cheng/subscriptions", "organizations_url": "https://api.github.com/users/edwin0cheng/orgs", "repos_url": "https://api.github.com/users/edwin0cheng/repos", "events_url": "https://api.github.com/users/edwin0cheng/events{/privacy}", "received_events_url": "https://api.github.com/users/edwin0cheng/received_events", "type": "User", "site_admin": false}, "committer": {"login": "edwin0cheng", "id": 11014119, "node_id": "MDQ6VXNlcjExMDE0MTE5", "avatar_url": "https://avatars.githubusercontent.com/u/11014119?v=4", "gravatar_id": "", "url": "https://api.github.com/users/edwin0cheng", "html_url": "https://github.com/edwin0cheng", "followers_url": "https://api.github.com/users/edwin0cheng/followers", "following_url": "https://api.github.com/users/edwin0cheng/following{/other_user}", "gists_url": "https://api.github.com/users/edwin0cheng/gists{/gist_id}", "starred_url": "https://api.github.com/users/edwin0cheng/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/edwin0cheng/subscriptions", "organizations_url": "https://api.github.com/users/edwin0cheng/orgs", "repos_url": "https://api.github.com/users/edwin0cheng/repos", "events_url": "https://api.github.com/users/edwin0cheng/events{/privacy}", "received_events_url": "https://api.github.com/users/edwin0cheng/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "27c516970bea6bee9d6bca52ac9ad619412809ef", "url": "https://api.github.com/repos/rust-lang/rust/commits/27c516970bea6bee9d6bca52ac9ad619412809ef", "html_url": "https://github.com/rust-lang/rust/commit/27c516970bea6bee9d6bca52ac9ad619412809ef"}], "stats": {"total": 394, "additions": 155, "deletions": 239}, "files": [{"sha": "540afc87c7b03ecb6354a8a4e9e14db0fc0bf210", "filename": "crates/ra_mbe/src/syntax_bridge.rs", "status": "modified", "additions": 152, "deletions": 236, "changes": 388, "blob_url": "https://github.com/rust-lang/rust/blob/622c843a4c598befaf459e64e9f75f31b4886a5b/crates%2Fra_mbe%2Fsrc%2Fsyntax_bridge.rs", "raw_url": "https://github.com/rust-lang/rust/raw/622c843a4c598befaf459e64e9f75f31b4886a5b/crates%2Fra_mbe%2Fsrc%2Fsyntax_bridge.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fra_mbe%2Fsrc%2Fsyntax_bridge.rs?ref=622c843a4c598befaf459e64e9f75f31b4886a5b", "patch": "@@ -3,12 +3,11 @@\n use ra_parser::{FragmentKind, ParseError, TreeSink};\n use ra_syntax::{\n     ast::{self, make::tokens::doc_comment},\n-    tokenize, AstToken, NodeOrToken, Parse, SmolStr, SyntaxKind,\n+    tokenize, AstToken, Parse, SmolStr, SyntaxKind,\n     SyntaxKind::*,\n-    SyntaxNode, SyntaxTreeBuilder, TextRange, TextUnit, Token, T,\n+    SyntaxNode, SyntaxToken, SyntaxTreeBuilder, TextRange, TextUnit, Token as RawToken, T,\n };\n use rustc_hash::FxHashMap;\n-use std::iter::successors;\n use tt::buffer::{Cursor, TokenBuffer};\n \n use crate::subtree_source::SubtreeTokenSource;\n@@ -50,10 +49,8 @@ pub fn ast_to_token_tree(ast: &impl ast::AstNode) -> Option<(tt::Subtree, TokenM\n /// will consume).\n pub fn syntax_node_to_token_tree(node: &SyntaxNode) -> Option<(tt::Subtree, TokenMap)> {\n     let global_offset = node.text_range().start();\n-    let mut c = Convertor {\n-        id_alloc: { TokenIdAlloc { map: TokenMap::default(), global_offset, next_id: 0 } },\n-    };\n-    let subtree = c.go(node)?;\n+    let mut c = Convertor::new(node, global_offset);\n+    let subtree = c.go()?;\n     Some((subtree, c.id_alloc.map))\n }\n \n@@ -237,16 +234,6 @@ impl TokenIdAlloc {\n         token_id\n     }\n \n-    fn delim(&mut self, open_abs_range: TextRange, close_abs_range: TextRange) -> tt::TokenId {\n-        let open_relative_range = open_abs_range - self.global_offset;\n-        let close_relative_range = close_abs_range - self.global_offset;\n-        let token_id = tt::TokenId(self.next_id);\n-        self.next_id += 1;\n-\n-        self.map.insert_delim(token_id, open_relative_range, close_relative_range);\n-        token_id\n-    }\n-\n     fn open_delim(&mut self, open_abs_range: TextRange) -> tt::TokenId {\n         let token_id = tt::TokenId(self.next_id);\n         self.next_id += 1;\n@@ -264,15 +251,19 @@ struct RawConvertor<'a> {\n     text: &'a str,\n     offset: TextUnit,\n     id_alloc: TokenIdAlloc,\n-    inner: std::slice::Iter<'a, Token>,\n+    inner: std::slice::Iter<'a, RawToken>,\n }\n \n trait SrcToken {\n-    fn kind() -> SyntaxKind;\n+    fn kind(&self) -> SyntaxKind;\n+\n+    fn to_char(&self) -> Option<char>;\n+\n+    fn to_text(&self) -> SmolStr;\n }\n \n-trait TokenConvertor  {\n-    type Token : SrcToken;\n+trait TokenConvertor {\n+    type Token: SrcToken;\n \n     fn go(&mut self) -> Option<tt::Subtree> {\n         let mut subtree = tt::Subtree::default();\n@@ -291,10 +282,6 @@ trait TokenConvertor  {\n         Some(subtree)\n     }\n \n-    fn bump(&mut self) -> Option<(Self::Token, TextRange)>;\n-\n-    fn peek(&self) -> Option<Self::Token>;\n-\n     fn collect_leaf(&mut self, result: &mut Vec<tt::TokenTree>) {\n         let (token, range) = match self.bump() {\n             None => return,\n@@ -303,8 +290,7 @@ trait TokenConvertor  {\n \n         let k: SyntaxKind = token.kind();\n         if k == COMMENT {\n-            let node = doc_comment(&self.text[range]);\n-            if let Some(tokens) = convert_doc_comment(&node) {\n+            if let Some(tokens) = self.convert_doc_comment(&token) {\n                 result.extend(tokens);\n             }\n             return;\n@@ -320,40 +306,39 @@ trait TokenConvertor  {\n \n             if let Some((kind, closed)) = delim {\n                 let mut subtree = tt::Subtree::default();\n-                let id = self.id_alloc.open_delim(range);\n+                let id = self.id_alloc().open_delim(range);\n                 subtree.delimiter = Some(tt::Delimiter { kind, id });\n \n-                while self.peek().map(|it| it.kind != closed).unwrap_or(false) {\n+                while self.peek().map(|it| it.kind() != closed).unwrap_or(false) {\n                     self.collect_leaf(&mut subtree.token_trees);\n                 }\n                 let last_range = match self.bump() {\n                     None => return,\n                     Some(it) => it.1,\n                 };\n-                self.id_alloc.close_delim(id, last_range);\n+                self.id_alloc().close_delim(id, last_range);\n                 subtree.into()\n             } else {\n                 let spacing = match self.peek() {\n                     Some(next)\n-                        if next.kind.is_trivia()\n-                            || next.kind == T!['[']\n-                            || next.kind == T!['{']\n-                            || next.kind == T!['('] =>\n+                        if next.kind().is_trivia()\n+                            || next.kind() == T!['[']\n+                            || next.kind() == T!['{']\n+                            || next.kind() == T!['('] =>\n                     {\n                         tt::Spacing::Alone\n                     }\n-                    Some(next) if next.kind.is_punct() => tt::Spacing::Joint,\n+                    Some(next) if next.kind().is_punct() => tt::Spacing::Joint,\n                     _ => tt::Spacing::Alone,\n                 };\n-                let char =\n-                    self.text[range].chars().next().expect(\"Token from lexer must be single char\");\n+                let char = token.to_char().expect(\"Token from lexer must be single char\");\n \n-                tt::Leaf::from(tt::Punct { char, spacing, id: self.id_alloc.alloc(range) }).into()\n+                tt::Leaf::from(tt::Punct { char, spacing, id: self.id_alloc().alloc(range) }).into()\n             }\n         } else {\n             macro_rules! make_leaf {\n                 ($i:ident) => {\n-                    tt::$i { id: self.id_alloc.alloc(range), text: self.text[range].into() }.into()\n+                    tt::$i { id: self.id_alloc().alloc(range), text: token.to_text() }.into()\n                 };\n             }\n             let leaf: tt::Leaf = match k {\n@@ -367,237 +352,168 @@ trait TokenConvertor  {\n             leaf.into()\n         });\n     }\n+\n+    fn convert_doc_comment(&self, token: &Self::Token) -> Option<Vec<tt::TokenTree>>;\n+\n+    fn bump(&mut self) -> Option<(Self::Token, TextRange)>;\n+\n+    fn peek(&self) -> Option<Self::Token>;\n+\n+    fn id_alloc(&mut self) -> &mut TokenIdAlloc;\n }\n \n-impl RawConvertor<'_> {\n-    fn go(&mut self) -> Option<tt::Subtree> {\n-        let mut subtree = tt::Subtree::default();\n-        subtree.delimiter = None;\n-        while self.peek().is_some() {\n-            self.collect_leaf(&mut subtree.token_trees);\n-        }\n-        if subtree.token_trees.is_empty() {\n-            return None;\n-        }\n-        if subtree.token_trees.len() == 1 {\n-            if let tt::TokenTree::Subtree(first) = &subtree.token_trees[0] {\n-                return Some(first.clone());\n-            }\n-        }\n-        Some(subtree)\n+impl<'a> SrcToken for (RawToken, &'a str) {\n+    fn kind(&self) -> SyntaxKind {\n+        self.0.kind\n     }\n \n-    fn bump(&mut self) -> Option<(Token, TextRange)> {\n-        let token = self.inner.next()?;\n-        let range = TextRange::offset_len(self.offset, token.len);\n-        self.offset += token.len;\n-        Some((*token, range))\n+    fn to_char(&self) -> Option<char> {\n+        self.1.chars().next()\n     }\n \n-    fn peek(&self) -> Option<Token> {\n-        self.inner.as_slice().get(0).cloned()\n+    fn to_text(&self) -> SmolStr {\n+        self.1.into()\n     }\n-    \n+}\n \n-    fn collect_leaf(&mut self, result: &mut Vec<tt::TokenTree>) {\n-        let (token, range) = match self.bump() {\n-            None => return,\n-            Some(it) => it,\n-        };\n+impl RawConvertor<'_> {}\n \n-        let k: SyntaxKind = token.kind;\n-        if k == COMMENT {\n-            let node = doc_comment(&self.text[range]);\n-            if let Some(tokens) = convert_doc_comment(&node) {\n-                result.extend(tokens);\n-            }\n-            return;\n-        }\n+impl<'a> TokenConvertor for RawConvertor<'a> {\n+    type Token = (RawToken, &'a str);\n \n-        result.push(if k.is_punct() {\n-            let delim = match k {\n-                T!['('] => Some((tt::DelimiterKind::Parenthesis, T![')'])),\n-                T!['{'] => Some((tt::DelimiterKind::Brace, T!['}'])),\n-                T!['['] => Some((tt::DelimiterKind::Bracket, T![']'])),\n-                _ => None,\n-            };\n+    fn convert_doc_comment(&self, token: &Self::Token) -> Option<Vec<tt::TokenTree>> {\n+        convert_doc_comment(&doc_comment(token.1))\n+    }\n \n-            if let Some((kind, closed)) = delim {\n-                let mut subtree = tt::Subtree::default();\n-                let id = self.id_alloc.open_delim(range);\n-                subtree.delimiter = Some(tt::Delimiter { kind, id });\n+    fn bump(&mut self) -> Option<(Self::Token, TextRange)> {\n+        let token = self.inner.next()?;\n+        let range = TextRange::offset_len(self.offset, token.len);\n+        self.offset += token.len;\n \n-                while self.peek().map(|it| it.kind != closed).unwrap_or(false) {\n-                    self.collect_leaf(&mut subtree.token_trees);\n-                }\n-                let last_range = match self.bump() {\n-                    None => return,\n-                    Some(it) => it.1,\n-                };\n-                self.id_alloc.close_delim(id, last_range);\n-                subtree.into()\n-            } else {\n-                let spacing = match self.peek() {\n-                    Some(next)\n-                        if next.kind.is_trivia()\n-                            || next.kind == T!['[']\n-                            || next.kind == T!['{']\n-                            || next.kind == T!['('] =>\n-                    {\n-                        tt::Spacing::Alone\n-                    }\n-                    Some(next) if next.kind.is_punct() => tt::Spacing::Joint,\n-                    _ => tt::Spacing::Alone,\n-                };\n-                let char =\n-                    self.text[range].chars().next().expect(\"Token from lexer must be single char\");\n+        Some(((*token, &self.text[range]), range))\n+    }\n \n-                tt::Leaf::from(tt::Punct { char, spacing, id: self.id_alloc.alloc(range) }).into()\n-            }\n-        } else {\n-            macro_rules! make_leaf {\n-                ($i:ident) => {\n-                    tt::$i { id: self.id_alloc.alloc(range), text: self.text[range].into() }.into()\n-                };\n-            }\n-            let leaf: tt::Leaf = match k {\n-                T![true] | T![false] => make_leaf!(Literal),\n-                IDENT | LIFETIME => make_leaf!(Ident),\n-                k if k.is_keyword() => make_leaf!(Ident),\n-                k if k.is_literal() => make_leaf!(Literal),\n-                _ => return,\n-            };\n+    fn peek(&self) -> Option<Self::Token> {\n+        let token = self.inner.as_slice().get(0).cloned();\n \n-            leaf.into()\n-        });\n+        token.map(|it| {\n+            let range = TextRange::offset_len(self.offset, it.len);\n+            (it, &self.text[range])\n+        })\n+    }\n+\n+    fn id_alloc(&mut self) -> &mut TokenIdAlloc {\n+        &mut self.id_alloc\n     }\n }\n \n-// FIXME: There are some duplicate logic between RawConvertor and Convertor\n-// It would be nice to refactor to converting SyntaxNode to ra_parser::Token and thus\n-// use RawConvertor directly. But performance-wise it may not be a good idea ?\n struct Convertor {\n     id_alloc: TokenIdAlloc,\n+    current: Option<SyntaxToken>,\n+    range: TextRange,\n+    punct_offset: Option<(SyntaxToken, TextUnit)>,\n }\n \n impl Convertor {\n-    fn go(&mut self, tt: &SyntaxNode) -> Option<tt::Subtree> {\n-        // This tree is empty\n-        if tt.first_child_or_token().is_none() {\n-            return Some(tt::Subtree { token_trees: vec![], delimiter: None });\n+    fn new(node: &SyntaxNode, global_offset: TextUnit) -> Convertor {\n+        Convertor {\n+            id_alloc: { TokenIdAlloc { map: TokenMap::default(), global_offset, next_id: 0 } },\n+            current: node.first_token(),\n+            range: node.text_range(),\n+            punct_offset: None,\n         }\n+    }\n+}\n \n-        let first_child = tt.first_child_or_token()?;\n-        let last_child = tt.last_child_or_token()?;\n+enum SynToken {\n+    Ordiniary(SyntaxToken),\n+    Punch(SyntaxToken, TextUnit),\n+}\n \n-        // ignore trivial first_child and last_child\n-        let first_child = successors(Some(first_child), |it| {\n-            if it.kind().is_trivia() {\n-                it.next_sibling_or_token()\n-            } else {\n-                None\n-            }\n-        })\n-        .last()\n-        .unwrap();\n-        if first_child.kind().is_trivia() {\n-            return Some(tt::Subtree { token_trees: vec![], delimiter: None });\n+impl SynToken {\n+    fn token(&self) -> &SyntaxToken {\n+        match self {\n+            SynToken::Ordiniary(it) => it,\n+            SynToken::Punch(it, _) => it,\n         }\n+    }\n+}\n \n-        let last_child = successors(Some(last_child), |it| {\n-            if it.kind().is_trivia() {\n-                it.prev_sibling_or_token()\n-            } else {\n-                None\n-            }\n-        })\n-        .last()\n-        .unwrap();\n-\n-        let (delimiter_kind, skip_first) = match (first_child.kind(), last_child.kind()) {\n-            (T!['('], T![')']) => (Some(tt::DelimiterKind::Parenthesis), true),\n-            (T!['{'], T!['}']) => (Some(tt::DelimiterKind::Brace), true),\n-            (T!['['], T![']']) => (Some(tt::DelimiterKind::Bracket), true),\n-            _ => (None, false),\n-        };\n-        let delimiter = delimiter_kind.map(|kind| tt::Delimiter {\n-            kind,\n-            id: self.id_alloc.delim(first_child.text_range(), last_child.text_range()),\n-        });\n+impl SrcToken for SynToken {\n+    fn kind(&self) -> SyntaxKind {\n+        self.token().kind()\n+    }\n+    fn to_char(&self) -> Option<char> {\n+        match self {\n+            SynToken::Ordiniary(_) => None,\n+            SynToken::Punch(it, i) => it.text().chars().nth(i.to_usize()),\n+        }\n+    }\n+    fn to_text(&self) -> SmolStr {\n+        self.token().text().clone()\n+    }\n+}\n+\n+impl TokenConvertor for Convertor {\n+    type Token = SynToken;\n+    fn convert_doc_comment(&self, token: &Self::Token) -> Option<Vec<tt::TokenTree>> {\n+        convert_doc_comment(token.token())\n+    }\n \n-        let mut token_trees = Vec::new();\n-        let mut child_iter = tt.children_with_tokens().skip(skip_first as usize).peekable();\n+    fn bump(&mut self) -> Option<(Self::Token, TextRange)> {\n+        let curr = self.current.clone()?;\n+        if !curr.text_range().is_subrange(&self.range) {\n+            return None;\n+        }\n \n-        while let Some(child) = child_iter.next() {\n-            if skip_first && (child == first_child || child == last_child) {\n-                continue;\n+        if let Some((punct, offset)) = self.punct_offset.clone() {\n+            if offset.to_usize() + 1 < punct.text().len() {\n+                let offset = offset + TextUnit::from_usize(1);\n+                let range = punct.text_range();\n+                self.punct_offset = Some((punct, offset));\n+                let range = TextRange::offset_len(range.start() + offset, TextUnit::from_usize(1));\n+                return Some((SynToken::Punch(curr, offset), range));\n             }\n+        }\n \n-            match child {\n-                NodeOrToken::Token(token) => {\n-                    if let Some(doc_tokens) = convert_doc_comment(&token) {\n-                        token_trees.extend(doc_tokens);\n-                    } else if token.kind().is_trivia() {\n-                        continue;\n-                    } else if token.kind().is_punct() {\n-                        // we need to pull apart joined punctuation tokens\n-                        let last_spacing = match child_iter.peek() {\n-                            Some(NodeOrToken::Token(token)) => {\n-                                if token.kind().is_punct() {\n-                                    tt::Spacing::Joint\n-                                } else {\n-                                    tt::Spacing::Alone\n-                                }\n-                            }\n-                            _ => tt::Spacing::Alone,\n-                        };\n-                        let spacing_iter = std::iter::repeat(tt::Spacing::Joint)\n-                            .take(token.text().len() - 1)\n-                            .chain(std::iter::once(last_spacing));\n-                        for (char, spacing) in token.text().chars().zip(spacing_iter) {\n-                            token_trees.push(\n-                                tt::Leaf::from(tt::Punct {\n-                                    char,\n-                                    spacing,\n-                                    id: self.id_alloc.alloc(token.text_range()),\n-                                })\n-                                .into(),\n-                            );\n-                        }\n-                    } else {\n-                        macro_rules! make_leaf {\n-                            ($i:ident) => {\n-                                tt::$i {\n-                                    id: self.id_alloc.alloc(token.text_range()),\n-                                    text: token.text().clone(),\n-                                }\n-                                .into()\n-                            };\n-                        }\n+        self.current = curr.next_token();\n \n-                        let child: tt::Leaf = match token.kind() {\n-                            T![true] | T![false] => make_leaf!(Literal),\n-                            IDENT | LIFETIME => make_leaf!(Ident),\n-                            k if k.is_keyword() => make_leaf!(Ident),\n-                            k if k.is_literal() => make_leaf!(Literal),\n-                            _ => return None,\n-                        };\n-                        token_trees.push(child.into());\n-                    }\n-                }\n-                NodeOrToken::Node(node) => {\n-                    let child_subtree = self.go(&node)?;\n-                    if child_subtree.delimiter.is_none() && node.kind() != SyntaxKind::TOKEN_TREE {\n-                        token_trees.extend(child_subtree.token_trees);\n-                    } else {\n-                        token_trees.push(child_subtree.into());\n-                    }\n-                }\n-            };\n+        let token = if curr.kind().is_punct() {\n+            let range = curr.text_range();\n+            self.punct_offset = Some((curr.clone(), TextUnit::from_usize(0)));\n+            (SynToken::Punch(curr, TextUnit::from_usize(0)), range)\n+        } else {\n+            self.punct_offset = None;\n+            let range = curr.text_range();\n+            (SynToken::Ordiniary(curr), range)\n+        };\n+\n+        Some(token)\n+    }\n+\n+    fn peek(&self) -> Option<Self::Token> {\n+        let curr = self.current.clone()?;\n+        if !curr.text_range().is_subrange(&self.range) {\n+            return None;\n+        }\n+\n+        if let Some((punct, mut offset)) = self.punct_offset.clone() {\n+            offset = offset + TextUnit::from_usize(1);\n+            if offset.to_usize() < punct.text().len() {\n+                return Some(SynToken::Punch(punct, offset));\n+            }\n         }\n \n-        let res = tt::Subtree { delimiter, token_trees };\n-        Some(res)\n+        let token = if curr.kind().is_punct() {\n+            SynToken::Punch(curr, TextUnit::from_usize(0))\n+        } else {\n+            SynToken::Ordiniary(curr)\n+        };\n+        Some(token)\n+    }\n+\n+    fn id_alloc(&mut self) -> &mut TokenIdAlloc {\n+        &mut self.id_alloc\n     }\n }\n "}, {"sha": "a3f242e495ba15985417a8c8c7cd7d728729effc", "filename": "crates/ra_mbe/src/tests.rs", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "blob_url": "https://github.com/rust-lang/rust/blob/622c843a4c598befaf459e64e9f75f31b4886a5b/crates%2Fra_mbe%2Fsrc%2Ftests.rs", "raw_url": "https://github.com/rust-lang/rust/raw/622c843a4c598befaf459e64e9f75f31b4886a5b/crates%2Fra_mbe%2Fsrc%2Ftests.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fra_mbe%2Fsrc%2Ftests.rs?ref=622c843a4c598befaf459e64e9f75f31b4886a5b", "patch": "@@ -1449,8 +1449,8 @@ impl MacroFixture {\n         let macro_invocation =\n             source_file.syntax().descendants().find_map(ast::MacroCall::cast).unwrap();\n \n-        let (invocation_tt, _) =\n-            ast_to_token_tree(&macro_invocation.token_tree().unwrap()).unwrap();\n+        let (invocation_tt, _) = ast_to_token_tree(&macro_invocation.token_tree().unwrap())\n+            .ok_or_else(|| ExpandError::ConversionError)?;\n \n         self.rules.expand(&invocation_tt).result()\n     }\n@@ -1694,5 +1694,5 @@ fn test_expand_bad_literal() {\n         macro_rules! foo { ($i:literal) => {}; }\n     \"#,\n     )\n-    .assert_expand_err(r#\"foo!(&k\");\"#, &ExpandError::BindingError(\"\".to_string()));\n+    .assert_expand_err(r#\"foo!(&k\");\"#, &ExpandError::ConversionError);\n }"}]}