{"sha": "2bfb64e5252179b5bd6d8a2e96664ca97a2496f3", "node_id": "MDY6Q29tbWl0NzI0NzEyOjJiZmI2NGU1MjUyMTc5YjViZDZkOGEyZTk2NjY0Y2E5N2EyNDk2ZjM=", "commit": {"author": {"name": "bors", "email": "bors@rust-lang.org", "date": "2014-12-13T17:27:15Z"}, "committer": {"name": "bors", "email": "bors@rust-lang.org", "date": "2014-12-13T17:27:15Z"}, "message": "auto merge of #19627 : steveklabnik/rust/testing_guide, r=cmr", "tree": {"sha": "b2d2fceb88310e39fbc8dd4c006bec0f2cdebde9", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/b2d2fceb88310e39fbc8dd4c006bec0f2cdebde9"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/2bfb64e5252179b5bd6d8a2e96664ca97a2496f3", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/2bfb64e5252179b5bd6d8a2e96664ca97a2496f3", "html_url": "https://github.com/rust-lang/rust/commit/2bfb64e5252179b5bd6d8a2e96664ca97a2496f3", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/2bfb64e5252179b5bd6d8a2e96664ca97a2496f3/comments", "author": {"login": "bors", "id": 3372342, "node_id": "MDQ6VXNlcjMzNzIzNDI=", "avatar_url": "https://avatars.githubusercontent.com/u/3372342?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bors", "html_url": "https://github.com/bors", "followers_url": "https://api.github.com/users/bors/followers", "following_url": "https://api.github.com/users/bors/following{/other_user}", "gists_url": "https://api.github.com/users/bors/gists{/gist_id}", "starred_url": "https://api.github.com/users/bors/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bors/subscriptions", "organizations_url": "https://api.github.com/users/bors/orgs", "repos_url": "https://api.github.com/users/bors/repos", "events_url": "https://api.github.com/users/bors/events{/privacy}", "received_events_url": "https://api.github.com/users/bors/received_events", "type": "User", "site_admin": false}, "committer": {"login": "bors", "id": 3372342, "node_id": "MDQ6VXNlcjMzNzIzNDI=", "avatar_url": "https://avatars.githubusercontent.com/u/3372342?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bors", "html_url": "https://github.com/bors", "followers_url": "https://api.github.com/users/bors/followers", "following_url": "https://api.github.com/users/bors/following{/other_user}", "gists_url": "https://api.github.com/users/bors/gists{/gist_id}", "starred_url": "https://api.github.com/users/bors/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bors/subscriptions", "organizations_url": "https://api.github.com/users/bors/orgs", "repos_url": "https://api.github.com/users/bors/repos", "events_url": "https://api.github.com/users/bors/events{/privacy}", "received_events_url": "https://api.github.com/users/bors/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "ce05ff5d4bd6f7ddfdb477ee1f0bdf6c146dcd40", "url": "https://api.github.com/repos/rust-lang/rust/commits/ce05ff5d4bd6f7ddfdb477ee1f0bdf6c146dcd40", "html_url": "https://github.com/rust-lang/rust/commit/ce05ff5d4bd6f7ddfdb477ee1f0bdf6c146dcd40"}, {"sha": "d4ea71dbc14768e3175af5f45f5ee64396cc7759", "url": "https://api.github.com/repos/rust-lang/rust/commits/d4ea71dbc14768e3175af5f45f5ee64396cc7759", "html_url": "https://github.com/rust-lang/rust/commit/d4ea71dbc14768e3175af5f45f5ee64396cc7759"}], "stats": {"total": 705, "additions": 450, "deletions": 255}, "files": [{"sha": "0452ed0bfeb559a209ff696899cda184ad72427c", "filename": "src/doc/guide-testing.md", "status": "modified", "additions": 450, "deletions": 255, "changes": 705, "blob_url": "https://github.com/rust-lang/rust/blob/2bfb64e5252179b5bd6d8a2e96664ca97a2496f3/src%2Fdoc%2Fguide-testing.md", "raw_url": "https://github.com/rust-lang/rust/raw/2bfb64e5252179b5bd6d8a2e96664ca97a2496f3/src%2Fdoc%2Fguide-testing.md", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Fdoc%2Fguide-testing.md?ref=2bfb64e5252179b5bd6d8a2e96664ca97a2496f3", "patch": "@@ -1,283 +1,515 @@\n % The Rust Testing Guide\n \n-# Quick start\n+> Program testing can be a very effective way to show the presence of bugs, but\n+> it is hopelessly inadequate for showing their absence. \n+>\n+> Edsger W. Dijkstra, \"The Humble Programmer\" (1972)\n \n-To create test functions, add a `#[test]` attribute like this:\n+Let's talk about how to test Rust code. What we will not be talking about is\n+the right way to test Rust code. There are many schools of thought regarding\n+the right and wrong way to write tests. All of these approaches use the same\n+basic tools, and so we'll show you the syntax for using them.\n \n-~~~test_harness\n-fn return_two() -> int {\n-    2\n-}\n+# The `test` attribute\n+\n+At its simplest, a test in Rust is a function that's annotated with the `test`\n+attribute. Let's make a new project with Cargo called `adder`:\n+\n+```bash\n+$ cargo new adder\n+$ cd adder\n+```\n \n+Cargo will automatically generate a simple test when you make a new project.\n+Here's the contents of `src/lib.rs`:\n+\n+```rust\n #[test]\n-fn return_two_test() {\n-    let x = return_two();\n-    assert!(x == 2);\n+fn it_works() {\n }\n-~~~\n+```\n+\n+Note the `#[test]`. This attribute indicates that this is a test function. It\n+currently has no body. That's good enough to pass! We can run the tests with\n+`cargo test`:\n \n-To run these tests, compile with `rustc --test` and run the resulting\n-binary:\n+```bash\n+$ cargo test\n+   Compiling adder v0.0.1 (file:///home/you/projects/adder)\n+     Running target/adder-91b3e234d4ed382a\n \n-~~~console\n-$ rustc --test foo.rs\n-$ ./foo\n running 1 test\n-test return_two_test ... ok\n+test it_works ... ok\n \n test result: ok. 1 passed; 0 failed; 0 ignored; 0 measured\n-~~~\n \n-`rustc foo.rs` will *not* compile the tests, since `#[test]` implies\n-`#[cfg(test)]`. The `--test` flag to `rustc` implies `--cfg test`.\n+   Doc-tests adder\n \n+running 0 tests\n \n-# Unit testing in Rust\n+test result: ok. 0 passed; 0 failed; 0 ignored; 0 measured\n+```\n \n-Rust has built in support for simple unit testing. Functions can be\n-marked as unit tests using the `test` attribute.\n+Cargo compiled and ran our tests. There are two sets of output here: one\n+for the test we wrote, and another for documentation tests. We'll talk about\n+those later. For now, see this line:\n+\n+```text\n+test it_works ... ok\n+```\n+\n+Note the `it_works`. This comes from the name of our function:\n+\n+```rust\n+fn it_works() {\n+# }\n+```\n \n-~~~test_harness\n+We also get a summary line:\n+\n+```text\n+test result: ok. 1 passed; 0 failed; 0 ignored; 0 measured\n+```\n+\n+So why does our do-nothing test pass? Any test which doesn't `panic!` passes,\n+and any test that does `panic!` fails. Let's make our test fail:\n+\n+```rust\n #[test]\n-fn return_none_if_empty() {\n-    // ... test code ...\n+fn it_works() {\n+    assert!(false);\n }\n-~~~\n+```\n \n-A test function's signature must have no arguments and no return\n-value. To run the tests in a crate, it must be compiled with the\n-`--test` flag: `rustc myprogram.rs --test -o myprogram-tests`. Running\n-the resulting executable will run all the tests in the crate. A test\n-is considered successful if its function returns; if the task running\n-the test fails, through a call to `panic!`, a failed `assert`, or some\n-other (`assert_eq`, ...) means, then the test fails.\n+`assert!` is a macro provided by Rust which takes one argument: if the argument\n+is `true`, nothing happens. If the argument is false, it `panic!`s. Let's run\n+our tests again:\n \n-When compiling a crate with the `--test` flag `--cfg test` is also\n-implied, so that tests can be conditionally compiled.\n+```bash\n+$ cargo test\n+   Compiling adder v0.0.1 (file:///home/you/projects/adder)\n+     Running target/adder-91b3e234d4ed382a\n \n-~~~test_harness\n-#[cfg(test)]\n-mod tests {\n-    #[test]\n-    fn return_none_if_empty() {\n-      // ... test code ...\n-    }\n+running 1 test\n+test it_works ... FAILED\n+\n+failures:\n+\n+---- it_works stdout ----\n+        task 'it_works' panicked at 'assertion failed: false', /home/steve/tmp/adder/src/lib.rs:3\n+\n+\n+\n+failures:\n+    it_works\n+\n+test result: FAILED. 0 passed; 1 failed; 0 ignored; 0 measured\n+\n+task '<main>' panicked at 'Some tests failed', /home/steve/src/rust/src/libtest/lib.rs:247\n+```\n+\n+Rust indicates that our test failed:\n+\n+```text\n+test it_works ... FAILED\n+```\n+\n+And that's reflected in the summary line:\n+\n+```text\n+test result: FAILED. 0 passed; 1 failed; 0 ignored; 0 measured\n+```\n+\n+We also get a non-zero status code:\n+\n+```bash\n+$ echo $?\n+101\n+```\n+\n+This is useful if you want to integrate `cargo test` into other tooling.\n+\n+We can invert our test's failure with another attribute: `should_fail`:\n+\n+```rust\n+#[test]\n+#[should_fail]\n+fn it_works() {\n+    assert!(false);\n }\n-~~~\n+```\n+\n+This test will now succeed if we `panic!` and fail if we complete. Let's try it:\n+\n+```bash\n+$ cargo test\n+   Compiling adder v0.0.1 (file:///home/you/projects/adder)\n+     Running target/adder-91b3e234d4ed382a\n+\n+running 1 test\n+test it_works ... ok\n+\n+test result: ok. 1 passed; 0 failed; 0 ignored; 0 measured\n+\n+   Doc-tests adder\n \n-Additionally `#[test]` items behave as if they also have the\n-`#[cfg(test)]` attribute, and will not be compiled when the `--test` flag\n-is not used.\n+running 0 tests\n \n-Tests that should not be run can be annotated with the `ignore`\n-attribute. The existence of these tests will be noted in the test\n-runner output, but the test will not be run. Tests can also be ignored\n-by configuration using the `cfg_attr` attribute so, for example, to ignore a\n-test on windows you can write `#[cfg_attr(windows, ignore)]`.\n+test result: ok. 0 passed; 0 failed; 0 ignored; 0 measured\n+```\n \n-Tests that are intended to fail can be annotated with the\n-`should_fail` attribute. The test will be run, and if it causes its\n-task to panic then the test will be counted as successful; otherwise it\n-will be counted as a failure. For example:\n+Rust provides another macro, `assert_eq!`, that compares two arguments for\n+equality:\n \n-~~~test_harness\n+```rust\n #[test]\n #[should_fail]\n-fn test_out_of_bounds_failure() {\n-    let v: &[int] = &[];\n-    v[0];\n+fn it_works() {\n+    assert_eq!(\"Hello\", \"world\");\n }\n-~~~\n+```\n+\n+Does this test pass or fail? Because of the `should_fail` attribute, it\n+passes:\n+\n+```bash\n+$ cargo test\n+   Compiling adder v0.0.1 (file:///home/you/projects/adder)\n+     Running target/adder-91b3e234d4ed382a\n+\n+running 1 test\n+test it_works ... ok\n+\n+test result: ok. 1 passed; 0 failed; 0 ignored; 0 measured\n+\n+   Doc-tests adder\n+\n+running 0 tests\n \n-`#[should_fail]` tests can be fragile as it's hard to guarantee that the test\n+test result: ok. 0 passed; 0 failed; 0 ignored; 0 measured\n+```\n+\n+`should_fail` tests can be fragile, as it's hard to guarantee that the test\n didn't fail for an unexpected reason. To help with this, an optional `expected`\n parameter can be added to the `should_fail` attribute. The test harness will\n make sure that the failure message contains the provided text. A safer version\n of the example above would be:\n \n-~~~test_harness\n+```\n #[test]\n-#[should_fail(expected = \"index out of bounds\")]\n-fn test_out_of_bounds_failure() {\n-    let v: &[int] = &[];\n-    v[0];\n+#[should_fail(expected = \"assertion failed\")]\n+fn it_works() {\n+    assert_eq!(\"Hello\", \"world\");\n }\n-~~~\n+```\n \n-A test runner built with the `--test` flag supports a limited set of\n-arguments to control which tests are run:\n+That's all there is to the basics! Let's write one 'real' test:\n \n-- the first free argument passed to a test runner is interpreted as a\n-  regular expression\n-  ([syntax reference](regex/index.html#syntax))\n-  and is used to narrow down the set of tests being run. Note: a plain\n-  string is a valid regular expression that matches itself.\n-- the `--ignored` flag tells the test runner to run only tests with the\n-  `ignore` attribute.\n+```{rust,ignore}\n+pub fn add_two(a: i32) -> i32 {\n+    a + 2\n+}\n \n-## Parallelism\n+#[test]\n+fn it_works() {\n+    assert_eq!(4, add_two(2));\n+}\n+```\n \n-By default, tests are run in parallel, which can make interpreting\n-failure output difficult. In these cases you can set the\n-`RUST_TEST_TASKS` environment variable to 1 to make the tests run\n-sequentially.\n+This is a very common use of `assert_eq!`: call some function with\n+some known arguments and compare it to the expected output.\n \n-## Examples\n+# The `test` module\n \n-### Typical test run\n+There is one way in which our existing example is not idiomatic: it's\n+missing the test module. The idiomatic way of writing our example\n+looks like this:\n \n-~~~console\n-$ mytests\n+```{rust,ignore}\n+pub fn add_two(a: i32) -> i32 {\n+    a + 2\n+}\n \n-running 30 tests\n-running driver::tests::mytest1 ... ok\n-running driver::tests::mytest2 ... ignored\n-... snip ...\n-running driver::tests::mytest30 ... ok\n+#[cfg(test)]\n+mod tests {\n+    use super::add_two;\n \n-result: ok. 28 passed; 0 failed; 2 ignored\n-~~~\n+    #[test]\n+    fn it_works() {\n+        assert_eq!(4, add_two(2));\n+    }\n+}\n+```\n \n-### Test run with failures\n+There's a few changes here. The first is the introduction of a `mod tests` with\n+a `cfg` attribute. The module allows us to group all of our tests together, and\n+to also define helper functions if needed, that don't become a part of the rest\n+of our crate. The `cfg` attribute only compiles our test code if we're\n+currently trying to run the tests. This can save compile time, and also ensures\n+that our tests are entirely left out of a normal build.\n \n-~~~console\n-$ mytests\n+The second change is the `use` declaration. Because we're in an inner module,\n+we need to bring our test function into scope. This can be annoying if you have\n+a large module, and so this is a common use of the `glob` feature. Let's change\n+our `src/lib.rs` to make use of it:\n \n-running 30 tests\n-running driver::tests::mytest1 ... ok\n-running driver::tests::mytest2 ... ignored\n-... snip ...\n-running driver::tests::mytest30 ... FAILED\n+```{rust,ignore}\n+#![feature(globs)]\n \n-result: FAILED. 27 passed; 1 failed; 2 ignored\n-~~~\n+pub fn add_two(a: i32) -> i32 {\n+    a + 2\n+}\n \n-### Running ignored tests\n+#[cfg(test)]\n+mod tests {\n+    use super::*;\n \n-~~~console\n-$ mytests --ignored\n+    #[test]\n+    fn it_works() {\n+        assert_eq!(4, add_two(2));\n+    }\n+}\n+```\n \n-running 2 tests\n-running driver::tests::mytest2 ... failed\n-running driver::tests::mytest10 ... ok\n+Note the `feature` attribute, as well as the different `use` line. Now we run\n+our tests:\n \n-result: FAILED. 1 passed; 1 failed; 0 ignored\n-~~~\n+```bash\n+$ cargo test\n+    Updating registry `https://github.com/rust-lang/crates.io-index`\n+   Compiling adder v0.0.1 (file:///home/you/projects/adder)\n+     Running target/adder-91b3e234d4ed382a\n \n-### Running a subset of tests\n+running 1 test\n+test test::it_works ... ok\n \n-Using a plain string:\n+test result: ok. 1 passed; 0 failed; 0 ignored; 0 measured\n \n-~~~console\n-$ mytests mytest23\n+   Doc-tests adder\n \n-running 1 tests\n-running driver::tests::mytest23 ... ok\n+running 0 tests\n \n-result: ok. 1 passed; 0 failed; 0 ignored\n-~~~\n+test result: ok. 0 passed; 0 failed; 0 ignored; 0 measured\n+```\n \n-Using some regular expression features:\n+It works!\n \n-~~~console\n-$ mytests 'mytest[145]'\n+The current convention is to use the `test` module to hold your \"unit\"-style\n+tests. Anything that just tests one small bit of functionality makes sense to\n+go here. But what about \"integration\"-style tests instead? For that, we have\n+the `tests` directory\n \n-running 13 tests\n-running driver::tests::mytest1 ... ok\n-running driver::tests::mytest4 ... ok\n-running driver::tests::mytest5 ... ok\n-running driver::tests::mytest10 ... ignored\n-... snip ...\n-running driver::tests::mytest19 ... ok\n+# The `tests` directory\n \n-result: ok. 13 passed; 0 failed; 1 ignored\n-~~~\n+To write an integration test, let's make a `tests` directory, and\n+put a `tests/lib.rs` file inside, with this as its contents:\n \n-# Microbenchmarking\n+```{rust,ignore}\n+extern crate adder;\n \n-The test runner also understands a simple form of benchmark execution.\n-Benchmark functions are marked with the `#[bench]` attribute, rather\n-than `#[test]`, and have a different form and meaning. They are\n-compiled along with `#[test]` functions when a crate is compiled with\n-`--test`, but they are not run by default. To run the benchmark\n-component of your testsuite, pass `--bench` to the compiled test\n-runner.\n+#[test]\n+fn it_works() {\n+    assert_eq(4, adder::add_two(2));\n+}   \n+```\n \n-The type signature of a benchmark function differs from a unit test:\n-it takes a mutable reference to type\n-`test::Bencher`. Inside the benchmark function, any\n-time-variable or \"setup\" code should execute first, followed by a call\n-to `iter` on the benchmark harness, passing a closure that contains\n-the portion of the benchmark you wish to actually measure the\n-per-iteration speed of.\n+This looks similar to our previous tests, but slightly different. We now have\n+an `extern crate adder` at the top. This is because the tests in the `tests`\n+directory are an entirely separate crate, and so we need to import our library.\n+This is also why `tests` is a suitable place to write integration-style tests:\n+they use the library like any other consumer of it would.\n \n-For benchmarks relating to processing/generating data, one can set the\n-`bytes` field to the number of bytes consumed/produced in each\n-iteration; this will be used to show the throughput of the benchmark.\n-This must be the amount used in each iteration, *not* the total\n-amount.\n+Let's run them:\n \n-For example:\n+```bash\n+$ cargo test\n+   Compiling adder v0.0.1 (file:///home/you/projects/adder)\n+     Running target/adder-91b3e234d4ed382a\n \n-~~~test_harness\n-extern crate test;\n+running 1 test\n+test test::it_works ... ok\n \n-use test::Bencher;\n+test result: ok. 1 passed; 0 failed; 0 ignored; 0 measured\n \n-#[bench]\n-fn bench_sum_1024_ints(b: &mut Bencher) {\n-    let v = Vec::from_fn(1024, |n| n);\n-    b.iter(|| v.iter().fold(0, |old, new| old + *new));\n+     Running target/lib-c18e7d3494509e74\n+\n+running 1 test\n+test it_works ... ok\n+\n+test result: ok. 1 passed; 0 failed; 0 ignored; 0 measured\n+\n+   Doc-tests adder\n+\n+running 0 tests\n+\n+test result: ok. 0 passed; 0 failed; 0 ignored; 0 measured\n+```\n+\n+Now we have three sections: our previous test is also run, as well as our new\n+one.\n+\n+That's all there is to the `tests` directory. The `test` module isn't needed\n+here, since the whole thing is focused on tests.\n+\n+Let's finally check out that third section: documentation tests.\n+\n+# Documentation tests\n+\n+Nothing is better than documentation with examples. Nothing is worse than\n+examples that don't actually work, because the code has changed since the\n+documentation has been written. To this end, Rust supports automaticaly\n+running examples in your documentation. Here's a fleshed-out `src/lib.rs`\n+with examples:\n+\n+```{rust,ignore}\n+//! The `adder` crate provides functions that add numbers to other numbers.\n+//!\n+//! # Examples\n+//!\n+//! ```\n+//! assert_eq!(4, adder::add_two(2));\n+//! ```\n+\n+#![feature(globs)]\n+\n+/// This function adds two to its argument.\n+///\n+/// # Examples\n+///\n+/// ```\n+/// use adder::add_two;\n+///\n+/// assert_eq!(4, add_two(2));\n+/// ```\n+pub fn add_two(a: i32) -> i32 {\n+    a + 2\n }\n \n-#[bench]\n-fn initialise_a_vector(b: &mut Bencher) {\n-    b.iter(|| Vec::from_elem(1024, 0u64));\n-    b.bytes = 1024 * 8;\n+#[cfg(test)]\n+mod tests {\n+    use super::*;\n+\n+    #[test]\n+    fn it_works() {\n+        assert_eq!(4, add_two(2));\n+    }\n }\n-~~~\n+```\n \n-The benchmark runner will calibrate measurement of the benchmark\n-function to run the `iter` block \"enough\" times to get a reliable\n-measure of the per-iteration speed.\n+Note the module-level documentation with `//!` and the function-level\n+documentation with `///`. Rust's documentation supports Markdown in comments,\n+and so triple graves mark code blocks. It is conventional to include the\n+`# Examples` section, exactly like that, with examples following.\n \n-Advice on writing benchmarks:\n+Let's run the tests again:\n \n-  - Move setup code outside the `iter` loop; only put the part you\n-    want to measure inside\n-  - Make the code do \"the same thing\" on each iteration; do not\n-    accumulate or change state\n-  - Make the outer function idempotent too; the benchmark runner is\n-    likely to run it many times\n-  - Make the inner `iter` loop short and fast so benchmark runs are\n-    fast and the calibrator can adjust the run-length at fine\n-    resolution\n-  - Make the code in the `iter` loop do something simple, to assist in\n-    pinpointing performance improvements (or regressions)\n-\n-To run benchmarks, pass the `--bench` flag to the compiled\n-test-runner. Benchmarks are compiled-in but not executed by default.\n-\n-~~~console\n-$ rustc mytests.rs -O --test\n-$ mytests --bench\n+```bash\n+$ cargo test\n+   Compiling adder v0.0.1 (file:///home/steve/tmp/adder)\n+     Running target/adder-91b3e234d4ed382a\n+\n+running 1 test\n+test test::it_works ... ok\n+\n+test result: ok. 1 passed; 0 failed; 0 ignored; 0 measured\n+\n+     Running target/lib-c18e7d3494509e74\n+\n+running 1 test\n+test it_works ... ok\n+\n+test result: ok. 1 passed; 0 failed; 0 ignored; 0 measured\n+\n+   Doc-tests adder\n \n running 2 tests\n-test bench_sum_1024_ints ... bench: 709 ns/iter (+/- 82)\n-test initialise_a_vector ... bench: 424 ns/iter (+/- 99) = 19320 MB/s\n+test add_two_0 ... ok\n+test _0 ... ok\n+\n+test result: ok. 2 passed; 0 failed; 0 ignored; 0 measured\n+```\n+\n+Now we have all three kinds of tests running! Note the names of the\n+documentation tests: the `_0` is generated for the module test, and `add_two_0`\n+for the function test. These will auto increment with names like `add_two_1` as\n+you add more examples.\n+\n+# Benchmark tests\n+\n+Rust also supports benchmark tests, which can test the performance of your\n+code. Let's make our `src/lib.rs` look like this (comments elided):\n+\n+```{rust,ignore}\n+#![feature(globs)]\n+\n+extern crate test;\n+\n+pub fn add_two(a: i32) -> i32 {\n+    a + 2\n+}\n+\n+#[cfg(test)]\n+mod tests {\n+    use super::*;\n+    use test::Bencher;\n+\n+    #[test]\n+    fn it_works() {\n+        assert_eq!(4, add_two(2));\n+    }\n+\n+    #[bench]\n+    fn bench_add_two(b: &mut Bencher) {\n+        b.iter(|| add_two(2));\n+    }\n+}\n+```\n+\n+We've imported the `test` crate, which contains our benchmarking support.\n+We have a new function as well, with the `bench` attribute. Unlike regular\n+tests, which take no arguments, benchmark tests take a `&mut Bencher`. This\n+`Bencher` provides an `iter` method, which takes a closure. This closure\n+contains the code we'd like to benchmark.\n+\n+We can run benchmark tests with `cargo bench`:\n+\n+```bash\n+$ cargo bench\n+   Compiling adder v0.0.1 (file:///home/steve/tmp/adder)\n+     Running target/release/adder-91b3e234d4ed382a\n+\n+running 2 tests\n+test tests::it_works ... ignored\n+test tests::bench_add_two ... bench:         1 ns/iter (+/- 0)\n+\n+test result: ok. 0 passed; 0 failed; 1 ignored; 1 measured\n+```\n+\n+Our non-benchmark test was ignored. You may have noticed that `cargo bench`\n+takes a bit longer than `cargo test`. This is because Rust runs our benchmark\n+a number of times, and then takes the average. Because we're doing so little\n+work in this example, we have a `1 ns/iter (+/- 0)`, but this would show\n+the variance if there was one.\n+\n+Advice on writing benchmarks:\n \n-test result: ok. 0 passed; 0 failed; 0 ignored; 2 measured\n-~~~\n \n-## Benchmarks and the optimizer\n+* Move setup code outside the `iter` loop; only put the part you want to measure inside\n+* Make the code do \"the same thing\" on each iteration; do not accumulate or change state\n+* Make the outer function idempotent too; the benchmark runner is likely to run\n+  it many times\n+*  Make the inner `iter` loop short and fast so benchmark runs are fast and the\n+   calibrator can adjust the run-length at fine resolution\n+* Make the code in the `iter` loop do something simple, to assist in pinpointing\n+  performance improvements (or regressions)\n \n-Benchmarks compiled with optimizations activated can be dramatically\n-changed by the optimizer so that the benchmark is no longer\n-benchmarking what one expects. For example, the compiler might\n-recognize that some calculation has no external effects and remove\n-it entirely.\n+There's another tricky part to writing benchmarks: benchmarks compiled with\n+optimizations activated can be dramatically changed by the optimizer so that\n+the benchmark is no longer benchmarking what one expects. For example, the\n+compiler might recognize that some calculation has no external effects and\n+remove it entirely.\n \n-~~~test_harness\n+```{rust,ignore}\n extern crate test;\n use test::Bencher;\n \n@@ -287,36 +519,36 @@ fn bench_xor_1000_ints(b: &mut Bencher) {\n         range(0u, 1000).fold(0, |old, new| old ^ new);\n     });\n }\n-~~~\n+```\n \n gives the following results\n \n-~~~console\n+```text\n running 1 test\n test bench_xor_1000_ints ... bench:         0 ns/iter (+/- 0)\n \n test result: ok. 0 passed; 0 failed; 0 ignored; 1 measured\n-~~~\n+```\n \n-The benchmarking runner offers two ways to avoid this. Either, the\n-closure that the `iter` method receives can return an arbitrary value\n-which forces the optimizer to consider the result used and ensures it\n-cannot remove the computation entirely. This could be done for the\n-example above by adjusting the `b.iter` call to\n+The benchmarking runner offers two ways to avoid this. Either, the closure that\n+the `iter` method receives can return an arbitrary value which forces the\n+optimizer to consider the result used and ensures it cannot remove the\n+computation entirely. This could be done for the example above by adjusting the\n+`b.iter` call to\n \n-~~~\n+```rust\n # struct X; impl X { fn iter<T>(&self, _: || -> T) {} } let b = X;\n b.iter(|| {\n     // note lack of `;` (could also use an explicit `return`).\n     range(0u, 1000).fold(0, |old, new| old ^ new)\n });\n-~~~\n+```\n \n-Or, the other option is to call the generic `test::black_box`\n-function, which is an opaque \"black box\" to the optimizer and so\n-forces it to consider any argument as used.\n+Or, the other option is to call the generic `test::black_box` function, which\n+is an opaque \"black box\" to the optimizer and so forces it to consider any\n+argument as used.\n \n-~~~\n+```rust\n extern crate test;\n \n # fn main() {\n@@ -325,54 +557,17 @@ b.iter(|| {\n     test::black_box(range(0u, 1000).fold(0, |old, new| old ^ new));\n });\n # }\n-~~~\n+```\n \n-Neither of these read or modify the value, and are very cheap for\n-small values. Larger values can be passed indirectly to reduce\n-overhead (e.g. `black_box(&huge_struct)`).\n+Neither of these read or modify the value, and are very cheap for small values.\n+Larger values can be passed indirectly to reduce overhead (e.g.\n+`black_box(&huge_struct)`).\n \n-Performing either of the above changes gives the following\n-benchmarking results\n+Performing either of the above changes gives the following benchmarking results\n \n-~~~console\n+```text\n running 1 test\n-test bench_xor_1000_ints ... bench:       375 ns/iter (+/- 148)\n+test bench_xor_1000_ints ... bench:       1 ns/iter (+/- 0)\n \n test result: ok. 0 passed; 0 failed; 0 ignored; 1 measured\n-~~~\n-\n-However, the optimizer can still modify a testcase in an undesirable\n-manner even when using either of the above. Benchmarks can be checked\n-by hand by looking at the output of the compiler using the `--emit=ir`\n-(for LLVM IR), `--emit=asm` (for assembly) or compiling normally and\n-using any method for examining object code.\n-\n-## Saving and ratcheting metrics\n-\n-When running benchmarks or other tests, the test runner can record\n-per-test \"metrics\". Each metric is a scalar `f64` value, plus a noise\n-value which represents uncertainty in the measurement. By default, all\n-`#[bench]` benchmarks are recorded as metrics, which can be saved as\n-JSON in an external file for further reporting.\n-\n-In addition, the test runner supports _ratcheting_ against a metrics\n-file. Ratcheting is like saving metrics, except that after each run,\n-if the output file already exists the results of the current run are\n-compared against the contents of the existing file, and any regression\n-_causes the testsuite to fail_. If the comparison passes -- if all\n-metrics stayed the same (within noise) or improved -- then the metrics\n-file is overwritten with the new values. In this way, a metrics file\n-in your workspace can be used to ensure your work does not regress\n-performance.\n-\n-Test runners take 3 options that are relevant to metrics:\n-\n-  - `--save-metrics=<file.json>` will save the metrics from a test run\n-    to `file.json`\n-  - `--ratchet-metrics=<file.json>` will ratchet the metrics against\n-    the `file.json`\n-  - `--ratchet-noise-percent=N` will override the noise measurements\n-    in `file.json`, and consider a metric change less than `N%` to be\n-    noise. This can be helpful if you are testing in a noisy\n-    environment where the benchmark calibration loop cannot acquire a\n-    clear enough signal.\n+```"}]}