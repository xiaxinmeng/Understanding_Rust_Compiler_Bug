{"sha": "0363c9495a6a07db276dce4c67fa35fbfc20153c", "node_id": "MDY6Q29tbWl0NzI0NzEyOjAzNjNjOTQ5NWE2YTA3ZGIyNzZkY2U0YzY3ZmEzNWZiZmMyMDE1M2M=", "commit": {"author": {"name": "bors[bot]", "email": "26634292+bors[bot]@users.noreply.github.com", "date": "2020-03-09T08:43:07Z"}, "committer": {"name": "GitHub", "email": "noreply@github.com", "date": "2020-03-09T08:43:07Z"}, "message": "Merge #3518\n\n3518: Add parse_to_token_tree r=matklad a=edwin0cheng\n\nThis PR introduce a function for parsing `&str` to `tt::TokenTree`:\r\n\r\n```rust\r\n// Convert a string to a `TokenTree`\r\npub fn parse_to_token_tree(text: &str) -> Option<(tt::Subtree, TokenMap)> {\r\n````\n\nCo-authored-by: Edwin Cheng <edwin0cheng@gmail.com>", "tree": {"sha": "0a98e2b4659d0d0f4df98f41613c6da90e19551f", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/0a98e2b4659d0d0f4df98f41613c6da90e19551f"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/0363c9495a6a07db276dce4c67fa35fbfc20153c", "comment_count": 0, "verification": {"verified": true, "reason": "valid", "signature": "-----BEGIN PGP SIGNATURE-----\n\nwsBcBAABCAAQBQJeZgGbCRBK7hj4Ov3rIwAAdHIIAGubZDjCrInPm5zOqV9L2JCN\nOLOgI4x00+jGPXS2pXTBEa+aWJY5Kc+ttzomXx4VKGRsT6+QyyndtQhq7vLnB20e\n8rsAc4TFmymlr5AitUOW6kMkF6NkmOeC7z/n8J5MjxbQ2u4V0EB2X0oxtF2SZsGW\nAxJ/W5IIQKbv4qLbmXLgeDT7AmGstpk/iZBSSdhynVvD90AGfATZ9+Qj8ZZroIvp\nEs3Oz5EMVoMBKhnrAbX3lubR0wzZd1jgrJ5E8RnkzT25g1F2BF3mxGn36ae0oVH5\n6hHrQnrxTfJJ9PrYv2rR+6CY+AvLzTBTHfucCVakMOpZBkwnfIhtc0F7f4EoeBg=\n=/Gev\n-----END PGP SIGNATURE-----\n", "payload": "tree 0a98e2b4659d0d0f4df98f41613c6da90e19551f\nparent 7ac99aad28a36e7cdb27edcb319d7f540dbd8471\nparent e7206467d57c555f1ca1fee6acc0461d7579f4f7\nauthor bors[bot] <26634292+bors[bot]@users.noreply.github.com> 1583743387 +0000\ncommitter GitHub <noreply@github.com> 1583743387 +0000\n\nMerge #3518\n\n3518: Add parse_to_token_tree r=matklad a=edwin0cheng\n\nThis PR introduce a function for parsing `&str` to `tt::TokenTree`:\r\n\r\n```rust\r\n// Convert a string to a `TokenTree`\r\npub fn parse_to_token_tree(text: &str) -> Option<(tt::Subtree, TokenMap)> {\r\n````\n\nCo-authored-by: Edwin Cheng <edwin0cheng@gmail.com>\n"}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/0363c9495a6a07db276dce4c67fa35fbfc20153c", "html_url": "https://github.com/rust-lang/rust/commit/0363c9495a6a07db276dce4c67fa35fbfc20153c", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/0363c9495a6a07db276dce4c67fa35fbfc20153c/comments", "author": {"login": "bors[bot]", "id": 26634292, "node_id": "MDM6Qm90MjY2MzQyOTI=", "avatar_url": "https://avatars.githubusercontent.com/in/1847?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bors%5Bbot%5D", "html_url": "https://github.com/apps/bors", "followers_url": "https://api.github.com/users/bors%5Bbot%5D/followers", "following_url": "https://api.github.com/users/bors%5Bbot%5D/following{/other_user}", "gists_url": "https://api.github.com/users/bors%5Bbot%5D/gists{/gist_id}", "starred_url": "https://api.github.com/users/bors%5Bbot%5D/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bors%5Bbot%5D/subscriptions", "organizations_url": "https://api.github.com/users/bors%5Bbot%5D/orgs", "repos_url": "https://api.github.com/users/bors%5Bbot%5D/repos", "events_url": "https://api.github.com/users/bors%5Bbot%5D/events{/privacy}", "received_events_url": "https://api.github.com/users/bors%5Bbot%5D/received_events", "type": "Bot", "site_admin": false}, "committer": {"login": "web-flow", "id": 19864447, "node_id": "MDQ6VXNlcjE5ODY0NDQ3", "avatar_url": "https://avatars.githubusercontent.com/u/19864447?v=4", "gravatar_id": "", "url": "https://api.github.com/users/web-flow", "html_url": "https://github.com/web-flow", "followers_url": "https://api.github.com/users/web-flow/followers", "following_url": "https://api.github.com/users/web-flow/following{/other_user}", "gists_url": "https://api.github.com/users/web-flow/gists{/gist_id}", "starred_url": "https://api.github.com/users/web-flow/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/web-flow/subscriptions", "organizations_url": "https://api.github.com/users/web-flow/orgs", "repos_url": "https://api.github.com/users/web-flow/repos", "events_url": "https://api.github.com/users/web-flow/events{/privacy}", "received_events_url": "https://api.github.com/users/web-flow/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "7ac99aad28a36e7cdb27edcb319d7f540dbd8471", "url": "https://api.github.com/repos/rust-lang/rust/commits/7ac99aad28a36e7cdb27edcb319d7f540dbd8471", "html_url": "https://github.com/rust-lang/rust/commit/7ac99aad28a36e7cdb27edcb319d7f540dbd8471"}, {"sha": "e7206467d57c555f1ca1fee6acc0461d7579f4f7", "url": "https://api.github.com/repos/rust-lang/rust/commits/e7206467d57c555f1ca1fee6acc0461d7579f4f7", "html_url": "https://github.com/rust-lang/rust/commit/e7206467d57c555f1ca1fee6acc0461d7579f4f7"}], "stats": {"total": 250, "additions": 213, "deletions": 37}, "files": [{"sha": "3f60b1cca2f31470bcf6168ecfedebec06173e6c", "filename": "crates/ra_hir_expand/src/builtin_macro.rs", "status": "modified", "additions": 4, "deletions": 4, "changes": 8, "blob_url": "https://github.com/rust-lang/rust/blob/0363c9495a6a07db276dce4c67fa35fbfc20153c/crates%2Fra_hir_expand%2Fsrc%2Fbuiltin_macro.rs", "raw_url": "https://github.com/rust-lang/rust/raw/0363c9495a6a07db276dce4c67fa35fbfc20153c/crates%2Fra_hir_expand%2Fsrc%2Fbuiltin_macro.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fra_hir_expand%2Fsrc%2Fbuiltin_macro.rs?ref=0363c9495a6a07db276dce4c67fa35fbfc20153c", "patch": "@@ -7,6 +7,7 @@ use crate::{\n \n use crate::{quote, EagerMacroId, LazyMacroId, MacroCallId};\n use either::Either;\n+use mbe::parse_to_token_tree;\n use ra_db::{FileId, RelativePath};\n use ra_parser::FragmentKind;\n \n@@ -306,10 +307,9 @@ fn include_expand(\n \n     // FIXME:\n     // Handle include as expression\n-    let node =\n-        db.parse_or_expand(file_id.into()).ok_or_else(|| mbe::ExpandError::ConversionError)?;\n-    let res =\n-        mbe::syntax_node_to_token_tree(&node).ok_or_else(|| mbe::ExpandError::ConversionError)?.0;\n+    let res = parse_to_token_tree(&db.file_text(file_id.into()))\n+        .ok_or_else(|| mbe::ExpandError::ConversionError)?\n+        .0;\n \n     Ok((res, FragmentKind::Items))\n }"}, {"sha": "43afe24ccb0caa70453b50c0ea7db65e47020553", "filename": "crates/ra_mbe/src/lib.rs", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "blob_url": "https://github.com/rust-lang/rust/blob/0363c9495a6a07db276dce4c67fa35fbfc20153c/crates%2Fra_mbe%2Fsrc%2Flib.rs", "raw_url": "https://github.com/rust-lang/rust/raw/0363c9495a6a07db276dce4c67fa35fbfc20153c/crates%2Fra_mbe%2Fsrc%2Flib.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fra_mbe%2Fsrc%2Flib.rs?ref=0363c9495a6a07db276dce4c67fa35fbfc20153c", "patch": "@@ -31,7 +31,8 @@ pub enum ExpandError {\n }\n \n pub use crate::syntax_bridge::{\n-    ast_to_token_tree, syntax_node_to_token_tree, token_tree_to_syntax_node, TokenMap,\n+    ast_to_token_tree, parse_to_token_tree, syntax_node_to_token_tree, token_tree_to_syntax_node,\n+    TokenMap,\n };\n \n /// This struct contains AST for a single `macro_rules` definition. What might"}, {"sha": "fcb73fbc7c59753cbe8fb673c4caff060b3bef4d", "filename": "crates/ra_mbe/src/syntax_bridge.rs", "status": "modified", "additions": 191, "deletions": 30, "changes": 221, "blob_url": "https://github.com/rust-lang/rust/blob/0363c9495a6a07db276dce4c67fa35fbfc20153c/crates%2Fra_mbe%2Fsrc%2Fsyntax_bridge.rs", "raw_url": "https://github.com/rust-lang/rust/raw/0363c9495a6a07db276dce4c67fa35fbfc20153c/crates%2Fra_mbe%2Fsrc%2Fsyntax_bridge.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fra_mbe%2Fsrc%2Fsyntax_bridge.rs?ref=0363c9495a6a07db276dce4c67fa35fbfc20153c", "patch": "@@ -2,8 +2,10 @@\n \n use ra_parser::{FragmentKind, ParseError, TreeSink};\n use ra_syntax::{\n-    ast, AstToken, NodeOrToken, Parse, SmolStr, SyntaxKind, SyntaxKind::*, SyntaxNode,\n-    SyntaxTreeBuilder, TextRange, TextUnit, T,\n+    ast::{self, make::tokens::doc_comment},\n+    tokenize, AstToken, NodeOrToken, Parse, SmolStr, SyntaxKind,\n+    SyntaxKind::*,\n+    SyntaxNode, SyntaxTreeBuilder, TextRange, TextUnit, Token, T,\n };\n use rustc_hash::FxHashMap;\n use std::iter::successors;\n@@ -48,9 +50,11 @@ pub fn ast_to_token_tree(ast: &impl ast::AstNode) -> Option<(tt::Subtree, TokenM\n /// will consume).\n pub fn syntax_node_to_token_tree(node: &SyntaxNode) -> Option<(tt::Subtree, TokenMap)> {\n     let global_offset = node.text_range().start();\n-    let mut c = Convertor { map: TokenMap::default(), global_offset, next_id: 0 };\n+    let mut c = Convertor {\n+        id_alloc: { TokenIdAlloc { map: TokenMap::default(), global_offset, next_id: 0 } },\n+    };\n     let subtree = c.go(node)?;\n-    Some((subtree, c.map))\n+    Some((subtree, c.id_alloc.map))\n }\n \n // The following items are what `rustc` macro can be parsed into :\n@@ -89,6 +93,28 @@ pub fn token_tree_to_syntax_node(\n     Ok((parse, range_map))\n }\n \n+/// Convert a string to a `TokenTree`\n+pub fn parse_to_token_tree(text: &str) -> Option<(tt::Subtree, TokenMap)> {\n+    let (tokens, errors) = tokenize(text);\n+    if !errors.is_empty() {\n+        return None;\n+    }\n+\n+    let mut conv = RawConvertor {\n+        text,\n+        offset: TextUnit::default(),\n+        inner: tokens.iter(),\n+        id_alloc: TokenIdAlloc {\n+            map: Default::default(),\n+            global_offset: TextUnit::default(),\n+            next_id: 0,\n+        },\n+    };\n+\n+    let subtree = conv.go()?;\n+    Some((subtree, conv.id_alloc.map))\n+}\n+\n impl TokenMap {\n     pub fn token_by_range(&self, relative_range: TextRange) -> Option<tt::TokenId> {\n         let &(token_id, _) = self.entries.iter().find(|(_, range)| match range {\n@@ -118,6 +144,14 @@ impl TokenMap {\n         self.entries\n             .push((token_id, TokenTextRange::Delimiter(open_relative_range, close_relative_range)));\n     }\n+\n+    fn update_close_delim(&mut self, token_id: tt::TokenId, close_relative_range: TextRange) {\n+        if let Some(entry) = self.entries.iter_mut().find(|(tid, _)| *tid == token_id) {\n+            if let TokenTextRange::Delimiter(dim, _) = entry.1 {\n+                entry.1 = TokenTextRange::Delimiter(dim, close_relative_range);\n+            }\n+        }\n+    }\n }\n \n /// Returns the textual content of a doc comment block as a quoted string\n@@ -188,12 +222,161 @@ fn convert_doc_comment(token: &ra_syntax::SyntaxToken) -> Option<Vec<tt::TokenTr\n     }\n }\n \n-struct Convertor {\n+struct TokenIdAlloc {\n     map: TokenMap,\n     global_offset: TextUnit,\n     next_id: u32,\n }\n \n+impl TokenIdAlloc {\n+    fn alloc(&mut self, absolute_range: TextRange) -> tt::TokenId {\n+        let relative_range = absolute_range - self.global_offset;\n+        let token_id = tt::TokenId(self.next_id);\n+        self.next_id += 1;\n+        self.map.insert(token_id, relative_range);\n+        token_id\n+    }\n+\n+    fn delim(&mut self, open_abs_range: TextRange, close_abs_range: TextRange) -> tt::TokenId {\n+        let open_relative_range = open_abs_range - self.global_offset;\n+        let close_relative_range = close_abs_range - self.global_offset;\n+        let token_id = tt::TokenId(self.next_id);\n+        self.next_id += 1;\n+\n+        self.map.insert_delim(token_id, open_relative_range, close_relative_range);\n+        token_id\n+    }\n+\n+    fn open_delim(&mut self, open_abs_range: TextRange) -> tt::TokenId {\n+        let token_id = tt::TokenId(self.next_id);\n+        self.next_id += 1;\n+        self.map.insert_delim(token_id, open_abs_range, open_abs_range);\n+        token_id\n+    }\n+\n+    fn close_delim(&mut self, id: tt::TokenId, close_abs_range: TextRange) {\n+        self.map.update_close_delim(id, close_abs_range);\n+    }\n+}\n+\n+/// A Raw Token (straightly from lexer) convertor\n+struct RawConvertor<'a> {\n+    text: &'a str,\n+    offset: TextUnit,\n+    id_alloc: TokenIdAlloc,\n+    inner: std::slice::Iter<'a, Token>,\n+}\n+\n+impl RawConvertor<'_> {\n+    fn go(&mut self) -> Option<tt::Subtree> {\n+        let mut subtree = tt::Subtree::default();\n+        subtree.delimiter = None;\n+        while self.peek().is_some() {\n+            self.collect_leaf(&mut subtree.token_trees);\n+        }\n+        if subtree.token_trees.is_empty() {\n+            return None;\n+        }\n+        if subtree.token_trees.len() == 1 {\n+            if let tt::TokenTree::Subtree(first) = &subtree.token_trees[0] {\n+                return Some(first.clone());\n+            }\n+        }\n+        Some(subtree)\n+    }\n+\n+    fn bump(&mut self) -> Option<(Token, TextRange)> {\n+        let token = self.inner.next()?;\n+        let range = TextRange::offset_len(self.offset, token.len);\n+        self.offset += token.len;\n+        Some((*token, range))\n+    }\n+\n+    fn peek(&self) -> Option<Token> {\n+        self.inner.as_slice().get(0).cloned()\n+    }\n+\n+    fn collect_leaf(&mut self, result: &mut Vec<tt::TokenTree>) {\n+        let (token, range) = match self.bump() {\n+            None => return,\n+            Some(it) => it,\n+        };\n+\n+        let k: SyntaxKind = token.kind;\n+        if k == COMMENT {\n+            let node = doc_comment(&self.text[range]);\n+            if let Some(tokens) = convert_doc_comment(&node) {\n+                result.extend(tokens);\n+            }\n+            return;\n+        }\n+\n+        result.push(if k.is_punct() {\n+            let delim = match k {\n+                T!['('] => Some((tt::DelimiterKind::Parenthesis, T![')'])),\n+                T!['{'] => Some((tt::DelimiterKind::Brace, T!['}'])),\n+                T!['['] => Some((tt::DelimiterKind::Bracket, T![']'])),\n+                _ => None,\n+            };\n+\n+            if let Some((kind, closed)) = delim {\n+                let mut subtree = tt::Subtree::default();\n+                let id = self.id_alloc.open_delim(range);\n+                subtree.delimiter = Some(tt::Delimiter { kind, id });\n+\n+                while self.peek().map(|it| it.kind != closed).unwrap_or(false) {\n+                    self.collect_leaf(&mut subtree.token_trees);\n+                }\n+                let last_range = match self.bump() {\n+                    None => return,\n+                    Some(it) => it.1,\n+                };\n+                self.id_alloc.close_delim(id, last_range);\n+                subtree.into()\n+            } else {\n+                let spacing = match self.peek() {\n+                    Some(next)\n+                        if next.kind.is_trivia()\n+                            || next.kind == T!['[']\n+                            || next.kind == T!['{']\n+                            || next.kind == T!['('] =>\n+                    {\n+                        tt::Spacing::Alone\n+                    }\n+                    Some(next) if next.kind.is_punct() => tt::Spacing::Joint,\n+                    _ => tt::Spacing::Alone,\n+                };\n+                let char =\n+                    self.text[range].chars().next().expect(\"Token from lexer must be single char\");\n+\n+                tt::Leaf::from(tt::Punct { char, spacing, id: self.id_alloc.alloc(range) }).into()\n+            }\n+        } else {\n+            macro_rules! make_leaf {\n+                ($i:ident) => {\n+                    tt::$i { id: self.id_alloc.alloc(range), text: self.text[range].into() }.into()\n+                };\n+            }\n+            let leaf: tt::Leaf = match k {\n+                T![true] | T![false] => make_leaf!(Literal),\n+                IDENT | LIFETIME => make_leaf!(Ident),\n+                k if k.is_keyword() => make_leaf!(Ident),\n+                k if k.is_literal() => make_leaf!(Literal),\n+                _ => return,\n+            };\n+\n+            leaf.into()\n+        });\n+    }\n+}\n+\n+// FIXME: There are some duplicate logic between RawConvertor and Convertor\n+// It would be nice to refactor to converting SyntaxNode to ra_parser::Token and thus\n+// use RawConvertor directly. But performance-wise it may not be a good idea ?\n+struct Convertor {\n+    id_alloc: TokenIdAlloc,\n+}\n+\n impl Convertor {\n     fn go(&mut self, tt: &SyntaxNode) -> Option<tt::Subtree> {\n         // This tree is empty\n@@ -236,7 +419,7 @@ impl Convertor {\n         };\n         let delimiter = delimiter_kind.map(|kind| tt::Delimiter {\n             kind,\n-            id: self.alloc_delim(first_child.text_range(), last_child.text_range()),\n+            id: self.id_alloc.delim(first_child.text_range(), last_child.text_range()),\n         });\n \n         let mut token_trees = Vec::new();\n@@ -273,7 +456,7 @@ impl Convertor {\n                                 tt::Leaf::from(tt::Punct {\n                                     char,\n                                     spacing,\n-                                    id: self.alloc(token.text_range()),\n+                                    id: self.id_alloc.alloc(token.text_range()),\n                                 })\n                                 .into(),\n                             );\n@@ -282,7 +465,7 @@ impl Convertor {\n                         macro_rules! make_leaf {\n                             ($i:ident) => {\n                                 tt::$i {\n-                                    id: self.alloc(token.text_range()),\n+                                    id: self.id_alloc.alloc(token.text_range()),\n                                     text: token.text().clone(),\n                                 }\n                                 .into()\n@@ -313,28 +496,6 @@ impl Convertor {\n         let res = tt::Subtree { delimiter, token_trees };\n         Some(res)\n     }\n-\n-    fn alloc(&mut self, absolute_range: TextRange) -> tt::TokenId {\n-        let relative_range = absolute_range - self.global_offset;\n-        let token_id = tt::TokenId(self.next_id);\n-        self.next_id += 1;\n-        self.map.insert(token_id, relative_range);\n-        token_id\n-    }\n-\n-    fn alloc_delim(\n-        &mut self,\n-        open_abs_range: TextRange,\n-        close_abs_range: TextRange,\n-    ) -> tt::TokenId {\n-        let open_relative_range = open_abs_range - self.global_offset;\n-        let close_relative_range = close_abs_range - self.global_offset;\n-        let token_id = tt::TokenId(self.next_id);\n-        self.next_id += 1;\n-\n-        self.map.insert_delim(token_id, open_relative_range, close_relative_range);\n-        token_id\n-    }\n }\n \n struct TtTreeSink<'a> {"}, {"sha": "6d5d1e9e601aca2b9ea3eebc5fe51487f8f84249", "filename": "crates/ra_mbe/src/tests.rs", "status": "modified", "additions": 10, "deletions": 2, "changes": 12, "blob_url": "https://github.com/rust-lang/rust/blob/0363c9495a6a07db276dce4c67fa35fbfc20153c/crates%2Fra_mbe%2Fsrc%2Ftests.rs", "raw_url": "https://github.com/rust-lang/rust/raw/0363c9495a6a07db276dce4c67fa35fbfc20153c/crates%2Fra_mbe%2Fsrc%2Ftests.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fra_mbe%2Fsrc%2Ftests.rs?ref=0363c9495a6a07db276dce4c67fa35fbfc20153c", "patch": "@@ -1499,12 +1499,20 @@ impl MacroFixture {\n     }\n }\n \n-pub(crate) fn parse_macro(macro_definition: &str) -> MacroFixture {\n-    let source_file = ast::SourceFile::parse(macro_definition).ok().unwrap();\n+pub(crate) fn parse_macro(ra_fixture: &str) -> MacroFixture {\n+    let source_file = ast::SourceFile::parse(ra_fixture).ok().unwrap();\n     let macro_definition =\n         source_file.syntax().descendants().find_map(ast::MacroCall::cast).unwrap();\n \n     let (definition_tt, _) = ast_to_token_tree(&macro_definition.token_tree().unwrap()).unwrap();\n+\n+    let parsed = parse_to_token_tree(\n+        &ra_fixture[macro_definition.token_tree().unwrap().syntax().text_range()],\n+    )\n+    .unwrap()\n+    .0;\n+    assert_eq!(definition_tt, parsed);\n+\n     let rules = MacroRules::parse(&definition_tt).unwrap();\n     MacroFixture { rules }\n }"}, {"sha": "ae8829807ce79ecc5ad84f41104db26c3681c402", "filename": "crates/ra_syntax/src/ast/make.rs", "status": "modified", "additions": 6, "deletions": 0, "changes": 6, "blob_url": "https://github.com/rust-lang/rust/blob/0363c9495a6a07db276dce4c67fa35fbfc20153c/crates%2Fra_syntax%2Fsrc%2Fast%2Fmake.rs", "raw_url": "https://github.com/rust-lang/rust/raw/0363c9495a6a07db276dce4c67fa35fbfc20153c/crates%2Fra_syntax%2Fsrc%2Fast%2Fmake.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fra_syntax%2Fsrc%2Fast%2Fmake.rs?ref=0363c9495a6a07db276dce4c67fa35fbfc20153c", "patch": "@@ -267,6 +267,12 @@ pub mod tokens {\n         sf.syntax().first_child_or_token().unwrap().into_token().unwrap()\n     }\n \n+    pub fn doc_comment(text: &str) -> SyntaxToken {\n+        assert!(!text.trim().is_empty());\n+        let sf = SourceFile::parse(text).ok().unwrap();\n+        sf.syntax().first_child_or_token().unwrap().into_token().unwrap()\n+    }\n+\n     pub fn literal(text: &str) -> SyntaxToken {\n         assert_eq!(text.trim(), text);\n         let lit: ast::Literal = super::ast_from_text(&format!(\"fn f() {{ let _ = {}; }}\", text));"}]}