{"sha": "ccfc292999e133193f61feacd70031c90884636b", "node_id": "MDY6Q29tbWl0NzI0NzEyOmNjZmMyOTI5OTllMTMzMTkzZjYxZmVhY2Q3MDAzMWM5MDg4NDYzNmI=", "commit": {"author": {"name": "Aaron Hill", "email": "aa1ronham@gmail.com", "date": "2021-01-14T15:42:01Z"}, "committer": {"name": "Aaron Hill", "email": "aa1ronham@gmail.com", "date": "2021-01-22T05:33:03Z"}, "message": "Refactor token collection to capture trailing token immediately", "tree": {"sha": "7b6c4584b12203c6943d213abcbe31bf5f18d885", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/7b6c4584b12203c6943d213abcbe31bf5f18d885"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/ccfc292999e133193f61feacd70031c90884636b", "comment_count": 0, "verification": {"verified": true, "reason": "valid", "signature": "-----BEGIN PGP SIGNATURE-----\n\niQIzBAABCAAdFiEE7J9Gc3TfBwj2K399tAh+UQ6YsWQFAmAKY48ACgkQtAh+UQ6Y\nsWRy/xAAhLUUpNH1y97p6EFtn6bpOueCxC/7pa+nk87dzbihxR4ElUwhfyGP4Emh\nqVHccSa22XcIMyX3ObytwJ7Llz44pCzSVmgf9Cx+Hwy62I/cNBCnTYj9mGdnoNuL\nE3p/826nCDfGFZ23WDVbQE8SS+8lkgFGjKxBHydPhCq4ZocJi3zWx+H1KoDFmoHs\nweikXI7ahnOkbppo6I/TipnWzmO86XA2HgUlVJVmdiWIWTi8ymFRGknNP7oaZcVR\n4dmsNEKrvD+S7lezlf1xqYBsFhIL0XyLe4s/9ayQP4ipX04xnftzXEPx/L/CwZF1\nCGeTJT6AkJHvUos1GSMnfYViqWsWdhFQmfqxmGfdK6979Lwhhi0k4QKUTcnf7XFi\n7ZTRrCJ1Lv7InGOshWobFjCYaYcLmeh3zk/JjOXFYfNSiJavFiU2yjkEXC26PnbX\ne7LmLzMBZRTNskK0PkmW5o5Gu3POHmyPE3AYms6Uhw2zD/bLDy+azXbHoKnn6CD7\nRkoYuluuas6iXLF4ppdb2sZoNnTmDWFBZa71J33HwIYDvzIhYyhWCsGE1FHb3O7Y\noHtlV17LvwrWUEZHRlbJcW6H/1fIn7KllBtrsED0SraX2EfOY0e6O19iFta4/kX7\nhFFr/5tFJyuouARzp4BPCGYOeQOIyA6h0nVthwpc8cEmFUTTaME=\n=Gjqx\n-----END PGP SIGNATURE-----", "payload": "tree 7b6c4584b12203c6943d213abcbe31bf5f18d885\nparent dc1eee2f256efbd1d3b50b6b090232f81cac6d72\nauthor Aaron Hill <aa1ronham@gmail.com> 1610638921 -0500\ncommitter Aaron Hill <aa1ronham@gmail.com> 1611293583 -0500\n\nRefactor token collection to capture trailing token immediately\n"}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/ccfc292999e133193f61feacd70031c90884636b", "html_url": "https://github.com/rust-lang/rust/commit/ccfc292999e133193f61feacd70031c90884636b", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/ccfc292999e133193f61feacd70031c90884636b/comments", "author": {"login": "Aaron1011", "id": 1408859, "node_id": "MDQ6VXNlcjE0MDg4NTk=", "avatar_url": "https://avatars.githubusercontent.com/u/1408859?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Aaron1011", "html_url": "https://github.com/Aaron1011", "followers_url": "https://api.github.com/users/Aaron1011/followers", "following_url": "https://api.github.com/users/Aaron1011/following{/other_user}", "gists_url": "https://api.github.com/users/Aaron1011/gists{/gist_id}", "starred_url": "https://api.github.com/users/Aaron1011/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Aaron1011/subscriptions", "organizations_url": "https://api.github.com/users/Aaron1011/orgs", "repos_url": "https://api.github.com/users/Aaron1011/repos", "events_url": "https://api.github.com/users/Aaron1011/events{/privacy}", "received_events_url": "https://api.github.com/users/Aaron1011/received_events", "type": "User", "site_admin": false}, "committer": {"login": "Aaron1011", "id": 1408859, "node_id": "MDQ6VXNlcjE0MDg4NTk=", "avatar_url": "https://avatars.githubusercontent.com/u/1408859?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Aaron1011", "html_url": "https://github.com/Aaron1011", "followers_url": "https://api.github.com/users/Aaron1011/followers", "following_url": "https://api.github.com/users/Aaron1011/following{/other_user}", "gists_url": "https://api.github.com/users/Aaron1011/gists{/gist_id}", "starred_url": "https://api.github.com/users/Aaron1011/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Aaron1011/subscriptions", "organizations_url": "https://api.github.com/users/Aaron1011/orgs", "repos_url": "https://api.github.com/users/Aaron1011/repos", "events_url": "https://api.github.com/users/Aaron1011/events{/privacy}", "received_events_url": "https://api.github.com/users/Aaron1011/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "dc1eee2f256efbd1d3b50b6b090232f81cac6d72", "url": "https://api.github.com/repos/rust-lang/rust/commits/dc1eee2f256efbd1d3b50b6b090232f81cac6d72", "html_url": "https://github.com/rust-lang/rust/commit/dc1eee2f256efbd1d3b50b6b090232f81cac6d72"}], "stats": {"total": 242, "additions": 124, "deletions": 118}, "files": [{"sha": "9ac05f316f0346e84a12e0882dc761c5d469441d", "filename": "compiler/rustc_ast/src/tokenstream.rs", "status": "modified", "additions": 0, "deletions": 11, "changes": 11, "blob_url": "https://github.com/rust-lang/rust/blob/ccfc292999e133193f61feacd70031c90884636b/compiler%2Frustc_ast%2Fsrc%2Ftokenstream.rs", "raw_url": "https://github.com/rust-lang/rust/raw/ccfc292999e133193f61feacd70031c90884636b/compiler%2Frustc_ast%2Fsrc%2Ftokenstream.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_ast%2Fsrc%2Ftokenstream.rs?ref=ccfc292999e133193f61feacd70031c90884636b", "patch": "@@ -127,14 +127,10 @@ where\n }\n \n pub trait CreateTokenStream: sync::Send + sync::Sync {\n-    fn add_trailing_semi(&self) -> Box<dyn CreateTokenStream>;\n     fn create_token_stream(&self) -> TokenStream;\n }\n \n impl CreateTokenStream for TokenStream {\n-    fn add_trailing_semi(&self) -> Box<dyn CreateTokenStream> {\n-        panic!(\"Cannot call `add_trailing_semi` on a `TokenStream`!\");\n-    }\n     fn create_token_stream(&self) -> TokenStream {\n         self.clone()\n     }\n@@ -151,13 +147,6 @@ impl LazyTokenStream {\n         LazyTokenStream(Lrc::new(Box::new(inner)))\n     }\n \n-    /// Extends the captured stream by one token,\n-    /// which must be a trailing semicolon. This\n-    /// affects the `TokenStream` created by `make_tokenstream`.\n-    pub fn add_trailing_semi(&self) -> LazyTokenStream {\n-        LazyTokenStream(Lrc::new(self.0.add_trailing_semi()))\n-    }\n-\n     pub fn create_token_stream(&self) -> TokenStream {\n         self.0.create_token_stream()\n     }"}, {"sha": "c365acac625a4a201550c1446ec60329b45d5f63", "filename": "compiler/rustc_parse/src/lib.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/ccfc292999e133193f61feacd70031c90884636b/compiler%2Frustc_parse%2Fsrc%2Flib.rs", "raw_url": "https://github.com/rust-lang/rust/raw/ccfc292999e133193f61feacd70031c90884636b/compiler%2Frustc_parse%2Fsrc%2Flib.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_parse%2Fsrc%2Flib.rs?ref=ccfc292999e133193f61feacd70031c90884636b", "patch": "@@ -292,7 +292,7 @@ pub fn nt_to_tokenstream(\n     } else if matches!(synthesize_tokens, CanSynthesizeMissingTokens::Yes) {\n         return fake_token_stream(sess, nt);\n     } else {\n-        panic!(\"Missing tokens for nt {:?}\", pprust::nonterminal_to_string(nt));\n+        panic!(\"Missing tokens for nt at {:?}: {:?}\", nt.span(), pprust::nonterminal_to_string(nt));\n     }\n }\n "}, {"sha": "1ed4d39cd0539a89cfa7767de6d208ecf5acf720", "filename": "compiler/rustc_parse/src/parser/item.rs", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/rust-lang/rust/blob/ccfc292999e133193f61feacd70031c90884636b/compiler%2Frustc_parse%2Fsrc%2Fparser%2Fitem.rs", "raw_url": "https://github.com/rust-lang/rust/raw/ccfc292999e133193f61feacd70031c90884636b/compiler%2Frustc_parse%2Fsrc%2Fparser%2Fitem.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_parse%2Fsrc%2Fparser%2Fitem.rs?ref=ccfc292999e133193f61feacd70031c90884636b", "patch": "@@ -1,6 +1,6 @@\n use super::diagnostics::{dummy_arg, ConsumeClosingDelim, Error};\n use super::ty::{AllowPlus, RecoverQPath, RecoverReturnSign};\n-use super::{FollowedByType, ForceCollect, Parser, PathStyle};\n+use super::{FollowedByType, ForceCollect, Parser, PathStyle, TrailingToken};\n \n use crate::{maybe_collect_tokens, maybe_whole};\n \n@@ -125,7 +125,7 @@ impl<'a> Parser<'a> {\n         let item = maybe_collect_tokens!(self, force_collect, &attrs, |this: &mut Self| {\n             let item = this.parse_item_common_(attrs, mac_allowed, attrs_allowed, req_name);\n             unclosed_delims.append(&mut this.unclosed_delims);\n-            item\n+            Ok((item?, TrailingToken::None))\n         })?;\n \n         self.unclosed_delims.append(&mut unclosed_delims);"}, {"sha": "b332005ca7424039e5286a3bcd29d9e477f4c696", "filename": "compiler/rustc_parse/src/parser/mod.rs", "status": "modified", "additions": 28, "deletions": 27, "changes": 55, "blob_url": "https://github.com/rust-lang/rust/blob/ccfc292999e133193f61feacd70031c90884636b/compiler%2Frustc_parse%2Fsrc%2Fparser%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/ccfc292999e133193f61feacd70031c90884636b/compiler%2Frustc_parse%2Fsrc%2Fparser%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_parse%2Fsrc%2Fparser%2Fmod.rs?ref=ccfc292999e133193f61feacd70031c90884636b", "patch": "@@ -61,6 +61,11 @@ pub enum ForceCollect {\n     No,\n }\n \n+pub enum TrailingToken {\n+    None,\n+    Semi,\n+}\n+\n /// Like `maybe_whole_expr`, but for things other than expressions.\n #[macro_export]\n macro_rules! maybe_whole {\n@@ -1225,6 +1230,13 @@ impl<'a> Parser<'a> {\n         }\n     }\n \n+    pub fn collect_tokens<R: HasTokens>(\n+        &mut self,\n+        f: impl FnOnce(&mut Self) -> PResult<'a, R>,\n+    ) -> PResult<'a, R> {\n+        self.collect_tokens_trailing_token(|this| Ok((f(this)?, TrailingToken::None)))\n+    }\n+\n     /// Records all tokens consumed by the provided callback,\n     /// including the current token. These tokens are collected\n     /// into a `LazyTokenStream`, and returned along with the result\n@@ -1241,9 +1253,9 @@ impl<'a> Parser<'a> {\n     /// This restriction shouldn't be an issue in practice,\n     /// since this function is used to record the tokens for\n     /// a parsed AST item, which always has matching delimiters.\n-    pub fn collect_tokens<R: HasTokens>(\n+    pub fn collect_tokens_trailing_token<R: HasTokens>(\n         &mut self,\n-        f: impl FnOnce(&mut Self) -> PResult<'a, R>,\n+        f: impl FnOnce(&mut Self) -> PResult<'a, (R, TrailingToken)>,\n     ) -> PResult<'a, R> {\n         let start_token = (self.token.clone(), self.token_spacing);\n         let cursor_snapshot = TokenCursor {\n@@ -1256,7 +1268,7 @@ impl<'a> Parser<'a> {\n             append_unglued_token: self.token_cursor.append_unglued_token.clone(),\n         };\n \n-        let mut ret = f(self)?;\n+        let (mut ret, trailing_token) = f(self)?;\n \n         // Produces a `TokenStream` on-demand. Using `cursor_snapshot`\n         // and `num_calls`, we can reconstruct the `TokenStream` seen\n@@ -1275,55 +1287,44 @@ impl<'a> Parser<'a> {\n             cursor_snapshot: TokenCursor,\n             num_calls: usize,\n             desugar_doc_comments: bool,\n-            trailing_semi: bool,\n             append_unglued_token: Option<TreeAndSpacing>,\n         }\n         impl CreateTokenStream for LazyTokenStreamImpl {\n             fn create_token_stream(&self) -> TokenStream {\n-                let mut num_calls = self.num_calls;\n-                if self.trailing_semi {\n-                    num_calls += 1;\n-                }\n                 // The token produced by the final call to `next` or `next_desugared`\n                 // was not actually consumed by the callback. The combination\n                 // of chaining the initial token and using `take` produces the desired\n                 // result - we produce an empty `TokenStream` if no calls were made,\n                 // and omit the final token otherwise.\n                 let mut cursor_snapshot = self.cursor_snapshot.clone();\n                 let tokens = std::iter::once(self.start_token.clone())\n-                    .chain((0..num_calls).map(|_| {\n+                    .chain((0..self.num_calls).map(|_| {\n                         if self.desugar_doc_comments {\n                             cursor_snapshot.next_desugared()\n                         } else {\n                             cursor_snapshot.next()\n                         }\n                     }))\n-                    .take(num_calls);\n+                    .take(self.num_calls);\n \n                 make_token_stream(tokens, self.append_unglued_token.clone())\n             }\n-            fn add_trailing_semi(&self) -> Box<dyn CreateTokenStream> {\n-                if self.trailing_semi {\n-                    panic!(\"Called `add_trailing_semi` twice!\");\n-                }\n-                if self.append_unglued_token.is_some() {\n-                    panic!(\n-                        \"Cannot call `add_trailing_semi` when we have an unglued token {:?}\",\n-                        self.append_unglued_token\n-                    );\n-                }\n-                let mut new = self.clone();\n-                new.trailing_semi = true;\n-                Box::new(new)\n+        }\n+\n+        let mut num_calls = self.token_cursor.num_next_calls - cursor_snapshot.num_next_calls;\n+        match trailing_token {\n+            TrailingToken::None => {}\n+            TrailingToken::Semi => {\n+                assert_eq!(self.token.kind, token::Semi);\n+                num_calls += 1;\n             }\n         }\n \n         let lazy_impl = LazyTokenStreamImpl {\n             start_token,\n-            num_calls: self.token_cursor.num_next_calls - cursor_snapshot.num_next_calls,\n+            num_calls,\n             cursor_snapshot,\n             desugar_doc_comments: self.desugar_doc_comments,\n-            trailing_semi: false,\n             append_unglued_token: self.token_cursor.append_unglued_token.clone(),\n         };\n         ret.finalize_tokens(LazyTokenStream::new(lazy_impl));\n@@ -1427,9 +1428,9 @@ macro_rules! maybe_collect_tokens {\n         if matches!($force_collect, ForceCollect::Yes)\n             || $crate::parser::attr::maybe_needs_tokens($attrs)\n         {\n-            $self.collect_tokens($f)\n+            $self.collect_tokens_trailing_token($f)\n         } else {\n-            $f($self)\n+            Ok($f($self)?.0)\n         }\n     };\n }"}, {"sha": "8373f6acd7e015f960bfe32d15bbc11796997b62", "filename": "compiler/rustc_parse/src/parser/stmt.rs", "status": "modified", "additions": 93, "deletions": 77, "changes": 170, "blob_url": "https://github.com/rust-lang/rust/blob/ccfc292999e133193f61feacd70031c90884636b/compiler%2Frustc_parse%2Fsrc%2Fparser%2Fstmt.rs", "raw_url": "https://github.com/rust-lang/rust/raw/ccfc292999e133193f61feacd70031c90884636b/compiler%2Frustc_parse%2Fsrc%2Fparser%2Fstmt.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_parse%2Fsrc%2Fparser%2Fstmt.rs?ref=ccfc292999e133193f61feacd70031c90884636b", "patch": "@@ -3,14 +3,13 @@ use super::diagnostics::{AttemptLocalParseRecovery, Error};\n use super::expr::LhsExpr;\n use super::pat::{GateOr, RecoverComma};\n use super::path::PathStyle;\n-use super::{BlockMode, ForceCollect, Parser, Restrictions, SemiColonMode};\n+use super::{BlockMode, ForceCollect, Parser, Restrictions, SemiColonMode, TrailingToken};\n use crate::{maybe_collect_tokens, maybe_whole};\n \n use rustc_ast as ast;\n use rustc_ast::attr::HasAttrs;\n use rustc_ast::ptr::P;\n use rustc_ast::token::{self, TokenKind};\n-use rustc_ast::tokenstream::LazyTokenStream;\n use rustc_ast::util::classify;\n use rustc_ast::{AttrStyle, AttrVec, Attribute, MacCall, MacCallStmt, MacStmtStyle};\n use rustc_ast::{Block, BlockCheckMode, Expr, ExprKind, Local, Stmt, StmtKind, DUMMY_NODE_ID};\n@@ -25,7 +24,7 @@ impl<'a> Parser<'a> {\n     /// e.g., a `StmtKind::Semi` parses to a `StmtKind::Expr`, leaving the trailing `;` unconsumed.\n     // Public for rustfmt usage.\n     pub fn parse_stmt(&mut self, force_collect: ForceCollect) -> PResult<'a, Option<Stmt>> {\n-        Ok(self.parse_stmt_without_recovery(force_collect).unwrap_or_else(|mut e| {\n+        Ok(self.parse_stmt_without_recovery(false, force_collect).unwrap_or_else(|mut e| {\n             e.emit();\n             self.recover_stmt_(SemiColonMode::Break, BlockMode::Ignore);\n             None\n@@ -36,6 +35,7 @@ impl<'a> Parser<'a> {\n     /// or not we have attributes\n     fn parse_stmt_without_recovery(\n         &mut self,\n+        capture_semi: bool,\n         force_collect: ForceCollect,\n     ) -> PResult<'a, Option<Stmt>> {\n         let mut attrs = self.parse_outer_attributes()?;\n@@ -50,68 +50,77 @@ impl<'a> Parser<'a> {\n             Some(stmt)\n         });\n \n-        maybe_collect_tokens!(self, force_collect, &attrs, |this: &mut Self| {\n-            let stmt = if this.eat_keyword(kw::Let) {\n-                this.parse_local_mk(lo, attrs.into())?\n-            } else if this.is_kw_followed_by_ident(kw::Mut) {\n-                this.recover_stmt_local(lo, attrs.into(), \"missing keyword\", \"let mut\")?\n-            } else if this.is_kw_followed_by_ident(kw::Auto) {\n-                this.bump(); // `auto`\n-                let msg = \"write `let` instead of `auto` to introduce a new variable\";\n-                this.recover_stmt_local(lo, attrs.into(), msg, \"let\")?\n-            } else if this.is_kw_followed_by_ident(sym::var) {\n-                this.bump(); // `var`\n-                let msg = \"write `let` instead of `var` to introduce a new variable\";\n-                this.recover_stmt_local(lo, attrs.into(), msg, \"let\")?\n-            } else if this.check_path()\n-                && !this.token.is_qpath_start()\n-                && !this.is_path_start_item()\n-            {\n-                // We have avoided contextual keywords like `union`, items with `crate` visibility,\n-                // or `auto trait` items. We aim to parse an arbitrary path `a::b` but not something\n-                // that starts like a path (1 token), but it fact not a path.\n-                // Also, we avoid stealing syntax from `parse_item_`.\n-                this.parse_stmt_path_start(lo, attrs)?\n-            } else if let Some(item) =\n-                this.parse_item_common(attrs.clone(), false, true, |_| true, force_collect)?\n-            {\n-                // FIXME: Bad copy of attrs\n-                this.mk_stmt(lo.to(item.span), StmtKind::Item(P(item)))\n-            } else if this.eat(&token::Semi) {\n-                // Do not attempt to parse an expression if we're done here.\n-                this.error_outer_attrs(&attrs);\n-                this.mk_stmt(lo, StmtKind::Empty)\n-            } else if this.token != token::CloseDelim(token::Brace) {\n-                // Remainder are line-expr stmts.\n-                let e = this.parse_expr_res(Restrictions::STMT_EXPR, Some(attrs.into()))?;\n-                this.mk_stmt(lo.to(e.span), StmtKind::Expr(e))\n-            } else {\n-                this.error_outer_attrs(&attrs);\n-                return Ok(None);\n-            };\n-            Ok(Some(stmt))\n-        })\n+        Ok(Some(if self.token.is_keyword(kw::Let) {\n+            self.parse_local_mk(lo, attrs.into(), capture_semi, force_collect)?\n+        } else if self.is_kw_followed_by_ident(kw::Mut) {\n+            self.recover_stmt_local(lo, attrs.into(), \"missing keyword\", \"let mut\")?\n+        } else if self.is_kw_followed_by_ident(kw::Auto) {\n+            self.bump(); // `auto`\n+            let msg = \"write `let` instead of `auto` to introduce a new variable\";\n+            self.recover_stmt_local(lo, attrs.into(), msg, \"let\")?\n+        } else if self.is_kw_followed_by_ident(sym::var) {\n+            self.bump(); // `var`\n+            let msg = \"write `let` instead of `var` to introduce a new variable\";\n+            self.recover_stmt_local(lo, attrs.into(), msg, \"let\")?\n+        } else if self.check_path() && !self.token.is_qpath_start() && !self.is_path_start_item() {\n+            // We have avoided contextual keywords like `union`, items with `crate` visibility,\n+            // or `auto trait` items. We aim to parse an arbitrary path `a::b` but not something\n+            // that starts like a path (1 token), but it fact not a path.\n+            // Also, we avoid stealing syntax from `parse_item_`.\n+            self.parse_stmt_path_start(lo, attrs, force_collect)?\n+        } else if let Some(item) =\n+            self.parse_item_common(attrs.clone(), false, true, |_| true, force_collect)?\n+        {\n+            // FIXME: Bad copy of attrs\n+            self.mk_stmt(lo.to(item.span), StmtKind::Item(P(item)))\n+        } else if self.eat(&token::Semi) {\n+            // Do not attempt to parse an expression if we're done here.\n+            self.error_outer_attrs(&attrs);\n+            self.mk_stmt(lo, StmtKind::Empty)\n+        } else if self.token != token::CloseDelim(token::Brace) {\n+            // Remainder are line-expr stmts.\n+            let e = self.parse_expr_res(Restrictions::STMT_EXPR, Some(attrs.into()))?;\n+            self.mk_stmt(lo.to(e.span), StmtKind::Expr(e))\n+        } else {\n+            self.error_outer_attrs(&attrs);\n+            return Ok(None);\n+        }))\n     }\n \n-    fn parse_stmt_path_start(&mut self, lo: Span, attrs: Vec<Attribute>) -> PResult<'a, Stmt> {\n-        let path = self.parse_path(PathStyle::Expr)?;\n+    fn parse_stmt_path_start(\n+        &mut self,\n+        lo: Span,\n+        attrs: Vec<Attribute>,\n+        force_collect: ForceCollect,\n+    ) -> PResult<'a, Stmt> {\n+        maybe_collect_tokens!(self, force_collect, &attrs, |this: &mut Parser<'a>| {\n+            let path = this.parse_path(PathStyle::Expr)?;\n \n-        if self.eat(&token::Not) {\n-            return self.parse_stmt_mac(lo, attrs.into(), path);\n-        }\n+            if this.eat(&token::Not) {\n+                let stmt_mac = this.parse_stmt_mac(lo, attrs.into(), path)?;\n+                if this.token == token::Semi {\n+                    return Ok((stmt_mac, TrailingToken::Semi));\n+                } else {\n+                    return Ok((stmt_mac, TrailingToken::None));\n+                }\n+            }\n \n-        let expr = if self.eat(&token::OpenDelim(token::Brace)) {\n-            self.parse_struct_expr(path, AttrVec::new(), true)?\n-        } else {\n-            let hi = self.prev_token.span;\n-            self.mk_expr(lo.to(hi), ExprKind::Path(None, path), AttrVec::new())\n-        };\n+            let expr = if this.eat(&token::OpenDelim(token::Brace)) {\n+                this.parse_struct_expr(path, AttrVec::new(), true)?\n+            } else {\n+                let hi = this.prev_token.span;\n+                this.mk_expr(lo.to(hi), ExprKind::Path(None, path), AttrVec::new())\n+            };\n \n-        let expr = self.with_res(Restrictions::STMT_EXPR, |this| {\n-            let expr = this.parse_dot_or_call_expr_with(expr, lo, attrs.into())?;\n-            this.parse_assoc_expr_with(0, LhsExpr::AlreadyParsed(expr))\n-        })?;\n-        Ok(self.mk_stmt(lo.to(self.prev_token.span), StmtKind::Expr(expr)))\n+            let expr = this.with_res(Restrictions::STMT_EXPR, |this| {\n+                let expr = this.parse_dot_or_call_expr_with(expr, lo, attrs.into())?;\n+                this.parse_assoc_expr_with(0, LhsExpr::AlreadyParsed(expr))\n+            })?;\n+            Ok((\n+                this.mk_stmt(lo.to(this.prev_token.span), StmtKind::Expr(expr)),\n+                TrailingToken::None,\n+            ))\n+        })\n     }\n \n     /// Parses a statement macro `mac!(args)` provided a `path` representing `mac`.\n@@ -159,15 +168,34 @@ impl<'a> Parser<'a> {\n         msg: &str,\n         sugg: &str,\n     ) -> PResult<'a, Stmt> {\n-        let stmt = self.parse_local_mk(lo, attrs)?;\n+        let stmt = self.recover_local_after_let(lo, attrs)?;\n         self.struct_span_err(lo, \"invalid variable declaration\")\n             .span_suggestion(lo, msg, sugg.to_string(), Applicability::MachineApplicable)\n             .emit();\n         Ok(stmt)\n     }\n \n-    fn parse_local_mk(&mut self, lo: Span, attrs: AttrVec) -> PResult<'a, Stmt> {\n-        let local = self.parse_local(attrs)?;\n+    fn parse_local_mk(\n+        &mut self,\n+        lo: Span,\n+        attrs: AttrVec,\n+        capture_semi: bool,\n+        force_collect: ForceCollect,\n+    ) -> PResult<'a, Stmt> {\n+        maybe_collect_tokens!(self, force_collect, &attrs, |this: &mut Parser<'a>| {\n+            this.expect_keyword(kw::Let)?;\n+            let local = this.parse_local(attrs.into())?;\n+            let trailing = if capture_semi && this.token.kind == token::Semi {\n+                TrailingToken::Semi\n+            } else {\n+                TrailingToken::None\n+            };\n+            Ok((this.mk_stmt(lo.to(this.prev_token.span), StmtKind::Local(local)), trailing))\n+        })\n+    }\n+\n+    fn recover_local_after_let(&mut self, lo: Span, attrs: AttrVec) -> PResult<'a, Stmt> {\n+        let local = self.parse_local(attrs.into())?;\n         Ok(self.mk_stmt(lo.to(self.prev_token.span), StmtKind::Local(local)))\n     }\n \n@@ -289,7 +317,7 @@ impl<'a> Parser<'a> {\n         //      bar;\n         //\n         // which is valid in other languages, but not Rust.\n-        match self.parse_stmt_without_recovery(ForceCollect::No) {\n+        match self.parse_stmt_without_recovery(false, ForceCollect::No) {\n             // If the next token is an open brace (e.g., `if a b {`), the place-\n             // inside-a-block suggestion would be more likely wrong than right.\n             Ok(Some(_))\n@@ -392,17 +420,11 @@ impl<'a> Parser<'a> {\n         // Skip looking for a trailing semicolon when we have an interpolated statement.\n         maybe_whole!(self, NtStmt, |x| Some(x));\n \n-        let mut stmt = match self.parse_stmt_without_recovery(ForceCollect::No)? {\n+        let mut stmt = match self.parse_stmt_without_recovery(true, ForceCollect::No)? {\n             Some(stmt) => stmt,\n             None => return Ok(None),\n         };\n \n-        let add_semi_token = |tokens: Option<&mut LazyTokenStream>| {\n-            if let Some(tokens) = tokens {\n-                *tokens = tokens.add_trailing_semi();\n-            }\n-        };\n-\n         let mut eat_semi = true;\n         match stmt.kind {\n             // Expression without semicolon.\n@@ -458,18 +480,12 @@ impl<'a> Parser<'a> {\n                     }\n                 }\n                 eat_semi = false;\n-                // We just checked that there's a semicolon in the tokenstream,\n-                // so capture it\n-                add_semi_token(local.tokens.as_mut());\n             }\n             StmtKind::Empty | StmtKind::Item(_) | StmtKind::Semi(_) => eat_semi = false,\n         }\n \n         if eat_semi && self.eat(&token::Semi) {\n             stmt = stmt.add_trailing_semicolon();\n-            // We just checked that we have a semicolon in the tokenstream,\n-            // so capture it\n-            add_semi_token(stmt.tokens_mut());\n         }\n         stmt.span = stmt.span.to(self.prev_token.span);\n         Ok(Some(stmt))"}]}