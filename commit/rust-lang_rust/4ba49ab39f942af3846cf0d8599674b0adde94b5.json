{"sha": "4ba49ab39f942af3846cf0d8599674b0adde94b5", "node_id": "MDY6Q29tbWl0NzI0NzEyOjRiYTQ5YWIzOWY5NDJhZjM4NDZjZjBkODU5OTY3NGIwYWRkZTk0YjU=", "commit": {"author": {"name": "Corey Farwell", "email": "coreyf@rwell.org", "date": "2017-03-01T03:55:26Z"}, "committer": {"name": "GitHub", "email": "noreply@github.com", "date": "2017-03-01T03:55:26Z"}, "message": "Rollup merge of #39419 - jseyfried:simplify_tokentree, r=nrc\n\nSimplify `TokenTree` and fix `macro_rules!` bugs\n\nThis PR\n - fixes #39390, fixes #39403, and fixes #39404 (each is a [breaking-change], see issues for examples),\n - fixes #39889,\n - simplifies and optimizes macro invocation parsing,\n - cleans up `ext::tt::transcribe`,\n - removes `tokenstream::TokenTree::Sequence` and `Token::MatchNt`,\n   - instead, adds a new type `ext::tt::quoted::TokenTree` for use by `macro_rules!` (`ext::tt`)\n - removes `parser.quote_depth` and `parser.parsing_token_tree`, and\n - removes `quote_matcher!`.\n   - Instead, use `quote_tokens!` and `ext::tt::quoted::parse` the result with `expect_matchers=true`.\n   - I found no outside uses of `quote_matcher!` when searching Rust code on Github.\n\nr? @nrc", "tree": {"sha": "0fb917efd8b1fe14e7fbf49c4c01c8f0f7504af4", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/0fb917efd8b1fe14e7fbf49c4c01c8f0f7504af4"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/4ba49ab39f942af3846cf0d8599674b0adde94b5", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/4ba49ab39f942af3846cf0d8599674b0adde94b5", "html_url": "https://github.com/rust-lang/rust/commit/4ba49ab39f942af3846cf0d8599674b0adde94b5", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/4ba49ab39f942af3846cf0d8599674b0adde94b5/comments", "author": {"login": "frewsxcv", "id": 416575, "node_id": "MDQ6VXNlcjQxNjU3NQ==", "avatar_url": "https://avatars.githubusercontent.com/u/416575?v=4", "gravatar_id": "", "url": "https://api.github.com/users/frewsxcv", "html_url": "https://github.com/frewsxcv", "followers_url": "https://api.github.com/users/frewsxcv/followers", "following_url": "https://api.github.com/users/frewsxcv/following{/other_user}", "gists_url": "https://api.github.com/users/frewsxcv/gists{/gist_id}", "starred_url": "https://api.github.com/users/frewsxcv/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/frewsxcv/subscriptions", "organizations_url": "https://api.github.com/users/frewsxcv/orgs", "repos_url": "https://api.github.com/users/frewsxcv/repos", "events_url": "https://api.github.com/users/frewsxcv/events{/privacy}", "received_events_url": "https://api.github.com/users/frewsxcv/received_events", "type": "User", "site_admin": false}, "committer": {"login": "web-flow", "id": 19864447, "node_id": "MDQ6VXNlcjE5ODY0NDQ3", "avatar_url": "https://avatars.githubusercontent.com/u/19864447?v=4", "gravatar_id": "", "url": "https://api.github.com/users/web-flow", "html_url": "https://github.com/web-flow", "followers_url": "https://api.github.com/users/web-flow/followers", "following_url": "https://api.github.com/users/web-flow/following{/other_user}", "gists_url": "https://api.github.com/users/web-flow/gists{/gist_id}", "starred_url": "https://api.github.com/users/web-flow/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/web-flow/subscriptions", "organizations_url": "https://api.github.com/users/web-flow/orgs", "repos_url": "https://api.github.com/users/web-flow/repos", "events_url": "https://api.github.com/users/web-flow/events{/privacy}", "received_events_url": "https://api.github.com/users/web-flow/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "2f52386f1072755d1b9973014e8e1d4b383e8eef", "url": "https://api.github.com/repos/rust-lang/rust/commits/2f52386f1072755d1b9973014e8e1d4b383e8eef", "html_url": "https://github.com/rust-lang/rust/commit/2f52386f1072755d1b9973014e8e1d4b383e8eef"}, {"sha": "839398a0b4a5b77fe3dd351107b1cbe45e1004de", "url": "https://api.github.com/repos/rust-lang/rust/commits/839398a0b4a5b77fe3dd351107b1cbe45e1004de", "html_url": "https://github.com/rust-lang/rust/commit/839398a0b4a5b77fe3dd351107b1cbe45e1004de"}], "stats": {"total": 1528, "additions": 762, "deletions": 766}, "files": [{"sha": "b33caefbcd2ecbac73e3b73a7b1d88ca4af3f2a5", "filename": "src/libproc_macro/lib.rs", "status": "modified", "additions": 1, "deletions": 2, "changes": 3, "blob_url": "https://github.com/rust-lang/rust/blob/4ba49ab39f942af3846cf0d8599674b0adde94b5/src%2Flibproc_macro%2Flib.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4ba49ab39f942af3846cf0d8599674b0adde94b5/src%2Flibproc_macro%2Flib.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibproc_macro%2Flib.rs?ref=4ba49ab39f942af3846cf0d8599674b0adde94b5", "patch": "@@ -173,8 +173,7 @@ impl FromStr for TokenStream {\n         __internal::with_parse_sess(|sess| {\n             let src = src.to_string();\n             let name = \"<proc-macro source code>\".to_string();\n-            let tts = try!(parse::parse_tts_from_source_str(name, src, sess)\n-                .map_err(parse_to_lex_err));\n+            let tts = parse::parse_tts_from_source_str(name, src, sess);\n \n             Ok(__internal::token_stream_wrap(tts.into_iter().collect()))\n         })"}, {"sha": "dc7c96a4e27672d88fb2f501d5d2b7b0e69b322d", "filename": "src/libproc_macro_plugin/qquote.rs", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "blob_url": "https://github.com/rust-lang/rust/blob/4ba49ab39f942af3846cf0d8599674b0adde94b5/src%2Flibproc_macro_plugin%2Fqquote.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4ba49ab39f942af3846cf0d8599674b0adde94b5/src%2Flibproc_macro_plugin%2Fqquote.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibproc_macro_plugin%2Fqquote.rs?ref=4ba49ab39f942af3846cf0d8599674b0adde94b5", "patch": "@@ -119,7 +119,6 @@ impl Quote for TokenTree {\n                 ::syntax::tokenstream::TokenTree::Delimited(::syntax::ext::quote::rt::DUMMY_SP,\n                                                             (quote delimited))\n             },\n-            _ => panic!(\"unexpected `TokenTree::Sequence` in `qquote`\"),\n         }\n     }\n }"}, {"sha": "b2f508ff26d6f46b888d183b32678eeefc3fdd95", "filename": "src/librustc/lint/builtin.rs", "status": "modified", "additions": 7, "deletions": 0, "changes": 7, "blob_url": "https://github.com/rust-lang/rust/blob/4ba49ab39f942af3846cf0d8599674b0adde94b5/src%2Flibrustc%2Flint%2Fbuiltin.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4ba49ab39f942af3846cf0d8599674b0adde94b5/src%2Flibrustc%2Flint%2Fbuiltin.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Flint%2Fbuiltin.rs?ref=4ba49ab39f942af3846cf0d8599674b0adde94b5", "patch": "@@ -236,6 +236,12 @@ declare_lint! {\n     \"detects use of struct constructors that would be invisible with new visibility rules\"\n }\n \n+declare_lint! {\n+    pub MISSING_FRAGMENT_SPECIFIER,\n+    Warn,\n+    \"detects missing fragment specifiers in unused `macro_rules!` patterns\"\n+}\n+\n declare_lint! {\n     pub DEPRECATED,\n     Warn,\n@@ -286,6 +292,7 @@ impl LintPass for HardwiredLints {\n             LEGACY_DIRECTORY_OWNERSHIP,\n             LEGACY_IMPORTS,\n             LEGACY_CONSTRUCTOR_VISIBILITY,\n+            MISSING_FRAGMENT_SPECIFIER,\n             DEPRECATED\n         )\n     }"}, {"sha": "9619ba8472404bb70e0a1fffca1e620841dbe528", "filename": "src/librustc_driver/driver.rs", "status": "modified", "additions": 8, "deletions": 0, "changes": 8, "blob_url": "https://github.com/rust-lang/rust/blob/4ba49ab39f942af3846cf0d8599674b0adde94b5/src%2Flibrustc_driver%2Fdriver.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4ba49ab39f942af3846cf0d8599674b0adde94b5/src%2Flibrustc_driver%2Fdriver.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_driver%2Fdriver.rs?ref=4ba49ab39f942af3846cf0d8599674b0adde94b5", "patch": "@@ -688,6 +688,14 @@ pub fn phase_2_configure_and_expand<F>(sess: &Session,\n \n         let krate = ecx.monotonic_expander().expand_crate(krate);\n \n+        let mut missing_fragment_specifiers: Vec<_> =\n+            ecx.parse_sess.missing_fragment_specifiers.borrow().iter().cloned().collect();\n+        missing_fragment_specifiers.sort();\n+        for span in missing_fragment_specifiers {\n+            let lint = lint::builtin::MISSING_FRAGMENT_SPECIFIER;\n+            let msg = \"missing fragment specifier\".to_string();\n+            sess.add_lint(lint, ast::CRATE_NODE_ID, span, msg);\n+        }\n         if ecx.parse_sess.span_diagnostic.err_count() - ecx.resolve_err_count > err_count {\n             ecx.parse_sess.span_diagnostic.abort_if_errors();\n         }"}, {"sha": "b075fa599924996650b2571afa356f312f2484c7", "filename": "src/librustc_incremental/calculate_svh/svh_visitor.rs", "status": "modified", "additions": 0, "deletions": 24, "changes": 24, "blob_url": "https://github.com/rust-lang/rust/blob/4ba49ab39f942af3846cf0d8599674b0adde94b5/src%2Flibrustc_incremental%2Fcalculate_svh%2Fsvh_visitor.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4ba49ab39f942af3846cf0d8599674b0adde94b5/src%2Flibrustc_incremental%2Fcalculate_svh%2Fsvh_visitor.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_incremental%2Fcalculate_svh%2Fsvh_visitor.rs?ref=4ba49ab39f942af3846cf0d8599674b0adde94b5", "patch": "@@ -1044,26 +1044,6 @@ impl<'a, 'hash, 'tcx> StrictVersionHashVisitor<'a, 'hash, 'tcx> {\n                     self.hash_token_tree(sub_tt);\n                 }\n             }\n-            tokenstream::TokenTree::Sequence(span, ref sequence_repetition) => {\n-                hash_span!(self, span);\n-                let tokenstream::SequenceRepetition {\n-                    ref tts,\n-                    ref separator,\n-                    op,\n-                    num_captures,\n-                } = **sequence_repetition;\n-\n-                tts.len().hash(self.st);\n-                for sub_tt in tts {\n-                    self.hash_token_tree(sub_tt);\n-                }\n-                self.hash_discriminant(separator);\n-                if let Some(ref separator) = *separator {\n-                    self.hash_token(separator, span);\n-                }\n-                op.hash(self.st);\n-                num_captures.hash(self.st);\n-            }\n         }\n     }\n \n@@ -1129,10 +1109,6 @@ impl<'a, 'hash, 'tcx> StrictVersionHashVisitor<'a, 'hash, 'tcx> {\n             token::Token::Ident(ident) |\n             token::Token::Lifetime(ident) |\n             token::Token::SubstNt(ident) => ident.name.as_str().hash(self.st),\n-            token::Token::MatchNt(ident1, ident2) => {\n-                ident1.name.as_str().hash(self.st);\n-                ident2.name.as_str().hash(self.st);\n-            }\n \n             token::Token::Interpolated(ref non_terminal) => {\n                 // FIXME(mw): This could be implemented properly. It's just a"}, {"sha": "b87edf548232492007482b4b2df53796e5f39554", "filename": "src/librustc_lint/lib.rs", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "blob_url": "https://github.com/rust-lang/rust/blob/4ba49ab39f942af3846cf0d8599674b0adde94b5/src%2Flibrustc_lint%2Flib.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4ba49ab39f942af3846cf0d8599674b0adde94b5/src%2Flibrustc_lint%2Flib.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_lint%2Flib.rs?ref=4ba49ab39f942af3846cf0d8599674b0adde94b5", "patch": "@@ -247,6 +247,10 @@ pub fn register_builtins(store: &mut lint::LintStore, sess: Option<&Session>) {\n             id: LintId::of(LEGACY_CONSTRUCTOR_VISIBILITY),\n             reference: \"issue #39207 <https://github.com/rust-lang/rust/issues/39207>\",\n         },\n+        FutureIncompatibleInfo {\n+            id: LintId::of(MISSING_FRAGMENT_SPECIFIER),\n+            reference: \"issue #40107 <https://github.com/rust-lang/rust/issues/40107>\",\n+        },\n         ]);\n \n     // Register renamed and removed lints"}, {"sha": "6c93744f014a3c2b678be69ef78b351c56e8bc2f", "filename": "src/librustc_save_analysis/span_utils.rs", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "blob_url": "https://github.com/rust-lang/rust/blob/4ba49ab39f942af3846cf0d8599674b0adde94b5/src%2Flibrustc_save_analysis%2Fspan_utils.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4ba49ab39f942af3846cf0d8599674b0adde94b5/src%2Flibrustc_save_analysis%2Fspan_utils.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_save_analysis%2Fspan_utils.rs?ref=4ba49ab39f942af3846cf0d8599674b0adde94b5", "patch": "@@ -287,7 +287,7 @@ impl<'a> SpanUtils<'a> {\n         let mut toks = toks.parse_all_token_trees().unwrap().into_iter();\n         let mut prev = toks.next().unwrap();\n \n-        let first_span = prev.get_span();\n+        let first_span = prev.span();\n         let mut angle_count = 0;\n         for tok in toks {\n             if let TokenTree::Token(_, ref tok) = prev {\n@@ -305,10 +305,10 @@ impl<'a> SpanUtils<'a> {\n                 continue;\n             }\n             if let TokenTree::Token(_, token::Semi) = tok {\n-                return self.snippet(mk_sp(first_span.lo, prev.get_span().hi));\n+                return self.snippet(mk_sp(first_span.lo, prev.span().hi));\n             } else if let TokenTree::Delimited(_, ref d) = tok {\n                 if d.delim == token::Brace {\n-                    return self.snippet(mk_sp(first_span.lo, prev.get_span().hi));\n+                    return self.snippet(mk_sp(first_span.lo, prev.span().hi));\n                 }\n             }\n             prev = tok;"}, {"sha": "2c15dd923237fe840362c352c72bac70d3756eca", "filename": "src/librustdoc/html/highlight.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/4ba49ab39f942af3846cf0d8599674b0adde94b5/src%2Flibrustdoc%2Fhtml%2Fhighlight.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4ba49ab39f942af3846cf0d8599674b0adde94b5/src%2Flibrustdoc%2Fhtml%2Fhighlight.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustdoc%2Fhtml%2Fhighlight.rs?ref=4ba49ab39f942af3846cf0d8599674b0adde94b5", "patch": "@@ -315,7 +315,7 @@ impl<'a> Classifier<'a> {\n             token::Lifetime(..) => Class::Lifetime,\n \n             token::Underscore | token::Eof | token::Interpolated(..) |\n-            token::MatchNt(..) | token::SubstNt(..) | token::Tilde | token::At => Class::None,\n+            token::SubstNt(..) | token::Tilde | token::At => Class::None,\n         };\n \n         // Anything that didn't return above is the simple case where we the"}, {"sha": "236d9f230b5d470718467b351ed6ce75d16fafa5", "filename": "src/librustdoc/visit_ast.rs", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/rust-lang/rust/blob/4ba49ab39f942af3846cf0d8599674b0adde94b5/src%2Flibrustdoc%2Fvisit_ast.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4ba49ab39f942af3846cf0d8599674b0adde94b5/src%2Flibrustdoc%2Fvisit_ast.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustdoc%2Fvisit_ast.rs?ref=4ba49ab39f942af3846cf0d8599674b0adde94b5", "patch": "@@ -211,7 +211,7 @@ impl<'a, 'tcx> RustdocVisitor<'a, 'tcx> {\n                     };\n \n                     // FIXME(jseyfried) merge with `self.visit_macro()`\n-                    let matchers = def.body.chunks(4).map(|arm| arm[0].get_span()).collect();\n+                    let matchers = def.body.chunks(4).map(|arm| arm[0].span()).collect();\n                     om.macros.push(Macro {\n                         def_id: def_id,\n                         attrs: def.attrs.clone().into(),\n@@ -521,7 +521,7 @@ impl<'a, 'tcx> RustdocVisitor<'a, 'tcx> {\n     // convert each exported_macro into a doc item\n     fn visit_local_macro(&self, def: &hir::MacroDef) -> Macro {\n         // Extract the spans of all matchers. They represent the \"interface\" of the macro.\n-        let matchers = def.body.chunks(4).map(|arm| arm[0].get_span()).collect();\n+        let matchers = def.body.chunks(4).map(|arm| arm[0].span()).collect();\n \n         Macro {\n             def_id: self.cx.tcx.hir.local_def_id(def.id),"}, {"sha": "b1b69c80f4d0016c790c4554d0df6cc73350fd7b", "filename": "src/libsyntax/ext/quote.rs", "status": "modified", "additions": 45, "deletions": 105, "changes": 150, "blob_url": "https://github.com/rust-lang/rust/blob/4ba49ab39f942af3846cf0d8599674b0adde94b5/src%2Flibsyntax%2Fext%2Fquote.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4ba49ab39f942af3846cf0d8599674b0adde94b5/src%2Flibsyntax%2Fext%2Fquote.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fext%2Fquote.rs?ref=4ba49ab39f942af3846cf0d8599674b0adde94b5", "patch": "@@ -14,10 +14,9 @@ use ext::base::ExtCtxt;\n use ext::base;\n use ext::build::AstBuilder;\n use parse::parser::{Parser, PathStyle};\n-use parse::token::*;\n use parse::token;\n use ptr::P;\n-use tokenstream::{self, TokenTree};\n+use tokenstream::TokenTree;\n \n \n /// Quasiquoting works via token trees.\n@@ -356,14 +355,35 @@ pub mod rt {\n         }\n \n         fn parse_tts(&self, s: String) -> Vec<TokenTree> {\n-            panictry!(parse::parse_tts_from_source_str(\n-                \"<quote expansion>\".to_string(),\n-                s,\n-                self.parse_sess()))\n+            parse::parse_tts_from_source_str(\"<quote expansion>\".to_string(), s, self.parse_sess())\n         }\n     }\n }\n \n+// Replaces `Token::OpenDelim .. Token::CloseDelim` with `TokenTree::Delimited(..)`.\n+pub fn unflatten(tts: Vec<TokenTree>) -> Vec<TokenTree> {\n+    use std::rc::Rc;\n+    use tokenstream::Delimited;\n+\n+    let mut results = Vec::new();\n+    let mut result = Vec::new();\n+    for tree in tts {\n+        match tree {\n+            TokenTree::Token(_, token::OpenDelim(..)) => {\n+                results.push(::std::mem::replace(&mut result, Vec::new()));\n+            }\n+            TokenTree::Token(span, token::CloseDelim(delim)) => {\n+                let tree =\n+                    TokenTree::Delimited(span, Rc::new(Delimited { delim: delim, tts: result }));\n+                result = results.pop().unwrap();\n+                result.push(tree);\n+            }\n+            tree @ _ => result.push(tree),\n+        }\n+    }\n+    result\n+}\n+\n // These panicking parsing functions are used by the quote_*!() syntax extensions,\n // but shouldn't be used otherwise.\n pub fn parse_expr_panic(parser: &mut Parser) -> P<Expr> {\n@@ -510,20 +530,6 @@ pub fn expand_quote_path(cx: &mut ExtCtxt,\n     base::MacEager::expr(expanded)\n }\n \n-pub fn expand_quote_matcher(cx: &mut ExtCtxt,\n-                            sp: Span,\n-                            tts: &[TokenTree])\n-                            -> Box<base::MacResult+'static> {\n-    let (cx_expr, tts) = parse_arguments_to_quote(cx, tts);\n-    let mut vector = mk_stmts_let(cx, sp);\n-    vector.extend(statements_mk_tts(cx, &tts[..], true));\n-    vector.push(cx.stmt_expr(cx.expr_ident(sp, id_ext(\"tt\"))));\n-    let block = cx.expr_block(cx.block(sp, vector));\n-\n-    let expanded = expand_wrapper(cx, sp, cx_expr, block, &[&[\"syntax\", \"ext\", \"quote\", \"rt\"]]);\n-    base::MacEager::expr(expanded)\n-}\n-\n fn ids_ext(strs: Vec<String>) -> Vec<ast::Ident> {\n     strs.iter().map(|s| ast::Ident::from_str(s)).collect()\n }\n@@ -669,12 +675,6 @@ fn expr_mk_token(cx: &ExtCtxt, sp: Span, tok: &token::Token) -> P<ast::Expr> {\n                                 vec![mk_name(cx, sp, ast::Ident::with_empty_ctxt(ident))]);\n         }\n \n-        token::MatchNt(name, kind) => {\n-            return cx.expr_call(sp,\n-                                mk_token_path(cx, sp, \"MatchNt\"),\n-                                vec![mk_ident(cx, sp, name), mk_ident(cx, sp, kind)]);\n-        }\n-\n         token::Interpolated(_) => panic!(\"quote! with interpolated token\"),\n \n         _ => ()\n@@ -712,9 +712,9 @@ fn expr_mk_token(cx: &ExtCtxt, sp: Span, tok: &token::Token) -> P<ast::Expr> {\n     mk_token_path(cx, sp, name)\n }\n \n-fn statements_mk_tt(cx: &ExtCtxt, tt: &TokenTree, matcher: bool) -> Vec<ast::Stmt> {\n+fn statements_mk_tt(cx: &ExtCtxt, tt: &TokenTree, quoted: bool) -> Vec<ast::Stmt> {\n     match *tt {\n-        TokenTree::Token(sp, SubstNt(ident)) => {\n+        TokenTree::Token(sp, token::Ident(ident)) if quoted => {\n             // tt.extend($ident.to_tokens(ext_cx))\n \n             let e_to_toks =\n@@ -733,13 +733,6 @@ fn statements_mk_tt(cx: &ExtCtxt, tt: &TokenTree, matcher: bool) -> Vec<ast::Stm\n \n             vec![cx.stmt_expr(e_push)]\n         }\n-        ref tt @ TokenTree::Token(_, MatchNt(..)) if !matcher => {\n-            let mut seq = vec![];\n-            for i in 0..tt.len() {\n-                seq.push(tt.get_tt(i));\n-            }\n-            statements_mk_tts(cx, &seq[..], matcher)\n-        }\n         TokenTree::Token(sp, ref tok) => {\n             let e_sp = cx.expr_ident(sp, id_ext(\"_sp\"));\n             let e_tok = cx.expr_call(sp,\n@@ -753,77 +746,17 @@ fn statements_mk_tt(cx: &ExtCtxt, tt: &TokenTree, matcher: bool) -> Vec<ast::Stm\n             vec![cx.stmt_expr(e_push)]\n         },\n         TokenTree::Delimited(span, ref delimed) => {\n-            statements_mk_tt(cx, &delimed.open_tt(span), matcher).into_iter()\n-                .chain(delimed.tts.iter()\n-                                  .flat_map(|tt| statements_mk_tt(cx, tt, matcher)))\n-                .chain(statements_mk_tt(cx, &delimed.close_tt(span), matcher))\n-                .collect()\n-        },\n-        TokenTree::Sequence(sp, ref seq) => {\n-            if !matcher {\n-                panic!(\"TokenTree::Sequence in quote!\");\n-            }\n-\n-            let e_sp = cx.expr_ident(sp, id_ext(\"_sp\"));\n-\n-            let stmt_let_tt = cx.stmt_let(sp, true, id_ext(\"tt\"), cx.expr_vec_ng(sp));\n-            let mut tts_stmts = vec![stmt_let_tt];\n-            tts_stmts.extend(statements_mk_tts(cx, &seq.tts[..], matcher));\n-            tts_stmts.push(cx.stmt_expr(cx.expr_ident(sp, id_ext(\"tt\"))));\n-            let e_tts = cx.expr_block(cx.block(sp, tts_stmts));\n-\n-            let e_separator = match seq.separator {\n-                Some(ref sep) => cx.expr_some(sp, expr_mk_token(cx, sp, sep)),\n-                None => cx.expr_none(sp),\n-            };\n-            let e_op = match seq.op {\n-                tokenstream::KleeneOp::ZeroOrMore => \"ZeroOrMore\",\n-                tokenstream::KleeneOp::OneOrMore => \"OneOrMore\",\n-            };\n-            let e_op_idents = vec![\n-                id_ext(\"syntax\"),\n-                id_ext(\"tokenstream\"),\n-                id_ext(\"KleeneOp\"),\n-                id_ext(e_op),\n-            ];\n-            let e_op = cx.expr_path(cx.path_global(sp, e_op_idents));\n-            let fields = vec![cx.field_imm(sp, id_ext(\"tts\"), e_tts),\n-                              cx.field_imm(sp, id_ext(\"separator\"), e_separator),\n-                              cx.field_imm(sp, id_ext(\"op\"), e_op),\n-                              cx.field_imm(sp, id_ext(\"num_captures\"),\n-                                               cx.expr_usize(sp, seq.num_captures))];\n-            let seq_path = vec![id_ext(\"syntax\"),\n-                                id_ext(\"tokenstream\"),\n-                                id_ext(\"SequenceRepetition\")];\n-            let e_seq_struct = cx.expr_struct(sp, cx.path_global(sp, seq_path), fields);\n-            let e_rc_new = cx.expr_call_global(sp, vec![id_ext(\"std\"),\n-                                                        id_ext(\"rc\"),\n-                                                        id_ext(\"Rc\"),\n-                                                        id_ext(\"new\")],\n-                                                   vec![e_seq_struct]);\n-            let e_tok = cx.expr_call(sp,\n-                                     mk_tt_path(cx, sp, \"Sequence\"),\n-                                     vec![e_sp, e_rc_new]);\n-            let e_push =\n-                cx.expr_method_call(sp,\n-                                    cx.expr_ident(sp, id_ext(\"tt\")),\n-                                    id_ext(\"push\"),\n-                                    vec![e_tok]);\n-            vec![cx.stmt_expr(e_push)]\n+            let mut stmts = statements_mk_tt(cx, &delimed.open_tt(span), false);\n+            stmts.extend(statements_mk_tts(cx, &delimed.tts));\n+            stmts.extend(statements_mk_tt(cx, &delimed.close_tt(span), false));\n+            stmts\n         }\n     }\n }\n \n fn parse_arguments_to_quote(cx: &ExtCtxt, tts: &[TokenTree])\n                             -> (P<ast::Expr>, Vec<TokenTree>) {\n-    // NB: It appears that the main parser loses its mind if we consider\n-    // $foo as a SubstNt during the main parse, so we have to re-parse\n-    // under quote_depth > 0. This is silly and should go away; the _guess_ is\n-    // it has to do with transition away from supporting old-style macros, so\n-    // try removing it when enough of them are gone.\n-\n     let mut p = cx.new_parser_from_tts(tts);\n-    p.quote_depth += 1;\n \n     let cx_expr = panictry!(p.parse_expr());\n     if !p.eat(&token::Comma) {\n@@ -877,24 +810,31 @@ fn mk_stmts_let(cx: &ExtCtxt, sp: Span) -> Vec<ast::Stmt> {\n     vec![stmt_let_sp, stmt_let_tt]\n }\n \n-fn statements_mk_tts(cx: &ExtCtxt, tts: &[TokenTree], matcher: bool) -> Vec<ast::Stmt> {\n+fn statements_mk_tts(cx: &ExtCtxt, tts: &[TokenTree]) -> Vec<ast::Stmt> {\n     let mut ss = Vec::new();\n+    let mut quoted = false;\n     for tt in tts {\n-        ss.extend(statements_mk_tt(cx, tt, matcher));\n+        quoted = match *tt {\n+            TokenTree::Token(_, token::Dollar) if !quoted => true,\n+            _ => {\n+                ss.extend(statements_mk_tt(cx, tt, quoted));\n+                false\n+            }\n+        }\n     }\n     ss\n }\n \n-fn expand_tts(cx: &ExtCtxt, sp: Span, tts: &[TokenTree])\n-              -> (P<ast::Expr>, P<ast::Expr>) {\n+fn expand_tts(cx: &ExtCtxt, sp: Span, tts: &[TokenTree]) -> (P<ast::Expr>, P<ast::Expr>) {\n     let (cx_expr, tts) = parse_arguments_to_quote(cx, tts);\n \n     let mut vector = mk_stmts_let(cx, sp);\n-    vector.extend(statements_mk_tts(cx, &tts[..], false));\n+    vector.extend(statements_mk_tts(cx, &tts[..]));\n     vector.push(cx.stmt_expr(cx.expr_ident(sp, id_ext(\"tt\"))));\n     let block = cx.expr_block(cx.block(sp, vector));\n+    let unflatten = vec![id_ext(\"syntax\"), id_ext(\"ext\"), id_ext(\"quote\"), id_ext(\"unflatten\")];\n \n-    (cx_expr, block)\n+    (cx_expr, cx.expr_call_global(sp, unflatten, vec![block]))\n }\n \n fn expand_wrapper(cx: &ExtCtxt,"}, {"sha": "6ab5123bc87b16b799aa6630843dfd08accd9af1", "filename": "src/libsyntax/ext/tt/macro_parser.rs", "status": "modified", "additions": 51, "deletions": 35, "changes": 86, "blob_url": "https://github.com/rust-lang/rust/blob/4ba49ab39f942af3846cf0d8599674b0adde94b5/src%2Flibsyntax%2Fext%2Ftt%2Fmacro_parser.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4ba49ab39f942af3846cf0d8599674b0adde94b5/src%2Flibsyntax%2Fext%2Ftt%2Fmacro_parser.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fext%2Ftt%2Fmacro_parser.rs?ref=4ba49ab39f942af3846cf0d8599674b0adde94b5", "patch": "@@ -82,13 +82,13 @@ use ast::Ident;\n use syntax_pos::{self, BytePos, mk_sp, Span};\n use codemap::Spanned;\n use errors::FatalError;\n+use ext::tt::quoted;\n use parse::{Directory, ParseSess};\n use parse::parser::{PathStyle, Parser};\n-use parse::token::{DocComment, MatchNt, SubstNt};\n-use parse::token::{Token, Nonterminal};\n-use parse::token;\n+use parse::token::{self, DocComment, Token, Nonterminal};\n use print::pprust;\n-use tokenstream::{self, TokenTree};\n+use symbol::keywords;\n+use tokenstream::TokenTree;\n use util::small_vector::SmallVector;\n \n use std::mem;\n@@ -101,8 +101,8 @@ use std::collections::hash_map::Entry::{Vacant, Occupied};\n \n #[derive(Clone)]\n enum TokenTreeOrTokenTreeVec {\n-    Tt(tokenstream::TokenTree),\n-    TtSeq(Vec<tokenstream::TokenTree>),\n+    Tt(quoted::TokenTree),\n+    TtSeq(Vec<quoted::TokenTree>),\n }\n \n impl TokenTreeOrTokenTreeVec {\n@@ -113,7 +113,7 @@ impl TokenTreeOrTokenTreeVec {\n         }\n     }\n \n-    fn get_tt(&self, index: usize) -> TokenTree {\n+    fn get_tt(&self, index: usize) -> quoted::TokenTree {\n         match *self {\n             TtSeq(ref v) => v[index].clone(),\n             Tt(ref tt) => tt.get_tt(index),\n@@ -144,7 +144,9 @@ struct MatcherPos {\n \n pub type NamedParseResult = ParseResult<HashMap<Ident, Rc<NamedMatch>>>;\n \n-pub fn count_names(ms: &[TokenTree]) -> usize {\n+pub fn count_names(ms: &[quoted::TokenTree]) -> usize {\n+    use self::quoted::TokenTree;\n+\n     ms.iter().fold(0, |count, elt| {\n         count + match *elt {\n             TokenTree::Sequence(_, ref seq) => {\n@@ -153,15 +155,15 @@ pub fn count_names(ms: &[TokenTree]) -> usize {\n             TokenTree::Delimited(_, ref delim) => {\n                 count_names(&delim.tts)\n             }\n-            TokenTree::Token(_, MatchNt(..)) => {\n+            TokenTree::MetaVarDecl(..) => {\n                 1\n             }\n             TokenTree::Token(..) => 0,\n         }\n     })\n }\n \n-fn initial_matcher_pos(ms: Vec<TokenTree>, lo: BytePos) -> Box<MatcherPos> {\n+fn initial_matcher_pos(ms: Vec<quoted::TokenTree>, lo: BytePos) -> Box<MatcherPos> {\n     let match_idx_hi = count_names(&ms[..]);\n     let matches = create_matches(match_idx_hi);\n     Box::new(MatcherPos {\n@@ -200,22 +202,30 @@ pub enum NamedMatch {\n     MatchedNonterminal(Rc<Nonterminal>)\n }\n \n-fn nameize<I: Iterator<Item=Rc<NamedMatch>>>(ms: &[TokenTree], mut res: I) -> NamedParseResult {\n-    fn n_rec<I: Iterator<Item=Rc<NamedMatch>>>(m: &TokenTree, mut res: &mut I,\n+fn nameize<I: Iterator<Item=Rc<NamedMatch>>>(sess: &ParseSess, ms: &[quoted::TokenTree], mut res: I)\n+                                             -> NamedParseResult {\n+    use self::quoted::TokenTree;\n+\n+    fn n_rec<I: Iterator<Item=Rc<NamedMatch>>>(sess: &ParseSess, m: &TokenTree, mut res: &mut I,\n              ret_val: &mut HashMap<Ident, Rc<NamedMatch>>)\n              -> Result<(), (syntax_pos::Span, String)> {\n         match *m {\n             TokenTree::Sequence(_, ref seq) => {\n                 for next_m in &seq.tts {\n-                    n_rec(next_m, res.by_ref(), ret_val)?\n+                    n_rec(sess, next_m, res.by_ref(), ret_val)?\n                 }\n             }\n             TokenTree::Delimited(_, ref delim) => {\n                 for next_m in &delim.tts {\n-                    n_rec(next_m, res.by_ref(), ret_val)?;\n+                    n_rec(sess, next_m, res.by_ref(), ret_val)?;\n                 }\n             }\n-            TokenTree::Token(sp, MatchNt(bind_name, _)) => {\n+            TokenTree::MetaVarDecl(span, _, id) if id.name == keywords::Invalid.name() => {\n+                if sess.missing_fragment_specifiers.borrow_mut().remove(&span) {\n+                    return Err((span, \"missing fragment specifier\".to_string()));\n+                }\n+            }\n+            TokenTree::MetaVarDecl(sp, bind_name, _) => {\n                 match ret_val.entry(bind_name) {\n                     Vacant(spot) => {\n                         spot.insert(res.next().unwrap());\n@@ -225,9 +235,6 @@ fn nameize<I: Iterator<Item=Rc<NamedMatch>>>(ms: &[TokenTree], mut res: I) -> Na\n                     }\n                 }\n             }\n-            TokenTree::Token(sp, SubstNt(..)) => {\n-                return Err((sp, \"missing fragment specifier\".to_string()))\n-            }\n             TokenTree::Token(..) => (),\n         }\n \n@@ -236,7 +243,7 @@ fn nameize<I: Iterator<Item=Rc<NamedMatch>>>(ms: &[TokenTree], mut res: I) -> Na\n \n     let mut ret_val = HashMap::new();\n     for m in ms {\n-        match n_rec(m, res.by_ref(), &mut ret_val) {\n+        match n_rec(sess, m, res.by_ref(), &mut ret_val) {\n             Ok(_) => {},\n             Err((sp, msg)) => return Error(sp, msg),\n         }\n@@ -276,11 +283,15 @@ fn create_matches(len: usize) -> Vec<Vec<Rc<NamedMatch>>> {\n     (0..len).into_iter().map(|_| Vec::new()).collect()\n }\n \n-fn inner_parse_loop(cur_eis: &mut SmallVector<Box<MatcherPos>>,\n+fn inner_parse_loop(sess: &ParseSess,\n+                    cur_eis: &mut SmallVector<Box<MatcherPos>>,\n                     next_eis: &mut Vec<Box<MatcherPos>>,\n                     eof_eis: &mut SmallVector<Box<MatcherPos>>,\n                     bb_eis: &mut SmallVector<Box<MatcherPos>>,\n-                    token: &Token, span: &syntax_pos::Span) -> ParseResult<()> {\n+                    token: &Token,\n+                    span: &syntax_pos::Span) -> ParseResult<()> {\n+    use self::quoted::TokenTree;\n+\n     while let Some(mut ei) = cur_eis.pop() {\n         // When unzipped trees end, remove them\n         while ei.idx >= ei.top_elts.len() {\n@@ -346,7 +357,7 @@ fn inner_parse_loop(cur_eis: &mut SmallVector<Box<MatcherPos>>,\n             match ei.top_elts.get_tt(idx) {\n                 /* need to descend into sequence */\n                 TokenTree::Sequence(sp, seq) => {\n-                    if seq.op == tokenstream::KleeneOp::ZeroOrMore {\n+                    if seq.op == quoted::KleeneOp::ZeroOrMore {\n                         // Examine the case where there are 0 matches of this sequence\n                         let mut new_ei = ei.clone();\n                         new_ei.match_cur += seq.num_captures;\n@@ -372,17 +383,19 @@ fn inner_parse_loop(cur_eis: &mut SmallVector<Box<MatcherPos>>,\n                         top_elts: Tt(TokenTree::Sequence(sp, seq)),\n                     }));\n                 }\n-                TokenTree::Token(_, MatchNt(..)) => {\n+                TokenTree::MetaVarDecl(span, _, id) if id.name == keywords::Invalid.name() => {\n+                    if sess.missing_fragment_specifiers.borrow_mut().remove(&span) {\n+                        return Error(span, \"missing fragment specifier\".to_string());\n+                    }\n+                }\n+                TokenTree::MetaVarDecl(..) => {\n                     // Built-in nonterminals never start with these tokens,\n                     // so we can eliminate them from consideration.\n                     match *token {\n                         token::CloseDelim(_) => {},\n                         _ => bb_eis.push(ei),\n                     }\n                 }\n-                TokenTree::Token(sp, SubstNt(..)) => {\n-                    return Error(sp, \"missing fragment specifier\".to_string())\n-                }\n                 seq @ TokenTree::Delimited(..) | seq @ TokenTree::Token(_, DocComment(..)) => {\n                     let lower_elts = mem::replace(&mut ei.top_elts, Tt(seq));\n                     let idx = ei.idx;\n@@ -406,8 +419,13 @@ fn inner_parse_loop(cur_eis: &mut SmallVector<Box<MatcherPos>>,\n     Success(())\n }\n \n-pub fn parse(sess: &ParseSess, tts: Vec<TokenTree>, ms: &[TokenTree], directory: Option<Directory>)\n+pub fn parse(sess: &ParseSess,\n+             tts: Vec<TokenTree>,\n+             ms: &[quoted::TokenTree],\n+             directory: Option<Directory>)\n              -> NamedParseResult {\n+    use self::quoted::TokenTree;\n+\n     let mut parser = Parser::new(sess, tts, directory, true);\n     let mut cur_eis = SmallVector::one(initial_matcher_pos(ms.to_owned(), parser.span.lo));\n     let mut next_eis = Vec::new(); // or proceed normally\n@@ -417,7 +435,7 @@ pub fn parse(sess: &ParseSess, tts: Vec<TokenTree>, ms: &[TokenTree], directory:\n         let mut eof_eis = SmallVector::new();\n         assert!(next_eis.is_empty());\n \n-        match inner_parse_loop(&mut cur_eis, &mut next_eis, &mut eof_eis, &mut bb_eis,\n+        match inner_parse_loop(sess, &mut cur_eis, &mut next_eis, &mut eof_eis, &mut bb_eis,\n                                &parser.token, &parser.span) {\n             Success(_) => {},\n             Failure(sp, tok) => return Failure(sp, tok),\n@@ -430,15 +448,16 @@ pub fn parse(sess: &ParseSess, tts: Vec<TokenTree>, ms: &[TokenTree], directory:\n         /* error messages here could be improved with links to orig. rules */\n         if token_name_eq(&parser.token, &token::Eof) {\n             if eof_eis.len() == 1 {\n-                return nameize(ms, eof_eis[0].matches.iter_mut().map(|mut dv| dv.pop().unwrap()));\n+                let matches = eof_eis[0].matches.iter_mut().map(|mut dv| dv.pop().unwrap());\n+                return nameize(sess, ms, matches);\n             } else if eof_eis.len() > 1 {\n                 return Error(parser.span, \"ambiguity: multiple successful parses\".to_string());\n             } else {\n                 return Failure(parser.span, token::Eof);\n             }\n         } else if (!bb_eis.is_empty() && !next_eis.is_empty()) || bb_eis.len() > 1 {\n             let nts = bb_eis.iter().map(|ei| match ei.top_elts.get_tt(ei.idx) {\n-                TokenTree::Token(_, MatchNt(bind, name)) => {\n+                TokenTree::MetaVarDecl(_, bind, name) => {\n                     format!(\"{} ('{}')\", name, bind)\n                 }\n                 _ => panic!()\n@@ -460,7 +479,7 @@ pub fn parse(sess: &ParseSess, tts: Vec<TokenTree>, ms: &[TokenTree], directory:\n             parser.bump();\n         } else /* bb_eis.len() == 1 */ {\n             let mut ei = bb_eis.pop().unwrap();\n-            if let TokenTree::Token(span, MatchNt(_, ident)) = ei.top_elts.get_tt(ei.idx) {\n+            if let TokenTree::MetaVarDecl(span, _, ident) = ei.top_elts.get_tt(ei.idx) {\n                 let match_cur = ei.match_cur;\n                 ei.matches[match_cur].push(Rc::new(MatchedNonterminal(\n                             Rc::new(parse_nt(&mut parser, span, &ident.name.as_str())))));\n@@ -479,10 +498,7 @@ pub fn parse(sess: &ParseSess, tts: Vec<TokenTree>, ms: &[TokenTree], directory:\n fn parse_nt<'a>(p: &mut Parser<'a>, sp: Span, name: &str) -> Nonterminal {\n     match name {\n         \"tt\" => {\n-            p.quote_depth += 1; //but in theory, non-quoted tts might be useful\n-            let tt = panictry!(p.parse_token_tree());\n-            p.quote_depth -= 1;\n-            return token::NtTT(tt);\n+            return token::NtTT(panictry!(p.parse_token_tree()));\n         }\n         _ => {}\n     }"}, {"sha": "193c06707c7a6860a2bb794aa01946f2fcc94442", "filename": "src/libsyntax/ext/tt/macro_rules.rs", "status": "modified", "additions": 104, "deletions": 87, "changes": 191, "blob_url": "https://github.com/rust-lang/rust/blob/4ba49ab39f942af3846cf0d8599674b0adde94b5/src%2Flibsyntax%2Fext%2Ftt%2Fmacro_rules.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4ba49ab39f942af3846cf0d8599674b0adde94b5/src%2Flibsyntax%2Fext%2Ftt%2Fmacro_rules.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fext%2Ftt%2Fmacro_rules.rs?ref=4ba49ab39f942af3846cf0d8599674b0adde94b5", "patch": "@@ -16,14 +16,15 @@ use ext::expand::{Expansion, ExpansionKind};\n use ext::tt::macro_parser::{Success, Error, Failure};\n use ext::tt::macro_parser::{MatchedSeq, MatchedNonterminal};\n use ext::tt::macro_parser::{parse, parse_failure_msg};\n+use ext::tt::quoted;\n use ext::tt::transcribe::transcribe;\n use parse::{Directory, ParseSess};\n use parse::parser::Parser;\n-use parse::token::{self, NtTT, Token};\n+use parse::token::{self, NtTT};\n use parse::token::Token::*;\n use print;\n use symbol::Symbol;\n-use tokenstream::{self, TokenTree};\n+use tokenstream::TokenTree;\n \n use std::collections::{HashMap};\n use std::collections::hash_map::{Entry};\n@@ -58,8 +59,8 @@ impl<'a> ParserAnyMacro<'a> {\n \n struct MacroRulesMacroExpander {\n     name: ast::Ident,\n-    lhses: Vec<TokenTree>,\n-    rhses: Vec<TokenTree>,\n+    lhses: Vec<quoted::TokenTree>,\n+    rhses: Vec<quoted::TokenTree>,\n     valid: bool,\n }\n \n@@ -86,8 +87,8 @@ fn generic_extension<'cx>(cx: &'cx ExtCtxt,\n                           sp: Span,\n                           name: ast::Ident,\n                           arg: &[TokenTree],\n-                          lhses: &[TokenTree],\n-                          rhses: &[TokenTree])\n+                          lhses: &[quoted::TokenTree],\n+                          rhses: &[quoted::TokenTree])\n                           -> Box<MacResult+'cx> {\n     if cx.trace_macros() {\n         println!(\"{}! {{ {} }}\",\n@@ -101,15 +102,15 @@ fn generic_extension<'cx>(cx: &'cx ExtCtxt,\n \n     for (i, lhs) in lhses.iter().enumerate() { // try each arm's matchers\n         let lhs_tt = match *lhs {\n-            TokenTree::Delimited(_, ref delim) => &delim.tts[..],\n+            quoted::TokenTree::Delimited(_, ref delim) => &delim.tts[..],\n             _ => cx.span_bug(sp, \"malformed macro lhs\")\n         };\n \n         match TokenTree::parse(cx, lhs_tt, arg) {\n             Success(named_matches) => {\n                 let rhs = match rhses[i] {\n                     // ignore delimiters\n-                    TokenTree::Delimited(_, ref delimed) => delimed.tts.clone(),\n+                    quoted::TokenTree::Delimited(_, ref delimed) => delimed.tts.clone(),\n                     _ => cx.span_bug(sp, \"malformed macro rhs\"),\n                 };\n                 // rhs has holes ( `$id` and `$(...)` that need filled)\n@@ -164,24 +165,22 @@ pub fn compile(sess: &ParseSess, def: &ast::MacroDef) -> SyntaxExtension {\n     // $( $lhs:tt => $rhs:tt );+\n     // ...quasiquoting this would be nice.\n     // These spans won't matter, anyways\n-    let match_lhs_tok = MatchNt(lhs_nm, ast::Ident::from_str(\"tt\"));\n-    let match_rhs_tok = MatchNt(rhs_nm, ast::Ident::from_str(\"tt\"));\n     let argument_gram = vec![\n-        TokenTree::Sequence(DUMMY_SP, Rc::new(tokenstream::SequenceRepetition {\n+        quoted::TokenTree::Sequence(DUMMY_SP, Rc::new(quoted::SequenceRepetition {\n             tts: vec![\n-                TokenTree::Token(DUMMY_SP, match_lhs_tok),\n-                TokenTree::Token(DUMMY_SP, token::FatArrow),\n-                TokenTree::Token(DUMMY_SP, match_rhs_tok),\n+                quoted::TokenTree::MetaVarDecl(DUMMY_SP, lhs_nm, ast::Ident::from_str(\"tt\")),\n+                quoted::TokenTree::Token(DUMMY_SP, token::FatArrow),\n+                quoted::TokenTree::MetaVarDecl(DUMMY_SP, rhs_nm, ast::Ident::from_str(\"tt\")),\n             ],\n             separator: Some(token::Semi),\n-            op: tokenstream::KleeneOp::OneOrMore,\n+            op: quoted::KleeneOp::OneOrMore,\n             num_captures: 2,\n         })),\n         // to phase into semicolon-termination instead of semicolon-separation\n-        TokenTree::Sequence(DUMMY_SP, Rc::new(tokenstream::SequenceRepetition {\n-            tts: vec![TokenTree::Token(DUMMY_SP, token::Semi)],\n+        quoted::TokenTree::Sequence(DUMMY_SP, Rc::new(quoted::SequenceRepetition {\n+            tts: vec![quoted::TokenTree::Token(DUMMY_SP, token::Semi)],\n             separator: None,\n-            op: tokenstream::KleeneOp::ZeroOrMore,\n+            op: quoted::KleeneOp::ZeroOrMore,\n             num_captures: 0\n         })),\n     ];\n@@ -206,12 +205,13 @@ pub fn compile(sess: &ParseSess, def: &ast::MacroDef) -> SyntaxExtension {\n             s.iter().map(|m| {\n                 if let MatchedNonterminal(ref nt) = **m {\n                     if let NtTT(ref tt) = **nt {\n-                        valid &= check_lhs_nt_follows(sess, tt);\n-                        return (*tt).clone();\n+                        let tt = quoted::parse(&[tt.clone()], true, sess).pop().unwrap();\n+                        valid &= check_lhs_nt_follows(sess, &tt);\n+                        return tt;\n                     }\n                 }\n                 sess.span_diagnostic.span_bug(def.span, \"wrong-structured lhs\")\n-            }).collect::<Vec<TokenTree>>()\n+            }).collect::<Vec<quoted::TokenTree>>()\n         }\n         _ => sess.span_diagnostic.span_bug(def.span, \"wrong-structured lhs\")\n     };\n@@ -221,11 +221,11 @@ pub fn compile(sess: &ParseSess, def: &ast::MacroDef) -> SyntaxExtension {\n             s.iter().map(|m| {\n                 if let MatchedNonterminal(ref nt) = **m {\n                     if let NtTT(ref tt) = **nt {\n-                        return (*tt).clone();\n+                        return quoted::parse(&[tt.clone()], false, sess).pop().unwrap();\n                     }\n                 }\n                 sess.span_diagnostic.span_bug(def.span, \"wrong-structured lhs\")\n-            }).collect()\n+            }).collect::<Vec<quoted::TokenTree>>()\n         }\n         _ => sess.span_diagnostic.span_bug(def.span, \"wrong-structured rhs\")\n     };\n@@ -249,14 +249,14 @@ pub fn compile(sess: &ParseSess, def: &ast::MacroDef) -> SyntaxExtension {\n     NormalTT(exp, Some(def.span), attr::contains_name(&def.attrs, \"allow_internal_unstable\"))\n }\n \n-fn check_lhs_nt_follows(sess: &ParseSess, lhs: &TokenTree) -> bool {\n+fn check_lhs_nt_follows(sess: &ParseSess, lhs: &quoted::TokenTree) -> bool {\n     // lhs is going to be like TokenTree::Delimited(...), where the\n     // entire lhs is those tts. Or, it can be a \"bare sequence\", not wrapped in parens.\n     match lhs {\n-        &TokenTree::Delimited(_, ref tts) => check_matcher(sess, &tts.tts),\n+        &quoted::TokenTree::Delimited(_, ref tts) => check_matcher(sess, &tts.tts),\n         _ => {\n             let msg = \"invalid macro matcher; matchers must be contained in balanced delimiters\";\n-            sess.span_diagnostic.span_err(lhs.get_span(), msg);\n+            sess.span_diagnostic.span_err(lhs.span(), msg);\n             false\n         }\n     }\n@@ -266,10 +266,11 @@ fn check_lhs_nt_follows(sess: &ParseSess, lhs: &TokenTree) -> bool {\n \n /// Check that the lhs contains no repetition which could match an empty token\n /// tree, because then the matcher would hang indefinitely.\n-fn check_lhs_no_empty_seq(sess: &ParseSess, tts: &[TokenTree]) -> bool {\n+fn check_lhs_no_empty_seq(sess: &ParseSess, tts: &[quoted::TokenTree]) -> bool {\n+    use self::quoted::TokenTree;\n     for tt in tts {\n         match *tt {\n-            TokenTree::Token(_, _) => (),\n+            TokenTree::Token(..) | TokenTree::MetaVarDecl(..) => (),\n             TokenTree::Delimited(_, ref del) => if !check_lhs_no_empty_seq(sess, &del.tts) {\n                 return false;\n             },\n@@ -278,7 +279,7 @@ fn check_lhs_no_empty_seq(sess: &ParseSess, tts: &[TokenTree]) -> bool {\n                     if seq.tts.iter().all(|seq_tt| {\n                         match *seq_tt {\n                             TokenTree::Sequence(_, ref sub_seq) =>\n-                                sub_seq.op == tokenstream::KleeneOp::ZeroOrMore,\n+                                sub_seq.op == quoted::KleeneOp::ZeroOrMore,\n                             _ => false,\n                         }\n                     }) {\n@@ -296,15 +297,15 @@ fn check_lhs_no_empty_seq(sess: &ParseSess, tts: &[TokenTree]) -> bool {\n     true\n }\n \n-fn check_rhs(sess: &ParseSess, rhs: &TokenTree) -> bool {\n+fn check_rhs(sess: &ParseSess, rhs: &quoted::TokenTree) -> bool {\n     match *rhs {\n-        TokenTree::Delimited(..) => return true,\n-        _ => sess.span_diagnostic.span_err(rhs.get_span(), \"macro rhs must be delimited\")\n+        quoted::TokenTree::Delimited(..) => return true,\n+        _ => sess.span_diagnostic.span_err(rhs.span(), \"macro rhs must be delimited\")\n     }\n     false\n }\n \n-fn check_matcher(sess: &ParseSess, matcher: &[TokenTree]) -> bool {\n+fn check_matcher(sess: &ParseSess, matcher: &[quoted::TokenTree]) -> bool {\n     let first_sets = FirstSets::new(matcher);\n     let empty_suffix = TokenSet::empty();\n     let err = sess.span_diagnostic.err_count();\n@@ -335,7 +336,9 @@ struct FirstSets {\n }\n \n impl FirstSets {\n-    fn new(tts: &[TokenTree]) -> FirstSets {\n+    fn new(tts: &[quoted::TokenTree]) -> FirstSets {\n+        use self::quoted::TokenTree;\n+\n         let mut sets = FirstSets { first: HashMap::new() };\n         build_recur(&mut sets, tts);\n         return sets;\n@@ -347,13 +350,12 @@ impl FirstSets {\n             let mut first = TokenSet::empty();\n             for tt in tts.iter().rev() {\n                 match *tt {\n-                    TokenTree::Token(sp, ref tok) => {\n-                        first.replace_with((sp, tok.clone()));\n+                    TokenTree::Token(..) | TokenTree::MetaVarDecl(..) => {\n+                        first.replace_with(tt.clone());\n                     }\n                     TokenTree::Delimited(span, ref delimited) => {\n                         build_recur(sets, &delimited.tts[..]);\n-                        first.replace_with((delimited.open_tt(span).span(),\n-                                            Token::OpenDelim(delimited.delim)));\n+                        first.replace_with(delimited.open_tt(span));\n                     }\n                     TokenTree::Sequence(sp, ref seq_rep) => {\n                         let subfirst = build_recur(sets, &seq_rep.tts[..]);\n@@ -378,11 +380,11 @@ impl FirstSets {\n \n                         if let (Some(ref sep), true) = (seq_rep.separator.clone(),\n                                                         subfirst.maybe_empty) {\n-                            first.add_one_maybe((sp, sep.clone()));\n+                            first.add_one_maybe(TokenTree::Token(sp, sep.clone()));\n                         }\n \n                         // Reverse scan: Sequence comes before `first`.\n-                        if subfirst.maybe_empty || seq_rep.op == tokenstream::KleeneOp::ZeroOrMore {\n+                        if subfirst.maybe_empty || seq_rep.op == quoted::KleeneOp::ZeroOrMore {\n                             // If sequence is potentially empty, then\n                             // union them (preserving first emptiness).\n                             first.add_all(&TokenSet { maybe_empty: true, ..subfirst });\n@@ -401,18 +403,19 @@ impl FirstSets {\n \n     // walks forward over `tts` until all potential FIRST tokens are\n     // identified.\n-    fn first(&self, tts: &[TokenTree]) -> TokenSet {\n+    fn first(&self, tts: &[quoted::TokenTree]) -> TokenSet {\n+        use self::quoted::TokenTree;\n+\n         let mut first = TokenSet::empty();\n         for tt in tts.iter() {\n             assert!(first.maybe_empty);\n             match *tt {\n-                TokenTree::Token(sp, ref tok) => {\n-                    first.add_one((sp, tok.clone()));\n+                TokenTree::Token(..) | TokenTree::MetaVarDecl(..) => {\n+                    first.add_one(tt.clone());\n                     return first;\n                 }\n                 TokenTree::Delimited(span, ref delimited) => {\n-                    first.add_one((delimited.open_tt(span).span(),\n-                                   Token::OpenDelim(delimited.delim)));\n+                    first.add_one(delimited.open_tt(span));\n                     return first;\n                 }\n                 TokenTree::Sequence(sp, ref seq_rep) => {\n@@ -424,13 +427,13 @@ impl FirstSets {\n \n                             if let (Some(ref sep), true) = (seq_rep.separator.clone(),\n                                                             subfirst.maybe_empty) {\n-                                first.add_one_maybe((sp, sep.clone()));\n+                                first.add_one_maybe(TokenTree::Token(sp, sep.clone()));\n                             }\n \n                             assert!(first.maybe_empty);\n                             first.add_all(subfirst);\n                             if subfirst.maybe_empty ||\n-                               seq_rep.op == tokenstream::KleeneOp::ZeroOrMore {\n+                               seq_rep.op == quoted::KleeneOp::ZeroOrMore {\n                                 // continue scanning for more first\n                                 // tokens, but also make sure we\n                                 // restore empty-tracking state\n@@ -460,8 +463,8 @@ impl FirstSets {\n     }\n }\n \n-// A set of Tokens, which may include MatchNt tokens (for\n-// macro-by-example syntactic variables). It also carries the\n+// A set of `quoted::TokenTree`s, which may include `TokenTree::Match`s\n+// (for macro-by-example syntactic variables). It also carries the\n // `maybe_empty` flag; that is true if and only if the matcher can\n // match an empty token sequence.\n //\n@@ -472,7 +475,7 @@ impl FirstSets {\n // (Notably, we must allow for *-op to occur zero times.)\n #[derive(Clone, Debug)]\n struct TokenSet {\n-    tokens: Vec<(Span, Token)>,\n+    tokens: Vec<quoted::TokenTree>,\n     maybe_empty: bool,\n }\n \n@@ -482,13 +485,13 @@ impl TokenSet {\n \n     // Returns the set `{ tok }` for the single-token (and thus\n     // non-empty) sequence [tok].\n-    fn singleton(tok: (Span, Token)) -> Self {\n+    fn singleton(tok: quoted::TokenTree) -> Self {\n         TokenSet { tokens: vec![tok], maybe_empty: false }\n     }\n \n     // Changes self to be the set `{ tok }`.\n     // Since `tok` is always present, marks self as non-empty.\n-    fn replace_with(&mut self, tok: (Span, Token)) {\n+    fn replace_with(&mut self, tok: quoted::TokenTree) {\n         self.tokens.clear();\n         self.tokens.push(tok);\n         self.maybe_empty = false;\n@@ -503,15 +506,15 @@ impl TokenSet {\n     }\n \n     // Adds `tok` to the set for `self`, marking sequence as non-empy.\n-    fn add_one(&mut self, tok: (Span, Token)) {\n+    fn add_one(&mut self, tok: quoted::TokenTree) {\n         if !self.tokens.contains(&tok) {\n             self.tokens.push(tok);\n         }\n         self.maybe_empty = false;\n     }\n \n     // Adds `tok` to the set for `self`. (Leaves `maybe_empty` flag alone.)\n-    fn add_one_maybe(&mut self, tok: (Span, Token)) {\n+    fn add_one_maybe(&mut self, tok: quoted::TokenTree) {\n         if !self.tokens.contains(&tok) {\n             self.tokens.push(tok);\n         }\n@@ -549,9 +552,9 @@ impl TokenSet {\n // see `FirstSets::new`.\n fn check_matcher_core(sess: &ParseSess,\n                       first_sets: &FirstSets,\n-                      matcher: &[TokenTree],\n+                      matcher: &[quoted::TokenTree],\n                       follow: &TokenSet) -> TokenSet {\n-    use print::pprust::token_to_string;\n+    use self::quoted::TokenTree;\n \n     let mut last = TokenSet::empty();\n \n@@ -576,11 +579,11 @@ fn check_matcher_core(sess: &ParseSess,\n         // First, update `last` so that it corresponds to the set\n         // of NT tokens that might end the sequence `... token`.\n         match *token {\n-            TokenTree::Token(sp, ref tok) => {\n+            TokenTree::Token(..) | TokenTree::MetaVarDecl(..) => {\n                 let can_be_followed_by_any;\n-                if let Err(bad_frag) = has_legal_fragment_specifier(tok) {\n+                if let Err(bad_frag) = has_legal_fragment_specifier(token) {\n                     let msg = format!(\"invalid fragment specifier `{}`\", bad_frag);\n-                    sess.span_diagnostic.struct_span_err(sp, &msg)\n+                    sess.span_diagnostic.struct_span_err(token.span(), &msg)\n                         .help(\"valid fragment specifiers are `ident`, `block`, \\\n                                `stmt`, `expr`, `pat`, `ty`, `path`, `meta`, `tt` \\\n                                and `item`\")\n@@ -589,7 +592,7 @@ fn check_matcher_core(sess: &ParseSess,\n                     // from error messages.)\n                     can_be_followed_by_any = true;\n                 } else {\n-                    can_be_followed_by_any = token_can_be_followed_by_any(tok);\n+                    can_be_followed_by_any = token_can_be_followed_by_any(token);\n                 }\n \n                 if can_be_followed_by_any {\n@@ -599,13 +602,12 @@ fn check_matcher_core(sess: &ParseSess,\n                     // followed by anything against SUFFIX.\n                     continue 'each_token;\n                 } else {\n-                    last.replace_with((sp, tok.clone()));\n+                    last.replace_with(token.clone());\n                     suffix_first = build_suffix_first();\n                 }\n             }\n             TokenTree::Delimited(span, ref d) => {\n-                let my_suffix = TokenSet::singleton((d.close_tt(span).span(),\n-                                                     Token::CloseDelim(d.delim)));\n+                let my_suffix = TokenSet::singleton(d.close_tt(span));\n                 check_matcher_core(sess, first_sets, &d.tts, &my_suffix);\n                 // don't track non NT tokens\n                 last.replace_with_irrelevant();\n@@ -629,7 +631,7 @@ fn check_matcher_core(sess: &ParseSess,\n                 let mut new;\n                 let my_suffix = if let Some(ref u) = seq_rep.separator {\n                     new = suffix_first.clone();\n-                    new.add_one_maybe((sp, u.clone()));\n+                    new.add_one_maybe(TokenTree::Token(sp, u.clone()));\n                     &new\n                 } else {\n                     &suffix_first\n@@ -655,12 +657,13 @@ fn check_matcher_core(sess: &ParseSess,\n \n         // Now `last` holds the complete set of NT tokens that could\n         // end the sequence before SUFFIX. Check that every one works with `suffix`.\n-        'each_last: for &(_sp, ref t) in &last.tokens {\n-            if let MatchNt(ref name, ref frag_spec) = *t {\n-                for &(sp, ref next_token) in &suffix_first.tokens {\n+        'each_last: for token in &last.tokens {\n+            if let TokenTree::MetaVarDecl(_, ref name, ref frag_spec) = *token {\n+                for next_token in &suffix_first.tokens {\n                     match is_in_follow(next_token, &frag_spec.name.as_str()) {\n                         Err((msg, help)) => {\n-                            sess.span_diagnostic.struct_span_err(sp, &msg).help(help).emit();\n+                            sess.span_diagnostic.struct_span_err(next_token.span(), &msg)\n+                                .help(help).emit();\n                             // don't bother reporting every source of\n                             // conflict for a particular element of `last`.\n                             continue 'each_last;\n@@ -676,12 +679,12 @@ fn check_matcher_core(sess: &ParseSess,\n                             };\n \n                             sess.span_diagnostic.span_err(\n-                                sp,\n+                                next_token.span(),\n                                 &format!(\"`${name}:{frag}` {may_be} followed by `{next}`, which \\\n                                           is not allowed for `{frag}` fragments\",\n                                          name=name,\n                                          frag=frag_spec,\n-                                         next=token_to_string(next_token),\n+                                         next=quoted_tt_to_string(next_token),\n                                          may_be=may_be)\n                             );\n                         }\n@@ -693,8 +696,8 @@ fn check_matcher_core(sess: &ParseSess,\n     last\n }\n \n-fn token_can_be_followed_by_any(tok: &Token) -> bool {\n-    if let &MatchNt(_, ref frag_spec) = tok {\n+fn token_can_be_followed_by_any(tok: &quoted::TokenTree) -> bool {\n+    if let quoted::TokenTree::MetaVarDecl(_, _, frag_spec) = *tok {\n         frag_can_be_followed_by_any(&frag_spec.name.as_str())\n     } else {\n         // (Non NT's can always be followed by anthing in matchers.)\n@@ -732,8 +735,10 @@ fn frag_can_be_followed_by_any(frag: &str) -> bool {\n /// break macros that were relying on that binary operator as a\n /// separator.\n // when changing this do not forget to update doc/book/macros.md!\n-fn is_in_follow(tok: &Token, frag: &str) -> Result<bool, (String, &'static str)> {\n-    if let &CloseDelim(_) = tok {\n+fn is_in_follow(tok: &quoted::TokenTree, frag: &str) -> Result<bool, (String, &'static str)> {\n+    use self::quoted::TokenTree;\n+\n+    if let TokenTree::Token(_, token::CloseDelim(_)) = *tok {\n         // closing a token tree can never be matched by any fragment;\n         // iow, we always require that `(` and `)` match, etc.\n         Ok(true)\n@@ -749,27 +754,30 @@ fn is_in_follow(tok: &Token, frag: &str) -> Result<bool, (String, &'static str)>\n                 // maintain\n                 Ok(true)\n             },\n-            \"stmt\" | \"expr\"  => {\n-                match *tok {\n+            \"stmt\" | \"expr\"  => match *tok {\n+                TokenTree::Token(_, ref tok) => match *tok {\n                     FatArrow | Comma | Semi => Ok(true),\n                     _ => Ok(false)\n-                }\n+                },\n+                _ => Ok(false),\n             },\n-            \"pat\" => {\n-                match *tok {\n+            \"pat\" => match *tok {\n+                TokenTree::Token(_, ref tok) => match *tok {\n                     FatArrow | Comma | Eq | BinOp(token::Or) => Ok(true),\n                     Ident(i) if i.name == \"if\" || i.name == \"in\" => Ok(true),\n                     _ => Ok(false)\n-                }\n+                },\n+                _ => Ok(false),\n             },\n-            \"path\" | \"ty\" => {\n-                match *tok {\n+            \"path\" | \"ty\" => match *tok {\n+                TokenTree::Token(_, ref tok) => match *tok {\n                     OpenDelim(token::DelimToken::Brace) | OpenDelim(token::DelimToken::Bracket) |\n                     Comma | FatArrow | Colon | Eq | Gt | Semi | BinOp(token::Or) => Ok(true),\n-                    MatchNt(_, ref frag) if frag.name == \"block\" => Ok(true),\n                     Ident(i) if i.name == \"as\" || i.name == \"where\" => Ok(true),\n                     _ => Ok(false)\n-                }\n+                },\n+                TokenTree::MetaVarDecl(_, _, frag) if frag.name == \"block\" => Ok(true),\n+                _ => Ok(false),\n             },\n             \"ident\" => {\n                 // being a single token, idents are harmless\n@@ -780,6 +788,7 @@ fn is_in_follow(tok: &Token, frag: &str) -> Result<bool, (String, &'static str)>\n                 // harmless\n                 Ok(true)\n             },\n+            \"\" => Ok(true), // keywords::Invalid\n             _ => Err((format!(\"invalid fragment specifier `{}`\", frag),\n                      \"valid fragment specifiers are `ident`, `block`, \\\n                       `stmt`, `expr`, `pat`, `ty`, `path`, `meta`, `tt` \\\n@@ -788,9 +797,9 @@ fn is_in_follow(tok: &Token, frag: &str) -> Result<bool, (String, &'static str)>\n     }\n }\n \n-fn has_legal_fragment_specifier(tok: &Token) -> Result<(), String> {\n+fn has_legal_fragment_specifier(tok: &quoted::TokenTree) -> Result<(), String> {\n     debug!(\"has_legal_fragment_specifier({:?})\", tok);\n-    if let &MatchNt(_, ref frag_spec) = tok {\n+    if let quoted::TokenTree::MetaVarDecl(_, _, frag_spec) = *tok {\n         let s = &frag_spec.name.as_str();\n         if !is_legal_fragment_specifier(s) {\n             return Err(s.to_string());\n@@ -802,7 +811,15 @@ fn has_legal_fragment_specifier(tok: &Token) -> Result<(), String> {\n fn is_legal_fragment_specifier(frag: &str) -> bool {\n     match frag {\n         \"item\" | \"block\" | \"stmt\" | \"expr\" | \"pat\" |\n-        \"path\" | \"ty\" | \"ident\" | \"meta\" | \"tt\" => true,\n+        \"path\" | \"ty\" | \"ident\" | \"meta\" | \"tt\" | \"\" => true,\n         _ => false,\n     }\n }\n+\n+fn quoted_tt_to_string(tt: &quoted::TokenTree) -> String {\n+    match *tt {\n+        quoted::TokenTree::Token(_, ref tok) => ::print::pprust::token_to_string(tok),\n+        quoted::TokenTree::MetaVarDecl(_, name, kind) => format!(\"${}:{}\", name, kind),\n+        _ => panic!(\"unexpected quoted::TokenTree::{Sequence or Delimited} in follow set checker\"),\n+    }\n+}"}, {"sha": "530824b28348a175fcb7c07ea8af1bce7f8b4006", "filename": "src/libsyntax/ext/tt/quoted.rs", "status": "added", "additions": 234, "deletions": 0, "changes": 234, "blob_url": "https://github.com/rust-lang/rust/blob/4ba49ab39f942af3846cf0d8599674b0adde94b5/src%2Flibsyntax%2Fext%2Ftt%2Fquoted.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4ba49ab39f942af3846cf0d8599674b0adde94b5/src%2Flibsyntax%2Fext%2Ftt%2Fquoted.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fext%2Ftt%2Fquoted.rs?ref=4ba49ab39f942af3846cf0d8599674b0adde94b5", "patch": "@@ -0,0 +1,234 @@\n+// Copyright 2017 The Rust Project Developers. See the COPYRIGHT\n+// file at the top-level directory of this distribution and at\n+// http://rust-lang.org/COPYRIGHT.\n+//\n+// Licensed under the Apache License, Version 2.0 <LICENSE-APACHE or\n+// http://www.apache.org/licenses/LICENSE-2.0> or the MIT license\n+// <LICENSE-MIT or http://opensource.org/licenses/MIT>, at your\n+// option. This file may not be copied, modified, or distributed\n+// except according to those terms.\n+\n+use ast;\n+use ext::tt::macro_parser;\n+use parse::{ParseSess, token};\n+use print::pprust;\n+use symbol::{keywords, Symbol};\n+use syntax_pos::{DUMMY_SP, Span, BytePos};\n+use tokenstream;\n+\n+use std::rc::Rc;\n+\n+#[derive(Clone, PartialEq, Eq, RustcEncodable, RustcDecodable, Hash, Debug)]\n+pub struct Delimited {\n+    pub delim: token::DelimToken,\n+    pub tts: Vec<TokenTree>,\n+}\n+\n+impl Delimited {\n+    pub fn open_token(&self) -> token::Token {\n+        token::OpenDelim(self.delim)\n+    }\n+\n+    pub fn close_token(&self) -> token::Token {\n+        token::CloseDelim(self.delim)\n+    }\n+\n+    pub fn open_tt(&self, span: Span) -> TokenTree {\n+        let open_span = match span {\n+            DUMMY_SP => DUMMY_SP,\n+            _ => Span { hi: span.lo + BytePos(self.delim.len() as u32), ..span },\n+        };\n+        TokenTree::Token(open_span, self.open_token())\n+    }\n+\n+    pub fn close_tt(&self, span: Span) -> TokenTree {\n+        let close_span = match span {\n+            DUMMY_SP => DUMMY_SP,\n+            _ => Span { lo: span.hi - BytePos(self.delim.len() as u32), ..span },\n+        };\n+        TokenTree::Token(close_span, self.close_token())\n+    }\n+}\n+\n+#[derive(Clone, PartialEq, Eq, RustcEncodable, RustcDecodable, Hash, Debug)]\n+pub struct SequenceRepetition {\n+    /// The sequence of token trees\n+    pub tts: Vec<TokenTree>,\n+    /// The optional separator\n+    pub separator: Option<token::Token>,\n+    /// Whether the sequence can be repeated zero (*), or one or more times (+)\n+    pub op: KleeneOp,\n+    /// The number of `Match`s that appear in the sequence (and subsequences)\n+    pub num_captures: usize,\n+}\n+\n+/// A Kleene-style [repetition operator](http://en.wikipedia.org/wiki/Kleene_star)\n+/// for token sequences.\n+#[derive(Clone, PartialEq, Eq, RustcEncodable, RustcDecodable, Hash, Debug, Copy)]\n+pub enum KleeneOp {\n+    ZeroOrMore,\n+    OneOrMore,\n+}\n+\n+/// Similar to `tokenstream::TokenTree`, except that `$i`, `$i:ident`, and `$(...)`\n+/// are \"first-class\" token trees.\n+#[derive(Debug, Clone, PartialEq, Eq, RustcEncodable, RustcDecodable, Hash)]\n+pub enum TokenTree {\n+    Token(Span, token::Token),\n+    Delimited(Span, Rc<Delimited>),\n+    /// A kleene-style repetition sequence with a span\n+    Sequence(Span, Rc<SequenceRepetition>),\n+    /// Matches a nonterminal. This is only used in the left hand side of MBE macros.\n+    MetaVarDecl(Span, ast::Ident /* name to bind */, ast::Ident /* kind of nonterminal */),\n+}\n+\n+impl TokenTree {\n+    pub fn len(&self) -> usize {\n+        match *self {\n+            TokenTree::Delimited(_, ref delimed) => match delimed.delim {\n+                token::NoDelim => delimed.tts.len(),\n+                _ => delimed.tts.len() + 2,\n+            },\n+            TokenTree::Sequence(_, ref seq) => seq.tts.len(),\n+            _ => 0,\n+        }\n+    }\n+\n+    pub fn get_tt(&self, index: usize) -> TokenTree {\n+        match (self, index) {\n+            (&TokenTree::Delimited(_, ref delimed), _) if delimed.delim == token::NoDelim => {\n+                delimed.tts[index].clone()\n+            }\n+            (&TokenTree::Delimited(span, ref delimed), _) => {\n+                if index == 0 {\n+                    return delimed.open_tt(span);\n+                }\n+                if index == delimed.tts.len() + 1 {\n+                    return delimed.close_tt(span);\n+                }\n+                delimed.tts[index - 1].clone()\n+            }\n+            (&TokenTree::Sequence(_, ref seq), _) => seq.tts[index].clone(),\n+            _ => panic!(\"Cannot expand a token tree\"),\n+        }\n+    }\n+\n+    /// Retrieve the TokenTree's span.\n+    pub fn span(&self) -> Span {\n+        match *self {\n+            TokenTree::Token(sp, _) |\n+            TokenTree::MetaVarDecl(sp, _, _) |\n+            TokenTree::Delimited(sp, _) |\n+            TokenTree::Sequence(sp, _) => sp,\n+        }\n+    }\n+}\n+\n+pub fn parse(input: &[tokenstream::TokenTree], expect_matchers: bool, sess: &ParseSess)\n+             -> Vec<TokenTree> {\n+    let mut result = Vec::new();\n+    let mut trees = input.iter().cloned();\n+    while let Some(tree) = trees.next() {\n+        let tree = parse_tree(tree, &mut trees, expect_matchers, sess);\n+        match tree {\n+            TokenTree::Token(start_sp, token::SubstNt(ident)) if expect_matchers => {\n+                let span = match trees.next() {\n+                    Some(tokenstream::TokenTree::Token(span, token::Colon)) => match trees.next() {\n+                        Some(tokenstream::TokenTree::Token(end_sp, token::Ident(kind))) => {\n+                            let span = Span { lo: start_sp.lo, ..end_sp };\n+                            result.push(TokenTree::MetaVarDecl(span, ident, kind));\n+                            continue\n+                        }\n+                        tree @ _ => tree.as_ref().map(tokenstream::TokenTree::span).unwrap_or(span),\n+                    },\n+                    tree @ _ => tree.as_ref().map(tokenstream::TokenTree::span).unwrap_or(start_sp),\n+                };\n+                sess.missing_fragment_specifiers.borrow_mut().insert(span);\n+                result.push(TokenTree::MetaVarDecl(span, ident, keywords::Invalid.ident()));\n+            }\n+            _ => result.push(tree),\n+        }\n+    }\n+    result\n+}\n+\n+fn parse_tree<I>(tree: tokenstream::TokenTree,\n+                 trees: &mut I,\n+                 expect_matchers: bool,\n+                 sess: &ParseSess)\n+                 -> TokenTree\n+    where I: Iterator<Item = tokenstream::TokenTree>,\n+{\n+    match tree {\n+        tokenstream::TokenTree::Token(span, token::Dollar) => match trees.next() {\n+            Some(tokenstream::TokenTree::Delimited(span, ref delimited)) => {\n+                if delimited.delim != token::Paren {\n+                    let tok = pprust::token_to_string(&token::OpenDelim(delimited.delim));\n+                    let msg = format!(\"expected `(`, found `{}`\", tok);\n+                    sess.span_diagnostic.span_err(span, &msg);\n+                }\n+                let sequence = parse(&delimited.tts, expect_matchers, sess);\n+                let (separator, op) = parse_sep_and_kleene_op(trees, span, sess);\n+                let name_captures = macro_parser::count_names(&sequence);\n+                TokenTree::Sequence(span, Rc::new(SequenceRepetition {\n+                    tts: sequence,\n+                    separator: separator,\n+                    op: op,\n+                    num_captures: name_captures,\n+                }))\n+            }\n+            Some(tokenstream::TokenTree::Token(ident_span, token::Ident(ident))) => {\n+                let span = Span { lo: span.lo, ..ident_span };\n+                if ident.name == keywords::Crate.name() {\n+                    let ident = ast::Ident { name: Symbol::intern(\"$crate\"), ..ident };\n+                    TokenTree::Token(span, token::Ident(ident))\n+                } else {\n+                    TokenTree::Token(span, token::SubstNt(ident))\n+                }\n+            }\n+            Some(tokenstream::TokenTree::Token(span, tok)) => {\n+                let msg = format!(\"expected identifier, found `{}`\", pprust::token_to_string(&tok));\n+                sess.span_diagnostic.span_err(span, &msg);\n+                TokenTree::Token(span, token::SubstNt(keywords::Invalid.ident()))\n+            }\n+            None => TokenTree::Token(span, token::Dollar),\n+        },\n+        tokenstream::TokenTree::Token(span, tok) => TokenTree::Token(span, tok),\n+        tokenstream::TokenTree::Delimited(span, delimited) => {\n+            TokenTree::Delimited(span, Rc::new(Delimited {\n+                delim: delimited.delim,\n+                tts: parse(&delimited.tts, expect_matchers, sess),\n+            }))\n+        }\n+    }\n+}\n+\n+fn parse_sep_and_kleene_op<I>(input: &mut I, span: Span, sess: &ParseSess)\n+                              -> (Option<token::Token>, KleeneOp)\n+    where I: Iterator<Item = tokenstream::TokenTree>,\n+{\n+    fn kleene_op(token: &token::Token) -> Option<KleeneOp> {\n+        match *token {\n+            token::BinOp(token::Star) => Some(KleeneOp::ZeroOrMore),\n+            token::BinOp(token::Plus) => Some(KleeneOp::OneOrMore),\n+            _ => None,\n+        }\n+    }\n+\n+    let span = match input.next() {\n+        Some(tokenstream::TokenTree::Token(span, tok)) => match kleene_op(&tok) {\n+            Some(op) => return (None, op),\n+            None => match input.next() {\n+                Some(tokenstream::TokenTree::Token(span, tok2)) => match kleene_op(&tok2) {\n+                    Some(op) => return (Some(tok), op),\n+                    None => span,\n+                },\n+                tree @ _ => tree.as_ref().map(tokenstream::TokenTree::span).unwrap_or(span),\n+            }\n+        },\n+        tree @ _ => tree.as_ref().map(tokenstream::TokenTree::span).unwrap_or(span),\n+    };\n+\n+    sess.span_diagnostic.span_err(span, \"expected `*` or `+`\");\n+    (None, KleeneOp::ZeroOrMore)\n+}"}, {"sha": "90f64a5208f75cc006830fc74b5abf51a68f2dff", "filename": "src/libsyntax/ext/tt/transcribe.rs", "status": "modified", "additions": 180, "deletions": 199, "changes": 379, "blob_url": "https://github.com/rust-lang/rust/blob/4ba49ab39f942af3846cf0d8599674b0adde94b5/src%2Flibsyntax%2Fext%2Ftt%2Ftranscribe.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4ba49ab39f942af3846cf0d8599674b0adde94b5/src%2Flibsyntax%2Fext%2Ftt%2Ftranscribe.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fext%2Ftt%2Ftranscribe.rs?ref=4ba49ab39f942af3846cf0d8599674b0adde94b5", "patch": "@@ -7,268 +7,249 @@\n // <LICENSE-MIT or http://opensource.org/licenses/MIT>, at your\n // option. This file may not be copied, modified, or distributed\n // except according to those terms.\n-use self::LockstepIterSize::*;\n \n use ast::Ident;\n use errors::Handler;\n use ext::tt::macro_parser::{NamedMatch, MatchedSeq, MatchedNonterminal};\n-use parse::token::{self, MatchNt, SubstNt, Token, NtIdent, NtTT};\n+use ext::tt::quoted;\n+use parse::token::{self, SubstNt, Token, NtIdent, NtTT};\n use syntax_pos::{Span, DUMMY_SP};\n-use tokenstream::{self, TokenTree};\n+use tokenstream::{TokenTree, Delimited};\n use util::small_vector::SmallVector;\n \n use std::rc::Rc;\n+use std::mem;\n use std::ops::Add;\n use std::collections::HashMap;\n \n-///an unzipping of `TokenTree`s\n-#[derive(Clone)]\n-struct TtFrame {\n-    forest: TokenTree,\n-    idx: usize,\n-    dotdotdoted: bool,\n-    sep: Option<Token>,\n+// An iterator over the token trees in a delimited token tree (`{ ... }`) or a sequence (`$(...)`).\n+enum Frame {\n+    Delimited {\n+        forest: Rc<quoted::Delimited>,\n+        idx: usize,\n+        span: Span,\n+    },\n+    Sequence {\n+        forest: Rc<quoted::SequenceRepetition>,\n+        idx: usize,\n+        sep: Option<Token>,\n+    },\n }\n \n-#[derive(Clone)]\n-struct TtReader<'a> {\n-    sp_diag: &'a Handler,\n-    /// the unzipped tree:\n-    stack: SmallVector<TtFrame>,\n-    /* for MBE-style macro transcription */\n-    interpolations: HashMap<Ident, Rc<NamedMatch>>,\n+impl Frame {\n+    fn new(tts: Vec<quoted::TokenTree>) -> Frame {\n+        let forest = Rc::new(quoted::Delimited { delim: token::NoDelim, tts: tts });\n+        Frame::Delimited { forest: forest, idx: 0, span: DUMMY_SP }\n+    }\n+}\n+\n+impl Iterator for Frame {\n+    type Item = quoted::TokenTree;\n \n-    repeat_idx: Vec<usize>,\n-    repeat_len: Vec<usize>,\n+    fn next(&mut self) -> Option<quoted::TokenTree> {\n+        match *self {\n+            Frame::Delimited { ref forest, ref mut idx, .. } => {\n+                *idx += 1;\n+                forest.tts.get(*idx - 1).cloned()\n+            }\n+            Frame::Sequence { ref forest, ref mut idx, .. } => {\n+                *idx += 1;\n+                forest.tts.get(*idx - 1).cloned()\n+            }\n+        }\n+    }\n }\n \n /// This can do Macro-By-Example transcription. On the other hand, if\n-/// `src` contains no `TokenTree::Sequence`s, `MatchNt`s or `SubstNt`s, `interp` can\n+/// `src` contains no `TokenTree::{Sequence, Match}`s, or `SubstNt`s, `interp` can\n /// (and should) be None.\n pub fn transcribe(sp_diag: &Handler,\n                   interp: Option<HashMap<Ident, Rc<NamedMatch>>>,\n-                  src: Vec<tokenstream::TokenTree>)\n+                  src: Vec<quoted::TokenTree>)\n                   -> Vec<TokenTree> {\n-    let mut r = TtReader {\n-        sp_diag: sp_diag,\n-        stack: SmallVector::one(TtFrame {\n-            forest: TokenTree::Sequence(DUMMY_SP, Rc::new(tokenstream::SequenceRepetition {\n-                tts: src,\n-                // doesn't matter. This merely holds the root unzipping.\n-                separator: None, op: tokenstream::KleeneOp::ZeroOrMore, num_captures: 0\n-            })),\n-            idx: 0,\n-            dotdotdoted: false,\n-            sep: None,\n-        }),\n-        interpolations: match interp { /* just a convenience */\n-            None => HashMap::new(),\n-            Some(x) => x,\n-        },\n-        repeat_idx: Vec::new(),\n-        repeat_len: Vec::new(),\n-    };\n+    let mut stack = SmallVector::one(Frame::new(src));\n+    let interpolations = interp.unwrap_or_else(HashMap::new); /* just a convenience */\n+    let mut repeats = Vec::new();\n+    let mut result = Vec::new();\n+    let mut result_stack = Vec::new();\n \n-    let mut tts = Vec::new();\n-    let mut prev_span = DUMMY_SP;\n-    while let Some(tt) = tt_next_token(&mut r, prev_span) {\n-        prev_span = tt.span();\n-        tts.push(tt);\n-    }\n-    tts\n-}\n-\n-fn lookup_cur_matched_by_matched(r: &TtReader, start: Rc<NamedMatch>) -> Rc<NamedMatch> {\n-    r.repeat_idx.iter().fold(start, |ad, idx| {\n-        match *ad {\n-            MatchedNonterminal(_) => {\n-                // end of the line; duplicate henceforth\n-                ad.clone()\n+    loop {\n+        let tree = if let Some(tree) = stack.last_mut().unwrap().next() {\n+            tree\n+        } else {\n+            if let Frame::Sequence { ref mut idx, ref sep, .. } = *stack.last_mut().unwrap() {\n+                let (ref mut repeat_idx, repeat_len) = *repeats.last_mut().unwrap();\n+                *repeat_idx += 1;\n+                if *repeat_idx < repeat_len {\n+                    *idx = 0;\n+                    if let Some(sep) = sep.clone() {\n+                        // repeat same span, I guess\n+                        let prev_span = result.last().map(TokenTree::span).unwrap_or(DUMMY_SP);\n+                        result.push(TokenTree::Token(prev_span, sep));\n+                    }\n+                    continue\n+                }\n             }\n-            MatchedSeq(ref ads, _) => ads[*idx].clone()\n-        }\n-    })\n-}\n-\n-fn lookup_cur_matched(r: &TtReader, name: Ident) -> Option<Rc<NamedMatch>> {\n-    let matched_opt = r.interpolations.get(&name).cloned();\n-    matched_opt.map(|s| lookup_cur_matched_by_matched(r, s))\n-}\n-\n-#[derive(Clone)]\n-enum LockstepIterSize {\n-    LisUnconstrained,\n-    LisConstraint(usize, Ident),\n-    LisContradiction(String),\n-}\n-\n-impl Add for LockstepIterSize {\n-    type Output = LockstepIterSize;\n \n-    fn add(self, other: LockstepIterSize) -> LockstepIterSize {\n-        match self {\n-            LisUnconstrained => other,\n-            LisContradiction(_) => self,\n-            LisConstraint(l_len, ref l_id) => match other {\n-                LisUnconstrained => self.clone(),\n-                LisContradiction(_) => other,\n-                LisConstraint(r_len, _) if l_len == r_len => self.clone(),\n-                LisConstraint(r_len, r_id) => {\n-                    LisContradiction(format!(\"inconsistent lockstep iteration: \\\n-                                              '{}' has {} items, but '{}' has {}\",\n-                                              l_id, l_len, r_id, r_len))\n+            match stack.pop().unwrap() {\n+                Frame::Sequence { .. } => {\n+                    repeats.pop();\n+                }\n+                Frame::Delimited { forest, span, .. } => {\n+                    if result_stack.is_empty() {\n+                        return result;\n+                    }\n+                    let tree = TokenTree::Delimited(span, Rc::new(Delimited {\n+                        delim: forest.delim,\n+                        tts: result,\n+                    }));\n+                    result = result_stack.pop().unwrap();\n+                    result.push(tree);\n                 }\n-            },\n-        }\n-    }\n-}\n-\n-fn lockstep_iter_size(t: &TokenTree, r: &TtReader) -> LockstepIterSize {\n-    match *t {\n-        TokenTree::Delimited(_, ref delimed) => {\n-            delimed.tts.iter().fold(LisUnconstrained, |size, tt| {\n-                size + lockstep_iter_size(tt, r)\n-            })\n-        },\n-        TokenTree::Sequence(_, ref seq) => {\n-            seq.tts.iter().fold(LisUnconstrained, |size, tt| {\n-                size + lockstep_iter_size(tt, r)\n-            })\n-        },\n-        TokenTree::Token(_, SubstNt(name)) | TokenTree::Token(_, MatchNt(name, _)) =>\n-            match lookup_cur_matched(r, name) {\n-                Some(matched) => match *matched {\n-                    MatchedNonterminal(_) => LisUnconstrained,\n-                    MatchedSeq(ref ads, _) => LisConstraint(ads.len(), name),\n-                },\n-                _ => LisUnconstrained\n-            },\n-        TokenTree::Token(..) => LisUnconstrained,\n-    }\n-}\n-\n-/// Return the next token from the TtReader.\n-/// EFFECT: advances the reader's token field\n-fn tt_next_token(r: &mut TtReader, prev_span: Span) -> Option<TokenTree> {\n-    loop {\n-        let should_pop = if let Some(frame) = r.stack.last() {\n-            if frame.idx < frame.forest.len() {\n-                break;\n             }\n-            !frame.dotdotdoted || *r.repeat_idx.last().unwrap() == *r.repeat_len.last().unwrap() - 1\n-        } else {\n-            return None;\n+            continue\n         };\n \n-        /* done with this set; pop or repeat? */\n-        if should_pop {\n-            let prev = r.stack.pop().unwrap();\n-            if let Some(frame) = r.stack.last_mut() {\n-                frame.idx += 1;\n-            } else {\n-                return None;\n-            }\n-            if prev.dotdotdoted {\n-                r.repeat_idx.pop();\n-                r.repeat_len.pop();\n-            }\n-        } else { /* repeat */\n-            *r.repeat_idx.last_mut().unwrap() += 1;\n-            r.stack.last_mut().unwrap().idx = 0;\n-            if let Some(tk) = r.stack.last().unwrap().sep.clone() {\n-                return Some(TokenTree::Token(prev_span, tk)); // repeat same span, I guess\n-            }\n-        }\n-    }\n-    loop { /* because it's easiest, this handles `TokenTree::Delimited` not starting\n-              with a `TokenTree::Token`, even though it won't happen */\n-        let t = {\n-            let frame = r.stack.last().unwrap();\n-            // FIXME(pcwalton): Bad copy.\n-            frame.forest.get_tt(frame.idx)\n-        };\n-        match t {\n-            TokenTree::Sequence(sp, seq) => {\n+        match tree {\n+            quoted::TokenTree::Sequence(sp, seq) => {\n                 // FIXME(pcwalton): Bad copy.\n-                match lockstep_iter_size(&TokenTree::Sequence(sp, seq.clone()),\n-                                         r) {\n-                    LisUnconstrained => {\n-                        panic!(r.sp_diag.span_fatal(\n+                match lockstep_iter_size(&quoted::TokenTree::Sequence(sp, seq.clone()),\n+                                         &interpolations,\n+                                         &repeats) {\n+                    LockstepIterSize::Unconstrained => {\n+                        panic!(sp_diag.span_fatal(\n                             sp.clone(), /* blame macro writer */\n                             \"attempted to repeat an expression \\\n                              containing no syntax \\\n                              variables matched as repeating at this depth\"));\n                     }\n-                    LisContradiction(ref msg) => {\n+                    LockstepIterSize::Contradiction(ref msg) => {\n                         // FIXME #2887 blame macro invoker instead\n-                        panic!(r.sp_diag.span_fatal(sp.clone(), &msg[..]));\n+                        panic!(sp_diag.span_fatal(sp.clone(), &msg[..]));\n                     }\n-                    LisConstraint(len, _) => {\n+                    LockstepIterSize::Constraint(len, _) => {\n                         if len == 0 {\n-                            if seq.op == tokenstream::KleeneOp::OneOrMore {\n+                            if seq.op == quoted::KleeneOp::OneOrMore {\n                                 // FIXME #2887 blame invoker\n-                                panic!(r.sp_diag.span_fatal(sp.clone(),\n-                                                     \"this must repeat at least once\"));\n+                                panic!(sp_diag.span_fatal(sp.clone(),\n+                                                          \"this must repeat at least once\"));\n                             }\n-\n-                            r.stack.last_mut().unwrap().idx += 1;\n-                            return tt_next_token(r, prev_span);\n+                        } else {\n+                            repeats.push((0, len));\n+                            stack.push(Frame::Sequence {\n+                                idx: 0,\n+                                sep: seq.separator.clone(),\n+                                forest: seq,\n+                            });\n                         }\n-                        r.repeat_len.push(len);\n-                        r.repeat_idx.push(0);\n-                        r.stack.push(TtFrame {\n-                            idx: 0,\n-                            dotdotdoted: true,\n-                            sep: seq.separator.clone(),\n-                            forest: TokenTree::Sequence(sp, seq),\n-                        });\n                     }\n                 }\n             }\n             // FIXME #2887: think about span stuff here\n-            TokenTree::Token(sp, SubstNt(ident)) => {\n-                r.stack.last_mut().unwrap().idx += 1;\n-                match lookup_cur_matched(r, ident) {\n-                    None => {\n-                        return Some(TokenTree::Token(sp, SubstNt(ident)));\n-                        // this can't be 0 length, just like TokenTree::Delimited\n-                    }\n+            quoted::TokenTree::Token(sp, SubstNt(ident)) => {\n+                match lookup_cur_matched(ident, &interpolations, &repeats) {\n+                    None => result.push(TokenTree::Token(sp, SubstNt(ident))),\n                     Some(cur_matched) => if let MatchedNonterminal(ref nt) = *cur_matched {\n                         match **nt {\n                             // sidestep the interpolation tricks for ident because\n                             // (a) idents can be in lots of places, so it'd be a pain\n                             // (b) we actually can, since it's a token.\n                             NtIdent(ref sn) => {\n-                                return Some(TokenTree::Token(sn.span, token::Ident(sn.node)));\n+                                result.push(TokenTree::Token(sn.span, token::Ident(sn.node)));\n                             }\n-                            NtTT(ref tt) => return Some(tt.clone()),\n+                            NtTT(ref tt) => result.push(tt.clone()),\n                             _ => {\n                                 // FIXME(pcwalton): Bad copy\n-                                return Some(TokenTree::Token(sp, token::Interpolated(nt.clone())));\n+                                result.push(TokenTree::Token(sp, token::Interpolated(nt.clone())));\n                             }\n                         }\n                     } else {\n-                        panic!(r.sp_diag.span_fatal(\n+                        panic!(sp_diag.span_fatal(\n                             sp, /* blame the macro writer */\n                             &format!(\"variable '{}' is still repeating at this depth\", ident)));\n                     }\n                 }\n             }\n-            // TokenTree::Delimited or any token that can be unzipped\n-            seq @ TokenTree::Delimited(..) | seq @ TokenTree::Token(_, MatchNt(..)) => {\n-                // do not advance the idx yet\n-                r.stack.push(TtFrame {\n-                   forest: seq,\n-                   idx: 0,\n-                   dotdotdoted: false,\n-                   sep: None\n-                });\n-                // if this could be 0-length, we'd need to potentially recur here\n+            quoted::TokenTree::Delimited(span, delimited) => {\n+                stack.push(Frame::Delimited { forest: delimited, idx: 0, span: span });\n+                result_stack.push(mem::replace(&mut result, Vec::new()));\n             }\n-            tt @ TokenTree::Token(..) => {\n-                r.stack.last_mut().unwrap().idx += 1;\n-                return Some(tt);\n+            quoted::TokenTree::Token(span, tok) => result.push(TokenTree::Token(span, tok)),\n+            quoted::TokenTree::MetaVarDecl(..) => panic!(\"unexpected `TokenTree::MetaVarDecl\"),\n+        }\n+    }\n+}\n+\n+fn lookup_cur_matched(ident: Ident,\n+                      interpolations: &HashMap<Ident, Rc<NamedMatch>>,\n+                      repeats: &[(usize, usize)])\n+                      -> Option<Rc<NamedMatch>> {\n+    interpolations.get(&ident).map(|matched| {\n+        repeats.iter().fold(matched.clone(), |ad, &(idx, _)| {\n+            match *ad {\n+                MatchedNonterminal(_) => {\n+                    // end of the line; duplicate henceforth\n+                    ad.clone()\n+                }\n+                MatchedSeq(ref ads, _) => ads[idx].clone()\n             }\n+        })\n+    })\n+}\n+\n+#[derive(Clone)]\n+enum LockstepIterSize {\n+    Unconstrained,\n+    Constraint(usize, Ident),\n+    Contradiction(String),\n+}\n+\n+impl Add for LockstepIterSize {\n+    type Output = LockstepIterSize;\n+\n+    fn add(self, other: LockstepIterSize) -> LockstepIterSize {\n+        match self {\n+            LockstepIterSize::Unconstrained => other,\n+            LockstepIterSize::Contradiction(_) => self,\n+            LockstepIterSize::Constraint(l_len, ref l_id) => match other {\n+                LockstepIterSize::Unconstrained => self.clone(),\n+                LockstepIterSize::Contradiction(_) => other,\n+                LockstepIterSize::Constraint(r_len, _) if l_len == r_len => self.clone(),\n+                LockstepIterSize::Constraint(r_len, r_id) => {\n+                    let msg = format!(\"inconsistent lockstep iteration: \\\n+                                       '{}' has {} items, but '{}' has {}\",\n+                                      l_id, l_len, r_id, r_len);\n+                    LockstepIterSize::Contradiction(msg)\n+                }\n+            },\n         }\n     }\n }\n+\n+fn lockstep_iter_size(tree: &quoted::TokenTree,\n+                      interpolations: &HashMap<Ident, Rc<NamedMatch>>,\n+                      repeats: &[(usize, usize)])\n+                      -> LockstepIterSize {\n+    use self::quoted::TokenTree;\n+    match *tree {\n+        TokenTree::Delimited(_, ref delimed) => {\n+            delimed.tts.iter().fold(LockstepIterSize::Unconstrained, |size, tt| {\n+                size + lockstep_iter_size(tt, interpolations, repeats)\n+            })\n+        },\n+        TokenTree::Sequence(_, ref seq) => {\n+            seq.tts.iter().fold(LockstepIterSize::Unconstrained, |size, tt| {\n+                size + lockstep_iter_size(tt, interpolations, repeats)\n+            })\n+        },\n+        TokenTree::Token(_, SubstNt(name)) | TokenTree::MetaVarDecl(_, name, _) =>\n+            match lookup_cur_matched(name, interpolations, repeats) {\n+                Some(matched) => match *matched {\n+                    MatchedNonterminal(_) => LockstepIterSize::Unconstrained,\n+                    MatchedSeq(ref ads, _) => LockstepIterSize::Constraint(ads.len(), name),\n+                },\n+                _ => LockstepIterSize::Unconstrained\n+            },\n+        TokenTree::Token(..) => LockstepIterSize::Unconstrained,\n+    }\n+}"}, {"sha": "257b7efba5c8e8d2f49cf96eef59d93a56a125f1", "filename": "src/libsyntax/fold.rs", "status": "modified", "additions": 0, "deletions": 8, "changes": 8, "blob_url": "https://github.com/rust-lang/rust/blob/4ba49ab39f942af3846cf0d8599674b0adde94b5/src%2Flibsyntax%2Ffold.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4ba49ab39f942af3846cf0d8599674b0adde94b5/src%2Flibsyntax%2Ffold.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Ffold.rs?ref=4ba49ab39f942af3846cf0d8599674b0adde94b5", "patch": "@@ -551,13 +551,6 @@ pub fn noop_fold_tt<T: Folder>(tt: &TokenTree, fld: &mut T) -> TokenTree {\n                             }\n                         ))\n         },\n-        TokenTree::Sequence(span, ref seq) =>\n-            TokenTree::Sequence(fld.new_span(span),\n-                       Rc::new(SequenceRepetition {\n-                           tts: fld.fold_tts(&seq.tts),\n-                           separator: seq.separator.clone().map(|tok| fld.fold_token(tok)),\n-                           ..**seq\n-                       })),\n     }\n }\n \n@@ -578,7 +571,6 @@ pub fn noop_fold_token<T: Folder>(t: token::Token, fld: &mut T) -> token::Token\n             token::Interpolated(Rc::new(fld.fold_interpolated(nt)))\n         }\n         token::SubstNt(ident) => token::SubstNt(fld.fold_ident(ident)),\n-        token::MatchNt(name, kind) => token::MatchNt(fld.fold_ident(name), fld.fold_ident(kind)),\n         _ => t\n     }\n }"}, {"sha": "39a9aff48bf27163d0b8dcea76916863b54bd7de", "filename": "src/libsyntax/lib.rs", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "blob_url": "https://github.com/rust-lang/rust/blob/4ba49ab39f942af3846cf0d8599674b0adde94b5/src%2Flibsyntax%2Flib.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4ba49ab39f942af3846cf0d8599674b0adde94b5/src%2Flibsyntax%2Flib.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Flib.rs?ref=4ba49ab39f942af3846cf0d8599674b0adde94b5", "patch": "@@ -139,6 +139,7 @@ pub mod ext {\n         pub mod transcribe;\n         pub mod macro_parser;\n         pub mod macro_rules;\n+        pub mod quoted;\n     }\n }\n "}, {"sha": "de8a87e3a2b3293c9531d75b6dc53b7fe1cf3833", "filename": "src/libsyntax/parse/lexer/mod.rs", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/4ba49ab39f942af3846cf0d8599674b0adde94b5/src%2Flibsyntax%2Fparse%2Flexer%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4ba49ab39f942af3846cf0d8599674b0adde94b5/src%2Flibsyntax%2Fparse%2Flexer%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Flexer%2Fmod.rs?ref=4ba49ab39f942af3846cf0d8599674b0adde94b5", "patch": "@@ -1693,6 +1693,7 @@ mod tests {\n     use feature_gate::UnstableFeatures;\n     use parse::token;\n     use std::cell::RefCell;\n+    use std::collections::HashSet;\n     use std::io;\n     use std::rc::Rc;\n \n@@ -1704,6 +1705,7 @@ mod tests {\n             config: CrateConfig::new(),\n             included_mod_stack: RefCell::new(Vec::new()),\n             code_map: cm,\n+            missing_fragment_specifiers: RefCell::new(HashSet::new()),\n         }\n     }\n "}, {"sha": "6fec49b229abeeb315d37c608d7c033381619f7a", "filename": "src/libsyntax/parse/mod.rs", "status": "modified", "additions": 5, "deletions": 7, "changes": 12, "blob_url": "https://github.com/rust-lang/rust/blob/4ba49ab39f942af3846cf0d8599674b0adde94b5/src%2Flibsyntax%2Fparse%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4ba49ab39f942af3846cf0d8599674b0adde94b5/src%2Flibsyntax%2Fparse%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Fmod.rs?ref=4ba49ab39f942af3846cf0d8599674b0adde94b5", "patch": "@@ -46,6 +46,7 @@ pub struct ParseSess {\n     pub span_diagnostic: Handler,\n     pub unstable_features: UnstableFeatures,\n     pub config: CrateConfig,\n+    pub missing_fragment_specifiers: RefCell<HashSet<Span>>,\n     /// Used to determine and report recursive mod inclusions\n     included_mod_stack: RefCell<Vec<PathBuf>>,\n     code_map: Rc<CodeMap>,\n@@ -66,6 +67,7 @@ impl ParseSess {\n             span_diagnostic: handler,\n             unstable_features: UnstableFeatures::from_environment(),\n             config: HashSet::new(),\n+            missing_fragment_specifiers: RefCell::new(HashSet::new()),\n             included_mod_stack: RefCell::new(vec![]),\n             code_map: code_map\n         }\n@@ -139,13 +141,9 @@ pub fn parse_stmt_from_source_str<'a>(name: String, source: String, sess: &'a Pa\n     new_parser_from_source_str(sess, name, source).parse_stmt()\n }\n \n-// Warning: This parses with quote_depth > 0, which is not the default.\n pub fn parse_tts_from_source_str<'a>(name: String, source: String, sess: &'a ParseSess)\n-                                     -> PResult<'a, Vec<tokenstream::TokenTree>> {\n-    let mut p = new_parser_from_source_str(sess, name, source);\n-    p.quote_depth += 1;\n-    // right now this is re-creating the token trees from ... token trees.\n-    p.parse_all_token_trees()\n+                                     -> Vec<tokenstream::TokenTree> {\n+    filemap_to_tts(sess, sess.codemap().new_filemap(name, None, source))\n }\n \n // Create a new parser from a source string\n@@ -986,7 +984,7 @@ mod tests {\n             _ => panic!(\"not a macro\"),\n         };\n \n-        let span = tts.iter().rev().next().unwrap().get_span();\n+        let span = tts.iter().rev().next().unwrap().span();\n \n         match sess.codemap().span_to_snippet(span) {\n             Ok(s) => assert_eq!(&s[..], \"{ body }\"),"}, {"sha": "71274c4fdaa4ead88e9512baac8a1ec75d7b065c", "filename": "src/libsyntax/parse/parser.rs", "status": "modified", "additions": 26, "deletions": 184, "changes": 210, "blob_url": "https://github.com/rust-lang/rust/blob/4ba49ab39f942af3846cf0d8599674b0adde94b5/src%2Flibsyntax%2Fparse%2Fparser.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4ba49ab39f942af3846cf0d8599674b0adde94b5/src%2Flibsyntax%2Fparse%2Fparser.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Fparser.rs?ref=4ba49ab39f942af3846cf0d8599674b0adde94b5", "patch": "@@ -43,19 +43,16 @@ use {ast, attr};\n use codemap::{self, CodeMap, Spanned, spanned, respan};\n use syntax_pos::{self, Span, Pos, BytePos, mk_sp};\n use errors::{self, DiagnosticBuilder};\n-use ext::tt::macro_parser;\n-use parse;\n-use parse::classify;\n+use parse::{self, classify, token};\n use parse::common::SeqSep;\n use parse::lexer::TokenAndSpan;\n use parse::obsolete::ObsoleteSyntax;\n-use parse::token::{self, MatchNt, SubstNt};\n use parse::{new_sub_parser_from_file, ParseSess, Directory, DirectoryOwnership};\n use util::parser::{AssocOp, Fixity};\n use print::pprust;\n use ptr::P;\n use parse::PResult;\n-use tokenstream::{self, Delimited, SequenceRepetition, TokenTree};\n+use tokenstream::{Delimited, TokenTree};\n use symbol::{Symbol, keywords};\n use util::ThinVec;\n \n@@ -168,8 +165,6 @@ pub struct Parser<'a> {\n     /// the previous token kind\n     prev_token_kind: PrevTokenKind,\n     pub restrictions: Restrictions,\n-    pub quote_depth: usize, // not (yet) related to the quasiquoter\n-    parsing_token_tree: bool,\n     /// The set of seen errors about obsolete syntax. Used to suppress\n     /// extra detail when the same error is seen twice\n     pub obsolete_set: HashSet<ObsoleteSyntax>,\n@@ -329,8 +324,6 @@ impl<'a> Parser<'a> {\n             prev_span: syntax_pos::DUMMY_SP,\n             prev_token_kind: PrevTokenKind::Other,\n             restrictions: Restrictions::empty(),\n-            quote_depth: 0,\n-            parsing_token_tree: false,\n             obsolete_set: HashSet::new(),\n             directory: Directory { path: PathBuf::new(), ownership: DirectoryOwnership::Owned },\n             root_module_name: None,\n@@ -359,20 +352,11 @@ impl<'a> Parser<'a> {\n                 if i + 1 < tts.len() {\n                     self.tts.push((tts, i + 1));\n                 }\n-                // FIXME(jseyfried): remove after fixing #39390 in #39419.\n-                if self.quote_depth > 0 {\n-                    if let TokenTree::Sequence(sp, _) = tt {\n-                        self.span_err(sp, \"attempted to repeat an expression containing no \\\n-                                           syntax variables matched as repeating at this depth\");\n-                    }\n-                }\n-                match tt {\n-                    TokenTree::Token(sp, tok) => TokenAndSpan { tok: tok, sp: sp },\n-                    _ if tt.len() > 0 => {\n-                        self.tts.push((tt, 0));\n-                        continue\n-                    }\n-                    _ => continue,\n+                if let TokenTree::Token(sp, tok) = tt {\n+                    TokenAndSpan { tok: tok, sp: sp }\n+                } else {\n+                    self.tts.push((tt, 0));\n+                    continue\n                 }\n             } else {\n                 TokenAndSpan { tok: token::Eof, sp: self.span }\n@@ -997,7 +981,6 @@ impl<'a> Parser<'a> {\n                 tok = match tts.get_tt(i) {\n                     TokenTree::Token(_, tok) => tok,\n                     TokenTree::Delimited(_, delimited) => token::OpenDelim(delimited.delim),\n-                    TokenTree::Sequence(..) => token::Dollar,\n                 };\n             }\n         }\n@@ -1187,10 +1170,7 @@ impl<'a> Parser<'a> {\n             self.expect(&token::Not)?;\n \n             // eat a matched-delimiter token tree:\n-            let delim = self.expect_open_delim()?;\n-            let tts = self.parse_seq_to_end(&token::CloseDelim(delim),\n-                                            SeqSep::none(),\n-                                            |pp| pp.parse_token_tree())?;\n+            let (delim, tts) = self.expect_delimited_token_tree()?;\n             if delim != token::Brace {\n                 self.expect(&token::Semi)?\n             }\n@@ -1448,10 +1428,7 @@ impl<'a> Parser<'a> {\n             let path = self.parse_path(PathStyle::Type)?;\n             if self.eat(&token::Not) {\n                 // MACRO INVOCATION\n-                let delim = self.expect_open_delim()?;\n-                let tts = self.parse_seq_to_end(&token::CloseDelim(delim),\n-                                                SeqSep::none(),\n-                                                |p| p.parse_token_tree())?;\n+                let (_, tts) = self.expect_delimited_token_tree()?;\n                 let hi = self.span.hi;\n                 TyKind::Mac(spanned(lo, hi, Mac_ { path: path, tts: tts }))\n             } else {\n@@ -2045,13 +2022,12 @@ impl<'a> Parser<'a> {\n         })\n     }\n \n-    fn expect_open_delim(&mut self) -> PResult<'a, token::DelimToken> {\n-        self.expected_tokens.push(TokenType::Token(token::Gt));\n+    fn expect_delimited_token_tree(&mut self) -> PResult<'a, (token::DelimToken, Vec<TokenTree>)> {\n         match self.token {\n-            token::OpenDelim(delim) => {\n-                self.bump();\n-                Ok(delim)\n-            },\n+            token::OpenDelim(delim) => self.parse_token_tree().map(|tree| match tree {\n+                TokenTree::Delimited(_, delimited) => (delim, delimited.tts.clone()),\n+                _ => unreachable!(),\n+            }),\n             _ => Err(self.fatal(\"expected open delimiter\")),\n         }\n     }\n@@ -2261,10 +2237,7 @@ impl<'a> Parser<'a> {\n                     // `!`, as an operator, is prefix, so we know this isn't that\n                     if self.eat(&token::Not) {\n                         // MACRO INVOCATION expression\n-                        let delim = self.expect_open_delim()?;\n-                        let tts = self.parse_seq_to_end(&token::CloseDelim(delim),\n-                                                        SeqSep::none(),\n-                                                        |p| p.parse_token_tree())?;\n+                        let (_, tts) = self.expect_delimited_token_tree()?;\n                         let hi = self.prev_span.hi;\n                         return Ok(self.mk_mac_expr(lo, hi, Mac_ { path: pth, tts: tts }, attrs));\n                     }\n@@ -2586,139 +2559,22 @@ impl<'a> Parser<'a> {\n         return Ok(e);\n     }\n \n-    // Parse unquoted tokens after a `$` in a token tree\n-    fn parse_unquoted(&mut self) -> PResult<'a, TokenTree> {\n-        let mut sp = self.span;\n-        let name = match self.token {\n-            token::Dollar => {\n-                self.bump();\n-\n-                if self.token == token::OpenDelim(token::Paren) {\n-                    let Spanned { node: seq, span: seq_span } = self.parse_seq(\n-                        &token::OpenDelim(token::Paren),\n-                        &token::CloseDelim(token::Paren),\n-                        SeqSep::none(),\n-                        |p| p.parse_token_tree()\n-                    )?;\n-                    let (sep, repeat) = self.parse_sep_and_kleene_op()?;\n-                    let name_num = macro_parser::count_names(&seq);\n-                    return Ok(TokenTree::Sequence(mk_sp(sp.lo, seq_span.hi),\n-                                      Rc::new(SequenceRepetition {\n-                                          tts: seq,\n-                                          separator: sep,\n-                                          op: repeat,\n-                                          num_captures: name_num\n-                                      })));\n-                } else if self.token.is_keyword(keywords::Crate) {\n-                    let ident = match self.token {\n-                        token::Ident(id) => ast::Ident { name: Symbol::intern(\"$crate\"), ..id },\n-                        _ => unreachable!(),\n-                    };\n-                    self.bump();\n-                    return Ok(TokenTree::Token(sp, token::Ident(ident)));\n-                } else {\n-                    sp = mk_sp(sp.lo, self.span.hi);\n-                    self.parse_ident().unwrap_or_else(|mut e| {\n-                        e.emit();\n-                        keywords::Invalid.ident()\n-                    })\n-                }\n-            }\n-            token::SubstNt(name) => {\n-                self.bump();\n-                name\n-            }\n-            _ => unreachable!()\n-        };\n-        // continue by trying to parse the `:ident` after `$name`\n-        if self.token == token::Colon &&\n-                self.look_ahead(1, |t| t.is_ident() && !t.is_any_keyword()) {\n-            self.bump();\n-            sp = mk_sp(sp.lo, self.span.hi);\n-            let nt_kind = self.parse_ident()?;\n-            Ok(TokenTree::Token(sp, MatchNt(name, nt_kind)))\n-        } else {\n-            Ok(TokenTree::Token(sp, SubstNt(name)))\n-        }\n-    }\n-\n     pub fn check_unknown_macro_variable(&mut self) {\n-        if self.quote_depth == 0 && !self.parsing_token_tree {\n-            match self.token {\n-                token::SubstNt(name) =>\n-                    self.fatal(&format!(\"unknown macro variable `{}`\", name)).emit(),\n-                _ => {}\n-            }\n-        }\n-    }\n-\n-    /// Parse an optional separator followed by a Kleene-style\n-    /// repetition token (+ or *).\n-    pub fn parse_sep_and_kleene_op(&mut self)\n-                                   -> PResult<'a, (Option<token::Token>, tokenstream::KleeneOp)> {\n-        fn parse_kleene_op<'a>(parser: &mut Parser<'a>) ->\n-          PResult<'a,  Option<tokenstream::KleeneOp>> {\n-            match parser.token {\n-                token::BinOp(token::Star) => {\n-                    parser.bump();\n-                    Ok(Some(tokenstream::KleeneOp::ZeroOrMore))\n-                },\n-                token::BinOp(token::Plus) => {\n-                    parser.bump();\n-                    Ok(Some(tokenstream::KleeneOp::OneOrMore))\n-                },\n-                _ => Ok(None)\n-            }\n-        };\n-\n-        if let Some(kleene_op) = parse_kleene_op(self)? {\n-            return Ok((None, kleene_op));\n-        }\n-\n-        let separator = match self.token {\n-            token::CloseDelim(..) => None,\n-            _ => Some(self.bump_and_get()),\n-        };\n-        match parse_kleene_op(self)? {\n-            Some(zerok) => Ok((separator, zerok)),\n-            None => return Err(self.fatal(\"expected `*` or `+`\"))\n+        if let token::SubstNt(name) = self.token {\n+            self.fatal(&format!(\"unknown macro variable `{}`\", name)).emit()\n         }\n     }\n \n     /// parse a single token tree from the input.\n     pub fn parse_token_tree(&mut self) -> PResult<'a, TokenTree> {\n-        // FIXME #6994: currently, this is too eager. It\n-        // parses token trees but also identifies TokenType::Sequence's\n-        // and token::SubstNt's; it's too early to know yet\n-        // whether something will be a nonterminal or a seq\n-        // yet.\n         match self.token {\n-            token::OpenDelim(delim) => {\n-                if self.quote_depth == 0 && self.tts.last().map(|&(_, i)| i == 1).unwrap_or(false) {\n-                    let tt = self.tts.pop().unwrap().0;\n-                    self.bump();\n-                    return Ok(tt);\n-                }\n-\n-                let parsing_token_tree = ::std::mem::replace(&mut self.parsing_token_tree, true);\n-                let lo = self.span.lo;\n-                self.bump();\n-                let tts = self.parse_seq_to_before_tokens(&[&token::CloseDelim(token::Brace),\n-                                                            &token::CloseDelim(token::Paren),\n-                                                            &token::CloseDelim(token::Bracket)],\n-                                                          SeqSep::none(),\n-                                                          |p| p.parse_token_tree(),\n-                                                          |mut e| e.emit());\n-                self.parsing_token_tree = parsing_token_tree;\n+            token::OpenDelim(..) => {\n+                let tt = self.tts.pop().unwrap().0;\n+                self.span = tt.span();\n                 self.bump();\n-\n-                Ok(TokenTree::Delimited(Span { lo: lo, ..self.prev_span }, Rc::new(Delimited {\n-                    delim: delim,\n-                    tts: tts,\n-                })))\n+                return Ok(tt);\n             },\n-            token::CloseDelim(..) | token::Eof => Ok(TokenTree::Token(self.span, token::Eof)),\n-            token::Dollar | token::SubstNt(..) if self.quote_depth > 0 => self.parse_unquoted(),\n+            token::CloseDelim(_) | token::Eof => unreachable!(),\n             _ => Ok(TokenTree::Token(self.span, self.bump_and_get())),\n         }\n     }\n@@ -3528,10 +3384,7 @@ impl<'a> Parser<'a> {\n                     token::Not if qself.is_none() => {\n                         // Parse macro invocation\n                         self.bump();\n-                        let delim = self.expect_open_delim()?;\n-                        let tts = self.parse_seq_to_end(&token::CloseDelim(delim),\n-                                                        SeqSep::none(),\n-                                                        |p| p.parse_token_tree())?;\n+                        let (_, tts) = self.expect_delimited_token_tree()?;\n                         let mac = spanned(lo, self.prev_span.hi, Mac_ { path: path, tts: tts });\n                         pat = PatKind::Mac(mac);\n                     }\n@@ -3831,12 +3684,7 @@ impl<'a> Parser<'a> {\n                 },\n             };\n \n-            let tts = self.parse_unspanned_seq(\n-                &token::OpenDelim(delim),\n-                &token::CloseDelim(delim),\n-                SeqSep::none(),\n-                |p| p.parse_token_tree()\n-            )?;\n+            let (_, tts) = self.expect_delimited_token_tree()?;\n             let hi = self.prev_span.hi;\n \n             let style = if delim == token::Brace {\n@@ -4744,10 +4592,7 @@ impl<'a> Parser<'a> {\n             self.expect(&token::Not)?;\n \n             // eat a matched-delimiter token tree:\n-            let delim = self.expect_open_delim()?;\n-            let tts = self.parse_seq_to_end(&token::CloseDelim(delim),\n-                                            SeqSep::none(),\n-                                            |p| p.parse_token_tree())?;\n+            let (delim, tts) = self.expect_delimited_token_tree()?;\n             if delim != token::Brace {\n                 self.expect(&token::Semi)?\n             }\n@@ -5893,10 +5738,7 @@ impl<'a> Parser<'a> {\n                 keywords::Invalid.ident() // no special identifier\n             };\n             // eat a matched-delimiter token tree:\n-            let delim = self.expect_open_delim()?;\n-            let tts = self.parse_seq_to_end(&token::CloseDelim(delim),\n-                                            SeqSep::none(),\n-                                            |p| p.parse_token_tree())?;\n+            let (delim, tts) = self.expect_delimited_token_tree()?;\n             if delim != token::Brace {\n                 if !self.eat(&token::Semi) {\n                     let prev_span = self.prev_span;"}, {"sha": "5b65aac92b81c2d28ad6efe19d6ba38e1d0c59bc", "filename": "src/libsyntax/parse/token.rs", "status": "modified", "additions": 2, "deletions": 5, "changes": 7, "blob_url": "https://github.com/rust-lang/rust/blob/4ba49ab39f942af3846cf0d8599674b0adde94b5/src%2Flibsyntax%2Fparse%2Ftoken.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4ba49ab39f942af3846cf0d8599674b0adde94b5/src%2Flibsyntax%2Fparse%2Ftoken.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Ftoken.rs?ref=4ba49ab39f942af3846cf0d8599674b0adde94b5", "patch": "@@ -50,8 +50,8 @@ pub enum DelimToken {\n }\n \n impl DelimToken {\n-    pub fn len(&self) -> u32 {\n-        if *self == NoDelim { 0 } else { 1 }\n+    pub fn len(self) -> usize {\n+        if self == NoDelim { 0 } else { 1 }\n     }\n }\n \n@@ -152,9 +152,6 @@ pub enum Token {\n     // Can be expanded into several tokens.\n     /// Doc comment\n     DocComment(ast::Name),\n-    // In left-hand-sides of MBE macros:\n-    /// Parse a nonterminal (name to bind, name of NT)\n-    MatchNt(ast::Ident, ast::Ident),\n     // In right-hand-sides of MBE macros:\n     /// A syntactic variable that will be filled in by macro expansion.\n     SubstNt(ast::Ident),"}, {"sha": "ec962d03458d1753c905e7cffd90269efb092f7d", "filename": "src/libsyntax/print/pprust.rs", "status": "modified", "additions": 0, "deletions": 15, "changes": 15, "blob_url": "https://github.com/rust-lang/rust/blob/4ba49ab39f942af3846cf0d8599674b0adde94b5/src%2Flibsyntax%2Fprint%2Fpprust.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4ba49ab39f942af3846cf0d8599674b0adde94b5/src%2Flibsyntax%2Fprint%2Fpprust.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fprint%2Fpprust.rs?ref=4ba49ab39f942af3846cf0d8599674b0adde94b5", "patch": "@@ -271,7 +271,6 @@ pub fn token_to_string(tok: &Token) -> String {\n         /* Other */\n         token::DocComment(s)        => s.to_string(),\n         token::SubstNt(s)           => format!(\"${}\", s),\n-        token::MatchNt(s, t)        => format!(\"${}:{}\", s, t),\n         token::Eof                  => \"<eof>\".to_string(),\n         token::Whitespace           => \" \".to_string(),\n         token::Comment              => \"/* */\".to_string(),\n@@ -1475,20 +1474,6 @@ impl<'a> State<'a> {\n                 space(&mut self.s)?;\n                 word(&mut self.s, &token_to_string(&delimed.close_token()))\n             },\n-            TokenTree::Sequence(_, ref seq) => {\n-                word(&mut self.s, \"$(\")?;\n-                for tt_elt in &seq.tts {\n-                    self.print_tt(tt_elt)?;\n-                }\n-                word(&mut self.s, \")\")?;\n-                if let Some(ref tk) = seq.separator {\n-                    word(&mut self.s, &token_to_string(tk))?;\n-                }\n-                match seq.op {\n-                    tokenstream::KleeneOp::ZeroOrMore => word(&mut self.s, \"*\"),\n-                    tokenstream::KleeneOp::OneOrMore => word(&mut self.s, \"+\"),\n-                }\n-            }\n         }\n     }\n "}, {"sha": "666540467213326d13bc18a0ac7a2691793897e6", "filename": "src/libsyntax/tokenstream.rs", "status": "modified", "additions": 7, "deletions": 59, "changes": 66, "blob_url": "https://github.com/rust-lang/rust/blob/4ba49ab39f942af3846cf0d8599674b0adde94b5/src%2Flibsyntax%2Ftokenstream.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4ba49ab39f942af3846cf0d8599674b0adde94b5/src%2Flibsyntax%2Ftokenstream.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Ftokenstream.rs?ref=4ba49ab39f942af3846cf0d8599674b0adde94b5", "patch": "@@ -12,9 +12,7 @@\n //!\n //! TokenStreams represent syntactic objects before they are converted into ASTs.\n //! A `TokenStream` is, roughly speaking, a sequence (eg stream) of `TokenTree`s,\n-//! which are themselves either a single Token, a Delimited subsequence of tokens,\n-//! or a SequenceRepetition specifier (for the purpose of sequence generation during macro\n-//! expansion).\n+//! which are themselves a single `Token` or a `Delimited` subsequence of tokens.\n //!\n //! ## Ownership\n //! TokenStreams are persistent data structures constructed as ropes with reference\n@@ -28,10 +26,10 @@ use ast::{self, AttrStyle, LitKind};\n use syntax_pos::{BytePos, Span, DUMMY_SP};\n use codemap::Spanned;\n use ext::base;\n-use ext::tt::macro_parser;\n+use ext::tt::{macro_parser, quoted};\n use parse::lexer::comments::{doc_comment_style, strip_doc_comment_decoration};\n use parse::{self, Directory};\n-use parse::token::{self, Token, Lit, Nonterminal};\n+use parse::token::{self, Token, Lit};\n use print::pprust;\n use serialize::{Decoder, Decodable, Encoder, Encodable};\n use symbol::Symbol;\n@@ -64,7 +62,7 @@ impl Delimited {\n     pub fn open_tt(&self, span: Span) -> TokenTree {\n         let open_span = match span {\n             DUMMY_SP => DUMMY_SP,\n-            _ => Span { hi: span.lo + BytePos(self.delim.len()), ..span },\n+            _ => Span { hi: span.lo + BytePos(self.delim.len() as u32), ..span },\n         };\n         TokenTree::Token(open_span, self.open_token())\n     }\n@@ -73,7 +71,7 @@ impl Delimited {\n     pub fn close_tt(&self, span: Span) -> TokenTree {\n         let close_span = match span {\n             DUMMY_SP => DUMMY_SP,\n-            _ => Span { lo: span.hi - BytePos(self.delim.len()), ..span },\n+            _ => Span { lo: span.hi - BytePos(self.delim.len() as u32), ..span },\n         };\n         TokenTree::Token(close_span, self.close_token())\n     }\n@@ -84,27 +82,6 @@ impl Delimited {\n     }\n }\n \n-/// A sequence of token trees\n-#[derive(Clone, PartialEq, Eq, RustcEncodable, RustcDecodable, Hash, Debug)]\n-pub struct SequenceRepetition {\n-    /// The sequence of token trees\n-    pub tts: Vec<TokenTree>,\n-    /// The optional separator\n-    pub separator: Option<token::Token>,\n-    /// Whether the sequence can be repeated zero (*), or one or more times (+)\n-    pub op: KleeneOp,\n-    /// The number of `MatchNt`s that appear in the sequence (and subsequences)\n-    pub num_captures: usize,\n-}\n-\n-/// A Kleene-style [repetition operator](http://en.wikipedia.org/wiki/Kleene_star)\n-/// for token sequences.\n-#[derive(Clone, PartialEq, Eq, RustcEncodable, RustcDecodable, Hash, Debug, Copy)]\n-pub enum KleeneOp {\n-    ZeroOrMore,\n-    OneOrMore,\n-}\n-\n /// When the main rust parser encounters a syntax-extension invocation, it\n /// parses the arguments to the invocation as a token-tree. This is a very\n /// loose structure, such that all sorts of different AST-fragments can\n@@ -123,10 +100,6 @@ pub enum TokenTree {\n     Token(Span, token::Token),\n     /// A delimited sequence of token trees\n     Delimited(Span, Rc<Delimited>),\n-\n-    // This only makes sense in MBE macros.\n-    /// A kleene-style repetition sequence with a span\n-    Sequence(Span, Rc<SequenceRepetition>),\n }\n \n impl TokenTree {\n@@ -138,15 +111,10 @@ impl TokenTree {\n                     AttrStyle::Inner => 3,\n                 }\n             }\n-            TokenTree::Token(_, token::Interpolated(ref nt)) => {\n-                if let Nonterminal::NtTT(..) = **nt { 1 } else { 0 }\n-            },\n-            TokenTree::Token(_, token::MatchNt(..)) => 3,\n             TokenTree::Delimited(_, ref delimed) => match delimed.delim {\n                 token::NoDelim => delimed.tts.len(),\n                 _ => delimed.tts.len() + 2,\n             },\n-            TokenTree::Sequence(_, ref seq) => seq.tts.len(),\n             TokenTree::Token(..) => 0,\n         }\n     }\n@@ -197,30 +165,12 @@ impl TokenTree {\n                 }\n                 delimed.tts[index - 1].clone()\n             }\n-            (&TokenTree::Token(sp, token::MatchNt(name, kind)), _) => {\n-                let v = [TokenTree::Token(sp, token::SubstNt(name)),\n-                         TokenTree::Token(sp, token::Colon),\n-                         TokenTree::Token(sp, token::Ident(kind))];\n-                v[index].clone()\n-            }\n-            (&TokenTree::Sequence(_, ref seq), _) => seq.tts[index].clone(),\n             _ => panic!(\"Cannot expand a token tree\"),\n         }\n     }\n \n-    /// Returns the `Span` corresponding to this token tree.\n-    pub fn get_span(&self) -> Span {\n-        match *self {\n-            TokenTree::Token(span, _) => span,\n-            TokenTree::Delimited(span, _) => span,\n-            TokenTree::Sequence(span, _) => span,\n-        }\n-    }\n-\n     /// Use this token tree as a matcher to parse given tts.\n-    pub fn parse(cx: &base::ExtCtxt,\n-                 mtch: &[TokenTree],\n-                 tts: &[TokenTree])\n+    pub fn parse(cx: &base::ExtCtxt, mtch: &[quoted::TokenTree], tts: &[TokenTree])\n                  -> macro_parser::NamedParseResult {\n         // `None` is because we're not interpolating\n         let directory = Directory {\n@@ -252,9 +202,7 @@ impl TokenTree {\n     /// Retrieve the TokenTree's span.\n     pub fn span(&self) -> Span {\n         match *self {\n-            TokenTree::Token(sp, _) |\n-            TokenTree::Delimited(sp, _) |\n-            TokenTree::Sequence(sp, _) => sp,\n+            TokenTree::Token(sp, _) | TokenTree::Delimited(sp, _) => sp,\n         }\n     }\n "}, {"sha": "f92cde4019f67b06e9147c6264badd7e4e41cfac", "filename": "src/libsyntax_ext/lib.rs", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "blob_url": "https://github.com/rust-lang/rust/blob/4ba49ab39f942af3846cf0d8599674b0adde94b5/src%2Flibsyntax_ext%2Flib.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4ba49ab39f942af3846cf0d8599674b0adde94b5/src%2Flibsyntax_ext%2Flib.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax_ext%2Flib.rs?ref=4ba49ab39f942af3846cf0d8599674b0adde94b5", "patch": "@@ -79,7 +79,6 @@ pub fn register_builtins(resolver: &mut syntax::ext::base::Resolver,\n             quote_pat: expand_quote_pat,\n             quote_arm: expand_quote_arm,\n             quote_stmt: expand_quote_stmt,\n-            quote_matcher: expand_quote_matcher,\n             quote_attr: expand_quote_attr,\n             quote_arg: expand_quote_arg,\n             quote_block: expand_quote_block,"}, {"sha": "63e1c6f16b3e6be4b8ac4727833ef6f6cf182d9c", "filename": "src/test/compile-fail-fulldeps/gated-quote.rs", "status": "modified", "additions": 0, "deletions": 2, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/4ba49ab39f942af3846cf0d8599674b0adde94b5/src%2Ftest%2Fcompile-fail-fulldeps%2Fgated-quote.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4ba49ab39f942af3846cf0d8599674b0adde94b5/src%2Ftest%2Fcompile-fail-fulldeps%2Fgated-quote.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ftest%2Fcompile-fail-fulldeps%2Fgated-quote.rs?ref=4ba49ab39f942af3846cf0d8599674b0adde94b5", "patch": "@@ -54,8 +54,6 @@ pub fn main() {\n     //~^ ERROR cannot find macro `quote_arm!` in this scope\n     let x = quote_stmt!(ecx, 3);\n     //~^ ERROR cannot find macro `quote_stmt!` in this scope\n-    let x = quote_matcher!(ecx, 3);\n-    //~^ ERROR cannot find macro `quote_matcher!` in this scope\n     let x = quote_attr!(ecx, 3);\n     //~^ ERROR cannot find macro `quote_attr!` in this scope\n     let x = quote_arg!(ecx, 3);"}, {"sha": "5f54f269c6c55694fb7596043f44abf457c81683", "filename": "src/test/compile-fail/issue-35450.rs", "status": "modified", "additions": 2, "deletions": 3, "changes": 5, "blob_url": "https://github.com/rust-lang/rust/blob/4ba49ab39f942af3846cf0d8599674b0adde94b5/src%2Ftest%2Fcompile-fail%2Fissue-35450.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4ba49ab39f942af3846cf0d8599674b0adde94b5/src%2Ftest%2Fcompile-fail%2Fissue-35450.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ftest%2Fcompile-fail%2Fissue-35450.rs?ref=4ba49ab39f942af3846cf0d8599674b0adde94b5", "patch": "@@ -8,9 +8,8 @@\n // option. This file may not be copied, modified, or distributed\n // except according to those terms.\n \n-macro_rules! m { ($t:tt) => { $t } }\n+macro_rules! m { ($($t:tt)*) => { $($t)* } }\n \n fn main() {\n-    m!($t); //~ ERROR unknown macro variable\n-            //~| ERROR expected expression\n+    m!($t); //~ ERROR expected expression\n }"}, {"sha": "0168ae7d910179ecf8a059ba923e25a7d8d4a13e", "filename": "src/test/compile-fail/issue-39404.rs", "status": "added", "additions": 18, "deletions": 0, "changes": 18, "blob_url": "https://github.com/rust-lang/rust/blob/4ba49ab39f942af3846cf0d8599674b0adde94b5/src%2Ftest%2Fcompile-fail%2Fissue-39404.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4ba49ab39f942af3846cf0d8599674b0adde94b5/src%2Ftest%2Fcompile-fail%2Fissue-39404.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ftest%2Fcompile-fail%2Fissue-39404.rs?ref=4ba49ab39f942af3846cf0d8599674b0adde94b5", "patch": "@@ -0,0 +1,18 @@\n+// Copyright 2017 The Rust Project Developers. See the COPYRIGHT\n+// file at the top-level directory of this distribution and at\n+// http://rust-lang.org/COPYRIGHT.\n+//\n+// Licensed under the Apache License, Version 2.0 <LICENSE-APACHE or\n+// http://www.apache.org/licenses/LICENSE-2.0> or the MIT license\n+// <LICENSE-MIT or http://opensource.org/licenses/MIT>, at your\n+// option. This file may not be copied, modified, or distributed\n+// except according to those terms.\n+\n+#![deny(missing_fragment_specifier)] //~ NOTE lint level defined here\n+\n+macro_rules! m { ($i) => {} }\n+//~^ ERROR missing fragment specifier\n+//~| WARN previously accepted\n+//~| NOTE issue #40107\n+\n+fn main() {}"}, {"sha": "82a5aa487291320cd4bc27dda7c77b30e7a9b459", "filename": "src/test/compile-fail/macro-error.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/4ba49ab39f942af3846cf0d8599674b0adde94b5/src%2Ftest%2Fcompile-fail%2Fmacro-error.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4ba49ab39f942af3846cf0d8599674b0adde94b5/src%2Ftest%2Fcompile-fail%2Fmacro-error.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ftest%2Fcompile-fail%2Fmacro-error.rs?ref=4ba49ab39f942af3846cf0d8599674b0adde94b5", "patch": "@@ -9,7 +9,7 @@\n // except according to those terms.\n \n macro_rules! foo {\n-    ($a:expr) => $a; //~ ERROR macro rhs must be delimited\n+    ($a:expr) => a; //~ ERROR macro rhs must be delimited\n }\n \n fn main() {"}, {"sha": "7255e7d00b6115c1a60caa9f279a969fb3c9cfe1", "filename": "src/test/compile-fail/macro-tt-matchers.rs", "status": "modified", "additions": 1, "deletions": 12, "changes": 13, "blob_url": "https://github.com/rust-lang/rust/blob/4ba49ab39f942af3846cf0d8599674b0adde94b5/src%2Ftest%2Fcompile-fail%2Fmacro-tt-matchers.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4ba49ab39f942af3846cf0d8599674b0adde94b5/src%2Ftest%2Fcompile-fail%2Fmacro-tt-matchers.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ftest%2Fcompile-fail%2Fmacro-tt-matchers.rs?ref=4ba49ab39f942af3846cf0d8599674b0adde94b5", "patch": "@@ -17,16 +17,5 @@ macro_rules! foo {\n \n foo!(Box);\n \n-macro_rules! bar {\n-    ($x:tt) => {\n-        macro_rules! baz {\n-            ($x:tt, $y:tt) => { ($x, $y) }\n-        }\n-    }\n-}\n-\n #[rustc_error]\n-fn main() { //~ ERROR compilation successful\n-    bar!($y);\n-    let _: (i8, i16) = baz!(0i8, 0i16);\n-}\n+fn main() {} //~ ERROR compilation successful"}, {"sha": "0b437be5393edac4aafdf4b3f4b36dc0c57382a2", "filename": "src/test/compile-fail/malformed_macro_lhs.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/4ba49ab39f942af3846cf0d8599674b0adde94b5/src%2Ftest%2Fcompile-fail%2Fmalformed_macro_lhs.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4ba49ab39f942af3846cf0d8599674b0adde94b5/src%2Ftest%2Fcompile-fail%2Fmalformed_macro_lhs.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ftest%2Fcompile-fail%2Fmalformed_macro_lhs.rs?ref=4ba49ab39f942af3846cf0d8599674b0adde94b5", "patch": "@@ -9,7 +9,7 @@\n // except according to those terms.\n \n macro_rules! my_precioooous {\n-    $($t:tt)* => (1); //~ ERROR invalid macro matcher\n+    t => (1); //~ ERROR invalid macro matcher\n }\n \n fn main() {"}, {"sha": "15d491719a6d58dd8edf2350edd32d3309d98fee", "filename": "src/test/parse-fail/issue-33569.rs", "status": "modified", "additions": 3, "deletions": 1, "changes": 4, "blob_url": "https://github.com/rust-lang/rust/blob/4ba49ab39f942af3846cf0d8599674b0adde94b5/src%2Ftest%2Fparse-fail%2Fissue-33569.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4ba49ab39f942af3846cf0d8599674b0adde94b5/src%2Ftest%2Fparse-fail%2Fissue-33569.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ftest%2Fparse-fail%2Fissue-33569.rs?ref=4ba49ab39f942af3846cf0d8599674b0adde94b5", "patch": "@@ -12,7 +12,9 @@\n \n macro_rules! foo {\n     { $+ } => { //~ ERROR expected identifier, found `+`\n+                //~^ ERROR missing fragment specifier\n         $(x)(y) //~ ERROR expected `*` or `+`\n-                //~^ ERROR no rules expected the token `)`\n     }\n }\n+\n+foo!();"}, {"sha": "3db69f2167cc6e4ba7bc5fb89456a7761c532004", "filename": "src/test/run-pass-fulldeps/auxiliary/procedural_mbe_matching.rs", "status": "modified", "additions": 3, "deletions": 1, "changes": 4, "blob_url": "https://github.com/rust-lang/rust/blob/4ba49ab39f942af3846cf0d8599674b0adde94b5/src%2Ftest%2Frun-pass-fulldeps%2Fauxiliary%2Fprocedural_mbe_matching.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4ba49ab39f942af3846cf0d8599674b0adde94b5/src%2Ftest%2Frun-pass-fulldeps%2Fauxiliary%2Fprocedural_mbe_matching.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ftest%2Frun-pass-fulldeps%2Fauxiliary%2Fprocedural_mbe_matching.rs?ref=4ba49ab39f942af3846cf0d8599674b0adde94b5", "patch": "@@ -23,6 +23,7 @@ use syntax::ast::{Ident, Pat};\n use syntax::tokenstream::{TokenTree};\n use syntax::ext::base::{ExtCtxt, MacResult, MacEager};\n use syntax::ext::build::AstBuilder;\n+use syntax::ext::tt::quoted;\n use syntax::ext::tt::macro_parser::{MatchedSeq, MatchedNonterminal};\n use syntax::ext::tt::macro_parser::{Success, Failure, Error};\n use syntax::ext::tt::macro_parser::parse_failure_msg;\n@@ -33,7 +34,8 @@ use rustc_plugin::Registry;\n fn expand_mbe_matches(cx: &mut ExtCtxt, _: Span, args: &[TokenTree])\n         -> Box<MacResult + 'static> {\n \n-    let mbe_matcher = quote_matcher!(cx, $matched:expr, $($pat:pat)|+);\n+    let mbe_matcher = quote_tokens!(cx, $$matched:expr, $$($$pat:pat)|+);\n+    let mbe_matcher = quoted::parse(&mbe_matcher, true, cx.parse_sess);\n     let map = match TokenTree::parse(cx, &mbe_matcher, args) {\n         Success(map) => map,\n         Failure(_, tok) => {"}, {"sha": "822b2c9b93b4a070d921f681fec3c50818b0c8e8", "filename": "src/test/run-pass-fulldeps/mbe_matching_test_macro.rs", "status": "modified", "additions": 0, "deletions": 4, "changes": 4, "blob_url": "https://github.com/rust-lang/rust/blob/4ba49ab39f942af3846cf0d8599674b0adde94b5/src%2Ftest%2Frun-pass-fulldeps%2Fmbe_matching_test_macro.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4ba49ab39f942af3846cf0d8599674b0adde94b5/src%2Ftest%2Frun-pass-fulldeps%2Fmbe_matching_test_macro.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ftest%2Frun-pass-fulldeps%2Fmbe_matching_test_macro.rs?ref=4ba49ab39f942af3846cf0d8599674b0adde94b5", "patch": "@@ -14,11 +14,7 @@\n #![feature(plugin)]\n #![plugin(procedural_mbe_matching)]\n \n-#[no_link]\n-extern crate procedural_mbe_matching;\n-\n pub fn main() {\n-    let abc = 123u32;\n     assert_eq!(matches!(Some(123), None | Some(0)), false);\n     assert_eq!(matches!(Some(123), None | Some(123)), true);\n     assert_eq!(matches!(true, true), true);"}, {"sha": "9094310fb3e766866d0835a5d161d7baca5d83eb", "filename": "src/test/run-pass-fulldeps/proc-macro/auxiliary/issue-39889.rs", "status": "added", "additions": 27, "deletions": 0, "changes": 27, "blob_url": "https://github.com/rust-lang/rust/blob/4ba49ab39f942af3846cf0d8599674b0adde94b5/src%2Ftest%2Frun-pass-fulldeps%2Fproc-macro%2Fauxiliary%2Fissue-39889.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4ba49ab39f942af3846cf0d8599674b0adde94b5/src%2Ftest%2Frun-pass-fulldeps%2Fproc-macro%2Fauxiliary%2Fissue-39889.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ftest%2Frun-pass-fulldeps%2Fproc-macro%2Fauxiliary%2Fissue-39889.rs?ref=4ba49ab39f942af3846cf0d8599674b0adde94b5", "patch": "@@ -0,0 +1,27 @@\n+// Copyright 2016 The Rust Project Developers. See the COPYRIGHT\n+// file at the top-level directory of this distribution and at\n+// http://rust-lang.org/COPYRIGHT.\n+//\n+// Licensed under the Apache License, Version 2.0 <LICENSE-APACHE or\n+// http://www.apache.org/licenses/LICENSE-2.0> or the MIT license\n+// <LICENSE-MIT or http://opensource.org/licenses/MIT>, at your\n+// option. This file may not be copied, modified, or distributed\n+// except according to those terms.\n+\n+// force-host\n+// no-prefer-dynamic\n+\n+#![crate_type = \"proc-macro\"]\n+\n+extern crate proc_macro;\n+use proc_macro::TokenStream;\n+\n+#[proc_macro_derive(Issue39889)]\n+pub fn f(_input: TokenStream) -> TokenStream {\n+    let rules = r#\"\n+        macro_rules! id {\n+            ($($tt:tt)*) => { $($tt)* };\n+        }\n+    \"#;\n+    rules.parse().unwrap()\n+}"}, {"sha": "05610116ad6bff88c260bb5607242612fcef1f8b", "filename": "src/test/run-pass-fulldeps/proc-macro/issue-39889.rs", "status": "added", "additions": 22, "deletions": 0, "changes": 22, "blob_url": "https://github.com/rust-lang/rust/blob/4ba49ab39f942af3846cf0d8599674b0adde94b5/src%2Ftest%2Frun-pass-fulldeps%2Fproc-macro%2Fissue-39889.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4ba49ab39f942af3846cf0d8599674b0adde94b5/src%2Ftest%2Frun-pass-fulldeps%2Fproc-macro%2Fissue-39889.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ftest%2Frun-pass-fulldeps%2Fproc-macro%2Fissue-39889.rs?ref=4ba49ab39f942af3846cf0d8599674b0adde94b5", "patch": "@@ -0,0 +1,22 @@\n+// Copyright 2016 The Rust Project Developers. See the COPYRIGHT\n+// file at the top-level directory of this distribution and at\n+// http://rust-lang.org/COPYRIGHT.\n+//\n+// Licensed under the Apache License, Version 2.0 <LICENSE-APACHE or\n+// http://www.apache.org/licenses/LICENSE-2.0> or the MIT license\n+// <LICENSE-MIT or http://opensource.org/licenses/MIT>, at your\n+// option. This file may not be copied, modified, or distributed\n+// except according to those terms.\n+\n+// aux-build:issue-39889.rs\n+\n+#![feature(proc_macro)]\n+#![allow(unused)]\n+\n+extern crate issue_39889;\n+use issue_39889::Issue39889;\n+\n+#[derive(Issue39889)]\n+struct S;\n+\n+fn main() {}"}, {"sha": "8e6a69cb58479e1af384cb19729e1554ca4d12dc", "filename": "src/test/run-pass-fulldeps/quote-tokens.rs", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "blob_url": "https://github.com/rust-lang/rust/blob/4ba49ab39f942af3846cf0d8599674b0adde94b5/src%2Ftest%2Frun-pass-fulldeps%2Fquote-tokens.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4ba49ab39f942af3846cf0d8599674b0adde94b5/src%2Ftest%2Frun-pass-fulldeps%2Fquote-tokens.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ftest%2Frun-pass-fulldeps%2Fquote-tokens.rs?ref=4ba49ab39f942af3846cf0d8599674b0adde94b5", "patch": "@@ -37,7 +37,6 @@ fn syntax_extension(cx: &ExtCtxt) {\n \n     let _l: P<syntax::ast::Ty> = quote_ty!(cx, &isize);\n \n-    let _m: Vec<syntax::tokenstream::TokenTree> = quote_matcher!(cx, $($foo:tt,)* bar);\n     let _n: syntax::ast::Attribute = quote_attr!(cx, #![cfg(foo, bar = \"baz\")]);\n \n     let _o: Option<P<syntax::ast::Item>> = quote_item!(cx, fn foo<T: ?Sized>() {});"}, {"sha": "ebca9312a64b0fc11df3756ec531029a4543ac29", "filename": "src/test/run-pass/issue-39709.rs", "status": "renamed", "additions": 1, "deletions": 2, "changes": 3, "blob_url": "https://github.com/rust-lang/rust/blob/4ba49ab39f942af3846cf0d8599674b0adde94b5/src%2Ftest%2Frun-pass%2Fissue-39709.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4ba49ab39f942af3846cf0d8599674b0adde94b5/src%2Ftest%2Frun-pass%2Fissue-39709.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ftest%2Frun-pass%2Fissue-39709.rs?ref=4ba49ab39f942af3846cf0d8599674b0adde94b5", "patch": "@@ -9,7 +9,6 @@\n // except according to those terms.\n \n fn main() {\n-    println!(\"{}\", { macro_rules! x { ($()*) => {} } 33 });\n-    //~^ ERROR no syntax variables matched as repeating at this depth\n+    println!(\"{}\", { macro_rules! x { ($(t:tt)*) => {} } 33 });\n }\n ", "previous_filename": "src/test/compile-fail/issue-39709.rs"}]}