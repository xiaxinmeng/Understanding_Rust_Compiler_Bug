{"sha": "8c082658bed1877d5741f7badceb8efc3015598d", "node_id": "MDY6Q29tbWl0NzI0NzEyOjhjMDgyNjU4YmVkMTg3N2Q1NzQxZjdiYWRjZWI4ZWZjMzAxNTU5OGQ=", "commit": {"author": {"name": "bors", "email": "bors@rust-lang.org", "date": "2013-07-17T20:07:24Z"}, "committer": {"name": "bors", "email": "bors@rust-lang.org", "date": "2013-07-17T20:07:24Z"}, "message": "auto merge of #7829 : graydon/rust/codegen-compiletests, r=cmr\n\nThis should get us over the hump of activating basic ratcheting on codegen tests, at least. It also puts in place optional (disabled by default) ratcheting on all #[bench] tests, and records all metrics from them to harvestable .json files in any case.", "tree": {"sha": "f1d3eccc16aea2556bdd3666b2f3e4116c25627d", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/f1d3eccc16aea2556bdd3666b2f3e4116c25627d"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/8c082658bed1877d5741f7badceb8efc3015598d", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/8c082658bed1877d5741f7badceb8efc3015598d", "html_url": "https://github.com/rust-lang/rust/commit/8c082658bed1877d5741f7badceb8efc3015598d", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/8c082658bed1877d5741f7badceb8efc3015598d/comments", "author": {"login": "bors", "id": 3372342, "node_id": "MDQ6VXNlcjMzNzIzNDI=", "avatar_url": "https://avatars.githubusercontent.com/u/3372342?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bors", "html_url": "https://github.com/bors", "followers_url": "https://api.github.com/users/bors/followers", "following_url": "https://api.github.com/users/bors/following{/other_user}", "gists_url": "https://api.github.com/users/bors/gists{/gist_id}", "starred_url": "https://api.github.com/users/bors/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bors/subscriptions", "organizations_url": "https://api.github.com/users/bors/orgs", "repos_url": "https://api.github.com/users/bors/repos", "events_url": "https://api.github.com/users/bors/events{/privacy}", "received_events_url": "https://api.github.com/users/bors/received_events", "type": "User", "site_admin": false}, "committer": {"login": "bors", "id": 3372342, "node_id": "MDQ6VXNlcjMzNzIzNDI=", "avatar_url": "https://avatars.githubusercontent.com/u/3372342?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bors", "html_url": "https://github.com/bors", "followers_url": "https://api.github.com/users/bors/followers", "following_url": "https://api.github.com/users/bors/following{/other_user}", "gists_url": "https://api.github.com/users/bors/gists{/gist_id}", "starred_url": "https://api.github.com/users/bors/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bors/subscriptions", "organizations_url": "https://api.github.com/users/bors/orgs", "repos_url": "https://api.github.com/users/bors/repos", "events_url": "https://api.github.com/users/bors/events{/privacy}", "received_events_url": "https://api.github.com/users/bors/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "060de1016b280732308452f25eb90810deaba002", "url": "https://api.github.com/repos/rust-lang/rust/commits/060de1016b280732308452f25eb90810deaba002", "html_url": "https://github.com/rust-lang/rust/commit/060de1016b280732308452f25eb90810deaba002"}, {"sha": "6d78a367b1f5721624c7f8b66b1796303f0b6f45", "url": "https://api.github.com/repos/rust-lang/rust/commits/6d78a367b1f5721624c7f8b66b1796303f0b6f45", "html_url": "https://github.com/rust-lang/rust/commit/6d78a367b1f5721624c7f8b66b1796303f0b6f45"}], "stats": {"total": 304, "additions": 270, "deletions": 34}, "files": [{"sha": "7138c395513ccd4407ccd77a4737c0c8450088df", "filename": "configure", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "blob_url": "https://github.com/rust-lang/rust/blob/8c082658bed1877d5741f7badceb8efc3015598d/configure", "raw_url": "https://github.com/rust-lang/rust/raw/8c082658bed1877d5741f7badceb8efc3015598d/configure", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/configure?ref=8c082658bed1877d5741f7badceb8efc3015598d", "patch": "@@ -372,6 +372,7 @@ opt optimize 1 \"build optimized rust code\"\n opt optimize-cxx 1 \"build optimized C++ code\"\n opt optimize-llvm 1 \"build optimized LLVM\"\n opt debug 0 \"build with extra debug fun\"\n+opt ratchet-bench 0 \"ratchet benchmarks\"\n opt fast-make 0 \"use .gitmodules as timestamp for submodule deps\"\n opt manage-submodules 1 \"let the build manage the git submodules\"\n opt mingw-cross 0 \"cross-compile for win32 using mingw\""}, {"sha": "770e72804913a33f59beb5c74f2f1d5e9b8fc361", "filename": "mk/tests.mk", "status": "modified", "additions": 22, "deletions": 0, "changes": 22, "blob_url": "https://github.com/rust-lang/rust/blob/8c082658bed1877d5741f7badceb8efc3015598d/mk%2Ftests.mk", "raw_url": "https://github.com/rust-lang/rust/raw/8c082658bed1877d5741f7badceb8efc3015598d/mk%2Ftests.mk", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/mk%2Ftests.mk?ref=8c082658bed1877d5741f7badceb8efc3015598d", "patch": "@@ -34,9 +34,12 @@ ifdef CHECK_XFAILS\n   TESTARGS += --ignored\n endif\n \n+CTEST_BENCH = --bench\n+\n # Arguments to the cfail/rfail/rpass/bench tests\n ifdef CFG_VALGRIND\n   CTEST_RUNTOOL = --runtool \"$(CFG_VALGRIND)\"\n+  CTEST_BENCH =\n endif\n \n # Arguments to the perf tests\n@@ -60,6 +63,21 @@ endif\n TEST_LOG_FILE=tmp/check-stage$(1)-T-$(2)-H-$(3)-$(4).log\n TEST_OK_FILE=tmp/check-stage$(1)-T-$(2)-H-$(3)-$(4).ok\n \n+TEST_RATCHET_FILE=tmp/check-stage$(1)-T-$(2)-H-$(3)-$(4)-metrics.json\n+TEST_RATCHET_NOISE_PERCENT=10.0\n+\n+# Whether to ratchet or merely save benchmarks\n+ifdef CFG_RATCHET_BENCH\n+CRATE_TEST_BENCH_ARGS=\\\n+  --test $(CTEST_BENCH) \\\n+  --ratchet-metrics $(call TEST_RATCHET_FILE,$(1),$(2),$(3),$(4)) \\\n+  --ratchet-noise-percent $(TEST_RATCHET_NOISE_PERCENT)\n+else\n+CRATE_TEST_BENCH_ARGS=\\\n+  --test $(CTEST_BENCH) \\\n+  --save-metrics $(call TEST_RATCHET_FILE,$(1),$(2),$(3),$(4))\n+endif\n+\n define DEF_TARGET_COMMANDS\n \n ifdef CFG_UNIXY_$(1)\n@@ -359,11 +377,14 @@ $(foreach host,$(CFG_HOST_TRIPLES), \\\n define DEF_TEST_CRATE_RULES\n check-stage$(1)-T-$(2)-H-$(3)-$(4)-exec: $$(call TEST_OK_FILE,$(1),$(2),$(3),$(4))\n \n+check-stage$(1)-T-$(2)-H-$(3)-$(4)-exec: $$(call TEST_OK_FILE,$(1),$(2),$(3),$(4))\n+\n $$(call TEST_OK_FILE,$(1),$(2),$(3),$(4)): \\\n \t\t$(3)/stage$(1)/test/$(4)test-$(2)$$(X_$(2))\n \t@$$(call E, run: $$<)\n \t$$(Q)$$(call CFG_RUN_TEST_$(2),$$<,$(2),$(3)) $$(TESTARGS)\t\\\n \t--logfile $$(call TEST_LOG_FILE,$(1),$(2),$(3),$(4)) \\\n+\t$$(call CRATE_TEST_BENCH_ARGS,$(1),$(2),$(3),$(4)) \\\n \t&& touch $$@\n endef\n \n@@ -552,6 +573,7 @@ CTEST_ARGS$(1)-T-$(2)-H-$(3)-$(4) := \\\n         $$(CTEST_COMMON_ARGS$(1)-T-$(2)-H-$(3))\t\\\n         --src-base $$(S)src/test/$$(CTEST_SRC_BASE_$(4))/ \\\n         --build-base $(3)/test/$$(CTEST_BUILD_BASE_$(4))/ \\\n+        --ratchet-metrics $(call TEST_RATCHET_FILE,$(1),$(2),$(3),$(4)) \\\n         --mode $$(CTEST_MODE_$(4)) \\\n \t$$(CTEST_RUNTOOL_$(4))\n "}, {"sha": "4add16fd7a95b08bf9c19b0cac74eec1d0678fd0", "filename": "src/compiletest/common.rs", "status": "modified", "additions": 9, "deletions": 0, "changes": 9, "blob_url": "https://github.com/rust-lang/rust/blob/8c082658bed1877d5741f7badceb8efc3015598d/src%2Fcompiletest%2Fcommon.rs", "raw_url": "https://github.com/rust-lang/rust/raw/8c082658bed1877d5741f7badceb8efc3015598d/src%2Fcompiletest%2Fcommon.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Fcompiletest%2Fcommon.rs?ref=8c082658bed1877d5741f7badceb8efc3015598d", "patch": "@@ -58,6 +58,15 @@ pub struct config {\n     // Write out a parseable log of tests that were run\n     logfile: Option<Path>,\n \n+    // Write out a json file containing any metrics of the run\n+    save_metrics: Option<Path>,\n+\n+    // Write and ratchet a metrics file\n+    ratchet_metrics: Option<Path>,\n+\n+    // Percent change in metrics to consider noise\n+    ratchet_noise_percent: Option<f64>,\n+\n     // A command line to prefix program execution with,\n     // for running under valgrind\n     runtool: Option<~str>,"}, {"sha": "39dc55b44f4cac4813f27dac40b7f030d33686ef", "filename": "src/compiletest/compiletest.rs", "status": "modified", "additions": 33, "deletions": 7, "changes": 40, "blob_url": "https://github.com/rust-lang/rust/blob/8c082658bed1877d5741f7badceb8efc3015598d/src%2Fcompiletest%2Fcompiletest.rs", "raw_url": "https://github.com/rust-lang/rust/raw/8c082658bed1877d5741f7badceb8efc3015598d/src%2Fcompiletest%2Fcompiletest.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Fcompiletest%2Fcompiletest.rs?ref=8c082658bed1877d5741f7badceb8efc3015598d", "patch": "@@ -17,6 +17,7 @@\n extern mod extra;\n \n use std::os;\n+use std::f64;\n \n use extra::getopts;\n use extra::getopts::groups::{optopt, optflag, reqopt};\n@@ -66,6 +67,10 @@ pub fn parse_config(args: ~[~str]) -> config {\n           optopt(\"\", \"rustcflags\", \"flags to pass to rustc\", \"FLAGS\"),\n           optflag(\"\", \"verbose\", \"run tests verbosely, showing all output\"),\n           optopt(\"\", \"logfile\", \"file to log test execution to\", \"FILE\"),\n+          optopt(\"\", \"save-metrics\", \"file to save metrics to\", \"FILE\"),\n+          optopt(\"\", \"ratchet-metrics\", \"file to ratchet metrics against\", \"FILE\"),\n+          optopt(\"\", \"ratchet-noise-percent\",\n+                 \"percent change in metrics to consider noise\", \"N\"),\n           optflag(\"\", \"jit\", \"run tests under the JIT\"),\n           optflag(\"\", \"newrt\", \"run tests on the new runtime / scheduler\"),\n           optopt(\"\", \"target\", \"the target to build for\", \"TARGET\"),\n@@ -116,6 +121,13 @@ pub fn parse_config(args: ~[~str]) -> config {\n                  Some(copy matches.free[0])\n              } else { None },\n         logfile: getopts::opt_maybe_str(matches, \"logfile\").map(|s| Path(*s)),\n+        save_metrics: getopts::opt_maybe_str(matches, \"save-metrics\").map(|s| Path(*s)),\n+        ratchet_metrics:\n+            getopts::opt_maybe_str(matches, \"ratchet-metrics\").map(|s| Path(*s)),\n+        ratchet_noise_percent:\n+            getopts::opt_maybe_str(matches,\n+                                   \"ratchet-noise-percent\").map(|s|\n+                                                                f64::from_str(*s).get()),\n         runtool: getopts::opt_maybe_str(matches, \"runtool\"),\n         rustcflags: getopts::opt_maybe_str(matches, \"rustcflags\"),\n         jit: getopts::opt_present(matches, \"jit\"),\n@@ -215,10 +227,10 @@ pub fn test_opts(config: &config) -> test::TestOpts {\n         run_ignored: config.run_ignored,\n         logfile: copy config.logfile,\n         run_tests: true,\n-        run_benchmarks: false,\n-        ratchet_metrics: None,\n-        ratchet_noise_percent: None,\n-        save_metrics: None,\n+        run_benchmarks: true,\n+        ratchet_metrics: copy config.ratchet_metrics,\n+        ratchet_noise_percent: copy config.ratchet_noise_percent,\n+        save_metrics: copy config.save_metrics,\n     }\n }\n \n@@ -231,7 +243,13 @@ pub fn make_tests(config: &config) -> ~[test::TestDescAndFn] {\n         let file = copy *file;\n         debug!(\"inspecting file %s\", file.to_str());\n         if is_test(config, file) {\n-            tests.push(make_test(config, file))\n+            let t = do make_test(config, file) {\n+                match config.mode {\n+                    mode_codegen => make_metrics_test_closure(config, file),\n+                    _ => make_test_closure(config, file)\n+                }\n+            };\n+            tests.push(t)\n         }\n     }\n     tests\n@@ -260,14 +278,15 @@ pub fn is_test(config: &config, testfile: &Path) -> bool {\n     return valid;\n }\n \n-pub fn make_test(config: &config, testfile: &Path) -> test::TestDescAndFn {\n+pub fn make_test(config: &config, testfile: &Path,\n+                 f: &fn()->test::TestFn) -> test::TestDescAndFn {\n     test::TestDescAndFn {\n         desc: test::TestDesc {\n             name: make_test_name(config, testfile),\n             ignore: header::is_test_ignored(config, testfile),\n             should_fail: false\n         },\n-        testfn: make_test_closure(config, testfile),\n+        testfn: f(),\n     }\n }\n \n@@ -291,3 +310,10 @@ pub fn make_test_closure(config: &config, testfile: &Path) -> test::TestFn {\n     let testfile = Cell::new(testfile.to_str());\n     test::DynTestFn(|| { runtest::run(config.take(), testfile.take()) })\n }\n+\n+pub fn make_metrics_test_closure(config: &config, testfile: &Path) -> test::TestFn {\n+    use std::cell::Cell;\n+    let config = Cell::new(copy *config);\n+    let testfile = Cell::new(testfile.to_str());\n+    test::DynMetricFn(|mm| { runtest::run_metrics(config.take(), testfile.take(), mm) })\n+}"}, {"sha": "a51ab8208566d1435f17ad336a74fa4abfb480d6", "filename": "src/compiletest/runtest.rs", "status": "modified", "additions": 26, "deletions": 2, "changes": 28, "blob_url": "https://github.com/rust-lang/rust/blob/8c082658bed1877d5741f7badceb8efc3015598d/src%2Fcompiletest%2Fruntest.rs", "raw_url": "https://github.com/rust-lang/rust/raw/8c082658bed1877d5741f7badceb8efc3015598d/src%2Fcompiletest%2Fruntest.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Fcompiletest%2Fruntest.rs?ref=8c082658bed1877d5741f7badceb8efc3015598d", "patch": "@@ -25,7 +25,14 @@ use std::os;\n use std::uint;\n use std::vec;\n \n+use extra::test::MetricMap;\n+\n pub fn run(config: config, testfile: ~str) {\n+    let mut _mm = MetricMap::new();\n+    run_metrics(config, testfile, &mut _mm);\n+}\n+\n+pub fn run_metrics(config: config, testfile: ~str, mm: &mut MetricMap) {\n     if config.verbose {\n         // We're going to be dumping a lot of info. Start on a new line.\n         io::stdout().write_str(\"\\n\\n\");\n@@ -40,7 +47,7 @@ pub fn run(config: config, testfile: ~str) {\n       mode_run_pass => run_rpass_test(&config, &props, &testfile),\n       mode_pretty => run_pretty_test(&config, &props, &testfile),\n       mode_debug_info => run_debuginfo_test(&config, &props, &testfile),\n-      mode_codegen => run_codegen_test(&config, &props, &testfile)\n+      mode_codegen => run_codegen_test(&config, &props, &testfile, mm)\n     }\n }\n \n@@ -906,7 +913,14 @@ fn disassemble_extract(config: &config, _props: &TestProps,\n }\n \n \n-fn run_codegen_test(config: &config, props: &TestProps, testfile: &Path) {\n+fn count_extracted_lines(p: &Path) -> uint {\n+    let x = io::read_whole_file_str(&p.with_filetype(\"ll\")).get();\n+    x.line_iter().len_()\n+}\n+\n+\n+fn run_codegen_test(config: &config, props: &TestProps,\n+                    testfile: &Path, mm: &mut MetricMap) {\n \n     if config.llvm_bin_path.is_none() {\n         fatal(~\"missing --llvm-bin-path\");\n@@ -947,7 +961,17 @@ fn run_codegen_test(config: &config, props: &TestProps, testfile: &Path) {\n         fatal_ProcRes(~\"disassembling extract failed\", &ProcRes);\n     }\n \n+    let base = output_base_name(config, testfile);\n+    let base_extract = append_suffix_to_stem(&base, \"extract\");\n+\n+    let base_clang = append_suffix_to_stem(&base, \"clang\");\n+    let base_clang_extract = append_suffix_to_stem(&base_clang, \"extract\");\n \n+    let base_lines = count_extracted_lines(&base_extract);\n+    let clang_lines = count_extracted_lines(&base_clang_extract);\n \n+    mm.insert_metric(\"clang-codegen-ratio\",\n+                     (base_lines as f64) / (clang_lines as f64),\n+                     0.001);\n }\n "}, {"sha": "deef1fc36138ddfa81603b6fb48257b6a9a1ba71", "filename": "src/libextra/test.rs", "status": "modified", "additions": 179, "deletions": 25, "changes": 204, "blob_url": "https://github.com/rust-lang/rust/blob/8c082658bed1877d5741f7badceb8efc3015598d/src%2Flibextra%2Ftest.rs", "raw_url": "https://github.com/rust-lang/rust/raw/8c082658bed1877d5741f7badceb8efc3015598d/src%2Flibextra%2Ftest.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibextra%2Ftest.rs?ref=8c082658bed1877d5741f7badceb8efc3015598d", "patch": "@@ -64,7 +64,9 @@ impl ToStr for TestName {\n pub enum TestFn {\n     StaticTestFn(extern fn()),\n     StaticBenchFn(extern fn(&mut BenchHarness)),\n+    StaticMetricFn(~fn(&mut MetricMap)),\n     DynTestFn(~fn()),\n+    DynMetricFn(~fn(&mut MetricMap)),\n     DynBenchFn(~fn(&mut BenchHarness))\n }\n \n@@ -95,9 +97,11 @@ pub struct Metric {\n     noise: f64\n }\n \n+#[deriving(Eq)]\n pub struct MetricMap(TreeMap<~str,Metric>);\n \n /// Analysis of a single change in metric\n+#[deriving(Eq)]\n pub enum MetricChange {\n     LikelyNoise,\n     MetricAdded,\n@@ -217,7 +221,13 @@ pub struct BenchSamples {\n }\n \n #[deriving(Eq)]\n-pub enum TestResult { TrOk, TrFailed, TrIgnored, TrBench(BenchSamples) }\n+pub enum TestResult {\n+    TrOk,\n+    TrFailed,\n+    TrIgnored,\n+    TrMetrics(MetricMap),\n+    TrBench(BenchSamples)\n+}\n \n struct ConsoleTestState {\n     out: @io::Writer,\n@@ -228,7 +238,7 @@ struct ConsoleTestState {\n     passed: uint,\n     failed: uint,\n     ignored: uint,\n-    benchmarked: uint,\n+    measured: uint,\n     metrics: MetricMap,\n     failures: ~[TestDesc]\n }\n@@ -260,7 +270,7 @@ impl ConsoleTestState {\n             passed: 0u,\n             failed: 0u,\n             ignored: 0u,\n-            benchmarked: 0u,\n+            measured: 0u,\n             metrics: MetricMap::new(),\n             failures: ~[]\n         }\n@@ -278,11 +288,14 @@ impl ConsoleTestState {\n         self.write_pretty(\"ignored\", term::color::YELLOW);\n     }\n \n+    pub fn write_metric(&self) {\n+        self.write_pretty(\"metric\", term::color::CYAN);\n+    }\n+\n     pub fn write_bench(&self) {\n         self.write_pretty(\"bench\", term::color::CYAN);\n     }\n \n-\n     pub fn write_added(&self) {\n         self.write_pretty(\"added\", term::color::GREEN);\n     }\n@@ -331,6 +344,10 @@ impl ConsoleTestState {\n             TrOk => self.write_ok(),\n             TrFailed => self.write_failed(),\n             TrIgnored => self.write_ignored(),\n+            TrMetrics(ref mm) => {\n+                self.write_metric();\n+                self.out.write_str(\": \" + fmt_metrics(mm));\n+            }\n             TrBench(ref bs) => {\n                 self.write_bench();\n                 self.out.write_str(\": \" + fmt_bench_samples(bs))\n@@ -348,6 +365,7 @@ impl ConsoleTestState {\n                                         TrOk => ~\"ok\",\n                                         TrFailed => ~\"failed\",\n                                         TrIgnored => ~\"ignored\",\n+                                        TrMetrics(ref mm) => fmt_metrics(mm),\n                                         TrBench(ref bs) => fmt_bench_samples(bs)\n                                     }, test.name.to_str()));\n             }\n@@ -415,7 +433,7 @@ impl ConsoleTestState {\n     pub fn write_run_finish(&self,\n                             ratchet_metrics: &Option<Path>,\n                             ratchet_pct: Option<f64>) -> bool {\n-        assert!(self.passed + self.failed + self.ignored + self.benchmarked == self.total);\n+        assert!(self.passed + self.failed + self.ignored + self.measured == self.total);\n \n         let ratchet_success = match *ratchet_metrics {\n             None => true,\n@@ -447,12 +465,23 @@ impl ConsoleTestState {\n         } else {\n             self.write_failed();\n         }\n-        self.out.write_str(fmt!(\". %u passed; %u failed; %u ignored, %u benchmarked\\n\\n\",\n-                                self.passed, self.failed, self.ignored, self.benchmarked));\n+        self.out.write_str(fmt!(\". %u passed; %u failed; %u ignored; %u measured\\n\\n\",\n+                                self.passed, self.failed, self.ignored, self.measured));\n         return success;\n     }\n }\n \n+pub fn fmt_metrics(mm: &MetricMap) -> ~str {\n+    use std::iterator::IteratorUtil;\n+    let v : ~[~str] = mm.iter()\n+        .transform(|(k,v)| fmt!(\"%s: %f (+/- %f)\",\n+                                *k,\n+                                v.value as float,\n+                                v.noise as float))\n+        .collect();\n+    v.connect(\", \")\n+}\n+\n pub fn fmt_bench_samples(bs: &BenchSamples) -> ~str {\n     if bs.mb_s != 0 {\n         fmt!(\"%u ns/iter (+/- %u) = %u MB/s\",\n@@ -480,11 +509,19 @@ pub fn run_tests_console(opts: &TestOpts,\n                 match result {\n                     TrOk => st.passed += 1,\n                     TrIgnored => st.ignored += 1,\n+                    TrMetrics(mm) => {\n+                        let tname = test.name.to_str();\n+                        for mm.iter().advance() |(k,v)| {\n+                            st.metrics.insert_metric(tname + \".\" + *k,\n+                                                     v.value, v.noise);\n+                        }\n+                        st.measured += 1\n+                    }\n                     TrBench(bs) => {\n                         st.metrics.insert_metric(test.name.to_str(),\n                                                  bs.ns_iter_summ.median,\n                                                  bs.ns_iter_summ.max - bs.ns_iter_summ.min);\n-                        st.benchmarked += 1\n+                        st.measured += 1\n                     }\n                     TrFailed => {\n                         st.failed += 1;\n@@ -532,7 +569,7 @@ fn should_sort_failures_before_printing_them() {\n             passed: 0u,\n             failed: 0u,\n             ignored: 0u,\n-            benchmarked: 0u,\n+            measured: 0u,\n             metrics: MetricMap::new(),\n             failures: ~[test_b, test_a]\n         };\n@@ -564,11 +601,11 @@ fn run_tests(opts: &TestOpts,\n \n     callback(TeFiltered(filtered_descs));\n \n-    let (filtered_tests, filtered_benchs) =\n+    let (filtered_tests, filtered_benchs_and_metrics) =\n         do filtered_tests.partition |e| {\n         match e.testfn {\n             StaticTestFn(_) | DynTestFn(_) => true,\n-            StaticBenchFn(_) | DynBenchFn(_) => false\n+            _ => false\n         }\n     };\n \n@@ -606,7 +643,8 @@ fn run_tests(opts: &TestOpts,\n     }\n \n     // All benchmarks run at the end, in serial.\n-    for filtered_benchs.consume_iter().advance |b| {\n+    // (this includes metric fns)\n+    for filtered_benchs_and_metrics.consume_iter().advance |b| {\n         callback(TeWait(copy b.desc));\n         run_test(!opts.run_benchmarks, b, ch.clone());\n         let (test, result) = p.recv();\n@@ -729,6 +767,18 @@ pub fn run_test(force_ignore: bool,\n             monitor_ch.send((desc, TrBench(bs)));\n             return;\n         }\n+        DynMetricFn(f) => {\n+            let mut mm = MetricMap::new();\n+            f(&mut mm);\n+            monitor_ch.send((desc, TrMetrics(mm)));\n+            return;\n+        }\n+        StaticMetricFn(f) => {\n+            let mut mm = MetricMap::new();\n+            f(&mut mm);\n+            monitor_ch.send((desc, TrMetrics(mm)));\n+            return;\n+        }\n         DynTestFn(f) => run_test_inner(desc, monitor_ch, f),\n         StaticTestFn(f) => run_test_inner(desc, monitor_ch, || f())\n     }\n@@ -756,12 +806,12 @@ impl ToJson for Metric {\n \n impl MetricMap {\n \n-    fn new() -> MetricMap {\n+    pub fn new() -> MetricMap {\n         MetricMap(TreeMap::new())\n     }\n \n     /// Load MetricDiff from a file.\n-    fn load(p: &Path) -> MetricMap {\n+    pub fn load(p: &Path) -> MetricMap {\n         assert!(os::path_exists(p));\n         let f = io::file_reader(p).get();\n         let mut decoder = json::Decoder(json::from_reader(f).get());\n@@ -774,8 +824,13 @@ impl MetricMap {\n         json::to_pretty_writer(f, &self.to_json());\n     }\n \n-    /// Compare against another MetricMap\n-    pub fn compare_to_old(&self, old: MetricMap,\n+    /// Compare against another MetricMap. Optionally compare all\n+    /// measurements in the maps using the provided `noise_pct` as a\n+    /// percentage of each value to consider noise. If `None`, each\n+    /// measurement's noise threshold is independently chosen as the\n+    /// maximum of that measurement's recorded noise quantity in either\n+    /// map.\n+    pub fn compare_to_old(&self, old: &MetricMap,\n                           noise_pct: Option<f64>) -> MetricDiff {\n         let mut diff : MetricDiff = TreeMap::new();\n         for old.iter().advance |(k, vold)| {\n@@ -787,10 +842,10 @@ impl MetricMap {\n                         None => f64::max(vold.noise.abs(), v.noise.abs()),\n                         Some(pct) => vold.value * pct / 100.0\n                     };\n-                    if delta.abs() < noise {\n+                    if delta.abs() <= noise {\n                         LikelyNoise\n                     } else {\n-                        let pct = delta.abs() / v.value * 100.0;\n+                        let pct = delta.abs() / (vold.value).max(&f64::epsilon) * 100.0;\n                         if vold.noise < 0.0 {\n                             // When 'noise' is negative, it means we want\n                             // to see deltas that go up over time, and can\n@@ -857,7 +912,7 @@ impl MetricMap {\n             MetricMap::new()\n         };\n \n-        let diff : MetricDiff = self.compare_to_old(old, pct);\n+        let diff : MetricDiff = self.compare_to_old(&old, pct);\n         let ok = do diff.iter().all() |(_, v)| {\n             match *v {\n                 Regression(_) => false,\n@@ -899,7 +954,7 @@ impl BenchHarness {\n         if self.iterations == 0 {\n             0\n         } else {\n-            self.ns_elapsed() / self.iterations\n+            self.ns_elapsed() / self.iterations.max(&1)\n         }\n     }\n \n@@ -922,7 +977,7 @@ impl BenchHarness {\n         if self.ns_per_iter() == 0 {\n             n = 1_000_000;\n         } else {\n-            n = 1_000_000 / self.ns_per_iter();\n+            n = 1_000_000 / self.ns_per_iter().max(&1);\n         }\n \n         let mut total_run = 0;\n@@ -964,8 +1019,8 @@ impl BenchHarness {\n             }\n \n             total_run += loop_run;\n-            // Longest we ever run for is 10s.\n-            if total_run > 10_000_000_000 {\n+            // Longest we ever run for is 3s.\n+            if total_run > 3_000_000_000 {\n                 return summ5;\n             }\n \n@@ -992,7 +1047,8 @@ pub mod bench {\n \n         let ns_iter_summ = bs.auto_bench(f);\n \n-        let iter_s = 1_000_000_000 / (ns_iter_summ.median as u64);\n+        let ns_iter = (ns_iter_summ.median as u64).max(&1);\n+        let iter_s = 1_000_000_000 / ns_iter;\n         let mb_s = (bs.bytes * iter_s) / 1_000_000;\n \n         BenchSamples {\n@@ -1006,13 +1062,16 @@ pub mod bench {\n mod tests {\n     use test::{TrFailed, TrIgnored, TrOk, filter_tests, parse_opts,\n                TestDesc, TestDescAndFn,\n+               Metric, MetricMap, MetricAdded, MetricRemoved,\n+               Improvement, Regression, LikelyNoise,\n                StaticTestName, DynTestName, DynTestFn};\n     use test::{TestOpts, run_test};\n \n     use std::either;\n     use std::comm::{stream, SharedChan};\n-    use std::option;\n     use std::vec;\n+    use tempfile;\n+    use std::os;\n \n     #[test]\n     pub fn do_not_run_ignored_tests() {\n@@ -1208,4 +1267,99 @@ mod tests {\n             }\n         }\n     }\n+\n+    #[test]\n+    pub fn test_metricmap_compare() {\n+        let mut m1 = MetricMap::new();\n+        let mut m2 = MetricMap::new();\n+        m1.insert_metric(\"in-both-noise\", 1000.0, 200.0);\n+        m2.insert_metric(\"in-both-noise\", 1100.0, 200.0);\n+\n+        m1.insert_metric(\"in-first-noise\", 1000.0, 2.0);\n+        m2.insert_metric(\"in-second-noise\", 1000.0, 2.0);\n+\n+        m1.insert_metric(\"in-both-want-downwards-but-regressed\", 1000.0, 10.0);\n+        m2.insert_metric(\"in-both-want-downwards-but-regressed\", 2000.0, 10.0);\n+\n+        m1.insert_metric(\"in-both-want-downwards-and-improved\", 2000.0, 10.0);\n+        m2.insert_metric(\"in-both-want-downwards-and-improved\", 1000.0, 10.0);\n+\n+        m1.insert_metric(\"in-both-want-upwards-but-regressed\", 2000.0, -10.0);\n+        m2.insert_metric(\"in-both-want-upwards-but-regressed\", 1000.0, -10.0);\n+\n+        m1.insert_metric(\"in-both-want-upwards-and-improved\", 1000.0, -10.0);\n+        m2.insert_metric(\"in-both-want-upwards-and-improved\", 2000.0, -10.0);\n+\n+        let diff1 = m2.compare_to_old(&m1, None);\n+\n+        assert_eq!(*(diff1.find(&~\"in-both-noise\").get()), LikelyNoise);\n+        assert_eq!(*(diff1.find(&~\"in-first-noise\").get()), MetricRemoved);\n+        assert_eq!(*(diff1.find(&~\"in-second-noise\").get()), MetricAdded);\n+        assert_eq!(*(diff1.find(&~\"in-both-want-downwards-but-regressed\").get()),\n+                   Regression(100.0));\n+        assert_eq!(*(diff1.find(&~\"in-both-want-downwards-and-improved\").get()),\n+                   Improvement(50.0));\n+        assert_eq!(*(diff1.find(&~\"in-both-want-upwards-but-regressed\").get()),\n+                   Regression(50.0));\n+        assert_eq!(*(diff1.find(&~\"in-both-want-upwards-and-improved\").get()),\n+                   Improvement(100.0));\n+        assert_eq!(diff1.len(), 7);\n+\n+        let diff2 = m2.compare_to_old(&m1, Some(200.0));\n+\n+        assert_eq!(*(diff2.find(&~\"in-both-noise\").get()), LikelyNoise);\n+        assert_eq!(*(diff2.find(&~\"in-first-noise\").get()), MetricRemoved);\n+        assert_eq!(*(diff2.find(&~\"in-second-noise\").get()), MetricAdded);\n+        assert_eq!(*(diff2.find(&~\"in-both-want-downwards-but-regressed\").get()), LikelyNoise);\n+        assert_eq!(*(diff2.find(&~\"in-both-want-downwards-and-improved\").get()), LikelyNoise);\n+        assert_eq!(*(diff2.find(&~\"in-both-want-upwards-but-regressed\").get()), LikelyNoise);\n+        assert_eq!(*(diff2.find(&~\"in-both-want-upwards-and-improved\").get()), LikelyNoise);\n+        assert_eq!(diff2.len(), 7);\n+    }\n+\n+    pub fn ratchet_test() {\n+\n+        let dpth = tempfile::mkdtemp(&os::tmpdir(),\n+                                     \"test-ratchet\").expect(\"missing test for ratchet\");\n+        let pth = dpth.push(\"ratchet.json\");\n+\n+        let mut m1 = MetricMap::new();\n+        m1.insert_metric(\"runtime\", 1000.0, 2.0);\n+        m1.insert_metric(\"throughput\", 50.0, 2.0);\n+\n+        let mut m2 = MetricMap::new();\n+        m2.insert_metric(\"runtime\", 1100.0, 2.0);\n+        m2.insert_metric(\"throughput\", 50.0, 2.0);\n+\n+        m1.save(&pth);\n+\n+        // Ask for a ratchet that should fail to advance.\n+        let (diff1, ok1) = m2.ratchet(&pth, None);\n+        assert_eq!(ok1, false);\n+        assert_eq!(diff1.len(), 2);\n+        assert_eq!(*(diff1.find(&~\"runtime\").get()), Regression(10.0));\n+        assert_eq!(*(diff1.find(&~\"throughput\").get()), LikelyNoise);\n+\n+        // Check that it was not rewritten.\n+        let m3 = MetricMap::load(&pth);\n+        assert_eq!(m3.len(), 2);\n+        assert_eq!(*(m3.find(&~\"runtime\").get()), Metric { value: 1000.0, noise: 2.0 });\n+        assert_eq!(*(m3.find(&~\"throughput\").get()), Metric { value: 50.0, noise: 2.0 });\n+\n+        // Ask for a ratchet with an explicit noise-percentage override,\n+        // that should advance.\n+        let (diff2, ok2) = m2.ratchet(&pth, Some(10.0));\n+        assert_eq!(ok2, true);\n+        assert_eq!(diff2.len(), 2);\n+        assert_eq!(*(diff2.find(&~\"runtime\").get()), LikelyNoise);\n+        assert_eq!(*(diff2.find(&~\"throughput\").get()), LikelyNoise);\n+\n+        // Check that it was rewritten.\n+        let m4 = MetricMap::load(&pth);\n+        assert_eq!(m4.len(), 2);\n+        assert_eq!(*(m4.find(&~\"runtime\").get()), Metric { value: 1100.0, noise: 2.0 });\n+        assert_eq!(*(m4.find(&~\"throughput\").get()), Metric { value: 50.0, noise: 2.0 });\n+\n+        os::remove_dir_recursive(&dpth);\n+    }\n }"}]}