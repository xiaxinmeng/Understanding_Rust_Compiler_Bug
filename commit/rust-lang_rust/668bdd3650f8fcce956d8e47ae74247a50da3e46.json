{"sha": "668bdd3650f8fcce956d8e47ae74247a50da3e46", "node_id": "MDY6Q29tbWl0NzI0NzEyOjY2OGJkZDM2NTBmOGZjY2U5NTZkOGU0N2FlNzQyNDdhNTBkYTNlNDY=", "commit": {"author": {"name": "Alexis Beingessner", "email": "a.beingessner@gmail.com", "date": "2015-07-08T04:19:04Z"}, "committer": {"name": "Alexis Beingessner", "email": "a.beingessner@gmail.com", "date": "2015-07-08T04:19:04Z"}, "message": "flesh out atomics", "tree": {"sha": "22da40dd18e9963dcf3d15e20f2dfe8cfb8bea61", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/22da40dd18e9963dcf3d15e20f2dfe8cfb8bea61"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/668bdd3650f8fcce956d8e47ae74247a50da3e46", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/668bdd3650f8fcce956d8e47ae74247a50da3e46", "html_url": "https://github.com/rust-lang/rust/commit/668bdd3650f8fcce956d8e47ae74247a50da3e46", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/668bdd3650f8fcce956d8e47ae74247a50da3e46/comments", "author": {"login": "Gankra", "id": 1136864, "node_id": "MDQ6VXNlcjExMzY4NjQ=", "avatar_url": "https://avatars.githubusercontent.com/u/1136864?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Gankra", "html_url": "https://github.com/Gankra", "followers_url": "https://api.github.com/users/Gankra/followers", "following_url": "https://api.github.com/users/Gankra/following{/other_user}", "gists_url": "https://api.github.com/users/Gankra/gists{/gist_id}", "starred_url": "https://api.github.com/users/Gankra/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Gankra/subscriptions", "organizations_url": "https://api.github.com/users/Gankra/orgs", "repos_url": "https://api.github.com/users/Gankra/repos", "events_url": "https://api.github.com/users/Gankra/events{/privacy}", "received_events_url": "https://api.github.com/users/Gankra/received_events", "type": "User", "site_admin": false}, "committer": {"login": "Gankra", "id": 1136864, "node_id": "MDQ6VXNlcjExMzY4NjQ=", "avatar_url": "https://avatars.githubusercontent.com/u/1136864?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Gankra", "html_url": "https://github.com/Gankra", "followers_url": "https://api.github.com/users/Gankra/followers", "following_url": "https://api.github.com/users/Gankra/following{/other_user}", "gists_url": "https://api.github.com/users/Gankra/gists{/gist_id}", "starred_url": "https://api.github.com/users/Gankra/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Gankra/subscriptions", "organizations_url": "https://api.github.com/users/Gankra/orgs", "repos_url": "https://api.github.com/users/Gankra/repos", "events_url": "https://api.github.com/users/Gankra/events{/privacy}", "received_events_url": "https://api.github.com/users/Gankra/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "d8f460c29d6b7e07775562e2090f3f36c6d651e0", "url": "https://api.github.com/repos/rust-lang/rust/commits/d8f460c29d6b7e07775562e2090f3f36c6d651e0", "html_url": "https://github.com/rust-lang/rust/commit/d8f460c29d6b7e07775562e2090f3f36c6d651e0"}], "stats": {"total": 214, "additions": 197, "deletions": 17}, "files": [{"sha": "9bafb761321de137125793863e17caba82c27653", "filename": "atomics.md", "status": "modified", "additions": 197, "deletions": 17, "changes": 214, "blob_url": "https://github.com/rust-lang/rust/blob/668bdd3650f8fcce956d8e47ae74247a50da3e46/atomics.md", "raw_url": "https://github.com/rust-lang/rust/raw/668bdd3650f8fcce956d8e47ae74247a50da3e46/atomics.md", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/atomics.md?ref=668bdd3650f8fcce956d8e47ae74247a50da3e46", "patch": "@@ -7,27 +7,138 @@ it is a pragmatic concession to the fact that *everyone* is pretty bad at modeli\n atomics. At very least, we can benefit from existing tooling and research around\n C.\n \n-Trying to fully explain the model is fairly hopeless. If you want all the\n-nitty-gritty details, you should check out [C's specification][C11-model].\n-Still, we'll try to cover the basics and some of the problems Rust developers\n-face.\n+Trying to fully explain the model in this book is fairly hopeless. It's defined\n+in terms of madness-inducing causality graphs that require a full book to properly\n+understand in a practical way. If you want all the nitty-gritty details, you\n+should check out [C's specification][C11-model]. Still, we'll try to cover the\n+basics and some of the problems Rust developers face.\n \n-The C11 memory model is fundamentally about trying to bridge the gap between C's\n-single-threaded semantics, common compiler optimizations, and hardware peculiarities\n-in the face of a multi-threaded environment. It does this by splitting memory\n-accesses into two worlds: data accesses, and atomic accesses.\n+The C11 memory model is fundamentally about trying to bridge the gap between\n+the semantics we want, the optimizations compilers want, and the inconsistent\n+chaos our hardware wants. *We* would like to just write programs and have them\n+do exactly what we said but, you know, *fast*. Wouldn't that be great?\n+\n+\n+\n+\n+# Compiler Reordering\n+\n+Compilers fundamentally want to be able to do all sorts of crazy transformations\n+to reduce data dependencies and eleminate dead code. In particular, they may\n+radically change the actual order of events, or make events never occur! If we\n+write something like\n+\n+```rust,ignore\n+x = 1;\n+y = 3;\n+x = 2;\n+```\n+\n+The compiler may conclude that it would *really* be best if your program did\n+\n+```rust,ignore\n+x = 2;\n+y = 3;\n+```\n+\n+This has inverted the order of events *and* completely eliminated one event. From\n+a single-threaded perspective this is completely unobservable: after all the\n+statements have executed we are in exactly the same state. But if our program is\n+multi-threaded, we may have been relying on `x` to *actually* be assigned to 1 before\n+`y` was assigned. We would *really* like the compiler to be able to make these kinds\n+of optimizations, because they can seriously improve performance. On the other hand,\n+we'd really like to be able to depend on our program *doing the thing we said*.\n+\n+\n+\n+\n+# Hardware Reordering\n+\n+On the other hand, even if the compiler totally understood what we wanted and\n+respected our wishes, our *hardware* might instead get us in trouble. Trouble comes\n+from CPUs in the form of memory hierarchies. There is indeed a global shared memory\n+space somewhere in your hardware, but from the perspective of each CPU core it is\n+*so very far away* and *so very slow*. Each CPU would rather work with its local\n+cache of the data and only go through all the *anguish* of talking to shared\n+memory *only* when it doesn't actually have that memory in cache.\n+\n+After all, that's the whole *point* of the cache, right? If every read from the\n+cache had to run back to shared memory to double check that it hadn't changed,\n+what would the point be? The end result is that the hardware doesn't guarantee\n+that events that occur in the same order on *one* thread, occur in the same order\n+on *another* thread. To guarantee this, we must issue special instructions to\n+the CPU telling it to be a bit less smart.\n+\n+For instance, say we convince the compiler to emit this logic:\n+\n+```text\n+initial state: x = 0, y = 1\n+\n+THREAD 1        THREAD2\n+y = 3;          if x == 1 {\n+x = 1;              y *= 2;\n+                }\n+```\n+\n+Ideally this program has 2 possible final states:\n+\n+* `y = 3`: (thread 2 did the check before thread 1 completed)\n+* `y = 6`: (thread 2 did the check after thread 1 completed)\n+\n+However there's a third potential state that the hardware enables:\n+\n+* `y = 2`: (thread 2 saw `x = 2`, but not `y = 3`, and then overwrote `y = 3`)\n+\n+```\n+\n+It's worth noting that different kinds of CPU provide different guarantees. It\n+is common to seperate hardware into two categories: strongly-ordered and weakly-\n+ordered. Most notably x86/64 provides strong ordering guarantees, while ARM and\n+provides weak ordering guarantees. This has two consequences for\n+concurrent programming:\n+\n+* Asking for stronger guarantees on strongly-ordered hardware may be cheap or\n+  even *free* because they already provide strong guarantees unconditionally.\n+  Weaker guarantees may only yield performance wins on weakly-ordered hardware.\n+\n+* Asking for guarantees that are *too* weak on strongly-ordered hardware\n+  is more likely to *happen* to work, even though your program is strictly\n+  incorrect. If possible, concurrent algorithms should be tested on\n+  weakly-ordered hardware.\n+\n+\n+\n+\n+\n+# Data Accesses\n+\n+The C11 memory model attempts to bridge the gap by allowing us to talk about\n+the *causality* of our program. Generally, this is by establishing a\n+*happens before* relationships between parts of the program and the threads\n+that are running them. This gives the hardware and compiler room to optimize the\n+program more aggressively where a strict happens-before relationship isn't\n+established, but forces them to be more careful where one *is* established.\n+The way we communicate these relationships are through *data accesses* and\n+*atomic accesses*.\n \n Data accesses are the bread-and-butter of the programming world. They are\n fundamentally unsynchronized and compilers are free to aggressively optimize\n-them. In particular data accesses are free to be reordered by the compiler\n+them. In particular, data accesses are free to be reordered by the compiler\n on the assumption that the program is single-threaded. The hardware is also free\n-to propagate the changes made in data accesses as lazily and inconsistently as\n-it wants to other threads. Mostly critically, data accesses are where we get data\n-races. These are pretty clearly awful semantics to try to write a multi-threaded\n-program with.\n+to propagate the changes made in data accesses to other threads\n+as lazily and inconsistently as it wants. Mostly critically, data accesses are\n+how data races happen. Data accesses are very friendly to the hardware and\n+compiler, but as we've seen they offer *awful* semantics to try to\n+write synchronized code with.\n \n-Atomic accesses are the answer to this. Each atomic access can be marked with\n-an *ordering*. The set of orderings Rust exposes are:\n+Atomic accesses are how we tell the hardware and compiler that our program is\n+multi-threaded. Each atomic access can be marked with\n+an *ordering* that specifies what kind of relationship it establishes with\n+other accesses. In practice, this boils down to telling the compiler and hardware\n+certain things they *can't* do. For the compiler, this largely revolves\n+around re-ordering of instructions. For the hardware, this largely revolves\n+around how writes are propagated to other threads. The set of orderings Rust\n+exposes are:\n \n * Sequentially Consistent (SeqCst)\n * Release\n@@ -36,11 +147,80 @@ an *ordering*. The set of orderings Rust exposes are:\n \n (Note: We explicitly do not expose the C11 *consume* ordering)\n \n-TODO: give simple \"basic\" explanation of these\n-TODO: implementing Arc example (why does Drop need the trailing barrier?)\n+TODO: negative reasoning vs positive reasoning?\n+\n+\n+\n+\n+# Sequentially Consistent\n+\n+Sequentially Consistent is the most powerful of all, implying the restrictions\n+of all other orderings. A Sequentially Consistent operation *cannot*\n+be reordered: all accesses on one thread that happen before and after it *stay*\n+before and after it. A program that has sequential consistency has the very nice\n+property that there is a single global execution of the program's instructions\n+that all threads agree on. This execution is also particularly nice to reason\n+about: it's just an interleaving of each thread's individual executions.\n+\n+The relative developer-friendliness of sequential consistency doesn't come for\n+free. Even on strongly-ordered platforms, sequential consistency involves\n+emitting memory fences.\n+\n+In practice, sequential consistency is rarely necessary for program correctness.\n+However sequential consistency is definitely the right choice if you're not\n+confident about the other memory orders. Having your program run a bit slower\n+than it needs to is certainly better than it running incorrectly! It's also\n+completely trivial to downgrade to a weaker consistency later.\n+\n+\n+\n+\n+# Acquire-Release\n \n+Acquire and Release are largely intended to be paired. Their names hint at\n+their use case: they're perfectly suited for acquiring and releasing locks,\n+and ensuring that critical sections don't overlap.\n \n+An acquire access ensures that every access after it *stays* after it. However\n+operations that occur before an acquire are free to be reordered to occur after\n+it.\n \n+A release access ensures that every access before it *stays* before it. However\n+operations that occur after a release are free to be reordered to occur before\n+it.\n+\n+Basic use of release-acquire is simple: you acquire a location of memory to\n+begin the critical section, and the release that location to end it. If\n+thread A releases a location of memory and thread B acquires that location of\n+memory, this establishes that A's critical section *happened before* B's\n+critical section. All accesses that happened before the release will be observed\n+by anything that happens after the acquire.\n+\n+On strongly-ordered platforms most accesses have release or acquire semantics,\n+making release and acquire often totally free. This is not the case on\n+weakly-ordered platforms.\n+\n+\n+\n+\n+# Relaxed\n+\n+Relaxed accesses are the absolute weakest. They can be freely re-ordered and\n+provide no happens-before relationship. Still, relaxed operations *are* still\n+atomic, which is valuable. Relaxed operations are appropriate for things that\n+you definitely want to happen, but don't particularly care about much else. For\n+instance, incrementing a counter can be relaxed if you're not using the\n+counter to synchronize any other accesses.\n+\n+There's rarely a benefit in making an operation relaxed on strongly-ordered\n+platforms, since they usually provide release-acquire semantics anyway. However\n+relaxed operations can be cheaper on weakly-ordered platforms.\n+\n+\n+\n+\n+\n+TODO: implementing Arc example (why does Drop need the trailing barrier?)\n \n \n [C11-busted]: http://plv.mpi-sws.org/c11comp/popl15.pdf"}]}