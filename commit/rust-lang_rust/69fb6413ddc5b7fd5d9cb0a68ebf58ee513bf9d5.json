{"sha": "69fb6413ddc5b7fd5d9cb0a68ebf58ee513bf9d5", "node_id": "MDY6Q29tbWl0NzI0NzEyOjY5ZmI2NDEzZGRjNWI3ZmQ1ZDljYjBhNjhlYmY1OGVlNTEzYmY5ZDU=", "commit": {"author": {"name": "JCTyBlaidd", "email": "JCTyblaidd@users.noreply.github.com", "date": "2020-11-15T18:30:26Z"}, "committer": {"name": "JCTyBlaidd", "email": "JCTyblaidd@users.noreply.github.com", "date": "2020-11-15T18:30:26Z"}, "message": "Tidy up comments and function layout, should fix most of the review notes.", "tree": {"sha": "11d027ff944b1811fb2cd44113e1a128546e721a", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/11d027ff944b1811fb2cd44113e1a128546e721a"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/69fb6413ddc5b7fd5d9cb0a68ebf58ee513bf9d5", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/69fb6413ddc5b7fd5d9cb0a68ebf58ee513bf9d5", "html_url": "https://github.com/rust-lang/rust/commit/69fb6413ddc5b7fd5d9cb0a68ebf58ee513bf9d5", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/69fb6413ddc5b7fd5d9cb0a68ebf58ee513bf9d5/comments", "author": {"login": "JCTyblaidd", "id": 8288600, "node_id": "MDQ6VXNlcjgyODg2MDA=", "avatar_url": "https://avatars.githubusercontent.com/u/8288600?v=4", "gravatar_id": "", "url": "https://api.github.com/users/JCTyblaidd", "html_url": "https://github.com/JCTyblaidd", "followers_url": "https://api.github.com/users/JCTyblaidd/followers", "following_url": "https://api.github.com/users/JCTyblaidd/following{/other_user}", "gists_url": "https://api.github.com/users/JCTyblaidd/gists{/gist_id}", "starred_url": "https://api.github.com/users/JCTyblaidd/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/JCTyblaidd/subscriptions", "organizations_url": "https://api.github.com/users/JCTyblaidd/orgs", "repos_url": "https://api.github.com/users/JCTyblaidd/repos", "events_url": "https://api.github.com/users/JCTyblaidd/events{/privacy}", "received_events_url": "https://api.github.com/users/JCTyblaidd/received_events", "type": "User", "site_admin": false}, "committer": {"login": "JCTyblaidd", "id": 8288600, "node_id": "MDQ6VXNlcjgyODg2MDA=", "avatar_url": "https://avatars.githubusercontent.com/u/8288600?v=4", "gravatar_id": "", "url": "https://api.github.com/users/JCTyblaidd", "html_url": "https://github.com/JCTyblaidd", "followers_url": "https://api.github.com/users/JCTyblaidd/followers", "following_url": "https://api.github.com/users/JCTyblaidd/following{/other_user}", "gists_url": "https://api.github.com/users/JCTyblaidd/gists{/gist_id}", "starred_url": "https://api.github.com/users/JCTyblaidd/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/JCTyblaidd/subscriptions", "organizations_url": "https://api.github.com/users/JCTyblaidd/orgs", "repos_url": "https://api.github.com/users/JCTyblaidd/repos", "events_url": "https://api.github.com/users/JCTyblaidd/events{/privacy}", "received_events_url": "https://api.github.com/users/JCTyblaidd/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "2a40d9b7a07f9a770455de26e46b766bdb395206", "url": "https://api.github.com/repos/rust-lang/rust/commits/2a40d9b7a07f9a770455de26e46b766bdb395206", "html_url": "https://github.com/rust-lang/rust/commit/2a40d9b7a07f9a770455de26e46b766bdb395206"}], "stats": {"total": 1800, "additions": 1005, "deletions": 795}, "files": [{"sha": "1117b69116a5d71ecd9fc7091d59a9affb01ee7f", "filename": "src/bin/miri.rs", "status": "modified", "additions": 3, "deletions": 0, "changes": 3, "blob_url": "https://github.com/rust-lang/rust/blob/69fb6413ddc5b7fd5d9cb0a68ebf58ee513bf9d5/src%2Fbin%2Fmiri.rs", "raw_url": "https://github.com/rust-lang/rust/raw/69fb6413ddc5b7fd5d9cb0a68ebf58ee513bf9d5/src%2Fbin%2Fmiri.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Fbin%2Fmiri.rs?ref=69fb6413ddc5b7fd5d9cb0a68ebf58ee513bf9d5", "patch": "@@ -195,6 +195,9 @@ fn main() {\n                 \"-Zmiri-disable-stacked-borrows\" => {\n                     miri_config.stacked_borrows = false;\n                 }\n+                \"-Zmiri-disable-data-race-detector\" => {\n+                    miri_config.data_race_detector = false;\n+                }\n                 \"-Zmiri-disable-alignment-check\" => {\n                     miri_config.check_alignment = miri::AlignmentCheck::None;\n                 }"}, {"sha": "822ceab8fa04c37f75314e0cc541788b661e4bc2", "filename": "src/data_race.rs", "status": "modified", "additions": 620, "deletions": 484, "changes": 1104, "blob_url": "https://github.com/rust-lang/rust/blob/69fb6413ddc5b7fd5d9cb0a68ebf58ee513bf9d5/src%2Fdata_race.rs", "raw_url": "https://github.com/rust-lang/rust/raw/69fb6413ddc5b7fd5d9cb0a68ebf58ee513bf9d5/src%2Fdata_race.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Fdata_race.rs?ref=69fb6413ddc5b7fd5d9cb0a68ebf58ee513bf9d5", "patch": "@@ -1,16 +1,36 @@\n-//! Implementation of a data-race detector\n-//!  uses Lamport Timestamps / Vector-clocks\n-//!  base on the Dyamic Race Detection for C++:\n-//!     - https://www.doc.ic.ac.uk/~afd/homepages/papers/pdfs/2017/POPL.pdf\n-//!  to extend data-race detection to work correctly with fences\n-//!  and RMW operations\n+//! Implementation of a data-race detector using Lamport Timestamps / Vector-clocks\n+//! based on the Dyamic Race Detection for C++:\n+//! https://www.doc.ic.ac.uk/~afd/homepages/papers/pdfs/2017/POPL.pdf\n+//! which does not report false-positives when fences are used, and gives better\n+//! accuracy in presence of read-modify-write operations.\n+//!\n //! This does not explore weak memory orders and so can still miss data-races\n-//!  but should not report false-positives\n+//! but should not report false-positives\n+//!\n //! Data-race definiton from(https://en.cppreference.com/w/cpp/language/memory_model#Threads_and_data_races):\n-//!  - if a memory location is accessed by twice is a data-race unless:\n-//!    - both operations execute on the same thread/signal-handler\n-//!    - both conflicting operations are atomic operations (1 atomic and 1 non-atomic race)\n-//!    - 1 of the operations happens-before the other operation (see link for definition)\n+//! a data race occurs between two memory accesses if they are on different threads, at least one operation\n+//! is non-atomic, at least one operation is a write and neither access happens-before the other. Read the link\n+//! for full definition.\n+//! \n+//! This re-uses vector indexes for threads that are known to be unable to report data-races, this is valid\n+//! because it only re-uses vector indexes once all currently-active (not-terminated) threads have an internal\n+//! vector clock that happens-after the join operation of the candidate thread. Threads that have not been joined\n+//! on are not considered. Since the thread's vector clock will only increase and a data-race implies that\n+//! there is some index x where clock[x] > thread_clock, when this is true clock[candidate-idx] > thread_clock\n+//! can never hold and hence a data-race can never be reported in that vector index again.\n+//! This means that the thread-index can be safely re-used, starting on the next timestamp for the newly created\n+//! thread.\n+//!\n+//! The sequentially consistant ordering corresponds to the ordering that the threads\n+//! are currently scheduled, this means that the data-race detector has no additional\n+//! logic for sequentially consistent accesses at the moment since they are indistinguishable\n+//! from acquire/release operations. If weak memory orderings are explored then this\n+//! may need to change or be updated accordingly.\n+//!\n+//! FIXME:\n+//! currently we have our own local copy of the currently active thread index and names, this is due\n+//! in part to the inability to access the current location of threads.active_thread inside the AllocExtra\n+//! read, write and deallocate functions and should be cleaned up in the future.\n \n use std::{\n     fmt::Debug, rc::Rc,\n@@ -19,39 +39,39 @@ use std::{\n \n use rustc_index::vec::{Idx, IndexVec};\n use rustc_target::abi::Size;\n-use rustc_middle::ty::layout::TyAndLayout;\n+use rustc_middle::{mir, ty::layout::TyAndLayout};\n use rustc_data_structures::fx::{FxHashSet, FxHashMap};\n \n use crate::{\n     MiriEvalContext, MiriEvalContextExt,\n     ThreadId, Tag, RangeMap,\n     InterpResult, Pointer, ScalarMaybeUninit,\n-    MPlaceTy, OpTy, MemPlaceMeta,\n-    VClock, VSmallClockSet, VectorIdx, VTimestamp\n+    MPlaceTy, OpTy, MemPlaceMeta, ImmTy, Immediate,\n+    VClock, VSmallClockMap, VectorIdx, VTimestamp\n };\n \n pub type AllocExtra = VClockAlloc;\n pub type MemoryExtra = Rc<GlobalState>;\n \n-/// Valid atomic read-write operations, alias of atomic::Ordering (not non-exhaustive)\n+/// Valid atomic read-write operations, alias of atomic::Ordering (not non-exhaustive).\n #[derive(Copy, Clone, PartialEq, Eq, Debug)]\n-pub enum AtomicRWOp {\n+pub enum AtomicRwOp {\n     Relaxed,\n     Acquire,\n     Release,\n     AcqRel,\n     SeqCst,\n }\n \n-/// Valid atomic read operations, subset of atomic::Ordering\n+/// Valid atomic read operations, subset of atomic::Ordering.\n #[derive(Copy, Clone, PartialEq, Eq, Debug)]\n pub enum AtomicReadOp {\n     Relaxed,\n     Acquire,\n     SeqCst,\n }\n \n-/// Valid atomic write operations, subset of atomic::Ordering\n+/// Valid atomic write operations, subset of atomic::Ordering.\n #[derive(Copy, Clone, PartialEq, Eq, Debug)]\n pub enum AtomicWriteOp {\n     Relaxed,\n@@ -60,7 +80,7 @@ pub enum AtomicWriteOp {\n }\n \n \n-/// Valid atomic fence operations, subset of atomic::Ordering\n+/// Valid atomic fence operations, subset of atomic::Ordering.\n #[derive(Copy, Clone, PartialEq, Eq, Debug)]\n pub enum AtomicFenceOp {\n     Acquire,\n@@ -69,315 +89,124 @@ pub enum AtomicFenceOp {\n     SeqCst,\n }\n \n-/// Evaluation context extensions\n-impl<'mir, 'tcx: 'mir> EvalContextExt<'mir, 'tcx> for MiriEvalContext<'mir, 'tcx> {}\n-pub trait EvalContextExt<'mir, 'tcx: 'mir>: MiriEvalContextExt<'mir, 'tcx> {\n \n-    // Temporarily allow data-races to occur, this should only be\n-    //  used if either one of the appropiate `validate_atomic` functions\n-    //  will be called to treat a memory access as atomic or if the memory\n-    //  being accessed should be treated as internal state, that cannot be\n-    //  accessed by the interpreted program.\n-    #[inline]\n-    fn allow_data_races_ref<R>(&self, op: impl FnOnce(&MiriEvalContext<'mir, 'tcx>) -> R) -> R {\n-        let this = self.eval_context_ref();\n-        let data_race = &*this.memory.extra.data_race;\n-        let old = data_race.multi_threaded.replace(false);\n-        let result = op(this);\n-        data_race.multi_threaded.set(old);\n-        result\n-    }\n \n-    /// Same as `allow_data_races_ref`, this temporarily disables any data-race detection and\n-    ///  so should only be used for atomic operations or internal state that the program cannot\n-    ///  access\n-    #[inline]\n-    fn allow_data_races_mut<R>(&mut self, op: impl FnOnce(&mut MiriEvalContext<'mir, 'tcx>) -> R) -> R {\n-        let this = self.eval_context_mut();\n-        let data_race = &*this.memory.extra.data_race;\n-        let old = data_race.multi_threaded.replace(false);\n-        let result = op(this);\n-        let data_race = &*this.memory.extra.data_race;\n-        data_race.multi_threaded.set(old);\n-        result\n-    }\n-\n-\n-    fn read_scalar_at_offset_atomic(\n-        &self,\n-        op: OpTy<'tcx, Tag>,\n-        offset: u64,\n-        layout: TyAndLayout<'tcx>,\n-        atomic: AtomicReadOp\n-    ) -> InterpResult<'tcx, ScalarMaybeUninit<Tag>> {\n-        let this = self.eval_context_ref();\n-        let op_place = this.deref_operand(op)?;\n-        let offset = Size::from_bytes(offset);\n-        // Ensure that the following read at an offset is within bounds\n-        assert!(op_place.layout.size >= offset + layout.size);\n-        let value_place = op_place.offset(offset, MemPlaceMeta::None, layout, this)?;\n-        this.read_scalar_atomic(value_place, atomic)\n-    }\n-    fn write_scalar_at_offset_atomic(\n-        &mut self,\n-        op: OpTy<'tcx, Tag>,\n-        offset: u64,\n-        value: impl Into<ScalarMaybeUninit<Tag>>,\n-        layout: TyAndLayout<'tcx>,\n-        atomic: AtomicWriteOp\n-    ) -> InterpResult<'tcx> {\n-        let this = self.eval_context_mut();\n-        let op_place = this.deref_operand(op)?;\n-        let offset = Size::from_bytes(offset);\n-        // Ensure that the following read at an offset is within bounds\n-        assert!(op_place.layout.size >= offset + layout.size);\n-        let value_place = op_place.offset(offset, MemPlaceMeta::None, layout, this)?;\n-        this.write_scalar_atomic(value.into(), value_place, atomic)\n-    }\n-    fn read_scalar_atomic(\n-        &self, place: MPlaceTy<'tcx, Tag>, atomic: AtomicReadOp\n-    ) -> InterpResult<'tcx, ScalarMaybeUninit<Tag>> {\n-        let scalar = self.allow_data_races_ref(move |this| {\n-            this.read_scalar(place.into())\n-        })?;\n-        self.validate_atomic_load(place, atomic)?;\n-        Ok(scalar)\n-    }\n-    fn write_scalar_atomic(\n-        &mut self, val: ScalarMaybeUninit<Tag>, dest: MPlaceTy<'tcx, Tag>,\n-        atomic: AtomicWriteOp\n-    ) -> InterpResult<'tcx> {\n-        self.allow_data_races_mut(move |this| {\n-            this.write_scalar(val, dest.into())\n-        })?;\n-        self.validate_atomic_store(dest, atomic)\n-    }\n-    \n-    /// Update the data-race detector for an atomic read occuring at the\n-    ///  associated memory-place and on the current thread\n-    fn validate_atomic_load(\n-        &self, place: MPlaceTy<'tcx, Tag>, atomic: AtomicReadOp\n-    ) -> InterpResult<'tcx> {\n-        let this = self.eval_context_ref();\n-        this.validate_atomic_op(\n-            place, atomic, \"Atomic Load\",\n-            move |memory, clocks, index, atomic| {\n-                if atomic == AtomicReadOp::Relaxed {\n-                    memory.load_relaxed(&mut *clocks, index)\n-                }else{\n-                    memory.acquire(&mut *clocks, index)\n-                }\n-            }\n-        )\n-    }\n+/// The current set of vector clocks describing the state\n+/// of a thread, contains the happens-before clock and\n+/// additional metadata to model atomic fence operations.\n+#[derive(Clone, Default, Debug)]\n+struct ThreadClockSet {\n \n-    /// Update the data-race detector for an atomic write occuring at the\n-    ///  associated memory-place and on the current thread\n-    fn validate_atomic_store(\n-        &mut self, place: MPlaceTy<'tcx, Tag>, atomic: AtomicWriteOp\n-    ) -> InterpResult<'tcx> {\n-        let this = self.eval_context_ref();\n-        this.validate_atomic_op(\n-            place, atomic, \"Atomic Store\",\n-            move |memory, clocks, index, atomic| {\n-                if atomic == AtomicWriteOp::Relaxed {\n-                    memory.store_relaxed(clocks, index)\n-                }else{\n-                    memory.release(clocks, index)\n-                }\n-            }\n-        )\n-    }\n+    /// The increasing clock representing timestamps\n+    /// that happen-before this thread.\n+    clock: VClock,\n \n-    /// Update the data-race detector for an atomic read-modify-write occuring\n-    ///  at the associated memory place and on the current thread\n-    fn validate_atomic_rmw(\n-        &mut self, place: MPlaceTy<'tcx, Tag>, atomic: AtomicRWOp\n-    ) -> InterpResult<'tcx> {\n-        use AtomicRWOp::*;\n-        let acquire = matches!(atomic, Acquire | AcqRel | SeqCst);\n-        let release = matches!(atomic, Release | AcqRel | SeqCst);\n-        let this = self.eval_context_ref();\n-        this.validate_atomic_op(\n-            place, atomic, \"Atomic RMW\",\n-            move |memory, clocks, index, _| {\n-                if acquire {\n-                    memory.acquire(clocks, index)?;\n-                }else{\n-                    memory.load_relaxed(clocks, index)?;\n-                }\n-                if release {\n-                    memory.rmw_release(clocks, index)\n-                }else{\n-                    memory.rmw_relaxed(clocks, index)\n-                }\n-            }\n-        )\n-    }\n+    /// The set of timestamps that will happen-before this\n+    /// thread once it performs an acquire fence.\n+    fence_acquire: VClock,\n \n-    /// Update the data-race detector for an atomic fence on the current thread\n-    fn validate_atomic_fence(&mut self, atomic: AtomicFenceOp) -> InterpResult<'tcx> {\n-        let this = self.eval_context_mut();\n-        let data_race = &*this.memory.extra.data_race;\n-        data_race.maybe_perform_sync_operation(move |index, mut clocks| {\n-            log::trace!(\"Atomic fence on {:?} with ordering {:?}\", index, atomic);\n-            // Apply data-race detection for the current fences\n-            //  this treats AcqRel and SeqCst as the same as a acquire\n-            //  and release fence applied in the same timestamp.\n-            if atomic != AtomicFenceOp::Release {\n-                // Either Acquire | AcqRel | SeqCst\n-                clocks.apply_acquire_fence();\n-            }\n-            if atomic != AtomicFenceOp::Acquire {\n-                // Either Release | AcqRel | SeqCst\n-                clocks.apply_release_fence();\n-            }\n-            Ok(())\n-        })\n-    }\n+    /// The last timesamp of happens-before relations that\n+    /// have been released by this thread by a fence.\n+    fence_release: VClock,\n }\n \n-impl<'mir, 'tcx: 'mir> EvalContextPrivExt<'mir, 'tcx> for MiriEvalContext<'mir, 'tcx> {}\n-trait EvalContextPrivExt<'mir, 'tcx: 'mir>: MiriEvalContextExt<'mir, 'tcx> {\n \n-    /// Generic atomic operation implementation,\n-    ///  this accesses memory via get_raw instead of\n-    ///  get_raw_mut, due to issues calling get_raw_mut\n-    ///  for atomic loads from read-only memory\n-    /// FIXME: is this valid, or should get_raw_mut be used for\n-    ///  atomic-stores/atomic-rmw?\n-    fn validate_atomic_op<A: Debug + Copy>(\n-        &self, place: MPlaceTy<'tcx, Tag>,\n-        atomic: A, description: &str,\n-        mut op: impl FnMut(\n-            &mut MemoryCellClocks, &mut ThreadClockSet, VectorIdx, A\n-        ) -> Result<(), DataRace>\n-    ) -> InterpResult<'tcx> {\n-        let this = self.eval_context_ref();\n-        let data_race = &*this.memory.extra.data_race;\n-        if data_race.multi_threaded.get() {\n-\n-            // Load an log the atomic operation\n-            let place_ptr = place.ptr.assert_ptr();\n-            let size = place.layout.size;\n-            let alloc_meta =  &this.memory.get_raw(place_ptr.alloc_id)?.extra.data_race;\n-            log::trace!(\n-                \"Atomic op({}) with ordering {:?} on memory({:?}, offset={}, size={})\",\n-                description, &atomic, place_ptr.alloc_id, place_ptr.offset.bytes(), size.bytes()\n-            );\n-\n-            // Perform the atomic operation\n-            let data_race = &alloc_meta.global;\n-            data_race.maybe_perform_sync_operation(|index, mut clocks| {\n-                for (_,range) in alloc_meta.alloc_ranges.borrow_mut().iter_mut(place_ptr.offset, size) {\n-                    if let Err(DataRace) = op(range, &mut *clocks, index, atomic) {\n-                        mem::drop(clocks);\n-                        return VClockAlloc::report_data_race(\n-                            &alloc_meta.global, range, description, true,\n-                            place_ptr, size\n-                        );\n-                    }\n-                }\n-                Ok(())\n-            })?;\n+impl ThreadClockSet {\n \n-            // Log changes to atomic memory\n-            if log::log_enabled!(log::Level::Trace) {\n-                for (_,range) in alloc_meta.alloc_ranges.borrow().iter(place_ptr.offset, size) {\n-                    log::trace!(\n-                        \"Updated atomic memory({:?}, offset={}, size={}) to {:#?}\",\n-                        place.ptr.assert_ptr().alloc_id, place_ptr.offset.bytes(), size.bytes(),\n-                        range.atomic_ops\n-                    );\n-                }\n-            }\n-        }\n-        Ok(())\n+    /// Apply the effects of a release fence to this\n+    /// set of thread vector clocks.\n+    #[inline]\n+    fn apply_release_fence(&mut self) {\n+        self.fence_release.clone_from(&self.clock);\n     }\n \n-}\n-\n-/// Handle for locks to express their\n-///  acquire-release semantics\n-#[derive(Clone, Debug, Default)]\n-pub struct DataRaceLockHandle {\n+    /// Apply the effects of a acquire fence to this\n+    /// set of thread vector clocks.\n+    #[inline]\n+    fn apply_acquire_fence(&mut self) {\n+        self.clock.join(&self.fence_acquire);\n+    }\n \n-    /// Internal acquire-release clock\n-    ///  to express the acquire release sync\n-    ///  found in concurrency primitives\n-    clock: VClock,\n-}\n-impl DataRaceLockHandle {\n-    pub fn set_values(&mut self, other: &Self) {\n-        self.clock.clone_from(&other.clock)\n+    /// Increment the happens-before clock at a\n+    /// known index.\n+    #[inline]\n+    fn increment_clock(&mut self, index: VectorIdx) {\n+        self.clock.increment_index(index);\n     }\n-    pub fn reset(&mut self) {\n-        self.clock.set_zero_vector();\n+\n+    /// Join the happens-before clock with that of\n+    /// another thread, used to model thread join\n+    /// operations.\n+    fn join_with(&mut self, other: &ThreadClockSet) {\n+        self.clock.join(&other.clock);\n     }\n }\n \n \n /// Error returned by finding a data race\n-///  should be elaborated upon\n+/// should be elaborated upon.\n #[derive(Copy, Clone, PartialEq, Eq, PartialOrd, Ord, Hash, Debug)]\n pub struct DataRace;\n \n /// Externally stored memory cell clocks\n-///  explicitly to reduce memory usage for the\n-///  common case where no atomic operations\n-///  exists on the memory cell\n+/// explicitly to reduce memory usage for the\n+/// common case where no atomic operations\n+/// exists on the memory cell.\n #[derive(Clone, PartialEq, Eq, Default, Debug)]\n struct AtomicMemoryCellClocks {\n \n-    /// The clock-vector for the set of atomic read operations\n-    ///  used for detecting data-races with non-atomic write\n-    ///  operations\n+    /// The clock-vector of the timestamp of the last atomic\n+    /// read operation performed by each thread.\n+    /// This detects potential data-races between atomic read\n+    /// and non-atomic write operations.\n     read_vector: VClock,\n \n-    /// The clock-vector for the set of atomic write operations\n-    ///  used for detecting data-races with non-atomic read or\n-    ///  write operations\n+    /// The clock-vector of the timestamp of the last atomic\n+    /// write operation performed by each thread.\n+    /// This detects potential data-races between atomic write\n+    /// and non-atomic read or write operations.\n     write_vector: VClock,\n \n     /// Synchronization vector for acquire-release semantics\n-    ///   contains the vector of timestamps that will\n-    ///   happen-before a thread if an acquire-load is \n-    ///   performed on the data\n+    /// contains the vector of timestamps that will\n+    /// happen-before a thread if an acquire-load is \n+    /// performed on the data.\n     sync_vector: VClock,\n \n     /// The Hash-Map of all threads for which a release\n-    ///  sequence exists in the memory cell, required\n-    ///  since read-modify-write operations do not\n-    ///  invalidate existing release sequences \n-    release_sequences: VSmallClockSet,\n+    /// sequence exists in the memory cell, required\n+    /// since read-modify-write operations do not\n+    /// invalidate existing release sequences.\n+    /// See page 6 of linked paper.\n+    release_sequences: VSmallClockMap,\n }\n \n /// Memory Cell vector clock metadata\n-///  for data-race detection\n+/// for data-race detection.\n #[derive(Clone, PartialEq, Eq, Debug)]\n struct MemoryCellClocks {\n \n-    /// The vector-clock of the last write, only one value is stored\n-    ///  since all previous writes happened-before the current write\n+    /// The vector-clock timestamp of the last write\n+    /// corresponding to the writing threads timestamp.\n     write: VTimestamp,\n \n-    /// The identifier of the thread that performed the last write\n-    ///  operation\n+    /// The identifier of the vector index, corresponding to a thread\n+    /// that performed the last write operation.\n     write_index: VectorIdx,\n \n-    /// The vector-clock of the set of previous reads\n-    ///  each index is set to the timestamp that the associated\n-    ///  thread last read this value.\n+    /// The vector-clock of the timestamp of the last read operation\n+    /// performed by a thread since the last write operation occured.\n     read: VClock,\n \n-    /// Atomic acquire & release sequence tracking clocks\n-    ///  for non-atomic memory in the common case this\n-    ///  value is set to None\n+    /// Atomic acquire & release sequence tracking clocks.\n+    /// For non-atomic memory in the common case this\n+    /// value is set to None.\n     atomic_ops: Option<Box<AtomicMemoryCellClocks>>,\n }\n \n+\n /// Create a default memory cell clocks instance\n-///  for uninitialized memory\n+/// for uninitialized memory.\n impl Default for MemoryCellClocks {\n     fn default() -> Self {\n         MemoryCellClocks {\n@@ -389,9 +218,10 @@ impl Default for MemoryCellClocks {\n     }\n }\n \n+\n impl MemoryCellClocks {\n \n-    /// Load the internal atomic memory cells if they exist\n+    /// Load the internal atomic memory cells if they exist.\n     #[inline]\n     fn atomic(&self) -> Option<&AtomicMemoryCellClocks> {\n         match &self.atomic_ops {\n@@ -401,25 +231,26 @@ impl MemoryCellClocks {\n     }\n \n     /// Load or create the internal atomic memory metadata\n-    ///  if it does not exist\n+    /// if it does not exist.\n     #[inline]\n     fn atomic_mut(&mut self) -> &mut AtomicMemoryCellClocks {\n         self.atomic_ops.get_or_insert_with(Default::default)\n     }\n \n     /// Update memory cell data-race tracking for atomic\n-    ///  load acquire semantics, is a no-op if this memory was\n-    ///  not used previously as atomic memory\n-    fn acquire(&mut self, clocks: &mut ThreadClockSet, index: VectorIdx) -> Result<(), DataRace> {\n+    /// load acquire semantics, is a no-op if this memory was\n+    /// not used previously as atomic memory.\n+    fn load_acquire(&mut self, clocks: &mut ThreadClockSet, index: VectorIdx) -> Result<(), DataRace> {\n         self.atomic_read_detect(clocks, index)?;\n         if let Some(atomic) = self.atomic() {\n             clocks.clock.join(&atomic.sync_vector);\n         }\n         Ok(())\n     }\n+\n     /// Update memory cell data-race tracking for atomic\n-    ///  load relaxed semantics, is a no-op if this memory was\n-    ///  not used previously as atomic memory\n+    /// load relaxed semantics, is a no-op if this memory was\n+    /// not used previously as atomic memory.\n     fn load_relaxed(&mut self, clocks: &mut ThreadClockSet, index: VectorIdx) -> Result<(), DataRace> {\n         self.atomic_read_detect(clocks, index)?;\n         if let Some(atomic) = self.atomic() {\n@@ -430,17 +261,18 @@ impl MemoryCellClocks {\n \n \n     /// Update the memory cell data-race tracking for atomic\n-    ///  store release semantics\n-    fn release(&mut self, clocks: &ThreadClockSet, index: VectorIdx) -> Result<(), DataRace> {\n+    /// store release semantics.\n+    fn store_release(&mut self, clocks: &ThreadClockSet, index: VectorIdx) -> Result<(), DataRace> {\n         self.atomic_write_detect(clocks, index)?;\n         let atomic = self.atomic_mut();\n         atomic.sync_vector.clone_from(&clocks.clock);\n         atomic.release_sequences.clear();\n         atomic.release_sequences.insert(index, &clocks.clock);\n         Ok(())\n     }\n+\n     /// Update the memory cell data-race tracking for atomic\n-    ///  store relaxed semantics\n+    /// store relaxed semantics.\n     fn store_relaxed(&mut self, clocks: &ThreadClockSet, index: VectorIdx) -> Result<(), DataRace> {\n         self.atomic_write_detect(clocks, index)?;\n         let atomic = self.atomic_mut();\n@@ -451,17 +283,19 @@ impl MemoryCellClocks {\n         atomic.release_sequences.retain_index(index);\n         Ok(())\n     }\n+\n     /// Update the memory cell data-race tracking for atomic\n-    ///  store release semantics for RMW operations\n+    /// store release semantics for RMW operations.\n     fn rmw_release(&mut self, clocks: &ThreadClockSet, index: VectorIdx) -> Result<(), DataRace> {\n         self.atomic_write_detect(clocks, index)?;\n         let atomic = self.atomic_mut();\n         atomic.sync_vector.join(&clocks.clock);\n         atomic.release_sequences.insert(index, &clocks.clock);\n         Ok(())\n     }\n+\n     /// Update the memory cell data-race tracking for atomic\n-    ///  store relaxed semantics for RMW operations\n+    /// store relaxed semantics for RMW operations.\n     fn rmw_relaxed(&mut self, clocks: &ThreadClockSet, index: VectorIdx) -> Result<(), DataRace> {\n         self.atomic_write_detect(clocks, index)?;\n         let atomic = self.atomic_mut();\n@@ -470,91 +304,330 @@ impl MemoryCellClocks {\n     }\n     \n     /// Detect data-races with an atomic read, caused by a non-atomic write that does\n-    ///  not happen-before the atomic-read\n+    /// not happen-before the atomic-read.\n     fn atomic_read_detect(&mut self, clocks: &ThreadClockSet, index: VectorIdx) -> Result<(), DataRace> {\n         log::trace!(\"Atomic read with vectors: {:#?} :: {:#?}\", self, clocks);\n         if self.write <= clocks.clock[self.write_index] {\n             let atomic = self.atomic_mut();\n             atomic.read_vector.set_at_index(&clocks.clock, index);\n             Ok(())\n-        }else{\n+        } else {\n             Err(DataRace)\n         }\n     }\n \n     /// Detect data-races with an atomic write, either with a non-atomic read or with\n-    ///  a non-atomic write:\n+    /// a non-atomic write.\n     fn atomic_write_detect(&mut self, clocks: &ThreadClockSet, index: VectorIdx) -> Result<(), DataRace> {\n         log::trace!(\"Atomic write with vectors: {:#?} :: {:#?}\", self, clocks);\n         if self.write <= clocks.clock[self.write_index] && self.read <= clocks.clock {\n             let atomic = self.atomic_mut();\n             atomic.write_vector.set_at_index(&clocks.clock, index);\n             Ok(())\n-        }else{\n+        } else {\n             Err(DataRace)\n         }\n     }\n \n     /// Detect races for non-atomic read operations at the current memory cell\n-    ///  returns true if a data-race is detected\n+    /// returns true if a data-race is detected.\n     fn read_race_detect(&mut self, clocks: &ThreadClockSet, index: VectorIdx) -> Result<(), DataRace> {\n         log::trace!(\"Unsynchronized read with vectors: {:#?} :: {:#?}\", self, clocks);\n         if self.write <= clocks.clock[self.write_index] {\n             let race_free = if let Some(atomic) = self.atomic() {\n                 atomic.write_vector <= clocks.clock\n-            }else{\n+            } else {\n                 true\n             };\n             if race_free {\n                 self.read.set_at_index(&clocks.clock, index);\n                 Ok(())\n-            }else{\n+            } else {\n                 Err(DataRace)\n             }\n-        }else{\n+        } else {\n             Err(DataRace)\n         }\n     }\n \n     /// Detect races for non-atomic write operations at the current memory cell\n-    ///  returns true if a data-race is detected\n+    /// returns true if a data-race is detected.\n     fn write_race_detect(&mut self, clocks: &ThreadClockSet, index: VectorIdx)  -> Result<(), DataRace> {\n         log::trace!(\"Unsynchronized write with vectors: {:#?} :: {:#?}\", self, clocks);\n         if self.write <= clocks.clock[self.write_index] && self.read <= clocks.clock {\n             let race_free = if let Some(atomic) = self.atomic() {\n                 atomic.write_vector <= clocks.clock && atomic.read_vector <= clocks.clock\n-            }else{\n+            } else {\n                 true\n             };\n             if race_free {\n                 self.write = clocks.clock[index];\n                 self.write_index = index;\n                 self.read.set_zero_vector();\n                 Ok(())\n-            }else{\n+            } else {\n                 Err(DataRace)\n             }\n-        }else{\n+        } else {\n             Err(DataRace)\n         }\n     }\n }\n \n-/// Vector clock metadata for a logical memory allocation\n+\n+/// Evaluation context extensions.\n+impl<'mir, 'tcx: 'mir> EvalContextExt<'mir, 'tcx> for MiriEvalContext<'mir, 'tcx> {}\n+pub trait EvalContextExt<'mir, 'tcx: 'mir>: MiriEvalContextExt<'mir, 'tcx> {\n+\n+    /// Atomic variant of read_scalar_at_offset.\n+    fn read_scalar_at_offset_atomic(\n+        &self,\n+        op: OpTy<'tcx, Tag>,\n+        offset: u64,\n+        layout: TyAndLayout<'tcx>,\n+        atomic: AtomicReadOp\n+    ) -> InterpResult<'tcx, ScalarMaybeUninit<Tag>> {\n+        let this = self.eval_context_ref();\n+        let op_place = this.deref_operand(op)?;\n+        let offset = Size::from_bytes(offset);\n+\n+        // Ensure that the following read at an offset is within bounds.\n+        assert!(op_place.layout.size >= offset + layout.size);\n+        let value_place = op_place.offset(offset, MemPlaceMeta::None, layout, this)?;\n+        this.read_scalar_atomic(value_place, atomic)\n+    }\n+\n+    /// Atomic variant of write_scalar_at_offset.\n+    fn write_scalar_at_offset_atomic(\n+        &mut self,\n+        op: OpTy<'tcx, Tag>,\n+        offset: u64,\n+        value: impl Into<ScalarMaybeUninit<Tag>>,\n+        layout: TyAndLayout<'tcx>,\n+        atomic: AtomicWriteOp\n+    ) -> InterpResult<'tcx> {\n+        let this = self.eval_context_mut();\n+        let op_place = this.deref_operand(op)?;\n+        let offset = Size::from_bytes(offset);\n+\n+        // Ensure that the following read at an offset is within bounds.\n+        assert!(op_place.layout.size >= offset + layout.size);\n+        let value_place = op_place.offset(offset, MemPlaceMeta::None, layout, this)?;\n+        this.write_scalar_atomic(value.into(), value_place, atomic)\n+    }\n+\n+    /// Perform an atomic read operation at the memory location.\n+    fn read_scalar_atomic(\n+        &self, place: MPlaceTy<'tcx, Tag>, atomic: AtomicReadOp\n+    ) -> InterpResult<'tcx, ScalarMaybeUninit<Tag>> {\n+        let this = self.eval_context_ref();\n+        let scalar = this.allow_data_races_ref(move |this| {\n+            this.read_scalar(place.into())\n+        })?;\n+        self.validate_atomic_load(place, atomic)?;\n+        Ok(scalar)\n+    }\n+\n+    /// Perform an atomic write operation at the memory location.\n+    fn write_scalar_atomic(\n+        &mut self, val: ScalarMaybeUninit<Tag>, dest: MPlaceTy<'tcx, Tag>,\n+        atomic: AtomicWriteOp\n+    ) -> InterpResult<'tcx> {\n+        let this = self.eval_context_mut();\n+        this.allow_data_races_mut(move |this| {\n+            this.write_scalar(val, dest.into())\n+        })?;\n+        self.validate_atomic_store(dest, atomic)\n+    }\n+\n+    /// Perform a atomic operation on a memory location.\n+    fn atomic_op_immediate(\n+        &mut self,\n+        place: MPlaceTy<'tcx, Tag>, rhs: ImmTy<'tcx, Tag>,\n+        op: mir::BinOp, neg: bool, atomic: AtomicRwOp\n+    ) -> InterpResult<'tcx, ImmTy<'tcx, Tag>> {\n+        let this = self.eval_context_mut();\n+\n+        let old = this.allow_data_races_mut(|this| {\n+            this.read_immediate(place. into())\n+        })?;        \n+\n+        // Atomics wrap around on overflow.\n+        let val = this.binary_op(op, old, rhs)?;\n+        let val = if neg { this.unary_op(mir::UnOp::Not, val)? } else { val };\n+        this.allow_data_races_mut(|this| {\n+            this.write_immediate(*val, place.into())\n+        })?;\n+\n+        this.validate_atomic_rmw(place, atomic)?;\n+        Ok(old)\n+    }\n+\n+    /// Perform an atomic exchange with a memory place and a new\n+    /// scalar value, the old value is returned.\n+    fn atomic_exchange_scalar(\n+        &mut self,\n+        place: MPlaceTy<'tcx, Tag>, new: ScalarMaybeUninit<Tag>,\n+        atomic: AtomicRwOp\n+    ) -> InterpResult<'tcx, ScalarMaybeUninit<Tag>> {\n+        let this = self.eval_context_mut();\n+\n+        let old = this.allow_data_races_mut(|this| {\n+            this.read_scalar(place.into())\n+        })?;\n+        this.allow_data_races_mut(|this| {\n+            this.write_scalar(new, place.into())\n+        })?;\n+        this.validate_atomic_rmw(place, atomic)?;\n+        Ok(old)\n+    }\n+\n+    /// Perform an atomic compare and exchange at a given memory location\n+    /// on success an atomic RMW operation is performed and on failure\n+    /// only an atomic read occurs.\n+    fn atomic_compare_exchange_scalar(\n+        &mut self, place: MPlaceTy<'tcx, Tag>,\n+        expect_old: ImmTy<'tcx, Tag>, new: ScalarMaybeUninit<Tag>,\n+        success: AtomicRwOp, fail: AtomicReadOp\n+    ) -> InterpResult<'tcx, Immediate<Tag>> {\n+        let this = self.eval_context_mut();\n+\n+        // Failure ordering cannot be stronger than success ordering, therefore first attempt\n+        // to read with the failure ordering and if successfull then try again with the success\n+        // read ordering and write in the success case.\n+        // Read as immediate for the sake of `binary_op()`\n+        let old = this.allow_data_races_mut(|this| {\n+            this.read_immediate(place.into())\n+        })?; \n+\n+        // `binary_op` will bail if either of them is not a scalar.\n+        let eq = this.overflowing_binary_op(mir::BinOp::Eq, old, expect_old)?.0;\n+        let res = Immediate::ScalarPair(old.to_scalar_or_uninit(), eq.into());\n+\n+        // Update ptr depending on comparison.\n+        // if successful, perform a full rw-atomic validation\n+        // otherwise treat this as an atomic load with the fail ordering.\n+        if eq.to_bool()? {\n+            this.allow_data_races_mut(|this| {\n+                this.write_scalar(new, place.into())\n+            })?;\n+            this.validate_atomic_rmw(place, success)?;\n+        } else {\n+            this.validate_atomic_load(place, fail)?;\n+        }\n+\n+        // Return the old value.\n+        Ok(res)\n+    }\n+    \n+    \n+    /// Update the data-race detector for an atomic read occuring at the\n+    /// associated memory-place and on the current thread.\n+    fn validate_atomic_load(\n+        &self, place: MPlaceTy<'tcx, Tag>, atomic: AtomicReadOp\n+    ) -> InterpResult<'tcx> {\n+        let this = self.eval_context_ref();\n+        this.validate_atomic_op(\n+            place, atomic, \"Atomic Load\",\n+            move |memory, clocks, index, atomic| {\n+                if atomic == AtomicReadOp::Relaxed {\n+                    memory.load_relaxed(&mut *clocks, index)\n+                } else {\n+                    memory.load_acquire(&mut *clocks, index)\n+                }\n+            }\n+        )\n+    }\n+\n+    /// Update the data-race detector for an atomic write occuring at the\n+    /// associated memory-place and on the current thread.\n+    fn validate_atomic_store(\n+        &mut self, place: MPlaceTy<'tcx, Tag>, atomic: AtomicWriteOp\n+    ) -> InterpResult<'tcx> {\n+        let this = self.eval_context_ref();\n+        this.validate_atomic_op(\n+            place, atomic, \"Atomic Store\",\n+            move |memory, clocks, index, atomic| {\n+                if atomic == AtomicWriteOp::Relaxed {\n+                    memory.store_relaxed(clocks, index)\n+                } else {\n+                    memory.store_release(clocks, index)\n+                }\n+            }\n+        )\n+    }\n+\n+    /// Update the data-race detector for an atomic read-modify-write occuring\n+    /// at the associated memory place and on the current thread.\n+    fn validate_atomic_rmw(\n+        &mut self, place: MPlaceTy<'tcx, Tag>, atomic: AtomicRwOp\n+    ) -> InterpResult<'tcx> {\n+        use AtomicRwOp::*;\n+        let acquire = matches!(atomic, Acquire | AcqRel | SeqCst);\n+        let release = matches!(atomic, Release | AcqRel | SeqCst);\n+        let this = self.eval_context_ref();\n+        this.validate_atomic_op(\n+            place, atomic, \"Atomic RMW\",\n+            move |memory, clocks, index, _| {\n+                if acquire {\n+                    memory.load_acquire(clocks, index)?;\n+                } else {\n+                    memory.load_relaxed(clocks, index)?;\n+                }\n+                if release {\n+                    memory.rmw_release(clocks, index)\n+                } else {\n+                    memory.rmw_relaxed(clocks, index)\n+                }\n+            }\n+        )\n+    }\n+\n+    /// Update the data-race detector for an atomic fence on the current thread.\n+    fn validate_atomic_fence(&mut self, atomic: AtomicFenceOp) -> InterpResult<'tcx> {\n+        let this = self.eval_context_mut();\n+        if let Some(data_race) = &this.memory.extra.data_race {\n+            data_race.maybe_perform_sync_operation(move |index, mut clocks| {\n+                log::trace!(\"Atomic fence on {:?} with ordering {:?}\", index, atomic);\n+\n+                // Apply data-race detection for the current fences\n+                // this treats AcqRel and SeqCst as the same as a acquire\n+                // and release fence applied in the same timestamp.\n+                if atomic != AtomicFenceOp::Release {\n+                    // Either Acquire | AcqRel | SeqCst\n+                    clocks.apply_acquire_fence();\n+                }\n+                if atomic != AtomicFenceOp::Acquire {\n+                    // Either Release | AcqRel | SeqCst\n+                    clocks.apply_release_fence();\n+                }\n+                Ok(())\n+            })\n+        } else {\n+            Ok(())\n+        }\n+    }\n+}\n+\n+\n+\n+/// Vector clock metadata for a logical memory allocation.\n #[derive(Debug, Clone)]\n pub struct VClockAlloc {\n \n-    /// Range of Vector clocks, mapping to the vector-clock\n-    ///  index of the last write to the bytes in this allocation\n+    /// Range of Vector clocks, this gives each byte a potentially\n+    /// unqiue set of vector clocks, but merges identical information\n+    /// together for improved efficiency.\n     alloc_ranges: RefCell<RangeMap<MemoryCellClocks>>,\n \n-    // Pointer to global state\n+    // Pointer to global state.\n     global: MemoryExtra,\n }\n \n+\n impl VClockAlloc {\n \n-    /// Create a new data-race allocation detector\n+    /// Create a new data-race allocation detector.\n     pub fn new_allocation(global: &MemoryExtra, len: Size) -> VClockAlloc {\n         VClockAlloc {\n             global: Rc::clone(global),\n@@ -565,7 +638,7 @@ impl VClockAlloc {\n     }\n \n     // Find an index, if one exists where the value\n-    //  in `l` is greater than the value in `r`\n+    // in `l` is greater than the value in `r`.\n     fn find_gt_index(l: &VClock, r: &VClock) -> Option<VectorIdx> {\n         let l_slice = l.as_slice();\n         let r_slice = r.as_slice();\n@@ -575,27 +648,28 @@ impl VClockAlloc {\n                 if l > r { Some(idx) } else { None }\n             }).or_else(|| {\n                 if l_slice.len() > r_slice.len() {\n+\n                     // By invariant, if l_slice is longer\n-                    //  then one element must be larger\n+                    // then one element must be larger.\n                     // This just validates that this is true\n-                    //  and reports earlier elements first\n+                    // and reports earlier elements first.\n                     let l_remainder_slice = &l_slice[r_slice.len()..];\n                     let idx = l_remainder_slice.iter().enumerate()\n                         .find_map(|(idx, &r)| {\n                             if r == 0 { None } else { Some(idx) }\n                         }).expect(\"Invalid VClock Invariant\");\n                     Some(idx)\n-                }else{\n+                } else {\n                     None\n                 }\n             }).map(|idx| VectorIdx::new(idx))\n     }\n \n-    /// Report a data-race found in the program\n-    ///  this finds the two racing threads and the type\n-    ///  of data-race that occured, this will also\n-    ///  return info about the memory location the data-race\n-    ///  occured in\n+    /// Report a data-race found in the program.\n+    /// This finds the two racing threads and the type\n+    /// of data-race that occured. This will also\n+    /// return info about the memory location the data-race\n+    /// occured in.\n     #[cold]\n     #[inline(never)]\n     fn report_data_race<'tcx>(\n@@ -608,39 +682,40 @@ impl VClockAlloc {\n         let (\n             other_action, other_thread, other_clock\n         ) = if range.write > current_clocks.clock[range.write_index] {\n+\n             // Convert the write action into the vector clock it\n-            //  represents for diagnostic purposes\n+            // represents for diagnostic purposes.\n             write_clock = VClock::new_with_index(range.write_index, range.write);\n             (\"WRITE\", range.write_index, &write_clock)\n-        }else if let Some(idx) = Self::find_gt_index(\n+        } else if let Some(idx) = Self::find_gt_index(\n             &range.read, &current_clocks.clock\n         ){\n             (\"READ\", idx, &range.read)\n-        }else if !is_atomic {\n+        } else if !is_atomic {\n             if let Some(atomic) = range.atomic() {\n                 if let Some(idx) = Self::find_gt_index(\n                     &atomic.write_vector, &current_clocks.clock\n                 ) {\n                     (\"ATOMIC_STORE\", idx, &atomic.write_vector)\n-                }else if let Some(idx) = Self::find_gt_index(\n+                } else if let Some(idx) = Self::find_gt_index(\n                     &atomic.read_vector, &current_clocks.clock\n                 ) {\n                     (\"ATOMIC_LOAD\", idx, &atomic.read_vector)\n-                }else{\n-                    unreachable!(\"Failed to find report data-race for non-atomic operation: no race found\")\n+                } else {\n+                    unreachable!(\"Failed to report data-race for non-atomic operation: no race found\")\n                 }\n-            }else{\n+            } else {\n                 unreachable!(\"Failed to report data-race for non-atomic operation: no atomic component\")\n             }\n-        }else{\n+        } else {\n             unreachable!(\"Failed to report data-race for atomic operation\")\n         };\n \n-        // Load elaborated thread information about the racing thread actions\n+        // Load elaborated thread information about the racing thread actions.\n         let current_thread_info = global.print_thread_metadata(current_index);\n         let other_thread_info = global.print_thread_metadata(other_thread);\n         \n-        // Throw the data-race detection\n+        // Throw the data-race detection.\n         throw_ub_format!(\n             \"Data race detected between {} on {} and {} on {}, memory({:?},offset={},size={})\\\n             \\n\\t\\t -current vector clock = {:?}\\\n@@ -654,23 +729,25 @@ impl VClockAlloc {\n     }\n \n     /// Detect data-races for an unsychronized read operation, will not perform\n-    ///  data-race threads if `multi-threaded` is false, either due to no threads\n-    ///  being created or if it is temporarily disabled during a racy read or write\n-    ///  operation\n+    /// data-race detection if `multi-threaded` is false, either due to no threads\n+    /// being created or if it is temporarily disabled during a racy read or write\n+    /// operation for which data-race detection is handled separately, for example\n+    /// atomic read operations.\n     pub fn read<'tcx>(&self, pointer: Pointer<Tag>, len: Size) -> InterpResult<'tcx> {\n         if self.global.multi_threaded.get() {\n             let (index, clocks) = self.global.current_thread_state();\n             let mut alloc_ranges = self.alloc_ranges.borrow_mut();\n             for (_,range) in alloc_ranges.iter_mut(pointer.offset, len) {\n                 if let Err(DataRace) = range.read_race_detect(&*clocks, index) {\n-                    // Report data-race\n+\n+                    // Report data-race.\n                     return Self::report_data_race(\n                         &self.global,range, \"READ\", false, pointer, len\n                     );\n                 }\n             }\n             Ok(())\n-        }else{\n+        } else {\n             Ok(())\n         }\n     }\n@@ -682,163 +759,216 @@ impl VClockAlloc {\n             let (index, clocks) = self.global.current_thread_state();\n             for (_,range) in self.alloc_ranges.get_mut().iter_mut(pointer.offset, len) {\n                 if let Err(DataRace) = range.write_race_detect(&*clocks, index) {\n+                    \n                     // Report data-race\n                     return Self::report_data_race(\n                         &self.global, range, action, false, pointer, len\n                     );\n                 }\n             }\n             Ok(())\n-        }else{\n+        } else {\n             Ok(())\n         }\n     }\n \n     /// Detect data-races for an unsychronized write operation, will not perform\n-    ///  data-race threads if `multi-threaded` is false, either due to no threads\n-    ///  being created or if it is temporarily disabled during a racy read or write\n-    ///  operation\n+    /// data-race threads if `multi-threaded` is false, either due to no threads\n+    /// being created or if it is temporarily disabled during a racy read or write\n+    /// operation\n     pub fn write<'tcx>(&mut self, pointer: Pointer<Tag>, len: Size) -> InterpResult<'tcx> {\n         self.unique_access(pointer, len, \"Write\")\n     }\n+\n     /// Detect data-races for an unsychronized deallocate operation, will not perform\n-    ///  data-race threads if `multi-threaded` is false, either due to no threads\n-    ///  being created or if it is temporarily disabled during a racy read or write\n-    ///  operation\n+    /// data-race threads if `multi-threaded` is false, either due to no threads\n+    /// being created or if it is temporarily disabled during a racy read or write\n+    /// operation\n     pub fn deallocate<'tcx>(&mut self, pointer: Pointer<Tag>, len: Size) -> InterpResult<'tcx> {\n         self.unique_access(pointer, len, \"Deallocate\")\n     }\n }\n \n-/// The current set of vector clocks describing the state\n-///  of a thread, contains the happens-before clock and\n-///  additional metadata to model atomic fence operations\n-#[derive(Clone, Default, Debug)]\n-struct ThreadClockSet {\n-\n-    /// The increasing clock representing timestamps\n-    ///  that happen-before this thread.\n-    clock: VClock,\n-\n-    /// The set of timestamps that will happen-before this\n-    ///  thread once it performs an acquire fence\n-    fence_acquire: VClock,\n-\n-    /// The last timesamp of happens-before relations that\n-    ///  have been released by this thread by a fence\n-    fence_release: VClock,\n-}\n-\n-impl ThreadClockSet {\n+impl<'mir, 'tcx: 'mir> EvalContextPrivExt<'mir, 'tcx> for MiriEvalContext<'mir, 'tcx> {}\n+trait EvalContextPrivExt<'mir, 'tcx: 'mir>: MiriEvalContextExt<'mir, 'tcx> {\n \n-    /// Apply the effects of a release fence to this\n-    ///  set of thread vector clocks\n+    // Temporarily allow data-races to occur, this should only be\n+    // used if either one of the appropiate `validate_atomic` functions\n+    // will be called to treat a memory access as atomic or if the memory\n+    // being accessed should be treated as internal state, that cannot be\n+    // accessed by the interpreted program.\n     #[inline]\n-    fn apply_release_fence(&mut self) {\n-        self.fence_release.clone_from(&self.clock);\n+    fn allow_data_races_ref<R>(&self, op: impl FnOnce(&MiriEvalContext<'mir, 'tcx>) -> R) -> R {\n+        let this = self.eval_context_ref();\n+        let old = if let Some(data_race) = &this.memory.extra.data_race {\n+            data_race.multi_threaded.replace(false)\n+        } else {\n+            false\n+        };\n+        let result = op(this);\n+        if let Some(data_race) = &this.memory.extra.data_race {\n+            data_race.multi_threaded.set(old);\n+        }\n+        result\n     }\n \n-    /// Apply the effects of a acquire fence to this\n-    ///  set of thread vector clocks\n+    /// Same as `allow_data_races_ref`, this temporarily disables any data-race detection and\n+    /// so should only be used for atomic operations or internal state that the program cannot\n+    /// access.\n     #[inline]\n-    fn apply_acquire_fence(&mut self) {\n-        self.clock.join(&self.fence_acquire);\n+    fn allow_data_races_mut<R>(&mut self, op: impl FnOnce(&mut MiriEvalContext<'mir, 'tcx>) -> R) -> R {\n+        let this = self.eval_context_mut();\n+        let old = if let Some(data_race) = &this.memory.extra.data_race {\n+            data_race.multi_threaded.replace(false)\n+        } else {\n+            false\n+        };\n+        let result = op(this);\n+        if let Some(data_race) = &this.memory.extra.data_race {\n+            data_race.multi_threaded.set(old);\n+        }\n+        result\n     }\n \n-    /// Increment the happens-before clock at a\n-    ///  known index\n-    #[inline]\n-    fn increment_clock(&mut self, index: VectorIdx) {\n-        self.clock.increment_index(index);\n+    /// Generic atomic operation implementation,\n+    /// this accesses memory via get_raw instead of\n+    /// get_raw_mut, due to issues calling get_raw_mut\n+    /// for atomic loads from read-only memory.\n+    /// FIXME: is this valid, or should get_raw_mut be used for\n+    /// atomic-stores/atomic-rmw?\n+    fn validate_atomic_op<A: Debug + Copy>(\n+        &self, place: MPlaceTy<'tcx, Tag>,\n+        atomic: A, description: &str,\n+        mut op: impl FnMut(\n+            &mut MemoryCellClocks, &mut ThreadClockSet, VectorIdx, A\n+        ) -> Result<(), DataRace>\n+    ) -> InterpResult<'tcx> {\n+        let this = self.eval_context_ref();\n+        if let Some(data_race) = &this.memory.extra.data_race {\n+            if data_race.multi_threaded.get() {\n+\n+                // Load and log the atomic operation.\n+                let place_ptr = place.ptr.assert_ptr();\n+                let size = place.layout.size;\n+                let alloc_meta =  &this.memory.get_raw(place_ptr.alloc_id)?.extra.data_race.as_ref().unwrap();\n+                log::trace!(\n+                    \"Atomic op({}) with ordering {:?} on memory({:?}, offset={}, size={})\",\n+                    description, &atomic, place_ptr.alloc_id, place_ptr.offset.bytes(), size.bytes()\n+                );\n+\n+                // Perform the atomic operation.\n+                let data_race = &alloc_meta.global;\n+                data_race.maybe_perform_sync_operation(|index, mut clocks| {\n+                    for (_,range) in alloc_meta.alloc_ranges.borrow_mut().iter_mut(place_ptr.offset, size) {\n+                        if let Err(DataRace) = op(range, &mut *clocks, index, atomic) {\n+                            mem::drop(clocks);\n+                            return VClockAlloc::report_data_race(\n+                                &alloc_meta.global, range, description, true,\n+                                place_ptr, size\n+                            );\n+                        }\n+                    }\n+                    Ok(())\n+                })?;\n+\n+                // Log changes to atomic memory.\n+                if log::log_enabled!(log::Level::Trace) {\n+                    for (_,range) in alloc_meta.alloc_ranges.borrow().iter(place_ptr.offset, size) {\n+                        log::trace!(\n+                            \"Updated atomic memory({:?}, offset={}, size={}) to {:#?}\",\n+                            place.ptr.assert_ptr().alloc_id, place_ptr.offset.bytes(), size.bytes(),\n+                            range.atomic_ops\n+                        );\n+                    }\n+                }\n+            }\n+        }\n+        Ok(())\n     }\n \n-    /// Join the happens-before clock with that of\n-    ///  another thread, used to model thread join\n-    ///  operations\n-    fn join_with(&mut self, other: &ThreadClockSet) {\n-        self.clock.join(&other.clock);\n-    }\n }\n \n-/// Extra metadata associated with a thread\n+\n+/// Extra metadata associated with a thread.\n #[derive(Debug, Clone, Default)]\n struct ThreadExtraState {\n \n     /// The current vector index in use by the\n-    ///  thread currently, this is set to None\n-    ///  after the vector index has been re-used\n-    ///  and hence the value will never need to be\n-    ///  read during data-race reporting\n+    /// thread currently, this is set to None\n+    /// after the vector index has been re-used\n+    /// and hence the value will never need to be\n+    /// read during data-race reporting.\n     vector_index: Option<VectorIdx>,\n \n     /// The name of the thread, updated for better\n-    ///  diagnostics when reporting detected data\n-    ///  races\n+    /// diagnostics when reporting detected data\n+    /// races.\n     thread_name: Option<Box<str>>,\n     \n     /// Thread termination vector clock, this\n-    ///  is set on thread termination and is used\n-    ///  for joining on threads since the vector_index\n-    ///  may be re-used when the join operation occurs\n+    /// is set on thread termination and is used\n+    /// for joining on threads since the vector_index\n+    /// may be re-used when the join operation occurs.\n     termination_vector_clock: Option<VClock>,\n }\n \n /// Global data-race detection state, contains the currently\n-///  executing thread as well as the vector-clocks associated\n-///  with each of the threads.\n+/// executing thread as well as the vector-clocks associated\n+/// with each of the threads.\n #[derive(Debug, Clone)]\n pub struct GlobalState {\n \n     /// Set to true once the first additional\n-    ///  thread has launched, due to the dependency\n-    ///  between before and after a thread launch\n+    /// thread has launched, due to the dependency\n+    /// between before and after a thread launch.\n     /// Any data-races must be recorded after this\n-    ///  so concurrent execution can ignore recording\n-    ///  any data-races\n+    /// so concurrent execution can ignore recording\n+    /// any data-races.\n     multi_threaded: Cell<bool>,\n \n     /// Mapping of a vector index to a known set of thread\n-    ///  clocks, this is not directly mapping from a thread id\n-    ///  since it may refer to multiple threads\n+    /// clocks, this is not directly mapping from a thread id\n+    /// since it may refer to multiple threads.\n     vector_clocks: RefCell<IndexVec<VectorIdx, ThreadClockSet>>,\n \n     /// Mapping of a given vector index to the current thread\n-    ///  that the execution is representing, this may change\n-    ///  if a vector index is re-assigned to a new thread\n+    /// that the execution is representing, this may change\n+    /// if a vector index is re-assigned to a new thread.\n     vector_info: RefCell<IndexVec<VectorIdx, ThreadId>>,\n \n-    /// The mapping of a given thread to assocaited thread metadata\n+    /// The mapping of a given thread to assocaited thread metadata.\n     thread_info: RefCell<IndexVec<ThreadId, ThreadExtraState>>,\n \n-    /// The current vector index being executed\n+    /// The current vector index being executed.\n     current_index: Cell<VectorIdx>,\n \n     /// Potential vector indices that could be re-used on thread creation\n-    ///  values are inserted here on after the thread has terminated and\n-    ///  been joined with, and hence may potentially become free\n-    ///  for use as the index for a new thread.\n+    /// values are inserted here on after the thread has terminated and\n+    /// been joined with, and hence may potentially become free\n+    /// for use as the index for a new thread.\n     /// Elements in this set may still require the vector index to\n-    ///  report data-races, and can only be re-used after all\n-    ///  active vector-clocks catch up with the threads timestamp.\n+    /// report data-races, and can only be re-used after all\n+    /// active vector-clocks catch up with the threads timestamp.\n     reuse_candidates: RefCell<FxHashSet<VectorIdx>>,\n \n     /// Counts the number of threads that are currently active\n-    ///  if the number of active threads reduces to 1 and then\n-    ///  a join operation occures with the remaining main thread\n-    ///  then multi-threaded execution may be disabled\n+    /// if the number of active threads reduces to 1 and then\n+    /// a join operation occures with the remaining main thread\n+    /// then multi-threaded execution may be disabled.\n     active_thread_count: Cell<usize>, \n \n     /// This contains threads that have terminated, but not yet joined\n-    ///  and so cannot become re-use candidates until a join operation\n-    ///  occurs.\n+    /// and so cannot become re-use candidates until a join operation\n+    /// occurs.\n     /// The associated vector index will be moved into re-use candidates\n-    ///  after the join operation occurs\n+    /// after the join operation occurs.\n     terminated_threads: RefCell<FxHashMap<ThreadId, VectorIdx>>,\n }\n+\n impl GlobalState {\n \n     /// Create a new global state, setup with just thread-id=0\n-    ///  advanced to timestamp = 1\n+    /// advanced to timestamp = 1.\n     pub fn new() -> Self {\n         let global_state = GlobalState {\n             multi_threaded: Cell::new(false),\n@@ -852,8 +982,8 @@ impl GlobalState {\n         };\n \n         // Setup the main-thread since it is not explicitly created:\n-        //  uses vector index and thread-id 0, also the rust runtime gives\n-        //  the main-thread a name of \"main\".\n+        // uses vector index and thread-id 0, also the rust runtime gives\n+        // the main-thread a name of \"main\".\n         let index = global_state.vector_clocks.borrow_mut().push(ThreadClockSet::default());\n         global_state.vector_info.borrow_mut().push(ThreadId::new(0));\n         global_state.thread_info.borrow_mut().push(\n@@ -868,7 +998,7 @@ impl GlobalState {\n     }\n     \n     // Try to find vector index values that can potentially be re-used\n-    //  by a new thread instead of a new vector index being created\n+    // by a new thread instead of a new vector index being created.\n     fn find_vector_index_reuse_candidate(&self) -> Option<VectorIdx> {\n         let mut reuse = self.reuse_candidates.borrow_mut();\n         let vector_clocks = self.vector_clocks.borrow();\n@@ -877,24 +1007,26 @@ impl GlobalState {\n         for  &candidate in reuse.iter() {\n             let target_timestamp = vector_clocks[candidate].clock[candidate];\n             if vector_clocks.iter_enumerated().all(|(clock_idx, clock)| {\n+\n                 // The thread happens before the clock, and hence cannot report\n-                //  a data-race with this the candidate index\n+                // a data-race with this the candidate index.\n                 let no_data_race = clock.clock[candidate] >= target_timestamp;\n \n                 // The vector represents a thread that has terminated and hence cannot\n-                //  report a data-race with the candidate index\n+                // report a data-race with the candidate index.\n                 let thread_id = vector_info[clock_idx];\n                 let vector_terminated = reuse.contains(&clock_idx)\n                     || terminated_threads.contains_key(&thread_id);\n \n                 // The vector index cannot report a race with the candidate index\n-                //  and hence allows the candidate index to be re-used\n+                // and hence allows the candidate index to be re-used.\n                 no_data_race || vector_terminated\n             }) {\n+\n                 // All vector clocks for each vector index are equal to\n-                //  the target timestamp, and the thread is known to have\n-                //  terminated, therefore this vector clock index cannot\n-                //  report any more data-races\n+                // the target timestamp, and the thread is known to have\n+                // terminated, therefore this vector clock index cannot\n+                // report any more data-races.\n                 assert!(reuse.remove(&candidate));\n                 return Some(candidate)\n             }\n@@ -903,119 +1035,123 @@ impl GlobalState {\n     }\n \n     // Hook for thread creation, enabled multi-threaded execution and marks\n-    //  the current thread timestamp as happening-before the current thread\n+    // the current thread timestamp as happening-before the current thread.\n     #[inline]\n     pub fn thread_created(&self, thread: ThreadId) {\n         let current_index = self.current_index();\n \n-        // Increment the number of active threads\n+        // Increment the number of active threads.\n         let active_threads = self.active_thread_count.get();\n         self.active_thread_count.set(active_threads + 1);\n \n         // Enable multi-threaded execution, there are now two threads\n-        //  so data-races are now possible.\n+        // so data-races are now possible.\n         self.multi_threaded.set(true);\n \n         // Load and setup the associated thread metadata\n         let mut thread_info = self.thread_info.borrow_mut();\n         thread_info.ensure_contains_elem(thread, Default::default);\n \n         // Assign a vector index for the thread, attempting to re-use an old\n-        //  vector index that can no longer report any data-races if possible\n+        // vector index that can no longer report any data-races if possible.\n         let created_index = if let Some(\n             reuse_index\n         ) = self.find_vector_index_reuse_candidate() {\n+\n             // Now re-configure the re-use candidate, increment the clock\n-            //  for the new sync use of the vector\n+            // for the new sync use of the vector.\n             let mut vector_clocks = self.vector_clocks.borrow_mut();\n             vector_clocks[reuse_index].increment_clock(reuse_index);\n \n             // Locate the old thread the vector was associated with and update\n-            //  it to represent the new thread instead\n+            // it to represent the new thread instead.\n             let mut vector_info = self.vector_info.borrow_mut();\n             let old_thread = vector_info[reuse_index];\n             vector_info[reuse_index] = thread;\n \n             // Mark the thread the vector index was associated with as no longer\n-            //  representing a thread index\n+            // representing a thread index.\n             thread_info[old_thread].vector_index = None;\n \n             reuse_index\n-        }else{\n+        } else {\n+\n             // No vector re-use candidates available, instead create\n-            //  a new vector index\n+            // a new vector index.\n             let mut vector_info = self.vector_info.borrow_mut();\n             vector_info.push(thread)\n         };\n \n-        // Mark the chosen vector index as in use by the thread\n+        // Mark the chosen vector index as in use by the thread.\n         thread_info[thread].vector_index = Some(created_index);\n \n-        // Create a thread clock set if applicable\n+        // Create a thread clock set if applicable.\n         let mut vector_clocks = self.vector_clocks.borrow_mut();\n         if created_index == vector_clocks.next_index() {\n             vector_clocks.push(ThreadClockSet::default());\n         }\n \n-        // Now load the two clocks and configure the initial state\n+        // Now load the two clocks and configure the initial state.\n         let (current, created) = vector_clocks.pick2_mut(current_index, created_index);\n \n-        // Advance the current thread before the synchronized operation\n+        // Advance the current thread before the synchronized operation.\n         current.increment_clock(current_index);\n \n         // Join the created with current, since the current threads\n-        //  previous actions happen-before the created thread\n+        // previous actions happen-before the created thread.\n         created.join_with(current);\n \n-        // Advance both threads after the synchronized operation\n+        // Advance both threads after the synchronized operation.\n         current.increment_clock(current_index);\n         created.increment_clock(created_index);\n     }\n \n     /// Hook on a thread join to update the implicit happens-before relation\n-    ///  between the joined thead and the current thread.\n+    /// between the joined thead and the current thread.\n     #[inline]\n     pub fn thread_joined(&self, current_thread: ThreadId, join_thread: ThreadId) {\n         let mut clocks_vec = self.vector_clocks.borrow_mut();\n         let thread_info = self.thread_info.borrow();\n \n-        // Load the vector clock of the current thread\n+        // Load the vector clock of the current thread.\n         let current_index = thread_info[current_thread].vector_index\n             .expect(\"Performed thread join on thread with no assigned vector\");\n         let current = &mut clocks_vec[current_index];\n \n-        // Load the associated vector clock for the terminated thread\n+        // Load the associated vector clock for the terminated thread.\n         let join_clock = thread_info[join_thread].termination_vector_clock\n             .as_ref().expect(\"Joined with thread but thread has not terminated\");\n \n-        // Pre increment clocks before atomic operation\n+        // Pre increment clocks before atomic operation.\n         current.increment_clock(current_index);\n \n         // The join thread happens-before the current thread\n-        //   so update the current vector clock\n+        // so update the current vector clock.\n         current.clock.join(join_clock);\n \n-        // Post increment clocks after atomic operation\n+        // Post increment clocks after atomic operation.\n         current.increment_clock(current_index);\n \n         // Check the number of active threads, if the value is 1\n-        //  then test for potentially disabling multi-threaded execution\n+        // then test for potentially disabling multi-threaded execution.\n         let active_threads = self.active_thread_count.get();\n         if active_threads == 1 {\n-            // May potentially be able to disable multi-threaded execution\n+\n+            // May potentially be able to disable multi-threaded execution.\n             let current_clock = &clocks_vec[current_index];\n             if clocks_vec.iter_enumerated().all(|(idx, clocks)| {\n                 clocks.clock[idx] <= current_clock.clock[idx]\n             }) {\n+\n                 // The all thread termations happen-before the current clock\n-                //  therefore no data-races can be reported until a new thread\n-                //  is created, so disable multi-threaded execution\n+                // therefore no data-races can be reported until a new thread\n+                // is created, so disable multi-threaded execution.\n                 self.multi_threaded.set(false);\n             }\n         }\n \n         // If the thread is marked as terminated but not joined\n-        //  then move the thread to the re-use set\n+        // then move the thread to the re-use set.\n         let mut termination = self.terminated_threads.borrow_mut();\n         if let Some(index) = termination.remove(&join_thread) {\n             let mut reuse = self.reuse_candidates.borrow_mut();\n@@ -1024,47 +1160,47 @@ impl GlobalState {\n     }\n \n     /// On thread termination, the vector-clock may re-used\n-    ///  in the future once all remaining thread-clocks catch\n-    ///  up with the time index of the terminated thread.\n+    /// in the future once all remaining thread-clocks catch\n+    /// up with the time index of the terminated thread.\n     /// This assiges thread termination with a unique index\n-    ///  which will be used to join the thread\n+    /// which will be used to join the thread\n     /// This should be called strictly before any calls to\n-    ///   `thread_joined`\n+    /// `thread_joined`.\n     #[inline]\n     pub fn thread_terminated(&self) {\n         let current_index = self.current_index();\n         \n-        // Increment the clock to a unique termination timestamp\n+        // Increment the clock to a unique termination timestamp.\n         let mut vector_clocks = self.vector_clocks.borrow_mut();\n         let current_clocks = &mut vector_clocks[current_index];\n         current_clocks.increment_clock(current_index);\n \n-        // Load the current thread id for the executing vector\n+        // Load the current thread id for the executing vector.\n         let vector_info = self.vector_info.borrow();\n         let current_thread = vector_info[current_index];\n \n         // Load the current thread metadata, and move to a terminated\n-        //  vector state. Setting up the vector clock all join operations\n-        //  will use.\n+        // vector state. Setting up the vector clock all join operations\n+        // will use.\n         let mut thread_info = self.thread_info.borrow_mut();\n         let current = &mut thread_info[current_thread];\n         current.termination_vector_clock = Some(current_clocks.clock.clone());\n \n         // Add this thread as a candidate for re-use after a thread join\n-        //  occurs\n+        // occurs.\n         let mut termination = self.terminated_threads.borrow_mut();\n         termination.insert(current_thread, current_index);\n             \n         // Reduce the number of active threads, now that a thread has\n-        //  terminated\n+        // terminated.\n         let mut active_threads = self.active_thread_count.get();\n         active_threads -= 1;\n         self.active_thread_count.set(active_threads);\n     }\n \n     /// Hook for updating the local tracker of the currently\n-    ///  enabled thread, should always be updated whenever\n-    ///  `active_thread` in thread.rs is updated\n+    /// enabled thread, should always be updated whenever\n+    /// `active_thread` in thread.rs is updated.\n     #[inline]\n     pub fn thread_set_active(&self, thread: ThreadId) {\n         let thread_info = self.thread_info.borrow();\n@@ -1074,9 +1210,9 @@ impl GlobalState {\n     }\n \n     /// Hook for updating the local tracker of the threads name\n-    ///  this should always mirror the local value in thread.rs\n-    ///  the thread name is used for improved diagnostics\n-    ///  during a data-race\n+    /// this should always mirror the local value in thread.rs\n+    /// the thread name is used for improved diagnostics\n+    /// during a data-race.\n     #[inline]\n     pub fn thread_set_name(&self, thread: ThreadId, name: String) {\n         let name = name.into_boxed_str();\n@@ -1086,12 +1222,12 @@ impl GlobalState {\n \n \n     /// Attempt to perform a synchronized operation, this\n-    ///  will perform no operation if multi-threading is\n-    ///  not currently enabled.\n+    /// will perform no operation if multi-threading is\n+    /// not currently enabled.\n     /// Otherwise it will increment the clock for the current\n-    ///  vector before and after the operation for data-race\n-    ///  detection between any happens-before edges the\n-    ///  operation may create\n+    /// vector before and after the operation for data-race\n+    /// detection between any happens-before edges the\n+    /// operation may create.\n     fn maybe_perform_sync_operation<'tcx>(\n         &self, op: impl FnOnce(VectorIdx, RefMut<'_,ThreadClockSet>) -> InterpResult<'tcx>,\n     ) -> InterpResult<'tcx> {\n@@ -1107,50 +1243,50 @@ impl GlobalState {\n     \n \n     /// Internal utility to identify a thread stored internally\n-    ///  returns the id and the name for better diagnostics\n+    /// returns the id and the name for better diagnostics.\n     fn print_thread_metadata(&self, vector: VectorIdx) -> String {\n         let thread = self.vector_info.borrow()[vector];\n         let thread_name = &self.thread_info.borrow()[thread].thread_name;\n         if let Some(name) = thread_name {\n             let name: &str = name;\n             format!(\"Thread(id = {:?}, name = {:?})\", thread.to_u32(), &*name)\n-        }else{\n+        } else {\n             format!(\"Thread(id = {:?})\", thread.to_u32())\n         }\n     }\n \n \n     /// Acquire a lock, express that the previous call of\n-    ///  `validate_lock_release` must happen before this\n-    pub fn validate_lock_acquire(&self, lock: &DataRaceLockHandle, thread: ThreadId) {\n+    /// `validate_lock_release` must happen before this.\n+    pub fn validate_lock_acquire(&self, lock: &VClock, thread: ThreadId) {\n         let (index, mut clocks) = self.load_thread_state_mut(thread);\n         clocks.increment_clock(index);\n-        clocks.clock.join(&lock.clock);\n+        clocks.clock.join(&lock);\n         clocks.increment_clock(index);\n     }\n \n     /// Release a lock handle, express that this happens-before\n-    ///  any subsequent calls to `validate_lock_acquire`\n-    pub fn validate_lock_release(&self, lock: &mut DataRaceLockHandle, thread: ThreadId) {\n+    /// any subsequent calls to `validate_lock_acquire`.\n+    pub fn validate_lock_release(&self, lock: &mut VClock, thread: ThreadId) {\n         let (index, mut clocks) = self.load_thread_state_mut(thread);\n         clocks.increment_clock(index);\n-        lock.clock.clone_from(&clocks.clock);\n+        lock.clone_from(&clocks.clock);\n         clocks.increment_clock(index);\n     }\n \n     /// Release a lock handle, express that this happens-before\n-    ///  any subsequent calls to `validate_lock_acquire` as well\n-    ///  as any previous calls to this function after any\n-    ///  `validate_lock_release` calls\n-    pub fn validate_lock_release_shared(&self, lock: &mut DataRaceLockHandle, thread: ThreadId) {\n+    /// any subsequent calls to `validate_lock_acquire` as well\n+    /// as any previous calls to this function after any\n+    /// `validate_lock_release` calls.\n+    pub fn validate_lock_release_shared(&self, lock: &mut VClock, thread: ThreadId) {\n         let (index, mut clocks) = self.load_thread_state_mut(thread);\n         clocks.increment_clock(index);\n-        lock.clock.join(&clocks.clock);\n+        lock.join(&clocks.clock);\n         clocks.increment_clock(index);\n     }\n \n     /// Load the vector index used by the given thread as well as the set of vector clocks\n-    ///  used by the thread\n+    /// used by the thread.\n     #[inline]\n     fn load_thread_state_mut(&self, thread: ThreadId) -> (VectorIdx, RefMut<'_, ThreadClockSet>) {\n         let index = self.thread_info.borrow()[thread].vector_index\n@@ -1161,7 +1297,7 @@ impl GlobalState {\n     }\n \n     /// Load the current vector clock in use and the current set of thread clocks\n-    ///  in use for the vector\n+    /// in use for the vector.\n     #[inline]\n     fn current_thread_state(&self) -> (VectorIdx, Ref<'_, ThreadClockSet>) {\n         let index = self.current_index();\n@@ -1171,7 +1307,7 @@ impl GlobalState {\n     }\n \n     /// Load the current vector clock in use and the current set of thread clocks\n-    ///  in use for the vector mutably for modification\n+    /// in use for the vector mutably for modification.\n     #[inline]\n     fn current_thread_state_mut(&self) -> (VectorIdx, RefMut<'_, ThreadClockSet>) {\n         let index = self.current_index();\n@@ -1181,7 +1317,7 @@ impl GlobalState {\n     }\n \n     /// Return the current thread, should be the same\n-    ///  as the data-race active thread\n+    /// as the data-race active thread.\n     #[inline]\n     fn current_index(&self) -> VectorIdx {\n         self.current_index.get()"}, {"sha": "0a62f14dd3a153f1332909f1c6ce125a351f63ec", "filename": "src/eval.rs", "status": "modified", "additions": 3, "deletions": 0, "changes": 3, "blob_url": "https://github.com/rust-lang/rust/blob/69fb6413ddc5b7fd5d9cb0a68ebf58ee513bf9d5/src%2Feval.rs", "raw_url": "https://github.com/rust-lang/rust/raw/69fb6413ddc5b7fd5d9cb0a68ebf58ee513bf9d5/src%2Feval.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Feval.rs?ref=69fb6413ddc5b7fd5d9cb0a68ebf58ee513bf9d5", "patch": "@@ -48,6 +48,8 @@ pub struct MiriConfig {\n     pub tracked_alloc_id: Option<AllocId>,\n     /// Whether to track raw pointers in stacked borrows.\n     pub track_raw: bool,\n+    /// Determine if data race detection should be enabled\n+    pub data_race_detector: bool,\n }\n \n impl Default for MiriConfig {\n@@ -65,6 +67,7 @@ impl Default for MiriConfig {\n             tracked_call_id: None,\n             tracked_alloc_id: None,\n             track_raw: false,\n+            data_race_detector: true,\n         }\n     }\n }"}, {"sha": "87effe9c6885215269e5cbd93ff0b28c6916224b", "filename": "src/lib.rs", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/rust-lang/rust/blob/69fb6413ddc5b7fd5d9cb0a68ebf58ee513bf9d5/src%2Flib.rs", "raw_url": "https://github.com/rust-lang/rust/raw/69fb6413ddc5b7fd5d9cb0a68ebf58ee513bf9d5/src%2Flib.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flib.rs?ref=69fb6413ddc5b7fd5d9cb0a68ebf58ee513bf9d5", "patch": "@@ -55,7 +55,7 @@ pub use crate::shims::tls::{EvalContextExt as _, TlsData};\n pub use crate::shims::EvalContextExt as _;\n \n pub use crate::data_race::{\n-    AtomicReadOp, AtomicWriteOp, AtomicRWOp, AtomicFenceOp, DataRaceLockHandle,\n+    AtomicReadOp, AtomicWriteOp, AtomicRwOp, AtomicFenceOp,\n     EvalContextExt as DataRaceEvalContextExt\n };\n pub use crate::diagnostics::{\n@@ -81,7 +81,7 @@ pub use crate::sync::{\n     EvalContextExt as SyncEvalContextExt, CondvarId, MutexId, RwLockId\n };\n pub use crate::vector_clock::{\n-    VClock, VSmallClockSet, VectorIdx, VTimestamp\n+    VClock, VSmallClockMap, VectorIdx, VTimestamp\n };\n \n /// Insert rustc arguments at the beginning of the argument list that Miri wants to be"}, {"sha": "9612d9e1911078e6d4ec4859eebe8d3d5e828b1a", "filename": "src/machine.rs", "status": "modified", "additions": 23, "deletions": 8, "changes": 31, "blob_url": "https://github.com/rust-lang/rust/blob/69fb6413ddc5b7fd5d9cb0a68ebf58ee513bf9d5/src%2Fmachine.rs", "raw_url": "https://github.com/rust-lang/rust/raw/69fb6413ddc5b7fd5d9cb0a68ebf58ee513bf9d5/src%2Fmachine.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Fmachine.rs?ref=69fb6413ddc5b7fd5d9cb0a68ebf58ee513bf9d5", "patch": "@@ -109,15 +109,16 @@ impl fmt::Display for MiriMemoryKind {\n pub struct AllocExtra {\n     /// Stacked Borrows state is only added if it is enabled.\n     pub stacked_borrows: Option<stacked_borrows::AllocExtra>,\n-    /// Data race detection via the use of a vector-clock.\n-    pub data_race: data_race::AllocExtra,\n+    /// Data race detection via the use of a vector-clock,\n+    ///  this is only added if it is enabled.\n+    pub data_race: Option<data_race::AllocExtra>,\n }\n \n /// Extra global memory data\n #[derive(Clone, Debug)]\n pub struct MemoryExtra {\n     pub stacked_borrows: Option<stacked_borrows::MemoryExtra>,\n-    pub data_race: data_race::MemoryExtra,\n+    pub data_race: Option<data_race::MemoryExtra>,\n     pub intptrcast: intptrcast::MemoryExtra,\n \n     /// Mapping extern static names to their canonical allocation.\n@@ -147,7 +148,11 @@ impl MemoryExtra {\n         } else {\n             None\n         };\n-        let data_race = Rc::new(data_race::GlobalState::new());\n+        let data_race = if config.data_race_detector {\n+            Some(Rc::new(data_race::GlobalState::new()))\n+        }else{\n+            None\n+        };\n         MemoryExtra {\n             stacked_borrows,\n             data_race,\n@@ -472,7 +477,11 @@ impl<'mir, 'tcx> Machine<'mir, 'tcx> for Evaluator<'mir, 'tcx> {\n                 // No stacks, no tag.\n                 (None, Tag::Untagged)\n             };\n-        let race_alloc = data_race::AllocExtra::new_allocation(&memory_extra.data_race, alloc.size);\n+        let race_alloc = if let Some(data_race) = &memory_extra.data_race {\n+            Some(data_race::AllocExtra::new_allocation(&data_race, alloc.size))\n+        } else {\n+            None\n+        };\n         let mut stacked_borrows = memory_extra.stacked_borrows.as_ref().map(|sb| sb.borrow_mut());\n         let alloc: Allocation<Tag, Self::AllocExtra> = alloc.with_tags_and_extra(\n             |alloc| {\n@@ -590,7 +599,9 @@ impl AllocationExtra<Tag> for AllocExtra {\n         ptr: Pointer<Tag>,\n         size: Size,\n     ) -> InterpResult<'tcx> {\n-        alloc.extra.data_race.read(ptr, size)?;\n+        if let Some(data_race) = &alloc.extra.data_race {\n+            data_race.read(ptr, size)?;\n+        }\n         if let Some(stacked_borrows) = &alloc.extra.stacked_borrows {\n             stacked_borrows.memory_read(ptr, size)\n         } else {\n@@ -604,7 +615,9 @@ impl AllocationExtra<Tag> for AllocExtra {\n         ptr: Pointer<Tag>,\n         size: Size,\n     ) -> InterpResult<'tcx> {\n-        alloc.extra.data_race.write(ptr, size)?;\n+        if let Some(data_race) = &mut alloc.extra.data_race {\n+            data_race.write(ptr, size)?;\n+        }\n         if let Some(stacked_borrows) = &mut alloc.extra.stacked_borrows {\n             stacked_borrows.memory_written(ptr, size)\n         } else {\n@@ -618,7 +631,9 @@ impl AllocationExtra<Tag> for AllocExtra {\n         ptr: Pointer<Tag>,\n         size: Size,\n     ) -> InterpResult<'tcx> {\n-        alloc.extra.data_race.deallocate(ptr, size)?;\n+        if let Some(data_race) = &mut alloc.extra.data_race {\n+            data_race.deallocate(ptr, size)?;\n+        }\n         if let Some(stacked_borrows) = &mut alloc.extra.stacked_borrows {\n             stacked_borrows.memory_deallocated(ptr, size)\n         } else {"}, {"sha": "8f7ae6bebb52ed9d4c525e0019ac56427fc4d219", "filename": "src/shims/intrinsics.rs", "status": "modified", "additions": 65, "deletions": 101, "changes": 166, "blob_url": "https://github.com/rust-lang/rust/blob/69fb6413ddc5b7fd5d9cb0a68ebf58ee513bf9d5/src%2Fshims%2Fintrinsics.rs", "raw_url": "https://github.com/rust-lang/rust/raw/69fb6413ddc5b7fd5d9cb0a68ebf58ee513bf9d5/src%2Fshims%2Fintrinsics.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Fshims%2Fintrinsics.rs?ref=69fb6413ddc5b7fd5d9cb0a68ebf58ee513bf9d5", "patch": "@@ -324,98 +324,98 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: crate::MiriEvalContextExt<'mir, 'tcx\n             \"atomic_singlethreadfence_acqrel\" => this.compiler_fence(args, AtomicFenceOp::AcqRel)?,\n             \"atomic_singlethreadfence\" => this.compiler_fence(args, AtomicFenceOp::SeqCst)?,\n \n-            \"atomic_xchg\" => this.atomic_exchange(args, dest, AtomicRWOp::SeqCst)?,\n-            \"atomic_xchg_acq\" => this.atomic_exchange(args, dest, AtomicRWOp::Acquire)?,\n-            \"atomic_xchg_rel\" => this.atomic_exchange(args, dest, AtomicRWOp::Release)?,\n-            \"atomic_xchg_acqrel\" => this.atomic_exchange(args, dest, AtomicRWOp::AcqRel)?,\n-            \"atomic_xchg_relaxed\" => this.atomic_exchange(args, dest, AtomicRWOp::Relaxed)?,\n+            \"atomic_xchg\" => this.atomic_exchange(args, dest, AtomicRwOp::SeqCst)?,\n+            \"atomic_xchg_acq\" => this.atomic_exchange(args, dest, AtomicRwOp::Acquire)?,\n+            \"atomic_xchg_rel\" => this.atomic_exchange(args, dest, AtomicRwOp::Release)?,\n+            \"atomic_xchg_acqrel\" => this.atomic_exchange(args, dest, AtomicRwOp::AcqRel)?,\n+            \"atomic_xchg_relaxed\" => this.atomic_exchange(args, dest, AtomicRwOp::Relaxed)?,\n \n             \"atomic_cxchg\" => this.atomic_compare_exchange(\n-                args, dest, AtomicRWOp::SeqCst, AtomicReadOp::SeqCst\n+                args, dest, AtomicRwOp::SeqCst, AtomicReadOp::SeqCst\n             )?,\n             \"atomic_cxchg_acq\" => this.atomic_compare_exchange(\n-                args, dest, AtomicRWOp::Acquire, AtomicReadOp::Acquire\n+                args, dest, AtomicRwOp::Acquire, AtomicReadOp::Acquire\n             )?,\n             \"atomic_cxchg_rel\" => this.atomic_compare_exchange(\n-                args, dest, AtomicRWOp::Release, AtomicReadOp::Relaxed\n+                args, dest, AtomicRwOp::Release, AtomicReadOp::Relaxed\n             )?,\n             \"atomic_cxchg_acqrel\" => this.atomic_compare_exchange\n-            (args, dest, AtomicRWOp::AcqRel, AtomicReadOp::Acquire\n+            (args, dest, AtomicRwOp::AcqRel, AtomicReadOp::Acquire\n             )?,\n             \"atomic_cxchg_relaxed\" => this.atomic_compare_exchange(\n-                args, dest, AtomicRWOp::Relaxed, AtomicReadOp::Relaxed\n+                args, dest, AtomicRwOp::Relaxed, AtomicReadOp::Relaxed\n             )?,\n             \"atomic_cxchg_acq_failrelaxed\" => this.atomic_compare_exchange(\n-                args, dest, AtomicRWOp::Acquire, AtomicReadOp::Relaxed\n+                args, dest, AtomicRwOp::Acquire, AtomicReadOp::Relaxed\n             )?,\n             \"atomic_cxchg_acqrel_failrelaxed\" => this.atomic_compare_exchange(\n-                args, dest, AtomicRWOp::AcqRel, AtomicReadOp::Relaxed\n+                args, dest, AtomicRwOp::AcqRel, AtomicReadOp::Relaxed\n             )?,\n             \"atomic_cxchg_failrelaxed\" => this.atomic_compare_exchange(\n-                args, dest, AtomicRWOp::SeqCst, AtomicReadOp::Relaxed\n+                args, dest, AtomicRwOp::SeqCst, AtomicReadOp::Relaxed\n             )?,\n             \"atomic_cxchg_failacq\" => this.atomic_compare_exchange(\n-                args, dest, AtomicRWOp::SeqCst, AtomicReadOp::Acquire\n+                args, dest, AtomicRwOp::SeqCst, AtomicReadOp::Acquire\n             )?,\n \n             \"atomic_cxchgweak\" => this.atomic_compare_exchange_weak(\n-                args, dest, AtomicRWOp::SeqCst, AtomicReadOp::SeqCst\n+                args, dest, AtomicRwOp::SeqCst, AtomicReadOp::SeqCst\n             )?,\n             \"atomic_cxchgweak_acq\" => this.atomic_compare_exchange_weak(\n-                args, dest, AtomicRWOp::Acquire, AtomicReadOp::Acquire\n+                args, dest, AtomicRwOp::Acquire, AtomicReadOp::Acquire\n             )?,\n             \"atomic_cxchgweak_rel\" => this.atomic_compare_exchange_weak(\n-                args, dest, AtomicRWOp::Release, AtomicReadOp::Relaxed\n+                args, dest, AtomicRwOp::Release, AtomicReadOp::Relaxed\n             )?,\n             \"atomic_cxchgweak_acqrel\" => this.atomic_compare_exchange_weak(\n-                args, dest, AtomicRWOp::AcqRel, AtomicReadOp::Acquire\n+                args, dest, AtomicRwOp::AcqRel, AtomicReadOp::Acquire\n             )?,\n             \"atomic_cxchgweak_relaxed\" => this.atomic_compare_exchange_weak(\n-                args, dest, AtomicRWOp::Relaxed, AtomicReadOp::Relaxed\n+                args, dest, AtomicRwOp::Relaxed, AtomicReadOp::Relaxed\n             )?,\n             \"atomic_cxchgweak_acq_failrelaxed\" => this.atomic_compare_exchange_weak(\n-                args, dest, AtomicRWOp::Acquire, AtomicReadOp::Relaxed\n+                args, dest, AtomicRwOp::Acquire, AtomicReadOp::Relaxed\n             )?,\n             \"atomic_cxchgweak_acqrel_failrelaxed\" => this.atomic_compare_exchange_weak(\n-                args, dest, AtomicRWOp::AcqRel, AtomicReadOp::Relaxed\n+                args, dest, AtomicRwOp::AcqRel, AtomicReadOp::Relaxed\n             )?,\n             \"atomic_cxchgweak_failrelaxed\" => this.atomic_compare_exchange_weak(\n-                args, dest, AtomicRWOp::SeqCst, AtomicReadOp::Relaxed\n+                args, dest, AtomicRwOp::SeqCst, AtomicReadOp::Relaxed\n             )?,\n             \"atomic_cxchgweak_failacq\" => this.atomic_compare_exchange_weak(\n-                args, dest, AtomicRWOp::SeqCst, AtomicReadOp::Acquire\n+                args, dest, AtomicRwOp::SeqCst, AtomicReadOp::Acquire\n             )?,\n \n-            \"atomic_or\" => this.atomic_op(args, dest, BinOp::BitOr, false, AtomicRWOp::SeqCst)?,\n-            \"atomic_or_acq\" => this.atomic_op(args, dest, BinOp::BitOr, false, AtomicRWOp::Acquire)?,\n-            \"atomic_or_rel\" => this.atomic_op(args, dest, BinOp::BitOr, false, AtomicRWOp::Release)?,\n-            \"atomic_or_acqrel\" => this.atomic_op(args, dest, BinOp::BitOr, false, AtomicRWOp::AcqRel)?,\n-            \"atomic_or_relaxed\" => this.atomic_op(args, dest, BinOp::BitOr, false, AtomicRWOp::Relaxed)?,\n-            \"atomic_xor\" => this.atomic_op(args, dest, BinOp::BitXor, false, AtomicRWOp::SeqCst)?,\n-            \"atomic_xor_acq\" => this.atomic_op(args, dest, BinOp::BitXor, false, AtomicRWOp::Acquire)?,\n-            \"atomic_xor_rel\" => this.atomic_op(args, dest, BinOp::BitXor, false, AtomicRWOp::Release)?,\n-            \"atomic_xor_acqrel\" => this.atomic_op(args, dest, BinOp::BitXor, false, AtomicRWOp::AcqRel)?,\n-            \"atomic_xor_relaxed\" => this.atomic_op(args, dest, BinOp::BitXor, false, AtomicRWOp::Relaxed)?,\n-            \"atomic_and\" => this.atomic_op(args, dest, BinOp::BitAnd, false, AtomicRWOp::SeqCst)?,\n-            \"atomic_and_acq\" => this.atomic_op(args, dest, BinOp::BitAnd, false, AtomicRWOp::Acquire)?,\n-            \"atomic_and_rel\" => this.atomic_op(args, dest, BinOp::BitAnd, false, AtomicRWOp::Release)?,\n-            \"atomic_and_acqrel\" => this.atomic_op(args, dest, BinOp::BitAnd, false, AtomicRWOp::AcqRel)?,\n-            \"atomic_and_relaxed\" => this.atomic_op(args, dest, BinOp::BitAnd, false, AtomicRWOp::Relaxed)?,\n-            \"atomic_nand\" => this.atomic_op(args, dest, BinOp::BitAnd, true, AtomicRWOp::SeqCst)?,\n-            \"atomic_nand_acq\" => this.atomic_op(args, dest, BinOp::BitAnd, true, AtomicRWOp::Acquire)?,\n-            \"atomic_nand_rel\" => this.atomic_op(args, dest, BinOp::BitAnd, true, AtomicRWOp::Release)?,\n-            \"atomic_nand_acqrel\" => this.atomic_op(args, dest, BinOp::BitAnd, true, AtomicRWOp::AcqRel)?,\n-            \"atomic_nand_relaxed\" => this.atomic_op(args, dest, BinOp::BitAnd, true, AtomicRWOp::Relaxed)?,\n-            \"atomic_xadd\" => this.atomic_op(args, dest, BinOp::Add, false, AtomicRWOp::SeqCst)?,\n-            \"atomic_xadd_acq\" => this.atomic_op(args, dest, BinOp::Add, false, AtomicRWOp::Acquire)?,\n-            \"atomic_xadd_rel\" => this.atomic_op(args, dest, BinOp::Add, false, AtomicRWOp::Release)?,\n-            \"atomic_xadd_acqrel\" => this.atomic_op(args, dest, BinOp::Add, false, AtomicRWOp::AcqRel)?,\n-            \"atomic_xadd_relaxed\" => this.atomic_op(args, dest, BinOp::Add, false, AtomicRWOp::Relaxed)?,\n-            \"atomic_xsub\" => this.atomic_op(args, dest, BinOp::Sub, false, AtomicRWOp::SeqCst)?,\n-            \"atomic_xsub_acq\" => this.atomic_op(args, dest, BinOp::Sub, false, AtomicRWOp::Acquire)?,\n-            \"atomic_xsub_rel\" => this.atomic_op(args, dest, BinOp::Sub, false, AtomicRWOp::Release)?,\n-            \"atomic_xsub_acqrel\" => this.atomic_op(args, dest, BinOp::Sub, false, AtomicRWOp::AcqRel)?,\n-            \"atomic_xsub_relaxed\" => this.atomic_op(args, dest, BinOp::Sub, false, AtomicRWOp::Relaxed)?,\n+            \"atomic_or\" => this.atomic_op(args, dest, BinOp::BitOr, false, AtomicRwOp::SeqCst)?,\n+            \"atomic_or_acq\" => this.atomic_op(args, dest, BinOp::BitOr, false, AtomicRwOp::Acquire)?,\n+            \"atomic_or_rel\" => this.atomic_op(args, dest, BinOp::BitOr, false, AtomicRwOp::Release)?,\n+            \"atomic_or_acqrel\" => this.atomic_op(args, dest, BinOp::BitOr, false, AtomicRwOp::AcqRel)?,\n+            \"atomic_or_relaxed\" => this.atomic_op(args, dest, BinOp::BitOr, false, AtomicRwOp::Relaxed)?,\n+            \"atomic_xor\" => this.atomic_op(args, dest, BinOp::BitXor, false, AtomicRwOp::SeqCst)?,\n+            \"atomic_xor_acq\" => this.atomic_op(args, dest, BinOp::BitXor, false, AtomicRwOp::Acquire)?,\n+            \"atomic_xor_rel\" => this.atomic_op(args, dest, BinOp::BitXor, false, AtomicRwOp::Release)?,\n+            \"atomic_xor_acqrel\" => this.atomic_op(args, dest, BinOp::BitXor, false, AtomicRwOp::AcqRel)?,\n+            \"atomic_xor_relaxed\" => this.atomic_op(args, dest, BinOp::BitXor, false, AtomicRwOp::Relaxed)?,\n+            \"atomic_and\" => this.atomic_op(args, dest, BinOp::BitAnd, false, AtomicRwOp::SeqCst)?,\n+            \"atomic_and_acq\" => this.atomic_op(args, dest, BinOp::BitAnd, false, AtomicRwOp::Acquire)?,\n+            \"atomic_and_rel\" => this.atomic_op(args, dest, BinOp::BitAnd, false, AtomicRwOp::Release)?,\n+            \"atomic_and_acqrel\" => this.atomic_op(args, dest, BinOp::BitAnd, false, AtomicRwOp::AcqRel)?,\n+            \"atomic_and_relaxed\" => this.atomic_op(args, dest, BinOp::BitAnd, false, AtomicRwOp::Relaxed)?,\n+            \"atomic_nand\" => this.atomic_op(args, dest, BinOp::BitAnd, true, AtomicRwOp::SeqCst)?,\n+            \"atomic_nand_acq\" => this.atomic_op(args, dest, BinOp::BitAnd, true, AtomicRwOp::Acquire)?,\n+            \"atomic_nand_rel\" => this.atomic_op(args, dest, BinOp::BitAnd, true, AtomicRwOp::Release)?,\n+            \"atomic_nand_acqrel\" => this.atomic_op(args, dest, BinOp::BitAnd, true, AtomicRwOp::AcqRel)?,\n+            \"atomic_nand_relaxed\" => this.atomic_op(args, dest, BinOp::BitAnd, true, AtomicRwOp::Relaxed)?,\n+            \"atomic_xadd\" => this.atomic_op(args, dest, BinOp::Add, false, AtomicRwOp::SeqCst)?,\n+            \"atomic_xadd_acq\" => this.atomic_op(args, dest, BinOp::Add, false, AtomicRwOp::Acquire)?,\n+            \"atomic_xadd_rel\" => this.atomic_op(args, dest, BinOp::Add, false, AtomicRwOp::Release)?,\n+            \"atomic_xadd_acqrel\" => this.atomic_op(args, dest, BinOp::Add, false, AtomicRwOp::AcqRel)?,\n+            \"atomic_xadd_relaxed\" => this.atomic_op(args, dest, BinOp::Add, false, AtomicRwOp::Relaxed)?,\n+            \"atomic_xsub\" => this.atomic_op(args, dest, BinOp::Sub, false, AtomicRwOp::SeqCst)?,\n+            \"atomic_xsub_acq\" => this.atomic_op(args, dest, BinOp::Sub, false, AtomicRwOp::Acquire)?,\n+            \"atomic_xsub_rel\" => this.atomic_op(args, dest, BinOp::Sub, false, AtomicRwOp::Release)?,\n+            \"atomic_xsub_acqrel\" => this.atomic_op(args, dest, BinOp::Sub, false, AtomicRwOp::AcqRel)?,\n+            \"atomic_xsub_relaxed\" => this.atomic_op(args, dest, BinOp::Sub, false, AtomicRwOp::Relaxed)?,\n \n \n             // Query type information\n@@ -514,7 +514,7 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: crate::MiriEvalContextExt<'mir, 'tcx\n \n     fn atomic_op(\n         &mut self, args: &[OpTy<'tcx, Tag>], dest: PlaceTy<'tcx, Tag>,\n-        op: mir::BinOp, neg: bool, atomic: AtomicRWOp\n+        op: mir::BinOp, neg: bool, atomic: AtomicRwOp\n     ) -> InterpResult<'tcx> {\n         let this = self.eval_context_mut();\n \n@@ -524,58 +524,41 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: crate::MiriEvalContextExt<'mir, 'tcx\n             bug!(\"Atomic arithmetic operations only work on integer types\");\n         }\n         let rhs = this.read_immediate(rhs)?;\n-        let old = this.allow_data_races_mut(|this| {\n-            this.read_immediate(place. into())\n-        })?;\n \n         // Check alignment requirements. Atomics must always be aligned to their size,\n         // even if the type they wrap would be less aligned (e.g. AtomicU64 on 32bit must\n         // be 8-aligned).\n         let align = Align::from_bytes(place.layout.size.bytes()).unwrap();\n         this.memory.check_ptr_access(place.ptr, place.layout.size, align)?;\n+        \n+        let old = this.atomic_op_immediate(place, rhs, op, neg, atomic)?;\n         this.write_immediate(*old, dest)?; // old value is returned\n-\n-        // Atomics wrap around on overflow.\n-        let val = this.binary_op(op, old, rhs)?;\n-        let val = if neg { this.unary_op(mir::UnOp::Not, val)? } else { val };\n-        this.allow_data_races_mut(|this| {\n-            this.write_immediate(*val, place.into())\n-        })?;\n-\n-        this.validate_atomic_rmw(place, atomic)?;\n         Ok(())\n     }\n     \n     fn atomic_exchange(\n-        &mut self, args: &[OpTy<'tcx, Tag>], dest: PlaceTy<'tcx, Tag>, atomic: AtomicRWOp\n+        &mut self, args: &[OpTy<'tcx, Tag>], dest: PlaceTy<'tcx, Tag>, atomic: AtomicRwOp\n     ) -> InterpResult<'tcx> {\n         let this = self.eval_context_mut();\n \n         let &[place, new] = check_arg_count(args)?;\n         let place = this.deref_operand(place)?;\n         let new = this.read_scalar(new)?;\n-        let old = this.allow_data_races_mut(|this| {\n-            this.read_scalar(place.into())\n-        })?;\n \n         // Check alignment requirements. Atomics must always be aligned to their size,\n         // even if the type they wrap would be less aligned (e.g. AtomicU64 on 32bit must\n         // be 8-aligned).\n         let align = Align::from_bytes(place.layout.size.bytes()).unwrap();\n         this.memory.check_ptr_access(place.ptr, place.layout.size, align)?;\n \n+        let old = this.atomic_exchange_scalar(place, new, atomic)?;\n         this.write_scalar(old, dest)?; // old value is returned\n-        this.allow_data_races_mut(|this| {\n-            this.write_scalar(new, place.into())\n-        })?;\n-\n-        this.validate_atomic_rmw(place, atomic)?;\n         Ok(())\n     }\n \n     fn atomic_compare_exchange(\n         &mut self, args: &[OpTy<'tcx, Tag>], dest: PlaceTy<'tcx, Tag>,\n-        success: AtomicRWOp, fail: AtomicReadOp\n+        success: AtomicRwOp, fail: AtomicReadOp\n     ) -> InterpResult<'tcx> {\n         let this = self.eval_context_mut();\n \n@@ -584,45 +567,26 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: crate::MiriEvalContextExt<'mir, 'tcx\n         let expect_old = this.read_immediate(expect_old)?; // read as immediate for the sake of `binary_op()`\n         let new = this.read_scalar(new)?;\n \n-        // Failure ordering cannot be stronger than success ordering, therefore first attempt\n-        //  to read with the failure ordering and if successfull then try again with the success\n-        //  read ordering and write in the success case.\n-        // Read as immediate for the sake of `binary_op()`\n-        let old = this.allow_data_races_mut(|this| {\n-            this.read_immediate(place.into())\n-        })?; \n \n         // Check alignment requirements. Atomics must always be aligned to their size,\n         // even if the type they wrap would be less aligned (e.g. AtomicU64 on 32bit must\n         // be 8-aligned).\n         let align = Align::from_bytes(place.layout.size.bytes()).unwrap();\n         this.memory.check_ptr_access(place.ptr, place.layout.size, align)?;\n \n-        // `binary_op` will bail if either of them is not a scalar.\n-        let eq = this.overflowing_binary_op(mir::BinOp::Eq, old, expect_old)?.0;\n-        let res = Immediate::ScalarPair(old.to_scalar_or_uninit(), eq.into());\n+        \n+        let old = this.atomic_compare_exchange_scalar(\n+            place, expect_old, new, success, fail\n+        )?;\n \n         // Return old value.\n-        this.write_immediate(res, dest)?;\n-\n-        // Update ptr depending on comparison.\n-        //  if successful, perform a full rw-atomic validation\n-        //  otherwise treat this as an atomic load with the fail ordering\n-        if eq.to_bool()? {\n-            this.allow_data_races_mut(|this| {\n-                this.write_scalar(new, place.into())\n-            })?;\n-            this.validate_atomic_rmw(place, success)?;\n-        } else {\n-            this.validate_atomic_load(place, fail)?;\n-        }\n-\n+        this.write_immediate(old, dest)?;\n         Ok(())\n     }\n \n     fn atomic_compare_exchange_weak(\n         &mut self, args: &[OpTy<'tcx, Tag>], dest: PlaceTy<'tcx, Tag>,\n-        success: AtomicRWOp, fail: AtomicReadOp\n+        success: AtomicRwOp, fail: AtomicReadOp\n     ) -> InterpResult<'tcx> {\n \n         // FIXME: the weak part of this is currently not modelled,"}, {"sha": "78244ab7b87947480cc1b6e852ea8fe8a77ca2ec", "filename": "src/shims/posix/linux/sync.rs", "status": "modified", "additions": 11, "deletions": 1, "changes": 12, "blob_url": "https://github.com/rust-lang/rust/blob/69fb6413ddc5b7fd5d9cb0a68ebf58ee513bf9d5/src%2Fshims%2Fposix%2Flinux%2Fsync.rs", "raw_url": "https://github.com/rust-lang/rust/raw/69fb6413ddc5b7fd5d9cb0a68ebf58ee513bf9d5/src%2Fshims%2Fposix%2Flinux%2Fsync.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Fshims%2Fposix%2Flinux%2Fsync.rs?ref=69fb6413ddc5b7fd5d9cb0a68ebf58ee513bf9d5", "patch": "@@ -78,7 +78,17 @@ pub fn futex<'tcx>(\n             // Read an `i32` through the pointer, regardless of any wrapper types.\n             // It's not uncommon for `addr` to be passed as another type than `*mut i32`, such as `*const AtomicI32`.\n             // FIXME: this fails if `addr` is not a pointer type.\n-            // FIXME: what form of atomic operation should the `futex` use to load the value?\n+            // The atomic ordering for futex(https://man7.org/linux/man-pages/man2/futex.2.html):\n+            //  \"The load of the value of the futex word is an\n+            //   atomic memory access (i.e., using atomic machine instructions\n+            //   of the respective architecture).  This load, the comparison\n+            //   with the expected value, and starting to sleep are performed\n+            //   atomically and totally ordered with respect to other futex\n+            //   operations on the same futex word.\"\n+            // SeqCst is total order over all operations, so uses acquire,\n+            // either are equal under the current implementation.\n+            // FIXME: is Acquire correct or should some additional ordering constraints be observed?\n+            // FIXME: use RMW or similar?\n             let futex_val = this.read_scalar_at_offset_atomic(\n                 addr.into(), 0, this.machine.layouts.i32, AtomicReadOp::Acquire\n             )?.to_i32()?;"}, {"sha": "64308d06139f347e7b7a69253c46354836e38f7c", "filename": "src/shims/posix/sync.rs", "status": "modified", "additions": 8, "deletions": 8, "changes": 16, "blob_url": "https://github.com/rust-lang/rust/blob/69fb6413ddc5b7fd5d9cb0a68ebf58ee513bf9d5/src%2Fshims%2Fposix%2Fsync.rs", "raw_url": "https://github.com/rust-lang/rust/raw/69fb6413ddc5b7fd5d9cb0a68ebf58ee513bf9d5/src%2Fshims%2Fposix%2Fsync.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Fshims%2Fposix%2Fsync.rs?ref=69fb6413ddc5b7fd5d9cb0a68ebf58ee513bf9d5", "patch": "@@ -64,7 +64,7 @@ fn mutex_get_kind<'mir, 'tcx: 'mir>(\n     let offset = if ecx.pointer_size().bytes() == 8 { 16 } else { 12 };\n     ecx.read_scalar_at_offset_atomic(\n         mutex_op, offset, ecx.machine.layouts.i32,\n-        AtomicReadOp::SeqCst\n+        AtomicReadOp::Acquire\n     )\n }\n \n@@ -76,7 +76,7 @@ fn mutex_set_kind<'mir, 'tcx: 'mir>(\n     let offset = if ecx.pointer_size().bytes() == 8 { 16 } else { 12 };\n     ecx.write_scalar_at_offset_atomic(\n         mutex_op, offset, kind, ecx.machine.layouts.i32, \n-        AtomicWriteOp::SeqCst\n+        AtomicWriteOp::Release\n     )\n }\n \n@@ -85,7 +85,7 @@ fn mutex_get_id<'mir, 'tcx: 'mir>(\n     mutex_op: OpTy<'tcx, Tag>,\n ) -> InterpResult<'tcx, ScalarMaybeUninit<Tag>> {\n     ecx.read_scalar_at_offset_atomic(\n-        mutex_op, 4, ecx.machine.layouts.u32, AtomicReadOp::SeqCst\n+        mutex_op, 4, ecx.machine.layouts.u32, AtomicReadOp::Acquire\n     )\n }\n \n@@ -96,7 +96,7 @@ fn mutex_set_id<'mir, 'tcx: 'mir>(\n ) -> InterpResult<'tcx, ()> {\n     ecx.write_scalar_at_offset_atomic(\n         mutex_op, 4, id, ecx.machine.layouts.u32,\n-        AtomicWriteOp::SeqCst\n+        AtomicWriteOp::Release\n     )\n }\n \n@@ -129,7 +129,7 @@ fn rwlock_get_id<'mir, 'tcx: 'mir>(\n ) -> InterpResult<'tcx, ScalarMaybeUninit<Tag>> {\n     ecx.read_scalar_at_offset_atomic(\n         rwlock_op, 4, ecx.machine.layouts.u32,\n-        AtomicReadOp::SeqCst\n+        AtomicReadOp::Acquire\n     )\n }\n \n@@ -140,7 +140,7 @@ fn rwlock_set_id<'mir, 'tcx: 'mir>(\n ) -> InterpResult<'tcx, ()> {\n     ecx.write_scalar_at_offset_atomic(\n         rwlock_op, 4, id, ecx.machine.layouts.u32,\n-        AtomicWriteOp::SeqCst\n+        AtomicWriteOp::Release\n     )\n }\n \n@@ -196,7 +196,7 @@ fn cond_get_id<'mir, 'tcx: 'mir>(\n ) -> InterpResult<'tcx, ScalarMaybeUninit<Tag>> {\n     ecx.read_scalar_at_offset_atomic(\n         cond_op, 4, ecx.machine.layouts.u32,\n-        AtomicReadOp::SeqCst\n+        AtomicReadOp::Acquire\n     )\n }\n \n@@ -207,7 +207,7 @@ fn cond_set_id<'mir, 'tcx: 'mir>(\n ) -> InterpResult<'tcx, ()> {\n     ecx.write_scalar_at_offset_atomic(\n         cond_op, 4, id, ecx.machine.layouts.u32,\n-        AtomicWriteOp::SeqCst\n+        AtomicWriteOp::Release\n     )\n }\n "}, {"sha": "847d083bfa9f7956dc93e9166e3e077515620ad8", "filename": "src/shims/posix/thread.rs", "status": "modified", "additions": 8, "deletions": 7, "changes": 15, "blob_url": "https://github.com/rust-lang/rust/blob/69fb6413ddc5b7fd5d9cb0a68ebf58ee513bf9d5/src%2Fshims%2Fposix%2Fthread.rs", "raw_url": "https://github.com/rust-lang/rust/raw/69fb6413ddc5b7fd5d9cb0a68ebf58ee513bf9d5/src%2Fshims%2Fposix%2Fthread.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Fshims%2Fposix%2Fthread.rs?ref=69fb6413ddc5b7fd5d9cb0a68ebf58ee513bf9d5", "patch": "@@ -15,30 +15,31 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: crate::MiriEvalContextExt<'mir, 'tcx\n         let this = self.eval_context_mut();\n \n         this.tcx.sess.warn(\n-            \"thread support is experimental.\",\n+            \"thread support is experimental, no weak memory effects are currently emulated.\",\n         );\n \n         // Create the new thread\n         let new_thread_id = this.create_thread();\n \n         // Write the current thread-id, switch to the next thread later\n-        //  to treat this write operation as occuring on this thread index\n+        // to treat this write operation as occuring on the current thread.\n         let thread_info_place = this.deref_operand(thread)?;\n         this.write_scalar(\n             Scalar::from_uint(new_thread_id.to_u32(), thread_info_place.layout.size),\n             thread_info_place.into(),\n         )?;\n \n         // Read the function argument that will be sent to the new thread\n-        //  again perform the read before the thread starts executing.\n+        // before the thread starts executing since reading after the \n+        // context switch will incorrectly report a data-race.\n         let fn_ptr = this.read_scalar(start_routine)?.check_init()?;\n         let func_arg = this.read_immediate(arg)?;\n \n-        // Also switch to new thread so that we can push the first stackframe.\n-        //  after this all accesses will be treated as occuring in the new thread\n+        // Finally switch to new thread so that we can push the first stackframe.\n+        // After this all accesses will be treated as occuring in the new thread.\n         let old_thread_id = this.set_active_thread(new_thread_id);\n \n-        // Perform the function pointer load in the new thread frame\n+        // Perform the function pointer load in the new thread frame.\n         let instance = this.memory.get_fn(fn_ptr)?.as_instance()?;\n \n         // Note: the returned value is currently ignored (see the FIXME in\n@@ -54,7 +55,7 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: crate::MiriEvalContextExt<'mir, 'tcx\n             StackPopCleanup::None { cleanup: true },\n         )?;\n \n-        // Restore the old active thread frame\n+        // Restore the old active thread frame.\n         this.set_active_thread(old_thread_id);\n \n         Ok(0)"}, {"sha": "828268c06ccf0c0797fcb6271ddf00f6601944c0", "filename": "src/sync.rs", "status": "modified", "additions": 39, "deletions": 19, "changes": 58, "blob_url": "https://github.com/rust-lang/rust/blob/69fb6413ddc5b7fd5d9cb0a68ebf58ee513bf9d5/src%2Fsync.rs", "raw_url": "https://github.com/rust-lang/rust/raw/69fb6413ddc5b7fd5d9cb0a68ebf58ee513bf9d5/src%2Fsync.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Fsync.rs?ref=69fb6413ddc5b7fd5d9cb0a68ebf58ee513bf9d5", "patch": "@@ -62,7 +62,7 @@ struct Mutex {\n     /// The queue of threads waiting for this mutex.\n     queue: VecDeque<ThreadId>,\n     /// Data race handle\n-    data_race: DataRaceLockHandle\n+    data_race: VClock\n }\n \n declare_id!(RwLockId);\n@@ -80,9 +80,9 @@ struct RwLock {\n     /// The queue of reader threads waiting for this lock.\n     reader_queue: VecDeque<ThreadId>,\n     /// Data race handle for writers\n-    data_race: DataRaceLockHandle,\n+    data_race: VClock,\n     /// Data race handle for readers\n-    data_race_reader: DataRaceLockHandle,\n+    data_race_reader: VClock,\n }\n \n declare_id!(CondvarId);\n@@ -100,14 +100,14 @@ struct CondvarWaiter {\n #[derive(Default, Debug)]\n struct Condvar {\n     waiters: VecDeque<CondvarWaiter>,\n-    data_race: DataRaceLockHandle,\n+    data_race: VClock,\n }\n \n /// The futex state.\n #[derive(Default, Debug)]\n struct Futex {\n     waiters: VecDeque<FutexWaiter>,\n-    data_race: DataRaceLockHandle,\n+    data_race: VClock,\n }\n \n /// A thread waiting on a futex.\n@@ -213,7 +213,9 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: crate::MiriEvalContextExt<'mir, 'tcx\n             mutex.owner = Some(thread);\n         }\n         mutex.lock_count = mutex.lock_count.checked_add(1).unwrap();\n-        this.memory.extra.data_race.validate_lock_acquire(&mutex.data_race, thread);\n+        if let Some(data_race) = &this.memory.extra.data_race {\n+            data_race.validate_lock_acquire(&mutex.data_race, thread);\n+        }\n     }\n \n     /// Try unlocking by decreasing the lock count and returning the old lock\n@@ -241,7 +243,9 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: crate::MiriEvalContextExt<'mir, 'tcx\n                 mutex.owner = None;\n                 // The mutex is completely unlocked. Try transfering ownership\n                 // to another thread.\n-                this.memory.extra.data_race.validate_lock_release(&mut mutex.data_race, current_owner);\n+                if let Some(data_race) = &this.memory.extra.data_race {\n+                    data_race.validate_lock_release(&mut mutex.data_race, current_owner);\n+                }\n                 this.mutex_dequeue_and_lock(id);\n             }\n             Some(old_lock_count)\n@@ -297,7 +301,9 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: crate::MiriEvalContextExt<'mir, 'tcx\n         let rwlock = &mut this.machine.threads.sync.rwlocks[id];\n         let count = rwlock.readers.entry(reader).or_insert(0);\n         *count = count.checked_add(1).expect(\"the reader counter overflowed\");\n-        this.memory.extra.data_race.validate_lock_acquire(&rwlock.data_race, reader);\n+        if let Some(data_race) = &this.memory.extra.data_race {\n+            data_race.validate_lock_acquire(&rwlock.data_race, reader);\n+        }\n     }\n \n     /// Try read-unlock the lock for `reader` and potentially give the lock to a new owner.\n@@ -319,7 +325,9 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: crate::MiriEvalContextExt<'mir, 'tcx\n             }\n             Entry::Vacant(_) => return false, // we did not even own this lock\n         }\n-        this.memory.extra.data_race.validate_lock_release_shared(&mut rwlock.data_race_reader, reader);\n+        if let Some(data_race) = &this.memory.extra.data_race {\n+            data_race.validate_lock_release_shared(&mut rwlock.data_race_reader, reader);\n+        }\n \n         // The thread was a reader. If the lock is not held any more, give it to a writer.\n         if this.rwlock_is_locked(id).not() {\n@@ -328,7 +336,7 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: crate::MiriEvalContextExt<'mir, 'tcx\n             //  of the union of all reader data race handles, since the set of readers\n             //  happen-before the writers\n             let rwlock = &mut this.machine.threads.sync.rwlocks[id];\n-            rwlock.data_race.set_values(&rwlock.data_race_reader);\n+            rwlock.data_race.clone_from(&rwlock.data_race_reader);\n             this.rwlock_dequeue_and_lock_writer(id);\n         }\n         true\n@@ -355,7 +363,9 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: crate::MiriEvalContextExt<'mir, 'tcx\n         trace!(\"rwlock_writer_lock: {:?} now held by {:?}\", id, writer);\n         let rwlock = &mut this.machine.threads.sync.rwlocks[id];\n         rwlock.writer = Some(writer);\n-        this.memory.extra.data_race.validate_lock_acquire(&rwlock.data_race, writer);\n+        if let Some(data_race) = &this.memory.extra.data_race {\n+            data_race.validate_lock_acquire(&rwlock.data_race, writer);\n+        }\n     }\n \n     #[inline]\n@@ -373,8 +383,10 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: crate::MiriEvalContextExt<'mir, 'tcx\n             // Release memory to both reader and writer vector clocks\n             //  since this writer happens-before both the union of readers once they are finished\n             //  and the next writer\n-            this.memory.extra.data_race.validate_lock_release(&mut rwlock.data_race, current_writer);\n-            this.memory.extra.data_race.validate_lock_release(&mut rwlock.data_race_reader, current_writer);\n+            if let Some(data_race) = &this.memory.extra.data_race {\n+                data_race.validate_lock_release(&mut rwlock.data_race, current_writer);\n+                data_race.validate_lock_release(&mut rwlock.data_race_reader, current_writer);\n+            }\n             // The thread was a writer.\n             //\n             // We are prioritizing writers here against the readers. As a\n@@ -435,14 +447,18 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: crate::MiriEvalContextExt<'mir, 'tcx\n         let this = self.eval_context_mut();\n         let current_thread = this.get_active_thread();\n         let condvar = &mut this.machine.threads.sync.condvars[id];\n-        let data_race = &mut this.memory.extra.data_race;\n+        let data_race = &this.memory.extra.data_race;\n \n         // Each condvar signal happens-before the end of the condvar wake\n-        data_race.validate_lock_release(&mut condvar.data_race, current_thread);\n+        if let Some(data_race) = data_race {\n+            data_race.validate_lock_release(&mut condvar.data_race, current_thread);\n+        }\n         condvar.waiters\n             .pop_front()\n             .map(|waiter| {\n-                data_race.validate_lock_acquire(&mut condvar.data_race, waiter.thread);\n+                if let Some(data_race) = data_race {\n+                    data_race.validate_lock_acquire(&mut condvar.data_race, waiter.thread);\n+                }\n                 (waiter.thread, waiter.mutex)\n             })\n     }\n@@ -466,12 +482,16 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: crate::MiriEvalContextExt<'mir, 'tcx\n         let this = self.eval_context_mut();\n         let current_thread = this.get_active_thread();\n         let futex = &mut this.machine.threads.sync.futexes.get_mut(&addr.erase_tag())?;\n-        let data_race =  &mut this.memory.extra.data_race;\n+        let data_race =  &this.memory.extra.data_race;\n \n         // Each futex-wake happens-before the end of the futex wait\n-        data_race.validate_lock_release(&mut futex.data_race, current_thread);\n+        if let Some(data_race) = data_race {\n+            data_race.validate_lock_release(&mut futex.data_race, current_thread);\n+        }\n         let res = futex.waiters.pop_front().map(|waiter| {\n-            data_race.validate_lock_acquire(&futex.data_race, waiter.thread);  \n+            if let Some(data_race) = data_race {\n+                data_race.validate_lock_acquire(&futex.data_race, waiter.thread);  \n+            }\n             waiter.thread\n         });\n         res"}, {"sha": "5d783430417bef7030c1fa732ff0a6ccbbe781a8", "filename": "src/thread.rs", "status": "modified", "additions": 32, "deletions": 17, "changes": 49, "blob_url": "https://github.com/rust-lang/rust/blob/69fb6413ddc5b7fd5d9cb0a68ebf58ee513bf9d5/src%2Fthread.rs", "raw_url": "https://github.com/rust-lang/rust/raw/69fb6413ddc5b7fd5d9cb0a68ebf58ee513bf9d5/src%2Fthread.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Fthread.rs?ref=69fb6413ddc5b7fd5d9cb0a68ebf58ee513bf9d5", "patch": "@@ -3,6 +3,7 @@\n use std::cell::RefCell;\n use std::collections::hash_map::Entry;\n use std::convert::TryFrom;\n+use std::rc::Rc;\n use std::num::TryFromIntError;\n use std::time::{Duration, Instant, SystemTime};\n \n@@ -327,7 +328,7 @@ impl<'mir, 'tcx: 'mir> ThreadManager<'mir, 'tcx> {\n     }\n \n     /// Mark that the active thread tries to join the thread with `joined_thread_id`.\n-    fn join_thread(&mut self, joined_thread_id: ThreadId, data_race: &data_race::GlobalState) -> InterpResult<'tcx> {\n+    fn join_thread(&mut self, joined_thread_id: ThreadId, data_race: &Option<Rc<data_race::GlobalState>>) -> InterpResult<'tcx> {\n         if self.threads[joined_thread_id].join_status != ThreadJoinStatus::Joinable {\n             throw_ub_format!(\"trying to join a detached or already joined thread\");\n         }\n@@ -351,9 +352,11 @@ impl<'mir, 'tcx: 'mir> ThreadManager<'mir, 'tcx> {\n                 self.active_thread,\n                 joined_thread_id\n             );\n-        }else{\n+        } else {\n             // The thread has already terminated - mark join happens-before\n-            data_race.thread_joined(self.active_thread, joined_thread_id);\n+            if let Some(data_race) = data_race {\n+                data_race.thread_joined(self.active_thread, joined_thread_id);\n+            }\n         }\n         Ok(())\n     }\n@@ -428,7 +431,7 @@ impl<'mir, 'tcx: 'mir> ThreadManager<'mir, 'tcx> {\n \n     /// Wakes up threads joining on the active one and deallocates thread-local statics.\n     /// The `AllocId` that can now be freed is returned.\n-    fn thread_terminated(&mut self, data_race: &data_race::GlobalState) -> Vec<AllocId> {\n+    fn thread_terminated(&mut self, data_race: &Option<Rc<data_race::GlobalState>>) -> Vec<AllocId> {\n         let mut free_tls_statics = Vec::new();\n         {\n             let mut thread_local_statics = self.thread_local_alloc_ids.borrow_mut();\n@@ -444,12 +447,16 @@ impl<'mir, 'tcx: 'mir> ThreadManager<'mir, 'tcx> {\n             });\n         }\n         // Set the thread into a terminated state in the data-race detector\n-        data_race.thread_terminated();\n+        if let Some(data_race) = data_race {\n+            data_race.thread_terminated();\n+        }\n         // Check if we need to unblock any threads.\n         for (i, thread) in self.threads.iter_enumerated_mut() {\n             if thread.state == ThreadState::BlockedOnJoin(self.active_thread) {\n                 // The thread has terminated, mark happens-before edge to joining thread\n-                data_race.thread_joined(i, self.active_thread);\n+                if let Some(data_race) = data_race {\n+                    data_race.thread_joined(i, self.active_thread);\n+                }\n                 trace!(\"unblocking {:?} because {:?} terminated\", i, self.active_thread);\n                 thread.state = ThreadState::Enabled;\n             }\n@@ -463,7 +470,7 @@ impl<'mir, 'tcx: 'mir> ThreadManager<'mir, 'tcx> {\n     /// used in stateless model checkers such as Loom: run the active thread as\n     /// long as we can and switch only when we have to (the active thread was\n     /// blocked, terminated, or has explicitly asked to be preempted).\n-    fn schedule(&mut self, data_race: &data_race::GlobalState) -> InterpResult<'tcx, SchedulingAction> {\n+    fn schedule(&mut self, data_race: &Option<Rc<data_race::GlobalState>>) -> InterpResult<'tcx, SchedulingAction> {\n         // Check whether the thread has **just** terminated (`check_terminated`\n         // checks whether the thread has popped all its stack and if yes, sets\n         // the thread state to terminated).\n@@ -508,7 +515,9 @@ impl<'mir, 'tcx: 'mir> ThreadManager<'mir, 'tcx> {\n             if thread.state == ThreadState::Enabled {\n                 if !self.yield_active_thread || id != self.active_thread {\n                     self.active_thread = id;\n-                    data_race.thread_set_active(self.active_thread);\n+                    if let Some(data_race) = data_race {\n+                        data_race.thread_set_active(self.active_thread);\n+                    }\n                     break;\n                 }\n             }\n@@ -563,7 +572,9 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: crate::MiriEvalContextExt<'mir, 'tcx\n     fn create_thread(&mut self) -> ThreadId {\n         let this = self.eval_context_mut();\n         let id = this.machine.threads.create_thread();\n-        this.memory.extra.data_race.thread_created(id);\n+        if let Some(data_race) = &this.memory.extra.data_race {\n+            data_race.thread_created(id);\n+        }\n         id\n     }\n \n@@ -576,15 +587,17 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: crate::MiriEvalContextExt<'mir, 'tcx\n     #[inline]\n     fn join_thread(&mut self, joined_thread_id: ThreadId) -> InterpResult<'tcx> {\n         let this = self.eval_context_mut();\n-        let data_race = &*this.memory.extra.data_race;\n+        let data_race = &this.memory.extra.data_race;\n         this.machine.threads.join_thread(joined_thread_id, data_race)?;\n         Ok(())\n     }\n \n     #[inline]\n     fn set_active_thread(&mut self, thread_id: ThreadId) -> ThreadId {\n         let this = self.eval_context_mut();\n-        this.memory.extra.data_race.thread_set_active(thread_id);\n+        if let Some(data_race) = &this.memory.extra.data_race {\n+            data_race.thread_set_active(thread_id);\n+        }\n         this.machine.threads.set_active_thread_id(thread_id)\n     }\n \n@@ -639,10 +652,12 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: crate::MiriEvalContextExt<'mir, 'tcx\n     #[inline]\n     fn set_active_thread_name(&mut self, new_thread_name: Vec<u8>) {\n         let this = self.eval_context_mut();\n-        if let Ok(string) = String::from_utf8(new_thread_name.clone()) {\n-            this.memory.extra.data_race.thread_set_name(\n-                this.machine.threads.active_thread, string\n-            );\n+        if let Some(data_race) = &this.memory.extra.data_race {\n+            if let Ok(string) = String::from_utf8(new_thread_name.clone()) {\n+                data_race.thread_set_name(\n+                    this.machine.threads.active_thread, string\n+                );\n+            }\n         }\n         this.machine.threads.set_thread_name(new_thread_name);\n     }\n@@ -713,7 +728,7 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: crate::MiriEvalContextExt<'mir, 'tcx\n     #[inline]\n     fn schedule(&mut self) -> InterpResult<'tcx, SchedulingAction> {\n         let this = self.eval_context_mut();\n-        let data_race = &*this.memory.extra.data_race;\n+        let data_race = &this.memory.extra.data_race;\n         this.machine.threads.schedule(data_race)\n     }\n \n@@ -724,7 +739,7 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: crate::MiriEvalContextExt<'mir, 'tcx\n     #[inline]\n     fn thread_terminated(&mut self) -> InterpResult<'tcx> {\n         let this = self.eval_context_mut();\n-        let data_race = &*this.memory.extra.data_race;\n+        let data_race = &this.memory.extra.data_race;\n         for alloc_id in this.machine.threads.thread_terminated(data_race) {\n             let ptr = this.memory.global_base_pointer(alloc_id.into())?;\n             this.memory.deallocate(ptr, None, MiriMemoryKind::Tls.into())?;"}, {"sha": "110b278852d50af4f128079c185bd5a7bbe8c2f7", "filename": "src/vector_clock.rs", "status": "modified", "additions": 183, "deletions": 140, "changes": 323, "blob_url": "https://github.com/rust-lang/rust/blob/69fb6413ddc5b7fd5d9cb0a68ebf58ee513bf9d5/src%2Fvector_clock.rs", "raw_url": "https://github.com/rust-lang/rust/raw/69fb6413ddc5b7fd5d9cb0a68ebf58ee513bf9d5/src%2Fvector_clock.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Fvector_clock.rs?ref=69fb6413ddc5b7fd5d9cb0a68ebf58ee513bf9d5", "patch": "@@ -1,121 +1,132 @@\n use std::{\n     fmt::{self, Debug}, cmp::Ordering, ops::Index,\n-    num::TryFromIntError, convert::TryFrom, mem\n+    convert::TryFrom, mem\n };\n use smallvec::SmallVec;\n use rustc_index::vec::Idx;\n use rustc_data_structures::fx::FxHashMap;\n \n /// A vector clock index, this is associated with a thread id\n-///  but in some cases one vector index may be shared with\n-///  multiple thread ids.\n+/// but in some cases one vector index may be shared with\n+/// multiple thread ids id it safe to do so.\n #[derive(Clone, Copy, Debug, PartialOrd, Ord, PartialEq, Eq, Hash)]\n pub struct VectorIdx(u32);\n \n-impl VectorIdx{\n+impl VectorIdx {\n+\n+    #[inline(always)]\n     pub fn to_u32(self) -> u32 {\n         self.0\n     }\n+\n     pub const MAX_INDEX: VectorIdx = VectorIdx(u32::MAX);\n+\n }\n \n impl Idx for VectorIdx {\n+\n+    #[inline]\n     fn new(idx: usize) -> Self {\n         VectorIdx(u32::try_from(idx).unwrap())\n     }\n \n+    #[inline]\n     fn index(self) -> usize {\n         usize::try_from(self.0).unwrap()\n     }\n-}\n \n-impl TryFrom<u64> for VectorIdx {\n-    type Error = TryFromIntError;\n-    fn try_from(id: u64) -> Result<Self, Self::Error> {\n-        u32::try_from(id).map(|id_u32| Self(id_u32))\n-    }\n }\n \n impl From<u32> for VectorIdx {\n+\n+    #[inline]\n     fn from(id: u32) -> Self {\n         Self(id)\n     }\n-}\n \n+}\n \n-/// A sparse set of vector clocks, where each vector index\n-///  is associated with a vector clock.\n-/// This treats all vector clocks that have not been assigned\n-///  as equal to the all zero vector clocks\n-/// Is optimized for the common case where only 1 element is stored\n-///  in the set and the rest can be ignored, falling-back to\n-///  using an internal hash-map once more than 1 element is assigned\n-///  at any one time\n+/// A sparse mapping of vector index values to vector clocks, this\n+/// is optimized for the common case with only one element stored\n+/// inside the map.\n+/// This is used to store the set of currently active release\n+/// sequences at a given memory location, since RMW operations\n+/// allow for multiple release sequences to be active at once\n+/// and to be collapsed back to one active release sequence \n+/// once a non RMW atomic store operation occurs.\n+/// An all zero vector is considered to be equal to no\n+/// element stored internally since it will never be\n+/// stored and has no meaning as a release sequence\n+/// vector clock.\n #[derive(Clone)]\n-pub struct VSmallClockSet(VSmallClockSetInner);\n+pub struct VSmallClockMap(VSmallClockMapInner);\n \n #[derive(Clone)]\n-enum VSmallClockSetInner {\n+enum VSmallClockMapInner {\n+\n     /// Zero or 1 vector elements, common\n-    ///  case for the sparse set.\n+    /// case for the sparse set.\n     /// The all zero vector clock is treated\n-    ///  as equal to the empty element\n+    /// as equal to the empty element.\n     Small(VectorIdx, VClock),\n \n-    /// Hash-map of vector clocks\n+    /// Hash-map of vector clocks.\n     Large(FxHashMap<VectorIdx, VClock>)\n }\n \n-impl VSmallClockSet {\n+impl VSmallClockMap {\n \n     /// Remove all clock vectors from the map, setting them\n-    ///  to the zero vector\n+    /// to the zero vector.\n     pub fn clear(&mut self) {\n         match &mut self.0 {\n-            VSmallClockSetInner::Small(_, clock) => {\n+            VSmallClockMapInner::Small(_, clock) => {\n                 clock.set_zero_vector()\n             }\n-            VSmallClockSetInner::Large(hash_map) => {\n+            VSmallClockMapInner::Large(hash_map) => {\n                 hash_map.clear();\n             }\n         }\n     }\n \n     /// Remove all clock vectors except for the clock vector\n-    ///  stored at the given index, which is retained\n+    /// stored at the given index, which is retained.\n     pub fn retain_index(&mut self, index: VectorIdx) {\n         match &mut self.0 {\n-            VSmallClockSetInner::Small(small_idx, clock) => {\n+            VSmallClockMapInner::Small(small_idx, clock) => {\n                 if index != *small_idx {\n+\n                     // The zero-vector is considered to equal\n-                    //  the empty element\n+                    // the empty element.\n                     clock.set_zero_vector()\n                 }\n             },\n-            VSmallClockSetInner::Large(hash_map) => {\n-                hash_map.retain(|idx,_| {\n-                    *idx == index\n-                });\n+            VSmallClockMapInner::Large(hash_map) => {\n+                let value = hash_map.remove(&index).unwrap_or_default();\n+                self.0 = VSmallClockMapInner::Small(index, value);\n             }\n         }\n     }\n \n     /// Insert the vector clock into the associated vector\n-    ///  index\n+    /// index.\n     pub fn insert(&mut self, index: VectorIdx, clock: &VClock) {\n         match &mut self.0 {\n-            VSmallClockSetInner::Small(small_idx, small_clock) => {\n+            VSmallClockMapInner::Small(small_idx, small_clock) => {\n                 if small_clock.is_zero_vector() {\n+\n                     *small_idx = index;\n                     small_clock.clone_from(clock);\n-                }else if !clock.is_zero_vector() {\n+                } else if !clock.is_zero_vector() {\n+\n+                    // Convert to using the hash-map representation.\n                     let mut hash_map = FxHashMap::default();\n                     hash_map.insert(*small_idx, mem::take(small_clock));\n                     hash_map.insert(index, clock.clone());\n-                    self.0 = VSmallClockSetInner::Large(hash_map);\n+                    self.0 = VSmallClockMapInner::Large(hash_map);\n                 }\n             },\n-            VSmallClockSetInner::Large(hash_map) => {\n+            VSmallClockMapInner::Large(hash_map) => {\n                 if !clock.is_zero_vector() {\n                     hash_map.insert(index, clock.clone());\n                 }\n@@ -127,72 +138,80 @@ impl VSmallClockSet {\n     ///  vector index.\n     pub fn get(&self, index: VectorIdx) -> Option<&VClock> {\n         match &self.0 {\n-            VSmallClockSetInner::Small(small_idx, small_clock) => {\n+            VSmallClockMapInner::Small(small_idx, small_clock) => {\n                 if *small_idx == index && !small_clock.is_zero_vector() {\n                     Some(small_clock)\n-                }else{\n+                } else {\n                     None\n                 }\n             },\n-            VSmallClockSetInner::Large(hash_map) => {\n+            VSmallClockMapInner::Large(hash_map) => {\n                 hash_map.get(&index)\n             }\n         }\n     }\n }\n \n-impl Default for VSmallClockSet {\n+impl Default for VSmallClockMap {\n+\n     #[inline]\n     fn default() -> Self {\n-        VSmallClockSet(\n-            VSmallClockSetInner::Small(VectorIdx::new(0), VClock::default())\n+        VSmallClockMap(\n+            VSmallClockMapInner::Small(VectorIdx::new(0), VClock::default())\n         )\n     }\n+\n }\n \n-impl Debug for VSmallClockSet {\n+impl Debug for VSmallClockMap {\n+\n     fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {\n         // Print the contents of the small vector clock set as the map\n-        //  of vector index to vector clock that they represent\n+        // of vector index to vector clock that they represent.\n         let mut map = f.debug_map();\n         match &self.0 {\n-            VSmallClockSetInner::Small(small_idx, small_clock) => {\n+            VSmallClockMapInner::Small(small_idx, small_clock) => {\n                 if !small_clock.is_zero_vector() {\n                     map.entry(&small_idx, &small_clock);\n                 }\n             },\n-            VSmallClockSetInner::Large(hash_map) => {\n+            VSmallClockMapInner::Large(hash_map) => {\n                 for (idx, elem) in hash_map.iter() {\n                     map.entry(idx, elem);\n                 }\n             }\n         }\n         map.finish()\n     }\n+\n }\n-impl PartialEq for VSmallClockSet {\n+\n+\n+impl PartialEq for VSmallClockMap {\n+\n     fn eq(&self, other: &Self) -> bool {\n-        use VSmallClockSetInner::*;\n+        use VSmallClockMapInner::*;\n         match (&self.0, &other.0) {\n             (Small(i1, c1), Small(i2, c2)) => {\n                 if c1.is_zero_vector() {\n                     // Either they are both zero or they are non-equal\n                     c2.is_zero_vector()\n-                }else{\n+                } else {\n                     // At least one is non-zero, so the full comparison is correct\n                     i1 == i2 && c1 == c2\n                 }\n             }\n-            (VSmallClockSetInner::Small(idx, clock), VSmallClockSetInner::Large(hash_map)) |\n-            (VSmallClockSetInner::Large(hash_map), VSmallClockSetInner::Small(idx, clock)) => {\n+            (Small(idx, clock), Large(hash_map)) |\n+            (Large(hash_map), Small(idx, clock)) => {\n+\n                 if hash_map.len() == 0 {\n                     // Equal to the empty hash-map\n                     clock.is_zero_vector()\n-                }else if hash_map.len() == 1 {\n+                } else if hash_map.len() == 1 {\n                     // Equal to the hash-map with one element\n                     let (hash_idx, hash_clock) = hash_map.iter().next().unwrap();\n                     hash_idx == idx && hash_clock == clock\n-                }else{\n+                } else {\n                     false\n                 }\n             }\n@@ -201,32 +220,38 @@ impl PartialEq for VSmallClockSet {\n             }\n         }\n     }\n+\n }\n-impl Eq for VSmallClockSet {}\n+\n+impl Eq for VSmallClockMap {}\n \n \n \n /// The size of the vector-clock to store inline\n-///  clock vectors larger than this will be stored on the heap\n+/// clock vectors larger than this will be stored on the heap\n const SMALL_VECTOR: usize = 4;\n \n /// The type of the time-stamps recorded in the data-race detector\n-///  set to a type of unsigned integer\n+/// set to a type of unsigned integer\n pub type VTimestamp = u32;\n \n-/// A vector clock for detecting data-races\n-///  invariants:\n-///   - the last element in a VClock must not be 0\n-///     -- this means that derive(PartialEq & Eq) is correct\n-///     --  as there is no implicit zero tail that might be equal\n-///     --  also simplifies the implementation of PartialOrd\n+/// A vector clock for detecting data-races, this is conceptually\n+/// a map from a vector index (and thus a thread id) to a timestamp.\n+/// The compare operations require that the invariant that the last\n+/// element in the internal timestamp slice must not be a 0, hence\n+/// all zero vector clocks are always represented by the empty slice;\n+/// and allows for the implementation of compare operations to short\n+/// circuit the calculation and return the correct result faster,\n+/// also this means that there is only one unique valid length\n+/// for each set of vector clock values and hence the PartialEq\n+//  and Eq derivations are correct.\n #[derive(PartialEq, Eq, Default, Debug)]\n pub struct VClock(SmallVec<[VTimestamp; SMALL_VECTOR]>);\n \n impl VClock {\n \n     /// Create a new vector-clock containing all zeros except\n-    ///  for a value at the given index\n+    /// for a value at the given index\n     pub fn new_with_index(index: VectorIdx, timestamp: VTimestamp) -> VClock {\n         let len = index.index() + 1;\n         let mut vec = smallvec::smallvec![0; len];\n@@ -241,8 +266,8 @@ impl VClock {\n     }\n \n     /// Get a mutable slice to the internal vector with minimum `min_len`\n-    ///  elements, to preserve invariants this vector must modify\n-    ///  the `min_len`-1 nth element to a non-zero value\n+    /// elements, to preserve invariants this vector must modify\n+    /// the `min_len`-1 nth element to a non-zero value\n     #[inline]\n     fn get_mut_with_min_len(&mut self, min_len: usize) -> &mut [VTimestamp] {\n         if self.0.len() < min_len {\n@@ -253,7 +278,7 @@ impl VClock {\n     }\n \n     /// Increment the vector clock at a known index\n-    ///  this will panic if the vector index overflows\n+    /// this will panic if the vector index overflows\n     #[inline]\n     pub fn increment_index(&mut self, idx: VectorIdx) {\n         let idx = idx.index();\n@@ -263,8 +288,8 @@ impl VClock {\n     }\n \n     // Join the two vector-clocks together, this\n-    //  sets each vector-element to the maximum value\n-    //  of that element in either of the two source elements.\n+    // sets each vector-element to the maximum value\n+    // of that element in either of the two source elements.\n     pub fn join(&mut self, other: &Self) {\n         let rhs_slice = other.as_slice();\n         let lhs_slice = self.get_mut_with_min_len(rhs_slice.len());\n@@ -291,30 +316,43 @@ impl VClock {\n     pub fn is_zero_vector(&self) -> bool {\n         self.0.is_empty()\n     }\n+\n }\n \n impl Clone for VClock {\n+\n     fn clone(&self) -> Self {\n         VClock(self.0.clone())\n     }\n+\n+    // Optimized clone-from, can be removed\n+    // and replaced with a derive once a similar\n+    // optimization is inserted into SmallVec's\n+    // clone implementation.\n     fn clone_from(&mut self, source: &Self) {\n         let source_slice = source.as_slice();\n         self.0.clear();\n         self.0.extend_from_slice(source_slice);\n     }\n+\n }\n \n impl PartialOrd for VClock {\n+\n     fn partial_cmp(&self, other: &VClock) -> Option<Ordering> {\n \n         // Load the values as slices\n         let lhs_slice = self.as_slice();\n         let rhs_slice = other.as_slice();\n \n-        // Iterate through the combined vector slice\n-        //  keeping track of the order that is currently possible to satisfy.\n-        // If an ordering relation is detected to be impossible, then bail and\n-        //  directly return None\n+        // Iterate through the combined vector slice continuously updating\n+        // the value of `order` to the current comparison of the vector from\n+        // index 0 to the currently checked index.\n+        // An Equal ordering can be converted into Less or Greater ordering\n+        // on finding an element that is less than or greater than the other\n+        // but if one Greater and one Less element-wise comparison is found\n+        // then no ordering is possible and so directly return an ordering\n+        // of None.\n         let mut iter = lhs_slice.iter().zip(rhs_slice.iter());\n         let mut order = match iter.next() {\n             Some((lhs, rhs)) => lhs.cmp(rhs),\n@@ -332,23 +370,23 @@ impl PartialOrd for VClock {\n             }\n         }\n \n-        //Now test if either left or right have trailing elements\n+        // Now test if either left or right have trailing elements,\n         // by the invariant the trailing elements have at least 1\n         // non zero value, so no additional calculation is required\n-        // to determine the result of the PartialOrder\n+        // to determine the result of the PartialOrder.\n         let l_len = lhs_slice.len();\n         let r_len = rhs_slice.len();\n         match l_len.cmp(&r_len) {\n-            // Equal has no additional elements: return current order\n+            // Equal means no additional elements: return current order\n             Ordering::Equal => Some(order),\n             // Right has at least 1 element > than the implicit 0,\n-            //  so the only valid values are Ordering::Less or None\n+            // so the only valid values are Ordering::Less or None.\n             Ordering::Less => match order {\n                 Ordering::Less | Ordering::Equal => Some(Ordering::Less),\n                 Ordering::Greater => None\n             }\n             // Left has at least 1 element > than the implicit 0,\n-            //  so the only valid values are Ordering::Greater or None\n+            // so the only valid values are Ordering::Greater or None.\n             Ordering::Greater => match order {\n                 Ordering::Greater | Ordering::Equal => Some(Ordering::Greater),\n                 Ordering::Less => None\n@@ -362,28 +400,28 @@ impl PartialOrd for VClock {\n         let rhs_slice = other.as_slice();\n \n         // If l_len > r_len then at least one element\n-        //  in l_len is > than r_len, therefore the result\n-        //  is either Some(Greater) or None, so return false\n-        //  early.\n+        // in l_len is > than r_len, therefore the result\n+        // is either Some(Greater) or None, so return false\n+        // early.\n         let l_len = lhs_slice.len();\n         let r_len = rhs_slice.len();\n         if l_len <= r_len {\n             // If any elements on the left are greater than the right\n-            //  then the result is None or Some(Greater), both of which\n-            //  return false, the earlier test asserts that no elements in the\n-            //  extended tail violate this assumption. Otherwise l <= r, finally\n-            //  the case where the values are potentially equal needs to be considered\n-            //  and false returned as well\n+            // then the result is None or Some(Greater), both of which\n+            // return false, the earlier test asserts that no elements in the\n+            // extended tail violate this assumption. Otherwise l <= r, finally\n+            // the case where the values are potentially equal needs to be considered\n+            // and false returned as well\n             let mut equal = l_len == r_len;\n             for (&l, &r) in lhs_slice.iter().zip(rhs_slice.iter()) {\n                 if l > r {\n                     return false\n-                }else if l < r {\n+                } else if l < r {\n                     equal = false;\n                 }\n             }\n             !equal\n-        }else{\n+         } else {\n             false\n         }\n     }\n@@ -394,18 +432,18 @@ impl PartialOrd for VClock {\n         let rhs_slice = other.as_slice();\n \n         // If l_len > r_len then at least one element\n-        //  in l_len is > than r_len, therefore the result\n-        //  is either Some(Greater) or None, so return false\n-        //  early.\n+        // in l_len is > than r_len, therefore the result\n+        // is either Some(Greater) or None, so return false\n+        // early.\n         let l_len = lhs_slice.len();\n         let r_len = rhs_slice.len();\n         if l_len <= r_len {\n             // If any elements on the left are greater than the right\n-            //  then the result is None or Some(Greater), both of which\n-            //  return false, the earlier test asserts that no elements in the\n-            //  extended tail violate this assumption. Otherwise l <= r\n+            // then the result is None or Some(Greater), both of which\n+            // return false, the earlier test asserts that no elements in the\n+            // extended tail violate this assumption. Otherwise l <= r\n             !lhs_slice.iter().zip(rhs_slice.iter()).any(|(&l, &r)| l > r)\n-        }else{\n+        } else {\n             false\n         }\n     }\n@@ -416,28 +454,28 @@ impl PartialOrd for VClock {\n         let rhs_slice = other.as_slice();\n \n         // If r_len > l_len then at least one element\n-        //  in r_len is > than l_len, therefore the result\n-        //  is either Some(Less) or None, so return false\n-        //  early.\n+        // in r_len is > than l_len, therefore the result\n+        // is either Some(Less) or None, so return false\n+        // early.\n         let l_len = lhs_slice.len();\n         let r_len = rhs_slice.len();\n         if l_len >= r_len {\n             // If any elements on the left are less than the right\n-            //  then the result is None or Some(Less), both of which\n-            //  return false, the earlier test asserts that no elements in the\n-            //  extended tail violate this assumption. Otherwise l >=, finally\n-            //  the case where the values are potentially equal needs to be considered\n-            //  and false returned as well\n+            // then the result is None or Some(Less), both of which\n+            // return false, the earlier test asserts that no elements in the\n+            // extended tail violate this assumption. Otherwise l >=, finally\n+            // the case where the values are potentially equal needs to be considered\n+            // and false returned as well\n             let mut equal = l_len == r_len;\n             for (&l, &r) in lhs_slice.iter().zip(rhs_slice.iter()) {\n                 if l < r {\n                     return false\n-                }else if l > r {\n+                } else if l > r {\n                     equal = false;\n                 }\n             }\n             !equal\n-        }else{\n+        } else {\n             false\n         }\n     }\n@@ -448,30 +486,33 @@ impl PartialOrd for VClock {\n         let rhs_slice = other.as_slice();\n \n         // If r_len > l_len then at least one element\n-        //  in r_len is > than l_len, therefore the result\n-        //  is either Some(Less) or None, so return false\n-        //  early.\n+        // in r_len is > than l_len, therefore the result\n+        // is either Some(Less) or None, so return false\n+        // early.\n         let l_len = lhs_slice.len();\n         let r_len = rhs_slice.len();\n         if l_len >= r_len {\n             // If any elements on the left are less than the right\n-            //  then the result is None or Some(Less), both of which\n-            //  return false, the earlier test asserts that no elements in the\n-            //  extended tail violate this assumption. Otherwise l >= r\n+            // then the result is None or Some(Less), both of which\n+            // return false, the earlier test asserts that no elements in the\n+            // extended tail violate this assumption. Otherwise l >= r\n             !lhs_slice.iter().zip(rhs_slice.iter()).any(|(&l, &r)| l < r)\n-        }else{\n+        } else {\n             false\n         }\n     }\n+\n }\n \n impl Index<VectorIdx> for VClock {\n+\n     type Output = VTimestamp;\n \n     #[inline]\n     fn index(&self, index: VectorIdx) -> &VTimestamp {\n        self.as_slice().get(index.to_u32() as usize).unwrap_or(&0)\n     }\n+\n }\n \n \n@@ -480,7 +521,8 @@ impl Index<VectorIdx> for VClock {\n ///  test suite\n #[cfg(test)]\n mod tests {\n-    use super::{VClock, VTimestamp, VectorIdx, VSmallClockSet};\n+\n+    use super::{VClock, VTimestamp, VectorIdx, VSmallClockMap};\n     use std::cmp::Ordering;\n \n     #[test]\n@@ -536,7 +578,7 @@ mod tests {\n         let alt_compare = r.partial_cmp(&l);\n         assert_eq!(alt_compare, o.map(Ordering::reverse), \"Invalid alt comparison\\n l: {:?}\\n r: {:?}\",l,r);\n \n-        //Test operatorsm with faster implementations\n+        //Test operators with faster implementations\n         assert_eq!(\n             matches!(compare,Some(Ordering::Less)), l < r,\n             \"Invalid (<):\\n l: {:?}\\n r: {:?}\",l,r\n@@ -573,30 +615,31 @@ mod tests {\n \n     #[test]\n     pub fn test_vclock_set() {\n-        let mut set = VSmallClockSet::default();\n+        let mut map = VSmallClockMap::default();\n         let v1 = from_slice(&[3,0,1]);\n         let v2 = from_slice(&[4,2,3]);\n         let v3 = from_slice(&[4,8,3]);\n-        set.insert(VectorIdx(0), &v1);\n-        assert_eq!(set.get(VectorIdx(0)), Some(&v1));\n-        set.insert(VectorIdx(5), &v2);\n-        assert_eq!(set.get(VectorIdx(0)), Some(&v1));\n-        assert_eq!(set.get(VectorIdx(5)), Some(&v2));\n-        set.insert(VectorIdx(53), &v3);\n-        assert_eq!(set.get(VectorIdx(0)), Some(&v1));\n-        assert_eq!(set.get(VectorIdx(5)), Some(&v2));\n-        assert_eq!(set.get(VectorIdx(53)), Some(&v3));\n-        set.retain_index(VectorIdx(53));\n-        assert_eq!(set.get(VectorIdx(0)), None);\n-        assert_eq!(set.get(VectorIdx(5)), None);\n-        assert_eq!(set.get(VectorIdx(53)), Some(&v3));\n-        set.clear();\n-        assert_eq!(set.get(VectorIdx(0)), None);\n-        assert_eq!(set.get(VectorIdx(5)), None);\n-        assert_eq!(set.get(VectorIdx(53)), None);\n-        set.insert(VectorIdx(53), &v3);\n-        assert_eq!(set.get(VectorIdx(0)), None);\n-        assert_eq!(set.get(VectorIdx(5)), None);\n-        assert_eq!(set.get(VectorIdx(53)), Some(&v3));\n-    }\n+        map.insert(VectorIdx(0), &v1);\n+        assert_eq!(map.get(VectorIdx(0)), Some(&v1));\n+        map.insert(VectorIdx(5), &v2);\n+        assert_eq!(map.get(VectorIdx(0)), Some(&v1));\n+        assert_eq!(map.get(VectorIdx(5)), Some(&v2));\n+        map.insert(VectorIdx(53), &v3);\n+        assert_eq!(map.get(VectorIdx(0)), Some(&v1));\n+        assert_eq!(map.get(VectorIdx(5)), Some(&v2));\n+        assert_eq!(map.get(VectorIdx(53)), Some(&v3));\n+        map.retain_index(VectorIdx(53));\n+        assert_eq!(map.get(VectorIdx(0)), None);\n+        assert_eq!(map.get(VectorIdx(5)), None);\n+        assert_eq!(map.get(VectorIdx(53)), Some(&v3));\n+        map.clear();\n+        assert_eq!(map.get(VectorIdx(0)), None);\n+        assert_eq!(map.get(VectorIdx(5)), None);\n+        assert_eq!(map.get(VectorIdx(53)), None);\n+        map.insert(VectorIdx(53), &v3);\n+        assert_eq!(map.get(VectorIdx(0)), None);\n+        assert_eq!(map.get(VectorIdx(5)), None);\n+        assert_eq!(map.get(VectorIdx(53)), Some(&v3));\n+    }\n+    \n }"}, {"sha": "7ba8087a9b4bc5938d4238c495318a9ba05d6f28", "filename": "tests/run-pass/concurrency/data_race.stderr", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/69fb6413ddc5b7fd5d9cb0a68ebf58ee513bf9d5/tests%2Frun-pass%2Fconcurrency%2Fdata_race.stderr", "raw_url": "https://github.com/rust-lang/rust/raw/69fb6413ddc5b7fd5d9cb0a68ebf58ee513bf9d5/tests%2Frun-pass%2Fconcurrency%2Fdata_race.stderr", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/tests%2Frun-pass%2Fconcurrency%2Fdata_race.stderr?ref=69fb6413ddc5b7fd5d9cb0a68ebf58ee513bf9d5", "patch": "@@ -1,2 +1,2 @@\n-warning: thread support is experimental.\n+warning: thread support is experimental, no weak memory effects are currently emulated.\n "}, {"sha": "7ba8087a9b4bc5938d4238c495318a9ba05d6f28", "filename": "tests/run-pass/concurrency/linux-futex.stderr", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/69fb6413ddc5b7fd5d9cb0a68ebf58ee513bf9d5/tests%2Frun-pass%2Fconcurrency%2Flinux-futex.stderr", "raw_url": "https://github.com/rust-lang/rust/raw/69fb6413ddc5b7fd5d9cb0a68ebf58ee513bf9d5/tests%2Frun-pass%2Fconcurrency%2Flinux-futex.stderr", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/tests%2Frun-pass%2Fconcurrency%2Flinux-futex.stderr?ref=69fb6413ddc5b7fd5d9cb0a68ebf58ee513bf9d5", "patch": "@@ -1,2 +1,2 @@\n-warning: thread support is experimental.\n+warning: thread support is experimental, no weak memory effects are currently emulated.\n "}, {"sha": "24444fdc17c1767cdcbd6f08fc0d586f95fcc67e", "filename": "tests/run-pass/concurrency/simple.stderr", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/69fb6413ddc5b7fd5d9cb0a68ebf58ee513bf9d5/tests%2Frun-pass%2Fconcurrency%2Fsimple.stderr", "raw_url": "https://github.com/rust-lang/rust/raw/69fb6413ddc5b7fd5d9cb0a68ebf58ee513bf9d5/tests%2Frun-pass%2Fconcurrency%2Fsimple.stderr", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/tests%2Frun-pass%2Fconcurrency%2Fsimple.stderr?ref=69fb6413ddc5b7fd5d9cb0a68ebf58ee513bf9d5", "patch": "@@ -1,4 +1,4 @@\n-warning: thread support is experimental.\n+warning: thread support is experimental, no weak memory effects are currently emulated.\n \n thread '<unnamed>' panicked at 'Hello!', $DIR/simple.rs:54:9\n note: run with `RUST_BACKTRACE=1` environment variable to display a backtrace"}, {"sha": "7ba8087a9b4bc5938d4238c495318a9ba05d6f28", "filename": "tests/run-pass/concurrency/sync.stderr", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/69fb6413ddc5b7fd5d9cb0a68ebf58ee513bf9d5/tests%2Frun-pass%2Fconcurrency%2Fsync.stderr", "raw_url": "https://github.com/rust-lang/rust/raw/69fb6413ddc5b7fd5d9cb0a68ebf58ee513bf9d5/tests%2Frun-pass%2Fconcurrency%2Fsync.stderr", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/tests%2Frun-pass%2Fconcurrency%2Fsync.stderr?ref=69fb6413ddc5b7fd5d9cb0a68ebf58ee513bf9d5", "patch": "@@ -1,2 +1,2 @@\n-warning: thread support is experimental.\n+warning: thread support is experimental, no weak memory effects are currently emulated.\n "}, {"sha": "7ba8087a9b4bc5938d4238c495318a9ba05d6f28", "filename": "tests/run-pass/concurrency/thread_locals.stderr", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/69fb6413ddc5b7fd5d9cb0a68ebf58ee513bf9d5/tests%2Frun-pass%2Fconcurrency%2Fthread_locals.stderr", "raw_url": "https://github.com/rust-lang/rust/raw/69fb6413ddc5b7fd5d9cb0a68ebf58ee513bf9d5/tests%2Frun-pass%2Fconcurrency%2Fthread_locals.stderr", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/tests%2Frun-pass%2Fconcurrency%2Fthread_locals.stderr?ref=69fb6413ddc5b7fd5d9cb0a68ebf58ee513bf9d5", "patch": "@@ -1,2 +1,2 @@\n-warning: thread support is experimental.\n+warning: thread support is experimental, no weak memory effects are currently emulated.\n "}, {"sha": "7ba8087a9b4bc5938d4238c495318a9ba05d6f28", "filename": "tests/run-pass/concurrency/tls_lib_drop.stderr", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/69fb6413ddc5b7fd5d9cb0a68ebf58ee513bf9d5/tests%2Frun-pass%2Fconcurrency%2Ftls_lib_drop.stderr", "raw_url": "https://github.com/rust-lang/rust/raw/69fb6413ddc5b7fd5d9cb0a68ebf58ee513bf9d5/tests%2Frun-pass%2Fconcurrency%2Ftls_lib_drop.stderr", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/tests%2Frun-pass%2Fconcurrency%2Ftls_lib_drop.stderr?ref=69fb6413ddc5b7fd5d9cb0a68ebf58ee513bf9d5", "patch": "@@ -1,2 +1,2 @@\n-warning: thread support is experimental.\n+warning: thread support is experimental, no weak memory effects are currently emulated.\n "}, {"sha": "7ba8087a9b4bc5938d4238c495318a9ba05d6f28", "filename": "tests/run-pass/libc.stderr", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/69fb6413ddc5b7fd5d9cb0a68ebf58ee513bf9d5/tests%2Frun-pass%2Flibc.stderr", "raw_url": "https://github.com/rust-lang/rust/raw/69fb6413ddc5b7fd5d9cb0a68ebf58ee513bf9d5/tests%2Frun-pass%2Flibc.stderr", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/tests%2Frun-pass%2Flibc.stderr?ref=69fb6413ddc5b7fd5d9cb0a68ebf58ee513bf9d5", "patch": "@@ -1,2 +1,2 @@\n-warning: thread support is experimental.\n+warning: thread support is experimental, no weak memory effects are currently emulated.\n "}, {"sha": "885385a8dd9373aa87c1f19be38eb4dfc0b8177c", "filename": "tests/run-pass/panic/concurrent-panic.stderr", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/69fb6413ddc5b7fd5d9cb0a68ebf58ee513bf9d5/tests%2Frun-pass%2Fpanic%2Fconcurrent-panic.stderr", "raw_url": "https://github.com/rust-lang/rust/raw/69fb6413ddc5b7fd5d9cb0a68ebf58ee513bf9d5/tests%2Frun-pass%2Fpanic%2Fconcurrent-panic.stderr", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/tests%2Frun-pass%2Fpanic%2Fconcurrent-panic.stderr?ref=69fb6413ddc5b7fd5d9cb0a68ebf58ee513bf9d5", "patch": "@@ -1,4 +1,4 @@\n-warning: thread support is experimental.\n+warning: thread support is experimental, no weak memory effects are currently emulated.\n \n Thread 1 starting, will block on mutex\n Thread 1 reported it has started"}]}