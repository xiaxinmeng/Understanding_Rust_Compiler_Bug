{"sha": "68b390ae2a099a598bfe44d4ec36ae15159c283f", "node_id": "C_kwDOAAsO6NoAKDY4YjM5MGFlMmEwOTlhNTk4YmZlNDRkNGVjMzZhZTE1MTU5YzI4M2Y", "commit": {"author": {"name": "Michael Goulet", "email": "michael@errs.io", "date": "2023-01-21T02:33:21Z"}, "committer": {"name": "GitHub", "email": "noreply@github.com", "date": "2023-01-21T02:33:21Z"}, "message": "Rollup merge of #104672 - Voultapher:unify-sort-modules, r=thomcc\n\nUnify stable and unstable sort implementations in same core module\n\nThis moves the stable sort implementation to the core::slice::sort module. By virtue of being in core it can't access `Vec`. The two `Vec` used by merge sort, `buf` and `runs`, are modelled as custom types that implement the very limited required `Vec` interface with the help of provided allocation and free functions. This is done to allow future re-use of functions and logic between stable and unstable sort. Such as `insert_head`.\n\nThis is in preparation of #100856 and #104116. It only moves code, it *doesn't* change any of the sort related logic. This unlocks the ability to share `insert_head`, `insert_tail`, `swap_if_less` `merge` and more.\n\nTagging ````@Mark-Simulacrum```` I hope this allows progress on #100856, by moving `merge_sort` here I hope future changes will be easier to review.", "tree": {"sha": "cbd2df16b4e222e6164576a0e9767ece46d943c2", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/cbd2df16b4e222e6164576a0e9767ece46d943c2"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/68b390ae2a099a598bfe44d4ec36ae15159c283f", "comment_count": 0, "verification": {"verified": true, "reason": "valid", "signature": "-----BEGIN PGP SIGNATURE-----\n\nwsBcBAABCAAQBQJjy07xCRBK7hj4Ov3rIwAARpoIAHG/HPp7pRt0QaQ9pPjQIci5\nJqdmWwmQYBok95ssnCMXP1m0HrUgQH0+KaYoV8OPQOn1bYvRCya3M2Imezsyyo0u\nhQfwpjBBKXPBonc2fhBgoIPUD68AVmIXCmh9YDcRyQgBrYvo9VedD78Bdg3S3yeR\nHwsLe4N04ClWGRMUTyXFqDyTRree8KSi9r7STr1UoEhHSSQd4EOp4Dty5mE5QJ89\nzaDR7c1Xcx0PAVGa6xWQgfGpos4/cexE+tMDyRvFPcRruXPyFm1A/THA7ku+L3CH\nE+tHHTWCjf3VbwvBr/iU/3B00qyABM5DVYzmuHcxGE08i2h7SjrRrys9WdRvZ+A=\n=GVxR\n-----END PGP SIGNATURE-----\n", "payload": "tree cbd2df16b4e222e6164576a0e9767ece46d943c2\nparent bf75f8177b42da40faef510838eb1114c30c49df\nparent 4b5844fbe95a35370c19d74f9b70286897c3e153\nauthor Michael Goulet <michael@errs.io> 1674268401 -0500\ncommitter GitHub <noreply@github.com> 1674268401 -0500\n\nRollup merge of #104672 - Voultapher:unify-sort-modules, r=thomcc\n\nUnify stable and unstable sort implementations in same core module\n\nThis moves the stable sort implementation to the core::slice::sort module. By virtue of being in core it can't access `Vec`. The two `Vec` used by merge sort, `buf` and `runs`, are modelled as custom types that implement the very limited required `Vec` interface with the help of provided allocation and free functions. This is done to allow future re-use of functions and logic between stable and unstable sort. Such as `insert_head`.\n\nThis is in preparation of #100856 and #104116. It only moves code, it *doesn't* change any of the sort related logic. This unlocks the ability to share `insert_head`, `insert_tail`, `swap_if_less` `merge` and more.\n\nTagging ````@Mark-Simulacrum```` I hope this allows progress on #100856, by moving `merge_sort` here I hope future changes will be easier to review.\n"}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/68b390ae2a099a598bfe44d4ec36ae15159c283f", "html_url": "https://github.com/rust-lang/rust/commit/68b390ae2a099a598bfe44d4ec36ae15159c283f", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/68b390ae2a099a598bfe44d4ec36ae15159c283f/comments", "author": {"login": "compiler-errors", "id": 3674314, "node_id": "MDQ6VXNlcjM2NzQzMTQ=", "avatar_url": "https://avatars.githubusercontent.com/u/3674314?v=4", "gravatar_id": "", "url": "https://api.github.com/users/compiler-errors", "html_url": "https://github.com/compiler-errors", "followers_url": "https://api.github.com/users/compiler-errors/followers", "following_url": "https://api.github.com/users/compiler-errors/following{/other_user}", "gists_url": "https://api.github.com/users/compiler-errors/gists{/gist_id}", "starred_url": "https://api.github.com/users/compiler-errors/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/compiler-errors/subscriptions", "organizations_url": "https://api.github.com/users/compiler-errors/orgs", "repos_url": "https://api.github.com/users/compiler-errors/repos", "events_url": "https://api.github.com/users/compiler-errors/events{/privacy}", "received_events_url": "https://api.github.com/users/compiler-errors/received_events", "type": "User", "site_admin": false}, "committer": {"login": "web-flow", "id": 19864447, "node_id": "MDQ6VXNlcjE5ODY0NDQ3", "avatar_url": "https://avatars.githubusercontent.com/u/19864447?v=4", "gravatar_id": "", "url": "https://api.github.com/users/web-flow", "html_url": "https://github.com/web-flow", "followers_url": "https://api.github.com/users/web-flow/followers", "following_url": "https://api.github.com/users/web-flow/following{/other_user}", "gists_url": "https://api.github.com/users/web-flow/gists{/gist_id}", "starred_url": "https://api.github.com/users/web-flow/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/web-flow/subscriptions", "organizations_url": "https://api.github.com/users/web-flow/orgs", "repos_url": "https://api.github.com/users/web-flow/repos", "events_url": "https://api.github.com/users/web-flow/events{/privacy}", "received_events_url": "https://api.github.com/users/web-flow/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "bf75f8177b42da40faef510838eb1114c30c49df", "url": "https://api.github.com/repos/rust-lang/rust/commits/bf75f8177b42da40faef510838eb1114c30c49df", "html_url": "https://github.com/rust-lang/rust/commit/bf75f8177b42da40faef510838eb1114c30c49df"}, {"sha": "4b5844fbe95a35370c19d74f9b70286897c3e153", "url": "https://api.github.com/repos/rust-lang/rust/commits/4b5844fbe95a35370c19d74f9b70286897c3e153", "html_url": "https://github.com/rust-lang/rust/commit/4b5844fbe95a35370c19d74f9b70286897c3e153"}], "stats": {"total": 870, "additions": 560, "deletions": 310}, "files": [{"sha": "fecacc2bb639508836b4090fa22280a368051a67", "filename": "library/alloc/src/slice.rs", "status": "modified", "additions": 39, "deletions": 309, "changes": 348, "blob_url": "https://github.com/rust-lang/rust/blob/68b390ae2a099a598bfe44d4ec36ae15159c283f/library%2Falloc%2Fsrc%2Fslice.rs", "raw_url": "https://github.com/rust-lang/rust/raw/68b390ae2a099a598bfe44d4ec36ae15159c283f/library%2Falloc%2Fsrc%2Fslice.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/library%2Falloc%2Fsrc%2Fslice.rs?ref=68b390ae2a099a598bfe44d4ec36ae15159c283f", "patch": "@@ -19,10 +19,12 @@ use core::cmp::Ordering::{self, Less};\n use core::mem::{self, SizedTypeProperties};\n #[cfg(not(no_global_oom_handling))]\n use core::ptr;\n+#[cfg(not(no_global_oom_handling))]\n+use core::slice::sort;\n \n use crate::alloc::Allocator;\n #[cfg(not(no_global_oom_handling))]\n-use crate::alloc::Global;\n+use crate::alloc::{self, Global};\n #[cfg(not(no_global_oom_handling))]\n use crate::borrow::ToOwned;\n use crate::boxed::Box;\n@@ -206,7 +208,7 @@ impl<T> [T] {\n     where\n         T: Ord,\n     {\n-        merge_sort(self, T::lt);\n+        stable_sort(self, T::lt);\n     }\n \n     /// Sorts the slice with a comparator function.\n@@ -262,7 +264,7 @@ impl<T> [T] {\n     where\n         F: FnMut(&T, &T) -> Ordering,\n     {\n-        merge_sort(self, |a, b| compare(a, b) == Less);\n+        stable_sort(self, |a, b| compare(a, b) == Less);\n     }\n \n     /// Sorts the slice with a key extraction function.\n@@ -305,7 +307,7 @@ impl<T> [T] {\n         F: FnMut(&T) -> K,\n         K: Ord,\n     {\n-        merge_sort(self, |a, b| f(a).lt(&f(b)));\n+        stable_sort(self, |a, b| f(a).lt(&f(b)));\n     }\n \n     /// Sorts the slice with a key extraction function.\n@@ -812,324 +814,52 @@ impl<T: Clone> ToOwned for [T] {\n // Sorting\n ////////////////////////////////////////////////////////////////////////////////\n \n-/// Inserts `v[0]` into pre-sorted sequence `v[1..]` so that whole `v[..]` becomes sorted.\n-///\n-/// This is the integral subroutine of insertion sort.\n-#[cfg(not(no_global_oom_handling))]\n-fn insert_head<T, F>(v: &mut [T], is_less: &mut F)\n-where\n-    F: FnMut(&T, &T) -> bool,\n-{\n-    if v.len() >= 2 && is_less(&v[1], &v[0]) {\n-        unsafe {\n-            // There are three ways to implement insertion here:\n-            //\n-            // 1. Swap adjacent elements until the first one gets to its final destination.\n-            //    However, this way we copy data around more than is necessary. If elements are big\n-            //    structures (costly to copy), this method will be slow.\n-            //\n-            // 2. Iterate until the right place for the first element is found. Then shift the\n-            //    elements succeeding it to make room for it and finally place it into the\n-            //    remaining hole. This is a good method.\n-            //\n-            // 3. Copy the first element into a temporary variable. Iterate until the right place\n-            //    for it is found. As we go along, copy every traversed element into the slot\n-            //    preceding it. Finally, copy data from the temporary variable into the remaining\n-            //    hole. This method is very good. Benchmarks demonstrated slightly better\n-            //    performance than with the 2nd method.\n-            //\n-            // All methods were benchmarked, and the 3rd showed best results. So we chose that one.\n-            let tmp = mem::ManuallyDrop::new(ptr::read(&v[0]));\n-\n-            // Intermediate state of the insertion process is always tracked by `hole`, which\n-            // serves two purposes:\n-            // 1. Protects integrity of `v` from panics in `is_less`.\n-            // 2. Fills the remaining hole in `v` in the end.\n-            //\n-            // Panic safety:\n-            //\n-            // If `is_less` panics at any point during the process, `hole` will get dropped and\n-            // fill the hole in `v` with `tmp`, thus ensuring that `v` still holds every object it\n-            // initially held exactly once.\n-            let mut hole = InsertionHole { src: &*tmp, dest: &mut v[1] };\n-            ptr::copy_nonoverlapping(&v[1], &mut v[0], 1);\n-\n-            for i in 2..v.len() {\n-                if !is_less(&v[i], &*tmp) {\n-                    break;\n-                }\n-                ptr::copy_nonoverlapping(&v[i], &mut v[i - 1], 1);\n-                hole.dest = &mut v[i];\n-            }\n-            // `hole` gets dropped and thus copies `tmp` into the remaining hole in `v`.\n-        }\n-    }\n-\n-    // When dropped, copies from `src` into `dest`.\n-    struct InsertionHole<T> {\n-        src: *const T,\n-        dest: *mut T,\n-    }\n-\n-    impl<T> Drop for InsertionHole<T> {\n-        fn drop(&mut self) {\n-            unsafe {\n-                ptr::copy_nonoverlapping(self.src, self.dest, 1);\n-            }\n-        }\n-    }\n-}\n-\n-/// Merges non-decreasing runs `v[..mid]` and `v[mid..]` using `buf` as temporary storage, and\n-/// stores the result into `v[..]`.\n-///\n-/// # Safety\n-///\n-/// The two slices must be non-empty and `mid` must be in bounds. Buffer `buf` must be long enough\n-/// to hold a copy of the shorter slice. Also, `T` must not be a zero-sized type.\n-#[cfg(not(no_global_oom_handling))]\n-unsafe fn merge<T, F>(v: &mut [T], mid: usize, buf: *mut T, is_less: &mut F)\n-where\n-    F: FnMut(&T, &T) -> bool,\n-{\n-    let len = v.len();\n-    let v = v.as_mut_ptr();\n-    let (v_mid, v_end) = unsafe { (v.add(mid), v.add(len)) };\n-\n-    // The merge process first copies the shorter run into `buf`. Then it traces the newly copied\n-    // run and the longer run forwards (or backwards), comparing their next unconsumed elements and\n-    // copying the lesser (or greater) one into `v`.\n-    //\n-    // As soon as the shorter run is fully consumed, the process is done. If the longer run gets\n-    // consumed first, then we must copy whatever is left of the shorter run into the remaining\n-    // hole in `v`.\n-    //\n-    // Intermediate state of the process is always tracked by `hole`, which serves two purposes:\n-    // 1. Protects integrity of `v` from panics in `is_less`.\n-    // 2. Fills the remaining hole in `v` if the longer run gets consumed first.\n-    //\n-    // Panic safety:\n-    //\n-    // If `is_less` panics at any point during the process, `hole` will get dropped and fill the\n-    // hole in `v` with the unconsumed range in `buf`, thus ensuring that `v` still holds every\n-    // object it initially held exactly once.\n-    let mut hole;\n-\n-    if mid <= len - mid {\n-        // The left run is shorter.\n-        unsafe {\n-            ptr::copy_nonoverlapping(v, buf, mid);\n-            hole = MergeHole { start: buf, end: buf.add(mid), dest: v };\n-        }\n-\n-        // Initially, these pointers point to the beginnings of their arrays.\n-        let left = &mut hole.start;\n-        let mut right = v_mid;\n-        let out = &mut hole.dest;\n-\n-        while *left < hole.end && right < v_end {\n-            // Consume the lesser side.\n-            // If equal, prefer the left run to maintain stability.\n-            unsafe {\n-                let to_copy = if is_less(&*right, &**left) {\n-                    get_and_increment(&mut right)\n-                } else {\n-                    get_and_increment(left)\n-                };\n-                ptr::copy_nonoverlapping(to_copy, get_and_increment(out), 1);\n-            }\n-        }\n-    } else {\n-        // The right run is shorter.\n-        unsafe {\n-            ptr::copy_nonoverlapping(v_mid, buf, len - mid);\n-            hole = MergeHole { start: buf, end: buf.add(len - mid), dest: v_mid };\n-        }\n-\n-        // Initially, these pointers point past the ends of their arrays.\n-        let left = &mut hole.dest;\n-        let right = &mut hole.end;\n-        let mut out = v_end;\n-\n-        while v < *left && buf < *right {\n-            // Consume the greater side.\n-            // If equal, prefer the right run to maintain stability.\n-            unsafe {\n-                let to_copy = if is_less(&*right.sub(1), &*left.sub(1)) {\n-                    decrement_and_get(left)\n-                } else {\n-                    decrement_and_get(right)\n-                };\n-                ptr::copy_nonoverlapping(to_copy, decrement_and_get(&mut out), 1);\n-            }\n-        }\n-    }\n-    // Finally, `hole` gets dropped. If the shorter run was not fully consumed, whatever remains of\n-    // it will now be copied into the hole in `v`.\n-\n-    unsafe fn get_and_increment<T>(ptr: &mut *mut T) -> *mut T {\n-        let old = *ptr;\n-        *ptr = unsafe { ptr.add(1) };\n-        old\n-    }\n-\n-    unsafe fn decrement_and_get<T>(ptr: &mut *mut T) -> *mut T {\n-        *ptr = unsafe { ptr.sub(1) };\n-        *ptr\n-    }\n-\n-    // When dropped, copies the range `start..end` into `dest..`.\n-    struct MergeHole<T> {\n-        start: *mut T,\n-        end: *mut T,\n-        dest: *mut T,\n-    }\n-\n-    impl<T> Drop for MergeHole<T> {\n-        fn drop(&mut self) {\n-            // `T` is not a zero-sized type, and these are pointers into a slice's elements.\n-            unsafe {\n-                let len = self.end.sub_ptr(self.start);\n-                ptr::copy_nonoverlapping(self.start, self.dest, len);\n-            }\n-        }\n-    }\n-}\n-\n-/// This merge sort borrows some (but not all) ideas from TimSort, which is described in detail\n-/// [here](https://github.com/python/cpython/blob/main/Objects/listsort.txt).\n-///\n-/// The algorithm identifies strictly descending and non-descending subsequences, which are called\n-/// natural runs. There is a stack of pending runs yet to be merged. Each newly found run is pushed\n-/// onto the stack, and then some pairs of adjacent runs are merged until these two invariants are\n-/// satisfied:\n-///\n-/// 1. for every `i` in `1..runs.len()`: `runs[i - 1].len > runs[i].len`\n-/// 2. for every `i` in `2..runs.len()`: `runs[i - 2].len > runs[i - 1].len + runs[i].len`\n-///\n-/// The invariants ensure that the total running time is *O*(*n* \\* log(*n*)) worst-case.\n+#[inline]\n #[cfg(not(no_global_oom_handling))]\n-fn merge_sort<T, F>(v: &mut [T], mut is_less: F)\n+fn stable_sort<T, F>(v: &mut [T], mut is_less: F)\n where\n     F: FnMut(&T, &T) -> bool,\n {\n-    // Slices of up to this length get sorted using insertion sort.\n-    const MAX_INSERTION: usize = 20;\n-    // Very short runs are extended using insertion sort to span at least this many elements.\n-    const MIN_RUN: usize = 10;\n-\n-    // Sorting has no meaningful behavior on zero-sized types.\n     if T::IS_ZST {\n+        // Sorting has no meaningful behavior on zero-sized types. Do nothing.\n         return;\n     }\n \n-    let len = v.len();\n-\n-    // Short arrays get sorted in-place via insertion sort to avoid allocations.\n-    if len <= MAX_INSERTION {\n-        if len >= 2 {\n-            for i in (0..len - 1).rev() {\n-                insert_head(&mut v[i..], &mut is_less);\n-            }\n-        }\n-        return;\n-    }\n-\n-    // Allocate a buffer to use as scratch memory. We keep the length 0 so we can keep in it\n-    // shallow copies of the contents of `v` without risking the dtors running on copies if\n-    // `is_less` panics. When merging two sorted runs, this buffer holds a copy of the shorter run,\n-    // which will always have length at most `len / 2`.\n-    let mut buf = Vec::with_capacity(len / 2);\n+    let elem_alloc_fn = |len: usize| -> *mut T {\n+        // SAFETY: Creating the layout is safe as long as merge_sort never calls this with len >\n+        // v.len(). Alloc in general will only be used as 'shadow-region' to store temporary swap\n+        // elements.\n+        unsafe { alloc::alloc(alloc::Layout::array::<T>(len).unwrap_unchecked()) as *mut T }\n+    };\n \n-    // In order to identify natural runs in `v`, we traverse it backwards. That might seem like a\n-    // strange decision, but consider the fact that merges more often go in the opposite direction\n-    // (forwards). According to benchmarks, merging forwards is slightly faster than merging\n-    // backwards. To conclude, identifying runs by traversing backwards improves performance.\n-    let mut runs = vec![];\n-    let mut end = len;\n-    while end > 0 {\n-        // Find the next natural run, and reverse it if it's strictly descending.\n-        let mut start = end - 1;\n-        if start > 0 {\n-            start -= 1;\n-            unsafe {\n-                if is_less(v.get_unchecked(start + 1), v.get_unchecked(start)) {\n-                    while start > 0 && is_less(v.get_unchecked(start), v.get_unchecked(start - 1)) {\n-                        start -= 1;\n-                    }\n-                    v[start..end].reverse();\n-                } else {\n-                    while start > 0 && !is_less(v.get_unchecked(start), v.get_unchecked(start - 1))\n-                    {\n-                        start -= 1;\n-                    }\n-                }\n-            }\n-        }\n-\n-        // Insert some more elements into the run if it's too short. Insertion sort is faster than\n-        // merge sort on short sequences, so this significantly improves performance.\n-        while start > 0 && end - start < MIN_RUN {\n-            start -= 1;\n-            insert_head(&mut v[start..end], &mut is_less);\n+    let elem_dealloc_fn = |buf_ptr: *mut T, len: usize| {\n+        // SAFETY: Creating the layout is safe as long as merge_sort never calls this with len >\n+        // v.len(). The caller must ensure that buf_ptr was created by elem_alloc_fn with the same\n+        // len.\n+        unsafe {\n+            alloc::dealloc(buf_ptr as *mut u8, alloc::Layout::array::<T>(len).unwrap_unchecked());\n         }\n+    };\n \n-        // Push this run onto the stack.\n-        runs.push(Run { start, len: end - start });\n-        end = start;\n-\n-        // Merge some pairs of adjacent runs to satisfy the invariants.\n-        while let Some(r) = collapse(&runs) {\n-            let left = runs[r + 1];\n-            let right = runs[r];\n-            unsafe {\n-                merge(\n-                    &mut v[left.start..right.start + right.len],\n-                    left.len,\n-                    buf.as_mut_ptr(),\n-                    &mut is_less,\n-                );\n-            }\n-            runs[r] = Run { start: left.start, len: left.len + right.len };\n-            runs.remove(r + 1);\n+    let run_alloc_fn = |len: usize| -> *mut sort::TimSortRun {\n+        // SAFETY: Creating the layout is safe as long as merge_sort never calls this with an\n+        // obscene length or 0.\n+        unsafe {\n+            alloc::alloc(alloc::Layout::array::<sort::TimSortRun>(len).unwrap_unchecked())\n+                as *mut sort::TimSortRun\n         }\n-    }\n-\n-    // Finally, exactly one run must remain in the stack.\n-    debug_assert!(runs.len() == 1 && runs[0].start == 0 && runs[0].len == len);\n+    };\n \n-    // Examines the stack of runs and identifies the next pair of runs to merge. More specifically,\n-    // if `Some(r)` is returned, that means `runs[r]` and `runs[r + 1]` must be merged next. If the\n-    // algorithm should continue building a new run instead, `None` is returned.\n-    //\n-    // TimSort is infamous for its buggy implementations, as described here:\n-    // http://envisage-project.eu/timsort-specification-and-verification/\n-    //\n-    // The gist of the story is: we must enforce the invariants on the top four runs on the stack.\n-    // Enforcing them on just top three is not sufficient to ensure that the invariants will still\n-    // hold for *all* runs in the stack.\n-    //\n-    // This function correctly checks invariants for the top four runs. Additionally, if the top\n-    // run starts at index 0, it will always demand a merge operation until the stack is fully\n-    // collapsed, in order to complete the sort.\n-    #[inline]\n-    fn collapse(runs: &[Run]) -> Option<usize> {\n-        let n = runs.len();\n-        if n >= 2\n-            && (runs[n - 1].start == 0\n-                || runs[n - 2].len <= runs[n - 1].len\n-                || (n >= 3 && runs[n - 3].len <= runs[n - 2].len + runs[n - 1].len)\n-                || (n >= 4 && runs[n - 4].len <= runs[n - 3].len + runs[n - 2].len))\n-        {\n-            if n >= 3 && runs[n - 3].len < runs[n - 1].len { Some(n - 3) } else { Some(n - 2) }\n-        } else {\n-            None\n+    let run_dealloc_fn = |buf_ptr: *mut sort::TimSortRun, len: usize| {\n+        // SAFETY: The caller must ensure that buf_ptr was created by elem_alloc_fn with the same\n+        // len.\n+        unsafe {\n+            alloc::dealloc(\n+                buf_ptr as *mut u8,\n+                alloc::Layout::array::<sort::TimSortRun>(len).unwrap_unchecked(),\n+            );\n         }\n-    }\n+    };\n \n-    #[derive(Clone, Copy)]\n-    struct Run {\n-        start: usize,\n-        len: usize,\n-    }\n+    sort::merge_sort(v, &mut is_less, elem_alloc_fn, elem_dealloc_fn, run_alloc_fn, run_dealloc_fn);\n }"}, {"sha": "d93a3a57ecd27fdcdb94a4aa479a5e8ceb02be3c", "filename": "library/core/src/slice/mod.rs", "status": "modified", "additions": 7, "deletions": 1, "changes": 8, "blob_url": "https://github.com/rust-lang/rust/blob/68b390ae2a099a598bfe44d4ec36ae15159c283f/library%2Fcore%2Fsrc%2Fslice%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/68b390ae2a099a598bfe44d4ec36ae15159c283f/library%2Fcore%2Fsrc%2Fslice%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/library%2Fcore%2Fsrc%2Fslice%2Fmod.rs?ref=68b390ae2a099a598bfe44d4ec36ae15159c283f", "patch": "@@ -29,13 +29,19 @@ use crate::slice;\n /// Pure rust memchr implementation, taken from rust-memchr\n pub mod memchr;\n \n+#[unstable(\n+    feature = \"slice_internals\",\n+    issue = \"none\",\n+    reason = \"exposed from core to be reused in std;\"\n+)]\n+pub mod sort;\n+\n mod ascii;\n mod cmp;\n mod index;\n mod iter;\n mod raw;\n mod rotate;\n-mod sort;\n mod specialize;\n \n #[stable(feature = \"rust1\", since = \"1.0.0\")]"}, {"sha": "2181f9a811855f7ac2a39352b481f0f865e193d5", "filename": "library/core/src/slice/sort.rs", "status": "modified", "additions": 514, "deletions": 0, "changes": 514, "blob_url": "https://github.com/rust-lang/rust/blob/68b390ae2a099a598bfe44d4ec36ae15159c283f/library%2Fcore%2Fsrc%2Fslice%2Fsort.rs", "raw_url": "https://github.com/rust-lang/rust/raw/68b390ae2a099a598bfe44d4ec36ae15159c283f/library%2Fcore%2Fsrc%2Fslice%2Fsort.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/library%2Fcore%2Fsrc%2Fslice%2Fsort.rs?ref=68b390ae2a099a598bfe44d4ec36ae15159c283f", "patch": "@@ -5,6 +5,9 @@\n //!\n //! Unstable sorting is compatible with core because it doesn't allocate memory, unlike our\n //! stable sorting implementation.\n+//!\n+//! In addition it also contains the core logic of the stable sort used by `slice::sort` based on\n+//! TimSort.\n \n use crate::cmp;\n use crate::mem::{self, MaybeUninit, SizedTypeProperties};\n@@ -905,6 +908,7 @@ fn partition_at_index_loop<'a, T, F>(\n     }\n }\n \n+/// Reorder the slice such that the element at `index` is at its final sorted position.\n pub fn partition_at_index<T, F>(\n     v: &mut [T],\n     index: usize,\n@@ -949,3 +953,513 @@ where\n     let pivot = &mut pivot[0];\n     (left, pivot, right)\n }\n+\n+/// Inserts `v[0]` into pre-sorted sequence `v[1..]` so that whole `v[..]` becomes sorted.\n+///\n+/// This is the integral subroutine of insertion sort.\n+fn insert_head<T, F>(v: &mut [T], is_less: &mut F)\n+where\n+    F: FnMut(&T, &T) -> bool,\n+{\n+    if v.len() >= 2 && is_less(&v[1], &v[0]) {\n+        // SAFETY: Copy tmp back even if panic, and ensure unique observation.\n+        unsafe {\n+            // There are three ways to implement insertion here:\n+            //\n+            // 1. Swap adjacent elements until the first one gets to its final destination.\n+            //    However, this way we copy data around more than is necessary. If elements are big\n+            //    structures (costly to copy), this method will be slow.\n+            //\n+            // 2. Iterate until the right place for the first element is found. Then shift the\n+            //    elements succeeding it to make room for it and finally place it into the\n+            //    remaining hole. This is a good method.\n+            //\n+            // 3. Copy the first element into a temporary variable. Iterate until the right place\n+            //    for it is found. As we go along, copy every traversed element into the slot\n+            //    preceding it. Finally, copy data from the temporary variable into the remaining\n+            //    hole. This method is very good. Benchmarks demonstrated slightly better\n+            //    performance than with the 2nd method.\n+            //\n+            // All methods were benchmarked, and the 3rd showed best results. So we chose that one.\n+            let tmp = mem::ManuallyDrop::new(ptr::read(&v[0]));\n+\n+            // Intermediate state of the insertion process is always tracked by `hole`, which\n+            // serves two purposes:\n+            // 1. Protects integrity of `v` from panics in `is_less`.\n+            // 2. Fills the remaining hole in `v` in the end.\n+            //\n+            // Panic safety:\n+            //\n+            // If `is_less` panics at any point during the process, `hole` will get dropped and\n+            // fill the hole in `v` with `tmp`, thus ensuring that `v` still holds every object it\n+            // initially held exactly once.\n+            let mut hole = InsertionHole { src: &*tmp, dest: &mut v[1] };\n+            ptr::copy_nonoverlapping(&v[1], &mut v[0], 1);\n+\n+            for i in 2..v.len() {\n+                if !is_less(&v[i], &*tmp) {\n+                    break;\n+                }\n+                ptr::copy_nonoverlapping(&v[i], &mut v[i - 1], 1);\n+                hole.dest = &mut v[i];\n+            }\n+            // `hole` gets dropped and thus copies `tmp` into the remaining hole in `v`.\n+        }\n+    }\n+\n+    // When dropped, copies from `src` into `dest`.\n+    struct InsertionHole<T> {\n+        src: *const T,\n+        dest: *mut T,\n+    }\n+\n+    impl<T> Drop for InsertionHole<T> {\n+        fn drop(&mut self) {\n+            // SAFETY: The caller must ensure that src and dest are correctly set.\n+            unsafe {\n+                ptr::copy_nonoverlapping(self.src, self.dest, 1);\n+            }\n+        }\n+    }\n+}\n+\n+/// Merges non-decreasing runs `v[..mid]` and `v[mid..]` using `buf` as temporary storage, and\n+/// stores the result into `v[..]`.\n+///\n+/// # Safety\n+///\n+/// The two slices must be non-empty and `mid` must be in bounds. Buffer `buf` must be long enough\n+/// to hold a copy of the shorter slice. Also, `T` must not be a zero-sized type.\n+unsafe fn merge<T, F>(v: &mut [T], mid: usize, buf: *mut T, is_less: &mut F)\n+where\n+    F: FnMut(&T, &T) -> bool,\n+{\n+    let len = v.len();\n+    let v = v.as_mut_ptr();\n+\n+    // SAFETY: mid and len must be in-bounds of v.\n+    let (v_mid, v_end) = unsafe { (v.add(mid), v.add(len)) };\n+\n+    // The merge process first copies the shorter run into `buf`. Then it traces the newly copied\n+    // run and the longer run forwards (or backwards), comparing their next unconsumed elements and\n+    // copying the lesser (or greater) one into `v`.\n+    //\n+    // As soon as the shorter run is fully consumed, the process is done. If the longer run gets\n+    // consumed first, then we must copy whatever is left of the shorter run into the remaining\n+    // hole in `v`.\n+    //\n+    // Intermediate state of the process is always tracked by `hole`, which serves two purposes:\n+    // 1. Protects integrity of `v` from panics in `is_less`.\n+    // 2. Fills the remaining hole in `v` if the longer run gets consumed first.\n+    //\n+    // Panic safety:\n+    //\n+    // If `is_less` panics at any point during the process, `hole` will get dropped and fill the\n+    // hole in `v` with the unconsumed range in `buf`, thus ensuring that `v` still holds every\n+    // object it initially held exactly once.\n+    let mut hole;\n+\n+    if mid <= len - mid {\n+        // The left run is shorter.\n+\n+        // SAFETY: buf must have enough capacity for `v[..mid]`.\n+        unsafe {\n+            ptr::copy_nonoverlapping(v, buf, mid);\n+            hole = MergeHole { start: buf, end: buf.add(mid), dest: v };\n+        }\n+\n+        // Initially, these pointers point to the beginnings of their arrays.\n+        let left = &mut hole.start;\n+        let mut right = v_mid;\n+        let out = &mut hole.dest;\n+\n+        while *left < hole.end && right < v_end {\n+            // Consume the lesser side.\n+            // If equal, prefer the left run to maintain stability.\n+\n+            // SAFETY: left and right must be valid and part of v same for out.\n+            unsafe {\n+                let to_copy = if is_less(&*right, &**left) {\n+                    get_and_increment(&mut right)\n+                } else {\n+                    get_and_increment(left)\n+                };\n+                ptr::copy_nonoverlapping(to_copy, get_and_increment(out), 1);\n+            }\n+        }\n+    } else {\n+        // The right run is shorter.\n+\n+        // SAFETY: buf must have enough capacity for `v[mid..]`.\n+        unsafe {\n+            ptr::copy_nonoverlapping(v_mid, buf, len - mid);\n+            hole = MergeHole { start: buf, end: buf.add(len - mid), dest: v_mid };\n+        }\n+\n+        // Initially, these pointers point past the ends of their arrays.\n+        let left = &mut hole.dest;\n+        let right = &mut hole.end;\n+        let mut out = v_end;\n+\n+        while v < *left && buf < *right {\n+            // Consume the greater side.\n+            // If equal, prefer the right run to maintain stability.\n+\n+            // SAFETY: left and right must be valid and part of v same for out.\n+            unsafe {\n+                let to_copy = if is_less(&*right.sub(1), &*left.sub(1)) {\n+                    decrement_and_get(left)\n+                } else {\n+                    decrement_and_get(right)\n+                };\n+                ptr::copy_nonoverlapping(to_copy, decrement_and_get(&mut out), 1);\n+            }\n+        }\n+    }\n+    // Finally, `hole` gets dropped. If the shorter run was not fully consumed, whatever remains of\n+    // it will now be copied into the hole in `v`.\n+\n+    unsafe fn get_and_increment<T>(ptr: &mut *mut T) -> *mut T {\n+        let old = *ptr;\n+\n+        // SAFETY: ptr.add(1) must still be a valid pointer and part of `v`.\n+        *ptr = unsafe { ptr.add(1) };\n+        old\n+    }\n+\n+    unsafe fn decrement_and_get<T>(ptr: &mut *mut T) -> *mut T {\n+        // SAFETY: ptr.sub(1) must still be a valid pointer and part of `v`.\n+        *ptr = unsafe { ptr.sub(1) };\n+        *ptr\n+    }\n+\n+    // When dropped, copies the range `start..end` into `dest..`.\n+    struct MergeHole<T> {\n+        start: *mut T,\n+        end: *mut T,\n+        dest: *mut T,\n+    }\n+\n+    impl<T> Drop for MergeHole<T> {\n+        fn drop(&mut self) {\n+            // SAFETY: `T` is not a zero-sized type, and these are pointers into a slice's elements.\n+            unsafe {\n+                let len = self.end.sub_ptr(self.start);\n+                ptr::copy_nonoverlapping(self.start, self.dest, len);\n+            }\n+        }\n+    }\n+}\n+\n+/// This merge sort borrows some (but not all) ideas from TimSort, which used to be described in\n+/// detail [here](https://github.com/python/cpython/blob/main/Objects/listsort.txt). However Python\n+/// has switched to a Powersort based implementation.\n+///\n+/// The algorithm identifies strictly descending and non-descending subsequences, which are called\n+/// natural runs. There is a stack of pending runs yet to be merged. Each newly found run is pushed\n+/// onto the stack, and then some pairs of adjacent runs are merged until these two invariants are\n+/// satisfied:\n+///\n+/// 1. for every `i` in `1..runs.len()`: `runs[i - 1].len > runs[i].len`\n+/// 2. for every `i` in `2..runs.len()`: `runs[i - 2].len > runs[i - 1].len + runs[i].len`\n+///\n+/// The invariants ensure that the total running time is *O*(*n* \\* log(*n*)) worst-case.\n+pub fn merge_sort<T, CmpF, ElemAllocF, ElemDeallocF, RunAllocF, RunDeallocF>(\n+    v: &mut [T],\n+    is_less: &mut CmpF,\n+    elem_alloc_fn: ElemAllocF,\n+    elem_dealloc_fn: ElemDeallocF,\n+    run_alloc_fn: RunAllocF,\n+    run_dealloc_fn: RunDeallocF,\n+) where\n+    CmpF: FnMut(&T, &T) -> bool,\n+    ElemAllocF: Fn(usize) -> *mut T,\n+    ElemDeallocF: Fn(*mut T, usize),\n+    RunAllocF: Fn(usize) -> *mut TimSortRun,\n+    RunDeallocF: Fn(*mut TimSortRun, usize),\n+{\n+    // Slices of up to this length get sorted using insertion sort.\n+    const MAX_INSERTION: usize = 20;\n+    // Very short runs are extended using insertion sort to span at least this many elements.\n+    const MIN_RUN: usize = 10;\n+\n+    // The caller should have already checked that.\n+    debug_assert!(!T::IS_ZST);\n+\n+    let len = v.len();\n+\n+    // Short arrays get sorted in-place via insertion sort to avoid allocations.\n+    if len <= MAX_INSERTION {\n+        if len >= 2 {\n+            for i in (0..len - 1).rev() {\n+                insert_head(&mut v[i..], is_less);\n+            }\n+        }\n+        return;\n+    }\n+\n+    // Allocate a buffer to use as scratch memory. We keep the length 0 so we can keep in it\n+    // shallow copies of the contents of `v` without risking the dtors running on copies if\n+    // `is_less` panics. When merging two sorted runs, this buffer holds a copy of the shorter run,\n+    // which will always have length at most `len / 2`.\n+    let buf = BufGuard::new(len / 2, elem_alloc_fn, elem_dealloc_fn);\n+    let buf_ptr = buf.buf_ptr;\n+\n+    let mut runs = RunVec::new(run_alloc_fn, run_dealloc_fn);\n+\n+    // In order to identify natural runs in `v`, we traverse it backwards. That might seem like a\n+    // strange decision, but consider the fact that merges more often go in the opposite direction\n+    // (forwards). According to benchmarks, merging forwards is slightly faster than merging\n+    // backwards. To conclude, identifying runs by traversing backwards improves performance.\n+    let mut end = len;\n+    while end > 0 {\n+        // Find the next natural run, and reverse it if it's strictly descending.\n+        let mut start = end - 1;\n+        if start > 0 {\n+            start -= 1;\n+\n+            // SAFETY: The v.get_unchecked must be fed with correct inbound indicies.\n+            unsafe {\n+                if is_less(v.get_unchecked(start + 1), v.get_unchecked(start)) {\n+                    while start > 0 && is_less(v.get_unchecked(start), v.get_unchecked(start - 1)) {\n+                        start -= 1;\n+                    }\n+                    v[start..end].reverse();\n+                } else {\n+                    while start > 0 && !is_less(v.get_unchecked(start), v.get_unchecked(start - 1))\n+                    {\n+                        start -= 1;\n+                    }\n+                }\n+            }\n+        }\n+\n+        // Insert some more elements into the run if it's too short. Insertion sort is faster than\n+        // merge sort on short sequences, so this significantly improves performance.\n+        while start > 0 && end - start < MIN_RUN {\n+            start -= 1;\n+            insert_head(&mut v[start..end], is_less);\n+        }\n+\n+        // Push this run onto the stack.\n+        runs.push(TimSortRun { start, len: end - start });\n+        end = start;\n+\n+        // Merge some pairs of adjacent runs to satisfy the invariants.\n+        while let Some(r) = collapse(runs.as_slice()) {\n+            let left = runs[r + 1];\n+            let right = runs[r];\n+            // SAFETY: `buf_ptr` must hold enough capacity for the shorter of the two sides, and\n+            // neither side may be on length 0.\n+            unsafe {\n+                merge(&mut v[left.start..right.start + right.len], left.len, buf_ptr, is_less);\n+            }\n+            runs[r] = TimSortRun { start: left.start, len: left.len + right.len };\n+            runs.remove(r + 1);\n+        }\n+    }\n+\n+    // Finally, exactly one run must remain in the stack.\n+    debug_assert!(runs.len() == 1 && runs[0].start == 0 && runs[0].len == len);\n+\n+    // Examines the stack of runs and identifies the next pair of runs to merge. More specifically,\n+    // if `Some(r)` is returned, that means `runs[r]` and `runs[r + 1]` must be merged next. If the\n+    // algorithm should continue building a new run instead, `None` is returned.\n+    //\n+    // TimSort is infamous for its buggy implementations, as described here:\n+    // http://envisage-project.eu/timsort-specification-and-verification/\n+    //\n+    // The gist of the story is: we must enforce the invariants on the top four runs on the stack.\n+    // Enforcing them on just top three is not sufficient to ensure that the invariants will still\n+    // hold for *all* runs in the stack.\n+    //\n+    // This function correctly checks invariants for the top four runs. Additionally, if the top\n+    // run starts at index 0, it will always demand a merge operation until the stack is fully\n+    // collapsed, in order to complete the sort.\n+    #[inline]\n+    fn collapse(runs: &[TimSortRun]) -> Option<usize> {\n+        let n = runs.len();\n+        if n >= 2\n+            && (runs[n - 1].start == 0\n+                || runs[n - 2].len <= runs[n - 1].len\n+                || (n >= 3 && runs[n - 3].len <= runs[n - 2].len + runs[n - 1].len)\n+                || (n >= 4 && runs[n - 4].len <= runs[n - 3].len + runs[n - 2].len))\n+        {\n+            if n >= 3 && runs[n - 3].len < runs[n - 1].len { Some(n - 3) } else { Some(n - 2) }\n+        } else {\n+            None\n+        }\n+    }\n+\n+    // Extremely basic versions of Vec.\n+    // Their use is super limited and by having the code here, it allows reuse between the sort\n+    // implementations.\n+    struct BufGuard<T, ElemDeallocF>\n+    where\n+        ElemDeallocF: Fn(*mut T, usize),\n+    {\n+        buf_ptr: *mut T,\n+        capacity: usize,\n+        elem_dealloc_fn: ElemDeallocF,\n+    }\n+\n+    impl<T, ElemDeallocF> BufGuard<T, ElemDeallocF>\n+    where\n+        ElemDeallocF: Fn(*mut T, usize),\n+    {\n+        fn new<ElemAllocF>(\n+            len: usize,\n+            elem_alloc_fn: ElemAllocF,\n+            elem_dealloc_fn: ElemDeallocF,\n+        ) -> Self\n+        where\n+            ElemAllocF: Fn(usize) -> *mut T,\n+        {\n+            Self { buf_ptr: elem_alloc_fn(len), capacity: len, elem_dealloc_fn }\n+        }\n+    }\n+\n+    impl<T, ElemDeallocF> Drop for BufGuard<T, ElemDeallocF>\n+    where\n+        ElemDeallocF: Fn(*mut T, usize),\n+    {\n+        fn drop(&mut self) {\n+            (self.elem_dealloc_fn)(self.buf_ptr, self.capacity);\n+        }\n+    }\n+\n+    struct RunVec<RunAllocF, RunDeallocF>\n+    where\n+        RunAllocF: Fn(usize) -> *mut TimSortRun,\n+        RunDeallocF: Fn(*mut TimSortRun, usize),\n+    {\n+        buf_ptr: *mut TimSortRun,\n+        capacity: usize,\n+        len: usize,\n+        run_alloc_fn: RunAllocF,\n+        run_dealloc_fn: RunDeallocF,\n+    }\n+\n+    impl<RunAllocF, RunDeallocF> RunVec<RunAllocF, RunDeallocF>\n+    where\n+        RunAllocF: Fn(usize) -> *mut TimSortRun,\n+        RunDeallocF: Fn(*mut TimSortRun, usize),\n+    {\n+        fn new(run_alloc_fn: RunAllocF, run_dealloc_fn: RunDeallocF) -> Self {\n+            // Most slices can be sorted with at most 16 runs in-flight.\n+            const START_RUN_CAPACITY: usize = 16;\n+\n+            Self {\n+                buf_ptr: run_alloc_fn(START_RUN_CAPACITY),\n+                capacity: START_RUN_CAPACITY,\n+                len: 0,\n+                run_alloc_fn,\n+                run_dealloc_fn,\n+            }\n+        }\n+\n+        fn push(&mut self, val: TimSortRun) {\n+            if self.len == self.capacity {\n+                let old_capacity = self.capacity;\n+                let old_buf_ptr = self.buf_ptr;\n+\n+                self.capacity = self.capacity * 2;\n+                self.buf_ptr = (self.run_alloc_fn)(self.capacity);\n+\n+                // SAFETY: buf_ptr new and old were correctly allocated and old_buf_ptr has\n+                // old_capacity valid elements.\n+                unsafe {\n+                    ptr::copy_nonoverlapping(old_buf_ptr, self.buf_ptr, old_capacity);\n+                }\n+\n+                (self.run_dealloc_fn)(old_buf_ptr, old_capacity);\n+            }\n+\n+            // SAFETY: The invariant was just checked.\n+            unsafe {\n+                self.buf_ptr.add(self.len).write(val);\n+            }\n+            self.len += 1;\n+        }\n+\n+        fn remove(&mut self, index: usize) {\n+            if index >= self.len {\n+                panic!(\"Index out of bounds\");\n+            }\n+\n+            // SAFETY: buf_ptr needs to be valid and len invariant upheld.\n+            unsafe {\n+                // the place we are taking from.\n+                let ptr = self.buf_ptr.add(index);\n+\n+                // Shift everything down to fill in that spot.\n+                ptr::copy(ptr.add(1), ptr, self.len - index - 1);\n+            }\n+            self.len -= 1;\n+        }\n+\n+        fn as_slice(&self) -> &[TimSortRun] {\n+            // SAFETY: Safe as long as buf_ptr is valid and len invariant was upheld.\n+            unsafe { &*ptr::slice_from_raw_parts(self.buf_ptr, self.len) }\n+        }\n+\n+        fn len(&self) -> usize {\n+            self.len\n+        }\n+    }\n+\n+    impl<RunAllocF, RunDeallocF> core::ops::Index<usize> for RunVec<RunAllocF, RunDeallocF>\n+    where\n+        RunAllocF: Fn(usize) -> *mut TimSortRun,\n+        RunDeallocF: Fn(*mut TimSortRun, usize),\n+    {\n+        type Output = TimSortRun;\n+\n+        fn index(&self, index: usize) -> &Self::Output {\n+            if index < self.len {\n+                // SAFETY: buf_ptr and len invariant must be upheld.\n+                unsafe {\n+                    return &*(self.buf_ptr.add(index));\n+                }\n+            }\n+\n+            panic!(\"Index out of bounds\");\n+        }\n+    }\n+\n+    impl<RunAllocF, RunDeallocF> core::ops::IndexMut<usize> for RunVec<RunAllocF, RunDeallocF>\n+    where\n+        RunAllocF: Fn(usize) -> *mut TimSortRun,\n+        RunDeallocF: Fn(*mut TimSortRun, usize),\n+    {\n+        fn index_mut(&mut self, index: usize) -> &mut Self::Output {\n+            if index < self.len {\n+                // SAFETY: buf_ptr and len invariant must be upheld.\n+                unsafe {\n+                    return &mut *(self.buf_ptr.add(index));\n+                }\n+            }\n+\n+            panic!(\"Index out of bounds\");\n+        }\n+    }\n+\n+    impl<RunAllocF, RunDeallocF> Drop for RunVec<RunAllocF, RunDeallocF>\n+    where\n+        RunAllocF: Fn(usize) -> *mut TimSortRun,\n+        RunDeallocF: Fn(*mut TimSortRun, usize),\n+    {\n+        fn drop(&mut self) {\n+            // As long as TimSortRun is Copy we don't need to drop them individually but just the\n+            // whole allocation.\n+            (self.run_dealloc_fn)(self.buf_ptr, self.capacity);\n+        }\n+    }\n+}\n+\n+/// Internal type used by merge_sort.\n+#[derive(Clone, Copy, Debug)]\n+pub struct TimSortRun {\n+    len: usize,\n+    start: usize,\n+}"}]}