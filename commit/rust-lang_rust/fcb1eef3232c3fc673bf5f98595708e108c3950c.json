{"sha": "fcb1eef3232c3fc673bf5f98595708e108c3950c", "node_id": "MDY6Q29tbWl0NzI0NzEyOmZjYjFlZWYzMjMyYzNmYzY3M2JmNWY5ODU5NTcwOGUxMDhjMzk1MGM=", "commit": {"author": {"name": "Edwin Cheng", "email": "edwin0cheng@gmail.com", "date": "2019-05-25T12:31:53Z"}, "committer": {"name": "Edwin Cheng", "email": "edwin0cheng@gmail.com", "date": "2019-05-25T12:41:03Z"}, "message": "Change TokenSource to iteration based", "tree": {"sha": "326a3961c5ca99cb50a12fb94f3398116af4a2f1", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/326a3961c5ca99cb50a12fb94f3398116af4a2f1"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/fcb1eef3232c3fc673bf5f98595708e108c3950c", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/fcb1eef3232c3fc673bf5f98595708e108c3950c", "html_url": "https://github.com/rust-lang/rust/commit/fcb1eef3232c3fc673bf5f98595708e108c3950c", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/fcb1eef3232c3fc673bf5f98595708e108c3950c/comments", "author": {"login": "edwin0cheng", "id": 11014119, "node_id": "MDQ6VXNlcjExMDE0MTE5", "avatar_url": "https://avatars.githubusercontent.com/u/11014119?v=4", "gravatar_id": "", "url": "https://api.github.com/users/edwin0cheng", "html_url": "https://github.com/edwin0cheng", "followers_url": "https://api.github.com/users/edwin0cheng/followers", "following_url": "https://api.github.com/users/edwin0cheng/following{/other_user}", "gists_url": "https://api.github.com/users/edwin0cheng/gists{/gist_id}", "starred_url": "https://api.github.com/users/edwin0cheng/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/edwin0cheng/subscriptions", "organizations_url": "https://api.github.com/users/edwin0cheng/orgs", "repos_url": "https://api.github.com/users/edwin0cheng/repos", "events_url": "https://api.github.com/users/edwin0cheng/events{/privacy}", "received_events_url": "https://api.github.com/users/edwin0cheng/received_events", "type": "User", "site_admin": false}, "committer": {"login": "edwin0cheng", "id": 11014119, "node_id": "MDQ6VXNlcjExMDE0MTE5", "avatar_url": "https://avatars.githubusercontent.com/u/11014119?v=4", "gravatar_id": "", "url": "https://api.github.com/users/edwin0cheng", "html_url": "https://github.com/edwin0cheng", "followers_url": "https://api.github.com/users/edwin0cheng/followers", "following_url": "https://api.github.com/users/edwin0cheng/following{/other_user}", "gists_url": "https://api.github.com/users/edwin0cheng/gists{/gist_id}", "starred_url": "https://api.github.com/users/edwin0cheng/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/edwin0cheng/subscriptions", "organizations_url": "https://api.github.com/users/edwin0cheng/orgs", "repos_url": "https://api.github.com/users/edwin0cheng/repos", "events_url": "https://api.github.com/users/edwin0cheng/events{/privacy}", "received_events_url": "https://api.github.com/users/edwin0cheng/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "ef00b5af1c7a7a7cac685eff661a10252825d84a", "url": "https://api.github.com/repos/rust-lang/rust/commits/ef00b5af1c7a7a7cac685eff661a10252825d84a", "html_url": "https://github.com/rust-lang/rust/commit/ef00b5af1c7a7a7cac685eff661a10252825d84a"}], "stats": {"total": 271, "additions": 171, "deletions": 100}, "files": [{"sha": "9cc989b23ae5fa23fda4cd7287fd5fecd98eb8a8", "filename": "crates/ra_mbe/src/subtree_parser.rs", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/rust-lang/rust/blob/fcb1eef3232c3fc673bf5f98595708e108c3950c/crates%2Fra_mbe%2Fsrc%2Fsubtree_parser.rs", "raw_url": "https://github.com/rust-lang/rust/raw/fcb1eef3232c3fc673bf5f98595708e108c3950c/crates%2Fra_mbe%2Fsrc%2Fsubtree_parser.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fra_mbe%2Fsrc%2Fsubtree_parser.rs?ref=fcb1eef3232c3fc673bf5f98595708e108c3950c", "patch": "@@ -68,13 +68,13 @@ impl<'a> Parser<'a> {\n \n     fn parse<F>(self, f: F) -> Option<tt::TokenTree>\n     where\n-        F: FnOnce(&dyn TokenSource, &mut dyn TreeSink),\n+        F: FnOnce(&mut dyn TokenSource, &mut dyn TreeSink),\n     {\n         let buffer = TokenBuffer::new(&self.subtree.token_trees[*self.cur_pos..]);\n         let mut src = SubtreeTokenSource::new(&buffer);\n         let mut sink = OffsetTokenSink { token_pos: 0, error: false };\n \n-        f(&src, &mut sink);\n+        f(&mut src, &mut sink);\n \n         let r = self.finish(sink.token_pos, &mut src);\n         if sink.error {"}, {"sha": "c4f79f38a3f76453d7058310befb5e432ca7754a", "filename": "crates/ra_mbe/src/subtree_source.rs", "status": "modified", "additions": 38, "deletions": 21, "changes": 59, "blob_url": "https://github.com/rust-lang/rust/blob/fcb1eef3232c3fc673bf5f98595708e108c3950c/crates%2Fra_mbe%2Fsrc%2Fsubtree_source.rs", "raw_url": "https://github.com/rust-lang/rust/raw/fcb1eef3232c3fc673bf5f98595708e108c3950c/crates%2Fra_mbe%2Fsrc%2Fsubtree_source.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fra_mbe%2Fsrc%2Fsubtree_source.rs?ref=fcb1eef3232c3fc673bf5f98595708e108c3950c", "patch": "@@ -1,6 +1,7 @@\n-use ra_parser::{TokenSource};\n+use ra_parser::{TokenSource, Token};\n use ra_syntax::{classify_literal, SmolStr, SyntaxKind, SyntaxKind::*, T};\n use std::cell::{RefCell, Cell};\n+use std::sync::Arc;\n use tt::buffer::{TokenBuffer, Cursor};\n \n pub(crate) trait Querier {\n@@ -65,7 +66,7 @@ impl<'a> SubtreeWalk<'a> {\n         return cached[pos].clone();\n     }\n \n-    fn collect_token_trees(&mut self, n: usize) -> Vec<tt::TokenTree> {\n+    fn collect_token_trees(&self, n: usize) -> Vec<tt::TokenTree> {\n         let mut res = vec![];\n \n         let mut pos = 0;\n@@ -117,43 +118,59 @@ impl<'a> Querier for SubtreeWalk<'a> {\n }\n \n pub(crate) struct SubtreeTokenSource<'a> {\n-    walker: SubtreeWalk<'a>,\n+    walker: Arc<SubtreeWalk<'a>>,\n+    curr: (Token, usize),\n }\n \n impl<'a> SubtreeTokenSource<'a> {\n     pub fn new(buffer: &'a TokenBuffer) -> SubtreeTokenSource<'a> {\n-        SubtreeTokenSource { walker: SubtreeWalk::new(buffer.begin()) }\n+        let mut res = SubtreeTokenSource {\n+            walker: Arc::new(SubtreeWalk::new(buffer.begin())),\n+            curr: (Token { kind: EOF, is_jointed_to_next: false }, 0),\n+        };\n+        res.curr = (res.mk_token(0), 0);\n+        res\n     }\n \n-    pub fn querier<'b>(&'a self) -> &'b SubtreeWalk<'a>\n-    where\n-        'a: 'b,\n-    {\n-        &self.walker\n+    pub fn querier(&self) -> Arc<SubtreeWalk<'a>> {\n+        self.walker.clone()\n     }\n \n     pub(crate) fn bump_n(&mut self, parsed_tokens: usize) -> Vec<tt::TokenTree> {\n         let res = self.walker.collect_token_trees(parsed_tokens);\n         res\n     }\n+\n+    fn mk_token(&self, pos: usize) -> Token {\n+        match self.walker.get(pos) {\n+            Some(tt) => Token { kind: tt.kind, is_jointed_to_next: tt.is_joint_to_next },\n+            None => Token { kind: EOF, is_jointed_to_next: false },\n+        }\n+    }\n }\n \n impl<'a> TokenSource for SubtreeTokenSource<'a> {\n-    fn token_kind(&self, pos: usize) -> SyntaxKind {\n-        if let Some(tok) = self.walker.get(pos) {\n-            tok.kind\n-        } else {\n-            SyntaxKind::EOF\n-        }\n+    fn current(&self) -> Token {\n+        self.curr.0\n     }\n-    fn is_token_joint_to_next(&self, pos: usize) -> bool {\n-        match self.walker.get(pos) {\n-            Some(t) => t.is_joint_to_next,\n-            _ => false,\n+\n+    /// Lookahead n token\n+    fn lookahead_nth(&self, n: usize) -> Token {\n+        self.mk_token(self.curr.1 + n)\n+    }\n+\n+    /// bump cursor to next token\n+    fn bump(&mut self) {\n+        if self.current().kind == EOF {\n+            return;\n         }\n+\n+        self.curr = (self.mk_token(self.curr.1 + 1), self.curr.1 + 1)\n     }\n-    fn is_keyword(&self, pos: usize, kw: &str) -> bool {\n-        match self.walker.get(pos) {\n+\n+    /// Is the current token a specified keyword?\n+    fn is_keyword(&self, kw: &str) -> bool {\n+        match self.walker.get(self.curr.1) {\n             Some(t) => t.text == *kw,\n             _ => false,\n         }"}, {"sha": "0aab5ea8b2511df8db7531b5cbc41c97189b30e3", "filename": "crates/ra_mbe/src/syntax_bridge.rs", "status": "modified", "additions": 24, "deletions": 18, "changes": 42, "blob_url": "https://github.com/rust-lang/rust/blob/fcb1eef3232c3fc673bf5f98595708e108c3950c/crates%2Fra_mbe%2Fsrc%2Fsyntax_bridge.rs", "raw_url": "https://github.com/rust-lang/rust/raw/fcb1eef3232c3fc673bf5f98595708e108c3950c/crates%2Fra_mbe%2Fsrc%2Fsyntax_bridge.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fra_mbe%2Fsrc%2Fsyntax_bridge.rs?ref=fcb1eef3232c3fc673bf5f98595708e108c3950c", "patch": "@@ -48,9 +48,10 @@ pub fn syntax_node_to_token_tree(node: &SyntaxNode) -> Option<(tt::Subtree, Toke\n /// Parses the token tree (result of macro expansion) to an expression\n pub fn token_tree_to_expr(tt: &tt::Subtree) -> Result<TreeArc<ast::Expr>, ExpandError> {\n     let buffer = tt::buffer::TokenBuffer::new(&[tt.clone().into()]);\n-    let token_source = SubtreeTokenSource::new(&buffer);\n-    let mut tree_sink = TtTreeSink::new(token_source.querier());\n-    ra_parser::parse_expr(&token_source, &mut tree_sink);\n+    let mut token_source = SubtreeTokenSource::new(&buffer);\n+    let querier = token_source.querier();\n+    let mut tree_sink = TtTreeSink::new(querier.as_ref());\n+    ra_parser::parse_expr(&mut token_source, &mut tree_sink);\n     if tree_sink.roots.len() != 1 {\n         return Err(ExpandError::ConversionError);\n     }\n@@ -64,9 +65,10 @@ pub fn token_tree_to_expr(tt: &tt::Subtree) -> Result<TreeArc<ast::Expr>, Expand\n /// Parses the token tree (result of macro expansion) to a Pattern\n pub fn token_tree_to_pat(tt: &tt::Subtree) -> Result<TreeArc<ast::Pat>, ExpandError> {\n     let buffer = tt::buffer::TokenBuffer::new(&[tt.clone().into()]);\n-    let token_source = SubtreeTokenSource::new(&buffer);\n-    let mut tree_sink = TtTreeSink::new(token_source.querier());\n-    ra_parser::parse_pat(&token_source, &mut tree_sink);\n+    let mut token_source = SubtreeTokenSource::new(&buffer);\n+    let querier = token_source.querier();\n+    let mut tree_sink = TtTreeSink::new(querier.as_ref());\n+    ra_parser::parse_pat(&mut token_source, &mut tree_sink);\n     if tree_sink.roots.len() != 1 {\n         return Err(ExpandError::ConversionError);\n     }\n@@ -78,9 +80,10 @@ pub fn token_tree_to_pat(tt: &tt::Subtree) -> Result<TreeArc<ast::Pat>, ExpandEr\n /// Parses the token tree (result of macro expansion) to a Type\n pub fn token_tree_to_ty(tt: &tt::Subtree) -> Result<TreeArc<ast::TypeRef>, ExpandError> {\n     let buffer = tt::buffer::TokenBuffer::new(&[tt.clone().into()]);\n-    let token_source = SubtreeTokenSource::new(&buffer);\n-    let mut tree_sink = TtTreeSink::new(token_source.querier());\n-    ra_parser::parse_ty(&token_source, &mut tree_sink);\n+    let mut token_source = SubtreeTokenSource::new(&buffer);\n+    let querier = token_source.querier();\n+    let mut tree_sink = TtTreeSink::new(querier.as_ref());\n+    ra_parser::parse_ty(&mut token_source, &mut tree_sink);\n     if tree_sink.roots.len() != 1 {\n         return Err(ExpandError::ConversionError);\n     }\n@@ -93,9 +96,10 @@ pub fn token_tree_to_macro_stmts(\n     tt: &tt::Subtree,\n ) -> Result<TreeArc<ast::MacroStmts>, ExpandError> {\n     let buffer = tt::buffer::TokenBuffer::new(&[tt.clone().into()]);\n-    let token_source = SubtreeTokenSource::new(&buffer);\n-    let mut tree_sink = TtTreeSink::new(token_source.querier());\n-    ra_parser::parse_macro_stmts(&token_source, &mut tree_sink);\n+    let mut token_source = SubtreeTokenSource::new(&buffer);\n+    let querier = token_source.querier();\n+    let mut tree_sink = TtTreeSink::new(querier.as_ref());\n+    ra_parser::parse_macro_stmts(&mut token_source, &mut tree_sink);\n     if tree_sink.roots.len() != 1 {\n         return Err(ExpandError::ConversionError);\n     }\n@@ -108,9 +112,10 @@ pub fn token_tree_to_macro_items(\n     tt: &tt::Subtree,\n ) -> Result<TreeArc<ast::MacroItems>, ExpandError> {\n     let buffer = tt::buffer::TokenBuffer::new(&[tt.clone().into()]);\n-    let token_source = SubtreeTokenSource::new(&buffer);\n-    let mut tree_sink = TtTreeSink::new(token_source.querier());\n-    ra_parser::parse_macro_items(&token_source, &mut tree_sink);\n+    let mut token_source = SubtreeTokenSource::new(&buffer);\n+    let querier = token_source.querier();\n+    let mut tree_sink = TtTreeSink::new(querier.as_ref());\n+    ra_parser::parse_macro_items(&mut token_source, &mut tree_sink);\n     if tree_sink.roots.len() != 1 {\n         return Err(ExpandError::ConversionError);\n     }\n@@ -121,9 +126,10 @@ pub fn token_tree_to_macro_items(\n /// Parses the token tree (result of macro expansion) as a sequence of items\n pub fn token_tree_to_ast_item_list(tt: &tt::Subtree) -> TreeArc<ast::SourceFile> {\n     let buffer = tt::buffer::TokenBuffer::new(&[tt.clone().into()]);\n-    let token_source = SubtreeTokenSource::new(&buffer);\n-    let mut tree_sink = TtTreeSink::new(token_source.querier());\n-    ra_parser::parse(&token_source, &mut tree_sink);\n+    let mut token_source = SubtreeTokenSource::new(&buffer);\n+    let querier = token_source.querier();\n+    let mut tree_sink = TtTreeSink::new(querier.as_ref());\n+    ra_parser::parse(&mut token_source, &mut tree_sink);\n     let syntax = tree_sink.inner.finish();\n     ast::SourceFile::cast(&syntax).unwrap().to_owned()\n }"}, {"sha": "3d88be6425e80e3b7a8879596c0963f462b12b24", "filename": "crates/ra_parser/src/lib.rs", "status": "modified", "additions": 36, "deletions": 18, "changes": 54, "blob_url": "https://github.com/rust-lang/rust/blob/fcb1eef3232c3fc673bf5f98595708e108c3950c/crates%2Fra_parser%2Fsrc%2Flib.rs", "raw_url": "https://github.com/rust-lang/rust/raw/fcb1eef3232c3fc673bf5f98595708e108c3950c/crates%2Fra_parser%2Fsrc%2Flib.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fra_parser%2Fsrc%2Flib.rs?ref=fcb1eef3232c3fc673bf5f98595708e108c3950c", "patch": "@@ -31,12 +31,26 @@ pub struct ParseError(pub String);\n ///\n /// Hopefully this will allow us to treat text and token trees in the same way!\n pub trait TokenSource {\n+    fn current(&self) -> Token;\n+\n+    /// Lookahead n token\n+    fn lookahead_nth(&self, n: usize) -> Token;\n+\n+    /// bump cursor to next token\n+    fn bump(&mut self);\n+\n+    /// Is the current token a specified keyword?\n+    fn is_keyword(&self, kw: &str) -> bool;\n+}\n+\n+/// `TokenCursor` abstracts the cursor of `TokenSource` operates one.\n+#[derive(Debug, Copy, Clone, Eq, PartialEq)]\n+pub struct Token {\n     /// What is the current token?\n-    fn token_kind(&self, pos: usize) -> SyntaxKind;\n+    pub kind: SyntaxKind,\n+\n     /// Is the current token joined to the next one (`> >` vs `>>`).\n-    fn is_token_joint_to_next(&self, pos: usize) -> bool;\n-    /// Is the current token a specified keyword?\n-    fn is_keyword(&self, pos: usize, kw: &str) -> bool;\n+    pub is_jointed_to_next: bool,\n }\n \n /// `TreeSink` abstracts details of a particular syntax tree implementation.\n@@ -54,7 +68,7 @@ pub trait TreeSink {\n     fn error(&mut self, error: ParseError);\n }\n \n-fn parse_from_tokens<F>(token_source: &dyn TokenSource, tree_sink: &mut dyn TreeSink, f: F)\n+fn parse_from_tokens<F>(token_source: &mut dyn TokenSource, tree_sink: &mut dyn TreeSink, f: F)\n where\n     F: FnOnce(&mut parser::Parser),\n {\n@@ -65,61 +79,65 @@ where\n }\n \n /// Parse given tokens into the given sink as a rust file.\n-pub fn parse(token_source: &dyn TokenSource, tree_sink: &mut dyn TreeSink) {\n+pub fn parse(token_source: &mut dyn TokenSource, tree_sink: &mut dyn TreeSink) {\n     parse_from_tokens(token_source, tree_sink, grammar::root);\n }\n \n /// Parse given tokens into the given sink as a path\n-pub fn parse_path(token_source: &dyn TokenSource, tree_sink: &mut dyn TreeSink) {\n+pub fn parse_path(token_source: &mut dyn TokenSource, tree_sink: &mut dyn TreeSink) {\n     parse_from_tokens(token_source, tree_sink, grammar::path);\n }\n \n /// Parse given tokens into the given sink as a expression\n-pub fn parse_expr(token_source: &dyn TokenSource, tree_sink: &mut dyn TreeSink) {\n+pub fn parse_expr(token_source: &mut dyn TokenSource, tree_sink: &mut dyn TreeSink) {\n     parse_from_tokens(token_source, tree_sink, grammar::expr);\n }\n \n /// Parse given tokens into the given sink as a ty\n-pub fn parse_ty(token_source: &dyn TokenSource, tree_sink: &mut dyn TreeSink) {\n+pub fn parse_ty(token_source: &mut dyn TokenSource, tree_sink: &mut dyn TreeSink) {\n     parse_from_tokens(token_source, tree_sink, grammar::type_);\n }\n \n /// Parse given tokens into the given sink as a pattern\n-pub fn parse_pat(token_source: &dyn TokenSource, tree_sink: &mut dyn TreeSink) {\n+pub fn parse_pat(token_source: &mut dyn TokenSource, tree_sink: &mut dyn TreeSink) {\n     parse_from_tokens(token_source, tree_sink, grammar::pattern);\n }\n \n /// Parse given tokens into the given sink as a statement\n-pub fn parse_stmt(token_source: &dyn TokenSource, tree_sink: &mut dyn TreeSink, with_semi: bool) {\n+pub fn parse_stmt(\n+    token_source: &mut dyn TokenSource,\n+    tree_sink: &mut dyn TreeSink,\n+    with_semi: bool,\n+) {\n     parse_from_tokens(token_source, tree_sink, |p| grammar::stmt(p, with_semi));\n }\n \n /// Parse given tokens into the given sink as a block\n-pub fn parse_block(token_source: &dyn TokenSource, tree_sink: &mut dyn TreeSink) {\n+pub fn parse_block(token_source: &mut dyn TokenSource, tree_sink: &mut dyn TreeSink) {\n     parse_from_tokens(token_source, tree_sink, grammar::block);\n }\n \n-pub fn parse_meta(token_source: &dyn TokenSource, tree_sink: &mut dyn TreeSink) {\n+pub fn parse_meta(token_source: &mut dyn TokenSource, tree_sink: &mut dyn TreeSink) {\n     parse_from_tokens(token_source, tree_sink, grammar::meta_item);\n }\n \n /// Parse given tokens into the given sink as an item\n-pub fn parse_item(token_source: &dyn TokenSource, tree_sink: &mut dyn TreeSink) {\n+pub fn parse_item(token_source: &mut dyn TokenSource, tree_sink: &mut dyn TreeSink) {\n     parse_from_tokens(token_source, tree_sink, grammar::item);\n }\n \n /// Parse given tokens into the given sink as an visibility qualifier\n-pub fn parse_vis(token_source: &dyn TokenSource, tree_sink: &mut dyn TreeSink) {\n+pub fn parse_vis(token_source: &mut dyn TokenSource, tree_sink: &mut dyn TreeSink) {\n     parse_from_tokens(token_source, tree_sink, |p| {\n         grammar::opt_visibility(p);\n     });\n }\n \n-pub fn parse_macro_items(token_source: &dyn TokenSource, tree_sink: &mut dyn TreeSink) {\n+pub fn parse_macro_items(token_source: &mut dyn TokenSource, tree_sink: &mut dyn TreeSink) {\n     parse_from_tokens(token_source, tree_sink, grammar::macro_items);\n }\n \n-pub fn parse_macro_stmts(token_source: &dyn TokenSource, tree_sink: &mut dyn TreeSink) {\n+pub fn parse_macro_stmts(token_source: &mut dyn TokenSource, tree_sink: &mut dyn TreeSink) {\n     parse_from_tokens(token_source, tree_sink, grammar::macro_stmts);\n }\n \n@@ -140,7 +158,7 @@ impl Reparser {\n     ///\n     /// Tokens must start with `{`, end with `}` and form a valid brace\n     /// sequence.\n-    pub fn parse(self, token_source: &dyn TokenSource, tree_sink: &mut dyn TreeSink) {\n+    pub fn parse(self, token_source: &mut dyn TokenSource, tree_sink: &mut dyn TreeSink) {\n         let Reparser(r) = self;\n         let mut p = parser::Parser::new(token_source);\n         r(&mut p);"}, {"sha": "8f654f04c0740e5eb18d2c74d7a60227bed482b6", "filename": "crates/ra_parser/src/parser.rs", "status": "modified", "additions": 33, "deletions": 24, "changes": 57, "blob_url": "https://github.com/rust-lang/rust/blob/fcb1eef3232c3fc673bf5f98595708e108c3950c/crates%2Fra_parser%2Fsrc%2Fparser.rs", "raw_url": "https://github.com/rust-lang/rust/raw/fcb1eef3232c3fc673bf5f98595708e108c3950c/crates%2Fra_parser%2Fsrc%2Fparser.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fra_parser%2Fsrc%2Fparser.rs?ref=fcb1eef3232c3fc673bf5f98595708e108c3950c", "patch": "@@ -19,15 +19,14 @@ use crate::{\n /// \"start expression, consume number literal,\n /// finish expression\". See `Event` docs for more.\n pub(crate) struct Parser<'t> {\n-    token_source: &'t dyn TokenSource,\n-    token_pos: usize,\n+    token_source: &'t mut dyn TokenSource,\n     events: Vec<Event>,\n     steps: Cell<u32>,\n }\n \n impl<'t> Parser<'t> {\n-    pub(super) fn new(token_source: &'t dyn TokenSource) -> Parser<'t> {\n-        Parser { token_source, token_pos: 0, events: Vec::new(), steps: Cell::new(0) }\n+    pub(super) fn new(token_source: &'t mut dyn TokenSource) -> Parser<'t> {\n+        Parser { token_source, events: Vec::new(), steps: Cell::new(0) }\n     }\n \n     pub(crate) fn finish(self) -> Vec<Event> {\n@@ -49,7 +48,7 @@ impl<'t> Parser<'t> {\n         let c1 = self.nth(0);\n         let c2 = self.nth(1);\n \n-        if self.token_source.is_token_joint_to_next(self.token_pos) {\n+        if self.token_source.current().is_jointed_to_next {\n             Some((c1, c2))\n         } else {\n             None\n@@ -64,8 +63,8 @@ impl<'t> Parser<'t> {\n         let c1 = self.nth(0);\n         let c2 = self.nth(1);\n         let c3 = self.nth(2);\n-        if self.token_source.is_token_joint_to_next(self.token_pos)\n-            && self.token_source.is_token_joint_to_next(self.token_pos + 1)\n+        if self.token_source.current().is_jointed_to_next\n+            && self.token_source.lookahead_nth(1).is_jointed_to_next\n         {\n             Some((c1, c2, c3))\n         } else {\n@@ -76,6 +75,8 @@ impl<'t> Parser<'t> {\n     /// Lookahead operation: returns the kind of the next nth\n     /// token.\n     pub(crate) fn nth(&self, n: usize) -> SyntaxKind {\n+        assert!(n <= 3);\n+\n         let steps = self.steps.get();\n         assert!(steps <= 10_000_000, \"the parser seems stuck\");\n         self.steps.set(steps + 1);\n@@ -86,7 +87,7 @@ impl<'t> Parser<'t> {\n         let mut i = 0;\n \n         loop {\n-            let mut kind = self.token_source.token_kind(self.token_pos + i);\n+            let mut kind = self.token_source.lookahead_nth(i).kind;\n             if let Some((composited, step)) = self.is_composite(kind, i) {\n                 kind = composited;\n                 i += step;\n@@ -115,7 +116,7 @@ impl<'t> Parser<'t> {\n \n     /// Checks if the current token is contextual keyword with text `t`.\n     pub(crate) fn at_contextual_kw(&self, kw: &str) -> bool {\n-        self.token_source.is_keyword(self.token_pos, kw)\n+        self.token_source.is_keyword(kw)\n     }\n \n     /// Starts a new node in the syntax tree. All nodes and tokens\n@@ -130,12 +131,12 @@ impl<'t> Parser<'t> {\n     /// Advances the parser by one token unconditionally\n     /// Mainly use in `token_tree` parsing\n     pub(crate) fn bump_raw(&mut self) {\n-        let mut kind = self.token_source.token_kind(self.token_pos);\n+        let mut kind = self.token_source.current().kind;\n \n         // Skip dollars, do_bump will eat these later\n         let mut i = 0;\n         while kind == SyntaxKind::L_DOLLAR || kind == SyntaxKind::R_DOLLAR {\n-            kind = self.token_source.token_kind(self.token_pos + i);\n+            kind = self.token_source.lookahead_nth(i).kind;\n             i += 1;\n         }\n \n@@ -236,7 +237,11 @@ impl<'t> Parser<'t> {\n \n     fn do_bump(&mut self, kind: SyntaxKind, n_raw_tokens: u8) {\n         self.eat_dollars();\n-        self.token_pos += usize::from(n_raw_tokens);\n+\n+        for _ in 0..n_raw_tokens {\n+            self.token_source.bump();\n+        }\n+\n         self.push_event(Event::Token { kind, n_raw_tokens });\n     }\n \n@@ -249,10 +254,14 @@ impl<'t> Parser<'t> {\n         // We assume the dollars will not occuried between\n         // mult-byte tokens\n \n-        let jn1 = self.token_source.is_token_joint_to_next(self.token_pos + n);\n-        let la2 = self.token_source.token_kind(self.token_pos + n + 1);\n-        let jn2 = self.token_source.is_token_joint_to_next(self.token_pos + n + 1);\n-        let la3 = self.token_source.token_kind(self.token_pos + n + 2);\n+        let first = self.token_source.lookahead_nth(n);\n+        let second = self.token_source.lookahead_nth(n + 1);\n+        let third = self.token_source.lookahead_nth(n + 2);\n+\n+        let jn1 = first.is_jointed_to_next;\n+        let la2 = second.kind;\n+        let jn2 = second.is_jointed_to_next;\n+        let la3 = third.kind;\n \n         match kind {\n             T![.] if jn1 && la2 == T![.] && jn2 && la3 == T![.] => Some((T![...], 3)),\n@@ -271,9 +280,9 @@ impl<'t> Parser<'t> {\n \n     fn eat_dollars(&mut self) {\n         loop {\n-            match self.token_source.token_kind(self.token_pos) {\n+            match self.token_source.current().kind {\n                 k @ SyntaxKind::L_DOLLAR | k @ SyntaxKind::R_DOLLAR => {\n-                    self.token_pos += 1;\n+                    self.token_source.bump();\n                     self.push_event(Event::Token { kind: k, n_raw_tokens: 1 });\n                 }\n                 _ => {\n@@ -286,9 +295,9 @@ impl<'t> Parser<'t> {\n     pub(crate) fn eat_l_dollars(&mut self) -> usize {\n         let mut ate_count = 0;\n         loop {\n-            match self.token_source.token_kind(self.token_pos) {\n+            match self.token_source.current().kind {\n                 k @ SyntaxKind::L_DOLLAR => {\n-                    self.token_pos += 1;\n+                    self.token_source.bump();\n                     self.push_event(Event::Token { kind: k, n_raw_tokens: 1 });\n                     ate_count += 1;\n                 }\n@@ -302,9 +311,9 @@ impl<'t> Parser<'t> {\n     pub(crate) fn eat_r_dollars(&mut self, max_count: usize) -> usize {\n         let mut ate_count = 0;\n         loop {\n-            match self.token_source.token_kind(self.token_pos) {\n+            match self.token_source.current().kind {\n                 k @ SyntaxKind::R_DOLLAR => {\n-                    self.token_pos += 1;\n+                    self.token_source.bump();\n                     self.push_event(Event::Token { kind: k, n_raw_tokens: 1 });\n                     ate_count += 1;\n \n@@ -320,12 +329,12 @@ impl<'t> Parser<'t> {\n     }\n \n     pub(crate) fn at_l_dollar(&self) -> bool {\n-        let kind = self.token_source.token_kind(self.token_pos);\n+        let kind = self.token_source.current().kind;\n         (kind == SyntaxKind::L_DOLLAR)\n     }\n \n     pub(crate) fn at_r_dollar(&self) -> bool {\n-        let kind = self.token_source.token_kind(self.token_pos);\n+        let kind = self.token_source.current().kind;\n         (kind == SyntaxKind::R_DOLLAR)\n     }\n }"}, {"sha": "4c1fa6c4f375acacc3736940906a1b06ba522a09", "filename": "crates/ra_syntax/src/parsing.rs", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/rust-lang/rust/blob/fcb1eef3232c3fc673bf5f98595708e108c3950c/crates%2Fra_syntax%2Fsrc%2Fparsing.rs", "raw_url": "https://github.com/rust-lang/rust/raw/fcb1eef3232c3fc673bf5f98595708e108c3950c/crates%2Fra_syntax%2Fsrc%2Fparsing.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fra_syntax%2Fsrc%2Fparsing.rs?ref=fcb1eef3232c3fc673bf5f98595708e108c3950c", "patch": "@@ -17,8 +17,8 @@ pub(crate) use self::reparsing::incremental_reparse;\n \n pub(crate) fn parse_text(text: &str) -> (GreenNode, Vec<SyntaxError>) {\n     let tokens = tokenize(&text);\n-    let token_source = text_token_source::TextTokenSource::new(text, &tokens);\n+    let mut token_source = text_token_source::TextTokenSource::new(text, &tokens);\n     let mut tree_sink = text_tree_sink::TextTreeSink::new(text, &tokens);\n-    ra_parser::parse(&token_source, &mut tree_sink);\n+    ra_parser::parse(&mut token_source, &mut tree_sink);\n     tree_sink.finish()\n }"}, {"sha": "3b6687f61b4c5ee4ec93f6f27920d7566496b9ac", "filename": "crates/ra_syntax/src/parsing/reparsing.rs", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/rust-lang/rust/blob/fcb1eef3232c3fc673bf5f98595708e108c3950c/crates%2Fra_syntax%2Fsrc%2Fparsing%2Freparsing.rs", "raw_url": "https://github.com/rust-lang/rust/raw/fcb1eef3232c3fc673bf5f98595708e108c3950c/crates%2Fra_syntax%2Fsrc%2Fparsing%2Freparsing.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fra_syntax%2Fsrc%2Fparsing%2Freparsing.rs?ref=fcb1eef3232c3fc673bf5f98595708e108c3950c", "patch": "@@ -85,9 +85,9 @@ fn reparse_block<'node>(\n     if !is_balanced(&tokens) {\n         return None;\n     }\n-    let token_source = TextTokenSource::new(&text, &tokens);\n+    let mut token_source = TextTokenSource::new(&text, &tokens);\n     let mut tree_sink = TextTreeSink::new(&text, &tokens);\n-    reparser.parse(&token_source, &mut tree_sink);\n+    reparser.parse(&mut token_source, &mut tree_sink);\n     let (green, new_errors) = tree_sink.finish();\n     Some((node.replace_with(green), new_errors, node.range()))\n }"}, {"sha": "71d2947f775740f4b49a315b2285dfe1de5d5d04", "filename": "crates/ra_syntax/src/parsing/text_token_source.rs", "status": "modified", "additions": 34, "deletions": 13, "changes": 47, "blob_url": "https://github.com/rust-lang/rust/blob/fcb1eef3232c3fc673bf5f98595708e108c3950c/crates%2Fra_syntax%2Fsrc%2Fparsing%2Ftext_token_source.rs", "raw_url": "https://github.com/rust-lang/rust/raw/fcb1eef3232c3fc673bf5f98595708e108c3950c/crates%2Fra_syntax%2Fsrc%2Fparsing%2Ftext_token_source.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fra_syntax%2Fsrc%2Fparsing%2Ftext_token_source.rs?ref=fcb1eef3232c3fc673bf5f98595708e108c3950c", "patch": "@@ -1,7 +1,8 @@\n use ra_parser::TokenSource;\n+use ra_parser::Token as PToken;\n \n use crate::{\n-    SyntaxKind, SyntaxKind::EOF, TextRange, TextUnit,\n+    SyntaxKind::EOF, TextRange, TextUnit,\n     parsing::lexer::Token,\n };\n \n@@ -23,31 +24,50 @@ pub(crate) struct TextTokenSource<'t> {\n     /// ```\n     /// tokens: `[struct, Foo, {, }]`\n     tokens: Vec<Token>,\n+\n+    /// Current token and position\n+    curr: (PToken, usize),\n }\n \n impl<'t> TokenSource for TextTokenSource<'t> {\n-    fn token_kind(&self, pos: usize) -> SyntaxKind {\n-        if !(pos < self.tokens.len()) {\n-            return EOF;\n-        }\n-        self.tokens[pos].kind\n+    fn current(&self) -> PToken {\n+        return self.curr.0;\n     }\n-    fn is_token_joint_to_next(&self, pos: usize) -> bool {\n-        if !(pos + 1 < self.tokens.len()) {\n-            return true;\n+\n+    fn lookahead_nth(&self, n: usize) -> PToken {\n+        mk_token(self.curr.1 + n, &self.start_offsets, &self.tokens)\n+    }\n+\n+    fn bump(&mut self) {\n+        if self.curr.0.kind == EOF {\n+            return;\n         }\n-        self.start_offsets[pos] + self.tokens[pos].len == self.start_offsets[pos + 1]\n+\n+        let pos = self.curr.1 + 1;\n+        self.curr = (mk_token(pos, &self.start_offsets, &self.tokens), pos);\n     }\n-    fn is_keyword(&self, pos: usize, kw: &str) -> bool {\n+\n+    fn is_keyword(&self, kw: &str) -> bool {\n+        let pos = self.curr.1;\n         if !(pos < self.tokens.len()) {\n             return false;\n         }\n         let range = TextRange::offset_len(self.start_offsets[pos], self.tokens[pos].len);\n-\n         self.text[range] == *kw\n     }\n }\n \n+fn mk_token(pos: usize, start_offsets: &[TextUnit], tokens: &[Token]) -> PToken {\n+    let kind = tokens.get(pos).map(|t| t.kind).unwrap_or(EOF);\n+    let is_jointed_to_next = if pos + 1 < start_offsets.len() {\n+        start_offsets[pos] + tokens[pos].len == start_offsets[pos + 1]\n+    } else {\n+        false\n+    };\n+\n+    PToken { kind, is_jointed_to_next }\n+}\n+\n impl<'t> TextTokenSource<'t> {\n     /// Generate input from tokens(expect comment and whitespace).\n     pub fn new(text: &'t str, raw_tokens: &'t [Token]) -> TextTokenSource<'t> {\n@@ -62,6 +82,7 @@ impl<'t> TextTokenSource<'t> {\n             len += token.len;\n         }\n \n-        TextTokenSource { text, start_offsets, tokens }\n+        let first = mk_token(0, &start_offsets, &tokens);\n+        TextTokenSource { text, start_offsets, tokens, curr: (first, 0) }\n     }\n }"}]}