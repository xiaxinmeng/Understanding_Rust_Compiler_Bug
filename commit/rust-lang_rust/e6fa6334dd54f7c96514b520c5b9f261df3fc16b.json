{"sha": "e6fa6334dd54f7c96514b520c5b9f261df3fc16b", "node_id": "MDY6Q29tbWl0NzI0NzEyOmU2ZmE2MzM0ZGQ1NGY3Yzk2NTE0YjUyMGM1YjlmMjYxZGYzZmMxNmI=", "commit": {"author": {"name": "Aaron Hill", "email": "aa1ronham@gmail.com", "date": "2020-12-12T20:20:22Z"}, "committer": {"name": "Aaron Hill", "email": "aa1ronham@gmail.com", "date": "2020-12-12T21:28:13Z"}, "message": "Properly capture trailing 'unglued' token\n\nIf we try to capture the `Vec<u8>` in `Option<Vec<u8>>`, we'll\nneed to capture a `>` token which was 'unglued' from a `>>` token.\nThe processing of unglueing a token for parsing purposes bypasses the\nusual capturing infrastructure, so we currently lose the trailing `>`.\nAs a result, we fall back to the reparsed `TokenStream`, causing us to\nlose spans.\n\nThis commit makes token capturing keep track of a trailing 'unglued'\ntoken. Note that we don't need to care about unglueing except at the end\nof the captured tokens - if we capture both the first and second unglued\ntokens, then we'll end up capturing the full 'glued' token, which\nalready works correctly.", "tree": {"sha": "8561fd33a725cedad047218ea2741deaff324a04", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/8561fd33a725cedad047218ea2741deaff324a04"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/e6fa6334dd54f7c96514b520c5b9f261df3fc16b", "comment_count": 0, "verification": {"verified": true, "reason": "valid", "signature": "-----BEGIN PGP SIGNATURE-----\n\niQIzBAABCAAdFiEE7J9Gc3TfBwj2K399tAh+UQ6YsWQFAl/VNfAACgkQtAh+UQ6Y\nsWQqvRAAjvQ/EfpbNkyKMyUKOl70XOJvCLSkunZYtiYZtWCqOOsU+KuZ+XbAZL4e\nTZQdMqq6Y+hGPvxWJrwZ0sI/dr0wFN6RzuwUeKDHNy0HPR7CyHL55XoQJMD13Nmb\nWCNkNY9mHDtc1m12rMHE5YKImNFTHv670Q3KQvE4ImiqfDVEg9Z5JFDjbK0nnqXf\nm1Vmx/p9KGK3zjmqj2CrciQG3YcK9f+QZUltQgBrAAOtxRuIMGA37DLkmEsVRM9L\nI/6ezGI7adSOESTlzppcgUDnQyeMmteVzsXkt0YO91maIFmY97vXjd3Y31azm035\nIOzASB9xeKTCbbYe+5Qoo6vamt0spGyBwJPPt9LAiKg1rcZNt/vroyFyBti/xZ1g\ncRPXXb7XCGbO65JI5k+qflnyoqzt1tTnVFOMY9+b7RuepciX+Mju2PtDwD8h8P0T\nrWATU9acnjcxhKMmnNWxQb2SOOQbh3243n79aOvb+GgqhuUmDOhOlv6ZrbR9JdjI\n76il8TEUB3rrHzqOEesa55sqN8Bahpf8XkMQavfEejVnE3jRJxD8zumT+alGqh8C\n0Av8EyDGwGQCHLG23ONnzRdHbGrfZw0nvANFOG/rXHADV46ppItb3Y1kS3t3movh\nf37da81YeKN6psO8LpFecsHTFdrs6NbtxZ5azq/G+ZjHXQYi2y0=\n=SggS\n-----END PGP SIGNATURE-----", "payload": "tree 8561fd33a725cedad047218ea2741deaff324a04\nparent 388eb24b6c479088a83c1b094d79221a32dfc7ff\nauthor Aaron Hill <aa1ronham@gmail.com> 1607804422 -0500\ncommitter Aaron Hill <aa1ronham@gmail.com> 1607808493 -0500\n\nProperly capture trailing 'unglued' token\n\nIf we try to capture the `Vec<u8>` in `Option<Vec<u8>>`, we'll\nneed to capture a `>` token which was 'unglued' from a `>>` token.\nThe processing of unglueing a token for parsing purposes bypasses the\nusual capturing infrastructure, so we currently lose the trailing `>`.\nAs a result, we fall back to the reparsed `TokenStream`, causing us to\nlose spans.\n\nThis commit makes token capturing keep track of a trailing 'unglued'\ntoken. Note that we don't need to care about unglueing except at the end\nof the captured tokens - if we capture both the first and second unglued\ntokens, then we'll end up capturing the full 'glued' token, which\nalready works correctly.\n"}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/e6fa6334dd54f7c96514b520c5b9f261df3fc16b", "html_url": "https://github.com/rust-lang/rust/commit/e6fa6334dd54f7c96514b520c5b9f261df3fc16b", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/e6fa6334dd54f7c96514b520c5b9f261df3fc16b/comments", "author": {"login": "Aaron1011", "id": 1408859, "node_id": "MDQ6VXNlcjE0MDg4NTk=", "avatar_url": "https://avatars.githubusercontent.com/u/1408859?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Aaron1011", "html_url": "https://github.com/Aaron1011", "followers_url": "https://api.github.com/users/Aaron1011/followers", "following_url": "https://api.github.com/users/Aaron1011/following{/other_user}", "gists_url": "https://api.github.com/users/Aaron1011/gists{/gist_id}", "starred_url": "https://api.github.com/users/Aaron1011/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Aaron1011/subscriptions", "organizations_url": "https://api.github.com/users/Aaron1011/orgs", "repos_url": "https://api.github.com/users/Aaron1011/repos", "events_url": "https://api.github.com/users/Aaron1011/events{/privacy}", "received_events_url": "https://api.github.com/users/Aaron1011/received_events", "type": "User", "site_admin": false}, "committer": {"login": "Aaron1011", "id": 1408859, "node_id": "MDQ6VXNlcjE0MDg4NTk=", "avatar_url": "https://avatars.githubusercontent.com/u/1408859?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Aaron1011", "html_url": "https://github.com/Aaron1011", "followers_url": "https://api.github.com/users/Aaron1011/followers", "following_url": "https://api.github.com/users/Aaron1011/following{/other_user}", "gists_url": "https://api.github.com/users/Aaron1011/gists{/gist_id}", "starred_url": "https://api.github.com/users/Aaron1011/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Aaron1011/subscriptions", "organizations_url": "https://api.github.com/users/Aaron1011/orgs", "repos_url": "https://api.github.com/users/Aaron1011/repos", "events_url": "https://api.github.com/users/Aaron1011/events{/privacy}", "received_events_url": "https://api.github.com/users/Aaron1011/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "388eb24b6c479088a83c1b094d79221a32dfc7ff", "url": "https://api.github.com/repos/rust-lang/rust/commits/388eb24b6c479088a83c1b094d79221a32dfc7ff", "html_url": "https://github.com/rust-lang/rust/commit/388eb24b6c479088a83c1b094d79221a32dfc7ff"}], "stats": {"total": 115, "additions": 106, "deletions": 9}, "files": [{"sha": "d51a0fcbf09e4fbd2fe053487bfd6c4fcb3573eb", "filename": "compiler/rustc_parse/src/parser/mod.rs", "status": "modified", "additions": 58, "deletions": 9, "changes": 67, "blob_url": "https://github.com/rust-lang/rust/blob/e6fa6334dd54f7c96514b520c5b9f261df3fc16b/compiler%2Frustc_parse%2Fsrc%2Fparser%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/e6fa6334dd54f7c96514b520c5b9f261df3fc16b/compiler%2Frustc_parse%2Fsrc%2Fparser%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_parse%2Fsrc%2Fparser%2Fmod.rs?ref=e6fa6334dd54f7c96514b520c5b9f261df3fc16b", "patch": "@@ -17,7 +17,7 @@ pub use path::PathStyle;\n use rustc_ast::ptr::P;\n use rustc_ast::token::{self, DelimToken, Token, TokenKind};\n use rustc_ast::tokenstream::{self, DelimSpan, LazyTokenStream, Spacing};\n-use rustc_ast::tokenstream::{CreateTokenStream, TokenStream, TokenTree};\n+use rustc_ast::tokenstream::{CreateTokenStream, TokenStream, TokenTree, TreeAndSpacing};\n use rustc_ast::DUMMY_NODE_ID;\n use rustc_ast::{self as ast, AnonConst, AttrStyle, AttrVec, Const, CrateSugar, Extern, Unsafe};\n use rustc_ast::{Async, Expr, ExprKind, MacArgs, MacDelimiter, Mutability, StrLit};\n@@ -132,6 +132,28 @@ struct TokenCursor {\n     // Counts the number of calls to `next` or `next_desugared`,\n     // depending on whether `desugar_doc_comments` is set.\n     num_next_calls: usize,\n+    // During parsing, we may sometimes need to 'unglue' a\n+    // glued token into two component tokens\n+    // (e.g. '>>' into '>' and '>), so that the parser\n+    // can consume them one at a time. This process\n+    // bypasses the normal capturing mechanism\n+    // (e.g. `num_next_calls` will not be incremented),\n+    // since the 'unglued' tokens due not exist in\n+    // the original `TokenStream`.\n+    //\n+    // If we end up consuming both unglued tokens,\n+    // then this is not an issue - we'll end up\n+    // capturing the single 'glued' token.\n+    //\n+    // However, in certain circumstances, we may\n+    // want to capture just the first 'unglued' token.\n+    // For example, capturing the `Vec<u8>`\n+    // in `Option<Vec<u8>>` requires us to unglue\n+    // the trailing `>>` token. The `append_unglued_token`\n+    // field is used to track this token - it gets\n+    // appended to the captured stream when\n+    // we evaluate a `LazyTokenStream`\n+    append_unglued_token: Option<TreeAndSpacing>,\n }\n \n #[derive(Clone)]\n@@ -336,6 +358,7 @@ impl<'a> Parser<'a> {\n                 stack: Vec::new(),\n                 num_next_calls: 0,\n                 desugar_doc_comments,\n+                append_unglued_token: None,\n             },\n             desugar_doc_comments,\n             unmatched_angle_bracket_count: 0,\n@@ -359,6 +382,10 @@ impl<'a> Parser<'a> {\n             self.token_cursor.next()\n         };\n         self.token_cursor.num_next_calls += 1;\n+        // We've retrieved an token from the underlying\n+        // cursor, so we no longer need to worry about\n+        // an unglued token. See `break_and_eat` for more details\n+        self.token_cursor.append_unglued_token = None;\n         if next.span.is_dummy() {\n             // Tweak the location for better diagnostics, but keep syntactic context intact.\n             next.span = fallback_span.with_ctxt(next.span.ctxt());\n@@ -555,6 +582,14 @@ impl<'a> Parser<'a> {\n                 let first_span = self.sess.source_map().start_point(self.token.span);\n                 let second_span = self.token.span.with_lo(first_span.hi());\n                 self.token = Token::new(first, first_span);\n+                // Keep track of this token - if we end token capturing now,\n+                // we'll want to append this token to the captured stream.\n+                //\n+                // If we consume any additional tokens, then this token\n+                // is not needed (we'll capture the entire 'glued' token),\n+                // and `next_tok` will set this field to `None`\n+                self.token_cursor.append_unglued_token =\n+                    Some((TokenTree::Token(self.token.clone()), Spacing::Alone));\n                 // Use the spacing of the glued token as the spacing\n                 // of the unglued second token.\n                 self.bump_with((Token::new(second, second_span), self.token_spacing));\n@@ -1230,6 +1265,7 @@ impl<'a> Parser<'a> {\n             num_calls: usize,\n             desugar_doc_comments: bool,\n             trailing_semi: bool,\n+            append_unglued_token: Option<TreeAndSpacing>,\n         }\n         impl CreateTokenStream for LazyTokenStreamImpl {\n             fn create_token_stream(&self) -> TokenStream {\n@@ -1253,12 +1289,18 @@ impl<'a> Parser<'a> {\n                     }))\n                     .take(num_calls);\n \n-                make_token_stream(tokens)\n+                make_token_stream(tokens, self.append_unglued_token.clone())\n             }\n             fn add_trailing_semi(&self) -> Box<dyn CreateTokenStream> {\n                 if self.trailing_semi {\n                     panic!(\"Called `add_trailing_semi` twice!\");\n                 }\n+                if self.append_unglued_token.is_some() {\n+                    panic!(\n+                        \"Cannot call `add_trailing_semi` when we have an unglued token {:?}\",\n+                        self.append_unglued_token\n+                    );\n+                }\n                 let mut new = self.clone();\n                 new.trailing_semi = true;\n                 Box::new(new)\n@@ -1271,6 +1313,7 @@ impl<'a> Parser<'a> {\n             cursor_snapshot,\n             desugar_doc_comments: self.desugar_doc_comments,\n             trailing_semi: false,\n+            append_unglued_token: self.token_cursor.append_unglued_token.clone(),\n         };\n         Ok((ret, Some(LazyTokenStream::new(lazy_impl))))\n     }\n@@ -1325,7 +1368,10 @@ pub fn emit_unclosed_delims(unclosed_delims: &mut Vec<UnmatchedBrace>, sess: &Pa\n /// Converts a flattened iterator of tokens (including open and close delimiter tokens)\n /// into a `TokenStream`, creating a `TokenTree::Delimited` for each matching pair\n /// of open and close delims.\n-fn make_token_stream(tokens: impl Iterator<Item = (Token, Spacing)>) -> TokenStream {\n+fn make_token_stream(\n+    tokens: impl Iterator<Item = (Token, Spacing)>,\n+    append_unglued_token: Option<TreeAndSpacing>,\n+) -> TokenStream {\n     #[derive(Debug)]\n     struct FrameData {\n         open: Span,\n@@ -1348,14 +1394,17 @@ fn make_token_stream(tokens: impl Iterator<Item = (Token, Spacing)>) -> TokenStr\n                     .inner\n                     .push((delimited, Spacing::Alone));\n             }\n-            token => stack\n-                .last_mut()\n-                .expect(\"Bottom token frame is missing!\")\n-                .inner\n-                .push((TokenTree::Token(token), spacing)),\n+            token => {\n+                stack\n+                    .last_mut()\n+                    .expect(\"Bottom token frame is missing!\")\n+                    .inner\n+                    .push((TokenTree::Token(token), spacing));\n+            }\n         }\n     }\n-    let final_buf = stack.pop().expect(\"Missing final buf!\");\n+    let mut final_buf = stack.pop().expect(\"Missing final buf!\");\n+    final_buf.inner.extend(append_unglued_token);\n     assert!(stack.is_empty(), \"Stack should be empty: final_buf={:?} stack={:?}\", final_buf, stack);\n     TokenStream::new(final_buf.inner)\n }"}, {"sha": "727b779776b9b4502ad685211e75a83a8e06a857", "filename": "src/test/ui/proc-macro/capture-unglued-token.rs", "status": "added", "additions": 20, "deletions": 0, "changes": 20, "blob_url": "https://github.com/rust-lang/rust/blob/e6fa6334dd54f7c96514b520c5b9f261df3fc16b/src%2Ftest%2Fui%2Fproc-macro%2Fcapture-unglued-token.rs", "raw_url": "https://github.com/rust-lang/rust/raw/e6fa6334dd54f7c96514b520c5b9f261df3fc16b/src%2Ftest%2Fui%2Fproc-macro%2Fcapture-unglued-token.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ftest%2Fui%2Fproc-macro%2Fcapture-unglued-token.rs?ref=e6fa6334dd54f7c96514b520c5b9f261df3fc16b", "patch": "@@ -0,0 +1,20 @@\n+// aux-build:test-macros.rs\n+// compile-flags: -Z span-debug\n+// check-pass\n+\n+// Tests that we properly handle parsing a nonterminal\n+// where we have two consecutive angle brackets (one inside\n+// the nonterminal, and one outside)\n+\n+#![no_std] // Don't load unnecessary hygiene information from std\n+extern crate std;\n+extern crate test_macros;\n+\n+macro_rules! trailing_angle {\n+    (Option<$field:ty>) => {\n+        test_macros::print_bang_consume!($field);\n+    }\n+}\n+\n+trailing_angle!(Option<Vec<u8>>);\n+fn main() {}"}, {"sha": "7e6b540332c7963c95654412c6378c84e7d01400", "filename": "src/test/ui/proc-macro/capture-unglued-token.stdout", "status": "added", "additions": 28, "deletions": 0, "changes": 28, "blob_url": "https://github.com/rust-lang/rust/blob/e6fa6334dd54f7c96514b520c5b9f261df3fc16b/src%2Ftest%2Fui%2Fproc-macro%2Fcapture-unglued-token.stdout", "raw_url": "https://github.com/rust-lang/rust/raw/e6fa6334dd54f7c96514b520c5b9f261df3fc16b/src%2Ftest%2Fui%2Fproc-macro%2Fcapture-unglued-token.stdout", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ftest%2Fui%2Fproc-macro%2Fcapture-unglued-token.stdout?ref=e6fa6334dd54f7c96514b520c5b9f261df3fc16b", "patch": "@@ -0,0 +1,28 @@\n+PRINT-BANG INPUT (DISPLAY): Vec<u8>\n+PRINT-BANG RE-COLLECTED (DISPLAY): Vec < u8 >\n+PRINT-BANG INPUT (DEBUG): TokenStream [\n+    Group {\n+        delimiter: None,\n+        stream: TokenStream [\n+            Ident {\n+                ident: \"Vec\",\n+                span: $DIR/capture-unglued-token.rs:19:24: 19:27 (#0),\n+            },\n+            Punct {\n+                ch: '<',\n+                spacing: Alone,\n+                span: $DIR/capture-unglued-token.rs:19:27: 19:28 (#0),\n+            },\n+            Ident {\n+                ident: \"u8\",\n+                span: $DIR/capture-unglued-token.rs:19:28: 19:30 (#0),\n+            },\n+            Punct {\n+                ch: '>',\n+                spacing: Alone,\n+                span: $DIR/capture-unglued-token.rs:19:30: 19:31 (#0),\n+            },\n+        ],\n+        span: $DIR/capture-unglued-token.rs:15:42: 15:48 (#4),\n+    },\n+]"}]}