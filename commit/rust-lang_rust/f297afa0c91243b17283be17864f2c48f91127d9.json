{"sha": "f297afa0c91243b17283be17864f2c48f91127d9", "node_id": "C_kwDOAAsO6NoAKGYyOTdhZmEwYzkxMjQzYjE3MjgzYmUxNzg2NGYyYzQ4ZjkxMTI3ZDk", "commit": {"author": {"name": "Lukas Bergdoll", "email": "lukas.bergdoll@gmail.com", "date": "2023-01-22T11:01:06Z"}, "committer": {"name": "Lukas Bergdoll", "email": "lukas.bergdoll@gmail.com", "date": "2023-01-22T11:01:06Z"}, "message": "Flip scanning direction of stable sort\n\nMemory pre-fetching prefers forward scanning vs backwards scanning, and the\ncode-gen is usually better. For the most sensitive types such as integers, these\nare planned to be merged bidirectionally at once. So there is no benefit in\nscanning backwards.\n\nThe largest perf gains are seen for full ascending and descending inputs, which\nsee 1.5x speedups. Random inputs benefit too, and some patterns can loose out,\nbut these losses are minimal.", "tree": {"sha": "517b578c6d2323bc1043bd703c4645737e74e8b5", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/517b578c6d2323bc1043bd703c4645737e74e8b5"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/f297afa0c91243b17283be17864f2c48f91127d9", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/f297afa0c91243b17283be17864f2c48f91127d9", "html_url": "https://github.com/rust-lang/rust/commit/f297afa0c91243b17283be17864f2c48f91127d9", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/f297afa0c91243b17283be17864f2c48f91127d9/comments", "author": {"login": "Voultapher", "id": 6864584, "node_id": "MDQ6VXNlcjY4NjQ1ODQ=", "avatar_url": "https://avatars.githubusercontent.com/u/6864584?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Voultapher", "html_url": "https://github.com/Voultapher", "followers_url": "https://api.github.com/users/Voultapher/followers", "following_url": "https://api.github.com/users/Voultapher/following{/other_user}", "gists_url": "https://api.github.com/users/Voultapher/gists{/gist_id}", "starred_url": "https://api.github.com/users/Voultapher/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Voultapher/subscriptions", "organizations_url": "https://api.github.com/users/Voultapher/orgs", "repos_url": "https://api.github.com/users/Voultapher/repos", "events_url": "https://api.github.com/users/Voultapher/events{/privacy}", "received_events_url": "https://api.github.com/users/Voultapher/received_events", "type": "User", "site_admin": false}, "committer": {"login": "Voultapher", "id": 6864584, "node_id": "MDQ6VXNlcjY4NjQ1ODQ=", "avatar_url": "https://avatars.githubusercontent.com/u/6864584?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Voultapher", "html_url": "https://github.com/Voultapher", "followers_url": "https://api.github.com/users/Voultapher/followers", "following_url": "https://api.github.com/users/Voultapher/following{/other_user}", "gists_url": "https://api.github.com/users/Voultapher/gists{/gist_id}", "starred_url": "https://api.github.com/users/Voultapher/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Voultapher/subscriptions", "organizations_url": "https://api.github.com/users/Voultapher/orgs", "repos_url": "https://api.github.com/users/Voultapher/repos", "events_url": "https://api.github.com/users/Voultapher/events{/privacy}", "received_events_url": "https://api.github.com/users/Voultapher/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "a3065a1a34fe1c0b85bdf3ff1f3d0bd470235e6b", "url": "https://api.github.com/repos/rust-lang/rust/commits/a3065a1a34fe1c0b85bdf3ff1f3d0bd470235e6b", "html_url": "https://github.com/rust-lang/rust/commit/a3065a1a34fe1c0b85bdf3ff1f3d0bd470235e6b"}], "stats": {"total": 112, "additions": 67, "deletions": 45}, "files": [{"sha": "227db51a0b4036a686caad2c4f4173ac35f0e354", "filename": "library/core/src/slice/sort.rs", "status": "modified", "additions": 67, "deletions": 45, "changes": 112, "blob_url": "https://github.com/rust-lang/rust/blob/f297afa0c91243b17283be17864f2c48f91127d9/library%2Fcore%2Fsrc%2Fslice%2Fsort.rs", "raw_url": "https://github.com/rust-lang/rust/raw/f297afa0c91243b17283be17864f2c48f91127d9/library%2Fcore%2Fsrc%2Fslice%2Fsort.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/library%2Fcore%2Fsrc%2Fslice%2Fsort.rs?ref=f297afa0c91243b17283be17864f2c48f91127d9", "patch": "@@ -1196,52 +1196,37 @@ pub fn merge_sort<T, CmpF, ElemAllocF, ElemDeallocF, RunAllocF, RunDeallocF>(\n \n     let mut runs = RunVec::new(run_alloc_fn, run_dealloc_fn);\n \n-    // In order to identify natural runs in `v`, we traverse it backwards. That might seem like a\n-    // strange decision, but consider the fact that merges more often go in the opposite direction\n-    // (forwards). According to benchmarks, merging forwards is slightly faster than merging\n-    // backwards. To conclude, identifying runs by traversing backwards improves performance.\n-    let mut end = len;\n-    while end > 0 {\n-        // Find the next natural run, and reverse it if it's strictly descending.\n-        let mut start = end - 1;\n-        if start > 0 {\n-            start -= 1;\n-\n-            // SAFETY: The v.get_unchecked must be fed with correct inbound indicies.\n-            unsafe {\n-                if is_less(v.get_unchecked(start + 1), v.get_unchecked(start)) {\n-                    while start > 0 && is_less(v.get_unchecked(start), v.get_unchecked(start - 1)) {\n-                        start -= 1;\n-                    }\n-                    v[start..end].reverse();\n-                } else {\n-                    while start > 0 && !is_less(v.get_unchecked(start), v.get_unchecked(start - 1))\n-                    {\n-                        start -= 1;\n-                    }\n-                }\n-            }\n+    let mut end = 0;\n+    let mut start = 0;\n+\n+    // Scan forward. Memory pre-fetching prefers forward scanning vs backwards scanning, and the\n+    // code-gen is usually better. For the most sensitive types such as integers, these are merged\n+    // bidirectionally at once. So there is no benefit in scanning backwards.\n+    while end < len {\n+        let (streak_end, was_reversed) = find_streak(&v[start..], is_less);\n+        end += streak_end;\n+        if was_reversed {\n+            v[start..end].reverse();\n         }\n \n         // Insert some more elements into the run if it's too short. Insertion sort is faster than\n         // merge sort on short sequences, so this significantly improves performance.\n-        start = provide_sorted_batch(v, start, end, is_less);\n+        end = provide_sorted_batch(v, start, end, is_less);\n \n         // Push this run onto the stack.\n         runs.push(TimSortRun { start, len: end - start });\n-        end = start;\n+        start = end;\n \n         // Merge some pairs of adjacent runs to satisfy the invariants.\n-        while let Some(r) = collapse(runs.as_slice()) {\n-            let left = runs[r + 1];\n-            let right = runs[r];\n-            // SAFETY: `buf_ptr` must hold enough capacity for the shorter of the two sides, and\n-            // neither side may be on length 0.\n+        while let Some(r) = collapse(runs.as_slice(), len) {\n+            let left = runs[r];\n+            let right = runs[r + 1];\n+            let merge_slice = &mut v[left.start..right.start + right.len];\n             unsafe {\n-                merge(&mut v[left.start..right.start + right.len], left.len, buf_ptr, is_less);\n+                merge(merge_slice, left.len, buf_ptr, is_less);\n             }\n-            runs[r] = TimSortRun { start: left.start, len: left.len + right.len };\n-            runs.remove(r + 1);\n+            runs[r + 1] = TimSortRun { start: left.start, len: left.len + right.len };\n+            runs.remove(r);\n         }\n     }\n \n@@ -1263,10 +1248,10 @@ pub fn merge_sort<T, CmpF, ElemAllocF, ElemDeallocF, RunAllocF, RunDeallocF>(\n     // run starts at index 0, it will always demand a merge operation until the stack is fully\n     // collapsed, in order to complete the sort.\n     #[inline]\n-    fn collapse(runs: &[TimSortRun]) -> Option<usize> {\n+    fn collapse(runs: &[TimSortRun], stop: usize) -> Option<usize> {\n         let n = runs.len();\n         if n >= 2\n-            && (runs[n - 1].start == 0\n+            && (runs[n - 1].start + runs[n - 1].len == stop\n                 || runs[n - 2].len <= runs[n - 1].len\n                 || (n >= 3 && runs[n - 3].len <= runs[n - 2].len + runs[n - 1].len)\n                 || (n >= 4 && runs[n - 4].len <= runs[n - 3].len + runs[n - 2].len))\n@@ -1454,33 +1439,70 @@ pub struct TimSortRun {\n     start: usize,\n }\n \n-/// Takes a range as denoted by start and end, that is already sorted and extends it to the left if\n+/// Takes a range as denoted by start and end, that is already sorted and extends it to the right if\n /// necessary with sorts optimized for smaller ranges such as insertion sort.\n #[cfg(not(no_global_oom_handling))]\n-fn provide_sorted_batch<T, F>(v: &mut [T], mut start: usize, end: usize, is_less: &mut F) -> usize\n+fn provide_sorted_batch<T, F>(v: &mut [T], start: usize, mut end: usize, is_less: &mut F) -> usize\n where\n     F: FnMut(&T, &T) -> bool,\n {\n-    debug_assert!(end > start);\n+    let len = v.len();\n+    assert!(end >= start && end <= len);\n \n     // This value is a balance between least comparisons and best performance, as\n     // influenced by for example cache locality.\n     const MIN_INSERTION_RUN: usize = 10;\n \n     // Insert some more elements into the run if it's too short. Insertion sort is faster than\n     // merge sort on short sequences, so this significantly improves performance.\n-    let start_found = start;\n     let start_end_diff = end - start;\n \n-    if start_end_diff < MIN_INSERTION_RUN && start != 0 {\n+    if start_end_diff < MIN_INSERTION_RUN && end < len {\n         // v[start_found..end] are elements that are already sorted in the input. We want to extend\n         // the sorted region to the left, so we push up MIN_INSERTION_RUN - 1 to the right. Which is\n         // more efficient that trying to push those already sorted elements to the left.\n+        end = cmp::min(start + MIN_INSERTION_RUN, len);\n+        let presorted_start = cmp::max(start_end_diff, 1);\n \n-        start = if end >= MIN_INSERTION_RUN { end - MIN_INSERTION_RUN } else { 0 };\n+        insertion_sort_shift_left(&mut v[start..end], presorted_start, is_less);\n+    }\n \n-        insertion_sort_shift_right(&mut v[start..end], start_found - start, is_less);\n+    end\n+}\n+\n+/// Finds a streak of presorted elements starting at the beginning of the slice. Returns the first\n+/// value that is not part of said streak, and a bool denoting wether the streak was reversed.\n+/// Streaks can be increasing or decreasing.\n+fn find_streak<T, F>(v: &[T], is_less: &mut F) -> (usize, bool)\n+where\n+    F: FnMut(&T, &T) -> bool,\n+{\n+    let len = v.len();\n+\n+    if len < 2 {\n+        return (len, false);\n     }\n \n-    start\n+    let mut end = 2;\n+\n+    // SAFETY: See below specific.\n+    unsafe {\n+        // SAFETY: We checked that len >= 2, so 0 and 1 are valid indices.\n+        let assume_reverse = is_less(v.get_unchecked(1), v.get_unchecked(0));\n+\n+        // SAFETY: We know end >= 2 and check end < len.\n+        // From that follows that accessing v at end and end - 1 is safe.\n+        if assume_reverse {\n+            while end < len && is_less(v.get_unchecked(end), v.get_unchecked(end - 1)) {\n+                end += 1;\n+            }\n+\n+            (end, true)\n+        } else {\n+            while end < len && !is_less(v.get_unchecked(end), v.get_unchecked(end - 1)) {\n+                end += 1;\n+            }\n+            (end, false)\n+        }\n+    }\n }"}]}