{"sha": "6d6e831c3398d99ffcaba3af4335e411535c68bb", "node_id": "MDY6Q29tbWl0NzI0NzEyOjZkNmU4MzFjMzM5OGQ5OWZmY2FiYTNhZjQzMzVlNDExNTM1YzY4YmI=", "commit": {"author": {"name": "Simonas Kazlauskas", "email": "github@kazlauskas.me", "date": "2016-01-11T19:17:52Z"}, "committer": {"name": "Simonas Kazlauskas", "email": "github@kazlauskas.me", "date": "2016-01-11T19:17:52Z"}, "message": "Rollup merge of #30694 - pnkfelix:issue-25658-real-first-follow, r=nrc\n\nProper first and follow sets for macro_rules future proofing\n\nimplements first stage of RFC amendment 1384; see #30450", "tree": {"sha": "f48e6f70a07860958298b58c393a742e39434550", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/f48e6f70a07860958298b58c393a742e39434550"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/6d6e831c3398d99ffcaba3af4335e411535c68bb", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/6d6e831c3398d99ffcaba3af4335e411535c68bb", "html_url": "https://github.com/rust-lang/rust/commit/6d6e831c3398d99ffcaba3af4335e411535c68bb", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/6d6e831c3398d99ffcaba3af4335e411535c68bb/comments", "author": {"login": "nagisa", "id": 679122, "node_id": "MDQ6VXNlcjY3OTEyMg==", "avatar_url": "https://avatars.githubusercontent.com/u/679122?v=4", "gravatar_id": "", "url": "https://api.github.com/users/nagisa", "html_url": "https://github.com/nagisa", "followers_url": "https://api.github.com/users/nagisa/followers", "following_url": "https://api.github.com/users/nagisa/following{/other_user}", "gists_url": "https://api.github.com/users/nagisa/gists{/gist_id}", "starred_url": "https://api.github.com/users/nagisa/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/nagisa/subscriptions", "organizations_url": "https://api.github.com/users/nagisa/orgs", "repos_url": "https://api.github.com/users/nagisa/repos", "events_url": "https://api.github.com/users/nagisa/events{/privacy}", "received_events_url": "https://api.github.com/users/nagisa/received_events", "type": "User", "site_admin": false}, "committer": {"login": "nagisa", "id": 679122, "node_id": "MDQ6VXNlcjY3OTEyMg==", "avatar_url": "https://avatars.githubusercontent.com/u/679122?v=4", "gravatar_id": "", "url": "https://api.github.com/users/nagisa", "html_url": "https://github.com/nagisa", "followers_url": "https://api.github.com/users/nagisa/followers", "following_url": "https://api.github.com/users/nagisa/following{/other_user}", "gists_url": "https://api.github.com/users/nagisa/gists{/gist_id}", "starred_url": "https://api.github.com/users/nagisa/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/nagisa/subscriptions", "organizations_url": "https://api.github.com/users/nagisa/orgs", "repos_url": "https://api.github.com/users/nagisa/repos", "events_url": "https://api.github.com/users/nagisa/events{/privacy}", "received_events_url": "https://api.github.com/users/nagisa/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "6cc2e371359349e93c159e75298adbff7f8cef01", "url": "https://api.github.com/repos/rust-lang/rust/commits/6cc2e371359349e93c159e75298adbff7f8cef01", "html_url": "https://github.com/rust-lang/rust/commit/6cc2e371359349e93c159e75298adbff7f8cef01"}, {"sha": "a2960bc7c6108f92a6c8c27418b20af1337208cf", "url": "https://api.github.com/repos/rust-lang/rust/commits/a2960bc7c6108f92a6c8c27418b20af1337208cf", "html_url": "https://github.com/rust-lang/rust/commit/a2960bc7c6108f92a6c8c27418b20af1337208cf"}], "stats": {"total": 591, "additions": 551, "deletions": 40}, "files": [{"sha": "9f069cb17ed9033b6174d720e148d563331de281", "filename": "src/libsyntax/ext/tt/macro_rules.rs", "status": "modified", "additions": 507, "deletions": 20, "changes": 527, "blob_url": "https://github.com/rust-lang/rust/blob/6d6e831c3398d99ffcaba3af4335e411535c68bb/src%2Flibsyntax%2Fext%2Ftt%2Fmacro_rules.rs", "raw_url": "https://github.com/rust-lang/rust/raw/6d6e831c3398d99ffcaba3af4335e411535c68bb/src%2Flibsyntax%2Fext%2Ftt%2Fmacro_rules.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fext%2Ftt%2Fmacro_rules.rs?ref=6d6e831c3398d99ffcaba3af4335e411535c68bb", "patch": "@@ -25,8 +25,9 @@ use ptr::P;\n use util::small_vector::SmallVector;\n \n use std::cell::RefCell;\n+use std::collections::{HashMap};\n+use std::collections::hash_map::{Entry};\n use std::rc::Rc;\n-use std::iter::once;\n \n struct ParserAnyMacro<'a> {\n     parser: RefCell<Parser<'a>>,\n@@ -320,15 +321,18 @@ pub fn compile<'cx>(cx: &'cx mut ExtCtxt,\n     NormalTT(exp, Some(def.span), def.allow_internal_unstable)\n }\n \n+// why is this here? because of https://github.com/rust-lang/rust/issues/27774\n+fn ref_slice<A>(s: &A) -> &[A] { use std::slice::from_raw_parts; unsafe { from_raw_parts(s, 1) } }\n+\n fn check_lhs_nt_follows(cx: &mut ExtCtxt, lhs: &TokenTree, sp: Span) {\n     // lhs is going to be like TokenTree::Delimited(...), where the\n     // entire lhs is those tts. Or, it can be a \"bare sequence\", not wrapped in parens.\n     match lhs {\n         &TokenTree::Delimited(_, ref tts) => {\n-            check_matcher(cx, tts.tts.iter(), &Eof);\n+            check_matcher(cx, &tts.tts);\n         },\n         tt @ &TokenTree::Sequence(..) => {\n-            check_matcher(cx, Some(tt).into_iter(), &Eof);\n+            check_matcher(cx, ref_slice(tt));\n         },\n         _ => cx.span_err(sp, \"invalid macro matcher; matchers must be contained \\\n                               in balanced delimiters or a repetition indicator\")\n@@ -345,10 +349,59 @@ fn check_rhs(cx: &mut ExtCtxt, rhs: &TokenTree) -> bool {\n     false\n }\n \n-// returns the last token that was checked, for TokenTree::Sequence. this gets used later on.\n-fn check_matcher<'a, I>(cx: &mut ExtCtxt, matcher: I, follow: &Token)\n+// Issue 30450: when we are through a warning cycle, we can just error\n+// on all failure conditions and remove this struct and enum.\n+\n+#[derive(Debug)]\n+struct OnFail {\n+    saw_failure: bool,\n+    action: OnFailAction,\n+}\n+\n+#[derive(Copy, Clone, Debug)]\n+enum OnFailAction { Warn, Error, DoNothing }\n+\n+impl OnFail {\n+    fn warn() -> OnFail { OnFail { saw_failure: false, action: OnFailAction::Warn } }\n+    fn error() -> OnFail { OnFail { saw_failure: false, action: OnFailAction::Error } }\n+    fn do_nothing() -> OnFail { OnFail { saw_failure: false, action: OnFailAction::DoNothing } }\n+    fn react(&mut self, cx: &mut ExtCtxt, sp: Span, msg: &str) {\n+        match self.action {\n+            OnFailAction::DoNothing => {}\n+            OnFailAction::Error => cx.span_err(sp, msg),\n+            OnFailAction::Warn => {\n+                cx.struct_span_warn(sp, msg)\n+                    .span_note(sp, \"The above warning will be a hard error in the next release.\")\n+                    .emit();\n+            }\n+        };\n+        self.saw_failure = true;\n+    }\n+}\n+\n+fn check_matcher(cx: &mut ExtCtxt, matcher: &[TokenTree]) {\n+    // Issue 30450: when we are through a warning cycle, we can just\n+    // error on all failure conditions (and remove check_matcher_old).\n+\n+    // First run the old-pass, but *only* to find out if it would have failed.\n+    let mut on_fail = OnFail::do_nothing();\n+    check_matcher_old(cx, matcher.iter(), &Eof, &mut on_fail);\n+    // Then run the new pass, but merely warn if the old pass accepts and new pass rejects.\n+    // (Note this silently accepts code if new pass accepts.)\n+    let mut on_fail = if on_fail.saw_failure {\n+        OnFail::error()\n+    } else {\n+        OnFail::warn()\n+    };\n+    check_matcher_new(cx, matcher, &mut on_fail);\n+}\n+\n+// returns the last token that was checked, for TokenTree::Sequence.\n+// return value is used by recursive calls.\n+fn check_matcher_old<'a, I>(cx: &mut ExtCtxt, matcher: I, follow: &Token, on_fail: &mut OnFail)\n -> Option<(Span, Token)> where I: Iterator<Item=&'a TokenTree> {\n     use print::pprust::token_to_string;\n+    use std::iter::once;\n \n     let mut last = None;\n \n@@ -375,7 +428,7 @@ fn check_matcher<'a, I>(cx: &mut ExtCtxt, matcher: I, follow: &Token)\n                             // look at the token that follows the\n                             // sequence, which may itself be a sequence,\n                             // and so on).\n-                            cx.span_err(sp,\n+                            on_fail.react(cx, sp,\n                                         &format!(\"`${0}:{1}` is followed by a \\\n                                                   sequence repetition, which is not \\\n                                                   allowed for `{1}` fragments\",\n@@ -398,13 +451,13 @@ fn check_matcher<'a, I>(cx: &mut ExtCtxt, matcher: I, follow: &Token)\n                     // If T' is in the set FOLLOW(NT), continue. Else, reject.\n                     match (&next_token, is_in_follow(cx, &next_token, &frag_spec.name.as_str())) {\n                         (_, Err(msg)) => {\n-                            cx.span_err(sp, &msg);\n+                            on_fail.react(cx, sp, &msg);\n                             continue\n                         }\n                         (&Eof, _) => return Some((sp, tok.clone())),\n                         (_, Ok(true)) => continue,\n                         (next, Ok(false)) => {\n-                            cx.span_err(sp, &format!(\"`${0}:{1}` is followed by `{2}`, which \\\n+                            on_fail.react(cx, sp, &format!(\"`${0}:{1}` is followed by `{2}`, which \\\n                                                       is not allowed for `{1}` fragments\",\n                                                      name, frag_spec,\n                                                      token_to_string(next)));\n@@ -420,7 +473,7 @@ fn check_matcher<'a, I>(cx: &mut ExtCtxt, matcher: I, follow: &Token)\n                     // run the algorithm on the contents with F set to U. If it\n                     // accepts, continue, else, reject.\n                     Some(ref u) => {\n-                        let last = check_matcher(cx, seq.tts.iter(), u);\n+                        let last = check_matcher_old(cx, seq.tts.iter(), u, on_fail);\n                         match last {\n                             // Since the delimiter isn't required after the last\n                             // repetition, make sure that the *next* token is\n@@ -434,14 +487,14 @@ fn check_matcher<'a, I>(cx: &mut ExtCtxt, matcher: I, follow: &Token)\n                                     Some(&&TokenTree::Delimited(_, ref delim)) =>\n                                         delim.close_token(),\n                                     Some(_) => {\n-                                        cx.span_err(sp, \"sequence repetition followed by \\\n+                                        on_fail.react(cx, sp, \"sequence repetition followed by \\\n                                                 another sequence repetition, which is not allowed\");\n                                         Eof\n                                     },\n                                     None => Eof\n                                 };\n-                                check_matcher(cx, once(&TokenTree::Token(span, tok.clone())),\n-                                              &fol)\n+                                check_matcher_old(cx, once(&TokenTree::Token(span, tok.clone())),\n+                                                  &fol, on_fail)\n                             },\n                             None => last,\n                         }\n@@ -454,13 +507,13 @@ fn check_matcher<'a, I>(cx: &mut ExtCtxt, matcher: I, follow: &Token)\n                             Some(&&TokenTree::Token(_, ref tok)) => tok.clone(),\n                             Some(&&TokenTree::Delimited(_, ref delim)) => delim.close_token(),\n                             Some(_) => {\n-                                cx.span_err(sp, \"sequence repetition followed by another \\\n+                                on_fail.react(cx, sp, \"sequence repetition followed by another \\\n                                              sequence repetition, which is not allowed\");\n                                 Eof\n                             },\n                             None => Eof\n                         };\n-                        check_matcher(cx, seq.tts.iter(), &fol)\n+                        check_matcher_old(cx, seq.tts.iter(), &fol, on_fail)\n                     }\n                 }\n             },\n@@ -471,13 +524,425 @@ fn check_matcher<'a, I>(cx: &mut ExtCtxt, matcher: I, follow: &Token)\n             TokenTree::Delimited(_, ref tts) => {\n                 // if we don't pass in that close delimiter, we'll incorrectly consider the matcher\n                 // `{ $foo:ty }` as having a follow that isn't `RBrace`\n-                check_matcher(cx, tts.tts.iter(), &tts.close_token())\n+                check_matcher_old(cx, tts.tts.iter(), &tts.close_token(), on_fail)\n             }\n         }\n     }\n     last\n }\n \n+fn check_matcher_new(cx: &mut ExtCtxt, matcher: &[TokenTree], on_fail: &mut OnFail) {\n+    let first_sets = FirstSets::new(matcher);\n+    let empty_suffix = TokenSet::empty();\n+    check_matcher_core(cx, &first_sets, matcher, &empty_suffix, on_fail);\n+}\n+\n+// The FirstSets for a matcher is a mapping from subsequences in the\n+// matcher to the FIRST set for that subsequence.\n+//\n+// This mapping is partially precomputed via a backwards scan over the\n+// token trees of the matcher, which provides a mapping from each\n+// repetition sequence to its FIRST set.\n+//\n+// (Hypothetically sequences should be uniquely identifiable via their\n+// spans, though perhaps that is false e.g. for macro-generated macros\n+// that do not try to inject artificial span information. My plan is\n+// to try to catch such cases ahead of time and not include them in\n+// the precomputed mapping.)\n+struct FirstSets {\n+    // this maps each TokenTree::Sequence `$(tt ...) SEP OP` that is uniquely identified by its\n+    // span in the original matcher to the First set for the inner sequence `tt ...`.\n+    //\n+    // If two sequences have the same span in a matcher, then map that\n+    // span to None (invalidating the mapping here and forcing the code to\n+    // use a slow path).\n+    first: HashMap<Span, Option<TokenSet>>,\n+}\n+\n+impl FirstSets {\n+    fn new(tts: &[TokenTree]) -> FirstSets {\n+        let mut sets = FirstSets { first: HashMap::new() };\n+        build_recur(&mut sets, tts);\n+        return sets;\n+\n+        // walks backward over `tts`, returning the FIRST for `tts`\n+        // and updating `sets` at the same time for all sequence\n+        // substructure we find within `tts`.\n+        fn build_recur(sets: &mut FirstSets, tts: &[TokenTree]) -> TokenSet {\n+            let mut first = TokenSet::empty();\n+            for tt in tts.iter().rev() {\n+                match *tt {\n+                    TokenTree::Token(sp, ref tok) => {\n+                        first.replace_with((sp, tok.clone()));\n+                    }\n+                    TokenTree::Delimited(_, ref delimited) => {\n+                        build_recur(sets, &delimited.tts[..]);\n+                        first.replace_with((delimited.open_span,\n+                                            Token::OpenDelim(delimited.delim)));\n+                    }\n+                    TokenTree::Sequence(sp, ref seq_rep) => {\n+                        let subfirst = build_recur(sets, &seq_rep.tts[..]);\n+\n+                        match sets.first.entry(sp) {\n+                            Entry::Vacant(vac) => {\n+                                vac.insert(Some(subfirst.clone()));\n+                            }\n+                            Entry::Occupied(mut occ) => {\n+                                // if there is already an entry, then a span must have collided.\n+                                // This should not happen with typical macro_rules macros,\n+                                // but syntax extensions need not maintain distinct spans,\n+                                // so distinct syntax trees can be assigned the same span.\n+                                // In such a case, the map cannot be trusted; so mark this\n+                                // entry as unusable.\n+                                occ.insert(None);\n+                            }\n+                        }\n+\n+                        // If the sequence contents can be empty, then the first\n+                        // token could be the separator token itself.\n+\n+                        if let (Some(ref sep), true) = (seq_rep.separator.clone(),\n+                                                        subfirst.maybe_empty) {\n+                            first.add_one_maybe((sp, sep.clone()));\n+                        }\n+\n+                        // Reverse scan: Sequence comes before `first`.\n+                        if subfirst.maybe_empty || seq_rep.op == ast::KleeneOp::ZeroOrMore {\n+                            // If sequence is potentially empty, then\n+                            // union them (preserving first emptiness).\n+                            first.add_all(&TokenSet { maybe_empty: true, ..subfirst });\n+                        } else {\n+                            // Otherwise, sequence guaranteed\n+                            // non-empty; replace first.\n+                            first = subfirst;\n+                        }\n+                    }\n+                }\n+            }\n+\n+            return first;\n+        }\n+    }\n+\n+    // walks forward over `tts` until all potential FIRST tokens are\n+    // identified.\n+    fn first(&self, tts: &[TokenTree]) -> TokenSet {\n+        let mut first = TokenSet::empty();\n+        for tt in tts.iter() {\n+            assert!(first.maybe_empty);\n+            match *tt {\n+                TokenTree::Token(sp, ref tok) => {\n+                    first.add_one((sp, tok.clone()));\n+                    return first;\n+                }\n+                TokenTree::Delimited(_, ref delimited) => {\n+                    first.add_one((delimited.open_span,\n+                                   Token::OpenDelim(delimited.delim)));\n+                    return first;\n+                }\n+                TokenTree::Sequence(sp, ref seq_rep) => {\n+                    match self.first.get(&sp) {\n+                        Some(&Some(ref subfirst)) => {\n+\n+                            // If the sequence contents can be empty, then the first\n+                            // token could be the separator token itself.\n+\n+                            if let (Some(ref sep), true) = (seq_rep.separator.clone(),\n+                                                            subfirst.maybe_empty) {\n+                                first.add_one_maybe((sp, sep.clone()));\n+                            }\n+\n+                            assert!(first.maybe_empty);\n+                            first.add_all(subfirst);\n+                            if subfirst.maybe_empty || seq_rep.op == ast::KleeneOp::ZeroOrMore {\n+                                // continue scanning for more first\n+                                // tokens, but also make sure we\n+                                // restore empty-tracking state\n+                                first.maybe_empty = true;\n+                                continue;\n+                            } else {\n+                                return first;\n+                            }\n+                        }\n+\n+                        Some(&None) => {\n+                            panic!(\"assume all sequences have (unique) spans for now\");\n+                        }\n+\n+                        None => {\n+                            panic!(\"We missed a sequence during FirstSets construction\");\n+                        }\n+                    }\n+                }\n+            }\n+        }\n+\n+        // we only exit the loop if `tts` was empty or if every\n+        // element of `tts` matches the empty sequence.\n+        assert!(first.maybe_empty);\n+        return first;\n+    }\n+}\n+\n+// A set of Tokens, which may include MatchNt tokens (for\n+// macro-by-example syntactic variables). It also carries the\n+// `maybe_empty` flag; that is true if and only if the matcher can\n+// match an empty token sequence.\n+//\n+// The First set is computed on submatchers like `$($a:expr b),* $(c)* d`,\n+// which has corresponding FIRST = {$a:expr, c, d}.\n+// Likewise, `$($a:expr b),* $(c)+ d` has FIRST = {$a:expr, c}.\n+//\n+// (Notably, we must allow for *-op to occur zero times.)\n+#[derive(Clone, Debug)]\n+struct TokenSet {\n+    tokens: Vec<(Span, Token)>,\n+    maybe_empty: bool,\n+}\n+\n+impl TokenSet {\n+    // Returns a set for the empty sequence.\n+    fn empty() -> Self { TokenSet { tokens: Vec::new(), maybe_empty: true } }\n+\n+    // Returns the set `{ tok }` for the single-token (and thus\n+    // non-empty) sequence [tok].\n+    fn singleton(tok: (Span, Token)) -> Self {\n+        TokenSet { tokens: vec![tok], maybe_empty: false }\n+    }\n+\n+    // Changes self to be the set `{ tok }`.\n+    // Since `tok` is always present, marks self as non-empty.\n+    fn replace_with(&mut self, tok: (Span, Token)) {\n+        self.tokens.clear();\n+        self.tokens.push(tok);\n+        self.maybe_empty = false;\n+    }\n+\n+    // Changes self to be the empty set `{}`; meant for use when\n+    // the particular token does not matter, but we want to\n+    // record that it occurs.\n+    fn replace_with_irrelevant(&mut self) {\n+        self.tokens.clear();\n+        self.maybe_empty = false;\n+    }\n+\n+    // Adds `tok` to the set for `self`, marking sequence as non-empy.\n+    fn add_one(&mut self, tok: (Span, Token)) {\n+        if !self.tokens.contains(&tok) {\n+            self.tokens.push(tok);\n+        }\n+        self.maybe_empty = false;\n+    }\n+\n+    // Adds `tok` to the set for `self`. (Leaves `maybe_empty` flag alone.)\n+    fn add_one_maybe(&mut self, tok: (Span, Token)) {\n+        if !self.tokens.contains(&tok) {\n+            self.tokens.push(tok);\n+        }\n+    }\n+\n+    // Adds all elements of `other` to this.\n+    //\n+    // (Since this is a set, we filter out duplicates.)\n+    //\n+    // If `other` is potentially empty, then preserves the previous\n+    // setting of the empty flag of `self`. If `other` is guaranteed\n+    // non-empty, then `self` is marked non-empty.\n+    fn add_all(&mut self, other: &Self) {\n+        for tok in &other.tokens {\n+            if !self.tokens.contains(tok) {\n+                self.tokens.push(tok.clone());\n+            }\n+        }\n+        if !other.maybe_empty {\n+            self.maybe_empty = false;\n+        }\n+    }\n+}\n+\n+// Checks that `matcher` is internally consistent and that it\n+// can legally by followed by a token N, for all N in `follow`.\n+// (If `follow` is empty, then it imposes no constraint on\n+// the `matcher`.)\n+//\n+// Returns the set of NT tokens that could possibly come last in\n+// `matcher`. (If `matcher` matches the empty sequence, then\n+// `maybe_empty` will be set to true.)\n+//\n+// Requires that `first_sets` is pre-computed for `matcher`;\n+// see `FirstSets::new`.\n+fn check_matcher_core(cx: &mut ExtCtxt,\n+                      first_sets: &FirstSets,\n+                      matcher: &[TokenTree],\n+                      follow: &TokenSet,\n+                      on_fail: &mut OnFail) -> TokenSet {\n+    use print::pprust::token_to_string;\n+\n+    let mut last = TokenSet::empty();\n+\n+    // 2. For each token and suffix  [T, SUFFIX] in M:\n+    // ensure that T can be followed by SUFFIX, and if SUFFIX may be empty,\n+    // then ensure T can also be followed by any element of FOLLOW.\n+    'each_token: for i in 0..matcher.len() {\n+        let token = &matcher[i];\n+        let suffix = &matcher[i+1..];\n+\n+        let build_suffix_first = || {\n+            let mut s = first_sets.first(suffix);\n+            if s.maybe_empty { s.add_all(follow); }\n+            return s;\n+        };\n+\n+        // (we build `suffix_first` on demand below; you can tell\n+        // which cases are supposed to fall through by looking for the\n+        // initialization of this variable.)\n+        let suffix_first;\n+\n+        // First, update `last` so that it corresponds to the set\n+        // of NT tokens that might end the sequence `... token`.\n+        match *token {\n+            TokenTree::Token(sp, ref tok) => {\n+                let can_be_followed_by_any;\n+                if let Err(bad_frag) = has_legal_fragment_specifier(tok) {\n+                    on_fail.react(cx, sp, &format!(\"invalid fragment specifier `{}`\", bad_frag));\n+                    // (This eliminates false positives and duplicates\n+                    // from error messages.)\n+                    can_be_followed_by_any = true;\n+                } else {\n+                    can_be_followed_by_any = token_can_be_followed_by_any(tok);\n+                }\n+\n+                if can_be_followed_by_any {\n+                    // don't need to track tokens that work with any,\n+                    last.replace_with_irrelevant();\n+                    // ... and don't need to check tokens that can be\n+                    // followed by anything against SUFFIX.\n+                    continue 'each_token;\n+                } else {\n+                    last.replace_with((sp, tok.clone()));\n+                    suffix_first = build_suffix_first();\n+                }\n+            }\n+            TokenTree::Delimited(_, ref d) => {\n+                let my_suffix = TokenSet::singleton((d.close_span, Token::CloseDelim(d.delim)));\n+                check_matcher_core(cx, first_sets, &d.tts, &my_suffix, on_fail);\n+                // don't track non NT tokens\n+                last.replace_with_irrelevant();\n+\n+                // also, we don't need to check delimited sequences\n+                // against SUFFIX\n+                continue 'each_token;\n+            }\n+            TokenTree::Sequence(sp, ref seq_rep) => {\n+                suffix_first = build_suffix_first();\n+                // The trick here: when we check the interior, we want\n+                // to include the separator (if any) as a potential\n+                // (but not guaranteed) element of FOLLOW. So in that\n+                // case, we make a temp copy of suffix and stuff\n+                // delimiter in there.\n+                //\n+                // FIXME: Should I first scan suffix_first to see if\n+                // delimiter is already in it before I go through the\n+                // work of cloning it? But then again, this way I may\n+                // get a \"tighter\" span?\n+                let mut new;\n+                let my_suffix = if let Some(ref u) = seq_rep.separator {\n+                    new = suffix_first.clone();\n+                    new.add_one_maybe((sp, u.clone()));\n+                    &new\n+                } else {\n+                    &suffix_first\n+                };\n+\n+                // At this point, `suffix_first` is built, and\n+                // `my_suffix` is some TokenSet that we can use\n+                // for checking the interior of `seq_rep`.\n+                let next = check_matcher_core(cx, first_sets, &seq_rep.tts, my_suffix, on_fail);\n+                if next.maybe_empty {\n+                    last.add_all(&next);\n+                } else {\n+                    last = next;\n+                }\n+\n+                // the recursive call to check_matcher_core already ran the 'each_last\n+                // check below, so we can just keep going forward here.\n+                continue 'each_token;\n+            }\n+        }\n+\n+        // (`suffix_first` guaranteed initialized once reaching here.)\n+\n+        // Now `last` holds the complete set of NT tokens that could\n+        // end the sequence before SUFFIX. Check that every one works with `suffix`.\n+        'each_last: for &(_sp, ref t) in &last.tokens {\n+            if let MatchNt(ref name, ref frag_spec, _, _) = *t {\n+                for &(sp, ref next_token) in &suffix_first.tokens {\n+                    match is_in_follow(cx, next_token, &frag_spec.name.as_str()) {\n+                        Err(msg) => {\n+                            on_fail.react(cx, sp, &msg);\n+                            // don't bother reporting every source of\n+                            // conflict for a particular element of `last`.\n+                            continue 'each_last;\n+                        }\n+                        Ok(true) => {}\n+                        Ok(false) => {\n+                            let may_be = if last.tokens.len() == 1 &&\n+                                suffix_first.tokens.len() == 1\n+                            {\n+                                \"is\"\n+                            } else {\n+                                \"may be\"\n+                            };\n+\n+                            on_fail.react(\n+                                cx, sp,\n+                                &format!(\"`${name}:{frag}` {may_be} followed by `{next}`, which \\\n+                                          is not allowed for `{frag}` fragments\",\n+                                         name=name,\n+                                         frag=frag_spec,\n+                                         next=token_to_string(next_token),\n+                                         may_be=may_be));\n+                        }\n+                    }\n+                }\n+            }\n+        }\n+    }\n+    last\n+}\n+\n+\n+fn token_can_be_followed_by_any(tok: &Token) -> bool {\n+    if let &MatchNt(_, ref frag_spec, _, _) = tok {\n+        frag_can_be_followed_by_any(&frag_spec.name.as_str())\n+    } else {\n+        // (Non NT's can always be followed by anthing in matchers.)\n+        true\n+    }\n+}\n+\n+/// True if a fragment of type `frag` can be followed by any sort of\n+/// token.  We use this (among other things) as a useful approximation\n+/// for when `frag` can be followed by a repetition like `$(...)*` or\n+/// `$(...)+`. In general, these can be a bit tricky to reason about,\n+/// so we adopt a conservative position that says that any fragment\n+/// specifier which consumes at most one token tree can be followed by\n+/// a fragment specifier (indeed, these fragments can be followed by\n+/// ANYTHING without fear of future compatibility hazards).\n+fn frag_can_be_followed_by_any(frag: &str) -> bool {\n+    match frag {\n+        \"item\" |  // always terminated by `}` or `;`\n+        \"block\" | // exactly one token tree\n+        \"ident\" | // exactly one token tree\n+        \"meta\" |  // exactly one token tree\n+        \"tt\" =>    // exactly one token tree\n+            true,\n+\n+        _ =>\n+            false,\n+    }\n+}\n+\n /// True if a fragment of type `frag` can be followed by any sort of\n /// token.  We use this (among other things) as a useful approximation\n /// for when `frag` can be followed by a repetition like `$(...)*` or\n@@ -501,7 +966,7 @@ fn can_be_followed_by_any(frag: &str) -> bool {\n }\n \n /// True if `frag` can legally be followed by the token `tok`. For\n-/// fragments that can consume an unbounded numbe of tokens, `tok`\n+/// fragments that can consume an unbounded number of tokens, `tok`\n /// must be within a well-defined follow set. This is intended to\n /// guarantee future compatibility: for example, without this rule, if\n /// we expanded `expr` to include a new binary operator, we might\n@@ -532,15 +997,18 @@ fn is_in_follow(_: &ExtCtxt, tok: &Token, frag: &str) -> Result<bool, String> {\n             },\n             \"pat\" => {\n                 match *tok {\n-                    FatArrow | Comma | Eq => Ok(true),\n-                    Ident(i, _) if i.name.as_str() == \"if\" || i.name.as_str() == \"in\" => Ok(true),\n+                    FatArrow | Comma | Eq | BinOp(token::Or) => Ok(true),\n+                    Ident(i, _) if (i.name.as_str() == \"if\" ||\n+                                    i.name.as_str() == \"in\") => Ok(true),\n                     _ => Ok(false)\n                 }\n             },\n             \"path\" | \"ty\" => {\n                 match *tok {\n-                    Comma | FatArrow | Colon | Eq | Gt | Semi => Ok(true),\n-                    Ident(i, _) if i.name.as_str() == \"as\" => Ok(true),\n+                    OpenDelim(token::DelimToken::Brace) |\n+                    Comma | FatArrow | Colon | Eq | Gt | Semi | BinOp(token::Or) => Ok(true),\n+                    Ident(i, _) if (i.name.as_str() == \"as\" ||\n+                                    i.name.as_str() == \"where\") => Ok(true),\n                     _ => Ok(false)\n                 }\n             },\n@@ -557,3 +1025,22 @@ fn is_in_follow(_: &ExtCtxt, tok: &Token, frag: &str) -> Result<bool, String> {\n         }\n     }\n }\n+\n+fn has_legal_fragment_specifier(tok: &Token) -> Result<(), String> {\n+    debug!(\"has_legal_fragment_specifier({:?})\", tok);\n+    if let &MatchNt(_, ref frag_spec, _, _) = tok {\n+        let s = &frag_spec.name.as_str();\n+        if !is_legal_fragment_specifier(s) {\n+            return Err(s.to_string());\n+        }\n+    }\n+    Ok(())\n+}\n+\n+fn is_legal_fragment_specifier(frag: &str) -> bool {\n+    match frag {\n+        \"item\" | \"block\" | \"stmt\" | \"expr\" | \"pat\" |\n+        \"path\" | \"ty\" | \"ident\" | \"meta\" | \"tt\" => true,\n+        _ => false,\n+    }\n+}"}, {"sha": "7ad43954010b9b4333e422e3cdab9c47a16ceaad", "filename": "src/test/compile-fail/issue-30715.rs", "status": "modified", "additions": 6, "deletions": 1, "changes": 7, "blob_url": "https://github.com/rust-lang/rust/blob/6d6e831c3398d99ffcaba3af4335e411535c68bb/src%2Ftest%2Fcompile-fail%2Fissue-30715.rs", "raw_url": "https://github.com/rust-lang/rust/raw/6d6e831c3398d99ffcaba3af4335e411535c68bb/src%2Ftest%2Fcompile-fail%2Fissue-30715.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ftest%2Fcompile-fail%2Fissue-30715.rs?ref=6d6e831c3398d99ffcaba3af4335e411535c68bb", "patch": "@@ -10,7 +10,12 @@\n \n macro_rules! parallel {\n     (\n-        for $id:ident in $iter:expr {\n+        // If future has `pred`/`moelarry` fragments (where \"pred\" is\n+        // \"like expr, but with `{` in its FOLLOW set\"), then could\n+        // use `pred` instead of future-proof erroring here. See also:\n+        //\n+        // https://github.com/rust-lang/rfcs/pull/1384#issuecomment-160165525\n+        for $id:ident in $iter:expr { //~ WARN `$iter:expr` is followed by `{`\n             $( $inner:expr; )*\n         }\n     ) => {};"}, {"sha": "fe758a4a6310fbfe925eeb75d0a1e9c234d0209c", "filename": "src/test/compile-fail/macro-input-future-proofing.rs", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "blob_url": "https://github.com/rust-lang/rust/blob/6d6e831c3398d99ffcaba3af4335e411535c68bb/src%2Ftest%2Fcompile-fail%2Fmacro-input-future-proofing.rs", "raw_url": "https://github.com/rust-lang/rust/raw/6d6e831c3398d99ffcaba3af4335e411535c68bb/src%2Ftest%2Fcompile-fail%2Fmacro-input-future-proofing.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ftest%2Fcompile-fail%2Fmacro-input-future-proofing.rs?ref=6d6e831c3398d99ffcaba3af4335e411535c68bb", "patch": "@@ -18,13 +18,14 @@ macro_rules! errors_everywhere {\n     ($bl:block < ) => ();\n     ($pa:pat >) => (); //~ ERROR `$pa:pat` is followed by `>`, which is not allowed for `pat`\n     ($pa:pat , ) => ();\n-    ($pa:pat | ) => (); //~ ERROR `$pa:pat` is followed by `|`\n     ($pa:pat $pb:pat $ty:ty ,) => ();\n     //~^ ERROR `$pa:pat` is followed by `$pb:pat`, which is not allowed\n     //~^^ ERROR `$pb:pat` is followed by `$ty:ty`, which is not allowed\n     ($($ty:ty)* -) => (); //~ ERROR `$ty:ty` is followed by `-`\n     ($($a:ty, $b:ty)* -) => (); //~ ERROR `$b:ty` is followed by `-`\n     ($($ty:ty)-+) => (); //~ ERROR `$ty:ty` is followed by `-`, which is not allowed for `ty`\n+    ( $($a:expr)* $($b:tt)* ) => { };\n+    //~^ ERROR `$a:expr` is followed by `$b:tt`, which is not allowed for `expr` fragments\n }\n \n fn main() { }"}, {"sha": "b4f71343d546afba29b2aa1b4a680d29b3118a19", "filename": "src/test/compile-fail/macro-seq-followed-by-seq.rs", "status": "removed", "additions": 0, "deletions": 18, "changes": 18, "blob_url": "https://github.com/rust-lang/rust/blob/6cc2e371359349e93c159e75298adbff7f8cef01/src%2Ftest%2Fcompile-fail%2Fmacro-seq-followed-by-seq.rs", "raw_url": "https://github.com/rust-lang/rust/raw/6cc2e371359349e93c159e75298adbff7f8cef01/src%2Ftest%2Fcompile-fail%2Fmacro-seq-followed-by-seq.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ftest%2Fcompile-fail%2Fmacro-seq-followed-by-seq.rs?ref=6cc2e371359349e93c159e75298adbff7f8cef01", "patch": "@@ -1,18 +0,0 @@\n-// Copyright 2015 The Rust Project Developers. See the COPYRIGHT\n-// file at the top-level directory of this distribution and at\n-// http://rust-lang.org/COPYRIGHT.\n-//\n-// Licensed under the Apache License, Version 2.0 <LICENSE-APACHE or\n-// http://www.apache.org/licenses/LICENSE-2.0> or the MIT license\n-// <LICENSE-MIT or http://opensource.org/licenses/MIT>, at your\n-// option. This file may not be copied, modified, or distributed\n-// except according to those terms.\n-\n-// Check that we cannot have two sequence repetitions in a row.\n-\n-macro_rules! foo {\n-  ( $($a:expr)* $($b:tt)* ) => { }; //~ ERROR sequence repetition followed by another sequence\n-  ( $($a:tt)* $($b:tt)* ) => { }; //~ ERROR sequence repetition followed by another sequence\n-}\n-\n-fn main() { }"}, {"sha": "c1abebd5f9040caa068c2813bbe2cafdbad9470c", "filename": "src/test/run-pass/macro-pat-follow.rs", "status": "modified", "additions": 10, "deletions": 0, "changes": 10, "blob_url": "https://github.com/rust-lang/rust/blob/6d6e831c3398d99ffcaba3af4335e411535c68bb/src%2Ftest%2Frun-pass%2Fmacro-pat-follow.rs", "raw_url": "https://github.com/rust-lang/rust/raw/6d6e831c3398d99ffcaba3af4335e411535c68bb/src%2Ftest%2Frun-pass%2Fmacro-pat-follow.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ftest%2Frun-pass%2Fmacro-pat-follow.rs?ref=6d6e831c3398d99ffcaba3af4335e411535c68bb", "patch": "@@ -24,7 +24,17 @@ macro_rules! pat_if {\n     }}\n }\n \n+macro_rules! pat_bar {\n+    ($p:pat | $p2:pat) => {{\n+        match Some(1u8) {\n+            $p | $p2 => {},\n+            _ => {}\n+        }\n+    }}\n+}\n+\n fn main() {\n     pat_in!(Some(_) in 0..10);\n     pat_if!(Some(x) if x > 0);\n+    pat_bar!(Some(1u8) | None);\n }"}, {"sha": "23c7d2516a27e832933d696c8614f45a30772803", "filename": "src/test/run-pass/macro-seq-followed-by-seq.rs", "status": "added", "additions": 26, "deletions": 0, "changes": 26, "blob_url": "https://github.com/rust-lang/rust/blob/6d6e831c3398d99ffcaba3af4335e411535c68bb/src%2Ftest%2Frun-pass%2Fmacro-seq-followed-by-seq.rs", "raw_url": "https://github.com/rust-lang/rust/raw/6d6e831c3398d99ffcaba3af4335e411535c68bb/src%2Ftest%2Frun-pass%2Fmacro-seq-followed-by-seq.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ftest%2Frun-pass%2Fmacro-seq-followed-by-seq.rs?ref=6d6e831c3398d99ffcaba3af4335e411535c68bb", "patch": "@@ -0,0 +1,26 @@\n+// Copyright 2016 The Rust Project Developers. See the COPYRIGHT\n+// file at the top-level directory of this distribution and at\n+// http://rust-lang.org/COPYRIGHT.\n+//\n+// Licensed under the Apache License, Version 2.0 <LICENSE-APACHE or\n+// http://www.apache.org/licenses/LICENSE-2.0> or the MIT license\n+// <LICENSE-MIT or http://opensource.org/licenses/MIT>, at your\n+// option. This file may not be copied, modified, or distributed\n+// except according to those terms.\n+\n+// Test of allowing two sequences repetitions in a row,\n+// functionality added as byproduct of RFC amendment #1384\n+//   https://github.com/rust-lang/rfcs/pull/1384\n+\n+// Old version of Rust would reject this macro definition, even though\n+// there are no local ambiguities (the initial `banana` and `orange`\n+// tokens are enough for the expander to distinguish which case is\n+// intended).\n+macro_rules! foo {\n+    ( $(banana $a:ident)* $(orange $b:tt)* ) => { };\n+}\n+\n+fn main() {\n+    foo!( banana id1 banana id2\n+          orange hi  orange (hello world) );\n+}"}]}