{"sha": "62a182cf7f91b11bfbf200ede67569da0f6488ec", "node_id": "C_kwDOAAsO6NoAKDYyYTE4MmNmN2Y5MWIxMWJmYmYyMDBlZGU2NzU2OWRhMGY2NDg4ZWM", "commit": {"author": {"name": "Simonas Kazlauskas", "email": "git@kazlauskas.me", "date": "2022-07-03T21:23:31Z"}, "committer": {"name": "Simonas Kazlauskas", "email": "git@kazlauskas.me", "date": "2022-07-16T22:27:37Z"}, "message": "Add a special case for align_offset /w stride != 1\n\nThis generalizes the previous `stride == 1` special case to apply to any\nsituation where the requested alignment is divisible by the stride. This\nin turn allows the test case from #98809 produce ideal assembly, along\nthe lines of:\n\n    leaq 15(%rdi), %rax\n    andq $-16, %rax\n\nThis also produces pretty high quality code for situations where the\nalignment of the input pointer isn\u2019t known:\n\n    pub unsafe fn ptr_u32(slice: *const u32) -> *const u32 {\n        slice.offset(slice.align_offset(16) as isize)\n    }\n\n    // =>\n\n    movl %edi, %eax\n    andl $3, %eax\n    leaq 15(%rdi), %rcx\n    andq $-16, %rcx\n    subq %rdi, %rcx\n    shrq $2, %rcx\n    negq %rax\n    sbbq %rax, %rax\n    orq  %rcx, %rax\n    leaq (%rdi,%rax,4), %rax\n\nHere LLVM is smart enough to replace the `usize::MAX` special case with\na branch-less bitwise-OR approach, where the mask is constructed using\nthe neg and sbb instructions. This appears to work across various\narchitectures I\u2019ve tried.\n\nThis change ends up introducing more branches and code in situations\nwhere there is less knowledge of the arguments. For example when the\nrequested alignment is entirely unknown. This use-case was never really\na focus of this function, so I\u2019m not particularly worried, especially\nsince llvm-mca is saying that the new code is still appreciably faster,\ndespite all the new branching.\n\nFixes #98809.\nSadly, this does not help with #72356.", "tree": {"sha": "70541eb4cf8b66dc5eeaa33570f0e553d8c53f31", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/70541eb4cf8b66dc5eeaa33570f0e553d8c53f31"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/62a182cf7f91b11bfbf200ede67569da0f6488ec", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/62a182cf7f91b11bfbf200ede67569da0f6488ec", "html_url": "https://github.com/rust-lang/rust/commit/62a182cf7f91b11bfbf200ede67569da0f6488ec", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/62a182cf7f91b11bfbf200ede67569da0f6488ec/comments", "author": {"login": "nagisa", "id": 679122, "node_id": "MDQ6VXNlcjY3OTEyMg==", "avatar_url": "https://avatars.githubusercontent.com/u/679122?v=4", "gravatar_id": "", "url": "https://api.github.com/users/nagisa", "html_url": "https://github.com/nagisa", "followers_url": "https://api.github.com/users/nagisa/followers", "following_url": "https://api.github.com/users/nagisa/following{/other_user}", "gists_url": "https://api.github.com/users/nagisa/gists{/gist_id}", "starred_url": "https://api.github.com/users/nagisa/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/nagisa/subscriptions", "organizations_url": "https://api.github.com/users/nagisa/orgs", "repos_url": "https://api.github.com/users/nagisa/repos", "events_url": "https://api.github.com/users/nagisa/events{/privacy}", "received_events_url": "https://api.github.com/users/nagisa/received_events", "type": "User", "site_admin": false}, "committer": {"login": "nagisa", "id": 679122, "node_id": "MDQ6VXNlcjY3OTEyMg==", "avatar_url": "https://avatars.githubusercontent.com/u/679122?v=4", "gravatar_id": "", "url": "https://api.github.com/users/nagisa", "html_url": "https://github.com/nagisa", "followers_url": "https://api.github.com/users/nagisa/followers", "following_url": "https://api.github.com/users/nagisa/following{/other_user}", "gists_url": "https://api.github.com/users/nagisa/gists{/gist_id}", "starred_url": "https://api.github.com/users/nagisa/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/nagisa/subscriptions", "organizations_url": "https://api.github.com/users/nagisa/orgs", "repos_url": "https://api.github.com/users/nagisa/repos", "events_url": "https://api.github.com/users/nagisa/events{/privacy}", "received_events_url": "https://api.github.com/users/nagisa/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "6a1092056441652fe5fe5c5b422644951e6b99ce", "url": "https://api.github.com/repos/rust-lang/rust/commits/6a1092056441652fe5fe5c5b422644951e6b99ce", "html_url": "https://github.com/rust-lang/rust/commit/6a1092056441652fe5fe5c5b422644951e6b99ce"}], "stats": {"total": 194, "additions": 137, "deletions": 57}, "files": [{"sha": "7fa4361cc8e1fef4fb09da7f596dcada0c2ca153", "filename": "library/core/src/ptr/mod.rs", "status": "modified", "additions": 53, "deletions": 30, "changes": 83, "blob_url": "https://github.com/rust-lang/rust/blob/62a182cf7f91b11bfbf200ede67569da0f6488ec/library%2Fcore%2Fsrc%2Fptr%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/62a182cf7f91b11bfbf200ede67569da0f6488ec/library%2Fcore%2Fsrc%2Fptr%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/library%2Fcore%2Fsrc%2Fptr%2Fmod.rs?ref=62a182cf7f91b11bfbf200ede67569da0f6488ec", "patch": "@@ -1594,11 +1594,10 @@ pub(crate) unsafe fn align_offset<T: Sized>(p: *const T, a: usize) -> usize {\n     // FIXME(#75598): Direct use of these intrinsics improves codegen significantly at opt-level <=\n     // 1, where the method versions of these operations are not inlined.\n     use intrinsics::{\n-        unchecked_shl, unchecked_shr, unchecked_sub, wrapping_add, wrapping_mul, wrapping_sub,\n+        cttz_nonzero, exact_div, unchecked_rem, unchecked_shl, unchecked_shr, unchecked_sub,\n+        wrapping_add, wrapping_mul, wrapping_sub,\n     };\n \n-    let addr = p.addr();\n-\n     /// Calculate multiplicative modular inverse of `x` modulo `m`.\n     ///\n     /// This implementation is tailored for `align_offset` and has following preconditions:\n@@ -1648,36 +1647,61 @@ pub(crate) unsafe fn align_offset<T: Sized>(p: *const T, a: usize) -> usize {\n         }\n     }\n \n+    let addr = p.addr();\n     let stride = mem::size_of::<T>();\n     // SAFETY: `a` is a power-of-two, therefore non-zero.\n     let a_minus_one = unsafe { unchecked_sub(a, 1) };\n-    if stride == 1 {\n-        // `stride == 1` case can be computed more simply through `-p (mod a)`, but doing so\n-        // inhibits LLVM's ability to select instructions like `lea`. Instead we compute\n+\n+    if stride == 0 {\n+        // SPECIAL_CASE: handle 0-sized types. No matter how many times we step, the address will\n+        // stay the same, so no offset will be able to align the pointer unless it is already\n+        // aligned. This branch _will_ be optimized out as `stride` is known at compile-time.\n+        let p_mod_a = addr & a_minus_one;\n+        return if p_mod_a == 0 { 0 } else { usize::MAX };\n+    }\n+\n+    // SAFETY: `stride == 0` case has been handled by the special case above.\n+    let a_mod_stride = unsafe { unchecked_rem(a, stride) };\n+    if a_mod_stride == 0 {\n+        // SPECIAL_CASE: In cases where the `a` is divisible by `stride`, byte offset to align a\n+        // pointer can be computed more simply through `-p (mod a)`. In the off-chance the byte\n+        // offset is not a multiple of `stride`, the input pointer was misaligned and no pointer\n+        // offset will be able to produce a `p` aligned to the specified `a`.\n         //\n-        //    round_up_to_next_alignment(p, a) - p\n+        // The naive `-p (mod a)` equation  inhibits LLVM's ability to select instructions\n+        // like `lea`. We compute `(round_up_to_next_alignment(p, a) - p)` instead. This\n+        // redistributes operations around the load-bearing, but pessimizing `and` instruction\n+        // sufficiently for LLVM to be able to utilize the various optimizations it knows about.\n         //\n-        // which distributes operations around the load-bearing, but pessimizing `and` sufficiently\n-        // for LLVM to be able to utilize the various optimizations it knows about.\n-        return wrapping_sub(wrapping_add(addr, a_minus_one) & wrapping_sub(0, a), addr);\n-    }\n+        // LLVM handles the branch here particularly nicely. If this branch needs to be evaluated\n+        // at runtime, it will produce a mask `if addr_mod_stride == 0 { 0 } else { usize::MAX }`\n+        // in a branch-free way and then bitwise-OR it with whatever result the `-p mod a`\n+        // computation produces.\n+\n+        // SAFETY: `stride == 0` case has been handled by the special case above.\n+        let addr_mod_stride = unsafe { unchecked_rem(addr, stride) };\n \n-    let pmoda = addr & a_minus_one;\n-    if pmoda == 0 {\n-        // Already aligned. Yay!\n-        return 0;\n-    } else if stride == 0 {\n-        // If the pointer is not aligned, and the element is zero-sized, then no amount of\n-        // elements will ever align the pointer.\n-        return usize::MAX;\n+        return if addr_mod_stride == 0 {\n+            let aligned_address = wrapping_add(addr, a_minus_one) & wrapping_sub(0, a);\n+            let byte_offset = wrapping_sub(aligned_address, addr);\n+            // SAFETY: `stride` is non-zero. This is guaranteed to divide exactly as well, because\n+            // addr has been verified to be aligned to the original type\u2019s alignment requirements.\n+            unsafe { exact_div(byte_offset, stride) }\n+        } else {\n+            usize::MAX\n+        };\n     }\n \n-    let smoda = stride & a_minus_one;\n+    // GENERAL_CASE: From here on we\u2019re handling the very general case where `addr` may be\n+    // misaligned, there isn\u2019t an obvious relationship between `stride` and `a` that we can take an\n+    // advantage of, etc. This case produces machine code that isn\u2019t particularly high quality,\n+    // compared to the special cases above. The code produced here is still within the realm of\n+    // miracles, given the situations this case has to deal with.\n+\n     // SAFETY: a is power-of-two hence non-zero. stride == 0 case is handled above.\n-    let gcdpow = unsafe { intrinsics::cttz_nonzero(stride).min(intrinsics::cttz_nonzero(a)) };\n+    let gcdpow = unsafe { cttz_nonzero(stride).min(cttz_nonzero(a)) };\n     // SAFETY: gcdpow has an upper-bound that\u2019s at most the number of bits in a usize.\n     let gcd = unsafe { unchecked_shl(1usize, gcdpow) };\n-\n     // SAFETY: gcd is always greater or equal to 1.\n     if addr & unsafe { unchecked_sub(gcd, 1) } == 0 {\n         // This branch solves for the following linear congruence equation:\n@@ -1693,14 +1717,13 @@ pub(crate) unsafe fn align_offset<T: Sized>(p: *const T, a: usize) -> usize {\n         // ` p' + s'o = 0 mod a' `\n         // ` o = (a' - (p' mod a')) * (s'^-1 mod a') `\n         //\n-        // The first term is \"the relative alignment of `p` to `a`\" (divided by the `g`), the second\n-        // term is \"how does incrementing `p` by `s` bytes change the relative alignment of `p`\" (again\n-        // divided by `g`).\n-        // Division by `g` is necessary to make the inverse well formed if `a` and `s` are not\n-        // co-prime.\n+        // The first term is \"the relative alignment of `p` to `a`\" (divided by the `g`), the\n+        // second term is \"how does incrementing `p` by `s` bytes change the relative alignment of\n+        // `p`\" (again divided by `g`). Division by `g` is necessary to make the inverse well\n+        // formed if `a` and `s` are not co-prime.\n         //\n         // Furthermore, the result produced by this solution is not \"minimal\", so it is necessary\n-        // to take the result `o mod lcm(s, a)`. We can replace `lcm(s, a)` with just a `a'`.\n+        // to take the result `o mod lcm(s, a)`. This `lcm(s, a)` is the same as `a'`.\n \n         // SAFETY: `gcdpow` has an upper-bound not greater than the number of trailing 0-bits in\n         // `a`.\n@@ -1710,11 +1733,11 @@ pub(crate) unsafe fn align_offset<T: Sized>(p: *const T, a: usize) -> usize {\n         let a2minus1 = unsafe { unchecked_sub(a2, 1) };\n         // SAFETY: `gcdpow` has an upper-bound not greater than the number of trailing 0-bits in\n         // `a`.\n-        let s2 = unsafe { unchecked_shr(smoda, gcdpow) };\n+        let s2 = unsafe { unchecked_shr(stride & a_minus_one, gcdpow) };\n         // SAFETY: `gcdpow` has an upper-bound not greater than the number of trailing 0-bits in\n         // `a`. Furthermore, the subtraction cannot overflow, because `a2 = a >> gcdpow` will\n         // always be strictly greater than `(p % a) >> gcdpow`.\n-        let minusp2 = unsafe { unchecked_sub(a2, unchecked_shr(pmoda, gcdpow)) };\n+        let minusp2 = unsafe { unchecked_sub(a2, unchecked_shr(addr & a_minus_one, gcdpow)) };\n         // SAFETY: `a2` is a power-of-two, as proven above. `s2` is strictly less than `a2`\n         // because `(s % a) >> gcdpow` is strictly less than `a >> gcdpow`.\n         return wrapping_mul(minusp2, unsafe { mod_inv(s2, a2) }) & a2minus1;"}, {"sha": "cc159741d79ec2755c1bdd43d24d7f6154a124fd", "filename": "library/core/tests/ptr.rs", "status": "modified", "additions": 36, "deletions": 27, "changes": 63, "blob_url": "https://github.com/rust-lang/rust/blob/62a182cf7f91b11bfbf200ede67569da0f6488ec/library%2Fcore%2Ftests%2Fptr.rs", "raw_url": "https://github.com/rust-lang/rust/raw/62a182cf7f91b11bfbf200ede67569da0f6488ec/library%2Fcore%2Ftests%2Fptr.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/library%2Fcore%2Ftests%2Fptr.rs?ref=62a182cf7f91b11bfbf200ede67569da0f6488ec", "patch": "@@ -362,7 +362,7 @@ fn align_offset_zst() {\n }\n \n #[test]\n-fn align_offset_stride1() {\n+fn align_offset_stride_one() {\n     // For pointers of stride = 1, the pointer can always be aligned. The offset is equal to\n     // number of bytes.\n     let mut align = 1;\n@@ -383,24 +383,8 @@ fn align_offset_stride1() {\n }\n \n #[test]\n-fn align_offset_weird_strides() {\n-    #[repr(packed)]\n-    struct A3(u16, u8);\n-    struct A4(u32);\n-    #[repr(packed)]\n-    struct A5(u32, u8);\n-    #[repr(packed)]\n-    struct A6(u32, u16);\n-    #[repr(packed)]\n-    struct A7(u32, u16, u8);\n-    #[repr(packed)]\n-    struct A8(u32, u32);\n-    #[repr(packed)]\n-    struct A9(u32, u32, u8);\n-    #[repr(packed)]\n-    struct A10(u32, u32, u16);\n-\n-    unsafe fn test_weird_stride<T>(ptr: *const T, align: usize) -> bool {\n+fn align_offset_various_strides() {\n+    unsafe fn test_stride<T>(ptr: *const T, align: usize) -> bool {\n         let numptr = ptr as usize;\n         let mut expected = usize::MAX;\n         // Naive but definitely correct way to find the *first* aligned element of stride::<T>.\n@@ -434,14 +418,39 @@ fn align_offset_weird_strides() {\n     while align < limit {\n         for ptr in 1usize..4 * align {\n             unsafe {\n-                x |= test_weird_stride::<A3>(ptr::invalid::<A3>(ptr), align);\n-                x |= test_weird_stride::<A4>(ptr::invalid::<A4>(ptr), align);\n-                x |= test_weird_stride::<A5>(ptr::invalid::<A5>(ptr), align);\n-                x |= test_weird_stride::<A6>(ptr::invalid::<A6>(ptr), align);\n-                x |= test_weird_stride::<A7>(ptr::invalid::<A7>(ptr), align);\n-                x |= test_weird_stride::<A8>(ptr::invalid::<A8>(ptr), align);\n-                x |= test_weird_stride::<A9>(ptr::invalid::<A9>(ptr), align);\n-                x |= test_weird_stride::<A10>(ptr::invalid::<A10>(ptr), align);\n+                #[repr(packed)]\n+                struct A3(u16, u8);\n+                x |= test_stride::<A3>(ptr::invalid::<A3>(ptr), align);\n+\n+                struct A4(u32);\n+                x |= test_stride::<A4>(ptr::invalid::<A4>(ptr), align);\n+\n+                #[repr(packed)]\n+                struct A5(u32, u8);\n+                x |= test_stride::<A5>(ptr::invalid::<A5>(ptr), align);\n+\n+                #[repr(packed)]\n+                struct A6(u32, u16);\n+                x |= test_stride::<A6>(ptr::invalid::<A6>(ptr), align);\n+\n+                #[repr(packed)]\n+                struct A7(u32, u16, u8);\n+                x |= test_stride::<A7>(ptr::invalid::<A7>(ptr), align);\n+\n+                #[repr(packed)]\n+                struct A8(u32, u32);\n+                x |= test_stride::<A8>(ptr::invalid::<A8>(ptr), align);\n+\n+                #[repr(packed)]\n+                struct A9(u32, u32, u8);\n+                x |= test_stride::<A9>(ptr::invalid::<A9>(ptr), align);\n+\n+                #[repr(packed)]\n+                struct A10(u32, u32, u16);\n+                x |= test_stride::<A10>(ptr::invalid::<A10>(ptr), align);\n+\n+                x |= test_stride::<u32>(ptr::invalid::<u32>(ptr), align);\n+                x |= test_stride::<u128>(ptr::invalid::<u128>(ptr), align);\n             }\n         }\n         align = (align + 1).next_power_of_two();"}, {"sha": "c5eefca3467bbb91f8ee321a14e999072a172483", "filename": "src/test/assembly/align_offset.rs", "status": "added", "additions": 48, "deletions": 0, "changes": 48, "blob_url": "https://github.com/rust-lang/rust/blob/62a182cf7f91b11bfbf200ede67569da0f6488ec/src%2Ftest%2Fassembly%2Falign_offset.rs", "raw_url": "https://github.com/rust-lang/rust/raw/62a182cf7f91b11bfbf200ede67569da0f6488ec/src%2Ftest%2Fassembly%2Falign_offset.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ftest%2Fassembly%2Falign_offset.rs?ref=62a182cf7f91b11bfbf200ede67569da0f6488ec", "patch": "@@ -0,0 +1,48 @@\n+// assembly-output: emit-asm\n+// compile-flags: -Copt-level=1\n+// only-x86_64\n+// min-llvm-version: 14.0\n+#![crate_type=\"rlib\"]\n+\n+// CHECK-LABEL: align_offset_byte_ptr\n+// CHECK: leaq 31\n+// CHECK: andq $-32\n+// CHECK: subq\n+#[no_mangle]\n+pub fn align_offset_byte_ptr(ptr: *const u8) -> usize {\n+    ptr.align_offset(32)\n+}\n+\n+// CHECK-LABEL: align_offset_byte_slice\n+// CHECK: leaq 31\n+// CHECK: andq $-32\n+// CHECK: subq\n+#[no_mangle]\n+pub fn align_offset_byte_slice(slice: &[u8]) -> usize {\n+    slice.as_ptr().align_offset(32)\n+}\n+\n+// CHECK-LABEL: align_offset_word_ptr\n+// CHECK: leaq 31\n+// CHECK: andq $-32\n+// CHECK: subq\n+// CHECK: shrq\n+// This `ptr` is not known to be aligned, so it is required to check if it is at all possible to\n+// align. LLVM applies a simple mask.\n+// CHECK: orq\n+#[no_mangle]\n+pub fn align_offset_word_ptr(ptr: *const u32) -> usize {\n+    ptr.align_offset(32)\n+}\n+\n+// CHECK-LABEL: align_offset_word_slice\n+// CHECK: leaq 31\n+// CHECK: andq $-32\n+// CHECK: subq\n+// CHECK: shrq\n+// `slice` is known to be aligned, so `!0` is not possible as a return\n+// CHECK-NOT: orq\n+#[no_mangle]\n+pub fn align_offset_word_slice(slice: &[u32]) -> usize {\n+    slice.as_ptr().align_offset(32)\n+}"}]}