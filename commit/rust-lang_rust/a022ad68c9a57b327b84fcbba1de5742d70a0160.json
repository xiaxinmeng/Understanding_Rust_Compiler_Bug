{"sha": "a022ad68c9a57b327b84fcbba1de5742d70a0160", "node_id": "C_kwDOAAsO6NoAKGEwMjJhZDY4YzlhNTdiMzI3Yjg0ZmNiYmExZGU1NzQyZDcwYTAxNjA", "commit": {"author": {"name": "Aleksey Kladov", "email": "aleksey.kladov@gmail.com", "date": "2021-12-18T14:20:38Z"}, "committer": {"name": "Aleksey Kladov", "email": "aleksey.kladov@gmail.com", "date": "2021-12-18T14:20:38Z"}, "message": "internal: move all the lexing to the parser crate", "tree": {"sha": "5e4b34f2c71b149765b1417835673645a26cb8de", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/5e4b34f2c71b149765b1417835673645a26cb8de"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/a022ad68c9a57b327b84fcbba1de5742d70a0160", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/a022ad68c9a57b327b84fcbba1de5742d70a0160", "html_url": "https://github.com/rust-lang/rust/commit/a022ad68c9a57b327b84fcbba1de5742d70a0160", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/a022ad68c9a57b327b84fcbba1de5742d70a0160/comments", "author": {"login": "matklad", "id": 1711539, "node_id": "MDQ6VXNlcjE3MTE1Mzk=", "avatar_url": "https://avatars.githubusercontent.com/u/1711539?v=4", "gravatar_id": "", "url": "https://api.github.com/users/matklad", "html_url": "https://github.com/matklad", "followers_url": "https://api.github.com/users/matklad/followers", "following_url": "https://api.github.com/users/matklad/following{/other_user}", "gists_url": "https://api.github.com/users/matklad/gists{/gist_id}", "starred_url": "https://api.github.com/users/matklad/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/matklad/subscriptions", "organizations_url": "https://api.github.com/users/matklad/orgs", "repos_url": "https://api.github.com/users/matklad/repos", "events_url": "https://api.github.com/users/matklad/events{/privacy}", "received_events_url": "https://api.github.com/users/matklad/received_events", "type": "User", "site_admin": false}, "committer": {"login": "matklad", "id": 1711539, "node_id": "MDQ6VXNlcjE3MTE1Mzk=", "avatar_url": "https://avatars.githubusercontent.com/u/1711539?v=4", "gravatar_id": "", "url": "https://api.github.com/users/matklad", "html_url": "https://github.com/matklad", "followers_url": "https://api.github.com/users/matklad/followers", "following_url": "https://api.github.com/users/matklad/following{/other_user}", "gists_url": "https://api.github.com/users/matklad/gists{/gist_id}", "starred_url": "https://api.github.com/users/matklad/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/matklad/subscriptions", "organizations_url": "https://api.github.com/users/matklad/orgs", "repos_url": "https://api.github.com/users/matklad/repos", "events_url": "https://api.github.com/users/matklad/events{/privacy}", "received_events_url": "https://api.github.com/users/matklad/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "78926027e33573a348c17d9bc4c5d4ca09718f96", "url": "https://api.github.com/repos/rust-lang/rust/commits/78926027e33573a348c17d9bc4c5d4ca09718f96", "html_url": "https://github.com/rust-lang/rust/commit/78926027e33573a348c17d9bc4c5d4ca09718f96"}], "stats": {"total": 626, "additions": 159, "deletions": 467}, "files": [{"sha": "44ef223557b9941733609ddb48a682d11a1759f1", "filename": "Cargo.lock", "status": "modified", "additions": 3, "deletions": 0, "changes": 3, "blob_url": "https://github.com/rust-lang/rust/blob/a022ad68c9a57b327b84fcbba1de5742d70a0160/Cargo.lock", "raw_url": "https://github.com/rust-lang/rust/raw/a022ad68c9a57b327b84fcbba1de5742d70a0160/Cargo.lock", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/Cargo.lock?ref=a022ad68c9a57b327b84fcbba1de5742d70a0160", "patch": "@@ -609,6 +609,7 @@ dependencies = [\n  \"hir\",\n  \"ide_db\",\n  \"itertools\",\n+ \"parser\",\n  \"profile\",\n  \"rustc-hash\",\n  \"sourcegen\",\n@@ -654,6 +655,7 @@ dependencies = [\n  \"itertools\",\n  \"limit\",\n  \"once_cell\",\n+ \"parser\",\n  \"profile\",\n  \"rayon\",\n  \"rustc-hash\",\n@@ -695,6 +697,7 @@ dependencies = [\n  \"hir\",\n  \"ide_db\",\n  \"itertools\",\n+ \"parser\",\n  \"rustc-hash\",\n  \"syntax\",\n  \"test_utils\","}, {"sha": "3cd186fdf554d814f3ec26a62a455518870d872d", "filename": "crates/ide_assists/Cargo.toml", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "blob_url": "https://github.com/rust-lang/rust/blob/a022ad68c9a57b327b84fcbba1de5742d70a0160/crates%2Fide_assists%2FCargo.toml", "raw_url": "https://github.com/rust-lang/rust/raw/a022ad68c9a57b327b84fcbba1de5742d70a0160/crates%2Fide_assists%2FCargo.toml", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fide_assists%2FCargo.toml?ref=a022ad68c9a57b327b84fcbba1de5742d70a0160", "patch": "@@ -16,6 +16,7 @@ itertools = \"0.10.0\"\n either = \"1.6.1\"\n \n stdx = { path = \"../stdx\", version = \"0.0.0\" }\n+parser = { path = \"../parser\", version = \"0.0.0\" }\n syntax = { path = \"../syntax\", version = \"0.0.0\" }\n text_edit = { path = \"../text_edit\", version = \"0.0.0\" }\n profile = { path = \"../profile\", version = \"0.0.0\" }"}, {"sha": "f91b2fe44e48e5ba042593a82065aa7b2418470e", "filename": "crates/ide_assists/src/utils/suggest_name.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/a022ad68c9a57b327b84fcbba1de5742d70a0160/crates%2Fide_assists%2Fsrc%2Futils%2Fsuggest_name.rs", "raw_url": "https://github.com/rust-lang/rust/raw/a022ad68c9a57b327b84fcbba1de5742d70a0160/crates%2Fide_assists%2Fsrc%2Futils%2Fsuggest_name.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fide_assists%2Fsrc%2Futils%2Fsuggest_name.rs?ref=a022ad68c9a57b327b84fcbba1de5742d70a0160", "patch": "@@ -135,7 +135,7 @@ fn normalize(name: &str) -> Option<String> {\n }\n \n fn is_valid_name(name: &str) -> bool {\n-    match syntax::lex_single_syntax_kind(name) {\n+    match parser::LexedStr::single_token(name) {\n         Some((syntax::SyntaxKind::IDENT, _error)) => true,\n         _ => false,\n     }"}, {"sha": "cfcf9f56c87c96c7def4c0d31e09dadd59ea9598", "filename": "crates/ide_db/Cargo.toml", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "blob_url": "https://github.com/rust-lang/rust/blob/a022ad68c9a57b327b84fcbba1de5742d70a0160/crates%2Fide_db%2FCargo.toml", "raw_url": "https://github.com/rust-lang/rust/raw/a022ad68c9a57b327b84fcbba1de5742d70a0160/crates%2Fide_db%2FCargo.toml", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fide_db%2FCargo.toml?ref=a022ad68c9a57b327b84fcbba1de5742d70a0160", "patch": "@@ -22,6 +22,7 @@ arrayvec = \"0.7\"\n indexmap = \"1.7\"\n \n stdx = { path = \"../stdx\", version = \"0.0.0\" }\n+parser = { path = \"../parser\", version = \"0.0.0\" }\n syntax = { path = \"../syntax\", version = \"0.0.0\" }\n text_edit = { path = \"../text_edit\", version = \"0.0.0\" }\n base_db = { path = \"../base_db\", version = \"0.0.0\" }"}, {"sha": "188499db72cf02bd0d9ac21b64418947823ba709", "filename": "crates/ide_db/src/rename.rs", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/rust-lang/rust/blob/a022ad68c9a57b327b84fcbba1de5742d70a0160/crates%2Fide_db%2Fsrc%2Frename.rs", "raw_url": "https://github.com/rust-lang/rust/raw/a022ad68c9a57b327b84fcbba1de5742d70a0160/crates%2Fide_db%2Fsrc%2Frename.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fide_db%2Fsrc%2Frename.rs?ref=a022ad68c9a57b327b84fcbba1de5742d70a0160", "patch": "@@ -28,7 +28,7 @@ use hir::{AsAssocItem, FieldSource, HasSource, InFile, ModuleSource, Semantics};\n use stdx::never;\n use syntax::{\n     ast::{self, HasName},\n-    lex_single_syntax_kind, AstNode, SyntaxKind, TextRange, T,\n+    AstNode, SyntaxKind, TextRange, T,\n };\n use text_edit::{TextEdit, TextEditBuilder};\n \n@@ -490,7 +490,7 @@ pub enum IdentifierKind {\n \n impl IdentifierKind {\n     pub fn classify(new_name: &str) -> Result<IdentifierKind> {\n-        match lex_single_syntax_kind(new_name) {\n+        match parser::LexedStr::single_token(new_name) {\n             Some(res) => match res {\n                 (SyntaxKind::IDENT, _) => Ok(IdentifierKind::Ident),\n                 (T![_], _) => Ok(IdentifierKind::Underscore),"}, {"sha": "9a8221ac6c42ca29971cd6c7f1524917689a391c", "filename": "crates/ide_ssr/Cargo.toml", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "blob_url": "https://github.com/rust-lang/rust/blob/a022ad68c9a57b327b84fcbba1de5742d70a0160/crates%2Fide_ssr%2FCargo.toml", "raw_url": "https://github.com/rust-lang/rust/raw/a022ad68c9a57b327b84fcbba1de5742d70a0160/crates%2Fide_ssr%2FCargo.toml", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fide_ssr%2FCargo.toml?ref=a022ad68c9a57b327b84fcbba1de5742d70a0160", "patch": "@@ -16,6 +16,7 @@ rustc-hash = \"1.1.0\"\n itertools = \"0.10.0\"\n \n text_edit = { path = \"../text_edit\", version = \"0.0.0\" }\n+parser = { path = \"../parser\", version = \"0.0.0\" }\n syntax = { path = \"../syntax\", version = \"0.0.0\" }\n ide_db = { path = \"../ide_db\", version = \"0.0.0\" }\n hir = { path = \"../hir\", version = \"0.0.0\" }"}, {"sha": "ae7d5b4bf156d9df3ba3af5312ee0372fbd6c3f3", "filename": "crates/ide_ssr/src/parsing.rs", "status": "modified", "additions": 4, "deletions": 10, "changes": 14, "blob_url": "https://github.com/rust-lang/rust/blob/a022ad68c9a57b327b84fcbba1de5742d70a0160/crates%2Fide_ssr%2Fsrc%2Fparsing.rs", "raw_url": "https://github.com/rust-lang/rust/raw/a022ad68c9a57b327b84fcbba1de5742d70a0160/crates%2Fide_ssr%2Fsrc%2Fparsing.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fide_ssr%2Fsrc%2Fparsing.rs?ref=a022ad68c9a57b327b84fcbba1de5742d70a0160", "patch": "@@ -256,19 +256,13 @@ fn validate_rule(rule: &SsrRule) -> Result<(), SsrError> {\n }\n \n fn tokenize(source: &str) -> Result<Vec<Token>, SsrError> {\n-    let mut start = 0;\n-    let (raw_tokens, errors) = syntax::tokenize(source);\n-    if let Some(first_error) = errors.first() {\n+    let lexed = parser::LexedStr::new(source);\n+    if let Some((_, first_error)) = lexed.errors().next() {\n         bail!(\"Failed to parse pattern: {}\", first_error);\n     }\n     let mut tokens: Vec<Token> = Vec::new();\n-    for raw_token in raw_tokens {\n-        let token_len = usize::from(raw_token.len);\n-        tokens.push(Token {\n-            kind: raw_token.kind,\n-            text: SmolStr::new(&source[start..start + token_len]),\n-        });\n-        start += token_len;\n+    for i in 0..lexed.len() {\n+        tokens.push(Token { kind: lexed.kind(i), text: lexed.text(i).into() });\n     }\n     Ok(tokens)\n }"}, {"sha": "109842b0cd06f57d842b32f10fa0924158217eb4", "filename": "crates/mbe/src/syntax_bridge.rs", "status": "modified", "additions": 50, "deletions": 50, "changes": 100, "blob_url": "https://github.com/rust-lang/rust/blob/a022ad68c9a57b327b84fcbba1de5742d70a0160/crates%2Fmbe%2Fsrc%2Fsyntax_bridge.rs", "raw_url": "https://github.com/rust-lang/rust/raw/a022ad68c9a57b327b84fcbba1de5742d70a0160/crates%2Fmbe%2Fsrc%2Fsyntax_bridge.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fmbe%2Fsrc%2Fsyntax_bridge.rs?ref=a022ad68c9a57b327b84fcbba1de5742d70a0160", "patch": "@@ -4,10 +4,9 @@ use parser::{ParseError, TreeSink};\n use rustc_hash::{FxHashMap, FxHashSet};\n use syntax::{\n     ast::{self, make::tokens::doc_comment},\n-    tokenize, AstToken, Parse, PreorderWithTokens, SmolStr, SyntaxElement, SyntaxKind,\n+    AstToken, Parse, PreorderWithTokens, SmolStr, SyntaxElement, SyntaxKind,\n     SyntaxKind::*,\n-    SyntaxNode, SyntaxToken, SyntaxTreeBuilder, TextRange, TextSize, Token as RawToken, WalkEvent,\n-    T,\n+    SyntaxNode, SyntaxToken, SyntaxTreeBuilder, TextRange, TextSize, WalkEvent, T,\n };\n use tt::buffer::{Cursor, TokenBuffer};\n \n@@ -69,15 +68,14 @@ pub fn token_tree_to_syntax_node(\n \n /// Convert a string to a `TokenTree`\n pub fn parse_to_token_tree(text: &str) -> Option<(tt::Subtree, TokenMap)> {\n-    let (tokens, errors) = tokenize(text);\n-    if !errors.is_empty() {\n+    let lexed = parser::LexedStr::new(text);\n+    if lexed.errors().next().is_some() {\n         return None;\n     }\n \n     let mut conv = RawConvertor {\n-        text,\n-        offset: TextSize::default(),\n-        inner: tokens.iter(),\n+        lexed: lexed,\n+        pos: 0,\n         id_alloc: TokenIdAlloc {\n             map: Default::default(),\n             global_offset: TextSize::default(),\n@@ -146,7 +144,7 @@ fn convert_tokens<C: TokenConvertor>(conv: &mut C) -> tt::Subtree {\n             Some(it) => it,\n         };\n \n-        let k: SyntaxKind = token.kind();\n+        let k: SyntaxKind = token.kind(&conv);\n         if k == COMMENT {\n             if let Some(tokens) = conv.convert_doc_comment(&token) {\n                 // FIXME: There has to be a better way to do this\n@@ -199,19 +197,19 @@ fn convert_tokens<C: TokenConvertor>(conv: &mut C) -> tt::Subtree {\n             } else {\n                 let spacing = match conv.peek() {\n                     Some(next)\n-                        if next.kind().is_trivia()\n-                            || next.kind() == T!['[']\n-                            || next.kind() == T!['{']\n-                            || next.kind() == T!['('] =>\n+                        if next.kind(&conv).is_trivia()\n+                            || next.kind(&conv) == T!['[']\n+                            || next.kind(&conv) == T!['{']\n+                            || next.kind(&conv) == T!['('] =>\n                     {\n                         tt::Spacing::Alone\n                     }\n-                    Some(next) if next.kind().is_punct() && next.kind() != UNDERSCORE => {\n+                    Some(next) if next.kind(&conv).is_punct() && next.kind(&conv) != UNDERSCORE => {\n                         tt::Spacing::Joint\n                     }\n                     _ => tt::Spacing::Alone,\n                 };\n-                let char = match token.to_char() {\n+                let char = match token.to_char(&conv) {\n                     Some(c) => c,\n                     None => {\n                         panic!(\"Token from lexer must be single char: token = {:#?}\", token);\n@@ -222,7 +220,7 @@ fn convert_tokens<C: TokenConvertor>(conv: &mut C) -> tt::Subtree {\n         } else {\n             macro_rules! make_leaf {\n                 ($i:ident) => {\n-                    tt::$i { id: conv.id_alloc().alloc(range), text: token.to_text() }.into()\n+                    tt::$i { id: conv.id_alloc().alloc(range), text: token.to_text(conv) }.into()\n                 };\n             }\n             let leaf: tt::Leaf = match k {\n@@ -243,7 +241,7 @@ fn convert_tokens<C: TokenConvertor>(conv: &mut C) -> tt::Subtree {\n \n                     let r = TextRange::at(range.start() + char_unit, range.len() - char_unit);\n                     let ident = tt::Leaf::from(tt::Ident {\n-                        text: SmolStr::new(&token.to_text()[1..]),\n+                        text: SmolStr::new(&token.to_text(conv)[1..]),\n                         id: conv.id_alloc().alloc(r),\n                     });\n                     result.push(ident.into());\n@@ -392,22 +390,21 @@ impl TokenIdAlloc {\n \n /// A Raw Token (straightly from lexer) convertor\n struct RawConvertor<'a> {\n-    text: &'a str,\n-    offset: TextSize,\n+    lexed: parser::LexedStr<'a>,\n+    pos: usize,\n     id_alloc: TokenIdAlloc,\n-    inner: std::slice::Iter<'a, RawToken>,\n }\n \n-trait SrcToken: std::fmt::Debug {\n-    fn kind(&self) -> SyntaxKind;\n+trait SrcToken<Ctx>: std::fmt::Debug {\n+    fn kind(&self, ctx: &Ctx) -> SyntaxKind;\n \n-    fn to_char(&self) -> Option<char>;\n+    fn to_char(&self, ctx: &Ctx) -> Option<char>;\n \n-    fn to_text(&self) -> SmolStr;\n+    fn to_text(&self, ctx: &Ctx) -> SmolStr;\n }\n \n-trait TokenConvertor {\n-    type Token: SrcToken;\n+trait TokenConvertor: Sized {\n+    type Token: SrcToken<Self>;\n \n     fn convert_doc_comment(&self, token: &Self::Token) -> Option<Vec<tt::TokenTree>>;\n \n@@ -418,42 +415,45 @@ trait TokenConvertor {\n     fn id_alloc(&mut self) -> &mut TokenIdAlloc;\n }\n \n-impl<'a> SrcToken for (&'a RawToken, &'a str) {\n-    fn kind(&self) -> SyntaxKind {\n-        self.0.kind\n+impl<'a> SrcToken<RawConvertor<'a>> for usize {\n+    fn kind(&self, ctx: &RawConvertor<'a>) -> SyntaxKind {\n+        ctx.lexed.kind(*self)\n     }\n \n-    fn to_char(&self) -> Option<char> {\n-        self.1.chars().next()\n+    fn to_char(&self, ctx: &RawConvertor<'a>) -> Option<char> {\n+        ctx.lexed.text(*self).chars().next()\n     }\n \n-    fn to_text(&self) -> SmolStr {\n-        self.1.into()\n+    fn to_text(&self, ctx: &RawConvertor<'_>) -> SmolStr {\n+        ctx.lexed.text(*self).into()\n     }\n }\n \n impl<'a> TokenConvertor for RawConvertor<'a> {\n-    type Token = (&'a RawToken, &'a str);\n+    type Token = usize;\n \n-    fn convert_doc_comment(&self, token: &Self::Token) -> Option<Vec<tt::TokenTree>> {\n-        convert_doc_comment(&doc_comment(token.1))\n+    fn convert_doc_comment(&self, token: &usize) -> Option<Vec<tt::TokenTree>> {\n+        let text = self.lexed.text(*token);\n+        convert_doc_comment(&doc_comment(text))\n     }\n \n     fn bump(&mut self) -> Option<(Self::Token, TextRange)> {\n-        let token = self.inner.next()?;\n-        let range = TextRange::at(self.offset, token.len);\n-        self.offset += token.len;\n+        if self.pos == self.lexed.len() {\n+            return None;\n+        }\n+        let token = self.pos;\n+        self.pos += 1;\n+        let range = self.lexed.text_range(token);\n+        let range = TextRange::new(range.start.try_into().unwrap(), range.end.try_into().unwrap());\n \n-        Some(((token, &self.text[range]), range))\n+        Some((token, range))\n     }\n \n     fn peek(&self) -> Option<Self::Token> {\n-        let token = self.inner.as_slice().get(0);\n-\n-        token.map(|it| {\n-            let range = TextRange::at(self.offset, it.len);\n-            (it, &self.text[range])\n-        })\n+        if self.pos == self.lexed.len() {\n+            return None;\n+        }\n+        Some(self.pos)\n     }\n \n     fn id_alloc(&mut self) -> &mut TokenIdAlloc {\n@@ -523,17 +523,17 @@ impl SynToken {\n     }\n }\n \n-impl SrcToken for SynToken {\n-    fn kind(&self) -> SyntaxKind {\n+impl<'a> SrcToken<Convertor<'a>> for SynToken {\n+    fn kind(&self, _ctx: &Convertor<'a>) -> SyntaxKind {\n         self.token().kind()\n     }\n-    fn to_char(&self) -> Option<char> {\n+    fn to_char(&self, _ctx: &Convertor<'a>) -> Option<char> {\n         match self {\n             SynToken::Ordinary(_) => None,\n             SynToken::Punch(it, i) => it.text().chars().nth((*i).into()),\n         }\n     }\n-    fn to_text(&self) -> SmolStr {\n+    fn to_text(&self, _ctx: &Convertor<'a>) -> SmolStr {\n         self.token().text().into()\n     }\n }"}, {"sha": "f419c78d46cc4def294df68ddb42c2f265ad55d8", "filename": "crates/mbe/src/to_parser_tokens.rs", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/rust-lang/rust/blob/a022ad68c9a57b327b84fcbba1de5742d70a0160/crates%2Fmbe%2Fsrc%2Fto_parser_tokens.rs", "raw_url": "https://github.com/rust-lang/rust/raw/a022ad68c9a57b327b84fcbba1de5742d70a0160/crates%2Fmbe%2Fsrc%2Fto_parser_tokens.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fmbe%2Fsrc%2Fto_parser_tokens.rs?ref=a022ad68c9a57b327b84fcbba1de5742d70a0160", "patch": "@@ -1,7 +1,7 @@\n //! Convert macro-by-example tokens which are specific to macro expansion into a\n //! format that works for our parser.\n \n-use syntax::{lex_single_syntax_kind, SyntaxKind, SyntaxKind::*, T};\n+use syntax::{SyntaxKind, SyntaxKind::*, T};\n use tt::buffer::TokenBuffer;\n \n pub(crate) fn to_parser_tokens(buffer: &TokenBuffer) -> parser::Tokens {\n@@ -35,7 +35,7 @@ pub(crate) fn to_parser_tokens(buffer: &TokenBuffer) -> parser::Tokens {\n                         let is_negated = lit.text.starts_with('-');\n                         let inner_text = &lit.text[if is_negated { 1 } else { 0 }..];\n \n-                        let kind = lex_single_syntax_kind(inner_text)\n+                        let kind = parser::LexedStr::single_token(inner_text)\n                             .map(|(kind, _error)| kind)\n                             .filter(|kind| {\n                                 kind.is_literal()"}, {"sha": "1ef29b5210fb6e1d4612abcc412794d40e9a6049", "filename": "crates/parser/src/lexed_str.rs", "status": "modified", "additions": 33, "deletions": 7, "changes": 40, "blob_url": "https://github.com/rust-lang/rust/blob/a022ad68c9a57b327b84fcbba1de5742d70a0160/crates%2Fparser%2Fsrc%2Flexed_str.rs", "raw_url": "https://github.com/rust-lang/rust/raw/a022ad68c9a57b327b84fcbba1de5742d70a0160/crates%2Fparser%2Fsrc%2Flexed_str.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fparser%2Fsrc%2Flexed_str.rs?ref=a022ad68c9a57b327b84fcbba1de5742d70a0160", "patch": "@@ -8,6 +8,8 @@\n //! Note that these tokens, unlike the tokens we feed into the parser, do\n //! include info about comments and whitespace.\n \n+use std::ops;\n+\n use crate::{\n     SyntaxKind::{self, *},\n     T,\n@@ -52,7 +54,7 @@ impl<'a> LexedStr<'a> {\n         res\n     }\n \n-    pub fn single_token(text: &'a str) -> Option<SyntaxKind> {\n+    pub fn single_token(text: &'a str) -> Option<(SyntaxKind, Option<String>)> {\n         if text.is_empty() {\n             return None;\n         }\n@@ -63,11 +65,7 @@ impl<'a> LexedStr<'a> {\n         }\n \n         let (kind, err) = from_rustc(&token.kind, text);\n-        if err.is_some() {\n-            return None;\n-        }\n-\n-        Some(kind)\n+        Some((kind, err.map(|it| it.to_owned())))\n     }\n \n     pub fn as_str(&self) -> &str {\n@@ -78,16 +76,40 @@ impl<'a> LexedStr<'a> {\n         self.kind.len() - 1\n     }\n \n+    pub fn is_empty(&self) -> bool {\n+        self.len() == 0\n+    }\n+\n     pub fn kind(&self, i: usize) -> SyntaxKind {\n         assert!(i < self.len());\n         self.kind[i]\n     }\n \n     pub fn text(&self, i: usize) -> &str {\n+        self.range_text(i..i + 1)\n+    }\n+    pub fn range_text(&self, r: ops::Range<usize>) -> &str {\n+        assert!(r.start < r.end && r.end <= self.len());\n+        let lo = self.start[r.start] as usize;\n+        let hi = self.start[r.end] as usize;\n+        &self.text[lo..hi]\n+    }\n+\n+    // Naming is hard.\n+    pub fn text_range(&self, i: usize) -> ops::Range<usize> {\n         assert!(i < self.len());\n         let lo = self.start[i] as usize;\n         let hi = self.start[i + 1] as usize;\n-        &self.text[lo..hi]\n+        lo..hi\n+    }\n+    pub fn text_start(&self, i: usize) -> usize {\n+        assert!(i <= self.len());\n+        self.start[i] as usize\n+    }\n+    pub fn text_len(&self, i: usize) -> usize {\n+        assert!(i < self.len());\n+        let r = self.text_range(i);\n+        r.end - r.start\n     }\n \n     pub fn error(&self, i: usize) -> Option<&str> {\n@@ -96,6 +118,10 @@ impl<'a> LexedStr<'a> {\n         Some(self.error[err].msg.as_str())\n     }\n \n+    pub fn errors(&self) -> impl Iterator<Item = (usize, &str)> + '_ {\n+        self.error.iter().map(|it| (it.token as usize, it.msg.as_str()))\n+    }\n+\n     pub fn to_tokens(&self) -> crate::Tokens {\n         let mut res = crate::Tokens::default();\n         let mut was_joint = false;"}, {"sha": "65a6b7ac4e4a046773e66155507349987a913846", "filename": "crates/syntax/src/lib.rs", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "blob_url": "https://github.com/rust-lang/rust/blob/a022ad68c9a57b327b84fcbba1de5742d70a0160/crates%2Fsyntax%2Fsrc%2Flib.rs", "raw_url": "https://github.com/rust-lang/rust/raw/a022ad68c9a57b327b84fcbba1de5742d70a0160/crates%2Fsyntax%2Fsrc%2Flib.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fsyntax%2Fsrc%2Flib.rs?ref=a022ad68c9a57b327b84fcbba1de5742d70a0160", "patch": "@@ -48,7 +48,6 @@ use text_edit::Indel;\n \n pub use crate::{\n     ast::{AstNode, AstToken},\n-    parsing::lexer::{lex_single_syntax_kind, tokenize, Token},\n     ptr::{AstPtr, SyntaxNodePtr},\n     syntax_error::SyntaxError,\n     syntax_node::{"}, {"sha": "cba1ddde855aa1578ae8a1e8bc27cd57eb1e9554", "filename": "crates/syntax/src/parsing.rs", "status": "modified", "additions": 9, "deletions": 38, "changes": 47, "blob_url": "https://github.com/rust-lang/rust/blob/a022ad68c9a57b327b84fcbba1de5742d70a0160/crates%2Fsyntax%2Fsrc%2Fparsing.rs", "raw_url": "https://github.com/rust-lang/rust/raw/a022ad68c9a57b327b84fcbba1de5742d70a0160/crates%2Fsyntax%2Fsrc%2Fparsing.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fsyntax%2Fsrc%2Fparsing.rs?ref=a022ad68c9a57b327b84fcbba1de5742d70a0160", "patch": "@@ -1,7 +1,6 @@\n //! Lexing, bridging to parser (which does the actual parsing) and\n //! incremental reparsing.\n \n-pub(crate) mod lexer;\n mod text_tree_sink;\n mod reparsing;\n \n@@ -10,18 +9,17 @@ use text_tree_sink::TextTreeSink;\n \n use crate::{syntax_node::GreenNode, AstNode, SyntaxError, SyntaxNode};\n \n-pub(crate) use crate::parsing::{lexer::*, reparsing::incremental_reparse};\n+pub(crate) use crate::parsing::reparsing::incremental_reparse;\n \n pub(crate) fn parse_text(text: &str) -> (GreenNode, Vec<SyntaxError>) {\n-    let (lexer_tokens, lexer_errors) = tokenize(text);\n-    let parser_tokens = to_parser_tokens(text, &lexer_tokens);\n+    let lexed = parser::LexedStr::new(text);\n+    let parser_tokens = lexed.to_tokens();\n \n-    let mut tree_sink = TextTreeSink::new(text, &lexer_tokens);\n+    let mut tree_sink = TextTreeSink::new(lexed);\n \n     parser::parse_source_file(&parser_tokens, &mut tree_sink);\n \n-    let (tree, mut parser_errors) = tree_sink.finish();\n-    parser_errors.extend(lexer_errors);\n+    let (tree, parser_errors) = tree_sink.finish();\n \n     (tree, parser_errors)\n }\n@@ -31,14 +29,13 @@ pub(crate) fn parse_text_as<T: AstNode>(\n     text: &str,\n     entry_point: parser::ParserEntryPoint,\n ) -> Result<T, ()> {\n-    let (lexer_tokens, lexer_errors) = tokenize(text);\n-    if !lexer_errors.is_empty() {\n+    let lexed = parser::LexedStr::new(text);\n+    if lexed.errors().next().is_some() {\n         return Err(());\n     }\n+    let parser_tokens = lexed.to_tokens();\n \n-    let parser_tokens = to_parser_tokens(text, &lexer_tokens);\n-\n-    let mut tree_sink = TextTreeSink::new(text, &lexer_tokens);\n+    let mut tree_sink = TextTreeSink::new(lexed);\n \n     // TextTreeSink assumes that there's at least some root node to which it can attach errors and\n     // tokens. We arbitrarily give it a SourceFile.\n@@ -54,29 +51,3 @@ pub(crate) fn parse_text_as<T: AstNode>(\n \n     SyntaxNode::new_root(tree).first_child().and_then(T::cast).ok_or(())\n }\n-\n-pub(crate) fn to_parser_tokens(text: &str, lexer_tokens: &[lexer::Token]) -> ::parser::Tokens {\n-    let mut off = 0;\n-    let mut res = parser::Tokens::default();\n-    let mut was_joint = false;\n-    for t in lexer_tokens {\n-        if t.kind.is_trivia() {\n-            was_joint = false;\n-        } else {\n-            if t.kind == SyntaxKind::IDENT {\n-                let token_text = &text[off..][..usize::from(t.len)];\n-                let contextual_kw =\n-                    SyntaxKind::from_contextual_keyword(token_text).unwrap_or(SyntaxKind::IDENT);\n-                res.push_ident(contextual_kw);\n-            } else {\n-                if was_joint {\n-                    res.was_joint();\n-                }\n-                res.push(t.kind);\n-            }\n-            was_joint = true;\n-        }\n-        off += usize::from(t.len);\n-    }\n-    res\n-}"}, {"sha": "d94f5f067de058a4cbe650057cd2f4021c3c85a4", "filename": "crates/syntax/src/parsing/lexer.rs", "status": "removed", "additions": 0, "deletions": 249, "changes": 249, "blob_url": "https://github.com/rust-lang/rust/blob/78926027e33573a348c17d9bc4c5d4ca09718f96/crates%2Fsyntax%2Fsrc%2Fparsing%2Flexer.rs", "raw_url": "https://github.com/rust-lang/rust/raw/78926027e33573a348c17d9bc4c5d4ca09718f96/crates%2Fsyntax%2Fsrc%2Fparsing%2Flexer.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fsyntax%2Fsrc%2Fparsing%2Flexer.rs?ref=78926027e33573a348c17d9bc4c5d4ca09718f96", "patch": "@@ -1,249 +0,0 @@\n-//! Lexer analyzes raw input string and produces lexemes (tokens).\n-//! It is just a bridge to `rustc_lexer`.\n-\n-use std::convert::TryInto;\n-\n-use rustc_lexer::RawStrError;\n-\n-use crate::{\n-    SyntaxError,\n-    SyntaxKind::{self, *},\n-    TextRange, TextSize, T,\n-};\n-\n-/// A token of Rust source.\n-#[derive(Debug, Clone, Copy, PartialEq, Eq, PartialOrd, Ord, Hash)]\n-pub struct Token {\n-    /// The kind of token.\n-    pub kind: SyntaxKind,\n-    /// The length of the token.\n-    pub len: TextSize,\n-}\n-\n-/// Break a string up into its component tokens.\n-/// Beware that it checks for shebang first and its length contributes to resulting\n-/// tokens offsets.\n-pub fn tokenize(text: &str) -> (Vec<Token>, Vec<SyntaxError>) {\n-    // non-empty string is a precondition of `rustc_lexer::strip_shebang()`.\n-    if text.is_empty() {\n-        return Default::default();\n-    }\n-\n-    let mut tokens = Vec::new();\n-    let mut errors = Vec::new();\n-\n-    let mut offset = match rustc_lexer::strip_shebang(text) {\n-        Some(shebang_len) => {\n-            tokens.push(Token { kind: SHEBANG, len: shebang_len.try_into().unwrap() });\n-            shebang_len\n-        }\n-        None => 0,\n-    };\n-\n-    let text_without_shebang = &text[offset..];\n-\n-    for rustc_token in rustc_lexer::tokenize(text_without_shebang) {\n-        let token_len: TextSize = rustc_token.len.try_into().unwrap();\n-        let token_range = TextRange::at(offset.try_into().unwrap(), token_len);\n-\n-        let (syntax_kind, err_message) =\n-            rustc_token_kind_to_syntax_kind(&rustc_token.kind, &text[token_range]);\n-\n-        tokens.push(Token { kind: syntax_kind, len: token_len });\n-\n-        if let Some(err_message) = err_message {\n-            errors.push(SyntaxError::new(err_message, token_range));\n-        }\n-\n-        offset += rustc_token.len;\n-    }\n-\n-    (tokens, errors)\n-}\n-\n-/// Returns `SyntaxKind` and `Option<SyntaxError>` if `text` parses as a single token.\n-///\n-/// Returns `None` if the string contains zero *or two or more* tokens.\n-/// The token is malformed if the returned error is not `None`.\n-///\n-/// Beware that unescape errors are not checked at tokenization time.\n-pub fn lex_single_syntax_kind(text: &str) -> Option<(SyntaxKind, Option<SyntaxError>)> {\n-    let (first_token, err) = lex_first_token(text)?;\n-    if first_token.len != TextSize::of(text) {\n-        return None;\n-    }\n-    Some((first_token.kind, err))\n-}\n-\n-/// Returns `SyntaxKind` and `Option<SyntaxError>` of the first token\n-/// encountered at the beginning of the string.\n-///\n-/// Returns `None` if the string contains zero tokens or if the token was parsed\n-/// with an error.\n-/// The token is malformed if the returned error is not `None`.\n-///\n-/// Beware that unescape errors are not checked at tokenization time.\n-fn lex_first_token(text: &str) -> Option<(Token, Option<SyntaxError>)> {\n-    // non-empty string is a precondition of `rustc_lexer::first_token()`.\n-    if text.is_empty() {\n-        return None;\n-    }\n-\n-    let rustc_token = rustc_lexer::first_token(text);\n-    let (syntax_kind, err_message) = rustc_token_kind_to_syntax_kind(&rustc_token.kind, text);\n-\n-    let token = Token { kind: syntax_kind, len: rustc_token.len.try_into().unwrap() };\n-    let optional_error = err_message\n-        .map(|err_message| SyntaxError::new(err_message, TextRange::up_to(TextSize::of(text))));\n-\n-    Some((token, optional_error))\n-}\n-\n-/// Returns `SyntaxKind` and an optional tokenize error message.\n-fn rustc_token_kind_to_syntax_kind(\n-    rustc_token_kind: &rustc_lexer::TokenKind,\n-    token_text: &str,\n-) -> (SyntaxKind, Option<&'static str>) {\n-    // A note on an intended tradeoff:\n-    // We drop some useful information here (see patterns with double dots `..`)\n-    // Storing that info in `SyntaxKind` is not possible due to its layout requirements of\n-    // being `u16` that come from `rowan::SyntaxKind`.\n-\n-    let syntax_kind = {\n-        match rustc_token_kind {\n-            rustc_lexer::TokenKind::LineComment { doc_style: _ } => COMMENT,\n-\n-            rustc_lexer::TokenKind::BlockComment { doc_style: _, terminated: true } => COMMENT,\n-            rustc_lexer::TokenKind::BlockComment { doc_style: _, terminated: false } => {\n-                return (\n-                    COMMENT,\n-                    Some(\"Missing trailing `*/` symbols to terminate the block comment\"),\n-                );\n-            }\n-\n-            rustc_lexer::TokenKind::Whitespace => WHITESPACE,\n-\n-            rustc_lexer::TokenKind::Ident => {\n-                if token_text == \"_\" {\n-                    UNDERSCORE\n-                } else {\n-                    SyntaxKind::from_keyword(token_text).unwrap_or(IDENT)\n-                }\n-            }\n-\n-            rustc_lexer::TokenKind::RawIdent => IDENT,\n-            rustc_lexer::TokenKind::Literal { kind, .. } => return match_literal_kind(kind),\n-\n-            rustc_lexer::TokenKind::Lifetime { starts_with_number: false } => LIFETIME_IDENT,\n-            rustc_lexer::TokenKind::Lifetime { starts_with_number: true } => {\n-                return (LIFETIME_IDENT, Some(\"Lifetime name cannot start with a number\"))\n-            }\n-\n-            rustc_lexer::TokenKind::Semi => T![;],\n-            rustc_lexer::TokenKind::Comma => T![,],\n-            rustc_lexer::TokenKind::Dot => T![.],\n-            rustc_lexer::TokenKind::OpenParen => T!['('],\n-            rustc_lexer::TokenKind::CloseParen => T![')'],\n-            rustc_lexer::TokenKind::OpenBrace => T!['{'],\n-            rustc_lexer::TokenKind::CloseBrace => T!['}'],\n-            rustc_lexer::TokenKind::OpenBracket => T!['['],\n-            rustc_lexer::TokenKind::CloseBracket => T![']'],\n-            rustc_lexer::TokenKind::At => T![@],\n-            rustc_lexer::TokenKind::Pound => T![#],\n-            rustc_lexer::TokenKind::Tilde => T![~],\n-            rustc_lexer::TokenKind::Question => T![?],\n-            rustc_lexer::TokenKind::Colon => T![:],\n-            rustc_lexer::TokenKind::Dollar => T![$],\n-            rustc_lexer::TokenKind::Eq => T![=],\n-            rustc_lexer::TokenKind::Bang => T![!],\n-            rustc_lexer::TokenKind::Lt => T![<],\n-            rustc_lexer::TokenKind::Gt => T![>],\n-            rustc_lexer::TokenKind::Minus => T![-],\n-            rustc_lexer::TokenKind::And => T![&],\n-            rustc_lexer::TokenKind::Or => T![|],\n-            rustc_lexer::TokenKind::Plus => T![+],\n-            rustc_lexer::TokenKind::Star => T![*],\n-            rustc_lexer::TokenKind::Slash => T![/],\n-            rustc_lexer::TokenKind::Caret => T![^],\n-            rustc_lexer::TokenKind::Percent => T![%],\n-            rustc_lexer::TokenKind::Unknown => ERROR,\n-        }\n-    };\n-\n-    return (syntax_kind, None);\n-\n-    fn match_literal_kind(kind: &rustc_lexer::LiteralKind) -> (SyntaxKind, Option<&'static str>) {\n-        let mut err = \"\";\n-        let syntax_kind = match *kind {\n-            rustc_lexer::LiteralKind::Int { empty_int, base: _ } => {\n-                if empty_int {\n-                    err = \"Missing digits after the integer base prefix\";\n-                }\n-                INT_NUMBER\n-            }\n-            rustc_lexer::LiteralKind::Float { empty_exponent, base: _ } => {\n-                if empty_exponent {\n-                    err = \"Missing digits after the exponent symbol\";\n-                }\n-                FLOAT_NUMBER\n-            }\n-            rustc_lexer::LiteralKind::Char { terminated } => {\n-                if !terminated {\n-                    err = \"Missing trailing `'` symbol to terminate the character literal\";\n-                }\n-                CHAR\n-            }\n-            rustc_lexer::LiteralKind::Byte { terminated } => {\n-                if !terminated {\n-                    err = \"Missing trailing `'` symbol to terminate the byte literal\";\n-                }\n-                BYTE\n-            }\n-            rustc_lexer::LiteralKind::Str { terminated } => {\n-                if !terminated {\n-                    err = \"Missing trailing `\\\"` symbol to terminate the string literal\";\n-                }\n-                STRING\n-            }\n-            rustc_lexer::LiteralKind::ByteStr { terminated } => {\n-                if !terminated {\n-                    err = \"Missing trailing `\\\"` symbol to terminate the byte string literal\";\n-                }\n-                BYTE_STRING\n-            }\n-            rustc_lexer::LiteralKind::RawStr { err: raw_str_err, .. } => {\n-                if let Some(raw_str_err) = raw_str_err {\n-                    err = match raw_str_err {\n-                        RawStrError::InvalidStarter { .. } => \"Missing `\\\"` symbol after `#` symbols to begin the raw string literal\",\n-                        RawStrError::NoTerminator { expected, found, .. } => if expected == found {\n-                            \"Missing trailing `\\\"` to terminate the raw string literal\"\n-                        } else {\n-                            \"Missing trailing `\\\"` with `#` symbols to terminate the raw string literal\"\n-                        },\n-                        RawStrError::TooManyDelimiters { .. } => \"Too many `#` symbols: raw strings may be delimited by up to 65535 `#` symbols\",\n-                    };\n-                };\n-                STRING\n-            }\n-            rustc_lexer::LiteralKind::RawByteStr { err: raw_str_err, .. } => {\n-                if let Some(raw_str_err) = raw_str_err {\n-                    err = match raw_str_err {\n-                        RawStrError::InvalidStarter { .. } => \"Missing `\\\"` symbol after `#` symbols to begin the raw byte string literal\",\n-                        RawStrError::NoTerminator { expected, found, .. } => if expected == found {\n-                            \"Missing trailing `\\\"` to terminate the raw byte string literal\"\n-                        } else {\n-                            \"Missing trailing `\\\"` with `#` symbols to terminate the raw byte string literal\"\n-                        },\n-                        RawStrError::TooManyDelimiters { .. } => \"Too many `#` symbols: raw byte strings may be delimited by up to 65535 `#` symbols\",\n-                    };\n-                };\n-\n-                BYTE_STRING\n-            }\n-        };\n-\n-        let err = if err.is_empty() { None } else { Some(err) };\n-\n-        (syntax_kind, err)\n-    }\n-}"}, {"sha": "e9567a838c6c7adc20e438b5a890d376b57ccc45", "filename": "crates/syntax/src/parsing/reparsing.rs", "status": "modified", "additions": 15, "deletions": 21, "changes": 36, "blob_url": "https://github.com/rust-lang/rust/blob/a022ad68c9a57b327b84fcbba1de5742d70a0160/crates%2Fsyntax%2Fsrc%2Fparsing%2Freparsing.rs", "raw_url": "https://github.com/rust-lang/rust/raw/a022ad68c9a57b327b84fcbba1de5742d70a0160/crates%2Fsyntax%2Fsrc%2Fparsing%2Freparsing.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fsyntax%2Fsrc%2Fparsing%2Freparsing.rs?ref=a022ad68c9a57b327b84fcbba1de5742d70a0160", "patch": "@@ -10,11 +10,7 @@ use parser::Reparser;\n use text_edit::Indel;\n \n use crate::{\n-    parsing::{\n-        lexer::{lex_single_syntax_kind, tokenize, Token},\n-        text_tree_sink::TextTreeSink,\n-        to_parser_tokens,\n-    },\n+    parsing::text_tree_sink::TextTreeSink,\n     syntax_node::{GreenNode, GreenToken, NodeOrToken, SyntaxElement, SyntaxNode},\n     SyntaxError,\n     SyntaxKind::*,\n@@ -53,7 +49,7 @@ fn reparse_token(\n             }\n \n             let mut new_text = get_text_after_edit(prev_token.clone().into(), edit);\n-            let (new_token_kind, new_err) = lex_single_syntax_kind(&new_text)?;\n+            let (new_token_kind, new_err) = parser::LexedStr::single_token(&new_text)?;\n \n             if new_token_kind != prev_token_kind\n                 || (new_token_kind == IDENT && is_contextual_kw(&new_text))\n@@ -66,17 +62,18 @@ fn reparse_token(\n             // `b` no longer remains an identifier, but becomes a part of byte string literal\n             if let Some(next_char) = root.text().char_at(prev_token.text_range().end()) {\n                 new_text.push(next_char);\n-                let token_with_next_char = lex_single_syntax_kind(&new_text);\n+                let token_with_next_char = parser::LexedStr::single_token(&new_text);\n                 if let Some((_kind, _error)) = token_with_next_char {\n                     return None;\n                 }\n                 new_text.pop();\n             }\n \n             let new_token = GreenToken::new(rowan::SyntaxKind(prev_token_kind.into()), &new_text);\n+            let range = TextRange::up_to(TextSize::of(&new_text));\n             Some((\n                 prev_token.replace_with(new_token),\n-                new_err.into_iter().collect(),\n+                new_err.into_iter().map(|msg| SyntaxError::new(msg, range)).collect(),\n                 prev_token.text_range(),\n             ))\n         }\n@@ -91,17 +88,17 @@ fn reparse_block(\n     let (node, reparser) = find_reparsable_node(root, edit.delete)?;\n     let text = get_text_after_edit(node.clone().into(), edit);\n \n-    let (lexer_tokens, new_lexer_errors) = tokenize(&text);\n-    if !is_balanced(&lexer_tokens) {\n+    let lexed = parser::LexedStr::new(text.as_str());\n+    let parser_tokens = lexed.to_tokens();\n+    if !is_balanced(&lexed) {\n         return None;\n     }\n-    let parser_tokens = to_parser_tokens(&text, &lexer_tokens);\n \n-    let mut tree_sink = TextTreeSink::new(&text, &lexer_tokens);\n+    let mut tree_sink = TextTreeSink::new(lexed);\n+\n     reparser.parse(&parser_tokens, &mut tree_sink);\n \n-    let (green, mut new_parser_errors) = tree_sink.finish();\n-    new_parser_errors.extend(new_lexer_errors);\n+    let (green, new_parser_errors) = tree_sink.finish();\n \n     Some((node.replace_with(green), new_parser_errors, node.text_range()))\n }\n@@ -131,16 +128,13 @@ fn find_reparsable_node(node: &SyntaxNode, range: TextRange) -> Option<(SyntaxNo\n     })\n }\n \n-fn is_balanced(tokens: &[Token]) -> bool {\n-    if tokens.is_empty()\n-        || tokens.first().unwrap().kind != T!['{']\n-        || tokens.last().unwrap().kind != T!['}']\n-    {\n+fn is_balanced(lexed: &parser::LexedStr<'_>) -> bool {\n+    if lexed.is_empty() || lexed.kind(0) != T!['{'] || lexed.kind(lexed.len() - 1) != T!['}'] {\n         return false;\n     }\n     let mut balance = 0usize;\n-    for t in &tokens[1..tokens.len() - 1] {\n-        match t.kind {\n+    for i in 1..lexed.len() - 1 {\n+        match lexed.kind(i) {\n             T!['{'] => balance += 1,\n             T!['}'] => {\n                 balance = match balance.checked_sub(1) {"}, {"sha": "c9e7feb96575d841b68acf1552b0977eb67cde51", "filename": "crates/syntax/src/parsing/text_tree_sink.rs", "status": "modified", "additions": 36, "deletions": 50, "changes": 86, "blob_url": "https://github.com/rust-lang/rust/blob/a022ad68c9a57b327b84fcbba1de5742d70a0160/crates%2Fsyntax%2Fsrc%2Fparsing%2Ftext_tree_sink.rs", "raw_url": "https://github.com/rust-lang/rust/raw/a022ad68c9a57b327b84fcbba1de5742d70a0160/crates%2Fsyntax%2Fsrc%2Fparsing%2Ftext_tree_sink.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fsyntax%2Fsrc%2Fparsing%2Ftext_tree_sink.rs?ref=a022ad68c9a57b327b84fcbba1de5742d70a0160", "patch": "@@ -2,25 +2,22 @@\n \n use std::mem;\n \n-use parser::{ParseError, TreeSink};\n+use parser::{LexedStr, ParseError, TreeSink};\n \n use crate::{\n     ast,\n-    parsing::Token,\n     syntax_node::GreenNode,\n     SyntaxError,\n     SyntaxKind::{self, *},\n-    SyntaxTreeBuilder, TextRange, TextSize,\n+    SyntaxTreeBuilder, TextRange,\n };\n \n /// Bridges the parser with our specific syntax tree representation.\n ///\n /// `TextTreeSink` also handles attachment of trivia (whitespace) to nodes.\n pub(crate) struct TextTreeSink<'a> {\n-    text: &'a str,\n-    tokens: &'a [Token],\n-    text_pos: TextSize,\n-    token_pos: usize,\n+    lexed: LexedStr<'a>,\n+    pos: usize,\n     state: State,\n     inner: SyntaxTreeBuilder,\n }\n@@ -39,12 +36,7 @@ impl<'a> TreeSink for TextTreeSink<'a> {\n             State::Normal => (),\n         }\n         self.eat_trivias();\n-        let n_tokens = n_tokens as usize;\n-        let len = self.tokens[self.token_pos..self.token_pos + n_tokens]\n-            .iter()\n-            .map(|it| it.len)\n-            .sum::<TextSize>();\n-        self.do_token(kind, len, n_tokens);\n+        self.do_token(kind, n_tokens as usize);\n     }\n \n     fn start_node(&mut self, kind: SyntaxKind) {\n@@ -60,20 +52,12 @@ impl<'a> TreeSink for TextTreeSink<'a> {\n         }\n \n         let n_trivias =\n-            self.tokens[self.token_pos..].iter().take_while(|it| it.kind.is_trivia()).count();\n-        let leading_trivias = &self.tokens[self.token_pos..self.token_pos + n_trivias];\n-        let mut trivia_end =\n-            self.text_pos + leading_trivias.iter().map(|it| it.len).sum::<TextSize>();\n-\n-        let n_attached_trivias = {\n-            let leading_trivias = leading_trivias.iter().rev().map(|it| {\n-                let next_end = trivia_end - it.len;\n-                let range = TextRange::new(next_end, trivia_end);\n-                trivia_end = next_end;\n-                (it.kind, &self.text[range])\n-            });\n-            n_attached_trivias(kind, leading_trivias)\n-        };\n+            (self.pos..self.lexed.len()).take_while(|&it| self.lexed.kind(it).is_trivia()).count();\n+        let leading_trivias = self.pos..self.pos + n_trivias;\n+        let n_attached_trivias = n_attached_trivias(\n+            kind,\n+            leading_trivias.rev().map(|it| (self.lexed.kind(it), self.lexed.text(it))),\n+        );\n         self.eat_n_trivias(n_trivias - n_attached_trivias);\n         self.inner.start_node(kind);\n         self.eat_n_trivias(n_attached_trivias);\n@@ -88,20 +72,14 @@ impl<'a> TreeSink for TextTreeSink<'a> {\n     }\n \n     fn error(&mut self, error: ParseError) {\n-        self.inner.error(error, self.text_pos);\n+        let text_pos = self.lexed.text_start(self.pos).try_into().unwrap();\n+        self.inner.error(error, text_pos);\n     }\n }\n \n impl<'a> TextTreeSink<'a> {\n-    pub(super) fn new(text: &'a str, tokens: &'a [Token]) -> Self {\n-        Self {\n-            text,\n-            tokens,\n-            text_pos: 0.into(),\n-            token_pos: 0,\n-            state: State::PendingStart,\n-            inner: SyntaxTreeBuilder::default(),\n-        }\n+    pub(super) fn new(lexed: parser::LexedStr<'a>) -> Self {\n+        Self { lexed, pos: 0, state: State::PendingStart, inner: SyntaxTreeBuilder::default() }\n     }\n \n     pub(super) fn finish_eof(mut self) -> (GreenNode, Vec<SyntaxError>, bool) {\n@@ -113,8 +91,17 @@ impl<'a> TextTreeSink<'a> {\n             State::PendingStart | State::Normal => unreachable!(),\n         }\n \n-        let (node, errors) = self.inner.finish_raw();\n-        let is_eof = self.token_pos == self.tokens.len();\n+        let (node, mut errors) = self.inner.finish_raw();\n+        for (i, err) in self.lexed.errors() {\n+            let text_range = self.lexed.text_range(i);\n+            let text_range = TextRange::new(\n+                text_range.start.try_into().unwrap(),\n+                text_range.end.try_into().unwrap(),\n+            );\n+            errors.push(SyntaxError::new(err, text_range))\n+        }\n+\n+        let is_eof = self.pos == self.lexed.len();\n \n         (node, errors, is_eof)\n     }\n@@ -125,27 +112,26 @@ impl<'a> TextTreeSink<'a> {\n     }\n \n     fn eat_trivias(&mut self) {\n-        while let Some(&token) = self.tokens.get(self.token_pos) {\n-            if !token.kind.is_trivia() {\n+        while self.pos < self.lexed.len() {\n+            let kind = self.lexed.kind(self.pos);\n+            if !kind.is_trivia() {\n                 break;\n             }\n-            self.do_token(token.kind, token.len, 1);\n+            self.do_token(kind, 1);\n         }\n     }\n \n     fn eat_n_trivias(&mut self, n: usize) {\n         for _ in 0..n {\n-            let token = self.tokens[self.token_pos];\n-            assert!(token.kind.is_trivia());\n-            self.do_token(token.kind, token.len, 1);\n+            let kind = self.lexed.kind(self.pos);\n+            assert!(kind.is_trivia());\n+            self.do_token(kind, 1);\n         }\n     }\n \n-    fn do_token(&mut self, kind: SyntaxKind, len: TextSize, n_tokens: usize) {\n-        let range = TextRange::at(self.text_pos, len);\n-        let text = &self.text[range];\n-        self.text_pos += len;\n-        self.token_pos += n_tokens;\n+    fn do_token(&mut self, kind: SyntaxKind, n_tokens: usize) {\n+        let text = &self.lexed.range_text(self.pos..self.pos + n_tokens);\n+        self.pos += n_tokens;\n         self.inner.token(kind, text);\n     }\n }"}, {"sha": "69c5b1cd35a842007c64f1aa7cd5722bd9a686e9", "filename": "crates/syntax/src/tests.rs", "status": "modified", "additions": 1, "deletions": 36, "changes": 37, "blob_url": "https://github.com/rust-lang/rust/blob/a022ad68c9a57b327b84fcbba1de5742d70a0160/crates%2Fsyntax%2Fsrc%2Ftests.rs", "raw_url": "https://github.com/rust-lang/rust/raw/a022ad68c9a57b327b84fcbba1de5742d70a0160/crates%2Fsyntax%2Fsrc%2Ftests.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fsyntax%2Fsrc%2Ftests.rs?ref=a022ad68c9a57b327b84fcbba1de5742d70a0160", "patch": "@@ -3,7 +3,6 @@ mod sourcegen_ast;\n mod ast_src;\n \n use std::{\n-    fmt::Write,\n     fs,\n     path::{Path, PathBuf},\n };\n@@ -13,25 +12,7 @@ use expect_test::expect_file;\n use rayon::prelude::*;\n use test_utils::{bench, bench_fixture, project_root};\n \n-use crate::{ast, fuzz, tokenize, AstNode, SourceFile, SyntaxError, TextRange, TextSize, Token};\n-\n-#[test]\n-fn lexer_tests() {\n-    // FIXME:\n-    // * Add tests for unicode escapes in byte-character and [raw]-byte-string literals\n-    // * Add tests for unescape errors\n-\n-    dir_tests(&test_data_dir(), &[\"lexer/ok\"], \"txt\", |text, path| {\n-        let (tokens, errors) = tokenize(text);\n-        assert_errors_are_absent(&errors, path);\n-        dump_tokens_and_errors(&tokens, &errors, text)\n-    });\n-    dir_tests(&test_data_dir(), &[\"lexer/err\"], \"txt\", |text, path| {\n-        let (tokens, errors) = tokenize(text);\n-        assert_errors_are_present(&errors, path);\n-        dump_tokens_and_errors(&tokens, &errors, text)\n-    });\n-}\n+use crate::{ast, fuzz, AstNode, SourceFile, SyntaxError};\n \n #[test]\n fn parse_smoke_test() {\n@@ -206,22 +187,6 @@ fn assert_errors_are_absent(errors: &[SyntaxError], path: &Path) {\n     );\n }\n \n-fn dump_tokens_and_errors(tokens: &[Token], errors: &[SyntaxError], text: &str) -> String {\n-    let mut acc = String::new();\n-    let mut offset: TextSize = 0.into();\n-    for token in tokens {\n-        let token_len = token.len;\n-        let token_text = &text[TextRange::at(offset, token.len)];\n-        offset += token.len;\n-        writeln!(acc, \"{:?} {:?} {:?}\", token.kind, token_len, token_text).unwrap();\n-    }\n-    for err in errors {\n-        writeln!(acc, \"> error{:?} token({:?}) msg({})\", err.range(), &text[err.range()], err)\n-            .unwrap();\n-    }\n-    acc\n-}\n-\n fn fragment_parser_dir_test<T, F>(ok_paths: &[&str], err_paths: &[&str], f: F)\n where\n     T: crate::AstNode,"}]}