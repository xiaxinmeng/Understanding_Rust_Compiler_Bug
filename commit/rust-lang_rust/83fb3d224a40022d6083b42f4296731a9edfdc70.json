{"sha": "83fb3d224a40022d6083b42f4296731a9edfdc70", "node_id": "MDY6Q29tbWl0NzI0NzEyOjgzZmIzZDIyNGE0MDAyMmQ2MDgzYjQyZjQyOTY3MzFhOWVkZmRjNzA=", "commit": {"author": {"name": "Graydon Hoare", "email": "graydon@mozilla.com", "date": "2013-07-10T23:17:41Z"}, "committer": {"name": "Graydon Hoare", "email": "graydon@mozilla.com", "date": "2013-07-11T20:15:52Z"}, "message": "extra: add metrics ratchet to test driver.", "tree": {"sha": "35b53af684f9e309d60b48518cf5d718f6bd86f6", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/35b53af684f9e309d60b48518cf5d718f6bd86f6"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/83fb3d224a40022d6083b42f4296731a9edfdc70", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/83fb3d224a40022d6083b42f4296731a9edfdc70", "html_url": "https://github.com/rust-lang/rust/commit/83fb3d224a40022d6083b42f4296731a9edfdc70", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/83fb3d224a40022d6083b42f4296731a9edfdc70/comments", "author": {"login": "graydon", "id": 14097, "node_id": "MDQ6VXNlcjE0MDk3", "avatar_url": "https://avatars.githubusercontent.com/u/14097?v=4", "gravatar_id": "", "url": "https://api.github.com/users/graydon", "html_url": "https://github.com/graydon", "followers_url": "https://api.github.com/users/graydon/followers", "following_url": "https://api.github.com/users/graydon/following{/other_user}", "gists_url": "https://api.github.com/users/graydon/gists{/gist_id}", "starred_url": "https://api.github.com/users/graydon/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/graydon/subscriptions", "organizations_url": "https://api.github.com/users/graydon/orgs", "repos_url": "https://api.github.com/users/graydon/repos", "events_url": "https://api.github.com/users/graydon/events{/privacy}", "received_events_url": "https://api.github.com/users/graydon/received_events", "type": "User", "site_admin": false}, "committer": {"login": "graydon", "id": 14097, "node_id": "MDQ6VXNlcjE0MDk3", "avatar_url": "https://avatars.githubusercontent.com/u/14097?v=4", "gravatar_id": "", "url": "https://api.github.com/users/graydon", "html_url": "https://github.com/graydon", "followers_url": "https://api.github.com/users/graydon/followers", "following_url": "https://api.github.com/users/graydon/following{/other_user}", "gists_url": "https://api.github.com/users/graydon/gists{/gist_id}", "starred_url": "https://api.github.com/users/graydon/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/graydon/subscriptions", "organizations_url": "https://api.github.com/users/graydon/orgs", "repos_url": "https://api.github.com/users/graydon/repos", "events_url": "https://api.github.com/users/graydon/events{/privacy}", "received_events_url": "https://api.github.com/users/graydon/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "8614d1694cda26118c2f44a2acd986f137935816", "url": "https://api.github.com/repos/rust-lang/rust/commits/8614d1694cda26118c2f44a2acd986f137935816", "html_url": "https://github.com/rust-lang/rust/commit/8614d1694cda26118c2f44a2acd986f137935816"}], "stats": {"total": 383, "additions": 299, "deletions": 84}, "files": [{"sha": "7a3d3bf0cf3558c10204c9c9fc5c46f8b5540bfc", "filename": "src/libextra/json.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/83fb3d224a40022d6083b42f4296731a9edfdc70/src%2Flibextra%2Fjson.rs", "raw_url": "https://github.com/rust-lang/rust/raw/83fb3d224a40022d6083b42f4296731a9edfdc70/src%2Flibextra%2Fjson.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibextra%2Fjson.rs?ref=83fb3d224a40022d6083b42f4296731a9edfdc70", "patch": "@@ -1226,7 +1226,7 @@ impl Ord for Json {\n }\n \n /// A trait for converting values to JSON\n-trait ToJson {\n+pub trait ToJson {\n     /// Converts the value of `self` to an instance of JSON\n     fn to_json(&self) -> Json;\n }"}, {"sha": "a284c8071696fef7b92bb6838bb257d5a8295ceb", "filename": "src/libextra/test.rs", "status": "modified", "additions": 298, "deletions": 83, "changes": 381, "blob_url": "https://github.com/rust-lang/rust/blob/83fb3d224a40022d6083b42f4296731a9edfdc70/src%2Flibextra%2Ftest.rs", "raw_url": "https://github.com/rust-lang/rust/raw/83fb3d224a40022d6083b42f4296731a9edfdc70/src%2Flibextra%2Ftest.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibextra%2Ftest.rs?ref=83fb3d224a40022d6083b42f4296731a9edfdc70", "patch": "@@ -17,11 +17,15 @@\n \n \n use getopts;\n+use json::ToJson;\n+use json;\n+use serialize::Decodable;\n use sort;\n-use stats;\n use stats::Stats;\n+use stats;\n use term;\n use time::precise_time_ns;\n+use treemap::TreeMap;\n \n use std::comm::{stream, SharedChan};\n use std::either;\n@@ -31,6 +35,8 @@ use std::result;\n use std::task;\n use std::to_str::ToStr;\n use std::u64;\n+use std::hashmap::HashMap;\n+use std::os;\n \n \n // The name of a test. By convention this follows the rules for rust\n@@ -83,6 +89,25 @@ pub struct TestDescAndFn {\n     testfn: TestFn,\n }\n \n+#[deriving(Encodable,Decodable,Eq)]\n+pub struct Metric {\n+    value: f64,\n+    noise: f64\n+}\n+\n+pub struct MetricMap(TreeMap<~str,Metric>);\n+\n+/// Analysis of a single change in metric\n+pub enum MetricChange {\n+    LikelyNoise,\n+    MetricAdded,\n+    MetricRemoved,\n+    Improvement(f64),\n+    Regression(f64)\n+}\n+\n+pub type MetricDiff = TreeMap<~str,MetricChange>;\n+\n // The default console test runner. It accepts the command line\n // arguments and a vector of test_descs.\n pub fn test_main(args: &[~str], tests: ~[TestDescAndFn]) {\n@@ -123,8 +148,8 @@ pub struct TestOpts {\n     run_ignored: bool,\n     run_tests: bool,\n     run_benchmarks: bool,\n-    save_results: Option<Path>,\n-    compare_results: Option<Path>,\n+    ratchet_metrics: Option<Path>,\n+    save_metrics: Option<Path>,\n     logfile: Option<Path>\n }\n \n@@ -136,8 +161,8 @@ pub fn parse_opts(args: &[~str]) -> OptRes {\n     let opts = ~[getopts::optflag(\"ignored\"),\n                  getopts::optflag(\"test\"),\n                  getopts::optflag(\"bench\"),\n-                 getopts::optopt(\"save\"),\n-                 getopts::optopt(\"diff\"),\n+                 getopts::optopt(\"save-metrics\"),\n+                 getopts::optopt(\"ratchet-metrics\"),\n                  getopts::optopt(\"logfile\")];\n     let matches =\n         match getopts::getopts(args_, opts) {\n@@ -159,19 +184,19 @@ pub fn parse_opts(args: &[~str]) -> OptRes {\n     let run_tests = ! run_benchmarks ||\n         getopts::opt_present(&matches, \"test\");\n \n-    let save_results = getopts::opt_maybe_str(&matches, \"save\");\n-    let save_results = save_results.map(|s| Path(*s));\n+    let ratchet_metrics = getopts::opt_maybe_str(&matches, \"ratchet-metrics\");\n+    let ratchet_metrics = ratchet_metrics.map(|s| Path(*s));\n \n-    let compare_results = getopts::opt_maybe_str(&matches, \"diff\");\n-    let compare_results = compare_results.map(|s| Path(*s));\n+    let save_metrics = getopts::opt_maybe_str(&matches, \"save-metrics\");\n+    let save_metrics = save_metrics.map(|s| Path(*s));\n \n     let test_opts = TestOpts {\n         filter: filter,\n         run_ignored: run_ignored,\n         run_tests: run_tests,\n         run_benchmarks: run_benchmarks,\n-        save_results: save_results,\n-        compare_results: compare_results,\n+        ratchet_metrics: ratchet_metrics,\n+        save_metrics: save_metrics,\n         logfile: logfile\n     };\n \n@@ -197,6 +222,7 @@ struct ConsoleTestState {\n     failed: uint,\n     ignored: uint,\n     benchmarked: uint,\n+    metrics: MetricMap,\n     failures: ~[TestDesc]\n }\n \n@@ -228,6 +254,7 @@ impl ConsoleTestState {\n             failed: 0u,\n             ignored: 0u,\n             benchmarked: 0u,\n+            metrics: MetricMap::new(),\n             failures: ~[]\n         }\n     }\n@@ -248,6 +275,23 @@ impl ConsoleTestState {\n         self.write_pretty(\"bench\", term::color::CYAN);\n     }\n \n+\n+    pub fn write_added(&self) {\n+        self.write_pretty(\"added\", term::color::GREEN);\n+    }\n+\n+    pub fn write_improved(&self) {\n+        self.write_pretty(\"improved\", term::color::GREEN);\n+    }\n+\n+    pub fn write_removed(&self) {\n+        self.write_pretty(\"removed\", term::color::YELLOW);\n+    }\n+\n+    pub fn write_regressed(&self) {\n+        self.write_pretty(\"regressed\", term::color::RED);\n+    }\n+\n     pub fn write_pretty(&self,\n                         word: &str,\n                         color: term::color::Color) {\n@@ -315,14 +359,73 @@ impl ConsoleTestState {\n         }\n     }\n \n-    pub fn write_run_finish(&self) -> bool {\n+    pub fn write_metric_diff(&self, diff: &MetricDiff) {\n+        let mut noise = 0;\n+        let mut improved = 0;\n+        let mut regressed = 0;\n+        let mut added = 0;\n+        let mut removed = 0;\n+\n+        for diff.iter().advance() |(k, v)| {\n+            match *v {\n+                LikelyNoise => noise += 1,\n+                MetricAdded => {\n+                    added += 1;\n+                    self.write_added();\n+                    self.out.write_line(fmt!(\": %s\", *k));\n+                }\n+                MetricRemoved => {\n+                    removed += 1;\n+                    self.write_removed();\n+                    self.out.write_line(fmt!(\": %s\", *k));\n+                }\n+                Improvement(pct) => {\n+                    improved += 1;\n+                    self.out.write_str(*k);\n+                    self.out.write_str(\": \");\n+                    self.write_improved();\n+                    self.out.write_line(fmt!(\" by %.2f%%\", pct as float))\n+                }\n+                Regression(pct) => {\n+                    regressed += 1;\n+                    self.out.write_str(*k);\n+                    self.out.write_str(\": \");\n+                    self.write_regressed();\n+                    self.out.write_line(fmt!(\" by %.2f%%\", pct as float))\n+                }\n+            }\n+        }\n+        self.out.write_line(fmt!(\"result of ratchet: %u matrics added, %u removed, \\\n+                                  %u improved, %u regressed, %u noise\",\n+                                 added, removed, improved, regressed, noise));\n+        if regressed == 0 {\n+            self.out.write_line(\"updated ratchet file\")\n+        } else {\n+            self.out.write_line(\"left ratchet file untouched\")\n+        }\n+    }\n+\n+    pub fn write_run_finish(&self, ratchet_metrics: &Option<Path>) -> bool {\n         assert!(self.passed + self.failed + self.ignored + self.benchmarked == self.total);\n-        let success = self.failed == 0u;\n-        if !success {\n+\n+        let ratchet_success = match *ratchet_metrics {\n+            None => true,\n+            Some(ref pth) => {\n+                self.out.write_str(fmt!(\"\\nusing metrics ratchet: %s\\n\", pth.to_str()));\n+                let (diff, ok) = self.metrics.ratchet(pth);\n+                self.write_metric_diff(&diff);\n+                ok\n+            }\n+        };\n+\n+        let test_success = self.failed == 0u;\n+        if !test_success {\n             self.write_failures();\n         }\n \n-        self.out.write_str(\"\\nresult: \");\n+        let success = ratchet_success && test_success;\n+\n+        self.out.write_str(\"\\ntest result: \");\n         if success {\n             // There's no parallelism at this point so it's safe to use color\n             self.write_ok();\n@@ -362,7 +465,12 @@ pub fn run_tests_console(opts: &TestOpts,\n                 match result {\n                     TrOk => st.passed += 1,\n                     TrIgnored => st.ignored += 1,\n-                    TrBench(_) => st.benchmarked += 1,\n+                    TrBench(bs) => {\n+                        st.metrics.insert_metric(test.name.to_str(),\n+                                                 bs.ns_iter_summ.median,\n+                                                 bs.ns_iter_summ.max - bs.ns_iter_summ.min);\n+                        st.benchmarked += 1\n+                    }\n                     TrFailed => {\n                         st.failed += 1;\n                         st.failures.push(test);\n@@ -373,7 +481,14 @@ pub fn run_tests_console(opts: &TestOpts,\n     }\n     let st = @mut ConsoleTestState::new(opts);\n     run_tests(opts, tests, |x| callback(&x, st));\n-    return st.write_run_finish();\n+    match opts.save_metrics {\n+        None => (),\n+        Some(ref pth) => {\n+            st.metrics.save(pth);\n+            st.out.write_str(fmt!(\"\\nmetrics saved to: %s\", pth.to_str()));\n+        }\n+    }\n+    return st.write_run_finish(&opts.ratchet_metrics);\n }\n \n #[test]\n@@ -402,6 +517,7 @@ fn should_sort_failures_before_printing_them() {\n             failed: 0u,\n             ignored: 0u,\n             benchmarked: 0u,\n+            metrics: MetricsMap::new(),\n             failures: ~[test_b, test_a]\n         };\n \n@@ -610,6 +726,133 @@ fn calc_result(desc: &TestDesc, task_succeeded: bool) -> TestResult {\n     }\n }\n \n+\n+impl ToJson for Metric {\n+    fn to_json(&self) -> json::Json {\n+        let mut map = ~HashMap::new();\n+        map.insert(~\"value\", json::Number(self.value as float));\n+        map.insert(~\"noise\", json::Number(self.noise as float));\n+        json::Object(map)\n+    }\n+}\n+\n+impl MetricMap {\n+\n+    fn new() -> MetricMap {\n+        MetricMap(TreeMap::new())\n+    }\n+\n+    /// Load MetricDiff from a file.\n+    fn load(p: &Path) -> MetricMap {\n+        assert!(os::path_exists(p));\n+        let f = io::file_reader(p).get();\n+        let mut decoder = json::Decoder(json::from_reader(f).get());\n+        MetricMap(Decodable::decode(&mut decoder))\n+    }\n+\n+    /// Write MetricDiff to a file.\n+    pub fn save(&self, p: &Path) {\n+        let f = io::file_writer(p, [io::Create, io::Truncate]).get();\n+        json::to_pretty_writer(f, &self.to_json());\n+    }\n+\n+    /// Compare against another MetricMap\n+    pub fn compare_to_old(&self, old: MetricMap) -> MetricDiff {\n+        let mut diff : MetricDiff = TreeMap::new();\n+        for old.iter().advance |(k, vold)| {\n+            let r = match self.find(k) {\n+                None => MetricRemoved,\n+                Some(v) => {\n+                    let delta = (v.value - vold.value);\n+                    if delta.abs() < vold.noise.abs() {\n+                        LikelyNoise\n+                    } else {\n+                        let pct = delta.abs() / v.value * 100.0;\n+                        if vold.noise < 0.0 {\n+                            // When 'noise' is negative, it means we want\n+                            // to see deltas that go up over time, and can\n+                            // only tolerate slight negative movement.\n+                            if delta < 0.0 {\n+                                Regression(pct)\n+                            } else {\n+                                Improvement(pct)\n+                            }\n+                        } else {\n+                            // When 'noise' is positive, it means we want\n+                            // to see deltas that go down over time, and\n+                            // can only tolerate slight positive movements.\n+                            if delta < 0.0 {\n+                                Improvement(pct)\n+                            } else {\n+                                Regression(pct)\n+                            }\n+                        }\n+                    }\n+                }\n+            };\n+            diff.insert(copy *k, r);\n+        }\n+        for self.iter().advance |(k, _)| {\n+            if !diff.contains_key(k) {\n+                diff.insert(copy *k, MetricAdded);\n+            }\n+        }\n+        diff\n+    }\n+\n+    /// Insert a named `value` (+/- `noise`) metric into the map. The value\n+    /// must be non-negative. The `noise` indicates the uncertainty of the\n+    /// metric, which doubles as the \"noise range\" of acceptable\n+    /// pairwise-regressions on this named value, when comparing from one\n+    /// metric to the next using `compare_to_old`.\n+    ///\n+    /// If `noise` is positive, then it means this metric is of a value\n+    /// you want to see grow smaller, so a change larger than `noise` in the\n+    /// positive direction represents a regression.\n+    ///\n+    /// If `noise` is negative, then it means this metric is of a value\n+    /// you want to see grow larger, so a change larger than `noise` in the\n+    /// negative direction represents a regression.\n+    pub fn insert_metric(&mut self, name: &str, value: f64, noise: f64) {\n+        let m = Metric {\n+            value: value,\n+            noise: noise\n+        };\n+        self.insert(name.to_owned(), m);\n+    }\n+\n+    /// Attempt to \"ratchet\" an external metric file. This involves loading\n+    /// metrics from a metric file (if it exists), comparing against\n+    /// the metrics in `self` using `compare_to_old`, and rewriting the\n+    /// file to contain the metrics in `self` if none of the\n+    /// `MetricChange`s are `Regression`. Returns the diff as well\n+    /// as a boolean indicating whether the ratchet succeeded.\n+    pub fn ratchet(&self, p: &Path) -> (MetricDiff, bool) {\n+        let old = if os::path_exists(p) {\n+            MetricMap::load(p)\n+        } else {\n+            MetricMap::new()\n+        };\n+\n+        let diff : MetricDiff = self.compare_to_old(old);\n+        let ok = do diff.iter().all() |(_, v)| {\n+            match *v {\n+                Regression(_) => false,\n+                _ => true\n+            }\n+        };\n+\n+        if ok {\n+            debug!(\"rewriting file '%s' with updated metrics\");\n+            self.save(p);\n+        }\n+        return (diff, ok)\n+    }\n+}\n+\n+\n+// Benchmarking\n+\n impl BenchHarness {\n     /// Callback for benchmark functions to run in their body.\n     pub fn iter(&mut self, inner:&fn()) {\n@@ -644,76 +887,42 @@ impl BenchHarness {\n         f(self);\n     }\n \n-    // This is the Go benchmark algorithm. It produces a single\n-    // datapoint and always tries to run for 1s.\n-    pub fn go_bench(&mut self, f: &fn(&mut BenchHarness)) {\n-\n-        // Rounds a number down to the nearest power of 10.\n-        fn round_down_10(n: u64) -> u64 {\n-            let mut n = n;\n-            let mut res = 1;\n-            while n > 10 {\n-                n = n / 10;\n-                res *= 10;\n-            }\n-            res\n-        }\n-\n-        // Rounds x up to a number of the form [1eX, 2eX, 5eX].\n-        fn round_up(n: u64) -> u64 {\n-            let base = round_down_10(n);\n-            if n < (2 * base) {\n-                2 * base\n-            } else if n < (5 * base) {\n-                5 * base\n-            } else {\n-                10 * base\n-            }\n-        }\n+    // This is a more statistics-driven benchmark algorithm\n+    pub fn auto_bench(&mut self, f: &fn(&mut BenchHarness)) -> stats::Summary {\n \n         // Initial bench run to get ballpark figure.\n         let mut n = 1_u64;\n         self.bench_n(n, |x| f(x));\n \n-        while n < 1_000_000_000 &&\n-            self.ns_elapsed() < 1_000_000_000 {\n-            let last = n;\n-\n-            // Try to estimate iter count for 1s falling back to 1bn\n-            // iterations if first run took < 1ns.\n-            if self.ns_per_iter() == 0 {\n-                n = 1_000_000_000;\n-            } else {\n-                n = 1_000_000_000 / self.ns_per_iter();\n-            }\n-\n-            n = u64::max(u64::min(n+n/2, 100*last), last+1);\n-            n = round_up(n);\n-            self.bench_n(n, |x| f(x));\n+        // Try to estimate iter count for 1ms falling back to 1m\n+        // iterations if first run took < 1ns.\n+        if self.ns_per_iter() == 0 {\n+            n = 1_000_000;\n+        } else {\n+            n = 1_000_000 / self.ns_per_iter();\n         }\n-    }\n \n-    // This is a more statistics-driven benchmark algorithm.  It stops as\n-    // quickly as 100ms, so long as the statistical properties are\n-    // satisfactory. If those properties are not met, it may run as long as\n-    // the Go algorithm.\n-    pub fn auto_bench(&mut self, f: &fn(&mut BenchHarness)) -> stats::Summary {\n-\n-        let mut magnitude = 1000;\n-\n-        let samples : &mut [f64] = [0.0_f64, ..100];\n+        let mut total_run = 0;\n+        let samples : &mut [f64] = [0.0_f64, ..50];\n         loop {\n             let loop_start = precise_time_ns();\n \n             for samples.mut_iter().advance() |p| {\n-                self.bench_n(magnitude as u64, |x| f(x));\n+                self.bench_n(n as u64, |x| f(x));\n                 *p = self.ns_per_iter() as f64;\n             };\n \n-            // Clip top 10% and bottom 10% of outliers\n-            stats::winsorize(samples, 10.0);\n+            stats::winsorize(samples, 5.0);\n             let summ = stats::Summary::new(samples);\n \n+            for samples.mut_iter().advance() |p| {\n+                self.bench_n(5 * n as u64, |x| f(x));\n+                *p = self.ns_per_iter() as f64;\n+            };\n+\n+            stats::winsorize(samples, 5.0);\n+            let summ5 = stats::Summary::new(samples);\n+\n             debug!(\"%u samples, median %f, MAD=%f, MADP=%f\",\n                    samples.len(),\n                    summ.median as float,\n@@ -723,20 +932,27 @@ impl BenchHarness {\n             let now = precise_time_ns();\n             let loop_run = now - loop_start;\n \n-            // Stop early if we have a good signal after a 100ms loop.\n-            if loop_run > 100_000_000 && summ.median_abs_dev_pct < 5.0 {\n-                return summ;\n+            // If we've run for 100ms an seem to have converged to a\n+            // stable median.\n+            if loop_run > 100_000_000 &&\n+                summ.median_abs_dev_pct < 1.0 &&\n+                summ.median - summ5.median < summ5.median_abs_dev {\n+                return summ5;\n             }\n \n-            // Longest we ever run for is 1s.\n-            if loop_run > 1_000_000_000 {\n-                return summ;\n+            total_run += loop_run;\n+            // Longest we ever run for is 10s.\n+            if total_run > 10_000_000_000 {\n+                return summ5;\n             }\n \n-            magnitude *= 3;\n-            magnitude /= 2;\n+            n *= 2;\n         }\n     }\n+\n+\n+\n+\n }\n \n pub mod bench {\n@@ -881,8 +1097,7 @@ mod tests {\n             logfile: option::None,\n             run_tests: true,\n             run_benchmarks: false,\n-            save_results: option::None,\n-            compare_results: option::None\n+            ratchet: option::None,\n         };\n \n         let tests = ~[\n@@ -918,8 +1133,8 @@ mod tests {\n             logfile: option::None,\n             run_tests: true,\n             run_benchmarks: false,\n-            save_results: option::None,\n-            compare_results: option::None\n+            ratchet_metrics: option::None,\n+            save_metrics: option::None,\n         };\n \n         let names ="}]}