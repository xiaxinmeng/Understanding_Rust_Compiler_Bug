{"sha": "7f482808f9700259da6585597486166148576d96", "node_id": "MDY6Q29tbWl0NzI0NzEyOjdmNDgyODA4Zjk3MDAyNTlkYTY1ODU1OTc0ODYxNjYxNDg1NzZkOTY=", "commit": {"author": {"name": "Michael Woerister", "email": "michaelwoerister@posteo.net", "date": "2017-06-09T13:01:44Z"}, "committer": {"name": "Michael Woerister", "email": "michaelwoerister@posteo.net", "date": "2017-06-09T13:03:34Z"}, "message": "incr.comp.: Clean up and optimize dep-graph loading.", "tree": {"sha": "375b7f168f3ca0894cb06f669ee0df9f4ee1520d", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/375b7f168f3ca0894cb06f669ee0df9f4ee1520d"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/7f482808f9700259da6585597486166148576d96", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/7f482808f9700259da6585597486166148576d96", "html_url": "https://github.com/rust-lang/rust/commit/7f482808f9700259da6585597486166148576d96", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/7f482808f9700259da6585597486166148576d96/comments", "author": {"login": "michaelwoerister", "id": 1825894, "node_id": "MDQ6VXNlcjE4MjU4OTQ=", "avatar_url": "https://avatars.githubusercontent.com/u/1825894?v=4", "gravatar_id": "", "url": "https://api.github.com/users/michaelwoerister", "html_url": "https://github.com/michaelwoerister", "followers_url": "https://api.github.com/users/michaelwoerister/followers", "following_url": "https://api.github.com/users/michaelwoerister/following{/other_user}", "gists_url": "https://api.github.com/users/michaelwoerister/gists{/gist_id}", "starred_url": "https://api.github.com/users/michaelwoerister/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/michaelwoerister/subscriptions", "organizations_url": "https://api.github.com/users/michaelwoerister/orgs", "repos_url": "https://api.github.com/users/michaelwoerister/repos", "events_url": "https://api.github.com/users/michaelwoerister/events{/privacy}", "received_events_url": "https://api.github.com/users/michaelwoerister/received_events", "type": "User", "site_admin": false}, "committer": {"login": "michaelwoerister", "id": 1825894, "node_id": "MDQ6VXNlcjE4MjU4OTQ=", "avatar_url": "https://avatars.githubusercontent.com/u/1825894?v=4", "gravatar_id": "", "url": "https://api.github.com/users/michaelwoerister", "html_url": "https://github.com/michaelwoerister", "followers_url": "https://api.github.com/users/michaelwoerister/followers", "following_url": "https://api.github.com/users/michaelwoerister/following{/other_user}", "gists_url": "https://api.github.com/users/michaelwoerister/gists{/gist_id}", "starred_url": "https://api.github.com/users/michaelwoerister/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/michaelwoerister/subscriptions", "organizations_url": "https://api.github.com/users/michaelwoerister/orgs", "repos_url": "https://api.github.com/users/michaelwoerister/repos", "events_url": "https://api.github.com/users/michaelwoerister/events{/privacy}", "received_events_url": "https://api.github.com/users/michaelwoerister/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "36071749092b03ba454aa16eaf68dd5341d69619", "url": "https://api.github.com/repos/rust-lang/rust/commits/36071749092b03ba454aa16eaf68dd5341d69619", "html_url": "https://github.com/rust-lang/rust/commit/36071749092b03ba454aa16eaf68dd5341d69619"}], "stats": {"total": 205, "additions": 116, "deletions": 89}, "files": [{"sha": "06acfb5d77807794016b86120ac9ab71f2f13ce5", "filename": "src/librustc_incremental/persist/data.rs", "status": "modified", "additions": 9, "deletions": 12, "changes": 21, "blob_url": "https://github.com/rust-lang/rust/blob/7f482808f9700259da6585597486166148576d96/src%2Flibrustc_incremental%2Fpersist%2Fdata.rs", "raw_url": "https://github.com/rust-lang/rust/raw/7f482808f9700259da6585597486166148576d96/src%2Flibrustc_incremental%2Fpersist%2Fdata.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_incremental%2Fpersist%2Fdata.rs?ref=7f482808f9700259da6585597486166148576d96", "patch": "@@ -26,7 +26,7 @@ pub struct SerializedDepGraph {\n     /// For each DepNode, stores the list of edges originating from that\n     /// DepNode. Encoded as a [start, end) pair indexing into edge_list_data,\n     /// which holds the actual DepNodeIndices of the target nodes.\n-    pub edge_list_indices: Vec<(u32, u32)>,\n+    pub edge_list_indices: IndexVec<DepNodeIndex, (u32, u32)>,\n     /// A flattened list of all edge targets in the graph. Edge sources are\n     /// implicit in edge_list_indices.\n     pub edge_list_data: Vec<DepNodeIndex>,\n@@ -55,7 +55,14 @@ pub struct SerializedDepGraph {\n     /// will be different when we next compile) related to each node,\n     /// but rather the `DefPathIndex`. This can then be retraced\n     /// to find the current def-id.\n-    pub hashes: Vec<SerializedHash>,\n+    pub hashes: Vec<(DepNodeIndex, Fingerprint)>,\n+}\n+\n+impl SerializedDepGraph {\n+    pub fn edge_targets_from(&self, source: DepNodeIndex) -> &[DepNodeIndex] {\n+        let targets = self.edge_list_indices[source];\n+        &self.edge_list_data[targets.0 as usize .. targets.1 as usize]\n+    }\n }\n \n /// The index of a DepNode in the SerializedDepGraph::nodes array.\n@@ -84,16 +91,6 @@ impl Idx for DepNodeIndex {\n     }\n }\n \n-#[derive(Debug, RustcEncodable, RustcDecodable)]\n-pub struct SerializedHash {\n-    /// def-id of thing being hashed\n-    pub dep_node: DepNode,\n-\n-    /// the hash as of previous compilation, computed by code in\n-    /// `hash` module\n-    pub hash: Fingerprint,\n-}\n-\n #[derive(Debug, RustcEncodable, RustcDecodable)]\n pub struct SerializedWorkProduct {\n     /// node that produced the work-product"}, {"sha": "3f3dc10365c673403cb1752e3a729527fccdcbe9", "filename": "src/librustc_incremental/persist/dirty_clean.rs", "status": "modified", "additions": 11, "deletions": 2, "changes": 13, "blob_url": "https://github.com/rust-lang/rust/blob/7f482808f9700259da6585597486166148576d96/src%2Flibrustc_incremental%2Fpersist%2Fdirty_clean.rs", "raw_url": "https://github.com/rust-lang/rust/raw/7f482808f9700259da6585597486166148576d96/src%2Flibrustc_incremental%2Fpersist%2Fdirty_clean.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_incremental%2Fpersist%2Fdirty_clean.rs?ref=7f482808f9700259da6585597486166148576d96", "patch": "@@ -40,6 +40,7 @@\n //! previous revision to compare things to.\n //!\n \n+use super::data::DepNodeIndex;\n use super::load::DirtyNodes;\n use rustc::dep_graph::{DepGraphQuery, DepNode, DepKind};\n use rustc::hir;\n@@ -50,13 +51,15 @@ use rustc::ich::{Fingerprint, ATTR_DIRTY, ATTR_CLEAN, ATTR_DIRTY_METADATA,\n                  ATTR_CLEAN_METADATA};\n use syntax::ast::{self, Attribute, NestedMetaItem};\n use rustc_data_structures::fx::{FxHashSet, FxHashMap};\n+use rustc_data_structures::indexed_vec::IndexVec;\n use syntax_pos::Span;\n use rustc::ty::TyCtxt;\n \n const LABEL: &'static str = \"label\";\n const CFG: &'static str = \"cfg\";\n \n pub fn check_dirty_clean_annotations<'a, 'tcx>(tcx: TyCtxt<'a, 'tcx, 'tcx>,\n+                                               nodes: &IndexVec<DepNodeIndex, DepNode>,\n                                                dirty_inputs: &DirtyNodes) {\n     // can't add `#[rustc_dirty]` etc without opting in to this feature\n     if !tcx.sess.features.borrow().rustc_attrs {\n@@ -66,8 +69,14 @@ pub fn check_dirty_clean_annotations<'a, 'tcx>(tcx: TyCtxt<'a, 'tcx, 'tcx>,\n     let _ignore = tcx.dep_graph.in_ignore();\n     let dirty_inputs: FxHashSet<DepNode> =\n         dirty_inputs.keys()\n-                    .filter(|dep_node| dep_node.extract_def_id(tcx).is_some())\n-                    .cloned()\n+                    .filter_map(|dep_node_index| {\n+                        let dep_node = nodes[*dep_node_index];\n+                        if dep_node.extract_def_id(tcx).is_some() {\n+                            Some(dep_node)\n+                        } else {\n+                            None\n+                        }\n+                    })\n                     .collect();\n \n     let query = tcx.dep_graph.query();"}, {"sha": "28a00bf4aa6c8fd8ebb34ada7c325f2ab85bd239", "filename": "src/librustc_incremental/persist/load.rs", "status": "modified", "additions": 53, "deletions": 61, "changes": 114, "blob_url": "https://github.com/rust-lang/rust/blob/7f482808f9700259da6585597486166148576d96/src%2Flibrustc_incremental%2Fpersist%2Fload.rs", "raw_url": "https://github.com/rust-lang/rust/raw/7f482808f9700259da6585597486166148576d96/src%2Flibrustc_incremental%2Fpersist%2Fload.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_incremental%2Fpersist%2Fload.rs?ref=7f482808f9700259da6585597486166148576d96", "patch": "@@ -17,9 +17,9 @@ use rustc::ich::Fingerprint;\n use rustc::session::Session;\n use rustc::ty::TyCtxt;\n use rustc_data_structures::fx::{FxHashSet, FxHashMap};\n+use rustc_data_structures::indexed_vec::IndexVec;\n use rustc_serialize::Decodable as RustcDecodable;\n use rustc_serialize::opaque::Decoder;\n-use std::default::Default;\n use std::path::{Path};\n \n use IncrementalHashesMap;\n@@ -32,7 +32,7 @@ use super::work_product;\n \n // The key is a dirty node. The value is **some** base-input that we\n // can blame it on.\n-pub type DirtyNodes = FxHashMap<DepNode, DepNode>;\n+pub type DirtyNodes = FxHashMap<DepNodeIndex, DepNodeIndex>;\n \n /// If we are in incremental mode, and a previous dep-graph exists,\n /// then load up those nodes/edges that are still valid into the\n@@ -166,48 +166,35 @@ pub fn decode_dep_graph<'a, 'tcx>(tcx: TyCtxt<'a, 'tcx, 'tcx>,\n \n     let serialized_dep_graph = SerializedDepGraph::decode(&mut dep_graph_decoder)?;\n \n-    let edge_map: FxHashMap<DepNode, Vec<DepNode>> = {\n-        let capacity = serialized_dep_graph.edge_list_data.len();\n-        let mut edge_map = FxHashMap::with_capacity_and_hasher(capacity, Default::default());\n-\n-        for (node_index, source) in serialized_dep_graph.nodes.iter().enumerate() {\n-            let (start, end) = serialized_dep_graph.edge_list_indices[node_index];\n-            let targets =\n-                (&serialized_dep_graph.edge_list_data[start as usize .. end as usize])\n-                .into_iter()\n-                .map(|&node_index| serialized_dep_graph.nodes[node_index].clone())\n-                .collect();\n-\n-            edge_map.insert(source.clone(), targets);\n-        }\n-\n-        edge_map\n-    };\n-\n     // Compute the set of nodes from the old graph where some input\n-    // has changed or been removed. These are \"raw\" source nodes,\n-    // which means that they still use the original `DefPathIndex`\n-    // values from the encoding, rather than having been retraced to a\n-    // `DefId`. The reason for this is that this way we can include\n-    // nodes that have been removed (which no longer have a `DefId` in\n-    // the current compilation).\n+    // has changed or been removed.\n     let dirty_raw_nodes = initial_dirty_nodes(tcx,\n                                               incremental_hashes_map,\n+                                              &serialized_dep_graph.nodes,\n                                               &serialized_dep_graph.hashes);\n-    let dirty_raw_nodes = transitive_dirty_nodes(&edge_map, dirty_raw_nodes);\n+    let dirty_raw_nodes = transitive_dirty_nodes(&serialized_dep_graph,\n+                                                 dirty_raw_nodes);\n \n     // Recreate the edges in the graph that are still clean.\n     let mut clean_work_products = FxHashSet();\n     let mut dirty_work_products = FxHashSet(); // incomplete; just used to suppress debug output\n-    for (source, targets) in &edge_map {\n-        for target in targets {\n-            process_edge(tcx, source, target, &dirty_raw_nodes,\n-                         &mut clean_work_products, &mut dirty_work_products);\n+    for (source, targets) in serialized_dep_graph.edge_list_indices.iter_enumerated() {\n+        let target_begin = targets.0 as usize;\n+        let target_end = targets.1 as usize;\n+\n+        for &target in &serialized_dep_graph.edge_list_data[target_begin .. target_end] {\n+            process_edge(tcx,\n+                         source,\n+                         target,\n+                         &serialized_dep_graph.nodes,\n+                         &dirty_raw_nodes,\n+                         &mut clean_work_products,\n+                         &mut dirty_work_products);\n         }\n     }\n \n-    // Recreate bootstrap outputs, which are outputs that have no incoming edges (and hence cannot\n-    // be dirty).\n+    // Recreate bootstrap outputs, which are outputs that have no incoming edges\n+    // (and hence cannot be dirty).\n     for bootstrap_output in &serialized_dep_graph.bootstrap_outputs {\n         if let DepKind::WorkProduct = bootstrap_output.kind {\n             let wp_id = WorkProductId::from_fingerprint(bootstrap_output.hash);\n@@ -225,7 +212,9 @@ pub fn decode_dep_graph<'a, 'tcx>(tcx: TyCtxt<'a, 'tcx, 'tcx>,\n     // dirty.\n     reconcile_work_products(tcx, work_products, &clean_work_products);\n \n-    dirty_clean::check_dirty_clean_annotations(tcx, &dirty_raw_nodes);\n+    dirty_clean::check_dirty_clean_annotations(tcx,\n+                                               &serialized_dep_graph.nodes,\n+                                               &dirty_raw_nodes);\n \n     load_prev_metadata_hashes(tcx,\n                               &mut *incremental_hashes_map.prev_metadata_hashes.borrow_mut());\n@@ -236,19 +225,20 @@ pub fn decode_dep_graph<'a, 'tcx>(tcx: TyCtxt<'a, 'tcx, 'tcx>,\n /// a bit vector where the index is the DefPathIndex.\n fn initial_dirty_nodes<'a, 'tcx>(tcx: TyCtxt<'a, 'tcx, 'tcx>,\n                                  incremental_hashes_map: &IncrementalHashesMap,\n-                                 serialized_hashes: &[SerializedHash])\n+                                 nodes: &IndexVec<DepNodeIndex, DepNode>,\n+                                 serialized_hashes: &[(DepNodeIndex, Fingerprint)])\n                                  -> DirtyNodes {\n     let mut hcx = HashContext::new(tcx, incremental_hashes_map);\n     let mut dirty_nodes = FxHashMap();\n \n-    for hash in serialized_hashes {\n-        let dep_node = hash.dep_node;\n+    for &(dep_node_index, prev_hash) in serialized_hashes {\n+        let dep_node = nodes[dep_node_index];\n         if does_still_exist(tcx, &dep_node) {\n             let current_hash = hcx.hash(&dep_node).unwrap_or_else(|| {\n                 bug!(\"Cannot find current ICH for input that still exists?\")\n             });\n \n-            if current_hash == hash.hash {\n+            if current_hash == prev_hash {\n                 debug!(\"initial_dirty_nodes: {:?} is clean (hash={:?})\",\n                        dep_node,\n                        current_hash);\n@@ -259,44 +249,41 @@ fn initial_dirty_nodes<'a, 'tcx>(tcx: TyCtxt<'a, 'tcx, 'tcx>,\n                 println!(\"node {:?} is dirty as hash is {:?}, was {:?}\",\n                          dep_node,\n                          current_hash,\n-                         hash.hash);\n+                         prev_hash);\n             }\n \n             debug!(\"initial_dirty_nodes: {:?} is dirty as hash is {:?}, was {:?}\",\n                    dep_node,\n                    current_hash,\n-                   hash.hash);\n+                   prev_hash);\n         } else {\n             if tcx.sess.opts.debugging_opts.incremental_dump_hash {\n                 println!(\"node {:?} is dirty as it was removed\", dep_node);\n             }\n \n             debug!(\"initial_dirty_nodes: {:?} is dirty as it was removed\", dep_node);\n         }\n-\n-        dirty_nodes.insert(hash.dep_node.clone(), hash.dep_node.clone());\n+        dirty_nodes.insert(dep_node_index, dep_node_index);\n     }\n \n     dirty_nodes\n }\n \n-fn transitive_dirty_nodes(edge_map: &FxHashMap<DepNode, Vec<DepNode>>,\n+fn transitive_dirty_nodes(serialized_dep_graph: &SerializedDepGraph,\n                           mut dirty_nodes: DirtyNodes)\n                           -> DirtyNodes\n {\n-    let mut stack: Vec<(DepNode, DepNode)> = vec![];\n-    stack.extend(dirty_nodes.iter().map(|(s, b)| (s.clone(), b.clone())));\n+    let mut stack: Vec<(DepNodeIndex, DepNodeIndex)> = vec![];\n+    stack.extend(dirty_nodes.iter().map(|(&s, &b)| (s, b)));\n     while let Some((source, blame)) = stack.pop() {\n         // we know the source is dirty (because of the node `blame`)...\n-        assert!(dirty_nodes.contains_key(&source));\n+        debug_assert!(dirty_nodes.contains_key(&source));\n \n         // ...so we dirty all the targets (with the same blame)\n-        if let Some(targets) = edge_map.get(&source) {\n-            for target in targets {\n-                if !dirty_nodes.contains_key(target) {\n-                    dirty_nodes.insert(target.clone(), blame.clone());\n-                    stack.push((target.clone(), blame.clone()));\n-                }\n+        for &target in serialized_dep_graph.edge_targets_from(source) {\n+            if !dirty_nodes.contains_key(&target) {\n+                dirty_nodes.insert(target, blame);\n+                stack.push((target, blame));\n             }\n         }\n     }\n@@ -402,16 +389,18 @@ fn load_prev_metadata_hashes(tcx: TyCtxt,\n \n fn process_edge<'a, 'tcx, 'edges>(\n     tcx: TyCtxt<'a, 'tcx, 'tcx>,\n-    source: &'edges DepNode,\n-    target: &'edges DepNode,\n+    source: DepNodeIndex,\n+    target: DepNodeIndex,\n+    nodes: &IndexVec<DepNodeIndex, DepNode>,\n     dirty_raw_nodes: &DirtyNodes,\n     clean_work_products: &mut FxHashSet<WorkProductId>,\n     dirty_work_products: &mut FxHashSet<WorkProductId>)\n {\n     // If the target is dirty, skip the edge. If this is an edge\n     // that targets a work-product, we can print the blame\n     // information now.\n-    if let Some(blame) = dirty_raw_nodes.get(target) {\n+    if let Some(&blame) = dirty_raw_nodes.get(&target) {\n+        let target = nodes[target];\n         if let DepKind::WorkProduct = target.kind {\n             if tcx.sess.opts.debugging_opts.incremental_info {\n                 let wp_id = WorkProductId::from_fingerprint(target.hash);\n@@ -420,6 +409,7 @@ fn process_edge<'a, 'tcx, 'edges>(\n                     // Try to reconstruct the human-readable version of the\n                     // DepNode. This cannot be done for things that where\n                     // removed.\n+                    let blame = nodes[blame];\n                     let blame_str = if let Some(def_id) = blame.extract_def_id(tcx) {\n                         format!(\"{:?}({})\",\n                                 blame.kind,\n@@ -444,21 +434,23 @@ fn process_edge<'a, 'tcx, 'edges>(\n \n     // We should never have an edge where the target is clean but the source\n     // was dirty. Otherwise something was wrong with the dirtying pass above:\n-    debug_assert!(!dirty_raw_nodes.contains_key(source));\n+    debug_assert!(!dirty_raw_nodes.contains_key(&source));\n \n     // We also never should encounter an edge going from a removed input to a\n     // clean target because removing the input would have dirtied the input\n     // node and transitively dirtied the target.\n-    debug_assert!(match source.kind {\n+    debug_assert!(match nodes[source].kind {\n         DepKind::Hir | DepKind::HirBody | DepKind::MetaData => {\n-            does_still_exist(tcx, source)\n+            does_still_exist(tcx, &nodes[source])\n         }\n         _ => true,\n     });\n \n-    if !dirty_raw_nodes.contains_key(target) {\n-        let _task = tcx.dep_graph.in_task(*target);\n-        tcx.dep_graph.read(*source);\n+    if !dirty_raw_nodes.contains_key(&target) {\n+        let target = nodes[target];\n+        let source = nodes[source];\n+        let _task = tcx.dep_graph.in_task(target);\n+        tcx.dep_graph.read(source);\n \n         if let DepKind::WorkProduct = target.kind {\n             let wp_id = WorkProductId::from_fingerprint(target.hash);"}, {"sha": "867452d97e8fbb124412bd2c44981ca9de00ddd2", "filename": "src/librustc_incremental/persist/save.rs", "status": "modified", "additions": 43, "deletions": 14, "changes": 57, "blob_url": "https://github.com/rust-lang/rust/blob/7f482808f9700259da6585597486166148576d96/src%2Flibrustc_incremental%2Fpersist%2Fsave.rs", "raw_url": "https://github.com/rust-lang/rust/raw/7f482808f9700259da6585597486166148576d96/src%2Flibrustc_incremental%2Fpersist%2Fsave.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_incremental%2Fpersist%2Fsave.rs?ref=7f482808f9700259da6585597486166148576d96", "patch": "@@ -174,14 +174,14 @@ pub fn encode_dep_graph(tcx: TyCtxt,\n     tcx.sess.opts.dep_tracking_hash().encode(encoder)?;\n \n     // NB: We rely on this Vec being indexable by reduced_graph's NodeIndex.\n-    let nodes: IndexVec<DepNodeIndex, DepNode> = preds\n+    let mut nodes: IndexVec<DepNodeIndex, DepNode> = preds\n         .reduced_graph\n         .all_nodes()\n         .iter()\n         .map(|node| node.data.clone())\n         .collect();\n \n-    let mut edge_list_indices = Vec::with_capacity(nodes.len());\n+    let mut edge_list_indices = IndexVec::with_capacity(nodes.len());\n     let mut edge_list_data = Vec::with_capacity(preds.reduced_graph.len_edges());\n \n     for node_index in 0 .. nodes.len() {\n@@ -196,7 +196,7 @@ pub fn encode_dep_graph(tcx: TyCtxt,\n         edge_list_indices.push((start, end));\n     }\n \n-    // Let's make we had no overflow there.\n+    // Let's make sure we had no overflow there.\n     assert!(edge_list_data.len() <= ::std::u32::MAX as usize);\n     // Check that we have a consistent number of edges.\n     assert_eq!(edge_list_data.len(), preds.reduced_graph.len_edges());\n@@ -206,23 +206,52 @@ pub fn encode_dep_graph(tcx: TyCtxt,\n                                  .map(|dep_node| (**dep_node).clone())\n                                  .collect();\n \n-    let hashes = preds\n-        .hashes\n-        .iter()\n-        .map(|(&dep_node, &hash)| {\n-            SerializedHash {\n-                dep_node: dep_node.clone(),\n-                hash: hash,\n-            }\n-        })\n-        .collect();\n+    // Next, build the map of content hashes. To this end, we need to transform\n+    // the (DepNode -> Fingerprint) map that we have into a\n+    // (DepNodeIndex -> Fingerprint) map. This may necessitate adding nodes back\n+    // to the dep-graph that have been filtered out during reduction.\n+    let content_hashes = {\n+        // We have to build a (DepNode -> DepNodeIndex) map. We over-allocate a\n+        // little because we expect some more nodes to be added.\n+        let capacity = (nodes.len() * 120) / 100;\n+        let mut node_to_index = FxHashMap::with_capacity_and_hasher(capacity,\n+                                                                    Default::default());\n+        // Add the nodes we already have in the graph.\n+        node_to_index.extend(nodes.iter_enumerated()\n+                                  .map(|(index, &node)| (node, index)));\n+\n+        let mut content_hashes = Vec::with_capacity(preds.hashes.len());\n+\n+        for (&&dep_node, &hash) in preds.hashes.iter() {\n+            let dep_node_index = *node_to_index\n+                .entry(dep_node)\n+                .or_insert_with(|| {\n+                    // There is no DepNodeIndex for this DepNode yet. This\n+                    // happens when the DepNode got filtered out during graph\n+                    // reduction. Since we have a content hash for the DepNode,\n+                    // we add it back to the graph.\n+                    let next_index = nodes.len();\n+                    nodes.push(dep_node);\n+\n+                    debug_assert_eq!(next_index, edge_list_indices.len());\n+                    // Push an empty list of edges\n+                    edge_list_indices.push((0,0));\n+\n+                    DepNodeIndex::new(next_index)\n+                });\n+\n+            content_hashes.push((dep_node_index, hash));\n+        }\n+\n+        content_hashes\n+    };\n \n     let graph = SerializedDepGraph {\n         nodes,\n         edge_list_indices,\n         edge_list_data,\n         bootstrap_outputs,\n-        hashes,\n+        hashes: content_hashes,\n     };\n \n     // Encode the graph data."}]}