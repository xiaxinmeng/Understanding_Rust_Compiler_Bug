{"sha": "310b9fc76002066feb89dcfbf8e88b34fe5f4ad3", "node_id": "MDY6Q29tbWl0NzI0NzEyOjMxMGI5ZmM3NjAwMjA2NmZlYjg5ZGNmYmY4ZTg4YjM0ZmU1ZjRhZDM=", "commit": {"author": {"name": "Vadim Petrochenkov", "email": "vadim.petrochenkov@gmail.com", "date": "2019-08-01T21:26:40Z"}, "committer": {"name": "Vadim Petrochenkov", "email": "vadim.petrochenkov@gmail.com", "date": "2019-08-01T22:59:01Z"}, "message": "libsyntax: Unconfigure tests during normal build", "tree": {"sha": "e501e5e1aea23c56ae22a86b18215da00e07b3c4", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/e501e5e1aea23c56ae22a86b18215da00e07b3c4"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/310b9fc76002066feb89dcfbf8e88b34fe5f4ad3", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/310b9fc76002066feb89dcfbf8e88b34fe5f4ad3", "html_url": "https://github.com/rust-lang/rust/commit/310b9fc76002066feb89dcfbf8e88b34fe5f4ad3", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/310b9fc76002066feb89dcfbf8e88b34fe5f4ad3/comments", "author": {"login": "petrochenkov", "id": 5751617, "node_id": "MDQ6VXNlcjU3NTE2MTc=", "avatar_url": "https://avatars.githubusercontent.com/u/5751617?v=4", "gravatar_id": "", "url": "https://api.github.com/users/petrochenkov", "html_url": "https://github.com/petrochenkov", "followers_url": "https://api.github.com/users/petrochenkov/followers", "following_url": "https://api.github.com/users/petrochenkov/following{/other_user}", "gists_url": "https://api.github.com/users/petrochenkov/gists{/gist_id}", "starred_url": "https://api.github.com/users/petrochenkov/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/petrochenkov/subscriptions", "organizations_url": "https://api.github.com/users/petrochenkov/orgs", "repos_url": "https://api.github.com/users/petrochenkov/repos", "events_url": "https://api.github.com/users/petrochenkov/events{/privacy}", "received_events_url": "https://api.github.com/users/petrochenkov/received_events", "type": "User", "site_admin": false}, "committer": {"login": "petrochenkov", "id": 5751617, "node_id": "MDQ6VXNlcjU3NTE2MTc=", "avatar_url": "https://avatars.githubusercontent.com/u/5751617?v=4", "gravatar_id": "", "url": "https://api.github.com/users/petrochenkov", "html_url": "https://github.com/petrochenkov", "followers_url": "https://api.github.com/users/petrochenkov/followers", "following_url": "https://api.github.com/users/petrochenkov/following{/other_user}", "gists_url": "https://api.github.com/users/petrochenkov/gists{/gist_id}", "starred_url": "https://api.github.com/users/petrochenkov/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/petrochenkov/subscriptions", "organizations_url": "https://api.github.com/users/petrochenkov/orgs", "repos_url": "https://api.github.com/users/petrochenkov/repos", "events_url": "https://api.github.com/users/petrochenkov/events{/privacy}", "received_events_url": "https://api.github.com/users/petrochenkov/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "a332e224a3bc2925fea584337d2d30e1186672be", "url": "https://api.github.com/repos/rust-lang/rust/commits/a332e224a3bc2925fea584337d2d30e1186672be", "html_url": "https://github.com/rust-lang/rust/commit/a332e224a3bc2925fea584337d2d30e1186672be"}], "stats": {"total": 2580, "additions": 1277, "deletions": 1303}, "files": [{"sha": "87113b4b98efcbea3404508b712999d66410cedc", "filename": "src/libsyntax/ast.rs", "status": "modified", "additions": 3, "deletions": 12, "changes": 15, "blob_url": "https://github.com/rust-lang/rust/blob/310b9fc76002066feb89dcfbf8e88b34fe5f4ad3/src%2Flibsyntax%2Fast.rs", "raw_url": "https://github.com/rust-lang/rust/raw/310b9fc76002066feb89dcfbf8e88b34fe5f4ad3/src%2Flibsyntax%2Fast.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fast.rs?ref=310b9fc76002066feb89dcfbf8e88b34fe5f4ad3", "patch": "@@ -27,6 +27,9 @@ use std::fmt;\n \n pub use rustc_target::abi::FloatTy;\n \n+#[cfg(test)]\n+mod tests;\n+\n #[derive(Clone, RustcEncodable, RustcDecodable, Copy)]\n pub struct Label {\n     pub ident: Ident,\n@@ -2432,15 +2435,3 @@ impl ForeignItemKind {\n         }\n     }\n }\n-\n-#[cfg(test)]\n-mod tests {\n-    use super::*;\n-\n-    // Are ASTs encodable?\n-    #[test]\n-    fn check_asts_encodable() {\n-        fn assert_encodable<T: rustc_serialize::Encodable>() {}\n-        assert_encodable::<Crate>();\n-    }\n-}"}, {"sha": "7558e9cc3a3afd676fde5742d528273bd41a534f", "filename": "src/libsyntax/ast/tests.rs", "status": "added", "additions": 8, "deletions": 0, "changes": 8, "blob_url": "https://github.com/rust-lang/rust/blob/310b9fc76002066feb89dcfbf8e88b34fe5f4ad3/src%2Flibsyntax%2Fast%2Ftests.rs", "raw_url": "https://github.com/rust-lang/rust/raw/310b9fc76002066feb89dcfbf8e88b34fe5f4ad3/src%2Flibsyntax%2Fast%2Ftests.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fast%2Ftests.rs?ref=310b9fc76002066feb89dcfbf8e88b34fe5f4ad3", "patch": "@@ -0,0 +1,8 @@\n+use super::*;\n+\n+// Are ASTs encodable?\n+#[test]\n+fn check_asts_encodable() {\n+    fn assert_encodable<T: rustc_serialize::Encodable>() {}\n+    assert_encodable::<Crate>();\n+}"}, {"sha": "8ac48d8d74a42dcf463854e36358a07010a59ee8", "filename": "src/libsyntax/lib.rs", "status": "modified", "additions": 3, "deletions": 5, "changes": 8, "blob_url": "https://github.com/rust-lang/rust/blob/310b9fc76002066feb89dcfbf8e88b34fe5f4ad3/src%2Flibsyntax%2Flib.rs", "raw_url": "https://github.com/rust-lang/rust/raw/310b9fc76002066feb89dcfbf8e88b34fe5f4ad3/src%2Flibsyntax%2Flib.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Flib.rs?ref=310b9fc76002066feb89dcfbf8e88b34fe5f4ad3", "patch": "@@ -33,6 +33,9 @@ pub use rustc_data_structures::thin_vec::ThinVec;\n use ast::AttrId;\n use syntax_pos::edition::Edition;\n \n+#[cfg(test)]\n+mod tests;\n+\n const MACRO_ARGUMENTS: Option<&'static str> = Some(\"macro arguments\");\n \n // A variant of 'try!' that panics on an Err. This is used as a crutch on the\n@@ -132,8 +135,6 @@ pub mod util {\n     pub mod lev_distance;\n     pub mod node_count;\n     pub mod parser;\n-    #[cfg(test)]\n-    pub mod parser_testing;\n     pub mod map_in_place;\n }\n \n@@ -183,7 +184,4 @@ pub mod ext {\n \n pub mod early_buffered_lints;\n \n-#[cfg(test)]\n-mod test_snippet;\n-\n __build_diagnostic_array! { libsyntax, DIAGNOSTICS }"}, {"sha": "a5085c5f879601cb50629c9fad95aa520a1d39a1", "filename": "src/libsyntax/mut_visit.rs", "status": "modified", "additions": 3, "deletions": 74, "changes": 77, "blob_url": "https://github.com/rust-lang/rust/blob/310b9fc76002066feb89dcfbf8e88b34fe5f4ad3/src%2Flibsyntax%2Fmut_visit.rs", "raw_url": "https://github.com/rust-lang/rust/raw/310b9fc76002066feb89dcfbf8e88b34fe5f4ad3/src%2Flibsyntax%2Fmut_visit.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fmut_visit.rs?ref=310b9fc76002066feb89dcfbf8e88b34fe5f4ad3", "patch": "@@ -22,6 +22,9 @@ use rustc_data_structures::sync::Lrc;\n use std::ops::DerefMut;\n use std::{panic, process, ptr};\n \n+#[cfg(test)]\n+mod tests;\n+\n pub trait ExpectOne<A: Array> {\n     fn expect_one(self, err: &'static str) -> A::Item;\n }\n@@ -1255,77 +1258,3 @@ pub fn noop_visit_vis<T: MutVisitor>(Spanned { node, span }: &mut Visibility, vi\n     }\n     vis.visit_span(span);\n }\n-\n-#[cfg(test)]\n-mod tests {\n-    use crate::ast::{self, Ident};\n-    use crate::util::parser_testing::{string_to_crate, matches_codepattern};\n-    use crate::print::pprust;\n-    use crate::mut_visit;\n-    use crate::with_default_globals;\n-    use super::*;\n-\n-    // this version doesn't care about getting comments or docstrings in.\n-    fn fake_print_crate(s: &mut pprust::State<'_>,\n-                        krate: &ast::Crate) {\n-        s.print_mod(&krate.module, &krate.attrs)\n-    }\n-\n-    // change every identifier to \"zz\"\n-    struct ToZzIdentMutVisitor;\n-\n-    impl MutVisitor for ToZzIdentMutVisitor {\n-        fn visit_ident(&mut self, ident: &mut ast::Ident) {\n-            *ident = Ident::from_str(\"zz\");\n-        }\n-        fn visit_mac(&mut self, mac: &mut ast::Mac) {\n-            mut_visit::noop_visit_mac(mac, self)\n-        }\n-    }\n-\n-    // maybe add to expand.rs...\n-    macro_rules! assert_pred {\n-        ($pred:expr, $predname:expr, $a:expr , $b:expr) => (\n-            {\n-                let pred_val = $pred;\n-                let a_val = $a;\n-                let b_val = $b;\n-                if !(pred_val(&a_val, &b_val)) {\n-                    panic!(\"expected args satisfying {}, got {} and {}\",\n-                          $predname, a_val, b_val);\n-                }\n-            }\n-        )\n-    }\n-\n-    // make sure idents get transformed everywhere\n-    #[test] fn ident_transformation () {\n-        with_default_globals(|| {\n-            let mut zz_visitor = ToZzIdentMutVisitor;\n-            let mut krate = string_to_crate(\n-                \"#[a] mod b {fn c (d : e, f : g) {h!(i,j,k);l;m}}\".to_string());\n-            zz_visitor.visit_crate(&mut krate);\n-            assert_pred!(\n-                matches_codepattern,\n-                \"matches_codepattern\",\n-                pprust::to_string(|s| fake_print_crate(s, &krate)),\n-                \"#[zz]mod zz{fn zz(zz:zz,zz:zz){zz!(zz,zz,zz);zz;zz}}\".to_string());\n-        })\n-    }\n-\n-    // even inside macro defs....\n-    #[test] fn ident_transformation_in_defs () {\n-        with_default_globals(|| {\n-            let mut zz_visitor = ToZzIdentMutVisitor;\n-            let mut krate = string_to_crate(\n-                \"macro_rules! a {(b $c:expr $(d $e:token)f+ => \\\n-                (g $(d $d $e)+))} \".to_string());\n-            zz_visitor.visit_crate(&mut krate);\n-            assert_pred!(\n-                matches_codepattern,\n-                \"matches_codepattern\",\n-                pprust::to_string(|s| fake_print_crate(s, &krate)),\n-                \"macro_rules! zz{(zz$zz:zz$(zz $zz:zz)zz+=>(zz$(zz$zz$zz)+))}\".to_string());\n-        })\n-    }\n-}"}, {"sha": "6868736976b253f9b2ccb150a9e1fc678f784423", "filename": "src/libsyntax/mut_visit/tests.rs", "status": "added", "additions": 71, "deletions": 0, "changes": 71, "blob_url": "https://github.com/rust-lang/rust/blob/310b9fc76002066feb89dcfbf8e88b34fe5f4ad3/src%2Flibsyntax%2Fmut_visit%2Ftests.rs", "raw_url": "https://github.com/rust-lang/rust/raw/310b9fc76002066feb89dcfbf8e88b34fe5f4ad3/src%2Flibsyntax%2Fmut_visit%2Ftests.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fmut_visit%2Ftests.rs?ref=310b9fc76002066feb89dcfbf8e88b34fe5f4ad3", "patch": "@@ -0,0 +1,71 @@\n+use super::*;\n+\n+use crate::ast::{self, Ident};\n+use crate::tests::{string_to_crate, matches_codepattern};\n+use crate::print::pprust;\n+use crate::mut_visit;\n+use crate::with_default_globals;\n+\n+// this version doesn't care about getting comments or docstrings in.\n+fn fake_print_crate(s: &mut pprust::State<'_>,\n+                    krate: &ast::Crate) {\n+    s.print_mod(&krate.module, &krate.attrs)\n+}\n+\n+// change every identifier to \"zz\"\n+struct ToZzIdentMutVisitor;\n+\n+impl MutVisitor for ToZzIdentMutVisitor {\n+    fn visit_ident(&mut self, ident: &mut ast::Ident) {\n+        *ident = Ident::from_str(\"zz\");\n+    }\n+    fn visit_mac(&mut self, mac: &mut ast::Mac) {\n+        mut_visit::noop_visit_mac(mac, self)\n+    }\n+}\n+\n+// maybe add to expand.rs...\n+macro_rules! assert_pred {\n+    ($pred:expr, $predname:expr, $a:expr , $b:expr) => (\n+        {\n+            let pred_val = $pred;\n+            let a_val = $a;\n+            let b_val = $b;\n+            if !(pred_val(&a_val, &b_val)) {\n+                panic!(\"expected args satisfying {}, got {} and {}\",\n+                        $predname, a_val, b_val);\n+            }\n+        }\n+    )\n+}\n+\n+// make sure idents get transformed everywhere\n+#[test] fn ident_transformation () {\n+    with_default_globals(|| {\n+        let mut zz_visitor = ToZzIdentMutVisitor;\n+        let mut krate = string_to_crate(\n+            \"#[a] mod b {fn c (d : e, f : g) {h!(i,j,k);l;m}}\".to_string());\n+        zz_visitor.visit_crate(&mut krate);\n+        assert_pred!(\n+            matches_codepattern,\n+            \"matches_codepattern\",\n+            pprust::to_string(|s| fake_print_crate(s, &krate)),\n+            \"#[zz]mod zz{fn zz(zz:zz,zz:zz){zz!(zz,zz,zz);zz;zz}}\".to_string());\n+    })\n+}\n+\n+// even inside macro defs....\n+#[test] fn ident_transformation_in_defs () {\n+    with_default_globals(|| {\n+        let mut zz_visitor = ToZzIdentMutVisitor;\n+        let mut krate = string_to_crate(\n+            \"macro_rules! a {(b $c:expr $(d $e:token)f+ => \\\n+            (g $(d $d $e)+))} \".to_string());\n+        zz_visitor.visit_crate(&mut krate);\n+        assert_pred!(\n+            matches_codepattern,\n+            \"matches_codepattern\",\n+            pprust::to_string(|s| fake_print_crate(s, &krate)),\n+            \"macro_rules! zz{(zz$zz:zz$(zz $zz:zz)zz+=>(zz$(zz$zz$zz)+))}\".to_string());\n+    })\n+}"}, {"sha": "5121a9ef7b5fca2561fb727c387dff0939892c7b", "filename": "src/libsyntax/parse/lexer/comments.rs", "status": "modified", "additions": 3, "deletions": 51, "changes": 54, "blob_url": "https://github.com/rust-lang/rust/blob/310b9fc76002066feb89dcfbf8e88b34fe5f4ad3/src%2Flibsyntax%2Fparse%2Flexer%2Fcomments.rs", "raw_url": "https://github.com/rust-lang/rust/raw/310b9fc76002066feb89dcfbf8e88b34fe5f4ad3/src%2Flibsyntax%2Fparse%2Flexer%2Fcomments.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Flexer%2Fcomments.rs?ref=310b9fc76002066feb89dcfbf8e88b34fe5f4ad3", "patch": "@@ -9,6 +9,9 @@ use syntax_pos::{BytePos, CharPos, Pos, FileName};\n \n use std::usize;\n \n+#[cfg(test)]\n+mod tests;\n+\n #[derive(Clone, Copy, PartialEq, Debug)]\n pub enum CommentStyle {\n     /// No code on either side of each line of the comment\n@@ -249,54 +252,3 @@ pub fn gather_comments(sess: &ParseSess, path: FileName, src: String) -> Vec<Com\n \n     comments\n }\n-\n-#[cfg(test)]\n-mod tests {\n-    use super::*;\n-\n-    #[test]\n-    fn test_block_doc_comment_1() {\n-        let comment = \"/**\\n * Test \\n **  Test\\n *   Test\\n*/\";\n-        let stripped = strip_doc_comment_decoration(comment);\n-        assert_eq!(stripped, \" Test \\n*  Test\\n   Test\");\n-    }\n-\n-    #[test]\n-    fn test_block_doc_comment_2() {\n-        let comment = \"/**\\n * Test\\n *  Test\\n*/\";\n-        let stripped = strip_doc_comment_decoration(comment);\n-        assert_eq!(stripped, \" Test\\n  Test\");\n-    }\n-\n-    #[test]\n-    fn test_block_doc_comment_3() {\n-        let comment = \"/**\\n let a: *i32;\\n *a = 5;\\n*/\";\n-        let stripped = strip_doc_comment_decoration(comment);\n-        assert_eq!(stripped, \" let a: *i32;\\n *a = 5;\");\n-    }\n-\n-    #[test]\n-    fn test_block_doc_comment_4() {\n-        let comment = \"/*******************\\n test\\n *********************/\";\n-        let stripped = strip_doc_comment_decoration(comment);\n-        assert_eq!(stripped, \" test\");\n-    }\n-\n-    #[test]\n-    fn test_line_doc_comment() {\n-        let stripped = strip_doc_comment_decoration(\"/// test\");\n-        assert_eq!(stripped, \" test\");\n-        let stripped = strip_doc_comment_decoration(\"///! test\");\n-        assert_eq!(stripped, \" test\");\n-        let stripped = strip_doc_comment_decoration(\"// test\");\n-        assert_eq!(stripped, \" test\");\n-        let stripped = strip_doc_comment_decoration(\"// test\");\n-        assert_eq!(stripped, \" test\");\n-        let stripped = strip_doc_comment_decoration(\"///test\");\n-        assert_eq!(stripped, \"test\");\n-        let stripped = strip_doc_comment_decoration(\"///!test\");\n-        assert_eq!(stripped, \"test\");\n-        let stripped = strip_doc_comment_decoration(\"//test\");\n-        assert_eq!(stripped, \"test\");\n-    }\n-}"}, {"sha": "f9cd69fb50d7472c9b3044c9c5f2fc3be279cdb2", "filename": "src/libsyntax/parse/lexer/comments/tests.rs", "status": "added", "additions": 47, "deletions": 0, "changes": 47, "blob_url": "https://github.com/rust-lang/rust/blob/310b9fc76002066feb89dcfbf8e88b34fe5f4ad3/src%2Flibsyntax%2Fparse%2Flexer%2Fcomments%2Ftests.rs", "raw_url": "https://github.com/rust-lang/rust/raw/310b9fc76002066feb89dcfbf8e88b34fe5f4ad3/src%2Flibsyntax%2Fparse%2Flexer%2Fcomments%2Ftests.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Flexer%2Fcomments%2Ftests.rs?ref=310b9fc76002066feb89dcfbf8e88b34fe5f4ad3", "patch": "@@ -0,0 +1,47 @@\n+use super::*;\n+\n+#[test]\n+fn test_block_doc_comment_1() {\n+    let comment = \"/**\\n * Test \\n **  Test\\n *   Test\\n*/\";\n+    let stripped = strip_doc_comment_decoration(comment);\n+    assert_eq!(stripped, \" Test \\n*  Test\\n   Test\");\n+}\n+\n+#[test]\n+fn test_block_doc_comment_2() {\n+    let comment = \"/**\\n * Test\\n *  Test\\n*/\";\n+    let stripped = strip_doc_comment_decoration(comment);\n+    assert_eq!(stripped, \" Test\\n  Test\");\n+}\n+\n+#[test]\n+fn test_block_doc_comment_3() {\n+    let comment = \"/**\\n let a: *i32;\\n *a = 5;\\n*/\";\n+    let stripped = strip_doc_comment_decoration(comment);\n+    assert_eq!(stripped, \" let a: *i32;\\n *a = 5;\");\n+}\n+\n+#[test]\n+fn test_block_doc_comment_4() {\n+    let comment = \"/*******************\\n test\\n *********************/\";\n+    let stripped = strip_doc_comment_decoration(comment);\n+    assert_eq!(stripped, \" test\");\n+}\n+\n+#[test]\n+fn test_line_doc_comment() {\n+    let stripped = strip_doc_comment_decoration(\"/// test\");\n+    assert_eq!(stripped, \" test\");\n+    let stripped = strip_doc_comment_decoration(\"///! test\");\n+    assert_eq!(stripped, \" test\");\n+    let stripped = strip_doc_comment_decoration(\"// test\");\n+    assert_eq!(stripped, \" test\");\n+    let stripped = strip_doc_comment_decoration(\"// test\");\n+    assert_eq!(stripped, \" test\");\n+    let stripped = strip_doc_comment_decoration(\"///test\");\n+    assert_eq!(stripped, \"test\");\n+    let stripped = strip_doc_comment_decoration(\"///!test\");\n+    assert_eq!(stripped, \"test\");\n+    let stripped = strip_doc_comment_decoration(\"//test\");\n+    assert_eq!(stripped, \"test\");\n+}"}, {"sha": "950b1b2ff53404639abe034df636a5b1e1b1f900", "filename": "src/libsyntax/parse/lexer/mod.rs", "status": "modified", "additions": 3, "deletions": 259, "changes": 262, "blob_url": "https://github.com/rust-lang/rust/blob/310b9fc76002066feb89dcfbf8e88b34fe5f4ad3/src%2Flibsyntax%2Fparse%2Flexer%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/310b9fc76002066feb89dcfbf8e88b34fe5f4ad3/src%2Flibsyntax%2Fparse%2Flexer%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Flexer%2Fmod.rs?ref=310b9fc76002066feb89dcfbf8e88b34fe5f4ad3", "patch": "@@ -15,6 +15,9 @@ use std::convert::TryInto;\n use rustc_data_structures::sync::Lrc;\n use log::debug;\n \n+#[cfg(test)]\n+mod tests;\n+\n pub mod comments;\n mod tokentrees;\n mod unicode_chars;\n@@ -777,262 +780,3 @@ fn is_block_doc_comment(s: &str) -> bool {\n     debug!(\"is {:?} a doc comment? {}\", s, res);\n     res\n }\n-\n-#[cfg(test)]\n-mod tests {\n-    use super::*;\n-\n-    use crate::ast::CrateConfig;\n-    use crate::symbol::Symbol;\n-    use crate::source_map::{SourceMap, FilePathMapping};\n-    use crate::feature_gate::UnstableFeatures;\n-    use crate::parse::token;\n-    use crate::diagnostics::plugin::ErrorMap;\n-    use crate::with_default_globals;\n-    use std::io;\n-    use std::path::PathBuf;\n-    use syntax_pos::{BytePos, Span, NO_EXPANSION, edition::Edition};\n-    use rustc_data_structures::fx::{FxHashSet, FxHashMap};\n-    use rustc_data_structures::sync::{Lock, Once};\n-\n-    fn mk_sess(sm: Lrc<SourceMap>) -> ParseSess {\n-        let emitter = errors::emitter::EmitterWriter::new(Box::new(io::sink()),\n-                                                          Some(sm.clone()),\n-                                                          false,\n-                                                          false,\n-                                                          false);\n-        ParseSess {\n-            span_diagnostic: errors::Handler::with_emitter(true, None, Box::new(emitter)),\n-            unstable_features: UnstableFeatures::from_environment(),\n-            config: CrateConfig::default(),\n-            included_mod_stack: Lock::new(Vec::new()),\n-            source_map: sm,\n-            missing_fragment_specifiers: Lock::new(FxHashSet::default()),\n-            raw_identifier_spans: Lock::new(Vec::new()),\n-            registered_diagnostics: Lock::new(ErrorMap::new()),\n-            buffered_lints: Lock::new(vec![]),\n-            edition: Edition::from_session(),\n-            ambiguous_block_expr_parse: Lock::new(FxHashMap::default()),\n-            param_attr_spans: Lock::new(Vec::new()),\n-            let_chains_spans: Lock::new(Vec::new()),\n-            async_closure_spans: Lock::new(Vec::new()),\n-            injected_crate_name: Once::new(),\n-        }\n-    }\n-\n-    // open a string reader for the given string\n-    fn setup<'a>(sm: &SourceMap,\n-                 sess: &'a ParseSess,\n-                 teststr: String)\n-                 -> StringReader<'a> {\n-        let sf = sm.new_source_file(PathBuf::from(teststr.clone()).into(), teststr);\n-        StringReader::new(sess, sf, None)\n-    }\n-\n-    #[test]\n-    fn t1() {\n-        with_default_globals(|| {\n-            let sm = Lrc::new(SourceMap::new(FilePathMapping::empty()));\n-            let sh = mk_sess(sm.clone());\n-            let mut string_reader = setup(&sm,\n-                                        &sh,\n-                                        \"/* my source file */ fn main() { println!(\\\"zebra\\\"); }\\n\"\n-                                            .to_string());\n-            assert_eq!(string_reader.next_token(), token::Comment);\n-            assert_eq!(string_reader.next_token(), token::Whitespace);\n-            let tok1 = string_reader.next_token();\n-            let tok2 = Token::new(\n-                mk_ident(\"fn\"),\n-                Span::new(BytePos(21), BytePos(23), NO_EXPANSION),\n-            );\n-            assert_eq!(tok1.kind, tok2.kind);\n-            assert_eq!(tok1.span, tok2.span);\n-            assert_eq!(string_reader.next_token(), token::Whitespace);\n-            // read another token:\n-            let tok3 = string_reader.next_token();\n-            assert_eq!(string_reader.pos.clone(), BytePos(28));\n-            let tok4 = Token::new(\n-                mk_ident(\"main\"),\n-                Span::new(BytePos(24), BytePos(28), NO_EXPANSION),\n-            );\n-            assert_eq!(tok3.kind, tok4.kind);\n-            assert_eq!(tok3.span, tok4.span);\n-\n-            assert_eq!(string_reader.next_token(), token::OpenDelim(token::Paren));\n-            assert_eq!(string_reader.pos.clone(), BytePos(29))\n-        })\n-    }\n-\n-    // check that the given reader produces the desired stream\n-    // of tokens (stop checking after exhausting the expected vec)\n-    fn check_tokenization(mut string_reader: StringReader<'_>, expected: Vec<TokenKind>) {\n-        for expected_tok in &expected {\n-            assert_eq!(&string_reader.next_token(), expected_tok);\n-        }\n-    }\n-\n-    // make the identifier by looking up the string in the interner\n-    fn mk_ident(id: &str) -> TokenKind {\n-        token::Ident(Symbol::intern(id), false)\n-    }\n-\n-    fn mk_lit(kind: token::LitKind, symbol: &str, suffix: Option<&str>) -> TokenKind {\n-        TokenKind::lit(kind, Symbol::intern(symbol), suffix.map(Symbol::intern))\n-    }\n-\n-    #[test]\n-    fn doublecolonparsing() {\n-        with_default_globals(|| {\n-            let sm = Lrc::new(SourceMap::new(FilePathMapping::empty()));\n-            let sh = mk_sess(sm.clone());\n-            check_tokenization(setup(&sm, &sh, \"a b\".to_string()),\n-                            vec![mk_ident(\"a\"), token::Whitespace, mk_ident(\"b\")]);\n-        })\n-    }\n-\n-    #[test]\n-    fn dcparsing_2() {\n-        with_default_globals(|| {\n-            let sm = Lrc::new(SourceMap::new(FilePathMapping::empty()));\n-            let sh = mk_sess(sm.clone());\n-            check_tokenization(setup(&sm, &sh, \"a::b\".to_string()),\n-                            vec![mk_ident(\"a\"), token::ModSep, mk_ident(\"b\")]);\n-        })\n-    }\n-\n-    #[test]\n-    fn dcparsing_3() {\n-        with_default_globals(|| {\n-            let sm = Lrc::new(SourceMap::new(FilePathMapping::empty()));\n-            let sh = mk_sess(sm.clone());\n-            check_tokenization(setup(&sm, &sh, \"a ::b\".to_string()),\n-                            vec![mk_ident(\"a\"), token::Whitespace, token::ModSep, mk_ident(\"b\")]);\n-        })\n-    }\n-\n-    #[test]\n-    fn dcparsing_4() {\n-        with_default_globals(|| {\n-            let sm = Lrc::new(SourceMap::new(FilePathMapping::empty()));\n-            let sh = mk_sess(sm.clone());\n-            check_tokenization(setup(&sm, &sh, \"a:: b\".to_string()),\n-                            vec![mk_ident(\"a\"), token::ModSep, token::Whitespace, mk_ident(\"b\")]);\n-        })\n-    }\n-\n-    #[test]\n-    fn character_a() {\n-        with_default_globals(|| {\n-            let sm = Lrc::new(SourceMap::new(FilePathMapping::empty()));\n-            let sh = mk_sess(sm.clone());\n-            assert_eq!(setup(&sm, &sh, \"'a'\".to_string()).next_token(),\n-                       mk_lit(token::Char, \"a\", None));\n-        })\n-    }\n-\n-    #[test]\n-    fn character_space() {\n-        with_default_globals(|| {\n-            let sm = Lrc::new(SourceMap::new(FilePathMapping::empty()));\n-            let sh = mk_sess(sm.clone());\n-            assert_eq!(setup(&sm, &sh, \"' '\".to_string()).next_token(),\n-                       mk_lit(token::Char, \" \", None));\n-        })\n-    }\n-\n-    #[test]\n-    fn character_escaped() {\n-        with_default_globals(|| {\n-            let sm = Lrc::new(SourceMap::new(FilePathMapping::empty()));\n-            let sh = mk_sess(sm.clone());\n-            assert_eq!(setup(&sm, &sh, \"'\\\\n'\".to_string()).next_token(),\n-                       mk_lit(token::Char, \"\\\\n\", None));\n-        })\n-    }\n-\n-    #[test]\n-    fn lifetime_name() {\n-        with_default_globals(|| {\n-            let sm = Lrc::new(SourceMap::new(FilePathMapping::empty()));\n-            let sh = mk_sess(sm.clone());\n-            assert_eq!(setup(&sm, &sh, \"'abc\".to_string()).next_token(),\n-                       token::Lifetime(Symbol::intern(\"'abc\")));\n-        })\n-    }\n-\n-    #[test]\n-    fn raw_string() {\n-        with_default_globals(|| {\n-            let sm = Lrc::new(SourceMap::new(FilePathMapping::empty()));\n-            let sh = mk_sess(sm.clone());\n-            assert_eq!(setup(&sm, &sh, \"r###\\\"\\\"#a\\\\b\\x00c\\\"\\\"###\".to_string()).next_token(),\n-                       mk_lit(token::StrRaw(3), \"\\\"#a\\\\b\\x00c\\\"\", None));\n-        })\n-    }\n-\n-    #[test]\n-    fn literal_suffixes() {\n-        with_default_globals(|| {\n-            let sm = Lrc::new(SourceMap::new(FilePathMapping::empty()));\n-            let sh = mk_sess(sm.clone());\n-            macro_rules! test {\n-                ($input: expr, $tok_type: ident, $tok_contents: expr) => {{\n-                    assert_eq!(setup(&sm, &sh, format!(\"{}suffix\", $input)).next_token(),\n-                               mk_lit(token::$tok_type, $tok_contents, Some(\"suffix\")));\n-                    // with a whitespace separator:\n-                    assert_eq!(setup(&sm, &sh, format!(\"{} suffix\", $input)).next_token(),\n-                               mk_lit(token::$tok_type, $tok_contents, None));\n-                }}\n-            }\n-\n-            test!(\"'a'\", Char, \"a\");\n-            test!(\"b'a'\", Byte, \"a\");\n-            test!(\"\\\"a\\\"\", Str, \"a\");\n-            test!(\"b\\\"a\\\"\", ByteStr, \"a\");\n-            test!(\"1234\", Integer, \"1234\");\n-            test!(\"0b101\", Integer, \"0b101\");\n-            test!(\"0xABC\", Integer, \"0xABC\");\n-            test!(\"1.0\", Float, \"1.0\");\n-            test!(\"1.0e10\", Float, \"1.0e10\");\n-\n-            assert_eq!(setup(&sm, &sh, \"2us\".to_string()).next_token(),\n-                       mk_lit(token::Integer, \"2\", Some(\"us\")));\n-            assert_eq!(setup(&sm, &sh, \"r###\\\"raw\\\"###suffix\".to_string()).next_token(),\n-                       mk_lit(token::StrRaw(3), \"raw\", Some(\"suffix\")));\n-            assert_eq!(setup(&sm, &sh, \"br###\\\"raw\\\"###suffix\".to_string()).next_token(),\n-                       mk_lit(token::ByteStrRaw(3), \"raw\", Some(\"suffix\")));\n-        })\n-    }\n-\n-    #[test]\n-    fn line_doc_comments() {\n-        assert!(is_doc_comment(\"///\"));\n-        assert!(is_doc_comment(\"/// blah\"));\n-        assert!(!is_doc_comment(\"////\"));\n-    }\n-\n-    #[test]\n-    fn nested_block_comments() {\n-        with_default_globals(|| {\n-            let sm = Lrc::new(SourceMap::new(FilePathMapping::empty()));\n-            let sh = mk_sess(sm.clone());\n-            let mut lexer = setup(&sm, &sh, \"/* /* */ */'a'\".to_string());\n-            assert_eq!(lexer.next_token(), token::Comment);\n-            assert_eq!(lexer.next_token(), mk_lit(token::Char, \"a\", None));\n-        })\n-    }\n-\n-    #[test]\n-    fn crlf_comments() {\n-        with_default_globals(|| {\n-            let sm = Lrc::new(SourceMap::new(FilePathMapping::empty()));\n-            let sh = mk_sess(sm.clone());\n-            let mut lexer = setup(&sm, &sh, \"// test\\r\\n/// test\\r\\n\".to_string());\n-            let comment = lexer.next_token();\n-            assert_eq!(comment.kind, token::Comment);\n-            assert_eq!((comment.span.lo(), comment.span.hi()), (BytePos(0), BytePos(7)));\n-            assert_eq!(lexer.next_token(), token::Whitespace);\n-            assert_eq!(lexer.next_token(), token::DocComment(Symbol::intern(\"/// test\")));\n-        })\n-    }\n-}"}, {"sha": "fc47e4f0b185ac219b164ddb646dedea5f465619", "filename": "src/libsyntax/parse/lexer/tests.rs", "status": "added", "additions": 255, "deletions": 0, "changes": 255, "blob_url": "https://github.com/rust-lang/rust/blob/310b9fc76002066feb89dcfbf8e88b34fe5f4ad3/src%2Flibsyntax%2Fparse%2Flexer%2Ftests.rs", "raw_url": "https://github.com/rust-lang/rust/raw/310b9fc76002066feb89dcfbf8e88b34fe5f4ad3/src%2Flibsyntax%2Fparse%2Flexer%2Ftests.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Flexer%2Ftests.rs?ref=310b9fc76002066feb89dcfbf8e88b34fe5f4ad3", "patch": "@@ -0,0 +1,255 @@\n+use super::*;\n+\n+use crate::ast::CrateConfig;\n+use crate::symbol::Symbol;\n+use crate::source_map::{SourceMap, FilePathMapping};\n+use crate::feature_gate::UnstableFeatures;\n+use crate::parse::token;\n+use crate::diagnostics::plugin::ErrorMap;\n+use crate::with_default_globals;\n+use std::io;\n+use std::path::PathBuf;\n+use syntax_pos::{BytePos, Span, NO_EXPANSION, edition::Edition};\n+use rustc_data_structures::fx::{FxHashSet, FxHashMap};\n+use rustc_data_structures::sync::{Lock, Once};\n+\n+fn mk_sess(sm: Lrc<SourceMap>) -> ParseSess {\n+    let emitter = errors::emitter::EmitterWriter::new(Box::new(io::sink()),\n+                                                        Some(sm.clone()),\n+                                                        false,\n+                                                        false,\n+                                                        false);\n+    ParseSess {\n+        span_diagnostic: errors::Handler::with_emitter(true, None, Box::new(emitter)),\n+        unstable_features: UnstableFeatures::from_environment(),\n+        config: CrateConfig::default(),\n+        included_mod_stack: Lock::new(Vec::new()),\n+        source_map: sm,\n+        missing_fragment_specifiers: Lock::new(FxHashSet::default()),\n+        raw_identifier_spans: Lock::new(Vec::new()),\n+        registered_diagnostics: Lock::new(ErrorMap::new()),\n+        buffered_lints: Lock::new(vec![]),\n+        edition: Edition::from_session(),\n+        ambiguous_block_expr_parse: Lock::new(FxHashMap::default()),\n+        param_attr_spans: Lock::new(Vec::new()),\n+        let_chains_spans: Lock::new(Vec::new()),\n+        async_closure_spans: Lock::new(Vec::new()),\n+        injected_crate_name: Once::new(),\n+    }\n+}\n+\n+// open a string reader for the given string\n+fn setup<'a>(sm: &SourceMap,\n+                sess: &'a ParseSess,\n+                teststr: String)\n+                -> StringReader<'a> {\n+    let sf = sm.new_source_file(PathBuf::from(teststr.clone()).into(), teststr);\n+    StringReader::new(sess, sf, None)\n+}\n+\n+#[test]\n+fn t1() {\n+    with_default_globals(|| {\n+        let sm = Lrc::new(SourceMap::new(FilePathMapping::empty()));\n+        let sh = mk_sess(sm.clone());\n+        let mut string_reader = setup(&sm,\n+                                    &sh,\n+                                    \"/* my source file */ fn main() { println!(\\\"zebra\\\"); }\\n\"\n+                                        .to_string());\n+        assert_eq!(string_reader.next_token(), token::Comment);\n+        assert_eq!(string_reader.next_token(), token::Whitespace);\n+        let tok1 = string_reader.next_token();\n+        let tok2 = Token::new(\n+            mk_ident(\"fn\"),\n+            Span::new(BytePos(21), BytePos(23), NO_EXPANSION),\n+        );\n+        assert_eq!(tok1.kind, tok2.kind);\n+        assert_eq!(tok1.span, tok2.span);\n+        assert_eq!(string_reader.next_token(), token::Whitespace);\n+        // read another token:\n+        let tok3 = string_reader.next_token();\n+        assert_eq!(string_reader.pos.clone(), BytePos(28));\n+        let tok4 = Token::new(\n+            mk_ident(\"main\"),\n+            Span::new(BytePos(24), BytePos(28), NO_EXPANSION),\n+        );\n+        assert_eq!(tok3.kind, tok4.kind);\n+        assert_eq!(tok3.span, tok4.span);\n+\n+        assert_eq!(string_reader.next_token(), token::OpenDelim(token::Paren));\n+        assert_eq!(string_reader.pos.clone(), BytePos(29))\n+    })\n+}\n+\n+// check that the given reader produces the desired stream\n+// of tokens (stop checking after exhausting the expected vec)\n+fn check_tokenization(mut string_reader: StringReader<'_>, expected: Vec<TokenKind>) {\n+    for expected_tok in &expected {\n+        assert_eq!(&string_reader.next_token(), expected_tok);\n+    }\n+}\n+\n+// make the identifier by looking up the string in the interner\n+fn mk_ident(id: &str) -> TokenKind {\n+    token::Ident(Symbol::intern(id), false)\n+}\n+\n+fn mk_lit(kind: token::LitKind, symbol: &str, suffix: Option<&str>) -> TokenKind {\n+    TokenKind::lit(kind, Symbol::intern(symbol), suffix.map(Symbol::intern))\n+}\n+\n+#[test]\n+fn doublecolonparsing() {\n+    with_default_globals(|| {\n+        let sm = Lrc::new(SourceMap::new(FilePathMapping::empty()));\n+        let sh = mk_sess(sm.clone());\n+        check_tokenization(setup(&sm, &sh, \"a b\".to_string()),\n+                        vec![mk_ident(\"a\"), token::Whitespace, mk_ident(\"b\")]);\n+    })\n+}\n+\n+#[test]\n+fn dcparsing_2() {\n+    with_default_globals(|| {\n+        let sm = Lrc::new(SourceMap::new(FilePathMapping::empty()));\n+        let sh = mk_sess(sm.clone());\n+        check_tokenization(setup(&sm, &sh, \"a::b\".to_string()),\n+                        vec![mk_ident(\"a\"), token::ModSep, mk_ident(\"b\")]);\n+    })\n+}\n+\n+#[test]\n+fn dcparsing_3() {\n+    with_default_globals(|| {\n+        let sm = Lrc::new(SourceMap::new(FilePathMapping::empty()));\n+        let sh = mk_sess(sm.clone());\n+        check_tokenization(setup(&sm, &sh, \"a ::b\".to_string()),\n+                        vec![mk_ident(\"a\"), token::Whitespace, token::ModSep, mk_ident(\"b\")]);\n+    })\n+}\n+\n+#[test]\n+fn dcparsing_4() {\n+    with_default_globals(|| {\n+        let sm = Lrc::new(SourceMap::new(FilePathMapping::empty()));\n+        let sh = mk_sess(sm.clone());\n+        check_tokenization(setup(&sm, &sh, \"a:: b\".to_string()),\n+                        vec![mk_ident(\"a\"), token::ModSep, token::Whitespace, mk_ident(\"b\")]);\n+    })\n+}\n+\n+#[test]\n+fn character_a() {\n+    with_default_globals(|| {\n+        let sm = Lrc::new(SourceMap::new(FilePathMapping::empty()));\n+        let sh = mk_sess(sm.clone());\n+        assert_eq!(setup(&sm, &sh, \"'a'\".to_string()).next_token(),\n+                    mk_lit(token::Char, \"a\", None));\n+    })\n+}\n+\n+#[test]\n+fn character_space() {\n+    with_default_globals(|| {\n+        let sm = Lrc::new(SourceMap::new(FilePathMapping::empty()));\n+        let sh = mk_sess(sm.clone());\n+        assert_eq!(setup(&sm, &sh, \"' '\".to_string()).next_token(),\n+                    mk_lit(token::Char, \" \", None));\n+    })\n+}\n+\n+#[test]\n+fn character_escaped() {\n+    with_default_globals(|| {\n+        let sm = Lrc::new(SourceMap::new(FilePathMapping::empty()));\n+        let sh = mk_sess(sm.clone());\n+        assert_eq!(setup(&sm, &sh, \"'\\\\n'\".to_string()).next_token(),\n+                    mk_lit(token::Char, \"\\\\n\", None));\n+    })\n+}\n+\n+#[test]\n+fn lifetime_name() {\n+    with_default_globals(|| {\n+        let sm = Lrc::new(SourceMap::new(FilePathMapping::empty()));\n+        let sh = mk_sess(sm.clone());\n+        assert_eq!(setup(&sm, &sh, \"'abc\".to_string()).next_token(),\n+                    token::Lifetime(Symbol::intern(\"'abc\")));\n+    })\n+}\n+\n+#[test]\n+fn raw_string() {\n+    with_default_globals(|| {\n+        let sm = Lrc::new(SourceMap::new(FilePathMapping::empty()));\n+        let sh = mk_sess(sm.clone());\n+        assert_eq!(setup(&sm, &sh, \"r###\\\"\\\"#a\\\\b\\x00c\\\"\\\"###\".to_string()).next_token(),\n+                    mk_lit(token::StrRaw(3), \"\\\"#a\\\\b\\x00c\\\"\", None));\n+    })\n+}\n+\n+#[test]\n+fn literal_suffixes() {\n+    with_default_globals(|| {\n+        let sm = Lrc::new(SourceMap::new(FilePathMapping::empty()));\n+        let sh = mk_sess(sm.clone());\n+        macro_rules! test {\n+            ($input: expr, $tok_type: ident, $tok_contents: expr) => {{\n+                assert_eq!(setup(&sm, &sh, format!(\"{}suffix\", $input)).next_token(),\n+                            mk_lit(token::$tok_type, $tok_contents, Some(\"suffix\")));\n+                // with a whitespace separator:\n+                assert_eq!(setup(&sm, &sh, format!(\"{} suffix\", $input)).next_token(),\n+                            mk_lit(token::$tok_type, $tok_contents, None));\n+            }}\n+        }\n+\n+        test!(\"'a'\", Char, \"a\");\n+        test!(\"b'a'\", Byte, \"a\");\n+        test!(\"\\\"a\\\"\", Str, \"a\");\n+        test!(\"b\\\"a\\\"\", ByteStr, \"a\");\n+        test!(\"1234\", Integer, \"1234\");\n+        test!(\"0b101\", Integer, \"0b101\");\n+        test!(\"0xABC\", Integer, \"0xABC\");\n+        test!(\"1.0\", Float, \"1.0\");\n+        test!(\"1.0e10\", Float, \"1.0e10\");\n+\n+        assert_eq!(setup(&sm, &sh, \"2us\".to_string()).next_token(),\n+                    mk_lit(token::Integer, \"2\", Some(\"us\")));\n+        assert_eq!(setup(&sm, &sh, \"r###\\\"raw\\\"###suffix\".to_string()).next_token(),\n+                    mk_lit(token::StrRaw(3), \"raw\", Some(\"suffix\")));\n+        assert_eq!(setup(&sm, &sh, \"br###\\\"raw\\\"###suffix\".to_string()).next_token(),\n+                    mk_lit(token::ByteStrRaw(3), \"raw\", Some(\"suffix\")));\n+    })\n+}\n+\n+#[test]\n+fn line_doc_comments() {\n+    assert!(is_doc_comment(\"///\"));\n+    assert!(is_doc_comment(\"/// blah\"));\n+    assert!(!is_doc_comment(\"////\"));\n+}\n+\n+#[test]\n+fn nested_block_comments() {\n+    with_default_globals(|| {\n+        let sm = Lrc::new(SourceMap::new(FilePathMapping::empty()));\n+        let sh = mk_sess(sm.clone());\n+        let mut lexer = setup(&sm, &sh, \"/* /* */ */'a'\".to_string());\n+        assert_eq!(lexer.next_token(), token::Comment);\n+        assert_eq!(lexer.next_token(), mk_lit(token::Char, \"a\", None));\n+    })\n+}\n+\n+#[test]\n+fn crlf_comments() {\n+    with_default_globals(|| {\n+        let sm = Lrc::new(SourceMap::new(FilePathMapping::empty()));\n+        let sh = mk_sess(sm.clone());\n+        let mut lexer = setup(&sm, &sh, \"// test\\r\\n/// test\\r\\n\".to_string());\n+        let comment = lexer.next_token();\n+        assert_eq!(comment.kind, token::Comment);\n+        assert_eq!((comment.span.lo(), comment.span.hi()), (BytePos(0), BytePos(7)));\n+        assert_eq!(lexer.next_token(), token::Whitespace);\n+        assert_eq!(lexer.next_token(), token::DocComment(Symbol::intern(\"/// test\")));\n+    })\n+}"}, {"sha": "b7deee688cab86a764898de7417a0c5dbe24f304", "filename": "src/libsyntax/parse/mod.rs", "status": "modified", "additions": 4, "deletions": 292, "changes": 296, "blob_url": "https://github.com/rust-lang/rust/blob/310b9fc76002066feb89dcfbf8e88b34fe5f4ad3/src%2Flibsyntax%2Fparse%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/310b9fc76002066feb89dcfbf8e88b34fe5f4ad3/src%2Flibsyntax%2Fparse%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Fmod.rs?ref=310b9fc76002066feb89dcfbf8e88b34fe5f4ad3", "patch": "@@ -22,7 +22,8 @@ use std::borrow::Cow;\n use std::path::{Path, PathBuf};\n use std::str;\n \n-pub type PResult<'a, T> = Result<T, DiagnosticBuilder<'a>>;\n+#[cfg(test)]\n+mod tests;\n \n #[macro_use]\n pub mod parser;\n@@ -35,6 +36,8 @@ crate mod diagnostics;\n crate mod literal;\n crate mod unescape_error_reporting;\n \n+pub type PResult<'a, T> = Result<T, DiagnosticBuilder<'a>>;\n+\n /// Info about a parsing session.\n pub struct ParseSess {\n     pub span_diagnostic: Handler,\n@@ -389,294 +392,3 @@ impl SeqSep {\n         }\n     }\n }\n-\n-#[cfg(test)]\n-mod tests {\n-    use super::*;\n-    use crate::ast::{self, Name, PatKind};\n-    use crate::attr::first_attr_value_str_by_name;\n-    use crate::ptr::P;\n-    use crate::parse::token::Token;\n-    use crate::print::pprust::item_to_string;\n-    use crate::symbol::{kw, sym};\n-    use crate::tokenstream::{DelimSpan, TokenTree};\n-    use crate::util::parser_testing::string_to_stream;\n-    use crate::util::parser_testing::{string_to_expr, string_to_item};\n-    use crate::with_default_globals;\n-    use syntax_pos::{Span, BytePos, Pos, NO_EXPANSION};\n-\n-    /// Parses an item.\n-    ///\n-    /// Returns `Ok(Some(item))` when successful, `Ok(None)` when no item was found, and `Err`\n-    /// when a syntax error occurred.\n-    fn parse_item_from_source_str(name: FileName, source: String, sess: &ParseSess)\n-                                        -> PResult<'_, Option<P<ast::Item>>> {\n-        new_parser_from_source_str(sess, name, source).parse_item()\n-    }\n-\n-    // produce a syntax_pos::span\n-    fn sp(a: u32, b: u32) -> Span {\n-        Span::new(BytePos(a), BytePos(b), NO_EXPANSION)\n-    }\n-\n-    #[should_panic]\n-    #[test] fn bad_path_expr_1() {\n-        with_default_globals(|| {\n-            string_to_expr(\"::abc::def::return\".to_string());\n-        })\n-    }\n-\n-    // check the token-tree-ization of macros\n-    #[test]\n-    fn string_to_tts_macro () {\n-        with_default_globals(|| {\n-            let tts: Vec<_> =\n-                string_to_stream(\"macro_rules! zip (($a)=>($a))\".to_string()).trees().collect();\n-            let tts: &[TokenTree] = &tts[..];\n-\n-            match tts {\n-                [\n-                    TokenTree::Token(Token { kind: token::Ident(name_macro_rules, false), .. }),\n-                    TokenTree::Token(Token { kind: token::Not, .. }),\n-                    TokenTree::Token(Token { kind: token::Ident(name_zip, false), .. }),\n-                    TokenTree::Delimited(_, macro_delim,  macro_tts)\n-                ]\n-                if name_macro_rules == &sym::macro_rules && name_zip.as_str() == \"zip\" => {\n-                    let tts = &macro_tts.trees().collect::<Vec<_>>();\n-                    match &tts[..] {\n-                        [\n-                            TokenTree::Delimited(_, first_delim, first_tts),\n-                            TokenTree::Token(Token { kind: token::FatArrow, .. }),\n-                            TokenTree::Delimited(_, second_delim, second_tts),\n-                        ]\n-                        if macro_delim == &token::Paren => {\n-                            let tts = &first_tts.trees().collect::<Vec<_>>();\n-                            match &tts[..] {\n-                                [\n-                                    TokenTree::Token(Token { kind: token::Dollar, .. }),\n-                                    TokenTree::Token(Token { kind: token::Ident(name, false), .. }),\n-                                ]\n-                                if first_delim == &token::Paren && name.as_str() == \"a\" => {},\n-                                _ => panic!(\"value 3: {:?} {:?}\", first_delim, first_tts),\n-                            }\n-                            let tts = &second_tts.trees().collect::<Vec<_>>();\n-                            match &tts[..] {\n-                                [\n-                                    TokenTree::Token(Token { kind: token::Dollar, .. }),\n-                                    TokenTree::Token(Token { kind: token::Ident(name, false), .. }),\n-                                ]\n-                                if second_delim == &token::Paren && name.as_str() == \"a\" => {},\n-                                _ => panic!(\"value 4: {:?} {:?}\", second_delim, second_tts),\n-                            }\n-                        },\n-                        _ => panic!(\"value 2: {:?} {:?}\", macro_delim, macro_tts),\n-                    }\n-                },\n-                _ => panic!(\"value: {:?}\",tts),\n-            }\n-        })\n-    }\n-\n-    #[test]\n-    fn string_to_tts_1() {\n-        with_default_globals(|| {\n-            let tts = string_to_stream(\"fn a (b : i32) { b; }\".to_string());\n-\n-            let expected = TokenStream::new(vec![\n-                TokenTree::token(token::Ident(kw::Fn, false), sp(0, 2)).into(),\n-                TokenTree::token(token::Ident(Name::intern(\"a\"), false), sp(3, 4)).into(),\n-                TokenTree::Delimited(\n-                    DelimSpan::from_pair(sp(5, 6), sp(13, 14)),\n-                    token::DelimToken::Paren,\n-                    TokenStream::new(vec![\n-                        TokenTree::token(token::Ident(Name::intern(\"b\"), false), sp(6, 7)).into(),\n-                        TokenTree::token(token::Colon, sp(8, 9)).into(),\n-                        TokenTree::token(token::Ident(sym::i32, false), sp(10, 13)).into(),\n-                    ]).into(),\n-                ).into(),\n-                TokenTree::Delimited(\n-                    DelimSpan::from_pair(sp(15, 16), sp(20, 21)),\n-                    token::DelimToken::Brace,\n-                    TokenStream::new(vec![\n-                        TokenTree::token(token::Ident(Name::intern(\"b\"), false), sp(17, 18)).into(),\n-                        TokenTree::token(token::Semi, sp(18, 19)).into(),\n-                    ]).into(),\n-                ).into()\n-            ]);\n-\n-            assert_eq!(tts, expected);\n-        })\n-    }\n-\n-    #[test] fn parse_use() {\n-        with_default_globals(|| {\n-            let use_s = \"use foo::bar::baz;\";\n-            let vitem = string_to_item(use_s.to_string()).unwrap();\n-            let vitem_s = item_to_string(&vitem);\n-            assert_eq!(&vitem_s[..], use_s);\n-\n-            let use_s = \"use foo::bar as baz;\";\n-            let vitem = string_to_item(use_s.to_string()).unwrap();\n-            let vitem_s = item_to_string(&vitem);\n-            assert_eq!(&vitem_s[..], use_s);\n-        })\n-    }\n-\n-    #[test] fn parse_extern_crate() {\n-        with_default_globals(|| {\n-            let ex_s = \"extern crate foo;\";\n-            let vitem = string_to_item(ex_s.to_string()).unwrap();\n-            let vitem_s = item_to_string(&vitem);\n-            assert_eq!(&vitem_s[..], ex_s);\n-\n-            let ex_s = \"extern crate foo as bar;\";\n-            let vitem = string_to_item(ex_s.to_string()).unwrap();\n-            let vitem_s = item_to_string(&vitem);\n-            assert_eq!(&vitem_s[..], ex_s);\n-        })\n-    }\n-\n-    fn get_spans_of_pat_idents(src: &str) -> Vec<Span> {\n-        let item = string_to_item(src.to_string()).unwrap();\n-\n-        struct PatIdentVisitor {\n-            spans: Vec<Span>\n-        }\n-        impl<'a> crate::visit::Visitor<'a> for PatIdentVisitor {\n-            fn visit_pat(&mut self, p: &'a ast::Pat) {\n-                match p.node {\n-                    PatKind::Ident(_ , ref spannedident, _) => {\n-                        self.spans.push(spannedident.span.clone());\n-                    }\n-                    _ => {\n-                        crate::visit::walk_pat(self, p);\n-                    }\n-                }\n-            }\n-        }\n-        let mut v = PatIdentVisitor { spans: Vec::new() };\n-        crate::visit::walk_item(&mut v, &item);\n-        return v.spans;\n-    }\n-\n-    #[test] fn span_of_self_arg_pat_idents_are_correct() {\n-        with_default_globals(|| {\n-\n-            let srcs = [\"impl z { fn a (&self, &myarg: i32) {} }\",\n-                        \"impl z { fn a (&mut self, &myarg: i32) {} }\",\n-                        \"impl z { fn a (&'a self, &myarg: i32) {} }\",\n-                        \"impl z { fn a (self, &myarg: i32) {} }\",\n-                        \"impl z { fn a (self: Foo, &myarg: i32) {} }\",\n-                        ];\n-\n-            for &src in &srcs {\n-                let spans = get_spans_of_pat_idents(src);\n-                let (lo, hi) = (spans[0].lo(), spans[0].hi());\n-                assert!(\"self\" == &src[lo.to_usize()..hi.to_usize()],\n-                        \"\\\"{}\\\" != \\\"self\\\". src=\\\"{}\\\"\",\n-                        &src[lo.to_usize()..hi.to_usize()], src)\n-            }\n-        })\n-    }\n-\n-    #[test] fn parse_exprs () {\n-        with_default_globals(|| {\n-            // just make sure that they parse....\n-            string_to_expr(\"3 + 4\".to_string());\n-            string_to_expr(\"a::z.froob(b,&(987+3))\".to_string());\n-        })\n-    }\n-\n-    #[test] fn attrs_fix_bug () {\n-        with_default_globals(|| {\n-            string_to_item(\"pub fn mk_file_writer(path: &Path, flags: &[FileFlag])\n-                   -> Result<Box<Writer>, String> {\n-    #[cfg(windows)]\n-    fn wb() -> c_int {\n-      (O_WRONLY | libc::consts::os::extra::O_BINARY) as c_int\n-    }\n-\n-    #[cfg(unix)]\n-    fn wb() -> c_int { O_WRONLY as c_int }\n-\n-    let mut fflags: c_int = wb();\n-}\".to_string());\n-        })\n-    }\n-\n-    #[test] fn crlf_doc_comments() {\n-        with_default_globals(|| {\n-            let sess = ParseSess::new(FilePathMapping::empty());\n-\n-            let name_1 = FileName::Custom(\"crlf_source_1\".to_string());\n-            let source = \"/// doc comment\\r\\nfn foo() {}\".to_string();\n-            let item = parse_item_from_source_str(name_1, source, &sess)\n-                .unwrap().unwrap();\n-            let doc = first_attr_value_str_by_name(&item.attrs, sym::doc).unwrap();\n-            assert_eq!(doc.as_str(), \"/// doc comment\");\n-\n-            let name_2 = FileName::Custom(\"crlf_source_2\".to_string());\n-            let source = \"/// doc comment\\r\\n/// line 2\\r\\nfn foo() {}\".to_string();\n-            let item = parse_item_from_source_str(name_2, source, &sess)\n-                .unwrap().unwrap();\n-            let docs = item.attrs.iter().filter(|a| a.path == sym::doc)\n-                        .map(|a| a.value_str().unwrap().to_string()).collect::<Vec<_>>();\n-            let b: &[_] = &[\"/// doc comment\".to_string(), \"/// line 2\".to_string()];\n-            assert_eq!(&docs[..], b);\n-\n-            let name_3 = FileName::Custom(\"clrf_source_3\".to_string());\n-            let source = \"/** doc comment\\r\\n *  with CRLF */\\r\\nfn foo() {}\".to_string();\n-            let item = parse_item_from_source_str(name_3, source, &sess).unwrap().unwrap();\n-            let doc = first_attr_value_str_by_name(&item.attrs, sym::doc).unwrap();\n-            assert_eq!(doc.as_str(), \"/** doc comment\\n *  with CRLF */\");\n-        });\n-    }\n-\n-    #[test]\n-    fn ttdelim_span() {\n-        fn parse_expr_from_source_str(\n-            name: FileName, source: String, sess: &ParseSess\n-        ) -> PResult<'_, P<ast::Expr>> {\n-            new_parser_from_source_str(sess, name, source).parse_expr()\n-        }\n-\n-        with_default_globals(|| {\n-            let sess = ParseSess::new(FilePathMapping::empty());\n-            let expr = parse_expr_from_source_str(PathBuf::from(\"foo\").into(),\n-                \"foo!( fn main() { body } )\".to_string(), &sess).unwrap();\n-\n-            let tts: Vec<_> = match expr.node {\n-                ast::ExprKind::Mac(ref mac) => mac.node.stream().trees().collect(),\n-                _ => panic!(\"not a macro\"),\n-            };\n-\n-            let span = tts.iter().rev().next().unwrap().span();\n-\n-            match sess.source_map().span_to_snippet(span) {\n-                Ok(s) => assert_eq!(&s[..], \"{ body }\"),\n-                Err(_) => panic!(\"could not get snippet\"),\n-            }\n-        });\n-    }\n-\n-    // This tests that when parsing a string (rather than a file) we don't try\n-    // and read in a file for a module declaration and just parse a stub.\n-    // See `recurse_into_file_modules` in the parser.\n-    #[test]\n-    fn out_of_line_mod() {\n-        with_default_globals(|| {\n-            let sess = ParseSess::new(FilePathMapping::empty());\n-            let item = parse_item_from_source_str(\n-                PathBuf::from(\"foo\").into(),\n-                \"mod foo { struct S; mod this_does_not_exist; }\".to_owned(),\n-                &sess,\n-            ).unwrap().unwrap();\n-\n-            if let ast::ItemKind::Mod(ref m) = item.node {\n-                assert!(m.items.len() == 2);\n-            } else {\n-                panic!();\n-            }\n-        });\n-    }\n-}"}, {"sha": "e619fd17fb5bc2ed71cb4d9bf4b0baac439cec21", "filename": "src/libsyntax/parse/tests.rs", "status": "added", "additions": 339, "deletions": 0, "changes": 339, "blob_url": "https://github.com/rust-lang/rust/blob/310b9fc76002066feb89dcfbf8e88b34fe5f4ad3/src%2Flibsyntax%2Fparse%2Ftests.rs", "raw_url": "https://github.com/rust-lang/rust/raw/310b9fc76002066feb89dcfbf8e88b34fe5f4ad3/src%2Flibsyntax%2Fparse%2Ftests.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Ftests.rs?ref=310b9fc76002066feb89dcfbf8e88b34fe5f4ad3", "patch": "@@ -0,0 +1,339 @@\n+use super::*;\n+\n+use crate::ast::{self, Name, PatKind};\n+use crate::attr::first_attr_value_str_by_name;\n+use crate::parse::{ParseSess, PResult};\n+use crate::parse::new_parser_from_source_str;\n+use crate::parse::token::Token;\n+use crate::print::pprust::item_to_string;\n+use crate::ptr::P;\n+use crate::source_map::FilePathMapping;\n+use crate::symbol::{kw, sym};\n+use crate::tests::{matches_codepattern, string_to_stream, with_error_checking_parse};\n+use crate::tokenstream::{DelimSpan, TokenTree, TokenStream};\n+use crate::with_default_globals;\n+use syntax_pos::{Span, BytePos, Pos, NO_EXPANSION};\n+\n+use std::path::PathBuf;\n+\n+/// Parses an item.\n+///\n+/// Returns `Ok(Some(item))` when successful, `Ok(None)` when no item was found, and `Err`\n+/// when a syntax error occurred.\n+fn parse_item_from_source_str(name: FileName, source: String, sess: &ParseSess)\n+                                    -> PResult<'_, Option<P<ast::Item>>> {\n+    new_parser_from_source_str(sess, name, source).parse_item()\n+}\n+\n+// produce a syntax_pos::span\n+fn sp(a: u32, b: u32) -> Span {\n+    Span::new(BytePos(a), BytePos(b), NO_EXPANSION)\n+}\n+\n+/// Parse a string, return an expr\n+fn string_to_expr(source_str : String) -> P<ast::Expr> {\n+    let ps = ParseSess::new(FilePathMapping::empty());\n+    with_error_checking_parse(source_str, &ps, |p| {\n+        p.parse_expr()\n+    })\n+}\n+\n+/// Parse a string, return an item\n+fn string_to_item(source_str : String) -> Option<P<ast::Item>> {\n+    let ps = ParseSess::new(FilePathMapping::empty());\n+    with_error_checking_parse(source_str, &ps, |p| {\n+        p.parse_item()\n+    })\n+}\n+\n+#[should_panic]\n+#[test] fn bad_path_expr_1() {\n+    with_default_globals(|| {\n+        string_to_expr(\"::abc::def::return\".to_string());\n+    })\n+}\n+\n+// check the token-tree-ization of macros\n+#[test]\n+fn string_to_tts_macro () {\n+    with_default_globals(|| {\n+        let tts: Vec<_> =\n+            string_to_stream(\"macro_rules! zip (($a)=>($a))\".to_string()).trees().collect();\n+        let tts: &[TokenTree] = &tts[..];\n+\n+        match tts {\n+            [\n+                TokenTree::Token(Token { kind: token::Ident(name_macro_rules, false), .. }),\n+                TokenTree::Token(Token { kind: token::Not, .. }),\n+                TokenTree::Token(Token { kind: token::Ident(name_zip, false), .. }),\n+                TokenTree::Delimited(_, macro_delim,  macro_tts)\n+            ]\n+            if name_macro_rules == &sym::macro_rules && name_zip.as_str() == \"zip\" => {\n+                let tts = &macro_tts.trees().collect::<Vec<_>>();\n+                match &tts[..] {\n+                    [\n+                        TokenTree::Delimited(_, first_delim, first_tts),\n+                        TokenTree::Token(Token { kind: token::FatArrow, .. }),\n+                        TokenTree::Delimited(_, second_delim, second_tts),\n+                    ]\n+                    if macro_delim == &token::Paren => {\n+                        let tts = &first_tts.trees().collect::<Vec<_>>();\n+                        match &tts[..] {\n+                            [\n+                                TokenTree::Token(Token { kind: token::Dollar, .. }),\n+                                TokenTree::Token(Token { kind: token::Ident(name, false), .. }),\n+                            ]\n+                            if first_delim == &token::Paren && name.as_str() == \"a\" => {},\n+                            _ => panic!(\"value 3: {:?} {:?}\", first_delim, first_tts),\n+                        }\n+                        let tts = &second_tts.trees().collect::<Vec<_>>();\n+                        match &tts[..] {\n+                            [\n+                                TokenTree::Token(Token { kind: token::Dollar, .. }),\n+                                TokenTree::Token(Token { kind: token::Ident(name, false), .. }),\n+                            ]\n+                            if second_delim == &token::Paren && name.as_str() == \"a\" => {},\n+                            _ => panic!(\"value 4: {:?} {:?}\", second_delim, second_tts),\n+                        }\n+                    },\n+                    _ => panic!(\"value 2: {:?} {:?}\", macro_delim, macro_tts),\n+                }\n+            },\n+            _ => panic!(\"value: {:?}\",tts),\n+        }\n+    })\n+}\n+\n+#[test]\n+fn string_to_tts_1() {\n+    with_default_globals(|| {\n+        let tts = string_to_stream(\"fn a (b : i32) { b; }\".to_string());\n+\n+        let expected = TokenStream::new(vec![\n+            TokenTree::token(token::Ident(kw::Fn, false), sp(0, 2)).into(),\n+            TokenTree::token(token::Ident(Name::intern(\"a\"), false), sp(3, 4)).into(),\n+            TokenTree::Delimited(\n+                DelimSpan::from_pair(sp(5, 6), sp(13, 14)),\n+                token::DelimToken::Paren,\n+                TokenStream::new(vec![\n+                    TokenTree::token(token::Ident(Name::intern(\"b\"), false), sp(6, 7)).into(),\n+                    TokenTree::token(token::Colon, sp(8, 9)).into(),\n+                    TokenTree::token(token::Ident(sym::i32, false), sp(10, 13)).into(),\n+                ]).into(),\n+            ).into(),\n+            TokenTree::Delimited(\n+                DelimSpan::from_pair(sp(15, 16), sp(20, 21)),\n+                token::DelimToken::Brace,\n+                TokenStream::new(vec![\n+                    TokenTree::token(token::Ident(Name::intern(\"b\"), false), sp(17, 18)).into(),\n+                    TokenTree::token(token::Semi, sp(18, 19)).into(),\n+                ]).into(),\n+            ).into()\n+        ]);\n+\n+        assert_eq!(tts, expected);\n+    })\n+}\n+\n+#[test] fn parse_use() {\n+    with_default_globals(|| {\n+        let use_s = \"use foo::bar::baz;\";\n+        let vitem = string_to_item(use_s.to_string()).unwrap();\n+        let vitem_s = item_to_string(&vitem);\n+        assert_eq!(&vitem_s[..], use_s);\n+\n+        let use_s = \"use foo::bar as baz;\";\n+        let vitem = string_to_item(use_s.to_string()).unwrap();\n+        let vitem_s = item_to_string(&vitem);\n+        assert_eq!(&vitem_s[..], use_s);\n+    })\n+}\n+\n+#[test] fn parse_extern_crate() {\n+    with_default_globals(|| {\n+        let ex_s = \"extern crate foo;\";\n+        let vitem = string_to_item(ex_s.to_string()).unwrap();\n+        let vitem_s = item_to_string(&vitem);\n+        assert_eq!(&vitem_s[..], ex_s);\n+\n+        let ex_s = \"extern crate foo as bar;\";\n+        let vitem = string_to_item(ex_s.to_string()).unwrap();\n+        let vitem_s = item_to_string(&vitem);\n+        assert_eq!(&vitem_s[..], ex_s);\n+    })\n+}\n+\n+fn get_spans_of_pat_idents(src: &str) -> Vec<Span> {\n+    let item = string_to_item(src.to_string()).unwrap();\n+\n+    struct PatIdentVisitor {\n+        spans: Vec<Span>\n+    }\n+    impl<'a> crate::visit::Visitor<'a> for PatIdentVisitor {\n+        fn visit_pat(&mut self, p: &'a ast::Pat) {\n+            match p.node {\n+                PatKind::Ident(_ , ref spannedident, _) => {\n+                    self.spans.push(spannedident.span.clone());\n+                }\n+                _ => {\n+                    crate::visit::walk_pat(self, p);\n+                }\n+            }\n+        }\n+    }\n+    let mut v = PatIdentVisitor { spans: Vec::new() };\n+    crate::visit::walk_item(&mut v, &item);\n+    return v.spans;\n+}\n+\n+#[test] fn span_of_self_arg_pat_idents_are_correct() {\n+    with_default_globals(|| {\n+\n+        let srcs = [\"impl z { fn a (&self, &myarg: i32) {} }\",\n+                    \"impl z { fn a (&mut self, &myarg: i32) {} }\",\n+                    \"impl z { fn a (&'a self, &myarg: i32) {} }\",\n+                    \"impl z { fn a (self, &myarg: i32) {} }\",\n+                    \"impl z { fn a (self: Foo, &myarg: i32) {} }\",\n+                    ];\n+\n+        for &src in &srcs {\n+            let spans = get_spans_of_pat_idents(src);\n+            let (lo, hi) = (spans[0].lo(), spans[0].hi());\n+            assert!(\"self\" == &src[lo.to_usize()..hi.to_usize()],\n+                    \"\\\"{}\\\" != \\\"self\\\". src=\\\"{}\\\"\",\n+                    &src[lo.to_usize()..hi.to_usize()], src)\n+        }\n+    })\n+}\n+\n+#[test] fn parse_exprs () {\n+    with_default_globals(|| {\n+        // just make sure that they parse....\n+        string_to_expr(\"3 + 4\".to_string());\n+        string_to_expr(\"a::z.froob(b,&(987+3))\".to_string());\n+    })\n+}\n+\n+#[test] fn attrs_fix_bug () {\n+    with_default_globals(|| {\n+        string_to_item(\"pub fn mk_file_writer(path: &Path, flags: &[FileFlag])\n+                -> Result<Box<Writer>, String> {\n+#[cfg(windows)]\n+fn wb() -> c_int {\n+    (O_WRONLY | libc::consts::os::extra::O_BINARY) as c_int\n+}\n+\n+#[cfg(unix)]\n+fn wb() -> c_int { O_WRONLY as c_int }\n+\n+let mut fflags: c_int = wb();\n+}\".to_string());\n+    })\n+}\n+\n+#[test] fn crlf_doc_comments() {\n+    with_default_globals(|| {\n+        let sess = ParseSess::new(FilePathMapping::empty());\n+\n+        let name_1 = FileName::Custom(\"crlf_source_1\".to_string());\n+        let source = \"/// doc comment\\r\\nfn foo() {}\".to_string();\n+        let item = parse_item_from_source_str(name_1, source, &sess)\n+            .unwrap().unwrap();\n+        let doc = first_attr_value_str_by_name(&item.attrs, sym::doc).unwrap();\n+        assert_eq!(doc.as_str(), \"/// doc comment\");\n+\n+        let name_2 = FileName::Custom(\"crlf_source_2\".to_string());\n+        let source = \"/// doc comment\\r\\n/// line 2\\r\\nfn foo() {}\".to_string();\n+        let item = parse_item_from_source_str(name_2, source, &sess)\n+            .unwrap().unwrap();\n+        let docs = item.attrs.iter().filter(|a| a.path == sym::doc)\n+                    .map(|a| a.value_str().unwrap().to_string()).collect::<Vec<_>>();\n+        let b: &[_] = &[\"/// doc comment\".to_string(), \"/// line 2\".to_string()];\n+        assert_eq!(&docs[..], b);\n+\n+        let name_3 = FileName::Custom(\"clrf_source_3\".to_string());\n+        let source = \"/** doc comment\\r\\n *  with CRLF */\\r\\nfn foo() {}\".to_string();\n+        let item = parse_item_from_source_str(name_3, source, &sess).unwrap().unwrap();\n+        let doc = first_attr_value_str_by_name(&item.attrs, sym::doc).unwrap();\n+        assert_eq!(doc.as_str(), \"/** doc comment\\n *  with CRLF */\");\n+    });\n+}\n+\n+#[test]\n+fn ttdelim_span() {\n+    fn parse_expr_from_source_str(\n+        name: FileName, source: String, sess: &ParseSess\n+    ) -> PResult<'_, P<ast::Expr>> {\n+        new_parser_from_source_str(sess, name, source).parse_expr()\n+    }\n+\n+    with_default_globals(|| {\n+        let sess = ParseSess::new(FilePathMapping::empty());\n+        let expr = parse_expr_from_source_str(PathBuf::from(\"foo\").into(),\n+            \"foo!( fn main() { body } )\".to_string(), &sess).unwrap();\n+\n+        let tts: Vec<_> = match expr.node {\n+            ast::ExprKind::Mac(ref mac) => mac.node.stream().trees().collect(),\n+            _ => panic!(\"not a macro\"),\n+        };\n+\n+        let span = tts.iter().rev().next().unwrap().span();\n+\n+        match sess.source_map().span_to_snippet(span) {\n+            Ok(s) => assert_eq!(&s[..], \"{ body }\"),\n+            Err(_) => panic!(\"could not get snippet\"),\n+        }\n+    });\n+}\n+\n+// This tests that when parsing a string (rather than a file) we don't try\n+// and read in a file for a module declaration and just parse a stub.\n+// See `recurse_into_file_modules` in the parser.\n+#[test]\n+fn out_of_line_mod() {\n+    with_default_globals(|| {\n+        let sess = ParseSess::new(FilePathMapping::empty());\n+        let item = parse_item_from_source_str(\n+            PathBuf::from(\"foo\").into(),\n+            \"mod foo { struct S; mod this_does_not_exist; }\".to_owned(),\n+            &sess,\n+        ).unwrap().unwrap();\n+\n+        if let ast::ItemKind::Mod(ref m) = item.node {\n+            assert!(m.items.len() == 2);\n+        } else {\n+            panic!();\n+        }\n+    });\n+}\n+\n+#[test]\n+fn eqmodws() {\n+    assert_eq!(matches_codepattern(\"\",\"\"),true);\n+    assert_eq!(matches_codepattern(\"\",\"a\"),false);\n+    assert_eq!(matches_codepattern(\"a\",\"\"),false);\n+    assert_eq!(matches_codepattern(\"a\",\"a\"),true);\n+    assert_eq!(matches_codepattern(\"a b\",\"a   \\n\\t\\r  b\"),true);\n+    assert_eq!(matches_codepattern(\"a b \",\"a   \\n\\t\\r  b\"),true);\n+    assert_eq!(matches_codepattern(\"a b\",\"a   \\n\\t\\r  b \"),false);\n+    assert_eq!(matches_codepattern(\"a   b\",\"a b\"),true);\n+    assert_eq!(matches_codepattern(\"ab\",\"a b\"),false);\n+    assert_eq!(matches_codepattern(\"a   b\",\"ab\"),true);\n+    assert_eq!(matches_codepattern(\" a   b\",\"ab\"),true);\n+}\n+\n+#[test]\n+fn pattern_whitespace() {\n+    assert_eq!(matches_codepattern(\"\",\"\\x0C\"), false);\n+    assert_eq!(matches_codepattern(\"a b \",\"a   \\u{0085}\\n\\t\\r  b\"),true);\n+    assert_eq!(matches_codepattern(\"a b\",\"a   \\u{0085}\\n\\t\\r  b \"),false);\n+}\n+\n+#[test]\n+fn non_pattern_whitespace() {\n+    // These have the property 'White_Space' but not 'Pattern_White_Space'\n+    assert_eq!(matches_codepattern(\"a b\",\"a\\u{2002}b\"), false);\n+    assert_eq!(matches_codepattern(\"a   b\",\"a\\u{2002}b\"), false);\n+    assert_eq!(matches_codepattern(\"\\u{205F}a   b\",\"ab\"), false);\n+    assert_eq!(matches_codepattern(\"a  \\u{3000}b\",\"ab\"), false);\n+}"}, {"sha": "f6dd95a7f4f3257fcac56b87c9da57ca0b1f6d77", "filename": "src/libsyntax/print/pprust.rs", "status": "modified", "additions": 3, "deletions": 57, "changes": 60, "blob_url": "https://github.com/rust-lang/rust/blob/310b9fc76002066feb89dcfbf8e88b34fe5f4ad3/src%2Flibsyntax%2Fprint%2Fpprust.rs", "raw_url": "https://github.com/rust-lang/rust/raw/310b9fc76002066feb89dcfbf8e88b34fe5f4ad3/src%2Flibsyntax%2Fprint%2Fpprust.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fprint%2Fpprust.rs?ref=310b9fc76002066feb89dcfbf8e88b34fe5f4ad3", "patch": "@@ -19,6 +19,9 @@ use syntax_pos::{DUMMY_SP, FileName, Span};\n \n use std::borrow::Cow;\n \n+#[cfg(test)]\n+mod tests;\n+\n pub enum MacHeader<'a> {\n     Path(&'a ast::Path),\n     Keyword(&'static str),\n@@ -2888,60 +2891,3 @@ impl<'a> State<'a> {\n         }\n     }\n }\n-\n-#[cfg(test)]\n-mod tests {\n-    use super::*;\n-\n-    use crate::ast;\n-    use crate::source_map;\n-    use crate::with_default_globals;\n-    use syntax_pos;\n-\n-    #[test]\n-    fn test_fun_to_string() {\n-        with_default_globals(|| {\n-            let abba_ident = ast::Ident::from_str(\"abba\");\n-\n-            let decl = ast::FnDecl {\n-                inputs: Vec::new(),\n-                output: ast::FunctionRetTy::Default(syntax_pos::DUMMY_SP),\n-                c_variadic: false\n-            };\n-            let generics = ast::Generics::default();\n-            assert_eq!(\n-                fun_to_string(\n-                    &decl,\n-                    ast::FnHeader {\n-                        unsafety: ast::Unsafety::Normal,\n-                        constness: source_map::dummy_spanned(ast::Constness::NotConst),\n-                        asyncness: source_map::dummy_spanned(ast::IsAsync::NotAsync),\n-                        abi: Abi::Rust,\n-                    },\n-                    abba_ident,\n-                    &generics\n-                ),\n-                \"fn abba()\"\n-            );\n-        })\n-    }\n-\n-    #[test]\n-    fn test_variant_to_string() {\n-        with_default_globals(|| {\n-            let ident = ast::Ident::from_str(\"principal_skinner\");\n-\n-            let var = source_map::respan(syntax_pos::DUMMY_SP, ast::Variant_ {\n-                ident,\n-                attrs: Vec::new(),\n-                id: ast::DUMMY_NODE_ID,\n-                // making this up as I go.... ?\n-                data: ast::VariantData::Unit(ast::DUMMY_NODE_ID),\n-                disr_expr: None,\n-            });\n-\n-            let varstr = variant_to_string(&var);\n-            assert_eq!(varstr, \"principal_skinner\");\n-        })\n-    }\n-}"}, {"sha": "97df7e6dcbd316148484b45682e01817291de40c", "filename": "src/libsyntax/print/pprust/tests.rs", "status": "added", "additions": 53, "deletions": 0, "changes": 53, "blob_url": "https://github.com/rust-lang/rust/blob/310b9fc76002066feb89dcfbf8e88b34fe5f4ad3/src%2Flibsyntax%2Fprint%2Fpprust%2Ftests.rs", "raw_url": "https://github.com/rust-lang/rust/raw/310b9fc76002066feb89dcfbf8e88b34fe5f4ad3/src%2Flibsyntax%2Fprint%2Fpprust%2Ftests.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fprint%2Fpprust%2Ftests.rs?ref=310b9fc76002066feb89dcfbf8e88b34fe5f4ad3", "patch": "@@ -0,0 +1,53 @@\n+use super::*;\n+\n+use crate::ast;\n+use crate::source_map;\n+use crate::with_default_globals;\n+use syntax_pos;\n+\n+#[test]\n+fn test_fun_to_string() {\n+    with_default_globals(|| {\n+        let abba_ident = ast::Ident::from_str(\"abba\");\n+\n+        let decl = ast::FnDecl {\n+            inputs: Vec::new(),\n+            output: ast::FunctionRetTy::Default(syntax_pos::DUMMY_SP),\n+            c_variadic: false\n+        };\n+        let generics = ast::Generics::default();\n+        assert_eq!(\n+            fun_to_string(\n+                &decl,\n+                ast::FnHeader {\n+                    unsafety: ast::Unsafety::Normal,\n+                    constness: source_map::dummy_spanned(ast::Constness::NotConst),\n+                    asyncness: source_map::dummy_spanned(ast::IsAsync::NotAsync),\n+                    abi: Abi::Rust,\n+                },\n+                abba_ident,\n+                &generics\n+            ),\n+            \"fn abba()\"\n+        );\n+    })\n+}\n+\n+#[test]\n+fn test_variant_to_string() {\n+    with_default_globals(|| {\n+        let ident = ast::Ident::from_str(\"principal_skinner\");\n+\n+        let var = source_map::respan(syntax_pos::DUMMY_SP, ast::Variant_ {\n+            ident,\n+            attrs: Vec::new(),\n+            id: ast::DUMMY_NODE_ID,\n+            // making this up as I go.... ?\n+            data: ast::VariantData::Unit(ast::DUMMY_NODE_ID),\n+            disr_expr: None,\n+        });\n+\n+        let varstr = variant_to_string(&var);\n+        assert_eq!(varstr, \"principal_skinner\");\n+    })\n+}"}, {"sha": "f83c1dbf7eed0842bb8ffd3b802490a8c0ae4905", "filename": "src/libsyntax/source_map.rs", "status": "modified", "additions": 3, "deletions": 220, "changes": 223, "blob_url": "https://github.com/rust-lang/rust/blob/310b9fc76002066feb89dcfbf8e88b34fe5f4ad3/src%2Flibsyntax%2Fsource_map.rs", "raw_url": "https://github.com/rust-lang/rust/raw/310b9fc76002066feb89dcfbf8e88b34fe5f4ad3/src%2Flibsyntax%2Fsource_map.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fsource_map.rs?ref=310b9fc76002066feb89dcfbf8e88b34fe5f4ad3", "patch": "@@ -24,6 +24,9 @@ use log::debug;\n \n use errors::SourceMapper;\n \n+#[cfg(test)]\n+mod tests;\n+\n /// Returns the span itself if it doesn't come from a macro expansion,\n /// otherwise return the call site span up to the `enclosing_sp` by\n /// following the `expn_info` chain.\n@@ -1020,223 +1023,3 @@ impl FilePathMapping {\n         (path, false)\n     }\n }\n-\n-// _____________________________________________________________________________\n-// Tests\n-//\n-\n-#[cfg(test)]\n-mod tests {\n-    use super::*;\n-    use rustc_data_structures::sync::Lrc;\n-\n-    fn init_source_map() -> SourceMap {\n-        let sm = SourceMap::new(FilePathMapping::empty());\n-        sm.new_source_file(PathBuf::from(\"blork.rs\").into(),\n-                       \"first line.\\nsecond line\".to_string());\n-        sm.new_source_file(PathBuf::from(\"empty.rs\").into(),\n-                       String::new());\n-        sm.new_source_file(PathBuf::from(\"blork2.rs\").into(),\n-                       \"first line.\\nsecond line\".to_string());\n-        sm\n-    }\n-\n-    #[test]\n-    fn t3() {\n-        // Test lookup_byte_offset\n-        let sm = init_source_map();\n-\n-        let srcfbp1 = sm.lookup_byte_offset(BytePos(23));\n-        assert_eq!(srcfbp1.sf.name, PathBuf::from(\"blork.rs\").into());\n-        assert_eq!(srcfbp1.pos, BytePos(23));\n-\n-        let srcfbp1 = sm.lookup_byte_offset(BytePos(24));\n-        assert_eq!(srcfbp1.sf.name, PathBuf::from(\"empty.rs\").into());\n-        assert_eq!(srcfbp1.pos, BytePos(0));\n-\n-        let srcfbp2 = sm.lookup_byte_offset(BytePos(25));\n-        assert_eq!(srcfbp2.sf.name, PathBuf::from(\"blork2.rs\").into());\n-        assert_eq!(srcfbp2.pos, BytePos(0));\n-    }\n-\n-    #[test]\n-    fn t4() {\n-        // Test bytepos_to_file_charpos\n-        let sm = init_source_map();\n-\n-        let cp1 = sm.bytepos_to_file_charpos(BytePos(22));\n-        assert_eq!(cp1, CharPos(22));\n-\n-        let cp2 = sm.bytepos_to_file_charpos(BytePos(25));\n-        assert_eq!(cp2, CharPos(0));\n-    }\n-\n-    #[test]\n-    fn t5() {\n-        // Test zero-length source_files.\n-        let sm = init_source_map();\n-\n-        let loc1 = sm.lookup_char_pos(BytePos(22));\n-        assert_eq!(loc1.file.name, PathBuf::from(\"blork.rs\").into());\n-        assert_eq!(loc1.line, 2);\n-        assert_eq!(loc1.col, CharPos(10));\n-\n-        let loc2 = sm.lookup_char_pos(BytePos(25));\n-        assert_eq!(loc2.file.name, PathBuf::from(\"blork2.rs\").into());\n-        assert_eq!(loc2.line, 1);\n-        assert_eq!(loc2.col, CharPos(0));\n-    }\n-\n-    fn init_source_map_mbc() -> SourceMap {\n-        let sm = SourceMap::new(FilePathMapping::empty());\n-        // \u20ac is a three byte utf8 char.\n-        sm.new_source_file(PathBuf::from(\"blork.rs\").into(),\n-                       \"fir\u20acst \u20ac\u20ac\u20ac\u20ac line.\\nsecond line\".to_string());\n-        sm.new_source_file(PathBuf::from(\"blork2.rs\").into(),\n-                       \"first line\u20ac\u20ac.\\n\u20ac second line\".to_string());\n-        sm\n-    }\n-\n-    #[test]\n-    fn t6() {\n-        // Test bytepos_to_file_charpos in the presence of multi-byte chars\n-        let sm = init_source_map_mbc();\n-\n-        let cp1 = sm.bytepos_to_file_charpos(BytePos(3));\n-        assert_eq!(cp1, CharPos(3));\n-\n-        let cp2 = sm.bytepos_to_file_charpos(BytePos(6));\n-        assert_eq!(cp2, CharPos(4));\n-\n-        let cp3 = sm.bytepos_to_file_charpos(BytePos(56));\n-        assert_eq!(cp3, CharPos(12));\n-\n-        let cp4 = sm.bytepos_to_file_charpos(BytePos(61));\n-        assert_eq!(cp4, CharPos(15));\n-    }\n-\n-    #[test]\n-    fn t7() {\n-        // Test span_to_lines for a span ending at the end of source_file\n-        let sm = init_source_map();\n-        let span = Span::new(BytePos(12), BytePos(23), NO_EXPANSION);\n-        let file_lines = sm.span_to_lines(span).unwrap();\n-\n-        assert_eq!(file_lines.file.name, PathBuf::from(\"blork.rs\").into());\n-        assert_eq!(file_lines.lines.len(), 1);\n-        assert_eq!(file_lines.lines[0].line_index, 1);\n-    }\n-\n-    /// Given a string like \" ~~~~~~~~~~~~ \", produces a span\n-    /// converting that range. The idea is that the string has the same\n-    /// length as the input, and we uncover the byte positions. Note\n-    /// that this can span lines and so on.\n-    fn span_from_selection(input: &str, selection: &str) -> Span {\n-        assert_eq!(input.len(), selection.len());\n-        let left_index = selection.find('~').unwrap() as u32;\n-        let right_index = selection.rfind('~').map(|x|x as u32).unwrap_or(left_index);\n-        Span::new(BytePos(left_index), BytePos(right_index + 1), NO_EXPANSION)\n-    }\n-\n-    /// Tests span_to_snippet and span_to_lines for a span converting 3\n-    /// lines in the middle of a file.\n-    #[test]\n-    fn span_to_snippet_and_lines_spanning_multiple_lines() {\n-        let sm = SourceMap::new(FilePathMapping::empty());\n-        let inputtext = \"aaaaa\\nbbbbBB\\nCCC\\nDDDDDddddd\\neee\\n\";\n-        let selection = \"     \\n    ~~\\n~~~\\n~~~~~     \\n   \\n\";\n-        sm.new_source_file(Path::new(\"blork.rs\").to_owned().into(), inputtext.to_string());\n-        let span = span_from_selection(inputtext, selection);\n-\n-        // check that we are extracting the text we thought we were extracting\n-        assert_eq!(&sm.span_to_snippet(span).unwrap(), \"BB\\nCCC\\nDDDDD\");\n-\n-        // check that span_to_lines gives us the complete result with the lines/cols we expected\n-        let lines = sm.span_to_lines(span).unwrap();\n-        let expected = vec![\n-            LineInfo { line_index: 1, start_col: CharPos(4), end_col: CharPos(6) },\n-            LineInfo { line_index: 2, start_col: CharPos(0), end_col: CharPos(3) },\n-            LineInfo { line_index: 3, start_col: CharPos(0), end_col: CharPos(5) }\n-            ];\n-        assert_eq!(lines.lines, expected);\n-    }\n-\n-    #[test]\n-    fn t8() {\n-        // Test span_to_snippet for a span ending at the end of source_file\n-        let sm = init_source_map();\n-        let span = Span::new(BytePos(12), BytePos(23), NO_EXPANSION);\n-        let snippet = sm.span_to_snippet(span);\n-\n-        assert_eq!(snippet, Ok(\"second line\".to_string()));\n-    }\n-\n-    #[test]\n-    fn t9() {\n-        // Test span_to_str for a span ending at the end of source_file\n-        let sm = init_source_map();\n-        let span = Span::new(BytePos(12), BytePos(23), NO_EXPANSION);\n-        let sstr =  sm.span_to_string(span);\n-\n-        assert_eq!(sstr, \"blork.rs:2:1: 2:12\");\n-    }\n-\n-    /// Tests failing to merge two spans on different lines\n-    #[test]\n-    fn span_merging_fail() {\n-        let sm = SourceMap::new(FilePathMapping::empty());\n-        let inputtext  = \"bbbb BB\\ncc CCC\\n\";\n-        let selection1 = \"     ~~\\n      \\n\";\n-        let selection2 = \"       \\n   ~~~\\n\";\n-        sm.new_source_file(Path::new(\"blork.rs\").to_owned().into(), inputtext.to_owned());\n-        let span1 = span_from_selection(inputtext, selection1);\n-        let span2 = span_from_selection(inputtext, selection2);\n-\n-        assert!(sm.merge_spans(span1, span2).is_none());\n-    }\n-\n-    /// Returns the span corresponding to the `n`th occurrence of\n-    /// `substring` in `source_text`.\n-    trait SourceMapExtension {\n-        fn span_substr(&self,\n-                    file: &Lrc<SourceFile>,\n-                    source_text: &str,\n-                    substring: &str,\n-                    n: usize)\n-                    -> Span;\n-    }\n-\n-    impl SourceMapExtension for SourceMap {\n-        fn span_substr(&self,\n-                    file: &Lrc<SourceFile>,\n-                    source_text: &str,\n-                    substring: &str,\n-                    n: usize)\n-                    -> Span\n-        {\n-            println!(\"span_substr(file={:?}/{:?}, substring={:?}, n={})\",\n-                    file.name, file.start_pos, substring, n);\n-            let mut i = 0;\n-            let mut hi = 0;\n-            loop {\n-                let offset = source_text[hi..].find(substring).unwrap_or_else(|| {\n-                    panic!(\"source_text `{}` does not have {} occurrences of `{}`, only {}\",\n-                        source_text, n, substring, i);\n-                });\n-                let lo = hi + offset;\n-                hi = lo + substring.len();\n-                if i == n {\n-                    let span = Span::new(\n-                        BytePos(lo as u32 + file.start_pos.0),\n-                        BytePos(hi as u32 + file.start_pos.0),\n-                        NO_EXPANSION,\n-                    );\n-                    assert_eq!(&self.span_to_snippet(span).unwrap()[..],\n-                            substring);\n-                    return span;\n-                }\n-                i += 1;\n-            }\n-        }\n-    }\n-}"}, {"sha": "427e86b56e12b39827e2916cdfd21aa253d0b1d5", "filename": "src/libsyntax/source_map/tests.rs", "status": "added", "additions": 213, "deletions": 0, "changes": 213, "blob_url": "https://github.com/rust-lang/rust/blob/310b9fc76002066feb89dcfbf8e88b34fe5f4ad3/src%2Flibsyntax%2Fsource_map%2Ftests.rs", "raw_url": "https://github.com/rust-lang/rust/raw/310b9fc76002066feb89dcfbf8e88b34fe5f4ad3/src%2Flibsyntax%2Fsource_map%2Ftests.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fsource_map%2Ftests.rs?ref=310b9fc76002066feb89dcfbf8e88b34fe5f4ad3", "patch": "@@ -0,0 +1,213 @@\n+use super::*;\n+\n+use rustc_data_structures::sync::Lrc;\n+\n+fn init_source_map() -> SourceMap {\n+    let sm = SourceMap::new(FilePathMapping::empty());\n+    sm.new_source_file(PathBuf::from(\"blork.rs\").into(),\n+                    \"first line.\\nsecond line\".to_string());\n+    sm.new_source_file(PathBuf::from(\"empty.rs\").into(),\n+                    String::new());\n+    sm.new_source_file(PathBuf::from(\"blork2.rs\").into(),\n+                    \"first line.\\nsecond line\".to_string());\n+    sm\n+}\n+\n+#[test]\n+fn t3() {\n+    // Test lookup_byte_offset\n+    let sm = init_source_map();\n+\n+    let srcfbp1 = sm.lookup_byte_offset(BytePos(23));\n+    assert_eq!(srcfbp1.sf.name, PathBuf::from(\"blork.rs\").into());\n+    assert_eq!(srcfbp1.pos, BytePos(23));\n+\n+    let srcfbp1 = sm.lookup_byte_offset(BytePos(24));\n+    assert_eq!(srcfbp1.sf.name, PathBuf::from(\"empty.rs\").into());\n+    assert_eq!(srcfbp1.pos, BytePos(0));\n+\n+    let srcfbp2 = sm.lookup_byte_offset(BytePos(25));\n+    assert_eq!(srcfbp2.sf.name, PathBuf::from(\"blork2.rs\").into());\n+    assert_eq!(srcfbp2.pos, BytePos(0));\n+}\n+\n+#[test]\n+fn t4() {\n+    // Test bytepos_to_file_charpos\n+    let sm = init_source_map();\n+\n+    let cp1 = sm.bytepos_to_file_charpos(BytePos(22));\n+    assert_eq!(cp1, CharPos(22));\n+\n+    let cp2 = sm.bytepos_to_file_charpos(BytePos(25));\n+    assert_eq!(cp2, CharPos(0));\n+}\n+\n+#[test]\n+fn t5() {\n+    // Test zero-length source_files.\n+    let sm = init_source_map();\n+\n+    let loc1 = sm.lookup_char_pos(BytePos(22));\n+    assert_eq!(loc1.file.name, PathBuf::from(\"blork.rs\").into());\n+    assert_eq!(loc1.line, 2);\n+    assert_eq!(loc1.col, CharPos(10));\n+\n+    let loc2 = sm.lookup_char_pos(BytePos(25));\n+    assert_eq!(loc2.file.name, PathBuf::from(\"blork2.rs\").into());\n+    assert_eq!(loc2.line, 1);\n+    assert_eq!(loc2.col, CharPos(0));\n+}\n+\n+fn init_source_map_mbc() -> SourceMap {\n+    let sm = SourceMap::new(FilePathMapping::empty());\n+    // \u20ac is a three byte utf8 char.\n+    sm.new_source_file(PathBuf::from(\"blork.rs\").into(),\n+                    \"fir\u20acst \u20ac\u20ac\u20ac\u20ac line.\\nsecond line\".to_string());\n+    sm.new_source_file(PathBuf::from(\"blork2.rs\").into(),\n+                    \"first line\u20ac\u20ac.\\n\u20ac second line\".to_string());\n+    sm\n+}\n+\n+#[test]\n+fn t6() {\n+    // Test bytepos_to_file_charpos in the presence of multi-byte chars\n+    let sm = init_source_map_mbc();\n+\n+    let cp1 = sm.bytepos_to_file_charpos(BytePos(3));\n+    assert_eq!(cp1, CharPos(3));\n+\n+    let cp2 = sm.bytepos_to_file_charpos(BytePos(6));\n+    assert_eq!(cp2, CharPos(4));\n+\n+    let cp3 = sm.bytepos_to_file_charpos(BytePos(56));\n+    assert_eq!(cp3, CharPos(12));\n+\n+    let cp4 = sm.bytepos_to_file_charpos(BytePos(61));\n+    assert_eq!(cp4, CharPos(15));\n+}\n+\n+#[test]\n+fn t7() {\n+    // Test span_to_lines for a span ending at the end of source_file\n+    let sm = init_source_map();\n+    let span = Span::new(BytePos(12), BytePos(23), NO_EXPANSION);\n+    let file_lines = sm.span_to_lines(span).unwrap();\n+\n+    assert_eq!(file_lines.file.name, PathBuf::from(\"blork.rs\").into());\n+    assert_eq!(file_lines.lines.len(), 1);\n+    assert_eq!(file_lines.lines[0].line_index, 1);\n+}\n+\n+/// Given a string like \" ~~~~~~~~~~~~ \", produces a span\n+/// converting that range. The idea is that the string has the same\n+/// length as the input, and we uncover the byte positions. Note\n+/// that this can span lines and so on.\n+fn span_from_selection(input: &str, selection: &str) -> Span {\n+    assert_eq!(input.len(), selection.len());\n+    let left_index = selection.find('~').unwrap() as u32;\n+    let right_index = selection.rfind('~').map(|x|x as u32).unwrap_or(left_index);\n+    Span::new(BytePos(left_index), BytePos(right_index + 1), NO_EXPANSION)\n+}\n+\n+/// Tests span_to_snippet and span_to_lines for a span converting 3\n+/// lines in the middle of a file.\n+#[test]\n+fn span_to_snippet_and_lines_spanning_multiple_lines() {\n+    let sm = SourceMap::new(FilePathMapping::empty());\n+    let inputtext = \"aaaaa\\nbbbbBB\\nCCC\\nDDDDDddddd\\neee\\n\";\n+    let selection = \"     \\n    ~~\\n~~~\\n~~~~~     \\n   \\n\";\n+    sm.new_source_file(Path::new(\"blork.rs\").to_owned().into(), inputtext.to_string());\n+    let span = span_from_selection(inputtext, selection);\n+\n+    // check that we are extracting the text we thought we were extracting\n+    assert_eq!(&sm.span_to_snippet(span).unwrap(), \"BB\\nCCC\\nDDDDD\");\n+\n+    // check that span_to_lines gives us the complete result with the lines/cols we expected\n+    let lines = sm.span_to_lines(span).unwrap();\n+    let expected = vec![\n+        LineInfo { line_index: 1, start_col: CharPos(4), end_col: CharPos(6) },\n+        LineInfo { line_index: 2, start_col: CharPos(0), end_col: CharPos(3) },\n+        LineInfo { line_index: 3, start_col: CharPos(0), end_col: CharPos(5) }\n+        ];\n+    assert_eq!(lines.lines, expected);\n+}\n+\n+#[test]\n+fn t8() {\n+    // Test span_to_snippet for a span ending at the end of source_file\n+    let sm = init_source_map();\n+    let span = Span::new(BytePos(12), BytePos(23), NO_EXPANSION);\n+    let snippet = sm.span_to_snippet(span);\n+\n+    assert_eq!(snippet, Ok(\"second line\".to_string()));\n+}\n+\n+#[test]\n+fn t9() {\n+    // Test span_to_str for a span ending at the end of source_file\n+    let sm = init_source_map();\n+    let span = Span::new(BytePos(12), BytePos(23), NO_EXPANSION);\n+    let sstr =  sm.span_to_string(span);\n+\n+    assert_eq!(sstr, \"blork.rs:2:1: 2:12\");\n+}\n+\n+/// Tests failing to merge two spans on different lines\n+#[test]\n+fn span_merging_fail() {\n+    let sm = SourceMap::new(FilePathMapping::empty());\n+    let inputtext  = \"bbbb BB\\ncc CCC\\n\";\n+    let selection1 = \"     ~~\\n      \\n\";\n+    let selection2 = \"       \\n   ~~~\\n\";\n+    sm.new_source_file(Path::new(\"blork.rs\").to_owned().into(), inputtext.to_owned());\n+    let span1 = span_from_selection(inputtext, selection1);\n+    let span2 = span_from_selection(inputtext, selection2);\n+\n+    assert!(sm.merge_spans(span1, span2).is_none());\n+}\n+\n+/// Returns the span corresponding to the `n`th occurrence of\n+/// `substring` in `source_text`.\n+trait SourceMapExtension {\n+    fn span_substr(&self,\n+                file: &Lrc<SourceFile>,\n+                source_text: &str,\n+                substring: &str,\n+                n: usize)\n+                -> Span;\n+}\n+\n+impl SourceMapExtension for SourceMap {\n+    fn span_substr(&self,\n+                file: &Lrc<SourceFile>,\n+                source_text: &str,\n+                substring: &str,\n+                n: usize)\n+                -> Span\n+    {\n+        println!(\"span_substr(file={:?}/{:?}, substring={:?}, n={})\",\n+                file.name, file.start_pos, substring, n);\n+        let mut i = 0;\n+        let mut hi = 0;\n+        loop {\n+            let offset = source_text[hi..].find(substring).unwrap_or_else(|| {\n+                panic!(\"source_text `{}` does not have {} occurrences of `{}`, only {}\",\n+                    source_text, n, substring, i);\n+            });\n+            let lo = hi + offset;\n+            hi = lo + substring.len();\n+            if i == n {\n+                let span = Span::new(\n+                    BytePos(lo as u32 + file.start_pos.0),\n+                    BytePos(hi as u32 + file.start_pos.0),\n+                    NO_EXPANSION,\n+                );\n+                assert_eq!(&self.span_to_snippet(span).unwrap()[..],\n+                        substring);\n+                return span;\n+            }\n+            i += 1;\n+        }\n+    }\n+}"}, {"sha": "cff034fdeb1e3892847d2b19982db4d5cabf2bf7", "filename": "src/libsyntax/tests.rs", "status": "renamed", "additions": 94, "deletions": 4, "changes": 98, "blob_url": "https://github.com/rust-lang/rust/blob/310b9fc76002066feb89dcfbf8e88b34fe5f4ad3/src%2Flibsyntax%2Ftests.rs", "raw_url": "https://github.com/rust-lang/rust/raw/310b9fc76002066feb89dcfbf8e88b34fe5f4ad3/src%2Flibsyntax%2Ftests.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Ftests.rs?ref=310b9fc76002066feb89dcfbf8e88b34fe5f4ad3", "patch": "@@ -1,16 +1,106 @@\n+use crate::{ast, panictry};\n+use crate::parse::{ParseSess, PResult, source_file_to_stream};\n+use crate::parse::new_parser_from_source_str;\n+use crate::parse::parser::Parser;\n use crate::source_map::{SourceMap, FilePathMapping};\n+use crate::tokenstream::TokenStream;\n use crate::with_default_globals;\n \n-use errors::Handler;\n use errors::emitter::EmitterWriter;\n+use errors::Handler;\n+use rustc_data_structures::sync::Lrc;\n+use syntax_pos::{BytePos, NO_EXPANSION, Span, MultiSpan};\n \n use std::io;\n use std::io::prelude::*;\n-use rustc_data_structures::sync::Lrc;\n+use std::iter::Peekable;\n+use std::path::{Path, PathBuf};\n use std::str;\n use std::sync::{Arc, Mutex};\n-use std::path::Path;\n-use syntax_pos::{BytePos, NO_EXPANSION, Span, MultiSpan};\n+\n+/// Map string to parser (via tts)\n+fn string_to_parser(ps: &ParseSess, source_str: String) -> Parser<'_> {\n+    new_parser_from_source_str(ps, PathBuf::from(\"bogofile\").into(), source_str)\n+}\n+\n+crate fn with_error_checking_parse<'a, T, F>(s: String, ps: &'a ParseSess, f: F) -> T where\n+    F: FnOnce(&mut Parser<'a>) -> PResult<'a, T>,\n+{\n+    let mut p = string_to_parser(&ps, s);\n+    let x = panictry!(f(&mut p));\n+    p.sess.span_diagnostic.abort_if_errors();\n+    x\n+}\n+\n+/// Map a string to tts, using a made-up filename:\n+crate fn string_to_stream(source_str: String) -> TokenStream {\n+    let ps = ParseSess::new(FilePathMapping::empty());\n+    source_file_to_stream(\n+        &ps,\n+        ps.source_map().new_source_file(PathBuf::from(\"bogofile\").into(),\n+        source_str,\n+    ), None).0\n+}\n+\n+/// Parse a string, return a crate.\n+crate fn string_to_crate(source_str : String) -> ast::Crate {\n+    let ps = ParseSess::new(FilePathMapping::empty());\n+    with_error_checking_parse(source_str, &ps, |p| {\n+        p.parse_crate_mod()\n+    })\n+}\n+\n+/// Does the given string match the pattern? whitespace in the first string\n+/// may be deleted or replaced with other whitespace to match the pattern.\n+/// This function is relatively Unicode-ignorant; fortunately, the careful design\n+/// of UTF-8 mitigates this ignorance. It doesn't do NKF-normalization(?).\n+crate fn matches_codepattern(a : &str, b : &str) -> bool {\n+    let mut a_iter = a.chars().peekable();\n+    let mut b_iter = b.chars().peekable();\n+\n+    loop {\n+        let (a, b) = match (a_iter.peek(), b_iter.peek()) {\n+            (None, None) => return true,\n+            (None, _) => return false,\n+            (Some(&a), None) => {\n+                if is_pattern_whitespace(a) {\n+                    break // trailing whitespace check is out of loop for borrowck\n+                } else {\n+                    return false\n+                }\n+            }\n+            (Some(&a), Some(&b)) => (a, b)\n+        };\n+\n+        if is_pattern_whitespace(a) && is_pattern_whitespace(b) {\n+            // skip whitespace for a and b\n+            scan_for_non_ws_or_end(&mut a_iter);\n+            scan_for_non_ws_or_end(&mut b_iter);\n+        } else if is_pattern_whitespace(a) {\n+            // skip whitespace for a\n+            scan_for_non_ws_or_end(&mut a_iter);\n+        } else if a == b {\n+            a_iter.next();\n+            b_iter.next();\n+        } else {\n+            return false\n+        }\n+    }\n+\n+    // check if a has *only* trailing whitespace\n+    a_iter.all(is_pattern_whitespace)\n+}\n+\n+/// Advances the given peekable `Iterator` until it reaches a non-whitespace character\n+fn scan_for_non_ws_or_end<I: Iterator<Item = char>>(iter: &mut Peekable<I>) {\n+    while iter.peek().copied().map(|c| is_pattern_whitespace(c)) == Some(true) {\n+        iter.next();\n+    }\n+}\n+\n+fn is_pattern_whitespace(c: char) -> bool {\n+    rustc_lexer::character_properties::is_whitespace(c)\n+}\n \n /// Identify a position in the text by the Nth occurrence of a string.\n struct Position {", "previous_filename": "src/libsyntax/test_snippet.rs"}, {"sha": "6ff8898fe21629e62e8f85052f9e80b5946b3a8d", "filename": "src/libsyntax/tokenstream.rs", "status": "modified", "additions": 3, "deletions": 111, "changes": 114, "blob_url": "https://github.com/rust-lang/rust/blob/310b9fc76002066feb89dcfbf8e88b34fe5f4ad3/src%2Flibsyntax%2Ftokenstream.rs", "raw_url": "https://github.com/rust-lang/rust/raw/310b9fc76002066feb89dcfbf8e88b34fe5f4ad3/src%2Flibsyntax%2Ftokenstream.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Ftokenstream.rs?ref=310b9fc76002066feb89dcfbf8e88b34fe5f4ad3", "patch": "@@ -29,6 +29,9 @@ use smallvec::{SmallVec, smallvec};\n use std::borrow::Cow;\n use std::{fmt, iter, mem};\n \n+#[cfg(test)]\n+mod tests;\n+\n /// When the main rust parser encounters a syntax-extension invocation, it\n /// parses the arguments to the invocation as a token-tree. This is a very\n /// loose structure, such that all sorts of different AST-fragments can\n@@ -552,114 +555,3 @@ impl DelimSpan {\n         }\n     }\n }\n-\n-#[cfg(test)]\n-mod tests {\n-    use super::*;\n-    use crate::ast::Name;\n-    use crate::with_default_globals;\n-    use crate::util::parser_testing::string_to_stream;\n-    use syntax_pos::{Span, BytePos, NO_EXPANSION};\n-\n-    fn string_to_ts(string: &str) -> TokenStream {\n-        string_to_stream(string.to_owned())\n-    }\n-\n-    fn sp(a: u32, b: u32) -> Span {\n-        Span::new(BytePos(a), BytePos(b), NO_EXPANSION)\n-    }\n-\n-    #[test]\n-    fn test_concat() {\n-        with_default_globals(|| {\n-            let test_res = string_to_ts(\"foo::bar::baz\");\n-            let test_fst = string_to_ts(\"foo::bar\");\n-            let test_snd = string_to_ts(\"::baz\");\n-            let eq_res = TokenStream::from_streams(smallvec![test_fst, test_snd]);\n-            assert_eq!(test_res.trees().count(), 5);\n-            assert_eq!(eq_res.trees().count(), 5);\n-            assert_eq!(test_res.eq_unspanned(&eq_res), true);\n-        })\n-    }\n-\n-    #[test]\n-    fn test_to_from_bijection() {\n-        with_default_globals(|| {\n-            let test_start = string_to_ts(\"foo::bar(baz)\");\n-            let test_end = test_start.trees().collect();\n-            assert_eq!(test_start, test_end)\n-        })\n-    }\n-\n-    #[test]\n-    fn test_eq_0() {\n-        with_default_globals(|| {\n-            let test_res = string_to_ts(\"foo\");\n-            let test_eqs = string_to_ts(\"foo\");\n-            assert_eq!(test_res, test_eqs)\n-        })\n-    }\n-\n-    #[test]\n-    fn test_eq_1() {\n-        with_default_globals(|| {\n-            let test_res = string_to_ts(\"::bar::baz\");\n-            let test_eqs = string_to_ts(\"::bar::baz\");\n-            assert_eq!(test_res, test_eqs)\n-        })\n-    }\n-\n-    #[test]\n-    fn test_eq_3() {\n-        with_default_globals(|| {\n-            let test_res = string_to_ts(\"\");\n-            let test_eqs = string_to_ts(\"\");\n-            assert_eq!(test_res, test_eqs)\n-        })\n-    }\n-\n-    #[test]\n-    fn test_diseq_0() {\n-        with_default_globals(|| {\n-            let test_res = string_to_ts(\"::bar::baz\");\n-            let test_eqs = string_to_ts(\"bar::baz\");\n-            assert_eq!(test_res == test_eqs, false)\n-        })\n-    }\n-\n-    #[test]\n-    fn test_diseq_1() {\n-        with_default_globals(|| {\n-            let test_res = string_to_ts(\"(bar,baz)\");\n-            let test_eqs = string_to_ts(\"bar,baz\");\n-            assert_eq!(test_res == test_eqs, false)\n-        })\n-    }\n-\n-    #[test]\n-    fn test_is_empty() {\n-        with_default_globals(|| {\n-            let test0: TokenStream = Vec::<TokenTree>::new().into_iter().collect();\n-            let test1: TokenStream =\n-                TokenTree::token(token::Ident(Name::intern(\"a\"), false), sp(0, 1)).into();\n-            let test2 = string_to_ts(\"foo(bar::baz)\");\n-\n-            assert_eq!(test0.is_empty(), true);\n-            assert_eq!(test1.is_empty(), false);\n-            assert_eq!(test2.is_empty(), false);\n-        })\n-    }\n-\n-    #[test]\n-    fn test_dotdotdot() {\n-        with_default_globals(|| {\n-            let mut builder = TokenStreamBuilder::new();\n-            builder.push(TokenTree::token(token::Dot, sp(0, 1)).joint());\n-            builder.push(TokenTree::token(token::Dot, sp(1, 2)).joint());\n-            builder.push(TokenTree::token(token::Dot, sp(2, 3)));\n-            let stream = builder.build();\n-            assert!(stream.eq_unspanned(&string_to_ts(\"...\")));\n-            assert_eq!(stream.trees().count(), 1);\n-        })\n-    }\n-}"}, {"sha": "72e22a49876e8f85924567b6687cc0ff0e2ba093", "filename": "src/libsyntax/tokenstream/tests.rs", "status": "added", "additions": 108, "deletions": 0, "changes": 108, "blob_url": "https://github.com/rust-lang/rust/blob/310b9fc76002066feb89dcfbf8e88b34fe5f4ad3/src%2Flibsyntax%2Ftokenstream%2Ftests.rs", "raw_url": "https://github.com/rust-lang/rust/raw/310b9fc76002066feb89dcfbf8e88b34fe5f4ad3/src%2Flibsyntax%2Ftokenstream%2Ftests.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Ftokenstream%2Ftests.rs?ref=310b9fc76002066feb89dcfbf8e88b34fe5f4ad3", "patch": "@@ -0,0 +1,108 @@\n+use super::*;\n+\n+use crate::ast::Name;\n+use crate::with_default_globals;\n+use crate::tests::string_to_stream;\n+use syntax_pos::{Span, BytePos, NO_EXPANSION};\n+\n+fn string_to_ts(string: &str) -> TokenStream {\n+    string_to_stream(string.to_owned())\n+}\n+\n+fn sp(a: u32, b: u32) -> Span {\n+    Span::new(BytePos(a), BytePos(b), NO_EXPANSION)\n+}\n+\n+#[test]\n+fn test_concat() {\n+    with_default_globals(|| {\n+        let test_res = string_to_ts(\"foo::bar::baz\");\n+        let test_fst = string_to_ts(\"foo::bar\");\n+        let test_snd = string_to_ts(\"::baz\");\n+        let eq_res = TokenStream::from_streams(smallvec![test_fst, test_snd]);\n+        assert_eq!(test_res.trees().count(), 5);\n+        assert_eq!(eq_res.trees().count(), 5);\n+        assert_eq!(test_res.eq_unspanned(&eq_res), true);\n+    })\n+}\n+\n+#[test]\n+fn test_to_from_bijection() {\n+    with_default_globals(|| {\n+        let test_start = string_to_ts(\"foo::bar(baz)\");\n+        let test_end = test_start.trees().collect();\n+        assert_eq!(test_start, test_end)\n+    })\n+}\n+\n+#[test]\n+fn test_eq_0() {\n+    with_default_globals(|| {\n+        let test_res = string_to_ts(\"foo\");\n+        let test_eqs = string_to_ts(\"foo\");\n+        assert_eq!(test_res, test_eqs)\n+    })\n+}\n+\n+#[test]\n+fn test_eq_1() {\n+    with_default_globals(|| {\n+        let test_res = string_to_ts(\"::bar::baz\");\n+        let test_eqs = string_to_ts(\"::bar::baz\");\n+        assert_eq!(test_res, test_eqs)\n+    })\n+}\n+\n+#[test]\n+fn test_eq_3() {\n+    with_default_globals(|| {\n+        let test_res = string_to_ts(\"\");\n+        let test_eqs = string_to_ts(\"\");\n+        assert_eq!(test_res, test_eqs)\n+    })\n+}\n+\n+#[test]\n+fn test_diseq_0() {\n+    with_default_globals(|| {\n+        let test_res = string_to_ts(\"::bar::baz\");\n+        let test_eqs = string_to_ts(\"bar::baz\");\n+        assert_eq!(test_res == test_eqs, false)\n+    })\n+}\n+\n+#[test]\n+fn test_diseq_1() {\n+    with_default_globals(|| {\n+        let test_res = string_to_ts(\"(bar,baz)\");\n+        let test_eqs = string_to_ts(\"bar,baz\");\n+        assert_eq!(test_res == test_eqs, false)\n+    })\n+}\n+\n+#[test]\n+fn test_is_empty() {\n+    with_default_globals(|| {\n+        let test0: TokenStream = Vec::<TokenTree>::new().into_iter().collect();\n+        let test1: TokenStream =\n+            TokenTree::token(token::Ident(Name::intern(\"a\"), false), sp(0, 1)).into();\n+        let test2 = string_to_ts(\"foo(bar::baz)\");\n+\n+        assert_eq!(test0.is_empty(), true);\n+        assert_eq!(test1.is_empty(), false);\n+        assert_eq!(test2.is_empty(), false);\n+    })\n+}\n+\n+#[test]\n+fn test_dotdotdot() {\n+    with_default_globals(|| {\n+        let mut builder = TokenStreamBuilder::new();\n+        builder.push(TokenTree::token(token::Dot, sp(0, 1)).joint());\n+        builder.push(TokenTree::token(token::Dot, sp(1, 2)).joint());\n+        builder.push(TokenTree::token(token::Dot, sp(2, 3)));\n+        let stream = builder.build();\n+        assert!(stream.eq_unspanned(&string_to_ts(\"...\")));\n+        assert_eq!(stream.trees().count(), 1);\n+    })\n+}"}, {"sha": "4127a8c7fce2502650e8d52868b0b8320632fab8", "filename": "src/libsyntax/util/lev_distance.rs", "status": "modified", "additions": 3, "deletions": 57, "changes": 60, "blob_url": "https://github.com/rust-lang/rust/blob/310b9fc76002066feb89dcfbf8e88b34fe5f4ad3/src%2Flibsyntax%2Futil%2Flev_distance.rs", "raw_url": "https://github.com/rust-lang/rust/raw/310b9fc76002066feb89dcfbf8e88b34fe5f4ad3/src%2Flibsyntax%2Futil%2Flev_distance.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Futil%2Flev_distance.rs?ref=310b9fc76002066feb89dcfbf8e88b34fe5f4ad3", "patch": "@@ -1,6 +1,9 @@\n use std::cmp;\n use crate::symbol::Symbol;\n \n+#[cfg(test)]\n+mod tests;\n+\n /// Finds the Levenshtein distance between two strings\n pub fn lev_distance(a: &str, b: &str) -> usize {\n     // cases which don't require further computation\n@@ -77,60 +80,3 @@ pub fn find_best_match_for_name<'a, T>(iter_names: T,\n         if let Some((candidate, _)) = levenstein_match { Some(candidate) } else { None }\n     }\n }\n-\n-#[test]\n-fn test_lev_distance() {\n-    use std::char::{from_u32, MAX};\n-    // Test bytelength agnosticity\n-    for c in (0..MAX as u32)\n-             .filter_map(|i| from_u32(i))\n-             .map(|i| i.to_string()) {\n-        assert_eq!(lev_distance(&c[..], &c[..]), 0);\n-    }\n-\n-    let a = \"\\nM\u00e4ry h\u00e4d \u00e4 little l\u00e4mb\\n\\nLittle l\u00e4mb\\n\";\n-    let b = \"\\nMary h\u00e4d \u00e4 little l\u00e4mb\\n\\nLittle l\u00e4mb\\n\";\n-    let c = \"Mary h\u00e4d \u00e4 little l\u00e4mb\\n\\nLittle l\u00e4mb\\n\";\n-    assert_eq!(lev_distance(a, b), 1);\n-    assert_eq!(lev_distance(b, a), 1);\n-    assert_eq!(lev_distance(a, c), 2);\n-    assert_eq!(lev_distance(c, a), 2);\n-    assert_eq!(lev_distance(b, c), 1);\n-    assert_eq!(lev_distance(c, b), 1);\n-}\n-\n-#[test]\n-fn test_find_best_match_for_name() {\n-    use crate::with_default_globals;\n-    with_default_globals(|| {\n-        let input = vec![Symbol::intern(\"aaab\"), Symbol::intern(\"aaabc\")];\n-        assert_eq!(\n-            find_best_match_for_name(input.iter(), \"aaaa\", None),\n-            Some(Symbol::intern(\"aaab\"))\n-        );\n-\n-        assert_eq!(\n-            find_best_match_for_name(input.iter(), \"1111111111\", None),\n-            None\n-        );\n-\n-        let input = vec![Symbol::intern(\"aAAA\")];\n-        assert_eq!(\n-            find_best_match_for_name(input.iter(), \"AAAA\", None),\n-            Some(Symbol::intern(\"aAAA\"))\n-        );\n-\n-        let input = vec![Symbol::intern(\"AAAA\")];\n-        // Returns None because `lev_distance > max_dist / 3`\n-        assert_eq!(\n-            find_best_match_for_name(input.iter(), \"aaaa\", None),\n-            None\n-        );\n-\n-        let input = vec![Symbol::intern(\"AAAA\")];\n-        assert_eq!(\n-            find_best_match_for_name(input.iter(), \"aaaa\", Some(4)),\n-            Some(Symbol::intern(\"AAAA\"))\n-        );\n-    })\n-}"}, {"sha": "1a746a67ec0521414cb306710bcf143b4eea98fd", "filename": "src/libsyntax/util/lev_distance/tests.rs", "status": "added", "additions": 58, "deletions": 0, "changes": 58, "blob_url": "https://github.com/rust-lang/rust/blob/310b9fc76002066feb89dcfbf8e88b34fe5f4ad3/src%2Flibsyntax%2Futil%2Flev_distance%2Ftests.rs", "raw_url": "https://github.com/rust-lang/rust/raw/310b9fc76002066feb89dcfbf8e88b34fe5f4ad3/src%2Flibsyntax%2Futil%2Flev_distance%2Ftests.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Futil%2Flev_distance%2Ftests.rs?ref=310b9fc76002066feb89dcfbf8e88b34fe5f4ad3", "patch": "@@ -0,0 +1,58 @@\n+use super::*;\n+\n+#[test]\n+fn test_lev_distance() {\n+    use std::char::{from_u32, MAX};\n+    // Test bytelength agnosticity\n+    for c in (0..MAX as u32)\n+             .filter_map(|i| from_u32(i))\n+             .map(|i| i.to_string()) {\n+        assert_eq!(lev_distance(&c[..], &c[..]), 0);\n+    }\n+\n+    let a = \"\\nM\u00e4ry h\u00e4d \u00e4 little l\u00e4mb\\n\\nLittle l\u00e4mb\\n\";\n+    let b = \"\\nMary h\u00e4d \u00e4 little l\u00e4mb\\n\\nLittle l\u00e4mb\\n\";\n+    let c = \"Mary h\u00e4d \u00e4 little l\u00e4mb\\n\\nLittle l\u00e4mb\\n\";\n+    assert_eq!(lev_distance(a, b), 1);\n+    assert_eq!(lev_distance(b, a), 1);\n+    assert_eq!(lev_distance(a, c), 2);\n+    assert_eq!(lev_distance(c, a), 2);\n+    assert_eq!(lev_distance(b, c), 1);\n+    assert_eq!(lev_distance(c, b), 1);\n+}\n+\n+#[test]\n+fn test_find_best_match_for_name() {\n+    use crate::with_default_globals;\n+    with_default_globals(|| {\n+        let input = vec![Symbol::intern(\"aaab\"), Symbol::intern(\"aaabc\")];\n+        assert_eq!(\n+            find_best_match_for_name(input.iter(), \"aaaa\", None),\n+            Some(Symbol::intern(\"aaab\"))\n+        );\n+\n+        assert_eq!(\n+            find_best_match_for_name(input.iter(), \"1111111111\", None),\n+            None\n+        );\n+\n+        let input = vec![Symbol::intern(\"aAAA\")];\n+        assert_eq!(\n+            find_best_match_for_name(input.iter(), \"AAAA\", None),\n+            Some(Symbol::intern(\"aAAA\"))\n+        );\n+\n+        let input = vec![Symbol::intern(\"AAAA\")];\n+        // Returns None because `lev_distance > max_dist / 3`\n+        assert_eq!(\n+            find_best_match_for_name(input.iter(), \"aaaa\", None),\n+            None\n+        );\n+\n+        let input = vec![Symbol::intern(\"AAAA\")];\n+        assert_eq!(\n+            find_best_match_for_name(input.iter(), \"aaaa\", Some(4)),\n+            Some(Symbol::intern(\"AAAA\"))\n+        );\n+    })\n+}"}, {"sha": "627422df1db1c9cc8c9edc4818638f9770a1df87", "filename": "src/libsyntax/util/parser_testing.rs", "status": "removed", "additions": 0, "deletions": 160, "changes": 160, "blob_url": "https://github.com/rust-lang/rust/blob/a332e224a3bc2925fea584337d2d30e1186672be/src%2Flibsyntax%2Futil%2Fparser_testing.rs", "raw_url": "https://github.com/rust-lang/rust/raw/a332e224a3bc2925fea584337d2d30e1186672be/src%2Flibsyntax%2Futil%2Fparser_testing.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Futil%2Fparser_testing.rs?ref=a332e224a3bc2925fea584337d2d30e1186672be", "patch": "@@ -1,160 +0,0 @@\n-use crate::ast::{self, Ident};\n-use crate::source_map::FilePathMapping;\n-use crate::parse::{ParseSess, PResult, source_file_to_stream};\n-use crate::parse::new_parser_from_source_str;\n-use crate::parse::parser::Parser;\n-use crate::ptr::P;\n-use crate::tokenstream::TokenStream;\n-\n-use std::iter::Peekable;\n-use std::path::PathBuf;\n-\n-/// Map a string to tts, using a made-up filename:\n-pub fn string_to_stream(source_str: String) -> TokenStream {\n-    let ps = ParseSess::new(FilePathMapping::empty());\n-    source_file_to_stream(\n-        &ps,\n-        ps.source_map().new_source_file(PathBuf::from(\"bogofile\").into(),\n-        source_str,\n-    ), None).0\n-}\n-\n-/// Map string to parser (via tts)\n-pub fn string_to_parser(ps: &ParseSess, source_str: String) -> Parser<'_> {\n-    new_parser_from_source_str(ps, PathBuf::from(\"bogofile\").into(), source_str)\n-}\n-\n-fn with_error_checking_parse<'a, T, F>(s: String, ps: &'a ParseSess, f: F) -> T where\n-    F: FnOnce(&mut Parser<'a>) -> PResult<'a, T>,\n-{\n-    let mut p = string_to_parser(&ps, s);\n-    let x = panictry!(f(&mut p));\n-    p.sess.span_diagnostic.abort_if_errors();\n-    x\n-}\n-\n-/// Parse a string, return a crate.\n-pub fn string_to_crate(source_str : String) -> ast::Crate {\n-    let ps = ParseSess::new(FilePathMapping::empty());\n-    with_error_checking_parse(source_str, &ps, |p| {\n-        p.parse_crate_mod()\n-    })\n-}\n-\n-/// Parse a string, return an expr\n-pub fn string_to_expr(source_str : String) -> P<ast::Expr> {\n-    let ps = ParseSess::new(FilePathMapping::empty());\n-    with_error_checking_parse(source_str, &ps, |p| {\n-        p.parse_expr()\n-    })\n-}\n-\n-/// Parse a string, return an item\n-pub fn string_to_item(source_str : String) -> Option<P<ast::Item>> {\n-    let ps = ParseSess::new(FilePathMapping::empty());\n-    with_error_checking_parse(source_str, &ps, |p| {\n-        p.parse_item()\n-    })\n-}\n-\n-/// Parse a string, return a pat. Uses \"irrefutable\"... which doesn't\n-/// (currently) affect parsing.\n-pub fn string_to_pat(source_str: String) -> P<ast::Pat> {\n-    let ps = ParseSess::new(FilePathMapping::empty());\n-    with_error_checking_parse(source_str, &ps, |p| {\n-        p.parse_pat(None)\n-    })\n-}\n-\n-/// Converts a vector of strings to a vector of Ident's\n-pub fn strs_to_idents(ids: Vec<&str> ) -> Vec<Ident> {\n-    ids.iter().map(|u| Ident::from_str(*u)).collect()\n-}\n-\n-/// Does the given string match the pattern? whitespace in the first string\n-/// may be deleted or replaced with other whitespace to match the pattern.\n-/// This function is relatively Unicode-ignorant; fortunately, the careful design\n-/// of UTF-8 mitigates this ignorance. It doesn't do NKF-normalization(?).\n-pub fn matches_codepattern(a : &str, b : &str) -> bool {\n-    let mut a_iter = a.chars().peekable();\n-    let mut b_iter = b.chars().peekable();\n-\n-    loop {\n-        let (a, b) = match (a_iter.peek(), b_iter.peek()) {\n-            (None, None) => return true,\n-            (None, _) => return false,\n-            (Some(&a), None) => {\n-                if is_pattern_whitespace(a) {\n-                    break // trailing whitespace check is out of loop for borrowck\n-                } else {\n-                    return false\n-                }\n-            }\n-            (Some(&a), Some(&b)) => (a, b)\n-        };\n-\n-        if is_pattern_whitespace(a) && is_pattern_whitespace(b) {\n-            // skip whitespace for a and b\n-            scan_for_non_ws_or_end(&mut a_iter);\n-            scan_for_non_ws_or_end(&mut b_iter);\n-        } else if is_pattern_whitespace(a) {\n-            // skip whitespace for a\n-            scan_for_non_ws_or_end(&mut a_iter);\n-        } else if a == b {\n-            a_iter.next();\n-            b_iter.next();\n-        } else {\n-            return false\n-        }\n-    }\n-\n-    // check if a has *only* trailing whitespace\n-    a_iter.all(is_pattern_whitespace)\n-}\n-\n-/// Advances the given peekable `Iterator` until it reaches a non-whitespace character\n-fn scan_for_non_ws_or_end<I: Iterator<Item = char>>(iter: &mut Peekable<I>) {\n-    while iter.peek().copied().map(|c| is_pattern_whitespace(c)) == Some(true) {\n-        iter.next();\n-    }\n-}\n-\n-pub fn is_pattern_whitespace(c: char) -> bool {\n-    rustc_lexer::character_properties::is_whitespace(c)\n-}\n-\n-#[cfg(test)]\n-mod tests {\n-    use super::*;\n-\n-    #[test]\n-    fn eqmodws() {\n-        assert_eq!(matches_codepattern(\"\",\"\"),true);\n-        assert_eq!(matches_codepattern(\"\",\"a\"),false);\n-        assert_eq!(matches_codepattern(\"a\",\"\"),false);\n-        assert_eq!(matches_codepattern(\"a\",\"a\"),true);\n-        assert_eq!(matches_codepattern(\"a b\",\"a   \\n\\t\\r  b\"),true);\n-        assert_eq!(matches_codepattern(\"a b \",\"a   \\n\\t\\r  b\"),true);\n-        assert_eq!(matches_codepattern(\"a b\",\"a   \\n\\t\\r  b \"),false);\n-        assert_eq!(matches_codepattern(\"a   b\",\"a b\"),true);\n-        assert_eq!(matches_codepattern(\"ab\",\"a b\"),false);\n-        assert_eq!(matches_codepattern(\"a   b\",\"ab\"),true);\n-        assert_eq!(matches_codepattern(\" a   b\",\"ab\"),true);\n-    }\n-\n-    #[test]\n-    fn pattern_whitespace() {\n-        assert_eq!(matches_codepattern(\"\",\"\\x0C\"), false);\n-        assert_eq!(matches_codepattern(\"a b \",\"a   \\u{0085}\\n\\t\\r  b\"),true);\n-        assert_eq!(matches_codepattern(\"a b\",\"a   \\u{0085}\\n\\t\\r  b \"),false);\n-    }\n-\n-    #[test]\n-    fn non_pattern_whitespace() {\n-        // These have the property 'White_Space' but not 'Pattern_White_Space'\n-        assert_eq!(matches_codepattern(\"a b\",\"a\\u{2002}b\"), false);\n-        assert_eq!(matches_codepattern(\"a   b\",\"a\\u{2002}b\"), false);\n-        assert_eq!(matches_codepattern(\"\\u{205F}a   b\",\"ab\"), false);\n-        assert_eq!(matches_codepattern(\"a  \\u{3000}b\",\"ab\"), false);\n-    }\n-}"}, {"sha": "d01069f4269584a91f1bfbd0aaa3e62a50610c56", "filename": "src/tools/tidy/src/unit_tests.rs", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "blob_url": "https://github.com/rust-lang/rust/blob/310b9fc76002066feb89dcfbf8e88b34fe5f4ad3/src%2Ftools%2Ftidy%2Fsrc%2Funit_tests.rs", "raw_url": "https://github.com/rust-lang/rust/raw/310b9fc76002066feb89dcfbf8e88b34fe5f4ad3/src%2Ftools%2Ftidy%2Fsrc%2Funit_tests.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ftools%2Ftidy%2Fsrc%2Funit_tests.rs?ref=310b9fc76002066feb89dcfbf8e88b34fe5f4ad3", "patch": "@@ -28,7 +28,6 @@ pub fn check(root_path: &Path, bad: &mut bool) {\n     let fixme = [\n         \"liballoc\",\n         \"libstd\",\n-        \"libsyntax\",\n     ];\n \n     let mut skip = |path: &Path| {"}]}