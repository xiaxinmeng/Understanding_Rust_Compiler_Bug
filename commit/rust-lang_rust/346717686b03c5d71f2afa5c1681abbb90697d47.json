{"sha": "346717686b03c5d71f2afa5c1681abbb90697d47", "node_id": "MDY6Q29tbWl0NzI0NzEyOjM0NjcxNzY4NmIwM2M1ZDcxZjJhZmE1YzE2ODFhYmJiOTA2OTdkNDc=", "commit": {"author": {"name": "Piotr Czarnecki", "email": "pioczarn@gmail.com", "date": "2017-07-23T09:55:52Z"}, "committer": {"name": "Piotr Czarnecki", "email": "pioczarn@gmail.com", "date": "2017-07-24T11:57:08Z"}, "message": "Make the macro parser theory description more accurate", "tree": {"sha": "76c8783c2064d8870b0ae7fcc1f9e941a29f837f", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/76c8783c2064d8870b0ae7fcc1f9e941a29f837f"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/346717686b03c5d71f2afa5c1681abbb90697d47", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/346717686b03c5d71f2afa5c1681abbb90697d47", "html_url": "https://github.com/rust-lang/rust/commit/346717686b03c5d71f2afa5c1681abbb90697d47", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/346717686b03c5d71f2afa5c1681abbb90697d47/comments", "author": {"login": "pczarn", "id": 3356767, "node_id": "MDQ6VXNlcjMzNTY3Njc=", "avatar_url": "https://avatars.githubusercontent.com/u/3356767?v=4", "gravatar_id": "", "url": "https://api.github.com/users/pczarn", "html_url": "https://github.com/pczarn", "followers_url": "https://api.github.com/users/pczarn/followers", "following_url": "https://api.github.com/users/pczarn/following{/other_user}", "gists_url": "https://api.github.com/users/pczarn/gists{/gist_id}", "starred_url": "https://api.github.com/users/pczarn/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/pczarn/subscriptions", "organizations_url": "https://api.github.com/users/pczarn/orgs", "repos_url": "https://api.github.com/users/pczarn/repos", "events_url": "https://api.github.com/users/pczarn/events{/privacy}", "received_events_url": "https://api.github.com/users/pczarn/received_events", "type": "User", "site_admin": false}, "committer": {"login": "pczarn", "id": 3356767, "node_id": "MDQ6VXNlcjMzNTY3Njc=", "avatar_url": "https://avatars.githubusercontent.com/u/3356767?v=4", "gravatar_id": "", "url": "https://api.github.com/users/pczarn", "html_url": "https://github.com/pczarn", "followers_url": "https://api.github.com/users/pczarn/followers", "following_url": "https://api.github.com/users/pczarn/following{/other_user}", "gists_url": "https://api.github.com/users/pczarn/gists{/gist_id}", "starred_url": "https://api.github.com/users/pczarn/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/pczarn/subscriptions", "organizations_url": "https://api.github.com/users/pczarn/orgs", "repos_url": "https://api.github.com/users/pczarn/repos", "events_url": "https://api.github.com/users/pczarn/events{/privacy}", "received_events_url": "https://api.github.com/users/pczarn/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "6270257f4ef6e65213631d6b5a2a85807b8b2364", "url": "https://api.github.com/repos/rust-lang/rust/commits/6270257f4ef6e65213631d6b5a2a85807b8b2364", "html_url": "https://github.com/rust-lang/rust/commit/6270257f4ef6e65213631d6b5a2a85807b8b2364"}], "stats": {"total": 202, "additions": 102, "deletions": 100}, "files": [{"sha": "60833c75a15961ceb525d5deb1813cc8bf0d49b2", "filename": "src/libsyntax/ext/tt/macro_parser.rs", "status": "modified", "additions": 102, "deletions": 100, "changes": 202, "blob_url": "https://github.com/rust-lang/rust/blob/346717686b03c5d71f2afa5c1681abbb90697d47/src%2Flibsyntax%2Fext%2Ftt%2Fmacro_parser.rs", "raw_url": "https://github.com/rust-lang/rust/raw/346717686b03c5d71f2afa5c1681abbb90697d47/src%2Flibsyntax%2Fext%2Ftt%2Fmacro_parser.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fext%2Ftt%2Fmacro_parser.rs?ref=346717686b03c5d71f2afa5c1681abbb90697d47", "patch": "@@ -1,4 +1,4 @@\n-// Copyright 2012-2014 The Rust Project Developers. See the COPYRIGHT\n+// Copyright 2012-2017 The Rust Project Developers. See the COPYRIGHT\n // file at the top-level directory of this distribution and at\n // http://rust-lang.org/COPYRIGHT.\n //\n@@ -8,41 +8,43 @@\n // option. This file may not be copied, modified, or distributed\n // except according to those terms.\n \n-//! This is an Earley-like parser, without support for in-grammar nonterminals,\n-//! only by calling out to the main rust parser for named nonterminals (which it\n-//! commits to fully when it hits one in a grammar). This means that there are no\n-//! completer or predictor rules, and therefore no need to store one column per\n-//! token: instead, there's a set of current Earley items and a set of next\n-//! ones. Instead of NTs, we have a special case for Kleene star. The big-O, in\n-//! pathological cases, is worse than traditional Earley parsing, but it's an\n-//! easier fit for Macro-by-Example-style rules, and I think the overhead is\n-//! lower. (In order to prevent the pathological case, we'd need to lazily\n-//! construct the resulting `NamedMatch`es at the very end. It'd be a pain,\n-//! and require more memory to keep around old items, but it would also save\n-//! overhead)\n+//! This is an NFA-based parser, which calls out to the main rust parser for named nonterminals\n+//! (which it commits to fully when it hits one in a grammar). There's a set of current NFA threads\n+//! and a set of next ones. Instead of NTs, we have a special case for Kleene star. The big-O, in\n+//! pathological cases, is worse than traditional use of NFA or Earley parsing, but it's an easier\n+//! fit for Macro-by-Example-style rules.\n+//!\n+//! (In order to prevent the pathological case, we'd need to lazily construct the resulting\n+//! `NamedMatch`es at the very end. It'd be a pain, and require more memory to keep around old\n+//! items, but it would also save overhead)\n+//!\n+//! We don't say this parser uses the Earley algorithm, because it's unnecessarily innacurate.\n+//! The macro parser restricts itself to the features of finite state automata. Earley parsers\n+//! can be described as an extension of NFAs with completion rules, prediction rules, and recursion.\n //!\n //! Quick intro to how the parser works:\n //!\n //! A 'position' is a dot in the middle of a matcher, usually represented as a\n //! dot. For example `\u00b7 a $( a )* a b` is a position, as is `a $( \u00b7 a )* a b`.\n //!\n //! The parser walks through the input a character at a time, maintaining a list\n-//! of items consistent with the current position in the input string: `cur_eis`.\n+//! of threads consistent with the current position in the input string: `cur_items`.\n //!\n-//! As it processes them, it fills up `eof_eis` with items that would be valid if\n-//! the macro invocation is now over, `bb_eis` with items that are waiting on\n-//! a Rust nonterminal like `$e:expr`, and `next_eis` with items that are waiting\n+//! As it processes them, it fills up `eof_items` with threads that would be valid if\n+//! the macro invocation is now over, `bb_items` with threads that are waiting on\n+//! a Rust nonterminal like `$e:expr`, and `next_items` with threads that are waiting\n //! on a particular token. Most of the logic concerns moving the \u00b7 through the\n-//! repetitions indicated by Kleene stars. It only advances or calls out to the\n-//! real Rust parser when no `cur_eis` items remain\n+//! repetitions indicated by Kleene stars. The rules for moving the \u00b7 without\n+//! consuming any input are called epsilon transitions. It only advances or calls\n+//! out to the real Rust parser when no `cur_items` threads remain.\n //!\n //! Example:\n //!\n //! ```text, ignore\n //! Start parsing a a a a b against [\u00b7 a $( a )* a b].\n //!\n //! Remaining input: a a a a b\n-//! next_eis: [\u00b7 a $( a )* a b]\n+//! next: [\u00b7 a $( a )* a b]\n //!\n //! - - - Advance over an a. - - -\n //!\n@@ -54,23 +56,23 @@\n //! - - - Advance over an a. - - -\n //!\n //! Remaining input: a a b\n-//! cur: [a $( a \u00b7 )* a b]  next: [a $( a )* a \u00b7 b]\n-//! Finish/Repeat (first item)\n+//! cur: [a $( a \u00b7 )* a b]  [a $( a )* a \u00b7 b]\n+//! Follow epsilon transition: Finish/Repeat (first item)\n //! next: [a $( a )* \u00b7 a b]  [a $( \u00b7 a )* a b]  [a $( a )* a \u00b7 b]\n //!\n //! - - - Advance over an a. - - - (this looks exactly like the last step)\n //!\n //! Remaining input: a b\n-//! cur: [a $( a \u00b7 )* a b]  next: [a $( a )* a \u00b7 b]\n-//! Finish/Repeat (first item)\n+//! cur: [a $( a \u00b7 )* a b]  [a $( a )* a \u00b7 b]\n+//! Follow epsilon transition: Finish/Repeat (first item)\n //! next: [a $( a )* \u00b7 a b]  [a $( \u00b7 a )* a b]  [a $( a )* a \u00b7 b]\n //!\n //! - - - Advance over an a. - - - (this looks exactly like the last step)\n //!\n //! Remaining input: b\n-//! cur: [a $( a \u00b7 )* a b]  next: [a $( a )* a \u00b7 b]\n-//! Finish/Repeat (first item)\n-//! next: [a $( a )* \u00b7 a b]  [a $( \u00b7 a )* a b]\n+//! cur: [a $( a \u00b7 )* a b]  [a $( a )* a \u00b7 b]\n+//! Follow epsilon transition: Finish/Repeat (first item)\n+//! next: [a $( a )* \u00b7 a b]  [a $( \u00b7 a )* a b]  [a $( a )* a \u00b7 b]\n //!\n //! - - - Advance over a b. - - -\n //!\n@@ -289,94 +291,94 @@ fn create_matches(len: usize) -> Vec<Rc<Vec<NamedMatch>>> {\n }\n \n fn inner_parse_loop(sess: &ParseSess,\n-                    cur_eis: &mut SmallVector<Box<MatcherPos>>,\n-                    next_eis: &mut Vec<Box<MatcherPos>>,\n-                    eof_eis: &mut SmallVector<Box<MatcherPos>>,\n-                    bb_eis: &mut SmallVector<Box<MatcherPos>>,\n+                    cur_items: &mut SmallVector<Box<MatcherPos>>,\n+                    next_items: &mut Vec<Box<MatcherPos>>,\n+                    eof_items: &mut SmallVector<Box<MatcherPos>>,\n+                    bb_items: &mut SmallVector<Box<MatcherPos>>,\n                     token: &Token,\n                     span: syntax_pos::Span)\n                     -> ParseResult<()> {\n-    while let Some(mut ei) = cur_eis.pop() {\n+    while let Some(mut item) = cur_items.pop() {\n         // When unzipped trees end, remove them\n-        while ei.idx >= ei.top_elts.len() {\n-            match ei.stack.pop() {\n+        while item.idx >= item.top_elts.len() {\n+            match item.stack.pop() {\n                 Some(MatcherTtFrame { elts, idx }) => {\n-                    ei.top_elts = elts;\n-                    ei.idx = idx + 1;\n+                    item.top_elts = elts;\n+                    item.idx = idx + 1;\n                 }\n                 None => break\n             }\n         }\n \n-        let idx = ei.idx;\n-        let len = ei.top_elts.len();\n+        let idx = item.idx;\n+        let len = item.top_elts.len();\n \n         // at end of sequence\n         if idx >= len {\n             // We are repeating iff there is a parent\n-            if ei.up.is_some() {\n+            if item.up.is_some() {\n                 // Disregarding the separator, add the \"up\" case to the tokens that should be\n                 // examined.\n                 // (remove this condition to make trailing seps ok)\n                 if idx == len {\n-                    let mut new_pos = ei.up.clone().unwrap();\n+                    let mut new_pos = item.up.clone().unwrap();\n \n                     // update matches (the MBE \"parse tree\") by appending\n                     // each tree as a subtree.\n \n                     // Only touch the binders we have actually bound\n-                    for idx in ei.match_lo..ei.match_hi {\n-                        let sub = ei.matches[idx].clone();\n-                        new_pos.push_match(idx, MatchedSeq(sub, Span { lo: ei.sp_lo, ..span }));\n+                    for idx in item.match_lo..item.match_hi {\n+                        let sub = item.matches[idx].clone();\n+                        new_pos.push_match(idx, MatchedSeq(sub, Span { lo: item.sp_lo, ..span }));\n                     }\n \n-                    new_pos.match_cur = ei.match_hi;\n+                    new_pos.match_cur = item.match_hi;\n                     new_pos.idx += 1;\n-                    cur_eis.push(new_pos);\n+                    cur_items.push(new_pos);\n                 }\n \n                 // Check if we need a separator\n-                if idx == len && ei.sep.is_some() {\n+                if idx == len && item.sep.is_some() {\n                     // We have a separator, and it is the current token.\n-                    if ei.sep.as_ref().map(|sep| token_name_eq(token, sep)).unwrap_or(false) {\n-                        ei.idx += 1;\n-                        next_eis.push(ei);\n+                    if item.sep.as_ref().map(|sep| token_name_eq(token, sep)).unwrap_or(false) {\n+                        item.idx += 1;\n+                        next_items.push(item);\n                     }\n                 } else { // we don't need a separator\n-                    ei.match_cur = ei.match_lo;\n-                    ei.idx = 0;\n-                    cur_eis.push(ei);\n+                    item.match_cur = item.match_lo;\n+                    item.idx = 0;\n+                    cur_items.push(item);\n                 }\n             } else {\n                 // We aren't repeating, so we must be potentially at the end of the input.\n-                eof_eis.push(ei);\n+                eof_items.push(item);\n             }\n         } else {\n-            match ei.top_elts.get_tt(idx) {\n+            match item.top_elts.get_tt(idx) {\n                 /* need to descend into sequence */\n                 TokenTree::Sequence(sp, seq) => {\n                     if seq.op == quoted::KleeneOp::ZeroOrMore {\n                         // Examine the case where there are 0 matches of this sequence\n-                        let mut new_ei = ei.clone();\n-                        new_ei.match_cur += seq.num_captures;\n-                        new_ei.idx += 1;\n-                        for idx in ei.match_cur..ei.match_cur + seq.num_captures {\n-                            new_ei.push_match(idx, MatchedSeq(Rc::new(vec![]), sp));\n+                        let mut new_item = item.clone();\n+                        new_item.match_cur += seq.num_captures;\n+                        new_item.idx += 1;\n+                        for idx in item.match_cur..item.match_cur + seq.num_captures {\n+                            new_item.push_match(idx, MatchedSeq(Rc::new(vec![]), sp));\n                         }\n-                        cur_eis.push(new_ei);\n+                        cur_items.push(new_item);\n                     }\n \n                     // Examine the case where there is at least one match of this sequence\n-                    let matches = create_matches(ei.matches.len());\n-                    cur_eis.push(Box::new(MatcherPos {\n+                    let matches = create_matches(item.matches.len());\n+                    cur_items.push(Box::new(MatcherPos {\n                         stack: vec![],\n                         sep: seq.separator.clone(),\n                         idx: 0,\n                         matches: matches,\n-                        match_lo: ei.match_cur,\n-                        match_cur: ei.match_cur,\n-                        match_hi: ei.match_cur + seq.num_captures,\n-                        up: Some(ei),\n+                        match_lo: item.match_cur,\n+                        match_cur: item.match_cur,\n+                        match_hi: item.match_cur + seq.num_captures,\n+                        up: Some(item),\n                         sp_lo: sp.lo,\n                         top_elts: Tt(TokenTree::Sequence(sp, seq)),\n                     }));\n@@ -390,22 +392,22 @@ fn inner_parse_loop(sess: &ParseSess,\n                     // Built-in nonterminals never start with these tokens,\n                     // so we can eliminate them from consideration.\n                     if may_begin_with(&*id.name.as_str(), token) {\n-                        bb_eis.push(ei);\n+                        bb_items.push(item);\n                     }\n                 }\n                 seq @ TokenTree::Delimited(..) | seq @ TokenTree::Token(_, DocComment(..)) => {\n-                    let lower_elts = mem::replace(&mut ei.top_elts, Tt(seq));\n-                    let idx = ei.idx;\n-                    ei.stack.push(MatcherTtFrame {\n+                    let lower_elts = mem::replace(&mut item.top_elts, Tt(seq));\n+                    let idx = item.idx;\n+                    item.stack.push(MatcherTtFrame {\n                         elts: lower_elts,\n                         idx: idx,\n                     });\n-                    ei.idx = 0;\n-                    cur_eis.push(ei);\n+                    item.idx = 0;\n+                    cur_items.push(item);\n                 }\n                 TokenTree::Token(_, ref t) if token_name_eq(t, token) => {\n-                    ei.idx += 1;\n-                    next_eis.push(ei);\n+                    item.idx += 1;\n+                    next_items.push(item);\n                 }\n                 TokenTree::Token(..) | TokenTree::MetaVar(..) => {}\n             }\n@@ -422,38 +424,38 @@ pub fn parse(sess: &ParseSess,\n              recurse_into_modules: bool)\n              -> NamedParseResult {\n     let mut parser = Parser::new(sess, tts, directory, recurse_into_modules, true);\n-    let mut cur_eis = SmallVector::one(initial_matcher_pos(ms.to_owned(), parser.span.lo));\n-    let mut next_eis = Vec::new(); // or proceed normally\n+    let mut cur_items = SmallVector::one(initial_matcher_pos(ms.to_owned(), parser.span.lo));\n+    let mut next_items = Vec::new(); // or proceed normally\n \n     loop {\n-        let mut bb_eis = SmallVector::new(); // black-box parsed by parser.rs\n-        let mut eof_eis = SmallVector::new();\n-        assert!(next_eis.is_empty());\n+        let mut bb_items = SmallVector::new(); // black-box parsed by parser.rs\n+        let mut eof_items = SmallVector::new();\n+        assert!(next_items.is_empty());\n \n-        match inner_parse_loop(sess, &mut cur_eis, &mut next_eis, &mut eof_eis, &mut bb_eis,\n+        match inner_parse_loop(sess, &mut cur_items, &mut next_items, &mut eof_items, &mut bb_items,\n                                &parser.token, parser.span) {\n             Success(_) => {},\n             Failure(sp, tok) => return Failure(sp, tok),\n             Error(sp, msg) => return Error(sp, msg),\n         }\n \n-        // inner parse loop handled all cur_eis, so it's empty\n-        assert!(cur_eis.is_empty());\n+        // inner parse loop handled all cur_items, so it's empty\n+        assert!(cur_items.is_empty());\n \n         /* error messages here could be improved with links to orig. rules */\n         if token_name_eq(&parser.token, &token::Eof) {\n-            if eof_eis.len() == 1 {\n-                let matches = eof_eis[0].matches.iter_mut().map(|mut dv| {\n+            if eof_items.len() == 1 {\n+                let matches = eof_items[0].matches.iter_mut().map(|mut dv| {\n                     Rc::make_mut(dv).pop().unwrap()\n                 });\n                 return nameize(sess, ms, matches);\n-            } else if eof_eis.len() > 1 {\n+            } else if eof_items.len() > 1 {\n                 return Error(parser.span, \"ambiguity: multiple successful parses\".to_string());\n             } else {\n                 return Failure(parser.span, token::Eof);\n             }\n-        } else if (!bb_eis.is_empty() && !next_eis.is_empty()) || bb_eis.len() > 1 {\n-            let nts = bb_eis.iter().map(|ei| match ei.top_elts.get_tt(ei.idx) {\n+        } else if (!bb_items.is_empty() && !next_items.is_empty()) || bb_items.len() > 1 {\n+            let nts = bb_items.iter().map(|item| match item.top_elts.get_tt(item.idx) {\n                 TokenTree::MetaVarDecl(_, bind, name) => {\n                     format!(\"{} ('{}')\", name, bind)\n                 }\n@@ -462,33 +464,33 @@ pub fn parse(sess: &ParseSess,\n \n             return Error(parser.span, format!(\n                 \"local ambiguity: multiple parsing options: {}\",\n-                match next_eis.len() {\n+                match next_items.len() {\n                     0 => format!(\"built-in NTs {}.\", nts),\n                     1 => format!(\"built-in NTs {} or 1 other option.\", nts),\n                     n => format!(\"built-in NTs {} or {} other options.\", nts, n),\n                 }\n             ));\n-        } else if bb_eis.is_empty() && next_eis.is_empty() {\n+        } else if bb_items.is_empty() && next_items.is_empty() {\n             return Failure(parser.span, parser.token);\n-        } else if !next_eis.is_empty() {\n+        } else if !next_items.is_empty() {\n             /* Now process the next token */\n-            cur_eis.extend(next_eis.drain(..));\n+            cur_items.extend(next_items.drain(..));\n             parser.bump();\n-        } else /* bb_eis.len() == 1 */ {\n-            let mut ei = bb_eis.pop().unwrap();\n-            if let TokenTree::MetaVarDecl(span, _, ident) = ei.top_elts.get_tt(ei.idx) {\n-                let match_cur = ei.match_cur;\n-                ei.push_match(match_cur,\n+        } else /* bb_items.len() == 1 */ {\n+            let mut item = bb_items.pop().unwrap();\n+            if let TokenTree::MetaVarDecl(span, _, ident) = item.top_elts.get_tt(item.idx) {\n+                let match_cur = item.match_cur;\n+                item.push_match(match_cur,\n                     MatchedNonterminal(Rc::new(parse_nt(&mut parser, span, &ident.name.as_str()))));\n-                ei.idx += 1;\n-                ei.match_cur += 1;\n+                item.idx += 1;\n+                item.match_cur += 1;\n             } else {\n                 unreachable!()\n             }\n-            cur_eis.push(ei);\n+            cur_items.push(item);\n         }\n \n-        assert!(!cur_eis.is_empty());\n+        assert!(!cur_items.is_empty());\n     }\n }\n "}]}