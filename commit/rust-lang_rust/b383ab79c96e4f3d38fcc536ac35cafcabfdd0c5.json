{"sha": "b383ab79c96e4f3d38fcc536ac35cafcabfdd0c5", "node_id": "MDY6Q29tbWl0NzI0NzEyOmIzODNhYjc5Yzk2ZTRmM2QzOGZjYzUzNmFjMzVjYWZjYWJmZGQwYzU=", "commit": {"author": {"name": "Niko Matsakis", "email": "niko@alum.mit.edu", "date": "2017-11-07T10:41:57Z"}, "committer": {"name": "Niko Matsakis", "email": "niko@alum.mit.edu", "date": "2017-11-16T10:57:47Z"}, "message": "update READMEs to describe the new situation\n\nThe inference README, in particular, was VERY out of date!", "tree": {"sha": "2e3122fa5237fc2b082f2db5e428906927ac03ca", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/2e3122fa5237fc2b082f2db5e428906927ac03ca"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/b383ab79c96e4f3d38fcc536ac35cafcabfdd0c5", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/b383ab79c96e4f3d38fcc536ac35cafcabfdd0c5", "html_url": "https://github.com/rust-lang/rust/commit/b383ab79c96e4f3d38fcc536ac35cafcabfdd0c5", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/b383ab79c96e4f3d38fcc536ac35cafcabfdd0c5/comments", "author": {"login": "nikomatsakis", "id": 155238, "node_id": "MDQ6VXNlcjE1NTIzOA==", "avatar_url": "https://avatars.githubusercontent.com/u/155238?v=4", "gravatar_id": "", "url": "https://api.github.com/users/nikomatsakis", "html_url": "https://github.com/nikomatsakis", "followers_url": "https://api.github.com/users/nikomatsakis/followers", "following_url": "https://api.github.com/users/nikomatsakis/following{/other_user}", "gists_url": "https://api.github.com/users/nikomatsakis/gists{/gist_id}", "starred_url": "https://api.github.com/users/nikomatsakis/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/nikomatsakis/subscriptions", "organizations_url": "https://api.github.com/users/nikomatsakis/orgs", "repos_url": "https://api.github.com/users/nikomatsakis/repos", "events_url": "https://api.github.com/users/nikomatsakis/events{/privacy}", "received_events_url": "https://api.github.com/users/nikomatsakis/received_events", "type": "User", "site_admin": false}, "committer": {"login": "nikomatsakis", "id": 155238, "node_id": "MDQ6VXNlcjE1NTIzOA==", "avatar_url": "https://avatars.githubusercontent.com/u/155238?v=4", "gravatar_id": "", "url": "https://api.github.com/users/nikomatsakis", "html_url": "https://github.com/nikomatsakis", "followers_url": "https://api.github.com/users/nikomatsakis/followers", "following_url": "https://api.github.com/users/nikomatsakis/following{/other_user}", "gists_url": "https://api.github.com/users/nikomatsakis/gists{/gist_id}", "starred_url": "https://api.github.com/users/nikomatsakis/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/nikomatsakis/subscriptions", "organizations_url": "https://api.github.com/users/nikomatsakis/orgs", "repos_url": "https://api.github.com/users/nikomatsakis/repos", "events_url": "https://api.github.com/users/nikomatsakis/events{/privacy}", "received_events_url": "https://api.github.com/users/nikomatsakis/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "4b743da596e94fbf56418e82ab92a4952d9bfb8b", "url": "https://api.github.com/repos/rust-lang/rust/commits/4b743da596e94fbf56418e82ab92a4952d9bfb8b", "html_url": "https://github.com/rust-lang/rust/commit/4b743da596e94fbf56418e82ab92a4952d9bfb8b"}], "stats": {"total": 965, "additions": 482, "deletions": 483}, "files": [{"sha": "e7daff3e2c371664789f24836a57521a165667f7", "filename": "src/librustc/infer/README.md", "status": "modified", "additions": 213, "deletions": 225, "changes": 438, "blob_url": "https://github.com/rust-lang/rust/blob/b383ab79c96e4f3d38fcc536ac35cafcabfdd0c5/src%2Flibrustc%2Finfer%2FREADME.md", "raw_url": "https://github.com/rust-lang/rust/raw/b383ab79c96e4f3d38fcc536ac35cafcabfdd0c5/src%2Flibrustc%2Finfer%2FREADME.md", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Finfer%2FREADME.md?ref=b383ab79c96e4f3d38fcc536ac35cafcabfdd0c5", "patch": "@@ -1,239 +1,227 @@\n # Type inference engine\n \n-This is loosely based on standard HM-type inference, but with an\n-extension to try and accommodate subtyping.  There is nothing\n-principled about this extension; it's sound---I hope!---but it's a\n-heuristic, ultimately, and does not guarantee that it finds a valid\n-typing even if one exists (in fact, there are known scenarios where it\n-fails, some of which may eventually become problematic).\n-\n-## Key idea\n-\n-The main change is that each type variable T is associated with a\n-lower-bound L and an upper-bound U.  L and U begin as bottom and top,\n-respectively, but gradually narrow in response to new constraints\n-being introduced.  When a variable is finally resolved to a concrete\n-type, it can (theoretically) select any type that is a supertype of L\n-and a subtype of U.\n-\n-There are several critical invariants which we maintain:\n-\n-- the upper-bound of a variable only becomes lower and the lower-bound\n-  only becomes higher over time;\n-- the lower-bound L is always a subtype of the upper bound U;\n-- the lower-bound L and upper-bound U never refer to other type variables,\n-  but only to types (though those types may contain type variables).\n-\n-> An aside: if the terms upper- and lower-bound confuse you, think of\n-> \"supertype\" and \"subtype\".  The upper-bound is a \"supertype\"\n-> (super=upper in Latin, or something like that anyway) and the lower-bound\n-> is a \"subtype\" (sub=lower in Latin).  I find it helps to visualize\n-> a simple class hierarchy, like Java minus interfaces and\n-> primitive types.  The class Object is at the root (top) and other\n-> types lie in between.  The bottom type is then the Null type.\n-> So the tree looks like:\n->\n-> ```text\n->         Object\n->         /    \\\n->     String   Other\n->         \\    /\n->         (null)\n-> ```\n->\n-> So the upper bound type is the \"supertype\" and the lower bound is the\n-> \"subtype\" (also, super and sub mean upper and lower in Latin, or something\n-> like that anyway).\n-\n-## Satisfying constraints\n-\n-At a primitive level, there is only one form of constraint that the\n-inference understands: a subtype relation.  So the outside world can\n-say \"make type A a subtype of type B\".  If there are variables\n-involved, the inferencer will adjust their upper- and lower-bounds as\n-needed to ensure that this relation is satisfied. (We also allow \"make\n-type A equal to type B\", but this is translated into \"A <: B\" and \"B\n-<: A\")\n-\n-As stated above, we always maintain the invariant that type bounds\n-never refer to other variables.  This keeps the inference relatively\n-simple, avoiding the scenario of having a kind of graph where we have\n-to pump constraints along and reach a fixed point, but it does impose\n-some heuristics in the case where the user is relating two type\n-variables A <: B.\n-\n-Combining two variables such that variable A will forever be a subtype\n-of variable B is the trickiest part of the algorithm because there is\n-often no right choice---that is, the right choice will depend on\n-future constraints which we do not yet know. The problem comes about\n-because both A and B have bounds that can be adjusted in the future.\n-Let's look at some of the cases that can come up.\n-\n-Imagine, to start, the best case, where both A and B have an upper and\n-lower bound (that is, the bounds are not top nor bot respectively). In\n-that case, if we're lucky, A.ub <: B.lb, and so we know that whatever\n-A and B should become, they will forever have the desired subtyping\n-relation.  We can just leave things as they are.\n-\n-### Option 1: Unify\n-\n-However, suppose that A.ub is *not* a subtype of B.lb.  In\n-that case, we must make a decision.  One option is to unify A\n-and B so that they are one variable whose bounds are:\n-\n-    UB = GLB(A.ub, B.ub)\n-    LB = LUB(A.lb, B.lb)\n-\n-(Note that we will have to verify that LB <: UB; if it does not, the\n-types are not intersecting and there is an error) In that case, A <: B\n-holds trivially because A==B.  However, we have now lost some\n-flexibility, because perhaps the user intended for A and B to end up\n-as different types and not the same type.\n-\n-Pictorially, what this does is to take two distinct variables with\n-(hopefully not completely) distinct type ranges and produce one with\n-the intersection.\n-\n-```text\n-                  B.ub                  B.ub\n-                   /\\                    /\n-           A.ub   /  \\           A.ub   /\n-           /   \\ /    \\              \\ /\n-          /     X      \\              UB\n-         /     / \\      \\            / \\\n-        /     /   /      \\          /   /\n-        \\     \\  /       /          \\  /\n-         \\      X       /             LB\n-          \\    / \\     /             / \\\n-           \\  /   \\   /             /   \\\n-           A.lb    B.lb          A.lb    B.lb\n-```\n+The type inference is based on standard HM-type inference, but\n+extended in various way to accommodate subtyping, region inference,\n+and higher-ranked types.\n+\n+## A note on terminology\n+\n+We use the notation `?T` to refer to inference variables, also called\n+existential variables.\n+\n+We use the term \"region\" and \"lifetime\" interchangeably. Both refer to\n+the `'a` in `&'a T`.\n+\n+The term \"bound region\" refers to regions bound in a function\n+signature, such as the `'a` in `for<'a> fn(&'a u32)`. A region is\n+\"free\" if it is not bound.\n \n+## Creating an inference context\n \n-### Option 2: Relate UB/LB\n-\n-Another option is to keep A and B as distinct variables but set their\n-bounds in such a way that, whatever happens, we know that A <: B will hold.\n-This can be achieved by ensuring that A.ub <: B.lb.  In practice there\n-are two ways to do that, depicted pictorially here:\n-\n-```text\n-    Before                Option #1            Option #2\n-\n-             B.ub                B.ub                B.ub\n-              /\\                 /  \\                /  \\\n-      A.ub   /  \\        A.ub   /(B')\\       A.ub   /(B')\\\n-      /   \\ /    \\           \\ /     /           \\ /     /\n-     /     X      \\         __UB____/             UB    /\n-    /     / \\      \\       /  |                   |    /\n-   /     /   /      \\     /   |                   |   /\n-   \\     \\  /       /    /(A')|                   |  /\n-    \\      X       /    /     LB            ______LB/\n-     \\    / \\     /    /     / \\           / (A')/ \\\n-      \\  /   \\   /     \\    /   \\          \\    /   \\\n-      A.lb    B.lb       A.lb    B.lb        A.lb    B.lb\n+You create and \"enter\" an inference context by doing something like\n+the following:\n+\n+```rust\n+tcx.infer_ctxt().enter(|infcx| {\n+    // use the inference context `infcx` in here\n+})\n ```\n \n-In these diagrams, UB and LB are defined as before.  As you can see,\n-the new ranges `A'` and `B'` are quite different from the range that\n-would be produced by unifying the variables.\n+Each inference context creates a short-lived type arena to store the\n+fresh types and things that it will create, as described in\n+[the README in the ty module][ty-readme]. This arena is created by the `enter`\n+function and disposed after it returns.\n \n-### What we do now\n+[ty-readme]: src/librustc/ty/README.md\n \n-Our current technique is to *try* (transactionally) to relate the\n-existing bounds of A and B, if there are any (i.e., if `UB(A) != top\n-&& LB(B) != bot`).  If that succeeds, we're done.  If it fails, then\n-we merge A and B into same variable.\n+Within the closure, the infcx will have the type `InferCtxt<'cx, 'gcx,\n+'tcx>` for some fresh `'cx` and `'tcx` -- the latter corresponds to\n+the lifetime of this temporary arena, and the `'cx` is the lifetime of\n+the `InferCtxt` itself. (Again, see [that ty README][ty-readme] for\n+more details on this setup.)\n \n-This is not clearly the correct course.  For example, if `UB(A) !=\n-top` but `LB(B) == bot`, we could conceivably set `LB(B)` to `UB(A)`\n-and leave the variables unmerged.  This is sometimes the better\n-course, it depends on the program.\n+The `tcx.infer_ctxt` method actually returns a build, which means\n+there are some kinds of configuration you can do before the `infcx` is\n+created. See `InferCtxtBuilder` for more information.\n \n-The main case which fails today that I would like to support is:\n+## Inference variables\n \n-```rust\n-fn foo<T>(x: T, y: T) { ... }\n+The main purpose of the inference context is to house a bunch of\n+**inference variables** -- these represent types or regions whose precise\n+value is not yet known, but will be uncovered as we perform type-checking.\n+\n+If you're familiar with the basic ideas of unification from H-M type\n+systems, or logic languages like Prolog, this is the same concept. If\n+you're not, you might want to read a tutorial on how H-M type\n+inference works, or perhaps this blog post on\n+[unification in the Chalk project].\n \n-fn bar() {\n-    let x: @mut int = @mut 3;\n-    let y: @int = @3;\n-    foo(x, y);\n-}\n+[Unification in the Chalk project]: http://smallcultfollowing.com/babysteps/blog/2017/03/25/unification-in-chalk-part-1/\n+\n+All told, the inference context stores four kinds of inference variables as of this\n+writing:\n+\n+- Type variables, which come in three varieties:\n+  - General type variables (the most common). These can be unified with any type.\n+  - Integral type variables, which can only be unified with an integral type, and\n+    arise from an integer literal expression like `22`.\n+  - Float type variables, which can only be unified with a float type, and\n+    arise from a float literal expression like `22.0`.\n+- Region variables, which represent lifetimes, and arise all over the dang place.\n+\n+All the type variables work in much the same way: you can create a new\n+type variable, and what you get is `Ty<'tcx>` representing an\n+unresolved type `?T`. Then later you can apply the various operations\n+that the inferencer supports, such as equality or subtyping, and it\n+will possibly **instantiate** (or **bind**) that `?T` to a specific\n+value as a result.\n+\n+The region variables work somewhat differently, and are described\n+below in a separate section.\n+\n+## Enforcing equality / subtyping\n+\n+The most basic operations you can perform in the type inferencer is\n+**equality**, which forces two types `T` and `U` to be the same. The\n+recommended way to add an equality constraint is using the `at`\n+method, roughly like so:\n+\n+```\n+infcx.at(...).eq(t, u);\n ```\n \n-In principle, the inferencer ought to find that the parameter `T` to\n-`foo(x, y)` is `@const int`.  Today, however, it does not; this is\n-because the type variable `T` is merged with the type variable for\n-`X`, and thus inherits its UB/LB of `@mut int`.  This leaves no\n-flexibility for `T` to later adjust to accommodate `@int`.\n-\n-Note: `@` and `@mut` are replaced with `Rc<T>` and `Rc<RefCell<T>>` in current Rust.\n-\n-### What to do when not all bounds are present\n-\n-In the prior discussion we assumed that A.ub was not top and B.lb was\n-not bot.  Unfortunately this is rarely the case.  Often type variables\n-have \"lopsided\" bounds.  For example, if a variable in the program has\n-been initialized but has not been used, then its corresponding type\n-variable will have a lower bound but no upper bound.  When that\n-variable is then used, we would like to know its upper bound---but we\n-don't have one!  In this case we'll do different things depending on\n-how the variable is being used.\n-\n-## Transactional support\n-\n-Whenever we adjust merge variables or adjust their bounds, we always\n-keep a record of the old value.  This allows the changes to be undone.\n-\n-## Regions\n-\n-I've only talked about type variables here, but region variables\n-follow the same principle.  They have upper- and lower-bounds.  A\n-region A is a subregion of a region B if A being valid implies that B\n-is valid.  This basically corresponds to the block nesting structure:\n-the regions for outer block scopes are superregions of those for inner\n-block scopes.\n-\n-## Integral and floating-point type variables\n-\n-There is a third variety of type variable that we use only for\n-inferring the types of unsuffixed integer literals.  Integral type\n-variables differ from general-purpose type variables in that there's\n-no subtyping relationship among the various integral types, so instead\n-of associating each variable with an upper and lower bound, we just\n-use simple unification.  Each integer variable is associated with at\n-most one integer type.  Floating point types are handled similarly to\n-integral types.\n-\n-## GLB/LUB\n-\n-Computing the greatest-lower-bound and least-upper-bound of two\n-types/regions is generally straightforward except when type variables\n-are involved. In that case, we follow a similar \"try to use the bounds\n-when possible but otherwise merge the variables\" strategy.  In other\n-words, `GLB(A, B)` where `A` and `B` are variables will often result\n-in `A` and `B` being merged and the result being `A`.\n-\n-## Type coercion\n-\n-We have a notion of assignability which differs somewhat from\n-subtyping; in particular it may cause region borrowing to occur.  See\n-the big comment later in this file on Type Coercion for specifics.\n-\n-### In conclusion\n-\n-I showed you three ways to relate `A` and `B`.  There are also more,\n-of course, though I'm not sure if there are any more sensible options.\n-The main point is that there are various options, each of which\n-produce a distinct range of types for `A` and `B`.  Depending on what\n-the correct values for A and B are, one of these options will be the\n-right choice: but of course we don't know the right values for A and B\n-yet, that's what we're trying to find!  In our code, we opt to unify\n-(Option #1).\n-\n-# Implementation details\n-\n-We make use of a trait-like implementation strategy to consolidate\n-duplicated code between subtypes, GLB, and LUB computations.  See the\n-section on \"Type Combining\" in combine.rs for more details.\n+The first `at()` call provides a bit of context, i.e., why you are\n+doing this unification, and in what environment, and the `eq` method\n+performs the actual equality constraint.\n+\n+When you equate things, you force them to be precisely equal. Equating\n+returns a `InferResult` -- if it returns `Err(err)`, then equating\n+failed, and the enclosing `TypeError` will tell you what went wrong.\n+\n+The success case is perhaps more interesting. The \"primary\" return\n+type of `eq` is `()` -- that is, when it succeeds, it doesn't return a\n+value of any particular interest. Rather, it is executed for its\n+side-effects of constraining type variables and so forth. However, the\n+actual return type is not `()`, but rather `InferOk<()>`. The\n+`InferOk` type is used to carry extra trait obligations -- your job is\n+to ensure that these are fulfilled (typically by enrolling them in a\n+fulfillment context). See the [trait README] for more background here.\n+\n+[trait README]: ../traits/README.md\n+\n+You can also enforce subtyping through `infcx.at(..).sub(..)`. The same\n+basic concepts apply as above.\n+\n+## \"Trying\" equality\n+\n+Sometimes you would like to know if it is *possible* to equate two\n+types without error.  You can test that with `infcx.can_eq` (or\n+`infcx.can_sub` for subtyping). If this returns `Ok`, then equality\n+is possible -- but in all cases, any side-effects are reversed.\n+\n+Be aware though that the success or failure of these methods is always\n+**modulo regions**. That is, two types `&'a u32` and `&'b u32` will\n+return `Ok` for `can_eq`, even if `'a != 'b`.  This falls out from the\n+\"two-phase\" nature of how we solve region constraints.\n+\n+## Snapshots\n+\n+As described in the previous section on `can_eq`, often it is useful\n+to be able to do a series of operations and then roll back their\n+side-effects. This is done for various reasons: one of them is to be\n+able to backtrack, trying out multiple possibilities before settling\n+on which path to take. Another is in order to ensure that a series of\n+smaller changes take place atomically or not at all.\n+\n+To allow for this, the inference context supports a `snapshot` method.\n+When you call it, it will start recording changes that occur from the\n+operations you perform. When you are done, you can either invoke\n+`rollback_to`, which will undo those changes, or else `confirm`, which\n+will make the permanent. Snapshots can be nested as long as you follow\n+a stack-like discipline.\n+\n+Rather than use snapshots directly, it is often helpful to use the\n+methods like `commit_if_ok` or `probe` that encapsulte higher-level\n+patterns.\n+\n+## Subtyping obligations\n+\n+One thing worth discussing are subtyping obligations. When you force\n+two types to be a subtype, like `?T <: i32`, we can often convert those\n+into equality constraints. This follows from Rust's rather limited notion\n+of subtyping: so, in the above case, `?T <: i32` is equivalent to `?T = i32`.\n+\n+However, in some cases we have to be more careful. For example, when\n+regions are involved. So if you have `?T <: &'a i32`, what we would do\n+is to first \"generalize\" `&'a i32` into a type with a region variable:\n+`&'?b i32`, and then unify `?T` with that (`?T = &'?b i32`). We then\n+relate this new variable with the original bound:\n+\n+    &'?b i32 <: &'a i32\n+    \n+This will result in a region constraint (see below) of `'?b: 'a`.\n+\n+One final interesting case is relating two unbound type variables,\n+like `?T <: ?U`.  In that case, we can't make progress, so we enqueue\n+an obligation `Subtype(?T, ?U)` and return it via the `InferOk`\n+mechanism. You'll have to try again when more details about `?T` or\n+`?U` are known.\n+\n+## Region constraints\n+\n+Regions are inferred somewhat differently from types. Rather than\n+eagerly unifying things, we simply collect constraints as we go, but\n+make (almost) no attempt to solve regions. These constraints have the\n+form of an outlives constraint:\n+\n+    'a: 'b\n+    \n+Actually the code tends to view them as a subregion relation, but it's the same\n+idea:\n+\n+    'b <= 'a\n+\n+(There are various other kinds of constriants, such as \"verifys\"; see\n+the `region_constraints` module for details.)\n+\n+There is one case where we do some amount of eager unification. If you have an equality constraint\n+between two regions\n+\n+    'a = 'b\n+    \n+we will record that fact in a unification table. You can then use\n+`opportunistic_resolve_var` to convert `'b` to `'a` (or vice\n+versa). This is sometimes needed to ensure termination of fixed-point\n+algorithms.\n+\n+## Extracting region constraints\n+\n+Ultimately, region constraints are only solved at the very end of\n+type-checking, once all other constraints are known. There are two\n+ways to solve region constraints right now: lexical and\n+non-lexical. Eventually there will only be one.\n+\n+To solve **lexical** region constraints, you invoke\n+`resolve_regions_and_report_errors`.  This will \"close\" the region\n+constraint process and invoke the `lexical_region_resolve` code. Once\n+this is done, any further attempt to equate or create a subtyping\n+relationship will yield an ICE.\n+\n+Non-lexical region constraints are not handled within the inference\n+context. Instead, the NLL solver (actually, the MIR type-checker)\n+invokes `take_and_reset_region_constraints` periodically. This\n+extracts all of the outlives constraints from the region solver, but\n+leaves the set of variables intact. This is used to get *just* the\n+region constraints that resulted from some particular point in the\n+program, since the NLL solver needs to know not just *what* regions\n+were subregions but *where*. Finally, the NLL solver invokes\n+`take_region_var_origins`, which \"closes\" the region constraint\n+process in the same way as normal solving.\n+\n+## Lexical region resolution\n+\n+Lexical region resolution is done by initially assigning each region\n+variable to an empty value. We then process each outlives constraint\n+repeatedly, growing region variables until a fixed-point is reached.\n+Region variables can be grown using a least-upper-bound relation on\n+the region lattice in a fairly straight-forward fashion."}, {"sha": "a53bfec80d981e0187fa0be6b8e8aa873b484cba", "filename": "src/librustc/infer/lexical_region_resolve/README.md", "status": "added", "additions": 262, "deletions": 0, "changes": 262, "blob_url": "https://github.com/rust-lang/rust/blob/b383ab79c96e4f3d38fcc536ac35cafcabfdd0c5/src%2Flibrustc%2Finfer%2Flexical_region_resolve%2FREADME.md", "raw_url": "https://github.com/rust-lang/rust/raw/b383ab79c96e4f3d38fcc536ac35cafcabfdd0c5/src%2Flibrustc%2Finfer%2Flexical_region_resolve%2FREADME.md", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Finfer%2Flexical_region_resolve%2FREADME.md?ref=b383ab79c96e4f3d38fcc536ac35cafcabfdd0c5", "patch": "@@ -0,0 +1,262 @@\n+# Region inference\n+\n+## Terminology\n+\n+Note that we use the terms region and lifetime interchangeably.\n+\n+## Introduction\n+\n+See the [general inference README](../README.md) for an overview of\n+how lexical-region-solving fits into the bigger picture.\n+\n+Region constraint collect uses a somewhat more involved algorithm than\n+type inference. It is not the most efficient thing ever written though\n+it seems to work well enough in practice (famous last words).  The\n+reason that we use a different algorithm is because, unlike with\n+types, it is impractical to hand-annotate with regions (in some cases,\n+there aren't even the requisite syntactic forms).  So we have to get\n+it right, and it's worth spending more time on a more involved\n+analysis.  Moreover, regions are a simpler case than types: they don't\n+have aggregate structure, for example.\n+\n+## The problem\n+\n+Basically our input is a directed graph where nodes can be divided\n+into two categories: region variables and concrete regions.  Each edge\n+`R -> S` in the graph represents a constraint that the region `R` is a\n+subregion of the region `S`.\n+\n+Region variable nodes can have arbitrary degree.  There is one region\n+variable node per region variable.\n+\n+Each concrete region node is associated with some, well, concrete\n+region: e.g., a free lifetime, or the region for a particular scope.\n+Note that there may be more than one concrete region node for a\n+particular region value.  Moreover, because of how the graph is built,\n+we know that all concrete region nodes have either in-degree 1 or\n+out-degree 1.\n+\n+Before resolution begins, we build up the constraints in a hashmap\n+that maps `Constraint` keys to spans.  During resolution, we construct\n+the actual `Graph` structure that we describe here.\n+\n+## Computing the values for region variables\n+\n+The algorithm is a simple dataflow algorithm. Each region variable\n+begins as empty. We iterate over the constraints, and for each constraint\n+we grow the relevant region variable to be as big as it must be to meet all the\n+constraints. This means the region variables can grow to be `'static` if\n+necessary.\n+\n+## Verification\n+\n+After all constraints are fully propoagated, we do a \"verification\"\n+step where we walk over the verify bounds and check that they are\n+satisfied. These bounds represent the \"maximal\" values that a region\n+variable can take on, basically.\n+\n+## The Region Hierarchy\n+\n+### Without closures\n+\n+Let's first consider the region hierarchy without thinking about\n+closures, because they add a lot of complications. The region\n+hierarchy *basically* mirrors the lexical structure of the code.\n+There is a region for every piece of 'evaluation' that occurs, meaning\n+every expression, block, and pattern (patterns are considered to\n+\"execute\" by testing the value they are applied to and creating any\n+relevant bindings).  So, for example:\n+\n+```rust\n+fn foo(x: isize, y: isize) { // -+\n+//  +------------+           //  |\n+//  |      +-----+           //  |\n+//  |  +-+ +-+ +-+           //  |\n+//  |  | | | | | |           //  |\n+//  v  v v v v v v           //  |\n+    let z = x + y;           //  |\n+    ...                      //  |\n+}                            // -+\n+\n+fn bar() { ... }\n+```\n+\n+In this example, there is a region for the fn body block as a whole,\n+and then a subregion for the declaration of the local variable.\n+Within that, there are sublifetimes for the assignment pattern and\n+also the expression `x + y`. The expression itself has sublifetimes\n+for evaluating `x` and `y`.\n+\n+#s## Function calls\n+\n+Function calls are a bit tricky. I will describe how we handle them\n+*now* and then a bit about how we can improve them (Issue #6268).\n+\n+Consider a function call like `func(expr1, expr2)`, where `func`,\n+`arg1`, and `arg2` are all arbitrary expressions. Currently,\n+we construct a region hierarchy like:\n+\n+    +----------------+\n+    |                |\n+    +--+ +---+  +---+|\n+    v  v v   v  v   vv\n+    func(expr1, expr2)\n+\n+Here you can see that the call as a whole has a region and the\n+function plus arguments are subregions of that. As a side-effect of\n+this, we get a lot of spurious errors around nested calls, in\n+particular when combined with `&mut` functions. For example, a call\n+like this one\n+\n+```rust\n+self.foo(self.bar())\n+```\n+\n+where both `foo` and `bar` are `&mut self` functions will always yield\n+an error.\n+\n+Here is a more involved example (which is safe) so we can see what's\n+going on:\n+\n+```rust\n+struct Foo { f: usize, g: usize }\n+// ...\n+fn add(p: &mut usize, v: usize) {\n+    *p += v;\n+}\n+// ...\n+fn inc(p: &mut usize) -> usize {\n+    *p += 1; *p\n+}\n+fn weird() {\n+    let mut x: Box<Foo> = box Foo { /* ... */ };\n+    'a: add(&mut (*x).f,\n+            'b: inc(&mut (*x).f)) // (..)\n+}\n+```\n+\n+The important part is the line marked `(..)` which contains a call to\n+`add()`. The first argument is a mutable borrow of the field `f`.  The\n+second argument also borrows the field `f`. Now, in the current borrow\n+checker, the first borrow is given the lifetime of the call to\n+`add()`, `'a`.  The second borrow is given the lifetime of `'b` of the\n+call to `inc()`. Because `'b` is considered to be a sublifetime of\n+`'a`, an error is reported since there are two co-existing mutable\n+borrows of the same data.\n+\n+However, if we were to examine the lifetimes a bit more carefully, we\n+can see that this error is unnecessary. Let's examine the lifetimes\n+involved with `'a` in detail. We'll break apart all the steps involved\n+in a call expression:\n+\n+```rust\n+'a: {\n+    'a_arg1: let a_temp1: ... = add;\n+    'a_arg2: let a_temp2: &'a mut usize = &'a mut (*x).f;\n+    'a_arg3: let a_temp3: usize = {\n+        let b_temp1: ... = inc;\n+        let b_temp2: &'b = &'b mut (*x).f;\n+        'b_call: b_temp1(b_temp2)\n+    };\n+    'a_call: a_temp1(a_temp2, a_temp3) // (**)\n+}\n+```\n+\n+Here we see that the lifetime `'a` includes a number of substatements.\n+In particular, there is this lifetime I've called `'a_call` that\n+corresponds to the *actual execution of the function `add()`*, after\n+all arguments have been evaluated. There is a corresponding lifetime\n+`'b_call` for the execution of `inc()`. If we wanted to be precise\n+about it, the lifetime of the two borrows should be `'a_call` and\n+`'b_call` respectively, since the references that were created\n+will not be dereferenced except during the execution itself.\n+\n+However, this model by itself is not sound. The reason is that\n+while the two references that are created will never be used\n+simultaneously, it is still true that the first reference is\n+*created* before the second argument is evaluated, and so even though\n+it will not be *dereferenced* during the evaluation of the second\n+argument, it can still be *invalidated* by that evaluation. Consider\n+this similar but unsound example:\n+\n+```rust\n+struct Foo { f: usize, g: usize }\n+// ...\n+fn add(p: &mut usize, v: usize) {\n+    *p += v;\n+}\n+// ...\n+fn consume(x: Box<Foo>) -> usize {\n+    x.f + x.g\n+}\n+fn weird() {\n+    let mut x: Box<Foo> = box Foo { ... };\n+    'a: add(&mut (*x).f, consume(x)) // (..)\n+}\n+```\n+\n+In this case, the second argument to `add` actually consumes `x`, thus\n+invalidating the first argument.\n+\n+So, for now, we exclude the `call` lifetimes from our model.\n+Eventually I would like to include them, but we will have to make the\n+borrow checker handle this situation correctly. In particular, if\n+there is a reference created whose lifetime does not enclose\n+the borrow expression, we must issue sufficient restrictions to ensure\n+that the pointee remains valid.\n+\n+### Modeling closures\n+\n+Integrating closures properly into the model is a bit of\n+work-in-progress. In an ideal world, we would model closures as\n+closely as possible after their desugared equivalents. That is, a\n+closure type would be modeled as a struct, and the region hierarchy of\n+different closure bodies would be completely distinct from all other\n+fns. We are generally moving in that direction but there are\n+complications in terms of the implementation.\n+\n+In practice what we currently do is somewhat different. The basis for\n+the current approach is the observation that the only time that\n+regions from distinct fn bodies interact with one another is through\n+an upvar or the type of a fn parameter (since closures live in the fn\n+body namespace, they can in fact have fn parameters whose types\n+include regions from the surrounding fn body). For these cases, there\n+are separate mechanisms which ensure that the regions that appear in\n+upvars/parameters outlive the dynamic extent of each call to the\n+closure:\n+\n+1. Types must outlive the region of any expression where they are used.\n+   For a closure type `C` to outlive a region `'r`, that implies that the\n+   types of all its upvars must outlive `'r`.\n+2. Parameters must outlive the region of any fn that they are passed to.\n+\n+Therefore, we can -- sort of -- assume that any region from an\n+enclosing fns is larger than any region from one of its enclosed\n+fn. And that is precisely what we do: when building the region\n+hierarchy, each region lives in its own distinct subtree, but if we\n+are asked to compute the `LUB(r1, r2)` of two regions, and those\n+regions are in disjoint subtrees, we compare the lexical nesting of\n+the two regions.\n+\n+*Ideas for improving the situation:* (FIXME #3696) The correctness\n+argument here is subtle and a bit hand-wavy. The ideal, as stated\n+earlier, would be to model things in such a way that it corresponds\n+more closely to the desugared code. The best approach for doing this\n+is a bit unclear: it may in fact be possible to *actually* desugar\n+before we start, but I don't think so. The main option that I've been\n+thinking through is imposing a \"view shift\" as we enter the fn body,\n+so that regions appearing in the types of fn parameters and upvars are\n+translated from being regions in the outer fn into free region\n+parameters, just as they would be if we applied the desugaring. The\n+challenge here is that type inference may not have fully run, so the\n+types may not be fully known: we could probably do this translation\n+lazilly, as type variables are instantiated. We would also have to\n+apply a kind of inverse translation to the return value. This would be\n+a good idea anyway, as right now it is possible for free regions\n+instantiated within the closure to leak into the parent: this\n+currently leads to type errors, since those regions cannot outlive any\n+expressions within the parent hierarchy. Much like the current\n+handling of closures, there are no known cases where this leads to a\n+type-checking accepting incorrect code (though it sometimes rejects\n+what might be considered correct code; see rust-lang/rust#22557), but\n+it still doesn't feel like the right approach."}, {"sha": "67ad08c753033554a18327bf3062001250ae5612", "filename": "src/librustc/infer/region_constraints/README.md", "status": "modified", "additions": 7, "deletions": 258, "changes": 265, "blob_url": "https://github.com/rust-lang/rust/blob/b383ab79c96e4f3d38fcc536ac35cafcabfdd0c5/src%2Flibrustc%2Finfer%2Fregion_constraints%2FREADME.md", "raw_url": "https://github.com/rust-lang/rust/raw/b383ab79c96e4f3d38fcc536ac35cafcabfdd0c5/src%2Flibrustc%2Finfer%2Fregion_constraints%2FREADME.md", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Finfer%2Fregion_constraints%2FREADME.md?ref=b383ab79c96e4f3d38fcc536ac35cafcabfdd0c5", "patch": "@@ -1,22 +1,13 @@\n-Region inference\n+# Region constraint collection\n \n-# Terminology\n+## Terminology\n \n Note that we use the terms region and lifetime interchangeably.\n \n-# Introduction\n+## Introduction\n \n-Region inference uses a somewhat more involved algorithm than type\n-inference. It is not the most efficient thing ever written though it\n-seems to work well enough in practice (famous last words).  The reason\n-that we use a different algorithm is because, unlike with types, it is\n-impractical to hand-annotate with regions (in some cases, there aren't\n-even the requisite syntactic forms).  So we have to get it right, and\n-it's worth spending more time on a more involved analysis.  Moreover,\n-regions are a simpler case than types: they don't have aggregate\n-structure, for example.\n-\n-Unlike normal type inference, which is similar in spirit to H-M and thus\n+As described in the [inference README](../README.md), and unlike\n+normal type inference, which is similar in spirit to H-M and thus\n works progressively, the region type inference works by accumulating\n constraints over the course of a function.  Finally, at the end of\n processing a function, we process and solve the constraints all at\n@@ -50,7 +41,7 @@ influence inference proper. These take the form of:\n   (say, from where clauses), then we can conclude that `T: 'a` if `'b:\n   'a` *or* `'c: 'a`.\n \n-# Building up the constraints\n+## Building up the constraints\n \n Variables and constraints are created using the following methods:\n \n@@ -73,249 +64,7 @@ Alternatively, you can call `commit()` which ends all snapshots.\n Snapshots can be recursive---so you can start a snapshot when another\n is in progress, but only the root snapshot can \"commit\".\n \n-## The problem\n-\n-Basically our input is a directed graph where nodes can be divided\n-into two categories: region variables and concrete regions.  Each edge\n-`R -> S` in the graph represents a constraint that the region `R` is a\n-subregion of the region `S`.\n-\n-Region variable nodes can have arbitrary degree.  There is one region\n-variable node per region variable.\n-\n-Each concrete region node is associated with some, well, concrete\n-region: e.g., a free lifetime, or the region for a particular scope.\n-Note that there may be more than one concrete region node for a\n-particular region value.  Moreover, because of how the graph is built,\n-we know that all concrete region nodes have either in-degree 1 or\n-out-degree 1.\n-\n-Before resolution begins, we build up the constraints in a hashmap\n-that maps `Constraint` keys to spans.  During resolution, we construct\n-the actual `Graph` structure that we describe here.\n-\n-## Computing the values for region variables\n-\n-The algorithm is a simple dataflow algorithm. Each region variable\n-begins as empty. We iterate over the constraints, and for each constraint\n-we grow the relevant region variable to be as big as it must be to meet all the\n-constraints. This means the region variables can grow to be `'static` if\n-necessary.\n-\n-## Verification\n-\n-After all constraints are fully propoagated, we do a \"verification\"\n-step where we walk over the verify bounds and check that they are\n-satisfied. These bounds represent the \"maximal\" values that a region\n-variable can take on, basically.\n-\n-# The Region Hierarchy\n-\n-## Without closures\n-\n-Let's first consider the region hierarchy without thinking about\n-closures, because they add a lot of complications. The region\n-hierarchy *basically* mirrors the lexical structure of the code.\n-There is a region for every piece of 'evaluation' that occurs, meaning\n-every expression, block, and pattern (patterns are considered to\n-\"execute\" by testing the value they are applied to and creating any\n-relevant bindings).  So, for example:\n-\n-```rust\n-fn foo(x: isize, y: isize) { // -+\n-//  +------------+           //  |\n-//  |      +-----+           //  |\n-//  |  +-+ +-+ +-+           //  |\n-//  |  | | | | | |           //  |\n-//  v  v v v v v v           //  |\n-    let z = x + y;           //  |\n-    ...                      //  |\n-}                            // -+\n-\n-fn bar() { ... }\n-```\n-\n-In this example, there is a region for the fn body block as a whole,\n-and then a subregion for the declaration of the local variable.\n-Within that, there are sublifetimes for the assignment pattern and\n-also the expression `x + y`. The expression itself has sublifetimes\n-for evaluating `x` and `y`.\n-\n-## Function calls\n-\n-Function calls are a bit tricky. I will describe how we handle them\n-*now* and then a bit about how we can improve them (Issue #6268).\n-\n-Consider a function call like `func(expr1, expr2)`, where `func`,\n-`arg1`, and `arg2` are all arbitrary expressions. Currently,\n-we construct a region hierarchy like:\n-\n-    +----------------+\n-    |                |\n-    +--+ +---+  +---+|\n-    v  v v   v  v   vv\n-    func(expr1, expr2)\n-\n-Here you can see that the call as a whole has a region and the\n-function plus arguments are subregions of that. As a side-effect of\n-this, we get a lot of spurious errors around nested calls, in\n-particular when combined with `&mut` functions. For example, a call\n-like this one\n-\n-```rust\n-self.foo(self.bar())\n-```\n-\n-where both `foo` and `bar` are `&mut self` functions will always yield\n-an error.\n-\n-Here is a more involved example (which is safe) so we can see what's\n-going on:\n-\n-```rust\n-struct Foo { f: usize, g: usize }\n-// ...\n-fn add(p: &mut usize, v: usize) {\n-    *p += v;\n-}\n-// ...\n-fn inc(p: &mut usize) -> usize {\n-    *p += 1; *p\n-}\n-fn weird() {\n-    let mut x: Box<Foo> = box Foo { /* ... */ };\n-    'a: add(&mut (*x).f,\n-            'b: inc(&mut (*x).f)) // (..)\n-}\n-```\n-\n-The important part is the line marked `(..)` which contains a call to\n-`add()`. The first argument is a mutable borrow of the field `f`.  The\n-second argument also borrows the field `f`. Now, in the current borrow\n-checker, the first borrow is given the lifetime of the call to\n-`add()`, `'a`.  The second borrow is given the lifetime of `'b` of the\n-call to `inc()`. Because `'b` is considered to be a sublifetime of\n-`'a`, an error is reported since there are two co-existing mutable\n-borrows of the same data.\n-\n-However, if we were to examine the lifetimes a bit more carefully, we\n-can see that this error is unnecessary. Let's examine the lifetimes\n-involved with `'a` in detail. We'll break apart all the steps involved\n-in a call expression:\n-\n-```rust\n-'a: {\n-    'a_arg1: let a_temp1: ... = add;\n-    'a_arg2: let a_temp2: &'a mut usize = &'a mut (*x).f;\n-    'a_arg3: let a_temp3: usize = {\n-        let b_temp1: ... = inc;\n-        let b_temp2: &'b = &'b mut (*x).f;\n-        'b_call: b_temp1(b_temp2)\n-    };\n-    'a_call: a_temp1(a_temp2, a_temp3) // (**)\n-}\n-```\n-\n-Here we see that the lifetime `'a` includes a number of substatements.\n-In particular, there is this lifetime I've called `'a_call` that\n-corresponds to the *actual execution of the function `add()`*, after\n-all arguments have been evaluated. There is a corresponding lifetime\n-`'b_call` for the execution of `inc()`. If we wanted to be precise\n-about it, the lifetime of the two borrows should be `'a_call` and\n-`'b_call` respectively, since the references that were created\n-will not be dereferenced except during the execution itself.\n-\n-However, this model by itself is not sound. The reason is that\n-while the two references that are created will never be used\n-simultaneously, it is still true that the first reference is\n-*created* before the second argument is evaluated, and so even though\n-it will not be *dereferenced* during the evaluation of the second\n-argument, it can still be *invalidated* by that evaluation. Consider\n-this similar but unsound example:\n-\n-```rust\n-struct Foo { f: usize, g: usize }\n-// ...\n-fn add(p: &mut usize, v: usize) {\n-    *p += v;\n-}\n-// ...\n-fn consume(x: Box<Foo>) -> usize {\n-    x.f + x.g\n-}\n-fn weird() {\n-    let mut x: Box<Foo> = box Foo { ... };\n-    'a: add(&mut (*x).f, consume(x)) // (..)\n-}\n-```\n-\n-In this case, the second argument to `add` actually consumes `x`, thus\n-invalidating the first argument.\n-\n-So, for now, we exclude the `call` lifetimes from our model.\n-Eventually I would like to include them, but we will have to make the\n-borrow checker handle this situation correctly. In particular, if\n-there is a reference created whose lifetime does not enclose\n-the borrow expression, we must issue sufficient restrictions to ensure\n-that the pointee remains valid.\n-\n-## Modeling closures\n-\n-Integrating closures properly into the model is a bit of\n-work-in-progress. In an ideal world, we would model closures as\n-closely as possible after their desugared equivalents. That is, a\n-closure type would be modeled as a struct, and the region hierarchy of\n-different closure bodies would be completely distinct from all other\n-fns. We are generally moving in that direction but there are\n-complications in terms of the implementation.\n-\n-In practice what we currently do is somewhat different. The basis for\n-the current approach is the observation that the only time that\n-regions from distinct fn bodies interact with one another is through\n-an upvar or the type of a fn parameter (since closures live in the fn\n-body namespace, they can in fact have fn parameters whose types\n-include regions from the surrounding fn body). For these cases, there\n-are separate mechanisms which ensure that the regions that appear in\n-upvars/parameters outlive the dynamic extent of each call to the\n-closure:\n-\n-1. Types must outlive the region of any expression where they are used.\n-   For a closure type `C` to outlive a region `'r`, that implies that the\n-   types of all its upvars must outlive `'r`.\n-2. Parameters must outlive the region of any fn that they are passed to.\n-\n-Therefore, we can -- sort of -- assume that any region from an\n-enclosing fns is larger than any region from one of its enclosed\n-fn. And that is precisely what we do: when building the region\n-hierarchy, each region lives in its own distinct subtree, but if we\n-are asked to compute the `LUB(r1, r2)` of two regions, and those\n-regions are in disjoint subtrees, we compare the lexical nesting of\n-the two regions.\n-\n-*Ideas for improving the situation:* (FIXME #3696) The correctness\n-argument here is subtle and a bit hand-wavy. The ideal, as stated\n-earlier, would be to model things in such a way that it corresponds\n-more closely to the desugared code. The best approach for doing this\n-is a bit unclear: it may in fact be possible to *actually* desugar\n-before we start, but I don't think so. The main option that I've been\n-thinking through is imposing a \"view shift\" as we enter the fn body,\n-so that regions appearing in the types of fn parameters and upvars are\n-translated from being regions in the outer fn into free region\n-parameters, just as they would be if we applied the desugaring. The\n-challenge here is that type inference may not have fully run, so the\n-types may not be fully known: we could probably do this translation\n-lazilly, as type variables are instantiated. We would also have to\n-apply a kind of inverse translation to the return value. This would be\n-a good idea anyway, as right now it is possible for free regions\n-instantiated within the closure to leak into the parent: this\n-currently leads to type errors, since those regions cannot outlive any\n-expressions within the parent hierarchy. Much like the current\n-handling of closures, there are no known cases where this leads to a\n-type-checking accepting incorrect code (though it sometimes rejects\n-what might be considered correct code; see rust-lang/rust#22557), but\n-it still doesn't feel like the right approach.\n-\n-### Skolemization\n+## Skolemization\n \n For a discussion on skolemization and higher-ranked subtyping, please\n see the module `middle::infer::higher_ranked::doc`."}]}