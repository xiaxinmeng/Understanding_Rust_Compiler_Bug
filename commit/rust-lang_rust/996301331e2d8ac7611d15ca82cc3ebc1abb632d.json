{"sha": "996301331e2d8ac7611d15ca82cc3ebc1abb632d", "node_id": "MDY6Q29tbWl0NzI0NzEyOjk5NjMwMTMzMWUyZDhhYzc2MTFkMTVjYTgyY2MzZWJjMWFiYjYzMmQ=", "commit": {"author": {"name": "bors", "email": "bors@rust-lang.org", "date": "2013-06-13T03:25:17Z"}, "committer": {"name": "bors", "email": "bors@rust-lang.org", "date": "2013-06-13T03:25:17Z"}, "message": "auto merge of #7079 : thestinger/rust/jemalloc, r=graydon\n\nMinor release, the ChangeLog is included in the update.", "tree": {"sha": "032c91b56939850145b16ede2a5bfa35e4620f92", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/032c91b56939850145b16ede2a5bfa35e4620f92"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/996301331e2d8ac7611d15ca82cc3ebc1abb632d", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/996301331e2d8ac7611d15ca82cc3ebc1abb632d", "html_url": "https://github.com/rust-lang/rust/commit/996301331e2d8ac7611d15ca82cc3ebc1abb632d", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/996301331e2d8ac7611d15ca82cc3ebc1abb632d/comments", "author": {"login": "bors", "id": 3372342, "node_id": "MDQ6VXNlcjMzNzIzNDI=", "avatar_url": "https://avatars.githubusercontent.com/u/3372342?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bors", "html_url": "https://github.com/bors", "followers_url": "https://api.github.com/users/bors/followers", "following_url": "https://api.github.com/users/bors/following{/other_user}", "gists_url": "https://api.github.com/users/bors/gists{/gist_id}", "starred_url": "https://api.github.com/users/bors/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bors/subscriptions", "organizations_url": "https://api.github.com/users/bors/orgs", "repos_url": "https://api.github.com/users/bors/repos", "events_url": "https://api.github.com/users/bors/events{/privacy}", "received_events_url": "https://api.github.com/users/bors/received_events", "type": "User", "site_admin": false}, "committer": {"login": "bors", "id": 3372342, "node_id": "MDQ6VXNlcjMzNzIzNDI=", "avatar_url": "https://avatars.githubusercontent.com/u/3372342?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bors", "html_url": "https://github.com/bors", "followers_url": "https://api.github.com/users/bors/followers", "following_url": "https://api.github.com/users/bors/following{/other_user}", "gists_url": "https://api.github.com/users/bors/gists{/gist_id}", "starred_url": "https://api.github.com/users/bors/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bors/subscriptions", "organizations_url": "https://api.github.com/users/bors/orgs", "repos_url": "https://api.github.com/users/bors/repos", "events_url": "https://api.github.com/users/bors/events{/privacy}", "received_events_url": "https://api.github.com/users/bors/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "6c45160f19cff60e8956491eabe9e4704735c0eb", "url": "https://api.github.com/repos/rust-lang/rust/commits/6c45160f19cff60e8956491eabe9e4704735c0eb", "html_url": "https://github.com/rust-lang/rust/commit/6c45160f19cff60e8956491eabe9e4704735c0eb"}, {"sha": "0685c657f0b29f6372bd4b031753ff92fc3b53ba", "url": "https://api.github.com/repos/rust-lang/rust/commits/0685c657f0b29f6372bd4b031753ff92fc3b53ba", "html_url": "https://github.com/rust-lang/rust/commit/0685c657f0b29f6372bd4b031753ff92fc3b53ba"}], "stats": {"total": 5618, "additions": 2833, "deletions": 2785}, "files": [{"sha": "8ab884875b0d67d610c61d3b5a006f62fbbe53b2", "filename": "src/rt/jemalloc/ChangeLog", "status": "modified", "additions": 14, "deletions": 1, "changes": 15, "blob_url": "https://github.com/rust-lang/rust/blob/996301331e2d8ac7611d15ca82cc3ebc1abb632d/src%2Frt%2Fjemalloc%2FChangeLog", "raw_url": "https://github.com/rust-lang/rust/raw/996301331e2d8ac7611d15ca82cc3ebc1abb632d/src%2Frt%2Fjemalloc%2FChangeLog", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Frt%2Fjemalloc%2FChangeLog?ref=996301331e2d8ac7611d15ca82cc3ebc1abb632d", "patch": "@@ -6,6 +6,19 @@ found in the git revision history:\n     http://www.canonware.com/cgi-bin/gitweb.cgi?p=jemalloc.git\n     git://canonware.com/jemalloc.git\n \n+* 3.4.0 (June 2, 2013)\n+\n+  This version is essentially a small bugfix release, but the addition of\n+  aarch64 support requires that the minor version be incremented.\n+\n+  Bug fixes:\n+  - Fix race-triggered deadlocks in chunk_record().  These deadlocks were\n+    typically triggered by multiple threads concurrently deallocating huge\n+    objects.\n+\n+  New features:\n+  - Add support for the aarch64 architecture.\n+\n * 3.3.1 (March 6, 2013)\n \n   This version fixes bugs that are typically encountered only when utilizing\n@@ -15,7 +28,7 @@ found in the git revision history:\n   - Fix a locking order bug that could cause deadlock during fork if heap\n     profiling were enabled.\n   - Fix a chunk recycling bug that could cause the allocator to lose track of\n-    whether a chunk was zeroed.   On FreeBSD, NetBSD, and OS X, it could cause\n+    whether a chunk was zeroed.  On FreeBSD, NetBSD, and OS X, it could cause\n     corruption if allocating via sbrk(2) (unlikely unless running with the\n     \"dss:primary\" option specified).  This was completely harmless on Linux\n     unless using mlockall(2) (and unlikely even then, unless the"}, {"sha": "84c9c557ac9c774c374901efc86bdf8cef9b814a", "filename": "src/rt/jemalloc/VERSION", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/996301331e2d8ac7611d15ca82cc3ebc1abb632d/src%2Frt%2Fjemalloc%2FVERSION", "raw_url": "https://github.com/rust-lang/rust/raw/996301331e2d8ac7611d15ca82cc3ebc1abb632d/src%2Frt%2Fjemalloc%2FVERSION", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Frt%2Fjemalloc%2FVERSION?ref=996301331e2d8ac7611d15ca82cc3ebc1abb632d", "patch": "@@ -1 +1 @@\n-3.3.1-0-g9ef9d9e8c271cdf14f664b871a8f98c827714784\n+3.4.0-0-g0ed518e5dab789ad2171bb38977a8927e2a26775"}, {"sha": "727eb43704f8c90222723986330c17b4545e4920", "filename": "src/rt/jemalloc/bin/pprof", "status": "modified", "additions": 11, "deletions": 11, "changes": 22, "blob_url": "https://github.com/rust-lang/rust/blob/996301331e2d8ac7611d15ca82cc3ebc1abb632d/src%2Frt%2Fjemalloc%2Fbin%2Fpprof", "raw_url": "https://github.com/rust-lang/rust/raw/996301331e2d8ac7611d15ca82cc3ebc1abb632d/src%2Frt%2Fjemalloc%2Fbin%2Fpprof", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Frt%2Fjemalloc%2Fbin%2Fpprof?ref=996301331e2d8ac7611d15ca82cc3ebc1abb632d", "patch": "@@ -2,11 +2,11 @@\n \n # Copyright (c) 1998-2007, Google Inc.\n # All rights reserved.\n-#\n+# \n # Redistribution and use in source and binary forms, with or without\n # modification, are permitted provided that the following conditions are\n # met:\n-#\n+# \n #     * Redistributions of source code must retain the above copyright\n # notice, this list of conditions and the following disclaimer.\n #     * Redistributions in binary form must reproduce the above\n@@ -16,7 +16,7 @@\n #     * Neither the name of Google Inc. nor the names of its\n # contributors may be used to endorse or promote products derived from\n # this software without specific prior written permission.\n-#\n+# \n # THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n # \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n # LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\n@@ -1683,23 +1683,23 @@ sub PrintSource {\n                         HtmlPrintNumber($c2),\n                         UnparseAddress($offset, $e->[0]),\n                         CleanDisassembly($e->[3]));\n-\n+      \n       # Append the most specific source line associated with this instruction\n       if (length($dis) < 80) { $dis .= (' ' x (80 - length($dis))) };\n       $dis = HtmlEscape($dis);\n       my $f = $e->[5];\n       my $l = $e->[6];\n       if ($f ne $last_dis_filename) {\n-        $dis .= sprintf(\"<span class=disasmloc>%s:%d</span>\",\n+        $dis .= sprintf(\"<span class=disasmloc>%s:%d</span>\", \n                         HtmlEscape(CleanFileName($f)), $l);\n       } elsif ($l ne $last_dis_linenum) {\n         # De-emphasize the unchanged file name portion\n         $dis .= sprintf(\"<span class=unimportant>%s</span>\" .\n-                        \"<span class=disasmloc>:%d</span>\",\n+                        \"<span class=disasmloc>:%d</span>\", \n                         HtmlEscape(CleanFileName($f)), $l);\n       } else {\n         # De-emphasize the entire location\n-        $dis .= sprintf(\"<span class=unimportant>%s:%d</span>\",\n+        $dis .= sprintf(\"<span class=unimportant>%s:%d</span>\", \n                         HtmlEscape(CleanFileName($f)), $l);\n       }\n       $last_dis_filename = $f;\n@@ -1788,8 +1788,8 @@ sub PrintSource {\n         if (defined($dis) && $dis ne '') {\n           $asm = \"<span class=\\\"asm\\\">\" . $dis . \"</span>\";\n         }\n-        my $source_class = (($n1 + $n2 > 0)\n-                            ? \"livesrc\"\n+        my $source_class = (($n1 + $n2 > 0) \n+                            ? \"livesrc\" \n                             : (($asm ne \"\") ? \"deadsrc\" : \"nop\"));\n         printf $output (\n           \"<span class=\\\"line\\\">%5d</span> \" .\n@@ -4723,7 +4723,7 @@ sub MapToSymbols {\n \t}\n       }\n     }\n-\n+    \n     # Prepend to accumulated symbols for pcstr\n     # (so that caller comes before callee)\n     my $sym = $symbols->{$pcstr};\n@@ -4917,7 +4917,7 @@ sub ConfigureTool {\n     my $dirname = $`;    # this is everything up to and including the last slash\n     if (-x \"$dirname$tool\") {\n       $path = \"$dirname$tool\";\n-    } else {\n+    } else { \n       $path = $tool;\n     }\n   }"}, {"sha": "882d3b3f3b02a27a053c623426af7252338fce2d", "filename": "src/rt/jemalloc/configure.ac", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "blob_url": "https://github.com/rust-lang/rust/blob/996301331e2d8ac7611d15ca82cc3ebc1abb632d/src%2Frt%2Fjemalloc%2Fconfigure.ac", "raw_url": "https://github.com/rust-lang/rust/raw/996301331e2d8ac7611d15ca82cc3ebc1abb632d/src%2Frt%2Fjemalloc%2Fconfigure.ac", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Frt%2Fjemalloc%2Fconfigure.ac?ref=996301331e2d8ac7611d15ca82cc3ebc1abb632d", "patch": "@@ -26,7 +26,7 @@ AC_COMPILE_IFELSE([AC_LANG_PROGRAM(\n ])\n \n dnl JE_COMPILABLE(label, hcode, mcode, rvar)\n-dnl\n+dnl \n dnl Use AC_LINK_IFELSE() rather than AC_COMPILE_IFELSE() so that linker errors\n dnl cause failure.\n AC_DEFUN([JE_COMPILABLE],\n@@ -232,7 +232,7 @@ CC_MM=1\n dnl Platform-specific settings.  abi and RPATH can probably be determined\n dnl programmatically, but doing so is error-prone, which makes it generally\n dnl not worth the trouble.\n-dnl\n+dnl \n dnl Define cpp macros in CPPFLAGS, rather than doing AC_DEFINE(macro), since the\n dnl definitions need to be seen before any headers are included, which is a pain\n dnl to make happen otherwise.\n@@ -965,7 +965,7 @@ fi\n \n dnl ============================================================================\n dnl jemalloc configuration.\n-dnl\n+dnl \n \n dnl Set VERSION if source directory has an embedded git repository.\n if test -d \"${srcroot}.git\" ; then"}, {"sha": "d0e0a23bab3286688d47c4194dd50f83cc88b149", "filename": "src/rt/jemalloc/doc/jemalloc.3", "status": "modified", "additions": 17, "deletions": 5, "changes": 22, "blob_url": "https://github.com/rust-lang/rust/blob/996301331e2d8ac7611d15ca82cc3ebc1abb632d/src%2Frt%2Fjemalloc%2Fdoc%2Fjemalloc.3", "raw_url": "https://github.com/rust-lang/rust/raw/996301331e2d8ac7611d15ca82cc3ebc1abb632d/src%2Frt%2Fjemalloc%2Fdoc%2Fjemalloc.3", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Frt%2Fjemalloc%2Fdoc%2Fjemalloc.3?ref=996301331e2d8ac7611d15ca82cc3ebc1abb632d", "patch": "@@ -2,12 +2,12 @@\n .\\\"     Title: JEMALLOC\n .\\\"    Author: Jason Evans\n .\\\" Generator: DocBook XSL Stylesheets v1.76.1 <http://docbook.sf.net/>\n-.\\\"      Date: 03/06/2013\n+.\\\"      Date: 06/02/2013\n .\\\"    Manual: User Manual\n-.\\\"    Source: jemalloc 3.3.1-0-g9ef9d9e8c271cdf14f664b871a8f98c827714784\n+.\\\"    Source: jemalloc 3.4.0-0-g0ed518e5dab789ad2171bb38977a8927e2a26775\n .\\\"  Language: English\n .\\\"\n-.TH \"JEMALLOC\" \"3\" \"03/06/2013\" \"jemalloc 3.3.1-0-g9ef9d9e8c271\" \"User Manual\"\n+.TH \"JEMALLOC\" \"3\" \"06/02/2013\" \"jemalloc 3.4.0-0-g0ed518e5dab7\" \"User Manual\"\n .\\\" -----------------------------------------------------------------\n .\\\" * Define some portability stuff\n .\\\" -----------------------------------------------------------------\n@@ -31,7 +31,7 @@\n jemalloc \\- general purpose memory allocation functions\n .SH \"LIBRARY\"\n .PP\n-This manual describes jemalloc 3\\&.3\\&.1\\-0\\-g9ef9d9e8c271cdf14f664b871a8f98c827714784\\&. More information can be found at the\n+This manual describes jemalloc 3\\&.4\\&.0\\-0\\-g0ed518e5dab789ad2171bb38977a8927e2a26775\\&. More information can be found at the\n \\m[blue]\\fBjemalloc website\\fR\\m[]\\&\\s-2\\u[1]\\d\\s+2\\&.\n .SH \"SYNOPSIS\"\n .sp\n@@ -376,7 +376,19 @@ Once, when the first call is made to one of the memory allocation routines, the\n The string pointed to by the global variable\n \\fImalloc_conf\\fR, the \\(lqname\\(rq of the file referenced by the symbolic link named\n /etc/malloc\\&.conf, and the value of the environment variable\n-\\fBMALLOC_CONF\\fR, will be interpreted, in that order, from left to right as options\\&.\n+\\fBMALLOC_CONF\\fR, will be interpreted, in that order, from left to right as options\\&. Note that\n+\\fImalloc_conf\\fR\n+may be read before\n+\\fBmain\\fR\\fB\\fR\n+is entered, so the declaration of\n+\\fImalloc_conf\\fR\n+should specify an initializer that contains the final value to be read by jemalloc\\&.\n+\\fImalloc_conf\\fR\n+is a compile\\-time setting, whereas\n+/etc/malloc\\&.conf\n+and\n+\\fBMALLOC_CONF\\fR\n+can be safely set any time prior to program invocation\\&.\n .PP\n An options string is a comma\\-separated list of option:value pairs\\&. There is one key corresponding to each\n \"opt\\&.*\""}, {"sha": "abd5e6fcd0f29eff32a74c4697087ca8d27ede19", "filename": "src/rt/jemalloc/doc/jemalloc.xml.in", "status": "modified", "additions": 8, "deletions": 1, "changes": 9, "blob_url": "https://github.com/rust-lang/rust/blob/996301331e2d8ac7611d15ca82cc3ebc1abb632d/src%2Frt%2Fjemalloc%2Fdoc%2Fjemalloc.xml.in", "raw_url": "https://github.com/rust-lang/rust/raw/996301331e2d8ac7611d15ca82cc3ebc1abb632d/src%2Frt%2Fjemalloc%2Fdoc%2Fjemalloc.xml.in", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Frt%2Fjemalloc%2Fdoc%2Fjemalloc.xml.in?ref=996301331e2d8ac7611d15ca82cc3ebc1abb632d", "patch": "@@ -432,7 +432,14 @@ for (i = 0; i < nbins; i++) {\n     referenced by the symbolic link named <filename\n     class=\"symlink\">/etc/malloc.conf</filename>, and the value of the\n     environment variable <envar>MALLOC_CONF</envar>, will be interpreted, in\n-    that order, from left to right as options.</para>\n+    that order, from left to right as options.  Note that\n+    <varname>malloc_conf</varname> may be read before\n+    <function>main<parameter/></function> is entered, so the declaration of\n+    <varname>malloc_conf</varname> should specify an initializer that contains\n+    the final value to be read by jemalloc.  <varname>malloc_conf</varname> is\n+    a compile-time setting, whereas <filename\n+    class=\"symlink\">/etc/malloc.conf</filename> and <envar>MALLOC_CONF</envar>\n+    can be safely set any time prior to program invocation.</para>\n \n     <para>An options string is a comma-separated list of option:value pairs.\n     There is one key corresponding to each <link"}, {"sha": "f2c18f43543d2c26cb88214c0f2195b0c6a6fe54", "filename": "src/rt/jemalloc/include/jemalloc/internal/arena.h", "status": "modified", "additions": 598, "deletions": 598, "changes": 1196, "blob_url": "https://github.com/rust-lang/rust/blob/996301331e2d8ac7611d15ca82cc3ebc1abb632d/src%2Frt%2Fjemalloc%2Finclude%2Fjemalloc%2Finternal%2Farena.h", "raw_url": "https://github.com/rust-lang/rust/raw/996301331e2d8ac7611d15ca82cc3ebc1abb632d/src%2Frt%2Fjemalloc%2Finclude%2Fjemalloc%2Finternal%2Farena.h", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Frt%2Fjemalloc%2Finclude%2Fjemalloc%2Finternal%2Farena.h?ref=996301331e2d8ac7611d15ca82cc3ebc1abb632d", "patch": "@@ -57,93 +57,93 @@ typedef struct arena_s arena_t;\n /* Each element of the chunk map corresponds to one page within the chunk. */\n struct arena_chunk_map_s {\n #ifndef JEMALLOC_PROF\n-    /*\n-     * Overlay prof_ctx in order to allow it to be referenced by dead code.\n-     * Such antics aren't warranted for per arena data structures, but\n-     * chunk map overhead accounts for a percentage of memory, rather than\n-     * being just a fixed cost.\n-     */\n-    union {\n+\t/*\n+\t * Overlay prof_ctx in order to allow it to be referenced by dead code.\n+\t * Such antics aren't warranted for per arena data structures, but\n+\t * chunk map overhead accounts for a percentage of memory, rather than\n+\t * being just a fixed cost.\n+\t */\n+\tunion {\n #endif\n-    union {\n-        /*\n-         * Linkage for run trees.  There are two disjoint uses:\n-         *\n-         * 1) arena_t's runs_avail tree.\n-         * 2) arena_run_t conceptually uses this linkage for in-use\n-         *    non-full runs, rather than directly embedding linkage.\n-         */\n-        rb_node(arena_chunk_map_t)\trb_link;\n-        /*\n-         * List of runs currently in purgatory.  arena_chunk_purge()\n-         * temporarily allocates runs that contain dirty pages while\n-         * purging, so that other threads cannot use the runs while the\n-         * purging thread is operating without the arena lock held.\n-         */\n-        ql_elm(arena_chunk_map_t)\tql_link;\n-    }\t\t\t\tu;\n-\n-    /* Profile counters, used for large object runs. */\n-    prof_ctx_t\t\t\t*prof_ctx;\n+\tunion {\n+\t\t/*\n+\t\t * Linkage for run trees.  There are two disjoint uses:\n+\t\t *\n+\t\t * 1) arena_t's runs_avail tree.\n+\t\t * 2) arena_run_t conceptually uses this linkage for in-use\n+\t\t *    non-full runs, rather than directly embedding linkage.\n+\t\t */\n+\t\trb_node(arena_chunk_map_t)\trb_link;\n+\t\t/*\n+\t\t * List of runs currently in purgatory.  arena_chunk_purge()\n+\t\t * temporarily allocates runs that contain dirty pages while\n+\t\t * purging, so that other threads cannot use the runs while the\n+\t\t * purging thread is operating without the arena lock held.\n+\t\t */\n+\t\tql_elm(arena_chunk_map_t)\tql_link;\n+\t}\t\t\t\tu;\n+\n+\t/* Profile counters, used for large object runs. */\n+\tprof_ctx_t\t\t\t*prof_ctx;\n #ifndef JEMALLOC_PROF\n-    }; /* union { ... }; */\n+\t}; /* union { ... }; */\n #endif\n \n-    /*\n-     * Run address (or size) and various flags are stored together.  The bit\n-     * layout looks like (assuming 32-bit system):\n-     *\n-     *   ???????? ???????? ????nnnn nnnndula\n-     *\n-     * ? : Unallocated: Run address for first/last pages, unset for internal\n-     *                  pages.\n-     *     Small: Run page offset.\n-     *     Large: Run size for first page, unset for trailing pages.\n-     * n : binind for small size class, BININD_INVALID for large size class.\n-     * d : dirty?\n-     * u : unzeroed?\n-     * l : large?\n-     * a : allocated?\n-     *\n-     * Following are example bit patterns for the three types of runs.\n-     *\n-     * p : run page offset\n-     * s : run size\n-     * n : binind for size class; large objects set these to BININD_INVALID\n-     *     except for promoted allocations (see prof_promote)\n-     * x : don't care\n-     * - : 0\n-     * + : 1\n-     * [DULA] : bit set\n-     * [dula] : bit unset\n-     *\n-     *   Unallocated (clean):\n-     *     ssssssss ssssssss ssss++++ ++++du-a\n-     *     xxxxxxxx xxxxxxxx xxxxxxxx xxxx-Uxx\n-     *     ssssssss ssssssss ssss++++ ++++dU-a\n-     *\n-     *   Unallocated (dirty):\n-     *     ssssssss ssssssss ssss++++ ++++D--a\n-     *     xxxxxxxx xxxxxxxx xxxxxxxx xxxxxxxx\n-     *     ssssssss ssssssss ssss++++ ++++D--a\n-     *\n-     *   Small:\n-     *     pppppppp pppppppp ppppnnnn nnnnd--A\n-     *     pppppppp pppppppp ppppnnnn nnnn---A\n-     *     pppppppp pppppppp ppppnnnn nnnnd--A\n-     *\n-     *   Large:\n-     *     ssssssss ssssssss ssss++++ ++++D-LA\n-     *     xxxxxxxx xxxxxxxx xxxxxxxx xxxxxxxx\n-     *     -------- -------- ----++++ ++++D-LA\n-     *\n-     *   Large (sampled, size <= PAGE):\n-     *     ssssssss ssssssss ssssnnnn nnnnD-LA\n-     *\n-     *   Large (not sampled, size == PAGE):\n-     *     ssssssss ssssssss ssss++++ ++++D-LA\n-     */\n-    size_t\t\t\t\tbits;\n+\t/*\n+\t * Run address (or size) and various flags are stored together.  The bit\n+\t * layout looks like (assuming 32-bit system):\n+\t *\n+\t *   ???????? ???????? ????nnnn nnnndula\n+\t *\n+\t * ? : Unallocated: Run address for first/last pages, unset for internal\n+\t *                  pages.\n+\t *     Small: Run page offset.\n+\t *     Large: Run size for first page, unset for trailing pages.\n+\t * n : binind for small size class, BININD_INVALID for large size class.\n+\t * d : dirty?\n+\t * u : unzeroed?\n+\t * l : large?\n+\t * a : allocated?\n+\t *\n+\t * Following are example bit patterns for the three types of runs.\n+\t *\n+\t * p : run page offset\n+\t * s : run size\n+\t * n : binind for size class; large objects set these to BININD_INVALID\n+\t *     except for promoted allocations (see prof_promote)\n+\t * x : don't care\n+\t * - : 0\n+\t * + : 1\n+\t * [DULA] : bit set\n+\t * [dula] : bit unset\n+\t *\n+\t *   Unallocated (clean):\n+\t *     ssssssss ssssssss ssss++++ ++++du-a\n+\t *     xxxxxxxx xxxxxxxx xxxxxxxx xxxx-Uxx\n+\t *     ssssssss ssssssss ssss++++ ++++dU-a\n+\t *\n+\t *   Unallocated (dirty):\n+\t *     ssssssss ssssssss ssss++++ ++++D--a\n+\t *     xxxxxxxx xxxxxxxx xxxxxxxx xxxxxxxx\n+\t *     ssssssss ssssssss ssss++++ ++++D--a\n+\t *\n+\t *   Small:\n+\t *     pppppppp pppppppp ppppnnnn nnnnd--A\n+\t *     pppppppp pppppppp ppppnnnn nnnn---A\n+\t *     pppppppp pppppppp ppppnnnn nnnnd--A\n+\t *\n+\t *   Large:\n+\t *     ssssssss ssssssss ssss++++ ++++D-LA\n+\t *     xxxxxxxx xxxxxxxx xxxxxxxx xxxxxxxx\n+\t *     -------- -------- ----++++ ++++D-LA\n+\t *\n+\t *   Large (sampled, size <= PAGE):\n+\t *     ssssssss ssssssss ssssnnnn nnnnD-LA\n+\t *\n+\t *   Large (not sampled, size == PAGE):\n+\t *     ssssssss ssssssss ssss++++ ++++D-LA\n+\t */\n+\tsize_t\t\t\t\tbits;\n #define\tCHUNK_MAP_BININD_SHIFT\t4\n #define\tBININD_INVALID\t\t((size_t)0xffU)\n /*     CHUNK_MAP_BININD_MASK == (BININD_INVALID << CHUNK_MAP_BININD_SHIFT) */\n@@ -161,45 +161,45 @@ typedef rb_tree(arena_chunk_map_t) arena_run_tree_t;\n \n /* Arena chunk header. */\n struct arena_chunk_s {\n-    /* Arena that owns the chunk. */\n-    arena_t\t\t\t*arena;\n-\n-    /* Linkage for tree of arena chunks that contain dirty runs. */\n-    rb_node(arena_chunk_t)\tdirty_link;\n-\n-    /* Number of dirty pages. */\n-    size_t\t\t\tndirty;\n-\n-    /* Number of available runs. */\n-    size_t\t\t\tnruns_avail;\n-\n-    /*\n-     * Number of available run adjacencies.  Clean and dirty available runs\n-     * are not coalesced, which causes virtual memory fragmentation.  The\n-     * ratio of (nruns_avail-nruns_adjac):nruns_adjac is used for tracking\n-     * this fragmentation.\n-     * */\n-    size_t\t\t\tnruns_adjac;\n-\n-    /*\n-     * Map of pages within chunk that keeps track of free/large/small.  The\n-     * first map_bias entries are omitted, since the chunk header does not\n-     * need to be tracked in the map.  This omission saves a header page\n-     * for common chunk sizes (e.g. 4 MiB).\n-     */\n-    arena_chunk_map_t\tmap[1]; /* Dynamically sized. */\n+\t/* Arena that owns the chunk. */\n+\tarena_t\t\t\t*arena;\n+\n+\t/* Linkage for tree of arena chunks that contain dirty runs. */\n+\trb_node(arena_chunk_t)\tdirty_link;\n+\n+\t/* Number of dirty pages. */\n+\tsize_t\t\t\tndirty;\n+\n+\t/* Number of available runs. */\n+\tsize_t\t\t\tnruns_avail;\n+\n+\t/*\n+\t * Number of available run adjacencies.  Clean and dirty available runs\n+\t * are not coalesced, which causes virtual memory fragmentation.  The\n+\t * ratio of (nruns_avail-nruns_adjac):nruns_adjac is used for tracking\n+\t * this fragmentation.\n+\t * */\n+\tsize_t\t\t\tnruns_adjac;\n+\n+\t/*\n+\t * Map of pages within chunk that keeps track of free/large/small.  The\n+\t * first map_bias entries are omitted, since the chunk header does not\n+\t * need to be tracked in the map.  This omission saves a header page\n+\t * for common chunk sizes (e.g. 4 MiB).\n+\t */\n+\tarena_chunk_map_t\tmap[1]; /* Dynamically sized. */\n };\n typedef rb_tree(arena_chunk_t) arena_chunk_tree_t;\n \n struct arena_run_s {\n-    /* Bin this run is associated with. */\n-    arena_bin_t\t*bin;\n+\t/* Bin this run is associated with. */\n+\tarena_bin_t\t*bin;\n \n-    /* Index of next region that has never been allocated, or nregs. */\n-    uint32_t\tnextind;\n+\t/* Index of next region that has never been allocated, or nregs. */\n+\tuint32_t\tnextind;\n \n-    /* Number of free regions in run. */\n-    unsigned\tnfree;\n+\t/* Number of free regions in run. */\n+\tunsigned\tnfree;\n };\n \n /*\n@@ -241,144 +241,144 @@ struct arena_run_s {\n  * either 0 or redzone_size; it is present only if needed to align reg0_offset.\n  */\n struct arena_bin_info_s {\n-    /* Size of regions in a run for this bin's size class. */\n-    size_t\t\treg_size;\n+\t/* Size of regions in a run for this bin's size class. */\n+\tsize_t\t\treg_size;\n \n-    /* Redzone size. */\n-    size_t\t\tredzone_size;\n+\t/* Redzone size. */\n+\tsize_t\t\tredzone_size;\n \n-    /* Interval between regions (reg_size + (redzone_size << 1)). */\n-    size_t\t\treg_interval;\n+\t/* Interval between regions (reg_size + (redzone_size << 1)). */\n+\tsize_t\t\treg_interval;\n \n-    /* Total size of a run for this bin's size class. */\n-    size_t\t\trun_size;\n+\t/* Total size of a run for this bin's size class. */\n+\tsize_t\t\trun_size;\n \n-    /* Total number of regions in a run for this bin's size class. */\n-    uint32_t\tnregs;\n+\t/* Total number of regions in a run for this bin's size class. */\n+\tuint32_t\tnregs;\n \n-    /*\n-     * Offset of first bitmap_t element in a run header for this bin's size\n-     * class.\n-     */\n-    uint32_t\tbitmap_offset;\n+\t/*\n+\t * Offset of first bitmap_t element in a run header for this bin's size\n+\t * class.\n+\t */\n+\tuint32_t\tbitmap_offset;\n \n-    /*\n-     * Metadata used to manipulate bitmaps for runs associated with this\n-     * bin.\n-     */\n-    bitmap_info_t\tbitmap_info;\n+\t/*\n+\t * Metadata used to manipulate bitmaps for runs associated with this\n+\t * bin.\n+\t */\n+\tbitmap_info_t\tbitmap_info;\n \n-    /*\n-     * Offset of first (prof_ctx_t *) in a run header for this bin's size\n-     * class, or 0 if (config_prof == false || opt_prof == false).\n-     */\n-    uint32_t\tctx0_offset;\n+\t/*\n+\t * Offset of first (prof_ctx_t *) in a run header for this bin's size\n+\t * class, or 0 if (config_prof == false || opt_prof == false).\n+\t */\n+\tuint32_t\tctx0_offset;\n \n-    /* Offset of first region in a run for this bin's size class. */\n-    uint32_t\treg0_offset;\n+\t/* Offset of first region in a run for this bin's size class. */\n+\tuint32_t\treg0_offset;\n };\n \n struct arena_bin_s {\n-    /*\n-     * All operations on runcur, runs, and stats require that lock be\n-     * locked.  Run allocation/deallocation are protected by the arena lock,\n-     * which may be acquired while holding one or more bin locks, but not\n-     * vise versa.\n-     */\n-    malloc_mutex_t\tlock;\n-\n-    /*\n-     * Current run being used to service allocations of this bin's size\n-     * class.\n-     */\n-    arena_run_t\t*runcur;\n-\n-    /*\n-     * Tree of non-full runs.  This tree is used when looking for an\n-     * existing run when runcur is no longer usable.  We choose the\n-     * non-full run that is lowest in memory; this policy tends to keep\n-     * objects packed well, and it can also help reduce the number of\n-     * almost-empty chunks.\n-     */\n-    arena_run_tree_t runs;\n-\n-    /* Bin statistics. */\n-    malloc_bin_stats_t stats;\n+\t/*\n+\t * All operations on runcur, runs, and stats require that lock be\n+\t * locked.  Run allocation/deallocation are protected by the arena lock,\n+\t * which may be acquired while holding one or more bin locks, but not\n+\t * vise versa.\n+\t */\n+\tmalloc_mutex_t\tlock;\n+\n+\t/*\n+\t * Current run being used to service allocations of this bin's size\n+\t * class.\n+\t */\n+\tarena_run_t\t*runcur;\n+\n+\t/*\n+\t * Tree of non-full runs.  This tree is used when looking for an\n+\t * existing run when runcur is no longer usable.  We choose the\n+\t * non-full run that is lowest in memory; this policy tends to keep\n+\t * objects packed well, and it can also help reduce the number of\n+\t * almost-empty chunks.\n+\t */\n+\tarena_run_tree_t runs;\n+\n+\t/* Bin statistics. */\n+\tmalloc_bin_stats_t stats;\n };\n \n struct arena_s {\n-    /* This arena's index within the arenas array. */\n-    unsigned\t\tind;\n-\n-    /*\n-     * Number of threads currently assigned to this arena.  This field is\n-     * protected by arenas_lock.\n-     */\n-    unsigned\t\tnthreads;\n-\n-    /*\n-     * There are three classes of arena operations from a locking\n-     * perspective:\n-     * 1) Thread asssignment (modifies nthreads) is protected by\n-     *    arenas_lock.\n-     * 2) Bin-related operations are protected by bin locks.\n-     * 3) Chunk- and run-related operations are protected by this mutex.\n-     */\n-    malloc_mutex_t\t\tlock;\n-\n-    arena_stats_t\t\tstats;\n-    /*\n-     * List of tcaches for extant threads associated with this arena.\n-     * Stats from these are merged incrementally, and at exit.\n-     */\n-    ql_head(tcache_t)\ttcache_ql;\n-\n-    uint64_t\t\tprof_accumbytes;\n-\n-    dss_prec_t\t\tdss_prec;\n-\n-    /* Tree of dirty-page-containing chunks this arena manages. */\n-    arena_chunk_tree_t\tchunks_dirty;\n-\n-    /*\n-     * In order to avoid rapid chunk allocation/deallocation when an arena\n-     * oscillates right on the cusp of needing a new chunk, cache the most\n-     * recently freed chunk.  The spare is left in the arena's chunk trees\n-     * until it is deleted.\n-     *\n-     * There is one spare chunk per arena, rather than one spare total, in\n-     * order to avoid interactions between multiple threads that could make\n-     * a single spare inadequate.\n-     */\n-    arena_chunk_t\t\t*spare;\n-\n-    /* Number of pages in active runs. */\n-    size_t\t\t\tnactive;\n-\n-    /*\n-     * Current count of pages within unused runs that are potentially\n-     * dirty, and for which madvise(... MADV_DONTNEED) has not been called.\n-     * By tracking this, we can institute a limit on how much dirty unused\n-     * memory is mapped for each arena.\n-     */\n-    size_t\t\t\tndirty;\n-\n-    /*\n-     * Approximate number of pages being purged.  It is possible for\n-     * multiple threads to purge dirty pages concurrently, and they use\n-     * npurgatory to indicate the total number of pages all threads are\n-     * attempting to purge.\n-     */\n-    size_t\t\t\tnpurgatory;\n-\n-    /*\n-     * Size/address-ordered trees of this arena's available runs.  The trees\n-     * are used for first-best-fit run allocation.\n-     */\n-    arena_avail_tree_t\truns_avail;\n-\n-    /* bins is used to store trees of free regions. */\n-    arena_bin_t\t\tbins[NBINS];\n+\t/* This arena's index within the arenas array. */\n+\tunsigned\t\tind;\n+\n+\t/*\n+\t * Number of threads currently assigned to this arena.  This field is\n+\t * protected by arenas_lock.\n+\t */\n+\tunsigned\t\tnthreads;\n+\n+\t/*\n+\t * There are three classes of arena operations from a locking\n+\t * perspective:\n+\t * 1) Thread asssignment (modifies nthreads) is protected by\n+\t *    arenas_lock.\n+\t * 2) Bin-related operations are protected by bin locks.\n+\t * 3) Chunk- and run-related operations are protected by this mutex.\n+\t */\n+\tmalloc_mutex_t\t\tlock;\n+\n+\tarena_stats_t\t\tstats;\n+\t/*\n+\t * List of tcaches for extant threads associated with this arena.\n+\t * Stats from these are merged incrementally, and at exit.\n+\t */\n+\tql_head(tcache_t)\ttcache_ql;\n+\n+\tuint64_t\t\tprof_accumbytes;\n+\n+\tdss_prec_t\t\tdss_prec;\n+\n+\t/* Tree of dirty-page-containing chunks this arena manages. */\n+\tarena_chunk_tree_t\tchunks_dirty;\n+\n+\t/*\n+\t * In order to avoid rapid chunk allocation/deallocation when an arena\n+\t * oscillates right on the cusp of needing a new chunk, cache the most\n+\t * recently freed chunk.  The spare is left in the arena's chunk trees\n+\t * until it is deleted.\n+\t *\n+\t * There is one spare chunk per arena, rather than one spare total, in\n+\t * order to avoid interactions between multiple threads that could make\n+\t * a single spare inadequate.\n+\t */\n+\tarena_chunk_t\t\t*spare;\n+\n+\t/* Number of pages in active runs. */\n+\tsize_t\t\t\tnactive;\n+\n+\t/*\n+\t * Current count of pages within unused runs that are potentially\n+\t * dirty, and for which madvise(... MADV_DONTNEED) has not been called.\n+\t * By tracking this, we can institute a limit on how much dirty unused\n+\t * memory is mapped for each arena.\n+\t */\n+\tsize_t\t\t\tndirty;\n+\n+\t/*\n+\t * Approximate number of pages being purged.  It is possible for\n+\t * multiple threads to purge dirty pages concurrently, and they use\n+\t * npurgatory to indicate the total number of pages all threads are\n+\t * attempting to purge.\n+\t */\n+\tsize_t\t\t\tnpurgatory;\n+\n+\t/*\n+\t * Size/address-ordered trees of this arena's available runs.  The trees\n+\t * are used for first-best-fit run allocation.\n+\t */\n+\tarena_avail_tree_t\truns_avail;\n+\n+\t/* bins is used to store trees of free regions. */\n+\tarena_bin_t\t\tbins[NBINS];\n };\n \n #endif /* JEMALLOC_H_STRUCTS */\n@@ -484,536 +484,536 @@ JEMALLOC_ALWAYS_INLINE arena_chunk_map_t *\n arena_mapp_get(arena_chunk_t *chunk, size_t pageind)\n {\n \n-    assert(pageind >= map_bias);\n-    assert(pageind < chunk_npages);\n+\tassert(pageind >= map_bias);\n+\tassert(pageind < chunk_npages);\n \n-    return (&chunk->map[pageind-map_bias]);\n+\treturn (&chunk->map[pageind-map_bias]);\n }\n \n JEMALLOC_ALWAYS_INLINE size_t *\n arena_mapbitsp_get(arena_chunk_t *chunk, size_t pageind)\n {\n \n-    return (&arena_mapp_get(chunk, pageind)->bits);\n+\treturn (&arena_mapp_get(chunk, pageind)->bits);\n }\n \n JEMALLOC_ALWAYS_INLINE size_t\n arena_mapbits_get(arena_chunk_t *chunk, size_t pageind)\n {\n \n-    return (*arena_mapbitsp_get(chunk, pageind));\n+\treturn (*arena_mapbitsp_get(chunk, pageind));\n }\n \n JEMALLOC_ALWAYS_INLINE size_t\n arena_mapbits_unallocated_size_get(arena_chunk_t *chunk, size_t pageind)\n {\n-    size_t mapbits;\n+\tsize_t mapbits;\n \n-    mapbits = arena_mapbits_get(chunk, pageind);\n-    assert((mapbits & (CHUNK_MAP_LARGE|CHUNK_MAP_ALLOCATED)) == 0);\n-    return (mapbits & ~PAGE_MASK);\n+\tmapbits = arena_mapbits_get(chunk, pageind);\n+\tassert((mapbits & (CHUNK_MAP_LARGE|CHUNK_MAP_ALLOCATED)) == 0);\n+\treturn (mapbits & ~PAGE_MASK);\n }\n \n JEMALLOC_ALWAYS_INLINE size_t\n arena_mapbits_large_size_get(arena_chunk_t *chunk, size_t pageind)\n {\n-    size_t mapbits;\n+\tsize_t mapbits;\n \n-    mapbits = arena_mapbits_get(chunk, pageind);\n-    assert((mapbits & (CHUNK_MAP_LARGE|CHUNK_MAP_ALLOCATED)) ==\n-        (CHUNK_MAP_LARGE|CHUNK_MAP_ALLOCATED));\n-    return (mapbits & ~PAGE_MASK);\n+\tmapbits = arena_mapbits_get(chunk, pageind);\n+\tassert((mapbits & (CHUNK_MAP_LARGE|CHUNK_MAP_ALLOCATED)) ==\n+\t    (CHUNK_MAP_LARGE|CHUNK_MAP_ALLOCATED));\n+\treturn (mapbits & ~PAGE_MASK);\n }\n \n JEMALLOC_ALWAYS_INLINE size_t\n arena_mapbits_small_runind_get(arena_chunk_t *chunk, size_t pageind)\n {\n-    size_t mapbits;\n+\tsize_t mapbits;\n \n-    mapbits = arena_mapbits_get(chunk, pageind);\n-    assert((mapbits & (CHUNK_MAP_LARGE|CHUNK_MAP_ALLOCATED)) ==\n-        CHUNK_MAP_ALLOCATED);\n-    return (mapbits >> LG_PAGE);\n+\tmapbits = arena_mapbits_get(chunk, pageind);\n+\tassert((mapbits & (CHUNK_MAP_LARGE|CHUNK_MAP_ALLOCATED)) ==\n+\t    CHUNK_MAP_ALLOCATED);\n+\treturn (mapbits >> LG_PAGE);\n }\n \n JEMALLOC_ALWAYS_INLINE size_t\n arena_mapbits_binind_get(arena_chunk_t *chunk, size_t pageind)\n {\n-    size_t mapbits;\n-    size_t binind;\n+\tsize_t mapbits;\n+\tsize_t binind;\n \n-    mapbits = arena_mapbits_get(chunk, pageind);\n-    binind = (mapbits & CHUNK_MAP_BININD_MASK) >> CHUNK_MAP_BININD_SHIFT;\n-    assert(binind < NBINS || binind == BININD_INVALID);\n-    return (binind);\n+\tmapbits = arena_mapbits_get(chunk, pageind);\n+\tbinind = (mapbits & CHUNK_MAP_BININD_MASK) >> CHUNK_MAP_BININD_SHIFT;\n+\tassert(binind < NBINS || binind == BININD_INVALID);\n+\treturn (binind);\n }\n \n JEMALLOC_ALWAYS_INLINE size_t\n arena_mapbits_dirty_get(arena_chunk_t *chunk, size_t pageind)\n {\n-    size_t mapbits;\n+\tsize_t mapbits;\n \n-    mapbits = arena_mapbits_get(chunk, pageind);\n-    return (mapbits & CHUNK_MAP_DIRTY);\n+\tmapbits = arena_mapbits_get(chunk, pageind);\n+\treturn (mapbits & CHUNK_MAP_DIRTY);\n }\n \n JEMALLOC_ALWAYS_INLINE size_t\n arena_mapbits_unzeroed_get(arena_chunk_t *chunk, size_t pageind)\n {\n-    size_t mapbits;\n+\tsize_t mapbits;\n \n-    mapbits = arena_mapbits_get(chunk, pageind);\n-    return (mapbits & CHUNK_MAP_UNZEROED);\n+\tmapbits = arena_mapbits_get(chunk, pageind);\n+\treturn (mapbits & CHUNK_MAP_UNZEROED);\n }\n \n JEMALLOC_ALWAYS_INLINE size_t\n arena_mapbits_large_get(arena_chunk_t *chunk, size_t pageind)\n {\n-    size_t mapbits;\n+\tsize_t mapbits;\n \n-    mapbits = arena_mapbits_get(chunk, pageind);\n-    return (mapbits & CHUNK_MAP_LARGE);\n+\tmapbits = arena_mapbits_get(chunk, pageind);\n+\treturn (mapbits & CHUNK_MAP_LARGE);\n }\n \n JEMALLOC_ALWAYS_INLINE size_t\n arena_mapbits_allocated_get(arena_chunk_t *chunk, size_t pageind)\n {\n-    size_t mapbits;\n+\tsize_t mapbits;\n \n-    mapbits = arena_mapbits_get(chunk, pageind);\n-    return (mapbits & CHUNK_MAP_ALLOCATED);\n+\tmapbits = arena_mapbits_get(chunk, pageind);\n+\treturn (mapbits & CHUNK_MAP_ALLOCATED);\n }\n \n JEMALLOC_ALWAYS_INLINE void\n arena_mapbits_unallocated_set(arena_chunk_t *chunk, size_t pageind, size_t size,\n     size_t flags)\n {\n-    size_t *mapbitsp;\n+\tsize_t *mapbitsp;\n \n-    mapbitsp = arena_mapbitsp_get(chunk, pageind);\n-    assert((size & PAGE_MASK) == 0);\n-    assert((flags & ~CHUNK_MAP_FLAGS_MASK) == 0);\n-    assert((flags & (CHUNK_MAP_DIRTY|CHUNK_MAP_UNZEROED)) == flags);\n-    *mapbitsp = size | CHUNK_MAP_BININD_INVALID | flags;\n+\tmapbitsp = arena_mapbitsp_get(chunk, pageind);\n+\tassert((size & PAGE_MASK) == 0);\n+\tassert((flags & ~CHUNK_MAP_FLAGS_MASK) == 0);\n+\tassert((flags & (CHUNK_MAP_DIRTY|CHUNK_MAP_UNZEROED)) == flags);\n+\t*mapbitsp = size | CHUNK_MAP_BININD_INVALID | flags;\n }\n \n JEMALLOC_ALWAYS_INLINE void\n arena_mapbits_unallocated_size_set(arena_chunk_t *chunk, size_t pageind,\n     size_t size)\n {\n-    size_t *mapbitsp;\n+\tsize_t *mapbitsp;\n \n-    mapbitsp = arena_mapbitsp_get(chunk, pageind);\n-    assert((size & PAGE_MASK) == 0);\n-    assert((*mapbitsp & (CHUNK_MAP_LARGE|CHUNK_MAP_ALLOCATED)) == 0);\n-    *mapbitsp = size | (*mapbitsp & PAGE_MASK);\n+\tmapbitsp = arena_mapbitsp_get(chunk, pageind);\n+\tassert((size & PAGE_MASK) == 0);\n+\tassert((*mapbitsp & (CHUNK_MAP_LARGE|CHUNK_MAP_ALLOCATED)) == 0);\n+\t*mapbitsp = size | (*mapbitsp & PAGE_MASK);\n }\n \n JEMALLOC_ALWAYS_INLINE void\n arena_mapbits_large_set(arena_chunk_t *chunk, size_t pageind, size_t size,\n     size_t flags)\n {\n-    size_t *mapbitsp;\n-    size_t unzeroed;\n-\n-    mapbitsp = arena_mapbitsp_get(chunk, pageind);\n-    assert((size & PAGE_MASK) == 0);\n-    assert((flags & CHUNK_MAP_DIRTY) == flags);\n-    unzeroed = *mapbitsp & CHUNK_MAP_UNZEROED; /* Preserve unzeroed. */\n-    *mapbitsp = size | CHUNK_MAP_BININD_INVALID | flags | unzeroed |\n-        CHUNK_MAP_LARGE | CHUNK_MAP_ALLOCATED;\n+\tsize_t *mapbitsp;\n+\tsize_t unzeroed;\n+\n+\tmapbitsp = arena_mapbitsp_get(chunk, pageind);\n+\tassert((size & PAGE_MASK) == 0);\n+\tassert((flags & CHUNK_MAP_DIRTY) == flags);\n+\tunzeroed = *mapbitsp & CHUNK_MAP_UNZEROED; /* Preserve unzeroed. */\n+\t*mapbitsp = size | CHUNK_MAP_BININD_INVALID | flags | unzeroed |\n+\t    CHUNK_MAP_LARGE | CHUNK_MAP_ALLOCATED;\n }\n \n JEMALLOC_ALWAYS_INLINE void\n arena_mapbits_large_binind_set(arena_chunk_t *chunk, size_t pageind,\n     size_t binind)\n {\n-    size_t *mapbitsp;\n+\tsize_t *mapbitsp;\n \n-    assert(binind <= BININD_INVALID);\n-    mapbitsp = arena_mapbitsp_get(chunk, pageind);\n-    assert(arena_mapbits_large_size_get(chunk, pageind) == PAGE);\n-    *mapbitsp = (*mapbitsp & ~CHUNK_MAP_BININD_MASK) | (binind <<\n-        CHUNK_MAP_BININD_SHIFT);\n+\tassert(binind <= BININD_INVALID);\n+\tmapbitsp = arena_mapbitsp_get(chunk, pageind);\n+\tassert(arena_mapbits_large_size_get(chunk, pageind) == PAGE);\n+\t*mapbitsp = (*mapbitsp & ~CHUNK_MAP_BININD_MASK) | (binind <<\n+\t    CHUNK_MAP_BININD_SHIFT);\n }\n \n JEMALLOC_ALWAYS_INLINE void\n arena_mapbits_small_set(arena_chunk_t *chunk, size_t pageind, size_t runind,\n     size_t binind, size_t flags)\n {\n-    size_t *mapbitsp;\n-    size_t unzeroed;\n-\n-    assert(binind < BININD_INVALID);\n-    mapbitsp = arena_mapbitsp_get(chunk, pageind);\n-    assert(pageind - runind >= map_bias);\n-    assert((flags & CHUNK_MAP_DIRTY) == flags);\n-    unzeroed = *mapbitsp & CHUNK_MAP_UNZEROED; /* Preserve unzeroed. */\n-    *mapbitsp = (runind << LG_PAGE) | (binind << CHUNK_MAP_BININD_SHIFT) |\n-        flags | unzeroed | CHUNK_MAP_ALLOCATED;\n+\tsize_t *mapbitsp;\n+\tsize_t unzeroed;\n+\n+\tassert(binind < BININD_INVALID);\n+\tmapbitsp = arena_mapbitsp_get(chunk, pageind);\n+\tassert(pageind - runind >= map_bias);\n+\tassert((flags & CHUNK_MAP_DIRTY) == flags);\n+\tunzeroed = *mapbitsp & CHUNK_MAP_UNZEROED; /* Preserve unzeroed. */\n+\t*mapbitsp = (runind << LG_PAGE) | (binind << CHUNK_MAP_BININD_SHIFT) |\n+\t    flags | unzeroed | CHUNK_MAP_ALLOCATED;\n }\n \n JEMALLOC_ALWAYS_INLINE void\n arena_mapbits_unzeroed_set(arena_chunk_t *chunk, size_t pageind,\n     size_t unzeroed)\n {\n-    size_t *mapbitsp;\n+\tsize_t *mapbitsp;\n \n-    mapbitsp = arena_mapbitsp_get(chunk, pageind);\n-    *mapbitsp = (*mapbitsp & ~CHUNK_MAP_UNZEROED) | unzeroed;\n+\tmapbitsp = arena_mapbitsp_get(chunk, pageind);\n+\t*mapbitsp = (*mapbitsp & ~CHUNK_MAP_UNZEROED) | unzeroed;\n }\n \n JEMALLOC_INLINE bool\n arena_prof_accum_impl(arena_t *arena, uint64_t accumbytes)\n {\n \n-    cassert(config_prof);\n-    assert(prof_interval != 0);\n+\tcassert(config_prof);\n+\tassert(prof_interval != 0);\n \n-    arena->prof_accumbytes += accumbytes;\n-    if (arena->prof_accumbytes >= prof_interval) {\n-        arena->prof_accumbytes -= prof_interval;\n-        return (true);\n-    }\n-    return (false);\n+\tarena->prof_accumbytes += accumbytes;\n+\tif (arena->prof_accumbytes >= prof_interval) {\n+\t\tarena->prof_accumbytes -= prof_interval;\n+\t\treturn (true);\n+\t}\n+\treturn (false);\n }\n \n JEMALLOC_INLINE bool\n arena_prof_accum_locked(arena_t *arena, uint64_t accumbytes)\n {\n \n-    cassert(config_prof);\n+\tcassert(config_prof);\n \n-    if (prof_interval == 0)\n-        return (false);\n-    return (arena_prof_accum_impl(arena, accumbytes));\n+\tif (prof_interval == 0)\n+\t\treturn (false);\n+\treturn (arena_prof_accum_impl(arena, accumbytes));\n }\n \n JEMALLOC_INLINE bool\n arena_prof_accum(arena_t *arena, uint64_t accumbytes)\n {\n \n-    cassert(config_prof);\n+\tcassert(config_prof);\n \n-    if (prof_interval == 0)\n-        return (false);\n+\tif (prof_interval == 0)\n+\t\treturn (false);\n \n-    {\n-        bool ret;\n+\t{\n+\t\tbool ret;\n \n-        malloc_mutex_lock(&arena->lock);\n-        ret = arena_prof_accum_impl(arena, accumbytes);\n-        malloc_mutex_unlock(&arena->lock);\n-        return (ret);\n-    }\n+\t\tmalloc_mutex_lock(&arena->lock);\n+\t\tret = arena_prof_accum_impl(arena, accumbytes);\n+\t\tmalloc_mutex_unlock(&arena->lock);\n+\t\treturn (ret);\n+\t}\n }\n \n JEMALLOC_ALWAYS_INLINE size_t\n arena_ptr_small_binind_get(const void *ptr, size_t mapbits)\n {\n-    size_t binind;\n-\n-    binind = (mapbits & CHUNK_MAP_BININD_MASK) >> CHUNK_MAP_BININD_SHIFT;\n-\n-    if (config_debug) {\n-        arena_chunk_t *chunk;\n-        arena_t *arena;\n-        size_t pageind;\n-        size_t actual_mapbits;\n-        arena_run_t *run;\n-        arena_bin_t *bin;\n-        size_t actual_binind;\n-        arena_bin_info_t *bin_info;\n-\n-        assert(binind != BININD_INVALID);\n-        assert(binind < NBINS);\n-        chunk = (arena_chunk_t *)CHUNK_ADDR2BASE(ptr);\n-        arena = chunk->arena;\n-        pageind = ((uintptr_t)ptr - (uintptr_t)chunk) >> LG_PAGE;\n-        actual_mapbits = arena_mapbits_get(chunk, pageind);\n-        assert(mapbits == actual_mapbits);\n-        assert(arena_mapbits_large_get(chunk, pageind) == 0);\n-        assert(arena_mapbits_allocated_get(chunk, pageind) != 0);\n-        run = (arena_run_t *)((uintptr_t)chunk + (uintptr_t)((pageind -\n-            (actual_mapbits >> LG_PAGE)) << LG_PAGE));\n-        bin = run->bin;\n-        actual_binind = bin - arena->bins;\n-        assert(binind == actual_binind);\n-        bin_info = &arena_bin_info[actual_binind];\n-        assert(((uintptr_t)ptr - ((uintptr_t)run +\n-            (uintptr_t)bin_info->reg0_offset)) % bin_info->reg_interval\n-            == 0);\n-    }\n-\n-    return (binind);\n+\tsize_t binind;\n+\n+\tbinind = (mapbits & CHUNK_MAP_BININD_MASK) >> CHUNK_MAP_BININD_SHIFT;\n+\n+\tif (config_debug) {\n+\t\tarena_chunk_t *chunk;\n+\t\tarena_t *arena;\n+\t\tsize_t pageind;\n+\t\tsize_t actual_mapbits;\n+\t\tarena_run_t *run;\n+\t\tarena_bin_t *bin;\n+\t\tsize_t actual_binind;\n+\t\tarena_bin_info_t *bin_info;\n+\n+\t\tassert(binind != BININD_INVALID);\n+\t\tassert(binind < NBINS);\n+\t\tchunk = (arena_chunk_t *)CHUNK_ADDR2BASE(ptr);\n+\t\tarena = chunk->arena;\n+\t\tpageind = ((uintptr_t)ptr - (uintptr_t)chunk) >> LG_PAGE;\n+\t\tactual_mapbits = arena_mapbits_get(chunk, pageind);\n+\t\tassert(mapbits == actual_mapbits);\n+\t\tassert(arena_mapbits_large_get(chunk, pageind) == 0);\n+\t\tassert(arena_mapbits_allocated_get(chunk, pageind) != 0);\n+\t\trun = (arena_run_t *)((uintptr_t)chunk + (uintptr_t)((pageind -\n+\t\t    (actual_mapbits >> LG_PAGE)) << LG_PAGE));\n+\t\tbin = run->bin;\n+\t\tactual_binind = bin - arena->bins;\n+\t\tassert(binind == actual_binind);\n+\t\tbin_info = &arena_bin_info[actual_binind];\n+\t\tassert(((uintptr_t)ptr - ((uintptr_t)run +\n+\t\t    (uintptr_t)bin_info->reg0_offset)) % bin_info->reg_interval\n+\t\t    == 0);\n+\t}\n+\n+\treturn (binind);\n }\n #  endif /* JEMALLOC_ARENA_INLINE_A */\n \n #  ifdef JEMALLOC_ARENA_INLINE_B\n JEMALLOC_INLINE size_t\n arena_bin_index(arena_t *arena, arena_bin_t *bin)\n {\n-    size_t binind = bin - arena->bins;\n-    assert(binind < NBINS);\n-    return (binind);\n+\tsize_t binind = bin - arena->bins;\n+\tassert(binind < NBINS);\n+\treturn (binind);\n }\n \n JEMALLOC_INLINE unsigned\n arena_run_regind(arena_run_t *run, arena_bin_info_t *bin_info, const void *ptr)\n {\n-    unsigned shift, diff, regind;\n-    size_t interval;\n-\n-    /*\n-     * Freeing a pointer lower than region zero can cause assertion\n-     * failure.\n-     */\n-    assert((uintptr_t)ptr >= (uintptr_t)run +\n-        (uintptr_t)bin_info->reg0_offset);\n-\n-    /*\n-     * Avoid doing division with a variable divisor if possible.  Using\n-     * actual division here can reduce allocator throughput by over 20%!\n-     */\n-    diff = (unsigned)((uintptr_t)ptr - (uintptr_t)run -\n-        bin_info->reg0_offset);\n-\n-    /* Rescale (factor powers of 2 out of the numerator and denominator). */\n-    interval = bin_info->reg_interval;\n-    shift = ffs(interval) - 1;\n-    diff >>= shift;\n-    interval >>= shift;\n-\n-    if (interval == 1) {\n-        /* The divisor was a power of 2. */\n-        regind = diff;\n-    } else {\n-        /*\n-         * To divide by a number D that is not a power of two we\n-         * multiply by (2^21 / D) and then right shift by 21 positions.\n-         *\n-         *   X / D\n-         *\n-         * becomes\n-         *\n-         *   (X * interval_invs[D - 3]) >> SIZE_INV_SHIFT\n-         *\n-         * We can omit the first three elements, because we never\n-         * divide by 0, and 1 and 2 are both powers of two, which are\n-         * handled above.\n-         */\n+\tunsigned shift, diff, regind;\n+\tsize_t interval;\n+\n+\t/*\n+\t * Freeing a pointer lower than region zero can cause assertion\n+\t * failure.\n+\t */\n+\tassert((uintptr_t)ptr >= (uintptr_t)run +\n+\t    (uintptr_t)bin_info->reg0_offset);\n+\n+\t/*\n+\t * Avoid doing division with a variable divisor if possible.  Using\n+\t * actual division here can reduce allocator throughput by over 20%!\n+\t */\n+\tdiff = (unsigned)((uintptr_t)ptr - (uintptr_t)run -\n+\t    bin_info->reg0_offset);\n+\n+\t/* Rescale (factor powers of 2 out of the numerator and denominator). */\n+\tinterval = bin_info->reg_interval;\n+\tshift = ffs(interval) - 1;\n+\tdiff >>= shift;\n+\tinterval >>= shift;\n+\n+\tif (interval == 1) {\n+\t\t/* The divisor was a power of 2. */\n+\t\tregind = diff;\n+\t} else {\n+\t\t/*\n+\t\t * To divide by a number D that is not a power of two we\n+\t\t * multiply by (2^21 / D) and then right shift by 21 positions.\n+\t\t *\n+\t\t *   X / D\n+\t\t *\n+\t\t * becomes\n+\t\t *\n+\t\t *   (X * interval_invs[D - 3]) >> SIZE_INV_SHIFT\n+\t\t *\n+\t\t * We can omit the first three elements, because we never\n+\t\t * divide by 0, and 1 and 2 are both powers of two, which are\n+\t\t * handled above.\n+\t\t */\n #define\tSIZE_INV_SHIFT\t((sizeof(unsigned) << 3) - LG_RUN_MAXREGS)\n #define\tSIZE_INV(s)\t(((1U << SIZE_INV_SHIFT) / (s)) + 1)\n-        static const unsigned interval_invs[] = {\n-            SIZE_INV(3),\n-            SIZE_INV(4), SIZE_INV(5), SIZE_INV(6), SIZE_INV(7),\n-            SIZE_INV(8), SIZE_INV(9), SIZE_INV(10), SIZE_INV(11),\n-            SIZE_INV(12), SIZE_INV(13), SIZE_INV(14), SIZE_INV(15),\n-            SIZE_INV(16), SIZE_INV(17), SIZE_INV(18), SIZE_INV(19),\n-            SIZE_INV(20), SIZE_INV(21), SIZE_INV(22), SIZE_INV(23),\n-            SIZE_INV(24), SIZE_INV(25), SIZE_INV(26), SIZE_INV(27),\n-            SIZE_INV(28), SIZE_INV(29), SIZE_INV(30), SIZE_INV(31)\n-        };\n-\n-        if (interval <= ((sizeof(interval_invs) / sizeof(unsigned)) +\n-            2)) {\n-            regind = (diff * interval_invs[interval - 3]) >>\n-                SIZE_INV_SHIFT;\n-        } else\n-            regind = diff / interval;\n+\t\tstatic const unsigned interval_invs[] = {\n+\t\t    SIZE_INV(3),\n+\t\t    SIZE_INV(4), SIZE_INV(5), SIZE_INV(6), SIZE_INV(7),\n+\t\t    SIZE_INV(8), SIZE_INV(9), SIZE_INV(10), SIZE_INV(11),\n+\t\t    SIZE_INV(12), SIZE_INV(13), SIZE_INV(14), SIZE_INV(15),\n+\t\t    SIZE_INV(16), SIZE_INV(17), SIZE_INV(18), SIZE_INV(19),\n+\t\t    SIZE_INV(20), SIZE_INV(21), SIZE_INV(22), SIZE_INV(23),\n+\t\t    SIZE_INV(24), SIZE_INV(25), SIZE_INV(26), SIZE_INV(27),\n+\t\t    SIZE_INV(28), SIZE_INV(29), SIZE_INV(30), SIZE_INV(31)\n+\t\t};\n+\n+\t\tif (interval <= ((sizeof(interval_invs) / sizeof(unsigned)) +\n+\t\t    2)) {\n+\t\t\tregind = (diff * interval_invs[interval - 3]) >>\n+\t\t\t    SIZE_INV_SHIFT;\n+\t\t} else\n+\t\t\tregind = diff / interval;\n #undef SIZE_INV\n #undef SIZE_INV_SHIFT\n-    }\n-    assert(diff == regind * interval);\n-    assert(regind < bin_info->nregs);\n+\t}\n+\tassert(diff == regind * interval);\n+\tassert(regind < bin_info->nregs);\n \n-    return (regind);\n+\treturn (regind);\n }\n \n JEMALLOC_INLINE prof_ctx_t *\n arena_prof_ctx_get(const void *ptr)\n {\n-    prof_ctx_t *ret;\n-    arena_chunk_t *chunk;\n-    size_t pageind, mapbits;\n-\n-    cassert(config_prof);\n-    assert(ptr != NULL);\n-    assert(CHUNK_ADDR2BASE(ptr) != ptr);\n-\n-    chunk = (arena_chunk_t *)CHUNK_ADDR2BASE(ptr);\n-    pageind = ((uintptr_t)ptr - (uintptr_t)chunk) >> LG_PAGE;\n-    mapbits = arena_mapbits_get(chunk, pageind);\n-    assert((mapbits & CHUNK_MAP_ALLOCATED) != 0);\n-    if ((mapbits & CHUNK_MAP_LARGE) == 0) {\n-        if (prof_promote)\n-            ret = (prof_ctx_t *)(uintptr_t)1U;\n-        else {\n-            arena_run_t *run = (arena_run_t *)((uintptr_t)chunk +\n-                (uintptr_t)((pageind - (mapbits >> LG_PAGE)) <<\n-                LG_PAGE));\n-            size_t binind = arena_ptr_small_binind_get(ptr,\n-                mapbits);\n-            arena_bin_info_t *bin_info = &arena_bin_info[binind];\n-            unsigned regind;\n-\n-            regind = arena_run_regind(run, bin_info, ptr);\n-            ret = *(prof_ctx_t **)((uintptr_t)run +\n-                bin_info->ctx0_offset + (regind *\n-                sizeof(prof_ctx_t *)));\n-        }\n-    } else\n-        ret = arena_mapp_get(chunk, pageind)->prof_ctx;\n-\n-    return (ret);\n+\tprof_ctx_t *ret;\n+\tarena_chunk_t *chunk;\n+\tsize_t pageind, mapbits;\n+\n+\tcassert(config_prof);\n+\tassert(ptr != NULL);\n+\tassert(CHUNK_ADDR2BASE(ptr) != ptr);\n+\n+\tchunk = (arena_chunk_t *)CHUNK_ADDR2BASE(ptr);\n+\tpageind = ((uintptr_t)ptr - (uintptr_t)chunk) >> LG_PAGE;\n+\tmapbits = arena_mapbits_get(chunk, pageind);\n+\tassert((mapbits & CHUNK_MAP_ALLOCATED) != 0);\n+\tif ((mapbits & CHUNK_MAP_LARGE) == 0) {\n+\t\tif (prof_promote)\n+\t\t\tret = (prof_ctx_t *)(uintptr_t)1U;\n+\t\telse {\n+\t\t\tarena_run_t *run = (arena_run_t *)((uintptr_t)chunk +\n+\t\t\t    (uintptr_t)((pageind - (mapbits >> LG_PAGE)) <<\n+\t\t\t    LG_PAGE));\n+\t\t\tsize_t binind = arena_ptr_small_binind_get(ptr,\n+\t\t\t    mapbits);\n+\t\t\tarena_bin_info_t *bin_info = &arena_bin_info[binind];\n+\t\t\tunsigned regind;\n+\n+\t\t\tregind = arena_run_regind(run, bin_info, ptr);\n+\t\t\tret = *(prof_ctx_t **)((uintptr_t)run +\n+\t\t\t    bin_info->ctx0_offset + (regind *\n+\t\t\t    sizeof(prof_ctx_t *)));\n+\t\t}\n+\t} else\n+\t\tret = arena_mapp_get(chunk, pageind)->prof_ctx;\n+\n+\treturn (ret);\n }\n \n JEMALLOC_INLINE void\n arena_prof_ctx_set(const void *ptr, prof_ctx_t *ctx)\n {\n-    arena_chunk_t *chunk;\n-    size_t pageind, mapbits;\n-\n-    cassert(config_prof);\n-    assert(ptr != NULL);\n-    assert(CHUNK_ADDR2BASE(ptr) != ptr);\n-\n-    chunk = (arena_chunk_t *)CHUNK_ADDR2BASE(ptr);\n-    pageind = ((uintptr_t)ptr - (uintptr_t)chunk) >> LG_PAGE;\n-    mapbits = arena_mapbits_get(chunk, pageind);\n-    assert((mapbits & CHUNK_MAP_ALLOCATED) != 0);\n-    if ((mapbits & CHUNK_MAP_LARGE) == 0) {\n-        if (prof_promote == false) {\n-            arena_run_t *run = (arena_run_t *)((uintptr_t)chunk +\n-                (uintptr_t)((pageind - (mapbits >> LG_PAGE)) <<\n-                LG_PAGE));\n-            size_t binind;\n-            arena_bin_info_t *bin_info;\n-            unsigned regind;\n-\n-            binind = arena_ptr_small_binind_get(ptr, mapbits);\n-            bin_info = &arena_bin_info[binind];\n-            regind = arena_run_regind(run, bin_info, ptr);\n-\n-            *((prof_ctx_t **)((uintptr_t)run + bin_info->ctx0_offset\n-                + (regind * sizeof(prof_ctx_t *)))) = ctx;\n-        } else\n-            assert((uintptr_t)ctx == (uintptr_t)1U);\n-    } else\n-        arena_mapp_get(chunk, pageind)->prof_ctx = ctx;\n+\tarena_chunk_t *chunk;\n+\tsize_t pageind, mapbits;\n+\n+\tcassert(config_prof);\n+\tassert(ptr != NULL);\n+\tassert(CHUNK_ADDR2BASE(ptr) != ptr);\n+\n+\tchunk = (arena_chunk_t *)CHUNK_ADDR2BASE(ptr);\n+\tpageind = ((uintptr_t)ptr - (uintptr_t)chunk) >> LG_PAGE;\n+\tmapbits = arena_mapbits_get(chunk, pageind);\n+\tassert((mapbits & CHUNK_MAP_ALLOCATED) != 0);\n+\tif ((mapbits & CHUNK_MAP_LARGE) == 0) {\n+\t\tif (prof_promote == false) {\n+\t\t\tarena_run_t *run = (arena_run_t *)((uintptr_t)chunk +\n+\t\t\t    (uintptr_t)((pageind - (mapbits >> LG_PAGE)) <<\n+\t\t\t    LG_PAGE));\n+\t\t\tsize_t binind;\n+\t\t\tarena_bin_info_t *bin_info;\n+\t\t\tunsigned regind;\n+\n+\t\t\tbinind = arena_ptr_small_binind_get(ptr, mapbits);\n+\t\t\tbin_info = &arena_bin_info[binind];\n+\t\t\tregind = arena_run_regind(run, bin_info, ptr);\n+\n+\t\t\t*((prof_ctx_t **)((uintptr_t)run + bin_info->ctx0_offset\n+\t\t\t    + (regind * sizeof(prof_ctx_t *)))) = ctx;\n+\t\t} else\n+\t\t\tassert((uintptr_t)ctx == (uintptr_t)1U);\n+\t} else\n+\t\tarena_mapp_get(chunk, pageind)->prof_ctx = ctx;\n }\n \n JEMALLOC_ALWAYS_INLINE void *\n arena_malloc(arena_t *arena, size_t size, bool zero, bool try_tcache)\n {\n-    tcache_t *tcache;\n-\n-    assert(size != 0);\n-    assert(size <= arena_maxclass);\n-\n-    if (size <= SMALL_MAXCLASS) {\n-        if (try_tcache && (tcache = tcache_get(true)) != NULL)\n-            return (tcache_alloc_small(tcache, size, zero));\n-        else {\n-            return (arena_malloc_small(choose_arena(arena), size,\n-                zero));\n-        }\n-    } else {\n-        /*\n-         * Initialize tcache after checking size in order to avoid\n-         * infinite recursion during tcache initialization.\n-         */\n-        if (try_tcache && size <= tcache_maxclass && (tcache =\n-            tcache_get(true)) != NULL)\n-            return (tcache_alloc_large(tcache, size, zero));\n-        else {\n-            return (arena_malloc_large(choose_arena(arena), size,\n-                zero));\n-        }\n-    }\n+\ttcache_t *tcache;\n+\n+\tassert(size != 0);\n+\tassert(size <= arena_maxclass);\n+\n+\tif (size <= SMALL_MAXCLASS) {\n+\t\tif (try_tcache && (tcache = tcache_get(true)) != NULL)\n+\t\t\treturn (tcache_alloc_small(tcache, size, zero));\n+\t\telse {\n+\t\t\treturn (arena_malloc_small(choose_arena(arena), size,\n+\t\t\t    zero));\n+\t\t}\n+\t} else {\n+\t\t/*\n+\t\t * Initialize tcache after checking size in order to avoid\n+\t\t * infinite recursion during tcache initialization.\n+\t\t */\n+\t\tif (try_tcache && size <= tcache_maxclass && (tcache =\n+\t\t    tcache_get(true)) != NULL)\n+\t\t\treturn (tcache_alloc_large(tcache, size, zero));\n+\t\telse {\n+\t\t\treturn (arena_malloc_large(choose_arena(arena), size,\n+\t\t\t    zero));\n+\t\t}\n+\t}\n }\n \n /* Return the size of the allocation pointed to by ptr. */\n JEMALLOC_ALWAYS_INLINE size_t\n arena_salloc(const void *ptr, bool demote)\n {\n-    size_t ret;\n-    arena_chunk_t *chunk;\n-    size_t pageind, binind;\n-\n-    assert(ptr != NULL);\n-    assert(CHUNK_ADDR2BASE(ptr) != ptr);\n-\n-    chunk = (arena_chunk_t *)CHUNK_ADDR2BASE(ptr);\n-    pageind = ((uintptr_t)ptr - (uintptr_t)chunk) >> LG_PAGE;\n-    assert(arena_mapbits_allocated_get(chunk, pageind) != 0);\n-    binind = arena_mapbits_binind_get(chunk, pageind);\n-    if (binind == BININD_INVALID || (config_prof && demote == false &&\n-        prof_promote && arena_mapbits_large_get(chunk, pageind) != 0)) {\n-        /*\n-         * Large allocation.  In the common case (demote == true), and\n-         * as this is an inline function, most callers will only end up\n-         * looking at binind to determine that ptr is a small\n-         * allocation.\n-         */\n-        assert(((uintptr_t)ptr & PAGE_MASK) == 0);\n-        ret = arena_mapbits_large_size_get(chunk, pageind);\n-        assert(ret != 0);\n-        assert(pageind + (ret>>LG_PAGE) <= chunk_npages);\n-        assert(ret == PAGE || arena_mapbits_large_size_get(chunk,\n-            pageind+(ret>>LG_PAGE)-1) == 0);\n-        assert(binind == arena_mapbits_binind_get(chunk,\n-            pageind+(ret>>LG_PAGE)-1));\n-        assert(arena_mapbits_dirty_get(chunk, pageind) ==\n-            arena_mapbits_dirty_get(chunk, pageind+(ret>>LG_PAGE)-1));\n-    } else {\n-        /*\n-         * Small allocation (possibly promoted to a large object due to\n-         * prof_promote).\n-         */\n-        assert(arena_mapbits_large_get(chunk, pageind) != 0 ||\n-            arena_ptr_small_binind_get(ptr, arena_mapbits_get(chunk,\n-            pageind)) == binind);\n-        ret = arena_bin_info[binind].reg_size;\n-    }\n-\n-    return (ret);\n+\tsize_t ret;\n+\tarena_chunk_t *chunk;\n+\tsize_t pageind, binind;\n+\n+\tassert(ptr != NULL);\n+\tassert(CHUNK_ADDR2BASE(ptr) != ptr);\n+\n+\tchunk = (arena_chunk_t *)CHUNK_ADDR2BASE(ptr);\n+\tpageind = ((uintptr_t)ptr - (uintptr_t)chunk) >> LG_PAGE;\n+\tassert(arena_mapbits_allocated_get(chunk, pageind) != 0);\n+\tbinind = arena_mapbits_binind_get(chunk, pageind);\n+\tif (binind == BININD_INVALID || (config_prof && demote == false &&\n+\t    prof_promote && arena_mapbits_large_get(chunk, pageind) != 0)) {\n+\t\t/*\n+\t\t * Large allocation.  In the common case (demote == true), and\n+\t\t * as this is an inline function, most callers will only end up\n+\t\t * looking at binind to determine that ptr is a small\n+\t\t * allocation.\n+\t\t */\n+\t\tassert(((uintptr_t)ptr & PAGE_MASK) == 0);\n+\t\tret = arena_mapbits_large_size_get(chunk, pageind);\n+\t\tassert(ret != 0);\n+\t\tassert(pageind + (ret>>LG_PAGE) <= chunk_npages);\n+\t\tassert(ret == PAGE || arena_mapbits_large_size_get(chunk,\n+\t\t    pageind+(ret>>LG_PAGE)-1) == 0);\n+\t\tassert(binind == arena_mapbits_binind_get(chunk,\n+\t\t    pageind+(ret>>LG_PAGE)-1));\n+\t\tassert(arena_mapbits_dirty_get(chunk, pageind) ==\n+\t\t    arena_mapbits_dirty_get(chunk, pageind+(ret>>LG_PAGE)-1));\n+\t} else {\n+\t\t/*\n+\t\t * Small allocation (possibly promoted to a large object due to\n+\t\t * prof_promote).\n+\t\t */\n+\t\tassert(arena_mapbits_large_get(chunk, pageind) != 0 ||\n+\t\t    arena_ptr_small_binind_get(ptr, arena_mapbits_get(chunk,\n+\t\t    pageind)) == binind);\n+\t\tret = arena_bin_info[binind].reg_size;\n+\t}\n+\n+\treturn (ret);\n }\n \n JEMALLOC_ALWAYS_INLINE void\n arena_dalloc(arena_t *arena, arena_chunk_t *chunk, void *ptr, bool try_tcache)\n {\n-    size_t pageind, mapbits;\n-    tcache_t *tcache;\n-\n-    assert(arena != NULL);\n-    assert(chunk->arena == arena);\n-    assert(ptr != NULL);\n-    assert(CHUNK_ADDR2BASE(ptr) != ptr);\n-\n-    pageind = ((uintptr_t)ptr - (uintptr_t)chunk) >> LG_PAGE;\n-    mapbits = arena_mapbits_get(chunk, pageind);\n-    assert(arena_mapbits_allocated_get(chunk, pageind) != 0);\n-    if ((mapbits & CHUNK_MAP_LARGE) == 0) {\n-        /* Small allocation. */\n-        if (try_tcache && (tcache = tcache_get(false)) != NULL) {\n-            size_t binind;\n-\n-            binind = arena_ptr_small_binind_get(ptr, mapbits);\n-            tcache_dalloc_small(tcache, ptr, binind);\n-        } else\n-            arena_dalloc_small(arena, chunk, ptr, pageind);\n-    } else {\n-        size_t size = arena_mapbits_large_size_get(chunk, pageind);\n-\n-        assert(((uintptr_t)ptr & PAGE_MASK) == 0);\n-\n-        if (try_tcache && size <= tcache_maxclass && (tcache =\n-            tcache_get(false)) != NULL) {\n-            tcache_dalloc_large(tcache, ptr, size);\n-        } else\n-            arena_dalloc_large(arena, chunk, ptr);\n-    }\n+\tsize_t pageind, mapbits;\n+\ttcache_t *tcache;\n+\n+\tassert(arena != NULL);\n+\tassert(chunk->arena == arena);\n+\tassert(ptr != NULL);\n+\tassert(CHUNK_ADDR2BASE(ptr) != ptr);\n+\n+\tpageind = ((uintptr_t)ptr - (uintptr_t)chunk) >> LG_PAGE;\n+\tmapbits = arena_mapbits_get(chunk, pageind);\n+\tassert(arena_mapbits_allocated_get(chunk, pageind) != 0);\n+\tif ((mapbits & CHUNK_MAP_LARGE) == 0) {\n+\t\t/* Small allocation. */\n+\t\tif (try_tcache && (tcache = tcache_get(false)) != NULL) {\n+\t\t\tsize_t binind;\n+\n+\t\t\tbinind = arena_ptr_small_binind_get(ptr, mapbits);\n+\t\t\ttcache_dalloc_small(tcache, ptr, binind);\n+\t\t} else\n+\t\t\tarena_dalloc_small(arena, chunk, ptr, pageind);\n+\t} else {\n+\t\tsize_t size = arena_mapbits_large_size_get(chunk, pageind);\n+\n+\t\tassert(((uintptr_t)ptr & PAGE_MASK) == 0);\n+\n+\t\tif (try_tcache && size <= tcache_maxclass && (tcache =\n+\t\t    tcache_get(false)) != NULL) {\n+\t\t\ttcache_dalloc_large(tcache, ptr, size);\n+\t\t} else\n+\t\t\tarena_dalloc_large(arena, chunk, ptr);\n+\t}\n }\n #  endif /* JEMALLOC_ARENA_INLINE_B */\n #endif"}, {"sha": "11a7b47fe0f991f17d48a3f62946211ea292fc3e", "filename": "src/rt/jemalloc/include/jemalloc/internal/atomic.h", "status": "modified", "additions": 64, "deletions": 64, "changes": 128, "blob_url": "https://github.com/rust-lang/rust/blob/996301331e2d8ac7611d15ca82cc3ebc1abb632d/src%2Frt%2Fjemalloc%2Finclude%2Fjemalloc%2Finternal%2Fatomic.h", "raw_url": "https://github.com/rust-lang/rust/raw/996301331e2d8ac7611d15ca82cc3ebc1abb632d/src%2Frt%2Fjemalloc%2Finclude%2Fjemalloc%2Finternal%2Fatomic.h", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Frt%2Fjemalloc%2Finclude%2Fjemalloc%2Finternal%2Fatomic.h?ref=996301331e2d8ac7611d15ca82cc3ebc1abb632d", "patch": "@@ -38,105 +38,105 @@ JEMALLOC_INLINE uint64_t\n atomic_add_uint64(uint64_t *p, uint64_t x)\n {\n \n-    return (__sync_add_and_fetch(p, x));\n+\treturn (__sync_add_and_fetch(p, x));\n }\n \n JEMALLOC_INLINE uint64_t\n atomic_sub_uint64(uint64_t *p, uint64_t x)\n {\n \n-    return (__sync_sub_and_fetch(p, x));\n+\treturn (__sync_sub_and_fetch(p, x));\n }\n #elif (defined(_MSC_VER))\n JEMALLOC_INLINE uint64_t\n atomic_add_uint64(uint64_t *p, uint64_t x)\n {\n \n-    return (InterlockedExchangeAdd64(p, x));\n+\treturn (InterlockedExchangeAdd64(p, x));\n }\n \n JEMALLOC_INLINE uint64_t\n atomic_sub_uint64(uint64_t *p, uint64_t x)\n {\n \n-    return (InterlockedExchangeAdd64(p, -((int64_t)x)));\n+\treturn (InterlockedExchangeAdd64(p, -((int64_t)x)));\n }\n #elif (defined(JEMALLOC_OSATOMIC))\n JEMALLOC_INLINE uint64_t\n atomic_add_uint64(uint64_t *p, uint64_t x)\n {\n \n-    return (OSAtomicAdd64((int64_t)x, (int64_t *)p));\n+\treturn (OSAtomicAdd64((int64_t)x, (int64_t *)p));\n }\n \n JEMALLOC_INLINE uint64_t\n atomic_sub_uint64(uint64_t *p, uint64_t x)\n {\n \n-    return (OSAtomicAdd64(-((int64_t)x), (int64_t *)p));\n+\treturn (OSAtomicAdd64(-((int64_t)x), (int64_t *)p));\n }\n #  elif (defined(__amd64__) || defined(__x86_64__))\n JEMALLOC_INLINE uint64_t\n atomic_add_uint64(uint64_t *p, uint64_t x)\n {\n \n-    asm volatile (\n-        \"lock; xaddq %0, %1;\"\n-        : \"+r\" (x), \"=m\" (*p) /* Outputs. */\n-        : \"m\" (*p) /* Inputs. */\n-        );\n+\tasm volatile (\n+\t    \"lock; xaddq %0, %1;\"\n+\t    : \"+r\" (x), \"=m\" (*p) /* Outputs. */\n+\t    : \"m\" (*p) /* Inputs. */\n+\t    );\n \n-    return (x);\n+\treturn (x);\n }\n \n JEMALLOC_INLINE uint64_t\n atomic_sub_uint64(uint64_t *p, uint64_t x)\n {\n \n-    x = (uint64_t)(-(int64_t)x);\n-    asm volatile (\n-        \"lock; xaddq %0, %1;\"\n-        : \"+r\" (x), \"=m\" (*p) /* Outputs. */\n-        : \"m\" (*p) /* Inputs. */\n-        );\n+\tx = (uint64_t)(-(int64_t)x);\n+\tasm volatile (\n+\t    \"lock; xaddq %0, %1;\"\n+\t    : \"+r\" (x), \"=m\" (*p) /* Outputs. */\n+\t    : \"m\" (*p) /* Inputs. */\n+\t    );\n \n-    return (x);\n+\treturn (x);\n }\n #  elif (defined(JEMALLOC_ATOMIC9))\n JEMALLOC_INLINE uint64_t\n atomic_add_uint64(uint64_t *p, uint64_t x)\n {\n \n-    /*\n-     * atomic_fetchadd_64() doesn't exist, but we only ever use this\n-     * function on LP64 systems, so atomic_fetchadd_long() will do.\n-     */\n-    assert(sizeof(uint64_t) == sizeof(unsigned long));\n+\t/*\n+\t * atomic_fetchadd_64() doesn't exist, but we only ever use this\n+\t * function on LP64 systems, so atomic_fetchadd_long() will do.\n+\t */\n+\tassert(sizeof(uint64_t) == sizeof(unsigned long));\n \n-    return (atomic_fetchadd_long(p, (unsigned long)x) + x);\n+\treturn (atomic_fetchadd_long(p, (unsigned long)x) + x);\n }\n \n JEMALLOC_INLINE uint64_t\n atomic_sub_uint64(uint64_t *p, uint64_t x)\n {\n \n-    assert(sizeof(uint64_t) == sizeof(unsigned long));\n+\tassert(sizeof(uint64_t) == sizeof(unsigned long));\n \n-    return (atomic_fetchadd_long(p, (unsigned long)(-(long)x)) - x);\n+\treturn (atomic_fetchadd_long(p, (unsigned long)(-(long)x)) - x);\n }\n #  elif (defined(JE_FORCE_SYNC_COMPARE_AND_SWAP_8))\n JEMALLOC_INLINE uint64_t\n atomic_add_uint64(uint64_t *p, uint64_t x)\n {\n \n-    return (__sync_add_and_fetch(p, x));\n+\treturn (__sync_add_and_fetch(p, x));\n }\n \n JEMALLOC_INLINE uint64_t\n atomic_sub_uint64(uint64_t *p, uint64_t x)\n {\n \n-    return (__sync_sub_and_fetch(p, x));\n+\treturn (__sync_sub_and_fetch(p, x));\n }\n #  else\n #    error \"Missing implementation for 64-bit atomic operations\"\n@@ -150,97 +150,97 @@ JEMALLOC_INLINE uint32_t\n atomic_add_uint32(uint32_t *p, uint32_t x)\n {\n \n-    return (__sync_add_and_fetch(p, x));\n+\treturn (__sync_add_and_fetch(p, x));\n }\n \n JEMALLOC_INLINE uint32_t\n atomic_sub_uint32(uint32_t *p, uint32_t x)\n {\n \n-    return (__sync_sub_and_fetch(p, x));\n+\treturn (__sync_sub_and_fetch(p, x));\n }\n #elif (defined(_MSC_VER))\n JEMALLOC_INLINE uint32_t\n atomic_add_uint32(uint32_t *p, uint32_t x)\n {\n \n-    return (InterlockedExchangeAdd(p, x));\n+\treturn (InterlockedExchangeAdd(p, x));\n }\n \n JEMALLOC_INLINE uint32_t\n atomic_sub_uint32(uint32_t *p, uint32_t x)\n {\n \n-    return (InterlockedExchangeAdd(p, -((int32_t)x)));\n+\treturn (InterlockedExchangeAdd(p, -((int32_t)x)));\n }\n #elif (defined(JEMALLOC_OSATOMIC))\n JEMALLOC_INLINE uint32_t\n atomic_add_uint32(uint32_t *p, uint32_t x)\n {\n \n-    return (OSAtomicAdd32((int32_t)x, (int32_t *)p));\n+\treturn (OSAtomicAdd32((int32_t)x, (int32_t *)p));\n }\n \n JEMALLOC_INLINE uint32_t\n atomic_sub_uint32(uint32_t *p, uint32_t x)\n {\n \n-    return (OSAtomicAdd32(-((int32_t)x), (int32_t *)p));\n+\treturn (OSAtomicAdd32(-((int32_t)x), (int32_t *)p));\n }\n #elif (defined(__i386__) || defined(__amd64__) || defined(__x86_64__))\n JEMALLOC_INLINE uint32_t\n atomic_add_uint32(uint32_t *p, uint32_t x)\n {\n \n-    asm volatile (\n-        \"lock; xaddl %0, %1;\"\n-        : \"+r\" (x), \"=m\" (*p) /* Outputs. */\n-        : \"m\" (*p) /* Inputs. */\n-        );\n+\tasm volatile (\n+\t    \"lock; xaddl %0, %1;\"\n+\t    : \"+r\" (x), \"=m\" (*p) /* Outputs. */\n+\t    : \"m\" (*p) /* Inputs. */\n+\t    );\n \n-    return (x);\n+\treturn (x);\n }\n \n JEMALLOC_INLINE uint32_t\n atomic_sub_uint32(uint32_t *p, uint32_t x)\n {\n \n-    x = (uint32_t)(-(int32_t)x);\n-    asm volatile (\n-        \"lock; xaddl %0, %1;\"\n-        : \"+r\" (x), \"=m\" (*p) /* Outputs. */\n-        : \"m\" (*p) /* Inputs. */\n-        );\n+\tx = (uint32_t)(-(int32_t)x);\n+\tasm volatile (\n+\t    \"lock; xaddl %0, %1;\"\n+\t    : \"+r\" (x), \"=m\" (*p) /* Outputs. */\n+\t    : \"m\" (*p) /* Inputs. */\n+\t    );\n \n-    return (x);\n+\treturn (x);\n }\n #elif (defined(JEMALLOC_ATOMIC9))\n JEMALLOC_INLINE uint32_t\n atomic_add_uint32(uint32_t *p, uint32_t x)\n {\n \n-    return (atomic_fetchadd_32(p, x) + x);\n+\treturn (atomic_fetchadd_32(p, x) + x);\n }\n \n JEMALLOC_INLINE uint32_t\n atomic_sub_uint32(uint32_t *p, uint32_t x)\n {\n \n-    return (atomic_fetchadd_32(p, (uint32_t)(-(int32_t)x)) - x);\n+\treturn (atomic_fetchadd_32(p, (uint32_t)(-(int32_t)x)) - x);\n }\n #elif (defined(JE_FORCE_SYNC_COMPARE_AND_SWAP_4))\n JEMALLOC_INLINE uint32_t\n atomic_add_uint32(uint32_t *p, uint32_t x)\n {\n \n-    return (__sync_add_and_fetch(p, x));\n+\treturn (__sync_add_and_fetch(p, x));\n }\n \n JEMALLOC_INLINE uint32_t\n atomic_sub_uint32(uint32_t *p, uint32_t x)\n {\n \n-    return (__sync_sub_and_fetch(p, x));\n+\treturn (__sync_sub_and_fetch(p, x));\n }\n #else\n #  error \"Missing implementation for 32-bit atomic operations\"\n@@ -253,9 +253,9 @@ atomic_add_z(size_t *p, size_t x)\n {\n \n #if (LG_SIZEOF_PTR == 3)\n-    return ((size_t)atomic_add_uint64((uint64_t *)p, (uint64_t)x));\n+\treturn ((size_t)atomic_add_uint64((uint64_t *)p, (uint64_t)x));\n #elif (LG_SIZEOF_PTR == 2)\n-    return ((size_t)atomic_add_uint32((uint32_t *)p, (uint32_t)x));\n+\treturn ((size_t)atomic_add_uint32((uint32_t *)p, (uint32_t)x));\n #endif\n }\n \n@@ -264,11 +264,11 @@ atomic_sub_z(size_t *p, size_t x)\n {\n \n #if (LG_SIZEOF_PTR == 3)\n-    return ((size_t)atomic_add_uint64((uint64_t *)p,\n-        (uint64_t)-((int64_t)x)));\n+\treturn ((size_t)atomic_add_uint64((uint64_t *)p,\n+\t    (uint64_t)-((int64_t)x)));\n #elif (LG_SIZEOF_PTR == 2)\n-    return ((size_t)atomic_add_uint32((uint32_t *)p,\n-        (uint32_t)-((int32_t)x)));\n+\treturn ((size_t)atomic_add_uint32((uint32_t *)p,\n+\t    (uint32_t)-((int32_t)x)));\n #endif\n }\n \n@@ -279,9 +279,9 @@ atomic_add_u(unsigned *p, unsigned x)\n {\n \n #if (LG_SIZEOF_INT == 3)\n-    return ((unsigned)atomic_add_uint64((uint64_t *)p, (uint64_t)x));\n+\treturn ((unsigned)atomic_add_uint64((uint64_t *)p, (uint64_t)x));\n #elif (LG_SIZEOF_INT == 2)\n-    return ((unsigned)atomic_add_uint32((uint32_t *)p, (uint32_t)x));\n+\treturn ((unsigned)atomic_add_uint32((uint32_t *)p, (uint32_t)x));\n #endif\n }\n \n@@ -290,11 +290,11 @@ atomic_sub_u(unsigned *p, unsigned x)\n {\n \n #if (LG_SIZEOF_INT == 3)\n-    return ((unsigned)atomic_add_uint64((uint64_t *)p,\n-        (uint64_t)-((int64_t)x)));\n+\treturn ((unsigned)atomic_add_uint64((uint64_t *)p,\n+\t    (uint64_t)-((int64_t)x)));\n #elif (LG_SIZEOF_INT == 2)\n-    return ((unsigned)atomic_add_uint32((uint32_t *)p,\n-        (uint32_t)-((int32_t)x)));\n+\treturn ((unsigned)atomic_add_uint32((uint32_t *)p,\n+\t    (uint32_t)-((int32_t)x)));\n #endif\n }\n /******************************************************************************/"}, {"sha": "605ebac58c17a2650cd7cbe65a9b0ae795dee7f6", "filename": "src/rt/jemalloc/include/jemalloc/internal/bitmap.h", "status": "modified", "additions": 98, "deletions": 98, "changes": 196, "blob_url": "https://github.com/rust-lang/rust/blob/996301331e2d8ac7611d15ca82cc3ebc1abb632d/src%2Frt%2Fjemalloc%2Finclude%2Fjemalloc%2Finternal%2Fbitmap.h", "raw_url": "https://github.com/rust-lang/rust/raw/996301331e2d8ac7611d15ca82cc3ebc1abb632d/src%2Frt%2Fjemalloc%2Finclude%2Fjemalloc%2Finternal%2Fbitmap.h", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Frt%2Fjemalloc%2Finclude%2Fjemalloc%2Finternal%2Fbitmap.h?ref=996301331e2d8ac7611d15ca82cc3ebc1abb632d", "patch": "@@ -24,22 +24,22 @@ typedef unsigned long bitmap_t;\n #ifdef JEMALLOC_H_STRUCTS\n \n struct bitmap_level_s {\n-    /* Offset of this level's groups within the array of groups. */\n-    size_t group_offset;\n+\t/* Offset of this level's groups within the array of groups. */\n+\tsize_t group_offset;\n };\n \n struct bitmap_info_s {\n-    /* Logical number of bits in bitmap (stored at bottom level). */\n-    size_t nbits;\n+\t/* Logical number of bits in bitmap (stored at bottom level). */\n+\tsize_t nbits;\n \n-    /* Number of levels necessary for nbits. */\n-    unsigned nlevels;\n+\t/* Number of levels necessary for nbits. */\n+\tunsigned nlevels;\n \n-    /*\n-     * Only the first (nlevels+1) elements are used, and levels are ordered\n-     * bottom to top (e.g. the bottom level is stored in levels[0]).\n-     */\n-    bitmap_level_t levels[BITMAP_MAX_LEVELS+1];\n+\t/*\n+\t * Only the first (nlevels+1) elements are used, and levels are ordered\n+\t * bottom to top (e.g. the bottom level is stored in levels[0]).\n+\t */\n+\tbitmap_level_t levels[BITMAP_MAX_LEVELS+1];\n };\n \n #endif /* JEMALLOC_H_STRUCTS */\n@@ -67,115 +67,115 @@ void\tbitmap_unset(bitmap_t *bitmap, const bitmap_info_t *binfo, size_t bit);\n JEMALLOC_INLINE bool\n bitmap_full(bitmap_t *bitmap, const bitmap_info_t *binfo)\n {\n-    unsigned rgoff = binfo->levels[binfo->nlevels].group_offset - 1;\n-    bitmap_t rg = bitmap[rgoff];\n-    /* The bitmap is full iff the root group is 0. */\n-    return (rg == 0);\n+\tunsigned rgoff = binfo->levels[binfo->nlevels].group_offset - 1;\n+\tbitmap_t rg = bitmap[rgoff];\n+\t/* The bitmap is full iff the root group is 0. */\n+\treturn (rg == 0);\n }\n \n JEMALLOC_INLINE bool\n bitmap_get(bitmap_t *bitmap, const bitmap_info_t *binfo, size_t bit)\n {\n-    size_t goff;\n-    bitmap_t g;\n+\tsize_t goff;\n+\tbitmap_t g;\n \n-    assert(bit < binfo->nbits);\n-    goff = bit >> LG_BITMAP_GROUP_NBITS;\n-    g = bitmap[goff];\n-    return (!(g & (1LU << (bit & BITMAP_GROUP_NBITS_MASK))));\n+\tassert(bit < binfo->nbits);\n+\tgoff = bit >> LG_BITMAP_GROUP_NBITS;\n+\tg = bitmap[goff];\n+\treturn (!(g & (1LU << (bit & BITMAP_GROUP_NBITS_MASK))));\n }\n \n JEMALLOC_INLINE void\n bitmap_set(bitmap_t *bitmap, const bitmap_info_t *binfo, size_t bit)\n {\n-    size_t goff;\n-    bitmap_t *gp;\n-    bitmap_t g;\n-\n-    assert(bit < binfo->nbits);\n-    assert(bitmap_get(bitmap, binfo, bit) == false);\n-    goff = bit >> LG_BITMAP_GROUP_NBITS;\n-    gp = &bitmap[goff];\n-    g = *gp;\n-    assert(g & (1LU << (bit & BITMAP_GROUP_NBITS_MASK)));\n-    g ^= 1LU << (bit & BITMAP_GROUP_NBITS_MASK);\n-    *gp = g;\n-    assert(bitmap_get(bitmap, binfo, bit));\n-    /* Propagate group state transitions up the tree. */\n-    if (g == 0) {\n-        unsigned i;\n-        for (i = 1; i < binfo->nlevels; i++) {\n-            bit = goff;\n-            goff = bit >> LG_BITMAP_GROUP_NBITS;\n-            gp = &bitmap[binfo->levels[i].group_offset + goff];\n-            g = *gp;\n-            assert(g & (1LU << (bit & BITMAP_GROUP_NBITS_MASK)));\n-            g ^= 1LU << (bit & BITMAP_GROUP_NBITS_MASK);\n-            *gp = g;\n-            if (g != 0)\n-                break;\n-        }\n-    }\n+\tsize_t goff;\n+\tbitmap_t *gp;\n+\tbitmap_t g;\n+\n+\tassert(bit < binfo->nbits);\n+\tassert(bitmap_get(bitmap, binfo, bit) == false);\n+\tgoff = bit >> LG_BITMAP_GROUP_NBITS;\n+\tgp = &bitmap[goff];\n+\tg = *gp;\n+\tassert(g & (1LU << (bit & BITMAP_GROUP_NBITS_MASK)));\n+\tg ^= 1LU << (bit & BITMAP_GROUP_NBITS_MASK);\n+\t*gp = g;\n+\tassert(bitmap_get(bitmap, binfo, bit));\n+\t/* Propagate group state transitions up the tree. */\n+\tif (g == 0) {\n+\t\tunsigned i;\n+\t\tfor (i = 1; i < binfo->nlevels; i++) {\n+\t\t\tbit = goff;\n+\t\t\tgoff = bit >> LG_BITMAP_GROUP_NBITS;\n+\t\t\tgp = &bitmap[binfo->levels[i].group_offset + goff];\n+\t\t\tg = *gp;\n+\t\t\tassert(g & (1LU << (bit & BITMAP_GROUP_NBITS_MASK)));\n+\t\t\tg ^= 1LU << (bit & BITMAP_GROUP_NBITS_MASK);\n+\t\t\t*gp = g;\n+\t\t\tif (g != 0)\n+\t\t\t\tbreak;\n+\t\t}\n+\t}\n }\n \n /* sfu: set first unset. */\n JEMALLOC_INLINE size_t\n bitmap_sfu(bitmap_t *bitmap, const bitmap_info_t *binfo)\n {\n-    size_t bit;\n-    bitmap_t g;\n-    unsigned i;\n-\n-    assert(bitmap_full(bitmap, binfo) == false);\n-\n-    i = binfo->nlevels - 1;\n-    g = bitmap[binfo->levels[i].group_offset];\n-    bit = ffsl(g) - 1;\n-    while (i > 0) {\n-        i--;\n-        g = bitmap[binfo->levels[i].group_offset + bit];\n-        bit = (bit << LG_BITMAP_GROUP_NBITS) + (ffsl(g) - 1);\n-    }\n-\n-    bitmap_set(bitmap, binfo, bit);\n-    return (bit);\n+\tsize_t bit;\n+\tbitmap_t g;\n+\tunsigned i;\n+\n+\tassert(bitmap_full(bitmap, binfo) == false);\n+\n+\ti = binfo->nlevels - 1;\n+\tg = bitmap[binfo->levels[i].group_offset];\n+\tbit = ffsl(g) - 1;\n+\twhile (i > 0) {\n+\t\ti--;\n+\t\tg = bitmap[binfo->levels[i].group_offset + bit];\n+\t\tbit = (bit << LG_BITMAP_GROUP_NBITS) + (ffsl(g) - 1);\n+\t}\n+\n+\tbitmap_set(bitmap, binfo, bit);\n+\treturn (bit);\n }\n \n JEMALLOC_INLINE void\n bitmap_unset(bitmap_t *bitmap, const bitmap_info_t *binfo, size_t bit)\n {\n-    size_t goff;\n-    bitmap_t *gp;\n-    bitmap_t g;\n-    bool propagate;\n-\n-    assert(bit < binfo->nbits);\n-    assert(bitmap_get(bitmap, binfo, bit));\n-    goff = bit >> LG_BITMAP_GROUP_NBITS;\n-    gp = &bitmap[goff];\n-    g = *gp;\n-    propagate = (g == 0);\n-    assert((g & (1LU << (bit & BITMAP_GROUP_NBITS_MASK))) == 0);\n-    g ^= 1LU << (bit & BITMAP_GROUP_NBITS_MASK);\n-    *gp = g;\n-    assert(bitmap_get(bitmap, binfo, bit) == false);\n-    /* Propagate group state transitions up the tree. */\n-    if (propagate) {\n-        unsigned i;\n-        for (i = 1; i < binfo->nlevels; i++) {\n-            bit = goff;\n-            goff = bit >> LG_BITMAP_GROUP_NBITS;\n-            gp = &bitmap[binfo->levels[i].group_offset + goff];\n-            g = *gp;\n-            propagate = (g == 0);\n-            assert((g & (1LU << (bit & BITMAP_GROUP_NBITS_MASK)))\n-                == 0);\n-            g ^= 1LU << (bit & BITMAP_GROUP_NBITS_MASK);\n-            *gp = g;\n-            if (propagate == false)\n-                break;\n-        }\n-    }\n+\tsize_t goff;\n+\tbitmap_t *gp;\n+\tbitmap_t g;\n+\tbool propagate;\n+\n+\tassert(bit < binfo->nbits);\n+\tassert(bitmap_get(bitmap, binfo, bit));\n+\tgoff = bit >> LG_BITMAP_GROUP_NBITS;\n+\tgp = &bitmap[goff];\n+\tg = *gp;\n+\tpropagate = (g == 0);\n+\tassert((g & (1LU << (bit & BITMAP_GROUP_NBITS_MASK))) == 0);\n+\tg ^= 1LU << (bit & BITMAP_GROUP_NBITS_MASK);\n+\t*gp = g;\n+\tassert(bitmap_get(bitmap, binfo, bit) == false);\n+\t/* Propagate group state transitions up the tree. */\n+\tif (propagate) {\n+\t\tunsigned i;\n+\t\tfor (i = 1; i < binfo->nlevels; i++) {\n+\t\t\tbit = goff;\n+\t\t\tgoff = bit >> LG_BITMAP_GROUP_NBITS;\n+\t\t\tgp = &bitmap[binfo->levels[i].group_offset + goff];\n+\t\t\tg = *gp;\n+\t\t\tpropagate = (g == 0);\n+\t\t\tassert((g & (1LU << (bit & BITMAP_GROUP_NBITS_MASK)))\n+\t\t\t    == 0);\n+\t\t\tg ^= 1LU << (bit & BITMAP_GROUP_NBITS_MASK);\n+\t\t\t*gp = g;\n+\t\t\tif (propagate == false)\n+\t\t\t\tbreak;\n+\t\t}\n+\t}\n }\n \n #endif"}, {"sha": "87d8700dac8adad1510d3b4a1cd978d7fe6b05e4", "filename": "src/rt/jemalloc/include/jemalloc/internal/chunk.h", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "blob_url": "https://github.com/rust-lang/rust/blob/996301331e2d8ac7611d15ca82cc3ebc1abb632d/src%2Frt%2Fjemalloc%2Finclude%2Fjemalloc%2Finternal%2Fchunk.h", "raw_url": "https://github.com/rust-lang/rust/raw/996301331e2d8ac7611d15ca82cc3ebc1abb632d/src%2Frt%2Fjemalloc%2Finclude%2Fjemalloc%2Finternal%2Fchunk.h", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Frt%2Fjemalloc%2Finclude%2Fjemalloc%2Finternal%2Fchunk.h?ref=996301331e2d8ac7611d15ca82cc3ebc1abb632d", "patch": "@@ -9,15 +9,15 @@\n \n /* Return the chunk address for allocation address a. */\n #define\tCHUNK_ADDR2BASE(a)\t\t\t\t\t\t\\\n-    ((void *)((uintptr_t)(a) & ~chunksize_mask))\n+\t((void *)((uintptr_t)(a) & ~chunksize_mask))\n \n /* Return the chunk offset of address a. */\n #define\tCHUNK_ADDR2OFFSET(a)\t\t\t\t\t\t\\\n-    ((size_t)((uintptr_t)(a) & chunksize_mask))\n+\t((size_t)((uintptr_t)(a) & chunksize_mask))\n \n /* Return the smallest chunk multiple that is >= s. */\n #define\tCHUNK_CEILING(s)\t\t\t\t\t\t\\\n-    (((s) + chunksize_mask) & ~chunksize_mask)\n+\t(((s) + chunksize_mask) & ~chunksize_mask)\n \n #endif /* JEMALLOC_H_TYPES */\n /******************************************************************************/"}, {"sha": "6585f071bbecd9ca9e8cbd193a767d21b18a7465", "filename": "src/rt/jemalloc/include/jemalloc/internal/chunk_dss.h", "status": "modified", "additions": 4, "deletions": 4, "changes": 8, "blob_url": "https://github.com/rust-lang/rust/blob/996301331e2d8ac7611d15ca82cc3ebc1abb632d/src%2Frt%2Fjemalloc%2Finclude%2Fjemalloc%2Finternal%2Fchunk_dss.h", "raw_url": "https://github.com/rust-lang/rust/raw/996301331e2d8ac7611d15ca82cc3ebc1abb632d/src%2Frt%2Fjemalloc%2Finclude%2Fjemalloc%2Finternal%2Fchunk_dss.h", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Frt%2Fjemalloc%2Finclude%2Fjemalloc%2Finternal%2Fchunk_dss.h?ref=996301331e2d8ac7611d15ca82cc3ebc1abb632d", "patch": "@@ -2,11 +2,11 @@\n #ifdef JEMALLOC_H_TYPES\n \n typedef enum {\n-    dss_prec_disabled  = 0,\n-    dss_prec_primary   = 1,\n-    dss_prec_secondary = 2,\n+\tdss_prec_disabled  = 0,\n+\tdss_prec_primary   = 1,\n+\tdss_prec_secondary = 2,\n \n-    dss_prec_limit     = 3\n+\tdss_prec_limit     = 3\n } dss_prec_t ;\n #define\tDSS_PREC_DEFAULT\tdss_prec_secondary\n #define\tDSS_DEFAULT\t\t\"secondary\""}, {"sha": "50c39ed958192a39b3c4359ad454a487e5e596fd", "filename": "src/rt/jemalloc/include/jemalloc/internal/ckh.h", "status": "modified", "additions": 23, "deletions": 23, "changes": 46, "blob_url": "https://github.com/rust-lang/rust/blob/996301331e2d8ac7611d15ca82cc3ebc1abb632d/src%2Frt%2Fjemalloc%2Finclude%2Fjemalloc%2Finternal%2Fckh.h", "raw_url": "https://github.com/rust-lang/rust/raw/996301331e2d8ac7611d15ca82cc3ebc1abb632d/src%2Frt%2Fjemalloc%2Finclude%2Fjemalloc%2Finternal%2Fckh.h", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Frt%2Fjemalloc%2Finclude%2Fjemalloc%2Finternal%2Fckh.h?ref=996301331e2d8ac7611d15ca82cc3ebc1abb632d", "patch": "@@ -25,41 +25,41 @@ typedef bool ckh_keycomp_t (const void *, const void *);\n \n /* Hash table cell. */\n struct ckhc_s {\n-    const void\t*key;\n-    const void\t*data;\n+\tconst void\t*key;\n+\tconst void\t*data;\n };\n \n struct ckh_s {\n #ifdef CKH_COUNT\n-    /* Counters used to get an idea of performance. */\n-    uint64_t\tngrows;\n-    uint64_t\tnshrinks;\n-    uint64_t\tnshrinkfails;\n-    uint64_t\tninserts;\n-    uint64_t\tnrelocs;\n+\t/* Counters used to get an idea of performance. */\n+\tuint64_t\tngrows;\n+\tuint64_t\tnshrinks;\n+\tuint64_t\tnshrinkfails;\n+\tuint64_t\tninserts;\n+\tuint64_t\tnrelocs;\n #endif\n \n-    /* Used for pseudo-random number generation. */\n+\t/* Used for pseudo-random number generation. */\n #define\tCKH_A\t\t1103515241\n #define\tCKH_C\t\t12347\n-    uint32_t\tprng_state;\n+\tuint32_t\tprng_state;\n \n-    /* Total number of items. */\n-    size_t\t\tcount;\n+\t/* Total number of items. */\n+\tsize_t\t\tcount;\n \n-    /*\n-     * Minimum and current number of hash table buckets.  There are\n-     * 2^LG_CKH_BUCKET_CELLS cells per bucket.\n-     */\n-    unsigned\tlg_minbuckets;\n-    unsigned\tlg_curbuckets;\n+\t/*\n+\t * Minimum and current number of hash table buckets.  There are\n+\t * 2^LG_CKH_BUCKET_CELLS cells per bucket.\n+\t */\n+\tunsigned\tlg_minbuckets;\n+\tunsigned\tlg_curbuckets;\n \n-    /* Hash and comparison functions. */\n-    ckh_hash_t\t*hash;\n-    ckh_keycomp_t\t*keycomp;\n+\t/* Hash and comparison functions. */\n+\tckh_hash_t\t*hash;\n+\tckh_keycomp_t\t*keycomp;\n \n-    /* Hash table with 2^lg_curbuckets buckets. */\n-    ckhc_t\t\t*tab;\n+\t/* Hash table with 2^lg_curbuckets buckets. */\n+\tckhc_t\t\t*tab;\n };\n \n #endif /* JEMALLOC_H_STRUCTS */"}, {"sha": "0ffecc5f2a23feeba1f5dba9547eee8e63ae0a4e", "filename": "src/rt/jemalloc/include/jemalloc/internal/ctl.h", "status": "modified", "additions": 59, "deletions": 58, "changes": 117, "blob_url": "https://github.com/rust-lang/rust/blob/996301331e2d8ac7611d15ca82cc3ebc1abb632d/src%2Frt%2Fjemalloc%2Finclude%2Fjemalloc%2Finternal%2Fctl.h", "raw_url": "https://github.com/rust-lang/rust/raw/996301331e2d8ac7611d15ca82cc3ebc1abb632d/src%2Frt%2Fjemalloc%2Finclude%2Fjemalloc%2Finternal%2Fctl.h", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Frt%2Fjemalloc%2Finclude%2Fjemalloc%2Finternal%2Fctl.h?ref=996301331e2d8ac7611d15ca82cc3ebc1abb632d", "patch": "@@ -12,58 +12,58 @@ typedef struct ctl_stats_s ctl_stats_t;\n #ifdef JEMALLOC_H_STRUCTS\n \n struct ctl_node_s {\n-    bool\t\t\tnamed;\n+\tbool\t\t\tnamed;\n };\n \n struct ctl_named_node_s {\n-    struct ctl_node_s\tnode;\n-    const char\t\t*name;\n-    /* If (nchildren == 0), this is a terminal node. */\n-    unsigned\t\tnchildren;\n-    const\t\t\tctl_node_t *children;\n-    int\t\t\t(*ctl)(const size_t *, size_t, void *, size_t *,\n-        void *, size_t);\n+\tstruct ctl_node_s\tnode;\n+\tconst char\t\t*name;\n+\t/* If (nchildren == 0), this is a terminal node. */\n+\tunsigned\t\tnchildren;\n+\tconst\t\t\tctl_node_t *children;\n+\tint\t\t\t(*ctl)(const size_t *, size_t, void *, size_t *,\n+\t    void *, size_t);\n };\n \n struct ctl_indexed_node_s {\n-    struct ctl_node_s\tnode;\n-    const ctl_named_node_t\t*(*index)(const size_t *, size_t, size_t);\n+\tstruct ctl_node_s\tnode;\n+\tconst ctl_named_node_t\t*(*index)(const size_t *, size_t, size_t);\n };\n \n struct ctl_arena_stats_s {\n-    bool\t\t\tinitialized;\n-    unsigned\t\tnthreads;\n-    const char\t\t*dss;\n-    size_t\t\t\tpactive;\n-    size_t\t\t\tpdirty;\n-    arena_stats_t\t\tastats;\n-\n-    /* Aggregate stats for small size classes, based on bin stats. */\n-    size_t\t\t\tallocated_small;\n-    uint64_t\t\tnmalloc_small;\n-    uint64_t\t\tndalloc_small;\n-    uint64_t\t\tnrequests_small;\n-\n-    malloc_bin_stats_t\tbstats[NBINS];\n-    malloc_large_stats_t\t*lstats;\t/* nlclasses elements. */\n+\tbool\t\t\tinitialized;\n+\tunsigned\t\tnthreads;\n+\tconst char\t\t*dss;\n+\tsize_t\t\t\tpactive;\n+\tsize_t\t\t\tpdirty;\n+\tarena_stats_t\t\tastats;\n+\n+\t/* Aggregate stats for small size classes, based on bin stats. */\n+\tsize_t\t\t\tallocated_small;\n+\tuint64_t\t\tnmalloc_small;\n+\tuint64_t\t\tndalloc_small;\n+\tuint64_t\t\tnrequests_small;\n+\n+\tmalloc_bin_stats_t\tbstats[NBINS];\n+\tmalloc_large_stats_t\t*lstats;\t/* nlclasses elements. */\n };\n \n struct ctl_stats_s {\n-    size_t\t\t\tallocated;\n-    size_t\t\t\tactive;\n-    size_t\t\t\tmapped;\n-    struct {\n-        size_t\t\tcurrent;\t/* stats_chunks.curchunks */\n-        uint64_t\ttotal;\t\t/* stats_chunks.nchunks */\n-        size_t\t\thigh;\t\t/* stats_chunks.highchunks */\n-    } chunks;\n-    struct {\n-        size_t\t\tallocated;\t/* huge_allocated */\n-        uint64_t\tnmalloc;\t/* huge_nmalloc */\n-        uint64_t\tndalloc;\t/* huge_ndalloc */\n-    } huge;\n-    unsigned\t\tnarenas;\n-    ctl_arena_stats_t\t*arenas;\t/* (narenas + 1) elements. */\n+\tsize_t\t\t\tallocated;\n+\tsize_t\t\t\tactive;\n+\tsize_t\t\t\tmapped;\n+\tstruct {\n+\t\tsize_t\t\tcurrent;\t/* stats_chunks.curchunks */\n+\t\tuint64_t\ttotal;\t\t/* stats_chunks.nchunks */\n+\t\tsize_t\t\thigh;\t\t/* stats_chunks.highchunks */\n+\t} chunks;\n+\tstruct {\n+\t\tsize_t\t\tallocated;\t/* huge_allocated */\n+\t\tuint64_t\tnmalloc;\t/* huge_nmalloc */\n+\t\tuint64_t\tndalloc;\t/* huge_ndalloc */\n+\t} huge;\n+\tunsigned\t\tnarenas;\n+\tctl_arena_stats_t\t*arenas;\t/* (narenas + 1) elements. */\n };\n \n #endif /* JEMALLOC_H_STRUCTS */\n@@ -82,30 +82,30 @@ void\tctl_postfork_parent(void);\n void\tctl_postfork_child(void);\n \n #define\txmallctl(name, oldp, oldlenp, newp, newlen) do {\t\t\\\n-    if (je_mallctl(name, oldp, oldlenp, newp, newlen)\t\t\\\n-        != 0) {\t\t\t\t\t\t\t\\\n-        malloc_printf(\t\t\t\t\t\t\\\n-            \"<jemalloc>: Failure in xmallctl(\\\"%s\\\", ...)\\n\",\t\\\n-            name);\t\t\t\t\t\t\\\n-        abort();\t\t\t\t\t\t\\\n-    }\t\t\t\t\t\t\t\t\\\n+\tif (je_mallctl(name, oldp, oldlenp, newp, newlen)\t\t\\\n+\t    != 0) {\t\t\t\t\t\t\t\\\n+\t\tmalloc_printf(\t\t\t\t\t\t\\\n+\t\t    \"<jemalloc>: Failure in xmallctl(\\\"%s\\\", ...)\\n\",\t\\\n+\t\t    name);\t\t\t\t\t\t\\\n+\t\tabort();\t\t\t\t\t\t\\\n+\t}\t\t\t\t\t\t\t\t\\\n } while (0)\n \n #define\txmallctlnametomib(name, mibp, miblenp) do {\t\t\t\\\n-    if (je_mallctlnametomib(name, mibp, miblenp) != 0) {\t\t\\\n-        malloc_printf(\"<jemalloc>: Failure in \"\t\t\t\\\n-            \"xmallctlnametomib(\\\"%s\\\", ...)\\n\", name);\t\t\\\n-        abort();\t\t\t\t\t\t\\\n-    }\t\t\t\t\t\t\t\t\\\n+\tif (je_mallctlnametomib(name, mibp, miblenp) != 0) {\t\t\\\n+\t\tmalloc_printf(\"<jemalloc>: Failure in \"\t\t\t\\\n+\t\t    \"xmallctlnametomib(\\\"%s\\\", ...)\\n\", name);\t\t\\\n+\t\tabort();\t\t\t\t\t\t\\\n+\t}\t\t\t\t\t\t\t\t\\\n } while (0)\n \n #define\txmallctlbymib(mib, miblen, oldp, oldlenp, newp, newlen) do {\t\\\n-    if (je_mallctlbymib(mib, miblen, oldp, oldlenp, newp,\t\t\\\n-        newlen) != 0) {\t\t\t\t\t\t\\\n-        malloc_write(\t\t\t\t\t\t\\\n-            \"<jemalloc>: Failure in xmallctlbymib()\\n\");\t\\\n-        abort();\t\t\t\t\t\t\\\n-    }\t\t\t\t\t\t\t\t\\\n+\tif (je_mallctlbymib(mib, miblen, oldp, oldlenp, newp,\t\t\\\n+\t    newlen) != 0) {\t\t\t\t\t\t\\\n+\t\tmalloc_write(\t\t\t\t\t\t\\\n+\t\t    \"<jemalloc>: Failure in xmallctlbymib()\\n\");\t\\\n+\t\tabort();\t\t\t\t\t\t\\\n+\t}\t\t\t\t\t\t\t\t\\\n } while (0)\n \n #endif /* JEMALLOC_H_EXTERNS */\n@@ -114,3 +114,4 @@ void\tctl_postfork_child(void);\n \n #endif /* JEMALLOC_H_INLINES */\n /******************************************************************************/\n+"}, {"sha": "ba95ca816bd9a81feb9bb65c3fa69f71ce10ab28", "filename": "src/rt/jemalloc/include/jemalloc/internal/extent.h", "status": "modified", "additions": 13, "deletions": 12, "changes": 25, "blob_url": "https://github.com/rust-lang/rust/blob/996301331e2d8ac7611d15ca82cc3ebc1abb632d/src%2Frt%2Fjemalloc%2Finclude%2Fjemalloc%2Finternal%2Fextent.h", "raw_url": "https://github.com/rust-lang/rust/raw/996301331e2d8ac7611d15ca82cc3ebc1abb632d/src%2Frt%2Fjemalloc%2Finclude%2Fjemalloc%2Finternal%2Fextent.h", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Frt%2Fjemalloc%2Finclude%2Fjemalloc%2Finternal%2Fextent.h?ref=996301331e2d8ac7611d15ca82cc3ebc1abb632d", "patch": "@@ -9,23 +9,23 @@ typedef struct extent_node_s extent_node_t;\n \n /* Tree of extents. */\n struct extent_node_s {\n-    /* Linkage for the size/address-ordered tree. */\n-    rb_node(extent_node_t)\tlink_szad;\n+\t/* Linkage for the size/address-ordered tree. */\n+\trb_node(extent_node_t)\tlink_szad;\n \n-    /* Linkage for the address-ordered tree. */\n-    rb_node(extent_node_t)\tlink_ad;\n+\t/* Linkage for the address-ordered tree. */\n+\trb_node(extent_node_t)\tlink_ad;\n \n-    /* Profile counters, used for huge objects. */\n-    prof_ctx_t\t\t*prof_ctx;\n+\t/* Profile counters, used for huge objects. */\n+\tprof_ctx_t\t\t*prof_ctx;\n \n-    /* Pointer to the extent that this tree node is responsible for. */\n-    void\t\t\t*addr;\n+\t/* Pointer to the extent that this tree node is responsible for. */\n+\tvoid\t\t\t*addr;\n \n-    /* Total region size. */\n-    size_t\t\t\tsize;\n+\t/* Total region size. */\n+\tsize_t\t\t\tsize;\n \n-    /* True if zero-filled; used by chunk recycling code. */\n-    bool\t\t\tzeroed;\n+\t/* True if zero-filled; used by chunk recycling code. */\n+\tbool\t\t\tzeroed;\n };\n typedef rb_tree(extent_node_t) extent_tree_t;\n \n@@ -43,3 +43,4 @@ rb_proto(, extent_tree_ad_, extent_tree_t, extent_node_t)\n \n #endif /* JEMALLOC_H_INLINES */\n /******************************************************************************/\n+"}, {"sha": "56ecc793b365e43abad3b4fcf242b368221d67a1", "filename": "src/rt/jemalloc/include/jemalloc/internal/hash.h", "status": "modified", "additions": 223, "deletions": 223, "changes": 446, "blob_url": "https://github.com/rust-lang/rust/blob/996301331e2d8ac7611d15ca82cc3ebc1abb632d/src%2Frt%2Fjemalloc%2Finclude%2Fjemalloc%2Finternal%2Fhash.h", "raw_url": "https://github.com/rust-lang/rust/raw/996301331e2d8ac7611d15ca82cc3ebc1abb632d/src%2Frt%2Fjemalloc%2Finclude%2Fjemalloc%2Finternal%2Fhash.h", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Frt%2Fjemalloc%2Finclude%2Fjemalloc%2Finternal%2Fhash.h?ref=996301331e2d8ac7611d15ca82cc3ebc1abb632d", "patch": "@@ -30,284 +30,284 @@ JEMALLOC_INLINE uint32_t\n hash_rotl_32(uint32_t x, int8_t r)\n {\n \n-    return (x << r) | (x >> (32 - r));\n+\treturn (x << r) | (x >> (32 - r));\n }\n \n JEMALLOC_INLINE uint64_t\n hash_rotl_64(uint64_t x, int8_t r)\n {\n-    return (x << r) | (x >> (64 - r));\n+\treturn (x << r) | (x >> (64 - r));\n }\n \n JEMALLOC_INLINE uint32_t\n hash_get_block_32(const uint32_t *p, int i)\n {\n \n-    return p[i];\n+\treturn p[i];\n }\n \n JEMALLOC_INLINE uint64_t\n hash_get_block_64(const uint64_t *p, int i)\n {\n \n-    return p[i];\n+\treturn p[i];\n }\n \n JEMALLOC_INLINE uint32_t\n hash_fmix_32(uint32_t h)\n {\n \n-    h ^= h >> 16;\n-    h *= 0x85ebca6b;\n-    h ^= h >> 13;\n-    h *= 0xc2b2ae35;\n-    h ^= h >> 16;\n+\th ^= h >> 16;\n+\th *= 0x85ebca6b;\n+\th ^= h >> 13;\n+\th *= 0xc2b2ae35;\n+\th ^= h >> 16;\n \n-    return h;\n+\treturn h;\n }\n \n JEMALLOC_INLINE uint64_t\n hash_fmix_64(uint64_t k)\n {\n \n-    k ^= k >> 33;\n-    k *= QU(0xff51afd7ed558ccdLLU);\n-    k ^= k >> 33;\n-    k *= QU(0xc4ceb9fe1a85ec53LLU);\n-    k ^= k >> 33;\n+\tk ^= k >> 33;\n+\tk *= QU(0xff51afd7ed558ccdLLU);\n+\tk ^= k >> 33;\n+\tk *= QU(0xc4ceb9fe1a85ec53LLU);\n+\tk ^= k >> 33;\n \n-    return k;\n+\treturn k;\n }\n \n JEMALLOC_INLINE uint32_t\n hash_x86_32(const void *key, int len, uint32_t seed)\n {\n-    const uint8_t *data = (const uint8_t *) key;\n-    const int nblocks = len / 4;\n+\tconst uint8_t *data = (const uint8_t *) key;\n+\tconst int nblocks = len / 4;\n \n-    uint32_t h1 = seed;\n+\tuint32_t h1 = seed;\n \n-    const uint32_t c1 = 0xcc9e2d51;\n-    const uint32_t c2 = 0x1b873593;\n+\tconst uint32_t c1 = 0xcc9e2d51;\n+\tconst uint32_t c2 = 0x1b873593;\n \n-    /* body */\n-    {\n-        const uint32_t *blocks = (const uint32_t *) (data + nblocks*4);\n-        int i;\n+\t/* body */\n+\t{\n+\t\tconst uint32_t *blocks = (const uint32_t *) (data + nblocks*4);\n+\t\tint i;\n \n-        for (i = -nblocks; i; i++) {\n-            uint32_t k1 = hash_get_block_32(blocks, i);\n+\t\tfor (i = -nblocks; i; i++) {\n+\t\t\tuint32_t k1 = hash_get_block_32(blocks, i);\n \n-            k1 *= c1;\n-            k1 = hash_rotl_32(k1, 15);\n-            k1 *= c2;\n+\t\t\tk1 *= c1;\n+\t\t\tk1 = hash_rotl_32(k1, 15);\n+\t\t\tk1 *= c2;\n \n-            h1 ^= k1;\n-            h1 = hash_rotl_32(h1, 13);\n-            h1 = h1*5 + 0xe6546b64;\n-        }\n-    }\n+\t\t\th1 ^= k1;\n+\t\t\th1 = hash_rotl_32(h1, 13);\n+\t\t\th1 = h1*5 + 0xe6546b64;\n+\t\t}\n+\t}\n \n-    /* tail */\n-    {\n-        const uint8_t *tail = (const uint8_t *) (data + nblocks*4);\n+\t/* tail */\n+\t{\n+\t\tconst uint8_t *tail = (const uint8_t *) (data + nblocks*4);\n \n-        uint32_t k1 = 0;\n+\t\tuint32_t k1 = 0;\n \n-        switch (len & 3) {\n-        case 3: k1 ^= tail[2] << 16;\n-        case 2: k1 ^= tail[1] << 8;\n-        case 1: k1 ^= tail[0]; k1 *= c1; k1 = hash_rotl_32(k1, 15);\n-            k1 *= c2; h1 ^= k1;\n-        }\n-    }\n+\t\tswitch (len & 3) {\n+\t\tcase 3: k1 ^= tail[2] << 16;\n+\t\tcase 2: k1 ^= tail[1] << 8;\n+\t\tcase 1: k1 ^= tail[0]; k1 *= c1; k1 = hash_rotl_32(k1, 15);\n+\t\t\tk1 *= c2; h1 ^= k1;\n+\t\t}\n+\t}\n \n-    /* finalization */\n-    h1 ^= len;\n+\t/* finalization */\n+\th1 ^= len;\n \n-    h1 = hash_fmix_32(h1);\n+\th1 = hash_fmix_32(h1);\n \n-    return h1;\n+\treturn h1;\n }\n \n UNUSED JEMALLOC_INLINE void\n hash_x86_128(const void *key, const int len, uint32_t seed,\n   uint64_t r_out[2])\n {\n-    const uint8_t * data = (const uint8_t *) key;\n-    const int nblocks = len / 16;\n-\n-    uint32_t h1 = seed;\n-    uint32_t h2 = seed;\n-    uint32_t h3 = seed;\n-    uint32_t h4 = seed;\n-\n-    const uint32_t c1 = 0x239b961b;\n-    const uint32_t c2 = 0xab0e9789;\n-    const uint32_t c3 = 0x38b34ae5;\n-    const uint32_t c4 = 0xa1e38b93;\n-\n-    /* body */\n-    {\n-        const uint32_t *blocks = (const uint32_t *) (data + nblocks*16);\n-        int i;\n-\n-        for (i = -nblocks; i; i++) {\n-            uint32_t k1 = hash_get_block_32(blocks, i*4 + 0);\n-            uint32_t k2 = hash_get_block_32(blocks, i*4 + 1);\n-            uint32_t k3 = hash_get_block_32(blocks, i*4 + 2);\n-            uint32_t k4 = hash_get_block_32(blocks, i*4 + 3);\n-\n-            k1 *= c1; k1 = hash_rotl_32(k1, 15); k1 *= c2; h1 ^= k1;\n-\n-            h1 = hash_rotl_32(h1, 19); h1 += h2;\n-            h1 = h1*5 + 0x561ccd1b;\n-\n-            k2 *= c2; k2 = hash_rotl_32(k2, 16); k2 *= c3; h2 ^= k2;\n-\n-            h2 = hash_rotl_32(h2, 17); h2 += h3;\n-            h2 = h2*5 + 0x0bcaa747;\n-\n-            k3 *= c3; k3 = hash_rotl_32(k3, 17); k3 *= c4; h3 ^= k3;\n-\n-            h3 = hash_rotl_32(h3, 15); h3 += h4;\n-            h3 = h3*5 + 0x96cd1c35;\n-\n-            k4 *= c4; k4 = hash_rotl_32(k4, 18); k4 *= c1; h4 ^= k4;\n-\n-            h4 = hash_rotl_32(h4, 13); h4 += h1;\n-            h4 = h4*5 + 0x32ac3b17;\n-        }\n-    }\n-\n-    /* tail */\n-    {\n-        const uint8_t *tail = (const uint8_t *) (data + nblocks*16);\n-        uint32_t k1 = 0;\n-        uint32_t k2 = 0;\n-        uint32_t k3 = 0;\n-        uint32_t k4 = 0;\n-\n-        switch (len & 15) {\n-        case 15: k4 ^= tail[14] << 16;\n-        case 14: k4 ^= tail[13] << 8;\n-        case 13: k4 ^= tail[12] << 0;\n-            k4 *= c4; k4 = hash_rotl_32(k4, 18); k4 *= c1; h4 ^= k4;\n-\n-        case 12: k3 ^= tail[11] << 24;\n-        case 11: k3 ^= tail[10] << 16;\n-        case 10: k3 ^= tail[ 9] << 8;\n-        case  9: k3 ^= tail[ 8] << 0;\n-             k3 *= c3; k3 = hash_rotl_32(k3, 17); k3 *= c4; h3 ^= k3;\n-\n-        case  8: k2 ^= tail[ 7] << 24;\n-        case  7: k2 ^= tail[ 6] << 16;\n-        case  6: k2 ^= tail[ 5] << 8;\n-        case  5: k2 ^= tail[ 4] << 0;\n-            k2 *= c2; k2 = hash_rotl_32(k2, 16); k2 *= c3; h2 ^= k2;\n-\n-        case  4: k1 ^= tail[ 3] << 24;\n-        case  3: k1 ^= tail[ 2] << 16;\n-        case  2: k1 ^= tail[ 1] << 8;\n-        case  1: k1 ^= tail[ 0] << 0;\n-            k1 *= c1; k1 = hash_rotl_32(k1, 15); k1 *= c2; h1 ^= k1;\n-        }\n-    }\n-\n-    /* finalization */\n-    h1 ^= len; h2 ^= len; h3 ^= len; h4 ^= len;\n-\n-    h1 += h2; h1 += h3; h1 += h4;\n-    h2 += h1; h3 += h1; h4 += h1;\n-\n-    h1 = hash_fmix_32(h1);\n-    h2 = hash_fmix_32(h2);\n-    h3 = hash_fmix_32(h3);\n-    h4 = hash_fmix_32(h4);\n-\n-    h1 += h2; h1 += h3; h1 += h4;\n-    h2 += h1; h3 += h1; h4 += h1;\n-\n-    r_out[0] = (((uint64_t) h2) << 32) | h1;\n-    r_out[1] = (((uint64_t) h4) << 32) | h3;\n+\tconst uint8_t * data = (const uint8_t *) key;\n+\tconst int nblocks = len / 16;\n+\n+\tuint32_t h1 = seed;\n+\tuint32_t h2 = seed;\n+\tuint32_t h3 = seed;\n+\tuint32_t h4 = seed;\n+\n+\tconst uint32_t c1 = 0x239b961b;\n+\tconst uint32_t c2 = 0xab0e9789;\n+\tconst uint32_t c3 = 0x38b34ae5;\n+\tconst uint32_t c4 = 0xa1e38b93;\n+\n+\t/* body */\n+\t{\n+\t\tconst uint32_t *blocks = (const uint32_t *) (data + nblocks*16);\n+\t\tint i;\n+\n+\t\tfor (i = -nblocks; i; i++) {\n+\t\t\tuint32_t k1 = hash_get_block_32(blocks, i*4 + 0);\n+\t\t\tuint32_t k2 = hash_get_block_32(blocks, i*4 + 1);\n+\t\t\tuint32_t k3 = hash_get_block_32(blocks, i*4 + 2);\n+\t\t\tuint32_t k4 = hash_get_block_32(blocks, i*4 + 3);\n+\n+\t\t\tk1 *= c1; k1 = hash_rotl_32(k1, 15); k1 *= c2; h1 ^= k1;\n+\n+\t\t\th1 = hash_rotl_32(h1, 19); h1 += h2;\n+\t\t\th1 = h1*5 + 0x561ccd1b;\n+\n+\t\t\tk2 *= c2; k2 = hash_rotl_32(k2, 16); k2 *= c3; h2 ^= k2;\n+\n+\t\t\th2 = hash_rotl_32(h2, 17); h2 += h3;\n+\t\t\th2 = h2*5 + 0x0bcaa747;\n+\n+\t\t\tk3 *= c3; k3 = hash_rotl_32(k3, 17); k3 *= c4; h3 ^= k3;\n+\n+\t\t\th3 = hash_rotl_32(h3, 15); h3 += h4;\n+\t\t\th3 = h3*5 + 0x96cd1c35;\n+\n+\t\t\tk4 *= c4; k4 = hash_rotl_32(k4, 18); k4 *= c1; h4 ^= k4;\n+\n+\t\t\th4 = hash_rotl_32(h4, 13); h4 += h1;\n+\t\t\th4 = h4*5 + 0x32ac3b17;\n+\t\t}\n+\t}\n+\n+\t/* tail */\n+\t{\n+\t\tconst uint8_t *tail = (const uint8_t *) (data + nblocks*16);\n+\t\tuint32_t k1 = 0;\n+\t\tuint32_t k2 = 0;\n+\t\tuint32_t k3 = 0;\n+\t\tuint32_t k4 = 0;\n+\n+\t\tswitch (len & 15) {\n+\t\tcase 15: k4 ^= tail[14] << 16;\n+\t\tcase 14: k4 ^= tail[13] << 8;\n+\t\tcase 13: k4 ^= tail[12] << 0;\n+\t\t\tk4 *= c4; k4 = hash_rotl_32(k4, 18); k4 *= c1; h4 ^= k4;\n+\n+\t\tcase 12: k3 ^= tail[11] << 24;\n+\t\tcase 11: k3 ^= tail[10] << 16;\n+\t\tcase 10: k3 ^= tail[ 9] << 8;\n+\t\tcase  9: k3 ^= tail[ 8] << 0;\n+\t\t     k3 *= c3; k3 = hash_rotl_32(k3, 17); k3 *= c4; h3 ^= k3;\n+\n+\t\tcase  8: k2 ^= tail[ 7] << 24;\n+\t\tcase  7: k2 ^= tail[ 6] << 16;\n+\t\tcase  6: k2 ^= tail[ 5] << 8;\n+\t\tcase  5: k2 ^= tail[ 4] << 0;\n+\t\t\tk2 *= c2; k2 = hash_rotl_32(k2, 16); k2 *= c3; h2 ^= k2;\n+\n+\t\tcase  4: k1 ^= tail[ 3] << 24;\n+\t\tcase  3: k1 ^= tail[ 2] << 16;\n+\t\tcase  2: k1 ^= tail[ 1] << 8;\n+\t\tcase  1: k1 ^= tail[ 0] << 0;\n+\t\t\tk1 *= c1; k1 = hash_rotl_32(k1, 15); k1 *= c2; h1 ^= k1;\n+\t\t}\n+\t}\n+\n+\t/* finalization */\n+\th1 ^= len; h2 ^= len; h3 ^= len; h4 ^= len;\n+\n+\th1 += h2; h1 += h3; h1 += h4;\n+\th2 += h1; h3 += h1; h4 += h1;\n+\n+\th1 = hash_fmix_32(h1);\n+\th2 = hash_fmix_32(h2);\n+\th3 = hash_fmix_32(h3);\n+\th4 = hash_fmix_32(h4);\n+\n+\th1 += h2; h1 += h3; h1 += h4;\n+\th2 += h1; h3 += h1; h4 += h1;\n+\n+\tr_out[0] = (((uint64_t) h2) << 32) | h1;\n+\tr_out[1] = (((uint64_t) h4) << 32) | h3;\n }\n \n UNUSED JEMALLOC_INLINE void\n hash_x64_128(const void *key, const int len, const uint32_t seed,\n   uint64_t r_out[2])\n {\n-    const uint8_t *data = (const uint8_t *) key;\n-    const int nblocks = len / 16;\n-\n-    uint64_t h1 = seed;\n-    uint64_t h2 = seed;\n-\n-    const uint64_t c1 = QU(0x87c37b91114253d5LLU);\n-    const uint64_t c2 = QU(0x4cf5ad432745937fLLU);\n-\n-    /* body */\n-    {\n-        const uint64_t *blocks = (const uint64_t *) (data);\n-        int i;\n-\n-        for (i = 0; i < nblocks; i++) {\n-            uint64_t k1 = hash_get_block_64(blocks, i*2 + 0);\n-            uint64_t k2 = hash_get_block_64(blocks, i*2 + 1);\n-\n-            k1 *= c1; k1 = hash_rotl_64(k1, 31); k1 *= c2; h1 ^= k1;\n-\n-            h1 = hash_rotl_64(h1, 27); h1 += h2;\n-            h1 = h1*5 + 0x52dce729;\n-\n-            k2 *= c2; k2 = hash_rotl_64(k2, 33); k2 *= c1; h2 ^= k2;\n-\n-            h2 = hash_rotl_64(h2, 31); h2 += h1;\n-            h2 = h2*5 + 0x38495ab5;\n-        }\n-    }\n-\n-    /* tail */\n-    {\n-        const uint8_t *tail = (const uint8_t*)(data + nblocks*16);\n-        uint64_t k1 = 0;\n-        uint64_t k2 = 0;\n-\n-        switch (len & 15) {\n-        case 15: k2 ^= ((uint64_t)(tail[14])) << 48;\n-        case 14: k2 ^= ((uint64_t)(tail[13])) << 40;\n-        case 13: k2 ^= ((uint64_t)(tail[12])) << 32;\n-        case 12: k2 ^= ((uint64_t)(tail[11])) << 24;\n-        case 11: k2 ^= ((uint64_t)(tail[10])) << 16;\n-        case 10: k2 ^= ((uint64_t)(tail[ 9])) << 8;\n-        case  9: k2 ^= ((uint64_t)(tail[ 8])) << 0;\n-            k2 *= c2; k2 = hash_rotl_64(k2, 33); k2 *= c1; h2 ^= k2;\n-\n-        case  8: k1 ^= ((uint64_t)(tail[ 7])) << 56;\n-        case  7: k1 ^= ((uint64_t)(tail[ 6])) << 48;\n-        case  6: k1 ^= ((uint64_t)(tail[ 5])) << 40;\n-        case  5: k1 ^= ((uint64_t)(tail[ 4])) << 32;\n-        case  4: k1 ^= ((uint64_t)(tail[ 3])) << 24;\n-        case  3: k1 ^= ((uint64_t)(tail[ 2])) << 16;\n-        case  2: k1 ^= ((uint64_t)(tail[ 1])) << 8;\n-        case  1: k1 ^= ((uint64_t)(tail[ 0])) << 0;\n-            k1 *= c1; k1 = hash_rotl_64(k1, 31); k1 *= c2; h1 ^= k1;\n-        }\n-    }\n-\n-    /* finalization */\n-    h1 ^= len; h2 ^= len;\n-\n-    h1 += h2;\n-    h2 += h1;\n-\n-    h1 = hash_fmix_64(h1);\n-    h2 = hash_fmix_64(h2);\n-\n-    h1 += h2;\n-    h2 += h1;\n-\n-    r_out[0] = h1;\n-    r_out[1] = h2;\n+\tconst uint8_t *data = (const uint8_t *) key;\n+\tconst int nblocks = len / 16;\n+\n+\tuint64_t h1 = seed;\n+\tuint64_t h2 = seed;\n+\n+\tconst uint64_t c1 = QU(0x87c37b91114253d5LLU);\n+\tconst uint64_t c2 = QU(0x4cf5ad432745937fLLU);\n+\n+\t/* body */\n+\t{\n+\t\tconst uint64_t *blocks = (const uint64_t *) (data);\n+\t\tint i;\n+\n+\t\tfor (i = 0; i < nblocks; i++) {\n+\t\t\tuint64_t k1 = hash_get_block_64(blocks, i*2 + 0);\n+\t\t\tuint64_t k2 = hash_get_block_64(blocks, i*2 + 1);\n+\n+\t\t\tk1 *= c1; k1 = hash_rotl_64(k1, 31); k1 *= c2; h1 ^= k1;\n+\n+\t\t\th1 = hash_rotl_64(h1, 27); h1 += h2;\n+\t\t\th1 = h1*5 + 0x52dce729;\n+\n+\t\t\tk2 *= c2; k2 = hash_rotl_64(k2, 33); k2 *= c1; h2 ^= k2;\n+\n+\t\t\th2 = hash_rotl_64(h2, 31); h2 += h1;\n+\t\t\th2 = h2*5 + 0x38495ab5;\n+\t\t}\n+\t}\n+\n+\t/* tail */\n+\t{\n+\t\tconst uint8_t *tail = (const uint8_t*)(data + nblocks*16);\n+\t\tuint64_t k1 = 0;\n+\t\tuint64_t k2 = 0;\n+\n+\t\tswitch (len & 15) {\n+\t\tcase 15: k2 ^= ((uint64_t)(tail[14])) << 48;\n+\t\tcase 14: k2 ^= ((uint64_t)(tail[13])) << 40;\n+\t\tcase 13: k2 ^= ((uint64_t)(tail[12])) << 32;\n+\t\tcase 12: k2 ^= ((uint64_t)(tail[11])) << 24;\n+\t\tcase 11: k2 ^= ((uint64_t)(tail[10])) << 16;\n+\t\tcase 10: k2 ^= ((uint64_t)(tail[ 9])) << 8;\n+\t\tcase  9: k2 ^= ((uint64_t)(tail[ 8])) << 0;\n+\t\t\tk2 *= c2; k2 = hash_rotl_64(k2, 33); k2 *= c1; h2 ^= k2;\n+\n+\t\tcase  8: k1 ^= ((uint64_t)(tail[ 7])) << 56;\n+\t\tcase  7: k1 ^= ((uint64_t)(tail[ 6])) << 48;\n+\t\tcase  6: k1 ^= ((uint64_t)(tail[ 5])) << 40;\n+\t\tcase  5: k1 ^= ((uint64_t)(tail[ 4])) << 32;\n+\t\tcase  4: k1 ^= ((uint64_t)(tail[ 3])) << 24;\n+\t\tcase  3: k1 ^= ((uint64_t)(tail[ 2])) << 16;\n+\t\tcase  2: k1 ^= ((uint64_t)(tail[ 1])) << 8;\n+\t\tcase  1: k1 ^= ((uint64_t)(tail[ 0])) << 0;\n+\t\t\tk1 *= c1; k1 = hash_rotl_64(k1, 31); k1 *= c2; h1 ^= k1;\n+\t\t}\n+\t}\n+\n+\t/* finalization */\n+\th1 ^= len; h2 ^= len;\n+\n+\th1 += h2;\n+\th2 += h1;\n+\n+\th1 = hash_fmix_64(h1);\n+\th2 = hash_fmix_64(h2);\n+\n+\th1 += h2;\n+\th2 += h1;\n+\n+\tr_out[0] = h1;\n+\tr_out[1] = h2;\n }\n \n \n@@ -317,12 +317,12 @@ JEMALLOC_INLINE void\n hash(const void *key, size_t len, const uint32_t seed, size_t r_hash[2])\n {\n #if (LG_SIZEOF_PTR == 3)\n-    hash_x64_128(key, len, seed, (uint64_t *)r_hash);\n+\thash_x64_128(key, len, seed, (uint64_t *)r_hash);\n #else\n-    uint64_t hashes[2];\n-    hash_x86_128(key, len, seed, hashes);\n-    r_hash[0] = (size_t)hashes[0];\n-    r_hash[1] = (size_t)hashes[1];\n+\tuint64_t hashes[2];\n+\thash_x86_128(key, len, seed, hashes);\n+\tr_hash[0] = (size_t)hashes[0];\n+\tr_hash[1] = (size_t)hashes[1];\n #endif\n }\n #endif"}, {"sha": "e46ac5440f29038d4a5bd5cab8ae1868fa5f6eb4", "filename": "src/rt/jemalloc/include/jemalloc/internal/jemalloc_internal.h.in", "status": "modified", "additions": 3, "deletions": 0, "changes": 3, "blob_url": "https://github.com/rust-lang/rust/blob/996301331e2d8ac7611d15ca82cc3ebc1abb632d/src%2Frt%2Fjemalloc%2Finclude%2Fjemalloc%2Finternal%2Fjemalloc_internal.h.in", "raw_url": "https://github.com/rust-lang/rust/raw/996301331e2d8ac7611d15ca82cc3ebc1abb632d/src%2Frt%2Fjemalloc%2Finclude%2Fjemalloc%2Finternal%2Fjemalloc_internal.h.in", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Frt%2Fjemalloc%2Finclude%2Fjemalloc%2Finternal%2Fjemalloc_internal.h.in?ref=996301331e2d8ac7611d15ca82cc3ebc1abb632d", "patch": "@@ -278,6 +278,9 @@ static const bool config_ivsalloc =\n #  ifdef __arm__\n #    define LG_QUANTUM\t\t3\n #  endif\n+#  ifdef __aarch64__\n+#    define LG_QUANTUM\t\t4\n+#  endif\n #  ifdef __hppa__\n #    define LG_QUANTUM\t\t4\n #  endif"}, {"sha": "3cfa7872942b5e1a070e7d3f234d1f69b4136069", "filename": "src/rt/jemalloc/include/jemalloc/internal/mb.h", "status": "modified", "additions": 38, "deletions": 38, "changes": 76, "blob_url": "https://github.com/rust-lang/rust/blob/996301331e2d8ac7611d15ca82cc3ebc1abb632d/src%2Frt%2Fjemalloc%2Finclude%2Fjemalloc%2Finternal%2Fmb.h", "raw_url": "https://github.com/rust-lang/rust/raw/996301331e2d8ac7611d15ca82cc3ebc1abb632d/src%2Frt%2Fjemalloc%2Finclude%2Fjemalloc%2Finternal%2Fmb.h", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Frt%2Fjemalloc%2Finclude%2Fjemalloc%2Finternal%2Fmb.h?ref=996301331e2d8ac7611d15ca82cc3ebc1abb632d", "patch": "@@ -33,66 +33,66 @@ mb_write(void)\n {\n \n #  if 0\n-    /* This is a true memory barrier. */\n-    asm volatile (\"pusha;\"\n-        \"xor  %%eax,%%eax;\"\n-        \"cpuid;\"\n-        \"popa;\"\n-        : /* Outputs. */\n-        : /* Inputs. */\n-        : \"memory\" /* Clobbers. */\n-        );\n+\t/* This is a true memory barrier. */\n+\tasm volatile (\"pusha;\"\n+\t    \"xor  %%eax,%%eax;\"\n+\t    \"cpuid;\"\n+\t    \"popa;\"\n+\t    : /* Outputs. */\n+\t    : /* Inputs. */\n+\t    : \"memory\" /* Clobbers. */\n+\t    );\n #else\n-    /*\n-     * This is hopefully enough to keep the compiler from reordering\n-     * instructions around this one.\n-     */\n-    asm volatile (\"nop;\"\n-        : /* Outputs. */\n-        : /* Inputs. */\n-        : \"memory\" /* Clobbers. */\n-        );\n+\t/*\n+\t * This is hopefully enough to keep the compiler from reordering\n+\t * instructions around this one.\n+\t */\n+\tasm volatile (\"nop;\"\n+\t    : /* Outputs. */\n+\t    : /* Inputs. */\n+\t    : \"memory\" /* Clobbers. */\n+\t    );\n #endif\n }\n #elif (defined(__amd64__) || defined(__x86_64__))\n JEMALLOC_INLINE void\n mb_write(void)\n {\n \n-    asm volatile (\"sfence\"\n-        : /* Outputs. */\n-        : /* Inputs. */\n-        : \"memory\" /* Clobbers. */\n-        );\n+\tasm volatile (\"sfence\"\n+\t    : /* Outputs. */\n+\t    : /* Inputs. */\n+\t    : \"memory\" /* Clobbers. */\n+\t    );\n }\n #elif defined(__powerpc__)\n JEMALLOC_INLINE void\n mb_write(void)\n {\n \n-    asm volatile (\"eieio\"\n-        : /* Outputs. */\n-        : /* Inputs. */\n-        : \"memory\" /* Clobbers. */\n-        );\n+\tasm volatile (\"eieio\"\n+\t    : /* Outputs. */\n+\t    : /* Inputs. */\n+\t    : \"memory\" /* Clobbers. */\n+\t    );\n }\n #elif defined(__sparc64__)\n JEMALLOC_INLINE void\n mb_write(void)\n {\n \n-    asm volatile (\"membar #StoreStore\"\n-        : /* Outputs. */\n-        : /* Inputs. */\n-        : \"memory\" /* Clobbers. */\n-        );\n+\tasm volatile (\"membar #StoreStore\"\n+\t    : /* Outputs. */\n+\t    : /* Inputs. */\n+\t    : \"memory\" /* Clobbers. */\n+\t    );\n }\n #elif defined(__tile__)\n JEMALLOC_INLINE void\n mb_write(void)\n {\n \n-    __sync_synchronize();\n+\t__sync_synchronize();\n }\n #else\n /*\n@@ -102,11 +102,11 @@ mb_write(void)\n JEMALLOC_INLINE void\n mb_write(void)\n {\n-    malloc_mutex_t mtx;\n+\tmalloc_mutex_t mtx;\n \n-    malloc_mutex_init(&mtx);\n-    malloc_mutex_lock(&mtx);\n-    malloc_mutex_unlock(&mtx);\n+\tmalloc_mutex_init(&mtx);\n+\tmalloc_mutex_lock(&mtx);\n+\tmalloc_mutex_unlock(&mtx);\n }\n #endif\n #endif"}, {"sha": "de44e1435ad31d8a8a7a8cf166d755a0cd534cc3", "filename": "src/rt/jemalloc/include/jemalloc/internal/mutex.h", "status": "modified", "additions": 15, "deletions": 15, "changes": 30, "blob_url": "https://github.com/rust-lang/rust/blob/996301331e2d8ac7611d15ca82cc3ebc1abb632d/src%2Frt%2Fjemalloc%2Finclude%2Fjemalloc%2Finternal%2Fmutex.h", "raw_url": "https://github.com/rust-lang/rust/raw/996301331e2d8ac7611d15ca82cc3ebc1abb632d/src%2Frt%2Fjemalloc%2Finclude%2Fjemalloc%2Finternal%2Fmutex.h", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Frt%2Fjemalloc%2Finclude%2Fjemalloc%2Finternal%2Fmutex.h?ref=996301331e2d8ac7611d15ca82cc3ebc1abb632d", "patch": "@@ -26,14 +26,14 @@ typedef struct malloc_mutex_s malloc_mutex_t;\n \n struct malloc_mutex_s {\n #ifdef _WIN32\n-    CRITICAL_SECTION\tlock;\n+\tCRITICAL_SECTION\tlock;\n #elif (defined(JEMALLOC_OSSPIN))\n-    OSSpinLock\t\tlock;\n+\tOSSpinLock\t\tlock;\n #elif (defined(JEMALLOC_MUTEX_INIT_CB))\n-    pthread_mutex_t\t\tlock;\n-    malloc_mutex_t\t\t*postponed_next;\n+\tpthread_mutex_t\t\tlock;\n+\tmalloc_mutex_t\t\t*postponed_next;\n #else\n-    pthread_mutex_t\t\tlock;\n+\tpthread_mutex_t\t\tlock;\n #endif\n };\n \n@@ -68,30 +68,30 @@ JEMALLOC_INLINE void\n malloc_mutex_lock(malloc_mutex_t *mutex)\n {\n \n-    if (isthreaded) {\n+\tif (isthreaded) {\n #ifdef _WIN32\n-        EnterCriticalSection(&mutex->lock);\n+\t\tEnterCriticalSection(&mutex->lock);\n #elif (defined(JEMALLOC_OSSPIN))\n-        OSSpinLockLock(&mutex->lock);\n+\t\tOSSpinLockLock(&mutex->lock);\n #else\n-        pthread_mutex_lock(&mutex->lock);\n+\t\tpthread_mutex_lock(&mutex->lock);\n #endif\n-    }\n+\t}\n }\n \n JEMALLOC_INLINE void\n malloc_mutex_unlock(malloc_mutex_t *mutex)\n {\n \n-    if (isthreaded) {\n+\tif (isthreaded) {\n #ifdef _WIN32\n-        LeaveCriticalSection(&mutex->lock);\n+\t\tLeaveCriticalSection(&mutex->lock);\n #elif (defined(JEMALLOC_OSSPIN))\n-        OSSpinLockUnlock(&mutex->lock);\n+\t\tOSSpinLockUnlock(&mutex->lock);\n #else\n-        pthread_mutex_unlock(&mutex->lock);\n+\t\tpthread_mutex_unlock(&mutex->lock);\n #endif\n-    }\n+\t}\n }\n #endif\n "}, {"sha": "83a5462b4dd0d0738ff8205ccaa6ae8946003b19", "filename": "src/rt/jemalloc/include/jemalloc/internal/prng.h", "status": "modified", "additions": 12, "deletions": 12, "changes": 24, "blob_url": "https://github.com/rust-lang/rust/blob/996301331e2d8ac7611d15ca82cc3ebc1abb632d/src%2Frt%2Fjemalloc%2Finclude%2Fjemalloc%2Finternal%2Fprng.h", "raw_url": "https://github.com/rust-lang/rust/raw/996301331e2d8ac7611d15ca82cc3ebc1abb632d/src%2Frt%2Fjemalloc%2Finclude%2Fjemalloc%2Finternal%2Fprng.h", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Frt%2Fjemalloc%2Finclude%2Fjemalloc%2Finternal%2Fprng.h?ref=996301331e2d8ac7611d15ca82cc3ebc1abb632d", "patch": "@@ -26,22 +26,22 @@\n  *   const uint32_t a, c : See above discussion.\n  */\n #define prng32(r, lg_range, state, a, c) do {\t\t\t\t\\\n-    assert(lg_range > 0);\t\t\t\t\t\t\\\n-    assert(lg_range <= 32);\t\t\t\t\t\t\\\n-                                    \\\n-    r = (state * (a)) + (c);\t\t\t\t\t\\\n-    state = r;\t\t\t\t\t\t\t\\\n-    r >>= (32 - lg_range);\t\t\t\t\t\t\\\n+\tassert(lg_range > 0);\t\t\t\t\t\t\\\n+\tassert(lg_range <= 32);\t\t\t\t\t\t\\\n+\t\t\t\t\t\t\t\t\t\\\n+\tr = (state * (a)) + (c);\t\t\t\t\t\\\n+\tstate = r;\t\t\t\t\t\t\t\\\n+\tr >>= (32 - lg_range);\t\t\t\t\t\t\\\n } while (false)\n \n /* Same as prng32(), but 64 bits of pseudo-randomness, using uint64_t. */\n #define prng64(r, lg_range, state, a, c) do {\t\t\t\t\\\n-    assert(lg_range > 0);\t\t\t\t\t\t\\\n-    assert(lg_range <= 64);\t\t\t\t\t\t\\\n-                                    \\\n-    r = (state * (a)) + (c);\t\t\t\t\t\\\n-    state = r;\t\t\t\t\t\t\t\\\n-    r >>= (64 - lg_range);\t\t\t\t\t\t\\\n+\tassert(lg_range > 0);\t\t\t\t\t\t\\\n+\tassert(lg_range <= 64);\t\t\t\t\t\t\\\n+\t\t\t\t\t\t\t\t\t\\\n+\tr = (state * (a)) + (c);\t\t\t\t\t\\\n+\tstate = r;\t\t\t\t\t\t\t\\\n+\tr >>= (64 - lg_range);\t\t\t\t\t\t\\\n } while (false)\n \n #endif /* JEMALLOC_H_TYPES */"}, {"sha": "119a5b1bcb7bf1d25d62038b96688b7c12c94655", "filename": "src/rt/jemalloc/include/jemalloc/internal/prof.h", "status": "modified", "additions": 377, "deletions": 377, "changes": 754, "blob_url": "https://github.com/rust-lang/rust/blob/996301331e2d8ac7611d15ca82cc3ebc1abb632d/src%2Frt%2Fjemalloc%2Finclude%2Fjemalloc%2Finternal%2Fprof.h", "raw_url": "https://github.com/rust-lang/rust/raw/996301331e2d8ac7611d15ca82cc3ebc1abb632d/src%2Frt%2Fjemalloc%2Finclude%2Fjemalloc%2Finternal%2Fprof.h", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Frt%2Fjemalloc%2Finclude%2Fjemalloc%2Finternal%2Fprof.h?ref=996301331e2d8ac7611d15ca82cc3ebc1abb632d", "patch": "@@ -50,131 +50,131 @@ typedef struct prof_tdata_s prof_tdata_t;\n #ifdef JEMALLOC_H_STRUCTS\n \n struct prof_bt_s {\n-    /* Backtrace, stored as len program counters. */\n-    void\t\t**vec;\n-    unsigned\tlen;\n+\t/* Backtrace, stored as len program counters. */\n+\tvoid\t\t**vec;\n+\tunsigned\tlen;\n };\n \n #ifdef JEMALLOC_PROF_LIBGCC\n /* Data structure passed to libgcc _Unwind_Backtrace() callback functions. */\n typedef struct {\n-    prof_bt_t\t*bt;\n-    unsigned\tnignore;\n-    unsigned\tmax;\n+\tprof_bt_t\t*bt;\n+\tunsigned\tnignore;\n+\tunsigned\tmax;\n } prof_unwind_data_t;\n #endif\n \n struct prof_cnt_s {\n-    /*\n-     * Profiling counters.  An allocation/deallocation pair can operate on\n-     * different prof_thr_cnt_t objects that are linked into the same\n-     * prof_ctx_t cnts_ql, so it is possible for the cur* counters to go\n-     * negative.  In principle it is possible for the *bytes counters to\n-     * overflow/underflow, but a general solution would require something\n-     * like 128-bit counters; this implementation doesn't bother to solve\n-     * that problem.\n-     */\n-    int64_t\t\tcurobjs;\n-    int64_t\t\tcurbytes;\n-    uint64_t\taccumobjs;\n-    uint64_t\taccumbytes;\n+\t/*\n+\t * Profiling counters.  An allocation/deallocation pair can operate on\n+\t * different prof_thr_cnt_t objects that are linked into the same\n+\t * prof_ctx_t cnts_ql, so it is possible for the cur* counters to go\n+\t * negative.  In principle it is possible for the *bytes counters to\n+\t * overflow/underflow, but a general solution would require something\n+\t * like 128-bit counters; this implementation doesn't bother to solve\n+\t * that problem.\n+\t */\n+\tint64_t\t\tcurobjs;\n+\tint64_t\t\tcurbytes;\n+\tuint64_t\taccumobjs;\n+\tuint64_t\taccumbytes;\n };\n \n struct prof_thr_cnt_s {\n-    /* Linkage into prof_ctx_t's cnts_ql. */\n-    ql_elm(prof_thr_cnt_t)\tcnts_link;\n-\n-    /* Linkage into thread's LRU. */\n-    ql_elm(prof_thr_cnt_t)\tlru_link;\n-\n-    /*\n-     * Associated context.  If a thread frees an object that it did not\n-     * allocate, it is possible that the context is not cached in the\n-     * thread's hash table, in which case it must be able to look up the\n-     * context, insert a new prof_thr_cnt_t into the thread's hash table,\n-     * and link it into the prof_ctx_t's cnts_ql.\n-     */\n-    prof_ctx_t\t\t*ctx;\n-\n-    /*\n-     * Threads use memory barriers to update the counters.  Since there is\n-     * only ever one writer, the only challenge is for the reader to get a\n-     * consistent read of the counters.\n-     *\n-     * The writer uses this series of operations:\n-     *\n-     * 1) Increment epoch to an odd number.\n-     * 2) Update counters.\n-     * 3) Increment epoch to an even number.\n-     *\n-     * The reader must assure 1) that the epoch is even while it reads the\n-     * counters, and 2) that the epoch doesn't change between the time it\n-     * starts and finishes reading the counters.\n-     */\n-    unsigned\t\tepoch;\n-\n-    /* Profiling counters. */\n-    prof_cnt_t\t\tcnts;\n+\t/* Linkage into prof_ctx_t's cnts_ql. */\n+\tql_elm(prof_thr_cnt_t)\tcnts_link;\n+\n+\t/* Linkage into thread's LRU. */\n+\tql_elm(prof_thr_cnt_t)\tlru_link;\n+\n+\t/*\n+\t * Associated context.  If a thread frees an object that it did not\n+\t * allocate, it is possible that the context is not cached in the\n+\t * thread's hash table, in which case it must be able to look up the\n+\t * context, insert a new prof_thr_cnt_t into the thread's hash table,\n+\t * and link it into the prof_ctx_t's cnts_ql.\n+\t */\n+\tprof_ctx_t\t\t*ctx;\n+\n+\t/*\n+\t * Threads use memory barriers to update the counters.  Since there is\n+\t * only ever one writer, the only challenge is for the reader to get a\n+\t * consistent read of the counters.\n+\t *\n+\t * The writer uses this series of operations:\n+\t *\n+\t * 1) Increment epoch to an odd number.\n+\t * 2) Update counters.\n+\t * 3) Increment epoch to an even number.\n+\t *\n+\t * The reader must assure 1) that the epoch is even while it reads the\n+\t * counters, and 2) that the epoch doesn't change between the time it\n+\t * starts and finishes reading the counters.\n+\t */\n+\tunsigned\t\tepoch;\n+\n+\t/* Profiling counters. */\n+\tprof_cnt_t\t\tcnts;\n };\n \n struct prof_ctx_s {\n-    /* Associated backtrace. */\n-    prof_bt_t\t\t*bt;\n-\n-    /* Protects nlimbo, cnt_merged, and cnts_ql. */\n-    malloc_mutex_t\t\t*lock;\n-\n-    /*\n-     * Number of threads that currently cause this ctx to be in a state of\n-     * limbo due to one of:\n-     *   - Initializing per thread counters associated with this ctx.\n-     *   - Preparing to destroy this ctx.\n-     * nlimbo must be 1 (single destroyer) in order to safely destroy the\n-     * ctx.\n-     */\n-    unsigned\t\tnlimbo;\n-\n-    /* Temporary storage for summation during dump. */\n-    prof_cnt_t\t\tcnt_summed;\n-\n-    /* When threads exit, they merge their stats into cnt_merged. */\n-    prof_cnt_t\t\tcnt_merged;\n-\n-    /*\n-     * List of profile counters, one for each thread that has allocated in\n-     * this context.\n-     */\n-    ql_head(prof_thr_cnt_t)\tcnts_ql;\n+\t/* Associated backtrace. */\n+\tprof_bt_t\t\t*bt;\n+\n+\t/* Protects nlimbo, cnt_merged, and cnts_ql. */\n+\tmalloc_mutex_t\t\t*lock;\n+\n+\t/*\n+\t * Number of threads that currently cause this ctx to be in a state of\n+\t * limbo due to one of:\n+\t *   - Initializing per thread counters associated with this ctx.\n+\t *   - Preparing to destroy this ctx.\n+\t * nlimbo must be 1 (single destroyer) in order to safely destroy the\n+\t * ctx.\n+\t */\n+\tunsigned\t\tnlimbo;\n+\n+\t/* Temporary storage for summation during dump. */\n+\tprof_cnt_t\t\tcnt_summed;\n+\n+\t/* When threads exit, they merge their stats into cnt_merged. */\n+\tprof_cnt_t\t\tcnt_merged;\n+\n+\t/*\n+\t * List of profile counters, one for each thread that has allocated in\n+\t * this context.\n+\t */\n+\tql_head(prof_thr_cnt_t)\tcnts_ql;\n };\n \n struct prof_tdata_s {\n-    /*\n-     * Hash of (prof_bt_t *)-->(prof_thr_cnt_t *).  Each thread keeps a\n-     * cache of backtraces, with associated thread-specific prof_thr_cnt_t\n-     * objects.  Other threads may read the prof_thr_cnt_t contents, but no\n-     * others will ever write them.\n-     *\n-     * Upon thread exit, the thread must merge all the prof_thr_cnt_t\n-     * counter data into the associated prof_ctx_t objects, and unlink/free\n-     * the prof_thr_cnt_t objects.\n-     */\n-    ckh_t\t\t\tbt2cnt;\n-\n-    /* LRU for contents of bt2cnt. */\n-    ql_head(prof_thr_cnt_t)\tlru_ql;\n-\n-    /* Backtrace vector, used for calls to prof_backtrace(). */\n-    void\t\t\t**vec;\n-\n-    /* Sampling state. */\n-    uint64_t\t\tprng_state;\n-    uint64_t\t\tthreshold;\n-    uint64_t\t\taccum;\n-\n-    /* State used to avoid dumping while operating on prof internals. */\n-    bool\t\t\tenq;\n-    bool\t\t\tenq_idump;\n-    bool\t\t\tenq_gdump;\n+\t/*\n+\t * Hash of (prof_bt_t *)-->(prof_thr_cnt_t *).  Each thread keeps a\n+\t * cache of backtraces, with associated thread-specific prof_thr_cnt_t\n+\t * objects.  Other threads may read the prof_thr_cnt_t contents, but no\n+\t * others will ever write them.\n+\t *\n+\t * Upon thread exit, the thread must merge all the prof_thr_cnt_t\n+\t * counter data into the associated prof_ctx_t objects, and unlink/free\n+\t * the prof_thr_cnt_t objects.\n+\t */\n+\tckh_t\t\t\tbt2cnt;\n+\n+\t/* LRU for contents of bt2cnt. */\n+\tql_head(prof_thr_cnt_t)\tlru_ql;\n+\n+\t/* Backtrace vector, used for calls to prof_backtrace(). */\n+\tvoid\t\t\t**vec;\n+\n+\t/* Sampling state. */\n+\tuint64_t\t\tprng_state;\n+\tuint64_t\t\tthreshold;\n+\tuint64_t\t\taccum;\n+\n+\t/* State used to avoid dumping while operating on prof internals. */\n+\tbool\t\t\tenq;\n+\tbool\t\t\tenq_idump;\n+\tbool\t\t\tenq_gdump;\n };\n \n #endif /* JEMALLOC_H_STRUCTS */\n@@ -232,55 +232,55 @@ void\tprof_postfork_child(void);\n #ifdef JEMALLOC_H_INLINES\n \n #define\tPROF_ALLOC_PREP(nignore, size, ret) do {\t\t\t\\\n-    prof_tdata_t *prof_tdata;\t\t\t\t\t\\\n-    prof_bt_t bt;\t\t\t\t\t\t\t\\\n-                                    \\\n-    assert(size == s2u(size));\t\t\t\t\t\\\n-                                    \\\n-    prof_tdata = prof_tdata_get(true);\t\t\t\t\\\n-    if ((uintptr_t)prof_tdata <= (uintptr_t)PROF_TDATA_STATE_MAX) {\t\\\n-        if (prof_tdata != NULL)\t\t\t\t\t\\\n-            ret = (prof_thr_cnt_t *)(uintptr_t)1U;\t\t\\\n-        else\t\t\t\t\t\t\t\\\n-            ret = NULL;\t\t\t\t\t\\\n-        break;\t\t\t\t\t\t\t\\\n-    }\t\t\t\t\t\t\t\t\\\n-                                    \\\n-    if (opt_prof_active == false) {\t\t\t\t\t\\\n-        /* Sampling is currently inactive, so avoid sampling. */\\\n-        ret = (prof_thr_cnt_t *)(uintptr_t)1U;\t\t\t\\\n-    } else if (opt_lg_prof_sample == 0) {\t\t\t\t\\\n-        /* Don't bother with sampling logic, since sampling   */\\\n-        /* interval is 1.                                     */\\\n-        bt_init(&bt, prof_tdata->vec);\t\t\t\t\\\n-        prof_backtrace(&bt, nignore);\t\t\t\t\\\n-        ret = prof_lookup(&bt);\t\t\t\t\t\\\n-    } else {\t\t\t\t\t\t\t\\\n-        if (prof_tdata->threshold == 0) {\t\t\t\\\n-            /* Initialize.  Seed the prng differently for */\\\n-            /* each thread.                               */\\\n-            prof_tdata->prng_state =\t\t\t\\\n-                (uint64_t)(uintptr_t)&size;\t\t\t\\\n-            prof_sample_threshold_update(prof_tdata);\t\\\n-        }\t\t\t\t\t\t\t\\\n-                                    \\\n-        /* Determine whether to capture a backtrace based on  */\\\n-        /* whether size is enough for prof_accum to reach     */\\\n-        /* prof_tdata->threshold.  However, delay updating    */\\\n-        /* these variables until prof_{m,re}alloc(), because  */\\\n-        /* we don't know for sure that the allocation will    */\\\n-        /* succeed.                                           */\\\n-        /*                                                    */\\\n-        /* Use subtraction rather than addition to avoid      */\\\n-        /* potential integer overflow.                        */\\\n-        if (size >= prof_tdata->threshold -\t\t\t\\\n-            prof_tdata->accum) {\t\t\t\t\\\n-            bt_init(&bt, prof_tdata->vec);\t\t\t\\\n-            prof_backtrace(&bt, nignore);\t\t\t\\\n-            ret = prof_lookup(&bt);\t\t\t\t\\\n-        } else\t\t\t\t\t\t\t\\\n-            ret = (prof_thr_cnt_t *)(uintptr_t)1U;\t\t\\\n-    }\t\t\t\t\t\t\t\t\\\n+\tprof_tdata_t *prof_tdata;\t\t\t\t\t\\\n+\tprof_bt_t bt;\t\t\t\t\t\t\t\\\n+\t\t\t\t\t\t\t\t\t\\\n+\tassert(size == s2u(size));\t\t\t\t\t\\\n+\t\t\t\t\t\t\t\t\t\\\n+\tprof_tdata = prof_tdata_get(true);\t\t\t\t\\\n+\tif ((uintptr_t)prof_tdata <= (uintptr_t)PROF_TDATA_STATE_MAX) {\t\\\n+\t\tif (prof_tdata != NULL)\t\t\t\t\t\\\n+\t\t\tret = (prof_thr_cnt_t *)(uintptr_t)1U;\t\t\\\n+\t\telse\t\t\t\t\t\t\t\\\n+\t\t\tret = NULL;\t\t\t\t\t\\\n+\t\tbreak;\t\t\t\t\t\t\t\\\n+\t}\t\t\t\t\t\t\t\t\\\n+\t\t\t\t\t\t\t\t\t\\\n+\tif (opt_prof_active == false) {\t\t\t\t\t\\\n+\t\t/* Sampling is currently inactive, so avoid sampling. */\\\n+\t\tret = (prof_thr_cnt_t *)(uintptr_t)1U;\t\t\t\\\n+\t} else if (opt_lg_prof_sample == 0) {\t\t\t\t\\\n+\t\t/* Don't bother with sampling logic, since sampling   */\\\n+\t\t/* interval is 1.                                     */\\\n+\t\tbt_init(&bt, prof_tdata->vec);\t\t\t\t\\\n+\t\tprof_backtrace(&bt, nignore);\t\t\t\t\\\n+\t\tret = prof_lookup(&bt);\t\t\t\t\t\\\n+\t} else {\t\t\t\t\t\t\t\\\n+\t\tif (prof_tdata->threshold == 0) {\t\t\t\\\n+\t\t\t/* Initialize.  Seed the prng differently for */\\\n+\t\t\t/* each thread.                               */\\\n+\t\t\tprof_tdata->prng_state =\t\t\t\\\n+\t\t\t    (uint64_t)(uintptr_t)&size;\t\t\t\\\n+\t\t\tprof_sample_threshold_update(prof_tdata);\t\\\n+\t\t}\t\t\t\t\t\t\t\\\n+\t\t\t\t\t\t\t\t\t\\\n+\t\t/* Determine whether to capture a backtrace based on  */\\\n+\t\t/* whether size is enough for prof_accum to reach     */\\\n+\t\t/* prof_tdata->threshold.  However, delay updating    */\\\n+\t\t/* these variables until prof_{m,re}alloc(), because  */\\\n+\t\t/* we don't know for sure that the allocation will    */\\\n+\t\t/* succeed.                                           */\\\n+\t\t/*                                                    */\\\n+\t\t/* Use subtraction rather than addition to avoid      */\\\n+\t\t/* potential integer overflow.                        */\\\n+\t\tif (size >= prof_tdata->threshold -\t\t\t\\\n+\t\t    prof_tdata->accum) {\t\t\t\t\\\n+\t\t\tbt_init(&bt, prof_tdata->vec);\t\t\t\\\n+\t\t\tprof_backtrace(&bt, nignore);\t\t\t\\\n+\t\t\tret = prof_lookup(&bt);\t\t\t\t\\\n+\t\t} else\t\t\t\t\t\t\t\\\n+\t\t\tret = (prof_thr_cnt_t *)(uintptr_t)1U;\t\t\\\n+\t}\t\t\t\t\t\t\t\t\\\n } while (0)\n \n #ifndef JEMALLOC_ENABLE_INLINE\n@@ -306,272 +306,272 @@ malloc_tsd_funcs(JEMALLOC_INLINE, prof_tdata, prof_tdata_t *, NULL,\n JEMALLOC_INLINE prof_tdata_t *\n prof_tdata_get(bool create)\n {\n-    prof_tdata_t *prof_tdata;\n+\tprof_tdata_t *prof_tdata;\n \n-    cassert(config_prof);\n+\tcassert(config_prof);\n \n-    prof_tdata = *prof_tdata_tsd_get();\n-    if (create && prof_tdata == NULL)\n-        prof_tdata = prof_tdata_init();\n+\tprof_tdata = *prof_tdata_tsd_get();\n+\tif (create && prof_tdata == NULL)\n+\t\tprof_tdata = prof_tdata_init();\n \n-    return (prof_tdata);\n+\treturn (prof_tdata);\n }\n \n JEMALLOC_INLINE void\n prof_sample_threshold_update(prof_tdata_t *prof_tdata)\n {\n-    uint64_t r;\n-    double u;\n-\n-    cassert(config_prof);\n-\n-    /*\n-     * Compute sample threshold as a geometrically distributed random\n-     * variable with mean (2^opt_lg_prof_sample).\n-     *\n-     *                         __        __\n-     *                         |  log(u)  |                     1\n-     * prof_tdata->threshold = | -------- |, where p = -------------------\n-     *                         | log(1-p) |             opt_lg_prof_sample\n-     *                                                 2\n-     *\n-     * For more information on the math, see:\n-     *\n-     *   Non-Uniform Random Variate Generation\n-     *   Luc Devroye\n-     *   Springer-Verlag, New York, 1986\n-     *   pp 500\n-     *   (http://cg.scs.carleton.ca/~luc/rnbookindex.html)\n-     */\n-    prng64(r, 53, prof_tdata->prng_state,\n-        UINT64_C(6364136223846793005), UINT64_C(1442695040888963407));\n-    u = (double)r * (1.0/9007199254740992.0L);\n-    prof_tdata->threshold = (uint64_t)(log(u) /\n-        log(1.0 - (1.0 / (double)((uint64_t)1U << opt_lg_prof_sample))))\n-        + (uint64_t)1U;\n+\tuint64_t r;\n+\tdouble u;\n+\n+\tcassert(config_prof);\n+\n+\t/*\n+\t * Compute sample threshold as a geometrically distributed random\n+\t * variable with mean (2^opt_lg_prof_sample).\n+\t *\n+\t *                         __        __\n+\t *                         |  log(u)  |                     1\n+\t * prof_tdata->threshold = | -------- |, where p = -------------------\n+\t *                         | log(1-p) |             opt_lg_prof_sample\n+\t *                                                 2\n+\t *\n+\t * For more information on the math, see:\n+\t *\n+\t *   Non-Uniform Random Variate Generation\n+\t *   Luc Devroye\n+\t *   Springer-Verlag, New York, 1986\n+\t *   pp 500\n+\t *   (http://cg.scs.carleton.ca/~luc/rnbookindex.html)\n+\t */\n+\tprng64(r, 53, prof_tdata->prng_state,\n+\t    UINT64_C(6364136223846793005), UINT64_C(1442695040888963407));\n+\tu = (double)r * (1.0/9007199254740992.0L);\n+\tprof_tdata->threshold = (uint64_t)(log(u) /\n+\t    log(1.0 - (1.0 / (double)((uint64_t)1U << opt_lg_prof_sample))))\n+\t    + (uint64_t)1U;\n }\n \n JEMALLOC_INLINE prof_ctx_t *\n prof_ctx_get(const void *ptr)\n {\n-    prof_ctx_t *ret;\n-    arena_chunk_t *chunk;\n+\tprof_ctx_t *ret;\n+\tarena_chunk_t *chunk;\n \n-    cassert(config_prof);\n-    assert(ptr != NULL);\n+\tcassert(config_prof);\n+\tassert(ptr != NULL);\n \n-    chunk = (arena_chunk_t *)CHUNK_ADDR2BASE(ptr);\n-    if (chunk != ptr) {\n-        /* Region. */\n-        ret = arena_prof_ctx_get(ptr);\n-    } else\n-        ret = huge_prof_ctx_get(ptr);\n+\tchunk = (arena_chunk_t *)CHUNK_ADDR2BASE(ptr);\n+\tif (chunk != ptr) {\n+\t\t/* Region. */\n+\t\tret = arena_prof_ctx_get(ptr);\n+\t} else\n+\t\tret = huge_prof_ctx_get(ptr);\n \n-    return (ret);\n+\treturn (ret);\n }\n \n JEMALLOC_INLINE void\n prof_ctx_set(const void *ptr, prof_ctx_t *ctx)\n {\n-    arena_chunk_t *chunk;\n+\tarena_chunk_t *chunk;\n \n-    cassert(config_prof);\n-    assert(ptr != NULL);\n+\tcassert(config_prof);\n+\tassert(ptr != NULL);\n \n-    chunk = (arena_chunk_t *)CHUNK_ADDR2BASE(ptr);\n-    if (chunk != ptr) {\n-        /* Region. */\n-        arena_prof_ctx_set(ptr, ctx);\n-    } else\n-        huge_prof_ctx_set(ptr, ctx);\n+\tchunk = (arena_chunk_t *)CHUNK_ADDR2BASE(ptr);\n+\tif (chunk != ptr) {\n+\t\t/* Region. */\n+\t\tarena_prof_ctx_set(ptr, ctx);\n+\t} else\n+\t\thuge_prof_ctx_set(ptr, ctx);\n }\n \n JEMALLOC_INLINE bool\n prof_sample_accum_update(size_t size)\n {\n-    prof_tdata_t *prof_tdata;\n-\n-    cassert(config_prof);\n-    /* Sampling logic is unnecessary if the interval is 1. */\n-    assert(opt_lg_prof_sample != 0);\n-\n-    prof_tdata = prof_tdata_get(false);\n-    if ((uintptr_t)prof_tdata <= (uintptr_t)PROF_TDATA_STATE_MAX)\n-        return (true);\n-\n-    /* Take care to avoid integer overflow. */\n-    if (size >= prof_tdata->threshold - prof_tdata->accum) {\n-        prof_tdata->accum -= (prof_tdata->threshold - size);\n-        /* Compute new sample threshold. */\n-        prof_sample_threshold_update(prof_tdata);\n-        while (prof_tdata->accum >= prof_tdata->threshold) {\n-            prof_tdata->accum -= prof_tdata->threshold;\n-            prof_sample_threshold_update(prof_tdata);\n-        }\n-        return (false);\n-    } else {\n-        prof_tdata->accum += size;\n-        return (true);\n-    }\n+\tprof_tdata_t *prof_tdata;\n+\n+\tcassert(config_prof);\n+\t/* Sampling logic is unnecessary if the interval is 1. */\n+\tassert(opt_lg_prof_sample != 0);\n+\n+\tprof_tdata = prof_tdata_get(false);\n+\tif ((uintptr_t)prof_tdata <= (uintptr_t)PROF_TDATA_STATE_MAX)\n+\t\treturn (true);\n+\n+\t/* Take care to avoid integer overflow. */\n+\tif (size >= prof_tdata->threshold - prof_tdata->accum) {\n+\t\tprof_tdata->accum -= (prof_tdata->threshold - size);\n+\t\t/* Compute new sample threshold. */\n+\t\tprof_sample_threshold_update(prof_tdata);\n+\t\twhile (prof_tdata->accum >= prof_tdata->threshold) {\n+\t\t\tprof_tdata->accum -= prof_tdata->threshold;\n+\t\t\tprof_sample_threshold_update(prof_tdata);\n+\t\t}\n+\t\treturn (false);\n+\t} else {\n+\t\tprof_tdata->accum += size;\n+\t\treturn (true);\n+\t}\n }\n \n JEMALLOC_INLINE void\n prof_malloc(const void *ptr, size_t size, prof_thr_cnt_t *cnt)\n {\n \n-    cassert(config_prof);\n-    assert(ptr != NULL);\n-    assert(size == isalloc(ptr, true));\n-\n-    if (opt_lg_prof_sample != 0) {\n-        if (prof_sample_accum_update(size)) {\n-            /*\n-             * Don't sample.  For malloc()-like allocation, it is\n-             * always possible to tell in advance how large an\n-             * object's usable size will be, so there should never\n-             * be a difference between the size passed to\n-             * PROF_ALLOC_PREP() and prof_malloc().\n-             */\n-            assert((uintptr_t)cnt == (uintptr_t)1U);\n-        }\n-    }\n-\n-    if ((uintptr_t)cnt > (uintptr_t)1U) {\n-        prof_ctx_set(ptr, cnt->ctx);\n-\n-        cnt->epoch++;\n-        /*********/\n-        mb_write();\n-        /*********/\n-        cnt->cnts.curobjs++;\n-        cnt->cnts.curbytes += size;\n-        if (opt_prof_accum) {\n-            cnt->cnts.accumobjs++;\n-            cnt->cnts.accumbytes += size;\n-        }\n-        /*********/\n-        mb_write();\n-        /*********/\n-        cnt->epoch++;\n-        /*********/\n-        mb_write();\n-        /*********/\n-    } else\n-        prof_ctx_set(ptr, (prof_ctx_t *)(uintptr_t)1U);\n+\tcassert(config_prof);\n+\tassert(ptr != NULL);\n+\tassert(size == isalloc(ptr, true));\n+\n+\tif (opt_lg_prof_sample != 0) {\n+\t\tif (prof_sample_accum_update(size)) {\n+\t\t\t/*\n+\t\t\t * Don't sample.  For malloc()-like allocation, it is\n+\t\t\t * always possible to tell in advance how large an\n+\t\t\t * object's usable size will be, so there should never\n+\t\t\t * be a difference between the size passed to\n+\t\t\t * PROF_ALLOC_PREP() and prof_malloc().\n+\t\t\t */\n+\t\t\tassert((uintptr_t)cnt == (uintptr_t)1U);\n+\t\t}\n+\t}\n+\n+\tif ((uintptr_t)cnt > (uintptr_t)1U) {\n+\t\tprof_ctx_set(ptr, cnt->ctx);\n+\n+\t\tcnt->epoch++;\n+\t\t/*********/\n+\t\tmb_write();\n+\t\t/*********/\n+\t\tcnt->cnts.curobjs++;\n+\t\tcnt->cnts.curbytes += size;\n+\t\tif (opt_prof_accum) {\n+\t\t\tcnt->cnts.accumobjs++;\n+\t\t\tcnt->cnts.accumbytes += size;\n+\t\t}\n+\t\t/*********/\n+\t\tmb_write();\n+\t\t/*********/\n+\t\tcnt->epoch++;\n+\t\t/*********/\n+\t\tmb_write();\n+\t\t/*********/\n+\t} else\n+\t\tprof_ctx_set(ptr, (prof_ctx_t *)(uintptr_t)1U);\n }\n \n JEMALLOC_INLINE void\n prof_realloc(const void *ptr, size_t size, prof_thr_cnt_t *cnt,\n     size_t old_size, prof_ctx_t *old_ctx)\n {\n-    prof_thr_cnt_t *told_cnt;\n-\n-    cassert(config_prof);\n-    assert(ptr != NULL || (uintptr_t)cnt <= (uintptr_t)1U);\n-\n-    if (ptr != NULL) {\n-        assert(size == isalloc(ptr, true));\n-        if (opt_lg_prof_sample != 0) {\n-            if (prof_sample_accum_update(size)) {\n-                /*\n-                 * Don't sample.  The size passed to\n-                 * PROF_ALLOC_PREP() was larger than what\n-                 * actually got allocated, so a backtrace was\n-                 * captured for this allocation, even though\n-                 * its actual size was insufficient to cross\n-                 * the sample threshold.\n-                 */\n-                cnt = (prof_thr_cnt_t *)(uintptr_t)1U;\n-            }\n-        }\n-    }\n-\n-    if ((uintptr_t)old_ctx > (uintptr_t)1U) {\n-        told_cnt = prof_lookup(old_ctx->bt);\n-        if (told_cnt == NULL) {\n-            /*\n-             * It's too late to propagate OOM for this realloc(),\n-             * so operate directly on old_cnt->ctx->cnt_merged.\n-             */\n-            malloc_mutex_lock(old_ctx->lock);\n-            old_ctx->cnt_merged.curobjs--;\n-            old_ctx->cnt_merged.curbytes -= old_size;\n-            malloc_mutex_unlock(old_ctx->lock);\n-            told_cnt = (prof_thr_cnt_t *)(uintptr_t)1U;\n-        }\n-    } else\n-        told_cnt = (prof_thr_cnt_t *)(uintptr_t)1U;\n-\n-    if ((uintptr_t)told_cnt > (uintptr_t)1U)\n-        told_cnt->epoch++;\n-    if ((uintptr_t)cnt > (uintptr_t)1U) {\n-        prof_ctx_set(ptr, cnt->ctx);\n-        cnt->epoch++;\n-    } else if (ptr != NULL)\n-        prof_ctx_set(ptr, (prof_ctx_t *)(uintptr_t)1U);\n-    /*********/\n-    mb_write();\n-    /*********/\n-    if ((uintptr_t)told_cnt > (uintptr_t)1U) {\n-        told_cnt->cnts.curobjs--;\n-        told_cnt->cnts.curbytes -= old_size;\n-    }\n-    if ((uintptr_t)cnt > (uintptr_t)1U) {\n-        cnt->cnts.curobjs++;\n-        cnt->cnts.curbytes += size;\n-        if (opt_prof_accum) {\n-            cnt->cnts.accumobjs++;\n-            cnt->cnts.accumbytes += size;\n-        }\n-    }\n-    /*********/\n-    mb_write();\n-    /*********/\n-    if ((uintptr_t)told_cnt > (uintptr_t)1U)\n-        told_cnt->epoch++;\n-    if ((uintptr_t)cnt > (uintptr_t)1U)\n-        cnt->epoch++;\n-    /*********/\n-    mb_write(); /* Not strictly necessary. */\n+\tprof_thr_cnt_t *told_cnt;\n+\n+\tcassert(config_prof);\n+\tassert(ptr != NULL || (uintptr_t)cnt <= (uintptr_t)1U);\n+\n+\tif (ptr != NULL) {\n+\t\tassert(size == isalloc(ptr, true));\n+\t\tif (opt_lg_prof_sample != 0) {\n+\t\t\tif (prof_sample_accum_update(size)) {\n+\t\t\t\t/*\n+\t\t\t\t * Don't sample.  The size passed to\n+\t\t\t\t * PROF_ALLOC_PREP() was larger than what\n+\t\t\t\t * actually got allocated, so a backtrace was\n+\t\t\t\t * captured for this allocation, even though\n+\t\t\t\t * its actual size was insufficient to cross\n+\t\t\t\t * the sample threshold.\n+\t\t\t\t */\n+\t\t\t\tcnt = (prof_thr_cnt_t *)(uintptr_t)1U;\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tif ((uintptr_t)old_ctx > (uintptr_t)1U) {\n+\t\ttold_cnt = prof_lookup(old_ctx->bt);\n+\t\tif (told_cnt == NULL) {\n+\t\t\t/*\n+\t\t\t * It's too late to propagate OOM for this realloc(),\n+\t\t\t * so operate directly on old_cnt->ctx->cnt_merged.\n+\t\t\t */\n+\t\t\tmalloc_mutex_lock(old_ctx->lock);\n+\t\t\told_ctx->cnt_merged.curobjs--;\n+\t\t\told_ctx->cnt_merged.curbytes -= old_size;\n+\t\t\tmalloc_mutex_unlock(old_ctx->lock);\n+\t\t\ttold_cnt = (prof_thr_cnt_t *)(uintptr_t)1U;\n+\t\t}\n+\t} else\n+\t\ttold_cnt = (prof_thr_cnt_t *)(uintptr_t)1U;\n+\n+\tif ((uintptr_t)told_cnt > (uintptr_t)1U)\n+\t\ttold_cnt->epoch++;\n+\tif ((uintptr_t)cnt > (uintptr_t)1U) {\n+\t\tprof_ctx_set(ptr, cnt->ctx);\n+\t\tcnt->epoch++;\n+\t} else if (ptr != NULL)\n+\t\tprof_ctx_set(ptr, (prof_ctx_t *)(uintptr_t)1U);\n+\t/*********/\n+\tmb_write();\n+\t/*********/\n+\tif ((uintptr_t)told_cnt > (uintptr_t)1U) {\n+\t\ttold_cnt->cnts.curobjs--;\n+\t\ttold_cnt->cnts.curbytes -= old_size;\n+\t}\n+\tif ((uintptr_t)cnt > (uintptr_t)1U) {\n+\t\tcnt->cnts.curobjs++;\n+\t\tcnt->cnts.curbytes += size;\n+\t\tif (opt_prof_accum) {\n+\t\t\tcnt->cnts.accumobjs++;\n+\t\t\tcnt->cnts.accumbytes += size;\n+\t\t}\n+\t}\n+\t/*********/\n+\tmb_write();\n+\t/*********/\n+\tif ((uintptr_t)told_cnt > (uintptr_t)1U)\n+\t\ttold_cnt->epoch++;\n+\tif ((uintptr_t)cnt > (uintptr_t)1U)\n+\t\tcnt->epoch++;\n+\t/*********/\n+\tmb_write(); /* Not strictly necessary. */\n }\n \n JEMALLOC_INLINE void\n prof_free(const void *ptr, size_t size)\n {\n-    prof_ctx_t *ctx = prof_ctx_get(ptr);\n-\n-    cassert(config_prof);\n-\n-    if ((uintptr_t)ctx > (uintptr_t)1) {\n-        prof_thr_cnt_t *tcnt;\n-        assert(size == isalloc(ptr, true));\n-        tcnt = prof_lookup(ctx->bt);\n-\n-        if (tcnt != NULL) {\n-            tcnt->epoch++;\n-            /*********/\n-            mb_write();\n-            /*********/\n-            tcnt->cnts.curobjs--;\n-            tcnt->cnts.curbytes -= size;\n-            /*********/\n-            mb_write();\n-            /*********/\n-            tcnt->epoch++;\n-            /*********/\n-            mb_write();\n-            /*********/\n-        } else {\n-            /*\n-             * OOM during free() cannot be propagated, so operate\n-             * directly on cnt->ctx->cnt_merged.\n-             */\n-            malloc_mutex_lock(ctx->lock);\n-            ctx->cnt_merged.curobjs--;\n-            ctx->cnt_merged.curbytes -= size;\n-            malloc_mutex_unlock(ctx->lock);\n-        }\n-    }\n+\tprof_ctx_t *ctx = prof_ctx_get(ptr);\n+\n+\tcassert(config_prof);\n+\n+\tif ((uintptr_t)ctx > (uintptr_t)1) {\n+\t\tprof_thr_cnt_t *tcnt;\n+\t\tassert(size == isalloc(ptr, true));\n+\t\ttcnt = prof_lookup(ctx->bt);\n+\n+\t\tif (tcnt != NULL) {\n+\t\t\ttcnt->epoch++;\n+\t\t\t/*********/\n+\t\t\tmb_write();\n+\t\t\t/*********/\n+\t\t\ttcnt->cnts.curobjs--;\n+\t\t\ttcnt->cnts.curbytes -= size;\n+\t\t\t/*********/\n+\t\t\tmb_write();\n+\t\t\t/*********/\n+\t\t\ttcnt->epoch++;\n+\t\t\t/*********/\n+\t\t\tmb_write();\n+\t\t\t/*********/\n+\t\t} else {\n+\t\t\t/*\n+\t\t\t * OOM during free() cannot be propagated, so operate\n+\t\t\t * directly on cnt->ctx->cnt_merged.\n+\t\t\t */\n+\t\t\tmalloc_mutex_lock(ctx->lock);\n+\t\t\tctx->cnt_merged.curobjs--;\n+\t\t\tctx->cnt_merged.curbytes -= size;\n+\t\t\tmalloc_mutex_unlock(ctx->lock);\n+\t\t}\n+\t}\n }\n #endif\n "}, {"sha": "a9ed2393f0c2eae5cedbcd139e8bc870ae1420ad", "filename": "src/rt/jemalloc/include/jemalloc/internal/ql.h", "status": "modified", "additions": 35, "deletions": 35, "changes": 70, "blob_url": "https://github.com/rust-lang/rust/blob/996301331e2d8ac7611d15ca82cc3ebc1abb632d/src%2Frt%2Fjemalloc%2Finclude%2Fjemalloc%2Finternal%2Fql.h", "raw_url": "https://github.com/rust-lang/rust/raw/996301331e2d8ac7611d15ca82cc3ebc1abb632d/src%2Frt%2Fjemalloc%2Finclude%2Fjemalloc%2Finternal%2Fql.h", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Frt%2Fjemalloc%2Finclude%2Fjemalloc%2Finternal%2Fql.h?ref=996301331e2d8ac7611d15ca82cc3ebc1abb632d", "patch": "@@ -3,7 +3,7 @@\n  */\n #define ql_head(a_type)\t\t\t\t\t\t\t\\\n struct {\t\t\t\t\t\t\t\t\\\n-    a_type *qlh_first;\t\t\t\t\t\t\\\n+\ta_type *qlh_first;\t\t\t\t\t\t\\\n }\n \n #define ql_head_initializer(a_head) {NULL}\n@@ -12,72 +12,72 @@ struct {\t\t\t\t\t\t\t\t\\\n \n /* List functions. */\n #define ql_new(a_head) do {\t\t\t\t\t\t\\\n-    (a_head)->qlh_first = NULL;\t\t\t\t\t\\\n+\t(a_head)->qlh_first = NULL;\t\t\t\t\t\\\n } while (0)\n \n #define ql_elm_new(a_elm, a_field) qr_new((a_elm), a_field)\n \n #define ql_first(a_head) ((a_head)->qlh_first)\n \n #define ql_last(a_head, a_field)\t\t\t\t\t\\\n-    ((ql_first(a_head) != NULL)\t\t\t\t\t\\\n-        ? qr_prev(ql_first(a_head), a_field) : NULL)\n+\t((ql_first(a_head) != NULL)\t\t\t\t\t\\\n+\t    ? qr_prev(ql_first(a_head), a_field) : NULL)\n \n #define ql_next(a_head, a_elm, a_field)\t\t\t\t\t\\\n-    ((ql_last(a_head, a_field) != (a_elm))\t\t\t\t\\\n-        ? qr_next((a_elm), a_field)\t: NULL)\n+\t((ql_last(a_head, a_field) != (a_elm))\t\t\t\t\\\n+\t    ? qr_next((a_elm), a_field)\t: NULL)\n \n #define ql_prev(a_head, a_elm, a_field)\t\t\t\t\t\\\n-    ((ql_first(a_head) != (a_elm)) ? qr_prev((a_elm), a_field)\t\\\n-                       : NULL)\n+\t((ql_first(a_head) != (a_elm)) ? qr_prev((a_elm), a_field)\t\\\n+\t\t\t\t       : NULL)\n \n #define ql_before_insert(a_head, a_qlelm, a_elm, a_field) do {\t\t\\\n-    qr_before_insert((a_qlelm), (a_elm), a_field);\t\t\t\\\n-    if (ql_first(a_head) == (a_qlelm)) {\t\t\t\t\\\n-        ql_first(a_head) = (a_elm);\t\t\t\t\\\n-    }\t\t\t\t\t\t\t\t\\\n+\tqr_before_insert((a_qlelm), (a_elm), a_field);\t\t\t\\\n+\tif (ql_first(a_head) == (a_qlelm)) {\t\t\t\t\\\n+\t\tql_first(a_head) = (a_elm);\t\t\t\t\\\n+\t}\t\t\t\t\t\t\t\t\\\n } while (0)\n \n #define ql_after_insert(a_qlelm, a_elm, a_field)\t\t\t\\\n-    qr_after_insert((a_qlelm), (a_elm), a_field)\n+\tqr_after_insert((a_qlelm), (a_elm), a_field)\n \n #define ql_head_insert(a_head, a_elm, a_field) do {\t\t\t\\\n-    if (ql_first(a_head) != NULL) {\t\t\t\t\t\\\n-        qr_before_insert(ql_first(a_head), (a_elm), a_field);\t\\\n-    }\t\t\t\t\t\t\t\t\\\n-    ql_first(a_head) = (a_elm);\t\t\t\t\t\\\n+\tif (ql_first(a_head) != NULL) {\t\t\t\t\t\\\n+\t\tqr_before_insert(ql_first(a_head), (a_elm), a_field);\t\\\n+\t}\t\t\t\t\t\t\t\t\\\n+\tql_first(a_head) = (a_elm);\t\t\t\t\t\\\n } while (0)\n \n #define ql_tail_insert(a_head, a_elm, a_field) do {\t\t\t\\\n-    if (ql_first(a_head) != NULL) {\t\t\t\t\t\\\n-        qr_before_insert(ql_first(a_head), (a_elm), a_field);\t\\\n-    }\t\t\t\t\t\t\t\t\\\n-    ql_first(a_head) = qr_next((a_elm), a_field);\t\t\t\\\n+\tif (ql_first(a_head) != NULL) {\t\t\t\t\t\\\n+\t\tqr_before_insert(ql_first(a_head), (a_elm), a_field);\t\\\n+\t}\t\t\t\t\t\t\t\t\\\n+\tql_first(a_head) = qr_next((a_elm), a_field);\t\t\t\\\n } while (0)\n \n #define ql_remove(a_head, a_elm, a_field) do {\t\t\t\t\\\n-    if (ql_first(a_head) == (a_elm)) {\t\t\t\t\\\n-        ql_first(a_head) = qr_next(ql_first(a_head), a_field);\t\\\n-    }\t\t\t\t\t\t\t\t\\\n-    if (ql_first(a_head) != (a_elm)) {\t\t\t\t\\\n-        qr_remove((a_elm), a_field);\t\t\t\t\\\n-    } else {\t\t\t\t\t\t\t\\\n-        ql_first(a_head) = NULL;\t\t\t\t\\\n-    }\t\t\t\t\t\t\t\t\\\n+\tif (ql_first(a_head) == (a_elm)) {\t\t\t\t\\\n+\t\tql_first(a_head) = qr_next(ql_first(a_head), a_field);\t\\\n+\t}\t\t\t\t\t\t\t\t\\\n+\tif (ql_first(a_head) != (a_elm)) {\t\t\t\t\\\n+\t\tqr_remove((a_elm), a_field);\t\t\t\t\\\n+\t} else {\t\t\t\t\t\t\t\\\n+\t\tql_first(a_head) = NULL;\t\t\t\t\\\n+\t}\t\t\t\t\t\t\t\t\\\n } while (0)\n \n #define ql_head_remove(a_head, a_type, a_field) do {\t\t\t\\\n-    a_type *t = ql_first(a_head);\t\t\t\t\t\\\n-    ql_remove((a_head), t, a_field);\t\t\t\t\\\n+\ta_type *t = ql_first(a_head);\t\t\t\t\t\\\n+\tql_remove((a_head), t, a_field);\t\t\t\t\\\n } while (0)\n \n #define ql_tail_remove(a_head, a_type, a_field) do {\t\t\t\\\n-    a_type *t = ql_last(a_head, a_field);\t\t\t\t\\\n-    ql_remove((a_head), t, a_field);\t\t\t\t\\\n+\ta_type *t = ql_last(a_head, a_field);\t\t\t\t\\\n+\tql_remove((a_head), t, a_field);\t\t\t\t\\\n } while (0)\n \n #define ql_foreach(a_var, a_head, a_field)\t\t\t\t\\\n-    qr_foreach((a_var), ql_first(a_head), a_field)\n+\tqr_foreach((a_var), ql_first(a_head), a_field)\n \n #define ql_reverse_foreach(a_var, a_head, a_field)\t\t\t\\\n-    qr_reverse_foreach((a_var), ql_first(a_head), a_field)\n+\tqr_reverse_foreach((a_var), ql_first(a_head), a_field)"}, {"sha": "fe22352feddc42cccfe751a1f24bbb0a55f7bf7b", "filename": "src/rt/jemalloc/include/jemalloc/internal/qr.h", "status": "modified", "additions": 33, "deletions": 33, "changes": 66, "blob_url": "https://github.com/rust-lang/rust/blob/996301331e2d8ac7611d15ca82cc3ebc1abb632d/src%2Frt%2Fjemalloc%2Finclude%2Fjemalloc%2Finternal%2Fqr.h", "raw_url": "https://github.com/rust-lang/rust/raw/996301331e2d8ac7611d15ca82cc3ebc1abb632d/src%2Frt%2Fjemalloc%2Finclude%2Fjemalloc%2Finternal%2Fqr.h", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Frt%2Fjemalloc%2Finclude%2Fjemalloc%2Finternal%2Fqr.h?ref=996301331e2d8ac7611d15ca82cc3ebc1abb632d", "patch": "@@ -1,67 +1,67 @@\n /* Ring definitions. */\n #define qr(a_type)\t\t\t\t\t\t\t\\\n struct {\t\t\t\t\t\t\t\t\\\n-    a_type\t*qre_next;\t\t\t\t\t\t\\\n-    a_type\t*qre_prev;\t\t\t\t\t\t\\\n+\ta_type\t*qre_next;\t\t\t\t\t\t\\\n+\ta_type\t*qre_prev;\t\t\t\t\t\t\\\n }\n \n /* Ring functions. */\n #define qr_new(a_qr, a_field) do {\t\t\t\t\t\\\n-    (a_qr)->a_field.qre_next = (a_qr);\t\t\t\t\\\n-    (a_qr)->a_field.qre_prev = (a_qr);\t\t\t\t\\\n+\t(a_qr)->a_field.qre_next = (a_qr);\t\t\t\t\\\n+\t(a_qr)->a_field.qre_prev = (a_qr);\t\t\t\t\\\n } while (0)\n \n #define qr_next(a_qr, a_field) ((a_qr)->a_field.qre_next)\n \n #define qr_prev(a_qr, a_field) ((a_qr)->a_field.qre_prev)\n \n #define qr_before_insert(a_qrelm, a_qr, a_field) do {\t\t\t\\\n-    (a_qr)->a_field.qre_prev = (a_qrelm)->a_field.qre_prev;\t\t\\\n-    (a_qr)->a_field.qre_next = (a_qrelm);\t\t\t\t\\\n-    (a_qr)->a_field.qre_prev->a_field.qre_next = (a_qr);\t\t\\\n-    (a_qrelm)->a_field.qre_prev = (a_qr);\t\t\t\t\\\n+\t(a_qr)->a_field.qre_prev = (a_qrelm)->a_field.qre_prev;\t\t\\\n+\t(a_qr)->a_field.qre_next = (a_qrelm);\t\t\t\t\\\n+\t(a_qr)->a_field.qre_prev->a_field.qre_next = (a_qr);\t\t\\\n+\t(a_qrelm)->a_field.qre_prev = (a_qr);\t\t\t\t\\\n } while (0)\n \n #define qr_after_insert(a_qrelm, a_qr, a_field)\t\t\t\t\\\n     do\t\t\t\t\t\t\t\t\t\\\n     {\t\t\t\t\t\t\t\t\t\\\n-    (a_qr)->a_field.qre_next = (a_qrelm)->a_field.qre_next;\t\t\\\n-    (a_qr)->a_field.qre_prev = (a_qrelm);\t\t\t\t\\\n-    (a_qr)->a_field.qre_next->a_field.qre_prev = (a_qr);\t\t\\\n-    (a_qrelm)->a_field.qre_next = (a_qr);\t\t\t\t\\\n+\t(a_qr)->a_field.qre_next = (a_qrelm)->a_field.qre_next;\t\t\\\n+\t(a_qr)->a_field.qre_prev = (a_qrelm);\t\t\t\t\\\n+\t(a_qr)->a_field.qre_next->a_field.qre_prev = (a_qr);\t\t\\\n+\t(a_qrelm)->a_field.qre_next = (a_qr);\t\t\t\t\\\n     } while (0)\n \n #define qr_meld(a_qr_a, a_qr_b, a_field) do {\t\t\t\t\\\n-    void *t;\t\t\t\t\t\t\t\\\n-    (a_qr_a)->a_field.qre_prev->a_field.qre_next = (a_qr_b);\t\\\n-    (a_qr_b)->a_field.qre_prev->a_field.qre_next = (a_qr_a);\t\\\n-    t = (a_qr_a)->a_field.qre_prev;\t\t\t\t\t\\\n-    (a_qr_a)->a_field.qre_prev = (a_qr_b)->a_field.qre_prev;\t\\\n-    (a_qr_b)->a_field.qre_prev = t;\t\t\t\t\t\\\n+\tvoid *t;\t\t\t\t\t\t\t\\\n+\t(a_qr_a)->a_field.qre_prev->a_field.qre_next = (a_qr_b);\t\\\n+\t(a_qr_b)->a_field.qre_prev->a_field.qre_next = (a_qr_a);\t\\\n+\tt = (a_qr_a)->a_field.qre_prev;\t\t\t\t\t\\\n+\t(a_qr_a)->a_field.qre_prev = (a_qr_b)->a_field.qre_prev;\t\\\n+\t(a_qr_b)->a_field.qre_prev = t;\t\t\t\t\t\\\n } while (0)\n \n /* qr_meld() and qr_split() are functionally equivalent, so there's no need to\n  * have two copies of the code. */\n #define qr_split(a_qr_a, a_qr_b, a_field)\t\t\t\t\\\n-    qr_meld((a_qr_a), (a_qr_b), a_field)\n+\tqr_meld((a_qr_a), (a_qr_b), a_field)\n \n #define qr_remove(a_qr, a_field) do {\t\t\t\t\t\\\n-    (a_qr)->a_field.qre_prev->a_field.qre_next\t\t\t\\\n-        = (a_qr)->a_field.qre_next;\t\t\t\t\t\\\n-    (a_qr)->a_field.qre_next->a_field.qre_prev\t\t\t\\\n-        = (a_qr)->a_field.qre_prev;\t\t\t\t\t\\\n-    (a_qr)->a_field.qre_next = (a_qr);\t\t\t\t\\\n-    (a_qr)->a_field.qre_prev = (a_qr);\t\t\t\t\\\n+\t(a_qr)->a_field.qre_prev->a_field.qre_next\t\t\t\\\n+\t    = (a_qr)->a_field.qre_next;\t\t\t\t\t\\\n+\t(a_qr)->a_field.qre_next->a_field.qre_prev\t\t\t\\\n+\t    = (a_qr)->a_field.qre_prev;\t\t\t\t\t\\\n+\t(a_qr)->a_field.qre_next = (a_qr);\t\t\t\t\\\n+\t(a_qr)->a_field.qre_prev = (a_qr);\t\t\t\t\\\n } while (0)\n \n #define qr_foreach(var, a_qr, a_field)\t\t\t\t\t\\\n-    for ((var) = (a_qr);\t\t\t\t\t\t\\\n-        (var) != NULL;\t\t\t\t\t\t\\\n-        (var) = (((var)->a_field.qre_next != (a_qr))\t\t\\\n-        ? (var)->a_field.qre_next : NULL))\n+\tfor ((var) = (a_qr);\t\t\t\t\t\t\\\n+\t    (var) != NULL;\t\t\t\t\t\t\\\n+\t    (var) = (((var)->a_field.qre_next != (a_qr))\t\t\\\n+\t    ? (var)->a_field.qre_next : NULL))\n \n #define qr_reverse_foreach(var, a_qr, a_field)\t\t\t\t\\\n-    for ((var) = ((a_qr) != NULL) ? qr_prev(a_qr, a_field) : NULL;\t\\\n-        (var) != NULL;\t\t\t\t\t\t\\\n-        (var) = (((var) != (a_qr))\t\t\t\t\t\\\n-        ? (var)->a_field.qre_prev : NULL))\n+\tfor ((var) = ((a_qr) != NULL) ? qr_prev(a_qr, a_field) : NULL;\t\\\n+\t    (var) != NULL;\t\t\t\t\t\t\\\n+\t    (var) = (((var) != (a_qr))\t\t\t\t\t\\\n+\t    ? (var)->a_field.qre_prev : NULL))"}, {"sha": "16f677f73da09690397bf728ca218cbe9d2b3ff7", "filename": "src/rt/jemalloc/include/jemalloc/internal/quarantine.h", "status": "modified", "additions": 13, "deletions": 12, "changes": 25, "blob_url": "https://github.com/rust-lang/rust/blob/996301331e2d8ac7611d15ca82cc3ebc1abb632d/src%2Frt%2Fjemalloc%2Finclude%2Fjemalloc%2Finternal%2Fquarantine.h", "raw_url": "https://github.com/rust-lang/rust/raw/996301331e2d8ac7611d15ca82cc3ebc1abb632d/src%2Frt%2Fjemalloc%2Finclude%2Fjemalloc%2Finternal%2Fquarantine.h", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Frt%2Fjemalloc%2Finclude%2Fjemalloc%2Finternal%2Fquarantine.h?ref=996301331e2d8ac7611d15ca82cc3ebc1abb632d", "patch": "@@ -12,17 +12,17 @@ typedef struct quarantine_s quarantine_t;\n #ifdef JEMALLOC_H_STRUCTS\n \n struct quarantine_obj_s {\n-    void\t*ptr;\n-    size_t\tusize;\n+\tvoid\t*ptr;\n+\tsize_t\tusize;\n };\n \n struct quarantine_s {\n-    size_t\t\t\tcurbytes;\n-    size_t\t\t\tcurobjs;\n-    size_t\t\t\tfirst;\n+\tsize_t\t\t\tcurbytes;\n+\tsize_t\t\t\tcurobjs;\n+\tsize_t\t\t\tfirst;\n #define\tLG_MAXOBJS_INIT 10\n-    size_t\t\t\tlg_maxobjs;\n-    quarantine_obj_t\tobjs[1]; /* Dynamically sized ring buffer. */\n+\tsize_t\t\t\tlg_maxobjs;\n+\tquarantine_obj_t\tobjs[1]; /* Dynamically sized ring buffer. */\n };\n \n #endif /* JEMALLOC_H_STRUCTS */\n@@ -52,15 +52,16 @@ malloc_tsd_funcs(JEMALLOC_ALWAYS_INLINE, quarantine, quarantine_t *, NULL,\n JEMALLOC_ALWAYS_INLINE void\n quarantine_alloc_hook(void)\n {\n-    quarantine_t *quarantine;\n+\tquarantine_t *quarantine;\n \n-    assert(config_fill && opt_quarantine);\n+\tassert(config_fill && opt_quarantine);\n \n-    quarantine = *quarantine_tsd_get();\n-    if (quarantine == NULL)\n-        quarantine_init(LG_MAXOBJS_INIT);\n+\tquarantine = *quarantine_tsd_get();\n+\tif (quarantine == NULL)\n+\t\tquarantine_init(LG_MAXOBJS_INIT);\n }\n #endif\n \n #endif /* JEMALLOC_H_INLINES */\n /******************************************************************************/\n+"}, {"sha": "7b675f09051e51bb09c976b2f2002469f8f658fd", "filename": "src/rt/jemalloc/include/jemalloc/internal/rb.h", "status": "modified", "additions": 497, "deletions": 497, "changes": 994, "blob_url": "https://github.com/rust-lang/rust/blob/996301331e2d8ac7611d15ca82cc3ebc1abb632d/src%2Frt%2Fjemalloc%2Finclude%2Fjemalloc%2Finternal%2Frb.h", "raw_url": "https://github.com/rust-lang/rust/raw/996301331e2d8ac7611d15ca82cc3ebc1abb632d/src%2Frt%2Fjemalloc%2Finclude%2Fjemalloc%2Finternal%2Frb.h", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Frt%2Fjemalloc%2Finclude%2Fjemalloc%2Finternal%2Frb.h?ref=996301331e2d8ac7611d15ca82cc3ebc1abb632d", "patch": "@@ -123,20 +123,20 @@ struct {\t\t\t\t\t\t\t\t\\\n #define\trbtn_first(a_type, a_field, a_rbt, a_root, r_node) do {\t\t\\\n     (r_node) = (a_root);\t\t\t\t\t\t\\\n     if ((r_node) != &(a_rbt)->rbt_nil) {\t\t\t\t\\\n-    for (;\t\t\t\t\t\t\t\t\\\n-      rbtn_left_get(a_type, a_field, (r_node)) != &(a_rbt)->rbt_nil;\\\n-      (r_node) = rbtn_left_get(a_type, a_field, (r_node))) {\t\\\n-    }\t\t\t\t\t\t\t\t\\\n+\tfor (;\t\t\t\t\t\t\t\t\\\n+\t  rbtn_left_get(a_type, a_field, (r_node)) != &(a_rbt)->rbt_nil;\\\n+\t  (r_node) = rbtn_left_get(a_type, a_field, (r_node))) {\t\\\n+\t}\t\t\t\t\t\t\t\t\\\n     }\t\t\t\t\t\t\t\t\t\\\n } while (0)\n \n #define\trbtn_last(a_type, a_field, a_rbt, a_root, r_node) do {\t\t\\\n     (r_node) = (a_root);\t\t\t\t\t\t\\\n     if ((r_node) != &(a_rbt)->rbt_nil) {\t\t\t\t\\\n-    for (; rbtn_right_get(a_type, a_field, (r_node)) !=\t\t\\\n-      &(a_rbt)->rbt_nil; (r_node) = rbtn_right_get(a_type, a_field,\t\\\n-      (r_node))) {\t\t\t\t\t\t\t\\\n-    }\t\t\t\t\t\t\t\t\\\n+\tfor (; rbtn_right_get(a_type, a_field, (r_node)) !=\t\t\\\n+\t  &(a_rbt)->rbt_nil; (r_node) = rbtn_right_get(a_type, a_field,\t\\\n+\t  (r_node))) {\t\t\t\t\t\t\t\\\n+\t}\t\t\t\t\t\t\t\t\\\n     }\t\t\t\t\t\t\t\t\t\\\n } while (0)\n \n@@ -318,7 +318,7 @@ a_prefix##first(a_rbt_type *rbtree) {\t\t\t\t\t\\\n     a_type *ret;\t\t\t\t\t\t\t\\\n     rbtn_first(a_type, a_field, rbtree, rbtree->rbt_root, ret);\t\t\\\n     if (ret == &rbtree->rbt_nil) {\t\t\t\t\t\\\n-    ret = NULL;\t\t\t\t\t\t\t\\\n+\tret = NULL;\t\t\t\t\t\t\t\\\n     }\t\t\t\t\t\t\t\t\t\\\n     return (ret);\t\t\t\t\t\t\t\\\n }\t\t\t\t\t\t\t\t\t\\\n@@ -327,63 +327,63 @@ a_prefix##last(a_rbt_type *rbtree) {\t\t\t\t\t\\\n     a_type *ret;\t\t\t\t\t\t\t\\\n     rbtn_last(a_type, a_field, rbtree, rbtree->rbt_root, ret);\t\t\\\n     if (ret == &rbtree->rbt_nil) {\t\t\t\t\t\\\n-    ret = NULL;\t\t\t\t\t\t\t\\\n+\tret = NULL;\t\t\t\t\t\t\t\\\n     }\t\t\t\t\t\t\t\t\t\\\n     return (ret);\t\t\t\t\t\t\t\\\n }\t\t\t\t\t\t\t\t\t\\\n a_attr a_type *\t\t\t\t\t\t\t\t\\\n a_prefix##next(a_rbt_type *rbtree, a_type *node) {\t\t\t\\\n     a_type *ret;\t\t\t\t\t\t\t\\\n     if (rbtn_right_get(a_type, a_field, node) != &rbtree->rbt_nil) {\t\\\n-    rbtn_first(a_type, a_field, rbtree, rbtn_right_get(a_type,\t\\\n-      a_field, node), ret);\t\t\t\t\t\t\\\n+\trbtn_first(a_type, a_field, rbtree, rbtn_right_get(a_type,\t\\\n+\t  a_field, node), ret);\t\t\t\t\t\t\\\n     } else {\t\t\t\t\t\t\t\t\\\n-    a_type *tnode = rbtree->rbt_root;\t\t\t\t\\\n-    assert(tnode != &rbtree->rbt_nil);\t\t\t\t\\\n-    ret = &rbtree->rbt_nil;\t\t\t\t\t\t\\\n-    while (true) {\t\t\t\t\t\t\t\\\n-        int cmp = (a_cmp)(node, tnode);\t\t\t\t\\\n-        if (cmp < 0) {\t\t\t\t\t\t\\\n-        ret = tnode;\t\t\t\t\t\t\\\n-        tnode = rbtn_left_get(a_type, a_field, tnode);\t\t\\\n-        } else if (cmp > 0) {\t\t\t\t\t\\\n-        tnode = rbtn_right_get(a_type, a_field, tnode);\t\t\\\n-        } else {\t\t\t\t\t\t\t\\\n-        break;\t\t\t\t\t\t\t\\\n-        }\t\t\t\t\t\t\t\t\\\n-        assert(tnode != &rbtree->rbt_nil);\t\t\t\t\\\n-    }\t\t\t\t\t\t\t\t\\\n+\ta_type *tnode = rbtree->rbt_root;\t\t\t\t\\\n+\tassert(tnode != &rbtree->rbt_nil);\t\t\t\t\\\n+\tret = &rbtree->rbt_nil;\t\t\t\t\t\t\\\n+\twhile (true) {\t\t\t\t\t\t\t\\\n+\t    int cmp = (a_cmp)(node, tnode);\t\t\t\t\\\n+\t    if (cmp < 0) {\t\t\t\t\t\t\\\n+\t\tret = tnode;\t\t\t\t\t\t\\\n+\t\ttnode = rbtn_left_get(a_type, a_field, tnode);\t\t\\\n+\t    } else if (cmp > 0) {\t\t\t\t\t\\\n+\t\ttnode = rbtn_right_get(a_type, a_field, tnode);\t\t\\\n+\t    } else {\t\t\t\t\t\t\t\\\n+\t\tbreak;\t\t\t\t\t\t\t\\\n+\t    }\t\t\t\t\t\t\t\t\\\n+\t    assert(tnode != &rbtree->rbt_nil);\t\t\t\t\\\n+\t}\t\t\t\t\t\t\t\t\\\n     }\t\t\t\t\t\t\t\t\t\\\n     if (ret == &rbtree->rbt_nil) {\t\t\t\t\t\\\n-    ret = (NULL);\t\t\t\t\t\t\t\\\n+\tret = (NULL);\t\t\t\t\t\t\t\\\n     }\t\t\t\t\t\t\t\t\t\\\n     return (ret);\t\t\t\t\t\t\t\\\n }\t\t\t\t\t\t\t\t\t\\\n a_attr a_type *\t\t\t\t\t\t\t\t\\\n a_prefix##prev(a_rbt_type *rbtree, a_type *node) {\t\t\t\\\n     a_type *ret;\t\t\t\t\t\t\t\\\n     if (rbtn_left_get(a_type, a_field, node) != &rbtree->rbt_nil) {\t\\\n-    rbtn_last(a_type, a_field, rbtree, rbtn_left_get(a_type,\t\\\n-      a_field, node), ret);\t\t\t\t\t\t\\\n+\trbtn_last(a_type, a_field, rbtree, rbtn_left_get(a_type,\t\\\n+\t  a_field, node), ret);\t\t\t\t\t\t\\\n     } else {\t\t\t\t\t\t\t\t\\\n-    a_type *tnode = rbtree->rbt_root;\t\t\t\t\\\n-    assert(tnode != &rbtree->rbt_nil);\t\t\t\t\\\n-    ret = &rbtree->rbt_nil;\t\t\t\t\t\t\\\n-    while (true) {\t\t\t\t\t\t\t\\\n-        int cmp = (a_cmp)(node, tnode);\t\t\t\t\\\n-        if (cmp < 0) {\t\t\t\t\t\t\\\n-        tnode = rbtn_left_get(a_type, a_field, tnode);\t\t\\\n-        } else if (cmp > 0) {\t\t\t\t\t\\\n-        ret = tnode;\t\t\t\t\t\t\\\n-        tnode = rbtn_right_get(a_type, a_field, tnode);\t\t\\\n-        } else {\t\t\t\t\t\t\t\\\n-        break;\t\t\t\t\t\t\t\\\n-        }\t\t\t\t\t\t\t\t\\\n-        assert(tnode != &rbtree->rbt_nil);\t\t\t\t\\\n-    }\t\t\t\t\t\t\t\t\\\n+\ta_type *tnode = rbtree->rbt_root;\t\t\t\t\\\n+\tassert(tnode != &rbtree->rbt_nil);\t\t\t\t\\\n+\tret = &rbtree->rbt_nil;\t\t\t\t\t\t\\\n+\twhile (true) {\t\t\t\t\t\t\t\\\n+\t    int cmp = (a_cmp)(node, tnode);\t\t\t\t\\\n+\t    if (cmp < 0) {\t\t\t\t\t\t\\\n+\t\ttnode = rbtn_left_get(a_type, a_field, tnode);\t\t\\\n+\t    } else if (cmp > 0) {\t\t\t\t\t\\\n+\t\tret = tnode;\t\t\t\t\t\t\\\n+\t\ttnode = rbtn_right_get(a_type, a_field, tnode);\t\t\\\n+\t    } else {\t\t\t\t\t\t\t\\\n+\t\tbreak;\t\t\t\t\t\t\t\\\n+\t    }\t\t\t\t\t\t\t\t\\\n+\t    assert(tnode != &rbtree->rbt_nil);\t\t\t\t\\\n+\t}\t\t\t\t\t\t\t\t\\\n     }\t\t\t\t\t\t\t\t\t\\\n     if (ret == &rbtree->rbt_nil) {\t\t\t\t\t\\\n-    ret = (NULL);\t\t\t\t\t\t\t\\\n+\tret = (NULL);\t\t\t\t\t\t\t\\\n     }\t\t\t\t\t\t\t\t\t\\\n     return (ret);\t\t\t\t\t\t\t\\\n }\t\t\t\t\t\t\t\t\t\\\n@@ -394,14 +394,14 @@ a_prefix##search(a_rbt_type *rbtree, a_type *key) {\t\t\t\\\n     ret = rbtree->rbt_root;\t\t\t\t\t\t\\\n     while (ret != &rbtree->rbt_nil\t\t\t\t\t\\\n       && (cmp = (a_cmp)(key, ret)) != 0) {\t\t\t\t\\\n-    if (cmp < 0) {\t\t\t\t\t\t\t\\\n-        ret = rbtn_left_get(a_type, a_field, ret);\t\t\t\\\n-    } else {\t\t\t\t\t\t\t\\\n-        ret = rbtn_right_get(a_type, a_field, ret);\t\t\t\\\n-    }\t\t\t\t\t\t\t\t\\\n+\tif (cmp < 0) {\t\t\t\t\t\t\t\\\n+\t    ret = rbtn_left_get(a_type, a_field, ret);\t\t\t\\\n+\t} else {\t\t\t\t\t\t\t\\\n+\t    ret = rbtn_right_get(a_type, a_field, ret);\t\t\t\\\n+\t}\t\t\t\t\t\t\t\t\\\n     }\t\t\t\t\t\t\t\t\t\\\n     if (ret == &rbtree->rbt_nil) {\t\t\t\t\t\\\n-    ret = (NULL);\t\t\t\t\t\t\t\\\n+\tret = (NULL);\t\t\t\t\t\t\t\\\n     }\t\t\t\t\t\t\t\t\t\\\n     return (ret);\t\t\t\t\t\t\t\\\n }\t\t\t\t\t\t\t\t\t\\\n@@ -411,19 +411,19 @@ a_prefix##nsearch(a_rbt_type *rbtree, a_type *key) {\t\t\t\\\n     a_type *tnode = rbtree->rbt_root;\t\t\t\t\t\\\n     ret = &rbtree->rbt_nil;\t\t\t\t\t\t\\\n     while (tnode != &rbtree->rbt_nil) {\t\t\t\t\t\\\n-    int cmp = (a_cmp)(key, tnode);\t\t\t\t\t\\\n-    if (cmp < 0) {\t\t\t\t\t\t\t\\\n-        ret = tnode;\t\t\t\t\t\t\\\n-        tnode = rbtn_left_get(a_type, a_field, tnode);\t\t\\\n-    } else if (cmp > 0) {\t\t\t\t\t\t\\\n-        tnode = rbtn_right_get(a_type, a_field, tnode);\t\t\\\n-    } else {\t\t\t\t\t\t\t\\\n-        ret = tnode;\t\t\t\t\t\t\\\n-        break;\t\t\t\t\t\t\t\\\n-    }\t\t\t\t\t\t\t\t\\\n+\tint cmp = (a_cmp)(key, tnode);\t\t\t\t\t\\\n+\tif (cmp < 0) {\t\t\t\t\t\t\t\\\n+\t    ret = tnode;\t\t\t\t\t\t\\\n+\t    tnode = rbtn_left_get(a_type, a_field, tnode);\t\t\\\n+\t} else if (cmp > 0) {\t\t\t\t\t\t\\\n+\t    tnode = rbtn_right_get(a_type, a_field, tnode);\t\t\\\n+\t} else {\t\t\t\t\t\t\t\\\n+\t    ret = tnode;\t\t\t\t\t\t\\\n+\t    break;\t\t\t\t\t\t\t\\\n+\t}\t\t\t\t\t\t\t\t\\\n     }\t\t\t\t\t\t\t\t\t\\\n     if (ret == &rbtree->rbt_nil) {\t\t\t\t\t\\\n-    ret = (NULL);\t\t\t\t\t\t\t\\\n+\tret = (NULL);\t\t\t\t\t\t\t\\\n     }\t\t\t\t\t\t\t\t\t\\\n     return (ret);\t\t\t\t\t\t\t\\\n }\t\t\t\t\t\t\t\t\t\\\n@@ -433,85 +433,85 @@ a_prefix##psearch(a_rbt_type *rbtree, a_type *key) {\t\t\t\\\n     a_type *tnode = rbtree->rbt_root;\t\t\t\t\t\\\n     ret = &rbtree->rbt_nil;\t\t\t\t\t\t\\\n     while (tnode != &rbtree->rbt_nil) {\t\t\t\t\t\\\n-    int cmp = (a_cmp)(key, tnode);\t\t\t\t\t\\\n-    if (cmp < 0) {\t\t\t\t\t\t\t\\\n-        tnode = rbtn_left_get(a_type, a_field, tnode);\t\t\\\n-    } else if (cmp > 0) {\t\t\t\t\t\t\\\n-        ret = tnode;\t\t\t\t\t\t\\\n-        tnode = rbtn_right_get(a_type, a_field, tnode);\t\t\\\n-    } else {\t\t\t\t\t\t\t\\\n-        ret = tnode;\t\t\t\t\t\t\\\n-        break;\t\t\t\t\t\t\t\\\n-    }\t\t\t\t\t\t\t\t\\\n+\tint cmp = (a_cmp)(key, tnode);\t\t\t\t\t\\\n+\tif (cmp < 0) {\t\t\t\t\t\t\t\\\n+\t    tnode = rbtn_left_get(a_type, a_field, tnode);\t\t\\\n+\t} else if (cmp > 0) {\t\t\t\t\t\t\\\n+\t    ret = tnode;\t\t\t\t\t\t\\\n+\t    tnode = rbtn_right_get(a_type, a_field, tnode);\t\t\\\n+\t} else {\t\t\t\t\t\t\t\\\n+\t    ret = tnode;\t\t\t\t\t\t\\\n+\t    break;\t\t\t\t\t\t\t\\\n+\t}\t\t\t\t\t\t\t\t\\\n     }\t\t\t\t\t\t\t\t\t\\\n     if (ret == &rbtree->rbt_nil) {\t\t\t\t\t\\\n-    ret = (NULL);\t\t\t\t\t\t\t\\\n+\tret = (NULL);\t\t\t\t\t\t\t\\\n     }\t\t\t\t\t\t\t\t\t\\\n     return (ret);\t\t\t\t\t\t\t\\\n }\t\t\t\t\t\t\t\t\t\\\n a_attr void\t\t\t\t\t\t\t\t\\\n a_prefix##insert(a_rbt_type *rbtree, a_type *node) {\t\t\t\\\n     struct {\t\t\t\t\t\t\t\t\\\n-    a_type *node;\t\t\t\t\t\t\t\\\n-    int cmp;\t\t\t\t\t\t\t\\\n+\ta_type *node;\t\t\t\t\t\t\t\\\n+\tint cmp;\t\t\t\t\t\t\t\\\n     } path[sizeof(void *) << 4], *pathp;\t\t\t\t\\\n     rbt_node_new(a_type, a_field, rbtree, node);\t\t\t\\\n     /* Wind. */\t\t\t\t\t\t\t\t\\\n     path->node = rbtree->rbt_root;\t\t\t\t\t\\\n     for (pathp = path; pathp->node != &rbtree->rbt_nil; pathp++) {\t\\\n-    int cmp = pathp->cmp = a_cmp(node, pathp->node);\t\t\\\n-    assert(cmp != 0);\t\t\t\t\t\t\\\n-    if (cmp < 0) {\t\t\t\t\t\t\t\\\n-        pathp[1].node = rbtn_left_get(a_type, a_field,\t\t\\\n-          pathp->node);\t\t\t\t\t\t\\\n-    } else {\t\t\t\t\t\t\t\\\n-        pathp[1].node = rbtn_right_get(a_type, a_field,\t\t\\\n-          pathp->node);\t\t\t\t\t\t\\\n-    }\t\t\t\t\t\t\t\t\\\n+\tint cmp = pathp->cmp = a_cmp(node, pathp->node);\t\t\\\n+\tassert(cmp != 0);\t\t\t\t\t\t\\\n+\tif (cmp < 0) {\t\t\t\t\t\t\t\\\n+\t    pathp[1].node = rbtn_left_get(a_type, a_field,\t\t\\\n+\t      pathp->node);\t\t\t\t\t\t\\\n+\t} else {\t\t\t\t\t\t\t\\\n+\t    pathp[1].node = rbtn_right_get(a_type, a_field,\t\t\\\n+\t      pathp->node);\t\t\t\t\t\t\\\n+\t}\t\t\t\t\t\t\t\t\\\n     }\t\t\t\t\t\t\t\t\t\\\n     pathp->node = node;\t\t\t\t\t\t\t\\\n     /* Unwind. */\t\t\t\t\t\t\t\\\n     for (pathp--; (uintptr_t)pathp >= (uintptr_t)path; pathp--) {\t\\\n-    a_type *cnode = pathp->node;\t\t\t\t\t\\\n-    if (pathp->cmp < 0) {\t\t\t\t\t\t\\\n-        a_type *left = pathp[1].node;\t\t\t\t\\\n-        rbtn_left_set(a_type, a_field, cnode, left);\t\t\\\n-        if (rbtn_red_get(a_type, a_field, left)) {\t\t\t\\\n-        a_type *leftleft = rbtn_left_get(a_type, a_field, left);\\\n-        if (rbtn_red_get(a_type, a_field, leftleft)) {\t\t\\\n-            /* Fix up 4-node. */\t\t\t\t\\\n-            a_type *tnode;\t\t\t\t\t\\\n-            rbtn_black_set(a_type, a_field, leftleft);\t\t\\\n-            rbtn_rotate_right(a_type, a_field, cnode, tnode);\t\\\n-            cnode = tnode;\t\t\t\t\t\\\n-        }\t\t\t\t\t\t\t\\\n-        } else {\t\t\t\t\t\t\t\\\n-        return;\t\t\t\t\t\t\t\\\n-        }\t\t\t\t\t\t\t\t\\\n-    } else {\t\t\t\t\t\t\t\\\n-        a_type *right = pathp[1].node;\t\t\t\t\\\n-        rbtn_right_set(a_type, a_field, cnode, right);\t\t\\\n-        if (rbtn_red_get(a_type, a_field, right)) {\t\t\t\\\n-        a_type *left = rbtn_left_get(a_type, a_field, cnode);\t\\\n-        if (rbtn_red_get(a_type, a_field, left)) {\t\t\\\n-            /* Split 4-node. */\t\t\t\t\t\\\n-            rbtn_black_set(a_type, a_field, left);\t\t\\\n-            rbtn_black_set(a_type, a_field, right);\t\t\\\n-            rbtn_red_set(a_type, a_field, cnode);\t\t\\\n-        } else {\t\t\t\t\t\t\\\n-            /* Lean left. */\t\t\t\t\t\\\n-            a_type *tnode;\t\t\t\t\t\\\n-            bool tred = rbtn_red_get(a_type, a_field, cnode);\t\\\n-            rbtn_rotate_left(a_type, a_field, cnode, tnode);\t\\\n-            rbtn_color_set(a_type, a_field, tnode, tred);\t\\\n-            rbtn_red_set(a_type, a_field, cnode);\t\t\\\n-            cnode = tnode;\t\t\t\t\t\\\n-        }\t\t\t\t\t\t\t\\\n-        } else {\t\t\t\t\t\t\t\\\n-        return;\t\t\t\t\t\t\t\\\n-        }\t\t\t\t\t\t\t\t\\\n-    }\t\t\t\t\t\t\t\t\\\n-    pathp->node = cnode;\t\t\t\t\t\t\\\n+\ta_type *cnode = pathp->node;\t\t\t\t\t\\\n+\tif (pathp->cmp < 0) {\t\t\t\t\t\t\\\n+\t    a_type *left = pathp[1].node;\t\t\t\t\\\n+\t    rbtn_left_set(a_type, a_field, cnode, left);\t\t\\\n+\t    if (rbtn_red_get(a_type, a_field, left)) {\t\t\t\\\n+\t\ta_type *leftleft = rbtn_left_get(a_type, a_field, left);\\\n+\t\tif (rbtn_red_get(a_type, a_field, leftleft)) {\t\t\\\n+\t\t    /* Fix up 4-node. */\t\t\t\t\\\n+\t\t    a_type *tnode;\t\t\t\t\t\\\n+\t\t    rbtn_black_set(a_type, a_field, leftleft);\t\t\\\n+\t\t    rbtn_rotate_right(a_type, a_field, cnode, tnode);\t\\\n+\t\t    cnode = tnode;\t\t\t\t\t\\\n+\t\t}\t\t\t\t\t\t\t\\\n+\t    } else {\t\t\t\t\t\t\t\\\n+\t\treturn;\t\t\t\t\t\t\t\\\n+\t    }\t\t\t\t\t\t\t\t\\\n+\t} else {\t\t\t\t\t\t\t\\\n+\t    a_type *right = pathp[1].node;\t\t\t\t\\\n+\t    rbtn_right_set(a_type, a_field, cnode, right);\t\t\\\n+\t    if (rbtn_red_get(a_type, a_field, right)) {\t\t\t\\\n+\t\ta_type *left = rbtn_left_get(a_type, a_field, cnode);\t\\\n+\t\tif (rbtn_red_get(a_type, a_field, left)) {\t\t\\\n+\t\t    /* Split 4-node. */\t\t\t\t\t\\\n+\t\t    rbtn_black_set(a_type, a_field, left);\t\t\\\n+\t\t    rbtn_black_set(a_type, a_field, right);\t\t\\\n+\t\t    rbtn_red_set(a_type, a_field, cnode);\t\t\\\n+\t\t} else {\t\t\t\t\t\t\\\n+\t\t    /* Lean left. */\t\t\t\t\t\\\n+\t\t    a_type *tnode;\t\t\t\t\t\\\n+\t\t    bool tred = rbtn_red_get(a_type, a_field, cnode);\t\\\n+\t\t    rbtn_rotate_left(a_type, a_field, cnode, tnode);\t\\\n+\t\t    rbtn_color_set(a_type, a_field, tnode, tred);\t\\\n+\t\t    rbtn_red_set(a_type, a_field, cnode);\t\t\\\n+\t\t    cnode = tnode;\t\t\t\t\t\\\n+\t\t}\t\t\t\t\t\t\t\\\n+\t    } else {\t\t\t\t\t\t\t\\\n+\t\treturn;\t\t\t\t\t\t\t\\\n+\t    }\t\t\t\t\t\t\t\t\\\n+\t}\t\t\t\t\t\t\t\t\\\n+\tpathp->node = cnode;\t\t\t\t\t\t\\\n     }\t\t\t\t\t\t\t\t\t\\\n     /* Set root, and make it black. */\t\t\t\t\t\\\n     rbtree->rbt_root = path->node;\t\t\t\t\t\\\n@@ -520,336 +520,336 @@ a_prefix##insert(a_rbt_type *rbtree, a_type *node) {\t\t\t\\\n a_attr void\t\t\t\t\t\t\t\t\\\n a_prefix##remove(a_rbt_type *rbtree, a_type *node) {\t\t\t\\\n     struct {\t\t\t\t\t\t\t\t\\\n-    a_type *node;\t\t\t\t\t\t\t\\\n-    int cmp;\t\t\t\t\t\t\t\\\n+\ta_type *node;\t\t\t\t\t\t\t\\\n+\tint cmp;\t\t\t\t\t\t\t\\\n     } *pathp, *nodep, path[sizeof(void *) << 4];\t\t\t\\\n     /* Wind. */\t\t\t\t\t\t\t\t\\\n     nodep = NULL; /* Silence compiler warning. */\t\t\t\\\n     path->node = rbtree->rbt_root;\t\t\t\t\t\\\n     for (pathp = path; pathp->node != &rbtree->rbt_nil; pathp++) {\t\\\n-    int cmp = pathp->cmp = a_cmp(node, pathp->node);\t\t\\\n-    if (cmp < 0) {\t\t\t\t\t\t\t\\\n-        pathp[1].node = rbtn_left_get(a_type, a_field,\t\t\\\n-          pathp->node);\t\t\t\t\t\t\\\n-    } else {\t\t\t\t\t\t\t\\\n-        pathp[1].node = rbtn_right_get(a_type, a_field,\t\t\\\n-          pathp->node);\t\t\t\t\t\t\\\n-        if (cmp == 0) {\t\t\t\t\t\t\\\n-            /* Find node's successor, in preparation for swap. */\t\\\n-        pathp->cmp = 1;\t\t\t\t\t\t\\\n-        nodep = pathp;\t\t\t\t\t\t\\\n-        for (pathp++; pathp->node != &rbtree->rbt_nil;\t\t\\\n-          pathp++) {\t\t\t\t\t\t\\\n-            pathp->cmp = -1;\t\t\t\t\t\\\n-            pathp[1].node = rbtn_left_get(a_type, a_field,\t\\\n-              pathp->node);\t\t\t\t\t\\\n-        }\t\t\t\t\t\t\t\\\n-        break;\t\t\t\t\t\t\t\\\n-        }\t\t\t\t\t\t\t\t\\\n-    }\t\t\t\t\t\t\t\t\\\n+\tint cmp = pathp->cmp = a_cmp(node, pathp->node);\t\t\\\n+\tif (cmp < 0) {\t\t\t\t\t\t\t\\\n+\t    pathp[1].node = rbtn_left_get(a_type, a_field,\t\t\\\n+\t      pathp->node);\t\t\t\t\t\t\\\n+\t} else {\t\t\t\t\t\t\t\\\n+\t    pathp[1].node = rbtn_right_get(a_type, a_field,\t\t\\\n+\t      pathp->node);\t\t\t\t\t\t\\\n+\t    if (cmp == 0) {\t\t\t\t\t\t\\\n+\t        /* Find node's successor, in preparation for swap. */\t\\\n+\t\tpathp->cmp = 1;\t\t\t\t\t\t\\\n+\t\tnodep = pathp;\t\t\t\t\t\t\\\n+\t\tfor (pathp++; pathp->node != &rbtree->rbt_nil;\t\t\\\n+\t\t  pathp++) {\t\t\t\t\t\t\\\n+\t\t    pathp->cmp = -1;\t\t\t\t\t\\\n+\t\t    pathp[1].node = rbtn_left_get(a_type, a_field,\t\\\n+\t\t      pathp->node);\t\t\t\t\t\\\n+\t\t}\t\t\t\t\t\t\t\\\n+\t\tbreak;\t\t\t\t\t\t\t\\\n+\t    }\t\t\t\t\t\t\t\t\\\n+\t}\t\t\t\t\t\t\t\t\\\n     }\t\t\t\t\t\t\t\t\t\\\n     assert(nodep->node == node);\t\t\t\t\t\\\n     pathp--;\t\t\t\t\t\t\t\t\\\n     if (pathp->node != node) {\t\t\t\t\t\t\\\n-    /* Swap node with its successor. */\t\t\t\t\\\n-    bool tred = rbtn_red_get(a_type, a_field, pathp->node);\t\t\\\n-    rbtn_color_set(a_type, a_field, pathp->node,\t\t\t\\\n-      rbtn_red_get(a_type, a_field, node));\t\t\t\t\\\n-    rbtn_left_set(a_type, a_field, pathp->node,\t\t\t\\\n-      rbtn_left_get(a_type, a_field, node));\t\t\t\\\n-    /* If node's successor is its right child, the following code */\\\n-    /* will do the wrong thing for the right child pointer.       */\\\n-    /* However, it doesn't matter, because the pointer will be    */\\\n-    /* properly set when the successor is pruned.                 */\\\n-    rbtn_right_set(a_type, a_field, pathp->node,\t\t\t\\\n-      rbtn_right_get(a_type, a_field, node));\t\t\t\\\n-    rbtn_color_set(a_type, a_field, node, tred);\t\t\t\\\n-    /* The pruned leaf node's child pointers are never accessed   */\\\n-    /* again, so don't bother setting them to nil.                */\\\n-    nodep->node = pathp->node;\t\t\t\t\t\\\n-    pathp->node = node;\t\t\t\t\t\t\\\n-    if (nodep == path) {\t\t\t\t\t\t\\\n-        rbtree->rbt_root = nodep->node;\t\t\t\t\\\n-    } else {\t\t\t\t\t\t\t\\\n-        if (nodep[-1].cmp < 0) {\t\t\t\t\t\\\n-        rbtn_left_set(a_type, a_field, nodep[-1].node,\t\t\\\n-          nodep->node);\t\t\t\t\t\t\\\n-        } else {\t\t\t\t\t\t\t\\\n-        rbtn_right_set(a_type, a_field, nodep[-1].node,\t\t\\\n-          nodep->node);\t\t\t\t\t\t\\\n-        }\t\t\t\t\t\t\t\t\\\n-    }\t\t\t\t\t\t\t\t\\\n+\t/* Swap node with its successor. */\t\t\t\t\\\n+\tbool tred = rbtn_red_get(a_type, a_field, pathp->node);\t\t\\\n+\trbtn_color_set(a_type, a_field, pathp->node,\t\t\t\\\n+\t  rbtn_red_get(a_type, a_field, node));\t\t\t\t\\\n+\trbtn_left_set(a_type, a_field, pathp->node,\t\t\t\\\n+\t  rbtn_left_get(a_type, a_field, node));\t\t\t\\\n+\t/* If node's successor is its right child, the following code */\\\n+\t/* will do the wrong thing for the right child pointer.       */\\\n+\t/* However, it doesn't matter, because the pointer will be    */\\\n+\t/* properly set when the successor is pruned.                 */\\\n+\trbtn_right_set(a_type, a_field, pathp->node,\t\t\t\\\n+\t  rbtn_right_get(a_type, a_field, node));\t\t\t\\\n+\trbtn_color_set(a_type, a_field, node, tred);\t\t\t\\\n+\t/* The pruned leaf node's child pointers are never accessed   */\\\n+\t/* again, so don't bother setting them to nil.                */\\\n+\tnodep->node = pathp->node;\t\t\t\t\t\\\n+\tpathp->node = node;\t\t\t\t\t\t\\\n+\tif (nodep == path) {\t\t\t\t\t\t\\\n+\t    rbtree->rbt_root = nodep->node;\t\t\t\t\\\n+\t} else {\t\t\t\t\t\t\t\\\n+\t    if (nodep[-1].cmp < 0) {\t\t\t\t\t\\\n+\t\trbtn_left_set(a_type, a_field, nodep[-1].node,\t\t\\\n+\t\t  nodep->node);\t\t\t\t\t\t\\\n+\t    } else {\t\t\t\t\t\t\t\\\n+\t\trbtn_right_set(a_type, a_field, nodep[-1].node,\t\t\\\n+\t\t  nodep->node);\t\t\t\t\t\t\\\n+\t    }\t\t\t\t\t\t\t\t\\\n+\t}\t\t\t\t\t\t\t\t\\\n     } else {\t\t\t\t\t\t\t\t\\\n-    a_type *left = rbtn_left_get(a_type, a_field, node);\t\t\\\n-    if (left != &rbtree->rbt_nil) {\t\t\t\t\t\\\n-        /* node has no successor, but it has a left child.        */\\\n-        /* Splice node out, without losing the left child.        */\\\n-        assert(rbtn_red_get(a_type, a_field, node) == false);\t\\\n-        assert(rbtn_red_get(a_type, a_field, left));\t\t\\\n-        rbtn_black_set(a_type, a_field, left);\t\t\t\\\n-        if (pathp == path) {\t\t\t\t\t\\\n-        rbtree->rbt_root = left;\t\t\t\t\\\n-        } else {\t\t\t\t\t\t\t\\\n-        if (pathp[-1].cmp < 0) {\t\t\t\t\\\n-            rbtn_left_set(a_type, a_field, pathp[-1].node,\t\\\n-              left);\t\t\t\t\t\t\\\n-        } else {\t\t\t\t\t\t\\\n-            rbtn_right_set(a_type, a_field, pathp[-1].node,\t\\\n-              left);\t\t\t\t\t\t\\\n-        }\t\t\t\t\t\t\t\\\n-        }\t\t\t\t\t\t\t\t\\\n-        return;\t\t\t\t\t\t\t\\\n-    } else if (pathp == path) {\t\t\t\t\t\\\n-        /* The tree only contained one node. */\t\t\t\\\n-        rbtree->rbt_root = &rbtree->rbt_nil;\t\t\t\\\n-        return;\t\t\t\t\t\t\t\\\n-    }\t\t\t\t\t\t\t\t\\\n+\ta_type *left = rbtn_left_get(a_type, a_field, node);\t\t\\\n+\tif (left != &rbtree->rbt_nil) {\t\t\t\t\t\\\n+\t    /* node has no successor, but it has a left child.        */\\\n+\t    /* Splice node out, without losing the left child.        */\\\n+\t    assert(rbtn_red_get(a_type, a_field, node) == false);\t\\\n+\t    assert(rbtn_red_get(a_type, a_field, left));\t\t\\\n+\t    rbtn_black_set(a_type, a_field, left);\t\t\t\\\n+\t    if (pathp == path) {\t\t\t\t\t\\\n+\t\trbtree->rbt_root = left;\t\t\t\t\\\n+\t    } else {\t\t\t\t\t\t\t\\\n+\t\tif (pathp[-1].cmp < 0) {\t\t\t\t\\\n+\t\t    rbtn_left_set(a_type, a_field, pathp[-1].node,\t\\\n+\t\t      left);\t\t\t\t\t\t\\\n+\t\t} else {\t\t\t\t\t\t\\\n+\t\t    rbtn_right_set(a_type, a_field, pathp[-1].node,\t\\\n+\t\t      left);\t\t\t\t\t\t\\\n+\t\t}\t\t\t\t\t\t\t\\\n+\t    }\t\t\t\t\t\t\t\t\\\n+\t    return;\t\t\t\t\t\t\t\\\n+\t} else if (pathp == path) {\t\t\t\t\t\\\n+\t    /* The tree only contained one node. */\t\t\t\\\n+\t    rbtree->rbt_root = &rbtree->rbt_nil;\t\t\t\\\n+\t    return;\t\t\t\t\t\t\t\\\n+\t}\t\t\t\t\t\t\t\t\\\n     }\t\t\t\t\t\t\t\t\t\\\n     if (rbtn_red_get(a_type, a_field, pathp->node)) {\t\t\t\\\n-    /* Prune red node, which requires no fixup. */\t\t\t\\\n-    assert(pathp[-1].cmp < 0);\t\t\t\t\t\\\n-    rbtn_left_set(a_type, a_field, pathp[-1].node,\t\t\t\\\n-      &rbtree->rbt_nil);\t\t\t\t\t\t\\\n-    return;\t\t\t\t\t\t\t\t\\\n+\t/* Prune red node, which requires no fixup. */\t\t\t\\\n+\tassert(pathp[-1].cmp < 0);\t\t\t\t\t\\\n+\trbtn_left_set(a_type, a_field, pathp[-1].node,\t\t\t\\\n+\t  &rbtree->rbt_nil);\t\t\t\t\t\t\\\n+\treturn;\t\t\t\t\t\t\t\t\\\n     }\t\t\t\t\t\t\t\t\t\\\n     /* The node to be pruned is black, so unwind until balance is     */\\\n     /* restored.                                                      */\\\n     pathp->node = &rbtree->rbt_nil;\t\t\t\t\t\\\n     for (pathp--; (uintptr_t)pathp >= (uintptr_t)path; pathp--) {\t\\\n-    assert(pathp->cmp != 0);\t\t\t\t\t\\\n-    if (pathp->cmp < 0) {\t\t\t\t\t\t\\\n-        rbtn_left_set(a_type, a_field, pathp->node,\t\t\t\\\n-          pathp[1].node);\t\t\t\t\t\t\\\n-        assert(rbtn_red_get(a_type, a_field, pathp[1].node)\t\t\\\n-          == false);\t\t\t\t\t\t\\\n-        if (rbtn_red_get(a_type, a_field, pathp->node)) {\t\t\\\n-        a_type *right = rbtn_right_get(a_type, a_field,\t\t\\\n-          pathp->node);\t\t\t\t\t\t\\\n-        a_type *rightleft = rbtn_left_get(a_type, a_field,\t\\\n-          right);\t\t\t\t\t\t\\\n-        a_type *tnode;\t\t\t\t\t\t\\\n-        if (rbtn_red_get(a_type, a_field, rightleft)) {\t\t\\\n-            /* In the following diagrams, ||, //, and \\\\      */\\\n-            /* indicate the path to the removed node.         */\\\n-            /*                                                */\\\n-            /*      ||                                        */\\\n-            /*    pathp(r)                                    */\\\n-            /*  //        \\                                   */\\\n-            /* (b)        (b)                                 */\\\n-            /*           /                                    */\\\n-            /*          (r)                                   */\\\n-            /*                                                */\\\n-            rbtn_black_set(a_type, a_field, pathp->node);\t\\\n-            rbtn_rotate_right(a_type, a_field, right, tnode);\t\\\n-            rbtn_right_set(a_type, a_field, pathp->node, tnode);\\\n-            rbtn_rotate_left(a_type, a_field, pathp->node,\t\\\n-              tnode);\t\t\t\t\t\t\\\n-        } else {\t\t\t\t\t\t\\\n-            /*      ||                                        */\\\n-            /*    pathp(r)                                    */\\\n-            /*  //        \\                                   */\\\n-            /* (b)        (b)                                 */\\\n-            /*           /                                    */\\\n-            /*          (b)                                   */\\\n-            /*                                                */\\\n-            rbtn_rotate_left(a_type, a_field, pathp->node,\t\\\n-              tnode);\t\t\t\t\t\t\\\n-        }\t\t\t\t\t\t\t\\\n-        /* Balance restored, but rotation modified subtree    */\\\n-        /* root.                                              */\\\n-        assert((uintptr_t)pathp > (uintptr_t)path);\t\t\\\n-        if (pathp[-1].cmp < 0) {\t\t\t\t\\\n-            rbtn_left_set(a_type, a_field, pathp[-1].node,\t\\\n-              tnode);\t\t\t\t\t\t\\\n-        } else {\t\t\t\t\t\t\\\n-            rbtn_right_set(a_type, a_field, pathp[-1].node,\t\\\n-              tnode);\t\t\t\t\t\t\\\n-        }\t\t\t\t\t\t\t\\\n-        return;\t\t\t\t\t\t\t\\\n-        } else {\t\t\t\t\t\t\t\\\n-        a_type *right = rbtn_right_get(a_type, a_field,\t\t\\\n-          pathp->node);\t\t\t\t\t\t\\\n-        a_type *rightleft = rbtn_left_get(a_type, a_field,\t\\\n-          right);\t\t\t\t\t\t\\\n-        if (rbtn_red_get(a_type, a_field, rightleft)) {\t\t\\\n-            /*      ||                                        */\\\n-            /*    pathp(b)                                    */\\\n-            /*  //        \\                                   */\\\n-            /* (b)        (b)                                 */\\\n-            /*           /                                    */\\\n-            /*          (r)                                   */\\\n-            a_type *tnode;\t\t\t\t\t\\\n-            rbtn_black_set(a_type, a_field, rightleft);\t\t\\\n-            rbtn_rotate_right(a_type, a_field, right, tnode);\t\\\n-            rbtn_right_set(a_type, a_field, pathp->node, tnode);\\\n-            rbtn_rotate_left(a_type, a_field, pathp->node,\t\\\n-              tnode);\t\t\t\t\t\t\\\n-            /* Balance restored, but rotation modified        */\\\n-            /* subree root, which may actually be the tree    */\\\n-            /* root.                                          */\\\n-            if (pathp == path) {\t\t\t\t\\\n-            /* Set root. */\t\t\t\t\t\\\n-            rbtree->rbt_root = tnode;\t\t\t\\\n-            } else {\t\t\t\t\t\t\\\n-            if (pathp[-1].cmp < 0) {\t\t\t\\\n-                rbtn_left_set(a_type, a_field,\t\t\\\n-                  pathp[-1].node, tnode);\t\t\t\\\n-            } else {\t\t\t\t\t\\\n-                rbtn_right_set(a_type, a_field,\t\t\\\n-                  pathp[-1].node, tnode);\t\t\t\\\n-            }\t\t\t\t\t\t\\\n-            }\t\t\t\t\t\t\t\\\n-            return;\t\t\t\t\t\t\\\n-        } else {\t\t\t\t\t\t\\\n-            /*      ||                                        */\\\n-            /*    pathp(b)                                    */\\\n-            /*  //        \\                                   */\\\n-            /* (b)        (b)                                 */\\\n-            /*           /                                    */\\\n-            /*          (b)                                   */\\\n-            a_type *tnode;\t\t\t\t\t\\\n-            rbtn_red_set(a_type, a_field, pathp->node);\t\t\\\n-            rbtn_rotate_left(a_type, a_field, pathp->node,\t\\\n-              tnode);\t\t\t\t\t\t\\\n-            pathp->node = tnode;\t\t\t\t\\\n-        }\t\t\t\t\t\t\t\\\n-        }\t\t\t\t\t\t\t\t\\\n-    } else {\t\t\t\t\t\t\t\\\n-        a_type *left;\t\t\t\t\t\t\\\n-        rbtn_right_set(a_type, a_field, pathp->node,\t\t\\\n-          pathp[1].node);\t\t\t\t\t\t\\\n-        left = rbtn_left_get(a_type, a_field, pathp->node);\t\t\\\n-        if (rbtn_red_get(a_type, a_field, left)) {\t\t\t\\\n-        a_type *tnode;\t\t\t\t\t\t\\\n-        a_type *leftright = rbtn_right_get(a_type, a_field,\t\\\n-          left);\t\t\t\t\t\t\\\n-        a_type *leftrightleft = rbtn_left_get(a_type, a_field,\t\\\n-          leftright);\t\t\t\t\t\t\\\n-        if (rbtn_red_get(a_type, a_field, leftrightleft)) {\t\\\n-            /*      ||                                        */\\\n-            /*    pathp(b)                                    */\\\n-            /*   /        \\\\                                  */\\\n-            /* (r)        (b)                                 */\\\n-            /*   \\                                            */\\\n-            /*   (b)                                          */\\\n-            /*   /                                            */\\\n-            /* (r)                                            */\\\n-            a_type *unode;\t\t\t\t\t\\\n-            rbtn_black_set(a_type, a_field, leftrightleft);\t\\\n-            rbtn_rotate_right(a_type, a_field, pathp->node,\t\\\n-              unode);\t\t\t\t\t\t\\\n-            rbtn_rotate_right(a_type, a_field, pathp->node,\t\\\n-              tnode);\t\t\t\t\t\t\\\n-            rbtn_right_set(a_type, a_field, unode, tnode);\t\\\n-            rbtn_rotate_left(a_type, a_field, unode, tnode);\t\\\n-        } else {\t\t\t\t\t\t\\\n-            /*      ||                                        */\\\n-            /*    pathp(b)                                    */\\\n-            /*   /        \\\\                                  */\\\n-            /* (r)        (b)                                 */\\\n-            /*   \\                                            */\\\n-            /*   (b)                                          */\\\n-            /*   /                                            */\\\n-            /* (b)                                            */\\\n-            assert(leftright != &rbtree->rbt_nil);\t\t\\\n-            rbtn_red_set(a_type, a_field, leftright);\t\t\\\n-            rbtn_rotate_right(a_type, a_field, pathp->node,\t\\\n-              tnode);\t\t\t\t\t\t\\\n-            rbtn_black_set(a_type, a_field, tnode);\t\t\\\n-        }\t\t\t\t\t\t\t\\\n-        /* Balance restored, but rotation modified subtree    */\\\n-        /* root, which may actually be the tree root.         */\\\n-        if (pathp == path) {\t\t\t\t\t\\\n-            /* Set root. */\t\t\t\t\t\\\n-            rbtree->rbt_root = tnode;\t\t\t\t\\\n-        } else {\t\t\t\t\t\t\\\n-            if (pathp[-1].cmp < 0) {\t\t\t\t\\\n-            rbtn_left_set(a_type, a_field, pathp[-1].node,\t\\\n-              tnode);\t\t\t\t\t\\\n-            } else {\t\t\t\t\t\t\\\n-            rbtn_right_set(a_type, a_field, pathp[-1].node,\t\\\n-              tnode);\t\t\t\t\t\\\n-            }\t\t\t\t\t\t\t\\\n-        }\t\t\t\t\t\t\t\\\n-        return;\t\t\t\t\t\t\t\\\n-        } else if (rbtn_red_get(a_type, a_field, pathp->node)) {\t\\\n-        a_type *leftleft = rbtn_left_get(a_type, a_field, left);\\\n-        if (rbtn_red_get(a_type, a_field, leftleft)) {\t\t\\\n-            /*        ||                                      */\\\n-            /*      pathp(r)                                  */\\\n-            /*     /        \\\\                                */\\\n-            /*   (b)        (b)                               */\\\n-            /*   /                                            */\\\n-            /* (r)                                            */\\\n-            a_type *tnode;\t\t\t\t\t\\\n-            rbtn_black_set(a_type, a_field, pathp->node);\t\\\n-            rbtn_red_set(a_type, a_field, left);\t\t\\\n-            rbtn_black_set(a_type, a_field, leftleft);\t\t\\\n-            rbtn_rotate_right(a_type, a_field, pathp->node,\t\\\n-              tnode);\t\t\t\t\t\t\\\n-            /* Balance restored, but rotation modified        */\\\n-            /* subtree root.                                  */\\\n-            assert((uintptr_t)pathp > (uintptr_t)path);\t\t\\\n-            if (pathp[-1].cmp < 0) {\t\t\t\t\\\n-            rbtn_left_set(a_type, a_field, pathp[-1].node,\t\\\n-              tnode);\t\t\t\t\t\\\n-            } else {\t\t\t\t\t\t\\\n-            rbtn_right_set(a_type, a_field, pathp[-1].node,\t\\\n-              tnode);\t\t\t\t\t\\\n-            }\t\t\t\t\t\t\t\\\n-            return;\t\t\t\t\t\t\\\n-        } else {\t\t\t\t\t\t\\\n-            /*        ||                                      */\\\n-            /*      pathp(r)                                  */\\\n-            /*     /        \\\\                                */\\\n-            /*   (b)        (b)                               */\\\n-            /*   /                                            */\\\n-            /* (b)                                            */\\\n-            rbtn_red_set(a_type, a_field, left);\t\t\\\n-            rbtn_black_set(a_type, a_field, pathp->node);\t\\\n-            /* Balance restored. */\t\t\t\t\\\n-            return;\t\t\t\t\t\t\\\n-        }\t\t\t\t\t\t\t\\\n-        } else {\t\t\t\t\t\t\t\\\n-        a_type *leftleft = rbtn_left_get(a_type, a_field, left);\\\n-        if (rbtn_red_get(a_type, a_field, leftleft)) {\t\t\\\n-            /*               ||                               */\\\n-            /*             pathp(b)                           */\\\n-            /*            /        \\\\                         */\\\n-            /*          (b)        (b)                        */\\\n-            /*          /                                     */\\\n-            /*        (r)                                     */\\\n-            a_type *tnode;\t\t\t\t\t\\\n-            rbtn_black_set(a_type, a_field, leftleft);\t\t\\\n-            rbtn_rotate_right(a_type, a_field, pathp->node,\t\\\n-              tnode);\t\t\t\t\t\t\\\n-            /* Balance restored, but rotation modified        */\\\n-            /* subtree root, which may actually be the tree   */\\\n-            /* root.                                          */\\\n-            if (pathp == path) {\t\t\t\t\\\n-            /* Set root. */\t\t\t\t\t\\\n-            rbtree->rbt_root = tnode;\t\t\t\\\n-            } else {\t\t\t\t\t\t\\\n-            if (pathp[-1].cmp < 0) {\t\t\t\\\n-                rbtn_left_set(a_type, a_field,\t\t\\\n-                  pathp[-1].node, tnode);\t\t\t\\\n-            } else {\t\t\t\t\t\\\n-                rbtn_right_set(a_type, a_field,\t\t\\\n-                  pathp[-1].node, tnode);\t\t\t\\\n-            }\t\t\t\t\t\t\\\n-            }\t\t\t\t\t\t\t\\\n-            return;\t\t\t\t\t\t\\\n-        } else {\t\t\t\t\t\t\\\n-            /*               ||                               */\\\n-            /*             pathp(b)                           */\\\n-            /*            /        \\\\                         */\\\n-            /*          (b)        (b)                        */\\\n-            /*          /                                     */\\\n-            /*        (b)                                     */\\\n-            rbtn_red_set(a_type, a_field, left);\t\t\\\n-        }\t\t\t\t\t\t\t\\\n-        }\t\t\t\t\t\t\t\t\\\n-    }\t\t\t\t\t\t\t\t\\\n+\tassert(pathp->cmp != 0);\t\t\t\t\t\\\n+\tif (pathp->cmp < 0) {\t\t\t\t\t\t\\\n+\t    rbtn_left_set(a_type, a_field, pathp->node,\t\t\t\\\n+\t      pathp[1].node);\t\t\t\t\t\t\\\n+\t    assert(rbtn_red_get(a_type, a_field, pathp[1].node)\t\t\\\n+\t      == false);\t\t\t\t\t\t\\\n+\t    if (rbtn_red_get(a_type, a_field, pathp->node)) {\t\t\\\n+\t\ta_type *right = rbtn_right_get(a_type, a_field,\t\t\\\n+\t\t  pathp->node);\t\t\t\t\t\t\\\n+\t\ta_type *rightleft = rbtn_left_get(a_type, a_field,\t\\\n+\t\t  right);\t\t\t\t\t\t\\\n+\t\ta_type *tnode;\t\t\t\t\t\t\\\n+\t\tif (rbtn_red_get(a_type, a_field, rightleft)) {\t\t\\\n+\t\t    /* In the following diagrams, ||, //, and \\\\      */\\\n+\t\t    /* indicate the path to the removed node.         */\\\n+\t\t    /*                                                */\\\n+\t\t    /*      ||                                        */\\\n+\t\t    /*    pathp(r)                                    */\\\n+\t\t    /*  //        \\                                   */\\\n+\t\t    /* (b)        (b)                                 */\\\n+\t\t    /*           /                                    */\\\n+\t\t    /*          (r)                                   */\\\n+\t\t    /*                                                */\\\n+\t\t    rbtn_black_set(a_type, a_field, pathp->node);\t\\\n+\t\t    rbtn_rotate_right(a_type, a_field, right, tnode);\t\\\n+\t\t    rbtn_right_set(a_type, a_field, pathp->node, tnode);\\\n+\t\t    rbtn_rotate_left(a_type, a_field, pathp->node,\t\\\n+\t\t      tnode);\t\t\t\t\t\t\\\n+\t\t} else {\t\t\t\t\t\t\\\n+\t\t    /*      ||                                        */\\\n+\t\t    /*    pathp(r)                                    */\\\n+\t\t    /*  //        \\                                   */\\\n+\t\t    /* (b)        (b)                                 */\\\n+\t\t    /*           /                                    */\\\n+\t\t    /*          (b)                                   */\\\n+\t\t    /*                                                */\\\n+\t\t    rbtn_rotate_left(a_type, a_field, pathp->node,\t\\\n+\t\t      tnode);\t\t\t\t\t\t\\\n+\t\t}\t\t\t\t\t\t\t\\\n+\t\t/* Balance restored, but rotation modified subtree    */\\\n+\t\t/* root.                                              */\\\n+\t\tassert((uintptr_t)pathp > (uintptr_t)path);\t\t\\\n+\t\tif (pathp[-1].cmp < 0) {\t\t\t\t\\\n+\t\t    rbtn_left_set(a_type, a_field, pathp[-1].node,\t\\\n+\t\t      tnode);\t\t\t\t\t\t\\\n+\t\t} else {\t\t\t\t\t\t\\\n+\t\t    rbtn_right_set(a_type, a_field, pathp[-1].node,\t\\\n+\t\t      tnode);\t\t\t\t\t\t\\\n+\t\t}\t\t\t\t\t\t\t\\\n+\t\treturn;\t\t\t\t\t\t\t\\\n+\t    } else {\t\t\t\t\t\t\t\\\n+\t\ta_type *right = rbtn_right_get(a_type, a_field,\t\t\\\n+\t\t  pathp->node);\t\t\t\t\t\t\\\n+\t\ta_type *rightleft = rbtn_left_get(a_type, a_field,\t\\\n+\t\t  right);\t\t\t\t\t\t\\\n+\t\tif (rbtn_red_get(a_type, a_field, rightleft)) {\t\t\\\n+\t\t    /*      ||                                        */\\\n+\t\t    /*    pathp(b)                                    */\\\n+\t\t    /*  //        \\                                   */\\\n+\t\t    /* (b)        (b)                                 */\\\n+\t\t    /*           /                                    */\\\n+\t\t    /*          (r)                                   */\\\n+\t\t    a_type *tnode;\t\t\t\t\t\\\n+\t\t    rbtn_black_set(a_type, a_field, rightleft);\t\t\\\n+\t\t    rbtn_rotate_right(a_type, a_field, right, tnode);\t\\\n+\t\t    rbtn_right_set(a_type, a_field, pathp->node, tnode);\\\n+\t\t    rbtn_rotate_left(a_type, a_field, pathp->node,\t\\\n+\t\t      tnode);\t\t\t\t\t\t\\\n+\t\t    /* Balance restored, but rotation modified        */\\\n+\t\t    /* subree root, which may actually be the tree    */\\\n+\t\t    /* root.                                          */\\\n+\t\t    if (pathp == path) {\t\t\t\t\\\n+\t\t\t/* Set root. */\t\t\t\t\t\\\n+\t\t\trbtree->rbt_root = tnode;\t\t\t\\\n+\t\t    } else {\t\t\t\t\t\t\\\n+\t\t\tif (pathp[-1].cmp < 0) {\t\t\t\\\n+\t\t\t    rbtn_left_set(a_type, a_field,\t\t\\\n+\t\t\t      pathp[-1].node, tnode);\t\t\t\\\n+\t\t\t} else {\t\t\t\t\t\\\n+\t\t\t    rbtn_right_set(a_type, a_field,\t\t\\\n+\t\t\t      pathp[-1].node, tnode);\t\t\t\\\n+\t\t\t}\t\t\t\t\t\t\\\n+\t\t    }\t\t\t\t\t\t\t\\\n+\t\t    return;\t\t\t\t\t\t\\\n+\t\t} else {\t\t\t\t\t\t\\\n+\t\t    /*      ||                                        */\\\n+\t\t    /*    pathp(b)                                    */\\\n+\t\t    /*  //        \\                                   */\\\n+\t\t    /* (b)        (b)                                 */\\\n+\t\t    /*           /                                    */\\\n+\t\t    /*          (b)                                   */\\\n+\t\t    a_type *tnode;\t\t\t\t\t\\\n+\t\t    rbtn_red_set(a_type, a_field, pathp->node);\t\t\\\n+\t\t    rbtn_rotate_left(a_type, a_field, pathp->node,\t\\\n+\t\t      tnode);\t\t\t\t\t\t\\\n+\t\t    pathp->node = tnode;\t\t\t\t\\\n+\t\t}\t\t\t\t\t\t\t\\\n+\t    }\t\t\t\t\t\t\t\t\\\n+\t} else {\t\t\t\t\t\t\t\\\n+\t    a_type *left;\t\t\t\t\t\t\\\n+\t    rbtn_right_set(a_type, a_field, pathp->node,\t\t\\\n+\t      pathp[1].node);\t\t\t\t\t\t\\\n+\t    left = rbtn_left_get(a_type, a_field, pathp->node);\t\t\\\n+\t    if (rbtn_red_get(a_type, a_field, left)) {\t\t\t\\\n+\t\ta_type *tnode;\t\t\t\t\t\t\\\n+\t\ta_type *leftright = rbtn_right_get(a_type, a_field,\t\\\n+\t\t  left);\t\t\t\t\t\t\\\n+\t\ta_type *leftrightleft = rbtn_left_get(a_type, a_field,\t\\\n+\t\t  leftright);\t\t\t\t\t\t\\\n+\t\tif (rbtn_red_get(a_type, a_field, leftrightleft)) {\t\\\n+\t\t    /*      ||                                        */\\\n+\t\t    /*    pathp(b)                                    */\\\n+\t\t    /*   /        \\\\                                  */\\\n+\t\t    /* (r)        (b)                                 */\\\n+\t\t    /*   \\                                            */\\\n+\t\t    /*   (b)                                          */\\\n+\t\t    /*   /                                            */\\\n+\t\t    /* (r)                                            */\\\n+\t\t    a_type *unode;\t\t\t\t\t\\\n+\t\t    rbtn_black_set(a_type, a_field, leftrightleft);\t\\\n+\t\t    rbtn_rotate_right(a_type, a_field, pathp->node,\t\\\n+\t\t      unode);\t\t\t\t\t\t\\\n+\t\t    rbtn_rotate_right(a_type, a_field, pathp->node,\t\\\n+\t\t      tnode);\t\t\t\t\t\t\\\n+\t\t    rbtn_right_set(a_type, a_field, unode, tnode);\t\\\n+\t\t    rbtn_rotate_left(a_type, a_field, unode, tnode);\t\\\n+\t\t} else {\t\t\t\t\t\t\\\n+\t\t    /*      ||                                        */\\\n+\t\t    /*    pathp(b)                                    */\\\n+\t\t    /*   /        \\\\                                  */\\\n+\t\t    /* (r)        (b)                                 */\\\n+\t\t    /*   \\                                            */\\\n+\t\t    /*   (b)                                          */\\\n+\t\t    /*   /                                            */\\\n+\t\t    /* (b)                                            */\\\n+\t\t    assert(leftright != &rbtree->rbt_nil);\t\t\\\n+\t\t    rbtn_red_set(a_type, a_field, leftright);\t\t\\\n+\t\t    rbtn_rotate_right(a_type, a_field, pathp->node,\t\\\n+\t\t      tnode);\t\t\t\t\t\t\\\n+\t\t    rbtn_black_set(a_type, a_field, tnode);\t\t\\\n+\t\t}\t\t\t\t\t\t\t\\\n+\t\t/* Balance restored, but rotation modified subtree    */\\\n+\t\t/* root, which may actually be the tree root.         */\\\n+\t\tif (pathp == path) {\t\t\t\t\t\\\n+\t\t    /* Set root. */\t\t\t\t\t\\\n+\t\t    rbtree->rbt_root = tnode;\t\t\t\t\\\n+\t\t} else {\t\t\t\t\t\t\\\n+\t\t    if (pathp[-1].cmp < 0) {\t\t\t\t\\\n+\t\t\trbtn_left_set(a_type, a_field, pathp[-1].node,\t\\\n+\t\t\t  tnode);\t\t\t\t\t\\\n+\t\t    } else {\t\t\t\t\t\t\\\n+\t\t\trbtn_right_set(a_type, a_field, pathp[-1].node,\t\\\n+\t\t\t  tnode);\t\t\t\t\t\\\n+\t\t    }\t\t\t\t\t\t\t\\\n+\t\t}\t\t\t\t\t\t\t\\\n+\t\treturn;\t\t\t\t\t\t\t\\\n+\t    } else if (rbtn_red_get(a_type, a_field, pathp->node)) {\t\\\n+\t\ta_type *leftleft = rbtn_left_get(a_type, a_field, left);\\\n+\t\tif (rbtn_red_get(a_type, a_field, leftleft)) {\t\t\\\n+\t\t    /*        ||                                      */\\\n+\t\t    /*      pathp(r)                                  */\\\n+\t\t    /*     /        \\\\                                */\\\n+\t\t    /*   (b)        (b)                               */\\\n+\t\t    /*   /                                            */\\\n+\t\t    /* (r)                                            */\\\n+\t\t    a_type *tnode;\t\t\t\t\t\\\n+\t\t    rbtn_black_set(a_type, a_field, pathp->node);\t\\\n+\t\t    rbtn_red_set(a_type, a_field, left);\t\t\\\n+\t\t    rbtn_black_set(a_type, a_field, leftleft);\t\t\\\n+\t\t    rbtn_rotate_right(a_type, a_field, pathp->node,\t\\\n+\t\t      tnode);\t\t\t\t\t\t\\\n+\t\t    /* Balance restored, but rotation modified        */\\\n+\t\t    /* subtree root.                                  */\\\n+\t\t    assert((uintptr_t)pathp > (uintptr_t)path);\t\t\\\n+\t\t    if (pathp[-1].cmp < 0) {\t\t\t\t\\\n+\t\t\trbtn_left_set(a_type, a_field, pathp[-1].node,\t\\\n+\t\t\t  tnode);\t\t\t\t\t\\\n+\t\t    } else {\t\t\t\t\t\t\\\n+\t\t\trbtn_right_set(a_type, a_field, pathp[-1].node,\t\\\n+\t\t\t  tnode);\t\t\t\t\t\\\n+\t\t    }\t\t\t\t\t\t\t\\\n+\t\t    return;\t\t\t\t\t\t\\\n+\t\t} else {\t\t\t\t\t\t\\\n+\t\t    /*        ||                                      */\\\n+\t\t    /*      pathp(r)                                  */\\\n+\t\t    /*     /        \\\\                                */\\\n+\t\t    /*   (b)        (b)                               */\\\n+\t\t    /*   /                                            */\\\n+\t\t    /* (b)                                            */\\\n+\t\t    rbtn_red_set(a_type, a_field, left);\t\t\\\n+\t\t    rbtn_black_set(a_type, a_field, pathp->node);\t\\\n+\t\t    /* Balance restored. */\t\t\t\t\\\n+\t\t    return;\t\t\t\t\t\t\\\n+\t\t}\t\t\t\t\t\t\t\\\n+\t    } else {\t\t\t\t\t\t\t\\\n+\t\ta_type *leftleft = rbtn_left_get(a_type, a_field, left);\\\n+\t\tif (rbtn_red_get(a_type, a_field, leftleft)) {\t\t\\\n+\t\t    /*               ||                               */\\\n+\t\t    /*             pathp(b)                           */\\\n+\t\t    /*            /        \\\\                         */\\\n+\t\t    /*          (b)        (b)                        */\\\n+\t\t    /*          /                                     */\\\n+\t\t    /*        (r)                                     */\\\n+\t\t    a_type *tnode;\t\t\t\t\t\\\n+\t\t    rbtn_black_set(a_type, a_field, leftleft);\t\t\\\n+\t\t    rbtn_rotate_right(a_type, a_field, pathp->node,\t\\\n+\t\t      tnode);\t\t\t\t\t\t\\\n+\t\t    /* Balance restored, but rotation modified        */\\\n+\t\t    /* subtree root, which may actually be the tree   */\\\n+\t\t    /* root.                                          */\\\n+\t\t    if (pathp == path) {\t\t\t\t\\\n+\t\t\t/* Set root. */\t\t\t\t\t\\\n+\t\t\trbtree->rbt_root = tnode;\t\t\t\\\n+\t\t    } else {\t\t\t\t\t\t\\\n+\t\t\tif (pathp[-1].cmp < 0) {\t\t\t\\\n+\t\t\t    rbtn_left_set(a_type, a_field,\t\t\\\n+\t\t\t      pathp[-1].node, tnode);\t\t\t\\\n+\t\t\t} else {\t\t\t\t\t\\\n+\t\t\t    rbtn_right_set(a_type, a_field,\t\t\\\n+\t\t\t      pathp[-1].node, tnode);\t\t\t\\\n+\t\t\t}\t\t\t\t\t\t\\\n+\t\t    }\t\t\t\t\t\t\t\\\n+\t\t    return;\t\t\t\t\t\t\\\n+\t\t} else {\t\t\t\t\t\t\\\n+\t\t    /*               ||                               */\\\n+\t\t    /*             pathp(b)                           */\\\n+\t\t    /*            /        \\\\                         */\\\n+\t\t    /*          (b)        (b)                        */\\\n+\t\t    /*          /                                     */\\\n+\t\t    /*        (b)                                     */\\\n+\t\t    rbtn_red_set(a_type, a_field, left);\t\t\\\n+\t\t}\t\t\t\t\t\t\t\\\n+\t    }\t\t\t\t\t\t\t\t\\\n+\t}\t\t\t\t\t\t\t\t\\\n     }\t\t\t\t\t\t\t\t\t\\\n     /* Set root. */\t\t\t\t\t\t\t\\\n     rbtree->rbt_root = path->node;\t\t\t\t\t\\\n@@ -859,72 +859,72 @@ a_attr a_type *\t\t\t\t\t\t\t\t\\\n a_prefix##iter_recurse(a_rbt_type *rbtree, a_type *node,\t\t\\\n   a_type *(*cb)(a_rbt_type *, a_type *, void *), void *arg) {\t\t\\\n     if (node == &rbtree->rbt_nil) {\t\t\t\t\t\\\n-    return (&rbtree->rbt_nil);\t\t\t\t\t\\\n+\treturn (&rbtree->rbt_nil);\t\t\t\t\t\\\n     } else {\t\t\t\t\t\t\t\t\\\n-    a_type *ret;\t\t\t\t\t\t\t\\\n-    if ((ret = a_prefix##iter_recurse(rbtree, rbtn_left_get(a_type,\t\\\n-      a_field, node), cb, arg)) != &rbtree->rbt_nil\t\t\t\\\n-      || (ret = cb(rbtree, node, arg)) != NULL) {\t\t\t\\\n-        return (ret);\t\t\t\t\t\t\\\n-    }\t\t\t\t\t\t\t\t\\\n-    return (a_prefix##iter_recurse(rbtree, rbtn_right_get(a_type,\t\\\n-      a_field, node), cb, arg));\t\t\t\t\t\\\n+\ta_type *ret;\t\t\t\t\t\t\t\\\n+\tif ((ret = a_prefix##iter_recurse(rbtree, rbtn_left_get(a_type,\t\\\n+\t  a_field, node), cb, arg)) != &rbtree->rbt_nil\t\t\t\\\n+\t  || (ret = cb(rbtree, node, arg)) != NULL) {\t\t\t\\\n+\t    return (ret);\t\t\t\t\t\t\\\n+\t}\t\t\t\t\t\t\t\t\\\n+\treturn (a_prefix##iter_recurse(rbtree, rbtn_right_get(a_type,\t\\\n+\t  a_field, node), cb, arg));\t\t\t\t\t\\\n     }\t\t\t\t\t\t\t\t\t\\\n }\t\t\t\t\t\t\t\t\t\\\n a_attr a_type *\t\t\t\t\t\t\t\t\\\n a_prefix##iter_start(a_rbt_type *rbtree, a_type *start, a_type *node,\t\\\n   a_type *(*cb)(a_rbt_type *, a_type *, void *), void *arg) {\t\t\\\n     int cmp = a_cmp(start, node);\t\t\t\t\t\\\n     if (cmp < 0) {\t\t\t\t\t\t\t\\\n-    a_type *ret;\t\t\t\t\t\t\t\\\n-    if ((ret = a_prefix##iter_start(rbtree, start,\t\t\t\\\n-      rbtn_left_get(a_type, a_field, node), cb, arg)) !=\t\t\\\n-      &rbtree->rbt_nil || (ret = cb(rbtree, node, arg)) != NULL) {\t\\\n-        return (ret);\t\t\t\t\t\t\\\n-    }\t\t\t\t\t\t\t\t\\\n-    return (a_prefix##iter_recurse(rbtree, rbtn_right_get(a_type,\t\\\n-      a_field, node), cb, arg));\t\t\t\t\t\\\n+\ta_type *ret;\t\t\t\t\t\t\t\\\n+\tif ((ret = a_prefix##iter_start(rbtree, start,\t\t\t\\\n+\t  rbtn_left_get(a_type, a_field, node), cb, arg)) !=\t\t\\\n+\t  &rbtree->rbt_nil || (ret = cb(rbtree, node, arg)) != NULL) {\t\\\n+\t    return (ret);\t\t\t\t\t\t\\\n+\t}\t\t\t\t\t\t\t\t\\\n+\treturn (a_prefix##iter_recurse(rbtree, rbtn_right_get(a_type,\t\\\n+\t  a_field, node), cb, arg));\t\t\t\t\t\\\n     } else if (cmp > 0) {\t\t\t\t\t\t\\\n-    return (a_prefix##iter_start(rbtree, start,\t\t\t\\\n-      rbtn_right_get(a_type, a_field, node), cb, arg));\t\t\\\n+\treturn (a_prefix##iter_start(rbtree, start,\t\t\t\\\n+\t  rbtn_right_get(a_type, a_field, node), cb, arg));\t\t\\\n     } else {\t\t\t\t\t\t\t\t\\\n-    a_type *ret;\t\t\t\t\t\t\t\\\n-    if ((ret = cb(rbtree, node, arg)) != NULL) {\t\t\t\\\n-        return (ret);\t\t\t\t\t\t\\\n-    }\t\t\t\t\t\t\t\t\\\n-    return (a_prefix##iter_recurse(rbtree, rbtn_right_get(a_type,\t\\\n-      a_field, node), cb, arg));\t\t\t\t\t\\\n+\ta_type *ret;\t\t\t\t\t\t\t\\\n+\tif ((ret = cb(rbtree, node, arg)) != NULL) {\t\t\t\\\n+\t    return (ret);\t\t\t\t\t\t\\\n+\t}\t\t\t\t\t\t\t\t\\\n+\treturn (a_prefix##iter_recurse(rbtree, rbtn_right_get(a_type,\t\\\n+\t  a_field, node), cb, arg));\t\t\t\t\t\\\n     }\t\t\t\t\t\t\t\t\t\\\n }\t\t\t\t\t\t\t\t\t\\\n a_attr a_type *\t\t\t\t\t\t\t\t\\\n a_prefix##iter(a_rbt_type *rbtree, a_type *start, a_type *(*cb)(\t\\\n   a_rbt_type *, a_type *, void *), void *arg) {\t\t\t\t\\\n     a_type *ret;\t\t\t\t\t\t\t\\\n     if (start != NULL) {\t\t\t\t\t\t\\\n-    ret = a_prefix##iter_start(rbtree, start, rbtree->rbt_root,\t\\\n-      cb, arg);\t\t\t\t\t\t\t\\\n+\tret = a_prefix##iter_start(rbtree, start, rbtree->rbt_root,\t\\\n+\t  cb, arg);\t\t\t\t\t\t\t\\\n     } else {\t\t\t\t\t\t\t\t\\\n-    ret = a_prefix##iter_recurse(rbtree, rbtree->rbt_root, cb, arg);\\\n+\tret = a_prefix##iter_recurse(rbtree, rbtree->rbt_root, cb, arg);\\\n     }\t\t\t\t\t\t\t\t\t\\\n     if (ret == &rbtree->rbt_nil) {\t\t\t\t\t\\\n-    ret = NULL;\t\t\t\t\t\t\t\\\n+\tret = NULL;\t\t\t\t\t\t\t\\\n     }\t\t\t\t\t\t\t\t\t\\\n     return (ret);\t\t\t\t\t\t\t\\\n }\t\t\t\t\t\t\t\t\t\\\n a_attr a_type *\t\t\t\t\t\t\t\t\\\n a_prefix##reverse_iter_recurse(a_rbt_type *rbtree, a_type *node,\t\\\n   a_type *(*cb)(a_rbt_type *, a_type *, void *), void *arg) {\t\t\\\n     if (node == &rbtree->rbt_nil) {\t\t\t\t\t\\\n-    return (&rbtree->rbt_nil);\t\t\t\t\t\\\n+\treturn (&rbtree->rbt_nil);\t\t\t\t\t\\\n     } else {\t\t\t\t\t\t\t\t\\\n-    a_type *ret;\t\t\t\t\t\t\t\\\n-    if ((ret = a_prefix##reverse_iter_recurse(rbtree,\t\t\\\n-      rbtn_right_get(a_type, a_field, node), cb, arg)) !=\t\t\\\n-      &rbtree->rbt_nil || (ret = cb(rbtree, node, arg)) != NULL) {\t\\\n-        return (ret);\t\t\t\t\t\t\\\n-    }\t\t\t\t\t\t\t\t\\\n-    return (a_prefix##reverse_iter_recurse(rbtree,\t\t\t\\\n-      rbtn_left_get(a_type, a_field, node), cb, arg));\t\t\\\n+\ta_type *ret;\t\t\t\t\t\t\t\\\n+\tif ((ret = a_prefix##reverse_iter_recurse(rbtree,\t\t\\\n+\t  rbtn_right_get(a_type, a_field, node), cb, arg)) !=\t\t\\\n+\t  &rbtree->rbt_nil || (ret = cb(rbtree, node, arg)) != NULL) {\t\\\n+\t    return (ret);\t\t\t\t\t\t\\\n+\t}\t\t\t\t\t\t\t\t\\\n+\treturn (a_prefix##reverse_iter_recurse(rbtree,\t\t\t\\\n+\t  rbtn_left_get(a_type, a_field, node), cb, arg));\t\t\\\n     }\t\t\t\t\t\t\t\t\t\\\n }\t\t\t\t\t\t\t\t\t\\\n a_attr a_type *\t\t\t\t\t\t\t\t\\\n@@ -933,39 +933,39 @@ a_prefix##reverse_iter_start(a_rbt_type *rbtree, a_type *start,\t\t\\\n   void *arg) {\t\t\t\t\t\t\t\t\\\n     int cmp = a_cmp(start, node);\t\t\t\t\t\\\n     if (cmp > 0) {\t\t\t\t\t\t\t\\\n-    a_type *ret;\t\t\t\t\t\t\t\\\n-    if ((ret = a_prefix##reverse_iter_start(rbtree, start,\t\t\\\n-      rbtn_right_get(a_type, a_field, node), cb, arg)) !=\t\t\\\n-      &rbtree->rbt_nil || (ret = cb(rbtree, node, arg)) != NULL) {\t\\\n-        return (ret);\t\t\t\t\t\t\\\n-    }\t\t\t\t\t\t\t\t\\\n-    return (a_prefix##reverse_iter_recurse(rbtree,\t\t\t\\\n-      rbtn_left_get(a_type, a_field, node), cb, arg));\t\t\\\n+\ta_type *ret;\t\t\t\t\t\t\t\\\n+\tif ((ret = a_prefix##reverse_iter_start(rbtree, start,\t\t\\\n+\t  rbtn_right_get(a_type, a_field, node), cb, arg)) !=\t\t\\\n+\t  &rbtree->rbt_nil || (ret = cb(rbtree, node, arg)) != NULL) {\t\\\n+\t    return (ret);\t\t\t\t\t\t\\\n+\t}\t\t\t\t\t\t\t\t\\\n+\treturn (a_prefix##reverse_iter_recurse(rbtree,\t\t\t\\\n+\t  rbtn_left_get(a_type, a_field, node), cb, arg));\t\t\\\n     } else if (cmp < 0) {\t\t\t\t\t\t\\\n-    return (a_prefix##reverse_iter_start(rbtree, start,\t\t\\\n-      rbtn_left_get(a_type, a_field, node), cb, arg));\t\t\\\n+\treturn (a_prefix##reverse_iter_start(rbtree, start,\t\t\\\n+\t  rbtn_left_get(a_type, a_field, node), cb, arg));\t\t\\\n     } else {\t\t\t\t\t\t\t\t\\\n-    a_type *ret;\t\t\t\t\t\t\t\\\n-    if ((ret = cb(rbtree, node, arg)) != NULL) {\t\t\t\\\n-        return (ret);\t\t\t\t\t\t\\\n-    }\t\t\t\t\t\t\t\t\\\n-    return (a_prefix##reverse_iter_recurse(rbtree,\t\t\t\\\n-      rbtn_left_get(a_type, a_field, node), cb, arg));\t\t\\\n+\ta_type *ret;\t\t\t\t\t\t\t\\\n+\tif ((ret = cb(rbtree, node, arg)) != NULL) {\t\t\t\\\n+\t    return (ret);\t\t\t\t\t\t\\\n+\t}\t\t\t\t\t\t\t\t\\\n+\treturn (a_prefix##reverse_iter_recurse(rbtree,\t\t\t\\\n+\t  rbtn_left_get(a_type, a_field, node), cb, arg));\t\t\\\n     }\t\t\t\t\t\t\t\t\t\\\n }\t\t\t\t\t\t\t\t\t\\\n a_attr a_type *\t\t\t\t\t\t\t\t\\\n a_prefix##reverse_iter(a_rbt_type *rbtree, a_type *start,\t\t\\\n   a_type *(*cb)(a_rbt_type *, a_type *, void *), void *arg) {\t\t\\\n     a_type *ret;\t\t\t\t\t\t\t\\\n     if (start != NULL) {\t\t\t\t\t\t\\\n-    ret = a_prefix##reverse_iter_start(rbtree, start,\t\t\\\n-      rbtree->rbt_root, cb, arg);\t\t\t\t\t\\\n+\tret = a_prefix##reverse_iter_start(rbtree, start,\t\t\\\n+\t  rbtree->rbt_root, cb, arg);\t\t\t\t\t\\\n     } else {\t\t\t\t\t\t\t\t\\\n-    ret = a_prefix##reverse_iter_recurse(rbtree, rbtree->rbt_root,\t\\\n-      cb, arg);\t\t\t\t\t\t\t\\\n+\tret = a_prefix##reverse_iter_recurse(rbtree, rbtree->rbt_root,\t\\\n+\t  cb, arg);\t\t\t\t\t\t\t\\\n     }\t\t\t\t\t\t\t\t\t\\\n     if (ret == &rbtree->rbt_nil) {\t\t\t\t\t\\\n-    ret = NULL;\t\t\t\t\t\t\t\\\n+\tret = NULL;\t\t\t\t\t\t\t\\\n     }\t\t\t\t\t\t\t\t\t\\\n     return (ret);\t\t\t\t\t\t\t\\\n }"}, {"sha": "9bd98548cfed704622cee7f1c76cd7661e2662ed", "filename": "src/rt/jemalloc/include/jemalloc/internal/rtree.h", "status": "modified", "additions": 68, "deletions": 68, "changes": 136, "blob_url": "https://github.com/rust-lang/rust/blob/996301331e2d8ac7611d15ca82cc3ebc1abb632d/src%2Frt%2Fjemalloc%2Finclude%2Fjemalloc%2Finternal%2Frtree.h", "raw_url": "https://github.com/rust-lang/rust/raw/996301331e2d8ac7611d15ca82cc3ebc1abb632d/src%2Frt%2Fjemalloc%2Finclude%2Fjemalloc%2Finternal%2Frtree.h", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Frt%2Fjemalloc%2Finclude%2Fjemalloc%2Finternal%2Frtree.h?ref=996301331e2d8ac7611d15ca82cc3ebc1abb632d", "patch": "@@ -25,10 +25,10 @@ typedef struct rtree_s rtree_t;\n #ifdef JEMALLOC_H_STRUCTS\n \n struct rtree_s {\n-    malloc_mutex_t\tmutex;\n-    void\t\t**root;\n-    unsigned\theight;\n-    unsigned\tlevel2bits[1]; /* Dynamically sized. */\n+\tmalloc_mutex_t\tmutex;\n+\tvoid\t\t**root;\n+\tunsigned\theight;\n+\tunsigned\tlevel2bits[1]; /* Dynamically sized. */\n };\n \n #endif /* JEMALLOC_H_STRUCTS */\n@@ -58,37 +58,37 @@ bool\trtree_set(rtree_t *rtree, uintptr_t key, void *val);\n JEMALLOC_INLINE void *\t\t\t\t\t\t\t\\\n f(rtree_t *rtree, uintptr_t key)\t\t\t\t\t\\\n {\t\t\t\t\t\t\t\t\t\\\n-    void *ret;\t\t\t\t\t\t\t\\\n-    uintptr_t subkey;\t\t\t\t\t\t\\\n-    unsigned i, lshift, height, bits;\t\t\t\t\\\n-    void **node, **child;\t\t\t\t\t\t\\\n-                                    \\\n-    RTREE_LOCK(&rtree->mutex);\t\t\t\t\t\\\n-    for (i = lshift = 0, height = rtree->height, node = rtree->root;\\\n-        i < height - 1;\t\t\t\t\t\t\\\n-        i++, lshift += bits, node = child) {\t\t\t\\\n-        bits = rtree->level2bits[i];\t\t\t\t\\\n-        subkey = (key << lshift) >> ((ZU(1) << (LG_SIZEOF_PTR + \\\n-            3)) - bits);\t\t\t\t\t\\\n-        child = (void**)node[subkey];\t\t\t\t\\\n-        if (child == NULL) {\t\t\t\t\t\\\n-            RTREE_UNLOCK(&rtree->mutex);\t\t\t\\\n-            return (NULL);\t\t\t\t\t\\\n-        }\t\t\t\t\t\t\t\\\n-    }\t\t\t\t\t\t\t\t\\\n-                                    \\\n-    /*\t\t\t\t\t\t\t\t\\\n-     * node is a leaf, so it contains values rather than node\t\\\n-     * pointers.\t\t\t\t\t\t\t\\\n-     */\t\t\t\t\t\t\t\t\\\n-    bits = rtree->level2bits[i];\t\t\t\t\t\\\n-    subkey = (key << lshift) >> ((ZU(1) << (LG_SIZEOF_PTR+3)) -\t\\\n-        bits);\t\t\t\t\t\t\t\\\n-    ret = node[subkey];\t\t\t\t\t\t\\\n-    RTREE_UNLOCK(&rtree->mutex);\t\t\t\t\t\\\n-                                    \\\n-    RTREE_GET_VALIDATE\t\t\t\t\t\t\\\n-    return (ret);\t\t\t\t\t\t\t\\\n+\tvoid *ret;\t\t\t\t\t\t\t\\\n+\tuintptr_t subkey;\t\t\t\t\t\t\\\n+\tunsigned i, lshift, height, bits;\t\t\t\t\\\n+\tvoid **node, **child;\t\t\t\t\t\t\\\n+\t\t\t\t\t\t\t\t\t\\\n+\tRTREE_LOCK(&rtree->mutex);\t\t\t\t\t\\\n+\tfor (i = lshift = 0, height = rtree->height, node = rtree->root;\\\n+\t    i < height - 1;\t\t\t\t\t\t\\\n+\t    i++, lshift += bits, node = child) {\t\t\t\\\n+\t\tbits = rtree->level2bits[i];\t\t\t\t\\\n+\t\tsubkey = (key << lshift) >> ((ZU(1) << (LG_SIZEOF_PTR + \\\n+\t\t    3)) - bits);\t\t\t\t\t\\\n+\t\tchild = (void**)node[subkey];\t\t\t\t\\\n+\t\tif (child == NULL) {\t\t\t\t\t\\\n+\t\t\tRTREE_UNLOCK(&rtree->mutex);\t\t\t\\\n+\t\t\treturn (NULL);\t\t\t\t\t\\\n+\t\t}\t\t\t\t\t\t\t\\\n+\t}\t\t\t\t\t\t\t\t\\\n+\t\t\t\t\t\t\t\t\t\\\n+\t/*\t\t\t\t\t\t\t\t\\\n+\t * node is a leaf, so it contains values rather than node\t\\\n+\t * pointers.\t\t\t\t\t\t\t\\\n+\t */\t\t\t\t\t\t\t\t\\\n+\tbits = rtree->level2bits[i];\t\t\t\t\t\\\n+\tsubkey = (key << lshift) >> ((ZU(1) << (LG_SIZEOF_PTR+3)) -\t\\\n+\t    bits);\t\t\t\t\t\t\t\\\n+\tret = node[subkey];\t\t\t\t\t\t\\\n+\tRTREE_UNLOCK(&rtree->mutex);\t\t\t\t\t\\\n+\t\t\t\t\t\t\t\t\t\\\n+\tRTREE_GET_VALIDATE\t\t\t\t\t\t\\\n+\treturn (ret);\t\t\t\t\t\t\t\\\n }\n \n #ifdef JEMALLOC_DEBUG\n@@ -113,7 +113,7 @@ RTREE_GET_GENERATE(rtree_get_locked)\n     * seems impossible, but the following assertion is a prudent sanity check.\n     */\n #  define RTREE_GET_VALIDATE\t\t\t\t\t\t\\\n-    assert(rtree_get_locked(rtree, key) == ret);\n+\tassert(rtree_get_locked(rtree, key) == ret);\n #else\n #  define RTREE_GET_VALIDATE\n #endif\n@@ -125,38 +125,38 @@ RTREE_GET_GENERATE(rtree_get)\n JEMALLOC_INLINE bool\n rtree_set(rtree_t *rtree, uintptr_t key, void *val)\n {\n-    uintptr_t subkey;\n-    unsigned i, lshift, height, bits;\n-    void **node, **child;\n-\n-    malloc_mutex_lock(&rtree->mutex);\n-    for (i = lshift = 0, height = rtree->height, node = rtree->root;\n-        i < height - 1;\n-        i++, lshift += bits, node = child) {\n-        bits = rtree->level2bits[i];\n-        subkey = (key << lshift) >> ((ZU(1) << (LG_SIZEOF_PTR+3)) -\n-            bits);\n-        child = (void**)node[subkey];\n-        if (child == NULL) {\n-            child = (void**)base_alloc(sizeof(void *) <<\n-                rtree->level2bits[i+1]);\n-            if (child == NULL) {\n-                malloc_mutex_unlock(&rtree->mutex);\n-                return (true);\n-            }\n-            memset(child, 0, sizeof(void *) <<\n-                rtree->level2bits[i+1]);\n-            node[subkey] = child;\n-        }\n-    }\n-\n-    /* node is a leaf, so it contains values rather than node pointers. */\n-    bits = rtree->level2bits[i];\n-    subkey = (key << lshift) >> ((ZU(1) << (LG_SIZEOF_PTR+3)) - bits);\n-    node[subkey] = val;\n-    malloc_mutex_unlock(&rtree->mutex);\n-\n-    return (false);\n+\tuintptr_t subkey;\n+\tunsigned i, lshift, height, bits;\n+\tvoid **node, **child;\n+\n+\tmalloc_mutex_lock(&rtree->mutex);\n+\tfor (i = lshift = 0, height = rtree->height, node = rtree->root;\n+\t    i < height - 1;\n+\t    i++, lshift += bits, node = child) {\n+\t\tbits = rtree->level2bits[i];\n+\t\tsubkey = (key << lshift) >> ((ZU(1) << (LG_SIZEOF_PTR+3)) -\n+\t\t    bits);\n+\t\tchild = (void**)node[subkey];\n+\t\tif (child == NULL) {\n+\t\t\tchild = (void**)base_alloc(sizeof(void *) <<\n+\t\t\t    rtree->level2bits[i+1]);\n+\t\t\tif (child == NULL) {\n+\t\t\t\tmalloc_mutex_unlock(&rtree->mutex);\n+\t\t\t\treturn (true);\n+\t\t\t}\n+\t\t\tmemset(child, 0, sizeof(void *) <<\n+\t\t\t    rtree->level2bits[i+1]);\n+\t\t\tnode[subkey] = child;\n+\t\t}\n+\t}\n+\n+\t/* node is a leaf, so it contains values rather than node pointers. */\n+\tbits = rtree->level2bits[i];\n+\tsubkey = (key << lshift) >> ((ZU(1) << (LG_SIZEOF_PTR+3)) - bits);\n+\tnode[subkey] = val;\n+\tmalloc_mutex_unlock(&rtree->mutex);\n+\n+\treturn (false);\n }\n #endif\n "}, {"sha": "27f68e3681cfa7480612b871a083b3ca8b126e6e", "filename": "src/rt/jemalloc/include/jemalloc/internal/stats.h", "status": "modified", "additions": 102, "deletions": 102, "changes": 204, "blob_url": "https://github.com/rust-lang/rust/blob/996301331e2d8ac7611d15ca82cc3ebc1abb632d/src%2Frt%2Fjemalloc%2Finclude%2Fjemalloc%2Finternal%2Fstats.h", "raw_url": "https://github.com/rust-lang/rust/raw/996301331e2d8ac7611d15ca82cc3ebc1abb632d/src%2Frt%2Fjemalloc%2Finclude%2Fjemalloc%2Finternal%2Fstats.h", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Frt%2Fjemalloc%2Finclude%2Fjemalloc%2Finternal%2Fstats.h?ref=996301331e2d8ac7611d15ca82cc3ebc1abb632d", "patch": "@@ -12,117 +12,117 @@ typedef struct chunk_stats_s chunk_stats_t;\n #ifdef JEMALLOC_H_STRUCTS\n \n struct tcache_bin_stats_s {\n-    /*\n-     * Number of allocation requests that corresponded to the size of this\n-     * bin.\n-     */\n-    uint64_t\tnrequests;\n+\t/*\n+\t * Number of allocation requests that corresponded to the size of this\n+\t * bin.\n+\t */\n+\tuint64_t\tnrequests;\n };\n \n struct malloc_bin_stats_s {\n-    /*\n-     * Current number of bytes allocated, including objects currently\n-     * cached by tcache.\n-     */\n-    size_t\t\tallocated;\n-\n-    /*\n-     * Total number of allocation/deallocation requests served directly by\n-     * the bin.  Note that tcache may allocate an object, then recycle it\n-     * many times, resulting many increments to nrequests, but only one\n-     * each to nmalloc and ndalloc.\n-     */\n-    uint64_t\tnmalloc;\n-    uint64_t\tndalloc;\n-\n-    /*\n-     * Number of allocation requests that correspond to the size of this\n-     * bin.  This includes requests served by tcache, though tcache only\n-     * periodically merges into this counter.\n-     */\n-    uint64_t\tnrequests;\n-\n-    /* Number of tcache fills from this bin. */\n-    uint64_t\tnfills;\n-\n-    /* Number of tcache flushes to this bin. */\n-    uint64_t\tnflushes;\n-\n-    /* Total number of runs created for this bin's size class. */\n-    uint64_t\tnruns;\n-\n-    /*\n-     * Total number of runs reused by extracting them from the runs tree for\n-     * this bin's size class.\n-     */\n-    uint64_t\treruns;\n-\n-    /* Current number of runs in this bin. */\n-    size_t\t\tcurruns;\n+\t/*\n+\t * Current number of bytes allocated, including objects currently\n+\t * cached by tcache.\n+\t */\n+\tsize_t\t\tallocated;\n+\n+\t/*\n+\t * Total number of allocation/deallocation requests served directly by\n+\t * the bin.  Note that tcache may allocate an object, then recycle it\n+\t * many times, resulting many increments to nrequests, but only one\n+\t * each to nmalloc and ndalloc.\n+\t */\n+\tuint64_t\tnmalloc;\n+\tuint64_t\tndalloc;\n+\n+\t/*\n+\t * Number of allocation requests that correspond to the size of this\n+\t * bin.  This includes requests served by tcache, though tcache only\n+\t * periodically merges into this counter.\n+\t */\n+\tuint64_t\tnrequests;\n+\n+\t/* Number of tcache fills from this bin. */\n+\tuint64_t\tnfills;\n+\n+\t/* Number of tcache flushes to this bin. */\n+\tuint64_t\tnflushes;\n+\n+\t/* Total number of runs created for this bin's size class. */\n+\tuint64_t\tnruns;\n+\n+\t/*\n+\t * Total number of runs reused by extracting them from the runs tree for\n+\t * this bin's size class.\n+\t */\n+\tuint64_t\treruns;\n+\n+\t/* Current number of runs in this bin. */\n+\tsize_t\t\tcurruns;\n };\n \n struct malloc_large_stats_s {\n-    /*\n-     * Total number of allocation/deallocation requests served directly by\n-     * the arena.  Note that tcache may allocate an object, then recycle it\n-     * many times, resulting many increments to nrequests, but only one\n-     * each to nmalloc and ndalloc.\n-     */\n-    uint64_t\tnmalloc;\n-    uint64_t\tndalloc;\n-\n-    /*\n-     * Number of allocation requests that correspond to this size class.\n-     * This includes requests served by tcache, though tcache only\n-     * periodically merges into this counter.\n-     */\n-    uint64_t\tnrequests;\n-\n-    /* Current number of runs of this size class. */\n-    size_t\t\tcurruns;\n+\t/*\n+\t * Total number of allocation/deallocation requests served directly by\n+\t * the arena.  Note that tcache may allocate an object, then recycle it\n+\t * many times, resulting many increments to nrequests, but only one\n+\t * each to nmalloc and ndalloc.\n+\t */\n+\tuint64_t\tnmalloc;\n+\tuint64_t\tndalloc;\n+\n+\t/*\n+\t * Number of allocation requests that correspond to this size class.\n+\t * This includes requests served by tcache, though tcache only\n+\t * periodically merges into this counter.\n+\t */\n+\tuint64_t\tnrequests;\n+\n+\t/* Current number of runs of this size class. */\n+\tsize_t\t\tcurruns;\n };\n \n struct arena_stats_s {\n-    /* Number of bytes currently mapped. */\n-    size_t\t\tmapped;\n-\n-    /*\n-     * Total number of purge sweeps, total number of madvise calls made,\n-     * and total pages purged in order to keep dirty unused memory under\n-     * control.\n-     */\n-    uint64_t\tnpurge;\n-    uint64_t\tnmadvise;\n-    uint64_t\tpurged;\n-\n-    /* Per-size-category statistics. */\n-    size_t\t\tallocated_large;\n-    uint64_t\tnmalloc_large;\n-    uint64_t\tndalloc_large;\n-    uint64_t\tnrequests_large;\n-\n-    /*\n-     * One element for each possible size class, including sizes that\n-     * overlap with bin size classes.  This is necessary because ipalloc()\n-     * sometimes has to use such large objects in order to assure proper\n-     * alignment.\n-     */\n-    malloc_large_stats_t\t*lstats;\n+\t/* Number of bytes currently mapped. */\n+\tsize_t\t\tmapped;\n+\n+\t/*\n+\t * Total number of purge sweeps, total number of madvise calls made,\n+\t * and total pages purged in order to keep dirty unused memory under\n+\t * control.\n+\t */\n+\tuint64_t\tnpurge;\n+\tuint64_t\tnmadvise;\n+\tuint64_t\tpurged;\n+\n+\t/* Per-size-category statistics. */\n+\tsize_t\t\tallocated_large;\n+\tuint64_t\tnmalloc_large;\n+\tuint64_t\tndalloc_large;\n+\tuint64_t\tnrequests_large;\n+\n+\t/*\n+\t * One element for each possible size class, including sizes that\n+\t * overlap with bin size classes.  This is necessary because ipalloc()\n+\t * sometimes has to use such large objects in order to assure proper\n+\t * alignment.\n+\t */\n+\tmalloc_large_stats_t\t*lstats;\n };\n \n struct chunk_stats_s {\n-    /* Number of chunks that were allocated. */\n-    uint64_t\tnchunks;\n-\n-    /* High-water mark for number of chunks allocated. */\n-    size_t\t\thighchunks;\n-\n-    /*\n-     * Current number of chunks allocated.  This value isn't maintained for\n-     * any other purpose, so keep track of it in order to be able to set\n-     * highchunks.\n-     */\n-    size_t\t\tcurchunks;\n+\t/* Number of chunks that were allocated. */\n+\tuint64_t\tnchunks;\n+\n+\t/* High-water mark for number of chunks allocated. */\n+\tsize_t\t\thighchunks;\n+\n+\t/*\n+\t * Current number of chunks allocated.  This value isn't maintained for\n+\t * any other purpose, so keep track of it in order to be able to set\n+\t * highchunks.\n+\t */\n+\tsize_t\t\tcurchunks;\n };\n \n #endif /* JEMALLOC_H_STRUCTS */\n@@ -151,21 +151,21 @@ JEMALLOC_INLINE size_t\n stats_cactive_get(void)\n {\n \n-    return (atomic_read_z(&stats_cactive));\n+\treturn (atomic_read_z(&stats_cactive));\n }\n \n JEMALLOC_INLINE void\n stats_cactive_add(size_t size)\n {\n \n-    atomic_add_z(&stats_cactive, size);\n+\tatomic_add_z(&stats_cactive, size);\n }\n \n JEMALLOC_INLINE void\n stats_cactive_sub(size_t size)\n {\n \n-    atomic_sub_z(&stats_cactive, size);\n+\tatomic_sub_z(&stats_cactive, size);\n }\n #endif\n "}, {"sha": "ba36204ff210b8783ccbd353588e7353aa1badb4", "filename": "src/rt/jemalloc/include/jemalloc/internal/tcache.h", "status": "modified", "additions": 250, "deletions": 250, "changes": 500, "blob_url": "https://github.com/rust-lang/rust/blob/996301331e2d8ac7611d15ca82cc3ebc1abb632d/src%2Frt%2Fjemalloc%2Finclude%2Fjemalloc%2Finternal%2Ftcache.h", "raw_url": "https://github.com/rust-lang/rust/raw/996301331e2d8ac7611d15ca82cc3ebc1abb632d/src%2Frt%2Fjemalloc%2Finclude%2Fjemalloc%2Finternal%2Ftcache.h", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Frt%2Fjemalloc%2Finclude%2Fjemalloc%2Finternal%2Ftcache.h?ref=996301331e2d8ac7611d15ca82cc3ebc1abb632d", "patch": "@@ -46,40 +46,40 @@ typedef struct tcache_s tcache_t;\n #ifdef JEMALLOC_H_STRUCTS\n \n typedef enum {\n-    tcache_enabled_false   = 0, /* Enable cast to/from bool. */\n-    tcache_enabled_true    = 1,\n-    tcache_enabled_default = 2\n+\ttcache_enabled_false   = 0, /* Enable cast to/from bool. */\n+\ttcache_enabled_true    = 1,\n+\ttcache_enabled_default = 2\n } tcache_enabled_t;\n \n /*\n  * Read-only information associated with each element of tcache_t's tbins array\n  * is stored separately, mainly to reduce memory usage.\n  */\n struct tcache_bin_info_s {\n-    unsigned\tncached_max;\t/* Upper limit on ncached. */\n+\tunsigned\tncached_max;\t/* Upper limit on ncached. */\n };\n \n struct tcache_bin_s {\n-    tcache_bin_stats_t tstats;\n-    int\t\tlow_water;\t/* Min # cached since last GC. */\n-    unsigned\tlg_fill_div;\t/* Fill (ncached_max >> lg_fill_div). */\n-    unsigned\tncached;\t/* # of cached objects. */\n-    void\t\t**avail;\t/* Stack of available objects. */\n+\ttcache_bin_stats_t tstats;\n+\tint\t\tlow_water;\t/* Min # cached since last GC. */\n+\tunsigned\tlg_fill_div;\t/* Fill (ncached_max >> lg_fill_div). */\n+\tunsigned\tncached;\t/* # of cached objects. */\n+\tvoid\t\t**avail;\t/* Stack of available objects. */\n };\n \n struct tcache_s {\n-    ql_elm(tcache_t) link;\t\t/* Used for aggregating stats. */\n-    uint64_t\tprof_accumbytes;/* Cleared after arena_prof_accum() */\n-    arena_t\t\t*arena;\t\t/* This thread's arena. */\n-    unsigned\tev_cnt;\t\t/* Event count since incremental GC. */\n-    unsigned\tnext_gc_bin;\t/* Next bin to GC. */\n-    tcache_bin_t\ttbins[1];\t/* Dynamically sized. */\n-    /*\n-     * The pointer stacks associated with tbins follow as a contiguous\n-     * array.  During tcache initialization, the avail pointer in each\n-     * element of tbins is initialized to point to the proper offset within\n-     * this array.\n-     */\n+\tql_elm(tcache_t) link;\t\t/* Used for aggregating stats. */\n+\tuint64_t\tprof_accumbytes;/* Cleared after arena_prof_accum() */\n+\tarena_t\t\t*arena;\t\t/* This thread's arena. */\n+\tunsigned\tev_cnt;\t\t/* Event count since incremental GC. */\n+\tunsigned\tnext_gc_bin;\t/* Next bin to GC. */\n+\ttcache_bin_t\ttbins[1];\t/* Dynamically sized. */\n+\t/*\n+\t * The pointer stacks associated with tbins follow as a contiguous\n+\t * array.  During tcache initialization, the avail pointer in each\n+\t * element of tbins is initialized to point to the proper offset within\n+\t * this array.\n+\t */\n };\n \n #endif /* JEMALLOC_H_STRUCTS */\n@@ -150,291 +150,291 @@ malloc_tsd_funcs(JEMALLOC_ALWAYS_INLINE, tcache_enabled, tcache_enabled_t,\n JEMALLOC_INLINE void\n tcache_flush(void)\n {\n-    tcache_t *tcache;\n+\ttcache_t *tcache;\n \n-    cassert(config_tcache);\n+\tcassert(config_tcache);\n \n-    tcache = *tcache_tsd_get();\n-    if ((uintptr_t)tcache <= (uintptr_t)TCACHE_STATE_MAX)\n-        return;\n-    tcache_destroy(tcache);\n-    tcache = NULL;\n-    tcache_tsd_set(&tcache);\n+\ttcache = *tcache_tsd_get();\n+\tif ((uintptr_t)tcache <= (uintptr_t)TCACHE_STATE_MAX)\n+\t\treturn;\n+\ttcache_destroy(tcache);\n+\ttcache = NULL;\n+\ttcache_tsd_set(&tcache);\n }\n \n JEMALLOC_INLINE bool\n tcache_enabled_get(void)\n {\n-    tcache_enabled_t tcache_enabled;\n+\ttcache_enabled_t tcache_enabled;\n \n-    cassert(config_tcache);\n+\tcassert(config_tcache);\n \n-    tcache_enabled = *tcache_enabled_tsd_get();\n-    if (tcache_enabled == tcache_enabled_default) {\n-        tcache_enabled = (tcache_enabled_t)opt_tcache;\n-        tcache_enabled_tsd_set(&tcache_enabled);\n-    }\n+\ttcache_enabled = *tcache_enabled_tsd_get();\n+\tif (tcache_enabled == tcache_enabled_default) {\n+\t\ttcache_enabled = (tcache_enabled_t)opt_tcache;\n+\t\ttcache_enabled_tsd_set(&tcache_enabled);\n+\t}\n \n-    return ((bool)tcache_enabled);\n+\treturn ((bool)tcache_enabled);\n }\n \n JEMALLOC_INLINE void\n tcache_enabled_set(bool enabled)\n {\n-    tcache_enabled_t tcache_enabled;\n-    tcache_t *tcache;\n-\n-    cassert(config_tcache);\n-\n-    tcache_enabled = (tcache_enabled_t)enabled;\n-    tcache_enabled_tsd_set(&tcache_enabled);\n-    tcache = *tcache_tsd_get();\n-    if (enabled) {\n-        if (tcache == TCACHE_STATE_DISABLED) {\n-            tcache = NULL;\n-            tcache_tsd_set(&tcache);\n-        }\n-    } else /* disabled */ {\n-        if (tcache > TCACHE_STATE_MAX) {\n-            tcache_destroy(tcache);\n-            tcache = NULL;\n-        }\n-        if (tcache == NULL) {\n-            tcache = TCACHE_STATE_DISABLED;\n-            tcache_tsd_set(&tcache);\n-        }\n-    }\n+\ttcache_enabled_t tcache_enabled;\n+\ttcache_t *tcache;\n+\n+\tcassert(config_tcache);\n+\n+\ttcache_enabled = (tcache_enabled_t)enabled;\n+\ttcache_enabled_tsd_set(&tcache_enabled);\n+\ttcache = *tcache_tsd_get();\n+\tif (enabled) {\n+\t\tif (tcache == TCACHE_STATE_DISABLED) {\n+\t\t\ttcache = NULL;\n+\t\t\ttcache_tsd_set(&tcache);\n+\t\t}\n+\t} else /* disabled */ {\n+\t\tif (tcache > TCACHE_STATE_MAX) {\n+\t\t\ttcache_destroy(tcache);\n+\t\t\ttcache = NULL;\n+\t\t}\n+\t\tif (tcache == NULL) {\n+\t\t\ttcache = TCACHE_STATE_DISABLED;\n+\t\t\ttcache_tsd_set(&tcache);\n+\t\t}\n+\t}\n }\n \n JEMALLOC_ALWAYS_INLINE tcache_t *\n tcache_get(bool create)\n {\n-    tcache_t *tcache;\n-\n-    if (config_tcache == false)\n-        return (NULL);\n-    if (config_lazy_lock && isthreaded == false)\n-        return (NULL);\n-\n-    tcache = *tcache_tsd_get();\n-    if ((uintptr_t)tcache <= (uintptr_t)TCACHE_STATE_MAX) {\n-        if (tcache == TCACHE_STATE_DISABLED)\n-            return (NULL);\n-        if (tcache == NULL) {\n-            if (create == false) {\n-                /*\n-                 * Creating a tcache here would cause\n-                 * allocation as a side effect of free().\n-                 * Ordinarily that would be okay since\n-                 * tcache_create() failure is a soft failure\n-                 * that doesn't propagate.  However, if TLS\n-                 * data are freed via free() as in glibc,\n-                 * subtle corruption could result from setting\n-                 * a TLS variable after its backing memory is\n-                 * freed.\n-                 */\n-                return (NULL);\n-            }\n-            if (tcache_enabled_get() == false) {\n-                tcache_enabled_set(false); /* Memoize. */\n-                return (NULL);\n-            }\n-            return (tcache_create(choose_arena(NULL)));\n-        }\n-        if (tcache == TCACHE_STATE_PURGATORY) {\n-            /*\n-             * Make a note that an allocator function was called\n-             * after tcache_thread_cleanup() was called.\n-             */\n-            tcache = TCACHE_STATE_REINCARNATED;\n-            tcache_tsd_set(&tcache);\n-            return (NULL);\n-        }\n-        if (tcache == TCACHE_STATE_REINCARNATED)\n-            return (NULL);\n-        not_reached();\n-    }\n-\n-    return (tcache);\n+\ttcache_t *tcache;\n+\n+\tif (config_tcache == false)\n+\t\treturn (NULL);\n+\tif (config_lazy_lock && isthreaded == false)\n+\t\treturn (NULL);\n+\n+\ttcache = *tcache_tsd_get();\n+\tif ((uintptr_t)tcache <= (uintptr_t)TCACHE_STATE_MAX) {\n+\t\tif (tcache == TCACHE_STATE_DISABLED)\n+\t\t\treturn (NULL);\n+\t\tif (tcache == NULL) {\n+\t\t\tif (create == false) {\n+\t\t\t\t/*\n+\t\t\t\t * Creating a tcache here would cause\n+\t\t\t\t * allocation as a side effect of free().\n+\t\t\t\t * Ordinarily that would be okay since\n+\t\t\t\t * tcache_create() failure is a soft failure\n+\t\t\t\t * that doesn't propagate.  However, if TLS\n+\t\t\t\t * data are freed via free() as in glibc,\n+\t\t\t\t * subtle corruption could result from setting\n+\t\t\t\t * a TLS variable after its backing memory is\n+\t\t\t\t * freed.\n+\t\t\t\t */\n+\t\t\t\treturn (NULL);\n+\t\t\t}\n+\t\t\tif (tcache_enabled_get() == false) {\n+\t\t\t\ttcache_enabled_set(false); /* Memoize. */\n+\t\t\t\treturn (NULL);\n+\t\t\t}\n+\t\t\treturn (tcache_create(choose_arena(NULL)));\n+\t\t}\n+\t\tif (tcache == TCACHE_STATE_PURGATORY) {\n+\t\t\t/*\n+\t\t\t * Make a note that an allocator function was called\n+\t\t\t * after tcache_thread_cleanup() was called.\n+\t\t\t */\n+\t\t\ttcache = TCACHE_STATE_REINCARNATED;\n+\t\t\ttcache_tsd_set(&tcache);\n+\t\t\treturn (NULL);\n+\t\t}\n+\t\tif (tcache == TCACHE_STATE_REINCARNATED)\n+\t\t\treturn (NULL);\n+\t\tnot_reached();\n+\t}\n+\n+\treturn (tcache);\n }\n \n JEMALLOC_ALWAYS_INLINE void\n tcache_event(tcache_t *tcache)\n {\n \n-    if (TCACHE_GC_INCR == 0)\n-        return;\n+\tif (TCACHE_GC_INCR == 0)\n+\t\treturn;\n \n-    tcache->ev_cnt++;\n-    assert(tcache->ev_cnt <= TCACHE_GC_INCR);\n-    if (tcache->ev_cnt == TCACHE_GC_INCR)\n-        tcache_event_hard(tcache);\n+\ttcache->ev_cnt++;\n+\tassert(tcache->ev_cnt <= TCACHE_GC_INCR);\n+\tif (tcache->ev_cnt == TCACHE_GC_INCR)\n+\t\ttcache_event_hard(tcache);\n }\n \n JEMALLOC_ALWAYS_INLINE void *\n tcache_alloc_easy(tcache_bin_t *tbin)\n {\n-    void *ret;\n-\n-    if (tbin->ncached == 0) {\n-        tbin->low_water = -1;\n-        return (NULL);\n-    }\n-    tbin->ncached--;\n-    if ((int)tbin->ncached < tbin->low_water)\n-        tbin->low_water = tbin->ncached;\n-    ret = tbin->avail[tbin->ncached];\n-    return (ret);\n+\tvoid *ret;\n+\n+\tif (tbin->ncached == 0) {\n+\t\ttbin->low_water = -1;\n+\t\treturn (NULL);\n+\t}\n+\ttbin->ncached--;\n+\tif ((int)tbin->ncached < tbin->low_water)\n+\t\ttbin->low_water = tbin->ncached;\n+\tret = tbin->avail[tbin->ncached];\n+\treturn (ret);\n }\n \n JEMALLOC_ALWAYS_INLINE void *\n tcache_alloc_small(tcache_t *tcache, size_t size, bool zero)\n {\n-    void *ret;\n-    size_t binind;\n-    tcache_bin_t *tbin;\n-\n-    binind = SMALL_SIZE2BIN(size);\n-    assert(binind < NBINS);\n-    tbin = &tcache->tbins[binind];\n-    ret = tcache_alloc_easy(tbin);\n-    if (ret == NULL) {\n-        ret = tcache_alloc_small_hard(tcache, tbin, binind);\n-        if (ret == NULL)\n-            return (NULL);\n-    }\n-    assert(tcache_salloc(ret) == arena_bin_info[binind].reg_size);\n-\n-    if (zero == false) {\n-        if (config_fill) {\n-            if (opt_junk) {\n-                arena_alloc_junk_small(ret,\n-                    &arena_bin_info[binind], false);\n-            } else if (opt_zero)\n-                memset(ret, 0, size);\n-        }\n-    } else {\n-        if (config_fill && opt_junk) {\n-            arena_alloc_junk_small(ret, &arena_bin_info[binind],\n-                true);\n-        }\n-        VALGRIND_MAKE_MEM_UNDEFINED(ret, size);\n-        memset(ret, 0, size);\n-    }\n-    VALGRIND_MAKE_MEM_UNDEFINED(ret, size);\n-\n-    if (config_stats)\n-        tbin->tstats.nrequests++;\n-    if (config_prof)\n-        tcache->prof_accumbytes += arena_bin_info[binind].reg_size;\n-    tcache_event(tcache);\n-    return (ret);\n+\tvoid *ret;\n+\tsize_t binind;\n+\ttcache_bin_t *tbin;\n+\n+\tbinind = SMALL_SIZE2BIN(size);\n+\tassert(binind < NBINS);\n+\ttbin = &tcache->tbins[binind];\n+\tret = tcache_alloc_easy(tbin);\n+\tif (ret == NULL) {\n+\t\tret = tcache_alloc_small_hard(tcache, tbin, binind);\n+\t\tif (ret == NULL)\n+\t\t\treturn (NULL);\n+\t}\n+\tassert(tcache_salloc(ret) == arena_bin_info[binind].reg_size);\n+\n+\tif (zero == false) {\n+\t\tif (config_fill) {\n+\t\t\tif (opt_junk) {\n+\t\t\t\tarena_alloc_junk_small(ret,\n+\t\t\t\t    &arena_bin_info[binind], false);\n+\t\t\t} else if (opt_zero)\n+\t\t\t\tmemset(ret, 0, size);\n+\t\t}\n+\t} else {\n+\t\tif (config_fill && opt_junk) {\n+\t\t\tarena_alloc_junk_small(ret, &arena_bin_info[binind],\n+\t\t\t    true);\n+\t\t}\n+\t\tVALGRIND_MAKE_MEM_UNDEFINED(ret, size);\n+\t\tmemset(ret, 0, size);\n+\t}\n+\tVALGRIND_MAKE_MEM_UNDEFINED(ret, size);\n+\n+\tif (config_stats)\n+\t\ttbin->tstats.nrequests++;\n+\tif (config_prof)\n+\t\ttcache->prof_accumbytes += arena_bin_info[binind].reg_size;\n+\ttcache_event(tcache);\n+\treturn (ret);\n }\n \n JEMALLOC_ALWAYS_INLINE void *\n tcache_alloc_large(tcache_t *tcache, size_t size, bool zero)\n {\n-    void *ret;\n-    size_t binind;\n-    tcache_bin_t *tbin;\n-\n-    size = PAGE_CEILING(size);\n-    assert(size <= tcache_maxclass);\n-    binind = NBINS + (size >> LG_PAGE) - 1;\n-    assert(binind < nhbins);\n-    tbin = &tcache->tbins[binind];\n-    ret = tcache_alloc_easy(tbin);\n-    if (ret == NULL) {\n-        /*\n-         * Only allocate one large object at a time, because it's quite\n-         * expensive to create one and not use it.\n-         */\n-        ret = arena_malloc_large(tcache->arena, size, zero);\n-        if (ret == NULL)\n-            return (NULL);\n-    } else {\n-        if (config_prof && prof_promote && size == PAGE) {\n-            arena_chunk_t *chunk =\n-                (arena_chunk_t *)CHUNK_ADDR2BASE(ret);\n-            size_t pageind = (((uintptr_t)ret - (uintptr_t)chunk) >>\n-                LG_PAGE);\n-            arena_mapbits_large_binind_set(chunk, pageind,\n-                BININD_INVALID);\n-        }\n-        if (zero == false) {\n-            if (config_fill) {\n-                if (opt_junk)\n-                    memset(ret, 0xa5, size);\n-                else if (opt_zero)\n-                    memset(ret, 0, size);\n-            }\n-        } else {\n-            VALGRIND_MAKE_MEM_UNDEFINED(ret, size);\n-            memset(ret, 0, size);\n-        }\n-        VALGRIND_MAKE_MEM_UNDEFINED(ret, size);\n-\n-        if (config_stats)\n-            tbin->tstats.nrequests++;\n-        if (config_prof)\n-            tcache->prof_accumbytes += size;\n-    }\n-\n-    tcache_event(tcache);\n-    return (ret);\n+\tvoid *ret;\n+\tsize_t binind;\n+\ttcache_bin_t *tbin;\n+\n+\tsize = PAGE_CEILING(size);\n+\tassert(size <= tcache_maxclass);\n+\tbinind = NBINS + (size >> LG_PAGE) - 1;\n+\tassert(binind < nhbins);\n+\ttbin = &tcache->tbins[binind];\n+\tret = tcache_alloc_easy(tbin);\n+\tif (ret == NULL) {\n+\t\t/*\n+\t\t * Only allocate one large object at a time, because it's quite\n+\t\t * expensive to create one and not use it.\n+\t\t */\n+\t\tret = arena_malloc_large(tcache->arena, size, zero);\n+\t\tif (ret == NULL)\n+\t\t\treturn (NULL);\n+\t} else {\n+\t\tif (config_prof && prof_promote && size == PAGE) {\n+\t\t\tarena_chunk_t *chunk =\n+\t\t\t    (arena_chunk_t *)CHUNK_ADDR2BASE(ret);\n+\t\t\tsize_t pageind = (((uintptr_t)ret - (uintptr_t)chunk) >>\n+\t\t\t    LG_PAGE);\n+\t\t\tarena_mapbits_large_binind_set(chunk, pageind,\n+\t\t\t    BININD_INVALID);\n+\t\t}\n+\t\tif (zero == false) {\n+\t\t\tif (config_fill) {\n+\t\t\t\tif (opt_junk)\n+\t\t\t\t\tmemset(ret, 0xa5, size);\n+\t\t\t\telse if (opt_zero)\n+\t\t\t\t\tmemset(ret, 0, size);\n+\t\t\t}\n+\t\t} else {\n+\t\t\tVALGRIND_MAKE_MEM_UNDEFINED(ret, size);\n+\t\t\tmemset(ret, 0, size);\n+\t\t}\n+\t\tVALGRIND_MAKE_MEM_UNDEFINED(ret, size);\n+\n+\t\tif (config_stats)\n+\t\t\ttbin->tstats.nrequests++;\n+\t\tif (config_prof)\n+\t\t\ttcache->prof_accumbytes += size;\n+\t}\n+\n+\ttcache_event(tcache);\n+\treturn (ret);\n }\n \n JEMALLOC_ALWAYS_INLINE void\n tcache_dalloc_small(tcache_t *tcache, void *ptr, size_t binind)\n {\n-    tcache_bin_t *tbin;\n-    tcache_bin_info_t *tbin_info;\n+\ttcache_bin_t *tbin;\n+\ttcache_bin_info_t *tbin_info;\n \n-    assert(tcache_salloc(ptr) <= SMALL_MAXCLASS);\n+\tassert(tcache_salloc(ptr) <= SMALL_MAXCLASS);\n \n-    if (config_fill && opt_junk)\n-        arena_dalloc_junk_small(ptr, &arena_bin_info[binind]);\n+\tif (config_fill && opt_junk)\n+\t\tarena_dalloc_junk_small(ptr, &arena_bin_info[binind]);\n \n-    tbin = &tcache->tbins[binind];\n-    tbin_info = &tcache_bin_info[binind];\n-    if (tbin->ncached == tbin_info->ncached_max) {\n-        tcache_bin_flush_small(tbin, binind, (tbin_info->ncached_max >>\n-            1), tcache);\n-    }\n-    assert(tbin->ncached < tbin_info->ncached_max);\n-    tbin->avail[tbin->ncached] = ptr;\n-    tbin->ncached++;\n+\ttbin = &tcache->tbins[binind];\n+\ttbin_info = &tcache_bin_info[binind];\n+\tif (tbin->ncached == tbin_info->ncached_max) {\n+\t\ttcache_bin_flush_small(tbin, binind, (tbin_info->ncached_max >>\n+\t\t    1), tcache);\n+\t}\n+\tassert(tbin->ncached < tbin_info->ncached_max);\n+\ttbin->avail[tbin->ncached] = ptr;\n+\ttbin->ncached++;\n \n-    tcache_event(tcache);\n+\ttcache_event(tcache);\n }\n \n JEMALLOC_ALWAYS_INLINE void\n tcache_dalloc_large(tcache_t *tcache, void *ptr, size_t size)\n {\n-    size_t binind;\n-    tcache_bin_t *tbin;\n-    tcache_bin_info_t *tbin_info;\n-\n-    assert((size & PAGE_MASK) == 0);\n-    assert(tcache_salloc(ptr) > SMALL_MAXCLASS);\n-    assert(tcache_salloc(ptr) <= tcache_maxclass);\n-\n-    binind = NBINS + (size >> LG_PAGE) - 1;\n-\n-    if (config_fill && opt_junk)\n-        memset(ptr, 0x5a, size);\n-\n-    tbin = &tcache->tbins[binind];\n-    tbin_info = &tcache_bin_info[binind];\n-    if (tbin->ncached == tbin_info->ncached_max) {\n-        tcache_bin_flush_large(tbin, binind, (tbin_info->ncached_max >>\n-            1), tcache);\n-    }\n-    assert(tbin->ncached < tbin_info->ncached_max);\n-    tbin->avail[tbin->ncached] = ptr;\n-    tbin->ncached++;\n-\n-    tcache_event(tcache);\n+\tsize_t binind;\n+\ttcache_bin_t *tbin;\n+\ttcache_bin_info_t *tbin_info;\n+\n+\tassert((size & PAGE_MASK) == 0);\n+\tassert(tcache_salloc(ptr) > SMALL_MAXCLASS);\n+\tassert(tcache_salloc(ptr) <= tcache_maxclass);\n+\n+\tbinind = NBINS + (size >> LG_PAGE) - 1;\n+\n+\tif (config_fill && opt_junk)\n+\t\tmemset(ptr, 0x5a, size);\n+\n+\ttbin = &tcache->tbins[binind];\n+\ttbin_info = &tcache_bin_info[binind];\n+\tif (tbin->ncached == tbin_info->ncached_max) {\n+\t\ttcache_bin_flush_large(tbin, binind, (tbin_info->ncached_max >>\n+\t\t    1), tcache);\n+\t}\n+\tassert(tbin->ncached < tbin_info->ncached_max);\n+\ttbin->avail[tbin->ncached] = ptr;\n+\ttbin->ncached++;\n+\n+\ttcache_event(tcache);\n }\n #endif\n "}, {"sha": "0037cf35e703152e75602ad071414398fbddd4f4", "filename": "src/rt/jemalloc/include/jemalloc/internal/tsd.h", "status": "modified", "additions": 170, "deletions": 170, "changes": 340, "blob_url": "https://github.com/rust-lang/rust/blob/996301331e2d8ac7611d15ca82cc3ebc1abb632d/src%2Frt%2Fjemalloc%2Finclude%2Fjemalloc%2Finternal%2Ftsd.h", "raw_url": "https://github.com/rust-lang/rust/raw/996301331e2d8ac7611d15ca82cc3ebc1abb632d/src%2Frt%2Fjemalloc%2Finclude%2Fjemalloc%2Finternal%2Ftsd.h", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Frt%2Fjemalloc%2Finclude%2Fjemalloc%2Finternal%2Ftsd.h?ref=996301331e2d8ac7611d15ca82cc3ebc1abb632d", "patch": "@@ -116,40 +116,40 @@ a_attr bool\t\ta_name##_booted = false;\n a_attr bool\t\t\t\t\t\t\t\t\\\n a_name##_tsd_cleanup_wrapper(void)\t\t\t\t\t\\\n {\t\t\t\t\t\t\t\t\t\\\n-                                    \\\n-    if (a_name##_initialized) {\t\t\t\t\t\\\n-        a_name##_initialized = false;\t\t\t\t\\\n-        a_cleanup(&a_name##_tls);\t\t\t\t\\\n-    }\t\t\t\t\t\t\t\t\\\n-    return (a_name##_initialized);\t\t\t\t\t\\\n+\t\t\t\t\t\t\t\t\t\\\n+\tif (a_name##_initialized) {\t\t\t\t\t\\\n+\t\ta_name##_initialized = false;\t\t\t\t\\\n+\t\ta_cleanup(&a_name##_tls);\t\t\t\t\\\n+\t}\t\t\t\t\t\t\t\t\\\n+\treturn (a_name##_initialized);\t\t\t\t\t\\\n }\t\t\t\t\t\t\t\t\t\\\n a_attr bool\t\t\t\t\t\t\t\t\\\n a_name##_tsd_boot(void)\t\t\t\t\t\t\t\\\n {\t\t\t\t\t\t\t\t\t\\\n-                                    \\\n-    if (a_cleanup != malloc_tsd_no_cleanup) {\t\t\t\\\n-        malloc_tsd_cleanup_register(\t\t\t\t\\\n-            &a_name##_tsd_cleanup_wrapper);\t\t\t\\\n-    }\t\t\t\t\t\t\t\t\\\n-    a_name##_booted = true;\t\t\t\t\t\t\\\n-    return (false);\t\t\t\t\t\t\t\\\n+\t\t\t\t\t\t\t\t\t\\\n+\tif (a_cleanup != malloc_tsd_no_cleanup) {\t\t\t\\\n+\t\tmalloc_tsd_cleanup_register(\t\t\t\t\\\n+\t\t    &a_name##_tsd_cleanup_wrapper);\t\t\t\\\n+\t}\t\t\t\t\t\t\t\t\\\n+\ta_name##_booted = true;\t\t\t\t\t\t\\\n+\treturn (false);\t\t\t\t\t\t\t\\\n }\t\t\t\t\t\t\t\t\t\\\n /* Get/set. */\t\t\t\t\t\t\t\t\\\n a_attr a_type *\t\t\t\t\t\t\t\t\\\n a_name##_tsd_get(void)\t\t\t\t\t\t\t\\\n {\t\t\t\t\t\t\t\t\t\\\n-                                    \\\n-    assert(a_name##_booted);\t\t\t\t\t\\\n-    return (&a_name##_tls);\t\t\t\t\t\t\\\n+\t\t\t\t\t\t\t\t\t\\\n+\tassert(a_name##_booted);\t\t\t\t\t\\\n+\treturn (&a_name##_tls);\t\t\t\t\t\t\\\n }\t\t\t\t\t\t\t\t\t\\\n a_attr void\t\t\t\t\t\t\t\t\\\n a_name##_tsd_set(a_type *val)\t\t\t\t\t\t\\\n {\t\t\t\t\t\t\t\t\t\\\n-                                    \\\n-    assert(a_name##_booted);\t\t\t\t\t\\\n-    a_name##_tls = (*val);\t\t\t\t\t\t\\\n-    if (a_cleanup != malloc_tsd_no_cleanup)\t\t\t\t\\\n-        a_name##_initialized = true;\t\t\t\t\\\n+\t\t\t\t\t\t\t\t\t\\\n+\tassert(a_name##_booted);\t\t\t\t\t\\\n+\ta_name##_tls = (*val);\t\t\t\t\t\t\\\n+\tif (a_cleanup != malloc_tsd_no_cleanup)\t\t\t\t\\\n+\t\ta_name##_initialized = true;\t\t\t\t\\\n }\n #elif (defined(JEMALLOC_TLS))\n #define\tmalloc_tsd_funcs(a_attr, a_name, a_type, a_initializer,\t\t\\\n@@ -158,220 +158,220 @@ a_name##_tsd_set(a_type *val)\t\t\t\t\t\t\\\n a_attr bool\t\t\t\t\t\t\t\t\\\n a_name##_tsd_boot(void)\t\t\t\t\t\t\t\\\n {\t\t\t\t\t\t\t\t\t\\\n-                                    \\\n-    if (a_cleanup != malloc_tsd_no_cleanup) {\t\t\t\\\n-        if (pthread_key_create(&a_name##_tsd, a_cleanup) != 0)\t\\\n-            return (true);\t\t\t\t\t\\\n-    }\t\t\t\t\t\t\t\t\\\n-    a_name##_booted = true;\t\t\t\t\t\t\\\n-    return (false);\t\t\t\t\t\t\t\\\n+\t\t\t\t\t\t\t\t\t\\\n+\tif (a_cleanup != malloc_tsd_no_cleanup) {\t\t\t\\\n+\t\tif (pthread_key_create(&a_name##_tsd, a_cleanup) != 0)\t\\\n+\t\t\treturn (true);\t\t\t\t\t\\\n+\t}\t\t\t\t\t\t\t\t\\\n+\ta_name##_booted = true;\t\t\t\t\t\t\\\n+\treturn (false);\t\t\t\t\t\t\t\\\n }\t\t\t\t\t\t\t\t\t\\\n /* Get/set. */\t\t\t\t\t\t\t\t\\\n a_attr a_type *\t\t\t\t\t\t\t\t\\\n a_name##_tsd_get(void)\t\t\t\t\t\t\t\\\n {\t\t\t\t\t\t\t\t\t\\\n-                                    \\\n-    assert(a_name##_booted);\t\t\t\t\t\\\n-    return (&a_name##_tls);\t\t\t\t\t\t\\\n+\t\t\t\t\t\t\t\t\t\\\n+\tassert(a_name##_booted);\t\t\t\t\t\\\n+\treturn (&a_name##_tls);\t\t\t\t\t\t\\\n }\t\t\t\t\t\t\t\t\t\\\n a_attr void\t\t\t\t\t\t\t\t\\\n a_name##_tsd_set(a_type *val)\t\t\t\t\t\t\\\n {\t\t\t\t\t\t\t\t\t\\\n-                                    \\\n-    assert(a_name##_booted);\t\t\t\t\t\\\n-    a_name##_tls = (*val);\t\t\t\t\t\t\\\n-    if (a_cleanup != malloc_tsd_no_cleanup) {\t\t\t\\\n-        if (pthread_setspecific(a_name##_tsd,\t\t\t\\\n-            (void *)(&a_name##_tls))) {\t\t\t\t\\\n-            malloc_write(\"<jemalloc>: Error\"\t\t\\\n-                \" setting TSD for \"#a_name\"\\n\");\t\t\\\n-            if (opt_abort)\t\t\t\t\t\\\n-                abort();\t\t\t\t\\\n-        }\t\t\t\t\t\t\t\\\n-    }\t\t\t\t\t\t\t\t\\\n+\t\t\t\t\t\t\t\t\t\\\n+\tassert(a_name##_booted);\t\t\t\t\t\\\n+\ta_name##_tls = (*val);\t\t\t\t\t\t\\\n+\tif (a_cleanup != malloc_tsd_no_cleanup) {\t\t\t\\\n+\t\tif (pthread_setspecific(a_name##_tsd,\t\t\t\\\n+\t\t    (void *)(&a_name##_tls))) {\t\t\t\t\\\n+\t\t\tmalloc_write(\"<jemalloc>: Error\"\t\t\\\n+\t\t\t    \" setting TSD for \"#a_name\"\\n\");\t\t\\\n+\t\t\tif (opt_abort)\t\t\t\t\t\\\n+\t\t\t\tabort();\t\t\t\t\\\n+\t\t}\t\t\t\t\t\t\t\\\n+\t}\t\t\t\t\t\t\t\t\\\n }\n #elif (defined(_WIN32))\n #define\tmalloc_tsd_funcs(a_attr, a_name, a_type, a_initializer,\t\t\\\n     a_cleanup)\t\t\t\t\t\t\t\t\\\n /* Data structure. */\t\t\t\t\t\t\t\\\n typedef struct {\t\t\t\t\t\t\t\\\n-    bool\tinitialized;\t\t\t\t\t\t\\\n-    a_type\tval;\t\t\t\t\t\t\t\\\n+\tbool\tinitialized;\t\t\t\t\t\t\\\n+\ta_type\tval;\t\t\t\t\t\t\t\\\n } a_name##_tsd_wrapper_t;\t\t\t\t\t\t\\\n /* Initialization/cleanup. */\t\t\t\t\t\t\\\n a_attr bool\t\t\t\t\t\t\t\t\\\n a_name##_tsd_cleanup_wrapper(void)\t\t\t\t\t\\\n {\t\t\t\t\t\t\t\t\t\\\n-    a_name##_tsd_wrapper_t *wrapper;\t\t\t\t\\\n-                                    \\\n-    wrapper = (a_name##_tsd_wrapper_t *) TlsGetValue(a_name##_tsd);\t\\\n-    if (wrapper == NULL)\t\t\t\t\t\t\\\n-        return (false);\t\t\t\t\t\t\\\n-    if (a_cleanup != malloc_tsd_no_cleanup &&\t\t\t\\\n-        wrapper->initialized) {\t\t\t\t\t\\\n-        a_type val = wrapper->val;\t\t\t\t\\\n-        a_type tsd_static_data = a_initializer;\t\t\t\\\n-        wrapper->initialized = false;\t\t\t\t\\\n-        wrapper->val = tsd_static_data;\t\t\t\t\\\n-        a_cleanup(&val);\t\t\t\t\t\\\n-        if (wrapper->initialized) {\t\t\t\t\\\n-            /* Trigger another cleanup round. */\t\t\\\n-            return (true);\t\t\t\t\t\\\n-        }\t\t\t\t\t\t\t\\\n-    }\t\t\t\t\t\t\t\t\\\n-    malloc_tsd_dalloc(wrapper);\t\t\t\t\t\\\n-    return (false);\t\t\t\t\t\t\t\\\n+\ta_name##_tsd_wrapper_t *wrapper;\t\t\t\t\\\n+\t\t\t\t\t\t\t\t\t\\\n+\twrapper = (a_name##_tsd_wrapper_t *) TlsGetValue(a_name##_tsd);\t\\\n+\tif (wrapper == NULL)\t\t\t\t\t\t\\\n+\t\treturn (false);\t\t\t\t\t\t\\\n+\tif (a_cleanup != malloc_tsd_no_cleanup &&\t\t\t\\\n+\t    wrapper->initialized) {\t\t\t\t\t\\\n+\t\ta_type val = wrapper->val;\t\t\t\t\\\n+\t\ta_type tsd_static_data = a_initializer;\t\t\t\\\n+\t\twrapper->initialized = false;\t\t\t\t\\\n+\t\twrapper->val = tsd_static_data;\t\t\t\t\\\n+\t\ta_cleanup(&val);\t\t\t\t\t\\\n+\t\tif (wrapper->initialized) {\t\t\t\t\\\n+\t\t\t/* Trigger another cleanup round. */\t\t\\\n+\t\t\treturn (true);\t\t\t\t\t\\\n+\t\t}\t\t\t\t\t\t\t\\\n+\t}\t\t\t\t\t\t\t\t\\\n+\tmalloc_tsd_dalloc(wrapper);\t\t\t\t\t\\\n+\treturn (false);\t\t\t\t\t\t\t\\\n }\t\t\t\t\t\t\t\t\t\\\n a_attr bool\t\t\t\t\t\t\t\t\\\n a_name##_tsd_boot(void)\t\t\t\t\t\t\t\\\n {\t\t\t\t\t\t\t\t\t\\\n-                                    \\\n-    a_name##_tsd = TlsAlloc();\t\t\t\t\t\\\n-    if (a_name##_tsd == TLS_OUT_OF_INDEXES)\t\t\t\t\\\n-        return (true);\t\t\t\t\t\t\\\n-    if (a_cleanup != malloc_tsd_no_cleanup) {\t\t\t\\\n-        malloc_tsd_cleanup_register(\t\t\t\t\\\n-            &a_name##_tsd_cleanup_wrapper);\t\t\t\\\n-    }\t\t\t\t\t\t\t\t\\\n-    a_name##_booted = true;\t\t\t\t\t\t\\\n-    return (false);\t\t\t\t\t\t\t\\\n+\t\t\t\t\t\t\t\t\t\\\n+\ta_name##_tsd = TlsAlloc();\t\t\t\t\t\\\n+\tif (a_name##_tsd == TLS_OUT_OF_INDEXES)\t\t\t\t\\\n+\t\treturn (true);\t\t\t\t\t\t\\\n+\tif (a_cleanup != malloc_tsd_no_cleanup) {\t\t\t\\\n+\t\tmalloc_tsd_cleanup_register(\t\t\t\t\\\n+\t\t    &a_name##_tsd_cleanup_wrapper);\t\t\t\\\n+\t}\t\t\t\t\t\t\t\t\\\n+\ta_name##_booted = true;\t\t\t\t\t\t\\\n+\treturn (false);\t\t\t\t\t\t\t\\\n }\t\t\t\t\t\t\t\t\t\\\n /* Get/set. */\t\t\t\t\t\t\t\t\\\n a_attr a_name##_tsd_wrapper_t *\t\t\t\t\t\t\\\n a_name##_tsd_get_wrapper(void)\t\t\t\t\t\t\\\n {\t\t\t\t\t\t\t\t\t\\\n-    a_name##_tsd_wrapper_t *wrapper = (a_name##_tsd_wrapper_t *)\t\\\n-        TlsGetValue(a_name##_tsd);\t\t\t\t\t\\\n-                                    \\\n-    if (wrapper == NULL) {\t\t\t\t\t\t\\\n-        wrapper = (a_name##_tsd_wrapper_t *)\t\t\t\\\n-            malloc_tsd_malloc(sizeof(a_name##_tsd_wrapper_t));\t\\\n-        if (wrapper == NULL) {\t\t\t\t\t\\\n-            malloc_write(\"<jemalloc>: Error allocating\"\t\\\n-                \" TSD for \"#a_name\"\\n\");\t\t\t\\\n-            abort();\t\t\t\t\t\\\n-        } else {\t\t\t\t\t\t\\\n-            static a_type tsd_static_data = a_initializer;\t\\\n-            wrapper->initialized = false;\t\t\t\\\n-            wrapper->val = tsd_static_data;\t\t\t\\\n-        }\t\t\t\t\t\t\t\\\n-        if (!TlsSetValue(a_name##_tsd, (void *)wrapper)) {\t\\\n-            malloc_write(\"<jemalloc>: Error setting\"\t\\\n-                \" TSD for \"#a_name\"\\n\");\t\t\t\\\n-            abort();\t\t\t\t\t\\\n-        }\t\t\t\t\t\t\t\\\n-    }\t\t\t\t\t\t\t\t\\\n-    return (wrapper);\t\t\t\t\t\t\\\n+\ta_name##_tsd_wrapper_t *wrapper = (a_name##_tsd_wrapper_t *)\t\\\n+\t    TlsGetValue(a_name##_tsd);\t\t\t\t\t\\\n+\t\t\t\t\t\t\t\t\t\\\n+\tif (wrapper == NULL) {\t\t\t\t\t\t\\\n+\t\twrapper = (a_name##_tsd_wrapper_t *)\t\t\t\\\n+\t\t    malloc_tsd_malloc(sizeof(a_name##_tsd_wrapper_t));\t\\\n+\t\tif (wrapper == NULL) {\t\t\t\t\t\\\n+\t\t\tmalloc_write(\"<jemalloc>: Error allocating\"\t\\\n+\t\t\t    \" TSD for \"#a_name\"\\n\");\t\t\t\\\n+\t\t\tabort();\t\t\t\t\t\\\n+\t\t} else {\t\t\t\t\t\t\\\n+\t\t\tstatic a_type tsd_static_data = a_initializer;\t\\\n+\t\t\twrapper->initialized = false;\t\t\t\\\n+\t\t\twrapper->val = tsd_static_data;\t\t\t\\\n+\t\t}\t\t\t\t\t\t\t\\\n+\t\tif (!TlsSetValue(a_name##_tsd, (void *)wrapper)) {\t\\\n+\t\t\tmalloc_write(\"<jemalloc>: Error setting\"\t\\\n+\t\t\t    \" TSD for \"#a_name\"\\n\");\t\t\t\\\n+\t\t\tabort();\t\t\t\t\t\\\n+\t\t}\t\t\t\t\t\t\t\\\n+\t}\t\t\t\t\t\t\t\t\\\n+\treturn (wrapper);\t\t\t\t\t\t\\\n }\t\t\t\t\t\t\t\t\t\\\n a_attr a_type *\t\t\t\t\t\t\t\t\\\n a_name##_tsd_get(void)\t\t\t\t\t\t\t\\\n {\t\t\t\t\t\t\t\t\t\\\n-    a_name##_tsd_wrapper_t *wrapper;\t\t\t\t\\\n-                                    \\\n-    assert(a_name##_booted);\t\t\t\t\t\\\n-    wrapper = a_name##_tsd_get_wrapper();\t\t\t\t\\\n-    return (&wrapper->val);\t\t\t\t\t\t\\\n+\ta_name##_tsd_wrapper_t *wrapper;\t\t\t\t\\\n+\t\t\t\t\t\t\t\t\t\\\n+\tassert(a_name##_booted);\t\t\t\t\t\\\n+\twrapper = a_name##_tsd_get_wrapper();\t\t\t\t\\\n+\treturn (&wrapper->val);\t\t\t\t\t\t\\\n }\t\t\t\t\t\t\t\t\t\\\n a_attr void\t\t\t\t\t\t\t\t\\\n a_name##_tsd_set(a_type *val)\t\t\t\t\t\t\\\n {\t\t\t\t\t\t\t\t\t\\\n-    a_name##_tsd_wrapper_t *wrapper;\t\t\t\t\\\n-                                    \\\n-    assert(a_name##_booted);\t\t\t\t\t\\\n-    wrapper = a_name##_tsd_get_wrapper();\t\t\t\t\\\n-    wrapper->val = *(val);\t\t\t\t\t\t\\\n-    if (a_cleanup != malloc_tsd_no_cleanup)\t\t\t\t\\\n-        wrapper->initialized = true;\t\t\t\t\\\n+\ta_name##_tsd_wrapper_t *wrapper;\t\t\t\t\\\n+\t\t\t\t\t\t\t\t\t\\\n+\tassert(a_name##_booted);\t\t\t\t\t\\\n+\twrapper = a_name##_tsd_get_wrapper();\t\t\t\t\\\n+\twrapper->val = *(val);\t\t\t\t\t\t\\\n+\tif (a_cleanup != malloc_tsd_no_cleanup)\t\t\t\t\\\n+\t\twrapper->initialized = true;\t\t\t\t\\\n }\n #else\n #define\tmalloc_tsd_funcs(a_attr, a_name, a_type, a_initializer,\t\t\\\n     a_cleanup)\t\t\t\t\t\t\t\t\\\n /* Data structure. */\t\t\t\t\t\t\t\\\n typedef struct {\t\t\t\t\t\t\t\\\n-    bool\tinitialized;\t\t\t\t\t\t\\\n-    a_type\tval;\t\t\t\t\t\t\t\\\n+\tbool\tinitialized;\t\t\t\t\t\t\\\n+\ta_type\tval;\t\t\t\t\t\t\t\\\n } a_name##_tsd_wrapper_t;\t\t\t\t\t\t\\\n /* Initialization/cleanup. */\t\t\t\t\t\t\\\n a_attr void\t\t\t\t\t\t\t\t\\\n a_name##_tsd_cleanup_wrapper(void *arg)\t\t\t\t\t\\\n {\t\t\t\t\t\t\t\t\t\\\n-    a_name##_tsd_wrapper_t *wrapper = (a_name##_tsd_wrapper_t *)arg;\\\n-                                    \\\n-    if (a_cleanup != malloc_tsd_no_cleanup &&\t\t\t\\\n-        wrapper->initialized) {\t\t\t\t\t\\\n-        wrapper->initialized = false;\t\t\t\t\\\n-        a_cleanup(&wrapper->val);\t\t\t\t\\\n-        if (wrapper->initialized) {\t\t\t\t\\\n-            /* Trigger another cleanup round. */\t\t\\\n-            if (pthread_setspecific(a_name##_tsd,\t\t\\\n-                (void *)wrapper)) {\t\t\t\t\\\n-                malloc_write(\"<jemalloc>: Error\"\t\\\n-                    \" setting TSD for \"#a_name\"\\n\");\t\\\n-                if (opt_abort)\t\t\t\t\\\n-                    abort();\t\t\t\\\n-            }\t\t\t\t\t\t\\\n-            return;\t\t\t\t\t\t\\\n-        }\t\t\t\t\t\t\t\\\n-    }\t\t\t\t\t\t\t\t\\\n-    malloc_tsd_dalloc(wrapper);\t\t\t\t\t\\\n+\ta_name##_tsd_wrapper_t *wrapper = (a_name##_tsd_wrapper_t *)arg;\\\n+\t\t\t\t\t\t\t\t\t\\\n+\tif (a_cleanup != malloc_tsd_no_cleanup &&\t\t\t\\\n+\t    wrapper->initialized) {\t\t\t\t\t\\\n+\t\twrapper->initialized = false;\t\t\t\t\\\n+\t\ta_cleanup(&wrapper->val);\t\t\t\t\\\n+\t\tif (wrapper->initialized) {\t\t\t\t\\\n+\t\t\t/* Trigger another cleanup round. */\t\t\\\n+\t\t\tif (pthread_setspecific(a_name##_tsd,\t\t\\\n+\t\t\t    (void *)wrapper)) {\t\t\t\t\\\n+\t\t\t\tmalloc_write(\"<jemalloc>: Error\"\t\\\n+\t\t\t\t    \" setting TSD for \"#a_name\"\\n\");\t\\\n+\t\t\t\tif (opt_abort)\t\t\t\t\\\n+\t\t\t\t\tabort();\t\t\t\\\n+\t\t\t}\t\t\t\t\t\t\\\n+\t\t\treturn;\t\t\t\t\t\t\\\n+\t\t}\t\t\t\t\t\t\t\\\n+\t}\t\t\t\t\t\t\t\t\\\n+\tmalloc_tsd_dalloc(wrapper);\t\t\t\t\t\\\n }\t\t\t\t\t\t\t\t\t\\\n a_attr bool\t\t\t\t\t\t\t\t\\\n a_name##_tsd_boot(void)\t\t\t\t\t\t\t\\\n {\t\t\t\t\t\t\t\t\t\\\n-                                    \\\n-    if (pthread_key_create(&a_name##_tsd,\t\t\t\t\\\n-        a_name##_tsd_cleanup_wrapper) != 0)\t\t\t\t\\\n-        return (true);\t\t\t\t\t\t\\\n-    a_name##_booted = true;\t\t\t\t\t\t\\\n-    return (false);\t\t\t\t\t\t\t\\\n+\t\t\t\t\t\t\t\t\t\\\n+\tif (pthread_key_create(&a_name##_tsd,\t\t\t\t\\\n+\t    a_name##_tsd_cleanup_wrapper) != 0)\t\t\t\t\\\n+\t\treturn (true);\t\t\t\t\t\t\\\n+\ta_name##_booted = true;\t\t\t\t\t\t\\\n+\treturn (false);\t\t\t\t\t\t\t\\\n }\t\t\t\t\t\t\t\t\t\\\n /* Get/set. */\t\t\t\t\t\t\t\t\\\n a_attr a_name##_tsd_wrapper_t *\t\t\t\t\t\t\\\n a_name##_tsd_get_wrapper(void)\t\t\t\t\t\t\\\n {\t\t\t\t\t\t\t\t\t\\\n-    a_name##_tsd_wrapper_t *wrapper = (a_name##_tsd_wrapper_t *)\t\\\n-        pthread_getspecific(a_name##_tsd);\t\t\t\t\\\n-                                    \\\n-    if (wrapper == NULL) {\t\t\t\t\t\t\\\n-        wrapper = (a_name##_tsd_wrapper_t *)\t\t\t\\\n-            malloc_tsd_malloc(sizeof(a_name##_tsd_wrapper_t));\t\\\n-        if (wrapper == NULL) {\t\t\t\t\t\\\n-            malloc_write(\"<jemalloc>: Error allocating\"\t\\\n-                \" TSD for \"#a_name\"\\n\");\t\t\t\\\n-            abort();\t\t\t\t\t\\\n-        } else {\t\t\t\t\t\t\\\n-            static a_type tsd_static_data = a_initializer;\t\\\n-            wrapper->initialized = false;\t\t\t\\\n-            wrapper->val = tsd_static_data;\t\t\t\\\n-        }\t\t\t\t\t\t\t\\\n-        if (pthread_setspecific(a_name##_tsd,\t\t\t\\\n-            (void *)wrapper)) {\t\t\t\t\t\\\n-            malloc_write(\"<jemalloc>: Error setting\"\t\\\n-                \" TSD for \"#a_name\"\\n\");\t\t\t\\\n-            abort();\t\t\t\t\t\\\n-        }\t\t\t\t\t\t\t\\\n-    }\t\t\t\t\t\t\t\t\\\n-    return (wrapper);\t\t\t\t\t\t\\\n+\ta_name##_tsd_wrapper_t *wrapper = (a_name##_tsd_wrapper_t *)\t\\\n+\t    pthread_getspecific(a_name##_tsd);\t\t\t\t\\\n+\t\t\t\t\t\t\t\t\t\\\n+\tif (wrapper == NULL) {\t\t\t\t\t\t\\\n+\t\twrapper = (a_name##_tsd_wrapper_t *)\t\t\t\\\n+\t\t    malloc_tsd_malloc(sizeof(a_name##_tsd_wrapper_t));\t\\\n+\t\tif (wrapper == NULL) {\t\t\t\t\t\\\n+\t\t\tmalloc_write(\"<jemalloc>: Error allocating\"\t\\\n+\t\t\t    \" TSD for \"#a_name\"\\n\");\t\t\t\\\n+\t\t\tabort();\t\t\t\t\t\\\n+\t\t} else {\t\t\t\t\t\t\\\n+\t\t\tstatic a_type tsd_static_data = a_initializer;\t\\\n+\t\t\twrapper->initialized = false;\t\t\t\\\n+\t\t\twrapper->val = tsd_static_data;\t\t\t\\\n+\t\t}\t\t\t\t\t\t\t\\\n+\t\tif (pthread_setspecific(a_name##_tsd,\t\t\t\\\n+\t\t    (void *)wrapper)) {\t\t\t\t\t\\\n+\t\t\tmalloc_write(\"<jemalloc>: Error setting\"\t\\\n+\t\t\t    \" TSD for \"#a_name\"\\n\");\t\t\t\\\n+\t\t\tabort();\t\t\t\t\t\\\n+\t\t}\t\t\t\t\t\t\t\\\n+\t}\t\t\t\t\t\t\t\t\\\n+\treturn (wrapper);\t\t\t\t\t\t\\\n }\t\t\t\t\t\t\t\t\t\\\n a_attr a_type *\t\t\t\t\t\t\t\t\\\n a_name##_tsd_get(void)\t\t\t\t\t\t\t\\\n {\t\t\t\t\t\t\t\t\t\\\n-    a_name##_tsd_wrapper_t *wrapper;\t\t\t\t\\\n-                                    \\\n-    assert(a_name##_booted);\t\t\t\t\t\\\n-    wrapper = a_name##_tsd_get_wrapper();\t\t\t\t\\\n-    return (&wrapper->val);\t\t\t\t\t\t\\\n+\ta_name##_tsd_wrapper_t *wrapper;\t\t\t\t\\\n+\t\t\t\t\t\t\t\t\t\\\n+\tassert(a_name##_booted);\t\t\t\t\t\\\n+\twrapper = a_name##_tsd_get_wrapper();\t\t\t\t\\\n+\treturn (&wrapper->val);\t\t\t\t\t\t\\\n }\t\t\t\t\t\t\t\t\t\\\n a_attr void\t\t\t\t\t\t\t\t\\\n a_name##_tsd_set(a_type *val)\t\t\t\t\t\t\\\n {\t\t\t\t\t\t\t\t\t\\\n-    a_name##_tsd_wrapper_t *wrapper;\t\t\t\t\\\n-                                    \\\n-    assert(a_name##_booted);\t\t\t\t\t\\\n-    wrapper = a_name##_tsd_get_wrapper();\t\t\t\t\\\n-    wrapper->val = *(val);\t\t\t\t\t\t\\\n-    if (a_cleanup != malloc_tsd_no_cleanup)\t\t\t\t\\\n-        wrapper->initialized = true;\t\t\t\t\\\n+\ta_name##_tsd_wrapper_t *wrapper;\t\t\t\t\\\n+\t\t\t\t\t\t\t\t\t\\\n+\tassert(a_name##_booted);\t\t\t\t\t\\\n+\twrapper = a_name##_tsd_get_wrapper();\t\t\t\t\\\n+\twrapper->val = *(val);\t\t\t\t\t\t\\\n+\tif (a_cleanup != malloc_tsd_no_cleanup)\t\t\t\t\\\n+\t\twrapper->initialized = true;\t\t\t\t\\\n }\n #endif\n "}, {"sha": "8479693631abde298d91a1e6cf5f1ed2b0a449e2", "filename": "src/rt/jemalloc/include/jemalloc/internal/util.h", "status": "modified", "additions": 34, "deletions": 34, "changes": 68, "blob_url": "https://github.com/rust-lang/rust/blob/996301331e2d8ac7611d15ca82cc3ebc1abb632d/src%2Frt%2Fjemalloc%2Finclude%2Fjemalloc%2Finternal%2Futil.h", "raw_url": "https://github.com/rust-lang/rust/raw/996301331e2d8ac7611d15ca82cc3ebc1abb632d/src%2Frt%2Fjemalloc%2Finclude%2Fjemalloc%2Finternal%2Futil.h", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Frt%2Fjemalloc%2Finclude%2Fjemalloc%2Finternal%2Futil.h?ref=996301331e2d8ac7611d15ca82cc3ebc1abb632d", "patch": "@@ -33,45 +33,45 @@\n  */\n #ifndef assert\n #define\tassert(e) do {\t\t\t\t\t\t\t\\\n-    if (config_debug && !(e)) {\t\t\t\t\t\\\n-        malloc_printf(\t\t\t\t\t\t\\\n-            \"<jemalloc>: %s:%d: Failed assertion: \\\"%s\\\"\\n\",\t\\\n-            __FILE__, __LINE__, #e);\t\t\t\t\\\n-        abort();\t\t\t\t\t\t\\\n-    }\t\t\t\t\t\t\t\t\\\n+\tif (config_debug && !(e)) {\t\t\t\t\t\\\n+\t\tmalloc_printf(\t\t\t\t\t\t\\\n+\t\t    \"<jemalloc>: %s:%d: Failed assertion: \\\"%s\\\"\\n\",\t\\\n+\t\t    __FILE__, __LINE__, #e);\t\t\t\t\\\n+\t\tabort();\t\t\t\t\t\t\\\n+\t}\t\t\t\t\t\t\t\t\\\n } while (0)\n #endif\n \n /* Use to assert a particular configuration, e.g., cassert(config_debug). */\n #define\tcassert(c) do {\t\t\t\t\t\t\t\\\n-    if ((c) == false)\t\t\t\t\t\t\\\n-        assert(false);\t\t\t\t\t\t\\\n+\tif ((c) == false)\t\t\t\t\t\t\\\n+\t\tassert(false);\t\t\t\t\t\t\\\n } while (0)\n \n #ifndef not_reached\n #define\tnot_reached() do {\t\t\t\t\t\t\\\n-    if (config_debug) {\t\t\t\t\t\t\\\n-        malloc_printf(\t\t\t\t\t\t\\\n-            \"<jemalloc>: %s:%d: Unreachable code reached\\n\",\t\\\n-            __FILE__, __LINE__);\t\t\t\t\\\n-        abort();\t\t\t\t\t\t\\\n-    }\t\t\t\t\t\t\t\t\\\n+\tif (config_debug) {\t\t\t\t\t\t\\\n+\t\tmalloc_printf(\t\t\t\t\t\t\\\n+\t\t    \"<jemalloc>: %s:%d: Unreachable code reached\\n\",\t\\\n+\t\t    __FILE__, __LINE__);\t\t\t\t\\\n+\t\tabort();\t\t\t\t\t\t\\\n+\t}\t\t\t\t\t\t\t\t\\\n } while (0)\n #endif\n \n #ifndef not_implemented\n #define\tnot_implemented() do {\t\t\t\t\t\t\\\n-    if (config_debug) {\t\t\t\t\t\t\\\n-        malloc_printf(\"<jemalloc>: %s:%d: Not implemented\\n\",\t\\\n-            __FILE__, __LINE__);\t\t\t\t\\\n-        abort();\t\t\t\t\t\t\\\n-    }\t\t\t\t\t\t\t\t\\\n+\tif (config_debug) {\t\t\t\t\t\t\\\n+\t\tmalloc_printf(\"<jemalloc>: %s:%d: Not implemented\\n\",\t\\\n+\t\t    __FILE__, __LINE__);\t\t\t\t\\\n+\t\tabort();\t\t\t\t\t\t\\\n+\t}\t\t\t\t\t\t\t\t\\\n } while (0)\n #endif\n \n #define\tassert_not_implemented(e) do {\t\t\t\t\t\\\n-    if (config_debug && !(e))\t\t\t\t\t\\\n-        not_implemented();\t\t\t\t\t\\\n+\tif (config_debug && !(e))\t\t\t\t\t\\\n+\t\tnot_implemented();\t\t\t\t\t\\\n } while (0)\n \n #endif /* JEMALLOC_H_TYPES */\n@@ -118,17 +118,17 @@ JEMALLOC_INLINE size_t\n pow2_ceil(size_t x)\n {\n \n-    x--;\n-    x |= x >> 1;\n-    x |= x >> 2;\n-    x |= x >> 4;\n-    x |= x >> 8;\n-    x |= x >> 16;\n+\tx--;\n+\tx |= x >> 1;\n+\tx |= x >> 2;\n+\tx |= x >> 4;\n+\tx |= x >> 8;\n+\tx |= x >> 16;\n #if (LG_SIZEOF_PTR == 3)\n-    x |= x >> 32;\n+\tx |= x >> 32;\n #endif\n-    x++;\n-    return (x);\n+\tx++;\n+\treturn (x);\n }\n \n /* Sets error code */\n@@ -137,9 +137,9 @@ set_errno(int errnum)\n {\n \n #ifdef _WIN32\n-    SetLastError(errnum);\n+\tSetLastError(errnum);\n #else\n-    errno = errnum;\n+\terrno = errnum;\n #endif\n }\n \n@@ -149,9 +149,9 @@ get_errno(void)\n {\n \n #ifdef _WIN32\n-    return (GetLastError());\n+\treturn (GetLastError());\n #else\n-    return (errno);\n+\treturn (errno);\n #endif\n }\n #endif"}, {"sha": "a4e6b75cb9146411972426ebf530a21cf4dccb15", "filename": "src/rt/jemalloc/include/msvc_compat/inttypes.h", "status": "modified", "additions": 9, "deletions": 9, "changes": 18, "blob_url": "https://github.com/rust-lang/rust/blob/996301331e2d8ac7611d15ca82cc3ebc1abb632d/src%2Frt%2Fjemalloc%2Finclude%2Fmsvc_compat%2Finttypes.h", "raw_url": "https://github.com/rust-lang/rust/raw/996301331e2d8ac7611d15ca82cc3ebc1abb632d/src%2Frt%2Fjemalloc%2Finclude%2Fmsvc_compat%2Finttypes.h", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Frt%2Fjemalloc%2Finclude%2Fmsvc_compat%2Finttypes.h?ref=996301331e2d8ac7611d15ca82cc3ebc1abb632d", "patch": "@@ -1,32 +1,32 @@\n // ISO C9x  compliant inttypes.h for Microsoft Visual Studio\n-// Based on ISO/IEC 9899:TC2 Committee draft (May 6, 2005) WG14/N1124\n-//\n+// Based on ISO/IEC 9899:TC2 Committee draft (May 6, 2005) WG14/N1124 \n+// \n //  Copyright (c) 2006 Alexander Chemeris\n-//\n+// \n // Redistribution and use in source and binary forms, with or without\n // modification, are permitted provided that the following conditions are met:\n-//\n+// \n //   1. Redistributions of source code must retain the above copyright notice,\n //      this list of conditions and the following disclaimer.\n-//\n+// \n //   2. Redistributions in binary form must reproduce the above copyright\n //      notice, this list of conditions and the following disclaimer in the\n //      documentation and/or other materials provided with the distribution.\n-//\n+// \n //   3. The name of the author may be used to endorse or promote products\n //      derived from this software without specific prior written permission.\n-//\n+// \n // THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR IMPLIED\n // WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF\n // MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO\n // EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\n // SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n // PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n-// OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n+// OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, \n // WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n // OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF\n // ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n-//\n+// \n ///////////////////////////////////////////////////////////////////////////////\n \n #ifndef _MSC_VER // ["}, {"sha": "d02608a5972642c7b7a13b987f21e2502a5af3ea", "filename": "src/rt/jemalloc/include/msvc_compat/stdint.h", "status": "modified", "additions": 9, "deletions": 9, "changes": 18, "blob_url": "https://github.com/rust-lang/rust/blob/996301331e2d8ac7611d15ca82cc3ebc1abb632d/src%2Frt%2Fjemalloc%2Finclude%2Fmsvc_compat%2Fstdint.h", "raw_url": "https://github.com/rust-lang/rust/raw/996301331e2d8ac7611d15ca82cc3ebc1abb632d/src%2Frt%2Fjemalloc%2Finclude%2Fmsvc_compat%2Fstdint.h", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Frt%2Fjemalloc%2Finclude%2Fmsvc_compat%2Fstdint.h?ref=996301331e2d8ac7611d15ca82cc3ebc1abb632d", "patch": "@@ -1,32 +1,32 @@\n // ISO C9x  compliant stdint.h for Microsoft Visual Studio\n-// Based on ISO/IEC 9899:TC2 Committee draft (May 6, 2005) WG14/N1124\n-//\n+// Based on ISO/IEC 9899:TC2 Committee draft (May 6, 2005) WG14/N1124 \n+// \n //  Copyright (c) 2006-2008 Alexander Chemeris\n-//\n+// \n // Redistribution and use in source and binary forms, with or without\n // modification, are permitted provided that the following conditions are met:\n-//\n+// \n //   1. Redistributions of source code must retain the above copyright notice,\n //      this list of conditions and the following disclaimer.\n-//\n+// \n //   2. Redistributions in binary form must reproduce the above copyright\n //      notice, this list of conditions and the following disclaimer in the\n //      documentation and/or other materials provided with the distribution.\n-//\n+// \n //   3. The name of the author may be used to endorse or promote products\n //      derived from this software without specific prior written permission.\n-//\n+// \n // THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR IMPLIED\n // WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF\n // MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO\n // EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\n // SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n // PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n-// OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n+// OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, \n // WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n // OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF\n // ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n-//\n+// \n ///////////////////////////////////////////////////////////////////////////////\n \n #ifndef _MSC_VER // ["}, {"sha": "c84975b6b8e1d682133614705f077f863ffef0a1", "filename": "src/rt/jemalloc/include/msvc_compat/strings.h", "status": "modified", "additions": 5, "deletions": 5, "changes": 10, "blob_url": "https://github.com/rust-lang/rust/blob/996301331e2d8ac7611d15ca82cc3ebc1abb632d/src%2Frt%2Fjemalloc%2Finclude%2Fmsvc_compat%2Fstrings.h", "raw_url": "https://github.com/rust-lang/rust/raw/996301331e2d8ac7611d15ca82cc3ebc1abb632d/src%2Frt%2Fjemalloc%2Finclude%2Fmsvc_compat%2Fstrings.h", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Frt%2Fjemalloc%2Finclude%2Fmsvc_compat%2Fstrings.h?ref=996301331e2d8ac7611d15ca82cc3ebc1abb632d", "patch": "@@ -7,17 +7,17 @@\n #pragma intrinsic(_BitScanForward)\n static __forceinline int ffsl(long x)\n {\n-    unsigned long i;\n+\tunsigned long i;\n \n-    if (_BitScanForward(&i, x))\n-        return (i + 1);\n-    return (0);\n+\tif (_BitScanForward(&i, x))\n+\t\treturn (i + 1);\n+\treturn (0);\n }\n \n static __forceinline int ffs(int x)\n {\n \n-    return (ffsl(x));\n+\treturn (ffsl(x));\n }\n \n #endif"}, {"sha": "ebc66913e94018bd82ce82789525e7f67ff340b8", "filename": "src/rt/jemalloc/install-sh", "status": "modified", "additions": 8, "deletions": 8, "changes": 16, "blob_url": "https://github.com/rust-lang/rust/blob/996301331e2d8ac7611d15ca82cc3ebc1abb632d/src%2Frt%2Fjemalloc%2Finstall-sh", "raw_url": "https://github.com/rust-lang/rust/raw/996301331e2d8ac7611d15ca82cc3ebc1abb632d/src%2Frt%2Fjemalloc%2Finstall-sh", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Frt%2Fjemalloc%2Finstall-sh?ref=996301331e2d8ac7611d15ca82cc3ebc1abb632d", "patch": "@@ -115,7 +115,7 @@ fi\n if [ x\"$dir_arg\" != x ]; then\n \tdst=$src\n \tsrc=\"\"\n-\n+\t\n \tif [ -d $dst ]; then\n \t\tinstcmd=:\n \telse\n@@ -124,7 +124,7 @@ if [ x\"$dir_arg\" != x ]; then\n else\n \n # Waiting for this to be detected by the \"$instcmd $src $dsttmp\" command\n-# might cause directories to be created, which would be especially bad\n+# might cause directories to be created, which would be especially bad \n # if $src (and thus $dsttmp) contains '*'.\n \n \tif [ -f $src -o -d $src ]\n@@ -134,7 +134,7 @@ else\n \t\techo \"install:  $src does not exist\"\n \t\texit 1\n \tfi\n-\n+\t\n \tif [ x\"$dst\" = x ]\n \tthen\n \t\techo \"install:\tno destination specified\"\n@@ -162,7 +162,7 @@ dstdir=`echo $dst | sed -e 's,[^/]*$,,;s,/$,,;s,^$,.,'`\n \n # Skip lots of stat calls in the usual case.\n if [ ! -d \"$dstdir\" ]; then\n-defaultIFS='\n+defaultIFS='\t\n '\n IFS=\"${IFS-${defaultIFS}}\"\n \n@@ -201,17 +201,17 @@ else\n \n # If we're going to rename the final executable, determine the name now.\n \n-\tif [ x\"$transformarg\" = x ]\n+\tif [ x\"$transformarg\" = x ] \n \tthen\n \t\tdstfile=`basename $dst`\n \telse\n-\t\tdstfile=`basename $dst $transformbasename |\n+\t\tdstfile=`basename $dst $transformbasename | \n \t\t\tsed $transformarg`$transformbasename\n \tfi\n \n # don't allow the sed command to completely eliminate the filename\n \n-\tif [ x\"$dstfile\" = x ]\n+\tif [ x\"$dstfile\" = x ] \n \tthen\n \t\tdstfile=`basename $dst`\n \telse\n@@ -242,7 +242,7 @@ else\n # Now rename the file to the real destination.\n \n \t$doit $rmcmd -f $dstdir/$dstfile &&\n-\t$doit $mvcmd $dsttmp $dstdir/$dstfile\n+\t$doit $mvcmd $dsttmp $dstdir/$dstfile \n \n fi &&\n "}, {"sha": "aef3fede6dceef410b1962b405c76c8bf1e1b36a", "filename": "src/rt/jemalloc/src/chunk.c", "status": "modified", "additions": 16, "deletions": 6, "changes": 22, "blob_url": "https://github.com/rust-lang/rust/blob/996301331e2d8ac7611d15ca82cc3ebc1abb632d/src%2Frt%2Fjemalloc%2Fsrc%2Fchunk.c", "raw_url": "https://github.com/rust-lang/rust/raw/996301331e2d8ac7611d15ca82cc3ebc1abb632d/src%2Frt%2Fjemalloc%2Fsrc%2Fchunk.c", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Frt%2Fjemalloc%2Fsrc%2Fchunk.c?ref=996301331e2d8ac7611d15ca82cc3ebc1abb632d", "patch": "@@ -214,7 +214,7 @@ chunk_record(extent_tree_t *chunks_szad, extent_tree_t *chunks_ad, void *chunk,\n     size_t size)\n {\n \tbool unzeroed;\n-\textent_node_t *xnode, *node, *prev, key;\n+\textent_node_t *xnode, *node, *prev, *xprev, key;\n \n \tunzeroed = pages_purge(chunk, size);\n \tVALGRIND_MAKE_MEM_NOACCESS(chunk, size);\n@@ -226,6 +226,8 @@ chunk_record(extent_tree_t *chunks_szad, extent_tree_t *chunks_ad, void *chunk,\n \t * held.\n \t */\n \txnode = base_node_alloc();\n+\t/* Use xprev to implement conditional deferred deallocation of prev. */\n+\txprev = NULL;\n \n \tmalloc_mutex_lock(&chunks_mtx);\n \tkey.addr = (void *)((uintptr_t)chunk + size);\n@@ -242,8 +244,6 @@ chunk_record(extent_tree_t *chunks_szad, extent_tree_t *chunks_ad, void *chunk,\n \t\tnode->size += size;\n \t\tnode->zeroed = (node->zeroed && (unzeroed == false));\n \t\textent_tree_szad_insert(chunks_szad, node);\n-\t\tif (xnode != NULL)\n-\t\t\tbase_node_dealloc(xnode);\n \t} else {\n \t\t/* Coalescing forward failed, so insert a new node. */\n \t\tif (xnode == NULL) {\n@@ -253,10 +253,10 @@ chunk_record(extent_tree_t *chunks_szad, extent_tree_t *chunks_ad, void *chunk,\n \t\t\t * already been purged, so this is only a virtual\n \t\t\t * memory leak.\n \t\t\t */\n-\t\t\tmalloc_mutex_unlock(&chunks_mtx);\n-\t\t\treturn;\n+\t\t\tgoto label_return;\n \t\t}\n \t\tnode = xnode;\n+\t\txnode = NULL; /* Prevent deallocation below. */\n \t\tnode->addr = chunk;\n \t\tnode->size = size;\n \t\tnode->zeroed = (unzeroed == false);\n@@ -282,9 +282,19 @@ chunk_record(extent_tree_t *chunks_szad, extent_tree_t *chunks_ad, void *chunk,\n \t\tnode->zeroed = (node->zeroed && prev->zeroed);\n \t\textent_tree_szad_insert(chunks_szad, node);\n \n-\t\tbase_node_dealloc(prev);\n+\t\txprev = prev;\n \t}\n+\n+label_return:\n \tmalloc_mutex_unlock(&chunks_mtx);\n+\t/*\n+\t * Deallocate xnode and/or xprev after unlocking chunks_mtx in order to\n+\t * avoid potential deadlock.\n+\t */\n+\tif (xnode != NULL)\n+\t\tbase_node_dealloc(xnode);\n+\tif (xprev != NULL)\n+\t\tbase_node_dealloc(prev);\n }\n \n void"}]}