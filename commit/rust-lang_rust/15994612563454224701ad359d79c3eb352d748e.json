{"sha": "15994612563454224701ad359d79c3eb352d748e", "node_id": "MDY6Q29tbWl0NzI0NzEyOjE1OTk0NjEyNTYzNDU0MjI0NzAxYWQzNTlkNzljM2ViMzUyZDc0OGU=", "commit": {"author": {"name": "Jeffrey Seyfried", "email": "jeffrey.seyfried@gmail.com", "date": "2016-09-20T20:29:13Z"}, "committer": {"name": "Jeffrey Seyfried", "email": "jeffrey.seyfried@gmail.com", "date": "2016-09-24T01:06:53Z"}, "message": "Refactor `ext::tt::macro_rules::compile` to take a `ParseSess` instead of an `ExtCtxt`.", "tree": {"sha": "0718ce3a829b910d880861dd0392fc5286a57be1", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/0718ce3a829b910d880861dd0392fc5286a57be1"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/15994612563454224701ad359d79c3eb352d748e", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/15994612563454224701ad359d79c3eb352d748e", "html_url": "https://github.com/rust-lang/rust/commit/15994612563454224701ad359d79c3eb352d748e", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/15994612563454224701ad359d79c3eb352d748e/comments", "author": {"login": "jseyfried", "id": 8652869, "node_id": "MDQ6VXNlcjg2NTI4Njk=", "avatar_url": "https://avatars.githubusercontent.com/u/8652869?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jseyfried", "html_url": "https://github.com/jseyfried", "followers_url": "https://api.github.com/users/jseyfried/followers", "following_url": "https://api.github.com/users/jseyfried/following{/other_user}", "gists_url": "https://api.github.com/users/jseyfried/gists{/gist_id}", "starred_url": "https://api.github.com/users/jseyfried/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jseyfried/subscriptions", "organizations_url": "https://api.github.com/users/jseyfried/orgs", "repos_url": "https://api.github.com/users/jseyfried/repos", "events_url": "https://api.github.com/users/jseyfried/events{/privacy}", "received_events_url": "https://api.github.com/users/jseyfried/received_events", "type": "User", "site_admin": false}, "committer": {"login": "jseyfried", "id": 8652869, "node_id": "MDQ6VXNlcjg2NTI4Njk=", "avatar_url": "https://avatars.githubusercontent.com/u/8652869?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jseyfried", "html_url": "https://github.com/jseyfried", "followers_url": "https://api.github.com/users/jseyfried/followers", "following_url": "https://api.github.com/users/jseyfried/following{/other_user}", "gists_url": "https://api.github.com/users/jseyfried/gists{/gist_id}", "starred_url": "https://api.github.com/users/jseyfried/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jseyfried/subscriptions", "organizations_url": "https://api.github.com/users/jseyfried/orgs", "repos_url": "https://api.github.com/users/jseyfried/repos", "events_url": "https://api.github.com/users/jseyfried/events{/privacy}", "received_events_url": "https://api.github.com/users/jseyfried/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "ee959a8cbe8d2931546248ac61f278e38ae8bc68", "url": "https://api.github.com/repos/rust-lang/rust/commits/ee959a8cbe8d2931546248ac61f278e38ae8bc68", "html_url": "https://github.com/rust-lang/rust/commit/ee959a8cbe8d2931546248ac61f278e38ae8bc68"}], "stats": {"total": 73, "additions": 33, "deletions": 40}, "files": [{"sha": "48a56e876d18c4e625968cacceb58936b689ed9f", "filename": "src/libsyntax/ext/base.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/15994612563454224701ad359d79c3eb352d748e/src%2Flibsyntax%2Fext%2Fbase.rs", "raw_url": "https://github.com/rust-lang/rust/raw/15994612563454224701ad359d79c3eb352d748e/src%2Flibsyntax%2Fext%2Fbase.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fext%2Fbase.rs?ref=15994612563454224701ad359d79c3eb352d748e", "patch": "@@ -816,7 +816,7 @@ impl<'a> ExtCtxt<'a> {\n             self.exported_macros.push(def.clone());\n         }\n         if def.use_locally {\n-            let ext = macro_rules::compile(self, &def);\n+            let ext = macro_rules::compile(self.parse_sess, &def);\n             self.resolver.add_macro(self.current_expansion.mark, def.ident, Rc::new(ext));\n         }\n     }"}, {"sha": "09edd39c06187c3f97d131412ac78e58cc2673bd", "filename": "src/libsyntax/ext/tt/macro_rules.rs", "status": "modified", "additions": 32, "deletions": 39, "changes": 71, "blob_url": "https://github.com/rust-lang/rust/blob/15994612563454224701ad359d79c3eb352d748e/src%2Flibsyntax%2Fext%2Ftt%2Fmacro_rules.rs", "raw_url": "https://github.com/rust-lang/rust/raw/15994612563454224701ad359d79c3eb352d748e/src%2Flibsyntax%2Fext%2Ftt%2Fmacro_rules.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fext%2Ftt%2Fmacro_rules.rs?ref=15994612563454224701ad359d79c3eb352d748e", "patch": "@@ -16,6 +16,7 @@ use ext::placeholders;\n use ext::tt::macro_parser::{Success, Error, Failure};\n use ext::tt::macro_parser::{MatchedSeq, MatchedNonterminal};\n use ext::tt::macro_parser::parse;\n+use parse::ParseSess;\n use parse::lexer::new_tt_reader;\n use parse::parser::{Parser, Restrictions};\n use parse::token::{self, gensym_ident, NtTT, Token};\n@@ -204,7 +205,7 @@ fn generic_extension<'cx>(cx: &'cx ExtCtxt,\n                     _ => cx.span_bug(sp, \"malformed macro rhs\"),\n                 };\n                 // rhs has holes ( `$id` and `$(...)` that need filled)\n-                let trncbr = new_tt_reader(&cx.parse_sess().span_diagnostic,\n+                let trncbr = new_tt_reader(&cx.parse_sess.span_diagnostic,\n                                            Some(named_matches),\n                                            imported_from,\n                                            rhs);\n@@ -278,9 +279,7 @@ impl IdentMacroExpander for MacroRulesExpander {\n // Holy self-referential!\n \n /// Converts a `macro_rules!` invocation into a syntax extension.\n-pub fn compile<'cx>(cx: &'cx mut ExtCtxt,\n-                    def: &ast::MacroDef) -> SyntaxExtension {\n-\n+pub fn compile(sess: &ParseSess, def: &ast::MacroDef) -> SyntaxExtension {\n     let lhs_nm =  gensym_ident(\"lhs\");\n     let rhs_nm =  gensym_ident(\"rhs\");\n \n@@ -312,19 +311,12 @@ pub fn compile<'cx>(cx: &'cx mut ExtCtxt,\n     ];\n \n     // Parse the macro_rules! invocation (`none` is for no interpolations):\n-    let arg_reader = new_tt_reader(&cx.parse_sess().span_diagnostic,\n-                                   None,\n-                                   None,\n-                                   def.body.clone());\n-\n-    let argument_map = match parse(cx.parse_sess(),\n-                                   cx.cfg(),\n-                                   arg_reader,\n-                                   &argument_gram) {\n+    let arg_reader = new_tt_reader(&sess.span_diagnostic, None, None, def.body.clone());\n+\n+    let argument_map = match parse(sess, Vec::new(), arg_reader, &argument_gram) {\n         Success(m) => m,\n         Failure(sp, str) | Error(sp, str) => {\n-            panic!(cx.parse_sess().span_diagnostic\n-                     .span_fatal(sp.substitute_dummy(def.span), &str[..]));\n+            panic!(sess.span_diagnostic.span_fatal(sp.substitute_dummy(def.span), &str));\n         }\n     };\n \n@@ -335,27 +327,27 @@ pub fn compile<'cx>(cx: &'cx mut ExtCtxt,\n         MatchedSeq(ref s, _) => {\n             s.iter().map(|m| match **m {\n                 MatchedNonterminal(NtTT(ref tt)) => {\n-                    valid &= check_lhs_nt_follows(cx, tt);\n+                    valid &= check_lhs_nt_follows(sess, tt);\n                     (**tt).clone()\n                 }\n-                _ => cx.span_bug(def.span, \"wrong-structured lhs\")\n+                _ => sess.span_diagnostic.span_bug(def.span, \"wrong-structured lhs\")\n             }).collect()\n         }\n-        _ => cx.span_bug(def.span, \"wrong-structured lhs\")\n+        _ => sess.span_diagnostic.span_bug(def.span, \"wrong-structured lhs\")\n     };\n \n     let rhses = match **argument_map.get(&rhs_nm).unwrap() {\n         MatchedSeq(ref s, _) => {\n             s.iter().map(|m| match **m {\n                 MatchedNonterminal(NtTT(ref tt)) => (**tt).clone(),\n-                _ => cx.span_bug(def.span, \"wrong-structured rhs\")\n+                _ => sess.span_diagnostic.span_bug(def.span, \"wrong-structured rhs\")\n             }).collect()\n         }\n-        _ => cx.span_bug(def.span, \"wrong-structured rhs\")\n+        _ => sess.span_diagnostic.span_bug(def.span, \"wrong-structured rhs\")\n     };\n \n     for rhs in &rhses {\n-        valid &= check_rhs(cx, rhs);\n+        valid &= check_rhs(sess, rhs);\n     }\n \n     let exp: Box<_> = Box::new(MacroRulesMacroExpander {\n@@ -369,35 +361,35 @@ pub fn compile<'cx>(cx: &'cx mut ExtCtxt,\n     NormalTT(exp, Some(def.span), def.allow_internal_unstable)\n }\n \n-fn check_lhs_nt_follows(cx: &mut ExtCtxt, lhs: &TokenTree) -> bool {\n+fn check_lhs_nt_follows(sess: &ParseSess, lhs: &TokenTree) -> bool {\n     // lhs is going to be like TokenTree::Delimited(...), where the\n     // entire lhs is those tts. Or, it can be a \"bare sequence\", not wrapped in parens.\n     match lhs {\n-        &TokenTree::Delimited(_, ref tts) => check_matcher(cx, &tts.tts),\n+        &TokenTree::Delimited(_, ref tts) => check_matcher(sess, &tts.tts),\n         _ => {\n-            cx.span_err(lhs.get_span(), \"invalid macro matcher; matchers must \\\n-                                         be contained in balanced delimiters\");\n+            let msg = \"invalid macro matcher; matchers must be contained in balanced delimiters\";\n+            sess.span_diagnostic.span_err(lhs.get_span(), msg);\n             false\n         }\n     }\n     // we don't abort on errors on rejection, the driver will do that for us\n     // after parsing/expansion. we can report every error in every macro this way.\n }\n \n-fn check_rhs(cx: &mut ExtCtxt, rhs: &TokenTree) -> bool {\n+fn check_rhs(sess: &ParseSess, rhs: &TokenTree) -> bool {\n     match *rhs {\n         TokenTree::Delimited(..) => return true,\n-        _ => cx.span_err(rhs.get_span(), \"macro rhs must be delimited\")\n+        _ => sess.span_diagnostic.span_err(rhs.get_span(), \"macro rhs must be delimited\")\n     }\n     false\n }\n \n-fn check_matcher(cx: &mut ExtCtxt, matcher: &[TokenTree]) -> bool {\n+fn check_matcher(sess: &ParseSess, matcher: &[TokenTree]) -> bool {\n     let first_sets = FirstSets::new(matcher);\n     let empty_suffix = TokenSet::empty();\n-    let err = cx.parse_sess.span_diagnostic.err_count();\n-    check_matcher_core(cx, &first_sets, matcher, &empty_suffix);\n-    err == cx.parse_sess.span_diagnostic.err_count()\n+    let err = sess.span_diagnostic.err_count();\n+    check_matcher_core(sess, &first_sets, matcher, &empty_suffix);\n+    err == sess.span_diagnostic.err_count()\n }\n \n // The FirstSets for a matcher is a mapping from subsequences in the\n@@ -635,7 +627,7 @@ impl TokenSet {\n //\n // Requires that `first_sets` is pre-computed for `matcher`;\n // see `FirstSets::new`.\n-fn check_matcher_core(cx: &mut ExtCtxt,\n+fn check_matcher_core(sess: &ParseSess,\n                       first_sets: &FirstSets,\n                       matcher: &[TokenTree],\n                       follow: &TokenSet) -> TokenSet {\n@@ -667,7 +659,8 @@ fn check_matcher_core(cx: &mut ExtCtxt,\n             TokenTree::Token(sp, ref tok) => {\n                 let can_be_followed_by_any;\n                 if let Err(bad_frag) = has_legal_fragment_specifier(tok) {\n-                    cx.struct_span_err(sp, &format!(\"invalid fragment specifier `{}`\", bad_frag))\n+                    let msg = format!(\"invalid fragment specifier `{}`\", bad_frag);\n+                    sess.span_diagnostic.struct_span_err(sp, &msg)\n                         .help(\"valid fragment specifiers are `ident`, `block`, \\\n                                `stmt`, `expr`, `pat`, `ty`, `path`, `meta`, `tt` \\\n                                and `item`\")\n@@ -692,7 +685,7 @@ fn check_matcher_core(cx: &mut ExtCtxt,\n             }\n             TokenTree::Delimited(_, ref d) => {\n                 let my_suffix = TokenSet::singleton((d.close_span, Token::CloseDelim(d.delim)));\n-                check_matcher_core(cx, first_sets, &d.tts, &my_suffix);\n+                check_matcher_core(sess, first_sets, &d.tts, &my_suffix);\n                 // don't track non NT tokens\n                 last.replace_with_irrelevant();\n \n@@ -724,7 +717,7 @@ fn check_matcher_core(cx: &mut ExtCtxt,\n                 // At this point, `suffix_first` is built, and\n                 // `my_suffix` is some TokenSet that we can use\n                 // for checking the interior of `seq_rep`.\n-                let next = check_matcher_core(cx, first_sets, &seq_rep.tts, my_suffix);\n+                let next = check_matcher_core(sess, first_sets, &seq_rep.tts, my_suffix);\n                 if next.maybe_empty {\n                     last.add_all(&next);\n                 } else {\n@@ -744,9 +737,9 @@ fn check_matcher_core(cx: &mut ExtCtxt,\n         'each_last: for &(_sp, ref t) in &last.tokens {\n             if let MatchNt(ref name, ref frag_spec) = *t {\n                 for &(sp, ref next_token) in &suffix_first.tokens {\n-                    match is_in_follow(cx, next_token, &frag_spec.name.as_str()) {\n+                    match is_in_follow(next_token, &frag_spec.name.as_str()) {\n                         Err((msg, help)) => {\n-                            cx.struct_span_err(sp, &msg).help(help).emit();\n+                            sess.span_diagnostic.struct_span_err(sp, &msg).help(help).emit();\n                             // don't bother reporting every source of\n                             // conflict for a particular element of `last`.\n                             continue 'each_last;\n@@ -761,7 +754,7 @@ fn check_matcher_core(cx: &mut ExtCtxt,\n                                 \"may be\"\n                             };\n \n-                            cx.span_err(\n+                            sess.span_diagnostic.span_err(\n                                 sp,\n                                 &format!(\"`${name}:{frag}` {may_be} followed by `{next}`, which \\\n                                           is not allowed for `{frag}` fragments\",\n@@ -818,7 +811,7 @@ fn frag_can_be_followed_by_any(frag: &str) -> bool {\n /// break macros that were relying on that binary operator as a\n /// separator.\n // when changing this do not forget to update doc/book/macros.md!\n-fn is_in_follow(_: &ExtCtxt, tok: &Token, frag: &str) -> Result<bool, (String, &'static str)> {\n+fn is_in_follow(tok: &Token, frag: &str) -> Result<bool, (String, &'static str)> {\n     if let &CloseDelim(_) = tok {\n         // closing a token tree can never be matched by any fragment;\n         // iow, we always require that `(` and `)` match, etc."}]}