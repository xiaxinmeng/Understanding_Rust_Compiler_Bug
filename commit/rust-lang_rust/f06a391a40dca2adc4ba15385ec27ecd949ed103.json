{"sha": "f06a391a40dca2adc4ba15385ec27ecd949ed103", "node_id": "MDY6Q29tbWl0NzI0NzEyOmYwNmEzOTFhNDBkY2EyYWRjNGJhMTUzODVlYzI3ZWNkOTQ5ZWQxMDM=", "commit": {"author": {"name": "kennytm", "email": "kennytm@gmail.com", "date": "2018-01-30T09:10:52Z"}, "committer": {"name": "GitHub", "email": "noreply@github.com", "date": "2018-01-30T09:10:52Z"}, "message": "Rollup merge of #47732 - mark-i-m:markim_comments_0001, r=jseyfried\n\nRun rustfmt and add doc comments to libsyntax/ext/tt/macro_parser.rs\n\nSimilar to #47603\n\ncc @theotherphil since you reviewed my other PR :smile:\n\nAnd because they are already assigned on #47603:\n\nr? @arielb1", "tree": {"sha": "d5687a4ecbb73a5cecaa9595c4af19e38b13da52", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/d5687a4ecbb73a5cecaa9595c4af19e38b13da52"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/f06a391a40dca2adc4ba15385ec27ecd949ed103", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/f06a391a40dca2adc4ba15385ec27ecd949ed103", "html_url": "https://github.com/rust-lang/rust/commit/f06a391a40dca2adc4ba15385ec27ecd949ed103", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/f06a391a40dca2adc4ba15385ec27ecd949ed103/comments", "author": {"login": "kennytm", "id": 103023, "node_id": "MDQ6VXNlcjEwMzAyMw==", "avatar_url": "https://avatars.githubusercontent.com/u/103023?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kennytm", "html_url": "https://github.com/kennytm", "followers_url": "https://api.github.com/users/kennytm/followers", "following_url": "https://api.github.com/users/kennytm/following{/other_user}", "gists_url": "https://api.github.com/users/kennytm/gists{/gist_id}", "starred_url": "https://api.github.com/users/kennytm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kennytm/subscriptions", "organizations_url": "https://api.github.com/users/kennytm/orgs", "repos_url": "https://api.github.com/users/kennytm/repos", "events_url": "https://api.github.com/users/kennytm/events{/privacy}", "received_events_url": "https://api.github.com/users/kennytm/received_events", "type": "User", "site_admin": false}, "committer": {"login": "web-flow", "id": 19864447, "node_id": "MDQ6VXNlcjE5ODY0NDQ3", "avatar_url": "https://avatars.githubusercontent.com/u/19864447?v=4", "gravatar_id": "", "url": "https://api.github.com/users/web-flow", "html_url": "https://github.com/web-flow", "followers_url": "https://api.github.com/users/web-flow/followers", "following_url": "https://api.github.com/users/web-flow/following{/other_user}", "gists_url": "https://api.github.com/users/web-flow/gists{/gist_id}", "starred_url": "https://api.github.com/users/web-flow/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/web-flow/subscriptions", "organizations_url": "https://api.github.com/users/web-flow/orgs", "repos_url": "https://api.github.com/users/web-flow/repos", "events_url": "https://api.github.com/users/web-flow/events{/privacy}", "received_events_url": "https://api.github.com/users/web-flow/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "fccc85a0ad151533c422b7071d8a2e954ec605e2", "url": "https://api.github.com/repos/rust-lang/rust/commits/fccc85a0ad151533c422b7071d8a2e954ec605e2", "html_url": "https://github.com/rust-lang/rust/commit/fccc85a0ad151533c422b7071d8a2e954ec605e2"}, {"sha": "2184400be7f6b695792af7ddde14482f3e72f1e1", "url": "https://api.github.com/repos/rust-lang/rust/commits/2184400be7f6b695792af7ddde14482f3e72f1e1", "html_url": "https://github.com/rust-lang/rust/commit/2184400be7f6b695792af7ddde14482f3e72f1e1"}], "stats": {"total": 477, "additions": 351, "deletions": 126}, "files": [{"sha": "1a9849ca5307de616f1dd4f662ac94e4457a828f", "filename": "src/libsyntax/ext/tt/macro_parser.rs", "status": "modified", "additions": 351, "deletions": 126, "changes": 477, "blob_url": "https://github.com/rust-lang/rust/blob/f06a391a40dca2adc4ba15385ec27ecd949ed103/src%2Flibsyntax%2Fext%2Ftt%2Fmacro_parser.rs", "raw_url": "https://github.com/rust-lang/rust/raw/f06a391a40dca2adc4ba15385ec27ecd949ed103/src%2Flibsyntax%2Fext%2Ftt%2Fmacro_parser.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fext%2Ftt%2Fmacro_parser.rs?ref=f06a391a40dca2adc4ba15385ec27ecd949ed103", "patch": "@@ -90,8 +90,8 @@ use codemap::Spanned;\n use errors::FatalError;\n use ext::tt::quoted::{self, TokenTree};\n use parse::{Directory, ParseSess};\n-use parse::parser::{PathStyle, Parser};\n-use parse::token::{self, DocComment, Token, Nonterminal};\n+use parse::parser::{Parser, PathStyle};\n+use parse::token::{self, DocComment, Nonterminal, Token};\n use print::pprust;\n use symbol::keywords;\n use tokenstream::TokenStream;\n@@ -100,25 +100,29 @@ use util::small_vector::SmallVector;\n use std::mem;\n use std::rc::Rc;\n use std::collections::HashMap;\n-use std::collections::hash_map::Entry::{Vacant, Occupied};\n+use std::collections::hash_map::Entry::{Occupied, Vacant};\n \n-// To avoid costly uniqueness checks, we require that `MatchSeq` always has\n-// a nonempty body.\n+// To avoid costly uniqueness checks, we require that `MatchSeq` always has a nonempty body.\n \n+/// Either a sequence of token trees or a single one. This is used as the representation of the\n+/// sequence of tokens that make up a matcher.\n #[derive(Clone)]\n enum TokenTreeOrTokenTreeVec {\n     Tt(TokenTree),\n     TtSeq(Vec<TokenTree>),\n }\n \n impl TokenTreeOrTokenTreeVec {\n+    /// Returns the number of constituent top-level token trees of `self` (top-level in that it\n+    /// will not recursively descend into subtrees).\n     fn len(&self) -> usize {\n         match *self {\n             TtSeq(ref v) => v.len(),\n             Tt(ref tt) => tt.len(),\n         }\n     }\n \n+    /// The the `index`-th token tree of `self`.\n     fn get_tt(&self, index: usize) -> TokenTree {\n         match *self {\n             TtSeq(ref v) => v[index].clone(),\n@@ -127,36 +131,96 @@ impl TokenTreeOrTokenTreeVec {\n     }\n }\n \n-/// an unzipping of `TokenTree`s\n+/// An unzipping of `TokenTree`s... see the `stack` field of `MatcherPos`.\n+///\n+/// This is used by `inner_parse_loop` to keep track of delimited submatchers that we have\n+/// descended into.\n #[derive(Clone)]\n struct MatcherTtFrame {\n+    /// The \"parent\" matcher that we are descending into.\n     elts: TokenTreeOrTokenTreeVec,\n+    /// The position of the \"dot\" in `elts` at the time we descended.\n     idx: usize,\n }\n \n+/// Represents a single \"position\" (aka \"matcher position\", aka \"item\"), as described in the module\n+/// documentation.\n #[derive(Clone)]\n struct MatcherPos {\n-    stack: Vec<MatcherTtFrame>,\n+    /// The token or sequence of tokens that make up the matcher\n     top_elts: TokenTreeOrTokenTreeVec,\n-    sep: Option<Token>,\n+    /// The position of the \"dot\" in this matcher\n     idx: usize,\n-    up: Option<Box<MatcherPos>>,\n+    /// The beginning position in the source that the beginning of this matcher corresponds to. In\n+    /// other words, the token in the source at `sp_lo` is matched against the first token of the\n+    /// matcher.\n+    sp_lo: BytePos,\n+\n+    /// For each named metavar in the matcher, we keep track of token trees matched against the\n+    /// metavar by the black box parser. In particular, there may be more than one match per\n+    /// metavar if we are in a repetition (each repetition matches each of the variables).\n+    /// Moreover, matchers and repetitions can be nested; the `matches` field is shared (hence the\n+    /// `Rc`) among all \"nested\" matchers. `match_lo`, `match_cur`, and `match_hi` keep track of\n+    /// the current position of the `self` matcher position in the shared `matches` list.\n+    ///\n+    /// Also, note that while we are descending into a sequence, matchers are given their own\n+    /// `matches` vector. Only once we reach the end of a full repetition of the sequence do we add\n+    /// all bound matches from the submatcher into the shared top-level `matches` vector. If `sep`\n+    /// and `up` are `Some`, then `matches` is _not_ the shared top-level list. Instead, if one\n+    /// wants the shared `matches`, one should use `up.matches`.\n     matches: Vec<Rc<Vec<NamedMatch>>>,\n+    /// The position in `matches` corresponding to the first metavar in this matcher's sequence of\n+    /// token trees. In other words, the first metavar in the first token of `top_elts` corresponds\n+    /// to `matches[match_lo]`.\n     match_lo: usize,\n+    /// The position in `matches` corresponding to the metavar we are currently trying to match\n+    /// against the source token stream. `match_lo <= match_cur <= match_hi`.\n     match_cur: usize,\n+    /// Similar to `match_lo` except `match_hi` is the position in `matches` of the _last_ metavar\n+    /// in this matcher.\n     match_hi: usize,\n-    sp_lo: BytePos,\n+\n+    // Specifically used if we are matching a repetition. If we aren't both should be `None`.\n+    /// The separator if we are in a repetition\n+    sep: Option<Token>,\n+    /// The \"parent\" matcher position if we are in a repetition. That is, the matcher position just\n+    /// before we enter the sequence.\n+    up: Option<Box<MatcherPos>>,\n+\n+    // Specifically used to \"unzip\" token trees. By \"unzip\", we mean to unwrap the delimiters from\n+    // a delimited token tree (e.g. something wrapped in `(` `)`) or to get the contents of a doc\n+    // comment...\n+    /// When matching against matchers with nested delimited submatchers (e.g. `pat ( pat ( .. )\n+    /// pat ) pat`), we need to keep track of the matchers we are descending into. This stack does\n+    /// that where the bottom of the stack is the outermost matcher.\n+    // Also, throughout the comments, this \"descent\" is often referred to as \"unzipping\"...\n+    stack: Vec<MatcherTtFrame>,\n }\n \n impl MatcherPos {\n+    /// Add `m` as a named match for the `idx`-th metavar.\n     fn push_match(&mut self, idx: usize, m: NamedMatch) {\n         let matches = Rc::make_mut(&mut self.matches[idx]);\n         matches.push(m);\n     }\n }\n \n+/// Represents the possible results of an attempted parse.\n+pub enum ParseResult<T> {\n+    /// Parsed successfully.\n+    Success(T),\n+    /// Arm failed to match. If the second parameter is `token::Eof`, it indicates an unexpected\n+    /// end of macro invocation. Otherwise, it indicates that no rules expected the given token.\n+    Failure(syntax_pos::Span, Token),\n+    /// Fatal error (malformed macro?). Abort compilation.\n+    Error(syntax_pos::Span, String),\n+}\n+\n+/// A `ParseResult` where the `Success` variant contains a mapping of `Ident`s to `NamedMatch`es.\n+/// This represents the mapping of metavars to the token trees they bind to.\n pub type NamedParseResult = ParseResult<HashMap<Ident, Rc<NamedMatch>>>;\n \n+/// Count how many metavars are named in the given matcher `ms`.\n pub fn count_names(ms: &[TokenTree]) -> usize {\n     ms.iter().fold(0, |count, elt| {\n         count + match *elt {\n@@ -169,20 +233,38 @@ pub fn count_names(ms: &[TokenTree]) -> usize {\n     })\n }\n \n+/// Initialize `len` empty shared `Vec`s to be used to store matches of metavars.\n+fn create_matches(len: usize) -> Vec<Rc<Vec<NamedMatch>>> {\n+    (0..len).into_iter().map(|_| Rc::new(Vec::new())).collect()\n+}\n+\n+/// Generate the top-level matcher position in which the \"dot\" is before the first token of the\n+/// matcher `ms` and we are going to start matching at position `lo` in the source.\n fn initial_matcher_pos(ms: Vec<TokenTree>, lo: BytePos) -> Box<MatcherPos> {\n     let match_idx_hi = count_names(&ms[..]);\n     let matches = create_matches(match_idx_hi);\n     Box::new(MatcherPos {\n-        stack: vec![],\n-        top_elts: TtSeq(ms),\n-        sep: None,\n+        // Start with the top level matcher given to us\n+        top_elts: TtSeq(ms), // \"elts\" is an abbr. for \"elements\"\n+        // The \"dot\" is before the first token of the matcher\n         idx: 0,\n-        up: None,\n+        // We start matching with byte `lo` in the source code\n+        sp_lo: lo,\n+\n+        // Initialize `matches` to a bunch of empty `Vec`s -- one for each metavar in `top_elts`.\n+        // `match_lo` for `top_elts` is 0 and `match_hi` is `matches.len()`. `match_cur` is 0 since\n+        // we haven't actually matched anything yet.\n         matches,\n         match_lo: 0,\n         match_cur: 0,\n         match_hi: match_idx_hi,\n-        sp_lo: lo\n+\n+        // Haven't descended into any delimiters, so empty stack\n+        stack: vec![],\n+\n+        // Haven't descended into any sequences, so both of these are `None`.\n+        sep: None,\n+        up: None,\n     })\n }\n \n@@ -202,29 +284,36 @@ fn initial_matcher_pos(ms: Vec<TokenTree>, lo: BytePos) -> Box<MatcherPos> {\n /// token tree. The depth of the `NamedMatch` structure will therefore depend\n /// only on the nesting depth of `ast::TTSeq`s in the originating\n /// token tree it was derived from.\n-\n #[derive(Debug, Clone)]\n pub enum NamedMatch {\n     MatchedSeq(Rc<Vec<NamedMatch>>, syntax_pos::Span),\n-    MatchedNonterminal(Rc<Nonterminal>)\n+    MatchedNonterminal(Rc<Nonterminal>),\n }\n \n-fn nameize<I: Iterator<Item=NamedMatch>>(sess: &ParseSess, ms: &[TokenTree], mut res: I)\n-                                             -> NamedParseResult {\n-    fn n_rec<I: Iterator<Item=NamedMatch>>(sess: &ParseSess, m: &TokenTree, res: &mut I,\n-             ret_val: &mut HashMap<Ident, Rc<NamedMatch>>)\n-             -> Result<(), (syntax_pos::Span, String)> {\n+/// Takes a sequence of token trees `ms` representing a matcher which successfully matched input\n+/// and an iterator of items that matched input and produces a `NamedParseResult`.\n+fn nameize<I: Iterator<Item = NamedMatch>>(\n+    sess: &ParseSess,\n+    ms: &[TokenTree],\n+    mut res: I,\n+) -> NamedParseResult {\n+    // Recursively descend into each type of matcher (e.g. sequences, delimited, metavars) and make\n+    // sure that each metavar has _exactly one_ binding. If a metavar does not have exactly one\n+    // binding, then there is an error. If it does, then we insert the binding into the\n+    // `NamedParseResult`.\n+    fn n_rec<I: Iterator<Item = NamedMatch>>(\n+        sess: &ParseSess,\n+        m: &TokenTree,\n+        res: &mut I,\n+        ret_val: &mut HashMap<Ident, Rc<NamedMatch>>,\n+    ) -> Result<(), (syntax_pos::Span, String)> {\n         match *m {\n-            TokenTree::Sequence(_, ref seq) => {\n-                for next_m in &seq.tts {\n-                    n_rec(sess, next_m, res.by_ref(), ret_val)?\n-                }\n-            }\n-            TokenTree::Delimited(_, ref delim) => {\n-                for next_m in &delim.tts {\n-                    n_rec(sess, next_m, res.by_ref(), ret_val)?;\n-                }\n-            }\n+            TokenTree::Sequence(_, ref seq) => for next_m in &seq.tts {\n+                n_rec(sess, next_m, res.by_ref(), ret_val)?\n+            },\n+            TokenTree::Delimited(_, ref delim) => for next_m in &delim.tts {\n+                n_rec(sess, next_m, res.by_ref(), ret_val)?;\n+            },\n             TokenTree::MetaVarDecl(span, _, id) if id.name == keywords::Invalid.name() => {\n                 if sess.missing_fragment_specifiers.borrow_mut().remove(&span) {\n                     return Err((span, \"missing fragment specifier\".to_string()));\n@@ -250,33 +339,28 @@ fn nameize<I: Iterator<Item=NamedMatch>>(sess: &ParseSess, ms: &[TokenTree], mut\n     let mut ret_val = HashMap::new();\n     for m in ms {\n         match n_rec(sess, m, res.by_ref(), &mut ret_val) {\n-            Ok(_) => {},\n+            Ok(_) => {}\n             Err((sp, msg)) => return Error(sp, msg),\n         }\n     }\n \n     Success(ret_val)\n }\n \n-pub enum ParseResult<T> {\n-    Success(T),\n-    /// Arm failed to match. If the second parameter is `token::Eof`, it\n-    /// indicates an unexpected end of macro invocation. Otherwise, it\n-    /// indicates that no rules expected the given token.\n-    Failure(syntax_pos::Span, Token),\n-    /// Fatal error (malformed macro?). Abort compilation.\n-    Error(syntax_pos::Span, String)\n-}\n-\n+/// Generate an appropriate parsing failure message. For EOF, this is \"unexpected end...\". For\n+/// other tokens, this is \"unexpected token...\".\n pub fn parse_failure_msg(tok: Token) -> String {\n     match tok {\n         token::Eof => \"unexpected end of macro invocation\".to_string(),\n-        _ => format!(\"no rules expected the token `{}`\", pprust::token_to_string(&tok)),\n+        _ => format!(\n+            \"no rules expected the token `{}`\",\n+            pprust::token_to_string(&tok)\n+        ),\n     }\n }\n \n /// Perform a token equality check, ignoring syntax context (that is, an unhygienic comparison)\n-fn token_name_eq(t1 : &Token, t2 : &Token) -> bool {\n+fn token_name_eq(t1: &Token, t2: &Token) -> bool {\n     if let (Some(id1), Some(id2)) = (t1.ident(), t2.ident()) {\n         id1.name == id2.name\n     } else if let (&token::Lifetime(id1), &token::Lifetime(id2)) = (t1, t2) {\n@@ -286,77 +370,121 @@ fn token_name_eq(t1 : &Token, t2 : &Token) -> bool {\n     }\n }\n \n-fn create_matches(len: usize) -> Vec<Rc<Vec<NamedMatch>>> {\n-    (0..len).into_iter().map(|_| Rc::new(Vec::new())).collect()\n-}\n-\n-fn inner_parse_loop(sess: &ParseSess,\n-                    cur_items: &mut SmallVector<Box<MatcherPos>>,\n-                    next_items: &mut Vec<Box<MatcherPos>>,\n-                    eof_items: &mut SmallVector<Box<MatcherPos>>,\n-                    bb_items: &mut SmallVector<Box<MatcherPos>>,\n-                    token: &Token,\n-                    span: syntax_pos::Span)\n-                    -> ParseResult<()> {\n+/// Process the matcher positions of `cur_items` until it is empty. In the process, this will\n+/// produce more items in `next_items`, `eof_items`, and `bb_items`.\n+///\n+/// For more info about the how this happens, see the module-level doc comments and the inline\n+/// comments of this function.\n+///\n+/// # Parameters\n+///\n+/// - `sess`: the parsing session into which errors are emitted.\n+/// - `cur_items`: the set of current items to be processed. This should be empty by the end of a\n+///   successful execution of this function.\n+/// - `next_items`: the set of newly generated items. These are used to replenish `cur_items` in\n+///   the function `parse`.\n+/// - `eof_items`: the set of items that would be valid if this was the EOF.\n+/// - `bb_items`: the set of items that are waiting for the black-box parser.\n+/// - `token`: the current token of the parser.\n+/// - `span`: the `Span` in the source code corresponding to the token trees we are trying to match\n+///   against the matcher positions in `cur_items`.\n+///\n+/// # Returns\n+///\n+/// A `ParseResult`. Note that matches are kept track of through the items generated.\n+fn inner_parse_loop(\n+    sess: &ParseSess,\n+    cur_items: &mut SmallVector<Box<MatcherPos>>,\n+    next_items: &mut Vec<Box<MatcherPos>>,\n+    eof_items: &mut SmallVector<Box<MatcherPos>>,\n+    bb_items: &mut SmallVector<Box<MatcherPos>>,\n+    token: &Token,\n+    span: syntax_pos::Span,\n+) -> ParseResult<()> {\n+    // Pop items from `cur_items` until it is empty.\n     while let Some(mut item) = cur_items.pop() {\n-        // When unzipped trees end, remove them\n+        // When unzipped trees end, remove them. This corresponds to backtracking out of a\n+        // delimited submatcher into which we already descended. In backtracking out again, we need\n+        // to advance the \"dot\" past the delimiters in the outer matcher.\n         while item.idx >= item.top_elts.len() {\n             match item.stack.pop() {\n                 Some(MatcherTtFrame { elts, idx }) => {\n                     item.top_elts = elts;\n                     item.idx = idx + 1;\n                 }\n-                None => break\n+                None => break,\n             }\n         }\n \n+        // Get the current position of the \"dot\" (`idx`) in `item` and the number of token trees in\n+        // the matcher (`len`).\n         let idx = item.idx;\n         let len = item.top_elts.len();\n \n-        // at end of sequence\n+        // If `idx >= len`, then we are at or past the end of the matcher of `item`.\n         if idx >= len {\n-            // We are repeating iff there is a parent\n+            // We are repeating iff there is a parent. If the matcher is inside of a repetition,\n+            // then we could be at the end of a sequence or at the beginning of the next\n+            // repetition.\n             if item.up.is_some() {\n-                // Disregarding the separator, add the \"up\" case to the tokens that should be\n-                // examined.\n-                // (remove this condition to make trailing seps ok)\n+                // At this point, regardless of whether there is a separator, we should add all\n+                // matches from the complete repetition of the sequence to the shared, top-level\n+                // `matches` list (actually, `up.matches`, which could itself not be the top-level,\n+                // but anyway...). Moreover, we add another item to `cur_items` in which the \"dot\"\n+                // is at the end of the `up` matcher. This ensures that the \"dot\" in the `up`\n+                // matcher is also advanced sufficiently.\n+                //\n+                // NOTE: removing the condition `idx == len` allows trailing separators.\n                 if idx == len {\n+                    // Get the `up` matcher\n                     let mut new_pos = item.up.clone().unwrap();\n \n-                    // update matches (the MBE \"parse tree\") by appending\n-                    // each tree as a subtree.\n-\n-                    // Only touch the binders we have actually bound\n+                    // Add matches from this repetition to the `matches` of `up`\n                     for idx in item.match_lo..item.match_hi {\n                         let sub = item.matches[idx].clone();\n                         let span = span.with_lo(item.sp_lo);\n                         new_pos.push_match(idx, MatchedSeq(sub, span));\n                     }\n \n+                    // Move the \"dot\" past the repetition in `up`\n                     new_pos.match_cur = item.match_hi;\n                     new_pos.idx += 1;\n                     cur_items.push(new_pos);\n                 }\n \n-                // Check if we need a separator\n+                // Check if we need a separator.\n                 if idx == len && item.sep.is_some() {\n-                    // We have a separator, and it is the current token.\n-                    if item.sep.as_ref().map(|sep| token_name_eq(token, sep)).unwrap_or(false) {\n+                    // We have a separator, and it is the current token. We can advance past the\n+                    // separator token.\n+                    if item.sep\n+                        .as_ref()\n+                        .map(|sep| token_name_eq(token, sep))\n+                        .unwrap_or(false)\n+                    {\n                         item.idx += 1;\n                         next_items.push(item);\n                     }\n-                } else { // we don't need a separator\n+                }\n+                // We don't need a separator. Move the \"dot\" back to the beginning of the matcher\n+                // and try to match again.\n+                else {\n                     item.match_cur = item.match_lo;\n                     item.idx = 0;\n                     cur_items.push(item);\n                 }\n-            } else {\n-                // We aren't repeating, so we must be potentially at the end of the input.\n+            }\n+            // If we are not in a repetition, then being at the end of a matcher means that we have\n+            // reached the potential end of the input.\n+            else {\n                 eof_items.push(item);\n             }\n-        } else {\n+        }\n+        // We are in the middle of a matcher.\n+        else {\n+            // Look at what token in the matcher we are trying to match the current token (`token`)\n+            // against. Depending on that, we may generate new items.\n             match item.top_elts.get_tt(idx) {\n-                /* need to descend into sequence */\n+                // Need to descend into a sequence\n                 TokenTree::Sequence(sp, seq) => {\n                     if seq.op == quoted::KleeneOp::ZeroOrMore {\n                         // Examine the case where there are 0 matches of this sequence\n@@ -384,18 +512,30 @@ fn inner_parse_loop(sess: &ParseSess,\n                         top_elts: Tt(TokenTree::Sequence(sp, seq)),\n                     }));\n                 }\n+\n+                // We need to match a metavar (but the identifier is invalid)... this is an error\n                 TokenTree::MetaVarDecl(span, _, id) if id.name == keywords::Invalid.name() => {\n                     if sess.missing_fragment_specifiers.borrow_mut().remove(&span) {\n                         return Error(span, \"missing fragment specifier\".to_string());\n                     }\n                 }\n+\n+                // We need to match a metavar with a valid ident... call out to the black-box\n+                // parser by adding an item to `bb_items`.\n                 TokenTree::MetaVarDecl(_, _, id) => {\n                     // Built-in nonterminals never start with these tokens,\n                     // so we can eliminate them from consideration.\n                     if may_begin_with(&*id.name.as_str(), token) {\n                         bb_items.push(item);\n                     }\n                 }\n+\n+                // We need to descend into a delimited submatcher or a doc comment. To do this, we\n+                // push the current matcher onto a stack and push a new item containing the\n+                // submatcher onto `cur_items`.\n+                //\n+                // At the beginning of the loop, if we reach the end of the delimited submatcher,\n+                // we pop the stack to backtrack out of the descent.\n                 seq @ TokenTree::Delimited(..) | seq @ TokenTree::Token(_, DocComment(..)) => {\n                     let lower_elts = mem::replace(&mut item.top_elts, Tt(seq));\n                     let idx = item.idx;\n@@ -406,83 +546,152 @@ fn inner_parse_loop(sess: &ParseSess,\n                     item.idx = 0;\n                     cur_items.push(item);\n                 }\n+\n+                // We just matched a normal token. We can just advance the parser.\n                 TokenTree::Token(_, ref t) if token_name_eq(t, token) => {\n                     item.idx += 1;\n                     next_items.push(item);\n                 }\n+\n+                // There was another token that was not `token`... This means we can't add any\n+                // rules. NOTE that this is not necessarily an error unless _all_ items in\n+                // `cur_items` end up doing this. There may still be some other matchers that do\n+                // end up working out.\n                 TokenTree::Token(..) | TokenTree::MetaVar(..) => {}\n             }\n         }\n     }\n \n+    // Yay a successful parse (so far)!\n     Success(())\n }\n \n-pub fn parse(sess: &ParseSess,\n-             tts: TokenStream,\n-             ms: &[TokenTree],\n-             directory: Option<Directory>,\n-             recurse_into_modules: bool)\n-             -> NamedParseResult {\n+/// Use the given sequence of token trees (`ms`) as a matcher. Match the given token stream `tts`\n+/// against it and return the match.\n+///\n+/// # Parameters\n+///\n+/// - `sess`: The session into which errors are emitted\n+/// - `tts`: The tokenstream we are matching against the pattern `ms`\n+/// - `ms`: A sequence of token trees representing a pattern against which we are matching\n+/// - `directory`: Information about the file locations (needed for the black-box parser)\n+/// - `recurse_into_modules`: Whether or not to recurse into modules (needed for the black-box\n+///   parser)\n+pub fn parse(\n+    sess: &ParseSess,\n+    tts: TokenStream,\n+    ms: &[TokenTree],\n+    directory: Option<Directory>,\n+    recurse_into_modules: bool,\n+) -> NamedParseResult {\n+    // Create a parser that can be used for the \"black box\" parts.\n     let mut parser = Parser::new(sess, tts, directory, recurse_into_modules, true);\n+\n+    // A queue of possible matcher positions. We initialize it with the matcher position in which\n+    // the \"dot\" is before the first token of the first token tree in `ms`. `inner_parse_loop` then\n+    // processes all of these possible matcher positions and produces posible next positions into\n+    // `next_items`. After some post-processing, the contents of `next_items` replenish `cur_items`\n+    // and we start over again.\n     let mut cur_items = SmallVector::one(initial_matcher_pos(ms.to_owned(), parser.span.lo()));\n-    let mut next_items = Vec::new(); // or proceed normally\n+    let mut next_items = Vec::new();\n \n     loop {\n-        let mut bb_items = SmallVector::new(); // black-box parsed by parser.rs\n+        // Matcher positions black-box parsed by parser.rs (`parser`)\n+        let mut bb_items = SmallVector::new();\n+\n+        // Matcher positions that would be valid if the macro invocation was over now\n         let mut eof_items = SmallVector::new();\n         assert!(next_items.is_empty());\n \n-        match inner_parse_loop(sess, &mut cur_items, &mut next_items, &mut eof_items, &mut bb_items,\n-                               &parser.token, parser.span) {\n-            Success(_) => {},\n+        // Process `cur_items` until either we have finished the input or we need to get some\n+        // parsing from the black-box parser done. The result is that `next_items` will contain a\n+        // bunch of possible next matcher positions in `next_items`.\n+        match inner_parse_loop(\n+            sess,\n+            &mut cur_items,\n+            &mut next_items,\n+            &mut eof_items,\n+            &mut bb_items,\n+            &parser.token,\n+            parser.span,\n+        ) {\n+            Success(_) => {}\n             Failure(sp, tok) => return Failure(sp, tok),\n             Error(sp, msg) => return Error(sp, msg),\n         }\n \n         // inner parse loop handled all cur_items, so it's empty\n         assert!(cur_items.is_empty());\n \n-        /* error messages here could be improved with links to orig. rules */\n+        // We need to do some post processing after the `inner_parser_loop`.\n+        //\n+        // Error messages here could be improved with links to original rules.\n+\n+        // If we reached the EOF, check that there is EXACTLY ONE possible matcher. Otherwise,\n+        // either the parse is ambiguous (which should never happen) or their is a syntax error.\n         if token_name_eq(&parser.token, &token::Eof) {\n             if eof_items.len() == 1 {\n-                let matches = eof_items[0].matches.iter_mut().map(|dv| {\n-                    Rc::make_mut(dv).pop().unwrap()\n-                });\n+                let matches = eof_items[0]\n+                    .matches\n+                    .iter_mut()\n+                    .map(|dv| Rc::make_mut(dv).pop().unwrap());\n                 return nameize(sess, ms, matches);\n             } else if eof_items.len() > 1 {\n-                return Error(parser.span, \"ambiguity: multiple successful parses\".to_string());\n+                return Error(\n+                    parser.span,\n+                    \"ambiguity: multiple successful parses\".to_string(),\n+                );\n             } else {\n                 return Failure(parser.span, token::Eof);\n             }\n-        } else if (!bb_items.is_empty() && !next_items.is_empty()) || bb_items.len() > 1 {\n-            let nts = bb_items.iter().map(|item| match item.top_elts.get_tt(item.idx) {\n-                TokenTree::MetaVarDecl(_, bind, name) => {\n-                    format!(\"{} ('{}')\", name, bind)\n-                }\n-                _ => panic!()\n-            }).collect::<Vec<String>>().join(\" or \");\n-\n-            return Error(parser.span, format!(\n-                \"local ambiguity: multiple parsing options: {}\",\n-                match next_items.len() {\n-                    0 => format!(\"built-in NTs {}.\", nts),\n-                    1 => format!(\"built-in NTs {} or 1 other option.\", nts),\n-                    n => format!(\"built-in NTs {} or {} other options.\", nts, n),\n-                }\n-            ));\n-        } else if bb_items.is_empty() && next_items.is_empty() {\n+        }\n+        // Another possibility is that we need to call out to parse some rust nonterminal\n+        // (black-box) parser. However, if there is not EXACTLY ONE of these, something is wrong.\n+        else if (!bb_items.is_empty() && !next_items.is_empty()) || bb_items.len() > 1 {\n+            let nts = bb_items\n+                .iter()\n+                .map(|item| match item.top_elts.get_tt(item.idx) {\n+                    TokenTree::MetaVarDecl(_, bind, name) => format!(\"{} ('{}')\", name, bind),\n+                    _ => panic!(),\n+                })\n+                .collect::<Vec<String>>()\n+                .join(\" or \");\n+\n+            return Error(\n+                parser.span,\n+                format!(\n+                    \"local ambiguity: multiple parsing options: {}\",\n+                    match next_items.len() {\n+                        0 => format!(\"built-in NTs {}.\", nts),\n+                        1 => format!(\"built-in NTs {} or 1 other option.\", nts),\n+                        n => format!(\"built-in NTs {} or {} other options.\", nts, n),\n+                    }\n+                ),\n+            );\n+        }\n+        // If there are no posible next positions AND we aren't waiting for the black-box parser,\n+        // then their is a syntax error.\n+        else if bb_items.is_empty() && next_items.is_empty() {\n             return Failure(parser.span, parser.token);\n-        } else if !next_items.is_empty() {\n-            /* Now process the next token */\n+        }\n+        // Dump all possible `next_items` into `cur_items` for the next iteration.\n+        else if !next_items.is_empty() {\n+            // Now process the next token\n             cur_items.extend(next_items.drain(..));\n             parser.bump();\n-        } else /* bb_items.len() == 1 */ {\n+        }\n+        // Finally, we have the case where we need to call the black-box parser to get some\n+        // nonterminal.\n+        else {\n+            assert_eq!(bb_items.len(), 1);\n+\n             let mut item = bb_items.pop().unwrap();\n             if let TokenTree::MetaVarDecl(span, _, ident) = item.top_elts.get_tt(item.idx) {\n                 let match_cur = item.match_cur;\n-                item.push_match(match_cur,\n-                    MatchedNonterminal(Rc::new(parse_nt(&mut parser, span, &ident.name.as_str()))));\n+                item.push_match(\n+                    match_cur,\n+                    MatchedNonterminal(Rc::new(parse_nt(&mut parser, span, &ident.name.as_str()))),\n+                );\n                 item.idx += 1;\n                 item.match_cur += 1;\n             } else {\n@@ -512,20 +721,21 @@ fn may_begin_with(name: &str, token: &Token) -> bool {\n         \"expr\" => token.can_begin_expr(),\n         \"ty\" => token.can_begin_type(),\n         \"ident\" => token.is_ident(),\n-        \"vis\" => match *token { // The follow-set of :vis + \"priv\" keyword + interpolated\n+        \"vis\" => match *token {\n+            // The follow-set of :vis + \"priv\" keyword + interpolated\n             Token::Comma | Token::Ident(_) | Token::Interpolated(_) => true,\n             _ => token.can_begin_type(),\n         },\n         \"block\" => match *token {\n             Token::OpenDelim(token::Brace) => true,\n             Token::Interpolated(ref nt) => match nt.0 {\n-                token::NtItem(_) |\n-                token::NtPat(_) |\n-                token::NtTy(_) |\n-                token::NtIdent(_) |\n-                token::NtMeta(_) |\n-                token::NtPath(_) |\n-                token::NtVis(_) => false, // none of these may start with '{'.\n+                token::NtItem(_)\n+                | token::NtPat(_)\n+                | token::NtTy(_)\n+                | token::NtIdent(_)\n+                | token::NtMeta(_)\n+                | token::NtPath(_)\n+                | token::NtVis(_) => false, // none of these may start with '{'.\n                 _ => true,\n             },\n             _ => false,\n@@ -562,6 +772,18 @@ fn may_begin_with(name: &str, token: &Token) -> bool {\n     }\n }\n \n+/// A call to the \"black-box\" parser to parse some rust nonterminal.\n+///\n+/// # Parameters\n+///\n+/// - `p`: the \"black-box\" parser to use\n+/// - `sp`: the `Span` we want to parse\n+/// - `name`: the name of the metavar _matcher_ we want to match (e.g. `tt`, `ident`, `block`,\n+///   etc...)\n+///\n+/// # Returns\n+///\n+/// The parsed nonterminal.\n fn parse_nt<'a>(p: &mut Parser<'a>, sp: Span, name: &str) -> Nonterminal {\n     if name == \"tt\" {\n         return token::NtTT(p.parse_token_tree());\n@@ -591,12 +813,15 @@ fn parse_nt<'a>(p: &mut Parser<'a>, sp: Span, name: &str) -> Nonterminal {\n         \"ident\" => match p.token {\n             token::Ident(sn) => {\n                 p.bump();\n-                token::NtIdent(Spanned::<Ident>{node: sn, span: p.prev_span})\n+                token::NtIdent(Spanned::<Ident> {\n+                    node: sn,\n+                    span: p.prev_span,\n+                })\n             }\n             _ => {\n                 let token_str = pprust::token_to_string(&p.token);\n-                p.fatal(&format!(\"expected ident, found {}\",\n-                                 &token_str[..])).emit();\n+                p.fatal(&format!(\"expected ident, found {}\", &token_str[..]))\n+                    .emit();\n                 FatalError.raise()\n             }\n         },\n@@ -606,6 +831,6 @@ fn parse_nt<'a>(p: &mut Parser<'a>, sp: Span, name: &str) -> Nonterminal {\n         \"lifetime\" => token::NtLifetime(p.expect_lifetime()),\n         // this is not supposed to happen, since it has been checked\n         // when compiling the macro.\n-        _ => p.span_bug(sp, \"invalid fragment specifier\")\n+        _ => p.span_bug(sp, \"invalid fragment specifier\"),\n     }\n }"}]}