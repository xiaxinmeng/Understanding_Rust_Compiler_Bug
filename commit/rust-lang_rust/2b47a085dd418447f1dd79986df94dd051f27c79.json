{"sha": "2b47a085dd418447f1dd79986df94dd051f27c79", "node_id": "MDY6Q29tbWl0NzI0NzEyOjJiNDdhMDg1ZGQ0MTg0NDdmMWRkNzk5ODZkZjk0ZGQwNTFmMjdjNzk=", "commit": {"author": {"name": "Pawe\u0142 Romanowski", "email": "pawroman@gmail.com", "date": "2019-06-10T19:13:01Z"}, "committer": {"name": "Pawe\u0142 Romanowski", "email": "pawroman@gmail.com", "date": "2019-07-01T17:43:48Z"}, "message": "Address review remarks in unicode.py", "tree": {"sha": "852cf248a5fc34ec2220d6b50e7d770c001123e9", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/852cf248a5fc34ec2220d6b50e7d770c001123e9"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/2b47a085dd418447f1dd79986df94dd051f27c79", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/2b47a085dd418447f1dd79986df94dd051f27c79", "html_url": "https://github.com/rust-lang/rust/commit/2b47a085dd418447f1dd79986df94dd051f27c79", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/2b47a085dd418447f1dd79986df94dd051f27c79/comments", "author": {"login": "pawroman", "id": 914977, "node_id": "MDQ6VXNlcjkxNDk3Nw==", "avatar_url": "https://avatars.githubusercontent.com/u/914977?v=4", "gravatar_id": "", "url": "https://api.github.com/users/pawroman", "html_url": "https://github.com/pawroman", "followers_url": "https://api.github.com/users/pawroman/followers", "following_url": "https://api.github.com/users/pawroman/following{/other_user}", "gists_url": "https://api.github.com/users/pawroman/gists{/gist_id}", "starred_url": "https://api.github.com/users/pawroman/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/pawroman/subscriptions", "organizations_url": "https://api.github.com/users/pawroman/orgs", "repos_url": "https://api.github.com/users/pawroman/repos", "events_url": "https://api.github.com/users/pawroman/events{/privacy}", "received_events_url": "https://api.github.com/users/pawroman/received_events", "type": "User", "site_admin": false}, "committer": {"login": "pawroman", "id": 914977, "node_id": "MDQ6VXNlcjkxNDk3Nw==", "avatar_url": "https://avatars.githubusercontent.com/u/914977?v=4", "gravatar_id": "", "url": "https://api.github.com/users/pawroman", "html_url": "https://github.com/pawroman", "followers_url": "https://api.github.com/users/pawroman/followers", "following_url": "https://api.github.com/users/pawroman/following{/other_user}", "gists_url": "https://api.github.com/users/pawroman/gists{/gist_id}", "starred_url": "https://api.github.com/users/pawroman/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/pawroman/subscriptions", "organizations_url": "https://api.github.com/users/pawroman/orgs", "repos_url": "https://api.github.com/users/pawroman/repos", "events_url": "https://api.github.com/users/pawroman/events{/privacy}", "received_events_url": "https://api.github.com/users/pawroman/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "60ccf89693037b3c010b027081d253b9c69a304c", "url": "https://api.github.com/repos/rust-lang/rust/commits/60ccf89693037b3c010b027081d253b9c69a304c", "html_url": "https://github.com/rust-lang/rust/commit/60ccf89693037b3c010b027081d253b9c69a304c"}], "stats": {"total": 116, "additions": 61, "deletions": 55}, "files": [{"sha": "a0539cd9ca9b6ee1bb641b83127e407b04ca2053", "filename": "src/libcore/unicode/unicode.py", "status": "modified", "additions": 61, "deletions": 55, "changes": 116, "blob_url": "https://github.com/rust-lang/rust/blob/2b47a085dd418447f1dd79986df94dd051f27c79/src%2Flibcore%2Funicode%2Funicode.py", "raw_url": "https://github.com/rust-lang/rust/raw/2b47a085dd418447f1dd79986df94dd051f27c79/src%2Flibcore%2Funicode%2Funicode.py", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibcore%2Funicode%2Funicode.py?ref=2b47a085dd418447f1dd79986df94dd051f27c79", "patch": "@@ -34,7 +34,7 @@\n     from StringIO import StringIO\n \n try:\n-    # completely optional type hinting\n+    # Completely optional type hinting\n     # (Python 2 compatible using comments,\n     # see: https://mypy.readthedocs.io/en/latest/python2.html)\n     # This is very helpful in typing-aware IDE like PyCharm.\n@@ -43,9 +43,9 @@\n     pass\n \n \n-# we don't use enum.Enum because of Python 2.7 compatibility\n+# We don't use enum.Enum because of Python 2.7 compatibility.\n class UnicodeFiles(object):\n-    # ReadMe does not contain any unicode data, we\n+    # ReadMe does not contain any Unicode data, we\n     # only use it to extract versions.\n     README = \"ReadMe.txt\"\n \n@@ -57,11 +57,15 @@ class UnicodeFiles(object):\n     UNICODE_DATA = \"UnicodeData.txt\"\n \n \n-UnicodeFiles.ALL_FILES = tuple(\n-    getattr(UnicodeFiles, name) for name in dir(UnicodeFiles)\n+# The order doesn't really matter (Python < 3.6 won't preserve it),\n+# we only want to aggregate all the file names.\n+ALL_UNICODE_FILES = tuple(\n+    value for name, value in UnicodeFiles.__dict__.items()\n     if not name.startswith(\"_\")\n )\n \n+assert len(ALL_UNICODE_FILES) == 7, \"Unexpected number of unicode files\"\n+\n # The directory this file is located in.\n THIS_DIR = os.path.dirname(os.path.realpath(__file__))\n \n@@ -97,18 +101,17 @@ class UnicodeFiles(object):\n \n # This is the (inclusive) range of surrogate codepoints.\n # These are not valid Rust characters.\n-# - they are not valid Rust characters\n SURROGATE_CODEPOINTS_RANGE = (0xd800, 0xdfff)\n \n UnicodeData = namedtuple(\n     \"UnicodeData\", (\n-        # conversions:\n+        # Conversions:\n         \"to_upper\", \"to_lower\", \"to_title\",\n \n-        # decompositions: canonical decompositions, compatibility decomp\n+        # Decompositions: canonical decompositions, compatibility decomp\n         \"canon_decomp\", \"compat_decomp\",\n \n-        # grouped: general categories and combining characters\n+        # Grouped: general categories and combining characters\n         \"general_categories\", \"combines\",\n     )\n )\n@@ -136,10 +139,10 @@ def fetch_files(version=None):\n         return have_version\n \n     if version:\n-        # check if the desired version exists on the server\n+        # Check if the desired version exists on the server.\n         get_fetch_url = lambda name: FETCH_URL_VERSION.format(version=version, filename=name)\n     else:\n-        # extract the latest version\n+        # Extract the latest version.\n         get_fetch_url = lambda name: FETCH_URL_LATEST.format(filename=name)\n \n     readme_url = get_fetch_url(UnicodeFiles.README)\n@@ -153,14 +156,14 @@ def fetch_files(version=None):\n \n     download_dir = get_unicode_dir(unicode_version)\n     if not os.path.exists(download_dir):\n-        # for 2.7 compat, we don't use exist_ok=True\n+        # For 2.7 compat, we don't use `exist_ok=True`.\n         os.makedirs(download_dir)\n \n-    for filename in UnicodeFiles.ALL_FILES:\n+    for filename in ALL_UNICODE_FILES:\n         file_path = get_unicode_file_path(unicode_version, filename)\n \n         if os.path.exists(file_path):\n-            # assume file on the server didn't change if it's been saved before\n+            # Assume file on the server didn't change if it's been saved before.\n             continue\n \n         if filename == UnicodeFiles.README:\n@@ -178,15 +181,16 @@ def check_stored_version(version):\n     # type: (Optional[str]) -> Optional[UnicodeVersion]\n     \"\"\"\n     Given desired Unicode version, return the version\n-    if stored files are all present, and None otherwise.\n+    if stored files are all present, and `None` otherwise.\n     \"\"\"\n     if not version:\n-        # should always check latest version\n+        # If no desired version specified, we should check what's the latest\n+        # version, skipping stored version checks.\n         return None\n \n     fetch_dir = os.path.join(FETCH_DIR, version)\n \n-    for filename in UnicodeFiles.ALL_FILES:\n+    for filename in ALL_UNICODE_FILES:\n         file_path = os.path.join(fetch_dir, filename)\n \n         if not os.path.exists(file_path):\n@@ -199,11 +203,11 @@ def check_stored_version(version):\n def parse_readme_unicode_version(readme_content):\n     # type: (str) -> UnicodeVersion\n     \"\"\"\n-    Parse the Unicode version contained in their ReadMe.txt file.\n+    Parse the Unicode version contained in their `ReadMe.txt` file.\n     \"\"\"\n-    # \"raw string\" is necessary for \\d not being treated as escape char\n-    # (for the sake of compat with future Python versions)\n-    # see: https://docs.python.org/3.6/whatsnew/3.6.html#deprecated-python-behavior\n+    # \"Raw string\" is necessary for \\d not being treated as escape char\n+    # (for the sake of compat with future Python versions).\n+    # See: https://docs.python.org/3.6/whatsnew/3.6.html#deprecated-python-behavior\n     pattern = r\"for Version (\\d+)\\.(\\d+)\\.(\\d+) of the Unicode\"\n     groups = re.search(pattern, readme_content).groups()\n \n@@ -213,7 +217,7 @@ def parse_readme_unicode_version(readme_content):\n def get_unicode_dir(unicode_version):\n     # type: (UnicodeVersion) -> str\n     \"\"\"\n-    Indicate where the unicode data files should be stored.\n+    Indicate in which parent dir the Unicode data files should be stored.\n \n     This returns a full, absolute path.\n     \"\"\"\n@@ -223,7 +227,7 @@ def get_unicode_dir(unicode_version):\n def get_unicode_file_path(unicode_version, filename):\n     # type: (UnicodeVersion, str) -> str\n     \"\"\"\n-    Indicate where the unicode data file should be stored.\n+    Indicate where the Unicode data file should be stored.\n     \"\"\"\n     return os.path.join(get_unicode_dir(unicode_version), filename)\n \n@@ -239,22 +243,22 @@ def is_surrogate(n):\n def load_unicode_data(file_path):\n     # type: (str) -> UnicodeData\n     \"\"\"\n-    Load main unicode data.\n+    Load main Unicode data.\n     \"\"\"\n-    # conversions\n+    # Conversions\n     to_lower = {}   # type: Dict[int, Tuple[int, int, int]]\n     to_upper = {}   # type: Dict[int, Tuple[int, int, int]]\n     to_title = {}   # type: Dict[int, Tuple[int, int, int]]\n \n-    # decompositions\n+    # Decompositions\n     compat_decomp = {}   # type: Dict[int, List[int]]\n     canon_decomp = {}    # type: Dict[int, List[int]]\n \n-    # combining characters\n+    # Combining characters\n     # FIXME: combines are not used\n     combines = defaultdict(set)   # type: Dict[str, Set[int]]\n \n-    # categories\n+    # Categories\n     general_categories = defaultdict(set)   # type: Dict[str, Set[int]]\n     category_assigned_codepoints = set()    # type: Set[int]\n \n@@ -283,41 +287,42 @@ def load_unicode_data(file_path):\n          decomp, deci, digit, num, mirror,\n          old, iso, upcase, lowcase, titlecase) = data\n \n-        # generate char to char direct common and simple conversions\n-        # uppercase to lowercase\n+        # Generate char to char direct common and simple conversions:\n+\n+        # Uppercase to lowercase\n         if lowcase != \"\" and code_org != lowcase:\n             to_lower[code] = (int(lowcase, 16), 0, 0)\n \n-        # lowercase to uppercase\n+        # Lowercase to uppercase\n         if upcase != \"\" and code_org != upcase:\n             to_upper[code] = (int(upcase, 16), 0, 0)\n \n-        # title case\n+        # Title case\n         if titlecase.strip() != \"\" and code_org != titlecase:\n             to_title[code] = (int(titlecase, 16), 0, 0)\n \n-        # store decomposition, if given\n+        # Store decomposition, if given\n         if decomp:\n             decompositions = decomp.split()[1:]\n             decomp_code_points = [int(i, 16) for i in decompositions]\n \n             if decomp.startswith(\"<\"):\n-                # compatibility decomposition\n+                # Compatibility decomposition\n                 compat_decomp[code] = decomp_code_points\n             else:\n-                # canonical decomposition\n+                # Canonical decomposition\n                 canon_decomp[code] = decomp_code_points\n \n-        # place letter in categories as appropriate\n+        # Place letter in categories as appropriate.\n         for cat in itertools.chain((gencat, ), EXPANDED_CATEGORIES.get(gencat, [])):\n             general_categories[cat].add(code)\n             category_assigned_codepoints.add(code)\n \n-        # record combining class, if any\n+        # Record combining class, if any.\n         if combine != \"0\":\n             combines[combine].add(code)\n \n-    # generate Not_Assigned from Assigned\n+    # Generate Not_Assigned from Assigned.\n     general_categories[\"Cn\"] = get_unassigned_codepoints(category_assigned_codepoints)\n \n     # Other contains Not_Assigned\n@@ -336,7 +341,7 @@ def load_unicode_data(file_path):\n def load_special_casing(file_path, unicode_data):\n     # type: (str, UnicodeData) -> None\n     \"\"\"\n-    Load special casing data and enrich given unicode data.\n+    Load special casing data and enrich given Unicode data.\n     \"\"\"\n     for line in fileinput.input(file_path):\n         data = line.split(\"#\")[0].split(\";\")\n@@ -474,9 +479,9 @@ def load_properties(file_path, interesting_props):\n     Load properties data and return in grouped form.\n     \"\"\"\n     props = defaultdict(list)   # type: Dict[str, List[Tuple[int, int]]]\n-    # \"raw string\" is necessary for \\. and \\w not to be treated as escape chars\n-    # (for the sake of compat with future Python versions)\n-    # see: https://docs.python.org/3.6/whatsnew/3.6.html#deprecated-python-behavior\n+    # \"Raw string\" is necessary for `\\.` and `\\w` not to be treated as escape chars\n+    # (for the sake of compat with future Python versions).\n+    # See: https://docs.python.org/3.6/whatsnew/3.6.html#deprecated-python-behavior\n     re1 = re.compile(r\"^ *([0-9A-F]+) *; *(\\w+)\")\n     re2 = re.compile(r\"^ *([0-9A-F]+)\\.\\.([0-9A-F]+) *; *(\\w+)\")\n \n@@ -486,7 +491,7 @@ def load_properties(file_path, interesting_props):\n             groups = match.groups()\n \n             if len(groups) == 2:\n-                # re1 matched\n+                # `re1` matched (2 groups).\n                 d_lo, prop = groups\n                 d_hi = d_lo\n             else:\n@@ -502,7 +507,7 @@ def load_properties(file_path, interesting_props):\n \n         props[prop].append((lo_value, hi_value))\n \n-    # optimize if possible\n+    # Optimize if possible.\n     for prop in props:\n         props[prop] = group_codepoints(ungroup_codepoints(props[prop]))\n \n@@ -587,10 +592,10 @@ def compute_trie(raw_data, chunk_size):\n     for i in range(len(raw_data) // chunk_size):\n         data = raw_data[i * chunk_size : (i + 1) * chunk_size]\n \n-        # postfix compression of child nodes (data chunks)\n-        # (identical child nodes are shared)\n+        # Postfix compression of child nodes (data chunks)\n+        # (identical child nodes are shared).\n \n-        # make a tuple out of the list so it's hashable\n+        # Make a tuple out of the list so it's hashable.\n         child = tuple(data)\n         if child not in childmap:\n             childmap[child] = len(childmap)\n@@ -609,15 +614,15 @@ def generate_bool_trie(name, codepoint_ranges, is_pub=True):\n     This yields string fragments that should be joined to produce\n     the final string.\n \n-    See: bool_trie.rs\n+    See: `bool_trie.rs`.\n     \"\"\"\n     chunk_size = 64\n     rawdata = [False] * 0x110000\n     for (lo, hi) in codepoint_ranges:\n         for cp in range(lo, hi + 1):\n             rawdata[cp] = True\n \n-    # convert to bitmap chunks of chunk_size bits each\n+    # Convert to bitmap chunks of `chunk_size` bits each.\n     chunks = []\n     for i in range(0x110000 // chunk_size):\n         chunk = 0\n@@ -679,9 +684,9 @@ def generate_bool_trie(name, codepoint_ranges, is_pub=True):\n def generate_small_bool_trie(name, codepoint_ranges, is_pub=True):\n     # type: (str, List[Tuple[int, int]], bool) -> Iterator[str]\n     \"\"\"\n-    Generate Rust code for SmallBoolTrie struct.\n+    Generate Rust code for `SmallBoolTrie` struct.\n \n-    See: bool_trie.rs\n+    See: `bool_trie.rs`.\n     \"\"\"\n     last_chunk = max(hi // 64 for (lo, hi) in codepoint_ranges)\n     n_chunks = last_chunk + 1\n@@ -813,8 +818,8 @@ def main():\n     unicode_version = fetch_files(args.version)\n     print(\"Using Unicode version: {}\".format(unicode_version.as_str))\n \n-    # all the writing happens entirely in memory, we only write to file\n-    # once we have generated the file content (it's not very large, <1 MB)\n+    # All the writing happens entirely in memory, we only write to file\n+    # once we have generated the file content (it's not very large, <1 MB).\n     buf = StringIO()\n     buf.write(PREAMBLE)\n \n@@ -844,7 +849,7 @@ def main():\n                             {\"White_Space\", \"Join_Control\", \"Noncharacter_Code_Point\",\n                              \"Pattern_White_Space\"})\n \n-    # category tables\n+    # Category tables\n     for (name, categories, category_subset) in (\n             (\"general_category\", unicode_data.general_categories, [\"N\", \"Cc\"]),\n             (\"derived_property\", derived, want_derived),\n@@ -858,7 +863,8 @@ def main():\n \n     tables_rs_path = os.path.join(THIS_DIR, \"tables.rs\")\n \n-    # will overwrite the file if it exists\n+    # Actually write out the file content.\n+    # Will overwrite the file if it exists.\n     with open(tables_rs_path, \"w\") as fd:\n         fd.write(buf.getvalue())\n "}]}