{"sha": "6b71fba9c1371a46e94917fabe042b6e3a14cf0a", "node_id": "MDY6Q29tbWl0NzI0NzEyOjZiNzFmYmE5YzEzNzFhNDZlOTQ5MTdmYWJlMDQyYjZlM2ExNGNmMGE=", "commit": {"author": {"name": "Mazdak Farrokhzad", "email": "twingoow@gmail.com", "date": "2019-06-08T22:20:38Z"}, "committer": {"name": "GitHub", "email": "noreply@github.com", "date": "2019-06-08T22:20:38Z"}, "message": "Rollup merge of #61669 - petrochenkov:tokderef2, r=oli-obk\n\n syntax: Remove `Deref` impl from `Token`\n\nFollow up to https://github.com/rust-lang/rust/pull/61541\n\nr? @oli-obk", "tree": {"sha": "8ac3e87f2d3fec1a4cff83a44b5ed18148a62abf", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/8ac3e87f2d3fec1a4cff83a44b5ed18148a62abf"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/6b71fba9c1371a46e94917fabe042b6e3a14cf0a", "comment_count": 0, "verification": {"verified": true, "reason": "valid", "signature": "-----BEGIN PGP SIGNATURE-----\n\nwsBcBAABCAAQBQJc/DS2CRBK7hj4Ov3rIwAAdHIIAIER4fuHiaI+lcXnb1vKTHiR\nW1+08Fww+GD4UHzllEhm1V6futmU13Zaz2o4M3IyqOhRSRilyhNv7rkTu9XtpHs8\nA7XL8p1dImnTnZB8n09pbpkezsbYfeR0hwyTuUEAmjPb8n+lVuyr1lmcFzLn6ts3\nUXQYyfPMigGXXQ/BAe6jU4pXvsX7F0FBOToEqVaKoco/RdQWNb12vglPQOaqQpum\nemOuWg63GjH9wqICR9fENemOcLKVIwC36DByFNt/C5haROHPWB+0TEZxSaMCLL4y\nQjXiS29qTE4HS7B3CrckS9ycubfynjp09Nbc8SXH7AGGAnGXPZihYWh4jI2EcNs=\n=PaGc\n-----END PGP SIGNATURE-----\n", "payload": "tree 8ac3e87f2d3fec1a4cff83a44b5ed18148a62abf\nparent 18ca48d746c345f780914cc85d6049ff9a179635\nparent 9aaa7c770c976e35b1614f1f6bf120204c5727c8\nauthor Mazdak Farrokhzad <twingoow@gmail.com> 1560032438 +0200\ncommitter GitHub <noreply@github.com> 1560032438 +0200\n\nRollup merge of #61669 - petrochenkov:tokderef2, r=oli-obk\n\n syntax: Remove `Deref` impl from `Token`\n\nFollow up to https://github.com/rust-lang/rust/pull/61541\n\nr? @oli-obk\n"}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/6b71fba9c1371a46e94917fabe042b6e3a14cf0a", "html_url": "https://github.com/rust-lang/rust/commit/6b71fba9c1371a46e94917fabe042b6e3a14cf0a", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/6b71fba9c1371a46e94917fabe042b6e3a14cf0a/comments", "author": {"login": "Centril", "id": 855702, "node_id": "MDQ6VXNlcjg1NTcwMg==", "avatar_url": "https://avatars.githubusercontent.com/u/855702?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Centril", "html_url": "https://github.com/Centril", "followers_url": "https://api.github.com/users/Centril/followers", "following_url": "https://api.github.com/users/Centril/following{/other_user}", "gists_url": "https://api.github.com/users/Centril/gists{/gist_id}", "starred_url": "https://api.github.com/users/Centril/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Centril/subscriptions", "organizations_url": "https://api.github.com/users/Centril/orgs", "repos_url": "https://api.github.com/users/Centril/repos", "events_url": "https://api.github.com/users/Centril/events{/privacy}", "received_events_url": "https://api.github.com/users/Centril/received_events", "type": "User", "site_admin": false}, "committer": {"login": "web-flow", "id": 19864447, "node_id": "MDQ6VXNlcjE5ODY0NDQ3", "avatar_url": "https://avatars.githubusercontent.com/u/19864447?v=4", "gravatar_id": "", "url": "https://api.github.com/users/web-flow", "html_url": "https://github.com/web-flow", "followers_url": "https://api.github.com/users/web-flow/followers", "following_url": "https://api.github.com/users/web-flow/following{/other_user}", "gists_url": "https://api.github.com/users/web-flow/gists{/gist_id}", "starred_url": "https://api.github.com/users/web-flow/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/web-flow/subscriptions", "organizations_url": "https://api.github.com/users/web-flow/orgs", "repos_url": "https://api.github.com/users/web-flow/repos", "events_url": "https://api.github.com/users/web-flow/events{/privacy}", "received_events_url": "https://api.github.com/users/web-flow/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "18ca48d746c345f780914cc85d6049ff9a179635", "url": "https://api.github.com/repos/rust-lang/rust/commits/18ca48d746c345f780914cc85d6049ff9a179635", "html_url": "https://github.com/rust-lang/rust/commit/18ca48d746c345f780914cc85d6049ff9a179635"}, {"sha": "9aaa7c770c976e35b1614f1f6bf120204c5727c8", "url": "https://api.github.com/repos/rust-lang/rust/commits/9aaa7c770c976e35b1614f1f6bf120204c5727c8", "html_url": "https://github.com/rust-lang/rust/commit/9aaa7c770c976e35b1614f1f6bf120204c5727c8"}], "stats": {"total": 394, "additions": 163, "deletions": 231}, "files": [{"sha": "99ca8c43cfbe202b577e848f0752d03d83aad3f1", "filename": "src/librustdoc/html/highlight.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/6b71fba9c1371a46e94917fabe042b6e3a14cf0a/src%2Flibrustdoc%2Fhtml%2Fhighlight.rs", "raw_url": "https://github.com/rust-lang/rust/raw/6b71fba9c1371a46e94917fabe042b6e3a14cf0a/src%2Flibrustdoc%2Fhtml%2Fhighlight.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustdoc%2Fhtml%2Fhighlight.rs?ref=6b71fba9c1371a46e94917fabe042b6e3a14cf0a", "patch": "@@ -257,7 +257,7 @@ impl<'a> Classifier<'a> {\n             token::Question => Class::QuestionMark,\n \n             token::Dollar => {\n-                if self.lexer.peek().kind.is_ident() {\n+                if self.lexer.peek().is_ident() {\n                     self.in_macro_nonterminal = true;\n                     Class::MacroNonTerminal\n                 } else {"}, {"sha": "d7e43f645df7bbb05fd3fa27c90bcae5ea935ebb", "filename": "src/libsyntax/attr/mod.rs", "status": "modified", "additions": 2, "deletions": 3, "changes": 5, "blob_url": "https://github.com/rust-lang/rust/blob/6b71fba9c1371a46e94917fabe042b6e3a14cf0a/src%2Flibsyntax%2Fattr%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/6b71fba9c1371a46e94917fabe042b6e3a14cf0a/src%2Flibsyntax%2Fattr%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fattr%2Fmod.rs?ref=6b71fba9c1371a46e94917fabe042b6e3a14cf0a", "patch": "@@ -20,7 +20,7 @@ use crate::source_map::{BytePos, Spanned, dummy_spanned};\n use crate::parse::lexer::comments::{doc_comment_style, strip_doc_comment_decoration};\n use crate::parse::parser::Parser;\n use crate::parse::{self, ParseSess, PResult};\n-use crate::parse::token::{self, Token, TokenKind};\n+use crate::parse::token::{self, Token};\n use crate::ptr::P;\n use crate::symbol::{sym, Symbol};\n use crate::ThinVec;\n@@ -467,8 +467,7 @@ impl MetaItem {\n                                              segment.ident.span.ctxt());\n                 idents.push(TokenTree::token(token::ModSep, mod_sep_span).into());\n             }\n-            idents.push(TokenTree::token(TokenKind::from_ast_ident(segment.ident),\n-                                         segment.ident.span).into());\n+            idents.push(TokenTree::Token(Token::from_ast_ident(segment.ident)).into());\n             last_pos = segment.ident.span.hi();\n         }\n         self.node.tokens(self.span).append_to_tree_and_joint_vec(&mut idents);"}, {"sha": "4758b6a50e520f8a4f3c5b9ff3e77b16a0fe65fa", "filename": "src/libsyntax/ext/tt/macro_parser.rs", "status": "modified", "additions": 14, "deletions": 14, "changes": 28, "blob_url": "https://github.com/rust-lang/rust/blob/6b71fba9c1371a46e94917fabe042b6e3a14cf0a/src%2Flibsyntax%2Fext%2Ftt%2Fmacro_parser.rs", "raw_url": "https://github.com/rust-lang/rust/raw/6b71fba9c1371a46e94917fabe042b6e3a14cf0a/src%2Flibsyntax%2Fext%2Ftt%2Fmacro_parser.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fext%2Ftt%2Fmacro_parser.rs?ref=6b71fba9c1371a46e94917fabe042b6e3a14cf0a", "patch": "@@ -78,7 +78,7 @@ use crate::ast::{Ident, Name};\n use crate::ext::tt::quoted::{self, TokenTree};\n use crate::parse::{Directory, ParseSess};\n use crate::parse::parser::{Parser, PathStyle};\n-use crate::parse::token::{self, DocComment, Nonterminal, Token, TokenKind};\n+use crate::parse::token::{self, DocComment, Nonterminal, Token};\n use crate::print::pprust;\n use crate::symbol::{kw, sym, Symbol};\n use crate::tokenstream::{DelimSpan, TokenStream};\n@@ -199,7 +199,7 @@ struct MatcherPos<'root, 'tt: 'root> {\n     seq_op: Option<quoted::KleeneOp>,\n \n     /// The separator if we are in a repetition.\n-    sep: Option<TokenKind>,\n+    sep: Option<Token>,\n \n     /// The \"parent\" matcher position if we are in a repetition. That is, the matcher position just\n     /// before we enter the sequence.\n@@ -417,24 +417,24 @@ fn nameize<I: Iterator<Item = NamedMatch>>(\n \n /// Generates an appropriate parsing failure message. For EOF, this is \"unexpected end...\". For\n /// other tokens, this is \"unexpected token...\".\n-pub fn parse_failure_msg(tok: TokenKind) -> String {\n-    match tok {\n+pub fn parse_failure_msg(tok: &Token) -> String {\n+    match tok.kind {\n         token::Eof => \"unexpected end of macro invocation\".to_string(),\n         _ => format!(\n             \"no rules expected the token `{}`\",\n-            pprust::token_to_string(&tok)\n+            pprust::token_to_string(tok)\n         ),\n     }\n }\n \n /// Performs a token equality check, ignoring syntax context (that is, an unhygienic comparison)\n-fn token_name_eq(t1: &TokenKind, t2: &TokenKind) -> bool {\n-    if let (Some((name1, is_raw1)), Some((name2, is_raw2))) = (t1.ident_name(), t2.ident_name()) {\n-        name1 == name2 && is_raw1 == is_raw2\n-    } else if let (Some(name1), Some(name2)) = (t1.lifetime_name(), t2.lifetime_name()) {\n-        name1 == name2\n+fn token_name_eq(t1: &Token, t2: &Token) -> bool {\n+    if let (Some((ident1, is_raw1)), Some((ident2, is_raw2))) = (t1.ident(), t2.ident()) {\n+        ident1.name == ident2.name && is_raw1 == is_raw2\n+    } else if let (Some(ident1), Some(ident2)) = (t1.lifetime(), t2.lifetime()) {\n+        ident1.name == ident2.name\n     } else {\n-        *t1 == *t2\n+        t1.kind == t2.kind\n     }\n }\n \n@@ -712,7 +712,7 @@ pub fn parse(\n \n         // If we reached the EOF, check that there is EXACTLY ONE possible matcher. Otherwise,\n         // either the parse is ambiguous (which should never happen) or there is a syntax error.\n-        if token_name_eq(&parser.token, &token::Eof) {\n+        if parser.token == token::Eof {\n             if eof_items.len() == 1 {\n                 let matches = eof_items[0]\n                     .matches\n@@ -804,8 +804,8 @@ pub fn parse(\n \n /// The token is an identifier, but not `_`.\n /// We prohibit passing `_` to macros expecting `ident` for now.\n-fn get_macro_name(token: &TokenKind) -> Option<(Name, bool)> {\n-    match *token {\n+fn get_macro_name(token: &Token) -> Option<(Name, bool)> {\n+    match token.kind {\n         token::Ident(name, is_raw) if name != kw::Underscore => Some((name, is_raw)),\n         _ => None,\n     }"}, {"sha": "6f82f5094651ea5c4beca1f5bf0f18d3e3897b0b", "filename": "src/libsyntax/ext/tt/macro_rules.rs", "status": "modified", "additions": 17, "deletions": 17, "changes": 34, "blob_url": "https://github.com/rust-lang/rust/blob/6b71fba9c1371a46e94917fabe042b6e3a14cf0a/src%2Flibsyntax%2Fext%2Ftt%2Fmacro_rules.rs", "raw_url": "https://github.com/rust-lang/rust/raw/6b71fba9c1371a46e94917fabe042b6e3a14cf0a/src%2Flibsyntax%2Fext%2Ftt%2Fmacro_rules.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fext%2Ftt%2Fmacro_rules.rs?ref=6b71fba9c1371a46e94917fabe042b6e3a14cf0a", "patch": "@@ -17,7 +17,7 @@ use crate::symbol::{Symbol, kw, sym};\n use crate::tokenstream::{DelimSpan, TokenStream, TokenTree};\n \n use errors::FatalError;\n-use syntax_pos::{Span, DUMMY_SP, symbol::Ident};\n+use syntax_pos::{Span, symbol::Ident};\n use log::debug;\n \n use rustc_data_structures::fx::{FxHashMap};\n@@ -200,7 +200,7 @@ fn generic_extension<'cx>(cx: &'cx mut ExtCtxt<'_>,\n \n     let (token, label) = best_failure.expect(\"ran no matchers\");\n     let span = token.span.substitute_dummy(sp);\n-    let mut err = cx.struct_span_err(span, &parse_failure_msg(token.kind));\n+    let mut err = cx.struct_span_err(span, &parse_failure_msg(&token));\n     err.span_label(span, label);\n     if let Some(sp) = def_span {\n         if cx.source_map().span_to_filename(sp).is_real() && !sp.is_dummy() {\n@@ -266,17 +266,19 @@ pub fn compile(\n     let argument_gram = vec![\n         quoted::TokenTree::Sequence(DelimSpan::dummy(), Lrc::new(quoted::SequenceRepetition {\n             tts: vec![\n-                quoted::TokenTree::MetaVarDecl(DUMMY_SP, lhs_nm, ast::Ident::from_str(\"tt\")),\n-                quoted::TokenTree::token(token::FatArrow, DUMMY_SP),\n-                quoted::TokenTree::MetaVarDecl(DUMMY_SP, rhs_nm, ast::Ident::from_str(\"tt\")),\n+                quoted::TokenTree::MetaVarDecl(def.span, lhs_nm, ast::Ident::from_str(\"tt\")),\n+                quoted::TokenTree::token(token::FatArrow, def.span),\n+                quoted::TokenTree::MetaVarDecl(def.span, rhs_nm, ast::Ident::from_str(\"tt\")),\n             ],\n-            separator: Some(if body.legacy { token::Semi } else { token::Comma }),\n+            separator: Some(Token::new(\n+                if body.legacy { token::Semi } else { token::Comma }, def.span\n+            )),\n             op: quoted::KleeneOp::OneOrMore,\n             num_captures: 2,\n         })),\n         // to phase into semicolon-termination instead of semicolon-separation\n         quoted::TokenTree::Sequence(DelimSpan::dummy(), Lrc::new(quoted::SequenceRepetition {\n-            tts: vec![quoted::TokenTree::token(token::Semi, DUMMY_SP)],\n+            tts: vec![quoted::TokenTree::token(token::Semi, def.span)],\n             separator: None,\n             op: quoted::KleeneOp::ZeroOrMore,\n             num_captures: 0\n@@ -286,7 +288,7 @@ pub fn compile(\n     let argument_map = match parse(sess, body.stream(), &argument_gram, None, true) {\n         Success(m) => m,\n         Failure(token, msg) => {\n-            let s = parse_failure_msg(token.kind);\n+            let s = parse_failure_msg(&token);\n             let sp = token.span.substitute_dummy(def.span);\n             let mut err = sess.span_diagnostic.struct_span_fatal(sp, &s);\n             err.span_label(sp, msg);\n@@ -608,9 +610,8 @@ impl FirstSets {\n                         // If the sequence contents can be empty, then the first\n                         // token could be the separator token itself.\n \n-                        if let (Some(ref sep), true) = (seq_rep.separator.clone(),\n-                                                        subfirst.maybe_empty) {\n-                            first.add_one_maybe(TokenTree::token(sep.clone(), sp.entire()));\n+                        if let (Some(sep), true) = (&seq_rep.separator, subfirst.maybe_empty) {\n+                            first.add_one_maybe(TokenTree::Token(sep.clone()));\n                         }\n \n                         // Reverse scan: Sequence comes before `first`.\n@@ -658,9 +659,8 @@ impl FirstSets {\n                             // If the sequence contents can be empty, then the first\n                             // token could be the separator token itself.\n \n-                            if let (Some(ref sep), true) = (seq_rep.separator.clone(),\n-                                                            subfirst.maybe_empty) {\n-                                first.add_one_maybe(TokenTree::token(sep.clone(), sp.entire()));\n+                            if let (Some(sep), true) = (&seq_rep.separator, subfirst.maybe_empty) {\n+                                first.add_one_maybe(TokenTree::Token(sep.clone()));\n                             }\n \n                             assert!(first.maybe_empty);\n@@ -851,7 +851,7 @@ fn check_matcher_core(sess: &ParseSess,\n                 // against SUFFIX\n                 continue 'each_token;\n             }\n-            TokenTree::Sequence(sp, ref seq_rep) => {\n+            TokenTree::Sequence(_, ref seq_rep) => {\n                 suffix_first = build_suffix_first();\n                 // The trick here: when we check the interior, we want\n                 // to include the separator (if any) as a potential\n@@ -864,9 +864,9 @@ fn check_matcher_core(sess: &ParseSess,\n                 // work of cloning it? But then again, this way I may\n                 // get a \"tighter\" span?\n                 let mut new;\n-                let my_suffix = if let Some(ref u) = seq_rep.separator {\n+                let my_suffix = if let Some(sep) = &seq_rep.separator {\n                     new = suffix_first.clone();\n-                    new.add_one_maybe(TokenTree::token(u.clone(), sp.entire()));\n+                    new.add_one_maybe(TokenTree::Token(sep.clone()));\n                     &new\n                 } else {\n                     &suffix_first"}, {"sha": "707fb65bcc52bfb363c18689714a81d449456a92", "filename": "src/libsyntax/ext/tt/quoted.rs", "status": "modified", "additions": 12, "deletions": 22, "changes": 34, "blob_url": "https://github.com/rust-lang/rust/blob/6b71fba9c1371a46e94917fabe042b6e3a14cf0a/src%2Flibsyntax%2Fext%2Ftt%2Fquoted.rs", "raw_url": "https://github.com/rust-lang/rust/raw/6b71fba9c1371a46e94917fabe042b6e3a14cf0a/src%2Flibsyntax%2Fext%2Ftt%2Fquoted.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fext%2Ftt%2Fquoted.rs?ref=6b71fba9c1371a46e94917fabe042b6e3a14cf0a", "patch": "@@ -23,24 +23,14 @@ pub struct Delimited {\n }\n \n impl Delimited {\n-    /// Returns the opening delimiter (possibly `NoDelim`).\n-    pub fn open_token(&self) -> TokenKind {\n-        token::OpenDelim(self.delim)\n-    }\n-\n-    /// Returns the closing delimiter (possibly `NoDelim`).\n-    pub fn close_token(&self) -> TokenKind {\n-        token::CloseDelim(self.delim)\n-    }\n-\n     /// Returns a `self::TokenTree` with a `Span` corresponding to the opening delimiter.\n     pub fn open_tt(&self, span: Span) -> TokenTree {\n         let open_span = if span.is_dummy() {\n             span\n         } else {\n             span.with_lo(span.lo() + BytePos(self.delim.len() as u32))\n         };\n-        TokenTree::token(self.open_token(), open_span)\n+        TokenTree::token(token::OpenDelim(self.delim), open_span)\n     }\n \n     /// Returns a `self::TokenTree` with a `Span` corresponding to the closing delimiter.\n@@ -50,7 +40,7 @@ impl Delimited {\n         } else {\n             span.with_lo(span.hi() - BytePos(self.delim.len() as u32))\n         };\n-        TokenTree::token(self.close_token(), close_span)\n+        TokenTree::token(token::CloseDelim(self.delim), close_span)\n     }\n }\n \n@@ -59,7 +49,7 @@ pub struct SequenceRepetition {\n     /// The sequence of token trees\n     pub tts: Vec<TokenTree>,\n     /// The optional separator\n-    pub separator: Option<TokenKind>,\n+    pub separator: Option<Token>,\n     /// Whether the sequence can be repeated zero (*), or one or more times (+)\n     pub op: KleeneOp,\n     /// The number of `Match`s that appear in the sequence (and subsequences)\n@@ -282,7 +272,7 @@ where\n             Some(tokenstream::TokenTree::Delimited(span, delim, tts)) => {\n                 // Must have `(` not `{` or `[`\n                 if delim != token::Paren {\n-                    let tok = pprust::token_to_string(&token::OpenDelim(delim));\n+                    let tok = pprust::token_kind_to_string(&token::OpenDelim(delim));\n                     let msg = format!(\"expected `(`, found `{}`\", tok);\n                     sess.span_diagnostic.span_err(span.entire(), &msg);\n                 }\n@@ -371,8 +361,8 @@ where\n \n /// Takes a token and returns `Some(KleeneOp)` if the token is `+` `*` or `?`. Otherwise, return\n /// `None`.\n-fn kleene_op(token: &TokenKind) -> Option<KleeneOp> {\n-    match *token {\n+fn kleene_op(token: &Token) -> Option<KleeneOp> {\n+    match token.kind {\n         token::BinOp(token::Star) => Some(KleeneOp::ZeroOrMore),\n         token::BinOp(token::Plus) => Some(KleeneOp::OneOrMore),\n         token::Question => Some(KleeneOp::ZeroOrOne),\n@@ -424,7 +414,7 @@ fn parse_sep_and_kleene_op<I>(\n     attrs: &[ast::Attribute],\n     edition: Edition,\n     macro_node_id: NodeId,\n-) -> (Option<TokenKind>, KleeneOp)\n+) -> (Option<Token>, KleeneOp)\n where\n     I: Iterator<Item = tokenstream::TokenTree>,\n {\n@@ -449,7 +439,7 @@ fn parse_sep_and_kleene_op_2015<I>(\n     _features: &Features,\n     _attrs: &[ast::Attribute],\n     macro_node_id: NodeId,\n-) -> (Option<TokenKind>, KleeneOp)\n+) -> (Option<Token>, KleeneOp)\n where\n     I: Iterator<Item = tokenstream::TokenTree>,\n {\n@@ -502,7 +492,7 @@ where\n                              a hard error in an upcoming edition\",\n                         );\n \n-                        return (Some(token::Question), op);\n+                        return (Some(Token::new(token::Question, op1_span)), op);\n                     }\n \n                     // #2 is a random token (this is an error) :(\n@@ -541,7 +531,7 @@ where\n             }\n \n             // #2 is a KleeneOp :D\n-            Ok(Ok((op, _))) => return (Some(token.kind), op),\n+            Ok(Ok((op, _))) => return (Some(token), op),\n \n             // #2 is a random token :(\n             Ok(Err(token)) => token.span,\n@@ -567,7 +557,7 @@ fn parse_sep_and_kleene_op_2018<I>(\n     sess: &ParseSess,\n     _features: &Features,\n     _attrs: &[ast::Attribute],\n-) -> (Option<TokenKind>, KleeneOp)\n+) -> (Option<Token>, KleeneOp)\n where\n     I: Iterator<Item = tokenstream::TokenTree>,\n {\n@@ -596,7 +586,7 @@ where\n             }\n \n             // #2 is a KleeneOp :D\n-            Ok(Ok((op, _))) => return (Some(token.kind), op),\n+            Ok(Ok((op, _))) => return (Some(token), op),\n \n             // #2 is a random token :(\n             Ok(Err(token)) => token.span,"}, {"sha": "c51f4b20c31c04689e7d2f5f68606adf259450f5", "filename": "src/libsyntax/ext/tt/transcribe.rs", "status": "modified", "additions": 8, "deletions": 13, "changes": 21, "blob_url": "https://github.com/rust-lang/rust/blob/6b71fba9c1371a46e94917fabe042b6e3a14cf0a/src%2Flibsyntax%2Fext%2Ftt%2Ftranscribe.rs", "raw_url": "https://github.com/rust-lang/rust/raw/6b71fba9c1371a46e94917fabe042b6e3a14cf0a/src%2Flibsyntax%2Fext%2Ftt%2Ftranscribe.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fext%2Ftt%2Ftranscribe.rs?ref=6b71fba9c1371a46e94917fabe042b6e3a14cf0a", "patch": "@@ -4,11 +4,10 @@ use crate::ext::expand::Marker;\n use crate::ext::tt::macro_parser::{MatchedNonterminal, MatchedSeq, NamedMatch};\n use crate::ext::tt::quoted;\n use crate::mut_visit::noop_visit_tt;\n-use crate::parse::token::{self, NtTT, TokenKind};\n+use crate::parse::token::{self, NtTT, Token};\n use crate::tokenstream::{DelimSpan, TokenStream, TokenTree, TreeAndJoint};\n \n use smallvec::{smallvec, SmallVec};\n-use syntax_pos::DUMMY_SP;\n \n use rustc_data_structures::fx::FxHashMap;\n use rustc_data_structures::sync::Lrc;\n@@ -18,7 +17,7 @@ use std::rc::Rc;\n /// An iterator over the token trees in a delimited token tree (`{ ... }`) or a sequence (`$(...)`).\n enum Frame {\n     Delimited { forest: Lrc<quoted::Delimited>, idx: usize, span: DelimSpan },\n-    Sequence { forest: Lrc<quoted::SequenceRepetition>, idx: usize, sep: Option<TokenKind> },\n+    Sequence { forest: Lrc<quoted::SequenceRepetition>, idx: usize, sep: Option<Token> },\n }\n \n impl Frame {\n@@ -109,17 +108,13 @@ pub fn transcribe(\n         else {\n             // Otherwise, if we have just reached the end of a sequence and we can keep repeating,\n             // go back to the beginning of the sequence.\n-            if let Frame::Sequence { ref mut idx, ref sep, .. } = *stack.last_mut().unwrap() {\n-                let (ref mut repeat_idx, repeat_len) = *repeats.last_mut().unwrap();\n+            if let Frame::Sequence { idx, sep, .. } = stack.last_mut().unwrap() {\n+                let (repeat_idx, repeat_len) = repeats.last_mut().unwrap();\n                 *repeat_idx += 1;\n-                if *repeat_idx < repeat_len {\n+                if repeat_idx < repeat_len {\n                     *idx = 0;\n-                    if let Some(sep) = sep.clone() {\n-                        let prev_span = match result.last() {\n-                            Some((tt, _)) => tt.span(),\n-                            None => DUMMY_SP,\n-                        };\n-                        result.push(TokenTree::token(sep, prev_span).into());\n+                    if let Some(sep) = sep {\n+                        result.push(TokenTree::Token(sep.clone()).into());\n                     }\n                     continue;\n                 }\n@@ -242,7 +237,7 @@ pub fn transcribe(\n                         Ident::new(ident.name, ident.span.apply_mark(cx.current_expansion.mark));\n                     sp = sp.apply_mark(cx.current_expansion.mark);\n                     result.push(TokenTree::token(token::Dollar, sp).into());\n-                    result.push(TokenTree::token(TokenKind::from_ast_ident(ident), sp).into());\n+                    result.push(TokenTree::Token(Token::from_ast_ident(ident)).into());\n                 }\n             }\n "}, {"sha": "9d2ac5b4b51688a82c2e94f86f4c3f650b3da221", "filename": "src/libsyntax/parse/diagnostics.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/6b71fba9c1371a46e94917fabe042b6e3a14cf0a/src%2Flibsyntax%2Fparse%2Fdiagnostics.rs", "raw_url": "https://github.com/rust-lang/rust/raw/6b71fba9c1371a46e94917fabe042b6e3a14cf0a/src%2Flibsyntax%2Fparse%2Fdiagnostics.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Fdiagnostics.rs?ref=6b71fba9c1371a46e94917fabe042b6e3a14cf0a", "patch": "@@ -729,7 +729,7 @@ impl<'a> Parser<'a> {\n         &mut self,\n         t: &TokenKind,\n     ) -> PResult<'a, bool /* recovered */> {\n-        let token_str = pprust::token_to_string(t);\n+        let token_str = pprust::token_kind_to_string(t);\n         let this_token_str = self.this_token_descr();\n         let (prev_sp, sp) = match (&self.token.kind, self.subparser_name) {\n             // Point at the end of the macro call when reaching end of macro arguments."}, {"sha": "2f4c48d4bf9e0c5581c96d39bf2226f88ecb9233", "filename": "src/libsyntax/parse/lexer/mod.rs", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "blob_url": "https://github.com/rust-lang/rust/blob/6b71fba9c1371a46e94917fabe042b6e3a14cf0a/src%2Flibsyntax%2Fparse%2Flexer%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/6b71fba9c1371a46e94917fabe042b6e3a14cf0a/src%2Flibsyntax%2Fparse%2Flexer%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Flexer%2Fmod.rs?ref=6b71fba9c1371a46e94917fabe042b6e3a14cf0a", "patch": "@@ -1501,7 +1501,7 @@ fn char_at(s: &str, byte: usize) -> char {\n mod tests {\n     use super::*;\n \n-    use crate::ast::{Ident, CrateConfig};\n+    use crate::ast::CrateConfig;\n     use crate::symbol::Symbol;\n     use crate::source_map::{SourceMap, FilePathMapping};\n     use crate::feature_gate::UnstableFeatures;\n@@ -1562,7 +1562,7 @@ mod tests {\n             assert_eq!(string_reader.next_token(), token::Whitespace);\n             let tok1 = string_reader.next_token();\n             let tok2 = Token::new(\n-                token::Ident(Symbol::intern(\"fn\"), false),\n+                mk_ident(\"fn\"),\n                 Span::new(BytePos(21), BytePos(23), NO_EXPANSION),\n             );\n             assert_eq!(tok1.kind, tok2.kind);\n@@ -1593,7 +1593,7 @@ mod tests {\n \n     // make the identifier by looking up the string in the interner\n     fn mk_ident(id: &str) -> TokenKind {\n-        TokenKind::from_ast_ident(Ident::from_str(id))\n+        token::Ident(Symbol::intern(id), false)\n     }\n \n     fn mk_lit(kind: token::LitKind, symbol: &str, suffix: Option<&str>) -> TokenKind {"}, {"sha": "99d9d40a45b931b353ee3e8eb037e206c51269f0", "filename": "src/libsyntax/parse/lexer/tokentrees.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/6b71fba9c1371a46e94917fabe042b6e3a14cf0a/src%2Flibsyntax%2Fparse%2Flexer%2Ftokentrees.rs", "raw_url": "https://github.com/rust-lang/rust/raw/6b71fba9c1371a46e94917fabe042b6e3a14cf0a/src%2Flibsyntax%2Fparse%2Flexer%2Ftokentrees.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Flexer%2Ftokentrees.rs?ref=6b71fba9c1371a46e94917fabe042b6e3a14cf0a", "patch": "@@ -211,7 +211,7 @@ impl<'a> TokenTreesReader<'a> {\n                 let raw = self.string_reader.peek_span_src_raw;\n                 self.real_token();\n                 let is_joint = raw.hi() == self.string_reader.peek_span_src_raw.lo()\n-                    && token::is_op(&self.token);\n+                    && self.token.is_op();\n                 Ok((tt, if is_joint { Joint } else { NonJoint }))\n             }\n         }"}, {"sha": "cde35681988db905f0188fab5343ba82d7a62037", "filename": "src/libsyntax/parse/mod.rs", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/rust-lang/rust/blob/6b71fba9c1371a46e94917fabe042b6e3a14cf0a/src%2Flibsyntax%2Fparse%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/6b71fba9c1371a46e94917fabe042b6e3a14cf0a/src%2Flibsyntax%2Fparse%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Fmod.rs?ref=6b71fba9c1371a46e94917fabe042b6e3a14cf0a", "patch": "@@ -9,7 +9,7 @@ use crate::parse::parser::emit_unclosed_delims;\n use crate::parse::token::TokenKind;\n use crate::tokenstream::{TokenStream, TokenTree};\n use crate::diagnostics::plugin::ErrorMap;\n-use crate::print::pprust::token_to_string;\n+use crate::print::pprust;\n \n use errors::{Applicability, FatalError, Level, Handler, ColorConfig, Diagnostic, DiagnosticBuilder};\n use rustc_data_structures::sync::{Lrc, Lock};\n@@ -312,7 +312,7 @@ pub fn maybe_file_to_stream(\n             for unmatched in unmatched_braces {\n                 let mut db = sess.span_diagnostic.struct_span_err(unmatched.found_span, &format!(\n                     \"incorrect close delimiter: `{}`\",\n-                    token_to_string(&token::CloseDelim(unmatched.found_delim)),\n+                    pprust::token_kind_to_string(&token::CloseDelim(unmatched.found_delim)),\n                 ));\n                 db.span_label(unmatched.found_span, \"incorrect close delimiter\");\n                 if let Some(sp) = unmatched.candidate_span {"}, {"sha": "d9eba3bbadb687a1964868c433541c8431766981", "filename": "src/libsyntax/parse/parser.rs", "status": "modified", "additions": 14, "deletions": 12, "changes": 26, "blob_url": "https://github.com/rust-lang/rust/blob/6b71fba9c1371a46e94917fabe042b6e3a14cf0a/src%2Flibsyntax%2Fparse%2Fparser.rs", "raw_url": "https://github.com/rust-lang/rust/raw/6b71fba9c1371a46e94917fabe042b6e3a14cf0a/src%2Flibsyntax%2Fparse%2Fparser.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Fparser.rs?ref=6b71fba9c1371a46e94917fabe042b6e3a14cf0a", "patch": "@@ -401,7 +401,7 @@ crate enum TokenType {\n impl TokenType {\n     crate fn to_string(&self) -> String {\n         match *self {\n-            TokenType::Token(ref t) => format!(\"`{}`\", pprust::token_to_string(t)),\n+            TokenType::Token(ref t) => format!(\"`{}`\", pprust::token_kind_to_string(t)),\n             TokenType::Keyword(kw) => format!(\"`{}`\", kw),\n             TokenType::Operator => \"an operator\".to_string(),\n             TokenType::Lifetime => \"lifetime\".to_string(),\n@@ -418,7 +418,7 @@ impl TokenType {\n ///\n /// Types can also be of the form `IDENT(u8, u8) -> u8`, however this assumes\n /// that `IDENT` is not the ident of a fn trait.\n-fn can_continue_type_after_non_fn_ident(t: &TokenKind) -> bool {\n+fn can_continue_type_after_non_fn_ident(t: &Token) -> bool {\n     t == &token::ModSep || t == &token::Lt ||\n     t == &token::BinOp(token::Shl)\n }\n@@ -586,10 +586,10 @@ impl<'a> Parser<'a> {\n         edible: &[TokenKind],\n         inedible: &[TokenKind],\n     ) -> PResult<'a, bool /* recovered */> {\n-        if edible.contains(&self.token) {\n+        if edible.contains(&self.token.kind) {\n             self.bump();\n             Ok(false)\n-        } else if inedible.contains(&self.token) {\n+        } else if inedible.contains(&self.token.kind) {\n             // leave it in the input\n             Ok(false)\n         } else if self.last_unexpected_token_span == Some(self.token.span) {\n@@ -951,7 +951,7 @@ impl<'a> Parser<'a> {\n                         Err(mut e) => {\n                             // Attempt to keep parsing if it was a similar separator\n                             if let Some(ref tokens) = t.similar_tokens() {\n-                                if tokens.contains(&self.token) {\n+                                if tokens.contains(&self.token.kind) {\n                                     self.bump();\n                                 }\n                             }\n@@ -1756,7 +1756,7 @@ impl<'a> Parser<'a> {\n     fn parse_path_segment(&mut self, style: PathStyle) -> PResult<'a, PathSegment> {\n         let ident = self.parse_path_segment_ident()?;\n \n-        let is_args_start = |token: &TokenKind| match *token {\n+        let is_args_start = |token: &Token| match token.kind {\n             token::Lt | token::BinOp(token::Shl) | token::OpenDelim(token::Paren)\n             | token::LArrow => true,\n             _ => false,\n@@ -2627,9 +2627,11 @@ impl<'a> Parser<'a> {\n                     token::Ident(name, _) => name,\n                     _ => unreachable!()\n                 };\n-                let mut err = self.fatal(&format!(\"unknown macro variable `{}`\", name));\n-                err.span_label(self.token.span, \"unknown macro variable\");\n-                err.emit();\n+                let span = self.prev_span.to(self.token.span);\n+                self.diagnostic()\n+                    .struct_span_fatal(span, &format!(\"unknown macro variable `{}`\", name))\n+                    .span_label(span, \"unknown macro variable\")\n+                    .emit();\n                 self.bump();\n                 return\n             }\n@@ -2820,7 +2822,7 @@ impl<'a> Parser<'a> {\n                 LhsExpr::AttributesParsed(attrs) => Some(attrs),\n                 _ => None,\n             };\n-            if [token::DotDot, token::DotDotDot, token::DotDotEq].contains(&self.token) {\n+            if [token::DotDot, token::DotDotDot, token::DotDotEq].contains(&self.token.kind) {\n                 return self.parse_prefix_range_expr(attrs);\n             } else {\n                 self.parse_prefix_expr(attrs)?\n@@ -3097,7 +3099,7 @@ impl<'a> Parser<'a> {\n             self.err_dotdotdot_syntax(self.token.span);\n         }\n \n-        debug_assert!([token::DotDot, token::DotDotDot, token::DotDotEq].contains(&self.token),\n+        debug_assert!([token::DotDot, token::DotDotDot, token::DotDotEq].contains(&self.token.kind),\n                       \"parse_prefix_range_expr: token {:?} is not DotDot/DotDotEq\",\n                       self.token);\n         let tok = self.token.clone();\n@@ -7865,7 +7867,7 @@ pub fn emit_unclosed_delims(unclosed_delims: &mut Vec<UnmatchedBrace>, handler:\n     for unmatched in unclosed_delims.iter() {\n         let mut err = handler.struct_span_err(unmatched.found_span, &format!(\n             \"incorrect close delimiter: `{}`\",\n-            pprust::token_to_string(&token::CloseDelim(unmatched.found_delim)),\n+            pprust::token_kind_to_string(&token::CloseDelim(unmatched.found_delim)),\n         ));\n         err.span_label(unmatched.found_span, \"incorrect close delimiter\");\n         if let Some(sp) = unmatched.candidate_span {"}, {"sha": "cc34883e2e8151fde9b682c6635c59995dae2f46", "filename": "src/libsyntax/parse/token.rs", "status": "modified", "additions": 73, "deletions": 122, "changes": 195, "blob_url": "https://github.com/rust-lang/rust/blob/6b71fba9c1371a46e94917fabe042b6e3a14cf0a/src%2Flibsyntax%2Fparse%2Ftoken.rs", "raw_url": "https://github.com/rust-lang/rust/raw/6b71fba9c1371a46e94917fabe042b6e3a14cf0a/src%2Flibsyntax%2Fparse%2Ftoken.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Ftoken.rs?ref=6b71fba9c1371a46e94917fabe042b6e3a14cf0a", "patch": "@@ -17,7 +17,6 @@ use log::info;\n \n use std::fmt;\n use std::mem;\n-use std::ops::Deref;\n #[cfg(target_arch = \"x86_64\")]\n use rustc_data_structures::static_assert_size;\n use rustc_data_structures::sync::Lrc;\n@@ -242,20 +241,57 @@ pub struct Token {\n }\n \n impl TokenKind {\n-    /// Recovers a `TokenKind` from an `ast::Ident`. This creates a raw identifier if necessary.\n-    pub fn from_ast_ident(ident: ast::Ident) -> TokenKind {\n-        Ident(ident.name, ident.is_raw_guess())\n+    pub fn lit(kind: LitKind, symbol: Symbol, suffix: Option<Symbol>) -> TokenKind {\n+        Literal(Lit::new(kind, symbol, suffix))\n     }\n \n-    crate fn is_like_plus(&self) -> bool {\n+    /// Returns tokens that are likely to be typed accidentally instead of the current token.\n+    /// Enables better error recovery when the wrong token is found.\n+    crate fn similar_tokens(&self) -> Option<Vec<TokenKind>> {\n         match *self {\n-            BinOp(Plus) | BinOpEq(Plus) => true,\n-            _ => false,\n+            Comma => Some(vec![Dot, Lt, Semi]),\n+            Semi => Some(vec![Colon, Comma]),\n+            _ => None\n         }\n     }\n }\n \n impl Token {\n+    crate fn new(kind: TokenKind, span: Span) -> Self {\n+        Token { kind, span }\n+    }\n+\n+    /// Some token that will be thrown away later.\n+    crate fn dummy() -> Self {\n+        Token::new(TokenKind::Whitespace, DUMMY_SP)\n+    }\n+\n+    /// Recovers a `Token` from an `ast::Ident`. This creates a raw identifier if necessary.\n+    crate fn from_ast_ident(ident: ast::Ident) -> Self {\n+        Token::new(Ident(ident.name, ident.is_raw_guess()), ident.span)\n+    }\n+\n+    /// Return this token by value and leave a dummy token in its place.\n+    crate fn take(&mut self) -> Self {\n+        mem::replace(self, Token::dummy())\n+    }\n+\n+    crate fn is_op(&self) -> bool {\n+        match self.kind {\n+            OpenDelim(..) | CloseDelim(..) | Literal(..) | DocComment(..) |\n+            Ident(..) | Lifetime(..) | Interpolated(..) |\n+            Whitespace | Comment | Shebang(..) | Eof => false,\n+            _ => true,\n+        }\n+    }\n+\n+    crate fn is_like_plus(&self) -> bool {\n+        match self.kind {\n+            BinOp(Plus) | BinOpEq(Plus) => true,\n+            _ => false,\n+        }\n+    }\n+\n     /// Returns `true` if the token can appear at the start of an expression.\n     crate fn can_begin_expr(&self) -> bool {\n         match self.kind {\n@@ -310,12 +346,10 @@ impl Token {\n             _ => false,\n         }\n     }\n-}\n \n-impl TokenKind {\n     /// Returns `true` if the token can appear at the start of a const param.\n-    pub fn can_begin_const_arg(&self) -> bool {\n-        match self {\n+    crate fn can_begin_const_arg(&self) -> bool {\n+        match self.kind {\n             OpenDelim(Brace) => true,\n             Interpolated(ref nt) => match **nt {\n                 NtExpr(..) => true,\n@@ -326,31 +360,23 @@ impl TokenKind {\n             _ => self.can_begin_literal_or_bool(),\n         }\n     }\n-}\n \n-impl Token {\n     /// Returns `true` if the token can appear at the start of a generic bound.\n     crate fn can_begin_bound(&self) -> bool {\n         self.is_path_start() || self.is_lifetime() || self.is_keyword(kw::For) ||\n         self == &Question || self == &OpenDelim(Paren)\n     }\n-}\n-\n-impl TokenKind {\n-    pub fn lit(kind: LitKind, symbol: Symbol, suffix: Option<Symbol>) -> TokenKind {\n-        Literal(Lit::new(kind, symbol, suffix))\n-    }\n \n     /// Returns `true` if the token is any literal\n     crate fn is_lit(&self) -> bool {\n-        match *self {\n+        match self.kind {\n             Literal(..) => true,\n             _           => false,\n         }\n     }\n \n     crate fn expect_lit(&self) -> Lit {\n-        match *self {\n+        match self.kind {\n             Literal(lit) => lit,\n             _=> panic!(\"`expect_lit` called on non-literal\"),\n         }\n@@ -359,7 +385,7 @@ impl TokenKind {\n     /// Returns `true` if the token is any literal, a minus (which can prefix a literal,\n     /// for example a '-42', or one of the boolean idents).\n     crate fn can_begin_literal_or_bool(&self) -> bool {\n-        match *self {\n+        match self.kind {\n             Literal(..)  => true,\n             BinOp(Minus) => true,\n             Ident(name, false) if name == kw::True => true,\n@@ -371,9 +397,7 @@ impl TokenKind {\n             _            => false,\n         }\n     }\n-}\n \n-impl Token {\n     /// Returns an identifier if this token is an identifier.\n     pub fn ident(&self) -> Option<(ast::Ident, /* is_raw */ bool)> {\n         match self.kind {\n@@ -397,49 +421,25 @@ impl Token {\n             _ => None,\n         }\n     }\n-}\n \n-impl TokenKind {\n-    /// Returns an identifier name if this token is an identifier.\n-    pub fn ident_name(&self) -> Option<(ast::Name, /* is_raw */ bool)> {\n-        match *self {\n-            Ident(name, is_raw) => Some((name, is_raw)),\n-            Interpolated(ref nt) => match **nt {\n-                NtIdent(ident, is_raw) => Some((ident.name, is_raw)),\n-                _ => None,\n-            },\n-            _ => None,\n-        }\n-    }\n-    /// Returns a lifetime name if this token is a lifetime.\n-    pub fn lifetime_name(&self) -> Option<ast::Name> {\n-        match *self {\n-            Lifetime(name) => Some(name),\n-            Interpolated(ref nt) => match **nt {\n-                NtLifetime(ident) => Some(ident.name),\n-                _ => None,\n-            },\n-            _ => None,\n-        }\n-    }\n     /// Returns `true` if the token is an identifier.\n     pub fn is_ident(&self) -> bool {\n-        self.ident_name().is_some()\n+        self.ident().is_some()\n     }\n     /// Returns `true` if the token is a lifetime.\n     crate fn is_lifetime(&self) -> bool {\n-        self.lifetime_name().is_some()\n+        self.lifetime().is_some()\n     }\n \n     /// Returns `true` if the token is a identifier whose name is the given\n     /// string slice.\n     crate fn is_ident_named(&self, name: Symbol) -> bool {\n-        self.ident_name().map_or(false, |(ident_name, _)| ident_name == name)\n+        self.ident().map_or(false, |(ident, _)| ident.name == name)\n     }\n \n     /// Returns `true` if the token is an interpolated path.\n     fn is_path(&self) -> bool {\n-        if let Interpolated(ref nt) = *self {\n+        if let Interpolated(ref nt) = self.kind {\n             if let NtPath(..) = **nt {\n                 return true;\n             }\n@@ -456,33 +456,27 @@ impl TokenKind {\n     crate fn is_qpath_start(&self) -> bool {\n         self == &Lt || self == &BinOp(Shl)\n     }\n-}\n \n-impl Token {\n     crate fn is_path_start(&self) -> bool {\n         self == &ModSep || self.is_qpath_start() || self.is_path() ||\n         self.is_path_segment_keyword() || self.is_ident() && !self.is_reserved_ident()\n     }\n-}\n \n-impl TokenKind {\n     /// Returns `true` if the token is a given keyword, `kw`.\n     pub fn is_keyword(&self, kw: Symbol) -> bool {\n-        self.ident_name().map(|(name, is_raw)| name == kw && !is_raw).unwrap_or(false)\n+        self.ident().map(|(id, is_raw)| id.name == kw && !is_raw).unwrap_or(false)\n     }\n \n-    pub fn is_path_segment_keyword(&self) -> bool {\n-        match self.ident_name() {\n-            Some((name, false)) => name.is_path_segment_keyword(),\n+    crate fn is_path_segment_keyword(&self) -> bool {\n+        match self.ident() {\n+            Some((id, false)) => id.is_path_segment_keyword(),\n             _ => false,\n         }\n     }\n-}\n \n-impl Token {\n     // Returns true for reserved identifiers used internally for elided lifetimes,\n     // unnamed method parameters, crate root module, error recovery etc.\n-    pub fn is_special_ident(&self) -> bool {\n+    crate fn is_special_ident(&self) -> bool {\n         match self.ident() {\n             Some((id, false)) => id.is_special(),\n             _ => false,\n@@ -512,55 +506,53 @@ impl Token {\n             _ => false,\n         }\n     }\n-}\n \n-impl TokenKind {\n-    crate fn glue(self, joint: TokenKind) -> Option<TokenKind> {\n-        Some(match self {\n-            Eq => match joint {\n+    crate fn glue(self, joint: Token) -> Option<Token> {\n+        let kind = match self.kind {\n+            Eq => match joint.kind {\n                 Eq => EqEq,\n                 Gt => FatArrow,\n                 _ => return None,\n             },\n-            Lt => match joint {\n+            Lt => match joint.kind {\n                 Eq => Le,\n                 Lt => BinOp(Shl),\n                 Le => BinOpEq(Shl),\n                 BinOp(Minus) => LArrow,\n                 _ => return None,\n             },\n-            Gt => match joint {\n+            Gt => match joint.kind {\n                 Eq => Ge,\n                 Gt => BinOp(Shr),\n                 Ge => BinOpEq(Shr),\n                 _ => return None,\n             },\n-            Not => match joint {\n+            Not => match joint.kind {\n                 Eq => Ne,\n                 _ => return None,\n             },\n-            BinOp(op) => match joint {\n+            BinOp(op) => match joint.kind {\n                 Eq => BinOpEq(op),\n                 BinOp(And) if op == And => AndAnd,\n                 BinOp(Or) if op == Or => OrOr,\n                 Gt if op == Minus => RArrow,\n                 _ => return None,\n             },\n-            Dot => match joint {\n+            Dot => match joint.kind {\n                 Dot => DotDot,\n                 DotDot => DotDotDot,\n                 _ => return None,\n             },\n-            DotDot => match joint {\n+            DotDot => match joint.kind {\n                 Dot => DotDotDot,\n                 Eq => DotDotEq,\n                 _ => return None,\n             },\n-            Colon => match joint {\n+            Colon => match joint.kind {\n                 Colon => ModSep,\n                 _ => return None,\n             },\n-            SingleQuote => match joint {\n+            SingleQuote => match joint.kind {\n                 Ident(name, false) => Lifetime(Symbol::intern(&format!(\"'{}\", name))),\n                 _ => return None,\n             },\n@@ -570,26 +562,18 @@ impl TokenKind {\n             Question | OpenDelim(..) | CloseDelim(..) |\n             Literal(..) | Ident(..) | Lifetime(..) | Interpolated(..) | DocComment(..) |\n             Whitespace | Comment | Shebang(..) | Eof => return None,\n-        })\n-    }\n+        };\n \n-    /// Returns tokens that are likely to be typed accidentally instead of the current token.\n-    /// Enables better error recovery when the wrong token is found.\n-    crate fn similar_tokens(&self) -> Option<Vec<TokenKind>> {\n-        match *self {\n-            Comma => Some(vec![Dot, Lt, Semi]),\n-            Semi => Some(vec![Colon, Comma]),\n-            _ => None\n-        }\n+        Some(Token::new(kind, self.span.to(joint.span)))\n     }\n \n     // See comments in `Nonterminal::to_tokenstream` for why we care about\n     // *probably* equal here rather than actual equality\n-    crate fn probably_equal_for_proc_macro(&self, other: &TokenKind) -> bool {\n-        if mem::discriminant(self) != mem::discriminant(other) {\n+    crate fn probably_equal_for_proc_macro(&self, other: &Token) -> bool {\n+        if mem::discriminant(&self.kind) != mem::discriminant(&other.kind) {\n             return false\n         }\n-        match (self, other) {\n+        match (&self.kind, &other.kind) {\n             (&Eq, &Eq) |\n             (&Lt, &Lt) |\n             (&Le, &Le) |\n@@ -643,36 +627,12 @@ impl TokenKind {\n     }\n }\n \n-impl Token {\n-    crate fn new(kind: TokenKind, span: Span) -> Self {\n-        Token { kind, span }\n-    }\n-\n-    /// Some token that will be thrown away later.\n-    crate fn dummy() -> Self {\n-        Token::new(TokenKind::Whitespace, DUMMY_SP)\n-    }\n-\n-    /// Return this token by value and leave a dummy token in its place.\n-    crate fn take(&mut self) -> Self {\n-        mem::replace(self, Token::dummy())\n-    }\n-}\n-\n impl PartialEq<TokenKind> for Token {\n     fn eq(&self, rhs: &TokenKind) -> bool {\n         self.kind == *rhs\n     }\n }\n \n-// FIXME: Remove this after all necessary methods are moved from `TokenKind` to `Token`.\n-impl Deref for Token {\n-    type Target = TokenKind;\n-    fn deref(&self) -> &Self::Target {\n-        &self.kind\n-    }\n-}\n-\n #[derive(Clone, RustcEncodable, RustcDecodable)]\n /// For interpolation during macro expansion.\n pub enum Nonterminal {\n@@ -812,15 +772,6 @@ impl Nonterminal {\n     }\n }\n \n-crate fn is_op(tok: &TokenKind) -> bool {\n-    match *tok {\n-        OpenDelim(..) | CloseDelim(..) | Literal(..) | DocComment(..) |\n-        Ident(..) | Lifetime(..) | Interpolated(..) |\n-        Whitespace | Comment | Shebang(..) | Eof => false,\n-        _ => true,\n-    }\n-}\n-\n fn prepend_attrs(sess: &ParseSess,\n                  attrs: &[ast::Attribute],\n                  tokens: Option<&tokenstream::TokenStream>,"}, {"sha": "4cbe590d44bfee9da94ac36e265f07c7bba801c6", "filename": "src/libsyntax/print/pprust.rs", "status": "modified", "additions": 8, "deletions": 4, "changes": 12, "blob_url": "https://github.com/rust-lang/rust/blob/6b71fba9c1371a46e94917fabe042b6e3a14cf0a/src%2Flibsyntax%2Fprint%2Fpprust.rs", "raw_url": "https://github.com/rust-lang/rust/raw/6b71fba9c1371a46e94917fabe042b6e3a14cf0a/src%2Flibsyntax%2Fprint%2Fpprust.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fprint%2Fpprust.rs?ref=6b71fba9c1371a46e94917fabe042b6e3a14cf0a", "patch": "@@ -6,7 +6,7 @@ use crate::ast::{Attribute, MacDelimiter, GenericArg};\n use crate::util::parser::{self, AssocOp, Fixity};\n use crate::attr;\n use crate::source_map::{self, SourceMap, Spanned};\n-use crate::parse::token::{self, BinOpToken, Nonterminal, TokenKind};\n+use crate::parse::token::{self, BinOpToken, Nonterminal, Token, TokenKind};\n use crate::parse::lexer::comments;\n use crate::parse::{self, ParseSess};\n use crate::print::pp::{self, Breaks};\n@@ -189,7 +189,7 @@ pub fn literal_to_string(lit: token::Lit) -> String {\n     out\n }\n \n-pub fn token_to_string(tok: &TokenKind) -> String {\n+pub fn token_kind_to_string(tok: &TokenKind) -> String {\n     match *tok {\n         token::Eq                   => \"=\".to_string(),\n         token::Lt                   => \"<\".to_string(),\n@@ -250,6 +250,10 @@ pub fn token_to_string(tok: &TokenKind) -> String {\n     }\n }\n \n+pub fn token_to_string(token: &Token) -> String {\n+    token_kind_to_string(&token.kind)\n+}\n+\n pub fn nonterminal_to_string(nt: &Nonterminal) -> String {\n     match *nt {\n         token::NtExpr(ref e)        => expr_to_string(e),\n@@ -734,11 +738,11 @@ pub trait PrintState<'a> {\n                 }\n             }\n             TokenTree::Delimited(_, delim, tts) => {\n-                self.writer().word(token_to_string(&token::OpenDelim(delim)))?;\n+                self.writer().word(token_kind_to_string(&token::OpenDelim(delim)))?;\n                 self.writer().space()?;\n                 self.print_tts(tts)?;\n                 self.writer().space()?;\n-                self.writer().word(token_to_string(&token::CloseDelim(delim)))\n+                self.writer().word(token_kind_to_string(&token::CloseDelim(delim)))\n             },\n         }\n     }"}, {"sha": "2daec9702798fd1ef5c97e2a2a5231929743f168", "filename": "src/libsyntax/tokenstream.rs", "status": "modified", "additions": 2, "deletions": 11, "changes": 13, "blob_url": "https://github.com/rust-lang/rust/blob/6b71fba9c1371a46e94917fabe042b6e3a14cf0a/src%2Flibsyntax%2Ftokenstream.rs", "raw_url": "https://github.com/rust-lang/rust/raw/6b71fba9c1371a46e94917fabe042b6e3a14cf0a/src%2Flibsyntax%2Ftokenstream.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Ftokenstream.rs?ref=6b71fba9c1371a46e94917fabe042b6e3a14cf0a", "patch": "@@ -126,14 +126,6 @@ impl TokenTree {\n         }\n     }\n \n-    /// Indicates if the stream is a token that is equal to the provided token.\n-    pub fn eq_token(&self, t: TokenKind) -> bool {\n-        match self {\n-            TokenTree::Token(token) => *token == t,\n-            _ => false,\n-        }\n-    }\n-\n     pub fn joint(self) -> TokenStream {\n         TokenStream::new(vec![(self, Joint)])\n     }\n@@ -430,11 +422,10 @@ impl TokenStreamBuilder {\n         let last_tree_if_joint = self.0.last().and_then(TokenStream::last_tree_if_joint);\n         if let Some(TokenTree::Token(last_token)) = last_tree_if_joint {\n             if let Some((TokenTree::Token(token), is_joint)) = stream.first_tree_and_joint() {\n-                if let Some(glued_tok) = last_token.kind.glue(token.kind) {\n+                if let Some(glued_tok) = last_token.glue(token) {\n                     let last_stream = self.0.pop().unwrap();\n                     self.push_all_but_last_tree(&last_stream);\n-                    let glued_span = last_token.span.to(token.span);\n-                    let glued_tt = TokenTree::token(glued_tok, glued_span);\n+                    let glued_tt = TokenTree::Token(glued_tok);\n                     let glued_tokenstream = TokenStream::new(vec![(glued_tt, is_joint)]);\n                     self.0.push(glued_tokenstream);\n                     self.push_all_but_first_tree(&stream);"}, {"sha": "69dd96625cc020d694e9aaa039f51658237745ab", "filename": "src/libsyntax/util/parser.rs", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "blob_url": "https://github.com/rust-lang/rust/blob/6b71fba9c1371a46e94917fabe042b6e3a14cf0a/src%2Flibsyntax%2Futil%2Fparser.rs", "raw_url": "https://github.com/rust-lang/rust/raw/6b71fba9c1371a46e94917fabe042b6e3a14cf0a/src%2Flibsyntax%2Futil%2Fparser.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Futil%2Fparser.rs?ref=6b71fba9c1371a46e94917fabe042b6e3a14cf0a", "patch": "@@ -1,4 +1,4 @@\n-use crate::parse::token::{self, TokenKind, BinOpToken};\n+use crate::parse::token::{self, Token, BinOpToken};\n use crate::symbol::kw;\n use crate::ast::{self, BinOpKind};\n \n@@ -69,9 +69,9 @@ pub enum Fixity {\n \n impl AssocOp {\n     /// Creates a new AssocOP from a token\n-    pub fn from_token(t: &TokenKind) -> Option<AssocOp> {\n+    pub fn from_token(t: &Token) -> Option<AssocOp> {\n         use AssocOp::*;\n-        match *t {\n+        match t.kind {\n             token::BinOpEq(k) => Some(AssignOp(k)),\n             token::Eq => Some(Assign),\n             token::BinOp(BinOpToken::Star) => Some(Multiply),"}, {"sha": "542486927dfd115f2bb7d9b33c5369ca2a83cc3b", "filename": "src/test/ui/macros/macro-input-future-proofing.stderr", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/rust-lang/rust/blob/6b71fba9c1371a46e94917fabe042b6e3a14cf0a/src%2Ftest%2Fui%2Fmacros%2Fmacro-input-future-proofing.stderr", "raw_url": "https://github.com/rust-lang/rust/raw/6b71fba9c1371a46e94917fabe042b6e3a14cf0a/src%2Ftest%2Fui%2Fmacros%2Fmacro-input-future-proofing.stderr", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ftest%2Fui%2Fmacros%2Fmacro-input-future-proofing.stderr?ref=6b71fba9c1371a46e94917fabe042b6e3a14cf0a", "patch": "@@ -55,10 +55,10 @@ LL |     ($($a:ty, $b:ty)* -) => ();\n    = note: allowed there are: `{`, `[`, `=>`, `,`, `>`, `=`, `:`, `;`, `|`, `as` or `where`\n \n error: `$ty:ty` is followed by `-`, which is not allowed for `ty` fragments\n-  --> $DIR/macro-input-future-proofing.rs:18:7\n+  --> $DIR/macro-input-future-proofing.rs:18:15\n    |\n LL |     ($($ty:ty)-+) => ();\n-   |       ^^^^^^^^ not allowed after `ty` fragments\n+   |               ^ not allowed after `ty` fragments\n    |\n    = note: allowed there are: `{`, `[`, `=>`, `,`, `>`, `=`, `:`, `;`, `|`, `as` or `where`\n "}]}