{"sha": "91a2c0d51241677d71b8c0abc80535e580fe3939", "node_id": "MDY6Q29tbWl0NzI0NzEyOjkxYTJjMGQ1MTI0MTY3N2Q3MWI4YzBhYmM4MDUzNWU1ODBmZTM5Mzk=", "commit": {"author": {"name": "Aaron Turon", "email": "aturon@mozilla.com", "date": "2014-11-14T21:56:15Z"}, "committer": {"name": "Aaron Turon", "email": "aturon@mozilla.com", "date": "2014-11-21T01:19:24Z"}, "message": "Remove libgreen\n\nWith runtime removal complete, there is no longer any reason to provide\nlibgreen.\n\n[breaking-change]", "tree": {"sha": "dd927c5d88d6a3e3150028c057779abc2447294a", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/dd927c5d88d6a3e3150028c057779abc2447294a"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/91a2c0d51241677d71b8c0abc80535e580fe3939", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/91a2c0d51241677d71b8c0abc80535e580fe3939", "html_url": "https://github.com/rust-lang/rust/commit/91a2c0d51241677d71b8c0abc80535e580fe3939", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/91a2c0d51241677d71b8c0abc80535e580fe3939/comments", "author": {"login": "aturon", "id": 709807, "node_id": "MDQ6VXNlcjcwOTgwNw==", "avatar_url": "https://avatars.githubusercontent.com/u/709807?v=4", "gravatar_id": "", "url": "https://api.github.com/users/aturon", "html_url": "https://github.com/aturon", "followers_url": "https://api.github.com/users/aturon/followers", "following_url": "https://api.github.com/users/aturon/following{/other_user}", "gists_url": "https://api.github.com/users/aturon/gists{/gist_id}", "starred_url": "https://api.github.com/users/aturon/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/aturon/subscriptions", "organizations_url": "https://api.github.com/users/aturon/orgs", "repos_url": "https://api.github.com/users/aturon/repos", "events_url": "https://api.github.com/users/aturon/events{/privacy}", "received_events_url": "https://api.github.com/users/aturon/received_events", "type": "User", "site_admin": false}, "committer": {"login": "aturon", "id": 709807, "node_id": "MDQ6VXNlcjcwOTgwNw==", "avatar_url": "https://avatars.githubusercontent.com/u/709807?v=4", "gravatar_id": "", "url": "https://api.github.com/users/aturon", "html_url": "https://github.com/aturon", "followers_url": "https://api.github.com/users/aturon/followers", "following_url": "https://api.github.com/users/aturon/following{/other_user}", "gists_url": "https://api.github.com/users/aturon/gists{/gist_id}", "starred_url": "https://api.github.com/users/aturon/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/aturon/subscriptions", "organizations_url": "https://api.github.com/users/aturon/orgs", "repos_url": "https://api.github.com/users/aturon/repos", "events_url": "https://api.github.com/users/aturon/events{/privacy}", "received_events_url": "https://api.github.com/users/aturon/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "3ee916e50bd86768cb2a9141f9b2c52d2601b412", "url": "https://api.github.com/repos/rust-lang/rust/commits/3ee916e50bd86768cb2a9141f9b2c52d2601b412", "html_url": "https://github.com/rust-lang/rust/commit/3ee916e50bd86768cb2a9141f9b2c52d2601b412"}], "stats": {"total": 3862, "additions": 0, "deletions": 3862}, "files": [{"sha": "aa933f182e54511654d434499bf1daf474f33b65", "filename": "src/libgreen/basic.rs", "status": "removed", "additions": 0, "deletions": 259, "changes": 259, "blob_url": "https://github.com/rust-lang/rust/blob/3ee916e50bd86768cb2a9141f9b2c52d2601b412/src%2Flibgreen%2Fbasic.rs", "raw_url": "https://github.com/rust-lang/rust/raw/3ee916e50bd86768cb2a9141f9b2c52d2601b412/src%2Flibgreen%2Fbasic.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibgreen%2Fbasic.rs?ref=3ee916e50bd86768cb2a9141f9b2c52d2601b412", "patch": "@@ -1,259 +0,0 @@\n-// Copyright 2013 The Rust Project Developers. See the COPYRIGHT\n-// file at the top-level directory of this distribution and at\n-// http://rust-lang.org/COPYRIGHT.\n-//\n-// Licensed under the Apache License, Version 2.0 <LICENSE-APACHE or\n-// http://www.apache.org/licenses/LICENSE-2.0> or the MIT license\n-// <LICENSE-MIT or http://opensource.org/licenses/MIT>, at your\n-// option. This file may not be copied, modified, or distributed\n-// except according to those terms.\n-\n-//! This is a basic event loop implementation not meant for any \"real purposes\"\n-//! other than testing the scheduler and proving that it's possible to have a\n-//! pluggable event loop.\n-//!\n-//! This implementation is also used as the fallback implementation of an event\n-//! loop if no other one is provided (and M:N scheduling is desired).\n-use self::Message::*;\n-\n-use alloc::arc::Arc;\n-use std::sync::atomic;\n-use std::mem;\n-use std::rt::rtio::{EventLoop, RemoteCallback};\n-use std::rt::rtio::{PausableIdleCallback, Callback};\n-use std::rt::exclusive::Exclusive;\n-\n-/// This is the only exported function from this module.\n-pub fn event_loop() -> Box<EventLoop + Send> {\n-    box BasicLoop::new() as Box<EventLoop + Send>\n-}\n-\n-struct BasicLoop {\n-    work: Vec<proc(): Send>,             // pending work\n-    remotes: Vec<(uint, Box<Callback + Send>)>,\n-    next_remote: uint,\n-    messages: Arc<Exclusive<Vec<Message>>>,\n-    idle: Option<Box<Callback + Send>>,\n-    idle_active: Option<Arc<atomic::AtomicBool>>,\n-}\n-\n-enum Message { RunRemote(uint), RemoveRemote(uint) }\n-\n-impl BasicLoop {\n-    fn new() -> BasicLoop {\n-        BasicLoop {\n-            work: vec![],\n-            idle: None,\n-            idle_active: None,\n-            next_remote: 0,\n-            remotes: vec![],\n-            messages: Arc::new(Exclusive::new(Vec::new())),\n-        }\n-    }\n-\n-    /// Process everything in the work queue (continually)\n-    fn work(&mut self) {\n-        while self.work.len() > 0 {\n-            for work in mem::replace(&mut self.work, vec![]).into_iter() {\n-                work();\n-            }\n-        }\n-    }\n-\n-    fn remote_work(&mut self) {\n-        let messages = unsafe {\n-            mem::replace(&mut *self.messages.lock(), Vec::new())\n-        };\n-        for message in messages.into_iter() {\n-            self.message(message);\n-        }\n-    }\n-\n-    fn message(&mut self, message: Message) {\n-        match message {\n-            RunRemote(i) => {\n-                match self.remotes.iter_mut().find(|& &(id, _)| id == i) {\n-                    Some(&(_, ref mut f)) => f.call(),\n-                    None => panic!(\"bad remote: {}\", i),\n-                }\n-            }\n-            RemoveRemote(i) => {\n-                match self.remotes.iter().position(|&(id, _)| id == i) {\n-                    Some(i) => { self.remotes.remove(i).unwrap(); }\n-                    None => panic!(\"bad remote: {}\", i),\n-                }\n-            }\n-        }\n-    }\n-\n-    /// Run the idle callback if one is registered\n-    fn idle(&mut self) {\n-        match self.idle {\n-            Some(ref mut idle) => {\n-                if self.idle_active.as_ref().unwrap().load(atomic::SeqCst) {\n-                    idle.call();\n-                }\n-            }\n-            None => {}\n-        }\n-    }\n-\n-    fn has_idle(&self) -> bool {\n-        self.idle.is_some() && self.idle_active.as_ref().unwrap().load(atomic::SeqCst)\n-    }\n-}\n-\n-impl EventLoop for BasicLoop {\n-    fn run(&mut self) {\n-        // Not exactly efficient, but it gets the job done.\n-        while self.remotes.len() > 0 || self.work.len() > 0 || self.has_idle() {\n-\n-            self.work();\n-            self.remote_work();\n-\n-            if self.has_idle() {\n-                self.idle();\n-                continue\n-            }\n-\n-            unsafe {\n-                let messages = self.messages.lock();\n-                // We block here if we have no messages to process and we may\n-                // receive a message at a later date\n-                if self.remotes.len() > 0 && messages.len() == 0 &&\n-                   self.work.len() == 0 {\n-                    messages.wait()\n-                }\n-            }\n-        }\n-    }\n-\n-    fn callback(&mut self, f: proc():Send) {\n-        self.work.push(f);\n-    }\n-\n-    // FIXME: Seems like a really weird requirement to have an event loop provide.\n-    fn pausable_idle_callback(&mut self, cb: Box<Callback + Send>)\n-                              -> Box<PausableIdleCallback + Send> {\n-        rtassert!(self.idle.is_none());\n-        self.idle = Some(cb);\n-        let a = Arc::new(atomic::AtomicBool::new(true));\n-        self.idle_active = Some(a.clone());\n-        box BasicPausable { active: a } as Box<PausableIdleCallback + Send>\n-    }\n-\n-    fn remote_callback(&mut self, f: Box<Callback + Send>)\n-                       -> Box<RemoteCallback + Send> {\n-        let id = self.next_remote;\n-        self.next_remote += 1;\n-        self.remotes.push((id, f));\n-        box BasicRemote::new(self.messages.clone(), id) as\n-            Box<RemoteCallback + Send>\n-    }\n-\n-    fn has_active_io(&self) -> bool { false }\n-}\n-\n-struct BasicRemote {\n-    queue: Arc<Exclusive<Vec<Message>>>,\n-    id: uint,\n-}\n-\n-impl BasicRemote {\n-    fn new(queue: Arc<Exclusive<Vec<Message>>>, id: uint) -> BasicRemote {\n-        BasicRemote { queue: queue, id: id }\n-    }\n-}\n-\n-impl RemoteCallback for BasicRemote {\n-    fn fire(&mut self) {\n-        let mut queue = unsafe { self.queue.lock() };\n-        queue.push(RunRemote(self.id));\n-        queue.signal();\n-    }\n-}\n-\n-impl Drop for BasicRemote {\n-    fn drop(&mut self) {\n-        let mut queue = unsafe { self.queue.lock() };\n-        queue.push(RemoveRemote(self.id));\n-        queue.signal();\n-    }\n-}\n-\n-struct BasicPausable {\n-    active: Arc<atomic::AtomicBool>,\n-}\n-\n-impl PausableIdleCallback for BasicPausable {\n-    fn pause(&mut self) {\n-        self.active.store(false, atomic::SeqCst);\n-    }\n-    fn resume(&mut self) {\n-        self.active.store(true, atomic::SeqCst);\n-    }\n-}\n-\n-impl Drop for BasicPausable {\n-    fn drop(&mut self) {\n-        self.active.store(false, atomic::SeqCst);\n-    }\n-}\n-\n-#[cfg(test)]\n-mod test {\n-    use std::rt::task::TaskOpts;\n-\n-    use basic;\n-    use PoolConfig;\n-    use SchedPool;\n-\n-    fn pool() -> SchedPool {\n-        SchedPool::new(PoolConfig {\n-            threads: 1,\n-            event_loop_factory: basic::event_loop,\n-        })\n-    }\n-\n-    fn run(f: proc():Send) {\n-        let mut pool = pool();\n-        pool.spawn(TaskOpts::new(), f);\n-        pool.shutdown();\n-    }\n-\n-    #[test]\n-    fn smoke() {\n-        run(proc() {});\n-    }\n-\n-    #[test]\n-    fn some_channels() {\n-        run(proc() {\n-            let (tx, rx) = channel();\n-            spawn(proc() {\n-                tx.send(());\n-            });\n-            rx.recv();\n-        });\n-    }\n-\n-    #[test]\n-    fn multi_thread() {\n-        let mut pool = SchedPool::new(PoolConfig {\n-            threads: 2,\n-            event_loop_factory: basic::event_loop,\n-        });\n-\n-        for _ in range(0u, 20) {\n-            pool.spawn(TaskOpts::new(), proc() {\n-                let (tx, rx) = channel();\n-                spawn(proc() {\n-                    tx.send(());\n-                });\n-                rx.recv();\n-            });\n-        }\n-\n-        pool.shutdown();\n-    }\n-}"}, {"sha": "2d3e85cc833f3460a4ebb4b5f30332ef4269dded", "filename": "src/libgreen/context.rs", "status": "removed", "additions": 0, "deletions": 325, "changes": 325, "blob_url": "https://github.com/rust-lang/rust/blob/3ee916e50bd86768cb2a9141f9b2c52d2601b412/src%2Flibgreen%2Fcontext.rs", "raw_url": "https://github.com/rust-lang/rust/raw/3ee916e50bd86768cb2a9141f9b2c52d2601b412/src%2Flibgreen%2Fcontext.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibgreen%2Fcontext.rs?ref=3ee916e50bd86768cb2a9141f9b2c52d2601b412", "patch": "@@ -1,325 +0,0 @@\n-// Copyright 2013-2014 The Rust Project Developers. See the COPYRIGHT\n-// file at the top-level directory of this distribution and at\n-// http://rust-lang.org/COPYRIGHT.\n-//\n-// Licensed under the Apache License, Version 2.0 <LICENSE-APACHE or\n-// http://www.apache.org/licenses/LICENSE-2.0> or the MIT license\n-// <LICENSE-MIT or http://opensource.org/licenses/MIT>, at your\n-// option. This file may not be copied, modified, or distributed\n-// except according to those terms.\n-\n-use stack::Stack;\n-use std::uint;\n-use std::mem::transmute;\n-use std::rt::stack;\n-use std::raw;\n-#[cfg(target_arch = \"x86_64\")]\n-use std::simd;\n-use libc;\n-\n-// FIXME #7761: Registers is boxed so that it is 16-byte aligned, for storing\n-// SSE regs.  It would be marginally better not to do this. In C++ we\n-// use an attribute on a struct.\n-// FIXME #7761: It would be nice to define regs as `Box<Option<Registers>>`\n-// since the registers are sometimes empty, but the discriminant would\n-// then misalign the regs again.\n-pub struct Context {\n-    /// Hold the registers while the task or scheduler is suspended\n-    regs: Box<Registers>,\n-    /// Lower bound and upper bound for the stack\n-    stack_bounds: Option<(uint, uint)>,\n-}\n-\n-pub type InitFn = extern \"C\" fn(uint, *mut (), *mut ()) -> !;\n-\n-impl Context {\n-    pub fn empty() -> Context {\n-        Context {\n-            regs: new_regs(),\n-            stack_bounds: None,\n-        }\n-    }\n-\n-    /// Create a new context that will resume execution by running proc()\n-    ///\n-    /// The `init` function will be run with `arg` and the `start` procedure\n-    /// split up into code and env pointers. It is required that the `init`\n-    /// function never return.\n-    ///\n-    /// FIXME: this is basically an awful the interface. The main reason for\n-    ///        this is to reduce the number of allocations made when a green\n-    ///        task is spawned as much as possible\n-    pub fn new(init: InitFn, arg: uint, start: proc():Send,\n-               stack: &mut Stack) -> Context {\n-\n-        let sp: *const uint = stack.end();\n-        let sp: *mut uint = sp as *mut uint;\n-        // Save and then immediately load the current context,\n-        // which we will then modify to call the given function when restored\n-        let mut regs = new_regs();\n-\n-        initialize_call_frame(&mut *regs,\n-                              init,\n-                              arg,\n-                              unsafe { transmute(start) },\n-                              sp);\n-\n-        // Scheduler tasks don't have a stack in the \"we allocated it\" sense,\n-        // but rather they run on pthreads stacks. We have complete control over\n-        // them in terms of the code running on them (and hopefully they don't\n-        // overflow). Additionally, their coroutine stacks are listed as being\n-        // zero-length, so that's how we detect what's what here.\n-        let stack_base: *const uint = stack.start();\n-        let bounds = if sp as libc::uintptr_t == stack_base as libc::uintptr_t {\n-            None\n-        } else {\n-            Some((stack_base as uint, sp as uint))\n-        };\n-        return Context {\n-            regs: regs,\n-            stack_bounds: bounds,\n-        }\n-    }\n-\n-    /* Switch contexts\n-\n-    Suspend the current execution context and resume another by\n-    saving the registers values of the executing thread to a Context\n-    then loading the registers from a previously saved Context.\n-    */\n-    pub fn swap(out_context: &mut Context, in_context: &Context) {\n-        rtdebug!(\"swapping contexts\");\n-        let out_regs: &mut Registers = match out_context {\n-            &Context { regs: box ref mut r, .. } => r\n-        };\n-        let in_regs: &Registers = match in_context {\n-            &Context { regs: box ref r, .. } => r\n-        };\n-\n-        rtdebug!(\"noting the stack limit and doing raw swap\");\n-\n-        unsafe {\n-            // Right before we switch to the new context, set the new context's\n-            // stack limit in the OS-specified TLS slot. This also  means that\n-            // we cannot call any more rust functions after record_stack_bounds\n-            // returns because they would all likely panic due to the limit being\n-            // invalid for the current task. Lucky for us `rust_swap_registers`\n-            // is a C function so we don't have to worry about that!\n-            match in_context.stack_bounds {\n-                Some((lo, hi)) => stack::record_rust_managed_stack_bounds(lo, hi),\n-                // If we're going back to one of the original contexts or\n-                // something that's possibly not a \"normal task\", then reset\n-                // the stack limit to 0 to make morestack never panic\n-                None => stack::record_rust_managed_stack_bounds(0, uint::MAX),\n-            }\n-            rust_swap_registers(out_regs, in_regs);\n-        }\n-    }\n-}\n-\n-#[link(name = \"context_switch\", kind = \"static\")]\n-extern {\n-    fn rust_swap_registers(out_regs: *mut Registers, in_regs: *const Registers);\n-}\n-\n-// Register contexts used in various architectures\n-//\n-// These structures all represent a context of one task throughout its\n-// execution. Each struct is a representation of the architecture's register\n-// set. When swapping between tasks, these register sets are used to save off\n-// the current registers into one struct, and load them all from another.\n-//\n-// Note that this is only used for context switching, which means that some of\n-// the registers may go unused. For example, for architectures with\n-// callee/caller saved registers, the context will only reflect the callee-saved\n-// registers. This is because the caller saved registers are already stored\n-// elsewhere on the stack (if it was necessary anyway).\n-//\n-// Additionally, there may be fields on various architectures which are unused\n-// entirely because they only reflect what is theoretically possible for a\n-// \"complete register set\" to show, but user-space cannot alter these registers.\n-// An example of this would be the segment selectors for x86.\n-//\n-// These structures/functions are roughly in-sync with the source files inside\n-// of src/rt/arch/$arch. The only currently used function from those folders is\n-// the `rust_swap_registers` function, but that's only because for now segmented\n-// stacks are disabled.\n-\n-#[cfg(target_arch = \"x86\")]\n-#[repr(C)]\n-struct Registers {\n-    eax: u32, ebx: u32, ecx: u32, edx: u32,\n-    ebp: u32, esi: u32, edi: u32, esp: u32,\n-    cs: u16, ds: u16, ss: u16, es: u16, fs: u16, gs: u16,\n-    eflags: u32, eip: u32\n-}\n-\n-#[cfg(target_arch = \"x86\")]\n-fn new_regs() -> Box<Registers> {\n-    box Registers {\n-        eax: 0, ebx: 0, ecx: 0, edx: 0,\n-        ebp: 0, esi: 0, edi: 0, esp: 0,\n-        cs: 0, ds: 0, ss: 0, es: 0, fs: 0, gs: 0,\n-        eflags: 0, eip: 0\n-    }\n-}\n-\n-#[cfg(target_arch = \"x86\")]\n-fn initialize_call_frame(regs: &mut Registers, fptr: InitFn, arg: uint,\n-                         procedure: raw::Procedure, sp: *mut uint) {\n-    let sp = sp as *mut uint;\n-    // x86 has interesting stack alignment requirements, so do some alignment\n-    // plus some offsetting to figure out what the actual stack should be.\n-    let sp = align_down(sp);\n-    let sp = mut_offset(sp, -4);\n-\n-    unsafe { *mut_offset(sp, 2) = procedure.env as uint };\n-    unsafe { *mut_offset(sp, 1) = procedure.code as uint };\n-    unsafe { *mut_offset(sp, 0) = arg as uint };\n-    let sp = mut_offset(sp, -1);\n-    unsafe { *sp = 0 }; // The final return address\n-\n-    regs.esp = sp as u32;\n-    regs.eip = fptr as u32;\n-\n-    // Last base pointer on the stack is 0\n-    regs.ebp = 0;\n-}\n-\n-// windows requires saving more registers (both general and XMM), so the windows\n-// register context must be larger.\n-#[cfg(all(windows, target_arch = \"x86_64\"))]\n-#[repr(C)]\n-struct Registers {\n-    gpr:[libc::uintptr_t, ..14],\n-    _xmm:[simd::u32x4, ..10]\n-}\n-#[cfg(all(not(windows), target_arch = \"x86_64\"))]\n-#[repr(C)]\n-struct Registers {\n-    gpr:[libc::uintptr_t, ..10],\n-    _xmm:[simd::u32x4, ..6]\n-}\n-\n-#[cfg(all(windows, target_arch = \"x86_64\"))]\n-fn new_regs() -> Box<Registers> {\n-    box() Registers {\n-        gpr:[0,..14],\n-        _xmm:[simd::u32x4(0,0,0,0),..10]\n-    }\n-}\n-#[cfg(all(not(windows), target_arch = \"x86_64\"))]\n-fn new_regs() -> Box<Registers> {\n-    box() Registers {\n-        gpr:[0,..10],\n-        _xmm:[simd::u32x4(0,0,0,0),..6]\n-    }\n-}\n-\n-#[cfg(target_arch = \"x86_64\")]\n-fn initialize_call_frame(regs: &mut Registers, fptr: InitFn, arg: uint,\n-                         procedure: raw::Procedure, sp: *mut uint) {\n-    extern { fn rust_bootstrap_green_task(); }\n-\n-    // Redefinitions from rt/arch/x86_64/regs.h\n-    static RUSTRT_RSP: uint = 1;\n-    static RUSTRT_IP: uint = 8;\n-    static RUSTRT_RBP: uint = 2;\n-    static RUSTRT_R12: uint = 4;\n-    static RUSTRT_R13: uint = 5;\n-    static RUSTRT_R14: uint = 6;\n-    static RUSTRT_R15: uint = 7;\n-\n-    let sp = align_down(sp);\n-    let sp = mut_offset(sp, -1);\n-\n-    // The final return address. 0 indicates the bottom of the stack\n-    unsafe { *sp = 0; }\n-\n-    rtdebug!(\"creating call frame\");\n-    rtdebug!(\"fptr {:#x}\", fptr as libc::uintptr_t);\n-    rtdebug!(\"arg {:#x}\", arg);\n-    rtdebug!(\"sp {}\", sp);\n-\n-    // These registers are frobbed by rust_bootstrap_green_task into the right\n-    // location so we can invoke the \"real init function\", `fptr`.\n-    regs.gpr[RUSTRT_R12] = arg as libc::uintptr_t;\n-    regs.gpr[RUSTRT_R13] = procedure.code as libc::uintptr_t;\n-    regs.gpr[RUSTRT_R14] = procedure.env as libc::uintptr_t;\n-    regs.gpr[RUSTRT_R15] = fptr as libc::uintptr_t;\n-\n-    // These registers are picked up by the regular context switch paths. These\n-    // will put us in \"mostly the right context\" except for frobbing all the\n-    // arguments to the right place. We have the small trampoline code inside of\n-    // rust_bootstrap_green_task to do that.\n-    regs.gpr[RUSTRT_RSP] = sp as libc::uintptr_t;\n-    regs.gpr[RUSTRT_IP] = rust_bootstrap_green_task as libc::uintptr_t;\n-\n-    // Last base pointer on the stack should be 0\n-    regs.gpr[RUSTRT_RBP] = 0;\n-}\n-\n-#[cfg(target_arch = \"arm\")]\n-type Registers = [libc::uintptr_t, ..32];\n-\n-#[cfg(target_arch = \"arm\")]\n-fn new_regs() -> Box<Registers> { box {[0, .. 32]} }\n-\n-#[cfg(target_arch = \"arm\")]\n-fn initialize_call_frame(regs: &mut Registers, fptr: InitFn, arg: uint,\n-                         procedure: raw::Procedure, sp: *mut uint) {\n-    extern { fn rust_bootstrap_green_task(); }\n-\n-    let sp = align_down(sp);\n-    // sp of arm eabi is 8-byte aligned\n-    let sp = mut_offset(sp, -2);\n-\n-    // The final return address. 0 indicates the bottom of the stack\n-    unsafe { *sp = 0; }\n-\n-    // ARM uses the same technique as x86_64 to have a landing pad for the start\n-    // of all new green tasks. Neither r1/r2 are saved on a context switch, so\n-    // the shim will copy r3/r4 into r1/r2 and then execute the function in r5\n-    regs[0] = arg as libc::uintptr_t;              // r0\n-    regs[3] = procedure.code as libc::uintptr_t;   // r3\n-    regs[4] = procedure.env as libc::uintptr_t;    // r4\n-    regs[5] = fptr as libc::uintptr_t;             // r5\n-    regs[13] = sp as libc::uintptr_t;                          // #52 sp, r13\n-    regs[14] = rust_bootstrap_green_task as libc::uintptr_t;   // #56 pc, r14 --> lr\n-}\n-\n-#[cfg(any(target_arch = \"mips\", target_arch = \"mipsel\"))]\n-type Registers = [libc::uintptr_t, ..32];\n-\n-#[cfg(any(target_arch = \"mips\", target_arch = \"mipsel\"))]\n-fn new_regs() -> Box<Registers> { box {[0, .. 32]} }\n-\n-#[cfg(any(target_arch = \"mips\", target_arch = \"mipsel\"))]\n-fn initialize_call_frame(regs: &mut Registers, fptr: InitFn, arg: uint,\n-                         procedure: raw::Procedure, sp: *mut uint) {\n-    let sp = align_down(sp);\n-    // sp of mips o32 is 8-byte aligned\n-    let sp = mut_offset(sp, -2);\n-\n-    // The final return address. 0 indicates the bottom of the stack\n-    unsafe { *sp = 0; }\n-\n-    regs[4] = arg as libc::uintptr_t;\n-    regs[5] = procedure.code as libc::uintptr_t;\n-    regs[6] = procedure.env as libc::uintptr_t;\n-    regs[29] = sp as libc::uintptr_t;\n-    regs[25] = fptr as libc::uintptr_t;\n-    regs[31] = fptr as libc::uintptr_t;\n-}\n-\n-fn align_down(sp: *mut uint) -> *mut uint {\n-    let sp = (sp as uint) & !(16 - 1);\n-    sp as *mut uint\n-}\n-\n-// ptr::mut_offset is positive ints only\n-#[inline]\n-pub fn mut_offset<T>(ptr: *mut T, count: int) -> *mut T {\n-    use std::mem::size_of;\n-    (ptr as int + count * (size_of::<T>() as int)) as *mut T\n-}"}, {"sha": "f2e64dc25a970423d442a5e3e8dbda92250ebb0f", "filename": "src/libgreen/coroutine.rs", "status": "removed", "additions": 0, "deletions": 44, "changes": 44, "blob_url": "https://github.com/rust-lang/rust/blob/3ee916e50bd86768cb2a9141f9b2c52d2601b412/src%2Flibgreen%2Fcoroutine.rs", "raw_url": "https://github.com/rust-lang/rust/raw/3ee916e50bd86768cb2a9141f9b2c52d2601b412/src%2Flibgreen%2Fcoroutine.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibgreen%2Fcoroutine.rs?ref=3ee916e50bd86768cb2a9141f9b2c52d2601b412", "patch": "@@ -1,44 +0,0 @@\n-// Copyright 2013 The Rust Project Developers. See the COPYRIGHT\n-// file at the top-level directory of this distribution and at\n-// http://rust-lang.org/COPYRIGHT.\n-//\n-// Licensed under the Apache License, Version 2.0 <LICENSE-APACHE or\n-// http://www.apache.org/licenses/LICENSE-2.0> or the MIT license\n-// <LICENSE-MIT or http://opensource.org/licenses/MIT>, at your\n-// option. This file may not be copied, modified, or distributed\n-// except according to those terms.\n-\n-// Coroutines represent nothing more than a context and a stack\n-// segment.\n-\n-use context::Context;\n-use stack::{StackPool, Stack};\n-\n-/// A coroutine is nothing more than a (register context, stack) pair.\n-pub struct Coroutine {\n-    /// The segment of stack on which the task is currently running or\n-    /// if the task is blocked, on which the task will resume\n-    /// execution.\n-    ///\n-    /// Servo needs this to be public in order to tell SpiderMonkey\n-    /// about the stack bounds.\n-    pub current_stack_segment: Stack,\n-\n-    /// Always valid if the task is alive and not running.\n-    pub saved_context: Context\n-}\n-\n-impl Coroutine {\n-    pub fn empty() -> Coroutine {\n-        Coroutine {\n-            current_stack_segment: unsafe { Stack::dummy_stack() },\n-            saved_context: Context::empty()\n-        }\n-    }\n-\n-    /// Destroy coroutine and try to reuse std::stack segment.\n-    pub fn recycle(self, stack_pool: &mut StackPool) {\n-        let Coroutine { current_stack_segment, .. } = self;\n-        stack_pool.give_stack(current_stack_segment);\n-    }\n-}"}, {"sha": "4e2908dd2b025b16fde151f25dfc7cbbc3ae8f38", "filename": "src/libgreen/lib.rs", "status": "removed", "additions": 0, "deletions": 567, "changes": 567, "blob_url": "https://github.com/rust-lang/rust/blob/3ee916e50bd86768cb2a9141f9b2c52d2601b412/src%2Flibgreen%2Flib.rs", "raw_url": "https://github.com/rust-lang/rust/raw/3ee916e50bd86768cb2a9141f9b2c52d2601b412/src%2Flibgreen%2Flib.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibgreen%2Flib.rs?ref=3ee916e50bd86768cb2a9141f9b2c52d2601b412", "patch": "@@ -1,567 +0,0 @@\n-// Copyright 2013 The Rust Project Developers. See the COPYRIGHT\n-// file at the top-level directory of this distribution and at\n-// http://rust-lang.org/COPYRIGHT.\n-//\n-// Licensed under the Apache License, Version 2.0 <LICENSE-APACHE or\n-// http://www.apache.org/licenses/LICENSE-2.0> or the MIT license\n-// <LICENSE-MIT or http://opensource.org/licenses/MIT>, at your\n-// option. This file may not be copied, modified, or distributed\n-// except according to those terms.\n-\n-//! The \"green scheduling\" library\n-//!\n-//! This library provides M:N threading for rust programs. Internally this has\n-//! the implementation of a green scheduler along with context switching and a\n-//! stack-allocation strategy. This can be optionally linked in to rust\n-//! programs in order to provide M:N functionality inside of 1:1 programs.\n-//!\n-//! # Architecture\n-//!\n-//! An M:N scheduling library implies that there are N OS thread upon which M\n-//! \"green threads\" are multiplexed. In other words, a set of green threads are\n-//! all run inside a pool of OS threads.\n-//!\n-//! With this design, you can achieve _concurrency_ by spawning many green\n-//! threads, and you can achieve _parallelism_ by running the green threads\n-//! simultaneously on multiple OS threads. Each OS thread is a candidate for\n-//! being scheduled on a different core (the source of parallelism), and then\n-//! all of the green threads cooperatively schedule amongst one another (the\n-//! source of concurrency).\n-//!\n-//! ## Schedulers\n-//!\n-//! In order to coordinate among green threads, each OS thread is primarily\n-//! running something which we call a Scheduler. Whenever a reference to a\n-//! Scheduler is made, it is synonymous to referencing one OS thread. Each\n-//! scheduler is bound to one and exactly one OS thread, and the thread that it\n-//! is bound to never changes.\n-//!\n-//! Each scheduler is connected to a pool of other schedulers (a `SchedPool`)\n-//! which is the thread pool term from above. A pool of schedulers all share the\n-//! work that they create. Furthermore, whenever a green thread is created (also\n-//! synonymously referred to as a green task), it is associated with a\n-//! `SchedPool` forevermore. A green thread cannot leave its scheduler pool.\n-//!\n-//! Schedulers can have at most one green thread running on them at a time. When\n-//! a scheduler is asleep on its event loop, there are no green tasks running on\n-//! the OS thread or the scheduler. The term \"context switch\" is used for when\n-//! the running green thread is swapped out, but this simply changes the one\n-//! green thread which is running on the scheduler.\n-//!\n-//! ## Green Threads\n-//!\n-//! A green thread can largely be summarized by a stack and a register context.\n-//! Whenever a green thread is spawned, it allocates a stack, and then prepares\n-//! a register context for execution. The green task may be executed across\n-//! multiple OS threads, but it will always use the same stack and it will carry\n-//! its register context across OS threads.\n-//!\n-//! Each green thread is cooperatively scheduled with other green threads.\n-//! Primarily, this means that there is no pre-emption of a green thread. The\n-//! major consequence of this design is that a green thread stuck in an infinite\n-//! loop will prevent all other green threads from running on that particular\n-//! scheduler.\n-//!\n-//! Scheduling events for green threads occur on communication and I/O\n-//! boundaries. For example, if a green task blocks waiting for a message on a\n-//! channel some other green thread can now run on the scheduler. This also has\n-//! the consequence that until a green thread performs any form of scheduling\n-//! event, it will be running on the same OS thread (unconditionally).\n-//!\n-//! ## Work Stealing\n-//!\n-//! With a pool of schedulers, a new green task has a number of options when\n-//! deciding where to run initially. The current implementation uses a concept\n-//! called work stealing in order to spread out work among schedulers.\n-//!\n-//! In a work-stealing model, each scheduler maintains a local queue of tasks to\n-//! run, and this queue is stolen from by other schedulers. Implementation-wise,\n-//! work stealing has some hairy parts, but from a user-perspective, work\n-//! stealing simply implies what with M green threads and N schedulers where\n-//! M > N it is very likely that all schedulers will be busy executing work.\n-//!\n-//! # Considerations when using libgreen\n-//!\n-//! An M:N runtime has both pros and cons, and there is no one answer as to\n-//! whether M:N or 1:1 is appropriate to use. As always, there are many\n-//! advantages and disadvantages between the two. Regardless of the workload,\n-//! however, there are some aspects of using green thread which you should be\n-//! aware of:\n-//!\n-//! * The largest concern when using libgreen is interoperating with native\n-//!   code. Care should be taken when calling native code that will block the OS\n-//!   thread as it will prevent further green tasks from being scheduled on the\n-//!   OS thread.\n-//!\n-//! * Native code using thread-local-storage should be approached\n-//!   with care. Green threads may migrate among OS threads at any time, so\n-//!   native libraries using thread-local state may not always work.\n-//!\n-//! * Native synchronization primitives (e.g. pthread mutexes) will also not\n-//!   work for green threads. The reason for this is because native primitives\n-//!   often operate on a _os thread_ granularity whereas green threads are\n-//!   operating on a more granular unit of work.\n-//!\n-//! * A green threading runtime is not fork-safe. If the process forks(), it\n-//!   cannot expect to make reasonable progress by continuing to use green\n-//!   threads.\n-//!\n-//! Note that these concerns do not mean that operating with native code is a\n-//! lost cause. These are simply just concerns which should be considered when\n-//! invoking native code.\n-//!\n-//! # Starting with libgreen\n-//!\n-//! ```rust\n-//! extern crate green;\n-//!\n-//! #[start]\n-//! fn start(argc: int, argv: *const *const u8) -> int {\n-//!     green::start(argc, argv, green::basic::event_loop, main)\n-//! }\n-//!\n-//! fn main() {\n-//!     // this code is running in a pool of schedulers\n-//! }\n-//! ```\n-//!\n-//! > **Note**: This `main` function in this example does *not* have I/O\n-//! >           support. The basic event loop does not provide any support\n-//!\n-//! # Using a scheduler pool\n-//!\n-//! This library adds a `GreenTaskBuilder` trait that extends the methods\n-//! available on `std::task::TaskBuilder` to allow spawning a green task,\n-//! possibly pinned to a particular scheduler thread:\n-//!\n-//! ```rust\n-//! extern crate green;\n-//!\n-//! # fn main() {\n-//! use std::task::TaskBuilder;\n-//! use green::{SchedPool, PoolConfig, GreenTaskBuilder};\n-//!\n-//! let mut config = PoolConfig::new();\n-//!\n-//! let mut pool = SchedPool::new(config);\n-//!\n-//! // Spawn tasks into the pool of schedulers\n-//! TaskBuilder::new().green(&mut pool).spawn(proc() {\n-//!     // this code is running inside the pool of schedulers\n-//!\n-//!     spawn(proc() {\n-//!         // this code is also running inside the same scheduler pool\n-//!     });\n-//! });\n-//!\n-//! // Dynamically add a new scheduler to the scheduler pool. This adds another\n-//! // OS thread that green threads can be multiplexed on to.\n-//! let mut handle = pool.spawn_sched();\n-//!\n-//! // Pin a task to the spawned scheduler\n-//! TaskBuilder::new().green_pinned(&mut pool, &mut handle).spawn(proc() {\n-//!     /* ... */\n-//! });\n-//!\n-//! // Handles keep schedulers alive, so be sure to drop all handles before\n-//! // destroying the sched pool\n-//! drop(handle);\n-//!\n-//! // Required to shut down this scheduler pool.\n-//! // The task will panic if `shutdown` is not called.\n-//! pool.shutdown();\n-//! # }\n-//! ```\n-\n-#![crate_name = \"green\"]\n-#![experimental]\n-#![license = \"MIT/ASL2\"]\n-#![crate_type = \"rlib\"]\n-#![crate_type = \"dylib\"]\n-#![doc(html_logo_url = \"http://www.rust-lang.org/logos/rust-logo-128x128-blk-v2.png\",\n-       html_favicon_url = \"http://www.rust-lang.org/favicon.ico\",\n-       html_root_url = \"http://doc.rust-lang.org/nightly/\",\n-       html_playground_url = \"http://play.rust-lang.org/\")]\n-\n-#![feature(macro_rules, phase, default_type_params, globs)]\n-#![allow(deprecated)]\n-\n-#[cfg(test)] #[phase(plugin, link)] extern crate log;\n-extern crate libc;\n-extern crate alloc;\n-\n-use alloc::arc::Arc;\n-use std::mem::replace;\n-use std::os;\n-use std::rt::rtio;\n-use std::rt::thread::Thread;\n-use std::rt::task::TaskOpts;\n-use std::rt;\n-use std::sync::atomic::{SeqCst, AtomicUint, INIT_ATOMIC_UINT};\n-use std::sync::deque;\n-use std::task::{TaskBuilder, Spawner};\n-\n-use sched::{Shutdown, Scheduler, SchedHandle, TaskFromFriend, PinnedTask, NewNeighbor};\n-use sleeper_list::SleeperList;\n-use stack::StackPool;\n-use task::GreenTask;\n-\n-mod macros;\n-mod simple;\n-mod message_queue;\n-\n-pub mod basic;\n-pub mod context;\n-pub mod coroutine;\n-pub mod sched;\n-pub mod sleeper_list;\n-pub mod stack;\n-pub mod task;\n-\n-/// Set up a default runtime configuration, given compiler-supplied arguments.\n-///\n-/// This function will block until the entire pool of M:N schedulers have\n-/// exited. This function also requires a local task to be available.\n-///\n-/// # Arguments\n-///\n-/// * `argc` & `argv` - The argument vector. On Unix this information is used\n-///   by os::args.\n-/// * `main` - The initial procedure to run inside of the M:N scheduling pool.\n-///            Once this procedure exits, the scheduling pool will begin to shut\n-///            down. The entire pool (and this function) will only return once\n-///            all child tasks have finished executing.\n-///\n-/// # Return value\n-///\n-/// The return value is used as the process return code. 0 on success, 101 on\n-/// error.\n-pub fn start(argc: int, argv: *const *const u8,\n-             event_loop_factory: fn() -> Box<rtio::EventLoop + Send>,\n-             main: proc():Send) -> int {\n-    rt::init(argc, argv);\n-    let mut main = Some(main);\n-    let mut ret = None;\n-    simple::task().run(|| {\n-        ret = Some(run(event_loop_factory, main.take().unwrap()));\n-    }).destroy();\n-    // unsafe is ok b/c we're sure that the runtime is gone\n-    unsafe { rt::cleanup() }\n-    ret.unwrap()\n-}\n-\n-/// Execute the main function in a pool of M:N schedulers.\n-///\n-/// Configures the runtime according to the environment, by default using a task\n-/// scheduler with the same number of threads as cores.  Returns a process exit\n-/// code.\n-///\n-/// This function will not return until all schedulers in the associated pool\n-/// have returned.\n-pub fn run(event_loop_factory: fn() -> Box<rtio::EventLoop + Send>,\n-           main: proc():Send) -> int {\n-    // Create a scheduler pool and spawn the main task into this pool. We will\n-    // get notified over a channel when the main task exits.\n-    let mut cfg = PoolConfig::new();\n-    cfg.event_loop_factory = event_loop_factory;\n-    let mut pool = SchedPool::new(cfg);\n-    let (tx, rx) = channel();\n-    let mut opts = TaskOpts::new();\n-    opts.on_exit = Some(proc(r) tx.send(r));\n-    opts.name = Some(\"<main>\".into_maybe_owned());\n-    pool.spawn(opts, main);\n-\n-    // Wait for the main task to return, and set the process error code\n-    // appropriately.\n-    if rx.recv().is_err() {\n-        os::set_exit_status(rt::DEFAULT_ERROR_CODE);\n-    }\n-\n-    // Now that we're sure all tasks are dead, shut down the pool of schedulers,\n-    // waiting for them all to return.\n-    pool.shutdown();\n-    os::get_exit_status()\n-}\n-\n-/// Configuration of how an M:N pool of schedulers is spawned.\n-pub struct PoolConfig {\n-    /// The number of schedulers (OS threads) to spawn into this M:N pool.\n-    pub threads: uint,\n-    /// A factory function used to create new event loops. If this is not\n-    /// specified then the default event loop factory is used.\n-    pub event_loop_factory: fn() -> Box<rtio::EventLoop + Send>,\n-}\n-\n-impl PoolConfig {\n-    /// Returns the default configuration, as determined the environment\n-    /// variables of this process.\n-    pub fn new() -> PoolConfig {\n-        PoolConfig {\n-            threads: rt::default_sched_threads(),\n-            event_loop_factory: basic::event_loop,\n-        }\n-    }\n-}\n-\n-/// A structure representing a handle to a pool of schedulers. This handle is\n-/// used to keep the pool alive and also reap the status from the pool.\n-pub struct SchedPool {\n-    id: uint,\n-    threads: Vec<Thread<()>>,\n-    handles: Vec<SchedHandle>,\n-    stealers: Vec<deque::Stealer<Box<task::GreenTask>>>,\n-    next_friend: uint,\n-    stack_pool: StackPool,\n-    deque_pool: deque::BufferPool<Box<task::GreenTask>>,\n-    sleepers: SleeperList,\n-    factory: fn() -> Box<rtio::EventLoop + Send>,\n-    task_state: TaskState,\n-    tasks_done: Receiver<()>,\n-}\n-\n-/// This is an internal state shared among a pool of schedulers. This is used to\n-/// keep track of how many tasks are currently running in the pool and then\n-/// sending on a channel once the entire pool has been drained of all tasks.\n-#[deriving(Clone)]\n-pub struct TaskState {\n-    cnt: Arc<AtomicUint>,\n-    done: Sender<()>,\n-}\n-\n-impl SchedPool {\n-    /// Execute the main function in a pool of M:N schedulers.\n-    ///\n-    /// This will configure the pool according to the `config` parameter, and\n-    /// initially run `main` inside the pool of schedulers.\n-    pub fn new(config: PoolConfig) -> SchedPool {\n-        static POOL_ID: AtomicUint = INIT_ATOMIC_UINT;\n-\n-        let PoolConfig {\n-            threads: nscheds,\n-            event_loop_factory: factory\n-        } = config;\n-        assert!(nscheds > 0);\n-\n-        // The pool of schedulers that will be returned from this function\n-        let (p, state) = TaskState::new();\n-        let mut pool = SchedPool {\n-            threads: vec![],\n-            handles: vec![],\n-            stealers: vec![],\n-            id: POOL_ID.fetch_add(1, SeqCst),\n-            sleepers: SleeperList::new(),\n-            stack_pool: StackPool::new(),\n-            deque_pool: deque::BufferPool::new(),\n-            next_friend: 0,\n-            factory: factory,\n-            task_state: state,\n-            tasks_done: p,\n-        };\n-\n-        // Create a work queue for each scheduler, ntimes. Create an extra\n-        // for the main thread if that flag is set. We won't steal from it.\n-        let mut workers = Vec::with_capacity(nscheds);\n-        let mut stealers = Vec::with_capacity(nscheds);\n-\n-        for _ in range(0, nscheds) {\n-            let (w, s) = pool.deque_pool.deque();\n-            workers.push(w);\n-            stealers.push(s);\n-        }\n-        pool.stealers = stealers;\n-\n-        // Now that we've got all our work queues, create one scheduler per\n-        // queue, spawn the scheduler into a thread, and be sure to keep a\n-        // handle to the scheduler and the thread to keep them alive.\n-        for worker in workers.into_iter() {\n-            rtdebug!(\"inserting a regular scheduler\");\n-\n-            let mut sched = box Scheduler::new(pool.id,\n-                                            (pool.factory)(),\n-                                            worker,\n-                                            pool.stealers.clone(),\n-                                            pool.sleepers.clone(),\n-                                            pool.task_state.clone());\n-            pool.handles.push(sched.make_handle());\n-            pool.threads.push(Thread::start(proc() { sched.bootstrap(); }));\n-        }\n-\n-        return pool;\n-    }\n-\n-    /// Creates a new task configured to run inside of this pool of schedulers.\n-    /// This is useful to create a task which can then be sent to a specific\n-    /// scheduler created by `spawn_sched` (and possibly pin it to that\n-    /// scheduler).\n-    #[deprecated = \"use the green and green_pinned methods of GreenTaskBuilder instead\"]\n-    pub fn task(&mut self, opts: TaskOpts, f: proc():Send) -> Box<GreenTask> {\n-        GreenTask::configure(&mut self.stack_pool, opts, f)\n-    }\n-\n-    /// Spawns a new task into this pool of schedulers, using the specified\n-    /// options to configure the new task which is spawned.\n-    ///\n-    /// New tasks are spawned in a round-robin fashion to the schedulers in this\n-    /// pool, but tasks can certainly migrate among schedulers once they're in\n-    /// the pool.\n-    #[deprecated = \"use the green and green_pinned methods of GreenTaskBuilder instead\"]\n-    pub fn spawn(&mut self, opts: TaskOpts, f: proc():Send) {\n-        let task = self.task(opts, f);\n-\n-        // Figure out someone to send this task to\n-        let idx = self.next_friend;\n-        self.next_friend += 1;\n-        if self.next_friend >= self.handles.len() {\n-            self.next_friend = 0;\n-        }\n-\n-        // Jettison the task away!\n-        self.handles[idx].send(TaskFromFriend(task));\n-    }\n-\n-    /// Spawns a new scheduler into this M:N pool. A handle is returned to the\n-    /// scheduler for use. The scheduler will not exit as long as this handle is\n-    /// active.\n-    ///\n-    /// The scheduler spawned will participate in work stealing with all of the\n-    /// other schedulers currently in the scheduler pool.\n-    pub fn spawn_sched(&mut self) -> SchedHandle {\n-        let (worker, stealer) = self.deque_pool.deque();\n-        self.stealers.push(stealer.clone());\n-\n-        // Tell all existing schedulers about this new scheduler so they can all\n-        // steal work from it\n-        for handle in self.handles.iter_mut() {\n-            handle.send(NewNeighbor(stealer.clone()));\n-        }\n-\n-        // Create the new scheduler, using the same sleeper list as all the\n-        // other schedulers as well as having a stealer handle to all other\n-        // schedulers.\n-        let mut sched = box Scheduler::new(self.id,\n-                                        (self.factory)(),\n-                                        worker,\n-                                        self.stealers.clone(),\n-                                        self.sleepers.clone(),\n-                                        self.task_state.clone());\n-        let ret = sched.make_handle();\n-        self.handles.push(sched.make_handle());\n-        self.threads.push(Thread::start(proc() { sched.bootstrap() }));\n-\n-        return ret;\n-    }\n-\n-    /// Consumes the pool of schedulers, waiting for all tasks to exit and all\n-    /// schedulers to shut down.\n-    ///\n-    /// This function is required to be called in order to drop a pool of\n-    /// schedulers, it is considered an error to drop a pool without calling\n-    /// this method.\n-    ///\n-    /// This only waits for all tasks in *this pool* of schedulers to exit, any\n-    /// native tasks or extern pools will not be waited on\n-    pub fn shutdown(mut self) {\n-        self.stealers = vec![];\n-\n-        // Wait for everyone to exit. We may have reached a 0-task count\n-        // multiple times in the past, meaning there could be several buffered\n-        // messages on the `tasks_done` port. We're guaranteed that after *some*\n-        // message the current task count will be 0, so we just receive in a\n-        // loop until everything is totally dead.\n-        while self.task_state.active() {\n-            self.tasks_done.recv();\n-        }\n-\n-        // Now that everyone's gone, tell everything to shut down.\n-        for mut handle in replace(&mut self.handles, vec![]).into_iter() {\n-            handle.send(Shutdown);\n-        }\n-        for thread in replace(&mut self.threads, vec![]).into_iter() {\n-            thread.join();\n-        }\n-    }\n-}\n-\n-impl TaskState {\n-    fn new() -> (Receiver<()>, TaskState) {\n-        let (tx, rx) = channel();\n-        (rx, TaskState {\n-            cnt: Arc::new(AtomicUint::new(0)),\n-            done: tx,\n-        })\n-    }\n-\n-    fn increment(&mut self) {\n-        self.cnt.fetch_add(1, SeqCst);\n-    }\n-\n-    fn active(&self) -> bool {\n-        self.cnt.load(SeqCst) != 0\n-    }\n-\n-    fn decrement(&mut self) {\n-        let prev = self.cnt.fetch_sub(1, SeqCst);\n-        if prev == 1 {\n-            self.done.send(());\n-        }\n-    }\n-}\n-\n-impl Drop for SchedPool {\n-    fn drop(&mut self) {\n-        if self.threads.len() > 0 {\n-            panic!(\"dropping a M:N scheduler pool that wasn't shut down\");\n-        }\n-    }\n-}\n-\n-/// A spawner for green tasks\n-pub struct GreenSpawner<'a>{\n-    pool: &'a mut SchedPool,\n-    handle: Option<&'a mut SchedHandle>\n-}\n-\n-impl<'a> Spawner for GreenSpawner<'a> {\n-    #[inline]\n-    fn spawn(self, opts: TaskOpts, f: proc():Send) {\n-        let GreenSpawner { pool, handle } = self;\n-        match handle {\n-            None    => pool.spawn(opts, f),\n-            Some(h) => h.send(PinnedTask(pool.task(opts, f)))\n-        }\n-    }\n-}\n-\n-/// An extension trait adding `green` configuration methods to `TaskBuilder`.\n-pub trait GreenTaskBuilder {\n-    fn green<'a>(self, &'a mut SchedPool) -> TaskBuilder<GreenSpawner<'a>>;\n-    fn green_pinned<'a>(self, &'a mut SchedPool, &'a mut SchedHandle)\n-                        -> TaskBuilder<GreenSpawner<'a>>;\n-}\n-\n-impl<S: Spawner> GreenTaskBuilder for TaskBuilder<S> {\n-    fn green<'a>(self, pool: &'a mut SchedPool) -> TaskBuilder<GreenSpawner<'a>> {\n-        self.spawner(GreenSpawner {pool: pool, handle: None})\n-    }\n-\n-    fn green_pinned<'a>(self, pool: &'a mut SchedPool, handle: &'a mut SchedHandle)\n-                        -> TaskBuilder<GreenSpawner<'a>> {\n-        self.spawner(GreenSpawner {pool: pool, handle: Some(handle)})\n-    }\n-}\n-\n-#[cfg(test)]\n-mod test {\n-    use std::task::TaskBuilder;\n-    use super::{SchedPool, PoolConfig, GreenTaskBuilder};\n-\n-    #[test]\n-    fn test_green_builder() {\n-        let mut pool = SchedPool::new(PoolConfig::new());\n-        let res = TaskBuilder::new().green(&mut pool).try(proc() {\n-            \"Success!\".to_string()\n-        });\n-        assert_eq!(res.ok().unwrap(), \"Success!\".to_string());\n-        pool.shutdown();\n-    }\n-}"}, {"sha": "4cce430d88a8d3522e1e75e776a64ab1a29cd97b", "filename": "src/libgreen/macros.rs", "status": "removed", "additions": 0, "deletions": 118, "changes": 118, "blob_url": "https://github.com/rust-lang/rust/blob/3ee916e50bd86768cb2a9141f9b2c52d2601b412/src%2Flibgreen%2Fmacros.rs", "raw_url": "https://github.com/rust-lang/rust/raw/3ee916e50bd86768cb2a9141f9b2c52d2601b412/src%2Flibgreen%2Fmacros.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibgreen%2Fmacros.rs?ref=3ee916e50bd86768cb2a9141f9b2c52d2601b412", "patch": "@@ -1,118 +0,0 @@\n-// Copyright 2013 The Rust Project Developers. See the COPYRIGHT\n-// file at the top-level directory of this distribution and at\n-// http://rust-lang.org/COPYRIGHT.\n-//\n-// Licensed under the Apache License, Version 2.0 <LICENSE-APACHE or\n-// http://www.apache.org/licenses/LICENSE-2.0> or the MIT license\n-// <LICENSE-MIT or http://opensource.org/licenses/MIT>, at your\n-// option. This file may not be copied, modified, or distributed\n-// except according to those terms.\n-\n-// FIXME: this file probably shouldn't exist\n-// ignore-lexer-test FIXME #15677\n-\n-#![macro_escape]\n-\n-use std::fmt;\n-\n-// Indicates whether we should perform expensive sanity checks, including rtassert!\n-// FIXME: Once the runtime matures remove the `true` below to turn off rtassert, etc.\n-pub static ENFORCE_SANITY: bool = true || !cfg!(rtopt) || cfg!(rtdebug) || cfg!(rtassert);\n-\n-macro_rules! rterrln (\n-    ($($arg:tt)*) => ( {\n-        format_args!(::macros::dumb_println, $($arg)*)\n-    } )\n-)\n-\n-// Some basic logging. Enabled by passing `--cfg rtdebug` to the libstd build.\n-macro_rules! rtdebug (\n-    ($($arg:tt)*) => ( {\n-        if cfg!(rtdebug) {\n-            rterrln!($($arg)*)\n-        }\n-    })\n-)\n-\n-macro_rules! rtassert (\n-    ( $arg:expr ) => ( {\n-        if ::macros::ENFORCE_SANITY {\n-            if !$arg {\n-                rtabort!(\" assertion failed: {}\", stringify!($arg));\n-            }\n-        }\n-    } )\n-)\n-\n-\n-macro_rules! rtabort (\n-    ($($arg:tt)*) => ( {\n-        ::macros::abort(format!($($arg)*).as_slice());\n-    } )\n-)\n-\n-pub fn dumb_println(args: &fmt::Arguments) {\n-    use std::rt;\n-    let mut w = rt::Stderr;\n-    let _ = writeln!(&mut w, \"{}\", args);\n-}\n-\n-pub fn abort(msg: &str) -> ! {\n-    let msg = if !msg.is_empty() { msg } else { \"aborted\" };\n-    let hash = msg.chars().fold(0, |accum, val| accum + (val as uint) );\n-    let quote = match hash % 10 {\n-        0 => \"\n-It was from the artists and poets that the pertinent answers came, and I\n-know that panic would have broken loose had they been able to compare notes.\n-As it was, lacking their original letters, I half suspected the compiler of\n-having asked leading questions, or of having edited the correspondence in\n-corroboration of what he had latently resolved to see.\",\n-        1 => \"\n-There are not many persons who know what wonders are opened to them in the\n-stories and visions of their youth; for when as children we listen and dream,\n-we think but half-formed thoughts, and when as men we try to remember, we are\n-dulled and prosaic with the poison of life. But some of us awake in the night\n-with strange phantasms of enchanted hills and gardens, of fountains that sing\n-in the sun, of golden cliffs overhanging murmuring seas, of plains that stretch\n-down to sleeping cities of bronze and stone, and of shadowy companies of heroes\n-that ride caparisoned white horses along the edges of thick forests; and then\n-we know that we have looked back through the ivory gates into that world of\n-wonder which was ours before we were wise and unhappy.\",\n-        2 => \"\n-Instead of the poems I had hoped for, there came only a shuddering blackness\n-and ineffable loneliness; and I saw at last a fearful truth which no one had\n-ever dared to breathe before \u2014 the unwhisperable secret of secrets \u2014 The fact\n-that this city of stone and stridor is not a sentient perpetuation of Old New\n-York as London is of Old London and Paris of Old Paris, but that it is in fact\n-quite dead, its sprawling body imperfectly embalmed and infested with queer\n-animate things which have nothing to do with it as it was in life.\",\n-        3 => \"\n-The ocean ate the last of the land and poured into the smoking gulf, thereby\n-giving up all it had ever conquered. From the new-flooded lands it flowed\n-again, uncovering death and decay; and from its ancient and immemorial bed it\n-trickled loathsomely, uncovering nighted secrets of the years when Time was\n-young and the gods unborn. Above the waves rose weedy remembered spires. The\n-moon laid pale lilies of light on dead London, and Paris stood up from its damp\n-grave to be sanctified with star-dust. Then rose spires and monoliths that were\n-weedy but not remembered; terrible spires and monoliths of lands that men never\n-knew were lands...\",\n-        4 => \"\n-There was a night when winds from unknown spaces whirled us irresistibly into\n-limitless vacuum beyond all thought and entity. Perceptions of the most\n-maddeningly untransmissible sort thronged upon us; perceptions of infinity\n-which at the time convulsed us with joy, yet which are now partly lost to my\n-memory and partly incapable of presentation to others.\",\n-        _ => \"You've met with a terrible fate, haven't you?\"\n-    };\n-    rterrln!(\"{}\", \"\");\n-    rterrln!(\"{}\", quote);\n-    rterrln!(\"{}\", \"\");\n-    rterrln!(\"fatal runtime error: {}\", msg);\n-\n-    abort();\n-\n-    fn abort() -> ! {\n-        use std::intrinsics;\n-        unsafe { intrinsics::abort() }\n-    }\n-}"}, {"sha": "c589a9fb592d8804d1407b0adabd1575005d6dd3", "filename": "src/libgreen/message_queue.rs", "status": "removed", "additions": 0, "deletions": 67, "changes": 67, "blob_url": "https://github.com/rust-lang/rust/blob/3ee916e50bd86768cb2a9141f9b2c52d2601b412/src%2Flibgreen%2Fmessage_queue.rs", "raw_url": "https://github.com/rust-lang/rust/raw/3ee916e50bd86768cb2a9141f9b2c52d2601b412/src%2Flibgreen%2Fmessage_queue.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibgreen%2Fmessage_queue.rs?ref=3ee916e50bd86768cb2a9141f9b2c52d2601b412", "patch": "@@ -1,67 +0,0 @@\n-// Copyright 2014 The Rust Project Developers. See the COPYRIGHT\n-// file at the top-level directory of this distribution and at\n-// http://rust-lang.org/COPYRIGHT.\n-//\n-// Licensed under the Apache License, Version 2.0 <LICENSE-APACHE or\n-// http://www.apache.org/licenses/LICENSE-2.0> or the MIT license\n-// <LICENSE-MIT or http://opensource.org/licenses/MIT>, at your\n-// option. This file may not be copied, modified, or distributed\n-// except according to those terms.\n-\n-pub use self::PopResult::*;\n-\n-use alloc::arc::Arc;\n-use std::sync::mpsc_queue as mpsc;\n-use std::kinds::marker;\n-\n-pub enum PopResult<T> {\n-    Inconsistent,\n-    Empty,\n-    Data(T),\n-}\n-\n-pub fn queue<T: Send>() -> (Consumer<T>, Producer<T>) {\n-    let a = Arc::new(mpsc::Queue::new());\n-    (Consumer { inner: a.clone(), noshare: marker::NoSync },\n-     Producer { inner: a, noshare: marker::NoSync })\n-}\n-\n-pub struct Producer<T> {\n-    inner: Arc<mpsc::Queue<T>>,\n-    noshare: marker::NoSync,\n-}\n-\n-pub struct Consumer<T> {\n-    inner: Arc<mpsc::Queue<T>>,\n-    noshare: marker::NoSync,\n-}\n-\n-impl<T: Send> Consumer<T> {\n-    pub fn pop(&self) -> PopResult<T> {\n-        match self.inner.pop() {\n-            mpsc::Inconsistent => Inconsistent,\n-            mpsc::Empty => Empty,\n-            mpsc::Data(t) => Data(t),\n-        }\n-    }\n-\n-    pub fn casual_pop(&self) -> Option<T> {\n-        match self.inner.pop() {\n-            mpsc::Inconsistent => None,\n-            mpsc::Empty => None,\n-            mpsc::Data(t) => Some(t),\n-        }\n-    }\n-}\n-\n-impl<T: Send> Producer<T> {\n-    pub fn push(&self, t: T) {\n-        self.inner.push(t);\n-    }\n-}\n-\n-impl<T: Send> Clone for Producer<T> {\n-    fn clone(&self) -> Producer<T> {\n-        Producer { inner: self.inner.clone(), noshare: marker::NoSync }\n-    }\n-}"}, {"sha": "e8cb65d35df6a1c1434f7bc91db12a440866f2ad", "filename": "src/libgreen/sched.rs", "status": "removed", "additions": 0, "deletions": 1523, "changes": 1523, "blob_url": "https://github.com/rust-lang/rust/blob/3ee916e50bd86768cb2a9141f9b2c52d2601b412/src%2Flibgreen%2Fsched.rs", "raw_url": "https://github.com/rust-lang/rust/raw/3ee916e50bd86768cb2a9141f9b2c52d2601b412/src%2Flibgreen%2Fsched.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibgreen%2Fsched.rs?ref=3ee916e50bd86768cb2a9141f9b2c52d2601b412", "patch": "@@ -1,1523 +0,0 @@\n-// Copyright 2013-2014 The Rust Project Developers. See the COPYRIGHT\n-// file at the top-level directory of this distribution and at\n-// http://rust-lang.org/COPYRIGHT.\n-//\n-// Licensed under the Apache License, Version 2.0 <LICENSE-APACHE or\n-// http://www.apache.org/licenses/LICENSE-2.0> or the MIT license\n-// <LICENSE-MIT or http://opensource.org/licenses/MIT>, at your\n-// option. This file may not be copied, modified, or distributed\n-// except according to those terms.\n-\n-pub use self::SchedMessage::*;\n-use self::EffortLevel::*;\n-\n-use std::mem;\n-use std::rt::local::Local;\n-use std::rt::mutex::NativeMutex;\n-use std::rt::rtio::{RemoteCallback, PausableIdleCallback, Callback, EventLoop};\n-use std::rt::task::BlockedTask;\n-use std::rt::task::Task;\n-use std::sync::deque;\n-use std::raw;\n-\n-use std::rand::{XorShiftRng, Rng, Rand};\n-\n-use TaskState;\n-use context::Context;\n-use coroutine::Coroutine;\n-use sleeper_list::SleeperList;\n-use stack::StackPool;\n-use task::{TypeSched, GreenTask, HomeSched, AnySched};\n-use message_queue as msgq;\n-\n-/// A scheduler is responsible for coordinating the execution of Tasks\n-/// on a single thread. The scheduler runs inside a slightly modified\n-/// Rust Task. When not running this task is stored in the scheduler\n-/// struct. The scheduler struct acts like a baton, all scheduling\n-/// actions are transfers of the baton.\n-///\n-/// FIXME: This creates too many callbacks to run_sched_once, resulting\n-/// in too much allocation and too many events.\n-pub struct Scheduler {\n-    /// ID number of the pool that this scheduler is a member of. When\n-    /// reawakening green tasks, this is used to ensure that tasks aren't\n-    /// reawoken on the wrong pool of schedulers.\n-    pub pool_id: uint,\n-    /// The pool of stacks that this scheduler has cached\n-    pub stack_pool: StackPool,\n-    /// Bookkeeping for the number of tasks which are currently running around\n-    /// inside this pool of schedulers\n-    pub task_state: TaskState,\n-    /// There are N work queues, one per scheduler.\n-    work_queue: deque::Worker<Box<GreenTask>>,\n-    /// Work queues for the other schedulers. These are created by\n-    /// cloning the core work queues.\n-    work_queues: Vec<deque::Stealer<Box<GreenTask>>>,\n-    /// The queue of incoming messages from other schedulers.\n-    /// These are enqueued by SchedHandles after which a remote callback\n-    /// is triggered to handle the message.\n-    message_queue: msgq::Consumer<SchedMessage>,\n-    /// Producer used to clone sched handles from\n-    message_producer: msgq::Producer<SchedMessage>,\n-    /// A shared list of sleeping schedulers. We'll use this to wake\n-    /// up schedulers when pushing work onto the work queue.\n-    sleeper_list: SleeperList,\n-    /// Indicates that we have previously pushed a handle onto the\n-    /// SleeperList but have not yet received the Wake message.\n-    /// Being `true` does not necessarily mean that the scheduler is\n-    /// not active since there are multiple event sources that may\n-    /// wake the scheduler. It just prevents the scheduler from pushing\n-    /// multiple handles onto the sleeper list.\n-    sleepy: bool,\n-    /// A flag to indicate we've received the shutdown message and should\n-    /// no longer try to go to sleep, but exit instead.\n-    no_sleep: bool,\n-    /// The scheduler runs on a special task. When it is not running\n-    /// it is stored here instead of the work queue.\n-    sched_task: Option<Box<GreenTask>>,\n-    /// An action performed after a context switch on behalf of the\n-    /// code running before the context switch\n-    cleanup_job: Option<CleanupJob>,\n-    /// If the scheduler shouldn't run some tasks, a friend to send\n-    /// them to.\n-    friend_handle: Option<SchedHandle>,\n-    /// Should this scheduler run any task, or only pinned tasks?\n-    run_anything: bool,\n-    /// A fast XorShift rng for scheduler use\n-    rng: XorShiftRng,\n-    /// A toggleable idle callback\n-    idle_callback: Option<Box<PausableIdleCallback + Send>>,\n-    /// A countdown that starts at a random value and is decremented\n-    /// every time a yield check is performed. When it hits 0 a task\n-    /// will yield.\n-    yield_check_count: uint,\n-    /// A flag to tell the scheduler loop it needs to do some stealing\n-    /// in order to introduce randomness as part of a yield\n-    steal_for_yield: bool,\n-\n-    // n.b. currently destructors of an object are run in top-to-bottom in order\n-    //      of field declaration. Due to its nature, the pausable idle callback\n-    //      must have some sort of handle to the event loop, so it needs to get\n-    //      destroyed before the event loop itself. For this reason, we destroy\n-    //      the event loop last to ensure that any unsafe references to it are\n-    //      destroyed before it's actually destroyed.\n-\n-    /// The event loop used to drive the scheduler and perform I/O\n-    pub event_loop: Box<EventLoop + Send>,\n-}\n-\n-/// An indication of how hard to work on a given operation, the difference\n-/// mainly being whether memory is synchronized or not\n-#[deriving(PartialEq)]\n-enum EffortLevel {\n-    DontTryTooHard,\n-    GiveItYourBest\n-}\n-\n-static MAX_YIELD_CHECKS: uint = 20000;\n-\n-fn reset_yield_check(rng: &mut XorShiftRng) -> uint {\n-    let r: uint = Rand::rand(rng);\n-    r % MAX_YIELD_CHECKS + 1\n-}\n-\n-impl Scheduler {\n-\n-    // * Initialization Functions\n-\n-    pub fn new(pool_id: uint,\n-               event_loop: Box<EventLoop + Send>,\n-               work_queue: deque::Worker<Box<GreenTask>>,\n-               work_queues: Vec<deque::Stealer<Box<GreenTask>>>,\n-               sleeper_list: SleeperList,\n-               state: TaskState)\n-        -> Scheduler {\n-\n-        Scheduler::new_special(pool_id, event_loop, work_queue, work_queues,\n-                               sleeper_list, true, None, state)\n-\n-    }\n-\n-    pub fn new_special(pool_id: uint,\n-                       event_loop: Box<EventLoop + Send>,\n-                       work_queue: deque::Worker<Box<GreenTask>>,\n-                       work_queues: Vec<deque::Stealer<Box<GreenTask>>>,\n-                       sleeper_list: SleeperList,\n-                       run_anything: bool,\n-                       friend: Option<SchedHandle>,\n-                       state: TaskState)\n-        -> Scheduler {\n-\n-        let (consumer, producer) = msgq::queue();\n-        let mut sched = Scheduler {\n-            pool_id: pool_id,\n-            sleeper_list: sleeper_list,\n-            message_queue: consumer,\n-            message_producer: producer,\n-            sleepy: false,\n-            no_sleep: false,\n-            event_loop: event_loop,\n-            work_queue: work_queue,\n-            work_queues: work_queues,\n-            stack_pool: StackPool::new(),\n-            sched_task: None,\n-            cleanup_job: None,\n-            run_anything: run_anything,\n-            friend_handle: friend,\n-            rng: new_sched_rng(),\n-            idle_callback: None,\n-            yield_check_count: 0,\n-            steal_for_yield: false,\n-            task_state: state,\n-        };\n-\n-        sched.yield_check_count = reset_yield_check(&mut sched.rng);\n-\n-        return sched;\n-    }\n-\n-    // FIXME: This may eventually need to be refactored so that\n-    // the scheduler itself doesn't have to call event_loop.run.\n-    // That will be important for embedding the runtime into external\n-    // event loops.\n-\n-    // Take a main task to run, and a scheduler to run it in. Create a\n-    // scheduler task and bootstrap into it.\n-    pub fn bootstrap(mut self: Box<Scheduler>) {\n-\n-        // Build an Idle callback.\n-        let cb = box SchedRunner as Box<Callback + Send>;\n-        self.idle_callback = Some(self.event_loop.pausable_idle_callback(cb));\n-\n-        // Create a task for the scheduler with an empty context.\n-        let sched_task = GreenTask::new_typed(Some(Coroutine::empty()),\n-                                              TypeSched);\n-\n-        // Before starting our first task, make sure the idle callback\n-        // is active. As we do not start in the sleep state this is\n-        // important.\n-        self.idle_callback.as_mut().unwrap().resume();\n-\n-        // Now, as far as all the scheduler state is concerned, we are inside\n-        // the \"scheduler\" context. The scheduler immediately hands over control\n-        // to the event loop, and this will only exit once the event loop no\n-        // longer has any references (handles or I/O objects).\n-        rtdebug!(\"starting scheduler {}\", self.sched_id());\n-        let mut sched_task = self.run(sched_task);\n-\n-        // Close the idle callback.\n-        let mut sched = sched_task.sched.take().unwrap();\n-        sched.idle_callback.take();\n-        // Make one go through the loop to run the close callback.\n-        let mut stask = sched.run(sched_task);\n-\n-        // Now that we are done with the scheduler, clean up the\n-        // scheduler task. Do so by removing it from TLS and manually\n-        // cleaning up the memory it uses. As we didn't actually call\n-        // task.run() on the scheduler task we never get through all\n-        // the cleanup code it runs.\n-        rtdebug!(\"stopping scheduler {}\", stask.sched.as_ref().unwrap().sched_id());\n-\n-        // Should not have any messages\n-        let message = stask.sched.as_mut().unwrap().message_queue.pop();\n-        rtassert!(match message { msgq::Empty => true, _ => false });\n-\n-        stask.task.take().unwrap().drop();\n-    }\n-\n-    // This does not return a scheduler, as the scheduler is placed\n-    // inside the task.\n-    pub fn run(mut self: Box<Scheduler>, stask: Box<GreenTask>)\n-               -> Box<GreenTask> {\n-\n-        // This is unsafe because we need to place the scheduler, with\n-        // the event_loop inside, inside our task. But we still need a\n-        // mutable reference to the event_loop to give it the \"run\"\n-        // command.\n-        unsafe {\n-            let event_loop: *mut Box<EventLoop + Send> = &mut self.event_loop;\n-            // Our scheduler must be in the task before the event loop\n-            // is started.\n-            stask.put_with_sched(self);\n-            (*event_loop).run();\n-        }\n-\n-        //  This is a serious code smell, but this function could be done away\n-        //  with if necessary. The ownership of `stask` was transferred into\n-        //  local storage just before the event loop ran, so it is possible to\n-        //  transmute `stask` as a uint across the running of the event loop to\n-        //  re-acquire ownership here.\n-        //\n-        // This would involve removing the Task from TLS, removing the runtime,\n-        // forgetting the runtime, and then putting the task into `stask`. For\n-        // now, because we have `GreenTask::convert`, I chose to take this\n-        // method for cleanliness. This function is *not* a fundamental reason\n-        // why this function should exist.\n-        GreenTask::convert(Local::take())\n-    }\n-\n-    // * Execution Functions - Core Loop Logic\n-\n-    // This function is run from the idle callback on the uv loop, indicating\n-    // that there are no I/O events pending. When this function returns, we will\n-    // fall back to epoll() in the uv event loop, waiting for more things to\n-    // happen. We may come right back off epoll() if the idle callback is still\n-    // active, in which case we're truly just polling to see if I/O events are\n-    // complete.\n-    //\n-    // The model for this function is to execute as much work as possible while\n-    // still fairly considering I/O tasks. Falling back to epoll() frequently is\n-    // often quite expensive, so we attempt to avoid it as much as possible. If\n-    // we have any active I/O on the event loop, then we're forced to fall back\n-    // to epoll() in order to provide fairness, but as long as we're doing work\n-    // and there's no active I/O, we can continue to do work.\n-    //\n-    // If we try really hard to do some work, but no work is available to be\n-    // done, then we fall back to epoll() to block this thread waiting for more\n-    // work (instead of busy waiting).\n-    fn run_sched_once(mut self: Box<Scheduler>, stask: Box<GreenTask>) {\n-        // Make sure that we're not lying in that the `stask` argument is indeed\n-        // the scheduler task for this scheduler.\n-        assert!(self.sched_task.is_none());\n-\n-        // Assume that we need to continue idling unless we reach the\n-        // end of this function without performing an action.\n-        self.idle_callback.as_mut().unwrap().resume();\n-\n-        // First we check for scheduler messages, these are higher\n-        // priority than regular tasks.\n-        let (mut sched, mut stask, mut did_work) =\n-            self.interpret_message_queue(stask, DontTryTooHard);\n-\n-        // After processing a message, we consider doing some more work on the\n-        // event loop. The \"keep going\" condition changes after the first\n-        // iteration because we don't want to spin here infinitely.\n-        //\n-        // Once we start doing work we can keep doing work so long as the\n-        // iteration does something. Note that we don't want to starve the\n-        // message queue here, so each iteration when we're done working we\n-        // check the message queue regardless of whether we did work or not.\n-        let mut keep_going = !did_work || !sched.event_loop.has_active_io();\n-        while keep_going {\n-            let (a, b, c) = match sched.do_work(stask) {\n-                (sched, task, false) => {\n-                    sched.interpret_message_queue(task, GiveItYourBest)\n-                }\n-                (sched, task, true) => {\n-                    let (sched, task, _) =\n-                        sched.interpret_message_queue(task, GiveItYourBest);\n-                    (sched, task, true)\n-                }\n-            };\n-            sched = a;\n-            stask = b;\n-            did_work = c;\n-\n-            // We only keep going if we managed to do something productive and\n-            // also don't have any active I/O. If we didn't do anything, we\n-            // should consider going to sleep, and if we have active I/O we need\n-            // to poll for completion.\n-            keep_going = did_work && !sched.event_loop.has_active_io();\n-        }\n-\n-        // If we ever did some work, then we shouldn't put our scheduler\n-        // entirely to sleep just yet. Leave the idle callback active and fall\n-        // back to epoll() to see what's going on.\n-        if did_work {\n-            return stask.put_with_sched(sched);\n-        }\n-\n-        // If we got here then there was no work to do.\n-        // Generate a SchedHandle and push it to the sleeper list so\n-        // somebody can wake us up later.\n-        if !sched.sleepy && !sched.no_sleep {\n-            rtdebug!(\"scheduler has no work to do, going to sleep\");\n-            sched.sleepy = true;\n-            let handle = sched.make_handle();\n-            sched.sleeper_list.push(handle);\n-            // Since we are sleeping, deactivate the idle callback.\n-            sched.idle_callback.as_mut().unwrap().pause();\n-        } else {\n-            rtdebug!(\"not sleeping, already doing so or no_sleep set\");\n-            // We may not be sleeping, but we still need to deactivate\n-            // the idle callback.\n-            sched.idle_callback.as_mut().unwrap().pause();\n-        }\n-\n-        // Finished a cycle without using the Scheduler. Place it back\n-        // in TLS.\n-        stask.put_with_sched(sched);\n-    }\n-\n-    // This function returns None if the scheduler is \"used\", or it\n-    // returns the still-available scheduler. At this point all\n-    // message-handling will count as a turn of work, and as a result\n-    // return None.\n-    fn interpret_message_queue(mut self: Box<Scheduler>,\n-                               stask: Box<GreenTask>,\n-                               effort: EffortLevel)\n-                               -> (Box<Scheduler>, Box<GreenTask>, bool) {\n-        let msg = if effort == DontTryTooHard {\n-            self.message_queue.casual_pop()\n-        } else {\n-            // When popping our message queue, we could see an \"inconsistent\"\n-            // state which means that we *should* be able to pop data, but we\n-            // are unable to at this time. Our options are:\n-            //\n-            //  1. Spin waiting for data\n-            //  2. Ignore this and pretend we didn't find a message\n-            //\n-            // If we choose route 1, then if the pusher in question is currently\n-            // pre-empted, we're going to take up our entire time slice just\n-            // spinning on this queue. If we choose route 2, then the pusher in\n-            // question is still guaranteed to make a send() on its async\n-            // handle, so we will guaranteed wake up and see its message at some\n-            // point.\n-            //\n-            // I have chosen to take route #2.\n-            match self.message_queue.pop() {\n-                msgq::Data(t) => Some(t),\n-                msgq::Empty | msgq::Inconsistent => None\n-            }\n-        };\n-\n-        match msg {\n-            Some(PinnedTask(task)) => {\n-                let mut task = task;\n-                task.give_home(HomeSched(self.make_handle()));\n-                let (sched, task) = self.resume_task_immediately(stask, task);\n-                (sched, task, true)\n-            }\n-            Some(TaskFromFriend(task)) => {\n-                rtdebug!(\"got a task from a friend. lovely!\");\n-                let (sched, task) =\n-                    self.process_task(stask, task,\n-                                      Scheduler::resume_task_immediately_cl);\n-                (sched, task, true)\n-            }\n-            Some(RunOnce(task)) => {\n-                // bypass the process_task logic to force running this task once\n-                // on this home scheduler. This is often used for I/O (homing).\n-                let (sched, task) = self.resume_task_immediately(stask, task);\n-                (sched, task, true)\n-            }\n-            Some(Wake) => {\n-                self.sleepy = false;\n-                (self, stask, true)\n-            }\n-            Some(Shutdown) => {\n-                rtdebug!(\"shutting down\");\n-                if self.sleepy {\n-                    // There may be an outstanding handle on the\n-                    // sleeper list.  Pop them all to make sure that's\n-                    // not the case.\n-                    loop {\n-                        match self.sleeper_list.pop() {\n-                            Some(handle) => {\n-                                let mut handle = handle;\n-                                handle.send(Wake);\n-                            }\n-                            None => break\n-                        }\n-                    }\n-                }\n-                // No more sleeping. After there are no outstanding\n-                // event loop references we will shut down.\n-                self.no_sleep = true;\n-                self.sleepy = false;\n-                (self, stask, true)\n-            }\n-            Some(NewNeighbor(neighbor)) => {\n-                self.work_queues.push(neighbor);\n-                (self, stask, false)\n-            }\n-            None => (self, stask, false)\n-        }\n-    }\n-\n-    fn do_work(mut self: Box<Scheduler>, stask: Box<GreenTask>)\n-               -> (Box<Scheduler>, Box<GreenTask>, bool) {\n-        rtdebug!(\"scheduler calling do work\");\n-        match self.find_work() {\n-            Some(task) => {\n-                rtdebug!(\"found some work! running the task\");\n-                let (sched, task) =\n-                    self.process_task(stask, task,\n-                                      Scheduler::resume_task_immediately_cl);\n-                (sched, task, true)\n-            }\n-            None => {\n-                rtdebug!(\"no work was found, returning the scheduler struct\");\n-                (self, stask, false)\n-            }\n-        }\n-    }\n-\n-    // Workstealing: In this iteration of the runtime each scheduler\n-    // thread has a distinct work queue. When no work is available\n-    // locally, make a few attempts to steal work from the queues of\n-    // other scheduler threads. If a few steals fail we end up in the\n-    // old \"no work\" path which is fine.\n-\n-    // First step in the process is to find a task. This function does\n-    // that by first checking the local queue, and if there is no work\n-    // there, trying to steal from the remote work queues.\n-    fn find_work(&mut self) -> Option<Box<GreenTask>> {\n-        rtdebug!(\"scheduler looking for work\");\n-        if !self.steal_for_yield {\n-            match self.work_queue.pop() {\n-                Some(task) => {\n-                    rtdebug!(\"found a task locally\");\n-                    return Some(task)\n-                }\n-                None => {\n-                    rtdebug!(\"scheduler trying to steal\");\n-                    return self.try_steals();\n-                }\n-            }\n-        } else {\n-            // During execution of the last task, it performed a 'yield',\n-            // so we're doing some work stealing in order to introduce some\n-            // scheduling randomness. Otherwise we would just end up popping\n-            // that same task again. This is pretty lame and is to work around\n-            // the problem that work stealing is not designed for 'non-strict'\n-            // (non-fork-join) task parallelism.\n-            self.steal_for_yield = false;\n-            match self.try_steals() {\n-                Some(task) => {\n-                    rtdebug!(\"stole a task after yielding\");\n-                    return Some(task);\n-                }\n-                None => {\n-                    rtdebug!(\"did not steal a task after yielding\");\n-                    // Back to business\n-                    return self.find_work();\n-                }\n-            }\n-        }\n-    }\n-\n-    // Try stealing from all queues the scheduler knows about. This\n-    // naive implementation can steal from our own queue or from other\n-    // special schedulers.\n-    fn try_steals(&mut self) -> Option<Box<GreenTask>> {\n-        let work_queues = &mut self.work_queues;\n-        let len = work_queues.len();\n-        let start_index = self.rng.gen_range(0, len);\n-        for index in range(0, len).map(|i| (i + start_index) % len) {\n-            match work_queues[index].steal() {\n-                deque::Data(task) => {\n-                    rtdebug!(\"found task by stealing\");\n-                    return Some(task)\n-                }\n-                _ => ()\n-            }\n-        };\n-        rtdebug!(\"giving up on stealing\");\n-        return None;\n-    }\n-\n-    // * Task Routing Functions - Make sure tasks send up in the right\n-    // place.\n-\n-    fn process_task(mut self: Box<Scheduler>,\n-                    cur: Box<GreenTask>,\n-                    mut next: Box<GreenTask>,\n-                    schedule_fn: SchedulingFn)\n-                    -> (Box<Scheduler>, Box<GreenTask>) {\n-        rtdebug!(\"processing a task\");\n-\n-        match next.take_unwrap_home() {\n-            HomeSched(home_handle) => {\n-                if home_handle.sched_id != self.sched_id() {\n-                    rtdebug!(\"sending task home\");\n-                    next.give_home(HomeSched(home_handle));\n-                    Scheduler::send_task_home(next);\n-                    (self, cur)\n-                } else {\n-                    rtdebug!(\"running task here\");\n-                    next.give_home(HomeSched(home_handle));\n-                    schedule_fn(self, cur, next)\n-                }\n-            }\n-            AnySched if self.run_anything => {\n-                rtdebug!(\"running anysched task here\");\n-                next.give_home(AnySched);\n-                schedule_fn(self, cur, next)\n-            }\n-            AnySched => {\n-                rtdebug!(\"sending task to friend\");\n-                next.give_home(AnySched);\n-                self.send_to_friend(next);\n-                (self, cur)\n-            }\n-        }\n-    }\n-\n-    fn send_task_home(task: Box<GreenTask>) {\n-        let mut task = task;\n-        match task.take_unwrap_home() {\n-            HomeSched(mut home_handle) => home_handle.send(PinnedTask(task)),\n-            AnySched => rtabort!(\"error: cannot send anysched task home\"),\n-        }\n-    }\n-\n-    /// Take a non-homed task we aren't allowed to run here and send\n-    /// it to the designated friend scheduler to execute.\n-    fn send_to_friend(&mut self, task: Box<GreenTask>) {\n-        rtdebug!(\"sending a task to friend\");\n-        match self.friend_handle {\n-            Some(ref mut handle) => {\n-                handle.send(TaskFromFriend(task));\n-            }\n-            None => {\n-                rtabort!(\"tried to send task to a friend but scheduler has no friends\");\n-            }\n-        }\n-    }\n-\n-    /// Schedule a task to be executed later.\n-    ///\n-    /// Pushes the task onto the work stealing queue and tells the\n-    /// event loop to run it later. Always use this instead of pushing\n-    /// to the work queue directly.\n-    pub fn enqueue_task(&mut self, task: Box<GreenTask>) {\n-\n-        // We push the task onto our local queue clone.\n-        assert!(!task.is_sched());\n-        self.work_queue.push(task);\n-        match self.idle_callback {\n-            Some(ref mut idle) => idle.resume(),\n-            None => {} // allow enqueuing before the scheduler starts\n-        }\n-\n-        // We've made work available. Notify a\n-        // sleeping scheduler.\n-\n-        match self.sleeper_list.casual_pop() {\n-            Some(handle) => {\n-                let mut handle = handle;\n-                handle.send(Wake)\n-            }\n-            None => { (/* pass */) }\n-        };\n-    }\n-\n-    // * Core Context Switching Functions\n-\n-    // The primary function for changing contexts. In the current\n-    // design the scheduler is just a slightly modified GreenTask, so\n-    // all context swaps are from GreenTask to GreenTask. The only difference\n-    // between the various cases is where the inputs come from, and\n-    // what is done with the resulting task. That is specified by the\n-    // cleanup function f, which takes the scheduler and the\n-    // old task as inputs.\n-\n-    pub fn change_task_context(mut self: Box<Scheduler>,\n-                               mut current_task: Box<GreenTask>,\n-                               mut next_task: Box<GreenTask>,\n-                               f: |&mut Scheduler, Box<GreenTask>|)\n-                               -> Box<GreenTask> {\n-        let f_opaque = ClosureConverter::from_fn(f);\n-\n-        let current_task_dupe = &mut *current_task as *mut GreenTask;\n-\n-        // The current task is placed inside an enum with the cleanup\n-        // function. This enum is then placed inside the scheduler.\n-        self.cleanup_job = Some(CleanupJob::new(current_task, f_opaque));\n-\n-        // The scheduler is then placed inside the next task.\n-        next_task.sched = Some(self);\n-\n-        // However we still need an internal mutable pointer to the\n-        // original task. The strategy here was \"arrange memory, then\n-        // get pointers\", so we crawl back up the chain using\n-        // transmute to eliminate borrowck errors.\n-        unsafe {\n-\n-            let sched: &mut Scheduler =\n-                mem::transmute(&**next_task.sched.as_mut().unwrap());\n-\n-            let current_task: &mut GreenTask = match sched.cleanup_job {\n-                Some(CleanupJob { ref mut task, .. }) => &mut **task,\n-                None => rtabort!(\"no cleanup job\")\n-            };\n-\n-            let (current_task_context, next_task_context) =\n-                Scheduler::get_contexts(current_task, &mut *next_task);\n-\n-            // Done with everything - put the next task in TLS. This\n-            // works because due to transmute the borrow checker\n-            // believes that we have no internal pointers to\n-            // next_task.\n-            mem::forget(next_task);\n-\n-            // The raw context swap operation. The next action taken\n-            // will be running the cleanup job from the context of the\n-            // next task.\n-            Context::swap(current_task_context, next_task_context);\n-        }\n-\n-        // When the context swaps back to this task we immediately\n-        // run the cleanup job, as expected by the previously called\n-        // swap_contexts function.\n-        let mut current_task: Box<GreenTask> = unsafe {\n-            mem::transmute(current_task_dupe)\n-        };\n-        current_task.sched.as_mut().unwrap().run_cleanup_job();\n-\n-        // See the comments in switch_running_tasks_and_then for why a lock\n-        // is acquired here. This is the resumption points and the \"bounce\"\n-        // that it is referring to.\n-        unsafe {\n-            let _guard = current_task.nasty_deschedule_lock.lock();\n-        }\n-        return current_task;\n-    }\n-\n-    // Returns a mutable reference to both contexts involved in this\n-    // swap. This is unsafe - we are getting mutable internal\n-    // references to keep even when we don't own the tasks. It looks\n-    // kinda safe because we are doing transmutes before passing in\n-    // the arguments.\n-    pub fn get_contexts<'a>(current_task: &mut GreenTask,\n-                            next_task: &mut GreenTask)\n-        -> (&'a mut Context, &'a mut Context)\n-    {\n-        let current_task_context =\n-            &mut current_task.coroutine.as_mut().unwrap().saved_context;\n-        let next_task_context =\n-                &mut next_task.coroutine.as_mut().unwrap().saved_context;\n-        unsafe {\n-            (mem::transmute(current_task_context),\n-             mem::transmute(next_task_context))\n-        }\n-    }\n-\n-    // * Context Swapping Helpers - Here be ugliness!\n-\n-    pub fn resume_task_immediately(self: Box<Scheduler>,\n-                                   cur: Box<GreenTask>,\n-                                   next: Box<GreenTask>)\n-                                   -> (Box<Scheduler>, Box<GreenTask>) {\n-        assert!(cur.is_sched());\n-        let mut cur = self.change_task_context(cur, next, |sched, stask| {\n-            assert!(sched.sched_task.is_none());\n-            sched.sched_task = Some(stask);\n-        });\n-        (cur.sched.take().unwrap(), cur)\n-    }\n-\n-    fn resume_task_immediately_cl(sched: Box<Scheduler>,\n-                                  cur: Box<GreenTask>,\n-                                  next: Box<GreenTask>)\n-                                  -> (Box<Scheduler>, Box<GreenTask>) {\n-        sched.resume_task_immediately(cur, next)\n-    }\n-\n-    /// Block a running task, context switch to the scheduler, then pass the\n-    /// blocked task to a closure.\n-    ///\n-    /// # Safety note\n-    ///\n-    /// The closure here is a *stack* closure that lives in the\n-    /// running task.  It gets transmuted to the scheduler's lifetime\n-    /// and called while the task is blocked.\n-    ///\n-    /// This passes a Scheduler pointer to the fn after the context switch\n-    /// in order to prevent that fn from performing further scheduling operations.\n-    /// Doing further scheduling could easily result in infinite recursion.\n-    ///\n-    /// Note that if the closure provided relinquishes ownership of the\n-    /// BlockedTask, then it is possible for the task to resume execution before\n-    /// the closure has finished executing. This would naturally introduce a\n-    /// race if the closure and task shared portions of the environment.\n-    ///\n-    /// This situation is currently prevented, or in other words it is\n-    /// guaranteed that this function will not return before the given closure\n-    /// has returned.\n-    pub fn deschedule_running_task_and_then(mut self: Box<Scheduler>,\n-                                            cur: Box<GreenTask>,\n-                                            f: |&mut Scheduler, BlockedTask|) {\n-        // Trickier - we need to get the scheduler task out of self\n-        // and use it as the destination.\n-        let stask = self.sched_task.take().unwrap();\n-        // Otherwise this is the same as below.\n-        self.switch_running_tasks_and_then(cur, stask, f)\n-    }\n-\n-    pub fn switch_running_tasks_and_then(self: Box<Scheduler>,\n-                                         cur: Box<GreenTask>,\n-                                         next: Box<GreenTask>,\n-                                         f: |&mut Scheduler, BlockedTask|) {\n-        // And here comes one of the sad moments in which a lock is used in a\n-        // core portion of the rust runtime. As always, this is highly\n-        // undesirable, so there's a good reason behind it.\n-        //\n-        // There is an excellent outline of the problem in issue #8132, and it's\n-        // summarized in that `f` is executed on a sched task, but its\n-        // environment is on the previous task. If `f` relinquishes ownership of\n-        // the BlockedTask, then it may introduce a race where `f` is using the\n-        // environment as well as the code after the 'deschedule' block.\n-        //\n-        // The solution we have chosen to adopt for now is to acquire a\n-        // task-local lock around this block. The resumption of the task in\n-        // context switching will bounce on the lock, thereby waiting for this\n-        // block to finish, eliminating the race mentioned above.\n-        // panic!(\"should never return!\");\n-        //\n-        // To actually maintain a handle to the lock, we use an unsafe pointer\n-        // to it, but we're guaranteed that the task won't exit until we've\n-        // unlocked the lock so there's no worry of this memory going away.\n-        let cur = self.change_task_context(cur, next, |sched, mut task| {\n-            let lock: *mut NativeMutex = &mut task.nasty_deschedule_lock;\n-            unsafe {\n-                let _guard = (*lock).lock();\n-                f(sched, BlockedTask::block(task.swap()));\n-            }\n-        });\n-        cur.put();\n-    }\n-\n-    fn switch_task(sched: Box<Scheduler>,\n-                   cur: Box<GreenTask>,\n-                   next: Box<GreenTask>)\n-                   -> (Box<Scheduler>, Box<GreenTask>) {\n-        let mut cur = sched.change_task_context(cur, next, |sched, last_task| {\n-            if last_task.is_sched() {\n-                assert!(sched.sched_task.is_none());\n-                sched.sched_task = Some(last_task);\n-            } else {\n-                sched.enqueue_task(last_task);\n-            }\n-        });\n-        (cur.sched.take().unwrap(), cur)\n-    }\n-\n-    // * Task Context Helpers\n-\n-    /// Called by a running task to end execution, after which it will\n-    /// be recycled by the scheduler for reuse in a new task.\n-    pub fn terminate_current_task(mut self: Box<Scheduler>,\n-                                  cur: Box<GreenTask>)\n-                                  -> ! {\n-        // Similar to deschedule running task and then, but cannot go through\n-        // the task-blocking path. The task is already dying.\n-        let stask = self.sched_task.take().unwrap();\n-        let _cur = self.change_task_context(cur, stask, |sched, mut dead_task| {\n-            let coroutine = dead_task.coroutine.take().unwrap();\n-            coroutine.recycle(&mut sched.stack_pool);\n-            sched.task_state.decrement();\n-        });\n-        panic!(\"should never return!\");\n-    }\n-\n-    pub fn run_task(self: Box<Scheduler>,\n-                    cur: Box<GreenTask>,\n-                    next: Box<GreenTask>) {\n-        let (sched, task) =\n-            self.process_task(cur, next, Scheduler::switch_task);\n-        task.put_with_sched(sched);\n-    }\n-\n-    pub fn run_task_later(mut cur: Box<GreenTask>, next: Box<GreenTask>) {\n-        let mut sched = cur.sched.take().unwrap();\n-        sched.enqueue_task(next);\n-        cur.put_with_sched(sched);\n-    }\n-\n-    /// Yield control to the scheduler, executing another task. This is guaranteed\n-    /// to introduce some amount of randomness to the scheduler. Currently the\n-    /// randomness is a result of performing a round of work stealing (which\n-    /// may end up stealing from the current scheduler).\n-    pub fn yield_now(mut self: Box<Scheduler>, cur: Box<GreenTask>) {\n-        // Async handles trigger the scheduler by calling yield_now on the local\n-        // task, which eventually gets us to here. See comments in SchedRunner\n-        // for more info on this.\n-        if cur.is_sched() {\n-            assert!(self.sched_task.is_none());\n-            self.run_sched_once(cur);\n-        } else {\n-            self.yield_check_count = reset_yield_check(&mut self.rng);\n-            // Tell the scheduler to start stealing on the next iteration\n-            self.steal_for_yield = true;\n-            let stask = self.sched_task.take().unwrap();\n-            let cur = self.change_task_context(cur, stask, |sched, task| {\n-                sched.enqueue_task(task);\n-            });\n-            cur.put()\n-        }\n-    }\n-\n-    pub fn maybe_yield(mut self: Box<Scheduler>, cur: Box<GreenTask>) {\n-        // It's possible for sched tasks to possibly call this function, and it\n-        // just means that they're likely sending on channels (which\n-        // occasionally call this function). Sched tasks follow different paths\n-        // when executing yield_now(), which may possibly trip the assertion\n-        // below. For this reason, we just have sched tasks bail out soon.\n-        //\n-        // Sched tasks have no need to yield anyway because as soon as they\n-        // return they'll yield to other threads by falling back to the event\n-        // loop. Additionally, we completely control sched tasks, so we can make\n-        // sure that they never execute more than enough code.\n-        if cur.is_sched() {\n-            return cur.put_with_sched(self)\n-        }\n-\n-        // The number of times to do the yield check before yielding, chosen\n-        // arbitrarily.\n-        rtassert!(self.yield_check_count > 0);\n-        self.yield_check_count -= 1;\n-        if self.yield_check_count == 0 {\n-            self.yield_now(cur);\n-        } else {\n-            cur.put_with_sched(self);\n-        }\n-    }\n-\n-\n-    // * Utility Functions\n-\n-    pub fn sched_id(&self) -> uint { self as *const Scheduler as uint }\n-\n-    pub fn run_cleanup_job(&mut self) {\n-        let cleanup_job = self.cleanup_job.take().unwrap();\n-        cleanup_job.run(self)\n-    }\n-\n-    pub fn make_handle(&mut self) -> SchedHandle {\n-        let remote = self.event_loop.remote_callback(box SchedRunner);\n-\n-        return SchedHandle {\n-            remote: remote,\n-            queue: self.message_producer.clone(),\n-            sched_id: self.sched_id()\n-        }\n-    }\n-}\n-\n-// Supporting types\n-\n-type SchedulingFn = fn(Box<Scheduler>, Box<GreenTask>, Box<GreenTask>)\n-                       -> (Box<Scheduler>, Box<GreenTask>);\n-\n-pub enum SchedMessage {\n-    Wake,\n-    Shutdown,\n-    NewNeighbor(deque::Stealer<Box<GreenTask>>),\n-    PinnedTask(Box<GreenTask>),\n-    TaskFromFriend(Box<GreenTask>),\n-    RunOnce(Box<GreenTask>),\n-}\n-\n-pub struct SchedHandle {\n-    remote: Box<RemoteCallback + Send>,\n-    queue: msgq::Producer<SchedMessage>,\n-    pub sched_id: uint\n-}\n-\n-impl SchedHandle {\n-    pub fn send(&mut self, msg: SchedMessage) {\n-        self.queue.push(msg);\n-        self.remote.fire();\n-    }\n-}\n-\n-struct SchedRunner;\n-\n-impl Callback for SchedRunner {\n-    fn call(&mut self) {\n-        // In theory, this function needs to invoke the `run_sched_once`\n-        // function on the scheduler. Sadly, we have no context here, except for\n-        // knowledge of the local `Task`. In order to avoid a call to\n-        // `GreenTask::convert`, we just call `yield_now` and the scheduler will\n-        // detect when a sched task performs a yield vs a green task performing\n-        // a yield (and act accordingly).\n-        //\n-        // This function could be converted to `GreenTask::convert` if\n-        // absolutely necessary, but for cleanliness it is much better to not\n-        // use the conversion function.\n-        let task: Box<Task> = Local::take();\n-        task.yield_now();\n-    }\n-}\n-\n-struct CleanupJob {\n-    task: Box<GreenTask>,\n-    f: UnsafeTaskReceiver\n-}\n-\n-impl CleanupJob {\n-    pub fn new(task: Box<GreenTask>, f: UnsafeTaskReceiver) -> CleanupJob {\n-        CleanupJob {\n-            task: task,\n-            f: f\n-        }\n-    }\n-\n-    pub fn run(self, sched: &mut Scheduler) {\n-        let CleanupJob { task, f } = self;\n-        f.to_fn()(sched, task)\n-    }\n-}\n-\n-// FIXME: Some hacks to put a || closure in Scheduler without borrowck\n-// complaining\n-type UnsafeTaskReceiver = raw::Closure;\n-trait ClosureConverter {\n-    fn from_fn(|&mut Scheduler, Box<GreenTask>|) -> Self;\n-    fn to_fn(self) -> |&mut Scheduler, Box<GreenTask>|:'static ;\n-}\n-impl ClosureConverter for UnsafeTaskReceiver {\n-    fn from_fn(f: |&mut Scheduler, Box<GreenTask>|) -> UnsafeTaskReceiver {\n-        unsafe { mem::transmute(f) }\n-    }\n-    fn to_fn(self) -> |&mut Scheduler, Box<GreenTask>|:'static {\n-        unsafe { mem::transmute(self) }\n-    }\n-}\n-\n-// On unix, we read randomness straight from /dev/urandom, but the\n-// default constructor of an XorShiftRng does this via io::fs, which\n-// relies on the scheduler existing, so we have to manually load\n-// randomness. Windows has its own C API for this, so we don't need to\n-// worry there.\n-#[cfg(windows)]\n-fn new_sched_rng() -> XorShiftRng {\n-    use std::rand::OsRng;\n-    match OsRng::new() {\n-        Ok(mut r) => r.gen(),\n-        Err(e) => {\n-            rtabort!(\"sched: failed to create seeded RNG: {}\", e)\n-        }\n-    }\n-}\n-#[cfg(unix)]\n-fn new_sched_rng() -> XorShiftRng {\n-    use libc;\n-    use std::mem;\n-    use std::rand::SeedableRng;\n-\n-    let fd = \"/dev/urandom\".with_c_str(|name| {\n-        unsafe { libc::open(name, libc::O_RDONLY, 0) }\n-    });\n-    if fd == -1 {\n-        rtabort!(\"could not open /dev/urandom for reading.\")\n-    }\n-\n-    let mut seeds = [0u32, .. 4];\n-    let size = mem::size_of_val(&seeds);\n-    loop {\n-        let nbytes = unsafe {\n-            libc::read(fd,\n-                       seeds.as_mut_ptr() as *mut libc::c_void,\n-                       size as libc::size_t)\n-        };\n-        rtassert!(nbytes as uint == size);\n-\n-        if !seeds.iter().all(|x| *x == 0) {\n-            break;\n-        }\n-    }\n-\n-    unsafe {libc::close(fd);}\n-\n-    SeedableRng::from_seed(seeds)\n-}\n-\n-#[cfg(test)]\n-mod test {\n-    use std::rt::task::TaskOpts;\n-    use std::rt::task::Task;\n-    use std::rt::local::Local;\n-\n-    use {TaskState, PoolConfig, SchedPool};\n-    use basic;\n-    use sched::{TaskFromFriend, PinnedTask};\n-    use task::{GreenTask, HomeSched, AnySched};\n-\n-    fn pool() -> SchedPool {\n-        SchedPool::new(PoolConfig {\n-            threads: 1,\n-            event_loop_factory: basic::event_loop,\n-        })\n-    }\n-\n-    fn run(f: proc():Send) {\n-        let mut pool = pool();\n-        pool.spawn(TaskOpts::new(), f);\n-        pool.shutdown();\n-    }\n-\n-    fn sched_id() -> uint {\n-        let mut task = Local::borrow(None::<Task>);\n-        match task.maybe_take_runtime::<GreenTask>() {\n-            Some(green) => {\n-                let ret = green.sched.as_ref().unwrap().sched_id();\n-                task.put_runtime(green);\n-                return ret;\n-            }\n-            None => panic!()\n-        }\n-    }\n-\n-    #[test]\n-    fn trivial_run_in_newsched_task_test() {\n-        let mut task_ran = false;\n-        let task_ran_ptr: *mut bool = &mut task_ran;\n-        run(proc() {\n-            unsafe { *task_ran_ptr = true };\n-            rtdebug!(\"executed from the new scheduler\")\n-        });\n-        assert!(task_ran);\n-    }\n-\n-    #[test]\n-    fn multiple_task_test() {\n-        let total = 10;\n-        let mut task_run_count = 0;\n-        let task_run_count_ptr: *mut uint = &mut task_run_count;\n-        // with only one thread this is safe to run in without worries of\n-        // contention.\n-        run(proc() {\n-            for _ in range(0u, total) {\n-                spawn(proc() {\n-                    unsafe { *task_run_count_ptr = *task_run_count_ptr + 1};\n-                });\n-            }\n-        });\n-        assert!(task_run_count == total);\n-    }\n-\n-    #[test]\n-    fn multiple_task_nested_test() {\n-        let mut task_run_count = 0;\n-        let task_run_count_ptr: *mut uint = &mut task_run_count;\n-        run(proc() {\n-            spawn(proc() {\n-                unsafe { *task_run_count_ptr = *task_run_count_ptr + 1 };\n-                spawn(proc() {\n-                    unsafe { *task_run_count_ptr = *task_run_count_ptr + 1 };\n-                    spawn(proc() {\n-                        unsafe { *task_run_count_ptr = *task_run_count_ptr + 1 };\n-                    })\n-                })\n-            })\n-        });\n-        assert!(task_run_count == 3);\n-    }\n-\n-    // A very simple test that confirms that a task executing on the\n-    // home scheduler notices that it is home.\n-    #[test]\n-    fn test_home_sched() {\n-        let mut pool = pool();\n-\n-        let (dtx, drx) = channel();\n-        {\n-            let (tx, rx) = channel();\n-            let mut handle1 = pool.spawn_sched();\n-            let mut handle2 = pool.spawn_sched();\n-\n-            handle1.send(TaskFromFriend(pool.task(TaskOpts::new(), proc() {\n-                tx.send(sched_id());\n-            })));\n-            let sched1_id = rx.recv();\n-\n-            let mut task = pool.task(TaskOpts::new(), proc() {\n-                assert_eq!(sched_id(), sched1_id);\n-                dtx.send(());\n-            });\n-            task.give_home(HomeSched(handle1));\n-            handle2.send(TaskFromFriend(task));\n-        }\n-        drx.recv();\n-\n-        pool.shutdown();\n-    }\n-\n-    // An advanced test that checks all four possible states that a\n-    // (task,sched) can be in regarding homes.\n-\n-    #[test]\n-    fn test_schedule_home_states() {\n-        use sleeper_list::SleeperList;\n-        use super::{Shutdown, Scheduler, SchedHandle};\n-        use std::rt::thread::Thread;\n-        use std::sync::deque::BufferPool;\n-\n-        Thread::start(proc() {\n-            let sleepers = SleeperList::new();\n-            let pool = BufferPool::new();\n-            let (normal_worker, normal_stealer) = pool.deque();\n-            let (special_worker, special_stealer) = pool.deque();\n-            let queues = vec![normal_stealer, special_stealer];\n-            let (_p, state) = TaskState::new();\n-\n-            // Our normal scheduler\n-            let mut normal_sched = box Scheduler::new(\n-                1,\n-                basic::event_loop(),\n-                normal_worker,\n-                queues.clone(),\n-                sleepers.clone(),\n-                state.clone());\n-\n-            let normal_handle = normal_sched.make_handle();\n-            let friend_handle = normal_sched.make_handle();\n-\n-            // Our special scheduler\n-            let mut special_sched = box Scheduler::new_special(\n-                1,\n-                basic::event_loop(),\n-                special_worker,\n-                queues.clone(),\n-                sleepers.clone(),\n-                false,\n-                Some(friend_handle),\n-                state);\n-\n-            let special_handle = special_sched.make_handle();\n-\n-            let t1_handle = special_sched.make_handle();\n-            let t4_handle = special_sched.make_handle();\n-\n-            // Four test tasks:\n-            //   1) task is home on special\n-            //   2) task not homed, sched doesn't care\n-            //   3) task not homed, sched requeues\n-            //   4) task not home, send home\n-\n-            // Grab both the scheduler and the task from TLS and check if the\n-            // task is executing on an appropriate scheduler.\n-            fn on_appropriate_sched() -> bool {\n-                use task::{TypeGreen, TypeSched, HomeSched};\n-                let task = GreenTask::convert(Local::take());\n-                let sched_id = task.sched.as_ref().unwrap().sched_id();\n-                let run_any = task.sched.as_ref().unwrap().run_anything;\n-                let ret = match task.task_type {\n-                    TypeGreen(Some(AnySched)) => {\n-                        run_any\n-                    }\n-                    TypeGreen(Some(HomeSched(SchedHandle {\n-                        sched_id: ref id,\n-                        ..\n-                    }))) => {\n-                        *id == sched_id\n-                    }\n-                    TypeGreen(None) => { panic!(\"task without home\"); }\n-                    TypeSched => { panic!(\"expected green task\"); }\n-                };\n-                task.put();\n-                ret\n-            }\n-\n-            let task1 = GreenTask::new_homed(&mut special_sched.stack_pool,\n-                                             None, HomeSched(t1_handle), proc() {\n-                rtassert!(on_appropriate_sched());\n-            });\n-\n-            let task2 = GreenTask::new(&mut normal_sched.stack_pool, None, proc() {\n-                rtassert!(on_appropriate_sched());\n-            });\n-\n-            let task3 = GreenTask::new(&mut normal_sched.stack_pool, None, proc() {\n-                rtassert!(on_appropriate_sched());\n-            });\n-\n-            let task4 = GreenTask::new_homed(&mut special_sched.stack_pool,\n-                                             None, HomeSched(t4_handle), proc() {\n-                rtassert!(on_appropriate_sched());\n-            });\n-\n-            // Signal from the special task that we are done.\n-            let (tx, rx) = channel::<()>();\n-\n-            fn run(next: Box<GreenTask>) {\n-                let mut task = GreenTask::convert(Local::take());\n-                let sched = task.sched.take().unwrap();\n-                sched.run_task(task, next)\n-            }\n-\n-            let normal_task = GreenTask::new(&mut normal_sched.stack_pool, None, proc() {\n-                run(task2);\n-                run(task4);\n-                rx.recv();\n-                let mut nh = normal_handle;\n-                nh.send(Shutdown);\n-                let mut sh = special_handle;\n-                sh.send(Shutdown);\n-            });\n-            normal_sched.enqueue_task(normal_task);\n-\n-            let special_task = GreenTask::new(&mut special_sched.stack_pool, None, proc() {\n-                run(task1);\n-                run(task3);\n-                tx.send(());\n-            });\n-            special_sched.enqueue_task(special_task);\n-\n-            let normal_sched = normal_sched;\n-            let normal_thread = Thread::start(proc() { normal_sched.bootstrap() });\n-\n-            let special_sched = special_sched;\n-            let special_thread = Thread::start(proc() { special_sched.bootstrap() });\n-\n-            normal_thread.join();\n-            special_thread.join();\n-        }).join();\n-    }\n-\n-    //#[test]\n-    //fn test_stress_schedule_task_states() {\n-    //    if util::limit_thread_creation_due_to_osx_and_valgrind() { return; }\n-    //    let n = stress_factor() * 120;\n-    //    for _ in range(0, n as int) {\n-    //        test_schedule_home_states();\n-    //    }\n-    //}\n-\n-    #[test]\n-    fn wakeup_across_scheds() {\n-        let (tx1, rx1) = channel();\n-        let (tx2, rx2) = channel();\n-\n-        let mut pool1 = pool();\n-        let mut pool2 = pool();\n-\n-        pool1.spawn(TaskOpts::new(), proc() {\n-            let id = sched_id();\n-            tx1.send(());\n-            rx2.recv();\n-            assert_eq!(id, sched_id());\n-        });\n-\n-        pool2.spawn(TaskOpts::new(), proc() {\n-            let id = sched_id();\n-            rx1.recv();\n-            assert_eq!(id, sched_id());\n-            tx2.send(());\n-        });\n-\n-        pool1.shutdown();\n-        pool2.shutdown();\n-    }\n-\n-    // A regression test that the final message is always handled.\n-    // Used to deadlock because Shutdown was never recvd.\n-    #[test]\n-    fn no_missed_messages() {\n-        let mut pool = pool();\n-\n-        let task = pool.task(TaskOpts::new(), proc()());\n-        pool.spawn_sched().send(TaskFromFriend(task));\n-\n-        pool.shutdown();\n-    }\n-\n-    #[test]\n-    fn multithreading() {\n-        run(proc() {\n-            let mut rxs = vec![];\n-            for _ in range(0u, 10) {\n-                let (tx, rx) = channel();\n-                spawn(proc() {\n-                    tx.send(());\n-                });\n-                rxs.push(rx);\n-            }\n-\n-            loop {\n-                match rxs.pop() {\n-                    Some(rx) => rx.recv(),\n-                    None => break,\n-                }\n-            }\n-        });\n-    }\n-\n-     #[test]\n-    fn thread_ring() {\n-        run(proc() {\n-            let (end_tx, end_rx) = channel();\n-\n-            let n_tasks = 10;\n-            let token = 2000;\n-\n-            let (tx1, mut rx) = channel();\n-            tx1.send((token, end_tx));\n-            let mut i = 2;\n-            while i <= n_tasks {\n-                let (tx, next_rx) = channel();\n-                let imm_i = i;\n-                let imm_rx = rx;\n-                spawn(proc() {\n-                    roundtrip(imm_i, n_tasks, &imm_rx, &tx);\n-                });\n-                rx = next_rx;\n-                i += 1;\n-            }\n-            let rx = rx;\n-            spawn(proc() {\n-                roundtrip(1, n_tasks, &rx, &tx1);\n-            });\n-\n-            end_rx.recv();\n-        });\n-\n-        fn roundtrip(id: int, n_tasks: int,\n-                     rx: &Receiver<(int, Sender<()>)>,\n-                     tx: &Sender<(int, Sender<()>)>) {\n-            loop {\n-                match rx.recv() {\n-                    (1, end_tx) => {\n-                        debug!(\"{}\\n\", id);\n-                        end_tx.send(());\n-                        return;\n-                    }\n-                    (token, end_tx) => {\n-                        debug!(\"thread: {}   got token: {}\", id, token);\n-                        tx.send((token - 1, end_tx));\n-                        if token <= n_tasks {\n-                            return;\n-                        }\n-                    }\n-                }\n-            }\n-        }\n-    }\n-\n-    #[test]\n-    fn start_closure_dtor() {\n-        // Regression test that the `start` task entrypoint can\n-        // contain dtors that use task resources\n-        run(proc() {\n-            #[allow(dead_code)]\n-            struct S { field: () }\n-\n-            impl Drop for S {\n-                fn drop(&mut self) {\n-                    let _foo = box 0i;\n-                }\n-            }\n-\n-            let s = S { field: () };\n-\n-            spawn(proc() {\n-                let _ss = &s;\n-            });\n-        });\n-    }\n-\n-    #[test]\n-    fn dont_starve_1() {\n-        let mut pool = SchedPool::new(PoolConfig {\n-            threads: 2, // this must be > 1\n-            event_loop_factory: basic::event_loop,\n-        });\n-        pool.spawn(TaskOpts::new(), proc() {\n-            let (tx, rx) = channel();\n-\n-            // This task should not be able to starve the sender;\n-            // The sender should get stolen to another thread.\n-            spawn(proc() {\n-                while rx.try_recv().is_err() { }\n-            });\n-\n-            tx.send(());\n-        });\n-        pool.shutdown();\n-    }\n-\n-    #[test]\n-    fn dont_starve_2() {\n-        run(proc() {\n-            let (tx1, rx1) = channel();\n-            let (tx2, _rx2) = channel();\n-\n-            // This task should not be able to starve the other task.\n-            // The sends should eventually yield.\n-            spawn(proc() {\n-                while rx1.try_recv().is_err() {\n-                    tx2.send(());\n-                }\n-            });\n-\n-            tx1.send(());\n-        });\n-    }\n-\n-    // Regression test for a logic bug that would cause single-threaded\n-    // schedulers to sleep forever after yielding and stealing another task.\n-    #[test]\n-    fn single_threaded_yield() {\n-        use std::task::deschedule;\n-        run(proc() {\n-            for _ in range(0u, 5) { deschedule(); }\n-        });\n-    }\n-\n-    #[test]\n-    fn test_spawn_sched_blocking() {\n-        use std::rt::mutex::{StaticNativeMutex, NATIVE_MUTEX_INIT};\n-        static LOCK: StaticNativeMutex = NATIVE_MUTEX_INIT;\n-\n-        // Testing that a task in one scheduler can block in foreign code\n-        // without affecting other schedulers\n-        for _ in range(0u, 20) {\n-            let mut pool = pool();\n-            let (start_tx, start_rx) = channel();\n-            let (fin_tx, fin_rx) = channel();\n-\n-            let mut handle = pool.spawn_sched();\n-            handle.send(PinnedTask(pool.task(TaskOpts::new(), proc() {\n-                unsafe {\n-                    let guard = LOCK.lock();\n-\n-                    start_tx.send(());\n-                    guard.wait();   // block the scheduler thread\n-                    guard.signal(); // let them know we have the lock\n-                }\n-\n-                fin_tx.send(());\n-            })));\n-            drop(handle);\n-\n-            let mut handle = pool.spawn_sched();\n-            handle.send(PinnedTask(pool.task(TaskOpts::new(), proc() {\n-                // Wait until the other task has its lock\n-                start_rx.recv();\n-\n-                fn pingpong(po: &Receiver<int>, ch: &Sender<int>) {\n-                    let mut val = 20;\n-                    while val > 0 {\n-                        val = po.recv();\n-                        let _ = ch.send_opt(val - 1);\n-                    }\n-                }\n-\n-                let (setup_tx, setup_rx) = channel();\n-                let (parent_tx, parent_rx) = channel();\n-                spawn(proc() {\n-                    let (child_tx, child_rx) = channel();\n-                    setup_tx.send(child_tx);\n-                    pingpong(&child_rx, &parent_tx);\n-                });\n-\n-                let child_tx = setup_rx.recv();\n-                child_tx.send(20);\n-                pingpong(&parent_rx, &child_tx);\n-                unsafe {\n-                    let guard = LOCK.lock();\n-                    guard.signal();   // wakeup waiting scheduler\n-                    guard.wait();     // wait for them to grab the lock\n-                }\n-            })));\n-            drop(handle);\n-\n-            fin_rx.recv();\n-            pool.shutdown();\n-        }\n-        unsafe { LOCK.destroy(); }\n-    }\n-}"}, {"sha": "e26a099c0282561f0a19628f1f0df3cbbdf504f3", "filename": "src/libgreen/simple.rs", "status": "removed", "additions": 0, "deletions": 96, "changes": 96, "blob_url": "https://github.com/rust-lang/rust/blob/3ee916e50bd86768cb2a9141f9b2c52d2601b412/src%2Flibgreen%2Fsimple.rs", "raw_url": "https://github.com/rust-lang/rust/raw/3ee916e50bd86768cb2a9141f9b2c52d2601b412/src%2Flibgreen%2Fsimple.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibgreen%2Fsimple.rs?ref=3ee916e50bd86768cb2a9141f9b2c52d2601b412", "patch": "@@ -1,96 +0,0 @@\n-// Copyright 2013 The Rust Project Developers. See the COPYRIGHT\n-// file at the top-level directory of this distribution and at\n-// http://rust-lang.org/COPYRIGHT.\n-//\n-// Licensed under the Apache License, Version 2.0 <LICENSE-APACHE or\n-// http://www.apache.org/licenses/LICENSE-2.0> or the MIT license\n-// <LICENSE-MIT or http://opensource.org/licenses/MIT>, at your\n-// option. This file may not be copied, modified, or distributed\n-// except according to those terms.\n-\n-//! A small module implementing a simple \"runtime\" used for bootstrapping a rust\n-//! scheduler pool and then interacting with it.\n-\n-use std::any::Any;\n-use std::mem;\n-use std::rt::Runtime;\n-use std::rt::local::Local;\n-use std::rt::mutex::NativeMutex;\n-use std::rt::task::{Task, BlockedTask, TaskOpts};\n-\n-struct SimpleTask {\n-    lock: NativeMutex,\n-    awoken: bool,\n-}\n-\n-impl Runtime for SimpleTask {\n-    // Implement the simple tasks of descheduling and rescheduling, but only in\n-    // a simple number of cases.\n-    fn deschedule(mut self: Box<SimpleTask>,\n-                  times: uint,\n-                  mut cur_task: Box<Task>,\n-                  f: |BlockedTask| -> Result<(), BlockedTask>) {\n-        assert!(times == 1);\n-\n-        let me = &mut *self as *mut SimpleTask;\n-        let cur_dupe = &mut *cur_task as *mut Task;\n-        cur_task.put_runtime(self);\n-        let task = BlockedTask::block(cur_task);\n-\n-        // See libnative/task.rs for what's going on here with the `awoken`\n-        // field and the while loop around wait()\n-        unsafe {\n-            let guard = (*me).lock.lock();\n-            (*me).awoken = false;\n-            match f(task) {\n-                Ok(()) => {\n-                    while !(*me).awoken {\n-                        guard.wait();\n-                    }\n-                }\n-                Err(task) => { mem::forget(task.wake()); }\n-            }\n-            drop(guard);\n-            cur_task = mem::transmute(cur_dupe);\n-        }\n-        Local::put(cur_task);\n-    }\n-    fn reawaken(mut self: Box<SimpleTask>, mut to_wake: Box<Task>) {\n-        let me = &mut *self as *mut SimpleTask;\n-        to_wake.put_runtime(self);\n-        unsafe {\n-            mem::forget(to_wake);\n-            let guard = (*me).lock.lock();\n-            (*me).awoken = true;\n-            guard.signal();\n-        }\n-    }\n-\n-    // These functions are all unimplemented and panic as a result. This is on\n-    // purpose. A \"simple task\" is just that, a very simple task that can't\n-    // really do a whole lot. The only purpose of the task is to get us off our\n-    // feet and running.\n-    fn yield_now(self: Box<SimpleTask>, _cur_task: Box<Task>) { panic!() }\n-    fn maybe_yield(self: Box<SimpleTask>, _cur_task: Box<Task>) { panic!() }\n-    fn spawn_sibling(self: Box<SimpleTask>,\n-                     _cur_task: Box<Task>,\n-                     _opts: TaskOpts,\n-                     _f: proc():Send) {\n-        panic!()\n-    }\n-\n-    fn stack_bounds(&self) -> (uint, uint) { panic!() }\n-    fn stack_guard(&self) -> Option<uint> { panic!() }\n-\n-    fn can_block(&self) -> bool { true }\n-    fn wrap(self: Box<SimpleTask>) -> Box<Any+'static> { panic!() }\n-}\n-\n-pub fn task() -> Box<Task> {\n-    let mut task = box Task::new();\n-    task.put_runtime(box SimpleTask {\n-        lock: unsafe {NativeMutex::new()},\n-        awoken: false,\n-    });\n-    return task;\n-}"}, {"sha": "5df866955e656101470378a26a3bcf5edb1f60b3", "filename": "src/libgreen/sleeper_list.rs", "status": "removed", "additions": 0, "deletions": 46, "changes": 46, "blob_url": "https://github.com/rust-lang/rust/blob/3ee916e50bd86768cb2a9141f9b2c52d2601b412/src%2Flibgreen%2Fsleeper_list.rs", "raw_url": "https://github.com/rust-lang/rust/raw/3ee916e50bd86768cb2a9141f9b2c52d2601b412/src%2Flibgreen%2Fsleeper_list.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibgreen%2Fsleeper_list.rs?ref=3ee916e50bd86768cb2a9141f9b2c52d2601b412", "patch": "@@ -1,46 +0,0 @@\n-// Copyright 2013 The Rust Project Developers. See the COPYRIGHT\n-// file at the top-level directory of this distribution and at\n-// http://rust-lang.org/COPYRIGHT.\n-//\n-// Licensed under the Apache License, Version 2.0 <LICENSE-APACHE or\n-// http://www.apache.org/licenses/LICENSE-2.0> or the MIT license\n-// <LICENSE-MIT or http://opensource.org/licenses/MIT>, at your\n-// option. This file may not be copied, modified, or distributed\n-// except according to those terms.\n-\n-//! Maintains a shared list of sleeping schedulers. Schedulers\n-//! use this to wake each other up.\n-\n-use std::sync::mpmc_bounded_queue::Queue;\n-\n-use sched::SchedHandle;\n-\n-pub struct SleeperList {\n-    q: Queue<SchedHandle>,\n-}\n-\n-impl SleeperList {\n-    pub fn new() -> SleeperList {\n-        SleeperList{q: Queue::with_capacity(8*1024)}\n-    }\n-\n-    pub fn push(&mut self, value: SchedHandle) {\n-        assert!(self.q.push(value))\n-    }\n-\n-    pub fn pop(&mut self) -> Option<SchedHandle> {\n-        self.q.pop()\n-    }\n-\n-    pub fn casual_pop(&mut self) -> Option<SchedHandle> {\n-        self.q.pop()\n-    }\n-}\n-\n-impl Clone for SleeperList {\n-    fn clone(&self) -> SleeperList {\n-        SleeperList {\n-            q: self.q.clone()\n-        }\n-    }\n-}"}, {"sha": "81e6152b3d7c3802499a7f9c11e8e8849b0af6f7", "filename": "src/libgreen/stack.rs", "status": "removed", "additions": 0, "deletions": 215, "changes": 215, "blob_url": "https://github.com/rust-lang/rust/blob/3ee916e50bd86768cb2a9141f9b2c52d2601b412/src%2Flibgreen%2Fstack.rs", "raw_url": "https://github.com/rust-lang/rust/raw/3ee916e50bd86768cb2a9141f9b2c52d2601b412/src%2Flibgreen%2Fstack.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibgreen%2Fstack.rs?ref=3ee916e50bd86768cb2a9141f9b2c52d2601b412", "patch": "@@ -1,215 +0,0 @@\n-// Copyright 2013 The Rust Project Developers. See the COPYRIGHT\n-// file at the top-level directory of this distribution and at\n-// http://rust-lang.org/COPYRIGHT.\n-//\n-// Licensed under the Apache License, Version 2.0 <LICENSE-APACHE or\n-// http://www.apache.org/licenses/LICENSE-2.0> or the MIT license\n-// <LICENSE-MIT or http://opensource.org/licenses/MIT>, at your\n-// option. This file may not be copied, modified, or distributed\n-// except according to those terms.\n-\n-use std::ptr;\n-use std::sync::atomic;\n-use std::os::{errno, page_size, MemoryMap, MapReadable, MapWritable,\n-              MapNonStandardFlags, getenv};\n-use libc;\n-\n-/// A task's stack. The name \"Stack\" is a vestige of segmented stacks.\n-pub struct Stack {\n-    buf: Option<MemoryMap>,\n-    min_size: uint,\n-    valgrind_id: libc::c_uint,\n-}\n-\n-// Try to use MAP_STACK on platforms that support it (it's what we're doing\n-// anyway), but some platforms don't support it at all. For example, it appears\n-// that there's a bug in freebsd that MAP_STACK implies MAP_FIXED (so it always\n-// panics): http://lists.freebsd.org/pipermail/freebsd-bugs/2011-July/044840.html\n-//\n-// DragonFly BSD also seems to suffer from the same problem. When MAP_STACK is\n-// used, it returns the same `ptr` multiple times.\n-#[cfg(not(any(windows, target_os = \"freebsd\", target_os = \"dragonfly\")))]\n-static STACK_FLAGS: libc::c_int = libc::MAP_STACK | libc::MAP_PRIVATE |\n-                                  libc::MAP_ANON;\n-#[cfg(any(target_os = \"freebsd\", target_os = \"dragonfly\"))]\n-static STACK_FLAGS: libc::c_int = libc::MAP_PRIVATE | libc::MAP_ANON;\n-#[cfg(windows)]\n-static STACK_FLAGS: libc::c_int = 0;\n-\n-impl Stack {\n-    /// Allocate a new stack of `size`. If size = 0, this will panic. Use\n-    /// `dummy_stack` if you want a zero-sized stack.\n-    pub fn new(size: uint) -> Stack {\n-        // Map in a stack. Eventually we might be able to handle stack\n-        // allocation failure, which would fail to spawn the task. But there's\n-        // not many sensible things to do on OOM. Panic seems fine (and is\n-        // what the old stack allocation did).\n-        let stack = match MemoryMap::new(size, &[MapReadable, MapWritable,\n-                                                 MapNonStandardFlags(STACK_FLAGS)]) {\n-            Ok(map) => map,\n-            Err(e) => panic!(\"mmap for stack of size {} failed: {}\", size, e)\n-        };\n-\n-        // Change the last page to be inaccessible. This is to provide safety;\n-        // when an FFI function overflows it will (hopefully) hit this guard\n-        // page. It isn't guaranteed, but that's why FFI is unsafe. buf.data is\n-        // guaranteed to be aligned properly.\n-        if !protect_last_page(&stack) {\n-            panic!(\"Could not memory-protect guard page. stack={}, errno={}\",\n-                  stack.data(), errno());\n-        }\n-\n-        let mut stk = Stack {\n-            buf: Some(stack),\n-            min_size: size,\n-            valgrind_id: 0\n-        };\n-\n-        // FIXME: Using the FFI to call a C macro. Slow\n-        stk.valgrind_id = unsafe {\n-            rust_valgrind_stack_register(stk.start() as *const libc::uintptr_t,\n-                                         stk.end() as *const libc::uintptr_t)\n-        };\n-        return stk;\n-    }\n-\n-    /// Create a 0-length stack which starts (and ends) at 0.\n-    pub unsafe fn dummy_stack() -> Stack {\n-        Stack {\n-            buf: None,\n-            min_size: 0,\n-            valgrind_id: 0\n-        }\n-    }\n-\n-    /// Point to the last writable byte of the stack\n-    pub fn guard(&self) -> *const uint {\n-        (self.start() as uint + page_size()) as *const uint\n-    }\n-\n-    /// Point to the low end of the allocated stack\n-    pub fn start(&self) -> *const uint {\n-        self.buf.as_ref().map(|m| m.data() as *const uint)\n-            .unwrap_or(ptr::null())\n-    }\n-\n-    /// Point one uint beyond the high end of the allocated stack\n-    pub fn end(&self) -> *const uint {\n-        self.buf.as_ref().map(|buf| unsafe {\n-            buf.data().offset(buf.len() as int) as *const uint\n-        }).unwrap_or(ptr::null())\n-    }\n-}\n-\n-#[cfg(unix)]\n-fn protect_last_page(stack: &MemoryMap) -> bool {\n-    unsafe {\n-        // This may seem backwards: the start of the segment is the last page?\n-        // Yes! The stack grows from higher addresses (the end of the allocated\n-        // block) to lower addresses (the start of the allocated block).\n-        let last_page = stack.data() as *mut libc::c_void;\n-        libc::mprotect(last_page, page_size() as libc::size_t,\n-                       libc::PROT_NONE) != -1\n-    }\n-}\n-\n-#[cfg(windows)]\n-fn protect_last_page(stack: &MemoryMap) -> bool {\n-    unsafe {\n-        // see above\n-        let last_page = stack.data() as *mut libc::c_void;\n-        let mut old_prot: libc::DWORD = 0;\n-        libc::VirtualProtect(last_page, page_size() as libc::SIZE_T,\n-                             libc::PAGE_NOACCESS,\n-                             &mut old_prot as libc::LPDWORD) != 0\n-    }\n-}\n-\n-impl Drop for Stack {\n-    fn drop(&mut self) {\n-        unsafe {\n-            // FIXME: Using the FFI to call a C macro. Slow\n-            rust_valgrind_stack_deregister(self.valgrind_id);\n-        }\n-    }\n-}\n-\n-pub struct StackPool {\n-    // Ideally this would be some data structure that preserved ordering on\n-    // Stack.min_size.\n-    stacks: Vec<Stack>,\n-}\n-\n-impl StackPool {\n-    pub fn new() -> StackPool {\n-        StackPool {\n-            stacks: vec![],\n-        }\n-    }\n-\n-    pub fn take_stack(&mut self, min_size: uint) -> Stack {\n-        // Ideally this would be a binary search\n-        match self.stacks.iter().position(|s| min_size <= s.min_size) {\n-            Some(idx) => self.stacks.swap_remove(idx).unwrap(),\n-            None => Stack::new(min_size)\n-        }\n-    }\n-\n-    pub fn give_stack(&mut self, stack: Stack) {\n-        if self.stacks.len() <= max_cached_stacks() {\n-            self.stacks.push(stack)\n-        }\n-    }\n-}\n-\n-fn max_cached_stacks() -> uint {\n-    static AMT: atomic::AtomicUint = atomic::INIT_ATOMIC_UINT;\n-    match AMT.load(atomic::SeqCst) {\n-        0 => {}\n-        n => return n - 1,\n-    }\n-    let amt = getenv(\"RUST_MAX_CACHED_STACKS\").and_then(|s| from_str(s.as_slice()));\n-    // This default corresponds to 20M of cache per scheduler (at the\n-    // default size).\n-    let amt = amt.unwrap_or(10);\n-    // 0 is our sentinel value, so ensure that we'll never see 0 after\n-    // initialization has run\n-    AMT.store(amt + 1, atomic::SeqCst);\n-    return amt;\n-}\n-\n-extern {\n-    fn rust_valgrind_stack_register(start: *const libc::uintptr_t,\n-                                    end: *const libc::uintptr_t) -> libc::c_uint;\n-    fn rust_valgrind_stack_deregister(id: libc::c_uint);\n-}\n-\n-#[cfg(test)]\n-mod tests {\n-    use super::StackPool;\n-\n-    #[test]\n-    fn stack_pool_caches() {\n-        let mut p = StackPool::new();\n-        let s = p.take_stack(10);\n-        p.give_stack(s);\n-        let s = p.take_stack(4);\n-        assert_eq!(s.min_size, 10);\n-        p.give_stack(s);\n-        let s = p.take_stack(14);\n-        assert_eq!(s.min_size, 14);\n-        p.give_stack(s);\n-    }\n-\n-    #[test]\n-    fn stack_pool_caches_exact() {\n-        let mut p = StackPool::new();\n-        let mut s = p.take_stack(10);\n-        s.valgrind_id = 100;\n-        p.give_stack(s);\n-\n-        let s = p.take_stack(10);\n-        assert_eq!(s.min_size, 10);\n-        assert_eq!(s.valgrind_id, 100);\n-    }\n-}"}, {"sha": "e159c153bc38c73a576db94442011dc53574129d", "filename": "src/libgreen/task.rs", "status": "removed", "additions": 0, "deletions": 602, "changes": 602, "blob_url": "https://github.com/rust-lang/rust/blob/3ee916e50bd86768cb2a9141f9b2c52d2601b412/src%2Flibgreen%2Ftask.rs", "raw_url": "https://github.com/rust-lang/rust/raw/3ee916e50bd86768cb2a9141f9b2c52d2601b412/src%2Flibgreen%2Ftask.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibgreen%2Ftask.rs?ref=3ee916e50bd86768cb2a9141f9b2c52d2601b412", "patch": "@@ -1,602 +0,0 @@\n-// Copyright 2013 The Rust Project Developers. See the COPYRIGHT\n-// file at the top-level directory of this distribution and at\n-// http://rust-lang.org/COPYRIGHT.\n-//\n-// Licensed under the Apache License, Version 2.0 <LICENSE-APACHE or\n-// http://www.apache.org/licenses/LICENSE-2.0> or the MIT license\n-// <LICENSE-MIT or http://opensource.org/licenses/MIT>, at your\n-// option. This file may not be copied, modified, or distributed\n-// except according to those terms.\n-\n-//! The Green Task implementation\n-//!\n-//! This module contains the glue to the libstd runtime necessary to integrate\n-//! M:N scheduling. This GreenTask structure is hidden as a trait object in all\n-//! rust tasks and virtual calls are made in order to interface with it.\n-//!\n-//! Each green task contains a scheduler if it is currently running, and it also\n-//! contains the rust task itself in order to juggle around ownership of the\n-//! values.\n-\n-pub use self::TaskType::*;\n-pub use self::Home::*;\n-\n-use std::any::Any;\n-use std::mem;\n-use std::raw;\n-use std::rt::Runtime;\n-use std::rt::local::Local;\n-use std::rt::mutex::NativeMutex;\n-use std::rt::stack;\n-use std::rt::task::{Task, BlockedTask, TaskOpts};\n-use std::rt;\n-\n-use context::Context;\n-use coroutine::Coroutine;\n-use sched::{Scheduler, SchedHandle, RunOnce};\n-use stack::StackPool;\n-\n-/// The necessary fields needed to keep track of a green task (as opposed to a\n-/// 1:1 task).\n-pub struct GreenTask {\n-    /// Coroutine that this task is running on, otherwise known as the register\n-    /// context and the stack that this task owns. This field is optional to\n-    /// relinquish ownership back to a scheduler to recycle stacks at a later\n-    /// date.\n-    pub coroutine: Option<Coroutine>,\n-\n-    /// Optional handle back into the home sched pool of this task. This field\n-    /// is lazily initialized.\n-    pub handle: Option<SchedHandle>,\n-\n-    /// Slot for maintaining ownership of a scheduler. If a task is running,\n-    /// this value will be Some(sched) where the task is running on \"sched\".\n-    pub sched: Option<Box<Scheduler>>,\n-\n-    /// Temporary ownership slot of a std::rt::task::Task object. This is used\n-    /// to squirrel that libstd task away while we're performing green task\n-    /// operations.\n-    pub task: Option<Box<Task>>,\n-\n-    /// Dictates whether this is a sched task or a normal green task\n-    pub task_type: TaskType,\n-\n-    /// Home pool that this task was spawned into. This field is lazily\n-    /// initialized until when the task is initially scheduled, and is used to\n-    /// make sure that tasks are always woken up in the correct pool of\n-    /// schedulers.\n-    pub pool_id: uint,\n-\n-    // See the comments in the scheduler about why this is necessary\n-    pub nasty_deschedule_lock: NativeMutex,\n-}\n-\n-pub enum TaskType {\n-    TypeGreen(Option<Home>),\n-    TypeSched,\n-}\n-\n-pub enum Home {\n-    AnySched,\n-    HomeSched(SchedHandle),\n-}\n-\n-/// Trampoline code for all new green tasks which are running around. This\n-/// function is passed through to Context::new as the initial rust landing pad\n-/// for all green tasks. This code is actually called after the initial context\n-/// switch onto a green thread.\n-///\n-/// The first argument to this function is the `Box<GreenTask>` pointer, and\n-/// the next two arguments are the user-provided procedure for running code.\n-///\n-/// The goal for having this weird-looking function is to reduce the number of\n-/// allocations done on a green-task startup as much as possible.\n-extern fn bootstrap_green_task(task: uint, code: *mut (), env: *mut ()) -> ! {\n-    // Acquire ownership of the `proc()`\n-    let start: proc() = unsafe {\n-        mem::transmute(raw::Procedure { code: code, env: env })\n-    };\n-\n-    // Acquire ownership of the `Box<GreenTask>`\n-    let mut task: Box<GreenTask> = unsafe { mem::transmute(task) };\n-\n-    // First code after swap to this new context. Run our cleanup job\n-    task.pool_id = {\n-        let sched = task.sched.as_mut().unwrap();\n-        sched.run_cleanup_job();\n-        sched.task_state.increment();\n-        sched.pool_id\n-    };\n-\n-    // Convert our green task to a libstd task and then execute the code\n-    // requested. This is the \"try/catch\" block for this green task and\n-    // is the wrapper for *all* code run in the task.\n-    let mut start = Some(start);\n-    let task = task.swap().run(|| start.take().unwrap()()).destroy();\n-\n-    // Once the function has exited, it's time to run the termination\n-    // routine. This means we need to context switch one more time but\n-    // clean ourselves up on the other end. Since we have no way of\n-    // preserving a handle to the GreenTask down to this point, this\n-    // unfortunately must call `GreenTask::convert`. In order to avoid\n-    // this we could add a `terminate` function to the `Runtime` trait\n-    // in libstd, but that seems less appropriate since the conversion\n-    // method exists.\n-    GreenTask::convert(task).terminate();\n-}\n-\n-impl GreenTask {\n-    /// Creates a new green task which is not homed to any particular scheduler\n-    /// and will not have any contained Task structure.\n-    pub fn new(stack_pool: &mut StackPool,\n-               stack_size: Option<uint>,\n-               start: proc():Send) -> Box<GreenTask> {\n-        GreenTask::new_homed(stack_pool, stack_size, AnySched, start)\n-    }\n-\n-    /// Creates a new task (like `new`), but specifies the home for new task.\n-    pub fn new_homed(stack_pool: &mut StackPool,\n-                     stack_size: Option<uint>,\n-                     home: Home,\n-                     start: proc():Send) -> Box<GreenTask> {\n-        // Allocate ourselves a GreenTask structure\n-        let mut ops = GreenTask::new_typed(None, TypeGreen(Some(home)));\n-\n-        // Allocate a stack for us to run on\n-        let stack_size = stack_size.unwrap_or_else(|| rt::min_stack());\n-        let mut stack = stack_pool.take_stack(stack_size);\n-        let context = Context::new(bootstrap_green_task, ops.as_uint(), start,\n-                                   &mut stack);\n-\n-        // Package everything up in a coroutine and return\n-        ops.coroutine = Some(Coroutine {\n-            current_stack_segment: stack,\n-            saved_context: context,\n-        });\n-        return ops;\n-    }\n-\n-    /// Creates a new green task with the specified coroutine and type, this is\n-    /// useful when creating scheduler tasks.\n-    pub fn new_typed(coroutine: Option<Coroutine>,\n-                     task_type: TaskType) -> Box<GreenTask> {\n-        box GreenTask {\n-            pool_id: 0,\n-            coroutine: coroutine,\n-            task_type: task_type,\n-            sched: None,\n-            handle: None,\n-            nasty_deschedule_lock: unsafe { NativeMutex::new() },\n-            task: Some(box Task::new()),\n-        }\n-    }\n-\n-    /// Creates a new green task with the given configuration options for the\n-    /// contained Task object. The given stack pool is also used to allocate a\n-    /// new stack for this task.\n-    pub fn configure(pool: &mut StackPool,\n-                     opts: TaskOpts,\n-                     f: proc():Send) -> Box<GreenTask> {\n-        let TaskOpts { name, stack_size, on_exit } = opts;\n-\n-        let mut green = GreenTask::new(pool, stack_size, f);\n-        {\n-            let task = green.task.as_mut().unwrap();\n-            task.name = name;\n-            task.death.on_exit = on_exit;\n-        }\n-        return green;\n-    }\n-\n-    /// Just like the `maybe_take_runtime` function, this function should *not*\n-    /// exist. Usage of this function is _strongly_ discouraged. This is an\n-    /// absolute last resort necessary for converting a libstd task to a green\n-    /// task.\n-    ///\n-    /// This function will assert that the task is indeed a green task before\n-    /// returning (and will kill the entire process if this is wrong).\n-    pub fn convert(mut task: Box<Task>) -> Box<GreenTask> {\n-        match task.maybe_take_runtime::<GreenTask>() {\n-            Some(mut green) => {\n-                green.put_task(task);\n-                green\n-            }\n-            None => rtabort!(\"not a green task any more?\"),\n-        }\n-    }\n-\n-    pub fn give_home(&mut self, new_home: Home) {\n-        match self.task_type {\n-            TypeGreen(ref mut home) => { *home = Some(new_home); }\n-            TypeSched => rtabort!(\"type error: used SchedTask as GreenTask\"),\n-        }\n-    }\n-\n-    pub fn take_unwrap_home(&mut self) -> Home {\n-        match self.task_type {\n-            TypeGreen(ref mut home) => home.take().unwrap(),\n-            TypeSched => rtabort!(\"type error: used SchedTask as GreenTask\"),\n-        }\n-    }\n-\n-    // New utility functions for homes.\n-\n-    pub fn is_home_no_tls(&self, sched: &Scheduler) -> bool {\n-        match self.task_type {\n-            TypeGreen(Some(AnySched)) => { false }\n-            TypeGreen(Some(HomeSched(SchedHandle { sched_id: ref id, .. }))) => {\n-                *id == sched.sched_id()\n-            }\n-            TypeGreen(None) => { rtabort!(\"task without home\"); }\n-            TypeSched => {\n-                // Awe yea\n-                rtabort!(\"type error: expected: TypeGreen, found: TaskSched\");\n-            }\n-        }\n-    }\n-\n-    pub fn homed(&self) -> bool {\n-        match self.task_type {\n-            TypeGreen(Some(AnySched)) => { false }\n-            TypeGreen(Some(HomeSched(SchedHandle { .. }))) => { true }\n-            TypeGreen(None) => {\n-                rtabort!(\"task without home\");\n-            }\n-            TypeSched => {\n-                rtabort!(\"type error: expected: TypeGreen, found: TaskSched\");\n-            }\n-        }\n-    }\n-\n-    pub fn is_sched(&self) -> bool {\n-        match self.task_type {\n-            TypeGreen(..) => false, TypeSched => true,\n-        }\n-    }\n-\n-    // Unsafe functions for transferring ownership of this GreenTask across\n-    // context switches\n-\n-    pub fn as_uint(&self) -> uint {\n-        self as *const GreenTask as uint\n-    }\n-\n-    pub unsafe fn from_uint(val: uint) -> Box<GreenTask> {\n-        mem::transmute(val)\n-    }\n-\n-    // Runtime glue functions and helpers\n-\n-    pub fn put_with_sched(mut self: Box<GreenTask>, sched: Box<Scheduler>) {\n-        assert!(self.sched.is_none());\n-        self.sched = Some(sched);\n-        self.put();\n-    }\n-\n-    pub fn put_task(&mut self, task: Box<Task>) {\n-        assert!(self.task.is_none());\n-        self.task = Some(task);\n-    }\n-\n-    pub fn swap(mut self: Box<GreenTask>) -> Box<Task> {\n-        let mut task = self.task.take().unwrap();\n-        task.put_runtime(self);\n-        return task;\n-    }\n-\n-    pub fn put(self: Box<GreenTask>) {\n-        assert!(self.sched.is_some());\n-        Local::put(self.swap());\n-    }\n-\n-    fn terminate(mut self: Box<GreenTask>) -> ! {\n-        let sched = self.sched.take().unwrap();\n-        sched.terminate_current_task(self)\n-    }\n-\n-    // This function is used to remotely wakeup this green task back on to its\n-    // original pool of schedulers. In order to do so, each tasks arranges a\n-    // SchedHandle upon descheduling to be available for sending itself back to\n-    // the original pool.\n-    //\n-    // Note that there is an interesting transfer of ownership going on here. We\n-    // must relinquish ownership of the green task, but then also send the task\n-    // over the handle back to the original scheduler. In order to safely do\n-    // this, we leverage the already-present \"nasty descheduling lock\". The\n-    // reason for doing this is that each task will bounce on this lock after\n-    // resuming after a context switch. By holding the lock over the enqueueing\n-    // of the task, we're guaranteed that the SchedHandle's memory will be valid\n-    // for this entire function.\n-    //\n-    // An alternative would include having incredibly cheaply cloneable handles,\n-    // but right now a SchedHandle is something like 6 allocations, so it is\n-    // *not* a cheap operation to clone a handle. Until the day comes that we\n-    // need to optimize this, a lock should do just fine (it's completely\n-    // uncontended except for when the task is rescheduled).\n-    fn reawaken_remotely(mut self: Box<GreenTask>) {\n-        unsafe {\n-            let mtx = &mut self.nasty_deschedule_lock as *mut NativeMutex;\n-            let handle = self.handle.as_mut().unwrap() as *mut SchedHandle;\n-            let _guard = (*mtx).lock();\n-            (*handle).send(RunOnce(self));\n-        }\n-    }\n-}\n-\n-impl Runtime for GreenTask {\n-    fn yield_now(mut self: Box<GreenTask>, cur_task: Box<Task>) {\n-        self.put_task(cur_task);\n-        let sched = self.sched.take().unwrap();\n-        sched.yield_now(self);\n-    }\n-\n-    fn maybe_yield(mut self: Box<GreenTask>, cur_task: Box<Task>) {\n-        self.put_task(cur_task);\n-        let sched = self.sched.take().unwrap();\n-        sched.maybe_yield(self);\n-    }\n-\n-    fn deschedule(mut self: Box<GreenTask>,\n-                  times: uint,\n-                  cur_task: Box<Task>,\n-                  f: |BlockedTask| -> Result<(), BlockedTask>) {\n-        self.put_task(cur_task);\n-        let mut sched = self.sched.take().unwrap();\n-\n-        // In order for this task to be reawoken in all possible contexts, we\n-        // may need a handle back in to the current scheduler. When we're woken\n-        // up in anything other than the local scheduler pool, this handle is\n-        // used to send this task back into the scheduler pool.\n-        if self.handle.is_none() {\n-            self.handle = Some(sched.make_handle());\n-            self.pool_id = sched.pool_id;\n-        }\n-\n-        // This code is pretty standard, except for the usage of\n-        // `GreenTask::convert`. Right now if we use `reawaken` directly it will\n-        // expect for there to be a task in local TLS, but that is not true for\n-        // this deschedule block (because the scheduler must retain ownership of\n-        // the task while the cleanup job is running). In order to get around\n-        // this for now, we invoke the scheduler directly with the converted\n-        // Task => GreenTask structure.\n-        if times == 1 {\n-            sched.deschedule_running_task_and_then(self, |sched, task| {\n-                match f(task) {\n-                    Ok(()) => {}\n-                    Err(t) => {\n-                        t.wake().map(|t| {\n-                            sched.enqueue_task(GreenTask::convert(t))\n-                        });\n-                    }\n-                }\n-            });\n-        } else {\n-            sched.deschedule_running_task_and_then(self, |sched, task| {\n-                for task in task.make_selectable(times) {\n-                    match f(task) {\n-                        Ok(()) => {},\n-                        Err(task) => {\n-                            task.wake().map(|t| {\n-                                sched.enqueue_task(GreenTask::convert(t))\n-                            });\n-                            break\n-                        }\n-                    }\n-                }\n-            });\n-        }\n-    }\n-\n-    fn reawaken(mut self: Box<GreenTask>, to_wake: Box<Task>) {\n-        self.put_task(to_wake);\n-        assert!(self.sched.is_none());\n-\n-        // Optimistically look for a local task, but if one's not available to\n-        // inspect (in order to see if it's in the same sched pool as we are),\n-        // then just use our remote wakeup routine and carry on!\n-        let mut running_task: Box<Task> = match Local::try_take() {\n-            Some(task) => task,\n-            None => return self.reawaken_remotely()\n-        };\n-\n-        // Waking up a green thread is a bit of a tricky situation. We have no\n-        // guarantee about where the current task is running. The options we\n-        // have for where this current task is running are:\n-        //\n-        //  1. Our original scheduler pool\n-        //  2. Some other scheduler pool\n-        //  3. Something that isn't a scheduler pool\n-        //\n-        // In order to figure out what case we're in, this is the reason that\n-        // the `maybe_take_runtime` function exists. Using this function we can\n-        // dynamically check to see which of these cases is the current\n-        // situation and then dispatch accordingly.\n-        //\n-        // In case 1, we just use the local scheduler to resume ourselves\n-        // immediately (if a rescheduling is possible).\n-        //\n-        // In case 2 and 3, we need to remotely reawaken ourself in order to be\n-        // transplanted back to the correct scheduler pool.\n-        match running_task.maybe_take_runtime::<GreenTask>() {\n-            Some(mut running_green_task) => {\n-                running_green_task.put_task(running_task);\n-                let sched = running_green_task.sched.take().unwrap();\n-\n-                if sched.pool_id == self.pool_id {\n-                    sched.run_task(running_green_task, self);\n-                } else {\n-                    self.reawaken_remotely();\n-\n-                    // put that thing back where it came from!\n-                    running_green_task.put_with_sched(sched);\n-                }\n-            }\n-            None => {\n-                self.reawaken_remotely();\n-                Local::put(running_task);\n-            }\n-        }\n-    }\n-\n-    fn spawn_sibling(mut self: Box<GreenTask>,\n-                     cur_task: Box<Task>,\n-                     opts: TaskOpts,\n-                     f: proc():Send) {\n-        self.put_task(cur_task);\n-\n-        // First, set up a bomb which when it goes off will restore the local\n-        // task unless its disarmed. This will allow us to gracefully panic from\n-        // inside of `configure` which allocates a new task.\n-        struct Bomb { inner: Option<Box<GreenTask>> }\n-        impl Drop for Bomb {\n-            fn drop(&mut self) {\n-                let _ = self.inner.take().map(|task| task.put());\n-            }\n-        }\n-        let mut bomb = Bomb { inner: Some(self) };\n-\n-        // Spawns a task into the current scheduler. We allocate the new task's\n-        // stack from the scheduler's stack pool, and then configure it\n-        // accordingly to `opts`. Afterwards we bootstrap it immediately by\n-        // switching to it.\n-        //\n-        // Upon returning, our task is back in TLS and we're good to return.\n-        let sibling = {\n-            let sched = bomb.inner.as_mut().unwrap().sched.as_mut().unwrap();\n-            GreenTask::configure(&mut sched.stack_pool, opts, f)\n-        };\n-        let mut me = bomb.inner.take().unwrap();\n-        let sched = me.sched.take().unwrap();\n-        sched.run_task(me, sibling)\n-    }\n-\n-    fn stack_bounds(&self) -> (uint, uint) {\n-        let c = self.coroutine.as_ref()\n-            .expect(\"GreenTask.stack_bounds called without a coroutine\");\n-\n-        // Don't return the red zone as part of the usable stack of this task,\n-        // it's essentially an implementation detail.\n-        (c.current_stack_segment.start() as uint + stack::RED_ZONE,\n-         c.current_stack_segment.end() as uint)\n-    }\n-\n-    fn stack_guard(&self) -> Option<uint> {\n-        let c = self.coroutine.as_ref()\n-            .expect(\"GreenTask.stack_guard called without a coroutine\");\n-\n-        Some(c.current_stack_segment.guard() as uint)\n-    }\n-\n-    fn can_block(&self) -> bool { false }\n-\n-    fn wrap(self: Box<GreenTask>) -> Box<Any+'static> {\n-        self as Box<Any+'static>\n-    }\n-}\n-\n-#[cfg(test)]\n-mod tests {\n-    use std::rt::local::Local;\n-    use std::rt::task::Task;\n-    use std::task;\n-    use std::rt::task::TaskOpts;\n-\n-    use super::super::{PoolConfig, SchedPool};\n-    use super::GreenTask;\n-\n-    fn spawn_opts(opts: TaskOpts, f: proc():Send) {\n-        let mut pool = SchedPool::new(PoolConfig {\n-            threads: 1,\n-            event_loop_factory: super::super::basic::event_loop,\n-        });\n-        pool.spawn(opts, f);\n-        pool.shutdown();\n-    }\n-\n-    #[test]\n-    fn smoke() {\n-        let (tx, rx) = channel();\n-        spawn_opts(TaskOpts::new(), proc() {\n-            tx.send(());\n-        });\n-        rx.recv();\n-    }\n-\n-    #[test]\n-    fn smoke_panic() {\n-        let (tx, rx) = channel::<int>();\n-        spawn_opts(TaskOpts::new(), proc() {\n-            let _tx = tx;\n-            panic!()\n-        });\n-        assert_eq!(rx.recv_opt(), Err(()));\n-    }\n-\n-    #[test]\n-    fn smoke_opts() {\n-        let mut opts = TaskOpts::new();\n-        opts.name = Some(\"test\".into_maybe_owned());\n-        opts.stack_size = Some(20 * 4096);\n-        let (tx, rx) = channel();\n-        opts.on_exit = Some(proc(r) tx.send(r));\n-        spawn_opts(opts, proc() {});\n-        assert!(rx.recv().is_ok());\n-    }\n-\n-    #[test]\n-    fn smoke_opts_panic() {\n-        let mut opts = TaskOpts::new();\n-        let (tx, rx) = channel();\n-        opts.on_exit = Some(proc(r) tx.send(r));\n-        spawn_opts(opts, proc() { panic!() });\n-        assert!(rx.recv().is_err());\n-    }\n-\n-    #[test]\n-    fn yield_test() {\n-        let (tx, rx) = channel();\n-        spawn_opts(TaskOpts::new(), proc() {\n-            for _ in range(0u, 10) { task::deschedule(); }\n-            tx.send(());\n-        });\n-        rx.recv();\n-    }\n-\n-    #[test]\n-    fn spawn_children() {\n-        let (tx1, rx) = channel();\n-        spawn_opts(TaskOpts::new(), proc() {\n-            let (tx2, rx) = channel();\n-            spawn(proc() {\n-                let (tx3, rx) = channel();\n-                spawn(proc() {\n-                    tx3.send(());\n-                });\n-                rx.recv();\n-                tx2.send(());\n-            });\n-            rx.recv();\n-            tx1.send(());\n-        });\n-        rx.recv();\n-    }\n-\n-    #[test]\n-    fn spawn_inherits() {\n-        let (tx, rx) = channel();\n-        spawn_opts(TaskOpts::new(), proc() {\n-            spawn(proc() {\n-                let mut task: Box<Task> = Local::take();\n-                match task.maybe_take_runtime::<GreenTask>() {\n-                    Some(ops) => {\n-                        task.put_runtime(ops);\n-                    }\n-                    None => panic!(),\n-                }\n-                Local::put(task);\n-                tx.send(());\n-            });\n-        });\n-        rx.recv();\n-    }\n-}"}]}