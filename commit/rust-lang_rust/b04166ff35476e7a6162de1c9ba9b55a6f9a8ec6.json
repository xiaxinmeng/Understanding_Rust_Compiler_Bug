{"sha": "b04166ff35476e7a6162de1c9ba9b55a6f9a8ec6", "node_id": "C_kwDOAAsO6NoAKGIwNDE2NmZmMzU0NzZlN2E2MTYyZGUxYzliYTliNTVhNmY5YThlYzY", "commit": {"author": {"name": "Ralf Jung", "email": "post@ralfj.de", "date": "2022-11-09T09:18:27Z"}, "committer": {"name": "Ralf Jung", "email": "post@ralfj.de", "date": "2022-11-09T09:18:27Z"}, "message": "another optimization attempt", "tree": {"sha": "ea7859bfa295a22323120dab1fd5314a97b079c1", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/ea7859bfa295a22323120dab1fd5314a97b079c1"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/b04166ff35476e7a6162de1c9ba9b55a6f9a8ec6", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/b04166ff35476e7a6162de1c9ba9b55a6f9a8ec6", "html_url": "https://github.com/rust-lang/rust/commit/b04166ff35476e7a6162de1c9ba9b55a6f9a8ec6", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/b04166ff35476e7a6162de1c9ba9b55a6f9a8ec6/comments", "author": {"login": "RalfJung", "id": 330628, "node_id": "MDQ6VXNlcjMzMDYyOA==", "avatar_url": "https://avatars.githubusercontent.com/u/330628?v=4", "gravatar_id": "", "url": "https://api.github.com/users/RalfJung", "html_url": "https://github.com/RalfJung", "followers_url": "https://api.github.com/users/RalfJung/followers", "following_url": "https://api.github.com/users/RalfJung/following{/other_user}", "gists_url": "https://api.github.com/users/RalfJung/gists{/gist_id}", "starred_url": "https://api.github.com/users/RalfJung/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/RalfJung/subscriptions", "organizations_url": "https://api.github.com/users/RalfJung/orgs", "repos_url": "https://api.github.com/users/RalfJung/repos", "events_url": "https://api.github.com/users/RalfJung/events{/privacy}", "received_events_url": "https://api.github.com/users/RalfJung/received_events", "type": "User", "site_admin": false}, "committer": {"login": "RalfJung", "id": 330628, "node_id": "MDQ6VXNlcjMzMDYyOA==", "avatar_url": "https://avatars.githubusercontent.com/u/330628?v=4", "gravatar_id": "", "url": "https://api.github.com/users/RalfJung", "html_url": "https://github.com/RalfJung", "followers_url": "https://api.github.com/users/RalfJung/followers", "following_url": "https://api.github.com/users/RalfJung/following{/other_user}", "gists_url": "https://api.github.com/users/RalfJung/gists{/gist_id}", "starred_url": "https://api.github.com/users/RalfJung/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/RalfJung/subscriptions", "organizations_url": "https://api.github.com/users/RalfJung/orgs", "repos_url": "https://api.github.com/users/RalfJung/repos", "events_url": "https://api.github.com/users/RalfJung/events{/privacy}", "received_events_url": "https://api.github.com/users/RalfJung/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "f0e554567018fc433ee7d0df2a270c2e14f81923", "url": "https://api.github.com/repos/rust-lang/rust/commits/f0e554567018fc433ee7d0df2a270c2e14f81923", "html_url": "https://github.com/rust-lang/rust/commit/f0e554567018fc433ee7d0df2a270c2e14f81923"}], "stats": {"total": 149, "additions": 83, "deletions": 66}, "files": [{"sha": "ab99cb6611404b0a1a8cef5d8c7bfba4d75419d6", "filename": "compiler/rustc_middle/src/mir/interpret/allocation/provenance_map.rs", "status": "modified", "additions": 83, "deletions": 66, "changes": 149, "blob_url": "https://github.com/rust-lang/rust/blob/b04166ff35476e7a6162de1c9ba9b55a6f9a8ec6/compiler%2Frustc_middle%2Fsrc%2Fmir%2Finterpret%2Fallocation%2Fprovenance_map.rs", "raw_url": "https://github.com/rust-lang/rust/raw/b04166ff35476e7a6162de1c9ba9b55a6f9a8ec6/compiler%2Frustc_middle%2Fsrc%2Fmir%2Finterpret%2Fallocation%2Fprovenance_map.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_middle%2Fsrc%2Fmir%2Finterpret%2Fallocation%2Fprovenance_map.rs?ref=b04166ff35476e7a6162de1c9ba9b55a6f9a8ec6", "patch": "@@ -18,18 +18,18 @@ pub struct ProvenanceMap<Prov = AllocId> {\n     /// Provenance in this map only applies to the given single byte.\n     /// This map is disjoint from the previous. It will always be empty when\n     /// `Prov::OFFSET_IS_ADDR` is false.\n-    bytes: SortedMap<Size, Prov>,\n+    bytes: Option<Box<SortedMap<Size, Prov>>>,\n }\n \n impl<Prov> ProvenanceMap<Prov> {\n     pub fn new() -> Self {\n-        ProvenanceMap { ptrs: SortedMap::new(), bytes: SortedMap::new() }\n+        ProvenanceMap { ptrs: SortedMap::new(), bytes: None }\n     }\n \n     /// The caller must guarantee that the given provenance list is already sorted\n     /// by address and contain no duplicates.\n     pub fn from_presorted_ptrs(r: Vec<(Size, Prov)>) -> Self {\n-        ProvenanceMap { ptrs: SortedMap::from_presorted_elements(r), bytes: SortedMap::new() }\n+        ProvenanceMap { ptrs: SortedMap::from_presorted_elements(r), bytes: None }\n     }\n }\n \n@@ -40,7 +40,7 @@ impl ProvenanceMap {\n     /// Only exposed with `AllocId` provenance, since it panics if there is bytewise provenance.\n     #[inline]\n     pub fn ptrs(&self) -> &SortedMap<Size, AllocId> {\n-        debug_assert!(self.bytes.is_empty()); // `AllocId::OFFSET_IS_ADDR` is false so this cannot fail\n+        debug_assert!(self.bytes.is_none()); // `AllocId::OFFSET_IS_ADDR` is false so this cannot fail\n         &self.ptrs\n     }\n }\n@@ -60,7 +60,11 @@ impl<Prov: Provenance> ProvenanceMap<Prov> {\n \n     /// Returns all byte-wise provenance in the given range.\n     fn range_get_bytes(&self, range: AllocRange) -> &[(Size, Prov)] {\n-        self.bytes.range(range.start..range.end())\n+        if let Some(bytes) = self.bytes.as_ref() {\n+            bytes.range(range.start..range.end())\n+        } else {\n+            &[]\n+        }\n     }\n \n     /// Get the provenance of a single byte.\n@@ -69,11 +73,11 @@ impl<Prov: Provenance> ProvenanceMap<Prov> {\n         debug_assert!(prov.len() <= 1);\n         if let Some(entry) = prov.first() {\n             // If it overlaps with this byte, it is on this byte.\n-            debug_assert!(self.bytes.get(&offset).is_none());\n+            debug_assert!(self.bytes.as_ref().map_or(true, |b| b.get(&offset).is_none()));\n             Some(entry.1)\n         } else {\n             // Look up per-byte provenance.\n-            self.bytes.get(&offset).copied()\n+            self.bytes.as_ref().and_then(|b| b.get(&offset).copied())\n         }\n     }\n \n@@ -94,7 +98,8 @@ impl<Prov: Provenance> ProvenanceMap<Prov> {\n \n     /// Yields all the provenances stored in this map.\n     pub fn provenances(&self) -> impl Iterator<Item = Prov> + '_ {\n-        self.ptrs.values().chain(self.bytes.values()).copied()\n+        let bytes = self.bytes.iter().flat_map(|b| b.values());\n+        self.ptrs.values().chain(bytes).copied()\n     }\n \n     pub fn insert_ptr(&mut self, offset: Size, prov: Prov, cx: &impl HasDataLayout) {\n@@ -109,9 +114,11 @@ impl<Prov: Provenance> ProvenanceMap<Prov> {\n         let end = range.end();\n         // Clear the bytewise part -- this is easy.\n         if Prov::OFFSET_IS_ADDR {\n-            self.bytes.remove_range(start..end);\n+            if let Some(bytes) = self.bytes.as_mut() {\n+                bytes.remove_range(start..end);\n+            }\n         } else {\n-            debug_assert!(self.bytes.is_empty());\n+            debug_assert!(self.bytes.is_none());\n         }\n \n         // For the ptr-sized part, find the first (inclusive) and last (exclusive) byte of\n@@ -138,8 +145,9 @@ impl<Prov: Provenance> ProvenanceMap<Prov> {\n             }\n             // Insert the remaining part in the bytewise provenance.\n             let prov = self.ptrs[&first];\n+            let bytes = self.bytes.get_or_insert_with(Box::default);\n             for offset in first..start {\n-                self.bytes.insert(offset, prov);\n+                bytes.insert(offset, prov);\n             }\n         }\n         if last > end {\n@@ -150,8 +158,9 @@ impl<Prov: Provenance> ProvenanceMap<Prov> {\n             }\n             // Insert the remaining part in the bytewise provenance.\n             let prov = self.ptrs[&begin_of_last];\n+            let bytes = self.bytes.get_or_insert_with(Box::default);\n             for offset in end..last {\n-                self.bytes.insert(offset, prov);\n+                bytes.insert(offset, prov);\n             }\n         }\n \n@@ -168,8 +177,8 @@ impl<Prov: Provenance> ProvenanceMap<Prov> {\n ///\n /// Offsets are already adjusted to the destination allocation.\n pub struct ProvenanceCopy<Prov> {\n-    dest_ptrs: Vec<(Size, Prov)>,\n-    dest_bytes: Vec<(Size, Prov)>,\n+    dest_ptrs: Option<Box<[(Size, Prov)]>>,\n+    dest_bytes: Option<Box<[(Size, Prov)]>>,\n }\n \n impl<Prov: Provenance> ProvenanceMap<Prov> {\n@@ -192,96 +201,104 @@ impl<Prov: Provenance> ProvenanceMap<Prov> {\n         // Get the provenances that are entirely within this range.\n         // (Different from `range_get_ptrs` which asks if they overlap the range.)\n         // Only makes sense if we are copying at least one pointer worth of bytes.\n-        let mut dest_ptrs = Vec::new();\n+        let mut dest_ptrs_box = None;\n         if src.size >= ptr_size {\n             let adjusted_end = Size::from_bytes(src.end().bytes() - (ptr_size.bytes() - 1));\n             let ptrs = self.ptrs.range(src.start..adjusted_end);\n-            dest_ptrs.reserve_exact(ptrs.len() * (count as usize));\n             // If `count` is large, this is rather wasteful -- we are allocating a big array here, which\n             // is mostly filled with redundant information since it's just N copies of the same `Prov`s\n             // at slightly adjusted offsets. The reason we do this is so that in `mark_provenance_range`\n             // we can use `insert_presorted`. That wouldn't work with an `Iterator` that just produces\n             // the right sequence of provenance for all N copies.\n             // Basically, this large array would have to be created anyway in the target allocation.\n+            let mut dest_ptrs = Vec::with_capacity(ptrs.len() * (count as usize));\n             for i in 0..count {\n                 dest_ptrs\n                     .extend(ptrs.iter().map(|&(offset, reloc)| (shift_offset(i, offset), reloc)));\n             }\n+            debug_assert_eq!(dest_ptrs.len(), dest_ptrs.capacity());\n+            dest_ptrs_box = Some(dest_ptrs.into_boxed_slice());\n         };\n \n         // # Byte-sized provenances\n-        let mut bytes = Vec::new();\n-        // First, if there is a part of a pointer at the start, add that.\n-        if let Some(entry) = self.range_get_ptrs(alloc_range(src.start, Size::ZERO), cx).first() {\n-            if !Prov::OFFSET_IS_ADDR {\n-                // We can't split up the provenance into less than a pointer.\n+        // This includes the existing bytewise provenance in the range, and ptr provenance\n+        // that overlaps with the begin/end of the range.\n+        let mut dest_bytes_box = None;\n+        let begin_overlap = self.range_get_ptrs(alloc_range(src.start, Size::ZERO), cx).first();\n+        let end_overlap = self.range_get_ptrs(alloc_range(src.end(), Size::ZERO), cx).first();\n+        if !Prov::OFFSET_IS_ADDR {\n+            // There can't be any bytewise provenance, and we cannot split up the begin/end overlap.\n+            if let Some(entry) = begin_overlap {\n                 return Err(AllocError::PartialPointerCopy(entry.0));\n             }\n-            trace!(\"start overlapping entry: {entry:?}\");\n-            // For really small copies, make sure we don't run off the end of the `src` range.\n-            let entry_end = cmp::min(entry.0 + ptr_size, src.end());\n-            for offset in src.start..entry_end {\n-                bytes.push((offset, entry.1));\n-            }\n-        } else {\n-            trace!(\"no start overlapping entry\");\n-        }\n-        // Then the main part, bytewise provenance from `self.bytes`.\n-        if Prov::OFFSET_IS_ADDR {\n-            bytes.extend(self.bytes.range(src.start..src.end()));\n-        } else {\n-            debug_assert!(self.bytes.is_empty());\n-        }\n-        // And finally possibly parts of a pointer at the end.\n-        if let Some(entry) = self.range_get_ptrs(alloc_range(src.end(), Size::ZERO), cx).first() {\n-            if !Prov::OFFSET_IS_ADDR {\n-                // We can't split up the provenance into less than a pointer.\n+            if let Some(entry) = end_overlap {\n                 return Err(AllocError::PartialPointerCopy(entry.0));\n             }\n-            trace!(\"end overlapping entry: {entry:?}\");\n-            // For really small copies, make sure we don't start before `src` does.\n-            let entry_start = cmp::max(entry.0, src.start);\n-            for offset in entry_start..src.end() {\n-                if bytes.last().map_or(true, |bytes_entry| bytes_entry.0 < offset) {\n-                    // The last entry, if it exists, has a lower offset than us.\n+            debug_assert!(self.bytes.is_none());\n+        } else {\n+            let mut bytes = Vec::new();\n+            // First, if there is a part of a pointer at the start, add that.\n+            if let Some(entry) = begin_overlap {\n+                trace!(\"start overlapping entry: {entry:?}\");\n+                // For really small copies, make sure we don't run off the end of the `src` range.\n+                let entry_end = cmp::min(entry.0 + ptr_size, src.end());\n+                for offset in src.start..entry_end {\n                     bytes.push((offset, entry.1));\n-                } else {\n-                    // There already is an entry for this offset in there! This can happen when the\n-                    // start and end range checks actually end up hitting the same pointer, so we\n-                    // already added this in the \"pointer at the start\" part above.\n-                    assert!(entry.0 <= src.start);\n                 }\n+            } else {\n+                trace!(\"no start overlapping entry\");\n             }\n-        } else {\n-            trace!(\"no end overlapping entry\");\n-        }\n-        trace!(\"byte provenances: {bytes:?}\");\n+            // Then the main part, bytewise provenance from `self.bytes`.\n+            if let Some(all_bytes) = self.bytes.as_ref() {\n+                bytes.extend(all_bytes.range(src.start..src.end()));\n+            }\n+            // And finally possibly parts of a pointer at the end.\n+            if let Some(entry) = end_overlap {\n+                trace!(\"end overlapping entry: {entry:?}\");\n+                // For really small copies, make sure we don't start before `src` does.\n+                let entry_start = cmp::max(entry.0, src.start);\n+                for offset in entry_start..src.end() {\n+                    if bytes.last().map_or(true, |bytes_entry| bytes_entry.0 < offset) {\n+                        // The last entry, if it exists, has a lower offset than us.\n+                        bytes.push((offset, entry.1));\n+                    } else {\n+                        // There already is an entry for this offset in there! This can happen when the\n+                        // start and end range checks actually end up hitting the same pointer, so we\n+                        // already added this in the \"pointer at the start\" part above.\n+                        assert!(entry.0 <= src.start);\n+                    }\n+                }\n+            } else {\n+                trace!(\"no end overlapping entry\");\n+            }\n+            trace!(\"byte provenances: {bytes:?}\");\n \n-        // And again a buffer for the new list on the target side.\n-        let mut dest_bytes = Vec::new();\n-        if Prov::OFFSET_IS_ADDR {\n-            dest_bytes.reserve_exact(bytes.len() * (count as usize));\n+            // And again a buffer for the new list on the target side.\n+            let mut dest_bytes = Vec::with_capacity(bytes.len() * (count as usize));\n             for i in 0..count {\n                 dest_bytes\n                     .extend(bytes.iter().map(|&(offset, reloc)| (shift_offset(i, offset), reloc)));\n             }\n-        } else {\n-            // There can't be any bytewise provenance when OFFSET_IS_ADDR is false.\n-            debug_assert!(bytes.is_empty());\n+            debug_assert_eq!(dest_bytes.len(), dest_bytes.capacity());\n+            dest_bytes_box = Some(dest_bytes.into_boxed_slice());\n         }\n \n-        Ok(ProvenanceCopy { dest_ptrs, dest_bytes })\n+        Ok(ProvenanceCopy { dest_ptrs: dest_ptrs_box, dest_bytes: dest_bytes_box })\n     }\n \n     /// Applies a provenance copy.\n     /// The affected range, as defined in the parameters to `prepare_copy` is expected\n     /// to be clear of provenance.\n     pub fn apply_copy(&mut self, copy: ProvenanceCopy<Prov>) {\n-        self.ptrs.insert_presorted(copy.dest_ptrs);\n+        if let Some(dest_ptrs) = copy.dest_ptrs {\n+            self.ptrs.insert_presorted(dest_ptrs.into());\n+        }\n         if Prov::OFFSET_IS_ADDR {\n-            self.bytes.insert_presorted(copy.dest_bytes);\n+            if let Some(dest_bytes) = copy.dest_bytes && !dest_bytes.is_empty() {\n+                self.bytes.get_or_insert_with(Box::default).insert_presorted(dest_bytes.into());\n+            }\n         } else {\n-            debug_assert!(copy.dest_bytes.is_empty());\n+            debug_assert!(copy.dest_bytes.is_none());\n         }\n     }\n }"}]}