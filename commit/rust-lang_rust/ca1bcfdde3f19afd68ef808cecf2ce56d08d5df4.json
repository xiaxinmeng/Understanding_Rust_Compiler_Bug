{"sha": "ca1bcfdde3f19afd68ef808cecf2ce56d08d5df4", "node_id": "MDY6Q29tbWl0NzI0NzEyOmNhMWJjZmRkZTNmMTlhZmQ2OGVmODA4Y2VjZjJjZTU2ZDA4ZDVkZjQ=", "commit": {"author": {"name": "bors", "email": "bors@rust-lang.org", "date": "2019-06-07T06:52:09Z"}, "committer": {"name": "bors", "email": "bors@rust-lang.org", "date": "2019-06-07T06:52:09Z"}, "message": "Auto merge of #61541 - petrochenkov:tsp, r=oli-obk\n\nsyntax: Keep token span as a part of `Token`\n\nIn the world with proc macros and edition hygiene `Token` without a span is not self-contained.\nIn practice this means that tokens and spans are always stored and passed somewhere along with each other.\nThis PR combines them into a single struct by doing the next renaming/replacement:\n\n- `Token` -> `TokenKind`\n- `TokenAndSpan` -> `Token`\n- `(Token, Span)` -> `Token`\n\nSome later commits (https://github.com/rust-lang/rust/commit/fb6e2fe8fd6caed247857758c6c3549fe2b59527 and https://github.com/rust-lang/rust/commit/1cdee86940db892cd17239c26add5364335e895a) remove duplicate spans in `token::Ident` and `token::Lifetime`.\nThose spans were supposed to be identical to token spans, but could easily go out of sync, as was noticed in https://github.com/rust-lang/rust/pull/60965#discussion_r285398523.\nThe `(Token, Span)` -> `Token` change is a soft pre-requisite for this de-duplication since it allows to avoid some larger churn (passing spans to most of functions classifying identifiers).", "tree": {"sha": "07a0d2ef9340fa064341cc697a8ae58e3762373a", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/07a0d2ef9340fa064341cc697a8ae58e3762373a"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/ca1bcfdde3f19afd68ef808cecf2ce56d08d5df4", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/ca1bcfdde3f19afd68ef808cecf2ce56d08d5df4", "html_url": "https://github.com/rust-lang/rust/commit/ca1bcfdde3f19afd68ef808cecf2ce56d08d5df4", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/ca1bcfdde3f19afd68ef808cecf2ce56d08d5df4/comments", "author": {"login": "bors", "id": 3372342, "node_id": "MDQ6VXNlcjMzNzIzNDI=", "avatar_url": "https://avatars.githubusercontent.com/u/3372342?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bors", "html_url": "https://github.com/bors", "followers_url": "https://api.github.com/users/bors/followers", "following_url": "https://api.github.com/users/bors/following{/other_user}", "gists_url": "https://api.github.com/users/bors/gists{/gist_id}", "starred_url": "https://api.github.com/users/bors/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bors/subscriptions", "organizations_url": "https://api.github.com/users/bors/orgs", "repos_url": "https://api.github.com/users/bors/repos", "events_url": "https://api.github.com/users/bors/events{/privacy}", "received_events_url": "https://api.github.com/users/bors/received_events", "type": "User", "site_admin": false}, "committer": {"login": "bors", "id": 3372342, "node_id": "MDQ6VXNlcjMzNzIzNDI=", "avatar_url": "https://avatars.githubusercontent.com/u/3372342?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bors", "html_url": "https://github.com/bors", "followers_url": "https://api.github.com/users/bors/followers", "following_url": "https://api.github.com/users/bors/following{/other_user}", "gists_url": "https://api.github.com/users/bors/gists{/gist_id}", "starred_url": "https://api.github.com/users/bors/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bors/subscriptions", "organizations_url": "https://api.github.com/users/bors/orgs", "repos_url": "https://api.github.com/users/bors/repos", "events_url": "https://api.github.com/users/bors/events{/privacy}", "received_events_url": "https://api.github.com/users/bors/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "c5295ac64a8f2c7aee9cdd13b8fe00b82aff8435", "url": "https://api.github.com/repos/rust-lang/rust/commits/c5295ac64a8f2c7aee9cdd13b8fe00b82aff8435", "html_url": "https://github.com/rust-lang/rust/commit/c5295ac64a8f2c7aee9cdd13b8fe00b82aff8435"}, {"sha": "3a31f0634bb1669eae64e83f595942986f867125", "url": "https://api.github.com/repos/rust-lang/rust/commits/3a31f0634bb1669eae64e83f595942986f867125", "html_url": "https://github.com/rust-lang/rust/commit/3a31f0634bb1669eae64e83f595942986f867125"}], "stats": {"total": 1937, "additions": 978, "deletions": 959}, "files": [{"sha": "1994cf491889bddd82e384f8a8aaa25504a6662e", "filename": "src/doc/unstable-book/src/language-features/plugin.md", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "blob_url": "https://github.com/rust-lang/rust/blob/ca1bcfdde3f19afd68ef808cecf2ce56d08d5df4/src%2Fdoc%2Funstable-book%2Fsrc%2Flanguage-features%2Fplugin.md", "raw_url": "https://github.com/rust-lang/rust/raw/ca1bcfdde3f19afd68ef808cecf2ce56d08d5df4/src%2Fdoc%2Funstable-book%2Fsrc%2Flanguage-features%2Fplugin.md", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Fdoc%2Funstable-book%2Fsrc%2Flanguage-features%2Fplugin.md?ref=ca1bcfdde3f19afd68ef808cecf2ce56d08d5df4", "patch": "@@ -56,15 +56,15 @@ extern crate syntax_pos;\n extern crate rustc;\n extern crate rustc_plugin;\n \n-use syntax::parse::token;\n+use syntax::parse::token::{self, Token};\n use syntax::tokenstream::TokenTree;\n use syntax::ext::base::{ExtCtxt, MacResult, DummyResult, MacEager};\n use syntax::ext::build::AstBuilder;  // A trait for expr_usize.\n use syntax_pos::Span;\n use rustc_plugin::Registry;\n \n fn expand_rn(cx: &mut ExtCtxt, sp: Span, args: &[TokenTree])\n-        -> Box<MacResult + 'static> {\n+        -> Box<dyn MacResult + 'static> {\n \n     static NUMERALS: &'static [(&'static str, usize)] = &[\n         (\"M\", 1000), (\"CM\", 900), (\"D\", 500), (\"CD\", 400),\n@@ -80,7 +80,7 @@ fn expand_rn(cx: &mut ExtCtxt, sp: Span, args: &[TokenTree])\n     }\n \n     let text = match args[0] {\n-        TokenTree::Token(_, token::Ident(s)) => s.to_string(),\n+        TokenTree::Token(Token { kind: token::Ident(s, _), .. }) => s.to_string(),\n         _ => {\n             cx.span_err(sp, \"argument should be a single identifier\");\n             return DummyResult::any(sp);"}, {"sha": "e7f52b48cb9ede5018ad0860123c722138c1fb58", "filename": "src/librustc/hir/lowering.rs", "status": "modified", "additions": 7, "deletions": 7, "changes": 14, "blob_url": "https://github.com/rust-lang/rust/blob/ca1bcfdde3f19afd68ef808cecf2ce56d08d5df4/src%2Flibrustc%2Fhir%2Flowering.rs", "raw_url": "https://github.com/rust-lang/rust/raw/ca1bcfdde3f19afd68ef808cecf2ce56d08d5df4/src%2Flibrustc%2Fhir%2Flowering.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Fhir%2Flowering.rs?ref=ca1bcfdde3f19afd68ef808cecf2ce56d08d5df4", "patch": "@@ -67,7 +67,7 @@ use syntax::source_map::CompilerDesugaringKind::IfTemporary;\n use syntax::std_inject;\n use syntax::symbol::{kw, sym, Symbol};\n use syntax::tokenstream::{TokenStream, TokenTree};\n-use syntax::parse::token::Token;\n+use syntax::parse::token::{self, Token};\n use syntax::visit::{self, Visitor};\n use syntax_pos::{DUMMY_SP, edition, Span};\n \n@@ -1328,7 +1328,7 @@ impl<'a> LoweringContext<'a> {\n \n     fn lower_token_tree(&mut self, tree: TokenTree) -> TokenStream {\n         match tree {\n-            TokenTree::Token(span, token) => self.lower_token(token, span),\n+            TokenTree::Token(token) => self.lower_token(token),\n             TokenTree::Delimited(span, delim, tts) => TokenTree::Delimited(\n                 span,\n                 delim,\n@@ -1337,13 +1337,13 @@ impl<'a> LoweringContext<'a> {\n         }\n     }\n \n-    fn lower_token(&mut self, token: Token, span: Span) -> TokenStream {\n-        match token {\n-            Token::Interpolated(nt) => {\n-                let tts = nt.to_tokenstream(&self.sess.parse_sess, span);\n+    fn lower_token(&mut self, token: Token) -> TokenStream {\n+        match token.kind {\n+            token::Interpolated(nt) => {\n+                let tts = nt.to_tokenstream(&self.sess.parse_sess, token.span);\n                 self.lower_token_stream(tts)\n             }\n-            other => TokenTree::Token(span, other).into(),\n+            _ => TokenTree::Token(token).into(),\n         }\n     }\n "}, {"sha": "41073773e9f9bb246571ecc108695566fd4bcdd2", "filename": "src/librustc/hir/map/def_collector.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/ca1bcfdde3f19afd68ef808cecf2ce56d08d5df4/src%2Flibrustc%2Fhir%2Fmap%2Fdef_collector.rs", "raw_url": "https://github.com/rust-lang/rust/raw/ca1bcfdde3f19afd68ef808cecf2ce56d08d5df4/src%2Flibrustc%2Fhir%2Fmap%2Fdef_collector.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Fhir%2Fmap%2Fdef_collector.rs?ref=ca1bcfdde3f19afd68ef808cecf2ce56d08d5df4", "patch": "@@ -326,7 +326,7 @@ impl<'a> visit::Visitor<'a> for DefCollector<'a> {\n     }\n \n     fn visit_token(&mut self, t: Token) {\n-        if let Token::Interpolated(nt) = t {\n+        if let token::Interpolated(nt) = t.kind {\n             if let token::NtExpr(ref expr) = *nt {\n                 if let ExprKind::Mac(..) = expr.node {\n                     self.visit_macro_invoc(expr.id);"}, {"sha": "abe4196abd19cc930dc6a1220fe88d7e416013e1", "filename": "src/librustc/ich/impls_syntax.rs", "status": "modified", "additions": 64, "deletions": 60, "changes": 124, "blob_url": "https://github.com/rust-lang/rust/blob/ca1bcfdde3f19afd68ef808cecf2ce56d08d5df4/src%2Flibrustc%2Fich%2Fimpls_syntax.rs", "raw_url": "https://github.com/rust-lang/rust/raw/ca1bcfdde3f19afd68ef808cecf2ce56d08d5df4/src%2Flibrustc%2Fich%2Fimpls_syntax.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Fich%2Fimpls_syntax.rs?ref=ca1bcfdde3f19afd68ef808cecf2ce56d08d5df4", "patch": "@@ -261,9 +261,8 @@ for tokenstream::TokenTree {\n                                           hasher: &mut StableHasher<W>) {\n         mem::discriminant(self).hash_stable(hcx, hasher);\n         match *self {\n-            tokenstream::TokenTree::Token(span, ref token) => {\n-                span.hash_stable(hcx, hasher);\n-                hash_token(token, hcx, hasher);\n+            tokenstream::TokenTree::Token(ref token) => {\n+                token.hash_stable(hcx, hasher);\n             }\n             tokenstream::TokenTree::Delimited(span, delim, ref tts) => {\n                 span.hash_stable(hcx, hasher);\n@@ -306,70 +305,75 @@ impl_stable_hash_for!(struct token::Lit {\n     suffix\n });\n \n-fn hash_token<'a, 'gcx, W: StableHasherResult>(\n-    token: &token::Token,\n-    hcx: &mut StableHashingContext<'a>,\n-    hasher: &mut StableHasher<W>,\n-) {\n-    mem::discriminant(token).hash_stable(hcx, hasher);\n-    match *token {\n-        token::Token::Eq |\n-        token::Token::Lt |\n-        token::Token::Le |\n-        token::Token::EqEq |\n-        token::Token::Ne |\n-        token::Token::Ge |\n-        token::Token::Gt |\n-        token::Token::AndAnd |\n-        token::Token::OrOr |\n-        token::Token::Not |\n-        token::Token::Tilde |\n-        token::Token::At |\n-        token::Token::Dot |\n-        token::Token::DotDot |\n-        token::Token::DotDotDot |\n-        token::Token::DotDotEq |\n-        token::Token::Comma |\n-        token::Token::Semi |\n-        token::Token::Colon |\n-        token::Token::ModSep |\n-        token::Token::RArrow |\n-        token::Token::LArrow |\n-        token::Token::FatArrow |\n-        token::Token::Pound |\n-        token::Token::Dollar |\n-        token::Token::Question |\n-        token::Token::SingleQuote |\n-        token::Token::Whitespace |\n-        token::Token::Comment |\n-        token::Token::Eof => {}\n-\n-        token::Token::BinOp(bin_op_token) |\n-        token::Token::BinOpEq(bin_op_token) => {\n-            std_hash::Hash::hash(&bin_op_token, hasher);\n-        }\n+impl<'a> HashStable<StableHashingContext<'a>> for token::TokenKind {\n+    fn hash_stable<W: StableHasherResult>(&self,\n+                                          hcx: &mut StableHashingContext<'a>,\n+                                          hasher: &mut StableHasher<W>) {\n+        mem::discriminant(self).hash_stable(hcx, hasher);\n+        match *self {\n+            token::Eq |\n+            token::Lt |\n+            token::Le |\n+            token::EqEq |\n+            token::Ne |\n+            token::Ge |\n+            token::Gt |\n+            token::AndAnd |\n+            token::OrOr |\n+            token::Not |\n+            token::Tilde |\n+            token::At |\n+            token::Dot |\n+            token::DotDot |\n+            token::DotDotDot |\n+            token::DotDotEq |\n+            token::Comma |\n+            token::Semi |\n+            token::Colon |\n+            token::ModSep |\n+            token::RArrow |\n+            token::LArrow |\n+            token::FatArrow |\n+            token::Pound |\n+            token::Dollar |\n+            token::Question |\n+            token::SingleQuote |\n+            token::Whitespace |\n+            token::Comment |\n+            token::Eof => {}\n+\n+            token::BinOp(bin_op_token) |\n+            token::BinOpEq(bin_op_token) => {\n+                std_hash::Hash::hash(&bin_op_token, hasher);\n+            }\n \n-        token::Token::OpenDelim(delim_token) |\n-        token::Token::CloseDelim(delim_token) => {\n-            std_hash::Hash::hash(&delim_token, hasher);\n-        }\n-        token::Token::Literal(lit) => lit.hash_stable(hcx, hasher),\n+            token::OpenDelim(delim_token) |\n+            token::CloseDelim(delim_token) => {\n+                std_hash::Hash::hash(&delim_token, hasher);\n+            }\n+            token::Literal(lit) => lit.hash_stable(hcx, hasher),\n \n-        token::Token::Ident(ident, is_raw) => {\n-            ident.name.hash_stable(hcx, hasher);\n-            is_raw.hash_stable(hcx, hasher);\n-        }\n-        token::Token::Lifetime(ident) => ident.name.hash_stable(hcx, hasher),\n+            token::Ident(name, is_raw) => {\n+                name.hash_stable(hcx, hasher);\n+                is_raw.hash_stable(hcx, hasher);\n+            }\n+            token::Lifetime(name) => name.hash_stable(hcx, hasher),\n \n-        token::Token::Interpolated(_) => {\n-            bug!(\"interpolated tokens should not be present in the HIR\")\n-        }\n+            token::Interpolated(_) => {\n+                bug!(\"interpolated tokens should not be present in the HIR\")\n+            }\n \n-        token::Token::DocComment(val) |\n-        token::Token::Shebang(val) => val.hash_stable(hcx, hasher),\n+            token::DocComment(val) |\n+            token::Shebang(val) => val.hash_stable(hcx, hasher),\n+        }\n     }\n }\n \n+impl_stable_hash_for!(struct token::Token {\n+    kind,\n+    span\n+});\n+\n impl_stable_hash_for!(enum ::syntax::ast::NestedMetaItem {\n     MetaItem(meta_item),\n     Literal(lit)"}, {"sha": "6e4d0e881f76bffad08b2763c21f5db35b6b04fe", "filename": "src/librustc_lint/builtin.rs", "status": "modified", "additions": 3, "deletions": 9, "changes": 12, "blob_url": "https://github.com/rust-lang/rust/blob/ca1bcfdde3f19afd68ef808cecf2ce56d08d5df4/src%2Flibrustc_lint%2Fbuiltin.rs", "raw_url": "https://github.com/rust-lang/rust/raw/ca1bcfdde3f19afd68ef808cecf2ce56d08d5df4/src%2Flibrustc_lint%2Fbuiltin.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_lint%2Fbuiltin.rs?ref=ca1bcfdde3f19afd68ef808cecf2ce56d08d5df4", "patch": "@@ -1414,15 +1414,9 @@ impl KeywordIdents {\n     fn check_tokens(&mut self, cx: &EarlyContext<'_>, tokens: TokenStream) {\n         for tt in tokens.into_trees() {\n             match tt {\n-                TokenTree::Token(span, tok) => match tok.ident() {\n-                    // only report non-raw idents\n-                    Some((ident, false)) => {\n-                        self.check_ident_token(cx, UnderMacro(true), ast::Ident {\n-                            span: span.substitute_dummy(ident.span),\n-                            ..ident\n-                        });\n-                    }\n-                    _ => {},\n+                // Only report non-raw idents.\n+                TokenTree::Token(token) => if let Some((ident, false)) = token.ident() {\n+                    self.check_ident_token(cx, UnderMacro(true), ident);\n                 }\n                 TokenTree::Delimited(_, _, tts) => {\n                     self.check_tokens(cx, tts)"}, {"sha": "6d0b142fb2409e713d21ccd7b5789144797bfb64", "filename": "src/librustc_resolve/build_reduced_graph.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/ca1bcfdde3f19afd68ef808cecf2ce56d08d5df4/src%2Flibrustc_resolve%2Fbuild_reduced_graph.rs", "raw_url": "https://github.com/rust-lang/rust/raw/ca1bcfdde3f19afd68ef808cecf2ce56d08d5df4/src%2Flibrustc_resolve%2Fbuild_reduced_graph.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_resolve%2Fbuild_reduced_graph.rs?ref=ca1bcfdde3f19afd68ef808cecf2ce56d08d5df4", "patch": "@@ -1053,7 +1053,7 @@ impl<'a, 'b> Visitor<'a> for BuildReducedGraphVisitor<'a, 'b> {\n     }\n \n     fn visit_token(&mut self, t: Token) {\n-        if let Token::Interpolated(nt) = t {\n+        if let token::Interpolated(nt) = t.kind {\n             if let token::NtExpr(ref expr) = *nt {\n                 if let ast::ExprKind::Mac(..) = expr.node {\n                     self.visit_invoc(expr.id);"}, {"sha": "5831b0bcd8fa37a65e8cdb307dc13d611d227d5d", "filename": "src/librustc_save_analysis/span_utils.rs", "status": "modified", "additions": 11, "deletions": 11, "changes": 22, "blob_url": "https://github.com/rust-lang/rust/blob/ca1bcfdde3f19afd68ef808cecf2ce56d08d5df4/src%2Flibrustc_save_analysis%2Fspan_utils.rs", "raw_url": "https://github.com/rust-lang/rust/raw/ca1bcfdde3f19afd68ef808cecf2ce56d08d5df4/src%2Flibrustc_save_analysis%2Fspan_utils.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_save_analysis%2Fspan_utils.rs?ref=ca1bcfdde3f19afd68ef808cecf2ce56d08d5df4", "patch": "@@ -5,7 +5,7 @@ use crate::generated_code;\n use std::cell::Cell;\n \n use syntax::parse::lexer::{self, StringReader};\n-use syntax::parse::token::{self, Token};\n+use syntax::parse::token::{self, TokenKind};\n use syntax_pos::*;\n \n #[derive(Clone)]\n@@ -56,15 +56,15 @@ impl<'a> SpanUtils<'a> {\n         lexer::StringReader::retokenize(&self.sess.parse_sess, span)\n     }\n \n-    pub fn sub_span_of_token(&self, span: Span, tok: Token) -> Option<Span> {\n+    pub fn sub_span_of_token(&self, span: Span, tok: TokenKind) -> Option<Span> {\n         let mut toks = self.retokenise_span(span);\n         loop {\n             let next = toks.real_token();\n-            if next.tok == token::Eof {\n+            if next == token::Eof {\n                 return None;\n             }\n-            if next.tok == tok {\n-                return Some(next.sp);\n+            if next == tok {\n+                return Some(next.span);\n             }\n         }\n     }\n@@ -74,12 +74,12 @@ impl<'a> SpanUtils<'a> {\n     //     let mut toks = self.retokenise_span(span);\n     //     loop {\n     //         let ts = toks.real_token();\n-    //         if ts.tok == token::Eof {\n+    //         if ts == token::Eof {\n     //             return None;\n     //         }\n-    //         if ts.tok == token::Not {\n+    //         if ts == token::Not {\n     //             let ts = toks.real_token();\n-    //             if ts.tok.is_ident() {\n+    //             if ts.kind.is_ident() {\n     //                 return Some(ts.sp);\n     //             } else {\n     //                 return None;\n@@ -93,12 +93,12 @@ impl<'a> SpanUtils<'a> {\n     //     let mut toks = self.retokenise_span(span);\n     //     let mut prev = toks.real_token();\n     //     loop {\n-    //         if prev.tok == token::Eof {\n+    //         if prev == token::Eof {\n     //             return None;\n     //         }\n     //         let ts = toks.real_token();\n-    //         if ts.tok == token::Not {\n-    //             if prev.tok.is_ident() {\n+    //         if ts == token::Not {\n+    //             if prev.kind.is_ident() {\n     //                 return Some(prev.sp);\n     //             } else {\n     //                 return None;"}, {"sha": "281bd72deeb805acc4d01ff5f2c0ae0190567127", "filename": "src/librustdoc/html/highlight.rs", "status": "modified", "additions": 17, "deletions": 17, "changes": 34, "blob_url": "https://github.com/rust-lang/rust/blob/ca1bcfdde3f19afd68ef808cecf2ce56d08d5df4/src%2Flibrustdoc%2Fhtml%2Fhighlight.rs", "raw_url": "https://github.com/rust-lang/rust/raw/ca1bcfdde3f19afd68ef808cecf2ce56d08d5df4/src%2Flibrustdoc%2Fhtml%2Fhighlight.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustdoc%2Fhtml%2Fhighlight.rs?ref=ca1bcfdde3f19afd68ef808cecf2ce56d08d5df4", "patch": "@@ -12,8 +12,8 @@ use std::io;\n use std::io::prelude::*;\n \n use syntax::source_map::{SourceMap, FilePathMapping};\n-use syntax::parse::lexer::{self, TokenAndSpan};\n-use syntax::parse::token;\n+use syntax::parse::lexer;\n+use syntax::parse::token::{self, Token};\n use syntax::parse;\n use syntax::symbol::{kw, sym};\n use syntax_pos::{Span, FileName};\n@@ -186,9 +186,9 @@ impl<'a> Classifier<'a> {\n     }\n \n     /// Gets the next token out of the lexer.\n-    fn try_next_token(&mut self) -> Result<TokenAndSpan, HighlightError> {\n+    fn try_next_token(&mut self) -> Result<Token, HighlightError> {\n         match self.lexer.try_next_token() {\n-            Ok(tas) => Ok(tas),\n+            Ok(token) => Ok(token),\n             Err(_) => Err(HighlightError::LexError),\n         }\n     }\n@@ -205,7 +205,7 @@ impl<'a> Classifier<'a> {\n                                    -> Result<(), HighlightError> {\n         loop {\n             let next = self.try_next_token()?;\n-            if next.tok == token::Eof {\n+            if next == token::Eof {\n                 break;\n             }\n \n@@ -218,9 +218,9 @@ impl<'a> Classifier<'a> {\n     // Handles an individual token from the lexer.\n     fn write_token<W: Writer>(&mut self,\n                               out: &mut W,\n-                              tas: TokenAndSpan)\n+                              token: Token)\n                               -> Result<(), HighlightError> {\n-        let klass = match tas.tok {\n+        let klass = match token.kind {\n             token::Shebang(s) => {\n                 out.string(Escape(&s.as_str()), Class::None)?;\n                 return Ok(());\n@@ -234,7 +234,7 @@ impl<'a> Classifier<'a> {\n             // reference or dereference operator or a reference or pointer type, instead of the\n             // bit-and or multiplication operator.\n             token::BinOp(token::And) | token::BinOp(token::Star)\n-                if self.lexer.peek().tok != token::Whitespace => Class::RefKeyWord,\n+                if self.lexer.peek() != &token::Whitespace => Class::RefKeyWord,\n \n             // Consider this as part of a macro invocation if there was a\n             // leading identifier.\n@@ -257,7 +257,7 @@ impl<'a> Classifier<'a> {\n             token::Question => Class::QuestionMark,\n \n             token::Dollar => {\n-                if self.lexer.peek().tok.is_ident() {\n+                if self.lexer.peek().kind.is_ident() {\n                     self.in_macro_nonterminal = true;\n                     Class::MacroNonTerminal\n                 } else {\n@@ -280,9 +280,9 @@ impl<'a> Classifier<'a> {\n                 // as an attribute.\n \n                 // Case 1: #![inner_attribute]\n-                if self.lexer.peek().tok == token::Not {\n+                if self.lexer.peek() == &token::Not {\n                     self.try_next_token()?; // NOTE: consumes `!` token!\n-                    if self.lexer.peek().tok == token::OpenDelim(token::Bracket) {\n+                    if self.lexer.peek() == &token::OpenDelim(token::Bracket) {\n                         self.in_attribute = true;\n                         out.enter_span(Class::Attribute)?;\n                     }\n@@ -292,7 +292,7 @@ impl<'a> Classifier<'a> {\n                 }\n \n                 // Case 2: #[outer_attribute]\n-                if self.lexer.peek().tok == token::OpenDelim(token::Bracket) {\n+                if self.lexer.peek() == &token::OpenDelim(token::Bracket) {\n                     self.in_attribute = true;\n                     out.enter_span(Class::Attribute)?;\n                 }\n@@ -325,8 +325,8 @@ impl<'a> Classifier<'a> {\n             }\n \n             // Keywords are also included in the identifier set.\n-            token::Ident(ident, is_raw) => {\n-                match ident.name {\n+            token::Ident(name, is_raw) => {\n+                match name {\n                     kw::Ref | kw::Mut if !is_raw => Class::RefKeyWord,\n \n                     kw::SelfLower | kw::SelfUpper => Class::Self_,\n@@ -335,13 +335,13 @@ impl<'a> Classifier<'a> {\n                     sym::Option | sym::Result => Class::PreludeTy,\n                     sym::Some | sym::None | sym::Ok | sym::Err => Class::PreludeVal,\n \n-                    _ if tas.tok.is_reserved_ident() => Class::KeyWord,\n+                    _ if token.is_reserved_ident() => Class::KeyWord,\n \n                     _ => {\n                         if self.in_macro_nonterminal {\n                             self.in_macro_nonterminal = false;\n                             Class::MacroNonTerminal\n-                        } else if self.lexer.peek().tok == token::Not {\n+                        } else if self.lexer.peek() == &token::Not {\n                             self.in_macro = true;\n                             Class::Macro\n                         } else {\n@@ -359,7 +359,7 @@ impl<'a> Classifier<'a> {\n \n         // Anything that didn't return above is the simple case where we the\n         // class just spans a single token, so we can use the `string` method.\n-        out.string(Escape(&self.snip(tas.sp)), klass)?;\n+        out.string(Escape(&self.snip(token.span)), klass)?;\n \n         Ok(())\n     }"}, {"sha": "694843ad7f71e1accb2f1ae3ad2d62c8a2ded3c1", "filename": "src/librustdoc/passes/check_code_block_syntax.rs", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "blob_url": "https://github.com/rust-lang/rust/blob/ca1bcfdde3f19afd68ef808cecf2ce56d08d5df4/src%2Flibrustdoc%2Fpasses%2Fcheck_code_block_syntax.rs", "raw_url": "https://github.com/rust-lang/rust/raw/ca1bcfdde3f19afd68ef808cecf2ce56d08d5df4/src%2Flibrustdoc%2Fpasses%2Fcheck_code_block_syntax.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustdoc%2Fpasses%2Fcheck_code_block_syntax.rs?ref=ca1bcfdde3f19afd68ef808cecf2ce56d08d5df4", "patch": "@@ -1,5 +1,5 @@\n use errors::Applicability;\n-use syntax::parse::lexer::{TokenAndSpan, StringReader as Lexer};\n+use syntax::parse::lexer::{StringReader as Lexer};\n use syntax::parse::{ParseSess, token};\n use syntax::source_map::FilePathMapping;\n use syntax_pos::FileName;\n@@ -33,8 +33,8 @@ impl<'a, 'tcx> SyntaxChecker<'a, 'tcx> {\n         );\n \n         let errors = Lexer::new_or_buffered_errs(&sess, source_file, None).and_then(|mut lexer| {\n-            while let Ok(TokenAndSpan { tok, .. }) = lexer.try_next_token() {\n-                if tok == token::Eof {\n+            while let Ok(token::Token { kind, .. }) = lexer.try_next_token() {\n+                if kind == token::Eof {\n                     break;\n                 }\n             }"}, {"sha": "edfe097c72f615c6a3340488b9db514d59a38848", "filename": "src/libsyntax/attr/mod.rs", "status": "modified", "additions": 25, "deletions": 23, "changes": 48, "blob_url": "https://github.com/rust-lang/rust/blob/ca1bcfdde3f19afd68ef808cecf2ce56d08d5df4/src%2Flibsyntax%2Fattr%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/ca1bcfdde3f19afd68ef808cecf2ce56d08d5df4/src%2Flibsyntax%2Fattr%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fattr%2Fmod.rs?ref=ca1bcfdde3f19afd68ef808cecf2ce56d08d5df4", "patch": "@@ -20,7 +20,7 @@ use crate::source_map::{BytePos, Spanned, dummy_spanned};\n use crate::parse::lexer::comments::{doc_comment_style, strip_doc_comment_decoration};\n use crate::parse::parser::Parser;\n use crate::parse::{self, ParseSess, PResult};\n-use crate::parse::token::{self, Token};\n+use crate::parse::token::{self, Token, TokenKind};\n use crate::ptr::P;\n use crate::symbol::{sym, Symbol};\n use crate::ThinVec;\n@@ -465,10 +465,10 @@ impl MetaItem {\n                 let mod_sep_span = Span::new(last_pos,\n                                              segment.ident.span.lo(),\n                                              segment.ident.span.ctxt());\n-                idents.push(TokenTree::Token(mod_sep_span, Token::ModSep).into());\n+                idents.push(TokenTree::token(token::ModSep, mod_sep_span).into());\n             }\n-            idents.push(TokenTree::Token(segment.ident.span,\n-                                         Token::from_ast_ident(segment.ident)).into());\n+            idents.push(TokenTree::token(TokenKind::from_ast_ident(segment.ident),\n+                                         segment.ident.span).into());\n             last_pos = segment.ident.span.hi();\n         }\n         self.node.tokens(self.span).append_to_tree_and_joint_vec(&mut idents);\n@@ -480,26 +480,28 @@ impl MetaItem {\n     {\n         // FIXME: Share code with `parse_path`.\n         let path = match tokens.next() {\n-            Some(TokenTree::Token(span, token @ Token::Ident(..))) |\n-            Some(TokenTree::Token(span, token @ Token::ModSep)) => 'arm: {\n-                let mut segments = if let Token::Ident(ident, _) = token {\n-                    if let Some(TokenTree::Token(_, Token::ModSep)) = tokens.peek() {\n+            Some(TokenTree::Token(Token { kind: kind @ token::Ident(..), span })) |\n+            Some(TokenTree::Token(Token { kind: kind @ token::ModSep, span })) => 'arm: {\n+                let mut segments = if let token::Ident(name, _) = kind {\n+                    if let Some(TokenTree::Token(Token { kind: token::ModSep, .. }))\n+                            = tokens.peek() {\n                         tokens.next();\n-                        vec![PathSegment::from_ident(ident.with_span_pos(span))]\n+                        vec![PathSegment::from_ident(Ident::new(name, span))]\n                     } else {\n-                        break 'arm Path::from_ident(ident.with_span_pos(span));\n+                        break 'arm Path::from_ident(Ident::new(name, span));\n                     }\n                 } else {\n                     vec![PathSegment::path_root(span)]\n                 };\n                 loop {\n-                    if let Some(TokenTree::Token(span,\n-                                                    Token::Ident(ident, _))) = tokens.next() {\n-                        segments.push(PathSegment::from_ident(ident.with_span_pos(span)));\n+                    if let Some(TokenTree::Token(Token { kind: token::Ident(name, _), span }))\n+                            = tokens.next() {\n+                        segments.push(PathSegment::from_ident(Ident::new(name, span)));\n                     } else {\n                         return None;\n                     }\n-                    if let Some(TokenTree::Token(_, Token::ModSep)) = tokens.peek() {\n+                    if let Some(TokenTree::Token(Token { kind: token::ModSep, .. }))\n+                            = tokens.peek() {\n                         tokens.next();\n                     } else {\n                         break;\n@@ -508,7 +510,7 @@ impl MetaItem {\n                 let span = span.with_hi(segments.last().unwrap().ident.span.hi());\n                 Path { span, segments }\n             }\n-            Some(TokenTree::Token(_, Token::Interpolated(nt))) => match *nt {\n+            Some(TokenTree::Token(Token { kind: token::Interpolated(nt), .. })) => match *nt {\n                 token::Nonterminal::NtIdent(ident, _) => Path::from_ident(ident),\n                 token::Nonterminal::NtMeta(ref meta) => return Some(meta.clone()),\n                 token::Nonterminal::NtPath(ref path) => path.clone(),\n@@ -533,15 +535,15 @@ impl MetaItemKind {\n         match *self {\n             MetaItemKind::Word => TokenStream::empty(),\n             MetaItemKind::NameValue(ref lit) => {\n-                let mut vec = vec![TokenTree::Token(span, Token::Eq).into()];\n+                let mut vec = vec![TokenTree::token(token::Eq, span).into()];\n                 lit.tokens().append_to_tree_and_joint_vec(&mut vec);\n                 TokenStream::new(vec)\n             }\n             MetaItemKind::List(ref list) => {\n                 let mut tokens = Vec::new();\n                 for (i, item) in list.iter().enumerate() {\n                     if i > 0 {\n-                        tokens.push(TokenTree::Token(span, Token::Comma).into());\n+                        tokens.push(TokenTree::token(token::Comma, span).into());\n                     }\n                     item.tokens().append_to_tree_and_joint_vec(&mut tokens);\n                 }\n@@ -558,10 +560,10 @@ impl MetaItemKind {\n         where I: Iterator<Item = TokenTree>,\n     {\n         let delimited = match tokens.peek().cloned() {\n-            Some(TokenTree::Token(_, token::Eq)) => {\n+            Some(TokenTree::Token(token)) if token == token::Eq => {\n                 tokens.next();\n-                return if let Some(TokenTree::Token(span, token)) = tokens.next() {\n-                    Lit::from_token(&token, span).ok().map(MetaItemKind::NameValue)\n+                return if let Some(TokenTree::Token(token)) = tokens.next() {\n+                    Lit::from_token(&token).ok().map(MetaItemKind::NameValue)\n                 } else {\n                     None\n                 };\n@@ -579,7 +581,7 @@ impl MetaItemKind {\n             let item = NestedMetaItem::from_tokens(&mut tokens)?;\n             result.push(item);\n             match tokens.next() {\n-                None | Some(TokenTree::Token(_, Token::Comma)) => {}\n+                None | Some(TokenTree::Token(Token { kind: token::Comma, .. })) => {}\n                 _ => return None,\n             }\n         }\n@@ -605,8 +607,8 @@ impl NestedMetaItem {\n     fn from_tokens<I>(tokens: &mut iter::Peekable<I>) -> Option<NestedMetaItem>\n         where I: Iterator<Item = TokenTree>,\n     {\n-        if let Some(TokenTree::Token(span, token)) = tokens.peek().cloned() {\n-            if let Ok(lit) = Lit::from_token(&token, span) {\n+        if let Some(TokenTree::Token(token)) = tokens.peek() {\n+            if let Ok(lit) = Lit::from_token(token) {\n                 tokens.next();\n                 return Some(NestedMetaItem::Literal(lit));\n             }"}, {"sha": "9f01b9b9f9b58e98971935465136beaeef935434", "filename": "src/libsyntax/diagnostics/plugin.rs", "status": "modified", "additions": 15, "deletions": 13, "changes": 28, "blob_url": "https://github.com/rust-lang/rust/blob/ca1bcfdde3f19afd68ef808cecf2ce56d08d5df4/src%2Flibsyntax%2Fdiagnostics%2Fplugin.rs", "raw_url": "https://github.com/rust-lang/rust/raw/ca1bcfdde3f19afd68ef808cecf2ce56d08d5df4/src%2Flibsyntax%2Fdiagnostics%2Fplugin.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fdiagnostics%2Fplugin.rs?ref=ca1bcfdde3f19afd68ef808cecf2ce56d08d5df4", "patch": "@@ -5,7 +5,7 @@ use crate::ast::{self, Ident, Name};\n use crate::source_map;\n use crate::ext::base::{ExtCtxt, MacEager, MacResult};\n use crate::ext::build::AstBuilder;\n-use crate::parse::token;\n+use crate::parse::token::{self, Token};\n use crate::ptr::P;\n use crate::symbol::kw;\n use crate::tokenstream::{TokenTree};\n@@ -34,12 +34,12 @@ pub fn expand_diagnostic_used<'cx>(ecx: &'cx mut ExtCtxt<'_>,\n                                    token_tree: &[TokenTree])\n                                    -> Box<dyn MacResult+'cx> {\n     let code = match (token_tree.len(), token_tree.get(0)) {\n-        (1, Some(&TokenTree::Token(_, token::Ident(code, _)))) => code,\n+        (1, Some(&TokenTree::Token(Token { kind: token::Ident(code, _), .. }))) => code,\n         _ => unreachable!()\n     };\n \n     ecx.parse_sess.registered_diagnostics.with_lock(|diagnostics| {\n-        match diagnostics.get_mut(&code.name) {\n+        match diagnostics.get_mut(&code) {\n             // Previously used errors.\n             Some(&mut ErrorInfo { description: _, use_site: Some(previous_span) }) => {\n                 ecx.struct_span_warn(span, &format!(\n@@ -72,12 +72,14 @@ pub fn expand_register_diagnostic<'cx>(ecx: &'cx mut ExtCtxt<'_>,\n         token_tree.get(1),\n         token_tree.get(2)\n     ) {\n-        (1, Some(&TokenTree::Token(_, token::Ident(ref code, _))), None, None) => {\n+        (1, Some(&TokenTree::Token(Token { kind: token::Ident(code, _), .. })), None, None) => {\n             (code, None)\n         },\n-        (3, Some(&TokenTree::Token(_, token::Ident(ref code, _))),\n-            Some(&TokenTree::Token(_, token::Comma)),\n-            Some(&TokenTree::Token(_, token::Literal(token::Lit { symbol, .. })))) => {\n+        (3, Some(&TokenTree::Token(Token { kind: token::Ident(code, _), .. })),\n+            Some(&TokenTree::Token(Token { kind: token::Comma, .. })),\n+            Some(&TokenTree::Token(Token {\n+                kind: token::Literal(token::Lit { symbol, .. }), ..\n+            }))) => {\n             (code, Some(symbol))\n         }\n         _ => unreachable!()\n@@ -112,7 +114,7 @@ pub fn expand_register_diagnostic<'cx>(ecx: &'cx mut ExtCtxt<'_>,\n             description,\n             use_site: None\n         };\n-        if diagnostics.insert(code.name, info).is_some() {\n+        if diagnostics.insert(code, info).is_some() {\n             ecx.span_err(span, &format!(\n                 \"diagnostic code {} already registered\", code\n             ));\n@@ -140,13 +142,13 @@ pub fn expand_build_diagnostic_array<'cx>(ecx: &'cx mut ExtCtxt<'_>,\n                                           token_tree: &[TokenTree])\n                                           -> Box<dyn MacResult+'cx> {\n     assert_eq!(token_tree.len(), 3);\n-    let (crate_name, name) = match (&token_tree[0], &token_tree[2]) {\n+    let (crate_name, ident) = match (&token_tree[0], &token_tree[2]) {\n         (\n             // Crate name.\n-            &TokenTree::Token(_, token::Ident(ref crate_name, _)),\n+            &TokenTree::Token(Token { kind: token::Ident(crate_name, _), .. }),\n             // DIAGNOSTICS ident.\n-            &TokenTree::Token(_, token::Ident(ref name, _))\n-        ) => (*&crate_name, name),\n+            &TokenTree::Token(Token { kind: token::Ident(name, _), span })\n+        ) => (crate_name, Ident::new(name, span)),\n         _ => unreachable!()\n     };\n \n@@ -209,7 +211,7 @@ pub fn expand_build_diagnostic_array<'cx>(ecx: &'cx mut ExtCtxt<'_>,\n \n     MacEager::items(smallvec![\n         P(ast::Item {\n-            ident: *name,\n+            ident,\n             attrs: Vec::new(),\n             id: ast::DUMMY_NODE_ID,\n             node: ast::ItemKind::Const("}, {"sha": "598c8459d159053ce822ef235e088fcff96707ba", "filename": "src/libsyntax/early_buffered_lints.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/ca1bcfdde3f19afd68ef808cecf2ce56d08d5df4/src%2Flibsyntax%2Fearly_buffered_lints.rs", "raw_url": "https://github.com/rust-lang/rust/raw/ca1bcfdde3f19afd68ef808cecf2ce56d08d5df4/src%2Flibsyntax%2Fearly_buffered_lints.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fearly_buffered_lints.rs?ref=ca1bcfdde3f19afd68ef808cecf2ce56d08d5df4", "patch": "@@ -3,7 +3,7 @@\n //! Since we cannot have a dependency on `librustc`, we implement some types here that are somewhat\n //! redundant. Later, these types can be converted to types for use by the rest of the compiler.\n \n-use crate::syntax::ast::NodeId;\n+use crate::ast::NodeId;\n use syntax_pos::MultiSpan;\n \n /// Since we cannot import `LintId`s from `rustc::lint`, we define some Ids here which can later be"}, {"sha": "61c736662c71e3d09db17455f3044a9e8ac0c3ea", "filename": "src/libsyntax/ext/base.rs", "status": "modified", "additions": 7, "deletions": 4, "changes": 11, "blob_url": "https://github.com/rust-lang/rust/blob/ca1bcfdde3f19afd68ef808cecf2ce56d08d5df4/src%2Flibsyntax%2Fext%2Fbase.rs", "raw_url": "https://github.com/rust-lang/rust/raw/ca1bcfdde3f19afd68ef808cecf2ce56d08d5df4/src%2Flibsyntax%2Fext%2Fbase.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fext%2Fbase.rs?ref=ca1bcfdde3f19afd68ef808cecf2ce56d08d5df4", "patch": "@@ -265,10 +265,13 @@ impl<F> TTMacroExpander for F\n \n         impl MutVisitor for AvoidInterpolatedIdents {\n             fn visit_tt(&mut self, tt: &mut tokenstream::TokenTree) {\n-                if let tokenstream::TokenTree::Token(_, token::Interpolated(nt)) = tt {\n-                    if let token::NtIdent(ident, is_raw) = **nt {\n-                        *tt = tokenstream::TokenTree::Token(ident.span,\n-                                                            token::Ident(ident, is_raw));\n+                if let tokenstream::TokenTree::Token(token) = tt {\n+                    if let token::Interpolated(nt) = &token.kind {\n+                        if let token::NtIdent(ident, is_raw) = **nt {\n+                            *tt = tokenstream::TokenTree::token(\n+                                token::Ident(ident.name, is_raw), ident.span\n+                            );\n+                        }\n                     }\n                 }\n                 mut_visit::noop_visit_tt(tt, self)"}, {"sha": "7cd847eac469039e7e4b0c1c7157db09b673645a", "filename": "src/libsyntax/ext/expand.rs", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "blob_url": "https://github.com/rust-lang/rust/blob/ca1bcfdde3f19afd68ef808cecf2ce56d08d5df4/src%2Flibsyntax%2Fext%2Fexpand.rs", "raw_url": "https://github.com/rust-lang/rust/raw/ca1bcfdde3f19afd68ef808cecf2ce56d08d5df4/src%2Flibsyntax%2Fext%2Fexpand.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fext%2Fexpand.rs?ref=ca1bcfdde3f19afd68ef808cecf2ce56d08d5df4", "patch": "@@ -10,7 +10,7 @@ use crate::ext::placeholders::{placeholder, PlaceholderExpander};\n use crate::feature_gate::{self, Features, GateIssue, is_builtin_attr, emit_feature_err};\n use crate::mut_visit::*;\n use crate::parse::{DirectoryOwnership, PResult, ParseSess};\n-use crate::parse::token::{self, Token};\n+use crate::parse::token;\n use crate::parse::parser::Parser;\n use crate::ptr::P;\n use crate::symbol::Symbol;\n@@ -585,14 +585,14 @@ impl<'a, 'b> MacroExpander<'a, 'b> {\n             }\n             AttrProcMacro(ref mac, ..) => {\n                 self.gate_proc_macro_attr_item(attr.span, &item);\n-                let item_tok = TokenTree::Token(DUMMY_SP, Token::Interpolated(Lrc::new(match item {\n+                let item_tok = TokenTree::token(token::Interpolated(Lrc::new(match item {\n                     Annotatable::Item(item) => token::NtItem(item),\n                     Annotatable::TraitItem(item) => token::NtTraitItem(item.into_inner()),\n                     Annotatable::ImplItem(item) => token::NtImplItem(item.into_inner()),\n                     Annotatable::ForeignItem(item) => token::NtForeignItem(item.into_inner()),\n                     Annotatable::Stmt(stmt) => token::NtStmt(stmt.into_inner()),\n                     Annotatable::Expr(expr) => token::NtExpr(expr),\n-                }))).into();\n+                })), DUMMY_SP).into();\n                 let input = self.extract_proc_macro_attr_input(attr.tokens, attr.span);\n                 let tok_result = mac.expand(self.cx, attr.span, input, item_tok);\n                 let res = self.parse_ast_fragment(tok_result, invoc.fragment_kind,"}, {"sha": "82cc9e8ac2280ec650b035a5c2d6c7996db40bb3", "filename": "src/libsyntax/ext/tt/macro_parser.rs", "status": "modified", "additions": 51, "deletions": 55, "changes": 106, "blob_url": "https://github.com/rust-lang/rust/blob/ca1bcfdde3f19afd68ef808cecf2ce56d08d5df4/src%2Flibsyntax%2Fext%2Ftt%2Fmacro_parser.rs", "raw_url": "https://github.com/rust-lang/rust/raw/ca1bcfdde3f19afd68ef808cecf2ce56d08d5df4/src%2Flibsyntax%2Fext%2Ftt%2Fmacro_parser.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fext%2Ftt%2Fmacro_parser.rs?ref=ca1bcfdde3f19afd68ef808cecf2ce56d08d5df4", "patch": "@@ -74,11 +74,11 @@ pub use NamedMatch::*;\n pub use ParseResult::*;\n use TokenTreeOrTokenTreeSlice::*;\n \n-use crate::ast::Ident;\n+use crate::ast::{Ident, Name};\n use crate::ext::tt::quoted::{self, TokenTree};\n use crate::parse::{Directory, ParseSess};\n use crate::parse::parser::{Parser, PathStyle};\n-use crate::parse::token::{self, DocComment, Nonterminal, Token};\n+use crate::parse::token::{self, DocComment, Nonterminal, Token, TokenKind};\n use crate::print::pprust;\n use crate::symbol::{kw, sym, Symbol};\n use crate::tokenstream::{DelimSpan, TokenStream};\n@@ -199,7 +199,7 @@ struct MatcherPos<'root, 'tt: 'root> {\n     seq_op: Option<quoted::KleeneOp>,\n \n     /// The separator if we are in a repetition.\n-    sep: Option<Token>,\n+    sep: Option<TokenKind>,\n \n     /// The \"parent\" matcher position if we are in a repetition. That is, the matcher position just\n     /// before we enter the sequence.\n@@ -273,7 +273,7 @@ pub enum ParseResult<T> {\n     Success(T),\n     /// Arm failed to match. If the second parameter is `token::Eof`, it indicates an unexpected\n     /// end of macro invocation. Otherwise, it indicates that no rules expected the given token.\n-    Failure(syntax_pos::Span, Token, &'static str),\n+    Failure(Token, &'static str),\n     /// Fatal error (malformed macro?). Abort compilation.\n     Error(syntax_pos::Span, String),\n }\n@@ -417,7 +417,7 @@ fn nameize<I: Iterator<Item = NamedMatch>>(\n \n /// Generates an appropriate parsing failure message. For EOF, this is \"unexpected end...\". For\n /// other tokens, this is \"unexpected token...\".\n-pub fn parse_failure_msg(tok: Token) -> String {\n+pub fn parse_failure_msg(tok: TokenKind) -> String {\n     match tok {\n         token::Eof => \"unexpected end of macro invocation\".to_string(),\n         _ => format!(\n@@ -428,11 +428,11 @@ pub fn parse_failure_msg(tok: Token) -> String {\n }\n \n /// Performs a token equality check, ignoring syntax context (that is, an unhygienic comparison)\n-fn token_name_eq(t1: &Token, t2: &Token) -> bool {\n-    if let (Some((id1, is_raw1)), Some((id2, is_raw2))) = (t1.ident(), t2.ident()) {\n-        id1.name == id2.name && is_raw1 == is_raw2\n-    } else if let (Some(id1), Some(id2)) = (t1.lifetime(), t2.lifetime()) {\n-        id1.name == id2.name\n+fn token_name_eq(t1: &TokenKind, t2: &TokenKind) -> bool {\n+    if let (Some((name1, is_raw1)), Some((name2, is_raw2))) = (t1.ident_name(), t2.ident_name()) {\n+        name1 == name2 && is_raw1 == is_raw2\n+    } else if let (Some(name1), Some(name2)) = (t1.lifetime_name(), t2.lifetime_name()) {\n+        name1 == name2\n     } else {\n         *t1 == *t2\n     }\n@@ -467,7 +467,6 @@ fn inner_parse_loop<'root, 'tt>(\n     eof_items: &mut SmallVec<[MatcherPosHandle<'root, 'tt>; 1]>,\n     bb_items: &mut SmallVec<[MatcherPosHandle<'root, 'tt>; 1]>,\n     token: &Token,\n-    span: syntax_pos::Span,\n ) -> ParseResult<()> {\n     // Pop items from `cur_items` until it is empty.\n     while let Some(mut item) = cur_items.pop() {\n@@ -510,7 +509,7 @@ fn inner_parse_loop<'root, 'tt>(\n                     // Add matches from this repetition to the `matches` of `up`\n                     for idx in item.match_lo..item.match_hi {\n                         let sub = item.matches[idx].clone();\n-                        let span = DelimSpan::from_pair(item.sp_open, span);\n+                        let span = DelimSpan::from_pair(item.sp_open, token.span);\n                         new_pos.push_match(idx, MatchedSeq(sub, span));\n                     }\n \n@@ -598,7 +597,7 @@ fn inner_parse_loop<'root, 'tt>(\n                 TokenTree::MetaVarDecl(_, _, id) => {\n                     // Built-in nonterminals never start with these tokens,\n                     // so we can eliminate them from consideration.\n-                    if may_begin_with(id.name, token) {\n+                    if may_begin_with(token, id.name) {\n                         bb_items.push(item);\n                     }\n                 }\n@@ -609,7 +608,8 @@ fn inner_parse_loop<'root, 'tt>(\n                 //\n                 // At the beginning of the loop, if we reach the end of the delimited submatcher,\n                 // we pop the stack to backtrack out of the descent.\n-                seq @ TokenTree::Delimited(..) | seq @ TokenTree::Token(_, DocComment(..)) => {\n+                seq @ TokenTree::Delimited(..) |\n+                seq @ TokenTree::Token(Token { kind: DocComment(..), .. }) => {\n                     let lower_elts = mem::replace(&mut item.top_elts, Tt(seq));\n                     let idx = item.idx;\n                     item.stack.push(MatcherTtFrame {\n@@ -621,7 +621,7 @@ fn inner_parse_loop<'root, 'tt>(\n                 }\n \n                 // We just matched a normal token. We can just advance the parser.\n-                TokenTree::Token(_, ref t) if token_name_eq(t, token) => {\n+                TokenTree::Token(t) if token_name_eq(&t, token) => {\n                     item.idx += 1;\n                     next_items.push(item);\n                 }\n@@ -697,10 +697,9 @@ pub fn parse(\n             &mut eof_items,\n             &mut bb_items,\n             &parser.token,\n-            parser.span,\n         ) {\n             Success(_) => {}\n-            Failure(sp, tok, t) => return Failure(sp, tok, t),\n+            Failure(token, msg) => return Failure(token, msg),\n             Error(sp, msg) => return Error(sp, msg),\n         }\n \n@@ -727,12 +726,11 @@ pub fn parse(\n                 );\n             } else {\n                 return Failure(\n-                    if parser.span.is_dummy() {\n+                    Token::new(token::Eof, if parser.span.is_dummy() {\n                         parser.span\n                     } else {\n                         sess.source_map().next_point(parser.span)\n-                    },\n-                    token::Eof,\n+                    }),\n                     \"missing tokens in macro arguments\",\n                 );\n             }\n@@ -770,8 +768,7 @@ pub fn parse(\n         // then there is a syntax error.\n         else if bb_items.is_empty() && next_items.is_empty() {\n             return Failure(\n-                parser.span,\n-                parser.token.clone(),\n+                parser.token.take(),\n                 \"no rules expected this token in macro call\",\n             );\n         }\n@@ -807,10 +804,9 @@ pub fn parse(\n \n /// The token is an identifier, but not `_`.\n /// We prohibit passing `_` to macros expecting `ident` for now.\n-fn get_macro_ident(token: &Token) -> Option<(Ident, bool)> {\n+fn get_macro_name(token: &TokenKind) -> Option<(Name, bool)> {\n     match *token {\n-        token::Ident(ident, is_raw) if ident.name != kw::Underscore =>\n-            Some((ident, is_raw)),\n+        token::Ident(name, is_raw) if name != kw::Underscore => Some((name, is_raw)),\n         _ => None,\n     }\n }\n@@ -819,7 +815,7 @@ fn get_macro_ident(token: &Token) -> Option<(Ident, bool)> {\n ///\n /// Returning `false` is a *stability guarantee* that such a matcher will *never* begin with that\n /// token. Be conservative (return true) if not sure.\n-fn may_begin_with(name: Symbol, token: &Token) -> bool {\n+fn may_begin_with(token: &Token, name: Name) -> bool {\n     /// Checks whether the non-terminal may contain a single (non-keyword) identifier.\n     fn may_be_ident(nt: &token::Nonterminal) -> bool {\n         match *nt {\n@@ -831,16 +827,16 @@ fn may_begin_with(name: Symbol, token: &Token) -> bool {\n     match name {\n         sym::expr => token.can_begin_expr(),\n         sym::ty => token.can_begin_type(),\n-        sym::ident => get_macro_ident(token).is_some(),\n+        sym::ident => get_macro_name(token).is_some(),\n         sym::literal => token.can_begin_literal_or_bool(),\n-        sym::vis => match *token {\n+        sym::vis => match token.kind {\n             // The follow-set of :vis + \"priv\" keyword + interpolated\n-            Token::Comma | Token::Ident(..) | Token::Interpolated(_) => true,\n+            token::Comma | token::Ident(..) | token::Interpolated(_) => true,\n             _ => token.can_begin_type(),\n         },\n-        sym::block => match *token {\n-            Token::OpenDelim(token::Brace) => true,\n-            Token::Interpolated(ref nt) => match **nt {\n+        sym::block => match token.kind {\n+            token::OpenDelim(token::Brace) => true,\n+            token::Interpolated(ref nt) => match **nt {\n                 token::NtItem(_)\n                 | token::NtPat(_)\n                 | token::NtTy(_)\n@@ -852,39 +848,39 @@ fn may_begin_with(name: Symbol, token: &Token) -> bool {\n             },\n             _ => false,\n         },\n-        sym::path | sym::meta => match *token {\n-            Token::ModSep | Token::Ident(..) => true,\n-            Token::Interpolated(ref nt) => match **nt {\n+        sym::path | sym::meta => match token.kind {\n+            token::ModSep | token::Ident(..) => true,\n+            token::Interpolated(ref nt) => match **nt {\n                 token::NtPath(_) | token::NtMeta(_) => true,\n                 _ => may_be_ident(&nt),\n             },\n             _ => false,\n         },\n-        sym::pat => match *token {\n-            Token::Ident(..) |               // box, ref, mut, and other identifiers (can stricten)\n-            Token::OpenDelim(token::Paren) |    // tuple pattern\n-            Token::OpenDelim(token::Bracket) |  // slice pattern\n-            Token::BinOp(token::And) |          // reference\n-            Token::BinOp(token::Minus) |        // negative literal\n-            Token::AndAnd |                     // double reference\n-            Token::Literal(..) |                // literal\n-            Token::DotDot |                     // range pattern (future compat)\n-            Token::DotDotDot |                  // range pattern (future compat)\n-            Token::ModSep |                     // path\n-            Token::Lt |                         // path (UFCS constant)\n-            Token::BinOp(token::Shl) => true,   // path (double UFCS)\n-            Token::Interpolated(ref nt) => may_be_ident(nt),\n+        sym::pat => match token.kind {\n+            token::Ident(..) |               // box, ref, mut, and other identifiers (can stricten)\n+            token::OpenDelim(token::Paren) |    // tuple pattern\n+            token::OpenDelim(token::Bracket) |  // slice pattern\n+            token::BinOp(token::And) |          // reference\n+            token::BinOp(token::Minus) |        // negative literal\n+            token::AndAnd |                     // double reference\n+            token::Literal(..) |                // literal\n+            token::DotDot |                     // range pattern (future compat)\n+            token::DotDotDot |                  // range pattern (future compat)\n+            token::ModSep |                     // path\n+            token::Lt |                         // path (UFCS constant)\n+            token::BinOp(token::Shl) => true,   // path (double UFCS)\n+            token::Interpolated(ref nt) => may_be_ident(nt),\n             _ => false,\n         },\n-        sym::lifetime => match *token {\n-            Token::Lifetime(_) => true,\n-            Token::Interpolated(ref nt) => match **nt {\n+        sym::lifetime => match token.kind {\n+            token::Lifetime(_) => true,\n+            token::Interpolated(ref nt) => match **nt {\n                 token::NtLifetime(_) | token::NtTT(_) => true,\n                 _ => false,\n             },\n             _ => false,\n         },\n-        _ => match *token {\n+        _ => match token.kind {\n             token::CloseDelim(_) => false,\n             _ => true,\n         },\n@@ -930,10 +926,10 @@ fn parse_nt<'a>(p: &mut Parser<'a>, sp: Span, name: Symbol) -> Nonterminal {\n         sym::literal => token::NtLiteral(panictry!(p.parse_literal_maybe_minus())),\n         sym::ty => token::NtTy(panictry!(p.parse_ty())),\n         // this could be handled like a token, since it is one\n-        sym::ident => if let Some((ident, is_raw)) = get_macro_ident(&p.token) {\n+        sym::ident => if let Some((name, is_raw)) = get_macro_name(&p.token) {\n             let span = p.span;\n             p.bump();\n-            token::NtIdent(Ident::new(ident.name, span), is_raw)\n+            token::NtIdent(Ident::new(name, span), is_raw)\n         } else {\n             let token_str = pprust::token_to_string(&p.token);\n             p.fatal(&format!(\"expected ident, found {}\", &token_str)).emit();"}, {"sha": "7ab51c1eb20c9468d87a42f7ab3fc74fe2446fd2", "filename": "src/libsyntax/ext/tt/macro_rules.rs", "status": "modified", "additions": 35, "deletions": 40, "changes": 75, "blob_url": "https://github.com/rust-lang/rust/blob/ca1bcfdde3f19afd68ef808cecf2ce56d08d5df4/src%2Flibsyntax%2Fext%2Ftt%2Fmacro_rules.rs", "raw_url": "https://github.com/rust-lang/rust/raw/ca1bcfdde3f19afd68ef808cecf2ce56d08d5df4/src%2Flibsyntax%2Fext%2Ftt%2Fmacro_rules.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fext%2Ftt%2Fmacro_rules.rs?ref=ca1bcfdde3f19afd68ef808cecf2ce56d08d5df4", "patch": "@@ -11,8 +11,8 @@ use crate::ext::tt::transcribe::transcribe;\n use crate::feature_gate::Features;\n use crate::parse::{Directory, ParseSess};\n use crate::parse::parser::Parser;\n-use crate::parse::token::{self, NtTT};\n-use crate::parse::token::Token::*;\n+use crate::parse::token::{self, Token, NtTT};\n+use crate::parse::token::TokenKind::*;\n use crate::symbol::{Symbol, kw, sym};\n use crate::tokenstream::{DelimSpan, TokenStream, TokenTree};\n \n@@ -130,9 +130,7 @@ fn generic_extension<'cx>(cx: &'cx mut ExtCtxt<'_>,\n     }\n \n     // Which arm's failure should we report? (the one furthest along)\n-    let mut best_fail_spot = DUMMY_SP;\n-    let mut best_fail_tok = None;\n-    let mut best_fail_text = None;\n+    let mut best_failure: Option<(Token, &str)> = None;\n \n     for (i, lhs) in lhses.iter().enumerate() { // try each arm's matchers\n         let lhs_tt = match *lhs {\n@@ -190,21 +188,20 @@ fn generic_extension<'cx>(cx: &'cx mut ExtCtxt<'_>,\n                     arm_span,\n                 })\n             }\n-            Failure(sp, tok, t) => if sp.lo() >= best_fail_spot.lo() {\n-                best_fail_spot = sp;\n-                best_fail_tok = Some(tok);\n-                best_fail_text = Some(t);\n-            },\n+            Failure(token, msg) => match best_failure {\n+                Some((ref best_token, _)) if best_token.span.lo() >= token.span.lo() => {}\n+                _ => best_failure = Some((token, msg))\n+            }\n             Error(err_sp, ref msg) => {\n                 cx.span_fatal(err_sp.substitute_dummy(sp), &msg[..])\n             }\n         }\n     }\n \n-    let best_fail_msg = parse_failure_msg(best_fail_tok.expect(\"ran no matchers\"));\n-    let span = best_fail_spot.substitute_dummy(sp);\n-    let mut err = cx.struct_span_err(span, &best_fail_msg);\n-    err.span_label(span, best_fail_text.unwrap_or(&best_fail_msg));\n+    let (token, label) = best_failure.expect(\"ran no matchers\");\n+    let span = token.span.substitute_dummy(sp);\n+    let mut err = cx.struct_span_err(span, &parse_failure_msg(token.kind));\n+    err.span_label(span, label);\n     if let Some(sp) = def_span {\n         if cx.source_map().span_to_filename(sp).is_real() && !sp.is_dummy() {\n             err.span_label(cx.source_map().def_span(sp), \"when calling this macro\");\n@@ -270,7 +267,7 @@ pub fn compile(\n         quoted::TokenTree::Sequence(DelimSpan::dummy(), Lrc::new(quoted::SequenceRepetition {\n             tts: vec![\n                 quoted::TokenTree::MetaVarDecl(DUMMY_SP, lhs_nm, ast::Ident::from_str(\"tt\")),\n-                quoted::TokenTree::Token(DUMMY_SP, token::FatArrow),\n+                quoted::TokenTree::token(token::FatArrow, DUMMY_SP),\n                 quoted::TokenTree::MetaVarDecl(DUMMY_SP, rhs_nm, ast::Ident::from_str(\"tt\")),\n             ],\n             separator: Some(if body.legacy { token::Semi } else { token::Comma }),\n@@ -279,7 +276,7 @@ pub fn compile(\n         })),\n         // to phase into semicolon-termination instead of semicolon-separation\n         quoted::TokenTree::Sequence(DelimSpan::dummy(), Lrc::new(quoted::SequenceRepetition {\n-            tts: vec![quoted::TokenTree::Token(DUMMY_SP, token::Semi)],\n+            tts: vec![quoted::TokenTree::token(token::Semi, DUMMY_SP)],\n             separator: None,\n             op: quoted::KleeneOp::ZeroOrMore,\n             num_captures: 0\n@@ -288,11 +285,11 @@ pub fn compile(\n \n     let argument_map = match parse(sess, body.stream(), &argument_gram, None, true) {\n         Success(m) => m,\n-        Failure(sp, tok, t) => {\n-            let s = parse_failure_msg(tok);\n-            let sp = sp.substitute_dummy(def.span);\n+        Failure(token, msg) => {\n+            let s = parse_failure_msg(token.kind);\n+            let sp = token.span.substitute_dummy(def.span);\n             let mut err = sess.span_diagnostic.struct_span_fatal(sp, &s);\n-            err.span_label(sp, t);\n+            err.span_label(sp, msg);\n             err.emit();\n             FatalError.raise();\n         }\n@@ -613,7 +610,7 @@ impl FirstSets {\n \n                         if let (Some(ref sep), true) = (seq_rep.separator.clone(),\n                                                         subfirst.maybe_empty) {\n-                            first.add_one_maybe(TokenTree::Token(sp.entire(), sep.clone()));\n+                            first.add_one_maybe(TokenTree::token(sep.clone(), sp.entire()));\n                         }\n \n                         // Reverse scan: Sequence comes before `first`.\n@@ -663,7 +660,7 @@ impl FirstSets {\n \n                             if let (Some(ref sep), true) = (seq_rep.separator.clone(),\n                                                             subfirst.maybe_empty) {\n-                                first.add_one_maybe(TokenTree::Token(sp.entire(), sep.clone()));\n+                                first.add_one_maybe(TokenTree::token(sep.clone(), sp.entire()));\n                             }\n \n                             assert!(first.maybe_empty);\n@@ -869,7 +866,7 @@ fn check_matcher_core(sess: &ParseSess,\n                 let mut new;\n                 let my_suffix = if let Some(ref u) = seq_rep.separator {\n                     new = suffix_first.clone();\n-                    new.add_one_maybe(TokenTree::Token(sp.entire(), u.clone()));\n+                    new.add_one_maybe(TokenTree::token(u.clone(), sp.entire()));\n                     &new\n                 } else {\n                     &suffix_first\n@@ -1015,7 +1012,7 @@ enum IsInFollow {\n fn is_in_follow(tok: &quoted::TokenTree, frag: &str) -> IsInFollow {\n     use quoted::TokenTree;\n \n-    if let TokenTree::Token(_, token::CloseDelim(_)) = *tok {\n+    if let TokenTree::Token(Token { kind: token::CloseDelim(_), .. }) = *tok {\n         // closing a token tree can never be matched by any fragment;\n         // iow, we always require that `(` and `)` match, etc.\n         IsInFollow::Yes\n@@ -1033,8 +1030,8 @@ fn is_in_follow(tok: &quoted::TokenTree, frag: &str) -> IsInFollow {\n             },\n             \"stmt\" | \"expr\"  => {\n                 let tokens = vec![\"`=>`\", \"`,`\", \"`;`\"];\n-                match *tok {\n-                    TokenTree::Token(_, ref tok) => match *tok {\n+                match tok {\n+                    TokenTree::Token(token) => match token.kind {\n                         FatArrow | Comma | Semi => IsInFollow::Yes,\n                         _ => IsInFollow::No(tokens),\n                     },\n@@ -1043,11 +1040,10 @@ fn is_in_follow(tok: &quoted::TokenTree, frag: &str) -> IsInFollow {\n             },\n             \"pat\" => {\n                 let tokens = vec![\"`=>`\", \"`,`\", \"`=`\", \"`|`\", \"`if`\", \"`in`\"];\n-                match *tok {\n-                    TokenTree::Token(_, ref tok) => match *tok {\n+                match tok {\n+                    TokenTree::Token(token) => match token.kind {\n                         FatArrow | Comma | Eq | BinOp(token::Or) => IsInFollow::Yes,\n-                        Ident(i, false) if i.name == kw::If ||\n-                                           i.name == kw::In => IsInFollow::Yes,\n+                        Ident(name, false) if name == kw::If || name == kw::In => IsInFollow::Yes,\n                         _ => IsInFollow::No(tokens),\n                     },\n                     _ => IsInFollow::No(tokens),\n@@ -1058,14 +1054,14 @@ fn is_in_follow(tok: &quoted::TokenTree, frag: &str) -> IsInFollow {\n                     \"`{`\", \"`[`\", \"`=>`\", \"`,`\", \"`>`\",\"`=`\", \"`:`\", \"`;`\", \"`|`\", \"`as`\",\n                     \"`where`\",\n                 ];\n-                match *tok {\n-                    TokenTree::Token(_, ref tok) => match *tok {\n+                match tok {\n+                    TokenTree::Token(token) => match token.kind {\n                         OpenDelim(token::DelimToken::Brace) |\n                         OpenDelim(token::DelimToken::Bracket) |\n                         Comma | FatArrow | Colon | Eq | Gt | BinOp(token::Shr) | Semi |\n                         BinOp(token::Or) => IsInFollow::Yes,\n-                        Ident(i, false) if i.name == kw::As ||\n-                                           i.name == kw::Where => IsInFollow::Yes,\n+                        Ident(name, false) if name == kw::As ||\n+                                              name == kw::Where => IsInFollow::Yes,\n                         _ => IsInFollow::No(tokens),\n                     },\n                     TokenTree::MetaVarDecl(_, _, frag) if frag.name == sym::block =>\n@@ -1089,12 +1085,11 @@ fn is_in_follow(tok: &quoted::TokenTree, frag: &str) -> IsInFollow {\n             \"vis\" => {\n                 // Explicitly disallow `priv`, on the off chance it comes back.\n                 let tokens = vec![\"`,`\", \"an ident\", \"a type\"];\n-                match *tok {\n-                    TokenTree::Token(_, ref tok) => match *tok {\n+                match tok {\n+                    TokenTree::Token(token) => match token.kind {\n                         Comma => IsInFollow::Yes,\n-                        Ident(i, is_raw) if is_raw || i.name != kw::Priv =>\n-                            IsInFollow::Yes,\n-                        ref tok => if tok.can_begin_type() {\n+                        Ident(name, is_raw) if is_raw || name != kw::Priv => IsInFollow::Yes,\n+                        _ => if token.can_begin_type() {\n                             IsInFollow::Yes\n                         } else {\n                             IsInFollow::No(tokens)\n@@ -1150,7 +1145,7 @@ fn is_legal_fragment_specifier(_sess: &ParseSess,\n \n fn quoted_tt_to_string(tt: &quoted::TokenTree) -> String {\n     match *tt {\n-        quoted::TokenTree::Token(_, ref tok) => crate::print::pprust::token_to_string(tok),\n+        quoted::TokenTree::Token(ref token) => crate::print::pprust::token_to_string(&token),\n         quoted::TokenTree::MetaVar(_, name) => format!(\"${}\", name),\n         quoted::TokenTree::MetaVarDecl(_, name, kind) => format!(\"${}:{}\", name, kind),\n         _ => panic!(\"unexpected quoted::TokenTree::{{Sequence or Delimited}} \\"}, {"sha": "ec7d7f705d8937ed5bcdba65c4274e8c1d5f9802", "filename": "src/libsyntax/ext/tt/quoted.rs", "status": "modified", "additions": 58, "deletions": 56, "changes": 114, "blob_url": "https://github.com/rust-lang/rust/blob/ca1bcfdde3f19afd68ef808cecf2ce56d08d5df4/src%2Flibsyntax%2Fext%2Ftt%2Fquoted.rs", "raw_url": "https://github.com/rust-lang/rust/raw/ca1bcfdde3f19afd68ef808cecf2ce56d08d5df4/src%2Flibsyntax%2Fext%2Ftt%2Fquoted.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fext%2Ftt%2Fquoted.rs?ref=ca1bcfdde3f19afd68ef808cecf2ce56d08d5df4", "patch": "@@ -2,7 +2,8 @@ use crate::ast::NodeId;\n use crate::early_buffered_lints::BufferedEarlyLintId;\n use crate::ext::tt::macro_parser;\n use crate::feature_gate::Features;\n-use crate::parse::{token, ParseSess};\n+use crate::parse::token::{self, Token, TokenKind};\n+use crate::parse::ParseSess;\n use crate::print::pprust;\n use crate::tokenstream::{self, DelimSpan};\n use crate::ast;\n@@ -23,12 +24,12 @@ pub struct Delimited {\n \n impl Delimited {\n     /// Returns the opening delimiter (possibly `NoDelim`).\n-    pub fn open_token(&self) -> token::Token {\n+    pub fn open_token(&self) -> TokenKind {\n         token::OpenDelim(self.delim)\n     }\n \n     /// Returns the closing delimiter (possibly `NoDelim`).\n-    pub fn close_token(&self) -> token::Token {\n+    pub fn close_token(&self) -> TokenKind {\n         token::CloseDelim(self.delim)\n     }\n \n@@ -39,7 +40,7 @@ impl Delimited {\n         } else {\n             span.with_lo(span.lo() + BytePos(self.delim.len() as u32))\n         };\n-        TokenTree::Token(open_span, self.open_token())\n+        TokenTree::token(self.open_token(), open_span)\n     }\n \n     /// Returns a `self::TokenTree` with a `Span` corresponding to the closing delimiter.\n@@ -49,7 +50,7 @@ impl Delimited {\n         } else {\n             span.with_lo(span.hi() - BytePos(self.delim.len() as u32))\n         };\n-        TokenTree::Token(close_span, self.close_token())\n+        TokenTree::token(self.close_token(), close_span)\n     }\n }\n \n@@ -58,7 +59,7 @@ pub struct SequenceRepetition {\n     /// The sequence of token trees\n     pub tts: Vec<TokenTree>,\n     /// The optional separator\n-    pub separator: Option<token::Token>,\n+    pub separator: Option<TokenKind>,\n     /// Whether the sequence can be repeated zero (*), or one or more times (+)\n     pub op: KleeneOp,\n     /// The number of `Match`s that appear in the sequence (and subsequences)\n@@ -81,7 +82,7 @@ pub enum KleeneOp {\n /// are \"first-class\" token trees. Useful for parsing macros.\n #[derive(Debug, Clone, PartialEq, RustcEncodable, RustcDecodable)]\n pub enum TokenTree {\n-    Token(Span, token::Token),\n+    Token(Token),\n     Delimited(DelimSpan, Lrc<Delimited>),\n     /// A kleene-style repetition sequence\n     Sequence(DelimSpan, Lrc<SequenceRepetition>),\n@@ -144,13 +145,17 @@ impl TokenTree {\n     /// Retrieves the `TokenTree`'s span.\n     pub fn span(&self) -> Span {\n         match *self {\n-            TokenTree::Token(sp, _)\n-            | TokenTree::MetaVar(sp, _)\n-            | TokenTree::MetaVarDecl(sp, _, _) => sp,\n-            TokenTree::Delimited(sp, _)\n-            | TokenTree::Sequence(sp, _) => sp.entire(),\n+            TokenTree::Token(Token { span, .. })\n+            | TokenTree::MetaVar(span, _)\n+            | TokenTree::MetaVarDecl(span, _, _) => span,\n+            TokenTree::Delimited(span, _)\n+            | TokenTree::Sequence(span, _) => span.entire(),\n         }\n     }\n+\n+    crate fn token(kind: TokenKind, span: Span) -> TokenTree {\n+        TokenTree::Token(Token::new(kind, span))\n+    }\n }\n \n /// Takes a `tokenstream::TokenStream` and returns a `Vec<self::TokenTree>`. Specifically, this\n@@ -205,20 +210,21 @@ pub fn parse(\n         match tree {\n             TokenTree::MetaVar(start_sp, ident) if expect_matchers => {\n                 let span = match trees.next() {\n-                    Some(tokenstream::TokenTree::Token(span, token::Colon)) => match trees.next() {\n-                        Some(tokenstream::TokenTree::Token(end_sp, ref tok)) => match tok.ident() {\n-                            Some((kind, _)) => {\n-                                let span = end_sp.with_lo(start_sp.lo());\n-                                result.push(TokenTree::MetaVarDecl(span, ident, kind));\n-                                continue;\n-                            }\n-                            _ => end_sp,\n+                    Some(tokenstream::TokenTree::Token(Token { kind: token::Colon, span })) =>\n+                        match trees.next() {\n+                            Some(tokenstream::TokenTree::Token(token)) => match token.ident() {\n+                                Some((kind, _)) => {\n+                                    let span = token.span.with_lo(start_sp.lo());\n+                                    result.push(TokenTree::MetaVarDecl(span, ident, kind));\n+                                    continue;\n+                                }\n+                                _ => token.span,\n+                            },\n+                            tree => tree\n+                                .as_ref()\n+                                .map(tokenstream::TokenTree::span)\n+                                .unwrap_or(span),\n                         },\n-                        tree => tree\n-                            .as_ref()\n-                            .map(tokenstream::TokenTree::span)\n-                            .unwrap_or(span),\n-                    },\n                     tree => tree\n                         .as_ref()\n                         .map(tokenstream::TokenTree::span)\n@@ -270,7 +276,7 @@ where\n     // Depending on what `tree` is, we could be parsing different parts of a macro\n     match tree {\n         // `tree` is a `$` token. Look at the next token in `trees`\n-        tokenstream::TokenTree::Token(span, token::Dollar) => match trees.next() {\n+        tokenstream::TokenTree::Token(Token { kind: token::Dollar, span }) => match trees.next() {\n             // `tree` is followed by a delimited set of token trees. This indicates the beginning\n             // of a repetition sequence in the macro (e.g. `$(pat)*`).\n             Some(tokenstream::TokenTree::Delimited(span, delim, tts)) => {\n@@ -316,33 +322,32 @@ where\n \n             // `tree` is followed by an `ident`. This could be `$meta_var` or the `$crate` special\n             // metavariable that names the crate of the invocation.\n-            Some(tokenstream::TokenTree::Token(ident_span, ref token)) if token.is_ident() => {\n+            Some(tokenstream::TokenTree::Token(token)) if token.is_ident() => {\n                 let (ident, is_raw) = token.ident().unwrap();\n-                let span = ident_span.with_lo(span.lo());\n+                let span = ident.span.with_lo(span.lo());\n                 if ident.name == kw::Crate && !is_raw {\n-                    let ident = ast::Ident::new(kw::DollarCrate, ident.span);\n-                    TokenTree::Token(span, token::Ident(ident, is_raw))\n+                    TokenTree::token(token::Ident(kw::DollarCrate, is_raw), span)\n                 } else {\n                     TokenTree::MetaVar(span, ident)\n                 }\n             }\n \n             // `tree` is followed by a random token. This is an error.\n-            Some(tokenstream::TokenTree::Token(span, tok)) => {\n+            Some(tokenstream::TokenTree::Token(token)) => {\n                 let msg = format!(\n                     \"expected identifier, found `{}`\",\n-                    pprust::token_to_string(&tok)\n+                    pprust::token_to_string(&token),\n                 );\n-                sess.span_diagnostic.span_err(span, &msg);\n-                TokenTree::MetaVar(span, ast::Ident::invalid())\n+                sess.span_diagnostic.span_err(token.span, &msg);\n+                TokenTree::MetaVar(token.span, ast::Ident::invalid())\n             }\n \n             // There are no more tokens. Just return the `$` we already have.\n-            None => TokenTree::Token(span, token::Dollar),\n+            None => TokenTree::token(token::Dollar, span),\n         },\n \n         // `tree` is an arbitrary token. Keep it.\n-        tokenstream::TokenTree::Token(span, tok) => TokenTree::Token(span, tok),\n+        tokenstream::TokenTree::Token(token) => TokenTree::Token(token),\n \n         // `tree` is the beginning of a delimited set of tokens (e.g., `(` or `{`). We need to\n         // descend into the delimited set and further parse it.\n@@ -366,7 +371,7 @@ where\n \n /// Takes a token and returns `Some(KleeneOp)` if the token is `+` `*` or `?`. Otherwise, return\n /// `None`.\n-fn kleene_op(token: &token::Token) -> Option<KleeneOp> {\n+fn kleene_op(token: &TokenKind) -> Option<KleeneOp> {\n     match *token {\n         token::BinOp(token::Star) => Some(KleeneOp::ZeroOrMore),\n         token::BinOp(token::Plus) => Some(KleeneOp::OneOrMore),\n@@ -380,17 +385,14 @@ fn kleene_op(token: &token::Token) -> Option<KleeneOp> {\n /// - Ok(Ok((op, span))) if the next token tree is a KleeneOp\n /// - Ok(Err(tok, span)) if the next token tree is a token but not a KleeneOp\n /// - Err(span) if the next token tree is not a token\n-fn parse_kleene_op<I>(\n-    input: &mut I,\n-    span: Span,\n-) -> Result<Result<(KleeneOp, Span), (token::Token, Span)>, Span>\n+fn parse_kleene_op<I>(input: &mut I, span: Span) -> Result<Result<(KleeneOp, Span), Token>, Span>\n where\n     I: Iterator<Item = tokenstream::TokenTree>,\n {\n     match input.next() {\n-        Some(tokenstream::TokenTree::Token(span, tok)) => match kleene_op(&tok) {\n-            Some(op) => Ok(Ok((op, span))),\n-            None => Ok(Err((tok, span))),\n+        Some(tokenstream::TokenTree::Token(token)) => match kleene_op(&token) {\n+            Some(op) => Ok(Ok((op, token.span))),\n+            None => Ok(Err(token)),\n         },\n         tree => Err(tree\n             .as_ref()\n@@ -422,7 +424,7 @@ fn parse_sep_and_kleene_op<I>(\n     attrs: &[ast::Attribute],\n     edition: Edition,\n     macro_node_id: NodeId,\n-) -> (Option<token::Token>, KleeneOp)\n+) -> (Option<TokenKind>, KleeneOp)\n where\n     I: Iterator<Item = tokenstream::TokenTree>,\n {\n@@ -447,7 +449,7 @@ fn parse_sep_and_kleene_op_2015<I>(\n     _features: &Features,\n     _attrs: &[ast::Attribute],\n     macro_node_id: NodeId,\n-) -> (Option<token::Token>, KleeneOp)\n+) -> (Option<TokenKind>, KleeneOp)\n where\n     I: Iterator<Item = tokenstream::TokenTree>,\n {\n@@ -466,7 +468,7 @@ where\n             assert_eq!(op, KleeneOp::ZeroOrOne);\n \n             // Lookahead at #2. If it is a KleenOp, then #1 is a separator.\n-            let is_1_sep = if let Some(&tokenstream::TokenTree::Token(_, ref tok2)) = input.peek() {\n+            let is_1_sep = if let Some(tokenstream::TokenTree::Token(tok2)) = input.peek() {\n                 kleene_op(tok2).is_some()\n             } else {\n                 false\n@@ -504,7 +506,7 @@ where\n                     }\n \n                     // #2 is a random token (this is an error) :(\n-                    Ok(Err((_, _))) => op1_span,\n+                    Ok(Err(_)) => op1_span,\n \n                     // #2 is not even a token at all :(\n                     Err(_) => op1_span,\n@@ -524,7 +526,7 @@ where\n         }\n \n         // #1 is a separator followed by #2, a KleeneOp\n-        Ok(Err((tok, span))) => match parse_kleene_op(input, span) {\n+        Ok(Err(token)) => match parse_kleene_op(input, token.span) {\n             // #2 is a `?`, which is not allowed as a Kleene op in 2015 edition,\n             // but is allowed in the 2018 edition\n             Ok(Ok((op, op2_span))) if op == KleeneOp::ZeroOrOne => {\n@@ -539,10 +541,10 @@ where\n             }\n \n             // #2 is a KleeneOp :D\n-            Ok(Ok((op, _))) => return (Some(tok), op),\n+            Ok(Ok((op, _))) => return (Some(token.kind), op),\n \n             // #2 is a random token :(\n-            Ok(Err((_, span))) => span,\n+            Ok(Err(token)) => token.span,\n \n             // #2 is not a token at all :(\n             Err(span) => span,\n@@ -565,7 +567,7 @@ fn parse_sep_and_kleene_op_2018<I>(\n     sess: &ParseSess,\n     _features: &Features,\n     _attrs: &[ast::Attribute],\n-) -> (Option<token::Token>, KleeneOp)\n+) -> (Option<TokenKind>, KleeneOp)\n where\n     I: Iterator<Item = tokenstream::TokenTree>,\n {\n@@ -580,12 +582,12 @@ where\n         Ok(Ok((op, _))) => return (None, op),\n \n         // #1 is a separator followed by #2, a KleeneOp\n-        Ok(Err((tok, span))) => match parse_kleene_op(input, span) {\n+        Ok(Err(token)) => match parse_kleene_op(input, token.span) {\n             // #2 is the `?` Kleene op, which does not take a separator (error)\n             Ok(Ok((op, _op2_span))) if op == KleeneOp::ZeroOrOne => {\n                 // Error!\n                 sess.span_diagnostic.span_err(\n-                    span,\n+                    token.span,\n                     \"the `?` macro repetition operator does not take a separator\",\n                 );\n \n@@ -594,10 +596,10 @@ where\n             }\n \n             // #2 is a KleeneOp :D\n-            Ok(Ok((op, _))) => return (Some(tok), op),\n+            Ok(Ok((op, _))) => return (Some(token.kind), op),\n \n             // #2 is a random token :(\n-            Ok(Err((_, span))) => span,\n+            Ok(Err(token)) => token.span,\n \n             // #2 is not a token at all :(\n             Err(span) => span,"}, {"sha": "90a9cc8f34d2d51f506c120db5e598ec2d45c14b", "filename": "src/libsyntax/ext/tt/transcribe.rs", "status": "modified", "additions": 8, "deletions": 8, "changes": 16, "blob_url": "https://github.com/rust-lang/rust/blob/ca1bcfdde3f19afd68ef808cecf2ce56d08d5df4/src%2Flibsyntax%2Fext%2Ftt%2Ftranscribe.rs", "raw_url": "https://github.com/rust-lang/rust/raw/ca1bcfdde3f19afd68ef808cecf2ce56d08d5df4/src%2Flibsyntax%2Fext%2Ftt%2Ftranscribe.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fext%2Ftt%2Ftranscribe.rs?ref=ca1bcfdde3f19afd68ef808cecf2ce56d08d5df4", "patch": "@@ -4,7 +4,7 @@ use crate::ext::expand::Marker;\n use crate::ext::tt::macro_parser::{MatchedNonterminal, MatchedSeq, NamedMatch};\n use crate::ext::tt::quoted;\n use crate::mut_visit::noop_visit_tt;\n-use crate::parse::token::{self, NtTT, Token};\n+use crate::parse::token::{self, NtTT, TokenKind};\n use crate::tokenstream::{DelimSpan, TokenStream, TokenTree, TreeAndJoint};\n \n use smallvec::{smallvec, SmallVec};\n@@ -18,7 +18,7 @@ use std::rc::Rc;\n /// An iterator over the token trees in a delimited token tree (`{ ... }`) or a sequence (`$(...)`).\n enum Frame {\n     Delimited { forest: Lrc<quoted::Delimited>, idx: usize, span: DelimSpan },\n-    Sequence { forest: Lrc<quoted::SequenceRepetition>, idx: usize, sep: Option<Token> },\n+    Sequence { forest: Lrc<quoted::SequenceRepetition>, idx: usize, sep: Option<TokenKind> },\n }\n \n impl Frame {\n@@ -119,7 +119,7 @@ pub fn transcribe(\n                             Some((tt, _)) => tt.span(),\n                             None => DUMMY_SP,\n                         };\n-                        result.push(TokenTree::Token(prev_span, sep).into());\n+                        result.push(TokenTree::token(sep, prev_span).into());\n                     }\n                     continue;\n                 }\n@@ -225,7 +225,7 @@ pub fn transcribe(\n                             result.push(tt.clone().into());\n                         } else {\n                             sp = sp.apply_mark(cx.current_expansion.mark);\n-                            let token = TokenTree::Token(sp, Token::Interpolated(nt.clone()));\n+                            let token = TokenTree::token(token::Interpolated(nt.clone()), sp);\n                             result.push(token.into());\n                         }\n                     } else {\n@@ -241,8 +241,8 @@ pub fn transcribe(\n                     let ident =\n                         Ident::new(ident.name, ident.span.apply_mark(cx.current_expansion.mark));\n                     sp = sp.apply_mark(cx.current_expansion.mark);\n-                    result.push(TokenTree::Token(sp, token::Dollar).into());\n-                    result.push(TokenTree::Token(sp, token::Token::from_ast_ident(ident)).into());\n+                    result.push(TokenTree::token(token::Dollar, sp).into());\n+                    result.push(TokenTree::token(TokenKind::from_ast_ident(ident), sp).into());\n                 }\n             }\n \n@@ -259,9 +259,9 @@ pub fn transcribe(\n \n             // Nothing much to do here. Just push the token to the result, being careful to\n             // preserve syntax context.\n-            quoted::TokenTree::Token(sp, tok) => {\n+            quoted::TokenTree::Token(token) => {\n                 let mut marker = Marker(cx.current_expansion.mark);\n-                let mut tt = TokenTree::Token(sp, tok);\n+                let mut tt = TokenTree::Token(token);\n                 noop_visit_tt(&mut tt, &mut marker);\n                 result.push(tt.into());\n             }"}, {"sha": "64415204047ba275fd3ba3e7ba2be5f1cea06360", "filename": "src/libsyntax/feature_gate.rs", "status": "modified", "additions": 5, "deletions": 3, "changes": 8, "blob_url": "https://github.com/rust-lang/rust/blob/ca1bcfdde3f19afd68ef808cecf2ce56d08d5df4/src%2Flibsyntax%2Ffeature_gate.rs", "raw_url": "https://github.com/rust-lang/rust/raw/ca1bcfdde3f19afd68ef808cecf2ce56d08d5df4/src%2Flibsyntax%2Ffeature_gate.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Ffeature_gate.rs?ref=ca1bcfdde3f19afd68ef808cecf2ce56d08d5df4", "patch": "@@ -1958,9 +1958,11 @@ impl<'a> Visitor<'a> for PostExpansionVisitor<'a> {\n                 name,\n                 template\n             ),\n-            None => if let Some(TokenTree::Token(_, token::Eq)) = attr.tokens.trees().next() {\n-                // All key-value attributes are restricted to meta-item syntax.\n-                attr.parse_meta(self.context.parse_sess).map_err(|mut err| err.emit()).ok();\n+            None => if let Some(TokenTree::Token(token)) = attr.tokens.trees().next() {\n+                if token == token::Eq {\n+                    // All key-value attributes are restricted to meta-item syntax.\n+                    attr.parse_meta(self.context.parse_sess).map_err(|mut err| err.emit()).ok();\n+                }\n             }\n         }\n     }"}, {"sha": "c69364d4e19bbb7eb790abadb71849aeb297b2dd", "filename": "src/libsyntax/lib.rs", "status": "modified", "additions": 1, "deletions": 6, "changes": 7, "blob_url": "https://github.com/rust-lang/rust/blob/ca1bcfdde3f19afd68ef808cecf2ce56d08d5df4/src%2Flibsyntax%2Flib.rs", "raw_url": "https://github.com/rust-lang/rust/raw/ca1bcfdde3f19afd68ef808cecf2ce56d08d5df4/src%2Flibsyntax%2Flib.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Flib.rs?ref=ca1bcfdde3f19afd68ef808cecf2ce56d08d5df4", "patch": "@@ -10,6 +10,7 @@\n #![deny(rust_2018_idioms)]\n #![deny(internal)]\n \n+#![feature(bind_by_move_pattern_guards)]\n #![feature(crate_visibility_modifier)]\n #![feature(label_break_value)]\n #![feature(nll)]\n@@ -136,12 +137,6 @@ pub mod util {\n \n pub mod json;\n \n-pub mod syntax {\n-    pub use crate::ext;\n-    pub use crate::parse;\n-    pub use crate::ast;\n-}\n-\n pub mod ast;\n pub mod attr;\n pub mod source_map;"}, {"sha": "d2a614c4a54ac320a803a6aa84c62ee7de43124d", "filename": "src/libsyntax/mut_visit.rs", "status": "modified", "additions": 15, "deletions": 7, "changes": 22, "blob_url": "https://github.com/rust-lang/rust/blob/ca1bcfdde3f19afd68ef808cecf2ce56d08d5df4/src%2Flibsyntax%2Fmut_visit.rs", "raw_url": "https://github.com/rust-lang/rust/raw/ca1bcfdde3f19afd68ef808cecf2ce56d08d5df4/src%2Flibsyntax%2Fmut_visit.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fmut_visit.rs?ref=ca1bcfdde3f19afd68ef808cecf2ce56d08d5df4", "patch": "@@ -576,9 +576,8 @@ pub fn noop_visit_arg<T: MutVisitor>(Arg { id, pat, ty }: &mut Arg, vis: &mut T)\n \n pub fn noop_visit_tt<T: MutVisitor>(tt: &mut TokenTree, vis: &mut T) {\n     match tt {\n-        TokenTree::Token(span, tok) => {\n-            vis.visit_span(span);\n-            vis.visit_token(tok);\n+        TokenTree::Token(token) => {\n+            vis.visit_token(token);\n         }\n         TokenTree::Delimited(DelimSpan { open, close }, _delim, tts) => {\n             vis.visit_span(open);\n@@ -595,17 +594,26 @@ pub fn noop_visit_tts<T: MutVisitor>(TokenStream(tts): &mut TokenStream, vis: &m\n     })\n }\n \n-// apply ident visitor if it's an ident, apply other visits to interpolated nodes\n+// Apply ident visitor if it's an ident, apply other visits to interpolated nodes.\n+// In practice the ident part is not actually used by specific visitors right now,\n+// but there's a test below checking that it works.\n pub fn noop_visit_token<T: MutVisitor>(t: &mut Token, vis: &mut T) {\n-    match t {\n-        token::Ident(id, _is_raw) => vis.visit_ident(id),\n-        token::Lifetime(id) => vis.visit_ident(id),\n+    let Token { kind, span } = t;\n+    match kind {\n+        token::Ident(name, _) | token::Lifetime(name) => {\n+            let mut ident = Ident::new(*name, *span);\n+            vis.visit_ident(&mut ident);\n+            *name = ident.name;\n+            *span = ident.span;\n+            return; // avoid visiting the span for the second time\n+        }\n         token::Interpolated(nt) => {\n             let mut nt = Lrc::make_mut(nt);\n             vis.visit_interpolated(&mut nt);\n         }\n         _ => {}\n     }\n+    vis.visit_span(span);\n }\n \n /// Apply visitor to elements of interpolated nodes."}, {"sha": "d83b76f4d236610be39f7408e1bea25d4dece762", "filename": "src/libsyntax/parse/attr.rs", "status": "modified", "additions": 7, "deletions": 7, "changes": 14, "blob_url": "https://github.com/rust-lang/rust/blob/ca1bcfdde3f19afd68ef808cecf2ce56d08d5df4/src%2Flibsyntax%2Fparse%2Fattr.rs", "raw_url": "https://github.com/rust-lang/rust/raw/ca1bcfdde3f19afd68ef808cecf2ce56d08d5df4/src%2Flibsyntax%2Fparse%2Fattr.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Fattr.rs?ref=ca1bcfdde3f19afd68ef808cecf2ce56d08d5df4", "patch": "@@ -24,7 +24,7 @@ impl<'a> Parser<'a> {\n         let mut just_parsed_doc_comment = false;\n         loop {\n             debug!(\"parse_outer_attributes: self.token={:?}\", self.token);\n-            match self.token {\n+            match self.token.kind {\n                 token::Pound => {\n                     let inner_error_reason = if just_parsed_doc_comment {\n                         \"an inner attribute is not permitted following an outer doc comment\"\n@@ -81,7 +81,7 @@ impl<'a> Parser<'a> {\n         debug!(\"parse_attribute_with_inner_parse_policy: inner_parse_policy={:?} self.token={:?}\",\n                inner_parse_policy,\n                self.token);\n-        let (span, path, tokens, style) = match self.token {\n+        let (span, path, tokens, style) = match self.token.kind {\n             token::Pound => {\n                 let lo = self.span;\n                 self.bump();\n@@ -140,7 +140,7 @@ impl<'a> Parser<'a> {\n     /// PATH `=` TOKEN_TREE\n     /// The delimiters or `=` are still put into the resulting token stream.\n     crate fn parse_meta_item_unrestricted(&mut self) -> PResult<'a, (ast::Path, TokenStream)> {\n-        let meta = match self.token {\n+        let meta = match self.token.kind {\n             token::Interpolated(ref nt) => match **nt {\n                 Nonterminal::NtMeta(ref meta) => Some(meta.clone()),\n                 _ => None,\n@@ -157,9 +157,9 @@ impl<'a> Parser<'a> {\n                self.check(&token::OpenDelim(DelimToken::Brace)) {\n                    self.parse_token_tree().into()\n             } else if self.eat(&token::Eq) {\n-                let eq = TokenTree::Token(self.prev_span, token::Eq);\n+                let eq = TokenTree::token(token::Eq, self.prev_span);\n                 let mut is_interpolated_expr = false;\n-                if let token::Interpolated(nt) = &self.token {\n+                if let token::Interpolated(nt) = &self.token.kind {\n                     if let token::NtExpr(..) = **nt {\n                         is_interpolated_expr = true;\n                     }\n@@ -188,7 +188,7 @@ impl<'a> Parser<'a> {\n     crate fn parse_inner_attributes(&mut self) -> PResult<'a, Vec<ast::Attribute>> {\n         let mut attrs: Vec<ast::Attribute> = vec![];\n         loop {\n-            match self.token {\n+            match self.token.kind {\n                 token::Pound => {\n                     // Don't even try to parse if it's not an inner attribute.\n                     if !self.look_ahead(1, |t| t == &token::Not) {\n@@ -236,7 +236,7 @@ impl<'a> Parser<'a> {\n     /// meta_item : IDENT ( '=' UNSUFFIXED_LIT | '(' meta_item_inner? ')' )? ;\n     /// meta_item_inner : (meta_item | UNSUFFIXED_LIT) (',' meta_item_inner)? ;\n     pub fn parse_meta_item(&mut self) -> PResult<'a, ast::MetaItem> {\n-        let nt_meta = match self.token {\n+        let nt_meta = match self.token.kind {\n             token::Interpolated(ref nt) => match **nt {\n                 token::NtMeta(ref e) => Some(e.clone()),\n                 _ => None,"}, {"sha": "7f0bf4a90508bcab564e28b04d40b4a4fe0afdeb", "filename": "src/libsyntax/parse/diagnostics.rs", "status": "modified", "additions": 21, "deletions": 20, "changes": 41, "blob_url": "https://github.com/rust-lang/rust/blob/ca1bcfdde3f19afd68ef808cecf2ce56d08d5df4/src%2Flibsyntax%2Fparse%2Fdiagnostics.rs", "raw_url": "https://github.com/rust-lang/rust/raw/ca1bcfdde3f19afd68ef808cecf2ce56d08d5df4/src%2Flibsyntax%2Fparse%2Fdiagnostics.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Fdiagnostics.rs?ref=ca1bcfdde3f19afd68ef808cecf2ce56d08d5df4", "patch": "@@ -2,8 +2,9 @@ use crate::ast::{\n     self, Arg, BinOpKind, BindingMode, BlockCheckMode, Expr, ExprKind, Ident, Item, ItemKind,\n     Mutability, Pat, PatKind, PathSegment, QSelf, Ty, TyKind, VariantData,\n };\n-use crate::parse::{SeqSep, token, PResult, Parser};\n+use crate::parse::{SeqSep, PResult, Parser};\n use crate::parse::parser::{BlockMode, PathStyle, SemiColonMode, TokenType, TokenExpectType};\n+use crate::parse::token::{self, TokenKind};\n use crate::print::pprust;\n use crate::ptr::P;\n use crate::source_map::Spanned;\n@@ -201,12 +202,12 @@ impl<'a> Parser<'a> {\n             self.span,\n             &format!(\"expected identifier, found {}\", self.this_token_descr()),\n         );\n-        if let token::Ident(ident, false) = &self.token {\n-            if ident.is_raw_guess() {\n+        if let token::Ident(name, false) = self.token.kind {\n+            if Ident::new(name, self.span).is_raw_guess() {\n                 err.span_suggestion(\n                     self.span,\n                     \"you can escape reserved keywords to use them as identifiers\",\n-                    format!(\"r#{}\", ident),\n+                    format!(\"r#{}\", name),\n                     Applicability::MaybeIncorrect,\n                 );\n             }\n@@ -229,8 +230,8 @@ impl<'a> Parser<'a> {\n \n     pub fn expected_one_of_not_found(\n         &mut self,\n-        edible: &[token::Token],\n-        inedible: &[token::Token],\n+        edible: &[TokenKind],\n+        inedible: &[TokenKind],\n     ) -> PResult<'a, bool /* recovered */> {\n         fn tokens_to_string(tokens: &[TokenType]) -> String {\n             let mut i = tokens.iter();\n@@ -294,7 +295,7 @@ impl<'a> Parser<'a> {\n                 Applicability::MaybeIncorrect,\n             );\n         }\n-        let sp = if self.token == token::Token::Eof {\n+        let sp = if self.token == token::Eof {\n             // This is EOF, don't want to point at the following char, but rather the last token\n             self.prev_span\n         } else {\n@@ -368,7 +369,7 @@ impl<'a> Parser<'a> {\n \n     /// Eats and discards tokens until one of `kets` is encountered. Respects token trees,\n     /// passes through any errors encountered. Used for error recovery.\n-    crate fn eat_to_tokens(&mut self, kets: &[&token::Token]) {\n+    crate fn eat_to_tokens(&mut self, kets: &[&TokenKind]) {\n         let handler = self.diagnostic();\n \n         if let Err(ref mut err) = self.parse_seq_to_before_tokens(\n@@ -388,7 +389,7 @@ impl<'a> Parser<'a> {\n     /// let _ = vec![1, 2, 3].into_iter().collect::<Vec<usize>>>>();\n     ///                                                        ^^ help: remove extra angle brackets\n     /// ```\n-    crate fn check_trailing_angle_brackets(&mut self, segment: &PathSegment, end: token::Token) {\n+    crate fn check_trailing_angle_brackets(&mut self, segment: &PathSegment, end: TokenKind) {\n         // This function is intended to be invoked after parsing a path segment where there are two\n         // cases:\n         //\n@@ -726,28 +727,28 @@ impl<'a> Parser<'a> {\n     /// closing delimiter.\n     pub fn unexpected_try_recover(\n         &mut self,\n-        t: &token::Token,\n+        t: &TokenKind,\n     ) -> PResult<'a, bool /* recovered */> {\n         let token_str = pprust::token_to_string(t);\n         let this_token_str = self.this_token_descr();\n-        let (prev_sp, sp) = match (&self.token, self.subparser_name) {\n+        let (prev_sp, sp) = match (&self.token.kind, self.subparser_name) {\n             // Point at the end of the macro call when reaching end of macro arguments.\n-            (token::Token::Eof, Some(_)) => {\n+            (token::Eof, Some(_)) => {\n                 let sp = self.sess.source_map().next_point(self.span);\n                 (sp, sp)\n             }\n             // We don't want to point at the following span after DUMMY_SP.\n             // This happens when the parser finds an empty TokenStream.\n             _ if self.prev_span == DUMMY_SP => (self.span, self.span),\n             // EOF, don't want to point at the following char, but rather the last token.\n-            (token::Token::Eof, None) => (self.prev_span, self.span),\n+            (token::Eof, None) => (self.prev_span, self.span),\n             _ => (self.sess.source_map().next_point(self.prev_span), self.span),\n         };\n         let msg = format!(\n             \"expected `{}`, found {}\",\n             token_str,\n-            match (&self.token, self.subparser_name) {\n-                (token::Token::Eof, Some(origin)) => format!(\"end of {}\", origin),\n+            match (&self.token.kind, self.subparser_name) {\n+                (token::Eof, Some(origin)) => format!(\"end of {}\", origin),\n                 _ => this_token_str,\n             },\n         );\n@@ -903,7 +904,7 @@ impl<'a> Parser<'a> {\n \n     crate fn recover_closing_delimiter(\n         &mut self,\n-        tokens: &[token::Token],\n+        tokens: &[TokenKind],\n         mut err: DiagnosticBuilder<'a>,\n     ) -> PResult<'a, bool> {\n         let mut pos = None;\n@@ -989,7 +990,7 @@ impl<'a> Parser<'a> {\n                break_on_semi, break_on_block);\n         loop {\n             debug!(\"recover_stmt_ loop {:?}\", self.token);\n-            match self.token {\n+            match self.token.kind {\n                 token::OpenDelim(token::DelimToken::Brace) => {\n                     brace_depth += 1;\n                     self.bump();\n@@ -1074,7 +1075,7 @@ impl<'a> Parser<'a> {\n     }\n \n     crate fn eat_incorrect_doc_comment(&mut self, applied_to: &str) {\n-        if let token::DocComment(_) = self.token {\n+        if let token::DocComment(_) = self.token.kind {\n             let mut err = self.diagnostic().struct_span_err(\n                 self.span,\n                 &format!(\"documentation comments cannot be applied to {}\", applied_to),\n@@ -1214,8 +1215,8 @@ impl<'a> Parser<'a> {\n     }\n \n     crate fn expected_expression_found(&self) -> DiagnosticBuilder<'a> {\n-        let (span, msg) = match (&self.token, self.subparser_name) {\n-            (&token::Token::Eof, Some(origin)) => {\n+        let (span, msg) = match (&self.token.kind, self.subparser_name) {\n+            (&token::Eof, Some(origin)) => {\n                 let sp = self.sess.source_map().next_point(self.span);\n                 (sp, format!(\"expected expression, found end of {}\", origin))\n             }"}, {"sha": "e3d959c2c54c450246716ae233849c6f3b100f18", "filename": "src/libsyntax/parse/lexer/mod.rs", "status": "modified", "additions": 90, "deletions": 147, "changes": 237, "blob_url": "https://github.com/rust-lang/rust/blob/ca1bcfdde3f19afd68ef808cecf2ce56d08d5df4/src%2Flibsyntax%2Fparse%2Flexer%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/ca1bcfdde3f19afd68ef808cecf2ce56d08d5df4/src%2Flibsyntax%2Fparse%2Flexer%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Flexer%2Fmod.rs?ref=ca1bcfdde3f19afd68ef808cecf2ce56d08d5df4", "patch": "@@ -1,6 +1,6 @@\n-use crate::ast::{self, Ident};\n+use crate::ast;\n use crate::parse::ParseSess;\n-use crate::parse::token::{self, Token};\n+use crate::parse::token::{self, Token, TokenKind};\n use crate::symbol::{sym, Symbol};\n use crate::parse::unescape;\n use crate::parse::unescape_error_reporting::{emit_unescape_error, push_escaped_char};\n@@ -12,29 +12,13 @@ use core::unicode::property::Pattern_White_Space;\n use std::borrow::Cow;\n use std::char;\n use std::iter;\n-use std::mem::replace;\n use rustc_data_structures::sync::Lrc;\n use log::debug;\n \n pub mod comments;\n mod tokentrees;\n mod unicode_chars;\n \n-#[derive(Clone, Debug)]\n-pub struct TokenAndSpan {\n-    pub tok: Token,\n-    pub sp: Span,\n-}\n-\n-impl Default for TokenAndSpan {\n-    fn default() -> Self {\n-        TokenAndSpan {\n-            tok: token::Whitespace,\n-            sp: syntax_pos::DUMMY_SP,\n-        }\n-    }\n-}\n-\n #[derive(Clone, Debug)]\n pub struct UnmatchedBrace {\n     pub expected_delim: token::DelimToken,\n@@ -56,8 +40,7 @@ pub struct StringReader<'a> {\n     /// Stop reading src at this index.\n     crate end_src_index: usize,\n     // cached:\n-    peek_tok: Token,\n-    peek_span: Span,\n+    peek_token: Token,\n     peek_span_src_raw: Span,\n     fatal_errs: Vec<DiagnosticBuilder<'a>>,\n     // cache a direct reference to the source text, so that we don't have to\n@@ -78,16 +61,7 @@ impl<'a> StringReader<'a> {\n         (real, raw)\n     }\n \n-    fn mk_ident(&self, string: &str) -> Ident {\n-        let mut ident = Ident::from_str(string);\n-        if let Some(span) = self.override_span {\n-            ident.span = span;\n-        }\n-\n-        ident\n-    }\n-\n-    fn unwrap_or_abort(&mut self, res: Result<TokenAndSpan, ()>) -> TokenAndSpan {\n+    fn unwrap_or_abort(&mut self, res: Result<Token, ()>) -> Token {\n         match res {\n             Ok(tok) => tok,\n             Err(_) => {\n@@ -97,18 +71,15 @@ impl<'a> StringReader<'a> {\n         }\n     }\n \n-    fn next_token(&mut self) -> TokenAndSpan where Self: Sized {\n+    fn next_token(&mut self) -> Token where Self: Sized {\n         let res = self.try_next_token();\n         self.unwrap_or_abort(res)\n     }\n \n     /// Returns the next token. EFFECT: advances the string_reader.\n-    pub fn try_next_token(&mut self) -> Result<TokenAndSpan, ()> {\n+    pub fn try_next_token(&mut self) -> Result<Token, ()> {\n         assert!(self.fatal_errs.is_empty());\n-        let ret_val = TokenAndSpan {\n-            tok: replace(&mut self.peek_tok, token::Whitespace),\n-            sp: self.peek_span,\n-        };\n+        let ret_val = self.peek_token.take();\n         self.advance_token()?;\n         Ok(ret_val)\n     }\n@@ -135,10 +106,10 @@ impl<'a> StringReader<'a> {\n         return None;\n     }\n \n-    fn try_real_token(&mut self) -> Result<TokenAndSpan, ()> {\n+    fn try_real_token(&mut self) -> Result<Token, ()> {\n         let mut t = self.try_next_token()?;\n         loop {\n-            match t.tok {\n+            match t.kind {\n                 token::Whitespace | token::Comment | token::Shebang(_) => {\n                     t = self.try_next_token()?;\n                 }\n@@ -149,7 +120,7 @@ impl<'a> StringReader<'a> {\n         Ok(t)\n     }\n \n-    pub fn real_token(&mut self) -> TokenAndSpan {\n+    pub fn real_token(&mut self) -> Token {\n         let res = self.try_real_token();\n         self.unwrap_or_abort(res)\n     }\n@@ -173,7 +144,7 @@ impl<'a> StringReader<'a> {\n     }\n \n     fn fatal(&self, m: &str) -> FatalError {\n-        self.fatal_span(self.peek_span, m)\n+        self.fatal_span(self.peek_token.span, m)\n     }\n \n     crate fn emit_fatal_errors(&mut self) {\n@@ -194,12 +165,8 @@ impl<'a> StringReader<'a> {\n         buffer\n     }\n \n-    pub fn peek(&self) -> TokenAndSpan {\n-        // FIXME(pcwalton): Bad copy!\n-        TokenAndSpan {\n-            tok: self.peek_tok.clone(),\n-            sp: self.peek_span,\n-        }\n+    pub fn peek(&self) -> &Token {\n+        &self.peek_token\n     }\n \n     /// For comments.rs, which hackily pokes into next_pos and ch\n@@ -229,9 +196,7 @@ impl<'a> StringReader<'a> {\n             ch: Some('\\n'),\n             source_file,\n             end_src_index: src.len(),\n-            // dummy values; not read\n-            peek_tok: token::Eof,\n-            peek_span: syntax_pos::DUMMY_SP,\n+            peek_token: Token::dummy(),\n             peek_span_src_raw: syntax_pos::DUMMY_SP,\n             src,\n             fatal_errs: Vec::new(),\n@@ -336,31 +301,24 @@ impl<'a> StringReader<'a> {\n         self.err_span_(from_pos, to_pos, &m[..]);\n     }\n \n-    /// Advance peek_tok and peek_span to refer to the next token, and\n+    /// Advance peek_token to refer to the next token, and\n     /// possibly update the interner.\n     fn advance_token(&mut self) -> Result<(), ()> {\n         match self.scan_whitespace_or_comment() {\n             Some(comment) => {\n-                self.peek_span_src_raw = comment.sp;\n-                self.peek_span = comment.sp;\n-                self.peek_tok = comment.tok;\n+                self.peek_span_src_raw = comment.span;\n+                self.peek_token = comment;\n             }\n             None => {\n-                if self.is_eof() {\n-                    self.peek_tok = token::Eof;\n-                    let (real, raw) = self.mk_sp_and_raw(\n-                        self.source_file.end_pos,\n-                        self.source_file.end_pos,\n-                    );\n-                    self.peek_span = real;\n-                    self.peek_span_src_raw = raw;\n+                let (kind, start_pos, end_pos) = if self.is_eof() {\n+                    (token::Eof, self.source_file.end_pos, self.source_file.end_pos)\n                 } else {\n-                    let start_bytepos = self.pos;\n-                    self.peek_tok = self.next_token_inner()?;\n-                    let (real, raw) = self.mk_sp_and_raw(start_bytepos, self.pos);\n-                    self.peek_span = real;\n-                    self.peek_span_src_raw = raw;\n+                    let start_pos = self.pos;\n+                    (self.next_token_inner()?, start_pos, self.pos)\n                 };\n+                let (real, raw) = self.mk_sp_and_raw(start_pos, end_pos);\n+                self.peek_token = Token::new(kind, real);\n+                self.peek_span_src_raw = raw;\n             }\n         }\n \n@@ -527,7 +485,7 @@ impl<'a> StringReader<'a> {\n \n     /// PRECONDITION: self.ch is not whitespace\n     /// Eats any kind of comment.\n-    fn scan_comment(&mut self) -> Option<TokenAndSpan> {\n+    fn scan_comment(&mut self) -> Option<Token> {\n         if let Some(c) = self.ch {\n             if c.is_whitespace() {\n                 let msg = \"called consume_any_line_comment, but there was whitespace\";\n@@ -563,14 +521,14 @@ impl<'a> StringReader<'a> {\n                         self.bump();\n                     }\n \n-                    let tok = if doc_comment {\n+                    let kind = if doc_comment {\n                         self.with_str_from(start_bpos, |string| {\n                             token::DocComment(Symbol::intern(string))\n                         })\n                     } else {\n                         token::Comment\n                     };\n-                    Some(TokenAndSpan { tok, sp: self.mk_sp(start_bpos, self.pos) })\n+                    Some(Token::new(kind, self.mk_sp(start_bpos, self.pos)))\n                 }\n                 Some('*') => {\n                     self.bump();\n@@ -594,10 +552,10 @@ impl<'a> StringReader<'a> {\n                     while !self.ch_is('\\n') && !self.is_eof() {\n                         self.bump();\n                     }\n-                    return Some(TokenAndSpan {\n-                        tok: token::Shebang(self.name_from(start)),\n-                        sp: self.mk_sp(start, self.pos),\n-                    });\n+                    return Some(Token::new(\n+                        token::Shebang(self.name_from(start)),\n+                        self.mk_sp(start, self.pos),\n+                    ));\n                 }\n             }\n             None\n@@ -608,7 +566,7 @@ impl<'a> StringReader<'a> {\n \n     /// If there is whitespace, shebang, or a comment, scan it. Otherwise,\n     /// return `None`.\n-    fn scan_whitespace_or_comment(&mut self) -> Option<TokenAndSpan> {\n+    fn scan_whitespace_or_comment(&mut self) -> Option<Token> {\n         match self.ch.unwrap_or('\\0') {\n             // # to handle shebang at start of file -- this is the entry point\n             // for skipping over all \"junk\"\n@@ -622,10 +580,7 @@ impl<'a> StringReader<'a> {\n                 while is_pattern_whitespace(self.ch) {\n                     self.bump();\n                 }\n-                let c = Some(TokenAndSpan {\n-                    tok: token::Whitespace,\n-                    sp: self.mk_sp(start_bpos, self.pos),\n-                });\n+                let c = Some(Token::new(token::Whitespace, self.mk_sp(start_bpos, self.pos)));\n                 debug!(\"scanning whitespace: {:?}\", c);\n                 c\n             }\n@@ -634,7 +589,7 @@ impl<'a> StringReader<'a> {\n     }\n \n     /// Might return a sugared-doc-attr\n-    fn scan_block_comment(&mut self) -> Option<TokenAndSpan> {\n+    fn scan_block_comment(&mut self) -> Option<Token> {\n         // block comments starting with \"/**\" or \"/*!\" are doc-comments\n         let is_doc_comment = self.ch_is('*') || self.ch_is('!');\n         let start_bpos = self.pos - BytePos(2);\n@@ -671,7 +626,7 @@ impl<'a> StringReader<'a> {\n \n         self.with_str_from(start_bpos, |string| {\n             // but comments with only \"*\"s between two \"/\"s are not\n-            let tok = if is_block_doc_comment(string) {\n+            let kind = if is_block_doc_comment(string) {\n                 let string = if has_cr {\n                     self.translate_crlf(start_bpos,\n                                         string,\n@@ -684,10 +639,7 @@ impl<'a> StringReader<'a> {\n                 token::Comment\n             };\n \n-            Some(TokenAndSpan {\n-                tok,\n-                sp: self.mk_sp(start_bpos, self.pos),\n-            })\n+            Some(Token::new(kind, self.mk_sp(start_bpos, self.pos)))\n         })\n     }\n \n@@ -847,7 +799,7 @@ impl<'a> StringReader<'a> {\n         }\n     }\n \n-    fn binop(&mut self, op: token::BinOpToken) -> Token {\n+    fn binop(&mut self, op: token::BinOpToken) -> TokenKind {\n         self.bump();\n         if self.ch_is('=') {\n             self.bump();\n@@ -859,7 +811,7 @@ impl<'a> StringReader<'a> {\n \n     /// Returns the next token from the string, advances the input past that\n     /// token, and updates the interner\n-    fn next_token_inner(&mut self) -> Result<Token, ()> {\n+    fn next_token_inner(&mut self) -> Result<TokenKind, ()> {\n         let c = self.ch;\n \n         if ident_start(c) {\n@@ -897,17 +849,17 @@ impl<'a> StringReader<'a> {\n \n                 return Ok(self.with_str_from(start, |string| {\n                     // FIXME: perform NFKC normalization here. (Issue #2253)\n-                    let ident = self.mk_ident(string);\n+                    let name = ast::Name::intern(string);\n \n                     if is_raw_ident {\n                         let span = self.mk_sp(raw_start, self.pos);\n-                        if !ident.can_be_raw() {\n-                            self.err_span(span, &format!(\"`{}` cannot be a raw identifier\", ident));\n+                        if !name.can_be_raw() {\n+                            self.err_span(span, &format!(\"`{}` cannot be a raw identifier\", name));\n                         }\n                         self.sess.raw_identifier_spans.borrow_mut().push(span);\n                     }\n \n-                    token::Ident(ident, is_raw_ident)\n+                    token::Ident(name, is_raw_ident)\n                 }));\n             }\n         }\n@@ -916,7 +868,7 @@ impl<'a> StringReader<'a> {\n             let (kind, symbol) = self.scan_number(c.unwrap());\n             let suffix = self.scan_optional_raw_name();\n             debug!(\"next_token_inner: scanned number {:?}, {:?}, {:?}\", kind, symbol, suffix);\n-            return Ok(Token::lit(kind, symbol, suffix));\n+            return Ok(TokenKind::lit(kind, symbol, suffix));\n         }\n \n         match c.expect(\"next_token_inner called at EOF\") {\n@@ -1077,16 +1029,9 @@ impl<'a> StringReader<'a> {\n                         let symbol = self.name_from(start);\n                         self.bump();\n                         self.validate_char_escape(start_with_quote);\n-                        return Ok(Token::lit(token::Char, symbol, None));\n+                        return Ok(TokenKind::lit(token::Char, symbol, None));\n                     }\n \n-                    // Include the leading `'` in the real identifier, for macro\n-                    // expansion purposes. See #12512 for the gory details of why\n-                    // this is necessary.\n-                    let ident = self.with_str_from(start_with_quote, |lifetime_name| {\n-                        self.mk_ident(lifetime_name)\n-                    });\n-\n                     if starts_with_number {\n                         // this is a recovered lifetime written `'1`, error but accept it\n                         self.err_span_(\n@@ -1096,13 +1041,16 @@ impl<'a> StringReader<'a> {\n                         );\n                     }\n \n-                    return Ok(token::Lifetime(ident));\n+                    // Include the leading `'` in the real identifier, for macro\n+                    // expansion purposes. See #12512 for the gory details of why\n+                    // this is necessary.\n+                    return Ok(token::Lifetime(self.name_from(start_with_quote)));\n                 }\n                 let msg = \"unterminated character literal\";\n                 let symbol = self.scan_single_quoted_string(start_with_quote, msg);\n                 self.validate_char_escape(start_with_quote);\n                 let suffix = self.scan_optional_raw_name();\n-                Ok(Token::lit(token::Char, symbol, suffix))\n+                Ok(TokenKind::lit(token::Char, symbol, suffix))\n             }\n             'b' => {\n                 self.bump();\n@@ -1127,15 +1075,15 @@ impl<'a> StringReader<'a> {\n                 };\n                 let suffix = self.scan_optional_raw_name();\n \n-                Ok(Token::lit(kind, symbol, suffix))\n+                Ok(TokenKind::lit(kind, symbol, suffix))\n             }\n             '\"' => {\n                 let start_with_quote = self.pos;\n                 let msg = \"unterminated double quote string\";\n                 let symbol = self.scan_double_quoted_string(msg);\n                 self.validate_str_escape(start_with_quote);\n                 let suffix = self.scan_optional_raw_name();\n-                Ok(Token::lit(token::Str, symbol, suffix))\n+                Ok(TokenKind::lit(token::Str, symbol, suffix))\n             }\n             'r' => {\n                 let start_bpos = self.pos;\n@@ -1213,7 +1161,7 @@ impl<'a> StringReader<'a> {\n                 };\n                 let suffix = self.scan_optional_raw_name();\n \n-                Ok(Token::lit(token::StrRaw(hash_count), symbol, suffix))\n+                Ok(TokenKind::lit(token::StrRaw(hash_count), symbol, suffix))\n             }\n             '-' => {\n                 if self.nextch_is('>') {\n@@ -1610,47 +1558,46 @@ mod tests {\n                                         &sh,\n                                         \"/* my source file */ fn main() { println!(\\\"zebra\\\"); }\\n\"\n                                             .to_string());\n-            let id = Ident::from_str(\"fn\");\n-            assert_eq!(string_reader.next_token().tok, token::Comment);\n-            assert_eq!(string_reader.next_token().tok, token::Whitespace);\n+            assert_eq!(string_reader.next_token(), token::Comment);\n+            assert_eq!(string_reader.next_token(), token::Whitespace);\n             let tok1 = string_reader.next_token();\n-            let tok2 = TokenAndSpan {\n-                tok: token::Ident(id, false),\n-                sp: Span::new(BytePos(21), BytePos(23), NO_EXPANSION),\n-            };\n-            assert_eq!(tok1.tok, tok2.tok);\n-            assert_eq!(tok1.sp, tok2.sp);\n-            assert_eq!(string_reader.next_token().tok, token::Whitespace);\n+            let tok2 = Token::new(\n+                token::Ident(Symbol::intern(\"fn\"), false),\n+                Span::new(BytePos(21), BytePos(23), NO_EXPANSION),\n+            );\n+            assert_eq!(tok1.kind, tok2.kind);\n+            assert_eq!(tok1.span, tok2.span);\n+            assert_eq!(string_reader.next_token(), token::Whitespace);\n             // the 'main' id is already read:\n             assert_eq!(string_reader.pos.clone(), BytePos(28));\n             // read another token:\n             let tok3 = string_reader.next_token();\n-            let tok4 = TokenAndSpan {\n-                tok: mk_ident(\"main\"),\n-                sp: Span::new(BytePos(24), BytePos(28), NO_EXPANSION),\n-            };\n-            assert_eq!(tok3.tok, tok4.tok);\n-            assert_eq!(tok3.sp, tok4.sp);\n+            let tok4 = Token::new(\n+                mk_ident(\"main\"),\n+                Span::new(BytePos(24), BytePos(28), NO_EXPANSION),\n+            );\n+            assert_eq!(tok3.kind, tok4.kind);\n+            assert_eq!(tok3.span, tok4.span);\n             // the lparen is already read:\n             assert_eq!(string_reader.pos.clone(), BytePos(29))\n         })\n     }\n \n     // check that the given reader produces the desired stream\n     // of tokens (stop checking after exhausting the expected vec)\n-    fn check_tokenization(mut string_reader: StringReader<'_>, expected: Vec<Token>) {\n+    fn check_tokenization(mut string_reader: StringReader<'_>, expected: Vec<TokenKind>) {\n         for expected_tok in &expected {\n-            assert_eq!(&string_reader.next_token().tok, expected_tok);\n+            assert_eq!(&string_reader.next_token(), expected_tok);\n         }\n     }\n \n     // make the identifier by looking up the string in the interner\n-    fn mk_ident(id: &str) -> Token {\n-        Token::from_ast_ident(Ident::from_str(id))\n+    fn mk_ident(id: &str) -> TokenKind {\n+        TokenKind::from_ast_ident(Ident::from_str(id))\n     }\n \n-    fn mk_lit(kind: token::LitKind, symbol: &str, suffix: Option<&str>) -> Token {\n-        Token::lit(kind, Symbol::intern(symbol), suffix.map(Symbol::intern))\n+    fn mk_lit(kind: token::LitKind, symbol: &str, suffix: Option<&str>) -> TokenKind {\n+        TokenKind::lit(kind, Symbol::intern(symbol), suffix.map(Symbol::intern))\n     }\n \n     #[test]\n@@ -1698,7 +1645,7 @@ mod tests {\n         with_default_globals(|| {\n             let sm = Lrc::new(SourceMap::new(FilePathMapping::empty()));\n             let sh = mk_sess(sm.clone());\n-            assert_eq!(setup(&sm, &sh, \"'a'\".to_string()).next_token().tok,\n+            assert_eq!(setup(&sm, &sh, \"'a'\".to_string()).next_token(),\n                        mk_lit(token::Char, \"a\", None));\n         })\n     }\n@@ -1708,7 +1655,7 @@ mod tests {\n         with_default_globals(|| {\n             let sm = Lrc::new(SourceMap::new(FilePathMapping::empty()));\n             let sh = mk_sess(sm.clone());\n-            assert_eq!(setup(&sm, &sh, \"' '\".to_string()).next_token().tok,\n+            assert_eq!(setup(&sm, &sh, \"' '\".to_string()).next_token(),\n                        mk_lit(token::Char, \" \", None));\n         })\n     }\n@@ -1718,7 +1665,7 @@ mod tests {\n         with_default_globals(|| {\n             let sm = Lrc::new(SourceMap::new(FilePathMapping::empty()));\n             let sh = mk_sess(sm.clone());\n-            assert_eq!(setup(&sm, &sh, \"'\\\\n'\".to_string()).next_token().tok,\n+            assert_eq!(setup(&sm, &sh, \"'\\\\n'\".to_string()).next_token(),\n                        mk_lit(token::Char, \"\\\\n\", None));\n         })\n     }\n@@ -1728,8 +1675,8 @@ mod tests {\n         with_default_globals(|| {\n             let sm = Lrc::new(SourceMap::new(FilePathMapping::empty()));\n             let sh = mk_sess(sm.clone());\n-            assert_eq!(setup(&sm, &sh, \"'abc\".to_string()).next_token().tok,\n-                       token::Lifetime(Ident::from_str(\"'abc\")));\n+            assert_eq!(setup(&sm, &sh, \"'abc\".to_string()).next_token(),\n+                       token::Lifetime(Symbol::intern(\"'abc\")));\n         })\n     }\n \n@@ -1738,7 +1685,7 @@ mod tests {\n         with_default_globals(|| {\n             let sm = Lrc::new(SourceMap::new(FilePathMapping::empty()));\n             let sh = mk_sess(sm.clone());\n-            assert_eq!(setup(&sm, &sh, \"r###\\\"\\\"#a\\\\b\\x00c\\\"\\\"###\".to_string()).next_token().tok,\n+            assert_eq!(setup(&sm, &sh, \"r###\\\"\\\"#a\\\\b\\x00c\\\"\\\"###\".to_string()).next_token(),\n                        mk_lit(token::StrRaw(3), \"\\\"#a\\\\b\\x00c\\\"\", None));\n         })\n     }\n@@ -1750,10 +1697,10 @@ mod tests {\n             let sh = mk_sess(sm.clone());\n             macro_rules! test {\n                 ($input: expr, $tok_type: ident, $tok_contents: expr) => {{\n-                    assert_eq!(setup(&sm, &sh, format!(\"{}suffix\", $input)).next_token().tok,\n+                    assert_eq!(setup(&sm, &sh, format!(\"{}suffix\", $input)).next_token(),\n                                mk_lit(token::$tok_type, $tok_contents, Some(\"suffix\")));\n                     // with a whitespace separator:\n-                    assert_eq!(setup(&sm, &sh, format!(\"{} suffix\", $input)).next_token().tok,\n+                    assert_eq!(setup(&sm, &sh, format!(\"{} suffix\", $input)).next_token(),\n                                mk_lit(token::$tok_type, $tok_contents, None));\n                 }}\n             }\n@@ -1768,11 +1715,11 @@ mod tests {\n             test!(\"1.0\", Float, \"1.0\");\n             test!(\"1.0e10\", Float, \"1.0e10\");\n \n-            assert_eq!(setup(&sm, &sh, \"2us\".to_string()).next_token().tok,\n+            assert_eq!(setup(&sm, &sh, \"2us\".to_string()).next_token(),\n                        mk_lit(token::Integer, \"2\", Some(\"us\")));\n-            assert_eq!(setup(&sm, &sh, \"r###\\\"raw\\\"###suffix\".to_string()).next_token().tok,\n+            assert_eq!(setup(&sm, &sh, \"r###\\\"raw\\\"###suffix\".to_string()).next_token(),\n                        mk_lit(token::StrRaw(3), \"raw\", Some(\"suffix\")));\n-            assert_eq!(setup(&sm, &sh, \"br###\\\"raw\\\"###suffix\".to_string()).next_token().tok,\n+            assert_eq!(setup(&sm, &sh, \"br###\\\"raw\\\"###suffix\".to_string()).next_token(),\n                        mk_lit(token::ByteStrRaw(3), \"raw\", Some(\"suffix\")));\n         })\n     }\n@@ -1790,11 +1737,8 @@ mod tests {\n             let sm = Lrc::new(SourceMap::new(FilePathMapping::empty()));\n             let sh = mk_sess(sm.clone());\n             let mut lexer = setup(&sm, &sh, \"/* /* */ */'a'\".to_string());\n-            match lexer.next_token().tok {\n-                token::Comment => {}\n-                _ => panic!(\"expected a comment!\"),\n-            }\n-            assert_eq!(lexer.next_token().tok, mk_lit(token::Char, \"a\", None));\n+            assert_eq!(lexer.next_token(), token::Comment);\n+            assert_eq!(lexer.next_token(), mk_lit(token::Char, \"a\", None));\n         })\n     }\n \n@@ -1805,11 +1749,10 @@ mod tests {\n             let sh = mk_sess(sm.clone());\n             let mut lexer = setup(&sm, &sh, \"// test\\r\\n/// test\\r\\n\".to_string());\n             let comment = lexer.next_token();\n-            assert_eq!(comment.tok, token::Comment);\n-            assert_eq!((comment.sp.lo(), comment.sp.hi()), (BytePos(0), BytePos(7)));\n-            assert_eq!(lexer.next_token().tok, token::Whitespace);\n-            assert_eq!(lexer.next_token().tok,\n-                    token::DocComment(Symbol::intern(\"/// test\")));\n+            assert_eq!(comment.kind, token::Comment);\n+            assert_eq!((comment.span.lo(), comment.span.hi()), (BytePos(0), BytePos(7)));\n+            assert_eq!(lexer.next_token(), token::Whitespace);\n+            assert_eq!(lexer.next_token(), token::DocComment(Symbol::intern(\"/// test\")));\n         })\n     }\n }"}, {"sha": "b809f99beba339dcb7e3fab2cf3875670c61acc1", "filename": "src/libsyntax/parse/lexer/tokentrees.rs", "status": "modified", "additions": 20, "deletions": 23, "changes": 43, "blob_url": "https://github.com/rust-lang/rust/blob/ca1bcfdde3f19afd68ef808cecf2ce56d08d5df4/src%2Flibsyntax%2Fparse%2Flexer%2Ftokentrees.rs", "raw_url": "https://github.com/rust-lang/rust/raw/ca1bcfdde3f19afd68ef808cecf2ce56d08d5df4/src%2Flibsyntax%2Fparse%2Flexer%2Ftokentrees.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Flexer%2Ftokentrees.rs?ref=ca1bcfdde3f19afd68ef808cecf2ce56d08d5df4", "patch": "@@ -2,15 +2,15 @@ use syntax_pos::Span;\n \n use crate::print::pprust::token_to_string;\n use crate::parse::lexer::{StringReader, UnmatchedBrace};\n-use crate::parse::{token, PResult};\n+use crate::parse::token::{self, Token};\n+use crate::parse::PResult;\n use crate::tokenstream::{DelimSpan, IsJoint::*, TokenStream, TokenTree, TreeAndJoint};\n \n impl<'a> StringReader<'a> {\n     crate fn into_token_trees(self) -> (PResult<'a, TokenStream>, Vec<UnmatchedBrace>) {\n         let mut tt_reader = TokenTreesReader {\n             string_reader: self,\n-            token: token::Eof,\n-            span: syntax_pos::DUMMY_SP,\n+            token: Token::dummy(),\n             open_braces: Vec::new(),\n             unmatched_braces: Vec::new(),\n             matching_delim_spans: Vec::new(),\n@@ -23,8 +23,7 @@ impl<'a> StringReader<'a> {\n \n struct TokenTreesReader<'a> {\n     string_reader: StringReader<'a>,\n-    token: token::Token,\n-    span: Span,\n+    token: Token,\n     /// Stack of open delimiters and their spans. Used for error message.\n     open_braces: Vec<(token::DelimToken, Span)>,\n     unmatched_braces: Vec<UnmatchedBrace>,\n@@ -52,7 +51,7 @@ impl<'a> TokenTreesReader<'a> {\n     fn parse_token_trees_until_close_delim(&mut self) -> TokenStream {\n         let mut tts = vec![];\n         loop {\n-            if let token::CloseDelim(..) = self.token {\n+            if let token::CloseDelim(..) = self.token.kind {\n                 return TokenStream::new(tts);\n             }\n \n@@ -68,11 +67,11 @@ impl<'a> TokenTreesReader<'a> {\n \n     fn parse_token_tree(&mut self) -> PResult<'a, TreeAndJoint> {\n         let sm = self.string_reader.sess.source_map();\n-        match self.token {\n+        match self.token.kind {\n             token::Eof => {\n                 let msg = \"this file contains an un-closed delimiter\";\n                 let mut err = self.string_reader.sess.span_diagnostic\n-                    .struct_span_err(self.span, msg);\n+                    .struct_span_err(self.token.span, msg);\n                 for &(_, sp) in &self.open_braces {\n                     err.span_label(sp, \"un-closed delimiter\");\n                 }\n@@ -102,10 +101,10 @@ impl<'a> TokenTreesReader<'a> {\n             },\n             token::OpenDelim(delim) => {\n                 // The span for beginning of the delimited section\n-                let pre_span = self.span;\n+                let pre_span = self.token.span;\n \n                 // Parse the open delimiter.\n-                self.open_braces.push((delim, self.span));\n+                self.open_braces.push((delim, self.token.span));\n                 self.real_token();\n \n                 // Parse the token trees within the delimiters.\n@@ -114,9 +113,9 @@ impl<'a> TokenTreesReader<'a> {\n                 let tts = self.parse_token_trees_until_close_delim();\n \n                 // Expand to cover the entire delimited token tree\n-                let delim_span = DelimSpan::from_pair(pre_span, self.span);\n+                let delim_span = DelimSpan::from_pair(pre_span, self.token.span);\n \n-                match self.token {\n+                match self.token.kind {\n                     // Correct delimiter.\n                     token::CloseDelim(d) if d == delim => {\n                         let (open_brace, open_brace_span) = self.open_braces.pop().unwrap();\n@@ -126,7 +125,7 @@ impl<'a> TokenTreesReader<'a> {\n                             self.matching_delim_spans.clear();\n                         } else {\n                             self.matching_delim_spans.push(\n-                                (open_brace, open_brace_span, self.span),\n+                                (open_brace, open_brace_span, self.token.span),\n                             );\n                         }\n                         // Parse the close delimiter.\n@@ -136,16 +135,16 @@ impl<'a> TokenTreesReader<'a> {\n                     token::CloseDelim(other) => {\n                         let mut unclosed_delimiter = None;\n                         let mut candidate = None;\n-                        if self.last_unclosed_found_span != Some(self.span) {\n+                        if self.last_unclosed_found_span != Some(self.token.span) {\n                             // do not complain about the same unclosed delimiter multiple times\n-                            self.last_unclosed_found_span = Some(self.span);\n+                            self.last_unclosed_found_span = Some(self.token.span);\n                             // This is a conservative error: only report the last unclosed\n                             // delimiter. The previous unclosed delimiters could actually be\n                             // closed! The parser just hasn't gotten to them yet.\n                             if let Some(&(_, sp)) = self.open_braces.last() {\n                                 unclosed_delimiter = Some(sp);\n                             };\n-                            if let Some(current_padding) = sm.span_to_margin(self.span) {\n+                            if let Some(current_padding) = sm.span_to_margin(self.token.span) {\n                                 for (brace, brace_span) in &self.open_braces {\n                                     if let Some(padding) = sm.span_to_margin(*brace_span) {\n                                         // high likelihood of these two corresponding\n@@ -159,7 +158,7 @@ impl<'a> TokenTreesReader<'a> {\n                             self.unmatched_braces.push(UnmatchedBrace {\n                                 expected_delim: tok,\n                                 found_delim: other,\n-                                found_span: self.span,\n+                                found_span: self.token.span,\n                                 unclosed_span: unclosed_delimiter,\n                                 candidate_span: candidate,\n                             });\n@@ -198,12 +197,12 @@ impl<'a> TokenTreesReader<'a> {\n                 let token_str = token_to_string(&self.token);\n                 let msg = format!(\"unexpected close delimiter: `{}`\", token_str);\n                 let mut err = self.string_reader.sess.span_diagnostic\n-                    .struct_span_err(self.span, &msg);\n-                err.span_label(self.span, \"unexpected close delimiter\");\n+                    .struct_span_err(self.token.span, &msg);\n+                err.span_label(self.token.span, \"unexpected close delimiter\");\n                 Err(err)\n             },\n             _ => {\n-                let tt = TokenTree::Token(self.span, self.token.clone());\n+                let tt = TokenTree::Token(self.token.take());\n                 // Note that testing for joint-ness here is done via the raw\n                 // source span as the joint-ness is a property of the raw source\n                 // rather than wanting to take `override_span` into account.\n@@ -219,8 +218,6 @@ impl<'a> TokenTreesReader<'a> {\n     }\n \n     fn real_token(&mut self) {\n-        let t = self.string_reader.real_token();\n-        self.token = t.tok;\n-        self.span = t.sp;\n+        self.token = self.string_reader.real_token();\n     }\n }"}, {"sha": "7d5356ffe4d8dc08835e7f4de3fdb94caaeb7291", "filename": "src/libsyntax/parse/literal.rs", "status": "modified", "additions": 21, "deletions": 22, "changes": 43, "blob_url": "https://github.com/rust-lang/rust/blob/ca1bcfdde3f19afd68ef808cecf2ce56d08d5df4/src%2Flibsyntax%2Fparse%2Fliteral.rs", "raw_url": "https://github.com/rust-lang/rust/raw/ca1bcfdde3f19afd68ef808cecf2ce56d08d5df4/src%2Flibsyntax%2Fparse%2Fliteral.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Fliteral.rs?ref=ca1bcfdde3f19afd68ef808cecf2ce56d08d5df4", "patch": "@@ -1,9 +1,9 @@\n //! Code related to parsing literals.\n \n-use crate::ast::{self, Ident, Lit, LitKind};\n+use crate::ast::{self, Lit, LitKind};\n use crate::parse::parser::Parser;\n use crate::parse::PResult;\n-use crate::parse::token::{self, Token};\n+use crate::parse::token::{self, Token, TokenKind};\n use crate::parse::unescape::{unescape_str, unescape_char, unescape_byte_str, unescape_byte};\n use crate::print::pprust;\n use crate::symbol::{kw, sym, Symbol};\n@@ -228,10 +228,10 @@ impl Lit {\n     }\n \n     /// Converts arbitrary token into an AST literal.\n-    crate fn from_token(token: &Token, span: Span) -> Result<Lit, LitError> {\n-        let lit = match *token {\n-            token::Ident(ident, false) if ident.name == kw::True || ident.name == kw::False =>\n-                token::Lit::new(token::Bool, ident.name, None),\n+    crate fn from_token(token: &Token) -> Result<Lit, LitError> {\n+        let lit = match token.kind {\n+            token::Ident(name, false) if name == kw::True || name == kw::False =>\n+                token::Lit::new(token::Bool, name, None),\n             token::Literal(lit) =>\n                 lit,\n             token::Interpolated(ref nt) => {\n@@ -245,7 +245,7 @@ impl Lit {\n             _ => return Err(LitError::NotLiteral)\n         };\n \n-        Lit::from_lit_token(lit, span)\n+        Lit::from_lit_token(lit, token.span)\n     }\n \n     /// Attempts to recover an AST literal from semantic literal.\n@@ -258,10 +258,10 @@ impl Lit {\n     /// Losslessly convert an AST literal into a token stream.\n     crate fn tokens(&self) -> TokenStream {\n         let token = match self.token.kind {\n-            token::Bool => token::Ident(Ident::new(self.token.symbol, self.span), false),\n+            token::Bool => token::Ident(self.token.symbol, false),\n             _ => token::Literal(self.token),\n         };\n-        TokenTree::Token(self.span, token).into()\n+        TokenTree::token(token, self.span).into()\n     }\n }\n \n@@ -272,44 +272,43 @@ impl<'a> Parser<'a> {\n         if self.token == token::Dot {\n             // Attempt to recover `.4` as `0.4`.\n             recovered = self.look_ahead(1, |t| {\n-                if let token::Literal(token::Lit { kind: token::Integer, symbol, suffix }) = *t {\n+                if let token::Literal(token::Lit { kind: token::Integer, symbol, suffix })\n+                        = t.kind {\n                     let next_span = self.look_ahead_span(1);\n                     if self.span.hi() == next_span.lo() {\n                         let s = String::from(\"0.\") + &symbol.as_str();\n-                        let token = Token::lit(token::Float, Symbol::intern(&s), suffix);\n-                        return Some((token, self.span.to(next_span)));\n+                        let kind = TokenKind::lit(token::Float, Symbol::intern(&s), suffix);\n+                        return Some(Token::new(kind, self.span.to(next_span)));\n                     }\n                 }\n                 None\n             });\n-            if let Some((ref token, span)) = recovered {\n+            if let Some(token) = &recovered {\n                 self.bump();\n                 self.diagnostic()\n-                    .struct_span_err(span, \"float literals must have an integer part\")\n+                    .struct_span_err(token.span, \"float literals must have an integer part\")\n                     .span_suggestion(\n-                        span,\n+                        token.span,\n                         \"must have an integer part\",\n-                        pprust::token_to_string(&token),\n+                        pprust::token_to_string(token),\n                         Applicability::MachineApplicable,\n                     )\n                     .emit();\n             }\n         }\n \n-        let (token, span) = recovered.as_ref().map_or((&self.token, self.span),\n-                                                      |(token, span)| (token, *span));\n-\n-        match Lit::from_token(token, span) {\n+        let token = recovered.as_ref().unwrap_or(&self.token);\n+        match Lit::from_token(token) {\n             Ok(lit) => {\n                 self.bump();\n                 Ok(lit)\n             }\n             Err(LitError::NotLiteral) => {\n                 let msg = format!(\"unexpected token: {}\", self.this_token_descr());\n-                Err(self.span_fatal(span, &msg))\n+                Err(self.span_fatal(token.span, &msg))\n             }\n             Err(err) => {\n-                let lit = token.expect_lit();\n+                let (lit, span) = (token.expect_lit(), token.span);\n                 self.bump();\n                 err.report(&self.sess.span_diagnostic, lit, span);\n                 let lit = token::Lit::new(token::Err, lit.symbol, lit.suffix);"}, {"sha": "063823bbf4d11a26282cb7f7b4980b61ac1fe902", "filename": "src/libsyntax/parse/mod.rs", "status": "modified", "additions": 33, "deletions": 32, "changes": 65, "blob_url": "https://github.com/rust-lang/rust/blob/ca1bcfdde3f19afd68ef808cecf2ce56d08d5df4/src%2Flibsyntax%2Fparse%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/ca1bcfdde3f19afd68ef808cecf2ce56d08d5df4/src%2Flibsyntax%2Fparse%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Fmod.rs?ref=ca1bcfdde3f19afd68ef808cecf2ce56d08d5df4", "patch": "@@ -5,7 +5,8 @@ use crate::early_buffered_lints::{BufferedEarlyLint, BufferedEarlyLintId};\n use crate::source_map::{SourceMap, FilePathMapping};\n use crate::feature_gate::UnstableFeatures;\n use crate::parse::parser::Parser;\n-use crate::syntax::parse::parser::emit_unclosed_delims;\n+use crate::parse::parser::emit_unclosed_delims;\n+use crate::parse::token::TokenKind;\n use crate::tokenstream::{TokenStream, TokenTree};\n use crate::diagnostics::plugin::ErrorMap;\n use crate::print::pprust::token_to_string;\n@@ -239,7 +240,7 @@ fn maybe_source_file_to_parser(\n     let mut parser = stream_to_parser(sess, stream, None);\n     parser.unclosed_delims = unclosed_delims;\n     if parser.token == token::Eof && parser.span.is_dummy() {\n-        parser.span = Span::new(end_pos, end_pos, parser.span.ctxt());\n+        parser.token.span = Span::new(end_pos, end_pos, parser.span.ctxt());\n     }\n \n     Ok(parser)\n@@ -311,7 +312,7 @@ pub fn maybe_file_to_stream(\n             for unmatched in unmatched_braces {\n                 let mut db = sess.span_diagnostic.struct_span_err(unmatched.found_span, &format!(\n                     \"incorrect close delimiter: `{}`\",\n-                    token_to_string(&token::Token::CloseDelim(unmatched.found_delim)),\n+                    token_to_string(&token::CloseDelim(unmatched.found_delim)),\n                 ));\n                 db.span_label(unmatched.found_span, \"incorrect close delimiter\");\n                 if let Some(sp) = unmatched.candidate_span {\n@@ -358,13 +359,13 @@ pub fn stream_to_parser_with_base_dir<'a>(\n /// A sequence separator.\n pub struct SeqSep {\n     /// The seperator token.\n-    pub sep: Option<token::Token>,\n+    pub sep: Option<TokenKind>,\n     /// `true` if a trailing separator is allowed.\n     pub trailing_sep_allowed: bool,\n }\n \n impl SeqSep {\n-    pub fn trailing_allowed(t: token::Token) -> SeqSep {\n+    pub fn trailing_allowed(t: TokenKind) -> SeqSep {\n         SeqSep {\n             sep: Some(t),\n             trailing_sep_allowed: true,\n@@ -382,10 +383,12 @@ impl SeqSep {\n #[cfg(test)]\n mod tests {\n     use super::*;\n-    use crate::ast::{self, Ident, PatKind};\n+    use crate::ast::{self, Name, PatKind};\n     use crate::attr::first_attr_value_str_by_name;\n     use crate::ptr::P;\n+    use crate::parse::token::Token;\n     use crate::print::pprust::item_to_string;\n+    use crate::symbol::{kw, sym};\n     use crate::tokenstream::{DelimSpan, TokenTree};\n     use crate::util::parser_testing::string_to_stream;\n     use crate::util::parser_testing::{string_to_expr, string_to_item};\n@@ -417,49 +420,52 @@ mod tests {\n     #[test]\n     fn string_to_tts_macro () {\n         with_default_globals(|| {\n-            use crate::symbol::sym;\n-\n             let tts: Vec<_> =\n                 string_to_stream(\"macro_rules! zip (($a)=>($a))\".to_string()).trees().collect();\n             let tts: &[TokenTree] = &tts[..];\n \n             match (tts.len(), tts.get(0), tts.get(1), tts.get(2), tts.get(3)) {\n                 (\n                     4,\n-                    Some(&TokenTree::Token(_, token::Ident(name_macro_rules, false))),\n-                    Some(&TokenTree::Token(_, token::Not)),\n-                    Some(&TokenTree::Token(_, token::Ident(name_zip, false))),\n+                    Some(&TokenTree::Token(Token {\n+                        kind: token::Ident(name_macro_rules, false), ..\n+                    })),\n+                    Some(&TokenTree::Token(Token { kind: token::Not, .. })),\n+                    Some(&TokenTree::Token(Token { kind: token::Ident(name_zip, false), .. })),\n                     Some(&TokenTree::Delimited(_, macro_delim, ref macro_tts)),\n                 )\n-                if name_macro_rules.name == sym::macro_rules\n-                && name_zip.name.as_str() == \"zip\" => {\n+                if name_macro_rules == sym::macro_rules && name_zip.as_str() == \"zip\" => {\n                     let tts = &macro_tts.trees().collect::<Vec<_>>();\n                     match (tts.len(), tts.get(0), tts.get(1), tts.get(2)) {\n                         (\n                             3,\n                             Some(&TokenTree::Delimited(_, first_delim, ref first_tts)),\n-                            Some(&TokenTree::Token(_, token::FatArrow)),\n+                            Some(&TokenTree::Token(Token { kind: token::FatArrow, .. })),\n                             Some(&TokenTree::Delimited(_, second_delim, ref second_tts)),\n                         )\n                         if macro_delim == token::Paren => {\n                             let tts = &first_tts.trees().collect::<Vec<_>>();\n                             match (tts.len(), tts.get(0), tts.get(1)) {\n                                 (\n                                     2,\n-                                    Some(&TokenTree::Token(_, token::Dollar)),\n-                                    Some(&TokenTree::Token(_, token::Ident(ident, false))),\n+                                    Some(&TokenTree::Token(Token { kind: token::Dollar, .. })),\n+                                    Some(&TokenTree::Token(Token {\n+                                        kind: token::Ident(name, false), ..\n+                                    })),\n                                 )\n-                                if first_delim == token::Paren && ident.name.as_str() == \"a\" => {},\n+                                if first_delim == token::Paren && name.as_str() == \"a\" => {},\n                                 _ => panic!(\"value 3: {:?} {:?}\", first_delim, first_tts),\n                             }\n                             let tts = &second_tts.trees().collect::<Vec<_>>();\n                             match (tts.len(), tts.get(0), tts.get(1)) {\n                                 (\n                                     2,\n-                                    Some(&TokenTree::Token(_, token::Dollar)),\n-                                    Some(&TokenTree::Token(_, token::Ident(ident, false))),\n+                                    Some(&TokenTree::Token(Token { kind: token::Dollar, .. })),\n+                                    Some(&TokenTree::Token(Token {\n+                                        kind: token::Ident(name, false), ..\n+                                    })),\n                                 )\n-                                if second_delim == token::Paren && ident.name.as_str() == \"a\" => {},\n+                                if second_delim == token::Paren && name.as_str() == \"a\" => {},\n                                 _ => panic!(\"value 4: {:?} {:?}\", second_delim, second_tts),\n                             }\n                         },\n@@ -477,26 +483,23 @@ mod tests {\n             let tts = string_to_stream(\"fn a (b : i32) { b; }\".to_string());\n \n             let expected = TokenStream::new(vec![\n-                TokenTree::Token(sp(0, 2), token::Ident(Ident::from_str(\"fn\"), false)).into(),\n-                TokenTree::Token(sp(3, 4), token::Ident(Ident::from_str(\"a\"), false)).into(),\n+                TokenTree::token(token::Ident(kw::Fn, false), sp(0, 2)).into(),\n+                TokenTree::token(token::Ident(Name::intern(\"a\"), false), sp(3, 4)).into(),\n                 TokenTree::Delimited(\n                     DelimSpan::from_pair(sp(5, 6), sp(13, 14)),\n                     token::DelimToken::Paren,\n                     TokenStream::new(vec![\n-                        TokenTree::Token(sp(6, 7),\n-                                         token::Ident(Ident::from_str(\"b\"), false)).into(),\n-                        TokenTree::Token(sp(8, 9), token::Colon).into(),\n-                        TokenTree::Token(sp(10, 13),\n-                                         token::Ident(Ident::from_str(\"i32\"), false)).into(),\n+                        TokenTree::token(token::Ident(Name::intern(\"b\"), false), sp(6, 7)).into(),\n+                        TokenTree::token(token::Colon, sp(8, 9)).into(),\n+                        TokenTree::token(token::Ident(sym::i32, false), sp(10, 13)).into(),\n                     ]).into(),\n                 ).into(),\n                 TokenTree::Delimited(\n                     DelimSpan::from_pair(sp(15, 16), sp(20, 21)),\n                     token::DelimToken::Brace,\n                     TokenStream::new(vec![\n-                        TokenTree::Token(sp(17, 18),\n-                                         token::Ident(Ident::from_str(\"b\"), false)).into(),\n-                        TokenTree::Token(sp(18, 19), token::Semi).into(),\n+                        TokenTree::token(token::Ident(Name::intern(\"b\"), false), sp(17, 18)).into(),\n+                        TokenTree::token(token::Semi, sp(18, 19)).into(),\n                     ]).into(),\n                 ).into()\n             ]);\n@@ -603,8 +606,6 @@ mod tests {\n \n     #[test] fn crlf_doc_comments() {\n         with_default_globals(|| {\n-            use crate::symbol::sym;\n-\n             let sess = ParseSess::new(FilePathMapping::empty());\n \n             let name_1 = FileName::Custom(\"crlf_source_1\".to_string());"}, {"sha": "43e7c9330e418ead1b52b47b1d22de011e33bcc6", "filename": "src/libsyntax/parse/parser.rs", "status": "modified", "additions": 151, "deletions": 146, "changes": 297, "blob_url": "https://github.com/rust-lang/rust/blob/ca1bcfdde3f19afd68ef808cecf2ce56d08d5df4/src%2Flibsyntax%2Fparse%2Fparser.rs", "raw_url": "https://github.com/rust-lang/rust/raw/ca1bcfdde3f19afd68ef808cecf2ce56d08d5df4/src%2Flibsyntax%2Fparse%2Fparser.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Fparser.rs?ref=ca1bcfdde3f19afd68ef808cecf2ce56d08d5df4", "patch": "@@ -36,9 +36,9 @@ use crate::{ast, attr};\n use crate::ext::base::DummyResult;\n use crate::source_map::{self, SourceMap, Spanned, respan};\n use crate::parse::{SeqSep, classify, literal, token};\n-use crate::parse::lexer::{TokenAndSpan, UnmatchedBrace};\n+use crate::parse::lexer::UnmatchedBrace;\n use crate::parse::lexer::comments::{doc_comment_style, strip_doc_comment_decoration};\n-use crate::parse::token::DelimToken;\n+use crate::parse::token::{Token, TokenKind, DelimToken};\n use crate::parse::{new_sub_parser_from_file, ParseSess, Directory, DirectoryOwnership};\n use crate::util::parser::{AssocOp, Fixity};\n use crate::print::pprust;\n@@ -57,6 +57,7 @@ use log::debug;\n use std::borrow::Cow;\n use std::cmp;\n use std::mem;\n+use std::ops::Deref;\n use std::path::{self, Path, PathBuf};\n use std::slice;\n \n@@ -121,7 +122,7 @@ crate enum BlockMode {\n /// `token::Interpolated` tokens.\n macro_rules! maybe_whole_expr {\n     ($p:expr) => {\n-        if let token::Interpolated(nt) = &$p.token {\n+        if let token::Interpolated(nt) = &$p.token.kind {\n             match &**nt {\n                 token::NtExpr(e) | token::NtLiteral(e) => {\n                     let e = e.clone();\n@@ -147,7 +148,7 @@ macro_rules! maybe_whole_expr {\n /// As maybe_whole_expr, but for things other than expressions\n macro_rules! maybe_whole {\n     ($p:expr, $constructor:ident, |$x:ident| $e:expr) => {\n-        if let token::Interpolated(nt) = &$p.token {\n+        if let token::Interpolated(nt) = &$p.token.kind {\n             if let token::$constructor(x) = &**nt {\n                 let $x = x.clone();\n                 $p.bump();\n@@ -161,7 +162,7 @@ macro_rules! maybe_whole {\n macro_rules! maybe_recover_from_interpolated_ty_qpath {\n     ($self: expr, $allow_qpath_recovery: expr) => {\n         if $allow_qpath_recovery && $self.look_ahead(1, |t| t == &token::ModSep) {\n-            if let token::Interpolated(nt) = &$self.token {\n+            if let token::Interpolated(nt) = &$self.token.kind {\n                 if let token::NtTy(ty) = &**nt {\n                     let ty = ty.clone();\n                     $self.bump();\n@@ -196,14 +197,17 @@ enum PrevTokenKind {\n #[derive(Clone)]\n pub struct Parser<'a> {\n     pub sess: &'a ParseSess,\n-    /// The current token.\n-    pub token: token::Token,\n-    /// The span of the current token.\n-    pub span: Span,\n+    /// The current normalized token.\n+    /// \"Normalized\" means that some interpolated tokens\n+    /// (`$i: ident` and `$l: lifetime` meta-variables) are replaced\n+    /// with non-interpolated identifier and lifetime tokens they refer to.\n+    /// Perhaps the normalized / non-normalized setup can be simplified somehow.\n+    pub token: Token,\n+    /// Span of the current non-normalized token.\n     meta_var_span: Option<Span>,\n-    /// The span of the previous token.\n+    /// Span of the previous non-normalized token.\n     pub prev_span: Span,\n-    /// The kind of the previous troken.\n+    /// Kind of the previous normalized token (in simplified form).\n     prev_token_kind: PrevTokenKind,\n     restrictions: Restrictions,\n     /// Used to determine the path to externally loaded source files.\n@@ -242,6 +246,15 @@ impl<'a> Drop for Parser<'a> {\n     }\n }\n \n+// FIXME: Parser uses `self.span` all the time.\n+// Remove this impl if you think that using `self.token.span` instead is acceptable.\n+impl Deref for Parser<'_> {\n+    type Target = Token;\n+    fn deref(&self) -> &Self::Target {\n+        &self.token\n+    }\n+}\n+\n #[derive(Clone)]\n crate struct TokenCursor {\n     crate frame: TokenCursorFrame,\n@@ -295,7 +308,7 @@ impl TokenCursorFrame {\n }\n \n impl TokenCursor {\n-    fn next(&mut self) -> TokenAndSpan {\n+    fn next(&mut self) -> Token {\n         loop {\n             let tree = if !self.frame.open_delim {\n                 self.frame.open_delim = true;\n@@ -309,7 +322,7 @@ impl TokenCursor {\n                 self.frame = frame;\n                 continue\n             } else {\n-                return TokenAndSpan { tok: token::Eof, sp: DUMMY_SP }\n+                return Token::new(token::Eof, DUMMY_SP);\n             };\n \n             match self.frame.last_token {\n@@ -318,7 +331,7 @@ impl TokenCursor {\n             }\n \n             match tree {\n-                TokenTree::Token(sp, tok) => return TokenAndSpan { tok: tok, sp: sp },\n+                TokenTree::Token(token) => return token,\n                 TokenTree::Delimited(sp, delim, tts) => {\n                     let frame = TokenCursorFrame::new(sp, delim, &tts);\n                     self.stack.push(mem::replace(&mut self.frame, frame));\n@@ -327,9 +340,9 @@ impl TokenCursor {\n         }\n     }\n \n-    fn next_desugared(&mut self) -> TokenAndSpan {\n-        let (sp, name) = match self.next() {\n-            TokenAndSpan { sp, tok: token::DocComment(name) } => (sp, name),\n+    fn next_desugared(&mut self) -> Token {\n+        let (name, sp) = match self.next() {\n+            Token { kind: token::DocComment(name), span } => (name, span),\n             tok => return tok,\n         };\n \n@@ -353,11 +366,11 @@ impl TokenCursor {\n             delim_span,\n             token::Bracket,\n             [\n-                TokenTree::Token(sp, token::Ident(ast::Ident::with_empty_ctxt(sym::doc), false)),\n-                TokenTree::Token(sp, token::Eq),\n-                TokenTree::Token(sp, token::Token::lit(\n+                TokenTree::token(token::Ident(sym::doc, false), sp),\n+                TokenTree::token(token::Eq, sp),\n+                TokenTree::token(TokenKind::lit(\n                     token::StrRaw(num_of_hashes), Symbol::intern(&stripped), None\n-                )),\n+                ), sp),\n             ]\n             .iter().cloned().collect::<TokenStream>().into(),\n         );\n@@ -366,10 +379,10 @@ impl TokenCursor {\n             delim_span,\n             token::NoDelim,\n             &if doc_comment_style(&name.as_str()) == AttrStyle::Inner {\n-                [TokenTree::Token(sp, token::Pound), TokenTree::Token(sp, token::Not), body]\n+                [TokenTree::token(token::Pound, sp), TokenTree::token(token::Not, sp), body]\n                     .iter().cloned().collect::<TokenStream>().into()\n             } else {\n-                [TokenTree::Token(sp, token::Pound), body]\n+                [TokenTree::token(token::Pound, sp), body]\n                     .iter().cloned().collect::<TokenStream>().into()\n             },\n         )));\n@@ -380,7 +393,7 @@ impl TokenCursor {\n \n #[derive(Clone, PartialEq)]\n crate enum TokenType {\n-    Token(token::Token),\n+    Token(TokenKind),\n     Keyword(Symbol),\n     Operator,\n     Lifetime,\n@@ -410,7 +423,7 @@ impl TokenType {\n ///\n /// Types can also be of the form `IDENT(u8, u8) -> u8`, however this assumes\n /// that `IDENT` is not the ident of a fn trait.\n-fn can_continue_type_after_non_fn_ident(t: &token::Token) -> bool {\n+fn can_continue_type_after_non_fn_ident(t: &TokenKind) -> bool {\n     t == &token::ModSep || t == &token::Lt ||\n     t == &token::BinOp(token::Shl)\n }\n@@ -468,8 +481,7 @@ impl<'a> Parser<'a> {\n     ) -> Self {\n         let mut parser = Parser {\n             sess,\n-            token: token::Whitespace,\n-            span: DUMMY_SP,\n+            token: Token::dummy(),\n             prev_span: DUMMY_SP,\n             meta_var_span: None,\n             prev_token_kind: PrevTokenKind::Other,\n@@ -498,9 +510,7 @@ impl<'a> Parser<'a> {\n             subparser_name,\n         };\n \n-        let tok = parser.next_tok();\n-        parser.token = tok.tok;\n-        parser.span = tok.sp;\n+        parser.token = parser.next_tok();\n \n         if let Some(directory) = directory {\n             parser.directory = directory;\n@@ -515,15 +525,15 @@ impl<'a> Parser<'a> {\n         parser\n     }\n \n-    fn next_tok(&mut self) -> TokenAndSpan {\n+    fn next_tok(&mut self) -> Token {\n         let mut next = if self.desugar_doc_comments {\n             self.token_cursor.next_desugared()\n         } else {\n             self.token_cursor.next()\n         };\n-        if next.sp.is_dummy() {\n+        if next.span.is_dummy() {\n             // Tweak the location for better diagnostics, but keep syntactic context intact.\n-            next.sp = self.prev_span.with_ctxt(next.sp.ctxt());\n+            next.span = self.prev_span.with_ctxt(next.span.ctxt());\n         }\n         next\n     }\n@@ -534,10 +544,10 @@ impl<'a> Parser<'a> {\n     }\n \n     crate fn token_descr(&self) -> Option<&'static str> {\n-        Some(match &self.token {\n-            t if t.is_special_ident() => \"reserved identifier\",\n-            t if t.is_used_keyword() => \"keyword\",\n-            t if t.is_unused_keyword() => \"reserved keyword\",\n+        Some(match &self.token.kind {\n+            _ if self.token.is_special_ident() => \"reserved identifier\",\n+            _ if self.token.is_used_keyword() => \"keyword\",\n+            _ if self.token.is_unused_keyword() => \"reserved keyword\",\n             token::DocComment(..) => \"doc comment\",\n             _ => return None,\n         })\n@@ -559,7 +569,7 @@ impl<'a> Parser<'a> {\n     }\n \n     /// Expects and consumes the token `t`. Signals an error if the next token is not `t`.\n-    pub fn expect(&mut self, t: &token::Token) -> PResult<'a, bool /* recovered */> {\n+    pub fn expect(&mut self, t: &TokenKind) -> PResult<'a, bool /* recovered */> {\n         if self.expected_tokens.is_empty() {\n             if self.token == *t {\n                 self.bump();\n@@ -577,8 +587,8 @@ impl<'a> Parser<'a> {\n     /// anything.  Signal a fatal error if next token is unexpected.\n     pub fn expect_one_of(\n         &mut self,\n-        edible: &[token::Token],\n-        inedible: &[token::Token],\n+        edible: &[TokenKind],\n+        inedible: &[TokenKind],\n     ) -> PResult<'a, bool /* recovered */> {\n         if edible.contains(&self.token) {\n             self.bump();\n@@ -612,8 +622,8 @@ impl<'a> Parser<'a> {\n     }\n \n     fn parse_ident_common(&mut self, recover: bool) -> PResult<'a, ast::Ident> {\n-        match self.token {\n-            token::Ident(ident, _) => {\n+        match self.token.kind {\n+            token::Ident(name, _) => {\n                 if self.token.is_reserved_ident() {\n                     let mut err = self.expected_ident_found();\n                     if recover {\n@@ -624,7 +634,7 @@ impl<'a> Parser<'a> {\n                 }\n                 let span = self.span;\n                 self.bump();\n-                Ok(Ident::new(ident.name, span))\n+                Ok(Ident::new(name, span))\n             }\n             _ => {\n                 Err(if self.prev_token_kind == PrevTokenKind::DocComment {\n@@ -640,14 +650,14 @@ impl<'a> Parser<'a> {\n     ///\n     /// This method will automatically add `tok` to `expected_tokens` if `tok` is not\n     /// encountered.\n-    crate fn check(&mut self, tok: &token::Token) -> bool {\n+    crate fn check(&mut self, tok: &TokenKind) -> bool {\n         let is_present = self.token == *tok;\n         if !is_present { self.expected_tokens.push(TokenType::Token(tok.clone())); }\n         is_present\n     }\n \n     /// Consumes a token 'tok' if it exists. Returns whether the given token was present.\n-    pub fn eat(&mut self, tok: &token::Token) -> bool {\n+    pub fn eat(&mut self, tok: &TokenKind) -> bool {\n         let is_present = self.check(tok);\n         if is_present { self.bump() }\n         is_present\n@@ -732,7 +742,7 @@ impl<'a> Parser<'a> {\n     /// See issue #47856 for an example of when this may occur.\n     fn eat_plus(&mut self) -> bool {\n         self.expected_tokens.push(TokenType::Token(token::BinOp(token::Plus)));\n-        match self.token {\n+        match self.token.kind {\n             token::BinOp(token::Plus) => {\n                 self.bump();\n                 true\n@@ -763,7 +773,7 @@ impl<'a> Parser<'a> {\n     /// `&` and continues. If an `&` is not seen, signals an error.\n     fn expect_and(&mut self) -> PResult<'a, ()> {\n         self.expected_tokens.push(TokenType::Token(token::BinOp(token::And)));\n-        match self.token {\n+        match self.token.kind {\n             token::BinOp(token::And) => {\n                 self.bump();\n                 Ok(())\n@@ -780,7 +790,7 @@ impl<'a> Parser<'a> {\n     /// `|` and continues. If an `|` is not seen, signals an error.\n     fn expect_or(&mut self) -> PResult<'a, ()> {\n         self.expected_tokens.push(TokenType::Token(token::BinOp(token::Or)));\n-        match self.token {\n+        match self.token.kind {\n             token::BinOp(token::Or) => {\n                 self.bump();\n                 Ok(())\n@@ -805,7 +815,7 @@ impl<'a> Parser<'a> {\n     /// starting token.\n     fn eat_lt(&mut self) -> bool {\n         self.expected_tokens.push(TokenType::Token(token::Lt));\n-        let ate = match self.token {\n+        let ate = match self.token.kind {\n             token::Lt => {\n                 self.bump();\n                 true\n@@ -845,7 +855,7 @@ impl<'a> Parser<'a> {\n     /// with a single `>` and continues. If a `>` is not seen, signals an error.\n     fn expect_gt(&mut self) -> PResult<'a, ()> {\n         self.expected_tokens.push(TokenType::Token(token::Gt));\n-        let ate = match self.token {\n+        let ate = match self.token.kind {\n             token::Gt => {\n                 self.bump();\n                 Some(())\n@@ -883,7 +893,7 @@ impl<'a> Parser<'a> {\n     /// `f` must consume tokens until reaching the next separator or\n     /// closing bracket.\n     pub fn parse_seq_to_end<T, F>(&mut self,\n-                                  ket: &token::Token,\n+                                  ket: &TokenKind,\n                                   sep: SeqSep,\n                                   f: F)\n                                   -> PResult<'a, Vec<T>> where\n@@ -901,7 +911,7 @@ impl<'a> Parser<'a> {\n     /// closing bracket.\n     pub fn parse_seq_to_before_end<T, F>(\n         &mut self,\n-        ket: &token::Token,\n+        ket: &TokenKind,\n         sep: SeqSep,\n         f: F,\n     ) -> PResult<'a, (Vec<T>, bool)>\n@@ -912,7 +922,7 @@ impl<'a> Parser<'a> {\n \n     crate fn parse_seq_to_before_tokens<T, F>(\n         &mut self,\n-        kets: &[&token::Token],\n+        kets: &[&TokenKind],\n         sep: SeqSep,\n         expect: TokenExpectType,\n         mut f: F,\n@@ -928,7 +938,7 @@ impl<'a> Parser<'a> {\n                     TokenExpectType::NoExpect => self.token == **k,\n                 }\n             }) {\n-            match self.token {\n+            match self.token.kind {\n                 token::CloseDelim(..) | token::Eof => break,\n                 _ => {}\n             };\n@@ -986,8 +996,8 @@ impl<'a> Parser<'a> {\n     /// closing bracket.\n     fn parse_unspanned_seq<T, F>(\n         &mut self,\n-        bra: &token::Token,\n-        ket: &token::Token,\n+        bra: &TokenKind,\n+        ket: &TokenKind,\n         sep: SeqSep,\n         f: F,\n     ) -> PResult<'a, Vec<T>> where\n@@ -1011,7 +1021,7 @@ impl<'a> Parser<'a> {\n         self.prev_span = self.meta_var_span.take().unwrap_or(self.span);\n \n         // Record last token kind for possible error recovery.\n-        self.prev_token_kind = match self.token {\n+        self.prev_token_kind = match self.token.kind {\n             token::DocComment(..) => PrevTokenKind::DocComment,\n             token::Comma => PrevTokenKind::Comma,\n             token::BinOp(token::Plus) => PrevTokenKind::Plus,\n@@ -1022,40 +1032,39 @@ impl<'a> Parser<'a> {\n             _ => PrevTokenKind::Other,\n         };\n \n-        let next = self.next_tok();\n-        self.span = next.sp;\n-        self.token = next.tok;\n+        self.token = self.next_tok();\n         self.expected_tokens.clear();\n         // check after each token\n         self.process_potential_macro_variable();\n     }\n \n     /// Advance the parser using provided token as a next one. Use this when\n     /// consuming a part of a token. For example a single `<` from `<<`.\n-    fn bump_with(&mut self, next: token::Token, span: Span) {\n+    fn bump_with(&mut self, next: TokenKind, span: Span) {\n         self.prev_span = self.span.with_hi(span.lo());\n         // It would be incorrect to record the kind of the current token, but\n         // fortunately for tokens currently using `bump_with`, the\n         // prev_token_kind will be of no use anyway.\n         self.prev_token_kind = PrevTokenKind::Other;\n-        self.span = span;\n-        self.token = next;\n+        self.token = Token::new(next, span);\n         self.expected_tokens.clear();\n     }\n \n     pub fn look_ahead<R, F>(&self, dist: usize, f: F) -> R where\n-        F: FnOnce(&token::Token) -> R,\n+        F: FnOnce(&Token) -> R,\n     {\n         if dist == 0 {\n-            return f(&self.token)\n+            return f(&self.token);\n         }\n \n-        f(&match self.token_cursor.frame.tree_cursor.look_ahead(dist - 1) {\n+        let frame = &self.token_cursor.frame;\n+        f(&match frame.tree_cursor.look_ahead(dist - 1) {\n             Some(tree) => match tree {\n-                TokenTree::Token(_, tok) => tok,\n-                TokenTree::Delimited(_, delim, _) => token::OpenDelim(delim),\n-            },\n-            None => token::CloseDelim(self.token_cursor.frame.delim),\n+                TokenTree::Token(token) => token,\n+                TokenTree::Delimited(dspan, delim, _) =>\n+                    Token::new(token::OpenDelim(delim), dspan.open),\n+            }\n+            None => Token::new(token::CloseDelim(frame.delim), frame.span.close)\n         })\n     }\n \n@@ -1065,7 +1074,7 @@ impl<'a> Parser<'a> {\n         }\n \n         match self.token_cursor.frame.tree_cursor.look_ahead(dist - 1) {\n-            Some(TokenTree::Token(span, _)) => span,\n+            Some(TokenTree::Token(token)) => token.span,\n             Some(TokenTree::Delimited(span, ..)) => span.entire(),\n             None => self.look_ahead_span(dist - 1),\n         }\n@@ -1209,7 +1218,7 @@ impl<'a> Parser<'a> {\n                 decl,\n             };\n \n-            let body = match self.token {\n+            let body = match self.token.kind {\n                 token::Semi => {\n                     self.bump();\n                     *at_end = true;\n@@ -1477,7 +1486,7 @@ impl<'a> Parser<'a> {\n     }\n \n     fn is_named_argument(&self) -> bool {\n-        let offset = match self.token {\n+        let offset = match self.token.kind {\n             token::Interpolated(ref nt) => match **nt {\n                 token::NtPat(..) => return self.look_ahead(1, |t| t == &token::Colon),\n                 _ => 0,\n@@ -1612,22 +1621,22 @@ impl<'a> Parser<'a> {\n     }\n \n     fn parse_path_segment_ident(&mut self) -> PResult<'a, ast::Ident> {\n-        match self.token {\n-            token::Ident(ident, _) if self.token.is_path_segment_keyword() => {\n+        match self.token.kind {\n+            token::Ident(name, _) if name.is_path_segment_keyword() => {\n                 let span = self.span;\n                 self.bump();\n-                Ok(Ident::new(ident.name, span))\n+                Ok(Ident::new(name, span))\n             }\n             _ => self.parse_ident(),\n         }\n     }\n \n     fn parse_ident_or_underscore(&mut self) -> PResult<'a, ast::Ident> {\n-        match self.token {\n-            token::Ident(ident, false) if ident.name == kw::Underscore => {\n+        match self.token.kind {\n+            token::Ident(name, false) if name == kw::Underscore => {\n                 let span = self.span;\n                 self.bump();\n-                Ok(Ident::new(ident.name, span))\n+                Ok(Ident::new(name, span))\n             }\n             _ => self.parse_ident(),\n         }\n@@ -1710,7 +1719,7 @@ impl<'a> Parser<'a> {\n     /// backwards-compatibility. This is used when parsing derive macro paths in `#[derive]`\n     /// attributes.\n     pub fn parse_path_allowing_meta(&mut self, style: PathStyle) -> PResult<'a, ast::Path> {\n-        let meta_ident = match self.token {\n+        let meta_ident = match self.token.kind {\n             token::Interpolated(ref nt) => match **nt {\n                 token::NtMeta(ref meta) => match meta.node {\n                     ast::MetaItemKind::Word => Some(meta.path.clone()),\n@@ -1763,7 +1772,7 @@ impl<'a> Parser<'a> {\n     fn parse_path_segment(&mut self, style: PathStyle) -> PResult<'a, PathSegment> {\n         let ident = self.parse_path_segment_ident()?;\n \n-        let is_args_start = |token: &token::Token| match *token {\n+        let is_args_start = |token: &TokenKind| match *token {\n             token::Lt | token::BinOp(token::Shl) | token::OpenDelim(token::Paren)\n             | token::LArrow => true,\n             _ => false,\n@@ -1859,7 +1868,8 @@ impl<'a> Parser<'a> {\n     }\n \n     fn parse_field_name(&mut self) -> PResult<'a, Ident> {\n-        if let token::Literal(token::Lit { kind: token::Integer, symbol, suffix }) = self.token {\n+        if let token::Literal(token::Lit { kind: token::Integer, symbol, suffix }) =\n+                self.token.kind {\n             self.expect_no_suffix(self.span, \"a tuple index\", suffix);\n             self.bump();\n             Ok(Ident::new(symbol, self.prev_span))\n@@ -1949,7 +1959,7 @@ impl<'a> Parser<'a> {\n     }\n \n     fn expect_delimited_token_tree(&mut self) -> PResult<'a, (MacDelimiter, TokenStream)> {\n-        let delim = match self.token {\n+        let delim = match self.token.kind {\n             token::OpenDelim(delim) => delim,\n             _ => {\n                 let msg = \"expected open delimiter\";\n@@ -1992,8 +2002,8 @@ impl<'a> Parser<'a> {\n \n         let ex: ExprKind;\n \n-        // Note: when adding new syntax here, don't forget to adjust Token::can_begin_expr().\n-        match self.token {\n+        // Note: when adding new syntax here, don't forget to adjust TokenKind::can_begin_expr().\n+        match self.token.kind {\n             token::OpenDelim(token::Paren) => {\n                 self.bump();\n \n@@ -2363,13 +2373,11 @@ impl<'a> Parser<'a> {\n             }\n \n             let mut recovery_field = None;\n-            if let token::Ident(ident, _) = self.token {\n+            if let token::Ident(name, _) = self.token.kind {\n                 if !self.token.is_reserved_ident() && self.look_ahead(1, |t| *t == token::Colon) {\n                     // Use in case of error after field-looking code: `S { foo: () with a }`\n-                    let mut ident = ident.clone();\n-                    ident.span = self.span;\n                     recovery_field = Some(ast::Field {\n-                        ident,\n+                        ident: Ident::new(name, self.span),\n                         span: self.span,\n                         expr: self.mk_expr(self.span, ExprKind::Err, ThinVec::new()),\n                         is_shorthand: false,\n@@ -2503,7 +2511,7 @@ impl<'a> Parser<'a> {\n         let segment = self.parse_path_segment(PathStyle::Expr)?;\n         self.check_trailing_angle_brackets(&segment, token::OpenDelim(token::Paren));\n \n-        Ok(match self.token {\n+        Ok(match self.token.kind {\n             token::OpenDelim(token::Paren) => {\n                 // Method call `expr.f()`\n                 let mut args = self.parse_unspanned_seq(\n@@ -2542,7 +2550,7 @@ impl<'a> Parser<'a> {\n \n             // expr.f\n             if self.eat(&token::Dot) {\n-                match self.token {\n+                match self.token.kind {\n                     token::Ident(..) => {\n                         e = self.parse_dot_suffix(e, lo)?;\n                     }\n@@ -2594,7 +2602,7 @@ impl<'a> Parser<'a> {\n                 continue;\n             }\n             if self.expr_is_complete(&e) { break; }\n-            match self.token {\n+            match self.token.kind {\n                 // expr(...)\n                 token::OpenDelim(token::Paren) => {\n                     let seq = self.parse_unspanned_seq(\n@@ -2627,12 +2635,12 @@ impl<'a> Parser<'a> {\n     }\n \n     crate fn process_potential_macro_variable(&mut self) {\n-        let (token, span) = match self.token {\n+        self.token = match self.token.kind {\n             token::Dollar if self.span.ctxt() != syntax_pos::hygiene::SyntaxContext::empty() &&\n                              self.look_ahead(1, |t| t.is_ident()) => {\n                 self.bump();\n-                let name = match self.token {\n-                    token::Ident(ident, _) => ident,\n+                let name = match self.token.kind {\n+                    token::Ident(name, _) => name,\n                     _ => unreachable!()\n                 };\n                 let mut err = self.fatal(&format!(\"unknown macro variable `{}`\", name));\n@@ -2646,24 +2654,24 @@ impl<'a> Parser<'a> {\n                 // Interpolated identifier and lifetime tokens are replaced with usual identifier\n                 // and lifetime tokens, so the former are never encountered during normal parsing.\n                 match **nt {\n-                    token::NtIdent(ident, is_raw) => (token::Ident(ident, is_raw), ident.span),\n-                    token::NtLifetime(ident) => (token::Lifetime(ident), ident.span),\n+                    token::NtIdent(ident, is_raw) =>\n+                        Token::new(token::Ident(ident.name, is_raw), ident.span),\n+                    token::NtLifetime(ident) =>\n+                        Token::new(token::Lifetime(ident.name), ident.span),\n                     _ => return,\n                 }\n             }\n             _ => return,\n         };\n-        self.token = token;\n-        self.span = span;\n     }\n \n     /// Parses a single token tree from the input.\n     crate fn parse_token_tree(&mut self) -> TokenTree {\n-        match self.token {\n+        match self.token.kind {\n             token::OpenDelim(..) => {\n                 let frame = mem::replace(&mut self.token_cursor.frame,\n                                          self.token_cursor.stack.pop().unwrap());\n-                self.span = frame.span.entire();\n+                self.token.span = frame.span.entire();\n                 self.bump();\n                 TokenTree::Delimited(\n                     frame.span,\n@@ -2673,9 +2681,9 @@ impl<'a> Parser<'a> {\n             },\n             token::CloseDelim(_) | token::Eof => unreachable!(),\n             _ => {\n-                let (token, span) = (mem::replace(&mut self.token, token::Whitespace), self.span);\n+                let token = self.token.take();\n                 self.bump();\n-                TokenTree::Token(span, token)\n+                TokenTree::Token(token)\n             }\n         }\n     }\n@@ -2692,7 +2700,7 @@ impl<'a> Parser<'a> {\n     pub fn parse_tokens(&mut self) -> TokenStream {\n         let mut result = Vec::new();\n         loop {\n-            match self.token {\n+            match self.token.kind {\n                 token::Eof | token::CloseDelim(..) => break,\n                 _ => result.push(self.parse_token_tree().into()),\n             }\n@@ -2706,8 +2714,8 @@ impl<'a> Parser<'a> {\n                              -> PResult<'a, P<Expr>> {\n         let attrs = self.parse_or_use_outer_attributes(already_parsed_attrs)?;\n         let lo = self.span;\n-        // Note: when adding new unary operators, don't forget to adjust Token::can_begin_expr()\n-        let (hi, ex) = match self.token {\n+        // Note: when adding new unary operators, don't forget to adjust TokenKind::can_begin_expr()\n+        let (hi, ex) = match self.token.kind {\n             token::Not => {\n                 self.bump();\n                 let e = self.parse_prefix_expr(None);\n@@ -2760,10 +2768,10 @@ impl<'a> Parser<'a> {\n                 // `not` is just an ordinary identifier in Rust-the-language,\n                 // but as `rustc`-the-compiler, we can issue clever diagnostics\n                 // for confused users who really want to say `!`\n-                let token_cannot_continue_expr = |t: &token::Token| match *t {\n+                let token_cannot_continue_expr = |t: &Token| match t.kind {\n                     // These tokens can start an expression after `!`, but\n                     // can't continue an expression after an ident\n-                    token::Ident(ident, is_raw) => token::ident_can_begin_expr(ident, is_raw),\n+                    token::Ident(name, is_raw) => token::ident_can_begin_expr(name, t.span, is_raw),\n                     token::Literal(..) | token::Pound => true,\n                     token::Interpolated(ref nt) => match **nt {\n                         token::NtIdent(..) | token::NtExpr(..) |\n@@ -3040,7 +3048,7 @@ impl<'a> Parser<'a> {\n \n                 match self.parse_path(PathStyle::Expr) {\n                     Ok(path) => {\n-                        let (op_noun, op_verb) = match self.token {\n+                        let (op_noun, op_verb) = match self.token.kind {\n                             token::Lt => (\"comparison\", \"comparing\"),\n                             token::BinOp(token::Shl) => (\"shift\", \"shifting\"),\n                             _ => {\n@@ -3359,7 +3367,7 @@ impl<'a> Parser<'a> {\n         let discriminant = self.parse_expr_res(Restrictions::NO_STRUCT_LITERAL,\n                                                None)?;\n         if let Err(mut e) = self.expect(&token::OpenDelim(token::Brace)) {\n-            if self.token == token::Token::Semi {\n+            if self.token == token::Semi {\n                 e.span_suggestion_short(\n                     match_span,\n                     \"try removing this `match`\",\n@@ -3844,14 +3852,14 @@ impl<'a> Parser<'a> {\n     // helper function to decide whether to parse as ident binding or to try to do\n     // something more complex like range patterns\n     fn parse_as_ident(&mut self) -> bool {\n-        self.look_ahead(1, |t| match *t {\n+        self.look_ahead(1, |t| match t.kind {\n             token::OpenDelim(token::Paren) | token::OpenDelim(token::Brace) |\n             token::DotDotDot | token::DotDotEq | token::ModSep | token::Not => Some(false),\n             // ensure slice patterns [a, b.., c] and [a, b, c..] don't go into the\n             // range pattern branch\n             token::DotDot => None,\n             _ => Some(true),\n-        }).unwrap_or_else(|| self.look_ahead(2, |t| match *t {\n+        }).unwrap_or_else(|| self.look_ahead(2, |t| match t.kind {\n             token::Comma | token::CloseDelim(token::Bracket) => true,\n             _ => false,\n         }))\n@@ -3914,14 +3922,13 @@ impl<'a> Parser<'a> {\n \n         let lo = self.span;\n         let pat;\n-        match self.token {\n+        match self.token.kind {\n             token::BinOp(token::And) | token::AndAnd => {\n                 // Parse &pat / &mut pat\n                 self.expect_and()?;\n                 let mutbl = self.parse_mutability();\n-                if let token::Lifetime(ident) = self.token {\n-                    let mut err = self.fatal(&format!(\"unexpected lifetime `{}` in pattern\",\n-                                                      ident));\n+                if let token::Lifetime(name) = self.token.kind {\n+                    let mut err = self.fatal(&format!(\"unexpected lifetime `{}` in pattern\", name));\n                     err.span_label(self.span, \"unexpected lifetime\");\n                     return Err(err);\n                 }\n@@ -3990,7 +3997,7 @@ impl<'a> Parser<'a> {\n                     // Parse an unqualified path\n                     (None, self.parse_path(PathStyle::Expr)?)\n                 };\n-                match self.token {\n+                match self.token.kind {\n                     token::Not if qself.is_none() => {\n                         // Parse macro invocation\n                         self.bump();\n@@ -3999,7 +4006,7 @@ impl<'a> Parser<'a> {\n                         pat = PatKind::Mac(mac);\n                     }\n                     token::DotDotDot | token::DotDotEq | token::DotDot => {\n-                        let end_kind = match self.token {\n+                        let end_kind = match self.token.kind {\n                             token::DotDot => RangeEnd::Excluded,\n                             token::DotDotDot => RangeEnd::Included(RangeSyntax::DotDotDot),\n                             token::DotDotEq => RangeEnd::Included(RangeSyntax::DotDotEq),\n@@ -4325,8 +4332,8 @@ impl<'a> Parser<'a> {\n     fn eat_macro_def(&mut self, attrs: &[Attribute], vis: &Visibility, lo: Span)\n                      -> PResult<'a, Option<P<Item>>> {\n         let token_lo = self.span;\n-        let (ident, def) = match self.token {\n-            token::Ident(ident, false) if ident.name == kw::Macro => {\n+        let (ident, def) = match self.token.kind {\n+            token::Ident(name, false) if name == kw::Macro => {\n                 self.bump();\n                 let ident = self.parse_ident()?;\n                 let tokens = if self.check(&token::OpenDelim(token::Brace)) {\n@@ -4344,7 +4351,7 @@ impl<'a> Parser<'a> {\n                     };\n                     TokenStream::new(vec![\n                         args.into(),\n-                        TokenTree::Token(token_lo.to(self.prev_span), token::FatArrow).into(),\n+                        TokenTree::token(token::FatArrow, token_lo.to(self.prev_span)).into(),\n                         body.into(),\n                     ])\n                 } else {\n@@ -4354,8 +4361,8 @@ impl<'a> Parser<'a> {\n \n                 (ident, ast::MacroDef { tokens: tokens.into(), legacy: false })\n             }\n-            token::Ident(ident, _) if ident.name == sym::macro_rules &&\n-                                   self.look_ahead(1, |t| *t == token::Not) => {\n+            token::Ident(name, _) if name == sym::macro_rules &&\n+                                     self.look_ahead(1, |t| *t == token::Not) => {\n                 let prev_span = self.prev_span;\n                 self.complain_if_pub_macro(&vis.node, prev_span);\n                 self.bump();\n@@ -4436,15 +4443,15 @@ impl<'a> Parser<'a> {\n             }\n \n             // it's a macro invocation\n-            let id = match self.token {\n+            let id = match self.token.kind {\n                 token::OpenDelim(_) => Ident::invalid(), // no special identifier\n                 _ => self.parse_ident()?,\n             };\n \n             // check that we're pointing at delimiters (need to check\n             // again after the `if`, because of `parse_ident`\n             // consuming more tokens).\n-            match self.token {\n+            match self.token.kind {\n                 token::OpenDelim(_) => {}\n                 _ => {\n                     // we only expect an ident if we didn't parse one\n@@ -4481,7 +4488,9 @@ impl<'a> Parser<'a> {\n                 // We used to incorrectly stop parsing macro-expanded statements here.\n                 // If the next token will be an error anyway but could have parsed with the\n                 // earlier behavior, stop parsing here and emit a warning to avoid breakage.\n-                else if macro_legacy_warnings && self.token.can_begin_expr() && match self.token {\n+                else if macro_legacy_warnings &&\n+                        self.token.can_begin_expr() &&\n+                        match self.token.kind {\n                     // These can continue an expression, so we can't stop parsing and warn.\n                     token::OpenDelim(token::Paren) | token::OpenDelim(token::Bracket) |\n                     token::BinOp(token::Minus) | token::BinOp(token::Star) |\n@@ -4779,7 +4788,7 @@ impl<'a> Parser<'a> {\n         let mut last_plus_span = None;\n         let mut was_negative = false;\n         loop {\n-            // This needs to be synchronized with `Token::can_begin_bound`.\n+            // This needs to be synchronized with `TokenKind::can_begin_bound`.\n             let is_bound_start = self.check_path() || self.check_lifetime() ||\n                                  self.check(&token::Not) || // used for error reporting only\n                                  self.check(&token::Question) ||\n@@ -5250,7 +5259,7 @@ impl<'a> Parser<'a> {\n                 assoc_ty_constraints.push(span);\n             } else if self.check_const_arg() {\n                 // Parse const argument.\n-                let expr = if let token::OpenDelim(token::Brace) = self.token {\n+                let expr = if let token::OpenDelim(token::Brace) = self.token.kind {\n                     self.parse_block_expr(None, self.span, BlockCheckMode::Default, ThinVec::new())?\n                 } else if self.token.is_ident() {\n                     // FIXME(const_generics): to distinguish between idents for types and consts,\n@@ -5477,10 +5486,10 @@ impl<'a> Parser<'a> {\n \n     /// Returns the parsed optional self argument and whether a self shortcut was used.\n     fn parse_self_arg(&mut self) -> PResult<'a, Option<Arg>> {\n-        let expect_ident = |this: &mut Self| match this.token {\n+        let expect_ident = |this: &mut Self| match this.token.kind {\n             // Preserve hygienic context.\n-            token::Ident(ident, _) =>\n-                { let span = this.span; this.bump(); Ident::new(ident.name, span) }\n+            token::Ident(name, _) =>\n+                { let span = this.span; this.bump(); Ident::new(name, span) }\n             _ => unreachable!()\n         };\n         let isolated_self = |this: &mut Self, n| {\n@@ -5492,7 +5501,7 @@ impl<'a> Parser<'a> {\n         // Only a limited set of initial token sequences is considered `self` parameters; anything\n         // else is parsed as a normal function parameter list, so some lookahead is required.\n         let eself_lo = self.span;\n-        let (eself, eself_ident, eself_hi) = match self.token {\n+        let (eself, eself_ident, eself_hi) = match self.token.kind {\n             token::BinOp(token::And) => {\n                 // `&self`\n                 // `&mut self`\n@@ -5803,11 +5812,7 @@ impl<'a> Parser<'a> {\n         match *vis {\n             VisibilityKind::Inherited => {}\n             _ => {\n-                let is_macro_rules: bool = match self.token {\n-                    token::Ident(sid, _) => sid.name == sym::macro_rules,\n-                    _ => false,\n-                };\n-                let mut err = if is_macro_rules {\n+                let mut err = if self.token.is_keyword(sym::macro_rules) {\n                     let mut err = self.diagnostic()\n                         .struct_span_err(sp, \"can't qualify macro_rules invocation with `pub`\");\n                     err.span_suggestion(\n@@ -5918,9 +5923,9 @@ impl<'a> Parser<'a> {\n             self.expect(&token::OpenDelim(token::Brace))?;\n             let mut trait_items = vec![];\n             while !self.eat(&token::CloseDelim(token::Brace)) {\n-                if let token::DocComment(_) = self.token {\n+                if let token::DocComment(_) = self.token.kind {\n                     if self.look_ahead(1,\n-                    |tok| tok == &token::Token::CloseDelim(token::Brace)) {\n+                    |tok| tok == &token::CloseDelim(token::Brace)) {\n                         let mut err = self.diagnostic().struct_span_err_with_code(\n                             self.span,\n                             \"found a documentation comment that doesn't document anything\",\n@@ -6246,7 +6251,7 @@ impl<'a> Parser<'a> {\n         if self.token == token::Comma {\n             seen_comma = true;\n         }\n-        match self.token {\n+        match self.token.kind {\n             token::Comma => {\n                 self.bump();\n             }\n@@ -6413,7 +6418,7 @@ impl<'a> Parser<'a> {\n     }\n \n     /// Given a termination token, parses all of the items in a module.\n-    fn parse_mod_items(&mut self, term: &token::Token, inner_lo: Span) -> PResult<'a, Mod> {\n+    fn parse_mod_items(&mut self, term: &TokenKind, inner_lo: Span) -> PResult<'a, Mod> {\n         let mut items = vec![];\n         while let Some(item) = self.parse_item()? {\n             items.push(item);\n@@ -6796,7 +6801,7 @@ impl<'a> Parser<'a> {\n         let mut replacement = vec![];\n         let mut fixed_crate_name = false;\n         // Accept `extern crate name-like-this` for better diagnostics\n-        let dash = token::Token::BinOp(token::BinOpToken::Minus);\n+        let dash = token::BinOp(token::BinOpToken::Minus);\n         if self.token == dash {  // Do not include `-` as part of the expected tokens list\n             while self.eat(&dash) {\n                 fixed_crate_name = true;\n@@ -7011,7 +7016,7 @@ impl<'a> Parser<'a> {\n     /// Parses a string as an ABI spec on an extern type or module. Consumes\n     /// the `extern` keyword, if one is found.\n     fn parse_opt_abi(&mut self) -> PResult<'a, Option<Abi>> {\n-        match self.token {\n+        match self.token.kind {\n             token::Literal(token::Lit { kind: token::Str, symbol, suffix }) |\n             token::Literal(token::Lit { kind: token::StrRaw(..), symbol, suffix }) => {\n                 let sp = self.span;\n@@ -7046,7 +7051,7 @@ impl<'a> Parser<'a> {\n                 if token.is_keyword(kw::Move) {\n                     return true;\n                 }\n-                match *token {\n+                match token.kind {\n                     token::BinOp(token::Or) | token::OrOr => true,\n                     _ => false,\n                 }\n@@ -7818,7 +7823,7 @@ impl<'a> Parser<'a> {\n     }\n \n     pub fn parse_optional_str(&mut self) -> Option<(Symbol, ast::StrStyle, Option<ast::Name>)> {\n-        let ret = match self.token {\n+        let ret = match self.token.kind {\n             token::Literal(token::Lit { kind: token::Str, symbol, suffix }) =>\n                 (symbol, ast::StrStyle::Cooked, suffix),\n             token::Literal(token::Lit { kind: token::StrRaw(n), symbol, suffix }) =>\n@@ -7869,7 +7874,7 @@ pub fn emit_unclosed_delims(unclosed_delims: &mut Vec<UnmatchedBrace>, handler:\n     for unmatched in unclosed_delims.iter() {\n         let mut err = handler.struct_span_err(unmatched.found_span, &format!(\n             \"incorrect close delimiter: `{}`\",\n-            pprust::token_to_string(&token::Token::CloseDelim(unmatched.found_delim)),\n+            pprust::token_to_string(&token::CloseDelim(unmatched.found_delim)),\n         ));\n         err.span_label(unmatched.found_span, \"incorrect close delimiter\");\n         if let Some(sp) = unmatched.candidate_span {"}, {"sha": "28a733728bf7b6a5769688f2b4ffcae45943b5b1", "filename": "src/libsyntax/parse/token.rs", "status": "modified", "additions": 134, "deletions": 65, "changes": 199, "blob_url": "https://github.com/rust-lang/rust/blob/ca1bcfdde3f19afd68ef808cecf2ce56d08d5df4/src%2Flibsyntax%2Fparse%2Ftoken.rs", "raw_url": "https://github.com/rust-lang/rust/raw/ca1bcfdde3f19afd68ef808cecf2ce56d08d5df4/src%2Flibsyntax%2Fparse%2Ftoken.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Ftoken.rs?ref=ca1bcfdde3f19afd68ef808cecf2ce56d08d5df4", "patch": "@@ -2,22 +2,22 @@ pub use BinOpToken::*;\n pub use Nonterminal::*;\n pub use DelimToken::*;\n pub use LitKind::*;\n-pub use Token::*;\n+pub use TokenKind::*;\n \n use crate::ast::{self};\n-use crate::parse::ParseSess;\n+use crate::parse::{parse_stream_from_source_str, ParseSess};\n use crate::print::pprust;\n use crate::ptr::P;\n use crate::symbol::kw;\n-use crate::syntax::parse::parse_stream_from_source_str;\n use crate::tokenstream::{self, DelimSpan, TokenStream, TokenTree};\n \n-use syntax_pos::symbol::{self, Symbol};\n-use syntax_pos::{self, Span, FileName};\n+use syntax_pos::symbol::Symbol;\n+use syntax_pos::{self, Span, FileName, DUMMY_SP};\n use log::info;\n \n use std::fmt;\n use std::mem;\n+use std::ops::Deref;\n #[cfg(target_arch = \"x86_64\")]\n use rustc_data_structures::static_assert_size;\n use rustc_data_structures::sync::Lrc;\n@@ -117,8 +117,8 @@ impl Lit {\n     }\n }\n \n-pub(crate) fn ident_can_begin_expr(ident: ast::Ident, is_raw: bool) -> bool {\n-    let ident_token: Token = Ident(ident, is_raw);\n+pub(crate) fn ident_can_begin_expr(name: ast::Name, span: Span, is_raw: bool) -> bool {\n+    let ident_token = Token::new(Ident(name, is_raw), span);\n \n     !ident_token.is_reserved_ident() ||\n     ident_token.is_path_segment_keyword() ||\n@@ -145,11 +145,11 @@ pub(crate) fn ident_can_begin_expr(ident: ast::Ident, is_raw: bool) -> bool {\n         kw::While,\n         kw::Yield,\n         kw::Static,\n-    ].contains(&ident.name)\n+    ].contains(&name)\n }\n \n-fn ident_can_begin_type(ident: ast::Ident, is_raw: bool) -> bool {\n-    let ident_token: Token = Ident(ident, is_raw);\n+fn ident_can_begin_type(name: ast::Name, span: Span, is_raw: bool) -> bool {\n+    let ident_token = Token::new(Ident(name, is_raw), span);\n \n     !ident_token.is_reserved_ident() ||\n     ident_token.is_path_segment_keyword() ||\n@@ -162,11 +162,11 @@ fn ident_can_begin_type(ident: ast::Ident, is_raw: bool) -> bool {\n         kw::Extern,\n         kw::Typeof,\n         kw::Dyn,\n-    ].contains(&ident.name)\n+    ].contains(&name)\n }\n \n-#[derive(Clone, RustcEncodable, RustcDecodable, PartialEq, Debug)]\n-pub enum Token {\n+#[derive(Clone, PartialEq, RustcEncodable, RustcDecodable, Debug)]\n+pub enum TokenKind {\n     /* Expression-operator symbols. */\n     Eq,\n     Lt,\n@@ -209,8 +209,8 @@ pub enum Token {\n     Literal(Lit),\n \n     /* Name components */\n-    Ident(ast::Ident, /* is_raw */ bool),\n-    Lifetime(ast::Ident),\n+    Ident(ast::Name, /* is_raw */ bool),\n+    Lifetime(ast::Name),\n \n     Interpolated(Lrc<Nonterminal>),\n \n@@ -231,14 +231,20 @@ pub enum Token {\n     Eof,\n }\n \n-// `Token` is used a lot. Make sure it doesn't unintentionally get bigger.\n+// `TokenKind` is used a lot. Make sure it doesn't unintentionally get bigger.\n #[cfg(target_arch = \"x86_64\")]\n-static_assert_size!(Token, 16);\n+static_assert_size!(TokenKind, 16);\n \n-impl Token {\n-    /// Recovers a `Token` from an `ast::Ident`. This creates a raw identifier if necessary.\n-    pub fn from_ast_ident(ident: ast::Ident) -> Token {\n-        Ident(ident, ident.is_raw_guess())\n+#[derive(Clone, PartialEq, RustcEncodable, RustcDecodable, Debug)]\n+pub struct Token {\n+    pub kind: TokenKind,\n+    pub span: Span,\n+}\n+\n+impl TokenKind {\n+    /// Recovers a `TokenKind` from an `ast::Ident`. This creates a raw identifier if necessary.\n+    pub fn from_ast_ident(ident: ast::Ident) -> TokenKind {\n+        Ident(ident.name, ident.is_raw_guess())\n     }\n \n     crate fn is_like_plus(&self) -> bool {\n@@ -247,12 +253,14 @@ impl Token {\n             _ => false,\n         }\n     }\n+}\n \n+impl Token {\n     /// Returns `true` if the token can appear at the start of an expression.\n     crate fn can_begin_expr(&self) -> bool {\n-        match *self {\n-            Ident(ident, is_raw)              =>\n-                ident_can_begin_expr(ident, is_raw), // value name or keyword\n+        match self.kind {\n+            Ident(name, is_raw)              =>\n+                ident_can_begin_expr(name, self.span, is_raw), // value name or keyword\n             OpenDelim(..)                     | // tuple, array or block\n             Literal(..)                       | // literal\n             Not                               | // operator not\n@@ -282,9 +290,9 @@ impl Token {\n \n     /// Returns `true` if the token can appear at the start of a type.\n     crate fn can_begin_type(&self) -> bool {\n-        match *self {\n-            Ident(ident, is_raw)        =>\n-                ident_can_begin_type(ident, is_raw), // type name or keyword\n+        match self.kind {\n+            Ident(name, is_raw)        =>\n+                ident_can_begin_type(name, self.span, is_raw), // type name or keyword\n             OpenDelim(Paren)            | // tuple\n             OpenDelim(Bracket)          | // array\n             Not                         | // never\n@@ -302,7 +310,9 @@ impl Token {\n             _ => false,\n         }\n     }\n+}\n \n+impl TokenKind {\n     /// Returns `true` if the token can appear at the start of a const param.\n     pub fn can_begin_const_arg(&self) -> bool {\n         match self {\n@@ -316,14 +326,18 @@ impl Token {\n             _ => self.can_begin_literal_or_bool(),\n         }\n     }\n+}\n \n+impl Token {\n     /// Returns `true` if the token can appear at the start of a generic bound.\n     crate fn can_begin_bound(&self) -> bool {\n         self.is_path_start() || self.is_lifetime() || self.is_keyword(kw::For) ||\n         self == &Question || self == &OpenDelim(Paren)\n     }\n+}\n \n-    pub fn lit(kind: LitKind, symbol: Symbol, suffix: Option<Symbol>) -> Token {\n+impl TokenKind {\n+    pub fn lit(kind: LitKind, symbol: Symbol, suffix: Option<Symbol>) -> TokenKind {\n         Literal(Lit::new(kind, symbol, suffix))\n     }\n \n@@ -348,54 +362,79 @@ impl Token {\n         match *self {\n             Literal(..)  => true,\n             BinOp(Minus) => true,\n-            Ident(ident, false) if ident.name == kw::True => true,\n-            Ident(ident, false) if ident.name == kw::False => true,\n+            Ident(name, false) if name == kw::True => true,\n+            Ident(name, false) if name == kw::False => true,\n             Interpolated(ref nt) => match **nt {\n                 NtLiteral(..) => true,\n                 _             => false,\n             },\n             _            => false,\n         }\n     }\n+}\n \n+impl Token {\n     /// Returns an identifier if this token is an identifier.\n     pub fn ident(&self) -> Option<(ast::Ident, /* is_raw */ bool)> {\n-        match *self {\n-            Ident(ident, is_raw) => Some((ident, is_raw)),\n+        match self.kind {\n+            Ident(name, is_raw) => Some((ast::Ident::new(name, self.span), is_raw)),\n             Interpolated(ref nt) => match **nt {\n                 NtIdent(ident, is_raw) => Some((ident, is_raw)),\n                 _ => None,\n             },\n             _ => None,\n         }\n     }\n+\n     /// Returns a lifetime identifier if this token is a lifetime.\n     pub fn lifetime(&self) -> Option<ast::Ident> {\n-        match *self {\n-            Lifetime(ident) => Some(ident),\n+        match self.kind {\n+            Lifetime(name) => Some(ast::Ident::new(name, self.span)),\n             Interpolated(ref nt) => match **nt {\n                 NtLifetime(ident) => Some(ident),\n                 _ => None,\n             },\n             _ => None,\n         }\n     }\n+}\n+\n+impl TokenKind {\n+    /// Returns an identifier name if this token is an identifier.\n+    pub fn ident_name(&self) -> Option<(ast::Name, /* is_raw */ bool)> {\n+        match *self {\n+            Ident(name, is_raw) => Some((name, is_raw)),\n+            Interpolated(ref nt) => match **nt {\n+                NtIdent(ident, is_raw) => Some((ident.name, is_raw)),\n+                _ => None,\n+            },\n+            _ => None,\n+        }\n+    }\n+    /// Returns a lifetime name if this token is a lifetime.\n+    pub fn lifetime_name(&self) -> Option<ast::Name> {\n+        match *self {\n+            Lifetime(name) => Some(name),\n+            Interpolated(ref nt) => match **nt {\n+                NtLifetime(ident) => Some(ident.name),\n+                _ => None,\n+            },\n+            _ => None,\n+        }\n+    }\n     /// Returns `true` if the token is an identifier.\n     pub fn is_ident(&self) -> bool {\n-        self.ident().is_some()\n+        self.ident_name().is_some()\n     }\n     /// Returns `true` if the token is a lifetime.\n     crate fn is_lifetime(&self) -> bool {\n-        self.lifetime().is_some()\n+        self.lifetime_name().is_some()\n     }\n \n     /// Returns `true` if the token is a identifier whose name is the given\n     /// string slice.\n     crate fn is_ident_named(&self, name: Symbol) -> bool {\n-        match self.ident() {\n-            Some((ident, _)) => ident.name == name,\n-            None => false\n-        }\n+        self.ident_name().map_or(false, |(ident_name, _)| ident_name == name)\n     }\n \n     /// Returns `true` if the token is an interpolated path.\n@@ -417,24 +456,30 @@ impl Token {\n     crate fn is_qpath_start(&self) -> bool {\n         self == &Lt || self == &BinOp(Shl)\n     }\n+}\n \n+impl Token {\n     crate fn is_path_start(&self) -> bool {\n         self == &ModSep || self.is_qpath_start() || self.is_path() ||\n         self.is_path_segment_keyword() || self.is_ident() && !self.is_reserved_ident()\n     }\n+}\n \n+impl TokenKind {\n     /// Returns `true` if the token is a given keyword, `kw`.\n     pub fn is_keyword(&self, kw: Symbol) -> bool {\n-        self.ident().map(|(ident, is_raw)| ident.name == kw && !is_raw).unwrap_or(false)\n+        self.ident_name().map(|(name, is_raw)| name == kw && !is_raw).unwrap_or(false)\n     }\n \n     pub fn is_path_segment_keyword(&self) -> bool {\n-        match self.ident() {\n-            Some((id, false)) => id.is_path_segment_keyword(),\n+        match self.ident_name() {\n+            Some((name, false)) => name.is_path_segment_keyword(),\n             _ => false,\n         }\n     }\n+}\n \n+impl Token {\n     // Returns true for reserved identifiers used internally for elided lifetimes,\n     // unnamed method parameters, crate root module, error recovery etc.\n     pub fn is_special_ident(&self) -> bool {\n@@ -467,8 +512,10 @@ impl Token {\n             _ => false,\n         }\n     }\n+}\n \n-    crate fn glue(self, joint: Token) -> Option<Token> {\n+impl TokenKind {\n+    crate fn glue(self, joint: TokenKind) -> Option<TokenKind> {\n         Some(match self {\n             Eq => match joint {\n                 Eq => EqEq,\n@@ -514,13 +561,7 @@ impl Token {\n                 _ => return None,\n             },\n             SingleQuote => match joint {\n-                Ident(ident, false) => {\n-                    let name = Symbol::intern(&format!(\"'{}\", ident));\n-                    Lifetime(symbol::Ident {\n-                        name,\n-                        span: ident.span,\n-                    })\n-                }\n+                Ident(name, false) => Lifetime(Symbol::intern(&format!(\"'{}\", name))),\n                 _ => return None,\n             },\n \n@@ -534,7 +575,7 @@ impl Token {\n \n     /// Returns tokens that are likely to be typed accidentally instead of the current token.\n     /// Enables better error recovery when the wrong token is found.\n-    crate fn similar_tokens(&self) -> Option<Vec<Token>> {\n+    crate fn similar_tokens(&self) -> Option<Vec<TokenKind>> {\n         match *self {\n             Comma => Some(vec![Dot, Lt, Semi]),\n             Semi => Some(vec![Colon, Comma]),\n@@ -544,7 +585,7 @@ impl Token {\n \n     // See comments in `Nonterminal::to_tokenstream` for why we care about\n     // *probably* equal here rather than actual equality\n-    crate fn probably_equal_for_proc_macro(&self, other: &Token) -> bool {\n+    crate fn probably_equal_for_proc_macro(&self, other: &TokenKind) -> bool {\n         if mem::discriminant(self) != mem::discriminant(other) {\n             return false\n         }\n@@ -590,10 +631,10 @@ impl Token {\n \n             (&Literal(a), &Literal(b)) => a == b,\n \n-            (&Lifetime(a), &Lifetime(b)) => a.name == b.name,\n-            (&Ident(a, b), &Ident(c, d)) => b == d && (a.name == c.name ||\n-                                                       a.name == kw::DollarCrate ||\n-                                                       c.name == kw::DollarCrate),\n+            (&Lifetime(a), &Lifetime(b)) => a == b,\n+            (&Ident(a, b), &Ident(c, d)) => b == d && (a == c ||\n+                                                       a == kw::DollarCrate ||\n+                                                       c == kw::DollarCrate),\n \n             (&Interpolated(_), &Interpolated(_)) => false,\n \n@@ -602,6 +643,36 @@ impl Token {\n     }\n }\n \n+impl Token {\n+    crate fn new(kind: TokenKind, span: Span) -> Self {\n+        Token { kind, span }\n+    }\n+\n+    /// Some token that will be thrown away later.\n+    crate fn dummy() -> Self {\n+        Token::new(TokenKind::Whitespace, DUMMY_SP)\n+    }\n+\n+    /// Return this token by value and leave a dummy token in its place.\n+    crate fn take(&mut self) -> Self {\n+        mem::replace(self, Token::dummy())\n+    }\n+}\n+\n+impl PartialEq<TokenKind> for Token {\n+    fn eq(&self, rhs: &TokenKind) -> bool {\n+        self.kind == *rhs\n+    }\n+}\n+\n+// FIXME: Remove this after all necessary methods are moved from `TokenKind` to `Token`.\n+impl Deref for Token {\n+    type Target = TokenKind;\n+    fn deref(&self) -> &Self::Target {\n+        &self.kind\n+    }\n+}\n+\n #[derive(Clone, RustcEncodable, RustcDecodable)]\n /// For interpolation during macro expansion.\n pub enum Nonterminal {\n@@ -691,12 +762,10 @@ impl Nonterminal {\n                 prepend_attrs(sess, &item.attrs, item.tokens.as_ref(), span)\n             }\n             Nonterminal::NtIdent(ident, is_raw) => {\n-                let token = Token::Ident(ident, is_raw);\n-                Some(TokenTree::Token(ident.span, token).into())\n+                Some(TokenTree::token(Ident(ident.name, is_raw), ident.span).into())\n             }\n             Nonterminal::NtLifetime(ident) => {\n-                let token = Token::Lifetime(ident);\n-                Some(TokenTree::Token(ident.span, token).into())\n+                Some(TokenTree::token(Lifetime(ident.name), ident.span).into())\n             }\n             Nonterminal::NtTT(ref tt) => {\n                 Some(tt.clone().into())\n@@ -743,7 +812,7 @@ impl Nonterminal {\n     }\n }\n \n-crate fn is_op(tok: &Token) -> bool {\n+crate fn is_op(tok: &TokenKind) -> bool {\n     match *tok {\n         OpenDelim(..) | CloseDelim(..) | Literal(..) | DocComment(..) |\n         Ident(..) | Lifetime(..) | Interpolated(..) |\n@@ -781,8 +850,8 @@ fn prepend_attrs(sess: &ParseSess,\n         // For simple paths, push the identifier directly\n         if attr.path.segments.len() == 1 && attr.path.segments[0].args.is_none() {\n             let ident = attr.path.segments[0].ident;\n-            let token = Ident(ident, ident.as_str().starts_with(\"r#\"));\n-            brackets.push(tokenstream::TokenTree::Token(ident.span, token));\n+            let token = Ident(ident.name, ident.as_str().starts_with(\"r#\"));\n+            brackets.push(tokenstream::TokenTree::token(token, ident.span));\n \n         // ... and for more complicated paths, fall back to a reparse hack that\n         // should eventually be removed.\n@@ -796,7 +865,7 @@ fn prepend_attrs(sess: &ParseSess,\n         // The span we list here for `#` and for `[ ... ]` are both wrong in\n         // that it encompasses more than each token, but it hopefully is \"good\n         // enough\" for now at least.\n-        builder.push(tokenstream::TokenTree::Token(attr.span, Pound));\n+        builder.push(tokenstream::TokenTree::token(Pound, attr.span));\n         let delim_span = DelimSpan::from_single(attr.span);\n         builder.push(tokenstream::TokenTree::Delimited(\n             delim_span, DelimToken::Bracket, brackets.build().into()));"}, {"sha": "07acfb5dc86c3f7feb152d8f98cacf4e83ff8024", "filename": "src/libsyntax/print/pprust.rs", "status": "modified", "additions": 6, "deletions": 6, "changes": 12, "blob_url": "https://github.com/rust-lang/rust/blob/ca1bcfdde3f19afd68ef808cecf2ce56d08d5df4/src%2Flibsyntax%2Fprint%2Fpprust.rs", "raw_url": "https://github.com/rust-lang/rust/raw/ca1bcfdde3f19afd68ef808cecf2ce56d08d5df4/src%2Flibsyntax%2Fprint%2Fpprust.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fprint%2Fpprust.rs?ref=ca1bcfdde3f19afd68ef808cecf2ce56d08d5df4", "patch": "@@ -6,7 +6,7 @@ use crate::ast::{Attribute, MacDelimiter, GenericArg};\n use crate::util::parser::{self, AssocOp, Fixity};\n use crate::attr;\n use crate::source_map::{self, SourceMap, Spanned};\n-use crate::parse::token::{self, BinOpToken, Nonterminal, Token};\n+use crate::parse::token::{self, BinOpToken, Nonterminal, TokenKind};\n use crate::parse::lexer::comments;\n use crate::parse::{self, ParseSess};\n use crate::print::pp::{self, Breaks};\n@@ -189,7 +189,7 @@ pub fn literal_to_string(lit: token::Lit) -> String {\n     out\n }\n \n-pub fn token_to_string(tok: &Token) -> String {\n+pub fn token_to_string(tok: &TokenKind) -> String {\n     match *tok {\n         token::Eq                   => \"=\".to_string(),\n         token::Lt                   => \"<\".to_string(),\n@@ -724,10 +724,10 @@ pub trait PrintState<'a> {\n     /// expression arguments as expressions). It can be done! I think.\n     fn print_tt(&mut self, tt: tokenstream::TokenTree) -> io::Result<()> {\n         match tt {\n-            TokenTree::Token(_, ref tk) => {\n-                self.writer().word(token_to_string(tk))?;\n-                match *tk {\n-                    parse::token::DocComment(..) => {\n+            TokenTree::Token(ref token) => {\n+                self.writer().word(token_to_string(&token))?;\n+                match token.kind {\n+                    token::DocComment(..) => {\n                         self.writer().hardbreak()\n                     }\n                     _ => Ok(())"}, {"sha": "9dea3a4dcc144ed1981b073ab38bb6805d62d4c5", "filename": "src/libsyntax/tokenstream.rs", "status": "modified", "additions": 58, "deletions": 61, "changes": 119, "blob_url": "https://github.com/rust-lang/rust/blob/ca1bcfdde3f19afd68ef808cecf2ce56d08d5df4/src%2Flibsyntax%2Ftokenstream.rs", "raw_url": "https://github.com/rust-lang/rust/raw/ca1bcfdde3f19afd68ef808cecf2ce56d08d5df4/src%2Flibsyntax%2Ftokenstream.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Ftokenstream.rs?ref=ca1bcfdde3f19afd68ef808cecf2ce56d08d5df4", "patch": "@@ -16,7 +16,7 @@\n use crate::ext::base;\n use crate::ext::tt::{macro_parser, quoted};\n use crate::parse::Directory;\n-use crate::parse::token::{self, DelimToken, Token};\n+use crate::parse::token::{self, DelimToken, Token, TokenKind};\n use crate::print::pprust;\n \n use syntax_pos::{BytePos, Mark, Span, DUMMY_SP};\n@@ -44,7 +44,7 @@ use std::{fmt, iter, mem};\n #[derive(Debug, Clone, PartialEq, RustcEncodable, RustcDecodable)]\n pub enum TokenTree {\n     /// A single token\n-    Token(Span, token::Token),\n+    Token(Token),\n     /// A delimited sequence of token trees\n     Delimited(DelimSpan, DelimToken, TokenStream),\n }\n@@ -53,8 +53,7 @@ pub enum TokenTree {\n #[cfg(parallel_compiler)]\n fn _dummy()\n where\n-    Span: Send + Sync,\n-    token::Token: Send + Sync,\n+    Token: Send + Sync,\n     DelimSpan: Send + Sync,\n     DelimToken: Send + Sync,\n     TokenStream: Send + Sync,\n@@ -86,12 +85,11 @@ impl TokenTree {\n     /// Checks if this TokenTree is equal to the other, regardless of span information.\n     pub fn eq_unspanned(&self, other: &TokenTree) -> bool {\n         match (self, other) {\n-            (&TokenTree::Token(_, ref tk), &TokenTree::Token(_, ref tk2)) => tk == tk2,\n-            (&TokenTree::Delimited(_, delim, ref tts),\n-             &TokenTree::Delimited(_, delim2, ref tts2)) => {\n+            (TokenTree::Token(token), TokenTree::Token(token2)) => token.kind == token2.kind,\n+            (TokenTree::Delimited(_, delim, tts), TokenTree::Delimited(_, delim2, tts2)) => {\n                 delim == delim2 && tts.eq_unspanned(&tts2)\n             }\n-            (_, _) => false,\n+            _ => false,\n         }\n     }\n \n@@ -102,37 +100,36 @@ impl TokenTree {\n     // different method.\n     pub fn probably_equal_for_proc_macro(&self, other: &TokenTree) -> bool {\n         match (self, other) {\n-            (&TokenTree::Token(_, ref tk), &TokenTree::Token(_, ref tk2)) => {\n-                tk.probably_equal_for_proc_macro(tk2)\n+            (TokenTree::Token(token), TokenTree::Token(token2)) => {\n+                token.probably_equal_for_proc_macro(token2)\n             }\n-            (&TokenTree::Delimited(_, delim, ref tts),\n-             &TokenTree::Delimited(_, delim2, ref tts2)) => {\n+            (TokenTree::Delimited(_, delim, tts), TokenTree::Delimited(_, delim2, tts2)) => {\n                 delim == delim2 && tts.probably_equal_for_proc_macro(&tts2)\n             }\n-            (_, _) => false,\n+            _ => false,\n         }\n     }\n \n     /// Retrieves the TokenTree's span.\n     pub fn span(&self) -> Span {\n-        match *self {\n-            TokenTree::Token(sp, _) => sp,\n+        match self {\n+            TokenTree::Token(token) => token.span,\n             TokenTree::Delimited(sp, ..) => sp.entire(),\n         }\n     }\n \n     /// Modify the `TokenTree`'s span in-place.\n     pub fn set_span(&mut self, span: Span) {\n-        match *self {\n-            TokenTree::Token(ref mut sp, _) => *sp = span,\n-            TokenTree::Delimited(ref mut sp, ..) => *sp = DelimSpan::from_single(span),\n+        match self {\n+            TokenTree::Token(token) => token.span = span,\n+            TokenTree::Delimited(dspan, ..) => *dspan = DelimSpan::from_single(span),\n         }\n     }\n \n     /// Indicates if the stream is a token that is equal to the provided token.\n-    pub fn eq_token(&self, t: Token) -> bool {\n-        match *self {\n-            TokenTree::Token(_, ref tk) => *tk == t,\n+    pub fn eq_token(&self, t: TokenKind) -> bool {\n+        match self {\n+            TokenTree::Token(token) => *token == t,\n             _ => false,\n         }\n     }\n@@ -141,14 +138,18 @@ impl TokenTree {\n         TokenStream::new(vec![(self, Joint)])\n     }\n \n+    pub fn token(kind: TokenKind, span: Span) -> TokenTree {\n+        TokenTree::Token(Token::new(kind, span))\n+    }\n+\n     /// Returns the opening delimiter as a token tree.\n     pub fn open_tt(span: Span, delim: DelimToken) -> TokenTree {\n         let open_span = if span.is_dummy() {\n             span\n         } else {\n             span.with_hi(span.lo() + BytePos(delim.len() as u32))\n         };\n-        TokenTree::Token(open_span, token::OpenDelim(delim))\n+        TokenTree::token(token::OpenDelim(delim), open_span)\n     }\n \n     /// Returns the closing delimiter as a token tree.\n@@ -158,7 +159,7 @@ impl TokenTree {\n         } else {\n             span.with_lo(span.hi() - BytePos(delim.len() as u32))\n         };\n-        TokenTree::Token(close_span, token::CloseDelim(delim))\n+        TokenTree::token(token::CloseDelim(delim), close_span)\n     }\n }\n \n@@ -167,7 +168,7 @@ impl TokenTree {\n /// A `TokenStream` is an abstract sequence of tokens, organized into `TokenTree`s.\n /// The goal is for procedural macros to work with `TokenStream`s and `TokenTree`s\n /// instead of a representation of the abstract syntax tree.\n-/// Today's `TokenTree`s can still contain AST via `Token::Interpolated` for back-compat.\n+/// Today's `TokenTree`s can still contain AST via `token::Interpolated` for back-compat.\n ///\n /// The use of `Option` is an optimization that avoids the need for an\n /// allocation when the stream is empty. However, it is not guaranteed that an\n@@ -201,18 +202,18 @@ impl TokenStream {\n             while let Some((pos, ts)) = iter.next() {\n                 if let Some((_, next)) = iter.peek() {\n                     let sp = match (&ts, &next) {\n-                        (_, (TokenTree::Token(_, token::Token::Comma), _)) => continue,\n-                        ((TokenTree::Token(sp, token_left), NonJoint),\n-                         (TokenTree::Token(_, token_right), _))\n+                        (_, (TokenTree::Token(Token { kind: token::Comma, .. }), _)) => continue,\n+                        ((TokenTree::Token(token_left), NonJoint),\n+                         (TokenTree::Token(token_right), _))\n                         if ((token_left.is_ident() && !token_left.is_reserved_ident())\n                             || token_left.is_lit()) &&\n                             ((token_right.is_ident() && !token_right.is_reserved_ident())\n-                            || token_right.is_lit()) => *sp,\n+                            || token_right.is_lit()) => token_left.span,\n                         ((TokenTree::Delimited(sp, ..), NonJoint), _) => sp.entire(),\n                         _ => continue,\n                     };\n                     let sp = sp.shrink_to_hi();\n-                    let comma = (TokenTree::Token(sp, token::Comma), NonJoint);\n+                    let comma = (TokenTree::token(token::Comma, sp), NonJoint);\n                     suggestion = Some((pos, comma, sp));\n                 }\n             }\n@@ -241,12 +242,6 @@ impl From<TokenTree> for TreeAndJoint {\n     }\n }\n \n-impl From<Token> for TokenStream {\n-    fn from(token: Token) -> TokenStream {\n-        TokenTree::Token(DUMMY_SP, token).into()\n-    }\n-}\n-\n impl<T: Into<TokenStream>> iter::FromIterator<T> for TokenStream {\n     fn from_iter<I: IntoIterator<Item = T>>(iter: I) -> Self {\n         TokenStream::from_streams(iter.into_iter().map(Into::into).collect::<SmallVec<_>>())\n@@ -349,22 +344,25 @@ impl TokenStream {\n         // streams, making a comparison between a token stream generated from an\n         // AST and a token stream which was parsed into an AST more reliable.\n         fn semantic_tree(tree: &TokenTree) -> bool {\n-            match tree {\n-                // The pretty printer tends to add trailing commas to\n-                // everything, and in particular, after struct fields.\n-                | TokenTree::Token(_, Token::Comma)\n-                // The pretty printer emits `NoDelim` as whitespace.\n-                | TokenTree::Token(_, Token::OpenDelim(DelimToken::NoDelim))\n-                | TokenTree::Token(_, Token::CloseDelim(DelimToken::NoDelim))\n-                // The pretty printer collapses many semicolons into one.\n-                | TokenTree::Token(_, Token::Semi)\n-                // The pretty printer collapses whitespace arbitrarily and can\n-                // introduce whitespace from `NoDelim`.\n-                | TokenTree::Token(_, Token::Whitespace)\n-                // The pretty printer can turn `$crate` into `::crate_name`\n-                | TokenTree::Token(_, Token::ModSep) => false,\n-                _ => true\n+            if let TokenTree::Token(token) = tree {\n+                if let\n+                    // The pretty printer tends to add trailing commas to\n+                    // everything, and in particular, after struct fields.\n+                    | token::Comma\n+                    // The pretty printer emits `NoDelim` as whitespace.\n+                    | token::OpenDelim(DelimToken::NoDelim)\n+                    | token::CloseDelim(DelimToken::NoDelim)\n+                    // The pretty printer collapses many semicolons into one.\n+                    | token::Semi\n+                    // The pretty printer collapses whitespace arbitrarily and can\n+                    // introduce whitespace from `NoDelim`.\n+                    | token::Whitespace\n+                    // The pretty printer can turn `$crate` into `::crate_name`\n+                    | token::ModSep = token.kind {\n+                    return false;\n+                }\n             }\n+            true\n         }\n \n         let mut t1 = self.trees().filter(semantic_tree);\n@@ -430,13 +428,13 @@ impl TokenStreamBuilder {\n     pub fn push<T: Into<TokenStream>>(&mut self, stream: T) {\n         let stream = stream.into();\n         let last_tree_if_joint = self.0.last().and_then(TokenStream::last_tree_if_joint);\n-        if let Some(TokenTree::Token(last_span, last_tok)) = last_tree_if_joint {\n-            if let Some((TokenTree::Token(span, tok), is_joint)) = stream.first_tree_and_joint() {\n-                if let Some(glued_tok) = last_tok.glue(tok) {\n+        if let Some(TokenTree::Token(last_token)) = last_tree_if_joint {\n+            if let Some((TokenTree::Token(token), is_joint)) = stream.first_tree_and_joint() {\n+                if let Some(glued_tok) = last_token.kind.glue(token.kind) {\n                     let last_stream = self.0.pop().unwrap();\n                     self.push_all_but_last_tree(&last_stream);\n-                    let glued_span = last_span.to(span);\n-                    let glued_tt = TokenTree::Token(glued_span, glued_tok);\n+                    let glued_span = last_token.span.to(token.span);\n+                    let glued_tt = TokenTree::token(glued_tok, glued_span);\n                     let glued_tokenstream = TokenStream::new(vec![(glued_tt, is_joint)]);\n                     self.0.push(glued_tokenstream);\n                     self.push_all_but_first_tree(&stream);\n@@ -578,9 +576,8 @@ impl DelimSpan {\n #[cfg(test)]\n mod tests {\n     use super::*;\n-    use crate::syntax::ast::Ident;\n+    use crate::ast::Name;\n     use crate::with_default_globals;\n-    use crate::parse::token::Token;\n     use crate::util::parser_testing::string_to_stream;\n     use syntax_pos::{Span, BytePos, NO_EXPANSION};\n \n@@ -664,7 +661,7 @@ mod tests {\n         with_default_globals(|| {\n             let test0: TokenStream = Vec::<TokenTree>::new().into_iter().collect();\n             let test1: TokenStream =\n-                TokenTree::Token(sp(0, 1), Token::Ident(Ident::from_str(\"a\"), false)).into();\n+                TokenTree::token(token::Ident(Name::intern(\"a\"), false), sp(0, 1)).into();\n             let test2 = string_to_ts(\"foo(bar::baz)\");\n \n             assert_eq!(test0.is_empty(), true);\n@@ -677,9 +674,9 @@ mod tests {\n     fn test_dotdotdot() {\n         with_default_globals(|| {\n             let mut builder = TokenStreamBuilder::new();\n-            builder.push(TokenTree::Token(sp(0, 1), Token::Dot).joint());\n-            builder.push(TokenTree::Token(sp(1, 2), Token::Dot).joint());\n-            builder.push(TokenTree::Token(sp(2, 3), Token::Dot));\n+            builder.push(TokenTree::token(token::Dot, sp(0, 1)).joint());\n+            builder.push(TokenTree::token(token::Dot, sp(1, 2)).joint());\n+            builder.push(TokenTree::token(token::Dot, sp(2, 3)));\n             let stream = builder.build();\n             assert!(stream.eq_unspanned(&string_to_ts(\"...\")));\n             assert_eq!(stream.trees().count(), 1);"}, {"sha": "9e26f1bf7d374644586f33e6e4c276044ed0b6df", "filename": "src/libsyntax/util/parser.rs", "status": "modified", "additions": 26, "deletions": 26, "changes": 52, "blob_url": "https://github.com/rust-lang/rust/blob/ca1bcfdde3f19afd68ef808cecf2ce56d08d5df4/src%2Flibsyntax%2Futil%2Fparser.rs", "raw_url": "https://github.com/rust-lang/rust/raw/ca1bcfdde3f19afd68ef808cecf2ce56d08d5df4/src%2Flibsyntax%2Futil%2Fparser.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Futil%2Fparser.rs?ref=ca1bcfdde3f19afd68ef808cecf2ce56d08d5df4", "patch": "@@ -1,4 +1,4 @@\n-use crate::parse::token::{Token, BinOpToken};\n+use crate::parse::token::{self, TokenKind, BinOpToken};\n use crate::symbol::kw;\n use crate::ast::{self, BinOpKind};\n \n@@ -69,34 +69,34 @@ pub enum Fixity {\n \n impl AssocOp {\n     /// Creates a new AssocOP from a token\n-    pub fn from_token(t: &Token) -> Option<AssocOp> {\n+    pub fn from_token(t: &TokenKind) -> Option<AssocOp> {\n         use AssocOp::*;\n         match *t {\n-            Token::BinOpEq(k) => Some(AssignOp(k)),\n-            Token::Eq => Some(Assign),\n-            Token::BinOp(BinOpToken::Star) => Some(Multiply),\n-            Token::BinOp(BinOpToken::Slash) => Some(Divide),\n-            Token::BinOp(BinOpToken::Percent) => Some(Modulus),\n-            Token::BinOp(BinOpToken::Plus) => Some(Add),\n-            Token::BinOp(BinOpToken::Minus) => Some(Subtract),\n-            Token::BinOp(BinOpToken::Shl) => Some(ShiftLeft),\n-            Token::BinOp(BinOpToken::Shr) => Some(ShiftRight),\n-            Token::BinOp(BinOpToken::And) => Some(BitAnd),\n-            Token::BinOp(BinOpToken::Caret) => Some(BitXor),\n-            Token::BinOp(BinOpToken::Or) => Some(BitOr),\n-            Token::Lt => Some(Less),\n-            Token::Le => Some(LessEqual),\n-            Token::Ge => Some(GreaterEqual),\n-            Token::Gt => Some(Greater),\n-            Token::EqEq => Some(Equal),\n-            Token::Ne => Some(NotEqual),\n-            Token::AndAnd => Some(LAnd),\n-            Token::OrOr => Some(LOr),\n-            Token::DotDot => Some(DotDot),\n-            Token::DotDotEq => Some(DotDotEq),\n+            token::BinOpEq(k) => Some(AssignOp(k)),\n+            token::Eq => Some(Assign),\n+            token::BinOp(BinOpToken::Star) => Some(Multiply),\n+            token::BinOp(BinOpToken::Slash) => Some(Divide),\n+            token::BinOp(BinOpToken::Percent) => Some(Modulus),\n+            token::BinOp(BinOpToken::Plus) => Some(Add),\n+            token::BinOp(BinOpToken::Minus) => Some(Subtract),\n+            token::BinOp(BinOpToken::Shl) => Some(ShiftLeft),\n+            token::BinOp(BinOpToken::Shr) => Some(ShiftRight),\n+            token::BinOp(BinOpToken::And) => Some(BitAnd),\n+            token::BinOp(BinOpToken::Caret) => Some(BitXor),\n+            token::BinOp(BinOpToken::Or) => Some(BitOr),\n+            token::Lt => Some(Less),\n+            token::Le => Some(LessEqual),\n+            token::Ge => Some(GreaterEqual),\n+            token::Gt => Some(Greater),\n+            token::EqEq => Some(Equal),\n+            token::Ne => Some(NotEqual),\n+            token::AndAnd => Some(LAnd),\n+            token::OrOr => Some(LOr),\n+            token::DotDot => Some(DotDot),\n+            token::DotDotEq => Some(DotDotEq),\n             // DotDotDot is no longer supported, but we need some way to display the error\n-            Token::DotDotDot => Some(DotDotEq),\n-            Token::Colon => Some(Colon),\n+            token::DotDotDot => Some(DotDotEq),\n+            token::Colon => Some(Colon),\n             _ if t.is_keyword(kw::As) => Some(As),\n             _ => None\n         }"}, {"sha": "4e6a8274a478caa6ceb82057a9898e26c01a1774", "filename": "src/libsyntax/visit.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/ca1bcfdde3f19afd68ef808cecf2ce56d08d5df4/src%2Flibsyntax%2Fvisit.rs", "raw_url": "https://github.com/rust-lang/rust/raw/ca1bcfdde3f19afd68ef808cecf2ce56d08d5df4/src%2Flibsyntax%2Fvisit.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fvisit.rs?ref=ca1bcfdde3f19afd68ef808cecf2ce56d08d5df4", "patch": "@@ -855,7 +855,7 @@ pub fn walk_attribute<'a, V: Visitor<'a>>(visitor: &mut V, attr: &'a Attribute)\n \n pub fn walk_tt<'a, V: Visitor<'a>>(visitor: &mut V, tt: TokenTree) {\n     match tt {\n-        TokenTree::Token(_, tok) => visitor.visit_token(tok),\n+        TokenTree::Token(token) => visitor.visit_token(token),\n         TokenTree::Delimited(_, _, tts) => visitor.visit_tts(tts),\n     }\n }"}, {"sha": "b015815ac9c1ea2b79b1554e3dc36c07dad40346", "filename": "src/libsyntax_ext/asm.rs", "status": "modified", "additions": 5, "deletions": 4, "changes": 9, "blob_url": "https://github.com/rust-lang/rust/blob/ca1bcfdde3f19afd68ef808cecf2ce56d08d5df4/src%2Flibsyntax_ext%2Fasm.rs", "raw_url": "https://github.com/rust-lang/rust/raw/ca1bcfdde3f19afd68ef808cecf2ce56d08d5df4/src%2Flibsyntax_ext%2Fasm.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax_ext%2Fasm.rs?ref=ca1bcfdde3f19afd68ef808cecf2ce56d08d5df4", "patch": "@@ -9,7 +9,8 @@ use errors::DiagnosticBuilder;\n use syntax::ast;\n use syntax::ext::base::{self, *};\n use syntax::feature_gate;\n-use syntax::parse::{self, token};\n+use syntax::parse;\n+use syntax::parse::token::{self, Token};\n use syntax::ptr::P;\n use syntax::symbol::{kw, sym, Symbol};\n use syntax::ast::AsmDialect;\n@@ -86,8 +87,8 @@ fn parse_inline_asm<'a>(\n     let first_colon = tts.iter()\n         .position(|tt| {\n             match *tt {\n-                tokenstream::TokenTree::Token(_, token::Colon) |\n-                tokenstream::TokenTree::Token(_, token::ModSep) => true,\n+                tokenstream::TokenTree::Token(Token { kind: token::Colon, .. }) |\n+                tokenstream::TokenTree::Token(Token { kind: token::ModSep, .. }) => true,\n                 _ => false,\n             }\n         })\n@@ -259,7 +260,7 @@ fn parse_inline_asm<'a>(\n         loop {\n             // MOD_SEP is a double colon '::' without space in between.\n             // When encountered, the state must be advanced twice.\n-            match (&p.token, state.next(), state.next().next()) {\n+            match (&p.token.kind, state.next(), state.next().next()) {\n                 (&token::Colon, StateNone, _) |\n                 (&token::ModSep, _, StateNone) => {\n                     p.bump();"}, {"sha": "3886528c74c2f6703b0d401e3f3ada021ee486f2", "filename": "src/libsyntax_ext/assert.rs", "status": "modified", "additions": 6, "deletions": 5, "changes": 11, "blob_url": "https://github.com/rust-lang/rust/blob/ca1bcfdde3f19afd68ef808cecf2ce56d08d5df4/src%2Flibsyntax_ext%2Fassert.rs", "raw_url": "https://github.com/rust-lang/rust/raw/ca1bcfdde3f19afd68ef808cecf2ce56d08d5df4/src%2Flibsyntax_ext%2Fassert.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax_ext%2Fassert.rs?ref=ca1bcfdde3f19afd68ef808cecf2ce56d08d5df4", "patch": "@@ -4,7 +4,7 @@ use syntax::ast::{self, *};\n use syntax::source_map::Spanned;\n use syntax::ext::base::*;\n use syntax::ext::build::AstBuilder;\n-use syntax::parse::token::{self, Token};\n+use syntax::parse::token::{self, TokenKind};\n use syntax::parse::parser::Parser;\n use syntax::print::pprust;\n use syntax::ptr::P;\n@@ -29,12 +29,12 @@ pub fn expand_assert<'cx>(\n     let panic_call = Mac_ {\n         path: Path::from_ident(Ident::new(sym::panic, sp)),\n         tts: custom_message.unwrap_or_else(|| {\n-            TokenStream::from(TokenTree::Token(\n-                DUMMY_SP,\n-                Token::lit(token::Str, Symbol::intern(&format!(\n+            TokenStream::from(TokenTree::token(\n+                TokenKind::lit(token::Str, Symbol::intern(&format!(\n                     \"assertion failed: {}\",\n                     pprust::expr_to_string(&cond_expr).escape_debug()\n                 )), None),\n+                DUMMY_SP,\n             ))\n         }).into(),\n         delim: MacDelimiter::Parenthesis,\n@@ -103,7 +103,8 @@ fn parse_assert<'a>(\n     //\n     // Parse this as an actual message, and suggest inserting a comma. Eventually, this should be\n     // turned into an error.\n-    let custom_message = if let token::Literal(token::Lit { kind: token::Str, .. }) = parser.token {\n+    let custom_message = if let token::Literal(token::Lit { kind: token::Str, .. })\n+                                = parser.token.kind {\n         let mut err = cx.struct_span_warn(parser.span, \"unexpected string literal\");\n         let comma_span = cx.source_map().next_point(parser.prev_span);\n         err.span_suggestion_short("}, {"sha": "8f061abc77b8d0f91eae10c5e79af82dd397c7a2", "filename": "src/libsyntax_ext/concat_idents.rs", "status": "modified", "additions": 4, "deletions": 4, "changes": 8, "blob_url": "https://github.com/rust-lang/rust/blob/ca1bcfdde3f19afd68ef808cecf2ce56d08d5df4/src%2Flibsyntax_ext%2Fconcat_idents.rs", "raw_url": "https://github.com/rust-lang/rust/raw/ca1bcfdde3f19afd68ef808cecf2ce56d08d5df4/src%2Flibsyntax_ext%2Fconcat_idents.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax_ext%2Fconcat_idents.rs?ref=ca1bcfdde3f19afd68ef808cecf2ce56d08d5df4", "patch": "@@ -3,7 +3,7 @@ use rustc_data_structures::thin_vec::ThinVec;\n use syntax::ast;\n use syntax::ext::base::{self, *};\n use syntax::feature_gate;\n-use syntax::parse::token;\n+use syntax::parse::token::{self, Token};\n use syntax::ptr::P;\n use syntax_pos::Span;\n use syntax_pos::symbol::{Symbol, sym};\n@@ -30,16 +30,16 @@ pub fn expand_syntax_ext<'cx>(cx: &'cx mut ExtCtxt<'_>,\n     for (i, e) in tts.iter().enumerate() {\n         if i & 1 == 1 {\n             match *e {\n-                TokenTree::Token(_, token::Comma) => {}\n+                TokenTree::Token(Token { kind: token::Comma, .. }) => {}\n                 _ => {\n                     cx.span_err(sp, \"concat_idents! expecting comma.\");\n                     return DummyResult::any(sp);\n                 }\n             }\n         } else {\n             match *e {\n-                TokenTree::Token(_, token::Ident(ident, _)) =>\n-                    res_str.push_str(&ident.as_str()),\n+                TokenTree::Token(Token { kind: token::Ident(name, _), .. }) =>\n+                    res_str.push_str(&name.as_str()),\n                 _ => {\n                     cx.span_err(sp, \"concat_idents! requires ident args.\");\n                     return DummyResult::any(sp);"}, {"sha": "98465d75e4680e9d31beb6665f871e22e0ade22b", "filename": "src/libsyntax_ext/deriving/custom.rs", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "blob_url": "https://github.com/rust-lang/rust/blob/ca1bcfdde3f19afd68ef808cecf2ce56d08d5df4/src%2Flibsyntax_ext%2Fderiving%2Fcustom.rs", "raw_url": "https://github.com/rust-lang/rust/raw/ca1bcfdde3f19afd68ef808cecf2ce56d08d5df4/src%2Flibsyntax_ext%2Fderiving%2Fcustom.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax_ext%2Fderiving%2Fcustom.rs?ref=ca1bcfdde3f19afd68ef808cecf2ce56d08d5df4", "patch": "@@ -8,7 +8,7 @@ use syntax::attr::{mark_used, mark_known};\n use syntax::source_map::Span;\n use syntax::ext::base::*;\n use syntax::parse;\n-use syntax::parse::token::{self, Token};\n+use syntax::parse::token;\n use syntax::tokenstream;\n use syntax::visit::Visitor;\n use syntax_pos::DUMMY_SP;\n@@ -68,8 +68,8 @@ impl MultiItemModifier for ProcMacroDerive {\n         // Mark attributes as known, and used.\n         MarkAttrs(&self.attrs).visit_item(&item);\n \n-        let token = Token::Interpolated(Lrc::new(token::NtItem(item)));\n-        let input = tokenstream::TokenTree::Token(DUMMY_SP, token).into();\n+        let token = token::Interpolated(Lrc::new(token::NtItem(item)));\n+        let input = tokenstream::TokenTree::token(token, DUMMY_SP).into();\n \n         let server = proc_macro_server::Rustc::new(ecx);\n         let stream = match self.client.run(&EXEC_STRATEGY, server, input) {"}, {"sha": "c78215b77a973d86dbfbc698681ce12aeccc1a13", "filename": "src/libsyntax_ext/format.rs", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "blob_url": "https://github.com/rust-lang/rust/blob/ca1bcfdde3f19afd68ef808cecf2ce56d08d5df4/src%2Flibsyntax_ext%2Fformat.rs", "raw_url": "https://github.com/rust-lang/rust/raw/ca1bcfdde3f19afd68ef808cecf2ce56d08d5df4/src%2Flibsyntax_ext%2Fformat.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax_ext%2Fformat.rs?ref=ca1bcfdde3f19afd68ef808cecf2ce56d08d5df4", "patch": "@@ -149,16 +149,16 @@ fn parse_args<'a>(\n         } // accept trailing commas\n         if named || (p.token.is_ident() && p.look_ahead(1, |t| *t == token::Eq)) {\n             named = true;\n-            let ident = if let token::Ident(i, _) = p.token {\n+            let name = if let token::Ident(name, _) = p.token.kind {\n                 p.bump();\n-                i\n+                name\n             } else {\n                 return Err(ecx.struct_span_err(\n                     p.span,\n                     \"expected ident, positional arguments cannot follow named arguments\",\n                 ));\n             };\n-            let name: &str = &ident.as_str();\n+            let name: &str = &name.as_str();\n \n             p.expect(&token::Eq)?;\n             let e = p.parse_expr()?;"}, {"sha": "29297aa913ed418c11c3e41f812db6e4478e5bd7", "filename": "src/libsyntax_ext/proc_macro_decls.rs", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/rust-lang/rust/blob/ca1bcfdde3f19afd68ef808cecf2ce56d08d5df4/src%2Flibsyntax_ext%2Fproc_macro_decls.rs", "raw_url": "https://github.com/rust-lang/rust/raw/ca1bcfdde3f19afd68ef808cecf2ce56d08d5df4/src%2Flibsyntax_ext%2Fproc_macro_decls.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax_ext%2Fproc_macro_decls.rs?ref=ca1bcfdde3f19afd68ef808cecf2ce56d08d5df4", "patch": "@@ -132,7 +132,7 @@ impl<'a> CollectProcMacros<'a> {\n             }\n         };\n \n-        if !trait_ident.can_be_raw() {\n+        if !trait_ident.name.can_be_raw() {\n             self.handler.span_err(trait_attr.span,\n                                   &format!(\"`{}` cannot be a name of derive macro\", trait_ident));\n         }\n@@ -166,7 +166,7 @@ impl<'a> CollectProcMacros<'a> {\n                         return None;\n                     }\n                 };\n-                if !ident.can_be_raw() {\n+                if !ident.name.can_be_raw() {\n                     self.handler.span_err(\n                         attr.span,\n                         &format!(\"`{}` cannot be a name of derive helper attribute\", ident),"}, {"sha": "00a420d3fa89922c9cd1758a5a99c9163e3700ac", "filename": "src/libsyntax_ext/proc_macro_server.rs", "status": "modified", "additions": 22, "deletions": 23, "changes": 45, "blob_url": "https://github.com/rust-lang/rust/blob/ca1bcfdde3f19afd68ef808cecf2ce56d08d5df4/src%2Flibsyntax_ext%2Fproc_macro_server.rs", "raw_url": "https://github.com/rust-lang/rust/raw/ca1bcfdde3f19afd68ef808cecf2ce56d08d5df4/src%2Flibsyntax_ext%2Fproc_macro_server.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax_ext%2Fproc_macro_server.rs?ref=ca1bcfdde3f19afd68ef808cecf2ce56d08d5df4", "patch": "@@ -55,7 +55,7 @@ impl FromInternal<(TreeAndJoint, &'_ ParseSess, &'_ mut Vec<Self>)>\n         use syntax::parse::token::*;\n \n         let joint = is_joint == Joint;\n-        let (span, token) = match tree {\n+        let Token { kind, span } = match tree {\n             tokenstream::TokenTree::Delimited(span, delim, tts) => {\n                 let delimiter = Delimiter::from_internal(delim);\n                 return TokenTree::Group(Group {\n@@ -64,7 +64,7 @@ impl FromInternal<(TreeAndJoint, &'_ ParseSess, &'_ mut Vec<Self>)>\n                     span,\n                 });\n             }\n-            tokenstream::TokenTree::Token(span, token) => (span, token),\n+            tokenstream::TokenTree::Token(token) => token,\n         };\n \n         macro_rules! tt {\n@@ -93,7 +93,7 @@ impl FromInternal<(TreeAndJoint, &'_ ParseSess, &'_ mut Vec<Self>)>\n             }};\n         }\n \n-        match token {\n+        match kind {\n             Eq => op!('='),\n             Lt => op!('<'),\n             Le => op!('<', '='),\n@@ -142,11 +142,10 @@ impl FromInternal<(TreeAndJoint, &'_ ParseSess, &'_ mut Vec<Self>)>\n             Question => op!('?'),\n             SingleQuote => op!('\\''),\n \n-            Ident(ident, false) if ident.name == kw::DollarCrate =>\n-                tt!(Ident::dollar_crate()),\n-            Ident(ident, is_raw) => tt!(Ident::new(ident.name, is_raw)),\n-            Lifetime(ident) => {\n-                let ident = ident.without_first_quote();\n+            Ident(name, false) if name == kw::DollarCrate => tt!(Ident::dollar_crate()),\n+            Ident(name, is_raw) => tt!(Ident::new(name, is_raw)),\n+            Lifetime(name) => {\n+                let ident = ast::Ident::new(name, span).without_first_quote();\n                 stack.push(tt!(Ident::new(ident.name, false)));\n                 tt!(Punct::new('\\'', true))\n             }\n@@ -159,12 +158,12 @@ impl FromInternal<(TreeAndJoint, &'_ ParseSess, &'_ mut Vec<Self>)>\n                     escaped.extend(ch.escape_debug());\n                 }\n                 let stream = vec![\n-                    Ident(ast::Ident::new(sym::doc, span), false),\n+                    Ident(sym::doc, false),\n                     Eq,\n-                    Token::lit(token::Str, Symbol::intern(&escaped), None),\n+                    TokenKind::lit(token::Str, Symbol::intern(&escaped), None),\n                 ]\n                 .into_iter()\n-                .map(|token| tokenstream::TokenTree::Token(span, token))\n+                .map(|kind| tokenstream::TokenTree::token(kind, span))\n                 .collect();\n                 stack.push(TokenTree::Group(Group {\n                     delimiter: Delimiter::Bracket,\n@@ -211,18 +210,17 @@ impl ToInternal<TokenStream> for TokenTree<Group, Punct, Ident, Literal> {\n                 .into();\n             }\n             TokenTree::Ident(self::Ident { sym, is_raw, span }) => {\n-                let token = Ident(ast::Ident::new(sym, span), is_raw);\n-                return tokenstream::TokenTree::Token(span, token).into();\n+                return tokenstream::TokenTree::token(Ident(sym, is_raw), span).into();\n             }\n             TokenTree::Literal(self::Literal {\n                 lit: token::Lit { kind: token::Integer, symbol, suffix },\n                 span,\n             }) if symbol.as_str().starts_with(\"-\") => {\n                 let minus = BinOp(BinOpToken::Minus);\n                 let symbol = Symbol::intern(&symbol.as_str()[1..]);\n-                let integer = Token::lit(token::Integer, symbol, suffix);\n-                let a = tokenstream::TokenTree::Token(span, minus);\n-                let b = tokenstream::TokenTree::Token(span, integer);\n+                let integer = TokenKind::lit(token::Integer, symbol, suffix);\n+                let a = tokenstream::TokenTree::token(minus, span);\n+                let b = tokenstream::TokenTree::token(integer, span);\n                 return vec![a, b].into_iter().collect();\n             }\n             TokenTree::Literal(self::Literal {\n@@ -231,17 +229,17 @@ impl ToInternal<TokenStream> for TokenTree<Group, Punct, Ident, Literal> {\n             }) if symbol.as_str().starts_with(\"-\") => {\n                 let minus = BinOp(BinOpToken::Minus);\n                 let symbol = Symbol::intern(&symbol.as_str()[1..]);\n-                let float = Token::lit(token::Float, symbol, suffix);\n-                let a = tokenstream::TokenTree::Token(span, minus);\n-                let b = tokenstream::TokenTree::Token(span, float);\n+                let float = TokenKind::lit(token::Float, symbol, suffix);\n+                let a = tokenstream::TokenTree::token(minus, span);\n+                let b = tokenstream::TokenTree::token(float, span);\n                 return vec![a, b].into_iter().collect();\n             }\n             TokenTree::Literal(self::Literal { lit, span }) => {\n-                return tokenstream::TokenTree::Token(span, Literal(lit)).into()\n+                return tokenstream::TokenTree::token(Literal(lit), span).into()\n             }\n         };\n \n-        let token = match ch {\n+        let kind = match ch {\n             '=' => Eq,\n             '<' => Lt,\n             '>' => Gt,\n@@ -267,7 +265,7 @@ impl ToInternal<TokenStream> for TokenTree<Group, Punct, Ident, Literal> {\n             _ => unreachable!(),\n         };\n \n-        let tree = tokenstream::TokenTree::Token(span, token);\n+        let tree = tokenstream::TokenTree::token(kind, span);\n         TokenStream::new(vec![(tree, if joint { Joint } else { NonJoint })])\n     }\n }\n@@ -338,7 +336,8 @@ impl Ident {\n         if !Self::is_valid(&string) {\n             panic!(\"`{:?}` is not a valid identifier\", string)\n         }\n-        if is_raw && !ast::Ident::from_interned_str(sym.as_interned_str()).can_be_raw() {\n+        // Get rid of gensyms to conservatively check rawness on the string contents only.\n+        if is_raw && !sym.as_interned_str().as_symbol().can_be_raw() {\n             panic!(\"`{}` cannot be a raw identifier\", string);\n         }\n         Ident { sym, is_raw, span }"}, {"sha": "6c74f77ff1fb5c4268c1248591c22e21bbd41f29", "filename": "src/libsyntax_ext/trace_macros.rs", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/rust-lang/rust/blob/ca1bcfdde3f19afd68ef808cecf2ce56d08d5df4/src%2Flibsyntax_ext%2Ftrace_macros.rs", "raw_url": "https://github.com/rust-lang/rust/raw/ca1bcfdde3f19afd68ef808cecf2ce56d08d5df4/src%2Flibsyntax_ext%2Ftrace_macros.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax_ext%2Ftrace_macros.rs?ref=ca1bcfdde3f19afd68ef808cecf2ce56d08d5df4", "patch": "@@ -17,10 +17,10 @@ pub fn expand_trace_macros(cx: &mut ExtCtxt<'_>,\n     }\n \n     match (tt.len(), tt.first()) {\n-        (1, Some(&TokenTree::Token(_, ref tok))) if tok.is_keyword(kw::True) => {\n+        (1, Some(TokenTree::Token(token))) if token.is_keyword(kw::True) => {\n             cx.set_trace_macros(true);\n         }\n-        (1, Some(&TokenTree::Token(_, ref tok))) if tok.is_keyword(kw::False) => {\n+        (1, Some(TokenTree::Token(token))) if token.is_keyword(kw::False) => {\n             cx.set_trace_macros(false);\n         }\n         _ => cx.span_err(sp, \"trace_macros! accepts only `true` or `false`\"),"}, {"sha": "5dd4d6566ed1c3bfa2568218f09937dce57b9974", "filename": "src/libsyntax_pos/symbol.rs", "status": "modified", "additions": 19, "deletions": 16, "changes": 35, "blob_url": "https://github.com/rust-lang/rust/blob/ca1bcfdde3f19afd68ef808cecf2ce56d08d5df4/src%2Flibsyntax_pos%2Fsymbol.rs", "raw_url": "https://github.com/rust-lang/rust/raw/ca1bcfdde3f19afd68ef808cecf2ce56d08d5df4/src%2Flibsyntax_pos%2Fsymbol.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax_pos%2Fsymbol.rs?ref=ca1bcfdde3f19afd68ef808cecf2ce56d08d5df4", "patch": "@@ -921,10 +921,9 @@ pub struct Interner {\n \n impl Interner {\n     fn prefill(init: &[&'static str]) -> Self {\n-        let symbols = (0 .. init.len() as u32).map(Symbol::new);\n         Interner {\n-            strings: init.to_vec(),\n-            names: init.iter().copied().zip(symbols).collect(),\n+            strings: init.into(),\n+            names: init.iter().copied().zip((0..).map(Symbol::new)).collect(),\n             ..Default::default()\n         }\n     }\n@@ -1019,6 +1018,21 @@ impl Symbol {\n     pub fn is_doc_keyword(self) -> bool {\n         self <= kw::Union\n     }\n+\n+    /// A keyword or reserved identifier that can be used as a path segment.\n+    pub fn is_path_segment_keyword(self) -> bool {\n+        self == kw::Super ||\n+        self == kw::SelfLower ||\n+        self == kw::SelfUpper ||\n+        self == kw::Crate ||\n+        self == kw::PathRoot ||\n+        self == kw::DollarCrate\n+    }\n+\n+    /// This symbol can be a raw identifier.\n+    pub fn can_be_raw(self) -> bool {\n+        self != kw::Invalid && self != kw::Underscore && !self.is_path_segment_keyword()\n+    }\n }\n \n impl Ident {\n@@ -1049,24 +1063,13 @@ impl Ident {\n \n     /// A keyword or reserved identifier that can be used as a path segment.\n     pub fn is_path_segment_keyword(self) -> bool {\n-        self.name == kw::Super ||\n-        self.name == kw::SelfLower ||\n-        self.name == kw::SelfUpper ||\n-        self.name == kw::Crate ||\n-        self.name == kw::PathRoot ||\n-        self.name == kw::DollarCrate\n-    }\n-\n-    /// This identifier can be a raw identifier.\n-    pub fn can_be_raw(self) -> bool {\n-        self.name != kw::Invalid && self.name != kw::Underscore &&\n-        !self.is_path_segment_keyword()\n+        self.name.is_path_segment_keyword()\n     }\n \n     /// We see this identifier in a normal identifier position, like variable name or a type.\n     /// How was it written originally? Did it use the raw form? Let's try to guess.\n     pub fn is_raw_guess(self) -> bool {\n-        self.can_be_raw() && self.is_reserved()\n+        self.name.can_be_raw() && self.is_reserved()\n     }\n }\n "}, {"sha": "4d9e0129e54db2d4d3a6b4e49113742271298a87", "filename": "src/test/run-pass-fulldeps/auxiliary/roman-numerals.rs", "status": "modified", "additions": 10, "deletions": 10, "changes": 20, "blob_url": "https://github.com/rust-lang/rust/blob/ca1bcfdde3f19afd68ef808cecf2ce56d08d5df4/src%2Ftest%2Frun-pass-fulldeps%2Fauxiliary%2Froman-numerals.rs", "raw_url": "https://github.com/rust-lang/rust/raw/ca1bcfdde3f19afd68ef808cecf2ce56d08d5df4/src%2Ftest%2Frun-pass-fulldeps%2Fauxiliary%2Froman-numerals.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ftest%2Frun-pass-fulldeps%2Fauxiliary%2Froman-numerals.rs?ref=ca1bcfdde3f19afd68ef808cecf2ce56d08d5df4", "patch": "@@ -1,3 +1,9 @@\n+// WARNING WARNING WARNING WARNING WARNING\n+// =======================================\n+//\n+// This code also appears in src/doc/unstable-book/src/language-features/plugin.md.\n+// Please keep the two copies in sync!  FIXME: have rustdoc read this file\n+\n // force-host\n \n #![crate_type=\"dylib\"]\n@@ -8,21 +14,15 @@ extern crate syntax_pos;\n extern crate rustc;\n extern crate rustc_plugin;\n \n-use syntax::parse::token;\n+use syntax::parse::token::{self, Token};\n use syntax::tokenstream::TokenTree;\n use syntax::ext::base::{ExtCtxt, MacResult, DummyResult, MacEager};\n-use syntax::ext::build::AstBuilder;  // trait for expr_usize\n+use syntax::ext::build::AstBuilder;  // A trait for expr_usize.\n use syntax_pos::Span;\n use rustc_plugin::Registry;\n \n-// WARNING WARNING WARNING WARNING WARNING\n-// =======================================\n-//\n-// This code also appears in src/doc/unstable-book/src/language-features/plugin.md.\n-// Please keep the two copies in sync!  FIXME: have rustdoc read this file\n-\n fn expand_rn(cx: &mut ExtCtxt, sp: Span, args: &[TokenTree])\n-        -> Box<MacResult + 'static> {\n+        -> Box<dyn MacResult + 'static> {\n \n     static NUMERALS: &'static [(&'static str, usize)] = &[\n         (\"M\", 1000), (\"CM\", 900), (\"D\", 500), (\"CD\", 400),\n@@ -38,7 +38,7 @@ fn expand_rn(cx: &mut ExtCtxt, sp: Span, args: &[TokenTree])\n     }\n \n     let text = match args[0] {\n-        TokenTree::Token(_, token::Ident(s, _)) => s.to_string(),\n+        TokenTree::Token(Token { kind: token::Ident(s, _), .. }) => s.to_string(),\n         _ => {\n             cx.span_err(sp, \"argument should be a single identifier\");\n             return DummyResult::any(sp);"}]}