{"sha": "ccffea5b6b3372cefd4e15bc738a2669bc6f69a0", "node_id": "MDY6Q29tbWl0NzI0NzEyOmNjZmZlYTViNmIzMzcyY2VmZDRlMTViYzczOGEyNjY5YmM2ZjY5YTA=", "commit": {"author": {"name": "Aleksey Kladov", "email": "aleksey.kladov@gmail.com", "date": "2020-08-29T10:10:16Z"}, "committer": {"name": "Aleksey Kladov", "email": "aleksey.kladov@gmail.com", "date": "2020-08-30T17:53:36Z"}, "message": "Move lexer unit tests to rustc_lexer\n\nStringReader is an intornal abstraction which at the moment changes a\nlot, so these unit tests cause quite a bit of friction.\n\nMoving them to rustc_lexer and more ingerated-testing style should\nmake them much less annoying, hopefully without decreasing their\nusefulness much.\n\nNote that coloncolon tests are removed (it's unclear what those are\ntesting).\n\n\\r\\n tests are removed as well, as we normalize line endings even\nbefore lexing.", "tree": {"sha": "161f0e5bad86487777bf89507534de16d17cb157", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/161f0e5bad86487777bf89507534de16d17cb157"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/ccffea5b6b3372cefd4e15bc738a2669bc6f69a0", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/ccffea5b6b3372cefd4e15bc738a2669bc6f69a0", "html_url": "https://github.com/rust-lang/rust/commit/ccffea5b6b3372cefd4e15bc738a2669bc6f69a0", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/ccffea5b6b3372cefd4e15bc738a2669bc6f69a0/comments", "author": {"login": "matklad", "id": 1711539, "node_id": "MDQ6VXNlcjE3MTE1Mzk=", "avatar_url": "https://avatars.githubusercontent.com/u/1711539?v=4", "gravatar_id": "", "url": "https://api.github.com/users/matklad", "html_url": "https://github.com/matklad", "followers_url": "https://api.github.com/users/matklad/followers", "following_url": "https://api.github.com/users/matklad/following{/other_user}", "gists_url": "https://api.github.com/users/matklad/gists{/gist_id}", "starred_url": "https://api.github.com/users/matklad/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/matklad/subscriptions", "organizations_url": "https://api.github.com/users/matklad/orgs", "repos_url": "https://api.github.com/users/matklad/repos", "events_url": "https://api.github.com/users/matklad/events{/privacy}", "received_events_url": "https://api.github.com/users/matklad/received_events", "type": "User", "site_admin": false}, "committer": {"login": "matklad", "id": 1711539, "node_id": "MDQ6VXNlcjE3MTE1Mzk=", "avatar_url": "https://avatars.githubusercontent.com/u/1711539?v=4", "gravatar_id": "", "url": "https://api.github.com/users/matklad", "html_url": "https://github.com/matklad", "followers_url": "https://api.github.com/users/matklad/followers", "following_url": "https://api.github.com/users/matklad/following{/other_user}", "gists_url": "https://api.github.com/users/matklad/gists{/gist_id}", "starred_url": "https://api.github.com/users/matklad/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/matklad/subscriptions", "organizations_url": "https://api.github.com/users/matklad/orgs", "repos_url": "https://api.github.com/users/matklad/repos", "events_url": "https://api.github.com/users/matklad/events{/privacy}", "received_events_url": "https://api.github.com/users/matklad/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "85fbf49ce0e2274d0acf798f6e703747674feec3", "url": "https://api.github.com/repos/rust-lang/rust/commits/85fbf49ce0e2274d0acf798f6e703747674feec3", "html_url": "https://github.com/rust-lang/rust/commit/85fbf49ce0e2274d0acf798f6e703747674feec3"}], "stats": {"total": 432, "additions": 154, "deletions": 278}, "files": [{"sha": "d5493969433927d3ed9fee3ee4a95576f8a9bfef", "filename": "Cargo.lock", "status": "modified", "additions": 14, "deletions": 1, "changes": 15, "blob_url": "https://github.com/rust-lang/rust/blob/ccffea5b6b3372cefd4e15bc738a2669bc6f69a0/Cargo.lock", "raw_url": "https://github.com/rust-lang/rust/raw/ccffea5b6b3372cefd4e15bc738a2669bc6f69a0/Cargo.lock", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/Cargo.lock?ref=ccffea5b6b3372cefd4e15bc738a2669bc6f69a0", "patch": "@@ -342,7 +342,7 @@ dependencies = [\n name = \"cargo-miri\"\n version = \"0.1.0\"\n dependencies = [\n- \"cargo_metadata 0.11.1\",\n+ \"cargo_metadata 0.9.1\",\n  \"directories\",\n  \"rustc-workspace-hack\",\n  \"rustc_version\",\n@@ -391,6 +391,18 @@ dependencies = [\n  \"serde_json\",\n ]\n \n+[[package]]\n+name = \"cargo_metadata\"\n+version = \"0.9.1\"\n+source = \"registry+https://github.com/rust-lang/crates.io-index\"\n+checksum = \"46e3374c604fb39d1a2f35ed5e4a4e30e60d01fab49446e08f1b3e9a90aef202\"\n+dependencies = [\n+ \"semver 0.9.0\",\n+ \"serde\",\n+ \"serde_derive\",\n+ \"serde_json\",\n+]\n+\n [[package]]\n name = \"cargo_metadata\"\n version = \"0.11.1\"\n@@ -1979,6 +1991,7 @@ dependencies = [\n name = \"miri\"\n version = \"0.1.0\"\n dependencies = [\n+ \"byteorder\",\n  \"colored\",\n  \"compiletest_rs\",\n  \"env_logger 0.7.1\","}, {"sha": "5436b1ef737f519ba7980bafaf62cbbf4ddb9ccc", "filename": "compiler/rustc_expand/src/lib.rs", "status": "modified", "additions": 0, "deletions": 5, "changes": 5, "blob_url": "https://github.com/rust-lang/rust/blob/ccffea5b6b3372cefd4e15bc738a2669bc6f69a0/compiler%2Frustc_expand%2Fsrc%2Flib.rs", "raw_url": "https://github.com/rust-lang/rust/raw/ccffea5b6b3372cefd4e15bc738a2669bc6f69a0/compiler%2Frustc_expand%2Fsrc%2Flib.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_expand%2Fsrc%2Flib.rs?ref=ccffea5b6b3372cefd4e15bc738a2669bc6f69a0", "patch": "@@ -39,11 +39,6 @@ mod tests;\n mod parse {\n     #[cfg(test)]\n     mod tests;\n-    #[cfg(test)]\n-    mod lexer {\n-        #[cfg(test)]\n-        mod tests;\n-    }\n }\n #[cfg(test)]\n mod tokenstream {"}, {"sha": "871844442839cf7fd27f1a735de3cb7be9dbc7b8", "filename": "compiler/rustc_expand/src/parse/lexer/tests.rs", "status": "removed", "additions": 0, "deletions": 252, "changes": 252, "blob_url": "https://github.com/rust-lang/rust/blob/85fbf49ce0e2274d0acf798f6e703747674feec3/compiler%2Frustc_expand%2Fsrc%2Fparse%2Flexer%2Ftests.rs", "raw_url": "https://github.com/rust-lang/rust/raw/85fbf49ce0e2274d0acf798f6e703747674feec3/compiler%2Frustc_expand%2Fsrc%2Fparse%2Flexer%2Ftests.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_expand%2Fsrc%2Fparse%2Flexer%2Ftests.rs?ref=85fbf49ce0e2274d0acf798f6e703747674feec3", "patch": "@@ -1,252 +0,0 @@\n-use rustc_ast::ast::AttrStyle;\n-use rustc_ast::token::{self, CommentKind, Token, TokenKind};\n-use rustc_data_structures::sync::Lrc;\n-use rustc_errors::{emitter::EmitterWriter, Handler};\n-use rustc_parse::lexer::StringReader;\n-use rustc_session::parse::ParseSess;\n-use rustc_span::source_map::{FilePathMapping, SourceMap};\n-use rustc_span::symbol::Symbol;\n-use rustc_span::with_default_session_globals;\n-use rustc_span::{BytePos, Span};\n-\n-use std::io;\n-use std::path::PathBuf;\n-\n-fn mk_sess(sm: Lrc<SourceMap>) -> ParseSess {\n-    let emitter = EmitterWriter::new(\n-        Box::new(io::sink()),\n-        Some(sm.clone()),\n-        false,\n-        false,\n-        false,\n-        None,\n-        false,\n-    );\n-    ParseSess::with_span_handler(Handler::with_emitter(true, None, Box::new(emitter)), sm)\n-}\n-\n-// Creates a string reader for the given string.\n-fn setup<'a>(sm: &SourceMap, sess: &'a ParseSess, teststr: String) -> StringReader<'a> {\n-    let sf = sm.new_source_file(PathBuf::from(teststr.clone()).into(), teststr);\n-    StringReader::new(sess, sf, None)\n-}\n-\n-#[test]\n-fn t1() {\n-    with_default_session_globals(|| {\n-        let sm = Lrc::new(SourceMap::new(FilePathMapping::empty()));\n-        let sh = mk_sess(sm.clone());\n-        let mut string_reader = setup(\n-            &sm,\n-            &sh,\n-            \"/* my source file */ fn main() { println!(\\\"zebra\\\"); }\\n\".to_string(),\n-        );\n-        assert_eq!(string_reader.next_token(), token::Comment);\n-        assert_eq!(string_reader.next_token(), token::Whitespace);\n-        let tok1 = string_reader.next_token();\n-        let tok2 = Token::new(mk_ident(\"fn\"), Span::with_root_ctxt(BytePos(21), BytePos(23)));\n-        assert_eq!(tok1.kind, tok2.kind);\n-        assert_eq!(tok1.span, tok2.span);\n-        assert_eq!(string_reader.next_token(), token::Whitespace);\n-        // Read another token.\n-        let tok3 = string_reader.next_token();\n-        assert_eq!(string_reader.pos(), BytePos(28));\n-        let tok4 = Token::new(mk_ident(\"main\"), Span::with_root_ctxt(BytePos(24), BytePos(28)));\n-        assert_eq!(tok3.kind, tok4.kind);\n-        assert_eq!(tok3.span, tok4.span);\n-\n-        assert_eq!(string_reader.next_token(), token::OpenDelim(token::Paren));\n-        assert_eq!(string_reader.pos(), BytePos(29))\n-    })\n-}\n-\n-// Checks that the given reader produces the desired stream\n-// of tokens (stop checking after exhausting `expected`).\n-fn check_tokenization(mut string_reader: StringReader<'_>, expected: Vec<TokenKind>) {\n-    for expected_tok in &expected {\n-        assert_eq!(&string_reader.next_token(), expected_tok);\n-    }\n-}\n-\n-// Makes the identifier by looking up the string in the interner.\n-fn mk_ident(id: &str) -> TokenKind {\n-    token::Ident(Symbol::intern(id), false)\n-}\n-\n-fn mk_lit(kind: token::LitKind, symbol: &str, suffix: Option<&str>) -> TokenKind {\n-    TokenKind::lit(kind, Symbol::intern(symbol), suffix.map(Symbol::intern))\n-}\n-\n-#[test]\n-fn doublecolon_parsing() {\n-    with_default_session_globals(|| {\n-        let sm = Lrc::new(SourceMap::new(FilePathMapping::empty()));\n-        let sh = mk_sess(sm.clone());\n-        check_tokenization(\n-            setup(&sm, &sh, \"a b\".to_string()),\n-            vec![mk_ident(\"a\"), token::Whitespace, mk_ident(\"b\")],\n-        );\n-    })\n-}\n-\n-#[test]\n-fn doublecolon_parsing_2() {\n-    with_default_session_globals(|| {\n-        let sm = Lrc::new(SourceMap::new(FilePathMapping::empty()));\n-        let sh = mk_sess(sm.clone());\n-        check_tokenization(\n-            setup(&sm, &sh, \"a::b\".to_string()),\n-            vec![mk_ident(\"a\"), token::Colon, token::Colon, mk_ident(\"b\")],\n-        );\n-    })\n-}\n-\n-#[test]\n-fn doublecolon_parsing_3() {\n-    with_default_session_globals(|| {\n-        let sm = Lrc::new(SourceMap::new(FilePathMapping::empty()));\n-        let sh = mk_sess(sm.clone());\n-        check_tokenization(\n-            setup(&sm, &sh, \"a ::b\".to_string()),\n-            vec![mk_ident(\"a\"), token::Whitespace, token::Colon, token::Colon, mk_ident(\"b\")],\n-        );\n-    })\n-}\n-\n-#[test]\n-fn doublecolon_parsing_4() {\n-    with_default_session_globals(|| {\n-        let sm = Lrc::new(SourceMap::new(FilePathMapping::empty()));\n-        let sh = mk_sess(sm.clone());\n-        check_tokenization(\n-            setup(&sm, &sh, \"a:: b\".to_string()),\n-            vec![mk_ident(\"a\"), token::Colon, token::Colon, token::Whitespace, mk_ident(\"b\")],\n-        );\n-    })\n-}\n-\n-#[test]\n-fn character_a() {\n-    with_default_session_globals(|| {\n-        let sm = Lrc::new(SourceMap::new(FilePathMapping::empty()));\n-        let sh = mk_sess(sm.clone());\n-        assert_eq!(setup(&sm, &sh, \"'a'\".to_string()).next_token(), mk_lit(token::Char, \"a\", None),);\n-    })\n-}\n-\n-#[test]\n-fn character_space() {\n-    with_default_session_globals(|| {\n-        let sm = Lrc::new(SourceMap::new(FilePathMapping::empty()));\n-        let sh = mk_sess(sm.clone());\n-        assert_eq!(setup(&sm, &sh, \"' '\".to_string()).next_token(), mk_lit(token::Char, \" \", None),);\n-    })\n-}\n-\n-#[test]\n-fn character_escaped() {\n-    with_default_session_globals(|| {\n-        let sm = Lrc::new(SourceMap::new(FilePathMapping::empty()));\n-        let sh = mk_sess(sm.clone());\n-        assert_eq!(\n-            setup(&sm, &sh, \"'\\\\n'\".to_string()).next_token(),\n-            mk_lit(token::Char, \"\\\\n\", None),\n-        );\n-    })\n-}\n-\n-#[test]\n-fn lifetime_name() {\n-    with_default_session_globals(|| {\n-        let sm = Lrc::new(SourceMap::new(FilePathMapping::empty()));\n-        let sh = mk_sess(sm.clone());\n-        assert_eq!(\n-            setup(&sm, &sh, \"'abc\".to_string()).next_token(),\n-            token::Lifetime(Symbol::intern(\"'abc\")),\n-        );\n-    })\n-}\n-\n-#[test]\n-fn raw_string() {\n-    with_default_session_globals(|| {\n-        let sm = Lrc::new(SourceMap::new(FilePathMapping::empty()));\n-        let sh = mk_sess(sm.clone());\n-        assert_eq!(\n-            setup(&sm, &sh, \"r###\\\"\\\"#a\\\\b\\x00c\\\"\\\"###\".to_string()).next_token(),\n-            mk_lit(token::StrRaw(3), \"\\\"#a\\\\b\\x00c\\\"\", None),\n-        );\n-    })\n-}\n-\n-#[test]\n-fn literal_suffixes() {\n-    with_default_session_globals(|| {\n-        let sm = Lrc::new(SourceMap::new(FilePathMapping::empty()));\n-        let sh = mk_sess(sm.clone());\n-        macro_rules! test {\n-            ($input: expr, $tok_type: ident, $tok_contents: expr) => {{\n-                assert_eq!(\n-                    setup(&sm, &sh, format!(\"{}suffix\", $input)).next_token(),\n-                    mk_lit(token::$tok_type, $tok_contents, Some(\"suffix\")),\n-                );\n-                // with a whitespace separator\n-                assert_eq!(\n-                    setup(&sm, &sh, format!(\"{} suffix\", $input)).next_token(),\n-                    mk_lit(token::$tok_type, $tok_contents, None),\n-                );\n-            }};\n-        }\n-\n-        test!(\"'a'\", Char, \"a\");\n-        test!(\"b'a'\", Byte, \"a\");\n-        test!(\"\\\"a\\\"\", Str, \"a\");\n-        test!(\"b\\\"a\\\"\", ByteStr, \"a\");\n-        test!(\"1234\", Integer, \"1234\");\n-        test!(\"0b101\", Integer, \"0b101\");\n-        test!(\"0xABC\", Integer, \"0xABC\");\n-        test!(\"1.0\", Float, \"1.0\");\n-        test!(\"1.0e10\", Float, \"1.0e10\");\n-\n-        assert_eq!(\n-            setup(&sm, &sh, \"2us\".to_string()).next_token(),\n-            mk_lit(token::Integer, \"2\", Some(\"us\")),\n-        );\n-        assert_eq!(\n-            setup(&sm, &sh, \"r###\\\"raw\\\"###suffix\".to_string()).next_token(),\n-            mk_lit(token::StrRaw(3), \"raw\", Some(\"suffix\")),\n-        );\n-        assert_eq!(\n-            setup(&sm, &sh, \"br###\\\"raw\\\"###suffix\".to_string()).next_token(),\n-            mk_lit(token::ByteStrRaw(3), \"raw\", Some(\"suffix\")),\n-        );\n-    })\n-}\n-\n-#[test]\n-fn nested_block_comments() {\n-    with_default_session_globals(|| {\n-        let sm = Lrc::new(SourceMap::new(FilePathMapping::empty()));\n-        let sh = mk_sess(sm.clone());\n-        let mut lexer = setup(&sm, &sh, \"/* /* */ */'a'\".to_string());\n-        assert_eq!(lexer.next_token(), token::Comment);\n-        assert_eq!(lexer.next_token(), mk_lit(token::Char, \"a\", None));\n-    })\n-}\n-\n-#[test]\n-fn crlf_comments() {\n-    with_default_session_globals(|| {\n-        let sm = Lrc::new(SourceMap::new(FilePathMapping::empty()));\n-        let sh = mk_sess(sm.clone());\n-        let mut lexer = setup(&sm, &sh, \"// test\\r\\n/// test\\r\\n\".to_string());\n-        let comment = lexer.next_token();\n-        assert_eq!(comment.kind, token::Comment);\n-        assert_eq!((comment.span.lo(), comment.span.hi()), (BytePos(0), BytePos(7)));\n-        assert_eq!(lexer.next_token(), token::Whitespace);\n-        assert_eq!(\n-            lexer.next_token(),\n-            token::DocComment(CommentKind::Line, AttrStyle::Outer, Symbol::intern(\" test\"))\n-        );\n-    })\n-}"}, {"sha": "94017b7b286e2bd79f364ad117bf54d2259a94ee", "filename": "compiler/rustc_lexer/src/tests.rs", "status": "modified", "additions": 140, "deletions": 20, "changes": 160, "blob_url": "https://github.com/rust-lang/rust/blob/ccffea5b6b3372cefd4e15bc738a2669bc6f69a0/compiler%2Frustc_lexer%2Fsrc%2Ftests.rs", "raw_url": "https://github.com/rust-lang/rust/raw/ccffea5b6b3372cefd4e15bc738a2669bc6f69a0/compiler%2Frustc_lexer%2Fsrc%2Ftests.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_lexer%2Fsrc%2Ftests.rs?ref=ccffea5b6b3372cefd4e15bc738a2669bc6f69a0", "patch": "@@ -128,6 +128,34 @@ fn check_lexing(src: &str, expect: Expect) {\n     expect.assert_eq(&actual)\n }\n \n+#[test]\n+fn smoke_test() {\n+    check_lexing(\n+        \"/* my source file */ fn main() { println!(\\\"zebra\\\"); }\\n\",\n+        expect![[r#\"\n+            Token { kind: BlockComment { doc_style: None, terminated: true }, len: 20 }\n+            Token { kind: Whitespace, len: 1 }\n+            Token { kind: Ident, len: 2 }\n+            Token { kind: Whitespace, len: 1 }\n+            Token { kind: Ident, len: 4 }\n+            Token { kind: OpenParen, len: 1 }\n+            Token { kind: CloseParen, len: 1 }\n+            Token { kind: Whitespace, len: 1 }\n+            Token { kind: OpenBrace, len: 1 }\n+            Token { kind: Whitespace, len: 1 }\n+            Token { kind: Ident, len: 7 }\n+            Token { kind: Bang, len: 1 }\n+            Token { kind: OpenParen, len: 1 }\n+            Token { kind: Literal { kind: Str { terminated: true }, suffix_start: 7 }, len: 7 }\n+            Token { kind: CloseParen, len: 1 }\n+            Token { kind: Semi, len: 1 }\n+            Token { kind: Whitespace, len: 1 }\n+            Token { kind: CloseBrace, len: 1 }\n+            Token { kind: Whitespace, len: 1 }\n+        \"#]],\n+    )\n+}\n+\n #[test]\n fn comment_flavors() {\n     check_lexing(\n@@ -143,25 +171,117 @@ fn comment_flavors() {\n /*! inner doc block */\n \",\n         expect![[r#\"\n-                Token { kind: Whitespace, len: 1 }\n-                Token { kind: LineComment { doc_style: None }, len: 7 }\n-                Token { kind: Whitespace, len: 1 }\n-                Token { kind: LineComment { doc_style: None }, len: 17 }\n-                Token { kind: Whitespace, len: 1 }\n-                Token { kind: LineComment { doc_style: Some(Outer) }, len: 18 }\n-                Token { kind: Whitespace, len: 1 }\n-                Token { kind: LineComment { doc_style: Some(Inner) }, len: 18 }\n-                Token { kind: Whitespace, len: 1 }\n-                Token { kind: BlockComment { doc_style: None, terminated: true }, len: 11 }\n-                Token { kind: Whitespace, len: 1 }\n-                Token { kind: BlockComment { doc_style: None, terminated: true }, len: 4 }\n-                Token { kind: Whitespace, len: 1 }\n-                Token { kind: BlockComment { doc_style: None, terminated: true }, len: 18 }\n-                Token { kind: Whitespace, len: 1 }\n-                Token { kind: BlockComment { doc_style: Some(Outer), terminated: true }, len: 22 }\n-                Token { kind: Whitespace, len: 1 }\n-                Token { kind: BlockComment { doc_style: Some(Inner), terminated: true }, len: 22 }\n-                Token { kind: Whitespace, len: 1 }\n-            \"#]],\n+            Token { kind: Whitespace, len: 1 }\n+            Token { kind: LineComment { doc_style: None }, len: 7 }\n+            Token { kind: Whitespace, len: 1 }\n+            Token { kind: LineComment { doc_style: None }, len: 17 }\n+            Token { kind: Whitespace, len: 1 }\n+            Token { kind: LineComment { doc_style: Some(Outer) }, len: 18 }\n+            Token { kind: Whitespace, len: 1 }\n+            Token { kind: LineComment { doc_style: Some(Inner) }, len: 18 }\n+            Token { kind: Whitespace, len: 1 }\n+            Token { kind: BlockComment { doc_style: None, terminated: true }, len: 11 }\n+            Token { kind: Whitespace, len: 1 }\n+            Token { kind: BlockComment { doc_style: None, terminated: true }, len: 4 }\n+            Token { kind: Whitespace, len: 1 }\n+            Token { kind: BlockComment { doc_style: None, terminated: true }, len: 18 }\n+            Token { kind: Whitespace, len: 1 }\n+            Token { kind: BlockComment { doc_style: Some(Outer), terminated: true }, len: 22 }\n+            Token { kind: Whitespace, len: 1 }\n+            Token { kind: BlockComment { doc_style: Some(Inner), terminated: true }, len: 22 }\n+            Token { kind: Whitespace, len: 1 }\n+        \"#]],\n+    )\n+}\n+\n+#[test]\n+fn nested_block_comments() {\n+    check_lexing(\n+        \"/* /* */ */'a'\",\n+        expect![[r#\"\n+            Token { kind: BlockComment { doc_style: None, terminated: true }, len: 11 }\n+            Token { kind: Literal { kind: Char { terminated: true }, suffix_start: 3 }, len: 3 }\n+        \"#]],\n+    )\n+}\n+\n+#[test]\n+fn characters() {\n+    check_lexing(\n+        \"'a' ' ' '\\\\n'\",\n+        expect![[r#\"\n+            Token { kind: Literal { kind: Char { terminated: true }, suffix_start: 3 }, len: 3 }\n+            Token { kind: Whitespace, len: 1 }\n+            Token { kind: Literal { kind: Char { terminated: true }, suffix_start: 3 }, len: 3 }\n+            Token { kind: Whitespace, len: 1 }\n+            Token { kind: Literal { kind: Char { terminated: true }, suffix_start: 4 }, len: 4 }\n+        \"#]],\n+    );\n+}\n+\n+#[test]\n+fn lifetime() {\n+    check_lexing(\n+        \"'abc\",\n+        expect![[r#\"\n+            Token { kind: Lifetime { starts_with_number: false }, len: 4 }\n+        \"#]],\n+    );\n+}\n+\n+#[test]\n+fn raw_string() {\n+    check_lexing(\n+        \"r###\\\"\\\"#a\\\\b\\x00c\\\"\\\"###\",\n+        expect![[r#\"\n+            Token { kind: Literal { kind: RawStr { n_hashes: 3, err: None }, suffix_start: 17 }, len: 17 }\n+        \"#]],\n+    )\n+}\n+\n+#[test]\n+fn literal_suffixes() {\n+    check_lexing(\n+        r####\"\n+'a'\n+b'a'\n+\"a\"\n+b\"a\"\n+1234\n+0b101\n+0xABC\n+1.0\n+1.0e10\n+2us\n+r###\"raw\"###suffix\n+br###\"raw\"###suffix\n+\"####,\n+        expect![[r#\"\n+            Token { kind: Whitespace, len: 1 }\n+            Token { kind: Literal { kind: Char { terminated: true }, suffix_start: 3 }, len: 3 }\n+            Token { kind: Whitespace, len: 1 }\n+            Token { kind: Literal { kind: Byte { terminated: true }, suffix_start: 4 }, len: 4 }\n+            Token { kind: Whitespace, len: 1 }\n+            Token { kind: Literal { kind: Str { terminated: true }, suffix_start: 3 }, len: 3 }\n+            Token { kind: Whitespace, len: 1 }\n+            Token { kind: Literal { kind: ByteStr { terminated: true }, suffix_start: 4 }, len: 4 }\n+            Token { kind: Whitespace, len: 1 }\n+            Token { kind: Literal { kind: Int { base: Decimal, empty_int: false }, suffix_start: 4 }, len: 4 }\n+            Token { kind: Whitespace, len: 1 }\n+            Token { kind: Literal { kind: Int { base: Binary, empty_int: false }, suffix_start: 5 }, len: 5 }\n+            Token { kind: Whitespace, len: 1 }\n+            Token { kind: Literal { kind: Int { base: Hexadecimal, empty_int: false }, suffix_start: 5 }, len: 5 }\n+            Token { kind: Whitespace, len: 1 }\n+            Token { kind: Literal { kind: Float { base: Decimal, empty_exponent: false }, suffix_start: 3 }, len: 3 }\n+            Token { kind: Whitespace, len: 1 }\n+            Token { kind: Literal { kind: Float { base: Decimal, empty_exponent: false }, suffix_start: 6 }, len: 6 }\n+            Token { kind: Whitespace, len: 1 }\n+            Token { kind: Literal { kind: Int { base: Decimal, empty_int: false }, suffix_start: 1 }, len: 3 }\n+            Token { kind: Whitespace, len: 1 }\n+            Token { kind: Literal { kind: RawStr { n_hashes: 3, err: None }, suffix_start: 12 }, len: 18 }\n+            Token { kind: Whitespace, len: 1 }\n+            Token { kind: Literal { kind: RawByteStr { n_hashes: 3, err: None }, suffix_start: 13 }, len: 19 }\n+            Token { kind: Whitespace, len: 1 }\n+        \"#]],\n     )\n }"}]}