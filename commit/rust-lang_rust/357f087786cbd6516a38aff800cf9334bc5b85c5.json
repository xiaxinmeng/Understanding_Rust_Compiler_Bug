{"sha": "357f087786cbd6516a38aff800cf9334bc5b85c5", "node_id": "MDY6Q29tbWl0NzI0NzEyOjM1N2YwODc3ODZjYmQ2NTE2YTM4YWZmODAwY2Y5MzM0YmM1Yjg1YzU=", "commit": {"author": {"name": "Brian Anderson", "email": "banderson@mozilla.com", "date": "2013-06-20T19:17:00Z"}, "committer": {"name": "Brian Anderson", "email": "banderson@mozilla.com", "date": "2013-06-20T19:17:00Z"}, "message": "Merge remote-tracking branch 'brson/io' into io-upstream\n\nConflicts:\n\tsrc/rt/rust_builtin.cpp\n\tsrc/rt/rustrt.def.in", "tree": {"sha": "e3582c4e0f16927c4dc1c377cf789b469cde8196", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/e3582c4e0f16927c4dc1c377cf789b469cde8196"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/357f087786cbd6516a38aff800cf9334bc5b85c5", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/357f087786cbd6516a38aff800cf9334bc5b85c5", "html_url": "https://github.com/rust-lang/rust/commit/357f087786cbd6516a38aff800cf9334bc5b85c5", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/357f087786cbd6516a38aff800cf9334bc5b85c5/comments", "author": {"login": "brson", "id": 147214, "node_id": "MDQ6VXNlcjE0NzIxNA==", "avatar_url": "https://avatars.githubusercontent.com/u/147214?v=4", "gravatar_id": "", "url": "https://api.github.com/users/brson", "html_url": "https://github.com/brson", "followers_url": "https://api.github.com/users/brson/followers", "following_url": "https://api.github.com/users/brson/following{/other_user}", "gists_url": "https://api.github.com/users/brson/gists{/gist_id}", "starred_url": "https://api.github.com/users/brson/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/brson/subscriptions", "organizations_url": "https://api.github.com/users/brson/orgs", "repos_url": "https://api.github.com/users/brson/repos", "events_url": "https://api.github.com/users/brson/events{/privacy}", "received_events_url": "https://api.github.com/users/brson/received_events", "type": "User", "site_admin": false}, "committer": {"login": "brson", "id": 147214, "node_id": "MDQ6VXNlcjE0NzIxNA==", "avatar_url": "https://avatars.githubusercontent.com/u/147214?v=4", "gravatar_id": "", "url": "https://api.github.com/users/brson", "html_url": "https://github.com/brson", "followers_url": "https://api.github.com/users/brson/followers", "following_url": "https://api.github.com/users/brson/following{/other_user}", "gists_url": "https://api.github.com/users/brson/gists{/gist_id}", "starred_url": "https://api.github.com/users/brson/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/brson/subscriptions", "organizations_url": "https://api.github.com/users/brson/orgs", "repos_url": "https://api.github.com/users/brson/repos", "events_url": "https://api.github.com/users/brson/events{/privacy}", "received_events_url": "https://api.github.com/users/brson/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "adeb7e77ccff938c0afb105a14a2ff4df4c7efc8", "url": "https://api.github.com/repos/rust-lang/rust/commits/adeb7e77ccff938c0afb105a14a2ff4df4c7efc8", "html_url": "https://github.com/rust-lang/rust/commit/adeb7e77ccff938c0afb105a14a2ff4df4c7efc8"}, {"sha": "4d39253a9623ff30c27cee3c9770634a41f4412d", "url": "https://api.github.com/repos/rust-lang/rust/commits/4d39253a9623ff30c27cee3c9770634a41f4412d", "html_url": "https://github.com/rust-lang/rust/commit/4d39253a9623ff30c27cee3c9770634a41f4412d"}], "stats": {"total": 3983, "additions": 3147, "deletions": 836}, "files": [{"sha": "7748c43efcd28b7eddf8fd356061d9e4f2c0a715", "filename": "src/libstd/macros.rs", "status": "modified", "additions": 12, "deletions": 20, "changes": 32, "blob_url": "https://github.com/rust-lang/rust/blob/357f087786cbd6516a38aff800cf9334bc5b85c5/src%2Flibstd%2Fmacros.rs", "raw_url": "https://github.com/rust-lang/rust/raw/357f087786cbd6516a38aff800cf9334bc5b85c5/src%2Flibstd%2Fmacros.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibstd%2Fmacros.rs?ref=357f087786cbd6516a38aff800cf9334bc5b85c5", "patch": "@@ -10,18 +10,16 @@\n \n #[macro_escape];\n \n+macro_rules! rterrln (\n+    ($( $arg:expr),+) => ( {\n+        ::rt::util::dumb_println(fmt!( $($arg),+ ));\n+    } )\n+)\n+\n // Some basic logging\n macro_rules! rtdebug_ (\n     ($( $arg:expr),+) => ( {\n-        dumb_println(fmt!( $($arg),+ ));\n-\n-        fn dumb_println(s: &str) {\n-            use io::WriterUtil;\n-            let dbg = ::libc::STDERR_FILENO as ::io::fd_t;\n-            dbg.write_str(s);\n-            dbg.write_str(\"\\n\");\n-        }\n-\n+        rterrln!( $($arg),+ )\n     } )\n )\n \n@@ -33,21 +31,15 @@ macro_rules! rtdebug (\n macro_rules! rtassert (\n     ( $arg:expr ) => ( {\n         if !$arg {\n-            abort!(\"assertion failed: %s\", stringify!($arg));\n+            rtabort!(\"assertion failed: %s\", stringify!($arg));\n         }\n     } )\n )\n \n-macro_rules! abort(\n-    ($( $msg:expr),+) => ( {\n-        rtdebug!($($msg),+);\n-\n-        do_abort();\n \n-        // NB: This is in a fn to avoid putting the `unsafe` block in a macro,\n-        // which causes spurious 'unnecessary unsafe block' warnings.\n-        fn do_abort() -> ! {\n-            unsafe { ::libc::abort(); }\n-        }\n+macro_rules! rtabort(\n+    ($( $msg:expr),+) => ( {\n+        ::rt::util::abort(fmt!($($msg),+));\n     } )\n )\n+"}, {"sha": "82e6d44fe6253b6645eb43061535aa1048278686", "filename": "src/libstd/rt/comm.rs", "status": "modified", "additions": 322, "deletions": 19, "changes": 341, "blob_url": "https://github.com/rust-lang/rust/blob/357f087786cbd6516a38aff800cf9334bc5b85c5/src%2Flibstd%2Frt%2Fcomm.rs", "raw_url": "https://github.com/rust-lang/rust/raw/357f087786cbd6516a38aff800cf9334bc5b85c5/src%2Flibstd%2Frt%2Fcomm.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibstd%2Frt%2Fcomm.rs?ref=357f087786cbd6516a38aff800cf9334bc5b85c5", "patch": "@@ -22,10 +22,12 @@ use ops::Drop;\n use kinds::Owned;\n use rt::sched::{Scheduler, Coroutine};\n use rt::local::Local;\n-use unstable::intrinsics::{atomic_xchg, atomic_load};\n+use unstable::atomics::{AtomicUint, AtomicOption, SeqCst};\n+use unstable::sync::UnsafeAtomicRcBox;\n use util::Void;\n use comm::{GenericChan, GenericSmartChan, GenericPort, Peekable};\n use cell::Cell;\n+use clone::Clone;\n \n /// A combined refcount / ~Task pointer.\n ///\n@@ -34,14 +36,14 @@ use cell::Cell;\n /// * 2 - both endpoints are alive\n /// * 1 - either the sender or the receiver is dead, determined by context\n /// * <ptr> - A pointer to a blocked Task that can be transmuted to ~Task\n-type State = int;\n+type State = uint;\n \n static STATE_BOTH: State = 2;\n static STATE_ONE: State = 1;\n \n /// The heap-allocated structure shared between two endpoints.\n struct Packet<T> {\n-    state: State,\n+    state: AtomicUint,\n     payload: Option<T>,\n }\n \n@@ -70,7 +72,7 @@ pub struct PortOneHack<T> {\n \n pub fn oneshot<T: Owned>() -> (PortOne<T>, ChanOne<T>) {\n     let packet: ~Packet<T> = ~Packet {\n-        state: STATE_BOTH,\n+        state: AtomicUint::new(STATE_BOTH),\n         payload: None\n     };\n \n@@ -114,20 +116,30 @@ impl<T> ChanOne<T> {\n             // reordering of the payload write. This also issues an\n             // acquire barrier that keeps the subsequent access of the\n             // ~Task pointer from being reordered.\n-            let oldstate = atomic_xchg(&mut (*packet).state, STATE_ONE);\n+            let oldstate = (*packet).state.swap(STATE_ONE, SeqCst);\n             match oldstate {\n                 STATE_BOTH => {\n                     // Port is not waiting yet. Nothing to do\n+                    do Local::borrow::<Scheduler, ()> |sched| {\n+                        rtdebug!(\"non-rendezvous send\");\n+                        sched.metrics.non_rendezvous_sends += 1;\n+                    }\n                 }\n                 STATE_ONE => {\n+                    do Local::borrow::<Scheduler, ()> |sched| {\n+                        rtdebug!(\"rendezvous send\");\n+                        sched.metrics.rendezvous_sends += 1;\n+                    }\n                     // Port has closed. Need to clean up.\n                     let _packet: ~Packet<T> = cast::transmute(this.inner.void_packet);\n                     recvr_active = false;\n                 }\n                 task_as_state => {\n                     // Port is blocked. Wake it up.\n                     let recvr: ~Coroutine = cast::transmute(task_as_state);\n-                    let sched = Local::take::<Scheduler>();\n+                    let mut sched = Local::take::<Scheduler>();\n+                    rtdebug!(\"rendezvous send\");\n+                    sched.metrics.rendezvous_sends += 1;\n                     sched.schedule_task(recvr);\n                 }\n             }\n@@ -158,23 +170,30 @@ impl<T> PortOne<T> {\n \n         // Switch to the scheduler to put the ~Task into the Packet state.\n         let sched = Local::take::<Scheduler>();\n-        do sched.deschedule_running_task_and_then |task| {\n+        do sched.deschedule_running_task_and_then |sched, task| {\n             unsafe {\n                 // Atomically swap the task pointer into the Packet state, issuing\n                 // an acquire barrier to prevent reordering of the subsequent read\n                 // of the payload. Also issues a release barrier to prevent reordering\n                 // of any previous writes to the task structure.\n                 let task_as_state: State = cast::transmute(task);\n-                let oldstate = atomic_xchg(&mut (*packet).state, task_as_state);\n+                let oldstate = (*packet).state.swap(task_as_state, SeqCst);\n                 match oldstate {\n                     STATE_BOTH => {\n                         // Data has not been sent. Now we're blocked.\n+                        rtdebug!(\"non-rendezvous recv\");\n+                        sched.metrics.non_rendezvous_recvs += 1;\n                     }\n                     STATE_ONE => {\n+                        rtdebug!(\"rendezvous recv\");\n+                        sched.metrics.rendezvous_recvs += 1;\n+\n                         // Channel is closed. Switch back and check the data.\n+                        // NB: We have to drop back into the scheduler event loop here\n+                        // instead of switching immediately back or we could end up\n+                        // triggering infinite recursion on the scheduler's stack.\n                         let task: ~Coroutine = cast::transmute(task_as_state);\n-                        let sched = Local::take::<Scheduler>();\n-                        sched.resume_task_immediately(task);\n+                        sched.enqueue_task(task);\n                     }\n                     _ => util::unreachable()\n                 }\n@@ -210,7 +229,7 @@ impl<T> Peekable<T> for PortOne<T> {\n     fn peek(&self) -> bool {\n         unsafe {\n             let packet: *mut Packet<T> = self.inner.packet();\n-            let oldstate = atomic_load(&mut (*packet).state);\n+            let oldstate = (*packet).state.load(SeqCst);\n             match oldstate {\n                 STATE_BOTH => false,\n                 STATE_ONE => (*packet).payload.is_some(),\n@@ -227,7 +246,7 @@ impl<T> Drop for ChanOneHack<T> {\n \n         unsafe {\n             let this = cast::transmute_mut(self);\n-            let oldstate = atomic_xchg(&mut (*this.packet()).state, STATE_ONE);\n+            let oldstate = (*this.packet()).state.swap(STATE_ONE, SeqCst);\n             match oldstate {\n                 STATE_BOTH => {\n                     // Port still active. It will destroy the Packet.\n@@ -254,7 +273,7 @@ impl<T> Drop for PortOneHack<T> {\n \n         unsafe {\n             let this = cast::transmute_mut(self);\n-            let oldstate = atomic_xchg(&mut (*this.packet()).state, STATE_ONE);\n+            let oldstate = (*this.packet()).state.swap(STATE_ONE, SeqCst);\n             match oldstate {\n                 STATE_BOTH => {\n                     // Chan still active. It will destroy the packet.\n@@ -295,16 +314,19 @@ struct StreamPayload<T> {\n     next: PortOne<StreamPayload<T>>\n }\n \n+type StreamChanOne<T> = ChanOne<StreamPayload<T>>;\n+type StreamPortOne<T> = PortOne<StreamPayload<T>>;\n+\n /// A channel with unbounded size.\n pub struct Chan<T> {\n     // FIXME #5372. Using Cell because we don't take &mut self\n-    next: Cell<ChanOne<StreamPayload<T>>>\n+    next: Cell<StreamChanOne<T>>\n }\n \n /// An port with unbounded size.\n pub struct Port<T> {\n     // FIXME #5372. Using Cell because we don't take &mut self\n-    next: Cell<PortOne<StreamPayload<T>>>\n+    next: Cell<StreamPortOne<T>>\n }\n \n pub fn stream<T: Owned>() -> (Port<T>, Chan<T>) {\n@@ -357,6 +379,148 @@ impl<T> Peekable<T> for Port<T> {\n     }\n }\n \n+pub struct SharedChan<T> {\n+    // Just like Chan, but a shared AtomicOption instead of Cell\n+    priv next: UnsafeAtomicRcBox<AtomicOption<StreamChanOne<T>>>\n+}\n+\n+impl<T> SharedChan<T> {\n+    pub fn new(chan: Chan<T>) -> SharedChan<T> {\n+        let next = chan.next.take();\n+        let next = AtomicOption::new(~next);\n+        SharedChan { next: UnsafeAtomicRcBox::new(next) }\n+    }\n+}\n+\n+impl<T: Owned> GenericChan<T> for SharedChan<T> {\n+    fn send(&self, val: T) {\n+        self.try_send(val);\n+    }\n+}\n+\n+impl<T: Owned> GenericSmartChan<T> for SharedChan<T> {\n+    #[cfg(stage0)] // odd type checking errors\n+    fn try_send(&self, _val: T) -> bool {\n+        fail!()\n+    }\n+\n+    #[cfg(not(stage0))]\n+    fn try_send(&self, val: T) -> bool {\n+        unsafe {\n+            let (next_pone, next_cone) = oneshot();\n+            let cone = (*self.next.get()).swap(~next_cone, SeqCst);\n+            cone.unwrap().try_send(StreamPayload { val: val, next: next_pone })\n+        }\n+    }\n+}\n+\n+impl<T> Clone for SharedChan<T> {\n+    fn clone(&self) -> SharedChan<T> {\n+        SharedChan {\n+            next: self.next.clone()\n+        }\n+    }\n+}\n+\n+pub struct SharedPort<T> {\n+    // The next port on which we will receive the next port on which we will receive T\n+    priv next_link: UnsafeAtomicRcBox<AtomicOption<PortOne<StreamPortOne<T>>>>\n+}\n+\n+impl<T> SharedPort<T> {\n+    pub fn new(port: Port<T>) -> SharedPort<T> {\n+        // Put the data port into a new link pipe\n+        let next_data_port = port.next.take();\n+        let (next_link_port, next_link_chan) = oneshot();\n+        next_link_chan.send(next_data_port);\n+        let next_link = AtomicOption::new(~next_link_port);\n+        SharedPort { next_link: UnsafeAtomicRcBox::new(next_link) }\n+    }\n+}\n+\n+impl<T: Owned> GenericPort<T> for SharedPort<T> {\n+    fn recv(&self) -> T {\n+        match self.try_recv() {\n+            Some(val) => val,\n+            None => {\n+                fail!(\"receiving on a closed channel\");\n+            }\n+        }\n+    }\n+\n+    #[cfg(stage0)] // odd type checking errors\n+    fn try_recv(&self) -> Option<T> {\n+        fail!()\n+    }\n+\n+    #[cfg(not(stage0))]\n+    fn try_recv(&self) -> Option<T> {\n+        unsafe {\n+            let (next_link_port, next_link_chan) = oneshot();\n+            let link_port = (*self.next_link.get()).swap(~next_link_port, SeqCst);\n+            let link_port = link_port.unwrap();\n+            let data_port = link_port.recv();\n+            let (next_data_port, res) = match data_port.try_recv() {\n+                Some(StreamPayload { val, next }) => {\n+                    (next, Some(val))\n+                }\n+                None => {\n+                    let (next_data_port, _) = oneshot();\n+                    (next_data_port, None)\n+                }\n+            };\n+            next_link_chan.send(next_data_port);\n+            return res;\n+        }\n+    }\n+}\n+\n+impl<T> Clone for SharedPort<T> {\n+    fn clone(&self) -> SharedPort<T> {\n+        SharedPort {\n+            next_link: self.next_link.clone()\n+        }\n+    }\n+}\n+\n+// XXX: Need better name\n+type MegaPipe<T> = (SharedPort<T>, SharedChan<T>);\n+\n+pub fn megapipe<T: Owned>() -> MegaPipe<T> {\n+    let (port, chan) = stream();\n+    (SharedPort::new(port), SharedChan::new(chan))\n+}\n+\n+impl<T: Owned> GenericChan<T> for MegaPipe<T> {\n+    fn send(&self, val: T) {\n+        match *self {\n+            (_, ref c) => c.send(val)\n+        }\n+    }\n+}\n+\n+impl<T: Owned> GenericSmartChan<T> for MegaPipe<T> {\n+    fn try_send(&self, val: T) -> bool {\n+        match *self {\n+            (_, ref c) => c.try_send(val)\n+        }\n+    }\n+}\n+\n+impl<T: Owned> GenericPort<T> for MegaPipe<T> {\n+    fn recv(&self) -> T {\n+        match *self {\n+            (ref p, _) => p.recv()\n+        }\n+    }\n+\n+    fn try_recv(&self) -> Option<T> {\n+        match *self {\n+            (ref p, _) => p.try_recv()\n+        }\n+    }\n+}\n+\n #[cfg(test)]\n mod test {\n     use super::*;\n@@ -584,7 +748,7 @@ mod test {\n     #[test]\n     fn stream_send_recv_stress() {\n         for stress_factor().times {\n-            do run_in_newsched_task {\n+            do run_in_mt_newsched_task {\n                 let (port, chan) = stream::<~int>();\n \n                 send(chan, 0);\n@@ -594,18 +758,18 @@ mod test {\n                     if i == 10 { return }\n \n                     let chan_cell = Cell::new(chan);\n-                    let _thread = do spawntask_thread {\n+                    do spawntask_random {\n                         let chan = chan_cell.take();\n                         chan.send(~i);\n                         send(chan, i + 1);\n-                    };\n+                    }\n                 }\n \n                 fn recv(port: Port<~int>, i: int) {\n                     if i == 10 { return }\n \n                     let port_cell = Cell::new(port);\n-                    let _thread = do spawntask_thread {\n+                    do spawntask_random {\n                         let port = port_cell.take();\n                         assert!(port.recv() == ~i);\n                         recv(port, i + 1);\n@@ -614,4 +778,143 @@ mod test {\n             }\n         }\n     }\n+\n+    #[test]\n+    fn recv_a_lot() {\n+        // Regression test that we don't run out of stack in scheduler context\n+        do run_in_newsched_task {\n+            let (port, chan) = stream();\n+            for 10000.times { chan.send(()) }\n+            for 10000.times { port.recv() }\n+        }\n+    }\n+\n+    #[test]\n+    fn shared_chan_stress() {\n+        do run_in_mt_newsched_task {\n+            let (port, chan) = stream();\n+            let chan = SharedChan::new(chan);\n+            let total = stress_factor() + 100;\n+            for total.times {\n+                let chan_clone = chan.clone();\n+                do spawntask_random {\n+                    chan_clone.send(());\n+                }\n+            }\n+\n+            for total.times {\n+                port.recv();\n+            }\n+        }\n+    }\n+\n+    #[test]\n+    fn shared_port_stress() {\n+        do run_in_mt_newsched_task {\n+            // XXX: Removing these type annotations causes an ICE\n+            let (end_port, end_chan) = stream::<()>();\n+            let (port, chan) = stream::<()>();\n+            let end_chan = SharedChan::new(end_chan);\n+            let port = SharedPort::new(port);\n+            let total = stress_factor() + 100;\n+            for total.times {\n+                let end_chan_clone = end_chan.clone();\n+                let port_clone = port.clone();\n+                do spawntask_random {\n+                    port_clone.recv();\n+                    end_chan_clone.send(());\n+                }\n+            }\n+\n+            for total.times {\n+                chan.send(());\n+            }\n+\n+            for total.times {\n+                end_port.recv();\n+            }\n+        }\n+    }\n+\n+    #[test]\n+    fn shared_port_close_simple() {\n+        do run_in_mt_newsched_task {\n+            let (port, chan) = stream::<()>();\n+            let port = SharedPort::new(port);\n+            { let _chan = chan; }\n+            assert!(port.try_recv().is_none());\n+        }\n+    }\n+\n+    #[test]\n+    fn shared_port_close() {\n+        do run_in_mt_newsched_task {\n+            let (end_port, end_chan) = stream::<bool>();\n+            let (port, chan) = stream::<()>();\n+            let end_chan = SharedChan::new(end_chan);\n+            let port = SharedPort::new(port);\n+            let chan = SharedChan::new(chan);\n+            let send_total = 10;\n+            let recv_total = 20;\n+            do spawntask_random {\n+                for send_total.times {\n+                    let chan_clone = chan.clone();\n+                    do spawntask_random {\n+                        chan_clone.send(());\n+                    }\n+                }\n+            }\n+            let end_chan_clone = end_chan.clone();\n+            do spawntask_random {\n+                for recv_total.times {\n+                    let port_clone = port.clone();\n+                    let end_chan_clone = end_chan_clone.clone();\n+                    do spawntask_random {\n+                        let recvd = port_clone.try_recv().is_some();\n+                        end_chan_clone.send(recvd);\n+                    }\n+                }\n+            }\n+\n+            let mut recvd = 0;\n+            for recv_total.times {\n+                recvd += if end_port.recv() { 1 } else { 0 };\n+            }\n+\n+            assert!(recvd == send_total);\n+        }\n+    }\n+\n+    #[test]\n+    fn megapipe_stress() {\n+        use rand;\n+        use rand::RngUtil;\n+\n+        do run_in_mt_newsched_task {\n+            let (end_port, end_chan) = stream::<()>();\n+            let end_chan = SharedChan::new(end_chan);\n+            let pipe = megapipe();\n+            let total = stress_factor() + 10;\n+            let mut rng = rand::rng();\n+            for total.times {\n+                let msgs = rng.gen_uint_range(0, 10);\n+                let pipe_clone = pipe.clone();\n+                let end_chan_clone = end_chan.clone();\n+                do spawntask_random {\n+                    for msgs.times {\n+                        pipe_clone.send(());\n+                    }\n+                    for msgs.times {\n+                        pipe_clone.recv();\n+                    }\n+                }\n+\n+                end_chan_clone.send(());\n+            }\n+\n+            for total.times {\n+                end_port.recv();\n+            }\n+        }\n+    }\n }"}, {"sha": "e89df2b1c93f5bcb7aec84c98c0cdae9a7e6466b", "filename": "src/libstd/rt/global_heap.rs", "status": "modified", "additions": 30, "deletions": 6, "changes": 36, "blob_url": "https://github.com/rust-lang/rust/blob/357f087786cbd6516a38aff800cf9334bc5b85c5/src%2Flibstd%2Frt%2Fglobal_heap.rs", "raw_url": "https://github.com/rust-lang/rust/raw/357f087786cbd6516a38aff800cf9334bc5b85c5/src%2Flibstd%2Frt%2Fglobal_heap.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibstd%2Frt%2Fglobal_heap.rs?ref=357f087786cbd6516a38aff800cf9334bc5b85c5", "patch": "@@ -14,7 +14,7 @@ use c_malloc = libc::malloc;\n use c_free = libc::free;\n use managed::raw::{BoxHeaderRepr, BoxRepr};\n use cast::transmute;\n-use unstable::intrinsics::{atomic_xadd,atomic_xsub};\n+use unstable::intrinsics::{atomic_xadd,atomic_xsub, atomic_load};\n use ptr::null;\n use intrinsic::TyDesc;\n \n@@ -34,8 +34,7 @@ pub unsafe fn malloc(td: *TypeDesc, size: uint) -> *c_void {\n     box.header.prev = null();\n     box.header.next = null();\n \n-    let exchange_count = &mut *exchange_count_ptr();\n-    atomic_xadd(exchange_count, 1);\n+    inc_count();\n \n     return transmute(box);\n }\n@@ -48,21 +47,46 @@ pub unsafe fn malloc_raw(size: uint) -> *c_void {\n     if p.is_null() {\n         fail!(\"Failure in malloc_raw: result ptr is null\");\n     }\n+    inc_count();\n     p\n }\n \n pub unsafe fn free(ptr: *c_void) {\n-    let exchange_count = &mut *exchange_count_ptr();\n-    atomic_xsub(exchange_count, 1);\n-\n     assert!(ptr.is_not_null());\n+    dec_count();\n     c_free(ptr);\n }\n ///Thin wrapper around libc::free, as with exchange_alloc::malloc_raw\n pub unsafe fn free_raw(ptr: *c_void) {\n+    assert!(ptr.is_not_null());\n+    dec_count();\n     c_free(ptr);\n }\n \n+fn inc_count() {\n+    unsafe {\n+        let exchange_count = &mut *exchange_count_ptr();\n+        atomic_xadd(exchange_count, 1);\n+    }\n+}\n+\n+fn dec_count() {\n+    unsafe {\n+        let exchange_count = &mut *exchange_count_ptr();\n+        atomic_xsub(exchange_count, 1);\n+    }\n+}\n+\n+pub fn cleanup() {\n+    unsafe {\n+        let count_ptr = exchange_count_ptr();\n+        let allocations = atomic_load(&*count_ptr);\n+        if allocations != 0 {\n+            rtabort!(\"exchange heap not empty on exit - %i dangling allocations\", allocations);\n+        }\n+    }\n+}\n+\n fn get_box_size(body_size: uint, body_align: uint) -> uint {\n     let header_size = size_of::<BoxHeaderRepr>();\n     // FIXME (#2699): This alignment calculation is suspicious. Is it right?"}, {"sha": "ad5cf2eb378c53d5bf31ee04940aa0898decd29b", "filename": "src/libstd/rt/join_latch.rs", "status": "added", "additions": 645, "deletions": 0, "changes": 645, "blob_url": "https://github.com/rust-lang/rust/blob/357f087786cbd6516a38aff800cf9334bc5b85c5/src%2Flibstd%2Frt%2Fjoin_latch.rs", "raw_url": "https://github.com/rust-lang/rust/raw/357f087786cbd6516a38aff800cf9334bc5b85c5/src%2Flibstd%2Frt%2Fjoin_latch.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibstd%2Frt%2Fjoin_latch.rs?ref=357f087786cbd6516a38aff800cf9334bc5b85c5", "patch": "@@ -0,0 +1,645 @@\n+// Copyright 2013 The Rust Project Developers. See the COPYRIGHT\n+// file at the top-level directory of this distribution and at\n+// http://rust-lang.org/COPYRIGHT.\n+//\n+// Licensed under the Apache License, Version 2.0 <LICENSE-APACHE or\n+// http://www.apache.org/licenses/LICENSE-2.0> or the MIT license\n+// <LICENSE-MIT or http://opensource.org/licenses/MIT>, at your\n+// option. This file may not be copied, modified, or distributed\n+// except according to those terms.\n+\n+//! The JoinLatch is a concurrent type that establishes the task\n+//! tree and propagates failure.\n+//!\n+//! Each task gets a JoinLatch that is derived from the JoinLatch\n+//! of its parent task. Every latch must be released by either calling\n+//! the non-blocking `release` method or the task-blocking `wait` method.\n+//! Releasing a latch does not complete until all of its child latches\n+//! complete.\n+//!\n+//! Latches carry a `success` flag that is set to `false` during task\n+//! failure and is propagated both from children to parents and parents\n+//! to children. The status af this flag may be queried for the purposes\n+//! of linked failure.\n+//!\n+//! In addition to failure propagation the task tree serves to keep the\n+//! default task schedulers alive. The runtime only sends the shutdown\n+//! message to schedulers once the root task exits.\n+//!\n+//! Under this scheme tasks that terminate before their children become\n+//! 'zombies' since they may not exit until their children do. Zombie\n+//! tasks are 'tombstoned' as `Tombstone(~JoinLatch)` and the tasks\n+//! themselves allowed to terminate.\n+//!\n+//! XXX: Propagate flag from parents to children.\n+//! XXX: Tombstoning actually doesn't work.\n+//! XXX: This could probably be done in a way that doesn't leak tombstones\n+//!      longer than the life of the child tasks.\n+\n+use comm::{GenericPort, Peekable, GenericSmartChan};\n+use clone::Clone;\n+use container::Container;\n+use option::{Option, Some, None};\n+use ops::Drop;\n+use rt::comm::{SharedChan, Port, stream};\n+use rt::local::Local;\n+use rt::sched::Scheduler;\n+use unstable::atomics::{AtomicUint, SeqCst};\n+use util;\n+use vec::OwnedVector;\n+\n+// FIXME #7026: Would prefer this to be an enum\n+pub struct JoinLatch {\n+    priv parent: Option<ParentLink>,\n+    priv child: Option<ChildLink>,\n+    closed: bool,\n+}\n+\n+// Shared between parents and all their children.\n+struct SharedState {\n+    /// Reference count, held by a parent and all children.\n+    count: AtomicUint,\n+    success: bool\n+}\n+\n+struct ParentLink {\n+    shared: *mut SharedState,\n+    // For communicating with the parent.\n+    chan: SharedChan<Message>\n+}\n+\n+struct ChildLink {\n+    shared: ~SharedState,\n+    // For receiving from children.\n+    port: Port<Message>,\n+    chan: SharedChan<Message>,\n+    // Prevents dropping the child SharedState reference counts multiple times.\n+    dropped_child: bool\n+}\n+\n+// Messages from child latches to parent.\n+enum Message {\n+    Tombstone(~JoinLatch),\n+    ChildrenTerminated\n+}\n+\n+impl JoinLatch {\n+    pub fn new_root() -> ~JoinLatch {\n+        let this = ~JoinLatch {\n+            parent: None,\n+            child: None,\n+            closed: false\n+        };\n+        rtdebug!(\"new root latch %x\", this.id());\n+        return this;\n+    }\n+\n+    fn id(&self) -> uint {\n+        unsafe { ::cast::transmute(&*self) }\n+    }\n+\n+    pub fn new_child(&mut self) -> ~JoinLatch {\n+        rtassert!(!self.closed);\n+\n+        if self.child.is_none() {\n+            // This is the first time spawning a child\n+            let shared = ~SharedState {\n+                count: AtomicUint::new(1),\n+                success: true\n+            };\n+            let (port, chan) = stream();\n+            let chan = SharedChan::new(chan);\n+            let child = ChildLink {\n+                shared: shared,\n+                port: port,\n+                chan: chan,\n+                dropped_child: false\n+            };\n+            self.child = Some(child);\n+        }\n+\n+        let child_link: &mut ChildLink = self.child.get_mut_ref();\n+        let shared_state: *mut SharedState = &mut *child_link.shared;\n+\n+        child_link.shared.count.fetch_add(1, SeqCst);\n+\n+        let child = ~JoinLatch {\n+            parent: Some(ParentLink {\n+                shared: shared_state,\n+                chan: child_link.chan.clone()\n+            }),\n+            child: None,\n+            closed: false\n+        };\n+        rtdebug!(\"NEW child latch %x\", child.id());\n+        return child;\n+    }\n+\n+    pub fn release(~self, local_success: bool) {\n+        // XXX: This should not block, but there's a bug in the below\n+        // code that I can't figure out.\n+        self.wait(local_success);\n+    }\n+\n+    // XXX: Should not require ~self\n+    fn release_broken(~self, local_success: bool) {\n+        rtassert!(!self.closed);\n+\n+        rtdebug!(\"releasing %x\", self.id());\n+\n+        let id = self.id();\n+        let _ = id; // XXX: `id` is only used in debug statements so appears unused\n+        let mut this = self;\n+        let mut child_success = true;\n+        let mut children_done = false;\n+\n+        if this.child.is_some() {\n+            rtdebug!(\"releasing children\");\n+            let child_link: &mut ChildLink = this.child.get_mut_ref();\n+            let shared: &mut SharedState = &mut *child_link.shared;\n+\n+            if !child_link.dropped_child {\n+                let last_count = shared.count.fetch_sub(1, SeqCst);\n+                rtdebug!(\"child count before sub %u %x\", last_count, id);\n+                if last_count == 1 {\n+                    assert!(child_link.chan.try_send(ChildrenTerminated));\n+                }\n+                child_link.dropped_child = true;\n+            }\n+\n+            // Wait for messages from children\n+            let mut tombstones = ~[];\n+            loop {\n+                if child_link.port.peek() {\n+                    match child_link.port.recv() {\n+                        Tombstone(t) => {\n+                            tombstones.push(t);\n+                        },\n+                        ChildrenTerminated => {\n+                            children_done = true;\n+                            break;\n+                        }\n+                    }\n+                } else {\n+                    break\n+                }\n+            }\n+\n+            rtdebug!(\"releasing %u tombstones %x\", tombstones.len(), id);\n+\n+            // Try to release the tombstones. Those that still have\n+            // outstanding will be re-enqueued.  When this task's\n+            // parents release their latch we'll end up back here\n+            // trying them again.\n+            while !tombstones.is_empty() {\n+                tombstones.pop().release(true);\n+            }\n+\n+            if children_done {\n+                let count = shared.count.load(SeqCst);\n+                assert!(count == 0);\n+                // self_count is the acquire-read barrier\n+                child_success = shared.success;\n+            }\n+        } else {\n+            children_done = true;\n+        }\n+\n+        let total_success = local_success && child_success;\n+\n+        rtassert!(this.parent.is_some());\n+\n+        unsafe {\n+            {\n+                let parent_link: &mut ParentLink = this.parent.get_mut_ref();\n+                let shared: *mut SharedState = parent_link.shared;\n+\n+                if !total_success {\n+                    // parent_count is the write-wait barrier\n+                    (*shared).success = false;\n+                }\n+            }\n+\n+            if children_done {\n+                rtdebug!(\"children done\");\n+                do Local::borrow::<Scheduler, ()> |sched| {\n+                    sched.metrics.release_tombstone += 1;\n+                }\n+                {\n+                    rtdebug!(\"RELEASING parent %x\", id);\n+                    let parent_link: &mut ParentLink = this.parent.get_mut_ref();\n+                    let shared: *mut SharedState = parent_link.shared;\n+                    let last_count = (*shared).count.fetch_sub(1, SeqCst);\n+                    rtdebug!(\"count before parent sub %u %x\", last_count, id);\n+                    if last_count == 1 {\n+                        assert!(parent_link.chan.try_send(ChildrenTerminated));\n+                    }\n+                }\n+                this.closed = true;\n+                util::ignore(this);\n+            } else {\n+                rtdebug!(\"children not done\");\n+                rtdebug!(\"TOMBSTONING %x\", id);\n+                do Local::borrow::<Scheduler, ()> |sched| {\n+                    sched.metrics.release_no_tombstone += 1;\n+                }\n+                let chan = {\n+                    let parent_link: &mut ParentLink = this.parent.get_mut_ref();\n+                    parent_link.chan.clone()\n+                };\n+                assert!(chan.try_send(Tombstone(this)));\n+            }\n+        }\n+    }\n+\n+    // XXX: Should not require ~self\n+    pub fn wait(~self, local_success: bool) -> bool {\n+        rtassert!(!self.closed);\n+\n+        rtdebug!(\"WAITING %x\", self.id());\n+\n+        let mut this = self;\n+        let mut child_success = true;\n+\n+        if this.child.is_some() {\n+            rtdebug!(\"waiting for children\");\n+            let child_link: &mut ChildLink = this.child.get_mut_ref();\n+            let shared: &mut SharedState = &mut *child_link.shared;\n+\n+            if !child_link.dropped_child {\n+                let last_count = shared.count.fetch_sub(1, SeqCst);\n+                rtdebug!(\"child count before sub %u\", last_count);\n+                if last_count == 1 {\n+                    assert!(child_link.chan.try_send(ChildrenTerminated));\n+                }\n+                child_link.dropped_child = true;\n+            }\n+\n+            // Wait for messages from children\n+            loop {\n+                match child_link.port.recv() {\n+                    Tombstone(t) => {\n+                        t.wait(true);\n+                    }\n+                    ChildrenTerminated => break\n+                }\n+            }\n+\n+            let count = shared.count.load(SeqCst);\n+            if count != 0 { ::io::println(fmt!(\"%u\", count)); }\n+            assert!(count == 0);\n+            // self_count is the acquire-read barrier\n+            child_success = shared.success;\n+        }\n+\n+        let total_success = local_success && child_success;\n+\n+        if this.parent.is_some() {\n+            rtdebug!(\"releasing parent\");\n+            unsafe {\n+                let parent_link: &mut ParentLink = this.parent.get_mut_ref();\n+                let shared: *mut SharedState = parent_link.shared;\n+\n+                if !total_success {\n+                    // parent_count is the write-wait barrier\n+                    (*shared).success = false;\n+                }\n+\n+                let last_count = (*shared).count.fetch_sub(1, SeqCst);\n+                rtdebug!(\"count before parent sub %u\", last_count);\n+                if last_count == 1 {\n+                    assert!(parent_link.chan.try_send(ChildrenTerminated));\n+                }\n+            }\n+        }\n+\n+        this.closed = true;\n+        util::ignore(this);\n+\n+        return total_success;\n+    }\n+}\n+\n+impl Drop for JoinLatch {\n+    fn finalize(&self) {\n+        rtdebug!(\"DESTROYING %x\", self.id());\n+        rtassert!(self.closed);\n+    }\n+}\n+\n+#[cfg(test)]\n+mod test {\n+    use super::*;\n+    use cell::Cell;\n+    use container::Container;\n+    use iter::Times;\n+    use old_iter::BaseIter;\n+    use rt::test::*;\n+    use rand;\n+    use rand::RngUtil;\n+    use vec::{CopyableVector, ImmutableVector};\n+\n+    #[test]\n+    fn success_immediately() {\n+        do run_in_newsched_task {\n+            let mut latch = JoinLatch::new_root();\n+\n+            let child_latch = latch.new_child();\n+            let child_latch = Cell::new(child_latch);\n+            do spawntask_immediately {\n+                let child_latch = child_latch.take();\n+                assert!(child_latch.wait(true));\n+            }\n+\n+            assert!(latch.wait(true));\n+        }\n+    }\n+\n+    #[test]\n+    fn success_later() {\n+        do run_in_newsched_task {\n+            let mut latch = JoinLatch::new_root();\n+\n+            let child_latch = latch.new_child();\n+            let child_latch = Cell::new(child_latch);\n+            do spawntask_later {\n+                let child_latch = child_latch.take();\n+                assert!(child_latch.wait(true));\n+            }\n+\n+            assert!(latch.wait(true));\n+        }\n+    }\n+\n+    #[test]\n+    fn mt_success() {\n+        do run_in_mt_newsched_task {\n+            let mut latch = JoinLatch::new_root();\n+\n+            for 10.times {\n+                let child_latch = latch.new_child();\n+                let child_latch = Cell::new(child_latch);\n+                do spawntask_random {\n+                    let child_latch = child_latch.take();\n+                    assert!(child_latch.wait(true));\n+                }\n+            }\n+\n+            assert!(latch.wait(true));\n+        }\n+    }\n+\n+    #[test]\n+    fn mt_failure() {\n+        do run_in_mt_newsched_task {\n+            let mut latch = JoinLatch::new_root();\n+\n+            let spawn = |status| {\n+                let child_latch = latch.new_child();\n+                let child_latch = Cell::new(child_latch);\n+                do spawntask_random {\n+                    let child_latch = child_latch.take();\n+                    child_latch.wait(status);\n+                }\n+            };\n+\n+            for 10.times { spawn(true) }\n+            spawn(false);\n+            for 10.times { spawn(true) }\n+\n+            assert!(!latch.wait(true));\n+        }\n+    }\n+\n+    #[test]\n+    fn mt_multi_level_success() {\n+        do run_in_mt_newsched_task {\n+            let mut latch = JoinLatch::new_root();\n+\n+            fn child(latch: &mut JoinLatch, i: int) {\n+                let child_latch = latch.new_child();\n+                let child_latch = Cell::new(child_latch);\n+                do spawntask_random {\n+                    let mut child_latch = child_latch.take();\n+                    if i != 0 {\n+                        child(&mut *child_latch, i - 1);\n+                        child_latch.wait(true);\n+                    } else {\n+                        child_latch.wait(true);\n+                    }\n+                }\n+            }\n+\n+            child(&mut *latch, 10);\n+\n+            assert!(latch.wait(true));\n+        }\n+    }\n+\n+    #[test]\n+    fn mt_multi_level_failure() {\n+        do run_in_mt_newsched_task {\n+            let mut latch = JoinLatch::new_root();\n+\n+            fn child(latch: &mut JoinLatch, i: int) {\n+                let child_latch = latch.new_child();\n+                let child_latch = Cell::new(child_latch);\n+                do spawntask_random {\n+                    let mut child_latch = child_latch.take();\n+                    if i != 0 {\n+                        child(&mut *child_latch, i - 1);\n+                        child_latch.wait(false);\n+                    } else {\n+                        child_latch.wait(true);\n+                    }\n+                }\n+            }\n+\n+            child(&mut *latch, 10);\n+\n+            assert!(!latch.wait(true));\n+        }\n+    }\n+\n+    #[test]\n+    fn release_child() {\n+        do run_in_newsched_task {\n+            let mut latch = JoinLatch::new_root();\n+            let child_latch = latch.new_child();\n+            let child_latch = Cell::new(child_latch);\n+\n+            do spawntask_immediately {\n+                let latch = child_latch.take();\n+                latch.release(false);\n+            }\n+\n+            assert!(!latch.wait(true));\n+        }\n+    }\n+\n+    #[test]\n+    fn release_child_tombstone() {\n+        do run_in_newsched_task {\n+            let mut latch = JoinLatch::new_root();\n+            let child_latch = latch.new_child();\n+            let child_latch = Cell::new(child_latch);\n+\n+            do spawntask_immediately {\n+                let mut latch = child_latch.take();\n+                let child_latch = latch.new_child();\n+                let child_latch = Cell::new(child_latch);\n+                do spawntask_later {\n+                    let latch = child_latch.take();\n+                    latch.release(false);\n+                }\n+                latch.release(true);\n+            }\n+\n+            assert!(!latch.wait(true));\n+        }\n+    }\n+\n+    #[test]\n+    fn release_child_no_tombstone() {\n+        do run_in_newsched_task {\n+            let mut latch = JoinLatch::new_root();\n+            let child_latch = latch.new_child();\n+            let child_latch = Cell::new(child_latch);\n+\n+            do spawntask_later {\n+                let mut latch = child_latch.take();\n+                let child_latch = latch.new_child();\n+                let child_latch = Cell::new(child_latch);\n+                do spawntask_immediately {\n+                    let latch = child_latch.take();\n+                    latch.release(false);\n+                }\n+                latch.release(true);\n+            }\n+\n+            assert!(!latch.wait(true));\n+        }\n+    }\n+\n+    #[test]\n+    fn release_child_tombstone_stress() {\n+        fn rand_orders() -> ~[bool] {\n+            let mut v = ~[false,.. 5];\n+            v[0] = true;\n+            let mut rng = rand::rng();\n+            return rng.shuffle(v);\n+        }\n+\n+        fn split_orders(orders: &[bool]) -> (~[bool], ~[bool]) {\n+            if orders.is_empty() {\n+                return (~[], ~[]);\n+            } else if orders.len() <= 2 {\n+                return (orders.to_owned(), ~[]);\n+            }\n+            let mut rng = rand::rng();\n+            let n = rng.gen_uint_range(1, orders.len());\n+            let first = orders.slice(0, n).to_owned();\n+            let last = orders.slice(n, orders.len()).to_owned();\n+            assert!(first.len() + last.len() == orders.len());\n+            return (first, last);\n+        }\n+\n+        for stress_factor().times {\n+            do run_in_newsched_task {\n+                fn doit(latch: &mut JoinLatch, orders: ~[bool], depth: uint) {\n+                    let (my_orders, remaining_orders) = split_orders(orders);\n+                    rtdebug!(\"(my_orders, remaining): %?\", (&my_orders, &remaining_orders));\n+                    rtdebug!(\"depth: %u\", depth);\n+                    let mut remaining_orders = remaining_orders;\n+                    let mut num = 0;\n+                    for my_orders.each |&order| {\n+                        let child_latch = latch.new_child();\n+                        let child_latch = Cell::new(child_latch);\n+                        let (child_orders, remaining) = split_orders(remaining_orders);\n+                        rtdebug!(\"(child_orders, remaining): %?\", (&child_orders, &remaining));\n+                        remaining_orders = remaining;\n+                        let child_orders = Cell::new(child_orders);\n+                        let child_num = num;\n+                        let _ = child_num; // XXX unused except in rtdebug!\n+                        do spawntask_random {\n+                            rtdebug!(\"depth %u num %u\", depth, child_num);\n+                            let mut child_latch = child_latch.take();\n+                            let child_orders = child_orders.take();\n+                            doit(&mut *child_latch, child_orders, depth + 1);\n+                            child_latch.release(order);\n+                        }\n+\n+                        num += 1;\n+                    }\n+                }\n+\n+                let mut latch = JoinLatch::new_root();\n+                let orders = rand_orders();\n+                rtdebug!(\"orders: %?\", orders);\n+\n+                doit(&mut *latch, orders, 0);\n+\n+                assert!(!latch.wait(true));\n+            }\n+        }\n+    }\n+\n+    #[test]\n+    fn whateverman() {\n+        struct Order {\n+            immediate: bool,\n+            succeed: bool,\n+            orders: ~[Order]\n+        }\n+        fn next(latch: &mut JoinLatch, orders: ~[Order]) {\n+            for orders.each |order| {\n+                let suborders = copy order.orders;\n+                let child_latch = Cell::new(latch.new_child());\n+                let succeed = order.succeed;\n+                if order.immediate {\n+                    do spawntask_immediately {\n+                        let mut child_latch = child_latch.take();\n+                        next(&mut *child_latch, copy suborders);\n+                        rtdebug!(\"immediate releasing\");\n+                        child_latch.release(succeed);\n+                    }\n+                } else {\n+                    do spawntask_later {\n+                        let mut child_latch = child_latch.take();\n+                        next(&mut *child_latch, copy suborders);\n+                        rtdebug!(\"later releasing\");\n+                        child_latch.release(succeed);\n+                    }\n+                }\n+            }\n+        }\n+\n+        do run_in_newsched_task {\n+            let mut latch = JoinLatch::new_root();\n+            let orders = ~[ Order { // 0 0\n+                immediate: true,\n+                succeed: true,\n+                orders: ~[ Order { // 1 0\n+                    immediate: true,\n+                    succeed: false,\n+                    orders: ~[ Order { // 2 0\n+                        immediate: false,\n+                        succeed: false,\n+                        orders: ~[ Order { // 3 0\n+                            immediate: true,\n+                            succeed: false,\n+                            orders: ~[]\n+                        }, Order { // 3 1\n+                            immediate: false,\n+                            succeed: false,\n+                            orders: ~[]\n+                        }]\n+                    }]\n+                }]\n+            }];\n+\n+            next(&mut *latch, orders);\n+            assert!(!latch.wait(true));\n+        }\n+    }\n+}"}, {"sha": "6df1ffaa453f3482b256206cfcab077e5a9ee8bf", "filename": "src/libstd/rt/local.rs", "status": "modified", "additions": 45, "deletions": 20, "changes": 65, "blob_url": "https://github.com/rust-lang/rust/blob/357f087786cbd6516a38aff800cf9334bc5b85c5/src%2Flibstd%2Frt%2Flocal.rs", "raw_url": "https://github.com/rust-lang/rust/raw/357f087786cbd6516a38aff800cf9334bc5b85c5/src%2Flibstd%2Frt%2Flocal.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibstd%2Frt%2Flocal.rs?ref=357f087786cbd6516a38aff800cf9334bc5b85c5", "patch": "@@ -18,7 +18,7 @@ pub trait Local {\n     fn put(value: ~Self);\n     fn take() -> ~Self;\n     fn exists() -> bool;\n-    fn borrow(f: &fn(&mut Self));\n+    fn borrow<T>(f: &fn(&mut Self) -> T) -> T;\n     unsafe fn unsafe_borrow() -> *mut Self;\n     unsafe fn try_unsafe_borrow() -> Option<*mut Self>;\n }\n@@ -27,23 +27,36 @@ impl Local for Scheduler {\n     fn put(value: ~Scheduler) { unsafe { local_ptr::put(value) }}\n     fn take() -> ~Scheduler { unsafe { local_ptr::take() } }\n     fn exists() -> bool { local_ptr::exists() }\n-    fn borrow(f: &fn(&mut Scheduler)) { unsafe { local_ptr::borrow(f) } }\n+    fn borrow<T>(f: &fn(&mut Scheduler) -> T) -> T {\n+        let mut res: Option<T> = None;\n+        let res_ptr: *mut Option<T> = &mut res;\n+        unsafe {\n+            do local_ptr::borrow |sched| {\n+                let result = f(sched);\n+                *res_ptr = Some(result);\n+            }\n+        }\n+        match res {\n+            Some(r) => { r }\n+            None => rtabort!(\"function failed!\")\n+        }\n+    }\n     unsafe fn unsafe_borrow() -> *mut Scheduler { local_ptr::unsafe_borrow() }\n-    unsafe fn try_unsafe_borrow() -> Option<*mut Scheduler> { abort!(\"unimpl\") }\n+    unsafe fn try_unsafe_borrow() -> Option<*mut Scheduler> { rtabort!(\"unimpl\") }\n }\n \n impl Local for Task {\n-    fn put(_value: ~Task) { abort!(\"unimpl\") }\n-    fn take() -> ~Task { abort!(\"unimpl\") }\n-    fn exists() -> bool { abort!(\"unimpl\") }\n-    fn borrow(f: &fn(&mut Task)) {\n-        do Local::borrow::<Scheduler> |sched| {\n+    fn put(_value: ~Task) { rtabort!(\"unimpl\") }\n+    fn take() -> ~Task { rtabort!(\"unimpl\") }\n+    fn exists() -> bool { rtabort!(\"unimpl\") }\n+    fn borrow<T>(f: &fn(&mut Task) -> T) -> T {\n+        do Local::borrow::<Scheduler, T> |sched| {\n             match sched.current_task {\n                 Some(~ref mut task) => {\n                     f(&mut *task.task)\n                 }\n                 None => {\n-                    abort!(\"no scheduler\")\n+                    rtabort!(\"no scheduler\")\n                 }\n             }\n         }\n@@ -56,7 +69,7 @@ impl Local for Task {\n             }\n             None => {\n                 // Don't fail. Infinite recursion\n-                abort!(\"no scheduler\")\n+                rtabort!(\"no scheduler\")\n             }\n         }\n     }\n@@ -71,48 +84,60 @@ impl Local for Task {\n \n // XXX: This formulation won't work once ~IoFactoryObject is a real trait pointer\n impl Local for IoFactoryObject {\n-    fn put(_value: ~IoFactoryObject) { abort!(\"unimpl\") }\n-    fn take() -> ~IoFactoryObject { abort!(\"unimpl\") }\n-    fn exists() -> bool { abort!(\"unimpl\") }\n-    fn borrow(_f: &fn(&mut IoFactoryObject)) { abort!(\"unimpl\") }\n+    fn put(_value: ~IoFactoryObject) { rtabort!(\"unimpl\") }\n+    fn take() -> ~IoFactoryObject { rtabort!(\"unimpl\") }\n+    fn exists() -> bool { rtabort!(\"unimpl\") }\n+    fn borrow<T>(_f: &fn(&mut IoFactoryObject) -> T) -> T { rtabort!(\"unimpl\") }\n     unsafe fn unsafe_borrow() -> *mut IoFactoryObject {\n         let sched = Local::unsafe_borrow::<Scheduler>();\n         let io: *mut IoFactoryObject = (*sched).event_loop.io().unwrap();\n         return io;\n     }\n-    unsafe fn try_unsafe_borrow() -> Option<*mut IoFactoryObject> { abort!(\"unimpl\") }\n+    unsafe fn try_unsafe_borrow() -> Option<*mut IoFactoryObject> { rtabort!(\"unimpl\") }\n }\n \n #[cfg(test)]\n mod test {\n+    use rt::test::*;\n     use rt::sched::Scheduler;\n-    use rt::uv::uvio::UvEventLoop;\n     use super::*;\n \n     #[test]\n     fn thread_local_scheduler_smoke_test() {\n-        let scheduler = ~UvEventLoop::new_scheduler();\n+        let scheduler = ~new_test_uv_sched();\n         Local::put(scheduler);\n         let _scheduler: ~Scheduler = Local::take();\n     }\n \n     #[test]\n     fn thread_local_scheduler_two_instances() {\n-        let scheduler = ~UvEventLoop::new_scheduler();\n+        let scheduler = ~new_test_uv_sched();\n         Local::put(scheduler);\n         let _scheduler: ~Scheduler = Local::take();\n-        let scheduler = ~UvEventLoop::new_scheduler();\n+        let scheduler = ~new_test_uv_sched();\n         Local::put(scheduler);\n         let _scheduler: ~Scheduler = Local::take();\n     }\n \n     #[test]\n     fn borrow_smoke_test() {\n-        let scheduler = ~UvEventLoop::new_scheduler();\n+        let scheduler = ~new_test_uv_sched();\n         Local::put(scheduler);\n         unsafe {\n             let _scheduler: *mut Scheduler = Local::unsafe_borrow();\n         }\n         let _scheduler: ~Scheduler = Local::take();\n     }\n+\n+    #[test]\n+    fn borrow_with_return() {\n+        let scheduler = ~new_test_uv_sched();\n+        Local::put(scheduler);\n+        let res = do Local::borrow::<Scheduler,bool> |_sched| {\n+            true\n+        };\n+        assert!(res)\n+        let _scheduler: ~Scheduler = Local::take();\n+    }\n+\n }"}, {"sha": "cd7c5daa444d7998970024b414c145ed22491cab", "filename": "src/libstd/rt/local_ptr.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/357f087786cbd6516a38aff800cf9334bc5b85c5/src%2Flibstd%2Frt%2Flocal_ptr.rs", "raw_url": "https://github.com/rust-lang/rust/raw/357f087786cbd6516a38aff800cf9334bc5b85c5/src%2Flibstd%2Frt%2Flocal_ptr.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibstd%2Frt%2Flocal_ptr.rs?ref=357f087786cbd6516a38aff800cf9334bc5b85c5", "patch": "@@ -109,7 +109,7 @@ pub unsafe fn unsafe_borrow<T>() -> *mut T {\n fn tls_key() -> tls::Key {\n     match maybe_tls_key() {\n         Some(key) => key,\n-        None => abort!(\"runtime tls key not initialized\")\n+        None => rtabort!(\"runtime tls key not initialized\")\n     }\n }\n "}, {"sha": "734be808797487920badd1db96ab89026ad1c07f", "filename": "src/libstd/rt/message_queue.rs", "status": "modified", "additions": 3, "deletions": 0, "changes": 3, "blob_url": "https://github.com/rust-lang/rust/blob/357f087786cbd6516a38aff800cf9334bc5b85c5/src%2Flibstd%2Frt%2Fmessage_queue.rs", "raw_url": "https://github.com/rust-lang/rust/raw/357f087786cbd6516a38aff800cf9334bc5b85c5/src%2Flibstd%2Frt%2Fmessage_queue.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibstd%2Frt%2Fmessage_queue.rs?ref=357f087786cbd6516a38aff800cf9334bc5b85c5", "patch": "@@ -8,6 +8,9 @@\n // option. This file may not be copied, modified, or distributed\n // except according to those terms.\n \n+//! A concurrent queue that supports multiple producers and a\n+//! single consumer.\n+\n use container::Container;\n use kinds::Owned;\n use vec::OwnedVector;"}, {"sha": "b0c0fa5d708623d216bdfb58bdf6947da29ada29", "filename": "src/libstd/rt/metrics.rs", "status": "added", "additions": 98, "deletions": 0, "changes": 98, "blob_url": "https://github.com/rust-lang/rust/blob/357f087786cbd6516a38aff800cf9334bc5b85c5/src%2Flibstd%2Frt%2Fmetrics.rs", "raw_url": "https://github.com/rust-lang/rust/raw/357f087786cbd6516a38aff800cf9334bc5b85c5/src%2Flibstd%2Frt%2Fmetrics.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibstd%2Frt%2Fmetrics.rs?ref=357f087786cbd6516a38aff800cf9334bc5b85c5", "patch": "@@ -0,0 +1,98 @@\n+// Copyright 2013 The Rust Project Developers. See the COPYRIGHT\n+// file at the top-level directory of this distribution and at\n+// http://rust-lang.org/COPYRIGHT.\n+//\n+// Licensed under the Apache License, Version 2.0 <LICENSE-APACHE or\n+// http://www.apache.org/licenses/LICENSE-2.0> or the MIT license\n+// <LICENSE-MIT or http://opensource.org/licenses/MIT>, at your\n+// option. This file may not be copied, modified, or distributed\n+// except according to those terms.\n+\n+use to_str::ToStr;\n+\n+pub struct SchedMetrics {\n+    // The number of times executing `run_sched_once`.\n+    turns: uint,\n+    // The number of turns that received a message.\n+    messages_received: uint,\n+    // The number of turns that ran a task from the queue.\n+    tasks_resumed_from_queue: uint,\n+    // The number of turns that found no work to perform.\n+    wasted_turns: uint,\n+    // The number of times the scheduler went to sleep.\n+    sleepy_times: uint,\n+    // Context switches from the scheduler into a task.\n+    context_switches_sched_to_task: uint,\n+    // Context switches from a task into the scheduler.\n+    context_switches_task_to_sched: uint,\n+    // Context switches from a task to a task.\n+    context_switches_task_to_task: uint,\n+    // Message sends that unblock the receiver\n+    rendezvous_sends: uint,\n+    // Message sends that do not unblock the receiver\n+    non_rendezvous_sends: uint,\n+    // Message receives that do not block the receiver\n+    rendezvous_recvs: uint,\n+    // Message receives that block the receiver\n+    non_rendezvous_recvs: uint,\n+    // JoinLatch releases that create tombstones\n+    release_tombstone: uint,\n+    // JoinLatch releases that do not create tombstones\n+    release_no_tombstone: uint,\n+}\n+\n+impl SchedMetrics {\n+    pub fn new() -> SchedMetrics {\n+        SchedMetrics {\n+            turns: 0,\n+            messages_received: 0,\n+            tasks_resumed_from_queue: 0,\n+            wasted_turns: 0,\n+            sleepy_times: 0,\n+            context_switches_sched_to_task: 0,\n+            context_switches_task_to_sched: 0,\n+            context_switches_task_to_task: 0,\n+            rendezvous_sends: 0,\n+            non_rendezvous_sends: 0,\n+            rendezvous_recvs: 0,\n+            non_rendezvous_recvs: 0,\n+            release_tombstone: 0,\n+            release_no_tombstone: 0\n+        }\n+    }\n+}\n+\n+impl ToStr for SchedMetrics {\n+    fn to_str(&self) -> ~str {\n+        fmt!(\"turns: %u\\n\\\n+              messages_received: %u\\n\\\n+              tasks_resumed_from_queue: %u\\n\\\n+              wasted_turns: %u\\n\\\n+              sleepy_times: %u\\n\\\n+              context_switches_sched_to_task: %u\\n\\\n+              context_switches_task_to_sched: %u\\n\\\n+              context_switches_task_to_task: %u\\n\\\n+              rendezvous_sends: %u\\n\\\n+              non_rendezvous_sends: %u\\n\\\n+              rendezvous_recvs: %u\\n\\\n+              non_rendezvous_recvs: %u\\n\\\n+              release_tombstone: %u\\n\\\n+              release_no_tombstone: %u\\n\\\n+              \",\n+             self.turns,\n+             self.messages_received,\n+             self.tasks_resumed_from_queue,\n+             self.wasted_turns,\n+             self.sleepy_times,\n+             self.context_switches_sched_to_task,\n+             self.context_switches_task_to_sched,\n+             self.context_switches_task_to_task,\n+             self.rendezvous_sends,\n+             self.non_rendezvous_sends,\n+             self.rendezvous_recvs,\n+             self.non_rendezvous_recvs,\n+             self.release_tombstone,\n+             self.release_no_tombstone\n+        )\n+    }\n+}\n\\ No newline at end of file"}, {"sha": "a80fb15bad798f6d96fc002f138f30c83bbeeea3", "filename": "src/libstd/rt/mod.rs", "status": "modified", "additions": 139, "deletions": 23, "changes": 162, "blob_url": "https://github.com/rust-lang/rust/blob/357f087786cbd6516a38aff800cf9334bc5b85c5/src%2Flibstd%2Frt%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/357f087786cbd6516a38aff800cf9334bc5b85c5/src%2Flibstd%2Frt%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibstd%2Frt%2Fmod.rs?ref=357f087786cbd6516a38aff800cf9334bc5b85c5", "patch": "@@ -55,8 +55,28 @@ Several modules in `core` are clients of `rt`:\n */\n \n #[doc(hidden)];\n-\n+#[deny(unused_imports)];\n+#[deny(unused_mut)];\n+#[deny(unused_variable)];\n+\n+use cell::Cell;\n+use clone::Clone;\n+use container::Container;\n+use from_str::FromStr;\n+use iter::Times;\n+use iterator::IteratorUtil;\n+use option::{Some, None};\n+use os;\n use ptr::RawPtr;\n+use rt::sched::{Scheduler, Coroutine, Shutdown};\n+use rt::sleeper_list::SleeperList;\n+use rt::task::Task;\n+use rt::thread::Thread;\n+use rt::work_queue::WorkQueue;\n+use rt::uv::uvio::UvEventLoop;\n+use unstable::atomics::{AtomicInt, SeqCst};\n+use unstable::sync::UnsafeAtomicRcBox;\n+use vec::{OwnedVector, MutableVector};\n \n /// The global (exchange) heap.\n pub mod global_heap;\n@@ -88,6 +108,9 @@ mod work_queue;\n /// A parallel queue.\n mod message_queue;\n \n+/// A parallel data structure for tracking sleeping schedulers.\n+mod sleeper_list;\n+\n /// Stack segments and caching.\n mod stack;\n \n@@ -127,6 +150,14 @@ pub mod local_ptr;\n /// Bindings to pthread/windows thread-local storage.\n pub mod thread_local_storage;\n \n+/// For waiting on child tasks.\n+pub mod join_latch;\n+\n+pub mod metrics;\n+\n+// FIXME #5248 shouldn't be pub\n+/// Just stuff\n+pub mod util;\n \n /// Set up a default runtime configuration, given compiler-supplied arguments.\n ///\n@@ -144,25 +175,114 @@ pub mod thread_local_storage;\n /// The return value is used as the process return code. 0 on success, 101 on error.\n pub fn start(_argc: int, _argv: **u8, crate_map: *u8, main: ~fn()) -> int {\n \n-    use self::sched::{Scheduler, Coroutine};\n-    use self::uv::uvio::UvEventLoop;\n-\n     init(crate_map);\n+    let exit_code = run(main);\n+    cleanup();\n \n-    let loop_ = ~UvEventLoop::new();\n-    let mut sched = ~Scheduler::new(loop_);\n-    let main_task = ~Coroutine::new(&mut sched.stack_pool, main);\n-\n-    sched.enqueue_task(main_task);\n-    sched.run();\n-\n-    return 0;\n+    return exit_code;\n }\n \n /// One-time runtime initialization. Currently all this does is set up logging\n /// based on the RUST_LOG environment variable.\n pub fn init(crate_map: *u8) {\n     logging::init(crate_map);\n+    unsafe { rust_update_gc_metadata(crate_map) }\n+\n+    extern {\n+        fn rust_update_gc_metadata(crate_map: *u8);\n+    }\n+}\n+\n+/// One-time runtime cleanup.\n+pub fn cleanup() {\n+    global_heap::cleanup();\n+}\n+\n+/// Execute the main function in a scheduler.\n+///\n+/// Configures the runtime according to the environment, by default\n+/// using a task scheduler with the same number of threads as cores.\n+/// Returns a process exit code.\n+pub fn run(main: ~fn()) -> int {\n+\n+    static DEFAULT_ERROR_CODE: int = 101;\n+\n+    let nthreads = match os::getenv(\"RUST_THREADS\") {\n+        Some(nstr) => FromStr::from_str(nstr).get(),\n+        None => unsafe { util::num_cpus() }\n+    };\n+\n+    // The shared list of sleeping schedulers. Schedulers wake each other\n+    // occassionally to do new work.\n+    let sleepers = SleeperList::new();\n+    // The shared work queue. Temporary until work stealing is implemented.\n+    let work_queue = WorkQueue::new();\n+\n+    // The schedulers.\n+    let mut scheds = ~[];\n+    // Handles to the schedulers. When the main task ends these will be\n+    // sent the Shutdown message to terminate the schedulers.\n+    let mut handles = ~[];\n+\n+    for nthreads.times {\n+        // Every scheduler is driven by an I/O event loop.\n+        let loop_ = ~UvEventLoop::new();\n+        let mut sched = ~Scheduler::new(loop_, work_queue.clone(), sleepers.clone());\n+        let handle = sched.make_handle();\n+\n+        scheds.push(sched);\n+        handles.push(handle);\n+    }\n+\n+    // Create a shared cell for transmitting the process exit\n+    // code from the main task to this function.\n+    let exit_code = UnsafeAtomicRcBox::new(AtomicInt::new(0));\n+    let exit_code_clone = exit_code.clone();\n+\n+    // When the main task exits, after all the tasks in the main\n+    // task tree, shut down the schedulers and set the exit code.\n+    let handles = Cell::new(handles);\n+    let on_exit: ~fn(bool) = |exit_success| {\n+\n+        let mut handles = handles.take();\n+        for handles.mut_iter().advance |handle| {\n+            handle.send(Shutdown);\n+        }\n+\n+        unsafe {\n+            let exit_code = if exit_success { 0 } else { DEFAULT_ERROR_CODE };\n+            (*exit_code_clone.get()).store(exit_code, SeqCst);\n+        }\n+    };\n+\n+    // Create and enqueue the main task.\n+    let main_cell = Cell::new(main);\n+    let mut new_task = ~Task::new_root();\n+    new_task.on_exit = Some(on_exit);\n+    let main_task = ~Coroutine::with_task(&mut scheds[0].stack_pool,\n+                                          new_task, main_cell.take());\n+    scheds[0].enqueue_task(main_task);\n+\n+    // Run each scheduler in a thread.\n+    let mut threads = ~[];\n+    while !scheds.is_empty() {\n+        let sched = scheds.pop();\n+        let sched_cell = Cell::new(sched);\n+        let thread = do Thread::start {\n+            let sched = sched_cell.take();\n+            sched.run();\n+        };\n+\n+        threads.push(thread);\n+    }\n+\n+    // Wait for schedulers\n+    { let _threads = threads; }\n+\n+    // Return the exit code\n+    unsafe {\n+        (*exit_code.get()).load(SeqCst)\n+    }\n }\n \n /// Possible contexts in which Rust code may be executing.\n@@ -194,8 +314,8 @@ pub fn context() -> RuntimeContext {\n         return OldTaskContext;\n     } else {\n         if Local::exists::<Scheduler>() {\n-            let context = ::cell::Cell::new_empty();\n-            do Local::borrow::<Scheduler> |sched| {\n+            let context = Cell::new_empty();\n+            do Local::borrow::<Scheduler, ()> |sched| {\n                 if sched.in_task_context() {\n                     context.put_back(TaskContext);\n                 } else {\n@@ -218,23 +338,19 @@ pub fn context() -> RuntimeContext {\n fn test_context() {\n     use unstable::run_in_bare_thread;\n     use self::sched::{Scheduler, Coroutine};\n-    use rt::uv::uvio::UvEventLoop;\n-    use cell::Cell;\n     use rt::local::Local;\n+    use rt::test::new_test_uv_sched;\n \n     assert_eq!(context(), OldTaskContext);\n     do run_in_bare_thread {\n         assert_eq!(context(), GlobalContext);\n-        let mut sched = ~UvEventLoop::new_scheduler();\n-        let task = ~do Coroutine::new(&mut sched.stack_pool) {\n+        let mut sched = ~new_test_uv_sched();\n+        let task = ~do Coroutine::new_root(&mut sched.stack_pool) {\n             assert_eq!(context(), TaskContext);\n             let sched = Local::take::<Scheduler>();\n-            do sched.deschedule_running_task_and_then() |task| {\n+            do sched.deschedule_running_task_and_then() |sched, task| {\n                 assert_eq!(context(), SchedulerContext);\n-                let task = Cell::new(task);\n-                do Local::borrow::<Scheduler> |sched| {\n-                    sched.enqueue_task(task.take());\n-                }\n+                sched.enqueue_task(task);\n             }\n         };\n         sched.enqueue_task(task);"}, {"sha": "fa657555f3aa0fc6f8012a9ad164746f7a1ae6a1", "filename": "src/libstd/rt/rtio.rs", "status": "modified", "additions": 11, "deletions": 0, "changes": 11, "blob_url": "https://github.com/rust-lang/rust/blob/357f087786cbd6516a38aff800cf9334bc5b85c5/src%2Flibstd%2Frt%2Frtio.rs", "raw_url": "https://github.com/rust-lang/rust/raw/357f087786cbd6516a38aff800cf9334bc5b85c5/src%2Flibstd%2Frt%2Frtio.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibstd%2Frt%2Frtio.rs?ref=357f087786cbd6516a38aff800cf9334bc5b85c5", "patch": "@@ -18,6 +18,7 @@ use rt::uv::uvio;\n // XXX: ~object doesn't work currently so these are some placeholder\n // types to use instead\n pub type EventLoopObject = uvio::UvEventLoop;\n+pub type RemoteCallbackObject = uvio::UvRemoteCallback;\n pub type IoFactoryObject = uvio::UvIoFactory;\n pub type RtioTcpStreamObject = uvio::UvTcpStream;\n pub type RtioTcpListenerObject = uvio::UvTcpListener;\n@@ -26,10 +27,20 @@ pub trait EventLoop {\n     fn run(&mut self);\n     fn callback(&mut self, ~fn());\n     fn callback_ms(&mut self, ms: u64, ~fn());\n+    fn remote_callback(&mut self, ~fn()) -> ~RemoteCallbackObject;\n     /// The asynchronous I/O services. Not all event loops may provide one\n     fn io<'a>(&'a mut self) -> Option<&'a mut IoFactoryObject>;\n }\n \n+pub trait RemoteCallback {\n+    /// Trigger the remote callback. Note that the number of times the callback\n+    /// is run is not guaranteed. All that is guaranteed is that, after calling 'fire',\n+    /// the callback will be called at least once, but multiple callbacks may be coalesced\n+    /// and callbacks may be called more often requested. Destruction also triggers the\n+    /// callback.\n+    fn fire(&mut self);\n+}\n+\n pub trait IoFactory {\n     fn tcp_connect(&mut self, addr: IpAddr) -> Result<~RtioTcpStreamObject, IoError>;\n     fn tcp_bind(&mut self, addr: IpAddr) -> Result<~RtioTcpListenerObject, IoError>;"}, {"sha": "bbe4aa25e2967d5eb01a31d59cec5b2285e21d63", "filename": "src/libstd/rt/sched.rs", "status": "modified", "additions": 958, "deletions": 100, "changes": 1058, "blob_url": "https://github.com/rust-lang/rust/blob/357f087786cbd6516a38aff800cf9334bc5b85c5/src%2Flibstd%2Frt%2Fsched.rs", "raw_url": "https://github.com/rust-lang/rust/raw/357f087786cbd6516a38aff800cf9334bc5b85c5/src%2Flibstd%2Frt%2Fsched.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibstd%2Frt%2Fsched.rs?ref=357f087786cbd6516a38aff800cf9334bc5b85c5", "patch": "@@ -12,21 +12,53 @@ use option::*;\n use sys;\n use cast::transmute;\n use cell::Cell;\n+use clone::Clone;\n \n+use super::sleeper_list::SleeperList;\n use super::work_queue::WorkQueue;\n use super::stack::{StackPool, StackSegment};\n-use super::rtio::{EventLoop, EventLoopObject};\n+use super::rtio::{EventLoop, EventLoopObject, RemoteCallbackObject};\n use super::context::Context;\n use super::task::Task;\n+use super::message_queue::MessageQueue;\n use rt::local_ptr;\n use rt::local::Local;\n+use rt::rtio::RemoteCallback;\n+use rt::metrics::SchedMetrics;\n+\n+//use to_str::ToStr;\n+\n+/// To allow for using pointers as scheduler ids\n+use borrow::{to_uint};\n \n /// The Scheduler is responsible for coordinating execution of Coroutines\n /// on a single thread. When the scheduler is running it is owned by\n /// thread local storage and the running task is owned by the\n /// scheduler.\n+///\n+/// XXX: This creates too many callbacks to run_sched_once, resulting\n+/// in too much allocation and too many events.\n pub struct Scheduler {\n+    /// A queue of available work. Under a work-stealing policy there\n+    /// is one per Scheduler.\n     priv work_queue: WorkQueue<~Coroutine>,\n+    /// The queue of incoming messages from other schedulers.\n+    /// These are enqueued by SchedHandles after which a remote callback\n+    /// is triggered to handle the message.\n+    priv message_queue: MessageQueue<SchedMessage>,\n+    /// A shared list of sleeping schedulers. We'll use this to wake\n+    /// up schedulers when pushing work onto the work queue.\n+    priv sleeper_list: SleeperList,\n+    /// Indicates that we have previously pushed a handle onto the\n+    /// SleeperList but have not yet received the Wake message.\n+    /// Being `true` does not necessarily mean that the scheduler is\n+    /// not active since there are multiple event sources that may\n+    /// wake the scheduler. It just prevents the scheduler from pushing\n+    /// multiple handles onto the sleeper list.\n+    priv sleepy: bool,\n+    /// A flag to indicate we've received the shutdown message and should\n+    /// no longer try to go to sleep, but exit instead.\n+    no_sleep: bool,\n     stack_pool: StackPool,\n     /// The event loop used to drive the scheduler and perform I/O\n     event_loop: ~EventLoopObject,\n@@ -37,19 +69,40 @@ pub struct Scheduler {\n     current_task: Option<~Coroutine>,\n     /// An action performed after a context switch on behalf of the\n     /// code running before the context switch\n-    priv cleanup_job: Option<CleanupJob>\n+    priv cleanup_job: Option<CleanupJob>,\n+    metrics: SchedMetrics,\n+    /// Should this scheduler run any task, or only pinned tasks?\n+    run_anything: bool\n }\n \n-// XXX: Some hacks to put a &fn in Scheduler without borrowck\n-// complaining\n-type UnsafeTaskReceiver = sys::Closure;\n-trait ClosureConverter {\n-    fn from_fn(&fn(~Coroutine)) -> Self;\n-    fn to_fn(self) -> &fn(~Coroutine);\n+pub struct SchedHandle {\n+    priv remote: ~RemoteCallbackObject,\n+    priv queue: MessageQueue<SchedMessage>,\n+    sched_id: uint\n }\n-impl ClosureConverter for UnsafeTaskReceiver {\n-    fn from_fn(f: &fn(~Coroutine)) -> UnsafeTaskReceiver { unsafe { transmute(f) } }\n-    fn to_fn(self) -> &fn(~Coroutine) { unsafe { transmute(self) } }\n+\n+pub struct Coroutine {\n+    /// The segment of stack on which the task is currently running or,\n+    /// if the task is blocked, on which the task will resume execution\n+    priv current_stack_segment: StackSegment,\n+    /// These are always valid when the task is not running, unless\n+    /// the task is dead\n+    priv saved_context: Context,\n+    /// The heap, GC, unwinding, local storage, logging\n+    task: ~Task,\n+}\n+\n+// A scheduler home is either a handle to the home scheduler, or an\n+// explicit \"AnySched\".\n+pub enum SchedHome {\n+    AnySched,\n+    Sched(SchedHandle)\n+}\n+\n+pub enum SchedMessage {\n+    Wake,\n+    Shutdown,\n+    PinnedTask(~Coroutine)\n }\n \n enum CleanupJob {\n@@ -60,18 +113,39 @@ enum CleanupJob {\n impl Scheduler {\n     pub fn in_task_context(&self) -> bool { self.current_task.is_some() }\n \n-    pub fn new(event_loop: ~EventLoopObject) -> Scheduler {\n+    pub fn sched_id(&self) -> uint { to_uint(self) }\n+\n+    pub fn new(event_loop: ~EventLoopObject,\n+               work_queue: WorkQueue<~Coroutine>,\n+               sleeper_list: SleeperList)\n+        -> Scheduler {\n+\n+        Scheduler::new_special(event_loop, work_queue, sleeper_list, true)\n+\n+    }\n+\n+    pub fn new_special(event_loop: ~EventLoopObject,\n+                       work_queue: WorkQueue<~Coroutine>,\n+                       sleeper_list: SleeperList,\n+                       run_anything: bool)\n+        -> Scheduler {\n \n         // Lazily initialize the runtime TLS key\n         local_ptr::init_tls_key();\n \n         Scheduler {\n+            sleeper_list: sleeper_list,\n+            message_queue: MessageQueue::new(),\n+            sleepy: false,\n+            no_sleep: false,\n             event_loop: event_loop,\n-            work_queue: WorkQueue::new(),\n+            work_queue: work_queue,\n             stack_pool: StackPool::new(),\n             saved_context: Context::empty(),\n             current_task: None,\n-            cleanup_job: None\n+            cleanup_job: None,\n+            metrics: SchedMetrics::new(),\n+            run_anything: run_anything\n         }\n     }\n \n@@ -84,6 +158,11 @@ impl Scheduler {\n \n         let mut self_sched = self;\n \n+        // Always run through the scheduler loop at least once so that\n+        // we enter the sleep state and can then be woken up by other\n+        // schedulers.\n+        self_sched.event_loop.callback(Scheduler::run_sched_once);\n+\n         unsafe {\n             let event_loop: *mut ~EventLoopObject = {\n                 let event_loop: *mut ~EventLoopObject = &mut self_sched.event_loop;\n@@ -96,46 +175,261 @@ impl Scheduler {\n             (*event_loop).run();\n         }\n \n+        rtdebug!(\"run taking sched\");\n         let sched = Local::take::<Scheduler>();\n-        assert!(sched.work_queue.is_empty());\n+        // XXX: Reenable this once we're using a per-task queue. With a shared\n+        // queue this is not true\n+        //assert!(sched.work_queue.is_empty());\n+        rtdebug!(\"scheduler metrics: %s\\n\", {\n+            use to_str::ToStr;\n+            sched.metrics.to_str()\n+        });\n         return sched;\n     }\n \n+    fn run_sched_once() {\n+\n+        let mut sched = Local::take::<Scheduler>();\n+        sched.metrics.turns += 1;\n+\n+        // First, check the message queue for instructions.\n+        // XXX: perf. Check for messages without atomics.\n+        // It's ok if we miss messages occasionally, as long as\n+        // we sync and check again before sleeping.\n+        if sched.interpret_message_queue() {\n+            // We performed a scheduling action. There may be other work\n+            // to do yet, so let's try again later.\n+            rtdebug!(\"run_sched_once, interpret_message_queue taking sched\");\n+            let mut sched = Local::take::<Scheduler>();\n+            sched.metrics.messages_received += 1;\n+            sched.event_loop.callback(Scheduler::run_sched_once);\n+            Local::put(sched);\n+            return;\n+        }\n+\n+        // Now, look in the work queue for tasks to run\n+        rtdebug!(\"run_sched_once taking\");\n+        let sched = Local::take::<Scheduler>();\n+        if sched.resume_task_from_queue() {\n+            // We performed a scheduling action. There may be other work\n+            // to do yet, so let's try again later.\n+            let mut sched = Local::take::<Scheduler>();\n+            sched.metrics.tasks_resumed_from_queue += 1;\n+            sched.event_loop.callback(Scheduler::run_sched_once);\n+            Local::put(sched);\n+            return;\n+        }\n+\n+        // If we got here then there was no work to do.\n+        // Generate a SchedHandle and push it to the sleeper list so\n+        // somebody can wake us up later.\n+        rtdebug!(\"no work to do\");\n+        let mut sched = Local::take::<Scheduler>();\n+        sched.metrics.wasted_turns += 1;\n+        if !sched.sleepy && !sched.no_sleep {\n+            rtdebug!(\"sleeping\");\n+            sched.metrics.sleepy_times += 1;\n+            sched.sleepy = true;\n+            let handle = sched.make_handle();\n+            sched.sleeper_list.push(handle);\n+        } else {\n+            rtdebug!(\"not sleeping\");\n+        }\n+        Local::put(sched);\n+    }\n+\n+    pub fn make_handle(&mut self) -> SchedHandle {\n+        let remote = self.event_loop.remote_callback(Scheduler::run_sched_once);\n+\n+        return SchedHandle {\n+            remote: remote,\n+            queue: self.message_queue.clone(),\n+            sched_id: self.sched_id()\n+        };\n+    }\n+\n     /// Schedule a task to be executed later.\n     ///\n-    /// Pushes the task onto the work stealing queue and tells the event loop\n-    /// to run it later. Always use this instead of pushing to the work queue\n-    /// directly.\n+    /// Pushes the task onto the work stealing queue and tells the\n+    /// event loop to run it later. Always use this instead of pushing\n+    /// to the work queue directly.\n     pub fn enqueue_task(&mut self, task: ~Coroutine) {\n-        self.work_queue.push(task);\n-        self.event_loop.callback(resume_task_from_queue);\n \n-        fn resume_task_from_queue() {\n-            let scheduler = Local::take::<Scheduler>();\n-            scheduler.resume_task_from_queue();\n-        }\n+        // We don't want to queue tasks that belong on other threads,\n+        // so we send them home at enqueue time.\n+\n+        // The borrow checker doesn't like our disassembly of the\n+        // Coroutine struct and partial use and mutation of the\n+        // fields. So completely disassemble here and stop using?\n+\n+        // XXX perf: I think we might be able to shuffle this code to\n+        // only destruct when we need to.\n+\n+        rtdebug!(\"a task was queued on: %u\", self.sched_id());\n+\n+        let this = self;\n+\n+        // We push the task onto our local queue clone.\n+        this.work_queue.push(task);\n+        this.event_loop.callback(Scheduler::run_sched_once);\n+\n+        // We've made work available. Notify a\n+        // sleeping scheduler.\n+\n+        // XXX: perf. Check for a sleeper without\n+        // synchronizing memory.  It's not critical\n+        // that we always find it.\n+\n+        // XXX: perf. If there's a sleeper then we\n+        // might as well just send it the task\n+        // directly instead of pushing it to the\n+        // queue. That is essentially the intent here\n+        // and it is less work.\n+        match this.sleeper_list.pop() {\n+            Some(handle) => {\n+                let mut handle = handle;\n+                handle.send(Wake)\n+            }\n+            None => { (/* pass */) }\n+        };\n     }\n \n     // * Scheduler-context operations\n \n-    pub fn resume_task_from_queue(~self) {\n+    fn interpret_message_queue(~self) -> bool {\n         assert!(!self.in_task_context());\n \n-        rtdebug!(\"looking in work queue for task to schedule\");\n+        rtdebug!(\"looking for scheduler messages\");\n \n         let mut this = self;\n-        match this.work_queue.pop() {\n-            Some(task) => {\n-                rtdebug!(\"resuming task from work queue\");\n+        match this.message_queue.pop() {\n+            Some(PinnedTask(task)) => {\n+                rtdebug!(\"recv BiasedTask message in sched: %u\",\n+                         this.sched_id());\n+                let mut task = task;\n+                task.task.home = Some(Sched(this.make_handle()));\n                 this.resume_task_immediately(task);\n+                return true;\n+            }\n+\n+            Some(Wake) => {\n+                rtdebug!(\"recv Wake message\");\n+                this.sleepy = false;\n+                Local::put(this);\n+                return true;\n+            }\n+            Some(Shutdown) => {\n+                rtdebug!(\"recv Shutdown message\");\n+                if this.sleepy {\n+                    // There may be an outstanding handle on the\n+                    // sleeper list.  Pop them all to make sure that's\n+                    // not the case.\n+                    loop {\n+                        match this.sleeper_list.pop() {\n+                            Some(handle) => {\n+                                let mut handle = handle;\n+                                handle.send(Wake);\n+                            }\n+                            None => break\n+                        }\n+                    }\n+                }\n+                // No more sleeping. After there are no outstanding\n+                // event loop references we will shut down.\n+                this.no_sleep = true;\n+                this.sleepy = false;\n+                Local::put(this);\n+                return true;\n             }\n             None => {\n-                rtdebug!(\"no tasks in queue\");\n                 Local::put(this);\n+                return false;\n+            }\n+        }\n+    }\n+\n+    /// Given an input Coroutine sends it back to its home scheduler.\n+    fn send_task_home(task: ~Coroutine) {\n+        let mut task = task;\n+        let mut home = task.task.home.swap_unwrap();\n+        match home {\n+            Sched(ref mut home_handle) => {\n+                home_handle.send(PinnedTask(task));\n+            }\n+            AnySched => {\n+                rtabort!(\"error: cannot send anysched task home\");\n             }\n         }\n     }\n \n+    // Resume a task from the queue - but also take into account that\n+    // it might not belong here.\n+    fn resume_task_from_queue(~self) -> bool {\n+        assert!(!self.in_task_context());\n+\n+        rtdebug!(\"looking in work queue for task to schedule\");\n+        let mut this = self;\n+\n+        // The borrow checker imposes the possibly absurd requirement\n+        // that we split this into two match expressions. This is due\n+        // to the inspection of the internal bits of task, as that\n+        // can't be in scope when we act on task.\n+        match this.work_queue.pop() {\n+            Some(task) => {\n+                let action_id = {\n+                    let home = &task.task.home;\n+                    match home {\n+                        &Some(Sched(ref home_handle))\n+                        if home_handle.sched_id != this.sched_id() => {\n+                            SendHome\n+                        }\n+                        &Some(AnySched) if this.run_anything => {\n+                            ResumeNow\n+                        }\n+                        &Some(AnySched) => {\n+                            Requeue\n+                        }\n+                        &Some(Sched(_)) => {\n+                            ResumeNow\n+                        }\n+                        &None => {\n+                            Homeless\n+                        }\n+                    }\n+                };\n+\n+                match action_id {\n+                    SendHome => {\n+                        rtdebug!(\"sending task home\");\n+                        Scheduler::send_task_home(task);\n+                        Local::put(this);\n+                        return false;\n+                    }\n+                    ResumeNow => {\n+                        rtdebug!(\"resuming now\");\n+                        this.resume_task_immediately(task);\n+                        return true;\n+                    }\n+                    Requeue => {\n+                        rtdebug!(\"re-queueing\")\n+                        this.enqueue_task(task);\n+                        Local::put(this);\n+                        return false;\n+                    }\n+                    Homeless => {\n+                        rtabort!(\"task home was None!\");\n+                    }\n+                }\n+            }\n+\n+            None => {\n+               rtdebug!(\"no tasks in queue\");\n+               Local::put(this);\n+               return false;\n+           }\n+        }\n+    }\n+\n     // * Task-context operations\n \n     /// Called by a running task to end execution, after which it will\n@@ -145,35 +439,40 @@ impl Scheduler {\n \n         rtdebug!(\"ending running task\");\n \n-        do self.deschedule_running_task_and_then |dead_task| {\n+        do self.deschedule_running_task_and_then |sched, dead_task| {\n             let dead_task = Cell::new(dead_task);\n-            do Local::borrow::<Scheduler> |sched| {\n-                dead_task.take().recycle(&mut sched.stack_pool);\n-            }\n+            dead_task.take().recycle(&mut sched.stack_pool);\n         }\n \n-        abort!(\"control reached end of task\");\n+        rtabort!(\"control reached end of task\");\n     }\n \n-    pub fn schedule_new_task(~self, task: ~Coroutine) {\n+    pub fn schedule_task(~self, task: ~Coroutine) {\n         assert!(self.in_task_context());\n \n-        do self.switch_running_tasks_and_then(task) |last_task| {\n-            let last_task = Cell::new(last_task);\n-            do Local::borrow::<Scheduler> |sched| {\n-                sched.enqueue_task(last_task.take());\n-            }\n-        }\n-    }\n+        // is the task home?\n+        let is_home = task.is_home_no_tls(&self);\n \n-    pub fn schedule_task(~self, task: ~Coroutine) {\n-        assert!(self.in_task_context());\n+        // does the task have a home?\n+        let homed = task.homed();\n+\n+        let mut this = self;\n \n-        do self.switch_running_tasks_and_then(task) |last_task| {\n-            let last_task = Cell::new(last_task);\n-            do Local::borrow::<Scheduler> |sched| {\n+        if is_home || (!homed && this.run_anything) {\n+            // here we know we are home, execute now OR we know we\n+            // aren't homed, and that this sched doesn't care\n+            do this.switch_running_tasks_and_then(task) |sched, last_task| {\n+                let last_task = Cell::new(last_task);\n                 sched.enqueue_task(last_task.take());\n             }\n+        } else if !homed && !this.run_anything {\n+            // the task isn't homed, but it can't be run here\n+            this.enqueue_task(task);\n+            Local::put(this);\n+        } else {\n+            // task isn't home, so don't run it here, send it home\n+            Scheduler::send_task_home(task);\n+            Local::put(this);\n         }\n     }\n \n@@ -184,6 +483,7 @@ impl Scheduler {\n         assert!(!this.in_task_context());\n \n         rtdebug!(\"scheduling a task\");\n+        this.metrics.context_switches_sched_to_task += 1;\n \n         // Store the task in the scheduler so it can be grabbed later\n         this.current_task = Some(task);\n@@ -217,15 +517,21 @@ impl Scheduler {\n     /// The closure here is a *stack* closure that lives in the\n     /// running task.  It gets transmuted to the scheduler's lifetime\n     /// and called while the task is blocked.\n-    pub fn deschedule_running_task_and_then(~self, f: &fn(~Coroutine)) {\n+    ///\n+    /// This passes a Scheduler pointer to the fn after the context switch\n+    /// in order to prevent that fn from performing further scheduling operations.\n+    /// Doing further scheduling could easily result in infinite recursion.\n+    pub fn deschedule_running_task_and_then(~self, f: &fn(&mut Scheduler, ~Coroutine)) {\n         let mut this = self;\n         assert!(this.in_task_context());\n \n         rtdebug!(\"blocking task\");\n+        this.metrics.context_switches_task_to_sched += 1;\n \n         unsafe {\n             let blocked_task = this.current_task.swap_unwrap();\n-            let f_fake_region = transmute::<&fn(~Coroutine), &fn(~Coroutine)>(f);\n+            let f_fake_region = transmute::<&fn(&mut Scheduler, ~Coroutine),\n+                                            &fn(&mut Scheduler, ~Coroutine)>(f);\n             let f_opaque = ClosureConverter::from_fn(f_fake_region);\n             this.enqueue_cleanup_job(GiveTask(blocked_task, f_opaque));\n         }\n@@ -247,16 +553,19 @@ impl Scheduler {\n     /// Switch directly to another task, without going through the scheduler.\n     /// You would want to think hard about doing this, e.g. if there are\n     /// pending I/O events it would be a bad idea.\n-    pub fn switch_running_tasks_and_then(~self,\n-                                         next_task: ~Coroutine,\n-                                         f: &fn(~Coroutine)) {\n+    pub fn switch_running_tasks_and_then(~self, next_task: ~Coroutine,\n+                                         f: &fn(&mut Scheduler, ~Coroutine)) {\n         let mut this = self;\n         assert!(this.in_task_context());\n \n         rtdebug!(\"switching tasks\");\n+        this.metrics.context_switches_task_to_task += 1;\n \n         let old_running_task = this.current_task.swap_unwrap();\n-        let f_fake_region = unsafe { transmute::<&fn(~Coroutine), &fn(~Coroutine)>(f) };\n+        let f_fake_region = unsafe {\n+            transmute::<&fn(&mut Scheduler, ~Coroutine),\n+                        &fn(&mut Scheduler, ~Coroutine)>(f)\n+        };\n         let f_opaque = ClosureConverter::from_fn(f_fake_region);\n         this.enqueue_cleanup_job(GiveTask(old_running_task, f_opaque));\n         this.current_task = Some(next_task);\n@@ -293,7 +602,7 @@ impl Scheduler {\n         let cleanup_job = self.cleanup_job.swap_unwrap();\n         match cleanup_job {\n             DoNothing => { }\n-            GiveTask(task, f) => (f.to_fn())(task)\n+            GiveTask(task, f) => (f.to_fn())(self, task)\n         }\n     }\n \n@@ -337,40 +646,127 @@ impl Scheduler {\n     }\n }\n \n-static MIN_STACK_SIZE: uint = 10000000; // XXX: Too much stack\n+// The cases for the below function.\n+enum ResumeAction {\n+    SendHome,\n+    Requeue,\n+    ResumeNow,\n+    Homeless\n+}\n \n-pub struct Coroutine {\n-    /// The segment of stack on which the task is currently running or,\n-    /// if the task is blocked, on which the task will resume execution\n-    priv current_stack_segment: StackSegment,\n-    /// These are always valid when the task is not running, unless\n-    /// the task is dead\n-    priv saved_context: Context,\n-    /// The heap, GC, unwinding, local storage, logging\n-    task: ~Task\n+impl SchedHandle {\n+    pub fn send(&mut self, msg: SchedMessage) {\n+        self.queue.push(msg);\n+        self.remote.fire();\n+    }\n }\n \n impl Coroutine {\n-    pub fn new(stack_pool: &mut StackPool, start: ~fn()) -> Coroutine {\n-        Coroutine::with_task(stack_pool, ~Task::new(), start)\n+\n+    /// This function checks that a coroutine is running \"home\".\n+    pub fn is_home(&self) -> bool {\n+        rtdebug!(\"checking if coroutine is home\");\n+        do Local::borrow::<Scheduler,bool> |sched| {\n+            match self.task.home {\n+                Some(AnySched) => { false }\n+                Some(Sched(SchedHandle { sched_id: ref id, _ })) => {\n+                    *id == sched.sched_id()\n+                }\n+                None => { rtabort!(\"error: homeless task!\"); }\n+            }\n+        }\n     }\n \n-    pub fn with_task(stack_pool: &mut StackPool,\n-                     task: ~Task,\n-                     start: ~fn()) -> Coroutine {\n+    /// Without access to self, but with access to the \"expected home\n+    /// id\", see if we are home.\n+    fn is_home_using_id(id: uint) -> bool {\n+        rtdebug!(\"checking if coroutine is home using id\");\n+        do Local::borrow::<Scheduler,bool> |sched| {\n+            if sched.sched_id() == id {\n+                true\n+            } else {\n+                false\n+            }\n+        }\n+    }\n+\n+    /// Check if this coroutine has a home\n+    fn homed(&self) -> bool {\n+        rtdebug!(\"checking if this coroutine has a home\");\n+        match self.task.home {\n+            Some(AnySched) => { false }\n+            Some(Sched(_)) => { true }\n+            None => { rtabort!(\"error: homeless task!\");\n+                    }\n+        }\n+    }\n+\n+    /// A version of is_home that does not need to use TLS, it instead\n+    /// takes local scheduler as a parameter.\n+    fn is_home_no_tls(&self, sched: &~Scheduler) -> bool {\n+        rtdebug!(\"checking if coroutine is home without tls\");\n+        match self.task.home {\n+            Some(AnySched) => { true }\n+            Some(Sched(SchedHandle { sched_id: ref id, _})) => {\n+                *id == sched.sched_id()\n+            }\n+            None => { rtabort!(\"error: homeless task!\"); }\n+        }\n+    }\n+\n+    /// Check TLS for the scheduler to see if we are on a special\n+    /// scheduler.\n+    pub fn on_special() -> bool {\n+        rtdebug!(\"checking if coroutine is executing on special sched\");\n+        do Local::borrow::<Scheduler,bool>() |sched| {\n+            !sched.run_anything\n+        }\n+    }\n+\n+    // Created new variants of \"new\" that takes a home scheduler\n+    // parameter. The original with_task now calls with_task_homed\n+    // using the AnySched paramter.\n+\n+    pub fn new_homed(stack_pool: &mut StackPool, home: SchedHome, start: ~fn()) -> Coroutine {\n+        Coroutine::with_task_homed(stack_pool, ~Task::new_root(), start, home)\n+    }\n+\n+    pub fn new_root(stack_pool: &mut StackPool, start: ~fn()) -> Coroutine {\n+        Coroutine::with_task(stack_pool, ~Task::new_root(), start)\n+    }\n+\n+    pub fn with_task_homed(stack_pool: &mut StackPool,\n+                           task: ~Task,\n+                           start: ~fn(),\n+                           home: SchedHome) -> Coroutine {\n+\n+        static MIN_STACK_SIZE: uint = 1000000; // XXX: Too much stack\n+\n         let start = Coroutine::build_start_wrapper(start);\n         let mut stack = stack_pool.take_segment(MIN_STACK_SIZE);\n         // NB: Context holds a pointer to that ~fn\n         let initial_context = Context::new(start, &mut stack);\n-        return Coroutine {\n+        let mut crt = Coroutine {\n             current_stack_segment: stack,\n             saved_context: initial_context,\n-            task: task\n+            task: task,\n         };\n+        crt.task.home = Some(home);\n+        return crt;\n+    }\n+\n+    pub fn with_task(stack_pool: &mut StackPool,\n+                 task: ~Task,\n+                 start: ~fn()) -> Coroutine {\n+        Coroutine::with_task_homed(stack_pool,\n+                                   task,\n+                                   start,\n+                                   AnySched)\n     }\n \n     fn build_start_wrapper(start: ~fn()) -> ~fn() {\n         // XXX: The old code didn't have this extra allocation\n+        let start_cell = Cell::new(start);\n         let wrapper: ~fn() = || {\n             // This is the first code to execute after the initial\n             // context switch to the task. The previous context may\n@@ -381,8 +777,23 @@ impl Coroutine {\n \n                 let sched = Local::unsafe_borrow::<Scheduler>();\n                 let task = (*sched).current_task.get_mut_ref();\n-                // FIXME #6141: shouldn't neet to put `start()` in another closure\n-                task.task.run(||start());\n+                // FIXME #6141: shouldn't neet to put `start()` in\n+                // another closure\n+                let start_cell = Cell::new(start_cell.take());\n+                do task.task.run {\n+                    // N.B. Removing `start` from the start wrapper\n+                    // closure by emptying a cell is critical for\n+                    // correctness. The ~Task pointer, and in turn the\n+                    // closure used to initialize the first call\n+                    // frame, is destroyed in scheduler context, not\n+                    // task context.  So any captured closures must\n+                    // not contain user-definable dtors that expect to\n+                    // be in task context. By moving `start` out of\n+                    // the closure, all the user code goes out of\n+                    // scope while the task is still running.\n+                    let start = start_cell.take();\n+                    start();\n+                };\n             }\n \n             let sched = Local::take::<Scheduler>();\n@@ -401,25 +812,337 @@ impl Coroutine {\n     }\n }\n \n+// XXX: Some hacks to put a &fn in Scheduler without borrowck\n+// complaining\n+type UnsafeTaskReceiver = sys::Closure;\n+trait ClosureConverter {\n+    fn from_fn(&fn(&mut Scheduler, ~Coroutine)) -> Self;\n+    fn to_fn(self) -> &fn(&mut Scheduler, ~Coroutine);\n+}\n+impl ClosureConverter for UnsafeTaskReceiver {\n+    fn from_fn(f: &fn(&mut Scheduler, ~Coroutine)) -> UnsafeTaskReceiver { unsafe { transmute(f) } }\n+    fn to_fn(self) -> &fn(&mut Scheduler, ~Coroutine) { unsafe { transmute(self) } }\n+}\n+\n #[cfg(test)]\n mod test {\n     use int;\n     use cell::Cell;\n-    use rt::uv::uvio::UvEventLoop;\n+    use iterator::IteratorUtil;\n     use unstable::run_in_bare_thread;\n     use task::spawn;\n     use rt::local::Local;\n     use rt::test::*;\n     use super::*;\n+    use rt::thread::Thread;\n+    use ptr::to_uint;\n+    use vec::MutableVector;\n+\n+    // Confirm that a sched_id actually is the uint form of the\n+    // pointer to the scheduler struct.\n+\n+    #[test]\n+    fn simple_sched_id_test() {\n+        do run_in_bare_thread {\n+            let sched = ~new_test_uv_sched();\n+            assert!(to_uint(sched) == sched.sched_id());\n+        }\n+    }\n+\n+    // Compare two scheduler ids that are different, this should never\n+    // fail but may catch a mistake someday.\n+\n+    #[test]\n+    fn compare_sched_id_test() {\n+        do run_in_bare_thread {\n+            let sched_one = ~new_test_uv_sched();\n+            let sched_two = ~new_test_uv_sched();\n+            assert!(sched_one.sched_id() != sched_two.sched_id());\n+        }\n+    }\n+\n+    // A simple test to check if a homed task run on a single\n+    // scheduler ends up executing while home.\n+\n+    #[test]\n+    fn test_home_sched() {\n+        do run_in_bare_thread {\n+            let mut task_ran = false;\n+            let task_ran_ptr: *mut bool = &mut task_ran;\n+            let mut sched = ~new_test_uv_sched();\n+\n+            let sched_handle = sched.make_handle();\n+            let sched_id = sched.sched_id();\n+\n+            let task = ~do Coroutine::new_homed(&mut sched.stack_pool,\n+                                                Sched(sched_handle)) {\n+                unsafe { *task_ran_ptr = true };\n+                let sched = Local::take::<Scheduler>();\n+                assert!(sched.sched_id() == sched_id);\n+                Local::put::<Scheduler>(sched);\n+            };\n+            sched.enqueue_task(task);\n+            sched.run();\n+            assert!(task_ran);\n+        }\n+    }\n+\n+    // A test for each state of schedule_task\n+\n+    #[test]\n+    fn test_schedule_home_states() {\n+\n+        use rt::uv::uvio::UvEventLoop;\n+        use rt::sched::Shutdown;\n+        use rt::sleeper_list::SleeperList;\n+        use rt::work_queue::WorkQueue;\n+\n+        do run_in_bare_thread {\n+//            let nthreads = 2;\n+\n+            let sleepers = SleeperList::new();\n+            let work_queue = WorkQueue::new();\n+\n+            // our normal scheduler\n+            let mut normal_sched = ~Scheduler::new(\n+                ~UvEventLoop::new(),\n+                work_queue.clone(),\n+                sleepers.clone());\n+\n+            let normal_handle = Cell::new(normal_sched.make_handle());\n+\n+            // our special scheduler\n+            let mut special_sched = ~Scheduler::new_special(\n+                ~UvEventLoop::new(),\n+                work_queue.clone(),\n+                sleepers.clone(),\n+                true);\n+\n+            let special_handle = Cell::new(special_sched.make_handle());\n+            let special_handle2 = Cell::new(special_sched.make_handle());\n+            let special_id = special_sched.sched_id();\n+            let t1_handle = special_sched.make_handle();\n+            let t4_handle = special_sched.make_handle();\n+\n+            let t1f = ~do Coroutine::new_homed(&mut special_sched.stack_pool,\n+                                            Sched(t1_handle)) {\n+                let is_home = Coroutine::is_home_using_id(special_id);\n+                rtdebug!(\"t1 should be home: %b\", is_home);\n+                assert!(is_home);\n+            };\n+            let t1f = Cell::new(t1f);\n+\n+            let t2f = ~do Coroutine::new_root(&mut normal_sched.stack_pool) {\n+                let on_special = Coroutine::on_special();\n+                rtdebug!(\"t2 should not be on special: %b\", on_special);\n+                assert!(!on_special);\n+            };\n+            let t2f = Cell::new(t2f);\n+\n+            let t3f = ~do Coroutine::new_root(&mut normal_sched.stack_pool) {\n+                // not on special\n+                let on_special = Coroutine::on_special();\n+                rtdebug!(\"t3 should not be on special: %b\", on_special);\n+                assert!(!on_special);\n+            };\n+            let t3f = Cell::new(t3f);\n+\n+            let t4f = ~do Coroutine::new_homed(&mut special_sched.stack_pool,\n+                                            Sched(t4_handle)) {\n+                // is home\n+                let home = Coroutine::is_home_using_id(special_id);\n+                rtdebug!(\"t4 should be home: %b\", home);\n+                assert!(home);\n+            };\n+            let t4f = Cell::new(t4f);\n+\n+            // we have four tests, make them as closures\n+            let t1: ~fn() = || {\n+                // task is home on special\n+                let task = t1f.take();\n+                let sched = Local::take::<Scheduler>();\n+                sched.schedule_task(task);\n+            };\n+            let t2: ~fn() = || {\n+                // not homed, task doesn't care\n+                let task = t2f.take();\n+                let sched = Local::take::<Scheduler>();\n+                sched.schedule_task(task);\n+            };\n+            let t3: ~fn() = || {\n+                // task not homed, must leave\n+                let task = t3f.take();\n+                let sched = Local::take::<Scheduler>();\n+                sched.schedule_task(task);\n+            };\n+            let t4: ~fn() = || {\n+                // task not home, send home\n+                let task = t4f.take();\n+                let sched = Local::take::<Scheduler>();\n+                sched.schedule_task(task);\n+            };\n+\n+            let t1 = Cell::new(t1);\n+            let t2 = Cell::new(t2);\n+            let t3 = Cell::new(t3);\n+            let t4 = Cell::new(t4);\n+\n+            // build a main task that runs our four tests\n+            let main_task = ~do Coroutine::new_root(&mut normal_sched.stack_pool) {\n+                // the two tasks that require a normal start location\n+                t2.take()();\n+                t4.take()();\n+                normal_handle.take().send(Shutdown);\n+                special_handle.take().send(Shutdown);\n+            };\n+\n+            // task to run the two \"special start\" tests\n+            let special_task = ~do Coroutine::new_homed(\n+                &mut special_sched.stack_pool,\n+                Sched(special_handle2.take())) {\n+                t1.take()();\n+                t3.take()();\n+            };\n+\n+            // enqueue the main tasks\n+            normal_sched.enqueue_task(special_task);\n+            normal_sched.enqueue_task(main_task);\n+\n+            let nsched_cell = Cell::new(normal_sched);\n+            let normal_thread = do Thread::start {\n+                let sched = nsched_cell.take();\n+                sched.run();\n+            };\n+\n+            let ssched_cell = Cell::new(special_sched);\n+            let special_thread = do Thread::start {\n+                let sched = ssched_cell.take();\n+                sched.run();\n+            };\n+\n+            // wait for the end\n+            let _thread1 = normal_thread;\n+            let _thread2 = special_thread;\n+\n+        }\n+    }\n+\n+    // The following test is a bit of a mess, but it trys to do\n+    // something tricky so I'm not sure how to get around this in the\n+    // short term.\n+\n+    // A number of schedulers are created, and then a task is created\n+    // and assigned a home scheduler. It is then \"started\" on a\n+    // different scheduler. The scheduler it is started on should\n+    // observe that the task is not home, and send it home.\n+\n+    // This test is light in that it does very little.\n+\n+    #[test]\n+    fn test_transfer_task_home() {\n+\n+        use rt::uv::uvio::UvEventLoop;\n+        use rt::sched::Shutdown;\n+        use rt::sleeper_list::SleeperList;\n+        use rt::work_queue::WorkQueue;\n+        use uint;\n+        use container::Container;\n+        use vec::OwnedVector;\n+\n+        do run_in_bare_thread {\n+\n+            static N: uint = 8;\n+\n+            let sleepers = SleeperList::new();\n+            let work_queue = WorkQueue::new();\n+\n+            let mut handles = ~[];\n+            let mut scheds = ~[];\n+\n+            for uint::range(0, N) |_| {\n+                let loop_ = ~UvEventLoop::new();\n+                let mut sched = ~Scheduler::new(loop_,\n+                                                work_queue.clone(),\n+                                                sleepers.clone());\n+                let handle = sched.make_handle();\n+                rtdebug!(\"sched id: %u\", handle.sched_id);\n+                handles.push(handle);\n+                scheds.push(sched);\n+            };\n+\n+            let handles = Cell::new(handles);\n+\n+            let home_handle = scheds[6].make_handle();\n+            let home_id = home_handle.sched_id;\n+            let home = Sched(home_handle);\n+\n+            let main_task = ~do Coroutine::new_homed(&mut scheds[1].stack_pool, home) {\n+\n+                // Here we check if the task is running on its home.\n+                let sched = Local::take::<Scheduler>();\n+                rtdebug!(\"run location scheduler id: %u, home: %u\",\n+                         sched.sched_id(),\n+                         home_id);\n+                assert!(sched.sched_id() == home_id);\n+                Local::put::<Scheduler>(sched);\n+\n+                let mut handles = handles.take();\n+                for handles.mut_iter().advance |handle| {\n+                    handle.send(Shutdown);\n+                }\n+            };\n+\n+            scheds[0].enqueue_task(main_task);\n+\n+            let mut threads = ~[];\n+\n+            while !scheds.is_empty() {\n+                let sched = scheds.pop();\n+                let sched_cell = Cell::new(sched);\n+                let thread = do Thread::start {\n+                    let sched = sched_cell.take();\n+                    sched.run();\n+                };\n+                threads.push(thread);\n+            }\n+\n+            let _threads = threads;\n+        }\n+    }\n+\n+    // Do it a lot\n+\n+    #[test]\n+    fn test_stress_schedule_task_states() {\n+        let n = stress_factor() * 120;\n+        for int::range(0,n as int) |_| {\n+            test_schedule_home_states();\n+        }\n+    }\n+\n+    // The goal is that this is the high-stress test for making sure\n+    // homing is working. It allocates RUST_RT_STRESS tasks that\n+    // do nothing but assert that they are home at execution\n+    // time. These tasks are queued to random schedulers, so sometimes\n+    // they are home and sometimes not. It also runs RUST_RT_STRESS\n+    // times.\n+\n+    #[test]\n+    fn test_stress_homed_tasks() {\n+        let n = stress_factor();\n+        for int::range(0,n as int) |_| {\n+            run_in_mt_newsched_task_random_homed();\n+        }\n+    }\n \n     #[test]\n     fn test_simple_scheduling() {\n         do run_in_bare_thread {\n             let mut task_ran = false;\n             let task_ran_ptr: *mut bool = &mut task_ran;\n \n-            let mut sched = ~UvEventLoop::new_scheduler();\n-            let task = ~do Coroutine::new(&mut sched.stack_pool) {\n+            let mut sched = ~new_test_uv_sched();\n+            let task = ~do Coroutine::new_root(&mut sched.stack_pool) {\n                 unsafe { *task_ran_ptr = true; }\n             };\n             sched.enqueue_task(task);\n@@ -435,9 +1158,9 @@ mod test {\n             let mut task_count = 0;\n             let task_count_ptr: *mut int = &mut task_count;\n \n-            let mut sched = ~UvEventLoop::new_scheduler();\n+            let mut sched = ~new_test_uv_sched();\n             for int::range(0, total) |_| {\n-                let task = ~do Coroutine::new(&mut sched.stack_pool) {\n+                let task = ~do Coroutine::new_root(&mut sched.stack_pool) {\n                     unsafe { *task_count_ptr = *task_count_ptr + 1; }\n                 };\n                 sched.enqueue_task(task);\n@@ -453,19 +1176,17 @@ mod test {\n             let mut count = 0;\n             let count_ptr: *mut int = &mut count;\n \n-            let mut sched = ~UvEventLoop::new_scheduler();\n-            let task1 = ~do Coroutine::new(&mut sched.stack_pool) {\n+            let mut sched = ~new_test_uv_sched();\n+            let task1 = ~do Coroutine::new_root(&mut sched.stack_pool) {\n                 unsafe { *count_ptr = *count_ptr + 1; }\n                 let mut sched = Local::take::<Scheduler>();\n-                let task2 = ~do Coroutine::new(&mut sched.stack_pool) {\n+                let task2 = ~do Coroutine::new_root(&mut sched.stack_pool) {\n                     unsafe { *count_ptr = *count_ptr + 1; }\n                 };\n                 // Context switch directly to the new task\n-                do sched.switch_running_tasks_and_then(task2) |task1| {\n+                do sched.switch_running_tasks_and_then(task2) |sched, task1| {\n                     let task1 = Cell::new(task1);\n-                    do Local::borrow::<Scheduler> |sched| {\n-                        sched.enqueue_task(task1.take());\n-                    }\n+                    sched.enqueue_task(task1.take());\n                 }\n                 unsafe { *count_ptr = *count_ptr + 1; }\n             };\n@@ -482,9 +1203,9 @@ mod test {\n             let mut count = 0;\n             let count_ptr: *mut int = &mut count;\n \n-            let mut sched = ~UvEventLoop::new_scheduler();\n+            let mut sched = ~new_test_uv_sched();\n \n-            let start_task = ~do Coroutine::new(&mut sched.stack_pool) {\n+            let start_task = ~do Coroutine::new_root(&mut sched.stack_pool) {\n                 run_task(count_ptr);\n             };\n             sched.enqueue_task(start_task);\n@@ -493,8 +1214,8 @@ mod test {\n             assert_eq!(count, MAX);\n \n             fn run_task(count_ptr: *mut int) {\n-                do Local::borrow::<Scheduler> |sched| {\n-                    let task = ~do Coroutine::new(&mut sched.stack_pool) {\n+                do Local::borrow::<Scheduler, ()> |sched| {\n+                    let task = ~do Coroutine::new_root(&mut sched.stack_pool) {\n                         unsafe {\n                             *count_ptr = *count_ptr + 1;\n                             if *count_ptr != MAX {\n@@ -511,16 +1232,14 @@ mod test {\n     #[test]\n     fn test_block_task() {\n         do run_in_bare_thread {\n-            let mut sched = ~UvEventLoop::new_scheduler();\n-            let task = ~do Coroutine::new(&mut sched.stack_pool) {\n+            let mut sched = ~new_test_uv_sched();\n+            let task = ~do Coroutine::new_root(&mut sched.stack_pool) {\n                 let sched = Local::take::<Scheduler>();\n                 assert!(sched.in_task_context());\n-                do sched.deschedule_running_task_and_then() |task| {\n+                do sched.deschedule_running_task_and_then() |sched, task| {\n                     let task = Cell::new(task);\n-                    do Local::borrow::<Scheduler> |sched| {\n-                        assert!(!sched.in_task_context());\n-                        sched.enqueue_task(task.take());\n-                    }\n+                    assert!(!sched.in_task_context());\n+                    sched.enqueue_task(task.take());\n                 }\n             };\n             sched.enqueue_task(task);\n@@ -537,18 +1256,157 @@ mod test {\n         do run_in_newsched_task {\n             do spawn {\n                 let sched = Local::take::<Scheduler>();\n-                do sched.deschedule_running_task_and_then |task| {\n-                    let mut sched = Local::take::<Scheduler>();\n+                do sched.deschedule_running_task_and_then |sched, task| {\n                     let task = Cell::new(task);\n                     do sched.event_loop.callback_ms(10) {\n                         rtdebug!(\"in callback\");\n                         let mut sched = Local::take::<Scheduler>();\n                         sched.enqueue_task(task.take());\n                         Local::put(sched);\n                     }\n-                    Local::put(sched);\n                 }\n             }\n         }\n     }\n+\n+    #[test]\n+    fn handle() {\n+        use rt::comm::*;\n+\n+        do run_in_bare_thread {\n+            let (port, chan) = oneshot::<()>();\n+            let port_cell = Cell::new(port);\n+            let chan_cell = Cell::new(chan);\n+            let mut sched1 = ~new_test_uv_sched();\n+            let handle1 = sched1.make_handle();\n+            let handle1_cell = Cell::new(handle1);\n+            let task1 = ~do Coroutine::new_root(&mut sched1.stack_pool) {\n+                chan_cell.take().send(());\n+            };\n+            sched1.enqueue_task(task1);\n+\n+            let mut sched2 = ~new_test_uv_sched();\n+            let task2 = ~do Coroutine::new_root(&mut sched2.stack_pool) {\n+                port_cell.take().recv();\n+                // Release the other scheduler's handle so it can exit\n+                handle1_cell.take();\n+            };\n+            sched2.enqueue_task(task2);\n+\n+            let sched1_cell = Cell::new(sched1);\n+            let _thread1 = do Thread::start {\n+                let sched1 = sched1_cell.take();\n+                sched1.run();\n+            };\n+\n+            let sched2_cell = Cell::new(sched2);\n+            let _thread2 = do Thread::start {\n+                let sched2 = sched2_cell.take();\n+                sched2.run();\n+            };\n+        }\n+    }\n+\n+    #[test]\n+    fn multithreading() {\n+        use rt::comm::*;\n+        use iter::Times;\n+        use vec::OwnedVector;\n+        use container::Container;\n+\n+        do run_in_mt_newsched_task {\n+            let mut ports = ~[];\n+            for 10.times {\n+                let (port, chan) = oneshot();\n+                let chan_cell = Cell::new(chan);\n+                do spawntask_later {\n+                    chan_cell.take().send(());\n+                }\n+                ports.push(port);\n+            }\n+\n+            while !ports.is_empty() {\n+                ports.pop().recv();\n+            }\n+        }\n+    }\n+\n+    #[test]\n+    fn thread_ring() {\n+        use rt::comm::*;\n+        use comm::{GenericPort, GenericChan};\n+\n+        do run_in_mt_newsched_task {\n+            let (end_port, end_chan) = oneshot();\n+\n+            let n_tasks = 10;\n+            let token = 2000;\n+\n+            let mut (p, ch1) = stream();\n+            ch1.send((token, end_chan));\n+            let mut i = 2;\n+            while i <= n_tasks {\n+                let (next_p, ch) = stream();\n+                let imm_i = i;\n+                let imm_p = p;\n+                do spawntask_random {\n+                    roundtrip(imm_i, n_tasks, &imm_p, &ch);\n+                };\n+                p = next_p;\n+                i += 1;\n+            }\n+            let imm_p = p;\n+            let imm_ch = ch1;\n+            do spawntask_random {\n+                roundtrip(1, n_tasks, &imm_p, &imm_ch);\n+            }\n+\n+            end_port.recv();\n+        }\n+\n+        fn roundtrip(id: int, n_tasks: int,\n+                     p: &Port<(int, ChanOne<()>)>, ch: &Chan<(int, ChanOne<()>)>) {\n+            while (true) {\n+                match p.recv() {\n+                    (1, end_chan) => {\n+                        debug!(\"%d\\n\", id);\n+                        end_chan.send(());\n+                        return;\n+                    }\n+                    (token, end_chan) => {\n+                        debug!(\"thread: %d   got token: %d\", id, token);\n+                        ch.send((token - 1, end_chan));\n+                        if token <= n_tasks {\n+                            return;\n+                        }\n+                    }\n+                }\n+            }\n+        }\n+\n+    }\n+\n+    #[test]\n+    fn start_closure_dtor() {\n+        use ops::Drop;\n+\n+        // Regression test that the `start` task entrypoint can\n+        // contain dtors that use task resources\n+        do run_in_newsched_task {\n+            struct S { field: () }\n+\n+            impl Drop for S {\n+                fn finalize(&self) {\n+                    let _foo = @0;\n+                }\n+            }\n+\n+            let s = S { field: () };\n+\n+            do spawntask {\n+                let _ss = &s;\n+            }\n+        }\n+    }\n+\n }"}, {"sha": "3d6e9ef5635e5ac48908477d40203ac7d838bf8f", "filename": "src/libstd/rt/sleeper_list.rs", "status": "added", "additions": 59, "deletions": 0, "changes": 59, "blob_url": "https://github.com/rust-lang/rust/blob/357f087786cbd6516a38aff800cf9334bc5b85c5/src%2Flibstd%2Frt%2Fsleeper_list.rs", "raw_url": "https://github.com/rust-lang/rust/raw/357f087786cbd6516a38aff800cf9334bc5b85c5/src%2Flibstd%2Frt%2Fsleeper_list.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibstd%2Frt%2Fsleeper_list.rs?ref=357f087786cbd6516a38aff800cf9334bc5b85c5", "patch": "@@ -0,0 +1,59 @@\n+// Copyright 2013 The Rust Project Developers. See the COPYRIGHT\n+// file at the top-level directory of this distribution and at\n+// http://rust-lang.org/COPYRIGHT.\n+//\n+// Licensed under the Apache License, Version 2.0 <LICENSE-APACHE or\n+// http://www.apache.org/licenses/LICENSE-2.0> or the MIT license\n+// <LICENSE-MIT or http://opensource.org/licenses/MIT>, at your\n+// option. This file may not be copied, modified, or distributed\n+// except according to those terms.\n+\n+//! Maintains a shared list of sleeping schedulers. Schedulers\n+//! use this to wake each other up.\n+\n+use container::Container;\n+use vec::OwnedVector;\n+use option::{Option, Some, None};\n+use cell::Cell;\n+use unstable::sync::{Exclusive, exclusive};\n+use rt::sched::SchedHandle;\n+use clone::Clone;\n+\n+pub struct SleeperList {\n+    priv stack: ~Exclusive<~[SchedHandle]>\n+}\n+\n+impl SleeperList {\n+    pub fn new() -> SleeperList {\n+        SleeperList {\n+            stack: ~exclusive(~[])\n+        }\n+    }\n+\n+    pub fn push(&mut self, handle: SchedHandle) {\n+        let handle = Cell::new(handle);\n+        unsafe {\n+            self.stack.with(|s| s.push(handle.take()));\n+        }\n+    }\n+\n+    pub fn pop(&mut self) -> Option<SchedHandle> {\n+        unsafe {\n+            do self.stack.with |s| {\n+                if !s.is_empty() {\n+                    Some(s.pop())\n+                } else {\n+                    None\n+                }\n+            }\n+        }\n+    }\n+}\n+\n+impl Clone for SleeperList {\n+    fn clone(&self) -> SleeperList {\n+        SleeperList {\n+            stack: self.stack.clone()\n+        }\n+    }\n+}\n\\ No newline at end of file"}, {"sha": "e7f87906fe59a0368c9cd2e2a52f799cfea0e7a1", "filename": "src/libstd/rt/task.rs", "status": "modified", "additions": 45, "deletions": 15, "changes": 60, "blob_url": "https://github.com/rust-lang/rust/blob/357f087786cbd6516a38aff800cf9334bc5b85c5/src%2Flibstd%2Frt%2Ftask.rs", "raw_url": "https://github.com/rust-lang/rust/raw/357f087786cbd6516a38aff800cf9334bc5b85c5/src%2Flibstd%2Frt%2Ftask.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibstd%2Frt%2Ftask.rs?ref=357f087786cbd6516a38aff800cf9334bc5b85c5", "patch": "@@ -18,16 +18,22 @@ use cast::transmute;\n use libc::{c_void, uintptr_t};\n use ptr;\n use prelude::*;\n+use option::{Option, Some, None};\n use rt::local::Local;\n use rt::logging::StdErrLogger;\n use super::local_heap::LocalHeap;\n+use rt::sched::{SchedHome, AnySched};\n+use rt::join_latch::JoinLatch;\n \n pub struct Task {\n     heap: LocalHeap,\n     gc: GarbageCollector,\n     storage: LocalStorage,\n     logger: StdErrLogger,\n-    unwinder: Option<Unwinder>,\n+    unwinder: Unwinder,\n+    home: Option<SchedHome>,\n+    join_latch: Option<~JoinLatch>,\n+    on_exit: Option<~fn(bool)>,\n     destroyed: bool\n }\n \n@@ -39,57 +45,71 @@ pub struct Unwinder {\n }\n \n impl Task {\n-    pub fn new() -> Task {\n+    pub fn new_root() -> Task {\n         Task {\n             heap: LocalHeap::new(),\n             gc: GarbageCollector,\n             storage: LocalStorage(ptr::null(), None),\n             logger: StdErrLogger,\n-            unwinder: Some(Unwinder { unwinding: false }),\n+            unwinder: Unwinder { unwinding: false },\n+            home: Some(AnySched),\n+            join_latch: Some(JoinLatch::new_root()),\n+            on_exit: None,\n             destroyed: false\n         }\n     }\n \n-    pub fn without_unwinding() -> Task {\n+    pub fn new_child(&mut self) -> Task {\n         Task {\n             heap: LocalHeap::new(),\n             gc: GarbageCollector,\n             storage: LocalStorage(ptr::null(), None),\n             logger: StdErrLogger,\n-            unwinder: None,\n+            home: Some(AnySched),\n+            unwinder: Unwinder { unwinding: false },\n+            join_latch: Some(self.join_latch.get_mut_ref().new_child()),\n+            on_exit: None,\n             destroyed: false\n         }\n     }\n \n+    pub fn give_home(&mut self, new_home: SchedHome) {\n+        self.home = Some(new_home);\n+    }\n+\n     pub fn run(&mut self, f: &fn()) {\n         // This is just an assertion that `run` was called unsafely\n         // and this instance of Task is still accessible.\n-        do Local::borrow::<Task> |task| {\n+        do Local::borrow::<Task, ()> |task| {\n             assert!(borrow::ref_eq(task, self));\n         }\n \n-        match self.unwinder {\n-            Some(ref mut unwinder) => {\n-                // If there's an unwinder then set up the catch block\n-                unwinder.try(f);\n+        self.unwinder.try(f);\n+        self.destroy();\n+\n+        // Wait for children. Possibly report the exit status.\n+        let local_success = !self.unwinder.unwinding;\n+        let join_latch = self.join_latch.swap_unwrap();\n+        match self.on_exit {\n+            Some(ref on_exit) => {\n+                let success = join_latch.wait(local_success);\n+                (*on_exit)(success);\n             }\n             None => {\n-                // Otherwise, just run the body\n-                f()\n+                join_latch.release(local_success);\n             }\n         }\n-        self.destroy();\n     }\n \n-    /// Must be called manually before finalization to clean up\n+    /// must be called manually before finalization to clean up\n     /// thread-local resources. Some of the routines here expect\n     /// Task to be available recursively so this must be\n     /// called unsafely, without removing Task from\n     /// thread-local-storage.\n     fn destroy(&mut self) {\n         // This is just an assertion that `destroy` was called unsafely\n         // and this instance of Task is still accessible.\n-        do Local::borrow::<Task> |task| {\n+        do Local::borrow::<Task, ()> |task| {\n             assert!(borrow::ref_eq(task, self));\n         }\n         match self.storage {\n@@ -227,4 +247,14 @@ mod test {\n             assert!(port.recv() == 10);\n         }\n     }\n+\n+    #[test]\n+    fn linked_failure() {\n+        do run_in_newsched_task() {\n+            let res = do spawntask_try {\n+                spawntask_random(|| fail!());\n+            };\n+            assert!(res.is_err());\n+        }\n+    }\n }"}, {"sha": "36efcd91834b8fd82a613e1d47ebd4e2c3decb70", "filename": "src/libstd/rt/test.rs", "status": "modified", "additions": 293, "deletions": 63, "changes": 356, "blob_url": "https://github.com/rust-lang/rust/blob/357f087786cbd6516a38aff800cf9334bc5b85c5/src%2Flibstd%2Frt%2Ftest.rs", "raw_url": "https://github.com/rust-lang/rust/raw/357f087786cbd6516a38aff800cf9334bc5b85c5/src%2Flibstd%2Frt%2Ftest.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibstd%2Frt%2Ftest.rs?ref=357f087786cbd6516a38aff800cf9334bc5b85c5", "patch": "@@ -9,73 +9,285 @@\n // except according to those terms.\n \n use uint;\n-use option::*;\n+use option::{Some, None};\n use cell::Cell;\n+use clone::Clone;\n+use container::Container;\n+use iterator::IteratorUtil;\n+use vec::{OwnedVector, MutableVector};\n use result::{Result, Ok, Err};\n+use unstable::run_in_bare_thread;\n use super::io::net::ip::{IpAddr, Ipv4};\n+use rt::comm::oneshot;\n use rt::task::Task;\n use rt::thread::Thread;\n use rt::local::Local;\n+use rt::sched::{Scheduler, Coroutine};\n+use rt::sleeper_list::SleeperList;\n+use rt::work_queue::WorkQueue;\n+\n+pub fn new_test_uv_sched() -> Scheduler {\n+    use rt::uv::uvio::UvEventLoop;\n+    use rt::work_queue::WorkQueue;\n+    use rt::sleeper_list::SleeperList;\n+\n+    let mut sched = Scheduler::new(~UvEventLoop::new(), WorkQueue::new(), SleeperList::new());\n+    // Don't wait for the Shutdown message\n+    sched.no_sleep = true;\n+    return sched;\n+}\n \n /// Creates a new scheduler in a new thread and runs a task in it,\n /// then waits for the scheduler to exit. Failure of the task\n /// will abort the process.\n pub fn run_in_newsched_task(f: ~fn()) {\n     use super::sched::*;\n     use unstable::run_in_bare_thread;\n-    use rt::uv::uvio::UvEventLoop;\n \n     let f = Cell::new(f);\n \n     do run_in_bare_thread {\n-        let mut sched = ~UvEventLoop::new_scheduler();\n+        let mut sched = ~new_test_uv_sched();\n+        let mut new_task = ~Task::new_root();\n+        let on_exit: ~fn(bool) = |exit_status| rtassert!(exit_status);\n+        new_task.on_exit = Some(on_exit);\n         let task = ~Coroutine::with_task(&mut sched.stack_pool,\n-                                         ~Task::without_unwinding(),\n+                                         new_task,\n                                          f.take());\n         sched.enqueue_task(task);\n         sched.run();\n     }\n }\n \n+/// Create more than one scheduler and run a function in a task\n+/// in one of the schedulers. The schedulers will stay alive\n+/// until the function `f` returns.\n+pub fn run_in_mt_newsched_task(f: ~fn()) {\n+    use os;\n+    use from_str::FromStr;\n+    use rt::uv::uvio::UvEventLoop;\n+    use rt::sched::Shutdown;\n+    use rt::util;\n+\n+    let f_cell = Cell::new(f);\n+\n+    do run_in_bare_thread {\n+        let nthreads = match os::getenv(\"RUST_TEST_THREADS\") {\n+            Some(nstr) => FromStr::from_str(nstr).get(),\n+            None => unsafe {\n+                // Using more threads than cores in test code\n+                // to force the OS to preempt them frequently.\n+                // Assuming that this help stress test concurrent types.\n+                util::num_cpus() * 2\n+            }\n+        };\n+\n+        let sleepers = SleeperList::new();\n+        let work_queue = WorkQueue::new();\n+\n+        let mut handles = ~[];\n+        let mut scheds = ~[];\n+\n+        for uint::range(0, nthreads) |_| {\n+            let loop_ = ~UvEventLoop::new();\n+            let mut sched = ~Scheduler::new(loop_, work_queue.clone(), sleepers.clone());\n+            let handle = sched.make_handle();\n+\n+            handles.push(handle);\n+            scheds.push(sched);\n+        }\n+\n+        let f_cell = Cell::new(f_cell.take());\n+        let handles = Cell::new(handles);\n+        let mut new_task = ~Task::new_root();\n+        let on_exit: ~fn(bool) = |exit_status| {\n+\n+            let mut handles = handles.take();\n+            // Tell schedulers to exit\n+            for handles.mut_iter().advance |handle| {\n+                handle.send(Shutdown);\n+            }\n+\n+            rtassert!(exit_status);\n+        };\n+        new_task.on_exit = Some(on_exit);\n+        let main_task = ~Coroutine::with_task(&mut scheds[0].stack_pool,\n+                                              new_task, f_cell.take());\n+        scheds[0].enqueue_task(main_task);\n+\n+        let mut threads = ~[];\n+\n+        while !scheds.is_empty() {\n+            let sched = scheds.pop();\n+            let sched_cell = Cell::new(sched);\n+            let thread = do Thread::start {\n+                let sched = sched_cell.take();\n+                sched.run();\n+            };\n+\n+            threads.push(thread);\n+        }\n+\n+        // Wait for schedulers\n+        let _threads = threads;\n+    }\n+\n+}\n+\n+// THIS IS AWFUL. Copy-pasted the above initialization function but\n+// with a number of hacks to make it spawn tasks on a variety of\n+// schedulers with a variety of homes using the new spawn.\n+\n+pub fn run_in_mt_newsched_task_random_homed() {\n+    use libc;\n+    use os;\n+    use from_str::FromStr;\n+    use rt::uv::uvio::UvEventLoop;\n+    use rt::sched::Shutdown;\n+\n+    do run_in_bare_thread {\n+        let nthreads = match os::getenv(\"RUST_TEST_THREADS\") {\n+            Some(nstr) => FromStr::from_str(nstr).get(),\n+            None => unsafe {\n+                // Using more threads than cores in test code to force\n+                // the OS to preempt them frequently.  Assuming that\n+                // this help stress test concurrent types.\n+                rust_get_num_cpus() * 2\n+            }\n+        };\n+\n+        let sleepers = SleeperList::new();\n+        let work_queue = WorkQueue::new();\n+\n+        let mut handles = ~[];\n+        let mut scheds = ~[];\n+\n+        // create a few special schedulers, those with even indicies\n+        // will be pinned-only\n+        for uint::range(0, nthreads) |i| {\n+            let special = (i % 2) == 0;\n+            let loop_ = ~UvEventLoop::new();\n+            let mut sched = ~Scheduler::new_special(\n+                loop_, work_queue.clone(), sleepers.clone(), special);\n+            let handle = sched.make_handle();\n+            handles.push(handle);\n+            scheds.push(sched);\n+        }\n+\n+        // Schedule a pile o tasks\n+        let n = 5*stress_factor();\n+        for uint::range(0,n) |_i| {\n+                rtdebug!(\"creating task: %u\", _i);\n+                let hf: ~fn() = || { assert!(true) };\n+                spawntask_homed(&mut scheds, hf);\n+            }\n+\n+        // Now we want another pile o tasks that do not ever run on a\n+        // special scheduler, because they are normal tasks. Because\n+        // we can we put these in the \"main\" task.\n+\n+        let n = 5*stress_factor();\n+\n+        let f: ~fn() = || {\n+            for uint::range(0,n) |_| {\n+                let f: ~fn()  = || {\n+                    // Borrow the scheduler we run on and check if it is\n+                    // privileged.\n+                    do Local::borrow::<Scheduler,()> |sched| {\n+                        assert!(sched.run_anything);\n+                    };\n+                };\n+                spawntask_random(f);\n+            };\n+        };\n+\n+        let f_cell = Cell::new(f);\n+        let handles = Cell::new(handles);\n+\n+        rtdebug!(\"creating main task\");\n+\n+        let main_task = ~do Coroutine::new_root(&mut scheds[0].stack_pool) {\n+            f_cell.take()();\n+            let mut handles = handles.take();\n+            // Tell schedulers to exit\n+            for handles.mut_iter().advance |handle| {\n+                handle.send(Shutdown);\n+            }\n+        };\n+\n+        rtdebug!(\"queuing main task\")\n+\n+        scheds[0].enqueue_task(main_task);\n+\n+        let mut threads = ~[];\n+\n+        while !scheds.is_empty() {\n+            let sched = scheds.pop();\n+            let sched_cell = Cell::new(sched);\n+            let thread = do Thread::start {\n+                let sched = sched_cell.take();\n+                rtdebug!(\"running sched: %u\", sched.sched_id());\n+                sched.run();\n+            };\n+\n+            threads.push(thread);\n+        }\n+\n+        rtdebug!(\"waiting on scheduler threads\");\n+\n+        // Wait for schedulers\n+        let _threads = threads;\n+    }\n+\n+    extern {\n+        fn rust_get_num_cpus() -> libc::uintptr_t;\n+    }\n+}\n+\n+\n /// Test tasks will abort on failure instead of unwinding\n pub fn spawntask(f: ~fn()) {\n     use super::sched::*;\n \n+    rtdebug!(\"spawntask taking the scheduler from TLS\")\n+    let task = do Local::borrow::<Task, ~Task>() |running_task| {\n+        ~running_task.new_child()\n+    };\n+\n     let mut sched = Local::take::<Scheduler>();\n     let task = ~Coroutine::with_task(&mut sched.stack_pool,\n-                                     ~Task::without_unwinding(),\n-                                     f);\n-    do sched.switch_running_tasks_and_then(task) |task| {\n-        let task = Cell::new(task);\n-        let sched = Local::take::<Scheduler>();\n-        sched.schedule_new_task(task.take());\n-    }\n+                                     task, f);\n+    rtdebug!(\"spawntask scheduling the new task\");\n+    sched.schedule_task(task);\n }\n \n /// Create a new task and run it right now. Aborts on failure\n pub fn spawntask_immediately(f: ~fn()) {\n     use super::sched::*;\n \n+    let task = do Local::borrow::<Task, ~Task>() |running_task| {\n+        ~running_task.new_child()\n+    };\n+\n     let mut sched = Local::take::<Scheduler>();\n     let task = ~Coroutine::with_task(&mut sched.stack_pool,\n-                                     ~Task::without_unwinding(),\n-                                     f);\n-    do sched.switch_running_tasks_and_then(task) |task| {\n-        let task = Cell::new(task);\n-        do Local::borrow::<Scheduler> |sched| {\n-            sched.enqueue_task(task.take());\n-        }\n+                                     task, f);\n+    do sched.switch_running_tasks_and_then(task) |sched, task| {\n+        sched.enqueue_task(task);\n     }\n }\n \n /// Create a new task and run it right now. Aborts on failure\n pub fn spawntask_later(f: ~fn()) {\n     use super::sched::*;\n \n+    let task = do Local::borrow::<Task, ~Task>() |running_task| {\n+        ~running_task.new_child()\n+    };\n+\n     let mut sched = Local::take::<Scheduler>();\n     let task = ~Coroutine::with_task(&mut sched.stack_pool,\n-                                     ~Task::without_unwinding(),\n-                                     f);\n+                                     task, f);\n \n     sched.enqueue_task(task);\n     Local::put(sched);\n@@ -86,78 +298,96 @@ pub fn spawntask_random(f: ~fn()) {\n     use super::sched::*;\n     use rand::{Rand, rng};\n \n-    let mut rng = rng();\n-    let run_now: bool = Rand::rand(&mut rng);\n+    let task = do Local::borrow::<Task, ~Task>() |running_task| {\n+        ~running_task.new_child()\n+    };\n \n     let mut sched = Local::take::<Scheduler>();\n     let task = ~Coroutine::with_task(&mut sched.stack_pool,\n-                                     ~Task::without_unwinding(),\n-                                     f);\n+                                     task, f);\n+\n+    let mut rng = rng();\n+    let run_now: bool = Rand::rand(&mut rng);\n \n     if run_now {\n-        do sched.switch_running_tasks_and_then(task) |task| {\n-            let task = Cell::new(task);\n-            do Local::borrow::<Scheduler> |sched| {\n-                sched.enqueue_task(task.take());\n-            }\n+        do sched.switch_running_tasks_and_then(task) |sched, task| {\n+            sched.enqueue_task(task);\n         }\n     } else {\n         sched.enqueue_task(task);\n         Local::put(sched);\n     }\n }\n \n+/// Spawn a task, with the current scheduler as home, and queue it to\n+/// run later.\n+pub fn spawntask_homed(scheds: &mut ~[~Scheduler], f: ~fn()) {\n+    use super::sched::*;\n+    use rand::{rng, RngUtil};\n+    let mut rng = rng();\n+\n+    let task = {\n+        let sched = &mut scheds[rng.gen_int_range(0,scheds.len() as int)];\n+        let handle = sched.make_handle();\n+        let home_id = handle.sched_id;\n+\n+        // now that we know where this is going, build a new function\n+        // that can assert it is in the right place\n+        let af: ~fn() = || {\n+            do Local::borrow::<Scheduler,()>() |sched| {\n+                rtdebug!(\"home_id: %u, runtime loc: %u\",\n+                         home_id,\n+                         sched.sched_id());\n+                assert!(home_id == sched.sched_id());\n+            };\n+            f()\n+        };\n+\n+        ~Coroutine::with_task_homed(&mut sched.stack_pool,\n+                                    ~Task::new_root(),\n+                                    af,\n+                                    Sched(handle))\n+    };\n+    let dest_sched = &mut scheds[rng.gen_int_range(0,scheds.len() as int)];\n+    // enqueue it for future execution\n+    dest_sched.enqueue_task(task);\n+}\n \n /// Spawn a task and wait for it to finish, returning whether it completed successfully or failed\n pub fn spawntask_try(f: ~fn()) -> Result<(), ()> {\n     use cell::Cell;\n     use super::sched::*;\n-    use task;\n-    use unstable::finally::Finally;\n-\n-    // Our status variables will be filled in from the scheduler context\n-    let mut failed = false;\n-    let failed_ptr: *mut bool = &mut failed;\n-\n-    // Switch to the scheduler\n-    let f = Cell::new(Cell::new(f));\n-    let sched = Local::take::<Scheduler>();\n-    do sched.deschedule_running_task_and_then() |old_task| {\n-        let old_task = Cell::new(old_task);\n-        let f = f.take();\n-        let mut sched = Local::take::<Scheduler>();\n-        let new_task = ~do Coroutine::new(&mut sched.stack_pool) {\n-            do (|| {\n-                (f.take())()\n-            }).finally {\n-                // Check for failure then resume the parent task\n-                unsafe { *failed_ptr = task::failing(); }\n-                let sched = Local::take::<Scheduler>();\n-                do sched.switch_running_tasks_and_then(old_task.take()) |new_task| {\n-                    let new_task = Cell::new(new_task);\n-                    do Local::borrow::<Scheduler> |sched| {\n-                        sched.enqueue_task(new_task.take());\n-                    }\n-                }\n-            }\n-        };\n \n-        sched.resume_task_immediately(new_task);\n+    let (port, chan) = oneshot();\n+    let chan = Cell::new(chan);\n+    let mut new_task = ~Task::new_root();\n+    let on_exit: ~fn(bool) = |exit_status| chan.take().send(exit_status);\n+    new_task.on_exit = Some(on_exit);\n+    let mut sched = Local::take::<Scheduler>();\n+    let new_task = ~Coroutine::with_task(&mut sched.stack_pool,\n+                                         new_task, f);\n+    do sched.switch_running_tasks_and_then(new_task) |sched, old_task| {\n+        sched.enqueue_task(old_task);\n     }\n \n-    if !failed { Ok(()) } else { Err(()) }\n+    let exit_status = port.recv();\n+    if exit_status { Ok(()) } else { Err(()) }\n }\n \n // Spawn a new task in a new scheduler and return a thread handle.\n pub fn spawntask_thread(f: ~fn()) -> Thread {\n     use rt::sched::*;\n-    use rt::uv::uvio::UvEventLoop;\n \n+    let task = do Local::borrow::<Task, ~Task>() |running_task| {\n+        ~running_task.new_child()\n+    };\n+\n+    let task = Cell::new(task);\n     let f = Cell::new(f);\n     let thread = do Thread::start {\n-        let mut sched = ~UvEventLoop::new_scheduler();\n+        let mut sched = ~new_test_uv_sched();\n         let task = ~Coroutine::with_task(&mut sched.stack_pool,\n-                                         ~Task::without_unwinding(),\n+                                         task.take(),\n                                          f.take());\n         sched.enqueue_task(task);\n         sched.run();"}, {"sha": "89f3d10b5e4cf85cf612352e50c753abb4f7d96e", "filename": "src/libstd/rt/tube.rs", "status": "modified", "additions": 15, "deletions": 21, "changes": 36, "blob_url": "https://github.com/rust-lang/rust/blob/357f087786cbd6516a38aff800cf9334bc5b85c5/src%2Flibstd%2Frt%2Ftube.rs", "raw_url": "https://github.com/rust-lang/rust/raw/357f087786cbd6516a38aff800cf9334bc5b85c5/src%2Flibstd%2Frt%2Ftube.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibstd%2Frt%2Ftube.rs?ref=357f087786cbd6516a38aff800cf9334bc5b85c5", "patch": "@@ -72,7 +72,7 @@ impl<T> Tube<T> {\n                 assert!(self.p.refcount() > 1); // There better be somebody to wake us up\n                 assert!((*state).blocked_task.is_none());\n                 let sched = Local::take::<Scheduler>();\n-                do sched.deschedule_running_task_and_then |task| {\n+                do sched.deschedule_running_task_and_then |_, task| {\n                     (*state).blocked_task = Some(task);\n                 }\n                 rtdebug!(\"waking after tube recv\");\n@@ -107,11 +107,10 @@ mod test {\n             let tube_clone = tube.clone();\n             let tube_clone_cell = Cell::new(tube_clone);\n             let sched = Local::take::<Scheduler>();\n-            do sched.deschedule_running_task_and_then |task| {\n+            do sched.deschedule_running_task_and_then |sched, task| {\n                 let mut tube_clone = tube_clone_cell.take();\n                 tube_clone.send(1);\n-                let sched = Local::take::<Scheduler>();\n-                sched.resume_task_immediately(task);\n+                sched.enqueue_task(task);\n             }\n \n             assert!(tube.recv() == 1);\n@@ -123,21 +122,17 @@ mod test {\n         do run_in_newsched_task {\n             let mut tube: Tube<int> = Tube::new();\n             let tube_clone = tube.clone();\n-            let tube_clone = Cell::new(Cell::new(Cell::new(tube_clone)));\n+            let tube_clone = Cell::new(tube_clone);\n             let sched = Local::take::<Scheduler>();\n-            do sched.deschedule_running_task_and_then |task| {\n-                let tube_clone = tube_clone.take();\n-                do Local::borrow::<Scheduler> |sched| {\n-                    let tube_clone = tube_clone.take();\n-                    do sched.event_loop.callback {\n-                        let mut tube_clone = tube_clone.take();\n-                        // The task should be blocked on this now and\n-                        // sending will wake it up.\n-                        tube_clone.send(1);\n-                    }\n+            do sched.deschedule_running_task_and_then |sched, task| {\n+                let tube_clone = Cell::new(tube_clone.take());\n+                do sched.event_loop.callback {\n+                    let mut tube_clone = tube_clone.take();\n+                    // The task should be blocked on this now and\n+                    // sending will wake it up.\n+                    tube_clone.send(1);\n                 }\n-                let sched = Local::take::<Scheduler>();\n-                sched.resume_task_immediately(task);\n+                sched.enqueue_task(task);\n             }\n \n             assert!(tube.recv() == 1);\n@@ -153,14 +148,14 @@ mod test {\n             let tube_clone = tube.clone();\n             let tube_clone = Cell::new(tube_clone);\n             let sched = Local::take::<Scheduler>();\n-            do sched.deschedule_running_task_and_then |task| {\n+            do sched.deschedule_running_task_and_then |sched, task| {\n                 callback_send(tube_clone.take(), 0);\n \n                 fn callback_send(tube: Tube<int>, i: int) {\n                     if i == 100 { return; }\n \n                     let tube = Cell::new(Cell::new(tube));\n-                    do Local::borrow::<Scheduler> |sched| {\n+                    do Local::borrow::<Scheduler, ()> |sched| {\n                         let tube = tube.take();\n                         do sched.event_loop.callback {\n                             let mut tube = tube.take();\n@@ -172,8 +167,7 @@ mod test {\n                     }\n                 }\n \n-                let sched = Local::take::<Scheduler>();\n-                sched.resume_task_immediately(task);\n+                sched.enqueue_task(task);\n             }\n \n             for int::range(0, MAX) |i| {"}, {"sha": "904b2f8bbb932a254daafd57c1e636fa98ac3ce3", "filename": "src/libstd/rt/util.rs", "status": "added", "additions": 87, "deletions": 0, "changes": 87, "blob_url": "https://github.com/rust-lang/rust/blob/357f087786cbd6516a38aff800cf9334bc5b85c5/src%2Flibstd%2Frt%2Futil.rs", "raw_url": "https://github.com/rust-lang/rust/raw/357f087786cbd6516a38aff800cf9334bc5b85c5/src%2Flibstd%2Frt%2Futil.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibstd%2Frt%2Futil.rs?ref=357f087786cbd6516a38aff800cf9334bc5b85c5", "patch": "@@ -0,0 +1,87 @@\n+// Copyright 2013 The Rust Project Developers. See the COPYRIGHT\n+// file at the top-level directory of this distribution and at\n+// http://rust-lang.org/COPYRIGHT.\n+//\n+// Licensed under the Apache License, Version 2.0 <LICENSE-APACHE or\n+// http://www.apache.org/licenses/LICENSE-2.0> or the MIT license\n+// <LICENSE-MIT or http://opensource.org/licenses/MIT>, at your\n+// option. This file may not be copied, modified, or distributed\n+// except according to those terms.\n+\n+use container::Container;\n+use iterator::IteratorUtil;\n+use libc;\n+use str::StrSlice;\n+\n+/// Get the number of cores available\n+pub fn num_cpus() -> uint {\n+    unsafe {\n+        return rust_get_num_cpus();\n+    }\n+\n+    extern {\n+        fn rust_get_num_cpus() -> libc::uintptr_t;\n+    }\n+}\n+\n+pub fn dumb_println(s: &str) {\n+    use io::WriterUtil;\n+    let dbg = ::libc::STDERR_FILENO as ::io::fd_t;\n+    dbg.write_str(s);\n+    dbg.write_str(\"\\n\");\n+}\n+\n+pub fn abort(msg: &str) -> ! {\n+    let msg = if !msg.is_empty() { msg } else { \"aborted\" };\n+    let hash = msg.iter().fold(0, |accum, val| accum + (val as uint) );\n+    let quote = match hash % 10 {\n+        0 => \"\n+It was from the artists and poets that the pertinent answers came, and I\n+know that panic would have broken loose had they been able to compare notes.\n+As it was, lacking their original letters, I half suspected the compiler of\n+having asked leading questions, or of having edited the correspondence in\n+corroboration of what he had latently resolved to see.\",\n+        1 => \"\n+There are not many persons who know what wonders are opened to them in the\n+stories and visions of their youth; for when as children we listen and dream,\n+we think but half-formed thoughts, and when as men we try to remember, we are\n+dulled and prosaic with the poison of life. But some of us awake in the night\n+with strange phantasms of enchanted hills and gardens, of fountains that sing\n+in the sun, of golden cliffs overhanging murmuring seas, of plains that stretch\n+down to sleeping cities of bronze and stone, and of shadowy companies of heroes\n+that ride caparisoned white horses along the edges of thick forests; and then\n+we know that we have looked back through the ivory gates into that world of\n+wonder which was ours before we were wise and unhappy.\",\n+        2 => \"\n+Instead of the poems I had hoped for, there came only a shuddering blackness\n+and ineffable loneliness; and I saw at last a fearful truth which no one had\n+ever dared to breathe before \u2014 the unwhisperable secret of secrets \u2014 The fact\n+that this city of stone and stridor is not a sentient perpetuation of Old New\n+York as London is of Old London and Paris of Old Paris, but that it is in fact\n+quite dead, its sprawling body imperfectly embalmed and infested with queer\n+animate things which have nothing to do with it as it was in life.\",\n+        3 => \"\n+The ocean ate the last of the land and poured into the smoking gulf, thereby\n+giving up all it had ever conquered. From the new-flooded lands it flowed\n+again, uncovering death and decay; and from its ancient and immemorial bed it\n+trickled loathsomely, uncovering nighted secrets of the years when Time was\n+young and the gods unborn. Above the waves rose weedy remembered spires. The\n+moon laid pale lilies of light on dead London, and Paris stood up from its damp\n+grave to be sanctified with star-dust. Then rose spires and monoliths that were\n+weedy but not remembered; terrible spires and monoliths of lands that men never\n+knew were lands...\",\n+        4 => \"\n+There was a night when winds from unknown spaces whirled us irresistibly into\n+limitless vacum beyond all thought and entity. Perceptions of the most\n+maddeningly untransmissible sort thronged upon us; perceptions of infinity\n+which at the time convulsed us with joy, yet which are now partly lost to my\n+memory and partly incapable of presentation to others.\",\n+        _ => \"You've met with a terrible fate, haven't you?\"\n+    };\n+    rterrln!(\"%s\", \"\");\n+    rterrln!(\"%s\", quote);\n+    rterrln!(\"%s\", \"\");\n+    rterrln!(\"fatal runtime error: %s\", msg);\n+\n+    unsafe { libc::abort(); }\n+}"}, {"sha": "f3d1024024ff80dc918dc21d5c154d85fd31dab7", "filename": "src/libstd/rt/uv/async.rs", "status": "added", "additions": 105, "deletions": 0, "changes": 105, "blob_url": "https://github.com/rust-lang/rust/blob/357f087786cbd6516a38aff800cf9334bc5b85c5/src%2Flibstd%2Frt%2Fuv%2Fasync.rs", "raw_url": "https://github.com/rust-lang/rust/raw/357f087786cbd6516a38aff800cf9334bc5b85c5/src%2Flibstd%2Frt%2Fuv%2Fasync.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibstd%2Frt%2Fuv%2Fasync.rs?ref=357f087786cbd6516a38aff800cf9334bc5b85c5", "patch": "@@ -0,0 +1,105 @@\n+// Copyright 2013 The Rust Project Developers. See the COPYRIGHT\n+// file at the top-level directory of this distribution and at\n+// http://rust-lang.org/COPYRIGHT.\n+//\n+// Licensed under the Apache License, Version 2.0 <LICENSE-APACHE or\n+// http://www.apache.org/licenses/LICENSE-2.0> or the MIT license\n+// <LICENSE-MIT or http://opensource.org/licenses/MIT>, at your\n+// option. This file may not be copied, modified, or distributed\n+// except according to those terms.\n+\n+use libc::{c_int, c_void};\n+use option::Some;\n+use rt::uv::uvll;\n+use rt::uv::uvll::UV_ASYNC;\n+use rt::uv::{Watcher, Loop, NativeHandle, AsyncCallback, NullCallback};\n+use rt::uv::WatcherInterop;\n+use rt::uv::status_to_maybe_uv_error;\n+\n+pub struct AsyncWatcher(*uvll::uv_async_t);\n+impl Watcher for AsyncWatcher { }\n+\n+impl AsyncWatcher {\n+    pub fn new(loop_: &mut Loop, cb: AsyncCallback) -> AsyncWatcher {\n+        unsafe {\n+            let handle = uvll::malloc_handle(UV_ASYNC);\n+            assert!(handle.is_not_null());\n+            let mut watcher: AsyncWatcher = NativeHandle::from_native_handle(handle);\n+            watcher.install_watcher_data();\n+            let data = watcher.get_watcher_data();\n+            data.async_cb = Some(cb);\n+            assert_eq!(0, uvll::async_init(loop_.native_handle(), handle, async_cb));\n+            return watcher;\n+        }\n+\n+        extern fn async_cb(handle: *uvll::uv_async_t, status: c_int) {\n+            let mut watcher: AsyncWatcher = NativeHandle::from_native_handle(handle);\n+            let status = status_to_maybe_uv_error(watcher.native_handle(), status);\n+            let data = watcher.get_watcher_data();\n+            let cb = data.async_cb.get_ref();\n+            (*cb)(watcher, status);\n+        }\n+    }\n+\n+    pub fn send(&mut self) {\n+        unsafe {\n+            let handle = self.native_handle();\n+            uvll::async_send(handle);\n+        }\n+    }\n+\n+    pub fn close(self, cb: NullCallback) {\n+        let mut this = self;\n+        let data = this.get_watcher_data();\n+        assert!(data.close_cb.is_none());\n+        data.close_cb = Some(cb);\n+\n+        unsafe {\n+            uvll::close(self.native_handle(), close_cb);\n+        }\n+\n+        extern fn close_cb(handle: *uvll::uv_stream_t) {\n+            let mut watcher: AsyncWatcher = NativeHandle::from_native_handle(handle);\n+            {\n+                let data = watcher.get_watcher_data();\n+                data.close_cb.swap_unwrap()();\n+            }\n+            watcher.drop_watcher_data();\n+            unsafe { uvll::free_handle(handle as *c_void); }\n+        }\n+    }\n+}\n+\n+impl NativeHandle<*uvll::uv_async_t> for AsyncWatcher {\n+    fn from_native_handle(handle: *uvll::uv_async_t) -> AsyncWatcher {\n+        AsyncWatcher(handle)\n+    }\n+    fn native_handle(&self) -> *uvll::uv_async_t {\n+        match self { &AsyncWatcher(ptr) => ptr }\n+    }\n+}\n+\n+#[cfg(test)]\n+mod test {\n+\n+    use super::*;\n+    use rt::uv::Loop;\n+    use unstable::run_in_bare_thread;\n+    use rt::thread::Thread;\n+    use cell::Cell;\n+\n+    #[test]\n+    fn smoke_test() {\n+        do run_in_bare_thread {\n+            let mut loop_ = Loop::new();\n+            let watcher = AsyncWatcher::new(&mut loop_, |w, _| w.close(||()) );\n+            let watcher_cell = Cell::new(watcher);\n+            let _thread = do Thread::start {\n+                let mut watcher = watcher_cell.take();\n+                watcher.send();\n+            };\n+            loop_.run();\n+            loop_.close();\n+        }\n+    }\n+}"}, {"sha": "a3630c9b9bf8d6df54da4e9e95a650542a60a89a", "filename": "src/libstd/rt/uv/idle.rs", "status": "modified", "additions": 62, "deletions": 0, "changes": 62, "blob_url": "https://github.com/rust-lang/rust/blob/357f087786cbd6516a38aff800cf9334bc5b85c5/src%2Flibstd%2Frt%2Fuv%2Fidle.rs", "raw_url": "https://github.com/rust-lang/rust/raw/357f087786cbd6516a38aff800cf9334bc5b85c5/src%2Flibstd%2Frt%2Fuv%2Fidle.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibstd%2Frt%2Fuv%2Fidle.rs?ref=357f087786cbd6516a38aff800cf9334bc5b85c5", "patch": "@@ -90,3 +90,65 @@ impl NativeHandle<*uvll::uv_idle_t> for IdleWatcher {\n         match self { &IdleWatcher(ptr) => ptr }\n     }\n }\n+\n+#[cfg(test)]\n+mod test {\n+\n+    use rt::uv::Loop;\n+    use super::*;\n+    use unstable::run_in_bare_thread;\n+\n+    #[test]\n+    #[ignore(reason = \"valgrind - loop destroyed before watcher?\")]\n+    fn idle_new_then_close() {\n+        do run_in_bare_thread {\n+            let mut loop_ = Loop::new();\n+            let idle_watcher = { IdleWatcher::new(&mut loop_) };\n+            idle_watcher.close(||());\n+        }\n+    }\n+\n+    #[test]\n+    fn idle_smoke_test() {\n+        do run_in_bare_thread {\n+            let mut loop_ = Loop::new();\n+            let mut idle_watcher = { IdleWatcher::new(&mut loop_) };\n+            let mut count = 10;\n+            let count_ptr: *mut int = &mut count;\n+            do idle_watcher.start |idle_watcher, status| {\n+                let mut idle_watcher = idle_watcher;\n+                assert!(status.is_none());\n+                if unsafe { *count_ptr == 10 } {\n+                    idle_watcher.stop();\n+                    idle_watcher.close(||());\n+                } else {\n+                    unsafe { *count_ptr = *count_ptr + 1; }\n+                }\n+            }\n+            loop_.run();\n+            loop_.close();\n+            assert_eq!(count, 10);\n+        }\n+    }\n+\n+    #[test]\n+    fn idle_start_stop_start() {\n+        do run_in_bare_thread {\n+            let mut loop_ = Loop::new();\n+            let mut idle_watcher = { IdleWatcher::new(&mut loop_) };\n+            do idle_watcher.start |idle_watcher, status| {\n+                let mut idle_watcher = idle_watcher;\n+                assert!(status.is_none());\n+                idle_watcher.stop();\n+                do idle_watcher.start |idle_watcher, status| {\n+                    assert!(status.is_none());\n+                    let mut idle_watcher = idle_watcher;\n+                    idle_watcher.stop();\n+                    idle_watcher.close(||());\n+                }\n+            }\n+            loop_.run();\n+            loop_.close();\n+        }\n+    }\n+}"}, {"sha": "883eda0057fdb325e0fbf7c1a211c05591a94481", "filename": "src/libstd/rt/uv/mod.rs", "status": "modified", "additions": 7, "deletions": 56, "changes": 63, "blob_url": "https://github.com/rust-lang/rust/blob/357f087786cbd6516a38aff800cf9334bc5b85c5/src%2Flibstd%2Frt%2Fuv%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/357f087786cbd6516a38aff800cf9334bc5b85c5/src%2Flibstd%2Frt%2Fuv%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibstd%2Frt%2Fuv%2Fmod.rs?ref=357f087786cbd6516a38aff800cf9334bc5b85c5", "patch": "@@ -55,6 +55,7 @@ pub use self::file::FsRequest;\n pub use self::net::{StreamWatcher, TcpWatcher};\n pub use self::idle::IdleWatcher;\n pub use self::timer::TimerWatcher;\n+pub use self::async::AsyncWatcher;\n \n /// The implementation of `rtio` for libuv\n pub mod uvio;\n@@ -66,6 +67,7 @@ pub mod file;\n pub mod net;\n pub mod idle;\n pub mod timer;\n+pub mod async;\n \n /// XXX: Loop(*handle) is buggy with destructors. Normal structs\n /// with dtors may not be destructured, but tuple structs can,\n@@ -123,6 +125,7 @@ pub type IdleCallback = ~fn(IdleWatcher, Option<UvError>);\n pub type ConnectionCallback = ~fn(StreamWatcher, Option<UvError>);\n pub type FsCallback = ~fn(FsRequest, Option<UvError>);\n pub type TimerCallback = ~fn(TimerWatcher, Option<UvError>);\n+pub type AsyncCallback = ~fn(AsyncWatcher, Option<UvError>);\n \n \n /// Callbacks used by StreamWatchers, set as custom data on the foreign handle\n@@ -133,7 +136,8 @@ struct WatcherData {\n     close_cb: Option<NullCallback>,\n     alloc_cb: Option<AllocCallback>,\n     idle_cb: Option<IdleCallback>,\n-    timer_cb: Option<TimerCallback>\n+    timer_cb: Option<TimerCallback>,\n+    async_cb: Option<AsyncCallback>\n }\n \n pub trait WatcherInterop {\n@@ -162,7 +166,8 @@ impl<H, W: Watcher + NativeHandle<*H>> WatcherInterop for W {\n                 close_cb: None,\n                 alloc_cb: None,\n                 idle_cb: None,\n-                timer_cb: None\n+                timer_cb: None,\n+                async_cb: None\n             };\n             let data = transmute::<~WatcherData, *c_void>(data);\n             uvll::set_data_for_uv_handle(self.native_handle(), data);\n@@ -347,57 +352,3 @@ fn loop_smoke_test() {\n         loop_.close();\n     }\n }\n-\n-#[test]\n-#[ignore(reason = \"valgrind - loop destroyed before watcher?\")]\n-fn idle_new_then_close() {\n-    do run_in_bare_thread {\n-        let mut loop_ = Loop::new();\n-        let idle_watcher = { IdleWatcher::new(&mut loop_) };\n-        idle_watcher.close(||());\n-    }\n-}\n-\n-#[test]\n-fn idle_smoke_test() {\n-    do run_in_bare_thread {\n-        let mut loop_ = Loop::new();\n-        let mut idle_watcher = { IdleWatcher::new(&mut loop_) };\n-        let mut count = 10;\n-        let count_ptr: *mut int = &mut count;\n-        do idle_watcher.start |idle_watcher, status| {\n-            let mut idle_watcher = idle_watcher;\n-            assert!(status.is_none());\n-            if unsafe { *count_ptr == 10 } {\n-                idle_watcher.stop();\n-                idle_watcher.close(||());\n-            } else {\n-                unsafe { *count_ptr = *count_ptr + 1; }\n-            }\n-        }\n-        loop_.run();\n-        loop_.close();\n-        assert_eq!(count, 10);\n-    }\n-}\n-\n-#[test]\n-fn idle_start_stop_start() {\n-    do run_in_bare_thread {\n-        let mut loop_ = Loop::new();\n-        let mut idle_watcher = { IdleWatcher::new(&mut loop_) };\n-        do idle_watcher.start |idle_watcher, status| {\n-            let mut idle_watcher = idle_watcher;\n-            assert!(status.is_none());\n-            idle_watcher.stop();\n-            do idle_watcher.start |idle_watcher, status| {\n-                assert!(status.is_none());\n-                let mut idle_watcher = idle_watcher;\n-                idle_watcher.stop();\n-                idle_watcher.close(||());\n-            }\n-        }\n-        loop_.run();\n-        loop_.close();\n-    }\n-}"}, {"sha": "298277b3df05b020bb01c73538ccae5b14065644", "filename": "src/libstd/rt/uv/uvio.rs", "status": "modified", "additions": 99, "deletions": 21, "changes": 120, "blob_url": "https://github.com/rust-lang/rust/blob/357f087786cbd6516a38aff800cf9334bc5b85c5/src%2Flibstd%2Frt%2Fuv%2Fuvio.rs", "raw_url": "https://github.com/rust-lang/rust/raw/357f087786cbd6516a38aff800cf9334bc5b85c5/src%2Flibstd%2Frt%2Fuv%2Fuvio.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibstd%2Frt%2Fuv%2Fuvio.rs?ref=357f087786cbd6516a38aff800cf9334bc5b85c5", "patch": "@@ -12,6 +12,7 @@ use option::*;\n use result::*;\n use ops::Drop;\n use cell::Cell;\n+use cast;\n use cast::transmute;\n use clone::Clone;\n use rt::io::IoError;\n@@ -23,6 +24,7 @@ use rt::sched::Scheduler;\n use rt::io::{standard_error, OtherIoError};\n use rt::tube::Tube;\n use rt::local::Local;\n+use unstable::sync::{Exclusive, exclusive};\n \n #[cfg(test)] use container::Container;\n #[cfg(test)] use uint;\n@@ -39,11 +41,6 @@ impl UvEventLoop {\n             uvio: UvIoFactory(Loop::new())\n         }\n     }\n-\n-    /// A convenience constructor\n-    pub fn new_scheduler() -> Scheduler {\n-        Scheduler::new(~UvEventLoop::new())\n-    }\n }\n \n impl Drop for UvEventLoop {\n@@ -81,6 +78,10 @@ impl EventLoop for UvEventLoop {\n         }\n     }\n \n+    fn remote_callback(&mut self, f: ~fn()) -> ~RemoteCallbackObject {\n+        ~UvRemoteCallback::new(self.uvio.uv_loop(), f)\n+    }\n+\n     fn io<'a>(&'a mut self) -> Option<&'a mut IoFactoryObject> {\n         Some(&mut self.uvio)\n     }\n@@ -100,6 +101,89 @@ fn test_callback_run_once() {\n     }\n }\n \n+pub struct UvRemoteCallback {\n+    // The uv async handle for triggering the callback\n+    async: AsyncWatcher,\n+    // A flag to tell the callback to exit, set from the dtor. This is\n+    // almost never contested - only in rare races with the dtor.\n+    exit_flag: Exclusive<bool>\n+}\n+\n+impl UvRemoteCallback {\n+    pub fn new(loop_: &mut Loop, f: ~fn()) -> UvRemoteCallback {\n+        let exit_flag = exclusive(false);\n+        let exit_flag_clone = exit_flag.clone();\n+        let async = do AsyncWatcher::new(loop_) |watcher, status| {\n+            assert!(status.is_none());\n+            f();\n+            unsafe {\n+                do exit_flag_clone.with_imm |&should_exit| {\n+                    if should_exit {\n+                        watcher.close(||());\n+                    }\n+                }\n+            }\n+        };\n+        UvRemoteCallback {\n+            async: async,\n+            exit_flag: exit_flag\n+        }\n+    }\n+}\n+\n+impl RemoteCallback for UvRemoteCallback {\n+    fn fire(&mut self) { self.async.send() }\n+}\n+\n+impl Drop for UvRemoteCallback {\n+    fn finalize(&self) {\n+        unsafe {\n+            let this: &mut UvRemoteCallback = cast::transmute_mut(self);\n+            do this.exit_flag.with |should_exit| {\n+                // NB: These two things need to happen atomically. Otherwise\n+                // the event handler could wake up due to a *previous*\n+                // signal and see the exit flag, destroying the handle\n+                // before the final send.\n+                *should_exit = true;\n+                this.async.send();\n+            }\n+        }\n+    }\n+}\n+\n+#[cfg(test)]\n+mod test_remote {\n+    use cell::Cell;\n+    use rt::test::*;\n+    use rt::thread::Thread;\n+    use rt::tube::Tube;\n+    use rt::rtio::EventLoop;\n+    use rt::local::Local;\n+    use rt::sched::Scheduler;\n+\n+    #[test]\n+    fn test_uv_remote() {\n+        do run_in_newsched_task {\n+            let mut tube = Tube::new();\n+            let tube_clone = tube.clone();\n+            let remote_cell = Cell::new_empty();\n+            do Local::borrow::<Scheduler, ()>() |sched| {\n+                let tube_clone = tube_clone.clone();\n+                let tube_clone_cell = Cell::new(tube_clone);\n+                let remote = do sched.event_loop.remote_callback {\n+                    tube_clone_cell.take().send(1);\n+                };\n+                remote_cell.put_back(remote);\n+            }\n+            let _thread = do Thread::start {\n+                remote_cell.take().fire();\n+            };\n+\n+            assert!(tube.recv() == 1);\n+        }\n+    }\n+}\n+\n pub struct UvIoFactory(Loop);\n \n impl UvIoFactory {\n@@ -122,12 +206,10 @@ impl IoFactory for UvIoFactory {\n         assert!(scheduler.in_task_context());\n \n         // Block this task and take ownership, switch to scheduler context\n-        do scheduler.deschedule_running_task_and_then |task| {\n+        do scheduler.deschedule_running_task_and_then |sched, task| {\n \n             rtdebug!(\"connect: entered scheduler context\");\n-            do Local::borrow::<Scheduler> |scheduler| {\n-                assert!(!scheduler.in_task_context());\n-            }\n+            assert!(!sched.in_task_context());\n             let mut tcp_watcher = TcpWatcher::new(self.uv_loop());\n             let task_cell = Cell::new(task);\n \n@@ -167,7 +249,7 @@ impl IoFactory for UvIoFactory {\n             Ok(_) => Ok(~UvTcpListener::new(watcher)),\n             Err(uverr) => {\n                 let scheduler = Local::take::<Scheduler>();\n-                do scheduler.deschedule_running_task_and_then |task| {\n+                do scheduler.deschedule_running_task_and_then |_, task| {\n                     let task_cell = Cell::new(task);\n                     do watcher.as_stream().close {\n                         let scheduler = Local::take::<Scheduler>();\n@@ -203,7 +285,7 @@ impl Drop for UvTcpListener {\n     fn finalize(&self) {\n         let watcher = self.watcher();\n         let scheduler = Local::take::<Scheduler>();\n-        do scheduler.deschedule_running_task_and_then |task| {\n+        do scheduler.deschedule_running_task_and_then |_, task| {\n             let task_cell = Cell::new(task);\n             do watcher.as_stream().close {\n                 let scheduler = Local::take::<Scheduler>();\n@@ -265,7 +347,7 @@ impl Drop for UvTcpStream {\n         rtdebug!(\"closing tcp stream\");\n         let watcher = self.watcher();\n         let scheduler = Local::take::<Scheduler>();\n-        do scheduler.deschedule_running_task_and_then |task| {\n+        do scheduler.deschedule_running_task_and_then |_, task| {\n             let task_cell = Cell::new(task);\n             do watcher.close {\n                 let scheduler = Local::take::<Scheduler>();\n@@ -284,11 +366,9 @@ impl RtioTcpStream for UvTcpStream {\n         assert!(scheduler.in_task_context());\n         let watcher = self.watcher();\n         let buf_ptr: *&mut [u8] = &buf;\n-        do scheduler.deschedule_running_task_and_then |task| {\n+        do scheduler.deschedule_running_task_and_then |sched, task| {\n             rtdebug!(\"read: entered scheduler context\");\n-            do Local::borrow::<Scheduler> |scheduler| {\n-                assert!(!scheduler.in_task_context());\n-            }\n+            assert!(!sched.in_task_context());\n             let mut watcher = watcher;\n             let task_cell = Cell::new(task);\n             // XXX: We shouldn't reallocate these callbacks every\n@@ -330,7 +410,7 @@ impl RtioTcpStream for UvTcpStream {\n         assert!(scheduler.in_task_context());\n         let watcher = self.watcher();\n         let buf_ptr: *&[u8] = &buf;\n-        do scheduler.deschedule_running_task_and_then |task| {\n+        do scheduler.deschedule_running_task_and_then |_, task| {\n             let mut watcher = watcher;\n             let task_cell = Cell::new(task);\n             let buf = unsafe { slice_to_uv_buf(*buf_ptr) };\n@@ -424,11 +504,9 @@ fn test_read_and_block() {\n                 // Yield to the other task in hopes that it\n                 // will trigger a read callback while we are\n                 // not ready for it\n-                do scheduler.deschedule_running_task_and_then |task| {\n+                do scheduler.deschedule_running_task_and_then |sched, task| {\n                     let task = Cell::new(task);\n-                    do Local::borrow::<Scheduler> |scheduler| {\n-                        scheduler.enqueue_task(task.take());\n-                    }\n+                    sched.enqueue_task(task.take());\n                 }\n             }\n "}, {"sha": "0d298bde6b50875d04d565f66af17f2670707805", "filename": "src/libstd/rt/uvll.rs", "status": "removed", "additions": 0, "deletions": 443, "changes": 443, "blob_url": "https://github.com/rust-lang/rust/blob/adeb7e77ccff938c0afb105a14a2ff4df4c7efc8/src%2Flibstd%2Frt%2Fuvll.rs", "raw_url": "https://github.com/rust-lang/rust/raw/adeb7e77ccff938c0afb105a14a2ff4df4c7efc8/src%2Flibstd%2Frt%2Fuvll.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibstd%2Frt%2Fuvll.rs?ref=adeb7e77ccff938c0afb105a14a2ff4df4c7efc8", "patch": "@@ -1,443 +0,0 @@\n-// Copyright 2012 The Rust Project Developers. See the COPYRIGHT\n-// file at the top-level directory of this distribution and at\n-// http://rust-lang.org/COPYRIGHT.\n-//\n-// Licensed under the Apache License, Version 2.0 <LICENSE-APACHE or\n-// http://www.apache.org/licenses/LICENSE-2.0> or the MIT license\n-// <LICENSE-MIT or http://opensource.org/licenses/MIT>, at your\n-// option. This file may not be copied, modified, or distributed\n-// except according to those terms.\n-\n-/*!\n- * Low-level bindings to the libuv library.\n- *\n- * This module contains a set of direct, 'bare-metal' wrappers around\n- * the libuv C-API.\n- *\n- * We're not bothering yet to redefine uv's structs as Rust structs\n- * because they are quite large and change often between versions.\n- * The maintenance burden is just too high. Instead we use the uv's\n- * `uv_handle_size` and `uv_req_size` to find the correct size of the\n- * structs and allocate them on the heap. This can be revisited later.\n- *\n- * There are also a collection of helper functions to ease interacting\n- * with the low-level API.\n- *\n- * As new functionality, existant in uv.h, is added to the rust stdlib,\n- * the mappings should be added in this module.\n- */\n-\n-#[allow(non_camel_case_types)]; // C types\n-\n-use libc::{size_t, c_int, c_uint, c_void, c_char, uintptr_t};\n-use libc::{malloc, free};\n-use prelude::*;\n-\n-pub struct uv_err_t {\n-    code: c_int,\n-    sys_errno_: c_int\n-}\n-\n-pub struct uv_buf_t {\n-    base: *u8,\n-    len: libc::size_t,\n-}\n-\n-pub type uv_handle_t = c_void;\n-pub type uv_loop_t = c_void;\n-pub type uv_idle_t = c_void;\n-pub type uv_tcp_t = c_void;\n-pub type uv_connect_t = c_void;\n-pub type uv_write_t = c_void;\n-pub type uv_async_t = c_void;\n-pub type uv_timer_t = c_void;\n-pub type uv_stream_t = c_void;\n-pub type uv_fs_t = c_void;\n-\n-pub type uv_idle_cb = *u8;\n-\n-pub type sockaddr_in = c_void;\n-pub type sockaddr_in6 = c_void;\n-\n-#[deriving(Eq)]\n-pub enum uv_handle_type {\n-    UV_UNKNOWN_HANDLE,\n-    UV_ASYNC,\n-    UV_CHECK,\n-    UV_FS_EVENT,\n-    UV_FS_POLL,\n-    UV_HANDLE,\n-    UV_IDLE,\n-    UV_NAMED_PIPE,\n-    UV_POLL,\n-    UV_PREPARE,\n-    UV_PROCESS,\n-    UV_STREAM,\n-    UV_TCP,\n-    UV_TIMER,\n-    UV_TTY,\n-    UV_UDP,\n-    UV_SIGNAL,\n-    UV_FILE,\n-    UV_HANDLE_TYPE_MAX\n-}\n-\n-#[deriving(Eq)]\n-pub enum uv_req_type {\n-    UV_UNKNOWN_REQ,\n-    UV_REQ,\n-    UV_CONNECT,\n-    UV_WRITE,\n-    UV_SHUTDOWN,\n-    UV_UDP_SEND,\n-    UV_FS,\n-    UV_WORK,\n-    UV_GETADDRINFO,\n-    UV_REQ_TYPE_MAX\n-}\n-\n-pub unsafe fn malloc_handle(handle: uv_handle_type) -> *c_void {\n-    assert!(handle != UV_UNKNOWN_HANDLE && handle != UV_HANDLE_TYPE_MAX);\n-    let size = rust_uv_handle_size(handle as uint);\n-    let p = malloc(size);\n-    assert!(p.is_not_null());\n-    return p;\n-}\n-\n-pub unsafe fn free_handle(v: *c_void) {\n-    free(v)\n-}\n-\n-pub unsafe fn malloc_req(req: uv_req_type) -> *c_void {\n-    assert!(req != UV_UNKNOWN_REQ && req != UV_REQ_TYPE_MAX);\n-    let size = rust_uv_req_size(req as uint);\n-    let p = malloc(size);\n-    assert!(p.is_not_null());\n-    return p;\n-}\n-\n-pub unsafe fn free_req(v: *c_void) {\n-    free(v)\n-}\n-\n-#[test]\n-fn handle_sanity_check() {\n-    unsafe {\n-        assert!(UV_HANDLE_TYPE_MAX as uint == rust_uv_handle_type_max());\n-    }\n-}\n-\n-#[test]\n-fn request_sanity_check() {\n-    unsafe {\n-        assert!(UV_REQ_TYPE_MAX as uint == rust_uv_req_type_max());\n-    }\n-}\n-\n-pub unsafe fn loop_new() -> *c_void {\n-    return rust_uv_loop_new();\n-}\n-\n-pub unsafe fn loop_delete(loop_handle: *c_void) {\n-    rust_uv_loop_delete(loop_handle);\n-}\n-\n-pub unsafe fn run(loop_handle: *c_void) {\n-    rust_uv_run(loop_handle);\n-}\n-\n-pub unsafe fn close<T>(handle: *T, cb: *u8) {\n-    rust_uv_close(handle as *c_void, cb);\n-}\n-\n-pub unsafe fn walk(loop_handle: *c_void, cb: *u8, arg: *c_void) {\n-    rust_uv_walk(loop_handle, cb, arg);\n-}\n-\n-pub unsafe fn idle_new() -> *uv_idle_t {\n-    rust_uv_idle_new()\n-}\n-\n-pub unsafe fn idle_delete(handle: *uv_idle_t) {\n-    rust_uv_idle_delete(handle)\n-}\n-\n-pub unsafe fn idle_init(loop_handle: *uv_loop_t, handle: *uv_idle_t) -> c_int {\n-    rust_uv_idle_init(loop_handle, handle)\n-}\n-\n-pub unsafe fn idle_start(handle: *uv_idle_t, cb: uv_idle_cb) -> c_int {\n-    rust_uv_idle_start(handle, cb)\n-}\n-\n-pub unsafe fn idle_stop(handle: *uv_idle_t) -> c_int {\n-    rust_uv_idle_stop(handle)\n-}\n-\n-pub unsafe fn tcp_init(loop_handle: *c_void, handle: *uv_tcp_t) -> c_int {\n-    return rust_uv_tcp_init(loop_handle, handle);\n-}\n-\n-// FIXME ref #2064\n-pub unsafe fn tcp_connect(connect_ptr: *uv_connect_t,\n-                          tcp_handle_ptr: *uv_tcp_t,\n-                          addr_ptr: *sockaddr_in,\n-                          after_connect_cb: *u8) -> c_int {\n-    return rust_uv_tcp_connect(connect_ptr, tcp_handle_ptr,\n-                                       after_connect_cb, addr_ptr);\n-}\n-// FIXME ref #2064\n-pub unsafe fn tcp_connect6(connect_ptr: *uv_connect_t,\n-                           tcp_handle_ptr: *uv_tcp_t,\n-                           addr_ptr: *sockaddr_in6,\n-                           after_connect_cb: *u8) -> c_int {\n-    return rust_uv_tcp_connect6(connect_ptr, tcp_handle_ptr,\n-                                        after_connect_cb, addr_ptr);\n-}\n-// FIXME ref #2064\n-pub unsafe fn tcp_bind(tcp_server_ptr: *uv_tcp_t, addr_ptr: *sockaddr_in) -> c_int {\n-    return rust_uv_tcp_bind(tcp_server_ptr, addr_ptr);\n-}\n-// FIXME ref #2064\n-pub unsafe fn tcp_bind6(tcp_server_ptr: *uv_tcp_t, addr_ptr: *sockaddr_in6) -> c_int {\n-    return rust_uv_tcp_bind6(tcp_server_ptr, addr_ptr);\n-}\n-\n-pub unsafe fn tcp_getpeername(tcp_handle_ptr: *uv_tcp_t, name: *sockaddr_in) -> c_int {\n-    return rust_uv_tcp_getpeername(tcp_handle_ptr, name);\n-}\n-\n-pub unsafe fn tcp_getpeername6(tcp_handle_ptr: *uv_tcp_t, name: *sockaddr_in6) ->c_int {\n-    return rust_uv_tcp_getpeername6(tcp_handle_ptr, name);\n-}\n-\n-pub unsafe fn listen<T>(stream: *T, backlog: c_int, cb: *u8) -> c_int {\n-    return rust_uv_listen(stream as *c_void, backlog, cb);\n-}\n-\n-pub unsafe fn accept(server: *c_void, client: *c_void) -> c_int {\n-    return rust_uv_accept(server as *c_void, client as *c_void);\n-}\n-\n-pub unsafe fn write<T>(req: *uv_write_t, stream: *T, buf_in: &[uv_buf_t], cb: *u8) -> c_int {\n-    let buf_ptr = vec::raw::to_ptr(buf_in);\n-    let buf_cnt = buf_in.len() as i32;\n-    return rust_uv_write(req as *c_void, stream as *c_void, buf_ptr, buf_cnt, cb);\n-}\n-pub unsafe fn read_start(stream: *uv_stream_t, on_alloc: *u8, on_read: *u8) -> c_int {\n-    return rust_uv_read_start(stream as *c_void, on_alloc, on_read);\n-}\n-\n-pub unsafe fn read_stop(stream: *uv_stream_t) -> c_int {\n-    return rust_uv_read_stop(stream as *c_void);\n-}\n-\n-pub unsafe fn last_error(loop_handle: *c_void) -> uv_err_t {\n-    return rust_uv_last_error(loop_handle);\n-}\n-\n-pub unsafe fn strerror(err: *uv_err_t) -> *c_char {\n-    return rust_uv_strerror(err);\n-}\n-pub unsafe fn err_name(err: *uv_err_t) -> *c_char {\n-    return rust_uv_err_name(err);\n-}\n-\n-pub unsafe fn async_init(loop_handle: *c_void, async_handle: *uv_async_t, cb: *u8) -> c_int {\n-    return rust_uv_async_init(loop_handle, async_handle, cb);\n-}\n-\n-pub unsafe fn async_send(async_handle: *uv_async_t) {\n-    return rust_uv_async_send(async_handle);\n-}\n-pub unsafe fn buf_init(input: *u8, len: uint) -> uv_buf_t {\n-    let out_buf = uv_buf_t { base: ptr::null(), len: 0 as size_t };\n-    let out_buf_ptr = ptr::to_unsafe_ptr(&out_buf);\n-    rust_uv_buf_init(out_buf_ptr, input, len as size_t);\n-    return out_buf;\n-}\n-\n-pub unsafe fn timer_init(loop_ptr: *c_void, timer_ptr: *uv_timer_t) -> c_int {\n-    return rust_uv_timer_init(loop_ptr, timer_ptr);\n-}\n-pub unsafe fn timer_start(timer_ptr: *uv_timer_t, cb: *u8, timeout: uint,\n-                          repeat: uint) -> c_int {\n-    return rust_uv_timer_start(timer_ptr, cb, timeout as c_uint, repeat as c_uint);\n-}\n-pub unsafe fn timer_stop(timer_ptr: *uv_timer_t) -> c_int {\n-    return rust_uv_timer_stop(timer_ptr);\n-}\n-\n-pub unsafe fn malloc_ip4_addr(ip: &str, port: int) -> *sockaddr_in {\n-    do str::as_c_str(ip) |ip_buf| {\n-        rust_uv_ip4_addrp(ip_buf as *u8, port as libc::c_int)\n-    }\n-}\n-pub unsafe fn malloc_ip6_addr(ip: &str, port: int) -> *sockaddr_in6 {\n-    do str::as_c_str(ip) |ip_buf| {\n-        rust_uv_ip6_addrp(ip_buf as *u8, port as libc::c_int)\n-    }\n-}\n-\n-pub unsafe fn free_ip4_addr(addr: *sockaddr_in) {\n-    rust_uv_free_ip4_addr(addr);\n-}\n-\n-pub unsafe fn free_ip6_addr(addr: *sockaddr_in6) {\n-    rust_uv_free_ip6_addr(addr);\n-}\n-\n-// data access helpers\n-pub unsafe fn get_loop_for_uv_handle<T>(handle: *T) -> *c_void {\n-    return rust_uv_get_loop_for_uv_handle(handle as *c_void);\n-}\n-pub unsafe fn get_stream_handle_from_connect_req(connect: *uv_connect_t) -> *uv_stream_t {\n-    return rust_uv_get_stream_handle_from_connect_req(connect);\n-}\n-pub unsafe fn get_stream_handle_from_write_req(write_req: *uv_write_t) -> *uv_stream_t {\n-    return rust_uv_get_stream_handle_from_write_req(write_req);\n-}\n-pub unsafe fn get_data_for_uv_loop(loop_ptr: *c_void) -> *c_void {\n-    rust_uv_get_data_for_uv_loop(loop_ptr)\n-}\n-pub unsafe fn set_data_for_uv_loop(loop_ptr: *c_void, data: *c_void) {\n-    rust_uv_set_data_for_uv_loop(loop_ptr, data);\n-}\n-pub unsafe fn get_data_for_uv_handle<T>(handle: *T) -> *c_void {\n-    return rust_uv_get_data_for_uv_handle(handle as *c_void);\n-}\n-pub unsafe fn set_data_for_uv_handle<T, U>(handle: *T, data: *U) {\n-    rust_uv_set_data_for_uv_handle(handle as *c_void, data as *c_void);\n-}\n-pub unsafe fn get_data_for_req<T>(req: *T) -> *c_void {\n-    return rust_uv_get_data_for_req(req as *c_void);\n-}\n-pub unsafe fn set_data_for_req<T, U>(req: *T, data: *U) {\n-    rust_uv_set_data_for_req(req as *c_void, data as *c_void);\n-}\n-pub unsafe fn get_base_from_buf(buf: uv_buf_t) -> *u8 {\n-    return rust_uv_get_base_from_buf(buf);\n-}\n-pub unsafe fn get_len_from_buf(buf: uv_buf_t) -> size_t {\n-    return rust_uv_get_len_from_buf(buf);\n-}\n-pub unsafe fn malloc_buf_base_of(suggested_size: size_t) -> *u8 {\n-    return rust_uv_malloc_buf_base_of(suggested_size);\n-}\n-pub unsafe fn free_base_of_buf(buf: uv_buf_t) {\n-    rust_uv_free_base_of_buf(buf);\n-}\n-\n-pub unsafe fn get_last_err_info(uv_loop: *c_void) -> ~str {\n-    let err = last_error(uv_loop);\n-    let err_ptr = ptr::to_unsafe_ptr(&err);\n-    let err_name = str::raw::from_c_str(err_name(err_ptr));\n-    let err_msg = str::raw::from_c_str(strerror(err_ptr));\n-    return fmt!(\"LIBUV ERROR: name: %s msg: %s\",\n-                    err_name, err_msg);\n-}\n-\n-pub unsafe fn get_last_err_data(uv_loop: *c_void) -> uv_err_data {\n-    let err = last_error(uv_loop);\n-    let err_ptr = ptr::to_unsafe_ptr(&err);\n-    let err_name = str::raw::from_c_str(err_name(err_ptr));\n-    let err_msg = str::raw::from_c_str(strerror(err_ptr));\n-    uv_err_data { err_name: err_name, err_msg: err_msg }\n-}\n-\n-pub struct uv_err_data {\n-    err_name: ~str,\n-    err_msg: ~str,\n-}\n-\n-extern {\n-\n-    fn rust_uv_handle_size(type_: uintptr_t) -> size_t;\n-    fn rust_uv_req_size(type_: uintptr_t) -> size_t;\n-    fn rust_uv_handle_type_max() -> uintptr_t;\n-    fn rust_uv_req_type_max() -> uintptr_t;\n-\n-    // libuv public API\n-    fn rust_uv_loop_new() -> *c_void;\n-    fn rust_uv_loop_delete(lp: *c_void);\n-    fn rust_uv_run(loop_handle: *c_void);\n-    fn rust_uv_close(handle: *c_void, cb: *u8);\n-    fn rust_uv_walk(loop_handle: *c_void, cb: *u8, arg: *c_void);\n-\n-    fn rust_uv_idle_new() -> *uv_idle_t;\n-    fn rust_uv_idle_delete(handle: *uv_idle_t);\n-    fn rust_uv_idle_init(loop_handle: *uv_loop_t, handle: *uv_idle_t) -> c_int;\n-    fn rust_uv_idle_start(handle: *uv_idle_t, cb: uv_idle_cb) -> c_int;\n-    fn rust_uv_idle_stop(handle: *uv_idle_t) -> c_int;\n-\n-    fn rust_uv_async_send(handle: *uv_async_t);\n-    fn rust_uv_async_init(loop_handle: *c_void,\n-                          async_handle: *uv_async_t,\n-                          cb: *u8) -> c_int;\n-    fn rust_uv_tcp_init(loop_handle: *c_void, handle_ptr: *uv_tcp_t) -> c_int;\n-    // FIXME ref #2604 .. ?\n-    fn rust_uv_buf_init(out_buf: *uv_buf_t, base: *u8, len: size_t);\n-    fn rust_uv_last_error(loop_handle: *c_void) -> uv_err_t;\n-    // FIXME ref #2064\n-    fn rust_uv_strerror(err: *uv_err_t) -> *c_char;\n-    // FIXME ref #2064\n-    fn rust_uv_err_name(err: *uv_err_t) -> *c_char;\n-    fn rust_uv_ip4_addrp(ip: *u8, port: c_int) -> *sockaddr_in;\n-    fn rust_uv_ip6_addrp(ip: *u8, port: c_int) -> *sockaddr_in6;\n-    fn rust_uv_free_ip4_addr(addr: *sockaddr_in);\n-    fn rust_uv_free_ip6_addr(addr: *sockaddr_in6);\n-    fn rust_uv_ip4_name(src: *sockaddr_in, dst: *u8, size: size_t) -> c_int;\n-    fn rust_uv_ip6_name(src: *sockaddr_in6, dst: *u8, size: size_t) -> c_int;\n-    fn rust_uv_ip4_port(src: *sockaddr_in) -> c_uint;\n-    fn rust_uv_ip6_port(src: *sockaddr_in6) -> c_uint;\n-    // FIXME ref #2064\n-    fn rust_uv_tcp_connect(connect_ptr: *uv_connect_t,\n-                           tcp_handle_ptr: *uv_tcp_t,\n-                           after_cb: *u8,\n-                           addr: *sockaddr_in) -> c_int;\n-    // FIXME ref #2064\n-    fn rust_uv_tcp_bind(tcp_server: *uv_tcp_t, addr: *sockaddr_in) -> c_int;\n-    // FIXME ref #2064\n-    fn rust_uv_tcp_connect6(connect_ptr: *uv_connect_t,\n-                            tcp_handle_ptr: *uv_tcp_t,\n-                            after_cb: *u8,\n-                            addr: *sockaddr_in6) -> c_int;\n-    // FIXME ref #2064\n-    fn rust_uv_tcp_bind6(tcp_server: *uv_tcp_t, addr: *sockaddr_in6) -> c_int;\n-    fn rust_uv_tcp_getpeername(tcp_handle_ptr: *uv_tcp_t,\n-                               name: *sockaddr_in) -> c_int;\n-    fn rust_uv_tcp_getpeername6(tcp_handle_ptr: *uv_tcp_t,\n-                                name: *sockaddr_in6) ->c_int;\n-    fn rust_uv_listen(stream: *c_void, backlog: c_int, cb: *u8) -> c_int;\n-    fn rust_uv_accept(server: *c_void, client: *c_void) -> c_int;\n-    fn rust_uv_write(req: *c_void,\n-                     stream: *c_void,\n-                     buf_in: *uv_buf_t,\n-                     buf_cnt: c_int,\n-                     cb: *u8) -> c_int;\n-    fn rust_uv_read_start(stream: *c_void,\n-                          on_alloc: *u8,\n-                          on_read: *u8) -> c_int;\n-    fn rust_uv_read_stop(stream: *c_void) -> c_int;\n-    fn rust_uv_timer_init(loop_handle: *c_void,\n-                          timer_handle: *uv_timer_t) -> c_int;\n-    fn rust_uv_timer_start(timer_handle: *uv_timer_t,\n-                           cb: *u8,\n-                           timeout: c_uint,\n-                           repeat: c_uint) -> c_int;\n-    fn rust_uv_timer_stop(handle: *uv_timer_t) -> c_int;\n-\n-    fn rust_uv_malloc_buf_base_of(sug_size: size_t) -> *u8;\n-    fn rust_uv_free_base_of_buf(buf: uv_buf_t);\n-    fn rust_uv_get_stream_handle_from_connect_req(connect_req: *uv_connect_t) -> *uv_stream_t;\n-    fn rust_uv_get_stream_handle_from_write_req(write_req: *uv_write_t) -> *uv_stream_t;\n-    fn rust_uv_get_loop_for_uv_handle(handle: *c_void) -> *c_void;\n-    fn rust_uv_get_data_for_uv_loop(loop_ptr: *c_void) -> *c_void;\n-    fn rust_uv_set_data_for_uv_loop(loop_ptr: *c_void, data: *c_void);\n-    fn rust_uv_get_data_for_uv_handle(handle: *c_void) -> *c_void;\n-    fn rust_uv_set_data_for_uv_handle(handle: *c_void, data: *c_void);\n-    fn rust_uv_get_data_for_req(req: *c_void) -> *c_void;\n-    fn rust_uv_set_data_for_req(req: *c_void, data: *c_void);\n-    fn rust_uv_get_base_from_buf(buf: uv_buf_t) -> *u8;\n-    fn rust_uv_get_len_from_buf(buf: uv_buf_t) -> size_t;\n-}"}, {"sha": "8ac60ffb979c6aed6bcbe3d25cb923c205100eec", "filename": "src/libstd/sys.rs", "status": "modified", "additions": 12, "deletions": 7, "changes": 19, "blob_url": "https://github.com/rust-lang/rust/blob/357f087786cbd6516a38aff800cf9334bc5b85c5/src%2Flibstd%2Fsys.rs", "raw_url": "https://github.com/rust-lang/rust/raw/357f087786cbd6516a38aff800cf9334bc5b85c5/src%2Flibstd%2Fsys.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibstd%2Fsys.rs?ref=357f087786cbd6516a38aff800cf9334bc5b85c5", "patch": "@@ -180,10 +180,13 @@ impl FailWithCause for &'static str {\n \n // FIXME #4427: Temporary until rt::rt_fail_ goes away\n pub fn begin_unwind_(msg: *c_char, file: *c_char, line: size_t) -> ! {\n+    use cell::Cell;\n     use option::Option;\n+    use either::Left;\n     use rt::{context, OldTaskContext, TaskContext};\n     use rt::task::{Task, Unwinder};\n     use rt::local::Local;\n+    use rt::logging::Logger;\n \n     let context = context();\n     match context {\n@@ -200,24 +203,26 @@ pub fn begin_unwind_(msg: *c_char, file: *c_char, line: size_t) -> ! {\n                 let msg = str::raw::from_c_str(msg);\n                 let file = str::raw::from_c_str(file);\n \n-                let outmsg = fmt!(\"%s at line %i of file %s\", msg, line as int, file);\n+                let outmsg = fmt!(\"task failed: '%s' at line %i of file %s\",\n+                                  msg, line as int, file);\n \n                 // XXX: Logging doesn't work correctly in non-task context because it\n                 // invokes the local heap\n                 if context == TaskContext {\n-                    error!(outmsg);\n+                    // XXX: Logging doesn't work here - the check to call the log\n+                    // function never passes - so calling the log function directly.\n+                    let outmsg = Cell::new(outmsg);\n+                    do Local::borrow::<Task, ()> |task| {\n+                        task.logger.log(Left(outmsg.take()));\n+                    }\n                 } else {\n                     rtdebug!(\"%s\", outmsg);\n                 }\n \n                 gc::cleanup_stack_for_failure();\n \n                 let task = Local::unsafe_borrow::<Task>();\n-                let unwinder: &mut Option<Unwinder> = &mut (*task).unwinder;\n-                match *unwinder {\n-                    Some(ref mut unwinder) => unwinder.begin_unwind(),\n-                    None => abort!(\"failure without unwinder. aborting process\")\n-                }\n+                (*task).unwinder.begin_unwind();\n             }\n         }\n     }"}, {"sha": "99858feab224c6934d98362a1f95e7113eb46b56", "filename": "src/libstd/task/mod.rs", "status": "modified", "additions": 2, "deletions": 13, "changes": 15, "blob_url": "https://github.com/rust-lang/rust/blob/357f087786cbd6516a38aff800cf9334bc5b85c5/src%2Flibstd%2Ftask%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/357f087786cbd6516a38aff800cf9334bc5b85c5/src%2Flibstd%2Ftask%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibstd%2Ftask%2Fmod.rs?ref=357f087786cbd6516a38aff800cf9334bc5b85c5", "patch": "@@ -520,20 +520,9 @@ pub fn failing() -> bool {\n             }\n         }\n         _ => {\n-            let mut unwinding = false;\n-            do Local::borrow::<Task> |local| {\n-                unwinding = match local.unwinder {\n-                    Some(unwinder) => {\n-                        unwinder.unwinding\n-                    }\n-                    None => {\n-                        // Because there is no unwinder we can't be unwinding.\n-                        // (The process will abort on failure)\n-                        false\n-                    }\n-                }\n+            do Local::borrow::<Task, bool> |local| {\n+                local.unwinder.unwinding\n             }\n-            return unwinding;\n         }\n     }\n }"}, {"sha": "344a58a877fc142b582de6f6b769bd0d58e7dd65", "filename": "src/libstd/task/spawn.rs", "status": "modified", "additions": 8, "deletions": 2, "changes": 10, "blob_url": "https://github.com/rust-lang/rust/blob/357f087786cbd6516a38aff800cf9334bc5b85c5/src%2Flibstd%2Ftask%2Fspawn.rs", "raw_url": "https://github.com/rust-lang/rust/raw/357f087786cbd6516a38aff800cf9334bc5b85c5/src%2Flibstd%2Ftask%2Fspawn.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibstd%2Ftask%2Fspawn.rs?ref=357f087786cbd6516a38aff800cf9334bc5b85c5", "patch": "@@ -92,6 +92,7 @@ use util;\n use unstable::sync::{Exclusive, exclusive};\n use rt::local::Local;\n use iterator::{IteratorUtil};\n+use rt::task::Task;\n \n #[cfg(test)] use task::default_task_opts;\n #[cfg(test)] use comm;\n@@ -580,9 +581,14 @@ pub fn spawn_raw(opts: TaskOpts, f: ~fn()) {\n fn spawn_raw_newsched(_opts: TaskOpts, f: ~fn()) {\n     use rt::sched::*;\n \n+    let task = do Local::borrow::<Task, ~Task>() |running_task| {\n+        ~running_task.new_child()\n+    };\n+\n     let mut sched = Local::take::<Scheduler>();\n-    let task = ~Coroutine::new(&mut sched.stack_pool, f);\n-    sched.schedule_new_task(task);\n+    let task = ~Coroutine::with_task(&mut sched.stack_pool,\n+                                     task, f);\n+    sched.schedule_task(task);\n }\n \n fn spawn_raw_oldsched(mut opts: TaskOpts, f: ~fn()) {"}, {"sha": "fa067da2ebc76e4121af5f2e43674d915cf28d44", "filename": "src/libstd/unstable/lang.rs", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/rust-lang/rust/blob/357f087786cbd6516a38aff800cf9334bc5b85c5/src%2Flibstd%2Funstable%2Flang.rs", "raw_url": "https://github.com/rust-lang/rust/raw/357f087786cbd6516a38aff800cf9334bc5b85c5/src%2Flibstd%2Funstable%2Flang.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibstd%2Funstable%2Flang.rs?ref=357f087786cbd6516a38aff800cf9334bc5b85c5", "patch": "@@ -245,7 +245,7 @@ pub unsafe fn local_malloc(td: *c_char, size: uintptr_t) -> *c_char {\n         }\n         _ => {\n             let mut alloc = ::ptr::null();\n-            do Local::borrow::<Task> |task| {\n+            do Local::borrow::<Task,()> |task| {\n                 alloc = task.heap.alloc(td as *c_void, size as uint) as *c_char;\n             }\n             return alloc;\n@@ -263,7 +263,7 @@ pub unsafe fn local_free(ptr: *c_char) {\n             rustrt::rust_upcall_free_noswitch(ptr);\n         }\n         _ => {\n-            do Local::borrow::<Task> |task| {\n+            do Local::borrow::<Task,()> |task| {\n                 task.heap.free(ptr as *c_void);\n             }\n         }"}, {"sha": "162891124f60c041a52c3c79008d7b0bc288ff7e", "filename": "src/libstd/unstable/sync.rs", "status": "modified", "additions": 69, "deletions": 0, "changes": 69, "blob_url": "https://github.com/rust-lang/rust/blob/357f087786cbd6516a38aff800cf9334bc5b85c5/src%2Flibstd%2Funstable%2Fsync.rs", "raw_url": "https://github.com/rust-lang/rust/raw/357f087786cbd6516a38aff800cf9334bc5b85c5/src%2Flibstd%2Funstable%2Fsync.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibstd%2Funstable%2Fsync.rs?ref=357f087786cbd6516a38aff800cf9334bc5b85c5", "patch": "@@ -205,8 +205,53 @@ extern {\n     fn rust_unlock_little_lock(lock: rust_little_lock);\n }\n \n+/* *********************************************************************/\n+\n+//FIXME: #5042 This should be replaced by proper atomic type\n+pub struct AtomicUint {\n+    priv inner: uint\n+}\n+\n+impl AtomicUint {\n+    pub fn new(val: uint) -> AtomicUint { AtomicUint { inner: val } }\n+    pub fn load(&self) -> uint {\n+        unsafe { intrinsics::atomic_load(cast::transmute(self)) as uint }\n+    }\n+    pub fn store(&mut self, val: uint) {\n+        unsafe { intrinsics::atomic_store(cast::transmute(self), val as int); }\n+    }\n+    pub fn add(&mut self, val: int) -> uint {\n+        unsafe { intrinsics::atomic_xadd(cast::transmute(self), val as int) as uint }\n+    }\n+    pub fn cas(&mut self, old:uint, new: uint) -> uint {\n+        unsafe { intrinsics::atomic_cxchg(cast::transmute(self), old as int, new as int) as uint }\n+    }\n+}\n+\n+pub struct AtomicInt {\n+    priv inner: int\n+}\n+\n+impl AtomicInt {\n+    pub fn new(val: int) -> AtomicInt { AtomicInt { inner: val } }\n+    pub fn load(&self) -> int {\n+        unsafe { intrinsics::atomic_load(&self.inner) }\n+    }\n+    pub fn store(&mut self, val: int) {\n+        unsafe { intrinsics::atomic_store(&mut self.inner, val); }\n+    }\n+    pub fn add(&mut self, val: int) -> int {\n+        unsafe { intrinsics::atomic_xadd(&mut self.inner, val) }\n+    }\n+    pub fn cas(&mut self, old: int, new: int) -> int {\n+        unsafe { intrinsics::atomic_cxchg(&mut self.inner, old, new) }\n+    }\n+}\n+\n+\n #[cfg(test)]\n mod tests {\n+    use super::*;\n     use comm;\n     use super::exclusive;\n     use task;\n@@ -262,4 +307,28 @@ mod tests {\n             }\n         }\n     }\n+\n+    #[test]\n+    fn atomic_int_smoke_test() {\n+        let mut i = AtomicInt::new(0);\n+        i.store(10);\n+        assert!(i.load() == 10);\n+        assert!(i.add(1) == 10);\n+        assert!(i.load() == 11);\n+        assert!(i.cas(11, 12) == 11);\n+        assert!(i.cas(11, 13) == 12);\n+        assert!(i.load() == 12);\n+    }\n+\n+    #[test]\n+    fn atomic_uint_smoke_test() {\n+        let mut i = AtomicUint::new(0);\n+        i.store(10);\n+        assert!(i.load() == 10);\n+        assert!(i.add(1) == 10);\n+        assert!(i.load() == 11);\n+        assert!(i.cas(11, 12) == 11);\n+        assert!(i.cas(11, 13) == 12);\n+        assert!(i.load() == 12);\n+    }\n }"}, {"sha": "8e494cb577b55274835755d5f619ebe34b1fa6b9", "filename": "src/rt/rust_builtin.cpp", "status": "modified", "additions": 7, "deletions": 0, "changes": 7, "blob_url": "https://github.com/rust-lang/rust/blob/357f087786cbd6516a38aff800cf9334bc5b85c5/src%2Frt%2Frust_builtin.cpp", "raw_url": "https://github.com/rust-lang/rust/raw/357f087786cbd6516a38aff800cf9334bc5b85c5/src%2Frt%2Frust_builtin.cpp", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Frt%2Frust_builtin.cpp?ref=357f087786cbd6516a38aff800cf9334bc5b85c5", "patch": "@@ -925,6 +925,13 @@ rust_running_on_valgrind() {\n     return RUNNING_ON_VALGRIND;\n }\n \n+extern int get_num_cpus();\n+\n+extern \"C\" CDECL uintptr_t\n+rust_get_num_cpus() {\n+    return get_num_cpus();\n+}\n+\n //\n // Local Variables:\n // mode: C++"}, {"sha": "ff03ea817b868cc1c2ed0e06a1143b70b98ce116", "filename": "src/rt/rust_env.cpp", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "blob_url": "https://github.com/rust-lang/rust/blob/357f087786cbd6516a38aff800cf9334bc5b85c5/src%2Frt%2Frust_env.cpp", "raw_url": "https://github.com/rust-lang/rust/raw/357f087786cbd6516a38aff800cf9334bc5b85c5/src%2Frt%2Frust_env.cpp", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Frt%2Frust_env.cpp?ref=357f087786cbd6516a38aff800cf9334bc5b85c5", "patch": "@@ -40,15 +40,15 @@ rust_drop_env_lock() {\n }\n \n #if defined(__WIN32__)\n-static int\n+int\n get_num_cpus() {\n     SYSTEM_INFO sysinfo;\n     GetSystemInfo(&sysinfo);\n \n     return (int) sysinfo.dwNumberOfProcessors;\n }\n #elif defined(__BSD__)\n-static int\n+int\n get_num_cpus() {\n     /* swiped from http://stackoverflow.com/questions/150355/\n        programmatically-find-the-number-of-cores-on-a-machine */\n@@ -75,7 +75,7 @@ get_num_cpus() {\n     return numCPU;\n }\n #elif defined(__GNUC__)\n-static int\n+int\n get_num_cpus() {\n     return sysconf(_SC_NPROCESSORS_ONLN);\n }"}, {"sha": "e37856255a7d60a360873d326cb66360cecb20c8", "filename": "src/rt/rust_gc_metadata.cpp", "status": "modified", "additions": 5, "deletions": 0, "changes": 5, "blob_url": "https://github.com/rust-lang/rust/blob/357f087786cbd6516a38aff800cf9334bc5b85c5/src%2Frt%2Frust_gc_metadata.cpp", "raw_url": "https://github.com/rust-lang/rust/raw/357f087786cbd6516a38aff800cf9334bc5b85c5/src%2Frt%2Frust_gc_metadata.cpp", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Frt%2Frust_gc_metadata.cpp?ref=357f087786cbd6516a38aff800cf9334bc5b85c5", "patch": "@@ -79,6 +79,11 @@ rust_gc_metadata() {\n     return (void *)global_safe_points;\n }\n \n+extern \"C\" CDECL void\n+rust_update_gc_metadata(const void* map) {\n+    update_gc_metadata(map);\n+}\n+\n //\n // Local Variables:\n // mode: C++"}, {"sha": "425f17d4290073ba37919f835afe8ff4684a6a92", "filename": "src/rt/rustrt.def.in", "status": "modified", "additions": 3, "deletions": 1, "changes": 4, "blob_url": "https://github.com/rust-lang/rust/blob/357f087786cbd6516a38aff800cf9334bc5b85c5/src%2Frt%2Frustrt.def.in", "raw_url": "https://github.com/rust-lang/rust/raw/357f087786cbd6516a38aff800cf9334bc5b85c5/src%2Frt%2Frustrt.def.in", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Frt%2Frustrt.def.in?ref=357f087786cbd6516a38aff800cf9334bc5b85c5", "patch": "@@ -178,6 +178,7 @@ rust_call_tydesc_glue\n tdefl_compress_mem_to_heap\n tinfl_decompress_mem_to_heap\n rust_gc_metadata\n+rust_update_gc_metadata\n rust_uv_ip4_port\n rust_uv_ip6_port\n rust_uv_tcp_getpeername\n@@ -239,4 +240,5 @@ rust_valgrind_stack_deregister\n rust_take_env_lock\n rust_drop_env_lock\n rust_update_log_settings\n-rust_running_on_valgrind\n\\ No newline at end of file\n+rust_running_on_valgrind\n+rust_get_num_cpus"}]}