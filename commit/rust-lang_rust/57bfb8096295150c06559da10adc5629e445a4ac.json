{"sha": "57bfb8096295150c06559da10adc5629e445a4ac", "node_id": "MDY6Q29tbWl0NzI0NzEyOjU3YmZiODA5NjI5NTE1MGMwNjU1OWRhMTBhZGM1NjI5ZTQ0NWE0YWM=", "commit": {"author": {"name": "bors", "email": "bors@rust-lang.org", "date": "2019-10-22T12:01:41Z"}, "committer": {"name": "bors", "email": "bors@rust-lang.org", "date": "2019-10-22T12:01:41Z"}, "message": "Auto merge of #65503 - popzxc:refactor-libtest, r=wesleywiser\n\nRefactor libtest\n\n## Short overview\n\n`libtest` got refactored and splitted into smaller modules\n\n## Description\n\n`libtest` module is already pretty big and hard to understand. Everything is mixed up: CLI, console output, test execution, etc.\n\nThis PR splits `libtest` into smaller logically-consistent modules, makes big functions smaller and more readable, and adds more comments, so `libtest` will be easier to understand and maintain.\n\nAlthough there are a lot of changes, all the refactoring is \"soft\", meaning that no public interfaces were affected and nothing should be broken.\n\nThus this PR (at least should be) completely backward-compatible.\n\nr? @wesleywiser\ncc @Centril", "tree": {"sha": "2a5ad1bf8c65eab2366dde584dc7bf1566daefa2", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/2a5ad1bf8c65eab2366dde584dc7bf1566daefa2"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/57bfb8096295150c06559da10adc5629e445a4ac", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/57bfb8096295150c06559da10adc5629e445a4ac", "html_url": "https://github.com/rust-lang/rust/commit/57bfb8096295150c06559da10adc5629e445a4ac", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/57bfb8096295150c06559da10adc5629e445a4ac/comments", "author": {"login": "bors", "id": 3372342, "node_id": "MDQ6VXNlcjMzNzIzNDI=", "avatar_url": "https://avatars.githubusercontent.com/u/3372342?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bors", "html_url": "https://github.com/bors", "followers_url": "https://api.github.com/users/bors/followers", "following_url": "https://api.github.com/users/bors/following{/other_user}", "gists_url": "https://api.github.com/users/bors/gists{/gist_id}", "starred_url": "https://api.github.com/users/bors/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bors/subscriptions", "organizations_url": "https://api.github.com/users/bors/orgs", "repos_url": "https://api.github.com/users/bors/repos", "events_url": "https://api.github.com/users/bors/events{/privacy}", "received_events_url": "https://api.github.com/users/bors/received_events", "type": "User", "site_admin": false}, "committer": {"login": "bors", "id": 3372342, "node_id": "MDQ6VXNlcjMzNzIzNDI=", "avatar_url": "https://avatars.githubusercontent.com/u/3372342?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bors", "html_url": "https://github.com/bors", "followers_url": "https://api.github.com/users/bors/followers", "following_url": "https://api.github.com/users/bors/following{/other_user}", "gists_url": "https://api.github.com/users/bors/gists{/gist_id}", "starred_url": "https://api.github.com/users/bors/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bors/subscriptions", "organizations_url": "https://api.github.com/users/bors/orgs", "repos_url": "https://api.github.com/users/bors/repos", "events_url": "https://api.github.com/users/bors/events{/privacy}", "received_events_url": "https://api.github.com/users/bors/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "50ffa79589600f515ff2710830c23cd2dce7cb76", "url": "https://api.github.com/repos/rust-lang/rust/commits/50ffa79589600f515ff2710830c23cd2dce7cb76", "html_url": "https://github.com/rust-lang/rust/commit/50ffa79589600f515ff2710830c23cd2dce7cb76"}, {"sha": "ae04dc8473f9ea53b71123eb4eb0fcec71e6d797", "url": "https://api.github.com/repos/rust-lang/rust/commits/ae04dc8473f9ea53b71123eb4eb0fcec71e6d797", "html_url": "https://github.com/rust-lang/rust/commit/ae04dc8473f9ea53b71123eb4eb0fcec71e6d797"}], "stats": {"total": 3916, "additions": 2123, "deletions": 1793}, "files": [{"sha": "c142c5213d2e0c579a2645753baf497a56c26152", "filename": "src/libtest/bench.rs", "status": "added", "additions": 258, "deletions": 0, "changes": 258, "blob_url": "https://github.com/rust-lang/rust/blob/57bfb8096295150c06559da10adc5629e445a4ac/src%2Flibtest%2Fbench.rs", "raw_url": "https://github.com/rust-lang/rust/raw/57bfb8096295150c06559da10adc5629e445a4ac/src%2Flibtest%2Fbench.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibtest%2Fbench.rs?ref=57bfb8096295150c06559da10adc5629e445a4ac", "patch": "@@ -0,0 +1,258 @@\n+//! Benchmarking module.\n+pub use std::hint::black_box;\n+\n+use super::{\n+    event::CompletedTest,\n+    helpers::sink::Sink,\n+    options::BenchMode,\n+    types::TestDesc,\n+    test_result::TestResult,\n+    Sender,\n+};\n+\n+use crate::stats;\n+use std::time::{Duration, Instant};\n+use std::cmp;\n+use std::io;\n+use std::panic::{catch_unwind, AssertUnwindSafe};\n+use std::sync::{Arc, Mutex};\n+\n+/// Manager of the benchmarking runs.\n+///\n+/// This is fed into functions marked with `#[bench]` to allow for\n+/// set-up & tear-down before running a piece of code repeatedly via a\n+/// call to `iter`.\n+#[derive(Clone)]\n+pub struct Bencher {\n+    mode: BenchMode,\n+    summary: Option<stats::Summary>,\n+    pub bytes: u64,\n+}\n+\n+impl Bencher {\n+    /// Callback for benchmark functions to run in their body.\n+    pub fn iter<T, F>(&mut self, mut inner: F)\n+    where\n+        F: FnMut() -> T,\n+    {\n+        if self.mode == BenchMode::Single {\n+            ns_iter_inner(&mut inner, 1);\n+            return;\n+        }\n+\n+        self.summary = Some(iter(&mut inner));\n+    }\n+\n+    pub fn bench<F>(&mut self, mut f: F) -> Option<stats::Summary>\n+    where\n+        F: FnMut(&mut Bencher),\n+    {\n+        f(self);\n+        return self.summary;\n+    }\n+}\n+\n+#[derive(Debug, Clone, PartialEq)]\n+pub struct BenchSamples {\n+    pub ns_iter_summ: stats::Summary,\n+    pub mb_s: usize,\n+}\n+\n+pub fn fmt_bench_samples(bs: &BenchSamples) -> String {\n+    use std::fmt::Write;\n+    let mut output = String::new();\n+\n+    let median = bs.ns_iter_summ.median as usize;\n+    let deviation = (bs.ns_iter_summ.max - bs.ns_iter_summ.min) as usize;\n+\n+    output\n+        .write_fmt(format_args!(\n+            \"{:>11} ns/iter (+/- {})\",\n+            fmt_thousands_sep(median, ','),\n+            fmt_thousands_sep(deviation, ',')\n+        ))\n+        .unwrap();\n+    if bs.mb_s != 0 {\n+        output\n+            .write_fmt(format_args!(\" = {} MB/s\", bs.mb_s))\n+            .unwrap();\n+    }\n+    output\n+}\n+\n+// Format a number with thousands separators\n+fn fmt_thousands_sep(mut n: usize, sep: char) -> String {\n+    use std::fmt::Write;\n+    let mut output = String::new();\n+    let mut trailing = false;\n+    for &pow in &[9, 6, 3, 0] {\n+        let base = 10_usize.pow(pow);\n+        if pow == 0 || trailing || n / base != 0 {\n+            if !trailing {\n+                output.write_fmt(format_args!(\"{}\", n / base)).unwrap();\n+            } else {\n+                output.write_fmt(format_args!(\"{:03}\", n / base)).unwrap();\n+            }\n+            if pow != 0 {\n+                output.push(sep);\n+            }\n+            trailing = true;\n+        }\n+        n %= base;\n+    }\n+\n+    output\n+}\n+\n+fn ns_from_dur(dur: Duration) -> u64 {\n+    dur.as_secs() * 1_000_000_000 + (dur.subsec_nanos() as u64)\n+}\n+\n+fn ns_iter_inner<T, F>(inner: &mut F, k: u64) -> u64\n+where\n+    F: FnMut() -> T,\n+{\n+    let start = Instant::now();\n+    for _ in 0..k {\n+        black_box(inner());\n+    }\n+    return ns_from_dur(start.elapsed());\n+}\n+\n+pub fn iter<T, F>(inner: &mut F) -> stats::Summary\n+where\n+    F: FnMut() -> T,\n+{\n+    // Initial bench run to get ballpark figure.\n+    let ns_single = ns_iter_inner(inner, 1);\n+\n+    // Try to estimate iter count for 1ms falling back to 1m\n+    // iterations if first run took < 1ns.\n+    let ns_target_total = 1_000_000; // 1ms\n+    let mut n = ns_target_total / cmp::max(1, ns_single);\n+\n+    // if the first run took more than 1ms we don't want to just\n+    // be left doing 0 iterations on every loop. The unfortunate\n+    // side effect of not being able to do as many runs is\n+    // automatically handled by the statistical analysis below\n+    // (i.e., larger error bars).\n+    n = cmp::max(1, n);\n+\n+    let mut total_run = Duration::new(0, 0);\n+    let samples: &mut [f64] = &mut [0.0_f64; 50];\n+    loop {\n+        let loop_start = Instant::now();\n+\n+        for p in &mut *samples {\n+            *p = ns_iter_inner(inner, n) as f64 / n as f64;\n+        }\n+\n+        stats::winsorize(samples, 5.0);\n+        let summ = stats::Summary::new(samples);\n+\n+        for p in &mut *samples {\n+            let ns = ns_iter_inner(inner, 5 * n);\n+            *p = ns as f64 / (5 * n) as f64;\n+        }\n+\n+        stats::winsorize(samples, 5.0);\n+        let summ5 = stats::Summary::new(samples);\n+\n+        let loop_run = loop_start.elapsed();\n+\n+        // If we've run for 100ms and seem to have converged to a\n+        // stable median.\n+        if loop_run > Duration::from_millis(100)\n+            && summ.median_abs_dev_pct < 1.0\n+            && summ.median - summ5.median < summ5.median_abs_dev\n+        {\n+            return summ5;\n+        }\n+\n+        total_run = total_run + loop_run;\n+        // Longest we ever run for is 3s.\n+        if total_run > Duration::from_secs(3) {\n+            return summ5;\n+        }\n+\n+        // If we overflow here just return the results so far. We check a\n+        // multiplier of 10 because we're about to multiply by 2 and the\n+        // next iteration of the loop will also multiply by 5 (to calculate\n+        // the summ5 result)\n+        n = match n.checked_mul(10) {\n+            Some(_) => n * 2,\n+            None => {\n+                return summ5;\n+            }\n+        };\n+    }\n+}\n+\n+pub fn benchmark<F>(desc: TestDesc, monitor_ch: Sender<CompletedTest>, nocapture: bool, f: F)\n+where\n+    F: FnMut(&mut Bencher),\n+{\n+    let mut bs = Bencher {\n+        mode: BenchMode::Auto,\n+        summary: None,\n+        bytes: 0,\n+    };\n+\n+    let data = Arc::new(Mutex::new(Vec::new()));\n+    let oldio = if !nocapture {\n+        Some((\n+            io::set_print(Some(Sink::new_boxed(&data))),\n+            io::set_panic(Some(Sink::new_boxed(&data))),\n+        ))\n+    } else {\n+        None\n+    };\n+\n+    let result = catch_unwind(AssertUnwindSafe(|| bs.bench(f)));\n+\n+    if let Some((printio, panicio)) = oldio {\n+        io::set_print(printio);\n+        io::set_panic(panicio);\n+    }\n+\n+    let test_result = match result {\n+        //bs.bench(f) {\n+        Ok(Some(ns_iter_summ)) => {\n+            let ns_iter = cmp::max(ns_iter_summ.median as u64, 1);\n+            let mb_s = bs.bytes * 1000 / ns_iter;\n+\n+            let bs = BenchSamples {\n+                ns_iter_summ,\n+                mb_s: mb_s as usize,\n+            };\n+            TestResult::TrBench(bs)\n+        }\n+        Ok(None) => {\n+            // iter not called, so no data.\n+            // FIXME: error in this case?\n+            let samples: &mut [f64] = &mut [0.0_f64; 1];\n+            let bs = BenchSamples {\n+                ns_iter_summ: stats::Summary::new(samples),\n+                mb_s: 0,\n+            };\n+            TestResult::TrBench(bs)\n+        }\n+        Err(_) => TestResult::TrFailed,\n+    };\n+\n+    let stdout = data.lock().unwrap().to_vec();\n+    let message = CompletedTest::new(desc, test_result, None, stdout);\n+    monitor_ch.send(message).unwrap();\n+}\n+\n+pub fn run_once<F>(f: F)\n+where\n+    F: FnMut(&mut Bencher),\n+{\n+    let mut bs = Bencher {\n+        mode: BenchMode::Single,\n+        summary: None,\n+        bytes: 0,\n+    };\n+    bs.bench(f);\n+}"}, {"sha": "f95d5aad18a654792815573564f12259244e0a29", "filename": "src/libtest/cli.rs", "status": "added", "additions": 444, "deletions": 0, "changes": 444, "blob_url": "https://github.com/rust-lang/rust/blob/57bfb8096295150c06559da10adc5629e445a4ac/src%2Flibtest%2Fcli.rs", "raw_url": "https://github.com/rust-lang/rust/raw/57bfb8096295150c06559da10adc5629e445a4ac/src%2Flibtest%2Fcli.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibtest%2Fcli.rs?ref=57bfb8096295150c06559da10adc5629e445a4ac", "patch": "@@ -0,0 +1,444 @@\n+//! Module converting command-line arguments into test configuration.\n+\n+use std::env;\n+use std::path::PathBuf;\n+use getopts;\n+\n+use super::options::{RunIgnored, ColorConfig, OutputFormat, Options};\n+use super::time::TestTimeOptions;\n+use super::helpers::isatty;\n+\n+#[derive(Debug)]\n+pub struct TestOpts {\n+    pub list: bool,\n+    pub filter: Option<String>,\n+    pub filter_exact: bool,\n+    pub exclude_should_panic: bool,\n+    pub run_ignored: RunIgnored,\n+    pub run_tests: bool,\n+    pub bench_benchmarks: bool,\n+    pub logfile: Option<PathBuf>,\n+    pub nocapture: bool,\n+    pub color: ColorConfig,\n+    pub format: OutputFormat,\n+    pub test_threads: Option<usize>,\n+    pub skip: Vec<String>,\n+    pub time_options: Option<TestTimeOptions>,\n+    pub options: Options,\n+}\n+\n+impl TestOpts {\n+    pub fn use_color(&self) -> bool {\n+        match self.color {\n+            ColorConfig::AutoColor => !self.nocapture && isatty::stdout_isatty(),\n+            ColorConfig::AlwaysColor => true,\n+            ColorConfig::NeverColor => false,\n+        }\n+    }\n+}\n+\n+/// Result of parsing the options.\n+pub type OptRes = Result<TestOpts, String>;\n+/// Result of parsing the option part.\n+type OptPartRes<T> = Result<T, String>;\n+\n+fn optgroups() -> getopts::Options {\n+    let mut opts = getopts::Options::new();\n+    opts.optflag(\"\", \"include-ignored\", \"Run ignored and not ignored tests\")\n+        .optflag(\"\", \"ignored\", \"Run only ignored tests\")\n+        .optflag(\"\", \"exclude-should-panic\", \"Excludes tests marked as should_panic\")\n+        .optflag(\"\", \"test\", \"Run tests and not benchmarks\")\n+        .optflag(\"\", \"bench\", \"Run benchmarks instead of tests\")\n+        .optflag(\"\", \"list\", \"List all tests and benchmarks\")\n+        .optflag(\"h\", \"help\", \"Display this message (longer with --help)\")\n+        .optopt(\n+            \"\",\n+            \"logfile\",\n+            \"Write logs to the specified file instead \\\n+             of stdout\",\n+            \"PATH\",\n+        )\n+        .optflag(\n+            \"\",\n+            \"nocapture\",\n+            \"don't capture stdout/stderr of each \\\n+             task, allow printing directly\",\n+        )\n+        .optopt(\n+            \"\",\n+            \"test-threads\",\n+            \"Number of threads used for running tests \\\n+             in parallel\",\n+            \"n_threads\",\n+        )\n+        .optmulti(\n+            \"\",\n+            \"skip\",\n+            \"Skip tests whose names contain FILTER (this flag can \\\n+             be used multiple times)\",\n+            \"FILTER\",\n+        )\n+        .optflag(\n+            \"q\",\n+            \"quiet\",\n+            \"Display one character per test instead of one line. \\\n+             Alias to --format=terse\",\n+        )\n+        .optflag(\n+            \"\",\n+            \"exact\",\n+            \"Exactly match filters rather than by substring\",\n+        )\n+        .optopt(\n+            \"\",\n+            \"color\",\n+            \"Configure coloring of output:\n+            auto   = colorize if stdout is a tty and tests are run on serially (default);\n+            always = always colorize output;\n+            never  = never colorize output;\",\n+            \"auto|always|never\",\n+        )\n+        .optopt(\n+            \"\",\n+            \"format\",\n+            \"Configure formatting of output:\n+            pretty = Print verbose output;\n+            terse  = Display one character per test;\n+            json   = Output a json document\",\n+            \"pretty|terse|json\",\n+        )\n+        .optflag(\n+            \"\",\n+            \"show-output\",\n+            \"Show captured stdout of successful tests\"\n+        )\n+        .optopt(\n+            \"Z\",\n+            \"\",\n+            \"Enable nightly-only flags:\n+            unstable-options = Allow use of experimental features\",\n+            \"unstable-options\",\n+        )\n+        .optflagopt(\n+            \"\",\n+            \"report-time\",\n+            \"Show execution time of each test. Awailable values:\n+            plain   = do not colorize the execution time (default);\n+            colored = colorize output according to the `color` parameter value;\n+\n+            Threshold values for colorized output can be configured via\n+            `RUST_TEST_TIME_UNIT`, `RUST_TEST_TIME_INTEGRATION` and\n+            `RUST_TEST_TIME_DOCTEST` environment variables.\n+\n+            Expected format of environment variable is `VARIABLE=WARN_TIME,CRITICAL_TIME`.\n+\n+            Not available for --format=terse\",\n+            \"plain|colored\"\n+        )\n+        .optflag(\n+            \"\",\n+            \"ensure-time\",\n+            \"Treat excess of the test execution time limit as error.\n+\n+            Threshold values for this option can be configured via\n+            `RUST_TEST_TIME_UNIT`, `RUST_TEST_TIME_INTEGRATION` and\n+            `RUST_TEST_TIME_DOCTEST` environment variables.\n+\n+            Expected format of environment variable is `VARIABLE=WARN_TIME,CRITICAL_TIME`.\n+\n+            `CRITICAL_TIME` here means the limit that should not be exceeded by test.\n+            \"\n+        );\n+    return opts;\n+}\n+\n+fn usage(binary: &str, options: &getopts::Options) {\n+    let message = format!(\"Usage: {} [OPTIONS] [FILTER]\", binary);\n+    println!(\n+        r#\"{usage}\n+\n+The FILTER string is tested against the name of all tests, and only those\n+tests whose names contain the filter are run.\n+\n+By default, all tests are run in parallel. This can be altered with the\n+--test-threads flag or the RUST_TEST_THREADS environment variable when running\n+tests (set it to 1).\n+\n+All tests have their standard output and standard error captured by default.\n+This can be overridden with the --nocapture flag or setting RUST_TEST_NOCAPTURE\n+environment variable to a value other than \"0\". Logging is not captured by default.\n+\n+Test Attributes:\n+\n+    `#[test]`        - Indicates a function is a test to be run. This function\n+                       takes no arguments.\n+    `#[bench]`       - Indicates a function is a benchmark to be run. This\n+                       function takes one argument (test::Bencher).\n+    `#[should_panic]` - This function (also labeled with `#[test]`) will only pass if\n+                        the code causes a panic (an assertion failure or panic!)\n+                        A message may be provided, which the failure string must\n+                        contain: #[should_panic(expected = \"foo\")].\n+    `#[ignore]`       - When applied to a function which is already attributed as a\n+                        test, then the test runner will ignore these tests during\n+                        normal test runs. Running with --ignored or --include-ignored will run\n+                        these tests.\"#,\n+        usage = options.usage(&message)\n+    );\n+}\n+\n+/// Parses command line arguments into test options.\n+/// Returns `None` if help was requested (since we only show help message and don't run tests),\n+/// returns `Some(Err(..))` if provided arguments are incorrect,\n+/// otherwise creates a `TestOpts` object and returns it.\n+pub fn parse_opts(args: &[String]) -> Option<OptRes> {\n+    // Parse matches.\n+    let opts = optgroups();\n+    let args = args.get(1..).unwrap_or(args);\n+    let matches = match opts.parse(args) {\n+        Ok(m) => m,\n+        Err(f) => return Some(Err(f.to_string())),\n+    };\n+\n+    // Check if help was requested.\n+    if matches.opt_present(\"h\") {\n+        // Show help and do nothing more.\n+        usage(&args[0], &opts);\n+        return None;\n+    }\n+\n+    // Actually parse the opts.\n+    let opts_result = parse_opts_impl(matches);\n+\n+    Some(opts_result)\n+}\n+\n+// Gets the option value and checks if unstable features are enabled.\n+macro_rules! unstable_optflag {\n+    ($matches:ident, $allow_unstable:ident, $option_name:literal) => {{\n+        let opt = $matches.opt_present($option_name);\n+        if !$allow_unstable && opt {\n+            return Err(format!(\n+                \"The \\\"{}\\\" flag is only accepted on the nightly compiler\",\n+                $option_name\n+            ));\n+        }\n+\n+        opt\n+    }};\n+}\n+\n+// Implementation of `parse_opts` that doesn't care about help message\n+// and returns a `Result`.\n+fn parse_opts_impl(matches: getopts::Matches) -> OptRes {\n+    let allow_unstable = get_allow_unstable(&matches)?;\n+\n+    // Unstable flags\n+    let exclude_should_panic = unstable_optflag!(matches, allow_unstable, \"exclude-should-panic\");\n+    let include_ignored = unstable_optflag!(matches, allow_unstable, \"include-ignored\");\n+    let time_options = get_time_options(&matches, allow_unstable)?;\n+\n+    let quiet = matches.opt_present(\"quiet\");\n+    let exact = matches.opt_present(\"exact\");\n+    let list = matches.opt_present(\"list\");\n+    let skip = matches.opt_strs(\"skip\");\n+\n+    let bench_benchmarks = matches.opt_present(\"bench\");\n+    let run_tests = !bench_benchmarks || matches.opt_present(\"test\");\n+\n+    let logfile = get_log_file(&matches)?;\n+    let run_ignored = get_run_ignored(&matches, include_ignored)?;\n+    let filter = get_filter(&matches)?;\n+    let nocapture = get_nocapture(&matches)?;\n+    let test_threads = get_test_threads(&matches)?;\n+    let color = get_color_config(&matches)?;\n+    let format = get_format(&matches, quiet, allow_unstable)?;\n+\n+    let options = Options::new().display_output(matches.opt_present(\"show-output\"));\n+\n+    let test_opts = TestOpts {\n+        list,\n+        filter,\n+        filter_exact: exact,\n+        exclude_should_panic,\n+        run_ignored,\n+        run_tests,\n+        bench_benchmarks,\n+        logfile,\n+        nocapture,\n+        color,\n+        format,\n+        test_threads,\n+        skip,\n+        time_options,\n+        options,\n+    };\n+\n+    Ok(test_opts)\n+}\n+\n+// FIXME: Copied from libsyntax until linkage errors are resolved. Issue #47566\n+fn is_nightly() -> bool {\n+    // Whether this is a feature-staged build, i.e., on the beta or stable channel\n+    let disable_unstable_features = option_env!(\"CFG_DISABLE_UNSTABLE_FEATURES\").is_some();\n+    // Whether we should enable unstable features for bootstrapping\n+    let bootstrap = env::var(\"RUSTC_BOOTSTRAP\").is_ok();\n+\n+    bootstrap || !disable_unstable_features\n+}\n+\n+// Gets the CLI options assotiated with `report-time` feature.\n+fn get_time_options(\n+    matches: &getopts::Matches,\n+    allow_unstable: bool)\n+-> OptPartRes<Option<TestTimeOptions>> {\n+    let report_time = unstable_optflag!(matches, allow_unstable, \"report-time\");\n+    let colored_opt_str = matches.opt_str(\"report-time\");\n+    let mut report_time_colored = report_time && colored_opt_str == Some(\"colored\".into());\n+    let ensure_test_time = unstable_optflag!(matches, allow_unstable, \"ensure-time\");\n+\n+    // If `ensure-test-time` option is provided, time output is enforced,\n+    // so user won't be confused if any of tests will silently fail.\n+    let options = if report_time || ensure_test_time {\n+        if ensure_test_time && !report_time {\n+            report_time_colored = true;\n+        }\n+        Some(TestTimeOptions::new_from_env(ensure_test_time, report_time_colored))\n+    } else {\n+        None\n+    };\n+\n+    Ok(options)\n+}\n+\n+fn get_test_threads(matches: &getopts::Matches) -> OptPartRes<Option<usize>> {\n+    let test_threads = match matches.opt_str(\"test-threads\") {\n+        Some(n_str) => match n_str.parse::<usize>() {\n+            Ok(0) => return Err(\"argument for --test-threads must not be 0\".to_string()),\n+            Ok(n) => Some(n),\n+            Err(e) => {\n+                return Err(format!(\n+                    \"argument for --test-threads must be a number > 0 \\\n+                     (error: {})\",\n+                    e\n+                ));\n+            }\n+        },\n+        None => None,\n+    };\n+\n+    Ok(test_threads)\n+}\n+\n+fn get_format(\n+    matches: &getopts::Matches,\n+    quiet: bool,\n+    allow_unstable: bool\n+) -> OptPartRes<OutputFormat> {\n+    let format = match matches.opt_str(\"format\").as_ref().map(|s| &**s) {\n+        None if quiet => OutputFormat::Terse,\n+        Some(\"pretty\") | None => OutputFormat::Pretty,\n+        Some(\"terse\") => OutputFormat::Terse,\n+        Some(\"json\") => {\n+            if !allow_unstable {\n+                return Err(\n+                    \"The \\\"json\\\" format is only accepted on the nightly compiler\".into(),\n+                );\n+            }\n+            OutputFormat::Json\n+        }\n+\n+        Some(v) => {\n+            return Err(format!(\n+                \"argument for --format must be pretty, terse, or json (was \\\n+                 {})\",\n+                v\n+            ));\n+        }\n+    };\n+\n+    Ok(format)\n+}\n+\n+fn get_color_config(matches: &getopts::Matches) -> OptPartRes<ColorConfig> {\n+    let color = match matches.opt_str(\"color\").as_ref().map(|s| &**s) {\n+        Some(\"auto\") | None => ColorConfig::AutoColor,\n+        Some(\"always\") => ColorConfig::AlwaysColor,\n+        Some(\"never\") => ColorConfig::NeverColor,\n+\n+        Some(v) => {\n+            return Err(format!(\n+                \"argument for --color must be auto, always, or never (was \\\n+                 {})\",\n+                v\n+            ));\n+        }\n+    };\n+\n+    Ok(color)\n+}\n+\n+fn get_nocapture(matches: &getopts::Matches) -> OptPartRes<bool> {\n+    let mut nocapture = matches.opt_present(\"nocapture\");\n+    if !nocapture {\n+        nocapture = match env::var(\"RUST_TEST_NOCAPTURE\") {\n+            Ok(val) => &val != \"0\",\n+            Err(_) => false,\n+        };\n+    }\n+\n+    Ok(nocapture)\n+}\n+\n+fn get_run_ignored(matches: &getopts::Matches, include_ignored: bool) -> OptPartRes<RunIgnored> {\n+    let run_ignored = match (include_ignored, matches.opt_present(\"ignored\")) {\n+        (true, true) => {\n+            return Err(\n+                \"the options --include-ignored and --ignored are mutually exclusive\".into(),\n+            );\n+        }\n+        (true, false) => RunIgnored::Yes,\n+        (false, true) => RunIgnored::Only,\n+        (false, false) => RunIgnored::No,\n+    };\n+\n+    Ok(run_ignored)\n+}\n+\n+fn get_filter(matches: &getopts::Matches) -> OptPartRes<Option<String>> {\n+    let filter = if !matches.free.is_empty() {\n+        Some(matches.free[0].clone())\n+    } else {\n+        None\n+    };\n+\n+    Ok(filter)\n+}\n+\n+fn get_allow_unstable(matches: &getopts::Matches) -> OptPartRes<bool> {\n+    let mut allow_unstable = false;\n+\n+    if let Some(opt) = matches.opt_str(\"Z\") {\n+        if !is_nightly() {\n+            return Err(\n+                \"the option `Z` is only accepted on the nightly compiler\".into(),\n+            );\n+        }\n+\n+        match &*opt {\n+            \"unstable-options\" => {\n+                allow_unstable = true;\n+            }\n+            _ => {\n+                return Err(\"Unrecognized option to `Z`\".into());\n+            }\n+        }\n+    };\n+\n+    Ok(allow_unstable)\n+}\n+\n+fn get_log_file(matches: &getopts::Matches) -> OptPartRes<Option<PathBuf>> {\n+    let logfile = matches.opt_str(\"logfile\").map(|s| PathBuf::from(&s));\n+\n+    Ok(logfile)\n+}"}, {"sha": "e17030726ceaa0e721b828bcdcc7a349602a72b6", "filename": "src/libtest/console.rs", "status": "added", "additions": 308, "deletions": 0, "changes": 308, "blob_url": "https://github.com/rust-lang/rust/blob/57bfb8096295150c06559da10adc5629e445a4ac/src%2Flibtest%2Fconsole.rs", "raw_url": "https://github.com/rust-lang/rust/raw/57bfb8096295150c06559da10adc5629e445a4ac/src%2Flibtest%2Fconsole.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibtest%2Fconsole.rs?ref=57bfb8096295150c06559da10adc5629e445a4ac", "patch": "@@ -0,0 +1,308 @@\n+//! Module providing interface for running tests in the console.\n+\n+use std::fs::File;\n+use std::io::prelude::Write;\n+use std::io;\n+\n+use term;\n+\n+use super::{\n+    bench::fmt_bench_samples,\n+    cli::TestOpts,\n+    event::{TestEvent, CompletedTest},\n+    formatters::{JsonFormatter, OutputFormatter, PrettyFormatter, TerseFormatter},\n+    helpers::{\n+        concurrency::get_concurrency,\n+        metrics::MetricMap,\n+    },\n+    types::{TestDesc, TestDescAndFn, NamePadding},\n+    options::{Options, OutputFormat},\n+    test_result::TestResult,\n+    time::TestExecTime,\n+    run_tests,\n+    filter_tests,\n+};\n+\n+/// Generic wrapper over stdout.\n+pub enum OutputLocation<T> {\n+    Pretty(Box<term::StdoutTerminal>),\n+    Raw(T),\n+}\n+\n+impl<T: Write> Write for OutputLocation<T> {\n+    fn write(&mut self, buf: &[u8]) -> io::Result<usize> {\n+        match *self {\n+            OutputLocation::Pretty(ref mut term) => term.write(buf),\n+            OutputLocation::Raw(ref mut stdout) => stdout.write(buf),\n+        }\n+    }\n+\n+    fn flush(&mut self) -> io::Result<()> {\n+        match *self {\n+            OutputLocation::Pretty(ref mut term) => term.flush(),\n+            OutputLocation::Raw(ref mut stdout) => stdout.flush(),\n+        }\n+    }\n+}\n+\n+pub struct ConsoleTestState {\n+    pub log_out: Option<File>,\n+    pub total: usize,\n+    pub passed: usize,\n+    pub failed: usize,\n+    pub ignored: usize,\n+    pub allowed_fail: usize,\n+    pub filtered_out: usize,\n+    pub measured: usize,\n+    pub metrics: MetricMap,\n+    pub failures: Vec<(TestDesc, Vec<u8>)>,\n+    pub not_failures: Vec<(TestDesc, Vec<u8>)>,\n+    pub time_failures: Vec<(TestDesc, Vec<u8>)>,\n+    pub options: Options,\n+}\n+\n+impl ConsoleTestState {\n+    pub fn new(opts: &TestOpts) -> io::Result<ConsoleTestState> {\n+        let log_out = match opts.logfile {\n+            Some(ref path) => Some(File::create(path)?),\n+            None => None,\n+        };\n+\n+        Ok(ConsoleTestState {\n+            log_out,\n+            total: 0,\n+            passed: 0,\n+            failed: 0,\n+            ignored: 0,\n+            allowed_fail: 0,\n+            filtered_out: 0,\n+            measured: 0,\n+            metrics: MetricMap::new(),\n+            failures: Vec::new(),\n+            not_failures: Vec::new(),\n+            time_failures: Vec::new(),\n+            options: opts.options,\n+        })\n+    }\n+\n+    pub fn write_log<F, S>(\n+        &mut self,\n+        msg: F,\n+    ) -> io::Result<()>\n+    where\n+        S: AsRef<str>,\n+        F: FnOnce() -> S,\n+    {\n+        match self.log_out {\n+            None => Ok(()),\n+            Some(ref mut o) => {\n+                let msg = msg();\n+                let msg = msg.as_ref();\n+                o.write_all(msg.as_bytes())\n+            },\n+        }\n+    }\n+\n+    pub fn write_log_result(&mut self,test: &TestDesc,\n+        result: &TestResult,\n+        exec_time: Option<&TestExecTime>,\n+    ) -> io::Result<()> {\n+        self.write_log(|| format!(\n+            \"{} {}\",\n+            match *result {\n+                TestResult::TrOk => \"ok\".to_owned(),\n+                TestResult::TrFailed => \"failed\".to_owned(),\n+                TestResult::TrFailedMsg(ref msg) => format!(\"failed: {}\", msg),\n+                TestResult::TrIgnored => \"ignored\".to_owned(),\n+                TestResult::TrAllowedFail => \"failed (allowed)\".to_owned(),\n+                TestResult::TrBench(ref bs) => fmt_bench_samples(bs),\n+                TestResult::TrTimedFail => \"failed (time limit exceeded)\".to_owned(),\n+            },\n+            test.name,\n+        ))?;\n+        if let Some(exec_time) = exec_time {\n+            self.write_log(|| format!(\" <{}>\", exec_time))?;\n+        }\n+        self.write_log(|| \"\\n\")\n+    }\n+\n+    fn current_test_count(&self) -> usize {\n+        self.passed + self.failed + self.ignored + self.measured + self.allowed_fail\n+    }\n+}\n+\n+// List the tests to console, and optionally to logfile. Filters are honored.\n+pub fn list_tests_console(opts: &TestOpts, tests: Vec<TestDescAndFn>) -> io::Result<()> {\n+    let mut output = match term::stdout() {\n+        None => OutputLocation::Raw(io::stdout()),\n+        Some(t) => OutputLocation::Pretty(t),\n+    };\n+\n+    let quiet = opts.format == OutputFormat::Terse;\n+    let mut st = ConsoleTestState::new(opts)?;\n+\n+    let mut ntest = 0;\n+    let mut nbench = 0;\n+\n+    for test in filter_tests(&opts, tests) {\n+        use crate::TestFn::*;\n+\n+        let TestDescAndFn {\n+            desc: TestDesc { name, .. },\n+            testfn,\n+        } = test;\n+\n+        let fntype = match testfn {\n+            StaticTestFn(..) | DynTestFn(..) => {\n+                ntest += 1;\n+                \"test\"\n+            }\n+            StaticBenchFn(..) | DynBenchFn(..) => {\n+                nbench += 1;\n+                \"benchmark\"\n+            }\n+        };\n+\n+        writeln!(output, \"{}: {}\", name, fntype)?;\n+        st.write_log(|| format!(\"{} {}\\n\", fntype, name))?;\n+    }\n+\n+    fn plural(count: u32, s: &str) -> String {\n+        match count {\n+            1 => format!(\"{} {}\", 1, s),\n+            n => format!(\"{} {}s\", n, s),\n+        }\n+    }\n+\n+    if !quiet {\n+        if ntest != 0 || nbench != 0 {\n+            writeln!(output, \"\")?;\n+        }\n+\n+        writeln!(\n+            output,\n+            \"{}, {}\",\n+            plural(ntest, \"test\"),\n+            plural(nbench, \"benchmark\")\n+        )?;\n+    }\n+\n+    Ok(())\n+}\n+\n+// Updates `ConsoleTestState` depending on result of the test execution.\n+fn handle_test_result(st: &mut ConsoleTestState, completed_test: CompletedTest) {\n+    let test = completed_test.desc;\n+    let stdout = completed_test.stdout;\n+    match completed_test.result {\n+        TestResult::TrOk => {\n+            st.passed += 1;\n+            st.not_failures.push((test, stdout));\n+        }\n+        TestResult::TrIgnored => st.ignored += 1,\n+        TestResult::TrAllowedFail => st.allowed_fail += 1,\n+        TestResult::TrBench(bs) => {\n+            st.metrics.insert_metric(\n+                test.name.as_slice(),\n+                bs.ns_iter_summ.median,\n+                bs.ns_iter_summ.max - bs.ns_iter_summ.min,\n+            );\n+            st.measured += 1\n+        }\n+        TestResult::TrFailed => {\n+            st.failed += 1;\n+            st.failures.push((test, stdout));\n+        }\n+        TestResult::TrFailedMsg(msg) => {\n+            st.failed += 1;\n+            let mut stdout = stdout;\n+            stdout.extend_from_slice(format!(\"note: {}\", msg).as_bytes());\n+            st.failures.push((test, stdout));\n+        }\n+        TestResult::TrTimedFail => {\n+            st.failed += 1;\n+            st.time_failures.push((test, stdout));\n+        }\n+    }\n+}\n+\n+// Handler for events that occur during test execution.\n+// It is provided as a callback to the `run_tests` function.\n+fn on_test_event(\n+    event: &TestEvent,\n+    st: &mut ConsoleTestState,\n+    out: &mut dyn OutputFormatter,\n+) -> io::Result<()> {\n+    match (*event).clone() {\n+        TestEvent::TeFiltered(ref filtered_tests) => {\n+            st.total = filtered_tests.len();\n+            out.write_run_start(filtered_tests.len())?;\n+        }\n+        TestEvent::TeFilteredOut(filtered_out) => {\n+            st.filtered_out = filtered_out;\n+        }\n+        TestEvent::TeWait(ref test) => out.write_test_start(test)?,\n+        TestEvent::TeTimeout(ref test) => out.write_timeout(test)?,\n+        TestEvent::TeResult(completed_test) => {\n+            let test = &completed_test.desc;\n+            let result = &completed_test.result;\n+            let exec_time = &completed_test.exec_time;\n+            let stdout = &completed_test.stdout;\n+\n+            st.write_log_result(test, result, exec_time.as_ref())?;\n+            out.write_result(test, result, exec_time.as_ref(), &*stdout, st)?;\n+            handle_test_result(st, completed_test);\n+        }\n+    }\n+\n+    Ok(())\n+}\n+\n+/// A simple console test runner.\n+/// Runs provided tests reporting process and results to the stdout.\n+pub fn run_tests_console(opts: &TestOpts, tests: Vec<TestDescAndFn>) -> io::Result<bool> {\n+    let output = match term::stdout() {\n+        None => OutputLocation::Raw(io::stdout()),\n+        Some(t) => OutputLocation::Pretty(t),\n+    };\n+\n+    let max_name_len = tests\n+        .iter()\n+        .max_by_key(|t| len_if_padded(*t))\n+        .map(|t| t.desc.name.as_slice().len())\n+        .unwrap_or(0);\n+\n+    let is_multithreaded = opts.test_threads.unwrap_or_else(get_concurrency) > 1;\n+\n+    let mut out: Box<dyn OutputFormatter> = match opts.format {\n+        OutputFormat::Pretty => Box::new(PrettyFormatter::new(\n+            output,\n+            opts.use_color(),\n+            max_name_len,\n+            is_multithreaded,\n+            opts.time_options,\n+        )),\n+        OutputFormat::Terse => Box::new(TerseFormatter::new(\n+            output,\n+            opts.use_color(),\n+            max_name_len,\n+            is_multithreaded,\n+        )),\n+        OutputFormat::Json => Box::new(JsonFormatter::new(output)),\n+    };\n+    let mut st = ConsoleTestState::new(opts)?;\n+\n+    run_tests(opts, tests, |x| on_test_event(&x, &mut st, &mut *out))?;\n+\n+    assert!(st.current_test_count() == st.total);\n+\n+    return out.write_run_finish(&st);\n+}\n+\n+// Calculates padding for given test description.\n+fn len_if_padded(t: &TestDescAndFn) -> usize {\n+    match t.testfn.padding() {\n+        NamePadding::PadNone => 0,\n+        NamePadding::PadOnRight => t.desc.name.as_slice().len(),\n+    }\n+}"}, {"sha": "eefbd2d6a813a5804ddd0d6926058f1cc26d683b", "filename": "src/libtest/event.rs", "status": "added", "additions": 41, "deletions": 0, "changes": 41, "blob_url": "https://github.com/rust-lang/rust/blob/57bfb8096295150c06559da10adc5629e445a4ac/src%2Flibtest%2Fevent.rs", "raw_url": "https://github.com/rust-lang/rust/raw/57bfb8096295150c06559da10adc5629e445a4ac/src%2Flibtest%2Fevent.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibtest%2Fevent.rs?ref=57bfb8096295150c06559da10adc5629e445a4ac", "patch": "@@ -0,0 +1,41 @@\n+//! Module containing different events that can occur\n+//! during tests execution process.\n+\n+use super::types::TestDesc;\n+use super::test_result::TestResult;\n+use super::time::TestExecTime;\n+\n+#[derive(Debug, Clone)]\n+pub struct CompletedTest {\n+    pub desc: TestDesc,\n+    pub result: TestResult,\n+    pub exec_time: Option<TestExecTime>,\n+    pub stdout: Vec<u8>,\n+}\n+\n+impl CompletedTest {\n+    pub fn new(\n+        desc: TestDesc,\n+        result: TestResult,\n+        exec_time: Option<TestExecTime>,\n+        stdout: Vec<u8>\n+    ) -> Self {\n+        Self {\n+            desc,\n+            result,\n+            exec_time,\n+            stdout,\n+        }\n+    }\n+}\n+\n+unsafe impl Send for CompletedTest {}\n+\n+#[derive(Debug, Clone)]\n+pub enum TestEvent {\n+    TeFiltered(Vec<TestDesc>),\n+    TeWait(TestDesc),\n+    TeResult(CompletedTest),\n+    TeTimeout(TestDesc),\n+    TeFilteredOut(usize),\n+}"}, {"sha": "b73d7349678a720c81f519ca233d7997e4452e8b", "filename": "src/libtest/formatters/json.rs", "status": "modified", "additions": 24, "deletions": 11, "changes": 35, "blob_url": "https://github.com/rust-lang/rust/blob/57bfb8096295150c06559da10adc5629e445a4ac/src%2Flibtest%2Fformatters%2Fjson.rs", "raw_url": "https://github.com/rust-lang/rust/raw/57bfb8096295150c06559da10adc5629e445a4ac/src%2Flibtest%2Fformatters%2Fjson.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibtest%2Fformatters%2Fjson.rs?ref=57bfb8096295150c06559da10adc5629e445a4ac", "patch": "@@ -1,4 +1,16 @@\n-use super::*;\n+use std::{\n+    io,\n+    io::prelude::Write,\n+    borrow::Cow,\n+};\n+\n+use crate::{\n+    types::TestDesc,\n+    time,\n+    test_result::TestResult,\n+    console::{ConsoleTestState, OutputLocation},\n+};\n+use super::OutputFormatter;\n \n pub(crate) struct JsonFormatter<T> {\n     out: OutputLocation<T>,\n@@ -27,7 +39,7 @@ impl<T: Write> JsonFormatter<T> {\n         ty: &str,\n         name: &str,\n         evt: &str,\n-        exec_time: Option<&TestExecTime>,\n+        exec_time: Option<&time::TestExecTime>,\n         stdout: Option<Cow<'_, str>>,\n         extra: Option<&str>,\n     ) -> io::Result<()> {\n@@ -76,25 +88,26 @@ impl<T: Write> OutputFormatter for JsonFormatter<T> {\n         &mut self,\n         desc: &TestDesc,\n         result: &TestResult,\n-        exec_time: Option<&TestExecTime>,\n+        exec_time: Option<&time::TestExecTime>,\n         stdout: &[u8],\n         state: &ConsoleTestState,\n     ) -> io::Result<()> {\n-        let stdout = if (state.options.display_output || *result != TrOk) && stdout.len() > 0 {\n+        let display_stdout = state.options.display_output || *result != TestResult::TrOk;\n+        let stdout = if display_stdout && stdout.len() > 0 {\n             Some(String::from_utf8_lossy(stdout))\n         } else {\n             None\n         };\n         match *result {\n-            TrOk => {\n+            TestResult::TrOk => {\n                 self.write_event(\"test\", desc.name.as_slice(), \"ok\", exec_time, stdout, None)\n             }\n \n-            TrFailed => {\n+            TestResult::TrFailed => {\n                 self.write_event(\"test\", desc.name.as_slice(), \"failed\", exec_time, stdout, None)\n             }\n \n-            TrTimedFail => self.write_event(\n+            TestResult::TrTimedFail => self.write_event(\n                 \"test\",\n                 desc.name.as_slice(),\n                 \"failed\",\n@@ -103,7 +116,7 @@ impl<T: Write> OutputFormatter for JsonFormatter<T> {\n                 Some(r#\"\"reason\": \"time limit exceeded\"\"#),\n             ),\n \n-            TrFailedMsg(ref m) => self.write_event(\n+            TestResult::TrFailedMsg(ref m) => self.write_event(\n                 \"test\",\n                 desc.name.as_slice(),\n                 \"failed\",\n@@ -112,11 +125,11 @@ impl<T: Write> OutputFormatter for JsonFormatter<T> {\n                 Some(&*format!(r#\"\"message\": \"{}\"\"#, EscapedString(m))),\n             ),\n \n-            TrIgnored => {\n+            TestResult::TrIgnored => {\n                 self.write_event(\"test\", desc.name.as_slice(), \"ignored\", exec_time, stdout, None)\n             }\n \n-            TrAllowedFail => self.write_event(\n+            TestResult::TrAllowedFail => self.write_event(\n                 \"test\",\n                 desc.name.as_slice(),\n                 \"allowed_failure\",\n@@ -125,7 +138,7 @@ impl<T: Write> OutputFormatter for JsonFormatter<T> {\n                 None,\n             ),\n \n-            TrBench(ref bs) => {\n+            TestResult::TrBench(ref bs) => {\n                 let median = bs.ns_iter_summ.median as usize;\n                 let deviation = (bs.ns_iter_summ.max - bs.ns_iter_summ.min) as usize;\n "}, {"sha": "b6649a3effc7c6ea048ac668969cfaa13c7b6d7c", "filename": "src/libtest/formatters/mod.rs", "status": "modified", "additions": 12, "deletions": 2, "changes": 14, "blob_url": "https://github.com/rust-lang/rust/blob/57bfb8096295150c06559da10adc5629e445a4ac/src%2Flibtest%2Fformatters%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/57bfb8096295150c06559da10adc5629e445a4ac/src%2Flibtest%2Fformatters%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibtest%2Fformatters%2Fmod.rs?ref=57bfb8096295150c06559da10adc5629e445a4ac", "patch": "@@ -1,4 +1,14 @@\n-use super::*;\n+use std::{\n+    io,\n+    io::prelude::Write,\n+};\n+\n+use crate::{\n+    types::{TestDesc, TestName},\n+    time,\n+    test_result::TestResult,\n+    console::{ConsoleTestState},\n+};\n \n mod pretty;\n mod json;\n@@ -16,7 +26,7 @@ pub(crate) trait OutputFormatter {\n         &mut self,\n         desc: &TestDesc,\n         result: &TestResult,\n-        exec_time: Option<&TestExecTime>,\n+        exec_time: Option<&time::TestExecTime>,\n         stdout: &[u8],\n         state: &ConsoleTestState,\n     ) -> io::Result<()>;"}, {"sha": "2fdbc63d51330eb7f464d0849ee20d83566bc717", "filename": "src/libtest/formatters/pretty.rs", "status": "modified", "additions": 26, "deletions": 14, "changes": 40, "blob_url": "https://github.com/rust-lang/rust/blob/57bfb8096295150c06559da10adc5629e445a4ac/src%2Flibtest%2Fformatters%2Fpretty.rs", "raw_url": "https://github.com/rust-lang/rust/raw/57bfb8096295150c06559da10adc5629e445a4ac/src%2Flibtest%2Fformatters%2Fpretty.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibtest%2Fformatters%2Fpretty.rs?ref=57bfb8096295150c06559da10adc5629e445a4ac", "patch": "@@ -1,9 +1,21 @@\n-use super::*;\n+use std::{\n+    io,\n+    io::prelude::Write,\n+};\n+\n+use crate::{\n+    types::TestDesc,\n+    time,\n+    test_result::TestResult,\n+    console::{ConsoleTestState, OutputLocation},\n+    bench::fmt_bench_samples,\n+};\n+use super::OutputFormatter;\n \n pub(crate) struct PrettyFormatter<T> {\n     out: OutputLocation<T>,\n     use_color: bool,\n-    time_options: Option<TestTimeOptions>,\n+    time_options: Option<time::TestTimeOptions>,\n \n     /// Number of columns to fill when aligning names\n     max_name_len: usize,\n@@ -17,7 +29,7 @@ impl<T: Write> PrettyFormatter<T> {\n         use_color: bool,\n         max_name_len: usize,\n         is_multithreaded: bool,\n-        time_options: Option<TestTimeOptions>,\n+        time_options: Option<time::TestTimeOptions>,\n     ) -> Self {\n         PrettyFormatter {\n             out,\n@@ -67,7 +79,7 @@ impl<T: Write> PrettyFormatter<T> {\n \n     pub fn write_pretty(&mut self, word: &str, color: term::color::Color) -> io::Result<()> {\n         match self.out {\n-            Pretty(ref mut term) => {\n+            OutputLocation::Pretty(ref mut term) => {\n                 if self.use_color {\n                     term.fg(color)?;\n                 }\n@@ -77,7 +89,7 @@ impl<T: Write> PrettyFormatter<T> {\n                 }\n                 term.flush()\n             }\n-            Raw(ref mut stdout) => {\n+            OutputLocation::Raw(ref mut stdout) => {\n                 stdout.write_all(word.as_bytes())?;\n                 stdout.flush()\n             }\n@@ -93,7 +105,7 @@ impl<T: Write> PrettyFormatter<T> {\n     fn write_time(\n         &mut self,\n         desc: &TestDesc,\n-        exec_time: Option<&TestExecTime>\n+        exec_time: Option<&time::TestExecTime>\n     ) -> io::Result<()> {\n         if let (Some(opts), Some(time)) = (self.time_options, exec_time) {\n             let time_str = format!(\" <{}>\", time);\n@@ -194,7 +206,7 @@ impl<T: Write> OutputFormatter for PrettyFormatter<T> {\n         &mut self,\n         desc: &TestDesc,\n         result: &TestResult,\n-        exec_time: Option<&TestExecTime>,\n+        exec_time: Option<&time::TestExecTime>,\n         _: &[u8],\n         _: &ConsoleTestState,\n     ) -> io::Result<()> {\n@@ -203,15 +215,15 @@ impl<T: Write> OutputFormatter for PrettyFormatter<T> {\n         }\n \n         match *result {\n-            TrOk => self.write_ok()?,\n-            TrFailed | TrFailedMsg(_) => self.write_failed()?,\n-            TrIgnored => self.write_ignored()?,\n-            TrAllowedFail => self.write_allowed_fail()?,\n-            TrBench(ref bs) => {\n+            TestResult::TrOk => self.write_ok()?,\n+            TestResult::TrFailed | TestResult::TrFailedMsg(_) => self.write_failed()?,\n+            TestResult::TrIgnored => self.write_ignored()?,\n+            TestResult::TrAllowedFail => self.write_allowed_fail()?,\n+            TestResult::TrBench(ref bs) => {\n                 self.write_bench()?;\n                 self.write_plain(&format!(\": {}\", fmt_bench_samples(bs)))?;\n             }\n-            TrTimedFail => self.write_time_failed()?,\n+            TestResult::TrTimedFail => self.write_time_failed()?,\n         }\n \n         self.write_time(desc, exec_time)?;\n@@ -225,7 +237,7 @@ impl<T: Write> OutputFormatter for PrettyFormatter<T> {\n \n         self.write_plain(&format!(\n             \"test {} has been running for over {} seconds\\n\",\n-            desc.name, TEST_WARN_TIMEOUT_S\n+            desc.name, time::TEST_WARN_TIMEOUT_S\n         ))\n     }\n "}, {"sha": "fe56157d9c10a2624f88f3b846621387daf331eb", "filename": "src/libtest/formatters/terse.rs", "status": "modified", "additions": 29, "deletions": 11, "changes": 40, "blob_url": "https://github.com/rust-lang/rust/blob/57bfb8096295150c06559da10adc5629e445a4ac/src%2Flibtest%2Fformatters%2Fterse.rs", "raw_url": "https://github.com/rust-lang/rust/raw/57bfb8096295150c06559da10adc5629e445a4ac/src%2Flibtest%2Fformatters%2Fterse.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibtest%2Fformatters%2Fterse.rs?ref=57bfb8096295150c06559da10adc5629e445a4ac", "patch": "@@ -1,4 +1,20 @@\n-use super::*;\n+use std::{\n+    io,\n+    io::prelude::Write,\n+};\n+\n+use crate::{\n+    types::TestDesc,\n+    time,\n+    test_result::TestResult,\n+    types::NamePadding,\n+    console::{ConsoleTestState, OutputLocation},\n+    bench::fmt_bench_samples,\n+};\n+use super::OutputFormatter;\n+\n+// insert a '\\n' after 100 tests in quiet mode\n+const QUIET_MODE_MAX_COLUMN: usize = 100;\n \n pub(crate) struct TerseFormatter<T> {\n     out: OutputLocation<T>,\n@@ -68,7 +84,7 @@ impl<T: Write> TerseFormatter<T> {\n \n     pub fn write_pretty(&mut self, word: &str, color: term::color::Color) -> io::Result<()> {\n         match self.out {\n-            Pretty(ref mut term) => {\n+            OutputLocation::Pretty(ref mut term) => {\n                 if self.use_color {\n                     term.fg(color)?;\n                 }\n@@ -78,7 +94,7 @@ impl<T: Write> TerseFormatter<T> {\n                 }\n                 term.flush()\n             }\n-            Raw(ref mut stdout) => {\n+            OutputLocation::Raw(ref mut stdout) => {\n                 stdout.write_all(word.as_bytes())?;\n                 stdout.flush()\n             }\n@@ -163,7 +179,7 @@ impl<T: Write> OutputFormatter for TerseFormatter<T> {\n         // in order to indicate benchmarks.\n         // When running benchmarks, terse-mode should still print their name as if\n         // it is the Pretty formatter.\n-        if !self.is_multithreaded && desc.name.padding() == PadOnRight {\n+        if !self.is_multithreaded && desc.name.padding() == NamePadding::PadOnRight {\n             self.write_test_name(desc)?;\n         }\n \n@@ -174,16 +190,18 @@ impl<T: Write> OutputFormatter for TerseFormatter<T> {\n         &mut self,\n         desc: &TestDesc,\n         result: &TestResult,\n-        _: Option<&TestExecTime>,\n+        _: Option<&time::TestExecTime>,\n         _: &[u8],\n         _: &ConsoleTestState,\n     ) -> io::Result<()> {\n         match *result {\n-            TrOk => self.write_ok(),\n-            TrFailed | TrFailedMsg(_) | TrTimedFail => self.write_failed(),\n-            TrIgnored => self.write_ignored(),\n-            TrAllowedFail => self.write_allowed_fail(),\n-            TrBench(ref bs) => {\n+            TestResult::TrOk => self.write_ok(),\n+            TestResult::TrFailed\n+                | TestResult::TrFailedMsg(_)\n+                | TestResult::TrTimedFail => self.write_failed(),\n+            TestResult::TrIgnored => self.write_ignored(),\n+            TestResult::TrAllowedFail => self.write_allowed_fail(),\n+            TestResult::TrBench(ref bs) => {\n                 if self.is_multithreaded {\n                     self.write_test_name(desc)?;\n                 }\n@@ -196,7 +214,7 @@ impl<T: Write> OutputFormatter for TerseFormatter<T> {\n     fn write_timeout(&mut self, desc: &TestDesc) -> io::Result<()> {\n         self.write_plain(&format!(\n             \"test {} has been running for over {} seconds\\n\",\n-            desc.name, TEST_WARN_TIMEOUT_S\n+            desc.name, time::TEST_WARN_TIMEOUT_S\n         ))\n     }\n "}, {"sha": "61651a927c5f7aea2222b461d33d64f8610ccc25", "filename": "src/libtest/helpers/concurrency.rs", "status": "added", "additions": 143, "deletions": 0, "changes": 143, "blob_url": "https://github.com/rust-lang/rust/blob/57bfb8096295150c06559da10adc5629e445a4ac/src%2Flibtest%2Fhelpers%2Fconcurrency.rs", "raw_url": "https://github.com/rust-lang/rust/raw/57bfb8096295150c06559da10adc5629e445a4ac/src%2Flibtest%2Fhelpers%2Fconcurrency.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibtest%2Fhelpers%2Fconcurrency.rs?ref=57bfb8096295150c06559da10adc5629e445a4ac", "patch": "@@ -0,0 +1,143 @@\n+//! Helper module which helps to determine amount of threads to be used\n+//! during tests execution.\n+use std::env;\n+\n+#[allow(deprecated)]\n+pub fn get_concurrency() -> usize {\n+    return match env::var(\"RUST_TEST_THREADS\") {\n+        Ok(s) => {\n+            let opt_n: Option<usize> = s.parse().ok();\n+            match opt_n {\n+                Some(n) if n > 0 => n,\n+                _ => panic!(\"RUST_TEST_THREADS is `{}`, should be a positive integer.\", s),\n+            }\n+        }\n+        Err(..) => num_cpus(),\n+    };\n+\n+    #[cfg(windows)]\n+    #[allow(nonstandard_style)]\n+    fn num_cpus() -> usize {\n+        #[repr(C)]\n+        struct SYSTEM_INFO {\n+            wProcessorArchitecture: u16,\n+            wReserved: u16,\n+            dwPageSize: u32,\n+            lpMinimumApplicationAddress: *mut u8,\n+            lpMaximumApplicationAddress: *mut u8,\n+            dwActiveProcessorMask: *mut u8,\n+            dwNumberOfProcessors: u32,\n+            dwProcessorType: u32,\n+            dwAllocationGranularity: u32,\n+            wProcessorLevel: u16,\n+            wProcessorRevision: u16,\n+        }\n+        extern \"system\" {\n+            fn GetSystemInfo(info: *mut SYSTEM_INFO) -> i32;\n+        }\n+        unsafe {\n+            let mut sysinfo = std::mem::zeroed();\n+            GetSystemInfo(&mut sysinfo);\n+            sysinfo.dwNumberOfProcessors as usize\n+        }\n+    }\n+\n+    #[cfg(target_os = \"vxworks\")]\n+    fn num_cpus() -> usize {\n+        // FIXME: Implement num_cpus on vxWorks\n+        1\n+    }\n+\n+    #[cfg(target_os = \"redox\")]\n+    fn num_cpus() -> usize {\n+        // FIXME: Implement num_cpus on Redox\n+        1\n+    }\n+\n+    #[cfg(any(\n+        all(target_arch = \"wasm32\", not(target_os = \"emscripten\")),\n+        all(target_vendor = \"fortanix\", target_env = \"sgx\")\n+    ))]\n+    fn num_cpus() -> usize {\n+        1\n+    }\n+\n+    #[cfg(any(\n+        target_os = \"android\",\n+        target_os = \"cloudabi\",\n+        target_os = \"emscripten\",\n+        target_os = \"fuchsia\",\n+        target_os = \"ios\",\n+        target_os = \"linux\",\n+        target_os = \"macos\",\n+        target_os = \"solaris\",\n+    ))]\n+    fn num_cpus() -> usize {\n+        unsafe { libc::sysconf(libc::_SC_NPROCESSORS_ONLN) as usize }\n+    }\n+\n+    #[cfg(any(target_os = \"freebsd\", target_os = \"dragonfly\", target_os = \"netbsd\"))]\n+    fn num_cpus() -> usize {\n+        use std::ptr;\n+\n+        let mut cpus: libc::c_uint = 0;\n+        let mut cpus_size = std::mem::size_of_val(&cpus);\n+\n+        unsafe {\n+            cpus = libc::sysconf(libc::_SC_NPROCESSORS_ONLN) as libc::c_uint;\n+        }\n+        if cpus < 1 {\n+            let mut mib = [libc::CTL_HW, libc::HW_NCPU, 0, 0];\n+            unsafe {\n+                libc::sysctl(\n+                    mib.as_mut_ptr(),\n+                    2,\n+                    &mut cpus as *mut _ as *mut _,\n+                    &mut cpus_size as *mut _ as *mut _,\n+                    ptr::null_mut(),\n+                    0,\n+                );\n+            }\n+            if cpus < 1 {\n+                cpus = 1;\n+            }\n+        }\n+        cpus as usize\n+    }\n+\n+    #[cfg(target_os = \"openbsd\")]\n+    fn num_cpus() -> usize {\n+        use std::ptr;\n+\n+        let mut cpus: libc::c_uint = 0;\n+        let mut cpus_size = std::mem::size_of_val(&cpus);\n+        let mut mib = [libc::CTL_HW, libc::HW_NCPU, 0, 0];\n+\n+        unsafe {\n+            libc::sysctl(\n+                mib.as_mut_ptr(),\n+                2,\n+                &mut cpus as *mut _ as *mut _,\n+                &mut cpus_size as *mut _ as *mut _,\n+                ptr::null_mut(),\n+                0,\n+            );\n+        }\n+        if cpus < 1 {\n+            cpus = 1;\n+        }\n+        cpus as usize\n+    }\n+\n+    #[cfg(target_os = \"haiku\")]\n+    fn num_cpus() -> usize {\n+        // FIXME: implement\n+        1\n+    }\n+\n+    #[cfg(target_os = \"l4re\")]\n+    fn num_cpus() -> usize {\n+        // FIXME: implement\n+        1\n+    }\n+}"}, {"sha": "831bef3b118ac1500f695885c0a9795faf77485d", "filename": "src/libtest/helpers/exit_code.rs", "status": "added", "additions": 20, "deletions": 0, "changes": 20, "blob_url": "https://github.com/rust-lang/rust/blob/57bfb8096295150c06559da10adc5629e445a4ac/src%2Flibtest%2Fhelpers%2Fexit_code.rs", "raw_url": "https://github.com/rust-lang/rust/raw/57bfb8096295150c06559da10adc5629e445a4ac/src%2Flibtest%2Fhelpers%2Fexit_code.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibtest%2Fhelpers%2Fexit_code.rs?ref=57bfb8096295150c06559da10adc5629e445a4ac", "patch": "@@ -0,0 +1,20 @@\n+//! Helper module to detect subprocess exit code.\n+\n+use std::process::ExitStatus;\n+\n+#[cfg(not(unix))]\n+pub fn get_exit_code(status: ExitStatus) -> Result<i32, String> {\n+    status.code().ok_or(\"received no exit code from child process\".into())\n+}\n+\n+#[cfg(unix)]\n+pub fn get_exit_code(status: ExitStatus) -> Result<i32, String> {\n+    use std::os::unix::process::ExitStatusExt;\n+    match status.code() {\n+        Some(code) => Ok(code),\n+        None => match status.signal() {\n+            Some(signal) => Err(format!(\"child process exited with signal {}\", signal)),\n+            None => Err(\"child process exited with unknown signal\".into()),\n+        }\n+    }\n+}"}, {"sha": "6e4954778e60523c90cd4011cbad0be3951b9bb4", "filename": "src/libtest/helpers/isatty.rs", "status": "added", "additions": 33, "deletions": 0, "changes": 33, "blob_url": "https://github.com/rust-lang/rust/blob/57bfb8096295150c06559da10adc5629e445a4ac/src%2Flibtest%2Fhelpers%2Fisatty.rs", "raw_url": "https://github.com/rust-lang/rust/raw/57bfb8096295150c06559da10adc5629e445a4ac/src%2Flibtest%2Fhelpers%2Fisatty.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibtest%2Fhelpers%2Fisatty.rs?ref=57bfb8096295150c06559da10adc5629e445a4ac", "patch": "@@ -0,0 +1,33 @@\n+//! Helper module which provides a function to test\n+//! if stdout is a tty.\n+\n+#[cfg(any(\n+    target_os = \"cloudabi\",\n+    all(target_arch = \"wasm32\", not(target_os = \"emscripten\")),\n+    all(target_vendor = \"fortanix\", target_env = \"sgx\")\n+))]\n+pub fn stdout_isatty() -> bool {\n+    // FIXME: Implement isatty on SGX\n+    false\n+}\n+#[cfg(unix)]\n+pub fn stdout_isatty() -> bool {\n+    unsafe { libc::isatty(libc::STDOUT_FILENO) != 0 }\n+}\n+#[cfg(windows)]\n+pub fn stdout_isatty() -> bool {\n+    type DWORD = u32;\n+    type BOOL = i32;\n+    type HANDLE = *mut u8;\n+    type LPDWORD = *mut u32;\n+    const STD_OUTPUT_HANDLE: DWORD = -11i32 as DWORD;\n+    extern \"system\" {\n+        fn GetStdHandle(which: DWORD) -> HANDLE;\n+        fn GetConsoleMode(hConsoleHandle: HANDLE, lpMode: LPDWORD) -> BOOL;\n+    }\n+    unsafe {\n+        let handle = GetStdHandle(STD_OUTPUT_HANDLE);\n+        let mut out = 0;\n+        GetConsoleMode(handle, &mut out) != 0\n+    }\n+}"}, {"sha": "f77a23e6875b2c1b6a65e1d00a9169c8872fec47", "filename": "src/libtest/helpers/metrics.rs", "status": "added", "additions": 50, "deletions": 0, "changes": 50, "blob_url": "https://github.com/rust-lang/rust/blob/57bfb8096295150c06559da10adc5629e445a4ac/src%2Flibtest%2Fhelpers%2Fmetrics.rs", "raw_url": "https://github.com/rust-lang/rust/raw/57bfb8096295150c06559da10adc5629e445a4ac/src%2Flibtest%2Fhelpers%2Fmetrics.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibtest%2Fhelpers%2Fmetrics.rs?ref=57bfb8096295150c06559da10adc5629e445a4ac", "patch": "@@ -0,0 +1,50 @@\n+//! Benchmark metrics.\n+use std::collections::BTreeMap;\n+\n+#[derive(Clone, PartialEq, Debug, Copy)]\n+pub struct Metric {\n+    value: f64,\n+    noise: f64,\n+}\n+\n+impl Metric {\n+    pub fn new(value: f64, noise: f64) -> Metric {\n+        Metric { value, noise }\n+    }\n+}\n+\n+#[derive(Clone, PartialEq)]\n+pub struct MetricMap(BTreeMap<String, Metric>);\n+\n+impl MetricMap {\n+    pub fn new() -> MetricMap {\n+        MetricMap(BTreeMap::new())\n+    }\n+\n+    /// Insert a named `value` (+/- `noise`) metric into the map. The value\n+    /// must be non-negative. The `noise` indicates the uncertainty of the\n+    /// metric, which doubles as the \"noise range\" of acceptable\n+    /// pairwise-regressions on this named value, when comparing from one\n+    /// metric to the next using `compare_to_old`.\n+    ///\n+    /// If `noise` is positive, then it means this metric is of a value\n+    /// you want to see grow smaller, so a change larger than `noise` in the\n+    /// positive direction represents a regression.\n+    ///\n+    /// If `noise` is negative, then it means this metric is of a value\n+    /// you want to see grow larger, so a change larger than `noise` in the\n+    /// negative direction represents a regression.\n+    pub fn insert_metric(&mut self, name: &str, value: f64, noise: f64) {\n+        let m = Metric { value, noise };\n+        self.0.insert(name.to_owned(), m);\n+    }\n+\n+    pub fn fmt_metrics(&self) -> String {\n+        let v = self\n+            .0\n+            .iter()\n+            .map(|(k, v)| format!(\"{}: {} (+/- {})\", *k, v.value, v.noise))\n+            .collect::<Vec<_>>();\n+        v.join(\", \")\n+    }\n+}"}, {"sha": "6a2ef6086cb92b55273655c8078ca93d79b6fc47", "filename": "src/libtest/helpers/mod.rs", "status": "added", "additions": 8, "deletions": 0, "changes": 8, "blob_url": "https://github.com/rust-lang/rust/blob/57bfb8096295150c06559da10adc5629e445a4ac/src%2Flibtest%2Fhelpers%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/57bfb8096295150c06559da10adc5629e445a4ac/src%2Flibtest%2Fhelpers%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibtest%2Fhelpers%2Fmod.rs?ref=57bfb8096295150c06559da10adc5629e445a4ac", "patch": "@@ -0,0 +1,8 @@\n+//! Module with common helpers not directly related to tests\n+//! but used in `libtest`.\n+\n+pub mod concurrency;\n+pub mod isatty;\n+pub mod metrics;\n+pub mod sink;\n+pub mod exit_code;"}, {"sha": "aa7fe2487730e3aa28b451ef91bd35103fda7aea", "filename": "src/libtest/helpers/sink.rs", "status": "added", "additions": 24, "deletions": 0, "changes": 24, "blob_url": "https://github.com/rust-lang/rust/blob/57bfb8096295150c06559da10adc5629e445a4ac/src%2Flibtest%2Fhelpers%2Fsink.rs", "raw_url": "https://github.com/rust-lang/rust/raw/57bfb8096295150c06559da10adc5629e445a4ac/src%2Flibtest%2Fhelpers%2Fsink.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibtest%2Fhelpers%2Fsink.rs?ref=57bfb8096295150c06559da10adc5629e445a4ac", "patch": "@@ -0,0 +1,24 @@\n+//! Module providing a helper structure to capture output in subprocesses.\n+\n+use std::{\n+    io,\n+    io::prelude::Write,\n+    sync::{Arc, Mutex},\n+};\n+\n+pub struct Sink(Arc<Mutex<Vec<u8>>>);\n+\n+impl Sink {\n+    pub fn new_boxed(data: &Arc<Mutex<Vec<u8>>>) -> Box<Self> {\n+        Box::new(Self(data.clone()))\n+    }\n+}\n+\n+impl Write for Sink {\n+    fn write(&mut self, data: &[u8]) -> io::Result<usize> {\n+        Write::write(&mut *self.0.lock().unwrap(), data)\n+    }\n+    fn flush(&mut self) -> io::Result<()> {\n+        Ok(())\n+    }\n+}"}, {"sha": "179558e8f9a184efcbe9af4ce88cdd86b336700a", "filename": "src/libtest/lib.rs", "status": "modified", "additions": 118, "deletions": 1726, "changes": 1844, "blob_url": "https://github.com/rust-lang/rust/blob/57bfb8096295150c06559da10adc5629e445a4ac/src%2Flibtest%2Flib.rs", "raw_url": "https://github.com/rust-lang/rust/raw/57bfb8096295150c06559da10adc5629e445a4ac/src%2Flibtest%2Flib.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibtest%2Flib.rs?ref=57bfb8096295150c06559da10adc5629e445a4ac", "patch": "@@ -30,338 +30,98 @@\n #![feature(termination_trait_lib)]\n #![feature(test)]\n \n-use getopts;\n-#[cfg(any(unix, target_os = \"cloudabi\"))]\n-extern crate libc;\n-use term;\n-\n+// Public reexports\n pub use self::ColorConfig::*;\n-use self::NamePadding::*;\n-use self::OutputLocation::*;\n-use self::TestEvent::*;\n-pub use self::TestFn::*;\n-pub use self::TestName::*;\n-pub use self::TestResult::*;\n-\n-use std::any::Any;\n-use std::borrow::Cow;\n-use std::cmp;\n-use std::collections::BTreeMap;\n-use std::env;\n-use std::fmt;\n-use std::fs::File;\n-use std::io;\n-use std::io::prelude::*;\n-use std::panic::{self, catch_unwind, AssertUnwindSafe, PanicInfo};\n-use std::path::PathBuf;\n-use std::process;\n-use std::process::{ExitStatus, Command, Termination};\n-use std::str::FromStr;\n-use std::sync::mpsc::{channel, Sender};\n-use std::sync::{Arc, Mutex};\n-use std::thread;\n-use std::time::{Duration, Instant};\n-\n-#[cfg(test)]\n-mod tests;\n-\n-const TEST_WARN_TIMEOUT_S: u64 = 60;\n-const QUIET_MODE_MAX_COLUMN: usize = 100; // insert a '\\n' after 100 tests in quiet mode\n-\n-const SECONDARY_TEST_INVOKER_VAR: &'static str = \"__RUST_TEST_INVOKE\";\n+pub use self::types::*;\n+pub use self::types::TestName::*;\n+pub use self::options::{Options, ShouldPanic};\n+pub use self::bench::{Bencher, black_box};\n \n-// Return codes for secondary process.\n-// Start somewhere other than 0 so we know the return code means what we think\n-// it means.\n-const TR_OK: i32 = 50;\n-const TR_FAILED: i32 = 51;\n-\n-/// This small module contains constants used by `report-time` option.\n-/// Those constants values will be used if corresponding environment variables are not set.\n-///\n-/// To override values for unit-tests, use a constant `RUST_TEST_TIME_UNIT`,\n-/// To override values for integration tests, use a constant `RUST_TEST_TIME_INTEGRATION`,\n-/// To override values for doctests, use a constant `RUST_TEST_TIME_DOCTEST`.\n-///\n-/// Example of the expected format is `RUST_TEST_TIME_xxx=100,200`, where 100 means\n-/// warn time, and 200 means critical time.\n-pub mod time_constants {\n-    use std::time::Duration;\n-    use super::TEST_WARN_TIMEOUT_S;\n-\n-    /// Environment variable for overriding default threshold for unit-tests.\n-    pub const UNIT_ENV_NAME: &str = \"RUST_TEST_TIME_UNIT\";\n-\n-    // Unit tests are supposed to be really quick.\n-    pub const UNIT_WARN: Duration = Duration::from_millis(50);\n-    pub const UNIT_CRITICAL: Duration = Duration::from_millis(100);\n-\n-    /// Environment variable for overriding default threshold for unit-tests.\n-    pub const INTEGRATION_ENV_NAME: &str = \"RUST_TEST_TIME_INTEGRATION\";\n-\n-    // Integration tests may have a lot of work, so they can take longer to execute.\n-    pub const INTEGRATION_WARN: Duration = Duration::from_millis(500);\n-    pub const INTEGRATION_CRITICAL: Duration = Duration::from_millis(1000);\n-\n-    /// Environment variable for overriding default threshold for unit-tests.\n-    pub const DOCTEST_ENV_NAME: &str = \"RUST_TEST_TIME_DOCTEST\";\n-\n-    // Doctests are similar to integration tests, because they can include a lot of\n-    // initialization code.\n-    pub const DOCTEST_WARN: Duration = INTEGRATION_WARN;\n-    pub const DOCTEST_CRITICAL: Duration = INTEGRATION_CRITICAL;\n-\n-    // Do not suppose anything about unknown tests, base limits on the\n-    // `TEST_WARN_TIMEOUT_S` constant.\n-    pub const UNKNOWN_WARN: Duration = Duration::from_secs(TEST_WARN_TIMEOUT_S);\n-    pub const UNKNOWN_CRITICAL: Duration = Duration::from_secs(TEST_WARN_TIMEOUT_S * 2);\n-}\n-\n-// to be used by rustc to compile tests in libtest\n+// Module to be used by rustc to compile tests in libtest\n pub mod test {\n     pub use crate::{\n-        assert_test_result, filter_tests, parse_opts, run_test, test_main, test_main_static,\n-        Bencher, DynTestFn, DynTestName, Metric, MetricMap, Options, RunIgnored, RunStrategy,\n-        ShouldPanic, StaticBenchFn, StaticTestFn, StaticTestName, TestDesc, TestDescAndFn, TestName,\n-        TestOpts, TestTimeOptions, TestType, TestResult, TrFailed, TrFailedMsg, TrIgnored, TrOk,\n+        bench::Bencher,\n+        cli::{parse_opts, TestOpts},\n+        helpers::metrics::{Metric, MetricMap},\n+        options::{ShouldPanic, Options, RunIgnored, RunStrategy},\n+        test_result::{TestResult, TrFailed, TrFailedMsg, TrIgnored, TrOk},\n+        time::{TestTimeOptions, TestExecTime},\n+        types::{\n+            DynTestFn, DynTestName, StaticBenchFn, StaticTestFn, StaticTestName,\n+            TestDesc, TestDescAndFn, TestName, TestType,\n+        },\n+        assert_test_result, filter_tests, run_test, test_main, test_main_static,\n     };\n }\n \n-mod formatters;\n-pub mod stats;\n-\n-use crate::formatters::{JsonFormatter, OutputFormatter, PrettyFormatter, TerseFormatter};\n-\n-/// Whether to execute tests concurrently or not\n-#[derive(Copy, Clone, Debug, PartialEq, Eq)]\n-pub enum Concurrent {\n-    Yes,\n-    No,\n-}\n-\n-/// Type of the test according to the [rust book](https://doc.rust-lang.org/cargo/guide/tests.html)\n-/// conventions.\n-#[derive(Copy, Clone, Debug, PartialEq, Eq, Hash)]\n-pub enum TestType {\n-    /// Unit-tests are expected to be in the `src` folder of the crate.\n-    UnitTest,\n-    /// Integration-style tests are expected to be in the `tests` folder of the crate.\n-    IntegrationTest,\n-    /// Doctests are created by the `librustdoc` manually, so it's a different type of test.\n-    DocTest,\n-    /// Tests for the sources that don't follow the project layout convention\n-    /// (e.g. tests in raw `main.rs` compiled by calling `rustc --test` directly).\n-    Unknown,\n-}\n-\n-// The name of a test. By convention this follows the rules for rust\n-// paths; i.e., it should be a series of identifiers separated by double\n-// colons. This way if some test runner wants to arrange the tests\n-// hierarchically it may.\n-\n-#[derive(Clone, PartialEq, Eq, Hash, Debug)]\n-pub enum TestName {\n-    StaticTestName(&'static str),\n-    DynTestName(String),\n-    AlignedTestName(Cow<'static, str>, NamePadding),\n-}\n-impl TestName {\n-    fn as_slice(&self) -> &str {\n-        match *self {\n-            StaticTestName(s) => s,\n-            DynTestName(ref s) => s,\n-            AlignedTestName(ref s, _) => &*s,\n-        }\n-    }\n-\n-    fn padding(&self) -> NamePadding {\n-        match self {\n-            &AlignedTestName(_, p) => p,\n-            _ => PadNone,\n-        }\n-    }\n-\n-    fn with_padding(&self, padding: NamePadding) -> TestName {\n-        let name = match self {\n-            &TestName::StaticTestName(name) => Cow::Borrowed(name),\n-            &TestName::DynTestName(ref name) => Cow::Owned(name.clone()),\n-            &TestName::AlignedTestName(ref name, _) => name.clone(),\n-        };\n-\n-        TestName::AlignedTestName(name, padding)\n-    }\n-}\n-impl fmt::Display for TestName {\n-    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {\n-        fmt::Display::fmt(self.as_slice(), f)\n-    }\n-}\n-\n-#[derive(Clone, Copy, PartialEq, Eq, Hash, Debug)]\n-pub enum NamePadding {\n-    PadNone,\n-    PadOnRight,\n-}\n-\n-impl TestDesc {\n-    fn padded_name(&self, column_count: usize, align: NamePadding) -> String {\n-        let mut name = String::from(self.name.as_slice());\n-        let fill = column_count.saturating_sub(name.len());\n-        let pad = \" \".repeat(fill);\n-        match align {\n-            PadNone => name,\n-            PadOnRight => {\n-                name.push_str(&pad);\n-                name\n-            }\n-        }\n-    }\n-}\n-\n-/// Represents a benchmark function.\n-pub trait TDynBenchFn: Send {\n-    fn run(&self, harness: &mut Bencher);\n-}\n-\n-// A function that runs a test. If the function returns successfully,\n-// the test succeeds; if the function panics then the test fails. We\n-// may need to come up with a more clever definition of test in order\n-// to support isolation of tests into threads.\n-pub enum TestFn {\n-    StaticTestFn(fn()),\n-    StaticBenchFn(fn(&mut Bencher)),\n-    DynTestFn(Box<dyn FnOnce() + Send>),\n-    DynBenchFn(Box<dyn TDynBenchFn + 'static>),\n-}\n-\n-impl TestFn {\n-    fn padding(&self) -> NamePadding {\n-        match *self {\n-            StaticTestFn(..) => PadNone,\n-            StaticBenchFn(..) => PadOnRight,\n-            DynTestFn(..) => PadNone,\n-            DynBenchFn(..) => PadOnRight,\n-        }\n-    }\n-}\n-\n-impl fmt::Debug for TestFn {\n-    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {\n-        f.write_str(match *self {\n-            StaticTestFn(..) => \"StaticTestFn(..)\",\n-            StaticBenchFn(..) => \"StaticBenchFn(..)\",\n-            DynTestFn(..) => \"DynTestFn(..)\",\n-            DynBenchFn(..) => \"DynBenchFn(..)\",\n-        })\n-    }\n-}\n-\n-/// Manager of the benchmarking runs.\n-///\n-/// This is fed into functions marked with `#[bench]` to allow for\n-/// set-up & tear-down before running a piece of code repeatedly via a\n-/// call to `iter`.\n-#[derive(Clone)]\n-pub struct Bencher {\n-    mode: BenchMode,\n-    summary: Option<stats::Summary>,\n-    pub bytes: u64,\n-}\n-\n-#[derive(Clone, PartialEq, Eq)]\n-pub enum BenchMode {\n-    Auto,\n-    Single,\n-}\n-\n-#[derive(Copy, Clone, Debug, PartialEq, Eq, Hash)]\n-pub enum ShouldPanic {\n-    No,\n-    Yes,\n-    YesWithMessage(&'static str),\n-}\n-\n-// The definition of a single test. A test runner will run a list of\n-// these.\n-#[derive(Clone, Debug, PartialEq, Eq, Hash)]\n-pub struct TestDesc {\n-    pub name: TestName,\n-    pub ignore: bool,\n-    pub should_panic: ShouldPanic,\n-    pub allow_fail: bool,\n-    pub test_type: TestType,\n-}\n-\n-#[derive(Debug)]\n-pub struct TestDescAndFn {\n-    pub desc: TestDesc,\n-    pub testfn: TestFn,\n-}\n+use std::{\n+    env,\n+    io,\n+    io::prelude::Write,\n+    panic::{self, catch_unwind, AssertUnwindSafe, PanicInfo},\n+    process,\n+    process::{Command, Termination},\n+    sync::mpsc::{channel, Sender},\n+    sync::{Arc, Mutex},\n+    thread,\n+    time::{Duration, Instant},\n+};\n \n-#[derive(Clone, PartialEq, Debug, Copy)]\n-pub struct Metric {\n-    value: f64,\n-    noise: f64,\n-}\n-\n-impl Metric {\n-    pub fn new(value: f64, noise: f64) -> Metric {\n-        Metric { value, noise }\n-    }\n-}\n+pub mod stats;\n+pub mod bench;\n+mod formatters;\n+mod cli;\n+mod console;\n+mod event;\n+mod helpers;\n+mod time;\n+mod types;\n+mod options;\n+mod test_result;\n \n-/// In case we want to add other options as well, just add them in this struct.\n-#[derive(Copy, Clone, Debug)]\n-pub struct Options {\n-    display_output: bool,\n-    panic_abort: bool,\n-}\n+#[cfg(test)]\n+mod tests;\n \n-impl Options {\n-    pub fn new() -> Options {\n-        Options {\n-            display_output: false,\n-            panic_abort: false,\n-        }\n-    }\n+use test_result::*;\n+use time::TestExecTime;\n+use options::{RunStrategy, Concurrent, RunIgnored, ColorConfig};\n+use event::{CompletedTest, TestEvent};\n+use cli::TestOpts;\n+use helpers::sink::Sink;\n+use helpers::concurrency::get_concurrency;\n+use helpers::exit_code::get_exit_code;\n \n-    pub fn display_output(mut self, display_output: bool) -> Options {\n-        self.display_output = display_output;\n-        self\n-    }\n+// Process exit code to be used to indicate test failures.\n+const ERROR_EXIT_CODE: i32 = 101;\n \n-    pub fn panic_abort(mut self, panic_abort: bool) -> Options {\n-        self.panic_abort = panic_abort;\n-        self\n-    }\n-}\n+const SECONDARY_TEST_INVOKER_VAR: &'static str = \"__RUST_TEST_INVOKE\";\n \n // The default console test runner. It accepts the command line\n // arguments and a vector of test_descs.\n pub fn test_main(args: &[String], tests: Vec<TestDescAndFn>, options: Option<Options>) {\n-    let mut opts = match parse_opts(args) {\n+    let mut opts = match cli::parse_opts(args) {\n         Some(Ok(o)) => o,\n         Some(Err(msg)) => {\n             eprintln!(\"error: {}\", msg);\n-            process::exit(101);\n+            process::exit(ERROR_EXIT_CODE);\n         }\n         None => return,\n     };\n     if let Some(options) = options {\n         opts.options = options;\n     }\n     if opts.list {\n-        if let Err(e) = list_tests_console(&opts, tests) {\n+        if let Err(e) = console::list_tests_console(&opts, tests) {\n             eprintln!(\"error: io error when listing tests: {:?}\", e);\n-            process::exit(101);\n+            process::exit(ERROR_EXIT_CODE);\n         }\n     } else {\n-        match run_tests_console(&opts, tests) {\n+        match console::run_tests_console(&opts, tests) {\n             Ok(true) => {}\n-            Ok(false) => process::exit(101),\n+            Ok(false) => process::exit(ERROR_EXIT_CODE),\n             Err(e) => {\n                 eprintln!(\"error: io error when listing tests: {:?}\", e);\n-                process::exit(101);\n+                process::exit(ERROR_EXIT_CODE);\n             }\n         }\n     }\n@@ -440,938 +200,11 @@ pub fn assert_test_result<T: Termination>(result: T) {\n     );\n }\n \n-#[derive(Copy, Clone, Debug)]\n-pub enum ColorConfig {\n-    AutoColor,\n-    AlwaysColor,\n-    NeverColor,\n-}\n-\n-#[derive(Copy, Clone, Debug, PartialEq, Eq)]\n-pub enum OutputFormat {\n-    Pretty,\n-    Terse,\n-    Json,\n-}\n-\n-#[derive(Copy, Clone, Debug, PartialEq, Eq)]\n-pub enum RunIgnored {\n-    Yes,\n-    No,\n-    Only,\n-}\n-\n-/// Structure denoting time limits for test execution.\n-#[derive(Copy, Clone, Debug, Default, PartialEq, Eq)]\n-pub struct TimeThreshold {\n-    pub warn: Duration,\n-    pub critical: Duration,\n-}\n-\n-impl TimeThreshold {\n-    /// Creates a new `TimeThreshold` instance with provided durations.\n-    pub fn new(warn: Duration, critical: Duration) -> Self {\n-        Self {\n-            warn,\n-            critical,\n-        }\n-    }\n-\n-    /// Attempts to create a `TimeThreshold` instance with values obtained\n-    /// from the environment variable, and returns `None` if the variable\n-    /// is not set.\n-    /// Environment variable format is expected to match `\\d+,\\d+`.\n-    ///\n-    /// # Panics\n-    ///\n-    /// Panics if variable with provided name is set but contains inappropriate\n-    /// value.\n-    pub fn from_env_var(env_var_name: &str) -> Option<Self> {\n-        let durations_str = env::var(env_var_name).ok()?;\n-\n-        // Split string into 2 substrings by comma and try to parse numbers.\n-        let mut durations = durations_str\n-            .splitn(2, ',')\n-            .map(|v| {\n-                u64::from_str(v).unwrap_or_else(|_| {\n-                    panic!(\n-                        \"Duration value in variable {} is expected to be a number, but got {}\",\n-                        env_var_name, v\n-                    )\n-                })\n-            });\n-\n-        // Callback to be called if the environment variable has unexpected structure.\n-        let panic_on_incorrect_value = || {\n-            panic!(\n-                \"Duration variable {} expected to have 2 numbers separated by comma, but got {}\",\n-                env_var_name, durations_str\n-            );\n-        };\n-\n-        let (warn, critical) = (\n-            durations.next().unwrap_or_else(panic_on_incorrect_value),\n-            durations.next().unwrap_or_else(panic_on_incorrect_value)\n-        );\n-\n-        if warn > critical {\n-            panic!(\"Test execution warn time should be less or equal to the critical time\");\n-        }\n-\n-        Some(Self::new(Duration::from_millis(warn), Duration::from_millis(critical)))\n-    }\n-}\n-\n-/// Structure with parameters for calculating test execution time.\n-#[derive(Copy, Clone, Debug, Default, PartialEq, Eq)]\n-pub struct TestTimeOptions {\n-    /// Denotes if the test critical execution time limit excess should be considered\n-    /// a test failure.\n-    pub error_on_excess: bool,\n-    pub colored: bool,\n-    pub unit_threshold: TimeThreshold,\n-    pub integration_threshold: TimeThreshold,\n-    pub doctest_threshold: TimeThreshold,\n-}\n-\n-impl TestTimeOptions {\n-    pub fn new_from_env(error_on_excess: bool, colored: bool) -> Self {\n-        let unit_threshold =\n-            TimeThreshold::from_env_var(time_constants::UNIT_ENV_NAME)\n-                .unwrap_or_else(Self::default_unit);\n-\n-        let integration_threshold =\n-            TimeThreshold::from_env_var(time_constants::INTEGRATION_ENV_NAME)\n-                .unwrap_or_else(Self::default_integration);\n-\n-        let doctest_threshold =\n-            TimeThreshold::from_env_var(time_constants::DOCTEST_ENV_NAME)\n-                .unwrap_or_else(Self::default_doctest);\n-\n-        Self {\n-            error_on_excess,\n-            colored,\n-            unit_threshold,\n-            integration_threshold,\n-            doctest_threshold,\n-        }\n-    }\n-\n-    pub fn is_warn(&self, test: &TestDesc, exec_time: &TestExecTime) -> bool {\n-        exec_time.0 >= self.warn_time(test)\n-    }\n-\n-    pub fn is_critical(&self, test: &TestDesc, exec_time: &TestExecTime) -> bool {\n-        exec_time.0 >= self.critical_time(test)\n-    }\n-\n-    fn warn_time(&self, test: &TestDesc) -> Duration {\n-        match test.test_type {\n-            TestType::UnitTest => self.unit_threshold.warn,\n-            TestType::IntegrationTest => self.integration_threshold.warn,\n-            TestType::DocTest => self.doctest_threshold.warn,\n-            TestType::Unknown => time_constants::UNKNOWN_WARN,\n-        }\n-    }\n-\n-    fn critical_time(&self, test: &TestDesc) -> Duration {\n-        match test.test_type {\n-            TestType::UnitTest => self.unit_threshold.critical,\n-            TestType::IntegrationTest => self.integration_threshold.critical,\n-            TestType::DocTest => self.doctest_threshold.critical,\n-            TestType::Unknown => time_constants::UNKNOWN_CRITICAL,\n-        }\n-    }\n-\n-    fn default_unit() -> TimeThreshold {\n-        TimeThreshold::new(time_constants::UNIT_WARN, time_constants::UNIT_CRITICAL)\n-    }\n-\n-    fn default_integration() -> TimeThreshold {\n-        TimeThreshold::new(time_constants::INTEGRATION_WARN, time_constants::INTEGRATION_CRITICAL)\n-    }\n-\n-    fn default_doctest() -> TimeThreshold {\n-        TimeThreshold::new(time_constants::DOCTEST_WARN, time_constants::DOCTEST_CRITICAL)\n-    }\n-}\n-\n-#[derive(Debug)]\n-pub struct TestOpts {\n-    pub list: bool,\n-    pub filter: Option<String>,\n-    pub filter_exact: bool,\n-    pub exclude_should_panic: bool,\n-    pub run_ignored: RunIgnored,\n-    pub run_tests: bool,\n-    pub bench_benchmarks: bool,\n-    pub logfile: Option<PathBuf>,\n-    pub nocapture: bool,\n-    pub color: ColorConfig,\n-    pub format: OutputFormat,\n-    pub test_threads: Option<usize>,\n-    pub skip: Vec<String>,\n-    pub time_options: Option<TestTimeOptions>,\n-    pub options: Options,\n-}\n-\n-/// Result of parsing the options.\n-pub type OptRes = Result<TestOpts, String>;\n-/// Result of parsing the option part.\n-type OptPartRes<T> = Result<Option<T>, String>;\n-\n-fn optgroups() -> getopts::Options {\n-    let mut opts = getopts::Options::new();\n-    opts.optflag(\"\", \"include-ignored\", \"Run ignored and not ignored tests\")\n-        .optflag(\"\", \"ignored\", \"Run only ignored tests\")\n-        .optflag(\"\", \"exclude-should-panic\", \"Excludes tests marked as should_panic\")\n-        .optflag(\"\", \"test\", \"Run tests and not benchmarks\")\n-        .optflag(\"\", \"bench\", \"Run benchmarks instead of tests\")\n-        .optflag(\"\", \"list\", \"List all tests and benchmarks\")\n-        .optflag(\"h\", \"help\", \"Display this message (longer with --help)\")\n-        .optopt(\n-            \"\",\n-            \"logfile\",\n-            \"Write logs to the specified file instead \\\n-             of stdout\",\n-            \"PATH\",\n-        )\n-        .optflag(\n-            \"\",\n-            \"nocapture\",\n-            \"don't capture stdout/stderr of each \\\n-             task, allow printing directly\",\n-        )\n-        .optopt(\n-            \"\",\n-            \"test-threads\",\n-            \"Number of threads used for running tests \\\n-             in parallel\",\n-            \"n_threads\",\n-        )\n-        .optmulti(\n-            \"\",\n-            \"skip\",\n-            \"Skip tests whose names contain FILTER (this flag can \\\n-             be used multiple times)\",\n-            \"FILTER\",\n-        )\n-        .optflag(\n-            \"q\",\n-            \"quiet\",\n-            \"Display one character per test instead of one line. \\\n-             Alias to --format=terse\",\n-        )\n-        .optflag(\n-            \"\",\n-            \"exact\",\n-            \"Exactly match filters rather than by substring\",\n-        )\n-        .optopt(\n-            \"\",\n-            \"color\",\n-            \"Configure coloring of output:\n-            auto   = colorize if stdout is a tty and tests are run on serially (default);\n-            always = always colorize output;\n-            never  = never colorize output;\",\n-            \"auto|always|never\",\n-        )\n-        .optopt(\n-            \"\",\n-            \"format\",\n-            \"Configure formatting of output:\n-            pretty = Print verbose output;\n-            terse  = Display one character per test;\n-            json   = Output a json document\",\n-            \"pretty|terse|json\",\n-        )\n-        .optflag(\n-            \"\",\n-            \"show-output\",\n-            \"Show captured stdout of successful tests\"\n-        )\n-        .optopt(\n-            \"Z\",\n-            \"\",\n-            \"Enable nightly-only flags:\n-            unstable-options = Allow use of experimental features\",\n-            \"unstable-options\",\n-        )\n-        .optflagopt(\n-            \"\",\n-            \"report-time\",\n-            \"Show execution time of each test. Awailable values:\n-            plain   = do not colorize the execution time (default);\n-            colored = colorize output according to the `color` parameter value;\n-\n-            Threshold values for colorized output can be configured via\n-            `RUST_TEST_TIME_UNIT`, `RUST_TEST_TIME_INTEGRATION` and\n-            `RUST_TEST_TIME_DOCTEST` environment variables.\n-\n-            Expected format of environment variable is `VARIABLE=WARN_TIME,CRITICAL_TIME`.\n-\n-            Not available for --format=terse\",\n-            \"plain|colored\"\n-        )\n-        .optflag(\n-            \"\",\n-            \"ensure-time\",\n-            \"Treat excess of the test execution time limit as error.\n-\n-            Threshold values for this option can be configured via\n-            `RUST_TEST_TIME_UNIT`, `RUST_TEST_TIME_INTEGRATION` and\n-            `RUST_TEST_TIME_DOCTEST` environment variables.\n-\n-            Expected format of environment variable is `VARIABLE=WARN_TIME,CRITICAL_TIME`.\n-\n-            `CRITICAL_TIME` here means the limit that should not be exceeded by test.\n-            \"\n-        );\n-    return opts;\n-}\n-\n-fn usage(binary: &str, options: &getopts::Options) {\n-    let message = format!(\"Usage: {} [OPTIONS] [FILTER]\", binary);\n-    println!(\n-        r#\"{usage}\n-\n-The FILTER string is tested against the name of all tests, and only those\n-tests whose names contain the filter are run.\n-\n-By default, all tests are run in parallel. This can be altered with the\n---test-threads flag or the RUST_TEST_THREADS environment variable when running\n-tests (set it to 1).\n-\n-All tests have their standard output and standard error captured by default.\n-This can be overridden with the --nocapture flag or setting RUST_TEST_NOCAPTURE\n-environment variable to a value other than \"0\". Logging is not captured by default.\n-\n-Test Attributes:\n-\n-    `#[test]`        - Indicates a function is a test to be run. This function\n-                       takes no arguments.\n-    `#[bench]`       - Indicates a function is a benchmark to be run. This\n-                       function takes one argument (test::Bencher).\n-    `#[should_panic]` - This function (also labeled with `#[test]`) will only pass if\n-                        the code causes a panic (an assertion failure or panic!)\n-                        A message may be provided, which the failure string must\n-                        contain: #[should_panic(expected = \"foo\")].\n-    `#[ignore]`       - When applied to a function which is already attributed as a\n-                        test, then the test runner will ignore these tests during\n-                        normal test runs. Running with --ignored or --include-ignored will run\n-                        these tests.\"#,\n-        usage = options.usage(&message)\n-    );\n-}\n-\n-// FIXME: Copied from libsyntax until linkage errors are resolved. Issue #47566\n-fn is_nightly() -> bool {\n-    // Whether this is a feature-staged build, i.e., on the beta or stable channel\n-    let disable_unstable_features = option_env!(\"CFG_DISABLE_UNSTABLE_FEATURES\").is_some();\n-    // Whether we should enable unstable features for bootstrapping\n-    let bootstrap = env::var(\"RUSTC_BOOTSTRAP\").is_ok();\n-\n-    bootstrap || !disable_unstable_features\n-}\n-\n-// Gets the option value and checks if unstable features are enabled.\n-macro_rules! unstable_optflag {\n-    ($matches:ident, $allow_unstable:ident, $option_name:literal) => {{\n-        let opt = $matches.opt_present($option_name);\n-        if !$allow_unstable && opt {\n-            return Some(Err(format!(\n-                \"The \\\"{}\\\" flag is only accepted on the nightly compiler\",\n-                $option_name\n-            )));\n-        }\n-\n-        opt\n-    }};\n-}\n-\n-// Gets the CLI options assotiated with `report-time` feature.\n-fn get_time_options(\n-    matches: &getopts::Matches,\n-    allow_unstable: bool)\n--> Option<OptPartRes<TestTimeOptions>> {\n-    let report_time = unstable_optflag!(matches, allow_unstable, \"report-time\");\n-    let colored_opt_str = matches.opt_str(\"report-time\");\n-    let mut report_time_colored = report_time && colored_opt_str == Some(\"colored\".into());\n-    let ensure_test_time = unstable_optflag!(matches, allow_unstable, \"ensure-time\");\n-\n-    // If `ensure-test-time` option is provided, time output is enforced,\n-    // so user won't be confused if any of tests will silently fail.\n-    let options = if report_time || ensure_test_time {\n-        if ensure_test_time && !report_time {\n-            report_time_colored = true;\n-        }\n-        Some(TestTimeOptions::new_from_env(ensure_test_time, report_time_colored))\n-    } else {\n-        None\n-    };\n-\n-    Some(Ok(options))\n-}\n-\n-// Parses command line arguments into test options\n-pub fn parse_opts(args: &[String]) -> Option<OptRes> {\n-    let mut allow_unstable = false;\n-    let opts = optgroups();\n-    let args = args.get(1..).unwrap_or(args);\n-    let matches = match opts.parse(args) {\n-        Ok(m) => m,\n-        Err(f) => return Some(Err(f.to_string())),\n-    };\n-\n-    if let Some(opt) = matches.opt_str(\"Z\") {\n-        if !is_nightly() {\n-            return Some(Err(\n-                \"the option `Z` is only accepted on the nightly compiler\".into(),\n-            ));\n-        }\n-\n-        match &*opt {\n-            \"unstable-options\" => {\n-                allow_unstable = true;\n-            }\n-            _ => {\n-                return Some(Err(\"Unrecognized option to `Z`\".into()));\n-            }\n-        }\n-    };\n-\n-    if matches.opt_present(\"h\") {\n-        usage(&args[0], &opts);\n-        return None;\n-    }\n-\n-    let filter = if !matches.free.is_empty() {\n-        Some(matches.free[0].clone())\n-    } else {\n-        None\n-    };\n-\n-    let exclude_should_panic = unstable_optflag!(matches, allow_unstable, \"exclude-should-panic\");\n-\n-    let include_ignored = unstable_optflag!(matches, allow_unstable, \"include-ignored\");\n-\n-    let run_ignored = match (include_ignored, matches.opt_present(\"ignored\")) {\n-        (true, true) => {\n-            return Some(Err(\n-                \"the options --include-ignored and --ignored are mutually exclusive\".into(),\n-            ));\n-        }\n-        (true, false) => RunIgnored::Yes,\n-        (false, true) => RunIgnored::Only,\n-        (false, false) => RunIgnored::No,\n-    };\n-    let quiet = matches.opt_present(\"quiet\");\n-    let exact = matches.opt_present(\"exact\");\n-    let list = matches.opt_present(\"list\");\n-\n-    let logfile = matches.opt_str(\"logfile\");\n-    let logfile = logfile.map(|s| PathBuf::from(&s));\n-\n-    let bench_benchmarks = matches.opt_present(\"bench\");\n-    let run_tests = !bench_benchmarks || matches.opt_present(\"test\");\n-\n-    let mut nocapture = matches.opt_present(\"nocapture\");\n-    if !nocapture {\n-        nocapture = match env::var(\"RUST_TEST_NOCAPTURE\") {\n-            Ok(val) => &val != \"0\",\n-            Err(_) => false,\n-        };\n-    }\n-\n-    let time_options = match get_time_options(&matches, allow_unstable) {\n-        Some(Ok(val)) => val,\n-        Some(Err(e)) => return Some(Err(e)),\n-        None => panic!(\"Unexpected output from `get_time_options`\"),\n-    };\n-\n-    let test_threads = match matches.opt_str(\"test-threads\") {\n-        Some(n_str) => match n_str.parse::<usize>() {\n-            Ok(0) => return Some(Err(\"argument for --test-threads must not be 0\".to_string())),\n-            Ok(n) => Some(n),\n-            Err(e) => {\n-                return Some(Err(format!(\n-                    \"argument for --test-threads must be a number > 0 \\\n-                     (error: {})\",\n-                    e\n-                )));\n-            }\n-        },\n-        None => None,\n-    };\n-\n-    let color = match matches.opt_str(\"color\").as_ref().map(|s| &**s) {\n-        Some(\"auto\") | None => AutoColor,\n-        Some(\"always\") => AlwaysColor,\n-        Some(\"never\") => NeverColor,\n-\n-        Some(v) => {\n-            return Some(Err(format!(\n-                \"argument for --color must be auto, always, or never (was \\\n-                 {})\",\n-                v\n-            )));\n-        }\n-    };\n-\n-    let format = match matches.opt_str(\"format\").as_ref().map(|s| &**s) {\n-        None if quiet => OutputFormat::Terse,\n-        Some(\"pretty\") | None => OutputFormat::Pretty,\n-        Some(\"terse\") => OutputFormat::Terse,\n-        Some(\"json\") => {\n-            if !allow_unstable {\n-                return Some(Err(\n-                    \"The \\\"json\\\" format is only accepted on the nightly compiler\".into(),\n-                ));\n-            }\n-            OutputFormat::Json\n-        }\n-\n-        Some(v) => {\n-            return Some(Err(format!(\n-                \"argument for --format must be pretty, terse, or json (was \\\n-                 {})\",\n-                v\n-            )));\n-        }\n-    };\n-\n-    let test_opts = TestOpts {\n-        list,\n-        filter,\n-        filter_exact: exact,\n-        exclude_should_panic,\n-        run_ignored,\n-        run_tests,\n-        bench_benchmarks,\n-        logfile,\n-        nocapture,\n-        color,\n-        format,\n-        test_threads,\n-        skip: matches.opt_strs(\"skip\"),\n-        time_options,\n-        options: Options::new().display_output(matches.opt_present(\"show-output\")),\n-    };\n-\n-    Some(Ok(test_opts))\n-}\n-\n-#[derive(Debug, Clone, PartialEq)]\n-pub struct BenchSamples {\n-    ns_iter_summ: stats::Summary,\n-    mb_s: usize,\n-}\n-\n-#[derive(Debug, Clone, PartialEq)]\n-pub enum TestResult {\n-    TrOk,\n-    TrFailed,\n-    TrFailedMsg(String),\n-    TrIgnored,\n-    TrAllowedFail,\n-    TrBench(BenchSamples),\n-    TrTimedFail,\n-}\n-\n-unsafe impl Send for TestResult {}\n-\n-/// The meassured execution time of a unit test.\n-#[derive(Clone, PartialEq)]\n-pub struct TestExecTime(Duration);\n-\n-impl fmt::Display for TestExecTime {\n-    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {\n-        write!(f, \"{:.3}s\", self.0.as_secs_f64())\n-    }\n-}\n-\n-enum OutputLocation<T> {\n-    Pretty(Box<term::StdoutTerminal>),\n-    Raw(T),\n-}\n-\n-impl<T: Write> Write for OutputLocation<T> {\n-    fn write(&mut self, buf: &[u8]) -> io::Result<usize> {\n-        match *self {\n-            Pretty(ref mut term) => term.write(buf),\n-            Raw(ref mut stdout) => stdout.write(buf),\n-        }\n-    }\n-\n-    fn flush(&mut self) -> io::Result<()> {\n-        match *self {\n-            Pretty(ref mut term) => term.flush(),\n-            Raw(ref mut stdout) => stdout.flush(),\n-        }\n-    }\n-}\n-\n-struct ConsoleTestState {\n-    log_out: Option<File>,\n-    total: usize,\n-    passed: usize,\n-    failed: usize,\n-    ignored: usize,\n-    allowed_fail: usize,\n-    filtered_out: usize,\n-    measured: usize,\n-    metrics: MetricMap,\n-    failures: Vec<(TestDesc, Vec<u8>)>,\n-    not_failures: Vec<(TestDesc, Vec<u8>)>,\n-    time_failures: Vec<(TestDesc, Vec<u8>)>,\n-    options: Options,\n-}\n-\n-impl ConsoleTestState {\n-    pub fn new(opts: &TestOpts) -> io::Result<ConsoleTestState> {\n-        let log_out = match opts.logfile {\n-            Some(ref path) => Some(File::create(path)?),\n-            None => None,\n-        };\n-\n-        Ok(ConsoleTestState {\n-            log_out,\n-            total: 0,\n-            passed: 0,\n-            failed: 0,\n-            ignored: 0,\n-            allowed_fail: 0,\n-            filtered_out: 0,\n-            measured: 0,\n-            metrics: MetricMap::new(),\n-            failures: Vec::new(),\n-            not_failures: Vec::new(),\n-            time_failures: Vec::new(),\n-            options: opts.options,\n-        })\n-    }\n-\n-    pub fn write_log<F, S>(\n-        &mut self,\n-        msg: F,\n-    ) -> io::Result<()>\n-    where\n-        S: AsRef<str>,\n-        F: FnOnce() -> S,\n-    {\n-        match self.log_out {\n-            None => Ok(()),\n-            Some(ref mut o) => {\n-                let msg = msg();\n-                let msg = msg.as_ref();\n-                o.write_all(msg.as_bytes())\n-            },\n-        }\n-    }\n-\n-    pub fn write_log_result(&mut self,test: &TestDesc,\n-        result: &TestResult,\n-        exec_time: Option<&TestExecTime>,\n-    ) -> io::Result<()> {\n-        self.write_log(|| format!(\n-            \"{} {}\",\n-            match *result {\n-                TrOk => \"ok\".to_owned(),\n-                TrFailed => \"failed\".to_owned(),\n-                TrFailedMsg(ref msg) => format!(\"failed: {}\", msg),\n-                TrIgnored => \"ignored\".to_owned(),\n-                TrAllowedFail => \"failed (allowed)\".to_owned(),\n-                TrBench(ref bs) => fmt_bench_samples(bs),\n-                TrTimedFail => \"failed (time limit exceeded)\".to_owned(),\n-            },\n-            test.name,\n-        ))?;\n-        if let Some(exec_time) = exec_time {\n-            self.write_log(|| format!(\" <{}>\", exec_time))?;\n-        }\n-        self.write_log(|| \"\\n\")\n-    }\n-\n-    fn current_test_count(&self) -> usize {\n-        self.passed + self.failed + self.ignored + self.measured + self.allowed_fail\n-    }\n-}\n-\n-// Format a number with thousands separators\n-fn fmt_thousands_sep(mut n: usize, sep: char) -> String {\n-    use std::fmt::Write;\n-    let mut output = String::new();\n-    let mut trailing = false;\n-    for &pow in &[9, 6, 3, 0] {\n-        let base = 10_usize.pow(pow);\n-        if pow == 0 || trailing || n / base != 0 {\n-            if !trailing {\n-                output.write_fmt(format_args!(\"{}\", n / base)).unwrap();\n-            } else {\n-                output.write_fmt(format_args!(\"{:03}\", n / base)).unwrap();\n-            }\n-            if pow != 0 {\n-                output.push(sep);\n-            }\n-            trailing = true;\n-        }\n-        n %= base;\n-    }\n-\n-    output\n-}\n-\n-pub fn fmt_bench_samples(bs: &BenchSamples) -> String {\n-    use std::fmt::Write;\n-    let mut output = String::new();\n-\n-    let median = bs.ns_iter_summ.median as usize;\n-    let deviation = (bs.ns_iter_summ.max - bs.ns_iter_summ.min) as usize;\n-\n-    output\n-        .write_fmt(format_args!(\n-            \"{:>11} ns/iter (+/- {})\",\n-            fmt_thousands_sep(median, ','),\n-            fmt_thousands_sep(deviation, ',')\n-        ))\n-        .unwrap();\n-    if bs.mb_s != 0 {\n-        output\n-            .write_fmt(format_args!(\" = {} MB/s\", bs.mb_s))\n-            .unwrap();\n-    }\n-    output\n-}\n-\n-// List the tests to console, and optionally to logfile. Filters are honored.\n-pub fn list_tests_console(opts: &TestOpts, tests: Vec<TestDescAndFn>) -> io::Result<()> {\n-    let mut output = match term::stdout() {\n-        None => Raw(io::stdout()),\n-        Some(t) => Pretty(t),\n-    };\n-\n-    let quiet = opts.format == OutputFormat::Terse;\n-    let mut st = ConsoleTestState::new(opts)?;\n-\n-    let mut ntest = 0;\n-    let mut nbench = 0;\n-\n-    for test in filter_tests(&opts, tests) {\n-        use crate::TestFn::*;\n-\n-        let TestDescAndFn {\n-            desc: TestDesc { name, .. },\n-            testfn,\n-        } = test;\n-\n-        let fntype = match testfn {\n-            StaticTestFn(..) | DynTestFn(..) => {\n-                ntest += 1;\n-                \"test\"\n-            }\n-            StaticBenchFn(..) | DynBenchFn(..) => {\n-                nbench += 1;\n-                \"benchmark\"\n-            }\n-        };\n-\n-        writeln!(output, \"{}: {}\", name, fntype)?;\n-        st.write_log(|| format!(\"{} {}\\n\", fntype, name))?;\n-    }\n-\n-    fn plural(count: u32, s: &str) -> String {\n-        match count {\n-            1 => format!(\"{} {}\", 1, s),\n-            n => format!(\"{} {}s\", n, s),\n-        }\n-    }\n-\n-    if !quiet {\n-        if ntest != 0 || nbench != 0 {\n-            writeln!(output, \"\")?;\n-        }\n-\n-        writeln!(\n-            output,\n-            \"{}, {}\",\n-            plural(ntest, \"test\"),\n-            plural(nbench, \"benchmark\")\n-        )?;\n-    }\n-\n-    Ok(())\n-}\n-\n-// A simple console test runner\n-pub fn run_tests_console(opts: &TestOpts, tests: Vec<TestDescAndFn>) -> io::Result<bool> {\n-    fn callback(\n-        event: &TestEvent,\n-        st: &mut ConsoleTestState,\n-        out: &mut dyn OutputFormatter,\n-    ) -> io::Result<()> {\n-        match (*event).clone() {\n-            TeFiltered(ref filtered_tests) => {\n-                st.total = filtered_tests.len();\n-                out.write_run_start(filtered_tests.len())\n-            }\n-            TeFilteredOut(filtered_out) => Ok(st.filtered_out = filtered_out),\n-            TeWait(ref test) => out.write_test_start(test),\n-            TeTimeout(ref test) => out.write_timeout(test),\n-            TeResult(test, result, exec_time, stdout) => {\n-                st.write_log_result(&test, &result, exec_time.as_ref())?;\n-                out.write_result(&test, &result, exec_time.as_ref(), &*stdout, &st)?;\n-                match result {\n-                    TrOk => {\n-                        st.passed += 1;\n-                        st.not_failures.push((test, stdout));\n-                    }\n-                    TrIgnored => st.ignored += 1,\n-                    TrAllowedFail => st.allowed_fail += 1,\n-                    TrBench(bs) => {\n-                        st.metrics.insert_metric(\n-                            test.name.as_slice(),\n-                            bs.ns_iter_summ.median,\n-                            bs.ns_iter_summ.max - bs.ns_iter_summ.min,\n-                        );\n-                        st.measured += 1\n-                    }\n-                    TrFailed => {\n-                        st.failed += 1;\n-                        st.failures.push((test, stdout));\n-                    }\n-                    TrFailedMsg(msg) => {\n-                        st.failed += 1;\n-                        let mut stdout = stdout;\n-                        stdout.extend_from_slice(format!(\"note: {}\", msg).as_bytes());\n-                        st.failures.push((test, stdout));\n-                    }\n-                    TrTimedFail => {\n-                        st.failed += 1;\n-                        st.time_failures.push((test, stdout));\n-                    }\n-                }\n-                Ok(())\n-            }\n-        }\n-    }\n-\n-    let output = match term::stdout() {\n-        None => Raw(io::stdout()),\n-        Some(t) => Pretty(t),\n-    };\n-\n-    let max_name_len = tests\n-        .iter()\n-        .max_by_key(|t| len_if_padded(*t))\n-        .map(|t| t.desc.name.as_slice().len())\n-        .unwrap_or(0);\n-\n-    let is_multithreaded = opts.test_threads.unwrap_or_else(get_concurrency) > 1;\n-\n-    let mut out: Box<dyn OutputFormatter> = match opts.format {\n-        OutputFormat::Pretty => Box::new(PrettyFormatter::new(\n-            output,\n-            use_color(opts),\n-            max_name_len,\n-            is_multithreaded,\n-            opts.time_options,\n-        )),\n-        OutputFormat::Terse => Box::new(TerseFormatter::new(\n-            output,\n-            use_color(opts),\n-            max_name_len,\n-            is_multithreaded,\n-        )),\n-        OutputFormat::Json => Box::new(JsonFormatter::new(output)),\n-    };\n-    let mut st = ConsoleTestState::new(opts)?;\n-    fn len_if_padded(t: &TestDescAndFn) -> usize {\n-        match t.testfn.padding() {\n-            PadNone => 0,\n-            PadOnRight => t.desc.name.as_slice().len(),\n-        }\n-    }\n-\n-    run_tests(opts, tests, |x| callback(&x, &mut st, &mut *out))?;\n-\n-    assert!(st.current_test_count() == st.total);\n-\n-    return out.write_run_finish(&st);\n-}\n-\n-fn use_color(opts: &TestOpts) -> bool {\n-    match opts.color {\n-        AutoColor => !opts.nocapture && stdout_isatty(),\n-        AlwaysColor => true,\n-        NeverColor => false,\n-    }\n-}\n-\n-#[cfg(any(\n-    target_os = \"cloudabi\",\n-    all(target_arch = \"wasm32\", not(target_os = \"emscripten\")),\n-    all(target_vendor = \"fortanix\", target_env = \"sgx\")\n-))]\n-fn stdout_isatty() -> bool {\n-    // FIXME: Implement isatty on SGX\n-    false\n-}\n-#[cfg(unix)]\n-fn stdout_isatty() -> bool {\n-    unsafe { libc::isatty(libc::STDOUT_FILENO) != 0 }\n-}\n-#[cfg(windows)]\n-fn stdout_isatty() -> bool {\n-    type DWORD = u32;\n-    type BOOL = i32;\n-    type HANDLE = *mut u8;\n-    type LPDWORD = *mut u32;\n-    const STD_OUTPUT_HANDLE: DWORD = -11i32 as DWORD;\n-    extern \"system\" {\n-        fn GetStdHandle(which: DWORD) -> HANDLE;\n-        fn GetConsoleMode(hConsoleHandle: HANDLE, lpMode: LPDWORD) -> BOOL;\n-    }\n-    unsafe {\n-        let handle = GetStdHandle(STD_OUTPUT_HANDLE);\n-        let mut out = 0;\n-        GetConsoleMode(handle, &mut out) != 0\n-    }\n-}\n-\n-#[derive(Clone)]\n-pub enum TestEvent {\n-    TeFiltered(Vec<TestDesc>),\n-    TeWait(TestDesc),\n-    TeResult(TestDesc, TestResult, Option<TestExecTime>, Vec<u8>),\n-    TeTimeout(TestDesc),\n-    TeFilteredOut(usize),\n-}\n-\n-pub type MonitorMsg = (TestDesc, TestResult, Option<TestExecTime>, Vec<u8>);\n-\n-struct Sink(Arc<Mutex<Vec<u8>>>);\n-impl Write for Sink {\n-    fn write(&mut self, data: &[u8]) -> io::Result<usize> {\n-        Write::write(&mut *self.0.lock().unwrap(), data)\n-    }\n-    fn flush(&mut self) -> io::Result<()> {\n-        Ok(())\n-    }\n-}\n-\n-#[derive(Clone, Copy)]\n-pub enum RunStrategy {\n-    /// Runs the test in the current process, and sends the result back over the\n-    /// supplied channel.\n-    InProcess,\n-\n-    /// Spawns a subprocess to run the test, and sends the result back over the\n-    /// supplied channel. Requires `argv[0]` to exist and point to the binary\n-    /// that's currently running.\n-    SpawnPrimary,\n-}\n-\n-pub fn run_tests<F>(opts: &TestOpts, tests: Vec<TestDescAndFn>, mut callback: F) -> io::Result<()>\n+pub fn run_tests<F>(\n+    opts: &TestOpts,\n+    tests: Vec<TestDescAndFn>,\n+    mut notify_about_test_event: F\n+) -> io::Result<()>\n where\n     F: FnMut(TestEvent) -> io::Result<()>,\n {\n@@ -1399,11 +232,13 @@ where\n     };\n \n     let filtered_out = tests_len - filtered_tests.len();\n-    callback(TeFilteredOut(filtered_out))?;\n+    let event = TestEvent::TeFilteredOut(filtered_out);\n+    notify_about_test_event(event)?;\n \n     let filtered_descs = filtered_tests.iter().map(|t| t.desc.clone()).collect();\n \n-    callback(TeFiltered(filtered_descs))?;\n+    let event = TestEvent::TeFiltered(filtered_descs);\n+    notify_about_test_event(event)?;\n \n     let (filtered_tests, filtered_benchs): (Vec<_>, _) =\n         filtered_tests.into_iter().partition(|e| match e.testfn {\n@@ -1417,7 +252,7 @@ where\n     remaining.reverse();\n     let mut pending = 0;\n \n-    let (tx, rx) = channel::<MonitorMsg>();\n+    let (tx, rx) = channel::<CompletedTest>();\n     let run_strategy = if opts.options.panic_abort {\n         RunStrategy::SpawnPrimary\n     } else {\n@@ -1458,18 +293,23 @@ where\n     if concurrency == 1 {\n         while !remaining.is_empty() {\n             let test = remaining.pop().unwrap();\n-            callback(TeWait(test.desc.clone()))?;\n+            let event = TestEvent::TeWait(test.desc.clone());\n+            notify_about_test_event(event)?;\n             run_test(opts, !opts.run_tests, test, run_strategy, tx.clone(), Concurrent::No);\n-            let (test, result, exec_time, stdout) = rx.recv().unwrap();\n-            callback(TeResult(test, result, exec_time, stdout))?;\n+            let completed_test = rx.recv().unwrap();\n+\n+            let event = TestEvent::TeResult(completed_test);\n+            notify_about_test_event(event)?;\n         }\n     } else {\n         while pending > 0 || !remaining.is_empty() {\n             while pending < concurrency && !remaining.is_empty() {\n                 let test = remaining.pop().unwrap();\n-                let timeout = Instant::now() + Duration::from_secs(TEST_WARN_TIMEOUT_S);\n+                let timeout = time::get_default_test_timeout();\n                 running_tests.insert(test.desc.clone(), timeout);\n-                callback(TeWait(test.desc.clone()))?; //here no pad\n+\n+                let event = TestEvent::TeWait(test.desc.clone());\n+                notify_about_test_event(event)?; //here no pad\n                 run_test(opts, !opts.run_tests, test, run_strategy, tx.clone(), Concurrent::Yes);\n                 pending += 1;\n             }\n@@ -1479,182 +319,47 @@ where\n                 if let Some(timeout) = calc_timeout(&running_tests) {\n                     res = rx.recv_timeout(timeout);\n                     for test in get_timed_out_tests(&mut running_tests) {\n-                        callback(TeTimeout(test))?;\n+                        let event = TestEvent::TeTimeout(test);\n+                        notify_about_test_event(event)?;\n                     }\n-                    if res != Err(RecvTimeoutError::Timeout) {\n-                        break;\n+\n+                    match res {\n+                        Err(RecvTimeoutError::Timeout) => {\n+                            // Result is not yet ready, continue waiting.\n+                        }\n+                        _ => {\n+                            // We've got a result, stop the loop.\n+                            break;\n+                        }\n                     }\n                 } else {\n                     res = rx.recv().map_err(|_| RecvTimeoutError::Disconnected);\n                     break;\n                 }\n             }\n \n-            let (desc, result, exec_time, stdout) = res.unwrap();\n-            running_tests.remove(&desc);\n+            let completed_test = res.unwrap();\n+            running_tests.remove(&completed_test.desc);\n \n-            callback(TeResult(desc, result, exec_time, stdout))?;\n+            let event = TestEvent::TeResult(completed_test);\n+            notify_about_test_event(event)?;\n             pending -= 1;\n         }\n     }\n \n     if opts.bench_benchmarks {\n         // All benchmarks run at the end, in serial.\n         for b in filtered_benchs {\n-            callback(TeWait(b.desc.clone()))?;\n+            let event = TestEvent::TeWait(b.desc.clone());\n+            notify_about_test_event(event)?;\n             run_test(opts, false, b, run_strategy, tx.clone(), Concurrent::No);\n-            let (test, result, exec_time, stdout) = rx.recv().unwrap();\n-            callback(TeResult(test, result, exec_time, stdout))?;\n-        }\n-    }\n-    Ok(())\n-}\n-\n-#[allow(deprecated)]\n-fn get_concurrency() -> usize {\n-    return match env::var(\"RUST_TEST_THREADS\") {\n-        Ok(s) => {\n-            let opt_n: Option<usize> = s.parse().ok();\n-            match opt_n {\n-                Some(n) if n > 0 => n,\n-                _ => panic!(\n-                    \"RUST_TEST_THREADS is `{}`, should be a positive integer.\",\n-                    s\n-                ),\n-            }\n-        }\n-        Err(..) => num_cpus(),\n-    };\n-\n-    #[cfg(windows)]\n-    #[allow(nonstandard_style)]\n-    fn num_cpus() -> usize {\n-        #[repr(C)]\n-        struct SYSTEM_INFO {\n-            wProcessorArchitecture: u16,\n-            wReserved: u16,\n-            dwPageSize: u32,\n-            lpMinimumApplicationAddress: *mut u8,\n-            lpMaximumApplicationAddress: *mut u8,\n-            dwActiveProcessorMask: *mut u8,\n-            dwNumberOfProcessors: u32,\n-            dwProcessorType: u32,\n-            dwAllocationGranularity: u32,\n-            wProcessorLevel: u16,\n-            wProcessorRevision: u16,\n-        }\n-        extern \"system\" {\n-            fn GetSystemInfo(info: *mut SYSTEM_INFO) -> i32;\n-        }\n-        unsafe {\n-            let mut sysinfo = std::mem::zeroed();\n-            GetSystemInfo(&mut sysinfo);\n-            sysinfo.dwNumberOfProcessors as usize\n-        }\n-    }\n-\n-    #[cfg(target_os = \"vxworks\")]\n-    fn num_cpus() -> usize {\n-        // FIXME: Implement num_cpus on vxWorks\n-        1\n-    }\n-\n-    #[cfg(target_os = \"redox\")]\n-    fn num_cpus() -> usize {\n-        // FIXME: Implement num_cpus on Redox\n-        1\n-    }\n-\n-    #[cfg(any(\n-        all(target_arch = \"wasm32\", not(target_os = \"emscripten\")),\n-        all(target_vendor = \"fortanix\", target_env = \"sgx\")\n-    ))]\n-    fn num_cpus() -> usize {\n-        1\n-    }\n-\n-    #[cfg(any(\n-        target_os = \"android\",\n-        target_os = \"cloudabi\",\n-        target_os = \"emscripten\",\n-        target_os = \"fuchsia\",\n-        target_os = \"ios\",\n-        target_os = \"linux\",\n-        target_os = \"macos\",\n-        target_os = \"solaris\",\n-    ))]\n-    fn num_cpus() -> usize {\n-        unsafe { libc::sysconf(libc::_SC_NPROCESSORS_ONLN) as usize }\n-    }\n-\n-    #[cfg(any(\n-        target_os = \"freebsd\",\n-        target_os = \"dragonfly\",\n-        target_os = \"netbsd\"\n-    ))]\n-    fn num_cpus() -> usize {\n-        use std::ptr;\n-\n-        let mut cpus: libc::c_uint = 0;\n-        let mut cpus_size = std::mem::size_of_val(&cpus);\n-\n-        unsafe {\n-            cpus = libc::sysconf(libc::_SC_NPROCESSORS_ONLN) as libc::c_uint;\n-        }\n-        if cpus < 1 {\n-            let mut mib = [libc::CTL_HW, libc::HW_NCPU, 0, 0];\n-            unsafe {\n-                libc::sysctl(\n-                    mib.as_mut_ptr(),\n-                    2,\n-                    &mut cpus as *mut _ as *mut _,\n-                    &mut cpus_size as *mut _ as *mut _,\n-                    ptr::null_mut(),\n-                    0,\n-                );\n-            }\n-            if cpus < 1 {\n-                cpus = 1;\n-            }\n-        }\n-        cpus as usize\n-    }\n+            let completed_test = rx.recv().unwrap();\n \n-    #[cfg(target_os = \"openbsd\")]\n-    fn num_cpus() -> usize {\n-        use std::ptr;\n-\n-        let mut cpus: libc::c_uint = 0;\n-        let mut cpus_size = std::mem::size_of_val(&cpus);\n-        let mut mib = [libc::CTL_HW, libc::HW_NCPU, 0, 0];\n-\n-        unsafe {\n-            libc::sysctl(\n-                mib.as_mut_ptr(),\n-                2,\n-                &mut cpus as *mut _ as *mut _,\n-                &mut cpus_size as *mut _ as *mut _,\n-                ptr::null_mut(),\n-                0,\n-            );\n-        }\n-        if cpus < 1 {\n-            cpus = 1;\n+            let event = TestEvent::TeResult(completed_test);\n+            notify_about_test_event(event)?;\n         }\n-        cpus as usize\n-    }\n-\n-    #[cfg(target_os = \"haiku\")]\n-    fn num_cpus() -> usize {\n-        // FIXME: implement\n-        1\n-    }\n-\n-    #[cfg(target_os = \"l4re\")]\n-    fn num_cpus() -> usize {\n-        // FIXME: implement\n-        1\n     }\n+    Ok(())\n }\n \n pub fn filter_tests(opts: &TestOpts, tests: Vec<TestDescAndFn>) -> Vec<TestDescAndFn> {\n@@ -1730,7 +435,7 @@ pub fn run_test(\n     force_ignore: bool,\n     test: TestDescAndFn,\n     strategy: RunStrategy,\n-    monitor_ch: Sender<MonitorMsg>,\n+    monitor_ch: Sender<CompletedTest>,\n     concurrency: Concurrent,\n ) {\n     let TestDescAndFn { desc, testfn } = test;\n@@ -1740,20 +445,21 @@ pub fn run_test(\n         && (cfg!(target_arch = \"wasm32\") || cfg!(target_os = \"emscripten\"));\n \n     if force_ignore || desc.ignore || ignore_because_no_process_support {\n-        monitor_ch.send((desc, TrIgnored, None, Vec::new())).unwrap();\n+        let message = CompletedTest::new(desc, TrIgnored, None, Vec::new());\n+        monitor_ch.send(message).unwrap();\n         return;\n     }\n \n     struct TestRunOpts {\n         pub strategy: RunStrategy,\n         pub nocapture: bool,\n         pub concurrency: Concurrent,\n-        pub time: Option<TestTimeOptions>,\n+        pub time: Option<time::TestTimeOptions>,\n     }\n \n     fn run_test_inner(\n         desc: TestDesc,\n-        monitor_ch: Sender<MonitorMsg>,\n+        monitor_ch: Sender<CompletedTest>,\n         testfn: Box<dyn FnOnce() + Send>,\n         opts: TestRunOpts,\n     ) {\n@@ -1835,94 +541,21 @@ fn __rust_begin_short_backtrace<F: FnOnce()>(f: F) {\n     f()\n }\n \n-fn calc_result<'a>(\n-    desc: &TestDesc,\n-    task_result: Result<(), &'a (dyn Any + 'static + Send)>,\n-    time_opts: &Option<TestTimeOptions>,\n-    exec_time: &Option<TestExecTime>\n-) -> TestResult {\n-    let result = match (&desc.should_panic, task_result) {\n-        (&ShouldPanic::No, Ok(())) | (&ShouldPanic::Yes, Err(_)) => TrOk,\n-        (&ShouldPanic::YesWithMessage(msg), Err(ref err)) => {\n-            if err\n-                .downcast_ref::<String>()\n-                .map(|e| &**e)\n-                .or_else(|| err.downcast_ref::<&'static str>().map(|e| *e))\n-                .map(|e| e.contains(msg))\n-                .unwrap_or(false)\n-            {\n-                TrOk\n-            } else {\n-                if desc.allow_fail {\n-                    TrAllowedFail\n-                } else {\n-                    TrFailedMsg(format!(\"panic did not include expected string '{}'\", msg))\n-                }\n-            }\n-        }\n-        (&ShouldPanic::Yes, Ok(())) => TrFailedMsg(\"test did not panic as expected\".to_string()),\n-        _ if desc.allow_fail => TrAllowedFail,\n-        _ => TrFailed,\n-    };\n-\n-    // If test is already failed (or allowed to fail), do not change the result.\n-    if result != TrOk {\n-        return result;\n-    }\n-\n-    // Check if test is failed due to timeout.\n-    if let (Some(opts), Some(time)) = (time_opts, exec_time) {\n-        if opts.error_on_excess && opts.is_critical(desc, time) {\n-            return TrTimedFail;\n-        }\n-    }\n-\n-    result\n-}\n-\n-fn get_result_from_exit_code(\n-    desc: &TestDesc,\n-    code: i32,\n-    time_opts: &Option<TestTimeOptions>,\n-    exec_time: &Option<TestExecTime>,\n-) -> TestResult {\n-    let result = match (desc.allow_fail, code) {\n-        (_, TR_OK) => TrOk,\n-        (true, TR_FAILED) => TrAllowedFail,\n-        (false, TR_FAILED) => TrFailed,\n-        (_, _) => TrFailedMsg(format!(\"got unexpected return code {}\", code)),\n-    };\n-\n-    // If test is already failed (or allowed to fail), do not change the result.\n-    if result != TrOk {\n-        return result;\n-    }\n-\n-    // Check if test is failed due to timeout.\n-    if let (Some(opts), Some(time)) = (time_opts, exec_time) {\n-        if opts.error_on_excess && opts.is_critical(desc, time) {\n-            return TrTimedFail;\n-        }\n-    }\n-\n-    result\n-}\n-\n fn run_test_in_process(\n     desc: TestDesc,\n     nocapture: bool,\n     report_time: bool,\n     testfn: Box<dyn FnOnce() + Send>,\n-    monitor_ch: Sender<MonitorMsg>,\n-    time_opts: Option<TestTimeOptions>,\n+    monitor_ch: Sender<CompletedTest>,\n+    time_opts: Option<time::TestTimeOptions>,\n ) {\n     // Buffer for capturing standard I/O\n     let data = Arc::new(Mutex::new(Vec::new()));\n \n     let oldio = if !nocapture {\n         Some((\n-            io::set_print(Some(Box::new(Sink(data.clone())))),\n-            io::set_panic(Some(Box::new(Sink(data.clone())))),\n+            io::set_print(Some(Sink::new_boxed(&data))),\n+            io::set_panic(Some(Sink::new_boxed(&data))),\n         ))\n     } else {\n         None\n@@ -1949,14 +582,15 @@ fn run_test_in_process(\n         Err(e) => calc_result(&desc, Err(e.as_ref()), &time_opts, &exec_time),\n     };\n     let stdout = data.lock().unwrap().to_vec();\n-    monitor_ch.send((desc.clone(), test_result, exec_time, stdout)).unwrap();\n+    let message = CompletedTest::new(desc.clone(), test_result, exec_time, stdout);\n+    monitor_ch.send(message).unwrap();\n }\n \n fn spawn_test_subprocess(\n     desc: TestDesc,\n     report_time: bool,\n-    monitor_ch: Sender<MonitorMsg>,\n-    time_opts: Option<TestTimeOptions>,\n+    monitor_ch: Sender<CompletedTest>,\n+    time_opts: Option<time::TestTimeOptions>,\n ) {\n     let (result, test_output, exec_time) = (|| {\n         let args = env::args().collect::<Vec<_>>();\n@@ -2000,7 +634,8 @@ fn spawn_test_subprocess(\n         (result, test_output, exec_time)\n     })();\n \n-    monitor_ch.send((desc.clone(), result, exec_time, test_output)).unwrap();\n+    let message = CompletedTest::new(desc.clone(), result, exec_time, test_output);\n+    monitor_ch.send(message).unwrap();\n }\n \n fn run_test_in_spawned_subprocess(\n@@ -2025,9 +660,9 @@ fn run_test_in_spawned_subprocess(\n         }\n \n         if let TrOk = test_result {\n-            process::exit(TR_OK);\n+            process::exit(test_result::TR_OK);\n         } else {\n-            process::exit(TR_FAILED);\n+            process::exit(test_result::TR_FAILED);\n         }\n     });\n     let record_result2 = record_result.clone();\n@@ -2036,246 +671,3 @@ fn run_test_in_spawned_subprocess(\n     record_result(None);\n     unreachable!(\"panic=abort callback should have exited the process\")\n }\n-\n-#[cfg(not(unix))]\n-fn get_exit_code(status: ExitStatus) -> Result<i32, String> {\n-    status.code().ok_or(\"received no exit code from child process\".into())\n-}\n-\n-#[cfg(unix)]\n-fn get_exit_code(status: ExitStatus) -> Result<i32, String> {\n-    use std::os::unix::process::ExitStatusExt;\n-    match status.code() {\n-        Some(code) => Ok(code),\n-        None => match status.signal() {\n-            Some(signal) => Err(format!(\"child process exited with signal {}\", signal)),\n-            None => Err(\"child process exited with unknown signal\".into()),\n-        }\n-    }\n-}\n-\n-#[derive(Clone, PartialEq)]\n-pub struct MetricMap(BTreeMap<String, Metric>);\n-\n-impl MetricMap {\n-    pub fn new() -> MetricMap {\n-        MetricMap(BTreeMap::new())\n-    }\n-\n-    /// Insert a named `value` (+/- `noise`) metric into the map. The value\n-    /// must be non-negative. The `noise` indicates the uncertainty of the\n-    /// metric, which doubles as the \"noise range\" of acceptable\n-    /// pairwise-regressions on this named value, when comparing from one\n-    /// metric to the next using `compare_to_old`.\n-    ///\n-    /// If `noise` is positive, then it means this metric is of a value\n-    /// you want to see grow smaller, so a change larger than `noise` in the\n-    /// positive direction represents a regression.\n-    ///\n-    /// If `noise` is negative, then it means this metric is of a value\n-    /// you want to see grow larger, so a change larger than `noise` in the\n-    /// negative direction represents a regression.\n-    pub fn insert_metric(&mut self, name: &str, value: f64, noise: f64) {\n-        let m = Metric { value, noise };\n-        self.0.insert(name.to_owned(), m);\n-    }\n-\n-    pub fn fmt_metrics(&self) -> String {\n-        let v = self\n-            .0\n-            .iter()\n-            .map(|(k, v)| format!(\"{}: {} (+/- {})\", *k, v.value, v.noise))\n-            .collect::<Vec<_>>();\n-        v.join(\", \")\n-    }\n-}\n-\n-// Benchmarking\n-\n-pub use std::hint::black_box;\n-\n-impl Bencher {\n-    /// Callback for benchmark functions to run in their body.\n-    pub fn iter<T, F>(&mut self, mut inner: F)\n-    where\n-        F: FnMut() -> T,\n-    {\n-        if self.mode == BenchMode::Single {\n-            ns_iter_inner(&mut inner, 1);\n-            return;\n-        }\n-\n-        self.summary = Some(iter(&mut inner));\n-    }\n-\n-    pub fn bench<F>(&mut self, mut f: F) -> Option<stats::Summary>\n-    where\n-        F: FnMut(&mut Bencher),\n-    {\n-        f(self);\n-        return self.summary;\n-    }\n-}\n-\n-fn ns_from_dur(dur: Duration) -> u64 {\n-    dur.as_secs() * 1_000_000_000 + (dur.subsec_nanos() as u64)\n-}\n-\n-fn ns_iter_inner<T, F>(inner: &mut F, k: u64) -> u64\n-where\n-    F: FnMut() -> T,\n-{\n-    let start = Instant::now();\n-    for _ in 0..k {\n-        black_box(inner());\n-    }\n-    return ns_from_dur(start.elapsed());\n-}\n-\n-pub fn iter<T, F>(inner: &mut F) -> stats::Summary\n-where\n-    F: FnMut() -> T,\n-{\n-    // Initial bench run to get ballpark figure.\n-    let ns_single = ns_iter_inner(inner, 1);\n-\n-    // Try to estimate iter count for 1ms falling back to 1m\n-    // iterations if first run took < 1ns.\n-    let ns_target_total = 1_000_000; // 1ms\n-    let mut n = ns_target_total / cmp::max(1, ns_single);\n-\n-    // if the first run took more than 1ms we don't want to just\n-    // be left doing 0 iterations on every loop. The unfortunate\n-    // side effect of not being able to do as many runs is\n-    // automatically handled by the statistical analysis below\n-    // (i.e., larger error bars).\n-    n = cmp::max(1, n);\n-\n-    let mut total_run = Duration::new(0, 0);\n-    let samples: &mut [f64] = &mut [0.0_f64; 50];\n-    loop {\n-        let loop_start = Instant::now();\n-\n-        for p in &mut *samples {\n-            *p = ns_iter_inner(inner, n) as f64 / n as f64;\n-        }\n-\n-        stats::winsorize(samples, 5.0);\n-        let summ = stats::Summary::new(samples);\n-\n-        for p in &mut *samples {\n-            let ns = ns_iter_inner(inner, 5 * n);\n-            *p = ns as f64 / (5 * n) as f64;\n-        }\n-\n-        stats::winsorize(samples, 5.0);\n-        let summ5 = stats::Summary::new(samples);\n-\n-        let loop_run = loop_start.elapsed();\n-\n-        // If we've run for 100ms and seem to have converged to a\n-        // stable median.\n-        if loop_run > Duration::from_millis(100)\n-            && summ.median_abs_dev_pct < 1.0\n-            && summ.median - summ5.median < summ5.median_abs_dev\n-        {\n-            return summ5;\n-        }\n-\n-        total_run = total_run + loop_run;\n-        // Longest we ever run for is 3s.\n-        if total_run > Duration::from_secs(3) {\n-            return summ5;\n-        }\n-\n-        // If we overflow here just return the results so far. We check a\n-        // multiplier of 10 because we're about to multiply by 2 and the\n-        // next iteration of the loop will also multiply by 5 (to calculate\n-        // the summ5 result)\n-        n = match n.checked_mul(10) {\n-            Some(_) => n * 2,\n-            None => {\n-                return summ5;\n-            }\n-        };\n-    }\n-}\n-\n-pub mod bench {\n-    use super::{\n-        BenchMode, BenchSamples, Bencher, MonitorMsg, Sender, Sink, TestDesc, TestResult\n-    };\n-    use crate::stats;\n-    use std::cmp;\n-    use std::io;\n-    use std::panic::{catch_unwind, AssertUnwindSafe};\n-    use std::sync::{Arc, Mutex};\n-\n-    pub fn benchmark<F>(desc: TestDesc, monitor_ch: Sender<MonitorMsg>, nocapture: bool, f: F)\n-    where\n-        F: FnMut(&mut Bencher),\n-    {\n-        let mut bs = Bencher {\n-            mode: BenchMode::Auto,\n-            summary: None,\n-            bytes: 0,\n-        };\n-\n-        let data = Arc::new(Mutex::new(Vec::new()));\n-        let oldio = if !nocapture {\n-            Some((\n-                io::set_print(Some(Box::new(Sink(data.clone())))),\n-                io::set_panic(Some(Box::new(Sink(data.clone())))),\n-            ))\n-        } else {\n-            None\n-        };\n-\n-        let result = catch_unwind(AssertUnwindSafe(|| bs.bench(f)));\n-\n-        if let Some((printio, panicio)) = oldio {\n-            io::set_print(printio);\n-            io::set_panic(panicio);\n-        }\n-\n-        let test_result = match result {\n-            //bs.bench(f) {\n-            Ok(Some(ns_iter_summ)) => {\n-                let ns_iter = cmp::max(ns_iter_summ.median as u64, 1);\n-                let mb_s = bs.bytes * 1000 / ns_iter;\n-\n-                let bs = BenchSamples {\n-                    ns_iter_summ,\n-                    mb_s: mb_s as usize,\n-                };\n-                TestResult::TrBench(bs)\n-            }\n-            Ok(None) => {\n-                // iter not called, so no data.\n-                // FIXME: error in this case?\n-                let samples: &mut [f64] = &mut [0.0_f64; 1];\n-                let bs = BenchSamples {\n-                    ns_iter_summ: stats::Summary::new(samples),\n-                    mb_s: 0,\n-                };\n-                TestResult::TrBench(bs)\n-            }\n-            Err(_) => TestResult::TrFailed,\n-        };\n-\n-        let stdout = data.lock().unwrap().to_vec();\n-        monitor_ch.send((desc, test_result, None, stdout)).unwrap();\n-    }\n-\n-    pub fn run_once<F>(f: F)\n-    where\n-        F: FnMut(&mut Bencher),\n-    {\n-        let mut bs = Bencher {\n-            mode: BenchMode::Single,\n-            summary: None,\n-            bytes: 0,\n-        };\n-        bs.bench(f);\n-    }\n-}"}, {"sha": "ec87b0fcd463b58ef2a4dccb2cddc8a8043d2730", "filename": "src/libtest/options.rs", "status": "added", "additions": 90, "deletions": 0, "changes": 90, "blob_url": "https://github.com/rust-lang/rust/blob/57bfb8096295150c06559da10adc5629e445a4ac/src%2Flibtest%2Foptions.rs", "raw_url": "https://github.com/rust-lang/rust/raw/57bfb8096295150c06559da10adc5629e445a4ac/src%2Flibtest%2Foptions.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibtest%2Foptions.rs?ref=57bfb8096295150c06559da10adc5629e445a4ac", "patch": "@@ -0,0 +1,90 @@\n+//! Enums denoting options for test execution.\n+\n+/// Whether to execute tests concurrently or not\n+#[derive(Copy, Clone, Debug, PartialEq, Eq)]\n+pub enum Concurrent {\n+    Yes,\n+    No,\n+}\n+\n+/// Number of times to run a benchmarked function\n+#[derive(Clone, PartialEq, Eq)]\n+pub enum BenchMode {\n+    Auto,\n+    Single,\n+}\n+\n+/// Whether test is expected to panic or not\n+#[derive(Copy, Clone, Debug, PartialEq, Eq, Hash)]\n+pub enum ShouldPanic {\n+    No,\n+    Yes,\n+    YesWithMessage(&'static str),\n+}\n+\n+/// Whether should console output be colored or not\n+#[derive(Copy, Clone, Debug)]\n+pub enum ColorConfig {\n+    AutoColor,\n+    AlwaysColor,\n+    NeverColor,\n+}\n+\n+/// Format of the test results output\n+#[derive(Copy, Clone, Debug, PartialEq, Eq)]\n+pub enum OutputFormat {\n+    /// Verbose output\n+    Pretty,\n+    /// Quiet output\n+    Terse,\n+    /// JSON output\n+    Json,\n+}\n+\n+/// Whether ignored test should be runned or not\n+#[derive(Copy, Clone, Debug, PartialEq, Eq)]\n+pub enum RunIgnored {\n+    Yes,\n+    No,\n+    /// Run only ignored tests\n+    Only,\n+}\n+\n+#[derive(Clone, Copy)]\n+pub enum RunStrategy {\n+    /// Runs the test in the current process, and sends the result back over the\n+    /// supplied channel.\n+    InProcess,\n+\n+    /// Spawns a subprocess to run the test, and sends the result back over the\n+    /// supplied channel. Requires `argv[0]` to exist and point to the binary\n+    /// that's currently running.\n+    SpawnPrimary,\n+}\n+\n+/// Options for the test run defined by the caller (instead of CLI arguments).\n+/// In case we want to add other options as well, just add them in this struct.\n+#[derive(Copy, Clone, Debug)]\n+pub struct Options {\n+    pub display_output: bool,\n+    pub panic_abort: bool,\n+}\n+\n+impl Options {\n+    pub fn new() -> Options {\n+        Options {\n+            display_output: false,\n+            panic_abort: false,\n+        }\n+    }\n+\n+    pub fn display_output(mut self, display_output: bool) -> Options {\n+        self.display_output = display_output;\n+        self\n+    }\n+\n+    pub fn panic_abort(mut self, panic_abort: bool) -> Options {\n+        self.panic_abort = panic_abort;\n+        self\n+    }\n+}"}, {"sha": "eaf41bc9e22559f1cf867c9614ff2903988487aa", "filename": "src/libtest/stats/tests.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/57bfb8096295150c06559da10adc5629e445a4ac/src%2Flibtest%2Fstats%2Ftests.rs", "raw_url": "https://github.com/rust-lang/rust/raw/57bfb8096295150c06559da10adc5629e445a4ac/src%2Flibtest%2Fstats%2Ftests.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibtest%2Fstats%2Ftests.rs?ref=57bfb8096295150c06559da10adc5629e445a4ac", "patch": "@@ -4,7 +4,7 @@ extern crate test;\n use std::f64;\n use std::io::prelude::*;\n use std::io;\n-use self::test::Bencher;\n+use self::test::test::Bencher;\n \n // Test vectors generated from R, using the script src/etc/stat-test-vectors.r.\n "}, {"sha": "80ca9dea18f5aff3ab26b4d57964850ddd7315bc", "filename": "src/libtest/test_result.rs", "status": "added", "additions": 107, "deletions": 0, "changes": 107, "blob_url": "https://github.com/rust-lang/rust/blob/57bfb8096295150c06559da10adc5629e445a4ac/src%2Flibtest%2Ftest_result.rs", "raw_url": "https://github.com/rust-lang/rust/raw/57bfb8096295150c06559da10adc5629e445a4ac/src%2Flibtest%2Ftest_result.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibtest%2Ftest_result.rs?ref=57bfb8096295150c06559da10adc5629e445a4ac", "patch": "@@ -0,0 +1,107 @@\n+use std::any::Any;\n+\n+use super::bench::BenchSamples;\n+use super::time;\n+use super::types::TestDesc;\n+use super::options::ShouldPanic;\n+\n+pub use self::TestResult::*;\n+\n+// Return codes for secondary process.\n+// Start somewhere other than 0 so we know the return code means what we think\n+// it means.\n+pub const TR_OK: i32 = 50;\n+pub const TR_FAILED: i32 = 51;\n+\n+#[derive(Debug, Clone, PartialEq)]\n+pub enum TestResult {\n+    TrOk,\n+    TrFailed,\n+    TrFailedMsg(String),\n+    TrIgnored,\n+    TrAllowedFail,\n+    TrBench(BenchSamples),\n+    TrTimedFail,\n+}\n+\n+unsafe impl Send for TestResult {}\n+\n+/// Creates a `TestResult` depending on the raw result of test execution\n+/// and assotiated data.\n+pub fn calc_result<'a>(\n+    desc: &TestDesc,\n+    task_result: Result<(), &'a (dyn Any + 'static + Send)>,\n+    time_opts: &Option<time::TestTimeOptions>,\n+    exec_time: &Option<time::TestExecTime>\n+) -> TestResult {\n+    let result = match (&desc.should_panic, task_result) {\n+        (&ShouldPanic::No, Ok(())) | (&ShouldPanic::Yes, Err(_)) => TestResult::TrOk,\n+        (&ShouldPanic::YesWithMessage(msg), Err(ref err)) => {\n+            if err\n+                .downcast_ref::<String>()\n+                .map(|e| &**e)\n+                .or_else(|| err.downcast_ref::<&'static str>().map(|e| *e))\n+                .map(|e| e.contains(msg))\n+                .unwrap_or(false)\n+            {\n+                TestResult::TrOk\n+            } else {\n+                if desc.allow_fail {\n+                    TestResult::TrAllowedFail\n+                } else {\n+                    TestResult::TrFailedMsg(\n+                        format!(\"panic did not include expected string '{}'\", msg)\n+                    )\n+                }\n+            }\n+        }\n+        (&ShouldPanic::Yes, Ok(())) => {\n+            TestResult::TrFailedMsg(\"test did not panic as expected\".to_string())\n+        }\n+        _ if desc.allow_fail => TestResult::TrAllowedFail,\n+        _ => TestResult::TrFailed,\n+    };\n+\n+    // If test is already failed (or allowed to fail), do not change the result.\n+    if result != TestResult::TrOk {\n+        return result;\n+    }\n+\n+    // Check if test is failed due to timeout.\n+    if let (Some(opts), Some(time)) = (time_opts, exec_time) {\n+        if opts.error_on_excess && opts.is_critical(desc, time) {\n+            return TestResult::TrTimedFail;\n+        }\n+    }\n+\n+    result\n+}\n+\n+/// Creates a `TestResult` depending on the exit code of test subprocess.\n+pub fn get_result_from_exit_code(\n+    desc: &TestDesc,\n+    code: i32,\n+    time_opts: &Option<time::TestTimeOptions>,\n+    exec_time: &Option<time::TestExecTime>,\n+) -> TestResult {\n+    let result = match (desc.allow_fail, code) {\n+        (_, TR_OK) => TestResult::TrOk,\n+        (true, TR_FAILED) => TestResult::TrAllowedFail,\n+        (false, TR_FAILED) => TestResult::TrFailed,\n+        (_, _) => TestResult::TrFailedMsg(format!(\"got unexpected return code {}\", code)),\n+    };\n+\n+    // If test is already failed (or allowed to fail), do not change the result.\n+    if result != TestResult::TrOk {\n+        return result;\n+    }\n+\n+    // Check if test is failed due to timeout.\n+    if let (Some(opts), Some(time)) = (time_opts, exec_time) {\n+        if opts.error_on_excess && opts.is_critical(desc, time) {\n+            return TestResult::TrTimedFail;\n+        }\n+    }\n+\n+    result\n+}"}, {"sha": "9de774555e9ccab037f333b110bb76b11c0b5ecf", "filename": "src/libtest/tests.rs", "status": "modified", "additions": 36, "deletions": 28, "changes": 64, "blob_url": "https://github.com/rust-lang/rust/blob/57bfb8096295150c06559da10adc5629e445a4ac/src%2Flibtest%2Ftests.rs", "raw_url": "https://github.com/rust-lang/rust/raw/57bfb8096295150c06559da10adc5629e445a4ac/src%2Flibtest%2Ftests.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibtest%2Ftests.rs?ref=57bfb8096295150c06559da10adc5629e445a4ac", "patch": "@@ -1,11 +1,19 @@\n use super::*;\n \n-use crate::test::{\n-    filter_tests, parse_opts, run_test, DynTestFn, DynTestName, MetricMap, RunIgnored, RunStrategy,\n-    // ShouldPanic, StaticTestName, TestDesc, TestDescAndFn, TestOpts, TestTimeOptions,\n-    // TestType, TrFailedMsg, TrIgnored, TrOk,\n-    ShouldPanic, StaticTestName, TestDesc, TestDescAndFn, TestOpts,\n-    TrIgnored, TrOk,\n+use crate::{\n+    bench::Bencher,\n+    console::OutputLocation,\n+    options::OutputFormat,\n+    time::{TimeThreshold, TestTimeOptions},\n+    formatters::PrettyFormatter,\n+    test::{\n+        filter_tests, parse_opts, run_test, DynTestFn, DynTestName, MetricMap,\n+        RunIgnored, RunStrategy, ShouldPanic, StaticTestName, TestDesc,\n+        TestDescAndFn, TestOpts, TrIgnored, TrOk,\n+        // FIXME (introduced by #65251)\n+        // ShouldPanic, StaticTestName, TestDesc, TestDescAndFn, TestOpts, TestTimeOptions,\n+        // TestType, TrFailedMsg, TrIgnored, TrOk,\n+    },\n };\n use std::sync::mpsc::channel;\n use std::time::Duration;\n@@ -74,8 +82,8 @@ pub fn do_not_run_ignored_tests() {\n     };\n     let (tx, rx) = channel();\n     run_test(&TestOpts::new(), false, desc, RunStrategy::InProcess, tx, Concurrent::No);\n-    let (_, res, _, _) = rx.recv().unwrap();\n-    assert!(res != TrOk);\n+    let result = rx.recv().unwrap().result;\n+    assert!(result != TrOk);\n }\n \n #[test]\n@@ -93,11 +101,11 @@ pub fn ignored_tests_result_in_ignored() {\n     };\n     let (tx, rx) = channel();\n     run_test(&TestOpts::new(), false, desc, RunStrategy::InProcess, tx, Concurrent::No);\n-    let (_, res, _, _) = rx.recv().unwrap();\n-    assert!(res == TrIgnored);\n+    let result = rx.recv().unwrap().result;\n+    assert!(result == TrIgnored);\n }\n \n-// FIXME: Re-enable emscripten once it can catch panics again\n+// FIXME: Re-enable emscripten once it can catch panics again (introduced by #65251)\n #[test]\n #[cfg(not(target_os = \"emscripten\"))]\n fn test_should_panic() {\n@@ -116,11 +124,11 @@ fn test_should_panic() {\n     };\n     let (tx, rx) = channel();\n     run_test(&TestOpts::new(), false, desc, RunStrategy::InProcess, tx, Concurrent::No);\n-    let (_, res, _, _) = rx.recv().unwrap();\n-    assert!(res == TrOk);\n+    let result = rx.recv().unwrap().result;\n+    assert!(result == TrOk);\n }\n \n-// FIXME: Re-enable emscripten once it can catch panics again\n+// FIXME: Re-enable emscripten once it can catch panics again (introduced by #65251)\n #[test]\n #[cfg(not(target_os = \"emscripten\"))]\n fn test_should_panic_good_message() {\n@@ -139,11 +147,11 @@ fn test_should_panic_good_message() {\n     };\n     let (tx, rx) = channel();\n     run_test(&TestOpts::new(), false, desc, RunStrategy::InProcess, tx, Concurrent::No);\n-    let (_, res, _, _) = rx.recv().unwrap();\n-    assert!(res == TrOk);\n+    let result = rx.recv().unwrap().result;\n+    assert!(result == TrOk);\n }\n \n-// FIXME: Re-enable emscripten once it can catch panics again\n+// FIXME: Re-enable emscripten once it can catch panics again (introduced by #65251)\n #[test]\n #[cfg(not(target_os = \"emscripten\"))]\n fn test_should_panic_bad_message() {\n@@ -165,11 +173,11 @@ fn test_should_panic_bad_message() {\n     };\n     let (tx, rx) = channel();\n     run_test(&TestOpts::new(), false, desc, RunStrategy::InProcess, tx, Concurrent::No);\n-    let (_, res, _, _) = rx.recv().unwrap();\n-    assert!(res == TrFailedMsg(format!(\"{} '{}'\", failed_msg, expected)));\n+    let result = rx.recv().unwrap().result;\n+    assert!(result == TrFailedMsg(format!(\"{} '{}'\", failed_msg, expected)));\n }\n \n-// FIXME: Re-enable emscripten once it can catch panics again\n+// FIXME: Re-enable emscripten once it can catch panics again (introduced by #65251)\n #[test]\n #[cfg(not(target_os = \"emscripten\"))]\n fn test_should_panic_but_succeeds() {\n@@ -186,8 +194,8 @@ fn test_should_panic_but_succeeds() {\n     };\n     let (tx, rx) = channel();\n     run_test(&TestOpts::new(), false, desc, RunStrategy::InProcess, tx, Concurrent::No);\n-    let (_, res, _, _) = rx.recv().unwrap();\n-    assert!(res == TrFailedMsg(\"test did not panic as expected\".to_string()));\n+    let result = rx.recv().unwrap().result;\n+    assert!(result == TrFailedMsg(\"test did not panic as expected\".to_string()));\n }\n \n fn report_time_test_template(report_time: bool) -> Option<TestExecTime> {\n@@ -214,7 +222,7 @@ fn report_time_test_template(report_time: bool) -> Option<TestExecTime> {\n     };\n     let (tx, rx) = channel();\n     run_test(&test_opts, false, desc, RunStrategy::InProcess, tx, Concurrent::No);\n-    let (_, _, exec_time, _) = rx.recv().unwrap();\n+    let exec_time = rx.recv().unwrap().exec_time;\n     exec_time\n }\n \n@@ -252,7 +260,7 @@ fn time_test_failure_template(test_type: TestType) -> TestResult {\n     };\n     let (tx, rx) = channel();\n     run_test(&test_opts, false, desc, RunStrategy::InProcess, tx, Concurrent::No);\n-    let (_, result, _, _) = rx.recv().unwrap();\n+    let result = rx.recv().unwrap().result;\n \n     result\n }\n@@ -658,9 +666,9 @@ fn should_sort_failures_before_printing_them() {\n         test_type: TestType::Unknown,\n     };\n \n-    let mut out = PrettyFormatter::new(Raw(Vec::new()), false, 10, false, None);\n+    let mut out = PrettyFormatter::new(OutputLocation::Raw(Vec::new()), false, 10, false, None);\n \n-    let st = ConsoleTestState {\n+    let st = console::ConsoleTestState {\n         log_out: None,\n         total: 0,\n         passed: 0,\n@@ -678,8 +686,8 @@ fn should_sort_failures_before_printing_them() {\n \n     out.write_failures(&st).unwrap();\n     let s = match out.output_location() {\n-        &Raw(ref m) => String::from_utf8_lossy(&m[..]),\n-        &Pretty(_) => unreachable!(),\n+        &OutputLocation::Raw(ref m) => String::from_utf8_lossy(&m[..]),\n+        &OutputLocation::Pretty(_) => unreachable!(),\n     };\n \n     let apos = s.find(\"a\").unwrap();"}, {"sha": "f4d4b17b620ba7c467f8171a26d60721291943ac", "filename": "src/libtest/time.rs", "status": "added", "additions": 206, "deletions": 0, "changes": 206, "blob_url": "https://github.com/rust-lang/rust/blob/57bfb8096295150c06559da10adc5629e445a4ac/src%2Flibtest%2Ftime.rs", "raw_url": "https://github.com/rust-lang/rust/raw/57bfb8096295150c06559da10adc5629e445a4ac/src%2Flibtest%2Ftime.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibtest%2Ftime.rs?ref=57bfb8096295150c06559da10adc5629e445a4ac", "patch": "@@ -0,0 +1,206 @@\n+//! Module `time` contains everything related to the time measurement of unit tests\n+//! execution.\n+//! Two main purposes of this module:\n+//! - Check whether test is timed out.\n+//! - Provide helpers for `report-time` and `measure-time` options.\n+\n+use std::time::{Duration, Instant};\n+use std::str::FromStr;\n+use std::fmt;\n+use std::env;\n+\n+use super::types::{TestDesc, TestType};\n+\n+pub const TEST_WARN_TIMEOUT_S: u64 = 60;\n+\n+/// This small module contains constants used by `report-time` option.\n+/// Those constants values will be used if corresponding environment variables are not set.\n+///\n+/// To override values for unit-tests, use a constant `RUST_TEST_TIME_UNIT`,\n+/// To override values for integration tests, use a constant `RUST_TEST_TIME_INTEGRATION`,\n+/// To override values for doctests, use a constant `RUST_TEST_TIME_DOCTEST`.\n+///\n+/// Example of the expected format is `RUST_TEST_TIME_xxx=100,200`, where 100 means\n+/// warn time, and 200 means critical time.\n+pub mod time_constants {\n+    use std::time::Duration;\n+    use super::TEST_WARN_TIMEOUT_S;\n+\n+    /// Environment variable for overriding default threshold for unit-tests.\n+    pub const UNIT_ENV_NAME: &str = \"RUST_TEST_TIME_UNIT\";\n+\n+    // Unit tests are supposed to be really quick.\n+    pub const UNIT_WARN: Duration = Duration::from_millis(50);\n+    pub const UNIT_CRITICAL: Duration = Duration::from_millis(100);\n+\n+    /// Environment variable for overriding default threshold for unit-tests.\n+    pub const INTEGRATION_ENV_NAME: &str = \"RUST_TEST_TIME_INTEGRATION\";\n+\n+    // Integration tests may have a lot of work, so they can take longer to execute.\n+    pub const INTEGRATION_WARN: Duration = Duration::from_millis(500);\n+    pub const INTEGRATION_CRITICAL: Duration = Duration::from_millis(1000);\n+\n+    /// Environment variable for overriding default threshold for unit-tests.\n+    pub const DOCTEST_ENV_NAME: &str = \"RUST_TEST_TIME_DOCTEST\";\n+\n+    // Doctests are similar to integration tests, because they can include a lot of\n+    // initialization code.\n+    pub const DOCTEST_WARN: Duration = INTEGRATION_WARN;\n+    pub const DOCTEST_CRITICAL: Duration = INTEGRATION_CRITICAL;\n+\n+    // Do not suppose anything about unknown tests, base limits on the\n+    // `TEST_WARN_TIMEOUT_S` constant.\n+    pub const UNKNOWN_WARN: Duration = Duration::from_secs(TEST_WARN_TIMEOUT_S);\n+    pub const UNKNOWN_CRITICAL: Duration = Duration::from_secs(TEST_WARN_TIMEOUT_S * 2);\n+}\n+\n+/// Returns an `Instance` object denoting when the test should be considered\n+/// timed out.\n+pub fn get_default_test_timeout() -> Instant {\n+    Instant::now() + Duration::from_secs(TEST_WARN_TIMEOUT_S)\n+}\n+\n+/// The meassured execution time of a unit test.\n+#[derive(Debug, Clone, PartialEq)]\n+pub struct TestExecTime(pub Duration);\n+\n+impl fmt::Display for TestExecTime {\n+    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {\n+        write!(f, \"{:.3}s\", self.0.as_secs_f64())\n+    }\n+}\n+\n+/// Structure denoting time limits for test execution.\n+#[derive(Copy, Clone, Debug, Default, PartialEq, Eq)]\n+pub struct TimeThreshold {\n+    pub warn: Duration,\n+    pub critical: Duration,\n+}\n+\n+impl TimeThreshold {\n+    /// Creates a new `TimeThreshold` instance with provided durations.\n+    pub fn new(warn: Duration, critical: Duration) -> Self {\n+        Self {\n+            warn,\n+            critical,\n+        }\n+    }\n+\n+    /// Attempts to create a `TimeThreshold` instance with values obtained\n+    /// from the environment variable, and returns `None` if the variable\n+    /// is not set.\n+    /// Environment variable format is expected to match `\\d+,\\d+`.\n+    ///\n+    /// # Panics\n+    ///\n+    /// Panics if variable with provided name is set but contains inappropriate\n+    /// value.\n+    pub fn from_env_var(env_var_name: &str) -> Option<Self> {\n+        let durations_str = env::var(env_var_name).ok()?;\n+\n+        // Split string into 2 substrings by comma and try to parse numbers.\n+        let mut durations = durations_str\n+            .splitn(2, ',')\n+            .map(|v| {\n+                u64::from_str(v).unwrap_or_else(|_| {\n+                    panic!(\n+                        \"Duration value in variable {} is expected to be a number, but got {}\",\n+                        env_var_name, v\n+                    )\n+                })\n+            });\n+\n+        // Callback to be called if the environment variable has unexpected structure.\n+        let panic_on_incorrect_value = || {\n+            panic!(\n+                \"Duration variable {} expected to have 2 numbers separated by comma, but got {}\",\n+                env_var_name, durations_str\n+            );\n+        };\n+\n+        let (warn, critical) = (\n+            durations.next().unwrap_or_else(panic_on_incorrect_value),\n+            durations.next().unwrap_or_else(panic_on_incorrect_value)\n+        );\n+\n+        if warn > critical {\n+            panic!(\"Test execution warn time should be less or equal to the critical time\");\n+        }\n+\n+        Some(Self::new(Duration::from_millis(warn), Duration::from_millis(critical)))\n+    }\n+}\n+\n+/// Structure with parameters for calculating test execution time.\n+#[derive(Copy, Clone, Debug, Default, PartialEq, Eq)]\n+pub struct TestTimeOptions {\n+    /// Denotes if the test critical execution time limit excess should be considered\n+    /// a test failure.\n+    pub error_on_excess: bool,\n+    pub colored: bool,\n+    pub unit_threshold: TimeThreshold,\n+    pub integration_threshold: TimeThreshold,\n+    pub doctest_threshold: TimeThreshold,\n+}\n+\n+impl TestTimeOptions {\n+    pub fn new_from_env(error_on_excess: bool, colored: bool) -> Self {\n+        let unit_threshold =\n+            TimeThreshold::from_env_var(time_constants::UNIT_ENV_NAME)\n+                .unwrap_or_else(Self::default_unit);\n+\n+        let integration_threshold =\n+            TimeThreshold::from_env_var(time_constants::INTEGRATION_ENV_NAME)\n+                .unwrap_or_else(Self::default_integration);\n+\n+        let doctest_threshold =\n+            TimeThreshold::from_env_var(time_constants::DOCTEST_ENV_NAME)\n+                .unwrap_or_else(Self::default_doctest);\n+\n+        Self {\n+            error_on_excess,\n+            colored,\n+            unit_threshold,\n+            integration_threshold,\n+            doctest_threshold,\n+        }\n+    }\n+\n+    pub fn is_warn(&self, test: &TestDesc, exec_time: &TestExecTime) -> bool {\n+        exec_time.0 >= self.warn_time(test)\n+    }\n+\n+    pub fn is_critical(&self, test: &TestDesc, exec_time: &TestExecTime) -> bool {\n+        exec_time.0 >= self.critical_time(test)\n+    }\n+\n+    fn warn_time(&self, test: &TestDesc) -> Duration {\n+        match test.test_type {\n+            TestType::UnitTest => self.unit_threshold.warn,\n+            TestType::IntegrationTest => self.integration_threshold.warn,\n+            TestType::DocTest => self.doctest_threshold.warn,\n+            TestType::Unknown => time_constants::UNKNOWN_WARN,\n+        }\n+    }\n+\n+    fn critical_time(&self, test: &TestDesc) -> Duration {\n+        match test.test_type {\n+            TestType::UnitTest => self.unit_threshold.critical,\n+            TestType::IntegrationTest => self.integration_threshold.critical,\n+            TestType::DocTest => self.doctest_threshold.critical,\n+            TestType::Unknown => time_constants::UNKNOWN_CRITICAL,\n+        }\n+    }\n+\n+    fn default_unit() -> TimeThreshold {\n+        TimeThreshold::new(time_constants::UNIT_WARN, time_constants::UNIT_CRITICAL)\n+    }\n+\n+    fn default_integration() -> TimeThreshold {\n+        TimeThreshold::new(time_constants::INTEGRATION_WARN, time_constants::INTEGRATION_CRITICAL)\n+    }\n+\n+    fn default_doctest() -> TimeThreshold {\n+        TimeThreshold::new(time_constants::DOCTEST_WARN, time_constants::DOCTEST_CRITICAL)\n+    }\n+}"}, {"sha": "89bcf2cf2853be1ee49aaa16c28f0ba30f18b02e", "filename": "src/libtest/types.rs", "status": "added", "additions": 145, "deletions": 0, "changes": 145, "blob_url": "https://github.com/rust-lang/rust/blob/57bfb8096295150c06559da10adc5629e445a4ac/src%2Flibtest%2Ftypes.rs", "raw_url": "https://github.com/rust-lang/rust/raw/57bfb8096295150c06559da10adc5629e445a4ac/src%2Flibtest%2Ftypes.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibtest%2Ftypes.rs?ref=57bfb8096295150c06559da10adc5629e445a4ac", "patch": "@@ -0,0 +1,145 @@\n+//! Common types used by `libtest`.\n+\n+use std::fmt;\n+use std::borrow::Cow;\n+\n+use super::options;\n+use super::bench::Bencher;\n+\n+pub use NamePadding::*;\n+pub use TestName::*;\n+pub use TestFn::*;\n+\n+/// Type of the test according to the [rust book](https://doc.rust-lang.org/cargo/guide/tests.html)\n+/// conventions.\n+#[derive(Copy, Clone, Debug, PartialEq, Eq, Hash)]\n+pub enum TestType {\n+    /// Unit-tests are expected to be in the `src` folder of the crate.\n+    UnitTest,\n+    /// Integration-style tests are expected to be in the `tests` folder of the crate.\n+    IntegrationTest,\n+    /// Doctests are created by the `librustdoc` manually, so it's a different type of test.\n+    DocTest,\n+    /// Tests for the sources that don't follow the project layout convention\n+    /// (e.g. tests in raw `main.rs` compiled by calling `rustc --test` directly).\n+    Unknown,\n+}\n+\n+#[derive(Clone, Copy, PartialEq, Eq, Hash, Debug)]\n+pub enum NamePadding {\n+    PadNone,\n+    PadOnRight,\n+}\n+\n+// The name of a test. By convention this follows the rules for rust\n+// paths; i.e., it should be a series of identifiers separated by double\n+// colons. This way if some test runner wants to arrange the tests\n+// hierarchically it may.\n+#[derive(Clone, PartialEq, Eq, Hash, Debug)]\n+pub enum TestName {\n+    StaticTestName(&'static str),\n+    DynTestName(String),\n+    AlignedTestName(Cow<'static, str>, NamePadding),\n+}\n+\n+impl TestName {\n+    pub fn as_slice(&self) -> &str {\n+        match *self {\n+            StaticTestName(s) => s,\n+            DynTestName(ref s) => s,\n+            AlignedTestName(ref s, _) => &*s,\n+        }\n+    }\n+\n+    pub fn padding(&self) -> NamePadding {\n+        match self {\n+            &AlignedTestName(_, p) => p,\n+            _ => PadNone,\n+        }\n+    }\n+\n+    pub fn with_padding(&self, padding: NamePadding) -> TestName {\n+        let name = match self {\n+            &TestName::StaticTestName(name) => Cow::Borrowed(name),\n+            &TestName::DynTestName(ref name) => Cow::Owned(name.clone()),\n+            &TestName::AlignedTestName(ref name, _) => name.clone(),\n+        };\n+\n+        TestName::AlignedTestName(name, padding)\n+    }\n+}\n+impl fmt::Display for TestName {\n+    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {\n+        fmt::Display::fmt(self.as_slice(), f)\n+    }\n+}\n+\n+/// Represents a benchmark function.\n+pub trait TDynBenchFn: Send {\n+    fn run(&self, harness: &mut Bencher);\n+}\n+\n+// A function that runs a test. If the function returns successfully,\n+// the test succeeds; if the function panics then the test fails. We\n+// may need to come up with a more clever definition of test in order\n+// to support isolation of tests into threads.\n+pub enum TestFn {\n+    StaticTestFn(fn()),\n+    StaticBenchFn(fn(&mut Bencher)),\n+    DynTestFn(Box<dyn FnOnce() + Send>),\n+    DynBenchFn(Box<dyn TDynBenchFn + 'static>),\n+}\n+\n+impl TestFn {\n+    pub fn padding(&self) -> NamePadding {\n+        match *self {\n+            StaticTestFn(..) => PadNone,\n+            StaticBenchFn(..) => PadOnRight,\n+            DynTestFn(..) => PadNone,\n+            DynBenchFn(..) => PadOnRight,\n+        }\n+    }\n+}\n+\n+impl fmt::Debug for TestFn {\n+    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {\n+        f.write_str(match *self {\n+            StaticTestFn(..) => \"StaticTestFn(..)\",\n+            StaticBenchFn(..) => \"StaticBenchFn(..)\",\n+            DynTestFn(..) => \"DynTestFn(..)\",\n+            DynBenchFn(..) => \"DynBenchFn(..)\",\n+        })\n+    }\n+}\n+\n+// The definition of a single test. A test runner will run a list of\n+// these.\n+#[derive(Clone, Debug, PartialEq, Eq, Hash)]\n+pub struct TestDesc {\n+    pub name: TestName,\n+    pub ignore: bool,\n+    pub should_panic: options::ShouldPanic,\n+    pub allow_fail: bool,\n+    pub test_type: TestType,\n+}\n+\n+impl TestDesc {\n+    pub fn padded_name(&self, column_count: usize, align: NamePadding) -> String {\n+        let mut name = String::from(self.name.as_slice());\n+        let fill = column_count.saturating_sub(name.len());\n+        let pad = \" \".repeat(fill);\n+        match align {\n+            PadNone => name,\n+            PadOnRight => {\n+                name.push_str(&pad);\n+                name\n+            }\n+        }\n+    }\n+}\n+\n+#[derive(Debug)]\n+pub struct TestDescAndFn {\n+    pub desc: TestDesc,\n+    pub testfn: TestFn,\n+}"}]}