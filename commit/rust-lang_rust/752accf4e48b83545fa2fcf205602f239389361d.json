{"sha": "752accf4e48b83545fa2fcf205602f239389361d", "node_id": "MDY6Q29tbWl0NzI0NzEyOjc1MmFjY2Y0ZTQ4YjgzNTQ1ZmEyZmNmMjA1NjAyZjIzOTM4OTM2MWQ=", "commit": {"author": {"name": "Oliver Schneider", "email": "github35764891676564198441@oli-obk.de", "date": "2018-08-30T08:31:05Z"}, "committer": {"name": "GitHub", "email": "noreply@github.com", "date": "2018-08-30T08:31:05Z"}, "message": "Merge pull request #434 from solson/rustup\n\nrustup for big refactor; kill most of validation", "tree": {"sha": "075666b2b6c6b096e39da3c3c6bcd42dcc505839", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/075666b2b6c6b096e39da3c3c6bcd42dcc505839"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/752accf4e48b83545fa2fcf205602f239389361d", "comment_count": 0, "verification": {"verified": true, "reason": "valid", "signature": "-----BEGIN PGP SIGNATURE-----\n\nwsBcBAABCAAQBQJbh6tJCRBK7hj4Ov3rIwAAdHIIAERxEu39PozHzZC8VMEXa4x8\nHwiPlk7vGH9/nk+RWtMb0bs62p8l19wY3lCyp3Z1wAnTJLxBcRDgLDhdbbfRFbUg\nRJpQMlUpQZsdWDudk34IcURwIdOYHyjnwAwjYrrv7w05UhVFguDcvrq3huI4ZpBI\nnO5Idv2VjAN3oL+hoasXsTXY6r+3cYqPCd5zP8ZrsOYVU0rNKzIu9DdCN1qQfIsP\niVGs1BihScFvrtx0JGziMhRH3gQrFiD/jmDdxDXQQ+nRNWtxb0iOejiq4jbKgCO8\n5rKHgSB13JHTIwlVXT9LRFuETioKNu1YGAUnWAxX7hBfCppAgFSoZOXndqzcZxc=\n=q5E0\n-----END PGP SIGNATURE-----\n", "payload": "tree 075666b2b6c6b096e39da3c3c6bcd42dcc505839\nparent 66ca0fb0a9c28265b7630c0119e4a7c524b845af\nparent 2a244dcb4893a26b02ae2bd01309b8499685457b\nauthor Oliver Schneider <github35764891676564198441@oli-obk.de> 1535617865 +0200\ncommitter GitHub <noreply@github.com> 1535617865 +0200\n\nMerge pull request #434 from solson/rustup\n\nrustup for big refactor; kill most of validation"}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/752accf4e48b83545fa2fcf205602f239389361d", "html_url": "https://github.com/rust-lang/rust/commit/752accf4e48b83545fa2fcf205602f239389361d", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/752accf4e48b83545fa2fcf205602f239389361d/comments", "author": {"login": "oli-obk", "id": 332036, "node_id": "MDQ6VXNlcjMzMjAzNg==", "avatar_url": "https://avatars.githubusercontent.com/u/332036?v=4", "gravatar_id": "", "url": "https://api.github.com/users/oli-obk", "html_url": "https://github.com/oli-obk", "followers_url": "https://api.github.com/users/oli-obk/followers", "following_url": "https://api.github.com/users/oli-obk/following{/other_user}", "gists_url": "https://api.github.com/users/oli-obk/gists{/gist_id}", "starred_url": "https://api.github.com/users/oli-obk/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/oli-obk/subscriptions", "organizations_url": "https://api.github.com/users/oli-obk/orgs", "repos_url": "https://api.github.com/users/oli-obk/repos", "events_url": "https://api.github.com/users/oli-obk/events{/privacy}", "received_events_url": "https://api.github.com/users/oli-obk/received_events", "type": "User", "site_admin": false}, "committer": {"login": "web-flow", "id": 19864447, "node_id": "MDQ6VXNlcjE5ODY0NDQ3", "avatar_url": "https://avatars.githubusercontent.com/u/19864447?v=4", "gravatar_id": "", "url": "https://api.github.com/users/web-flow", "html_url": "https://github.com/web-flow", "followers_url": "https://api.github.com/users/web-flow/followers", "following_url": "https://api.github.com/users/web-flow/following{/other_user}", "gists_url": "https://api.github.com/users/web-flow/gists{/gist_id}", "starred_url": "https://api.github.com/users/web-flow/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/web-flow/subscriptions", "organizations_url": "https://api.github.com/users/web-flow/orgs", "repos_url": "https://api.github.com/users/web-flow/repos", "events_url": "https://api.github.com/users/web-flow/events{/privacy}", "received_events_url": "https://api.github.com/users/web-flow/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "66ca0fb0a9c28265b7630c0119e4a7c524b845af", "url": "https://api.github.com/repos/rust-lang/rust/commits/66ca0fb0a9c28265b7630c0119e4a7c524b845af", "html_url": "https://github.com/rust-lang/rust/commit/66ca0fb0a9c28265b7630c0119e4a7c524b845af"}, {"sha": "2a244dcb4893a26b02ae2bd01309b8499685457b", "url": "https://api.github.com/repos/rust-lang/rust/commits/2a244dcb4893a26b02ae2bd01309b8499685457b", "html_url": "https://github.com/rust-lang/rust/commit/2a244dcb4893a26b02ae2bd01309b8499685457b"}], "stats": {"total": 2940, "additions": 746, "deletions": 2194}, "files": [{"sha": "f54012d5e14b9393ed4500424049b2fbf94e1f6f", "filename": "rust-toolchain", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/752accf4e48b83545fa2fcf205602f239389361d/rust-toolchain", "raw_url": "https://github.com/rust-lang/rust/raw/752accf4e48b83545fa2fcf205602f239389361d/rust-toolchain", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/rust-toolchain?ref=752accf4e48b83545fa2fcf205602f239389361d", "patch": "@@ -1 +1 @@\n-nightly-2018-08-14\n+nightly-2018-08-30"}, {"sha": "a9791e2b7ca7fd7ede758c8085ffda1a71e6d94d", "filename": "src/fn_call.rs", "status": "modified", "additions": 207, "deletions": 278, "changes": 485, "blob_url": "https://github.com/rust-lang/rust/blob/752accf4e48b83545fa2fcf205602f239389361d/src%2Ffn_call.rs", "raw_url": "https://github.com/rust-lang/rust/raw/752accf4e48b83545fa2fcf205602f239389361d/src%2Ffn_call.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ffn_call.rs?ref=752accf4e48b83545fa2fcf205602f239389361d", "patch": "@@ -1,10 +1,8 @@\n-use rustc::ty::{self, Ty};\n-use rustc::ty::layout::{self, Align, LayoutOf, Size};\n+use rustc::ty;\n+use rustc::ty::layout::{Align, LayoutOf, Size};\n use rustc::hir::def_id::{DefId, CRATE_DEF_INDEX};\n use rustc::mir;\n-use rustc_data_structures::indexed_vec::Idx;\n use syntax::attr;\n-use syntax::codemap::Span;\n \n use std::mem;\n \n@@ -14,196 +12,130 @@ use tls::MemoryExt;\n \n use super::memory::MemoryKind;\n \n-fn write_discriminant_value<'a, 'mir, 'tcx: 'a + 'mir>(\n-        ecx: &mut EvalContext<'a, 'mir, 'tcx, super::Evaluator<'tcx>>,\n-        dest_ty: Ty<'tcx>,\n-        dest: Place,\n-        variant_index: usize,\n-    ) -> EvalResult<'tcx> {\n-        let layout = ecx.layout_of(dest_ty)?;\n-\n-        match layout.variants {\n-            layout::Variants::Single { index } => {\n-                if index != variant_index {\n-                    // If the layout of an enum is `Single`, all\n-                    // other variants are necessarily uninhabited.\n-                    assert_eq!(layout.for_variant(&ecx, variant_index).abi,\n-                               layout::Abi::Uninhabited);\n-                }\n-            }\n-            layout::Variants::Tagged { .. } => {\n-                let discr_val = dest_ty.ty_adt_def().unwrap()\n-                    .discriminant_for_variant(*ecx.tcx, variant_index)\n-                    .val;\n-\n-                let (discr_dest, discr) = ecx.place_field(dest, mir::Field::new(0), layout)?;\n-                ecx.write_scalar(discr_dest, Scalar::from_uint(discr_val, discr.size), discr.ty)?;\n-            }\n-            layout::Variants::NicheFilling {\n-                dataful_variant,\n-                ref niche_variants,\n-                niche_start,\n-                ..\n-            } => {\n-                if variant_index != dataful_variant {\n-                    let (niche_dest, niche) =\n-                        ecx.place_field(dest, mir::Field::new(0), layout)?;\n-                    let niche_value = ((variant_index - niche_variants.start()) as u128)\n-                        .wrapping_add(niche_start);\n-                    ecx.write_scalar(niche_dest, Scalar::from_uint(niche_value, niche.size), niche.ty)?;\n-                }\n-            }\n-        }\n-\n-        Ok(())\n-    }\n-\n-pub trait EvalContextExt<'tcx> {\n-    fn call_foreign_item(\n+pub trait EvalContextExt<'tcx, 'mir> {\n+    /// Emulate calling a foreign item, fail if the item is not supported.\n+    /// This function will handle `goto_block` if needed.\n+    fn emulate_foreign_item(\n         &mut self,\n         def_id: DefId,\n-        args: &[ValTy<'tcx>],\n-        dest: Place,\n-        dest_ty: Ty<'tcx>,\n-        dest_block: mir::BasicBlock,\n+        args: &[OpTy<'tcx>],\n+        dest: PlaceTy<'tcx>,\n+        ret: mir::BasicBlock,\n     ) -> EvalResult<'tcx>;\n \n     fn resolve_path(&self, path: &[&str]) -> EvalResult<'tcx, ty::Instance<'tcx>>;\n \n-    fn call_missing_fn(\n+    /// Emulate a function that should have MIR but does not.\n+    /// This is solely to support execution without full MIR.\n+    /// Fail if emulating this function is not supported.\n+    /// This function will handle `goto_block` if needed.\n+    fn emulate_missing_fn(\n         &mut self,\n-        instance: ty::Instance<'tcx>,\n-        destination: Option<(Place, mir::BasicBlock)>,\n-        args: &[ValTy<'tcx>],\n-        sig: ty::FnSig<'tcx>,\n         path: String,\n+        args: &[OpTy<'tcx>],\n+        dest: Option<PlaceTy<'tcx>>,\n+        ret: Option<mir::BasicBlock>,\n     ) -> EvalResult<'tcx>;\n \n-    fn eval_fn_call(\n+    fn find_fn(\n         &mut self,\n         instance: ty::Instance<'tcx>,\n-        destination: Option<(Place, mir::BasicBlock)>,\n-        args: &[ValTy<'tcx>],\n-        span: Span,\n-        sig: ty::FnSig<'tcx>,\n-    ) -> EvalResult<'tcx, bool>;\n+        args: &[OpTy<'tcx>],\n+        dest: Option<PlaceTy<'tcx>>,\n+        ret: Option<mir::BasicBlock>,\n+    ) -> EvalResult<'tcx, Option<&'mir mir::Mir<'tcx>>>;\n \n-    fn write_null(&mut self, dest: Place, dest_layout: TyLayout<'tcx>) -> EvalResult<'tcx>;\n+    fn write_null(&mut self, dest: PlaceTy<'tcx>) -> EvalResult<'tcx>;\n }\n \n-impl<'a, 'mir, 'tcx: 'mir + 'a> EvalContextExt<'tcx> for EvalContext<'a, 'mir, 'tcx, super::Evaluator<'tcx>> {\n-    fn eval_fn_call(\n+impl<'a, 'mir, 'tcx: 'mir + 'a> EvalContextExt<'tcx, 'mir> for EvalContext<'a, 'mir, 'tcx, super::Evaluator<'tcx>> {\n+    fn find_fn(\n         &mut self,\n         instance: ty::Instance<'tcx>,\n-        destination: Option<(Place, mir::BasicBlock)>,\n-        args: &[ValTy<'tcx>],\n-        span: Span,\n-        sig: ty::FnSig<'tcx>,\n-    ) -> EvalResult<'tcx, bool> {\n-        trace!(\"eval_fn_call: {:#?}, {:#?}\", instance, destination);\n-\n-        let def_id = instance.def_id();\n-        let item_path = self.tcx.absolute_item_path_str(def_id);\n-        match &*item_path {\n-            \"std::sys::unix::thread::guard::init\" | \"std::sys::unix::thread::guard::current\" => {\n-                // Return None, as it doesn't make sense to return Some, because miri detects stack overflow itself.\n-                let ret_ty = sig.output();\n-                match ret_ty.sty {\n-                    ty::TyAdt(ref adt_def, _) => {\n-                        assert!(adt_def.is_enum(), \"Unexpected return type for {}\", item_path);\n-                        let none_variant_index = adt_def.variants.iter().position(|def| {\n-                            def.name.as_str() == \"None\"\n-                        }).expect(\"No None variant\");\n-                        let (return_place, return_to_block) = destination.unwrap();\n-                        write_discriminant_value(self, ret_ty, return_place, none_variant_index)?;\n-                        self.goto_block(return_to_block);\n-                        return Ok(true);\n-                    }\n-                    _ => panic!(\"Unexpected return type for {}\", item_path)\n-                }\n-            }\n-            \"std::sys::unix::fast_thread_local::register_dtor\" => {\n-                // TODO: register the dtor\n-                let (_return_place, return_to_block) = destination.unwrap();\n-                self.goto_block(return_to_block);\n-                return Ok(true);\n-            }\n-            _ => {}\n+        args: &[OpTy<'tcx>],\n+        dest: Option<PlaceTy<'tcx>>,\n+        ret: Option<mir::BasicBlock>,\n+    ) -> EvalResult<'tcx, Option<&'mir mir::Mir<'tcx>>> {\n+        trace!(\"eval_fn_call: {:#?}, {:?}\", instance, dest.map(|place| *place));\n+\n+        // first run the common hooks also supported by CTFE\n+        if self.hook_fn(instance, args, dest)? {\n+            self.goto_block(ret)?;\n+            return Ok(None);\n         }\n-\n+        // there are some more lang items we want to hook that CTFE does not hook (yet)\n         if self.tcx.lang_items().align_offset_fn() == Some(instance.def.def_id()) {\n             // FIXME: return a real value in case the target allocation has an\n             // alignment bigger than the one requested\n             let n = u128::max_value();\n-            let amt = 128 - self.memory.pointer_size().bytes() * 8;\n-            let (dest, return_to_block) = destination.unwrap();\n-            let ty = self.tcx.types.usize;\n-            let ptr_size = self.memory.pointer_size();\n-            self.write_scalar(dest, Scalar::from_uint((n << amt) >> amt, ptr_size), ty)?;\n-            self.goto_block(return_to_block);\n-            return Ok(true);\n+            let dest = dest.unwrap();\n+            let n = self.truncate(n, dest.layout);\n+            self.write_scalar(Scalar::from_uint(n, dest.layout.size), dest)?;\n+            self.goto_block(ret)?;\n+            return Ok(None);\n+        }\n+\n+        // Try to see if we can do something about foreign items\n+        if self.tcx.is_foreign_item(instance.def_id()) {\n+            // An external function that we cannot find MIR for, but we can still run enough\n+            // of them to make miri viable.\n+            self.emulate_foreign_item(\n+                instance.def_id(),\n+                args,\n+                dest.unwrap(),\n+                ret.unwrap(),\n+            )?;\n+            // `goto_block` already handled\n+            return Ok(None);\n         }\n \n+        // Otherwise we really want to see the MIR -- but if we do not have it, maybe we can\n+        // emulate something. This is a HACK to support running without a full-MIR libstd.\n         let mir = match self.load_mir(instance.def) {\n             Ok(mir) => mir,\n             Err(EvalError { kind: EvalErrorKind::NoMirFor(path), .. }) => {\n-                self.call_missing_fn(\n-                    instance,\n-                    destination,\n-                    args,\n-                    sig,\n+                self.emulate_missing_fn(\n                     path,\n+                    args,\n+                    dest,\n+                    ret,\n                 )?;\n-                return Ok(true);\n+                // `goto_block` already handled\n+                return Ok(None);\n             }\n             Err(other) => return Err(other),\n         };\n \n-        let (return_place, return_to_block) = match destination {\n-            Some((place, block)) => (place, StackPopCleanup::Goto(block)),\n-            None => (Place::undef(), StackPopCleanup::None),\n-        };\n-\n-        self.push_stack_frame(\n-            instance,\n-            span,\n-            mir,\n-            return_place,\n-            return_to_block,\n-        )?;\n-\n-        Ok(false)\n+        Ok(Some(mir))\n     }\n \n-    fn call_foreign_item(\n+    fn emulate_foreign_item(\n         &mut self,\n         def_id: DefId,\n-        args: &[ValTy<'tcx>],\n-        dest: Place,\n-        dest_ty: Ty<'tcx>,\n-        dest_block: mir::BasicBlock,\n+        args: &[OpTy<'tcx>],\n+        dest: PlaceTy<'tcx>,\n+        ret: mir::BasicBlock,\n     ) -> EvalResult<'tcx> {\n         let attrs = self.tcx.get_attrs(def_id);\n         let link_name = match attr::first_attr_value_str_by_name(&attrs, \"link_name\") {\n             Some(name) => name.as_str(),\n             None => self.tcx.item_name(def_id).as_str(),\n         };\n-        let dest_layout = self.layout_of(dest_ty)?;\n \n         match &link_name[..] {\n             \"malloc\" => {\n-                let size = self.value_to_scalar(args[0])?.to_usize(self)?;\n+                let size = self.read_scalar(args[0])?.to_usize(&self)?;\n                 if size == 0 {\n-                    self.write_null(dest, dest_layout)?;\n+                    self.write_null(dest)?;\n                 } else {\n                     let align = self.tcx.data_layout.pointer_align;\n                     let ptr = self.memory.allocate(Size::from_bytes(size), align, MemoryKind::C.into())?;\n-                    self.write_scalar(dest, Scalar::Ptr(ptr), dest_ty)?;\n+                    self.write_scalar(Scalar::Ptr(ptr), dest)?;\n                 }\n             }\n \n             \"free\" => {\n-                let ptr = self.into_ptr(args[0].value)?.unwrap_or_err()?;\n+                let ptr = self.read_scalar(args[0])?.not_undef()?;\n                 if !ptr.is_null() {\n                     self.memory.deallocate(\n                         ptr.to_ptr()?,\n@@ -214,8 +146,8 @@ impl<'a, 'mir, 'tcx: 'mir + 'a> EvalContextExt<'tcx> for EvalContext<'a, 'mir, '\n             }\n \n             \"__rust_alloc\" => {\n-                let size = self.value_to_scalar(args[0])?.to_usize(self)?;\n-                let align = self.value_to_scalar(args[1])?.to_usize(self)?;\n+                let size = self.read_scalar(args[0])?.to_usize(&self)?;\n+                let align = self.read_scalar(args[1])?.to_usize(&self)?;\n                 if size == 0 {\n                     return err!(HeapAllocZeroBytes);\n                 }\n@@ -225,11 +157,11 @@ impl<'a, 'mir, 'tcx: 'mir + 'a> EvalContextExt<'tcx> for EvalContext<'a, 'mir, '\n                 let ptr = self.memory.allocate(Size::from_bytes(size),\n                                                Align::from_bytes(align, align).unwrap(),\n                                                MemoryKind::Rust.into())?;\n-                self.write_scalar(dest, Scalar::Ptr(ptr), dest_ty)?;\n+                self.write_scalar(Scalar::Ptr(ptr), dest)?;\n             }\n             \"__rust_alloc_zeroed\" => {\n-                let size = self.value_to_scalar(args[0])?.to_usize(self)?;\n-                let align = self.value_to_scalar(args[1])?.to_usize(self)?;\n+                let size = self.read_scalar(args[0])?.to_usize(&self)?;\n+                let align = self.read_scalar(args[1])?.to_usize(&self)?;\n                 if size == 0 {\n                     return err!(HeapAllocZeroBytes);\n                 }\n@@ -240,12 +172,12 @@ impl<'a, 'mir, 'tcx: 'mir + 'a> EvalContextExt<'tcx> for EvalContext<'a, 'mir, '\n                                                Align::from_bytes(align, align).unwrap(),\n                                                MemoryKind::Rust.into())?;\n                 self.memory.write_repeat(ptr.into(), 0, Size::from_bytes(size))?;\n-                self.write_scalar(dest, Scalar::Ptr(ptr), dest_ty)?;\n+                self.write_scalar(Scalar::Ptr(ptr), dest)?;\n             }\n             \"__rust_dealloc\" => {\n-                let ptr = self.into_ptr(args[0].value)?.unwrap_or_err()?.to_ptr()?;\n-                let old_size = self.value_to_scalar(args[1])?.to_usize(self)?;\n-                let align = self.value_to_scalar(args[2])?.to_usize(self)?;\n+                let ptr = self.read_scalar(args[0])?.to_ptr()?;\n+                let old_size = self.read_scalar(args[1])?.to_usize(&self)?;\n+                let align = self.read_scalar(args[2])?.to_usize(&self)?;\n                 if old_size == 0 {\n                     return err!(HeapAllocZeroBytes);\n                 }\n@@ -259,10 +191,10 @@ impl<'a, 'mir, 'tcx: 'mir + 'a> EvalContextExt<'tcx> for EvalContext<'a, 'mir, '\n                 )?;\n             }\n             \"__rust_realloc\" => {\n-                let ptr = self.into_ptr(args[0].value)?.unwrap_or_err()?.to_ptr()?;\n-                let old_size = self.value_to_scalar(args[1])?.to_usize(self)?;\n-                let align = self.value_to_scalar(args[2])?.to_usize(self)?;\n-                let new_size = self.value_to_scalar(args[3])?.to_usize(self)?;\n+                let ptr = self.read_scalar(args[0])?.to_ptr()?;\n+                let old_size = self.read_scalar(args[1])?.to_usize(&self)?;\n+                let align = self.read_scalar(args[2])?.to_usize(&self)?;\n+                let new_size = self.read_scalar(args[3])?.to_usize(&self)?;\n                 if old_size == 0 || new_size == 0 {\n                     return err!(HeapAllocZeroBytes);\n                 }\n@@ -277,7 +209,7 @@ impl<'a, 'mir, 'tcx: 'mir + 'a> EvalContextExt<'tcx> for EvalContext<'a, 'mir, '\n                     Align::from_bytes(align, align).unwrap(),\n                     MemoryKind::Rust.into(),\n                 )?;\n-                self.write_scalar(dest, Scalar::Ptr(new_ptr), dest_ty)?;\n+                self.write_scalar(Scalar::Ptr(new_ptr), dest)?;\n             }\n \n             \"syscall\" => {\n@@ -286,7 +218,7 @@ impl<'a, 'mir, 'tcx: 'mir + 'a> EvalContextExt<'tcx> for EvalContext<'a, 'mir, '\n                 //\n                 // libc::syscall(NR_GETRANDOM, buf.as_mut_ptr(), buf.len(), GRND_NONBLOCK)\n                 // is called if a `HashMap` is created the regular way.\n-                match self.value_to_scalar(args[0])?.to_usize(self)? {\n+                match self.read_scalar(args[0])?.to_usize(&self)? {\n                     318 | 511 => {\n                         return err!(Unimplemented(\n                             \"miri does not support random number generators\".to_owned(),\n@@ -301,8 +233,8 @@ impl<'a, 'mir, 'tcx: 'mir + 'a> EvalContextExt<'tcx> for EvalContext<'a, 'mir, '\n             }\n \n             \"dlsym\" => {\n-                let _handle = self.into_ptr(args[0].value)?;\n-                let symbol = self.into_ptr(args[1].value)?.unwrap_or_err()?.to_ptr()?;\n+                let _handle = self.read_scalar(args[0])?;\n+                let symbol = self.read_scalar(args[1])?.to_ptr()?;\n                 let symbol_name = self.memory.read_c_str(symbol)?;\n                 let err = format!(\"bad c unicode symbol: {:?}\", symbol_name);\n                 let symbol_name = ::std::str::from_utf8(symbol_name).unwrap_or(&err);\n@@ -315,21 +247,22 @@ impl<'a, 'mir, 'tcx: 'mir + 'a> EvalContextExt<'tcx> for EvalContext<'a, 'mir, '\n             \"__rust_maybe_catch_panic\" => {\n                 // fn __rust_maybe_catch_panic(f: fn(*mut u8), data: *mut u8, data_ptr: *mut usize, vtable_ptr: *mut usize) -> u32\n                 // We abort on panic, so not much is going on here, but we still have to call the closure\n-                let u8_ptr_ty = self.tcx.mk_mut_ptr(self.tcx.types.u8);\n-                let f = self.into_ptr(args[0].value)?.unwrap_or_err()?.to_ptr()?;\n-                let data = self.into_ptr(args[1].value)?.unwrap_or_err()?;\n+                let f = self.read_scalar(args[0])?.to_ptr()?;\n+                let data = self.read_scalar(args[1])?.not_undef()?;\n                 let f_instance = self.memory.get_fn(f)?;\n-                self.write_null(dest, dest_layout)?;\n+                self.write_null(dest)?;\n+                trace!(\"__rust_maybe_catch_panic: {:?}\", f_instance);\n \n                 // Now we make a function call.  TODO: Consider making this re-usable?  EvalContext::step does sth. similar for the TLS dtors,\n                 // and of course eval_main.\n                 let mir = self.load_mir(f_instance.def)?;\n+                let closure_dest = Place::null(&self);\n                 self.push_stack_frame(\n                     f_instance,\n                     mir.span,\n                     mir,\n-                    Place::undef(),\n-                    StackPopCleanup::Goto(dest_block),\n+                    closure_dest,\n+                    StackPopCleanup::Goto(Some(ret)), // directly return to caller\n                 )?;\n                 let mut args = self.frame().mir.args_iter();\n \n@@ -340,25 +273,24 @@ impl<'a, 'mir, 'tcx: 'mir + 'a> EvalContextExt<'tcx> for EvalContext<'a, 'mir, '\n                     ),\n                 )?;\n                 let arg_dest = self.eval_place(&mir::Place::Local(arg_local))?;\n-                self.write_ptr(arg_dest, data, u8_ptr_ty)?;\n+                self.write_scalar(data, arg_dest)?;\n \n                 assert!(args.next().is_none(), \"__rust_maybe_catch_panic argument has more arguments than expected\");\n \n-                // We ourselves return 0\n-                self.write_null(dest, dest_layout)?;\n+                // We ourselves will return 0, eventually (because we will not return if we paniced)\n+                self.write_null(dest)?;\n \n-                // Don't fall through\n+                // Don't fall through, we do NOT want to `goto_block`!\n                 return Ok(());\n             }\n \n-            \"__rust_start_panic\" => {\n-                return err!(Panic);\n-            }\n+            \"__rust_start_panic\" =>\n+                return err!(MachineError(\"the evaluated program panicked\".to_string())),\n \n             \"memcmp\" => {\n-                let left = self.into_ptr(args[0].value)?.unwrap_or_err()?;\n-                let right = self.into_ptr(args[1].value)?.unwrap_or_err()?;\n-                let n = Size::from_bytes(self.value_to_scalar(args[2])?.to_usize(self)?);\n+                let left = self.read_scalar(args[0])?.not_undef()?;\n+                let right = self.read_scalar(args[1])?.not_undef()?;\n+                let n = Size::from_bytes(self.read_scalar(args[2])?.to_usize(&self)?);\n \n                 let result = {\n                     let left_bytes = self.memory.read_bytes(left, n)?;\n@@ -373,58 +305,57 @@ impl<'a, 'mir, 'tcx: 'mir + 'a> EvalContextExt<'tcx> for EvalContext<'a, 'mir, '\n                 };\n \n                 self.write_scalar(\n-                    dest,\n                     Scalar::from_i32(result),\n-                    dest_ty,\n+                    dest,\n                 )?;\n             }\n \n             \"memrchr\" => {\n-                let ptr = self.into_ptr(args[0].value)?.unwrap_or_err()?;\n-                let val = self.value_to_scalar(args[1])?.to_bytes()? as u8;\n-                let num = self.value_to_scalar(args[2])?.to_usize(self)?;\n+                let ptr = self.read_scalar(args[0])?.not_undef()?;\n+                let val = self.read_scalar(args[1])?.to_bytes()? as u8;\n+                let num = self.read_scalar(args[2])?.to_usize(&self)?;\n                 if let Some(idx) = self.memory.read_bytes(ptr, Size::from_bytes(num))?.iter().rev().position(\n                     |&c| c == val,\n                 )\n                 {\n                     let new_ptr = ptr.ptr_offset(Size::from_bytes(num - idx as u64 - 1), &self)?;\n-                    self.write_ptr(dest, new_ptr, dest_ty)?;\n+                    self.write_scalar(new_ptr, dest)?;\n                 } else {\n-                    self.write_null(dest, dest_layout)?;\n+                    self.write_null(dest)?;\n                 }\n             }\n \n             \"memchr\" => {\n-                let ptr = self.into_ptr(args[0].value)?.unwrap_or_err()?;\n-                let val = self.value_to_scalar(args[1])?.to_bytes()? as u8;\n-                let num = self.value_to_scalar(args[2])?.to_usize(self)?;\n+                let ptr = self.read_scalar(args[0])?.not_undef()?;\n+                let val = self.read_scalar(args[1])?.to_bytes()? as u8;\n+                let num = self.read_scalar(args[2])?.to_usize(&self)?;\n                 if let Some(idx) = self.memory.read_bytes(ptr, Size::from_bytes(num))?.iter().position(\n                     |&c| c == val,\n                 )\n                 {\n                     let new_ptr = ptr.ptr_offset(Size::from_bytes(idx as u64), &self)?;\n-                    self.write_ptr(dest, new_ptr, dest_ty)?;\n+                    self.write_scalar(new_ptr, dest)?;\n                 } else {\n-                    self.write_null(dest, dest_layout)?;\n+                    self.write_null(dest)?;\n                 }\n             }\n \n             \"getenv\" => {\n                 let result = {\n-                    let name_ptr = self.into_ptr(args[0].value)?.unwrap_or_err()?.to_ptr()?;\n+                    let name_ptr = self.read_scalar(args[0])?.to_ptr()?;\n                     let name = self.memory.read_c_str(name_ptr)?;\n                     match self.machine.env_vars.get(name) {\n                         Some(&var) => Scalar::Ptr(var),\n                         None => Scalar::null(self.memory.pointer_size()),\n                     }\n                 };\n-                self.write_scalar(dest, result, dest_ty)?;\n+                self.write_scalar(result, dest)?;\n             }\n \n             \"unsetenv\" => {\n                 let mut success = None;\n                 {\n-                    let name_ptr = self.into_ptr(args[0].value)?.unwrap_or_err()?;\n+                    let name_ptr = self.read_scalar(args[0])?.not_undef()?;\n                     if !name_ptr.is_null() {\n                         let name = self.memory.read_c_str(name_ptr.to_ptr()?)?;\n                         if !name.is_empty() && !name.contains(&b'=') {\n@@ -436,17 +367,17 @@ impl<'a, 'mir, 'tcx: 'mir + 'a> EvalContextExt<'tcx> for EvalContext<'a, 'mir, '\n                     if let Some(var) = old {\n                         self.memory.deallocate(var, None, MemoryKind::Env.into())?;\n                     }\n-                    self.write_null(dest, dest_layout)?;\n+                    self.write_null(dest)?;\n                 } else {\n-                    self.write_scalar(dest, Scalar::from_int(-1, dest_layout.size), dest_ty)?;\n+                    self.write_scalar(Scalar::from_int(-1, dest.layout.size), dest)?;\n                 }\n             }\n \n             \"setenv\" => {\n                 let mut new = None;\n                 {\n-                    let name_ptr = self.into_ptr(args[0].value)?.unwrap_or_err()?;\n-                    let value_ptr = self.into_ptr(args[1].value)?.unwrap_or_err()?.to_ptr()?;\n+                    let name_ptr = self.read_scalar(args[0])?.not_undef()?;\n+                    let value_ptr = self.read_scalar(args[1])?.to_ptr()?;\n                     let value = self.memory.read_c_str(value_ptr)?;\n                     if !name_ptr.is_null() {\n                         let name = self.memory.read_c_str(name_ptr.to_ptr()?)?;\n@@ -472,16 +403,16 @@ impl<'a, 'mir, 'tcx: 'mir + 'a> EvalContextExt<'tcx> for EvalContext<'a, 'mir, '\n                     {\n                         self.memory.deallocate(var, None, MemoryKind::Env.into())?;\n                     }\n-                    self.write_null(dest, dest_layout)?;\n+                    self.write_null(dest)?;\n                 } else {\n-                    self.write_scalar(dest, Scalar::from_int(-1, dest_layout.size), dest_ty)?;\n+                    self.write_scalar(Scalar::from_int(-1, dest.layout.size), dest)?;\n                 }\n             }\n \n             \"write\" => {\n-                let fd = self.value_to_scalar(args[0])?.to_bytes()?;\n-                let buf = self.into_ptr(args[1].value)?.unwrap_or_err()?;\n-                let n = self.value_to_scalar(args[2])?.to_bytes()? as u64;\n+                let fd = self.read_scalar(args[0])?.to_bytes()?;\n+                let buf = self.read_scalar(args[1])?.not_undef()?;\n+                let n = self.read_scalar(args[2])?.to_bytes()? as u64;\n                 trace!(\"Called write({:?}, {:?}, {:?})\", fd, buf, n);\n                 let result = if fd == 1 || fd == 2 {\n                     // stdout/stderr\n@@ -501,36 +432,31 @@ impl<'a, 'mir, 'tcx: 'mir + 'a> EvalContextExt<'tcx> for EvalContext<'a, 'mir, '\n                     warn!(\"Ignored output to FD {}\", fd);\n                     n as i64 // pretend it all went well\n                 }; // now result is the value we return back to the program\n-                let ptr_size = self.memory.pointer_size();\n                 self.write_scalar(\n+                    Scalar::from_int(result, dest.layout.size),\n                     dest,\n-                    Scalar::from_int(result, ptr_size),\n-                    dest_ty,\n                 )?;\n             }\n \n             \"strlen\" => {\n-                let ptr = self.into_ptr(args[0].value)?.unwrap_or_err()?.to_ptr()?;\n+                let ptr = self.read_scalar(args[0])?.to_ptr()?;\n                 let n = self.memory.read_c_str(ptr)?.len();\n-                let ptr_size = self.memory.pointer_size();\n-                self.write_scalar(dest, Scalar::from_uint(n as u64, ptr_size), dest_ty)?;\n+                self.write_scalar(Scalar::from_uint(n as u64, dest.layout.size), dest)?;\n             }\n \n             // Some things needed for sys::thread initialization to go through\n             \"signal\" | \"sigaction\" | \"sigaltstack\" => {\n-                let ptr_size = self.memory.pointer_size();\n-                self.write_scalar(dest, Scalar::null(ptr_size), dest_ty)?;\n+                self.write_scalar(Scalar::null(dest.layout.size), dest)?;\n             }\n \n             \"sysconf\" => {\n-                let name = self.value_to_scalar(args[0])?.to_usize(self)?;\n-                let ptr_size = self.memory.pointer_size();\n+                let name = self.read_scalar(args[0])?.to_i32()?;\n \n                 trace!(\"sysconf() called with name {}\", name);\n                 // cache the sysconf integers via miri's global cache\n                 let paths = &[\n-                    (&[\"libc\", \"_SC_PAGESIZE\"], Scalar::from_int(4096, ptr_size)),\n-                    (&[\"libc\", \"_SC_GETPW_R_SIZE_MAX\"], Scalar::from_int(-1, ptr_size)),\n+                    (&[\"libc\", \"_SC_PAGESIZE\"], Scalar::from_int(4096, dest.layout.size)),\n+                    (&[\"libc\", \"_SC_GETPW_R_SIZE_MAX\"], Scalar::from_int(-1, dest.layout.size)),\n                 ];\n                 let mut result = None;\n                 for &(path, path_value) in paths {\n@@ -540,15 +466,17 @@ impl<'a, 'mir, 'tcx: 'mir + 'a> EvalContextExt<'tcx> for EvalContext<'a, 'mir, '\n                             promoted: None,\n                         };\n                         let const_val = self.const_eval(cid)?;\n-                        let value = const_val.unwrap_usize(self.tcx.tcx);\n+                        let value = const_val.unwrap_bits(\n+                            self.tcx.tcx,\n+                            ty::ParamEnv::empty().and(self.tcx.types.i32)) as i32;\n                         if value == name {\n                             result = Some(path_value);\n                             break;\n                         }\n                     }\n                 }\n                 if let Some(result) = result {\n-                    self.write_scalar(dest, result, dest_ty)?;\n+                    self.write_scalar(result, dest)?;\n                 } else {\n                     return err!(Unimplemented(\n                         format!(\"Unimplemented sysconf name: {}\", name),\n@@ -558,10 +486,10 @@ impl<'a, 'mir, 'tcx: 'mir + 'a> EvalContextExt<'tcx> for EvalContext<'a, 'mir, '\n \n             // Hook pthread calls that go to the thread-local storage memory subsystem\n             \"pthread_key_create\" => {\n-                let key_ptr = self.into_ptr(args[0].value)?.unwrap_or_err()?;\n+                let key_ptr = self.read_scalar(args[0])?.to_ptr()?;\n \n                 // Extract the function type out of the signature (that seems easier than constructing it ourselves...)\n-                let dtor = match self.into_ptr(args[1].value)?.unwrap_or_err()? {\n+                let dtor = match self.read_scalar(args[1])?.not_undef()? {\n                     Scalar::Ptr(dtor_ptr) => Some(self.memory.get_fn(dtor_ptr)?),\n                     Scalar::Bits { bits: 0, size } => {\n                         assert_eq!(size as u64, self.memory.pointer_size().bytes());\n@@ -571,7 +499,7 @@ impl<'a, 'mir, 'tcx: 'mir + 'a> EvalContextExt<'tcx> for EvalContext<'a, 'mir, '\n                 };\n \n                 // Figure out how large a pthread TLS key actually is. This is libc::pthread_key_t.\n-                let key_type = args[0].ty.builtin_deref(true)\n+                let key_type = args[0].layout.ty.builtin_deref(true)\n                                    .ok_or_else(|| EvalErrorKind::AbiViolation(\"Wrong signature used for pthread_key_create: First argument must be a raw pointer.\".to_owned()))?.ty;\n                 let key_layout = self.layout_of(key_type)?;\n \n@@ -585,54 +513,76 @@ impl<'a, 'mir, 'tcx: 'mir + 'a> EvalContextExt<'tcx> for EvalContext<'a, 'mir, '\n                     key_layout.align,\n                     Scalar::from_uint(key, key_layout.size).into(),\n                     key_layout.size,\n-                    key_layout.align,\n-                    false,\n                 )?;\n \n                 // Return success (0)\n-                self.write_null(dest, dest_layout)?;\n+                self.write_null(dest)?;\n             }\n             \"pthread_key_delete\" => {\n-                let key = self.value_to_scalar(args[0])?.to_bytes()?;\n+                let key = self.read_scalar(args[0])?.to_bytes()?;\n                 self.memory.delete_tls_key(key)?;\n                 // Return success (0)\n-                self.write_null(dest, dest_layout)?;\n+                self.write_null(dest)?;\n             }\n             \"pthread_getspecific\" => {\n-                let key = self.value_to_scalar(args[0])?.to_bytes()?;\n+                let key = self.read_scalar(args[0])?.to_bytes()?;\n                 let ptr = self.memory.load_tls(key)?;\n-                self.write_ptr(dest, ptr, dest_ty)?;\n+                self.write_scalar(ptr, dest)?;\n             }\n             \"pthread_setspecific\" => {\n-                let key = self.value_to_scalar(args[0])?.to_bytes()?;\n-                let new_ptr = self.into_ptr(args[1].value)?.unwrap_or_err()?;\n+                let key = self.read_scalar(args[0])?.to_bytes()?;\n+                let new_ptr = self.read_scalar(args[1])?.not_undef()?;\n                 self.memory.store_tls(key, new_ptr)?;\n \n                 // Return success (0)\n-                self.write_null(dest, dest_layout)?;\n+                self.write_null(dest)?;\n             }\n \n             \"_tlv_atexit\" => {\n-                return err!(Unimplemented(\"Thread-local store is not fully supported on macOS\".to_owned()));\n+                // FIXME: Register the dtor\n             },\n \n-            // Stub out all the other pthread calls to just return 0\n-            link_name if link_name.starts_with(\"pthread_\") => {\n-                debug!(\"ignoring C ABI call: {}\", link_name);\n-                self.write_null(dest, dest_layout)?;\n+            // Determining stack base address\n+            \"pthread_attr_init\" | \"pthread_attr_destroy\" | \"pthread_attr_get_np\" |\n+            \"pthread_getattr_np\" | \"pthread_self\" | \"pthread_get_stacksize_np\" => {\n+                self.write_null(dest)?;\n+            }\n+            \"pthread_attr_getstack\" => {\n+                // second argument is where we are supposed to write the stack size\n+                let ptr = self.ref_to_mplace(self.read_value(args[1])?)?;\n+                let stackaddr = Scalar::from_int(0x80000, args[1].layout.size); // just any address\n+                self.write_scalar(stackaddr, ptr.into())?;\n+                // return 0\n+                self.write_null(dest)?;\n+            }\n+            \"pthread_get_stackaddr_np\" => {\n+                let stackaddr = Scalar::from_int(0x80000, dest.layout.size); // just any address\n+                self.write_scalar(stackaddr, dest)?;\n+            }\n+\n+            // Stub out calls for condvar, mutex and rwlock to just return 0\n+            \"pthread_mutexattr_init\" | \"pthread_mutexattr_settype\" | \"pthread_mutex_init\" |\n+            \"pthread_mutexattr_destroy\" | \"pthread_mutex_lock\" | \"pthread_mutex_unlock\" |\n+            \"pthread_mutex_destroy\" | \"pthread_rwlock_rdlock\" | \"pthread_rwlock_unlock\" |\n+            \"pthread_rwlock_wrlock\" | \"pthread_rwlock_destroy\" | \"pthread_condattr_init\" |\n+            \"pthread_condattr_setclock\" | \"pthread_cond_init\" | \"pthread_condattr_destroy\" |\n+            \"pthread_cond_destroy\" => {\n+                self.write_null(dest)?;\n             }\n \n             \"mmap\" => {\n                 // This is a horrible hack, but well... the guard page mechanism calls mmap and expects a particular return value, so we give it that value\n-                let addr = self.into_ptr(args[0].value)?.unwrap_or_err()?;\n-                self.write_ptr(dest, addr, dest_ty)?;\n+                let addr = self.read_scalar(args[0])?.not_undef()?;\n+                self.write_scalar(addr, dest)?;\n+            }\n+            \"mprotect\" => {\n+                self.write_null(dest)?;\n             }\n \n             // Windows API subs\n             \"AddVectoredExceptionHandler\" => {\n                 // any non zero value works for the stdlib. This is just used for stackoverflows anyway\n-                let ptr_size = self.memory.pointer_size();\n-                self.write_scalar(dest, Scalar::from_int(1, ptr_size), dest_ty)?;\n+                self.write_scalar(Scalar::from_int(1, dest.layout.size), dest)?;\n             },\n             \"InitializeCriticalSection\" |\n             \"EnterCriticalSection\" |\n@@ -645,11 +595,11 @@ impl<'a, 'mir, 'tcx: 'mir + 'a> EvalContextExt<'tcx> for EvalContext<'a, 'mir, '\n             \"GetProcAddress\" |\n             \"TryEnterCriticalSection\" => {\n                 // pretend these do not exist/nothing happened, by returning zero\n-                self.write_scalar(dest, Scalar::from_int(0, dest_layout.size), dest_ty)?;\n+                self.write_null(dest)?;\n             },\n             \"GetLastError\" => {\n                 // this is c::ERROR_CALL_NOT_IMPLEMENTED\n-                self.write_scalar(dest, Scalar::from_int(120, dest_layout.size), dest_ty)?;\n+                self.write_scalar(Scalar::from_int(120, dest.layout.size), dest)?;\n             },\n \n             // Windows TLS\n@@ -660,23 +610,23 @@ impl<'a, 'mir, 'tcx: 'mir + 'a> EvalContextExt<'tcx> for EvalContext<'a, 'mir, '\n                 let key = self.memory.create_tls_key(None) as u128;\n \n                 // Figure out how large a TLS key actually is. This is c::DWORD.\n-                if dest_layout.size.bits() < 128 && key >= (1u128 << dest_layout.size.bits() as u128) {\n+                if dest.layout.size.bits() < 128 && key >= (1u128 << dest.layout.size.bits() as u128) {\n                     return err!(OutOfTls);\n                 }\n-                self.write_scalar(dest, Scalar::from_uint(key, dest_layout.size), dest_layout.ty)?;\n+                self.write_scalar(Scalar::from_uint(key, dest.layout.size), dest)?;\n             }\n             \"TlsGetValue\" => {\n-                let key = self.value_to_scalar(args[0])?.to_bytes()?;\n+                let key = self.read_scalar(args[0])?.to_bytes()?;\n                 let ptr = self.memory.load_tls(key)?;\n-                self.write_ptr(dest, ptr, dest_ty)?;\n+                self.write_scalar(ptr, dest)?;\n             }\n             \"TlsSetValue\" => {\n-                let key = self.value_to_scalar(args[0])?.to_bytes()?;\n-                let new_ptr = self.into_ptr(args[1].value)?.unwrap_or_err()?;\n+                let key = self.read_scalar(args[0])?.to_bytes()?;\n+                let new_ptr = self.read_scalar(args[1])?.not_undef()?;\n                 self.memory.store_tls(key, new_ptr)?;\n \n                 // Return success (1)\n-                self.write_scalar(dest, Scalar::from_int(1, dest_layout.size), dest_ty)?;\n+                self.write_scalar(Scalar::from_int(1, dest.layout.size), dest)?;\n             }\n \n             // We can't execute anything else\n@@ -687,11 +637,8 @@ impl<'a, 'mir, 'tcx: 'mir + 'a> EvalContextExt<'tcx> for EvalContext<'a, 'mir, '\n             }\n         }\n \n-        // Since we pushed no stack frame, the main loop will act\n-        // as if the call just completed and it's returning to the\n-        // current frame.\n-        self.dump_local(dest);\n-        self.goto_block(dest_block);\n+        self.goto_block(Some(ret))?;\n+        self.dump_place(*dest);\n         Ok(())\n     }\n \n@@ -729,41 +676,27 @@ impl<'a, 'mir, 'tcx: 'mir + 'a> EvalContextExt<'tcx> for EvalContext<'a, 'mir, '\n             })\n     }\n \n-    fn call_missing_fn(\n+    fn emulate_missing_fn(\n         &mut self,\n-        instance: ty::Instance<'tcx>,\n-        destination: Option<(Place, mir::BasicBlock)>,\n-        args: &[ValTy<'tcx>],\n-        sig: ty::FnSig<'tcx>,\n         path: String,\n+        _args: &[OpTy<'tcx>],\n+        dest: Option<PlaceTy<'tcx>>,\n+        ret: Option<mir::BasicBlock>,\n     ) -> EvalResult<'tcx> {\n         // In some cases in non-MIR libstd-mode, not having a destination is legit.  Handle these early.\n         match &path[..] {\n             \"std::panicking::rust_panic_with_hook\" |\n             \"core::panicking::panic_fmt::::panic_impl\" |\n-            \"std::rt::begin_panic_fmt\" => return err!(Panic),\n+            \"std::rt::begin_panic_fmt\" =>\n+                return err!(MachineError(\"the evaluated program panicked\".to_string())),\n             _ => {}\n         }\n \n-        let dest_ty = sig.output();\n-        let (dest, dest_block) = destination.ok_or_else(\n+        let dest = dest.ok_or_else(\n+            // Must be some function we do not support\n             || EvalErrorKind::NoMirFor(path.clone()),\n         )?;\n \n-        if self.tcx.is_foreign_item(instance.def_id()) {\n-            // An external function\n-            // TODO: That functions actually has a similar preamble to what follows here.  May make sense to\n-            // unify these two mechanisms for \"hooking into missing functions\".\n-            self.call_foreign_item(\n-                instance.def_id(),\n-                args,\n-                dest,\n-                dest_ty,\n-                dest_block,\n-            )?;\n-            return Ok(());\n-        }\n-\n         match &path[..] {\n             // A Rust function is missing, which means we are running with MIR missing for libstd (or other dependencies).\n             // Still, we can make many things mostly work by \"emulating\" or ignoring some functions.\n@@ -784,22 +717,18 @@ impl<'a, 'mir, 'tcx: 'mir + 'a> EvalContextExt<'tcx> for EvalContext<'a, 'mir, '\n             \"std::panicking::panicking\" |\n             \"std::rt::panicking\" => {\n                 // we abort on panic -> `std::rt::panicking` always returns false\n-                let bool = self.tcx.types.bool;\n-                self.write_scalar(dest, Scalar::from_bool(false), bool)?;\n+                self.write_scalar(Scalar::from_bool(false), dest)?;\n             }\n \n             _ => return err!(NoMirFor(path)),\n         }\n \n-        // Since we pushed no stack frame, the main loop will act\n-        // as if the call just completed and it's returning to the\n-        // current frame.\n-        self.dump_local(dest);\n-        self.goto_block(dest_block);\n+        self.goto_block(ret)?;\n+        self.dump_place(*dest);\n         Ok(())\n     }\n \n-    fn write_null(&mut self, dest: Place, dest_layout: TyLayout<'tcx>) -> EvalResult<'tcx> {\n-        self.write_scalar(dest, Scalar::null(dest_layout.size), dest_layout.ty)\n+    fn write_null(&mut self, dest: PlaceTy<'tcx>) -> EvalResult<'tcx> {\n+        self.write_scalar(Scalar::null(dest.layout.size), dest)\n     }\n }"}, {"sha": "24285fa4a6e8f7cc0ed7738f4b7641fa416bbbaf", "filename": "src/helpers.rs", "status": "modified", "additions": 52, "deletions": 109, "changes": 161, "blob_url": "https://github.com/rust-lang/rust/blob/752accf4e48b83545fa2fcf205602f239389361d/src%2Fhelpers.rs", "raw_url": "https://github.com/rust-lang/rust/raw/752accf4e48b83545fa2fcf205602f239389361d/src%2Fhelpers.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Fhelpers.rs?ref=752accf4e48b83545fa2fcf205602f239389361d", "patch": "@@ -1,128 +1,71 @@\n-use mir;\n-use rustc::ty::Ty;\n-use rustc::ty::layout::{LayoutOf, Size};\n+use rustc::ty::layout::Size;\n \n-use super::{Scalar, ScalarExt, EvalResult, EvalContext, ValTy};\n-use rustc_mir::interpret::sign_extend;\n+use super::{Scalar, ScalarMaybeUndef, EvalResult};\n \n-pub trait EvalContextExt<'tcx> {\n-    fn wrapping_pointer_offset(\n-        &self,\n-        ptr: Scalar,\n-        pointee_ty: Ty<'tcx>,\n-        offset: i64,\n-    ) -> EvalResult<'tcx, Scalar>;\n-\n-    fn pointer_offset(\n-        &self,\n-        ptr: Scalar,\n-        pointee_ty: Ty<'tcx>,\n-        offset: i64,\n-    ) -> EvalResult<'tcx, Scalar>;\n-\n-    fn value_to_isize(\n-        &self,\n-        value: ValTy<'tcx>,\n-    ) -> EvalResult<'tcx, i64>;\n+pub trait ScalarExt {\n+    fn null(size: Size) -> Self;\n+    fn from_i32(i: i32) -> Self;\n+    fn from_uint(i: impl Into<u128>, ptr_size: Size) -> Self;\n+    fn from_int(i: impl Into<i128>, ptr_size: Size) -> Self;\n+    fn from_f32(f: f32) -> Self;\n+    fn from_f64(f: f64) -> Self;\n+    fn is_null(self) -> bool;\n+}\n \n-    fn value_to_usize(\n-        &self,\n-        value: ValTy<'tcx>,\n-    ) -> EvalResult<'tcx, u64>;\n+pub trait FalibleScalarExt {\n+    /// HACK: this function just extracts all bits if `defined != 0`\n+    /// Mainly used for args of C-functions and we should totally correctly fetch the size\n+    /// of their arguments\n+    fn to_bytes(self) -> EvalResult<'static, u128>;\n+}\n \n-    fn value_to_i32(\n-        &self,\n-        value: ValTy<'tcx>,\n-    ) -> EvalResult<'tcx, i32>;\n+impl ScalarExt for Scalar {\n+    fn null(size: Size) -> Self {\n+        Scalar::Bits { bits: 0, size: size.bytes() as u8 }\n+    }\n \n-    fn value_to_u8(\n-        &self,\n-        value: ValTy<'tcx>,\n-    ) -> EvalResult<'tcx, u8>;\n-}\n+    fn from_i32(i: i32) -> Self {\n+        Scalar::Bits { bits: i as u32 as u128, size: 4 }\n+    }\n \n-impl<'a, 'mir, 'tcx> EvalContextExt<'tcx> for EvalContext<'a, 'mir, 'tcx, super::Evaluator<'tcx>> {\n-    fn wrapping_pointer_offset(\n-        &self,\n-        ptr: Scalar,\n-        pointee_ty: Ty<'tcx>,\n-        offset: i64,\n-    ) -> EvalResult<'tcx, Scalar> {\n-        // FIXME: assuming here that type size is < i64::max_value()\n-        let pointee_size = self.layout_of(pointee_ty)?.size.bytes() as i64;\n-        let offset = offset.overflowing_mul(pointee_size).0;\n-        Ok(ptr.ptr_wrapping_signed_offset(offset, self))\n+    fn from_uint(i: impl Into<u128>, size: Size) -> Self {\n+        Scalar::Bits { bits: i.into(), size: size.bytes() as u8 }\n     }\n \n-    fn pointer_offset(\n-        &self,\n-        ptr: Scalar,\n-        pointee_ty: Ty<'tcx>,\n-        offset: i64,\n-    ) -> EvalResult<'tcx, Scalar> {\n-        // This function raises an error if the offset moves the pointer outside of its allocation.  We consider\n-        // ZSTs their own huge allocation that doesn't overlap with anything (and nothing moves in there because the size is 0).\n-        // We also consider the NULL pointer its own separate allocation, and all the remaining integers pointers their own\n-        // allocation.\n+    fn from_int(i: impl Into<i128>, size: Size) -> Self {\n+        Scalar::Bits { bits: i.into() as u128, size: size.bytes() as u8 }\n+    }\n \n-        if ptr.is_null() {\n-            // NULL pointers must only be offset by 0\n-            return if offset == 0 {\n-                Ok(ptr)\n-            } else {\n-                err!(InvalidNullPointerUsage)\n-            };\n-        }\n-        // FIXME: assuming here that type size is < i64::max_value()\n-        let pointee_size = self.layout_of(pointee_ty)?.size.bytes() as i64;\n-         if let Some(offset) = offset.checked_mul(pointee_size) {\n-            let ptr = ptr.ptr_signed_offset(offset, self)?;\n-            // Do not do bounds-checking for integers; they can never alias a normal pointer anyway.\n-            if let Scalar::Ptr(ptr) = ptr {\n-                self.memory.check_bounds(ptr, false)?;\n-            } else if ptr.is_null() {\n-                // We moved *to* a NULL pointer.  That seems wrong, LLVM considers the NULL pointer its own small allocation.  Reject this, for now.\n-                return err!(InvalidNullPointerUsage);\n-            }\n-            Ok(ptr)\n-        } else {\n-            err!(Overflow(mir::BinOp::Mul))\n-        }\n+    fn from_f32(f: f32) -> Self {\n+        Scalar::Bits { bits: f.to_bits() as u128, size: 4 }\n     }\n \n-    fn value_to_isize(\n-        &self,\n-        value: ValTy<'tcx>,\n-    ) -> EvalResult<'tcx, i64> {\n-        assert_eq!(value.ty, self.tcx.types.isize);\n-        let raw = self.value_to_scalar(value)?.to_bits(self.memory.pointer_size())?;\n-        let raw = sign_extend(raw, self.layout_of(self.tcx.types.isize).unwrap());\n-        Ok(raw as i64)\n+    fn from_f64(f: f64) -> Self {\n+        Scalar::Bits { bits: f.to_bits() as u128, size: 8 }\n     }\n \n-    fn value_to_usize(\n-        &self,\n-        value: ValTy<'tcx>,\n-    ) -> EvalResult<'tcx, u64> {\n-        assert_eq!(value.ty, self.tcx.types.usize);\n-        self.value_to_scalar(value)?.to_bits(self.memory.pointer_size()).map(|v| v as u64)\n+    fn is_null(self) -> bool {\n+        match self {\n+            Scalar::Bits { bits, .. } => bits == 0,\n+            Scalar::Ptr(_) => false\n+        }\n     }\n+}\n \n-    fn value_to_i32(\n-        &self,\n-        value: ValTy<'tcx>,\n-    ) -> EvalResult<'tcx, i32> {\n-        assert_eq!(value.ty, self.tcx.types.i32);\n-        let raw = self.value_to_scalar(value)?.to_bits(Size::from_bits(32))?;\n-        let raw = sign_extend(raw, self.layout_of(self.tcx.types.i32).unwrap());\n-        Ok(raw as i32)\n+impl FalibleScalarExt for Scalar {\n+    fn to_bytes(self) -> EvalResult<'static, u128> {\n+        match self {\n+            Scalar::Bits { bits, size } => {\n+                assert_ne!(size, 0);\n+                Ok(bits)\n+            },\n+            Scalar::Ptr(_) => err!(ReadPointerAsBytes),\n+        }\n     }\n+}\n \n-    fn value_to_u8(\n-        &self,\n-        value: ValTy<'tcx>,\n-    ) -> EvalResult<'tcx, u8> {\n-        assert_eq!(value.ty, self.tcx.types.u8);\n-        self.value_to_scalar(value)?.to_bits(Size::from_bits(8)).map(|v| v as u8)\n+impl FalibleScalarExt for ScalarMaybeUndef {\n+    fn to_bytes(self) -> EvalResult<'static, u128> {\n+        self.not_undef()?.to_bytes()\n     }\n }"}, {"sha": "6847d8e546f363835f94a5bec8708b0457d37491", "filename": "src/intrinsic.rs", "status": "modified", "additions": 211, "deletions": 338, "changes": 549, "blob_url": "https://github.com/rust-lang/rust/blob/752accf4e48b83545fa2fcf205602f239389361d/src%2Fintrinsic.rs", "raw_url": "https://github.com/rust-lang/rust/raw/752accf4e48b83545fa2fcf205602f239389361d/src%2Fintrinsic.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Fintrinsic.rs?ref=752accf4e48b83545fa2fcf205602f239389361d", "patch": "@@ -1,77 +1,84 @@\n use rustc::mir;\n-use rustc::ty::layout::{TyLayout, LayoutOf, Size, Primitive, Integer::*};\n+use rustc::ty::layout::{self, LayoutOf, Size};\n use rustc::ty;\n \n-use rustc::mir::interpret::{EvalResult, Scalar, Value, ScalarMaybeUndef};\n-use rustc_mir::interpret::{Place, PlaceExtra, HasMemory, EvalContext, ValTy};\n+use rustc::mir::interpret::{EvalResult, Scalar, ScalarMaybeUndef};\n+use rustc_mir::interpret::{\n+    PlaceTy, EvalContext, OpTy, Value\n+};\n \n-use helpers::EvalContextExt as HelperEvalContextExt;\n-\n-use super::ScalarExt;\n+use super::{ScalarExt, FalibleScalarExt, OperatorEvalContextExt};\n \n pub trait EvalContextExt<'tcx> {\n     fn call_intrinsic(\n         &mut self,\n         instance: ty::Instance<'tcx>,\n-        args: &[ValTy<'tcx>],\n-        dest: Place,\n-        dest_layout: TyLayout<'tcx>,\n-        target: mir::BasicBlock,\n+        args: &[OpTy<'tcx>],\n+        dest: PlaceTy<'tcx>,\n     ) -> EvalResult<'tcx>;\n }\n \n impl<'a, 'mir, 'tcx> EvalContextExt<'tcx> for EvalContext<'a, 'mir, 'tcx, super::Evaluator<'tcx>> {\n     fn call_intrinsic(\n         &mut self,\n         instance: ty::Instance<'tcx>,\n-        args: &[ValTy<'tcx>],\n-        dest: Place,\n-        dest_layout: TyLayout<'tcx>,\n-        target: mir::BasicBlock,\n+        args: &[OpTy<'tcx>],\n+        dest: PlaceTy<'tcx>,\n     ) -> EvalResult<'tcx> {\n+        if self.emulate_intrinsic(instance, args, dest)? {\n+            return Ok(());\n+        }\n+\n         let substs = instance.substs;\n \n         let intrinsic_name = &self.tcx.item_name(instance.def_id()).as_str()[..];\n         match intrinsic_name {\n             \"add_with_overflow\" => {\n-                self.intrinsic_with_overflow(\n+                let l = self.read_value(args[0])?;\n+                let r = self.read_value(args[1])?;\n+                self.binop_with_overflow(\n                     mir::BinOp::Add,\n-                    args[0],\n-                    args[1],\n+                    l,\n+                    r,\n                     dest,\n-                    dest_layout.ty,\n                 )?\n             }\n \n             \"sub_with_overflow\" => {\n-                self.intrinsic_with_overflow(\n+                let l = self.read_value(args[0])?;\n+                let r = self.read_value(args[1])?;\n+                self.binop_with_overflow(\n                     mir::BinOp::Sub,\n-                    args[0],\n-                    args[1],\n+                    l,\n+                    r,\n                     dest,\n-                    dest_layout.ty,\n                 )?\n             }\n \n             \"mul_with_overflow\" => {\n-                self.intrinsic_with_overflow(\n+                let l = self.read_value(args[0])?;\n+                let r = self.read_value(args[1])?;\n+                self.binop_with_overflow(\n                     mir::BinOp::Mul,\n-                    args[0],\n-                    args[1],\n+                    l,\n+                    r,\n                     dest,\n-                    dest_layout.ty,\n                 )?\n             }\n \n             \"arith_offset\" => {\n-                let offset = self.value_to_isize(args[1])?;\n-                let ptr = self.into_ptr(args[0].value)?.unwrap_or_err()?;\n-                let result_ptr = self.wrapping_pointer_offset(ptr, substs.type_at(0), offset)?;\n-                self.write_ptr(dest, result_ptr, dest_layout.ty)?;\n+                let offset = self.read_scalar(args[1])?.to_isize(&self)?;\n+                let ptr = self.read_scalar(args[0])?.not_undef()?;\n+\n+                let pointee_ty = substs.type_at(0);\n+                let pointee_size = self.layout_of(pointee_ty)?.size.bytes() as i64;\n+                let offset = offset.overflowing_mul(pointee_size).0;\n+                let result_ptr = ptr.ptr_wrapping_signed_offset(offset, &self);\n+                self.write_scalar(result_ptr, dest)?;\n             }\n \n             \"assume\" => {\n-                let cond = self.value_to_scalar(args[0])?.to_bool()?;\n+                let cond = self.read_scalar(args[0])?.to_bool()?;\n                 if !cond {\n                     return err!(AssumptionNotHeld);\n                 }\n@@ -81,72 +88,45 @@ impl<'a, 'mir, 'tcx> EvalContextExt<'tcx> for EvalContext<'a, 'mir, 'tcx, super:\n             \"atomic_load_relaxed\" |\n             \"atomic_load_acq\" |\n             \"volatile_load\" => {\n-                let ptr = self.into_ptr(args[0].value)?.unwrap_or_err()?;\n-                let align = self.layout_of(args[0].ty)?.align;\n-\n-                let valty = ValTy {\n-                    value: Value::ByRef(ptr, align),\n-                    ty: substs.type_at(0),\n-                };\n-                self.write_value(valty, dest)?;\n+                let ptr = self.ref_to_mplace(self.read_value(args[0])?)?;\n+                let val = self.read_scalar(ptr.into())?; // make sure it fits into a scalar; otherwise it cannot be atomic\n+                self.write_scalar(val, dest)?;\n             }\n \n             \"atomic_store\" |\n             \"atomic_store_relaxed\" |\n             \"atomic_store_rel\" |\n             \"volatile_store\" => {\n-                let ty = substs.type_at(0);\n-                let align = self.layout_of(ty)?.align;\n-                let dest = self.into_ptr(args[0].value)?.unwrap_or_err()?;\n-                self.write_value_to_ptr(args[1].value, dest, align, ty)?;\n+                let ptr = self.ref_to_mplace(self.read_value(args[0])?)?;\n+                let val = self.read_scalar(args[1])?; // make sure it fits into a scalar; otherwise it cannot be atomic\n+                self.write_scalar(val, ptr.into())?;\n             }\n \n             \"atomic_fence_acq\" => {\n                 // we are inherently singlethreaded and singlecored, this is a nop\n             }\n \n             _ if intrinsic_name.starts_with(\"atomic_xchg\") => {\n-                let ty = substs.type_at(0);\n-                let align = self.layout_of(ty)?.align;\n-                let ptr = self.into_ptr(args[0].value)?.unwrap_or_err()?;\n-                let change = self.value_to_scalar(args[1])?;\n-                let old = self.read_value(ptr, align, ty)?;\n-                let old = match old {\n-                    Value::Scalar(val) => val,\n-                    Value::ByRef { .. } => bug!(\"just read the value, can't be byref\"),\n-                    Value::ScalarPair(..) => bug!(\"atomic_xchg doesn't work with nonprimitives\"),\n-                };\n-                self.write_scalar(dest, old, ty)?;\n-                self.write_scalar(\n-                    Place::from_scalar_ptr(ptr.into(), align),\n-                    change,\n-                    ty,\n-                )?;\n+                let ptr = self.ref_to_mplace(self.read_value(args[0])?)?;\n+                let new = self.read_scalar(args[1])?;\n+                let old = self.read_scalar(ptr.into())?;\n+                self.write_scalar(old, dest)?; // old value is returned\n+                self.write_scalar(new, ptr.into())?;\n             }\n \n             _ if intrinsic_name.starts_with(\"atomic_cxchg\") => {\n-                let ty = substs.type_at(0);\n-                let align = self.layout_of(ty)?.align;\n-                let ptr = self.into_ptr(args[0].value)?.unwrap_or_err()?;\n-                let expect_old = self.value_to_scalar(args[1])?;\n-                let change = self.value_to_scalar(args[2])?;\n-                let old = self.read_value(ptr, align, ty)?;\n-                let old = match old {\n-                    Value::Scalar(val) => val.unwrap_or_err()?,\n-                    Value::ByRef { .. } => bug!(\"just read the value, can't be byref\"),\n-                    Value::ScalarPair(..) => bug!(\"atomic_cxchg doesn't work with nonprimitives\"),\n-                };\n-                let (val, _) = self.binary_op(mir::BinOp::Eq, old, ty, expect_old, ty)?;\n-                let valty = ValTy {\n-                    value: Value::ScalarPair(old.into(), val.into()),\n-                    ty: dest_layout.ty,\n-                };\n-                self.write_value(valty, dest)?;\n-                self.write_scalar(\n-                    Place::from_scalar_ptr(ptr.into(), dest_layout.align),\n-                    change,\n-                    ty,\n-                )?;\n+                let ptr = self.ref_to_mplace(self.read_value(args[0])?)?;\n+                let expect_old = self.read_value(args[1])?; // read as value for the sake of `binary_op()`\n+                let new = self.read_scalar(args[2])?;\n+                let old = self.read_value(ptr.into())?; // read as value for the sake of `binary_op()`\n+                // binary_op will bail if either of them is not a scalar\n+                let (eq, _) = self.binary_op(mir::BinOp::Eq, old, expect_old)?;\n+                let res = Value::ScalarPair(old.to_scalar_or_undef(), eq.into());\n+                self.write_value(res, dest)?; // old value is returned\n+                // update ptr depending on comparison\n+                if eq.to_bool()? {\n+                    self.write_scalar(new, ptr.into())?;\n+                }\n             }\n \n             \"atomic_or\" |\n@@ -174,19 +154,10 @@ impl<'a, 'mir, 'tcx> EvalContextExt<'tcx> for EvalContext<'a, 'mir, 'tcx, super:\n             \"atomic_xsub_rel\" |\n             \"atomic_xsub_acqrel\" |\n             \"atomic_xsub_relaxed\" => {\n-                let ty = substs.type_at(0);\n-                let align = self.layout_of(ty)?.align;\n-                let ptr = self.into_ptr(args[0].value)?.unwrap_or_err()?;\n-                let change = self.value_to_scalar(args[1])?;\n-                let old = self.read_value(ptr, align, ty)?;\n-                let old = match old {\n-                    Value::Scalar(val) => val,\n-                    Value::ByRef { .. } => bug!(\"just read the value, can't be byref\"),\n-                    Value::ScalarPair(..) => {\n-                        bug!(\"atomic_xadd_relaxed doesn't work with nonprimitives\")\n-                    }\n-                };\n-                self.write_scalar(dest, old, ty)?;\n+                let ptr = self.ref_to_mplace(self.read_value(args[0])?)?;\n+                let rhs = self.read_value(args[1])?;\n+                let old = self.read_value(ptr.into())?;\n+                self.write_value(*old, dest)?; // old value is returned\n                 let op = match intrinsic_name.split('_').nth(1).unwrap() {\n                     \"or\" => mir::BinOp::BitOr,\n                     \"xor\" => mir::BinOp::BitXor,\n@@ -196,8 +167,8 @@ impl<'a, 'mir, 'tcx> EvalContextExt<'tcx> for EvalContext<'a, 'mir, 'tcx, super:\n                     _ => bug!(),\n                 };\n                 // FIXME: what do atomics do on overflow?\n-                let (val, _) = self.binary_op(op, old.unwrap_or_err()?, ty, change, ty)?;\n-                self.write_scalar(Place::from_scalar_ptr(ptr.into(), dest_layout.align), val, ty)?;\n+                let (val, _) = self.binary_op(op, old, rhs)?;\n+                self.write_scalar(val, ptr.into())?;\n             }\n \n             \"breakpoint\" => unimplemented!(), // halt miri\n@@ -207,13 +178,13 @@ impl<'a, 'mir, 'tcx> EvalContextExt<'tcx> for EvalContext<'a, 'mir, 'tcx, super:\n                 let elem_ty = substs.type_at(0);\n                 let elem_layout = self.layout_of(elem_ty)?;\n                 let elem_size = elem_layout.size.bytes();\n-                let count = self.value_to_usize(args[2])?;\n+                let count = self.read_scalar(args[2])?.to_usize(&self)?;\n                 if count * elem_size != 0 {\n                     // TODO: We do not even validate alignment for the 0-bytes case.  libstd relies on this in vec::IntoIter::next.\n                     // Also see the write_bytes intrinsic.\n                     let elem_align = elem_layout.align;\n-                    let src = self.into_ptr(args[0].value)?.unwrap_or_err()?;\n-                    let dest = self.into_ptr(args[1].value)?.unwrap_or_err()?;\n+                    let src = self.read_scalar(args[0])?.not_undef()?;\n+                    let dest = self.read_scalar(args[1])?.not_undef()?;\n                     self.memory.copy(\n                         src,\n                         elem_align,\n@@ -225,37 +196,15 @@ impl<'a, 'mir, 'tcx> EvalContextExt<'tcx> for EvalContext<'a, 'mir, 'tcx, super:\n                 }\n             }\n \n-            \"ctpop\" | \"cttz\" | \"cttz_nonzero\" | \"ctlz\" | \"ctlz_nonzero\" | \"bswap\" => {\n-                let ty = substs.type_at(0);\n-                let num = self.value_to_scalar(args[0])?.to_bytes()?;\n-                let kind = match self.layout_of(ty)?.abi {\n-                    ty::layout::Abi::Scalar(ref scalar) => scalar.value,\n-                    _ => Err(::rustc::mir::interpret::EvalErrorKind::TypeNotPrimitive(ty))?,\n-                };\n-                let num = if intrinsic_name.ends_with(\"_nonzero\") {\n-                    if num == 0 {\n-                        return err!(Intrinsic(format!(\"{} called on 0\", intrinsic_name)));\n-                    }\n-                    numeric_intrinsic(intrinsic_name.trim_right_matches(\"_nonzero\"), num, kind)?\n-                } else {\n-                    numeric_intrinsic(intrinsic_name, num, kind)?\n-                };\n-                self.write_scalar(dest, num, ty)?;\n-            }\n-\n             \"discriminant_value\" => {\n-                let ty = substs.type_at(0);\n-                let layout = self.layout_of(ty)?;\n-                let adt_ptr = self.into_ptr(args[0].value)?;\n-                let adt_align = self.layout_of(args[0].ty)?.align;\n-                let place = Place::from_scalar_ptr(adt_ptr, adt_align);\n-                let discr_val = self.read_discriminant_value(place, layout)?;\n-                self.write_scalar(dest, Scalar::from_uint(discr_val, dest_layout.size), dest_layout.ty)?;\n+                let place = self.ref_to_mplace(self.read_value(args[0])?)?;\n+                let discr_val = self.read_discriminant(place.into())?.0;\n+                self.write_scalar(Scalar::from_uint(discr_val, dest.layout.size), dest)?;\n             }\n \n             \"sinf32\" | \"fabsf32\" | \"cosf32\" | \"sqrtf32\" | \"expf32\" | \"exp2f32\" | \"logf32\" |\n             \"log10f32\" | \"log2f32\" | \"floorf32\" | \"ceilf32\" | \"truncf32\" => {\n-                let f = self.value_to_scalar(args[0])?.to_bytes()?;\n+                let f = self.read_scalar(args[0])?.to_bytes()?;\n                 let f = f32::from_bits(f as u32);\n                 let f = match intrinsic_name {\n                     \"sinf32\" => f.sin(),\n@@ -272,12 +221,12 @@ impl<'a, 'mir, 'tcx> EvalContextExt<'tcx> for EvalContext<'a, 'mir, 'tcx, super:\n                     \"truncf32\" => f.trunc(),\n                     _ => bug!(),\n                 };\n-                self.write_scalar(dest, Scalar::from_f32(f), dest_layout.ty)?;\n+                self.write_scalar(Scalar::from_f32(f), dest)?;\n             }\n \n             \"sinf64\" | \"fabsf64\" | \"cosf64\" | \"sqrtf64\" | \"expf64\" | \"exp2f64\" | \"logf64\" |\n             \"log10f64\" | \"log2f64\" | \"floorf64\" | \"ceilf64\" | \"truncf64\" => {\n-                let f = self.value_to_scalar(args[0])?.to_bytes()?;\n+                let f = self.read_scalar(args[0])?.to_bytes()?;\n                 let f = f64::from_bits(f as u64);\n                 let f = match intrinsic_name {\n                     \"sinf64\" => f.sin(),\n@@ -294,13 +243,12 @@ impl<'a, 'mir, 'tcx> EvalContextExt<'tcx> for EvalContext<'a, 'mir, 'tcx, super:\n                     \"truncf64\" => f.trunc(),\n                     _ => bug!(),\n                 };\n-                self.write_scalar(dest, Scalar::from_f64(f), dest_layout.ty)?;\n+                self.write_scalar(Scalar::from_f64(f), dest)?;\n             }\n \n             \"fadd_fast\" | \"fsub_fast\" | \"fmul_fast\" | \"fdiv_fast\" | \"frem_fast\" => {\n-                let ty = substs.type_at(0);\n-                let a = self.value_to_scalar(args[0])?;\n-                let b = self.value_to_scalar(args[1])?;\n+                let a = self.read_value(args[0])?;\n+                let b = self.read_value(args[1])?;\n                 let op = match intrinsic_name {\n                     \"fadd_fast\" => mir::BinOp::Add,\n                     \"fsub_fast\" => mir::BinOp::Sub,\n@@ -309,346 +257,312 @@ impl<'a, 'mir, 'tcx> EvalContextExt<'tcx> for EvalContext<'a, 'mir, 'tcx, super:\n                     \"frem_fast\" => mir::BinOp::Rem,\n                     _ => bug!(),\n                 };\n-                let result = self.binary_op(op, a, ty, b, ty)?;\n-                self.write_scalar(dest, result.0, dest_layout.ty)?;\n+                let result = self.binary_op(op, a, b)?;\n+                self.write_scalar(result.0, dest)?;\n             }\n \n             \"exact_div\" => {\n                 // Performs an exact division, resulting in undefined behavior where\n                 // `x % y != 0` or `y == 0` or `x == T::min_value() && y == -1`\n-                let ty = substs.type_at(0);\n-                let a = self.value_to_scalar(args[0])?;\n-                let b = self.value_to_scalar(args[1])?;\n+                let a = self.read_value(args[0])?;\n+                let b = self.read_value(args[1])?;\n                 // check x % y != 0\n-                if !self.binary_op(mir::BinOp::Rem, a, ty, b, ty)?.0.is_null() {\n+                if !self.binary_op(mir::BinOp::Rem, a, b)?.0.is_null() {\n                     return err!(ValidationFailure(format!(\"exact_div: {:?} cannot be divided by {:?}\", a, b)));\n                 }\n-                let result = self.binary_op(mir::BinOp::Div, a, ty, b, ty)?;\n-                self.write_scalar(dest, result.0, dest_layout.ty)?;\n+                let result = self.binary_op(mir::BinOp::Div, a, b)?;\n+                self.write_scalar(result.0, dest)?;\n             },\n \n             \"likely\" | \"unlikely\" | \"forget\" => {}\n \n             \"init\" => {\n-                // we don't want to force an allocation in case the destination is a simple value\n-                match dest {\n-                    Place::Local { frame, local } => {\n-                        match self.stack()[frame].locals[local].access()? {\n-                            Value::ByRef(ptr, _) => {\n-                                // These writes have no alignment restriction anyway.\n-                                self.memory.write_repeat(ptr, 0, dest_layout.size)?;\n-                            }\n-                            Value::Scalar(_) => self.write_value(ValTy { value: Value::Scalar(Scalar::null(dest_layout.size).into()), ty: dest_layout.ty }, dest)?,\n-                            Value::ScalarPair(..) => {\n-                                self.write_value(ValTy { value: Value::ScalarPair(Scalar::null(dest_layout.size).into(), Scalar::null(dest_layout.size).into()), ty: dest_layout.ty }, dest)?;\n-                            }\n+                // Check fast path: we don't want to force an allocation in case the destination is a simple value,\n+                // but we also do not want to create a new allocation with 0s and then copy that over.\n+                if !dest.layout.is_zst() { // notzhing to do for ZST\n+                    match dest.layout.abi {\n+                        layout::Abi::Scalar(ref s) => {\n+                            let x = Scalar::null(s.value.size(&self));\n+                            self.write_value(Value::Scalar(x.into()), dest)?;\n+                        }\n+                        layout::Abi::ScalarPair(ref s1, ref s2) => {\n+                            let x = Scalar::null(s1.value.size(&self));\n+                            let y = Scalar::null(s2.value.size(&self));\n+                            self.write_value(Value::ScalarPair(x.into(), y.into()), dest)?;\n+                        }\n+                        _ => {\n+                            // Do it in memory\n+                            let mplace = self.force_allocation(dest)?;\n+                            assert!(mplace.extra.is_none());\n+                            self.memory.write_repeat(mplace.ptr, 0, dest.layout.size)?;\n                         }\n-                    },\n-                    Place::Ptr {\n-                        ptr,\n-                        align: _align,\n-                        extra: PlaceExtra::None,\n-                    } => self.memory.write_repeat(ptr.unwrap_or_err()?, 0, dest_layout.size)?,\n-                    Place::Ptr { .. } => {\n-                        bug!(\"init intrinsic tried to write to fat or unaligned ptr target\")\n                     }\n                 }\n             }\n \n-            \"min_align_of\" => {\n-                let elem_ty = substs.type_at(0);\n-                let elem_align = self.layout_of(elem_ty)?.align.abi();\n-                let ptr_size = self.memory.pointer_size();\n-                let align_val = Scalar::from_uint(elem_align as u128, ptr_size);\n-                self.write_scalar(dest, align_val, dest_layout.ty)?;\n-            }\n-\n             \"pref_align_of\" => {\n                 let ty = substs.type_at(0);\n                 let layout = self.layout_of(ty)?;\n                 let align = layout.align.pref();\n                 let ptr_size = self.memory.pointer_size();\n                 let align_val = Scalar::from_uint(align as u128, ptr_size);\n-                self.write_scalar(dest, align_val, dest_layout.ty)?;\n+                self.write_scalar(align_val, dest)?;\n             }\n \n             \"move_val_init\" => {\n-                let ty = substs.type_at(0);\n-                let ptr = self.into_ptr(args[0].value)?.unwrap_or_err()?;\n-                let align = self.layout_of(args[0].ty)?.align;\n-                self.write_value_to_ptr(args[1].value, ptr, align, ty)?;\n+                let ptr = self.ref_to_mplace(self.read_value(args[0])?)?;\n+                self.copy_op(args[1], ptr.into())?;\n             }\n \n             \"needs_drop\" => {\n                 let ty = substs.type_at(0);\n                 let env = ty::ParamEnv::reveal_all();\n                 let needs_drop = ty.needs_drop(self.tcx.tcx, env);\n                 self.write_scalar(\n-                    dest,\n                     Scalar::from_bool(needs_drop),\n-                    dest_layout.ty,\n+                    dest,\n                 )?;\n             }\n \n             \"offset\" => {\n-                let offset = self.value_to_isize(args[1])?;\n-                let ptr = self.into_ptr(args[0].value)?.unwrap_or_err()?;\n-                let result_ptr = self.pointer_offset(ptr, substs.type_at(0), offset)?;\n-                self.write_ptr(dest, result_ptr, dest_layout.ty)?;\n+                let offset = self.read_scalar(args[1])?.to_isize(&self)?;\n+                let ptr = self.read_scalar(args[0])?.not_undef()?;\n+                let result_ptr = self.pointer_offset_inbounds(ptr, substs.type_at(0), offset)?;\n+                self.write_scalar(result_ptr, dest)?;\n             }\n \n             \"overflowing_sub\" => {\n-                self.intrinsic_overflowing(\n+                let l = self.read_value(args[0])?;\n+                let r = self.read_value(args[1])?;\n+                self.binop_ignore_overflow(\n                     mir::BinOp::Sub,\n-                    args[0],\n-                    args[1],\n+                    l,\n+                    r,\n                     dest,\n-                    dest_layout.ty,\n                 )?;\n             }\n \n             \"overflowing_mul\" => {\n-                self.intrinsic_overflowing(\n+                let l = self.read_value(args[0])?;\n+                let r = self.read_value(args[1])?;\n+                self.binop_ignore_overflow(\n                     mir::BinOp::Mul,\n-                    args[0],\n-                    args[1],\n+                    r,\n+                    l,\n                     dest,\n-                    dest_layout.ty,\n                 )?;\n             }\n \n             \"overflowing_add\" => {\n-                self.intrinsic_overflowing(\n+                let l = self.read_value(args[0])?;\n+                let r = self.read_value(args[1])?;\n+                self.binop_ignore_overflow(\n                     mir::BinOp::Add,\n-                    args[0],\n-                    args[1],\n+                    r,\n+                    l,\n                     dest,\n-                    dest_layout.ty,\n                 )?;\n             }\n \n             \"powf32\" => {\n-                let f = self.value_to_scalar(args[0])?.to_bits(Size::from_bits(32))?;\n+                let f = self.read_scalar(args[0])?.to_bits(Size::from_bits(32))?;\n                 let f = f32::from_bits(f as u32);\n-                let f2 = self.value_to_scalar(args[1])?.to_bits(Size::from_bits(32))?;\n+                let f2 = self.read_scalar(args[1])?.to_bits(Size::from_bits(32))?;\n                 let f2 = f32::from_bits(f2 as u32);\n                 self.write_scalar(\n-                    dest,\n                     Scalar::from_f32(f.powf(f2)),\n-                    dest_layout.ty,\n+                    dest,\n                 )?;\n             }\n \n             \"powf64\" => {\n-                let f = self.value_to_scalar(args[0])?.to_bits(Size::from_bits(64))?;\n+                let f = self.read_scalar(args[0])?.to_bits(Size::from_bits(64))?;\n                 let f = f64::from_bits(f as u64);\n-                let f2 = self.value_to_scalar(args[1])?.to_bits(Size::from_bits(64))?;\n+                let f2 = self.read_scalar(args[1])?.to_bits(Size::from_bits(64))?;\n                 let f2 = f64::from_bits(f2 as u64);\n                 self.write_scalar(\n-                    dest,\n                     Scalar::from_f64(f.powf(f2)),\n-                    dest_layout.ty,\n+                    dest,\n                 )?;\n             }\n \n             \"fmaf32\" => {\n-                let a = self.value_to_scalar(args[0])?.to_bits(Size::from_bits(32))?;\n+                let a = self.read_scalar(args[0])?.to_bits(Size::from_bits(32))?;\n                 let a = f32::from_bits(a as u32);\n-                let b = self.value_to_scalar(args[1])?.to_bits(Size::from_bits(32))?;\n+                let b = self.read_scalar(args[1])?.to_bits(Size::from_bits(32))?;\n                 let b = f32::from_bits(b as u32);\n-                let c = self.value_to_scalar(args[2])?.to_bits(Size::from_bits(32))?;\n+                let c = self.read_scalar(args[2])?.to_bits(Size::from_bits(32))?;\n                 let c = f32::from_bits(c as u32);\n                 self.write_scalar(\n-                    dest,\n                     Scalar::from_f32(a * b + c),\n-                    dest_layout.ty,\n+                    dest,\n                 )?;\n             }\n \n             \"fmaf64\" => {\n-                let a = self.value_to_scalar(args[0])?.to_bits(Size::from_bits(64))?;\n+                let a = self.read_scalar(args[0])?.to_bits(Size::from_bits(64))?;\n                 let a = f64::from_bits(a as u64);\n-                let b = self.value_to_scalar(args[1])?.to_bits(Size::from_bits(64))?;\n+                let b = self.read_scalar(args[1])?.to_bits(Size::from_bits(64))?;\n                 let b = f64::from_bits(b as u64);\n-                let c = self.value_to_scalar(args[2])?.to_bits(Size::from_bits(64))?;\n+                let c = self.read_scalar(args[2])?.to_bits(Size::from_bits(64))?;\n                 let c = f64::from_bits(c as u64);\n                 self.write_scalar(\n-                    dest,\n                     Scalar::from_f64(a * b + c),\n-                    dest_layout.ty,\n+                    dest,\n                 )?;\n             }\n \n             \"powif32\" => {\n-                let f = self.value_to_scalar(args[0])?.to_bits(Size::from_bits(32))?;\n+                let f = self.read_scalar(args[0])?.to_bits(Size::from_bits(32))?;\n                 let f = f32::from_bits(f as u32);\n-                let i = self.value_to_i32(args[1])?;\n+                let i = self.read_scalar(args[1])?.to_i32()?;\n                 self.write_scalar(\n-                    dest,\n                     Scalar::from_f32(f.powi(i)),\n-                    dest_layout.ty,\n+                    dest,\n                 )?;\n             }\n \n             \"powif64\" => {\n-                let f = self.value_to_scalar(args[0])?.to_bits(Size::from_bits(64))?;\n+                let f = self.read_scalar(args[0])?.to_bits(Size::from_bits(64))?;\n                 let f = f64::from_bits(f as u64);\n-                let i = self.value_to_i32(args[1])?;\n+                let i = self.read_scalar(args[1])?.to_i32()?;\n                 self.write_scalar(\n-                    dest,\n                     Scalar::from_f64(f.powi(i)),\n-                    dest_layout.ty,\n+                    dest,\n                 )?;\n             }\n \n-            \"size_of\" => {\n-                let ty = substs.type_at(0);\n-                let size = self.layout_of(ty)?.size.bytes();\n-                let ptr_size = self.memory.pointer_size();\n-                self.write_scalar(dest, Scalar::from_uint(size, ptr_size), dest_layout.ty)?;\n-            }\n-\n             \"size_of_val\" => {\n-                let ty = substs.type_at(0);\n-                let (size, _) = self.size_and_align_of_dst(ty, args[0].value)?;\n+                let mplace = self.ref_to_mplace(self.read_value(args[0])?)?;\n+                let (size, _) = self.size_and_align_of_mplace(mplace)?;\n                 let ptr_size = self.memory.pointer_size();\n                 self.write_scalar(\n-                    dest,\n                     Scalar::from_uint(size.bytes() as u128, ptr_size),\n-                    dest_layout.ty,\n+                    dest,\n                 )?;\n             }\n \n             \"min_align_of_val\" |\n             \"align_of_val\" => {\n-                let ty = substs.type_at(0);\n-                let (_, align) = self.size_and_align_of_dst(ty, args[0].value)?;\n+                let mplace = self.ref_to_mplace(self.read_value(args[0])?)?;\n+                let (_, align) = self.size_and_align_of_mplace(mplace)?;\n                 let ptr_size = self.memory.pointer_size();\n                 self.write_scalar(\n-                    dest,\n                     Scalar::from_uint(align.abi(), ptr_size),\n-                    dest_layout.ty,\n+                    dest,\n                 )?;\n             }\n \n             \"type_name\" => {\n                 let ty = substs.type_at(0);\n                 let ty_name = ty.to_string();\n                 let value = self.str_to_value(&ty_name)?;\n-                self.write_value(ValTy { value, ty: dest_layout.ty }, dest)?;\n-            }\n-            \"type_id\" => {\n-                let ty = substs.type_at(0);\n-                let n = self.tcx.type_id_hash(ty);\n-                self.write_scalar(dest, Scalar::Bits { bits: n as u128, size: 8 }, dest_layout.ty)?;\n+                self.write_value(value, dest)?;\n             }\n \n             \"transmute\" => {\n-                let src_ty = substs.type_at(0);\n-                let _src_align = self.layout_of(src_ty)?.align;\n-                let ptr = self.force_allocation(dest)?.to_ptr()?;\n-                let dest_align = self.layout_of(substs.type_at(1))?.align;\n-                self.write_value_to_ptr(args[0].value, ptr.into(), dest_align, src_ty).unwrap();\n+                // Go through an allocation, to make sure the completely different layouts\n+                // do not pose a problem.  (When the user transmutes through a union,\n+                // there will not be a layout mismatch.)\n+                let dest = self.force_allocation(dest)?;\n+                self.copy_op(args[0], dest.into())?;\n             }\n \n             \"unchecked_shl\" => {\n-                let bits = dest_layout.size.bytes() as u128 * 8;\n-                let rhs = self.value_to_scalar(args[1])?\n-                    .to_bytes()?;\n-                if rhs >= bits {\n+                let bits = dest.layout.size.bytes() as u128 * 8;\n+                let l = self.read_value(args[0])?;\n+                let r = self.read_value(args[1])?;\n+                let rval = r.to_scalar()?.to_bytes()?;\n+                if rval >= bits {\n                     return err!(Intrinsic(\n-                        format!(\"Overflowing shift by {} in unchecked_shl\", rhs),\n+                        format!(\"Overflowing shift by {} in unchecked_shl\", rval),\n                     ));\n                 }\n-                self.intrinsic_overflowing(\n+                self.binop_ignore_overflow(\n                     mir::BinOp::Shl,\n-                    args[0],\n-                    args[1],\n+                    l,\n+                    r,\n                     dest,\n-                    dest_layout.ty,\n                 )?;\n             }\n \n             \"unchecked_shr\" => {\n-                let bits = dest_layout.size.bytes() as u128 * 8;\n-                let rhs = self.value_to_scalar(args[1])?\n-                    .to_bytes()?;\n-                if rhs >= bits {\n+                let bits = dest.layout.size.bytes() as u128 * 8;\n+                let l = self.read_value(args[0])?;\n+                let r = self.read_value(args[1])?;\n+                let rval = r.to_scalar()?.to_bytes()?;\n+                if rval >= bits {\n                     return err!(Intrinsic(\n-                        format!(\"Overflowing shift by {} in unchecked_shr\", rhs),\n+                        format!(\"Overflowing shift by {} in unchecked_shr\", rval),\n                     ));\n                 }\n-                self.intrinsic_overflowing(\n+                self.binop_ignore_overflow(\n                     mir::BinOp::Shr,\n-                    args[0],\n-                    args[1],\n+                    l,\n+                    r,\n                     dest,\n-                    dest_layout.ty,\n                 )?;\n             }\n \n             \"unchecked_div\" => {\n-                let rhs = self.value_to_scalar(args[1])?\n-                    .to_bytes()?;\n-                if rhs == 0 {\n+                let l = self.read_value(args[0])?;\n+                let r = self.read_value(args[1])?;\n+                let rval = r.to_scalar()?.to_bytes()?;\n+                if rval == 0 {\n                     return err!(Intrinsic(format!(\"Division by 0 in unchecked_div\")));\n                 }\n-                self.intrinsic_overflowing(\n+                self.binop_ignore_overflow(\n                     mir::BinOp::Div,\n-                    args[0],\n-                    args[1],\n+                    l,\n+                    r,\n                     dest,\n-                    dest_layout.ty,\n                 )?;\n             }\n \n             \"unchecked_rem\" => {\n-                let rhs = self.value_to_scalar(args[1])?\n-                    .to_bytes()?;\n-                if rhs == 0 {\n+                let l = self.read_value(args[0])?;\n+                let r = self.read_value(args[1])?;\n+                let rval = r.to_scalar()?.to_bytes()?;\n+                if rval == 0 {\n                     return err!(Intrinsic(format!(\"Division by 0 in unchecked_rem\")));\n                 }\n-                self.intrinsic_overflowing(\n+                self.binop_ignore_overflow(\n                     mir::BinOp::Rem,\n-                    args[0],\n-                    args[1],\n+                    l,\n+                    r,\n                     dest,\n-                    dest_layout.ty,\n                 )?;\n             }\n \n             \"uninit\" => {\n-                // we don't want to force an allocation in case the destination is a simple value\n-                match dest {\n-                    Place::Local { frame, local } => {\n-                        match self.stack()[frame].locals[local].access()? {\n-                            Value::ByRef(ptr, _) => {\n-                                // These writes have no alignment restriction anyway.\n-                                self.memory.mark_definedness(ptr, dest_layout.size, false)?;\n-                            }\n-                            Value::Scalar(_) => self.write_value(ValTy { value: Value::Scalar(ScalarMaybeUndef::Undef), ty: dest_layout.ty }, dest)?,\n-                            Value::ScalarPair(..) => {\n-                                self.write_value(ValTy { value: Value::ScalarPair(ScalarMaybeUndef::Undef, ScalarMaybeUndef::Undef), ty: dest_layout.ty }, dest)?;\n-                            }\n+                // Check fast path: we don't want to force an allocation in case the destination is a simple value,\n+                // but we also do not want to create a new allocation with 0s and then copy that over.\n+                if !dest.layout.is_zst() { // nothing to do for ZST\n+                    match dest.layout.abi {\n+                        layout::Abi::Scalar(..) => {\n+                            let x = ScalarMaybeUndef::Undef;\n+                            self.write_value(Value::Scalar(x), dest)?;\n+                        }\n+                        layout::Abi::ScalarPair(..) => {\n+                            let x = ScalarMaybeUndef::Undef;\n+                            self.write_value(Value::ScalarPair(x, x), dest)?;\n+                        }\n+                        _ => {\n+                            // Do it in memory\n+                            let mplace = self.force_allocation(dest)?;\n+                            assert!(mplace.extra.is_none());\n+                            self.memory.mark_definedness(mplace.ptr.to_ptr()?, dest.layout.size, false)?;\n                         }\n-                    },\n-                    Place::Ptr {\n-                        ptr,\n-                        align: _align,\n-                        extra: PlaceExtra::None,\n-                    } => self.memory.mark_definedness(ptr.unwrap_or_err()?, dest_layout.size, false)?,\n-                    Place::Ptr { .. } => {\n-                        bug!(\"uninit intrinsic tried to write to fat or unaligned ptr target\")\n                     }\n                 }\n             }\n \n             \"write_bytes\" => {\n                 let ty = substs.type_at(0);\n                 let ty_layout = self.layout_of(ty)?;\n-                let val_byte = self.value_to_u8(args[1])?;\n-                let ptr = self.into_ptr(args[0].value)?.unwrap_or_err()?;\n-                let count = self.value_to_usize(args[2])?;\n+                let val_byte = self.read_scalar(args[1])?.to_u8()?;\n+                let ptr = self.read_scalar(args[0])?.not_undef()?;\n+                let count = self.read_scalar(args[2])?.to_usize(&self)?;\n                 if count > 0 {\n                     // HashMap relies on write_bytes on a NULL ptr with count == 0 to work\n                     // TODO: Should we, at least, validate the alignment? (Also see the copy intrinsic)\n@@ -660,47 +574,6 @@ impl<'a, 'mir, 'tcx> EvalContextExt<'tcx> for EvalContext<'a, 'mir, 'tcx, super:\n             name => return err!(Unimplemented(format!(\"unimplemented intrinsic: {}\", name))),\n         }\n \n-        self.goto_block(target);\n-\n-        // Since we pushed no stack frame, the main loop will act\n-        // as if the call just completed and it's returning to the\n-        // current frame.\n         Ok(())\n     }\n }\n-\n-fn numeric_intrinsic<'tcx>(\n-    name: &str,\n-    bytes: u128,\n-    kind: Primitive,\n-) -> EvalResult<'tcx, Scalar> {\n-    macro_rules! integer_intrinsic {\n-        ($method:ident) => ({\n-            let (result_bytes, size) = match kind {\n-                Primitive::Int(I8, true) => ((bytes as i8).$method() as u128, 1),\n-                Primitive::Int(I8, false) => ((bytes as u8).$method() as u128, 1),\n-                Primitive::Int(I16, true) => ((bytes as i16).$method() as u128, 2),\n-                Primitive::Int(I16, false) => ((bytes as u16).$method() as u128, 2),\n-                Primitive::Int(I32, true) => ((bytes as i32).$method() as u128, 4),\n-                Primitive::Int(I32, false) => ((bytes as u32).$method() as u128, 4),\n-                Primitive::Int(I64, true) => ((bytes as i64).$method() as u128, 8),\n-                Primitive::Int(I64, false) => ((bytes as u64).$method() as u128, 8),\n-                Primitive::Int(I128, true) => ((bytes as i128).$method() as u128, 16),\n-                Primitive::Int(I128, false) => (bytes.$method() as u128, 16),\n-                _ => bug!(\"invalid `{}` argument: {:?}\", name, bytes),\n-            };\n-\n-            Scalar::from_uint(result_bytes, Size::from_bytes(size))\n-        });\n-    }\n-\n-    let result_val = match name {\n-        \"bswap\" => integer_intrinsic!(swap_bytes),\n-        \"ctlz\" => integer_intrinsic!(leading_zeros),\n-        \"ctpop\" => integer_intrinsic!(count_ones),\n-        \"cttz\" => integer_intrinsic!(trailing_zeros),\n-        _ => bug!(\"not a numeric intrinsic: {}\", name),\n-    };\n-\n-    Ok(result_val)\n-}"}, {"sha": "42dac6a28d855a061e2287d8e63d1d477bfeb967", "filename": "src/lib.rs", "status": "modified", "additions": 88, "deletions": 300, "changes": 388, "blob_url": "https://github.com/rust-lang/rust/blob/752accf4e48b83545fa2fcf205602f239389361d/src%2Flib.rs", "raw_url": "https://github.com/rust-lang/rust/raw/752accf4e48b83545fa2fcf205602f239389361d/src%2Flib.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flib.rs?ref=752accf4e48b83545fa2fcf205602f239389361d", "patch": "@@ -1,7 +1,4 @@\n-#![feature(\n-    rustc_private,\n-    catch_expr,\n-)]\n+#![feature(rustc_private)]\n \n #![cfg_attr(feature = \"cargo-clippy\", allow(cast_lossless))]\n \n@@ -16,17 +13,17 @@ extern crate rustc_mir;\n extern crate rustc_target;\n extern crate syntax;\n \n-use rustc::ty::{self, TyCtxt};\n+use rustc::ty::{self, TyCtxt, query::TyCtxtAt};\n use rustc::ty::layout::{TyLayout, LayoutOf, Size};\n-use rustc::ty::subst::Subst;\n use rustc::hir::def_id::DefId;\n use rustc::mir;\n \n use rustc_data_structures::fx::FxHasher;\n \n use syntax::ast::Mutability;\n-use syntax::codemap::Span;\n+use syntax::attr;\n \n+use std::marker::PhantomData;\n use std::collections::{HashMap, BTreeMap};\n use std::hash::{Hash, Hasher};\n \n@@ -41,97 +38,30 @@ mod memory;\n mod tls;\n mod locks;\n mod range_map;\n-mod validation;\n \n use fn_call::EvalContextExt as MissingFnsEvalContextExt;\n use operator::EvalContextExt as OperatorEvalContextExt;\n use intrinsic::EvalContextExt as IntrinsicEvalContextExt;\n use tls::EvalContextExt as TlsEvalContextExt;\n+use memory::MemoryKind as MiriMemoryKind;\n use locks::LockInfo;\n-use locks::MemoryExt as LockMemoryExt;\n-use validation::EvalContextExt as ValidationEvalContextExt;\n use range_map::RangeMap;\n-use validation::{ValidationQuery, AbsPlace};\n-\n-pub trait ScalarExt {\n-    fn null(size: Size) -> Self;\n-    fn from_i32(i: i32) -> Self;\n-    fn from_uint(i: impl Into<u128>, ptr_size: Size) -> Self;\n-    fn from_int(i: impl Into<i128>, ptr_size: Size) -> Self;\n-    fn from_f32(f: f32) -> Self;\n-    fn from_f64(f: f64) -> Self;\n-    fn to_usize<'a, 'mir, 'tcx>(self, ecx: &rustc_mir::interpret::EvalContext<'a, 'mir, 'tcx, Evaluator<'tcx>>) -> EvalResult<'static, u64>;\n-    fn is_null(self) -> bool;\n-    /// HACK: this function just extracts all bits if `defined != 0`\n-    /// Mainly used for args of C-functions and we should totally correctly fetch the size\n-    /// of their arguments\n-    fn to_bytes(self) -> EvalResult<'static, u128>;\n-}\n-\n-impl ScalarExt for Scalar {\n-    fn null(size: Size) -> Self {\n-        Scalar::Bits { bits: 0, size: size.bytes() as u8 }\n-    }\n-\n-    fn from_i32(i: i32) -> Self {\n-        Scalar::Bits { bits: i as u32 as u128, size: 4 }\n-    }\n-\n-    fn from_uint(i: impl Into<u128>, ptr_size: Size) -> Self {\n-        Scalar::Bits { bits: i.into(), size: ptr_size.bytes() as u8 }\n-    }\n-\n-    fn from_int(i: impl Into<i128>, ptr_size: Size) -> Self {\n-        Scalar::Bits { bits: i.into() as u128, size: ptr_size.bytes() as u8 }\n-    }\n-\n-    fn from_f32(f: f32) -> Self {\n-        Scalar::Bits { bits: f.to_bits() as u128, size: 4 }\n-    }\n-\n-    fn from_f64(f: f64) -> Self {\n-        Scalar::Bits { bits: f.to_bits() as u128, size: 8 }\n-    }\n-\n-    fn to_usize<'a, 'mir, 'tcx>(self, ecx: &rustc_mir::interpret::EvalContext<'a, 'mir, 'tcx, Evaluator<'tcx>>) -> EvalResult<'static, u64> {\n-        let b = self.to_bits(ecx.memory.pointer_size())?;\n-        assert_eq!(b as u64 as u128, b);\n-        Ok(b as u64)\n-    }\n-\n-    fn is_null(self) -> bool {\n-        match self {\n-            Scalar::Bits { bits, .. } => bits == 0,\n-            Scalar::Ptr(_) => false\n-        }\n-    }\n-\n-    fn to_bytes(self) -> EvalResult<'static, u128> {\n-        match self {\n-            Scalar::Bits { bits, size } => {\n-                assert_ne!(size, 0);\n-                Ok(bits)\n-            },\n-            Scalar::Ptr(_) => err!(ReadPointerAsBytes),\n-        }\n-    }\n-}\n+use helpers::{ScalarExt, FalibleScalarExt};\n \n pub fn create_ecx<'a, 'mir: 'a, 'tcx: 'mir>(\n     tcx: TyCtxt<'a, 'tcx, 'tcx>,\n     main_id: DefId,\n     start_wrapper: Option<DefId>,\n-) -> EvalResult<'tcx, (EvalContext<'a, 'mir, 'tcx, Evaluator<'tcx>>, Option<Pointer>)> {\n+) -> EvalResult<'tcx, EvalContext<'a, 'mir, 'tcx, Evaluator<'tcx>>> {\n     let mut ecx = EvalContext::new(\n-        tcx.at(syntax::codemap::DUMMY_SP),\n+        tcx.at(syntax::source_map::DUMMY_SP),\n         ty::ParamEnv::reveal_all(),\n         Default::default(),\n         MemoryData::new()\n     );\n \n     let main_instance = ty::Instance::mono(ecx.tcx.tcx, main_id);\n     let main_mir = ecx.load_mir(main_instance.def)?;\n-    let mut cleanup_ptr = None; // Scalar to be deallocated when we are done\n \n     if !main_mir.return_ty().is_nil() || main_mir.arg_count != 0 {\n         return err!(Unimplemented(\n@@ -160,51 +90,41 @@ pub fn create_ecx<'a, 'mir: 'a, 'tcx: 'mir>(\n             )));\n         }\n \n-        // Return value\n+        // Return value (in static memory so that it does not count as leak)\n         let size = ecx.tcx.data_layout.pointer_size;\n         let align = ecx.tcx.data_layout.pointer_align;\n-        let ret_ptr = ecx.memory_mut().allocate(size, align, MemoryKind::Stack)?;\n-        cleanup_ptr = Some(ret_ptr);\n+        let ret_ptr = ecx.memory_mut().allocate(size, align, MiriMemoryKind::MutStatic.into())?;\n \n         // Push our stack frame\n         ecx.push_stack_frame(\n             start_instance,\n             start_mir.span,\n             start_mir,\n             Place::from_ptr(ret_ptr, align),\n-            StackPopCleanup::None,\n+            StackPopCleanup::None { cleanup: true },\n         )?;\n \n         let mut args = ecx.frame().mir.args_iter();\n \n         // First argument: pointer to main()\n         let main_ptr = ecx.memory_mut().create_fn_alloc(main_instance);\n         let dest = ecx.eval_place(&mir::Place::Local(args.next().unwrap()))?;\n-        let main_ty = main_instance.ty(ecx.tcx.tcx);\n-        let main_ptr_ty = ecx.tcx.mk_fn_ptr(main_ty.fn_sig(ecx.tcx.tcx));\n-        ecx.write_value(\n-            ValTy {\n-                value: Value::Scalar(Scalar::Ptr(main_ptr).into()),\n-                ty: main_ptr_ty,\n-            },\n-            dest,\n-        )?;\n+        ecx.write_scalar(Scalar::Ptr(main_ptr), dest)?;\n \n         // Second argument (argc): 1\n         let dest = ecx.eval_place(&mir::Place::Local(args.next().unwrap()))?;\n-        let ty = ecx.tcx.types.isize;\n-        ecx.write_scalar(dest, Scalar::from_int(1, ptr_size), ty)?;\n+        ecx.write_scalar(Scalar::from_int(1, dest.layout.size), dest)?;\n \n         // FIXME: extract main source file path\n         // Third argument (argv): &[b\"foo\"]\n         let dest = ecx.eval_place(&mir::Place::Local(args.next().unwrap()))?;\n-        let ty = ecx.tcx.mk_imm_ptr(ecx.tcx.mk_imm_ptr(ecx.tcx.types.u8));\n-        let foo = ecx.memory.allocate_bytes(b\"foo\\0\");\n-        let ptr_align = ecx.tcx.data_layout.pointer_align;\n-        let foo_ptr = ecx.memory.allocate(ptr_size, ptr_align, MemoryKind::Stack)?;\n-        ecx.memory.write_scalar(foo_ptr.into(), ptr_align, Scalar::Ptr(foo).into(), ptr_size, ptr_align, false)?;\n-        ecx.memory.mark_static_initialized(foo_ptr.alloc_id, Mutability::Immutable)?;\n-        ecx.write_ptr(dest, foo_ptr.into(), ty)?;\n+        let foo = ecx.memory.allocate_static_bytes(b\"foo\\0\");\n+        let foo_ty = ecx.tcx.mk_imm_ptr(ecx.tcx.types.u8);\n+        let foo_layout = ecx.layout_of(foo_ty)?;\n+        let foo_place = ecx.allocate(foo_layout, MemoryKind::Stack)?; // will be interned in just a second\n+        ecx.write_scalar(Scalar::Ptr(foo), foo_place.into())?;\n+        ecx.memory.intern_static(foo_place.to_ptr()?.alloc_id, Mutability::Immutable)?;\n+        ecx.write_scalar(foo_place.ptr, dest)?;\n \n         assert!(args.next().is_none(), \"start lang item has more arguments than expected\");\n     } else {\n@@ -213,42 +133,38 @@ pub fn create_ecx<'a, 'mir: 'a, 'tcx: 'mir>(\n             main_mir.span,\n             main_mir,\n             Place::from_scalar_ptr(Scalar::from_int(1, ptr_size).into(), ty::layout::Align::from_bytes(1, 1).unwrap()),\n-            StackPopCleanup::None,\n+            StackPopCleanup::None { cleanup: true },\n         )?;\n \n         // No arguments\n         let mut args = ecx.frame().mir.args_iter();\n         assert!(args.next().is_none(), \"main function must not have arguments\");\n     }\n \n-    Ok((ecx, cleanup_ptr))\n+    Ok(ecx)\n }\n \n pub fn eval_main<'a, 'tcx: 'a>(\n     tcx: TyCtxt<'a, 'tcx, 'tcx>,\n     main_id: DefId,\n     start_wrapper: Option<DefId>,\n ) {\n-    let (mut ecx, cleanup_ptr) = create_ecx(tcx, main_id, start_wrapper).expect(\"Couldn't create ecx\");\n-\n-    let res: EvalResult = do catch {\n-        while ecx.step()? {}\n-        ecx.run_tls_dtors()?;\n-        if let Some(cleanup_ptr) = cleanup_ptr {\n-            ecx.memory_mut().deallocate(\n-                cleanup_ptr,\n-                None,\n-                MemoryKind::Stack,\n-            )?;\n-        }\n-    };\n+    let mut ecx = create_ecx(tcx, main_id, start_wrapper).expect(\"Couldn't create ecx\");\n+\n+    let res: EvalResult = (|| {\n+        ecx.run()?;\n+        ecx.run_tls_dtors()\n+    })();\n \n     match res {\n         Ok(()) => {\n             let leaks = ecx.memory().leak_report();\n-            if leaks != 0 {\n-                // TODO: Prevent leaks which aren't supposed to be there\n-                //tcx.sess.err(\"the evaluated program leaked memory\");\n+            // Disable the leak test on some platforms where we likely do not\n+            // correctly implement TLS destructors.\n+            let target_os = ecx.tcx.tcx.sess.target.target.target_os.to_lowercase();\n+            let ignore_leaks = target_os == \"windows\" || target_os == \"macos\";\n+            if !ignore_leaks && leaks != 0 {\n+                tcx.sess.err(\"the evaluated program leaked memory\");\n             }\n         }\n         Err(e) => {\n@@ -273,6 +189,7 @@ pub fn eval_main<'a, 'tcx: 'a>(\n                 ecx.tcx.sess.err(&e.to_string());\n             }\n \n+            /* Nice try, but with MIRI_BACKTRACE this shows 100s of backtraces.\n             for (i, frame) in ecx.stack().iter().enumerate() {\n                 trace!(\"-------------------\");\n                 trace!(\"Frame {}\", i);\n@@ -282,7 +199,7 @@ pub fn eval_main<'a, 'tcx: 'a>(\n                         trace!(\"    local {}: {:?}\", i, local);\n                     }\n                 }\n-            }\n+            }*/\n         }\n     }\n }\n@@ -293,15 +210,15 @@ pub struct Evaluator<'tcx> {\n     /// Miri does not expose env vars from the host to the emulated program\n     pub(crate) env_vars: HashMap<Vec<u8>, Pointer>,\n \n-    /// Places that were suspended by the validation subsystem, and will be recovered later\n-    pub(crate) suspended: HashMap<DynamicLifetime, Vec<ValidationQuery<'tcx>>>,\n+    /// Use the lifetime\n+    _dummy : PhantomData<&'tcx ()>,\n }\n \n impl<'tcx> Hash for Evaluator<'tcx> {\n     fn hash<H: Hasher>(&self, state: &mut H) {\n         let Evaluator {\n             env_vars,\n-            suspended: _,\n+            _dummy: _,\n         } = self;\n \n         env_vars.iter()\n@@ -337,8 +254,6 @@ pub struct MemoryData<'tcx> {\n     /// Only mutable (static mut, heap, stack) allocations have an entry in this map.\n     /// The entry is created when allocating the memory and deleted after deallocation.\n     locks: HashMap<AllocId, RangeMap<LockInfo<'tcx>>>,\n-\n-    statics: HashMap<GlobalId<'tcx>, AllocId>,\n }\n \n impl<'tcx> MemoryData<'tcx> {\n@@ -347,7 +262,6 @@ impl<'tcx> MemoryData<'tcx> {\n             next_thread_local: 1, // start with 1 as we must not use 0 on Windows\n             thread_local: BTreeMap::new(),\n             locks: HashMap::new(),\n-            statics: HashMap::new(),\n         }\n     }\n }\n@@ -358,134 +272,54 @@ impl<'tcx> Hash for MemoryData<'tcx> {\n             next_thread_local: _,\n             thread_local,\n             locks: _,\n-            statics: _,\n         } = self;\n \n         thread_local.hash(state);\n     }\n }\n \n-impl<'mir, 'tcx: 'mir> Machine<'mir, 'tcx> for Evaluator<'tcx> {\n+impl<'mir, 'tcx> Machine<'mir, 'tcx> for Evaluator<'tcx> {\n     type MemoryData = MemoryData<'tcx>;\n     type MemoryKinds = memory::MemoryKind;\n \n+    const MUT_STATIC_KIND: Option<memory::MemoryKind> = Some(memory::MemoryKind::MutStatic);\n+\n     /// Returns Ok() when the function was handled, fail otherwise\n-    fn eval_fn_call<'a>(\n+    fn find_fn<'a>(\n         ecx: &mut EvalContext<'a, 'mir, 'tcx, Self>,\n         instance: ty::Instance<'tcx>,\n-        destination: Option<(Place, mir::BasicBlock)>,\n-        args: &[ValTy<'tcx>],\n-        span: Span,\n-        sig: ty::FnSig<'tcx>,\n-    ) -> EvalResult<'tcx, bool> {\n-        ecx.eval_fn_call(instance, destination, args, span, sig)\n+        args: &[OpTy<'tcx>],\n+        dest: Option<PlaceTy<'tcx>>,\n+        ret: Option<mir::BasicBlock>,\n+    ) -> EvalResult<'tcx, Option<&'mir mir::Mir<'tcx>>> {\n+        ecx.find_fn(instance, args, dest, ret)\n     }\n \n     fn call_intrinsic<'a>(\n         ecx: &mut rustc_mir::interpret::EvalContext<'a, 'mir, 'tcx, Self>,\n         instance: ty::Instance<'tcx>,\n-        args: &[ValTy<'tcx>],\n-        dest: Place,\n-        dest_layout: TyLayout<'tcx>,\n-        target: mir::BasicBlock,\n+        args: &[OpTy<'tcx>],\n+        dest: PlaceTy<'tcx>,\n     ) -> EvalResult<'tcx> {\n-        ecx.call_intrinsic(instance, args, dest, dest_layout, target)\n+        ecx.call_intrinsic(instance, args, dest)\n     }\n \n     fn try_ptr_op<'a>(\n         ecx: &rustc_mir::interpret::EvalContext<'a, 'mir, 'tcx, Self>,\n         bin_op: mir::BinOp,\n         left: Scalar,\n-        left_ty: ty::Ty<'tcx>,\n+        left_layout: TyLayout<'tcx>,\n         right: Scalar,\n-        right_ty: ty::Ty<'tcx>,\n+        right_layout: TyLayout<'tcx>,\n     ) -> EvalResult<'tcx, Option<(Scalar, bool)>> {\n-        ecx.ptr_op(bin_op, left, left_ty, right, right_ty)\n-    }\n-\n-    fn mark_static_initialized<'a>(\n-        mem: &mut Memory<'a, 'mir, 'tcx, Self>,\n-        id: AllocId,\n-        _mutability: Mutability,\n-    ) -> EvalResult<'tcx, bool> {\n-        use memory::MemoryKind::*;\n-        match mem.get_alloc_kind(id) {\n-            // FIXME: This could be allowed, but not for env vars set during miri execution\n-            Some(MemoryKind::Machine(Env)) => err!(Unimplemented(\"statics can't refer to env vars\".to_owned())),\n-            _ => Ok(false), // TODO: What does the bool mean?\n-        }\n-    }\n-\n-    fn init_static<'a>(\n-        ecx: &mut EvalContext<'a, 'mir, 'tcx, Self>,\n-        cid: GlobalId<'tcx>,\n-    ) -> EvalResult<'tcx, AllocId> {\n-        // Step 1: If the static has already been evaluated return the cached version\n-        if let Some(alloc_id) = ecx.memory.data.statics.get(&cid) {\n-            return Ok(*alloc_id);\n-        }\n-\n-        let tcx = ecx.tcx.tcx;\n-\n-        // Step 2: Load mir\n-        let mut mir = ecx.load_mir(cid.instance.def)?;\n-        if let Some(index) = cid.promoted {\n-            mir = &mir.promoted[index];\n-        }\n-        assert!(mir.arg_count == 0);\n-\n-        // Step 3: Allocate storage\n-        let layout = ecx.layout_of(mir.return_ty().subst(tcx, cid.instance.substs))?;\n-        assert!(!layout.is_unsized());\n-        let ptr = ecx.memory.allocate(\n-            layout.size,\n-            layout.align,\n-            MemoryKind::Stack,\n-        )?;\n-\n-        // Step 4: Cache allocation id for recursive statics\n-        assert!(ecx.memory.data.statics.insert(cid, ptr.alloc_id).is_none());\n-\n-        // Step 5: Push stackframe to evaluate static\n-        let cleanup = StackPopCleanup::None;\n-        ecx.push_stack_frame(\n-            cid.instance,\n-            mir.span,\n-            mir,\n-            Place::from_ptr(ptr, layout.align),\n-            cleanup,\n-        )?;\n-\n-        // Step 6: Step until static has been initialized\n-        let call_stackframe = ecx.stack().len();\n-        while ecx.step()? && ecx.stack().len() >= call_stackframe {\n-            if ecx.stack().len() == call_stackframe {\n-                let frame = ecx.frame_mut();\n-                let bb = &frame.mir.basic_blocks()[frame.block];\n-                if bb.statements.len() == frame.stmt && !bb.is_cleanup {\n-                    if let ::rustc::mir::TerminatorKind::Return = bb.terminator().kind {\n-                        for (local, _local_decl) in mir.local_decls.iter_enumerated().skip(1) {\n-                            // Don't deallocate locals, because the return value might reference them\n-                            frame.storage_dead(local);\n-                        }\n-                    }\n-                }\n-            }\n-        }\n-\n-        // TODO: Freeze immutable statics without copying them to the global static cache\n-\n-        // Step 7: Return the alloc\n-        Ok(ptr.alloc_id)\n+        ecx.ptr_op(bin_op, left, left_layout, right, right_layout)\n     }\n \n     fn box_alloc<'a>(\n         ecx: &mut EvalContext<'a, 'mir, 'tcx, Self>,\n-        ty: ty::Ty<'tcx>,\n-        dest: Place,\n+        dest: PlaceTy<'tcx>,\n     ) -> EvalResult<'tcx> {\n-        let layout = ecx.layout_of(ty)?;\n-\n+        trace!(\"box_alloc for {:?}\", dest.layout.ty);\n         // Call the `exchange_malloc` lang item\n         let malloc = ecx.tcx.lang_items().exchange_malloc_fn().unwrap();\n         let malloc = ty::Instance::mono(ecx.tcx.tcx, malloc);\n@@ -494,100 +328,54 @@ impl<'mir, 'tcx: 'mir> Machine<'mir, 'tcx> for Evaluator<'tcx> {\n             malloc,\n             malloc_mir.span,\n             malloc_mir,\n-            dest,\n+            *dest,\n             // Don't do anything when we are done.  The statement() function will increment\n             // the old stack frame's stmt counter to the next statement, which means that when\n             // exchange_malloc returns, we go on evaluating exactly where we want to be.\n-            StackPopCleanup::None,\n+            StackPopCleanup::None { cleanup: true },\n         )?;\n \n         let mut args = ecx.frame().mir.args_iter();\n-        let usize = ecx.tcx.types.usize;\n-        let ptr_size = ecx.memory.pointer_size();\n+        let layout = ecx.layout_of(dest.layout.ty.builtin_deref(false).unwrap().ty)?;\n \n         // First argument: size\n-        let dest = ecx.eval_place(&mir::Place::Local(args.next().unwrap()))?;\n-        ecx.write_value(\n-            ValTy {\n-                value: Value::Scalar(Scalar::from_uint(match layout.size.bytes() {\n-                    0 => 1,\n-                    size => size,\n-                }, ptr_size).into()),\n-                ty: usize,\n-            },\n-            dest,\n-        )?;\n+        // (0 is allowed here, this is expected to be handled by the lang item)\n+        let arg = ecx.eval_place(&mir::Place::Local(args.next().unwrap()))?;\n+        let size = layout.size.bytes();\n+        ecx.write_scalar(Scalar::from_uint(size, arg.layout.size), arg)?;\n \n         // Second argument: align\n-        let dest = ecx.eval_place(&mir::Place::Local(args.next().unwrap()))?;\n-        ecx.write_value(\n-            ValTy {\n-                value: Value::Scalar(Scalar::from_uint(layout.align.abi(), ptr_size).into()),\n-                ty: usize,\n-            },\n-            dest,\n-        )?;\n+        let arg = ecx.eval_place(&mir::Place::Local(args.next().unwrap()))?;\n+        let align = layout.align.abi();\n+        ecx.write_scalar(Scalar::from_uint(align, arg.layout.size), arg)?;\n \n         // No more arguments\n         assert!(args.next().is_none(), \"exchange_malloc lang item has more arguments than expected\");\n         Ok(())\n     }\n \n-    fn global_item_with_linkage<'a>(\n-        _ecx: &mut EvalContext<'a, 'mir, 'tcx, Self>,\n-        _instance: ty::Instance<'tcx>,\n-        _mutability: Mutability,\n-    ) -> EvalResult<'tcx> {\n-        panic!(\"remove this function from rustc\");\n-    }\n-\n-    fn check_locks<'a>(\n-        mem: &Memory<'a, 'mir, 'tcx, Self>,\n-        ptr: Pointer,\n-        size: Size,\n-        access: AccessKind,\n-    ) -> EvalResult<'tcx> {\n-        mem.check_locks(ptr, size.bytes(), access)\n-    }\n-\n-    fn add_lock<'a>(\n-        mem: &mut Memory<'a, 'mir, 'tcx, Self>,\n-        id: AllocId,\n-    ) {\n-        mem.data.locks.insert(id, RangeMap::new());\n-    }\n-\n-    fn free_lock<'a>(\n-        mem: &mut Memory<'a, 'mir, 'tcx, Self>,\n-        id: AllocId,\n-        len: u64,\n-    ) -> EvalResult<'tcx> {\n-        mem.data.locks\n-            .remove(&id)\n-            .expect(\"allocation has no corresponding locks\")\n-            .check(\n-                Some(mem.cur_frame),\n-                0,\n-                len,\n-                AccessKind::Read,\n-            )\n-            .map_err(|lock| {\n-                EvalErrorKind::DeallocatedLockedMemory {\n-                    //ptr, FIXME\n-                    ptr: Pointer {\n-                        alloc_id: AllocId(0),\n-                        offset: Size::from_bytes(0),\n-                    },\n-                    lock: lock.active,\n-                }.into()\n-            })\n-    }\n-\n-    fn end_region<'a>(\n-        ecx: &mut EvalContext<'a, 'mir, 'tcx, Self>,\n-        reg: Option<::rustc::middle::region::Scope>,\n-    ) -> EvalResult<'tcx> {\n-        ecx.end_region(reg)\n+    fn find_foreign_static<'a>(\n+        tcx: TyCtxtAt<'a, 'tcx, 'tcx>,\n+        def_id: DefId,\n+    ) -> EvalResult<'tcx, &'tcx Allocation> {\n+        let attrs = tcx.get_attrs(def_id);\n+        let link_name = match attr::first_attr_value_str_by_name(&attrs, \"link_name\") {\n+            Some(name) => name.as_str(),\n+            None => tcx.item_name(def_id).as_str(),\n+        };\n+\n+        let alloc = match &link_name[..] {\n+            \"__cxa_thread_atexit_impl\" => {\n+                // This should be all-zero, pointer-sized\n+                let data = vec![0; tcx.data_layout.pointer_size.bytes() as usize];\n+                let alloc = Allocation::from_bytes(&data[..], tcx.data_layout.pointer_align);\n+                tcx.intern_const_alloc(alloc)\n+            }\n+            _ => return err!(Unimplemented(\n+                    format!(\"can't access foreign static: {}\", link_name),\n+                )),\n+        };\n+        Ok(alloc)\n     }\n \n     fn validation_op<'a>("}, {"sha": "a87ff6367e3ad89c398360e87013b3d994586d22", "filename": "src/locks.rs", "status": "modified", "additions": 5, "deletions": 315, "changes": 320, "blob_url": "https://github.com/rust-lang/rust/blob/752accf4e48b83545fa2fcf205602f239389361d/src%2Flocks.rs", "raw_url": "https://github.com/rust-lang/rust/raw/752accf4e48b83545fa2fcf205602f239389361d/src%2Flocks.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flocks.rs?ref=752accf4e48b83545fa2fcf205602f239389361d", "patch": "@@ -1,3 +1,5 @@\n+#![allow(unused)]\n+\n use super::*;\n use rustc::middle::region;\n use rustc::ty::layout::Size;\n@@ -6,6 +8,9 @@ use rustc::ty::layout::Size;\n // Locks\n ////////////////////////////////////////////////////////////////////////////////\n \n+// Just some dummy to keep this compiling; I think some of this will be useful later\n+type AbsPlace<'tcx> = ::rustc::ty::Ty<'tcx>;\n+\n /// Information about a lock that is currently held.\n #[derive(Clone, Debug, PartialEq, Eq)]\n pub struct LockInfo<'tcx> {\n@@ -67,321 +72,6 @@ impl<'tcx> LockInfo<'tcx> {\n     }\n }\n \n-pub trait MemoryExt<'tcx> {\n-    fn check_locks(\n-        &self,\n-        ptr: Pointer,\n-        len: u64,\n-        access: AccessKind,\n-    ) -> EvalResult<'tcx>;\n-    fn acquire_lock(\n-        &mut self,\n-        ptr: Pointer,\n-        len: u64,\n-        region: Option<region::Scope>,\n-        kind: AccessKind,\n-    ) -> EvalResult<'tcx>;\n-    fn suspend_write_lock(\n-        &mut self,\n-        ptr: Pointer,\n-        len: u64,\n-        lock_path: &AbsPlace<'tcx>,\n-        suspend: Option<region::Scope>,\n-    ) -> EvalResult<'tcx>;\n-    fn recover_write_lock(\n-        &mut self,\n-        ptr: Pointer,\n-        len: u64,\n-        lock_path: &AbsPlace<'tcx>,\n-        lock_region: Option<region::Scope>,\n-        suspended_region: region::Scope,\n-    ) -> EvalResult<'tcx>;\n-    fn locks_lifetime_ended(&mut self, ending_region: Option<region::Scope>);\n-}\n-\n-\n-impl<'a, 'mir, 'tcx: 'mir + 'a> MemoryExt<'tcx> for Memory<'a, 'mir, 'tcx, Evaluator<'tcx>> {\n-    fn check_locks(\n-        &self,\n-        ptr: Pointer,\n-        len: u64,\n-        access: AccessKind,\n-    ) -> EvalResult<'tcx> {\n-        if len == 0 {\n-            return Ok(());\n-        }\n-        let locks = match self.data.locks.get(&ptr.alloc_id) {\n-            Some(locks) => locks,\n-            // immutable static or other constant memory\n-            None => return Ok(()),\n-        };\n-        let frame = self.cur_frame;\n-        locks\n-            .check(Some(frame), ptr.offset.bytes(), len, access)\n-            .map_err(|lock| {\n-                EvalErrorKind::MemoryLockViolation {\n-                    ptr,\n-                    len,\n-                    frame,\n-                    access,\n-                    lock: lock.active,\n-                }.into()\n-            })\n-    }\n-\n-    /// Acquire the lock for the given lifetime\n-    fn acquire_lock(\n-        &mut self,\n-        ptr: Pointer,\n-        len: u64,\n-        region: Option<region::Scope>,\n-        kind: AccessKind,\n-    ) -> EvalResult<'tcx> {\n-        let frame = self.cur_frame;\n-        assert!(len > 0);\n-        trace!(\n-            \"Frame {} acquiring {:?} lock at {:?}, size {} for region {:?}\",\n-            frame,\n-            kind,\n-            ptr,\n-            len,\n-            region\n-        );\n-        self.check_bounds(ptr.offset(Size::from_bytes(len), &*self)?, true)?; // if ptr.offset is in bounds, then so is ptr (because offset checks for overflow)\n-\n-        let locks = match self.data.locks.get_mut(&ptr.alloc_id) {\n-            Some(locks) => locks,\n-            // immutable static or other constant memory\n-            None => return Ok(()),\n-        };\n-\n-        // Iterate over our range and acquire the lock.  If the range is already split into pieces,\n-        // we have to manipulate all of them.\n-        let lifetime = DynamicLifetime { frame, region };\n-        for lock in locks.iter_mut(ptr.offset.bytes(), len) {\n-            if !lock.access_permitted(None, kind) {\n-                return err!(MemoryAcquireConflict {\n-                    ptr,\n-                    len,\n-                    kind,\n-                    lock: lock.active.clone(),\n-                });\n-            }\n-            // See what we have to do\n-            match (&mut lock.active, kind) {\n-                (active @ &mut NoLock, AccessKind::Write) => {\n-                    *active = WriteLock(lifetime);\n-                }\n-                (active @ &mut NoLock, AccessKind::Read) => {\n-                    *active = ReadLock(vec![lifetime]);\n-                }\n-                (&mut ReadLock(ref mut lifetimes), AccessKind::Read) => {\n-                    lifetimes.push(lifetime);\n-                }\n-                _ => bug!(\"We already checked that there is no conflicting lock\"),\n-            }\n-        }\n-        Ok(())\n-    }\n-\n-    /// Release or suspend a write lock of the given lifetime prematurely.\n-    /// When releasing, if there is a read lock or someone else's write lock, that's an error.\n-    /// If no lock is held, that's fine.  This can happen when e.g. a local is initialized\n-    /// from a constant, and then suspended.\n-    /// When suspending, the same cases are fine; we just register an additional suspension.\n-    fn suspend_write_lock(\n-        &mut self,\n-        ptr: Pointer,\n-        len: u64,\n-        lock_path: &AbsPlace<'tcx>,\n-        suspend: Option<region::Scope>,\n-    ) -> EvalResult<'tcx> {\n-        assert!(len > 0);\n-        let cur_frame = self.cur_frame;\n-        let locks = match self.data.locks.get_mut(&ptr.alloc_id) {\n-            Some(locks) => locks,\n-            // immutable static or other constant memory\n-            None => return Ok(()),\n-        };\n-\n-        'locks: for lock in locks.iter_mut(ptr.offset.bytes(), len) {\n-            let is_our_lock = match lock.active {\n-                WriteLock(lft) =>\n-                    // Double-check that we are holding the lock.\n-                    // (Due to subtyping, checking the region would not make any sense.)\n-                    lft.frame == cur_frame,\n-                ReadLock(_) | NoLock => false,\n-            };\n-            if is_our_lock {\n-                trace!(\"Releasing {:?}\", lock.active);\n-                // Disable the lock\n-                lock.active = NoLock;\n-            } else {\n-                trace!(\n-                    \"Not touching {:?} as it is not our lock\",\n-                    lock.active,\n-                );\n-            }\n-            // Check if we want to register a suspension\n-            if let Some(suspend_region) = suspend {\n-                let lock_id = WriteLockId {\n-                    frame: cur_frame,\n-                    path: lock_path.clone(),\n-                };\n-                trace!(\"Adding suspension to {:?}\", lock_id);\n-                let mut new_suspension = false;\n-                lock.suspended\n-                    .entry(lock_id)\n-                    // Remember whether we added a new suspension or not\n-                    .or_insert_with(|| { new_suspension = true; Vec::new() })\n-                    .push(suspend_region);\n-                // If the suspension is new, we should have owned this.\n-                // If there already was a suspension, we should NOT have owned this.\n-                if new_suspension == is_our_lock {\n-                    // All is well\n-                    continue 'locks;\n-                }\n-            } else if !is_our_lock {\n-                // All is well.\n-                continue 'locks;\n-            }\n-            // If we get here, releasing this is an error except for NoLock.\n-            if lock.active != NoLock {\n-                return err!(InvalidMemoryLockRelease {\n-                    ptr,\n-                    len,\n-                    frame: cur_frame,\n-                    lock: lock.active.clone(),\n-                });\n-            }\n-        }\n-\n-        Ok(())\n-    }\n-\n-    /// Release a suspension from the write lock.  If this is the last suspension or if there is no suspension, acquire the lock.\n-    fn recover_write_lock(\n-        &mut self,\n-        ptr: Pointer,\n-        len: u64,\n-        lock_path: &AbsPlace<'tcx>,\n-        lock_region: Option<region::Scope>,\n-        suspended_region: region::Scope,\n-    ) -> EvalResult<'tcx> {\n-        assert!(len > 0);\n-        let cur_frame = self.cur_frame;\n-        let lock_id = WriteLockId {\n-            frame: cur_frame,\n-            path: lock_path.clone(),\n-        };\n-        let locks = match self.data.locks.get_mut(&ptr.alloc_id) {\n-            Some(locks) => locks,\n-            // immutable static or other constant memory\n-            None => return Ok(()),\n-        };\n-\n-        for lock in locks.iter_mut(ptr.offset.bytes(), len) {\n-            // Check if we have a suspension here\n-            let (got_the_lock, remove_suspension) = match lock.suspended.get_mut(&lock_id) {\n-                None => {\n-                    trace!(\"No suspension around, we can just acquire\");\n-                    (true, false)\n-                }\n-                Some(suspensions) => {\n-                    trace!(\"Found suspension of {:?}, removing it\", lock_id);\n-                    // That's us!  Remove suspension (it should be in there).  The same suspension can\n-                    // occur multiple times (when there are multiple shared borrows of this that have the same\n-                    // lifetime); only remove one of them.\n-                    let idx = match suspensions.iter().enumerate().find(|&(_, re)| re == &suspended_region) {\n-                        None => // TODO: Can the user trigger this?\n-                            bug!(\"We have this lock suspended, but not for the given region.\"),\n-                        Some((idx, _)) => idx\n-                    };\n-                    suspensions.remove(idx);\n-                    let got_lock = suspensions.is_empty();\n-                    if got_lock {\n-                        trace!(\"All suspensions are gone, we can have the lock again\");\n-                    }\n-                    (got_lock, got_lock)\n-                }\n-            };\n-            if remove_suspension {\n-                // with NLL, we could do that up in the match above...\n-                assert!(got_the_lock);\n-                lock.suspended.remove(&lock_id);\n-            }\n-            if got_the_lock {\n-                match lock.active {\n-                    ref mut active @ NoLock => {\n-                        *active = WriteLock(\n-                            DynamicLifetime {\n-                                frame: cur_frame,\n-                                region: lock_region,\n-                            }\n-                        );\n-                    }\n-                    _ => {\n-                        return err!(MemoryAcquireConflict {\n-                            ptr,\n-                            len,\n-                            kind: AccessKind::Write,\n-                            lock: lock.active.clone(),\n-                        })\n-                    }\n-                }\n-            }\n-        }\n-\n-        Ok(())\n-    }\n-\n-    fn locks_lifetime_ended(&mut self, ending_region: Option<region::Scope>) {\n-        let cur_frame = self.cur_frame;\n-        trace!(\n-            \"Releasing frame {} locks that expire at {:?}\",\n-            cur_frame,\n-            ending_region\n-        );\n-        let has_ended = |lifetime: &DynamicLifetime| -> bool {\n-            if lifetime.frame != cur_frame {\n-                return false;\n-            }\n-            match ending_region {\n-                None => true, // When a function ends, we end *all* its locks. It's okay for a function to still have lifetime-related locks\n-                // when it returns, that can happen e.g. with NLL when a lifetime can, but does not have to, extend beyond the\n-                // end of a function.  Same for a function still having recoveries.\n-                Some(ending_region) => lifetime.region == Some(ending_region),\n-            }\n-        };\n-\n-        for alloc_locks in self.data.locks.values_mut() {\n-            for lock in alloc_locks.iter_mut_all() {\n-                // Delete everything that ends now -- i.e., keep only all the other lifetimes.\n-                let lock_ended = match lock.active {\n-                    WriteLock(ref lft) => has_ended(lft),\n-                    ReadLock(ref mut lfts) => {\n-                        lfts.retain(|lft| !has_ended(lft));\n-                        lfts.is_empty()\n-                    }\n-                    NoLock => false,\n-                };\n-                if lock_ended {\n-                    lock.active = NoLock;\n-                }\n-                // Also clean up suspended write locks when the function returns\n-                if ending_region.is_none() {\n-                    lock.suspended.retain(|id, _suspensions| id.frame != cur_frame);\n-                }\n-            }\n-            // Clean up the map\n-            alloc_locks.retain(|lock| match lock.active {\n-                NoLock => !lock.suspended.is_empty(),\n-                _ => true,\n-            });\n-        }\n-    }\n-}\n-\n impl<'tcx> RangeMap<LockInfo<'tcx>> {\n     pub fn check(\n         &self,"}, {"sha": "d4575d677fa14094d9404524527ba1bb5640d922", "filename": "src/memory.rs", "status": "modified", "additions": 5, "deletions": 4, "changes": 9, "blob_url": "https://github.com/rust-lang/rust/blob/752accf4e48b83545fa2fcf205602f239389361d/src%2Fmemory.rs", "raw_url": "https://github.com/rust-lang/rust/raw/752accf4e48b83545fa2fcf205602f239389361d/src%2Fmemory.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Fmemory.rs?ref=752accf4e48b83545fa2fcf205602f239389361d", "patch": "@@ -1,12 +1,13 @@\n-\n-#[derive(Debug, PartialEq, Copy, Clone)]\n+#[derive(Debug, PartialEq, Copy, Clone, Hash, Eq)]\n pub enum MemoryKind {\n-    /// Error if deallocated any other way than `rust_deallocate`\n+    /// `__rust_alloc` memory\n     Rust,\n-    /// Error if deallocated any other way than `free`\n+    /// `malloc` memory\n     C,\n     /// Part of env var emulation\n     Env,\n+    // mutable statics\n+    MutStatic,\n }\n \n impl Into<::rustc_mir::interpret::MemoryKind<MemoryKind>> for MemoryKind {"}, {"sha": "03383ae61c89473fd84236a44f81dedabda99c83", "filename": "src/operator.rs", "status": "modified", "additions": 62, "deletions": 21, "changes": 83, "blob_url": "https://github.com/rust-lang/rust/blob/752accf4e48b83545fa2fcf205602f239389361d/src%2Foperator.rs", "raw_url": "https://github.com/rust-lang/rust/raw/752accf4e48b83545fa2fcf205602f239389361d/src%2Foperator.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Foperator.rs?ref=752accf4e48b83545fa2fcf205602f239389361d", "patch": "@@ -1,19 +1,17 @@\n-use rustc::ty;\n-use rustc::ty::layout::Primitive;\n+use rustc::ty::{self, Ty};\n+use rustc::ty::layout::{TyLayout, Primitive};\n use rustc::mir;\n \n use super::*;\n \n-use helpers::EvalContextExt as HelperEvalContextExt;\n-\n pub trait EvalContextExt<'tcx> {\n     fn ptr_op(\n         &self,\n         bin_op: mir::BinOp,\n         left: Scalar,\n-        left_ty: ty::Ty<'tcx>,\n+        left_layout: TyLayout<'tcx>,\n         right: Scalar,\n-        right_ty: ty::Ty<'tcx>,\n+        right_layout: TyLayout<'tcx>,\n     ) -> EvalResult<'tcx, Option<(Scalar, bool)>>;\n \n     fn ptr_int_arithmetic(\n@@ -23,16 +21,23 @@ pub trait EvalContextExt<'tcx> {\n         right: u128,\n         signed: bool,\n     ) -> EvalResult<'tcx, (Scalar, bool)>;\n+\n+    fn pointer_offset_inbounds(\n+        &self,\n+        ptr: Scalar,\n+        pointee_ty: Ty<'tcx>,\n+        offset: i64,\n+    ) -> EvalResult<'tcx, Scalar>;\n }\n \n impl<'a, 'mir, 'tcx> EvalContextExt<'tcx> for EvalContext<'a, 'mir, 'tcx, super::Evaluator<'tcx>> {\n     fn ptr_op(\n         &self,\n         bin_op: mir::BinOp,\n         left: Scalar,\n-        left_ty: ty::Ty<'tcx>,\n+        left_layout: TyLayout<'tcx>,\n         right: Scalar,\n-        right_ty: ty::Ty<'tcx>,\n+        right_layout: TyLayout<'tcx>,\n     ) -> EvalResult<'tcx, Option<(Scalar, bool)>> {\n         trace!(\"ptr_op: {:?} {:?} {:?}\", left, bin_op, right);\n \n@@ -45,32 +50,31 @@ impl<'a, 'mir, 'tcx> EvalContextExt<'tcx> for EvalContext<'a, 'mir, 'tcx, super:\n             8 => I64,\n             16 => I128,\n             _ => unreachable!(),\n-        }, false);\n+        }, /*signed*/ false);\n         let isize = Primitive::Int(match self.memory.pointer_size().bytes() {\n             1 => I8,\n             2 => I16,\n             4 => I32,\n             8 => I64,\n             16 => I128,\n             _ => unreachable!(),\n-        }, true);\n-        let left_layout = self.layout_of(left_ty)?;\n+        }, /*signed*/ true);\n         let left_kind = match left_layout.abi {\n             ty::layout::Abi::Scalar(ref scalar) => scalar.value,\n-            _ => Err(EvalErrorKind::TypeNotPrimitive(left_ty))?,\n+            _ => Err(EvalErrorKind::TypeNotPrimitive(left_layout.ty))?,\n         };\n-        let right_layout = self.layout_of(right_ty)?;\n         let right_kind = match right_layout.abi {\n             ty::layout::Abi::Scalar(ref scalar) => scalar.value,\n-            _ => Err(EvalErrorKind::TypeNotPrimitive(right_ty))?,\n+            _ => Err(EvalErrorKind::TypeNotPrimitive(right_layout.ty))?,\n         };\n         match bin_op {\n-            Offset if left_kind == Primitive::Pointer && right_kind == usize => {\n-                let pointee_ty = left_ty\n+            Offset => {\n+                assert!(left_kind == Primitive::Pointer && right_kind == usize);\n+                let pointee_ty = left_layout.ty\n                     .builtin_deref(true)\n                     .expect(\"Offset called on non-ptr type\")\n                     .ty;\n-                let ptr = self.pointer_offset(\n+                let ptr = self.pointer_offset_inbounds(\n                     left,\n                     pointee_ty,\n                     right.to_bits(self.memory.pointer_size())? as i64,\n@@ -114,12 +118,13 @@ impl<'a, 'mir, 'tcx> EvalContextExt<'tcx> for EvalContext<'a, 'mir, 'tcx, super:\n                         Gt => left.offset > right.offset,\n                         Ge => left.offset >= right.offset,\n                         Sub => {\n+                            let left_offset = Scalar::from_uint(left.offset.bytes(), self.memory.pointer_size());\n+                            let right_offset = Scalar::from_uint(right.offset.bytes(), self.memory.pointer_size());\n+                            let layout = self.layout_of(self.tcx.types.usize)?;\n                             return self.binary_op(\n                                 Sub,\n-                                Scalar::Bits { bits: left.offset.bytes() as u128, size: self.memory.pointer_size().bytes() as u8 },\n-                                self.tcx.types.usize,\n-                                Scalar::Bits { bits: right.offset.bytes() as u128, size: self.memory.pointer_size().bytes() as u8 },\n-                                self.tcx.types.usize,\n+                                ValTy { value: Value::Scalar(left_offset.into()), layout },\n+                                ValTy { value: Value::Scalar(right_offset.into()), layout },\n                             ).map(Some)\n                         }\n                         _ => bug!(\"We already established it has to be one of these operators.\"),\n@@ -228,4 +233,40 @@ impl<'a, 'mir, 'tcx> EvalContextExt<'tcx> for EvalContext<'a, 'mir, 'tcx, super:\n             }\n         })\n     }\n+\n+    /// This function raises an error if the offset moves the pointer outside of its allocation.  We consider\n+    /// ZSTs their own huge allocation that doesn't overlap with anything (and nothing moves in there because the size is 0).\n+    /// We also consider the NULL pointer its own separate allocation, and all the remaining integers pointers their own\n+    /// allocation.\n+    fn pointer_offset_inbounds(\n+        &self,\n+        ptr: Scalar,\n+        pointee_ty: Ty<'tcx>,\n+        offset: i64,\n+    ) -> EvalResult<'tcx, Scalar> {\n+        if ptr.is_null() {\n+            // NULL pointers must only be offset by 0\n+            return if offset == 0 {\n+                Ok(ptr)\n+            } else {\n+                err!(InvalidNullPointerUsage)\n+            };\n+        }\n+        // FIXME: assuming here that type size is < i64::max_value()\n+        let pointee_size = self.layout_of(pointee_ty)?.size.bytes() as i64;\n+        let offset = offset.checked_mul(pointee_size).ok_or_else(|| EvalErrorKind::Overflow(mir::BinOp::Mul))?;\n+        // Now let's see what kind of pointer this is\n+        if let Scalar::Ptr(ptr) = ptr {\n+            // Both old and new pointer must be in-bounds.\n+            // (Of the same allocation, but that part is trivial with our representation.)\n+            self.memory.check_bounds(ptr, false)?;\n+            let ptr = ptr.signed_offset(offset, self)?;\n+            self.memory.check_bounds(ptr, false)?;\n+            Ok(Scalar::Ptr(ptr))\n+        } else {\n+            // An integer pointer. They can move around freely, as long as they do not overflow\n+            // (which ptr_signed_offset checks).\n+            ptr.ptr_signed_offset(offset, self)\n+        }\n+    }\n }"}, {"sha": "e55534e36fd2ed08708b077df16224a24a4f1b8e", "filename": "src/range_map.rs", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/752accf4e48b83545fa2fcf205602f239389361d/src%2Frange_map.rs", "raw_url": "https://github.com/rust-lang/rust/raw/752accf4e48b83545fa2fcf205602f239389361d/src%2Frange_map.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Frange_map.rs?ref=752accf4e48b83545fa2fcf205602f239389361d", "patch": "@@ -1,3 +1,5 @@\n+#![allow(unused)]\n+\n //! Implements a map from integer indices to data.\n //! Rather than storing data for every index, internally, this maps entire ranges to the data.\n //! To this end, the APIs all work on ranges, not on individual integers. Ranges are split as"}, {"sha": "6cec39483c46548b6da5801af9b43c8cabeff0ca", "filename": "src/tls.rs", "status": "modified", "additions": 5, "deletions": 5, "changes": 10, "blob_url": "https://github.com/rust-lang/rust/blob/752accf4e48b83545fa2fcf205602f239389361d/src%2Ftls.rs", "raw_url": "https://github.com/rust-lang/rust/raw/752accf4e48b83545fa2fcf205602f239389361d/src%2Ftls.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ftls.rs?ref=752accf4e48b83545fa2fcf205602f239389361d", "patch": "@@ -119,22 +119,22 @@ impl<'a, 'mir, 'tcx: 'mir + 'a> EvalContextExt<'tcx> for EvalContext<'a, 'mir, '\n             // TODO: Potentially, this has to support all the other possible instances?\n             // See eval_fn_call in interpret/terminator/mod.rs\n             let mir = self.load_mir(instance.def)?;\n+            let ret = Place::null(&self);\n             self.push_stack_frame(\n                 instance,\n                 mir.span,\n                 mir,\n-                Place::undef(),\n-                StackPopCleanup::None,\n+                ret,\n+                StackPopCleanup::None { cleanup: true },\n             )?;\n             let arg_local = self.frame().mir.args_iter().next().ok_or_else(\n                 || EvalErrorKind::AbiViolation(\"TLS dtor does not take enough arguments.\".to_owned()),\n             )?;\n             let dest = self.eval_place(&mir::Place::Local(arg_local))?;\n-            let ty = self.tcx.mk_mut_ptr(self.tcx.types.u8);\n-            self.write_ptr(dest, ptr, ty)?;\n+            self.write_scalar(ptr, dest)?;\n \n             // step until out of stackframes\n-            while self.step()? {}\n+            self.run()?;\n \n             dtor = match self.memory.fetch_tls_dtor(Some(key)) {\n                 dtor @ Some(_) => dtor,"}, {"sha": "7f0abb9ae0bad40f78f1e6993965644c7b30c129", "filename": "src/validation.rs", "status": "removed", "additions": 0, "deletions": 803, "changes": 803, "blob_url": "https://github.com/rust-lang/rust/blob/66ca0fb0a9c28265b7630c0119e4a7c524b845af/src%2Fvalidation.rs", "raw_url": "https://github.com/rust-lang/rust/raw/66ca0fb0a9c28265b7630c0119e4a7c524b845af/src%2Fvalidation.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Fvalidation.rs?ref=66ca0fb0a9c28265b7630c0119e4a7c524b845af", "patch": "@@ -1,803 +0,0 @@\n-use rustc::hir::{self, Mutability};\n-use rustc::hir::Mutability::*;\n-use rustc::mir::{self, ValidationOp, ValidationOperand};\n-use rustc::mir::interpret::GlobalId;\n-use rustc::ty::{self, Ty, TypeFoldable, TyCtxt, Instance};\n-use rustc::ty::layout::{LayoutOf, PrimitiveExt};\n-use rustc::ty::subst::{Substs, Subst};\n-use rustc::traits::{self, TraitEngine};\n-use rustc::infer::InferCtxt;\n-use rustc::middle::region;\n-use rustc::mir::interpret::{ConstValue};\n-use rustc_data_structures::indexed_vec::Idx;\n-use rustc_mir::interpret::HasMemory;\n-\n-use super::{EvalContext, Place, PlaceExtra, ValTy, ScalarExt};\n-use rustc::mir::interpret::{DynamicLifetime, AccessKind, EvalErrorKind, Value, EvalError, EvalResult};\n-use locks::MemoryExt;\n-\n-pub type ValidationQuery<'tcx> = ValidationOperand<'tcx, (AbsPlace<'tcx>, Place)>;\n-\n-#[derive(Copy, Clone, Debug, PartialEq)]\n-pub(crate) enum ValidationMode {\n-    Acquire,\n-    /// Recover because the given region ended\n-    Recover(region::Scope),\n-    ReleaseUntil(Option<region::Scope>),\n-}\n-\n-impl ValidationMode {\n-    fn acquiring(self) -> bool {\n-        use self::ValidationMode::*;\n-        match self {\n-            Acquire | Recover(_) => true,\n-            ReleaseUntil(_) => false,\n-        }\n-    }\n-}\n-\n-// Abstract places\n-#[derive(Clone, Debug, PartialEq, Eq, Hash)]\n-pub enum AbsPlace<'tcx> {\n-    Local(mir::Local),\n-    Static(hir::def_id::DefId),\n-    Projection(Box<AbsPlaceProjection<'tcx>>),\n-}\n-\n-type AbsPlaceProjection<'tcx> = mir::Projection<'tcx, AbsPlace<'tcx>, u64, ()>;\n-type AbsPlaceElem<'tcx> = mir::ProjectionElem<'tcx, u64, ()>;\n-\n-impl<'tcx> AbsPlace<'tcx> {\n-    pub fn field(self, f: mir::Field) -> AbsPlace<'tcx> {\n-        self.elem(mir::ProjectionElem::Field(f, ()))\n-    }\n-\n-    pub fn deref(self) -> AbsPlace<'tcx> {\n-        self.elem(mir::ProjectionElem::Deref)\n-    }\n-\n-    pub fn downcast(self, adt_def: &'tcx ty::AdtDef, variant_index: usize) -> AbsPlace<'tcx> {\n-        self.elem(mir::ProjectionElem::Downcast(adt_def, variant_index))\n-    }\n-\n-    pub fn index(self, index: u64) -> AbsPlace<'tcx> {\n-        self.elem(mir::ProjectionElem::Index(index))\n-    }\n-\n-    fn elem(self, elem: AbsPlaceElem<'tcx>) -> AbsPlace<'tcx> {\n-        AbsPlace::Projection(Box::new(AbsPlaceProjection {\n-            base: self,\n-            elem,\n-        }))\n-    }\n-}\n-\n-pub(crate) trait EvalContextExt<'tcx> {\n-    fn abstract_place_projection(&self, proj: &mir::PlaceProjection<'tcx>) -> EvalResult<'tcx, AbsPlaceProjection<'tcx>>;\n-    fn abstract_place(&self, place: &mir::Place<'tcx>) -> EvalResult<'tcx, AbsPlace<'tcx>>;\n-    fn validation_op(\n-        &mut self,\n-        op: ValidationOp,\n-        operand: &ValidationOperand<'tcx, mir::Place<'tcx>>,\n-    ) -> EvalResult<'tcx>;\n-    fn end_region(&mut self, scope: Option<region::Scope>) -> EvalResult<'tcx>;\n-    fn normalize_type_unerased(&self, ty: Ty<'tcx>) -> Ty<'tcx>;\n-    fn field_with_lifetimes(\n-        &mut self,\n-        base: Place,\n-        layout: ty::layout::TyLayout<'tcx>,\n-        i: usize,\n-    ) -> EvalResult<'tcx, Ty<'tcx>>;\n-    fn validate_fields(\n-        &mut self,\n-        query: ValidationQuery<'tcx>,\n-        mode: ValidationMode,\n-    ) -> EvalResult<'tcx>;\n-    fn validate_ptr(\n-        &mut self,\n-        val: Value,\n-        abs_place: AbsPlace<'tcx>,\n-        pointee_ty: Ty<'tcx>,\n-        re: Option<region::Scope>,\n-        mutbl: Mutability,\n-        mode: ValidationMode,\n-    ) -> EvalResult<'tcx>;\n-    fn validate(\n-        &mut self,\n-        query: ValidationQuery<'tcx>,\n-        mode: ValidationMode,\n-    ) -> EvalResult<'tcx>;\n-}\n-\n-impl<'a, 'mir, 'tcx: 'mir + 'a> EvalContextExt<'tcx> for EvalContext<'a, 'mir, 'tcx, super::Evaluator<'tcx>> {\n-    fn abstract_place_projection(&self, proj: &mir::PlaceProjection<'tcx>) -> EvalResult<'tcx, AbsPlaceProjection<'tcx>> {\n-        use self::mir::ProjectionElem::*;\n-\n-        let elem = match proj.elem {\n-            Deref => Deref,\n-            Field(f, _) => Field(f, ()),\n-            Index(v) => {\n-                let value = self.frame().locals[v].access()?;\n-                let ty = self.tcx.tcx.types.usize;\n-                let n = self.value_to_scalar(ValTy { value, ty })?.to_usize(self)?;\n-                Index(n)\n-            },\n-            ConstantIndex { offset, min_length, from_end } =>\n-                ConstantIndex { offset, min_length, from_end },\n-            Subslice { from, to } =>\n-                Subslice { from, to },\n-            Downcast(adt, sz) => Downcast(adt, sz),\n-        };\n-        Ok(AbsPlaceProjection {\n-            base: self.abstract_place(&proj.base)?,\n-            elem\n-        })\n-    }\n-\n-    fn abstract_place(&self, place: &mir::Place<'tcx>) -> EvalResult<'tcx, AbsPlace<'tcx>> {\n-        Ok(match *place {\n-            mir::Place::Local(l) => AbsPlace::Local(l),\n-            mir::Place::Static(ref s) => AbsPlace::Static(s.def_id),\n-            mir::Place::Projection(ref p) =>\n-                AbsPlace::Projection(Box::new(self.abstract_place_projection(&*p)?)),\n-            _ => unimplemented!(\"validation is not currently maintained\"),\n-        })\n-    }\n-\n-    // Validity checks\n-    fn validation_op(\n-        &mut self,\n-        op: ValidationOp,\n-        operand: &ValidationOperand<'tcx, mir::Place<'tcx>>,\n-    ) -> EvalResult<'tcx> {\n-        // If mir-emit-validate is set to 0 (i.e., disabled), we may still see validation commands\n-        // because other crates may have been compiled with mir-emit-validate > 0.  Ignore those\n-        // commands.  This makes mir-emit-validate also a flag to control whether miri will do\n-        // validation or not.\n-        if self.tcx.tcx.sess.opts.debugging_opts.mir_emit_validate == 0 {\n-            return Ok(());\n-        }\n-        debug_assert!(self.memory.cur_frame == self.cur_frame());\n-\n-        // We need to monomorphize ty *without* erasing lifetimes\n-        trace!(\"validation_op1: {:?}\", operand.ty.sty);\n-        let ty = operand.ty.subst(self.tcx.tcx, self.substs());\n-        trace!(\"validation_op2: {:?}\", operand.ty.sty);\n-        let place = self.eval_place(&operand.place)?;\n-        let abs_place = self.abstract_place(&operand.place)?;\n-        let query = ValidationQuery {\n-            place: (abs_place, place),\n-            ty,\n-            re: operand.re,\n-            mutbl: operand.mutbl,\n-        };\n-\n-        // Check the mode, and also perform mode-specific operations\n-        let mode = match op {\n-            ValidationOp::Acquire => ValidationMode::Acquire,\n-            ValidationOp::Release => ValidationMode::ReleaseUntil(None),\n-            ValidationOp::Suspend(scope) => {\n-                if query.mutbl == MutMutable {\n-                    let lft = DynamicLifetime {\n-                        frame: self.cur_frame(),\n-                        region: Some(scope), // Notably, we only ever suspend things for given regions.\n-                        // Suspending for the entire function does not make any sense.\n-                    };\n-                    trace!(\"Suspending {:?} until {:?}\", query, scope);\n-                    self.machine.suspended.entry(lft).or_insert_with(Vec::new).push(\n-                        query.clone(),\n-                    );\n-                }\n-                ValidationMode::ReleaseUntil(Some(scope))\n-            }\n-        };\n-        self.validate(query, mode)\n-    }\n-\n-    /// Release locks and executes suspensions of the given region (or the entire fn, in case of None).\n-    fn end_region(&mut self, scope: Option<region::Scope>) -> EvalResult<'tcx> {\n-        debug_assert!(self.memory.cur_frame == self.cur_frame());\n-        self.memory.locks_lifetime_ended(scope);\n-        match scope {\n-            Some(scope) => {\n-                // Recover suspended places\n-                let lft = DynamicLifetime {\n-                    frame: self.cur_frame(),\n-                    region: Some(scope),\n-                };\n-                if let Some(queries) = self.machine.suspended.remove(&lft) {\n-                    for query in queries {\n-                        trace!(\"Recovering {:?} from suspension\", query);\n-                        self.validate(query, ValidationMode::Recover(scope))?;\n-                    }\n-                }\n-            }\n-            None => {\n-                // Clean suspension table of current frame\n-                let cur_frame = self.cur_frame();\n-                self.machine.suspended.retain(|lft, _| {\n-                    lft.frame != cur_frame // keep only what is in the other (lower) frames\n-                });\n-            }\n-        }\n-        Ok(())\n-    }\n-\n-    fn normalize_type_unerased(&self, ty: Ty<'tcx>) -> Ty<'tcx> {\n-        return normalize_associated_type(self.tcx.tcx, &ty);\n-\n-        use syntax::codemap::{Span, DUMMY_SP};\n-\n-        // We copy a bunch of stuff from rustc/infer/mod.rs to be able to tweak its behavior\n-        fn normalize_projections_in<'a, 'gcx, 'tcx, T>(\n-            self_: &InferCtxt<'a, 'gcx, 'tcx>,\n-            param_env: ty::ParamEnv<'tcx>,\n-            value: &T,\n-        ) -> T::Lifted\n-        where\n-            T: TypeFoldable<'tcx> + ty::Lift<'gcx>,\n-        {\n-            let mut selcx = traits::SelectionContext::new(self_);\n-            let cause = traits::ObligationCause::dummy();\n-            let traits::Normalized {\n-                value: result,\n-                obligations,\n-            } = traits::normalize(&mut selcx, param_env, cause, value);\n-\n-            let mut fulfill_cx = traits::FulfillmentContext::new();\n-\n-            for obligation in obligations {\n-                fulfill_cx.register_predicate_obligation(self_, obligation);\n-            }\n-\n-            drain_fulfillment_cx_or_panic(self_, DUMMY_SP, &mut fulfill_cx, &result)\n-        }\n-\n-        fn drain_fulfillment_cx_or_panic<'a, 'gcx, 'tcx, T>(\n-            self_: &InferCtxt<'a, 'gcx, 'tcx>,\n-            span: Span,\n-            fulfill_cx: &mut traits::FulfillmentContext<'tcx>,\n-            result: &T,\n-        ) -> T::Lifted\n-        where\n-            T: TypeFoldable<'tcx> + ty::Lift<'gcx>,\n-        {\n-            // In principle, we only need to do this so long as `result`\n-            // contains unbound type parameters. It could be a slight\n-            // optimization to stop iterating early.\n-            match fulfill_cx.select_all_or_error(self_) {\n-                Ok(()) => { }\n-                Err(errors) => {\n-                    span_bug!(\n-                        span,\n-                        \"Encountered errors `{:?}` resolving bounds after type-checking\",\n-                        errors\n-                    );\n-                }\n-            }\n-\n-            let result = self_.resolve_type_vars_if_possible(result);\n-            let result = self_.tcx.fold_regions(\n-                &result,\n-                &mut false,\n-                |r, _| match *r {\n-                    ty::ReVar(_) => self_.tcx.types.re_erased,\n-                    _ => r,\n-                },\n-            );\n-\n-            match self_.tcx.lift_to_global(&result) {\n-                Some(result) => result,\n-                None => {\n-                    span_bug!(span, \"Uninferred types/regions in `{:?}`\", result);\n-                }\n-            }\n-        }\n-\n-        trait MyTransNormalize<'gcx>: TypeFoldable<'gcx> {\n-            fn my_trans_normalize<'a, 'tcx>(\n-                &self,\n-                infcx: &InferCtxt<'a, 'gcx, 'tcx>,\n-                param_env: ty::ParamEnv<'tcx>,\n-            ) -> Self;\n-        }\n-\n-        macro_rules! items { ($($item:item)+) => ($($item)+) }\n-        macro_rules! impl_trans_normalize {\n-            ($lt_gcx:tt, $($ty:ty),+) => {\n-                items!($(impl<$lt_gcx> MyTransNormalize<$lt_gcx> for $ty {\n-                    fn my_trans_normalize<'a, 'tcx>(&self,\n-                                                infcx: &InferCtxt<'a, $lt_gcx, 'tcx>,\n-                                                param_env: ty::ParamEnv<'tcx>)\n-                                                -> Self {\n-                        normalize_projections_in(infcx, param_env, self)\n-                    }\n-                })+);\n-            }\n-        }\n-\n-        impl_trans_normalize!('gcx,\n-            Ty<'gcx>,\n-            &'gcx Substs<'gcx>,\n-            ty::FnSig<'gcx>,\n-            ty::PolyFnSig<'gcx>,\n-            ty::ClosureSubsts<'gcx>,\n-            ty::PolyTraitRef<'gcx>,\n-            ty::ExistentialTraitRef<'gcx>\n-        );\n-\n-        fn normalize_associated_type<'a, 'tcx, T>(self_: TyCtxt<'a, 'tcx, 'tcx>, value: &T) -> T\n-        where\n-            T: MyTransNormalize<'tcx>,\n-        {\n-            let param_env = ty::ParamEnv::reveal_all();\n-\n-            if !value.has_projections() {\n-                return value.clone();\n-            }\n-\n-            self_.infer_ctxt().enter(|infcx| {\n-                value.my_trans_normalize(&infcx, param_env)\n-            })\n-        }\n-    }\n-\n-    // This is a copy of `Layout::field`\n-    //\n-    // FIXME: remove once validation does not depend on lifetimes\n-    fn field_with_lifetimes(\n-        &mut self,\n-        base: Place,\n-        mut layout: ty::layout::TyLayout<'tcx>,\n-        i: usize,\n-    ) -> EvalResult<'tcx, Ty<'tcx>> {\n-        if let Place::Ptr { extra: PlaceExtra::DowncastVariant(variant_index), .. } = base {\n-            layout = layout.for_variant(&self, variant_index);\n-        }\n-        let tcx = self.tcx.tcx;\n-        Ok(match layout.ty.sty {\n-            ty::TyBool |\n-            ty::TyChar |\n-            ty::TyInt(_) |\n-            ty::TyUint(_) |\n-            ty::TyFloat(_) |\n-            ty::TyFnPtr(_) |\n-            ty::TyNever |\n-            ty::TyFnDef(..) |\n-            ty::TyGeneratorWitness(..) |\n-            ty::TyDynamic(..) |\n-            ty::TyForeign(..) => {\n-                bug!(\"TyLayout::field_type({:?}): not applicable\", layout)\n-            }\n-\n-            // Potentially-fat pointers.\n-            ty::TyRef(_, pointee, _) |\n-            ty::TyRawPtr(ty::TypeAndMut { ty: pointee, .. }) => {\n-                assert!(i < 2);\n-\n-                // Reuse the fat *T type as its own thin pointer data field.\n-                // This provides information about e.g. DST struct pointees\n-                // (which may have no non-DST form), and will work as long\n-                // as the `Abi` or `FieldPlacement` is checked by users.\n-                if i == 0 {\n-                    return Ok(layout.ty);\n-                }\n-\n-                match tcx.struct_tail(pointee).sty {\n-                    ty::TySlice(_) |\n-                    ty::TyStr => tcx.types.usize,\n-                    ty::TyDynamic(..) => {\n-                        // FIXME(eddyb) use an usize/fn() array with\n-                        // the correct number of vtables slots.\n-                        tcx.mk_imm_ref(tcx.types.re_static, tcx.mk_nil())\n-                    }\n-                    _ => bug!(\"TyLayout::field_type({:?}): not applicable\", layout)\n-                }\n-            }\n-\n-            // Arrays and slices.\n-            ty::TyArray(element, _) |\n-            ty::TySlice(element) => element,\n-            ty::TyStr => tcx.types.u8,\n-\n-            // Tuples, generators and closures.\n-            ty::TyClosure(def_id, ref substs) => {\n-                substs.upvar_tys(def_id, tcx).nth(i).unwrap()\n-            }\n-\n-            ty::TyGenerator(def_id, ref substs, _) => {\n-                substs.field_tys(def_id, tcx).nth(i).unwrap()\n-            }\n-\n-            ty::TyTuple(tys) => tys[i],\n-\n-            // SIMD vector types.\n-            ty::TyAdt(def, ..) if def.repr.simd() => {\n-                layout.ty.simd_type(tcx)\n-            }\n-\n-            // ADTs.\n-            ty::TyAdt(def, substs) => {\n-                use rustc::ty::layout::Variants;\n-                match layout.variants {\n-                    Variants::Single { index } => {\n-                        def.variants[index].fields[i].ty(tcx, substs)\n-                    }\n-\n-                    // Discriminant field for enums (where applicable).\n-                    Variants::Tagged { tag: ref discr, .. } |\n-                    Variants::NicheFilling { niche: ref discr, .. } => {\n-                        assert_eq!(i, 0);\n-                        return Ok(discr.value.to_ty(tcx))\n-                    }\n-                }\n-            }\n-\n-            ty::TyProjection(_) | ty::TyAnon(..) | ty::TyParam(_) |\n-            ty::TyInfer(_) | ty::TyError => {\n-                bug!(\"TyLayout::field_type: unexpected type `{}`\", layout.ty)\n-            }\n-        })\n-    }\n-\n-    fn validate_fields(\n-        &mut self,\n-        query: ValidationQuery<'tcx>,\n-        mode: ValidationMode,\n-    ) -> EvalResult<'tcx> {\n-        let mut layout = self.layout_of(query.ty)?;\n-        layout.ty = query.ty;\n-\n-        // TODO: Maybe take visibility/privacy into account.\n-        for idx in 0..layout.fields.count() {\n-            let field = mir::Field::new(idx);\n-            let (field_place, field_layout) =\n-                self.place_field(query.place.1, field, layout)?;\n-            // layout stuff erases lifetimes, get the field ourselves\n-            let field_ty = self.field_with_lifetimes(query.place.1, layout, idx)?;\n-            trace!(\"assuming \\n{:?}\\n == \\n{:?}\\n except for lifetimes\", field_layout.ty, field_ty);\n-            self.validate(\n-                ValidationQuery {\n-                    place: (query.place.0.clone().field(field), field_place),\n-                    ty: field_ty,\n-                    ..query\n-                },\n-                mode,\n-            )?;\n-        }\n-\n-        Ok(())\n-    }\n-\n-    fn validate_ptr(\n-        &mut self,\n-        val: Value,\n-        abs_place: AbsPlace<'tcx>,\n-        pointee_ty: Ty<'tcx>,\n-        re: Option<region::Scope>,\n-        mutbl: Mutability,\n-        mode: ValidationMode,\n-    ) -> EvalResult<'tcx> {\n-        // Check alignment and non-NULLness\n-        let (_, align) = self.size_and_align_of_dst(pointee_ty, val)?;\n-        let ptr = self.into_ptr(val)?.unwrap_or_err()?;\n-        self.memory.check_align(ptr, align)?;\n-\n-        // Recurse\n-        let pointee_place = self.val_to_place(val, pointee_ty)?;\n-        self.validate(\n-            ValidationQuery {\n-                place: (abs_place.deref(), pointee_place),\n-                ty: pointee_ty,\n-                re,\n-                mutbl,\n-            },\n-            mode,\n-        )\n-    }\n-\n-    /// Validate the place at the given type. If `acquire` is false, just do a release of all write locks\n-    fn validate(\n-        &mut self,\n-        mut query: ValidationQuery<'tcx>,\n-        mode: ValidationMode,\n-    ) -> EvalResult<'tcx> {\n-        use rustc::ty::TypeVariants::*;\n-        use rustc::ty::RegionKind::*;\n-        use rustc::ty::AdtKind;\n-\n-        // No point releasing shared stuff.\n-        if !mode.acquiring() && query.mutbl == MutImmutable {\n-            return Ok(());\n-        }\n-        // When we recover, we may see data whose validity *just* ended.  Do not acquire it.\n-        if let ValidationMode::Recover(ending_ce) = mode {\n-            if query.re == Some(ending_ce) {\n-                return Ok(());\n-            }\n-        }\n-\n-        query.ty = self.normalize_type_unerased(&query.ty);\n-        trace!(\"{:?} on {:#?}\", mode, query);\n-        trace!(\"{:#?}\", query.ty.sty);\n-\n-        // Decide whether this type *owns* the memory it covers (like integers), or whether it\n-        // just assembles pieces (that each own their memory) together to a larger whole.\n-        // TODO: Currently, we don't acquire locks for padding and discriminants. We should.\n-        let is_owning = match query.ty.sty {\n-            TyInt(_) | TyUint(_) | TyRawPtr(_) | TyBool | TyFloat(_) | TyChar | TyStr |\n-            TyRef(..) | TyFnPtr(..) | TyFnDef(..) | TyNever => true,\n-            TyAdt(adt, _) if adt.is_box() => true,\n-            TySlice(_) | TyAdt(_, _) | TyTuple(..) | TyClosure(..) | TyArray(..) |\n-            TyDynamic(..) | TyGenerator(..) | TyForeign(_) => false,\n-            TyGeneratorWitness(..) => unreachable!(\"TyGeneratorWitness in validate\"),\n-            TyParam(_) | TyInfer(_) | TyProjection(_) | TyAnon(..) | TyError => {\n-                bug!(\"I got an incomplete/unnormalized type for validation\")\n-            }\n-        };\n-        if is_owning {\n-            // We need to lock.  So we need memory.  So we have to force_acquire.\n-            // Tracking the same state for locals not backed by memory would just duplicate too\n-            // much machinery.\n-            // FIXME: We ignore alignment.\n-            let (ptr, _, extra) = self.force_allocation(query.place.1)?.to_ptr_align_extra();\n-            // Determine the size\n-            // FIXME: Can we reuse size_and_align_of_dst for Places?\n-            let layout = self.layout_of(query.ty)?;\n-            let len = if !layout.is_unsized() {\n-                assert_eq!(extra, PlaceExtra::None, \"Got a fat ptr to a sized type\");\n-                layout.size.bytes()\n-            } else {\n-                // The only unsized typ we concider \"owning\" is TyStr.\n-                assert_eq!(\n-                    query.ty.sty,\n-                    TyStr,\n-                    \"Found a surprising unsized owning type\"\n-                );\n-                // The extra must be the length, in bytes.\n-                match extra {\n-                    PlaceExtra::Length(len) => len,\n-                    _ => bug!(\"TyStr must have a length as extra\"),\n-                }\n-            };\n-            // Handle locking\n-            if len > 0 {\n-                let ptr = ptr.unwrap_or_err()?.to_ptr()?;\n-                match query.mutbl {\n-                    MutImmutable => {\n-                        if mode.acquiring() {\n-                            self.memory.acquire_lock(\n-                                ptr,\n-                                len,\n-                                query.re,\n-                                AccessKind::Read,\n-                            )?;\n-                        }\n-                    }\n-                    // No releasing of read locks, ever.\n-                    MutMutable => {\n-                        match mode {\n-                            ValidationMode::Acquire => {\n-                                self.memory.acquire_lock(\n-                                    ptr,\n-                                    len,\n-                                    query.re,\n-                                    AccessKind::Write,\n-                                )?\n-                            }\n-                            ValidationMode::Recover(ending_ce) => {\n-                                self.memory.recover_write_lock(\n-                                    ptr,\n-                                    len,\n-                                    &query.place.0,\n-                                    query.re,\n-                                    ending_ce,\n-                                )?\n-                            }\n-                            ValidationMode::ReleaseUntil(suspended_ce) => {\n-                                self.memory.suspend_write_lock(\n-                                    ptr,\n-                                    len,\n-                                    &query.place.0,\n-                                    suspended_ce,\n-                                )?\n-                            }\n-                        }\n-                    }\n-                }\n-            }\n-        }\n-\n-        let res: EvalResult<'tcx> = do catch {\n-            match query.ty.sty {\n-                TyInt(_) | TyUint(_) | TyRawPtr(_) => {\n-                    if mode.acquiring() {\n-                        // Make sure we can read this.\n-                        let val = self.read_place(query.place.1)?;\n-                        self.follow_by_ref_value(val, query.ty)?;\n-                        // FIXME: It would be great to rule out Undef here, but that doesn't actually work.\n-                        // Passing around undef data is a thing that e.g. Vec::extend_with does.\n-                    }\n-                }\n-                TyBool | TyFloat(_) | TyChar => {\n-                    if mode.acquiring() {\n-                        let val = self.read_place(query.place.1)?;\n-                        let val = self.value_to_scalar(ValTy { value: val, ty: query.ty })?;\n-                        val.to_bytes()?;\n-                        // TODO: Check if these are valid bool/float/codepoint/UTF-8\n-                    }\n-                }\n-                TyNever => return err!(ValidationFailure(format!(\"The empty type is never valid.\"))),\n-                TyRef(region, pointee_ty, mutbl) => {\n-                    let val = self.read_place(query.place.1)?;\n-                    // Sharing restricts our context\n-                    if mutbl == MutImmutable {\n-                        query.mutbl = MutImmutable;\n-                    }\n-                    // Inner lifetimes *outlive* outer ones, so only if we have no lifetime restriction yet,\n-                    // we record the region of this borrow to the context.\n-                    if query.re == None {\n-                        if let ReScope(scope) = *region {\n-                            query.re = Some(scope);\n-                        }\n-                        // It is possible for us to encounter erased lifetimes here because the lifetimes in\n-                        // this functions' Subst will be erased.\n-                    }\n-                    self.validate_ptr(val, query.place.0, pointee_ty, query.re, query.mutbl, mode)?;\n-                }\n-                TyAdt(adt, _) if adt.is_box() => {\n-                    let val = self.read_place(query.place.1)?;\n-                    self.validate_ptr(val, query.place.0, query.ty.boxed_ty(), query.re, query.mutbl, mode)?;\n-                }\n-                TyFnPtr(_sig) => {\n-                    let ptr = self.read_place(query.place.1)?;\n-                    let ptr = self.into_ptr(ptr)?.unwrap_or_err()?.to_ptr()?;\n-                    self.memory.get_fn(ptr)?;\n-                    // TODO: Check if the signature matches (should be the same check as what terminator/mod.rs already does on call?).\n-                }\n-                TyFnDef(..) => {\n-                    // This is a zero-sized type with all relevant data sitting in the type.\n-                    // There is nothing to validate.\n-                }\n-\n-                // Compound types\n-                TyStr => {\n-                    // TODO: Validate strings\n-                }\n-                TySlice(elem_ty) => {\n-                    let len = match query.place.1 {\n-                        Place::Ptr { extra: PlaceExtra::Length(len), .. } => len,\n-                        _ => {\n-                            bug!(\n-                                \"acquire_valid of a TySlice given non-slice place: {:?}\",\n-                                query.place\n-                            )\n-                        }\n-                    };\n-                    for i in 0..len {\n-                        let inner_place = self.place_index(query.place.1, query.ty, i)?;\n-                        self.validate(\n-                            ValidationQuery {\n-                                place: (query.place.0.clone().index(i), inner_place),\n-                                ty: elem_ty,\n-                                ..query\n-                            },\n-                            mode,\n-                        )?;\n-                    }\n-                }\n-                TyArray(elem_ty, len) => {\n-                    let len = match len.val {\n-                        ConstValue::Unevaluated(def_id, substs) => {\n-                            self.tcx.const_eval(self.tcx.param_env(def_id).and(GlobalId {\n-                                instance: Instance::new(def_id, substs),\n-                                promoted: None,\n-                            }))\n-                                .map_err(|_err|EvalErrorKind::MachineError(\"<already reported>\".to_string()))?\n-                        }\n-                        _ => len,\n-                    };\n-                    let len = len.unwrap_usize(self.tcx.tcx);\n-                    for i in 0..len {\n-                        let inner_place = self.place_index(query.place.1, query.ty, i as u64)?;\n-                        self.validate(\n-                            ValidationQuery {\n-                                place: (query.place.0.clone().index(i as u64), inner_place),\n-                                ty: elem_ty,\n-                                ..query\n-                            },\n-                            mode,\n-                        )?;\n-                    }\n-                }\n-                TyDynamic(_data, _region) => {\n-                    // Check that this is a valid vtable\n-                    let vtable = match query.place.1 {\n-                        Place::Ptr { extra: PlaceExtra::Vtable(vtable), .. } => vtable,\n-                        _ => {\n-                            bug!(\n-                                \"acquire_valid of a TyDynamic given non-trait-object place: {:?}\",\n-                                query.place\n-                            )\n-                        }\n-                    };\n-                    self.read_size_and_align_from_vtable(vtable)?;\n-                    // TODO: Check that the vtable contains all the function pointers we expect it to have.\n-                    // Trait objects cannot have any operations performed\n-                    // on them directly.  We cannot, in general, even acquire any locks as the trait object *could*\n-                    // contain an UnsafeCell.  If we call functions to get access to data, we will validate\n-                    // their return values.  So, it doesn't seem like there's anything else to do.\n-                }\n-                TyAdt(adt, _) => {\n-                    if Some(adt.did) == self.tcx.tcx.lang_items().unsafe_cell_type() &&\n-                        query.mutbl == MutImmutable\n-                    {\n-                        // No locks for shared unsafe cells.  Also no other validation, the only field is private anyway.\n-                        return Ok(());\n-                    }\n-\n-                    match adt.adt_kind() {\n-                        AdtKind::Enum => {\n-                            let layout = self.layout_of(query.ty)?;\n-                            let variant_idx = self.read_discriminant_as_variant_index(query.place.1, layout)?;\n-                            let variant = &adt.variants[variant_idx];\n-\n-                            if !variant.fields.is_empty() {\n-                                // Downcast to this variant, if needed\n-                                let place = if adt.is_enum() {\n-                                    (\n-                                        query.place.0.downcast(adt, variant_idx),\n-                                        self.eval_place_projection(\n-                                            query.place.1,\n-                                            query.ty,\n-                                            &mir::ProjectionElem::Downcast(adt, variant_idx),\n-                                        )?,\n-                                    )\n-                                } else {\n-                                    query.place\n-                                };\n-\n-                                // Recursively validate the fields\n-                                self.validate_fields(\n-                                    ValidationQuery { place, ..query },\n-                                    mode,\n-                                )?;\n-                            } else {\n-                                // No fields, nothing left to check.  Downcasting may fail, e.g. in case of a CEnum.\n-                            }\n-                        }\n-                        AdtKind::Struct => {\n-                            self.validate_fields(query, mode)?;\n-                        }\n-                        AdtKind::Union => {\n-                            // No guarantees are provided for union types.\n-                            // TODO: Make sure that all access to union fields is unsafe; otherwise, we may have some checking to do (but what exactly?)\n-                        }\n-                    }\n-                }\n-                TyTuple(..) |\n-                TyClosure(..) => {\n-                    // TODO: Check if the signature matches for `TyClosure`\n-                    // (should be the same check as what terminator/mod.rs already does on call?).\n-                    // Is there other things we can/should check?  Like vtable pointers?\n-                    self.validate_fields(query, mode)?;\n-                }\n-                // FIXME: generators aren't validated right now\n-                TyGenerator(..) => {},\n-                _ => bug!(\"We already established that this is a type we support. ({})\", query.ty),\n-            }\n-        };\n-        match res {\n-            // ReleaseUntil(None) of an uninitalized variable is a NOP.  This is needed because\n-            // we have to release the return value of a function; due to destination-passing-style\n-            // the callee may directly write there.\n-            // TODO: Ideally we would know whether the destination is already initialized, and only\n-            // release if it is.  But of course that can't even always be statically determined.\n-            Err(EvalError { kind: EvalErrorKind::ReadUndefBytes, .. })\n-                if mode == ValidationMode::ReleaseUntil(None) => {\n-                Ok(())\n-            }\n-            res => res,\n-        }\n-    }\n-}"}, {"sha": "bd08c0ce4eb9d97175076854c8ff4eba01a6cb46", "filename": "tests/compile-fail/invalid_bool.rs", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/752accf4e48b83545fa2fcf205602f239389361d/tests%2Fcompile-fail%2Finvalid_bool.rs", "raw_url": "https://github.com/rust-lang/rust/raw/752accf4e48b83545fa2fcf205602f239389361d/tests%2Fcompile-fail%2Finvalid_bool.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/tests%2Fcompile-fail%2Finvalid_bool.rs?ref=752accf4e48b83545fa2fcf205602f239389361d", "patch": "@@ -1,4 +1,6 @@\n //ignore-test FIXME (do some basic validation of invariants for all values in flight)\n+//This does currently not get caught becuase it compiles to SwitchInt, which\n+//has no knowledge about data invariants.\n \n fn main() {\n     let b = unsafe { std::mem::transmute::<u8, bool>(2) };"}, {"sha": "47c4e8b410ebe89854c03ad71709f5ec6ba1e70a", "filename": "tests/compile-fail/invalid_bool2.rs", "status": "added", "additions": 4, "deletions": 0, "changes": 4, "blob_url": "https://github.com/rust-lang/rust/blob/752accf4e48b83545fa2fcf205602f239389361d/tests%2Fcompile-fail%2Finvalid_bool2.rs", "raw_url": "https://github.com/rust-lang/rust/raw/752accf4e48b83545fa2fcf205602f239389361d/tests%2Fcompile-fail%2Finvalid_bool2.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/tests%2Fcompile-fail%2Finvalid_bool2.rs?ref=752accf4e48b83545fa2fcf205602f239389361d", "patch": "@@ -0,0 +1,4 @@\n+fn main() {\n+    let b = unsafe { std::mem::transmute::<u8, bool>(2) };\n+    let _x = b == true; //~ ERROR invalid boolean value read\n+}"}, {"sha": "a188623a1e0ef16baf940a5928aad3e6c024bf42", "filename": "tests/compile-fail/invalid_enum_discriminant.rs", "status": "modified", "additions": 2, "deletions": 3, "changes": 5, "blob_url": "https://github.com/rust-lang/rust/blob/752accf4e48b83545fa2fcf205602f239389361d/tests%2Fcompile-fail%2Finvalid_enum_discriminant.rs", "raw_url": "https://github.com/rust-lang/rust/raw/752accf4e48b83545fa2fcf205602f239389361d/tests%2Fcompile-fail%2Finvalid_enum_discriminant.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/tests%2Fcompile-fail%2Finvalid_enum_discriminant.rs?ref=752accf4e48b83545fa2fcf205602f239389361d", "patch": "@@ -8,11 +8,10 @@ pub enum Foo {\n \n fn main() {\n     let f = unsafe { std::mem::transmute::<i32, Foo>(42) };\n-    match f {\n+    match f { //~ ERROR invalid enum discriminant\n         Foo::A => {},\n         Foo::B => {},\n         Foo::C => {},\n         Foo::D => {},\n     }\n-} //~ ERROR constant evaluation error\n-//~^ NOTE entered unreachable code\n+}"}, {"sha": "5a5a20c48695372d2892b7ecfb9eaca330be6200", "filename": "tests/compile-fail/invalid_enum_discriminant2.rs", "status": "added", "additions": 16, "deletions": 0, "changes": 16, "blob_url": "https://github.com/rust-lang/rust/blob/752accf4e48b83545fa2fcf205602f239389361d/tests%2Fcompile-fail%2Finvalid_enum_discriminant2.rs", "raw_url": "https://github.com/rust-lang/rust/raw/752accf4e48b83545fa2fcf205602f239389361d/tests%2Fcompile-fail%2Finvalid_enum_discriminant2.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/tests%2Fcompile-fail%2Finvalid_enum_discriminant2.rs?ref=752accf4e48b83545fa2fcf205602f239389361d", "patch": "@@ -0,0 +1,16 @@\n+// Validation makes this fail in the wrong place\n+// compile-flags: -Zmir-emit-validate=0\n+\n+// error-pattern: invalid enum discriminant\n+\n+use std::mem;\n+\n+#[repr(C)]\n+pub enum Foo {\n+    A, B, C, D\n+}\n+\n+fn main() {\n+    let f = unsafe { std::mem::transmute::<i32, Foo>(42) };\n+    let _ = mem::discriminant(&f);\n+}"}, {"sha": "fd4431992d9b603909434a3639c3f76629b8efd6", "filename": "tests/compile-fail/match_char.rs", "status": "modified", "additions": 7, "deletions": 5, "changes": 12, "blob_url": "https://github.com/rust-lang/rust/blob/752accf4e48b83545fa2fcf205602f239389361d/tests%2Fcompile-fail%2Fmatch_char.rs", "raw_url": "https://github.com/rust-lang/rust/raw/752accf4e48b83545fa2fcf205602f239389361d/tests%2Fcompile-fail%2Fmatch_char.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/tests%2Fcompile-fail%2Fmatch_char.rs?ref=752accf4e48b83545fa2fcf205602f239389361d", "patch": "@@ -1,11 +1,13 @@\n // ignore-test FIXME: we are not checking these things on match any more?\n+//This does currently not get caught becuase it compiles to SwitchInt, which\n+//has no knowledge about data invariants.\n \n fn main() {\n     assert!(std::char::from_u32(-1_i32 as u32).is_none());\n-    match unsafe { std::mem::transmute::<i32, char>(-1) } { //~ ERROR constant evaluation error\n+    let _ = match unsafe { std::mem::transmute::<i32, char>(-1) } { //~ ERROR constant evaluation error\n         //~^ NOTE tried to interpret an invalid 32-bit value as a char: 4294967295\n-        'a' => {},\n-        'b' => {},\n-        _ => {},\n-    }\n+        'a' => {true},\n+        'b' => {false},\n+        _ => {true},\n+    };\n }"}, {"sha": "786dd813a1eb9bb4e65ff4d5329082e2e1a70f80", "filename": "tests/compile-fail/match_char2.rs", "status": "added", "additions": 5, "deletions": 0, "changes": 5, "blob_url": "https://github.com/rust-lang/rust/blob/752accf4e48b83545fa2fcf205602f239389361d/tests%2Fcompile-fail%2Fmatch_char2.rs", "raw_url": "https://github.com/rust-lang/rust/raw/752accf4e48b83545fa2fcf205602f239389361d/tests%2Fcompile-fail%2Fmatch_char2.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/tests%2Fcompile-fail%2Fmatch_char2.rs?ref=752accf4e48b83545fa2fcf205602f239389361d", "patch": "@@ -0,0 +1,5 @@\n+fn main() {\n+    assert!(std::char::from_u32(-1_i32 as u32).is_none());\n+    let c = unsafe { std::mem::transmute::<i32, char>(-1) };\n+    let _x = c == 'x'; //~ ERROR tried to interpret an invalid 32-bit value as a char\n+}"}, {"sha": "06e01d7aea3a52458ec55e6e521c2e0d165c9edb", "filename": "tests/compile-fail/memleak.rs", "status": "modified", "additions": 3, "deletions": 1, "changes": 4, "blob_url": "https://github.com/rust-lang/rust/blob/752accf4e48b83545fa2fcf205602f239389361d/tests%2Fcompile-fail%2Fmemleak.rs", "raw_url": "https://github.com/rust-lang/rust/raw/752accf4e48b83545fa2fcf205602f239389361d/tests%2Fcompile-fail%2Fmemleak.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/tests%2Fcompile-fail%2Fmemleak.rs?ref=752accf4e48b83545fa2fcf205602f239389361d", "patch": "@@ -1,4 +1,6 @@\n-// ignore-test FIXME: leak detection is disabled\n+// ignore-windows: We do not check leaks on Windows\n+// ignore-macos: We do not check leaks on macOS\n+\n //error-pattern: the evaluated program leaked memory\n \n fn main() {"}, {"sha": "14a85ecd8947cd2c051013fe7678547f4c5dc448", "filename": "tests/compile-fail/memleak_rc.rs", "status": "modified", "additions": 3, "deletions": 1, "changes": 4, "blob_url": "https://github.com/rust-lang/rust/blob/752accf4e48b83545fa2fcf205602f239389361d/tests%2Fcompile-fail%2Fmemleak_rc.rs", "raw_url": "https://github.com/rust-lang/rust/raw/752accf4e48b83545fa2fcf205602f239389361d/tests%2Fcompile-fail%2Fmemleak_rc.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/tests%2Fcompile-fail%2Fmemleak_rc.rs?ref=752accf4e48b83545fa2fcf205602f239389361d", "patch": "@@ -1,4 +1,6 @@\n-// ignore-test FIXME: leak detection is disabled\n+// ignore-windows: We do not check leaks on Windows\n+// ignore-macos: We do not check leaks on macOS\n+\n //error-pattern: the evaluated program leaked memory\n \n use std::rc::Rc;"}, {"sha": "ba46651f58ee4f25e90a2c9a2820d157a45fa142", "filename": "tests/compile-fail/modifying_constants.rs", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "blob_url": "https://github.com/rust-lang/rust/blob/752accf4e48b83545fa2fcf205602f239389361d/tests%2Fcompile-fail%2Fmodifying_constants.rs", "raw_url": "https://github.com/rust-lang/rust/raw/752accf4e48b83545fa2fcf205602f239389361d/tests%2Fcompile-fail%2Fmodifying_constants.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/tests%2Fcompile-fail%2Fmodifying_constants.rs?ref=752accf4e48b83545fa2fcf205602f239389361d", "patch": "@@ -1,4 +1,3 @@\n-// ignore-test FIXME: we are not making these statics read-only any more?\n \n fn main() {\n     let x = &1; // the `&1` is promoted to a constant, but it used to be that only the pointer is marked static, not the pointee"}, {"sha": "5645217fe1ff04fed17b4fe7d0a1a64dfa116f78", "filename": "tests/compile-fail/stack_limit.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/752accf4e48b83545fa2fcf205602f239389361d/tests%2Fcompile-fail%2Fstack_limit.rs", "raw_url": "https://github.com/rust-lang/rust/raw/752accf4e48b83545fa2fcf205602f239389361d/tests%2Fcompile-fail%2Fstack_limit.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/tests%2Fcompile-fail%2Fstack_limit.rs?ref=752accf4e48b83545fa2fcf205602f239389361d", "patch": "@@ -1,4 +1,4 @@\n-#![feature(custom_attribute, attr_literals)]\n+#![feature(custom_attribute)]\n #![miri(stack_limit=16)]\n \n //error-pattern: reached the configured maximum number of stack frames"}, {"sha": "9e39c2c01c2b20cfdc674424c343de86ada771f7", "filename": "tests/compile-fail/static_memory_modification.rs", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "blob_url": "https://github.com/rust-lang/rust/blob/752accf4e48b83545fa2fcf205602f239389361d/tests%2Fcompile-fail%2Fstatic_memory_modification.rs", "raw_url": "https://github.com/rust-lang/rust/raw/752accf4e48b83545fa2fcf205602f239389361d/tests%2Fcompile-fail%2Fstatic_memory_modification.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/tests%2Fcompile-fail%2Fstatic_memory_modification.rs?ref=752accf4e48b83545fa2fcf205602f239389361d", "patch": "@@ -1,4 +1,3 @@\n-// ignore-test FIXME: we are not making these statics read-only any more?\n static X: usize = 5;\n \n #[allow(mutable_transmutes)]"}, {"sha": "f920bc52edde3b91ae378c5d9ae4bac13cd963e6", "filename": "tests/run-pass-fullmir/threads.rs", "status": "added", "additions": 19, "deletions": 0, "changes": 19, "blob_url": "https://github.com/rust-lang/rust/blob/752accf4e48b83545fa2fcf205602f239389361d/tests%2Frun-pass-fullmir%2Fthreads.rs", "raw_url": "https://github.com/rust-lang/rust/raw/752accf4e48b83545fa2fcf205602f239389361d/tests%2Frun-pass-fullmir%2Fthreads.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/tests%2Frun-pass-fullmir%2Fthreads.rs?ref=752accf4e48b83545fa2fcf205602f239389361d", "patch": "@@ -0,0 +1,19 @@\n+// Just instantiate some data structures to make sure we got all their foreign items covered.\n+// Requires full MIR on Windows.\n+\n+use std::sync;\n+\n+fn main() {\n+    let m = sync::Mutex::new(0);\n+    let _ = m.lock();\n+    drop(m);\n+\n+    // We don't provide RwLock on Windows\n+    #[cfg(not(target_os = \"windows\"))]\n+    {\n+        let rw = sync::RwLock::new(0);\n+        let _ = rw.read();\n+        let _ = rw.write();\n+        drop(rw);\n+    }\n+}"}, {"sha": "381169505ec9f50b5bfca110635935372e24ee4a", "filename": "tests/run-pass-fullmir/vecdeque.rs", "status": "added", "additions": 15, "deletions": 0, "changes": 15, "blob_url": "https://github.com/rust-lang/rust/blob/752accf4e48b83545fa2fcf205602f239389361d/tests%2Frun-pass-fullmir%2Fvecdeque.rs", "raw_url": "https://github.com/rust-lang/rust/raw/752accf4e48b83545fa2fcf205602f239389361d/tests%2Frun-pass-fullmir%2Fvecdeque.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/tests%2Frun-pass-fullmir%2Fvecdeque.rs?ref=752accf4e48b83545fa2fcf205602f239389361d", "patch": "@@ -0,0 +1,15 @@\n+use std::collections::VecDeque;\n+\n+fn main() {\n+    let mut dst = VecDeque::new();\n+    dst.push_front(Box::new(1));\n+    dst.push_front(Box::new(2));\n+    dst.pop_back();\n+\n+    let mut src = VecDeque::new();\n+    src.push_front(Box::new(2));\n+    dst.append(&mut src);\n+    for a in dst {\n+      assert_eq!(*a, 2);\n+    }\n+}"}, {"sha": "ec8e16d33e42b7eba0f1137299bf651f3608b3ae", "filename": "tests/run-pass/atomic-compare_exchange.rs", "status": "modified", "additions": 11, "deletions": 7, "changes": 18, "blob_url": "https://github.com/rust-lang/rust/blob/752accf4e48b83545fa2fcf205602f239389361d/tests%2Frun-pass%2Fatomic-compare_exchange.rs", "raw_url": "https://github.com/rust-lang/rust/raw/752accf4e48b83545fa2fcf205602f239389361d/tests%2Frun-pass%2Fatomic-compare_exchange.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/tests%2Frun-pass%2Fatomic-compare_exchange.rs?ref=752accf4e48b83545fa2fcf205602f239389361d", "patch": "@@ -15,18 +15,22 @@ static ATOMIC: AtomicIsize = ATOMIC_ISIZE_INIT;\n \n fn main() {\n     // Make sure trans can emit all the intrinsics correctly\n-    ATOMIC.compare_exchange(0, 1, Relaxed, Relaxed).ok();\n-    ATOMIC.compare_exchange(0, 1, Acquire, Relaxed).ok();\n-    ATOMIC.compare_exchange(0, 1, Release, Relaxed).ok();\n-    ATOMIC.compare_exchange(0, 1, AcqRel, Relaxed).ok();\n+    assert_eq!(ATOMIC.compare_exchange(0, 1, Relaxed, Relaxed), Ok(0));\n+    assert_eq!(ATOMIC.compare_exchange(0, 2, Acquire, Relaxed), Err(1));\n+    assert_eq!(ATOMIC.compare_exchange(0, 1, Release, Relaxed), Err(1));\n+    assert_eq!(ATOMIC.compare_exchange(1, 0, AcqRel, Relaxed), Ok(1));\n     ATOMIC.compare_exchange(0, 1, SeqCst, Relaxed).ok();\n     ATOMIC.compare_exchange(0, 1, Acquire, Acquire).ok();\n     ATOMIC.compare_exchange(0, 1, AcqRel, Acquire).ok();\n     ATOMIC.compare_exchange(0, 1, SeqCst, Acquire).ok();\n     ATOMIC.compare_exchange(0, 1, SeqCst, SeqCst).ok();\n-    ATOMIC.compare_exchange_weak(0, 1, Relaxed, Relaxed).ok();\n-    ATOMIC.compare_exchange_weak(0, 1, Acquire, Relaxed).ok();\n-    ATOMIC.compare_exchange_weak(0, 1, Release, Relaxed).ok();\n+\n+    ATOMIC.store(0, SeqCst);\n+\n+    assert_eq!(ATOMIC.compare_exchange_weak(0, 1, Relaxed, Relaxed), Ok(0));\n+    assert_eq!(ATOMIC.compare_exchange_weak(0, 2, Acquire, Relaxed), Err(1));\n+    assert_eq!(ATOMIC.compare_exchange_weak(0, 1, Release, Relaxed), Err(1));\n+    assert_eq!(ATOMIC.compare_exchange_weak(1, 0, AcqRel, Relaxed), Ok(1));\n     ATOMIC.compare_exchange_weak(0, 1, AcqRel, Relaxed).ok();\n     ATOMIC.compare_exchange_weak(0, 1, SeqCst, Relaxed).ok();\n     ATOMIC.compare_exchange_weak(0, 1, Acquire, Acquire).ok();"}, {"sha": "30fc14704d9d0aece08f0e3b5a5067bc7249586d", "filename": "tests/run-pass/bools.rs", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "blob_url": "https://github.com/rust-lang/rust/blob/752accf4e48b83545fa2fcf205602f239389361d/tests%2Frun-pass%2Fbools.rs", "raw_url": "https://github.com/rust-lang/rust/raw/752accf4e48b83545fa2fcf205602f239389361d/tests%2Frun-pass%2Fbools.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/tests%2Frun-pass%2Fbools.rs?ref=752accf4e48b83545fa2fcf205602f239389361d", "patch": "@@ -25,4 +25,5 @@ fn main() {\n     assert_eq!(if_false(), 0);\n     assert_eq!(if_true(), 1);\n     assert_eq!(match_bool(), 1);\n+    assert_eq!(true == true, true);\n }"}, {"sha": "b37cd8408ba10ae32b653c169b862b3fee49ee40", "filename": "tests/run-pass/extern_types.rs", "status": "added", "additions": 10, "deletions": 0, "changes": 10, "blob_url": "https://github.com/rust-lang/rust/blob/752accf4e48b83545fa2fcf205602f239389361d/tests%2Frun-pass%2Fextern_types.rs", "raw_url": "https://github.com/rust-lang/rust/raw/752accf4e48b83545fa2fcf205602f239389361d/tests%2Frun-pass%2Fextern_types.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/tests%2Frun-pass%2Fextern_types.rs?ref=752accf4e48b83545fa2fcf205602f239389361d", "patch": "@@ -0,0 +1,10 @@\n+#![feature(extern_types)]\n+\n+extern {\n+    type Foo;\n+}\n+\n+fn main() {\n+    let x: &Foo = unsafe { &*(16 as *const Foo) };\n+    let _y: &Foo = &*x;\n+}"}, {"sha": "39fdbce49202efd3b5d639bd9304c0eb1923a4ff", "filename": "tests/run-pass/floats.rs", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "blob_url": "https://github.com/rust-lang/rust/blob/752accf4e48b83545fa2fcf205602f239389361d/tests%2Frun-pass%2Ffloats.rs", "raw_url": "https://github.com/rust-lang/rust/raw/752accf4e48b83545fa2fcf205602f239389361d/tests%2Frun-pass%2Ffloats.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/tests%2Frun-pass%2Ffloats.rs?ref=752accf4e48b83545fa2fcf205602f239389361d", "patch": "@@ -8,4 +8,8 @@ fn main() {\n     let x: u64 = unsafe { std::mem::transmute(42.0_f64) };\n     let y: f64 = unsafe { std::mem::transmute(x) };\n     assert_eq!(y, 42.0_f64);\n+\n+    assert_eq!(5.0f32 as u32, 5);\n+    assert_eq!(5.0f32 as i32, 5);\n+    assert_eq!(-5.0f32 as i32, -5);\n }"}, {"sha": "650c51ba5d997bfce1463fd97996e49eed0c6b50", "filename": "tests/run-pass/unops.rs", "status": "added", "additions": 5, "deletions": 0, "changes": 5, "blob_url": "https://github.com/rust-lang/rust/blob/752accf4e48b83545fa2fcf205602f239389361d/tests%2Frun-pass%2Funops.rs", "raw_url": "https://github.com/rust-lang/rust/raw/752accf4e48b83545fa2fcf205602f239389361d/tests%2Frun-pass%2Funops.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/tests%2Frun-pass%2Funops.rs?ref=752accf4e48b83545fa2fcf205602f239389361d", "patch": "@@ -0,0 +1,5 @@\n+fn main() {\n+    assert_eq!(!true, false);\n+    assert_eq!(!0xFFu16, 0xFF00);\n+    assert_eq!(-{1i16}, -1i16);\n+}"}]}