{"sha": "679245769b6984ec5a7edf70fb4744d8411468b8", "node_id": "MDY6Q29tbWl0NzI0NzEyOjY3OTI0NTc2OWI2OTg0ZWM1YTdlZGY3MGZiNDc0NGQ4NDExNDY4Yjg=", "commit": {"author": {"name": "Vytautas Astrauskas", "email": "astrauv@amazon.com", "date": "2020-04-21T23:38:14Z"}, "committer": {"name": "Vytautas Astrauskas", "email": "vastrauskas@gmail.com", "date": "2020-05-24T22:02:54Z"}, "message": "Implement support for synchronization primitives.", "tree": {"sha": "484e499c2afb03256fe737ac812ee9c2970ceeb6", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/484e499c2afb03256fe737ac812ee9c2970ceeb6"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/679245769b6984ec5a7edf70fb4744d8411468b8", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/679245769b6984ec5a7edf70fb4744d8411468b8", "html_url": "https://github.com/rust-lang/rust/commit/679245769b6984ec5a7edf70fb4744d8411468b8", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/679245769b6984ec5a7edf70fb4744d8411468b8/comments", "author": null, "committer": {"login": "vakaras", "id": 75347, "node_id": "MDQ6VXNlcjc1MzQ3", "avatar_url": "https://avatars.githubusercontent.com/u/75347?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vakaras", "html_url": "https://github.com/vakaras", "followers_url": "https://api.github.com/users/vakaras/followers", "following_url": "https://api.github.com/users/vakaras/following{/other_user}", "gists_url": "https://api.github.com/users/vakaras/gists{/gist_id}", "starred_url": "https://api.github.com/users/vakaras/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vakaras/subscriptions", "organizations_url": "https://api.github.com/users/vakaras/orgs", "repos_url": "https://api.github.com/users/vakaras/repos", "events_url": "https://api.github.com/users/vakaras/events{/privacy}", "received_events_url": "https://api.github.com/users/vakaras/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "726373fcaa8a94c1dea242992a19bb7d58f94df4", "url": "https://api.github.com/repos/rust-lang/rust/commits/726373fcaa8a94c1dea242992a19bb7d58f94df4", "html_url": "https://github.com/rust-lang/rust/commit/726373fcaa8a94c1dea242992a19bb7d58f94df4"}], "stats": {"total": 1548, "additions": 1276, "deletions": 272}, "files": [{"sha": "30901a8f127f48fdfd5cfcc0bc5e0dd6b6ed099f", "filename": "src/eval.rs", "status": "modified", "additions": 6, "deletions": 0, "changes": 6, "blob_url": "https://github.com/rust-lang/rust/blob/679245769b6984ec5a7edf70fb4744d8411468b8/src%2Feval.rs", "raw_url": "https://github.com/rust-lang/rust/raw/679245769b6984ec5a7edf70fb4744d8411468b8/src%2Feval.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Feval.rs?ref=679245769b6984ec5a7edf70fb4744d8411468b8", "patch": "@@ -210,6 +210,12 @@ pub fn eval_main<'tcx>(tcx: TyCtxt<'tcx>, main_id: DefId, config: MiriConfig) ->\n                 SchedulingAction::ExecuteStep => {\n                     assert!(ecx.step()?, \"a terminated thread was scheduled for execution\");\n                 }\n+                SchedulingAction::ExecuteCallback => {\n+                    assert!(ecx.machine.communicate,\n+                        \"scheduler callbacks require disabled isolation, but the code \\\n+                        that created the callback did not check it\");\n+                    ecx.run_scheduler_callback()?;\n+                }\n                 SchedulingAction::ExecuteDtors => {\n                     // This will either enable the thread again (so we go back\n                     // to `ExecuteStep`), or determine that this thread is done"}, {"sha": "e79fc2add39e1e2152957ad203f2eb0ec71a5e0f", "filename": "src/lib.rs", "status": "modified", "additions": 5, "deletions": 1, "changes": 6, "blob_url": "https://github.com/rust-lang/rust/blob/679245769b6984ec5a7edf70fb4744d8411468b8/src%2Flib.rs", "raw_url": "https://github.com/rust-lang/rust/raw/679245769b6984ec5a7edf70fb4744d8411468b8/src%2Flib.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flib.rs?ref=679245769b6984ec5a7edf70fb4744d8411468b8", "patch": "@@ -31,6 +31,7 @@ mod operator;\n mod range_map;\n mod shims;\n mod stacked_borrows;\n+mod sync;\n mod thread;\n \n // Make all those symbols available in the same place as our own.\n@@ -45,7 +46,7 @@ pub use crate::shims::fs::{DirHandler, EvalContextExt as FileEvalContextExt, Fil\n pub use crate::shims::intrinsics::EvalContextExt as IntrinsicsEvalContextExt;\n pub use crate::shims::os_str::EvalContextExt as OsStrEvalContextExt;\n pub use crate::shims::panic::{CatchUnwindData, EvalContextExt as PanicEvalContextExt};\n-pub use crate::shims::sync::{EvalContextExt as SyncEvalContextExt};\n+pub use crate::shims::sync::{EvalContextExt as SyncShimsEvalContextExt};\n pub use crate::shims::thread::EvalContextExt as ThreadShimsEvalContextExt;\n pub use crate::shims::time::EvalContextExt as TimeEvalContextExt;\n pub use crate::shims::tls::{EvalContextExt as TlsEvalContextExt, TlsData};\n@@ -70,6 +71,9 @@ pub use crate::stacked_borrows::{\n pub use crate::thread::{\n     EvalContextExt as ThreadsEvalContextExt, SchedulingAction, ThreadId, ThreadManager, ThreadState,\n };\n+pub use crate::sync::{\n+    EvalContextExt as SyncEvalContextExt, CondvarId, MutexId, RwLockId\n+};\n \n /// Insert rustc arguments at the beginning of the argument list that Miri wants to be\n /// set per default, for maximal validation power."}, {"sha": "4fb08cd259b675846708ca294798e60d51559c5b", "filename": "src/machine.rs", "status": "modified", "additions": 7, "deletions": 1, "changes": 8, "blob_url": "https://github.com/rust-lang/rust/blob/679245769b6984ec5a7edf70fb4744d8411468b8/src%2Fmachine.rs", "raw_url": "https://github.com/rust-lang/rust/raw/679245769b6984ec5a7edf70fb4744d8411468b8/src%2Fmachine.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Fmachine.rs?ref=679245769b6984ec5a7edf70fb4744d8411468b8", "patch": "@@ -5,7 +5,7 @@ use std::borrow::Cow;\n use std::cell::RefCell;\n use std::num::NonZeroU64;\n use std::rc::Rc;\n-use std::time::Instant;\n+use std::time::{Instant, SystemTime};\n use std::fmt;\n \n use log::trace;\n@@ -251,6 +251,11 @@ pub struct Evaluator<'mir, 'tcx> {\n     /// The \"time anchor\" for this machine's monotone clock (for `Instant` simulation).\n     pub(crate) time_anchor: Instant,\n \n+    /// The approximate system time when \"time anchor\" was created. This is used\n+    /// for converting system time to monotone time so that we can simplify the\n+    /// thread scheduler to deal only with a single representation of time.\n+    pub(crate) time_anchor_timestamp: SystemTime,\n+\n     /// The set of threads.\n     pub(crate) threads: ThreadManager<'mir, 'tcx>,\n \n@@ -281,6 +286,7 @@ impl<'mir, 'tcx> Evaluator<'mir, 'tcx> {\n             dir_handler: Default::default(),\n             panic_payload: None,\n             time_anchor: Instant::now(),\n+            time_anchor_timestamp: SystemTime::now(),\n             layouts,\n             threads: ThreadManager::default(),\n         }"}, {"sha": "352e38113abbeeb53ff8bc168b4ab578487ae529", "filename": "src/shims/foreign_items/posix.rs", "status": "modified", "additions": 39, "deletions": 9, "changes": 48, "blob_url": "https://github.com/rust-lang/rust/blob/679245769b6984ec5a7edf70fb4744d8411468b8/src%2Fshims%2Fforeign_items%2Fposix.rs", "raw_url": "https://github.com/rust-lang/rust/raw/679245769b6984ec5a7edf70fb4744d8411468b8/src%2Fshims%2Fforeign_items%2Fposix.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Fshims%2Fforeign_items%2Fposix.rs?ref=679245769b6984ec5a7edf70fb4744d8411468b8", "patch": "@@ -330,6 +330,45 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: crate::MiriEvalContextExt<'mir, 'tcx\n                 let result = this.pthread_rwlock_destroy(rwlock)?;\n                 this.write_scalar(Scalar::from_i32(result), dest)?;\n             }\n+            \"pthread_condattr_init\" => {\n+                let result = this.pthread_condattr_init(args[0])?;\n+                this.write_scalar(Scalar::from_i32(result), dest)?;\n+            }\n+            \"pthread_condattr_setclock\" => {\n+                let result = this.pthread_condattr_setclock(args[0], args[1])?;\n+                this.write_scalar(Scalar::from_i32(result), dest)?;\n+            }\n+            \"pthread_condattr_getclock\" => {\n+                let result = this.pthread_condattr_getclock(args[0], args[1])?;\n+                this.write_scalar(Scalar::from_i32(result), dest)?;\n+            }\n+            \"pthread_condattr_destroy\" => {\n+                let result = this.pthread_condattr_destroy(args[0])?;\n+                this.write_scalar(Scalar::from_i32(result), dest)?;\n+            }\n+            \"pthread_cond_init\" => {\n+                let result = this.pthread_cond_init(args[0], args[1])?;\n+                this.write_scalar(Scalar::from_i32(result), dest)?;\n+            }\n+            \"pthread_cond_signal\" => {\n+                let result = this.pthread_cond_signal(args[0])?;\n+                this.write_scalar(Scalar::from_i32(result), dest)?;\n+            }\n+            \"pthread_cond_broadcast\" => {\n+                let result = this.pthread_cond_broadcast(args[0])?;\n+                this.write_scalar(Scalar::from_i32(result), dest)?;\n+            }\n+            \"pthread_cond_wait\" => {\n+                let result = this.pthread_cond_wait(args[0], args[1])?;\n+                this.write_scalar(Scalar::from_i32(result), dest)?;\n+            }\n+            \"pthread_cond_timedwait\" => {\n+                this.pthread_cond_timedwait(args[0], args[1], args[2], dest)?;\n+            }\n+            \"pthread_cond_destroy\" => {\n+                let result = this.pthread_cond_destroy(args[0])?;\n+                this.write_scalar(Scalar::from_i32(result), dest)?;\n+            }\n \n             // Threading\n             \"pthread_create\" => {\n@@ -391,16 +430,7 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: crate::MiriEvalContextExt<'mir, 'tcx\n \n             | \"pthread_attr_init\"\n             | \"pthread_attr_destroy\"\n-            | \"pthread_condattr_init\"\n-            | \"pthread_condattr_destroy\"\n-            | \"pthread_cond_destroy\"\n-            if this.frame().instance.to_string().starts_with(\"std::sys::unix::\") => {\n-                let &[_] = check_arg_count(args)?;\n-                this.write_null(dest)?;\n-            }\n-            | \"pthread_cond_init\"\n             | \"pthread_attr_setstacksize\"\n-            | \"pthread_condattr_setclock\"\n             if this.frame().instance.to_string().starts_with(\"std::sys::unix::\") => {\n                 let &[_, _] = check_arg_count(args)?;\n                 this.write_null(dest)?;"}, {"sha": "dfd7999457eb953de24cf913ce5568fe6bd1cd33", "filename": "src/shims/sync.rs", "status": "modified", "additions": 418, "deletions": 232, "changes": 650, "blob_url": "https://github.com/rust-lang/rust/blob/679245769b6984ec5a7edf70fb4744d8411468b8/src%2Fshims%2Fsync.rs", "raw_url": "https://github.com/rust-lang/rust/raw/679245769b6984ec5a7edf70fb4744d8411468b8/src%2Fshims%2Fsync.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Fshims%2Fsync.rs?ref=679245769b6984ec5a7edf70fb4744d8411468b8", "patch": "@@ -1,8 +1,10 @@\n+use std::time::{Duration, SystemTime};\n+\n use rustc_middle::ty::{layout::TyAndLayout, TyKind, TypeAndMut};\n use rustc_target::abi::{LayoutOf, Size};\n \n use crate::stacked_borrows::Tag;\n-use crate::thread::BlockSetId;\n+\n use crate::*;\n \n fn assert_ptr_target_min_size<'mir, 'tcx: 'mir>(\n@@ -76,45 +78,12 @@ fn mutexattr_set_kind<'mir, 'tcx: 'mir>(\n // Our chosen memory layout for the emulated mutex (does not have to match the platform layout!):\n // bytes 0-3: reserved for signature on macOS\n // (need to avoid this because it is set by static initializer macros)\n-// bytes 4-7: count of how many times this mutex has been locked, as a u32\n-// bytes 8-11: when count > 0, id of the owner thread as a u32\n+// bytes 4-7: mutex id as u32 or 0 if id is not assigned yet.\n // bytes 12-15 or 16-19 (depending on platform): mutex kind, as an i32\n // (the kind has to be at its offset for compatibility with static initializer macros)\n-// bytes 20-23: when count > 0, id of the blockset in which the blocked threads\n-// are waiting or 0 if blockset is not yet assigned.\n \n const PTHREAD_MUTEX_T_MIN_SIZE: u64 = 24;\n \n-fn mutex_get_locked_count<'mir, 'tcx: 'mir>(\n-    ecx: &MiriEvalContext<'mir, 'tcx>,\n-    mutex_op: OpTy<'tcx, Tag>,\n-) -> InterpResult<'tcx, ScalarMaybeUninit<Tag>> {\n-    get_at_offset(ecx, mutex_op, 4, ecx.machine.layouts.u32, PTHREAD_MUTEX_T_MIN_SIZE)\n-}\n-\n-fn mutex_set_locked_count<'mir, 'tcx: 'mir>(\n-    ecx: &mut MiriEvalContext<'mir, 'tcx>,\n-    mutex_op: OpTy<'tcx, Tag>,\n-    locked_count: impl Into<ScalarMaybeUninit<Tag>>,\n-) -> InterpResult<'tcx, ()> {\n-    set_at_offset(ecx, mutex_op, 4, locked_count, ecx.machine.layouts.u32, PTHREAD_MUTEX_T_MIN_SIZE)\n-}\n-\n-fn mutex_get_owner<'mir, 'tcx: 'mir>(\n-    ecx: &MiriEvalContext<'mir, 'tcx>,\n-    mutex_op: OpTy<'tcx, Tag>,\n-) -> InterpResult<'tcx, ScalarMaybeUninit<Tag>> {\n-    get_at_offset(ecx, mutex_op, 8, ecx.machine.layouts.u32, PTHREAD_MUTEX_T_MIN_SIZE)\n-}\n-\n-fn mutex_set_owner<'mir, 'tcx: 'mir>(\n-    ecx: &mut MiriEvalContext<'mir, 'tcx>,\n-    mutex_op: OpTy<'tcx, Tag>,\n-    owner: impl Into<ScalarMaybeUninit<Tag>>,\n-) -> InterpResult<'tcx, ()> {\n-    set_at_offset(ecx, mutex_op, 8, owner, ecx.machine.layouts.u32, PTHREAD_MUTEX_T_MIN_SIZE)\n-}\n-\n fn mutex_get_kind<'mir, 'tcx: 'mir>(\n     ecx: &mut MiriEvalContext<'mir, 'tcx>,\n     mutex_op: OpTy<'tcx, Tag>,\n@@ -132,34 +101,34 @@ fn mutex_set_kind<'mir, 'tcx: 'mir>(\n     set_at_offset(ecx, mutex_op, offset, kind, ecx.machine.layouts.i32, PTHREAD_MUTEX_T_MIN_SIZE)\n }\n \n-fn mutex_get_blockset<'mir, 'tcx: 'mir>(\n+fn mutex_get_id<'mir, 'tcx: 'mir>(\n     ecx: &MiriEvalContext<'mir, 'tcx>,\n     mutex_op: OpTy<'tcx, Tag>,\n-) -> InterpResult<'tcx, ScalarMaybeUninit<Tag>> {\n-    get_at_offset(ecx, mutex_op, 20, ecx.machine.layouts.u32, PTHREAD_MUTEX_T_MIN_SIZE)\n+) -> InterpResult<'tcx, ScalarMaybeUndef<Tag>> {\n+    get_at_offset(ecx, mutex_op, 4, ecx.machine.layouts.u32, PTHREAD_MUTEX_T_MIN_SIZE)\n }\n \n-fn mutex_set_blockset<'mir, 'tcx: 'mir>(\n+fn mutex_set_id<'mir, 'tcx: 'mir>(\n     ecx: &mut MiriEvalContext<'mir, 'tcx>,\n     mutex_op: OpTy<'tcx, Tag>,\n-    blockset: impl Into<ScalarMaybeUninit<Tag>>,\n+    id: impl Into<ScalarMaybeUndef<Tag>>,\n ) -> InterpResult<'tcx, ()> {\n-    set_at_offset(ecx, mutex_op, 20, blockset, ecx.machine.layouts.u32, PTHREAD_MUTEX_T_MIN_SIZE)\n+    set_at_offset(ecx, mutex_op, 4, id, ecx.machine.layouts.u32, PTHREAD_MUTEX_T_MIN_SIZE)\n }\n \n-fn mutex_get_or_create_blockset<'mir, 'tcx: 'mir>(\n+fn mutex_get_or_create_id<'mir, 'tcx: 'mir>(\n     ecx: &mut MiriEvalContext<'mir, 'tcx>,\n     mutex_op: OpTy<'tcx, Tag>,\n-) -> InterpResult<'tcx, BlockSetId> {\n-    let blockset = mutex_get_blockset(ecx, mutex_op)?.to_u32()?;\n-    if blockset == 0 {\n-        // 0 is a default value and also not a valid blockset id. Need to\n-        // allocate a new blockset.\n-        let blockset = ecx.create_blockset()?;\n-        mutex_set_blockset(ecx, mutex_op, blockset.to_u32_scalar())?;\n-        Ok(blockset)\n+) -> InterpResult<'tcx, MutexId> {\n+    let id = mutex_get_id(ecx, mutex_op)?.to_u32()?;\n+    if id == 0 {\n+        // 0 is a default value and also not a valid mutex id. Need to allocate\n+        // a new mutex.\n+        let id = ecx.mutex_create();\n+        mutex_set_id(ecx, mutex_op, id.to_u32_scalar())?;\n+        Ok(id)\n     } else {\n-        Ok(BlockSetId::new(blockset))\n+        Ok(id.into())\n     }\n }\n \n@@ -168,105 +137,160 @@ fn mutex_get_or_create_blockset<'mir, 'tcx: 'mir>(\n // Our chosen memory layout for the emulated rwlock (does not have to match the platform layout!):\n // bytes 0-3: reserved for signature on macOS\n // (need to avoid this because it is set by static initializer macros)\n-// bytes 4-7: reader count, as a u32\n-// bytes 8-11: writer count, as a u32\n-// bytes 12-15: when writer or reader count > 0, id of the blockset in which the\n-// blocked writers are waiting or 0 if blockset is not yet assigned.\n-// bytes 16-20: when writer count > 0, id of the blockset in which the blocked\n-// readers are waiting or 0 if blockset is not yet assigned.\n+// bytes 4-7: rwlock id as u32 or 0 if id is not assigned yet.\n \n-const PTHREAD_RWLOCK_T_MIN_SIZE: u64 = 20;\n+const PTHREAD_RWLOCK_T_MIN_SIZE: u64 = 32;\n \n-fn rwlock_get_readers<'mir, 'tcx: 'mir>(\n+fn rwlock_get_id<'mir, 'tcx: 'mir>(\n     ecx: &MiriEvalContext<'mir, 'tcx>,\n     rwlock_op: OpTy<'tcx, Tag>,\n ) -> InterpResult<'tcx, ScalarMaybeUninit<Tag>> {\n     get_at_offset(ecx, rwlock_op, 4, ecx.machine.layouts.u32, PTHREAD_RWLOCK_T_MIN_SIZE)\n }\n \n-fn rwlock_set_readers<'mir, 'tcx: 'mir>(\n+fn rwlock_set_id<'mir, 'tcx: 'mir>(\n     ecx: &mut MiriEvalContext<'mir, 'tcx>,\n     rwlock_op: OpTy<'tcx, Tag>,\n-    readers: impl Into<ScalarMaybeUninit<Tag>>,\n+    id: impl Into<ScalarMaybeUndef<Tag>>,\n ) -> InterpResult<'tcx, ()> {\n-    set_at_offset(ecx, rwlock_op, 4, readers, ecx.machine.layouts.u32, PTHREAD_RWLOCK_T_MIN_SIZE)\n+    set_at_offset(ecx, rwlock_op, 4, id, ecx.machine.layouts.u32, PTHREAD_RWLOCK_T_MIN_SIZE)\n }\n \n-fn rwlock_get_writers<'mir, 'tcx: 'mir>(\n-    ecx: &MiriEvalContext<'mir, 'tcx>,\n+fn rwlock_get_or_create_id<'mir, 'tcx: 'mir>(\n+    ecx: &mut MiriEvalContext<'mir, 'tcx>,\n     rwlock_op: OpTy<'tcx, Tag>,\n-) -> InterpResult<'tcx, ScalarMaybeUninit<Tag>> {\n-    get_at_offset(ecx, rwlock_op, 8, ecx.machine.layouts.u32, PTHREAD_RWLOCK_T_MIN_SIZE)\n+) -> InterpResult<'tcx, RwLockId> {\n+    let id = rwlock_get_id(ecx, rwlock_op)?.to_u32()?;\n+    if id == 0 {\n+        // 0 is a default value and also not a valid rwlock id. Need to allocate\n+        // a new read-write lock.\n+        let id = ecx.rwlock_create();\n+        rwlock_set_id(ecx, rwlock_op, id.to_u32_scalar())?;\n+        Ok(id)\n+    } else {\n+        Ok(id.into())\n+    }\n+}\n+\n+// pthread_condattr_t\n+\n+// Our chosen memory layout for emulation (does not have to match the platform layout!):\n+// store an i32 in the first four bytes equal to the corresponding libc clock id constant\n+// (e.g. CLOCK_REALTIME).\n+\n+const PTHREAD_CONDATTR_T_MIN_SIZE: u64 = 4;\n+\n+fn condattr_get_clock_id<'mir, 'tcx: 'mir>(\n+    ecx: &MiriEvalContext<'mir, 'tcx>,\n+    attr_op: OpTy<'tcx, Tag>,\n+) -> InterpResult<'tcx, ScalarMaybeUndef<Tag>> {\n+    get_at_offset(ecx, attr_op, 0, ecx.machine.layouts.i32, PTHREAD_CONDATTR_T_MIN_SIZE)\n }\n \n-fn rwlock_set_writers<'mir, 'tcx: 'mir>(\n+fn condattr_set_clock_id<'mir, 'tcx: 'mir>(\n     ecx: &mut MiriEvalContext<'mir, 'tcx>,\n-    rwlock_op: OpTy<'tcx, Tag>,\n-    writers: impl Into<ScalarMaybeUninit<Tag>>,\n+    attr_op: OpTy<'tcx, Tag>,\n+    clock_id: impl Into<ScalarMaybeUndef<Tag>>,\n ) -> InterpResult<'tcx, ()> {\n-    set_at_offset(ecx, rwlock_op, 8, writers, ecx.machine.layouts.u32, PTHREAD_RWLOCK_T_MIN_SIZE)\n+    set_at_offset(ecx, attr_op, 0, clock_id, ecx.machine.layouts.i32, PTHREAD_CONDATTR_T_MIN_SIZE)\n }\n \n-fn rwlock_get_writer_blockset<'mir, 'tcx: 'mir>(\n+// pthread_cond_t\n+\n+// Our chosen memory layout for the emulated conditional variable (does not have\n+// to match the platform layout!):\n+\n+// bytes 4-7: the conditional variable id as u32 or 0 if id is not assigned yet.\n+// bytes 8-11: the clock id constant as i32\n+\n+const PTHREAD_COND_T_MIN_SIZE: u64 = 12;\n+\n+fn cond_get_id<'mir, 'tcx: 'mir>(\n     ecx: &MiriEvalContext<'mir, 'tcx>,\n-    rwlock_op: OpTy<'tcx, Tag>,\n-) -> InterpResult<'tcx, ScalarMaybeUninit<Tag>> {\n-    get_at_offset(ecx, rwlock_op, 12, ecx.machine.layouts.u32, PTHREAD_RWLOCK_T_MIN_SIZE)\n+    cond_op: OpTy<'tcx, Tag>,\n+) -> InterpResult<'tcx, ScalarMaybeUndef<Tag>> {\n+    get_at_offset(ecx, cond_op, 4, ecx.machine.layouts.u32, PTHREAD_COND_T_MIN_SIZE)\n }\n \n-fn rwlock_set_writer_blockset<'mir, 'tcx: 'mir>(\n+fn cond_set_id<'mir, 'tcx: 'mir>(\n     ecx: &mut MiriEvalContext<'mir, 'tcx>,\n-    rwlock_op: OpTy<'tcx, Tag>,\n-    blockset: impl Into<ScalarMaybeUninit<Tag>>,\n+    cond_op: OpTy<'tcx, Tag>,\n+    id: impl Into<ScalarMaybeUndef<Tag>>,\n ) -> InterpResult<'tcx, ()> {\n-    set_at_offset(ecx, rwlock_op, 12, blockset, ecx.machine.layouts.u32, PTHREAD_RWLOCK_T_MIN_SIZE)\n+    set_at_offset(ecx, cond_op, 4, id, ecx.machine.layouts.u32, PTHREAD_COND_T_MIN_SIZE)\n }\n \n-fn rwlock_get_or_create_writer_blockset<'mir, 'tcx: 'mir>(\n+fn cond_get_or_create_id<'mir, 'tcx: 'mir>(\n     ecx: &mut MiriEvalContext<'mir, 'tcx>,\n-    rwlock_op: OpTy<'tcx, Tag>,\n-) -> InterpResult<'tcx, BlockSetId> {\n-    let blockset = rwlock_get_writer_blockset(ecx, rwlock_op)?.to_u32()?;\n-    if blockset == 0 {\n-        // 0 is a default value and also not a valid blockset id. Need to\n-        // allocate a new blockset.\n-        let blockset = ecx.create_blockset()?;\n-        rwlock_set_writer_blockset(ecx, rwlock_op, blockset.to_u32_scalar())?;\n-        Ok(blockset)\n+    cond_op: OpTy<'tcx, Tag>,\n+) -> InterpResult<'tcx, CondvarId> {\n+    let id = cond_get_id(ecx, cond_op)?.to_u32()?;\n+    if id == 0 {\n+        // 0 is a default value and also not a valid conditional variable id.\n+        // Need to allocate a new id.\n+        let id = ecx.condvar_create();\n+        cond_set_id(ecx, cond_op, id.to_u32_scalar())?;\n+        Ok(id)\n     } else {\n-        Ok(BlockSetId::new(blockset))\n+        Ok(id.into())\n     }\n }\n \n-fn rwlock_get_reader_blockset<'mir, 'tcx: 'mir>(\n+fn cond_get_clock_id<'mir, 'tcx: 'mir>(\n     ecx: &MiriEvalContext<'mir, 'tcx>,\n-    rwlock_op: OpTy<'tcx, Tag>,\n-) -> InterpResult<'tcx, ScalarMaybeUninit<Tag>> {\n-    get_at_offset(ecx, rwlock_op, 16, ecx.machine.layouts.u32, PTHREAD_RWLOCK_T_MIN_SIZE)\n+    cond_op: OpTy<'tcx, Tag>,\n+) -> InterpResult<'tcx, ScalarMaybeUndef<Tag>> {\n+    get_at_offset(ecx, cond_op, 8, ecx.machine.layouts.i32, PTHREAD_COND_T_MIN_SIZE)\n }\n \n-fn rwlock_set_reader_blockset<'mir, 'tcx: 'mir>(\n+fn cond_set_clock_id<'mir, 'tcx: 'mir>(\n     ecx: &mut MiriEvalContext<'mir, 'tcx>,\n-    rwlock_op: OpTy<'tcx, Tag>,\n-    blockset: impl Into<ScalarMaybeUninit<Tag>>,\n+    cond_op: OpTy<'tcx, Tag>,\n+    clock_id: impl Into<ScalarMaybeUndef<Tag>>,\n ) -> InterpResult<'tcx, ()> {\n-    set_at_offset(ecx, rwlock_op, 16, blockset, ecx.machine.layouts.u32, PTHREAD_RWLOCK_T_MIN_SIZE)\n+    set_at_offset(ecx, cond_op, 8, clock_id, ecx.machine.layouts.i32, PTHREAD_COND_T_MIN_SIZE)\n }\n \n-fn rwlock_get_or_create_reader_blockset<'mir, 'tcx: 'mir>(\n+/// Try to reacquire the mutex associated with the condition variable after we were signaled.\n+fn reacquire_cond_mutex<'mir, 'tcx: 'mir>(\n     ecx: &mut MiriEvalContext<'mir, 'tcx>,\n-    rwlock_op: OpTy<'tcx, Tag>,\n-) -> InterpResult<'tcx, BlockSetId> {\n-    let blockset = rwlock_get_reader_blockset(ecx, rwlock_op)?.to_u32()?;\n-    if blockset == 0 {\n-        // 0 is a default value and also not a valid blockset id. Need to\n-        // allocate a new blockset.\n-        let blockset = ecx.create_blockset()?;\n-        rwlock_set_reader_blockset(ecx, rwlock_op, blockset.to_u32_scalar())?;\n-        Ok(blockset)\n+    thread: ThreadId,\n+    mutex: MutexId,\n+) -> InterpResult<'tcx> {\n+    if ecx.mutex_is_locked(mutex) {\n+        ecx.mutex_enqueue(mutex, thread);\n     } else {\n-        Ok(BlockSetId::new(blockset))\n+        ecx.mutex_lock(mutex, thread);\n+        ecx.unblock_thread(thread)?;\n     }\n+    Ok(())\n+}\n+\n+/// Release the mutex associated with the condition variable because we are\n+/// entering the waiting state.\n+fn release_cond_mutex<'mir, 'tcx: 'mir>(\n+    ecx: &mut MiriEvalContext<'mir, 'tcx>,\n+    active_thread: ThreadId,\n+    mutex: MutexId,\n+) -> InterpResult<'tcx> {\n+    if let Some((owner_thread, current_locked_count)) = ecx.mutex_unlock(mutex) {\n+        if current_locked_count != 0 {\n+            throw_unsup_format!(\"awaiting on multiple times acquired lock is not supported\");\n+        }\n+        if owner_thread != active_thread {\n+            throw_ub_format!(\"awaiting on a mutex owned by a different thread\");\n+        }\n+        if let Some(thread) = ecx.mutex_dequeue(mutex) {\n+            // We have at least one thread waiting on this mutex. Transfer\n+            // ownership to it.\n+            ecx.mutex_lock(mutex, thread);\n+            ecx.unblock_thread(thread)?;\n+        }\n+    } else {\n+        throw_ub_format!(\"awaiting on unlocked mutex\");\n+    }\n+    ecx.block_thread(active_thread)?;\n+    Ok(())\n }\n \n impl<'mir, 'tcx> EvalContextExt<'mir, 'tcx> for crate::MiriEvalContext<'mir, 'tcx> {}\n@@ -323,7 +347,7 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: crate::MiriEvalContextExt<'mir, 'tcx\n             mutexattr_get_kind(this, attr_op)?.not_undef()?\n         };\n \n-        mutex_set_locked_count(this, mutex_op, Scalar::from_u32(0))?;\n+        let _ = mutex_get_or_create_id(this, mutex_op)?;\n         mutex_set_kind(this, mutex_op, kind)?;\n \n         Ok(0)\n@@ -333,21 +357,15 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: crate::MiriEvalContextExt<'mir, 'tcx\n         let this = self.eval_context_mut();\n \n         let kind = mutex_get_kind(this, mutex_op)?.not_undef()?;\n-        let locked_count = mutex_get_locked_count(this, mutex_op)?.to_u32()?;\n+        let id = mutex_get_or_create_id(this, mutex_op)?;\n         let active_thread = this.get_active_thread()?;\n \n-        if locked_count == 0 {\n-            // The mutex is unlocked. Let's lock it.\n-            mutex_set_locked_count(this, mutex_op, Scalar::from_u32(1))?;\n-            mutex_set_owner(this, mutex_op, active_thread.to_u32_scalar())?;\n-            Ok(0)\n-        } else {\n-            // The mutex is locked. Let's check by whom.\n-            let owner_thread: ThreadId = mutex_get_owner(this, mutex_op)?.to_u32()?.into();\n+        if this.mutex_is_locked(id) {\n+            let owner_thread = this.mutex_get_owner(id);\n             if owner_thread != active_thread {\n                 // Block the active thread.\n-                let blockset = mutex_get_or_create_blockset(this, mutex_op)?;\n-                this.block_active_thread(blockset)?;\n+                this.block_thread(active_thread)?;\n+                this.mutex_enqueue(id, active_thread);\n                 Ok(0)\n             } else {\n                 // Trying to acquire the same mutex again.\n@@ -356,34 +374,28 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: crate::MiriEvalContextExt<'mir, 'tcx\n                 } else if kind == this.eval_libc(\"PTHREAD_MUTEX_ERRORCHECK\")? {\n                     this.eval_libc_i32(\"EDEADLK\")\n                 } else if kind == this.eval_libc(\"PTHREAD_MUTEX_RECURSIVE\")? {\n-                    match locked_count.checked_add(1) {\n-                        Some(new_count) => {\n-                            mutex_set_locked_count(this, mutex_op, Scalar::from_u32(new_count))?;\n-                            Ok(0)\n-                        }\n-                        None => this.eval_libc_i32(\"EAGAIN\"),\n-                    }\n+                    this.mutex_lock(id, active_thread);\n+                    Ok(0)\n                 } else {\n                     throw_ub_format!(\"called pthread_mutex_lock on an unsupported type of mutex\");\n                 }\n             }\n+        } else {\n+            // The mutex is unlocked. Let's lock it.\n+            this.mutex_lock(id, active_thread);\n+            Ok(0)\n         }\n     }\n \n     fn pthread_mutex_trylock(&mut self, mutex_op: OpTy<'tcx, Tag>) -> InterpResult<'tcx, i32> {\n         let this = self.eval_context_mut();\n \n         let kind = mutex_get_kind(this, mutex_op)?.not_undef()?;\n-        let locked_count = mutex_get_locked_count(this, mutex_op)?.to_u32()?;\n+        let id = mutex_get_or_create_id(this, mutex_op)?;\n         let active_thread = this.get_active_thread()?;\n \n-        if locked_count == 0 {\n-            // The mutex is unlocked. Let's lock it.\n-            mutex_set_locked_count(this, mutex_op, Scalar::from_u32(1))?;\n-            mutex_set_owner(this, mutex_op, active_thread.to_u32_scalar())?;\n-            Ok(0)\n-        } else {\n-            let owner_thread: ThreadId = mutex_get_owner(this, mutex_op)?.to_u32()?.into();\n+        if this.mutex_is_locked(id) {\n+            let owner_thread = this.mutex_get_owner(id);\n             if owner_thread != active_thread {\n                 this.eval_libc_i32(\"EBUSY\")\n             } else {\n@@ -392,41 +404,39 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: crate::MiriEvalContextExt<'mir, 'tcx\n                 {\n                     this.eval_libc_i32(\"EBUSY\")\n                 } else if kind == this.eval_libc(\"PTHREAD_MUTEX_RECURSIVE\")? {\n-                    match locked_count.checked_add(1) {\n-                        Some(new_count) => {\n-                            mutex_set_locked_count(this, mutex_op, Scalar::from_u32(new_count))?;\n-                            Ok(0)\n-                        }\n-                        None => this.eval_libc_i32(\"EAGAIN\"),\n-                    }\n+                    this.mutex_lock(id, active_thread);\n+                    Ok(0)\n                 } else {\n                     throw_ub_format!(\n                         \"called pthread_mutex_trylock on an unsupported type of mutex\"\n                     );\n                 }\n             }\n+        } else {\n+            // The mutex is unlocked. Let's lock it.\n+            this.mutex_lock(id, active_thread);\n+            Ok(0)\n         }\n     }\n \n     fn pthread_mutex_unlock(&mut self, mutex_op: OpTy<'tcx, Tag>) -> InterpResult<'tcx, i32> {\n         let this = self.eval_context_mut();\n \n         let kind = mutex_get_kind(this, mutex_op)?.not_undef()?;\n-        let locked_count = mutex_get_locked_count(this, mutex_op)?.to_u32()?;\n-        let owner_thread: ThreadId = mutex_get_owner(this, mutex_op)?.to_u32()?.into();\n-\n-        if owner_thread != this.get_active_thread()? {\n-            throw_ub_format!(\"called pthread_mutex_unlock on a mutex owned by another thread\");\n-        } else if locked_count == 1 {\n-            let blockset = mutex_get_or_create_blockset(this, mutex_op)?;\n-            if let Some(new_owner) = this.unblock_some_thread(blockset)? {\n-                // We have at least one thread waiting on this mutex. Transfer\n-                // ownership to it.\n-                mutex_set_owner(this, mutex_op, new_owner.to_u32_scalar())?;\n-            } else {\n-                // No thread is waiting on this mutex.\n-                mutex_set_owner(this, mutex_op, Scalar::from_u32(0))?;\n-                mutex_set_locked_count(this, mutex_op, Scalar::from_u32(0))?;\n+        let id = mutex_get_or_create_id(this, mutex_op)?;\n+\n+        if let Some((owner_thread, current_locked_count)) = this.mutex_unlock(id) {\n+            if owner_thread != this.get_active_thread()? {\n+                throw_ub_format!(\"called pthread_mutex_unlock on a mutex owned by another thread\");\n+            }\n+            if current_locked_count == 0 {\n+                // The mutex is unlocked.\n+                if let Some(thread) = this.mutex_dequeue(id) {\n+                    // We have at least one thread waiting on this mutex. Transfer\n+                    // ownership to it.\n+                    this.mutex_lock(id, thread);\n+                    this.unblock_thread(thread)?;\n+                }\n             }\n             Ok(0)\n         } else {\n@@ -435,16 +445,7 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: crate::MiriEvalContextExt<'mir, 'tcx\n             } else if kind == this.eval_libc(\"PTHREAD_MUTEX_ERRORCHECK\")? {\n                 this.eval_libc_i32(\"EPERM\")\n             } else if kind == this.eval_libc(\"PTHREAD_MUTEX_RECURSIVE\")? {\n-                match locked_count.checked_sub(1) {\n-                    Some(new_count) => {\n-                        mutex_set_locked_count(this, mutex_op, Scalar::from_u32(new_count))?;\n-                        Ok(0)\n-                    }\n-                    None => {\n-                        // locked_count was already zero\n-                        this.eval_libc_i32(\"EPERM\")\n-                    }\n-                }\n+                this.eval_libc_i32(\"EPERM\")\n             } else {\n                 throw_ub_format!(\"called pthread_mutex_unlock on an unsupported type of mutex\");\n             }\n@@ -454,135 +455,320 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: crate::MiriEvalContextExt<'mir, 'tcx\n     fn pthread_mutex_destroy(&mut self, mutex_op: OpTy<'tcx, Tag>) -> InterpResult<'tcx, i32> {\n         let this = self.eval_context_mut();\n \n-        if mutex_get_locked_count(this, mutex_op)?.to_u32()? != 0 {\n+        let id = mutex_get_or_create_id(this, mutex_op)?;\n+\n+        if this.mutex_is_locked(id) {\n             throw_ub_format!(\"destroyed a locked mutex\");\n         }\n \n-        mutex_set_kind(this, mutex_op, ScalarMaybeUninit::Uninit)?;\n-        mutex_set_locked_count(this, mutex_op, ScalarMaybeUninit::Uninit)?;\n-        mutex_set_blockset(this, mutex_op, ScalarMaybeUninit::Uninit)?;\n+        mutex_set_kind(this, mutex_op, ScalarMaybeUndef::Undef)?;\n+        mutex_set_id(this, mutex_op, ScalarMaybeUndef::Undef)?;\n \n         Ok(0)\n     }\n \n     fn pthread_rwlock_rdlock(&mut self, rwlock_op: OpTy<'tcx, Tag>) -> InterpResult<'tcx, i32> {\n         let this = self.eval_context_mut();\n \n-        let readers = rwlock_get_readers(this, rwlock_op)?.to_u32()?;\n-        let writers = rwlock_get_writers(this, rwlock_op)?.to_u32()?;\n+        let id = rwlock_get_or_create_id(this, rwlock_op)?;\n+        let active_thread = this.get_active_thread()?;\n \n-        if writers != 0 {\n-            // The lock is locked by a writer.\n-            assert_eq!(writers, 1);\n-            let reader_blockset = rwlock_get_or_create_reader_blockset(this, rwlock_op)?;\n-            this.block_active_thread(reader_blockset)?;\n+        if this.rwlock_is_write_locked(id) {\n+            this.rwlock_enqueue_reader(id, active_thread);\n+            this.block_thread(active_thread)?;\n             Ok(0)\n         } else {\n-            match readers.checked_add(1) {\n-                Some(new_readers) => {\n-                    rwlock_set_readers(this, rwlock_op, Scalar::from_u32(new_readers))?;\n-                    Ok(0)\n-                }\n-                None => this.eval_libc_i32(\"EAGAIN\"),\n-            }\n+            this.rwlock_reader_add(id, active_thread);\n+            Ok(0)\n         }\n     }\n \n     fn pthread_rwlock_tryrdlock(&mut self, rwlock_op: OpTy<'tcx, Tag>) -> InterpResult<'tcx, i32> {\n         let this = self.eval_context_mut();\n \n-        let readers = rwlock_get_readers(this, rwlock_op)?.to_u32()?;\n-        let writers = rwlock_get_writers(this, rwlock_op)?.to_u32()?;\n-        if writers != 0 {\n+        let id = rwlock_get_or_create_id(this, rwlock_op)?;\n+        let active_thread = this.get_active_thread()?;\n+\n+        if this.rwlock_is_write_locked(id) {\n             this.eval_libc_i32(\"EBUSY\")\n         } else {\n-            match readers.checked_add(1) {\n-                Some(new_readers) => {\n-                    rwlock_set_readers(this, rwlock_op, Scalar::from_u32(new_readers))?;\n-                    Ok(0)\n-                }\n-                None => this.eval_libc_i32(\"EAGAIN\"),\n-            }\n+            this.rwlock_reader_add(id, active_thread);\n+            Ok(0)\n         }\n     }\n \n     fn pthread_rwlock_wrlock(&mut self, rwlock_op: OpTy<'tcx, Tag>) -> InterpResult<'tcx, i32> {\n         let this = self.eval_context_mut();\n \n-        let readers = rwlock_get_readers(this, rwlock_op)?.to_u32()?;\n-        let writers = rwlock_get_writers(this, rwlock_op)?.to_u32()?;\n-        let writer_blockset = rwlock_get_or_create_writer_blockset(this, rwlock_op)?;\n-        if readers != 0 || writers != 0 {\n-            this.block_active_thread(writer_blockset)?;\n+        let id = rwlock_get_or_create_id(this, rwlock_op)?;\n+        let active_thread = this.get_active_thread()?;\n+\n+        if this.rwlock_is_locked(id) {\n+            this.block_thread(active_thread)?;\n+            this.rwlock_enqueue_writer(id, active_thread);\n         } else {\n-            rwlock_set_writers(this, rwlock_op, Scalar::from_u32(1))?;\n+            this.rwlock_writer_set(id, active_thread);\n         }\n+\n         Ok(0)\n     }\n \n     fn pthread_rwlock_trywrlock(&mut self, rwlock_op: OpTy<'tcx, Tag>) -> InterpResult<'tcx, i32> {\n         let this = self.eval_context_mut();\n \n-        let readers = rwlock_get_readers(this, rwlock_op)?.to_u32()?;\n-        let writers = rwlock_get_writers(this, rwlock_op)?.to_u32()?;\n-        if readers != 0 || writers != 0 {\n+        let id = rwlock_get_or_create_id(this, rwlock_op)?;\n+        let active_thread = this.get_active_thread()?;\n+\n+        if this.rwlock_is_locked(id) {\n             this.eval_libc_i32(\"EBUSY\")\n         } else {\n-            rwlock_set_writers(this, rwlock_op, Scalar::from_u32(1))?;\n+            this.rwlock_writer_set(id, active_thread);\n             Ok(0)\n         }\n     }\n \n-    // FIXME: We should check that this lock was locked by the active thread.\n     fn pthread_rwlock_unlock(&mut self, rwlock_op: OpTy<'tcx, Tag>) -> InterpResult<'tcx, i32> {\n         let this = self.eval_context_mut();\n \n-        let readers = rwlock_get_readers(this, rwlock_op)?.to_u32()?;\n-        let writers = rwlock_get_writers(this, rwlock_op)?.to_u32()?;\n-        let writer_blockset = rwlock_get_or_create_writer_blockset(this, rwlock_op)?;\n-        if let Some(new_readers) = readers.checked_sub(1) {\n-            assert_eq!(writers, 0);\n-            rwlock_set_readers(this, rwlock_op, Scalar::from_u32(new_readers))?;\n-            if new_readers == 0 {\n-                if let Some(_writer) = this.unblock_some_thread(writer_blockset)? {\n-                    rwlock_set_writers(this, rwlock_op, Scalar::from_u32(1))?;\n+        let id = rwlock_get_or_create_id(this, rwlock_op)?;\n+        let active_thread = this.get_active_thread()?;\n+\n+        if this.rwlock_reader_remove(id, active_thread) {\n+            // The thread was a reader.\n+            if this.rwlock_is_locked(id) {\n+                // No more readers owning the lock. Give it to a writer if there\n+                // is any.\n+                if let Some(writer) = this.rwlock_dequeue_writer(id) {\n+                    this.unblock_thread(writer)?;\n+                    this.rwlock_writer_set(id, writer);\n                 }\n             }\n             Ok(0)\n-        } else if writers != 0 {\n-            let reader_blockset = rwlock_get_or_create_reader_blockset(this, rwlock_op)?;\n+        } else if Some(active_thread) == this.rwlock_writer_remove(id) {\n+            // The thread was a writer.\n+            //\n             // We are prioritizing writers here against the readers. As a\n             // result, not only readers can starve writers, but also writers can\n             // starve readers.\n-            if let Some(_writer) = this.unblock_some_thread(writer_blockset)? {\n-                assert_eq!(writers, 1);\n+            if let Some(writer) = this.rwlock_dequeue_writer(id) {\n+                // Give the lock to another writer.\n+                this.unblock_thread(writer)?;\n+                this.rwlock_writer_set(id, writer);\n             } else {\n-                rwlock_set_writers(this, rwlock_op, Scalar::from_u32(0))?;\n-                let mut readers = 0;\n-                while let Some(_reader) = this.unblock_some_thread(reader_blockset)? {\n-                    readers += 1;\n+                // Give the lock to all readers.\n+                while let Some(reader) = this.rwlock_dequeue_reader(id) {\n+                    this.unblock_thread(reader)?;\n+                    this.rwlock_reader_add(id, reader);\n                 }\n-                rwlock_set_readers(this, rwlock_op, Scalar::from_u32(readers))?\n             }\n             Ok(0)\n         } else {\n-            throw_ub_format!(\"unlocked an rwlock that was not locked\");\n+            throw_ub_format!(\"unlocked an rwlock that was not locked by the active thread\");\n         }\n     }\n \n     fn pthread_rwlock_destroy(&mut self, rwlock_op: OpTy<'tcx, Tag>) -> InterpResult<'tcx, i32> {\n         let this = self.eval_context_mut();\n \n-        if rwlock_get_readers(this, rwlock_op)?.to_u32()? != 0\n-            || rwlock_get_writers(this, rwlock_op)?.to_u32()? != 0\n-        {\n+        let id = rwlock_get_or_create_id(this, rwlock_op)?;\n+\n+        if this.rwlock_is_locked(id) {\n             throw_ub_format!(\"destroyed a locked rwlock\");\n         }\n \n-        rwlock_set_readers(this, rwlock_op, ScalarMaybeUninit::Uninit)?;\n-        rwlock_set_writers(this, rwlock_op, ScalarMaybeUninit::Uninit)?;\n-        rwlock_set_reader_blockset(this, rwlock_op, ScalarMaybeUninit::Uninit)?;\n-        rwlock_set_writer_blockset(this, rwlock_op, ScalarMaybeUninit::Uninit)?;\n+        rwlock_set_id(this, rwlock_op, ScalarMaybeUndef::Undef)?;\n+\n+        Ok(0)\n+    }\n+\n+    fn pthread_condattr_init(&mut self, attr_op: OpTy<'tcx, Tag>) -> InterpResult<'tcx, i32> {\n+        let this = self.eval_context_mut();\n+\n+        let default_clock_id = this.eval_libc(\"CLOCK_REALTIME\")?;\n+        condattr_set_clock_id(this, attr_op, default_clock_id)?;\n+\n+        Ok(0)\n+    }\n+\n+    fn pthread_condattr_setclock(\n+        &mut self,\n+        attr_op: OpTy<'tcx, Tag>,\n+        clock_id_op: OpTy<'tcx, Tag>,\n+    ) -> InterpResult<'tcx, i32> {\n+        let this = self.eval_context_mut();\n+\n+        let clock_id = this.read_scalar(clock_id_op)?.not_undef()?;\n+        if clock_id == this.eval_libc(\"CLOCK_REALTIME\")?\n+            || clock_id == this.eval_libc(\"CLOCK_MONOTONIC\")?\n+        {\n+            condattr_set_clock_id(this, attr_op, clock_id)?;\n+        } else {\n+            let einval = this.eval_libc_i32(\"EINVAL\")?;\n+            return Ok(einval);\n+        }\n+\n+        Ok(0)\n+    }\n+\n+    fn pthread_condattr_getclock(\n+        &mut self,\n+        attr_op: OpTy<'tcx, Tag>,\n+        clk_id_op: OpTy<'tcx, Tag>,\n+    ) -> InterpResult<'tcx, i32> {\n+        let this = self.eval_context_mut();\n+\n+        let clock_id = condattr_get_clock_id(this, attr_op)?;\n+        this.write_scalar(clock_id, this.deref_operand(clk_id_op)?.into())?;\n+\n+        Ok(0)\n+    }\n+\n+    fn pthread_condattr_destroy(&mut self, attr_op: OpTy<'tcx, Tag>) -> InterpResult<'tcx, i32> {\n+        let this = self.eval_context_mut();\n+\n+        condattr_set_clock_id(this, attr_op, ScalarMaybeUndef::Undef)?;\n+\n+        Ok(0)\n+    }\n+\n+    fn pthread_cond_init(\n+        &mut self,\n+        cond_op: OpTy<'tcx, Tag>,\n+        attr_op: OpTy<'tcx, Tag>,\n+    ) -> InterpResult<'tcx, i32> {\n+        let this = self.eval_context_mut();\n+\n+        let attr = this.read_scalar(attr_op)?.not_undef()?;\n+        let clock_id = if this.is_null(attr)? {\n+            this.eval_libc(\"CLOCK_REALTIME\")?\n+        } else {\n+            condattr_get_clock_id(this, attr_op)?.not_undef()?\n+        };\n+\n+        let _ = cond_get_or_create_id(this, cond_op)?;\n+        cond_set_clock_id(this, cond_op, clock_id)?;\n+\n+        Ok(0)\n+    }\n+\n+    fn pthread_cond_signal(&mut self, cond_op: OpTy<'tcx, Tag>) -> InterpResult<'tcx, i32> {\n+        let this = self.eval_context_mut();\n+        let id = cond_get_or_create_id(this, cond_op)?;\n+        if let Some((thread, mutex)) = this.condvar_signal(id) {\n+            reacquire_cond_mutex(this, thread, mutex)?;\n+            this.unregister_callback_if_exists(thread)?;\n+        }\n+\n+        Ok(0)\n+    }\n+\n+    fn pthread_cond_broadcast(&mut self, cond_op: OpTy<'tcx, Tag>) -> InterpResult<'tcx, i32> {\n+        let this = self.eval_context_mut();\n+        let id = cond_get_or_create_id(this, cond_op)?;\n+\n+        while let Some((thread, mutex)) = this.condvar_signal(id) {\n+            reacquire_cond_mutex(this, thread, mutex)?;\n+            this.unregister_callback_if_exists(thread)?;\n+        }\n+\n+        Ok(0)\n+    }\n+\n+    fn pthread_cond_wait(\n+        &mut self,\n+        cond_op: OpTy<'tcx, Tag>,\n+        mutex_op: OpTy<'tcx, Tag>,\n+    ) -> InterpResult<'tcx, i32> {\n+        let this = self.eval_context_mut();\n+\n+        let id = cond_get_or_create_id(this, cond_op)?;\n+        let mutex_id = mutex_get_or_create_id(this, mutex_op)?;\n+        let active_thread = this.get_active_thread()?;\n+\n+        release_cond_mutex(this, active_thread, mutex_id)?;\n+        this.condvar_wait(id, active_thread, mutex_id);\n+\n+        Ok(0)\n+    }\n+\n+    fn pthread_cond_timedwait(\n+        &mut self,\n+        cond_op: OpTy<'tcx, Tag>,\n+        mutex_op: OpTy<'tcx, Tag>,\n+        abstime_op: OpTy<'tcx, Tag>,\n+        dest: PlaceTy<'tcx, Tag>,\n+    ) -> InterpResult<'tcx> {\n+        let this = self.eval_context_mut();\n+\n+        this.check_no_isolation(\"pthread_cond_timedwait\")?;\n+\n+        let id = cond_get_or_create_id(this, cond_op)?;\n+        let mutex_id = mutex_get_or_create_id(this, mutex_op)?;\n+        let active_thread = this.get_active_thread()?;\n+\n+        release_cond_mutex(this, active_thread, mutex_id)?;\n+        this.condvar_wait(id, active_thread, mutex_id);\n+\n+        // We return success for now and override it in the timeout callback.\n+        this.write_scalar(Scalar::from_i32(0), dest)?;\n+\n+        // Extract the timeout.\n+        let clock_id = cond_get_clock_id(this, cond_op)?.to_i32()?;\n+        let duration = {\n+            let tp = this.deref_operand(abstime_op)?;\n+            let mut offset = Size::from_bytes(0);\n+            let layout = this.libc_ty_layout(\"time_t\")?;\n+            let seconds_place = tp.offset(offset, MemPlaceMeta::None, layout, this)?;\n+            let seconds = this.read_scalar(seconds_place.into())?.to_u64()?;\n+            offset += layout.size;\n+            let layout = this.libc_ty_layout(\"c_long\")?;\n+            let nanoseconds_place = tp.offset(offset, MemPlaceMeta::None, layout, this)?;\n+            let nanoseconds = this.read_scalar(nanoseconds_place.into())?.to_u64()?;\n+            Duration::new(seconds, nanoseconds as u32)\n+        };\n+\n+        let timeout_time = if clock_id == this.eval_libc_i32(\"CLOCK_REALTIME\")? {\n+            let time_anchor_since_epoch =\n+                this.machine.time_anchor_timestamp.duration_since(SystemTime::UNIX_EPOCH).unwrap();\n+            let duration_since_time_anchor = duration.checked_sub(time_anchor_since_epoch).unwrap();\n+            this.machine.time_anchor.checked_add(duration_since_time_anchor).unwrap()\n+        } else if clock_id == this.eval_libc_i32(\"CLOCK_MONOTONIC\")? {\n+            this.machine.time_anchor.checked_add(duration).unwrap()\n+        } else {\n+            throw_ub_format!(\"Unsupported clock id.\");\n+        };\n+\n+        // Register the timeout callback.\n+        this.register_callback(\n+            active_thread,\n+            timeout_time,\n+            Box::new(move |ecx| {\n+                // Try to reacquire the mutex.\n+                reacquire_cond_mutex(ecx, active_thread, mutex_id)?;\n+\n+                // Remove the thread from the conditional variable.\n+                ecx.condvar_remove_waiter(id, active_thread);\n+\n+                // Set the timeout value.\n+                let timeout = ecx.eval_libc_i32(\"ETIMEDOUT\")?;\n+                ecx.write_scalar(Scalar::from_i32(timeout), dest)?;\n+\n+                Ok(())\n+            }),\n+        )?;\n+\n+        Ok(())\n+    }\n+\n+    fn pthread_cond_destroy(&mut self, cond_op: OpTy<'tcx, Tag>) -> InterpResult<'tcx, i32> {\n+        let this = self.eval_context_mut();\n+\n+        let id = cond_get_or_create_id(this, cond_op)?;\n+        if this.condvar_is_awaited(id) {\n+            throw_ub_format!(\"destroyed an awaited conditional variable\");\n+        }\n+        cond_set_id(this, cond_op, ScalarMaybeUndef::Undef)?;\n+        cond_set_clock_id(this, cond_op, ScalarMaybeUndef::Undef)?;\n \n         Ok(0)\n     }"}, {"sha": "5d181692fb2a3a94089d9f2a004e4c75e287cca6", "filename": "src/sync.rs", "status": "added", "additions": 299, "deletions": 0, "changes": 299, "blob_url": "https://github.com/rust-lang/rust/blob/679245769b6984ec5a7edf70fb4744d8411468b8/src%2Fsync.rs", "raw_url": "https://github.com/rust-lang/rust/raw/679245769b6984ec5a7edf70fb4744d8411468b8/src%2Fsync.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Fsync.rs?ref=679245769b6984ec5a7edf70fb4744d8411468b8", "patch": "@@ -0,0 +1,299 @@\n+use std::collections::{hash_map::Entry, HashMap, VecDeque};\n+use std::convert::TryFrom;\n+use std::num::NonZeroU32;\n+use std::time::Instant;\n+\n+use rustc_index::vec::{Idx, IndexVec};\n+\n+use crate::*;\n+\n+macro_rules! declare_id {\n+    ($name: ident) => {\n+        #[derive(Clone, Copy, Debug, PartialOrd, Ord, PartialEq, Eq, Hash)]\n+        pub struct $name(NonZeroU32);\n+\n+        impl Idx for $name {\n+            fn new(idx: usize) -> Self {\n+                $name(NonZeroU32::new(u32::try_from(idx).unwrap() + 1).unwrap())\n+            }\n+            fn index(self) -> usize {\n+                usize::try_from(self.0.get() - 1).unwrap()\n+            }\n+        }\n+\n+        impl From<u32> for $name {\n+            fn from(id: u32) -> Self {\n+                Self(NonZeroU32::new(id).unwrap())\n+            }\n+        }\n+\n+        impl $name {\n+            pub fn to_u32_scalar<'tcx>(&self) -> Scalar<Tag> {\n+                Scalar::from_u32(self.0.get())\n+            }\n+        }\n+    };\n+}\n+\n+declare_id!(MutexId);\n+\n+/// The mutex state.\n+#[derive(Default, Debug)]\n+struct Mutex {\n+    /// The thread that currently owns the lock.\n+    owner: Option<ThreadId>,\n+    /// How many times the mutex was locked by the owner.\n+    lock_count: usize,\n+    /// The queue of threads waiting for this mutex.\n+    queue: VecDeque<ThreadId>,\n+}\n+\n+declare_id!(RwLockId);\n+\n+/// The read-write lock state.\n+#[derive(Default, Debug)]\n+struct RwLock {\n+    /// The writer thread that currently owns the lock.\n+    writer: Option<ThreadId>,\n+    /// The readers that currently own the lock and how many times they acquired\n+    /// the lock.\n+    readers: HashMap<ThreadId, usize>,\n+    /// The queue of writer threads waiting for this lock.\n+    writer_queue: VecDeque<ThreadId>,\n+    /// The queue of reader threads waiting for this lock.\n+    reader_queue: VecDeque<ThreadId>,\n+}\n+\n+declare_id!(CondvarId);\n+\n+/// A thread waiting on a conditional variable.\n+#[derive(Debug)]\n+struct CondvarWaiter {\n+    /// The thread that is waiting on this variable.\n+    thread: ThreadId,\n+    /// The mutex on which the thread is waiting.\n+    mutex: MutexId,\n+    /// The moment in time when the waiter should time out.\n+    timeout: Option<Instant>,\n+}\n+\n+/// The conditional variable state.\n+#[derive(Default, Debug)]\n+struct Condvar {\n+    waiters: VecDeque<CondvarWaiter>,\n+}\n+\n+/// The state of all synchronization variables.\n+#[derive(Default, Debug)]\n+pub(super) struct SynchronizationState {\n+    mutexes: IndexVec<MutexId, Mutex>,\n+    rwlocks: IndexVec<RwLockId, RwLock>,\n+    condvars: IndexVec<CondvarId, Condvar>,\n+}\n+\n+// Public interface to synchronization primitives. Please note that in most\n+// cases, the function calls are infallible and it is the client's (shim\n+// implementation's) responsibility to detect and deal with erroneous\n+// situations.\n+impl<'mir, 'tcx: 'mir> EvalContextExt<'mir, 'tcx> for crate::MiriEvalContext<'mir, 'tcx> {}\n+pub trait EvalContextExt<'mir, 'tcx: 'mir>: crate::MiriEvalContextExt<'mir, 'tcx> {\n+    #[inline]\n+    /// Create state for a new mutex.\n+    fn mutex_create(&mut self) -> MutexId {\n+        let this = self.eval_context_mut();\n+        this.machine.threads.sync.mutexes.push(Default::default())\n+    }\n+\n+    #[inline]\n+    /// Get the id of the thread that currently owns this lock.\n+    fn mutex_get_owner(&mut self, id: MutexId) -> ThreadId {\n+        let this = self.eval_context_ref();\n+        this.machine.threads.sync.mutexes[id].owner.unwrap()\n+    }\n+\n+    #[inline]\n+    /// Check if locked.\n+    fn mutex_is_locked(&mut self, id: MutexId) -> bool {\n+        let this = self.eval_context_mut();\n+        this.machine.threads.sync.mutexes[id].owner.is_some()\n+    }\n+\n+    /// Lock by setting the mutex owner and increasing the lock count.\n+    fn mutex_lock(&mut self, id: MutexId, thread: ThreadId) {\n+        let this = self.eval_context_mut();\n+        let mutex = &mut this.machine.threads.sync.mutexes[id];\n+        if let Some(current_owner) = mutex.owner {\n+            assert_eq!(thread, current_owner, \"mutex already locked by another thread\");\n+            assert!(\n+                mutex.lock_count > 0,\n+                \"invariant violation: lock_count == 0 iff the thread is unlocked\"\n+            );\n+        } else {\n+            mutex.owner = Some(thread);\n+        }\n+        mutex.lock_count = mutex.lock_count.checked_add(1).unwrap();\n+    }\n+\n+    /// Unlock by decreasing the lock count. If the lock count reaches 0, unset\n+    /// the owner.\n+    fn mutex_unlock(&mut self, id: MutexId) -> Option<(ThreadId, usize)> {\n+        let this = self.eval_context_mut();\n+        let mutex = &mut this.machine.threads.sync.mutexes[id];\n+        if let Some(current_owner) = mutex.owner {\n+            mutex.lock_count = mutex\n+                .lock_count\n+                .checked_sub(1)\n+                .expect(\"invariant violation: lock_count == 0 iff the thread is unlocked\");\n+            if mutex.lock_count == 0 {\n+                mutex.owner = None;\n+            }\n+            Some((current_owner, mutex.lock_count))\n+        } else {\n+            None\n+        }\n+    }\n+\n+    #[inline]\n+    /// Take a thread out the queue waiting for the lock.\n+    fn mutex_enqueue(&mut self, id: MutexId, thread: ThreadId) {\n+        let this = self.eval_context_mut();\n+        this.machine.threads.sync.mutexes[id].queue.push_back(thread);\n+    }\n+\n+    #[inline]\n+    /// Take a thread out the queue waiting for the lock.\n+    fn mutex_dequeue(&mut self, id: MutexId) -> Option<ThreadId> {\n+        let this = self.eval_context_mut();\n+        this.machine.threads.sync.mutexes[id].queue.pop_front()\n+    }\n+\n+    #[inline]\n+    /// Create state for a new read write lock.\n+    fn rwlock_create(&mut self) -> RwLockId {\n+        let this = self.eval_context_mut();\n+        this.machine.threads.sync.rwlocks.push(Default::default())\n+    }\n+\n+    #[inline]\n+    /// Check if locked.\n+    fn rwlock_is_locked(&mut self, id: RwLockId) -> bool {\n+        let this = self.eval_context_mut();\n+        this.machine.threads.sync.rwlocks[id].writer.is_some()\n+            || !this.machine.threads.sync.rwlocks[id].readers.is_empty()\n+    }\n+\n+    #[inline]\n+    /// Check if write locked.\n+    fn rwlock_is_write_locked(&mut self, id: RwLockId) -> bool {\n+        let this = self.eval_context_mut();\n+        this.machine.threads.sync.rwlocks[id].writer.is_some()\n+    }\n+\n+    /// Add a reader that collectively with other readers owns the lock.\n+    fn rwlock_reader_add(&mut self, id: RwLockId, reader: ThreadId) {\n+        let this = self.eval_context_mut();\n+        assert!(!this.rwlock_is_write_locked(id), \"the lock is write locked\");\n+        let count = this.machine.threads.sync.rwlocks[id].readers.entry(reader).or_insert(0);\n+        *count += 1;\n+    }\n+\n+    /// Try removing the reader. Returns `true` if succeeded.\n+    fn rwlock_reader_remove(&mut self, id: RwLockId, reader: ThreadId) -> bool {\n+        let this = self.eval_context_mut();\n+        match this.machine.threads.sync.rwlocks[id].readers.entry(reader) {\n+            Entry::Occupied(mut entry) => {\n+                let count = entry.get_mut();\n+                *count -= 1;\n+                if *count == 0 {\n+                    entry.remove();\n+                }\n+                true\n+            }\n+            Entry::Vacant(_) => false,\n+        }\n+    }\n+\n+    #[inline]\n+    /// Put the reader in the queue waiting for the lock.\n+    fn rwlock_enqueue_reader(&mut self, id: RwLockId, reader: ThreadId) {\n+        let this = self.eval_context_mut();\n+        assert!(this.rwlock_is_write_locked(id), \"queueing on not write locked lock\");\n+        this.machine.threads.sync.rwlocks[id].reader_queue.push_back(reader);\n+    }\n+\n+    #[inline]\n+    /// Take the reader out the queue waiting for the lock.\n+    fn rwlock_dequeue_reader(&mut self, id: RwLockId) -> Option<ThreadId> {\n+        let this = self.eval_context_mut();\n+        this.machine.threads.sync.rwlocks[id].reader_queue.pop_front()\n+    }\n+\n+    #[inline]\n+    /// Lock by setting the writer that owns the lock.\n+    fn rwlock_writer_set(&mut self, id: RwLockId, writer: ThreadId) {\n+        let this = self.eval_context_mut();\n+        assert!(!this.rwlock_is_locked(id), \"the lock is already locked\");\n+        this.machine.threads.sync.rwlocks[id].writer = Some(writer);\n+    }\n+\n+    #[inline]\n+    /// Try removing the writer.\n+    fn rwlock_writer_remove(&mut self, id: RwLockId) -> Option<ThreadId> {\n+        let this = self.eval_context_mut();\n+        this.machine.threads.sync.rwlocks[id].writer.take()\n+    }\n+\n+    #[inline]\n+    /// Put the writer in the queue waiting for the lock.\n+    fn rwlock_enqueue_writer(&mut self, id: RwLockId, writer: ThreadId) {\n+        let this = self.eval_context_mut();\n+        assert!(this.rwlock_is_locked(id), \"queueing on unlocked lock\");\n+        this.machine.threads.sync.rwlocks[id].writer_queue.push_back(writer);\n+    }\n+\n+    #[inline]\n+    /// Take the writer out the queue waiting for the lock.\n+    fn rwlock_dequeue_writer(&mut self, id: RwLockId) -> Option<ThreadId> {\n+        let this = self.eval_context_mut();\n+        this.machine.threads.sync.rwlocks[id].writer_queue.pop_front()\n+    }\n+\n+    #[inline]\n+    /// Create state for a new conditional variable.\n+    fn condvar_create(&mut self) -> CondvarId {\n+        let this = self.eval_context_mut();\n+        this.machine.threads.sync.condvars.push(Default::default())\n+    }\n+\n+    #[inline]\n+    /// Is the conditional variable awaited?\n+    fn condvar_is_awaited(&mut self, id: CondvarId) -> bool {\n+        let this = self.eval_context_mut();\n+        !this.machine.threads.sync.condvars[id].waiters.is_empty()\n+    }\n+\n+    /// Mark that the thread is waiting on the conditional variable.\n+    fn condvar_wait(&mut self, id: CondvarId, thread: ThreadId, mutex: MutexId) {\n+        let this = self.eval_context_mut();\n+        let waiters = &mut this.machine.threads.sync.condvars[id].waiters;\n+        assert!(waiters.iter().all(|waiter| waiter.thread != thread), \"thread is already waiting\");\n+        waiters.push_back(CondvarWaiter { thread, mutex, timeout: None });\n+    }\n+\n+    /// Wake up some thread (if there is any) sleeping on the conditional\n+    /// variable.\n+    fn condvar_signal(&mut self, id: CondvarId) -> Option<(ThreadId, MutexId)> {\n+        let this = self.eval_context_mut();\n+        this.machine.threads.sync.condvars[id]\n+            .waiters\n+            .pop_front()\n+            .map(|waiter| (waiter.thread, waiter.mutex))\n+    }\n+\n+    #[inline]\n+    /// Remove the thread from the queue of threads waiting on this conditional variable.\n+    fn condvar_remove_waiter(&mut self, id: CondvarId, thread: ThreadId) {\n+        let this = self.eval_context_mut();\n+        this.machine.threads.sync.condvars[id].waiters.retain(|waiter| waiter.thread != thread);\n+    }\n+}"}, {"sha": "6ebf35a6527f565874e1649f8cd614948751b33d", "filename": "src/thread.rs", "status": "modified", "additions": 118, "deletions": 29, "changes": 147, "blob_url": "https://github.com/rust-lang/rust/blob/679245769b6984ec5a7edf70fb4744d8411468b8/src%2Fthread.rs", "raw_url": "https://github.com/rust-lang/rust/raw/679245769b6984ec5a7edf70fb4744d8411468b8/src%2Fthread.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Fthread.rs?ref=679245769b6984ec5a7edf70fb4744d8411468b8", "patch": "@@ -1,8 +1,10 @@\n //! Implements threads.\n \n use std::cell::RefCell;\n+use std::collections::hash_map::Entry;\n use std::convert::TryFrom;\n use std::num::{NonZeroU32, TryFromIntError};\n+use std::time::Instant;\n \n use log::trace;\n \n@@ -15,18 +17,24 @@ use rustc_middle::{\n     ty::{self, Instance},\n };\n \n+use crate::sync::SynchronizationState;\n use crate::*;\n \n #[derive(Clone, Copy, Debug, PartialEq, Eq)]\n pub enum SchedulingAction {\n     /// Execute step on the active thread.\n     ExecuteStep,\n+    /// Execute a scheduler's callback.\n+    ExecuteCallback,\n     /// Execute destructors of the active thread.\n     ExecuteDtors,\n     /// Stop the program.\n     Stop,\n }\n \n+type EventCallback<'mir, 'tcx> =\n+    Box<dyn FnOnce(&mut InterpCx<'mir, 'tcx, Evaluator<'mir, 'tcx>>) -> InterpResult<'tcx> + 'tcx>;\n+\n /// A thread identifier.\n #[derive(Clone, Copy, Debug, PartialOrd, Ord, PartialEq, Eq, Hash)]\n pub struct ThreadId(u32);\n@@ -94,6 +102,7 @@ pub enum ThreadState {\n     BlockedOnJoin(ThreadId),\n     /// The thread is blocked and belongs to the given blockset.\n     Blocked(BlockSetId),\n+    BlockedThread,\n     /// The thread has terminated its execution (we do not delete terminated\n     /// threads).\n     Terminated,\n@@ -162,6 +171,23 @@ impl<'mir, 'tcx> Default for Thread<'mir, 'tcx> {\n     }\n }\n \n+/// Callbacks are used to implement timeouts. For example, waiting on a\n+/// conditional variable with a timeout creates a callback that is called after\n+/// the specified time and unblocks the thread. If another thread signals on the\n+/// conditional variable, the signal handler deletes the callback.\n+struct CallBackInfo<'mir, 'tcx> {\n+    /// The callback should be called no earlier than this time.\n+    call_time: Instant,\n+    /// The called function.\n+    callback: EventCallback<'mir, 'tcx>,\n+}\n+\n+impl<'mir, 'tcx> std::fmt::Debug for CallBackInfo<'mir, 'tcx> {\n+    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {\n+        write!(f, \"CallBack({:?})\", self.call_time)\n+    }\n+}\n+\n /// A set of threads.\n #[derive(Debug)]\n pub struct ThreadManager<'mir, 'tcx> {\n@@ -171,13 +197,17 @@ pub struct ThreadManager<'mir, 'tcx> {\n     ///\n     /// Note that this vector also contains terminated threads.\n     threads: IndexVec<ThreadId, Thread<'mir, 'tcx>>,\n+    /// FIXME: make private.\n+    pub(crate) sync: SynchronizationState,\n     /// A counter used to generate unique identifiers for blocksets.\n     blockset_counter: u32,\n     /// A mapping from a thread-local static to an allocation id of a thread\n     /// specific allocation.\n     thread_local_alloc_ids: RefCell<FxHashMap<(DefId, ThreadId), AllocId>>,\n     /// A flag that indicates that we should change the active thread.\n     yield_active_thread: bool,\n+    /// Callbacks that are called once the specified time passes.\n+    callbacks: FxHashMap<ThreadId, CallBackInfo<'mir, 'tcx>>,\n }\n \n impl<'mir, 'tcx> Default for ThreadManager<'mir, 'tcx> {\n@@ -191,9 +221,11 @@ impl<'mir, 'tcx> Default for ThreadManager<'mir, 'tcx> {\n         Self {\n             active_thread: ThreadId::new(0),\n             threads: threads,\n+            sync: SynchronizationState::default(),\n             blockset_counter: 0,\n             thread_local_alloc_ids: Default::default(),\n             yield_active_thread: false,\n+            callbacks: FxHashMap::default(),\n         }\n     }\n }\n@@ -321,37 +353,58 @@ impl<'mir, 'tcx: 'mir> ThreadManager<'mir, 'tcx> {\n         self.active_thread_ref().thread_name()\n     }\n \n-    /// Allocate a new blockset id.\n-    fn create_blockset(&mut self) -> BlockSetId {\n-        self.blockset_counter = self.blockset_counter.checked_add(1).unwrap();\n-        BlockSetId::new(self.blockset_counter)\n-    }\n-\n-    /// Block the currently active thread and put it into the given blockset.\n-    fn block_active_thread(&mut self, set: BlockSetId) {\n-        let state = &mut self.active_thread_mut().state;\n+    /// Put the thread into the blocked state.\n+    fn block_thread(&mut self, thread: ThreadId) {\n+        let state = &mut self.threads[thread].state;\n         assert_eq!(*state, ThreadState::Enabled);\n-        *state = ThreadState::Blocked(set);\n+        *state = ThreadState::BlockedThread;\n     }\n \n-    /// Unblock any one thread from the given blockset if it contains at least\n-    /// one. Return the id of the unblocked thread.\n-    fn unblock_some_thread(&mut self, set: BlockSetId) -> Option<ThreadId> {\n-        for (id, thread) in self.threads.iter_enumerated_mut() {\n-            if thread.state == ThreadState::Blocked(set) {\n-                trace!(\"unblocking {:?} in blockset {:?}\", id, set);\n-                thread.state = ThreadState::Enabled;\n-                return Some(id);\n-            }\n-        }\n-        None\n+    /// Put the blocked thread into the enabled state.\n+    fn unblock_thread(&mut self, thread: ThreadId) {\n+        let state = &mut self.threads[thread].state;\n+        assert_eq!(*state, ThreadState::BlockedThread);\n+        *state = ThreadState::Enabled;\n     }\n \n     /// Change the active thread to some enabled thread.\n     fn yield_active_thread(&mut self) {\n         self.yield_active_thread = true;\n     }\n \n+    /// Register the given `callback` to be called once the `call_time` passes.\n+    fn register_callback(\n+        &mut self,\n+        thread: ThreadId,\n+        call_time: Instant,\n+        callback: EventCallback<'mir, 'tcx>,\n+    ) {\n+        self.callbacks\n+            .insert(thread, CallBackInfo { call_time: call_time, callback: callback })\n+            .unwrap_none();\n+    }\n+\n+    /// Unregister the callback for the `thread`.\n+    fn unregister_callback_if_exists(&mut self, thread: ThreadId) {\n+        self.callbacks.remove(&thread);\n+    }\n+\n+    /// Get a callback that is ready to be called.\n+    fn get_callback(&mut self) -> Option<(ThreadId, EventCallback<'mir, 'tcx>)> {\n+        let current_time = Instant::now();\n+        // We use a for loop here to make the scheduler more deterministic.\n+        for thread in self.threads.indices() {\n+            match self.callbacks.entry(thread) {\n+                Entry::Occupied(entry) =>\n+                    if current_time >= entry.get().call_time {\n+                        return Some((thread, entry.remove().callback));\n+                    },\n+                Entry::Vacant(_) => {}\n+            }\n+        }\n+        None\n+    }\n+\n     /// Decide which action to take next and on which thread.\n     ///\n     /// The currently implemented scheduling policy is the one that is commonly\n@@ -407,6 +460,18 @@ impl<'mir, 'tcx: 'mir> ThreadManager<'mir, 'tcx> {\n         // We have not found a thread to execute.\n         if self.threads.iter().all(|thread| thread.state == ThreadState::Terminated) {\n             unreachable!();\n+        } else if let Some(next_call_time) =\n+            self.callbacks.values().min_by_key(|info| info.call_time)\n+        {\n+            // All threads are currently blocked, but we have unexecuted\n+            // callbacks, which may unblock some of the threads. Hence,\n+            // sleep until the first callback.\n+            if let Some(sleep_time) =\n+                next_call_time.call_time.checked_duration_since(Instant::now())\n+            {\n+                std::thread::sleep(sleep_time);\n+            }\n+            Ok(SchedulingAction::ExecuteCallback)\n         } else {\n             throw_machine_stop!(TerminationInfo::Deadlock);\n         }\n@@ -577,27 +642,51 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: crate::MiriEvalContextExt<'mir, 'tcx\n     }\n \n     #[inline]\n-    fn create_blockset(&mut self) -> InterpResult<'tcx, BlockSetId> {\n+    fn block_thread(&mut self, thread: ThreadId) -> InterpResult<'tcx> {\n         let this = self.eval_context_mut();\n-        Ok(this.machine.threads.create_blockset())\n+        Ok(this.machine.threads.block_thread(thread))\n     }\n \n     #[inline]\n-    fn block_active_thread(&mut self, set: BlockSetId) -> InterpResult<'tcx> {\n+    fn unblock_thread(&mut self, thread: ThreadId) -> InterpResult<'tcx> {\n         let this = self.eval_context_mut();\n-        Ok(this.machine.threads.block_active_thread(set))\n+        Ok(this.machine.threads.unblock_thread(thread))\n     }\n \n     #[inline]\n-    fn unblock_some_thread(&mut self, set: BlockSetId) -> InterpResult<'tcx, Option<ThreadId>> {\n+    fn yield_active_thread(&mut self) -> InterpResult<'tcx> {\n         let this = self.eval_context_mut();\n-        Ok(this.machine.threads.unblock_some_thread(set))\n+        this.machine.threads.yield_active_thread();\n+        Ok(())\n     }\n \n     #[inline]\n-    fn yield_active_thread(&mut self) -> InterpResult<'tcx> {\n+    fn register_callback(\n+        &mut self,\n+        thread: ThreadId,\n+        call_time: Instant,\n+        callback: EventCallback<'mir, 'tcx>,\n+    ) -> InterpResult<'tcx> {\n         let this = self.eval_context_mut();\n-        this.machine.threads.yield_active_thread();\n+        this.machine.threads.register_callback(thread, call_time, callback);\n+        Ok(())\n+    }\n+\n+    #[inline]\n+    fn unregister_callback_if_exists(&mut self, thread: ThreadId) -> InterpResult<'tcx> {\n+        let this = self.eval_context_mut();\n+        this.machine.threads.unregister_callback_if_exists(thread);\n+        Ok(())\n+    }\n+\n+    /// Execute the callback on the callback's thread.\n+    #[inline]\n+    fn run_scheduler_callback(&mut self) -> InterpResult<'tcx> {\n+        let this = self.eval_context_mut();\n+        let (thread, callback) = this.machine.threads.get_callback().expect(\"no callback found\");\n+        let old_thread = this.set_active_thread(thread)?;\n+        callback(this)?;\n+        this.set_active_thread(old_thread)?;\n         Ok(())\n     }\n "}, {"sha": "1e976a63453dbd8fd5e376a97203f7dbcc802d92", "filename": "tests/run-pass/concurrency/barrier.rs", "status": "added", "additions": 27, "deletions": 0, "changes": 27, "blob_url": "https://github.com/rust-lang/rust/blob/679245769b6984ec5a7edf70fb4744d8411468b8/tests%2Frun-pass%2Fconcurrency%2Fbarrier.rs", "raw_url": "https://github.com/rust-lang/rust/raw/679245769b6984ec5a7edf70fb4744d8411468b8/tests%2Frun-pass%2Fconcurrency%2Fbarrier.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/tests%2Frun-pass%2Fconcurrency%2Fbarrier.rs?ref=679245769b6984ec5a7edf70fb4744d8411468b8", "patch": "@@ -0,0 +1,27 @@\n+// ignore-windows: Concurrency on Windows is not supported yet.\n+\n+//! Check if Rust barriers are working.\n+\n+use std::sync::{Arc, Barrier};\n+use std::thread;\n+\n+\n+/// This test is taken from the Rust documentation.\n+fn main() {\n+    let mut handles = Vec::with_capacity(10);\n+    let barrier = Arc::new(Barrier::new(10));\n+    for _ in 0..10 {\n+        let c = barrier.clone();\n+        // The same messages will be printed together.\n+        // You will NOT see any interleaving.\n+        handles.push(thread::spawn(move|| {\n+            println!(\"before wait\");\n+            c.wait();\n+            println!(\"after wait\");\n+        }));\n+    }\n+    // Wait for other threads to finish.\n+    for handle in handles {\n+        handle.join().unwrap();\n+    }\n+}\n\\ No newline at end of file"}, {"sha": "2dbfb7721d3683fb4d82c2ce771e3784be86fdc9", "filename": "tests/run-pass/concurrency/barrier.stderr", "status": "added", "additions": 2, "deletions": 0, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/679245769b6984ec5a7edf70fb4744d8411468b8/tests%2Frun-pass%2Fconcurrency%2Fbarrier.stderr", "raw_url": "https://github.com/rust-lang/rust/raw/679245769b6984ec5a7edf70fb4744d8411468b8/tests%2Frun-pass%2Fconcurrency%2Fbarrier.stderr", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/tests%2Frun-pass%2Fconcurrency%2Fbarrier.stderr?ref=679245769b6984ec5a7edf70fb4744d8411468b8", "patch": "@@ -0,0 +1,2 @@\n+warning: thread support is experimental. For example, Miri does not detect data races yet.\n+"}, {"sha": "f2c036a1735eda7186df3a4b5249b8fc8abe5896", "filename": "tests/run-pass/concurrency/barrier.stdout", "status": "added", "additions": 20, "deletions": 0, "changes": 20, "blob_url": "https://github.com/rust-lang/rust/blob/679245769b6984ec5a7edf70fb4744d8411468b8/tests%2Frun-pass%2Fconcurrency%2Fbarrier.stdout", "raw_url": "https://github.com/rust-lang/rust/raw/679245769b6984ec5a7edf70fb4744d8411468b8/tests%2Frun-pass%2Fconcurrency%2Fbarrier.stdout", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/tests%2Frun-pass%2Fconcurrency%2Fbarrier.stdout?ref=679245769b6984ec5a7edf70fb4744d8411468b8", "patch": "@@ -0,0 +1,20 @@\n+before wait\n+before wait\n+before wait\n+before wait\n+before wait\n+before wait\n+before wait\n+before wait\n+before wait\n+before wait\n+after wait\n+after wait\n+after wait\n+after wait\n+after wait\n+after wait\n+after wait\n+after wait\n+after wait\n+after wait"}, {"sha": "ab971ee6e8c632d9ffeaac131aef6d8652039d9a", "filename": "tests/run-pass/concurrency/condvar.rs", "status": "added", "additions": 28, "deletions": 0, "changes": 28, "blob_url": "https://github.com/rust-lang/rust/blob/679245769b6984ec5a7edf70fb4744d8411468b8/tests%2Frun-pass%2Fconcurrency%2Fcondvar.rs", "raw_url": "https://github.com/rust-lang/rust/raw/679245769b6984ec5a7edf70fb4744d8411468b8/tests%2Frun-pass%2Fconcurrency%2Fcondvar.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/tests%2Frun-pass%2Fconcurrency%2Fcondvar.rs?ref=679245769b6984ec5a7edf70fb4744d8411468b8", "patch": "@@ -0,0 +1,28 @@\n+// ignore-windows: Concurrency on Windows is not supported yet.\n+\n+//! Check if Rust conditional variables are working.\n+\n+use std::sync::{Arc, Condvar, Mutex};\n+use std::thread;\n+\n+/// The test taken from the Rust documentation.\n+fn main() {\n+    let pair = Arc::new((Mutex::new(false), Condvar::new()));\n+    let pair2 = pair.clone();\n+\n+    // Inside of our lock, spawn a new thread, and then wait for it to start.\n+    thread::spawn(move || {\n+        let (lock, cvar) = &*pair2;\n+        let mut started = lock.lock().unwrap();\n+        *started = true;\n+        // We notify the condvar that the value has changed.\n+        cvar.notify_one();\n+    });\n+\n+    // Wait for the thread to start up.\n+    let (lock, cvar) = &*pair;\n+    let mut started = lock.lock().unwrap();\n+    while !*started {\n+        started = cvar.wait(started).unwrap();\n+    }\n+}"}, {"sha": "2dbfb7721d3683fb4d82c2ce771e3784be86fdc9", "filename": "tests/run-pass/concurrency/condvar.stderr", "status": "added", "additions": 2, "deletions": 0, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/679245769b6984ec5a7edf70fb4744d8411468b8/tests%2Frun-pass%2Fconcurrency%2Fcondvar.stderr", "raw_url": "https://github.com/rust-lang/rust/raw/679245769b6984ec5a7edf70fb4744d8411468b8/tests%2Frun-pass%2Fconcurrency%2Fcondvar.stderr", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/tests%2Frun-pass%2Fconcurrency%2Fcondvar.stderr?ref=679245769b6984ec5a7edf70fb4744d8411468b8", "patch": "@@ -0,0 +1,2 @@\n+warning: thread support is experimental. For example, Miri does not detect data races yet.\n+"}, {"sha": "83a651e6f04a71c3393b26395f05163c84bdb042", "filename": "tests/run-pass/concurrency/libc_pthread_cond.rs", "status": "added", "additions": 199, "deletions": 0, "changes": 199, "blob_url": "https://github.com/rust-lang/rust/blob/679245769b6984ec5a7edf70fb4744d8411468b8/tests%2Frun-pass%2Fconcurrency%2Flibc_pthread_cond.rs", "raw_url": "https://github.com/rust-lang/rust/raw/679245769b6984ec5a7edf70fb4744d8411468b8/tests%2Frun-pass%2Fconcurrency%2Flibc_pthread_cond.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/tests%2Frun-pass%2Fconcurrency%2Flibc_pthread_cond.rs?ref=679245769b6984ec5a7edf70fb4744d8411468b8", "patch": "@@ -0,0 +1,199 @@\n+// ignore-windows: No libc on Windows\n+// compile-flags: -Zmiri-disable-isolation\n+\n+#![feature(rustc_private)]\n+\n+extern crate libc;\n+\n+use std::cell::UnsafeCell;\n+use std::mem::{self, MaybeUninit};\n+use std::sync::Arc;\n+use std::thread;\n+\n+struct Mutex {\n+    inner: UnsafeCell<libc::pthread_mutex_t>,\n+}\n+\n+unsafe impl Sync for Mutex {}\n+\n+impl std::fmt::Debug for Mutex {\n+    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {\n+        write!(f, \"Mutex\")\n+    }\n+}\n+\n+struct Cond {\n+    inner: UnsafeCell<libc::pthread_cond_t>,\n+}\n+\n+unsafe impl Sync for Cond {}\n+\n+impl std::fmt::Debug for Cond {\n+    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {\n+        write!(f, \"Cond\")\n+    }\n+}\n+\n+unsafe fn create_cond_attr_monotonic() -> libc::pthread_condattr_t {\n+    let mut attr = MaybeUninit::<libc::pthread_condattr_t>::uninit();\n+    assert_eq!(libc::pthread_condattr_init(attr.as_mut_ptr()), 0);\n+    assert_eq!(libc::pthread_condattr_setclock(attr.as_mut_ptr(), libc::CLOCK_MONOTONIC), 0);\n+    attr.assume_init()\n+}\n+\n+unsafe fn create_cond(attr: Option<libc::pthread_condattr_t>) -> Cond {\n+    let cond: Cond = mem::zeroed();\n+    if let Some(mut attr) = attr {\n+        assert_eq!(libc::pthread_cond_init(cond.inner.get() as *mut _, &attr as *const _), 0);\n+        assert_eq!(libc::pthread_condattr_destroy(&mut attr as *mut _), 0);\n+    } else {\n+        assert_eq!(libc::pthread_cond_init(cond.inner.get() as *mut _, 0 as *const _), 0);\n+    }\n+    cond\n+}\n+\n+unsafe fn create_mutex() -> Mutex {\n+    mem::zeroed()\n+}\n+\n+unsafe fn create_timeout(seconds: i64) -> libc::timespec {\n+    let mut now: libc::timespec = mem::zeroed();\n+    assert_eq!(libc::clock_gettime(libc::CLOCK_MONOTONIC, &mut now), 0);\n+    libc::timespec { tv_sec: now.tv_sec + seconds, tv_nsec: now.tv_nsec }\n+}\n+\n+fn test_pthread_condattr_t() {\n+    unsafe {\n+        let mut attr = create_cond_attr_monotonic();\n+        let mut clock_id = MaybeUninit::<libc::clockid_t>::uninit();\n+        assert_eq!(libc::pthread_condattr_getclock(&attr as *const _, clock_id.as_mut_ptr()), 0);\n+        assert_eq!(clock_id.assume_init(), libc::CLOCK_MONOTONIC);\n+        assert_eq!(libc::pthread_condattr_destroy(&mut attr as *mut _), 0);\n+    }\n+}\n+\n+fn test_signal() {\n+    unsafe {\n+        let cond = Arc::new(create_cond(None));\n+        let mutex = Arc::new(create_mutex());\n+\n+        assert_eq!(libc::pthread_mutex_lock(mutex.inner.get() as *mut _), 0);\n+\n+        let spawn_mutex = Arc::clone(&mutex);\n+        let spawn_cond = Arc::clone(&cond);\n+        let handle = thread::spawn(move || {\n+            assert_eq!(libc::pthread_mutex_lock(spawn_mutex.inner.get() as *mut _), 0);\n+            assert_eq!(libc::pthread_cond_signal(spawn_cond.inner.get() as *mut _), 0);\n+            assert_eq!(libc::pthread_mutex_unlock(spawn_mutex.inner.get() as *mut _), 0);\n+        });\n+\n+        assert_eq!(\n+            libc::pthread_cond_wait(cond.inner.get() as *mut _, mutex.inner.get() as *mut _),\n+            0\n+        );\n+        assert_eq!(libc::pthread_mutex_unlock(mutex.inner.get() as *mut _), 0);\n+\n+        handle.join().unwrap();\n+\n+        let mutex = Arc::try_unwrap(mutex).unwrap();\n+        assert_eq!(libc::pthread_mutex_destroy(mutex.inner.get() as *mut _), 0);\n+        let cond = Arc::try_unwrap(cond).unwrap();\n+        assert_eq!(libc::pthread_cond_destroy(cond.inner.get() as *mut _), 0);\n+    }\n+}\n+\n+fn test_broadcast() {\n+    unsafe {\n+        let cond = Arc::new(create_cond(None));\n+        let mutex = Arc::new(create_mutex());\n+\n+        assert_eq!(libc::pthread_mutex_lock(mutex.inner.get() as *mut _), 0);\n+\n+        let spawn_mutex = Arc::clone(&mutex);\n+        let spawn_cond = Arc::clone(&cond);\n+        let handle = thread::spawn(move || {\n+            assert_eq!(libc::pthread_mutex_lock(spawn_mutex.inner.get() as *mut _), 0);\n+            assert_eq!(libc::pthread_cond_broadcast(spawn_cond.inner.get() as *mut _), 0);\n+            assert_eq!(libc::pthread_mutex_unlock(spawn_mutex.inner.get() as *mut _), 0);\n+        });\n+\n+        assert_eq!(\n+            libc::pthread_cond_wait(cond.inner.get() as *mut _, mutex.inner.get() as *mut _),\n+            0\n+        );\n+        assert_eq!(libc::pthread_mutex_unlock(mutex.inner.get() as *mut _), 0);\n+\n+        handle.join().unwrap();\n+\n+        let mutex = Arc::try_unwrap(mutex).unwrap();\n+        assert_eq!(libc::pthread_mutex_destroy(mutex.inner.get() as *mut _), 0);\n+        let cond = Arc::try_unwrap(cond).unwrap();\n+        assert_eq!(libc::pthread_cond_destroy(cond.inner.get() as *mut _), 0);\n+    }\n+}\n+\n+fn test_timed_wait_timeout() {\n+    unsafe {\n+        let attr = create_cond_attr_monotonic();\n+        let cond = create_cond(Some(attr));\n+        let mutex = create_mutex();\n+        let timeout = create_timeout(1);\n+\n+        assert_eq!(libc::pthread_mutex_lock(mutex.inner.get() as *mut _), 0);\n+        assert_eq!(\n+            libc::pthread_cond_timedwait(\n+                cond.inner.get() as *mut _,\n+                mutex.inner.get() as *mut _,\n+                &timeout\n+            ),\n+            libc::ETIMEDOUT\n+        );\n+        assert_eq!(libc::pthread_mutex_unlock(mutex.inner.get() as *mut _), 0);\n+        assert_eq!(libc::pthread_mutex_destroy(mutex.inner.get() as *mut _), 0);\n+        assert_eq!(libc::pthread_cond_destroy(cond.inner.get() as *mut _), 0);\n+    }\n+}\n+\n+fn test_timed_wait_notimeout() {\n+    unsafe {\n+        let attr = create_cond_attr_monotonic();\n+        let cond = Arc::new(create_cond(Some(attr)));\n+        let mutex = Arc::new(create_mutex());\n+        let timeout = create_timeout(100);\n+\n+        assert_eq!(libc::pthread_mutex_lock(mutex.inner.get() as *mut _), 0);\n+\n+        let spawn_mutex = Arc::clone(&mutex);\n+        let spawn_cond = Arc::clone(&cond);\n+        let handle = thread::spawn(move || {\n+            assert_eq!(libc::pthread_mutex_lock(spawn_mutex.inner.get() as *mut _), 0);\n+            assert_eq!(libc::pthread_cond_signal(spawn_cond.inner.get() as *mut _), 0);\n+            assert_eq!(libc::pthread_mutex_unlock(spawn_mutex.inner.get() as *mut _), 0);\n+        });\n+\n+        assert_eq!(\n+            libc::pthread_cond_timedwait(\n+                cond.inner.get() as *mut _,\n+                mutex.inner.get() as *mut _,\n+                &timeout\n+            ),\n+            0\n+        );\n+        assert_eq!(libc::pthread_mutex_unlock(mutex.inner.get() as *mut _), 0);\n+\n+        handle.join().unwrap();\n+\n+        let mutex = Arc::try_unwrap(mutex).unwrap();\n+        assert_eq!(libc::pthread_mutex_destroy(mutex.inner.get() as *mut _), 0);\n+        let cond = Arc::try_unwrap(cond).unwrap();\n+        assert_eq!(libc::pthread_cond_destroy(cond.inner.get() as *mut _), 0);\n+    }\n+}\n+\n+fn main() {\n+    test_pthread_condattr_t();\n+    test_signal();\n+    test_broadcast();\n+    test_timed_wait_timeout();\n+    test_timed_wait_notimeout();\n+}"}, {"sha": "2dbfb7721d3683fb4d82c2ce771e3784be86fdc9", "filename": "tests/run-pass/concurrency/libc_pthread_cond.stderr", "status": "added", "additions": 2, "deletions": 0, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/679245769b6984ec5a7edf70fb4744d8411468b8/tests%2Frun-pass%2Fconcurrency%2Flibc_pthread_cond.stderr", "raw_url": "https://github.com/rust-lang/rust/raw/679245769b6984ec5a7edf70fb4744d8411468b8/tests%2Frun-pass%2Fconcurrency%2Flibc_pthread_cond.stderr", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/tests%2Frun-pass%2Fconcurrency%2Flibc_pthread_cond.stderr?ref=679245769b6984ec5a7edf70fb4744d8411468b8", "patch": "@@ -0,0 +1,2 @@\n+warning: thread support is experimental. For example, Miri does not detect data races yet.\n+"}, {"sha": "3558f5415d071559fa5dddd445564c652631d673", "filename": "tests/run-pass/concurrency/mpsc.rs", "status": "added", "additions": 56, "deletions": 0, "changes": 56, "blob_url": "https://github.com/rust-lang/rust/blob/679245769b6984ec5a7edf70fb4744d8411468b8/tests%2Frun-pass%2Fconcurrency%2Fmpsc.rs", "raw_url": "https://github.com/rust-lang/rust/raw/679245769b6984ec5a7edf70fb4744d8411468b8/tests%2Frun-pass%2Fconcurrency%2Fmpsc.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/tests%2Frun-pass%2Fconcurrency%2Fmpsc.rs?ref=679245769b6984ec5a7edf70fb4744d8411468b8", "patch": "@@ -0,0 +1,56 @@\n+// ignore-windows: Concurrency on Windows is not supported yet.\n+\n+//! Check if Rust channels are working.\n+\n+use std::sync::mpsc::{channel, sync_channel};\n+use std::thread;\n+\n+/// The test taken from the Rust documentation.\n+fn simple_send() {\n+    let (tx, rx) = channel();\n+    thread::spawn(move || {\n+        tx.send(10).unwrap();\n+    });\n+    assert_eq!(rx.recv().unwrap(), 10);\n+}\n+\n+/// The test taken from the Rust documentation.\n+fn multiple_send() {\n+    let (tx, rx) = channel();\n+    for i in 0..10 {\n+        let tx = tx.clone();\n+        thread::spawn(move || {\n+            tx.send(i).unwrap();\n+        });\n+    }\n+\n+    let mut sum = 0;\n+    for _ in 0..10 {\n+        let j = rx.recv().unwrap();\n+        assert!(0 <= j && j < 10);\n+        sum += j;\n+    }\n+    assert_eq!(sum, 45);\n+}\n+\n+/// The test taken from the Rust documentation.\n+fn send_on_sync() {\n+    let (sender, receiver) = sync_channel(1);\n+\n+    // this returns immediately\n+    sender.send(1).unwrap();\n+\n+    thread::spawn(move || {\n+        // this will block until the previous message has been received\n+        sender.send(2).unwrap();\n+    });\n+\n+    assert_eq!(receiver.recv().unwrap(), 1);\n+    assert_eq!(receiver.recv().unwrap(), 2);\n+}\n+\n+fn main() {\n+    simple_send();\n+    multiple_send();\n+    send_on_sync();\n+}"}, {"sha": "2dbfb7721d3683fb4d82c2ce771e3784be86fdc9", "filename": "tests/run-pass/concurrency/mpsc.stderr", "status": "added", "additions": 2, "deletions": 0, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/679245769b6984ec5a7edf70fb4744d8411468b8/tests%2Frun-pass%2Fconcurrency%2Fmpsc.stderr", "raw_url": "https://github.com/rust-lang/rust/raw/679245769b6984ec5a7edf70fb4744d8411468b8/tests%2Frun-pass%2Fconcurrency%2Fmpsc.stderr", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/tests%2Frun-pass%2Fconcurrency%2Fmpsc.stderr?ref=679245769b6984ec5a7edf70fb4744d8411468b8", "patch": "@@ -0,0 +1,2 @@\n+warning: thread support is experimental. For example, Miri does not detect data races yet.\n+"}, {"sha": "499ceacfa8c40a8b57f1dba09e02de2c346271f4", "filename": "tests/run-pass/concurrency/once.rs", "status": "added", "additions": 44, "deletions": 0, "changes": 44, "blob_url": "https://github.com/rust-lang/rust/blob/679245769b6984ec5a7edf70fb4744d8411468b8/tests%2Frun-pass%2Fconcurrency%2Fonce.rs", "raw_url": "https://github.com/rust-lang/rust/raw/679245769b6984ec5a7edf70fb4744d8411468b8/tests%2Frun-pass%2Fconcurrency%2Fonce.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/tests%2Frun-pass%2Fconcurrency%2Fonce.rs?ref=679245769b6984ec5a7edf70fb4744d8411468b8", "patch": "@@ -0,0 +1,44 @@\n+// ignore-windows: Concurrency on Windows is not supported yet.\n+\n+//! Check if Rust once statics are working. The test taken from the Rust\n+//! documentation.\n+\n+use std::sync::Once;\n+use std::thread;\n+\n+static mut VAL: usize = 0;\n+static INIT: Once = Once::new();\n+\n+fn get_cached_val() -> usize {\n+    unsafe {\n+        INIT.call_once(|| {\n+            VAL = expensive_computation();\n+        });\n+        VAL\n+    }\n+}\n+\n+fn expensive_computation() -> usize {\n+    let mut i = 1;\n+    let mut c = 1;\n+    while i < 10000 {\n+        i *= c;\n+        c += 1;\n+    }\n+    i\n+}\n+\n+fn main() {\n+    let handles: Vec<_> = (0..10)\n+        .map(|_| {\n+            thread::spawn(|| {\n+                thread::yield_now();\n+                let val = get_cached_val();\n+                assert_eq!(val, 40320);\n+            })\n+        })\n+        .collect();\n+    for handle in handles {\n+        handle.join().unwrap();\n+    }\n+}"}, {"sha": "2dbfb7721d3683fb4d82c2ce771e3784be86fdc9", "filename": "tests/run-pass/concurrency/once.stderr", "status": "added", "additions": 2, "deletions": 0, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/679245769b6984ec5a7edf70fb4744d8411468b8/tests%2Frun-pass%2Fconcurrency%2Fonce.stderr", "raw_url": "https://github.com/rust-lang/rust/raw/679245769b6984ec5a7edf70fb4744d8411468b8/tests%2Frun-pass%2Fconcurrency%2Fonce.stderr", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/tests%2Frun-pass%2Fconcurrency%2Fonce.stderr?ref=679245769b6984ec5a7edf70fb4744d8411468b8", "patch": "@@ -0,0 +1,2 @@\n+warning: thread support is experimental. For example, Miri does not detect data races yet.\n+"}]}