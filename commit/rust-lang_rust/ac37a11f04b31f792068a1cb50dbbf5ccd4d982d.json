{"sha": "ac37a11f04b31f792068a1cb50dbbf5ccd4d982d", "node_id": "MDY6Q29tbWl0NzI0NzEyOmFjMzdhMTFmMDRiMzFmNzkyMDY4YTFjYjUwZGJiZjVjY2Q0ZDk4MmQ=", "commit": {"author": {"name": "Veetaha", "email": "gerzoh1@gmail.com", "date": "2020-01-26T18:44:49Z"}, "committer": {"name": "Veetaha", "email": "gerzoh1@gmail.com", "date": "2020-02-03T22:00:55Z"}, "message": "Reimplemented lexer with vectors instead of iterators", "tree": {"sha": "52542f3e7b7ec9f4cfbedf2245c4fd4bb8cdffcb", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/52542f3e7b7ec9f4cfbedf2245c4fd4bb8cdffcb"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/ac37a11f04b31f792068a1cb50dbbf5ccd4d982d", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/ac37a11f04b31f792068a1cb50dbbf5ccd4d982d", "html_url": "https://github.com/rust-lang/rust/commit/ac37a11f04b31f792068a1cb50dbbf5ccd4d982d", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/ac37a11f04b31f792068a1cb50dbbf5ccd4d982d/comments", "author": null, "committer": null, "parents": [{"sha": "ad24976da38482948c586bdbc16004273662ff7e", "url": "https://api.github.com/repos/rust-lang/rust/commits/ad24976da38482948c586bdbc16004273662ff7e", "html_url": "https://github.com/rust-lang/rust/commit/ad24976da38482948c586bdbc16004273662ff7e"}], "stats": {"total": 446, "additions": 250, "deletions": 196}, "files": [{"sha": "ad3e86f7c0d86cad04014104bce905c5ea9b399c", "filename": "crates/ra_ide/src/references/rename.rs", "status": "modified", "additions": 4, "deletions": 6, "changes": 10, "blob_url": "https://github.com/rust-lang/rust/blob/ac37a11f04b31f792068a1cb50dbbf5ccd4d982d/crates%2Fra_ide%2Fsrc%2Freferences%2Frename.rs", "raw_url": "https://github.com/rust-lang/rust/raw/ac37a11f04b31f792068a1cb50dbbf5ccd4d982d/crates%2Fra_ide%2Fsrc%2Freferences%2Frename.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fra_ide%2Fsrc%2Freferences%2Frename.rs?ref=ac37a11f04b31f792068a1cb50dbbf5ccd4d982d", "patch": "@@ -2,7 +2,7 @@\n \n use hir::ModuleSource;\n use ra_db::{RelativePath, RelativePathBuf, SourceDatabase, SourceDatabaseExt};\n-use ra_syntax::{algo::find_node_at_offset, ast, tokenize, AstNode, SyntaxKind, SyntaxNode};\n+use ra_syntax::{algo::find_node_at_offset, ast, single_token, AstNode, SyntaxKind, SyntaxNode};\n use ra_text_edit::TextEdit;\n \n use crate::{\n@@ -17,11 +17,9 @@ pub(crate) fn rename(\n     position: FilePosition,\n     new_name: &str,\n ) -> Option<RangeInfo<SourceChange>> {\n-    let tokens = tokenize(new_name);\n-    if tokens.len() != 1\n-        || (tokens[0].kind != SyntaxKind::IDENT && tokens[0].kind != SyntaxKind::UNDERSCORE)\n-    {\n-        return None;\n+    match single_token(new_name)?.token.kind {\n+        SyntaxKind::IDENT | SyntaxKind::UNDERSCORE => (),\n+        _ => return None,\n     }\n \n     let parse = db.parse(position.file_id);"}, {"sha": "72ac8df030cb127e646b41cc6790d5e2622afe62", "filename": "crates/ra_mbe/src/subtree_source.rs", "status": "modified", "additions": 5, "deletions": 3, "changes": 8, "blob_url": "https://github.com/rust-lang/rust/blob/ac37a11f04b31f792068a1cb50dbbf5ccd4d982d/crates%2Fra_mbe%2Fsrc%2Fsubtree_source.rs", "raw_url": "https://github.com/rust-lang/rust/raw/ac37a11f04b31f792068a1cb50dbbf5ccd4d982d/crates%2Fra_mbe%2Fsrc%2Fsubtree_source.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fra_mbe%2Fsrc%2Fsubtree_source.rs?ref=ac37a11f04b31f792068a1cb50dbbf5ccd4d982d", "patch": "@@ -1,7 +1,7 @@\n //! FIXME: write short doc here\n \n use ra_parser::{Token, TokenSource};\n-use ra_syntax::{classify_literal, SmolStr, SyntaxKind, SyntaxKind::*, T};\n+use ra_syntax::{single_token, SmolStr, SyntaxKind, SyntaxKind::*, T};\n use std::cell::{Cell, Ref, RefCell};\n use tt::buffer::{Cursor, TokenBuffer};\n \n@@ -129,8 +129,10 @@ fn convert_delim(d: Option<tt::DelimiterKind>, closing: bool) -> TtToken {\n }\n \n fn convert_literal(l: &tt::Literal) -> TtToken {\n-    let kind =\n-        classify_literal(&l.text).map(|tkn| tkn.kind).unwrap_or_else(|| match l.text.as_ref() {\n+    let kind = single_token(&l.text)\n+        .map(|parsed| parsed.token.kind)\n+        .filter(|kind| kind.is_literal())\n+        .unwrap_or_else(|| match l.text.as_ref() {\n             \"true\" => T![true],\n             \"false\" => T![false],\n             _ => panic!(\"Fail to convert given literal {:#?}\", &l),"}, {"sha": "80b3a0b22a0b1572180471f1dc7947d97b1b57d1", "filename": "crates/ra_syntax/src/lib.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/ac37a11f04b31f792068a1cb50dbbf5ccd4d982d/crates%2Fra_syntax%2Fsrc%2Flib.rs", "raw_url": "https://github.com/rust-lang/rust/raw/ac37a11f04b31f792068a1cb50dbbf5ccd4d982d/crates%2Fra_syntax%2Fsrc%2Flib.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fra_syntax%2Fsrc%2Flib.rs?ref=ac37a11f04b31f792068a1cb50dbbf5ccd4d982d", "patch": "@@ -41,7 +41,7 @@ use crate::syntax_node::GreenNode;\n pub use crate::{\n     algo::InsertPosition,\n     ast::{AstNode, AstToken},\n-    parsing::{classify_literal, tokenize, Token},\n+    parsing::{first_token, single_token, tokenize, tokenize_append, Token, TokenizeError},\n     ptr::{AstPtr, SyntaxNodePtr},\n     syntax_error::{Location, SyntaxError, SyntaxErrorKind},\n     syntax_node::{"}, {"sha": "4e51f920b6c49e0bee1ce311970a18998f336b70", "filename": "crates/ra_syntax/src/parsing.rs", "status": "modified", "additions": 6, "deletions": 4, "changes": 10, "blob_url": "https://github.com/rust-lang/rust/blob/ac37a11f04b31f792068a1cb50dbbf5ccd4d982d/crates%2Fra_syntax%2Fsrc%2Fparsing.rs", "raw_url": "https://github.com/rust-lang/rust/raw/ac37a11f04b31f792068a1cb50dbbf5ccd4d982d/crates%2Fra_syntax%2Fsrc%2Fparsing.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fra_syntax%2Fsrc%2Fparsing.rs?ref=ac37a11f04b31f792068a1cb50dbbf5ccd4d982d", "patch": "@@ -7,15 +7,17 @@ mod text_tree_sink;\n mod reparsing;\n \n use crate::{syntax_node::GreenNode, SyntaxError};\n+use text_token_source::TextTokenSource;\n+use text_tree_sink::TextTreeSink;\n \n-pub use self::lexer::{classify_literal, tokenize, Token};\n+pub use lexer::*;\n \n pub(crate) use self::reparsing::incremental_reparse;\n \n pub(crate) fn parse_text(text: &str) -> (GreenNode, Vec<SyntaxError>) {\n-    let tokens = tokenize(&text);\n-    let mut token_source = text_token_source::TextTokenSource::new(text, &tokens);\n-    let mut tree_sink = text_tree_sink::TextTreeSink::new(text, &tokens);\n+    let ParsedTokens { tokens, errors } = tokenize(&text);\n+    let mut token_source = TextTokenSource::new(text, &tokens);\n+    let mut tree_sink = TextTreeSink::new(text, &tokens, errors);\n     ra_parser::parse(&mut token_source, &mut tree_sink);\n     tree_sink.finish()\n }"}, {"sha": "6d96f8400eef4b25bba0d9d3c3a56e8eedf8bd47", "filename": "crates/ra_syntax/src/parsing/lexer.rs", "status": "modified", "additions": 149, "deletions": 155, "changes": 304, "blob_url": "https://github.com/rust-lang/rust/blob/ac37a11f04b31f792068a1cb50dbbf5ccd4d982d/crates%2Fra_syntax%2Fsrc%2Fparsing%2Flexer.rs", "raw_url": "https://github.com/rust-lang/rust/raw/ac37a11f04b31f792068a1cb50dbbf5ccd4d982d/crates%2Fra_syntax%2Fsrc%2Fparsing%2Flexer.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fra_syntax%2Fsrc%2Fparsing%2Flexer.rs?ref=ac37a11f04b31f792068a1cb50dbbf5ccd4d982d", "patch": "@@ -1,10 +1,10 @@\n //! Lexer analyzes raw input string and produces lexemes (tokens).\n-\n-use std::iter::{FromIterator, IntoIterator};\n+//! It is just a bridge to `rustc_lexer`.\n \n use crate::{\n+    SyntaxError, SyntaxErrorKind,\n     SyntaxKind::{self, *},\n-    TextUnit,\n+    TextRange, TextUnit,\n };\n \n /// A token of Rust source.\n@@ -15,93 +15,96 @@ pub struct Token {\n     /// The length of the token.\n     pub len: TextUnit,\n }\n-impl Token {\n-    pub const fn new(kind: SyntaxKind, len: TextUnit) -> Self {\n-        Self { kind, len }\n-    }\n-}\n \n #[derive(Debug)]\n-/// Represents the result of parsing one token.\n+/// Represents the result of parsing one token. Beware that the token may be malformed.\n pub struct ParsedToken {\n     /// Parsed token.\n     pub token: Token,\n     /// If error is present then parsed token is malformed.\n-    pub error: Option<TokenizeError>,\n-}\n-impl ParsedToken {\n-    pub const fn new(token: Token, error: Option<TokenizeError>) -> Self {\n-        Self { token, error }\n-    }\n+    pub error: Option<SyntaxError>,\n }\n \n #[derive(Debug, Default)]\n-/// Represents the result of parsing one token.\n+/// Represents the result of parsing source code of Rust language.\n pub struct ParsedTokens {\n-    /// Parsed token.\n+    /// Parsed tokens in order they appear in source code.\n     pub tokens: Vec<Token>,\n-    /// If error is present then parsed token is malformed.\n-    pub errors: Vec<TokenizeError>,\n+    /// Collection of all occured tokenization errors.\n+    /// In general `self.errors.len() <= self.tokens.len()`\n+    pub errors: Vec<SyntaxError>,\n }\n-\n-impl FromIterator<ParsedToken> for ParsedTokens {\n-    fn from_iter<I: IntoIterator<Item = ParsedToken>>(iter: I) -> Self {\n-        let res = Self::default();\n-        for entry in iter {\n-            res.tokens.push(entry.token);\n-            if let Some(error) = entry.error {\n-                res.errors.push(error);\n-            }\n+impl ParsedTokens {\n+    /// Append `token` and `error` (if pressent) to the result.\n+    pub fn push(&mut self, ParsedToken { token, error }: ParsedToken) {\n+        self.tokens.push(token);\n+        if let Some(error) = error {\n+            self.errors.push(error)\n         }\n-        res\n     }\n }\n \n-/// Returns the first encountered token from the string.\n-/// If the string contains zero or two or more tokens returns `None`.\n-pub fn single_token(text: &str) -> Option<ParsedToken> {\n-    // TODO: test whether this condition indeed checks for a single token\n-    first_token(text).filter(|parsed| parsed.token.len.to_usize() == text.len())\n+/// Same as `tokenize_append()`, just a shortcut for creating `ParsedTokens`\n+/// and returning the result the usual way.\n+pub fn tokenize(text: &str) -> ParsedTokens {\n+    let mut parsed = ParsedTokens::default();\n+    tokenize_append(text, &mut parsed);\n+    parsed\n }\n \n-/*\n-/// Returns `ParsedTokens` which are basically a pair `(Vec<Token>, Vec<TokenizeError>)`\n-/// This is just a shorthand for `tokenize(text).collect()`\n-pub fn tokenize_to_vec_with_errors(text: &str) -> ParsedTokens {\n-    tokenize(text).collect()\n-}\n+/// Break a string up into its component tokens.\n+/// Returns `ParsedTokens` which are basically a pair `(Vec<Token>, Vec<SyntaxError>)`.\n+/// Beware that it checks for shebang first and its length contributes to resulting\n+/// tokens offsets.\n+pub fn tokenize_append(text: &str, parsed: &mut ParsedTokens) {\n+    // non-empty string is a precondtion of `rustc_lexer::strip_shebang()`.\n+    if text.is_empty() {\n+        return;\n+    }\n \n-/// The simplest version of tokenize, it just retunst a ready-made `Vec<Token>`.\n-/// It discards all tokenization errors while parsing. If you need that infromation\n-/// consider using `tokenize()` or `tokenize_to_vec_with_errors()`.\n-pub fn tokenize_to_vec(text: &str) -> Vec<Token> {\n-    tokenize(text).map(|parsed_token| parsed_token.token).collect()\n-}\n-*/\n+    let mut offset: usize = rustc_lexer::strip_shebang(text)\n+        .map(|shebang_len| {\n+            parsed.tokens.push(Token { kind: SHEBANG, len: TextUnit::from_usize(shebang_len) });\n+            shebang_len\n+        })\n+        .unwrap_or(0);\n \n-/// Break a string up into its component tokens\n-/// This is the core function, all other `tokenize*()` functions are simply\n-/// handy shortcuts for this one.\n-pub fn tokenize(text: &str) -> impl Iterator<Item = ParsedToken> + '_ {\n-    let shebang = rustc_lexer::strip_shebang(text).map(|shebang_len| {\n-        text = &text[shebang_len..];\n-        ParsedToken::new(Token::new(SHEBANG, TextUnit::from_usize(shebang_len)), None)\n-    });\n+    let text_without_shebang = &text[offset..];\n \n-    // Notice that we eagerly evaluate shebang since it may change text slice\n-    // and we cannot simplify this into a single method call chain\n-    shebang.into_iter().chain(tokenize_without_shebang(text))\n+    for rustc_token in rustc_lexer::tokenize(text_without_shebang) {\n+        parsed.push(rustc_token_to_parsed_token(&rustc_token, text, TextUnit::from_usize(offset)));\n+        offset += rustc_token.len;\n+    }\n }\n \n-pub fn tokenize_without_shebang(text: &str) -> impl Iterator<Item = ParsedToken> + '_ {\n-    rustc_lexer::tokenize(text).map(|rustc_token| {\n-        let token_text = &text[..rustc_token.len];\n-        text = &text[rustc_token.len..];\n-        rustc_token_kind_to_parsed_token(&rustc_token.kind, token_text)\n-    })\n+/// Returns the first encountered token at the beginning of the string.\n+/// If the string contains zero or *two or more tokens* returns `None`.\n+///\n+/// The main difference between `first_token()` and `single_token()` is that\n+/// the latter returns `None` if the string contains more than one token.\n+pub fn single_token(text: &str) -> Option<ParsedToken> {\n+    first_token(text).filter(|parsed| parsed.token.len.to_usize() == text.len())\n }\n \n-#[derive(Debug)]\n+/// Returns the first encountered token at the beginning of the string.\n+/// If the string contains zero tokens returns `None`.\n+///\n+/// The main difference between `first_token() and single_token()` is that\n+/// the latter returns `None` if the string contains more than one token.\n+pub fn first_token(text: &str) -> Option<ParsedToken> {\n+    // non-empty string is a precondtion of `rustc_lexer::first_token()`.\n+    if text.is_empty() {\n+        None\n+    } else {\n+        let rustc_token = rustc_lexer::first_token(text);\n+        Some(rustc_token_to_parsed_token(&rustc_token, text, TextUnit::from(0)))\n+    }\n+}\n+\n+/// Describes the values of `SyntaxErrorKind::TokenizeError` enum variant.\n+/// It describes all the types of errors that may happen during the tokenization\n+/// of Rust source.\n+#[derive(Debug, Clone, PartialEq, Eq, Hash)]\n pub enum TokenizeError {\n     /// Base prefix was provided, but there were no digits\n     /// after it, e.g. `0x`.\n@@ -124,94 +127,95 @@ pub enum TokenizeError {\n     /// Raw byte string literal lacks trailing delimiter e.g. `\"##`\n     UnterminatedRawByteString,\n \n-    /// Raw string lacks a quote after pound characters e.g. `r###`\n+    /// Raw string lacks a quote after the pound characters e.g. `r###`\n     UnstartedRawString,\n-    /// Raw byte string lacks a quote after pound characters e.g. `br###`\n+    /// Raw byte string lacks a quote after the pound characters e.g. `br###`\n     UnstartedRawByteString,\n \n     /// Lifetime starts with a number e.g. `'4ever`\n     LifetimeStartsWithNumber,\n }\n \n-fn rustc_token_kind_to_parsed_token(\n-    rustc_token_kind: &rustc_lexer::TokenKind,\n-    token_text: &str,\n+/// Mapper function that converts `rustc_lexer::Token` with some additional context\n+/// to `ParsedToken`\n+fn rustc_token_to_parsed_token(\n+    rustc_token: &rustc_lexer::Token,\n+    text: &str,\n+    token_start_offset: TextUnit,\n ) -> ParsedToken {\n-    use rustc_lexer::TokenKind as TK;\n-    use TokenizeError as TE;\n-\n     // We drop some useful infromation here (see patterns with double dots `..`)\n     // Storing that info in `SyntaxKind` is not possible due to its layout requirements of\n     // being `u16` that come from `rowan::SyntaxKind` type and changes to `rowan::SyntaxKind`\n-    // would mean hell of a rewrite.\n+    // would mean hell of a rewrite\n \n-    let (syntax_kind, error) = match *rustc_token_kind {\n-        TK::LineComment => ok(COMMENT),\n-        TK::BlockComment { terminated } => ok_if(terminated, COMMENT, TE::UnterminatedBlockComment),\n-        TK::Whitespace => ok(WHITESPACE),\n-        TK::Ident => ok(if token_text == \"_\" {\n-            UNDERSCORE\n-        } else {\n-            SyntaxKind::from_keyword(token_text).unwrap_or(IDENT)\n-        }),\n-        TK::RawIdent => ok(IDENT),\n-        TK::Literal { kind, .. } => match_literal_kind(&kind),\n-        TK::Lifetime { starts_with_number } => {\n-            ok_if(!starts_with_number, LIFETIME, TE::LifetimeStartsWithNumber)\n+    let token_range =\n+        TextRange::offset_len(token_start_offset, TextUnit::from_usize(rustc_token.len));\n+\n+    let token_text = &text[token_range];\n+\n+    let (syntax_kind, error) = {\n+        use rustc_lexer::TokenKind as TK;\n+        use TokenizeError as TE;\n+\n+        match rustc_token.kind {\n+            TK::LineComment => ok(COMMENT),\n+            TK::BlockComment { terminated } => {\n+                ok_if(terminated, COMMENT, TE::UnterminatedBlockComment)\n+            }\n+            TK::Whitespace => ok(WHITESPACE),\n+            TK::Ident => ok(if token_text == \"_\" {\n+                UNDERSCORE\n+            } else {\n+                SyntaxKind::from_keyword(token_text).unwrap_or(IDENT)\n+            }),\n+            TK::RawIdent => ok(IDENT),\n+            TK::Literal { kind, .. } => match_literal_kind(&kind),\n+            TK::Lifetime { starts_with_number } => {\n+                ok_if(!starts_with_number, LIFETIME, TE::LifetimeStartsWithNumber)\n+            }\n+            TK::Semi => ok(SEMI),\n+            TK::Comma => ok(COMMA),\n+            TK::Dot => ok(DOT),\n+            TK::OpenParen => ok(L_PAREN),\n+            TK::CloseParen => ok(R_PAREN),\n+            TK::OpenBrace => ok(L_CURLY),\n+            TK::CloseBrace => ok(R_CURLY),\n+            TK::OpenBracket => ok(L_BRACK),\n+            TK::CloseBracket => ok(R_BRACK),\n+            TK::At => ok(AT),\n+            TK::Pound => ok(POUND),\n+            TK::Tilde => ok(TILDE),\n+            TK::Question => ok(QUESTION),\n+            TK::Colon => ok(COLON),\n+            TK::Dollar => ok(DOLLAR),\n+            TK::Eq => ok(EQ),\n+            TK::Not => ok(EXCL),\n+            TK::Lt => ok(L_ANGLE),\n+            TK::Gt => ok(R_ANGLE),\n+            TK::Minus => ok(MINUS),\n+            TK::And => ok(AMP),\n+            TK::Or => ok(PIPE),\n+            TK::Plus => ok(PLUS),\n+            TK::Star => ok(STAR),\n+            TK::Slash => ok(SLASH),\n+            TK::Caret => ok(CARET),\n+            TK::Percent => ok(PERCENT),\n+            TK::Unknown => ok(ERROR),\n         }\n-        TK::Semi => ok(SEMI),\n-        TK::Comma => ok(COMMA),\n-        TK::Dot => ok(DOT),\n-        TK::OpenParen => ok(L_PAREN),\n-        TK::CloseParen => ok(R_PAREN),\n-        TK::OpenBrace => ok(L_CURLY),\n-        TK::CloseBrace => ok(R_CURLY),\n-        TK::OpenBracket => ok(L_BRACK),\n-        TK::CloseBracket => ok(R_BRACK),\n-        TK::At => ok(AT),\n-        TK::Pound => ok(POUND),\n-        TK::Tilde => ok(TILDE),\n-        TK::Question => ok(QUESTION),\n-        TK::Colon => ok(COLON),\n-        TK::Dollar => ok(DOLLAR),\n-        TK::Eq => ok(EQ),\n-        TK::Not => ok(EXCL),\n-        TK::Lt => ok(L_ANGLE),\n-        TK::Gt => ok(R_ANGLE),\n-        TK::Minus => ok(MINUS),\n-        TK::And => ok(AMP),\n-        TK::Or => ok(PIPE),\n-        TK::Plus => ok(PLUS),\n-        TK::Star => ok(STAR),\n-        TK::Slash => ok(SLASH),\n-        TK::Caret => ok(CARET),\n-        TK::Percent => ok(PERCENT),\n-        TK::Unknown => ok(ERROR),\n     };\n \n-    return ParsedToken::new(\n-        Token::new(syntax_kind, TextUnit::from_usize(token_text.len())),\n-        error,\n-    );\n+    return ParsedToken {\n+        token: Token { kind: syntax_kind, len: token_range.len() },\n+        error: error\n+            .map(|error| SyntaxError::new(SyntaxErrorKind::TokenizeError(error), token_range)),\n+    };\n \n     type ParsedSyntaxKind = (SyntaxKind, Option<TokenizeError>);\n \n-    const fn ok(syntax_kind: SyntaxKind) -> ParsedSyntaxKind {\n-        (syntax_kind, None)\n-    }\n-    const fn ok_if(cond: bool, syntax_kind: SyntaxKind, error: TokenizeError) -> ParsedSyntaxKind {\n-        if cond {\n-            ok(syntax_kind)\n-        } else {\n-            err(syntax_kind, error)\n-        }\n-    }\n-    const fn err(syntax_kind: SyntaxKind, error: TokenizeError) -> ParsedSyntaxKind {\n-        (syntax_kind, Some(error))\n-    }\n-\n-    const fn match_literal_kind(kind: &rustc_lexer::LiteralKind) -> ParsedSyntaxKind {\n+    fn match_literal_kind(kind: &rustc_lexer::LiteralKind) -> ParsedSyntaxKind {\n         use rustc_lexer::LiteralKind as LK;\n+        use TokenizeError as TE;\n+\n         match *kind {\n             LK::Int { empty_int, .. } => ok_if(!empty_int, INT_NUMBER, TE::EmptyInt),\n             LK::Float { empty_exponent, .. } => {\n@@ -237,27 +241,17 @@ fn rustc_token_kind_to_parsed_token(\n             }\n         }\n     }\n-}\n-\n-pub fn first_token(text: &str) -> Option<ParsedToken> {\n-    // Checking for emptyness because of `rustc_lexer::first_token()` invariant (see its body)\n-    if text.is_empty() {\n-        None\n-    } else {\n-        let rustc_token = rustc_lexer::first_token(text);\n-        Some(rustc_token_kind_to_parsed_token(&rustc_token.kind, &text[..rustc_token.len]))\n+    const fn ok(syntax_kind: SyntaxKind) -> ParsedSyntaxKind {\n+        (syntax_kind, None)\n     }\n-}\n-\n-// TODO: think what to do with this ad hoc function\n-pub fn classify_literal(text: &str) -> Option<ParsedToken> {\n-    let t = rustc_lexer::first_token(text);\n-    if t.len != text.len() {\n-        return None;\n+    const fn err(syntax_kind: SyntaxKind, error: TokenizeError) -> ParsedSyntaxKind {\n+        (syntax_kind, Some(error))\n+    }\n+    fn ok_if(cond: bool, syntax_kind: SyntaxKind, error: TokenizeError) -> ParsedSyntaxKind {\n+        if cond {\n+            ok(syntax_kind)\n+        } else {\n+            err(syntax_kind, error)\n+        }\n     }\n-    let kind = match t.kind {\n-        rustc_lexer::TokenKind::Literal { kind, .. } => match_literal_kind(kind),\n-        _ => return None,\n-    };\n-    Some(ParsedToken::new(Token::new(kind, TextUnit::from_usize(t.len))))\n }"}, {"sha": "ad1a7c855faa428e7af727c07a42cf3889a110ed", "filename": "crates/ra_syntax/src/parsing/reparsing.rs", "status": "modified", "additions": 33, "deletions": 19, "changes": 52, "blob_url": "https://github.com/rust-lang/rust/blob/ac37a11f04b31f792068a1cb50dbbf5ccd4d982d/crates%2Fra_syntax%2Fsrc%2Fparsing%2Freparsing.rs", "raw_url": "https://github.com/rust-lang/rust/raw/ac37a11f04b31f792068a1cb50dbbf5ccd4d982d/crates%2Fra_syntax%2Fsrc%2Fparsing%2Freparsing.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fra_syntax%2Fsrc%2Fparsing%2Freparsing.rs?ref=ac37a11f04b31f792068a1cb50dbbf5ccd4d982d", "patch": "@@ -12,7 +12,7 @@ use ra_text_edit::AtomTextEdit;\n use crate::{\n     algo,\n     parsing::{\n-        lexer::{tokenize, Token},\n+        lexer::{single_token, tokenize, ParsedTokens, Token},\n         text_token_source::TextTokenSource,\n         text_tree_sink::TextTreeSink,\n     },\n@@ -41,36 +41,42 @@ fn reparse_token<'node>(\n     root: &'node SyntaxNode,\n     edit: &AtomTextEdit,\n ) -> Option<(GreenNode, TextRange)> {\n-    let token = algo::find_covering_element(root, edit.delete).as_token()?.clone();\n-    match token.kind() {\n+    let prev_token = algo::find_covering_element(root, edit.delete).as_token()?.clone();\n+    let prev_token_kind = prev_token.kind();\n+    match prev_token_kind {\n         WHITESPACE | COMMENT | IDENT | STRING | RAW_STRING => {\n-            if token.kind() == WHITESPACE || token.kind() == COMMENT {\n+            if prev_token_kind == WHITESPACE || prev_token_kind == COMMENT {\n                 // removing a new line may extends previous token\n-                if token.text()[edit.delete - token.text_range().start()].contains('\\n') {\n+                let deleted_range = edit.delete - prev_token.text_range().start();\n+                if prev_token.text()[deleted_range].contains('\\n') {\n                     return None;\n                 }\n             }\n \n-            let text = get_text_after_edit(token.clone().into(), &edit);\n-            let lex_tokens = tokenize(&text);\n-            let lex_token = match lex_tokens[..] {\n-                [lex_token] if lex_token.kind == token.kind() => lex_token,\n-                _ => return None,\n-            };\n+            let mut new_text = get_text_after_edit(prev_token.clone().into(), &edit);\n+            let new_token_kind = single_token(&new_text)?.token.kind;\n \n-            if lex_token.kind == IDENT && is_contextual_kw(&text) {\n+            if new_token_kind != prev_token_kind\n+                || (new_token_kind == IDENT && is_contextual_kw(&new_text))\n+            {\n                 return None;\n             }\n \n-            if let Some(next_char) = root.text().char_at(token.text_range().end()) {\n-                let tokens_with_next_char = tokenize(&format!(\"{}{}\", text, next_char));\n-                if tokens_with_next_char.len() == 1 {\n+            // Check that edited token is not a part of the bigger token.\n+            // E.g. if for source code `bruh\"str\"` the user removed `ruh`, then\n+            // `b` no longer remains an identifier, but becomes a part of byte string literal\n+            if let Some(next_char) = root.text().char_at(prev_token.text_range().end()) {\n+                new_text.push(next_char);\n+                let token_with_next_char = single_token(&new_text);\n+                if token_with_next_char.is_some() {\n                     return None;\n                 }\n+                new_text.pop();\n             }\n \n-            let new_token = GreenToken::new(rowan::SyntaxKind(token.kind().into()), text.into());\n-            Some((token.replace_with(new_token), token.text_range()))\n+            let new_token =\n+                GreenToken::new(rowan::SyntaxKind(prev_token_kind.into()), new_text.into());\n+            Some((prev_token.replace_with(new_token), prev_token.text_range()))\n         }\n         _ => None,\n     }\n@@ -82,12 +88,12 @@ fn reparse_block<'node>(\n ) -> Option<(GreenNode, Vec<SyntaxError>, TextRange)> {\n     let (node, reparser) = find_reparsable_node(root, edit.delete)?;\n     let text = get_text_after_edit(node.clone().into(), &edit);\n-    let tokens = tokenize(&text);\n+    let ParsedTokens { tokens, errors } = tokenize(&text);\n     if !is_balanced(&tokens) {\n         return None;\n     }\n     let mut token_source = TextTokenSource::new(&text, &tokens);\n-    let mut tree_sink = TextTreeSink::new(&text, &tokens);\n+    let mut tree_sink = TextTreeSink::new(&text, &tokens, errors);\n     reparser.parse(&mut token_source, &mut tree_sink);\n     let (green, new_errors) = tree_sink.finish();\n     Some((node.replace_with(green), new_errors, node.text_range()))\n@@ -96,6 +102,9 @@ fn reparse_block<'node>(\n fn get_text_after_edit(element: SyntaxElement, edit: &AtomTextEdit) -> String {\n     let edit =\n         AtomTextEdit::replace(edit.delete - element.text_range().start(), edit.insert.clone());\n+\n+    // Note: we could move this match to a method or even further: use enum_dispatch crate\n+    // https://crates.io/crates/enum_dispatch\n     let text = match element {\n         NodeOrToken::Token(token) => token.text().to_string(),\n         NodeOrToken::Node(node) => node.text().to_string(),\n@@ -112,6 +121,9 @@ fn is_contextual_kw(text: &str) -> bool {\n \n fn find_reparsable_node(node: &SyntaxNode, range: TextRange) -> Option<(SyntaxNode, Reparser)> {\n     let node = algo::find_covering_element(node, range);\n+\n+    // Note: we could move this match to a method or even further: use enum_dispatch crate\n+    // https://crates.io/crates/enum_dispatch\n     let mut ancestors = match node {\n         NodeOrToken::Token(it) => it.parent().ancestors(),\n         NodeOrToken::Node(it) => it.ancestors(),\n@@ -181,6 +193,8 @@ mod tests {\n         let fully_reparsed = SourceFile::parse(&after);\n         let incrementally_reparsed: Parse<SourceFile> = {\n             let f = SourceFile::parse(&before);\n+            // FIXME: it seems this initialization statement is unnecessary (see edit in outer scope)\n+            // Investigate whether it should really be removed.\n             let edit = AtomTextEdit { delete: range, insert: replace_with.to_string() };\n             let (green, new_errors, range) =\n                 incremental_reparse(f.tree().syntax(), &edit, f.errors.to_vec()).unwrap();"}, {"sha": "5faac588b2e38402cd6893ca769d0aafc82d2b74", "filename": "crates/ra_syntax/src/parsing/text_tree_sink.rs", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "blob_url": "https://github.com/rust-lang/rust/blob/ac37a11f04b31f792068a1cb50dbbf5ccd4d982d/crates%2Fra_syntax%2Fsrc%2Fparsing%2Ftext_tree_sink.rs", "raw_url": "https://github.com/rust-lang/rust/raw/ac37a11f04b31f792068a1cb50dbbf5ccd4d982d/crates%2Fra_syntax%2Fsrc%2Fparsing%2Ftext_tree_sink.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fra_syntax%2Fsrc%2Fparsing%2Ftext_tree_sink.rs?ref=ac37a11f04b31f792068a1cb50dbbf5ccd4d982d", "patch": "@@ -92,14 +92,14 @@ impl<'a> TreeSink for TextTreeSink<'a> {\n }\n \n impl<'a> TextTreeSink<'a> {\n-    pub(super) fn new(text: &'a str, tokens: &'a [Token]) -> TextTreeSink<'a> {\n-        TextTreeSink {\n+    pub(super) fn new(text: &'a str, tokens: &'a [Token], errors: Vec<SyntaxError>) -> Self {\n+        Self {\n             text,\n             tokens,\n             text_pos: 0.into(),\n             token_pos: 0,\n             state: State::PendingStart,\n-            inner: SyntaxTreeBuilder::default(),\n+            inner: SyntaxTreeBuilder::new(errors),\n         }\n     }\n "}, {"sha": "af18a30f2295da0347813c02727e1f631df82a6c", "filename": "crates/ra_syntax/src/syntax_error.rs", "status": "modified", "additions": 42, "deletions": 0, "changes": 42, "blob_url": "https://github.com/rust-lang/rust/blob/ac37a11f04b31f792068a1cb50dbbf5ccd4d982d/crates%2Fra_syntax%2Fsrc%2Fsyntax_error.rs", "raw_url": "https://github.com/rust-lang/rust/raw/ac37a11f04b31f792068a1cb50dbbf5ccd4d982d/crates%2Fra_syntax%2Fsrc%2Fsyntax_error.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fra_syntax%2Fsrc%2Fsyntax_error.rs?ref=ac37a11f04b31f792068a1cb50dbbf5ccd4d982d", "patch": "@@ -84,6 +84,9 @@ pub enum SyntaxErrorKind {\n     ParseError(ParseError),\n     EscapeError(EscapeError),\n     TokenizeError(TokenizeError),\n+    // FIXME: the obvious pattern of this enum dictates that the following enum variants\n+    // should be wrapped into something like `SemmanticError(SemmanticError)`\n+    // or `ValidateError(ValidateError)` or `SemmanticValidateError(...)`\n     InvalidBlockAttr,\n     InvalidMatchInnerAttr,\n     InvalidTupleIndexFormat,\n@@ -106,6 +109,7 @@ impl fmt::Display for SyntaxErrorKind {\n             }\n             ParseError(msg) => write!(f, \"{}\", msg.0),\n             EscapeError(err) => write!(f, \"{}\", err),\n+            TokenizeError(err) => write!(f, \"{}\", err),\n             VisibilityNotAllowed => {\n                 write!(f, \"unnecessary visibility qualifier\")\n             }\n@@ -116,6 +120,44 @@ impl fmt::Display for SyntaxErrorKind {\n     }\n }\n \n+impl fmt::Display for TokenizeError {\n+    fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {\n+        let msg = match self {\n+            TokenizeError::EmptyInt => \"Missing digits after integer base prefix\",\n+            TokenizeError::EmptyExponent => \"Missing digits after the exponent symbol\",\n+            TokenizeError::UnterminatedBlockComment => {\n+                \"Missing trailing `*/` symbols to terminate the block comment\"\n+            }\n+            TokenizeError::UnterminatedChar => {\n+                \"Missing trailing `'` symbol to terminate the character literal\"\n+            }\n+            TokenizeError::UnterminatedByte => {\n+                \"Missing trailing `'` symbol to terminate the byte literal\"\n+            }\n+            TokenizeError::UnterminatedString => {\n+                \"Missing trailing `\\\"` symbol to terminate the string literal\"\n+            }\n+            TokenizeError::UnterminatedByteString => {\n+                \"Missing trailing `\\\"` symbol to terminate the byte string literal\"\n+            }\n+            TokenizeError::UnterminatedRawString => {\n+                \"Missing trailing `\\\"` with `#` symbols to terminate the raw string literal\"\n+            }\n+            TokenizeError::UnterminatedRawByteString => {\n+                \"Missing trailing `\\\"` with `#` symbols to terminate the raw byte string literal\"\n+            }\n+            TokenizeError::UnstartedRawString => {\n+                \"Missing `\\\"` symbol after `#` symbols to begin the raw string literal\"\n+            }\n+            TokenizeError::UnstartedRawByteString => {\n+                \"Missing `\\\"` symbol after `#` symbols to begin the raw byte string literal\"\n+            }\n+            TokenizeError::LifetimeStartsWithNumber => \"Lifetime name cannot start with a number\",\n+        };\n+        write!(f, \"{}\", msg)\n+    }\n+}\n+\n impl fmt::Display for EscapeError {\n     fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {\n         let msg = match self {"}, {"sha": "591855302fa039884566ce42674c6f95703d50a1", "filename": "crates/ra_syntax/src/syntax_node.rs", "status": "modified", "additions": 5, "deletions": 4, "changes": 9, "blob_url": "https://github.com/rust-lang/rust/blob/ac37a11f04b31f792068a1cb50dbbf5ccd4d982d/crates%2Fra_syntax%2Fsrc%2Fsyntax_node.rs", "raw_url": "https://github.com/rust-lang/rust/raw/ac37a11f04b31f792068a1cb50dbbf5ccd4d982d/crates%2Fra_syntax%2Fsrc%2Fsyntax_node.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fra_syntax%2Fsrc%2Fsyntax_node.rs?ref=ac37a11f04b31f792068a1cb50dbbf5ccd4d982d", "patch": "@@ -4,7 +4,7 @@\n //! `SyntaxNode`, and a basic traversal API (parent, children, siblings).\n //!\n //! The *real* implementation is in the (language-agnostic) `rowan` crate, this\n-//! modules just wraps its API.\n+//! module just wraps its API.\n \n use ra_parser::ParseError;\n use rowan::{GreenNodeBuilder, Language};\n@@ -38,14 +38,15 @@ pub type SyntaxElementChildren = rowan::SyntaxElementChildren<RustLanguage>;\n \n pub use rowan::{Direction, NodeOrToken};\n \n+#[derive(Default)]\n pub struct SyntaxTreeBuilder {\n     errors: Vec<SyntaxError>,\n     inner: GreenNodeBuilder<'static>,\n }\n \n-impl Default for SyntaxTreeBuilder {\n-    fn default() -> SyntaxTreeBuilder {\n-        SyntaxTreeBuilder { errors: Vec::new(), inner: GreenNodeBuilder::new() }\n+impl SyntaxTreeBuilder {\n+    pub fn new(errors: Vec<SyntaxError>) -> Self {\n+        Self { errors, inner: GreenNodeBuilder::default() }\n     }\n }\n "}, {"sha": "df21c957c884906b7f0263bd5c44663b2ee93973", "filename": "crates/ra_syntax/src/tests.rs", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "blob_url": "https://github.com/rust-lang/rust/blob/ac37a11f04b31f792068a1cb50dbbf5ccd4d982d/crates%2Fra_syntax%2Fsrc%2Ftests.rs", "raw_url": "https://github.com/rust-lang/rust/raw/ac37a11f04b31f792068a1cb50dbbf5ccd4d982d/crates%2Fra_syntax%2Fsrc%2Ftests.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fra_syntax%2Fsrc%2Ftests.rs?ref=ac37a11f04b31f792068a1cb50dbbf5ccd4d982d", "patch": "@@ -10,7 +10,8 @@ use crate::{fuzz, SourceFile};\n #[test]\n fn lexer_tests() {\n     dir_tests(&test_data_dir(), &[\"lexer\"], |text, _| {\n-        let tokens = crate::tokenize(text);\n+        // FIXME: add tests for errors (their format is up to discussion)\n+        let tokens = crate::tokenize(text).tokens;\n         dump_tokens(&tokens, text)\n     })\n }"}]}