{"sha": "d77c4b0fa63d420b2f995e37ee07402f9a243361", "node_id": "MDY6Q29tbWl0NzI0NzEyOmQ3N2M0YjBmYTYzZDQyMGIyZjk5NWUzN2VlMDc0MDJmOWEyNDMzNjE=", "commit": {"author": {"name": "Aaron Turon", "email": "aturon@mozilla.com", "date": "2015-06-26T21:32:34Z"}, "committer": {"name": "Aaron Turon", "email": "aturon@mozilla.com", "date": "2015-07-02T20:58:38Z"}, "message": "Fix race condition in Arc's get_mut and make_unqiue\n\nThis commit resolves the race condition in the `get_mut` and\n`make_unique` functions, which arose through interaction with weak\npointers. The basic strategy is to \"lock\" the weak pointer count when\ntrying to establish uniqueness, by reusing the field as a simple\nspinlock. The overhead for normal use of `Arc` is expected to be minimal\n-- it will be *none* when only strong pointers are used, and only\nrequires a move from atomic increment to CAS for usage of weak pointers.\n\nThe commit also removes the `unsafe` and deprecated status of these\nfunctions.\n\nAlong the way, the commit also improves several memory orderings, and\nadds commentary about why various orderings suffice.", "tree": {"sha": "a437a534c5dee700cfbf834cfa80a2153c2d143d", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/a437a534c5dee700cfbf834cfa80a2153c2d143d"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/d77c4b0fa63d420b2f995e37ee07402f9a243361", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/d77c4b0fa63d420b2f995e37ee07402f9a243361", "html_url": "https://github.com/rust-lang/rust/commit/d77c4b0fa63d420b2f995e37ee07402f9a243361", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/d77c4b0fa63d420b2f995e37ee07402f9a243361/comments", "author": {"login": "aturon", "id": 709807, "node_id": "MDQ6VXNlcjcwOTgwNw==", "avatar_url": "https://avatars.githubusercontent.com/u/709807?v=4", "gravatar_id": "", "url": "https://api.github.com/users/aturon", "html_url": "https://github.com/aturon", "followers_url": "https://api.github.com/users/aturon/followers", "following_url": "https://api.github.com/users/aturon/following{/other_user}", "gists_url": "https://api.github.com/users/aturon/gists{/gist_id}", "starred_url": "https://api.github.com/users/aturon/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/aturon/subscriptions", "organizations_url": "https://api.github.com/users/aturon/orgs", "repos_url": "https://api.github.com/users/aturon/repos", "events_url": "https://api.github.com/users/aturon/events{/privacy}", "received_events_url": "https://api.github.com/users/aturon/received_events", "type": "User", "site_admin": false}, "committer": {"login": "aturon", "id": 709807, "node_id": "MDQ6VXNlcjcwOTgwNw==", "avatar_url": "https://avatars.githubusercontent.com/u/709807?v=4", "gravatar_id": "", "url": "https://api.github.com/users/aturon", "html_url": "https://github.com/aturon", "followers_url": "https://api.github.com/users/aturon/followers", "following_url": "https://api.github.com/users/aturon/following{/other_user}", "gists_url": "https://api.github.com/users/aturon/gists{/gist_id}", "starred_url": "https://api.github.com/users/aturon/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/aturon/subscriptions", "organizations_url": "https://api.github.com/users/aturon/orgs", "repos_url": "https://api.github.com/users/aturon/repos", "events_url": "https://api.github.com/users/aturon/events{/privacy}", "received_events_url": "https://api.github.com/users/aturon/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "2cb8a31e7cea1ee499630c9c6744dedd0c0b371f", "url": "https://api.github.com/repos/rust-lang/rust/commits/2cb8a31e7cea1ee499630c9c6744dedd0c0b371f", "html_url": "https://github.com/rust-lang/rust/commit/2cb8a31e7cea1ee499630c9c6744dedd0c0b371f"}], "stats": {"total": 256, "additions": 177, "deletions": 79}, "files": [{"sha": "e1bee633e13bbd18ee88c3ebc937eb1c99ea7e04", "filename": "src/liballoc/arc.rs", "status": "modified", "additions": 177, "deletions": 79, "changes": 256, "blob_url": "https://github.com/rust-lang/rust/blob/d77c4b0fa63d420b2f995e37ee07402f9a243361/src%2Fliballoc%2Farc.rs", "raw_url": "https://github.com/rust-lang/rust/raw/d77c4b0fa63d420b2f995e37ee07402f9a243361/src%2Fliballoc%2Farc.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Fliballoc%2Farc.rs?ref=d77c4b0fa63d420b2f995e37ee07402f9a243361", "patch": "@@ -82,8 +82,10 @@ use core::intrinsics::drop_in_place;\n use core::mem;\n use core::nonzero::NonZero;\n use core::ops::{Deref, CoerceUnsized};\n+use core::ptr;\n use core::marker::Unsize;\n use core::hash::{Hash, Hasher};\n+use core::usize;\n use heap::deallocate;\n \n /// An atomically reference counted wrapper for shared state.\n@@ -154,7 +156,12 @@ impl<T: ?Sized + fmt::Debug> fmt::Debug for Weak<T> {\n \n struct ArcInner<T: ?Sized> {\n     strong: atomic::AtomicUsize,\n+\n+    // the value usize::MAX acts as a sentinel for temporarily \"locking\" the\n+    // ability to upgrade weak pointers or downgrade strong ones; this is used\n+    // to avoid races in `make_unique` and `get_mut`.\n     weak: atomic::AtomicUsize,\n+\n     data: T,\n }\n \n@@ -201,9 +208,25 @@ impl<T: ?Sized> Arc<T> {\n     #[unstable(feature = \"arc_weak\",\n                reason = \"Weak pointers may not belong in this module.\")]\n     pub fn downgrade(&self) -> Weak<T> {\n-        // See the clone() impl for why this is relaxed\n-        self.inner().weak.fetch_add(1, Relaxed);\n-        Weak { _ptr: self._ptr }\n+        loop {\n+            // This Relaaxed is OK because we're checking the value in the CAS\n+            // below.\n+            let cur = self.inner().weak.load(Relaxed);\n+\n+            // check if the weak counter is currently \"locked\"; if so, spin.\n+            if cur == usize::MAX { continue }\n+\n+            // NOTE: this code currently ignores the possibility of overflow\n+            // into usize::MAX; in general both Rc and Arc need to be adjusted\n+            // to deal with overflow.\n+\n+            // Unlike with Clone(), we need this to be an Acquire read to\n+            // synchronize with the write coming from `is_unique`, so that the\n+            // events prior to that write happen before this read.\n+            if self.inner().weak.compare_and_swap(cur, cur + 1, Acquire) == cur {\n+                return Weak { _ptr: self._ptr }\n+            }\n+        }\n     }\n \n     /// Get the number of weak references to this value.\n@@ -258,51 +281,6 @@ pub fn weak_count<T: ?Sized>(this: &Arc<T>) -> usize { Arc::weak_count(this) }\n #[deprecated(since = \"1.2.0\", reason = \"renamed to Arc::strong_count\")]\n pub fn strong_count<T: ?Sized>(this: &Arc<T>) -> usize { Arc::strong_count(this) }\n \n-\n-/// Returns a mutable reference to the contained value if the `Arc<T>` is unique.\n-///\n-/// Returns `None` if the `Arc<T>` is not unique.\n-///\n-/// This function is marked **unsafe** because it is racy if weak pointers\n-/// are active.\n-///\n-/// # Examples\n-///\n-/// ```\n-/// # #![feature(arc_unique, alloc)]\n-/// extern crate alloc;\n-/// # fn main() {\n-/// use alloc::arc::{Arc, get_mut};\n-///\n-/// # unsafe {\n-/// let mut x = Arc::new(3);\n-/// *get_mut(&mut x).unwrap() = 4;\n-/// assert_eq!(*x, 4);\n-///\n-/// let _y = x.clone();\n-/// assert!(get_mut(&mut x).is_none());\n-/// # }\n-/// # }\n-/// ```\n-#[inline]\n-#[unstable(feature = \"arc_unique\")]\n-#[deprecated(since = \"1.2.0\",\n-             reason = \"this function is unsafe with weak pointers\")]\n-pub unsafe fn get_mut<T: ?Sized>(this: &mut Arc<T>) -> Option<&mut T> {\n-    // FIXME(#24880) potential race with upgraded weak pointers here\n-    if Arc::strong_count(this) == 1 && Arc::weak_count(this) == 0 {\n-        // This unsafety is ok because we're guaranteed that the pointer\n-        // returned is the *only* pointer that will ever be returned to T. Our\n-        // reference count is guaranteed to be 1 at this point, and we required\n-        // the Arc itself to be `mut`, so we're returning the only possible\n-        // reference to the inner data.\n-        let inner = &mut **this._ptr;\n-        Some(&mut inner.data)\n-    } else {\n-        None\n-    }\n-}\n-\n #[stable(feature = \"rust1\", since = \"1.0.0\")]\n impl<T: ?Sized> Clone for Arc<T> {\n     /// Makes a clone of the `Arc<T>`.\n@@ -350,44 +328,150 @@ impl<T: Clone> Arc<T> {\n     /// Make a mutable reference from the given `Arc<T>`.\n     ///\n     /// This is also referred to as a copy-on-write operation because the inner\n-    /// data is cloned if the reference count is greater than one.\n-    ///\n-    /// This method is marked **unsafe** because it is racy if weak pointers\n-    /// are active.\n+    /// data is cloned if the (strong) reference count is greater than one. If\n+    /// we hold the only strong reference, any existing weak references will no\n+    /// longer be upgradeable.\n     ///\n     /// # Examples\n     ///\n     /// ```\n     /// # #![feature(arc_unique)]\n     /// use std::sync::Arc;\n     ///\n-    /// # unsafe {\n     /// let mut five = Arc::new(5);\n     ///\n-    /// let mut_five = five.make_unique();\n-    /// # }\n+    /// let mut_five = Arc::make_unique(&mut five);\n     /// ```\n     #[inline]\n     #[unstable(feature = \"arc_unique\")]\n-    #[deprecated(since = \"1.2.0\",\n-                 reason = \"this function is unsafe with weak pointers\")]\n-    pub unsafe fn make_unique(&mut self) -> &mut T {\n-        // FIXME(#24880) potential race with upgraded weak pointers here\n+    pub fn make_unique(this: &mut Arc<T>) -> &mut T {\n+        // Note that we hold both a strong reference and a weak reference.\n+        // Thus, releasing our strong reference only will not, by itself, cause\n+        // the memory to be deallocated.\n         //\n-        // Note that we hold a strong reference, which also counts as a weak\n-        // reference, so we only clone if there is an additional reference of\n-        // either kind.\n-        if self.inner().strong.load(SeqCst) != 1 ||\n-           self.inner().weak.load(SeqCst) != 1 {\n-            *self = Arc::new((**self).clone())\n+        // Use Acquire to ensure that we see any writes to `weak` that happen\n+        // before release writes (i.e., decrements) to `strong`. Since we hold a\n+        // weak count, there's no chance the ArcInner itself could be\n+        // deallocated.\n+        if this.inner().strong.compare_and_swap(1, 0, Acquire) != 1 {\n+            // Another srong pointer exists; clone\n+            *this = Arc::new((**this).clone());\n+        } else if this.inner().weak.load(Relaxed) != 1 {\n+            // Relaxed suffices in the above because this is fundamentally an\n+            // optimization: we are always racing with weak pointers being\n+            // dropped. Worst case, we end up allocated a new Arc unnecessarily.\n+\n+            // We removed the last strong ref, but there are additional weak\n+            // refs remaining. We'll move the contents to a new Arc, and\n+            // invalidate the other weak refs.\n+\n+            // Note that it is not possible for the read of `weak` to yield\n+            // usize::MAX (i.e., locked), since the weak count can only be\n+            // locked by a thread with a strong reference.\n+\n+            // Materialize our own implicit weak pointer, so that it can clean\n+            // up the ArcInner as needed.\n+            let weak = Weak { _ptr: this._ptr };\n+\n+            // mark the data itself as already deallocated\n+            unsafe {\n+                // there is no data race in the implicit write caused by `read`\n+                // here (due to zeroing) because data is no longer accessed by\n+                // other threads (due to there being no more strong refs at this\n+                // point).\n+                let mut swap = Arc::new(ptr::read(&(**weak._ptr).data));\n+                mem::swap(this, &mut swap);\n+                mem::forget(swap);\n+            }\n+        } else {\n+            // We were the sole reference of either kind; bump back up the\n+            // strong ref count.\n+            this.inner().strong.store(1, Release);\n         }\n+\n         // As with `get_mut()`, the unsafety is ok because our reference was\n         // either unique to begin with, or became one upon cloning the contents.\n-        let inner = &mut **self._ptr;\n-        &mut inner.data\n+        unsafe {\n+            let inner = &mut **this._ptr;\n+            &mut inner.data\n+        }\n     }\n }\n \n+impl<T: ?Sized> Arc<T> {\n+    /// Returns a mutable reference to the contained value if the `Arc<T>` is unique.\n+    ///\n+    /// Returns `None` if the `Arc<T>` is not unique.\n+    ///\n+    /// # Examples\n+    ///\n+    /// ```\n+    /// # #![feature(arc_unique, alloc)]\n+    /// extern crate alloc;\n+    /// # fn main() {\n+    /// use alloc::arc::Arc;\n+    ///\n+    /// let mut x = Arc::new(3);\n+    /// *Arc::get_mut(&mut x).unwrap() = 4;\n+    /// assert_eq!(*x, 4);\n+    ///\n+    /// let _y = x.clone();\n+    /// assert!(Arc::get_mut(&mut x).is_none());\n+    /// # }\n+    /// ```\n+    #[inline]\n+    #[unstable(feature = \"arc_unique\")]\n+    pub fn get_mut(this: &mut Arc<T>) -> Option<&mut T> {\n+        if this.is_unique() {\n+            // This unsafety is ok because we're guaranteed that the pointer\n+            // returned is the *only* pointer that will ever be returned to T. Our\n+            // reference count is guaranteed to be 1 at this point, and we required\n+            // the Arc itself to be `mut`, so we're returning the only possible\n+            // reference to the inner data.\n+            unsafe {\n+                let inner = &mut **this._ptr;\n+                Some(&mut inner.data)\n+            }\n+        } else {\n+            None\n+        }\n+    }\n+\n+    /// Determine whether this is the unique reference (including weak refs) to\n+    /// the underlying data.\n+    ///\n+    /// Note that this requires locking the weak ref count.\n+    fn is_unique(&mut self) -> bool {\n+        // lock the weak pointer count if we appear to be the sole weak pointer\n+        // holder.\n+        //\n+        // The acquire label here ensures a happens-before relationship with any\n+        // writes to `strong` prior to decrements of the `weak` count (via drop,\n+        // which uses Release).\n+        if self.inner().weak.compare_and_swap(1, usize::MAX, Acquire) == 1 {\n+            // Due to the previous acquire read, this will observe any writes to\n+            // `strong` that were due to upgrading weak pointers; only strong\n+            // clones remain, which require that the strong count is > 1 anyway.\n+            let unique = self.inner().strong.load(Relaxed) == 1;\n+\n+            // The release write here synchronizes with a read in `downgrade`,\n+            // effectively preventing the above read of `strong` from happening\n+            // after the write.\n+            self.inner().weak.store(1, Release); // release the lock\n+            unique\n+        } else {\n+            false\n+        }\n+    }\n+}\n+\n+#[inline]\n+#[unstable(feature = \"arc_unique\")]\n+#[deprecated(since = \"1.2\", reason = \"use Arc::get_mut instead\")]\n+pub fn get_mut<T: ?Sized>(this: &mut Arc<T>) -> Option<&mut T> {\n+    Arc::get_mut(this)\n+}\n+\n #[stable(feature = \"rust1\", since = \"1.0.0\")]\n impl<T: ?Sized> Drop for Arc<T> {\n     /// Drops the `Arc<T>`.\n@@ -483,9 +567,15 @@ impl<T: ?Sized> Weak<T> {\n         // fetch_add because once the count hits 0 it must never be above 0.\n         let inner = self.inner();\n         loop {\n-            let n = inner.strong.load(SeqCst);\n+            // Relaxed load because any write of 0 that we can observe\n+            // leaves the field in a permanently zero state (so a\n+            // \"stale\" read of 0 is fine), and any other value is\n+            // confirmed via the CAS below.\n+            let n = inner.strong.load(Relaxed);\n             if n == 0 { return None }\n-            let old = inner.strong.compare_and_swap(n, n + 1, SeqCst);\n+\n+            // Relaxed is valid for the same reason it is on Arc's Clone impl\n+            let old = inner.strong.compare_and_swap(n, n + 1, Relaxed);\n             if old == n { return Some(Arc { _ptr: self._ptr }) }\n         }\n     }\n@@ -516,9 +606,12 @@ impl<T: ?Sized> Clone for Weak<T> {\n     /// ```\n     #[inline]\n     fn clone(&self) -> Weak<T> {\n-        // See comments in Arc::clone() for why this is relaxed\n+        // See comments in Arc::clone() for why this is relaxed.  This can use a\n+        // fetch_add (ignoring the lock) because the weak count is only locked\n+        // where are *no other* weak pointers in existence. (So we can't be\n+        // running this code in that case).\n         self.inner().weak.fetch_add(1, Relaxed);\n-        Weak { _ptr: self._ptr }\n+        return Weak { _ptr: self._ptr }\n     }\n }\n \n@@ -561,6 +654,11 @@ impl<T: ?Sized> Drop for Weak<T> {\n         // If we find out that we were the last weak pointer, then its time to\n         // deallocate the data entirely. See the discussion in Arc::drop() about\n         // the memory orderings\n+        //\n+        // It's not necessary to check for the locked state here, because the\n+        // weak count can only be locked if there was precisely one weak ref,\n+        // meaning that drop could only subsequently run ON that remaining weak\n+        // ref, which can only happen after the lock is released.\n         if self.inner().weak.fetch_sub(1, Release) == 1 {\n             atomic::fence(Acquire);\n             unsafe { deallocate(ptr as *mut u8,\n@@ -792,13 +890,13 @@ mod tests {\n             let mut cow1 = cow0.clone();\n             let mut cow2 = cow1.clone();\n \n-            assert!(75 == *cow0.make_unique());\n-            assert!(75 == *cow1.make_unique());\n-            assert!(75 == *cow2.make_unique());\n+            assert!(75 == *Arc::make_unique(&mut cow0));\n+            assert!(75 == *Arc::make_unique(&mut cow1));\n+            assert!(75 == *Arc::make_unique(&mut cow2));\n \n-            *cow0.make_unique() += 1;\n-            *cow1.make_unique() += 2;\n-            *cow2.make_unique() += 3;\n+            *Arc::make_unique(&mut cow0) += 1;\n+            *Arc::make_unique(&mut cow1) += 2;\n+            *Arc::make_unique(&mut cow2) += 3;\n \n             assert!(76 == *cow0);\n             assert!(77 == *cow1);\n@@ -822,7 +920,7 @@ mod tests {\n         assert!(75 == *cow2);\n \n         unsafe {\n-            *cow0.make_unique() += 1;\n+            *Arc::make_unique(&mut cow0) += 1;\n         }\n \n         assert!(76 == *cow0);\n@@ -845,7 +943,7 @@ mod tests {\n         assert!(75 == *cow1_weak.upgrade().unwrap());\n \n         unsafe {\n-            *cow0.make_unique() += 1;\n+            *Arc::make_unique(&mut cow0) += 1;\n         }\n \n         assert!(76 == *cow0);"}]}