{"sha": "b4acb110333392ecdaf890fce080e4b576106aae", "node_id": "MDY6Q29tbWl0NzI0NzEyOmI0YWNiMTEwMzMzMzkyZWNkYWY4OTBmY2UwODBlNGI1NzYxMDZhYWU=", "commit": {"author": {"name": "bors", "email": "bors@rust-lang.org", "date": "2020-09-02T03:19:38Z"}, "committer": {"name": "bors", "email": "bors@rust-lang.org", "date": "2020-09-02T03:19:38Z"}, "message": "Auto merge of #76170 - matklad:notrivia, r=petrochenkov\n\nRemove trivia tokens\n\nr? @ghost", "tree": {"sha": "b025ea8c490ac55356886220a1917e0e66242498", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/b025ea8c490ac55356886220a1917e0e66242498"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/b4acb110333392ecdaf890fce080e4b576106aae", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/b4acb110333392ecdaf890fce080e4b576106aae", "html_url": "https://github.com/rust-lang/rust/commit/b4acb110333392ecdaf890fce080e4b576106aae", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/b4acb110333392ecdaf890fce080e4b576106aae/comments", "author": {"login": "bors", "id": 3372342, "node_id": "MDQ6VXNlcjMzNzIzNDI=", "avatar_url": "https://avatars.githubusercontent.com/u/3372342?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bors", "html_url": "https://github.com/bors", "followers_url": "https://api.github.com/users/bors/followers", "following_url": "https://api.github.com/users/bors/following{/other_user}", "gists_url": "https://api.github.com/users/bors/gists{/gist_id}", "starred_url": "https://api.github.com/users/bors/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bors/subscriptions", "organizations_url": "https://api.github.com/users/bors/orgs", "repos_url": "https://api.github.com/users/bors/repos", "events_url": "https://api.github.com/users/bors/events{/privacy}", "received_events_url": "https://api.github.com/users/bors/received_events", "type": "User", "site_admin": false}, "committer": {"login": "bors", "id": 3372342, "node_id": "MDQ6VXNlcjMzNzIzNDI=", "avatar_url": "https://avatars.githubusercontent.com/u/3372342?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bors", "html_url": "https://github.com/bors", "followers_url": "https://api.github.com/users/bors/followers", "following_url": "https://api.github.com/users/bors/following{/other_user}", "gists_url": "https://api.github.com/users/bors/gists{/gist_id}", "starred_url": "https://api.github.com/users/bors/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bors/subscriptions", "organizations_url": "https://api.github.com/users/bors/orgs", "repos_url": "https://api.github.com/users/bors/repos", "events_url": "https://api.github.com/users/bors/events{/privacy}", "received_events_url": "https://api.github.com/users/bors/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "e36e4bd0f7e722d3c97d1ca45387e58f81e4e8ea", "url": "https://api.github.com/repos/rust-lang/rust/commits/e36e4bd0f7e722d3c97d1ca45387e58f81e4e8ea", "html_url": "https://github.com/rust-lang/rust/commit/e36e4bd0f7e722d3c97d1ca45387e58f81e4e8ea"}, {"sha": "fabd8a68345270c053cce906e5f037e0cfe7b6ef", "url": "https://api.github.com/repos/rust-lang/rust/commits/fabd8a68345270c053cce906e5f037e0cfe7b6ef", "html_url": "https://github.com/rust-lang/rust/commit/fabd8a68345270c053cce906e5f037e0cfe7b6ef"}], "stats": {"total": 182, "additions": 71, "deletions": 111}, "files": [{"sha": "c6cc890b47f79bebc9e20657bbd98cd89dd2312f", "filename": "compiler/rustc_ast/src/token.rs", "status": "modified", "additions": 3, "deletions": 15, "changes": 18, "blob_url": "https://github.com/rust-lang/rust/blob/b4acb110333392ecdaf890fce080e4b576106aae/compiler%2Frustc_ast%2Fsrc%2Ftoken.rs", "raw_url": "https://github.com/rust-lang/rust/raw/b4acb110333392ecdaf890fce080e4b576106aae/compiler%2Frustc_ast%2Fsrc%2Ftoken.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_ast%2Fsrc%2Ftoken.rs?ref=b4acb110333392ecdaf890fce080e4b576106aae", "patch": "@@ -251,17 +251,6 @@ pub enum TokenKind {\n     /// similarly to symbols in string literal tokens.\n     DocComment(CommentKind, ast::AttrStyle, Symbol),\n \n-    // Junk. These carry no data because we don't really care about the data\n-    // they *would* carry, and don't really want to allocate a new ident for\n-    // them. Instead, users could extract that from the associated span.\n-    /// Whitespace.\n-    Whitespace,\n-    /// A comment.\n-    Comment,\n-    Shebang(Symbol),\n-    /// A completely invalid token which should be skipped.\n-    Unknown(Symbol),\n-\n     Eof,\n }\n \n@@ -331,7 +320,7 @@ impl Token {\n \n     /// Some token that will be thrown away later.\n     pub fn dummy() -> Self {\n-        Token::new(TokenKind::Whitespace, DUMMY_SP)\n+        Token::new(TokenKind::Question, DUMMY_SP)\n     }\n \n     /// Recovers a `Token` from an `Ident`. This creates a raw identifier if necessary.\n@@ -360,7 +349,7 @@ impl Token {\n     pub fn is_op(&self) -> bool {\n         match self.kind {\n             OpenDelim(..) | CloseDelim(..) | Literal(..) | DocComment(..) | Ident(..)\n-            | Lifetime(..) | Interpolated(..) | Whitespace | Comment | Shebang(..) | Eof => false,\n+            | Lifetime(..) | Interpolated(..) | Eof => false,\n             _ => true,\n         }\n     }\n@@ -676,8 +665,7 @@ impl Token {\n             Le | EqEq | Ne | Ge | AndAnd | OrOr | Tilde | BinOpEq(..) | At | DotDotDot\n             | DotDotEq | Comma | Semi | ModSep | RArrow | LArrow | FatArrow | Pound | Dollar\n             | Question | OpenDelim(..) | CloseDelim(..) | Literal(..) | Ident(..)\n-            | Lifetime(..) | Interpolated(..) | DocComment(..) | Whitespace | Comment\n-            | Shebang(..) | Unknown(..) | Eof => return None,\n+            | Lifetime(..) | Interpolated(..) | DocComment(..) | Eof => return None,\n         };\n \n         Some(Token::new(kind, self.span.to(joint.span)))"}, {"sha": "9743a0004296995ebc4ced369a852809e04ccc75", "filename": "compiler/rustc_ast_pretty/src/pprust.rs", "status": "modified", "additions": 0, "deletions": 4, "changes": 4, "blob_url": "https://github.com/rust-lang/rust/blob/b4acb110333392ecdaf890fce080e4b576106aae/compiler%2Frustc_ast_pretty%2Fsrc%2Fpprust.rs", "raw_url": "https://github.com/rust-lang/rust/raw/b4acb110333392ecdaf890fce080e4b576106aae/compiler%2Frustc_ast_pretty%2Fsrc%2Fpprust.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_ast_pretty%2Fsrc%2Fpprust.rs?ref=b4acb110333392ecdaf890fce080e4b576106aae", "patch": "@@ -289,10 +289,6 @@ fn token_kind_to_string_ext(tok: &TokenKind, convert_dollar_crate: Option<Span>)\n             doc_comment_to_string(comment_kind, attr_style, data)\n         }\n         token::Eof => \"<eof>\".to_string(),\n-        token::Whitespace => \" \".to_string(),\n-        token::Comment => \"/* */\".to_string(),\n-        token::Shebang(s) => format!(\"/* shebang: {}*/\", s),\n-        token::Unknown(s) => s.to_string(),\n \n         token::Interpolated(ref nt) => nonterminal_to_string(nt),\n     }"}, {"sha": "39c82f97e0a39ce3503f084269ba7fba98285941", "filename": "compiler/rustc_expand/src/proc_macro_server.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/b4acb110333392ecdaf890fce080e4b576106aae/compiler%2Frustc_expand%2Fsrc%2Fproc_macro_server.rs", "raw_url": "https://github.com/rust-lang/rust/raw/b4acb110333392ecdaf890fce080e4b576106aae/compiler%2Frustc_expand%2Fsrc%2Fproc_macro_server.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_expand%2Fsrc%2Fproc_macro_server.rs?ref=b4acb110333392ecdaf890fce080e4b576106aae", "patch": "@@ -189,7 +189,7 @@ impl FromInternal<(TreeAndJoint, &'_ ParseSess, &'_ mut Vec<Self>)>\n             }\n \n             OpenDelim(..) | CloseDelim(..) => unreachable!(),\n-            Whitespace | Comment | Shebang(..) | Unknown(..) | Eof => unreachable!(),\n+            Eof => unreachable!(),\n         }\n     }\n }"}, {"sha": "034442b798b29b193264469a66c2477fd326026e", "filename": "compiler/rustc_parse/src/lexer/mod.rs", "status": "modified", "additions": 53, "deletions": 60, "changes": 113, "blob_url": "https://github.com/rust-lang/rust/blob/b4acb110333392ecdaf890fce080e4b576106aae/compiler%2Frustc_parse%2Fsrc%2Flexer%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/b4acb110333392ecdaf890fce080e4b576106aae/compiler%2Frustc_parse%2Fsrc%2Flexer%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_parse%2Fsrc%2Flexer%2Fmod.rs?ref=b4acb110333392ecdaf890fce080e4b576106aae", "patch": "@@ -1,5 +1,6 @@\n use rustc_ast::ast::AttrStyle;\n use rustc_ast::token::{self, CommentKind, Token, TokenKind};\n+use rustc_ast::tokenstream::IsJoint;\n use rustc_data_structures::sync::Lrc;\n use rustc_errors::{error_code, Applicability, DiagnosticBuilder, FatalError};\n use rustc_lexer::Base;\n@@ -65,42 +66,46 @@ impl<'a> StringReader<'a> {\n         self.override_span.unwrap_or_else(|| Span::with_root_ctxt(lo, hi))\n     }\n \n-    /// Returns the next token, including trivia like whitespace or comments.\n-    fn next_token(&mut self) -> Token {\n+    /// Returns the next token, and info about preceding whitespace, if any.\n+    fn next_token(&mut self) -> (IsJoint, Token) {\n+        let mut is_joint = IsJoint::Joint;\n+\n+        // Skip `#!` at the start of the file\n         let start_src_index = self.src_index(self.pos);\n         let text: &str = &self.src[start_src_index..self.end_src_index];\n-\n-        if text.is_empty() {\n-            let span = self.mk_sp(self.pos, self.pos);\n-            return Token::new(token::Eof, span);\n+        let is_beginning_of_file = self.pos == self.start_pos;\n+        if is_beginning_of_file {\n+            if let Some(shebang_len) = rustc_lexer::strip_shebang(text) {\n+                self.pos = self.pos + BytePos::from_usize(shebang_len);\n+                is_joint = IsJoint::NonJoint;\n+            }\n         }\n \n-        {\n-            let is_beginning_of_file = self.pos == self.start_pos;\n-            if is_beginning_of_file {\n-                if let Some(shebang_len) = rustc_lexer::strip_shebang(text) {\n-                    let start = self.pos;\n-                    self.pos = self.pos + BytePos::from_usize(shebang_len);\n+        // Skip trivial (whitespace & comments) tokens\n+        loop {\n+            let start_src_index = self.src_index(self.pos);\n+            let text: &str = &self.src[start_src_index..self.end_src_index];\n \n-                    let sym = self.symbol_from(start + BytePos::from_usize(\"#!\".len()));\n-                    let kind = token::Shebang(sym);\n-\n-                    let span = self.mk_sp(start, self.pos);\n-                    return Token::new(kind, span);\n-                }\n+            if text.is_empty() {\n+                let span = self.mk_sp(self.pos, self.pos);\n+                return (is_joint, Token::new(token::Eof, span));\n             }\n-        }\n \n-        let token = rustc_lexer::first_token(text);\n+            let token = rustc_lexer::first_token(text);\n \n-        let start = self.pos;\n-        self.pos = self.pos + BytePos::from_usize(token.len);\n+            let start = self.pos;\n+            self.pos = self.pos + BytePos::from_usize(token.len);\n \n-        debug!(\"try_next_token: {:?}({:?})\", token.kind, self.str_from(start));\n+            debug!(\"next_token: {:?}({:?})\", token.kind, self.str_from(start));\n \n-        let kind = self.cook_lexer_token(token.kind, start);\n-        let span = self.mk_sp(start, self.pos);\n-        Token::new(kind, span)\n+            match self.cook_lexer_token(token.kind, start) {\n+                Some(kind) => {\n+                    let span = self.mk_sp(start, self.pos);\n+                    return (is_joint, Token::new(kind, span));\n+                }\n+                None => is_joint = IsJoint::NonJoint,\n+            }\n+        }\n     }\n \n     /// Report a fatal lexical error with a given span.\n@@ -140,19 +145,16 @@ impl<'a> StringReader<'a> {\n     /// Turns simple `rustc_lexer::TokenKind` enum into a rich\n     /// `librustc_ast::TokenKind`. This turns strings into interned\n     /// symbols and runs additional validation.\n-    fn cook_lexer_token(&self, token: rustc_lexer::TokenKind, start: BytePos) -> TokenKind {\n-        match token {\n+    fn cook_lexer_token(&self, token: rustc_lexer::TokenKind, start: BytePos) -> Option<TokenKind> {\n+        Some(match token {\n             rustc_lexer::TokenKind::LineComment { doc_style } => {\n-                match doc_style {\n-                    Some(doc_style) => {\n-                        // Opening delimiter of the length 3 is not included into the symbol.\n-                        let content_start = start + BytePos(3);\n-                        let content = self.str_from(content_start);\n+                // Skip non-doc comments\n+                let doc_style = doc_style?;\n \n-                        self.cook_doc_comment(content_start, content, CommentKind::Line, doc_style)\n-                    }\n-                    None => token::Comment,\n-                }\n+                // Opening delimiter of the length 3 is not included into the symbol.\n+                let content_start = start + BytePos(3);\n+                let content = self.str_from(content_start);\n+                self.cook_doc_comment(content_start, content, CommentKind::Line, doc_style)\n             }\n             rustc_lexer::TokenKind::BlockComment { doc_style, terminated } => {\n                 if !terminated {\n@@ -171,20 +173,18 @@ impl<'a> StringReader<'a> {\n                         .emit();\n                     FatalError.raise();\n                 }\n-                match doc_style {\n-                    Some(doc_style) => {\n-                        // Opening delimiter of the length 3 and closing delimiter of the length 2\n-                        // are not included into the symbol.\n-                        let content_start = start + BytePos(3);\n-                        let content_end = self.pos - BytePos(if terminated { 2 } else { 0 });\n-                        let content = self.str_from_to(content_start, content_end);\n-\n-                        self.cook_doc_comment(content_start, content, CommentKind::Block, doc_style)\n-                    }\n-                    None => token::Comment,\n-                }\n+\n+                // Skip non-doc comments\n+                let doc_style = doc_style?;\n+\n+                // Opening delimiter of the length 3 and closing delimiter of the length 2\n+                // are not included into the symbol.\n+                let content_start = start + BytePos(3);\n+                let content_end = self.pos - BytePos(if terminated { 2 } else { 0 });\n+                let content = self.str_from_to(content_start, content_end);\n+                self.cook_doc_comment(content_start, content, CommentKind::Block, doc_style)\n             }\n-            rustc_lexer::TokenKind::Whitespace => token::Whitespace,\n+            rustc_lexer::TokenKind::Whitespace => return None,\n             rustc_lexer::TokenKind::Ident | rustc_lexer::TokenKind::RawIdent => {\n                 let is_raw_ident = token == rustc_lexer::TokenKind::RawIdent;\n                 let mut ident_start = start;\n@@ -282,12 +282,11 @@ impl<'a> StringReader<'a> {\n                 // this should be inside `rustc_lexer`. However, we should first remove compound\n                 // tokens like `<<` from `rustc_lexer`, and then add fancier error recovery to it,\n                 // as there will be less overall work to do this way.\n-                let token = unicode_chars::check_for_substitution(self, start, c, &mut err)\n-                    .unwrap_or_else(|| token::Unknown(self.symbol_from(start)));\n+                let token = unicode_chars::check_for_substitution(self, start, c, &mut err);\n                 err.emit();\n-                token\n+                token?\n             }\n-        }\n+        })\n     }\n \n     fn cook_doc_comment(\n@@ -450,12 +449,6 @@ impl<'a> StringReader<'a> {\n         self.str_from_to(start, self.pos)\n     }\n \n-    /// Creates a Symbol from a given offset to the current offset.\n-    fn symbol_from(&self, start: BytePos) -> Symbol {\n-        debug!(\"taking an ident from {:?} to {:?}\", start, self.pos);\n-        Symbol::intern(self.str_from(start))\n-    }\n-\n     /// As symbol_from, with an explicit endpoint.\n     fn symbol_from_to(&self, start: BytePos, end: BytePos) -> Symbol {\n         debug!(\"taking an ident from {:?} to {:?}\", start, end);"}, {"sha": "d5977ca3c7d2fb8a735841b50dc13c578af613b9", "filename": "compiler/rustc_parse/src/lexer/tokentrees.rs", "status": "modified", "additions": 13, "deletions": 23, "changes": 36, "blob_url": "https://github.com/rust-lang/rust/blob/b4acb110333392ecdaf890fce080e4b576106aae/compiler%2Frustc_parse%2Fsrc%2Flexer%2Ftokentrees.rs", "raw_url": "https://github.com/rust-lang/rust/raw/b4acb110333392ecdaf890fce080e4b576106aae/compiler%2Frustc_parse%2Fsrc%2Flexer%2Ftokentrees.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_parse%2Fsrc%2Flexer%2Ftokentrees.rs?ref=b4acb110333392ecdaf890fce080e4b576106aae", "patch": "@@ -16,7 +16,6 @@ impl<'a> StringReader<'a> {\n         let mut tt_reader = TokenTreesReader {\n             string_reader: self,\n             token: Token::dummy(),\n-            joint_to_prev: Joint,\n             open_braces: Vec::new(),\n             unmatched_braces: Vec::new(),\n             matching_delim_spans: Vec::new(),\n@@ -32,7 +31,6 @@ impl<'a> StringReader<'a> {\n struct TokenTreesReader<'a> {\n     string_reader: StringReader<'a>,\n     token: Token,\n-    joint_to_prev: IsJoint,\n     /// Stack of open delimiters and their spans. Used for error message.\n     open_braces: Vec<(token::DelimToken, Span)>,\n     unmatched_braces: Vec<UnmatchedBrace>,\n@@ -53,7 +51,7 @@ impl<'a> TokenTreesReader<'a> {\n     fn parse_all_token_trees(&mut self) -> PResult<'a, TokenStream> {\n         let mut buf = TokenStreamBuilder::default();\n \n-        self.real_token();\n+        self.bump();\n         while self.token != token::Eof {\n             buf.push(self.parse_token_tree()?);\n         }\n@@ -126,7 +124,7 @@ impl<'a> TokenTreesReader<'a> {\n \n                 // Parse the open delimiter.\n                 self.open_braces.push((delim, self.token.span));\n-                self.real_token();\n+                self.bump();\n \n                 // Parse the token trees within the delimiters.\n                 // We stop at any delimiter so we can try to recover if the user\n@@ -171,7 +169,7 @@ impl<'a> TokenTreesReader<'a> {\n                             ));\n                         }\n                         // Parse the closing delimiter.\n-                        self.real_token();\n+                        self.bump();\n                     }\n                     // Incorrect delimiter.\n                     token::CloseDelim(other) => {\n@@ -217,7 +215,7 @@ impl<'a> TokenTreesReader<'a> {\n                         //     bar(baz(\n                         // }  // Incorrect delimiter but matches the earlier `{`\n                         if !self.open_braces.iter().any(|&(b, _)| b == other) {\n-                            self.real_token();\n+                            self.bump();\n                         }\n                     }\n                     token::Eof => {\n@@ -264,27 +262,19 @@ impl<'a> TokenTreesReader<'a> {\n             }\n             _ => {\n                 let tt = TokenTree::Token(self.token.take());\n-                self.real_token();\n-                let is_joint = self.joint_to_prev == Joint && self.token.is_op();\n-                Ok((tt, if is_joint { Joint } else { NonJoint }))\n+                let mut is_joint = self.bump();\n+                if !self.token.is_op() {\n+                    is_joint = NonJoint;\n+                }\n+                Ok((tt, is_joint))\n             }\n         }\n     }\n \n-    fn real_token(&mut self) {\n-        self.joint_to_prev = Joint;\n-        loop {\n-            let token = self.string_reader.next_token();\n-            match token.kind {\n-                token::Whitespace | token::Comment | token::Shebang(_) | token::Unknown(_) => {\n-                    self.joint_to_prev = NonJoint;\n-                }\n-                _ => {\n-                    self.token = token;\n-                    return;\n-                }\n-            }\n-        }\n+    fn bump(&mut self) -> IsJoint {\n+        let (joint_to_prev, token) = self.string_reader.next_token();\n+        self.token = token;\n+        joint_to_prev\n     }\n }\n "}, {"sha": "8dc0db01ecb510bb1ee355db412f3e2ecaaa484e", "filename": "compiler/rustc_parse/src/lexer/unicode_chars.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/b4acb110333392ecdaf890fce080e4b576106aae/compiler%2Frustc_parse%2Fsrc%2Flexer%2Funicode_chars.rs", "raw_url": "https://github.com/rust-lang/rust/raw/b4acb110333392ecdaf890fce080e4b576106aae/compiler%2Frustc_parse%2Fsrc%2Flexer%2Funicode_chars.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_parse%2Fsrc%2Flexer%2Funicode_chars.rs?ref=b4acb110333392ecdaf890fce080e4b576106aae", "patch": "@@ -303,7 +303,7 @@ const UNICODE_ARRAY: &[(char, &str, char)] = &[\n // However, we should first remove compound tokens like `<<` from `rustc_lexer`, and then add\n // fancier error recovery to it, as there will be less overall work to do this way.\n const ASCII_ARRAY: &[(char, &str, Option<token::TokenKind>)] = &[\n-    (' ', \"Space\", Some(token::Whitespace)),\n+    (' ', \"Space\", None),\n     ('_', \"Underscore\", Some(token::Ident(kw::Underscore, false))),\n     ('-', \"Minus/Hyphen\", Some(token::BinOp(token::Minus))),\n     (',', \"Comma\", Some(token::Comma)),"}, {"sha": "462279b0a9e039b3f2892f5a37329df865214cb5", "filename": "compiler/rustc_parse/src/lib.rs", "status": "modified", "additions": 0, "deletions": 7, "changes": 7, "blob_url": "https://github.com/rust-lang/rust/blob/b4acb110333392ecdaf890fce080e4b576106aae/compiler%2Frustc_parse%2Fsrc%2Flib.rs", "raw_url": "https://github.com/rust-lang/rust/raw/b4acb110333392ecdaf890fce080e4b576106aae/compiler%2Frustc_parse%2Fsrc%2Flib.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_parse%2Fsrc%2Flib.rs?ref=b4acb110333392ecdaf890fce080e4b576106aae", "patch": "@@ -348,9 +348,6 @@ pub fn tokenstream_probably_equal_for_proc_macro(\n                 | token::CloseDelim(DelimToken::NoDelim)\n                 // The pretty printer collapses many semicolons into one.\n                 | token::Semi\n-                // The pretty printer collapses whitespace arbitrarily and can\n-                // introduce whitespace from `NoDelim`.\n-                | token::Whitespace\n                 // The pretty printer can turn `$crate` into `::crate_name`\n                 | token::ModSep = token.kind {\n                 return false;\n@@ -506,8 +503,6 @@ fn token_probably_equal_for_proc_macro(first: &Token, other: &Token) -> bool {\n         | (&Pound, &Pound)\n         | (&Dollar, &Dollar)\n         | (&Question, &Question)\n-        | (&Whitespace, &Whitespace)\n-        | (&Comment, &Comment)\n         | (&Eof, &Eof) => true,\n \n         (&BinOp(a), &BinOp(b)) | (&BinOpEq(a), &BinOpEq(b)) => a == b,\n@@ -516,8 +511,6 @@ fn token_probably_equal_for_proc_macro(first: &Token, other: &Token) -> bool {\n \n         (&DocComment(a1, a2, a3), &DocComment(b1, b2, b3)) => a1 == b1 && a2 == b2 && a3 == b3,\n \n-        (&Shebang(a), &Shebang(b)) => a == b,\n-\n         (&Literal(a), &Literal(b)) => a == b,\n \n         (&Lifetime(a), &Lifetime(b)) => a == b,"}]}