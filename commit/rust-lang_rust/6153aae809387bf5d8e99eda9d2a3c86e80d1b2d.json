{"sha": "6153aae809387bf5d8e99eda9d2a3c86e80d1b2d", "node_id": "MDY6Q29tbWl0NzI0NzEyOjYxNTNhYWU4MDkzODdiZjVkOGU5OWVkYTlkMmEzYzg2ZTgwZDFiMmQ=", "commit": {"author": {"name": "bors", "email": "bors@rust-lang.org", "date": "2013-04-03T18:31:03Z"}, "committer": {"name": "bors", "email": "bors@rust-lang.org", "date": "2013-04-03T18:31:03Z"}, "message": "auto merge of #5559 : jbclements/rust/change-to-tt-based-parsing, r=jbclements\n\nChanges the parser to parse all streams into token-trees before hitting the parser proper, in preparation for hygiene.  As an added bonus, it appears to speed up the parser (albeit by a totally imperceptible 1%).\r\n\r\nAlso, many comments in the parser.\r\nAlso, field renaming in token-trees (readme->forest, cur->stack).", "tree": {"sha": "8ace223fbff35ea48430490b55fb4af533391a59", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/8ace223fbff35ea48430490b55fb4af533391a59"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/6153aae809387bf5d8e99eda9d2a3c86e80d1b2d", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/6153aae809387bf5d8e99eda9d2a3c86e80d1b2d", "html_url": "https://github.com/rust-lang/rust/commit/6153aae809387bf5d8e99eda9d2a3c86e80d1b2d", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/6153aae809387bf5d8e99eda9d2a3c86e80d1b2d/comments", "author": {"login": "bors", "id": 3372342, "node_id": "MDQ6VXNlcjMzNzIzNDI=", "avatar_url": "https://avatars.githubusercontent.com/u/3372342?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bors", "html_url": "https://github.com/bors", "followers_url": "https://api.github.com/users/bors/followers", "following_url": "https://api.github.com/users/bors/following{/other_user}", "gists_url": "https://api.github.com/users/bors/gists{/gist_id}", "starred_url": "https://api.github.com/users/bors/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bors/subscriptions", "organizations_url": "https://api.github.com/users/bors/orgs", "repos_url": "https://api.github.com/users/bors/repos", "events_url": "https://api.github.com/users/bors/events{/privacy}", "received_events_url": "https://api.github.com/users/bors/received_events", "type": "User", "site_admin": false}, "committer": {"login": "bors", "id": 3372342, "node_id": "MDQ6VXNlcjMzNzIzNDI=", "avatar_url": "https://avatars.githubusercontent.com/u/3372342?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bors", "html_url": "https://github.com/bors", "followers_url": "https://api.github.com/users/bors/followers", "following_url": "https://api.github.com/users/bors/following{/other_user}", "gists_url": "https://api.github.com/users/bors/gists{/gist_id}", "starred_url": "https://api.github.com/users/bors/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bors/subscriptions", "organizations_url": "https://api.github.com/users/bors/orgs", "repos_url": "https://api.github.com/users/bors/repos", "events_url": "https://api.github.com/users/bors/events{/privacy}", "received_events_url": "https://api.github.com/users/bors/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "260d74dfcc095d02add8fc97c31922824ddf92fd", "url": "https://api.github.com/repos/rust-lang/rust/commits/260d74dfcc095d02add8fc97c31922824ddf92fd", "html_url": "https://github.com/rust-lang/rust/commit/260d74dfcc095d02add8fc97c31922824ddf92fd"}, {"sha": "f2e47cddf835af49a925d91639d7fefb8c23d08f", "url": "https://api.github.com/repos/rust-lang/rust/commits/f2e47cddf835af49a925d91639d7fefb8c23d08f", "html_url": "https://github.com/rust-lang/rust/commit/f2e47cddf835af49a925d91639d7fefb8c23d08f"}], "stats": {"total": 157, "additions": 123, "deletions": 34}, "files": [{"sha": "809a4a591ac8342ff381acc6c3b386644cffae05", "filename": "src/librustc/driver/driver.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/6153aae809387bf5d8e99eda9d2a3c86e80d1b2d/src%2Flibrustc%2Fdriver%2Fdriver.rs", "raw_url": "https://github.com/rust-lang/rust/raw/6153aae809387bf5d8e99eda9d2a3c86e80d1b2d/src%2Flibrustc%2Fdriver%2Fdriver.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Fdriver%2Fdriver.rs?ref=6153aae809387bf5d8e99eda9d2a3c86e80d1b2d", "patch": "@@ -151,7 +151,7 @@ pub fn parse_input(sess: Session, +cfg: ast::crate_cfg, input: input)\n     -> @ast::crate {\n     match input {\n       file_input(ref file) => {\n-        parse::parse_crate_from_file(&(*file), cfg, sess.parse_sess)\n+        parse::parse_crate_from_file_using_tts(&(*file), cfg, sess.parse_sess)\n       }\n       str_input(ref src) => {\n         // FIXME (#2319): Don't really want to box the source string"}, {"sha": "f0e1273534ab18ea82d218745c49dbb27d3cfe8d", "filename": "src/libsyntax/ext/tt/transcribe.rs", "status": "modified", "additions": 29, "deletions": 28, "changes": 57, "blob_url": "https://github.com/rust-lang/rust/blob/6153aae809387bf5d8e99eda9d2a3c86e80d1b2d/src%2Flibsyntax%2Fext%2Ftt%2Ftranscribe.rs", "raw_url": "https://github.com/rust-lang/rust/raw/6153aae809387bf5d8e99eda9d2a3c86e80d1b2d/src%2Flibsyntax%2Fext%2Ftt%2Ftranscribe.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fext%2Ftt%2Ftranscribe.rs?ref=6153aae809387bf5d8e99eda9d2a3c86e80d1b2d", "patch": "@@ -26,7 +26,7 @@ use core::vec;\n    `~` */\n ///an unzipping of `token_tree`s\n struct TtFrame {\n-    readme: @mut ~[ast::token_tree],\n+    forest: @mut ~[ast::token_tree],\n     idx: uint,\n     dotdotdoted: bool,\n     sep: Option<Token>,\n@@ -37,7 +37,7 @@ pub struct TtReader {\n     sp_diag: @span_handler,\n     interner: @ident_interner,\n     // the unzipped tree:\n-    cur: @mut TtFrame,\n+    stack: @mut TtFrame,\n     /* for MBE-style macro transcription */\n     interpolations: LinearMap<ident, @named_match>,\n     repeat_idx: ~[uint],\n@@ -58,8 +58,8 @@ pub fn new_tt_reader(sp_diag: @span_handler,\n     let r = @mut TtReader {\n         sp_diag: sp_diag,\n         interner: itr,\n-        cur: @mut TtFrame {\n-            readme: @mut src,\n+        stack: @mut TtFrame {\n+            forest: @mut src,\n             idx: 0u,\n             dotdotdoted: false,\n             sep: None,\n@@ -81,7 +81,7 @@ pub fn new_tt_reader(sp_diag: @span_handler,\n \n fn dup_tt_frame(f: @mut TtFrame) -> @mut TtFrame {\n     @mut TtFrame {\n-        readme: @mut (copy *f.readme),\n+        forest: @mut (copy *f.forest),\n         idx: f.idx,\n         dotdotdoted: f.dotdotdoted,\n         sep: copy f.sep,\n@@ -96,7 +96,7 @@ pub fn dup_tt_reader(r: @mut TtReader) -> @mut TtReader {\n     @mut TtReader {\n         sp_diag: r.sp_diag,\n         interner: r.interner,\n-        cur: dup_tt_frame(r.cur),\n+        stack: dup_tt_frame(r.stack),\n         interpolations: r.interpolations,\n         repeat_idx: copy r.repeat_idx,\n         repeat_len: copy r.repeat_len,\n@@ -167,45 +167,46 @@ fn lockstep_iter_size(t: token_tree, r: &mut TtReader) -> lis {\n     }\n }\n \n-\n+// return the next token from the TtReader.\n+// EFFECT: advances the reader's token field\n pub fn tt_next_token(r: &mut TtReader) -> TokenAndSpan {\n     let ret_val = TokenAndSpan {\n         tok: copy r.cur_tok,\n         sp: r.cur_span,\n     };\n     loop {\n         {\n-            let cur = &mut *r.cur;\n-            let readme = &mut *cur.readme;\n-            if cur.idx < readme.len() {\n+            let stack = &mut *r.stack;\n+            let forest = &mut *stack.forest;\n+            if stack.idx < forest.len() {\n                 break;\n             }\n         }\n \n         /* done with this set; pop or repeat? */\n-        if ! r.cur.dotdotdoted\n+        if ! r.stack.dotdotdoted\n             || { *r.repeat_idx.last() == *r.repeat_len.last() - 1 } {\n \n-            match r.cur.up {\n+            match r.stack.up {\n               None => {\n                 r.cur_tok = EOF;\n                 return ret_val;\n               }\n               Some(tt_f) => {\n-                if r.cur.dotdotdoted {\n+                if r.stack.dotdotdoted {\n                     r.repeat_idx.pop();\n                     r.repeat_len.pop();\n                 }\n \n-                r.cur = tt_f;\n-                r.cur.idx += 1u;\n+                r.stack = tt_f;\n+                r.stack.idx += 1u;\n               }\n             }\n \n         } else { /* repeat */\n-            r.cur.idx = 0u;\n+            r.stack.idx = 0u;\n             r.repeat_idx[r.repeat_idx.len() - 1u] += 1u;\n-            match r.cur.sep {\n+            match r.stack.sep {\n               Some(copy tk) => {\n                 r.cur_tok = tk; /* repeat same span, I guess */\n                 return ret_val;\n@@ -216,21 +217,21 @@ pub fn tt_next_token(r: &mut TtReader) -> TokenAndSpan {\n     }\n     loop { /* because it's easiest, this handles `tt_delim` not starting\n     with a `tt_tok`, even though it won't happen */\n-        match r.cur.readme[r.cur.idx] {\n+        match r.stack.forest[r.stack.idx] {\n           tt_delim(copy tts) => {\n-            r.cur = @mut TtFrame {\n-                readme: @mut tts,\n+            r.stack = @mut TtFrame {\n+                forest: @mut tts,\n                 idx: 0u,\n                 dotdotdoted: false,\n                 sep: None,\n-                up: option::Some(r.cur)\n+                up: option::Some(r.stack)\n             };\n             // if this could be 0-length, we'd need to potentially recur here\n           }\n           tt_tok(sp, copy tok) => {\n             r.cur_span = sp;\n             r.cur_tok = tok;\n-            r.cur.idx += 1u;\n+            r.stack.idx += 1u;\n             return ret_val;\n           }\n           tt_seq(sp, copy tts, copy sep, zerok) => {\n@@ -256,17 +257,17 @@ pub fn tt_next_token(r: &mut TtReader) -> TokenAndSpan {\n                                                once\");\n                           }\n \n-                    r.cur.idx += 1u;\n+                    r.stack.idx += 1u;\n                     return tt_next_token(r);\n                 } else {\n                     r.repeat_len.push(len);\n                     r.repeat_idx.push(0u);\n-                    r.cur = @mut TtFrame {\n-                        readme: @mut tts,\n+                    r.stack = @mut TtFrame {\n+                        forest: @mut tts,\n                         idx: 0u,\n                         dotdotdoted: true,\n                         sep: sep,\n-                        up: Some(r.cur)\n+                        up: Some(r.stack)\n                     };\n                 }\n               }\n@@ -280,13 +281,13 @@ pub fn tt_next_token(r: &mut TtReader) -> TokenAndSpan {\n               (b) we actually can, since it's a token. */\n               matched_nonterminal(nt_ident(sn,b)) => {\n                 r.cur_span = sp; r.cur_tok = IDENT(sn,b);\n-                r.cur.idx += 1u;\n+                r.stack.idx += 1u;\n                 return ret_val;\n               }\n               matched_nonterminal(ref other_whole_nt) => {\n                 r.cur_span = sp;\n                 r.cur_tok = INTERPOLATED(copy *other_whole_nt);\n-                r.cur.idx += 1u;\n+                r.stack.idx += 1u;\n                 return ret_val;\n               }\n               matched_seq(*) => {"}, {"sha": "ae7dd8ff96fce3bf311110d21e7a9d8f2e41d2cb", "filename": "src/libsyntax/parse/common.rs", "status": "modified", "additions": 14, "deletions": 1, "changes": 15, "blob_url": "https://github.com/rust-lang/rust/blob/6153aae809387bf5d8e99eda9d2a3c86e80d1b2d/src%2Flibsyntax%2Fparse%2Fcommon.rs", "raw_url": "https://github.com/rust-lang/rust/raw/6153aae809387bf5d8e99eda9d2a3c86e80d1b2d/src%2Flibsyntax%2Fparse%2Fcommon.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Fcommon.rs?ref=6153aae809387bf5d8e99eda9d2a3c86e80d1b2d", "patch": "@@ -159,6 +159,9 @@ pub impl Parser {\n         }\n     }\n \n+    // if the given word is not a keyword, signal an error.\n+    // if the next token is the given keyword, eat it and return\n+    // true. Otherwise, return false.\n     fn eat_keyword(&self, word: &~str) -> bool {\n         self.require_keyword(word);\n         let is_kw = match *self.token {\n@@ -169,6 +172,9 @@ pub impl Parser {\n         is_kw\n     }\n \n+    // if the given word is not a keyword, signal an error.\n+    // if the next token is not the given word, signal an error.\n+    // otherwise, eat it.\n     fn expect_keyword(&self, word: &~str) {\n         self.require_keyword(word);\n         if !self.eat_keyword(word) {\n@@ -182,10 +188,12 @@ pub impl Parser {\n         }\n     }\n \n+    // return true if the given string is a strict keyword\n     fn is_strict_keyword(&self, word: &~str) -> bool {\n         self.strict_keywords.contains(word)\n     }\n \n+    // signal an error if the current token is a strict keyword\n     fn check_strict_keywords(&self) {\n         match *self.token {\n             token::IDENT(_, false) => {\n@@ -196,16 +204,19 @@ pub impl Parser {\n         }\n     }\n \n+    // signal an error if the given string is a strict keyword\n     fn check_strict_keywords_(&self, w: &~str) {\n         if self.is_strict_keyword(w) {\n             self.fatal(fmt!(\"found `%s` in ident position\", *w));\n         }\n     }\n \n+    // return true if this is a reserved keyword\n     fn is_reserved_keyword(&self, word: &~str) -> bool {\n         self.reserved_keywords.contains(word)\n     }\n \n+    // signal an error if the current token is a reserved keyword\n     fn check_reserved_keywords(&self) {\n         match *self.token {\n             token::IDENT(_, false) => {\n@@ -216,14 +227,16 @@ pub impl Parser {\n         }\n     }\n \n+    // signal an error if the given string is a reserved keyword\n     fn check_reserved_keywords_(&self, w: &~str) {\n         if self.is_reserved_keyword(w) {\n             self.fatal(fmt!(\"`%s` is a reserved keyword\", *w));\n         }\n     }\n \n     // expect and consume a GT. if a >> is seen, replace it\n-    // with a single > and continue.\n+    // with a single > and continue. If a GT is not seen,\n+    // signal an error.\n     fn expect_gt(&self) {\n         if *self.token == token::GT {\n             self.bump();"}, {"sha": "5e06ecf60908ee1df222642c7cccac983589ddfb", "filename": "src/libsyntax/parse/lexer.rs", "status": "modified", "additions": 6, "deletions": 1, "changes": 7, "blob_url": "https://github.com/rust-lang/rust/blob/6153aae809387bf5d8e99eda9d2a3c86e80d1b2d/src%2Flibsyntax%2Fparse%2Flexer.rs", "raw_url": "https://github.com/rust-lang/rust/raw/6153aae809387bf5d8e99eda9d2a3c86e80d1b2d/src%2Flibsyntax%2Fparse%2Flexer.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Flexer.rs?ref=6153aae809387bf5d8e99eda9d2a3c86e80d1b2d", "patch": "@@ -80,7 +80,8 @@ pub fn new_low_level_string_reader(span_diagnostic: @span_handler,\n         last_pos: filemap.start_pos,\n         col: CharPos(0),\n         curr: initial_char,\n-        filemap: filemap, interner: itr,\n+        filemap: filemap,\n+        interner: itr,\n         /* dummy values; not read */\n         peek_tok: token::EOF,\n         peek_span: codemap::dummy_sp()\n@@ -150,6 +151,7 @@ impl reader for TtReader {\n }\n \n // EFFECT: advance peek_tok and peek_span to refer to the next token.\n+// EFFECT: update the interner, maybe.\n fn string_advance_token(r: @mut StringReader) {\n     match (consume_whitespace_and_comments(r)) {\n         Some(comment) => {\n@@ -539,6 +541,9 @@ fn ident_continue(c: char) -> bool {\n         || (c > 'z' && char::is_XID_continue(c))\n }\n \n+// return the next token from the string\n+// EFFECT: advances the input past that token\n+// EFFECT: updates the interner\n fn next_token_inner(rdr: @mut StringReader) -> token::Token {\n     let mut accum_str = ~\"\";\n     let mut c = rdr.curr;"}, {"sha": "9348b72981d9e82e2aba356b4e35c624fbe74ec1", "filename": "src/libsyntax/parse/mod.rs", "status": "modified", "additions": 49, "deletions": 3, "changes": 52, "blob_url": "https://github.com/rust-lang/rust/blob/6153aae809387bf5d8e99eda9d2a3c86e80d1b2d/src%2Flibsyntax%2Fparse%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/6153aae809387bf5d8e99eda9d2a3c86e80d1b2d/src%2Flibsyntax%2Fparse%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Fmod.rs?ref=6153aae809387bf5d8e99eda9d2a3c86e80d1b2d", "patch": "@@ -45,10 +45,14 @@ pub mod classify;\n /// Reporting obsolete syntax\n pub mod obsolete;\n \n+// info about a parsing session.\n+// This structure and the reader both have\n+// an interner associated with them. If they're\n+// not the same, bad things can happen.\n pub struct ParseSess {\n-    cm: @codemap::CodeMap,\n+    cm: @codemap::CodeMap, // better be the same as the one in the reader!\n     next_id: node_id,\n-    span_diagnostic: @span_handler,\n+    span_diagnostic: @span_handler, // better be the same as the one in the reader!\n     interner: @ident_interner,\n }\n \n@@ -90,6 +94,19 @@ pub fn parse_crate_from_file(\n     // why is there no p.abort_if_errors here?\n }\n \n+pub fn parse_crate_from_file_using_tts(\n+    input: &Path,\n+    cfg: ast::crate_cfg,\n+    sess: @mut ParseSess\n+) -> @ast::crate {\n+    let p = new_parser_from_file(sess, /*bad*/ copy cfg, input);\n+    let tts = p.parse_all_token_trees();\n+    new_parser_from_tts(sess,cfg,tts).parse_crate_mod(/*bad*/ copy cfg)\n+    // why is there no p.abort_if_errors here?\n+}\n+\n+\n+\n pub fn parse_crate_from_source_str(\n     name: ~str,\n     source: @~str,\n@@ -313,17 +330,46 @@ mod test {\n     use std;\n     use core::io;\n     use core::option::None;\n+    use ast;\n \n     #[test] fn to_json_str<E : Encodable<std::json::Encoder>>(val: @E) -> ~str {\n         do io::with_str_writer |writer| {\n             val.encode(~std::json::Encoder(writer));\n         }\n     }\n \n+    fn string_to_crate (source_str : @~str) -> @ast::crate {\n+        parse_crate_from_source_str(\n+            ~\"bogofile\",\n+            source_str,\n+            ~[],\n+            new_parse_sess(None))\n+    }\n+\n+    fn string_to_tt_to_crate (source_str : @~str) -> @ast::crate {\n+        let tts = parse_tts_from_source_str(\n+            ~\"bogofile\",\n+           source_str,\n+           ~[],\n+           new_parse_sess(None));\n+        new_parser_from_tts(new_parse_sess(None),~[],tts)\n+            .parse_crate_mod(~[])\n+    }\n+\n+    // make sure that parsing from TTs produces the same result\n+    // as parsing from strings\n+    #[test] fn tts_produce_the_same_result () {\n+        let source_str = @~\"fn foo (x : int) { x; }\";\n+        assert_eq!(string_to_tt_to_crate(source_str),\n+                     string_to_crate(source_str));\n+    }\n+\n+    // check the contents of the tt manually:\n     #[test] fn alltts () {\n+        let source_str = @~\"fn foo (x : int) { x; }\";\n         let tts = parse_tts_from_source_str(\n             ~\"bogofile\",\n-            @~\"fn foo (x : int) { x; }\",\n+            source_str,\n             ~[],\n             new_parse_sess(None));\n         assert_eq!("}, {"sha": "d068c887abd8e9ea551226641001d400b09f8b8e", "filename": "src/libsyntax/parse/parser.rs", "status": "modified", "additions": 24, "deletions": 0, "changes": 24, "blob_url": "https://github.com/rust-lang/rust/blob/6153aae809387bf5d8e99eda9d2a3c86e80d1b2d/src%2Flibsyntax%2Fparse%2Fparser.rs", "raw_url": "https://github.com/rust-lang/rust/raw/6153aae809387bf5d8e99eda9d2a3c86e80d1b2d/src%2Flibsyntax%2Fparse%2Fparser.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Fparser.rs?ref=6153aae809387bf5d8e99eda9d2a3c86e80d1b2d", "patch": "@@ -248,6 +248,7 @@ pub fn Parser(sess: @mut ParseSess,\n     }\n }\n \n+// ooh, nasty mutable fields everywhere....\n pub struct Parser {\n     sess: @mut ParseSess,\n     cfg: crate_cfg,\n@@ -340,13 +341,15 @@ pub impl Parser {\n         self.sess.interner.get(id)\n     }\n \n+    // is this one of the keywords that signals a closure type?\n     fn token_is_closure_keyword(&self, tok: &token::Token) -> bool {\n         self.token_is_keyword(&~\"pure\", tok) ||\n             self.token_is_keyword(&~\"unsafe\", tok) ||\n             self.token_is_keyword(&~\"once\", tok) ||\n             self.token_is_keyword(&~\"fn\", tok)\n     }\n \n+    // parse a ty_bare_fun type:\n     fn parse_ty_bare_fn(&self) -> ty_\n     {\n         /*\n@@ -376,6 +379,7 @@ pub impl Parser {\n         });\n     }\n \n+    // parse a ty_closure type\n     fn parse_ty_closure(&self,\n                         sigil: ast::Sigil,\n                         region: Option<@ast::Lifetime>) -> ty_\n@@ -434,6 +438,7 @@ pub impl Parser {\n         }\n     }\n \n+    // parse a function type (following the 'fn')\n     fn parse_ty_fn_decl(&self) -> (fn_decl, OptVec<ast::Lifetime>) {\n         /*\n \n@@ -545,12 +550,14 @@ pub impl Parser {\n     }\n \n \n+    // parse a possibly mutable type\n     fn parse_mt(&self) -> mt {\n         let mutbl = self.parse_mutability();\n         let t = self.parse_ty(false);\n         mt { ty: t, mutbl: mutbl }\n     }\n \n+    // parse [mut/const/imm] ID : TY\n     fn parse_ty_field(&self) -> ty_field {\n         let lo = self.span.lo;\n         let mutbl = self.parse_mutability();\n@@ -567,6 +574,7 @@ pub impl Parser {\n         )\n     }\n \n+    // parse optional return type [ -> TY ] in function decl\n     fn parse_ret_ty(&self) -> (ret_style, @Ty) {\n         return if self.eat(&token::RARROW) {\n             let lo = self.span.lo;\n@@ -595,6 +603,7 @@ pub impl Parser {\n         }\n     }\n \n+    // parse a type.\n     // Useless second parameter for compatibility with quasiquote macros.\n     // Bleh!\n     fn parse_ty(&self, _: bool) -> @Ty {\n@@ -631,15 +640,19 @@ pub impl Parser {\n                 t\n             }\n         } else if *self.token == token::AT {\n+            // MANAGED POINTER\n             self.bump();\n             self.parse_box_or_uniq_pointee(ManagedSigil, ty_box)\n         } else if *self.token == token::TILDE {\n+            // OWNED POINTER\n             self.bump();\n             self.parse_box_or_uniq_pointee(OwnedSigil, ty_uniq)\n         } else if *self.token == token::BINOP(token::STAR) {\n+            // STAR POINTER (bare pointer?)\n             self.bump();\n             ty_ptr(self.parse_mt())\n         } else if *self.token == token::LBRACE {\n+            // STRUCTURAL RECORD (remove?)\n             let elems = self.parse_unspanned_seq(\n                 &token::LBRACE,\n                 &token::RBRACE,\n@@ -652,6 +665,7 @@ pub impl Parser {\n             self.obsolete(*self.last_span, ObsoleteRecordType);\n             ty_nil\n         } else if *self.token == token::LBRACKET {\n+            // VECTOR\n             self.expect(&token::LBRACKET);\n             let mt = self.parse_mt();\n             if mt.mutbl == m_mutbl {    // `m_const` too after snapshot\n@@ -667,16 +681,20 @@ pub impl Parser {\n             self.expect(&token::RBRACKET);\n             t\n         } else if *self.token == token::BINOP(token::AND) {\n+            // BORROWED POINTER\n             self.bump();\n             self.parse_borrowed_pointee()\n         } else if self.eat_keyword(&~\"extern\") {\n+            // EXTERN FUNCTION\n             self.parse_ty_bare_fn()\n         } else if self.token_is_closure_keyword(&copy *self.token) {\n+            // CLOSURE\n             let result = self.parse_ty_closure(ast::BorrowedSigil, None);\n             self.obsolete(*self.last_span, ObsoleteBareFnType);\n             result\n         } else if *self.token == token::MOD_SEP\n             || is_ident_or_path(&*self.token) {\n+            // NAMED TYPE\n             let path = self.parse_path_with_tps(false);\n             ty_path(path, self.get_id())\n         } else {\n@@ -885,6 +903,8 @@ pub impl Parser {\n         let global = self.eat(&token::MOD_SEP);\n         let mut ids = ~[];\n         loop {\n+            // if there's a ::< coming, stop processing\n+            // the path.\n             let is_not_last =\n                 self.look_ahead(2u) != token::LT\n                 && self.look_ahead(1u) == token::MOD_SEP;\n@@ -904,6 +924,9 @@ pub impl Parser {\n                      types: ~[] }\n     }\n \n+    // parse a path optionally with type parameters. If 'colons'\n+    // is true, then type parameters must be preceded by colons,\n+    // as in a::t::<t1,t2>\n     fn parse_path_with_tps(&self, colons: bool) -> @ast::path {\n         debug!(\"parse_path_with_tps(colons=%b)\", colons);\n \n@@ -1071,6 +1094,7 @@ pub impl Parser {\n         self.token_is_keyword(&~\"const\", tok)\n     }\n \n+    // parse mutability declaration (mut/const/imm)\n     fn parse_mutability(&self) -> mutability {\n         if self.eat_keyword(&~\"mut\") {\n             m_mutbl"}]}