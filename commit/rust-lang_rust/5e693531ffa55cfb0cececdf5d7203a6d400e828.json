{"sha": "5e693531ffa55cfb0cececdf5d7203a6d400e828", "node_id": "MDY6Q29tbWl0NzI0NzEyOjVlNjkzNTMxZmZhNTVjZmIwY2VjZWNkZjVkNzIwM2E2ZDQwMGU4Mjg=", "commit": {"author": {"name": "Vadim Petrochenkov", "email": "vadim.petrochenkov@gmail.com", "date": "2019-06-05T06:39:34Z"}, "committer": {"name": "Vadim Petrochenkov", "email": "vadim.petrochenkov@gmail.com", "date": "2019-06-06T11:04:02Z"}, "message": "syntax: Add some helper methods to `Token`", "tree": {"sha": "762458c4a3823ed70ae5eb01ca214a1d68858b92", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/762458c4a3823ed70ae5eb01ca214a1d68858b92"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/5e693531ffa55cfb0cececdf5d7203a6d400e828", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/5e693531ffa55cfb0cececdf5d7203a6d400e828", "html_url": "https://github.com/rust-lang/rust/commit/5e693531ffa55cfb0cececdf5d7203a6d400e828", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/5e693531ffa55cfb0cececdf5d7203a6d400e828/comments", "author": {"login": "petrochenkov", "id": 5751617, "node_id": "MDQ6VXNlcjU3NTE2MTc=", "avatar_url": "https://avatars.githubusercontent.com/u/5751617?v=4", "gravatar_id": "", "url": "https://api.github.com/users/petrochenkov", "html_url": "https://github.com/petrochenkov", "followers_url": "https://api.github.com/users/petrochenkov/followers", "following_url": "https://api.github.com/users/petrochenkov/following{/other_user}", "gists_url": "https://api.github.com/users/petrochenkov/gists{/gist_id}", "starred_url": "https://api.github.com/users/petrochenkov/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/petrochenkov/subscriptions", "organizations_url": "https://api.github.com/users/petrochenkov/orgs", "repos_url": "https://api.github.com/users/petrochenkov/repos", "events_url": "https://api.github.com/users/petrochenkov/events{/privacy}", "received_events_url": "https://api.github.com/users/petrochenkov/received_events", "type": "User", "site_admin": false}, "committer": {"login": "petrochenkov", "id": 5751617, "node_id": "MDQ6VXNlcjU3NTE2MTc=", "avatar_url": "https://avatars.githubusercontent.com/u/5751617?v=4", "gravatar_id": "", "url": "https://api.github.com/users/petrochenkov", "html_url": "https://github.com/petrochenkov", "followers_url": "https://api.github.com/users/petrochenkov/followers", "following_url": "https://api.github.com/users/petrochenkov/following{/other_user}", "gists_url": "https://api.github.com/users/petrochenkov/gists{/gist_id}", "starred_url": "https://api.github.com/users/petrochenkov/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/petrochenkov/subscriptions", "organizations_url": "https://api.github.com/users/petrochenkov/orgs", "repos_url": "https://api.github.com/users/petrochenkov/repos", "events_url": "https://api.github.com/users/petrochenkov/events{/privacy}", "received_events_url": "https://api.github.com/users/petrochenkov/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "aa6fba98ae717d6090cdd5d0569114adfc825680", "url": "https://api.github.com/repos/rust-lang/rust/commits/aa6fba98ae717d6090cdd5d0569114adfc825680", "html_url": "https://github.com/rust-lang/rust/commit/aa6fba98ae717d6090cdd5d0569114adfc825680"}], "stats": {"total": 126, "additions": 64, "deletions": 62}, "files": [{"sha": "56afc8728b4cab37f3fd74c91744a03bfc30b0d5", "filename": "src/libsyntax/attr/mod.rs", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/rust-lang/rust/blob/5e693531ffa55cfb0cececdf5d7203a6d400e828/src%2Flibsyntax%2Fattr%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/5e693531ffa55cfb0cececdf5d7203a6d400e828/src%2Flibsyntax%2Fattr%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fattr%2Fmod.rs?ref=5e693531ffa55cfb0cececdf5d7203a6d400e828", "patch": "@@ -604,8 +604,8 @@ impl NestedMetaItem {\n     fn from_tokens<I>(tokens: &mut iter::Peekable<I>) -> Option<NestedMetaItem>\n         where I: Iterator<Item = TokenTree>,\n     {\n-        if let Some(TokenTree::Token(token)) = tokens.peek().cloned() {\n-            if let Ok(lit) = Lit::from_token(&token, token.span) {\n+        if let Some(TokenTree::Token(token)) = tokens.peek() {\n+            if let Ok(lit) = Lit::from_token(token, token.span) {\n                 tokens.next();\n                 return Some(NestedMetaItem::Literal(lit));\n             }"}, {"sha": "7127acabb44e00792e5307464eaca608987658a6", "filename": "src/libsyntax/ext/tt/macro_parser.rs", "status": "modified", "additions": 6, "deletions": 7, "changes": 13, "blob_url": "https://github.com/rust-lang/rust/blob/5e693531ffa55cfb0cececdf5d7203a6d400e828/src%2Flibsyntax%2Fext%2Ftt%2Fmacro_parser.rs", "raw_url": "https://github.com/rust-lang/rust/raw/5e693531ffa55cfb0cececdf5d7203a6d400e828/src%2Flibsyntax%2Fext%2Ftt%2Fmacro_parser.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fext%2Ftt%2Fmacro_parser.rs?ref=5e693531ffa55cfb0cececdf5d7203a6d400e828", "patch": "@@ -727,13 +727,12 @@ pub fn parse(\n                     \"ambiguity: multiple successful parses\".to_string(),\n                 );\n             } else {\n-                let span = if parser.span.is_dummy() {\n-                    parser.span\n-                } else {\n-                    sess.source_map().next_point(parser.span)\n-                };\n                 return Failure(\n-                    Token { kind: token::Eof, span },\n+                    Token::new(token::Eof, if parser.span.is_dummy() {\n+                        parser.span\n+                    } else {\n+                        sess.source_map().next_point(parser.span)\n+                    }),\n                     \"missing tokens in macro arguments\",\n                 );\n             }\n@@ -771,7 +770,7 @@ pub fn parse(\n         // then there is a syntax error.\n         else if bb_items.is_empty() && next_items.is_empty() {\n             return Failure(\n-                parser.token.clone(),\n+                parser.token.take(),\n                 \"no rules expected this token in macro call\",\n             );\n         }"}, {"sha": "558b07af6110cd0f50e24b78fca987cd7e6e685c", "filename": "src/libsyntax/ext/tt/quoted.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/5e693531ffa55cfb0cececdf5d7203a6d400e828/src%2Flibsyntax%2Fext%2Ftt%2Fquoted.rs", "raw_url": "https://github.com/rust-lang/rust/raw/5e693531ffa55cfb0cececdf5d7203a6d400e828/src%2Flibsyntax%2Fext%2Ftt%2Fquoted.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fext%2Ftt%2Fquoted.rs?ref=5e693531ffa55cfb0cececdf5d7203a6d400e828", "patch": "@@ -154,7 +154,7 @@ impl TokenTree {\n     }\n \n     crate fn token(span: Span, kind: TokenKind) -> TokenTree {\n-        TokenTree::Token(Token { kind, span })\n+        TokenTree::Token(Token::new(kind, span))\n     }\n }\n "}, {"sha": "47428c9a14ce7ef49f3c017c4587667b45e9ea28", "filename": "src/libsyntax/parse/lexer/mod.rs", "status": "modified", "additions": 24, "deletions": 37, "changes": 61, "blob_url": "https://github.com/rust-lang/rust/blob/5e693531ffa55cfb0cececdf5d7203a6d400e828/src%2Flibsyntax%2Fparse%2Flexer%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/5e693531ffa55cfb0cececdf5d7203a6d400e828/src%2Flibsyntax%2Fparse%2Flexer%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Flexer%2Fmod.rs?ref=5e693531ffa55cfb0cececdf5d7203a6d400e828", "patch": "@@ -88,7 +88,7 @@ impl<'a> StringReader<'a> {\n     /// Returns the next token. EFFECT: advances the string_reader.\n     pub fn try_next_token(&mut self) -> Result<Token, ()> {\n         assert!(self.fatal_errs.is_empty());\n-        let ret_val = self.peek_token.clone();\n+        let ret_val = self.peek_token.take();\n         self.advance_token()?;\n         Ok(ret_val)\n     }\n@@ -205,8 +205,7 @@ impl<'a> StringReader<'a> {\n             ch: Some('\\n'),\n             source_file,\n             end_src_index: src.len(),\n-            // dummy values; not read\n-            peek_token: Token { kind: token::Eof, span: syntax_pos::DUMMY_SP },\n+            peek_token: Token::dummy(),\n             peek_span_src_raw: syntax_pos::DUMMY_SP,\n             src,\n             fatal_errs: Vec::new(),\n@@ -320,21 +319,15 @@ impl<'a> StringReader<'a> {\n                 self.peek_token = comment;\n             }\n             None => {\n-                if self.is_eof() {\n-\n-                    let (real, raw) = self.mk_sp_and_raw(\n-                        self.source_file.end_pos,\n-                        self.source_file.end_pos,\n-                    );\n-                    self.peek_token = Token { kind: token::Eof, span: real };\n-                    self.peek_span_src_raw = raw;\n+                let (kind, start_pos, end_pos) = if self.is_eof() {\n+                    (token::Eof, self.source_file.end_pos, self.source_file.end_pos)\n                 } else {\n-                    let start_bytepos = self.pos;\n-                    let kind = self.next_token_inner()?;\n-                    let (real, raw) = self.mk_sp_and_raw(start_bytepos, self.pos);\n-                    self.peek_token = Token { kind, span: real };\n-                    self.peek_span_src_raw = raw;\n+                    let start_pos = self.pos;\n+                    (self.next_token_inner()?, start_pos, self.pos)\n                 };\n+                let (real, raw) = self.mk_sp_and_raw(start_pos, end_pos);\n+                self.peek_token = Token::new(kind, real);\n+                self.peek_span_src_raw = raw;\n             }\n         }\n \n@@ -544,7 +537,7 @@ impl<'a> StringReader<'a> {\n                     } else {\n                         token::Comment\n                     };\n-                    Some(Token { kind, span: self.mk_sp(start_bpos, self.pos) })\n+                    Some(Token::new(kind, self.mk_sp(start_bpos, self.pos)))\n                 }\n                 Some('*') => {\n                     self.bump();\n@@ -568,10 +561,10 @@ impl<'a> StringReader<'a> {\n                     while !self.ch_is('\\n') && !self.is_eof() {\n                         self.bump();\n                     }\n-                    return Some(Token {\n-                        kind: token::Shebang(self.name_from(start)),\n-                        span: self.mk_sp(start, self.pos),\n-                    });\n+                    return Some(Token::new(\n+                        token::Shebang(self.name_from(start)),\n+                        self.mk_sp(start, self.pos),\n+                    ));\n                 }\n             }\n             None\n@@ -596,10 +589,7 @@ impl<'a> StringReader<'a> {\n                 while is_pattern_whitespace(self.ch) {\n                     self.bump();\n                 }\n-                let c = Some(Token {\n-                    kind: token::Whitespace,\n-                    span: self.mk_sp(start_bpos, self.pos),\n-                });\n+                let c = Some(Token::new(token::Whitespace, self.mk_sp(start_bpos, self.pos)));\n                 debug!(\"scanning whitespace: {:?}\", c);\n                 c\n             }\n@@ -658,10 +648,7 @@ impl<'a> StringReader<'a> {\n                 token::Comment\n             };\n \n-            Some(Token {\n-                kind,\n-                span: self.mk_sp(start_bpos, self.pos),\n-            })\n+            Some(Token::new(kind, self.mk_sp(start_bpos, self.pos)))\n         })\n     }\n \n@@ -1588,21 +1575,21 @@ mod tests {\n             assert_eq!(string_reader.next_token(), token::Comment);\n             assert_eq!(string_reader.next_token(), token::Whitespace);\n             let tok1 = string_reader.next_token();\n-            let tok2 = Token {\n-                kind: token::Ident(id, false),\n-                span: Span::new(BytePos(21), BytePos(23), NO_EXPANSION),\n-            };\n+            let tok2 = Token::new(\n+                token::Ident(id, false),\n+                Span::new(BytePos(21), BytePos(23), NO_EXPANSION),\n+            );\n             assert_eq!(tok1.kind, tok2.kind);\n             assert_eq!(tok1.span, tok2.span);\n             assert_eq!(string_reader.next_token(), token::Whitespace);\n             // the 'main' id is already read:\n             assert_eq!(string_reader.pos.clone(), BytePos(28));\n             // read another token:\n             let tok3 = string_reader.next_token();\n-            let tok4 = Token {\n-                kind: mk_ident(\"main\"),\n-                span: Span::new(BytePos(24), BytePos(28), NO_EXPANSION),\n-            };\n+            let tok4 = Token::new(\n+                mk_ident(\"main\"),\n+                Span::new(BytePos(24), BytePos(28), NO_EXPANSION),\n+            );\n             assert_eq!(tok3.kind, tok4.kind);\n             assert_eq!(tok3.span, tok4.span);\n             // the lparen is already read:"}, {"sha": "b809f99beba339dcb7e3fab2cf3875670c61acc1", "filename": "src/libsyntax/parse/lexer/tokentrees.rs", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/rust-lang/rust/blob/5e693531ffa55cfb0cececdf5d7203a6d400e828/src%2Flibsyntax%2Fparse%2Flexer%2Ftokentrees.rs", "raw_url": "https://github.com/rust-lang/rust/raw/5e693531ffa55cfb0cececdf5d7203a6d400e828/src%2Flibsyntax%2Fparse%2Flexer%2Ftokentrees.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Flexer%2Ftokentrees.rs?ref=5e693531ffa55cfb0cececdf5d7203a6d400e828", "patch": "@@ -10,7 +10,7 @@ impl<'a> StringReader<'a> {\n     crate fn into_token_trees(self) -> (PResult<'a, TokenStream>, Vec<UnmatchedBrace>) {\n         let mut tt_reader = TokenTreesReader {\n             string_reader: self,\n-            token: token::Token { kind: token::Eof, span: syntax_pos::DUMMY_SP },\n+            token: Token::dummy(),\n             open_braces: Vec::new(),\n             unmatched_braces: Vec::new(),\n             matching_delim_spans: Vec::new(),\n@@ -202,7 +202,7 @@ impl<'a> TokenTreesReader<'a> {\n                 Err(err)\n             },\n             _ => {\n-                let tt = TokenTree::Token(self.token.clone());\n+                let tt = TokenTree::Token(self.token.take());\n                 // Note that testing for joint-ness here is done via the raw\n                 // source span as the joint-ness is a property of the raw source\n                 // rather than wanting to take `override_span` into account."}, {"sha": "978fd205ea489b7cc514d19525c3e42731e3181e", "filename": "src/libsyntax/parse/literal.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/5e693531ffa55cfb0cececdf5d7203a6d400e828/src%2Flibsyntax%2Fparse%2Fliteral.rs", "raw_url": "https://github.com/rust-lang/rust/raw/5e693531ffa55cfb0cececdf5d7203a6d400e828/src%2Flibsyntax%2Fparse%2Fliteral.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Fliteral.rs?ref=5e693531ffa55cfb0cececdf5d7203a6d400e828", "patch": "@@ -277,7 +277,7 @@ impl<'a> Parser<'a> {\n                     if self.span.hi() == next_span.lo() {\n                         let s = String::from(\"0.\") + &symbol.as_str();\n                         let kind = TokenKind::lit(token::Float, Symbol::intern(&s), suffix);\n-                        return Some(Token { kind, span: self.span.to(next_span) });\n+                        return Some(Token::new(kind, self.span.to(next_span)));\n                     }\n                 }\n                 None"}, {"sha": "7dd92f022e1f8fbed8bf41b0d937de7a7f3019cb", "filename": "src/libsyntax/parse/parser.rs", "status": "modified", "additions": 10, "deletions": 10, "changes": 20, "blob_url": "https://github.com/rust-lang/rust/blob/5e693531ffa55cfb0cececdf5d7203a6d400e828/src%2Flibsyntax%2Fparse%2Fparser.rs", "raw_url": "https://github.com/rust-lang/rust/raw/5e693531ffa55cfb0cececdf5d7203a6d400e828/src%2Flibsyntax%2Fparse%2Fparser.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Fparser.rs?ref=5e693531ffa55cfb0cececdf5d7203a6d400e828", "patch": "@@ -318,7 +318,7 @@ impl TokenCursor {\n                 self.frame = frame;\n                 continue\n             } else {\n-                return Token { kind: token::Eof, span: DUMMY_SP }\n+                return Token::new(token::Eof, DUMMY_SP);\n             };\n \n             match self.frame.last_token {\n@@ -477,7 +477,7 @@ impl<'a> Parser<'a> {\n     ) -> Self {\n         let mut parser = Parser {\n             sess,\n-            token: Token { kind: token::Whitespace, span: DUMMY_SP },\n+            token: Token::dummy(),\n             prev_span: DUMMY_SP,\n             meta_var_span: None,\n             prev_token_kind: PrevTokenKind::Other,\n@@ -1042,12 +1042,12 @@ impl<'a> Parser<'a> {\n         // fortunately for tokens currently using `bump_with`, the\n         // prev_token_kind will be of no use anyway.\n         self.prev_token_kind = PrevTokenKind::Other;\n-        self.token = Token { kind: next, span };\n+        self.token = Token::new(next, span);\n         self.expected_tokens.clear();\n     }\n \n     pub fn look_ahead<R, F>(&self, dist: usize, f: F) -> R where\n-        F: FnOnce(&token::Token) -> R,\n+        F: FnOnce(&Token) -> R,\n     {\n         if dist == 0 {\n             // FIXME: Avoid cloning here.\n@@ -1058,9 +1058,9 @@ impl<'a> Parser<'a> {\n         f(&match frame.tree_cursor.look_ahead(dist - 1) {\n             Some(tree) => match tree {\n                 TokenTree::Token(token) => token,\n-                TokenTree::Delimited(dspan, delim, _) => Token { kind: token::OpenDelim(delim), span: dspan.open },\n+                TokenTree::Delimited(dspan, delim, _) => Token::new(token::OpenDelim(delim), dspan.open),\n             }\n-            None => Token { kind: token::CloseDelim(frame.delim), span: frame.span.close }\n+            None => Token::new(token::CloseDelim(frame.delim), frame.span.close)\n         })\n     }\n \n@@ -2651,8 +2651,8 @@ impl<'a> Parser<'a> {\n                 // Interpolated identifier and lifetime tokens are replaced with usual identifier\n                 // and lifetime tokens, so the former are never encountered during normal parsing.\n                 match **nt {\n-                    token::NtIdent(ident, is_raw) => Token { kind: token::Ident(ident, is_raw), span: ident.span },\n-                    token::NtLifetime(ident) => Token { kind: token::Lifetime(ident), span: ident.span },\n+                    token::NtIdent(ident, is_raw) => Token::new(token::Ident(ident, is_raw), ident.span),\n+                    token::NtLifetime(ident) => Token::new(token::Lifetime(ident), ident.span),\n                     _ => return,\n                 }\n             }\n@@ -2676,7 +2676,7 @@ impl<'a> Parser<'a> {\n             },\n             token::CloseDelim(_) | token::Eof => unreachable!(),\n             _ => {\n-                let token = mem::replace(&mut self.token, Token { kind: token::Whitespace, span: DUMMY_SP });\n+                let token = self.token.take();\n                 self.bump();\n                 TokenTree::Token(token)\n             }\n@@ -2763,7 +2763,7 @@ impl<'a> Parser<'a> {\n                 // `not` is just an ordinary identifier in Rust-the-language,\n                 // but as `rustc`-the-compiler, we can issue clever diagnostics\n                 // for confused users who really want to say `!`\n-                let token_cannot_continue_expr = |t: &token::Token| match t.kind {\n+                let token_cannot_continue_expr = |t: &Token| match t.kind {\n                     // These tokens can start an expression after `!`, but\n                     // can't continue an expression after an ident\n                     token::Ident(ident, is_raw) => token::ident_can_begin_expr(ident, is_raw),"}, {"sha": "559e0524a4bf110f4afb7d802c4f950d5509688b", "filename": "src/libsyntax/parse/token.rs", "status": "modified", "additions": 17, "deletions": 1, "changes": 18, "blob_url": "https://github.com/rust-lang/rust/blob/5e693531ffa55cfb0cececdf5d7203a6d400e828/src%2Flibsyntax%2Fparse%2Ftoken.rs", "raw_url": "https://github.com/rust-lang/rust/raw/5e693531ffa55cfb0cececdf5d7203a6d400e828/src%2Flibsyntax%2Fparse%2Ftoken.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Ftoken.rs?ref=5e693531ffa55cfb0cececdf5d7203a6d400e828", "patch": "@@ -13,7 +13,7 @@ use crate::syntax::parse::parse_stream_from_source_str;\n use crate::tokenstream::{self, DelimSpan, TokenStream, TokenTree};\n \n use syntax_pos::symbol::{self, Symbol};\n-use syntax_pos::{self, Span, FileName};\n+use syntax_pos::{self, Span, FileName, DUMMY_SP};\n use log::info;\n \n use std::fmt;\n@@ -609,6 +609,22 @@ impl TokenKind {\n     }\n }\n \n+impl Token {\n+    crate fn new(kind: TokenKind, span: Span) -> Self {\n+        Token { kind, span }\n+    }\n+\n+    /// Some token that will be thrown away later.\n+    crate fn dummy() -> Self {\n+        Token::new(TokenKind::Whitespace, DUMMY_SP)\n+    }\n+\n+    /// Return this token by value and leave a dummy token in its place.\n+    crate fn take(&mut self) -> Self {\n+        mem::replace(self, Token::dummy())\n+    }\n+}\n+\n impl PartialEq<TokenKind> for Token {\n     fn eq(&self, rhs: &TokenKind) -> bool {\n         self.kind == *rhs"}, {"sha": "140b77b6b5f91b0da8821a4c003670c7eb3083a1", "filename": "src/libsyntax/tokenstream.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/5e693531ffa55cfb0cececdf5d7203a6d400e828/src%2Flibsyntax%2Ftokenstream.rs", "raw_url": "https://github.com/rust-lang/rust/raw/5e693531ffa55cfb0cececdf5d7203a6d400e828/src%2Flibsyntax%2Ftokenstream.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Ftokenstream.rs?ref=5e693531ffa55cfb0cececdf5d7203a6d400e828", "patch": "@@ -139,7 +139,7 @@ impl TokenTree {\n     }\n \n     pub fn token(span: Span, kind: TokenKind) -> TokenTree {\n-        TokenTree::Token(Token { kind, span })\n+        TokenTree::Token(Token::new(kind, span))\n     }\n \n     /// Returns the opening delimiter as a token tree."}]}