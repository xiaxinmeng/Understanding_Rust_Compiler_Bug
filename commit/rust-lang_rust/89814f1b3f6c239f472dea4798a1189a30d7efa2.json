{"sha": "89814f1b3f6c239f472dea4798a1189a30d7efa2", "node_id": "MDY6Q29tbWl0NzI0NzEyOjg5ODE0ZjFiM2Y2YzIzOWY0NzJkZWE0Nzk4YTExODlhMzBkN2VmYTI=", "commit": {"author": {"name": "JCTyBlaidd", "email": "JCTyblaidd@users.noreply.github.com", "date": "2020-11-02T00:23:27Z"}, "committer": {"name": "JCTyBlaidd", "email": "JCTyblaidd@users.noreply.github.com", "date": "2020-11-02T02:57:05Z"}, "message": "Initial data-race detector,\npasses all current tests but additional tests are required", "tree": {"sha": "d878f4a262231e8a0c6e0643d07957bd17d9cdb3", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/d878f4a262231e8a0c6e0643d07957bd17d9cdb3"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/89814f1b3f6c239f472dea4798a1189a30d7efa2", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/89814f1b3f6c239f472dea4798a1189a30d7efa2", "html_url": "https://github.com/rust-lang/rust/commit/89814f1b3f6c239f472dea4798a1189a30d7efa2", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/89814f1b3f6c239f472dea4798a1189a30d7efa2/comments", "author": {"login": "JCTyblaidd", "id": 8288600, "node_id": "MDQ6VXNlcjgyODg2MDA=", "avatar_url": "https://avatars.githubusercontent.com/u/8288600?v=4", "gravatar_id": "", "url": "https://api.github.com/users/JCTyblaidd", "html_url": "https://github.com/JCTyblaidd", "followers_url": "https://api.github.com/users/JCTyblaidd/followers", "following_url": "https://api.github.com/users/JCTyblaidd/following{/other_user}", "gists_url": "https://api.github.com/users/JCTyblaidd/gists{/gist_id}", "starred_url": "https://api.github.com/users/JCTyblaidd/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/JCTyblaidd/subscriptions", "organizations_url": "https://api.github.com/users/JCTyblaidd/orgs", "repos_url": "https://api.github.com/users/JCTyblaidd/repos", "events_url": "https://api.github.com/users/JCTyblaidd/events{/privacy}", "received_events_url": "https://api.github.com/users/JCTyblaidd/received_events", "type": "User", "site_admin": false}, "committer": {"login": "JCTyblaidd", "id": 8288600, "node_id": "MDQ6VXNlcjgyODg2MDA=", "avatar_url": "https://avatars.githubusercontent.com/u/8288600?v=4", "gravatar_id": "", "url": "https://api.github.com/users/JCTyblaidd", "html_url": "https://github.com/JCTyblaidd", "followers_url": "https://api.github.com/users/JCTyblaidd/followers", "following_url": "https://api.github.com/users/JCTyblaidd/following{/other_user}", "gists_url": "https://api.github.com/users/JCTyblaidd/gists{/gist_id}", "starred_url": "https://api.github.com/users/JCTyblaidd/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/JCTyblaidd/subscriptions", "organizations_url": "https://api.github.com/users/JCTyblaidd/orgs", "repos_url": "https://api.github.com/users/JCTyblaidd/repos", "events_url": "https://api.github.com/users/JCTyblaidd/events{/privacy}", "received_events_url": "https://api.github.com/users/JCTyblaidd/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "9a2cfbfc0a6029944fa87372da6870e147961187", "url": "https://api.github.com/repos/rust-lang/rust/commits/9a2cfbfc0a6029944fa87372da6870e147961187", "html_url": "https://github.com/rust-lang/rust/commit/9a2cfbfc0a6029944fa87372da6870e147961187"}], "stats": {"total": 1982, "additions": 1802, "deletions": 180}, "files": [{"sha": "78838acb2a5fc6b2ded43fcc6c33b0bcb973386f", "filename": "Cargo.lock", "status": "modified", "additions": 7, "deletions": 0, "changes": 7, "blob_url": "https://github.com/rust-lang/rust/blob/89814f1b3f6c239f472dea4798a1189a30d7efa2/Cargo.lock", "raw_url": "https://github.com/rust-lang/rust/raw/89814f1b3f6c239f472dea4798a1189a30d7efa2/Cargo.lock", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/Cargo.lock?ref=89814f1b3f6c239f472dea4798a1189a30d7efa2", "patch": "@@ -282,6 +282,7 @@ dependencies = [\n  \"rustc-workspace-hack\",\n  \"rustc_version\",\n  \"shell-escape\",\n+ \"smallvec\",\n ]\n \n [[package]]\n@@ -496,6 +497,12 @@ version = \"0.1.5\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n checksum = \"45bb67a18fa91266cc7807181f62f9178a6873bfad7dc788c42e6430db40184f\"\n \n+[[package]]\n+name = \"smallvec\"\n+version = \"1.4.2\"\n+source = \"registry+https://github.com/rust-lang/crates.io-index\"\n+checksum = \"fbee7696b84bbf3d89a1c2eccff0850e3047ed46bfcd2e92c29a2d074d57e252\"\n+\n [[package]]\n name = \"socket2\"\n version = \"0.3.15\""}, {"sha": "4413dab321e72e20557d2ffee6bfdf2316176c58", "filename": "Cargo.toml", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "blob_url": "https://github.com/rust-lang/rust/blob/89814f1b3f6c239f472dea4798a1189a30d7efa2/Cargo.toml", "raw_url": "https://github.com/rust-lang/rust/raw/89814f1b3f6c239f472dea4798a1189a30d7efa2/Cargo.toml", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/Cargo.toml?ref=89814f1b3f6c239f472dea4798a1189a30d7efa2", "patch": "@@ -30,6 +30,7 @@ log = \"0.4\"\n shell-escape = \"0.1.4\"\n hex = \"0.4.0\"\n rand = \"0.7\"\n+smallvec = \"1.4.2\"\n \n # A noop dependency that changes in the Rust repository, it's a bit of a hack.\n # See the `src/tools/rustc-workspace-hack/README.md` file in `rust-lang/rust`"}, {"sha": "59526063945a9267b0a159966d9f6660c2b5d142", "filename": "src/data_race.rs", "status": "added", "additions": 1406, "deletions": 0, "changes": 1406, "blob_url": "https://github.com/rust-lang/rust/blob/89814f1b3f6c239f472dea4798a1189a30d7efa2/src%2Fdata_race.rs", "raw_url": "https://github.com/rust-lang/rust/raw/89814f1b3f6c239f472dea4798a1189a30d7efa2/src%2Fdata_race.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Fdata_race.rs?ref=89814f1b3f6c239f472dea4798a1189a30d7efa2", "patch": "@@ -0,0 +1,1406 @@\n+//! Implementation of a data-race detector\n+//!  uses Lamport Timestamps / Vector-clocks\n+//!  base on the Dyamic Race Detection for C++:\n+//!     - https://www.doc.ic.ac.uk/~afd/homepages/papers/pdfs/2017/POPL.pdf\n+//!  to extend data-race detection to work correctly with fences\n+//!  and RMW operations\n+//! This does not explore weak memory orders and so can still miss data-races\n+//!  but should not report false-positives\n+\n+use std::{fmt::{self, Debug}, cmp::Ordering, rc::Rc, cell::{Cell, RefCell, Ref, RefMut}, ops::Index};\n+\n+use rustc_index::vec::{Idx, IndexVec};\n+use rustc_target::abi::Size;\n+use rustc_middle::ty::layout::TyAndLayout;\n+use rustc_data_structures::fx::FxHashMap;\n+\n+use smallvec::SmallVec;\n+\n+use crate::*;\n+\n+pub type AllocExtra = VClockAlloc;\n+pub type MemoryExtra = Rc<GlobalState>;\n+\n+/// Valid atomic read-write operations, alias of atomic::Ordering (not non-exhaustive)\n+#[derive(Copy, Clone, PartialEq, Eq, Debug)]\n+pub enum AtomicRWOp {\n+    Relaxed,\n+    Acquire,\n+    Release,\n+    AcqRel,\n+    SeqCst,\n+}\n+\n+/// Valid atomic read operations, subset of atomic::Ordering\n+#[derive(Copy, Clone, PartialEq, Eq, Debug)]\n+pub enum AtomicReadOp {\n+    Relaxed,\n+    Acquire,\n+    SeqCst,\n+}\n+\n+/// Valid atomic write operations, subset of atomic::Ordering\n+#[derive(Copy, Clone, PartialEq, Eq, Debug)]\n+pub enum AtomicWriteOp {\n+    Relaxed,\n+    Release,\n+    SeqCst,\n+}\n+\n+\n+/// Valid atomic fence operations, subset of atomic::Ordering\n+#[derive(Copy, Clone, PartialEq, Eq, Debug)]\n+pub enum AtomicFenceOp {\n+    Acquire,\n+    Release,\n+    AcqRel,\n+    SeqCst,\n+}\n+\n+/// Evaluation context extensions\n+impl<'mir, 'tcx: 'mir> EvalContextExt<'mir, 'tcx> for crate::MiriEvalContext<'mir, 'tcx> {}\n+pub trait EvalContextExt<'mir, 'tcx: 'mir>: crate::MiriEvalContextExt<'mir, 'tcx> {\n+\n+    /// Variant of `read_immediate` that does not perform `data-race` checks.\n+    fn read_immediate_racy(&self, op: MPlaceTy<'tcx, Tag>) -> InterpResult<'tcx, ImmTy<'tcx, Tag>> {\n+        let this = self.eval_context_ref();\n+        let data_race = &*this.memory.extra.data_race;\n+        let old = data_race.multi_threaded.get();\n+\n+        data_race.multi_threaded.set(false);\n+        let res = this.read_immediate(op.into());\n+\n+        data_race.multi_threaded.set(old);\n+        res\n+    }\n+    \n+    /// Variant of `write_immediate` that does not perform `data-race` checks.\n+    fn write_immediate_racy(\n+        &mut self, src: Immediate<Tag>, dest: MPlaceTy<'tcx, Tag>\n+    ) -> InterpResult<'tcx> {\n+        let this = self.eval_context_mut();\n+        let data_race = &*this.memory.extra.data_race;\n+        let old = data_race.multi_threaded.get();\n+\n+        data_race.multi_threaded.set(false);\n+        let imm = this.write_immediate(src, dest.into());\n+\n+        let data_race = &*this.memory.extra.data_race;\n+        data_race.multi_threaded.set(old);\n+        imm\n+    }\n+\n+    /// Variant of `read_scalar` that does not perform data-race checks.\n+    fn read_scalar_racy(\n+        &self, op: MPlaceTy<'tcx, Tag>\n+    )-> InterpResult<'tcx, ScalarMaybeUninit<Tag>> {\n+        Ok(self.read_immediate_racy(op)?.to_scalar_or_uninit())\n+    }\n+\n+    /// Variant of `write_scalar` that does not perform data-race checks.\n+    fn write_scalar_racy(\n+        &mut self, val: ScalarMaybeUninit<Tag>, dest: MPlaceTy<'tcx, Tag>\n+    ) -> InterpResult<'tcx> {\n+        self.write_immediate_racy(Immediate::Scalar(val.into()), dest)\n+    }\n+\n+    /// Variant of `read_scalar_at_offset` helper function that does not perform\n+    /// `data-race checks.\n+    fn read_scalar_at_offset_racy(\n+        &self,\n+        op: OpTy<'tcx, Tag>,\n+        offset: u64,\n+        layout: TyAndLayout<'tcx>,\n+    ) -> InterpResult<'tcx, ScalarMaybeUninit<Tag>> {\n+        let this = self.eval_context_ref();\n+        let op_place = this.deref_operand(op)?;\n+        let offset = Size::from_bytes(offset);\n+        // Ensure that the following read at an offset is within bounds\n+        assert!(op_place.layout.size >= offset + layout.size);\n+        let value_place = op_place.offset(offset, MemPlaceMeta::None, layout, this)?;\n+        this.read_scalar_racy(value_place.into())\n+    }\n+\n+    /// Variant of `write_scalar_at_offfset` helper function that does not perform\n+    ///  data-race checks.\n+    fn write_scalar_at_offset_racy(\n+        &mut self,\n+        op: OpTy<'tcx, Tag>,\n+        offset: u64,\n+        value: impl Into<ScalarMaybeUninit<Tag>>,\n+        layout: TyAndLayout<'tcx>,\n+    ) -> InterpResult<'tcx, ()> {\n+        let this = self.eval_context_mut();\n+        let op_place = this.deref_operand(op)?;\n+        let offset = Size::from_bytes(offset);\n+        // Ensure that the following read at an offset is within bounds\n+        assert!(op_place.layout.size >= offset + layout.size);\n+        let value_place = op_place.offset(offset, MemPlaceMeta::None, layout, this)?;\n+        this.write_scalar_racy(value.into(), value_place.into())\n+    }\n+\n+    /// Load the data race allocation state for a given memory place\n+    ///  also returns the size and the offset of the result in the allocation\n+    ///  metadata\n+    fn load_data_race_state<'a>(\n+        &'a mut self, place: MPlaceTy<'tcx, Tag>\n+    ) -> InterpResult<'tcx, (&'a mut VClockAlloc, Size, Size)> where 'mir: 'a {\n+        let this = self.eval_context_mut();\n+\n+        let ptr = place.ptr.assert_ptr();\n+        let size = place.layout.size;\n+        let data_race = &mut this.memory.get_raw_mut(ptr.alloc_id)?.extra.data_race;\n+\n+        Ok((data_race, size, ptr.offset))\n+    }\n+    \n+    /// Update the data-race detector for an atomic read occuring at the\n+    ///  associated memory-place and on the current thread\n+    fn validate_atomic_load(\n+        &mut self, place: MPlaceTy<'tcx, Tag>, atomic: AtomicReadOp\n+    ) -> InterpResult<'tcx> {\n+        let this = self.eval_context_mut();\n+        let data_race = &*this.memory.extra.data_race;\n+        if data_race.multi_threaded.get() {\n+            data_race.advance_vector_clock();\n+\n+            let (\n+                alloc, size, offset\n+            ) = this.load_data_race_state(place)?;\n+            log::trace!(\n+                \"Atomic load on {:?} with ordering {:?}, in memory({:?}, offset={}, size={})\",\n+                alloc.global.current_thread(), atomic,\n+                place.ptr.assert_ptr().alloc_id, offset.bytes(), size.bytes()\n+            );\n+\n+            let mut current_state = alloc.global.current_thread_state_mut();\n+            if atomic == AtomicReadOp::Relaxed {\n+                // Perform relaxed atomic load\n+                for range in alloc.alloc_ranges.get_mut().iter_mut(offset, size) {\n+                    range.load_relaxed(&mut *current_state);\n+                }\n+            }else{\n+                // Perform acquire(or seq-cst) atomic load\n+                for range in alloc.alloc_ranges.get_mut().iter_mut(offset, size) {\n+                    range.acquire(&mut *current_state);\n+                }\n+            }\n+\n+            // Log changes to atomic memory\n+            if log::log_enabled!(log::Level::Trace) {\n+                for range in alloc.alloc_ranges.get_mut().iter(offset, size) {\n+                    log::trace!(\n+                        \"  updated atomic memory({:?}, offset={}, size={}) to {:#?}\",\n+                        place.ptr.assert_ptr().alloc_id, offset.bytes(), size.bytes(),\n+                        range.atomic_ops\n+                    );\n+                }\n+            }\n+\n+            std::mem::drop(current_state);\n+            let data_race = &*this.memory.extra.data_race;\n+            data_race.advance_vector_clock();\n+        }\n+        Ok(())\n+    }\n+\n+    /// Update the data-race detector for an atomic write occuring at the\n+    ///  associated memory-place and on the current thread\n+    fn validate_atomic_store(\n+        &mut self, place: MPlaceTy<'tcx, Tag>, atomic: AtomicWriteOp\n+    ) -> InterpResult<'tcx> {\n+        let this = self.eval_context_mut();\n+        let data_race = &*this.memory.extra.data_race;\n+        if data_race.multi_threaded.get() {\n+            data_race.advance_vector_clock();\n+\n+            let (\n+                alloc, size, offset\n+            ) = this.load_data_race_state(place)?;\n+            let current_thread = alloc.global.current_thread();\n+            let mut current_state = alloc.global.current_thread_state_mut();\n+            log::trace!(\n+                \"Atomic store on {:?} with ordering {:?}, in memory({:?}, offset={}, size={})\",\n+                current_thread, atomic,\n+                place.ptr.assert_ptr().alloc_id, offset.bytes(), size.bytes()\n+            );\n+\n+            if atomic == AtomicWriteOp::Relaxed {\n+                // Perform relaxed atomic store\n+                for range in alloc.alloc_ranges.get_mut().iter_mut(offset, size) {\n+                    range.store_relaxed(&mut *current_state, current_thread);\n+                }\n+            }else{\n+                // Perform release(or seq-cst) atomic store\n+                for range in alloc.alloc_ranges.get_mut().iter_mut(offset, size) {\n+                    range.release(&mut *current_state, current_thread);\n+                }\n+            }\n+\n+            // Log changes to atomic memory\n+            if log::log_enabled!(log::Level::Trace) {\n+                for range in alloc.alloc_ranges.get_mut().iter(offset, size) {\n+                    log::trace!(\n+                        \"  updated atomic memory({:?}, offset={}, size={}) to {:#?}\",\n+                        place.ptr.assert_ptr().alloc_id, offset.bytes(), size.bytes(),\n+                        range.atomic_ops\n+                    );\n+                }\n+            }\n+\n+            std::mem::drop(current_state);\n+            let data_race = &*this.memory.extra.data_race;\n+            data_race.advance_vector_clock();\n+        }\n+        Ok(())\n+    }\n+\n+    /// Update the data-race detector for an atomic read-modify-write occuring\n+    ///  at the associated memory place and on the current thread\n+    fn validate_atomic_rmw(\n+        &mut self, place: MPlaceTy<'tcx, Tag>, atomic: AtomicRWOp\n+    ) -> InterpResult<'tcx> {\n+        use AtomicRWOp::*;\n+        let this = self.eval_context_mut();\n+        let data_race = &*this.memory.extra.data_race;\n+        if data_race.multi_threaded.get() {\n+            data_race.advance_vector_clock();\n+\n+            let (\n+                alloc, size, offset\n+            ) = this.load_data_race_state(place)?;\n+            let current_thread = alloc.global.current_thread();\n+            let mut current_state = alloc.global.current_thread_state_mut();\n+            log::trace!(\n+                \"Atomic RMW on {:?} with ordering {:?}, in memory({:?}, offset={}, size={})\",\n+                current_thread, atomic,\n+                place.ptr.assert_ptr().alloc_id, offset.bytes(), size.bytes()\n+            );\n+\n+            let acquire = matches!(atomic, Acquire | AcqRel | SeqCst);\n+            let release = matches!(atomic, Release | AcqRel | SeqCst);\n+            for range in alloc.alloc_ranges.get_mut().iter_mut(offset, size) {\n+                //FIXME: this is probably still slightly wrong due to the quirks\n+                // in the c++11 memory model\n+                if acquire {\n+                    // Atomic RW-Op acquire\n+                    range.acquire(&mut *current_state);\n+                }else{\n+                    range.load_relaxed(&mut *current_state);\n+                }\n+                if release {\n+                    // Atomic RW-Op release\n+                    range.rmw_release(&mut *current_state, current_thread);\n+                }else{\n+                    range.rmw_relaxed(&mut *current_state);\n+                }\n+            }\n+\n+            // Log changes to atomic memory\n+            if log::log_enabled!(log::Level::Trace) {\n+                for range in alloc.alloc_ranges.get_mut().iter(offset, size) {\n+                    log::trace!(\n+                        \"  updated atomic memory({:?}, offset={}, size={}) to {:#?}\",\n+                        place.ptr.assert_ptr().alloc_id, offset.bytes(), size.bytes(),\n+                        range.atomic_ops\n+                    );\n+                }\n+            }\n+\n+            std::mem::drop(current_state);\n+            let data_race = &*this.memory.extra.data_race;\n+            data_race.advance_vector_clock();\n+        }\n+        Ok(())\n+    }\n+\n+    /// Update the data-race detector for an atomic fence on the current thread\n+    fn validate_atomic_fence(&mut self, atomic: AtomicFenceOp) -> InterpResult<'tcx> {\n+        let this = self.eval_context_mut();\n+        let data_race = &*this.memory.extra.data_race;\n+        if data_race.multi_threaded.get() {\n+            data_race.advance_vector_clock();\n+\n+            log::trace!(\"Atomic fence on {:?} with ordering {:?}\", data_race.current_thread(), atomic);\n+            // Apply data-race detection for the current fences\n+            //  this treats AcqRel and SeqCst as the same as a acquire\n+            //  and release fence applied in the same timestamp.\n+            if atomic != AtomicFenceOp::Release {\n+                // Either Acquire | AcqRel | SeqCst\n+                data_race.current_thread_state_mut().apply_acquire_fence();\n+            }\n+            if atomic != AtomicFenceOp::Acquire {\n+                // Either Release | AcqRel | SeqCst\n+                data_race.current_thread_state_mut().apply_release_fence();\n+            }\n+\n+            data_race.advance_vector_clock();\n+        }\n+        Ok(())\n+    }\n+}\n+\n+/// Handle for locks to express their\n+///  acquire-release semantics\n+#[derive(Clone, Debug, Default)]\n+pub struct DataRaceLockHandle {\n+\n+    /// Internal acquire-release clock\n+    ///  to express the acquire release sync\n+    ///  found in concurrency primitives\n+    clock: VClock,\n+}\n+impl DataRaceLockHandle {\n+    pub fn set_values(&mut self, other: &Self) {\n+        self.clock.set_values(&other.clock)\n+    }\n+    pub fn reset(&mut self) {\n+        self.clock.set_zero_vector();\n+    }\n+}\n+\n+\n+/// Avoid an atomic allocation for the common\n+///  case with atomic operations where the number\n+///  of active release sequences is small\n+#[derive(Clone, PartialEq, Eq)]\n+enum AtomicReleaseSequences {\n+\n+    /// Contains one or no values\n+    ///  if empty: (None, reset vector clock)\n+    ///  if one:   (Some(thread), thread_clock)\n+    ReleaseOneOrEmpty(Option<ThreadId>, VClock),\n+\n+    /// Contains two or more values\n+    ///  stored in a hash-map of thread id to\n+    ///  vector clocks\n+    ReleaseMany(FxHashMap<ThreadId, VClock>)\n+}\n+impl AtomicReleaseSequences {\n+\n+    /// Return an empty set of atomic release sequences\n+    #[inline]\n+    fn new() -> AtomicReleaseSequences {\n+        Self::ReleaseOneOrEmpty(None, VClock::default())\n+    }\n+\n+    /// Remove all values except for the value stored at `thread` and set\n+    ///  the vector clock to the associated `clock` value\n+    #[inline]\n+    fn clear_and_set(&mut self, thread: ThreadId, clock: &VClock) {\n+        match self {\n+            Self::ReleaseOneOrEmpty(id, rel_clock) => {\n+                *id = Some(thread);\n+                rel_clock.set_values(clock);\n+            }\n+            Self::ReleaseMany(_) => {\n+                *self = Self::ReleaseOneOrEmpty(Some(thread), clock.clone());\n+            }\n+        }\n+    }\n+\n+    /// Remove all values except for the value stored at `thread`\n+    #[inline]\n+    fn clear_and_retain(&mut self, thread: ThreadId) {\n+        match self {\n+            Self::ReleaseOneOrEmpty(id, rel_clock) => {\n+                // Keep or forget depending on id\n+                if *id == Some(thread) {\n+                    *id = None;\n+                    rel_clock.set_zero_vector();\n+                }\n+            },\n+            Self::ReleaseMany(hash_map) => {\n+                // Retain only the thread element, so reduce to size\n+                //  of 1 or 0, and move to smaller format\n+                if let Some(clock) = hash_map.remove(&thread) {\n+                    *self = Self::ReleaseOneOrEmpty(Some(thread), clock);\n+                }else{\n+                    *self = Self::new();\n+                }\n+            }\n+        }\n+    }\n+\n+    /// Insert a release sequence at `thread` with values `clock`\n+    fn insert(&mut self, thread: ThreadId, clock: &VClock) {\n+        match self {\n+            Self::ReleaseOneOrEmpty(id, rel_clock) => {\n+                if id.map_or(true, |id| id == thread) {\n+                    *id = Some(thread);\n+                    rel_clock.set_values(clock);\n+                }else{\n+                    let mut hash_map = FxHashMap::default();\n+                    hash_map.insert(thread, clock.clone());\n+                    hash_map.insert(id.unwrap(), rel_clock.clone());\n+                    *self = Self::ReleaseMany(hash_map);\n+                }\n+            },\n+            Self::ReleaseMany(hash_map) => {\n+                hash_map.insert(thread, clock.clone());\n+            }\n+        }\n+    }\n+\n+    /// Return the release sequence at `thread` if one exists\n+    #[inline]\n+    fn load(&self, thread: ThreadId) -> Option<&VClock> {\n+        match self {\n+            Self::ReleaseOneOrEmpty(id, clock) => {\n+                if *id == Some(thread) {\n+                    Some(clock)\n+                }else{\n+                    None\n+                }\n+            },\n+            Self::ReleaseMany(hash_map) => {\n+                hash_map.get(&thread)\n+            }\n+        }\n+    }\n+}\n+\n+/// Custom debug implementation to correctly\n+///  print debug as a logical mapping from threads\n+///  to vector-clocks\n+impl Debug for AtomicReleaseSequences {\n+    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {\n+        match self {\n+            Self::ReleaseOneOrEmpty(None,_) => {\n+                f.debug_map().finish()\n+            },\n+            Self::ReleaseOneOrEmpty(Some(id), clock) => {\n+                f.debug_map().entry(&id, &clock).finish()\n+            },\n+            Self::ReleaseMany(hash_map) => {\n+                Debug::fmt(hash_map, f)\n+            }\n+        }\n+    }\n+}\n+\n+/// Externally stored memory cell clocks\n+///  explicitly to reduce memory usage for the\n+///  common case where no atomic operations\n+///  exists on the memory cell\n+#[derive(Clone, PartialEq, Eq, Debug)]\n+struct AtomicMemoryCellClocks {\n+\n+    /// Synchronization vector for acquire-release semantics\n+    sync_vector: VClock,\n+\n+    /// The Hash-Map of all threads for which a release\n+    ///  sequence exists in the memory cell \n+    release_sequences: AtomicReleaseSequences,\n+}\n+\n+/// Memory Cell vector clock metadata\n+///  for data-race detection\n+#[derive(Clone, PartialEq, Eq, Debug)]\n+struct MemoryCellClocks {\n+\n+    /// The vector-clock of the last write\n+    write: Timestamp,\n+\n+    /// The id of the thread that performed the last write to this memory location\n+    write_thread: ThreadId,\n+\n+    /// The vector-clock of the set of previous reads\n+    ///  each index is set to the timestamp that the associated\n+    ///  thread last read this value.\n+    read: VClock,\n+\n+    /// Atomic acquire & release sequence tracking clocks\n+    ///  for non-atomic memory in the common case this\n+    ///  value is set to None\n+    atomic_ops: Option<Box<AtomicMemoryCellClocks>>,\n+}\n+\n+/// Create a default memory cell clocks instance\n+///  for uninitialized memory\n+impl Default for MemoryCellClocks {\n+    fn default() -> Self {\n+        MemoryCellClocks {\n+            read: VClock::default(),\n+            write: 0,\n+            write_thread: ThreadId::new(u32::MAX as usize),\n+            atomic_ops: None\n+        }\n+    }\n+}\n+\n+impl MemoryCellClocks {\n+\n+    /// Load the internal atomic memory cells if they exist\n+    #[inline]\n+    fn atomic(&mut self) -> Option<&AtomicMemoryCellClocks> {\n+        match &self.atomic_ops {\n+            Some(op) => Some(&*op),\n+            None => None\n+        }\n+    }\n+\n+    /// Load or create the internal atomic memory metadata\n+    ///  if it does not exist\n+    #[inline]\n+    fn atomic_mut(&mut self) -> &mut AtomicMemoryCellClocks {\n+        self.atomic_ops.get_or_insert_with(|| {\n+            Box::new(AtomicMemoryCellClocks {\n+                sync_vector: VClock::default(),\n+                release_sequences: AtomicReleaseSequences::new()\n+            })\n+        })\n+    }\n+\n+    /// Update memory cell data-race tracking for atomic\n+    ///  load acquire semantics, is a no-op if this memory was\n+    ///  not used previously as atomic memory\n+    fn acquire(&mut self, clocks: &mut ThreadClockSet) {\n+        if let Some(atomic) = self.atomic() {\n+            clocks.clock.join(&atomic.sync_vector);\n+        }\n+    }\n+    /// Update memory cell data-race tracking for atomic\n+    ///  load relaxed semantics, is a no-op if this memory was\n+    ///  not used previously as atomic memory\n+    fn load_relaxed(&mut self, clocks: &mut ThreadClockSet) {\n+        if let Some(atomic) = self.atomic() {\n+            clocks.fence_acquire.join(&atomic.sync_vector);\n+        }\n+    }\n+\n+\n+    /// Update the memory cell data-race tracking for atomic\n+    ///  store release semantics\n+    fn release(&mut self, clocks: &ThreadClockSet, thread: ThreadId) {\n+        let atomic = self.atomic_mut();\n+        atomic.sync_vector.set_values(&clocks.clock);\n+        atomic.release_sequences.clear_and_set(thread, &clocks.clock);\n+    }\n+    /// Update the memory cell data-race tracking for atomic\n+    ///  store relaxed semantics\n+    fn store_relaxed(&mut self, clocks: &ThreadClockSet, thread: ThreadId) {\n+        let atomic = self.atomic_mut();\n+        atomic.sync_vector.set_values(&clocks.fence_release);\n+        if let Some(release) = atomic.release_sequences.load(thread) {\n+            atomic.sync_vector.join(release);\n+        }\n+        atomic.release_sequences.clear_and_retain(thread);\n+    }\n+    /// Update the memory cell data-race tracking for atomic\n+    ///  store release semantics for RMW operations\n+    fn rmw_release(&mut self, clocks: &ThreadClockSet, thread: ThreadId) {\n+        let atomic = self.atomic_mut();\n+        atomic.sync_vector.join(&clocks.clock);\n+        atomic.release_sequences.insert(thread, &clocks.clock);\n+    }\n+    /// Update the memory cell data-race tracking for atomic\n+    ///  store relaxed semantics for RMW operations\n+    fn rmw_relaxed(&mut self, clocks: &ThreadClockSet) {\n+        let atomic = self.atomic_mut();\n+        atomic.sync_vector.join(&clocks.fence_release);\n+    }\n+    \n+    \n+\n+    /// Detect races for non-atomic read operations at the current memory cell\n+    ///  returns true if a data-race is detected\n+    fn read_race_detect(&mut self, clocks: &ThreadClockSet, thread: ThreadId) -> bool {\n+        if self.write <= clocks.clock[self.write_thread] {\n+            self.read.set_at_thread(&clocks.clock, thread);\n+            false\n+        }else{\n+            true\n+        }\n+    }\n+\n+    /// Detect races for non-atomic write operations at the current memory cell\n+    ///  returns true if a data-race is detected\n+    fn write_race_detect(&mut self, clocks: &ThreadClockSet, thread: ThreadId) -> bool {\n+        if self.write <= clocks.clock[self.write_thread] && self.read <= clocks.clock {\n+            self.write = clocks.clock[thread];\n+            self.write_thread = thread;\n+            self.read.set_zero_vector();\n+            false\n+        }else{\n+            true\n+        }\n+    }\n+}\n+\n+/// Vector clock metadata for a logical memory allocation\n+#[derive(Debug, Clone)]\n+pub struct VClockAlloc {\n+\n+    /// Range of Vector clocks, mapping to the vector-clock\n+    ///  index of the last write to the bytes in this allocation\n+    alloc_ranges: RefCell<RangeMap<MemoryCellClocks>>,\n+\n+    // Pointer to global state\n+    global: MemoryExtra,\n+}\n+\n+impl VClockAlloc {\n+\n+    /// Create a new data-race allocation detector\n+    pub fn new_allocation(global: &MemoryExtra, len: Size) -> VClockAlloc {\n+        VClockAlloc {\n+            global: Rc::clone(global),\n+            alloc_ranges: RefCell::new(\n+                RangeMap::new(len, MemoryCellClocks::default())\n+            )\n+        }\n+    }\n+\n+    /// Report a data-race found in the program\n+    ///  this finds the two racing threads and the type\n+    ///  of data-race that occured, this will also\n+    ///  return info about the memory location the data-race\n+    ///  occured in\n+    #[cold]\n+    #[inline(never)]\n+    fn report_data_race<'tcx>(\n+        global: &MemoryExtra, range: &MemoryCellClocks, action: &str,\n+        pointer: Pointer<Tag>, len: Size\n+    ) -> InterpResult<'tcx> {\n+        let current_thread = global.current_thread();\n+        let current_state = global.current_thread_state();\n+        let mut write_clock = VClock::default();\n+        let (\n+            other_action, other_thread, other_clock\n+        ) = if range.write > current_state.clock[range.write_thread] {\n+\n+            // Create effective write-clock that the data-race occured with\n+            let wclock = write_clock.get_mut_with_min_len(\n+                current_state.clock.as_slice().len()\n+                .max(range.write_thread.to_u32() as usize + 1)\n+            );\n+            wclock[range.write_thread.to_u32() as usize] = range.write;\n+            (\"WRITE\", range.write_thread, write_clock.as_slice())\n+        }else{\n+\n+            // Find index in the read-clock that the data-race occured with\n+            let read_slice = range.read.as_slice();\n+            let clock_slice = current_state.clock.as_slice();\n+            let conflicting_index = read_slice.iter()\n+                .zip(clock_slice.iter())\n+                .enumerate().find_map(|(idx,(&read, &clock))| {\n+                    if read > clock {\n+                        Some(idx)\n+                    }else{\n+                        None\n+                    }\n+            }).unwrap_or_else(|| {\n+                assert!(read_slice.len() > clock_slice.len(), \"BUG: cannot find read race yet reported data-race\");\n+                let rest_read = &read_slice[clock_slice.len()..];\n+                rest_read.iter().enumerate().find_map(|(idx, &val)| {\n+                    if val > 0 {\n+                        Some(idx + clock_slice.len())\n+                    }else{\n+                        None\n+                    }\n+                }).expect(\"Invariant broken for read-slice, no 0 element at the tail\")\n+            });\n+            (\"READ\", ThreadId::new(conflicting_index), range.read.as_slice())\n+        };\n+\n+        let current_thread_info = global.print_thread_metadata(current_thread);\n+        let other_thread_info = global.print_thread_metadata(other_thread);\n+        \n+        // Throw the data-race detection\n+        throw_ub_format!(\n+            \"Data race detected between {} on {} and {} on {}, memory({:?},offset={},size={})\\\n+            \\n\\t\\t -current vector clock = {:?}\\\n+            \\n\\t\\t -conflicting timestamp = {:?}\",\n+            action, current_thread_info, \n+            other_action, other_thread_info,\n+            pointer.alloc_id, pointer.offset.bytes(), len.bytes(),\n+            current_state.clock,\n+            other_clock\n+        )\n+    }\n+\n+    /// Detect data-races for an unsychronized read operation, will not perform\n+    ///  data-race threads if `multi-threaded` is false, either due to no threads\n+    ///  being created or if it is temporarily disabled during a racy read or write\n+    ///  operation\n+    pub fn read<'tcx>(&self, pointer: Pointer<Tag>, len: Size) -> InterpResult<'tcx> {\n+        if self.global.multi_threaded.get() {\n+            let current_thread = self.global.current_thread();\n+            let current_state = self.global.current_thread_state();\n+\n+            // The alloc-ranges are not split, however changes are not going to be made\n+            //  to the ranges being tested, so this is ok\n+            let mut alloc_ranges = self.alloc_ranges.borrow_mut();\n+            for range in alloc_ranges.iter_mut(pointer.offset, len) {\n+                if range.read_race_detect(&*current_state, current_thread) {\n+                    // Report data-race\n+                    return Self::report_data_race(\n+                        &self.global,range, \"READ\", pointer, len\n+                    );\n+                }\n+            }\n+            Ok(())\n+        }else{\n+            Ok(())\n+        }\n+    }\n+    /// Detect data-races for an unsychronized write operation, will not perform\n+    ///  data-race threads if `multi-threaded` is false, either due to no threads\n+    ///  being created or if it is temporarily disabled during a racy read or write\n+    ///  operation\n+    pub fn write<'tcx>(&mut self, pointer: Pointer<Tag>, len: Size) -> InterpResult<'tcx> {\n+        if self.global.multi_threaded.get() {\n+            let current_thread = self.global.current_thread();\n+            let current_state = self.global.current_thread_state();\n+            for range in self.alloc_ranges.get_mut().iter_mut(pointer.offset, len) {\n+                if range.write_race_detect(&*current_state, current_thread) {\n+                    // Report data-race\n+                    return Self::report_data_race(\n+                        &self.global, range, \"WRITE\", pointer, len\n+                    );\n+                }\n+            }\n+            Ok(())\n+        }else{\n+            Ok(())\n+        }\n+    }\n+    /// Detect data-races for an unsychronized deallocate operation, will not perform\n+    ///  data-race threads if `multi-threaded` is false, either due to no threads\n+    ///  being created or if it is temporarily disabled during a racy read or write\n+    ///  operation\n+    pub fn deallocate<'tcx>(&mut self, pointer: Pointer<Tag>, len: Size) -> InterpResult<'tcx> {\n+        if self.global.multi_threaded.get() {\n+            let current_thread = self.global.current_thread();\n+            let current_state = self.global.current_thread_state();\n+            for range in self.alloc_ranges.get_mut().iter_mut(pointer.offset, len) {\n+                if range.write_race_detect(&*current_state, current_thread) {\n+                    // Report data-race\n+                    return Self::report_data_race(\n+                        &self.global, range, \"DEALLOCATE\", pointer, len\n+                    );\n+                }\n+            }\n+           Ok(())\n+        }else{\n+            Ok(())\n+        }\n+    }\n+}\n+\n+/// The current set of vector clocks describing the state\n+///  of a thread, contains the happens-before clock and\n+///  additional metadata to model atomic fence operations\n+#[derive(Clone, Default, Debug)]\n+struct ThreadClockSet {\n+    /// The increasing clock representing timestamps\n+    ///  that happen-before this thread.\n+    clock: VClock,\n+\n+    /// The set of timestamps that will happen-before this\n+    ///  thread once it performs an acquire fence\n+    fence_acquire: VClock,\n+\n+    /// The last timesamp of happens-before relations that\n+    ///  have been released by this thread by a fence\n+    fence_release: VClock,\n+}\n+\n+impl ThreadClockSet {\n+\n+    /// Apply the effects of a release fence to this\n+    ///  set of thread vector clocks\n+    #[inline]\n+    fn apply_release_fence(&mut self) {\n+        self.fence_release.set_values(&self.clock);\n+    }\n+\n+    /// Apply the effects of a acquire fence to this\n+    ///  set of thread vector clocks\n+    #[inline]\n+    fn apply_acquire_fence(&mut self) {\n+        self.clock.join(&self.fence_acquire);\n+    }\n+\n+    /// Increment the happens-before clock at a\n+    ///  known index\n+    #[inline]\n+    fn increment_clock(&mut self, thread: ThreadId) {\n+        self.clock.increment_thread(thread);\n+    }\n+\n+    /// Join the happens-before clock with that of\n+    ///  another thread, used to model thread join\n+    ///  operations\n+    fn join_with(&mut self, other: &ThreadClockSet) {\n+        self.clock.join(&other.clock);\n+    }\n+}\n+\n+/// Global data-race detection state, contains the currently\n+///  executing thread as well as the vector-clocks associated\n+///  with each of the threads.\n+#[derive(Debug, Clone)]\n+pub struct GlobalState {\n+\n+    /// Set to true once the first additional\n+    ///  thread has launched, due to the dependency\n+    ///  between before and after a thread launch\n+    /// Any data-races must be recorded after this\n+    ///  so concurrent execution can ignore recording\n+    ///  any data-races\n+    multi_threaded: Cell<bool>,\n+\n+    /// The current vector clock for all threads\n+    ///  this includes threads that have terminated\n+    ///  execution\n+    thread_clocks: RefCell<IndexVec<ThreadId, ThreadClockSet>>,\n+\n+    /// Thread name cache for better diagnostics on the reporting\n+    ///  of a data-race\n+    thread_names: RefCell<IndexVec<ThreadId, Option<Box<str>>>>,\n+\n+    /// The current thread being executed,\n+    ///  this is mirrored from the scheduler since\n+    ///  it is required for loading the current vector\n+    ///  clock for data-race detection\n+    current_thread_id: Cell<ThreadId>,\n+}\n+impl GlobalState {\n+\n+    /// Create a new global state, setup with just thread-id=0\n+    ///  advanced to timestamp = 1\n+    pub fn new() -> Self {\n+        let mut vec = IndexVec::new();\n+        let thread_id = vec.push(ThreadClockSet::default());\n+        vec[thread_id].increment_clock(thread_id);\n+        GlobalState {\n+            multi_threaded: Cell::new(false),\n+            thread_clocks: RefCell::new(vec),\n+            thread_names: RefCell::new(IndexVec::new()),\n+            current_thread_id: Cell::new(thread_id),\n+        }\n+    }\n+    \n+\n+    // Hook for thread creation, enabled multi-threaded execution and marks\n+    //  the current thread timestamp as happening-before the current thread\n+    #[inline]\n+    pub fn thread_created(&self, thread: ThreadId) {\n+\n+        // Enable multi-threaded execution mode now that there are at least\n+        //  two threads\n+        self.multi_threaded.set(true);\n+        let current_thread = self.current_thread_id.get();\n+        let mut vectors = self.thread_clocks.borrow_mut();\n+        vectors.ensure_contains_elem(thread, Default::default);\n+        let (current, created) = vectors.pick2_mut(current_thread, thread);\n+\n+        // Pre increment clocks before atomic operation\n+        current.increment_clock(current_thread);\n+\n+        // The current thread happens-before the created thread\n+        //  so update the created vector clock\n+        created.join_with(current);\n+\n+        // Post increment clocks after atomic operation\n+        current.increment_clock(current_thread);\n+        created.increment_clock(thread);\n+    }\n+\n+    /// Hook on a thread join to update the implicit happens-before relation\n+    ///  between the joined thead and the current thread\n+    #[inline]\n+    pub fn thread_joined(&self, current_thread: ThreadId, join_thread: ThreadId) {\n+        let mut vectors = self.thread_clocks.borrow_mut();\n+        let (current, join) = vectors.pick2_mut(current_thread, join_thread);\n+\n+        // Pre increment clocks before atomic operation\n+        current.increment_clock(current_thread);\n+        join.increment_clock(join_thread);\n+\n+        // The join thread happens-before the current thread\n+        //   so update the current vector clock\n+        current.join_with(join);\n+\n+        // Post increment clocks after atomic operation\n+        current.increment_clock(current_thread);\n+        join.increment_clock(join_thread);\n+    }\n+\n+    /// Hook for updating the local tracker of the currently\n+    ///  enabled thread, should always be updated whenever\n+    ///  `active_thread` in thread.rs is updated\n+    #[inline]\n+    pub fn thread_set_active(&self, thread: ThreadId) {\n+        self.current_thread_id.set(thread);\n+    }\n+\n+    /// Hook for updating the local tracker of the threads name\n+    ///  this should always mirror the local value in thread.rs\n+    ///  the thread name is used for improved diagnostics\n+    ///  during a data-race\n+    #[inline]\n+    pub fn thread_set_name(&self, name: String) {\n+        let name = name.into_boxed_str();\n+        let mut names = self.thread_names.borrow_mut();\n+        let thread = self.current_thread_id.get();\n+        names.ensure_contains_elem(thread, Default::default);\n+        names[thread] = Some(name);\n+    }\n+\n+\n+    /// Advance the vector clock for a thread\n+    ///  this is called before and after any atomic/synchronizing operations\n+    ///  that may manipulate state\n+    #[inline]\n+    fn advance_vector_clock(&self) {\n+        let thread = self.current_thread_id.get();\n+        let mut vectors = self.thread_clocks.borrow_mut();\n+        vectors[thread].increment_clock(thread);\n+\n+        // Log the increment in the atomic vector clock\n+        log::trace!(\"Atomic vector clock increase for {:?} to {:?}\",thread, vectors[thread].clock);\n+    }\n+    \n+\n+    /// Internal utility to identify a thread stored internally\n+    ///  returns the id and the name for better diagnostics\n+    fn print_thread_metadata(&self, thread: ThreadId) -> String {\n+        if let Some(Some(name)) = self.thread_names.borrow().get(thread) {\n+            let name: &str = name;\n+            format!(\"Thread(id = {:?}, name = {:?})\", thread.to_u32(), &*name)\n+        }else{\n+            format!(\"Thread(id = {:?})\", thread.to_u32())\n+        }\n+    }\n+\n+\n+    /// Acquire a lock, express that the previous call of\n+    ///  `validate_lock_release` must happen before this\n+    pub fn validate_lock_acquire(&self, lock: &DataRaceLockHandle, thread: ThreadId) {\n+        let mut ref_vector = self.thread_clocks.borrow_mut();\n+        ref_vector[thread].increment_clock(thread);\n+\n+        let clocks = &mut ref_vector[thread];\n+        clocks.clock.join(&lock.clock);\n+\n+        ref_vector[thread].increment_clock(thread);\n+    }\n+\n+    /// Release a lock handle, express that this happens-before\n+    ///  any subsequent calls to `validate_lock_acquire`\n+    pub fn validate_lock_release(&self, lock: &mut DataRaceLockHandle, thread: ThreadId) {\n+        let mut ref_vector = self.thread_clocks.borrow_mut();\n+        ref_vector[thread].increment_clock(thread);\n+\n+        let clocks = &ref_vector[thread];\n+        lock.clock.set_values(&clocks.clock);\n+\n+        ref_vector[thread].increment_clock(thread);\n+    }\n+\n+    /// Release a lock handle, express that this happens-before\n+    ///  any subsequent calls to `validate_lock_acquire` as well\n+    ///  as any previous calls to this function after any\n+    ///  `validate_lock_release` calls\n+    pub fn validate_lock_release_shared(&self, lock: &mut DataRaceLockHandle, thread: ThreadId) {\n+        let mut ref_vector = self.thread_clocks.borrow_mut();\n+        ref_vector[thread].increment_clock(thread);\n+\n+        let clocks = &ref_vector[thread];\n+        lock.clock.join(&clocks.clock);\n+\n+        ref_vector[thread].increment_clock(thread);\n+    }\n+\n+    /// Load the thread clock set associated with the current thread\n+    #[inline]\n+    fn current_thread_state(&self) -> Ref<'_, ThreadClockSet> {\n+        let ref_vector = self.thread_clocks.borrow();\n+        let thread = self.current_thread_id.get();\n+        Ref::map(ref_vector, |vector| &vector[thread])\n+    }\n+\n+    /// Load the thread clock set associated with the current thread\n+    ///  mutably for modification\n+    #[inline]\n+    fn current_thread_state_mut(&self) -> RefMut<'_, ThreadClockSet> {\n+        let ref_vector = self.thread_clocks.borrow_mut();\n+        let thread = self.current_thread_id.get();\n+        RefMut::map(ref_vector, |vector| &mut vector[thread])\n+    }\n+\n+    /// Return the current thread, should be the same\n+    ///  as the data-race active thread\n+    #[inline]\n+    fn current_thread(&self) -> ThreadId {\n+        self.current_thread_id.get()\n+    }\n+}\n+\n+\n+/// The size of the vector-clock to store inline\n+///  clock vectors larger than this will be stored on the heap\n+const SMALL_VECTOR: usize = 4;\n+\n+/// The type of the time-stamps recorded in the data-race detector\n+///  set to a type of unsigned integer\n+type Timestamp = u32;\n+\n+/// A vector clock for detecting data-races\n+///  invariants:\n+///   - the last element in a VClock must not be 0\n+///     -- this means that derive(PartialEq & Eq) is correct\n+///     --  as there is no implicit zero tail that might be equal\n+///     --  also simplifies the implementation of PartialOrd\n+#[derive(Clone, PartialEq, Eq, Default, Debug)]\n+pub struct VClock(SmallVec<[Timestamp; SMALL_VECTOR]>);\n+\n+impl VClock {\n+\n+    /// Load the backing slice behind the clock vector.\n+    #[inline]\n+    fn as_slice(&self) -> &[Timestamp] {\n+        self.0.as_slice()\n+    }\n+\n+    /// Get a mutable slice to the internal vector with minimum `min_len`\n+    ///  elements, to preserve invariants this vector must modify\n+    ///  the `min_len`-1 nth element to a non-zero value\n+    #[inline]\n+    fn get_mut_with_min_len(&mut self, min_len: usize) -> &mut [Timestamp] {\n+        if self.0.len() < min_len {\n+            self.0.resize(min_len, 0);\n+        }\n+        assert!(self.0.len() >= min_len);\n+        self.0.as_mut_slice()\n+    }\n+\n+    /// Increment the vector clock at a known index\n+    #[inline]\n+    fn increment_index(&mut self, idx: usize) {\n+        let mut_slice = self.get_mut_with_min_len(idx + 1);\n+        let idx_ref = &mut mut_slice[idx];\n+        *idx_ref = idx_ref.checked_add(1).expect(\"Vector clock overflow\")\n+    }\n+\n+    // Increment the vector element representing the progress\n+    //  of execution in the given thread\n+    #[inline]\n+    pub fn increment_thread(&mut self, thread: ThreadId) {\n+        self.increment_index(thread.to_u32() as usize);\n+    }\n+\n+    // Join the two vector-clocks together, this\n+    //  sets each vector-element to the maximum value\n+    //  of that element in either of the two source elements.\n+    pub fn join(&mut self, other: &Self) {\n+        let rhs_slice = other.as_slice();\n+        let lhs_slice = self.get_mut_with_min_len(rhs_slice.len());\n+\n+        // Element-wise set to maximum.\n+        for (l, &r) in lhs_slice.iter_mut().zip(rhs_slice.iter()) {\n+            *l = r.max(*l);\n+        }\n+    }\n+\n+    /// Joins with a thread at a known index\n+    fn set_at_index(&mut self, other: &Self, idx: usize){\n+        let mut_slice = self.get_mut_with_min_len(idx + 1);\n+        let slice = other.as_slice();\n+        mut_slice[idx] = slice[idx];\n+    }\n+\n+    /// Join with a threads vector clock only at the desired index\n+    ///  returns true if the value updated\n+    #[inline]\n+    pub fn set_at_thread(&mut self, other: &Self, thread: ThreadId){\n+        self.set_at_index(other, thread.to_u32() as usize);\n+    }\n+\n+    /// Clear the vector to all zeros, stored as an empty internal\n+    ///  vector\n+    #[inline]\n+    pub fn set_zero_vector(&mut self) {\n+        self.0.clear();\n+    }\n+\n+    /// Set the values stored in this vector clock\n+    ///  to the values stored in another.\n+    pub fn set_values(&mut self, new_value: &VClock) {\n+        let new_slice = new_value.as_slice();\n+        self.0.resize(new_slice.len(), 0);\n+        self.0.copy_from_slice(new_slice);\n+    }\n+}\n+\n+\n+impl PartialOrd for VClock {\n+    fn partial_cmp(&self, other: &VClock) -> Option<Ordering> {\n+\n+        // Load the values as slices\n+        let lhs_slice = self.as_slice();\n+        let rhs_slice = other.as_slice();\n+\n+        // Iterate through the combined vector slice\n+        //  keeping track of the order that is currently possible to satisfy.\n+        // If an ordering relation is detected to be impossible, then bail and\n+        //  directly return None\n+        let mut iter = lhs_slice.iter().zip(rhs_slice.iter());\n+        let mut order = match iter.next() {\n+            Some((lhs, rhs)) => lhs.cmp(rhs),\n+            None => Ordering::Equal\n+        };\n+        for (l, r) in iter {\n+            match order {\n+                Ordering::Equal => order = l.cmp(r),\n+                Ordering::Less => if l > r {\n+                    return None\n+                },\n+                Ordering::Greater => if l < r {\n+                    return None\n+                }\n+            }\n+        }\n+\n+        //Now test if either left or right have trailing elements\n+        // by the invariant the trailing elements have at least 1\n+        // non zero value, so no additional calculation is required\n+        // to determine the result of the PartialOrder\n+        let l_len = lhs_slice.len();\n+        let r_len = rhs_slice.len();\n+        match l_len.cmp(&r_len) {\n+            // Equal has no additional elements: return current order\n+            Ordering::Equal => Some(order),\n+            // Right has at least 1 element > than the implicit 0,\n+            //  so the only valid values are Ordering::Less or None\n+            Ordering::Less => match order {\n+                Ordering::Less | Ordering::Equal => Some(Ordering::Less),\n+                Ordering::Greater => None\n+            }\n+            // Left has at least 1 element > than the implicit 0,\n+            //  so the only valid values are Ordering::Greater or None\n+            Ordering::Greater => match order {\n+                Ordering::Greater | Ordering::Equal => Some(Ordering::Greater),\n+                Ordering::Less => None\n+            }\n+        }\n+    }\n+\n+    fn lt(&self, other: &VClock) -> bool {\n+        // Load the values as slices\n+        let lhs_slice = self.as_slice();\n+        let rhs_slice = other.as_slice();\n+\n+        // If l_len > r_len then at least one element\n+        //  in l_len is > than r_len, therefore the result\n+        //  is either Some(Greater) or None, so return false\n+        //  early.\n+        let l_len = lhs_slice.len();\n+        let r_len = rhs_slice.len();\n+        if l_len <= r_len {\n+            // If any elements on the left are greater than the right\n+            //  then the result is None or Some(Greater), both of which\n+            //  return false, the earlier test asserts that no elements in the\n+            //  extended tail violate this assumption. Otherwise l <= r, finally\n+            //  the case where the values are potentially equal needs to be considered\n+            //  and false returned as well\n+            let mut equal = l_len == r_len;\n+            for (&l, &r) in lhs_slice.iter().zip(rhs_slice.iter()) {\n+                if l > r {\n+                    return false\n+                }else if l < r {\n+                    equal = false;\n+                }\n+            }\n+            !equal\n+        }else{\n+            false\n+        }\n+    }\n+\n+    fn le(&self, other: &VClock) -> bool {\n+        // Load the values as slices\n+        let lhs_slice = self.as_slice();\n+        let rhs_slice = other.as_slice();\n+\n+        // If l_len > r_len then at least one element\n+        //  in l_len is > than r_len, therefore the result\n+        //  is either Some(Greater) or None, so return false\n+        //  early.\n+        let l_len = lhs_slice.len();\n+        let r_len = rhs_slice.len();\n+        if l_len <= r_len {\n+            // If any elements on the left are greater than the right\n+            //  then the result is None or Some(Greater), both of which\n+            //  return false, the earlier test asserts that no elements in the\n+            //  extended tail violate this assumption. Otherwise l <= r\n+            !lhs_slice.iter().zip(rhs_slice.iter()).any(|(&l, &r)| l > r)\n+        }else{\n+            false\n+        }\n+    }\n+\n+    fn gt(&self, other: &VClock) -> bool {\n+        // Load the values as slices\n+        let lhs_slice = self.as_slice();\n+        let rhs_slice = other.as_slice();\n+\n+        // If r_len > l_len then at least one element\n+        //  in r_len is > than l_len, therefore the result\n+        //  is either Some(Less) or None, so return false\n+        //  early.\n+        let l_len = lhs_slice.len();\n+        let r_len = rhs_slice.len();\n+        if l_len >= r_len {\n+            // If any elements on the left are less than the right\n+            //  then the result is None or Some(Less), both of which\n+            //  return false, the earlier test asserts that no elements in the\n+            //  extended tail violate this assumption. Otherwise l >=, finally\n+            //  the case where the values are potentially equal needs to be considered\n+            //  and false returned as well\n+            let mut equal = l_len == r_len;\n+            for (&l, &r) in lhs_slice.iter().zip(rhs_slice.iter()) {\n+                if l < r {\n+                    return false\n+                }else if l > r {\n+                    equal = false;\n+                }\n+            }\n+            !equal\n+        }else{\n+            false\n+        }\n+    }\n+\n+    fn ge(&self, other: &VClock) -> bool {\n+        // Load the values as slices\n+        let lhs_slice = self.as_slice();\n+        let rhs_slice = other.as_slice();\n+\n+        // If r_len > l_len then at least one element\n+        //  in r_len is > than l_len, therefore the result\n+        //  is either Some(Less) or None, so return false\n+        //  early.\n+        let l_len = lhs_slice.len();\n+        let r_len = rhs_slice.len();\n+        if l_len >= r_len {\n+            // If any elements on the left are less than the right\n+            //  then the result is None or Some(Less), both of which\n+            //  return false, the earlier test asserts that no elements in the\n+            //  extended tail violate this assumption. Otherwise l >= r\n+            !lhs_slice.iter().zip(rhs_slice.iter()).any(|(&l, &r)| l < r)\n+        }else{\n+            false\n+        }\n+    }\n+}\n+\n+impl Index<ThreadId> for VClock {\n+    type Output = Timestamp;\n+\n+    #[inline]\n+    fn index(&self, index: ThreadId) -> &Timestamp {\n+       self.as_slice().get(index.to_u32() as usize).unwrap_or(&0)\n+    }\n+}\n+\n+\n+/// Test vector clock ordering operations\n+///  data-race detection is tested in the external\n+///  test suite\n+#[cfg(test)]\n+mod tests {\n+    use super::{VClock, Timestamp};\n+    use std::cmp::Ordering;\n+\n+    #[test]\n+    fn test_equal() {\n+        let mut c1 = VClock::default();\n+        let mut c2 = VClock::default();\n+        assert_eq!(c1, c2);\n+        c1.increment_index(5);\n+        assert_ne!(c1, c2);\n+        c2.increment_index(53);\n+        assert_ne!(c1, c2);\n+        c1.increment_index(53);\n+        assert_ne!(c1, c2);\n+        c2.increment_index(5);\n+        assert_eq!(c1, c2);\n+    }\n+\n+    #[test]\n+    fn test_partial_order() {\n+        // Small test\n+        assert_order(&[1], &[1], Some(Ordering::Equal));\n+        assert_order(&[1], &[2], Some(Ordering::Less));\n+        assert_order(&[2], &[1], Some(Ordering::Greater));\n+        assert_order(&[1], &[1,2], Some(Ordering::Less));\n+        assert_order(&[2], &[1,2], None);\n+\n+        // Misc tests\n+        assert_order(&[400], &[0, 1], None);\n+\n+        // Large test\n+        assert_order(&[0,1,2,3,4,5,6,7,8,9,10], &[0,1,2,3,4,5,6,7,8,9,10,0,0,0], Some(Ordering::Equal));\n+        assert_order(&[0,1,2,3,4,5,6,7,8,9,10], &[0,1,2,3,4,5,6,7,8,9,10,0,1,0], Some(Ordering::Less));\n+        assert_order(&[0,1,2,3,4,5,6,7,8,9,11], &[0,1,2,3,4,5,6,7,8,9,10,0,0,0], Some(Ordering::Greater));\n+        assert_order(&[0,1,2,3,4,5,6,7,8,9,11], &[0,1,2,3,4,5,6,7,8,9,10,0,1,0], None);\n+        assert_order(&[0,1,2,3,4,5,6,7,8,9,9 ], &[0,1,2,3,4,5,6,7,8,9,10,0,0,0], Some(Ordering::Less));\n+        assert_order(&[0,1,2,3,4,5,6,7,8,9,9 ], &[0,1,2,3,4,5,6,7,8,9,10,0,1,0], Some(Ordering::Less));\n+    }\n+\n+    fn from_slice(mut slice: &[Timestamp]) -> VClock {\n+        while let Some(0) = slice.last() {\n+            slice = &slice[..slice.len() - 1]\n+        }\n+        VClock(smallvec::SmallVec::from_slice(slice))\n+    }\n+\n+    fn assert_order(l: &[Timestamp], r: &[Timestamp], o: Option<Ordering>) {\n+        let l = from_slice(l);\n+        let r = from_slice(r);\n+\n+        //Test partial_cmp\n+        let compare = l.partial_cmp(&r);\n+        assert_eq!(compare, o, \"Invalid comparison\\n l: {:?}\\n r: {:?}\",l,r);\n+        let alt_compare = r.partial_cmp(&l);\n+        assert_eq!(alt_compare, o.map(Ordering::reverse), \"Invalid alt comparison\\n l: {:?}\\n r: {:?}\",l,r);\n+\n+        //Test operatorsm with faster implementations\n+        assert_eq!(\n+            matches!(compare,Some(Ordering::Less)), l < r,\n+            \"Invalid (<):\\n l: {:?}\\n r: {:?}\",l,r\n+        );\n+        assert_eq!(\n+            matches!(compare,Some(Ordering::Less) | Some(Ordering::Equal)), l <= r,\n+            \"Invalid (<=):\\n l: {:?}\\n r: {:?}\",l,r\n+        );\n+        assert_eq!(\n+            matches!(compare,Some(Ordering::Greater)), l > r,\n+            \"Invalid (>):\\n l: {:?}\\n r: {:?}\",l,r\n+        );\n+        assert_eq!(\n+            matches!(compare,Some(Ordering::Greater) | Some(Ordering::Equal)), l >= r,\n+            \"Invalid (>=):\\n l: {:?}\\n r: {:?}\",l,r\n+        );\n+        assert_eq!(\n+            matches!(alt_compare,Some(Ordering::Less)), r < l,\n+            \"Invalid alt (<):\\n l: {:?}\\n r: {:?}\",l,r\n+        );\n+        assert_eq!(\n+            matches!(alt_compare,Some(Ordering::Less) | Some(Ordering::Equal)), r <= l,\n+            \"Invalid alt (<=):\\n l: {:?}\\n r: {:?}\",l,r\n+        );\n+        assert_eq!(\n+            matches!(alt_compare,Some(Ordering::Greater)), r > l,\n+            \"Invalid alt (>):\\n l: {:?}\\n r: {:?}\",l,r\n+        );\n+        assert_eq!(\n+            matches!(alt_compare,Some(Ordering::Greater) | Some(Ordering::Equal)), r >= l,\n+            \"Invalid alt (>=):\\n l: {:?}\\n r: {:?}\",l,r\n+        );\n+    }\n+}\n\\ No newline at end of file"}, {"sha": "f384787e4c681ee06e6223c88ebc24208d396475", "filename": "src/lib.rs", "status": "modified", "additions": 5, "deletions": 0, "changes": 5, "blob_url": "https://github.com/rust-lang/rust/blob/89814f1b3f6c239f472dea4798a1189a30d7efa2/src%2Flib.rs", "raw_url": "https://github.com/rust-lang/rust/raw/89814f1b3f6c239f472dea4798a1189a30d7efa2/src%2Flib.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flib.rs?ref=89814f1b3f6c239f472dea4798a1189a30d7efa2", "patch": "@@ -22,6 +22,7 @@ extern crate rustc_mir;\n extern crate rustc_span;\n extern crate rustc_target;\n \n+mod data_race;\n mod diagnostics;\n mod eval;\n mod helpers;\n@@ -52,6 +53,10 @@ pub use crate::shims::panic::{CatchUnwindData, EvalContextExt as _};\n pub use crate::shims::tls::{EvalContextExt as _, TlsData};\n pub use crate::shims::EvalContextExt as _;\n \n+pub use crate::data_race::{\n+    AtomicReadOp, AtomicWriteOp, AtomicRWOp, AtomicFenceOp, DataRaceLockHandle,\n+    EvalContextExt as DataRaceEvalContextExt\n+};\n pub use crate::diagnostics::{\n     register_diagnostic, report_error, EvalContextExt as DiagnosticsEvalContextExt,\n     TerminationInfo, NonHaltingDiagnostic,"}, {"sha": "363513f636c9ec3976f9eef63fea3d90f272ce37", "filename": "src/machine.rs", "status": "modified", "additions": 10, "deletions": 1, "changes": 11, "blob_url": "https://github.com/rust-lang/rust/blob/89814f1b3f6c239f472dea4798a1189a30d7efa2/src%2Fmachine.rs", "raw_url": "https://github.com/rust-lang/rust/raw/89814f1b3f6c239f472dea4798a1189a30d7efa2/src%2Fmachine.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Fmachine.rs?ref=89814f1b3f6c239f472dea4798a1189a30d7efa2", "patch": "@@ -109,12 +109,15 @@ impl fmt::Display for MiriMemoryKind {\n pub struct AllocExtra {\n     /// Stacked Borrows state is only added if it is enabled.\n     pub stacked_borrows: Option<stacked_borrows::AllocExtra>,\n+    /// Data race detection via the use of a vector-clock.\n+    pub data_race: data_race::AllocExtra,\n }\n \n /// Extra global memory data\n #[derive(Clone, Debug)]\n pub struct MemoryExtra {\n     pub stacked_borrows: Option<stacked_borrows::MemoryExtra>,\n+    pub data_race: data_race::MemoryExtra,\n     pub intptrcast: intptrcast::MemoryExtra,\n \n     /// Mapping extern static names to their canonical allocation.\n@@ -144,8 +147,10 @@ impl MemoryExtra {\n         } else {\n             None\n         };\n+        let data_race = Rc::new(data_race::GlobalState::new());\n         MemoryExtra {\n             stacked_borrows,\n+            data_race,\n             intptrcast: Default::default(),\n             extern_statics: FxHashMap::default(),\n             rng: RefCell::new(rng),\n@@ -467,6 +472,7 @@ impl<'mir, 'tcx> Machine<'mir, 'tcx> for Evaluator<'mir, 'tcx> {\n                 // No stacks, no tag.\n                 (None, Tag::Untagged)\n             };\n+        let race_alloc = data_race::AllocExtra::new_allocation(&memory_extra.data_race, alloc.size);\n         let mut stacked_borrows = memory_extra.stacked_borrows.as_ref().map(|sb| sb.borrow_mut());\n         let alloc: Allocation<Tag, Self::AllocExtra> = alloc.with_tags_and_extra(\n             |alloc| {\n@@ -478,7 +484,7 @@ impl<'mir, 'tcx> Machine<'mir, 'tcx> for Evaluator<'mir, 'tcx> {\n                     Tag::Untagged\n                 }\n             },\n-            AllocExtra { stacked_borrows: stacks },\n+            AllocExtra { stacked_borrows: stacks, data_race: race_alloc },\n         );\n         (Cow::Owned(alloc), base_tag)\n     }\n@@ -584,6 +590,7 @@ impl AllocationExtra<Tag> for AllocExtra {\n         ptr: Pointer<Tag>,\n         size: Size,\n     ) -> InterpResult<'tcx> {\n+        alloc.extra.data_race.read(ptr, size)?;\n         if let Some(stacked_borrows) = &alloc.extra.stacked_borrows {\n             stacked_borrows.memory_read(ptr, size)\n         } else {\n@@ -597,6 +604,7 @@ impl AllocationExtra<Tag> for AllocExtra {\n         ptr: Pointer<Tag>,\n         size: Size,\n     ) -> InterpResult<'tcx> {\n+        alloc.extra.data_race.write(ptr, size)?;\n         if let Some(stacked_borrows) = &mut alloc.extra.stacked_borrows {\n             stacked_borrows.memory_written(ptr, size)\n         } else {\n@@ -610,6 +618,7 @@ impl AllocationExtra<Tag> for AllocExtra {\n         ptr: Pointer<Tag>,\n         size: Size,\n     ) -> InterpResult<'tcx> {\n+        alloc.extra.data_race.deallocate(ptr, size)?;\n         if let Some(stacked_borrows) = &mut alloc.extra.stacked_borrows {\n             stacked_borrows.memory_deallocated(ptr, size)\n         } else {"}, {"sha": "2bb15e712c5f2130da1c05477de67e8f5cdab648", "filename": "src/shims/intrinsics.rs", "status": "modified", "additions": 274, "deletions": 151, "changes": 425, "blob_url": "https://github.com/rust-lang/rust/blob/89814f1b3f6c239f472dea4798a1189a30d7efa2/src%2Fshims%2Fintrinsics.rs", "raw_url": "https://github.com/rust-lang/rust/raw/89814f1b3f6c239f472dea4798a1189a30d7efa2/src%2Fshims%2Fintrinsics.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Fshims%2Fintrinsics.rs?ref=89814f1b3f6c239f472dea4798a1189a30d7efa2", "patch": "@@ -4,7 +4,7 @@ use log::trace;\n \n use rustc_attr as attr;\n use rustc_ast::ast::FloatTy;\n-use rustc_middle::{mir, ty};\n+use rustc_middle::{mir, mir::BinOp, ty};\n use rustc_middle::ty::layout::IntegerExt;\n use rustc_apfloat::{Float, Round};\n use rustc_target::abi::{Align, Integer, LayoutOf};\n@@ -306,157 +306,117 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: crate::MiriEvalContextExt<'mir, 'tcx\n             }\n \n             // Atomic operations\n-            #[rustfmt::skip]\n-            | \"atomic_load\"\n-            | \"atomic_load_relaxed\"\n-            | \"atomic_load_acq\"\n-            => {\n-                let &[place] = check_arg_count(args)?;\n-                let place = this.deref_operand(place)?;\n-                let val = this.read_scalar(place.into())?; // make sure it fits into a scalar; otherwise it cannot be atomic\n-\n-                // Check alignment requirements. Atomics must always be aligned to their size,\n-                // even if the type they wrap would be less aligned (e.g. AtomicU64 on 32bit must\n-                // be 8-aligned).\n-                let align = Align::from_bytes(place.layout.size.bytes()).unwrap();\n-                this.memory.check_ptr_access(place.ptr, place.layout.size, align)?;\n-\n-                this.write_scalar(val, dest)?;\n-            }\n-\n-            #[rustfmt::skip]\n-            | \"atomic_store\"\n-            | \"atomic_store_relaxed\"\n-            | \"atomic_store_rel\"\n-            => {\n-                let &[place, val] = check_arg_count(args)?;\n-                let place = this.deref_operand(place)?;\n-                let val = this.read_scalar(val)?; // make sure it fits into a scalar; otherwise it cannot be atomic\n+            \"atomic_load\" => this.atomic_load(args, dest, AtomicReadOp::SeqCst)?,\n+            \"atomic_load_relaxed\" => this.atomic_load(args, dest, AtomicReadOp::Relaxed)?,\n+            \"atomic_load_acq\" => this.atomic_load(args, dest, AtomicReadOp::Acquire)?,\n+\n+            \"atomic_store\" => this.atomic_store(args, AtomicWriteOp::SeqCst)?,\n+            \"atomic_store_relaxed\" => this.atomic_store(args, AtomicWriteOp::Relaxed)?,\n+            \"atomic_store_rel\" => this.atomic_store(args, AtomicWriteOp::Release)?,\n+\n+            \"atomic_fence_acq\" => this.atomic_fence(args, AtomicFenceOp::Acquire)?,\n+            \"atomic_fence_rel\" => this.atomic_fence(args, AtomicFenceOp::Release)?,\n+            \"atomic_fence_acqrel\" => this.atomic_fence(args, AtomicFenceOp::AcqRel)?,\n+            \"atomic_fence\" => this.atomic_fence(args, AtomicFenceOp::SeqCst)?,\n+\n+            \"atomic_singlethreadfence_acq\" => this.compiler_fence(args, AtomicFenceOp::Acquire)?,\n+            \"atomic_singlethreadfence_rel\" => this.compiler_fence(args, AtomicFenceOp::Release)?,\n+            \"atomic_singlethreadfence_acqrel\" => this.compiler_fence(args, AtomicFenceOp::AcqRel)?,\n+            \"atomic_singlethreadfence\" => this.compiler_fence(args, AtomicFenceOp::SeqCst)?,\n+\n+            \"atomic_xchg\" => this.atomic_exchange(args, dest, AtomicRWOp::SeqCst)?,\n+            \"atomic_xchg_acq\" => this.atomic_exchange(args, dest, AtomicRWOp::Acquire)?,\n+            \"atomic_xchg_rel\" => this.atomic_exchange(args, dest, AtomicRWOp::Release)?,\n+            \"atomic_xchg_acqrel\" => this.atomic_exchange(args, dest, AtomicRWOp::AcqRel)?,\n+            \"atomic_xchg_relaxed\" => this.atomic_exchange(args, dest, AtomicRWOp::Relaxed)?,\n+\n+            \"atomic_cxchg\" => this.atomic_compare_exchange(\n+                args, dest, AtomicRWOp::SeqCst, AtomicReadOp::SeqCst\n+            )?,\n+            \"atomic_cxchg_acq\" => this.atomic_compare_exchange(\n+                args, dest, AtomicRWOp::Acquire, AtomicReadOp::Acquire\n+            )?,\n+            \"atomic_cxchg_rel\" => this.atomic_compare_exchange(\n+                args, dest, AtomicRWOp::Release, AtomicReadOp::Relaxed\n+            )?,\n+            \"atomic_cxchg_acqrel\" => this.atomic_compare_exchange\n+            (args, dest, AtomicRWOp::AcqRel, AtomicReadOp::Acquire\n+            )?,\n+            \"atomic_cxchg_relaxed\" => this.atomic_compare_exchange(\n+                args, dest, AtomicRWOp::Relaxed, AtomicReadOp::Relaxed\n+            )?,\n+            \"atomic_cxchg_acq_failrelaxed\" => this.atomic_compare_exchange(\n+                args, dest, AtomicRWOp::Acquire, AtomicReadOp::Relaxed\n+            )?,\n+            \"atomic_cxchg_acqrel_failrelaxed\" => this.atomic_compare_exchange(\n+                args, dest, AtomicRWOp::AcqRel, AtomicReadOp::Relaxed\n+            )?,\n+            \"atomic_cxchg_failrelaxed\" => this.atomic_compare_exchange(\n+                args, dest, AtomicRWOp::SeqCst, AtomicReadOp::Relaxed\n+            )?,\n+            \"atomic_cxchg_failacq\" => this.atomic_compare_exchange(\n+                args, dest, AtomicRWOp::SeqCst, AtomicReadOp::Acquire\n+            )?,\n+\n+            \"atomic_cxchgweak\" => this.atomic_compare_exchange_weak(\n+                args, dest, AtomicRWOp::SeqCst, AtomicReadOp::SeqCst\n+            )?,\n+            \"atomic_cxchgweak_acq\" => this.atomic_compare_exchange_weak(\n+                args, dest, AtomicRWOp::Acquire, AtomicReadOp::Acquire\n+            )?,\n+            \"atomic_cxchgweak_rel\" => this.atomic_compare_exchange_weak(\n+                args, dest, AtomicRWOp::Release, AtomicReadOp::Relaxed\n+            )?,\n+            \"atomic_cxchgweak_acqrel\" => this.atomic_compare_exchange_weak(\n+                args, dest, AtomicRWOp::AcqRel, AtomicReadOp::Acquire\n+            )?,\n+            \"atomic_cxchgweak_relaxed\" => this.atomic_compare_exchange_weak(\n+                args, dest, AtomicRWOp::Relaxed, AtomicReadOp::Relaxed\n+            )?,\n+            \"atomic_cxchgweak_acq_failrelaxed\" => this.atomic_compare_exchange_weak(\n+                args, dest, AtomicRWOp::Acquire, AtomicReadOp::Relaxed\n+            )?,\n+            \"atomic_cxchgweak_acqrel_failrelaxed\" => this.atomic_compare_exchange_weak(\n+                args, dest, AtomicRWOp::AcqRel, AtomicReadOp::Relaxed\n+            )?,\n+            \"atomic_cxchgweak_failrelaxed\" => this.atomic_compare_exchange_weak(\n+                args, dest, AtomicRWOp::SeqCst, AtomicReadOp::Relaxed\n+            )?,\n+            \"atomic_cxchgweak_failacq\" => this.atomic_compare_exchange_weak(\n+                args, dest, AtomicRWOp::SeqCst, AtomicReadOp::Acquire\n+            )?,\n+\n+            \"atomic_or\" => this.atomic_op(args, dest, BinOp::BitOr, false, AtomicRWOp::SeqCst)?,\n+            \"atomic_or_acq\" => this.atomic_op(args, dest, BinOp::BitOr, false, AtomicRWOp::Acquire)?,\n+            \"atomic_or_rel\" => this.atomic_op(args, dest, BinOp::BitOr, false, AtomicRWOp::Release)?,\n+            \"atomic_or_acqrel\" => this.atomic_op(args, dest, BinOp::BitOr, false, AtomicRWOp::AcqRel)?,\n+            \"atomic_or_relaxed\" => this.atomic_op(args, dest, BinOp::BitOr, false, AtomicRWOp::Relaxed)?,\n+            \"atomic_xor\" => this.atomic_op(args, dest, BinOp::BitXor, false, AtomicRWOp::SeqCst)?,\n+            \"atomic_xor_acq\" => this.atomic_op(args, dest, BinOp::BitXor, false, AtomicRWOp::Acquire)?,\n+            \"atomic_xor_rel\" => this.atomic_op(args, dest, BinOp::BitXor, false, AtomicRWOp::Release)?,\n+            \"atomic_xor_acqrel\" => this.atomic_op(args, dest, BinOp::BitXor, false, AtomicRWOp::AcqRel)?,\n+            \"atomic_xor_relaxed\" => this.atomic_op(args, dest, BinOp::BitXor, false, AtomicRWOp::Relaxed)?,\n+            \"atomic_and\" => this.atomic_op(args, dest, BinOp::BitAnd, false, AtomicRWOp::SeqCst)?,\n+            \"atomic_and_acq\" => this.atomic_op(args, dest, BinOp::BitAnd, false, AtomicRWOp::Acquire)?,\n+            \"atomic_and_rel\" => this.atomic_op(args, dest, BinOp::BitAnd, false, AtomicRWOp::Release)?,\n+            \"atomic_and_acqrel\" => this.atomic_op(args, dest, BinOp::BitAnd, false, AtomicRWOp::AcqRel)?,\n+            \"atomic_and_relaxed\" => this.atomic_op(args, dest, BinOp::BitAnd, false, AtomicRWOp::Relaxed)?,\n+            \"atomic_nand\" => this.atomic_op(args, dest, BinOp::BitAnd, true, AtomicRWOp::SeqCst)?,\n+            \"atomic_nand_acq\" => this.atomic_op(args, dest, BinOp::BitAnd, true, AtomicRWOp::Acquire)?,\n+            \"atomic_nand_rel\" => this.atomic_op(args, dest, BinOp::BitAnd, true, AtomicRWOp::Release)?,\n+            \"atomic_nand_acqrel\" => this.atomic_op(args, dest, BinOp::BitAnd, true, AtomicRWOp::AcqRel)?,\n+            \"atomic_nand_relaxed\" => this.atomic_op(args, dest, BinOp::BitAnd, true, AtomicRWOp::Relaxed)?,\n+            \"atomic_xadd\" => this.atomic_op(args, dest, BinOp::Add, false, AtomicRWOp::SeqCst)?,\n+            \"atomic_xadd_acq\" => this.atomic_op(args, dest, BinOp::Add, false, AtomicRWOp::Acquire)?,\n+            \"atomic_xadd_rel\" => this.atomic_op(args, dest, BinOp::Add, false, AtomicRWOp::Release)?,\n+            \"atomic_xadd_acqrel\" => this.atomic_op(args, dest, BinOp::Add, false, AtomicRWOp::AcqRel)?,\n+            \"atomic_xadd_relaxed\" => this.atomic_op(args, dest, BinOp::Add, false, AtomicRWOp::Relaxed)?,\n+            \"atomic_xsub\" => this.atomic_op(args, dest, BinOp::Sub, false, AtomicRWOp::SeqCst)?,\n+            \"atomic_xsub_acq\" => this.atomic_op(args, dest, BinOp::Sub, false, AtomicRWOp::Acquire)?,\n+            \"atomic_xsub_rel\" => this.atomic_op(args, dest, BinOp::Sub, false, AtomicRWOp::Release)?,\n+            \"atomic_xsub_acqrel\" => this.atomic_op(args, dest, BinOp::Sub, false, AtomicRWOp::AcqRel)?,\n+            \"atomic_xsub_relaxed\" => this.atomic_op(args, dest, BinOp::Sub, false, AtomicRWOp::Relaxed)?,\n \n-                // Check alignment requirements. Atomics must always be aligned to their size,\n-                // even if the type they wrap would be less aligned (e.g. AtomicU64 on 32bit must\n-                // be 8-aligned).\n-                let align = Align::from_bytes(place.layout.size.bytes()).unwrap();\n-                this.memory.check_ptr_access(place.ptr, place.layout.size, align)?;\n-\n-                this.write_scalar(val, place.into())?;\n-            }\n-\n-            #[rustfmt::skip]\n-            | \"atomic_fence_acq\"\n-            | \"atomic_fence_rel\"\n-            | \"atomic_fence_acqrel\"\n-            | \"atomic_fence\"\n-            | \"atomic_singlethreadfence_acq\"\n-            | \"atomic_singlethreadfence_rel\"\n-            | \"atomic_singlethreadfence_acqrel\"\n-            | \"atomic_singlethreadfence\"\n-            => {\n-                let &[] = check_arg_count(args)?;\n-                // FIXME: this will become relevant once we try to detect data races.\n-            }\n-\n-            _ if intrinsic_name.starts_with(\"atomic_xchg\") => {\n-                let &[place, new] = check_arg_count(args)?;\n-                let place = this.deref_operand(place)?;\n-                let new = this.read_scalar(new)?;\n-                let old = this.read_scalar(place.into())?;\n-\n-                // Check alignment requirements. Atomics must always be aligned to their size,\n-                // even if the type they wrap would be less aligned (e.g. AtomicU64 on 32bit must\n-                // be 8-aligned).\n-                let align = Align::from_bytes(place.layout.size.bytes()).unwrap();\n-                this.memory.check_ptr_access(place.ptr, place.layout.size, align)?;\n-\n-                this.write_scalar(old, dest)?; // old value is returned\n-                this.write_scalar(new, place.into())?;\n-            }\n-\n-            _ if intrinsic_name.starts_with(\"atomic_cxchg\") => {\n-                let &[place, expect_old, new] = check_arg_count(args)?;\n-                let place = this.deref_operand(place)?;\n-                let expect_old = this.read_immediate(expect_old)?; // read as immediate for the sake of `binary_op()`\n-                let new = this.read_scalar(new)?;\n-                let old = this.read_immediate(place.into())?; // read as immediate for the sake of `binary_op()`\n-\n-                // Check alignment requirements. Atomics must always be aligned to their size,\n-                // even if the type they wrap would be less aligned (e.g. AtomicU64 on 32bit must\n-                // be 8-aligned).\n-                let align = Align::from_bytes(place.layout.size.bytes()).unwrap();\n-                this.memory.check_ptr_access(place.ptr, place.layout.size, align)?;\n-\n-                // `binary_op` will bail if either of them is not a scalar.\n-                let eq = this.overflowing_binary_op(mir::BinOp::Eq, old, expect_old)?.0;\n-                let res = Immediate::ScalarPair(old.to_scalar_or_uninit(), eq.into());\n-                // Return old value.\n-                this.write_immediate(res, dest)?;\n-                // Update ptr depending on comparison.\n-                if eq.to_bool()? {\n-                    this.write_scalar(new, place.into())?;\n-                }\n-            }\n-\n-            #[rustfmt::skip]\n-            | \"atomic_or\"\n-            | \"atomic_or_acq\"\n-            | \"atomic_or_rel\"\n-            | \"atomic_or_acqrel\"\n-            | \"atomic_or_relaxed\"\n-            | \"atomic_xor\"\n-            | \"atomic_xor_acq\"\n-            | \"atomic_xor_rel\"\n-            | \"atomic_xor_acqrel\"\n-            | \"atomic_xor_relaxed\"\n-            | \"atomic_and\"\n-            | \"atomic_and_acq\"\n-            | \"atomic_and_rel\"\n-            | \"atomic_and_acqrel\"\n-            | \"atomic_and_relaxed\"\n-            | \"atomic_nand\"\n-            | \"atomic_nand_acq\"\n-            | \"atomic_nand_rel\"\n-            | \"atomic_nand_acqrel\"\n-            | \"atomic_nand_relaxed\"\n-            | \"atomic_xadd\"\n-            | \"atomic_xadd_acq\"\n-            | \"atomic_xadd_rel\"\n-            | \"atomic_xadd_acqrel\"\n-            | \"atomic_xadd_relaxed\"\n-            | \"atomic_xsub\"\n-            | \"atomic_xsub_acq\"\n-            | \"atomic_xsub_rel\"\n-            | \"atomic_xsub_acqrel\"\n-            | \"atomic_xsub_relaxed\"\n-            => {\n-                let &[place, rhs] = check_arg_count(args)?;\n-                let place = this.deref_operand(place)?;\n-                if !place.layout.ty.is_integral() {\n-                    bug!(\"Atomic arithmetic operations only work on integer types\");\n-                }\n-                let rhs = this.read_immediate(rhs)?;\n-                let old = this.read_immediate(place.into())?;\n-\n-                // Check alignment requirements. Atomics must always be aligned to their size,\n-                // even if the type they wrap would be less aligned (e.g. AtomicU64 on 32bit must\n-                // be 8-aligned).\n-                let align = Align::from_bytes(place.layout.size.bytes()).unwrap();\n-                this.memory.check_ptr_access(place.ptr, place.layout.size, align)?;\n-\n-                this.write_immediate(*old, dest)?; // old value is returned\n-                let (op, neg) = match intrinsic_name.split('_').nth(1).unwrap() {\n-                    \"or\" => (mir::BinOp::BitOr, false),\n-                    \"xor\" => (mir::BinOp::BitXor, false),\n-                    \"and\" => (mir::BinOp::BitAnd, false),\n-                    \"xadd\" => (mir::BinOp::Add, false),\n-                    \"xsub\" => (mir::BinOp::Sub, false),\n-                    \"nand\" => (mir::BinOp::BitAnd, true),\n-                    _ => bug!(),\n-                };\n-                // Atomics wrap around on overflow.\n-                let val = this.binary_op(op, old, rhs)?;\n-                let val = if neg { this.unary_op(mir::UnOp::Not, val)? } else { val };\n-                this.write_immediate(*val, place.into())?;\n-            }\n \n             // Query type information\n             \"assert_inhabited\" |\n@@ -498,6 +458,169 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: crate::MiriEvalContextExt<'mir, 'tcx\n         Ok(())\n     }\n \n+    fn atomic_load(\n+        &mut self, args: &[OpTy<'tcx, Tag>], dest: PlaceTy<'tcx, Tag>,\n+        atomic: AtomicReadOp\n+    ) -> InterpResult<'tcx> {\n+        let this = self.eval_context_mut();\n+\n+\n+        let &[place] = check_arg_count(args)?;\n+        let place = this.deref_operand(place)?;\n+\n+        // make sure it fits into a scalar; otherwise it cannot be atomic\n+        let val = this.read_scalar_racy(place)?;\n+        this.validate_atomic_load(place, atomic)?;\n+\n+        // Check alignment requirements. Atomics must always be aligned to their size,\n+        // even if the type they wrap would be less aligned (e.g. AtomicU64 on 32bit must\n+        // be 8-aligned).\n+        let align = Align::from_bytes(place.layout.size.bytes()).unwrap();\n+        this.memory.check_ptr_access(place.ptr, place.layout.size, align)?;\n+        this.write_scalar(val, dest)?;\n+        Ok(())\n+    }\n+\n+    fn atomic_store(&mut self, args: &[OpTy<'tcx, Tag>], atomic: AtomicWriteOp) -> InterpResult<'tcx> {\n+        let this = self.eval_context_mut();\n+\n+        let &[place, val] = check_arg_count(args)?;\n+        let place = this.deref_operand(place)?;\n+        let val = this.read_scalar(val)?; // make sure it fits into a scalar; otherwise it cannot be atomic\n+\n+        // Check alignment requirements. Atomics must always be aligned to their size,\n+        // even if the type they wrap would be less aligned (e.g. AtomicU64 on 32bit must\n+        // be 8-aligned).\n+        let align = Align::from_bytes(place.layout.size.bytes()).unwrap();\n+        this.memory.check_ptr_access(place.ptr, place.layout.size, align)?;\n+\n+        // Perform atomic store\n+        this.write_scalar_racy(val, place)?;\n+\n+        this.validate_atomic_store(place, atomic)?;\n+        Ok(())\n+    }\n+\n+    fn compiler_fence(&mut self, args: &[OpTy<'tcx, Tag>], atomic: AtomicFenceOp) -> InterpResult<'tcx> {\n+        let &[] = check_arg_count(args)?;\n+        let _ = atomic;\n+        //FIXME: compiler fences are currently ignored\n+        Ok(())\n+    }\n+\n+    fn atomic_fence(&mut self, args: &[OpTy<'tcx, Tag>], atomic: AtomicFenceOp) -> InterpResult<'tcx> {\n+        let this = self.eval_context_mut();\n+        let &[] = check_arg_count(args)?;\n+        this.validate_atomic_fence(atomic)?;\n+        Ok(())\n+    }\n+\n+    fn atomic_op(\n+        &mut self, args: &[OpTy<'tcx, Tag>], dest: PlaceTy<'tcx, Tag>,\n+        op: mir::BinOp, neg: bool, atomic: AtomicRWOp\n+    ) -> InterpResult<'tcx> {\n+        let this = self.eval_context_mut();\n+\n+        let &[place, rhs] = check_arg_count(args)?;\n+        let place = this.deref_operand(place)?;\n+        if !place.layout.ty.is_integral() {\n+            bug!(\"Atomic arithmetic operations only work on integer types\");\n+        }\n+        let rhs = this.read_immediate(rhs)?;\n+        let old = this.read_immediate_racy(place)?;\n+\n+        // Check alignment requirements. Atomics must always be aligned to their size,\n+        // even if the type they wrap would be less aligned (e.g. AtomicU64 on 32bit must\n+        // be 8-aligned).\n+        let align = Align::from_bytes(place.layout.size.bytes()).unwrap();\n+        this.memory.check_ptr_access(place.ptr, place.layout.size, align)?;\n+        this.write_immediate(*old, dest)?; // old value is returned\n+\n+        // Atomics wrap around on overflow.\n+        let val = this.binary_op(op, old, rhs)?;\n+        let val = if neg { this.unary_op(mir::UnOp::Not, val)? } else { val };\n+        this.write_immediate_racy(*val, place)?;\n+\n+        this.validate_atomic_rmw(place, atomic)?;\n+        Ok(())\n+    }\n+    \n+    fn atomic_exchange(\n+        &mut self, args: &[OpTy<'tcx, Tag>], dest: PlaceTy<'tcx, Tag>, atomic: AtomicRWOp\n+    ) -> InterpResult<'tcx> {\n+        let this = self.eval_context_mut();\n+\n+        let &[place, new] = check_arg_count(args)?;\n+        let place = this.deref_operand(place)?;\n+        let new = this.read_scalar(new)?;\n+        let old = this.read_scalar_racy(place)?;\n+\n+        // Check alignment requirements. Atomics must always be aligned to their size,\n+        // even if the type they wrap would be less aligned (e.g. AtomicU64 on 32bit must\n+        // be 8-aligned).\n+        let align = Align::from_bytes(place.layout.size.bytes()).unwrap();\n+        this.memory.check_ptr_access(place.ptr, place.layout.size, align)?;\n+\n+        this.write_scalar(old, dest)?; // old value is returned\n+        this.write_scalar_racy(new, place)?;\n+\n+        this.validate_atomic_rmw(place, atomic)?;\n+        Ok(())\n+    }\n+\n+    fn atomic_compare_exchange(\n+        &mut self, args: &[OpTy<'tcx, Tag>], dest: PlaceTy<'tcx, Tag>,\n+        success: AtomicRWOp, fail: AtomicReadOp\n+    ) -> InterpResult<'tcx> {\n+        let this = self.eval_context_mut();\n+\n+        let &[place, expect_old, new] = check_arg_count(args)?;\n+        let place = this.deref_operand(place)?;\n+        let expect_old = this.read_immediate(expect_old)?; // read as immediate for the sake of `binary_op()`\n+        let new = this.read_scalar(new)?;\n+\n+        // Failure ordering cannot be stronger than success ordering, therefore first attempt\n+        //  to read with the failure ordering and if successfull then try again with the success\n+        //  read ordering and write in the success case.\n+        // Read as immediate for the sake of `binary_op()`\n+        let old = this.read_immediate_racy(place)?; \n+\n+        // Check alignment requirements. Atomics must always be aligned to their size,\n+        // even if the type they wrap would be less aligned (e.g. AtomicU64 on 32bit must\n+        // be 8-aligned).\n+        let align = Align::from_bytes(place.layout.size.bytes()).unwrap();\n+        this.memory.check_ptr_access(place.ptr, place.layout.size, align)?;\n+\n+        // `binary_op` will bail if either of them is not a scalar.\n+        let eq = this.overflowing_binary_op(mir::BinOp::Eq, old, expect_old)?.0;\n+        let res = Immediate::ScalarPair(old.to_scalar_or_uninit(), eq.into());\n+\n+        // Return old value.\n+        this.write_immediate(res, dest)?;\n+\n+        // Update ptr depending on comparison.\n+        //  if successful, perform a full rw-atomic validation\n+        //  otherwise treat this as an atomic load with the fail ordering\n+        if eq.to_bool()? {\n+            this.write_scalar_racy(new, place)?;\n+            this.validate_atomic_rmw(place, success)?;\n+        } else {\n+            this.validate_atomic_load(place, fail)?;\n+        }\n+\n+        Ok(())\n+    }\n+\n+    fn atomic_compare_exchange_weak(\n+        &mut self, args: &[OpTy<'tcx, Tag>], dest: PlaceTy<'tcx, Tag>,\n+        success: AtomicRWOp, fail: AtomicReadOp\n+    ) -> InterpResult<'tcx> {\n+\n+        // FIXME: the weak part of this is currently not modelled,\n+        //  it is assumed to always succeed unconditionally.\n+        self.atomic_compare_exchange(args, dest, success, fail)\n+    }\n+\n     fn float_to_int_unchecked<F>(\n         &self,\n         f: F,"}, {"sha": "332e79071a0ab18fa9ef84d6d074e79377f9bf5b", "filename": "src/shims/posix/sync.rs", "status": "modified", "additions": 8, "deletions": 8, "changes": 16, "blob_url": "https://github.com/rust-lang/rust/blob/89814f1b3f6c239f472dea4798a1189a30d7efa2/src%2Fshims%2Fposix%2Fsync.rs", "raw_url": "https://github.com/rust-lang/rust/raw/89814f1b3f6c239f472dea4798a1189a30d7efa2/src%2Fshims%2Fposix%2Fsync.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Fshims%2Fposix%2Fsync.rs?ref=89814f1b3f6c239f472dea4798a1189a30d7efa2", "patch": "@@ -62,7 +62,7 @@ fn mutex_get_kind<'mir, 'tcx: 'mir>(\n     mutex_op: OpTy<'tcx, Tag>,\n ) -> InterpResult<'tcx, ScalarMaybeUninit<Tag>> {\n     let offset = if ecx.pointer_size().bytes() == 8 { 16 } else { 12 };\n-    ecx.read_scalar_at_offset(mutex_op, offset, ecx.machine.layouts.i32)\n+    ecx.read_scalar_at_offset_racy(mutex_op, offset, ecx.machine.layouts.i32)\n }\n \n fn mutex_set_kind<'mir, 'tcx: 'mir>(\n@@ -71,22 +71,22 @@ fn mutex_set_kind<'mir, 'tcx: 'mir>(\n     kind: impl Into<ScalarMaybeUninit<Tag>>,\n ) -> InterpResult<'tcx, ()> {\n     let offset = if ecx.pointer_size().bytes() == 8 { 16 } else { 12 };\n-    ecx.write_scalar_at_offset(mutex_op, offset, kind, ecx.machine.layouts.i32)\n+    ecx.write_scalar_at_offset_racy(mutex_op, offset, kind, ecx.machine.layouts.i32)\n }\n \n fn mutex_get_id<'mir, 'tcx: 'mir>(\n     ecx: &MiriEvalContext<'mir, 'tcx>,\n     mutex_op: OpTy<'tcx, Tag>,\n ) -> InterpResult<'tcx, ScalarMaybeUninit<Tag>> {\n-    ecx.read_scalar_at_offset(mutex_op, 4, ecx.machine.layouts.u32)\n+    ecx.read_scalar_at_offset_racy(mutex_op, 4, ecx.machine.layouts.u32)\n }\n \n fn mutex_set_id<'mir, 'tcx: 'mir>(\n     ecx: &mut MiriEvalContext<'mir, 'tcx>,\n     mutex_op: OpTy<'tcx, Tag>,\n     id: impl Into<ScalarMaybeUninit<Tag>>,\n ) -> InterpResult<'tcx, ()> {\n-    ecx.write_scalar_at_offset(mutex_op, 4, id, ecx.machine.layouts.u32)\n+    ecx.write_scalar_at_offset_racy(mutex_op, 4, id, ecx.machine.layouts.u32)\n }\n \n fn mutex_get_or_create_id<'mir, 'tcx: 'mir>(\n@@ -116,15 +116,15 @@ fn rwlock_get_id<'mir, 'tcx: 'mir>(\n     ecx: &MiriEvalContext<'mir, 'tcx>,\n     rwlock_op: OpTy<'tcx, Tag>,\n ) -> InterpResult<'tcx, ScalarMaybeUninit<Tag>> {\n-    ecx.read_scalar_at_offset(rwlock_op, 4, ecx.machine.layouts.u32)\n+    ecx.read_scalar_at_offset_racy(rwlock_op, 4, ecx.machine.layouts.u32)\n }\n \n fn rwlock_set_id<'mir, 'tcx: 'mir>(\n     ecx: &mut MiriEvalContext<'mir, 'tcx>,\n     rwlock_op: OpTy<'tcx, Tag>,\n     id: impl Into<ScalarMaybeUninit<Tag>>,\n ) -> InterpResult<'tcx, ()> {\n-    ecx.write_scalar_at_offset(rwlock_op, 4, id, ecx.machine.layouts.u32)\n+    ecx.write_scalar_at_offset_racy(rwlock_op, 4, id, ecx.machine.layouts.u32)\n }\n \n fn rwlock_get_or_create_id<'mir, 'tcx: 'mir>(\n@@ -177,15 +177,15 @@ fn cond_get_id<'mir, 'tcx: 'mir>(\n     ecx: &MiriEvalContext<'mir, 'tcx>,\n     cond_op: OpTy<'tcx, Tag>,\n ) -> InterpResult<'tcx, ScalarMaybeUninit<Tag>> {\n-    ecx.read_scalar_at_offset(cond_op, 4, ecx.machine.layouts.u32)\n+    ecx.read_scalar_at_offset_racy(cond_op, 4, ecx.machine.layouts.u32)\n }\n \n fn cond_set_id<'mir, 'tcx: 'mir>(\n     ecx: &mut MiriEvalContext<'mir, 'tcx>,\n     cond_op: OpTy<'tcx, Tag>,\n     id: impl Into<ScalarMaybeUninit<Tag>>,\n ) -> InterpResult<'tcx, ()> {\n-    ecx.write_scalar_at_offset(cond_op, 4, id, ecx.machine.layouts.u32)\n+    ecx.write_scalar_at_offset_racy(cond_op, 4, id, ecx.machine.layouts.u32)\n }\n \n fn cond_get_or_create_id<'mir, 'tcx: 'mir>("}, {"sha": "e420457765b2d8a87cbb2675590bb459fa7724a9", "filename": "src/shims/posix/thread.rs", "status": "modified", "additions": 13, "deletions": 4, "changes": 17, "blob_url": "https://github.com/rust-lang/rust/blob/89814f1b3f6c239f472dea4798a1189a30d7efa2/src%2Fshims%2Fposix%2Fthread.rs", "raw_url": "https://github.com/rust-lang/rust/raw/89814f1b3f6c239f472dea4798a1189a30d7efa2/src%2Fshims%2Fposix%2Fthread.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Fshims%2Fposix%2Fthread.rs?ref=89814f1b3f6c239f472dea4798a1189a30d7efa2", "patch": "@@ -19,21 +19,29 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: crate::MiriEvalContextExt<'mir, 'tcx\n              For example, Miri does not detect data races yet.\",\n         );\n \n+        // Create the new thread\n         let new_thread_id = this.create_thread();\n-        // Also switch to new thread so that we can push the first stackframe.\n-        let old_thread_id = this.set_active_thread(new_thread_id);\n \n+        // Write the current thread-id, switch to the next thread later\n+        //  to treat this write operation as occuring on this thread index\n         let thread_info_place = this.deref_operand(thread)?;\n         this.write_scalar(\n             Scalar::from_uint(new_thread_id.to_u32(), thread_info_place.layout.size),\n             thread_info_place.into(),\n         )?;\n \n+        // Read the function argument that will be sent to the new thread\n+        //  again perform the read before the thread starts executing.\n         let fn_ptr = this.read_scalar(start_routine)?.check_init()?;\n-        let instance = this.memory.get_fn(fn_ptr)?.as_instance()?;\n-\n         let func_arg = this.read_immediate(arg)?;\n \n+        // Also switch to new thread so that we can push the first stackframe.\n+        //  after this all accesses will be treated as occuring in the new thread\n+        let old_thread_id = this.set_active_thread(new_thread_id);\n+\n+        // Perform the function pointer load in the new thread frame\n+        let instance = this.memory.get_fn(fn_ptr)?.as_instance()?;\n+\n         // Note: the returned value is currently ignored (see the FIXME in\n         // pthread_join below) because the Rust standard library does not use\n         // it.\n@@ -47,6 +55,7 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: crate::MiriEvalContextExt<'mir, 'tcx\n             StackPopCleanup::None { cleanup: true },\n         )?;\n \n+        // Restore the old active thread frame\n         this.set_active_thread(old_thread_id);\n \n         Ok(0)"}, {"sha": "3469afdcd276e63e1a1a51b230b0a5a483aa7507", "filename": "src/sync.rs", "status": "modified", "additions": 55, "deletions": 9, "changes": 64, "blob_url": "https://github.com/rust-lang/rust/blob/89814f1b3f6c239f472dea4798a1189a30d7efa2/src%2Fsync.rs", "raw_url": "https://github.com/rust-lang/rust/raw/89814f1b3f6c239f472dea4798a1189a30d7efa2/src%2Fsync.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Fsync.rs?ref=89814f1b3f6c239f472dea4798a1189a30d7efa2", "patch": "@@ -61,6 +61,8 @@ struct Mutex {\n     lock_count: usize,\n     /// The queue of threads waiting for this mutex.\n     queue: VecDeque<ThreadId>,\n+    /// Data race handle\n+    data_race: DataRaceLockHandle\n }\n \n declare_id!(RwLockId);\n@@ -77,6 +79,10 @@ struct RwLock {\n     writer_queue: VecDeque<ThreadId>,\n     /// The queue of reader threads waiting for this lock.\n     reader_queue: VecDeque<ThreadId>,\n+    /// Data race handle for writers\n+    data_race: DataRaceLockHandle,\n+    /// Data race handle for readers\n+    data_race_reader: DataRaceLockHandle,\n }\n \n declare_id!(CondvarId);\n@@ -94,12 +100,14 @@ struct CondvarWaiter {\n #[derive(Default, Debug)]\n struct Condvar {\n     waiters: VecDeque<CondvarWaiter>,\n+    data_race: DataRaceLockHandle,\n }\n \n /// The futex state.\n #[derive(Default, Debug)]\n struct Futex {\n     waiters: VecDeque<FutexWaiter>,\n+    data_race: DataRaceLockHandle,\n }\n \n /// A thread waiting on a futex.\n@@ -205,6 +213,7 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: crate::MiriEvalContextExt<'mir, 'tcx\n             mutex.owner = Some(thread);\n         }\n         mutex.lock_count = mutex.lock_count.checked_add(1).unwrap();\n+        this.memory.extra.data_race.validate_lock_acquire(&mutex.data_race, thread);\n     }\n \n     /// Try unlocking by decreasing the lock count and returning the old lock\n@@ -232,6 +241,7 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: crate::MiriEvalContextExt<'mir, 'tcx\n                 mutex.owner = None;\n                 // The mutex is completely unlocked. Try transfering ownership\n                 // to another thread.\n+                this.memory.extra.data_race.validate_lock_release(&mut mutex.data_race, current_owner);\n                 this.mutex_dequeue_and_lock(id);\n             }\n             Some(old_lock_count)\n@@ -284,15 +294,18 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: crate::MiriEvalContextExt<'mir, 'tcx\n         let this = self.eval_context_mut();\n         assert!(!this.rwlock_is_write_locked(id), \"the lock is write locked\");\n         trace!(\"rwlock_reader_lock: {:?} now also held (one more time) by {:?}\", id, reader);\n-        let count = this.machine.threads.sync.rwlocks[id].readers.entry(reader).or_insert(0);\n+        let rwlock = &mut this.machine.threads.sync.rwlocks[id];\n+        let count = rwlock.readers.entry(reader).or_insert(0);\n         *count = count.checked_add(1).expect(\"the reader counter overflowed\");\n+        this.memory.extra.data_race.validate_lock_acquire(&rwlock.data_race, reader);\n     }\n \n     /// Try read-unlock the lock for `reader` and potentially give the lock to a new owner.\n     /// Returns `true` if succeeded, `false` if this `reader` did not hold the lock.\n     fn rwlock_reader_unlock(&mut self, id: RwLockId, reader: ThreadId) -> bool {\n         let this = self.eval_context_mut();\n-        match this.machine.threads.sync.rwlocks[id].readers.entry(reader) {\n+        let rwlock = &mut this.machine.threads.sync.rwlocks[id];\n+        match rwlock.readers.entry(reader) {\n             Entry::Occupied(mut entry) => {\n                 let count = entry.get_mut();\n                 assert!(*count > 0, \"rwlock locked with count == 0\");\n@@ -306,8 +319,16 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: crate::MiriEvalContextExt<'mir, 'tcx\n             }\n             Entry::Vacant(_) => return false, // we did not even own this lock\n         }\n+        this.memory.extra.data_race.validate_lock_release_shared(&mut rwlock.data_race_reader, reader);\n+\n         // The thread was a reader. If the lock is not held any more, give it to a writer.\n         if this.rwlock_is_locked(id).not() {\n+\n+            // All the readers are finished, so set the writer data-race handle to the value\n+            //  of the union of all reader data race handles, since the set of readers\n+            //  happen-before the writers\n+            let rwlock = &mut this.machine.threads.sync.rwlocks[id];\n+            rwlock.data_race.set_values(&rwlock.data_race_reader);\n             this.rwlock_dequeue_and_lock_writer(id);\n         }\n         true\n@@ -332,7 +353,9 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: crate::MiriEvalContextExt<'mir, 'tcx\n         let this = self.eval_context_mut();\n         assert!(!this.rwlock_is_locked(id), \"the rwlock is already locked\");\n         trace!(\"rwlock_writer_lock: {:?} now held by {:?}\", id, writer);\n-        this.machine.threads.sync.rwlocks[id].writer = Some(writer);\n+        let rwlock = &mut this.machine.threads.sync.rwlocks[id];\n+        rwlock.writer = Some(writer);\n+        this.memory.extra.data_race.validate_lock_acquire(&rwlock.data_race, writer);\n     }\n \n     #[inline]\n@@ -347,6 +370,11 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: crate::MiriEvalContextExt<'mir, 'tcx\n             }\n             rwlock.writer = None;\n             trace!(\"rwlock_writer_unlock: {:?} unlocked by {:?}\", id, expected_writer);\n+            // Release memory to both reader and writer vector clocks\n+            //  since this writer happens-before both the union of readers once they are finished\n+            //  and the next writer\n+            this.memory.extra.data_race.validate_lock_release(&mut rwlock.data_race, current_writer);\n+            this.memory.extra.data_race.validate_lock_release(&mut rwlock.data_race_reader, current_writer);\n             // The thread was a writer.\n             //\n             // We are prioritizing writers here against the readers. As a\n@@ -405,10 +433,18 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: crate::MiriEvalContextExt<'mir, 'tcx\n     /// variable.\n     fn condvar_signal(&mut self, id: CondvarId) -> Option<(ThreadId, MutexId)> {\n         let this = self.eval_context_mut();\n-        this.machine.threads.sync.condvars[id]\n-            .waiters\n+        let current_thread = this.get_active_thread();\n+        let condvar = &mut this.machine.threads.sync.condvars[id];\n+        let data_race = &mut this.memory.extra.data_race;\n+\n+        // Each condvar signal happens-before the end of the condvar wake\n+        data_race.validate_lock_release(&mut condvar.data_race, current_thread);\n+        condvar.waiters\n             .pop_front()\n-            .map(|waiter| (waiter.thread, waiter.mutex))\n+            .map(|waiter| {\n+                data_race.validate_lock_acquire(&mut condvar.data_race, waiter.thread);\n+                (waiter.thread, waiter.mutex)\n+            })\n     }\n \n     #[inline]\n@@ -420,15 +456,25 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: crate::MiriEvalContextExt<'mir, 'tcx\n \n     fn futex_wait(&mut self, addr: Pointer<stacked_borrows::Tag>, thread: ThreadId) {\n         let this = self.eval_context_mut();\n-        let waiters = &mut this.machine.threads.sync.futexes.entry(addr.erase_tag()).or_default().waiters;\n+        let futex = &mut this.machine.threads.sync.futexes.entry(addr.erase_tag()).or_default();\n+        let waiters = &mut futex.waiters;\n         assert!(waiters.iter().all(|waiter| waiter.thread != thread), \"thread is already waiting\");\n         waiters.push_back(FutexWaiter { thread });\n     }\n \n     fn futex_wake(&mut self, addr: Pointer<stacked_borrows::Tag>) -> Option<ThreadId> {\n         let this = self.eval_context_mut();\n-        let waiters = &mut this.machine.threads.sync.futexes.get_mut(&addr.erase_tag())?.waiters;\n-        waiters.pop_front().map(|waiter| waiter.thread)\n+        let current_thread = this.get_active_thread();\n+        let futex = &mut this.machine.threads.sync.futexes.get_mut(&addr.erase_tag())?;\n+        let data_race =  &mut this.memory.extra.data_race;\n+\n+        // Each futex-wake happens-before the end of the futex wait\n+        data_race.validate_lock_release(&mut futex.data_race, current_thread);\n+        let res = futex.waiters.pop_front().map(|waiter| {\n+            data_race.validate_lock_acquire(&futex.data_race, waiter.thread);  \n+            waiter.thread\n+        });\n+        res\n     }\n \n     fn futex_remove_waiter(&mut self, addr: Pointer<stacked_borrows::Tag>, thread: ThreadId) {"}, {"sha": "08aeaa4fd095fc9bfb2ac050c49d9a723cb3de57", "filename": "src/thread.rs", "status": "modified", "additions": 23, "deletions": 7, "changes": 30, "blob_url": "https://github.com/rust-lang/rust/blob/89814f1b3f6c239f472dea4798a1189a30d7efa2/src%2Fthread.rs", "raw_url": "https://github.com/rust-lang/rust/raw/89814f1b3f6c239f472dea4798a1189a30d7efa2/src%2Fthread.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Fthread.rs?ref=89814f1b3f6c239f472dea4798a1189a30d7efa2", "patch": "@@ -327,7 +327,7 @@ impl<'mir, 'tcx: 'mir> ThreadManager<'mir, 'tcx> {\n     }\n \n     /// Mark that the active thread tries to join the thread with `joined_thread_id`.\n-    fn join_thread(&mut self, joined_thread_id: ThreadId) -> InterpResult<'tcx> {\n+    fn join_thread(&mut self, joined_thread_id: ThreadId, data_race: &data_race::GlobalState) -> InterpResult<'tcx> {\n         if self.threads[joined_thread_id].join_status != ThreadJoinStatus::Joinable {\n             throw_ub_format!(\"trying to join a detached or already joined thread\");\n         }\n@@ -351,6 +351,9 @@ impl<'mir, 'tcx: 'mir> ThreadManager<'mir, 'tcx> {\n                 self.active_thread,\n                 joined_thread_id\n             );\n+        }else{\n+            // The thread has already terminated - mark join happens-before\n+            data_race.thread_joined(self.active_thread, joined_thread_id);\n         }\n         Ok(())\n     }\n@@ -425,7 +428,7 @@ impl<'mir, 'tcx: 'mir> ThreadManager<'mir, 'tcx> {\n \n     /// Wakes up threads joining on the active one and deallocates thread-local statics.\n     /// The `AllocId` that can now be freed is returned.\n-    fn thread_terminated(&mut self) -> Vec<AllocId> {\n+    fn thread_terminated(&mut self, data_race: &data_race::GlobalState) -> Vec<AllocId> {\n         let mut free_tls_statics = Vec::new();\n         {\n             let mut thread_local_statics = self.thread_local_alloc_ids.borrow_mut();\n@@ -443,6 +446,8 @@ impl<'mir, 'tcx: 'mir> ThreadManager<'mir, 'tcx> {\n         // Check if we need to unblock any threads.\n         for (i, thread) in self.threads.iter_enumerated_mut() {\n             if thread.state == ThreadState::BlockedOnJoin(self.active_thread) {\n+                // The thread has terminated, mark happens-before edge to joining thread\n+                data_race.thread_joined(i, self.active_thread);\n                 trace!(\"unblocking {:?} because {:?} terminated\", i, self.active_thread);\n                 thread.state = ThreadState::Enabled;\n             }\n@@ -456,7 +461,7 @@ impl<'mir, 'tcx: 'mir> ThreadManager<'mir, 'tcx> {\n     /// used in stateless model checkers such as Loom: run the active thread as\n     /// long as we can and switch only when we have to (the active thread was\n     /// blocked, terminated, or has explicitly asked to be preempted).\n-    fn schedule(&mut self) -> InterpResult<'tcx, SchedulingAction> {\n+    fn schedule(&mut self, data_race: &data_race::GlobalState) -> InterpResult<'tcx, SchedulingAction> {\n         // Check whether the thread has **just** terminated (`check_terminated`\n         // checks whether the thread has popped all its stack and if yes, sets\n         // the thread state to terminated).\n@@ -501,6 +506,7 @@ impl<'mir, 'tcx: 'mir> ThreadManager<'mir, 'tcx> {\n             if thread.state == ThreadState::Enabled {\n                 if !self.yield_active_thread || id != self.active_thread {\n                     self.active_thread = id;\n+                    data_race.thread_set_active(self.active_thread);\n                     break;\n                 }\n             }\n@@ -554,7 +560,9 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: crate::MiriEvalContextExt<'mir, 'tcx\n     #[inline]\n     fn create_thread(&mut self) -> ThreadId {\n         let this = self.eval_context_mut();\n-        this.machine.threads.create_thread()\n+        let id = this.machine.threads.create_thread();\n+        this.memory.extra.data_race.thread_created(id);\n+        id\n     }\n \n     #[inline]\n@@ -566,12 +574,15 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: crate::MiriEvalContextExt<'mir, 'tcx\n     #[inline]\n     fn join_thread(&mut self, joined_thread_id: ThreadId) -> InterpResult<'tcx> {\n         let this = self.eval_context_mut();\n-        this.machine.threads.join_thread(joined_thread_id)\n+        let data_race = &*this.memory.extra.data_race;\n+        this.machine.threads.join_thread(joined_thread_id, data_race)?;\n+        Ok(())\n     }\n \n     #[inline]\n     fn set_active_thread(&mut self, thread_id: ThreadId) -> ThreadId {\n         let this = self.eval_context_mut();\n+        this.memory.extra.data_race.thread_set_active(thread_id);\n         this.machine.threads.set_active_thread_id(thread_id)\n     }\n \n@@ -626,6 +637,9 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: crate::MiriEvalContextExt<'mir, 'tcx\n     #[inline]\n     fn set_active_thread_name(&mut self, new_thread_name: Vec<u8>) {\n         let this = self.eval_context_mut();\n+        if let Ok(string) = String::from_utf8(new_thread_name.clone()) {\n+            this.memory.extra.data_race.thread_set_name(string);\n+        }\n         this.machine.threads.set_thread_name(new_thread_name);\n     }\n \n@@ -695,7 +709,8 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: crate::MiriEvalContextExt<'mir, 'tcx\n     #[inline]\n     fn schedule(&mut self) -> InterpResult<'tcx, SchedulingAction> {\n         let this = self.eval_context_mut();\n-        this.machine.threads.schedule()\n+        let data_race = &*this.memory.extra.data_race;\n+        this.machine.threads.schedule(data_race)\n     }\n \n     /// Handles thread termination of the active thread: wakes up threads joining on this one,\n@@ -705,7 +720,8 @@ pub trait EvalContextExt<'mir, 'tcx: 'mir>: crate::MiriEvalContextExt<'mir, 'tcx\n     #[inline]\n     fn thread_terminated(&mut self) -> InterpResult<'tcx> {\n         let this = self.eval_context_mut();\n-        for alloc_id in this.machine.threads.thread_terminated() {\n+        let data_race = &*this.memory.extra.data_race;\n+        for alloc_id in this.machine.threads.thread_terminated(data_race) {\n             let ptr = this.memory.global_base_pointer(alloc_id.into())?;\n             this.memory.deallocate(ptr, None, MiriMemoryKind::Tls.into())?;\n         }"}]}