{"sha": "00676c8ea20a7310dacc85759daf57eab86ac965", "node_id": "MDY6Q29tbWl0NzI0NzEyOjAwNjc2YzhlYTIwYTczMTBkYWNjODU3NTlkYWY1N2VhYjg2YWM5NjU=", "commit": {"author": {"name": "Piotr Czarnecki", "email": "pioczarn@gmail.com", "date": "2014-11-02T11:21:16Z"}, "committer": {"name": "Piotr Czarnecki", "email": "pioczarn@gmail.com", "date": "2014-11-07T09:21:57Z"}, "message": "Add `ast::SequenceRepetition`", "tree": {"sha": "bb02492e716dbaec7000567d63b8b49fea719d55", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/bb02492e716dbaec7000567d63b8b49fea719d55"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/00676c8ea20a7310dacc85759daf57eab86ac965", "comment_count": 5, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/00676c8ea20a7310dacc85759daf57eab86ac965", "html_url": "https://github.com/rust-lang/rust/commit/00676c8ea20a7310dacc85759daf57eab86ac965", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/00676c8ea20a7310dacc85759daf57eab86ac965/comments", "author": {"login": "pczarn", "id": 3356767, "node_id": "MDQ6VXNlcjMzNTY3Njc=", "avatar_url": "https://avatars.githubusercontent.com/u/3356767?v=4", "gravatar_id": "", "url": "https://api.github.com/users/pczarn", "html_url": "https://github.com/pczarn", "followers_url": "https://api.github.com/users/pczarn/followers", "following_url": "https://api.github.com/users/pczarn/following{/other_user}", "gists_url": "https://api.github.com/users/pczarn/gists{/gist_id}", "starred_url": "https://api.github.com/users/pczarn/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/pczarn/subscriptions", "organizations_url": "https://api.github.com/users/pczarn/orgs", "repos_url": "https://api.github.com/users/pczarn/repos", "events_url": "https://api.github.com/users/pczarn/events{/privacy}", "received_events_url": "https://api.github.com/users/pczarn/received_events", "type": "User", "site_admin": false}, "committer": {"login": "pczarn", "id": 3356767, "node_id": "MDQ6VXNlcjMzNTY3Njc=", "avatar_url": "https://avatars.githubusercontent.com/u/3356767?v=4", "gravatar_id": "", "url": "https://api.github.com/users/pczarn", "html_url": "https://github.com/pczarn", "followers_url": "https://api.github.com/users/pczarn/followers", "following_url": "https://api.github.com/users/pczarn/following{/other_user}", "gists_url": "https://api.github.com/users/pczarn/gists{/gist_id}", "starred_url": "https://api.github.com/users/pczarn/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/pczarn/subscriptions", "organizations_url": "https://api.github.com/users/pczarn/orgs", "repos_url": "https://api.github.com/users/pczarn/repos", "events_url": "https://api.github.com/users/pczarn/events{/privacy}", "received_events_url": "https://api.github.com/users/pczarn/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "964191a313b84785b29b7a33560ae8959f66b582", "url": "https://api.github.com/repos/rust-lang/rust/commits/964191a313b84785b29b7a33560ae8959f66b582", "html_url": "https://github.com/rust-lang/rust/commit/964191a313b84785b29b7a33560ae8959f66b582"}], "stats": {"total": 253, "additions": 160, "deletions": 93}, "files": [{"sha": "3ebbede82ab4aa16a051de9e04a03122d8a26bc5", "filename": "src/libsyntax/ast.rs", "status": "modified", "additions": 63, "deletions": 32, "changes": 95, "blob_url": "https://github.com/rust-lang/rust/blob/00676c8ea20a7310dacc85759daf57eab86ac965/src%2Flibsyntax%2Fast.rs", "raw_url": "https://github.com/rust-lang/rust/raw/00676c8ea20a7310dacc85759daf57eab86ac965/src%2Flibsyntax%2Fast.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fast.rs?ref=00676c8ea20a7310dacc85759daf57eab86ac965", "patch": "@@ -627,6 +627,19 @@ impl Delimited {\n     }\n }\n \n+/// A sequence of token treesee\n+#[deriving(Clone, PartialEq, Eq, Encodable, Decodable, Hash, Show)]\n+pub struct SequenceRepetition {\n+    /// The sequence of token trees\n+    pub tts: Vec<TokenTree>,\n+    /// The optional separator\n+    pub separator: Option<token::Token>,\n+    /// Whether the sequence can be repeated zero (*), or one or more times (+)\n+    pub op: KleeneOp,\n+    /// The number of `MatchNt`s that appear in the sequence (and subsequences)\n+    pub num_captures: uint,\n+}\n+\n /// A Kleene-style [repetition operator](http://en.wikipedia.org/wiki/Kleene_star)\n /// for token sequences.\n #[deriving(Clone, PartialEq, Eq, Encodable, Decodable, Hash, Show)]\n@@ -657,58 +670,76 @@ pub enum TokenTree {\n \n     // This only makes sense in MBE macros.\n \n-    /// A kleene-style repetition sequence with a span, a TT forest,\n-    /// an optional separator, and a boolean where true indicates\n-    /// zero or more (..), and false indicates one or more (+).\n-    /// The last member denotes the number of `MATCH_NONTERMINAL`s\n-    /// in the forest.\n-    // FIXME(eddyb) #12938 Use Rc<[TokenTree]> after DST.\n-    TtSequence(Span, Rc<Vec<TokenTree>>, Option<::parse::token::Token>, KleeneOp, uint),\n+    /// A kleene-style repetition sequence with a span\n+    // FIXME(eddyb) #12938 Use DST.\n+    TtSequence(Span, Rc<SequenceRepetition>),\n }\n \n impl TokenTree {\n-    /// For unrolling some tokens or token trees into equivalent sequences.\n-    pub fn expand_into_tts(self) -> Rc<Vec<TokenTree>> {\n-        match self {\n-            TtToken(sp, token::DocComment(name)) => {\n+    pub fn len(&self) -> uint {\n+        match *self {\n+            TtToken(_, token::DocComment(_)) => 2,\n+            TtToken(_, token::SubstNt(..)) => 2,\n+            TtToken(_, token::MatchNt(..)) => 3,\n+            TtDelimited(_, ref delimed) => {\n+                delimed.tts.len() + 2\n+            }\n+            TtSequence(_, ref seq) => {\n+                seq.tts.len()\n+            }\n+            TtToken(..) => 0\n+        }\n+    }\n+\n+    pub fn get_tt(&self, index: uint) -> TokenTree {\n+        match (self, index) {\n+            (&TtToken(sp, token::DocComment(_)), 0) => {\n+                TtToken(sp, token::Pound)\n+            }\n+            (&TtToken(sp, token::DocComment(name)), 1) => {\n                 let doc = MetaNameValue(token::intern_and_get_ident(\"doc\"),\n                                         respan(sp, LitStr(token::get_name(name), CookedStr)));\n                 let doc = token::NtMeta(P(respan(sp, doc)));\n-                let delimed = Delimited {\n+                TtDelimited(sp, Rc::new(Delimited {\n                     delim: token::Bracket,\n                     open_span: sp,\n                     tts: vec![TtToken(sp, token::Interpolated(doc))],\n                     close_span: sp,\n-                };\n-                Rc::new(vec![TtToken(sp, token::Pound),\n-                             TtDelimited(sp, Rc::new(delimed))])\n+                }))\n             }\n-            TtDelimited(_, ref delimed) => {\n-                let mut tts = Vec::with_capacity(1 + delimed.tts.len() + 1);\n-                tts.push(delimed.open_tt());\n-                tts.extend(delimed.tts.iter().map(|tt| tt.clone()));\n-                tts.push(delimed.close_tt());\n-                Rc::new(tts)\n+            (&TtDelimited(_, ref delimed), _) => {\n+                if index == 0 {\n+                    return delimed.open_tt();\n+                }\n+                if index == delimed.tts.len() + 1 {\n+                    return delimed.close_tt();\n+                }\n+                delimed.tts[index - 1].clone()\n+            }\n+            (&TtToken(sp, token::SubstNt(name, name_st)), _) => {\n+                let v = [TtToken(sp, token::Dollar),\n+                         TtToken(sp, token::Ident(name, name_st))];\n+                v[index]\n             }\n-            TtToken(sp, token::SubstNt(name, namep)) => {\n-                Rc::new(vec![TtToken(sp, token::Dollar),\n-                             TtToken(sp, token::Ident(name, namep))])\n+            (&TtToken(sp, token::MatchNt(name, kind, name_st, kind_st)), _) => {\n+                let v = [TtToken(sp, token::SubstNt(name, name_st)),\n+                         TtToken(sp, token::Colon),\n+                         TtToken(sp, token::Ident(kind, kind_st))];\n+                v[index]\n             }\n-            TtToken(sp, token::MatchNt(name, kind, namep, kindp)) => {\n-                Rc::new(vec![TtToken(sp, token::SubstNt(name, namep)),\n-                             TtToken(sp, token::Colon),\n-                             TtToken(sp, token::Ident(kind, kindp))])\n+            (&TtSequence(_, ref seq), _) => {\n+                seq.tts[index].clone()\n             }\n-            _ => panic!(\"Cannot expand a token\")\n+            _ => panic!(\"Cannot expand a token tree\")\n         }\n     }\n \n     /// Returns the `Span` corresponding to this token tree.\n     pub fn get_span(&self) -> Span {\n         match *self {\n-            TtToken(span, _)              => span,\n-            TtDelimited(span, _)          => span,\n-            TtSequence(span, _, _, _, _)  => span,\n+            TtToken(span, _)     => span,\n+            TtDelimited(span, _) => span,\n+            TtSequence(span, _)  => span,\n         }\n     }\n }"}, {"sha": "1f0b6672594737bfdb33e5073b8ccc6f7e76f029", "filename": "src/libsyntax/ext/tt/macro_parser.rs", "status": "modified", "additions": 46, "deletions": 26, "changes": 72, "blob_url": "https://github.com/rust-lang/rust/blob/00676c8ea20a7310dacc85759daf57eab86ac965/src%2Flibsyntax%2Fext%2Ftt%2Fmacro_parser.rs", "raw_url": "https://github.com/rust-lang/rust/raw/00676c8ea20a7310dacc85759daf57eab86ac965/src%2Flibsyntax%2Fext%2Ftt%2Fmacro_parser.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fext%2Ftt%2Fmacro_parser.rs?ref=00676c8ea20a7310dacc85759daf57eab86ac965", "patch": "@@ -100,17 +100,39 @@ use std::collections::hash_map::{Vacant, Occupied};\n // To avoid costly uniqueness checks, we require that `MatchSeq` always has\n // a nonempty body.\n \n+#[deriving(Clone)]\n+enum TokenTreeOrTokenTreeVec {\n+    Tt(ast::TokenTree),\n+    TtSeq(Rc<Vec<ast::TokenTree>>),\n+}\n+\n+impl TokenTreeOrTokenTreeVec {\n+    fn len(&self) -> uint {\n+        match self {\n+            &TtSeq(ref v) => v.len(),\n+            &Tt(ref tt) => tt.len(),\n+        }\n+    }\n+\n+    fn get_tt(&self, index: uint) -> TokenTree {\n+        match self {\n+            &TtSeq(ref v) => v[index].clone(),\n+            &Tt(ref tt) => tt.get_tt(index),\n+        }\n+    }\n+}\n+\n /// an unzipping of `TokenTree`s\n #[deriving(Clone)]\n struct MatcherTtFrame {\n-    elts: Rc<Vec<ast::TokenTree>>,\n+    elts: TokenTreeOrTokenTreeVec,\n     idx: uint,\n }\n \n #[deriving(Clone)]\n pub struct MatcherPos {\n     stack: Vec<MatcherTtFrame>,\n-    elts: Rc<Vec<ast::TokenTree>>,\n+    top_elts: TokenTreeOrTokenTreeVec,\n     sep: Option<Token>,\n     idx: uint,\n     up: Option<Box<MatcherPos>>,\n@@ -124,8 +146,8 @@ pub struct MatcherPos {\n pub fn count_names(ms: &[TokenTree]) -> uint {\n     ms.iter().fold(0, |count, elt| {\n         count + match elt {\n-            &TtSequence(_, _, _, _, advance_by) => {\n-                advance_by\n+            &TtSequence(_, ref seq) => {\n+                seq.num_captures\n             }\n             &TtDelimited(_, ref delim) => {\n                 count_names(delim.tts.as_slice())\n@@ -144,7 +166,7 @@ pub fn initial_matcher_pos(ms: Rc<Vec<TokenTree>>, sep: Option<Token>, lo: ByteP\n     let matches = Vec::from_fn(match_idx_hi, |_i| Vec::new());\n     box MatcherPos {\n         stack: vec![],\n-        elts: ms,\n+        top_elts: TtSeq(ms),\n         sep: sep,\n         idx: 0u,\n         up: None,\n@@ -183,8 +205,8 @@ pub fn nameize(p_s: &ParseSess, ms: &[TokenTree], res: &[Rc<NamedMatch>])\n     fn n_rec(p_s: &ParseSess, m: &TokenTree, res: &[Rc<NamedMatch>],\n              ret_val: &mut HashMap<Ident, Rc<NamedMatch>>, idx: &mut uint) {\n         match m {\n-            &TtSequence(_, ref more_ms, _, _, _) => {\n-                for next_m in more_ms.iter() {\n+            &TtSequence(_, ref seq) => {\n+                for next_m in seq.tts.iter() {\n                     n_rec(p_s, next_m, res, ret_val, idx)\n                 }\n             }\n@@ -278,18 +300,18 @@ pub fn parse(sess: &ParseSess,\n             };\n \n             // When unzipped trees end, remove them\n-            while ei.idx >= ei.elts.len() {\n+            while ei.idx >= ei.top_elts.len() {\n                 match ei.stack.pop() {\n                     Some(MatcherTtFrame { elts, idx }) => {\n-                        ei.elts = elts;\n+                        ei.top_elts = elts;\n                         ei.idx = idx + 1;\n                     }\n                     None => break\n                 }\n             }\n \n             let idx = ei.idx;\n-            let len = ei.elts.len();\n+            let len = ei.top_elts.len();\n \n             /* at end of sequence */\n             if idx >= len {\n@@ -352,17 +374,16 @@ pub fn parse(sess: &ParseSess,\n                     eof_eis.push(ei);\n                 }\n             } else {\n-                match (*ei.elts)[idx].clone() {\n+                match ei.top_elts.get_tt(idx) {\n                     /* need to descend into sequence */\n-                    TtSequence(_, ref matchers, ref sep, kleene_op, match_num) => {\n-                        if kleene_op == ast::ZeroOrMore {\n+                    TtSequence(sp, seq) => {\n+                        if seq.op == ast::ZeroOrMore {\n                             let mut new_ei = ei.clone();\n-                            new_ei.match_cur += match_num;\n+                            new_ei.match_cur += seq.num_captures;\n                             new_ei.idx += 1u;\n                             //we specifically matched zero repeats.\n-                            for idx in range(ei.match_cur, ei.match_cur + match_num) {\n-                                new_ei.matches[idx]\n-                                      .push(Rc::new(MatchedSeq(Vec::new(), sp)));\n+                            for idx in range(ei.match_cur, ei.match_cur + seq.num_captures) {\n+                                new_ei.matches[idx].push(Rc::new(MatchedSeq(Vec::new(), sp)));\n                             }\n \n                             cur_eis.push(new_ei);\n@@ -372,15 +393,15 @@ pub fn parse(sess: &ParseSess,\n                         let ei_t = ei;\n                         cur_eis.push(box MatcherPos {\n                             stack: vec![],\n-                            elts: matchers.clone(),\n-                            sep: (*sep).clone(),\n+                            sep: seq.separator.clone(),\n                             idx: 0u,\n                             matches: matches,\n                             match_lo: ei_t.match_cur,\n                             match_cur: ei_t.match_cur,\n-                            match_hi: ei_t.match_cur + match_num,\n+                            match_hi: ei_t.match_cur + seq.num_captures,\n                             up: Some(ei_t),\n-                            sp_lo: sp.lo\n+                            sp_lo: sp.lo,\n+                            top_elts: Tt(TtSequence(sp, seq)),\n                         });\n                     }\n                     TtToken(_, MatchNt(..)) => {\n@@ -395,11 +416,10 @@ pub fn parse(sess: &ParseSess,\n                         return Error(sp, \"Cannot transcribe in macro LHS\".into_string())\n                     }\n                     seq @ TtDelimited(..) | seq @ TtToken(_, DocComment(..)) => {\n-                        let tts = seq.expand_into_tts();\n-                        let elts = mem::replace(&mut ei.elts, tts);\n+                        let lower_elts = mem::replace(&mut ei.top_elts, Tt(seq));\n                         let idx = ei.idx;\n                         ei.stack.push(MatcherTtFrame {\n-                            elts: elts,\n+                            elts: lower_elts,\n                             idx: idx,\n                         });\n                         ei.idx = 0;\n@@ -433,7 +453,7 @@ pub fn parse(sess: &ParseSess,\n             if (bb_eis.len() > 0u && next_eis.len() > 0u)\n                 || bb_eis.len() > 1u {\n                 let nts = bb_eis.iter().map(|ei| {\n-                    match (*ei.elts)[ei.idx] {\n+                    match ei.top_elts.get_tt(ei.idx) {\n                       TtToken(_, MatchNt(bind, name, _, _)) => {\n                         (format!(\"{} ('{}')\",\n                                 token::get_ident(name),\n@@ -458,7 +478,7 @@ pub fn parse(sess: &ParseSess,\n                 let mut rust_parser = Parser::new(sess, cfg.clone(), box rdr.clone());\n \n                 let mut ei = bb_eis.pop().unwrap();\n-                match (*ei.elts)[ei.idx] {\n+                match ei.top_elts.get_tt(ei.idx) {\n                   TtToken(_, MatchNt(_, name, _, _)) => {\n                     let name_string = token::get_ident(name);\n                     let match_cur = ei.match_cur;"}, {"sha": "92c68b7a9c7247d0a04cd4128b4b084cb7a56aee", "filename": "src/libsyntax/ext/tt/macro_rules.rs", "status": "modified", "additions": 15, "deletions": 11, "changes": 26, "blob_url": "https://github.com/rust-lang/rust/blob/00676c8ea20a7310dacc85759daf57eab86ac965/src%2Flibsyntax%2Fext%2Ftt%2Fmacro_rules.rs", "raw_url": "https://github.com/rust-lang/rust/raw/00676c8ea20a7310dacc85759daf57eab86ac965/src%2Flibsyntax%2Fext%2Ftt%2Fmacro_rules.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fext%2Ftt%2Fmacro_rules.rs?ref=00676c8ea20a7310dacc85759daf57eab86ac965", "patch": "@@ -233,20 +233,24 @@ pub fn add_new_extension<'cx>(cx: &'cx mut ExtCtxt,\n     let match_rhs_tok = MatchNt(rhs_nm, special_idents::tt, token::Plain, token::Plain);\n     let argument_gram = vec!(\n         TtSequence(DUMMY_SP,\n-                   Rc::new(vec![\n-                       TtToken(DUMMY_SP, match_lhs),\n-                       TtToken(DUMMY_SP, token::FatArrow),\n-                       TtToken(DUMMY_SP, match_rhs)]),\n-                   Some(token::Semi),\n-                   ast::OneOrMore,\n-                   2),\n+                   Rc::new(ast::SequenceRepetition {\n+                       tts: vec![\n+                           TtToken(DUMMY_SP, match_lhs_tok),\n+                           TtToken(DUMMY_SP, token::FatArrow),\n+                           TtToken(DUMMY_SP, match_rhs_tok)],\n+                       separator: Some(token::Semi),\n+                       op: ast::OneOrMore,\n+                       num_captures: 2\n+                   })),\n         //to phase into semicolon-termination instead of\n         //semicolon-separation\n         TtSequence(DUMMY_SP,\n-                   Rc::new(vec![TtToken(DUMMY_SP, token::Semi)]),\n-                   None,\n-                   ast::ZeroOrMore,\n-                   0));\n+                   Rc::new(ast::SequenceRepetition {\n+                       tts: vec![TtToken(DUMMY_SP, token::Semi)],\n+                       separator: None,\n+                       op: ast::ZeroOrMore,\n+                       num_captures: 0\n+                   })));\n \n \n     // Parse the macro_rules! invocation (`none` is for no interpolations):"}, {"sha": "5842afe11ce2ccf67cfdef4c1f9ad5cf34bc3091", "filename": "src/libsyntax/ext/tt/transcribe.rs", "status": "modified", "additions": 17, "deletions": 13, "changes": 30, "blob_url": "https://github.com/rust-lang/rust/blob/00676c8ea20a7310dacc85759daf57eab86ac965/src%2Flibsyntax%2Fext%2Ftt%2Ftranscribe.rs", "raw_url": "https://github.com/rust-lang/rust/raw/00676c8ea20a7310dacc85759daf57eab86ac965/src%2Flibsyntax%2Fext%2Ftt%2Ftranscribe.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fext%2Ftt%2Ftranscribe.rs?ref=00676c8ea20a7310dacc85759daf57eab86ac965", "patch": "@@ -25,7 +25,7 @@ use std::collections::HashMap;\n ///an unzipping of `TokenTree`s\n #[deriving(Clone)]\n struct TtFrame {\n-    forest: Rc<Vec<ast::TokenTree>>,\n+    forest: TokenTree,\n     idx: uint,\n     dotdotdoted: bool,\n     sep: Option<Token>,\n@@ -57,7 +57,11 @@ pub fn new_tt_reader<'a>(sp_diag: &'a SpanHandler,\n     let mut r = TtReader {\n         sp_diag: sp_diag,\n         stack: vec!(TtFrame {\n-            forest: Rc::new(src),\n+            forest: TtSequence(DUMMY_SP, Rc::new(ast::SequenceRepetition {\n+                tts: src,\n+                // doesn't matter. This merely holds the root unzipping.\n+                separator: None, op: ast::ZeroOrMore, num_captures: 0\n+            })),\n             idx: 0,\n             dotdotdoted: false,\n             sep: None,\n@@ -129,8 +133,8 @@ fn lockstep_iter_size(t: &TokenTree, r: &TtReader) -> LockstepIterSize {\n                 size + lockstep_iter_size(tt, r)\n             })\n         },\n-        TtSequence(_, ref tts, _, _, _) => {\n-            tts.iter().fold(LisUnconstrained, |size, tt| {\n+        TtSequence(_, ref seq) => {\n+            seq.tts.iter().fold(LisUnconstrained, |size, tt| {\n                 size + lockstep_iter_size(tt, r)\n             })\n         },\n@@ -202,12 +206,12 @@ pub fn tt_next_token(r: &mut TtReader) -> TokenAndSpan {\n         let t = {\n             let frame = r.stack.last().unwrap();\n             // FIXME(pcwalton): Bad copy.\n-            (*frame.forest)[frame.idx].clone()\n+            frame.forest.get_tt(frame.idx)\n         };\n         match t {\n-            TtSequence(sp, tts, sep, kleene_op, n) => {\n+            TtSequence(sp, seq) => {\n                 // FIXME(pcwalton): Bad copy.\n-                match lockstep_iter_size(&TtSequence(sp, tts.clone(), sep.clone(), kleene_op, n),\n+                match lockstep_iter_size(&TtSequence(sp, seq.clone()),\n                                          r) {\n                     LisUnconstrained => {\n                         r.sp_diag.span_fatal(\n@@ -222,7 +226,7 @@ pub fn tt_next_token(r: &mut TtReader) -> TokenAndSpan {\n                     }\n                     LisConstraint(len, _) => {\n                         if len == 0 {\n-                            if kleene_op == ast::OneOrMore {\n+                            if seq.op == ast::OneOrMore {\n                                 // FIXME #2887 blame invoker\n                                 r.sp_diag.span_fatal(sp.clone(),\n                                                      \"this must repeat at least once\");\n@@ -234,10 +238,10 @@ pub fn tt_next_token(r: &mut TtReader) -> TokenAndSpan {\n                         r.repeat_len.push(len);\n                         r.repeat_idx.push(0);\n                         r.stack.push(TtFrame {\n-                            forest: tts,\n                             idx: 0,\n                             dotdotdoted: true,\n-                            sep: sep.clone()\n+                            sep: seq.separator.clone(),\n+                            forest: TtSequence(sp, seq),\n                         });\n                     }\n                 }\n@@ -247,7 +251,7 @@ pub fn tt_next_token(r: &mut TtReader) -> TokenAndSpan {\n                 match lookup_cur_matched(r, ident) {\n                     None => {\n                         r.stack.push(TtFrame {\n-                            forest: TtToken(sp, SubstNt(ident, namep)).expand_into_tts(),\n+                            forest: TtToken(sp, SubstNt(ident, namep)),\n                             idx: 0,\n                             dotdotdoted: false,\n                             sep: None\n@@ -285,7 +289,7 @@ pub fn tt_next_token(r: &mut TtReader) -> TokenAndSpan {\n             seq @ TtDelimited(..) | seq @ TtToken(_, MatchNt(..)) => {\n                 // do not advance the idx yet\n                 r.stack.push(TtFrame {\n-                   forest: seq.expand_into_tts(),\n+                   forest: seq,\n                    idx: 0,\n                    dotdotdoted: false,\n                    sep: None\n@@ -294,7 +298,7 @@ pub fn tt_next_token(r: &mut TtReader) -> TokenAndSpan {\n             }\n             TtToken(sp, DocComment(name)) if r.desugar_doc_comments => {\n                 r.stack.push(TtFrame {\n-                   forest: TtToken(sp, DocComment(name)).expand_into_tts(),\n+                   forest: TtToken(sp, DocComment(name)),\n                    idx: 0,\n                    dotdotdoted: false,\n                    sep: None"}, {"sha": "1d4ac0edc3639024c8d48f7b4fb0d4c8e4d04053", "filename": "src/libsyntax/fold.rs", "status": "modified", "additions": 6, "deletions": 5, "changes": 11, "blob_url": "https://github.com/rust-lang/rust/blob/00676c8ea20a7310dacc85759daf57eab86ac965/src%2Flibsyntax%2Ffold.rs", "raw_url": "https://github.com/rust-lang/rust/raw/00676c8ea20a7310dacc85759daf57eab86ac965/src%2Flibsyntax%2Ffold.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Ffold.rs?ref=00676c8ea20a7310dacc85759daf57eab86ac965", "patch": "@@ -581,12 +581,13 @@ pub fn noop_fold_tt<T: Folder>(tt: &TokenTree, fld: &mut T) -> TokenTree {\n                             }\n                         ))\n         },\n-        TtSequence(span, ref pattern, ref sep, is_optional, advance_by) =>\n+        TtSequence(span, ref seq) =>\n             TtSequence(span,\n-                       Rc::new(fld.fold_tts(pattern.as_slice())),\n-                       sep.clone().map(|tok| fld.fold_token(tok)),\n-                       is_optional,\n-                       advance_by),\n+                       Rc::new(SequenceRepetition {\n+                           tts: fld.fold_tts(seq.tts.as_slice()),\n+                           separator: seq.separator.clone().map(|tok| fld.fold_token(tok)),\n+                           ..**seq\n+                       })),\n     }\n }\n "}, {"sha": "08a1001a30beb821fe9cebdc06ec3c946b530920", "filename": "src/libsyntax/parse/parser.rs", "status": "modified", "additions": 9, "deletions": 2, "changes": 11, "blob_url": "https://github.com/rust-lang/rust/blob/00676c8ea20a7310dacc85759daf57eab86ac965/src%2Flibsyntax%2Fparse%2Fparser.rs", "raw_url": "https://github.com/rust-lang/rust/raw/00676c8ea20a7310dacc85759daf57eab86ac965/src%2Flibsyntax%2Fparse%2Fparser.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Fparser.rs?ref=00676c8ea20a7310dacc85759daf57eab86ac965", "patch": "@@ -48,7 +48,8 @@ use ast::{StmtExpr, StmtSemi, StmtMac, StructDef, StructField};\n use ast::{StructVariantKind, BiSub};\n use ast::StrStyle;\n use ast::{SelfExplicit, SelfRegion, SelfStatic, SelfValue};\n-use ast::{Delimited, TokenTree, TraitItem, TraitRef, TtDelimited, TtSequence, TtToken};\n+use ast::{Delimited, SequenceRepetition, TokenTree, TraitItem, TraitRef};\n+use ast::{TtDelimited, TtSequence, TtToken};\n use ast::{TupleVariantKind, Ty, Ty_, TyBot};\n use ast::{TypeField, TyFixedLengthVec, TyClosure, TyProc, TyBareFn};\n use ast::{TyTypeof, TyInfer, TypeMethod};\n@@ -2551,7 +2552,13 @@ impl<'a> Parser<'a> {\n                         Spanned { node, .. } => node,\n                     };\n                     let name_num = macro_parser::count_names(seq.as_slice());\n-                    TtSequence(mk_sp(sp.lo, p.span.hi), Rc::new(seq), sep, repeat, name_num)\n+                    TtSequence(mk_sp(sp.lo, p.span.hi),\n+                               Rc::new(SequenceRepetition {\n+                                   tts: seq,\n+                                   separator: sep,\n+                                   op: repeat,\n+                                   num_captures: name_num\n+                               }))\n                 } else {\n                     // A nonterminal that matches or not\n                     let namep = match p.token { token::Ident(_, p) => p, _ => token::Plain };"}, {"sha": "bd4b70bc52cf38405f8981800a7aad816d17bb6a", "filename": "src/libsyntax/print/pprust.rs", "status": "modified", "additions": 4, "deletions": 4, "changes": 8, "blob_url": "https://github.com/rust-lang/rust/blob/00676c8ea20a7310dacc85759daf57eab86ac965/src%2Flibsyntax%2Fprint%2Fpprust.rs", "raw_url": "https://github.com/rust-lang/rust/raw/00676c8ea20a7310dacc85759daf57eab86ac965/src%2Flibsyntax%2Fprint%2Fpprust.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fprint%2Fpprust.rs?ref=00676c8ea20a7310dacc85759daf57eab86ac965", "patch": "@@ -1137,19 +1137,19 @@ impl<'a> State<'a> {\n                 try!(space(&mut self.s));\n                 word(&mut self.s, token_to_string(&delimed.close_token()).as_slice())\n             },\n-            ast::TtSequence(_, ref tts, ref separator, kleene_op, _) => {\n+            ast::TtSequence(_, ref seq) => {\n                 try!(word(&mut self.s, \"$(\"));\n-                for tt_elt in (*tts).iter() {\n+                for tt_elt in seq.tts.iter() {\n                     try!(self.print_tt(tt_elt));\n                 }\n                 try!(word(&mut self.s, \")\"));\n-                match *separator {\n+                match seq.separator {\n                     Some(ref tk) => {\n                         try!(word(&mut self.s, token_to_string(tk).as_slice()));\n                     }\n                     None => {},\n                 }\n-                match kleene_op {\n+                match seq.op {\n                     ast::ZeroOrMore => word(&mut self.s, \"*\"),\n                     ast::OneOrMore => word(&mut self.s, \"+\"),\n                 }"}]}