{"sha": "4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e", "node_id": "MDY6Q29tbWl0NzI0NzEyOjRkNTI2ZTBkMTRiNDNhODc2MjdjZDZhY2E2YzZmNzFhZDFlMDdiNmU=", "commit": {"author": {"name": "bors", "email": "bors@rust-lang.org", "date": "2017-07-05T21:16:34Z"}, "committer": {"name": "bors", "email": "bors@rust-lang.org", "date": "2017-07-05T21:16:34Z"}, "message": "Auto merge of #40939 - jseyfried:proc_macro_api, r=nrc\n\nproc_macro: implement `TokenTree`, `TokenKind`, hygienic `quote!`, and other API\n\nAll new API is gated behind `#![feature(proc_macro)]` and may be used with `#[proc_macro]`, `#[proc_macro_attribute]`, and `#[proc_macro_derive]` procedural macros.\n\nMore specifically, this PR adds the following in `proc_macro`:\n```rust\n// `TokenStream` constructors:\nimpl TokenStream { fn empty() -> TokenStream { ... } }\nimpl From<TokenTree> for TokenStream { ... }\nimpl From<TokenKind> for TokenStream { ... }\nimpl<T: Into<TokenStream>> FromIterator<T> for TokenStream { ... }\nmacro quote($($t:tt)*) { ... } // A hygienic `TokenStream` quoter\n\n// `TokenStream` destructuring:\nimpl TokenStream { fn is_empty(&self) -> bool { ... } }\nimpl IntoIterator for TokenStream { type Item = TokenTree; ... }\n\nstruct TokenTree { span: Span, kind: TokenKind }\nimpl From<TokenKind> for TokenTree { ... }\nimpl Display for TokenTree { ... }\n\nstruct Span { ... } // a region of source code along with expansion/hygiene information\nimpl Default for Span { ... } // a span from the current procedural macro definition\nimpl Span { fn call_site() -> Span { ... } } // the call site of the current expansion\nfn quote_span(span: Span) -> TokenStream;\n\nenum TokenKind {\n    Group(Delimiter, TokenStream), // A delimited sequence, e.g. `( ... )`\n    Term(Term), // a unicode identifier, lifetime ('a), or underscore\n    Op(char, Spacing), // a punctuation character (`+`, `,`, `$`, etc.).\n    Literal(Literal), // a literal character (`'a'`), string (`\"hello\"`), or number (`2.3`)\n}\n\nenum Delimiter {\n    Parenthesis, // `( ... )`\n    Brace, // `[ ... ]`\n    Bracket, // `{ ... }`\n    None, // an implicit delimiter, e.g. `$var`, where $var is  `...`.\n}\n\nstruct Term { ... } // An interned string\nimpl Term {\n    fn intern(string: &str) -> Symbol { ... }\n    fn as_str(&self) -> &str { ... }\n}\n\nenum Spacing {\n    Alone, // not immediately followed by another `Op`, e.g. `+` in `+ =`.\n    Joint, // immediately followed by another `Op`, e.g. `+` in `+=`\n}\n\nstruct Literal { ... }\nimpl Display for Literal { ... }\nimpl Literal {\n    fn integer(n: i128) -> Literal { .. } // unsuffixed integer literal\n    fn float(n: f64) -> Literal { .. } // unsuffixed floating point literal\n    fn u8(n: u8) -> Literal { ... } // similarly: i8, u16, i16, u32, i32, u64, i64, f32, f64\n    fn string(string: &str) -> Literal { ... }\n    fn character(ch: char) -> Literal { ... }\n    fn byte_string(bytes: &[u8]) -> Literal { ... }\n}\n```\nFor details on `quote!` hygiene, see [this example](https://github.com/rust-lang/rust/pull/40939/commits/20a90485c040df87a667e9b6ee38e4d8a7d7fc5d) and [declarative macros 2.0](https://github.com/rust-lang/rust/pull/40847).\n\nr? @nrc", "tree": {"sha": "93ba1e7852501c25569fb7f614b884052207afb8", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/93ba1e7852501c25569fb7f614b884052207afb8"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e", "html_url": "https://github.com/rust-lang/rust/commit/4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e/comments", "author": {"login": "bors", "id": 3372342, "node_id": "MDQ6VXNlcjMzNzIzNDI=", "avatar_url": "https://avatars.githubusercontent.com/u/3372342?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bors", "html_url": "https://github.com/bors", "followers_url": "https://api.github.com/users/bors/followers", "following_url": "https://api.github.com/users/bors/following{/other_user}", "gists_url": "https://api.github.com/users/bors/gists{/gist_id}", "starred_url": "https://api.github.com/users/bors/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bors/subscriptions", "organizations_url": "https://api.github.com/users/bors/orgs", "repos_url": "https://api.github.com/users/bors/repos", "events_url": "https://api.github.com/users/bors/events{/privacy}", "received_events_url": "https://api.github.com/users/bors/received_events", "type": "User", "site_admin": false}, "committer": {"login": "bors", "id": 3372342, "node_id": "MDQ6VXNlcjMzNzIzNDI=", "avatar_url": "https://avatars.githubusercontent.com/u/3372342?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bors", "html_url": "https://github.com/bors", "followers_url": "https://api.github.com/users/bors/followers", "following_url": "https://api.github.com/users/bors/following{/other_user}", "gists_url": "https://api.github.com/users/bors/gists{/gist_id}", "starred_url": "https://api.github.com/users/bors/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bors/subscriptions", "organizations_url": "https://api.github.com/users/bors/orgs", "repos_url": "https://api.github.com/users/bors/repos", "events_url": "https://api.github.com/users/bors/events{/privacy}", "received_events_url": "https://api.github.com/users/bors/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "3610a70ce488953c5b0379fece70f2baad30a825", "url": "https://api.github.com/repos/rust-lang/rust/commits/3610a70ce488953c5b0379fece70f2baad30a825", "html_url": "https://github.com/rust-lang/rust/commit/3610a70ce488953c5b0379fece70f2baad30a825"}, {"sha": "78fdbfc4008b52bcce201fd589ed84d2abb0419d", "url": "https://api.github.com/repos/rust-lang/rust/commits/78fdbfc4008b52bcce201fd589ed84d2abb0419d", "html_url": "https://github.com/rust-lang/rust/commit/78fdbfc4008b52bcce201fd589ed84d2abb0419d"}], "stats": {"total": 2404, "additions": 1562, "deletions": 842}, "files": [{"sha": "dc94ee27e930d93265e9c9e6842d347841bb203c", "filename": "src/Cargo.lock", "status": "modified", "additions": 0, "deletions": 9, "changes": 9, "blob_url": "https://github.com/rust-lang/rust/blob/4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e/src%2FCargo.lock", "raw_url": "https://github.com/rust-lang/rust/raw/4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e/src%2FCargo.lock", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2FCargo.lock?ref=4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e", "patch": "@@ -883,14 +883,6 @@ name = \"proc_macro\"\n version = \"0.0.0\"\n dependencies = [\n  \"syntax 0.0.0\",\n-]\n-\n-[[package]]\n-name = \"proc_macro_plugin\"\n-version = \"0.0.0\"\n-dependencies = [\n- \"rustc_plugin 0.0.0\",\n- \"syntax 0.0.0\",\n  \"syntax_pos 0.0.0\",\n ]\n \n@@ -1203,7 +1195,6 @@ dependencies = [\n  \"env_logger 0.4.3 (registry+https://github.com/rust-lang/crates.io-index)\",\n  \"graphviz 0.0.0\",\n  \"log 0.3.8 (registry+https://github.com/rust-lang/crates.io-index)\",\n- \"proc_macro_plugin 0.0.0\",\n  \"rustc 0.0.0\",\n  \"rustc_back 0.0.0\",\n  \"rustc_borrowck 0.0.0\","}, {"sha": "5a3785b1ed63475160e824624b76c085d09de928", "filename": "src/bootstrap/lib.rs", "status": "modified", "additions": 3, "deletions": 1, "changes": 4, "blob_url": "https://github.com/rust-lang/rust/blob/4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e/src%2Fbootstrap%2Flib.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e/src%2Fbootstrap%2Flib.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Fbootstrap%2Flib.rs?ref=4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e", "patch": "@@ -482,7 +482,9 @@ impl Build {\n             }\n         }\n \n-        if self.config.extended && compiler.is_final_stage(self) {\n+        if mode == Mode::Libstd &&\n+           self.config.extended &&\n+           compiler.is_final_stage(self) {\n             cargo.env(\"RUSTC_SAVE_ANALYSIS\", \"api\".to_string());\n         }\n "}, {"sha": "19e7f663c7ac33228c76f4c17a92c8a1b0f47222", "filename": "src/doc/unstable-book/src/library-features/proc-macro.md", "status": "added", "additions": 7, "deletions": 0, "changes": 7, "blob_url": "https://github.com/rust-lang/rust/blob/4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e/src%2Fdoc%2Funstable-book%2Fsrc%2Flibrary-features%2Fproc-macro.md", "raw_url": "https://github.com/rust-lang/rust/raw/4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e/src%2Fdoc%2Funstable-book%2Fsrc%2Flibrary-features%2Fproc-macro.md", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Fdoc%2Funstable-book%2Fsrc%2Flibrary-features%2Fproc-macro.md?ref=4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e", "patch": "@@ -0,0 +1,7 @@\n+# `proc_macro`\n+\n+The tracking issue for this feature is: [#38356]\n+\n+[#38356]: https://github.com/rust-lang/rust/issues/38356\n+\n+------------------------"}, {"sha": "1b5141773a96719549892f3b2edba0fedbe7f298", "filename": "src/libproc_macro/Cargo.toml", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "blob_url": "https://github.com/rust-lang/rust/blob/4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e/src%2Flibproc_macro%2FCargo.toml", "raw_url": "https://github.com/rust-lang/rust/raw/4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e/src%2Flibproc_macro%2FCargo.toml", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibproc_macro%2FCargo.toml?ref=4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e", "patch": "@@ -9,3 +9,4 @@ crate-type = [\"dylib\"]\n \n [dependencies]\n syntax = { path = \"../libsyntax\" }\n+syntax_pos = { path = \"../libsyntax_pos\" }"}, {"sha": "06f9634d70613e2a47fe4b7d5a246087aff9a865", "filename": "src/libproc_macro/lib.rs", "status": "modified", "additions": 532, "deletions": 52, "changes": 584, "blob_url": "https://github.com/rust-lang/rust/blob/4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e/src%2Flibproc_macro%2Flib.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e/src%2Flibproc_macro%2Flib.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibproc_macro%2Flib.rs?ref=4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e", "patch": "@@ -37,18 +37,26 @@\n        test(no_crate_inject, attr(deny(warnings))),\n        test(attr(allow(dead_code, deprecated, unused_variables, unused_mut))))]\n \n+#![feature(i128_type)]\n #![feature(rustc_private)]\n #![feature(staged_api)]\n #![feature(lang_items)]\n \n+#[macro_use]\n extern crate syntax;\n+extern crate syntax_pos;\n \n-use std::fmt;\n+use std::{ascii, fmt, iter};\n use std::str::FromStr;\n \n+use syntax::ast;\n use syntax::errors::DiagnosticBuilder;\n-use syntax::parse;\n-use syntax::tokenstream::TokenStream as TokenStream_;\n+use syntax::parse::{self, token, parse_stream_from_source_str};\n+use syntax::print::pprust;\n+use syntax::symbol::Symbol;\n+use syntax::tokenstream;\n+use syntax_pos::DUMMY_SP;\n+use syntax_pos::SyntaxContext;\n \n /// The main type provided by this crate, representing an abstract stream of\n /// tokens.\n@@ -60,17 +68,508 @@ use syntax::tokenstream::TokenStream as TokenStream_;\n /// The API of this type is intentionally bare-bones, but it'll be expanded over\n /// time!\n #[stable(feature = \"proc_macro_lib\", since = \"1.15.0\")]\n-pub struct TokenStream {\n-    inner: TokenStream_,\n-}\n+#[derive(Clone, Debug)]\n+pub struct TokenStream(tokenstream::TokenStream);\n \n /// Error returned from `TokenStream::from_str`.\n-#[derive(Debug)]\n #[stable(feature = \"proc_macro_lib\", since = \"1.15.0\")]\n+#[derive(Debug)]\n pub struct LexError {\n     _inner: (),\n }\n \n+#[stable(feature = \"proc_macro_lib\", since = \"1.15.0\")]\n+impl FromStr for TokenStream {\n+    type Err = LexError;\n+\n+    fn from_str(src: &str) -> Result<TokenStream, LexError> {\n+        __internal::with_sess(|(sess, mark)| {\n+            let src = src.to_string();\n+            let name = \"<proc-macro source code>\".to_string();\n+            let call_site = mark.expn_info().unwrap().call_site;\n+            let stream = parse::parse_stream_from_source_str(name, src, sess, Some(call_site));\n+            Ok(__internal::token_stream_wrap(stream))\n+        })\n+    }\n+}\n+\n+#[stable(feature = \"proc_macro_lib\", since = \"1.15.0\")]\n+impl fmt::Display for TokenStream {\n+    fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {\n+        self.0.fmt(f)\n+    }\n+}\n+\n+/// `quote!(..)` accepts arbitrary tokens and expands into a `TokenStream` describing the input.\n+/// For example, `quote!(a + b)` will produce a expression, that, when evaluated, constructs\n+/// constructs the `TokenStream` `[Word(\"a\"), Op('+', Alone), Word(\"b\")]`.\n+///\n+/// Unquoting is done with `$`, and works by taking the single next ident as the unquoted term.\n+/// To quote `$` itself, use `$$`.\n+#[unstable(feature = \"proc_macro\", issue = \"38356\")]\n+#[macro_export]\n+macro_rules! quote { () => {} }\n+\n+#[unstable(feature = \"proc_macro_internals\", issue = \"27812\")]\n+#[doc(hidden)]\n+mod quote;\n+\n+#[unstable(feature = \"proc_macro\", issue = \"38356\")]\n+impl From<TokenTree> for TokenStream {\n+    fn from(tree: TokenTree) -> TokenStream {\n+        TokenStream(tree.to_internal())\n+    }\n+}\n+\n+#[unstable(feature = \"proc_macro\", issue = \"38356\")]\n+impl From<TokenNode> for TokenStream {\n+    fn from(kind: TokenNode) -> TokenStream {\n+        TokenTree::from(kind).into()\n+    }\n+}\n+\n+#[unstable(feature = \"proc_macro\", issue = \"38356\")]\n+impl<T: Into<TokenStream>> iter::FromIterator<T> for TokenStream {\n+    fn from_iter<I: IntoIterator<Item = T>>(streams: I) -> Self {\n+        let mut builder = tokenstream::TokenStreamBuilder::new();\n+        for stream in streams {\n+            builder.push(stream.into().0);\n+        }\n+        TokenStream(builder.build())\n+    }\n+}\n+\n+#[unstable(feature = \"proc_macro\", issue = \"38356\")]\n+impl IntoIterator for TokenStream {\n+    type Item = TokenTree;\n+    type IntoIter = TokenTreeIter;\n+\n+    fn into_iter(self) -> TokenTreeIter {\n+        TokenTreeIter { cursor: self.0.trees(), next: None }\n+    }\n+}\n+\n+impl TokenStream {\n+    /// Returns an empty `TokenStream`.\n+    #[unstable(feature = \"proc_macro\", issue = \"38356\")]\n+    pub fn empty() -> TokenStream {\n+        TokenStream(tokenstream::TokenStream::empty())\n+    }\n+\n+    /// Checks if this `TokenStream` is empty.\n+    #[unstable(feature = \"proc_macro\", issue = \"38356\")]\n+    pub fn is_empty(&self) -> bool {\n+        self.0.is_empty()\n+    }\n+}\n+\n+/// A region of source code, along with macro expansion information.\n+#[unstable(feature = \"proc_macro\", issue = \"38356\")]\n+#[derive(Copy, Clone, Debug)]\n+pub struct Span(syntax_pos::Span);\n+\n+#[unstable(feature = \"proc_macro\", issue = \"38356\")]\n+impl Default for Span {\n+    fn default() -> Span {\n+        ::__internal::with_sess(|(_, mark)| Span(syntax_pos::Span {\n+            ctxt: SyntaxContext::empty().apply_mark(mark),\n+            ..mark.expn_info().unwrap().call_site\n+        }))\n+    }\n+}\n+\n+/// Quote a `Span` into a `TokenStream`.\n+/// This is needed to implement a custom quoter.\n+#[unstable(feature = \"proc_macro\", issue = \"38356\")]\n+pub fn quote_span(span: Span) -> TokenStream {\n+    TokenStream(quote::Quote::quote(&span.0))\n+}\n+\n+impl Span {\n+    /// The span of the invocation of the current procedural macro.\n+    #[unstable(feature = \"proc_macro\", issue = \"38356\")]\n+    pub fn call_site() -> Span {\n+        ::__internal::with_sess(|(_, mark)| Span(mark.expn_info().unwrap().call_site))\n+    }\n+}\n+\n+/// A single token or a delimited sequence of token trees (e.g. `[1, (), ..]`).\n+#[unstable(feature = \"proc_macro\", issue = \"38356\")]\n+#[derive(Clone, Debug)]\n+pub struct TokenTree {\n+    /// The `TokenTree`'s span\n+    pub span: Span,\n+    /// Description of the `TokenTree`\n+    pub kind: TokenNode,\n+}\n+\n+#[unstable(feature = \"proc_macro\", issue = \"38356\")]\n+impl From<TokenNode> for TokenTree {\n+    fn from(kind: TokenNode) -> TokenTree {\n+        TokenTree { span: Span::default(), kind: kind }\n+    }\n+}\n+\n+#[unstable(feature = \"proc_macro\", issue = \"38356\")]\n+impl fmt::Display for TokenTree {\n+    fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {\n+        TokenStream::from(self.clone()).fmt(f)\n+    }\n+}\n+\n+/// Description of a `TokenTree`\n+#[derive(Clone, Debug)]\n+#[unstable(feature = \"proc_macro\", issue = \"38356\")]\n+pub enum TokenNode {\n+    /// A delimited tokenstream.\n+    Group(Delimiter, TokenStream),\n+    /// A unicode identifier.\n+    Term(Term),\n+    /// A punctuation character (`+`, `,`, `$`, etc.).\n+    Op(char, Spacing),\n+    /// A literal character (`'a'`), string (`\"hello\"`), or number (`2.3`).\n+    Literal(Literal),\n+}\n+\n+/// Describes how a sequence of token trees is delimited.\n+#[derive(Copy, Clone, Debug)]\n+#[unstable(feature = \"proc_macro\", issue = \"38356\")]\n+pub enum Delimiter {\n+    /// `( ... )`\n+    Parenthesis,\n+    /// `[ ... ]`\n+    Brace,\n+    /// `{ ... }`\n+    Bracket,\n+    /// An implicit delimiter, e.g. `$var`, where $var is  `...`.\n+    None,\n+}\n+\n+/// An interned string.\n+#[derive(Copy, Clone, Debug)]\n+#[unstable(feature = \"proc_macro\", issue = \"38356\")]\n+pub struct Term(Symbol);\n+\n+impl Term {\n+    /// Intern a string into a `Term`.\n+    #[unstable(feature = \"proc_macro\", issue = \"38356\")]\n+    pub fn intern(string: &str) -> Term {\n+        Term(Symbol::intern(string))\n+    }\n+\n+    /// Get a reference to the interned string.\n+    #[unstable(feature = \"proc_macro\", issue = \"38356\")]\n+    pub fn as_str(&self) -> &str {\n+        unsafe { &*(&*self.0.as_str() as *const str) }\n+    }\n+}\n+\n+/// Whether an `Op` is either followed immediately by another `Op` or followed by whitespace.\n+#[derive(Copy, Clone, Debug)]\n+#[unstable(feature = \"proc_macro\", issue = \"38356\")]\n+pub enum Spacing {\n+    /// e.g. `+` is `Alone` in `+ =`.\n+    Alone,\n+    /// e.g. `+` is `Joint` in `+=`.\n+    Joint,\n+}\n+\n+/// A literal character (`'a'`), string (`\"hello\"`), or number (`2.3`).\n+#[derive(Clone, Debug)]\n+#[unstable(feature = \"proc_macro\", issue = \"38356\")]\n+pub struct Literal(token::Token);\n+\n+#[unstable(feature = \"proc_macro\", issue = \"38356\")]\n+impl fmt::Display for Literal {\n+    fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {\n+        TokenTree { kind: TokenNode::Literal(self.clone()), span: Span(DUMMY_SP) }.fmt(f)\n+    }\n+}\n+\n+macro_rules! int_literals {\n+    ($($int_kind:ident),*) => {$(\n+        /// Integer literal.\n+        #[unstable(feature = \"proc_macro\", issue = \"38356\")]\n+        pub fn $int_kind(n: $int_kind) -> Literal {\n+            Literal::typed_integer(n as i128, stringify!($int_kind))\n+        }\n+    )*}\n+}\n+\n+impl Literal {\n+    /// Integer literal\n+    #[unstable(feature = \"proc_macro\", issue = \"38356\")]\n+    pub fn integer(n: i128) -> Literal {\n+        Literal(token::Literal(token::Lit::Integer(Symbol::intern(&n.to_string())), None))\n+    }\n+\n+    int_literals!(u8, i8, u16, i16, u32, i32, u64, i64);\n+    fn typed_integer(n: i128, kind: &'static str) -> Literal {\n+        Literal(token::Literal(token::Lit::Integer(Symbol::intern(&n.to_string())),\n+                               Some(Symbol::intern(kind))))\n+    }\n+\n+    /// Floating point literal.\n+    #[unstable(feature = \"proc_macro\", issue = \"38356\")]\n+    pub fn float(n: f64) -> Literal {\n+        if !n.is_finite() {\n+            panic!(\"Invalid float literal {}\", n);\n+        }\n+        Literal(token::Literal(token::Lit::Float(Symbol::intern(&n.to_string())), None))\n+    }\n+\n+    /// Floating point literal.\n+    #[unstable(feature = \"proc_macro\", issue = \"38356\")]\n+    pub fn f32(n: f32) -> Literal {\n+        if !n.is_finite() {\n+            panic!(\"Invalid f32 literal {}\", n);\n+        }\n+        Literal(token::Literal(token::Lit::Float(Symbol::intern(&n.to_string())),\n+                               Some(Symbol::intern(\"f32\"))))\n+    }\n+\n+    /// Floating point literal.\n+    #[unstable(feature = \"proc_macro\", issue = \"38356\")]\n+    pub fn f64(n: f64) -> Literal {\n+        if !n.is_finite() {\n+            panic!(\"Invalid f64 literal {}\", n);\n+        }\n+        Literal(token::Literal(token::Lit::Float(Symbol::intern(&n.to_string())),\n+                               Some(Symbol::intern(\"f64\"))))\n+    }\n+\n+    /// String literal.\n+    #[unstable(feature = \"proc_macro\", issue = \"38356\")]\n+    pub fn string(string: &str) -> Literal {\n+        let mut escaped = String::new();\n+        for ch in string.chars() {\n+            escaped.extend(ch.escape_unicode());\n+        }\n+        Literal(token::Literal(token::Lit::Str_(Symbol::intern(&escaped)), None))\n+    }\n+\n+    /// Character literal.\n+    #[unstable(feature = \"proc_macro\", issue = \"38356\")]\n+    pub fn character(ch: char) -> Literal {\n+        let mut escaped = String::new();\n+        escaped.extend(ch.escape_unicode());\n+        Literal(token::Literal(token::Lit::Char(Symbol::intern(&escaped)), None))\n+    }\n+\n+    /// Byte string literal.\n+    #[unstable(feature = \"proc_macro\", issue = \"38356\")]\n+    pub fn byte_string(bytes: &[u8]) -> Literal {\n+        let string = bytes.iter().cloned().flat_map(ascii::escape_default)\n+            .map(Into::<char>::into).collect::<String>();\n+        Literal(token::Literal(token::Lit::ByteStr(Symbol::intern(&string)), None))\n+    }\n+}\n+\n+/// An iterator over `TokenTree`s.\n+#[unstable(feature = \"proc_macro\", issue = \"38356\")]\n+pub struct TokenTreeIter {\n+    cursor: tokenstream::Cursor,\n+    next: Option<tokenstream::TokenStream>,\n+}\n+\n+#[unstable(feature = \"proc_macro\", issue = \"38356\")]\n+impl Iterator for TokenTreeIter {\n+    type Item = TokenTree;\n+\n+    fn next(&mut self) -> Option<TokenTree> {\n+        loop {\n+            let next =\n+                unwrap_or!(self.next.take().or_else(|| self.cursor.next_as_stream()), return None);\n+            let tree = TokenTree::from_internal(next, &mut self.next);\n+            if tree.span.0 == DUMMY_SP {\n+                if let TokenNode::Group(Delimiter::None, stream) = tree.kind {\n+                    self.cursor.insert(stream.0);\n+                    continue\n+                }\n+            }\n+            return Some(tree);\n+        }\n+    }\n+}\n+\n+impl Delimiter {\n+    fn from_internal(delim: token::DelimToken) -> Delimiter {\n+        match delim {\n+            token::Paren => Delimiter::Parenthesis,\n+            token::Brace => Delimiter::Brace,\n+            token::Bracket => Delimiter::Bracket,\n+            token::NoDelim => Delimiter::None,\n+        }\n+    }\n+\n+    fn to_internal(self) -> token::DelimToken {\n+        match self {\n+            Delimiter::Parenthesis => token::Paren,\n+            Delimiter::Brace => token::Brace,\n+            Delimiter::Bracket => token::Bracket,\n+            Delimiter::None => token::NoDelim,\n+        }\n+    }\n+}\n+\n+impl TokenTree {\n+    fn from_internal(stream: tokenstream::TokenStream, next: &mut Option<tokenstream::TokenStream>)\n+                -> TokenTree {\n+        use syntax::parse::token::*;\n+\n+        let (tree, is_joint) = stream.as_tree();\n+        let (mut span, token) = match tree {\n+            tokenstream::TokenTree::Token(span, token) => (span, token),\n+            tokenstream::TokenTree::Delimited(span, delimed) => {\n+                let delimiter = Delimiter::from_internal(delimed.delim);\n+                return TokenTree {\n+                    span: Span(span),\n+                    kind: TokenNode::Group(delimiter, TokenStream(delimed.tts.into())),\n+                };\n+            }\n+        };\n+\n+        let op_kind = if is_joint { Spacing::Joint } else { Spacing::Alone };\n+        macro_rules! op {\n+            ($op:expr) => { TokenNode::Op($op, op_kind) }\n+        }\n+\n+        macro_rules! joint {\n+            ($first:expr, $rest:expr) => { joint($first, $rest, is_joint, &mut span, next) }\n+        }\n+\n+        fn joint(first: char, rest: Token, is_joint: bool, span: &mut syntax_pos::Span,\n+                 next: &mut Option<tokenstream::TokenStream>)\n+                 -> TokenNode {\n+            let (first_span, rest_span) = (*span, *span);\n+            *span = first_span;\n+            let tree = tokenstream::TokenTree::Token(rest_span, rest);\n+            *next = Some(if is_joint { tree.joint() } else { tree.into() });\n+            TokenNode::Op(first, Spacing::Joint)\n+        }\n+\n+        let kind = match token {\n+            Eq => op!('='),\n+            Lt => op!('<'),\n+            Le => joint!('<', Eq),\n+            EqEq => joint!('=', Eq),\n+            Ne => joint!('!', Eq),\n+            Ge => joint!('>', Eq),\n+            Gt => op!('>'),\n+            AndAnd => joint!('&', BinOp(And)),\n+            OrOr => joint!('|', BinOp(Or)),\n+            Not => op!('!'),\n+            Tilde => op!('~'),\n+            BinOp(Plus) => op!('+'),\n+            BinOp(Minus) => op!('-'),\n+            BinOp(Star) => op!('*'),\n+            BinOp(Slash) => op!('/'),\n+            BinOp(Percent) => op!('%'),\n+            BinOp(Caret) => op!('^'),\n+            BinOp(And) => op!('&'),\n+            BinOp(Or) => op!('|'),\n+            BinOp(Shl) => joint!('<', Lt),\n+            BinOp(Shr) => joint!('>', Gt),\n+            BinOpEq(Plus) => joint!('+', Eq),\n+            BinOpEq(Minus) => joint!('-', Eq),\n+            BinOpEq(Star) => joint!('*', Eq),\n+            BinOpEq(Slash) => joint!('/', Eq),\n+            BinOpEq(Percent) => joint!('%', Eq),\n+            BinOpEq(Caret) => joint!('^', Eq),\n+            BinOpEq(And) => joint!('&', Eq),\n+            BinOpEq(Or) => joint!('|', Eq),\n+            BinOpEq(Shl) => joint!('<', Le),\n+            BinOpEq(Shr) => joint!('>', Ge),\n+            At => op!('@'),\n+            Dot => op!('.'),\n+            DotDot => joint!('.', Dot),\n+            DotDotDot => joint!('.', DotDot),\n+            Comma => op!(','),\n+            Semi => op!(';'),\n+            Colon => op!(':'),\n+            ModSep => joint!(':', Colon),\n+            RArrow => joint!('-', Gt),\n+            LArrow => joint!('<', BinOp(Minus)),\n+            FatArrow => joint!('=', Gt),\n+            Pound => op!('#'),\n+            Dollar => op!('$'),\n+            Question => op!('?'),\n+            Underscore => op!('_'),\n+\n+            Ident(ident) | Lifetime(ident) => TokenNode::Term(Term(ident.name)),\n+            Literal(..) | DocComment(..) => TokenNode::Literal(self::Literal(token)),\n+\n+            Interpolated(ref nt) => __internal::with_sess(|(sess, _)| {\n+                TokenNode::Group(Delimiter::None, TokenStream(nt.1.force(|| {\n+                    // FIXME(jseyfried): Avoid this pretty-print + reparse hack\n+                    let name = \"<macro expansion>\".to_owned();\n+                    let source = pprust::token_to_string(&token);\n+                    parse_stream_from_source_str(name, source, sess, Some(span))\n+                })))\n+            }),\n+\n+            OpenDelim(..) | CloseDelim(..) => unreachable!(),\n+            Whitespace | Comment | Shebang(..) | Eof => unreachable!(),\n+        };\n+\n+        TokenTree { span: Span(span), kind: kind }\n+    }\n+\n+    fn to_internal(self) -> tokenstream::TokenStream {\n+        use syntax::parse::token::*;\n+        use syntax::tokenstream::{TokenTree, Delimited};\n+\n+        let (op, kind) = match self.kind {\n+            TokenNode::Op(op, kind) => (op, kind),\n+            TokenNode::Group(delimiter, tokens) => {\n+                return TokenTree::Delimited(self.span.0, Delimited {\n+                    delim: delimiter.to_internal(),\n+                    tts: tokens.0.into(),\n+                }).into();\n+            },\n+            TokenNode::Term(symbol) => {\n+                let ident = ast::Ident { name: symbol.0, ctxt: self.span.0.ctxt };\n+                let token =\n+                    if symbol.0.as_str().starts_with(\"'\") { Lifetime(ident) } else { Ident(ident) };\n+                return TokenTree::Token(self.span.0, token).into();\n+            }\n+            TokenNode::Literal(token) => return TokenTree::Token(self.span.0, token.0).into(),\n+        };\n+\n+        let token = match op {\n+            '=' => Eq,\n+            '<' => Lt,\n+            '>' => Gt,\n+            '!' => Not,\n+            '~' => Tilde,\n+            '+' => BinOp(Plus),\n+            '-' => BinOp(Minus),\n+            '*' => BinOp(Star),\n+            '/' => BinOp(Slash),\n+            '%' => BinOp(Percent),\n+            '^' => BinOp(Caret),\n+            '&' => BinOp(And),\n+            '|' => BinOp(Or),\n+            '@' => At,\n+            '.' => Dot,\n+            ',' => Comma,\n+            ';' => Semi,\n+            ':' => Colon,\n+            '#' => Pound,\n+            '$' => Dollar,\n+            '?' => Question,\n+            '_' => Underscore,\n+            _ => panic!(\"unsupported character {}\", op),\n+        };\n+\n+        let tree = TokenTree::Token(self.span.0, token);\n+        match kind {\n+            Spacing::Alone => tree.into(),\n+            Spacing::Joint => tree.joint(),\n+        }\n+    }\n+}\n+\n /// Permanently unstable internal implementation details of this crate. This\n /// should not be used.\n ///\n@@ -83,32 +582,33 @@ pub struct LexError {\n #[unstable(feature = \"proc_macro_internals\", issue = \"27812\")]\n #[doc(hidden)]\n pub mod __internal {\n+    pub use quote::{Quoter, __rt};\n+\n     use std::cell::Cell;\n-    use std::rc::Rc;\n \n     use syntax::ast;\n+    use syntax::ext::base::ExtCtxt;\n+    use syntax::ext::hygiene::Mark;\n     use syntax::ptr::P;\n-    use syntax::parse::{self, token, ParseSess};\n-    use syntax::tokenstream::{TokenTree, TokenStream as TokenStream_};\n+    use syntax::parse::{self, ParseSess};\n+    use syntax::parse::token::{self, Token};\n+    use syntax::tokenstream;\n+    use syntax_pos::DUMMY_SP;\n \n     use super::{TokenStream, LexError};\n \n     pub fn new_token_stream(item: P<ast::Item>) -> TokenStream {\n-        TokenStream {\n-            inner: TokenTree::Token(item.span, token::Interpolated(Rc::new(token::NtItem(item))))\n-                .into()\n-        }\n+        let token = Token::interpolated(token::NtItem(item));\n+        TokenStream(tokenstream::TokenTree::Token(DUMMY_SP, token).into())\n     }\n \n-    pub fn token_stream_wrap(inner: TokenStream_) -> TokenStream {\n-        TokenStream {\n-            inner: inner\n-        }\n+    pub fn token_stream_wrap(inner: tokenstream::TokenStream) -> TokenStream {\n+        TokenStream(inner)\n     }\n \n     pub fn token_stream_parse_items(stream: TokenStream) -> Result<Vec<P<ast::Item>>, LexError> {\n-        with_parse_sess(move |sess| {\n-            let mut parser = parse::stream_to_parser(sess, stream.inner);\n+        with_sess(move |(sess, _)| {\n+            let mut parser = parse::stream_to_parser(sess, stream.0);\n             let mut items = Vec::new();\n \n             while let Some(item) = try!(parser.parse_item().map_err(super::parse_to_lex_err)) {\n@@ -119,8 +619,8 @@ pub mod __internal {\n         })\n     }\n \n-    pub fn token_stream_inner(stream: TokenStream) -> TokenStream_ {\n-        stream.inner\n+    pub fn token_stream_inner(stream: TokenStream) -> tokenstream::TokenStream {\n+        stream.0\n     }\n \n     pub trait Registry {\n@@ -140,13 +640,14 @@ pub mod __internal {\n \n     // Emulate scoped_thread_local!() here essentially\n     thread_local! {\n-        static CURRENT_SESS: Cell<*const ParseSess> = Cell::new(0 as *const _);\n+        static CURRENT_SESS: Cell<(*const ParseSess, Mark)> =\n+            Cell::new((0 as *const _, Mark::root()));\n     }\n \n-    pub fn set_parse_sess<F, R>(sess: &ParseSess, f: F) -> R\n+    pub fn set_sess<F, R>(cx: &ExtCtxt, f: F) -> R\n         where F: FnOnce() -> R\n     {\n-        struct Reset { prev: *const ParseSess }\n+        struct Reset { prev: (*const ParseSess, Mark) }\n \n         impl Drop for Reset {\n             fn drop(&mut self) {\n@@ -156,43 +657,22 @@ pub mod __internal {\n \n         CURRENT_SESS.with(|p| {\n             let _reset = Reset { prev: p.get() };\n-            p.set(sess);\n+            p.set((cx.parse_sess, cx.current_expansion.mark));\n             f()\n         })\n     }\n \n-    pub fn with_parse_sess<F, R>(f: F) -> R\n-        where F: FnOnce(&ParseSess) -> R\n+    pub fn with_sess<F, R>(f: F) -> R\n+        where F: FnOnce((&ParseSess, Mark)) -> R\n     {\n         let p = CURRENT_SESS.with(|p| p.get());\n-        assert!(!p.is_null(), \"proc_macro::__internal::with_parse_sess() called \\\n-                               before set_parse_sess()!\");\n-        f(unsafe { &*p })\n+        assert!(!p.0.is_null(), \"proc_macro::__internal::with_sess() called \\\n+                                 before set_parse_sess()!\");\n+        f(unsafe { (&*p.0, p.1) })\n     }\n }\n \n fn parse_to_lex_err(mut err: DiagnosticBuilder) -> LexError {\n     err.cancel();\n     LexError { _inner: () }\n }\n-\n-#[stable(feature = \"proc_macro_lib\", since = \"1.15.0\")]\n-impl FromStr for TokenStream {\n-    type Err = LexError;\n-\n-    fn from_str(src: &str) -> Result<TokenStream, LexError> {\n-        __internal::with_parse_sess(|sess| {\n-            let src = src.to_string();\n-            let name = \"<proc-macro source code>\".to_string();\n-            let stream = parse::parse_stream_from_source_str(name, src, sess);\n-            Ok(__internal::token_stream_wrap(stream))\n-        })\n-    }\n-}\n-\n-#[stable(feature = \"proc_macro_lib\", since = \"1.15.0\")]\n-impl fmt::Display for TokenStream {\n-    fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {\n-        self.inner.fmt(f)\n-    }\n-}"}, {"sha": "bee2c1e0eb6b608a21e273a383bea64221d4a3ba", "filename": "src/libproc_macro/quote.rs", "status": "added", "additions": 263, "deletions": 0, "changes": 263, "blob_url": "https://github.com/rust-lang/rust/blob/4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e/src%2Flibproc_macro%2Fquote.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e/src%2Flibproc_macro%2Fquote.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibproc_macro%2Fquote.rs?ref=4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e", "patch": "@@ -0,0 +1,263 @@\n+// Copyright 2016 The Rust Project Developers. See the COPYRIGHT\n+// file at the top-level directory of this distribution and at\n+// http://rust-lang.org/COPYRIGHT.\n+//\n+// Licensed under the Apache License, Version 2.0 <LICENSE-APACHE or\n+// http://www.apache.org/licenses/LICENSE-2.0> or the MIT license\n+// <LICENSE-MIT or http://opensource.org/licenses/MIT>, at your\n+// option. This file may not be copied, modified, or distributed\n+// except according to those terms.\n+\n+//! # Quasiquoter\n+//! This file contains the implementation internals of the quasiquoter provided by `quote!`.\n+\n+//! This quasiquoter uses macros 2.0 hygiene to reliably use items from `__rt`,\n+//! including re-exported API `libsyntax`, to build a `syntax::tokenstream::TokenStream`\n+//! and wrap it into a `proc_macro::TokenStream`.\n+\n+use syntax::ast::Ident;\n+use syntax::ext::base::{ExtCtxt, ProcMacro};\n+use syntax::parse::token::{self, Token, Lit};\n+use syntax::symbol::Symbol;\n+use syntax::tokenstream::{Delimited, TokenTree, TokenStream, TokenStreamBuilder};\n+use syntax_pos::{DUMMY_SP, Span};\n+use syntax_pos::hygiene::SyntaxContext;\n+\n+pub struct Quoter;\n+\n+pub mod __rt {\n+    pub use syntax::ast::Ident;\n+    pub use syntax::parse::token;\n+    pub use syntax::symbol::Symbol;\n+    pub use syntax::tokenstream::{TokenStream, TokenStreamBuilder, TokenTree, Delimited};\n+    pub use super::{ctxt, span};\n+\n+    pub fn unquote<T: Into<::TokenStream> + Clone>(tokens: &T) -> TokenStream {\n+        T::into(tokens.clone()).0\n+    }\n+}\n+\n+pub fn ctxt() -> SyntaxContext {\n+    ::__internal::with_sess(|(_, mark)| SyntaxContext::empty().apply_mark(mark))\n+}\n+\n+pub fn span() -> Span {\n+    ::Span::default().0\n+}\n+\n+pub trait Quote {\n+    fn quote(&self) -> TokenStream;\n+}\n+\n+macro_rules! quote_tok {\n+    (,) => { Token::Comma };\n+    (.) => { Token::Dot };\n+    (:) => { Token::Colon };\n+    (::) => { Token::ModSep };\n+    (!) => { Token::Not };\n+    (<) => { Token::Lt };\n+    (>) => { Token::Gt };\n+    (_) => { Token::Underscore };\n+    (0) => { Token::Literal(token::Lit::Integer(Symbol::intern(\"0\")), None) };\n+    (&) => { Token::BinOp(token::And) };\n+    ($i:ident) => { Token::Ident(Ident { name: Symbol::intern(stringify!($i)), ctxt: ctxt() }) };\n+}\n+\n+macro_rules! quote_tree {\n+    ((unquote $($t:tt)*)) => { TokenStream::from($($t)*) };\n+    ((quote $($t:tt)*)) => { ($($t)*).quote() };\n+    (($($t:tt)*)) => { delimit(token::Paren, quote!($($t)*)) };\n+    ([$($t:tt)*]) => { delimit(token::Bracket, quote!($($t)*)) };\n+    ({$($t:tt)*}) => { delimit(token::Brace, quote!($($t)*)) };\n+    (rt) => { quote!(::__internal::__rt) };\n+    ($t:tt) => { TokenStream::from(TokenTree::Token(span(), quote_tok!($t))) };\n+}\n+\n+fn delimit(delim: token::DelimToken, stream: TokenStream) -> TokenStream {\n+    TokenTree::Delimited(span(), Delimited { delim: delim, tts: stream.into() }).into()\n+}\n+\n+macro_rules! quote {\n+    () => { TokenStream::empty() };\n+    ($($t:tt)*) => { [ $( quote_tree!($t), )* ].iter().cloned().collect::<TokenStream>() };\n+}\n+\n+impl ProcMacro for Quoter {\n+    fn expand<'cx>(&self, cx: &'cx mut ExtCtxt, _: Span, stream: TokenStream) -> TokenStream {\n+        let mut info = cx.current_expansion.mark.expn_info().unwrap();\n+        info.callee.allow_internal_unstable = true;\n+        cx.current_expansion.mark.set_expn_info(info);\n+        ::__internal::set_sess(cx, || quote!(::TokenStream((quote stream))))\n+    }\n+}\n+\n+impl<T: Quote> Quote for Option<T> {\n+    fn quote(&self) -> TokenStream {\n+        match *self {\n+            Some(ref t) => quote!(Some((quote t))),\n+            None => quote!(None),\n+        }\n+    }\n+}\n+\n+impl Quote for TokenStream {\n+    fn quote(&self) -> TokenStream {\n+        let mut builder = TokenStreamBuilder::new();\n+        builder.push(quote!(rt::TokenStreamBuilder::new()));\n+\n+        let mut trees = self.trees();\n+        loop {\n+            let (mut tree, mut is_joint) = match trees.next_as_stream() {\n+                Some(next) => next.as_tree(),\n+                None => return builder.add(quote!(.build())).build(),\n+            };\n+            if let TokenTree::Token(_, Token::Dollar) = tree {\n+                let (next_tree, next_is_joint) = match trees.next_as_stream() {\n+                    Some(next) => next.as_tree(),\n+                    None => panic!(\"unexpected trailing `$` in `quote!`\"),\n+                };\n+                match next_tree {\n+                    TokenTree::Token(_, Token::Ident(..)) => {\n+                        builder.push(quote!(.add(rt::unquote(&(unquote next_tree)))));\n+                        continue\n+                    }\n+                    TokenTree::Token(_, Token::Dollar) => {\n+                        tree = next_tree;\n+                        is_joint = next_is_joint;\n+                    }\n+                    _ => panic!(\"`$` must be followed by an ident or `$` in `quote!`\"),\n+                }\n+            }\n+\n+            builder.push(match is_joint {\n+                true => quote!(.add((quote tree).joint())),\n+                false => quote!(.add(rt::TokenStream::from((quote tree)))),\n+            });\n+        }\n+    }\n+}\n+\n+impl Quote for TokenTree {\n+    fn quote(&self) -> TokenStream {\n+        match *self {\n+            TokenTree::Token(span, ref token) => quote! {\n+                rt::TokenTree::Token((quote span), (quote token))\n+            },\n+            TokenTree::Delimited(span, ref delimited) => quote! {\n+                rt::TokenTree::Delimited((quote span), (quote delimited))\n+            },\n+        }\n+    }\n+}\n+\n+impl Quote for Delimited {\n+    fn quote(&self) -> TokenStream {\n+        quote!(rt::Delimited { delim: (quote self.delim), tts: (quote self.stream()).into() })\n+    }\n+}\n+\n+impl<'a> Quote for &'a str {\n+    fn quote(&self) -> TokenStream {\n+        TokenTree::Token(span(), Token::Literal(token::Lit::Str_(Symbol::intern(self)), None))\n+            .into()\n+    }\n+}\n+\n+impl Quote for usize {\n+    fn quote(&self) -> TokenStream {\n+        let integer_symbol = Symbol::intern(&self.to_string());\n+        TokenTree::Token(DUMMY_SP, Token::Literal(token::Lit::Integer(integer_symbol), None))\n+            .into()\n+    }\n+}\n+\n+impl Quote for Ident {\n+    fn quote(&self) -> TokenStream {\n+        quote!(rt::Ident { name: (quote self.name), ctxt: rt::ctxt() })\n+    }\n+}\n+\n+impl Quote for Symbol {\n+    fn quote(&self) -> TokenStream {\n+        quote!(rt::Symbol::intern((quote &*self.as_str())))\n+    }\n+}\n+\n+impl Quote for Span {\n+    fn quote(&self) -> TokenStream {\n+        quote!(rt::span())\n+    }\n+}\n+\n+impl Quote for Token {\n+    fn quote(&self) -> TokenStream {\n+        macro_rules! gen_match {\n+            ($($i:ident),*; $($t:tt)*) => {\n+                match *self {\n+                    $( Token::$i => quote!(rt::token::$i), )*\n+                    $( $t )*\n+                }\n+            }\n+        }\n+\n+        gen_match! {\n+            Eq, Lt, Le, EqEq, Ne, Ge, Gt, AndAnd, OrOr, Not, Tilde, At, Dot, DotDot, DotDotDot,\n+            Comma, Semi, Colon, ModSep, RArrow, LArrow, FatArrow, Pound, Dollar, Question,\n+            Underscore;\n+\n+            Token::OpenDelim(delim) => quote!(rt::token::OpenDelim((quote delim))),\n+            Token::CloseDelim(delim) => quote!(rt::token::CloseDelim((quote delim))),\n+            Token::BinOp(tok) => quote!(rt::token::BinOp((quote tok))),\n+            Token::BinOpEq(tok) => quote!(rt::token::BinOpEq((quote tok))),\n+            Token::Ident(ident) => quote!(rt::token::Ident((quote ident))),\n+            Token::Lifetime(ident) => quote!(rt::token::Lifetime((quote ident))),\n+            Token::Literal(lit, sfx) => quote!(rt::token::Literal((quote lit), (quote sfx))),\n+            _ => panic!(\"Unhandled case!\"),\n+        }\n+    }\n+}\n+\n+impl Quote for token::BinOpToken {\n+    fn quote(&self) -> TokenStream {\n+        macro_rules! gen_match {\n+            ($($i:ident),*) => {\n+                match *self {\n+                    $( token::BinOpToken::$i => quote!(rt::token::BinOpToken::$i), )*\n+                }\n+            }\n+        }\n+\n+        gen_match!(Plus, Minus, Star, Slash, Percent, Caret, And, Or, Shl, Shr)\n+    }\n+}\n+\n+impl Quote for Lit {\n+    fn quote(&self) -> TokenStream {\n+        macro_rules! gen_match {\n+            ($($i:ident),*; $($raw:ident),*) => {\n+                match *self {\n+                    $( Lit::$i(lit) => quote!(rt::token::Lit::$i((quote lit))), )*\n+                    $( Lit::$raw(lit, n) => {\n+                        quote!(::syntax::parse::token::Lit::$raw((quote lit), (quote n)))\n+                    })*\n+                }\n+            }\n+        }\n+\n+        gen_match!(Byte, Char, Float, Str_, Integer, ByteStr; StrRaw, ByteStrRaw)\n+    }\n+}\n+\n+impl Quote for token::DelimToken {\n+    fn quote(&self) -> TokenStream {\n+        macro_rules! gen_match {\n+            ($($i:ident),*) => {\n+                match *self {\n+                    $(token::DelimToken::$i => { quote!(rt::token::DelimToken::$i) })*\n+                }\n+            }\n+        }\n+\n+        gen_match!(Paren, Bracket, Brace, NoDelim)\n+    }\n+}"}, {"sha": "146a66cdf01cb484586677b7c68cbdc89e2dd233", "filename": "src/libproc_macro_plugin/Cargo.toml", "status": "removed", "additions": 0, "deletions": 13, "changes": 13, "blob_url": "https://github.com/rust-lang/rust/blob/3610a70ce488953c5b0379fece70f2baad30a825/src%2Flibproc_macro_plugin%2FCargo.toml", "raw_url": "https://github.com/rust-lang/rust/raw/3610a70ce488953c5b0379fece70f2baad30a825/src%2Flibproc_macro_plugin%2FCargo.toml", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibproc_macro_plugin%2FCargo.toml?ref=3610a70ce488953c5b0379fece70f2baad30a825", "patch": "@@ -1,13 +0,0 @@\n-[package]\n-authors = [\"The Rust Project Developers\"]\n-name = \"proc_macro_plugin\"\n-version = \"0.0.0\"\n-\n-[lib]\n-path = \"lib.rs\"\n-crate-type = [\"dylib\"]\n-\n-[dependencies]\n-rustc_plugin = { path = \"../librustc_plugin\" }\n-syntax = { path = \"../libsyntax\" }\n-syntax_pos = { path = \"../libsyntax_pos\" }"}, {"sha": "d1bc0966eb567188dbc15d2e6c39f6a82eedf64f", "filename": "src/libproc_macro_plugin/lib.rs", "status": "removed", "additions": 0, "deletions": 103, "changes": 103, "blob_url": "https://github.com/rust-lang/rust/blob/3610a70ce488953c5b0379fece70f2baad30a825/src%2Flibproc_macro_plugin%2Flib.rs", "raw_url": "https://github.com/rust-lang/rust/raw/3610a70ce488953c5b0379fece70f2baad30a825/src%2Flibproc_macro_plugin%2Flib.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibproc_macro_plugin%2Flib.rs?ref=3610a70ce488953c5b0379fece70f2baad30a825", "patch": "@@ -1,103 +0,0 @@\n-// Copyright 2016 The Rust Project Developers. See the COPYRIGHT\n-// file at the top-level directory of this distribution and at\n-// http://rust-lang.org/COPYRIGHT.\n-//\n-// Licensed under the Apache License, Version 2.0 <LICENSE-APACHE or\n-// http://www.apache.org/licenses/LICENSE-2.0> or the MIT license\n-// <LICENSE-MIT or http://opensource.org/licenses/MIT>, at your\n-// option. This file may not be copied, modified, or distributed\n-// except according to those terms.\n-\n-//! # Proc_Macro\n-//!\n-//! A library for procedural macro writers.\n-//!\n-//! ## Usage\n-//! This crate provides the `quote!` macro for syntax creation.\n-//!\n-//! The `quote!` macro uses the crate `syntax`, so users must declare `extern crate syntax;`\n-//! at the crate root. This is a temporary solution until we have better hygiene.\n-//!\n-//! ## Quasiquotation\n-//!\n-//! The quasiquoter creates output that, when run, constructs the tokenstream specified as\n-//! input. For example, `quote!(5 + 5)` will produce a program, that, when run, will\n-//! construct the TokenStream `5 | + | 5`.\n-//!\n-//! ### Unquoting\n-//!\n-//! Unquoting is done with `$`, and works by taking the single next ident as the unquoted term.\n-//! To quote `$` itself, use `$$`.\n-//!\n-//! A simple example is:\n-//!\n-//!```\n-//!fn double(tmp: TokenStream) -> TokenStream {\n-//!    quote!($tmp * 2)\n-//!}\n-//!```\n-//!\n-//! ### Large example: Scheme's `cond`\n-//!\n-//! Below is an example implementation of Scheme's `cond`.\n-//!\n-//! ```\n-//! fn cond(input: TokenStream) -> TokenStream {\n-//!     let mut conds = Vec::new();\n-//!     let mut input = input.trees().peekable();\n-//!     while let Some(tree) = input.next() {\n-//!         let mut cond = match tree {\n-//!             TokenTree::Delimited(_, ref delimited) => delimited.stream(),\n-//!             _ => panic!(\"Invalid input\"),\n-//!         };\n-//!         let mut trees = cond.trees();\n-//!         let test = trees.next();\n-//!         let rhs = trees.collect::<TokenStream>();\n-//!         if rhs.is_empty() {\n-//!             panic!(\"Invalid macro usage in cond: {}\", cond);\n-//!         }\n-//!         let is_else = match test {\n-//!             Some(TokenTree::Token(_, Token::Ident(ident))) if ident.name == \"else\" => true,\n-//!             _ => false,\n-//!         };\n-//!         conds.push(if is_else || input.peek().is_none() {\n-//!             quote!({ $rhs })\n-//!         } else {\n-//!             let test = test.unwrap();\n-//!             quote!(if $test { $rhs } else)\n-//!         });\n-//!     }\n-//!\n-//!     conds.into_iter().collect()\n-//! }\n-//! ```\n-#![crate_name = \"proc_macro_plugin\"]\n-#![feature(plugin_registrar)]\n-#![crate_type = \"dylib\"]\n-#![crate_type = \"rlib\"]\n-#![doc(html_logo_url = \"https://www.rust-lang.org/logos/rust-logo-128x128-blk-v2.png\",\n-       html_favicon_url = \"https://doc.rust-lang.org/favicon.ico\",\n-       html_root_url = \"https://doc.rust-lang.org/nightly/\")]\n-#![deny(warnings)]\n-\n-#![feature(rustc_diagnostic_macros)]\n-\n-extern crate rustc_plugin;\n-extern crate syntax;\n-extern crate syntax_pos;\n-\n-mod quote;\n-use quote::quote;\n-\n-use rustc_plugin::Registry;\n-use syntax::ext::base::SyntaxExtension;\n-use syntax::symbol::Symbol;\n-\n-// ____________________________________________________________________________________________\n-// Main macro definition\n-\n-#[plugin_registrar]\n-pub fn plugin_registrar(reg: &mut Registry) {\n-    reg.register_syntax_extension(Symbol::intern(\"quote\"),\n-                                  SyntaxExtension::ProcMacro(Box::new(quote)));\n-}"}, {"sha": "09675564291a248b4ded93dddccee6aa6371a73f", "filename": "src/libproc_macro_plugin/quote.rs", "status": "removed", "additions": 0, "deletions": 230, "changes": 230, "blob_url": "https://github.com/rust-lang/rust/blob/3610a70ce488953c5b0379fece70f2baad30a825/src%2Flibproc_macro_plugin%2Fquote.rs", "raw_url": "https://github.com/rust-lang/rust/raw/3610a70ce488953c5b0379fece70f2baad30a825/src%2Flibproc_macro_plugin%2Fquote.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibproc_macro_plugin%2Fquote.rs?ref=3610a70ce488953c5b0379fece70f2baad30a825", "patch": "@@ -1,230 +0,0 @@\n-// Copyright 2016 The Rust Project Developers. See the COPYRIGHT\n-// file at the top-level directory of this distribution and at\n-// http://rust-lang.org/COPYRIGHT.\n-//\n-// Licensed under the Apache License, Version 2.0 <LICENSE-APACHE or\n-// http://www.apache.org/licenses/LICENSE-2.0> or the MIT license\n-// <LICENSE-MIT or http://opensource.org/licenses/MIT>, at your\n-// option. This file may not be copied, modified, or distributed\n-// except according to those terms.\n-\n-//! # Quasiquoter\n-//! This file contains the implementation internals of the quasiquoter provided by `qquote!`.\n-\n-use syntax::ast::Ident;\n-use syntax::parse::token::{self, Token, Lit};\n-use syntax::symbol::Symbol;\n-use syntax::tokenstream::{self, Delimited, TokenTree, TokenStream};\n-use syntax_pos::DUMMY_SP;\n-\n-use std::iter;\n-\n-pub fn quote<'cx>(stream: TokenStream) -> TokenStream {\n-    stream.quote()\n-}\n-\n-trait Quote {\n-    fn quote(&self) -> TokenStream;\n-}\n-\n-macro_rules! quote_tok {\n-    (,) => { Token::Comma };\n-    (.) => { Token::Dot };\n-    (:) => { Token::Colon };\n-    (::) => { Token::ModSep };\n-    (!) => { Token::Not };\n-    (<) => { Token::Lt };\n-    (>) => { Token::Gt };\n-    (_) => { Token::Underscore };\n-    ($i:ident) => { Token::Ident(Ident::from_str(stringify!($i))) };\n-}\n-\n-macro_rules! quote_tree {\n-    ((unquote $($t:tt)*)) => { $($t)* };\n-    ((quote $($t:tt)*)) => { ($($t)*).quote() };\n-    (($($t:tt)*)) => { delimit(token::Paren, quote!($($t)*)) };\n-    ([$($t:tt)*]) => { delimit(token::Bracket, quote!($($t)*)) };\n-    ({$($t:tt)*}) => { delimit(token::Brace, quote!($($t)*)) };\n-    ($t:tt) => { TokenStream::from(TokenTree::Token(DUMMY_SP, quote_tok!($t))) };\n-}\n-\n-fn delimit(delim: token::DelimToken, stream: TokenStream) -> TokenStream {\n-    TokenTree::Delimited(DUMMY_SP, Delimited { delim: delim, tts: stream.into() }).into()\n-}\n-\n-macro_rules! quote {\n-    () => { TokenStream::empty() };\n-    ($($t:tt)*) => { [ $( quote_tree!($t), )* ].iter().cloned().collect::<TokenStream>() };\n-}\n-\n-impl<T: Quote> Quote for Option<T> {\n-    fn quote(&self) -> TokenStream {\n-        match *self {\n-            Some(ref t) => quote!(::std::option::Option::Some((quote t))),\n-            None => quote!(::std::option::Option::None),\n-        }\n-    }\n-}\n-\n-impl Quote for TokenStream {\n-    fn quote(&self) -> TokenStream {\n-        if self.is_empty() {\n-            return quote!(::syntax::tokenstream::TokenStream::empty());\n-        }\n-\n-        struct Quoter(iter::Peekable<tokenstream::Cursor>);\n-\n-        impl Iterator for Quoter {\n-            type Item = TokenStream;\n-\n-            fn next(&mut self) -> Option<TokenStream> {\n-                let quoted_tree = if let Some(&TokenTree::Token(_, Token::Dollar)) = self.0.peek() {\n-                    self.0.next();\n-                    match self.0.next() {\n-                        Some(tree @ TokenTree::Token(_, Token::Ident(..))) => Some(tree.into()),\n-                        Some(tree @ TokenTree::Token(_, Token::Dollar)) => Some(tree.quote()),\n-                        // FIXME(jseyfried): improve these diagnostics\n-                        Some(..) => panic!(\"`$` must be followed by an ident or `$` in `quote!`\"),\n-                        None => panic!(\"unexpected trailing `$` in `quote!`\"),\n-                    }\n-                } else {\n-                    self.0.next().as_ref().map(Quote::quote)\n-                };\n-\n-                quoted_tree.map(|quoted_tree| {\n-                    quote!(::syntax::tokenstream::TokenStream::from((unquote quoted_tree)),)\n-                })\n-            }\n-        }\n-\n-        let quoted = Quoter(self.trees().peekable()).collect::<TokenStream>();\n-        quote!([(unquote quoted)].iter().cloned().collect::<::syntax::tokenstream::TokenStream>())\n-    }\n-}\n-\n-impl Quote for TokenTree {\n-    fn quote(&self) -> TokenStream {\n-        match *self {\n-            TokenTree::Token(_, ref token) => quote! {\n-                ::syntax::tokenstream::TokenTree::Token(::syntax::ext::quote::rt::DUMMY_SP,\n-                                                        (quote token))\n-            },\n-            TokenTree::Delimited(_, ref delimited) => quote! {\n-                ::syntax::tokenstream::TokenTree::Delimited(::syntax::ext::quote::rt::DUMMY_SP,\n-                                                            (quote delimited))\n-            },\n-        }\n-    }\n-}\n-\n-impl Quote for Delimited {\n-    fn quote(&self) -> TokenStream {\n-        quote!(::syntax::tokenstream::Delimited {\n-            delim: (quote self.delim),\n-            tts: (quote self.stream()).into(),\n-        })\n-    }\n-}\n-\n-impl<'a> Quote for &'a str {\n-    fn quote(&self) -> TokenStream {\n-        TokenTree::Token(DUMMY_SP, Token::Literal(token::Lit::Str_(Symbol::intern(self)), None))\n-            .into()\n-    }\n-}\n-\n-impl Quote for usize {\n-    fn quote(&self) -> TokenStream {\n-        let integer_symbol = Symbol::intern(&self.to_string());\n-        TokenTree::Token(DUMMY_SP, Token::Literal(token::Lit::Integer(integer_symbol), None))\n-            .into()\n-    }\n-}\n-\n-impl Quote for Ident {\n-    fn quote(&self) -> TokenStream {\n-        // FIXME(jseyfried) quote hygiene\n-        quote!(::syntax::ast::Ident::from_str((quote &*self.name.as_str())))\n-    }\n-}\n-\n-impl Quote for Symbol {\n-    fn quote(&self) -> TokenStream {\n-        quote!(::syntax::symbol::Symbol::intern((quote &*self.as_str())))\n-    }\n-}\n-\n-impl Quote for Token {\n-    fn quote(&self) -> TokenStream {\n-        macro_rules! gen_match {\n-            ($($i:ident),*; $($t:tt)*) => {\n-                match *self {\n-                    $( Token::$i => quote!(::syntax::parse::token::$i), )*\n-                    $( $t )*\n-                }\n-            }\n-        }\n-\n-        gen_match! {\n-            Eq, Lt, Le, EqEq, Ne, Ge, Gt, AndAnd, OrOr, Not, Tilde, At, Dot, DotDot, DotDotDot,\n-            Comma, Semi, Colon, ModSep, RArrow, LArrow, FatArrow, Pound, Dollar, Question,\n-            Underscore;\n-\n-            Token::OpenDelim(delim) => quote!(::syntax::parse::token::OpenDelim((quote delim))),\n-            Token::CloseDelim(delim) => quote!(::syntax::parse::token::CloseDelim((quote delim))),\n-            Token::BinOp(tok) => quote!(::syntax::parse::token::BinOp((quote tok))),\n-            Token::BinOpEq(tok) => quote!(::syntax::parse::token::BinOpEq((quote tok))),\n-            Token::Ident(ident) => quote!(::syntax::parse::token::Ident((quote ident))),\n-            Token::Lifetime(ident) => quote!(::syntax::parse::token::Lifetime((quote ident))),\n-            Token::Literal(lit, sfx) => quote! {\n-                ::syntax::parse::token::Literal((quote lit), (quote sfx))\n-            },\n-            _ => panic!(\"Unhandled case!\"),\n-        }\n-    }\n-}\n-\n-impl Quote for token::BinOpToken {\n-    fn quote(&self) -> TokenStream {\n-        macro_rules! gen_match {\n-            ($($i:ident),*) => {\n-                match *self {\n-                    $( token::BinOpToken::$i => quote!(::syntax::parse::token::BinOpToken::$i), )*\n-                }\n-            }\n-        }\n-\n-        gen_match!(Plus, Minus, Star, Slash, Percent, Caret, And, Or, Shl, Shr)\n-    }\n-}\n-\n-impl Quote for Lit {\n-    fn quote(&self) -> TokenStream {\n-        macro_rules! gen_match {\n-            ($($i:ident),*; $($raw:ident),*) => {\n-                match *self {\n-                    $( Lit::$i(lit) => quote!(::syntax::parse::token::Lit::$i((quote lit))), )*\n-                    $( Lit::$raw(lit, n) => {\n-                        quote!(::syntax::parse::token::Lit::$raw((quote lit), (quote n)))\n-                    })*\n-                }\n-            }\n-        }\n-\n-        gen_match!(Byte, Char, Float, Str_, Integer, ByteStr; StrRaw, ByteStrRaw)\n-    }\n-}\n-\n-impl Quote for token::DelimToken {\n-    fn quote(&self) -> TokenStream {\n-        macro_rules! gen_match {\n-            ($($i:ident),*) => {\n-                match *self {\n-                    $(token::DelimToken::$i => { quote!(::syntax::parse::token::DelimToken::$i) })*\n-                }\n-            }\n-        }\n-\n-        gen_match!(Paren, Bracket, Brace, NoDelim)\n-    }\n-}"}, {"sha": "b827284271ed2235925313eadd31a1b662332fd8", "filename": "src/librustc/ich/impls_syntax.rs", "status": "modified", "additions": 1, "deletions": 2, "changes": 3, "blob_url": "https://github.com/rust-lang/rust/blob/4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e/src%2Flibrustc%2Fich%2Fimpls_syntax.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e/src%2Flibrustc%2Fich%2Fimpls_syntax.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Fich%2Fimpls_syntax.rs?ref=4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e", "patch": "@@ -283,8 +283,7 @@ fn hash_token<'a, 'gcx, 'tcx, W: StableHasherResult>(token: &token::Token,\n         }\n \n         token::Token::Ident(ident) |\n-        token::Token::Lifetime(ident) |\n-        token::Token::SubstNt(ident) => ident.name.hash_stable(hcx, hasher),\n+        token::Token::Lifetime(ident) => ident.name.hash_stable(hcx, hasher),\n \n         token::Token::Interpolated(ref non_terminal) => {\n             // FIXME(mw): This could be implemented properly. It's just a"}, {"sha": "e6dc5da969a88a2c23449806b746b4ea51a8e163", "filename": "src/librustc/middle/stability.rs", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "blob_url": "https://github.com/rust-lang/rust/blob/4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e/src%2Flibrustc%2Fmiddle%2Fstability.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e/src%2Flibrustc%2Fmiddle%2Fstability.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Fmiddle%2Fstability.rs?ref=4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e", "patch": "@@ -728,6 +728,7 @@ pub fn check_unused_or_stable_features<'a, 'tcx>(tcx: TyCtxt<'a, 'tcx, 'tcx>) {\n     let ref declared_lib_features = sess.features.borrow().declared_lib_features;\n     let mut remaining_lib_features: FxHashMap<Symbol, Span>\n         = declared_lib_features.clone().into_iter().collect();\n+    remaining_lib_features.remove(&Symbol::intern(\"proc_macro\"));\n \n     fn format_stable_since_msg(version: &str) -> String {\n         format!(\"this feature has been stable since {}. Attribute no longer needed\", version)"}, {"sha": "0b950787e3b91123a87ed6b7d653648adf870ed7", "filename": "src/librustc_driver/Cargo.toml", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "blob_url": "https://github.com/rust-lang/rust/blob/4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e/src%2Flibrustc_driver%2FCargo.toml", "raw_url": "https://github.com/rust-lang/rust/raw/4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e/src%2Flibrustc_driver%2FCargo.toml", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_driver%2FCargo.toml?ref=4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e", "patch": "@@ -13,7 +13,6 @@ arena = { path = \"../libarena\" }\n graphviz = { path = \"../libgraphviz\" }\n log = { version = \"0.3\", features = [\"release_max_level_info\"] }\n env_logger = { version = \"0.4\", default-features = false }\n-proc_macro_plugin = { path = \"../libproc_macro_plugin\" }\n rustc = { path = \"../librustc\" }\n rustc_back = { path = \"../librustc_back\" }\n rustc_borrowck = { path = \"../librustc_borrowck\" }"}, {"sha": "54138e2e3b028461ae4ecd4cead4485e7d9abce7", "filename": "src/librustc_metadata/cstore_impl.rs", "status": "modified", "additions": 6, "deletions": 1, "changes": 7, "blob_url": "https://github.com/rust-lang/rust/blob/4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e/src%2Flibrustc_metadata%2Fcstore_impl.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e/src%2Flibrustc_metadata%2Fcstore_impl.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_metadata%2Fcstore_impl.rs?ref=4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e", "patch": "@@ -33,6 +33,7 @@ use std::rc::Rc;\n \n use syntax::ast;\n use syntax::attr;\n+use syntax::ext::base::SyntaxExtension;\n use syntax::parse::filemap_to_stream;\n use syntax::symbol::Symbol;\n use syntax_pos::{Span, NO_EXPANSION};\n@@ -365,14 +366,18 @@ impl CrateStore for cstore::CStore {\n         let data = self.get_crate_data(id.krate);\n         if let Some(ref proc_macros) = data.proc_macros {\n             return LoadedMacro::ProcMacro(proc_macros[id.index.as_usize() - 1].1.clone());\n+        } else if data.name == \"proc_macro\" &&\n+                  self.get_crate_data(id.krate).item_name(id.index) == \"quote\" {\n+            let ext = SyntaxExtension::ProcMacro(Box::new(::proc_macro::__internal::Quoter));\n+            return LoadedMacro::ProcMacro(Rc::new(ext));\n         }\n \n         let (name, def) = data.get_macro(id.index);\n         let source_name = format!(\"<{} macros>\", name);\n \n         let filemap = sess.parse_sess.codemap().new_filemap(source_name, def.body);\n         let local_span = Span { lo: filemap.start_pos, hi: filemap.end_pos, ctxt: NO_EXPANSION };\n-        let body = filemap_to_stream(&sess.parse_sess, filemap);\n+        let body = filemap_to_stream(&sess.parse_sess, filemap, None);\n \n         // Mark the attrs as used\n         let attrs = data.get_item_attrs(id.index, &self.dep_graph);"}, {"sha": "ad3a9dd9fefaf4c354000aef492aa2110e290d7a", "filename": "src/librustc_metadata/decoder.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e/src%2Flibrustc_metadata%2Fdecoder.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e/src%2Flibrustc_metadata%2Fdecoder.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_metadata%2Fdecoder.rs?ref=4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e", "patch": "@@ -472,7 +472,7 @@ impl<'a, 'tcx> CrateMetadata {\n         }\n     }\n \n-    fn item_name(&self, item_index: DefIndex) -> ast::Name {\n+    pub fn item_name(&self, item_index: DefIndex) -> ast::Name {\n         self.def_key(item_index)\n             .disambiguated_data\n             .data"}, {"sha": "f74aac255a03909a33a10192b879dad55d8195fe", "filename": "src/librustc_metadata/encoder.rs", "status": "modified", "additions": 4, "deletions": 3, "changes": 7, "blob_url": "https://github.com/rust-lang/rust/blob/4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e/src%2Flibrustc_metadata%2Fencoder.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e/src%2Flibrustc_metadata%2Fencoder.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_metadata%2Fencoder.rs?ref=4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e", "patch": "@@ -1129,18 +1129,19 @@ impl<'a, 'b: 'a, 'tcx: 'b> IsolatedEncoder<'a, 'b, 'tcx> {\n     /// Serialize the text of exported macros\n     fn encode_info_for_macro_def(&mut self, macro_def: &hir::MacroDef) -> Entry<'tcx> {\n         use syntax::print::pprust;\n+        let def_id = self.tcx.hir.local_def_id(macro_def.id);\n         Entry {\n             kind: EntryKind::MacroDef(self.lazy(&MacroDef {\n                 body: pprust::tts_to_string(&macro_def.body.trees().collect::<Vec<_>>()),\n                 legacy: macro_def.legacy,\n             })),\n             visibility: self.lazy(&ty::Visibility::Public),\n             span: self.lazy(&macro_def.span),\n-\n             attributes: self.encode_attributes(&macro_def.attrs),\n+            stability: self.encode_stability(def_id),\n+            deprecation: self.encode_deprecation(def_id),\n+\n             children: LazySeq::empty(),\n-            stability: None,\n-            deprecation: None,\n             ty: None,\n             inherent_impls: LazySeq::empty(),\n             variances: LazySeq::empty(),"}, {"sha": "89a40b0db9662f76bf34b23368fd4f6b72bba0af", "filename": "src/librustdoc/html/highlight.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e/src%2Flibrustdoc%2Fhtml%2Fhighlight.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e/src%2Flibrustdoc%2Fhtml%2Fhighlight.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustdoc%2Fhtml%2Fhighlight.rs?ref=4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e", "patch": "@@ -319,7 +319,7 @@ impl<'a> Classifier<'a> {\n             token::Lifetime(..) => Class::Lifetime,\n \n             token::Underscore | token::Eof | token::Interpolated(..) |\n-            token::SubstNt(..) | token::Tilde | token::At => Class::None,\n+            token::Tilde | token::At => Class::None,\n         };\n \n         // Anything that didn't return above is the simple case where we the"}, {"sha": "d00e29d954fc6938821b18763664762abcbb6d75", "filename": "src/libsyntax/ast.rs", "status": "modified", "additions": 4, "deletions": 4, "changes": 8, "blob_url": "https://github.com/rust-lang/rust/blob/4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e/src%2Flibsyntax%2Fast.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e/src%2Flibsyntax%2Fast.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fast.rs?ref=4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e", "patch": "@@ -99,7 +99,7 @@ impl Path {\n     pub fn default_to_global(mut self) -> Path {\n         if !self.is_global() &&\n            !::parse::token::Ident(self.segments[0].identifier).is_path_segment_keyword() {\n-            self.segments.insert(0, PathSegment::crate_root());\n+            self.segments.insert(0, PathSegment::crate_root(self.span));\n         }\n         self\n     }\n@@ -133,10 +133,10 @@ impl PathSegment {\n     pub fn from_ident(ident: Ident, span: Span) -> Self {\n         PathSegment { identifier: ident, span: span, parameters: None }\n     }\n-    pub fn crate_root() -> Self {\n+    pub fn crate_root(span: Span) -> Self {\n         PathSegment {\n-            identifier: keywords::CrateRoot.ident(),\n-            span: DUMMY_SP,\n+            identifier: Ident { ctxt: span.ctxt, ..keywords::CrateRoot.ident() },\n+            span: span,\n             parameters: None,\n         }\n     }"}, {"sha": "f0fc849c0c596a67f4ad0935ab920f33cfa209f6", "filename": "src/libsyntax/attr.rs", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/rust-lang/rust/blob/4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e/src%2Flibsyntax%2Fattr.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e/src%2Flibsyntax%2Fattr.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fattr.rs?ref=4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e", "patch": "@@ -1057,7 +1057,7 @@ impl MetaItem {\n     {\n         let (mut span, name) = match tokens.next() {\n             Some(TokenTree::Token(span, Token::Ident(ident))) => (span, ident.name),\n-            Some(TokenTree::Token(_, Token::Interpolated(ref nt))) => match **nt {\n+            Some(TokenTree::Token(_, Token::Interpolated(ref nt))) => match nt.0 {\n                 token::Nonterminal::NtIdent(ident) => (ident.span, ident.node.name),\n                 token::Nonterminal::NtMeta(ref meta) => return Some(meta.clone()),\n                 _ => return None,\n@@ -1229,7 +1229,7 @@ impl LitKind {\n         match token {\n             Token::Ident(ident) if ident.name == \"true\" => Some(LitKind::Bool(true)),\n             Token::Ident(ident) if ident.name == \"false\" => Some(LitKind::Bool(false)),\n-            Token::Interpolated(ref nt) => match **nt {\n+            Token::Interpolated(ref nt) => match nt.0 {\n                 token::NtExpr(ref v) => match v.node {\n                     ExprKind::Lit(ref lit) => Some(lit.node.clone()),\n                     _ => None,"}, {"sha": "4881170c1d13af7aa8c9cf3d43ed93faff6c8b21", "filename": "src/libsyntax/ext/base.rs", "status": "modified", "additions": 5, "deletions": 16, "changes": 21, "blob_url": "https://github.com/rust-lang/rust/blob/4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e/src%2Flibsyntax%2Fext%2Fbase.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e/src%2Flibsyntax%2Fext%2Fbase.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fext%2Fbase.rs?ref=4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e", "patch": "@@ -215,7 +215,7 @@ impl<F> TTMacroExpander for F\n         impl Folder for AvoidInterpolatedIdents {\n             fn fold_tt(&mut self, tt: tokenstream::TokenTree) -> tokenstream::TokenTree {\n                 if let tokenstream::TokenTree::Token(_, token::Interpolated(ref nt)) = tt {\n-                    if let token::NtIdent(ident) = **nt {\n+                    if let token::NtIdent(ident) = nt.0 {\n                         return tokenstream::TokenTree::Token(ident.span, token::Ident(ident.node));\n                     }\n                 }\n@@ -578,7 +578,10 @@ impl SyntaxExtension {\n \n     pub fn is_modern(&self) -> bool {\n         match *self {\n-            SyntaxExtension::DeclMacro(..) => true,\n+            SyntaxExtension::DeclMacro(..) |\n+            SyntaxExtension::ProcMacro(..) |\n+            SyntaxExtension::AttrProcMacro(..) |\n+            SyntaxExtension::ProcMacroDerive(..) => true,\n             _ => false,\n         }\n     }\n@@ -903,17 +906,3 @@ pub fn get_exprs_from_tts(cx: &mut ExtCtxt,\n     }\n     Some(es)\n }\n-\n-pub struct ChangeSpan {\n-    pub span: Span\n-}\n-\n-impl Folder for ChangeSpan {\n-    fn new_span(&mut self, _sp: Span) -> Span {\n-        self.span\n-    }\n-\n-    fn fold_mac(&mut self, mac: ast::Mac) -> ast::Mac {\n-        fold::noop_fold_mac(mac, self)\n-    }\n-}"}, {"sha": "1eb749623d8ae3d9367f8806ec0378ad3fa4de3a", "filename": "src/libsyntax/ext/build.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e/src%2Flibsyntax%2Fext%2Fbuild.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e/src%2Flibsyntax%2Fext%2Fbuild.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fext%2Fbuild.rs?ref=4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e", "patch": "@@ -320,7 +320,7 @@ impl<'a> AstBuilder for ExtCtxt<'a> {\n         let last_identifier = idents.pop().unwrap();\n         let mut segments: Vec<ast::PathSegment> = Vec::new();\n         if global {\n-            segments.push(ast::PathSegment::crate_root());\n+            segments.push(ast::PathSegment::crate_root(sp));\n         }\n \n         segments.extend(idents.into_iter().map(|i| ast::PathSegment::from_ident(i, sp)));"}, {"sha": "d2e51c9cb4868df9be0c10938d4063b44569dde3", "filename": "src/libsyntax/ext/expand.rs", "status": "modified", "additions": 23, "deletions": 47, "changes": 70, "blob_url": "https://github.com/rust-lang/rust/blob/4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e/src%2Flibsyntax%2Fext%2Fexpand.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e/src%2Flibsyntax%2Fext%2Fexpand.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fext%2Fexpand.rs?ref=4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e", "patch": "@@ -16,20 +16,20 @@ use config::{is_test_or_bench, StripUnconfigured};\n use errors::FatalError;\n use ext::base::*;\n use ext::derive::{add_derived_markers, collect_derives};\n-use ext::hygiene::Mark;\n+use ext::hygiene::{Mark, SyntaxContext};\n use ext::placeholders::{placeholder, PlaceholderExpander};\n use feature_gate::{self, Features, is_builtin_attr};\n use fold;\n use fold::*;\n-use parse::{filemap_to_stream, ParseSess, DirectoryOwnership, PResult, token};\n+use parse::{DirectoryOwnership, PResult};\n+use parse::token::{self, Token};\n use parse::parser::Parser;\n-use print::pprust;\n use ptr::P;\n use std_inject;\n use symbol::Symbol;\n use symbol::keywords;\n use syntax_pos::{Span, DUMMY_SP};\n-use tokenstream::TokenStream;\n+use tokenstream::{TokenStream, TokenTree};\n use util::small_vector::SmallVector;\n use visit::Visitor;\n \n@@ -427,11 +427,13 @@ impl<'a, 'b> MacroExpander<'a, 'b> {\n                 kind.expect_from_annotatables(items)\n             }\n             SyntaxExtension::AttrProcMacro(ref mac) => {\n-                let item_toks = stream_for_item(&item, self.cx.parse_sess);\n-\n-                let span = Span { ctxt: self.cx.backtrace(), ..attr.span };\n-                let tok_result = mac.expand(self.cx, attr.span, attr.tokens, item_toks);\n-                self.parse_expansion(tok_result, kind, &attr.path, span)\n+                let item_tok = TokenTree::Token(DUMMY_SP, Token::interpolated(match item {\n+                    Annotatable::Item(item) => token::NtItem(item),\n+                    Annotatable::TraitItem(item) => token::NtTraitItem(item.unwrap()),\n+                    Annotatable::ImplItem(item) => token::NtImplItem(item.unwrap()),\n+                })).into();\n+                let tok_result = mac.expand(self.cx, attr.span, attr.tokens, item_tok);\n+                self.parse_expansion(tok_result, kind, &attr.path, attr.span)\n             }\n             SyntaxExtension::ProcMacroDerive(..) | SyntaxExtension::BuiltinDerive(..) => {\n                 self.cx.span_err(attr.span, &format!(\"`{}` is a derive mode\", attr.path));\n@@ -470,15 +472,14 @@ impl<'a, 'b> MacroExpander<'a, 'b> {\n             Ok(())\n         };\n \n-        let marked_tts = noop_fold_tts(mac.node.stream(), &mut Marker(mark));\n         let opt_expanded = match *ext {\n             SyntaxExtension::DeclMacro(ref expand, def_span) => {\n                 if let Err(msg) = validate_and_set_expn_info(def_span.map(|(_, s)| s),\n                                                              false) {\n                     self.cx.span_err(path.span, &msg);\n                     return kind.dummy(span);\n                 }\n-                kind.make_from(expand.expand(self.cx, span, marked_tts))\n+                kind.make_from(expand.expand(self.cx, span, mac.node.stream()))\n             }\n \n             NormalTT(ref expandfun, def_info, allow_internal_unstable) => {\n@@ -487,7 +488,7 @@ impl<'a, 'b> MacroExpander<'a, 'b> {\n                     self.cx.span_err(path.span, &msg);\n                     return kind.dummy(span);\n                 }\n-                kind.make_from(expandfun.expand(self.cx, span, marked_tts))\n+                kind.make_from(expandfun.expand(self.cx, span, mac.node.stream()))\n             }\n \n             IdentTT(ref expander, tt_span, allow_internal_unstable) => {\n@@ -506,7 +507,7 @@ impl<'a, 'b> MacroExpander<'a, 'b> {\n                     }\n                 });\n \n-                let input: Vec<_> = marked_tts.into_trees().collect();\n+                let input: Vec<_> = mac.node.stream().into_trees().collect();\n                 kind.make_from(expander.expand(self.cx, span, ident, input))\n             }\n \n@@ -541,21 +542,17 @@ impl<'a, 'b> MacroExpander<'a, 'b> {\n                     },\n                 });\n \n-                let tok_result = expandfun.expand(self.cx, span, marked_tts);\n+                let tok_result = expandfun.expand(self.cx, span, mac.node.stream());\n                 Some(self.parse_expansion(tok_result, kind, path, span))\n             }\n         };\n \n-        let expanded = if let Some(expanded) = opt_expanded {\n-            expanded\n-        } else {\n+        unwrap_or!(opt_expanded, {\n             let msg = format!(\"non-{kind} macro in {kind} position: {name}\",\n                               name = path.segments[0].identifier.name, kind = kind.name());\n             self.cx.span_err(path.span, &msg);\n-            return kind.dummy(span);\n-        };\n-\n-        expanded.fold_with(&mut Marker(mark))\n+            kind.dummy(span)\n+        })\n     }\n \n     /// Expand a derive invocation. Returns the result of expansion.\n@@ -621,8 +618,7 @@ impl<'a, 'b> MacroExpander<'a, 'b> {\n             }\n         };\n         parser.ensure_complete_parse(path, kind.name(), span);\n-        // FIXME better span info\n-        expansion.fold_with(&mut ChangeSpan { span: span })\n+        expansion\n     }\n }\n \n@@ -673,7 +669,9 @@ impl<'a> Parser<'a> {\n         if self.token != token::Eof {\n             let msg = format!(\"macro expansion ignores token `{}` and any following\",\n                               self.this_token_to_string());\n-            let mut err = self.diagnostic().struct_span_err(self.span, &msg);\n+            let mut def_site_span = self.span;\n+            def_site_span.ctxt = SyntaxContext::empty(); // Avoid emitting backtrace info twice.\n+            let mut err = self.diagnostic().struct_span_err(def_site_span, &msg);\n             let msg = format!(\"caused by the macro expansion here; the usage \\\n                                of `{}!` is likely invalid in {} context\",\n                                macro_path, kind_name);\n@@ -773,28 +771,6 @@ pub fn find_attr_invoc(attrs: &mut Vec<ast::Attribute>) -> Option<ast::Attribute\n          .map(|i| attrs.remove(i))\n }\n \n-// These are pretty nasty. Ideally, we would keep the tokens around, linked from\n-// the AST. However, we don't so we need to create new ones. Since the item might\n-// have come from a macro expansion (possibly only in part), we can't use the\n-// existing codemap.\n-//\n-// Therefore, we must use the pretty printer (yuck) to turn the AST node into a\n-// string, which we then re-tokenise (double yuck), but first we have to patch\n-// the pretty-printed string on to the end of the existing codemap (infinity-yuck).\n-fn stream_for_item(item: &Annotatable, parse_sess: &ParseSess) -> TokenStream {\n-    let text = match *item {\n-        Annotatable::Item(ref i) => pprust::item_to_string(i),\n-        Annotatable::TraitItem(ref ti) => pprust::trait_item_to_string(ti),\n-        Annotatable::ImplItem(ref ii) => pprust::impl_item_to_string(ii),\n-    };\n-    string_to_stream(text, parse_sess)\n-}\n-\n-fn string_to_stream(text: String, parse_sess: &ParseSess) -> TokenStream {\n-    let filename = String::from(\"<macro expansion>\");\n-    filemap_to_stream(parse_sess, parse_sess.codemap().new_filemap(filename, text))\n-}\n-\n impl<'a, 'b> Folder for InvocationCollector<'a, 'b> {\n     fn fold_expr(&mut self, expr: P<ast::Expr>) -> P<ast::Expr> {\n         let mut expr = self.cfg.configure_expr(expr).unwrap();\n@@ -1070,7 +1046,7 @@ impl<'feat> ExpansionConfig<'feat> {\n }\n \n // A Marker adds the given mark to the syntax context.\n-struct Marker(Mark);\n+pub struct Marker(pub Mark);\n \n impl Folder for Marker {\n     fn fold_ident(&mut self, mut ident: Ident) -> Ident {"}, {"sha": "9907dfe341e75d4465789a2a1ef6316f992a3b9e", "filename": "src/libsyntax/ext/quote.rs", "status": "modified", "additions": 20, "deletions": 20, "changes": 40, "blob_url": "https://github.com/rust-lang/rust/blob/4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e/src%2Flibsyntax%2Fext%2Fquote.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e/src%2Flibsyntax%2Fext%2Fquote.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fext%2Fquote.rs?ref=4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e", "patch": "@@ -30,9 +30,9 @@ pub mod rt {\n     use ast;\n     use codemap::Spanned;\n     use ext::base::ExtCtxt;\n-    use parse::{self, token, classify};\n+    use parse::{self, classify};\n+    use parse::token::{self, Token};\n     use ptr::P;\n-    use std::rc::Rc;\n     use symbol::Symbol;\n \n     use tokenstream::{self, TokenTree, TokenStream};\n@@ -82,70 +82,70 @@ pub mod rt {\n     impl ToTokens for ast::Path {\n         fn to_tokens(&self, _cx: &ExtCtxt) -> Vec<TokenTree> {\n             let nt = token::NtPath(self.clone());\n-            vec![TokenTree::Token(DUMMY_SP, token::Interpolated(Rc::new(nt)))]\n+            vec![TokenTree::Token(DUMMY_SP, Token::interpolated(nt))]\n         }\n     }\n \n     impl ToTokens for ast::Ty {\n         fn to_tokens(&self, _cx: &ExtCtxt) -> Vec<TokenTree> {\n             let nt = token::NtTy(P(self.clone()));\n-            vec![TokenTree::Token(self.span, token::Interpolated(Rc::new(nt)))]\n+            vec![TokenTree::Token(self.span, Token::interpolated(nt))]\n         }\n     }\n \n     impl ToTokens for ast::Block {\n         fn to_tokens(&self, _cx: &ExtCtxt) -> Vec<TokenTree> {\n             let nt = token::NtBlock(P(self.clone()));\n-            vec![TokenTree::Token(self.span, token::Interpolated(Rc::new(nt)))]\n+            vec![TokenTree::Token(self.span, Token::interpolated(nt))]\n         }\n     }\n \n     impl ToTokens for ast::Generics {\n         fn to_tokens(&self, _cx: &ExtCtxt) -> Vec<TokenTree> {\n             let nt = token::NtGenerics(self.clone());\n-            vec![TokenTree::Token(DUMMY_SP, token::Interpolated(Rc::new(nt)))]\n+            vec![TokenTree::Token(DUMMY_SP, Token::interpolated(nt))]\n         }\n     }\n \n     impl ToTokens for ast::WhereClause {\n         fn to_tokens(&self, _cx: &ExtCtxt) -> Vec<TokenTree> {\n             let nt = token::NtWhereClause(self.clone());\n-            vec![TokenTree::Token(DUMMY_SP, token::Interpolated(Rc::new(nt)))]\n+            vec![TokenTree::Token(DUMMY_SP, Token::interpolated(nt))]\n         }\n     }\n \n     impl ToTokens for P<ast::Item> {\n         fn to_tokens(&self, _cx: &ExtCtxt) -> Vec<TokenTree> {\n             let nt = token::NtItem(self.clone());\n-            vec![TokenTree::Token(self.span, token::Interpolated(Rc::new(nt)))]\n+            vec![TokenTree::Token(self.span, Token::interpolated(nt))]\n         }\n     }\n \n     impl ToTokens for ast::ImplItem {\n         fn to_tokens(&self, _cx: &ExtCtxt) -> Vec<TokenTree> {\n             let nt = token::NtImplItem(self.clone());\n-            vec![TokenTree::Token(self.span, token::Interpolated(Rc::new(nt)))]\n+            vec![TokenTree::Token(self.span, Token::interpolated(nt))]\n         }\n     }\n \n     impl ToTokens for P<ast::ImplItem> {\n         fn to_tokens(&self, _cx: &ExtCtxt) -> Vec<TokenTree> {\n             let nt = token::NtImplItem((**self).clone());\n-            vec![TokenTree::Token(self.span, token::Interpolated(Rc::new(nt)))]\n+            vec![TokenTree::Token(self.span, Token::interpolated(nt))]\n         }\n     }\n \n     impl ToTokens for ast::TraitItem {\n         fn to_tokens(&self, _cx: &ExtCtxt) -> Vec<TokenTree> {\n             let nt = token::NtTraitItem(self.clone());\n-            vec![TokenTree::Token(self.span, token::Interpolated(Rc::new(nt)))]\n+            vec![TokenTree::Token(self.span, Token::interpolated(nt))]\n         }\n     }\n \n     impl ToTokens for ast::Stmt {\n         fn to_tokens(&self, _cx: &ExtCtxt) -> Vec<TokenTree> {\n             let nt = token::NtStmt(self.clone());\n-            let mut tts = vec![TokenTree::Token(self.span, token::Interpolated(Rc::new(nt)))];\n+            let mut tts = vec![TokenTree::Token(self.span, Token::interpolated(nt))];\n \n             // Some statements require a trailing semicolon.\n             if classify::stmt_ends_with_semi(&self.node) {\n@@ -159,35 +159,35 @@ pub mod rt {\n     impl ToTokens for P<ast::Expr> {\n         fn to_tokens(&self, _cx: &ExtCtxt) -> Vec<TokenTree> {\n             let nt = token::NtExpr(self.clone());\n-            vec![TokenTree::Token(self.span, token::Interpolated(Rc::new(nt)))]\n+            vec![TokenTree::Token(self.span, Token::interpolated(nt))]\n         }\n     }\n \n     impl ToTokens for P<ast::Pat> {\n         fn to_tokens(&self, _cx: &ExtCtxt) -> Vec<TokenTree> {\n             let nt = token::NtPat(self.clone());\n-            vec![TokenTree::Token(self.span, token::Interpolated(Rc::new(nt)))]\n+            vec![TokenTree::Token(self.span, Token::interpolated(nt))]\n         }\n     }\n \n     impl ToTokens for ast::Arm {\n         fn to_tokens(&self, _cx: &ExtCtxt) -> Vec<TokenTree> {\n             let nt = token::NtArm(self.clone());\n-            vec![TokenTree::Token(DUMMY_SP, token::Interpolated(Rc::new(nt)))]\n+            vec![TokenTree::Token(DUMMY_SP, Token::interpolated(nt))]\n         }\n     }\n \n     impl ToTokens for ast::Arg {\n         fn to_tokens(&self, _cx: &ExtCtxt) -> Vec<TokenTree> {\n             let nt = token::NtArg(self.clone());\n-            vec![TokenTree::Token(DUMMY_SP, token::Interpolated(Rc::new(nt)))]\n+            vec![TokenTree::Token(DUMMY_SP, Token::interpolated(nt))]\n         }\n     }\n \n     impl ToTokens for P<ast::Block> {\n         fn to_tokens(&self, _cx: &ExtCtxt) -> Vec<TokenTree> {\n             let nt = token::NtBlock(self.clone());\n-            vec![TokenTree::Token(DUMMY_SP, token::Interpolated(Rc::new(nt)))]\n+            vec![TokenTree::Token(DUMMY_SP, Token::interpolated(nt))]\n         }\n     }\n \n@@ -215,7 +215,7 @@ pub mod rt {\n     impl ToTokens for ast::MetaItem {\n         fn to_tokens(&self, _cx: &ExtCtxt) -> Vec<TokenTree> {\n             let nt = token::NtMeta(self.clone());\n-            vec![TokenTree::Token(DUMMY_SP, token::Interpolated(Rc::new(nt)))]\n+            vec![TokenTree::Token(DUMMY_SP, Token::interpolated(nt))]\n         }\n     }\n \n@@ -364,7 +364,7 @@ pub mod rt {\n \n         fn parse_tts(&self, s: String) -> Vec<TokenTree> {\n             let source_name = \"<quote expansion>\".to_owned();\n-            parse::parse_stream_from_source_str(source_name, s, self.parse_sess())\n+            parse::parse_stream_from_source_str(source_name, s, self.parse_sess(), None)\n                 .into_trees().collect()\n         }\n     }\n@@ -700,7 +700,7 @@ fn expr_mk_token(cx: &ExtCtxt, sp: Span, tok: &token::Token) -> P<ast::Expr> {\n         token::Underscore   => \"Underscore\",\n         token::Eof          => \"Eof\",\n \n-        token::Whitespace | token::SubstNt(_) | token::Comment | token::Shebang(_) => {\n+        token::Whitespace | token::Comment | token::Shebang(_) => {\n             panic!(\"unhandled token in quote!\");\n         }\n     };"}, {"sha": "e877f1fedd40980c08f1f921cc78e95d80fe7a31", "filename": "src/libsyntax/ext/tt/macro_parser.rs", "status": "modified", "additions": 9, "deletions": 15, "changes": 24, "blob_url": "https://github.com/rust-lang/rust/blob/4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e/src%2Flibsyntax%2Fext%2Ftt%2Fmacro_parser.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e/src%2Flibsyntax%2Fext%2Ftt%2Fmacro_parser.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fext%2Ftt%2Fmacro_parser.rs?ref=4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e", "patch": "@@ -158,15 +158,10 @@ pub type NamedParseResult = ParseResult<HashMap<Ident, Rc<NamedMatch>>>;\n pub fn count_names(ms: &[TokenTree]) -> usize {\n     ms.iter().fold(0, |count, elt| {\n         count + match *elt {\n-            TokenTree::Sequence(_, ref seq) => {\n-                seq.num_captures\n-            }\n-            TokenTree::Delimited(_, ref delim) => {\n-                count_names(&delim.tts)\n-            }\n-            TokenTree::MetaVarDecl(..) => {\n-                1\n-            }\n+            TokenTree::Sequence(_, ref seq) => seq.num_captures,\n+            TokenTree::Delimited(_, ref delim) => count_names(&delim.tts),\n+            TokenTree::MetaVar(..) => 0,\n+            TokenTree::MetaVarDecl(..) => 1,\n             TokenTree::Token(..) => 0,\n         }\n     })\n@@ -244,7 +239,7 @@ fn nameize<I: Iterator<Item=NamedMatch>>(sess: &ParseSess, ms: &[TokenTree], mut\n                     }\n                 }\n             }\n-            TokenTree::Token(..) => (),\n+            TokenTree::MetaVar(..) | TokenTree::Token(..) => (),\n         }\n \n         Ok(())\n@@ -409,12 +404,11 @@ fn inner_parse_loop(sess: &ParseSess,\n                     ei.idx = 0;\n                     cur_eis.push(ei);\n                 }\n-                TokenTree::Token(_, ref t) => {\n-                    if token_name_eq(t, token) {\n-                        ei.idx += 1;\n-                        next_eis.push(ei);\n-                    }\n+                TokenTree::Token(_, ref t) if token_name_eq(t, token) => {\n+                    ei.idx += 1;\n+                    next_eis.push(ei);\n                 }\n+                TokenTree::Token(..) | TokenTree::MetaVar(..) => {}\n             }\n         }\n     }"}, {"sha": "b732f47ce6a93da34d7fdbb87e2169e5d9a376fc", "filename": "src/libsyntax/ext/tt/macro_rules.rs", "status": "modified", "additions": 6, "deletions": 5, "changes": 11, "blob_url": "https://github.com/rust-lang/rust/blob/4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e/src%2Flibsyntax%2Fext%2Ftt%2Fmacro_rules.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e/src%2Flibsyntax%2Fext%2Ftt%2Fmacro_rules.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fext%2Ftt%2Fmacro_rules.rs?ref=4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e", "patch": "@@ -120,7 +120,7 @@ fn generic_extension<'cx>(cx: &'cx mut ExtCtxt,\n                     _ => cx.span_bug(sp, \"malformed macro rhs\"),\n                 };\n                 // rhs has holes ( `$id` and `$(...)` that need filled)\n-                let tts = transcribe(&cx.parse_sess.span_diagnostic, Some(named_matches), rhs);\n+                let tts = transcribe(cx, Some(named_matches), rhs);\n \n                 if cx.trace_macros() {\n                     trace_macros_note(cx, sp, format!(\"to `{}`\", tts));\n@@ -292,7 +292,7 @@ fn check_lhs_no_empty_seq(sess: &ParseSess, tts: &[quoted::TokenTree]) -> bool {\n     use self::quoted::TokenTree;\n     for tt in tts {\n         match *tt {\n-            TokenTree::Token(..) | TokenTree::MetaVarDecl(..) => (),\n+            TokenTree::Token(..) | TokenTree::MetaVar(..) | TokenTree::MetaVarDecl(..) => (),\n             TokenTree::Delimited(_, ref del) => if !check_lhs_no_empty_seq(sess, &del.tts) {\n                 return false;\n             },\n@@ -372,7 +372,7 @@ impl FirstSets {\n             let mut first = TokenSet::empty();\n             for tt in tts.iter().rev() {\n                 match *tt {\n-                    TokenTree::Token(..) | TokenTree::MetaVarDecl(..) => {\n+                    TokenTree::Token(..) | TokenTree::MetaVar(..) | TokenTree::MetaVarDecl(..) => {\n                         first.replace_with(tt.clone());\n                     }\n                     TokenTree::Delimited(span, ref delimited) => {\n@@ -432,7 +432,7 @@ impl FirstSets {\n         for tt in tts.iter() {\n             assert!(first.maybe_empty);\n             match *tt {\n-                TokenTree::Token(..) | TokenTree::MetaVarDecl(..) => {\n+                TokenTree::Token(..) | TokenTree::MetaVar(..) | TokenTree::MetaVarDecl(..) => {\n                     first.add_one(tt.clone());\n                     return first;\n                 }\n@@ -602,7 +602,7 @@ fn check_matcher_core(sess: &ParseSess,\n         // First, update `last` so that it corresponds to the set\n         // of NT tokens that might end the sequence `... token`.\n         match *token {\n-            TokenTree::Token(..) | TokenTree::MetaVarDecl(..) => {\n+            TokenTree::Token(..) | TokenTree::MetaVar(..) | TokenTree::MetaVarDecl(..) => {\n                 let can_be_followed_by_any;\n                 if let Err(bad_frag) = has_legal_fragment_specifier(sess, features, token) {\n                     let msg = format!(\"invalid fragment specifier `{}`\", bad_frag);\n@@ -872,6 +872,7 @@ fn is_legal_fragment_specifier(sess: &ParseSess,\n fn quoted_tt_to_string(tt: &quoted::TokenTree) -> String {\n     match *tt {\n         quoted::TokenTree::Token(_, ref tok) => ::print::pprust::token_to_string(tok),\n+        quoted::TokenTree::MetaVar(_, name) => format!(\"${}\", name),\n         quoted::TokenTree::MetaVarDecl(_, name, kind) => format!(\"${}:{}\", name, kind),\n         _ => panic!(\"unexpected quoted::TokenTree::{{Sequence or Delimited}} \\\n                      in follow set checker\"),"}, {"sha": "4e9e30857b1e88df08dcd51252488c2856ccaa97", "filename": "src/libsyntax/ext/tt/quoted.rs", "status": "modified", "additions": 8, "deletions": 5, "changes": 13, "blob_url": "https://github.com/rust-lang/rust/blob/4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e/src%2Flibsyntax%2Fext%2Ftt%2Fquoted.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e/src%2Flibsyntax%2Fext%2Ftt%2Fquoted.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fext%2Ftt%2Fquoted.rs?ref=4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e", "patch": "@@ -78,9 +78,11 @@ pub enum KleeneOp {\n pub enum TokenTree {\n     Token(Span, token::Token),\n     Delimited(Span, Rc<Delimited>),\n-    /// A kleene-style repetition sequence with a span\n+    /// A kleene-style repetition sequence\n     Sequence(Span, Rc<SequenceRepetition>),\n-    /// Matches a nonterminal. This is only used in the left hand side of MBE macros.\n+    /// E.g. `$var`\n+    MetaVar(Span, ast::Ident),\n+    /// E.g. `$var:expr`. This is only used in the left hand side of MBE macros.\n     MetaVarDecl(Span, ast::Ident /* name to bind */, ast::Ident /* kind of nonterminal */),\n }\n \n@@ -130,6 +132,7 @@ impl TokenTree {\n     pub fn span(&self) -> Span {\n         match *self {\n             TokenTree::Token(sp, _) |\n+            TokenTree::MetaVar(sp, _) |\n             TokenTree::MetaVarDecl(sp, _, _) |\n             TokenTree::Delimited(sp, _) |\n             TokenTree::Sequence(sp, _) => sp,\n@@ -144,7 +147,7 @@ pub fn parse(input: tokenstream::TokenStream, expect_matchers: bool, sess: &Pars\n     while let Some(tree) = trees.next() {\n         let tree = parse_tree(tree, &mut trees, expect_matchers, sess);\n         match tree {\n-            TokenTree::Token(start_sp, token::SubstNt(ident)) if expect_matchers => {\n+            TokenTree::MetaVar(start_sp, ident) if expect_matchers => {\n                 let span = match trees.next() {\n                     Some(tokenstream::TokenTree::Token(span, token::Colon)) => match trees.next() {\n                         Some(tokenstream::TokenTree::Token(end_sp, ref tok)) => match tok.ident() {\n@@ -199,13 +202,13 @@ fn parse_tree<I>(tree: tokenstream::TokenTree,\n                     let ident = ast::Ident { name: keywords::DollarCrate.name(), ..ident };\n                     TokenTree::Token(span, token::Ident(ident))\n                 } else {\n-                    TokenTree::Token(span, token::SubstNt(ident))\n+                    TokenTree::MetaVar(span, ident)\n                 }\n             }\n             Some(tokenstream::TokenTree::Token(span, tok)) => {\n                 let msg = format!(\"expected identifier, found `{}`\", pprust::token_to_string(&tok));\n                 sess.span_diagnostic.span_err(span, &msg);\n-                TokenTree::Token(span, token::SubstNt(keywords::Invalid.ident()))\n+                TokenTree::MetaVar(span, keywords::Invalid.ident())\n             }\n             None => TokenTree::Token(span, token::Dollar),\n         },"}, {"sha": "fe3dd83f9d5c0e60af278b297feba77380eb4608", "filename": "src/libsyntax/ext/tt/transcribe.rs", "status": "modified", "additions": 34, "deletions": 26, "changes": 60, "blob_url": "https://github.com/rust-lang/rust/blob/4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e/src%2Flibsyntax%2Fext%2Ftt%2Ftranscribe.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e/src%2Flibsyntax%2Fext%2Ftt%2Ftranscribe.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fext%2Ftt%2Ftranscribe.rs?ref=4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e", "patch": "@@ -9,10 +9,12 @@\n // except according to those terms.\n \n use ast::Ident;\n-use errors::Handler;\n+use ext::base::ExtCtxt;\n+use ext::expand::Marker;\n use ext::tt::macro_parser::{NamedMatch, MatchedSeq, MatchedNonterminal};\n use ext::tt::quoted;\n-use parse::token::{self, SubstNt, Token, NtTT};\n+use fold::noop_fold_tt;\n+use parse::token::{self, Token, NtTT};\n use syntax_pos::{Span, DUMMY_SP};\n use tokenstream::{TokenStream, TokenTree, Delimited};\n use util::small_vector::SmallVector;\n@@ -61,9 +63,9 @@ impl Iterator for Frame {\n }\n \n /// This can do Macro-By-Example transcription. On the other hand, if\n-/// `src` contains no `TokenTree::{Sequence, Match}`s, or `SubstNt`s, `interp` can\n+/// `src` contains no `TokenTree::{Sequence, MetaVar, MetaVarDecl}`s, `interp` can\n /// (and should) be None.\n-pub fn transcribe(sp_diag: &Handler,\n+pub fn transcribe(cx: &ExtCtxt,\n                   interp: Option<HashMap<Ident, Rc<NamedMatch>>>,\n                   src: Vec<quoted::TokenTree>)\n                   -> TokenStream {\n@@ -120,22 +122,20 @@ pub fn transcribe(sp_diag: &Handler,\n                                          &interpolations,\n                                          &repeats) {\n                     LockstepIterSize::Unconstrained => {\n-                        panic!(sp_diag.span_fatal(\n-                            sp, /* blame macro writer */\n+                        cx.span_fatal(sp, /* blame macro writer */\n                             \"attempted to repeat an expression \\\n                              containing no syntax \\\n-                             variables matched as repeating at this depth\"));\n+                             variables matched as repeating at this depth\");\n                     }\n                     LockstepIterSize::Contradiction(ref msg) => {\n                         // FIXME #2887 blame macro invoker instead\n-                        panic!(sp_diag.span_fatal(sp, &msg[..]));\n+                        cx.span_fatal(sp, &msg[..]);\n                     }\n                     LockstepIterSize::Constraint(len, _) => {\n                         if len == 0 {\n                             if seq.op == quoted::KleeneOp::OneOrMore {\n                                 // FIXME #2887 blame invoker\n-                                panic!(sp_diag.span_fatal(sp,\n-                                                          \"this must repeat at least once\"));\n+                                cx.span_fatal(sp, \"this must repeat at least once\");\n                             }\n                         } else {\n                             repeats.push((0, len));\n@@ -149,29 +149,37 @@ pub fn transcribe(sp_diag: &Handler,\n                 }\n             }\n             // FIXME #2887: think about span stuff here\n-            quoted::TokenTree::Token(sp, SubstNt(ident)) => {\n-                match lookup_cur_matched(ident, &interpolations, &repeats) {\n-                    None => result.push(TokenTree::Token(sp, SubstNt(ident)).into()),\n-                    Some(cur_matched) => if let MatchedNonterminal(ref nt) = *cur_matched {\n-                        match **nt {\n-                            NtTT(ref tt) => result.push(tt.clone().into()),\n-                            _ => {\n-                                let token = TokenTree::Token(sp, token::Interpolated(nt.clone()));\n-                                result.push(token.into());\n-                            }\n+            quoted::TokenTree::MetaVar(mut sp, ident) => {\n+                if let Some(cur_matched) = lookup_cur_matched(ident, &interpolations, &repeats) {\n+                    if let MatchedNonterminal(ref nt) = *cur_matched {\n+                        if let NtTT(ref tt) = **nt {\n+                            result.push(tt.clone().into());\n+                        } else {\n+                            sp.ctxt = sp.ctxt.apply_mark(cx.current_expansion.mark);\n+                            let token = TokenTree::Token(sp, Token::interpolated((**nt).clone()));\n+                            result.push(token.into());\n                         }\n                     } else {\n-                        panic!(sp_diag.span_fatal(\n-                            sp, /* blame the macro writer */\n-                            &format!(\"variable '{}' is still repeating at this depth\", ident)));\n+                        cx.span_fatal(sp, /* blame the macro writer */\n+                            &format!(\"variable '{}' is still repeating at this depth\", ident));\n                     }\n+                } else {\n+                    let ident =\n+                        Ident { ctxt: ident.ctxt.apply_mark(cx.current_expansion.mark), ..ident };\n+                    sp.ctxt = sp.ctxt.apply_mark(cx.current_expansion.mark);\n+                    result.push(TokenTree::Token(sp, token::Dollar).into());\n+                    result.push(TokenTree::Token(sp, token::Ident(ident)).into());\n                 }\n             }\n-            quoted::TokenTree::Delimited(span, delimited) => {\n+            quoted::TokenTree::Delimited(mut span, delimited) => {\n+                span.ctxt = span.ctxt.apply_mark(cx.current_expansion.mark);\n                 stack.push(Frame::Delimited { forest: delimited, idx: 0, span: span });\n                 result_stack.push(mem::replace(&mut result, Vec::new()));\n             }\n-            quoted::TokenTree::Token(span, tok) => result.push(TokenTree::Token(span, tok).into()),\n+            quoted::TokenTree::Token(sp, tok) => {\n+                let mut marker = Marker(cx.current_expansion.mark);\n+                result.push(noop_fold_tt(TokenTree::Token(sp, tok), &mut marker).into())\n+            }\n             quoted::TokenTree::MetaVarDecl(..) => panic!(\"unexpected `TokenTree::MetaVarDecl\"),\n         }\n     }\n@@ -240,7 +248,7 @@ fn lockstep_iter_size(tree: &quoted::TokenTree,\n                 size + lockstep_iter_size(tt, interpolations, repeats)\n             })\n         },\n-        TokenTree::Token(_, SubstNt(name)) | TokenTree::MetaVarDecl(_, name, _) =>\n+        TokenTree::MetaVar(_, name) | TokenTree::MetaVarDecl(_, name, _) =>\n             match lookup_cur_matched(name, interpolations, repeats) {\n                 Some(matched) => match *matched {\n                     MatchedNonterminal(_) => LockstepIterSize::Unconstrained,"}, {"sha": "4b0ec1a20e33177a59fab36c1c2dfad83f965e2e", "filename": "src/libsyntax/feature_gate.rs", "status": "modified", "additions": 17, "deletions": 11, "changes": 28, "blob_url": "https://github.com/rust-lang/rust/blob/4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e/src%2Flibsyntax%2Ffeature_gate.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e/src%2Flibsyntax%2Ffeature_gate.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Ffeature_gate.rs?ref=4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e", "patch": "@@ -38,23 +38,29 @@ use symbol::Symbol;\n use std::ascii::AsciiExt;\n use std::env;\n \n-macro_rules! setter {\n+macro_rules! set {\n+    (proc_macro) => {{\n+        fn f(features: &mut Features, span: Span) {\n+            features.declared_lib_features.push((Symbol::intern(\"proc_macro\"), span));\n+            features.proc_macro = true;\n+        }\n+        f as fn(&mut Features, Span)\n+    }};\n     ($field: ident) => {{\n-        fn f(features: &mut Features) -> &mut bool {\n-            &mut features.$field\n+        fn f(features: &mut Features, _: Span) {\n+            features.$field = true;\n         }\n-        f as fn(&mut Features) -> &mut bool\n+        f as fn(&mut Features, Span)\n     }}\n }\n \n macro_rules! declare_features {\n     ($((active, $feature: ident, $ver: expr, $issue: expr),)+) => {\n         /// Represents active features that are currently being implemented or\n         /// currently being considered for addition/removal.\n-        const ACTIVE_FEATURES: &'static [(&'static str, &'static str,\n-                                          Option<u32>, fn(&mut Features) -> &mut bool)] = &[\n-            $((stringify!($feature), $ver, $issue, setter!($feature))),+\n-        ];\n+        const ACTIVE_FEATURES:\n+                &'static [(&'static str, &'static str, Option<u32>, fn(&mut Features, Span))] =\n+            &[$((stringify!($feature), $ver, $issue, set!($feature))),+];\n \n         /// A set of features to be used by later passes.\n         pub struct Features {\n@@ -1478,9 +1484,9 @@ pub fn get_features(span_handler: &Handler, krate_attrs: &[ast::Attribute]) -> F\n                         continue\n                     };\n \n-                    if let Some(&(_, _, _, setter)) = ACTIVE_FEATURES.iter()\n+                    if let Some(&(_, _, _, set)) = ACTIVE_FEATURES.iter()\n                         .find(|& &(n, _, _, _)| name == n) {\n-                        *(setter(&mut features)) = true;\n+                        set(&mut features, mi.span);\n                         feature_checker.collect(&features, mi.span);\n                     }\n                     else if let Some(&(_, _, _)) = REMOVED_FEATURES.iter()\n@@ -1514,7 +1520,7 @@ struct MutexFeatureChecker {\n \n impl MutexFeatureChecker {\n     // If this method turns out to be a hotspot due to branching,\n-    // the branching can be eliminated by modifying `setter!()` to set these spans\n+    // the branching can be eliminated by modifying `set!()` to set these spans\n     // only for the features that need to be checked for mutual exclusion.\n     fn collect(&mut self, features: &Features, span: Span) {\n         if features.proc_macro {"}, {"sha": "1fc670ec9f7fb26dad735284192e131eb3d27d24", "filename": "src/libsyntax/fold.rs", "status": "modified", "additions": 3, "deletions": 4, "changes": 7, "blob_url": "https://github.com/rust-lang/rust/blob/4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e/src%2Flibsyntax%2Ffold.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e/src%2Flibsyntax%2Ffold.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Ffold.rs?ref=4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e", "patch": "@@ -22,7 +22,7 @@ use ast::*;\n use ast;\n use syntax_pos::Span;\n use codemap::{Spanned, respan};\n-use parse::token;\n+use parse::token::{self, Token};\n use ptr::P;\n use symbol::keywords;\n use tokenstream::*;\n@@ -573,7 +573,7 @@ pub fn noop_fold_tt<T: Folder>(tt: TokenTree, fld: &mut T) -> TokenTree {\n }\n \n pub fn noop_fold_tts<T: Folder>(tts: TokenStream, fld: &mut T) -> TokenStream {\n-    tts.trees().map(|tt| fld.fold_tt(tt)).collect()\n+    tts.map(|tt| fld.fold_tt(tt))\n }\n \n // apply ident folder if it's an ident, apply other folds to interpolated nodes\n@@ -586,9 +586,8 @@ pub fn noop_fold_token<T: Folder>(t: token::Token, fld: &mut T) -> token::Token\n                 Ok(nt) => nt,\n                 Err(nt) => (*nt).clone(),\n             };\n-            token::Interpolated(Rc::new(fld.fold_interpolated(nt)))\n+            Token::interpolated(fld.fold_interpolated(nt.0))\n         }\n-        token::SubstNt(ident) => token::SubstNt(fld.fold_ident(ident)),\n         _ => t\n     }\n }"}, {"sha": "c99a09ab24e6b076ff12d4e9dc3312829c5c4bdc", "filename": "src/libsyntax/parse/attr.rs", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/rust-lang/rust/blob/4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e/src%2Flibsyntax%2Fparse%2Fattr.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e/src%2Flibsyntax%2Fparse%2Fattr.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Fattr.rs?ref=4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e", "patch": "@@ -151,7 +151,7 @@ impl<'a> Parser<'a> {\n \n     pub fn parse_path_and_tokens(&mut self) -> PResult<'a, (ast::Path, TokenStream)> {\n         let meta = match self.token {\n-            token::Interpolated(ref nt) => match **nt {\n+            token::Interpolated(ref nt) => match nt.0 {\n                 Nonterminal::NtMeta(ref meta) => Some(meta.clone()),\n                 _ => None,\n             },\n@@ -223,7 +223,7 @@ impl<'a> Parser<'a> {\n     /// meta_item_inner : (meta_item | UNSUFFIXED_LIT) (',' meta_item_inner)? ;\n     pub fn parse_meta_item(&mut self) -> PResult<'a, ast::MetaItem> {\n         let nt_meta = match self.token {\n-            token::Interpolated(ref nt) => match **nt {\n+            token::Interpolated(ref nt) => match nt.0 {\n                 token::NtMeta(ref e) => Some(e.clone()),\n                 _ => None,\n             },"}, {"sha": "09cdf26bf1fff4390a6bd1508051efacf86b01c6", "filename": "src/libsyntax/parse/lexer/mod.rs", "status": "modified", "additions": 23, "deletions": 21, "changes": 44, "blob_url": "https://github.com/rust-lang/rust/blob/4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e/src%2Flibsyntax%2Fparse%2Flexer%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e/src%2Flibsyntax%2Fparse%2Flexer%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Flexer%2Fmod.rs?ref=4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e", "patch": "@@ -66,14 +66,15 @@ pub struct StringReader<'a> {\n     token: token::Token,\n     span: Span,\n     open_braces: Vec<(token::DelimToken, Span)>,\n-}\n-\n-fn mk_sp(lo: BytePos, hi: BytePos) -> Span {\n-    Span { lo: lo, hi: hi, ctxt: NO_EXPANSION }\n+    pub override_span: Option<Span>,\n }\n \n impl<'a> StringReader<'a> {\n-    fn next_token(&mut self) -> TokenAndSpan {\n+    fn mk_sp(&self, lo: BytePos, hi: BytePos) -> Span {\n+        unwrap_or!(self.override_span, Span { lo: lo, hi: hi, ctxt: NO_EXPANSION})\n+    }\n+\n+    fn next_token(&mut self) -> TokenAndSpan where Self: Sized {\n         let res = self.try_next_token();\n         self.unwrap_or_abort(res)\n     }\n@@ -175,6 +176,7 @@ impl<'a> StringReader<'a> {\n             token: token::Eof,\n             span: syntax_pos::DUMMY_SP,\n             open_braces: Vec::new(),\n+            override_span: None,\n         }\n     }\n \n@@ -229,12 +231,12 @@ impl<'a> StringReader<'a> {\n \n     /// Report a fatal error spanning [`from_pos`, `to_pos`).\n     fn fatal_span_(&self, from_pos: BytePos, to_pos: BytePos, m: &str) -> FatalError {\n-        self.fatal_span(mk_sp(from_pos, to_pos), m)\n+        self.fatal_span(self.mk_sp(from_pos, to_pos), m)\n     }\n \n     /// Report a lexical error spanning [`from_pos`, `to_pos`).\n     fn err_span_(&self, from_pos: BytePos, to_pos: BytePos, m: &str) {\n-        self.err_span(mk_sp(from_pos, to_pos), m)\n+        self.err_span(self.mk_sp(from_pos, to_pos), m)\n     }\n \n     /// Report a lexical error spanning [`from_pos`, `to_pos`), appending an\n@@ -258,7 +260,7 @@ impl<'a> StringReader<'a> {\n         for c in c.escape_default() {\n             m.push(c)\n         }\n-        self.sess.span_diagnostic.struct_span_fatal(mk_sp(from_pos, to_pos), &m[..])\n+        self.sess.span_diagnostic.struct_span_fatal(self.mk_sp(from_pos, to_pos), &m[..])\n     }\n \n     /// Report a lexical error spanning [`from_pos`, `to_pos`), appending an\n@@ -282,7 +284,7 @@ impl<'a> StringReader<'a> {\n         for c in c.escape_default() {\n             m.push(c)\n         }\n-        self.sess.span_diagnostic.struct_span_err(mk_sp(from_pos, to_pos), &m[..])\n+        self.sess.span_diagnostic.struct_span_err(self.mk_sp(from_pos, to_pos), &m[..])\n     }\n \n     /// Report a lexical error spanning [`from_pos`, `to_pos`), appending the\n@@ -306,11 +308,11 @@ impl<'a> StringReader<'a> {\n             None => {\n                 if self.is_eof() {\n                     self.peek_tok = token::Eof;\n-                    self.peek_span = mk_sp(self.filemap.end_pos, self.filemap.end_pos);\n+                    self.peek_span = self.mk_sp(self.filemap.end_pos, self.filemap.end_pos);\n                 } else {\n                     let start_bytepos = self.pos;\n                     self.peek_tok = self.next_token_inner()?;\n-                    self.peek_span = mk_sp(start_bytepos, self.pos);\n+                    self.peek_span = self.mk_sp(start_bytepos, self.pos);\n                 };\n             }\n         }\n@@ -481,7 +483,7 @@ impl<'a> StringReader<'a> {\n         self.with_str_from(start, |string| {\n             if string == \"_\" {\n                 self.sess.span_diagnostic\n-                    .struct_span_warn(mk_sp(start, self.pos),\n+                    .struct_span_warn(self.mk_sp(start, self.pos),\n                                       \"underscore literal suffix is not allowed\")\n                     .warn(\"this was previously accepted by the compiler but is \\\n                           being phased out; it will become a hard error in \\\n@@ -502,7 +504,7 @@ impl<'a> StringReader<'a> {\n         if let Some(c) = self.ch {\n             if c.is_whitespace() {\n                 let msg = \"called consume_any_line_comment, but there was whitespace\";\n-                self.sess.span_diagnostic.span_err(mk_sp(self.pos, self.pos), msg);\n+                self.sess.span_diagnostic.span_err(self.mk_sp(self.pos, self.pos), msg);\n             }\n         }\n \n@@ -545,13 +547,13 @@ impl<'a> StringReader<'a> {\n \n                             Some(TokenAndSpan {\n                                 tok: tok,\n-                                sp: mk_sp(start_bpos, self.pos),\n+                                sp: self.mk_sp(start_bpos, self.pos),\n                             })\n                         })\n                     } else {\n                         Some(TokenAndSpan {\n                             tok: token::Comment,\n-                            sp: mk_sp(start_bpos, self.pos),\n+                            sp: self.mk_sp(start_bpos, self.pos),\n                         })\n                     }\n                 }\n@@ -584,7 +586,7 @@ impl<'a> StringReader<'a> {\n                     }\n                     return Some(TokenAndSpan {\n                         tok: token::Shebang(self.name_from(start)),\n-                        sp: mk_sp(start, self.pos),\n+                        sp: self.mk_sp(start, self.pos),\n                     });\n                 }\n             }\n@@ -612,7 +614,7 @@ impl<'a> StringReader<'a> {\n                 }\n                 let c = Some(TokenAndSpan {\n                     tok: token::Whitespace,\n-                    sp: mk_sp(start_bpos, self.pos),\n+                    sp: self.mk_sp(start_bpos, self.pos),\n                 });\n                 debug!(\"scanning whitespace: {:?}\", c);\n                 c\n@@ -674,7 +676,7 @@ impl<'a> StringReader<'a> {\n \n             Some(TokenAndSpan {\n                 tok: tok,\n-                sp: mk_sp(start_bpos, self.pos),\n+                sp: self.mk_sp(start_bpos, self.pos),\n             })\n         })\n     }\n@@ -869,7 +871,7 @@ impl<'a> StringReader<'a> {\n                                 let valid = if self.ch_is('{') {\n                                     self.scan_unicode_escape(delim) && !ascii_only\n                                 } else {\n-                                    let span = mk_sp(start, self.pos);\n+                                    let span = self.mk_sp(start, self.pos);\n                                     self.sess.span_diagnostic\n                                         .struct_span_err(span, \"incorrect unicode escape sequence\")\n                                         .span_help(span,\n@@ -907,13 +909,13 @@ impl<'a> StringReader<'a> {\n                                                                         },\n                                                                         c);\n                                 if e == '\\r' {\n-                                    err.span_help(mk_sp(escaped_pos, pos),\n+                                    err.span_help(self.mk_sp(escaped_pos, pos),\n                                                   \"this is an isolated carriage return; consider \\\n                                                    checking your editor and version control \\\n                                                    settings\");\n                                 }\n                                 if (e == '{' || e == '}') && !ascii_only {\n-                                    err.span_help(mk_sp(escaped_pos, pos),\n+                                    err.span_help(self.mk_sp(escaped_pos, pos),\n                                                   \"if used in a formatting string, curly braces \\\n                                                    are escaped with `{{` and `}}`\");\n                                 }"}, {"sha": "63a396c14db858607ea1a17ba61564b894bff080", "filename": "src/libsyntax/parse/lexer/tokentrees.rs", "status": "modified", "additions": 8, "deletions": 4, "changes": 12, "blob_url": "https://github.com/rust-lang/rust/blob/4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e/src%2Flibsyntax%2Fparse%2Flexer%2Ftokentrees.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e/src%2Flibsyntax%2Fparse%2Flexer%2Ftokentrees.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Flexer%2Ftokentrees.rs?ref=4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e", "patch": "@@ -19,7 +19,9 @@ impl<'a> StringReader<'a> {\n     pub fn parse_all_token_trees(&mut self) -> PResult<'a, TokenStream> {\n         let mut tts = Vec::new();\n         while self.token != token::Eof {\n-            tts.push(self.parse_token_tree()?.into());\n+            let tree = self.parse_token_tree()?;\n+            let is_joint = tree.span().hi == self.span.lo && token::is_op(&self.token);\n+            tts.push(if is_joint { tree.joint() } else { tree.into() });\n         }\n         Ok(TokenStream::concat(tts))\n     }\n@@ -31,13 +33,15 @@ impl<'a> StringReader<'a> {\n             if let token::CloseDelim(..) = self.token {\n                 return TokenStream::concat(tts);\n             }\n-            match self.parse_token_tree() {\n-                Ok(tt) => tts.push(tt.into()),\n+            let tree = match self.parse_token_tree() {\n+                Ok(tree) => tree,\n                 Err(mut e) => {\n                     e.emit();\n                     return TokenStream::concat(tts);\n                 }\n-            }\n+            };\n+            let is_joint = tree.span().hi == self.span.lo && token::is_op(&self.token);\n+            tts.push(if is_joint { tree.joint() } else { tree.into() });\n         }\n     }\n "}, {"sha": "bd9a621c00c00a692f639c29171fb131b4ba1d68", "filename": "src/libsyntax/parse/mod.rs", "status": "modified", "additions": 9, "deletions": 6, "changes": 15, "blob_url": "https://github.com/rust-lang/rust/blob/4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e/src%2Flibsyntax%2Fparse%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e/src%2Flibsyntax%2Fparse%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Fmod.rs?ref=4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e", "patch": "@@ -141,9 +141,10 @@ pub fn parse_stmt_from_source_str(name: String, source: String, sess: &ParseSess\n     new_parser_from_source_str(sess, name, source).parse_stmt()\n }\n \n-pub fn parse_stream_from_source_str(name: String, source: String, sess: &ParseSess)\n-                                        -> TokenStream {\n-    filemap_to_stream(sess, sess.codemap().new_filemap(name, source))\n+pub fn parse_stream_from_source_str(name: String, source: String, sess: &ParseSess,\n+                                    override_span: Option<Span>)\n+                                    -> TokenStream {\n+    filemap_to_stream(sess, sess.codemap().new_filemap(name, source), override_span)\n }\n \n // Create a new parser from a source string\n@@ -177,7 +178,7 @@ pub fn new_sub_parser_from_file<'a>(sess: &'a ParseSess,\n /// Given a filemap and config, return a parser\n pub fn filemap_to_parser(sess: & ParseSess, filemap: Rc<FileMap>, ) -> Parser {\n     let end_pos = filemap.end_pos;\n-    let mut parser = stream_to_parser(sess, filemap_to_stream(sess, filemap));\n+    let mut parser = stream_to_parser(sess, filemap_to_stream(sess, filemap, None));\n \n     if parser.token == token::Eof && parser.span == syntax_pos::DUMMY_SP {\n         parser.span = Span { lo: end_pos, hi: end_pos, ctxt: NO_EXPANSION };\n@@ -212,8 +213,10 @@ fn file_to_filemap(sess: &ParseSess, path: &Path, spanopt: Option<Span>)\n }\n \n /// Given a filemap, produce a sequence of token-trees\n-pub fn filemap_to_stream(sess: &ParseSess, filemap: Rc<FileMap>) -> TokenStream {\n+pub fn filemap_to_stream(sess: &ParseSess, filemap: Rc<FileMap>, override_span: Option<Span>)\n+                         -> TokenStream {\n     let mut srdr = lexer::StringReader::new(sess, filemap);\n+    srdr.override_span = override_span;\n     srdr.real_token();\n     panictry!(srdr.parse_all_token_trees())\n }\n@@ -684,7 +687,7 @@ mod tests {\n                     id: ast::DUMMY_NODE_ID,\n                     node: ast::ExprKind::Path(None, ast::Path {\n                         span: sp(0, 6),\n-                        segments: vec![ast::PathSegment::crate_root(),\n+                        segments: vec![ast::PathSegment::crate_root(sp(0, 2)),\n                                        str2seg(\"a\", 2, 3),\n                                        str2seg(\"b\", 5, 6)]\n                     }),"}, {"sha": "c248e20b608fc4ca322ae71e29bd2ded053e889c", "filename": "src/libsyntax/parse/parser.rs", "status": "modified", "additions": 11, "deletions": 8, "changes": 19, "blob_url": "https://github.com/rust-lang/rust/blob/4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e/src%2Flibsyntax%2Fparse%2Fparser.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e/src%2Flibsyntax%2Fparse%2Fparser.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Fparser.rs?ref=4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e", "patch": "@@ -107,7 +107,7 @@ pub enum BlockMode {\n macro_rules! maybe_whole_expr {\n     ($p:expr) => {\n         if let token::Interpolated(nt) = $p.token.clone() {\n-            match *nt {\n+            match nt.0 {\n                 token::NtExpr(ref e) => {\n                     $p.bump();\n                     return Ok((*e).clone());\n@@ -134,7 +134,7 @@ macro_rules! maybe_whole_expr {\n macro_rules! maybe_whole {\n     ($p:expr, $constructor:ident, |$x:ident| $e:expr) => {\n         if let token::Interpolated(nt) = $p.token.clone() {\n-            if let token::$constructor($x) = (*nt).clone() {\n+            if let token::$constructor($x) = nt.0.clone() {\n                 $p.bump();\n                 return Ok($e);\n             }\n@@ -1602,7 +1602,7 @@ impl<'a> Parser<'a> {\n     /// Matches token_lit = LIT_INTEGER | ...\n     pub fn parse_lit_token(&mut self) -> PResult<'a, LitKind> {\n         let out = match self.token {\n-            token::Interpolated(ref nt) => match **nt {\n+            token::Interpolated(ref nt) => match nt.0 {\n                 token::NtExpr(ref v) => match v.node {\n                     ExprKind::Lit(ref lit) => { lit.node.clone() }\n                     _ => { return self.unexpected_last(&self.token); }\n@@ -1761,7 +1761,7 @@ impl<'a> Parser<'a> {\n         };\n \n         if is_global {\n-            segments.insert(0, PathSegment::crate_root());\n+            segments.insert(0, PathSegment::crate_root(lo));\n         }\n \n         // Assemble the result.\n@@ -1775,7 +1775,7 @@ impl<'a> Parser<'a> {\n     /// This is used when parsing derive macro paths in `#[derive]` attributes.\n     pub fn parse_path_allowing_meta(&mut self, mode: PathStyle) -> PResult<'a, ast::Path> {\n         let meta_ident = match self.token {\n-            token::Interpolated(ref nt) => match **nt {\n+            token::Interpolated(ref nt) => match nt.0 {\n                 token::NtMeta(ref meta) => match meta.node {\n                     ast::MetaItemKind::Word => Some(ast::Ident::with_empty_ctxt(meta.name)),\n                     _ => None,\n@@ -2610,13 +2610,16 @@ impl<'a> Parser<'a> {\n \n     pub fn process_potential_macro_variable(&mut self) {\n         let ident = match self.token {\n-            token::SubstNt(name) => {\n+            token::Dollar if self.span.ctxt != syntax_pos::hygiene::SyntaxContext::empty() &&\n+                             self.look_ahead(1, |t| t.is_ident()) => {\n+                self.bump();\n+                let name = match self.token { token::Ident(ident) => ident, _ => unreachable!() };\n                 self.fatal(&format!(\"unknown macro variable `{}`\", name)).emit();\n                 return\n             }\n             token::Interpolated(ref nt) => {\n                 self.meta_var_span = Some(self.span);\n-                match **nt {\n+                match nt.0 {\n                     token::NtIdent(ident) => ident,\n                     _ => return,\n                 }\n@@ -6168,7 +6171,7 @@ impl<'a> Parser<'a> {\n             // `{foo, bar}`, `::{foo, bar}`, `*`, or `::*`.\n             self.eat(&token::ModSep);\n             let prefix = ast::Path {\n-                segments: vec![PathSegment::crate_root()],\n+                segments: vec![PathSegment::crate_root(lo)],\n                 span: lo.to(self.span),\n             };\n             let view_path_kind = if self.eat(&token::BinOp(token::Star)) {"}, {"sha": "834ac38af9870473db68bf4f2a3df298d655851e", "filename": "src/libsyntax/parse/token.rs", "status": "modified", "additions": 132, "deletions": 11, "changes": 143, "blob_url": "https://github.com/rust-lang/rust/blob/4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e/src%2Flibsyntax%2Fparse%2Ftoken.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e/src%2Flibsyntax%2Fparse%2Ftoken.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Ftoken.rs?ref=4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e", "patch": "@@ -16,10 +16,12 @@ pub use self::Token::*;\n \n use ast::{self};\n use ptr::P;\n+use serialize::{Decodable, Decoder, Encodable, Encoder};\n use symbol::keywords;\n-use tokenstream::TokenTree;\n+use tokenstream::{TokenStream, TokenTree};\n \n-use std::fmt;\n+use std::cell::Cell;\n+use std::{cmp, fmt};\n use std::rc::Rc;\n \n #[derive(Clone, RustcEncodable, RustcDecodable, PartialEq, Eq, Hash, Debug, Copy)]\n@@ -167,14 +169,12 @@ pub enum Token {\n     Underscore,\n     Lifetime(ast::Ident),\n \n-    /* For interpolation */\n-    Interpolated(Rc<Nonterminal>),\n+    // The `LazyTokenStream` is a pure function of the `Nonterminal`,\n+    // and so the `LazyTokenStream` can be ignored by Eq, Hash, etc.\n+    Interpolated(Rc<(Nonterminal, LazyTokenStream)>),\n     // Can be expanded into several tokens.\n     /// Doc comment\n     DocComment(ast::Name),\n-    // In right-hand-sides of MBE macros:\n-    /// A syntactic variable that will be filled in by macro expansion.\n-    SubstNt(ast::Ident),\n \n     // Junk. These carry no data because we don't really care about the data\n     // they *would* carry, and don't really want to allocate a new ident for\n@@ -190,6 +190,10 @@ pub enum Token {\n }\n \n impl Token {\n+    pub fn interpolated(nt: Nonterminal) -> Token {\n+        Token::Interpolated(Rc::new((nt, LazyTokenStream::new())))\n+    }\n+\n     /// Returns `true` if the token starts with '>'.\n     pub fn is_like_gt(&self) -> bool {\n         match *self {\n@@ -214,7 +218,7 @@ impl Token {\n             Lt | BinOp(Shl)             | // associated path\n             ModSep                      | // global path\n             Pound                       => true, // expression attributes\n-            Interpolated(ref nt) => match **nt {\n+            Interpolated(ref nt) => match nt.0 {\n                 NtIdent(..) | NtExpr(..) | NtBlock(..) | NtPath(..) => true,\n                 _ => false,\n             },\n@@ -237,7 +241,7 @@ impl Token {\n             Lifetime(..)                | // lifetime bound in trait object\n             Lt | BinOp(Shl)             | // associated path\n             ModSep                      => true, // global path\n-            Interpolated(ref nt) => match **nt {\n+            Interpolated(ref nt) => match nt.0 {\n                 NtIdent(..) | NtTy(..) | NtPath(..) => true,\n                 _ => false,\n             },\n@@ -256,7 +260,7 @@ impl Token {\n     pub fn ident(&self) -> Option<ast::Ident> {\n         match *self {\n             Ident(ident) => Some(ident),\n-            Interpolated(ref nt) => match **nt {\n+            Interpolated(ref nt) => match nt.0 {\n                 NtIdent(ident) => Some(ident.node),\n                 _ => None,\n             },\n@@ -288,7 +292,7 @@ impl Token {\n     /// Returns `true` if the token is an interpolated path.\n     pub fn is_path(&self) -> bool {\n         if let Interpolated(ref nt) = *self {\n-            if let NtPath(..) = **nt {\n+            if let NtPath(..) = nt.0 {\n                 return true;\n             }\n         }\n@@ -358,6 +362,60 @@ impl Token {\n         }\n     }\n \n+    pub fn glue(self, joint: Token) -> Option<Token> {\n+        Some(match self {\n+            Eq => match joint {\n+                Eq => EqEq,\n+                Gt => FatArrow,\n+                _ => return None,\n+            },\n+            Lt => match joint {\n+                Eq => Le,\n+                Lt => BinOp(Shl),\n+                Le => BinOpEq(Shl),\n+                BinOp(Minus) => LArrow,\n+                _ => return None,\n+            },\n+            Gt => match joint {\n+                Eq => Ge,\n+                Gt => BinOp(Shr),\n+                Ge => BinOpEq(Shr),\n+                _ => return None,\n+            },\n+            Not => match joint {\n+                Eq => Ne,\n+                _ => return None,\n+            },\n+            BinOp(op) => match joint {\n+                Eq => BinOpEq(op),\n+                BinOp(And) if op == And => AndAnd,\n+                BinOp(Or) if op == Or => OrOr,\n+                Gt if op == Minus => RArrow,\n+                _ => return None,\n+            },\n+            Dot => match joint {\n+                Dot => DotDot,\n+                DotDot => DotDotDot,\n+                _ => return None,\n+            },\n+            DotDot => match joint {\n+                Dot => DotDotDot,\n+                _ => return None,\n+            },\n+            Colon => match joint {\n+                Colon => ModSep,\n+                _ => return None,\n+            },\n+\n+            Le | EqEq | Ne | Ge | AndAnd | OrOr | Tilde | BinOpEq(..) | At | DotDotDot | Comma |\n+            Semi | ModSep | RArrow | LArrow | FatArrow | Pound | Dollar | Question |\n+            OpenDelim(..) | CloseDelim(..) | Underscore => return None,\n+\n+            Literal(..) | Ident(..) | Lifetime(..) | Interpolated(..) | DocComment(..) |\n+            Whitespace | Comment | Shebang(..) | Eof => return None,\n+        })\n+    }\n+\n     /// Returns `true` if the token is either a special identifier or a keyword.\n     pub fn is_reserved_ident(&self) -> bool {\n         self.is_special_ident() || self.is_used_keyword() || self.is_unused_keyword()\n@@ -411,3 +469,66 @@ impl fmt::Debug for Nonterminal {\n         }\n     }\n }\n+\n+pub fn is_op(tok: &Token) -> bool {\n+    match *tok {\n+        OpenDelim(..) | CloseDelim(..) | Literal(..) | DocComment(..) |\n+        Ident(..) | Underscore | Lifetime(..) | Interpolated(..) |\n+        Whitespace | Comment | Shebang(..) | Eof => false,\n+        _ => true,\n+    }\n+}\n+\n+pub struct LazyTokenStream(Cell<Option<TokenStream>>);\n+\n+impl Clone for LazyTokenStream {\n+    fn clone(&self) -> Self {\n+        let opt_stream = self.0.take();\n+        self.0.set(opt_stream.clone());\n+        LazyTokenStream(Cell::new(opt_stream))\n+    }\n+}\n+\n+impl cmp::Eq for LazyTokenStream {}\n+impl PartialEq for LazyTokenStream {\n+    fn eq(&self, _other: &LazyTokenStream) -> bool {\n+        true\n+    }\n+}\n+\n+impl fmt::Debug for LazyTokenStream {\n+    fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {\n+        fmt::Debug::fmt(&self.clone().0.into_inner(), f)\n+    }\n+}\n+\n+impl LazyTokenStream {\n+    pub fn new() -> Self {\n+        LazyTokenStream(Cell::new(None))\n+    }\n+\n+    pub fn force<F: FnOnce() -> TokenStream>(&self, f: F) -> TokenStream {\n+        let mut opt_stream = self.0.take();\n+        if opt_stream.is_none() {\n+            opt_stream = Some(f());\n+        }\n+        self.0.set(opt_stream.clone());\n+        opt_stream.clone().unwrap()\n+    }\n+}\n+\n+impl Encodable for LazyTokenStream {\n+    fn encode<S: Encoder>(&self, _: &mut S) -> Result<(), S::Error> {\n+        Ok(())\n+    }\n+}\n+\n+impl Decodable for LazyTokenStream {\n+    fn decode<D: Decoder>(_: &mut D) -> Result<LazyTokenStream, D::Error> {\n+        Ok(LazyTokenStream::new())\n+    }\n+}\n+\n+impl ::std::hash::Hash for LazyTokenStream {\n+    fn hash<H: ::std::hash::Hasher>(&self, _hasher: &mut H) {}\n+}"}, {"sha": "d449e412d6cc304fbb9d5c30cbc3da36d60c3661", "filename": "src/libsyntax/print/pprust.rs", "status": "modified", "additions": 1, "deletions": 2, "changes": 3, "blob_url": "https://github.com/rust-lang/rust/blob/4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e/src%2Flibsyntax%2Fprint%2Fpprust.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e/src%2Flibsyntax%2Fprint%2Fpprust.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fprint%2Fpprust.rs?ref=4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e", "patch": "@@ -270,13 +270,12 @@ pub fn token_to_string(tok: &Token) -> String {\n \n         /* Other */\n         token::DocComment(s)        => s.to_string(),\n-        token::SubstNt(s)           => format!(\"${}\", s),\n         token::Eof                  => \"<eof>\".to_string(),\n         token::Whitespace           => \" \".to_string(),\n         token::Comment              => \"/* */\".to_string(),\n         token::Shebang(s)           => format!(\"/* shebang: {}*/\", s),\n \n-        token::Interpolated(ref nt) => match **nt {\n+        token::Interpolated(ref nt) => match nt.0 {\n             token::NtExpr(ref e)        => expr_to_string(e),\n             token::NtMeta(ref e)        => meta_item_to_string(e),\n             token::NtTy(ref e)          => ty_to_string(e),"}, {"sha": "8eee25405df6bca679924f97ca76acd4e41ad9a2", "filename": "src/libsyntax/tokenstream.rs", "status": "modified", "additions": 184, "deletions": 41, "changes": 225, "blob_url": "https://github.com/rust-lang/rust/blob/4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e/src%2Flibsyntax%2Ftokenstream.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e/src%2Flibsyntax%2Ftokenstream.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Ftokenstream.rs?ref=4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e", "patch": "@@ -138,6 +138,10 @@ impl TokenTree {\n             _ => false,\n         }\n     }\n+\n+    pub fn joint(self) -> TokenStream {\n+        TokenStream { kind: TokenStreamKind::JointTree(self) }\n+    }\n }\n \n /// # Token Streams\n@@ -155,6 +159,7 @@ pub struct TokenStream {\n enum TokenStreamKind {\n     Empty,\n     Tree(TokenTree),\n+    JointTree(TokenTree),\n     Stream(RcSlice<TokenStream>),\n }\n \n@@ -199,7 +204,7 @@ impl TokenStream {\n     pub fn concat(mut streams: Vec<TokenStream>) -> TokenStream {\n         match streams.len() {\n             0 => TokenStream::empty(),\n-            1 => TokenStream::from(streams.pop().unwrap()),\n+            1 => streams.pop().unwrap(),\n             _ => TokenStream::concat_rc_slice(RcSlice::new(streams)),\n         }\n     }\n@@ -225,6 +230,105 @@ impl TokenStream {\n         }\n         true\n     }\n+\n+    /// Precondition: `self` consists of a single token tree.\n+    /// Returns true if the token tree is a joint operation w.r.t. `proc_macro::TokenNode`.\n+    pub fn as_tree(self) -> (TokenTree, bool /* joint? */) {\n+        match self.kind {\n+            TokenStreamKind::Tree(tree) => (tree, false),\n+            TokenStreamKind::JointTree(tree) => (tree, true),\n+            _ => unreachable!(),\n+        }\n+    }\n+\n+    pub fn map<F: FnMut(TokenTree) -> TokenTree>(self, mut f: F) -> TokenStream {\n+        let mut trees = self.into_trees();\n+        let mut result = Vec::new();\n+        while let Some(stream) = trees.next_as_stream() {\n+            result.push(match stream.kind {\n+                TokenStreamKind::Tree(tree) => f(tree).into(),\n+                TokenStreamKind::JointTree(tree) => f(tree).joint(),\n+                _ => unreachable!()\n+            });\n+        }\n+        TokenStream::concat(result)\n+    }\n+\n+    fn first_tree(&self) -> Option<TokenTree> {\n+        match self.kind {\n+            TokenStreamKind::Empty => None,\n+            TokenStreamKind::Tree(ref tree) |\n+            TokenStreamKind::JointTree(ref tree) => Some(tree.clone()),\n+            TokenStreamKind::Stream(ref stream) => stream.first().unwrap().first_tree(),\n+        }\n+    }\n+\n+    fn last_tree_if_joint(&self) -> Option<TokenTree> {\n+        match self.kind {\n+            TokenStreamKind::Empty | TokenStreamKind::Tree(..) => None,\n+            TokenStreamKind::JointTree(ref tree) => Some(tree.clone()),\n+            TokenStreamKind::Stream(ref stream) => stream.last().unwrap().last_tree_if_joint(),\n+        }\n+    }\n+}\n+\n+pub struct TokenStreamBuilder(Vec<TokenStream>);\n+\n+impl TokenStreamBuilder {\n+    pub fn new() -> TokenStreamBuilder {\n+        TokenStreamBuilder(Vec::new())\n+    }\n+\n+    pub fn push<T: Into<TokenStream>>(&mut self, stream: T) {\n+        let stream = stream.into();\n+        let last_tree_if_joint = self.0.last().and_then(TokenStream::last_tree_if_joint);\n+        if let Some(TokenTree::Token(last_span, last_tok)) = last_tree_if_joint {\n+            if let Some(TokenTree::Token(span, tok)) = stream.first_tree() {\n+                if let Some(glued_tok) = last_tok.glue(tok) {\n+                    let last_stream = self.0.pop().unwrap();\n+                    self.push_all_but_last_tree(&last_stream);\n+                    let glued_span = last_span.to(span);\n+                    self.0.push(TokenTree::Token(glued_span, glued_tok).into());\n+                    self.push_all_but_first_tree(&stream);\n+                    return\n+                }\n+            }\n+        }\n+        self.0.push(stream);\n+    }\n+\n+    pub fn add<T: Into<TokenStream>>(mut self, stream: T) -> Self {\n+        self.push(stream);\n+        self\n+    }\n+\n+    pub fn build(self) -> TokenStream {\n+        TokenStream::concat(self.0)\n+    }\n+\n+    fn push_all_but_last_tree(&mut self, stream: &TokenStream) {\n+        if let TokenStreamKind::Stream(ref streams) = stream.kind {\n+            let len = streams.len();\n+            match len {\n+                1 => {}\n+                2 => self.0.push(streams[0].clone().into()),\n+                _ => self.0.push(TokenStream::concat_rc_slice(streams.sub_slice(0 .. len - 1))),\n+            }\n+            self.push_all_but_last_tree(&streams[len - 1])\n+        }\n+    }\n+\n+    fn push_all_but_first_tree(&mut self, stream: &TokenStream) {\n+        if let TokenStreamKind::Stream(ref streams) = stream.kind {\n+            let len = streams.len();\n+            match len {\n+                1 => {}\n+                2 => self.0.push(streams[1].clone().into()),\n+                _ => self.0.push(TokenStream::concat_rc_slice(streams.sub_slice(1 .. len))),\n+            }\n+            self.push_all_but_first_tree(&streams[0])\n+        }\n+    }\n }\n \n #[derive(Clone)]\n@@ -234,6 +338,7 @@ pub struct Cursor(CursorKind);\n enum CursorKind {\n     Empty,\n     Tree(TokenTree, bool /* consumed? */),\n+    JointTree(TokenTree, bool /* consumed? */),\n     Stream(StreamCursor),\n }\n \n@@ -244,61 +349,95 @@ struct StreamCursor {\n     stack: Vec<(RcSlice<TokenStream>, usize)>,\n }\n \n-impl Iterator for Cursor {\n-    type Item = TokenTree;\n-\n-    fn next(&mut self) -> Option<TokenTree> {\n-        let cursor = match self.0 {\n-            CursorKind::Stream(ref mut cursor) => cursor,\n-            CursorKind::Tree(ref tree, ref mut consumed @ false) => {\n-                *consumed = true;\n-                return Some(tree.clone());\n-            }\n-            _ => return None,\n-        };\n+impl StreamCursor {\n+    fn new(stream: RcSlice<TokenStream>) -> Self {\n+        StreamCursor { stream: stream, index: 0, stack: Vec::new() }\n+    }\n \n+    fn next_as_stream(&mut self) -> Option<TokenStream> {\n         loop {\n-            if cursor.index < cursor.stream.len() {\n-                match cursor.stream[cursor.index].kind.clone() {\n-                    TokenStreamKind::Tree(tree) => {\n-                        cursor.index += 1;\n-                        return Some(tree);\n-                    }\n-                    TokenStreamKind::Stream(stream) => {\n-                        cursor.stack.push((mem::replace(&mut cursor.stream, stream),\n-                                           mem::replace(&mut cursor.index, 0) + 1));\n-                    }\n-                    TokenStreamKind::Empty => {\n-                        cursor.index += 1;\n-                    }\n+            if self.index < self.stream.len() {\n+                self.index += 1;\n+                let next = self.stream[self.index - 1].clone();\n+                match next.kind {\n+                    TokenStreamKind::Tree(..) | TokenStreamKind::JointTree(..) => return Some(next),\n+                    TokenStreamKind::Stream(stream) => self.insert(stream),\n+                    TokenStreamKind::Empty => {}\n                 }\n-            } else if let Some((stream, index)) = cursor.stack.pop() {\n-                cursor.stream = stream;\n-                cursor.index = index;\n+            } else if let Some((stream, index)) = self.stack.pop() {\n+                self.stream = stream;\n+                self.index = index;\n             } else {\n                 return None;\n             }\n         }\n     }\n+\n+    fn insert(&mut self, stream: RcSlice<TokenStream>) {\n+        self.stack.push((mem::replace(&mut self.stream, stream),\n+                         mem::replace(&mut self.index, 0)));\n+    }\n+}\n+\n+impl Iterator for Cursor {\n+    type Item = TokenTree;\n+\n+    fn next(&mut self) -> Option<TokenTree> {\n+        self.next_as_stream().map(|stream| match stream.kind {\n+            TokenStreamKind::Tree(tree) | TokenStreamKind::JointTree(tree) => tree,\n+            _ => unreachable!()\n+        })\n+    }\n }\n \n impl Cursor {\n     fn new(stream: TokenStream) -> Self {\n         Cursor(match stream.kind {\n             TokenStreamKind::Empty => CursorKind::Empty,\n             TokenStreamKind::Tree(tree) => CursorKind::Tree(tree, false),\n-            TokenStreamKind::Stream(stream) => {\n-                CursorKind::Stream(StreamCursor { stream: stream, index: 0, stack: Vec::new() })\n-            }\n+            TokenStreamKind::JointTree(tree) => CursorKind::JointTree(tree, false),\n+            TokenStreamKind::Stream(stream) => CursorKind::Stream(StreamCursor::new(stream)),\n         })\n     }\n \n-    pub fn original_stream(self) -> TokenStream {\n+    pub fn next_as_stream(&mut self) -> Option<TokenStream> {\n+        let (stream, consumed) = match self.0 {\n+            CursorKind::Tree(ref tree, ref mut consumed @ false) =>\n+                (tree.clone().into(), consumed),\n+            CursorKind::JointTree(ref tree, ref mut consumed @ false) =>\n+                (tree.clone().joint(), consumed),\n+            CursorKind::Stream(ref mut cursor) => return cursor.next_as_stream(),\n+            _ => return None,\n+        };\n+\n+        *consumed = true;\n+        Some(stream)\n+    }\n+\n+    pub fn insert(&mut self, stream: TokenStream) {\n+        match self.0 {\n+            _ if stream.is_empty() => return,\n+            CursorKind::Empty => *self = stream.trees(),\n+            CursorKind::Tree(_, consumed) | CursorKind::JointTree(_, consumed) => {\n+                *self = TokenStream::concat(vec![self.original_stream(), stream]).trees();\n+                if consumed {\n+                    self.next();\n+                }\n+            }\n+            CursorKind::Stream(ref mut cursor) => {\n+                cursor.insert(ThinTokenStream::from(stream).0.unwrap());\n+            }\n+        }\n+    }\n+\n+    pub fn original_stream(&self) -> TokenStream {\n         match self.0 {\n             CursorKind::Empty => TokenStream::empty(),\n-            CursorKind::Tree(tree, _) => tree.into(),\n-            CursorKind::Stream(cursor) => TokenStream::concat_rc_slice({\n-                cursor.stack.get(0).cloned().map(|(stream, _)| stream).unwrap_or(cursor.stream)\n+            CursorKind::Tree(ref tree, _) => tree.clone().into(),\n+            CursorKind::JointTree(ref tree, _) => tree.clone().joint(),\n+            CursorKind::Stream(ref cursor) => TokenStream::concat_rc_slice({\n+                cursor.stack.get(0).cloned().map(|(stream, _)| stream)\n+                    .unwrap_or(cursor.stream.clone())\n             }),\n         }\n     }\n@@ -307,22 +446,25 @@ impl Cursor {\n         fn look_ahead(streams: &[TokenStream], mut n: usize) -> Result<TokenTree, usize> {\n             for stream in streams {\n                 n = match stream.kind {\n-                    TokenStreamKind::Tree(ref tree) if n == 0 => return Ok(tree.clone()),\n-                    TokenStreamKind::Tree(..) => n - 1,\n+                    TokenStreamKind::Tree(ref tree) | TokenStreamKind::JointTree(ref tree)\n+                        if n == 0 => return Ok(tree.clone()),\n+                    TokenStreamKind::Tree(..) | TokenStreamKind::JointTree(..) => n - 1,\n                     TokenStreamKind::Stream(ref stream) => match look_ahead(stream, n) {\n                         Ok(tree) => return Ok(tree),\n                         Err(n) => n,\n                     },\n                     _ => n,\n                 };\n             }\n-\n             Err(n)\n         }\n \n         match self.0 {\n-            CursorKind::Empty | CursorKind::Tree(_, true) => Err(n),\n-            CursorKind::Tree(ref tree, false) => look_ahead(&[tree.clone().into()], n),\n+            CursorKind::Empty |\n+            CursorKind::Tree(_, true) |\n+            CursorKind::JointTree(_, true) => Err(n),\n+            CursorKind::Tree(ref tree, false) |\n+            CursorKind::JointTree(ref tree, false) => look_ahead(&[tree.clone().into()], n),\n             CursorKind::Stream(ref cursor) => {\n                 look_ahead(&cursor.stream[cursor.index ..], n).or_else(|mut n| {\n                     for &(ref stream, index) in cursor.stack.iter().rev() {\n@@ -350,6 +492,7 @@ impl From<TokenStream> for ThinTokenStream {\n         ThinTokenStream(match stream.kind {\n             TokenStreamKind::Empty => None,\n             TokenStreamKind::Tree(tree) => Some(RcSlice::new(vec![tree.into()])),\n+            TokenStreamKind::JointTree(tree) => Some(RcSlice::new(vec![tree.joint()])),\n             TokenStreamKind::Stream(stream) => Some(stream),\n         })\n     }"}, {"sha": "d993ba14a4ab562bfeafc858801b025088da3cb5", "filename": "src/libsyntax/util/parser_testing.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e/src%2Flibsyntax%2Futil%2Fparser_testing.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e/src%2Flibsyntax%2Futil%2Fparser_testing.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Futil%2Fparser_testing.rs?ref=4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e", "patch": "@@ -20,7 +20,7 @@ use std::iter::Peekable;\n /// Map a string to tts, using a made-up filename:\n pub fn string_to_stream(source_str: String) -> TokenStream {\n     let ps = ParseSess::new(FilePathMapping::empty());\n-    filemap_to_stream(&ps, ps.codemap().new_filemap(\"bogofile\".to_string(), source_str))\n+    filemap_to_stream(&ps, ps.codemap().new_filemap(\"bogofile\".to_string(), source_str), None)\n }\n \n /// Map string to parser (via tts)"}, {"sha": "d6939d71129e4e276b7c94f3a915f44d293db569", "filename": "src/libsyntax/util/rc_slice.rs", "status": "modified", "additions": 9, "deletions": 1, "changes": 10, "blob_url": "https://github.com/rust-lang/rust/blob/4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e/src%2Flibsyntax%2Futil%2Frc_slice.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e/src%2Flibsyntax%2Futil%2Frc_slice.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Futil%2Frc_slice.rs?ref=4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e", "patch": "@@ -9,7 +9,7 @@\n // except according to those terms.\n \n use std::fmt;\n-use std::ops::Deref;\n+use std::ops::{Deref, Range};\n use std::rc::Rc;\n \n use rustc_data_structures::stable_hasher::{StableHasher, StableHasherResult,\n@@ -30,6 +30,14 @@ impl<T> RcSlice<T> {\n             data: Rc::new(vec.into_boxed_slice()),\n         }\n     }\n+\n+    pub fn sub_slice(&self, range: Range<usize>) -> Self {\n+        RcSlice {\n+            data: self.data.clone(),\n+            offset: self.offset + range.start as u32,\n+            len: (range.end - range.start) as u32,\n+        }\n+    }\n }\n \n impl<T> Deref for RcSlice<T> {"}, {"sha": "6f4c112acb6c6430eadb8b34608e2c08bdb5ae8c", "filename": "src/libsyntax_ext/concat_idents.rs", "status": "modified", "additions": 6, "deletions": 1, "changes": 7, "blob_url": "https://github.com/rust-lang/rust/blob/4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e/src%2Flibsyntax_ext%2Fconcat_idents.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e/src%2Flibsyntax_ext%2Fconcat_idents.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax_ext%2Fconcat_idents.rs?ref=4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e", "patch": "@@ -15,6 +15,8 @@ use syntax::feature_gate;\n use syntax::parse::token;\n use syntax::ptr::P;\n use syntax_pos::Span;\n+use syntax_pos::symbol::Symbol;\n+use syntax_pos::hygiene::SyntaxContext;\n use syntax::tokenstream::TokenTree;\n \n pub fn expand_syntax_ext<'cx>(cx: &'cx mut ExtCtxt,\n@@ -50,7 +52,10 @@ pub fn expand_syntax_ext<'cx>(cx: &'cx mut ExtCtxt,\n             }\n         }\n     }\n-    let res = ast::Ident::from_str(&res_str);\n+    let res = ast::Ident {\n+        name: Symbol::intern(&res_str),\n+        ctxt: SyntaxContext::empty().apply_mark(cx.current_expansion.mark),\n+    };\n \n     struct Result {\n         ident: ast::Ident,"}, {"sha": "fa5537b5d8fe3e93f18da7581c992891d8e64f12", "filename": "src/libsyntax_ext/deriving/custom.rs", "status": "modified", "additions": 4, "deletions": 11, "changes": 15, "blob_url": "https://github.com/rust-lang/rust/blob/4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e/src%2Flibsyntax_ext%2Fderiving%2Fcustom.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e/src%2Flibsyntax_ext%2Fderiving%2Fcustom.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax_ext%2Fderiving%2Fcustom.rs?ref=4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e", "patch": "@@ -16,7 +16,6 @@ use syntax::ast::{self, ItemKind, Attribute, Mac};\n use syntax::attr::{mark_used, mark_known};\n use syntax::codemap::Span;\n use syntax::ext::base::*;\n-use syntax::fold::Folder;\n use syntax::visit::Visitor;\n \n struct MarkAttrs<'a>(&'a [ast::Name]);\n@@ -75,7 +74,7 @@ impl MultiItemModifier for ProcMacroDerive {\n         MarkAttrs(&self.attrs).visit_item(&item);\n \n         let input = __internal::new_token_stream(ecx.resolver.eliminate_crate_var(item.clone()));\n-        let res = __internal::set_parse_sess(&ecx.parse_sess, || {\n+        let res = __internal::set_sess(ecx, || {\n             let inner = self.inner;\n             panic::catch_unwind(panic::AssertUnwindSafe(|| inner(input)))\n         });\n@@ -97,22 +96,16 @@ impl MultiItemModifier for ProcMacroDerive {\n             }\n         };\n \n-        let new_items = __internal::set_parse_sess(&ecx.parse_sess, || {\n+        __internal::set_sess(ecx, || {\n             match __internal::token_stream_parse_items(stream) {\n-                Ok(new_items) => new_items,\n+                Ok(new_items) => new_items.into_iter().map(Annotatable::Item).collect(),\n                 Err(_) => {\n                     // FIXME: handle this better\n                     let msg = \"proc-macro derive produced unparseable tokens\";\n                     ecx.struct_span_fatal(span, msg).emit();\n                     panic!(FatalError);\n                 }\n             }\n-        });\n-\n-        // Reassign spans of all expanded items to the input `item`\n-        // for better errors here.\n-        new_items.into_iter().map(|item| {\n-            Annotatable::Item(ChangeSpan { span: span }.fold_item(item).expect_one(\"\"))\n-        }).collect()\n+        })\n     }\n }"}, {"sha": "144d1930df90b54d3b99f77118fe477ec85eb469", "filename": "src/libsyntax_ext/format.rs", "status": "modified", "additions": 8, "deletions": 5, "changes": 13, "blob_url": "https://github.com/rust-lang/rust/blob/4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e/src%2Flibsyntax_ext%2Fformat.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e/src%2Flibsyntax_ext%2Fformat.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax_ext%2Fformat.rs?ref=4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e", "patch": "@@ -20,7 +20,7 @@ use syntax::ext::build::AstBuilder;\n use syntax::parse::token;\n use syntax::ptr::P;\n use syntax::symbol::{Symbol, keywords};\n-use syntax_pos::{Span, DUMMY_SP};\n+use syntax_pos::Span;\n use syntax::tokenstream;\n \n use std::collections::{HashMap, HashSet};\n@@ -558,7 +558,9 @@ impl<'a, 'b> Context<'a, 'b> {\n         // passed to this function.\n         for (i, e) in self.args.into_iter().enumerate() {\n             let name = self.ecx.ident_of(&format!(\"__arg{}\", i));\n-            pats.push(self.ecx.pat_ident(DUMMY_SP, name));\n+            let span =\n+                Span { ctxt: e.span.ctxt.apply_mark(self.ecx.current_expansion.mark), ..e.span };\n+            pats.push(self.ecx.pat_ident(span, name));\n             for ref arg_ty in self.arg_unique_types[i].iter() {\n                 locals.push(Context::format_arg(self.ecx, self.macsp, e.span, arg_ty, name));\n             }\n@@ -672,10 +674,10 @@ impl<'a, 'b> Context<'a, 'b> {\n }\n \n pub fn expand_format_args<'cx>(ecx: &'cx mut ExtCtxt,\n-                               sp: Span,\n+                               mut sp: Span,\n                                tts: &[tokenstream::TokenTree])\n                                -> Box<base::MacResult + 'cx> {\n-\n+    sp.ctxt = sp.ctxt.apply_mark(ecx.current_expansion.mark);\n     match parse_args(ecx, sp, tts) {\n         Some((efmt, args, names)) => {\n             MacEager::expr(expand_preparsed_format_args(ecx, sp, efmt, args, names))\n@@ -696,7 +698,8 @@ pub fn expand_preparsed_format_args(ecx: &mut ExtCtxt,\n     // `ArgumentType` does not derive `Clone`.\n     let arg_types: Vec<_> = (0..args.len()).map(|_| Vec::new()).collect();\n     let arg_unique_types: Vec<_> = (0..args.len()).map(|_| Vec::new()).collect();\n-    let macsp = ecx.call_site();\n+    let mut macsp = ecx.call_site();\n+    macsp.ctxt = macsp.ctxt.apply_mark(ecx.current_expansion.mark);\n     let msg = \"format argument must be a string literal.\";\n     let fmt = match expr_to_spanned_string(ecx, efmt, msg) {\n         Some(fmt) => fmt,"}, {"sha": "5fcedbf50c60f71caeba36d8c8811acab084f204", "filename": "src/libsyntax_ext/proc_macro_impl.rs", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/rust-lang/rust/blob/4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e/src%2Flibsyntax_ext%2Fproc_macro_impl.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e/src%2Flibsyntax_ext%2Fproc_macro_impl.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax_ext%2Fproc_macro_impl.rs?ref=4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e", "patch": "@@ -34,7 +34,7 @@ impl base::AttrProcMacro for AttrProcMacro {\n         let annotation = __internal::token_stream_wrap(annotation);\n         let annotated = __internal::token_stream_wrap(annotated);\n \n-        let res = __internal::set_parse_sess(&ecx.parse_sess, || {\n+        let res = __internal::set_sess(ecx, || {\n             panic::catch_unwind(panic::AssertUnwindSafe(|| (self.inner)(annotation, annotated)))\n         });\n \n@@ -69,7 +69,7 @@ impl base::ProcMacro for BangProcMacro {\n                    -> TokenStream {\n         let input = __internal::token_stream_wrap(input);\n \n-        let res = __internal::set_parse_sess(&ecx.parse_sess, || {\n+        let res = __internal::set_sess(ecx, || {\n             panic::catch_unwind(panic::AssertUnwindSafe(|| (self.inner)(input)))\n         });\n "}, {"sha": "804b91ab09e3c333363305262763886f8de88cbd", "filename": "src/libsyntax_pos/hygiene.rs", "status": "modified", "additions": 9, "deletions": 15, "changes": 24, "blob_url": "https://github.com/rust-lang/rust/blob/4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e/src%2Flibsyntax_pos%2Fhygiene.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e/src%2Flibsyntax_pos%2Fhygiene.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax_pos%2Fhygiene.rs?ref=4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e", "patch": "@@ -144,24 +144,18 @@ impl SyntaxContext {\n     pub fn apply_mark(self, mark: Mark) -> SyntaxContext {\n         HygieneData::with(|data| {\n             let syntax_contexts = &mut data.syntax_contexts;\n-            let ctxt_data = syntax_contexts[self.0 as usize];\n-            if mark == ctxt_data.outer_mark {\n-                return ctxt_data.prev_ctxt;\n-            }\n-\n-            let modern = if data.marks[mark.0 as usize].modern {\n-                *data.markings.entry((ctxt_data.modern, mark)).or_insert_with(|| {\n-                    let modern = SyntaxContext(syntax_contexts.len() as u32);\n+            let mut modern = syntax_contexts[self.0 as usize].modern;\n+            if data.marks[mark.0 as usize].modern {\n+                modern = *data.markings.entry((modern, mark)).or_insert_with(|| {\n+                    let len = syntax_contexts.len() as u32;\n                     syntax_contexts.push(SyntaxContextData {\n                         outer_mark: mark,\n-                        prev_ctxt: ctxt_data.modern,\n-                        modern: modern,\n+                        prev_ctxt: modern,\n+                        modern: SyntaxContext(len),\n                     });\n-                    modern\n-                })\n-            } else {\n-                ctxt_data.modern\n-            };\n+                    SyntaxContext(len)\n+                });\n+            }\n \n             *data.markings.entry((self, mark)).or_insert_with(|| {\n                 syntax_contexts.push(SyntaxContextData {"}, {"sha": "a7c247689cce88b08cbef538fde74eb6cff8e17b", "filename": "src/libsyntax_pos/lib.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e/src%2Flibsyntax_pos%2Flib.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e/src%2Flibsyntax_pos%2Flib.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax_pos%2Flib.rs?ref=4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e", "patch": "@@ -186,7 +186,7 @@ impl Span {\n \n     pub fn to(self, end: Span) -> Span {\n         // FIXME(jseyfried): self.ctxt should always equal end.ctxt here (c.f. issue #23480)\n-        if end.ctxt == SyntaxContext::empty() {\n+        if self.ctxt == SyntaxContext::empty() {\n             Span { lo: self.lo, ..end }\n         } else {\n             Span { hi: end.hi, ..self }"}, {"sha": "f95e4410381d96d1df37e111d080b6fc9f692c97", "filename": "src/test/compile-fail/asm-out-assign-imm.rs", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "blob_url": "https://github.com/rust-lang/rust/blob/4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e/src%2Ftest%2Fcompile-fail%2Fasm-out-assign-imm.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e/src%2Ftest%2Fcompile-fail%2Fasm-out-assign-imm.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ftest%2Fcompile-fail%2Fasm-out-assign-imm.rs?ref=4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e", "patch": "@@ -28,7 +28,6 @@ pub fn main() {\n         asm!(\"mov $1, $0\" : \"=r\"(x) : \"r\"(5));\n         //~^ ERROR re-assignment of immutable variable `x`\n         //~| NOTE re-assignment of immutable\n-        //~| NOTE in this expansion of asm!\n     }\n     foo(x);\n }"}, {"sha": "cc714a6e43141744d3beeb394e59fdab3694c2fd", "filename": "src/test/compile-fail/macro-context.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e/src%2Ftest%2Fcompile-fail%2Fmacro-context.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e/src%2Ftest%2Fcompile-fail%2Fmacro-context.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ftest%2Fcompile-fail%2Fmacro-context.rs?ref=4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e", "patch": "@@ -23,5 +23,5 @@ fn main() {\n         m!() => {}  //~ NOTE the usage of `m!` is likely invalid in pattern context\n     }\n \n-    m!();\n+    m!(); //~ NOTE in this expansion\n }"}, {"sha": "e2c68a626f91e9b87d478a935cde7541ec7fe314", "filename": "src/test/run-pass-fulldeps/auxiliary/cond_plugin.rs", "status": "modified", "additions": 15, "deletions": 28, "changes": 43, "blob_url": "https://github.com/rust-lang/rust/blob/4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e/src%2Ftest%2Frun-pass-fulldeps%2Fauxiliary%2Fcond_plugin.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e/src%2Ftest%2Frun-pass-fulldeps%2Fauxiliary%2Fcond_plugin.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ftest%2Frun-pass-fulldeps%2Fauxiliary%2Fcond_plugin.rs?ref=4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e", "patch": "@@ -8,50 +8,37 @@\n // option. This file may not be copied, modified, or distributed\n // except according to those terms.\n \n-#![allow(unused_parens)]\n-#![feature(plugin)]\n-#![feature(plugin_registrar)]\n-#![feature(rustc_private)]\n-#![plugin(proc_macro_plugin)]\n+// no-prefer-dynamic\n \n-extern crate rustc_plugin;\n-extern crate syntax;\n+#![crate_type = \"proc-macro\"]\n+#![feature(proc_macro)]\n \n-use rustc_plugin::Registry;\n+extern crate proc_macro;\n \n-use syntax::ext::base::SyntaxExtension;\n-use syntax::parse::token::Token;\n-use syntax::symbol::Symbol;\n-use syntax::tokenstream::{TokenTree, TokenStream};\n+use proc_macro::{TokenStream, TokenNode, quote};\n \n-#[plugin_registrar]\n-pub fn plugin_registrar(reg: &mut Registry) {\n-    reg.register_syntax_extension(Symbol::intern(\"cond\"),\n-                                  SyntaxExtension::ProcMacro(Box::new(cond)));\n-}\n-\n-fn cond(input: TokenStream) -> TokenStream {\n+#[proc_macro]\n+pub fn cond(input: TokenStream) -> TokenStream {\n     let mut conds = Vec::new();\n-    let mut input = input.trees().peekable();\n+    let mut input = input.into_iter().peekable();\n     while let Some(tree) = input.next() {\n-        let mut cond = match tree {\n-            TokenTree::Delimited(_, ref delimited) => delimited.stream(),\n+        let cond = match tree.kind {\n+            TokenNode::Group(_, cond) => cond,\n             _ => panic!(\"Invalid input\"),\n         };\n-        let mut trees = cond.trees();\n-        let test = trees.next();\n-        let rhs = trees.collect::<TokenStream>();\n+        let mut cond_trees = cond.clone().into_iter();\n+        let test = cond_trees.next().expect(\"Unexpected empty condition in `cond!`\");\n+        let rhs = cond_trees.collect::<TokenStream>();\n         if rhs.is_empty() {\n             panic!(\"Invalid macro usage in cond: {}\", cond);\n         }\n-        let is_else = match test {\n-            Some(TokenTree::Token(_, Token::Ident(ident))) if ident.name == \"else\" => true,\n+        let is_else = match test.kind {\n+            TokenNode::Term(word) => word.as_str() == \"else\",\n             _ => false,\n         };\n         conds.push(if is_else || input.peek().is_none() {\n             quote!({ $rhs })\n         } else {\n-            let test = test.unwrap();\n             quote!(if $test { $rhs } else)\n         });\n     }"}, {"sha": "cf6584e961a67a5997525be85d3488dc6dfbe628", "filename": "src/test/run-pass-fulldeps/auxiliary/hello_macro.rs", "status": "modified", "additions": 7, "deletions": 16, "changes": 23, "blob_url": "https://github.com/rust-lang/rust/blob/4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e/src%2Ftest%2Frun-pass-fulldeps%2Fauxiliary%2Fhello_macro.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e/src%2Ftest%2Frun-pass-fulldeps%2Fauxiliary%2Fhello_macro.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ftest%2Frun-pass-fulldeps%2Fauxiliary%2Fhello_macro.rs?ref=4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e", "patch": "@@ -8,29 +8,20 @@\n // option. This file may not be copied, modified, or distributed\n // except according to those terms.\n \n-#![feature(plugin)]\n-#![feature(plugin_registrar)]\n-#![feature(rustc_private)]\n-#![plugin(proc_macro_plugin)]\n+// no-prefer-dynamic\n \n-extern crate rustc_plugin;\n-extern crate syntax;\n+#![crate_type = \"proc-macro\"]\n+#![feature(proc_macro, proc_macro_lib)]\n \n-use rustc_plugin::Registry;\n-use syntax::ext::base::SyntaxExtension;\n-use syntax::symbol::Symbol;\n-use syntax::tokenstream::TokenStream;\n+extern crate proc_macro;\n \n-#[plugin_registrar]\n-pub fn plugin_registrar(reg: &mut Registry) {\n-    reg.register_syntax_extension(Symbol::intern(\"hello\"),\n-                                  SyntaxExtension::ProcMacro(Box::new(hello)));\n-}\n+use proc_macro::{TokenStream, quote};\n \n // This macro is not very interesting, but it does contain delimited tokens with\n // no content - `()` and `{}` - which has caused problems in the past.\n // Also, it tests that we can escape `$` via `$$`.\n-fn hello(_: TokenStream) -> TokenStream {\n+#[proc_macro]\n+pub fn hello(_: TokenStream) -> TokenStream {\n     quote!({\n         fn hello() {}\n         macro_rules! m { ($$($$t:tt)*) => { $$($$t)* } }"}, {"sha": "1b47043884844b49409b550d4356e09efed08785", "filename": "src/test/run-pass-fulldeps/auxiliary/proc_macro_def.rs", "status": "modified", "additions": 20, "deletions": 30, "changes": 50, "blob_url": "https://github.com/rust-lang/rust/blob/4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e/src%2Ftest%2Frun-pass-fulldeps%2Fauxiliary%2Fproc_macro_def.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e/src%2Ftest%2Frun-pass-fulldeps%2Fauxiliary%2Fproc_macro_def.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ftest%2Frun-pass-fulldeps%2Fauxiliary%2Fproc_macro_def.rs?ref=4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e", "patch": "@@ -8,47 +8,37 @@\n // option. This file may not be copied, modified, or distributed\n // except according to those terms.\n \n-#![feature(plugin, plugin_registrar, rustc_private)]\n-#![plugin(proc_macro_plugin)]\n-\n-extern crate rustc_plugin;\n-extern crate syntax;\n-\n-use rustc_plugin::Registry;\n-use syntax::ext::base::SyntaxExtension;\n-use syntax::tokenstream::TokenStream;\n-use syntax::symbol::Symbol;\n-\n-#[plugin_registrar]\n-pub fn plugin_registrar(reg: &mut Registry) {\n-    reg.register_syntax_extension(Symbol::intern(\"attr_tru\"),\n-                                  SyntaxExtension::AttrProcMacro(Box::new(attr_tru)));\n-    reg.register_syntax_extension(Symbol::intern(\"attr_identity\"),\n-                                  SyntaxExtension::AttrProcMacro(Box::new(attr_identity)));\n-    reg.register_syntax_extension(Symbol::intern(\"tru\"),\n-                                  SyntaxExtension::ProcMacro(Box::new(tru)));\n-    reg.register_syntax_extension(Symbol::intern(\"ret_tru\"),\n-                                  SyntaxExtension::ProcMacro(Box::new(ret_tru)));\n-    reg.register_syntax_extension(Symbol::intern(\"identity\"),\n-                                  SyntaxExtension::ProcMacro(Box::new(identity)));\n-}\n+// no-prefer-dynamic\n+\n+#![crate_type = \"proc-macro\"]\n+#![feature(proc_macro, proc_macro_lib)]\n+\n+extern crate proc_macro;\n+\n+use proc_macro::{TokenStream, quote};\n \n-fn attr_tru(_attr: TokenStream, _item: TokenStream) -> TokenStream {\n-    quote!(fn f1() -> bool { true })\n+#[proc_macro_attribute]\n+pub fn attr_tru(_attr: TokenStream, item: TokenStream) -> TokenStream {\n+    let name = item.into_iter().skip(1).next().unwrap();\n+    quote!(fn $name() -> bool { true })\n }\n \n-fn attr_identity(_attr: TokenStream, item: TokenStream) -> TokenStream {\n+#[proc_macro_attribute]\n+pub fn attr_identity(_attr: TokenStream, item: TokenStream) -> TokenStream {\n     quote!($item)\n }\n \n-fn tru(_ts: TokenStream) -> TokenStream {\n+#[proc_macro]\n+pub fn tru(_ts: TokenStream) -> TokenStream {\n     quote!(true)\n }\n \n-fn ret_tru(_ts: TokenStream) -> TokenStream {\n+#[proc_macro]\n+pub fn ret_tru(_ts: TokenStream) -> TokenStream {\n     quote!(return true;)\n }\n \n-fn identity(ts: TokenStream) -> TokenStream {\n+#[proc_macro]\n+pub fn identity(ts: TokenStream) -> TokenStream {\n     quote!($ts)\n }"}, {"sha": "e7d0a83017be004c4128d318ea1978515e18ed22", "filename": "src/test/run-pass-fulldeps/macro-quote-1.rs", "status": "removed", "additions": 0, "deletions": 40, "changes": 40, "blob_url": "https://github.com/rust-lang/rust/blob/3610a70ce488953c5b0379fece70f2baad30a825/src%2Ftest%2Frun-pass-fulldeps%2Fmacro-quote-1.rs", "raw_url": "https://github.com/rust-lang/rust/raw/3610a70ce488953c5b0379fece70f2baad30a825/src%2Ftest%2Frun-pass-fulldeps%2Fmacro-quote-1.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ftest%2Frun-pass-fulldeps%2Fmacro-quote-1.rs?ref=3610a70ce488953c5b0379fece70f2baad30a825", "patch": "@@ -1,40 +0,0 @@\n-// Copyright 2012-2014 The Rust Project Developers. See the COPYRIGHT\n-// file at the top-level directory of this distribution and at\n-// http://rust-lang.org/COPYRIGHT.\n-//\n-// Licensed under the Apache License, Version 2.0 <LICENSE-APACHE or\n-// http://www.apache.org/licenses/LICENSE-2.0> or the MIT license\n-// <LICENSE-MIT or http://opensource.org/licenses/MIT>, at your\n-// option. This file may not be copied, modified, or distributed\n-// except according to those terms.\n-\n-// ignore-stage1\n-\n-#![feature(plugin)]\n-#![feature(rustc_private)]\n-#![plugin(proc_macro_plugin)]\n-\n-extern crate syntax;\n-extern crate syntax_pos;\n-\n-use syntax::ast::{Ident, Name};\n-use syntax::parse::token::{self, Token, Lit};\n-use syntax::tokenstream::TokenTree;\n-\n-fn main() {\n-    let true_tok = token::Ident(Ident::from_str(\"true\"));\n-    assert!(quote!(true).eq_unspanned(&true_tok.into()));\n-\n-    // issue #35829, extended check to proc_macro.\n-    let triple_dot_tok = Token::DotDotDot;\n-    assert!(quote!(...).eq_unspanned(&triple_dot_tok.into()));\n-\n-    let byte_str_tok = Token::Literal(Lit::ByteStr(Name::intern(\"one\")), None);\n-    assert!(quote!(b\"one\").eq_unspanned(&byte_str_tok.into()));\n-\n-    let byte_str_raw_tok = Token::Literal(Lit::ByteStrRaw(Name::intern(\"#\\\"two\\\"#\"), 3), None);\n-    assert!(quote!(br###\"#\"two\"#\"###).eq_unspanned(&byte_str_raw_tok.into()));\n-\n-    let str_raw_tok = Token::Literal(Lit::StrRaw(Name::intern(\"#\\\"three\\\"#\"), 2), None);\n-    assert!(quote!(r##\"#\"three\"#\"##).eq_unspanned(&str_raw_tok.into()));\n-}"}, {"sha": "cff743bdae6cd485c756a145d2f5ad903a93b9da", "filename": "src/test/run-pass-fulldeps/macro-quote-cond.rs", "status": "modified", "additions": 5, "deletions": 3, "changes": 8, "blob_url": "https://github.com/rust-lang/rust/blob/4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e/src%2Ftest%2Frun-pass-fulldeps%2Fmacro-quote-cond.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e/src%2Ftest%2Frun-pass-fulldeps%2Fmacro-quote-cond.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ftest%2Frun-pass-fulldeps%2Fmacro-quote-cond.rs?ref=4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e", "patch": "@@ -11,9 +11,11 @@\n // aux-build:cond_plugin.rs\n // ignore-stage1\n \n-#![feature(plugin)]\n-#![feature(rustc_private)]\n-#![plugin(cond_plugin)]\n+#![feature(proc_macro)]\n+\n+extern crate cond_plugin;\n+\n+use cond_plugin::cond;\n \n fn fact(n : i64) -> i64 {\n     if n == 0 {"}, {"sha": "eb77895e2d7ad6da63429583b983ee4321f19173", "filename": "src/test/run-pass-fulldeps/macro-quote-test.rs", "status": "modified", "additions": 4, "deletions": 4, "changes": 8, "blob_url": "https://github.com/rust-lang/rust/blob/4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e/src%2Ftest%2Frun-pass-fulldeps%2Fmacro-quote-test.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e/src%2Ftest%2Frun-pass-fulldeps%2Fmacro-quote-test.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ftest%2Frun-pass-fulldeps%2Fmacro-quote-test.rs?ref=4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e", "patch": "@@ -13,10 +13,10 @@\n // aux-build:hello_macro.rs\n // ignore-stage1\n \n-#![feature(plugin)]\n-#![feature(rustc_private)]\n-#![plugin(hello_macro)]\n+#![feature(proc_macro)]\n+\n+extern crate hello_macro;\n \n fn main() {\n-    hello!();\n+    hello_macro::hello!();\n }"}, {"sha": "93815d16837d30fa40cda8ae9ee2f387ed41e44a", "filename": "src/test/run-pass-fulldeps/proc-macro/auxiliary/attr-args.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e/src%2Ftest%2Frun-pass-fulldeps%2Fproc-macro%2Fauxiliary%2Fattr-args.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e/src%2Ftest%2Frun-pass-fulldeps%2Fproc-macro%2Fauxiliary%2Fattr-args.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ftest%2Frun-pass-fulldeps%2Fproc-macro%2Fauxiliary%2Fattr-args.rs?ref=4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e", "patch": "@@ -24,7 +24,7 @@ pub fn attr_with_args(args: TokenStream, input: TokenStream) -> TokenStream {\n \n     let input = input.to_string();\n \n-    assert_eq!(input, \"fn foo (  ) {  }\");\n+    assert_eq!(input, \"fn foo() { }\");\n \n     r#\"\n         fn foo() -> &'static str { \"Hello, world!\" }"}, {"sha": "ec2ff0d1e2b8c81769fd777fa2ff9cfa7d243014", "filename": "src/test/run-pass-fulldeps/proc-macro/auxiliary/count_compound_ops.rs", "status": "added", "additions": 36, "deletions": 0, "changes": 36, "blob_url": "https://github.com/rust-lang/rust/blob/4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e/src%2Ftest%2Frun-pass-fulldeps%2Fproc-macro%2Fauxiliary%2Fcount_compound_ops.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e/src%2Ftest%2Frun-pass-fulldeps%2Fproc-macro%2Fauxiliary%2Fcount_compound_ops.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ftest%2Frun-pass-fulldeps%2Fproc-macro%2Fauxiliary%2Fcount_compound_ops.rs?ref=4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e", "patch": "@@ -0,0 +1,36 @@\n+// Copyright 2017 The Rust Project Developers. See the COPYRIGHT\n+// file at the top-level directory of this distribution and at\n+// http://rust-lang.org/COPYRIGHT.\n+//\n+// Licensed under the Apache License, Version 2.0 <LICENSE-APACHE or\n+// http://www.apache.org/licenses/LICENSE-2.0> or the MIT license\n+// <LICENSE-MIT or http://opensource.org/licenses/MIT>, at your\n+// option. This file may not be copied, modified, or distributed\n+// except according to those terms.\n+\n+// no-prefer-dynamic\n+\n+#![feature(proc_macro)]\n+#![crate_type = \"proc-macro\"]\n+\n+extern crate proc_macro;\n+\n+use proc_macro::{TokenStream, TokenNode, Spacing, Literal, quote};\n+\n+#[proc_macro]\n+pub fn count_compound_ops(input: TokenStream) -> TokenStream {\n+    assert_eq!(count_compound_ops_helper(quote!(++ (&&) 4@a)), 3);\n+    TokenNode::Literal(Literal::u32(count_compound_ops_helper(input))).into()\n+}\n+\n+fn count_compound_ops_helper(input: TokenStream) -> u32 {\n+    let mut count = 0;\n+    for token in input {\n+        match token.kind {\n+            TokenNode::Op(c, Spacing::Alone) => count += 1,\n+            TokenNode::Group(_, tokens) => count += count_compound_ops_helper(tokens),\n+            _ => {}\n+        }\n+    }\n+    count\n+}"}, {"sha": "8ffa7abe6f7f943e35f6029ef74679df939c0b92", "filename": "src/test/run-pass-fulldeps/proc-macro/auxiliary/hygiene_example.rs", "status": "added", "additions": 19, "deletions": 0, "changes": 19, "blob_url": "https://github.com/rust-lang/rust/blob/4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e/src%2Ftest%2Frun-pass-fulldeps%2Fproc-macro%2Fauxiliary%2Fhygiene_example.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e/src%2Ftest%2Frun-pass-fulldeps%2Fproc-macro%2Fauxiliary%2Fhygiene_example.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ftest%2Frun-pass-fulldeps%2Fproc-macro%2Fauxiliary%2Fhygiene_example.rs?ref=4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e", "patch": "@@ -0,0 +1,19 @@\n+// Copyright 2017 The Rust Project Developers. See the COPYRIGHT\n+// file at the top-level directory of this distribution and at\n+// http://rust-lang.org/COPYRIGHT.\n+//\n+// Licensed under the Apache License, Version 2.0 <LICENSE-APACHE or\n+// http://www.apache.org/licenses/LICENSE-2.0> or the MIT license\n+// <LICENSE-MIT or http://opensource.org/licenses/MIT>, at your\n+// option. This file may not be copied, modified, or distributed\n+// except according to those terms.\n+\n+#![feature(proc_macro)]\n+\n+extern crate hygiene_example_codegen;\n+\n+pub use hygiene_example_codegen::hello;\n+\n+pub fn print(string: &str) {\n+    println!(\"{}\", string);\n+}"}, {"sha": "055e4e2fad7af52c84f2fc5850ea6fc6969a2b79", "filename": "src/test/run-pass-fulldeps/proc-macro/auxiliary/hygiene_example_codegen.rs", "status": "added", "additions": 36, "deletions": 0, "changes": 36, "blob_url": "https://github.com/rust-lang/rust/blob/4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e/src%2Ftest%2Frun-pass-fulldeps%2Fproc-macro%2Fauxiliary%2Fhygiene_example_codegen.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e/src%2Ftest%2Frun-pass-fulldeps%2Fproc-macro%2Fauxiliary%2Fhygiene_example_codegen.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ftest%2Frun-pass-fulldeps%2Fproc-macro%2Fauxiliary%2Fhygiene_example_codegen.rs?ref=4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e", "patch": "@@ -0,0 +1,36 @@\n+// Copyright 2017 The Rust Project Developers. See the COPYRIGHT\n+// file at the top-level directory of this distribution and at\n+// http://rust-lang.org/COPYRIGHT.\n+//\n+// Licensed under the Apache License, Version 2.0 <LICENSE-APACHE or\n+// http://www.apache.org/licenses/LICENSE-2.0> or the MIT license\n+// <LICENSE-MIT or http://opensource.org/licenses/MIT>, at your\n+// option. This file may not be copied, modified, or distributed\n+// except according to those terms.\n+\n+// no-prefer-dynamic\n+\n+#![feature(proc_macro)]\n+#![crate_type = \"proc-macro\"]\n+\n+extern crate proc_macro as proc_macro_renamed; // This does not break `quote!`\n+\n+use proc_macro_renamed::{TokenStream, quote};\n+\n+#[proc_macro]\n+pub fn hello(input: TokenStream) -> TokenStream {\n+    quote!(hello_helper!($input))\n+    //^ `hello_helper!` always resolves to the following proc macro,\n+    //| no matter where `hello!` is used.\n+}\n+\n+#[proc_macro]\n+pub fn hello_helper(input: TokenStream) -> TokenStream {\n+    quote! {\n+        extern crate hygiene_example; // This is never a conflict error\n+        let string = format!(\"hello {}\", $input);\n+        //^ `format!` always resolves to the prelude macro,\n+        //| even if a different `format!` is in scope where `hello!` is used.\n+        hygiene_example::print(&string)\n+    }\n+}"}, {"sha": "1a2b144e4717bae285eeb833986f12c6c5f13d30", "filename": "src/test/run-pass-fulldeps/proc-macro/count_compound_ops.rs", "status": "added", "additions": 20, "deletions": 0, "changes": 20, "blob_url": "https://github.com/rust-lang/rust/blob/4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e/src%2Ftest%2Frun-pass-fulldeps%2Fproc-macro%2Fcount_compound_ops.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e/src%2Ftest%2Frun-pass-fulldeps%2Fproc-macro%2Fcount_compound_ops.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ftest%2Frun-pass-fulldeps%2Fproc-macro%2Fcount_compound_ops.rs?ref=4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e", "patch": "@@ -0,0 +1,20 @@\n+// Copyright 2017 The Rust Project Developers. See the COPYRIGHT\n+// file at the top-level directory of this distribution and at\n+// http://rust-lang.org/COPYRIGHT.\n+//\n+// Licensed under the Apache License, Version 2.0 <LICENSE-APACHE or\n+// http://www.apache.org/licenses/LICENSE-2.0> or the MIT license\n+// <LICENSE-MIT or http://opensource.org/licenses/MIT>, at your\n+// option. This file may not be copied, modified, or distributed\n+// except according to those terms.\n+\n+// aux-build:count_compound_ops.rs\n+\n+#![feature(proc_macro)]\n+\n+extern crate count_compound_ops;\n+use count_compound_ops::count_compound_ops;\n+\n+fn main() {\n+    assert_eq!(count_compound_ops!(foo<=>bar <<<! -baz ++), 4);\n+}"}, {"sha": "51198db5aa76d137fdc81d1da315c29ed7ae0113", "filename": "src/test/run-pass-fulldeps/proc-macro/hygiene_example.rs", "status": "added", "additions": 27, "deletions": 0, "changes": 27, "blob_url": "https://github.com/rust-lang/rust/blob/4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e/src%2Ftest%2Frun-pass-fulldeps%2Fproc-macro%2Fhygiene_example.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e/src%2Ftest%2Frun-pass-fulldeps%2Fproc-macro%2Fhygiene_example.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ftest%2Frun-pass-fulldeps%2Fproc-macro%2Fhygiene_example.rs?ref=4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e", "patch": "@@ -0,0 +1,27 @@\n+// Copyright 2017 The Rust Project Developers. See the COPYRIGHT\n+// file at the top-level directory of this distribution and at\n+// http://rust-lang.org/COPYRIGHT.\n+//\n+// Licensed under the Apache License, Version 2.0 <LICENSE-APACHE or\n+// http://www.apache.org/licenses/LICENSE-2.0> or the MIT license\n+// <LICENSE-MIT or http://opensource.org/licenses/MIT>, at your\n+// option. This file may not be copied, modified, or distributed\n+// except according to those terms.\n+\n+// aux-build:hygiene_example_codegen.rs\n+// aux-build:hygiene_example.rs\n+\n+#![feature(proc_macro)]\n+\n+extern crate hygiene_example;\n+use hygiene_example::hello;\n+\n+fn main() {\n+    mod hygiene_example {} // no conflict with `extern crate hygiene_example;` from the proc macro\n+    macro_rules! format { () => {} } // does not interfere with `format!` from the proc macro\n+    macro_rules! hello_helper { () => {} } // similarly does not intefere with the proc macro\n+\n+    let string = \"world\"; // no conflict with `string` from the proc macro\n+    hello!(string);\n+    hello!(string);\n+}"}, {"sha": "cdda723585b7a850bb23de640358c26919ffb8b1", "filename": "src/test/run-pass-fulldeps/proc_macro.rs", "status": "modified", "additions": 4, "deletions": 3, "changes": 7, "blob_url": "https://github.com/rust-lang/rust/blob/4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e/src%2Ftest%2Frun-pass-fulldeps%2Fproc_macro.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e/src%2Ftest%2Frun-pass-fulldeps%2Fproc_macro.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ftest%2Frun-pass-fulldeps%2Fproc_macro.rs?ref=4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e", "patch": "@@ -12,10 +12,11 @@\n // ignore-stage1\n // ignore-cross-compile\n \n-#![feature(plugin, custom_attribute)]\n-#![feature(type_macros)]\n+#![feature(proc_macro)]\n \n-#![plugin(proc_macro_def)]\n+extern crate proc_macro_def;\n+\n+use proc_macro_def::{attr_tru, attr_identity, identity, ret_tru, tru};\n \n #[attr_tru]\n fn f1() -> bool {"}, {"sha": "08749373432f567761c01f2e208497baa93f86fd", "filename": "src/test/ui/token/macro-incomplete-parse.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e/src%2Ftest%2Fui%2Ftoken%2Fmacro-incomplete-parse.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e/src%2Ftest%2Fui%2Ftoken%2Fmacro-incomplete-parse.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ftest%2Fui%2Ftoken%2Fmacro-incomplete-parse.rs?ref=4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e", "patch": "@@ -32,7 +32,7 @@ macro_rules! ignored_pat {\n ignored_item!(); //~ NOTE caused by the macro expansion here\n \n fn main() {\n-    ignored_expr!();\n+    ignored_expr!(); //~ NOTE in this expansion\n     match 1 {\n         ignored_pat!() => (), //~ NOTE caused by the macro expansion here\n         _ => (),"}, {"sha": "6bce09af052500cfec2353ba1043aa1b548931ad", "filename": "src/test/ui/token/macro-incomplete-parse.stderr", "status": "modified", "additions": 3, "deletions": 0, "changes": 3, "blob_url": "https://github.com/rust-lang/rust/blob/4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e/src%2Ftest%2Fui%2Ftoken%2Fmacro-incomplete-parse.stderr", "raw_url": "https://github.com/rust-lang/rust/raw/4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e/src%2Ftest%2Fui%2Ftoken%2Fmacro-incomplete-parse.stderr", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ftest%2Fui%2Ftoken%2Fmacro-incomplete-parse.stderr?ref=4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e", "patch": "@@ -15,6 +15,9 @@ error: expected one of `.`, `;`, `?`, `}`, or an operator, found `,`\n    |\n 22 |     () => ( 1,  //~ ERROR expected one of `.`, `;`, `?`, `}`, or an operator, found `,`\n    |              ^ expected one of `.`, `;`, `?`, `}`, or an operator here\n+...\n+35 |     ignored_expr!(); //~ NOTE in this expansion\n+   |     ---------------- in this macro invocation\n \n error: macro expansion ignores token `,` and any following\n   --> $DIR/macro-incomplete-parse.rs:29:14"}, {"sha": "f40fea60f40a89bbf6b297da58cb89bad8178583", "filename": "src/tools/tidy/src/cargo.rs", "status": "modified", "additions": 0, "deletions": 8, "changes": 8, "blob_url": "https://github.com/rust-lang/rust/blob/4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e/src%2Ftools%2Ftidy%2Fsrc%2Fcargo.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e/src%2Ftools%2Ftidy%2Fsrc%2Fcargo.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ftools%2Ftidy%2Fsrc%2Fcargo.rs?ref=4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e", "patch": "@@ -91,14 +91,6 @@ fn verify(tomlfile: &Path, libfile: &Path, bad: &mut bool) {\n             continue\n         }\n \n-        // We want the compiler to depend on the proc_macro_plugin crate so\n-        // that it is built and included in the end, but we don't want to\n-        // actually use it in the compiler.\n-        if toml.contains(\"name = \\\"rustc_driver\\\"\") &&\n-           krate == \"proc_macro_plugin\" {\n-            continue\n-        }\n-\n         if !librs.contains(&format!(\"extern crate {}\", krate)) {\n             tidy_error!(bad, \"{} doesn't have `extern crate {}`, but Cargo.toml \\\n                               depends on it\", libfile.display(), krate);"}, {"sha": "4c94ade98d96524749bf00a3ab7845e91bcd4925", "filename": "src/tools/tidy/src/features.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e/src%2Ftools%2Ftidy%2Fsrc%2Ffeatures.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e/src%2Ftools%2Ftidy%2Fsrc%2Ffeatures.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ftools%2Ftidy%2Fsrc%2Ffeatures.rs?ref=4d526e0d14b43a87627cd6aca6c6f71ad1e07b6e", "patch": "@@ -245,7 +245,7 @@ fn get_and_check_lib_features(base_src_path: &Path,\n                     let mut err = |msg: &str| {\n                         tidy_error!(bad, \"{}:{}: {}\", file.display(), line, msg);\n                     };\n-                    if lang_features.contains_key(name) {\n+                    if lang_features.contains_key(name) && name != \"proc_macro\" {\n                         err(\"duplicating a lang feature\");\n                     }\n                     if let Some(ref s) = lib_features.get(name) {"}]}