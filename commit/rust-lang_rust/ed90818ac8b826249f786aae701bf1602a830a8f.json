{"sha": "ed90818ac8b826249f786aae701bf1602a830a8f", "node_id": "MDY6Q29tbWl0NzI0NzEyOmVkOTA4MThhYzhiODI2MjQ5Zjc4NmFhZTcwMWJmMTYwMmE4MzBhOGY=", "commit": {"author": {"name": "Paul Daniel Faria", "email": "Nashenas88@users.noreply.github.com", "date": "2019-11-06T17:34:16Z"}, "committer": {"name": "Paul Daniel Faria", "email": "Nashenas88@users.noreply.github.com", "date": "2019-12-02T13:40:56Z"}, "message": "Remove files created during conflict resolution", "tree": {"sha": "c8922e71c80dd1cec5b264f9334f49fd62c4110a", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/c8922e71c80dd1cec5b264f9334f49fd62c4110a"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/ed90818ac8b826249f786aae701bf1602a830a8f", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/ed90818ac8b826249f786aae701bf1602a830a8f", "html_url": "https://github.com/rust-lang/rust/commit/ed90818ac8b826249f786aae701bf1602a830a8f", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/ed90818ac8b826249f786aae701bf1602a830a8f/comments", "author": {"login": "Nashenas88", "id": 1673130, "node_id": "MDQ6VXNlcjE2NzMxMzA=", "avatar_url": "https://avatars.githubusercontent.com/u/1673130?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Nashenas88", "html_url": "https://github.com/Nashenas88", "followers_url": "https://api.github.com/users/Nashenas88/followers", "following_url": "https://api.github.com/users/Nashenas88/following{/other_user}", "gists_url": "https://api.github.com/users/Nashenas88/gists{/gist_id}", "starred_url": "https://api.github.com/users/Nashenas88/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Nashenas88/subscriptions", "organizations_url": "https://api.github.com/users/Nashenas88/orgs", "repos_url": "https://api.github.com/users/Nashenas88/repos", "events_url": "https://api.github.com/users/Nashenas88/events{/privacy}", "received_events_url": "https://api.github.com/users/Nashenas88/received_events", "type": "User", "site_admin": false}, "committer": {"login": "Nashenas88", "id": 1673130, "node_id": "MDQ6VXNlcjE2NzMxMzA=", "avatar_url": "https://avatars.githubusercontent.com/u/1673130?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Nashenas88", "html_url": "https://github.com/Nashenas88", "followers_url": "https://api.github.com/users/Nashenas88/followers", "following_url": "https://api.github.com/users/Nashenas88/following{/other_user}", "gists_url": "https://api.github.com/users/Nashenas88/gists{/gist_id}", "starred_url": "https://api.github.com/users/Nashenas88/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Nashenas88/subscriptions", "organizations_url": "https://api.github.com/users/Nashenas88/orgs", "repos_url": "https://api.github.com/users/Nashenas88/repos", "events_url": "https://api.github.com/users/Nashenas88/events{/privacy}", "received_events_url": "https://api.github.com/users/Nashenas88/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "51b06656da0106c254466429fa1f9d58bc74ea72", "url": "https://api.github.com/repos/rust-lang/rust/commits/51b06656da0106c254466429fa1f9d58bc74ea72", "html_url": "https://github.com/rust-lang/rust/commit/51b06656da0106c254466429fa1f9d58bc74ea72"}], "stats": {"total": 1696, "additions": 0, "deletions": 1696}, "files": [{"sha": "d715a4e0e09c5a9b6f9afb979fc6e8e4e2098870", "filename": "src/librustc_codegen_ssa/mir/block.rs.orig", "status": "removed", "additions": 0, "deletions": 1257, "changes": 1257, "blob_url": "https://github.com/rust-lang/rust/blob/51b06656da0106c254466429fa1f9d58bc74ea72/src%2Flibrustc_codegen_ssa%2Fmir%2Fblock.rs.orig", "raw_url": "https://github.com/rust-lang/rust/raw/51b06656da0106c254466429fa1f9d58bc74ea72/src%2Flibrustc_codegen_ssa%2Fmir%2Fblock.rs.orig", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_codegen_ssa%2Fmir%2Fblock.rs.orig?ref=51b06656da0106c254466429fa1f9d58bc74ea72", "patch": "@@ -1,1257 +0,0 @@\n-use rustc_index::vec::Idx;\n-use rustc::middle::lang_items;\n-use rustc::ty::{self, Ty, TypeFoldable, Instance};\n-use rustc::ty::layout::{self, LayoutOf, HasTyCtxt, FnAbiExt};\n-use rustc::mir::{self, PlaceBase, Static, StaticKind};\n-use rustc::mir::interpret::PanicInfo;\n-use rustc_target::abi::call::{ArgAbi, FnAbi, PassMode};\n-use rustc_target::spec::abi::Abi;\n-use crate::base;\n-use crate::MemFlags;\n-use crate::common::{self, IntPredicate};\n-use crate::meth;\n-\n-use crate::traits::*;\n-\n-use std::borrow::Cow;\n-\n-use syntax::{source_map::Span, symbol::Symbol};\n-\n-use super::{FunctionCx, LocalRef};\n-use super::place::PlaceRef;\n-use super::operand::OperandRef;\n-use super::operand::OperandValue::{Pair, Ref, Immediate};\n-\n-/// Used by `FunctionCx::codegen_terminator` for emitting common patterns\n-/// e.g., creating a basic block, calling a function, etc.\n-struct TerminatorCodegenHelper<'a, 'tcx> {\n-    bb: &'a mir::BasicBlock,\n-    terminator: &'a mir::Terminator<'tcx>,\n-    funclet_bb: Option<mir::BasicBlock>,\n-}\n-\n-impl<'a, 'tcx> TerminatorCodegenHelper<'a, 'tcx> {\n-    /// Returns the associated funclet from `FunctionCx::funclets` for the\n-    /// `funclet_bb` member if it is not `None`.\n-    fn funclet<'c, 'b, Bx: BuilderMethods<'b, 'tcx>>(\n-        &self,\n-        fx: &'c mut FunctionCx<'b, 'tcx, Bx>,\n-    ) -> Option<&'c Bx::Funclet> {\n-        match self.funclet_bb {\n-            Some(funcl) => fx.funclets[funcl].as_ref(),\n-            None => None,\n-        }\n-    }\n-\n-    fn lltarget<'b, 'c, Bx: BuilderMethods<'b, 'tcx>>(\n-        &self,\n-        fx: &'c mut FunctionCx<'b, 'tcx, Bx>,\n-        target: mir::BasicBlock,\n-    ) -> (Bx::BasicBlock, bool) {\n-        let span = self.terminator.source_info.span;\n-        let lltarget = fx.blocks[target];\n-        let target_funclet = fx.cleanup_kinds[target].funclet_bb(target);\n-        match (self.funclet_bb, target_funclet) {\n-            (None, None) => (lltarget, false),\n-            (Some(f), Some(t_f)) if f == t_f || !base::wants_msvc_seh(fx.cx.tcx().sess) =>\n-                (lltarget, false),\n-            // jump *into* cleanup - need a landing pad if GNU\n-            (None, Some(_)) => (fx.landing_pad_to(target), false),\n-            (Some(_), None) => span_bug!(span, \"{:?} - jump out of cleanup?\", self.terminator),\n-            (Some(_), Some(_)) => (fx.landing_pad_to(target), true),\n-        }\n-    }\n-\n-    /// Create a basic block.\n-    fn llblock<'c, 'b, Bx: BuilderMethods<'b, 'tcx>>(\n-        &self,\n-        fx: &'c mut FunctionCx<'b, 'tcx, Bx>,\n-        target: mir::BasicBlock,\n-    ) -> Bx::BasicBlock {\n-        let (lltarget, is_cleanupret) = self.lltarget(fx, target);\n-        if is_cleanupret {\n-            // MSVC cross-funclet jump - need a trampoline\n-\n-            debug!(\"llblock: creating cleanup trampoline for {:?}\", target);\n-            let name = &format!(\"{:?}_cleanup_trampoline_{:?}\", self.bb, target);\n-            let mut trampoline = fx.new_block(name);\n-            trampoline.cleanup_ret(self.funclet(fx).unwrap(),\n-                                   Some(lltarget));\n-            trampoline.llbb()\n-        } else {\n-            lltarget\n-        }\n-    }\n-\n-    fn funclet_br<'c, 'b, Bx: BuilderMethods<'b, 'tcx>>(\n-        &self,\n-        fx: &'c mut FunctionCx<'b, 'tcx, Bx>,\n-        bx: &mut Bx,\n-        target: mir::BasicBlock,\n-    ) {\n-        let (lltarget, is_cleanupret) = self.lltarget(fx, target);\n-        if is_cleanupret {\n-            // micro-optimization: generate a `ret` rather than a jump\n-            // to a trampoline.\n-            bx.cleanup_ret(self.funclet(fx).unwrap(), Some(lltarget));\n-        } else {\n-            bx.br(lltarget);\n-        }\n-    }\n-\n-    /// Call `fn_ptr` of `fn_abi` with the arguments `llargs`, the optional\n-    /// return destination `destination` and the cleanup function `cleanup`.\n-    fn do_call<'c, 'b, Bx: BuilderMethods<'b, 'tcx>>(\n-        &self,\n-        fx: &'c mut FunctionCx<'b, 'tcx, Bx>,\n-        bx: &mut Bx,\n-        fn_abi: FnAbi<'tcx, Ty<'tcx>>,\n-        fn_ptr: Bx::Value,\n-        llargs: &[Bx::Value],\n-        destination: Option<(ReturnDest<'tcx, Bx::Value>, mir::BasicBlock)>,\n-        cleanup: Option<mir::BasicBlock>,\n-    ) {\n-        if let Some(cleanup) = cleanup {\n-            let ret_bx = if let Some((_, target)) = destination {\n-                fx.blocks[target]\n-            } else {\n-                fx.unreachable_block()\n-            };\n-            let invokeret = bx.invoke(fn_ptr,\n-                                      &llargs,\n-                                      ret_bx,\n-                                      self.llblock(fx, cleanup),\n-                                      self.funclet(fx));\n-            bx.apply_attrs_callsite(&fn_abi, invokeret);\n-\n-            if let Some((ret_dest, target)) = destination {\n-                let mut ret_bx = fx.build_block(target);\n-                fx.set_debug_loc(&mut ret_bx, self.terminator.source_info);\n-                fx.store_return(&mut ret_bx, ret_dest, &fn_abi.ret, invokeret);\n-            }\n-        } else {\n-            let llret = bx.call(fn_ptr, &llargs, self.funclet(fx));\n-            bx.apply_attrs_callsite(&fn_abi, llret);\n-            if fx.mir[*self.bb].is_cleanup {\n-                // Cleanup is always the cold path. Don't inline\n-                // drop glue. Also, when there is a deeply-nested\n-                // struct, there are \"symmetry\" issues that cause\n-                // exponential inlining - see issue #41696.\n-                bx.do_not_inline(llret);\n-            }\n-\n-            if let Some((ret_dest, target)) = destination {\n-                fx.store_return(bx, ret_dest, &fn_abi.ret, llret);\n-                self.funclet_br(fx, bx, target);\n-            } else {\n-                bx.unreachable();\n-            }\n-        }\n-    }\n-\n-    // Generate sideeffect intrinsic if jumping to any of the targets can form\n-    // a loop.\n-    fn maybe_sideeffect<'b, 'tcx2: 'b, Bx: BuilderMethods<'b, 'tcx2>>(\n-        &self,\n-        mir: mir::ReadOnlyBodyCache<'b, 'tcx>,\n-        bx: &mut Bx,\n-        targets: &[mir::BasicBlock],\n-    ) {\n-        if bx.tcx().sess.opts.debugging_opts.insert_sideeffect {\n-            if targets.iter().any(|target| {\n-                *target <= *self.bb\n-                    && target\n-                        .start_location()\n-                        .is_predecessor_of(self.bb.start_location(), mir)\n-            }) {\n-                bx.sideeffect();\n-            }\n-        }\n-    }\n-}\n-\n-/// Codegen implementations for some terminator variants.\n-impl<'a, 'tcx, Bx: BuilderMethods<'a, 'tcx>> FunctionCx<'a, 'tcx, Bx> {\n-    /// Generates code for a `Resume` terminator.\n-    fn codegen_resume_terminator<'b>(\n-        &mut self,\n-        helper: TerminatorCodegenHelper<'b, 'tcx>,\n-        mut bx: Bx,\n-    ) {\n-        if let Some(funclet) = helper.funclet(self) {\n-            bx.cleanup_ret(funclet, None);\n-        } else {\n-            let slot = self.get_personality_slot(&mut bx);\n-            let lp0 = slot.project_field(&mut bx, 0);\n-            let lp0 = bx.load_operand(lp0).immediate();\n-            let lp1 = slot.project_field(&mut bx, 1);\n-            let lp1 = bx.load_operand(lp1).immediate();\n-            slot.storage_dead(&mut bx);\n-\n-            if !bx.sess().target.target.options.custom_unwind_resume {\n-                let mut lp = bx.const_undef(self.landing_pad_type());\n-                lp = bx.insert_value(lp, lp0, 0);\n-                lp = bx.insert_value(lp, lp1, 1);\n-                bx.resume(lp);\n-            } else {\n-                bx.call(bx.eh_unwind_resume(), &[lp0],\n-                        helper.funclet(self));\n-                bx.unreachable();\n-            }\n-        }\n-    }\n-\n-    fn codegen_switchint_terminator<'b>(\n-        &mut self,\n-        helper: TerminatorCodegenHelper<'b, 'tcx>,\n-        mut bx: Bx,\n-        discr: &mir::Operand<'tcx>,\n-        switch_ty: Ty<'tcx>,\n-        values: &Cow<'tcx, [u128]>,\n-        targets: &Vec<mir::BasicBlock>,\n-    ) {\n-        let discr = self.codegen_operand(&mut bx, &discr);\n-        if targets.len() == 2 {\n-            // If there are two targets, emit br instead of switch\n-            let lltrue = helper.llblock(self, targets[0]);\n-            let llfalse = helper.llblock(self, targets[1]);\n-            if switch_ty == bx.tcx().types.bool {\n-                helper.maybe_sideeffect(self.mir, &mut bx, targets.as_slice());\n-                // Don't generate trivial icmps when switching on bool\n-                if let [0] = values[..] {\n-                    bx.cond_br(discr.immediate(), llfalse, lltrue);\n-                } else {\n-                    assert_eq!(&values[..], &[1]);\n-                    bx.cond_br(discr.immediate(), lltrue, llfalse);\n-                }\n-            } else {\n-                let switch_llty = bx.immediate_backend_type(\n-                    bx.layout_of(switch_ty)\n-                );\n-                let llval = bx.const_uint_big(switch_llty, values[0]);\n-                let cmp = bx.icmp(IntPredicate::IntEQ, discr.immediate(), llval);\n-                helper.maybe_sideeffect(self.mir, &mut bx, targets.as_slice());\n-                bx.cond_br(cmp, lltrue, llfalse);\n-            }\n-        } else {\n-            helper.maybe_sideeffect(self.mir, &mut bx, targets.as_slice());\n-            let (otherwise, targets) = targets.split_last().unwrap();\n-            bx.switch(\n-                discr.immediate(),\n-                helper.llblock(self, *otherwise),\n-                values.iter().zip(targets).map(|(&value, target)| {\n-                    (value, helper.llblock(self, *target))\n-                })\n-            );\n-        }\n-    }\n-\n-    fn codegen_return_terminator(&mut self, mut bx: Bx) {\n-        // Call `va_end` if this is the definition of a C-variadic function.\n-        if self.fn_abi.c_variadic {\n-            // The `VaList` \"spoofed\" argument is just after all the real arguments.\n-            let va_list_arg_idx = self.fn_abi.args.len();\n-            match self.locals[mir::Local::new(1 + va_list_arg_idx)] {\n-                LocalRef::Place(va_list) => {\n-                    bx.va_end(va_list.llval);\n-                }\n-                _ => bug!(\"C-variadic function must have a `VaList` place\"),\n-            }\n-        }\n-        if self.fn_abi.ret.layout.abi.is_uninhabited() {\n-            // Functions with uninhabited return values are marked `noreturn`,\n-            // so we should make sure that we never actually do.\n-            bx.abort();\n-            bx.unreachable();\n-            return;\n-        }\n-        let llval = match self.fn_abi.ret.mode {\n-            PassMode::Ignore | PassMode::Indirect(..) => {\n-                bx.ret_void();\n-                return;\n-            }\n-\n-            PassMode::Direct(_) | PassMode::Pair(..) => {\n-                let op =\n-                    self.codegen_consume(&mut bx, &mir::Place::return_place().as_ref());\n-                if let Ref(llval, _, align) = op.val {\n-                    bx.load(llval, align)\n-                } else {\n-                    op.immediate_or_packed_pair(&mut bx)\n-                }\n-            }\n-\n-            PassMode::Cast(cast_ty) => {\n-                let op = match self.locals[mir::RETURN_PLACE] {\n-                    LocalRef::Operand(Some(op)) => op,\n-                    LocalRef::Operand(None) => bug!(\"use of return before def\"),\n-                    LocalRef::Place(cg_place) => {\n-                        OperandRef {\n-                            val: Ref(cg_place.llval, None, cg_place.align),\n-                            layout: cg_place.layout\n-                        }\n-                    }\n-                    LocalRef::UnsizedPlace(_) => bug!(\"return type must be sized\"),\n-                };\n-                let llslot = match op.val {\n-                    Immediate(_) | Pair(..) => {\n-                        let scratch =\n-                            PlaceRef::alloca(&mut bx, self.fn_abi.ret.layout);\n-                        op.val.store(&mut bx, scratch);\n-                        scratch.llval\n-                    }\n-                    Ref(llval, _, align) => {\n-                        assert_eq!(align, op.layout.align.abi,\n-                                   \"return place is unaligned!\");\n-                        llval\n-                    }\n-                };\n-                let addr = bx.pointercast(llslot, bx.type_ptr_to(\n-                    bx.cast_backend_type(&cast_ty)\n-                ));\n-                bx.load(addr, self.fn_abi.ret.layout.align.abi)\n-            }\n-        };\n-        bx.ret(llval);\n-    }\n-\n-\n-    fn codegen_drop_terminator<'b>(\n-        &mut self,\n-        helper: TerminatorCodegenHelper<'b, 'tcx>,\n-        mut bx: Bx,\n-        location: &mir::Place<'tcx>,\n-        target: mir::BasicBlock,\n-        unwind: Option<mir::BasicBlock>,\n-    ) {\n-        let ty = location.ty(self.mir.body(), bx.tcx()).ty;\n-        let ty = self.monomorphize(&ty);\n-        let drop_fn = Instance::resolve_drop_in_place(bx.tcx(), ty);\n-\n-        if let ty::InstanceDef::DropGlue(_, None) = drop_fn.def {\n-            // we don't actually need to drop anything.\n-            helper.maybe_sideeffect(self.mir, &mut bx, &[target]);\n-            helper.funclet_br(self, &mut bx, target);\n-            return\n-        }\n-\n-        let place = self.codegen_place(&mut bx, &location.as_ref());\n-        let (args1, args2);\n-        let mut args = if let Some(llextra) = place.llextra {\n-            args2 = [place.llval, llextra];\n-            &args2[..]\n-        } else {\n-            args1 = [place.llval];\n-            &args1[..]\n-        };\n-        let (drop_fn, fn_abi) = match ty.kind {\n-            ty::Dynamic(..) => {\n-                let sig = drop_fn.fn_sig(self.cx.tcx());\n-                let sig = self.cx.tcx().normalize_erasing_late_bound_regions(\n-                    ty::ParamEnv::reveal_all(),\n-                    &sig,\n-                );\n-                let fn_abi = FnAbi::new_vtable(&bx, sig, &[]);\n-                let vtable = args[1];\n-                args = &args[..1];\n-                (meth::DESTRUCTOR.get_fn(&mut bx, vtable, &fn_abi), fn_abi)\n-            }\n-            _ => {\n-                (bx.get_fn_addr(drop_fn),\n-                 FnAbi::of_instance(&bx, drop_fn))\n-            }\n-        };\n-        helper.maybe_sideeffect(self.mir, &mut bx, &[target]);\n-        helper.do_call(self, &mut bx, fn_ty, drop_fn, args,\n-                       Some((ReturnDest::Nothing, target)),\n-                       unwind);\n-    }\n-\n-    fn codegen_assert_terminator<'b>(\n-        &mut self,\n-        helper: TerminatorCodegenHelper<'b, 'tcx>,\n-        mut bx: Bx,\n-        terminator: &mir::Terminator<'tcx>,\n-        cond: &mir::Operand<'tcx>,\n-        expected: bool,\n-        msg: &mir::AssertMessage<'tcx>,\n-        target: mir::BasicBlock,\n-        cleanup: Option<mir::BasicBlock>,\n-    ) {\n-        let span = terminator.source_info.span;\n-        let cond = self.codegen_operand(&mut bx, cond).immediate();\n-        let mut const_cond = bx.const_to_opt_u128(cond, false).map(|c| c == 1);\n-\n-        // This case can currently arise only from functions marked\n-        // with #[rustc_inherit_overflow_checks] and inlined from\n-        // another crate (mostly core::num generic/#[inline] fns),\n-        // while the current crate doesn't use overflow checks.\n-        // NOTE: Unlike binops, negation doesn't have its own\n-        // checked operation, just a comparison with the minimum\n-        // value, so we have to check for the assert message.\n-        if !bx.check_overflow() {\n-            if let PanicInfo::OverflowNeg = *msg {\n-                const_cond = Some(expected);\n-            }\n-        }\n-\n-        // Don't codegen the panic block if success if known.\n-        if const_cond == Some(expected) {\n-            helper.maybe_sideeffect(self.mir, &mut bx, &[target]);\n-            helper.funclet_br(self, &mut bx, target);\n-            return;\n-        }\n-\n-        // Pass the condition through llvm.expect for branch hinting.\n-        let cond = bx.expect(cond, expected);\n-\n-        // Create the failure block and the conditional branch to it.\n-        let lltarget = helper.llblock(self, target);\n-        let panic_block = self.new_block(\"panic\");\n-        helper.maybe_sideeffect(self.mir, &mut bx, &[target]);\n-        if expected {\n-            bx.cond_br(cond, lltarget, panic_block.llbb());\n-        } else {\n-            bx.cond_br(cond, panic_block.llbb(), lltarget);\n-        }\n-\n-        // After this point, bx is the block for the call to panic.\n-        bx = panic_block;\n-        self.set_debug_loc(&mut bx, terminator.source_info);\n-\n-        // Get the location information.\n-        let location = self.get_caller_location(&mut bx, span).immediate();\n-\n-        // Put together the arguments to the panic entry point.\n-        let (lang_item, args) = match msg {\n-            PanicInfo::BoundsCheck { ref len, ref index } => {\n-                let len = self.codegen_operand(&mut bx, len).immediate();\n-                let index = self.codegen_operand(&mut bx, index).immediate();\n-                (lang_items::PanicBoundsCheckFnLangItem, vec![location, index, len])\n-            }\n-            _ => {\n-                let msg_str = Symbol::intern(msg.description());\n-                let msg = bx.const_str(msg_str);\n-                (lang_items::PanicFnLangItem, vec![msg.0, msg.1, location])\n-            }\n-        };\n-\n-        // Obtain the panic entry point.\n-        let def_id = common::langcall(bx.tcx(), Some(span), \"\", lang_item);\n-        let instance = ty::Instance::mono(bx.tcx(), def_id);\n-        let fn_abi = FnAbi::of_instance(&bx, instance);\n-        let llfn = bx.get_fn_addr(instance);\n-\n-        // Codegen the actual panic invoke/call.\n-        helper.do_call(self, &mut bx, fn_abi, llfn, &args, None, cleanup);\n-    }\n-\n-    fn codegen_call_terminator<'b>(\n-        &mut self,\n-        helper: TerminatorCodegenHelper<'b, 'tcx>,\n-        mut bx: Bx,\n-        terminator: &mir::Terminator<'tcx>,\n-        func: &mir::Operand<'tcx>,\n-        args: &Vec<mir::Operand<'tcx>>,\n-        destination: &Option<(mir::Place<'tcx>, mir::BasicBlock)>,\n-        cleanup: Option<mir::BasicBlock>,\n-    ) {\n-        let span = terminator.source_info.span;\n-        // Create the callee. This is a fn ptr or zero-sized and hence a kind of scalar.\n-        let callee = self.codegen_operand(&mut bx, func);\n-\n-        let (instance, mut llfn) = match callee.layout.ty.kind {\n-            ty::FnDef(def_id, substs) => {\n-                (Some(ty::Instance::resolve(bx.tcx(),\n-                                            ty::ParamEnv::reveal_all(),\n-                                            def_id,\n-                                            substs).unwrap()),\n-                 None)\n-            }\n-            ty::FnPtr(_) => {\n-                (None, Some(callee.immediate()))\n-            }\n-            _ => bug!(\"{} is not callable\", callee.layout.ty),\n-        };\n-        let def = instance.map(|i| i.def);\n-        let sig = callee.layout.ty.fn_sig(bx.tcx());\n-        let sig = bx.tcx().normalize_erasing_late_bound_regions(\n-            ty::ParamEnv::reveal_all(),\n-            &sig,\n-        );\n-        let abi = sig.abi;\n-\n-        // Handle intrinsics old codegen wants Expr's for, ourselves.\n-        let intrinsic = match def {\n-            Some(ty::InstanceDef::Intrinsic(def_id)) =>\n-                Some(bx.tcx().item_name(def_id).as_str()),\n-            _ => None\n-        };\n-        let intrinsic = intrinsic.as_ref().map(|s| &s[..]);\n-\n-        if intrinsic == Some(\"transmute\") {\n-            if let Some(destination_ref) = destination.as_ref() {\n-                let &(ref dest, target) = destination_ref;\n-                self.codegen_transmute(&mut bx, &args[0], dest);\n-                helper.maybe_sideeffect(self.mir, &mut bx, &[target]);\n-                helper.funclet_br(self, &mut bx, target);\n-            } else {\n-                // If we are trying to transmute to an uninhabited type,\n-                // it is likely there is no allotted destination. In fact,\n-                // transmuting to an uninhabited type is UB, which means\n-                // we can do what we like. Here, we declare that transmuting\n-                // into an uninhabited type is impossible, so anything following\n-                // it must be unreachable.\n-                assert_eq!(bx.layout_of(sig.output()).abi, layout::Abi::Uninhabited);\n-                bx.unreachable();\n-            }\n-            return;\n-        }\n-\n-        let extra_args = &args[sig.inputs().len()..];\n-        let extra_args = extra_args.iter().map(|op_arg| {\n-            let op_ty = op_arg.ty(self.mir.body(), bx.tcx());\n-            self.monomorphize(&op_ty)\n-        }).collect::<Vec<_>>();\n-\n-        let fn_abi = match def {\n-            Some(ty::InstanceDef::Virtual(..)) => {\n-                FnAbi::new_vtable(&bx, sig, &extra_args)\n-            }\n-            Some(ty::InstanceDef::DropGlue(_, None)) => {\n-                // Empty drop glue; a no-op.\n-                let &(_, target) = destination.as_ref().unwrap();\n-                helper.maybe_sideeffect(self.mir, &mut bx, &[target]);\n-                helper.funclet_br(self, &mut bx, target);\n-                return;\n-            }\n-            _ => FnAbi::new(&bx, sig, &extra_args)\n-        };\n-\n-        // Emit a panic or a no-op for `panic_if_uninhabited`.\n-        if intrinsic == Some(\"panic_if_uninhabited\") {\n-            let ty = instance.unwrap().substs.type_at(0);\n-            let layout = bx.layout_of(ty);\n-            if layout.abi.is_uninhabited() {\n-                let msg_str = format!(\"Attempted to instantiate uninhabited type {}\", ty);\n-                let msg = bx.const_str(Symbol::intern(&msg_str));\n-                let location = self.get_caller_location(&mut bx, span).immediate();\n-\n-                // Obtain the panic entry point.\n-                let def_id =\n-                    common::langcall(bx.tcx(), Some(span), \"\", lang_items::PanicFnLangItem);\n-                let instance = ty::Instance::mono(bx.tcx(), def_id);\n-                let fn_abi = FnAbi::of_instance(&bx, instance);\n-                let llfn = bx.get_fn_addr(instance);\n-\n-                if let Some((_, target)) = destination.as_ref() {\n-                    helper.maybe_sideeffect(self.mir, &mut bx, &[*target]);\n-                }\n-                // Codegen the actual panic invoke/call.\n-                helper.do_call(\n-                    self,\n-                    &mut bx,\n-                    fn_abi,\n-                    llfn,\n-                    &[msg.0, msg.1, location],\n-                    destination.as_ref().map(|(_, bb)| (ReturnDest::Nothing, *bb)),\n-                    cleanup,\n-                );\n-            } else {\n-                // a NOP\n-                let target = destination.as_ref().unwrap().1;\n-                helper.maybe_sideeffect(self.mir, &mut bx, &[target]);\n-                helper.funclet_br(self, &mut bx, destination.as_ref().unwrap().1)\n-            }\n-            return;\n-        }\n-\n-        // The arguments we'll be passing. Plus one to account for outptr, if used.\n-        let arg_count = fn_abi.args.len() + fn_abi.ret.is_indirect() as usize;\n-        let mut llargs = Vec::with_capacity(arg_count);\n-\n-        // Prepare the return value destination\n-        let ret_dest = if let Some((ref dest, _)) = *destination {\n-            let is_intrinsic = intrinsic.is_some();\n-<<<<<<< HEAD\n-            self.make_return_dest(&mut bx, dest, &fn_abi.ret, &mut llargs, is_intrinsic)\n-=======\n-            self.make_return_dest(&mut bx, dest, &fn_ty.ret, &mut llargs,\n-                                  is_intrinsic)\n->>>>>>> Undo minor changes that weren't needed, fix one lifetime typo\n-        } else {\n-            ReturnDest::Nothing\n-        };\n-\n-        if intrinsic == Some(\"caller_location\") {\n-            if let Some((_, target)) = destination.as_ref() {\n-                let location = self.get_caller_location(&mut bx, span);\n-\n-                if let ReturnDest::IndirectOperand(tmp, _) = ret_dest {\n-                    location.val.store(&mut bx, tmp);\n-                }\n-                self.store_return(&mut bx, ret_dest, &fn_abi.ret, location.immediate());\n-\n-                helper.maybe_sideeffect(self.mir, &mut bx, &[*target]);\n-                helper.funclet_br(self, &mut bx, *target);\n-            }\n-            return;\n-        }\n-\n-        if intrinsic.is_some() && intrinsic != Some(\"drop_in_place\") {\n-            let dest = match ret_dest {\n-                _ if fn_abi.ret.is_indirect() => llargs[0],\n-                ReturnDest::Nothing =>\n-                    bx.const_undef(bx.type_ptr_to(bx.arg_memory_ty(&fn_abi.ret))),\n-                ReturnDest::IndirectOperand(dst, _) | ReturnDest::Store(dst) =>\n-                    dst.llval,\n-                ReturnDest::DirectOperand(_) =>\n-                    bug!(\"Cannot use direct operand with an intrinsic call\"),\n-            };\n-\n-            let args: Vec<_> = args.iter().enumerate().map(|(i, arg)| {\n-                // The indices passed to simd_shuffle* in the\n-                // third argument must be constant. This is\n-                // checked by const-qualification, which also\n-                // promotes any complex rvalues to constants.\n-                if i == 2 && intrinsic.unwrap().starts_with(\"simd_shuffle\") {\n-                    match arg {\n-                        // The shuffle array argument is usually not an explicit constant,\n-                        // but specified directly in the code. This means it gets promoted\n-                        // and we can then extract the value by evaluating the promoted.\n-                        mir::Operand::Copy(place) | mir::Operand::Move(place) => {\n-                            if let mir::PlaceRef {\n-                                base:\n-                                    &PlaceBase::Static(box Static {\n-                                        kind: StaticKind::Promoted(promoted, _),\n-                                        ty,\n-                                        def_id: _,\n-                                    }),\n-                                projection: &[],\n-                            } = place.as_ref()\n-                            {\n-                                let param_env = ty::ParamEnv::reveal_all();\n-                                let cid = mir::interpret::GlobalId {\n-                                    instance: self.instance,\n-                                    promoted: Some(promoted),\n-                                };\n-                                let c = bx.tcx().const_eval(param_env.and(cid));\n-                                let (llval, ty) = self.simd_shuffle_indices(\n-                                    &bx,\n-                                    terminator.source_info.span,\n-                                    ty,\n-                                    c,\n-                                );\n-                                return OperandRef {\n-                                    val: Immediate(llval),\n-                                    layout: bx.layout_of(ty),\n-                                };\n-                            } else {\n-                                span_bug!(span, \"shuffle indices must be constant\");\n-                            }\n-                        }\n-\n-                        mir::Operand::Constant(constant) => {\n-                            let c = self.eval_mir_constant(constant);\n-                            let (llval, ty) = self.simd_shuffle_indices(\n-                                &bx,\n-                                constant.span,\n-                                constant.literal.ty,\n-                                c,\n-                            );\n-                            return OperandRef {\n-                                val: Immediate(llval),\n-                                layout: bx.layout_of(ty)\n-                            };\n-                        }\n-                    }\n-                }\n-\n-                self.codegen_operand(&mut bx, arg)\n-            }).collect();\n-\n-\n-            bx.codegen_intrinsic_call(*instance.as_ref().unwrap(), &fn_abi, &args, dest,\n-                                      terminator.source_info.span);\n-\n-            if let ReturnDest::IndirectOperand(dst, _) = ret_dest {\n-                self.store_return(&mut bx, ret_dest, &fn_abi.ret, dst.llval);\n-            }\n-\n-            if let Some((_, target)) = *destination {\n-                helper.maybe_sideeffect(self.mir, &mut bx, &[target]);\n-                helper.funclet_br(self, &mut bx, target);\n-            } else {\n-                bx.unreachable();\n-            }\n-\n-            return;\n-        }\n-\n-        // Split the rust-call tupled arguments off.\n-        let (first_args, untuple) = if abi == Abi::RustCall && !args.is_empty() {\n-            let (tup, args) = args.split_last().unwrap();\n-            (args, Some(tup))\n-        } else {\n-            (&args[..], None)\n-        };\n-\n-        'make_args: for (i, arg) in first_args.iter().enumerate() {\n-            let mut op = self.codegen_operand(&mut bx, arg);\n-\n-            if let (0, Some(ty::InstanceDef::Virtual(_, idx))) = (i, def) {\n-                if let Pair(..) = op.val {\n-                    // In the case of Rc<Self>, we need to explicitly pass a\n-                    // *mut RcBox<Self> with a Scalar (not ScalarPair) ABI. This is a hack\n-                    // that is understood elsewhere in the compiler as a method on\n-                    // `dyn Trait`.\n-                    // To get a `*mut RcBox<Self>`, we just keep unwrapping newtypes until\n-                    // we get a value of a built-in pointer type\n-                    'descend_newtypes: while !op.layout.ty.is_unsafe_ptr()\n-                                    && !op.layout.ty.is_region_ptr()\n-                    {\n-                        'iter_fields: for i in 0..op.layout.fields.count() {\n-                            let field = op.extract_field(&mut bx, i);\n-                            if !field.layout.is_zst() {\n-                                // we found the one non-zero-sized field that is allowed\n-                                // now find *its* non-zero-sized field, or stop if it's a\n-                                // pointer\n-                                op = field;\n-                                continue 'descend_newtypes\n-                            }\n-                        }\n-\n-                        span_bug!(span, \"receiver has no non-zero-sized fields {:?}\", op);\n-                    }\n-\n-                    // now that we have `*dyn Trait` or `&dyn Trait`, split it up into its\n-                    // data pointer and vtable. Look up the method in the vtable, and pass\n-                    // the data pointer as the first argument\n-                    match op.val {\n-                        Pair(data_ptr, meta) => {\n-                            llfn = Some(meth::VirtualIndex::from_index(idx)\n-                                .get_fn(&mut bx, meta, &fn_abi));\n-                            llargs.push(data_ptr);\n-                            continue 'make_args\n-                        }\n-                        other => bug!(\"expected a Pair, got {:?}\", other),\n-                    }\n-                } else if let Ref(data_ptr, Some(meta), _) = op.val {\n-                    // by-value dynamic dispatch\n-                    llfn = Some(meth::VirtualIndex::from_index(idx)\n-                        .get_fn(&mut bx, meta, &fn_abi));\n-                    llargs.push(data_ptr);\n-                    continue;\n-                } else {\n-                    span_bug!(span, \"can't codegen a virtual call on {:?}\", op);\n-                }\n-            }\n-\n-            // The callee needs to own the argument memory if we pass it\n-            // by-ref, so make a local copy of non-immediate constants.\n-            match (arg, op.val) {\n-                (&mir::Operand::Copy(_), Ref(_, None, _)) |\n-                (&mir::Operand::Constant(_), Ref(_, None, _)) => {\n-                    let tmp = PlaceRef::alloca(&mut bx, op.layout);\n-                    op.val.store(&mut bx, tmp);\n-                    op.val = Ref(tmp.llval, None, tmp.align);\n-                }\n-                _ => {}\n-            }\n-\n-            self.codegen_argument(&mut bx, op, &mut llargs, &fn_abi.args[i]);\n-        }\n-        if let Some(tup) = untuple {\n-            self.codegen_arguments_untupled(&mut bx, tup, &mut llargs,\n-                &fn_abi.args[first_args.len()..])\n-        }\n-\n-        let fn_ptr = match (llfn, instance) {\n-            (Some(llfn), _) => llfn,\n-            (None, Some(instance)) => bx.get_fn_addr(instance),\n-            _ => span_bug!(span, \"no llfn for call\"),\n-        };\n-\n-        if let Some((_, target)) = destination.as_ref() {\n-            helper.maybe_sideeffect(self.mir, &mut bx, &[*target]);\n-        }\n-        helper.do_call(self, &mut bx, fn_ty, fn_ptr, &llargs,\n-                       destination.as_ref().map(|&(_, target)| (ret_dest, target)),\n-                       cleanup);\n-    }\n-}\n-\n-impl<'a, 'tcx, Bx: BuilderMethods<'a, 'tcx>> FunctionCx<'a, 'tcx, Bx> {\n-    pub fn codegen_block(\n-        &mut self,\n-        bb: mir::BasicBlock,\n-    ) {\n-        let mut bx = self.build_block(bb);\n-        let data = &self.mir.body()[bb];\n-\n-        debug!(\"codegen_block({:?}={:?})\", bb, data);\n-\n-        for statement in &data.statements {\n-            bx = self.codegen_statement(bx, statement);\n-        }\n-\n-        self.codegen_terminator(bx, bb, data.terminator());\n-    }\n-\n-    fn codegen_terminator(\n-        &mut self,\n-        mut bx: Bx,\n-        bb: mir::BasicBlock,\n-        terminator: &mir::Terminator<'tcx>\n-    ) {\n-        debug!(\"codegen_terminator: {:?}\", terminator);\n-\n-        // Create the cleanup bundle, if needed.\n-        let funclet_bb = self.cleanup_kinds[bb].funclet_bb(bb);\n-        let helper = TerminatorCodegenHelper {\n-            bb: &bb, terminator, funclet_bb\n-        };\n-\n-        self.set_debug_loc(&mut bx, terminator.source_info);\n-        match terminator.kind {\n-            mir::TerminatorKind::Resume => {\n-                self.codegen_resume_terminator(helper, bx)\n-            }\n-\n-            mir::TerminatorKind::Abort => {\n-                bx.abort();\n-                bx.unreachable();\n-            }\n-\n-            mir::TerminatorKind::Goto { target } => {\n-                helper.maybe_sideeffect(self.mir, &mut bx, &[target]);\n-                helper.funclet_br(self, &mut bx, target);\n-            }\n-\n-            mir::TerminatorKind::SwitchInt {\n-                ref discr, switch_ty, ref values, ref targets\n-            } => {\n-                self.codegen_switchint_terminator(helper, bx, discr, switch_ty,\n-                                                  values, targets);\n-            }\n-\n-            mir::TerminatorKind::Return => {\n-                self.codegen_return_terminator(bx);\n-            }\n-\n-            mir::TerminatorKind::Unreachable => {\n-                bx.unreachable();\n-            }\n-\n-            mir::TerminatorKind::Drop { ref location, target, unwind } => {\n-                self.codegen_drop_terminator(helper, bx, location, target, unwind);\n-            }\n-\n-            mir::TerminatorKind::Assert { ref cond, expected, ref msg, target, cleanup } => {\n-                self.codegen_assert_terminator(helper, bx, terminator, cond,\n-                                               expected, msg, target, cleanup);\n-            }\n-\n-            mir::TerminatorKind::DropAndReplace { .. } => {\n-                bug!(\"undesugared DropAndReplace in codegen: {:?}\", terminator);\n-            }\n-\n-            mir::TerminatorKind::Call {\n-                ref func,\n-                ref args,\n-                ref destination,\n-                cleanup,\n-                from_hir_call: _\n-            } => {\n-                self.codegen_call_terminator(helper, bx, terminator, func,\n-                                             args, destination, cleanup);\n-            }\n-            mir::TerminatorKind::GeneratorDrop |\n-            mir::TerminatorKind::Yield { .. } => bug!(\"generator ops in codegen\"),\n-            mir::TerminatorKind::FalseEdges { .. } |\n-            mir::TerminatorKind::FalseUnwind { .. } => bug!(\"borrowck false edges in codegen\"),\n-        }\n-    }\n-\n-    fn codegen_argument(\n-        &mut self,\n-        bx: &mut Bx,\n-        op: OperandRef<'tcx, Bx::Value>,\n-        llargs: &mut Vec<Bx::Value>,\n-        arg: &ArgAbi<'tcx, Ty<'tcx>>\n-    ) {\n-        // Fill padding with undef value, where applicable.\n-        if let Some(ty) = arg.pad {\n-            llargs.push(bx.const_undef(bx.reg_backend_type(&ty)))\n-        }\n-\n-        if arg.is_ignore() {\n-            return;\n-        }\n-\n-        if let PassMode::Pair(..) = arg.mode {\n-            match op.val {\n-                Pair(a, b) => {\n-                    llargs.push(a);\n-                    llargs.push(b);\n-                    return;\n-                }\n-                _ => bug!(\"codegen_argument: {:?} invalid for pair argument\", op)\n-            }\n-        } else if arg.is_unsized_indirect() {\n-            match op.val {\n-                Ref(a, Some(b), _) => {\n-                    llargs.push(a);\n-                    llargs.push(b);\n-                    return;\n-                }\n-                _ => bug!(\"codegen_argument: {:?} invalid for unsized indirect argument\", op)\n-            }\n-        }\n-\n-        // Force by-ref if we have to load through a cast pointer.\n-        let (mut llval, align, by_ref) = match op.val {\n-            Immediate(_) | Pair(..) => {\n-                match arg.mode {\n-                    PassMode::Indirect(..) | PassMode::Cast(_) => {\n-                        let scratch = PlaceRef::alloca(bx, arg.layout);\n-                        op.val.store(bx, scratch);\n-                        (scratch.llval, scratch.align, true)\n-                    }\n-                    _ => {\n-                        (op.immediate_or_packed_pair(bx), arg.layout.align.abi, false)\n-                    }\n-                }\n-            }\n-            Ref(llval, _, align) => {\n-                if arg.is_indirect() && align < arg.layout.align.abi {\n-                    // `foo(packed.large_field)`. We can't pass the (unaligned) field directly. I\n-                    // think that ATM (Rust 1.16) we only pass temporaries, but we shouldn't\n-                    // have scary latent bugs around.\n-\n-                    let scratch = PlaceRef::alloca(bx, arg.layout);\n-                    base::memcpy_ty(bx, scratch.llval, scratch.align, llval, align,\n-                                    op.layout, MemFlags::empty());\n-                    (scratch.llval, scratch.align, true)\n-                } else {\n-                    (llval, align, true)\n-                }\n-            }\n-        };\n-\n-        if by_ref && !arg.is_indirect() {\n-            // Have to load the argument, maybe while casting it.\n-            if let PassMode::Cast(ty) = arg.mode {\n-                let addr = bx.pointercast(llval, bx.type_ptr_to(\n-                    bx.cast_backend_type(&ty))\n-                );\n-                llval = bx.load(addr, align.min(arg.layout.align.abi));\n-            } else {\n-                // We can't use `PlaceRef::load` here because the argument\n-                // may have a type we don't treat as immediate, but the ABI\n-                // used for this call is passing it by-value. In that case,\n-                // the load would just produce `OperandValue::Ref` instead\n-                // of the `OperandValue::Immediate` we need for the call.\n-                llval = bx.load(llval, align);\n-                if let layout::Abi::Scalar(ref scalar) = arg.layout.abi {\n-                    if scalar.is_bool() {\n-                        bx.range_metadata(llval, 0..2);\n-                    }\n-                }\n-                // We store bools as `i8` so we need to truncate to `i1`.\n-                llval = base::to_immediate(bx, llval, arg.layout);\n-            }\n-        }\n-\n-        llargs.push(llval);\n-    }\n-\n-    fn codegen_arguments_untupled(\n-        &mut self,\n-        bx: &mut Bx,\n-        operand: &mir::Operand<'tcx>,\n-        llargs: &mut Vec<Bx::Value>,\n-        args: &[ArgAbi<'tcx, Ty<'tcx>>]\n-    ) {\n-        let tuple = self.codegen_operand(bx, operand);\n-\n-        // Handle both by-ref and immediate tuples.\n-        if let Ref(llval, None, align) = tuple.val {\n-            let tuple_ptr = PlaceRef::new_sized_aligned(llval, tuple.layout, align);\n-            for i in 0..tuple.layout.fields.count() {\n-                let field_ptr = tuple_ptr.project_field(bx, i);\n-                let field = bx.load_operand(field_ptr);\n-                self.codegen_argument(bx, field, llargs, &args[i]);\n-            }\n-        } else if let Ref(_, Some(_), _) = tuple.val {\n-            bug!(\"closure arguments must be sized\")\n-        } else {\n-            // If the tuple is immediate, the elements are as well.\n-            for i in 0..tuple.layout.fields.count() {\n-                let op = tuple.extract_field(bx, i);\n-                self.codegen_argument(bx, op, llargs, &args[i]);\n-            }\n-        }\n-    }\n-\n-    fn get_caller_location(\n-        &mut self,\n-        bx: &mut Bx,\n-        span: Span,\n-    ) -> OperandRef<'tcx, Bx::Value> {\n-        let topmost = span.ctxt().outer_expn().expansion_cause().unwrap_or(span);\n-        let caller = bx.tcx().sess.source_map().lookup_char_pos(topmost.lo());\n-        let const_loc = bx.tcx().const_caller_location((\n-            Symbol::intern(&caller.file.name.to_string()),\n-            caller.line as u32,\n-            caller.col_display as u32 + 1,\n-        ));\n-        OperandRef::from_const(bx, const_loc)\n-    }\n-\n-    fn get_personality_slot(\n-        &mut self,\n-        bx: &mut Bx\n-    ) -> PlaceRef<'tcx, Bx::Value> {\n-        let cx = bx.cx();\n-        if let Some(slot) = self.personality_slot {\n-            slot\n-        } else {\n-            let layout = cx.layout_of(cx.tcx().intern_tup(&[\n-                cx.tcx().mk_mut_ptr(cx.tcx().types.u8),\n-                cx.tcx().types.i32\n-            ]));\n-            let slot = PlaceRef::alloca(bx, layout);\n-            self.personality_slot = Some(slot);\n-            slot\n-        }\n-    }\n-\n-    /// Returns the landing-pad wrapper around the given basic block.\n-    ///\n-    /// No-op in MSVC SEH scheme.\n-    fn landing_pad_to(\n-        &mut self,\n-        target_bb: mir::BasicBlock\n-    ) -> Bx::BasicBlock {\n-        if let Some(block) = self.landing_pads[target_bb] {\n-            return block;\n-        }\n-\n-        let block = self.blocks[target_bb];\n-        let landing_pad = self.landing_pad_uncached(block);\n-        self.landing_pads[target_bb] = Some(landing_pad);\n-        landing_pad\n-    }\n-\n-    fn landing_pad_uncached(\n-        &mut self,\n-        target_bb: Bx::BasicBlock,\n-    ) -> Bx::BasicBlock {\n-        if base::wants_msvc_seh(self.cx.sess()) {\n-            span_bug!(self.mir.span, \"landing pad was not inserted?\")\n-        }\n-\n-        let mut bx = self.new_block(\"cleanup\");\n-\n-        let llpersonality = self.cx.eh_personality();\n-        let llretty = self.landing_pad_type();\n-        let lp = bx.landing_pad(llretty, llpersonality, 1);\n-        bx.set_cleanup(lp);\n-\n-        let slot = self.get_personality_slot(&mut bx);\n-        slot.storage_live(&mut bx);\n-        Pair(bx.extract_value(lp, 0), bx.extract_value(lp, 1)).store(&mut bx, slot);\n-\n-        bx.br(target_bb);\n-        bx.llbb()\n-    }\n-\n-    fn landing_pad_type(&self) -> Bx::Type {\n-        let cx = self.cx;\n-        cx.type_struct(&[cx.type_i8p(), cx.type_i32()], false)\n-    }\n-\n-    fn unreachable_block(\n-        &mut self\n-    ) -> Bx::BasicBlock {\n-        self.unreachable_block.unwrap_or_else(|| {\n-            let mut bx = self.new_block(\"unreachable\");\n-            bx.unreachable();\n-            self.unreachable_block = Some(bx.llbb());\n-            bx.llbb()\n-        })\n-    }\n-\n-    pub fn new_block(&self, name: &str) -> Bx {\n-        Bx::new_block(self.cx, self.llfn, name)\n-    }\n-\n-    pub fn build_block(\n-        &self,\n-        bb: mir::BasicBlock\n-    ) -> Bx {\n-        let mut bx = Bx::with_cx(self.cx);\n-        bx.position_at_end(self.blocks[bb]);\n-        bx\n-    }\n-\n-    fn make_return_dest(\n-        &mut self,\n-        bx: &mut Bx,\n-        dest: &mir::Place<'tcx>,\n-<<<<<<< HEAD\n-        fn_ret: &ArgAbi<'tcx, Ty<'tcx>>,\n-        llargs: &mut Vec<Bx::Value>, is_intrinsic: bool,\n-=======\n-        fn_ret: &ArgType<'tcx, Ty<'tcx>>,\n-        llargs: &mut Vec<Bx::Value>, is_intrinsic: bool\n->>>>>>> Undo minor changes that weren't needed, fix one lifetime typo\n-    ) -> ReturnDest<'tcx, Bx::Value> {\n-        // If the return is ignored, we can just return a do-nothing `ReturnDest`.\n-        if fn_ret.is_ignore() {\n-            return ReturnDest::Nothing;\n-        }\n-        let dest = if let Some(index) = dest.as_local() {\n-            match self.locals[index] {\n-                LocalRef::Place(dest) => dest,\n-                LocalRef::UnsizedPlace(_) => bug!(\"return type must be sized\"),\n-                LocalRef::Operand(None) => {\n-                    // Handle temporary places, specifically `Operand` ones, as\n-                    // they don't have `alloca`s.\n-                    return if fn_ret.is_indirect() {\n-                        // Odd, but possible, case, we have an operand temporary,\n-                        // but the calling convention has an indirect return.\n-                        let tmp = PlaceRef::alloca(bx, fn_ret.layout);\n-                        tmp.storage_live(bx);\n-                        llargs.push(tmp.llval);\n-                        ReturnDest::IndirectOperand(tmp, index)\n-                    } else if is_intrinsic {\n-                        // Currently, intrinsics always need a location to store\n-                        // the result, so we create a temporary `alloca` for the\n-                        // result.\n-                        let tmp = PlaceRef::alloca(bx, fn_ret.layout);\n-                        tmp.storage_live(bx);\n-                        ReturnDest::IndirectOperand(tmp, index)\n-                    } else {\n-                        ReturnDest::DirectOperand(index)\n-                    };\n-                }\n-                LocalRef::Operand(Some(_)) => {\n-                    bug!(\"place local already assigned to\");\n-                }\n-            }\n-        } else {\n-            self.codegen_place(bx, &mir::PlaceRef {\n-                base: &dest.base,\n-                projection: &dest.projection,\n-            })\n-        };\n-        if fn_ret.is_indirect() {\n-            if dest.align < dest.layout.align.abi {\n-                // Currently, MIR code generation does not create calls\n-                // that store directly to fields of packed structs (in\n-                // fact, the calls it creates write only to temps).\n-                //\n-                // If someone changes that, please update this code path\n-                // to create a temporary.\n-                span_bug!(self.mir.span, \"can't directly store to unaligned value\");\n-            }\n-            llargs.push(dest.llval);\n-            ReturnDest::Nothing\n-        } else {\n-            ReturnDest::Store(dest)\n-        }\n-    }\n-\n-    fn codegen_transmute(\n-        &mut self,\n-        bx: &mut Bx,\n-        src: &mir::Operand<'tcx>,\n-        dst: &mir::Place<'tcx>\n-    ) {\n-        if let Some(index) = dst.as_local() {\n-            match self.locals[index] {\n-                LocalRef::Place(place) => self.codegen_transmute_into(bx, src, place),\n-                LocalRef::UnsizedPlace(_) => bug!(\"transmute must not involve unsized locals\"),\n-                LocalRef::Operand(None) => {\n-                    let dst_layout = bx.layout_of(self.monomorphized_place_ty(&dst.as_ref()));\n-                    assert!(!dst_layout.ty.has_erasable_regions());\n-                    let place = PlaceRef::alloca(bx, dst_layout);\n-                    place.storage_live(bx);\n-                    self.codegen_transmute_into(bx, src, place);\n-                    let op = bx.load_operand(place);\n-                    place.storage_dead(bx);\n-                    self.locals[index] = LocalRef::Operand(Some(op));\n-                }\n-                LocalRef::Operand(Some(op)) => {\n-                    assert!(op.layout.is_zst(),\n-                            \"assigning to initialized SSAtemp\");\n-                }\n-            }\n-        } else {\n-            let dst = self.codegen_place(bx, &dst.as_ref());\n-            self.codegen_transmute_into(bx, src, dst);\n-        }\n-    }\n-\n-    fn codegen_transmute_into(\n-        &mut self,\n-        bx: &mut Bx,\n-        src: &mir::Operand<'tcx>,\n-        dst: PlaceRef<'tcx, Bx::Value>\n-    ) {\n-        let src = self.codegen_operand(bx, src);\n-        let llty = bx.backend_type(src.layout);\n-        let cast_ptr = bx.pointercast(dst.llval, bx.type_ptr_to(llty));\n-        let align = src.layout.align.abi.min(dst.align);\n-        src.val.store(bx, PlaceRef::new_sized_aligned(cast_ptr, src.layout, align));\n-    }\n-\n-\n-    // Stores the return value of a function call into it's final location.\n-    fn store_return(\n-        &mut self,\n-        bx: &mut Bx,\n-        dest: ReturnDest<'tcx, Bx::Value>,\n-        ret_abi: &ArgAbi<'tcx, Ty<'tcx>>,\n-        llval: Bx::Value\n-    ) {\n-        use self::ReturnDest::*;\n-\n-        match dest {\n-            Nothing => (),\n-            Store(dst) => bx.store_arg(&ret_abi, llval, dst),\n-            IndirectOperand(tmp, index) => {\n-                let op = bx.load_operand(tmp);\n-                tmp.storage_dead(bx);\n-                self.locals[index] = LocalRef::Operand(Some(op));\n-            }\n-            DirectOperand(index) => {\n-                // If there is a cast, we have to store and reload.\n-                let op = if let PassMode::Cast(_) = ret_abi.mode {\n-                    let tmp = PlaceRef::alloca(bx, ret_abi.layout);\n-                    tmp.storage_live(bx);\n-                    bx.store_arg(&ret_abi, llval, tmp);\n-                    let op = bx.load_operand(tmp);\n-                    tmp.storage_dead(bx);\n-                    op\n-                } else {\n-                    OperandRef::from_immediate_or_packed_pair(bx, llval, ret_abi.layout)\n-                };\n-                self.locals[index] = LocalRef::Operand(Some(op));\n-            }\n-        }\n-    }\n-}\n-\n-enum ReturnDest<'tcx, V> {\n-    // Do nothing; the return value is indirect or ignored.\n-    Nothing,\n-    // Store the return value to the pointer.\n-    Store(PlaceRef<'tcx, V>),\n-    // Store an indirect return value to an operand local place.\n-    IndirectOperand(PlaceRef<'tcx, V>, mir::Local),\n-    // Store a direct return value to an operand local place.\n-    DirectOperand(mir::Local)\n-}"}, {"sha": "83365f2ab222c26d135ebc942c779274aef48111", "filename": "src/librustc_codegen_ssa/mir/mod.rs.orig", "status": "removed", "additions": 0, "deletions": 439, "changes": 439, "blob_url": "https://github.com/rust-lang/rust/blob/51b06656da0106c254466429fa1f9d58bc74ea72/src%2Flibrustc_codegen_ssa%2Fmir%2Fmod.rs.orig", "raw_url": "https://github.com/rust-lang/rust/raw/51b06656da0106c254466429fa1f9d58bc74ea72/src%2Flibrustc_codegen_ssa%2Fmir%2Fmod.rs.orig", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_codegen_ssa%2Fmir%2Fmod.rs.orig?ref=51b06656da0106c254466429fa1f9d58bc74ea72", "patch": "@@ -1,439 +0,0 @@\n-use rustc::ty::{self, Ty, TypeFoldable, Instance};\n-<<<<<<< HEAD\n-use rustc::ty::layout::{TyLayout, HasTyCtxt, FnAbiExt};\n-use rustc::mir::{self, Body, BodyCache};\n-use rustc_target::abi::call::{FnAbi, PassMode};\n-=======\n-use rustc::ty::layout::{TyLayout, HasTyCtxt, FnTypeExt};\n-use rustc::mir::{self, Body, ReadOnlyBodyCache};\n-use rustc_target::abi::call::{FnType, PassMode};\n->>>>>>> Simplify BodyCache impl and fix all remaining type errors in librustc_mir (lifetime errors still exist)\n-use crate::base;\n-use crate::traits::*;\n-\n-use std::iter;\n-\n-use rustc_index::bit_set::BitSet;\n-use rustc_index::vec::IndexVec;\n-\n-use self::analyze::CleanupKind;\n-use self::debuginfo::FunctionDebugContext;\n-use self::place::PlaceRef;\n-use rustc::mir::traversal;\n-\n-use self::operand::{OperandRef, OperandValue};\n-\n-/// Master context for codegenning from MIR.\n-pub struct FunctionCx<'a, 'tcx, Bx: BuilderMethods<'a, 'tcx>> {\n-    instance: Instance<'tcx>,\n-\n-    mir: mir::ReadOnlyBodyCache<'a, 'tcx>,\n-\n-    debug_context: Option<FunctionDebugContext<Bx::DIScope>>,\n-\n-    llfn: Bx::Function,\n-\n-    cx: &'a Bx::CodegenCx,\n-\n-    fn_abi: FnAbi<'tcx, Ty<'tcx>>,\n-\n-    /// When unwinding is initiated, we have to store this personality\n-    /// value somewhere so that we can load it and re-use it in the\n-    /// resume instruction. The personality is (afaik) some kind of\n-    /// value used for C++ unwinding, which must filter by type: we\n-    /// don't really care about it very much. Anyway, this value\n-    /// contains an alloca into which the personality is stored and\n-    /// then later loaded when generating the DIVERGE_BLOCK.\n-    personality_slot: Option<PlaceRef<'tcx, Bx::Value>>,\n-\n-    /// A `Block` for each MIR `BasicBlock`\n-    blocks: IndexVec<mir::BasicBlock, Bx::BasicBlock>,\n-\n-    /// The funclet status of each basic block\n-    cleanup_kinds: IndexVec<mir::BasicBlock, analyze::CleanupKind>,\n-\n-    /// When targeting MSVC, this stores the cleanup info for each funclet\n-    /// BB. This is initialized as we compute the funclets' head block in RPO.\n-    funclets: IndexVec<mir::BasicBlock, Option<Bx::Funclet>>,\n-\n-    /// This stores the landing-pad block for a given BB, computed lazily on GNU\n-    /// and eagerly on MSVC.\n-    landing_pads: IndexVec<mir::BasicBlock, Option<Bx::BasicBlock>>,\n-\n-    /// Cached unreachable block\n-    unreachable_block: Option<Bx::BasicBlock>,\n-\n-    /// The location where each MIR arg/var/tmp/ret is stored. This is\n-    /// usually an `PlaceRef` representing an alloca, but not always:\n-    /// sometimes we can skip the alloca and just store the value\n-    /// directly using an `OperandRef`, which makes for tighter LLVM\n-    /// IR. The conditions for using an `OperandRef` are as follows:\n-    ///\n-    /// - the type of the local must be judged \"immediate\" by `is_llvm_immediate`\n-    /// - the operand must never be referenced indirectly\n-    ///     - we should not take its address using the `&` operator\n-    ///     - nor should it appear in a place path like `tmp.a`\n-    /// - the operand must be defined by an rvalue that can generate immediate\n-    ///   values\n-    ///\n-    /// Avoiding allocs can also be important for certain intrinsics,\n-    /// notably `expect`.\n-    locals: IndexVec<mir::Local, LocalRef<'tcx, Bx::Value>>,\n-\n-    per_local_var_debug_info: Option<IndexVec<mir::Local, Vec<debuginfo::VarDebugInfo<'tcx>>>>,\n-}\n-\n-impl<'a, 'tcx, Bx: BuilderMethods<'a, 'tcx>> FunctionCx<'a, 'tcx, Bx> {\n-    pub fn monomorphize<T>(&self, value: &T) -> T\n-        where T: TypeFoldable<'tcx>\n-    {\n-        self.cx.tcx().subst_and_normalize_erasing_regions(\n-            self.instance.substs,\n-            ty::ParamEnv::reveal_all(),\n-            value,\n-        )\n-    }\n-}\n-\n-enum LocalRef<'tcx, V> {\n-    Place(PlaceRef<'tcx, V>),\n-    /// `UnsizedPlace(p)`: `p` itself is a thin pointer (indirect place).\n-    /// `*p` is the fat pointer that references the actual unsized place.\n-    /// Every time it is initialized, we have to reallocate the place\n-    /// and update the fat pointer. That's the reason why it is indirect.\n-    UnsizedPlace(PlaceRef<'tcx, V>),\n-    Operand(Option<OperandRef<'tcx, V>>),\n-}\n-\n-impl<'a, 'tcx, V: CodegenObject> LocalRef<'tcx, V> {\n-    fn new_operand<Bx: BuilderMethods<'a, 'tcx, Value = V>>(\n-        bx: &mut Bx,\n-        layout: TyLayout<'tcx>,\n-    ) -> LocalRef<'tcx, V> {\n-        if layout.is_zst() {\n-            // Zero-size temporaries aren't always initialized, which\n-            // doesn't matter because they don't contain data, but\n-            // we need something in the operand.\n-            LocalRef::Operand(Some(OperandRef::new_zst(bx, layout)))\n-        } else {\n-            LocalRef::Operand(None)\n-        }\n-    }\n-}\n-\n-///////////////////////////////////////////////////////////////////////////\n-\n-pub fn codegen_mir<'a, 'tcx, Bx: BuilderMethods<'a, 'tcx>>(\n-    cx: &'a Bx::CodegenCx,\n-    llfn: Bx::Function,\n-    mir: ReadOnlyBodyCache<'a, 'tcx>,\n-    instance: Instance<'tcx>,\n-    sig: ty::FnSig<'tcx>,\n-) {\n-    assert!(!instance.substs.needs_infer());\n-\n-    let fn_abi = FnAbi::new(cx, sig, &[]);\n-    debug!(\"fn_abi: {:?}\", fn_abi);\n-\n-    let debug_context =\n-        cx.create_function_debug_context(instance, sig, llfn, &mir);\n-\n-    let mut bx = Bx::new_block(cx, llfn, \"start\");\n-\n-    if mir.basic_blocks().iter().any(|bb| bb.is_cleanup) {\n-        bx.set_personality_fn(cx.eh_personality());\n-    }\n-\n-    bx.sideeffect();\n-\n-    let cleanup_kinds = analyze::cleanup_kinds(&mir);\n-    // Allocate a `Block` for every basic block, except\n-    // the start block, if nothing loops back to it.\n-    let reentrant_start_block = !mir.predecessors_for(mir::START_BLOCK).is_empty();\n-    let block_bxs: IndexVec<mir::BasicBlock, Bx::BasicBlock> =\n-        mir.basic_blocks().indices().map(|bb| {\n-            if bb == mir::START_BLOCK && !reentrant_start_block {\n-                bx.llbb()\n-            } else {\n-                bx.build_sibling_block(&format!(\"{:?}\", bb)).llbb()\n-            }\n-        }).collect();\n-\n-    let (landing_pads, funclets) = create_funclets(&mir, &mut bx, &cleanup_kinds, &block_bxs);\n-    let mir_body = mir.body();\n-    let mut fx = FunctionCx {\n-        instance,\n-        mir,\n-        llfn,\n-        fn_abi,\n-        cx,\n-        personality_slot: None,\n-        blocks: block_bxs,\n-        unreachable_block: None,\n-        cleanup_kinds,\n-        landing_pads,\n-        funclets,\n-        locals: IndexVec::new(),\n-        debug_context,\n-        per_local_var_debug_info: debuginfo::per_local_var_debug_info(cx.tcx(), mir),\n-    };\n-\n-    let memory_locals = analyze::non_ssa_locals(&fx);\n-\n-    // Allocate variable and temp allocas\n-    fx.locals = {\n-        let args = arg_local_refs(&mut bx, &fx, &memory_locals);\n-\n-        let mut allocate_local = |local| {\n-            let decl = &mir_body.local_decls[local];\n-            let layout = bx.layout_of(fx.monomorphize(&decl.ty));\n-            assert!(!layout.ty.has_erasable_regions());\n-\n-            if local == mir::RETURN_PLACE && fx.fn_abi.ret.is_indirect() {\n-                debug!(\"alloc: {:?} (return place) -> place\", local);\n-                let llretptr = bx.get_param(0);\n-                return LocalRef::Place(PlaceRef::new_sized(llretptr, layout));\n-            }\n-\n-            if memory_locals.contains(local) {\n-                debug!(\"alloc: {:?} -> place\", local);\n-                if layout.is_unsized() {\n-                    LocalRef::UnsizedPlace(PlaceRef::alloca_unsized_indirect(&mut bx, layout))\n-                } else {\n-                    LocalRef::Place(PlaceRef::alloca(&mut bx, layout))\n-                }\n-            } else {\n-                debug!(\"alloc: {:?} -> operand\", local);\n-                LocalRef::new_operand(&mut bx, layout)\n-            }\n-        };\n-\n-        let retptr = allocate_local(mir::RETURN_PLACE);\n-        iter::once(retptr)\n-            .chain(args.into_iter())\n-            .chain(mir_body.vars_and_temps_iter().map(allocate_local))\n-            .collect()\n-    };\n-\n-    // Apply debuginfo to the newly allocated locals.\n-    fx.debug_introduce_locals(&mut bx);\n-\n-    // Branch to the START block, if it's not the entry block.\n-    if reentrant_start_block {\n-        bx.br(fx.blocks[mir::START_BLOCK]);\n-    }\n-\n-    // Up until here, IR instructions for this function have explicitly not been annotated with\n-    // source code location, so we don't step into call setup code. From here on, source location\n-    // emitting should be enabled.\n-    if let Some(debug_context) = &mut fx.debug_context {\n-        debug_context.source_locations_enabled = true;\n-    }\n-\n-    let rpo = traversal::reverse_postorder(&mir_body);\n-    let mut visited = BitSet::new_empty(mir_body.basic_blocks().len());\n-\n-    // Codegen the body of each block using reverse postorder\n-    for (bb, _) in rpo {\n-        visited.insert(bb.index());\n-        fx.codegen_block(bb);\n-    }\n-\n-    // Remove blocks that haven't been visited, or have no\n-    // predecessors.\n-    for bb in mir_body.basic_blocks().indices() {\n-        // Unreachable block\n-        if !visited.contains(bb.index()) {\n-            debug!(\"codegen_mir: block {:?} was not visited\", bb);\n-            unsafe {\n-                bx.delete_basic_block(fx.blocks[bb]);\n-            }\n-        }\n-    }\n-}\n-\n-fn create_funclets<'a, 'b, 'tcx, Bx: BuilderMethods<'a, 'tcx>>(\n-    mir: &'b Body<'tcx>,\n-    bx: &mut Bx,\n-    cleanup_kinds: &IndexVec<mir::BasicBlock, CleanupKind>,\n-    block_bxs: &IndexVec<mir::BasicBlock, Bx::BasicBlock>,\n-) -> (\n-    IndexVec<mir::BasicBlock, Option<Bx::BasicBlock>>,\n-    IndexVec<mir::BasicBlock, Option<Bx::Funclet>>,\n-) {\n-    block_bxs.iter_enumerated().zip(cleanup_kinds).map(|((bb, &llbb), cleanup_kind)| {\n-        match *cleanup_kind {\n-            CleanupKind::Funclet if base::wants_msvc_seh(bx.sess()) => {}\n-            _ => return (None, None)\n-        }\n-\n-        let funclet;\n-        let ret_llbb;\n-        match mir[bb].terminator.as_ref().map(|t| &t.kind) {\n-            // This is a basic block that we're aborting the program for,\n-            // notably in an `extern` function. These basic blocks are inserted\n-            // so that we assert that `extern` functions do indeed not panic,\n-            // and if they do we abort the process.\n-            //\n-            // On MSVC these are tricky though (where we're doing funclets). If\n-            // we were to do a cleanuppad (like below) the normal functions like\n-            // `longjmp` would trigger the abort logic, terminating the\n-            // program. Instead we insert the equivalent of `catch(...)` for C++\n-            // which magically doesn't trigger when `longjmp` files over this\n-            // frame.\n-            //\n-            // Lots more discussion can be found on #48251 but this codegen is\n-            // modeled after clang's for:\n-            //\n-            //      try {\n-            //          foo();\n-            //      } catch (...) {\n-            //          bar();\n-            //      }\n-            Some(&mir::TerminatorKind::Abort) => {\n-                let mut cs_bx = bx.build_sibling_block(&format!(\"cs_funclet{:?}\", bb));\n-                let mut cp_bx = bx.build_sibling_block(&format!(\"cp_funclet{:?}\", bb));\n-                ret_llbb = cs_bx.llbb();\n-\n-                let cs = cs_bx.catch_switch(None, None, 1);\n-                cs_bx.add_handler(cs, cp_bx.llbb());\n-\n-                // The \"null\" here is actually a RTTI type descriptor for the\n-                // C++ personality function, but `catch (...)` has no type so\n-                // it's null. The 64 here is actually a bitfield which\n-                // represents that this is a catch-all block.\n-                let null = bx.const_null(bx.type_i8p());\n-                let sixty_four = bx.const_i32(64);\n-                funclet = cp_bx.catch_pad(cs, &[null, sixty_four, null]);\n-                cp_bx.br(llbb);\n-            }\n-            _ => {\n-                let mut cleanup_bx = bx.build_sibling_block(&format!(\"funclet_{:?}\", bb));\n-                ret_llbb = cleanup_bx.llbb();\n-                funclet = cleanup_bx.cleanup_pad(None, &[]);\n-                cleanup_bx.br(llbb);\n-            }\n-        };\n-\n-        (Some(ret_llbb), Some(funclet))\n-    }).unzip()\n-}\n-\n-/// Produces, for each argument, a `Value` pointing at the\n-/// argument's value. As arguments are places, these are always\n-/// indirect.\n-fn arg_local_refs<'a, 'tcx, Bx: BuilderMethods<'a, 'tcx>>(\n-    bx: &mut Bx,\n-    fx: &FunctionCx<'a, 'tcx, Bx>,\n-    memory_locals: &BitSet<mir::Local>,\n-) -> Vec<LocalRef<'tcx, Bx::Value>> {\n-    let mut idx = 0;\n-    let mut llarg_idx = fx.fn_abi.ret.is_indirect() as usize;\n-\n-    fx.mir.args_iter().enumerate().map(|(arg_index, local)| {\n-        let arg_decl = &fx.mir.local_decls[local];\n-\n-        if Some(local) == fx.mir.spread_arg {\n-            // This argument (e.g., the last argument in the \"rust-call\" ABI)\n-            // is a tuple that was spread at the ABI level and now we have\n-            // to reconstruct it into a tuple local variable, from multiple\n-            // individual LLVM function arguments.\n-\n-            let arg_ty = fx.monomorphize(&arg_decl.ty);\n-            let tupled_arg_tys = match arg_ty.kind {\n-                ty::Tuple(ref tys) => tys,\n-                _ => bug!(\"spread argument isn't a tuple?!\")\n-            };\n-\n-            let place = PlaceRef::alloca(bx, bx.layout_of(arg_ty));\n-            for i in 0..tupled_arg_tys.len() {\n-                let arg = &fx.fn_abi.args[idx];\n-                idx += 1;\n-                if arg.pad.is_some() {\n-                    llarg_idx += 1;\n-                }\n-                let pr_field = place.project_field(bx, i);\n-                bx.store_fn_arg(arg, &mut llarg_idx, pr_field);\n-            }\n-\n-            return LocalRef::Place(place);\n-        }\n-\n-        if fx.fn_abi.c_variadic && arg_index == fx.fn_abi.args.len() {\n-            let arg_ty = fx.monomorphize(&arg_decl.ty);\n-\n-            let va_list = PlaceRef::alloca(bx, bx.layout_of(arg_ty));\n-            bx.va_start(va_list.llval);\n-\n-            return LocalRef::Place(va_list);\n-        }\n-\n-        let arg = &fx.fn_abi.args[idx];\n-        idx += 1;\n-        if arg.pad.is_some() {\n-            llarg_idx += 1;\n-        }\n-\n-        if !memory_locals.contains(local) {\n-            // We don't have to cast or keep the argument in the alloca.\n-            // FIXME(eddyb): We should figure out how to use llvm.dbg.value instead\n-            // of putting everything in allocas just so we can use llvm.dbg.declare.\n-            let local = |op| LocalRef::Operand(Some(op));\n-            match arg.mode {\n-                PassMode::Ignore => {\n-                    return local(OperandRef::new_zst(bx, arg.layout));\n-                }\n-                PassMode::Direct(_) => {\n-                    let llarg = bx.get_param(llarg_idx);\n-                    llarg_idx += 1;\n-                    return local(\n-                        OperandRef::from_immediate_or_packed_pair(bx, llarg, arg.layout));\n-                }\n-                PassMode::Pair(..) => {\n-                    let (a, b) = (bx.get_param(llarg_idx), bx.get_param(llarg_idx + 1));\n-                    llarg_idx += 2;\n-\n-                    return local(OperandRef {\n-                        val: OperandValue::Pair(a, b),\n-                        layout: arg.layout\n-                    });\n-                }\n-                _ => {}\n-            }\n-        }\n-\n-        if arg.is_sized_indirect() {\n-            // Don't copy an indirect argument to an alloca, the caller\n-            // already put it in a temporary alloca and gave it up.\n-            // FIXME: lifetimes\n-            let llarg = bx.get_param(llarg_idx);\n-            llarg_idx += 1;\n-            LocalRef::Place(PlaceRef::new_sized(llarg, arg.layout))\n-        } else if arg.is_unsized_indirect() {\n-            // As the storage for the indirect argument lives during\n-            // the whole function call, we just copy the fat pointer.\n-            let llarg = bx.get_param(llarg_idx);\n-            llarg_idx += 1;\n-            let llextra = bx.get_param(llarg_idx);\n-            llarg_idx += 1;\n-            let indirect_operand = OperandValue::Pair(llarg, llextra);\n-\n-            let tmp = PlaceRef::alloca_unsized_indirect(bx, arg.layout);\n-            indirect_operand.store(bx, tmp);\n-            LocalRef::UnsizedPlace(tmp)\n-        } else {\n-            let tmp = PlaceRef::alloca(bx, arg.layout);\n-            bx.store_fn_arg(arg, &mut llarg_idx, tmp);\n-            LocalRef::Place(tmp)\n-        }\n-    }).collect()\n-}\n-\n-mod analyze;\n-mod block;\n-pub mod constant;\n-pub mod debuginfo;\n-pub mod place;\n-pub mod operand;\n-mod rvalue;\n-mod statement;"}]}