{"sha": "2c9c978e1d4a3541d8df593346c7520c8ef4d69e", "node_id": "MDY6Q29tbWl0NzI0NzEyOjJjOWM5NzhlMWQ0YTM1NDFkOGRmNTkzMzQ2Yzc1MjBjOGVmNGQ2OWU=", "commit": {"author": {"name": "Pawe\u0142 Romanowski", "email": "pawroman@gmail.com", "date": "2019-04-19T09:42:08Z"}, "committer": {"name": "Pawe\u0142 Romanowski", "email": "pawroman@gmail.com", "date": "2019-04-19T09:42:08Z"}, "message": "Refactor and document unicode.py script", "tree": {"sha": "f263259d77fb9fadfa2554b872bf64eed1b740d8", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/f263259d77fb9fadfa2554b872bf64eed1b740d8"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/2c9c978e1d4a3541d8df593346c7520c8ef4d69e", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/2c9c978e1d4a3541d8df593346c7520c8ef4d69e", "html_url": "https://github.com/rust-lang/rust/commit/2c9c978e1d4a3541d8df593346c7520c8ef4d69e", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/2c9c978e1d4a3541d8df593346c7520c8ef4d69e/comments", "author": {"login": "pawroman", "id": 914977, "node_id": "MDQ6VXNlcjkxNDk3Nw==", "avatar_url": "https://avatars.githubusercontent.com/u/914977?v=4", "gravatar_id": "", "url": "https://api.github.com/users/pawroman", "html_url": "https://github.com/pawroman", "followers_url": "https://api.github.com/users/pawroman/followers", "following_url": "https://api.github.com/users/pawroman/following{/other_user}", "gists_url": "https://api.github.com/users/pawroman/gists{/gist_id}", "starred_url": "https://api.github.com/users/pawroman/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/pawroman/subscriptions", "organizations_url": "https://api.github.com/users/pawroman/orgs", "repos_url": "https://api.github.com/users/pawroman/repos", "events_url": "https://api.github.com/users/pawroman/events{/privacy}", "received_events_url": "https://api.github.com/users/pawroman/received_events", "type": "User", "site_admin": false}, "committer": {"login": "pawroman", "id": 914977, "node_id": "MDQ6VXNlcjkxNDk3Nw==", "avatar_url": "https://avatars.githubusercontent.com/u/914977?v=4", "gravatar_id": "", "url": "https://api.github.com/users/pawroman", "html_url": "https://github.com/pawroman", "followers_url": "https://api.github.com/users/pawroman/followers", "following_url": "https://api.github.com/users/pawroman/following{/other_user}", "gists_url": "https://api.github.com/users/pawroman/gists{/gist_id}", "starred_url": "https://api.github.com/users/pawroman/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/pawroman/subscriptions", "organizations_url": "https://api.github.com/users/pawroman/orgs", "repos_url": "https://api.github.com/users/pawroman/repos", "events_url": "https://api.github.com/users/pawroman/events{/privacy}", "received_events_url": "https://api.github.com/users/pawroman/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "edbc27da2dc5a75f8e0ac70e7a5e07aa6f6f0a16", "url": "https://api.github.com/repos/rust-lang/rust/commits/edbc27da2dc5a75f8e0ac70e7a5e07aa6f6f0a16", "html_url": "https://github.com/rust-lang/rust/commit/edbc27da2dc5a75f8e0ac70e7a5e07aa6f6f0a16"}], "stats": {"total": 820, "additions": 518, "deletions": 302}, "files": [{"sha": "f66e82299100dcbd100fb5d50a4981612f2a41f3", "filename": "src/libcore/unicode/unicode.py", "status": "modified", "additions": 518, "deletions": 302, "changes": 820, "blob_url": "https://github.com/rust-lang/rust/blob/2c9c978e1d4a3541d8df593346c7520c8ef4d69e/src%2Flibcore%2Funicode%2Funicode.py", "raw_url": "https://github.com/rust-lang/rust/raw/2c9c978e1d4a3541d8df593346c7520c8ef4d69e/src%2Flibcore%2Funicode%2Funicode.py", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibcore%2Funicode%2Funicode.py?ref=2c9c978e1d4a3541d8df593346c7520c8ef4d69e", "patch": "@@ -16,13 +16,31 @@\n import argparse\n import datetime\n import fileinput\n-import operator\n+import itertools\n import os\n import re\n import textwrap\n import subprocess\n \n-from collections import namedtuple\n+from collections import defaultdict, namedtuple\n+\n+try:\n+    # Python 3\n+    from itertools import zip_longest\n+    from io import StringIO\n+except ImportError:\n+    # Python 2 compatibility\n+    zip_longest = itertools.izip_longest\n+    from StringIO import StringIO\n+\n+try:\n+    # completely optional type hinting\n+    # (Python 2 compatible using comments,\n+    #  see: https://mypy.readthedocs.io/en/latest/python2.html)\n+    # This is very helpful in typing-aware IDE like PyCharm.\n+    from typing import Any, Callable, Dict, Iterable, Iterator, List, Optional, Set, Tuple\n+except ImportError:\n+    pass\n \n \n # we don't use enum.Enum because of Python 2.7 compatibility\n@@ -77,12 +95,21 @@ class UnicodeFiles(object):\n     \"Cc\": [\"C\"], \"Cf\": [\"C\"], \"Cs\": [\"C\"], \"Co\": [\"C\"], \"Cn\": [\"C\"],\n }\n \n-# these are the surrogate codepoints, which are not valid rust characters\n-SURROGATE_CODEPOINTS = (0xd800, 0xdfff)\n+# this is the surrogate codepoints range (both ends inclusive)\n+# - they are not valid Rust characters\n+SURROGATE_CODEPOINTS_RANGE = (0xd800, 0xdfff)\n \n UnicodeData = namedtuple(\n-    \"UnicodeData\", (\"canon_decomp\", \"compat_decomp\", \"gencats\", \"combines\",\n-                    \"to_upper\", \"to_lower\", \"to_title\", )\n+    \"UnicodeData\", (\n+        # conversions:\n+        \"to_upper\", \"to_lower\", \"to_title\",\n+\n+        # decompositions: canonical decompositions, compatibility decomp\n+        \"canon_decomp\", \"compat_decomp\",\n+\n+        # grouped: general categories and combining characters\n+        \"general_categories\", \"combines\",\n+    )\n )\n \n UnicodeVersion = namedtuple(\n@@ -91,14 +118,19 @@ class UnicodeFiles(object):\n \n \n def fetch_files(version=None):\n+    # type: (str) -> UnicodeVersion\n     \"\"\"\n-    Fetch all the Unicode files from unicode.org\n+    Fetch all the Unicode files from unicode.org.\n+\n+    This will use cached files (stored in FETCH_DIR) if they exist,\n+    creating them if they don't.  In any case, the Unicode version\n+    is always returned.\n \n     :param version: The desired Unicode version, as string.\n-        (If None, defaults to latest final release available).\n-    :return: The version downloaded (UnicodeVersion object).\n+        (If None, defaults to latest final release available,\n+         querying the unicode.org service).\n     \"\"\"\n-    have_version = should_skip_fetch(version)\n+    have_version = check_stored_version(version)\n     if have_version:\n         return have_version\n \n@@ -114,47 +146,60 @@ def fetch_files(version=None):\n     print(\"Fetching: {}\".format(readme_url))\n     readme_content = subprocess.check_output((\"curl\", readme_url))\n \n-    unicode_version = parse_unicode_version(\n+    unicode_version = parse_readme_unicode_version(\n         readme_content.decode(\"utf8\")\n     )\n \n-    download_dir = os.path.join(FETCH_DIR, unicode_version.as_str)\n+    download_dir = get_unicode_dir(unicode_version)\n     if not os.path.exists(download_dir):\n         # for 2.7 compat, we don't use exist_ok=True\n         os.makedirs(download_dir)\n \n     for filename in UnicodeFiles.ALL_FILES:\n-        file_path = os.path.join(download_dir, filename)\n+        file_path = get_unicode_file_path(unicode_version, filename)\n+\n+        if os.path.exists(file_path):\n+            # assume file on the server didn't change if it's been saved before\n+            continue\n \n         if filename == UnicodeFiles.README:\n             with open(file_path, \"wb\") as fd:\n                 fd.write(readme_content)\n-        elif not os.path.exists(file_path):\n+        else:\n             url = get_fetch_url(filename)\n             print(\"Fetching: {}\".format(url))\n             subprocess.check_call((\"curl\", \"-o\", file_path, url))\n \n     return unicode_version\n \n \n-def should_skip_fetch(version):\n+def check_stored_version(version):\n+    # type: (Optional[str]) -> Optional[UnicodeVersion]\n+    \"\"\"\n+    Given desired Unicode version, return the version\n+    if stored files are all present, and None otherwise.\n+    \"\"\"\n     if not version:\n         # should always check latest version\n-        return False\n+        return None\n \n     fetch_dir = os.path.join(FETCH_DIR, version)\n \n     for filename in UnicodeFiles.ALL_FILES:\n         file_path = os.path.join(fetch_dir, filename)\n \n         if not os.path.exists(file_path):\n-            return False\n+            return None\n \n     with open(os.path.join(fetch_dir, UnicodeFiles.README)) as fd:\n-        return parse_unicode_version(fd.read())\n+        return parse_readme_unicode_version(fd.read())\n \n \n-def parse_unicode_version(readme_content):\n+def parse_readme_unicode_version(readme_content):\n+    # type: (str) -> UnicodeVersion\n+    \"\"\"\n+    Parse the Unicode version contained in their ReadMe.txt file.\n+    \"\"\"\n     # \"raw string\" is necessary for \\d not being treated as escape char\n     # (for the sake of compat with future Python versions)\n     # see: https://docs.python.org/3.6/whatsnew/3.6.html#deprecated-python-behavior\n@@ -164,45 +209,78 @@ def parse_unicode_version(readme_content):\n     return UnicodeVersion(*map(int, groups), as_str=\".\".join(groups))\n \n \n+def get_unicode_dir(unicode_version):\n+    # type: (UnicodeVersion) -> str\n+    \"\"\"\n+    Indicate where the unicode data files should be stored.\n+\n+    This returns a full, absolute path.\n+    \"\"\"\n+    return os.path.join(FETCH_DIR, unicode_version.as_str)\n+\n+\n def get_unicode_file_path(unicode_version, filename):\n-    return os.path.join(FETCH_DIR, unicode_version.as_str, filename)\n+    # type: (UnicodeVersion, str) -> str\n+    \"\"\"\n+    Indicate where the unicode data file should be stored.\n+    \"\"\"\n+    return os.path.join(get_unicode_dir(unicode_version), filename)\n \n \n def is_surrogate(n):\n-    return SURROGATE_CODEPOINTS[0] <= n <= SURROGATE_CODEPOINTS[1]\n+    # type: (int) -> bool\n+    \"\"\"\n+    Tell if given codepoint is a surrogate (not a valid Rust character).\n+    \"\"\"\n+    return SURROGATE_CODEPOINTS_RANGE[0] <= n <= SURROGATE_CODEPOINTS_RANGE[1]\n \n \n def load_unicode_data(file_path):\n-    gencats = {}\n-    to_lower = {}\n-    to_upper = {}\n-    to_title = {}\n-    combines = {}\n-    canon_decomp = {}\n-    compat_decomp = {}\n-\n-    udict = {}\n+    # type: (str) -> UnicodeData\n+    \"\"\"\n+    Load main unicode data.\n+    \"\"\"\n+    # conversions\n+    to_lower = {}   # type: Dict[int, Tuple[int, int, int]]\n+    to_upper = {}   # type: Dict[int, Tuple[int, int, int]]\n+    to_title = {}   # type: Dict[int, Tuple[int, int, int]]\n+\n+    # decompositions\n+    compat_decomp = {}   # type: Dict[int, List[int]]\n+    canon_decomp = {}    # type: Dict[int, List[int]]\n+\n+    # combining characters\n+    # FIXME: combines are not used\n+    combines = defaultdict(set)   # type: Dict[str, Set[int]]\n+\n+    # categories\n+    general_categories = defaultdict(set)   # type: Dict[str, Set[int]]\n+    category_assigned_codepoints = set()    # type: Set[int]\n+\n+    all_codepoints = {}\n+\n     range_start = -1\n+\n     for line in fileinput.input(file_path):\n         data = line.split(\";\")\n         if len(data) != 15:\n             continue\n-        cp = int(data[0], 16)\n-        if is_surrogate(cp):\n+        codepoint = int(data[0], 16)\n+        if is_surrogate(codepoint):\n             continue\n         if range_start >= 0:\n-            for i in range(range_start, cp):\n-                udict[i] = data\n+            for i in range(range_start, codepoint):\n+                all_codepoints[i] = data\n             range_start = -1\n         if data[1].endswith(\", First>\"):\n-            range_start = cp\n+            range_start = codepoint\n             continue\n-        udict[cp] = data\n+        all_codepoints[codepoint] = data\n \n-    for code in udict:\n+    for code, data in all_codepoints.items():\n         (code_org, name, gencat, combine, bidi,\n          decomp, deci, digit, num, mirror,\n-         old, iso, upcase, lowcase, titlecase) = udict[code]\n+         old, iso, upcase, lowcase, titlecase) = data\n \n         # generate char to char direct common and simple conversions\n         # uppercase to lowercase\n@@ -218,46 +296,47 @@ def load_unicode_data(file_path):\n             to_title[code] = (int(titlecase, 16), 0, 0)\n \n         # store decomposition, if given\n-        if decomp != \"\":\n+        if decomp:\n+            decompositions = decomp.split()[1:]\n+            decomp_code_points = [int(i, 16) for i in decompositions]\n+\n             if decomp.startswith(\"<\"):\n-                seq = []\n-                for i in decomp.split()[1:]:\n-                    seq.append(int(i, 16))\n-                compat_decomp[code] = seq\n+                # compatibility decomposition\n+                compat_decomp[code] = decomp_code_points\n             else:\n-                seq = []\n-                for i in decomp.split():\n-                    seq.append(int(i, 16))\n-                canon_decomp[code] = seq\n+                # canonical decomposition\n+                canon_decomp[code] = decomp_code_points\n \n         # place letter in categories as appropriate\n-        for cat in [gencat, \"Assigned\"] + EXPANDED_CATEGORIES.get(gencat, []):\n-            if cat not in gencats:\n-                gencats[cat] = []\n-            gencats[cat].append(code)\n+        for cat in itertools.chain((gencat, ), EXPANDED_CATEGORIES.get(gencat, [])):\n+            general_categories[cat].add(code)\n+            category_assigned_codepoints.add(code)\n \n         # record combining class, if any\n         if combine != \"0\":\n-            if combine not in combines:\n-                combines[combine] = []\n-            combines[combine].append(code)\n+            combines[combine].add(code)\n \n     # generate Not_Assigned from Assigned\n-    gencats[\"Cn\"] = gen_unassigned(gencats[\"Assigned\"])\n-    # Assigned is not a real category\n-    del(gencats[\"Assigned\"])\n+    general_categories[\"Cn\"] = get_unassigned_codepoints(category_assigned_codepoints)\n+\n     # Other contains Not_Assigned\n-    gencats[\"C\"].extend(gencats[\"Cn\"])\n-    gencats = group_cats(gencats)\n-    combines = to_combines(group_cats(combines))\n+    general_categories[\"C\"].update(general_categories[\"Cn\"])\n+\n+    grouped_categories = group_categories(general_categories)\n \n+    # FIXME: combines are not used\n     return UnicodeData(\n-        canon_decomp, compat_decomp, gencats, combines, to_upper,\n-        to_lower, to_title,\n+        to_lower=to_lower, to_upper=to_upper, to_title=to_title,\n+        compat_decomp=compat_decomp, canon_decomp=canon_decomp,\n+        general_categories=grouped_categories, combines=combines,\n     )\n \n \n def load_special_casing(file_path, unicode_data):\n+    # type: (str, UnicodeData) -> None\n+    \"\"\"\n+    Load special casing data and enrich given unicode data.\n+    \"\"\"\n     for line in fileinput.input(file_path):\n         data = line.split(\"#\")[0].split(\";\")\n         if len(data) == 5:\n@@ -277,258 +356,395 @@ def load_special_casing(file_path, unicode_data):\n                                (unicode_data.to_upper, upper),\n                                (unicode_data.to_title, title)):\n             if values != code:\n-                values = [int(i, 16) for i in values.split()]\n-                for _ in range(len(values), 3):\n-                    values.append(0)\n-                assert len(values) == 3\n-                map_[key] = values\n-\n-\n-def group_cats(cats):\n-    cats_out = {}\n-    for cat in cats:\n-        cats_out[cat] = group_cat(cats[cat])\n-    return cats_out\n-\n-\n-def group_cat(cat):\n-    cat_out = []\n-    letters = sorted(set(cat))\n-    cur_start = letters.pop(0)\n-    cur_end = cur_start\n-    for letter in letters:\n-        assert letter > cur_end, \\\n-            \"cur_end: %s, letter: %s\" % (hex(cur_end), hex(letter))\n-        if letter == cur_end + 1:\n-            cur_end = letter\n-        else:\n-            cat_out.append((cur_start, cur_end))\n-            cur_start = cur_end = letter\n-    cat_out.append((cur_start, cur_end))\n-    return cat_out\n+                split = values.split()\n+\n+                codepoints = list(itertools.chain(\n+                    (int(i, 16) for i in split),\n+                    (0 for _ in range(len(split), 3))\n+                ))\n+\n+                assert len(codepoints) == 3\n+                map_[key] = codepoints\n+\n+\n+def group_categories(mapping):\n+    # type: (Dict[Any, Iterable[int]]) -> Dict[str, List[Tuple[int, int]]]\n+    \"\"\"\n+    Group codepoints mapped in \"categories\".\n+    \"\"\"\n+    return {category: group_codepoints(codepoints)\n+            for category, codepoints in mapping.items()}\n+\n+\n+def group_codepoints(codepoints):\n+    # type: (Iterable[int]) -> List[Tuple[int, int]]\n+    \"\"\"\n+    Group integral values into continuous, disjoint value ranges.\n+\n+    Performs value deduplication.\n+\n+    :return: sorted list of pairs denoting start and end of codepoint\n+        group values, both ends inclusive.\n+\n+    >>> group_codepoints([1, 2, 10, 11, 12, 3, 4])\n+    [(1, 4), (10, 12)]\n+    >>> group_codepoints([1])\n+    [(1, 1)]\n+    >>> group_codepoints([1, 5, 6])\n+    [(1, 1), (5, 6)]\n+    >>> group_codepoints([])\n+    []\n+    \"\"\"\n+    sorted_codes = sorted(set(codepoints))\n+    result = []     # type: List[Tuple[int, int]]\n \n+    if not sorted_codes:\n+        return result\n \n-def ungroup_cat(cat):\n-    cat_out = []\n-    for (lo, hi) in cat:\n-        while lo <= hi:\n-            cat_out.append(lo)\n-            lo += 1\n-    return cat_out\n+    next_codes = sorted_codes[1:]\n+    start_code = sorted_codes[0]\n \n+    for code, next_code in zip_longest(sorted_codes, next_codes, fillvalue=None):\n+        if next_code is None or next_code - code != 1:\n+            result.append((start_code, code))\n+            start_code = next_code\n \n-def gen_unassigned(assigned):\n-    assigned = set(assigned)\n-    return ([i for i in range(0, 0xd800) if i not in assigned] +\n-            [i for i in range(0xe000, 0x110000) if i not in assigned])\n+    return result\n \n \n-def to_combines(combs):\n-    combs_out = []\n-    for comb in combs:\n-        for (lo, hi) in combs[comb]:\n-            combs_out.append((lo, hi, comb))\n-    combs_out.sort(key=lambda c: c[0])\n-    return combs_out\n+def ungroup_codepoints(codepoint_pairs):\n+    # type: (Iterable[Tuple[int, int]]) -> List[int]\n+    \"\"\"\n+    The inverse of group_codepoints -- produce a flat list of values\n+    from value range pairs.\n+\n+    >>> ungroup_codepoints([(1, 4), (10, 12)])\n+    [1, 2, 3, 4, 10, 11, 12]\n+    >>> ungroup_codepoints([(1, 1), (5, 6)])\n+    [1, 5, 6]\n+    >>> ungroup_codepoints(group_codepoints([1, 2, 7, 8]))\n+    [1, 2, 7, 8]\n+    >>> ungroup_codepoints([])\n+    []\n+    \"\"\"\n+    return list(itertools.chain.from_iterable(\n+        range(lo, hi + 1) for lo, hi in codepoint_pairs\n+    ))\n+\n+\n+def get_unassigned_codepoints(assigned_codepoints):\n+    # type: (Set[int]) -> Set[int]\n+    \"\"\"\n+    Given a set of \"assigned\" codepoints, return a set\n+    of these that are not in assigned and not surrogate.\n+    \"\"\"\n+    return {i for i in range(0, 0x110000)\n+            if i not in assigned_codepoints and not is_surrogate(i)}\n+\n+\n+def generate_table_lines(items, indent, wrap=98):\n+    # type: (Iterable[str], int, int) -> Iterator[str]\n+    \"\"\"\n+    Given table items, generate wrapped lines of text with comma-separated items.\n \n+    This is a generator function.\n \n-def format_table_content(f, content, indent):\n+    :param wrap: soft wrap limit (characters per line), integer.\n+    \"\"\"\n     line = \" \" * indent\n     first = True\n-    for chunk in content.split(\",\"):\n-        if len(line) + len(chunk) < 98:\n+    for item in items:\n+        if len(line) + len(item) < wrap:\n             if first:\n-                line += chunk\n+                line += item\n             else:\n-                line += \", \" + chunk\n+                line += \", \" + item\n             first = False\n         else:\n-            f.write(line + \",\\n\")\n-            line = \" \" * indent + chunk\n-    f.write(line)\n+            yield line + \",\\n\"\n+            line = \" \" * indent + item\n \n+    yield line\n \n-def load_properties(file_path, interestingprops):\n-    props = {}\n-    # \"raw string\" is necessary for \\w not to be treated as escape char\n+\n+def load_properties(file_path, interesting_props):\n+    # type: (str, Iterable[str]) -> Dict[str, List[Tuple[int, int]]]\n+    \"\"\"\n+    Load properties data and return in grouped form.\n+    \"\"\"\n+    props = defaultdict(list)   # type: Dict[str, List[Tuple[int, int]]]\n+    # \"raw string\" is necessary for \\. and \\w not to be treated as escape chars\n     # (for the sake of compat with future Python versions)\n     # see: https://docs.python.org/3.6/whatsnew/3.6.html#deprecated-python-behavior\n     re1 = re.compile(r\"^ *([0-9A-F]+) *; *(\\w+)\")\n     re2 = re.compile(r\"^ *([0-9A-F]+)\\.\\.([0-9A-F]+) *; *(\\w+)\")\n \n     for line in fileinput.input(file_path):\n-        prop = None\n-        d_lo = 0\n-        d_hi = 0\n-        m = re1.match(line)\n-        if m:\n-            d_lo = m.group(1)\n-            d_hi = m.group(1)\n-            prop = m.group(2)\n-        else:\n-            m = re2.match(line)\n-            if m:\n-                d_lo = m.group(1)\n-                d_hi = m.group(2)\n-                prop = m.group(3)\n+        match = re1.match(line) or re2.match(line)\n+        if match:\n+            groups = match.groups()\n+\n+            if len(groups) == 2:\n+                # re1 matched\n+                d_lo, prop = groups\n+                d_hi = d_lo\n             else:\n-                continue\n-        if interestingprops and prop not in interestingprops:\n+                d_lo, d_hi, prop = groups\n+        else:\n             continue\n-        d_lo = int(d_lo, 16)\n-        d_hi = int(d_hi, 16)\n-        if prop not in props:\n-            props[prop] = []\n-        props[prop].append((d_lo, d_hi))\n+\n+        if interesting_props and prop not in interesting_props:\n+            continue\n+\n+        lo_value = int(d_lo, 16)\n+        hi_value = int(d_hi, 16)\n+\n+        props[prop].append((lo_value, hi_value))\n \n     # optimize if possible\n     for prop in props:\n-        props[prop] = group_cat(ungroup_cat(props[prop]))\n+        props[prop] = group_codepoints(ungroup_codepoints(props[prop]))\n \n     return props\n \n \n def escape_char(c):\n-    return \"'\\\\u{%x}'\" % c if c != 0 else \"'\\\\0'\"\n+    # type: (int) -> str\n+    r\"\"\"\n+    Escape a codepoint for use as Rust char literal.\n \n+    Outputs are OK to use as Rust source code as char literals\n+    and they also include necessary quotes.\n \n-def emit_table(f, name, t_data, t_type=\"&[(char, char)]\", is_pub=True,\n-        pfun=lambda x: \"(%s,%s)\" % (escape_char(x[0]), escape_char(x[1]))):\n+    >>> escape_char(97)\n+    \"'\\\\u{61}'\"\n+    >>> escape_char(0)\n+    \"'\\\\0'\"\n+    \"\"\"\n+    return r\"'\\u{%x}'\" % c if c != 0 else r\"'\\0'\"\n+\n+\n+def format_char_pair(pair):\n+    # type: (Tuple[int, int]) -> str\n+    \"\"\"\n+    Format a pair of two Rust chars.\n+    \"\"\"\n+    return \"(%s,%s)\" % (escape_char(pair[0]), escape_char(pair[1]))\n+\n+\n+def generate_table(\n+    name,   # type: str\n+    items,  # type: List[Tuple[int, int]]\n+    decl_type=\"&[(char, char)]\",    # type: str\n+    is_pub=True,                    # type: bool\n+    format_item=format_char_pair,   # type: Callable[[Tuple[int, int]], str]\n+):\n+    # type: (...) -> Iterator[str]\n+    \"\"\"\n+    Generate a nicely formatted Rust constant \"table\" array.\n+\n+    This generates actual Rust code.\n+    \"\"\"\n     pub_string = \"\"\n     if is_pub:\n         pub_string = \"pub \"\n-    f.write(\"    %sconst %s: %s = &[\\n\" % (pub_string, name, t_type))\n-    data = \"\"\n+\n+    yield \"    %sconst %s: %s = &[\\n\" % (pub_string, name, decl_type)\n+\n+    data = []\n     first = True\n-    for dat in t_data:\n+    for item in items:\n         if not first:\n-            data += \",\"\n+            data.append(\",\")\n         first = False\n-        data += pfun(dat)\n-    format_table_content(f, data, 8)\n-    f.write(\"\\n    ];\\n\\n\")\n+        data.extend(format_item(item))\n+\n+    for table_line in generate_table_lines(\"\".join(data).split(\",\"), 8):\n+        yield table_line\n \n+    yield \"\\n    ];\\n\\n\"\n \n-def compute_trie(rawdata, chunksize):\n+\n+def compute_trie(raw_data, chunk_size):\n+    # type: (List[int], int) -> Tuple[List[int], List[int]]\n+    \"\"\"\n+    Compute postfix-compressed trie.\n+\n+    See: bool_trie.rs for more details.\n+\n+    >>> compute_trie([1, 2, 3, 1, 2, 3, 4, 5, 6], 3)\n+    ([0, 0, 1], [1, 2, 3, 4, 5, 6])\n+    >>> compute_trie([1, 2, 3, 1, 2, 4, 4, 5, 6], 3)\n+    ([0, 1, 2], [1, 2, 3, 1, 2, 4, 4, 5, 6])\n+    \"\"\"\n     root = []\n-    childmap = {}\n+    childmap = {}       # type: Dict[Tuple[int, ...], int]\n     child_data = []\n-    for i in range(len(rawdata) // chunksize):\n-        data = rawdata[i * chunksize: (i + 1) * chunksize]\n-        child = \"|\".join(map(str, data))\n+\n+    assert len(raw_data) % chunk_size == 0, \"Chunks must be equally sized\"\n+\n+    for i in range(len(raw_data) // chunk_size):\n+        data = raw_data[i * chunk_size : (i + 1) * chunk_size]\n+\n+        # postfix compression of child nodes (data chunks)\n+        # (identical child nodes are shared)\n+\n+        # make a tuple out of the list so it's hashable\n+        child = tuple(data)\n         if child not in childmap:\n             childmap[child] = len(childmap)\n             child_data.extend(data)\n+\n         root.append(childmap[child])\n+\n     return root, child_data\n \n \n-def emit_bool_trie(f, name, t_data, is_pub=True):\n+def generate_bool_trie(name, codepoint_ranges, is_pub=True):\n+    # type: (str, List[Tuple[int, int]], bool) -> Iterator[str]\n+    \"\"\"\n+    Generate Rust code for BoolTrie struct.\n+\n+    This yields string fragments that should be joined to produce\n+    the final string.\n+\n+    See: bool_trie.rs\n+    \"\"\"\n     chunk_size = 64\n     rawdata = [False] * 0x110000\n-    for (lo, hi) in t_data:\n+    for (lo, hi) in codepoint_ranges:\n         for cp in range(lo, hi + 1):\n             rawdata[cp] = True\n \n-    # convert to bitmap chunks of 64 bits each\n+    # convert to bitmap chunks of chunk_size bits each\n     chunks = []\n     for i in range(0x110000 // chunk_size):\n         chunk = 0\n-        for j in range(64):\n-            if rawdata[i * 64 + j]:\n+        for j in range(chunk_size):\n+            if rawdata[i * chunk_size + j]:\n                 chunk |= 1 << j\n         chunks.append(chunk)\n \n     pub_string = \"\"\n     if is_pub:\n         pub_string = \"pub \"\n-    f.write(\"    %sconst %s: &super::BoolTrie = &super::BoolTrie {\\n\" % (pub_string, name))\n-    f.write(\"        r1: [\\n\")\n-    data = \",\".join(\"0x%016x\" % chunk for chunk in chunks[0:0x800 // chunk_size])\n-    format_table_content(f, data, 12)\n-    f.write(\"\\n        ],\\n\")\n+    yield \"    %sconst %s: &super::BoolTrie = &super::BoolTrie {\\n\" % (pub_string, name)\n+    yield \"        r1: [\\n\"\n+    data = (\"0x%016x\" % chunk for chunk in chunks[:0x800 // chunk_size])\n+    for fragment in generate_table_lines(data, 12):\n+        yield fragment\n+    yield \"\\n        ],\\n\"\n \n     # 0x800..0x10000 trie\n     (r2, r3) = compute_trie(chunks[0x800 // chunk_size : 0x10000 // chunk_size], 64 // chunk_size)\n-    f.write(\"        r2: [\\n\")\n-    data = \",\".join(str(node) for node in r2)\n-    format_table_content(f, data, 12)\n-    f.write(\"\\n        ],\\n\")\n-    f.write(\"        r3: &[\\n\")\n-    data = \",\".join(\"0x%016x\" % chunk for chunk in r3)\n-    format_table_content(f, data, 12)\n-    f.write(\"\\n        ],\\n\")\n+    yield \"        r2: [\\n\"\n+    data = map(str, r2)\n+    for fragment in generate_table_lines(data, 12):\n+        yield fragment\n+    yield \"\\n        ],\\n\"\n+\n+    yield \"        r3: &[\\n\"\n+    data = (\"0x%016x\" % node for node in r3)\n+    for fragment in generate_table_lines(data, 12):\n+        yield fragment\n+    yield \"\\n        ],\\n\"\n \n     # 0x10000..0x110000 trie\n     (mid, r6) = compute_trie(chunks[0x10000 // chunk_size : 0x110000 // chunk_size],\n                              64 // chunk_size)\n     (r4, r5) = compute_trie(mid, 64)\n-    f.write(\"        r4: [\\n\")\n-    data = \",\".join(str(node) for node in r4)\n-    format_table_content(f, data, 12)\n-    f.write(\"\\n        ],\\n\")\n-    f.write(\"        r5: &[\\n\")\n-    data = \",\".join(str(node) for node in r5)\n-    format_table_content(f, data, 12)\n-    f.write(\"\\n        ],\\n\")\n-    f.write(\"        r6: &[\\n\")\n-    data = \",\".join(\"0x%016x\" % chunk for chunk in r6)\n-    format_table_content(f, data, 12)\n-    f.write(\"\\n        ],\\n\")\n-\n-    f.write(\"    };\\n\\n\")\n-\n-\n-def emit_small_bool_trie(f, name, t_data, is_pub=True):\n-    last_chunk = max(hi // 64 for (lo, hi) in t_data)\n+\n+    yield \"        r4: [\\n\"\n+    data = map(str, r4)\n+    for fragment in generate_table_lines(data, 12):\n+        yield fragment\n+    yield \"\\n        ],\\n\"\n+\n+    yield \"        r5: &[\\n\"\n+    data = map(str, r5)\n+    for fragment in generate_table_lines(data, 12):\n+        yield fragment\n+    yield \"\\n        ],\\n\"\n+\n+    yield \"        r6: &[\\n\"\n+    data = (\"0x%016x\" % node for node in r6)\n+    for fragment in generate_table_lines(data, 12):\n+        yield fragment\n+    yield \"\\n        ],\\n\"\n+\n+    yield \"    };\\n\\n\"\n+\n+\n+def generate_small_bool_trie(name, codepoint_ranges, is_pub=True):\n+    # type: (str, List[Tuple[int, int]], bool) -> Iterator[str]\n+    \"\"\"\n+    Generate Rust code for SmallBoolTrie struct.\n+\n+    See: bool_trie.rs\n+    \"\"\"\n+    last_chunk = max(hi // 64 for (lo, hi) in codepoint_ranges)\n     n_chunks = last_chunk + 1\n     chunks = [0] * n_chunks\n-    for (lo, hi) in t_data:\n+    for (lo, hi) in codepoint_ranges:\n         for cp in range(lo, hi + 1):\n-            if cp // 64 >= len(chunks):\n-                print(cp, cp // 64, len(chunks), lo, hi)\n+            assert cp // 64 < len(chunks)\n             chunks[cp // 64] |= 1 << (cp & 63)\n \n     pub_string = \"\"\n     if is_pub:\n         pub_string = \"pub \"\n-    f.write(\"    %sconst %s: &super::SmallBoolTrie = &super::SmallBoolTrie {\\n\"\n-            % (pub_string, name))\n+\n+    yield (\"    %sconst %s: &super::SmallBoolTrie = &super::SmallBoolTrie {\\n\"\n+           % (pub_string, name))\n \n     (r1, r2) = compute_trie(chunks, 1)\n \n-    f.write(\"        r1: &[\\n\")\n-    data = \",\".join(str(node) for node in r1)\n-    format_table_content(f, data, 12)\n-    f.write(\"\\n        ],\\n\")\n+    yield \"        r1: &[\\n\"\n+    data = (str(node) for node in r1)\n+    for fragment in generate_table_lines(data, 12):\n+        yield fragment\n+    yield \"\\n        ],\\n\"\n+\n+    yield \"        r2: &[\\n\"\n+    data = (\"0x%016x\" % node for node in r2)\n+    for fragment in generate_table_lines(data, 12):\n+        yield fragment\n+    yield \"\\n        ],\\n\"\n \n-    f.write(\"        r2: &[\\n\")\n-    data = \",\".join(\"0x%016x\" % node for node in r2)\n-    format_table_content(f, data, 12)\n-    f.write(\"\\n        ],\\n\")\n+    yield \"    };\\n\\n\"\n \n-    f.write(\"    };\\n\\n\")\n \n+def generate_property_module(mod, grouped_categories, category_subset):\n+    # type: (str, Dict[str, List[Tuple[int, int]]], Iterable[str]) -> Iterator[str]\n+    \"\"\"\n+    Generate Rust code for module defining properties.\n+    \"\"\"\n \n-def emit_property_module(f, mod, tbl, emit):\n-    f.write(\"pub mod %s {\\n\" % mod)\n-    for cat in sorted(emit):\n-        if cat in [\"Cc\", \"White_Space\", \"Pattern_White_Space\"]:\n-            emit_small_bool_trie(f, \"%s_table\" % cat, tbl[cat])\n-            f.write(\"    pub fn %s(c: char) -> bool {\\n\" % cat)\n-            f.write(\"        %s_table.lookup(c)\\n\" % cat)\n-            f.write(\"    }\\n\\n\")\n+    yield \"pub mod %s {\\n\" % mod\n+    for cat in sorted(category_subset):\n+        if cat in (\"Cc\", \"White_Space\", \"Pattern_White_Space\"):\n+            generator = generate_small_bool_trie(\"%s_table\" % cat, grouped_categories[cat])\n         else:\n-            emit_bool_trie(f, \"%s_table\" % cat, tbl[cat])\n-            f.write(\"    pub fn %s(c: char) -> bool {\\n\" % cat)\n-            f.write(\"        %s_table.lookup(c)\\n\" % cat)\n-            f.write(\"    }\\n\\n\")\n-    f.write(\"}\\n\\n\")\n+            generator = generate_bool_trie(\"%s_table\" % cat, grouped_categories[cat])\n+\n+        for fragment in generator:\n+            yield fragment\n+\n+        yield \"    pub fn %s(c: char) -> bool {\\n\" % cat\n+        yield \"        %s_table.lookup(c)\\n\" % cat\n+        yield \"    }\\n\\n\"\n+\n+    yield \"}\\n\\n\"\n+\n \n+def generate_conversions_module(unicode_data):\n+    # type: (UnicodeData) -> Iterator[str]\n+    \"\"\"\n+    Generate Rust code for module defining conversions.\n+    \"\"\"\n \n-def emit_conversions_module(f, unicode_data):\n-    f.write(\"pub mod conversions {\")\n-    f.write(\"\"\"\n+    yield \"pub mod conversions {\"\n+    yield \"\"\"\n     pub fn to_lower(c: char) -> [char; 3] {\n         match bsearch_case_table(c, to_lowercase_table) {\n             None        => [c, '\\\\0', '\\\\0'],\n@@ -545,46 +761,39 @@ def emit_conversions_module(f, unicode_data):\n \n     fn bsearch_case_table(c: char, table: &[(char, [char; 3])]) -> Option<usize> {\n         table.binary_search_by(|&(key, _)| key.cmp(&c)).ok()\n-    }\n-\n-\"\"\")\n-    t_type = \"&[(char, [char; 3])]\"\n-    pfun = lambda x: \"(%s,[%s,%s,%s])\" % (\n-        escape_char(x[0]), escape_char(x[1][0]), escape_char(x[1][1]), escape_char(x[1][2]))\n-\n-    emit_table(f,\n-               name=\"to_lowercase_table\",\n-               t_data=sorted(unicode_data.to_lower.items(), key=operator.itemgetter(0)),\n-               t_type=t_type,\n-               is_pub=False,\n-               pfun=pfun)\n-\n-    emit_table(f,\n-               name=\"to_uppercase_table\",\n-               t_data=sorted(unicode_data.to_upper.items(), key=operator.itemgetter(0)),\n-               t_type=t_type,\n-               is_pub=False,\n-               pfun=pfun)\n-\n-    f.write(\"}\\n\")\n-\n-\n-def emit_norm_module(f, unicode_data, norm_props):\n-    canon_keys = sorted(unicode_data.canon_decomp.keys())\n-\n-    canon_comp = {}\n-    comp_exclusions = norm_props[\"Full_Composition_Exclusion\"]\n-    for char in canon_keys:\n-        if any(lo <= char <= hi for lo, hi in comp_exclusions):\n-            continue\n-        decomp = unicode_data.canon_decomp[char]\n-        if len(decomp) == 2:\n-            if decomp[0] not in canon_comp:\n-                canon_comp[decomp[0]] = []\n-            canon_comp[decomp[0]].append((decomp[1], char))\n+    }\\n\\n\"\"\"\n+\n+    decl_type = \"&[(char, [char; 3])]\"\n+    format_conversion = lambda x: \"({},[{},{},{}])\".format(*(\n+        escape_char(c) for c in (x[0], x[1][0], x[1][1], x[1][2])\n+    ))\n+\n+    for fragment in generate_table(\n+        name=\"to_lowercase_table\",\n+        items=sorted(unicode_data.to_lower.items(), key=lambda x: x[0]),\n+        decl_type=decl_type,\n+        is_pub=False,\n+        format_item=format_conversion\n+    ):\n+        yield fragment\n+\n+    for fragment in generate_table(\n+        name=\"to_uppercase_table\",\n+        items=sorted(unicode_data.to_upper.items(), key=lambda x: x[0]),\n+        decl_type=decl_type,\n+        is_pub=False,\n+        format_item=format_conversion\n+    ):\n+        yield fragment\n+\n+    yield \"}\\n\"\n \n \n def parse_args():\n+    # type: () -> argparse.Namespace\n+    \"\"\"\n+    Parse command line arguments.\n+    \"\"\"\n     parser = argparse.ArgumentParser(description=__doc__)\n     parser.add_argument(\"-v\", \"--version\", default=None, type=str,\n                         help=\"Unicode version to use (if not specified,\"\n@@ -594,56 +803,63 @@ def parse_args():\n \n \n def main():\n+    # type: () -> None\n+    \"\"\"\n+    Script entry point.\n+    \"\"\"\n     args = parse_args()\n \n     unicode_version = fetch_files(args.version)\n     print(\"Using Unicode version: {}\".format(unicode_version.as_str))\n \n+    # all the writing happens entirely in memory, we only write to file\n+    # once we have generated the file content (it's not very large, <1 MB)\n+    buf = StringIO()\n+    buf.write(PREAMBLE)\n+\n+    unicode_version_notice = textwrap.dedent(\"\"\"\n+    /// The version of [Unicode](http://www.unicode.org/) that the Unicode parts of\n+    /// `char` and `str` methods are based on.\n+    #[unstable(feature = \"unicode_version\", issue = \"49726\")]\n+    pub const UNICODE_VERSION: UnicodeVersion = UnicodeVersion {{\n+        major: {version.major},\n+        minor: {version.minor},\n+        micro: {version.micro},\n+        _priv: (),\n+    }};\n+    \"\"\").format(version=unicode_version)\n+    buf.write(unicode_version_notice)\n+\n+    get_path = lambda f: get_unicode_file_path(unicode_version, f)\n+\n+    unicode_data = load_unicode_data(get_path(UnicodeFiles.UNICODE_DATA))\n+    load_special_casing(get_path(UnicodeFiles.SPECIAL_CASING), unicode_data)\n+\n+    want_derived = {\"XID_Start\", \"XID_Continue\", \"Alphabetic\", \"Lowercase\", \"Uppercase\",\n+                    \"Cased\", \"Case_Ignorable\", \"Grapheme_Extend\"}\n+    derived = load_properties(get_path(UnicodeFiles.DERIVED_CORE_PROPERTIES), want_derived)\n+\n+    props = load_properties(get_path(UnicodeFiles.PROPS),\n+                            {\"White_Space\", \"Join_Control\", \"Noncharacter_Code_Point\",\n+                             \"Pattern_White_Space\"})\n+\n+    # category tables\n+    for (name, categories, category_subset) in (\n+            (\"general_category\", unicode_data.general_categories, [\"N\", \"Cc\"]),\n+            (\"derived_property\", derived, want_derived),\n+            (\"property\", props, [\"White_Space\", \"Pattern_White_Space\"])\n+    ):\n+        for fragment in generate_property_module(name, categories, category_subset):\n+            buf.write(fragment)\n+\n+    for fragment in generate_conversions_module(unicode_data):\n+        buf.write(fragment)\n+\n     tables_rs_path = os.path.join(THIS_DIR, \"tables.rs\")\n \n     # will overwrite the file if it exists\n-    with open(tables_rs_path, \"w\") as rf:\n-        rf.write(PREAMBLE)\n-\n-        unicode_version_notice = textwrap.dedent(\"\"\"\n-        /// The version of [Unicode](http://www.unicode.org/) that the Unicode parts of\n-        /// `char` and `str` methods are based on.\n-        #[unstable(feature = \"unicode_version\", issue = \"49726\")]\n-        pub const UNICODE_VERSION: UnicodeVersion = UnicodeVersion {{\n-            major: {version.major},\n-            minor: {version.minor},\n-            micro: {version.micro},\n-            _priv: (),\n-        }};\n-        \"\"\").format(version=unicode_version)\n-        rf.write(unicode_version_notice)\n-\n-        get_path = lambda f: get_unicode_file_path(unicode_version, f)\n-\n-        unicode_data = load_unicode_data(get_path(UnicodeFiles.UNICODE_DATA))\n-        load_special_casing(get_path(UnicodeFiles.SPECIAL_CASING), unicode_data)\n-\n-        want_derived = [\"XID_Start\", \"XID_Continue\", \"Alphabetic\", \"Lowercase\", \"Uppercase\",\n-                        \"Cased\", \"Case_Ignorable\", \"Grapheme_Extend\"]\n-        derived = load_properties(get_path(UnicodeFiles.DERIVED_CORE_PROPERTIES), want_derived)\n-\n-        # FIXME scripts not used?\n-        scripts = load_properties(get_path(UnicodeFiles.SCRIPTS), [])\n-        props = load_properties(get_path(UnicodeFiles.PROPS),\n-                                [\"White_Space\", \"Join_Control\", \"Noncharacter_Code_Point\",\n-                                 \"Pattern_White_Space\"])\n-        norm_props = load_properties(get_path(UnicodeFiles.DERIVED_NORMALIZATION_PROPS),\n-                                     [\"Full_Composition_Exclusion\"])\n-\n-        # category tables\n-        for (name, cat, pfuns) in ((\"general_category\", unicode_data.gencats, [\"N\", \"Cc\"]),\n-                                   (\"derived_property\", derived, want_derived),\n-                                   (\"property\", props, [\"White_Space\", \"Pattern_White_Space\"])):\n-            emit_property_module(rf, name, cat, pfuns)\n-\n-        # normalizations and conversions module\n-        emit_norm_module(rf, unicode_data, norm_props)\n-        emit_conversions_module(rf, unicode_data)\n+    with open(tables_rs_path, \"w\") as fd:\n+        fd.write(buf.getvalue())\n \n     print(\"Regenerated tables.rs.\")\n "}]}