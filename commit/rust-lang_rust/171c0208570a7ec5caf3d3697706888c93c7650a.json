{"sha": "171c0208570a7ec5caf3d3697706888c93c7650a", "node_id": "MDY6Q29tbWl0NzI0NzEyOjE3MWMwMjA4NTcwYTdlYzVjYWYzZDM2OTc3MDY4ODhjOTNjNzY1MGE=", "commit": {"author": {"name": "Michael Woerister", "email": "michaelwoerister@posteo", "date": "2017-09-28T13:26:11Z"}, "committer": {"name": "Michael Woerister", "email": "michaelwoerister@posteo", "date": "2017-10-02T13:47:11Z"}, "message": "incr.comp.: Remove saving and loading of legacy dep-graph.", "tree": {"sha": "c2cf23582c5accb7e66a45a55c8c31100f60d198", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/c2cf23582c5accb7e66a45a55c8c31100f60d198"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/171c0208570a7ec5caf3d3697706888c93c7650a", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/171c0208570a7ec5caf3d3697706888c93c7650a", "html_url": "https://github.com/rust-lang/rust/commit/171c0208570a7ec5caf3d3697706888c93c7650a", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/171c0208570a7ec5caf3d3697706888c93c7650a/comments", "author": {"login": "michaelwoerister", "id": 1825894, "node_id": "MDQ6VXNlcjE4MjU4OTQ=", "avatar_url": "https://avatars.githubusercontent.com/u/1825894?v=4", "gravatar_id": "", "url": "https://api.github.com/users/michaelwoerister", "html_url": "https://github.com/michaelwoerister", "followers_url": "https://api.github.com/users/michaelwoerister/followers", "following_url": "https://api.github.com/users/michaelwoerister/following{/other_user}", "gists_url": "https://api.github.com/users/michaelwoerister/gists{/gist_id}", "starred_url": "https://api.github.com/users/michaelwoerister/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/michaelwoerister/subscriptions", "organizations_url": "https://api.github.com/users/michaelwoerister/orgs", "repos_url": "https://api.github.com/users/michaelwoerister/repos", "events_url": "https://api.github.com/users/michaelwoerister/events{/privacy}", "received_events_url": "https://api.github.com/users/michaelwoerister/received_events", "type": "User", "site_admin": false}, "committer": {"login": "michaelwoerister", "id": 1825894, "node_id": "MDQ6VXNlcjE4MjU4OTQ=", "avatar_url": "https://avatars.githubusercontent.com/u/1825894?v=4", "gravatar_id": "", "url": "https://api.github.com/users/michaelwoerister", "html_url": "https://github.com/michaelwoerister", "followers_url": "https://api.github.com/users/michaelwoerister/followers", "following_url": "https://api.github.com/users/michaelwoerister/following{/other_user}", "gists_url": "https://api.github.com/users/michaelwoerister/gists{/gist_id}", "starred_url": "https://api.github.com/users/michaelwoerister/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/michaelwoerister/subscriptions", "organizations_url": "https://api.github.com/users/michaelwoerister/orgs", "repos_url": "https://api.github.com/users/michaelwoerister/repos", "events_url": "https://api.github.com/users/michaelwoerister/events{/privacy}", "received_events_url": "https://api.github.com/users/michaelwoerister/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "272c324b25ebbb05d5de4f55268ded976d863210", "url": "https://api.github.com/repos/rust-lang/rust/commits/272c324b25ebbb05d5de4f55268ded976d863210", "html_url": "https://github.com/rust-lang/rust/commit/272c324b25ebbb05d5de4f55268ded976d863210"}], "stats": {"total": 1688, "additions": 66, "deletions": 1622}, "files": [{"sha": "56697e9fa05a3cbdb0947e089dd47bc6bba2d291", "filename": "src/librustc/dep_graph/graph.rs", "status": "modified", "additions": 3, "deletions": 1, "changes": 4, "blob_url": "https://github.com/rust-lang/rust/blob/171c0208570a7ec5caf3d3697706888c93c7650a/src%2Flibrustc%2Fdep_graph%2Fgraph.rs", "raw_url": "https://github.com/rust-lang/rust/raw/171c0208570a7ec5caf3d3697706888c93c7650a/src%2Flibrustc%2Fdep_graph%2Fgraph.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Fdep_graph%2Fgraph.rs?ref=171c0208570a7ec5caf3d3697706888c93c7650a", "patch": "@@ -506,7 +506,9 @@ impl DepGraph {\n                 None => {\n                     if dep_dep_node.kind.is_input() {\n                         // This input does not exist anymore.\n-                        debug_assert!(dep_dep_node.extract_def_id(tcx).is_none());\n+                        debug_assert!(dep_dep_node.extract_def_id(tcx).is_none(),\n+                                      \"Encountered input {:?} without color\",\n+                                      dep_dep_node);\n                         debug!(\"try_mark_green({:?}) - END - dependency {:?} \\\n                                 was deleted input\", dep_node, dep_dep_node);\n                         return None;"}, {"sha": "ad6f7fbf1119bf97ca375583813afd9ba84d14e9", "filename": "src/librustc_driver/driver.rs", "status": "modified", "additions": 5, "deletions": 5, "changes": 10, "blob_url": "https://github.com/rust-lang/rust/blob/171c0208570a7ec5caf3d3697706888c93c7650a/src%2Flibrustc_driver%2Fdriver.rs", "raw_url": "https://github.com/rust-lang/rust/raw/171c0208570a7ec5caf3d3697706888c93c7650a/src%2Flibrustc_driver%2Fdriver.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_driver%2Fdriver.rs?ref=171c0208570a7ec5caf3d3697706888c93c7650a", "patch": "@@ -642,8 +642,8 @@ pub fn phase_2_configure_and_expand<F>(sess: &Session,\n     );\n \n     let dep_graph = if sess.opts.build_dep_graph() {\n-        let prev_dep_graph = time(time_passes, \"load prev dep-graph (new)\", || {\n-            rustc_incremental::load_dep_graph_new(sess)\n+        let prev_dep_graph = time(time_passes, \"load prev dep-graph\", || {\n+            rustc_incremental::load_dep_graph(sess)\n         });\n \n         DepGraph::new(prev_dep_graph)\n@@ -1052,9 +1052,9 @@ pub fn phase_3_run_analysis_passes<'tcx, F, R>(sess: &'tcx Session,\n                              tx,\n                              output_filenames,\n                              |tcx| {\n-        time(time_passes,\n-             \"load_dep_graph\",\n-             || rustc_incremental::load_dep_graph(tcx));\n+        // Do some initialization of the DepGraph that can only be done with the\n+        // tcx available.\n+        rustc_incremental::dep_graph_tcx_init(tcx);\n \n         time(time_passes,\n              \"stability checking\","}, {"sha": "0294adb3f5debc8efc8b95d2fa9a5513df7bdadf", "filename": "src/librustc_incremental/lib.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/171c0208570a7ec5caf3d3697706888c93c7650a/src%2Flibrustc_incremental%2Flib.rs", "raw_url": "https://github.com/rust-lang/rust/raw/171c0208570a7ec5caf3d3697706888c93c7650a/src%2Flibrustc_incremental%2Flib.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_incremental%2Flib.rs?ref=171c0208570a7ec5caf3d3697706888c93c7650a", "patch": "@@ -32,7 +32,7 @@ mod persist;\n \n pub use assert_dep_graph::assert_dep_graph;\n pub use persist::load_dep_graph;\n-pub use persist::load_dep_graph_new;\n+pub use persist::dep_graph_tcx_init;\n pub use persist::save_dep_graph;\n pub use persist::save_trans_partition;\n pub use persist::save_work_products;"}, {"sha": "fc417851b8897ac5f7be023411e5310f729ab482", "filename": "src/librustc_incremental/persist/data.rs", "status": "modified", "additions": 1, "deletions": 68, "changes": 69, "blob_url": "https://github.com/rust-lang/rust/blob/171c0208570a7ec5caf3d3697706888c93c7650a/src%2Flibrustc_incremental%2Fpersist%2Fdata.rs", "raw_url": "https://github.com/rust-lang/rust/raw/171c0208570a7ec5caf3d3697706888c93c7650a/src%2Flibrustc_incremental%2Fpersist%2Fdata.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_incremental%2Fpersist%2Fdata.rs?ref=171c0208570a7ec5caf3d3697706888c93c7650a", "patch": "@@ -10,78 +10,11 @@\n \n //! The data that we will serialize and deserialize.\n \n-use rustc::dep_graph::{DepNode, WorkProduct, WorkProductId};\n+use rustc::dep_graph::{WorkProduct, WorkProductId};\n use rustc::hir::def_id::DefIndex;\n use rustc::hir::map::DefPathHash;\n-use rustc::ich::Fingerprint;\n use rustc::middle::cstore::EncodedMetadataHash;\n use rustc_data_structures::fx::FxHashMap;\n-use rustc_data_structures::indexed_vec::{IndexVec, Idx};\n-\n-/// Data for use when recompiling the **current crate**.\n-#[derive(Debug, RustcEncodable, RustcDecodable)]\n-pub struct SerializedDepGraph {\n-    /// The set of all DepNodes in the graph\n-    pub nodes: IndexVec<DepNodeIndex, DepNode>,\n-    /// For each DepNode, stores the list of edges originating from that\n-    /// DepNode. Encoded as a [start, end) pair indexing into edge_list_data,\n-    /// which holds the actual DepNodeIndices of the target nodes.\n-    pub edge_list_indices: IndexVec<DepNodeIndex, (u32, u32)>,\n-    /// A flattened list of all edge targets in the graph. Edge sources are\n-    /// implicit in edge_list_indices.\n-    pub edge_list_data: Vec<DepNodeIndex>,\n-\n-    /// These are output nodes that have no incoming edges. We track\n-    /// these separately so that when we reload all edges, we don't\n-    /// lose track of these nodes.\n-    pub bootstrap_outputs: Vec<DepNode>,\n-\n-    /// These are hashes of two things:\n-    /// - the HIR nodes in this crate\n-    /// - the metadata nodes from dependent crates we use\n-    ///\n-    /// In each case, we store a hash summarizing the contents of\n-    /// those items as they were at the time we did this compilation.\n-    /// In the case of HIR nodes, this hash is derived by walking the\n-    /// HIR itself. In the case of metadata nodes, the hash is loaded\n-    /// from saved state.\n-    ///\n-    /// When we do the next compile, we will load these back up and\n-    /// compare them against the hashes we see at that time, which\n-    /// will tell us what has changed, either in this crate or in some\n-    /// crate that we depend on.\n-    ///\n-    /// Because they will be reloaded, we don't store the DefId (which\n-    /// will be different when we next compile) related to each node,\n-    /// but rather the `DefPathIndex`. This can then be retraced\n-    /// to find the current def-id.\n-    pub hashes: Vec<(DepNodeIndex, Fingerprint)>,\n-}\n-\n-impl SerializedDepGraph {\n-    pub fn edge_targets_from(&self, source: DepNodeIndex) -> &[DepNodeIndex] {\n-        let targets = self.edge_list_indices[source];\n-        &self.edge_list_data[targets.0 as usize .. targets.1 as usize]\n-    }\n-}\n-\n-/// The index of a DepNode in the SerializedDepGraph::nodes array.\n-#[derive(Copy, Clone, Hash, Eq, PartialEq, Ord, PartialOrd, Debug,\n-         RustcEncodable, RustcDecodable)]\n-pub struct DepNodeIndex(pub u32);\n-\n-impl Idx for DepNodeIndex {\n-    #[inline]\n-    fn new(idx: usize) -> Self {\n-        assert!(idx <= ::std::u32::MAX as usize);\n-        DepNodeIndex(idx as u32)\n-    }\n-\n-    #[inline]\n-    fn index(self) -> usize {\n-        self.0 as usize\n-    }\n-}\n \n #[derive(Debug, RustcEncodable, RustcDecodable)]\n pub struct SerializedWorkProduct {"}, {"sha": "9b12b755581d5f2b1de69b00fdfad710cc17f7d7", "filename": "src/librustc_incremental/persist/fs.rs", "status": "modified", "additions": 0, "deletions": 5, "changes": 5, "blob_url": "https://github.com/rust-lang/rust/blob/171c0208570a7ec5caf3d3697706888c93c7650a/src%2Flibrustc_incremental%2Fpersist%2Ffs.rs", "raw_url": "https://github.com/rust-lang/rust/raw/171c0208570a7ec5caf3d3697706888c93c7650a/src%2Flibrustc_incremental%2Fpersist%2Ffs.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_incremental%2Fpersist%2Ffs.rs?ref=171c0208570a7ec5caf3d3697706888c93c7650a", "patch": "@@ -129,7 +129,6 @@ use std::__rand::{thread_rng, Rng};\n \n const LOCK_FILE_EXT: &'static str = \".lock\";\n const DEP_GRAPH_FILENAME: &'static str = \"dep-graph.bin\";\n-const DEP_GRAPH_NEW_FILENAME: &'static str = \"dep-graph-new.bin\";\n const WORK_PRODUCTS_FILENAME: &'static str = \"work-products.bin\";\n const METADATA_HASHES_FILENAME: &'static str = \"metadata.bin\";\n \n@@ -143,10 +142,6 @@ pub fn dep_graph_path(sess: &Session) -> PathBuf {\n     in_incr_comp_dir_sess(sess, DEP_GRAPH_FILENAME)\n }\n \n-pub fn dep_graph_path_new(sess: &Session) -> PathBuf {\n-    in_incr_comp_dir_sess(sess, DEP_GRAPH_NEW_FILENAME)\n-}\n-\n pub fn work_products_path(sess: &Session) -> PathBuf {\n     in_incr_comp_dir_sess(sess, WORK_PRODUCTS_FILENAME)\n }"}, {"sha": "ab2630d6a17a4017eb6f99c995a4fe3fec9e4adb", "filename": "src/librustc_incremental/persist/load.rs", "status": "modified", "additions": 41, "deletions": 310, "changes": 351, "blob_url": "https://github.com/rust-lang/rust/blob/171c0208570a7ec5caf3d3697706888c93c7650a/src%2Flibrustc_incremental%2Fpersist%2Fload.rs", "raw_url": "https://github.com/rust-lang/rust/raw/171c0208570a7ec5caf3d3697706888c93c7650a/src%2Flibrustc_incremental%2Fpersist%2Fload.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_incremental%2Fpersist%2Fload.rs?ref=171c0208570a7ec5caf3d3697706888c93c7650a", "patch": "@@ -10,63 +10,61 @@\n \n //! Code to save/load the dep-graph from files.\n \n-use rustc::dep_graph::{DepNode, WorkProductId, DepKind, PreviousDepGraph};\n+use rustc::dep_graph::{PreviousDepGraph, SerializedDepGraph};\n use rustc::hir::svh::Svh;\n use rustc::ich::Fingerprint;\n use rustc::session::Session;\n use rustc::ty::TyCtxt;\n use rustc::util::nodemap::DefIdMap;\n-use rustc_data_structures::fx::{FxHashSet, FxHashMap};\n-use rustc_data_structures::indexed_vec::IndexVec;\n use rustc_serialize::Decodable as RustcDecodable;\n use rustc_serialize::opaque::Decoder;\n-use std::path::{Path};\n+use std::path::Path;\n \n use super::data::*;\n use super::fs::*;\n use super::file_format;\n use super::work_product;\n \n-// The key is a dirty node. The value is **some** base-input that we\n-// can blame it on.\n-pub type DirtyNodes = FxHashMap<DepNodeIndex, DepNodeIndex>;\n-\n-/// If we are in incremental mode, and a previous dep-graph exists,\n-/// then load up those nodes/edges that are still valid into the\n-/// dep-graph for this session. (This is assumed to be running very\n-/// early in compilation, before we've really done any work, but\n-/// actually it doesn't matter all that much.) See `README.md` for\n-/// more general overview.\n-pub fn load_dep_graph<'a, 'tcx>(tcx: TyCtxt<'a, 'tcx, 'tcx>) {\n-    tcx.allocate_metadata_dep_nodes();\n-    tcx.precompute_in_scope_traits_hashes();\n-    if tcx.sess.incr_session_load_dep_graph() {\n-        let _ignore = tcx.dep_graph.in_ignore();\n-        load_dep_graph_if_exists(tcx);\n+pub fn dep_graph_tcx_init<'a, 'tcx>(tcx: TyCtxt<'a, 'tcx, 'tcx>) {\n+    if !tcx.dep_graph.is_fully_enabled() {\n+        return\n     }\n-}\n \n-fn load_dep_graph_if_exists<'a, 'tcx>(tcx: TyCtxt<'a, 'tcx, 'tcx>) {\n-    let dep_graph_path = dep_graph_path(tcx.sess);\n-    let dep_graph_data = match load_data(tcx.sess, &dep_graph_path) {\n-        Some(p) => p,\n-        None => return // no file\n-    };\n+    tcx.allocate_metadata_dep_nodes();\n+    tcx.precompute_in_scope_traits_hashes();\n \n     let work_products_path = work_products_path(tcx.sess);\n-    let work_products_data = match load_data(tcx.sess, &work_products_path) {\n-        Some(p) => p,\n-        None => return // no file\n-    };\n+    if let Some(work_products_data) = load_data(tcx.sess, &work_products_path) {\n+        // Decode the list of work_products\n+        let mut work_product_decoder = Decoder::new(&work_products_data[..], 0);\n+        let work_products: Vec<SerializedWorkProduct> =\n+            RustcDecodable::decode(&mut work_product_decoder).unwrap_or_else(|e| {\n+                let msg = format!(\"Error decoding `work-products` from incremental \\\n+                                   compilation session directory: {}\", e);\n+                tcx.sess.fatal(&msg[..])\n+            });\n \n-    match decode_dep_graph(tcx, &dep_graph_data, &work_products_data) {\n-        Ok(dirty_nodes) => dirty_nodes,\n-        Err(err) => {\n-            tcx.sess.warn(\n-                &format!(\"decoding error in dep-graph from `{}` and `{}`: {}\",\n-                         dep_graph_path.display(),\n-                         work_products_path.display(),\n-                         err));\n+        for swp in work_products {\n+            let mut all_files_exist = true;\n+            for &(_, ref file_name) in swp.work_product.saved_files.iter() {\n+                let path = in_incr_comp_dir_sess(tcx.sess, file_name);\n+                if !path.exists() {\n+                    all_files_exist = false;\n+\n+                    if tcx.sess.opts.debugging_opts.incremental_info {\n+                        eprintln!(\"incremental: could not find file for work \\\n+                                   product: {}\", path.display());\n+                    }\n+                }\n+            }\n+\n+            if all_files_exist {\n+                debug!(\"reconcile_work_products: all files for {:?} exist\", swp);\n+                tcx.dep_graph.insert_previous_work_product(&swp.id, swp.work_product);\n+            } else {\n+                debug!(\"reconcile_work_products: some file for {:?} does not exist\", swp);\n+                delete_dirty_work_product(tcx, swp);\n+            }\n         }\n     }\n }\n@@ -94,197 +92,6 @@ fn load_data(sess: &Session, path: &Path) -> Option<Vec<u8>> {\n     None\n }\n \n-/// Check if a DepNode from the previous dep-graph refers to something that\n-/// still exists in the current compilation session. Only works for DepNode\n-/// variants that represent inputs (HIR and imported Metadata).\n-fn does_still_exist(tcx: TyCtxt, dep_node: &DepNode) -> bool {\n-    match dep_node.kind {\n-        DepKind::Krate |\n-        DepKind::Hir |\n-        DepKind::HirBody |\n-        DepKind::InScopeTraits |\n-        DepKind::CrateMetadata => {\n-            dep_node.extract_def_id(tcx).is_some()\n-        }\n-        _ => {\n-            bug!(\"unexpected Input DepNode: {:?}\", dep_node)\n-        }\n-    }\n-}\n-\n-/// Decode the dep graph and load the edges/nodes that are still clean\n-/// into `tcx.dep_graph`.\n-pub fn decode_dep_graph<'a, 'tcx>(tcx: TyCtxt<'a, 'tcx, 'tcx>,\n-                                  dep_graph_data: &[u8],\n-                                  work_products_data: &[u8])\n-                                  -> Result<(), String>\n-{\n-    // Decode the list of work_products\n-    let mut work_product_decoder = Decoder::new(work_products_data, 0);\n-    let work_products = <Vec<SerializedWorkProduct>>::decode(&mut work_product_decoder)?;\n-\n-    // Deserialize the directory and dep-graph.\n-    let mut dep_graph_decoder = Decoder::new(dep_graph_data, 0);\n-    let prev_commandline_args_hash = u64::decode(&mut dep_graph_decoder)?;\n-\n-    if prev_commandline_args_hash != tcx.sess.opts.dep_tracking_hash() {\n-        if tcx.sess.opts.debugging_opts.incremental_info {\n-            eprintln!(\"incremental: completely ignoring cache because of \\\n-                       differing commandline arguments\");\n-        }\n-        // We can't reuse the cache, purge it.\n-        debug!(\"decode_dep_graph: differing commandline arg hashes\");\n-        for swp in work_products {\n-            delete_dirty_work_product(tcx, swp);\n-        }\n-\n-        // No need to do any further work\n-        return Ok(());\n-    }\n-\n-    let serialized_dep_graph = SerializedDepGraph::decode(&mut dep_graph_decoder)?;\n-\n-    // Compute the set of nodes from the old graph where some input\n-    // has changed or been removed.\n-    let dirty_raw_nodes = initial_dirty_nodes(tcx,\n-                                              &serialized_dep_graph.nodes,\n-                                              &serialized_dep_graph.hashes);\n-    let dirty_raw_nodes = transitive_dirty_nodes(&serialized_dep_graph,\n-                                                 dirty_raw_nodes);\n-\n-    // Recreate the edges in the graph that are still clean.\n-    let mut clean_work_products = FxHashSet();\n-    let mut dirty_work_products = FxHashSet(); // incomplete; just used to suppress debug output\n-    for (source, targets) in serialized_dep_graph.edge_list_indices.iter_enumerated() {\n-        let target_begin = targets.0 as usize;\n-        let target_end = targets.1 as usize;\n-\n-        for &target in &serialized_dep_graph.edge_list_data[target_begin .. target_end] {\n-            process_edge(tcx,\n-                         source,\n-                         target,\n-                         &serialized_dep_graph.nodes,\n-                         &dirty_raw_nodes,\n-                         &mut clean_work_products,\n-                         &mut dirty_work_products,\n-                         &work_products);\n-        }\n-    }\n-\n-    // Recreate bootstrap outputs, which are outputs that have no incoming edges\n-    // (and hence cannot be dirty).\n-    for bootstrap_output in &serialized_dep_graph.bootstrap_outputs {\n-        if let DepKind::WorkProduct = bootstrap_output.kind {\n-            let wp_id = WorkProductId::from_fingerprint(bootstrap_output.hash);\n-            clean_work_products.insert(wp_id);\n-        }\n-\n-        tcx.dep_graph.add_node_directly(*bootstrap_output);\n-    }\n-\n-    // Add in work-products that are still clean, and delete those that are\n-    // dirty.\n-    reconcile_work_products(tcx, work_products, &clean_work_products);\n-\n-    Ok(())\n-}\n-\n-/// Computes which of the original set of def-ids are dirty. Stored in\n-/// a bit vector where the index is the DefPathIndex.\n-fn initial_dirty_nodes<'a, 'tcx>(tcx: TyCtxt<'a, 'tcx, 'tcx>,\n-                                 nodes: &IndexVec<DepNodeIndex, DepNode>,\n-                                 serialized_hashes: &[(DepNodeIndex, Fingerprint)])\n-                                 -> DirtyNodes {\n-    let mut dirty_nodes = FxHashMap();\n-\n-    for &(dep_node_index, prev_hash) in serialized_hashes {\n-        let dep_node = nodes[dep_node_index];\n-        if does_still_exist(tcx, &dep_node) {\n-            let current_hash = tcx.dep_graph.fingerprint_of(&dep_node);\n-\n-            if current_hash == prev_hash {\n-                debug!(\"initial_dirty_nodes: {:?} is clean (hash={:?})\",\n-                       dep_node,\n-                       current_hash);\n-                continue;\n-            }\n-\n-            if tcx.sess.opts.debugging_opts.incremental_dump_hash {\n-                println!(\"node {:?} is dirty as hash is {:?}, was {:?}\",\n-                         dep_node,\n-                         current_hash,\n-                         prev_hash);\n-            }\n-\n-            debug!(\"initial_dirty_nodes: {:?} is dirty as hash is {:?}, was {:?}\",\n-                   dep_node,\n-                   current_hash,\n-                   prev_hash);\n-        } else {\n-            if tcx.sess.opts.debugging_opts.incremental_dump_hash {\n-                println!(\"node {:?} is dirty as it was removed\", dep_node);\n-            }\n-\n-            debug!(\"initial_dirty_nodes: {:?} is dirty as it was removed\", dep_node);\n-        }\n-        dirty_nodes.insert(dep_node_index, dep_node_index);\n-    }\n-\n-    dirty_nodes\n-}\n-\n-fn transitive_dirty_nodes(serialized_dep_graph: &SerializedDepGraph,\n-                          mut dirty_nodes: DirtyNodes)\n-                          -> DirtyNodes\n-{\n-    let mut stack: Vec<(DepNodeIndex, DepNodeIndex)> = vec![];\n-    stack.extend(dirty_nodes.iter().map(|(&s, &b)| (s, b)));\n-    while let Some((source, blame)) = stack.pop() {\n-        // we know the source is dirty (because of the node `blame`)...\n-        debug_assert!(dirty_nodes.contains_key(&source));\n-\n-        // ...so we dirty all the targets (with the same blame)\n-        for &target in serialized_dep_graph.edge_targets_from(source) {\n-            if !dirty_nodes.contains_key(&target) {\n-                dirty_nodes.insert(target, blame);\n-                stack.push((target, blame));\n-            }\n-        }\n-    }\n-    dirty_nodes\n-}\n-\n-/// Go through the list of work-products produced in the previous run.\n-/// Delete any whose nodes have been found to be dirty or which are\n-/// otherwise no longer applicable.\n-fn reconcile_work_products<'a, 'tcx>(tcx: TyCtxt<'a, 'tcx, 'tcx>,\n-                                     work_products: Vec<SerializedWorkProduct>,\n-                                     _clean_work_products: &FxHashSet<WorkProductId>) {\n-    debug!(\"reconcile_work_products({:?})\", work_products);\n-    for swp in work_products {\n-        let mut all_files_exist = true;\n-        for &(_, ref file_name) in swp.work_product.saved_files.iter() {\n-            let path = in_incr_comp_dir_sess(tcx.sess, file_name);\n-            if !path.exists() {\n-                all_files_exist = false;\n-\n-                if tcx.sess.opts.debugging_opts.incremental_info {\n-                    eprintln!(\"incremental: could not find file for \\\n-                               up-to-date work product: {}\", path.display());\n-                }\n-            }\n-        }\n-\n-        if all_files_exist {\n-            debug!(\"reconcile_work_products: all files for {:?} exist\", swp);\n-            tcx.dep_graph.insert_previous_work_product(&swp.id, swp.work_product);\n-        } else {\n-            debug!(\"reconcile_work_products: some file for {:?} does not exist\", swp);\n-            delete_dirty_work_product(tcx, swp);\n-        }\n-    }\n-}\n-\n fn delete_dirty_work_product(tcx: TyCtxt,\n                              swp: SerializedWorkProduct) {\n     debug!(\"delete_dirty_work_product({:?})\", swp);\n@@ -349,90 +156,14 @@ pub fn load_prev_metadata_hashes(tcx: TyCtxt) -> DefIdMap<Fingerprint> {\n     output\n }\n \n-fn process_edge<'a, 'tcx, 'edges>(\n-    tcx: TyCtxt<'a, 'tcx, 'tcx>,\n-    source: DepNodeIndex,\n-    target: DepNodeIndex,\n-    nodes: &IndexVec<DepNodeIndex, DepNode>,\n-    dirty_raw_nodes: &DirtyNodes,\n-    clean_work_products: &mut FxHashSet<WorkProductId>,\n-    dirty_work_products: &mut FxHashSet<WorkProductId>,\n-    work_products: &[SerializedWorkProduct])\n-{\n-    // If the target is dirty, skip the edge. If this is an edge\n-    // that targets a work-product, we can print the blame\n-    // information now.\n-    if let Some(&blame) = dirty_raw_nodes.get(&target) {\n-        let target = nodes[target];\n-        if let DepKind::WorkProduct = target.kind {\n-            if tcx.sess.opts.debugging_opts.incremental_info {\n-                let wp_id = WorkProductId::from_fingerprint(target.hash);\n-\n-                if dirty_work_products.insert(wp_id) {\n-                    // Try to reconstruct the human-readable version of the\n-                    // DepNode. This cannot be done for things that where\n-                    // removed.\n-                    let blame = nodes[blame];\n-                    let blame_str = if let Some(def_id) = blame.extract_def_id(tcx) {\n-                        format!(\"{:?}({})\",\n-                                blame.kind,\n-                                tcx.def_path(def_id).to_string(tcx))\n-                    } else {\n-                        format!(\"{:?}\", blame)\n-                    };\n-\n-                    let wp = work_products.iter().find(|swp| swp.id == wp_id).unwrap();\n-\n-                    eprintln!(\"incremental: module {:?} is dirty because \\\n-                              {:?} changed or was removed\",\n-                              wp.work_product.cgu_name,\n-                              blame_str);\n-                }\n-            }\n-        }\n-        return;\n-    }\n-\n-    // At this point we have asserted that the target is clean -- otherwise, we\n-    // would have hit the return above. We can do some further consistency\n-    // checks based on this fact:\n-\n-    // We should never have an edge where the target is clean but the source\n-    // was dirty. Otherwise something was wrong with the dirtying pass above:\n-    debug_assert!(!dirty_raw_nodes.contains_key(&source));\n-\n-    // We also never should encounter an edge going from a removed input to a\n-    // clean target because removing the input would have dirtied the input\n-    // node and transitively dirtied the target.\n-    debug_assert!(match nodes[source].kind {\n-        DepKind::Hir | DepKind::HirBody | DepKind::CrateMetadata => {\n-            does_still_exist(tcx, &nodes[source])\n-        }\n-        _ => true,\n-    });\n-\n-    if !dirty_raw_nodes.contains_key(&target) {\n-        let target = nodes[target];\n-        let source = nodes[source];\n-        tcx.dep_graph.add_edge_directly(source, target);\n-\n-        if let DepKind::WorkProduct = target.kind {\n-            let wp_id = WorkProductId::from_fingerprint(target.hash);\n-            clean_work_products.insert(wp_id);\n-        }\n-    }\n-}\n-\n-pub fn load_dep_graph_new(sess: &Session) -> PreviousDepGraph {\n-    use rustc::dep_graph::SerializedDepGraph as SerializedDepGraphNew;\n-\n-    let empty = PreviousDepGraph::new(SerializedDepGraphNew::new());\n+pub fn load_dep_graph(sess: &Session) -> PreviousDepGraph {\n+    let empty = PreviousDepGraph::new(SerializedDepGraph::new());\n \n     if sess.opts.incremental.is_none() {\n         return empty\n     }\n \n-    if let Some(bytes) = load_data(sess, &dep_graph_path_new(sess)) {\n+    if let Some(bytes) = load_data(sess, &dep_graph_path(sess)) {\n         let mut decoder = Decoder::new(&bytes, 0);\n         let prev_commandline_args_hash = u64::decode(&mut decoder)\n             .expect(\"Error reading commandline arg hash from cached dep-graph\");\n@@ -449,7 +180,7 @@ pub fn load_dep_graph_new(sess: &Session) -> PreviousDepGraph {\n             return empty\n         }\n \n-        let dep_graph = SerializedDepGraphNew::decode(&mut decoder)\n+        let dep_graph = SerializedDepGraph::decode(&mut decoder)\n             .expect(\"Error reading cached dep-graph\");\n \n         PreviousDepGraph::new(dep_graph)"}, {"sha": "88d49e7aedca7d03a781c638fda565fd9e216a1f", "filename": "src/librustc_incremental/persist/mod.rs", "status": "modified", "additions": 1, "deletions": 2, "changes": 3, "blob_url": "https://github.com/rust-lang/rust/blob/171c0208570a7ec5caf3d3697706888c93c7650a/src%2Flibrustc_incremental%2Fpersist%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/171c0208570a7ec5caf3d3697706888c93c7650a/src%2Flibrustc_incremental%2Fpersist%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_incremental%2Fpersist%2Fmod.rs?ref=171c0208570a7ec5caf3d3697706888c93c7650a", "patch": "@@ -16,7 +16,6 @@ mod data;\n mod dirty_clean;\n mod fs;\n mod load;\n-mod preds;\n mod save;\n mod work_product;\n mod file_format;\n@@ -25,7 +24,7 @@ pub use self::fs::prepare_session_directory;\n pub use self::fs::finalize_session_directory;\n pub use self::fs::in_incr_comp_dir;\n pub use self::load::load_dep_graph;\n-pub use self::load::load_dep_graph_new;\n+pub use self::load::dep_graph_tcx_init;\n pub use self::save::save_dep_graph;\n pub use self::save::save_work_products;\n pub use self::work_product::save_trans_partition;"}, {"sha": "d2aa245c7c9424633718f3eadfe052b8accc5083", "filename": "src/librustc_incremental/persist/preds/compress/README.md", "status": "removed", "additions": 0, "deletions": 48, "changes": 48, "blob_url": "https://github.com/rust-lang/rust/blob/272c324b25ebbb05d5de4f55268ded976d863210/src%2Flibrustc_incremental%2Fpersist%2Fpreds%2Fcompress%2FREADME.md", "raw_url": "https://github.com/rust-lang/rust/raw/272c324b25ebbb05d5de4f55268ded976d863210/src%2Flibrustc_incremental%2Fpersist%2Fpreds%2Fcompress%2FREADME.md", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_incremental%2Fpersist%2Fpreds%2Fcompress%2FREADME.md?ref=272c324b25ebbb05d5de4f55268ded976d863210", "patch": "@@ -1,48 +0,0 @@\n-Graph compression\n-\n-The graph compression algorithm is intended to remove and minimize the\n-size of the dependency graph so it can be saved, while preserving\n-everything we care about. In particular, given a set of input/output\n-nodes in the graph (which must be disjoint), we ensure that the set of\n-input nodes that can reach a given output node does not change,\n-although the intermediate nodes may change in various ways. In short,\n-the output nodes are intended to be the ones whose existence we care\n-about when we start up, because they have some associated data that we\n-will try to re-use (and hence if they are dirty, we have to throw that\n-data away). The other intermediate nodes don't really matter so much.\n-\n-### Overview\n-\n-The algorithm works as follows:\n-\n-1. Do a single walk of the graph to construct a DAG\n-    - in this walk, we identify and unify all cycles, electing a representative \"head\" node\n-    - this is done using the union-find implementation\n-    - this code is found in the `classify` module\n-2. The result from this walk is a `Dag`: \n-   - the set of SCCs, represented by the union-find table\n-   - a set of edges in the new DAG, represented by:\n-     - a vector of parent nodes for each child node\n-     - a vector of cross-edges\n-     - once these are canonicalized, some of these edges may turn out to be cyclic edges\n-       (i.e., an edge A -> A where A is the head of some SCC)\n-3. We pass this `Dag` into the construct code, which then creates a\n-   new graph.  This graph has a smaller set of indices which includes\n-   *at least* the inputs/outputs from the original graph, but may have\n-   other nodes as well, if keeping them reduces the overall size of\n-   the graph.\n-   - This code is found in the `construct` module.\n-   \n-### Some notes\n-\n-The input graph is assumed to have *read-by* edges. i.e., `A -> B`\n-means that the task B reads data from A. But the DAG defined by\n-classify is expressed in terms of *reads-from* edges, which are the\n-inverse. So `A -> B` is the same as `B -rf-> A`. *reads-from* edges\n-are more natural since we want to walk from the outputs to the inputs,\n-effectively. When we construct the final graph, we reverse these edges\n-back into the *read-by* edges common elsewhere.\n-\n-   \n-   \n-   "}, {"sha": "aa29afd543c77f8f400ff256d3bf276f07334e53", "filename": "src/librustc_incremental/persist/preds/compress/classify/mod.rs", "status": "removed", "additions": 0, "deletions": 151, "changes": 151, "blob_url": "https://github.com/rust-lang/rust/blob/272c324b25ebbb05d5de4f55268ded976d863210/src%2Flibrustc_incremental%2Fpersist%2Fpreds%2Fcompress%2Fclassify%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/272c324b25ebbb05d5de4f55268ded976d863210/src%2Flibrustc_incremental%2Fpersist%2Fpreds%2Fcompress%2Fclassify%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_incremental%2Fpersist%2Fpreds%2Fcompress%2Fclassify%2Fmod.rs?ref=272c324b25ebbb05d5de4f55268ded976d863210", "patch": "@@ -1,151 +0,0 @@\n-// Copyright 2014 The Rust Project Developers. See the COPYRIGHT\n-// file at the top-level directory of this distribution and at\n-// http://rust-lang.org/COPYRIGHT.\n-//\n-// Licensed under the Apache License, Version 2.0 <LICENSE-APACHE or\n-// http://www.apache.org/licenses/LICENSE-2.0> or the MIT license\n-// <LICENSE-MIT or http://opensource.org/licenses/MIT>, at your\n-// option. This file may not be copied, modified, or distributed\n-// except according to those terms.\n-\n-//! First phase. Detect cycles and cross-edges.\n-\n-use super::*;\n-\n-#[cfg(test)]\n-mod test;\n-\n-pub struct Classify<'a, 'g: 'a, N: 'g, I: 'a, O: 'a>\n-    where N: Debug + Clone + 'g,\n-          I: Fn(&N) -> bool,\n-          O: Fn(&N) -> bool,\n-{\n-    r: &'a mut GraphReduce<'g, N, I, O>,\n-    stack: Vec<NodeIndex>,\n-    colors: Vec<Color>,\n-    dag: Dag,\n-}\n-\n-#[derive(Copy, Clone, Debug, PartialEq)]\n-enum Color {\n-    // not yet visited\n-    White,\n-\n-    // visiting; usize is index on stack\n-    Grey(usize),\n-\n-    // finished visiting\n-    Black,\n-}\n-\n-impl<'a, 'g, N, I, O> Classify<'a, 'g, N, I, O>\n-    where N: Debug + Clone + 'g,\n-          I: Fn(&N) -> bool,\n-          O: Fn(&N) -> bool,\n-{\n-    pub(super) fn new(r: &'a mut GraphReduce<'g, N, I, O>) -> Self {\n-        Classify {\n-            r,\n-            colors: vec![Color::White; r.in_graph.len_nodes()],\n-            stack: vec![],\n-            dag: Dag {\n-                parents: (0..r.in_graph.len_nodes()).map(|i| NodeIndex(i)).collect(),\n-                cross_edges: vec![],\n-                input_nodes: vec![],\n-                output_nodes: vec![],\n-            },\n-        }\n-    }\n-\n-    pub(super) fn walk(mut self) -> Dag {\n-        for (index, node) in self.r.in_graph.all_nodes().iter().enumerate() {\n-            if (self.r.is_output)(&node.data) {\n-                let index = NodeIndex(index);\n-                self.dag.output_nodes.push(index);\n-                match self.colors[index.0] {\n-                    Color::White => self.open(index),\n-                    Color::Grey(_) => panic!(\"grey node but have not yet started a walk\"),\n-                    Color::Black => (), // already visited, skip\n-                }\n-            }\n-        }\n-\n-        // At this point we've identifed all the cycles, and we've\n-        // constructed a spanning tree over the original graph\n-        // (encoded in `self.parents`) as well as a list of\n-        // cross-edges that reflect additional edges from the DAG.\n-        //\n-        // If we converted each node to its `cycle-head` (a\n-        // representative choice from each SCC, basically) and then\n-        // take the union of `self.parents` and `self.cross_edges`\n-        // (after canonicalization), that is basically our DAG.\n-        //\n-        // Note that both of those may well contain trivial `X -rf-> X`\n-        // cycle edges after canonicalization, though. e.g., if you\n-        // have a graph `{A -rf-> B, B -rf-> A}`, we will have unioned A and\n-        // B, but A will also be B's parent (or vice versa), and hence\n-        // when we canonicalize the parent edge it would become `A -rf->\n-        // A` (or `B -rf-> B`).\n-        self.dag\n-    }\n-\n-    fn open(&mut self, node: NodeIndex) {\n-        let index = self.stack.len();\n-        self.stack.push(node);\n-        self.colors[node.0] = Color::Grey(index);\n-        for child in self.r.inputs(node) {\n-            self.walk_edge(node, child);\n-        }\n-        self.stack.pop().unwrap();\n-        self.colors[node.0] = Color::Black;\n-\n-        if (self.r.is_input)(&self.r.in_graph.node_data(node)) {\n-            // base inputs should have no inputs\n-            assert!(self.r.inputs(node).next().is_none());\n-            debug!(\"input: `{:?}`\", self.r.in_graph.node_data(node));\n-            self.dag.input_nodes.push(node);\n-        }\n-    }\n-\n-    fn walk_edge(&mut self, parent: NodeIndex, child: NodeIndex) {\n-        debug!(\"walk_edge: {:?} -rf-> {:?}, {:?}\",\n-               self.r.in_graph.node_data(parent),\n-               self.r.in_graph.node_data(child),\n-               self.colors[child.0]);\n-\n-        // Ignore self-edges, just in case they exist.\n-        if child == parent {\n-            return;\n-        }\n-\n-        match self.colors[child.0] {\n-            Color::White => {\n-                // Not yet visited this node; start walking it.\n-                assert_eq!(self.dag.parents[child.0], child);\n-                self.dag.parents[child.0] = parent;\n-                self.open(child);\n-            }\n-\n-            Color::Grey(stack_index) => {\n-                // Back-edge; unify everything on stack between here and `stack_index`\n-                // since we are all participating in a cycle\n-                assert!(self.stack[stack_index] == child);\n-\n-                for &n in &self.stack[stack_index..] {\n-                    debug!(\"cycle `{:?}` and `{:?}`\",\n-                           self.r.in_graph.node_data(n),\n-                           self.r.in_graph.node_data(parent));\n-                    self.r.mark_cycle(n, parent);\n-                }\n-            }\n-\n-            Color::Black => {\n-                // Cross-edge, record and ignore\n-                self.dag.cross_edges.push((parent, child));\n-                debug!(\"cross-edge `{:?} -rf-> {:?}`\",\n-                       self.r.in_graph.node_data(parent),\n-                       self.r.in_graph.node_data(child));\n-            }\n-        }\n-    }\n-}"}, {"sha": "ca26f714a2a747aa978df35c5401d1dfdd46b9f8", "filename": "src/librustc_incremental/persist/preds/compress/classify/test.rs", "status": "removed", "additions": 0, "deletions": 94, "changes": 94, "blob_url": "https://github.com/rust-lang/rust/blob/272c324b25ebbb05d5de4f55268ded976d863210/src%2Flibrustc_incremental%2Fpersist%2Fpreds%2Fcompress%2Fclassify%2Ftest.rs", "raw_url": "https://github.com/rust-lang/rust/raw/272c324b25ebbb05d5de4f55268ded976d863210/src%2Flibrustc_incremental%2Fpersist%2Fpreds%2Fcompress%2Fclassify%2Ftest.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_incremental%2Fpersist%2Fpreds%2Fcompress%2Fclassify%2Ftest.rs?ref=272c324b25ebbb05d5de4f55268ded976d863210", "patch": "@@ -1,94 +0,0 @@\n-// Copyright 2014 The Rust Project Developers. See the COPYRIGHT\n-// file at the top-level directory of this distribution and at\n-// http://rust-lang.org/COPYRIGHT.\n-//\n-// Licensed under the Apache License, Version 2.0 <LICENSE-APACHE or\n-// http://www.apache.org/licenses/LICENSE-2.0> or the MIT license\n-// <LICENSE-MIT or http://opensource.org/licenses/MIT>, at your\n-// option. This file may not be copied, modified, or distributed\n-// except according to those terms.\n-\n-use super::*;\n-\n-#[test]\n-fn detect_cycles() {\n-    let (graph, nodes) = graph! {\n-        A -> C0,\n-        A -> C1,\n-        B -> C1,\n-        C0 -> C1,\n-        C1 -> C0,\n-        C0 -> D,\n-        C1 -> E,\n-    };\n-    let inputs = [\"A\", \"B\"];\n-    let outputs = [\"D\", \"E\"];\n-    let mut reduce = GraphReduce::new(&graph, |n| inputs.contains(n), |n| outputs.contains(n));\n-    Classify::new(&mut reduce).walk();\n-\n-    assert!(!reduce.in_cycle(nodes(\"A\"), nodes(\"C0\")));\n-    assert!(!reduce.in_cycle(nodes(\"B\"), nodes(\"C0\")));\n-    assert!(reduce.in_cycle(nodes(\"C0\"), nodes(\"C1\")));\n-    assert!(!reduce.in_cycle(nodes(\"D\"), nodes(\"C0\")));\n-    assert!(!reduce.in_cycle(nodes(\"E\"), nodes(\"C0\")));\n-    assert!(!reduce.in_cycle(nodes(\"E\"), nodes(\"A\")));\n-}\n-\n-/// Regr test for a bug where we forgot to pop nodes off of the stack\n-/// as we were walking. In this case, because edges are pushed to the front\n-/// of the list, we would visit OUT, then A, then IN, and then close IN (but forget\n-/// to POP. Then visit B, C, and then A, which would mark everything from A to C as\n-/// cycle. But since we failed to pop IN, the stack was `OUT, A, IN, B, C` so that\n-/// marked C and IN as being in a cycle.\n-#[test]\n-fn edge_order1() {\n-    let (graph, nodes) = graph! {\n-        A -> C,\n-        C -> B,\n-        B -> A,\n-        IN -> B,\n-        IN -> A,\n-        A -> OUT,\n-    };\n-    let inputs = [\"IN\"];\n-    let outputs = [\"OUT\"];\n-    let mut reduce = GraphReduce::new(&graph, |n| inputs.contains(n), |n| outputs.contains(n));\n-    Classify::new(&mut reduce).walk();\n-\n-    // A, B, and C are mutually in a cycle, but IN/OUT are not participating.\n-    let names = [\"A\", \"B\", \"C\", \"IN\", \"OUT\"];\n-    let cycle_names = [\"A\", \"B\", \"C\"];\n-    for &i in &names {\n-        for &j in names.iter().filter(|&&j| j != i) {\n-            let in_cycle = cycle_names.contains(&i) && cycle_names.contains(&j);\n-            assert_eq!(reduce.in_cycle(nodes(i), nodes(j)), in_cycle,\n-                       \"cycle status for nodes {} and {} is incorrect\",\n-                       i, j);\n-        }\n-    }\n-}\n-\n-/// Same as `edge_order1` but in reverse order so as to detect a failure\n-/// if we were to enqueue edges onto end of list instead.\n-#[test]\n-fn edge_order2() {\n-    let (graph, nodes) = graph! {\n-        A -> OUT,\n-        IN -> A,\n-        IN -> B,\n-        B -> A,\n-        C -> B,\n-        A -> C,\n-    };\n-    let inputs = [\"IN\"];\n-    let outputs = [\"OUT\"];\n-    let mut reduce = GraphReduce::new(&graph, |n| inputs.contains(n), |n| outputs.contains(n));\n-    Classify::new(&mut reduce).walk();\n-\n-    assert!(reduce.in_cycle(nodes(\"B\"), nodes(\"C\")));\n-\n-    assert!(!reduce.in_cycle(nodes(\"IN\"), nodes(\"A\")));\n-    assert!(!reduce.in_cycle(nodes(\"IN\"), nodes(\"B\")));\n-    assert!(!reduce.in_cycle(nodes(\"IN\"), nodes(\"C\")));\n-    assert!(!reduce.in_cycle(nodes(\"IN\"), nodes(\"OUT\")));\n-}"}, {"sha": "0ad8d1789167df38a7cb9e0a882a64b42d7a2c65", "filename": "src/librustc_incremental/persist/preds/compress/construct.rs", "status": "removed", "additions": 0, "deletions": 223, "changes": 223, "blob_url": "https://github.com/rust-lang/rust/blob/272c324b25ebbb05d5de4f55268ded976d863210/src%2Flibrustc_incremental%2Fpersist%2Fpreds%2Fcompress%2Fconstruct.rs", "raw_url": "https://github.com/rust-lang/rust/raw/272c324b25ebbb05d5de4f55268ded976d863210/src%2Flibrustc_incremental%2Fpersist%2Fpreds%2Fcompress%2Fconstruct.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_incremental%2Fpersist%2Fpreds%2Fcompress%2Fconstruct.rs?ref=272c324b25ebbb05d5de4f55268ded976d863210", "patch": "@@ -1,223 +0,0 @@\n-// Copyright 2014 The Rust Project Developers. See the COPYRIGHT\n-// file at the top-level directory of this distribution and at\n-// http://rust-lang.org/COPYRIGHT.\n-//\n-// Licensed under the Apache License, Version 2.0 <LICENSE-APACHE or\n-// http://www.apache.org/licenses/LICENSE-2.0> or the MIT license\n-// <LICENSE-MIT or http://opensource.org/licenses/MIT>, at your\n-// option. This file may not be copied, modified, or distributed\n-// except according to those terms.\n-\n-//! Second phase. Construct new graph. The previous phase has\n-//! converted the input graph into a DAG by detecting and unifying\n-//! cycles. It provides us with the following (which is a\n-//! representation of the DAG):\n-//!\n-//! - SCCs, in the form of a union-find repr that can convert each node to\n-//!   its *cycle head* (an arbitrarily chosen representative from the cycle)\n-//! - a vector of *leaf nodes*, just a convenience\n-//! - a vector of *parents* for each node (in some cases, nodes have no parents,\n-//!   or their parent is another member of same cycle; in that case, the vector\n-//!   will be stored `v[i] == i`, after canonicalization)\n-//! - a vector of *cross edges*, meaning add'l edges between graphs nodes beyond\n-//!   the parents.\n-\n-use rustc_data_structures::fx::FxHashMap;\n-\n-use super::*;\n-\n-pub(super) fn construct_graph<'g, N, I, O>(r: &mut GraphReduce<'g, N, I, O>, dag: Dag)\n-                                           -> Reduction<'g, N>\n-    where N: Debug + Clone, I: Fn(&N) -> bool, O: Fn(&N) -> bool,\n-{\n-    let Dag { parents: old_parents, input_nodes, output_nodes, cross_edges } = dag;\n-    let in_graph = r.in_graph;\n-\n-    debug!(\"construct_graph\");\n-\n-    // Create a canonical list of edges; this includes both parent and\n-    // cross-edges. We store this in `(target -> Vec<source>)` form.\n-    // We call the first edge to any given target its \"parent\".\n-    let mut edges = FxHashMap();\n-    let old_parent_edges = old_parents.iter().cloned().zip((0..).map(NodeIndex));\n-    for (source, target) in old_parent_edges.chain(cross_edges) {\n-        debug!(\"original edge `{:?} -rf-> {:?}`\",\n-               in_graph.node_data(source),\n-               in_graph.node_data(target));\n-        let source = r.cycle_head(source);\n-        let target = r.cycle_head(target);\n-        if source != target {\n-            let v = edges.entry(target).or_insert(vec![]);\n-            if !v.contains(&source) {\n-                debug!(\"edge `{:?} -rf-> {:?}` is edge #{} with that target\",\n-                       in_graph.node_data(source),\n-                       in_graph.node_data(target),\n-                       v.len());\n-                v.push(source);\n-            }\n-        }\n-    }\n-    let parent = |ni: NodeIndex| -> NodeIndex {\n-        edges[&ni][0]\n-    };\n-\n-    // `retain_map`: a map of those nodes that we will want to\n-    // *retain* in the ultimate graph; the key is the node index in\n-    // the old graph, the value is the node index in the new\n-    // graph. These are nodes in the following categories:\n-    //\n-    // - inputs\n-    // - work-products\n-    // - targets of a cross-edge\n-    //\n-    // The first two categories hopefully make sense. We want the\n-    // inputs so we can compare hashes later. We want the\n-    // work-products so we can tell precisely when a given\n-    // work-product is invalidated. But the last one isn't strictly\n-    // needed; we keep cross-target edges so as to minimize the total\n-    // graph size.\n-    //\n-    // Consider a graph like:\n-    //\n-    //     WP0 -rf-> Y\n-    //     WP1 -rf-> Y\n-    //     Y -rf-> INPUT0\n-    //     Y -rf-> INPUT1\n-    //     Y -rf-> INPUT2\n-    //     Y -rf-> INPUT3\n-    //\n-    // Now if we were to remove Y, we would have a total of 8 edges: both WP0 and WP1\n-    // depend on INPUT0...INPUT3. As it is, we have 6 edges.\n-    //\n-    // NB: The current rules are not optimal. For example, given this\n-    // input graph:\n-    //\n-    //     OUT0 -rf-> X\n-    //     OUT1 -rf-> X\n-    //     X -rf -> INPUT0\n-    //\n-    // we will preserve X because it has two \"consumers\" (OUT0 and\n-    // OUT1).  We could as easily skip it, but we'd have to tally up\n-    // the number of input nodes that it (transitively) reaches, and I\n-    // was too lazy to do so. This is the unit test `suboptimal`.\n-\n-    let mut retain_map = FxHashMap();\n-    let mut new_graph = Graph::new();\n-\n-    {\n-        // Start by adding start-nodes and inputs.\n-        let retained_nodes = output_nodes.iter().chain(&input_nodes).map(|&n| r.cycle_head(n));\n-\n-        // Next add in targets of cross-edges. Due to the canonicalization,\n-        // some of these may be self-edges or may may duplicate the parent\n-        // edges, so ignore those.\n-        let retained_nodes = retained_nodes.chain(\n-            edges.iter()\n-                 .filter(|&(_, ref sources)| sources.len() > 1)\n-                 .map(|(&target, _)| target));\n-\n-        // Now create the new graph, adding in the entries from the map.\n-        for n in retained_nodes {\n-            retain_map.entry(n)\n-                      .or_insert_with(|| {\n-                          let data = in_graph.node_data(n);\n-                          debug!(\"retaining node `{:?}`\", data);\n-                          new_graph.add_node(data)\n-                      });\n-        }\n-    }\n-\n-    // Given a cycle-head `ni`, converts it to the closest parent that has\n-    // been retained in the output graph.\n-    let retained_parent = |mut ni: NodeIndex| -> NodeIndex {\n-        loop {\n-            debug!(\"retained_parent({:?})\", in_graph.node_data(ni));\n-            match retain_map.get(&ni) {\n-                Some(&v) => return v,\n-                None => ni = parent(ni),\n-            }\n-        }\n-    };\n-\n-    // Now add in the edges into the graph.\n-    for (&target, sources) in &edges {\n-        if let Some(&r_target) = retain_map.get(&target) {\n-            debug!(\"adding edges that target `{:?}`\", in_graph.node_data(target));\n-            for &source in sources {\n-                debug!(\"new edge `{:?} -rf-> {:?}`\",\n-                       in_graph.node_data(source),\n-                       in_graph.node_data(target));\n-                let r_source = retained_parent(source);\n-\n-                // NB. In the input graph, we have `a -> b` if b\n-                // **reads from** a. But in the terminology of this\n-                // code, we would describe that edge as `b -> a`,\n-                // because we have edges *from* outputs *to* inputs.\n-                // Therefore, when we create our new graph, we have to\n-                // reverse the edge.\n-                new_graph.add_edge(r_target, r_source, ());\n-            }\n-        } else {\n-            assert_eq!(sources.len(), 1);\n-        }\n-    }\n-\n-    // One complication. In some cases, output nodes *may* participate in\n-    // cycles. An example:\n-    //\n-    //             [HIR0]                    [HIR1]\n-    //               |                         |\n-    //               v                         v\n-    //      TypeckClosureBody(X) -> ItemSignature(X::SomeClosureInX)\n-    //            |  ^                         | |\n-    //            |  +-------------------------+ |\n-    //            |                              |\n-    //            v                              v\n-    //           Foo                            Bar\n-    //\n-    // In these cases, the output node may not wind up as the head\n-    // of the cycle, in which case it would be absent from the\n-    // final graph. We don't wish this to happen, therefore we go\n-    // over the list of output nodes again and check for any that\n-    // are not their own cycle-head. If we find such a node, we\n-    // add it to the graph now with an edge from the cycle head.\n-    // So the graph above could get transformed into this:\n-    //\n-    //                                    [HIR0, HIR1]\n-    //                                         |\n-    //                                         v\n-    //      TypeckClosureBody(X)    ItemSignature(X::SomeClosureInX)\n-    //               ^                         | |\n-    //               +-------------------------+ |\n-    //                                           v\n-    //                                       [Foo, Bar]\n-    //\n-    // (Note that all the edges here are \"read-by\" edges, not\n-    // \"reads-from\" edges.)\n-    for &output_node in &output_nodes {\n-        let head = r.cycle_head(output_node);\n-        if output_node == head {\n-            assert!(retain_map.contains_key(&output_node));\n-        } else {\n-            assert!(!retain_map.contains_key(&output_node));\n-            let output_data = in_graph.node_data(output_node);\n-            let new_node = new_graph.add_node(output_data);\n-            let new_head_node = retain_map[&head];\n-            new_graph.add_edge(new_head_node, new_node, ());\n-        }\n-    }\n-\n-    // Finally, prepare a list of the input node indices as found in\n-    // the new graph. Note that since all input nodes are leaves in\n-    // the graph, they should never participate in a cycle.\n-    let input_nodes =\n-        input_nodes.iter()\n-                   .map(|&n| {\n-                       assert_eq!(r.cycle_head(n), n, \"input node participating in a cycle\");\n-                       retain_map[&n]\n-                   })\n-                   .collect();\n-\n-    Reduction { graph: new_graph, input_nodes: input_nodes }\n-}\n-"}, {"sha": "a286862e9551f9d9a468e9c6f26d12157173c939", "filename": "src/librustc_incremental/persist/preds/compress/dag_id.rs", "status": "removed", "additions": 0, "deletions": 43, "changes": 43, "blob_url": "https://github.com/rust-lang/rust/blob/272c324b25ebbb05d5de4f55268ded976d863210/src%2Flibrustc_incremental%2Fpersist%2Fpreds%2Fcompress%2Fdag_id.rs", "raw_url": "https://github.com/rust-lang/rust/raw/272c324b25ebbb05d5de4f55268ded976d863210/src%2Flibrustc_incremental%2Fpersist%2Fpreds%2Fcompress%2Fdag_id.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_incremental%2Fpersist%2Fpreds%2Fcompress%2Fdag_id.rs?ref=272c324b25ebbb05d5de4f55268ded976d863210", "patch": "@@ -1,43 +0,0 @@\n-// Copyright 2014 The Rust Project Developers. See the COPYRIGHT\n-// file at the top-level directory of this distribution and at\n-// http://rust-lang.org/COPYRIGHT.\n-//\n-// Licensed under the Apache License, Version 2.0 <LICENSE-APACHE or\n-// http://www.apache.org/licenses/LICENSE-2.0> or the MIT license\n-// <LICENSE-MIT or http://opensource.org/licenses/MIT>, at your\n-// option. This file may not be copied, modified, or distributed\n-// except according to those terms.\n-\n-use rustc_data_structures::graph::NodeIndex;\n-use rustc_data_structures::unify::UnifyKey;\n-\n-#[derive(Copy, Clone, Debug, PartialEq, Eq, Hash)]\n-pub struct DagId {\n-    index: u32,\n-}\n-\n-impl DagId {\n-    pub fn from_input_index(n: NodeIndex) -> Self {\n-        DagId { index: n.0 as u32 }\n-    }\n-\n-    pub fn as_input_index(&self) -> NodeIndex {\n-        NodeIndex(self.index as usize)\n-    }\n-}\n-\n-impl UnifyKey for DagId {\n-    type Value = ();\n-\n-    fn index(&self) -> u32 {\n-        self.index\n-    }\n-\n-    fn from_index(u: u32) -> Self {\n-        DagId { index: u }\n-    }\n-\n-    fn tag(_: Option<Self>) -> &'static str {\n-        \"DagId\"\n-    }\n-}"}, {"sha": "974a2221a4575bf6f35107c044a81c03492b97dc", "filename": "src/librustc_incremental/persist/preds/compress/mod.rs", "status": "removed", "additions": 0, "deletions": 125, "changes": 125, "blob_url": "https://github.com/rust-lang/rust/blob/272c324b25ebbb05d5de4f55268ded976d863210/src%2Flibrustc_incremental%2Fpersist%2Fpreds%2Fcompress%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/272c324b25ebbb05d5de4f55268ded976d863210/src%2Flibrustc_incremental%2Fpersist%2Fpreds%2Fcompress%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_incremental%2Fpersist%2Fpreds%2Fcompress%2Fmod.rs?ref=272c324b25ebbb05d5de4f55268ded976d863210", "patch": "@@ -1,125 +0,0 @@\n-// Copyright 2012-2013 The Rust Project Developers. See the COPYRIGHT\n-// file at the top-level directory of this distribution and at\n-// http://rust-lang.org/COPYRIGHT.\n-//\n-// Licensed under the Apache License, Version 2.0 <LICENSE-APACHE or\n-// http://www.apache.org/licenses/LICENSE-2.0> or the MIT license\n-// <LICENSE-MIT or http://opensource.org/licenses/MIT>, at your\n-// option. This file may not be copied, modified, or distributed\n-// except according to those terms.\n-\n-//! Graph compression. See `README.md`.\n-\n-use rustc_data_structures::graph::{Graph, NodeIndex};\n-use rustc_data_structures::unify::UnificationTable;\n-use std::fmt::Debug;\n-\n-#[cfg(test)]\n-#[macro_use]\n-mod test_macro;\n-\n-mod construct;\n-\n-mod classify;\n-use self::classify::Classify;\n-\n-mod dag_id;\n-use self::dag_id::DagId;\n-\n-#[cfg(test)]\n-mod test;\n-\n-pub fn reduce_graph<N, I, O>(graph: &Graph<N, ()>,\n-                             is_input: I,\n-                             is_output: O) -> Reduction<N>\n-    where N: Debug + Clone,\n-          I: Fn(&N) -> bool,\n-          O: Fn(&N) -> bool,\n-{\n-    GraphReduce::new(graph, is_input, is_output).compute()\n-}\n-\n-pub struct Reduction<'q, N> where N: 'q + Debug + Clone {\n-    pub graph: Graph<&'q N, ()>,\n-    pub input_nodes: Vec<NodeIndex>,\n-}\n-\n-struct GraphReduce<'q, N, I, O>\n-    where N: 'q + Debug + Clone,\n-          I: Fn(&N) -> bool,\n-          O: Fn(&N) -> bool,\n-{\n-    in_graph: &'q Graph<N, ()>,\n-    unify: UnificationTable<DagId>,\n-    is_input: I,\n-    is_output: O,\n-}\n-\n-struct Dag {\n-    // The \"parent\" of a node is the node which reached it during the\n-    // initial DFS. To encode the case of \"no parent\" (i.e., for the\n-    // roots of the walk), we make `parents[i] == i` to start, which\n-    // turns out be convenient.\n-    parents: Vec<NodeIndex>,\n-\n-    // Additional edges beyond the parents.\n-    cross_edges: Vec<(NodeIndex, NodeIndex)>,\n-\n-    // Nodes which we found that are considered \"outputs\"\n-    output_nodes: Vec<NodeIndex>,\n-\n-    // Nodes which we found that are considered \"inputs\"\n-    input_nodes: Vec<NodeIndex>,\n-}\n-\n-#[derive(Copy, Clone, PartialEq, Eq, Hash)]\n-struct DagNode {\n-    in_index: NodeIndex\n-}\n-\n-impl<'q, N, I, O> GraphReduce<'q, N, I, O>\n-    where N: Debug + Clone,\n-          I: Fn(&N) -> bool,\n-          O: Fn(&N) -> bool,\n-{\n-    fn new(in_graph: &'q Graph<N, ()>, is_input: I, is_output: O) -> Self {\n-        let mut unify = UnificationTable::new();\n-\n-        // create a set of unification keys whose indices\n-        // correspond to the indices from the input graph\n-        for i in 0..in_graph.len_nodes() {\n-            let k = unify.new_key(());\n-            assert!(k == DagId::from_input_index(NodeIndex(i)));\n-        }\n-\n-        GraphReduce { in_graph, unify, is_input, is_output }\n-    }\n-\n-    fn compute(mut self) -> Reduction<'q, N> {\n-        let dag = Classify::new(&mut self).walk();\n-        construct::construct_graph(&mut self, dag)\n-    }\n-\n-    fn inputs(&self, in_node: NodeIndex) -> impl Iterator<Item = NodeIndex> + 'q {\n-        self.in_graph.predecessor_nodes(in_node)\n-    }\n-\n-    fn mark_cycle(&mut self, in_node1: NodeIndex, in_node2: NodeIndex) {\n-        let dag_id1 = DagId::from_input_index(in_node1);\n-        let dag_id2 = DagId::from_input_index(in_node2);\n-        self.unify.union(dag_id1, dag_id2);\n-    }\n-\n-    /// Convert a dag-id into its cycle head representative. This will\n-    /// be a no-op unless `in_node` participates in a cycle, in which\n-    /// case a distinct node *may* be returned.\n-    fn cycle_head(&mut self, in_node: NodeIndex) -> NodeIndex {\n-        let i = DagId::from_input_index(in_node);\n-        self.unify.find(i).as_input_index()\n-    }\n-\n-    #[cfg(test)]\n-    fn in_cycle(&mut self, ni1: NodeIndex, ni2: NodeIndex) -> bool {\n-        self.cycle_head(ni1) == self.cycle_head(ni2)\n-    }\n-}"}, {"sha": "1c5130845a855ed9c75d313e707a6d8f912b0c11", "filename": "src/librustc_incremental/persist/preds/compress/test.rs", "status": "removed", "additions": 0, "deletions": 259, "changes": 259, "blob_url": "https://github.com/rust-lang/rust/blob/272c324b25ebbb05d5de4f55268ded976d863210/src%2Flibrustc_incremental%2Fpersist%2Fpreds%2Fcompress%2Ftest.rs", "raw_url": "https://github.com/rust-lang/rust/raw/272c324b25ebbb05d5de4f55268ded976d863210/src%2Flibrustc_incremental%2Fpersist%2Fpreds%2Fcompress%2Ftest.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_incremental%2Fpersist%2Fpreds%2Fcompress%2Ftest.rs?ref=272c324b25ebbb05d5de4f55268ded976d863210", "patch": "@@ -1,259 +0,0 @@\n-// Copyright 2014 The Rust Project Developers. See the COPYRIGHT\n-// file at the top-level directory of this distribution and at\n-// http://rust-lang.org/COPYRIGHT.\n-//\n-// Licensed under the Apache License, Version 2.0 <LICENSE-APACHE or\n-// http://www.apache.org/licenses/LICENSE-2.0> or the MIT license\n-// <LICENSE-MIT or http://opensource.org/licenses/MIT>, at your\n-// option. This file may not be copied, modified, or distributed\n-// except according to those terms.\n-\n-use super::*;\n-\n-fn reduce(graph: &Graph<&'static str, ()>,\n-          inputs: &[&'static str],\n-          outputs: &[&'static str],\n-          expected: &[&'static str])\n-{\n-    let reduce = GraphReduce::new(&graph,\n-                                  |n| inputs.contains(n),\n-                                  |n| outputs.contains(n));\n-    let result = reduce.compute();\n-    let mut edges: Vec<String> =\n-        result.graph\n-              .all_edges()\n-              .iter()\n-              .map(|edge| format!(\"{} -> {}\",\n-                                  result.graph.node_data(edge.source()),\n-                                  result.graph.node_data(edge.target())))\n-              .collect();\n-    edges.sort();\n-    println!(\"{:#?}\", edges);\n-    assert_eq!(edges.len(), expected.len());\n-    for (expected, actual) in expected.iter().zip(&edges) {\n-        assert_eq!(expected, actual);\n-    }\n-}\n-\n-#[test]\n-fn test1() {\n-    //  +---------------+\n-    //  |               |\n-    //  |      +--------|------+\n-    //  |      |        v      v\n-    // [A] -> [C0] -> [C1]    [D]\n-    //        [  ] <- [  ] -> [E]\n-    //                  ^\n-    // [B] -------------+\n-    let (graph, _nodes) = graph! {\n-        A -> C0,\n-        A -> C1,\n-        B -> C1,\n-        C0 -> C1,\n-        C1 -> C0,\n-        C0 -> D,\n-        C1 -> E,\n-    };\n-\n-    // [A] -> [C1] -> [D]\n-    // [B] -> [  ] -> [E]\n-    reduce(&graph, &[\"A\", \"B\"], &[\"D\", \"E\"], &[\n-        \"A -> C1\",\n-        \"B -> C1\",\n-        \"C1 -> D\",\n-        \"C1 -> E\",\n-    ]);\n-}\n-\n-#[test]\n-fn test2() {\n-    //  +---------------+\n-    //  |               |\n-    //  |      +--------|------+\n-    //  |      |        v      v\n-    // [A] -> [C0] -> [C1]    [D] -> [E]\n-    //        [  ] <- [  ]\n-    //                  ^\n-    // [B] -------------+\n-    let (graph, _nodes) = graph! {\n-        A -> C0,\n-        A -> C1,\n-        B -> C1,\n-        C0 -> C1,\n-        C1 -> C0,\n-        C0 -> D,\n-        D -> E,\n-    };\n-\n-    // [A] -> [D] -> [E]\n-    // [B] -> [ ]\n-    reduce(&graph, &[\"A\", \"B\"], &[\"D\", \"E\"], &[\n-        \"A -> D\",\n-        \"B -> D\",\n-        \"D -> E\",\n-    ]);\n-}\n-\n-#[test]\n-fn test2b() {\n-    // Variant on test2 in which [B] is not\n-    // considered an input.\n-    let (graph, _nodes) = graph! {\n-        A -> C0,\n-        A -> C1,\n-        B -> C1,\n-        C0 -> C1,\n-        C1 -> C0,\n-        C0 -> D,\n-        D -> E,\n-    };\n-\n-    // [A] -> [D] -> [E]\n-    reduce(&graph, &[\"A\"], &[\"D\", \"E\"], &[\n-        \"A -> D\",\n-        \"D -> E\",\n-    ]);\n-}\n-\n-#[test]\n-fn test3() {\n-\n-    // Edges going *downwards*, so 0, 1 and 2 are inputs,\n-    // while 7, 8, and 9 are outputs.\n-    //\n-    //     0     1   2\n-    //     |      \\ /\n-    //     3---+   |\n-    //     |   |   |\n-    //     |   |   |\n-    //     4   5   6\n-    //      \\ / \\ / \\\n-    //       |   |   |\n-    //       7   8   9\n-    //\n-    // Here the end result removes node 4, instead encoding an edge\n-    // from n3 -> n7, but keeps nodes 5 and 6, as they are common\n-    // inputs to nodes 8/9.\n-\n-    let (graph, _nodes) = graph! {\n-        n0 -> n3,\n-        n3 -> n4,\n-        n3 -> n5,\n-        n4 -> n7,\n-        n5 -> n7,\n-        n5 -> n8,\n-        n1 -> n6,\n-        n2 -> n6,\n-        n6 -> n8,\n-        n6 -> n9,\n-    };\n-\n-    reduce(&graph, &[\"n0\", \"n1\", \"n2\"], &[\"n7\", \"n8\", \"n9\"], &[\n-        \"n0 -> n3\",\n-        \"n1 -> n6\",\n-        \"n2 -> n6\",\n-        \"n3 -> n5\",\n-        \"n3 -> n7\",\n-        \"n5 -> n7\",\n-        \"n5 -> n8\",\n-        \"n6 -> n8\",\n-        \"n6 -> n9\"\n-    ]);\n-}\n-\n-#[test]\n-fn test_cached_dfs_cyclic() {\n-\n-    //    0       1 <---- 2       3\n-    //    ^       |       ^       ^\n-    //    |       v       |       |\n-    //    4 ----> 5 ----> 6 ----> 7\n-    //    ^       ^       ^       ^\n-    //    |       |       |       |\n-    //    8       9      10      11\n-\n-    let (graph, _nodes) = graph! {\n-        // edges from above diagram, in columns, top-to-bottom:\n-        n4 -> n0,\n-        n8 -> n4,\n-        n4 -> n5,\n-        n1 -> n5,\n-        n9 -> n5,\n-        n2 -> n1,\n-        n5 -> n6,\n-        n6 -> n2,\n-        n10 -> n6,\n-        n6 -> n7,\n-        n7 -> n3,\n-        n11 -> n7,\n-    };\n-\n-    //    0       1  2            3\n-    //    ^       ^ /             ^\n-    //    |       |/              |\n-    //    4 ----> 5 --------------+\n-    //    ^       ^ \\             |\n-    //    |       |  \\            |\n-    //    8       9   10         11\n-\n-    reduce(&graph, &[\"n8\", \"n9\", \"n10\", \"n11\"], &[\"n0\", \"n1\", \"n2\", \"n3\"], &[\n-        \"n10 -> n5\",\n-        \"n11 -> n3\",\n-        \"n4 -> n0\",\n-        \"n4 -> n5\",\n-        \"n5 -> n1\",\n-        \"n5 -> n2\",\n-        \"n5 -> n3\",\n-        \"n8 -> n4\",\n-        \"n9 -> n5\"\n-    ]);\n-}\n-\n-/// Demonstrates the case where we don't reduce as much as we could.\n-#[test]\n-fn suboptimal() {\n-    let (graph, _nodes) = graph! {\n-        INPUT0 -> X,\n-        X -> OUTPUT0,\n-        X -> OUTPUT1,\n-    };\n-\n-    reduce(&graph, &[\"INPUT0\"], &[\"OUTPUT0\", \"OUTPUT1\"], &[\n-        \"INPUT0 -> X\",\n-        \"X -> OUTPUT0\",\n-        \"X -> OUTPUT1\"\n-    ]);\n-}\n-\n-#[test]\n-fn test_cycle_output() {\n-    //  +---------------+\n-    //  |               |\n-    //  |      +--------|------+\n-    //  |      |        v      v\n-    // [A] -> [C0] <-> [C1] <- [D]\n-    //                  +----> [E]\n-    //                          ^\n-    // [B] ----------------- ---+\n-    let (graph, _nodes) = graph! {\n-        A -> C0,\n-        A -> C1,\n-        B -> E,\n-        C0 -> C1,\n-        C1 -> C0,\n-        C0 -> D,\n-        C1 -> E,\n-        D -> C1,\n-    };\n-\n-    // [A] -> [C0] --> [D]\n-    //          +----> [E]\n-    //                  ^\n-    // [B] -------------+\n-    reduce(&graph, &[\"A\", \"B\"], &[\"D\", \"E\"], &[\n-        \"A -> C0\",\n-        \"B -> E\",\n-        \"C0 -> D\",\n-        \"C0 -> E\",\n-    ]);\n-}"}, {"sha": "044b143e306250944542e6b7fb12b3c73a090a21", "filename": "src/librustc_incremental/persist/preds/compress/test_macro.rs", "status": "removed", "additions": 0, "deletions": 39, "changes": 39, "blob_url": "https://github.com/rust-lang/rust/blob/272c324b25ebbb05d5de4f55268ded976d863210/src%2Flibrustc_incremental%2Fpersist%2Fpreds%2Fcompress%2Ftest_macro.rs", "raw_url": "https://github.com/rust-lang/rust/raw/272c324b25ebbb05d5de4f55268ded976d863210/src%2Flibrustc_incremental%2Fpersist%2Fpreds%2Fcompress%2Ftest_macro.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_incremental%2Fpersist%2Fpreds%2Fcompress%2Ftest_macro.rs?ref=272c324b25ebbb05d5de4f55268ded976d863210", "patch": "@@ -1,39 +0,0 @@\n-// Copyright 2014 The Rust Project Developers. See the COPYRIGHT\n-// file at the top-level directory of this distribution and at\n-// http://rust-lang.org/COPYRIGHT.\n-//\n-// Licensed under the Apache License, Version 2.0 <LICENSE-APACHE or\n-// http://www.apache.org/licenses/LICENSE-2.0> or the MIT license\n-// <LICENSE-MIT or http://opensource.org/licenses/MIT>, at your\n-// option. This file may not be copied, modified, or distributed\n-// except according to those terms.\n-\n-macro_rules! graph {\n-    ($( $source:ident -> $target:ident, )*) => {\n-        {\n-            use $crate::rustc_data_structures::graph::{Graph, NodeIndex};\n-            use $crate::rustc_data_structures::fx::FxHashMap;\n-\n-            let mut graph = Graph::new();\n-            let mut nodes: FxHashMap<&'static str, NodeIndex> = FxHashMap();\n-\n-            for &name in &[ $(stringify!($source), stringify!($target)),* ] {\n-                let name: &'static str = name;\n-                nodes.entry(name)\n-                     .or_insert_with(|| graph.add_node(name));\n-            }\n-\n-            $(\n-                {\n-                    let source = nodes[&stringify!($source)];\n-                    let target = nodes[&stringify!($target)];\n-                    graph.add_edge(source, target, ());\n-                }\n-            )*\n-\n-            let f = move |name: &'static str| -> NodeIndex { nodes[&name] };\n-\n-            (graph, f)\n-        }\n-    }\n-}"}, {"sha": "a552a27c62af0f9eec9356a9a3e1ed88393d4fbf", "filename": "src/librustc_incremental/persist/preds/mod.rs", "status": "removed", "additions": 0, "deletions": 108, "changes": 108, "blob_url": "https://github.com/rust-lang/rust/blob/272c324b25ebbb05d5de4f55268ded976d863210/src%2Flibrustc_incremental%2Fpersist%2Fpreds%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/272c324b25ebbb05d5de4f55268ded976d863210/src%2Flibrustc_incremental%2Fpersist%2Fpreds%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_incremental%2Fpersist%2Fpreds%2Fmod.rs?ref=272c324b25ebbb05d5de4f55268ded976d863210", "patch": "@@ -1,108 +0,0 @@\n-// Copyright 2012-2015 The Rust Project Developers. See the COPYRIGHT\n-// file at the top-level directory of this distribution and at\n-// http://rust-lang.org/COPYRIGHT.\n-//\n-// Licensed under the Apache License, Version 2.0 <LICENSE-APACHE or\n-// http://www.apache.org/licenses/LICENSE-2.0> or the MIT license\n-// <LICENSE-MIT or http://opensource.org/licenses/MIT>, at your\n-// option. This file may not be copied, modified, or distributed\n-// except according to those terms.\n-\n-use rustc::dep_graph::{DepGraphQuery, DepNode, DepKind};\n-use rustc::ich::Fingerprint;\n-use rustc::ty::TyCtxt;\n-use rustc_data_structures::fx::FxHashMap;\n-use rustc_data_structures::graph::{Graph, NodeIndex};\n-\n-\n-mod compress;\n-\n-/// A data-structure that makes it easy to enumerate the hashable\n-/// predecessors of any given dep-node.\n-pub struct Predecessors<'query> {\n-    // A reduced version of the input graph that contains fewer nodes.\n-    // This is intended to keep all of the base inputs (i.e., HIR\n-    // nodes) and all of the \"work-products\" we may care about\n-    // later. Other nodes may be retained if it keeps the overall size\n-    // of the graph down.\n-    pub reduced_graph: Graph<&'query DepNode, ()>,\n-\n-    // These are output nodes that have no incoming edges. We have to\n-    // track these specially because, when we load the data back up\n-    // again, we want to make sure and recreate these nodes (we want\n-    // to recreate the nodes where all incoming edges are clean; but\n-    // since we ordinarily just serialize edges, we wind up just\n-    // forgetting that bootstrap outputs even exist in that case.)\n-    pub bootstrap_outputs: Vec<&'query DepNode>,\n-\n-    // For the inputs (hir/foreign-metadata), we include hashes.\n-    pub hashes: FxHashMap<&'query DepNode, Fingerprint>,\n-}\n-\n-impl<'q> Predecessors<'q> {\n-    pub fn new(tcx: TyCtxt, query: &'q DepGraphQuery) -> Self {\n-        // Find the set of \"start nodes\". These are nodes that we will\n-        // possibly query later.\n-        let is_output = |node: &DepNode| -> bool {\n-            match node.kind {\n-                DepKind::WorkProduct => true,\n-                DepKind::CrateMetadata => {\n-                    // We do *not* create dep-nodes for the current crate's\n-                    // metadata anymore, just for metadata that we import/read\n-                    // from other crates.\n-                    debug_assert!(!node.extract_def_id(tcx).unwrap().is_local());\n-                    false\n-                }\n-                // if -Z query-dep-graph is passed, save more extended data\n-                // to enable better unit testing\n-                DepKind::TypeckTables => tcx.sess.opts.debugging_opts.query_dep_graph,\n-\n-                _ => false,\n-            }\n-        };\n-\n-        // Reduce the graph to the most important nodes.\n-        let compress::Reduction { graph, input_nodes } =\n-            compress::reduce_graph(&query.graph,\n-                                   |n| n.kind.is_input(),\n-                                   |n| is_output(n));\n-\n-        let mut hashes = FxHashMap();\n-        for input_index in input_nodes {\n-            let input = *graph.node_data(input_index);\n-            debug!(\"computing hash for input node `{:?}`\", input);\n-            hashes.entry(input)\n-                  .or_insert_with(|| tcx.dep_graph.fingerprint_of(&input));\n-        }\n-\n-        if tcx.sess.opts.debugging_opts.query_dep_graph {\n-            // Not all inputs might have been reachable from an output node,\n-            // but we still want their hash for our unit tests.\n-            let hir_nodes = query.graph.all_nodes().iter().filter_map(|node| {\n-                match node.data.kind {\n-                    DepKind::Hir => Some(&node.data),\n-                    _ => None,\n-                }\n-            });\n-\n-            for node in hir_nodes {\n-                hashes.entry(node)\n-                      .or_insert_with(|| tcx.dep_graph.fingerprint_of(&node));\n-            }\n-        }\n-\n-        let bootstrap_outputs: Vec<&'q DepNode> =\n-            (0 .. graph.len_nodes())\n-            .map(NodeIndex)\n-            .filter(|&n| graph.incoming_edges(n).next().is_none())\n-            .map(|n| *graph.node_data(n))\n-            .filter(|n| is_output(n))\n-            .collect();\n-\n-        Predecessors {\n-            reduced_graph: graph,\n-            bootstrap_outputs,\n-            hashes,\n-        }\n-    }\n-}"}, {"sha": "4919870fcd5198ee62c5664111fa94bf3dbfb191", "filename": "src/librustc_incremental/persist/save.rs", "status": "modified", "additions": 14, "deletions": 140, "changes": 154, "blob_url": "https://github.com/rust-lang/rust/blob/171c0208570a7ec5caf3d3697706888c93c7650a/src%2Flibrustc_incremental%2Fpersist%2Fsave.rs", "raw_url": "https://github.com/rust-lang/rust/raw/171c0208570a7ec5caf3d3697706888c93c7650a/src%2Flibrustc_incremental%2Fpersist%2Fsave.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_incremental%2Fpersist%2Fsave.rs?ref=171c0208570a7ec5caf3d3697706888c93c7650a", "patch": "@@ -8,7 +8,7 @@\n // option. This file may not be copied, modified, or distributed\n // except according to those terms.\n \n-use rustc::dep_graph::{DepGraph, DepNode};\n+use rustc::dep_graph::DepGraph;\n use rustc::hir::def_id::DefId;\n use rustc::hir::svh::Svh;\n use rustc::ich::Fingerprint;\n@@ -18,16 +18,13 @@ use rustc::ty::TyCtxt;\n use rustc::util::common::time;\n use rustc::util::nodemap::DefIdMap;\n use rustc_data_structures::fx::FxHashMap;\n-use rustc_data_structures::graph;\n-use rustc_data_structures::indexed_vec::{IndexVec, Idx};\n use rustc_serialize::Encodable as RustcEncodable;\n use rustc_serialize::opaque::Encoder;\n use std::io::{self, Cursor, Write};\n use std::fs::{self, File};\n use std::path::PathBuf;\n \n use super::data::*;\n-use super::preds::*;\n use super::fs::*;\n use super::dirty_clean;\n use super::file_format;\n@@ -55,9 +52,6 @@ pub fn save_dep_graph<'a, 'tcx>(tcx: TyCtxt<'a, 'tcx, 'tcx>,\n \n     let mut current_metadata_hashes = FxHashMap();\n \n-    // IMPORTANT: We are saving the metadata hashes *before* the dep-graph,\n-    //            since metadata-encoding might add new entries to the\n-    //            DefIdDirectory (which is saved in the dep-graph file).\n     if sess.opts.debugging_opts.incremental_cc ||\n        sess.opts.debugging_opts.query_dep_graph {\n         save_in(sess,\n@@ -69,24 +63,10 @@ pub fn save_dep_graph<'a, 'tcx>(tcx: TyCtxt<'a, 'tcx, 'tcx>,\n                                            e));\n     }\n \n-    time(sess.time_passes(), \"persist dep-graph (old)\", || {\n-        let query = tcx.dep_graph.query();\n-\n-        if tcx.sess.opts.debugging_opts.incremental_info {\n-            eprintln!(\"incremental: {} nodes in dep-graph\", query.graph.len_nodes());\n-            eprintln!(\"incremental: {} edges in dep-graph\", query.graph.len_edges());\n-        }\n-\n-        let preds = Predecessors::new(tcx, &query);\n+    time(sess.time_passes(), \"persist dep-graph\", || {\n         save_in(sess,\n                 dep_graph_path(sess),\n-                |e| encode_dep_graph(tcx, &preds, e));\n-    });\n-\n-    time(sess.time_passes(), \"persist dep-graph (new)\", || {\n-        save_in(sess,\n-                dep_graph_path_new(sess),\n-                |e| encode_dep_graph_new(tcx, e));\n+                |e| encode_dep_graph(tcx, e));\n     });\n \n     dirty_clean::check_dirty_clean_annotations(tcx);\n@@ -182,9 +162,9 @@ fn save_in<F>(sess: &Session, path_buf: PathBuf, encode: F)\n     }\n }\n \n-fn encode_dep_graph_new(tcx: TyCtxt,\n-                        encoder: &mut Encoder)\n-                        -> io::Result<()> {\n+fn encode_dep_graph(tcx: TyCtxt,\n+                    encoder: &mut Encoder)\n+                    -> io::Result<()> {\n     // First encode the commandline arguments hash\n     tcx.sess.opts.dep_tracking_hash().encode(encoder)?;\n \n@@ -195,118 +175,12 @@ fn encode_dep_graph_new(tcx: TyCtxt,\n     Ok(())\n }\n \n-pub fn encode_dep_graph(tcx: TyCtxt,\n-                        preds: &Predecessors,\n-                        encoder: &mut Encoder)\n-                        -> io::Result<()> {\n-    // First encode the commandline arguments hash\n-    tcx.sess.opts.dep_tracking_hash().encode(encoder)?;\n-\n-    // NB: We rely on this Vec being indexable by reduced_graph's NodeIndex.\n-    let mut nodes: IndexVec<DepNodeIndex, DepNode> = preds\n-        .reduced_graph\n-        .all_nodes()\n-        .iter()\n-        .map(|node| node.data.clone())\n-        .collect();\n-\n-    let mut edge_list_indices = IndexVec::with_capacity(nodes.len());\n-    let mut edge_list_data = Vec::with_capacity(preds.reduced_graph.len_edges());\n-\n-    for node_index in 0 .. nodes.len() {\n-        let start = edge_list_data.len() as u32;\n-\n-        for target in preds.reduced_graph.successor_nodes(graph::NodeIndex(node_index)) {\n-            edge_list_data.push(DepNodeIndex::new(target.node_id()));\n-        }\n-\n-        let end = edge_list_data.len() as u32;\n-        debug_assert_eq!(node_index, edge_list_indices.len());\n-        edge_list_indices.push((start, end));\n-    }\n-\n-    // Let's make sure we had no overflow there.\n-    assert!(edge_list_data.len() <= ::std::u32::MAX as usize);\n-    // Check that we have a consistent number of edges.\n-    assert_eq!(edge_list_data.len(), preds.reduced_graph.len_edges());\n-\n-    let bootstrap_outputs = preds.bootstrap_outputs\n-                                 .iter()\n-                                 .map(|dep_node| (**dep_node).clone())\n-                                 .collect();\n-\n-    // Next, build the map of content hashes. To this end, we need to transform\n-    // the (DepNode -> Fingerprint) map that we have into a\n-    // (DepNodeIndex -> Fingerprint) map. This may necessitate adding nodes back\n-    // to the dep-graph that have been filtered out during reduction.\n-    let content_hashes = {\n-        // We have to build a (DepNode -> DepNodeIndex) map. We over-allocate a\n-        // little because we expect some more nodes to be added.\n-        let capacity = (nodes.len() * 120) / 100;\n-        let mut node_to_index = FxHashMap::with_capacity_and_hasher(capacity,\n-                                                                    Default::default());\n-        // Add the nodes we already have in the graph.\n-        node_to_index.extend(nodes.iter_enumerated()\n-                                  .map(|(index, &node)| (node, index)));\n-\n-        let mut content_hashes = Vec::with_capacity(preds.hashes.len());\n-\n-        for (&&dep_node, &hash) in preds.hashes.iter() {\n-            let dep_node_index = *node_to_index\n-                .entry(dep_node)\n-                .or_insert_with(|| {\n-                    // There is no DepNodeIndex for this DepNode yet. This\n-                    // happens when the DepNode got filtered out during graph\n-                    // reduction. Since we have a content hash for the DepNode,\n-                    // we add it back to the graph.\n-                    let next_index = nodes.len();\n-                    nodes.push(dep_node);\n-\n-                    debug_assert_eq!(next_index, edge_list_indices.len());\n-                    // Push an empty list of edges\n-                    edge_list_indices.push((0,0));\n-\n-                    DepNodeIndex::new(next_index)\n-                });\n-\n-            content_hashes.push((dep_node_index, hash));\n-        }\n-\n-        content_hashes\n-    };\n-\n-    let graph = SerializedDepGraph {\n-        nodes,\n-        edge_list_indices,\n-        edge_list_data,\n-        bootstrap_outputs,\n-        hashes: content_hashes,\n-    };\n-\n-    // Encode the graph data.\n-    graph.encode(encoder)?;\n-\n-    if tcx.sess.opts.debugging_opts.incremental_info {\n-        eprintln!(\"incremental: {} nodes in reduced dep-graph\", graph.nodes.len());\n-        eprintln!(\"incremental: {} edges in serialized dep-graph\", graph.edge_list_data.len());\n-        eprintln!(\"incremental: {} hashes in serialized dep-graph\", graph.hashes.len());\n-    }\n-\n-    if tcx.sess.opts.debugging_opts.incremental_dump_hash {\n-        for (dep_node, hash) in &preds.hashes {\n-            println!(\"ICH for {:?} is {}\", dep_node, hash);\n-        }\n-    }\n-\n-    Ok(())\n-}\n-\n-pub fn encode_metadata_hashes(tcx: TyCtxt,\n-                              svh: Svh,\n-                              metadata_hashes: &EncodedMetadataHashes,\n-                              current_metadata_hashes: &mut FxHashMap<DefId, Fingerprint>,\n-                              encoder: &mut Encoder)\n-                              -> io::Result<()> {\n+fn encode_metadata_hashes(tcx: TyCtxt,\n+                          svh: Svh,\n+                          metadata_hashes: &EncodedMetadataHashes,\n+                          current_metadata_hashes: &mut FxHashMap<DefId, Fingerprint>,\n+                          encoder: &mut Encoder)\n+                          -> io::Result<()> {\n     assert_eq!(metadata_hashes.hashes.len(),\n         metadata_hashes.hashes.iter().map(|x| (x.def_index, ())).collect::<FxHashMap<_,_>>().len());\n \n@@ -338,8 +212,8 @@ pub fn encode_metadata_hashes(tcx: TyCtxt,\n     Ok(())\n }\n \n-pub fn encode_work_products(dep_graph: &DepGraph,\n-                            encoder: &mut Encoder) -> io::Result<()> {\n+fn encode_work_products(dep_graph: &DepGraph,\n+                        encoder: &mut Encoder) -> io::Result<()> {\n     let work_products: Vec<_> = dep_graph\n         .work_products()\n         .iter()"}]}