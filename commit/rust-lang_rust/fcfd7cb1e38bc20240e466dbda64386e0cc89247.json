{"sha": "fcfd7cb1e38bc20240e466dbda64386e0cc89247", "node_id": "MDY6Q29tbWl0NzI0NzEyOmZjZmQ3Y2IxZTM4YmMyMDI0MGU0NjZkYmRhNjQzODZlMGNjODkyNDc=", "commit": {"author": {"name": "kjeremy", "email": "kjeremy@gmail.com", "date": "2020-07-24T21:55:17Z"}, "committer": {"name": "Jeremy Kolb", "email": "kjeremy@gmail.com", "date": "2020-08-01T00:57:53Z"}, "message": "Handle semantic token deltas", "tree": {"sha": "f0e4b61091d5bf165b3321644c7a9bc3a5547439", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/f0e4b61091d5bf165b3321644c7a9bc3a5547439"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/fcfd7cb1e38bc20240e466dbda64386e0cc89247", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/fcfd7cb1e38bc20240e466dbda64386e0cc89247", "html_url": "https://github.com/rust-lang/rust/commit/fcfd7cb1e38bc20240e466dbda64386e0cc89247", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/fcfd7cb1e38bc20240e466dbda64386e0cc89247/comments", "author": {"login": "kjeremy", "id": 4325700, "node_id": "MDQ6VXNlcjQzMjU3MDA=", "avatar_url": "https://avatars.githubusercontent.com/u/4325700?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kjeremy", "html_url": "https://github.com/kjeremy", "followers_url": "https://api.github.com/users/kjeremy/followers", "following_url": "https://api.github.com/users/kjeremy/following{/other_user}", "gists_url": "https://api.github.com/users/kjeremy/gists{/gist_id}", "starred_url": "https://api.github.com/users/kjeremy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kjeremy/subscriptions", "organizations_url": "https://api.github.com/users/kjeremy/orgs", "repos_url": "https://api.github.com/users/kjeremy/repos", "events_url": "https://api.github.com/users/kjeremy/events{/privacy}", "received_events_url": "https://api.github.com/users/kjeremy/received_events", "type": "User", "site_admin": false}, "committer": {"login": "kjeremy", "id": 4325700, "node_id": "MDQ6VXNlcjQzMjU3MDA=", "avatar_url": "https://avatars.githubusercontent.com/u/4325700?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kjeremy", "html_url": "https://github.com/kjeremy", "followers_url": "https://api.github.com/users/kjeremy/followers", "following_url": "https://api.github.com/users/kjeremy/following{/other_user}", "gists_url": "https://api.github.com/users/kjeremy/gists{/gist_id}", "starred_url": "https://api.github.com/users/kjeremy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kjeremy/subscriptions", "organizations_url": "https://api.github.com/users/kjeremy/orgs", "repos_url": "https://api.github.com/users/kjeremy/repos", "events_url": "https://api.github.com/users/kjeremy/events{/privacy}", "received_events_url": "https://api.github.com/users/kjeremy/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "2346a28c638dc8fe945059b68126d268dd7fb690", "url": "https://api.github.com/repos/rust-lang/rust/commits/2346a28c638dc8fe945059b68126d268dd7fb690", "html_url": "https://github.com/rust-lang/rust/commit/2346a28c638dc8fe945059b68126d268dd7fb690"}], "stats": {"total": 226, "additions": 213, "deletions": 13}, "files": [{"sha": "92a743fd8e7b5f028f8707b7c49f237e9c943907", "filename": "crates/rust-analyzer/src/caps.rs", "status": "modified", "additions": 3, "deletions": 1, "changes": 4, "blob_url": "https://github.com/rust-lang/rust/blob/fcfd7cb1e38bc20240e466dbda64386e0cc89247/crates%2Frust-analyzer%2Fsrc%2Fcaps.rs", "raw_url": "https://github.com/rust-lang/rust/raw/fcfd7cb1e38bc20240e466dbda64386e0cc89247/crates%2Frust-analyzer%2Fsrc%2Fcaps.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Frust-analyzer%2Fsrc%2Fcaps.rs?ref=fcfd7cb1e38bc20240e466dbda64386e0cc89247", "patch": "@@ -76,7 +76,9 @@ pub fn server_capabilities(client_caps: &ClientCapabilities) -> ServerCapabiliti\n                     token_modifiers: semantic_tokens::SUPPORTED_MODIFIERS.to_vec(),\n                 },\n \n-                document_provider: Some(SemanticTokensDocumentProvider::Bool(true)),\n+                document_provider: Some(SemanticTokensDocumentProvider::Edits {\n+                    edits: Some(true),\n+                }),\n                 range_provider: Some(true),\n                 work_done_progress_options: Default::default(),\n             }"}, {"sha": "e882c9865c0b0daa3a0c78c2783df284b8fb47ab", "filename": "crates/rust-analyzer/src/document.rs", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "blob_url": "https://github.com/rust-lang/rust/blob/fcfd7cb1e38bc20240e466dbda64386e0cc89247/crates%2Frust-analyzer%2Fsrc%2Fdocument.rs", "raw_url": "https://github.com/rust-lang/rust/raw/fcfd7cb1e38bc20240e466dbda64386e0cc89247/crates%2Frust-analyzer%2Fsrc%2Fdocument.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Frust-analyzer%2Fsrc%2Fdocument.rs?ref=fcfd7cb1e38bc20240e466dbda64386e0cc89247", "patch": "@@ -1,9 +1,9 @@\n //! In-memory document information.\n \n /// Information about a document that the Language Client\n-// knows about.\n-// Its lifetime is driven by the textDocument/didOpen and textDocument/didClose\n-// client notifications.\n+/// knows about.\n+/// Its lifetime is driven by the textDocument/didOpen and textDocument/didClose\n+/// client notifications.\n #[derive(Debug, Clone)]\n pub(crate) struct DocumentData {\n     pub version: Option<i64>,"}, {"sha": "4b34c3ec5f77414e2d8ef773a5ffac361e63a370", "filename": "crates/rust-analyzer/src/global_state.rs", "status": "modified", "additions": 9, "deletions": 2, "changes": 11, "blob_url": "https://github.com/rust-lang/rust/blob/fcfd7cb1e38bc20240e466dbda64386e0cc89247/crates%2Frust-analyzer%2Fsrc%2Fglobal_state.rs", "raw_url": "https://github.com/rust-lang/rust/raw/fcfd7cb1e38bc20240e466dbda64386e0cc89247/crates%2Frust-analyzer%2Fsrc%2Fglobal_state.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Frust-analyzer%2Fsrc%2Fglobal_state.rs?ref=fcfd7cb1e38bc20240e466dbda64386e0cc89247", "patch": "@@ -3,11 +3,14 @@\n //!\n //! Each tick provides an immutable snapshot of the state as `WorldSnapshot`.\n \n-use std::{sync::Arc, time::Instant};\n+use std::{\n+    sync::{Arc, Mutex},\n+    time::Instant,\n+};\n \n use crossbeam_channel::{unbounded, Receiver, Sender};\n use flycheck::FlycheckHandle;\n-use lsp_types::Url;\n+use lsp_types::{SemanticTokens, Url};\n use parking_lot::RwLock;\n use ra_db::{CrateId, VfsPath};\n use ra_ide::{Analysis, AnalysisChange, AnalysisHost, FileId};\n@@ -71,6 +74,7 @@ pub(crate) struct GlobalState {\n     pub(crate) analysis_host: AnalysisHost,\n     pub(crate) diagnostics: DiagnosticCollection,\n     pub(crate) mem_docs: FxHashMap<VfsPath, DocumentData>,\n+    pub(crate) semantic_tokens_cache: Arc<Mutex<FxHashMap<Url, SemanticTokens>>>,\n     pub(crate) vfs: Arc<RwLock<(vfs::Vfs, FxHashMap<FileId, LineEndings>)>>,\n     pub(crate) status: Status,\n     pub(crate) source_root_config: SourceRootConfig,\n@@ -86,6 +90,7 @@ pub(crate) struct GlobalStateSnapshot {\n     pub(crate) check_fixes: CheckFixes,\n     pub(crate) latest_requests: Arc<RwLock<LatestRequests>>,\n     mem_docs: FxHashMap<VfsPath, DocumentData>,\n+    pub semantic_tokens_cache: Arc<Mutex<FxHashMap<Url, SemanticTokens>>>,\n     vfs: Arc<RwLock<(vfs::Vfs, FxHashMap<FileId, LineEndings>)>>,\n     pub(crate) workspaces: Arc<Vec<ProjectWorkspace>>,\n }\n@@ -120,6 +125,7 @@ impl GlobalState {\n             analysis_host,\n             diagnostics: Default::default(),\n             mem_docs: FxHashMap::default(),\n+            semantic_tokens_cache: Arc::new(Default::default()),\n             vfs: Arc::new(RwLock::new((vfs::Vfs::default(), FxHashMap::default()))),\n             status: Status::default(),\n             source_root_config: SourceRootConfig::default(),\n@@ -186,6 +192,7 @@ impl GlobalState {\n             latest_requests: Arc::clone(&self.latest_requests),\n             check_fixes: Arc::clone(&self.diagnostics.check_fixes),\n             mem_docs: self.mem_docs.clone(),\n+            semantic_tokens_cache: Arc::clone(&self.semantic_tokens_cache),\n         }\n     }\n "}, {"sha": "0b0ea23fdc3797be715b0dc852043ee5635d76c6", "filename": "crates/rust-analyzer/src/handlers.rs", "status": "modified", "additions": 41, "deletions": 3, "changes": 44, "blob_url": "https://github.com/rust-lang/rust/blob/fcfd7cb1e38bc20240e466dbda64386e0cc89247/crates%2Frust-analyzer%2Fsrc%2Fhandlers.rs", "raw_url": "https://github.com/rust-lang/rust/raw/fcfd7cb1e38bc20240e466dbda64386e0cc89247/crates%2Frust-analyzer%2Fsrc%2Fhandlers.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Frust-analyzer%2Fsrc%2Fhandlers.rs?ref=fcfd7cb1e38bc20240e466dbda64386e0cc89247", "patch": "@@ -13,9 +13,10 @@ use lsp_types::{\n     CallHierarchyOutgoingCall, CallHierarchyOutgoingCallsParams, CallHierarchyPrepareParams,\n     CodeActionKind, CodeLens, Command, CompletionItem, Diagnostic, DocumentFormattingParams,\n     DocumentHighlight, DocumentSymbol, FoldingRange, FoldingRangeParams, HoverContents, Location,\n-    Position, PrepareRenameResponse, Range, RenameParams, SemanticTokensParams,\n-    SemanticTokensRangeParams, SemanticTokensRangeResult, SemanticTokensResult, SymbolInformation,\n-    SymbolTag, TextDocumentIdentifier, Url, WorkspaceEdit,\n+    Position, PrepareRenameResponse, Range, RenameParams, SemanticTokensEditResult,\n+    SemanticTokensEditsParams, SemanticTokensParams, SemanticTokensRangeParams,\n+    SemanticTokensRangeResult, SemanticTokensResult, SymbolInformation, SymbolTag,\n+    TextDocumentIdentifier, Url, WorkspaceEdit,\n };\n use ra_ide::{\n     FileId, FilePosition, FileRange, HoverAction, HoverGotoTypeData, NavigationTarget, Query,\n@@ -1184,6 +1185,43 @@ pub(crate) fn handle_semantic_tokens(\n \n     let highlights = snap.analysis.highlight(file_id)?;\n     let semantic_tokens = to_proto::semantic_tokens(&text, &line_index, highlights);\n+\n+    // Unconditionally cache the tokens\n+    snap.semantic_tokens_cache\n+        .lock()\n+        .unwrap()\n+        .insert(params.text_document.uri, semantic_tokens.clone());\n+\n+    Ok(Some(semantic_tokens.into()))\n+}\n+\n+pub(crate) fn handle_semantic_tokens_edits(\n+    snap: GlobalStateSnapshot,\n+    params: SemanticTokensEditsParams,\n+) -> Result<Option<SemanticTokensEditResult>> {\n+    let _p = profile(\"handle_semantic_tokens_edits\");\n+\n+    let file_id = from_proto::file_id(&snap, &params.text_document.uri)?;\n+    let text = snap.analysis.file_text(file_id)?;\n+    let line_index = snap.analysis.file_line_index(file_id)?;\n+\n+    let highlights = snap.analysis.highlight(file_id)?;\n+\n+    let semantic_tokens = to_proto::semantic_tokens(&text, &line_index, highlights);\n+\n+    let mut cache = snap.semantic_tokens_cache.lock().unwrap();\n+    let cached_tokens = cache.entry(params.text_document.uri).or_default();\n+\n+    if let Some(prev_id) = &cached_tokens.result_id {\n+        if *prev_id == params.previous_result_id {\n+            let edits = to_proto::semantic_token_edits(&cached_tokens, &semantic_tokens);\n+            *cached_tokens = semantic_tokens;\n+            return Ok(Some(edits.into()));\n+        }\n+    }\n+\n+    *cached_tokens = semantic_tokens.clone();\n+\n     Ok(Some(semantic_tokens.into()))\n }\n "}, {"sha": "eb2a86972e743dccdecf1bb2d6b1cb21bd79af55", "filename": "crates/rust-analyzer/src/main_loop.rs", "status": "modified", "additions": 5, "deletions": 0, "changes": 5, "blob_url": "https://github.com/rust-lang/rust/blob/fcfd7cb1e38bc20240e466dbda64386e0cc89247/crates%2Frust-analyzer%2Fsrc%2Fmain_loop.rs", "raw_url": "https://github.com/rust-lang/rust/raw/fcfd7cb1e38bc20240e466dbda64386e0cc89247/crates%2Frust-analyzer%2Fsrc%2Fmain_loop.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Frust-analyzer%2Fsrc%2Fmain_loop.rs?ref=fcfd7cb1e38bc20240e466dbda64386e0cc89247", "patch": "@@ -387,6 +387,9 @@ impl GlobalState {\n                 handlers::handle_call_hierarchy_outgoing,\n             )?\n             .on::<lsp_types::request::SemanticTokensRequest>(handlers::handle_semantic_tokens)?\n+            .on::<lsp_types::request::SemanticTokensEditsRequest>(\n+                handlers::handle_semantic_tokens_edits,\n+            )?\n             .on::<lsp_types::request::SemanticTokensRangeRequest>(\n                 handlers::handle_semantic_tokens_range,\n             )?\n@@ -449,6 +452,8 @@ impl GlobalState {\n                         None => log::error!(\"orphan DidCloseTextDocument: {}\", path),\n                     }\n \n+                    this.semantic_tokens_cache.lock().unwrap().remove(&params.text_document.uri);\n+\n                     if let Some(path) = path.as_path() {\n                         this.loader.handle.invalidate(path.to_path_buf());\n                     }"}, {"sha": "afc38fb4e8683de50c1481f9997958731eb8e966", "filename": "crates/rust-analyzer/src/semantic_tokens.rs", "status": "modified", "additions": 136, "deletions": 3, "changes": 139, "blob_url": "https://github.com/rust-lang/rust/blob/fcfd7cb1e38bc20240e466dbda64386e0cc89247/crates%2Frust-analyzer%2Fsrc%2Fsemantic_tokens.rs", "raw_url": "https://github.com/rust-lang/rust/raw/fcfd7cb1e38bc20240e466dbda64386e0cc89247/crates%2Frust-analyzer%2Fsrc%2Fsemantic_tokens.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Frust-analyzer%2Fsrc%2Fsemantic_tokens.rs?ref=fcfd7cb1e38bc20240e466dbda64386e0cc89247", "patch": "@@ -2,7 +2,10 @@\n \n use std::ops;\n \n-use lsp_types::{Range, SemanticToken, SemanticTokenModifier, SemanticTokenType, SemanticTokens};\n+use lsp_types::{\n+    Range, SemanticToken, SemanticTokenModifier, SemanticTokenType, SemanticTokens,\n+    SemanticTokensEdit,\n+};\n \n macro_rules! define_semantic_token_types {\n     ($(($ident:ident, $string:literal)),*$(,)?) => {\n@@ -89,14 +92,18 @@ impl ops::BitOrAssign<SemanticTokenModifier> for ModifierSet {\n /// Tokens are encoded relative to each other.\n ///\n /// This is a direct port of https://github.com/microsoft/vscode-languageserver-node/blob/f425af9de46a0187adb78ec8a46b9b2ce80c5412/server/src/sematicTokens.proposed.ts#L45\n-#[derive(Default)]\n pub(crate) struct SemanticTokensBuilder {\n+    id: String,\n     prev_line: u32,\n     prev_char: u32,\n     data: Vec<SemanticToken>,\n }\n \n impl SemanticTokensBuilder {\n+    pub fn new(id: String) -> Self {\n+        SemanticTokensBuilder { id, prev_line: 0, prev_char: 0, data: Default::default() }\n+    }\n+\n     /// Push a new token onto the builder\n     pub fn push(&mut self, range: Range, token_index: u32, modifier_bitset: u32) {\n         let mut push_line = range.start.line as u32;\n@@ -127,10 +134,136 @@ impl SemanticTokensBuilder {\n     }\n \n     pub fn build(self) -> SemanticTokens {\n-        SemanticTokens { result_id: None, data: self.data }\n+        SemanticTokens { result_id: Some(self.id), data: self.data }\n+    }\n+}\n+\n+pub fn diff_tokens(old: &[SemanticToken], new: &[SemanticToken]) -> Vec<SemanticTokensEdit> {\n+    let offset = new.iter().zip(old.iter()).take_while(|&(n, p)| n == p).count();\n+\n+    let (_, old) = old.split_at(offset);\n+    let (_, new) = new.split_at(offset);\n+\n+    let offset_from_end =\n+        new.iter().rev().zip(old.iter().rev()).take_while(|&(n, p)| n == p).count();\n+\n+    let (old, _) = old.split_at(old.len() - offset_from_end);\n+    let (new, _) = new.split_at(new.len() - offset_from_end);\n+\n+    if old.is_empty() && new.is_empty() {\n+        vec![]\n+    } else {\n+        // The lsp data field is actually a byte-diff but we\n+        // travel in tokens so `start` and `delete_count` are in multiples of the\n+        // serialized size of `SemanticToken`.\n+        vec![SemanticTokensEdit {\n+            start: 5 * offset as u32,\n+            delete_count: 5 * old.len() as u32,\n+            data: Some(new.into()),\n+        }]\n     }\n }\n \n pub fn type_index(type_: SemanticTokenType) -> u32 {\n     SUPPORTED_TYPES.iter().position(|it| *it == type_).unwrap() as u32\n }\n+\n+#[cfg(test)]\n+mod tests {\n+    use super::*;\n+\n+    fn from(t: (u32, u32, u32, u32, u32)) -> SemanticToken {\n+        SemanticToken {\n+            delta_line: t.0,\n+            delta_start: t.1,\n+            length: t.2,\n+            token_type: t.3,\n+            token_modifiers_bitset: t.4,\n+        }\n+    }\n+\n+    #[test]\n+    fn test_diff_insert_at_end() {\n+        let before = [from((1, 2, 3, 4, 5)), from((6, 7, 8, 9, 10))];\n+        let after = [from((1, 2, 3, 4, 5)), from((6, 7, 8, 9, 10)), from((11, 12, 13, 14, 15))];\n+\n+        let edits = diff_tokens(&before, &after);\n+        assert_eq!(\n+            edits[0],\n+            SemanticTokensEdit {\n+                start: 10,\n+                delete_count: 0,\n+                data: Some(vec![from((11, 12, 13, 14, 15))])\n+            }\n+        );\n+    }\n+\n+    #[test]\n+    fn test_diff_insert_at_beginning() {\n+        let before = [from((1, 2, 3, 4, 5)), from((6, 7, 8, 9, 10))];\n+        let after = [from((11, 12, 13, 14, 15)), from((1, 2, 3, 4, 5)), from((6, 7, 8, 9, 10))];\n+\n+        let edits = diff_tokens(&before, &after);\n+        assert_eq!(\n+            edits[0],\n+            SemanticTokensEdit {\n+                start: 0,\n+                delete_count: 0,\n+                data: Some(vec![from((11, 12, 13, 14, 15))])\n+            }\n+        );\n+    }\n+\n+    #[test]\n+    fn test_diff_insert_in_middle() {\n+        let before = [from((1, 2, 3, 4, 5)), from((6, 7, 8, 9, 10))];\n+        let after = [\n+            from((1, 2, 3, 4, 5)),\n+            from((10, 20, 30, 40, 50)),\n+            from((60, 70, 80, 90, 100)),\n+            from((6, 7, 8, 9, 10)),\n+        ];\n+\n+        let edits = diff_tokens(&before, &after);\n+        assert_eq!(\n+            edits[0],\n+            SemanticTokensEdit {\n+                start: 5,\n+                delete_count: 0,\n+                data: Some(vec![from((10, 20, 30, 40, 50)), from((60, 70, 80, 90, 100))])\n+            }\n+        );\n+    }\n+\n+    #[test]\n+    fn test_diff_remove_from_end() {\n+        let before = [from((1, 2, 3, 4, 5)), from((6, 7, 8, 9, 10)), from((11, 12, 13, 14, 15))];\n+        let after = [from((1, 2, 3, 4, 5)), from((6, 7, 8, 9, 10))];\n+\n+        let edits = diff_tokens(&before, &after);\n+        assert_eq!(edits[0], SemanticTokensEdit { start: 10, delete_count: 5, data: Some(vec![]) });\n+    }\n+\n+    #[test]\n+    fn test_diff_remove_from_beginning() {\n+        let before = [from((11, 12, 13, 14, 15)), from((1, 2, 3, 4, 5)), from((6, 7, 8, 9, 10))];\n+        let after = [from((1, 2, 3, 4, 5)), from((6, 7, 8, 9, 10))];\n+\n+        let edits = diff_tokens(&before, &after);\n+        assert_eq!(edits[0], SemanticTokensEdit { start: 0, delete_count: 5, data: Some(vec![]) });\n+    }\n+\n+    #[test]\n+    fn test_diff_remove_from_middle() {\n+        let before = [\n+            from((1, 2, 3, 4, 5)),\n+            from((10, 20, 30, 40, 50)),\n+            from((60, 70, 80, 90, 100)),\n+            from((6, 7, 8, 9, 10)),\n+        ];\n+        let after = [from((1, 2, 3, 4, 5)), from((6, 7, 8, 9, 10))];\n+\n+        let edits = diff_tokens(&before, &after);\n+        assert_eq!(edits[0], SemanticTokensEdit { start: 5, delete_count: 10, data: Some(vec![]) });\n+    }\n+}"}, {"sha": "8da883ae435835354a95e57aef362b6f691dfdbf", "filename": "crates/rust-analyzer/src/to_proto.rs", "status": "modified", "additions": 16, "deletions": 1, "changes": 17, "blob_url": "https://github.com/rust-lang/rust/blob/fcfd7cb1e38bc20240e466dbda64386e0cc89247/crates%2Frust-analyzer%2Fsrc%2Fto_proto.rs", "raw_url": "https://github.com/rust-lang/rust/raw/fcfd7cb1e38bc20240e466dbda64386e0cc89247/crates%2Frust-analyzer%2Fsrc%2Fto_proto.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Frust-analyzer%2Fsrc%2Fto_proto.rs?ref=fcfd7cb1e38bc20240e466dbda64386e0cc89247", "patch": "@@ -1,5 +1,6 @@\n //! Conversion of rust-analyzer specific types to lsp_types equivalents.\n use std::path::{self, Path};\n+use std::time::SystemTime;\n \n use itertools::Itertools;\n use ra_db::{FileId, FileRange};\n@@ -308,7 +309,12 @@ pub(crate) fn semantic_tokens(\n     line_index: &LineIndex,\n     highlights: Vec<HighlightedRange>,\n ) -> lsp_types::SemanticTokens {\n-    let mut builder = semantic_tokens::SemanticTokensBuilder::default();\n+    let id = match SystemTime::now().duration_since(SystemTime::UNIX_EPOCH) {\n+        Ok(d) => d.as_millis().to_string(),\n+        Err(_) => String::new(),\n+    };\n+\n+    let mut builder = semantic_tokens::SemanticTokensBuilder::new(id);\n \n     for highlight_range in highlights {\n         let (type_, mods) = semantic_token_type_and_modifiers(highlight_range.highlight);\n@@ -328,6 +334,15 @@ pub(crate) fn semantic_tokens(\n     builder.build()\n }\n \n+pub(crate) fn semantic_token_edits(\n+    previous: &lsp_types::SemanticTokens,\n+    current: &lsp_types::SemanticTokens,\n+) -> lsp_types::SemanticTokensEdits {\n+    let result_id = current.result_id.clone();\n+    let edits = semantic_tokens::diff_tokens(&previous.data, &current.data);\n+    lsp_types::SemanticTokensEdits { result_id, edits }\n+}\n+\n fn semantic_token_type_and_modifiers(\n     highlight: Highlight,\n ) -> (lsp_types::SemanticTokenType, semantic_tokens::ModifierSet) {"}]}