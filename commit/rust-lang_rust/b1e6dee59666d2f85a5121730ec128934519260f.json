{"sha": "b1e6dee59666d2f85a5121730ec128934519260f", "node_id": "C_kwDOAAsO6NoAKGIxZTZkZWU1OTY2NmQyZjg1YTUxMjE3MzBlYzEyODkzNDUxOTI2MGY", "commit": {"author": {"name": "Nicholas Nethercote", "email": "n.nethercote@gmail.com", "date": "2022-04-19T01:36:13Z"}, "committer": {"name": "Nicholas Nethercote", "email": "n.nethercote@gmail.com", "date": "2022-04-19T07:02:48Z"}, "message": "Merge `TokenCursor::{next,next_desugared}`.\n\nAnd likewise for the inlined variants.\n\nI did this for simplicity, but interesting it was a performance win as\nwell.", "tree": {"sha": "4c0f09d21bf4a53a8aca01e5d772558f75e6463b", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/4c0f09d21bf4a53a8aca01e5d772558f75e6463b"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/b1e6dee59666d2f85a5121730ec128934519260f", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/b1e6dee59666d2f85a5121730ec128934519260f", "html_url": "https://github.com/rust-lang/rust/commit/b1e6dee59666d2f85a5121730ec128934519260f", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/b1e6dee59666d2f85a5121730ec128934519260f/comments", "author": {"login": "nnethercote", "id": 1940286, "node_id": "MDQ6VXNlcjE5NDAyODY=", "avatar_url": "https://avatars.githubusercontent.com/u/1940286?v=4", "gravatar_id": "", "url": "https://api.github.com/users/nnethercote", "html_url": "https://github.com/nnethercote", "followers_url": "https://api.github.com/users/nnethercote/followers", "following_url": "https://api.github.com/users/nnethercote/following{/other_user}", "gists_url": "https://api.github.com/users/nnethercote/gists{/gist_id}", "starred_url": "https://api.github.com/users/nnethercote/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/nnethercote/subscriptions", "organizations_url": "https://api.github.com/users/nnethercote/orgs", "repos_url": "https://api.github.com/users/nnethercote/repos", "events_url": "https://api.github.com/users/nnethercote/events{/privacy}", "received_events_url": "https://api.github.com/users/nnethercote/received_events", "type": "User", "site_admin": false}, "committer": {"login": "nnethercote", "id": 1940286, "node_id": "MDQ6VXNlcjE5NDAyODY=", "avatar_url": "https://avatars.githubusercontent.com/u/1940286?v=4", "gravatar_id": "", "url": "https://api.github.com/users/nnethercote", "html_url": "https://github.com/nnethercote", "followers_url": "https://api.github.com/users/nnethercote/followers", "following_url": "https://api.github.com/users/nnethercote/following{/other_user}", "gists_url": "https://api.github.com/users/nnethercote/gists{/gist_id}", "starred_url": "https://api.github.com/users/nnethercote/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/nnethercote/subscriptions", "organizations_url": "https://api.github.com/users/nnethercote/orgs", "repos_url": "https://api.github.com/users/nnethercote/repos", "events_url": "https://api.github.com/users/nnethercote/events{/privacy}", "received_events_url": "https://api.github.com/users/nnethercote/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "89ec75b0e95a62a2d1ac76f7918a469c7bb228ec", "url": "https://api.github.com/repos/rust-lang/rust/commits/89ec75b0e95a62a2d1ac76f7918a469c7bb228ec", "html_url": "https://github.com/rust-lang/rust/commit/89ec75b0e95a62a2d1ac76f7918a469c7bb228ec"}], "stats": {"total": 156, "additions": 71, "deletions": 85}, "files": [{"sha": "02749088c3139e50e9745dee12243021de5a73ac", "filename": "compiler/rustc_parse/src/parser/attr_wrapper.rs", "status": "modified", "additions": 6, "deletions": 11, "changes": 17, "blob_url": "https://github.com/rust-lang/rust/blob/b1e6dee59666d2f85a5121730ec128934519260f/compiler%2Frustc_parse%2Fsrc%2Fparser%2Fattr_wrapper.rs", "raw_url": "https://github.com/rust-lang/rust/raw/b1e6dee59666d2f85a5121730ec128934519260f/compiler%2Frustc_parse%2Fsrc%2Fparser%2Fattr_wrapper.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_parse%2Fsrc%2Fparser%2Fattr_wrapper.rs?ref=b1e6dee59666d2f85a5121730ec128934519260f", "patch": "@@ -100,21 +100,16 @@ rustc_data_structures::static_assert_size!(LazyTokenStreamImpl, 144);\n \n impl CreateTokenStream for LazyTokenStreamImpl {\n     fn create_token_stream(&self) -> AttrAnnotatedTokenStream {\n-        // The token produced by the final call to `{,inlined_}next` or\n-        // `{,inlined_}next_desugared` was not actually consumed by the\n-        // callback. The combination of chaining the initial token and using\n-        // `take` produces the desired result - we produce an empty\n-        // `TokenStream` if no calls were made, and omit the final token\n-        // otherwise.\n+        // The token produced by the final call to `{,inlined_}next` was not\n+        // actually consumed by the callback. The combination of chaining the\n+        // initial token and using `take` produces the desired result - we\n+        // produce an empty `TokenStream` if no calls were made, and omit the\n+        // final token otherwise.\n         let mut cursor_snapshot = self.cursor_snapshot.clone();\n         let tokens =\n             std::iter::once((FlatToken::Token(self.start_token.0.clone()), self.start_token.1))\n                 .chain((0..self.num_calls).map(|_| {\n-                    let token = if cursor_snapshot.desugar_doc_comments {\n-                        cursor_snapshot.next_desugared()\n-                    } else {\n-                        cursor_snapshot.next()\n-                    };\n+                    let token = cursor_snapshot.next(cursor_snapshot.desugar_doc_comments);\n                     (FlatToken::Token(token.0), token.1)\n                 }))\n                 .take(self.num_calls);"}, {"sha": "b6f4cd119e00c84b5cde6abfb8605c3d6b404f39", "filename": "compiler/rustc_parse/src/parser/mod.rs", "status": "modified", "additions": 65, "deletions": 74, "changes": 139, "blob_url": "https://github.com/rust-lang/rust/blob/b1e6dee59666d2f85a5121730ec128934519260f/compiler%2Frustc_parse%2Fsrc%2Fparser%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/b1e6dee59666d2f85a5121730ec128934519260f/compiler%2Frustc_parse%2Fsrc%2Fparser%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_parse%2Fsrc%2Fparser%2Fmod.rs?ref=b1e6dee59666d2f85a5121730ec128934519260f", "patch": "@@ -206,9 +206,7 @@ struct TokenCursor {\n     frame: TokenCursorFrame,\n     stack: Vec<TokenCursorFrame>,\n     desugar_doc_comments: bool,\n-    // Counts the number of calls to `{,inlined_}next` or\n-    // `{,inlined_}next_desugared`, depending on whether\n-    // `desugar_doc_comments` is set.\n+    // Counts the number of calls to `{,inlined_}next`.\n     num_next_calls: usize,\n     // During parsing, we may sometimes need to 'unglue' a\n     // glued token into two component tokens\n@@ -256,14 +254,14 @@ impl TokenCursorFrame {\n }\n \n impl TokenCursor {\n-    fn next(&mut self) -> (Token, Spacing) {\n-        self.inlined_next()\n+    fn next(&mut self, desugar_doc_comments: bool) -> (Token, Spacing) {\n+        self.inlined_next(desugar_doc_comments)\n     }\n \n     /// This always-inlined version should only be used on hot code paths.\n     #[inline(always)]\n-    fn inlined_next(&mut self) -> (Token, Spacing) {\n-        loop {\n+    fn inlined_next(&mut self, desugar_doc_comments: bool) -> (Token, Spacing) {\n+        let (token, spacing) = loop {\n             let (tree, spacing) = if !self.frame.open_delim {\n                 self.frame.open_delim = true;\n                 TokenTree::token(token::OpenDelim(self.frame.delim), self.frame.span.open).into()\n@@ -281,77 +279,74 @@ impl TokenCursor {\n \n             match tree {\n                 TokenTree::Token(token) => {\n-                    return (token, spacing);\n+                    break (token, spacing);\n                 }\n                 TokenTree::Delimited(sp, delim, tts) => {\n                     let frame = TokenCursorFrame::new(sp, delim, tts);\n                     self.stack.push(mem::replace(&mut self.frame, frame));\n                 }\n             }\n-        }\n-    }\n+        };\n \n-    fn next_desugared(&mut self) -> (Token, Spacing) {\n-        self.inlined_next_desugared()\n-    }\n+        match (desugar_doc_comments, &token) {\n+            (true, &Token { kind: token::DocComment(_, attr_style, data), span }) => {\n+                // Searches for the occurrences of `\"#*` and returns the minimum number of `#`s\n+                // required to wrap the text.\n+                let mut num_of_hashes = 0;\n+                let mut count = 0;\n+                for ch in data.as_str().chars() {\n+                    count = match ch {\n+                        '\"' => 1,\n+                        '#' if count > 0 => count + 1,\n+                        _ => 0,\n+                    };\n+                    num_of_hashes = cmp::max(num_of_hashes, count);\n+                }\n \n-    /// This always-inlined version should only be used on hot code paths.\n-    #[inline(always)]\n-    fn inlined_next_desugared(&mut self) -> (Token, Spacing) {\n-        let (data, attr_style, sp) = match self.inlined_next() {\n-            (Token { kind: token::DocComment(_, attr_style, data), span }, _) => {\n-                (data, attr_style, span)\n+                let delim_span = DelimSpan::from_single(span);\n+                let body = TokenTree::Delimited(\n+                    delim_span,\n+                    token::Bracket,\n+                    [\n+                        TokenTree::token(token::Ident(sym::doc, false), span),\n+                        TokenTree::token(token::Eq, span),\n+                        TokenTree::token(\n+                            TokenKind::lit(token::StrRaw(num_of_hashes), data, None),\n+                            span,\n+                        ),\n+                    ]\n+                    .iter()\n+                    .cloned()\n+                    .collect::<TokenStream>(),\n+                );\n+\n+                self.stack.push(mem::replace(\n+                    &mut self.frame,\n+                    TokenCursorFrame::new(\n+                        delim_span,\n+                        token::NoDelim,\n+                        if attr_style == AttrStyle::Inner {\n+                            [\n+                                TokenTree::token(token::Pound, span),\n+                                TokenTree::token(token::Not, span),\n+                                body,\n+                            ]\n+                            .iter()\n+                            .cloned()\n+                            .collect::<TokenStream>()\n+                        } else {\n+                            [TokenTree::token(token::Pound, span), body]\n+                                .iter()\n+                                .cloned()\n+                                .collect::<TokenStream>()\n+                        },\n+                    ),\n+                ));\n+\n+                self.next(/* desugar_doc_comments */ false)\n             }\n-            tok => return tok,\n-        };\n-\n-        // Searches for the occurrences of `\"#*` and returns the minimum number of `#`s\n-        // required to wrap the text.\n-        let mut num_of_hashes = 0;\n-        let mut count = 0;\n-        for ch in data.as_str().chars() {\n-            count = match ch {\n-                '\"' => 1,\n-                '#' if count > 0 => count + 1,\n-                _ => 0,\n-            };\n-            num_of_hashes = cmp::max(num_of_hashes, count);\n+            _ => (token, spacing),\n         }\n-\n-        let delim_span = DelimSpan::from_single(sp);\n-        let body = TokenTree::Delimited(\n-            delim_span,\n-            token::Bracket,\n-            [\n-                TokenTree::token(token::Ident(sym::doc, false), sp),\n-                TokenTree::token(token::Eq, sp),\n-                TokenTree::token(TokenKind::lit(token::StrRaw(num_of_hashes), data, None), sp),\n-            ]\n-            .iter()\n-            .cloned()\n-            .collect::<TokenStream>(),\n-        );\n-\n-        self.stack.push(mem::replace(\n-            &mut self.frame,\n-            TokenCursorFrame::new(\n-                delim_span,\n-                token::NoDelim,\n-                if attr_style == AttrStyle::Inner {\n-                    [TokenTree::token(token::Pound, sp), TokenTree::token(token::Not, sp), body]\n-                        .iter()\n-                        .cloned()\n-                        .collect::<TokenStream>()\n-                } else {\n-                    [TokenTree::token(token::Pound, sp), body]\n-                        .iter()\n-                        .cloned()\n-                        .collect::<TokenStream>()\n-                },\n-            ),\n-        ));\n-\n-        self.next()\n     }\n }\n \n@@ -1010,11 +1005,7 @@ impl<'a> Parser<'a> {\n     pub fn bump(&mut self) {\n         let fallback_span = self.token.span;\n         loop {\n-            let (mut next, spacing) = if self.desugar_doc_comments {\n-                self.token_cursor.inlined_next_desugared()\n-            } else {\n-                self.token_cursor.inlined_next()\n-            };\n+            let (mut next, spacing) = self.token_cursor.inlined_next(self.desugar_doc_comments);\n             self.token_cursor.num_next_calls += 1;\n             // We've retrieved an token from the underlying\n             // cursor, so we no longer need to worry about\n@@ -1063,7 +1054,7 @@ impl<'a> Parser<'a> {\n         let mut i = 0;\n         let mut token = Token::dummy();\n         while i < dist {\n-            token = cursor.next().0;\n+            token = cursor.next(/* desugar_doc_comments */ false).0;\n             if matches!(\n                 token.kind,\n                 token::OpenDelim(token::NoDelim) | token::CloseDelim(token::NoDelim)"}]}