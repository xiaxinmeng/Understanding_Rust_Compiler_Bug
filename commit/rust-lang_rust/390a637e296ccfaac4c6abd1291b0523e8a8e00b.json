{"sha": "390a637e296ccfaac4c6abd1291b0523e8a8e00b", "node_id": "C_kwDOAAsO6NoAKDM5MGE2MzdlMjk2Y2NmYWFjNGM2YWJkMTI5MWIwNTIzZThhOGUwMGI", "commit": {"author": {"name": "hkalbasi", "email": "hamidrezakalbasi@protonmail.com", "date": "2022-11-06T21:06:11Z"}, "committer": {"name": "hkalbasi", "email": "hamidrezakalbasi@protonmail.com", "date": "2022-11-24T12:56:13Z"}, "message": "move things from rustc_target::abi to rustc_abi", "tree": {"sha": "e142a6d3e6c7619782f0c682242f2f5cb440c1b4", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/e142a6d3e6c7619782f0c682242f2f5cb440c1b4"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/390a637e296ccfaac4c6abd1291b0523e8a8e00b", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/390a637e296ccfaac4c6abd1291b0523e8a8e00b", "html_url": "https://github.com/rust-lang/rust/commit/390a637e296ccfaac4c6abd1291b0523e8a8e00b", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/390a637e296ccfaac4c6abd1291b0523e8a8e00b/comments", "author": {"login": "HKalbasi", "id": 45197576, "node_id": "MDQ6VXNlcjQ1MTk3NTc2", "avatar_url": "https://avatars.githubusercontent.com/u/45197576?v=4", "gravatar_id": "", "url": "https://api.github.com/users/HKalbasi", "html_url": "https://github.com/HKalbasi", "followers_url": "https://api.github.com/users/HKalbasi/followers", "following_url": "https://api.github.com/users/HKalbasi/following{/other_user}", "gists_url": "https://api.github.com/users/HKalbasi/gists{/gist_id}", "starred_url": "https://api.github.com/users/HKalbasi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/HKalbasi/subscriptions", "organizations_url": "https://api.github.com/users/HKalbasi/orgs", "repos_url": "https://api.github.com/users/HKalbasi/repos", "events_url": "https://api.github.com/users/HKalbasi/events{/privacy}", "received_events_url": "https://api.github.com/users/HKalbasi/received_events", "type": "User", "site_admin": false}, "committer": {"login": "HKalbasi", "id": 45197576, "node_id": "MDQ6VXNlcjQ1MTk3NTc2", "avatar_url": "https://avatars.githubusercontent.com/u/45197576?v=4", "gravatar_id": "", "url": "https://api.github.com/users/HKalbasi", "html_url": "https://github.com/HKalbasi", "followers_url": "https://api.github.com/users/HKalbasi/followers", "following_url": "https://api.github.com/users/HKalbasi/following{/other_user}", "gists_url": "https://api.github.com/users/HKalbasi/gists{/gist_id}", "starred_url": "https://api.github.com/users/HKalbasi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/HKalbasi/subscriptions", "organizations_url": "https://api.github.com/users/HKalbasi/orgs", "repos_url": "https://api.github.com/users/HKalbasi/repos", "events_url": "https://api.github.com/users/HKalbasi/events{/privacy}", "received_events_url": "https://api.github.com/users/HKalbasi/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "27fb904d680996fe48e04aef65d4d655bdab843b", "url": "https://api.github.com/repos/rust-lang/rust/commits/27fb904d680996fe48e04aef65d4d655bdab843b", "html_url": "https://github.com/rust-lang/rust/commit/27fb904d680996fe48e04aef65d4d655bdab843b"}], "stats": {"total": 3373, "additions": 1700, "deletions": 1673}, "files": [{"sha": "d8612b3a2561b62989edaf3a72b6f8a580c1e2d7", "filename": "Cargo.lock", "status": "modified", "additions": 15, "deletions": 4, "changes": 19, "blob_url": "https://github.com/rust-lang/rust/blob/390a637e296ccfaac4c6abd1291b0523e8a8e00b/Cargo.lock", "raw_url": "https://github.com/rust-lang/rust/raw/390a637e296ccfaac4c6abd1291b0523e8a8e00b/Cargo.lock", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/Cargo.lock?ref=390a637e296ccfaac4c6abd1291b0523e8a8e00b", "patch": "@@ -3202,6 +3202,20 @@ dependencies = [\n  \"winapi\",\n ]\n \n+[[package]]\n+name = \"rustc_abi\"\n+version = \"0.0.0\"\n+dependencies = [\n+ \"bitflags\",\n+ \"rand 0.8.5\",\n+ \"rand_xoshiro\",\n+ \"rustc_data_structures\",\n+ \"rustc_index\",\n+ \"rustc_macros\",\n+ \"rustc_serialize\",\n+ \"tracing\",\n+]\n+\n [[package]]\n name = \"rustc_apfloat\"\n version = \"0.0.0\"\n@@ -4281,8 +4295,7 @@ name = \"rustc_target\"\n version = \"0.0.0\"\n dependencies = [\n  \"bitflags\",\n- \"rand 0.8.5\",\n- \"rand_xoshiro\",\n+ \"rustc_abi\",\n  \"rustc_data_structures\",\n  \"rustc_feature\",\n  \"rustc_index\",\n@@ -4363,8 +4376,6 @@ dependencies = [\n name = \"rustc_ty_utils\"\n version = \"0.0.0\"\n dependencies = [\n- \"rand 0.8.5\",\n- \"rand_xoshiro\",\n  \"rustc_data_structures\",\n  \"rustc_errors\",\n  \"rustc_hir\","}, {"sha": "48b199cb8eed9dad92c33e20ffb6390b2062992b", "filename": "compiler/rustc_abi/Cargo.toml", "status": "added", "additions": 24, "deletions": 0, "changes": 24, "blob_url": "https://github.com/rust-lang/rust/blob/390a637e296ccfaac4c6abd1291b0523e8a8e00b/compiler%2Frustc_abi%2FCargo.toml", "raw_url": "https://github.com/rust-lang/rust/raw/390a637e296ccfaac4c6abd1291b0523e8a8e00b/compiler%2Frustc_abi%2FCargo.toml", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_abi%2FCargo.toml?ref=390a637e296ccfaac4c6abd1291b0523e8a8e00b", "patch": "@@ -0,0 +1,24 @@\n+[package]\n+name = \"rustc_abi\"\n+version = \"0.0.0\"\n+edition = \"2021\"\n+\n+[dependencies]\n+bitflags = \"1.2.1\"\n+tracing = \"0.1\"\n+rand = { version = \"0.8.4\", default-features = false, optional = true }\n+rand_xoshiro = { version = \"0.6.0\", optional = true }\n+rustc_data_structures = { path = \"../rustc_data_structures\", optional = true  }\n+rustc_index = { path = \"../rustc_index\", default-features = false }\n+rustc_macros = { path = \"../rustc_macros\", optional = true }\n+rustc_serialize = { path = \"../rustc_serialize\", optional = true  }\n+\n+[features]\n+default = [\"nightly\", \"randomize\"]\n+randomize = [\"rand\", \"rand_xoshiro\"]\n+nightly = [\n+    \"rustc_data_structures\",\n+    \"rustc_index/nightly\",\n+    \"rustc_macros\",\n+    \"rustc_serialize\",\n+]"}, {"sha": "39ea7a85be652b74d61ad9ac40f293b508856e5c", "filename": "compiler/rustc_abi/src/layout.rs", "status": "renamed", "additions": 12, "deletions": 8, "changes": 20, "blob_url": "https://github.com/rust-lang/rust/blob/390a637e296ccfaac4c6abd1291b0523e8a8e00b/compiler%2Frustc_abi%2Fsrc%2Flayout.rs", "raw_url": "https://github.com/rust-lang/rust/raw/390a637e296ccfaac4c6abd1291b0523e8a8e00b/compiler%2Frustc_abi%2Fsrc%2Flayout.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_abi%2Fsrc%2Flayout.rs?ref=390a637e296ccfaac4c6abd1291b0523e8a8e00b", "patch": "@@ -7,7 +7,9 @@ use std::{\n     ops::{Bound, Deref},\n };\n \n+#[cfg(feature = \"randomize\")]\n use rand::{seq::SliceRandom, SeedableRng};\n+#[cfg(feature = \"randomize\")]\n use rand_xoshiro::Xoshiro128StarStar;\n \n use tracing::debug;\n@@ -91,14 +93,16 @@ pub trait LayoutCalculator {\n             // If `-Z randomize-layout` was enabled for the type definition we can shuffle\n             // the field ordering to try and catch some code making assumptions about layouts\n             // we don't guarantee\n-            if repr.can_randomize_type_layout() {\n-                // `ReprOptions.layout_seed` is a deterministic seed that we can use to\n-                // randomize field ordering with\n-                let mut rng = Xoshiro128StarStar::seed_from_u64(repr.field_shuffle_seed);\n-\n-                // Shuffle the ordering of the fields\n-                optimizing.shuffle(&mut rng);\n+            if repr.can_randomize_type_layout() && cfg!(feature = \"randomize\") {\n+                #[cfg(feature = \"randomize\")]\n+                {\n+                    // `ReprOptions.layout_seed` is a deterministic seed that we can use to\n+                    // randomize field ordering with\n+                    let mut rng = Xoshiro128StarStar::seed_from_u64(repr.field_shuffle_seed);\n \n+                    // Shuffle the ordering of the fields\n+                    optimizing.shuffle(&mut rng);\n+                }\n                 // Otherwise we just leave things alone and actually optimize the type's fields\n             } else {\n                 match kind {\n@@ -900,7 +904,7 @@ pub trait LayoutCalculator {\n         let mut abi = Abi::Aggregate { sized: true };\n         let index = V::new(0);\n         for field in &variants[index] {\n-            assert!(!field.is_unsized());\n+            assert!(field.is_sized());\n             align = align.max(field.align);\n \n             // If all non-ZST fields have the same ABI, forward this ABI", "previous_filename": "compiler/rustc_target/src/abi/layout.rs"}, {"sha": "4f4a4bf314f14f76fa5422dbe9e6baaa9c7c92a8", "filename": "compiler/rustc_abi/src/lib.rs", "status": "added", "additions": 1399, "deletions": 0, "changes": 1399, "blob_url": "https://github.com/rust-lang/rust/blob/390a637e296ccfaac4c6abd1291b0523e8a8e00b/compiler%2Frustc_abi%2Fsrc%2Flib.rs", "raw_url": "https://github.com/rust-lang/rust/raw/390a637e296ccfaac4c6abd1291b0523e8a8e00b/compiler%2Frustc_abi%2Fsrc%2Flib.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_abi%2Fsrc%2Flib.rs?ref=390a637e296ccfaac4c6abd1291b0523e8a8e00b", "patch": "@@ -0,0 +1,1399 @@\n+#![cfg_attr(feature = \"nightly\", feature(step_trait, rustc_attrs, min_specialization))]\n+\n+use std::convert::{TryFrom, TryInto};\n+use std::fmt;\n+#[cfg(feature = \"nightly\")]\n+use std::iter::Step;\n+use std::num::{NonZeroUsize, ParseIntError};\n+use std::ops::{Add, AddAssign, Mul, RangeInclusive, Sub};\n+use std::str::FromStr;\n+\n+use bitflags::bitflags;\n+use rustc_index::vec::{Idx, IndexVec};\n+#[cfg(feature = \"nightly\")]\n+use rustc_macros::HashStable_Generic;\n+#[cfg(feature = \"nightly\")]\n+use rustc_macros::{Decodable, Encodable};\n+\n+mod layout;\n+\n+pub use layout::LayoutCalculator;\n+\n+/// Requirements for a `StableHashingContext` to be used in this crate.\n+/// This is a hack to allow using the `HashStable_Generic` derive macro\n+/// instead of implementing everything in `rustc_middle`.\n+pub trait HashStableContext {}\n+\n+use Integer::*;\n+use Primitive::*;\n+\n+bitflags! {\n+    #[derive(Default)]\n+    #[cfg_attr(feature = \"nightly\", derive(Encodable, Decodable, HashStable_Generic))]\n+    pub struct ReprFlags: u8 {\n+        const IS_C               = 1 << 0;\n+        const IS_SIMD            = 1 << 1;\n+        const IS_TRANSPARENT     = 1 << 2;\n+        // Internal only for now. If true, don't reorder fields.\n+        const IS_LINEAR          = 1 << 3;\n+        // If true, the type's layout can be randomized using\n+        // the seed stored in `ReprOptions.layout_seed`\n+        const RANDOMIZE_LAYOUT   = 1 << 4;\n+        // Any of these flags being set prevent field reordering optimisation.\n+        const IS_UNOPTIMISABLE   = ReprFlags::IS_C.bits\n+                                 | ReprFlags::IS_SIMD.bits\n+                                 | ReprFlags::IS_LINEAR.bits;\n+    }\n+}\n+\n+#[derive(Copy, Clone, Debug, Eq, PartialEq)]\n+#[cfg_attr(feature = \"nightly\", derive(Encodable, Decodable, HashStable_Generic))]\n+pub enum IntegerType {\n+    /// Pointer sized integer type, i.e. isize and usize. The field shows signedness, that\n+    /// is, `Pointer(true)` is isize.\n+    Pointer(bool),\n+    /// Fix sized integer type, e.g. i8, u32, i128 The bool field shows signedness, `Fixed(I8, false)` means `u8`\n+    Fixed(Integer, bool),\n+}\n+\n+impl IntegerType {\n+    pub fn is_signed(&self) -> bool {\n+        match self {\n+            IntegerType::Pointer(b) => *b,\n+            IntegerType::Fixed(_, b) => *b,\n+        }\n+    }\n+}\n+\n+/// Represents the repr options provided by the user,\n+#[derive(Copy, Clone, Debug, Eq, PartialEq, Default)]\n+#[cfg_attr(feature = \"nightly\", derive(Encodable, Decodable, HashStable_Generic))]\n+pub struct ReprOptions {\n+    pub int: Option<IntegerType>,\n+    pub align: Option<Align>,\n+    pub pack: Option<Align>,\n+    pub flags: ReprFlags,\n+    /// The seed to be used for randomizing a type's layout\n+    ///\n+    /// Note: This could technically be a `[u8; 16]` (a `u128`) which would\n+    /// be the \"most accurate\" hash as it'd encompass the item and crate\n+    /// hash without loss, but it does pay the price of being larger.\n+    /// Everything's a tradeoff, a `u64` seed should be sufficient for our\n+    /// purposes (primarily `-Z randomize-layout`)\n+    pub field_shuffle_seed: u64,\n+}\n+\n+impl ReprOptions {\n+    #[inline]\n+    pub fn simd(&self) -> bool {\n+        self.flags.contains(ReprFlags::IS_SIMD)\n+    }\n+\n+    #[inline]\n+    pub fn c(&self) -> bool {\n+        self.flags.contains(ReprFlags::IS_C)\n+    }\n+\n+    #[inline]\n+    pub fn packed(&self) -> bool {\n+        self.pack.is_some()\n+    }\n+\n+    #[inline]\n+    pub fn transparent(&self) -> bool {\n+        self.flags.contains(ReprFlags::IS_TRANSPARENT)\n+    }\n+\n+    #[inline]\n+    pub fn linear(&self) -> bool {\n+        self.flags.contains(ReprFlags::IS_LINEAR)\n+    }\n+\n+    /// Returns the discriminant type, given these `repr` options.\n+    /// This must only be called on enums!\n+    pub fn discr_type(&self) -> IntegerType {\n+        self.int.unwrap_or(IntegerType::Pointer(true))\n+    }\n+\n+    /// Returns `true` if this `#[repr()]` should inhabit \"smart enum\n+    /// layout\" optimizations, such as representing `Foo<&T>` as a\n+    /// single pointer.\n+    pub fn inhibit_enum_layout_opt(&self) -> bool {\n+        self.c() || self.int.is_some()\n+    }\n+\n+    /// Returns `true` if this `#[repr()]` should inhibit struct field reordering\n+    /// optimizations, such as with `repr(C)`, `repr(packed(1))`, or `repr(<int>)`.\n+    pub fn inhibit_struct_field_reordering_opt(&self) -> bool {\n+        if let Some(pack) = self.pack {\n+            if pack.bytes() == 1 {\n+                return true;\n+            }\n+        }\n+\n+        self.flags.intersects(ReprFlags::IS_UNOPTIMISABLE) || self.int.is_some()\n+    }\n+\n+    /// Returns `true` if this type is valid for reordering and `-Z randomize-layout`\n+    /// was enabled for its declaration crate\n+    pub fn can_randomize_type_layout(&self) -> bool {\n+        !self.inhibit_struct_field_reordering_opt()\n+            && self.flags.contains(ReprFlags::RANDOMIZE_LAYOUT)\n+    }\n+\n+    /// Returns `true` if this `#[repr()]` should inhibit union ABI optimisations.\n+    pub fn inhibit_union_abi_opt(&self) -> bool {\n+        self.c()\n+    }\n+}\n+\n+/// Parsed [Data layout](https://llvm.org/docs/LangRef.html#data-layout)\n+/// for a target, which contains everything needed to compute layouts.\n+#[derive(Debug, PartialEq, Eq)]\n+pub struct TargetDataLayout {\n+    pub endian: Endian,\n+    pub i1_align: AbiAndPrefAlign,\n+    pub i8_align: AbiAndPrefAlign,\n+    pub i16_align: AbiAndPrefAlign,\n+    pub i32_align: AbiAndPrefAlign,\n+    pub i64_align: AbiAndPrefAlign,\n+    pub i128_align: AbiAndPrefAlign,\n+    pub f32_align: AbiAndPrefAlign,\n+    pub f64_align: AbiAndPrefAlign,\n+    pub pointer_size: Size,\n+    pub pointer_align: AbiAndPrefAlign,\n+    pub aggregate_align: AbiAndPrefAlign,\n+\n+    /// Alignments for vector types.\n+    pub vector_align: Vec<(Size, AbiAndPrefAlign)>,\n+\n+    pub instruction_address_space: AddressSpace,\n+\n+    /// Minimum size of #[repr(C)] enums (default I32 bits)\n+    pub c_enum_min_size: Integer,\n+}\n+\n+impl Default for TargetDataLayout {\n+    /// Creates an instance of `TargetDataLayout`.\n+    fn default() -> TargetDataLayout {\n+        let align = |bits| Align::from_bits(bits).unwrap();\n+        TargetDataLayout {\n+            endian: Endian::Big,\n+            i1_align: AbiAndPrefAlign::new(align(8)),\n+            i8_align: AbiAndPrefAlign::new(align(8)),\n+            i16_align: AbiAndPrefAlign::new(align(16)),\n+            i32_align: AbiAndPrefAlign::new(align(32)),\n+            i64_align: AbiAndPrefAlign { abi: align(32), pref: align(64) },\n+            i128_align: AbiAndPrefAlign { abi: align(32), pref: align(64) },\n+            f32_align: AbiAndPrefAlign::new(align(32)),\n+            f64_align: AbiAndPrefAlign::new(align(64)),\n+            pointer_size: Size::from_bits(64),\n+            pointer_align: AbiAndPrefAlign::new(align(64)),\n+            aggregate_align: AbiAndPrefAlign { abi: align(0), pref: align(64) },\n+            vector_align: vec![\n+                (Size::from_bits(64), AbiAndPrefAlign::new(align(64))),\n+                (Size::from_bits(128), AbiAndPrefAlign::new(align(128))),\n+            ],\n+            instruction_address_space: AddressSpace::DATA,\n+            c_enum_min_size: Integer::I32,\n+        }\n+    }\n+}\n+\n+pub enum TargetDataLayoutErrors<'a> {\n+    InvalidAddressSpace { addr_space: &'a str, cause: &'a str, err: ParseIntError },\n+    InvalidBits { kind: &'a str, bit: &'a str, cause: &'a str, err: ParseIntError },\n+    MissingAlignment { cause: &'a str },\n+    InvalidAlignment { cause: &'a str, err: String },\n+    InconsistentTargetArchitecture { dl: &'a str, target: &'a str },\n+    InconsistentTargetPointerWidth { pointer_size: u64, target: u32 },\n+    InvalidBitsSize { err: String },\n+}\n+\n+impl TargetDataLayout {\n+    /// Returns exclusive upper bound on object size.\n+    ///\n+    /// The theoretical maximum object size is defined as the maximum positive `isize` value.\n+    /// This ensures that the `offset` semantics remain well-defined by allowing it to correctly\n+    /// index every address within an object along with one byte past the end, along with allowing\n+    /// `isize` to store the difference between any two pointers into an object.\n+    ///\n+    /// The upper bound on 64-bit currently needs to be lower because LLVM uses a 64-bit integer\n+    /// to represent object size in bits. It would need to be 1 << 61 to account for this, but is\n+    /// currently conservatively bounded to 1 << 47 as that is enough to cover the current usable\n+    /// address space on 64-bit ARMv8 and x86_64.\n+    #[inline]\n+    pub fn obj_size_bound(&self) -> u64 {\n+        match self.pointer_size.bits() {\n+            16 => 1 << 15,\n+            32 => 1 << 31,\n+            64 => 1 << 47,\n+            bits => panic!(\"obj_size_bound: unknown pointer bit size {}\", bits),\n+        }\n+    }\n+\n+    #[inline]\n+    pub fn ptr_sized_integer(&self) -> Integer {\n+        match self.pointer_size.bits() {\n+            16 => I16,\n+            32 => I32,\n+            64 => I64,\n+            bits => panic!(\"ptr_sized_integer: unknown pointer bit size {}\", bits),\n+        }\n+    }\n+\n+    #[inline]\n+    pub fn vector_align(&self, vec_size: Size) -> AbiAndPrefAlign {\n+        for &(size, align) in &self.vector_align {\n+            if size == vec_size {\n+                return align;\n+            }\n+        }\n+        // Default to natural alignment, which is what LLVM does.\n+        // That is, use the size, rounded up to a power of 2.\n+        AbiAndPrefAlign::new(Align::from_bytes(vec_size.bytes().next_power_of_two()).unwrap())\n+    }\n+}\n+\n+pub trait HasDataLayout {\n+    fn data_layout(&self) -> &TargetDataLayout;\n+}\n+\n+impl HasDataLayout for TargetDataLayout {\n+    #[inline]\n+    fn data_layout(&self) -> &TargetDataLayout {\n+        self\n+    }\n+}\n+\n+/// Endianness of the target, which must match cfg(target-endian).\n+#[derive(Copy, Clone, PartialEq, Eq)]\n+pub enum Endian {\n+    Little,\n+    Big,\n+}\n+\n+impl Endian {\n+    pub fn as_str(&self) -> &'static str {\n+        match self {\n+            Self::Little => \"little\",\n+            Self::Big => \"big\",\n+        }\n+    }\n+}\n+\n+impl fmt::Debug for Endian {\n+    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {\n+        f.write_str(self.as_str())\n+    }\n+}\n+\n+impl FromStr for Endian {\n+    type Err = String;\n+\n+    fn from_str(s: &str) -> Result<Self, Self::Err> {\n+        match s {\n+            \"little\" => Ok(Self::Little),\n+            \"big\" => Ok(Self::Big),\n+            _ => Err(format!(r#\"unknown endian: \"{}\"\"#, s)),\n+        }\n+    }\n+}\n+\n+/// Size of a type in bytes.\n+#[derive(Copy, Clone, PartialEq, Eq, PartialOrd, Ord, Hash)]\n+#[cfg_attr(feature = \"nightly\", derive(Encodable, Decodable, HashStable_Generic))]\n+pub struct Size {\n+    raw: u64,\n+}\n+\n+// This is debug-printed a lot in larger structs, don't waste too much space there\n+impl fmt::Debug for Size {\n+    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {\n+        write!(f, \"Size({} bytes)\", self.bytes())\n+    }\n+}\n+\n+impl Size {\n+    pub const ZERO: Size = Size { raw: 0 };\n+\n+    /// Rounds `bits` up to the next-higher byte boundary, if `bits` is\n+    /// not a multiple of 8.\n+    pub fn from_bits(bits: impl TryInto<u64>) -> Size {\n+        let bits = bits.try_into().ok().unwrap();\n+        // Avoid potential overflow from `bits + 7`.\n+        Size { raw: bits / 8 + ((bits % 8) + 7) / 8 }\n+    }\n+\n+    #[inline]\n+    pub fn from_bytes(bytes: impl TryInto<u64>) -> Size {\n+        let bytes: u64 = bytes.try_into().ok().unwrap();\n+        Size { raw: bytes }\n+    }\n+\n+    #[inline]\n+    pub fn bytes(self) -> u64 {\n+        self.raw\n+    }\n+\n+    #[inline]\n+    pub fn bytes_usize(self) -> usize {\n+        self.bytes().try_into().unwrap()\n+    }\n+\n+    #[inline]\n+    pub fn bits(self) -> u64 {\n+        #[cold]\n+        fn overflow(bytes: u64) -> ! {\n+            panic!(\"Size::bits: {} bytes in bits doesn't fit in u64\", bytes)\n+        }\n+\n+        self.bytes().checked_mul(8).unwrap_or_else(|| overflow(self.bytes()))\n+    }\n+\n+    #[inline]\n+    pub fn bits_usize(self) -> usize {\n+        self.bits().try_into().unwrap()\n+    }\n+\n+    #[inline]\n+    pub fn align_to(self, align: Align) -> Size {\n+        let mask = align.bytes() - 1;\n+        Size::from_bytes((self.bytes() + mask) & !mask)\n+    }\n+\n+    #[inline]\n+    pub fn is_aligned(self, align: Align) -> bool {\n+        let mask = align.bytes() - 1;\n+        self.bytes() & mask == 0\n+    }\n+\n+    #[inline]\n+    pub fn checked_add<C: HasDataLayout>(self, offset: Size, cx: &C) -> Option<Size> {\n+        let dl = cx.data_layout();\n+\n+        let bytes = self.bytes().checked_add(offset.bytes())?;\n+\n+        if bytes < dl.obj_size_bound() { Some(Size::from_bytes(bytes)) } else { None }\n+    }\n+\n+    #[inline]\n+    pub fn checked_mul<C: HasDataLayout>(self, count: u64, cx: &C) -> Option<Size> {\n+        let dl = cx.data_layout();\n+\n+        let bytes = self.bytes().checked_mul(count)?;\n+        if bytes < dl.obj_size_bound() { Some(Size::from_bytes(bytes)) } else { None }\n+    }\n+\n+    /// Truncates `value` to `self` bits and then sign-extends it to 128 bits\n+    /// (i.e., if it is negative, fill with 1's on the left).\n+    #[inline]\n+    pub fn sign_extend(self, value: u128) -> u128 {\n+        let size = self.bits();\n+        if size == 0 {\n+            // Truncated until nothing is left.\n+            return 0;\n+        }\n+        // Sign-extend it.\n+        let shift = 128 - size;\n+        // Shift the unsigned value to the left, then shift back to the right as signed\n+        // (essentially fills with sign bit on the left).\n+        (((value << shift) as i128) >> shift) as u128\n+    }\n+\n+    /// Truncates `value` to `self` bits.\n+    #[inline]\n+    pub fn truncate(self, value: u128) -> u128 {\n+        let size = self.bits();\n+        if size == 0 {\n+            // Truncated until nothing is left.\n+            return 0;\n+        }\n+        let shift = 128 - size;\n+        // Truncate (shift left to drop out leftover values, shift right to fill with zeroes).\n+        (value << shift) >> shift\n+    }\n+\n+    #[inline]\n+    pub fn signed_int_min(&self) -> i128 {\n+        self.sign_extend(1_u128 << (self.bits() - 1)) as i128\n+    }\n+\n+    #[inline]\n+    pub fn signed_int_max(&self) -> i128 {\n+        i128::MAX >> (128 - self.bits())\n+    }\n+\n+    #[inline]\n+    pub fn unsigned_int_max(&self) -> u128 {\n+        u128::MAX >> (128 - self.bits())\n+    }\n+}\n+\n+// Panicking addition, subtraction and multiplication for convenience.\n+// Avoid during layout computation, return `LayoutError` instead.\n+\n+impl Add for Size {\n+    type Output = Size;\n+    #[inline]\n+    fn add(self, other: Size) -> Size {\n+        Size::from_bytes(self.bytes().checked_add(other.bytes()).unwrap_or_else(|| {\n+            panic!(\"Size::add: {} + {} doesn't fit in u64\", self.bytes(), other.bytes())\n+        }))\n+    }\n+}\n+\n+impl Sub for Size {\n+    type Output = Size;\n+    #[inline]\n+    fn sub(self, other: Size) -> Size {\n+        Size::from_bytes(self.bytes().checked_sub(other.bytes()).unwrap_or_else(|| {\n+            panic!(\"Size::sub: {} - {} would result in negative size\", self.bytes(), other.bytes())\n+        }))\n+    }\n+}\n+\n+impl Mul<Size> for u64 {\n+    type Output = Size;\n+    #[inline]\n+    fn mul(self, size: Size) -> Size {\n+        size * self\n+    }\n+}\n+\n+impl Mul<u64> for Size {\n+    type Output = Size;\n+    #[inline]\n+    fn mul(self, count: u64) -> Size {\n+        match self.bytes().checked_mul(count) {\n+            Some(bytes) => Size::from_bytes(bytes),\n+            None => panic!(\"Size::mul: {} * {} doesn't fit in u64\", self.bytes(), count),\n+        }\n+    }\n+}\n+\n+impl AddAssign for Size {\n+    #[inline]\n+    fn add_assign(&mut self, other: Size) {\n+        *self = *self + other;\n+    }\n+}\n+\n+#[cfg(feature = \"nightly\")]\n+impl Step for Size {\n+    #[inline]\n+    fn steps_between(start: &Self, end: &Self) -> Option<usize> {\n+        u64::steps_between(&start.bytes(), &end.bytes())\n+    }\n+\n+    #[inline]\n+    fn forward_checked(start: Self, count: usize) -> Option<Self> {\n+        u64::forward_checked(start.bytes(), count).map(Self::from_bytes)\n+    }\n+\n+    #[inline]\n+    fn forward(start: Self, count: usize) -> Self {\n+        Self::from_bytes(u64::forward(start.bytes(), count))\n+    }\n+\n+    #[inline]\n+    unsafe fn forward_unchecked(start: Self, count: usize) -> Self {\n+        Self::from_bytes(u64::forward_unchecked(start.bytes(), count))\n+    }\n+\n+    #[inline]\n+    fn backward_checked(start: Self, count: usize) -> Option<Self> {\n+        u64::backward_checked(start.bytes(), count).map(Self::from_bytes)\n+    }\n+\n+    #[inline]\n+    fn backward(start: Self, count: usize) -> Self {\n+        Self::from_bytes(u64::backward(start.bytes(), count))\n+    }\n+\n+    #[inline]\n+    unsafe fn backward_unchecked(start: Self, count: usize) -> Self {\n+        Self::from_bytes(u64::backward_unchecked(start.bytes(), count))\n+    }\n+}\n+\n+/// Alignment of a type in bytes (always a power of two).\n+#[derive(Copy, Clone, PartialEq, Eq, PartialOrd, Ord, Hash)]\n+#[cfg_attr(feature = \"nightly\", derive(Encodable, Decodable, HashStable_Generic))]\n+pub struct Align {\n+    pow2: u8,\n+}\n+\n+// This is debug-printed a lot in larger structs, don't waste too much space there\n+impl fmt::Debug for Align {\n+    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {\n+        write!(f, \"Align({} bytes)\", self.bytes())\n+    }\n+}\n+\n+impl Align {\n+    pub const ONE: Align = Align { pow2: 0 };\n+    pub const MAX: Align = Align { pow2: 29 };\n+\n+    #[inline]\n+    pub fn from_bits(bits: u64) -> Result<Align, String> {\n+        Align::from_bytes(Size::from_bits(bits).bytes())\n+    }\n+\n+    #[inline]\n+    pub fn from_bytes(align: u64) -> Result<Align, String> {\n+        // Treat an alignment of 0 bytes like 1-byte alignment.\n+        if align == 0 {\n+            return Ok(Align::ONE);\n+        }\n+\n+        #[cold]\n+        fn not_power_of_2(align: u64) -> String {\n+            format!(\"`{}` is not a power of 2\", align)\n+        }\n+\n+        #[cold]\n+        fn too_large(align: u64) -> String {\n+            format!(\"`{}` is too large\", align)\n+        }\n+\n+        let mut bytes = align;\n+        let mut pow2: u8 = 0;\n+        while (bytes & 1) == 0 {\n+            pow2 += 1;\n+            bytes >>= 1;\n+        }\n+        if bytes != 1 {\n+            return Err(not_power_of_2(align));\n+        }\n+        if pow2 > Self::MAX.pow2 {\n+            return Err(too_large(align));\n+        }\n+\n+        Ok(Align { pow2 })\n+    }\n+\n+    #[inline]\n+    pub fn bytes(self) -> u64 {\n+        1 << self.pow2\n+    }\n+\n+    #[inline]\n+    pub fn bits(self) -> u64 {\n+        self.bytes() * 8\n+    }\n+\n+    /// Computes the best alignment possible for the given offset\n+    /// (the largest power of two that the offset is a multiple of).\n+    ///\n+    /// N.B., for an offset of `0`, this happens to return `2^64`.\n+    #[inline]\n+    pub fn max_for_offset(offset: Size) -> Align {\n+        Align { pow2: offset.bytes().trailing_zeros() as u8 }\n+    }\n+\n+    /// Lower the alignment, if necessary, such that the given offset\n+    /// is aligned to it (the offset is a multiple of the alignment).\n+    #[inline]\n+    pub fn restrict_for_offset(self, offset: Size) -> Align {\n+        self.min(Align::max_for_offset(offset))\n+    }\n+}\n+\n+/// A pair of alignments, ABI-mandated and preferred.\n+#[derive(Copy, Clone, PartialEq, Eq, Hash, Debug)]\n+#[cfg_attr(feature = \"nightly\", derive(HashStable_Generic))]\n+\n+pub struct AbiAndPrefAlign {\n+    pub abi: Align,\n+    pub pref: Align,\n+}\n+\n+impl AbiAndPrefAlign {\n+    #[inline]\n+    pub fn new(align: Align) -> AbiAndPrefAlign {\n+        AbiAndPrefAlign { abi: align, pref: align }\n+    }\n+\n+    #[inline]\n+    pub fn min(self, other: AbiAndPrefAlign) -> AbiAndPrefAlign {\n+        AbiAndPrefAlign { abi: self.abi.min(other.abi), pref: self.pref.min(other.pref) }\n+    }\n+\n+    #[inline]\n+    pub fn max(self, other: AbiAndPrefAlign) -> AbiAndPrefAlign {\n+        AbiAndPrefAlign { abi: self.abi.max(other.abi), pref: self.pref.max(other.pref) }\n+    }\n+}\n+\n+/// Integers, also used for enum discriminants.\n+#[derive(Copy, Clone, PartialEq, Eq, PartialOrd, Ord, Hash, Debug)]\n+#[cfg_attr(feature = \"nightly\", derive(Encodable, Decodable, HashStable_Generic))]\n+\n+pub enum Integer {\n+    I8,\n+    I16,\n+    I32,\n+    I64,\n+    I128,\n+}\n+\n+impl Integer {\n+    #[inline]\n+    pub fn size(self) -> Size {\n+        match self {\n+            I8 => Size::from_bytes(1),\n+            I16 => Size::from_bytes(2),\n+            I32 => Size::from_bytes(4),\n+            I64 => Size::from_bytes(8),\n+            I128 => Size::from_bytes(16),\n+        }\n+    }\n+\n+    /// Gets the Integer type from an IntegerType.\n+    pub fn from_attr<C: HasDataLayout>(cx: &C, ity: IntegerType) -> Integer {\n+        let dl = cx.data_layout();\n+\n+        match ity {\n+            IntegerType::Pointer(_) => dl.ptr_sized_integer(),\n+            IntegerType::Fixed(x, _) => x,\n+        }\n+    }\n+\n+    pub fn align<C: HasDataLayout>(self, cx: &C) -> AbiAndPrefAlign {\n+        let dl = cx.data_layout();\n+\n+        match self {\n+            I8 => dl.i8_align,\n+            I16 => dl.i16_align,\n+            I32 => dl.i32_align,\n+            I64 => dl.i64_align,\n+            I128 => dl.i128_align,\n+        }\n+    }\n+\n+    /// Finds the smallest Integer type which can represent the signed value.\n+    #[inline]\n+    pub fn fit_signed(x: i128) -> Integer {\n+        match x {\n+            -0x0000_0000_0000_0080..=0x0000_0000_0000_007f => I8,\n+            -0x0000_0000_0000_8000..=0x0000_0000_0000_7fff => I16,\n+            -0x0000_0000_8000_0000..=0x0000_0000_7fff_ffff => I32,\n+            -0x8000_0000_0000_0000..=0x7fff_ffff_ffff_ffff => I64,\n+            _ => I128,\n+        }\n+    }\n+\n+    /// Finds the smallest Integer type which can represent the unsigned value.\n+    #[inline]\n+    pub fn fit_unsigned(x: u128) -> Integer {\n+        match x {\n+            0..=0x0000_0000_0000_00ff => I8,\n+            0..=0x0000_0000_0000_ffff => I16,\n+            0..=0x0000_0000_ffff_ffff => I32,\n+            0..=0xffff_ffff_ffff_ffff => I64,\n+            _ => I128,\n+        }\n+    }\n+\n+    /// Finds the smallest integer with the given alignment.\n+    pub fn for_align<C: HasDataLayout>(cx: &C, wanted: Align) -> Option<Integer> {\n+        let dl = cx.data_layout();\n+\n+        for candidate in [I8, I16, I32, I64, I128] {\n+            if wanted == candidate.align(dl).abi && wanted.bytes() == candidate.size().bytes() {\n+                return Some(candidate);\n+            }\n+        }\n+        None\n+    }\n+\n+    /// Find the largest integer with the given alignment or less.\n+    pub fn approximate_align<C: HasDataLayout>(cx: &C, wanted: Align) -> Integer {\n+        let dl = cx.data_layout();\n+\n+        // FIXME(eddyb) maybe include I128 in the future, when it works everywhere.\n+        for candidate in [I64, I32, I16] {\n+            if wanted >= candidate.align(dl).abi && wanted.bytes() >= candidate.size().bytes() {\n+                return candidate;\n+            }\n+        }\n+        I8\n+    }\n+\n+    // FIXME(eddyb) consolidate this and other methods that find the appropriate\n+    // `Integer` given some requirements.\n+    #[inline]\n+    pub fn from_size(size: Size) -> Result<Self, String> {\n+        match size.bits() {\n+            8 => Ok(Integer::I8),\n+            16 => Ok(Integer::I16),\n+            32 => Ok(Integer::I32),\n+            64 => Ok(Integer::I64),\n+            128 => Ok(Integer::I128),\n+            _ => Err(format!(\"rust does not support integers with {} bits\", size.bits())),\n+        }\n+    }\n+}\n+\n+/// Fundamental unit of memory access and layout.\n+#[derive(Copy, Clone, PartialEq, Eq, Hash, Debug)]\n+#[cfg_attr(feature = \"nightly\", derive(HashStable_Generic))]\n+pub enum Primitive {\n+    /// The `bool` is the signedness of the `Integer` type.\n+    ///\n+    /// One would think we would not care about such details this low down,\n+    /// but some ABIs are described in terms of C types and ISAs where the\n+    /// integer arithmetic is done on {sign,zero}-extended registers, e.g.\n+    /// a negative integer passed by zero-extension will appear positive in\n+    /// the callee, and most operations on it will produce the wrong values.\n+    Int(Integer, bool),\n+    F32,\n+    F64,\n+    Pointer,\n+}\n+\n+impl Primitive {\n+    pub fn size<C: HasDataLayout>(self, cx: &C) -> Size {\n+        let dl = cx.data_layout();\n+\n+        match self {\n+            Int(i, _) => i.size(),\n+            F32 => Size::from_bits(32),\n+            F64 => Size::from_bits(64),\n+            Pointer => dl.pointer_size,\n+        }\n+    }\n+\n+    pub fn align<C: HasDataLayout>(self, cx: &C) -> AbiAndPrefAlign {\n+        let dl = cx.data_layout();\n+\n+        match self {\n+            Int(i, _) => i.align(dl),\n+            F32 => dl.f32_align,\n+            F64 => dl.f64_align,\n+            Pointer => dl.pointer_align,\n+        }\n+    }\n+\n+    // FIXME(eddyb) remove, it's trivial thanks to `matches!`.\n+    #[inline]\n+    pub fn is_float(self) -> bool {\n+        matches!(self, F32 | F64)\n+    }\n+\n+    // FIXME(eddyb) remove, it's completely unused.\n+    #[inline]\n+    pub fn is_int(self) -> bool {\n+        matches!(self, Int(..))\n+    }\n+\n+    #[inline]\n+    pub fn is_ptr(self) -> bool {\n+        matches!(self, Pointer)\n+    }\n+}\n+\n+/// Inclusive wrap-around range of valid values, that is, if\n+/// start > end, it represents `start..=MAX`,\n+/// followed by `0..=end`.\n+///\n+/// That is, for an i8 primitive, a range of `254..=2` means following\n+/// sequence:\n+///\n+///    254 (-2), 255 (-1), 0, 1, 2\n+///\n+/// This is intended specifically to mirror LLVM\u2019s `!range` metadata semantics.\n+#[derive(Clone, Copy, PartialEq, Eq, Hash)]\n+#[cfg_attr(feature = \"nightly\", derive(HashStable_Generic))]\n+pub struct WrappingRange {\n+    pub start: u128,\n+    pub end: u128,\n+}\n+\n+impl WrappingRange {\n+    pub fn full(size: Size) -> Self {\n+        Self { start: 0, end: size.unsigned_int_max() }\n+    }\n+\n+    /// Returns `true` if `v` is contained in the range.\n+    #[inline(always)]\n+    pub fn contains(&self, v: u128) -> bool {\n+        if self.start <= self.end {\n+            self.start <= v && v <= self.end\n+        } else {\n+            self.start <= v || v <= self.end\n+        }\n+    }\n+\n+    /// Returns `self` with replaced `start`\n+    #[inline(always)]\n+    pub fn with_start(mut self, start: u128) -> Self {\n+        self.start = start;\n+        self\n+    }\n+\n+    /// Returns `self` with replaced `end`\n+    #[inline(always)]\n+    pub fn with_end(mut self, end: u128) -> Self {\n+        self.end = end;\n+        self\n+    }\n+\n+    /// Returns `true` if `size` completely fills the range.\n+    #[inline]\n+    pub fn is_full_for(&self, size: Size) -> bool {\n+        let max_value = size.unsigned_int_max();\n+        debug_assert!(self.start <= max_value && self.end <= max_value);\n+        self.start == (self.end.wrapping_add(1) & max_value)\n+    }\n+}\n+\n+impl fmt::Debug for WrappingRange {\n+    fn fmt(&self, fmt: &mut fmt::Formatter<'_>) -> fmt::Result {\n+        if self.start > self.end {\n+            write!(fmt, \"(..={}) | ({}..)\", self.end, self.start)?;\n+        } else {\n+            write!(fmt, \"{}..={}\", self.start, self.end)?;\n+        }\n+        Ok(())\n+    }\n+}\n+\n+/// Information about one scalar component of a Rust type.\n+#[derive(Clone, Copy, PartialEq, Eq, Hash, Debug)]\n+#[cfg_attr(feature = \"nightly\", derive(HashStable_Generic))]\n+pub enum Scalar {\n+    Initialized {\n+        value: Primitive,\n+\n+        // FIXME(eddyb) always use the shortest range, e.g., by finding\n+        // the largest space between two consecutive valid values and\n+        // taking everything else as the (shortest) valid range.\n+        valid_range: WrappingRange,\n+    },\n+    Union {\n+        /// Even for unions, we need to use the correct registers for the kind of\n+        /// values inside the union, so we keep the `Primitive` type around. We\n+        /// also use it to compute the size of the scalar.\n+        /// However, unions never have niches and even allow undef,\n+        /// so there is no `valid_range`.\n+        value: Primitive,\n+    },\n+}\n+\n+impl Scalar {\n+    #[inline]\n+    pub fn is_bool(&self) -> bool {\n+        matches!(\n+            self,\n+            Scalar::Initialized {\n+                value: Int(I8, false),\n+                valid_range: WrappingRange { start: 0, end: 1 }\n+            }\n+        )\n+    }\n+\n+    /// Get the primitive representation of this type, ignoring the valid range and whether the\n+    /// value is allowed to be undefined (due to being a union).\n+    pub fn primitive(&self) -> Primitive {\n+        match *self {\n+            Scalar::Initialized { value, .. } | Scalar::Union { value } => value,\n+        }\n+    }\n+\n+    pub fn align(self, cx: &impl HasDataLayout) -> AbiAndPrefAlign {\n+        self.primitive().align(cx)\n+    }\n+\n+    pub fn size(self, cx: &impl HasDataLayout) -> Size {\n+        self.primitive().size(cx)\n+    }\n+\n+    #[inline]\n+    pub fn to_union(&self) -> Self {\n+        Self::Union { value: self.primitive() }\n+    }\n+\n+    #[inline]\n+    pub fn valid_range(&self, cx: &impl HasDataLayout) -> WrappingRange {\n+        match *self {\n+            Scalar::Initialized { valid_range, .. } => valid_range,\n+            Scalar::Union { value } => WrappingRange::full(value.size(cx)),\n+        }\n+    }\n+\n+    #[inline]\n+    /// Allows the caller to mutate the valid range. This operation will panic if attempted on a union.\n+    pub fn valid_range_mut(&mut self) -> &mut WrappingRange {\n+        match self {\n+            Scalar::Initialized { valid_range, .. } => valid_range,\n+            Scalar::Union { .. } => panic!(\"cannot change the valid range of a union\"),\n+        }\n+    }\n+\n+    /// Returns `true` if all possible numbers are valid, i.e `valid_range` covers the whole layout\n+    #[inline]\n+    pub fn is_always_valid<C: HasDataLayout>(&self, cx: &C) -> bool {\n+        match *self {\n+            Scalar::Initialized { valid_range, .. } => valid_range.is_full_for(self.size(cx)),\n+            Scalar::Union { .. } => true,\n+        }\n+    }\n+\n+    /// Returns `true` if this type can be left uninit.\n+    #[inline]\n+    pub fn is_uninit_valid(&self) -> bool {\n+        match *self {\n+            Scalar::Initialized { .. } => false,\n+            Scalar::Union { .. } => true,\n+        }\n+    }\n+}\n+\n+/// Describes how the fields of a type are located in memory.\n+#[derive(PartialEq, Eq, Hash, Clone, Debug)]\n+#[cfg_attr(feature = \"nightly\", derive(HashStable_Generic))]\n+pub enum FieldsShape {\n+    /// Scalar primitives and `!`, which never have fields.\n+    Primitive,\n+\n+    /// All fields start at no offset. The `usize` is the field count.\n+    Union(NonZeroUsize),\n+\n+    /// Array/vector-like placement, with all fields of identical types.\n+    Array { stride: Size, count: u64 },\n+\n+    /// Struct-like placement, with precomputed offsets.\n+    ///\n+    /// Fields are guaranteed to not overlap, but note that gaps\n+    /// before, between and after all the fields are NOT always\n+    /// padding, and as such their contents may not be discarded.\n+    /// For example, enum variants leave a gap at the start,\n+    /// where the discriminant field in the enum layout goes.\n+    Arbitrary {\n+        /// Offsets for the first byte of each field,\n+        /// ordered to match the source definition order.\n+        /// This vector does not go in increasing order.\n+        // FIXME(eddyb) use small vector optimization for the common case.\n+        offsets: Vec<Size>,\n+\n+        /// Maps source order field indices to memory order indices,\n+        /// depending on how the fields were reordered (if at all).\n+        /// This is a permutation, with both the source order and the\n+        /// memory order using the same (0..n) index ranges.\n+        ///\n+        /// Note that during computation of `memory_index`, sometimes\n+        /// it is easier to operate on the inverse mapping (that is,\n+        /// from memory order to source order), and that is usually\n+        /// named `inverse_memory_index`.\n+        ///\n+        // FIXME(eddyb) build a better abstraction for permutations, if possible.\n+        // FIXME(camlorn) also consider small vector  optimization here.\n+        memory_index: Vec<u32>,\n+    },\n+}\n+\n+impl FieldsShape {\n+    #[inline]\n+    pub fn count(&self) -> usize {\n+        match *self {\n+            FieldsShape::Primitive => 0,\n+            FieldsShape::Union(count) => count.get(),\n+            FieldsShape::Array { count, .. } => count.try_into().unwrap(),\n+            FieldsShape::Arbitrary { ref offsets, .. } => offsets.len(),\n+        }\n+    }\n+\n+    #[inline]\n+    pub fn offset(&self, i: usize) -> Size {\n+        match *self {\n+            FieldsShape::Primitive => {\n+                unreachable!(\"FieldsShape::offset: `Primitive`s have no fields\")\n+            }\n+            FieldsShape::Union(count) => {\n+                assert!(\n+                    i < count.get(),\n+                    \"tried to access field {} of union with {} fields\",\n+                    i,\n+                    count\n+                );\n+                Size::ZERO\n+            }\n+            FieldsShape::Array { stride, count } => {\n+                let i = u64::try_from(i).unwrap();\n+                assert!(i < count);\n+                stride * i\n+            }\n+            FieldsShape::Arbitrary { ref offsets, .. } => offsets[i],\n+        }\n+    }\n+\n+    #[inline]\n+    pub fn memory_index(&self, i: usize) -> usize {\n+        match *self {\n+            FieldsShape::Primitive => {\n+                unreachable!(\"FieldsShape::memory_index: `Primitive`s have no fields\")\n+            }\n+            FieldsShape::Union(_) | FieldsShape::Array { .. } => i,\n+            FieldsShape::Arbitrary { ref memory_index, .. } => memory_index[i].try_into().unwrap(),\n+        }\n+    }\n+\n+    /// Gets source indices of the fields by increasing offsets.\n+    #[inline]\n+    pub fn index_by_increasing_offset<'a>(&'a self) -> impl Iterator<Item = usize> + 'a {\n+        let mut inverse_small = [0u8; 64];\n+        let mut inverse_big = vec![];\n+        let use_small = self.count() <= inverse_small.len();\n+\n+        // We have to write this logic twice in order to keep the array small.\n+        if let FieldsShape::Arbitrary { ref memory_index, .. } = *self {\n+            if use_small {\n+                for i in 0..self.count() {\n+                    inverse_small[memory_index[i] as usize] = i as u8;\n+                }\n+            } else {\n+                inverse_big = vec![0; self.count()];\n+                for i in 0..self.count() {\n+                    inverse_big[memory_index[i] as usize] = i as u32;\n+                }\n+            }\n+        }\n+\n+        (0..self.count()).map(move |i| match *self {\n+            FieldsShape::Primitive | FieldsShape::Union(_) | FieldsShape::Array { .. } => i,\n+            FieldsShape::Arbitrary { .. } => {\n+                if use_small {\n+                    inverse_small[i] as usize\n+                } else {\n+                    inverse_big[i] as usize\n+                }\n+            }\n+        })\n+    }\n+}\n+\n+/// An identifier that specifies the address space that some operation\n+/// should operate on. Special address spaces have an effect on code generation,\n+/// depending on the target and the address spaces it implements.\n+#[derive(Copy, Clone, Debug, PartialEq, Eq, PartialOrd, Ord)]\n+pub struct AddressSpace(pub u32);\n+\n+impl AddressSpace {\n+    /// The default address space, corresponding to data space.\n+    pub const DATA: Self = AddressSpace(0);\n+}\n+\n+/// Describes how values of the type are passed by target ABIs,\n+/// in terms of categories of C types there are ABI rules for.\n+#[derive(Clone, Copy, PartialEq, Eq, Hash, Debug)]\n+#[cfg_attr(feature = \"nightly\", derive(HashStable_Generic))]\n+\n+pub enum Abi {\n+    Uninhabited,\n+    Scalar(Scalar),\n+    ScalarPair(Scalar, Scalar),\n+    Vector {\n+        element: Scalar,\n+        count: u64,\n+    },\n+    Aggregate {\n+        /// If true, the size is exact, otherwise it's only a lower bound.\n+        sized: bool,\n+    },\n+}\n+\n+impl Abi {\n+    /// Returns `true` if the layout corresponds to an unsized type.\n+    #[inline]\n+    pub fn is_unsized(&self) -> bool {\n+        match *self {\n+            Abi::Uninhabited | Abi::Scalar(_) | Abi::ScalarPair(..) | Abi::Vector { .. } => false,\n+            Abi::Aggregate { sized } => !sized,\n+        }\n+    }\n+\n+    #[inline]\n+    pub fn is_sized(&self) -> bool {\n+        !self.is_unsized()\n+    }\n+\n+    /// Returns `true` if this is a single signed integer scalar\n+    #[inline]\n+    pub fn is_signed(&self) -> bool {\n+        match self {\n+            Abi::Scalar(scal) => match scal.primitive() {\n+                Primitive::Int(_, signed) => signed,\n+                _ => false,\n+            },\n+            _ => panic!(\"`is_signed` on non-scalar ABI {:?}\", self),\n+        }\n+    }\n+\n+    /// Returns `true` if this is an uninhabited type\n+    #[inline]\n+    pub fn is_uninhabited(&self) -> bool {\n+        matches!(*self, Abi::Uninhabited)\n+    }\n+\n+    /// Returns `true` is this is a scalar type\n+    #[inline]\n+    pub fn is_scalar(&self) -> bool {\n+        matches!(*self, Abi::Scalar(_))\n+    }\n+}\n+\n+#[derive(PartialEq, Eq, Hash, Clone, Debug)]\n+#[cfg_attr(feature = \"nightly\", derive(HashStable_Generic))]\n+pub enum Variants<V: Idx> {\n+    /// Single enum variants, structs/tuples, unions, and all non-ADTs.\n+    Single { index: V },\n+\n+    /// Enum-likes with more than one inhabited variant: each variant comes with\n+    /// a *discriminant* (usually the same as the variant index but the user can\n+    /// assign explicit discriminant values).  That discriminant is encoded\n+    /// as a *tag* on the machine.  The layout of each variant is\n+    /// a struct, and they all have space reserved for the tag.\n+    /// For enums, the tag is the sole field of the layout.\n+    Multiple {\n+        tag: Scalar,\n+        tag_encoding: TagEncoding<V>,\n+        tag_field: usize,\n+        variants: IndexVec<V, LayoutS<V>>,\n+    },\n+}\n+\n+#[derive(PartialEq, Eq, Hash, Clone, Debug)]\n+#[cfg_attr(feature = \"nightly\", derive(HashStable_Generic))]\n+pub enum TagEncoding<V: Idx> {\n+    /// The tag directly stores the discriminant, but possibly with a smaller layout\n+    /// (so converting the tag to the discriminant can require sign extension).\n+    Direct,\n+\n+    /// Niche (values invalid for a type) encoding the discriminant:\n+    /// Discriminant and variant index coincide.\n+    /// The variant `untagged_variant` contains a niche at an arbitrary\n+    /// offset (field `tag_field` of the enum), which for a variant with\n+    /// discriminant `d` is set to\n+    /// `(d - niche_variants.start).wrapping_add(niche_start)`.\n+    ///\n+    /// For example, `Option<(usize, &T)>`  is represented such that\n+    /// `None` has a null pointer for the second tuple field, and\n+    /// `Some` is the identity function (with a non-null reference).\n+    Niche { untagged_variant: V, niche_variants: RangeInclusive<V>, niche_start: u128 },\n+}\n+\n+#[derive(Clone, Copy, PartialEq, Eq, Hash, Debug)]\n+#[cfg_attr(feature = \"nightly\", derive(HashStable_Generic))]\n+pub struct Niche {\n+    pub offset: Size,\n+    pub value: Primitive,\n+    pub valid_range: WrappingRange,\n+}\n+\n+impl Niche {\n+    pub fn from_scalar<C: HasDataLayout>(cx: &C, offset: Size, scalar: Scalar) -> Option<Self> {\n+        let Scalar::Initialized { value, valid_range } = scalar else { return None };\n+        let niche = Niche { offset, value, valid_range };\n+        if niche.available(cx) > 0 { Some(niche) } else { None }\n+    }\n+\n+    pub fn available<C: HasDataLayout>(&self, cx: &C) -> u128 {\n+        let Self { value, valid_range: v, .. } = *self;\n+        let size = value.size(cx);\n+        assert!(size.bits() <= 128);\n+        let max_value = size.unsigned_int_max();\n+\n+        // Find out how many values are outside the valid range.\n+        let niche = v.end.wrapping_add(1)..v.start;\n+        niche.end.wrapping_sub(niche.start) & max_value\n+    }\n+\n+    pub fn reserve<C: HasDataLayout>(&self, cx: &C, count: u128) -> Option<(u128, Scalar)> {\n+        assert!(count > 0);\n+\n+        let Self { value, valid_range: v, .. } = *self;\n+        let size = value.size(cx);\n+        assert!(size.bits() <= 128);\n+        let max_value = size.unsigned_int_max();\n+\n+        let niche = v.end.wrapping_add(1)..v.start;\n+        let available = niche.end.wrapping_sub(niche.start) & max_value;\n+        if count > available {\n+            return None;\n+        }\n+\n+        // Extend the range of valid values being reserved by moving either `v.start` or `v.end` bound.\n+        // Given an eventual `Option<T>`, we try to maximize the chance for `None` to occupy the niche of zero.\n+        // This is accomplished by preferring enums with 2 variants(`count==1`) and always taking the shortest path to niche zero.\n+        // Having `None` in niche zero can enable some special optimizations.\n+        //\n+        // Bound selection criteria:\n+        // 1. Select closest to zero given wrapping semantics.\n+        // 2. Avoid moving past zero if possible.\n+        //\n+        // In practice this means that enums with `count > 1` are unlikely to claim niche zero, since they have to fit perfectly.\n+        // If niche zero is already reserved, the selection of bounds are of little interest.\n+        let move_start = |v: WrappingRange| {\n+            let start = v.start.wrapping_sub(count) & max_value;\n+            Some((start, Scalar::Initialized { value, valid_range: v.with_start(start) }))\n+        };\n+        let move_end = |v: WrappingRange| {\n+            let start = v.end.wrapping_add(1) & max_value;\n+            let end = v.end.wrapping_add(count) & max_value;\n+            Some((start, Scalar::Initialized { value, valid_range: v.with_end(end) }))\n+        };\n+        let distance_end_zero = max_value - v.end;\n+        if v.start > v.end {\n+            // zero is unavailable because wrapping occurs\n+            move_end(v)\n+        } else if v.start <= distance_end_zero {\n+            if count <= v.start {\n+                move_start(v)\n+            } else {\n+                // moved past zero, use other bound\n+                move_end(v)\n+            }\n+        } else {\n+            let end = v.end.wrapping_add(count) & max_value;\n+            let overshot_zero = (1..=v.end).contains(&end);\n+            if overshot_zero {\n+                // moved past zero, use other bound\n+                move_start(v)\n+            } else {\n+                move_end(v)\n+            }\n+        }\n+    }\n+}\n+\n+#[derive(PartialEq, Eq, Hash, Clone)]\n+#[cfg_attr(feature = \"nightly\", derive(HashStable_Generic))]\n+pub struct LayoutS<V: Idx> {\n+    /// Says where the fields are located within the layout.\n+    pub fields: FieldsShape,\n+\n+    /// Encodes information about multi-variant layouts.\n+    /// Even with `Multiple` variants, a layout still has its own fields! Those are then\n+    /// shared between all variants. One of them will be the discriminant,\n+    /// but e.g. generators can have more.\n+    ///\n+    /// To access all fields of this layout, both `fields` and the fields of the active variant\n+    /// must be taken into account.\n+    pub variants: Variants<V>,\n+\n+    /// The `abi` defines how this data is passed between functions, and it defines\n+    /// value restrictions via `valid_range`.\n+    ///\n+    /// Note that this is entirely orthogonal to the recursive structure defined by\n+    /// `variants` and `fields`; for example, `ManuallyDrop<Result<isize, isize>>` has\n+    /// `Abi::ScalarPair`! So, even with non-`Aggregate` `abi`, `fields` and `variants`\n+    /// have to be taken into account to find all fields of this layout.\n+    pub abi: Abi,\n+\n+    /// The leaf scalar with the largest number of invalid values\n+    /// (i.e. outside of its `valid_range`), if it exists.\n+    pub largest_niche: Option<Niche>,\n+\n+    pub align: AbiAndPrefAlign,\n+    pub size: Size,\n+}\n+\n+impl<V: Idx> LayoutS<V> {\n+    pub fn scalar<C: HasDataLayout>(cx: &C, scalar: Scalar) -> Self {\n+        let largest_niche = Niche::from_scalar(cx, Size::ZERO, scalar);\n+        let size = scalar.size(cx);\n+        let align = scalar.align(cx);\n+        LayoutS {\n+            variants: Variants::Single { index: V::new(0) },\n+            fields: FieldsShape::Primitive,\n+            abi: Abi::Scalar(scalar),\n+            largest_niche,\n+            size,\n+            align,\n+        }\n+    }\n+}\n+\n+impl<V: Idx> fmt::Debug for LayoutS<V> {\n+    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {\n+        // This is how `Layout` used to print before it become\n+        // `Interned<LayoutS>`. We print it like this to avoid having to update\n+        // expected output in a lot of tests.\n+        let LayoutS { size, align, abi, fields, largest_niche, variants } = self;\n+        f.debug_struct(\"Layout\")\n+            .field(\"size\", size)\n+            .field(\"align\", align)\n+            .field(\"abi\", abi)\n+            .field(\"fields\", fields)\n+            .field(\"largest_niche\", largest_niche)\n+            .field(\"variants\", variants)\n+            .finish()\n+    }\n+}\n+\n+#[derive(Copy, Clone, PartialEq, Eq, Debug)]\n+pub enum PointerKind {\n+    /// Most general case, we know no restrictions to tell LLVM.\n+    SharedMutable,\n+\n+    /// `&T` where `T` contains no `UnsafeCell`, is `dereferenceable`, `noalias` and `readonly`.\n+    Frozen,\n+\n+    /// `&mut T` which is `dereferenceable` and `noalias` but not `readonly`.\n+    UniqueBorrowed,\n+\n+    /// `&mut !Unpin`, which is `dereferenceable` but neither `noalias` nor `readonly`.\n+    UniqueBorrowedPinned,\n+\n+    /// `Box<T>`, which is `noalias` (even on return types, unlike the above) but neither `readonly`\n+    /// nor `dereferenceable`.\n+    UniqueOwned,\n+}\n+\n+#[derive(Copy, Clone, Debug)]\n+pub struct PointeeInfo {\n+    pub size: Size,\n+    pub align: Align,\n+    pub safe: Option<PointerKind>,\n+    pub address_space: AddressSpace,\n+}\n+\n+/// Used in `might_permit_raw_init` to indicate the kind of initialisation\n+/// that is checked to be valid\n+#[derive(Copy, Clone, Debug, PartialEq, Eq)]\n+pub enum InitKind {\n+    Zero,\n+    UninitMitigated0x01Fill,\n+}\n+\n+impl<V: Idx> LayoutS<V> {\n+    /// Returns `true` if the layout corresponds to an unsized type.\n+    pub fn is_unsized(&self) -> bool {\n+        self.abi.is_unsized()\n+    }\n+\n+    pub fn is_sized(&self) -> bool {\n+        self.abi.is_sized()\n+    }\n+\n+    /// Returns `true` if the type is a ZST and not unsized.\n+    pub fn is_zst(&self) -> bool {\n+        match self.abi {\n+            Abi::Scalar(_) | Abi::ScalarPair(..) | Abi::Vector { .. } => false,\n+            Abi::Uninhabited => self.size.bytes() == 0,\n+            Abi::Aggregate { sized } => sized && self.size.bytes() == 0,\n+        }\n+    }\n+}\n+\n+#[derive(Copy, Clone, Debug)]\n+pub enum StructKind {\n+    /// A tuple, closure, or univariant which cannot be coerced to unsized.\n+    AlwaysSized,\n+    /// A univariant, the last field of which may be coerced to unsized.\n+    MaybeUnsized,\n+    /// A univariant, but with a prefix of an arbitrary size & alignment (e.g., enum tag).\n+    Prefixed(Size, Align),\n+}"}, {"sha": "4c1d95a452d5e571c94229e4e11b064f56c4769c", "filename": "compiler/rustc_hir_analysis/src/collect.rs", "status": "modified", "additions": 1, "deletions": 2, "changes": 3, "blob_url": "https://github.com/rust-lang/rust/blob/390a637e296ccfaac4c6abd1291b0523e8a8e00b/compiler%2Frustc_hir_analysis%2Fsrc%2Fcollect.rs", "raw_url": "https://github.com/rust-lang/rust/raw/390a637e296ccfaac4c6abd1291b0523e8a8e00b/compiler%2Frustc_hir_analysis%2Fsrc%2Fcollect.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_hir_analysis%2Fsrc%2Fcollect.rs?ref=390a637e296ccfaac4c6abd1291b0523e8a8e00b", "patch": "@@ -32,7 +32,6 @@ use rustc_middle::hir::nested_filter;\n use rustc_middle::middle::codegen_fn_attrs::{CodegenFnAttrFlags, CodegenFnAttrs};\n use rustc_middle::mir::mono::Linkage;\n use rustc_middle::ty::query::Providers;\n-use rustc_middle::ty::repr_options_of_def;\n use rustc_middle::ty::util::{Discr, IntTypeExt};\n use rustc_middle::ty::{self, AdtKind, Const, DefIdTree, IsSuggestable, Ty, TyCtxt};\n use rustc_session::lint;\n@@ -860,7 +859,7 @@ fn adt_def<'tcx>(tcx: TyCtxt<'tcx>, def_id: DefId) -> ty::AdtDef<'tcx> {\n         bug!();\n     };\n \n-    let repr = repr_options_of_def(tcx, def_id.to_def_id());\n+    let repr = tcx.repr_options_of_def(def_id.to_def_id());\n     let (kind, variants) = match item.kind {\n         ItemKind::Enum(ref def, _) => {\n             let mut distance_from_explicit = 0;"}, {"sha": "db2c791525645ba156e3e9b3eff440c5d0eacbcd", "filename": "compiler/rustc_index/src/lib.rs", "status": "modified", "additions": 12, "deletions": 7, "changes": 19, "blob_url": "https://github.com/rust-lang/rust/blob/390a637e296ccfaac4c6abd1291b0523e8a8e00b/compiler%2Frustc_index%2Fsrc%2Flib.rs", "raw_url": "https://github.com/rust-lang/rust/raw/390a637e296ccfaac4c6abd1291b0523e8a8e00b/compiler%2Frustc_index%2Fsrc%2Flib.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_index%2Fsrc%2Flib.rs?ref=390a637e296ccfaac4c6abd1291b0523e8a8e00b", "patch": "@@ -1,12 +1,17 @@\n #![deny(rustc::untranslatable_diagnostic)]\n #![deny(rustc::diagnostic_outside_of_impl)]\n-#![cfg_attr(feature = \"nightly\", feature(allow_internal_unstable))]\n-#![cfg_attr(feature = \"nightly\", feature(extend_one))]\n-#![cfg_attr(feature = \"nightly\", feature(min_specialization))]\n-#![cfg_attr(feature = \"nightly\", feature(new_uninit))]\n-#![cfg_attr(feature = \"nightly\", feature(step_trait))]\n-#![cfg_attr(feature = \"nightly\", feature(stmt_expr_attributes))]\n-#![cfg_attr(feature = \"nightly\", feature(test))]\n+#![cfg_attr(\n+    feature = \"nightly\",\n+    feature(\n+        allow_internal_unstable,\n+        extend_one,\n+        min_specialization,\n+        new_uninit,\n+        step_trait,\n+        stmt_expr_attributes,\n+        test\n+    )\n+)]\n \n #[cfg(feature = \"nightly\")]\n pub mod bit_set;"}, {"sha": "297b509d4023d40138c47233316a38c2e41270e6", "filename": "compiler/rustc_lint/src/types.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/390a637e296ccfaac4c6abd1291b0523e8a8e00b/compiler%2Frustc_lint%2Fsrc%2Ftypes.rs", "raw_url": "https://github.com/rust-lang/rust/raw/390a637e296ccfaac4c6abd1291b0523e8a8e00b/compiler%2Frustc_lint%2Fsrc%2Ftypes.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_lint%2Fsrc%2Ftypes.rs?ref=390a637e296ccfaac4c6abd1291b0523e8a8e00b", "patch": "@@ -1378,7 +1378,7 @@ impl<'tcx> LateLintPass<'tcx> for VariantSizeDifferences {\n             let (largest, slargest, largest_index) = iter::zip(enum_definition.variants, variants)\n                 .map(|(variant, variant_layout)| {\n                     // Subtract the size of the enum tag.\n-                    let bytes = variant_layout.size().bytes().saturating_sub(tag_size);\n+                    let bytes = variant_layout.size.bytes().saturating_sub(tag_size);\n \n                     debug!(\"- variant `{}` is {} bytes large\", variant.ident, bytes);\n                     bytes"}, {"sha": "b5327ad0cecc634e699a351e8aba4405d08b8c1c", "filename": "compiler/rustc_middle/src/ty/context.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/390a637e296ccfaac4c6abd1291b0523e8a8e00b/compiler%2Frustc_middle%2Fsrc%2Fty%2Fcontext.rs", "raw_url": "https://github.com/rust-lang/rust/raw/390a637e296ccfaac4c6abd1291b0523e8a8e00b/compiler%2Frustc_middle%2Fsrc%2Fty%2Fcontext.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_middle%2Fsrc%2Fty%2Fcontext.rs?ref=390a637e296ccfaac4c6abd1291b0523e8a8e00b", "patch": "@@ -1233,7 +1233,7 @@ impl<'tcx> TyCtxt<'tcx> {\n             global_ctxt: untracked_resolutions,\n             ast_lowering: untracked_resolver_for_lowering,\n         } = resolver_outputs;\n-        let data_layout = TargetDataLayout::parse(&s.target).unwrap_or_else(|err| {\n+        let data_layout = s.target.parse_data_layout().unwrap_or_else(|err| {\n             s.emit_fatal(err);\n         });\n         let interners = CtxtInterners::new(arena);"}, {"sha": "9d778ff2fb6e3a2dd2907dcb81e2b26a54c0e0db", "filename": "compiler/rustc_middle/src/ty/mod.rs", "status": "modified", "additions": 75, "deletions": 72, "changes": 147, "blob_url": "https://github.com/rust-lang/rust/blob/390a637e296ccfaac4c6abd1291b0523e8a8e00b/compiler%2Frustc_middle%2Fsrc%2Fty%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/390a637e296ccfaac4c6abd1291b0523e8a8e00b/compiler%2Frustc_middle%2Fsrc%2Fty%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_middle%2Fsrc%2Fty%2Fmod.rs?ref=390a637e296ccfaac4c6abd1291b0523e8a8e00b", "patch": "@@ -1995,78 +1995,6 @@ impl Hash for FieldDef {\n     }\n }\n \n-pub fn repr_options_of_def(tcx: TyCtxt<'_>, did: DefId) -> ReprOptions {\n-    let mut flags = ReprFlags::empty();\n-    let mut size = None;\n-    let mut max_align: Option<Align> = None;\n-    let mut min_pack: Option<Align> = None;\n-\n-    // Generate a deterministically-derived seed from the item's path hash\n-    // to allow for cross-crate compilation to actually work\n-    let mut field_shuffle_seed = tcx.def_path_hash(did).0.to_smaller_hash();\n-\n-    // If the user defined a custom seed for layout randomization, xor the item's\n-    // path hash with the user defined seed, this will allowing determinism while\n-    // still allowing users to further randomize layout generation for e.g. fuzzing\n-    if let Some(user_seed) = tcx.sess.opts.unstable_opts.layout_seed {\n-        field_shuffle_seed ^= user_seed;\n-    }\n-\n-    for attr in tcx.get_attrs(did, sym::repr) {\n-        for r in attr::parse_repr_attr(&tcx.sess, attr) {\n-            flags.insert(match r {\n-                attr::ReprC => ReprFlags::IS_C,\n-                attr::ReprPacked(pack) => {\n-                    let pack = Align::from_bytes(pack as u64).unwrap();\n-                    min_pack =\n-                        Some(if let Some(min_pack) = min_pack { min_pack.min(pack) } else { pack });\n-                    ReprFlags::empty()\n-                }\n-                attr::ReprTransparent => ReprFlags::IS_TRANSPARENT,\n-                attr::ReprSimd => ReprFlags::IS_SIMD,\n-                attr::ReprInt(i) => {\n-                    size = Some(match i {\n-                        attr::IntType::SignedInt(x) => match x {\n-                            ast::IntTy::Isize => IntegerType::Pointer(true),\n-                            ast::IntTy::I8 => IntegerType::Fixed(Integer::I8, true),\n-                            ast::IntTy::I16 => IntegerType::Fixed(Integer::I16, true),\n-                            ast::IntTy::I32 => IntegerType::Fixed(Integer::I32, true),\n-                            ast::IntTy::I64 => IntegerType::Fixed(Integer::I64, true),\n-                            ast::IntTy::I128 => IntegerType::Fixed(Integer::I128, true),\n-                        },\n-                        attr::IntType::UnsignedInt(x) => match x {\n-                            ast::UintTy::Usize => IntegerType::Pointer(false),\n-                            ast::UintTy::U8 => IntegerType::Fixed(Integer::I8, false),\n-                            ast::UintTy::U16 => IntegerType::Fixed(Integer::I16, false),\n-                            ast::UintTy::U32 => IntegerType::Fixed(Integer::I32, false),\n-                            ast::UintTy::U64 => IntegerType::Fixed(Integer::I64, false),\n-                            ast::UintTy::U128 => IntegerType::Fixed(Integer::I128, false),\n-                        },\n-                    });\n-                    ReprFlags::empty()\n-                }\n-                attr::ReprAlign(align) => {\n-                    max_align = max_align.max(Some(Align::from_bytes(align as u64).unwrap()));\n-                    ReprFlags::empty()\n-                }\n-            });\n-        }\n-    }\n-\n-    // If `-Z randomize-layout` was enabled for the type definition then we can\n-    // consider performing layout randomization\n-    if tcx.sess.opts.unstable_opts.randomize_layout {\n-        flags.insert(ReprFlags::RANDOMIZE_LAYOUT);\n-    }\n-\n-    // This is here instead of layout because the choice must make it into metadata.\n-    if !tcx.consider_optimizing(|| format!(\"Reorder fields of {:?}\", tcx.def_path_str(did))) {\n-        flags.insert(ReprFlags::IS_LINEAR);\n-    }\n-\n-    ReprOptions { int: size, align: max_align, pack: min_pack, flags, field_shuffle_seed }\n-}\n-\n impl<'tcx> FieldDef {\n     /// Returns the type of this field. The resulting type is not normalized. The `subst` is\n     /// typically obtained via the second field of [`TyKind::Adt`].\n@@ -2134,6 +2062,81 @@ impl<'tcx> TyCtxt<'tcx> {\n             .filter(move |item| item.kind == AssocKind::Fn && item.defaultness(self).has_value())\n     }\n \n+    pub fn repr_options_of_def(self, did: DefId) -> ReprOptions {\n+        let mut flags = ReprFlags::empty();\n+        let mut size = None;\n+        let mut max_align: Option<Align> = None;\n+        let mut min_pack: Option<Align> = None;\n+\n+        // Generate a deterministically-derived seed from the item's path hash\n+        // to allow for cross-crate compilation to actually work\n+        let mut field_shuffle_seed = self.def_path_hash(did).0.to_smaller_hash();\n+\n+        // If the user defined a custom seed for layout randomization, xor the item's\n+        // path hash with the user defined seed, this will allowing determinism while\n+        // still allowing users to further randomize layout generation for e.g. fuzzing\n+        if let Some(user_seed) = self.sess.opts.unstable_opts.layout_seed {\n+            field_shuffle_seed ^= user_seed;\n+        }\n+\n+        for attr in self.get_attrs(did, sym::repr) {\n+            for r in attr::parse_repr_attr(&self.sess, attr) {\n+                flags.insert(match r {\n+                    attr::ReprC => ReprFlags::IS_C,\n+                    attr::ReprPacked(pack) => {\n+                        let pack = Align::from_bytes(pack as u64).unwrap();\n+                        min_pack = Some(if let Some(min_pack) = min_pack {\n+                            min_pack.min(pack)\n+                        } else {\n+                            pack\n+                        });\n+                        ReprFlags::empty()\n+                    }\n+                    attr::ReprTransparent => ReprFlags::IS_TRANSPARENT,\n+                    attr::ReprSimd => ReprFlags::IS_SIMD,\n+                    attr::ReprInt(i) => {\n+                        size = Some(match i {\n+                            attr::IntType::SignedInt(x) => match x {\n+                                ast::IntTy::Isize => IntegerType::Pointer(true),\n+                                ast::IntTy::I8 => IntegerType::Fixed(Integer::I8, true),\n+                                ast::IntTy::I16 => IntegerType::Fixed(Integer::I16, true),\n+                                ast::IntTy::I32 => IntegerType::Fixed(Integer::I32, true),\n+                                ast::IntTy::I64 => IntegerType::Fixed(Integer::I64, true),\n+                                ast::IntTy::I128 => IntegerType::Fixed(Integer::I128, true),\n+                            },\n+                            attr::IntType::UnsignedInt(x) => match x {\n+                                ast::UintTy::Usize => IntegerType::Pointer(false),\n+                                ast::UintTy::U8 => IntegerType::Fixed(Integer::I8, false),\n+                                ast::UintTy::U16 => IntegerType::Fixed(Integer::I16, false),\n+                                ast::UintTy::U32 => IntegerType::Fixed(Integer::I32, false),\n+                                ast::UintTy::U64 => IntegerType::Fixed(Integer::I64, false),\n+                                ast::UintTy::U128 => IntegerType::Fixed(Integer::I128, false),\n+                            },\n+                        });\n+                        ReprFlags::empty()\n+                    }\n+                    attr::ReprAlign(align) => {\n+                        max_align = max_align.max(Some(Align::from_bytes(align as u64).unwrap()));\n+                        ReprFlags::empty()\n+                    }\n+                });\n+            }\n+        }\n+\n+        // If `-Z randomize-layout` was enabled for the type definition then we can\n+        // consider performing layout randomization\n+        if self.sess.opts.unstable_opts.randomize_layout {\n+            flags.insert(ReprFlags::RANDOMIZE_LAYOUT);\n+        }\n+\n+        // This is here instead of layout because the choice must make it into metadata.\n+        if !self.consider_optimizing(|| format!(\"Reorder fields of {:?}\", self.def_path_str(did))) {\n+            flags.insert(ReprFlags::IS_LINEAR);\n+        }\n+\n+        ReprOptions { int: size, align: max_align, pack: min_pack, flags, field_shuffle_seed }\n+    }\n+\n     /// Look up the name of a definition across crates. This does not look at HIR.\n     pub fn opt_item_name(self, def_id: DefId) -> Option<Symbol> {\n         if let Some(cnum) = def_id.as_crate_root() {"}, {"sha": "be0aa0fc4c1d05fcf9f3b5e9751c41e2bb254322", "filename": "compiler/rustc_mir_transform/src/uninhabited_enum_branching.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/390a637e296ccfaac4c6abd1291b0523e8a8e00b/compiler%2Frustc_mir_transform%2Fsrc%2Funinhabited_enum_branching.rs", "raw_url": "https://github.com/rust-lang/rust/raw/390a637e296ccfaac4c6abd1291b0523e8a8e00b/compiler%2Frustc_mir_transform%2Fsrc%2Funinhabited_enum_branching.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_mir_transform%2Fsrc%2Funinhabited_enum_branching.rs?ref=390a637e296ccfaac4c6abd1291b0523e8a8e00b", "patch": "@@ -65,7 +65,7 @@ fn variant_discriminants<'tcx>(\n         Variants::Multiple { variants, .. } => variants\n             .iter_enumerated()\n             .filter_map(|(idx, layout)| {\n-                (layout.abi() != Abi::Uninhabited)\n+                (layout.abi != Abi::Uninhabited)\n                     .then(|| ty.discriminant_for_variant(tcx, idx).unwrap().val)\n             })\n             .collect(),"}, {"sha": "3b1b33aa095a1a3b7f9c526e888d8c078864d74c", "filename": "compiler/rustc_session/src/config.rs", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/rust-lang/rust/blob/390a637e296ccfaac4c6abd1291b0523e8a8e00b/compiler%2Frustc_session%2Fsrc%2Fconfig.rs", "raw_url": "https://github.com/rust-lang/rust/raw/390a637e296ccfaac4c6abd1291b0523e8a8e00b/compiler%2Frustc_session%2Fsrc%2Fconfig.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_session%2Fsrc%2Fconfig.rs?ref=390a637e296ccfaac4c6abd1291b0523e8a8e00b", "patch": "@@ -11,7 +11,7 @@ use crate::{lint, HashStableContext};\n use rustc_data_structures::fx::{FxHashMap, FxHashSet};\n \n use rustc_data_structures::stable_hasher::ToStableHashKey;\n-use rustc_target::abi::{Align, TargetDataLayout};\n+use rustc_target::abi::Align;\n use rustc_target::spec::{PanicStrategy, SanitizerSet, SplitDebuginfo};\n use rustc_target::spec::{Target, TargetTriple, TargetWarnings, TARGETS};\n \n@@ -900,7 +900,7 @@ fn default_configuration(sess: &Session) -> CrateConfig {\n     let min_atomic_width = sess.target.min_atomic_width();\n     let max_atomic_width = sess.target.max_atomic_width();\n     let atomic_cas = sess.target.atomic_cas;\n-    let layout = TargetDataLayout::parse(&sess.target).unwrap_or_else(|err| {\n+    let layout = sess.target.parse_data_layout().unwrap_or_else(|err| {\n         sess.emit_fatal(err);\n     });\n "}, {"sha": "568c916a163284b3db17645d62338c55c20cef12", "filename": "compiler/rustc_target/Cargo.toml", "status": "modified", "additions": 7, "deletions": 19, "changes": 26, "blob_url": "https://github.com/rust-lang/rust/blob/390a637e296ccfaac4c6abd1291b0523e8a8e00b/compiler%2Frustc_target%2FCargo.toml", "raw_url": "https://github.com/rust-lang/rust/raw/390a637e296ccfaac4c6abd1291b0523e8a8e00b/compiler%2Frustc_target%2FCargo.toml", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_target%2FCargo.toml?ref=390a637e296ccfaac4c6abd1291b0523e8a8e00b", "patch": "@@ -6,23 +6,11 @@ edition = \"2021\"\n [dependencies]\n bitflags = \"1.2.1\"\n tracing = \"0.1\"\n-rand = \"0.8.4\"\n-rand_xoshiro = \"0.6.0\"\n serde_json = \"1.0.59\"\n-rustc_data_structures = { path = \"../rustc_data_structures\", optional = true  }\n-rustc_feature = { path = \"../rustc_feature\", optional = true }\n-rustc_index = { path = \"../rustc_index\", default-features = false }\n-rustc_macros = { path = \"../rustc_macros\", optional = true }\n-rustc_serialize = { path = \"../rustc_serialize\", optional = true  }\n-rustc_span = { path = \"../rustc_span\", optional = true }\n-\n-[features]\n-default = [\"nightly\"]\n-nightly = [\n-    \"rustc_data_structures\",\n-    \"rustc_feature\",\n-    \"rustc_index/nightly\",\n-    \"rustc_macros\",\n-    \"rustc_serialize\",\n-    \"rustc_span\",\n-]\n+rustc_abi = { path = \"../rustc_abi\" }\n+rustc_data_structures = { path = \"../rustc_data_structures\" }\n+rustc_feature = { path = \"../rustc_feature\" }\n+rustc_index = { path = \"../rustc_index\" }\n+rustc_macros = { path = \"../rustc_macros\" }\n+rustc_serialize = { path = \"../rustc_serialize\" }\n+rustc_span = { path = \"../rustc_span\" }"}, {"sha": "a5ffaebea0b98cb76986ac4a1bfc2530d2035d3c", "filename": "compiler/rustc_target/src/abi/call/mod.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/390a637e296ccfaac4c6abd1291b0523e8a8e00b/compiler%2Frustc_target%2Fsrc%2Fabi%2Fcall%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/390a637e296ccfaac4c6abd1291b0523e8a8e00b/compiler%2Frustc_target%2Fsrc%2Fabi%2Fcall%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_target%2Fsrc%2Fabi%2Fcall%2Fmod.rs?ref=390a637e296ccfaac4c6abd1291b0523e8a8e00b", "patch": "@@ -262,7 +262,7 @@ impl CastTarget {\n         let mut size = self.rest.total;\n         for i in 0..self.prefix.iter().count() {\n             match self.prefix[i] {\n-                Some(v) => size += Size { raw: v.size.bytes() },\n+                Some(v) => size += v.size,\n                 None => {}\n             }\n         }"}, {"sha": "ec8f20fe692166fe30c5d7fa23e0d1015ffd4802", "filename": "compiler/rustc_target/src/abi/call/sparc64.rs", "status": "modified", "additions": 7, "deletions": 7, "changes": 14, "blob_url": "https://github.com/rust-lang/rust/blob/390a637e296ccfaac4c6abd1291b0523e8a8e00b/compiler%2Frustc_target%2Fsrc%2Fabi%2Fcall%2Fsparc64.rs", "raw_url": "https://github.com/rust-lang/rust/raw/390a637e296ccfaac4c6abd1291b0523e8a8e00b/compiler%2Frustc_target%2Fsrc%2Fabi%2Fcall%2Fsparc64.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_target%2Fsrc%2Fabi%2Fcall%2Fsparc64.rs?ref=390a637e296ccfaac4c6abd1291b0523e8a8e00b", "patch": "@@ -87,8 +87,8 @@ where\n         _ => {}\n     }\n \n-    if (offset.raw % 4) != 0 && scalar2.primitive().is_float() {\n-        offset.raw += 4 - (offset.raw % 4);\n+    if (offset.bytes() % 4) != 0 && scalar2.primitive().is_float() {\n+        offset += Size::from_bytes(4 - (offset.bytes() % 4));\n     }\n     data = arg_scalar(cx, &scalar2, offset, data);\n     return data;\n@@ -169,14 +169,14 @@ where\n                     has_float: false,\n                     arg_attribute: ArgAttribute::default(),\n                 },\n-                Size { raw: 0 },\n+                Size::ZERO,\n             );\n \n             if data.has_float {\n                 // Structure { float, int, int } doesn't like to be handled like\n                 // { float, long int }. Other way around it doesn't mind.\n                 if data.last_offset < arg.layout.size\n-                    && (data.last_offset.raw % 8) != 0\n+                    && (data.last_offset.bytes() % 8) != 0\n                     && data.prefix_index < data.prefix.len()\n                 {\n                     data.prefix[data.prefix_index] = Some(Reg::i32());\n@@ -185,7 +185,7 @@ where\n                 }\n \n                 let mut rest_size = arg.layout.size - data.last_offset;\n-                if (rest_size.raw % 8) != 0 && data.prefix_index < data.prefix.len() {\n+                if (rest_size.bytes() % 8) != 0 && data.prefix_index < data.prefix.len() {\n                     data.prefix[data.prefix_index] = Some(Reg::i32());\n                     rest_size = rest_size - Reg::i32().size;\n                 }\n@@ -214,13 +214,13 @@ where\n     C: HasDataLayout,\n {\n     if !fn_abi.ret.is_ignore() {\n-        classify_arg(cx, &mut fn_abi.ret, Size { raw: 32 });\n+        classify_arg(cx, &mut fn_abi.ret, Size::from_bytes(32));\n     }\n \n     for arg in fn_abi.args.iter_mut() {\n         if arg.is_ignore() {\n             continue;\n         }\n-        classify_arg(cx, arg, Size { raw: 16 });\n+        classify_arg(cx, arg, Size::from_bytes(16));\n     }\n }"}, {"sha": "53c9878ab8740be4c8187d1eb90a07e9f350dbf0", "filename": "compiler/rustc_target/src/abi/mod.rs", "status": "modified", "additions": 4, "deletions": 1518, "changes": 1522, "blob_url": "https://github.com/rust-lang/rust/blob/390a637e296ccfaac4c6abd1291b0523e8a8e00b/compiler%2Frustc_target%2Fsrc%2Fabi%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/390a637e296ccfaac4c6abd1291b0523e8a8e00b/compiler%2Frustc_target%2Fsrc%2Fabi%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_target%2Fsrc%2Fabi%2Fmod.rs?ref=390a637e296ccfaac4c6abd1291b0523e8a8e00b", "patch": "@@ -2,1504 +2,40 @@ pub use Integer::*;\n pub use Primitive::*;\n \n use crate::json::{Json, ToJson};\n-#[cfg(feature = \"nightly\")]\n-use crate::spec::Target;\n \n-use std::convert::{TryFrom, TryInto};\n use std::fmt;\n-#[cfg(feature = \"nightly\")]\n-use std::iter::Step;\n-use std::num::{NonZeroUsize, ParseIntError};\n-use std::ops::{Add, AddAssign, Deref, Mul, RangeInclusive, Sub};\n-use std::str::FromStr;\n+use std::ops::Deref;\n \n-use bitflags::bitflags;\n-#[cfg(feature = \"nightly\")]\n use rustc_data_structures::intern::Interned;\n-use rustc_index::vec::{Idx, IndexVec};\n-#[cfg(feature = \"nightly\")]\n use rustc_macros::HashStable_Generic;\n \n-#[cfg(feature = \"nightly\")]\n pub mod call;\n \n-mod layout;\n-\n-pub use layout::LayoutCalculator;\n-\n-bitflags! {\n-    #[derive(Default)]\n-    #[cfg_attr(feature = \"nightly\", derive(Encodable, Decodable, HashStable_Generic))]\n-    pub struct ReprFlags: u8 {\n-        const IS_C               = 1 << 0;\n-        const IS_SIMD            = 1 << 1;\n-        const IS_TRANSPARENT     = 1 << 2;\n-        // Internal only for now. If true, don't reorder fields.\n-        const IS_LINEAR          = 1 << 3;\n-        // If true, the type's layout can be randomized using\n-        // the seed stored in `ReprOptions.layout_seed`\n-        const RANDOMIZE_LAYOUT   = 1 << 4;\n-        // Any of these flags being set prevent field reordering optimisation.\n-        const IS_UNOPTIMISABLE   = ReprFlags::IS_C.bits\n-                                 | ReprFlags::IS_SIMD.bits\n-                                 | ReprFlags::IS_LINEAR.bits;\n-    }\n-}\n-\n-#[derive(Copy, Clone, Debug, Eq, PartialEq)]\n-#[cfg_attr(feature = \"nightly\", derive(Encodable, Decodable, HashStable_Generic))]\n-pub enum IntegerType {\n-    Pointer(bool),\n-    Fixed(Integer, bool),\n-}\n-\n-impl IntegerType {\n-    pub fn is_signed(&self) -> bool {\n-        match self {\n-            IntegerType::Pointer(b) => *b,\n-            IntegerType::Fixed(_, b) => *b,\n-        }\n-    }\n-}\n-\n-/// Represents the repr options provided by the user,\n-#[derive(Copy, Clone, Debug, Eq, PartialEq, Default)]\n-#[cfg_attr(feature = \"nightly\", derive(Encodable, Decodable, HashStable_Generic))]\n-pub struct ReprOptions {\n-    pub int: Option<IntegerType>,\n-    pub align: Option<Align>,\n-    pub pack: Option<Align>,\n-    pub flags: ReprFlags,\n-    /// The seed to be used for randomizing a type's layout\n-    ///\n-    /// Note: This could technically be a `[u8; 16]` (a `u128`) which would\n-    /// be the \"most accurate\" hash as it'd encompass the item and crate\n-    /// hash without loss, but it does pay the price of being larger.\n-    /// Everything's a tradeoff, a `u64` seed should be sufficient for our\n-    /// purposes (primarily `-Z randomize-layout`)\n-    pub field_shuffle_seed: u64,\n-}\n-\n-impl ReprOptions {\n-    #[inline]\n-    pub fn simd(&self) -> bool {\n-        self.flags.contains(ReprFlags::IS_SIMD)\n-    }\n-\n-    #[inline]\n-    pub fn c(&self) -> bool {\n-        self.flags.contains(ReprFlags::IS_C)\n-    }\n-\n-    #[inline]\n-    pub fn packed(&self) -> bool {\n-        self.pack.is_some()\n-    }\n-\n-    #[inline]\n-    pub fn transparent(&self) -> bool {\n-        self.flags.contains(ReprFlags::IS_TRANSPARENT)\n-    }\n-\n-    #[inline]\n-    pub fn linear(&self) -> bool {\n-        self.flags.contains(ReprFlags::IS_LINEAR)\n-    }\n-\n-    /// Returns the discriminant type, given these `repr` options.\n-    /// This must only be called on enums!\n-    pub fn discr_type(&self) -> IntegerType {\n-        self.int.unwrap_or(IntegerType::Pointer(true))\n-    }\n-\n-    /// Returns `true` if this `#[repr()]` should inhabit \"smart enum\n-    /// layout\" optimizations, such as representing `Foo<&T>` as a\n-    /// single pointer.\n-    pub fn inhibit_enum_layout_opt(&self) -> bool {\n-        self.c() || self.int.is_some()\n-    }\n-\n-    /// Returns `true` if this `#[repr()]` should inhibit struct field reordering\n-    /// optimizations, such as with `repr(C)`, `repr(packed(1))`, or `repr(<int>)`.\n-    pub fn inhibit_struct_field_reordering_opt(&self) -> bool {\n-        if let Some(pack) = self.pack {\n-            if pack.bytes() == 1 {\n-                return true;\n-            }\n-        }\n-\n-        self.flags.intersects(ReprFlags::IS_UNOPTIMISABLE) || self.int.is_some()\n-    }\n-\n-    /// Returns `true` if this type is valid for reordering and `-Z randomize-layout`\n-    /// was enabled for its declaration crate\n-    pub fn can_randomize_type_layout(&self) -> bool {\n-        !self.inhibit_struct_field_reordering_opt()\n-            && self.flags.contains(ReprFlags::RANDOMIZE_LAYOUT)\n-    }\n-\n-    /// Returns `true` if this `#[repr()]` should inhibit union ABI optimisations.\n-    pub fn inhibit_union_abi_opt(&self) -> bool {\n-        self.c()\n-    }\n-}\n-\n-/// Parsed [Data layout](https://llvm.org/docs/LangRef.html#data-layout)\n-/// for a target, which contains everything needed to compute layouts.\n-#[derive(Debug, PartialEq, Eq)]\n-pub struct TargetDataLayout {\n-    pub endian: Endian,\n-    pub i1_align: AbiAndPrefAlign,\n-    pub i8_align: AbiAndPrefAlign,\n-    pub i16_align: AbiAndPrefAlign,\n-    pub i32_align: AbiAndPrefAlign,\n-    pub i64_align: AbiAndPrefAlign,\n-    pub i128_align: AbiAndPrefAlign,\n-    pub f32_align: AbiAndPrefAlign,\n-    pub f64_align: AbiAndPrefAlign,\n-    pub pointer_size: Size,\n-    pub pointer_align: AbiAndPrefAlign,\n-    pub aggregate_align: AbiAndPrefAlign,\n-\n-    /// Alignments for vector types.\n-    pub vector_align: Vec<(Size, AbiAndPrefAlign)>,\n-\n-    pub instruction_address_space: AddressSpace,\n-\n-    /// Minimum size of #[repr(C)] enums (default I32 bits)\n-    pub c_enum_min_size: Integer,\n-}\n-\n-impl Default for TargetDataLayout {\n-    /// Creates an instance of `TargetDataLayout`.\n-    fn default() -> TargetDataLayout {\n-        let align = |bits| Align::from_bits(bits).unwrap();\n-        TargetDataLayout {\n-            endian: Endian::Big,\n-            i1_align: AbiAndPrefAlign::new(align(8)),\n-            i8_align: AbiAndPrefAlign::new(align(8)),\n-            i16_align: AbiAndPrefAlign::new(align(16)),\n-            i32_align: AbiAndPrefAlign::new(align(32)),\n-            i64_align: AbiAndPrefAlign { abi: align(32), pref: align(64) },\n-            i128_align: AbiAndPrefAlign { abi: align(32), pref: align(64) },\n-            f32_align: AbiAndPrefAlign::new(align(32)),\n-            f64_align: AbiAndPrefAlign::new(align(64)),\n-            pointer_size: Size::from_bits(64),\n-            pointer_align: AbiAndPrefAlign::new(align(64)),\n-            aggregate_align: AbiAndPrefAlign { abi: align(0), pref: align(64) },\n-            vector_align: vec![\n-                (Size::from_bits(64), AbiAndPrefAlign::new(align(64))),\n-                (Size::from_bits(128), AbiAndPrefAlign::new(align(128))),\n-            ],\n-            instruction_address_space: AddressSpace::DATA,\n-            c_enum_min_size: Integer::I32,\n-        }\n-    }\n-}\n-\n-pub enum TargetDataLayoutErrors<'a> {\n-    InvalidAddressSpace { addr_space: &'a str, cause: &'a str, err: ParseIntError },\n-    InvalidBits { kind: &'a str, bit: &'a str, cause: &'a str, err: ParseIntError },\n-    MissingAlignment { cause: &'a str },\n-    InvalidAlignment { cause: &'a str, err: String },\n-    InconsistentTargetArchitecture { dl: &'a str, target: &'a str },\n-    InconsistentTargetPointerWidth { pointer_size: u64, target: u32 },\n-    InvalidBitsSize { err: String },\n-}\n-\n-impl TargetDataLayout {\n-    #[cfg(feature = \"nightly\")]\n-    pub fn parse<'a>(target: &'a Target) -> Result<TargetDataLayout, TargetDataLayoutErrors<'a>> {\n-        // Parse an address space index from a string.\n-        let parse_address_space = |s: &'a str, cause: &'a str| {\n-            s.parse::<u32>().map(AddressSpace).map_err(|err| {\n-                TargetDataLayoutErrors::InvalidAddressSpace { addr_space: s, cause, err }\n-            })\n-        };\n-\n-        // Parse a bit count from a string.\n-        let parse_bits = |s: &'a str, kind: &'a str, cause: &'a str| {\n-            s.parse::<u64>().map_err(|err| TargetDataLayoutErrors::InvalidBits {\n-                kind,\n-                bit: s,\n-                cause,\n-                err,\n-            })\n-        };\n-\n-        // Parse a size string.\n-        let size = |s: &'a str, cause: &'a str| parse_bits(s, \"size\", cause).map(Size::from_bits);\n-\n-        // Parse an alignment string.\n-        let align = |s: &[&'a str], cause: &'a str| {\n-            if s.is_empty() {\n-                return Err(TargetDataLayoutErrors::MissingAlignment { cause });\n-            }\n-            let align_from_bits = |bits| {\n-                Align::from_bits(bits)\n-                    .map_err(|err| TargetDataLayoutErrors::InvalidAlignment { cause, err })\n-            };\n-            let abi = parse_bits(s[0], \"alignment\", cause)?;\n-            let pref = s.get(1).map_or(Ok(abi), |pref| parse_bits(pref, \"alignment\", cause))?;\n-            Ok(AbiAndPrefAlign { abi: align_from_bits(abi)?, pref: align_from_bits(pref)? })\n-        };\n-\n-        let mut dl = TargetDataLayout::default();\n-        let mut i128_align_src = 64;\n-        for spec in target.data_layout.split('-') {\n-            let spec_parts = spec.split(':').collect::<Vec<_>>();\n-\n-            match &*spec_parts {\n-                [\"e\"] => dl.endian = Endian::Little,\n-                [\"E\"] => dl.endian = Endian::Big,\n-                [p] if p.starts_with('P') => {\n-                    dl.instruction_address_space = parse_address_space(&p[1..], \"P\")?\n-                }\n-                [\"a\", ref a @ ..] => dl.aggregate_align = align(a, \"a\")?,\n-                [\"f32\", ref a @ ..] => dl.f32_align = align(a, \"f32\")?,\n-                [\"f64\", ref a @ ..] => dl.f64_align = align(a, \"f64\")?,\n-                [p @ \"p\", s, ref a @ ..] | [p @ \"p0\", s, ref a @ ..] => {\n-                    dl.pointer_size = size(s, p)?;\n-                    dl.pointer_align = align(a, p)?;\n-                }\n-                [s, ref a @ ..] if s.starts_with('i') => {\n-                    let Ok(bits) = s[1..].parse::<u64>() else {\n-                        size(&s[1..], \"i\")?; // For the user error.\n-                        continue;\n-                    };\n-                    let a = align(a, s)?;\n-                    match bits {\n-                        1 => dl.i1_align = a,\n-                        8 => dl.i8_align = a,\n-                        16 => dl.i16_align = a,\n-                        32 => dl.i32_align = a,\n-                        64 => dl.i64_align = a,\n-                        _ => {}\n-                    }\n-                    if bits >= i128_align_src && bits <= 128 {\n-                        // Default alignment for i128 is decided by taking the alignment of\n-                        // largest-sized i{64..=128}.\n-                        i128_align_src = bits;\n-                        dl.i128_align = a;\n-                    }\n-                }\n-                [s, ref a @ ..] if s.starts_with('v') => {\n-                    let v_size = size(&s[1..], \"v\")?;\n-                    let a = align(a, s)?;\n-                    if let Some(v) = dl.vector_align.iter_mut().find(|v| v.0 == v_size) {\n-                        v.1 = a;\n-                        continue;\n-                    }\n-                    // No existing entry, add a new one.\n-                    dl.vector_align.push((v_size, a));\n-                }\n-                _ => {} // Ignore everything else.\n-            }\n-        }\n-\n-        // Perform consistency checks against the Target information.\n-        if dl.endian != target.endian {\n-            return Err(TargetDataLayoutErrors::InconsistentTargetArchitecture {\n-                dl: dl.endian.as_str(),\n-                target: target.endian.as_str(),\n-            });\n-        }\n-\n-        let target_pointer_width: u64 = target.pointer_width.into();\n-        if dl.pointer_size.bits() != target_pointer_width {\n-            return Err(TargetDataLayoutErrors::InconsistentTargetPointerWidth {\n-                pointer_size: dl.pointer_size.bits(),\n-                target: target.pointer_width,\n-            });\n-        }\n-\n-        dl.c_enum_min_size = match Integer::from_size(Size::from_bits(target.c_enum_min_bits)) {\n-            Ok(bits) => bits,\n-            Err(err) => return Err(TargetDataLayoutErrors::InvalidBitsSize { err }),\n-        };\n-\n-        Ok(dl)\n-    }\n-\n-    /// Returns exclusive upper bound on object size.\n-    ///\n-    /// The theoretical maximum object size is defined as the maximum positive `isize` value.\n-    /// This ensures that the `offset` semantics remain well-defined by allowing it to correctly\n-    /// index every address within an object along with one byte past the end, along with allowing\n-    /// `isize` to store the difference between any two pointers into an object.\n-    ///\n-    /// The upper bound on 64-bit currently needs to be lower because LLVM uses a 64-bit integer\n-    /// to represent object size in bits. It would need to be 1 << 61 to account for this, but is\n-    /// currently conservatively bounded to 1 << 47 as that is enough to cover the current usable\n-    /// address space on 64-bit ARMv8 and x86_64.\n-    #[inline]\n-    pub fn obj_size_bound(&self) -> u64 {\n-        match self.pointer_size.bits() {\n-            16 => 1 << 15,\n-            32 => 1 << 31,\n-            64 => 1 << 47,\n-            bits => panic!(\"obj_size_bound: unknown pointer bit size {}\", bits),\n-        }\n-    }\n-\n-    #[inline]\n-    pub fn ptr_sized_integer(&self) -> Integer {\n-        match self.pointer_size.bits() {\n-            16 => I16,\n-            32 => I32,\n-            64 => I64,\n-            bits => panic!(\"ptr_sized_integer: unknown pointer bit size {}\", bits),\n-        }\n-    }\n-\n-    #[inline]\n-    pub fn vector_align(&self, vec_size: Size) -> AbiAndPrefAlign {\n-        for &(size, align) in &self.vector_align {\n-            if size == vec_size {\n-                return align;\n-            }\n-        }\n-        // Default to natural alignment, which is what LLVM does.\n-        // That is, use the size, rounded up to a power of 2.\n-        AbiAndPrefAlign::new(Align::from_bytes(vec_size.bytes().next_power_of_two()).unwrap())\n-    }\n-}\n-\n-pub trait HasDataLayout {\n-    fn data_layout(&self) -> &TargetDataLayout;\n-}\n-\n-impl HasDataLayout for TargetDataLayout {\n-    #[inline]\n-    fn data_layout(&self) -> &TargetDataLayout {\n-        self\n-    }\n-}\n-\n-/// Endianness of the target, which must match cfg(target-endian).\n-#[derive(Copy, Clone, PartialEq, Eq)]\n-pub enum Endian {\n-    Little,\n-    Big,\n-}\n-\n-impl Endian {\n-    pub fn as_str(&self) -> &'static str {\n-        match self {\n-            Self::Little => \"little\",\n-            Self::Big => \"big\",\n-        }\n-    }\n-}\n-\n-impl fmt::Debug for Endian {\n-    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {\n-        f.write_str(self.as_str())\n-    }\n-}\n-\n-impl FromStr for Endian {\n-    type Err = String;\n-\n-    fn from_str(s: &str) -> Result<Self, Self::Err> {\n-        match s {\n-            \"little\" => Ok(Self::Little),\n-            \"big\" => Ok(Self::Big),\n-            _ => Err(format!(r#\"unknown endian: \"{}\"\"#, s)),\n-        }\n-    }\n-}\n+pub use rustc_abi::*;\n \n impl ToJson for Endian {\n     fn to_json(&self) -> Json {\n         self.as_str().to_json()\n     }\n }\n \n-/// Size of a type in bytes.\n-#[derive(Copy, Clone, PartialEq, Eq, PartialOrd, Ord, Hash)]\n-#[cfg_attr(feature = \"nightly\", derive(Encodable, Decodable, HashStable_Generic))]\n-pub struct Size {\n-    raw: u64,\n-}\n-\n-// This is debug-printed a lot in larger structs, don't waste too much space there\n-impl fmt::Debug for Size {\n-    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {\n-        write!(f, \"Size({} bytes)\", self.bytes())\n-    }\n-}\n-\n-impl Size {\n-    pub const ZERO: Size = Size { raw: 0 };\n-\n-    /// Rounds `bits` up to the next-higher byte boundary, if `bits` is\n-    /// not a multiple of 8.\n-    pub fn from_bits(bits: impl TryInto<u64>) -> Size {\n-        let bits = bits.try_into().ok().unwrap();\n-        // Avoid potential overflow from `bits + 7`.\n-        Size { raw: bits / 8 + ((bits % 8) + 7) / 8 }\n-    }\n-\n-    #[inline]\n-    pub fn from_bytes(bytes: impl TryInto<u64>) -> Size {\n-        let bytes: u64 = bytes.try_into().ok().unwrap();\n-        Size { raw: bytes }\n-    }\n-\n-    #[inline]\n-    pub fn bytes(self) -> u64 {\n-        self.raw\n-    }\n-\n-    #[inline]\n-    pub fn bytes_usize(self) -> usize {\n-        self.bytes().try_into().unwrap()\n-    }\n-\n-    #[inline]\n-    pub fn bits(self) -> u64 {\n-        #[cold]\n-        fn overflow(bytes: u64) -> ! {\n-            panic!(\"Size::bits: {} bytes in bits doesn't fit in u64\", bytes)\n-        }\n-\n-        self.bytes().checked_mul(8).unwrap_or_else(|| overflow(self.bytes()))\n-    }\n-\n-    #[inline]\n-    pub fn bits_usize(self) -> usize {\n-        self.bits().try_into().unwrap()\n-    }\n-\n-    #[inline]\n-    pub fn align_to(self, align: Align) -> Size {\n-        let mask = align.bytes() - 1;\n-        Size::from_bytes((self.bytes() + mask) & !mask)\n-    }\n-\n-    #[inline]\n-    pub fn is_aligned(self, align: Align) -> bool {\n-        let mask = align.bytes() - 1;\n-        self.bytes() & mask == 0\n-    }\n-\n-    #[inline]\n-    pub fn checked_add<C: HasDataLayout>(self, offset: Size, cx: &C) -> Option<Size> {\n-        let dl = cx.data_layout();\n-\n-        let bytes = self.bytes().checked_add(offset.bytes())?;\n-\n-        if bytes < dl.obj_size_bound() { Some(Size::from_bytes(bytes)) } else { None }\n-    }\n-\n-    #[inline]\n-    pub fn checked_mul<C: HasDataLayout>(self, count: u64, cx: &C) -> Option<Size> {\n-        let dl = cx.data_layout();\n-\n-        let bytes = self.bytes().checked_mul(count)?;\n-        if bytes < dl.obj_size_bound() { Some(Size::from_bytes(bytes)) } else { None }\n-    }\n-\n-    /// Truncates `value` to `self` bits and then sign-extends it to 128 bits\n-    /// (i.e., if it is negative, fill with 1's on the left).\n-    #[inline]\n-    pub fn sign_extend(self, value: u128) -> u128 {\n-        let size = self.bits();\n-        if size == 0 {\n-            // Truncated until nothing is left.\n-            return 0;\n-        }\n-        // Sign-extend it.\n-        let shift = 128 - size;\n-        // Shift the unsigned value to the left, then shift back to the right as signed\n-        // (essentially fills with sign bit on the left).\n-        (((value << shift) as i128) >> shift) as u128\n-    }\n-\n-    /// Truncates `value` to `self` bits.\n-    #[inline]\n-    pub fn truncate(self, value: u128) -> u128 {\n-        let size = self.bits();\n-        if size == 0 {\n-            // Truncated until nothing is left.\n-            return 0;\n-        }\n-        let shift = 128 - size;\n-        // Truncate (shift left to drop out leftover values, shift right to fill with zeroes).\n-        (value << shift) >> shift\n-    }\n-\n-    #[inline]\n-    pub fn signed_int_min(&self) -> i128 {\n-        self.sign_extend(1_u128 << (self.bits() - 1)) as i128\n-    }\n-\n-    #[inline]\n-    pub fn signed_int_max(&self) -> i128 {\n-        i128::MAX >> (128 - self.bits())\n-    }\n-\n-    #[inline]\n-    pub fn unsigned_int_max(&self) -> u128 {\n-        u128::MAX >> (128 - self.bits())\n-    }\n-}\n-\n-// Panicking addition, subtraction and multiplication for convenience.\n-// Avoid during layout computation, return `LayoutError` instead.\n-\n-impl Add for Size {\n-    type Output = Size;\n-    #[inline]\n-    fn add(self, other: Size) -> Size {\n-        Size::from_bytes(self.bytes().checked_add(other.bytes()).unwrap_or_else(|| {\n-            panic!(\"Size::add: {} + {} doesn't fit in u64\", self.bytes(), other.bytes())\n-        }))\n-    }\n-}\n-\n-impl Sub for Size {\n-    type Output = Size;\n-    #[inline]\n-    fn sub(self, other: Size) -> Size {\n-        Size::from_bytes(self.bytes().checked_sub(other.bytes()).unwrap_or_else(|| {\n-            panic!(\"Size::sub: {} - {} would result in negative size\", self.bytes(), other.bytes())\n-        }))\n-    }\n-}\n-\n-impl Mul<Size> for u64 {\n-    type Output = Size;\n-    #[inline]\n-    fn mul(self, size: Size) -> Size {\n-        size * self\n-    }\n-}\n-\n-impl Mul<u64> for Size {\n-    type Output = Size;\n-    #[inline]\n-    fn mul(self, count: u64) -> Size {\n-        match self.bytes().checked_mul(count) {\n-            Some(bytes) => Size::from_bytes(bytes),\n-            None => panic!(\"Size::mul: {} * {} doesn't fit in u64\", self.bytes(), count),\n-        }\n-    }\n-}\n-\n-impl AddAssign for Size {\n-    #[inline]\n-    fn add_assign(&mut self, other: Size) {\n-        *self = *self + other;\n-    }\n-}\n-\n-#[cfg(feature = \"nightly\")]\n-impl Step for Size {\n-    #[inline]\n-    fn steps_between(start: &Self, end: &Self) -> Option<usize> {\n-        u64::steps_between(&start.bytes(), &end.bytes())\n-    }\n-\n-    #[inline]\n-    fn forward_checked(start: Self, count: usize) -> Option<Self> {\n-        u64::forward_checked(start.bytes(), count).map(Self::from_bytes)\n-    }\n-\n-    #[inline]\n-    fn forward(start: Self, count: usize) -> Self {\n-        Self::from_bytes(u64::forward(start.bytes(), count))\n-    }\n-\n-    #[inline]\n-    unsafe fn forward_unchecked(start: Self, count: usize) -> Self {\n-        Self::from_bytes(u64::forward_unchecked(start.bytes(), count))\n-    }\n-\n-    #[inline]\n-    fn backward_checked(start: Self, count: usize) -> Option<Self> {\n-        u64::backward_checked(start.bytes(), count).map(Self::from_bytes)\n-    }\n-\n-    #[inline]\n-    fn backward(start: Self, count: usize) -> Self {\n-        Self::from_bytes(u64::backward(start.bytes(), count))\n-    }\n-\n-    #[inline]\n-    unsafe fn backward_unchecked(start: Self, count: usize) -> Self {\n-        Self::from_bytes(u64::backward_unchecked(start.bytes(), count))\n-    }\n-}\n-\n-/// Alignment of a type in bytes (always a power of two).\n-#[derive(Copy, Clone, PartialEq, Eq, PartialOrd, Ord, Hash)]\n-#[cfg_attr(feature = \"nightly\", derive(Encodable, Decodable, HashStable_Generic))]\n-pub struct Align {\n-    pow2: u8,\n-}\n-\n-// This is debug-printed a lot in larger structs, don't waste too much space there\n-impl fmt::Debug for Align {\n-    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {\n-        write!(f, \"Align({} bytes)\", self.bytes())\n-    }\n-}\n-\n-impl Align {\n-    pub const ONE: Align = Align { pow2: 0 };\n-    pub const MAX: Align = Align { pow2: 29 };\n-\n-    #[inline]\n-    pub fn from_bits(bits: u64) -> Result<Align, String> {\n-        Align::from_bytes(Size::from_bits(bits).bytes())\n-    }\n-\n-    #[inline]\n-    pub fn from_bytes(align: u64) -> Result<Align, String> {\n-        // Treat an alignment of 0 bytes like 1-byte alignment.\n-        if align == 0 {\n-            return Ok(Align::ONE);\n-        }\n-\n-        #[cold]\n-        fn not_power_of_2(align: u64) -> String {\n-            format!(\"`{}` is not a power of 2\", align)\n-        }\n-\n-        #[cold]\n-        fn too_large(align: u64) -> String {\n-            format!(\"`{}` is too large\", align)\n-        }\n-\n-        let mut bytes = align;\n-        let mut pow2: u8 = 0;\n-        while (bytes & 1) == 0 {\n-            pow2 += 1;\n-            bytes >>= 1;\n-        }\n-        if bytes != 1 {\n-            return Err(not_power_of_2(align));\n-        }\n-        if pow2 > Self::MAX.pow2 {\n-            return Err(too_large(align));\n-        }\n-\n-        Ok(Align { pow2 })\n-    }\n-\n-    #[inline]\n-    pub fn bytes(self) -> u64 {\n-        1 << self.pow2\n-    }\n-\n-    #[inline]\n-    pub fn bits(self) -> u64 {\n-        self.bytes() * 8\n-    }\n-\n-    /// Computes the best alignment possible for the given offset\n-    /// (the largest power of two that the offset is a multiple of).\n-    ///\n-    /// N.B., for an offset of `0`, this happens to return `2^64`.\n-    #[inline]\n-    pub fn max_for_offset(offset: Size) -> Align {\n-        Align { pow2: offset.bytes().trailing_zeros() as u8 }\n-    }\n-\n-    /// Lower the alignment, if necessary, such that the given offset\n-    /// is aligned to it (the offset is a multiple of the alignment).\n-    #[inline]\n-    pub fn restrict_for_offset(self, offset: Size) -> Align {\n-        self.min(Align::max_for_offset(offset))\n-    }\n-}\n-\n-/// A pair of alignments, ABI-mandated and preferred.\n-#[derive(Copy, Clone, PartialEq, Eq, Hash, Debug)]\n-#[cfg_attr(feature = \"nightly\", derive(HashStable_Generic))]\n-\n-pub struct AbiAndPrefAlign {\n-    pub abi: Align,\n-    pub pref: Align,\n-}\n-\n-impl AbiAndPrefAlign {\n-    #[inline]\n-    pub fn new(align: Align) -> AbiAndPrefAlign {\n-        AbiAndPrefAlign { abi: align, pref: align }\n-    }\n-\n-    #[inline]\n-    pub fn min(self, other: AbiAndPrefAlign) -> AbiAndPrefAlign {\n-        AbiAndPrefAlign { abi: self.abi.min(other.abi), pref: self.pref.min(other.pref) }\n-    }\n-\n-    #[inline]\n-    pub fn max(self, other: AbiAndPrefAlign) -> AbiAndPrefAlign {\n-        AbiAndPrefAlign { abi: self.abi.max(other.abi), pref: self.pref.max(other.pref) }\n-    }\n-}\n-\n-/// Integers, also used for enum discriminants.\n-#[derive(Copy, Clone, PartialEq, Eq, PartialOrd, Ord, Hash, Debug)]\n-#[cfg_attr(feature = \"nightly\", derive(Encodable, Decodable, HashStable_Generic))]\n-\n-pub enum Integer {\n-    I8,\n-    I16,\n-    I32,\n-    I64,\n-    I128,\n-}\n-\n-impl Integer {\n-    #[inline]\n-    pub fn size(self) -> Size {\n-        match self {\n-            I8 => Size::from_bytes(1),\n-            I16 => Size::from_bytes(2),\n-            I32 => Size::from_bytes(4),\n-            I64 => Size::from_bytes(8),\n-            I128 => Size::from_bytes(16),\n-        }\n-    }\n-\n-    /// Gets the Integer type from an attr::IntType.\n-    pub fn from_attr<C: HasDataLayout>(cx: &C, ity: IntegerType) -> Integer {\n-        let dl = cx.data_layout();\n-\n-        match ity {\n-            IntegerType::Pointer(_) => dl.ptr_sized_integer(),\n-            IntegerType::Fixed(x, _) => x,\n-        }\n-    }\n-\n-    pub fn align<C: HasDataLayout>(self, cx: &C) -> AbiAndPrefAlign {\n-        let dl = cx.data_layout();\n-\n-        match self {\n-            I8 => dl.i8_align,\n-            I16 => dl.i16_align,\n-            I32 => dl.i32_align,\n-            I64 => dl.i64_align,\n-            I128 => dl.i128_align,\n-        }\n-    }\n-\n-    /// Finds the smallest Integer type which can represent the signed value.\n-    #[inline]\n-    pub fn fit_signed(x: i128) -> Integer {\n-        match x {\n-            -0x0000_0000_0000_0080..=0x0000_0000_0000_007f => I8,\n-            -0x0000_0000_0000_8000..=0x0000_0000_0000_7fff => I16,\n-            -0x0000_0000_8000_0000..=0x0000_0000_7fff_ffff => I32,\n-            -0x8000_0000_0000_0000..=0x7fff_ffff_ffff_ffff => I64,\n-            _ => I128,\n-        }\n-    }\n-\n-    /// Finds the smallest Integer type which can represent the unsigned value.\n-    #[inline]\n-    pub fn fit_unsigned(x: u128) -> Integer {\n-        match x {\n-            0..=0x0000_0000_0000_00ff => I8,\n-            0..=0x0000_0000_0000_ffff => I16,\n-            0..=0x0000_0000_ffff_ffff => I32,\n-            0..=0xffff_ffff_ffff_ffff => I64,\n-            _ => I128,\n-        }\n-    }\n-\n-    /// Finds the smallest integer with the given alignment.\n-    pub fn for_align<C: HasDataLayout>(cx: &C, wanted: Align) -> Option<Integer> {\n-        let dl = cx.data_layout();\n-\n-        for candidate in [I8, I16, I32, I64, I128] {\n-            if wanted == candidate.align(dl).abi && wanted.bytes() == candidate.size().bytes() {\n-                return Some(candidate);\n-            }\n-        }\n-        None\n-    }\n-\n-    /// Find the largest integer with the given alignment or less.\n-    pub fn approximate_align<C: HasDataLayout>(cx: &C, wanted: Align) -> Integer {\n-        let dl = cx.data_layout();\n-\n-        // FIXME(eddyb) maybe include I128 in the future, when it works everywhere.\n-        for candidate in [I64, I32, I16] {\n-            if wanted >= candidate.align(dl).abi && wanted.bytes() >= candidate.size().bytes() {\n-                return candidate;\n-            }\n-        }\n-        I8\n-    }\n-\n-    // FIXME(eddyb) consolidate this and other methods that find the appropriate\n-    // `Integer` given some requirements.\n-    #[inline]\n-    fn from_size(size: Size) -> Result<Self, String> {\n-        match size.bits() {\n-            8 => Ok(Integer::I8),\n-            16 => Ok(Integer::I16),\n-            32 => Ok(Integer::I32),\n-            64 => Ok(Integer::I64),\n-            128 => Ok(Integer::I128),\n-            _ => Err(format!(\"rust does not support integers with {} bits\", size.bits())),\n-        }\n-    }\n-}\n-\n-/// Fundamental unit of memory access and layout.\n-#[derive(Copy, Clone, PartialEq, Eq, Hash, Debug)]\n-#[cfg_attr(feature = \"nightly\", derive(HashStable_Generic))]\n-pub enum Primitive {\n-    /// The `bool` is the signedness of the `Integer` type.\n-    ///\n-    /// One would think we would not care about such details this low down,\n-    /// but some ABIs are described in terms of C types and ISAs where the\n-    /// integer arithmetic is done on {sign,zero}-extended registers, e.g.\n-    /// a negative integer passed by zero-extension will appear positive in\n-    /// the callee, and most operations on it will produce the wrong values.\n-    Int(Integer, bool),\n-    F32,\n-    F64,\n-    Pointer,\n-}\n-\n-impl Primitive {\n-    pub fn size<C: HasDataLayout>(self, cx: &C) -> Size {\n-        let dl = cx.data_layout();\n-\n-        match self {\n-            Int(i, _) => i.size(),\n-            F32 => Size::from_bits(32),\n-            F64 => Size::from_bits(64),\n-            Pointer => dl.pointer_size,\n-        }\n-    }\n-\n-    pub fn align<C: HasDataLayout>(self, cx: &C) -> AbiAndPrefAlign {\n-        let dl = cx.data_layout();\n-\n-        match self {\n-            Int(i, _) => i.align(dl),\n-            F32 => dl.f32_align,\n-            F64 => dl.f64_align,\n-            Pointer => dl.pointer_align,\n-        }\n-    }\n-\n-    // FIXME(eddyb) remove, it's trivial thanks to `matches!`.\n-    #[inline]\n-    pub fn is_float(self) -> bool {\n-        matches!(self, F32 | F64)\n-    }\n-\n-    // FIXME(eddyb) remove, it's completely unused.\n-    #[inline]\n-    pub fn is_int(self) -> bool {\n-        matches!(self, Int(..))\n-    }\n-\n-    #[inline]\n-    pub fn is_ptr(self) -> bool {\n-        matches!(self, Pointer)\n-    }\n-}\n-\n-/// Inclusive wrap-around range of valid values, that is, if\n-/// start > end, it represents `start..=MAX`,\n-/// followed by `0..=end`.\n-///\n-/// That is, for an i8 primitive, a range of `254..=2` means following\n-/// sequence:\n-///\n-///    254 (-2), 255 (-1), 0, 1, 2\n-///\n-/// This is intended specifically to mirror LLVM\u2019s `!range` metadata semantics.\n-#[derive(Clone, Copy, PartialEq, Eq, Hash)]\n-#[cfg_attr(feature = \"nightly\", derive(HashStable_Generic))]\n-pub struct WrappingRange {\n-    pub start: u128,\n-    pub end: u128,\n-}\n-\n-impl WrappingRange {\n-    pub fn full(size: Size) -> Self {\n-        Self { start: 0, end: size.unsigned_int_max() }\n-    }\n-\n-    /// Returns `true` if `v` is contained in the range.\n-    #[inline(always)]\n-    pub fn contains(&self, v: u128) -> bool {\n-        if self.start <= self.end {\n-            self.start <= v && v <= self.end\n-        } else {\n-            self.start <= v || v <= self.end\n-        }\n-    }\n-\n-    /// Returns `self` with replaced `start`\n-    #[inline(always)]\n-    pub fn with_start(mut self, start: u128) -> Self {\n-        self.start = start;\n-        self\n-    }\n-\n-    /// Returns `self` with replaced `end`\n-    #[inline(always)]\n-    pub fn with_end(mut self, end: u128) -> Self {\n-        self.end = end;\n-        self\n-    }\n-\n-    /// Returns `true` if `size` completely fills the range.\n-    #[inline]\n-    pub fn is_full_for(&self, size: Size) -> bool {\n-        let max_value = size.unsigned_int_max();\n-        debug_assert!(self.start <= max_value && self.end <= max_value);\n-        self.start == (self.end.wrapping_add(1) & max_value)\n-    }\n-}\n-\n-impl fmt::Debug for WrappingRange {\n-    fn fmt(&self, fmt: &mut fmt::Formatter<'_>) -> fmt::Result {\n-        if self.start > self.end {\n-            write!(fmt, \"(..={}) | ({}..)\", self.end, self.start)?;\n-        } else {\n-            write!(fmt, \"{}..={}\", self.start, self.end)?;\n-        }\n-        Ok(())\n-    }\n-}\n-\n-/// Information about one scalar component of a Rust type.\n-#[derive(Clone, Copy, PartialEq, Eq, Hash, Debug)]\n-#[cfg_attr(feature = \"nightly\", derive(HashStable_Generic))]\n-pub enum Scalar {\n-    Initialized {\n-        value: Primitive,\n-\n-        // FIXME(eddyb) always use the shortest range, e.g., by finding\n-        // the largest space between two consecutive valid values and\n-        // taking everything else as the (shortest) valid range.\n-        valid_range: WrappingRange,\n-    },\n-    Union {\n-        /// Even for unions, we need to use the correct registers for the kind of\n-        /// values inside the union, so we keep the `Primitive` type around. We\n-        /// also use it to compute the size of the scalar.\n-        /// However, unions never have niches and even allow undef,\n-        /// so there is no `valid_range`.\n-        value: Primitive,\n-    },\n-}\n-\n-impl Scalar {\n-    #[inline]\n-    pub fn is_bool(&self) -> bool {\n-        matches!(\n-            self,\n-            Scalar::Initialized {\n-                value: Int(I8, false),\n-                valid_range: WrappingRange { start: 0, end: 1 }\n-            }\n-        )\n-    }\n-\n-    /// Get the primitive representation of this type, ignoring the valid range and whether the\n-    /// value is allowed to be undefined (due to being a union).\n-    pub fn primitive(&self) -> Primitive {\n-        match *self {\n-            Scalar::Initialized { value, .. } | Scalar::Union { value } => value,\n-        }\n-    }\n-\n-    pub fn align(self, cx: &impl HasDataLayout) -> AbiAndPrefAlign {\n-        self.primitive().align(cx)\n-    }\n-\n-    pub fn size(self, cx: &impl HasDataLayout) -> Size {\n-        self.primitive().size(cx)\n-    }\n-\n-    #[inline]\n-    pub fn to_union(&self) -> Self {\n-        Self::Union { value: self.primitive() }\n-    }\n-\n-    #[inline]\n-    pub fn valid_range(&self, cx: &impl HasDataLayout) -> WrappingRange {\n-        match *self {\n-            Scalar::Initialized { valid_range, .. } => valid_range,\n-            Scalar::Union { value } => WrappingRange::full(value.size(cx)),\n-        }\n-    }\n-\n-    #[inline]\n-    /// Allows the caller to mutate the valid range. This operation will panic if attempted on a union.\n-    pub fn valid_range_mut(&mut self) -> &mut WrappingRange {\n-        match self {\n-            Scalar::Initialized { valid_range, .. } => valid_range,\n-            Scalar::Union { .. } => panic!(\"cannot change the valid range of a union\"),\n-        }\n-    }\n-\n-    /// Returns `true` if all possible numbers are valid, i.e `valid_range` covers the whole layout\n-    #[inline]\n-    pub fn is_always_valid<C: HasDataLayout>(&self, cx: &C) -> bool {\n-        match *self {\n-            Scalar::Initialized { valid_range, .. } => valid_range.is_full_for(self.size(cx)),\n-            Scalar::Union { .. } => true,\n-        }\n-    }\n-\n-    /// Returns `true` if this type can be left uninit.\n-    #[inline]\n-    pub fn is_uninit_valid(&self) -> bool {\n-        match *self {\n-            Scalar::Initialized { .. } => false,\n-            Scalar::Union { .. } => true,\n-        }\n-    }\n-}\n-\n-/// Describes how the fields of a type are located in memory.\n-#[derive(PartialEq, Eq, Hash, Clone, Debug)]\n-#[cfg_attr(feature = \"nightly\", derive(HashStable_Generic))]\n-pub enum FieldsShape {\n-    /// Scalar primitives and `!`, which never have fields.\n-    Primitive,\n-\n-    /// All fields start at no offset. The `usize` is the field count.\n-    Union(NonZeroUsize),\n-\n-    /// Array/vector-like placement, with all fields of identical types.\n-    Array { stride: Size, count: u64 },\n-\n-    /// Struct-like placement, with precomputed offsets.\n-    ///\n-    /// Fields are guaranteed to not overlap, but note that gaps\n-    /// before, between and after all the fields are NOT always\n-    /// padding, and as such their contents may not be discarded.\n-    /// For example, enum variants leave a gap at the start,\n-    /// where the discriminant field in the enum layout goes.\n-    Arbitrary {\n-        /// Offsets for the first byte of each field,\n-        /// ordered to match the source definition order.\n-        /// This vector does not go in increasing order.\n-        // FIXME(eddyb) use small vector optimization for the common case.\n-        offsets: Vec<Size>,\n-\n-        /// Maps source order field indices to memory order indices,\n-        /// depending on how the fields were reordered (if at all).\n-        /// This is a permutation, with both the source order and the\n-        /// memory order using the same (0..n) index ranges.\n-        ///\n-        /// Note that during computation of `memory_index`, sometimes\n-        /// it is easier to operate on the inverse mapping (that is,\n-        /// from memory order to source order), and that is usually\n-        /// named `inverse_memory_index`.\n-        ///\n-        // FIXME(eddyb) build a better abstraction for permutations, if possible.\n-        // FIXME(camlorn) also consider small vector  optimization here.\n-        memory_index: Vec<u32>,\n-    },\n-}\n-\n-impl FieldsShape {\n-    #[inline]\n-    pub fn count(&self) -> usize {\n-        match *self {\n-            FieldsShape::Primitive => 0,\n-            FieldsShape::Union(count) => count.get(),\n-            FieldsShape::Array { count, .. } => count.try_into().unwrap(),\n-            FieldsShape::Arbitrary { ref offsets, .. } => offsets.len(),\n-        }\n-    }\n-\n-    #[inline]\n-    pub fn offset(&self, i: usize) -> Size {\n-        match *self {\n-            FieldsShape::Primitive => {\n-                unreachable!(\"FieldsShape::offset: `Primitive`s have no fields\")\n-            }\n-            FieldsShape::Union(count) => {\n-                assert!(\n-                    i < count.get(),\n-                    \"tried to access field {} of union with {} fields\",\n-                    i,\n-                    count\n-                );\n-                Size::ZERO\n-            }\n-            FieldsShape::Array { stride, count } => {\n-                let i = u64::try_from(i).unwrap();\n-                assert!(i < count);\n-                stride * i\n-            }\n-            FieldsShape::Arbitrary { ref offsets, .. } => offsets[i],\n-        }\n-    }\n-\n-    #[inline]\n-    pub fn memory_index(&self, i: usize) -> usize {\n-        match *self {\n-            FieldsShape::Primitive => {\n-                unreachable!(\"FieldsShape::memory_index: `Primitive`s have no fields\")\n-            }\n-            FieldsShape::Union(_) | FieldsShape::Array { .. } => i,\n-            FieldsShape::Arbitrary { ref memory_index, .. } => memory_index[i].try_into().unwrap(),\n-        }\n-    }\n-\n-    /// Gets source indices of the fields by increasing offsets.\n-    #[inline]\n-    pub fn index_by_increasing_offset<'a>(&'a self) -> impl Iterator<Item = usize> + 'a {\n-        let mut inverse_small = [0u8; 64];\n-        let mut inverse_big = vec![];\n-        let use_small = self.count() <= inverse_small.len();\n-\n-        // We have to write this logic twice in order to keep the array small.\n-        if let FieldsShape::Arbitrary { ref memory_index, .. } = *self {\n-            if use_small {\n-                for i in 0..self.count() {\n-                    inverse_small[memory_index[i] as usize] = i as u8;\n-                }\n-            } else {\n-                inverse_big = vec![0; self.count()];\n-                for i in 0..self.count() {\n-                    inverse_big[memory_index[i] as usize] = i as u32;\n-                }\n-            }\n-        }\n-\n-        (0..self.count()).map(move |i| match *self {\n-            FieldsShape::Primitive | FieldsShape::Union(_) | FieldsShape::Array { .. } => i,\n-            FieldsShape::Arbitrary { .. } => {\n-                if use_small {\n-                    inverse_small[i] as usize\n-                } else {\n-                    inverse_big[i] as usize\n-                }\n-            }\n-        })\n-    }\n-}\n-\n-/// An identifier that specifies the address space that some operation\n-/// should operate on. Special address spaces have an effect on code generation,\n-/// depending on the target and the address spaces it implements.\n-#[derive(Copy, Clone, Debug, PartialEq, Eq, PartialOrd, Ord)]\n-pub struct AddressSpace(pub u32);\n-\n-impl AddressSpace {\n-    /// The default address space, corresponding to data space.\n-    pub const DATA: Self = AddressSpace(0);\n-}\n-\n-/// Describes how values of the type are passed by target ABIs,\n-/// in terms of categories of C types there are ABI rules for.\n-#[derive(Clone, Copy, PartialEq, Eq, Hash, Debug)]\n-#[cfg_attr(feature = \"nightly\", derive(HashStable_Generic))]\n-\n-pub enum Abi {\n-    Uninhabited,\n-    Scalar(Scalar),\n-    ScalarPair(Scalar, Scalar),\n-    Vector {\n-        element: Scalar,\n-        count: u64,\n-    },\n-    Aggregate {\n-        /// If true, the size is exact, otherwise it's only a lower bound.\n-        sized: bool,\n-    },\n-}\n-\n-impl Abi {\n-    /// Returns `true` if the layout corresponds to an unsized type.\n-    #[inline]\n-    pub fn is_unsized(&self) -> bool {\n-        match *self {\n-            Abi::Uninhabited | Abi::Scalar(_) | Abi::ScalarPair(..) | Abi::Vector { .. } => false,\n-            Abi::Aggregate { sized } => !sized,\n-        }\n-    }\n-\n-    #[inline]\n-    pub fn is_sized(&self) -> bool {\n-        !self.is_unsized()\n-    }\n-\n-    /// Returns `true` if this is a single signed integer scalar\n-    #[inline]\n-    pub fn is_signed(&self) -> bool {\n-        match self {\n-            Abi::Scalar(scal) => match scal.primitive() {\n-                Primitive::Int(_, signed) => signed,\n-                _ => false,\n-            },\n-            _ => panic!(\"`is_signed` on non-scalar ABI {:?}\", self),\n-        }\n-    }\n-\n-    /// Returns `true` if this is an uninhabited type\n-    #[inline]\n-    pub fn is_uninhabited(&self) -> bool {\n-        matches!(*self, Abi::Uninhabited)\n-    }\n-\n-    /// Returns `true` is this is a scalar type\n-    #[inline]\n-    pub fn is_scalar(&self) -> bool {\n-        matches!(*self, Abi::Scalar(_))\n-    }\n-}\n-\n-#[cfg(feature = \"nightly\")]\n rustc_index::newtype_index! {\n     pub struct VariantIdx {\n         derive [HashStable_Generic]\n     }\n }\n \n-#[derive(PartialEq, Eq, Hash, Clone, Debug)]\n-#[cfg_attr(feature = \"nightly\", derive(HashStable_Generic))]\n-pub enum Variants<V: Idx> {\n-    /// Single enum variants, structs/tuples, unions, and all non-ADTs.\n-    Single { index: V },\n-\n-    /// Enum-likes with more than one inhabited variant: each variant comes with\n-    /// a *discriminant* (usually the same as the variant index but the user can\n-    /// assign explicit discriminant values).  That discriminant is encoded\n-    /// as a *tag* on the machine.  The layout of each variant is\n-    /// a struct, and they all have space reserved for the tag.\n-    /// For enums, the tag is the sole field of the layout.\n-    Multiple {\n-        tag: Scalar,\n-        tag_encoding: TagEncoding<V>,\n-        tag_field: usize,\n-        variants: IndexVec<V, LayoutS<V>>,\n-    },\n-}\n-\n-#[derive(PartialEq, Eq, Hash, Clone, Debug)]\n-#[cfg_attr(feature = \"nightly\", derive(HashStable_Generic))]\n-pub enum TagEncoding<V: Idx> {\n-    /// The tag directly stores the discriminant, but possibly with a smaller layout\n-    /// (so converting the tag to the discriminant can require sign extension).\n-    Direct,\n-\n-    /// Niche (values invalid for a type) encoding the discriminant:\n-    /// Discriminant and variant index coincide.\n-    /// The variant `untagged_variant` contains a niche at an arbitrary\n-    /// offset (field `tag_field` of the enum), which for a variant with\n-    /// discriminant `d` is set to\n-    /// `(d - niche_variants.start).wrapping_add(niche_start)`.\n-    ///\n-    /// For example, `Option<(usize, &T)>`  is represented such that\n-    /// `None` has a null pointer for the second tuple field, and\n-    /// `Some` is the identity function (with a non-null reference).\n-    Niche { untagged_variant: V, niche_variants: RangeInclusive<V>, niche_start: u128 },\n-}\n-\n-#[derive(Clone, Copy, PartialEq, Eq, Hash, Debug)]\n-#[cfg_attr(feature = \"nightly\", derive(HashStable_Generic))]\n-pub struct Niche {\n-    pub offset: Size,\n-    pub value: Primitive,\n-    pub valid_range: WrappingRange,\n-}\n-\n-impl Niche {\n-    pub fn from_scalar<C: HasDataLayout>(cx: &C, offset: Size, scalar: Scalar) -> Option<Self> {\n-        let Scalar::Initialized { value, valid_range } = scalar else { return None };\n-        let niche = Niche { offset, value, valid_range };\n-        if niche.available(cx) > 0 { Some(niche) } else { None }\n-    }\n-\n-    pub fn available<C: HasDataLayout>(&self, cx: &C) -> u128 {\n-        let Self { value, valid_range: v, .. } = *self;\n-        let size = value.size(cx);\n-        assert!(size.bits() <= 128);\n-        let max_value = size.unsigned_int_max();\n-\n-        // Find out how many values are outside the valid range.\n-        let niche = v.end.wrapping_add(1)..v.start;\n-        niche.end.wrapping_sub(niche.start) & max_value\n-    }\n-\n-    pub fn reserve<C: HasDataLayout>(&self, cx: &C, count: u128) -> Option<(u128, Scalar)> {\n-        assert!(count > 0);\n-\n-        let Self { value, valid_range: v, .. } = *self;\n-        let size = value.size(cx);\n-        assert!(size.bits() <= 128);\n-        let max_value = size.unsigned_int_max();\n-\n-        let niche = v.end.wrapping_add(1)..v.start;\n-        let available = niche.end.wrapping_sub(niche.start) & max_value;\n-        if count > available {\n-            return None;\n-        }\n-\n-        // Extend the range of valid values being reserved by moving either `v.start` or `v.end` bound.\n-        // Given an eventual `Option<T>`, we try to maximize the chance for `None` to occupy the niche of zero.\n-        // This is accomplished by preferring enums with 2 variants(`count==1`) and always taking the shortest path to niche zero.\n-        // Having `None` in niche zero can enable some special optimizations.\n-        //\n-        // Bound selection criteria:\n-        // 1. Select closest to zero given wrapping semantics.\n-        // 2. Avoid moving past zero if possible.\n-        //\n-        // In practice this means that enums with `count > 1` are unlikely to claim niche zero, since they have to fit perfectly.\n-        // If niche zero is already reserved, the selection of bounds are of little interest.\n-        let move_start = |v: WrappingRange| {\n-            let start = v.start.wrapping_sub(count) & max_value;\n-            Some((start, Scalar::Initialized { value, valid_range: v.with_start(start) }))\n-        };\n-        let move_end = |v: WrappingRange| {\n-            let start = v.end.wrapping_add(1) & max_value;\n-            let end = v.end.wrapping_add(count) & max_value;\n-            Some((start, Scalar::Initialized { value, valid_range: v.with_end(end) }))\n-        };\n-        let distance_end_zero = max_value - v.end;\n-        if v.start > v.end {\n-            // zero is unavailable because wrapping occurs\n-            move_end(v)\n-        } else if v.start <= distance_end_zero {\n-            if count <= v.start {\n-                move_start(v)\n-            } else {\n-                // moved past zero, use other bound\n-                move_end(v)\n-            }\n-        } else {\n-            let end = v.end.wrapping_add(count) & max_value;\n-            let overshot_zero = (1..=v.end).contains(&end);\n-            if overshot_zero {\n-                // moved past zero, use other bound\n-                move_start(v)\n-            } else {\n-                move_end(v)\n-            }\n-        }\n-    }\n-}\n-\n-#[derive(PartialEq, Eq, Hash, Clone)]\n-#[cfg_attr(feature = \"nightly\", derive(HashStable_Generic))]\n-pub struct LayoutS<V: Idx> {\n-    /// Says where the fields are located within the layout.\n-    pub fields: FieldsShape,\n-\n-    /// Encodes information about multi-variant layouts.\n-    /// Even with `Multiple` variants, a layout still has its own fields! Those are then\n-    /// shared between all variants. One of them will be the discriminant,\n-    /// but e.g. generators can have more.\n-    ///\n-    /// To access all fields of this layout, both `fields` and the fields of the active variant\n-    /// must be taken into account.\n-    pub variants: Variants<V>,\n-\n-    /// The `abi` defines how this data is passed between functions, and it defines\n-    /// value restrictions via `valid_range`.\n-    ///\n-    /// Note that this is entirely orthogonal to the recursive structure defined by\n-    /// `variants` and `fields`; for example, `ManuallyDrop<Result<isize, isize>>` has\n-    /// `Abi::ScalarPair`! So, even with non-`Aggregate` `abi`, `fields` and `variants`\n-    /// have to be taken into account to find all fields of this layout.\n-    pub abi: Abi,\n-\n-    /// The leaf scalar with the largest number of invalid values\n-    /// (i.e. outside of its `valid_range`), if it exists.\n-    pub largest_niche: Option<Niche>,\n-\n-    pub align: AbiAndPrefAlign,\n-    pub size: Size,\n-}\n-\n-impl<V: Idx> LayoutS<V> {\n-    pub fn scalar<C: HasDataLayout>(cx: &C, scalar: Scalar) -> Self {\n-        let largest_niche = Niche::from_scalar(cx, Size::ZERO, scalar);\n-        let size = scalar.size(cx);\n-        let align = scalar.align(cx);\n-        LayoutS {\n-            variants: Variants::Single { index: V::new(0) },\n-            fields: FieldsShape::Primitive,\n-            abi: Abi::Scalar(scalar),\n-            largest_niche,\n-            size,\n-            align,\n-        }\n-    }\n-\n-    #[inline]\n-    pub fn fields(&self) -> &FieldsShape {\n-        &self.fields\n-    }\n-\n-    #[inline]\n-    pub fn variants(&self) -> &Variants<V> {\n-        &self.variants\n-    }\n-\n-    #[inline]\n-    pub fn abi(&self) -> Abi {\n-        self.abi\n-    }\n-\n-    #[inline]\n-    pub fn largest_niche(&self) -> Option<Niche> {\n-        self.largest_niche\n-    }\n-\n-    #[inline]\n-    pub fn align(&self) -> AbiAndPrefAlign {\n-        self.align\n-    }\n-\n-    #[inline]\n-    pub fn size(&self) -> Size {\n-        self.size\n-    }\n-}\n-\n-impl<V: Idx> fmt::Debug for LayoutS<V> {\n-    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {\n-        // This is how `Layout` used to print before it become\n-        // `Interned<LayoutS>`. We print it like this to avoid having to update\n-        // expected output in a lot of tests.\n-        let LayoutS { size, align, abi, fields, largest_niche, variants } = self;\n-        f.debug_struct(\"Layout\")\n-            .field(\"size\", size)\n-            .field(\"align\", align)\n-            .field(\"abi\", abi)\n-            .field(\"fields\", fields)\n-            .field(\"largest_niche\", largest_niche)\n-            .field(\"variants\", variants)\n-            .finish()\n-    }\n-}\n-\n-#[cfg(feature = \"nightly\")]\n #[derive(Copy, Clone, PartialEq, Eq, Hash, HashStable_Generic)]\n #[rustc_pass_by_value]\n pub struct Layout<'a>(pub Interned<'a, LayoutS<VariantIdx>>);\n \n-#[cfg(feature = \"nightly\")]\n impl<'a> fmt::Debug for Layout<'a> {\n     fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {\n         // See comment on `<LayoutS as Debug>::fmt` above.\n         self.0.0.fmt(f)\n     }\n }\n \n-#[cfg(feature = \"nightly\")]\n impl<'a> Layout<'a> {\n     pub fn fields(self) -> &'a FieldsShape {\n         &self.0.0.fields\n@@ -1533,60 +69,21 @@ impl<'a> Layout<'a> {\n /// to that obtained from `layout_of(ty)`, as we need to produce\n /// layouts for which Rust types do not exist, such as enum variants\n /// or synthetic fields of enums (i.e., discriminants) and fat pointers.\n-#[cfg(feature = \"nightly\")]\n-#[derive(Copy, Clone, Debug, PartialEq, Eq, Hash)]\n-#[cfg_attr(feature = \"nightly\", derive(HashStable_Generic))]\n+#[derive(Copy, Clone, Debug, PartialEq, Eq, Hash, HashStable_Generic)]\n pub struct TyAndLayout<'a, Ty> {\n     pub ty: Ty,\n     pub layout: Layout<'a>,\n }\n \n-#[cfg(feature = \"nightly\")]\n impl<'a, Ty> Deref for TyAndLayout<'a, Ty> {\n     type Target = &'a LayoutS<VariantIdx>;\n     fn deref(&self) -> &&'a LayoutS<VariantIdx> {\n         &self.layout.0.0\n     }\n }\n \n-#[derive(Copy, Clone, PartialEq, Eq, Debug)]\n-pub enum PointerKind {\n-    /// Most general case, we know no restrictions to tell LLVM.\n-    SharedMutable,\n-\n-    /// `&T` where `T` contains no `UnsafeCell`, is `dereferenceable`, `noalias` and `readonly`.\n-    Frozen,\n-\n-    /// `&mut T` which is `dereferenceable` and `noalias` but not `readonly`.\n-    UniqueBorrowed,\n-\n-    /// `&mut !Unpin`, which is `dereferenceable` but neither `noalias` nor `readonly`.\n-    UniqueBorrowedPinned,\n-\n-    /// `Box<T>`, which is `noalias` (even on return types, unlike the above) but neither `readonly`\n-    /// nor `dereferenceable`.\n-    UniqueOwned,\n-}\n-\n-#[derive(Copy, Clone, Debug)]\n-pub struct PointeeInfo {\n-    pub size: Size,\n-    pub align: Align,\n-    pub safe: Option<PointerKind>,\n-    pub address_space: AddressSpace,\n-}\n-\n-/// Used in `might_permit_raw_init` to indicate the kind of initialisation\n-/// that is checked to be valid\n-#[derive(Copy, Clone, Debug, PartialEq, Eq)]\n-pub enum InitKind {\n-    Zero,\n-    UninitMitigated0x01Fill,\n-}\n-\n /// Trait that needs to be implemented by the higher-level type representation\n /// (e.g. `rustc_middle::ty::Ty`), to provide `rustc_target::abi` functionality.\n-#[cfg(feature = \"nightly\")]\n pub trait TyAbiInterface<'a, C>: Sized {\n     fn ty_and_layout_for_variant(\n         this: TyAndLayout<'a, Self>,\n@@ -1605,7 +102,6 @@ pub trait TyAbiInterface<'a, C>: Sized {\n     fn is_unit(this: TyAndLayout<'a, Self>) -> bool;\n }\n \n-#[cfg(feature = \"nightly\")]\n impl<'a, Ty> TyAndLayout<'a, Ty> {\n     pub fn for_variant<C>(self, cx: &C, variant_index: VariantIdx) -> Self\n     where\n@@ -1675,7 +171,7 @@ impl<'a, Ty> TyAndLayout<'a, Ty> {\n     }\n }\n \n-impl<V: Idx> LayoutS<V> {\n+impl<'a, Ty> TyAndLayout<'a, Ty> {\n     /// Returns `true` if the layout corresponds to an unsized type.\n     pub fn is_unsized(&self) -> bool {\n         self.abi.is_unsized()\n@@ -1695,13 +191,3 @@ impl<V: Idx> LayoutS<V> {\n         }\n     }\n }\n-\n-#[derive(Copy, Clone, Debug)]\n-pub enum StructKind {\n-    /// A tuple, closure, or univariant which cannot be coerced to unsized.\n-    AlwaysSized,\n-    /// A univariant, the last field of which may be coerced to unsized.\n-    MaybeUnsized,\n-    /// A univariant, but with a prefix of an arbitrary size & alignment (e.g., enum tag).\n-    Prefixed(Size, Align),\n-}"}, {"sha": "b69a0a645a415913909e0dd127bff1485cfbb770", "filename": "compiler/rustc_target/src/lib.rs", "status": "modified", "additions": 8, "deletions": 15, "changes": 23, "blob_url": "https://github.com/rust-lang/rust/blob/390a637e296ccfaac4c6abd1291b0523e8a8e00b/compiler%2Frustc_target%2Fsrc%2Flib.rs", "raw_url": "https://github.com/rust-lang/rust/raw/390a637e296ccfaac4c6abd1291b0523e8a8e00b/compiler%2Frustc_target%2Fsrc%2Flib.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_target%2Fsrc%2Flib.rs?ref=390a637e296ccfaac4c6abd1291b0523e8a8e00b", "patch": "@@ -8,41 +8,34 @@\n //! LLVM.\n \n #![doc(html_root_url = \"https://doc.rust-lang.org/nightly/nightly-rustc/\")]\n-#![cfg_attr(feature = \"nightly\", feature(assert_matches))]\n-#![cfg_attr(feature = \"nightly\", feature(associated_type_bounds))]\n-#![cfg_attr(feature = \"nightly\", feature(exhaustive_patterns))]\n-#![cfg_attr(feature = \"nightly\", feature(min_specialization))]\n-#![cfg_attr(feature = \"nightly\", feature(never_type))]\n-#![cfg_attr(feature = \"nightly\", feature(rustc_attrs))]\n-#![cfg_attr(feature = \"nightly\", feature(step_trait))]\n+#![feature(assert_matches)]\n+#![feature(associated_type_bounds)]\n+#![feature(exhaustive_patterns)]\n+#![feature(min_specialization)]\n+#![feature(never_type)]\n+#![feature(rustc_attrs)]\n+#![feature(step_trait)]\n #![deny(rustc::untranslatable_diagnostic)]\n #![deny(rustc::diagnostic_outside_of_impl)]\n \n use std::iter::FromIterator;\n use std::path::{Path, PathBuf};\n \n #[macro_use]\n-#[cfg(feature = \"nightly\")]\n extern crate rustc_macros;\n \n #[macro_use]\n-#[cfg(feature = \"nightly\")]\n extern crate tracing;\n \n pub mod abi;\n-#[cfg(feature = \"nightly\")]\n pub mod asm;\n pub mod json;\n-#[cfg(feature = \"nightly\")]\n pub mod spec;\n \n #[cfg(test)]\n mod tests;\n \n-/// Requirements for a `StableHashingContext` to be used in this crate.\n-/// This is a hack to allow using the `HashStable_Generic` derive macro\n-/// instead of implementing everything in `rustc_middle`.\n-pub trait HashStableContext {}\n+pub use rustc_abi::HashStableContext;\n \n /// The name of rustc's own place to organize libraries.\n ///"}, {"sha": "bd5b10d6aa7c36d5093f775dce6800b2ba71c634", "filename": "compiler/rustc_target/src/spec/mod.rs", "status": "modified", "additions": 118, "deletions": 1, "changes": 119, "blob_url": "https://github.com/rust-lang/rust/blob/390a637e296ccfaac4c6abd1291b0523e8a8e00b/compiler%2Frustc_target%2Fsrc%2Fspec%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/390a637e296ccfaac4c6abd1291b0523e8a8e00b/compiler%2Frustc_target%2Fsrc%2Fspec%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_target%2Fsrc%2Fspec%2Fmod.rs?ref=390a637e296ccfaac4c6abd1291b0523e8a8e00b", "patch": "@@ -35,7 +35,10 @@\n //! to the list specified by the target, rather than replace.\n \n use crate::abi::call::Conv;\n-use crate::abi::Endian;\n+use crate::abi::{\n+    AbiAndPrefAlign, AddressSpace, Align, Endian, Integer, Size, TargetDataLayout,\n+    TargetDataLayoutErrors,\n+};\n use crate::json::{Json, ToJson};\n use crate::spec::abi::{lookup as lookup_abi, Abi};\n use crate::spec::crt_objects::{CrtObjects, LinkSelfContainedDefault};\n@@ -1317,6 +1320,120 @@ pub struct Target {\n     pub options: TargetOptions,\n }\n \n+impl Target {\n+    pub fn parse_data_layout<'a>(&'a self) -> Result<TargetDataLayout, TargetDataLayoutErrors<'a>> {\n+        // Parse an address space index from a string.\n+        let parse_address_space = |s: &'a str, cause: &'a str| {\n+            s.parse::<u32>().map(AddressSpace).map_err(|err| {\n+                TargetDataLayoutErrors::InvalidAddressSpace { addr_space: s, cause, err }\n+            })\n+        };\n+\n+        // Parse a bit count from a string.\n+        let parse_bits = |s: &'a str, kind: &'a str, cause: &'a str| {\n+            s.parse::<u64>().map_err(|err| TargetDataLayoutErrors::InvalidBits {\n+                kind,\n+                bit: s,\n+                cause,\n+                err,\n+            })\n+        };\n+\n+        // Parse a size string.\n+        let size = |s: &'a str, cause: &'a str| parse_bits(s, \"size\", cause).map(Size::from_bits);\n+\n+        // Parse an alignment string.\n+        let align = |s: &[&'a str], cause: &'a str| {\n+            if s.is_empty() {\n+                return Err(TargetDataLayoutErrors::MissingAlignment { cause });\n+            }\n+            let align_from_bits = |bits| {\n+                Align::from_bits(bits)\n+                    .map_err(|err| TargetDataLayoutErrors::InvalidAlignment { cause, err })\n+            };\n+            let abi = parse_bits(s[0], \"alignment\", cause)?;\n+            let pref = s.get(1).map_or(Ok(abi), |pref| parse_bits(pref, \"alignment\", cause))?;\n+            Ok(AbiAndPrefAlign { abi: align_from_bits(abi)?, pref: align_from_bits(pref)? })\n+        };\n+\n+        let mut dl = TargetDataLayout::default();\n+        let mut i128_align_src = 64;\n+        for spec in self.data_layout.split('-') {\n+            let spec_parts = spec.split(':').collect::<Vec<_>>();\n+\n+            match &*spec_parts {\n+                [\"e\"] => dl.endian = Endian::Little,\n+                [\"E\"] => dl.endian = Endian::Big,\n+                [p] if p.starts_with('P') => {\n+                    dl.instruction_address_space = parse_address_space(&p[1..], \"P\")?\n+                }\n+                [\"a\", ref a @ ..] => dl.aggregate_align = align(a, \"a\")?,\n+                [\"f32\", ref a @ ..] => dl.f32_align = align(a, \"f32\")?,\n+                [\"f64\", ref a @ ..] => dl.f64_align = align(a, \"f64\")?,\n+                [p @ \"p\", s, ref a @ ..] | [p @ \"p0\", s, ref a @ ..] => {\n+                    dl.pointer_size = size(s, p)?;\n+                    dl.pointer_align = align(a, p)?;\n+                }\n+                [s, ref a @ ..] if s.starts_with('i') => {\n+                    let Ok(bits) = s[1..].parse::<u64>() else {\n+                        size(&s[1..], \"i\")?; // For the user error.\n+                        continue;\n+                    };\n+                    let a = align(a, s)?;\n+                    match bits {\n+                        1 => dl.i1_align = a,\n+                        8 => dl.i8_align = a,\n+                        16 => dl.i16_align = a,\n+                        32 => dl.i32_align = a,\n+                        64 => dl.i64_align = a,\n+                        _ => {}\n+                    }\n+                    if bits >= i128_align_src && bits <= 128 {\n+                        // Default alignment for i128 is decided by taking the alignment of\n+                        // largest-sized i{64..=128}.\n+                        i128_align_src = bits;\n+                        dl.i128_align = a;\n+                    }\n+                }\n+                [s, ref a @ ..] if s.starts_with('v') => {\n+                    let v_size = size(&s[1..], \"v\")?;\n+                    let a = align(a, s)?;\n+                    if let Some(v) = dl.vector_align.iter_mut().find(|v| v.0 == v_size) {\n+                        v.1 = a;\n+                        continue;\n+                    }\n+                    // No existing entry, add a new one.\n+                    dl.vector_align.push((v_size, a));\n+                }\n+                _ => {} // Ignore everything else.\n+            }\n+        }\n+\n+        // Perform consistency checks against the Target information.\n+        if dl.endian != self.endian {\n+            return Err(TargetDataLayoutErrors::InconsistentTargetArchitecture {\n+                dl: dl.endian.as_str(),\n+                target: self.endian.as_str(),\n+            });\n+        }\n+\n+        let target_pointer_width: u64 = self.pointer_width.into();\n+        if dl.pointer_size.bits() != target_pointer_width {\n+            return Err(TargetDataLayoutErrors::InconsistentTargetPointerWidth {\n+                pointer_size: dl.pointer_size.bits(),\n+                target: self.pointer_width,\n+            });\n+        }\n+\n+        dl.c_enum_min_size = match Integer::from_size(Size::from_bits(self.c_enum_min_bits)) {\n+            Ok(bits) => bits,\n+            Err(err) => return Err(TargetDataLayoutErrors::InvalidBitsSize { err }),\n+        };\n+\n+        Ok(dl)\n+    }\n+}\n+\n pub trait HasTargetSpec {\n     fn target_spec(&self) -> &Target;\n }"}, {"sha": "52fbd3ae047732c18b504514e4c210baec942559", "filename": "compiler/rustc_ty_utils/Cargo.toml", "status": "modified", "additions": 0, "deletions": 2, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/390a637e296ccfaac4c6abd1291b0523e8a8e00b/compiler%2Frustc_ty_utils%2FCargo.toml", "raw_url": "https://github.com/rust-lang/rust/raw/390a637e296ccfaac4c6abd1291b0523e8a8e00b/compiler%2Frustc_ty_utils%2FCargo.toml", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_ty_utils%2FCargo.toml?ref=390a637e296ccfaac4c6abd1291b0523e8a8e00b", "patch": "@@ -4,8 +4,6 @@ version = \"0.0.0\"\n edition = \"2021\"\n \n [dependencies]\n-rand = \"0.8.4\"\n-rand_xoshiro = \"0.6.0\"\n tracing = \"0.1\"\n rustc_middle = { path = \"../rustc_middle\" }\n rustc_data_structures = { path = \"../rustc_data_structures\" }"}, {"sha": "7a1cc1e9e6dedfd4d26be986f3aa463281865898", "filename": "compiler/rustc_ty_utils/src/layout.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/390a637e296ccfaac4c6abd1291b0523e8a8e00b/compiler%2Frustc_ty_utils%2Fsrc%2Flayout.rs", "raw_url": "https://github.com/rust-lang/rust/raw/390a637e296ccfaac4c6abd1291b0523e8a8e00b/compiler%2Frustc_ty_utils%2Fsrc%2Flayout.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_ty_utils%2Fsrc%2Flayout.rs?ref=390a637e296ccfaac4c6abd1291b0523e8a8e00b", "patch": "@@ -755,7 +755,7 @@ fn generator_layout<'tcx>(\n \n     size = size.align_to(align.abi);\n \n-    let abi = if prefix.abi.is_uninhabited() || variants.iter().all(|v| v.abi().is_uninhabited()) {\n+    let abi = if prefix.abi.is_uninhabited() || variants.iter().all(|v| v.abi.is_uninhabited()) {\n         Abi::Uninhabited\n     } else {\n         Abi::Aggregate { sized: true }"}, {"sha": "9eb8f684bdb59fc2b81bcaca1e3a09f2d8b166d6", "filename": "compiler/rustc_ty_utils/src/layout_sanity_check.rs", "status": "modified", "additions": 9, "deletions": 9, "changes": 18, "blob_url": "https://github.com/rust-lang/rust/blob/390a637e296ccfaac4c6abd1291b0523e8a8e00b/compiler%2Frustc_ty_utils%2Fsrc%2Flayout_sanity_check.rs", "raw_url": "https://github.com/rust-lang/rust/raw/390a637e296ccfaac4c6abd1291b0523e8a8e00b/compiler%2Frustc_ty_utils%2Fsrc%2Flayout_sanity_check.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/compiler%2Frustc_ty_utils%2Fsrc%2Flayout_sanity_check.rs?ref=390a637e296ccfaac4c6abd1291b0523e8a8e00b", "patch": "@@ -249,27 +249,27 @@ pub(super) fn sanity_check_layout<'tcx>(\n         if let Variants::Multiple { variants, .. } = &layout.variants {\n             for variant in variants.iter() {\n                 // No nested \"multiple\".\n-                assert!(matches!(variant.variants(), Variants::Single { .. }));\n+                assert!(matches!(variant.variants, Variants::Single { .. }));\n                 // Variants should have the same or a smaller size as the full thing,\n                 // and same for alignment.\n-                if variant.size() > layout.size {\n+                if variant.size > layout.size {\n                     bug!(\n                         \"Type with size {} bytes has variant with size {} bytes: {layout:#?}\",\n                         layout.size.bytes(),\n-                        variant.size().bytes(),\n+                        variant.size.bytes(),\n                     )\n                 }\n-                if variant.align().abi > layout.align.abi {\n+                if variant.align.abi > layout.align.abi {\n                     bug!(\n                         \"Type with alignment {} bytes has variant with alignment {} bytes: {layout:#?}\",\n                         layout.align.abi.bytes(),\n-                        variant.align().abi.bytes(),\n+                        variant.align.abi.bytes(),\n                     )\n                 }\n                 // Skip empty variants.\n-                if variant.size() == Size::ZERO\n-                    || variant.fields().count() == 0\n-                    || variant.abi().is_uninhabited()\n+                if variant.size == Size::ZERO\n+                    || variant.fields.count() == 0\n+                    || variant.abi.is_uninhabited()\n                 {\n                     // These are never actually accessed anyway, so we can skip the coherence check\n                     // for them. They also fail that check, since they have\n@@ -282,7 +282,7 @@ pub(super) fn sanity_check_layout<'tcx>(\n                 let scalar_coherent = |s1: Scalar, s2: Scalar| {\n                     s1.size(cx) == s2.size(cx) && s1.align(cx) == s2.align(cx)\n                 };\n-                let abi_coherent = match (layout.abi, variant.abi()) {\n+                let abi_coherent = match (layout.abi, variant.abi) {\n                     (Abi::Scalar(s1), Abi::Scalar(s2)) => scalar_coherent(s1, s2),\n                     (Abi::ScalarPair(a1, b1), Abi::ScalarPair(a2, b2)) => {\n                         scalar_coherent(a1, a2) && scalar_coherent(b1, b2)"}, {"sha": "acbe3f22889cc03d10cf4f541477c8f6a2b9d8f7", "filename": "src/librustdoc/html/render/print_item.rs", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/rust-lang/rust/blob/390a637e296ccfaac4c6abd1291b0523e8a8e00b/src%2Flibrustdoc%2Fhtml%2Frender%2Fprint_item.rs", "raw_url": "https://github.com/rust-lang/rust/raw/390a637e296ccfaac4c6abd1291b0523e8a8e00b/src%2Flibrustdoc%2Fhtml%2Frender%2Fprint_item.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustdoc%2Fhtml%2Frender%2Fprint_item.rs?ref=390a637e296ccfaac4c6abd1291b0523e8a8e00b", "patch": "@@ -1893,10 +1893,10 @@ fn document_non_exhaustive(w: &mut Buffer, item: &clean::Item) {\n \n fn document_type_layout(w: &mut Buffer, cx: &Context<'_>, ty_def_id: DefId) {\n     fn write_size_of_layout(w: &mut Buffer, layout: &LayoutS<VariantIdx>, tag_size: u64) {\n-        if layout.abi().is_unsized() {\n+        if layout.abi.is_unsized() {\n             write!(w, \"(unsized)\");\n         } else {\n-            let bytes = layout.size().bytes() - tag_size;\n+            let bytes = layout.size.bytes() - tag_size;\n             write!(w, \"{size} byte{pl}\", size = bytes, pl = if bytes == 1 { \"\" } else { \"s\" },);\n         }\n     }"}]}