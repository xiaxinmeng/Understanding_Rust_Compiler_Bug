{"sha": "82ad4f1f45a60995c0955e28bbed3885008e3ee5", "node_id": "MDY6Q29tbWl0NzI0NzEyOjgyYWQ0ZjFmNDVhNjA5OTVjMDk1NWUyOGJiZWQzODg1MDA4ZTNlZTU=", "commit": {"author": {"name": "Nicholas Nethercote", "email": "nnethercote@mozilla.com", "date": "2019-02-17T23:06:26Z"}, "committer": {"name": "Nicholas Nethercote", "email": "nnethercote@mozilla.com", "date": "2019-02-17T23:06:26Z"}, "message": "Make `interpolated_to_tokenstream` a method on `Nonterminal`.", "tree": {"sha": "07331d91a2eab7290a259d56c1ebfea6aba116a6", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/07331d91a2eab7290a259d56c1ebfea6aba116a6"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/82ad4f1f45a60995c0955e28bbed3885008e3ee5", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/82ad4f1f45a60995c0955e28bbed3885008e3ee5", "html_url": "https://github.com/rust-lang/rust/commit/82ad4f1f45a60995c0955e28bbed3885008e3ee5", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/82ad4f1f45a60995c0955e28bbed3885008e3ee5/comments", "author": {"login": "nnethercote", "id": 1940286, "node_id": "MDQ6VXNlcjE5NDAyODY=", "avatar_url": "https://avatars.githubusercontent.com/u/1940286?v=4", "gravatar_id": "", "url": "https://api.github.com/users/nnethercote", "html_url": "https://github.com/nnethercote", "followers_url": "https://api.github.com/users/nnethercote/followers", "following_url": "https://api.github.com/users/nnethercote/following{/other_user}", "gists_url": "https://api.github.com/users/nnethercote/gists{/gist_id}", "starred_url": "https://api.github.com/users/nnethercote/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/nnethercote/subscriptions", "organizations_url": "https://api.github.com/users/nnethercote/orgs", "repos_url": "https://api.github.com/users/nnethercote/repos", "events_url": "https://api.github.com/users/nnethercote/events{/privacy}", "received_events_url": "https://api.github.com/users/nnethercote/received_events", "type": "User", "site_admin": false}, "committer": {"login": "nnethercote", "id": 1940286, "node_id": "MDQ6VXNlcjE5NDAyODY=", "avatar_url": "https://avatars.githubusercontent.com/u/1940286?v=4", "gravatar_id": "", "url": "https://api.github.com/users/nnethercote", "html_url": "https://github.com/nnethercote", "followers_url": "https://api.github.com/users/nnethercote/followers", "following_url": "https://api.github.com/users/nnethercote/following{/other_user}", "gists_url": "https://api.github.com/users/nnethercote/gists{/gist_id}", "starred_url": "https://api.github.com/users/nnethercote/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/nnethercote/subscriptions", "organizations_url": "https://api.github.com/users/nnethercote/orgs", "repos_url": "https://api.github.com/users/nnethercote/repos", "events_url": "https://api.github.com/users/nnethercote/events{/privacy}", "received_events_url": "https://api.github.com/users/nnethercote/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "f0d8fbd283654479c50a2fec94bf2362eed0f189", "url": "https://api.github.com/repos/rust-lang/rust/commits/f0d8fbd283654479c50a2fec94bf2362eed0f189", "html_url": "https://github.com/rust-lang/rust/commit/f0d8fbd283654479c50a2fec94bf2362eed0f189"}], "stats": {"total": 171, "additions": 85, "deletions": 86}, "files": [{"sha": "961e892789a81e919663c35a462aa37dff1285c2", "filename": "src/librustc/hir/lowering.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/82ad4f1f45a60995c0955e28bbed3885008e3ee5/src%2Flibrustc%2Fhir%2Flowering.rs", "raw_url": "https://github.com/rust-lang/rust/raw/82ad4f1f45a60995c0955e28bbed3885008e3ee5/src%2Flibrustc%2Fhir%2Flowering.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc%2Fhir%2Flowering.rs?ref=82ad4f1f45a60995c0955e28bbed3885008e3ee5", "patch": "@@ -1132,7 +1132,7 @@ impl<'a> LoweringContext<'a> {\n     fn lower_token(&mut self, token: Token, span: Span) -> TokenStream {\n         match token {\n             Token::Interpolated(nt) => {\n-                let tts = Token::interpolated_to_tokenstream(&self.sess.parse_sess, nt, span);\n+                let tts = nt.to_tokenstream(&self.sess.parse_sess, span);\n                 self.lower_token_stream(tts)\n             }\n             other => TokenTree::Token(span, other).into(),"}, {"sha": "eec422d6266c36574f01db02cb660d3510d26833", "filename": "src/libsyntax/parse/token.rs", "status": "modified", "additions": 81, "deletions": 82, "changes": 163, "blob_url": "https://github.com/rust-lang/rust/blob/82ad4f1f45a60995c0955e28bbed3885008e3ee5/src%2Flibsyntax%2Fparse%2Ftoken.rs", "raw_url": "https://github.com/rust-lang/rust/raw/82ad4f1f45a60995c0955e28bbed3885008e3ee5/src%2Flibsyntax%2Fparse%2Ftoken.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Ftoken.rs?ref=82ad4f1f45a60995c0955e28bbed3885008e3ee5", "patch": "@@ -86,7 +86,7 @@ impl Lit {\n         }\n     }\n \n-    // See comments in `interpolated_to_tokenstream` for why we care about\n+    // See comments in `Nonterminal::to_tokenstream` for why we care about\n     // *probably* equal here rather than actual equality\n     fn probably_equal_for_proc_macro(&self, other: &Lit) -> bool {\n         mem::discriminant(self) == mem::discriminant(other)\n@@ -502,87 +502,7 @@ impl Token {\n         }\n     }\n \n-    pub fn interpolated_to_tokenstream(sess: &ParseSess, nt: Lrc<Nonterminal>, span: Span)\n-                                       -> TokenStream {\n-        // An `Interpolated` token means that we have a `Nonterminal`\n-        // which is often a parsed AST item. At this point we now need\n-        // to convert the parsed AST to an actual token stream, e.g.\n-        // un-parse it basically.\n-        //\n-        // Unfortunately there's not really a great way to do that in a\n-        // guaranteed lossless fashion right now. The fallback here is\n-        // to just stringify the AST node and reparse it, but this loses\n-        // all span information.\n-        //\n-        // As a result, some AST nodes are annotated with the token\n-        // stream they came from. Here we attempt to extract these\n-        // lossless token streams before we fall back to the\n-        // stringification.\n-        let tokens = match *nt {\n-            Nonterminal::NtItem(ref item) => {\n-                prepend_attrs(sess, &item.attrs, item.tokens.as_ref(), span)\n-            }\n-            Nonterminal::NtTraitItem(ref item) => {\n-                prepend_attrs(sess, &item.attrs, item.tokens.as_ref(), span)\n-            }\n-            Nonterminal::NtImplItem(ref item) => {\n-                prepend_attrs(sess, &item.attrs, item.tokens.as_ref(), span)\n-            }\n-            Nonterminal::NtIdent(ident, is_raw) => {\n-                let token = Token::Ident(ident, is_raw);\n-                Some(TokenTree::Token(ident.span, token).into())\n-            }\n-            Nonterminal::NtLifetime(ident) => {\n-                let token = Token::Lifetime(ident);\n-                Some(TokenTree::Token(ident.span, token).into())\n-            }\n-            Nonterminal::NtTT(ref tt) => {\n-                Some(tt.clone().into())\n-            }\n-            _ => None,\n-        };\n-\n-        // FIXME(#43081): Avoid this pretty-print + reparse hack\n-        let source = pprust::nonterminal_to_string(&nt);\n-        let filename = FileName::macro_expansion_source_code(&source);\n-        let (tokens_for_real, errors) =\n-            parse_stream_from_source_str(filename, source, sess, Some(span));\n-        emit_unclosed_delims(&errors, &sess.span_diagnostic);\n-\n-        // During early phases of the compiler the AST could get modified\n-        // directly (e.g., attributes added or removed) and the internal cache\n-        // of tokens my not be invalidated or updated. Consequently if the\n-        // \"lossless\" token stream disagrees with our actual stringification\n-        // (which has historically been much more battle-tested) then we go\n-        // with the lossy stream anyway (losing span information).\n-        //\n-        // Note that the comparison isn't `==` here to avoid comparing spans,\n-        // but it *also* is a \"probable\" equality which is a pretty weird\n-        // definition. We mostly want to catch actual changes to the AST\n-        // like a `#[cfg]` being processed or some weird `macro_rules!`\n-        // expansion.\n-        //\n-        // What we *don't* want to catch is the fact that a user-defined\n-        // literal like `0xf` is stringified as `15`, causing the cached token\n-        // stream to not be literal `==` token-wise (ignoring spans) to the\n-        // token stream we got from stringification.\n-        //\n-        // Instead the \"probably equal\" check here is \"does each token\n-        // recursively have the same discriminant?\" We basically don't look at\n-        // the token values here and assume that such fine grained token stream\n-        // modifications, including adding/removing typically non-semantic\n-        // tokens such as extra braces and commas, don't happen.\n-        if let Some(tokens) = tokens {\n-            if tokens.probably_equal_for_proc_macro(&tokens_for_real) {\n-                return tokens\n-            }\n-            info!(\"cached tokens found, but they're not \\\"probably equal\\\", \\\n-                   going with stringified version\");\n-        }\n-        return tokens_for_real\n-    }\n-\n-    // See comments in `interpolated_to_tokenstream` for why we care about\n+    // See comments in `Nonterminal::to_tokenstream` for why we care about\n     // *probably* equal here rather than actual equality\n     crate fn probably_equal_for_proc_macro(&self, other: &Token) -> bool {\n         if mem::discriminant(self) != mem::discriminant(other) {\n@@ -714,6 +634,85 @@ impl fmt::Debug for Nonterminal {\n     }\n }\n \n+impl Nonterminal {\n+    pub fn to_tokenstream(&self, sess: &ParseSess, span: Span) -> TokenStream {\n+        // A `Nonterminal` is often a parsed AST item. At this point we now\n+        // need to convert the parsed AST to an actual token stream, e.g.\n+        // un-parse it basically.\n+        //\n+        // Unfortunately there's not really a great way to do that in a\n+        // guaranteed lossless fashion right now. The fallback here is to just\n+        // stringify the AST node and reparse it, but this loses all span\n+        // information.\n+        //\n+        // As a result, some AST nodes are annotated with the token stream they\n+        // came from. Here we attempt to extract these lossless token streams\n+        // before we fall back to the stringification.\n+        let tokens = match *self {\n+            Nonterminal::NtItem(ref item) => {\n+                prepend_attrs(sess, &item.attrs, item.tokens.as_ref(), span)\n+            }\n+            Nonterminal::NtTraitItem(ref item) => {\n+                prepend_attrs(sess, &item.attrs, item.tokens.as_ref(), span)\n+            }\n+            Nonterminal::NtImplItem(ref item) => {\n+                prepend_attrs(sess, &item.attrs, item.tokens.as_ref(), span)\n+            }\n+            Nonterminal::NtIdent(ident, is_raw) => {\n+                let token = Token::Ident(ident, is_raw);\n+                Some(TokenTree::Token(ident.span, token).into())\n+            }\n+            Nonterminal::NtLifetime(ident) => {\n+                let token = Token::Lifetime(ident);\n+                Some(TokenTree::Token(ident.span, token).into())\n+            }\n+            Nonterminal::NtTT(ref tt) => {\n+                Some(tt.clone().into())\n+            }\n+            _ => None,\n+        };\n+\n+        // FIXME(#43081): Avoid this pretty-print + reparse hack\n+        let source = pprust::nonterminal_to_string(self);\n+        let filename = FileName::macro_expansion_source_code(&source);\n+        let (tokens_for_real, errors) =\n+            parse_stream_from_source_str(filename, source, sess, Some(span));\n+        emit_unclosed_delims(&errors, &sess.span_diagnostic);\n+\n+        // During early phases of the compiler the AST could get modified\n+        // directly (e.g., attributes added or removed) and the internal cache\n+        // of tokens my not be invalidated or updated. Consequently if the\n+        // \"lossless\" token stream disagrees with our actual stringification\n+        // (which has historically been much more battle-tested) then we go\n+        // with the lossy stream anyway (losing span information).\n+        //\n+        // Note that the comparison isn't `==` here to avoid comparing spans,\n+        // but it *also* is a \"probable\" equality which is a pretty weird\n+        // definition. We mostly want to catch actual changes to the AST\n+        // like a `#[cfg]` being processed or some weird `macro_rules!`\n+        // expansion.\n+        //\n+        // What we *don't* want to catch is the fact that a user-defined\n+        // literal like `0xf` is stringified as `15`, causing the cached token\n+        // stream to not be literal `==` token-wise (ignoring spans) to the\n+        // token stream we got from stringification.\n+        //\n+        // Instead the \"probably equal\" check here is \"does each token\n+        // recursively have the same discriminant?\" We basically don't look at\n+        // the token values here and assume that such fine grained token stream\n+        // modifications, including adding/removing typically non-semantic\n+        // tokens such as extra braces and commas, don't happen.\n+        if let Some(tokens) = tokens {\n+            if tokens.probably_equal_for_proc_macro(&tokens_for_real) {\n+                return tokens\n+            }\n+            info!(\"cached tokens found, but they're not \\\"probably equal\\\", \\\n+                   going with stringified version\");\n+        }\n+        return tokens_for_real\n+    }\n+}\n+\n crate fn is_op(tok: &Token) -> bool {\n     match *tok {\n         OpenDelim(..) | CloseDelim(..) | Literal(..) | DocComment(..) |"}, {"sha": "283679e758b54e5f6c45268b1ac4a9998a1b6b18", "filename": "src/libsyntax/tokenstream.rs", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/rust-lang/rust/blob/82ad4f1f45a60995c0955e28bbed3885008e3ee5/src%2Flibsyntax%2Ftokenstream.rs", "raw_url": "https://github.com/rust-lang/rust/raw/82ad4f1f45a60995c0955e28bbed3885008e3ee5/src%2Flibsyntax%2Ftokenstream.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Ftokenstream.rs?ref=82ad4f1f45a60995c0955e28bbed3885008e3ee5", "patch": "@@ -72,7 +72,7 @@ impl TokenTree {\n         }\n     }\n \n-    // See comments in `interpolated_to_tokenstream` for why we care about\n+    // See comments in `Nonterminal::to_tokenstream` for why we care about\n     // *probably* equal here rather than actual equality\n     //\n     // This is otherwise the same as `eq_unspanned`, only recursing with a\n@@ -310,7 +310,7 @@ impl TokenStream {\n         t1.next().is_none() && t2.next().is_none()\n     }\n \n-    // See comments in `interpolated_to_tokenstream` for why we care about\n+    // See comments in `Nonterminal::to_tokenstream` for why we care about\n     // *probably* equal here rather than actual equality\n     //\n     // This is otherwise the same as `eq_unspanned`, only recursing with a"}, {"sha": "699539b62f515b98238bd718e6523ff5aa2864e0", "filename": "src/libsyntax_ext/proc_macro_server.rs", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/82ad4f1f45a60995c0955e28bbed3885008e3ee5/src%2Flibsyntax_ext%2Fproc_macro_server.rs", "raw_url": "https://github.com/rust-lang/rust/raw/82ad4f1f45a60995c0955e28bbed3885008e3ee5/src%2Flibsyntax_ext%2Fproc_macro_server.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax_ext%2Fproc_macro_server.rs?ref=82ad4f1f45a60995c0955e28bbed3885008e3ee5", "patch": "@@ -179,7 +179,7 @@ impl FromInternal<(TreeAndJoint, &'_ ParseSess, &'_ mut Vec<Self>)>\n             }\n \n             Interpolated(nt) => {\n-                let stream = Token::interpolated_to_tokenstream(sess, nt, span);\n+                let stream = nt.to_tokenstream(sess, span);\n                 TokenTree::Group(Group {\n                     delimiter: Delimiter::None,\n                     stream,"}]}