{"sha": "4989a56448c7e3047e0538ff4ef54c49db8a5a4f", "node_id": "MDY6Q29tbWl0NzI0NzEyOjQ5ODlhNTY0NDhjN2UzMDQ3ZTA1MzhmZjRlZjU0YzQ5ZGI4YTVhNGY=", "commit": {"author": {"name": "Corey Richardson", "email": "corey@octayn.net", "date": "2014-06-09T20:12:30Z"}, "committer": {"name": "Corey Richardson", "email": "corey@octayn.net", "date": "2014-07-09T07:06:27Z"}, "message": "syntax: doc comments all the things", "tree": {"sha": "99a15ab91675cd360008b542c3cde8a1f74d6f86", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/99a15ab91675cd360008b542c3cde8a1f74d6f86"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/4989a56448c7e3047e0538ff4ef54c49db8a5a4f", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/4989a56448c7e3047e0538ff4ef54c49db8a5a4f", "html_url": "https://github.com/rust-lang/rust/commit/4989a56448c7e3047e0538ff4ef54c49db8a5a4f", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/4989a56448c7e3047e0538ff4ef54c49db8a5a4f/comments", "author": {"login": "emberian", "id": 704250, "node_id": "MDQ6VXNlcjcwNDI1MA==", "avatar_url": "https://avatars.githubusercontent.com/u/704250?v=4", "gravatar_id": "", "url": "https://api.github.com/users/emberian", "html_url": "https://github.com/emberian", "followers_url": "https://api.github.com/users/emberian/followers", "following_url": "https://api.github.com/users/emberian/following{/other_user}", "gists_url": "https://api.github.com/users/emberian/gists{/gist_id}", "starred_url": "https://api.github.com/users/emberian/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/emberian/subscriptions", "organizations_url": "https://api.github.com/users/emberian/orgs", "repos_url": "https://api.github.com/users/emberian/repos", "events_url": "https://api.github.com/users/emberian/events{/privacy}", "received_events_url": "https://api.github.com/users/emberian/received_events", "type": "User", "site_admin": false}, "committer": {"login": "emberian", "id": 704250, "node_id": "MDQ6VXNlcjcwNDI1MA==", "avatar_url": "https://avatars.githubusercontent.com/u/704250?v=4", "gravatar_id": "", "url": "https://api.github.com/users/emberian", "html_url": "https://github.com/emberian", "followers_url": "https://api.github.com/users/emberian/followers", "following_url": "https://api.github.com/users/emberian/following{/other_user}", "gists_url": "https://api.github.com/users/emberian/gists{/gist_id}", "starred_url": "https://api.github.com/users/emberian/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/emberian/subscriptions", "organizations_url": "https://api.github.com/users/emberian/orgs", "repos_url": "https://api.github.com/users/emberian/repos", "events_url": "https://api.github.com/users/emberian/events{/privacy}", "received_events_url": "https://api.github.com/users/emberian/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "5716abe3f019ab7d9c8cdde9879332040191cf88", "url": "https://api.github.com/repos/rust-lang/rust/commits/5716abe3f019ab7d9c8cdde9879332040191cf88", "html_url": "https://github.com/rust-lang/rust/commit/5716abe3f019ab7d9c8cdde9879332040191cf88"}], "stats": {"total": 2273, "additions": 1136, "deletions": 1137}, "files": [{"sha": "5aaf7ed3dba5df1953da27414c34a52120a74c24", "filename": "src/libsyntax/abi.rs", "status": "modified", "additions": 8, "deletions": 13, "changes": 21, "blob_url": "https://github.com/rust-lang/rust/blob/4989a56448c7e3047e0538ff4ef54c49db8a5a4f/src%2Flibsyntax%2Fabi.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4989a56448c7e3047e0538ff4ef54c49db8a5a4f/src%2Flibsyntax%2Fabi.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fabi.rs?ref=4989a56448c7e3047e0538ff4ef54c49db8a5a4f", "patch": "@@ -60,9 +60,12 @@ pub struct AbiData {\n }\n \n pub enum AbiArchitecture {\n-    RustArch,   // Not a real ABI (e.g., intrinsic)\n-    AllArch,    // An ABI that specifies cross-platform defaults (e.g., \"C\")\n-    Archs(u32)  // Multiple architectures (bitset)\n+    /// Not a real ABI (e.g., intrinsic)\n+    RustArch,\n+    /// An ABI that specifies cross-platform defaults (e.g., \"C\")\n+    AllArch,\n+    /// Multiple architectures (bitset)\n+    Archs(u32)\n }\n \n static AbiDatas: &'static [AbiData] = &[\n@@ -84,21 +87,13 @@ static AbiDatas: &'static [AbiData] = &[\n     AbiData {abi: RustIntrinsic, name: \"rust-intrinsic\", abi_arch: RustArch},\n ];\n \n+/// Iterates through each of the defined ABIs.\n fn each_abi(op: |abi: Abi| -> bool) -> bool {\n-    /*!\n-     *\n-     * Iterates through each of the defined ABIs.\n-     */\n-\n     AbiDatas.iter().advance(|abi_data| op(abi_data.abi))\n }\n \n+/// Returns the ABI with the given name (if any).\n pub fn lookup(name: &str) -> Option<Abi> {\n-    /*!\n-     *\n-     * Returns the ABI with the given name (if any).\n-     */\n-\n     let mut res = None;\n \n     each_abi(|abi| {"}, {"sha": "c5afc5067b6a980bd990d23ef167d1649e13e031", "filename": "src/libsyntax/ast.rs", "status": "modified", "additions": 188, "deletions": 178, "changes": 366, "blob_url": "https://github.com/rust-lang/rust/blob/4989a56448c7e3047e0538ff4ef54c49db8a5a4f/src%2Flibsyntax%2Fast.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4989a56448c7e3047e0538ff4ef54c49db8a5a4f/src%2Flibsyntax%2Fast.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fast.rs?ref=4989a56448c7e3047e0538ff4ef54c49db8a5a4f", "patch": "@@ -24,7 +24,8 @@ use std::rc::Rc;\n use std::gc::{Gc, GC};\n use serialize::{Encodable, Decodable, Encoder, Decoder};\n \n-/// A pointer abstraction. FIXME(eddyb) #10676 use Rc<T> in the future.\n+/// A pointer abstraction.\n+// FIXME(eddyb) #10676 use Rc<T> in the future.\n pub type P<T> = Gc<T>;\n \n #[allow(non_snake_case_functions)]\n@@ -36,10 +37,10 @@ pub fn P<T: 'static>(value: T) -> P<T> {\n // FIXME #6993: in librustc, uses of \"ident\" should be replaced\n // by just \"Name\".\n \n-// an identifier contains a Name (index into the interner\n-// table) and a SyntaxContext to track renaming and\n-// macro expansion per Flatt et al., \"Macros\n-// That Work Together\"\n+/// An identifier contains a Name (index into the interner\n+/// table) and a SyntaxContext to track renaming and\n+/// macro expansion per Flatt et al., \"Macros\n+/// That Work Together\"\n #[deriving(Clone, Hash, PartialOrd, Eq, Ord, Show)]\n pub struct Ident {\n     pub name: Name,\n@@ -122,10 +123,9 @@ pub struct Lifetime {\n     pub name: Name\n }\n \n-// a \"Path\" is essentially Rust's notion of a name;\n-// for instance: std::cmp::PartialEq  .  It's represented\n-// as a sequence of identifiers, along with a bunch\n-// of supporting information.\n+/// A \"Path\" is essentially Rust's notion of a name; for instance:\n+/// std::cmp::PartialEq  .  It's represented as a sequence of identifiers,\n+/// along with a bunch of supporting information.\n #[deriving(Clone, PartialEq, Eq, Encodable, Decodable, Hash)]\n pub struct Path {\n     pub span: Span,\n@@ -163,15 +163,15 @@ pub struct DefId {\n pub static LOCAL_CRATE: CrateNum = 0;\n pub static CRATE_NODE_ID: NodeId = 0;\n \n-// When parsing and doing expansions, we initially give all AST nodes this AST\n-// node value. Then later, in the renumber pass, we renumber them to have\n-// small, positive ids.\n+/// When parsing and doing expansions, we initially give all AST nodes this AST\n+/// node value. Then later, in the renumber pass, we renumber them to have\n+/// small, positive ids.\n pub static DUMMY_NODE_ID: NodeId = -1;\n \n-// The AST represents all type param bounds as types.\n-// typeck::collect::compute_bounds matches these against\n-// the \"special\" built-in traits (see middle::lang_items) and\n-// detects Copy, Send and Share.\n+/// The AST represents all type param bounds as types.\n+/// typeck::collect::compute_bounds matches these against\n+/// the \"special\" built-in traits (see middle::lang_items) and\n+/// detects Copy, Send and Share.\n #[deriving(Clone, PartialEq, Eq, Encodable, Decodable, Hash)]\n pub enum TyParamBound {\n     TraitTyParamBound(TraitRef),\n@@ -210,9 +210,9 @@ impl Generics {\n     }\n }\n \n-// The set of MetaItems that define the compilation environment of the crate,\n-// used to drive conditional compilation\n-pub type CrateConfig = Vec<Gc<MetaItem>>;\n+/// The set of MetaItems that define the compilation environment of the crate,\n+/// used to drive conditional compilation\n+pub type CrateConfig = Vec<Gc<MetaItem>> ;\n \n #[deriving(Clone, PartialEq, Eq, Encodable, Decodable, Hash)]\n pub struct Crate {\n@@ -289,13 +289,13 @@ pub enum BindingMode {\n pub enum Pat_ {\n     PatWild,\n     PatWildMulti,\n-    // A PatIdent may either be a new bound variable,\n-    // or a nullary enum (in which case the third field\n-    // is None).\n-    // In the nullary enum case, the parser can't determine\n-    // which it is. The resolver determines this, and\n-    // records this pattern's NodeId in an auxiliary\n-    // set (of \"PatIdents that refer to nullary enums\")\n+    /// A PatIdent may either be a new bound variable,\n+    /// or a nullary enum (in which case the third field\n+    /// is None).\n+    /// In the nullary enum case, the parser can't determine\n+    /// which it is. The resolver determines this, and\n+    /// records this pattern's NodeId in an auxiliary\n+    /// set (of \"PatIdents that refer to nullary enums\")\n     PatIdent(BindingMode, SpannedIdent, Option<Gc<Pat>>),\n     PatEnum(Path, Option<Vec<Gc<Pat>>>), /* \"none\" means a * pattern where\n                                      * we don't bind the fields to names */\n@@ -305,8 +305,8 @@ pub enum Pat_ {\n     PatRegion(Gc<Pat>), // reference pattern\n     PatLit(Gc<Expr>),\n     PatRange(Gc<Expr>, Gc<Expr>),\n-    // [a, b, ..i, y, z] is represented as\n-    // PatVec(~[a, b], Some(i), ~[y, z])\n+    /// [a, b, ..i, y, z] is represented as:\n+    ///     PatVec(~[a, b], Some(i), ~[y, z])\n     PatVec(Vec<Gc<Pat>>, Option<Gc<Pat>>, Vec<Gc<Pat>>),\n     PatMac(Mac),\n }\n@@ -319,9 +319,12 @@ pub enum Mutability {\n \n #[deriving(Clone, PartialEq, Eq, Encodable, Decodable, Hash)]\n pub enum ExprVstore {\n-    ExprVstoreUniq,                 // ~[1,2,3,4]\n-    ExprVstoreSlice,                // &[1,2,3,4]\n-    ExprVstoreMutSlice,             // &mut [1,2,3,4]\n+    /// ~[1, 2, 3, 4]\n+    ExprVstoreUniq,\n+    /// &[1, 2, 3, 4]\n+    ExprVstoreSlice,\n+    /// &mut [1, 2, 3, 4]\n+    ExprVstoreMutSlice,\n }\n \n #[deriving(Clone, PartialEq, Eq, Encodable, Decodable, Hash)]\n@@ -359,16 +362,16 @@ pub type Stmt = Spanned<Stmt_>;\n \n #[deriving(Clone, PartialEq, Eq, Encodable, Decodable, Hash)]\n pub enum Stmt_ {\n-    // could be an item or a local (let) binding:\n+    /// Could be an item or a local (let) binding:\n     StmtDecl(Gc<Decl>, NodeId),\n \n-    // expr without trailing semi-colon (must have unit type):\n+    /// Expr without trailing semi-colon (must have unit type):\n     StmtExpr(Gc<Expr>, NodeId),\n \n-    // expr with trailing semi-colon (may have any type):\n+    /// Expr with trailing semi-colon (may have any type):\n     StmtSemi(Gc<Expr>, NodeId),\n \n-    // bool: is there a trailing sem-colon?\n+    /// bool: is there a trailing sem-colon?\n     StmtMac(Mac, bool),\n }\n \n@@ -397,9 +400,9 @@ pub type Decl = Spanned<Decl_>;\n \n #[deriving(PartialEq, Eq, Encodable, Decodable, Hash)]\n pub enum Decl_ {\n-    // a local (let) binding:\n+    /// A local (let) binding:\n     DeclLocal(Gc<Local>),\n-    // an item binding:\n+    /// An item binding:\n     DeclItem(Gc<Item>),\n }\n \n@@ -443,7 +446,7 @@ pub struct Expr {\n #[deriving(Clone, PartialEq, Eq, Encodable, Decodable, Hash)]\n pub enum Expr_ {\n     ExprVstore(Gc<Expr>, ExprVstore),\n-    // First expr is the place; second expr is the value.\n+    /// First expr is the place; second expr is the value.\n     ExprBox(Gc<Expr>, Gc<Expr>),\n     ExprVec(Vec<Gc<Expr>>),\n     ExprCall(Gc<Expr>, Vec<Gc<Expr>>),\n@@ -483,124 +486,121 @@ pub enum Expr_ {\n \n     ExprMac(Mac),\n \n-    // A struct literal expression.\n+    /// A struct literal expression.\n     ExprStruct(Path, Vec<Field> , Option<Gc<Expr>> /* base */),\n \n-    // A vector literal constructed from one repeated element.\n+    /// A vector literal constructed from one repeated element.\n     ExprRepeat(Gc<Expr> /* element */, Gc<Expr> /* count */),\n \n-    // No-op: used solely so we can pretty-print faithfully\n+    /// No-op: used solely so we can pretty-print faithfully\n     ExprParen(Gc<Expr>)\n }\n \n-// When the main rust parser encounters a syntax-extension invocation, it\n-// parses the arguments to the invocation as a token-tree. This is a very\n-// loose structure, such that all sorts of different AST-fragments can\n-// be passed to syntax extensions using a uniform type.\n-//\n-// If the syntax extension is an MBE macro, it will attempt to match its\n-// LHS \"matchers\" against the provided token tree, and if it finds a\n-// match, will transcribe the RHS token tree, splicing in any captured\n-// macro_parser::matched_nonterminals into the TTNonterminals it finds.\n-//\n-// The RHS of an MBE macro is the only place a TTNonterminal or TTSeq\n-// makes any real sense. You could write them elsewhere but nothing\n-// else knows what to do with them, so you'll probably get a syntax\n-// error.\n-//\n+/// When the main rust parser encounters a syntax-extension invocation, it\n+/// parses the arguments to the invocation as a token-tree. This is a very\n+/// loose structure, such that all sorts of different AST-fragments can\n+/// be passed to syntax extensions using a uniform type.\n+///\n+/// If the syntax extension is an MBE macro, it will attempt to match its\n+/// LHS \"matchers\" against the provided token tree, and if it finds a\n+/// match, will transcribe the RHS token tree, splicing in any captured\n+/// macro_parser::matched_nonterminals into the TTNonterminals it finds.\n+///\n+/// The RHS of an MBE macro is the only place a TTNonterminal or TTSeq\n+/// makes any real sense. You could write them elsewhere but nothing\n+/// else knows what to do with them, so you'll probably get a syntax\n+/// error.\n #[deriving(Clone, PartialEq, Eq, Encodable, Decodable, Hash)]\n #[doc=\"For macro invocations; parsing is delegated to the macro\"]\n pub enum TokenTree {\n-    // a single token\n+    /// A single token\n     TTTok(Span, ::parse::token::Token),\n-    // a delimited sequence (the delimiters appear as the first\n-    // and last elements of the vector)\n+    /// A delimited sequence (the delimiters appear as the first\n+    /// and last elements of the vector)\n     // FIXME(eddyb) #6308 Use Rc<[TokenTree]> after DST.\n     TTDelim(Rc<Vec<TokenTree>>),\n \n     // These only make sense for right-hand-sides of MBE macros:\n \n-    // a kleene-style repetition sequence with a span, a TTForest,\n-    // an optional separator, and a boolean where true indicates\n-    // zero or more (..), and false indicates one or more (+).\n+    /// A kleene-style repetition sequence with a span, a TTForest,\n+    /// an optional separator, and a boolean where true indicates\n+    /// zero or more (..), and false indicates one or more (+).\n     // FIXME(eddyb) #6308 Use Rc<[TokenTree]> after DST.\n     TTSeq(Span, Rc<Vec<TokenTree>>, Option<::parse::token::Token>, bool),\n \n-    // a syntactic variable that will be filled in by macro expansion.\n+    /// A syntactic variable that will be filled in by macro expansion.\n     TTNonterminal(Span, Ident)\n }\n \n-//\n-// Matchers are nodes defined-by and recognized-by the main rust parser and\n-// language, but they're only ever found inside syntax-extension invocations;\n-// indeed, the only thing that ever _activates_ the rules in the rust parser\n-// for parsing a matcher is a matcher looking for the 'matchers' nonterminal\n-// itself. Matchers represent a small sub-language for pattern-matching\n-// token-trees, and are thus primarily used by the macro-defining extension\n-// itself.\n-//\n-// MatchTok\n-// --------\n-//\n-//     A matcher that matches a single token, denoted by the token itself. So\n-//     long as there's no $ involved.\n-//\n-//\n-// MatchSeq\n-// --------\n-//\n-//     A matcher that matches a sequence of sub-matchers, denoted various\n-//     possible ways:\n-//\n-//             $(M)*       zero or more Ms\n-//             $(M)+       one or more Ms\n-//             $(M),+      one or more comma-separated Ms\n-//             $(A B C);*  zero or more semi-separated 'A B C' seqs\n-//\n-//\n-// MatchNonterminal\n-// -----------------\n-//\n-//     A matcher that matches one of a few interesting named rust\n-//     nonterminals, such as types, expressions, items, or raw token-trees. A\n-//     black-box matcher on expr, for example, binds an expr to a given ident,\n-//     and that ident can re-occur as an interpolation in the RHS of a\n-//     macro-by-example rule. For example:\n-//\n-//        $foo:expr   =>     1 + $foo    // interpolate an expr\n-//        $foo:tt     =>     $foo        // interpolate a token-tree\n-//        $foo:tt     =>     bar! $foo   // only other valid interpolation\n-//                                       // is in arg position for another\n-//                                       // macro\n-//\n-// As a final, horrifying aside, note that macro-by-example's input is\n-// also matched by one of these matchers. Holy self-referential! It is matched\n-// by a MatchSeq, specifically this one:\n-//\n-//                   $( $lhs:matchers => $rhs:tt );+\n-//\n-// If you understand that, you have closed to loop and understand the whole\n-// macro system. Congratulations.\n-//\n+/// Matchers are nodes defined-by and recognized-by the main rust parser and\n+/// language, but they're only ever found inside syntax-extension invocations;\n+/// indeed, the only thing that ever _activates_ the rules in the rust parser\n+/// for parsing a matcher is a matcher looking for the 'matchers' nonterminal\n+/// itself. Matchers represent a small sub-language for pattern-matching\n+/// token-trees, and are thus primarily used by the macro-defining extension\n+/// itself.\n+///\n+/// MatchTok\n+/// --------\n+///\n+///     A matcher that matches a single token, denoted by the token itself. So\n+///     long as there's no $ involved.\n+///\n+///\n+/// MatchSeq\n+/// --------\n+///\n+///     A matcher that matches a sequence of sub-matchers, denoted various\n+///     possible ways:\n+///\n+///             $(M)*       zero or more Ms\n+///             $(M)+       one or more Ms\n+///             $(M),+      one or more comma-separated Ms\n+///             $(A B C);*  zero or more semi-separated 'A B C' seqs\n+///\n+///\n+/// MatchNonterminal\n+/// -----------------\n+///\n+///     A matcher that matches one of a few interesting named rust\n+///     nonterminals, such as types, expressions, items, or raw token-trees. A\n+///     black-box matcher on expr, for example, binds an expr to a given ident,\n+///     and that ident can re-occur as an interpolation in the RHS of a\n+///     macro-by-example rule. For example:\n+///\n+///        $foo:expr   =>     1 + $foo    // interpolate an expr\n+///        $foo:tt     =>     $foo        // interpolate a token-tree\n+///        $foo:tt     =>     bar! $foo   // only other valid interpolation\n+///                                       // is in arg position for another\n+///                                       // macro\n+///\n+/// As a final, horrifying aside, note that macro-by-example's input is\n+/// also matched by one of these matchers. Holy self-referential! It is matched\n+/// by a MatchSeq, specifically this one:\n+///\n+///                   $( $lhs:matchers => $rhs:tt );+\n+///\n+/// If you understand that, you have closed the loop and understand the whole\n+/// macro system. Congratulations.\n pub type Matcher = Spanned<Matcher_>;\n \n #[deriving(Clone, PartialEq, Eq, Encodable, Decodable, Hash)]\n pub enum Matcher_ {\n-    // match one token\n+    /// Match one token\n     MatchTok(::parse::token::Token),\n-    // match repetitions of a sequence: body, separator, zero ok?,\n-    // lo, hi position-in-match-array used:\n+    /// Match repetitions of a sequence: body, separator, zero ok?,\n+    /// lo, hi position-in-match-array used:\n     MatchSeq(Vec<Matcher> , Option<::parse::token::Token>, bool, uint, uint),\n-    // parse a Rust NT: name to bind, name of NT, position in match array:\n+    /// Parse a Rust NT: name to bind, name of NT, position in match array:\n     MatchNonterminal(Ident, Ident, uint)\n }\n \n pub type Mac = Spanned<Mac_>;\n \n-// represents a macro invocation. The Path indicates which macro\n-// is being invoked, and the vector of token-trees contains the source\n-// of the macro invocation.\n-// There's only one flavor, now, so this could presumably be simplified.\n+/// Represents a macro invocation. The Path indicates which macro\n+/// is being invoked, and the vector of token-trees contains the source\n+/// of the macro invocation.\n+/// There's only one flavor, now, so this could presumably be simplified.\n #[deriving(Clone, PartialEq, Eq, Encodable, Decodable, Hash)]\n pub enum Mac_ {\n     MacInvocTT(Path, Vec<TokenTree> , SyntaxContext),   // new macro-invocation\n@@ -659,11 +659,10 @@ pub struct TypeMethod {\n     pub vis: Visibility,\n }\n \n-/// Represents a method declaration in a trait declaration, possibly\n-/// including a default implementation\n-// A trait method is either required (meaning it doesn't have an\n-// implementation, just a signature) or provided (meaning it has a default\n-// implementation).\n+/// Represents a method declaration in a trait declaration, possibly including\n+/// a default implementation A trait method is either required (meaning it\n+/// doesn't have an implementation, just a signature) or provided (meaning it\n+/// has a default implementation).\n #[deriving(Clone, PartialEq, Eq, Encodable, Decodable, Hash)]\n pub enum TraitMethod {\n     Required(TypeMethod),\n@@ -720,7 +719,7 @@ pub struct Ty {\n     pub span: Span,\n }\n \n-// Not represented directly in the AST, referred to by name through a ty_path.\n+/// Not represented directly in the AST, referred to by name through a ty_path.\n #[deriving(Clone, PartialEq, Eq, Encodable, Decodable, Hash)]\n pub enum PrimTy {\n     TyInt(IntTy),\n@@ -753,10 +752,10 @@ pub struct ClosureTy {\n     pub fn_style: FnStyle,\n     pub onceness: Onceness,\n     pub decl: P<FnDecl>,\n-    // Optional optvec distinguishes between \"fn()\" and \"fn:()\" so we can\n-    // implement issue #7264. None means \"fn()\", which means infer a default\n-    // bound based on pointer sigil during typeck. Some(Empty) means \"fn:()\",\n-    // which means use no bounds (e.g., not even Owned on a ~fn()).\n+    /// Optional optvec distinguishes between \"fn()\" and \"fn:()\" so we can\n+    /// implement issue #7264. None means \"fn()\", which means infer a default\n+    /// bound based on pointer sigil during typeck. Some(Empty) means \"fn:()\",\n+    /// which means use no bounds (e.g., not even Owned on a ~fn()).\n     pub bounds: Option<OwnedSlice<TyParamBound>>,\n }\n \n@@ -789,11 +788,11 @@ pub enum Ty_ {\n     TyUnboxedFn(Gc<UnboxedFnTy>),\n     TyTup(Vec<P<Ty>> ),\n     TyPath(Path, Option<OwnedSlice<TyParamBound>>, NodeId), // for #7264; see above\n-    // No-op; kept solely so that we can pretty-print faithfully\n+    /// No-op; kept solely so that we can pretty-print faithfully\n     TyParen(P<Ty>),\n     TyTypeof(Gc<Expr>),\n-    // TyInfer means the type should be inferred instead of it having been\n-    // specified. This can appear anywhere in a type.\n+    /// TyInfer means the type should be inferred instead of it having been\n+    /// specified. This can appear anywhere in a type.\n     TyInfer,\n }\n \n@@ -854,8 +853,10 @@ pub struct FnDecl {\n \n #[deriving(Clone, PartialEq, Eq, Encodable, Decodable, Hash)]\n pub enum FnStyle {\n-    UnsafeFn, // declared with \"unsafe fn\"\n-    NormalFn, // declared with \"fn\"\n+    /// Declared with \"unsafe fn\"\n+    UnsafeFn,\n+    /// Declared with \"fn\"\n+    NormalFn,\n }\n \n impl fmt::Show for FnStyle {\n@@ -869,18 +870,24 @@ impl fmt::Show for FnStyle {\n \n #[deriving(Clone, PartialEq, Eq, Encodable, Decodable, Hash)]\n pub enum RetStyle {\n-    NoReturn, // functions with return type _|_ that always\n-              // raise an error or exit (i.e. never return to the caller)\n-    Return, // everything else\n+    /// Functions with return type ! that always\n+    /// raise an error or exit (i.e. never return to the caller)\n+    NoReturn,\n+    /// Everything else\n+    Return,\n }\n \n /// Represents the kind of 'self' associated with a method\n #[deriving(Clone, PartialEq, Eq, Encodable, Decodable, Hash)]\n pub enum ExplicitSelf_ {\n-    SelfStatic,                                       // no self\n-    SelfValue(Ident),                                 // `self`\n-    SelfRegion(Option<Lifetime>, Mutability, Ident),  // `&'lt self`, `&'lt mut self`\n-    SelfUniq(Ident),                                  // `~self`\n+    /// No self\n+    SelfStatic,\n+    /// `self\n+    SelfValue(Ident),\n+    /// `&'lt self`, `&'lt mut self`\n+    SelfRegion(Option<Lifetime>, Mutability, Ident),\n+    /// `~self`\n+    SelfUniq(Ident)\n }\n \n pub type ExplicitSelf = Spanned<ExplicitSelf_>;\n@@ -959,17 +966,17 @@ pub type ViewPath = Spanned<ViewPath_>;\n #[deriving(PartialEq, Eq, Encodable, Decodable, Hash)]\n pub enum ViewPath_ {\n \n-    // quux = foo::bar::baz\n-    //\n-    // or just\n-    //\n-    // foo::bar::baz  (with 'baz =' implicitly on the left)\n+    /// `quux = foo::bar::baz`\n+    ///\n+    /// or just\n+    ///\n+    /// `foo::bar::baz ` (with 'baz =' implicitly on the left)\n     ViewPathSimple(Ident, Path, NodeId),\n \n-    // foo::bar::*\n+    /// `foo::bar::*`\n     ViewPathGlob(Path, NodeId),\n \n-    // foo::bar::{a,b,c}\n+    /// `foo::bar::{a,b,c}`\n     ViewPathList(Path, Vec<PathListIdent> , NodeId)\n }\n \n@@ -983,20 +990,20 @@ pub struct ViewItem {\n \n #[deriving(Clone, PartialEq, Eq, Encodable, Decodable, Hash)]\n pub enum ViewItem_ {\n-    // ident: name used to refer to this crate in the code\n-    // optional (InternedString,StrStyle): if present, this is a location\n-    // (containing arbitrary characters) from which to fetch the crate sources\n-    // For example, extern crate whatever = \"github.com/rust-lang/rust\"\n+    /// Ident: name used to refer to this crate in the code\n+    /// optional (InternedString,StrStyle): if present, this is a location\n+    /// (containing arbitrary characters) from which to fetch the crate sources\n+    /// For example, extern crate whatever = \"github.com/rust-lang/rust\"\n     ViewItemExternCrate(Ident, Option<(InternedString,StrStyle)>, NodeId),\n     ViewItemUse(Gc<ViewPath>),\n }\n \n-// Meta-data associated with an item\n+/// Meta-data associated with an item\n pub type Attribute = Spanned<Attribute_>;\n \n-// Distinguishes between Attributes that decorate items and Attributes that\n-// are contained as statements within items. These two cases need to be\n-// distinguished for pretty-printing.\n+/// Distinguishes between Attributes that decorate items and Attributes that\n+/// are contained as statements within items. These two cases need to be\n+/// distinguished for pretty-printing.\n #[deriving(Clone, PartialEq, Eq, Encodable, Decodable, Hash)]\n pub enum AttrStyle {\n     AttrOuter,\n@@ -1006,7 +1013,7 @@ pub enum AttrStyle {\n #[deriving(Clone, PartialEq, Eq, Encodable, Decodable, Hash)]\n pub struct AttrId(pub uint);\n \n-// doc-comments are promoted to attributes that have is_sugared_doc = true\n+/// Doc-comments are promoted to attributes that have is_sugared_doc = true\n #[deriving(Clone, PartialEq, Eq, Encodable, Decodable, Hash)]\n pub struct Attribute_ {\n     pub id: AttrId,\n@@ -1015,13 +1022,12 @@ pub struct Attribute_ {\n     pub is_sugared_doc: bool,\n }\n \n-/*\n-  TraitRef's appear in impls.\n-  resolve maps each TraitRef's ref_id to its defining trait; that's all\n-  that the ref_id is for. The impl_id maps to the \"self type\" of this impl.\n-  If this impl is an ItemImpl, the impl_id is redundant (it could be the\n-  same as the impl's node id).\n- */\n+\n+/// TraitRef's appear in impls.\n+/// resolve maps each TraitRef's ref_id to its defining trait; that's all\n+/// that the ref_id is for. The impl_id maps to the \"self type\" of this impl.\n+/// If this impl is an ItemImpl, the impl_id is redundant (it could be the\n+/// same as the impl's node id).\n #[deriving(Clone, PartialEq, Eq, Encodable, Decodable, Hash)]\n pub struct TraitRef {\n     pub path: Path,\n@@ -1065,7 +1071,8 @@ pub type StructField = Spanned<StructField_>;\n #[deriving(Clone, PartialEq, Eq, Encodable, Decodable, Hash)]\n pub enum StructFieldKind {\n     NamedField(Ident, Visibility),\n-    UnnamedField(Visibility), // element of a tuple-like struct\n+    /// Element of a tuple-like struct\n+    UnnamedField(Visibility),\n }\n \n impl StructFieldKind {\n@@ -1079,12 +1086,15 @@ impl StructFieldKind {\n \n #[deriving(PartialEq, Eq, Encodable, Decodable, Hash)]\n pub struct StructDef {\n-    pub fields: Vec<StructField>, /* fields, not including ctor */\n-    /* ID of the constructor. This is only used for tuple- or enum-like\n-     * structs. */\n+    /// Fields, not including ctor\n+    pub fields: Vec<StructField>,\n+    /// ID of the constructor. This is only used for tuple- or enum-like\n+    /// structs.\n     pub ctor_id: Option<NodeId>,\n-    pub super_struct: Option<P<Ty>>, // Super struct, if specified.\n-    pub is_virtual: bool,            // True iff the struct may be inherited from.\n+    /// Super struct, if specified.\n+    pub super_struct: Option<P<Ty>>,\n+    /// True iff the struct may be inherited from.\n+    pub is_virtual: bool,\n }\n \n /*\n@@ -1120,7 +1130,7 @@ pub enum Item_ {\n              Option<TraitRef>, // (optional) trait this impl implements\n              P<Ty>, // self\n              Vec<Gc<Method>>),\n-    // a macro invocation (which includes macro definition)\n+    /// A macro invocation (which includes macro definition)\n     ItemMac(Mac),\n }\n \n@@ -1140,9 +1150,9 @@ pub enum ForeignItem_ {\n     ForeignItemStatic(P<Ty>, /* is_mutbl */ bool),\n }\n \n-// The data we save and restore about an inlined item or method.  This is not\n-// part of the AST that we parse from a file, but it becomes part of the tree\n-// that we trans.\n+/// The data we save and restore about an inlined item or method.  This is not\n+/// part of the AST that we parse from a file, but it becomes part of the tree\n+/// that we trans.\n #[deriving(PartialEq, Eq, Encodable, Decodable, Hash)]\n pub enum InlinedItem {\n     IIItem(Gc<Item>),"}, {"sha": "25c8e81bdbc91397bb5e5bb8436eb69be026289a", "filename": "src/libsyntax/ast_map.rs", "status": "modified", "additions": 11, "deletions": 11, "changes": 22, "blob_url": "https://github.com/rust-lang/rust/blob/4989a56448c7e3047e0538ff4ef54c49db8a5a4f/src%2Flibsyntax%2Fast_map.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4989a56448c7e3047e0538ff4ef54c49db8a5a4f/src%2Flibsyntax%2Fast_map.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fast_map.rs?ref=4989a56448c7e3047e0538ff4ef54c49db8a5a4f", "patch": "@@ -112,13 +112,13 @@ pub enum Node {\n     NodeLifetime(Gc<Lifetime>),\n }\n \n-// The odd layout is to bring down the total size.\n+/// The odd layout is to bring down the total size.\n #[deriving(Clone)]\n enum MapEntry {\n-    // Placeholder for holes in the map.\n+    /// Placeholder for holes in the map.\n     NotPresent,\n \n-    // All the node types, with a parent ID.\n+    /// All the node types, with a parent ID.\n     EntryItem(NodeId, Gc<Item>),\n     EntryForeignItem(NodeId, Gc<ForeignItem>),\n     EntryTraitMethod(NodeId, Gc<TraitMethod>),\n@@ -133,14 +133,14 @@ enum MapEntry {\n     EntryStructCtor(NodeId, Gc<StructDef>),\n     EntryLifetime(NodeId, Gc<Lifetime>),\n \n-    // Roots for node trees.\n+    /// Roots for node trees.\n     RootCrate,\n     RootInlinedParent(P<InlinedParent>)\n }\n \n struct InlinedParent {\n     path: Vec<PathElem> ,\n-    // Required by NodeTraitMethod and NodeMethod.\n+    /// Required by NodeTraitMethod and NodeMethod.\n     def_id: DefId\n }\n \n@@ -243,7 +243,7 @@ impl Map {\n                 ItemForeignMod(ref nm) => Some(nm.abi),\n                 _ => None\n             },\n-            // Wrong but OK, because the only inlined foreign items are intrinsics.\n+            /// Wrong but OK, because the only inlined foreign items are intrinsics.\n             Some(RootInlinedParent(_)) => Some(abi::RustIntrinsic),\n             _ => None\n         };\n@@ -432,8 +432,8 @@ pub trait FoldOps {\n \n pub struct Ctx<'a, F> {\n     map: &'a Map,\n-    // The node in which we are currently mapping (an item or a method).\n-    // When equal to DUMMY_NODE_ID, the next mapped node becomes the parent.\n+    /// The node in which we are currently mapping (an item or a method).\n+    /// When equal to DUMMY_NODE_ID, the next mapped node becomes the parent.\n     parent: NodeId,\n     fold_ops: F\n }\n@@ -618,9 +618,9 @@ pub fn map_crate<F: FoldOps>(krate: Crate, fold_ops: F) -> (Crate, Map) {\n     (krate, map)\n }\n \n-// Used for items loaded from external crate that are being inlined into this\n-// crate.  The `path` should be the path to the item but should not include\n-// the item itself.\n+/// Used for items loaded from external crate that are being inlined into this\n+/// crate.  The `path` should be the path to the item but should not include\n+/// the item itself.\n pub fn map_decoded_item<F: FoldOps>(map: &Map,\n                                     path: Vec<PathElem> ,\n                                     fold_ops: F,"}, {"sha": "004991814fff4442e7ff526df5f9c60e1bf433cd", "filename": "src/libsyntax/ast_util.rs", "status": "modified", "additions": 7, "deletions": 7, "changes": 14, "blob_url": "https://github.com/rust-lang/rust/blob/4989a56448c7e3047e0538ff4ef54c49db8a5a4f/src%2Flibsyntax%2Fast_util.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4989a56448c7e3047e0538ff4ef54c49db8a5a4f/src%2Flibsyntax%2Fast_util.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fast_util.rs?ref=4989a56448c7e3047e0538ff4ef54c49db8a5a4f", "patch": "@@ -101,8 +101,8 @@ pub fn is_path(e: Gc<Expr>) -> bool {\n     return match e.node { ExprPath(_) => true, _ => false };\n }\n \n-// Get a string representation of a signed int type, with its value.\n-// We want to avoid \"45int\" and \"-3int\" in favor of \"45\" and \"-3\"\n+/// Get a string representation of a signed int type, with its value.\n+/// We want to avoid \"45int\" and \"-3int\" in favor of \"45\" and \"-3\"\n pub fn int_ty_to_string(t: IntTy, val: Option<i64>) -> String {\n     let s = match t {\n         TyI if val.is_some() => \"i\",\n@@ -131,8 +131,8 @@ pub fn int_ty_max(t: IntTy) -> u64 {\n     }\n }\n \n-// Get a string representation of an unsigned int type, with its value.\n-// We want to avoid \"42uint\" in favor of \"42u\"\n+/// Get a string representation of an unsigned int type, with its value.\n+/// We want to avoid \"42uint\" in favor of \"42u\"\n pub fn uint_ty_to_string(t: UintTy, val: Option<u64>) -> String {\n     let s = match t {\n         TyU if val.is_some() => \"u\",\n@@ -249,8 +249,8 @@ pub fn public_methods(ms: Vec<Gc<Method>> ) -> Vec<Gc<Method>> {\n     }).collect()\n }\n \n-// extract a TypeMethod from a TraitMethod. if the TraitMethod is\n-// a default, pull out the useful fields to make a TypeMethod\n+/// extract a TypeMethod from a TraitMethod. if the TraitMethod is\n+/// a default, pull out the useful fields to make a TypeMethod\n pub fn trait_method_to_ty_method(method: &TraitMethod) -> TypeMethod {\n     match *method {\n         Required(ref m) => (*m).clone(),\n@@ -705,7 +705,7 @@ pub fn segments_name_eq(a : &[ast::PathSegment], b : &[ast::PathSegment]) -> boo\n     }\n }\n \n-// Returns true if this literal is a string and false otherwise.\n+/// Returns true if this literal is a string and false otherwise.\n pub fn lit_is_str(lit: Gc<Lit>) -> bool {\n     match lit.node {\n         LitStr(..) => true,"}, {"sha": "e8b9ec9628f7deba896a0b87283faf22dd264842", "filename": "src/libsyntax/attr.rs", "status": "modified", "additions": 12, "deletions": 16, "changes": 28, "blob_url": "https://github.com/rust-lang/rust/blob/4989a56448c7e3047e0538ff4ef54c49db8a5a4f/src%2Flibsyntax%2Fattr.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4989a56448c7e3047e0538ff4ef54c49db8a5a4f/src%2Flibsyntax%2Fattr.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fattr.rs?ref=4989a56448c7e3047e0538ff4ef54c49db8a5a4f", "patch": "@@ -46,10 +46,8 @@ pub trait AttrMetaMethods {\n     /// #[foo=\"bar\"] and #[foo(bar)]\n     fn name(&self) -> InternedString;\n \n-    /**\n-     * Gets the string value if self is a MetaNameValue variant\n-     * containing a string, otherwise None.\n-     */\n+    /// Gets the string value if self is a MetaNameValue variant\n+    /// containing a string, otherwise None.\n     fn value_str(&self) -> Option<InternedString>;\n     /// Gets a list of inner meta items from a list MetaItem type.\n     fn meta_item_list<'a>(&'a self) -> Option<&'a [Gc<MetaItem>]>;\n@@ -420,18 +418,16 @@ pub fn require_unique_names(diagnostic: &SpanHandler, metas: &[Gc<MetaItem>]) {\n }\n \n \n-/**\n- * Fold this over attributes to parse #[repr(...)] forms.\n- *\n- * Valid repr contents: any of the primitive integral type names (see\n- * `int_type_of_word`, below) to specify the discriminant type; and `C`, to use\n- * the same discriminant size that the corresponding C enum would.  These are\n- * not allowed on univariant or zero-variant enums, which have no discriminant.\n- *\n- * If a discriminant type is so specified, then the discriminant will be\n- * present (before fields, if any) with that type; reprensentation\n- * optimizations which would remove it will not be done.\n- */\n+/// Fold this over attributes to parse #[repr(...)] forms.\n+///\n+/// Valid repr contents: any of the primitive integral type names (see\n+/// `int_type_of_word`, below) to specify the discriminant type; and `C`, to use\n+/// the same discriminant size that the corresponding C enum would.  These are\n+/// not allowed on univariant or zero-variant enums, which have no discriminant.\n+///\n+/// If a discriminant type is so specified, then the discriminant will be\n+/// present (before fields, if any) with that type; reprensentation\n+/// optimizations which would remove it will not be done.\n pub fn find_repr_attr(diagnostic: &SpanHandler, attr: &Attribute, acc: ReprAttr)\n     -> ReprAttr {\n     let mut acc = acc;"}, {"sha": "2f1e01b239d4c1a6cf4c89ace92d0321980f6bcc", "filename": "src/libsyntax/codemap.rs", "status": "modified", "additions": 11, "deletions": 11, "changes": 22, "blob_url": "https://github.com/rust-lang/rust/blob/4989a56448c7e3047e0538ff4ef54c49db8a5a4f/src%2Flibsyntax%2Fcodemap.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4989a56448c7e3047e0538ff4ef54c49db8a5a4f/src%2Flibsyntax%2Fcodemap.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fcodemap.rs?ref=4989a56448c7e3047e0538ff4ef54c49db8a5a4f", "patch": "@@ -252,15 +252,15 @@ pub struct FileMap {\n }\n \n impl FileMap {\n-    // EFFECT: register a start-of-line offset in the\n-    // table of line-beginnings.\n-    // UNCHECKED INVARIANT: these offsets must be added in the right\n-    // order and must be in the right places; there is shared knowledge\n-    // about what ends a line between this file and parse.rs\n-    // WARNING: pos param here is the offset relative to start of CodeMap,\n-    // and CodeMap will append a newline when adding a filemap without a newline at the end,\n-    // so the safe way to call this is with value calculated as\n-    // filemap.start_pos + newline_offset_relative_to_the_start_of_filemap.\n+    /// EFFECT: register a start-of-line offset in the\n+    /// table of line-beginnings.\n+    /// UNCHECKED INVARIANT: these offsets must be added in the right\n+    /// order and must be in the right places; there is shared knowledge\n+    /// about what ends a line between this file and parse.rs\n+    /// WARNING: pos param here is the offset relative to start of CodeMap,\n+    /// and CodeMap will append a newline when adding a filemap without a newline at the end,\n+    /// so the safe way to call this is with value calculated as\n+    /// filemap.start_pos + newline_offset_relative_to_the_start_of_filemap.\n     pub fn next_line(&self, pos: BytePos) {\n         // the new charpos must be > the last one (or it's the first one).\n         let mut lines = self.lines.borrow_mut();;\n@@ -269,7 +269,7 @@ impl FileMap {\n         lines.push(pos);\n     }\n \n-    // get a line from the list of pre-computed line-beginnings\n+    /// get a line from the list of pre-computed line-beginnings\n     pub fn get_line(&self, line: int) -> String {\n         let mut lines = self.lines.borrow_mut();\n         let begin: BytePos = *lines.get(line as uint) - self.start_pos;\n@@ -428,7 +428,7 @@ impl CodeMap {\n         FileMapAndBytePos {fm: fm, pos: offset}\n     }\n \n-    // Converts an absolute BytePos to a CharPos relative to the filemap and above.\n+    /// Converts an absolute BytePos to a CharPos relative to the filemap and above.\n     pub fn bytepos_to_file_charpos(&self, bpos: BytePos) -> CharPos {\n         debug!(\"codemap: converting {:?} to char pos\", bpos);\n         let idx = self.lookup_filemap_idx(bpos);"}, {"sha": "e469f327ae8ba403a3620ad65d0189978095f4a0", "filename": "src/libsyntax/diagnostic.rs", "status": "modified", "additions": 13, "deletions": 13, "changes": 26, "blob_url": "https://github.com/rust-lang/rust/blob/4989a56448c7e3047e0538ff4ef54c49db8a5a4f/src%2Flibsyntax%2Fdiagnostic.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4989a56448c7e3047e0538ff4ef54c49db8a5a4f/src%2Flibsyntax%2Fdiagnostic.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fdiagnostic.rs?ref=4989a56448c7e3047e0538ff4ef54c49db8a5a4f", "patch": "@@ -21,7 +21,7 @@ use std::string::String;\n use term::WriterWrapper;\n use term;\n \n-// maximum number of lines we will print for each error; arbitrary.\n+/// maximum number of lines we will print for each error; arbitrary.\n static MAX_LINES: uint = 6u;\n \n #[deriving(Clone)]\n@@ -73,9 +73,9 @@ pub struct FatalError;\n /// or `.span_bug` rather than a failed assertion, etc.\n pub struct ExplicitBug;\n \n-// a span-handler is like a handler but also\n-// accepts span information for source-location\n-// reporting.\n+/// A span-handler is like a handler but also\n+/// accepts span information for source-location\n+/// reporting.\n pub struct SpanHandler {\n     pub handler: Handler,\n     pub cm: codemap::CodeMap,\n@@ -114,9 +114,9 @@ impl SpanHandler {\n     }\n }\n \n-// a handler deals with errors; certain errors\n-// (fatal, bug, unimpl) may cause immediate exit,\n-// others log errors for later reporting.\n+/// A handler deals with errors; certain errors\n+/// (fatal, bug, unimpl) may cause immediate exit,\n+/// others log errors for later reporting.\n pub struct Handler {\n     err_count: Cell<uint>,\n     emit: RefCell<Box<Emitter + Send>>,\n@@ -442,12 +442,12 @@ fn highlight_lines(err: &mut EmitterWriter,\n     Ok(())\n }\n \n-// Here are the differences between this and the normal `highlight_lines`:\n-// `custom_highlight_lines` will always put arrow on the last byte of the\n-// span (instead of the first byte). Also, when the span is too long (more\n-// than 6 lines), `custom_highlight_lines` will print the first line, then\n-// dot dot dot, then last line, whereas `highlight_lines` prints the first\n-// six lines.\n+/// Here are the differences between this and the normal `highlight_lines`:\n+/// `custom_highlight_lines` will always put arrow on the last byte of the\n+/// span (instead of the first byte). Also, when the span is too long (more\n+/// than 6 lines), `custom_highlight_lines` will print the first line, then\n+/// dot dot dot, then last line, whereas `highlight_lines` prints the first\n+/// six lines.\n fn custom_highlight_lines(w: &mut EmitterWriter,\n                           cm: &codemap::CodeMap,\n                           sp: Span,"}, {"sha": "bbf38fd7a9d05c8dd2d86da31fdab81e89e53218", "filename": "src/libsyntax/ext/base.rs", "status": "modified", "additions": 11, "deletions": 12, "changes": 23, "blob_url": "https://github.com/rust-lang/rust/blob/4989a56448c7e3047e0538ff4ef54c49db8a5a4f/src%2Flibsyntax%2Fext%2Fbase.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4989a56448c7e3047e0538ff4ef54c49db8a5a4f/src%2Flibsyntax%2Fext%2Fbase.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fext%2Fbase.rs?ref=4989a56448c7e3047e0538ff4ef54c49db8a5a4f", "patch": "@@ -278,9 +278,9 @@ pub enum SyntaxExtension {\n pub type NamedSyntaxExtension = (Name, SyntaxExtension);\n \n pub struct BlockInfo {\n-    // should macros escape from this scope?\n+    /// Should macros escape from this scope?\n     pub macros_escape: bool,\n-    // what are the pending renames?\n+    /// What are the pending renames?\n     pub pending_renames: mtwt::RenameList,\n }\n \n@@ -293,8 +293,8 @@ impl BlockInfo {\n     }\n }\n \n-// The base map of methods for expanding syntax extension\n-// AST nodes into full ASTs\n+/// The base map of methods for expanding syntax extension\n+/// AST nodes into full ASTs\n pub fn syntax_expander_table() -> SyntaxEnv {\n     // utility function to simplify creating NormalTT syntax extensions\n     fn builtin_normal_expander(f: MacroExpanderFn) -> SyntaxExtension {\n@@ -398,9 +398,9 @@ pub fn syntax_expander_table() -> SyntaxEnv {\n     syntax_expanders\n }\n \n-// One of these is made during expansion and incrementally updated as we go;\n-// when a macro expansion occurs, the resulting nodes have the backtrace()\n-// -> expn_info of their expansion context stored into their span.\n+/// One of these is made during expansion and incrementally updated as we go;\n+/// when a macro expansion occurs, the resulting nodes have the backtrace()\n+/// -> expn_info of their expansion context stored into their span.\n pub struct ExtCtxt<'a> {\n     pub parse_sess: &'a parse::ParseSess,\n     pub cfg: ast::CrateConfig,\n@@ -612,11 +612,11 @@ pub fn get_exprs_from_tts(cx: &mut ExtCtxt,\n     Some(es)\n }\n \n-// in order to have some notion of scoping for macros,\n-// we want to implement the notion of a transformation\n-// environment.\n+/// In order to have some notion of scoping for macros,\n+/// we want to implement the notion of a transformation\n+/// environment.\n \n-// This environment maps Names to SyntaxExtensions.\n+/// This environment maps Names to SyntaxExtensions.\n \n //impl question: how to implement it? Initially, the\n // env will contain only macros, so it might be painful\n@@ -633,7 +633,6 @@ struct MapChainFrame {\n     map: HashMap<Name, SyntaxExtension>,\n }\n \n-// Only generic to make it easy to test\n pub struct SyntaxEnv {\n     chain: Vec<MapChainFrame> ,\n }"}, {"sha": "3b34407edfeaa364b0d98a667e6cfdb1aa593f42", "filename": "src/libsyntax/ext/deriving/encodable.rs", "status": "modified", "additions": 70, "deletions": 73, "changes": 143, "blob_url": "https://github.com/rust-lang/rust/blob/4989a56448c7e3047e0538ff4ef54c49db8a5a4f/src%2Flibsyntax%2Fext%2Fderiving%2Fencodable.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4989a56448c7e3047e0538ff4ef54c49db8a5a4f/src%2Flibsyntax%2Fext%2Fderiving%2Fencodable.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fext%2Fderiving%2Fencodable.rs?ref=4989a56448c7e3047e0538ff4ef54c49db8a5a4f", "patch": "@@ -8,79 +8,76 @@\n // option. This file may not be copied, modified, or distributed\n // except according to those terms.\n \n-/*!\n-\n-The compiler code necessary to implement the `#[deriving(Encodable)]`\n-(and `Decodable`, in decodable.rs) extension.  The idea here is that\n-type-defining items may be tagged with `#[deriving(Encodable, Decodable)]`.\n-\n-For example, a type like:\n-\n-```ignore\n-#[deriving(Encodable, Decodable)]\n-struct Node { id: uint }\n-```\n-\n-would generate two implementations like:\n-\n-```ignore\n-impl<S:serialize::Encoder> Encodable<S> for Node {\n-    fn encode(&self, s: &S) {\n-        s.emit_struct(\"Node\", 1, || {\n-            s.emit_field(\"id\", 0, || s.emit_uint(self.id))\n-        })\n-    }\n-}\n-\n-impl<D:Decoder> Decodable for node_id {\n-    fn decode(d: &D) -> Node {\n-        d.read_struct(\"Node\", 1, || {\n-            Node {\n-                id: d.read_field(\"x\".to_string(), 0, || decode(d))\n-            }\n-        })\n-    }\n-}\n-```\n-\n-Other interesting scenarios are whe the item has type parameters or\n-references other non-built-in types.  A type definition like:\n-\n-```ignore\n-#[deriving(Encodable, Decodable)]\n-struct spanned<T> { node: T, span: Span }\n-```\n-\n-would yield functions like:\n-\n-```ignore\n-    impl<\n-        S: Encoder,\n-        T: Encodable<S>\n-    > spanned<T>: Encodable<S> {\n-        fn encode<S:Encoder>(s: &S) {\n-            s.emit_rec(|| {\n-                s.emit_field(\"node\", 0, || self.node.encode(s));\n-                s.emit_field(\"span\", 1, || self.span.encode(s));\n-            })\n-        }\n-    }\n-\n-    impl<\n-        D: Decoder,\n-        T: Decodable<D>\n-    > spanned<T>: Decodable<D> {\n-        fn decode(d: &D) -> spanned<T> {\n-            d.read_rec(|| {\n-                {\n-                    node: d.read_field(\"node\".to_string(), 0, || decode(d)),\n-                    span: d.read_field(\"span\".to_string(), 1, || decode(d)),\n-                }\n-            })\n-        }\n-    }\n-```\n-*/\n+//! The compiler code necessary to implement the `#[deriving(Encodable)]`\n+//! (and `Decodable`, in decodable.rs) extension.  The idea here is that\n+//! type-defining items may be tagged with `#[deriving(Encodable, Decodable)]`.\n+//!\n+//! For example, a type like:\n+//!\n+//! ```ignore\n+//! #[deriving(Encodable, Decodable)]\n+//! struct Node { id: uint }\n+//! ```\n+//!\n+//! would generate two implementations like:\n+//!\n+//! ```ignore\n+//! impl<S:serialize::Encoder> Encodable<S> for Node {\n+//!     fn encode(&self, s: &S) {\n+//!         s.emit_struct(\"Node\", 1, || {\n+//!             s.emit_field(\"id\", 0, || s.emit_uint(self.id))\n+//!         })\n+//!     }\n+//! }\n+//!\n+//! impl<D:Decoder> Decodable for node_id {\n+//!     fn decode(d: &D) -> Node {\n+//!         d.read_struct(\"Node\", 1, || {\n+//!             Node {\n+//!                 id: d.read_field(\"x\".to_string(), 0, || decode(d))\n+//!             }\n+//!         })\n+//!     }\n+//! }\n+//! ```\n+//!\n+//! Other interesting scenarios are whe the item has type parameters or\n+//! references other non-built-in types.  A type definition like:\n+//!\n+//! ```ignore\n+//! #[deriving(Encodable, Decodable)]\n+//! struct spanned<T> { node: T, span: Span }\n+//! ```\n+//!\n+//! would yield functions like:\n+//!\n+//! ```ignore\n+//!     impl<\n+//!         S: Encoder,\n+//!         T: Encodable<S>\n+//!     > spanned<T>: Encodable<S> {\n+//!         fn encode<S:Encoder>(s: &S) {\n+//!             s.emit_rec(|| {\n+//!                 s.emit_field(\"node\", 0, || self.node.encode(s));\n+//!                 s.emit_field(\"span\", 1, || self.span.encode(s));\n+//!             })\n+//!         }\n+//!     }\n+//!\n+//!     impl<\n+//!         D: Decoder,\n+//!         T: Decodable<D>\n+//!     > spanned<T>: Decodable<D> {\n+//!         fn decode(d: &D) -> spanned<T> {\n+//!             d.read_rec(|| {\n+//!                 {\n+//!                     node: d.read_field(\"node\".to_string(), 0, || decode(d)),\n+//!                     span: d.read_field(\"span\".to_string(), 1, || decode(d)),\n+//!                 }\n+//!             })\n+//!         }\n+//!     }\n+//! ```\n \n use ast::{MetaItem, Item, Expr, ExprRet, MutMutable, LitNil};\n use codemap::Span;"}, {"sha": "c9f5936a9bb0532cc94deabae2a2a46380c75e5c", "filename": "src/libsyntax/ext/deriving/generic/mod.rs", "status": "modified", "additions": 164, "deletions": 168, "changes": 332, "blob_url": "https://github.com/rust-lang/rust/blob/4989a56448c7e3047e0538ff4ef54c49db8a5a4f/src%2Flibsyntax%2Fext%2Fderiving%2Fgeneric%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4989a56448c7e3047e0538ff4ef54c49db8a5a4f/src%2Flibsyntax%2Fext%2Fderiving%2Fgeneric%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fext%2Fderiving%2Fgeneric%2Fmod.rs?ref=4989a56448c7e3047e0538ff4ef54c49db8a5a4f", "patch": "@@ -8,174 +8,170 @@\n // option. This file may not be copied, modified, or distributed\n // except according to those terms.\n \n-/*!\n-\n-Some code that abstracts away much of the boilerplate of writing\n-`deriving` instances for traits. Among other things it manages getting\n-access to the fields of the 4 different sorts of structs and enum\n-variants, as well as creating the method and impl ast instances.\n-\n-Supported features (fairly exhaustive):\n-\n-- Methods taking any number of parameters of any type, and returning\n-  any type, other than vectors, bottom and closures.\n-- Generating `impl`s for types with type parameters and lifetimes\n-  (e.g. `Option<T>`), the parameters are automatically given the\n-  current trait as a bound. (This includes separate type parameters\n-  and lifetimes for methods.)\n-- Additional bounds on the type parameters, e.g. the `Ord` instance\n-  requires an explicit `PartialEq` bound at the\n-  moment. (`TraitDef.additional_bounds`)\n-\n-Unsupported: FIXME #6257: calling methods on reference fields,\n-e.g. deriving Eq/Ord/Clone don't work on `struct A(&int)`,\n-because of how the auto-dereferencing happens.\n-\n-The most important thing for implementers is the `Substructure` and\n-`SubstructureFields` objects. The latter groups 5 possibilities of the\n-arguments:\n-\n-- `Struct`, when `Self` is a struct (including tuple structs, e.g\n-  `struct T(int, char)`).\n-- `EnumMatching`, when `Self` is an enum and all the arguments are the\n-  same variant of the enum (e.g. `Some(1)`, `Some(3)` and `Some(4)`)\n-- `EnumNonMatching` when `Self` is an enum and the arguments are not\n-  the same variant (e.g. `None`, `Some(1)` and `None`). If\n-  `const_nonmatching` is true, this will contain an empty list.\n-- `StaticEnum` and `StaticStruct` for static methods, where the type\n-  being derived upon is either an enum or struct respectively. (Any\n-  argument with type Self is just grouped among the non-self\n-  arguments.)\n-\n-In the first two cases, the values from the corresponding fields in\n-all the arguments are grouped together. In the `EnumNonMatching` case\n-this isn't possible (different variants have different fields), so the\n-fields are grouped by which argument they come from. There are no\n-fields with values in the static cases, so these are treated entirely\n-differently.\n-\n-The non-static cases have `Option<ident>` in several places associated\n-with field `expr`s. This represents the name of the field it is\n-associated with. It is only not `None` when the associated field has\n-an identifier in the source code. For example, the `x`s in the\n-following snippet\n-\n-```rust\n-struct A { x : int }\n-\n-struct B(int);\n-\n-enum C {\n-    C0(int),\n-    C1 { x: int }\n-}\n-```\n-\n-The `int`s in `B` and `C0` don't have an identifier, so the\n-`Option<ident>`s would be `None` for them.\n-\n-In the static cases, the structure is summarised, either into the just\n-spans of the fields or a list of spans and the field idents (for tuple\n-structs and record structs, respectively), or a list of these, for\n-enums (one for each variant). For empty struct and empty enum\n-variants, it is represented as a count of 0.\n-\n-# Examples\n-\n-The following simplified `PartialEq` is used for in-code examples:\n-\n-```rust\n-trait PartialEq {\n-    fn eq(&self, other: &Self);\n-}\n-impl PartialEq for int {\n-    fn eq(&self, other: &int) -> bool {\n-        *self == *other\n-    }\n-}\n-```\n-\n-Some examples of the values of `SubstructureFields` follow, using the\n-above `PartialEq`, `A`, `B` and `C`.\n-\n-## Structs\n-\n-When generating the `expr` for the `A` impl, the `SubstructureFields` is\n-\n-~~~text\n-Struct(~[FieldInfo {\n-           span: <span of x>\n-           name: Some(<ident of x>),\n-           self_: <expr for &self.x>,\n-           other: ~[<expr for &other.x]\n-         }])\n-~~~\n-\n-For the `B` impl, called with `B(a)` and `B(b)`,\n-\n-~~~text\n-Struct(~[FieldInfo {\n-          span: <span of `int`>,\n-          name: None,\n-          <expr for &a>\n-          ~[<expr for &b>]\n-         }])\n-~~~\n-\n-## Enums\n-\n-When generating the `expr` for a call with `self == C0(a)` and `other\n-== C0(b)`, the SubstructureFields is\n-\n-~~~text\n-EnumMatching(0, <ast::Variant for C0>,\n-             ~[FieldInfo {\n-                span: <span of int>\n-                name: None,\n-                self_: <expr for &a>,\n-                other: ~[<expr for &b>]\n-              }])\n-~~~\n-\n-For `C1 {x}` and `C1 {x}`,\n-\n-~~~text\n-EnumMatching(1, <ast::Variant for C1>,\n-             ~[FieldInfo {\n-                span: <span of x>\n-                name: Some(<ident of x>),\n-                self_: <expr for &self.x>,\n-                other: ~[<expr for &other.x>]\n-               }])\n-~~~\n-\n-For `C0(a)` and `C1 {x}` ,\n-\n-~~~text\n-EnumNonMatching(~[(0, <ast::Variant for B0>,\n-                   ~[(<span of int>, None, <expr for &a>)]),\n-                  (1, <ast::Variant for B1>,\n-                   ~[(<span of x>, Some(<ident of x>),\n-                      <expr for &other.x>)])])\n-~~~\n-\n-(and vice versa, but with the order of the outermost list flipped.)\n-\n-## Static\n-\n-A static method on the above would result in,\n-\n-~~~text\n-StaticStruct(<ast::StructDef of A>, Named(~[(<ident of x>, <span of x>)]))\n-\n-StaticStruct(<ast::StructDef of B>, Unnamed(~[<span of x>]))\n-\n-StaticEnum(<ast::EnumDef of C>, ~[(<ident of C0>, <span of C0>, Unnamed(~[<span of int>])),\n-                                  (<ident of C1>, <span of C1>,\n-                                   Named(~[(<ident of x>, <span of x>)]))])\n-~~~\n-\n-*/\n+//! Some code that abstracts away much of the boilerplate of writing\n+//! `deriving` instances for traits. Among other things it manages getting\n+//! access to the fields of the 4 different sorts of structs and enum\n+//! variants, as well as creating the method and impl ast instances.\n+//!\n+//! Supported features (fairly exhaustive):\n+//!\n+//! - Methods taking any number of parameters of any type, and returning\n+//!   any type, other than vectors, bottom and closures.\n+//! - Generating `impl`s for types with type parameters and lifetimes\n+//!   (e.g. `Option<T>`), the parameters are automatically given the\n+//!   current trait as a bound. (This includes separate type parameters\n+//!   and lifetimes for methods.)\n+//! - Additional bounds on the type parameters, e.g. the `Ord` instance\n+//!   requires an explicit `PartialEq` bound at the\n+//!   moment. (`TraitDef.additional_bounds`)\n+//!\n+//! Unsupported: FIXME #6257: calling methods on reference fields,\n+//! e.g. deriving Eq/Ord/Clone don't work on `struct A(&int)`,\n+//! because of how the auto-dereferencing happens.\n+//!\n+//! The most important thing for implementers is the `Substructure` and\n+//! `SubstructureFields` objects. The latter groups 5 possibilities of the\n+//! arguments:\n+//!\n+//! - `Struct`, when `Self` is a struct (including tuple structs, e.g\n+//!   `struct T(int, char)`).\n+//! - `EnumMatching`, when `Self` is an enum and all the arguments are the\n+//!   same variant of the enum (e.g. `Some(1)`, `Some(3)` and `Some(4)`)\n+//! - `EnumNonMatching` when `Self` is an enum and the arguments are not\n+//!   the same variant (e.g. `None`, `Some(1)` and `None`). If\n+//!   `const_nonmatching` is true, this will contain an empty list.\n+//! - `StaticEnum` and `StaticStruct` for static methods, where the type\n+//!   being derived upon is either an enum or struct respectively. (Any\n+//!   argument with type Self is just grouped among the non-self\n+//!   arguments.)\n+//!\n+//! In the first two cases, the values from the corresponding fields in\n+//! all the arguments are grouped together. In the `EnumNonMatching` case\n+//! this isn't possible (different variants have different fields), so the\n+//! fields are grouped by which argument they come from. There are no\n+//! fields with values in the static cases, so these are treated entirely\n+//! differently.\n+//!\n+//! The non-static cases have `Option<ident>` in several places associated\n+//! with field `expr`s. This represents the name of the field it is\n+//! associated with. It is only not `None` when the associated field has\n+//! an identifier in the source code. For example, the `x`s in the\n+//! following snippet\n+//!\n+//! ```rust\n+//! struct A { x : int }\n+//!\n+//! struct B(int);\n+//!\n+//! enum C {\n+//!     C0(int),\n+//!     C1 { x: int }\n+//! }\n+//! ```\n+//!\n+//! The `int`s in `B` and `C0` don't have an identifier, so the\n+//! `Option<ident>`s would be `None` for them.\n+//!\n+//! In the static cases, the structure is summarised, either into the just\n+//! spans of the fields or a list of spans and the field idents (for tuple\n+//! structs and record structs, respectively), or a list of these, for\n+//! enums (one for each variant). For empty struct and empty enum\n+//! variants, it is represented as a count of 0.\n+//!\n+//! # Examples\n+//!\n+//! The following simplified `PartialEq` is used for in-code examples:\n+//!\n+//! ```rust\n+//! trait PartialEq {\n+//!     fn eq(&self, other: &Self);\n+//! }\n+//! impl PartialEq for int {\n+//!     fn eq(&self, other: &int) -> bool {\n+//!         *self == *other\n+//!     }\n+//! }\n+//! ```\n+//!\n+//! Some examples of the values of `SubstructureFields` follow, using the\n+//! above `PartialEq`, `A`, `B` and `C`.\n+//!\n+//! ## Structs\n+//!\n+//! When generating the `expr` for the `A` impl, the `SubstructureFields` is\n+//!\n+//! ~~~text\n+//! Struct(~[FieldInfo {\n+//!            span: <span of x>\n+//!            name: Some(<ident of x>),\n+//!            self_: <expr for &self.x>,\n+//!            other: ~[<expr for &other.x]\n+//!          }])\n+//! ~~~\n+//!\n+//! For the `B` impl, called with `B(a)` and `B(b)`,\n+//!\n+//! ~~~text\n+//! Struct(~[FieldInfo {\n+//!           span: <span of `int`>,\n+//!           name: None,\n+//!           <expr for &a>\n+//!           ~[<expr for &b>]\n+//!          }])\n+//! ~~~\n+//!\n+//! ## Enums\n+//!\n+//! When generating the `expr` for a call with `self == C0(a)` and `other\n+//! == C0(b)`, the SubstructureFields is\n+//!\n+//! ~~~text\n+//! EnumMatching(0, <ast::Variant for C0>,\n+//!              ~[FieldInfo {\n+//!                 span: <span of int>\n+//!                 name: None,\n+//!                 self_: <expr for &a>,\n+//!                 other: ~[<expr for &b>]\n+//!               }])\n+//! ~~~\n+//!\n+//! For `C1 {x}` and `C1 {x}`,\n+//!\n+//! ~~~text\n+//! EnumMatching(1, <ast::Variant for C1>,\n+//!              ~[FieldInfo {\n+//!                 span: <span of x>\n+//!                 name: Some(<ident of x>),\n+//!                 self_: <expr for &self.x>,\n+//!                 other: ~[<expr for &other.x>]\n+//!                }])\n+//! ~~~\n+//!\n+//! For `C0(a)` and `C1 {x}` ,\n+//!\n+//! ~~~text\n+//! EnumNonMatching(~[(0, <ast::Variant for B0>,\n+//!                    ~[(<span of int>, None, <expr for &a>)]),\n+//!                   (1, <ast::Variant for B1>,\n+//!                    ~[(<span of x>, Some(<ident of x>),\n+//!                       <expr for &other.x>)])])\n+//! ~~~\n+//!\n+//! (and vice versa, but with the order of the outermost list flipped.)\n+//!\n+//! ## Static\n+//!\n+//! A static method on the above would result in,\n+//!\n+//! ~~~text\n+//! StaticStruct(<ast::StructDef of A>, Named(~[(<ident of x>, <span of x>)]))\n+//!\n+//! StaticStruct(<ast::StructDef of B>, Unnamed(~[<span of x>]))\n+//!\n+//! StaticEnum(<ast::EnumDef of C>, ~[(<ident of C0>, <span of C0>, Unnamed(~[<span of int>])),\n+//!                                   (<ident of C1>, <span of C1>,\n+//!                                    Named(~[(<ident of x>, <span of x>)]))])\n+//! ~~~\n \n use std::cell::RefCell;\n use std::gc::{Gc, GC};"}, {"sha": "f6a39d7b2e6c1b6dda6c57356f7de62592c1a98e", "filename": "src/libsyntax/ext/deriving/generic/ty.rs", "status": "modified", "additions": 8, "deletions": 6, "changes": 14, "blob_url": "https://github.com/rust-lang/rust/blob/4989a56448c7e3047e0538ff4ef54c49db8a5a4f/src%2Flibsyntax%2Fext%2Fderiving%2Fgeneric%2Fty.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4989a56448c7e3047e0538ff4ef54c49db8a5a4f/src%2Flibsyntax%2Fext%2Fderiving%2Fgeneric%2Fty.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fext%2Fderiving%2Fgeneric%2Fty.rs?ref=4989a56448c7e3047e0538ff4ef54c49db8a5a4f", "patch": "@@ -25,8 +25,10 @@ use std::gc::Gc;\n \n /// The types of pointers\n pub enum PtrTy<'a> {\n-    Send, // ~\n-    Borrowed(Option<&'a str>, ast::Mutability), // &['lifetime] [mut]\n+    /// ~\n+    Send,\n+    /// &'lifetime mut\n+    Borrowed(Option<&'a str>, ast::Mutability),\n }\n \n /// A path, e.g. `::std::option::Option::<int>` (global). Has support\n@@ -83,12 +85,12 @@ impl<'a> Path<'a> {\n /// A type. Supports pointers (except for *), Self, and literals\n pub enum Ty<'a> {\n     Self,\n-    // &/Box/ Ty\n+    /// &/Box/ Ty\n     Ptr(Box<Ty<'a>>, PtrTy<'a>),\n-    // mod::mod::Type<[lifetime], [Params...]>, including a plain type\n-    // parameter, and things like `int`\n+    /// mod::mod::Type<[lifetime], [Params...]>, including a plain type\n+    /// parameter, and things like `int`\n     Literal(Path<'a>),\n-    // includes nil\n+    /// includes unit\n     Tuple(Vec<Ty<'a>> )\n }\n "}, {"sha": "05b5131d7e4d332d6843e089e39ae192233bb425", "filename": "src/libsyntax/ext/deriving/show.rs", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/rust-lang/rust/blob/4989a56448c7e3047e0538ff4ef54c49db8a5a4f/src%2Flibsyntax%2Fext%2Fderiving%2Fshow.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4989a56448c7e3047e0538ff4ef54c49db8a5a4f/src%2Flibsyntax%2Fext%2Fderiving%2Fshow.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fext%2Fderiving%2Fshow.rs?ref=4989a56448c7e3047e0538ff4ef54c49db8a5a4f", "patch": "@@ -55,8 +55,8 @@ pub fn expand_deriving_show(cx: &mut ExtCtxt,\n     trait_def.expand(cx, mitem, item, push)\n }\n \n-// we construct a format string and then defer to std::fmt, since that\n-// knows what's up with formatting at so on.\n+/// We construct a format string and then defer to std::fmt, since that\n+/// knows what's up with formatting and so on.\n fn show_substructure(cx: &mut ExtCtxt, span: Span,\n                      substr: &Substructure) -> Gc<Expr> {\n     // build `<name>`, `<name>({}, {}, ...)` or `<name> { <field>: {},"}, {"sha": "a095317f663a963d836a8bb89e184463170bbf7f", "filename": "src/libsyntax/ext/expand.rs", "status": "modified", "additions": 5, "deletions": 5, "changes": 10, "blob_url": "https://github.com/rust-lang/rust/blob/4989a56448c7e3047e0538ff4ef54c49db8a5a4f/src%2Flibsyntax%2Fext%2Fexpand.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4989a56448c7e3047e0538ff4ef54c49db8a5a4f/src%2Flibsyntax%2Fext%2Fexpand.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fext%2Fexpand.rs?ref=4989a56448c7e3047e0538ff4ef54c49db8a5a4f", "patch": "@@ -246,11 +246,11 @@ pub fn expand_expr(e: Gc<ast::Expr>, fld: &mut MacroExpander) -> Gc<ast::Expr> {\n     }\n }\n \n-// Rename loop label and expand its loop body\n-//\n-// The renaming procedure for loop is different in the sense that the loop\n-// body is in a block enclosed by loop head so the renaming of loop label\n-// must be propagated to the enclosed context.\n+/// Rename loop label and expand its loop body\n+///\n+/// The renaming procedure for loop is different in the sense that the loop\n+/// body is in a block enclosed by loop head so the renaming of loop label\n+/// must be propagated to the enclosed context.\n fn expand_loop_block(loop_block: P<Block>,\n                      opt_ident: Option<Ident>,\n                      fld: &mut MacroExpander) -> (P<Block>, Option<Ident>) {"}, {"sha": "786fd953f8901ca9ccfe5d7799f73882fe9bda81", "filename": "src/libsyntax/ext/format.rs", "status": "modified", "additions": 8, "deletions": 8, "changes": 16, "blob_url": "https://github.com/rust-lang/rust/blob/4989a56448c7e3047e0538ff4ef54c49db8a5a4f/src%2Flibsyntax%2Fext%2Fformat.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4989a56448c7e3047e0538ff4ef54c49db8a5a4f/src%2Flibsyntax%2Fext%2Fformat.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fext%2Fformat.rs?ref=4989a56448c7e3047e0538ff4ef54c49db8a5a4f", "patch": "@@ -37,24 +37,24 @@ struct Context<'a, 'b> {\n     ecx: &'a mut ExtCtxt<'b>,\n     fmtsp: Span,\n \n-    // Parsed argument expressions and the types that we've found so far for\n-    // them.\n+    /// Parsed argument expressions and the types that we've found so far for\n+    /// them.\n     args: Vec<Gc<ast::Expr>>,\n     arg_types: Vec<Option<ArgumentType>>,\n-    // Parsed named expressions and the types that we've found for them so far.\n-    // Note that we keep a side-array of the ordering of the named arguments\n-    // found to be sure that we can translate them in the same order that they\n-    // were declared in.\n+    /// Parsed named expressions and the types that we've found for them so far.\n+    /// Note that we keep a side-array of the ordering of the named arguments\n+    /// found to be sure that we can translate them in the same order that they\n+    /// were declared in.\n     names: HashMap<String, Gc<ast::Expr>>,\n     name_types: HashMap<String, ArgumentType>,\n     name_ordering: Vec<String>,\n \n-    // Collection of the compiled `rt::Piece` structures\n+    /// Collection of the compiled `rt::Piece` structures\n     pieces: Vec<Gc<ast::Expr>>,\n     name_positions: HashMap<String, uint>,\n     method_statics: Vec<Gc<ast::Item>>,\n \n-    // Updated as arguments are consumed or methods are entered\n+    /// Updated as arguments are consumed or methods are entered\n     nest_level: uint,\n     next_arg: uint,\n }"}, {"sha": "8608f7fb54553fd7d75a18631095c10852ba3123", "filename": "src/libsyntax/ext/mtwt.rs", "status": "modified", "additions": 26, "deletions": 26, "changes": 52, "blob_url": "https://github.com/rust-lang/rust/blob/4989a56448c7e3047e0538ff4ef54c49db8a5a4f/src%2Flibsyntax%2Fext%2Fmtwt.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4989a56448c7e3047e0538ff4ef54c49db8a5a4f/src%2Flibsyntax%2Fext%2Fmtwt.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fext%2Fmtwt.rs?ref=4989a56448c7e3047e0538ff4ef54c49db8a5a4f", "patch": "@@ -21,16 +21,16 @@ use std::cell::RefCell;\n use std::rc::Rc;\n use std::collections::HashMap;\n \n-// the SCTable contains a table of SyntaxContext_'s. It\n-// represents a flattened tree structure, to avoid having\n-// managed pointers everywhere (that caused an ICE).\n-// the mark_memo and rename_memo fields are side-tables\n-// that ensure that adding the same mark to the same context\n-// gives you back the same context as before. This shouldn't\n-// change the semantics--everything here is immutable--but\n-// it should cut down on memory use *a lot*; applying a mark\n-// to a tree containing 50 identifiers would otherwise generate\n-// 50 new contexts\n+/// The SCTable contains a table of SyntaxContext_'s. It\n+/// represents a flattened tree structure, to avoid having\n+/// managed pointers everywhere (that caused an ICE).\n+/// the mark_memo and rename_memo fields are side-tables\n+/// that ensure that adding the same mark to the same context\n+/// gives you back the same context as before. This shouldn't\n+/// change the semantics--everything here is immutable--but\n+/// it should cut down on memory use *a lot*; applying a mark\n+/// to a tree containing 50 identifiers would otherwise generate\n+/// 50 new contexts\n pub struct SCTable {\n     table: RefCell<Vec<SyntaxContext_>>,\n     mark_memo: RefCell<HashMap<(SyntaxContext,Mrk),SyntaxContext>>,\n@@ -41,16 +41,16 @@ pub struct SCTable {\n pub enum SyntaxContext_ {\n     EmptyCtxt,\n     Mark (Mrk,SyntaxContext),\n-    // flattening the name and syntaxcontext into the rename...\n-    // HIDDEN INVARIANTS:\n-    // 1) the first name in a Rename node\n-    // can only be a programmer-supplied name.\n-    // 2) Every Rename node with a given Name in the\n-    // \"to\" slot must have the same name and context\n-    // in the \"from\" slot. In essence, they're all\n-    // pointers to a single \"rename\" event node.\n+    /// flattening the name and syntaxcontext into the rename...\n+    /// HIDDEN INVARIANTS:\n+    /// 1) the first name in a Rename node\n+    /// can only be a programmer-supplied name.\n+    /// 2) Every Rename node with a given Name in the\n+    /// \"to\" slot must have the same name and context\n+    /// in the \"from\" slot. In essence, they're all\n+    /// pointers to a single \"rename\" event node.\n     Rename (Ident,Name,SyntaxContext),\n-    // actually, IllegalCtxt may not be necessary.\n+    /// actually, IllegalCtxt may not be necessary.\n     IllegalCtxt\n }\n \n@@ -62,7 +62,7 @@ pub fn apply_mark(m: Mrk, ctxt: SyntaxContext) -> SyntaxContext {\n     with_sctable(|table| apply_mark_internal(m, ctxt, table))\n }\n \n-// Extend a syntax context with a given mark and sctable (explicit memoization)\n+/// Extend a syntax context with a given mark and sctable (explicit memoization)\n fn apply_mark_internal(m: Mrk, ctxt: SyntaxContext, table: &SCTable) -> SyntaxContext {\n     let key = (ctxt, m);\n     let new_ctxt = |_: &(SyntaxContext, Mrk)|\n@@ -77,7 +77,7 @@ pub fn apply_rename(id: Ident, to:Name,\n     with_sctable(|table| apply_rename_internal(id, to, ctxt, table))\n }\n \n-// Extend a syntax context with a given rename and sctable (explicit memoization)\n+/// Extend a syntax context with a given rename and sctable (explicit memoization)\n fn apply_rename_internal(id: Ident,\n                        to: Name,\n                        ctxt: SyntaxContext,\n@@ -141,7 +141,7 @@ pub fn clear_tables() {\n     with_resolve_table_mut(|table| *table = HashMap::new());\n }\n \n-// Add a value to the end of a vec, return its index\n+/// Add a value to the end of a vec, return its index\n fn idx_push<T>(vec: &mut Vec<T> , val: T) -> u32 {\n     vec.push(val);\n     (vec.len() - 1) as u32\n@@ -173,8 +173,8 @@ fn with_resolve_table_mut<T>(op: |&mut ResolveTable| -> T) -> T {\n     }\n }\n \n-// Resolve a syntax object to a name, per MTWT.\n-// adding memoization to resolve 500+ seconds in resolve for librustc (!)\n+/// Resolve a syntax object to a name, per MTWT.\n+/// adding memoization to resolve 500+ seconds in resolve for librustc (!)\n fn resolve_internal(id: Ident,\n                     table: &SCTable,\n                     resolve_table: &mut ResolveTable) -> Name {\n@@ -264,8 +264,8 @@ pub fn outer_mark(ctxt: SyntaxContext) -> Mrk {\n     })\n }\n \n-// Push a name... unless it matches the one on top, in which\n-// case pop and discard (so two of the same marks cancel)\n+/// Push a name... unless it matches the one on top, in which\n+/// case pop and discard (so two of the same marks cancel)\n fn xor_push(marks: &mut Vec<Mrk>, mark: Mrk) {\n     if (marks.len() > 0) && (*marks.last().unwrap() == mark) {\n         marks.pop().unwrap();"}, {"sha": "5ac9dc86fcec2017e648743ff443165f581667bb", "filename": "src/libsyntax/ext/source_util.rs", "status": "modified", "additions": 7, "deletions": 7, "changes": 14, "blob_url": "https://github.com/rust-lang/rust/blob/4989a56448c7e3047e0538ff4ef54c49db8a5a4f/src%2Flibsyntax%2Fext%2Fsource_util.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4989a56448c7e3047e0538ff4ef54c49db8a5a4f/src%2Flibsyntax%2Fext%2Fsource_util.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fext%2Fsource_util.rs?ref=4989a56448c7e3047e0538ff4ef54c49db8a5a4f", "patch": "@@ -28,7 +28,7 @@ use std::str;\n // the column/row/filename of the expression, or they include\n // a given file into the current one.\n \n-/* line!(): expands to the current line number */\n+/// line!(): expands to the current line number\n pub fn expand_line(cx: &mut ExtCtxt, sp: Span, tts: &[ast::TokenTree])\n                    -> Box<base::MacResult> {\n     base::check_zero_tts(cx, sp, tts, \"line!\");\n@@ -49,9 +49,9 @@ pub fn expand_col(cx: &mut ExtCtxt, sp: Span, tts: &[ast::TokenTree])\n     base::MacExpr::new(cx.expr_uint(topmost.call_site, loc.col.to_uint()))\n }\n \n-/* file!(): expands to the current filename */\n-/* The filemap (`loc.file`) contains a bunch more information we could spit\n- * out if we wanted. */\n+/// file!(): expands to the current filename */\n+/// The filemap (`loc.file`) contains a bunch more information we could spit\n+/// out if we wanted.\n pub fn expand_file(cx: &mut ExtCtxt, sp: Span, tts: &[ast::TokenTree])\n                    -> Box<base::MacResult> {\n     base::check_zero_tts(cx, sp, tts, \"file!\");\n@@ -82,9 +82,9 @@ pub fn expand_mod(cx: &mut ExtCtxt, sp: Span, tts: &[ast::TokenTree])\n             token::intern_and_get_ident(string.as_slice())))\n }\n \n-// include! : parse the given file as an expr\n-// This is generally a bad idea because it's going to behave\n-// unhygienically.\n+/// include! : parse the given file as an expr\n+/// This is generally a bad idea because it's going to behave\n+/// unhygienically.\n pub fn expand_include(cx: &mut ExtCtxt, sp: Span, tts: &[ast::TokenTree])\n                       -> Box<base::MacResult> {\n     let file = match get_single_str_from_tts(cx, sp, tts, \"include!\") {"}, {"sha": "bdf1f6eb6007e3bc2454069b26b905ad2a11b846", "filename": "src/libsyntax/ext/tt/macro_parser.rs", "status": "modified", "additions": 86, "deletions": 89, "changes": 175, "blob_url": "https://github.com/rust-lang/rust/blob/4989a56448c7e3047e0538ff4ef54c49db8a5a4f/src%2Flibsyntax%2Fext%2Ftt%2Fmacro_parser.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4989a56448c7e3047e0538ff4ef54c49db8a5a4f/src%2Flibsyntax%2Fext%2Ftt%2Fmacro_parser.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fext%2Ftt%2Fmacro_parser.rs?ref=4989a56448c7e3047e0538ff4ef54c49db8a5a4f", "patch": "@@ -8,7 +8,72 @@\n // option. This file may not be copied, modified, or distributed\n // except according to those terms.\n \n-// Earley-like parser for macros.\n+//! This is an Earley-like parser, without support for in-grammar nonterminals,\n+//! only by calling out to the main rust parser for named nonterminals (which it\n+//! commits to fully when it hits one in a grammar). This means that there are no\n+//! completer or predictor rules, and therefore no need to store one column per\n+//! token: instead, there's a set of current Earley items and a set of next\n+//! ones. Instead of NTs, we have a special case for Kleene star. The big-O, in\n+//! pathological cases, is worse than traditional Earley parsing, but it's an\n+//! easier fit for Macro-by-Example-style rules, and I think the overhead is\n+//! lower. (In order to prevent the pathological case, we'd need to lazily\n+//! construct the resulting `NamedMatch`es at the very end. It'd be a pain,\n+//! and require more memory to keep around old items, but it would also save\n+//! overhead)\n+//!\n+//! Quick intro to how the parser works:\n+//!\n+//! A 'position' is a dot in the middle of a matcher, usually represented as a\n+//! dot. For example `\u00b7 a $( a )* a b` is a position, as is `a $( \u00b7 a )* a b`.\n+//!\n+//! The parser walks through the input a character at a time, maintaining a list\n+//! of items consistent with the current position in the input string: `cur_eis`.\n+//!\n+//! As it processes them, it fills up `eof_eis` with items that would be valid if\n+//! the macro invocation is now over, `bb_eis` with items that are waiting on\n+//! a Rust nonterminal like `$e:expr`, and `next_eis` with items that are waiting\n+//! on the a particular token. Most of the logic concerns moving the \u00b7 through the\n+//! repetitions indicated by Kleene stars. It only advances or calls out to the\n+//! real Rust parser when no `cur_eis` items remain\n+//!\n+//! Example: Start parsing `a a a a b` against [\u00b7 a $( a )* a b].\n+//!\n+//! Remaining input: `a a a a b`\n+//! next_eis: [\u00b7 a $( a )* a b]\n+//!\n+//! - - - Advance over an `a`. - - -\n+//!\n+//! Remaining input: `a a a b`\n+//! cur: [a \u00b7 $( a )* a b]\n+//! Descend/Skip (first item).\n+//! next: [a $( \u00b7 a )* a b]  [a $( a )* \u00b7 a b].\n+//!\n+//! - - - Advance over an `a`. - - -\n+//!\n+//! Remaining input: `a a b`\n+//! cur: [a $( a \u00b7 )* a b]  next: [a $( a )* a \u00b7 b]\n+//! Finish/Repeat (first item)\n+//! next: [a $( a )* \u00b7 a b]  [a $( \u00b7 a )* a b]  [a $( a )* a \u00b7 b]\n+//!\n+//! - - - Advance over an `a`. - - - (this looks exactly like the last step)\n+//!\n+//! Remaining input: `a b`\n+//! cur: [a $( a \u00b7 )* a b]  next: [a $( a )* a \u00b7 b]\n+//! Finish/Repeat (first item)\n+//! next: [a $( a )* \u00b7 a b]  [a $( \u00b7 a )* a b]  [a $( a )* a \u00b7 b]\n+//!\n+//! - - - Advance over an `a`. - - - (this looks exactly like the last step)\n+//!\n+//! Remaining input: `b`\n+//! cur: [a $( a \u00b7 )* a b]  next: [a $( a )* a \u00b7 b]\n+//! Finish/Repeat (first item)\n+//! next: [a $( a )* \u00b7 a b]  [a $( \u00b7 a )* a b]\n+//!\n+//! - - - Advance over a `b`. - - -\n+//!\n+//! Remaining input: ``\n+//! eof: [a $( a )* a b \u00b7]\n+\n \n use ast;\n use ast::{Matcher, MatchTok, MatchSeq, MatchNonterminal, Ident};\n@@ -25,75 +90,6 @@ use std::rc::Rc;\n use std::gc::GC;\n use std::collections::HashMap;\n \n-/* This is an Earley-like parser, without support for in-grammar nonterminals,\n-only by calling out to the main rust parser for named nonterminals (which it\n-commits to fully when it hits one in a grammar). This means that there are no\n-completer or predictor rules, and therefore no need to store one column per\n-token: instead, there's a set of current Earley items and a set of next\n-ones. Instead of NTs, we have a special case for Kleene star. The big-O, in\n-pathological cases, is worse than traditional Earley parsing, but it's an\n-easier fit for Macro-by-Example-style rules, and I think the overhead is\n-lower. (In order to prevent the pathological case, we'd need to lazily\n-construct the resulting `NamedMatch`es at the very end. It'd be a pain,\n-and require more memory to keep around old items, but it would also save\n-overhead)*/\n-\n-/* Quick intro to how the parser works:\n-\n-A 'position' is a dot in the middle of a matcher, usually represented as a\n-dot. For example `\u00b7 a $( a )* a b` is a position, as is `a $( \u00b7 a )* a b`.\n-\n-The parser walks through the input a character at a time, maintaining a list\n-of items consistent with the current position in the input string: `cur_eis`.\n-\n-As it processes them, it fills up `eof_eis` with items that would be valid if\n-the macro invocation is now over, `bb_eis` with items that are waiting on\n-a Rust nonterminal like `$e:expr`, and `next_eis` with items that are waiting\n-on the a particular token. Most of the logic concerns moving the \u00b7 through the\n-repetitions indicated by Kleene stars. It only advances or calls out to the\n-real Rust parser when no `cur_eis` items remain\n-\n-Example: Start parsing `a a a a b` against [\u00b7 a $( a )* a b].\n-\n-Remaining input: `a a a a b`\n-next_eis: [\u00b7 a $( a )* a b]\n-\n-- - - Advance over an `a`. - - -\n-\n-Remaining input: `a a a b`\n-cur: [a \u00b7 $( a )* a b]\n-Descend/Skip (first item).\n-next: [a $( \u00b7 a )* a b]  [a $( a )* \u00b7 a b].\n-\n-- - - Advance over an `a`. - - -\n-\n-Remaining input: `a a b`\n-cur: [a $( a \u00b7 )* a b]  next: [a $( a )* a \u00b7 b]\n-Finish/Repeat (first item)\n-next: [a $( a )* \u00b7 a b]  [a $( \u00b7 a )* a b]  [a $( a )* a \u00b7 b]\n-\n-- - - Advance over an `a`. - - - (this looks exactly like the last step)\n-\n-Remaining input: `a b`\n-cur: [a $( a \u00b7 )* a b]  next: [a $( a )* a \u00b7 b]\n-Finish/Repeat (first item)\n-next: [a $( a )* \u00b7 a b]  [a $( \u00b7 a )* a b]  [a $( a )* a \u00b7 b]\n-\n-- - - Advance over an `a`. - - - (this looks exactly like the last step)\n-\n-Remaining input: `b`\n-cur: [a $( a \u00b7 )* a b]  next: [a $( a )* a \u00b7 b]\n-Finish/Repeat (first item)\n-next: [a $( a )* \u00b7 a b]  [a $( \u00b7 a )* a b]\n-\n-- - - Advance over a `b`. - - -\n-\n-Remaining input: ``\n-eof: [a $( a )* a b \u00b7]\n-\n- */\n-\n-\n /* to avoid costly uniqueness checks, we require that `MatchSeq` always has a\n nonempty body. */\n \n@@ -147,24 +143,24 @@ pub fn initial_matcher_pos(ms: Vec<Matcher> , sep: Option<Token>, lo: BytePos)\n     }\n }\n \n-// NamedMatch is a pattern-match result for a single ast::MatchNonterminal:\n-// so it is associated with a single ident in a parse, and all\n-// MatchedNonterminal's in the NamedMatch have the same nonterminal type\n-// (expr, item, etc). All the leaves in a single NamedMatch correspond to a\n-// single matcher_nonterminal in the ast::Matcher that produced it.\n-//\n-// It should probably be renamed, it has more or less exact correspondence to\n-// ast::match nodes, and the in-memory structure of a particular NamedMatch\n-// represents the match that occurred when a particular subset of an\n-// ast::match -- those ast::Matcher nodes leading to a single\n-// MatchNonterminal -- was applied to a particular token tree.\n-//\n-// The width of each MatchedSeq in the NamedMatch, and the identity of the\n-// MatchedNonterminal's, will depend on the token tree it was applied to: each\n-// MatchedSeq corresponds to a single MatchSeq in the originating\n-// ast::Matcher. The depth of the NamedMatch structure will therefore depend\n-// only on the nesting depth of ast::MatchSeq's in the originating\n-// ast::Matcher it was derived from.\n+/// NamedMatch is a pattern-match result for a single ast::MatchNonterminal:\n+/// so it is associated with a single ident in a parse, and all\n+/// MatchedNonterminal's in the NamedMatch have the same nonterminal type\n+/// (expr, item, etc). All the leaves in a single NamedMatch correspond to a\n+/// single matcher_nonterminal in the ast::Matcher that produced it.\n+///\n+/// It should probably be renamed, it has more or less exact correspondence to\n+/// ast::match nodes, and the in-memory structure of a particular NamedMatch\n+/// represents the match that occurred when a particular subset of an\n+/// ast::match -- those ast::Matcher nodes leading to a single\n+/// MatchNonterminal -- was applied to a particular token tree.\n+///\n+/// The width of each MatchedSeq in the NamedMatch, and the identity of the\n+/// MatchedNonterminal's, will depend on the token tree it was applied to: each\n+/// MatchedSeq corresponds to a single MatchSeq in the originating\n+/// ast::Matcher. The depth of the NamedMatch structure will therefore depend\n+/// only on the nesting depth of ast::MatchSeq's in the originating\n+/// ast::Matcher it was derived from.\n \n pub enum NamedMatch {\n     MatchedSeq(Vec<Rc<NamedMatch>>, codemap::Span),\n@@ -224,7 +220,8 @@ pub fn parse_or_else(sess: &ParseSess,\n     }\n }\n \n-// perform a token equality check, ignoring syntax context (that is, an unhygienic comparison)\n+/// Perform a token equality check, ignoring syntax context (that is, an\n+/// unhygienic comparison)\n pub fn token_name_eq(t1 : &Token, t2 : &Token) -> bool {\n     match (t1,t2) {\n         (&token::IDENT(id1,_),&token::IDENT(id2,_))"}, {"sha": "249e9305150d665acf4da8d348ea4c45b0359461", "filename": "src/libsyntax/ext/tt/macro_rules.rs", "status": "modified", "additions": 4, "deletions": 4, "changes": 8, "blob_url": "https://github.com/rust-lang/rust/blob/4989a56448c7e3047e0538ff4ef54c49db8a5a4f/src%2Flibsyntax%2Fext%2Ftt%2Fmacro_rules.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4989a56448c7e3047e0538ff4ef54c49db8a5a4f/src%2Flibsyntax%2Fext%2Ftt%2Fmacro_rules.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fext%2Ftt%2Fmacro_rules.rs?ref=4989a56448c7e3047e0538ff4ef54c49db8a5a4f", "patch": "@@ -119,7 +119,7 @@ impl MacResult for MacroRulesDefiner {\n     }\n }\n \n-// Given `lhses` and `rhses`, this is the new macro we create\n+/// Given `lhses` and `rhses`, this is the new macro we create\n fn generic_extension(cx: &ExtCtxt,\n                      sp: Span,\n                      name: Ident,\n@@ -193,9 +193,9 @@ fn generic_extension(cx: &ExtCtxt,\n     cx.span_fatal(best_fail_spot, best_fail_msg.as_slice());\n }\n \n-// this procedure performs the expansion of the\n-// macro_rules! macro. It parses the RHS and adds\n-// an extension to the current context.\n+/// This procedure performs the expansion of the\n+/// macro_rules! macro. It parses the RHS and adds\n+/// an extension to the current context.\n pub fn add_new_extension(cx: &mut ExtCtxt,\n                          sp: Span,\n                          name: Ident,"}, {"sha": "726a7315f69913c387b89c814edac51c863fa4ba", "filename": "src/libsyntax/ext/tt/transcribe.rs", "status": "modified", "additions": 6, "deletions": 6, "changes": 12, "blob_url": "https://github.com/rust-lang/rust/blob/4989a56448c7e3047e0538ff4ef54c49db8a5a4f/src%2Flibsyntax%2Fext%2Ftt%2Ftranscribe.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4989a56448c7e3047e0538ff4ef54c49db8a5a4f/src%2Flibsyntax%2Fext%2Ftt%2Ftranscribe.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fext%2Ftt%2Ftranscribe.rs?ref=4989a56448c7e3047e0538ff4ef54c49db8a5a4f", "patch": "@@ -32,7 +32,7 @@ struct TtFrame {\n #[deriving(Clone)]\n pub struct TtReader<'a> {\n     pub sp_diag: &'a SpanHandler,\n-    // the unzipped tree:\n+    /// the unzipped tree:\n     stack: Vec<TtFrame>,\n     /* for MBE-style macro transcription */\n     interpolations: HashMap<Ident, Rc<NamedMatch>>,\n@@ -43,9 +43,9 @@ pub struct TtReader<'a> {\n     pub cur_span: Span,\n }\n \n-/** This can do Macro-By-Example transcription. On the other hand, if\n- *  `src` contains no `TTSeq`s and `TTNonterminal`s, `interp` can (and\n- *  should) be none. */\n+/// This can do Macro-By-Example transcription. On the other hand, if\n+/// `src` contains no `TTSeq`s and `TTNonterminal`s, `interp` can (and\n+/// should) be none.\n pub fn new_tt_reader<'a>(sp_diag: &'a SpanHandler,\n                          interp: Option<HashMap<Ident, Rc<NamedMatch>>>,\n                          src: Vec<ast::TokenTree> )\n@@ -138,8 +138,8 @@ fn lockstep_iter_size(t: &TokenTree, r: &TtReader) -> LockstepIterSize {\n     }\n }\n \n-// return the next token from the TtReader.\n-// EFFECT: advances the reader's token field\n+/// Return the next token from the TtReader.\n+/// EFFECT: advances the reader's token field\n pub fn tt_next_token(r: &mut TtReader) -> TokenAndSpan {\n     // FIXME(pcwalton): Bad copy?\n     let ret_val = TokenAndSpan {"}, {"sha": "53ee991385ae3aafca4f50650b252d159f2afe85", "filename": "src/libsyntax/lib.rs", "status": "modified", "additions": 5, "deletions": 9, "changes": 14, "blob_url": "https://github.com/rust-lang/rust/blob/4989a56448c7e3047e0538ff4ef54c49db8a5a4f/src%2Flibsyntax%2Flib.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4989a56448c7e3047e0538ff4ef54c49db8a5a4f/src%2Flibsyntax%2Flib.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Flib.rs?ref=4989a56448c7e3047e0538ff4ef54c49db8a5a4f", "patch": "@@ -8,15 +8,11 @@\n // option. This file may not be copied, modified, or distributed\n // except according to those terms.\n \n-/*!\n-\n-The Rust parser and macro expander.\n-\n-# Note\n-\n-This API is completely unstable and subject to change.\n-\n-*/\n+//! The Rust parser and macro expander.\n+//!\n+//! # Note\n+//!\n+//! This API is completely unstable and subject to change.\n \n #![crate_id = \"syntax#0.11.0\"] // NOTE: remove after stage0\n #![crate_name = \"syntax\"]"}, {"sha": "b2297ec770cc33740785f043866fa5fd04eb7ade", "filename": "src/libsyntax/parse/attr.rs", "status": "modified", "additions": 21, "deletions": 21, "changes": 42, "blob_url": "https://github.com/rust-lang/rust/blob/4989a56448c7e3047e0538ff4ef54c49db8a5a4f/src%2Flibsyntax%2Fparse%2Fattr.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4989a56448c7e3047e0538ff4ef54c49db8a5a4f/src%2Flibsyntax%2Fparse%2Fattr.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Fattr.rs?ref=4989a56448c7e3047e0538ff4ef54c49db8a5a4f", "patch": "@@ -18,7 +18,7 @@ use parse::token::INTERPOLATED;\n \n use std::gc::{Gc, GC};\n \n-// a parser that can parse attributes.\n+/// A parser that can parse attributes.\n pub trait ParserAttr {\n     fn parse_outer_attributes(&mut self) -> Vec<ast::Attribute>;\n     fn parse_attribute(&mut self, permit_inner: bool) -> ast::Attribute;\n@@ -30,7 +30,7 @@ pub trait ParserAttr {\n }\n \n impl<'a> ParserAttr for Parser<'a> {\n-    // Parse attributes that appear before an item\n+    /// Parse attributes that appear before an item\n     fn parse_outer_attributes(&mut self) -> Vec<ast::Attribute> {\n         let mut attrs: Vec<ast::Attribute> = Vec::new();\n         loop {\n@@ -59,10 +59,10 @@ impl<'a> ParserAttr for Parser<'a> {\n         return attrs;\n     }\n \n-    // matches attribute = # ! [ meta_item ]\n-    //\n-    // if permit_inner is true, then a leading `!` indicates an inner\n-    // attribute\n+    /// Matches `attribute = # ! [ meta_item ]`\n+    ///\n+    /// If permit_inner is true, then a leading `!` indicates an inner\n+    /// attribute\n     fn parse_attribute(&mut self, permit_inner: bool) -> ast::Attribute {\n         debug!(\"parse_attributes: permit_inner={:?} self.token={:?}\",\n                permit_inner, self.token);\n@@ -114,17 +114,17 @@ impl<'a> ParserAttr for Parser<'a> {\n         };\n     }\n \n-    // Parse attributes that appear after the opening of an item. These should\n-    // be preceded by an exclamation mark, but we accept and warn about one\n-    // terminated by a semicolon. In addition to a vector of inner attributes,\n-    // this function also returns a vector that may contain the first outer\n-    // attribute of the next item (since we can't know whether the attribute\n-    // is an inner attribute of the containing item or an outer attribute of\n-    // the first contained item until we see the semi).\n-\n-    // matches inner_attrs* outer_attr?\n-    // you can make the 'next' field an Option, but the result is going to be\n-    // more useful as a vector.\n+    /// Parse attributes that appear after the opening of an item. These should\n+    /// be preceded by an exclamation mark, but we accept and warn about one\n+    /// terminated by a semicolon. In addition to a vector of inner attributes,\n+    /// this function also returns a vector that may contain the first outer\n+    /// attribute of the next item (since we can't know whether the attribute\n+    /// is an inner attribute of the containing item or an outer attribute of\n+    /// the first contained item until we see the semi).\n+\n+    /// matches inner_attrs* outer_attr?\n+    /// you can make the 'next' field an Option, but the result is going to be\n+    /// more useful as a vector.\n     fn parse_inner_attrs_and_next(&mut self)\n                                   -> (Vec<ast::Attribute> , Vec<ast::Attribute> ) {\n         let mut inner_attrs: Vec<ast::Attribute> = Vec::new();\n@@ -157,9 +157,9 @@ impl<'a> ParserAttr for Parser<'a> {\n         (inner_attrs, next_outer_attrs)\n     }\n \n-    // matches meta_item = IDENT\n-    // | IDENT = lit\n-    // | IDENT meta_seq\n+    /// matches meta_item = IDENT\n+    /// | IDENT = lit\n+    /// | IDENT meta_seq\n     fn parse_meta_item(&mut self) -> Gc<ast::MetaItem> {\n         match self.token {\n             token::INTERPOLATED(token::NtMeta(e)) => {\n@@ -201,7 +201,7 @@ impl<'a> ParserAttr for Parser<'a> {\n         }\n     }\n \n-    // matches meta_seq = ( COMMASEP(meta_item) )\n+    /// matches meta_seq = ( COMMASEP(meta_item) )\n     fn parse_meta_seq(&mut self) -> Vec<Gc<ast::MetaItem>> {\n         self.parse_seq(&token::LPAREN,\n                        &token::RPAREN,"}, {"sha": "516f22cdf4d606abab30e64661184aa4714a36f1", "filename": "src/libsyntax/parse/classify.rs", "status": "modified", "additions": 10, "deletions": 10, "changes": 20, "blob_url": "https://github.com/rust-lang/rust/blob/4989a56448c7e3047e0538ff4ef54c49db8a5a4f/src%2Flibsyntax%2Fparse%2Fclassify.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4989a56448c7e3047e0538ff4ef54c49db8a5a4f/src%2Flibsyntax%2Fparse%2Fclassify.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Fclassify.rs?ref=4989a56448c7e3047e0538ff4ef54c49db8a5a4f", "patch": "@@ -15,13 +15,13 @@\n use ast;\n use std::gc::Gc;\n \n-// does this expression require a semicolon to be treated\n-// as a statement? The negation of this: 'can this expression\n-// be used as a statement without a semicolon' -- is used\n-// as an early-bail-out in the parser so that, for instance,\n-// 'if true {...} else {...}\n-//  |x| 5 '\n-// isn't parsed as (if true {...} else {...} | x) | 5\n+/// Does this expression require a semicolon to be treated\n+/// as a statement? The negation of this: 'can this expression\n+/// be used as a statement without a semicolon' -- is used\n+/// as an early-bail-out in the parser so that, for instance,\n+///     if true {...} else {...}\n+///      |x| 5\n+/// isn't parsed as (if true {...} else {...} | x) | 5\n pub fn expr_requires_semi_to_be_stmt(e: Gc<ast::Expr>) -> bool {\n     match e.node {\n         ast::ExprIf(..)\n@@ -41,9 +41,9 @@ pub fn expr_is_simple_block(e: Gc<ast::Expr>) -> bool {\n     }\n }\n \n-// this statement requires a semicolon after it.\n-// note that in one case (stmt_semi), we've already\n-// seen the semicolon, and thus don't need another.\n+/// this statement requires a semicolon after it.\n+/// note that in one case (stmt_semi), we've already\n+/// seen the semicolon, and thus don't need another.\n pub fn stmt_ends_with_semi(stmt: &ast::Stmt) -> bool {\n     return match stmt.node {\n         ast::StmtDecl(d, _) => {"}, {"sha": "3842170d67777ac039b12ca3536ca1888f29cbb9", "filename": "src/libsyntax/parse/common.rs", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/rust-lang/rust/blob/4989a56448c7e3047e0538ff4ef54c49db8a5a4f/src%2Flibsyntax%2Fparse%2Fcommon.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4989a56448c7e3047e0538ff4ef54c49db8a5a4f/src%2Flibsyntax%2Fparse%2Fcommon.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Fcommon.rs?ref=4989a56448c7e3047e0538ff4ef54c49db8a5a4f", "patch": "@@ -12,8 +12,8 @@\n \n use parse::token;\n \n-// SeqSep : a sequence separator (token)\n-// and whether a trailing separator is allowed.\n+/// SeqSep : a sequence separator (token)\n+/// and whether a trailing separator is allowed.\n pub struct SeqSep {\n     pub sep: Option<token::Token>,\n     pub trailing_sep_allowed: bool"}, {"sha": "c5dd10382a9591498dda52f34c2afaf1fbc19809", "filename": "src/libsyntax/parse/lexer/comments.rs", "status": "modified", "additions": 11, "deletions": 7, "changes": 18, "blob_url": "https://github.com/rust-lang/rust/blob/4989a56448c7e3047e0538ff4ef54c49db8a5a4f/src%2Flibsyntax%2Fparse%2Flexer%2Fcomments.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4989a56448c7e3047e0538ff4ef54c49db8a5a4f/src%2Flibsyntax%2Fparse%2Flexer%2Fcomments.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Flexer%2Fcomments.rs?ref=4989a56448c7e3047e0538ff4ef54c49db8a5a4f", "patch": "@@ -24,10 +24,14 @@ use std::uint;\n \n #[deriving(Clone, PartialEq)]\n pub enum CommentStyle {\n-    Isolated, // No code on either side of each line of the comment\n-    Trailing, // Code exists to the left of the comment\n-    Mixed, // Code before /* foo */ and after the comment\n-    BlankLine, // Just a manual blank line \"\\n\\n\", for layout\n+    /// No code on either side of each line of the comment\n+    Isolated,\n+    /// Code exists to the left of the comment\n+    Trailing,\n+    /// Code before /* foo */ and after the comment\n+    Mixed,\n+    /// Just a manual blank line \"\\n\\n\", for layout\n+    BlankLine,\n }\n \n #[deriving(Clone)]\n@@ -198,9 +202,9 @@ fn read_line_comments(rdr: &mut StringReader, code_to_the_left: bool,\n     }\n }\n \n-// Returns None if the first col chars of s contain a non-whitespace char.\n-// Otherwise returns Some(k) where k is first char offset after that leading\n-// whitespace.  Note k may be outside bounds of s.\n+/// Returns None if the first col chars of s contain a non-whitespace char.\n+/// Otherwise returns Some(k) where k is first char offset after that leading\n+/// whitespace.  Note k may be outside bounds of s.\n fn all_whitespace(s: &str, col: CharPos) -> Option<uint> {\n     let len = s.len();\n     let mut col = col.to_uint();"}, {"sha": "43bbba8527199fc8b38096bb3b9e83ed4b8f8b9a", "filename": "src/libsyntax/parse/lexer/mod.rs", "status": "modified", "additions": 6, "deletions": 6, "changes": 12, "blob_url": "https://github.com/rust-lang/rust/blob/4989a56448c7e3047e0538ff4ef54c49db8a5a4f/src%2Flibsyntax%2Fparse%2Flexer%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4989a56448c7e3047e0538ff4ef54c49db8a5a4f/src%2Flibsyntax%2Fparse%2Flexer%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Flexer%2Fmod.rs?ref=4989a56448c7e3047e0538ff4ef54c49db8a5a4f", "patch": "@@ -44,13 +44,13 @@ pub struct TokenAndSpan {\n \n pub struct StringReader<'a> {\n     pub span_diagnostic: &'a SpanHandler,\n-    // The absolute offset within the codemap of the next character to read\n+    /// The absolute offset within the codemap of the next character to read\n     pub pos: BytePos,\n-    // The absolute offset within the codemap of the last character read(curr)\n+    /// The absolute offset within the codemap of the last character read(curr)\n     pub last_pos: BytePos,\n-    // The column of the next character to read\n+    /// The column of the next character to read\n     pub col: CharPos,\n-    // The last character to be read\n+    /// The last character to be read\n     pub curr: Option<char>,\n     pub filemap: Rc<codemap::FileMap>,\n     /* cached: */\n@@ -60,7 +60,7 @@ pub struct StringReader<'a> {\n \n impl<'a> Reader for StringReader<'a> {\n     fn is_eof(&self) -> bool { self.curr.is_none() }\n-    // return the next token. EFFECT: advances the string_reader.\n+    /// Return the next token. EFFECT: advances the string_reader.\n     fn next_token(&mut self) -> TokenAndSpan {\n         let ret_val = TokenAndSpan {\n             tok: replace(&mut self.peek_tok, token::UNDERSCORE),\n@@ -417,7 +417,7 @@ impl<'a> StringReader<'a> {\n         return self.consume_any_line_comment();\n     }\n \n-    // might return a sugared-doc-attr\n+    /// Might return a sugared-doc-attr\n     fn consume_block_comment(&mut self) -> Option<TokenAndSpan> {\n         // block comments starting with \"/**\" or \"/*!\" are doc-comments\n         let is_doc_comment = self.curr_is('*') || self.curr_is('!');"}, {"sha": "bea8b6a94d43db7a5b38a75a39c3ac40c5f48943", "filename": "src/libsyntax/parse/mod.rs", "status": "modified", "additions": 6, "deletions": 7, "changes": 13, "blob_url": "https://github.com/rust-lang/rust/blob/4989a56448c7e3047e0538ff4ef54c49db8a5a4f/src%2Flibsyntax%2Fparse%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4989a56448c7e3047e0538ff4ef54c49db8a5a4f/src%2Flibsyntax%2Fparse%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Fmod.rs?ref=4989a56448c7e3047e0538ff4ef54c49db8a5a4f", "patch": "@@ -10,7 +10,6 @@\n \n //! The main parser interface\n \n-\n use ast;\n use codemap::{Span, CodeMap, FileMap};\n use diagnostic::{SpanHandler, mk_span_handler, default_handler, Auto};\n@@ -32,7 +31,7 @@ pub mod common;\n pub mod classify;\n pub mod obsolete;\n \n-// info about a parsing session.\n+/// Info about a parsing session.\n pub struct ParseSess {\n     pub span_diagnostic: SpanHandler, // better be the same as the one in the reader!\n     /// Used to determine and report recursive mod inclusions\n@@ -241,14 +240,14 @@ pub fn file_to_filemap(sess: &ParseSess, path: &Path, spanopt: Option<Span>)\n     unreachable!()\n }\n \n-// given a session and a string, add the string to\n-// the session's codemap and return the new filemap\n+/// Given a session and a string, add the string to\n+/// the session's codemap and return the new filemap\n pub fn string_to_filemap(sess: &ParseSess, source: String, path: String)\n                          -> Rc<FileMap> {\n     sess.span_diagnostic.cm.new_filemap(path, source)\n }\n \n-// given a filemap, produce a sequence of token-trees\n+/// Given a filemap, produce a sequence of token-trees\n pub fn filemap_to_tts(sess: &ParseSess, filemap: Rc<FileMap>)\n     -> Vec<ast::TokenTree> {\n     // it appears to me that the cfg doesn't matter here... indeed,\n@@ -259,15 +258,15 @@ pub fn filemap_to_tts(sess: &ParseSess, filemap: Rc<FileMap>)\n     p1.parse_all_token_trees()\n }\n \n-// given tts and cfg, produce a parser\n+/// Given tts and cfg, produce a parser\n pub fn tts_to_parser<'a>(sess: &'a ParseSess,\n                          tts: Vec<ast::TokenTree>,\n                          cfg: ast::CrateConfig) -> Parser<'a> {\n     let trdr = lexer::new_tt_reader(&sess.span_diagnostic, None, tts);\n     Parser::new(sess, cfg, box trdr)\n }\n \n-// abort if necessary\n+/// Abort if necessary\n pub fn maybe_aborted<T>(result: T, mut p: Parser) -> T {\n     p.abort_if_errors();\n     result"}, {"sha": "cadae7ef12f8078b38dfc67a6dde7aed99d7000f", "filename": "src/libsyntax/parse/obsolete.rs", "status": "modified", "additions": 4, "deletions": 4, "changes": 8, "blob_url": "https://github.com/rust-lang/rust/blob/4989a56448c7e3047e0538ff4ef54c49db8a5a4f/src%2Flibsyntax%2Fparse%2Fobsolete.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4989a56448c7e3047e0538ff4ef54c49db8a5a4f/src%2Flibsyntax%2Fparse%2Fobsolete.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Fobsolete.rs?ref=4989a56448c7e3047e0538ff4ef54c49db8a5a4f", "patch": "@@ -38,8 +38,8 @@ pub enum ObsoleteSyntax {\n pub trait ParserObsoleteMethods {\n     /// Reports an obsolete syntax non-fatal error.\n     fn obsolete(&mut self, sp: Span, kind: ObsoleteSyntax);\n-    // Reports an obsolete syntax non-fatal error, and returns\n-    // a placeholder expression\n+    /// Reports an obsolete syntax non-fatal error, and returns\n+    /// a placeholder expression\n     fn obsolete_expr(&mut self, sp: Span, kind: ObsoleteSyntax) -> Gc<Expr>;\n     fn report(&mut self,\n               sp: Span,\n@@ -83,8 +83,8 @@ impl<'a> ParserObsoleteMethods for parser::Parser<'a> {\n         self.report(sp, kind, kind_str, desc);\n     }\n \n-    // Reports an obsolete syntax non-fatal error, and returns\n-    // a placeholder expression\n+    /// Reports an obsolete syntax non-fatal error, and returns\n+    /// a placeholder expression\n     fn obsolete_expr(&mut self, sp: Span, kind: ObsoleteSyntax) -> Gc<Expr> {\n         self.obsolete(sp, kind);\n         self.mk_expr(sp.lo, sp.hi, ExprLit(box(GC) respan(sp, LitNil)))"}, {"sha": "3bf88424891bb2b4e8ab3d8cd536eeff13187e96", "filename": "src/libsyntax/parse/parser.rs", "status": "modified", "additions": 225, "deletions": 224, "changes": 449, "blob_url": "https://github.com/rust-lang/rust/blob/4989a56448c7e3047e0538ff4ef54c49db8a5a4f/src%2Flibsyntax%2Fparse%2Fparser.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4989a56448c7e3047e0538ff4ef54c49db8a5a4f/src%2Flibsyntax%2Fparse%2Fparser.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Fparser.rs?ref=4989a56448c7e3047e0538ff4ef54c49db8a5a4f", "patch": "@@ -117,21 +117,21 @@ pub struct PathAndBounds {\n }\n \n enum ItemOrViewItem {\n-    // Indicates a failure to parse any kind of item. The attributes are\n-    // returned.\n+    /// Indicates a failure to parse any kind of item. The attributes are\n+    /// returned.\n     IoviNone(Vec<Attribute>),\n     IoviItem(Gc<Item>),\n     IoviForeignItem(Gc<ForeignItem>),\n     IoviViewItem(ViewItem)\n }\n \n \n-// Possibly accept an `INTERPOLATED` expression (a pre-parsed expression\n-// dropped into the token stream, which happens while parsing the\n-// result of macro expansion)\n-/* Placement of these is not as complex as I feared it would be.\n-The important thing is to make sure that lookahead doesn't balk\n-at INTERPOLATED tokens */\n+/// Possibly accept an `INTERPOLATED` expression (a pre-parsed expression\n+/// dropped into the token stream, which happens while parsing the\n+/// result of macro expansion)\n+/// Placement of these is not as complex as I feared it would be.\n+/// The important thing is to make sure that lookahead doesn't balk\n+/// at INTERPOLATED tokens\n macro_rules! maybe_whole_expr (\n     ($p:expr) => (\n         {\n@@ -166,7 +166,7 @@ macro_rules! maybe_whole_expr (\n     )\n )\n \n-// As above, but for things other than expressions\n+/// As maybe_whole_expr, but for things other than expressions\n macro_rules! maybe_whole (\n     ($p:expr, $constructor:ident) => (\n         {\n@@ -287,14 +287,14 @@ struct ParsedItemsAndViewItems {\n \n pub struct Parser<'a> {\n     pub sess: &'a ParseSess,\n-    // the current token:\n+    /// the current token:\n     pub token: token::Token,\n-    // the span of the current token:\n+    /// the span of the current token:\n     pub span: Span,\n-    // the span of the prior token:\n+    /// the span of the prior token:\n     pub last_span: Span,\n     pub cfg: CrateConfig,\n-    // the previous token or None (only stashed sometimes).\n+    /// the previous token or None (only stashed sometimes).\n     pub last_token: Option<Box<token::Token>>,\n     pub buffer: [TokenAndSpan, ..4],\n     pub buffer_start: int,\n@@ -361,12 +361,13 @@ impl<'a> Parser<'a> {\n             root_module_name: None,\n         }\n     }\n-    // convert a token to a string using self's reader\n+\n+    /// Convert a token to a string using self's reader\n     pub fn token_to_string(token: &token::Token) -> String {\n         token::to_string(token)\n     }\n \n-    // convert the current token to a string using self's reader\n+    /// Convert the current token to a string using self's reader\n     pub fn this_token_to_string(&mut self) -> String {\n         Parser::token_to_string(&self.token)\n     }\n@@ -383,8 +384,8 @@ impl<'a> Parser<'a> {\n         self.fatal(format!(\"unexpected token: `{}`\", this_token).as_slice());\n     }\n \n-    // expect and consume the token t. Signal an error if\n-    // the next token is not t.\n+    /// Expect and consume the token t. Signal an error if\n+    /// the next token is not t.\n     pub fn expect(&mut self, t: &token::Token) {\n         if self.token == *t {\n             self.bump();\n@@ -397,9 +398,9 @@ impl<'a> Parser<'a> {\n         }\n     }\n \n-    // Expect next token to be edible or inedible token.  If edible,\n-    // then consume it; if inedible, then return without consuming\n-    // anything.  Signal a fatal error if next token is unexpected.\n+    /// Expect next token to be edible or inedible token.  If edible,\n+    /// then consume it; if inedible, then return without consuming\n+    /// anything.  Signal a fatal error if next token is unexpected.\n     pub fn expect_one_of(&mut self,\n                          edible: &[token::Token],\n                          inedible: &[token::Token]) {\n@@ -437,9 +438,9 @@ impl<'a> Parser<'a> {\n         }\n     }\n \n-    // Check for erroneous `ident { }`; if matches, signal error and\n-    // recover (without consuming any expected input token).  Returns\n-    // true if and only if input was consumed for recovery.\n+    /// Check for erroneous `ident { }`; if matches, signal error and\n+    /// recover (without consuming any expected input token).  Returns\n+    /// true if and only if input was consumed for recovery.\n     pub fn check_for_erroneous_unit_struct_expecting(&mut self, expected: &[token::Token]) -> bool {\n         if self.token == token::LBRACE\n             && expected.iter().all(|t| *t != token::LBRACE)\n@@ -456,9 +457,9 @@ impl<'a> Parser<'a> {\n         }\n     }\n \n-    // Commit to parsing a complete expression `e` expected to be\n-    // followed by some token from the set edible + inedible.  Recover\n-    // from anticipated input errors, discarding erroneous characters.\n+    /// Commit to parsing a complete expression `e` expected to be\n+    /// followed by some token from the set edible + inedible.  Recover\n+    /// from anticipated input errors, discarding erroneous characters.\n     pub fn commit_expr(&mut self, e: Gc<Expr>, edible: &[token::Token],\n                        inedible: &[token::Token]) {\n         debug!(\"commit_expr {:?}\", e);\n@@ -479,9 +480,9 @@ impl<'a> Parser<'a> {\n         self.commit_expr(e, &[edible], &[])\n     }\n \n-    // Commit to parsing a complete statement `s`, which expects to be\n-    // followed by some token from the set edible + inedible.  Check\n-    // for recoverable input errors, discarding erroneous characters.\n+    /// Commit to parsing a complete statement `s`, which expects to be\n+    /// followed by some token from the set edible + inedible.  Check\n+    /// for recoverable input errors, discarding erroneous characters.\n     pub fn commit_stmt(&mut self, s: Gc<Stmt>, edible: &[token::Token],\n                        inedible: &[token::Token]) {\n         debug!(\"commit_stmt {:?}\", s);\n@@ -526,8 +527,8 @@ impl<'a> Parser<'a> {\n                                               id: ast::DUMMY_NODE_ID })\n     }\n \n-    // consume token 'tok' if it exists. Returns true if the given\n-    // token was present, false otherwise.\n+    /// Consume token 'tok' if it exists. Returns true if the given\n+    /// token was present, false otherwise.\n     pub fn eat(&mut self, tok: &token::Token) -> bool {\n         let is_present = self.token == *tok;\n         if is_present { self.bump() }\n@@ -538,8 +539,8 @@ impl<'a> Parser<'a> {\n         token::is_keyword(kw, &self.token)\n     }\n \n-    // if the next token is the given keyword, eat it and return\n-    // true. Otherwise, return false.\n+    /// If the next token is the given keyword, eat it and return\n+    /// true. Otherwise, return false.\n     pub fn eat_keyword(&mut self, kw: keywords::Keyword) -> bool {\n         match self.token {\n             token::IDENT(sid, false) if kw.to_name() == sid.name => {\n@@ -550,9 +551,9 @@ impl<'a> Parser<'a> {\n         }\n     }\n \n-    // if the given word is not a keyword, signal an error.\n-    // if the next token is not the given word, signal an error.\n-    // otherwise, eat it.\n+    /// If the given word is not a keyword, signal an error.\n+    /// If the next token is not the given word, signal an error.\n+    /// Otherwise, eat it.\n     pub fn expect_keyword(&mut self, kw: keywords::Keyword) {\n         if !self.eat_keyword(kw) {\n             let id_interned_str = token::get_name(kw.to_name());\n@@ -562,7 +563,7 @@ impl<'a> Parser<'a> {\n         }\n     }\n \n-    // signal an error if the given string is a strict keyword\n+    /// Signal an error if the given string is a strict keyword\n     pub fn check_strict_keywords(&mut self) {\n         if token::is_strict_keyword(&self.token) {\n             let token_str = self.this_token_to_string();\n@@ -573,7 +574,7 @@ impl<'a> Parser<'a> {\n         }\n     }\n \n-    // signal an error if the current token is a reserved keyword\n+    /// Signal an error if the current token is a reserved keyword\n     pub fn check_reserved_keywords(&mut self) {\n         if token::is_reserved_keyword(&self.token) {\n             let token_str = self.this_token_to_string();\n@@ -582,8 +583,8 @@ impl<'a> Parser<'a> {\n         }\n     }\n \n-    // Expect and consume an `&`. If `&&` is seen, replace it with a single\n-    // `&` and continue. If an `&` is not seen, signal an error.\n+    /// Expect and consume an `&`. If `&&` is seen, replace it with a single\n+    /// `&` and continue. If an `&` is not seen, signal an error.\n     fn expect_and(&mut self) {\n         match self.token {\n             token::BINOP(token::AND) => self.bump(),\n@@ -603,8 +604,8 @@ impl<'a> Parser<'a> {\n         }\n     }\n \n-    // Expect and consume a `|`. If `||` is seen, replace it with a single\n-    // `|` and continue. If a `|` is not seen, signal an error.\n+    /// Expect and consume a `|`. If `||` is seen, replace it with a single\n+    /// `|` and continue. If a `|` is not seen, signal an error.\n     fn expect_or(&mut self) {\n         match self.token {\n             token::BINOP(token::OR) => self.bump(),\n@@ -624,26 +625,26 @@ impl<'a> Parser<'a> {\n         }\n     }\n \n-    // Attempt to consume a `<`. If `<<` is seen, replace it with a single\n-    // `<` and continue. If a `<` is not seen, return false.\n-    //\n-    // This is meant to be used when parsing generics on a path to get the\n-    // starting token. The `force` parameter is used to forcefully break up a\n-    // `<<` token. If `force` is false, then `<<` is only broken when a lifetime\n-    // shows up next. For example, consider the expression:\n-    //\n-    //      foo as bar << test\n-    //\n-    // The parser needs to know if `bar <<` is the start of a generic path or if\n-    // it's a left-shift token. If `test` were a lifetime, then it's impossible\n-    // for the token to be a left-shift, but if it's not a lifetime, then it's\n-    // considered a left-shift.\n-    //\n-    // The reason for this is that the only current ambiguity with `<<` is when\n-    // parsing closure types:\n-    //\n-    //      foo::<<'a> ||>();\n-    //      impl Foo<<'a> ||>() { ... }\n+    /// Attempt to consume a `<`. If `<<` is seen, replace it with a single\n+    /// `<` and continue. If a `<` is not seen, return false.\n+    ///\n+    /// This is meant to be used when parsing generics on a path to get the\n+    /// starting token. The `force` parameter is used to forcefully break up a\n+    /// `<<` token. If `force` is false, then `<<` is only broken when a lifetime\n+    /// shows up next. For example, consider the expression:\n+    ///\n+    ///      foo as bar << test\n+    ///\n+    /// The parser needs to know if `bar <<` is the start of a generic path or if\n+    /// it's a left-shift token. If `test` were a lifetime, then it's impossible\n+    /// for the token to be a left-shift, but if it's not a lifetime, then it's\n+    /// considered a left-shift.\n+    ///\n+    /// The reason for this is that the only current ambiguity with `<<` is when\n+    /// parsing closure types:\n+    ///\n+    ///      foo::<<'a> ||>();\n+    ///      impl Foo<<'a> ||>() { ... }\n     fn eat_lt(&mut self, force: bool) -> bool {\n         match self.token {\n             token::LT => { self.bump(); true }\n@@ -675,7 +676,7 @@ impl<'a> Parser<'a> {\n         }\n     }\n \n-    // Parse a sequence bracketed by `|` and `|`, stopping before the `|`.\n+    /// Parse a sequence bracketed by `|` and `|`, stopping before the `|`.\n     fn parse_seq_to_before_or<T>(\n                               &mut self,\n                               sep: &token::Token,\n@@ -696,9 +697,9 @@ impl<'a> Parser<'a> {\n         vector\n     }\n \n-    // expect and consume a GT. if a >> is seen, replace it\n-    // with a single > and continue. If a GT is not seen,\n-    // signal an error.\n+    /// Expect and consume a GT. if a >> is seen, replace it\n+    /// with a single > and continue. If a GT is not seen,\n+    /// signal an error.\n     pub fn expect_gt(&mut self) {\n         match self.token {\n             token::GT => self.bump(),\n@@ -727,8 +728,8 @@ impl<'a> Parser<'a> {\n         }\n     }\n \n-    // parse a sequence bracketed by '<' and '>', stopping\n-    // before the '>'.\n+    /// Parse a sequence bracketed by '<' and '>', stopping\n+    /// before the '>'.\n     pub fn parse_seq_to_before_gt<T>(\n                                   &mut self,\n                                   sep: Option<token::Token>,\n@@ -762,9 +763,9 @@ impl<'a> Parser<'a> {\n         return v;\n     }\n \n-    // parse a sequence, including the closing delimiter. The function\n-    // f must consume tokens until reaching the next separator or\n-    // closing bracket.\n+    /// Parse a sequence, including the closing delimiter. The function\n+    /// f must consume tokens until reaching the next separator or\n+    /// closing bracket.\n     pub fn parse_seq_to_end<T>(\n                             &mut self,\n                             ket: &token::Token,\n@@ -776,9 +777,9 @@ impl<'a> Parser<'a> {\n         val\n     }\n \n-    // parse a sequence, not including the closing delimiter. The function\n-    // f must consume tokens until reaching the next separator or\n-    // closing bracket.\n+    /// Parse a sequence, not including the closing delimiter. The function\n+    /// f must consume tokens until reaching the next separator or\n+    /// closing bracket.\n     pub fn parse_seq_to_before_end<T>(\n                                    &mut self,\n                                    ket: &token::Token,\n@@ -801,9 +802,9 @@ impl<'a> Parser<'a> {\n         return v;\n     }\n \n-    // parse a sequence, including the closing delimiter. The function\n-    // f must consume tokens until reaching the next separator or\n-    // closing bracket.\n+    /// Parse a sequence, including the closing delimiter. The function\n+    /// f must consume tokens until reaching the next separator or\n+    /// closing bracket.\n     pub fn parse_unspanned_seq<T>(\n                                &mut self,\n                                bra: &token::Token,\n@@ -817,8 +818,8 @@ impl<'a> Parser<'a> {\n         result\n     }\n \n-    // parse a sequence parameter of enum variant. For consistency purposes,\n-    // these should not be empty.\n+    /// Parse a sequence parameter of enum variant. For consistency purposes,\n+    /// these should not be empty.\n     pub fn parse_enum_variant_seq<T>(\n                                &mut self,\n                                bra: &token::Token,\n@@ -852,7 +853,7 @@ impl<'a> Parser<'a> {\n         spanned(lo, hi, result)\n     }\n \n-    // advance the parser by one token\n+    /// Advance the parser by one token\n     pub fn bump(&mut self) {\n         self.last_span = self.span;\n         // Stash token for error recovery (sometimes; clone is not necessarily cheap).\n@@ -880,14 +881,14 @@ impl<'a> Parser<'a> {\n         self.tokens_consumed += 1u;\n     }\n \n-    // Advance the parser by one token and return the bumped token.\n+    /// Advance the parser by one token and return the bumped token.\n     pub fn bump_and_get(&mut self) -> token::Token {\n         let old_token = replace(&mut self.token, token::UNDERSCORE);\n         self.bump();\n         old_token\n     }\n \n-    // EFFECT: replace the current token and span with the given one\n+    /// EFFECT: replace the current token and span with the given one\n     pub fn replace_token(&mut self,\n                          next: token::Token,\n                          lo: BytePos,\n@@ -940,8 +941,8 @@ impl<'a> Parser<'a> {\n         token::get_ident(id)\n     }\n \n-    // Is the current token one of the keywords that signals a bare function\n-    // type?\n+    /// Is the current token one of the keywords that signals a bare function\n+    /// type?\n     pub fn token_is_bare_fn_keyword(&mut self) -> bool {\n         if token::is_keyword(keywords::Fn, &self.token) {\n             return true\n@@ -955,14 +956,14 @@ impl<'a> Parser<'a> {\n         false\n     }\n \n-    // Is the current token one of the keywords that signals a closure type?\n+    /// Is the current token one of the keywords that signals a closure type?\n     pub fn token_is_closure_keyword(&mut self) -> bool {\n         token::is_keyword(keywords::Unsafe, &self.token) ||\n             token::is_keyword(keywords::Once, &self.token)\n     }\n \n-    // Is the current token one of the keywords that signals an old-style\n-    // closure type (with explicit sigil)?\n+    /// Is the current token one of the keywords that signals an old-style\n+    /// closure type (with explicit sigil)?\n     pub fn token_is_old_style_closure_keyword(&mut self) -> bool {\n         token::is_keyword(keywords::Unsafe, &self.token) ||\n             token::is_keyword(keywords::Once, &self.token) ||\n@@ -983,7 +984,7 @@ impl<'a> Parser<'a> {\n         }\n     }\n \n-    // parse a TyBareFn type:\n+    /// parse a TyBareFn type:\n     pub fn parse_ty_bare_fn(&mut self) -> Ty_ {\n         /*\n \n@@ -1014,8 +1015,8 @@ impl<'a> Parser<'a> {\n         });\n     }\n \n-    // Parses a procedure type (`proc`). The initial `proc` keyword must\n-    // already have been parsed.\n+    /// Parses a procedure type (`proc`). The initial `proc` keyword must\n+    /// already have been parsed.\n     pub fn parse_proc_type(&mut self) -> Ty_ {\n         /*\n \n@@ -1063,7 +1064,7 @@ impl<'a> Parser<'a> {\n         })\n     }\n \n-    // parse a TyClosure type\n+    /// Parse a TyClosure type\n     pub fn parse_ty_closure(&mut self) -> Ty_ {\n         /*\n \n@@ -1154,7 +1155,7 @@ impl<'a> Parser<'a> {\n         }\n     }\n \n-    // parse a function type (following the 'fn')\n+    /// Parse a function type (following the 'fn')\n     pub fn parse_ty_fn_decl(&mut self, allow_variadic: bool)\n                             -> (P<FnDecl>, Vec<ast::Lifetime>) {\n         /*\n@@ -1186,7 +1187,7 @@ impl<'a> Parser<'a> {\n         (decl, lifetimes)\n     }\n \n-    // parse the methods in a trait declaration\n+    /// Parse the methods in a trait declaration\n     pub fn parse_trait_methods(&mut self) -> Vec<TraitMethod> {\n         self.parse_unspanned_seq(\n             &token::LBRACE,\n@@ -1255,15 +1256,15 @@ impl<'a> Parser<'a> {\n         })\n     }\n \n-    // parse a possibly mutable type\n+    /// Parse a possibly mutable type\n     pub fn parse_mt(&mut self) -> MutTy {\n         let mutbl = self.parse_mutability();\n         let t = self.parse_ty(true);\n         MutTy { ty: t, mutbl: mutbl }\n     }\n \n-    // parse [mut/const/imm] ID : TY\n-    // now used only by obsolete record syntax parser...\n+    /// Parse [mut/const/imm] ID : TY\n+    /// now used only by obsolete record syntax parser...\n     pub fn parse_ty_field(&mut self) -> TypeField {\n         let lo = self.span.lo;\n         let mutbl = self.parse_mutability();\n@@ -1278,7 +1279,7 @@ impl<'a> Parser<'a> {\n         }\n     }\n \n-    // parse optional return type [ -> TY ] in function decl\n+    /// Parse optional return type [ -> TY ] in function decl\n     pub fn parse_ret_ty(&mut self) -> (RetStyle, P<Ty>) {\n         return if self.eat(&token::RARROW) {\n             let lo = self.span.lo;\n@@ -1478,8 +1479,8 @@ impl<'a> Parser<'a> {\n         }\n     }\n \n-    // This version of parse arg doesn't necessarily require\n-    // identifier names.\n+    /// This version of parse arg doesn't necessarily require\n+    /// identifier names.\n     pub fn parse_arg_general(&mut self, require_name: bool) -> Arg {\n         let pat = if require_name || self.is_named_argument() {\n             debug!(\"parse_arg_general parse_pat (require_name:{:?})\",\n@@ -1504,12 +1505,12 @@ impl<'a> Parser<'a> {\n         }\n     }\n \n-    // parse a single function argument\n+    /// Parse a single function argument\n     pub fn parse_arg(&mut self) -> Arg {\n         self.parse_arg_general(true)\n     }\n \n-    // parse an argument in a lambda header e.g. |arg, arg|\n+    /// Parse an argument in a lambda header e.g. |arg, arg|\n     pub fn parse_fn_block_arg(&mut self) -> Arg {\n         let pat = self.parse_pat();\n         let t = if self.eat(&token::COLON) {\n@@ -1539,7 +1540,7 @@ impl<'a> Parser<'a> {\n         }\n     }\n \n-    // matches token_lit = LIT_INT | ...\n+    /// Matches token_lit = LIT_INT | ...\n     pub fn lit_from_token(&mut self, tok: &token::Token) -> Lit_ {\n         match *tok {\n             token::LIT_BYTE(i) => LitByte(i),\n@@ -1566,7 +1567,7 @@ impl<'a> Parser<'a> {\n         }\n     }\n \n-    // matches lit = true | false | token_lit\n+    /// Matches lit = true | false | token_lit\n     pub fn parse_lit(&mut self) -> Lit {\n         let lo = self.span.lo;\n         let lit = if self.eat_keyword(keywords::True) {\n@@ -1581,7 +1582,7 @@ impl<'a> Parser<'a> {\n         codemap::Spanned { node: lit, span: mk_sp(lo, self.last_span.hi) }\n     }\n \n-    // matches '-' lit | lit\n+    /// matches '-' lit | lit\n     pub fn parse_literal_maybe_minus(&mut self) -> Gc<Expr> {\n         let minus_lo = self.span.lo;\n         let minus_present = self.eat(&token::BINOP(token::MINUS));\n@@ -1719,7 +1720,7 @@ impl<'a> Parser<'a> {\n     }\n \n     /// Parses a single lifetime\n-    // matches lifetime = LIFETIME\n+    /// Matches lifetime = LIFETIME\n     pub fn parse_lifetime(&mut self) -> ast::Lifetime {\n         match self.token {\n             token::LIFETIME(i) => {\n@@ -1779,7 +1780,7 @@ impl<'a> Parser<'a> {\n         token::is_keyword(keywords::Const, tok)\n     }\n \n-    // parse mutability declaration (mut/const/imm)\n+    /// Parse mutability declaration (mut/const/imm)\n     pub fn parse_mutability(&mut self) -> Mutability {\n         if self.eat_keyword(keywords::Mut) {\n             MutMutable\n@@ -1788,7 +1789,7 @@ impl<'a> Parser<'a> {\n         }\n     }\n \n-    // parse ident COLON expr\n+    /// Parse ident COLON expr\n     pub fn parse_field(&mut self) -> Field {\n         let lo = self.span.lo;\n         let i = self.parse_ident();\n@@ -1867,9 +1868,9 @@ impl<'a> Parser<'a> {\n         }\n     }\n \n-    // at the bottom (top?) of the precedence hierarchy,\n-    // parse things like parenthesized exprs,\n-    // macros, return, etc.\n+    /// At the bottom (top?) of the precedence hierarchy,\n+    /// parse things like parenthesized exprs,\n+    /// macros, return, etc.\n     pub fn parse_bottom_expr(&mut self) -> Gc<Expr> {\n         maybe_whole_expr!(self);\n \n@@ -2107,15 +2108,15 @@ impl<'a> Parser<'a> {\n         return self.mk_expr(lo, hi, ex);\n     }\n \n-    // parse a block or unsafe block\n+    /// Parse a block or unsafe block\n     pub fn parse_block_expr(&mut self, lo: BytePos, blk_mode: BlockCheckMode)\n                             -> Gc<Expr> {\n         self.expect(&token::LBRACE);\n         let blk = self.parse_block_tail(lo, blk_mode);\n         return self.mk_expr(blk.span.lo, blk.span.hi, ExprBlock(blk));\n     }\n \n-    // parse a.b or a(13) or a[4] or just a\n+    /// parse a.b or a(13) or a[4] or just a\n     pub fn parse_dot_or_call_expr(&mut self) -> Gc<Expr> {\n         let b = self.parse_bottom_expr();\n         self.parse_dot_or_call_expr_with(b)\n@@ -2199,8 +2200,8 @@ impl<'a> Parser<'a> {\n         return e;\n     }\n \n-    // parse an optional separator followed by a kleene-style\n-    // repetition token (+ or *).\n+    /// Parse an optional separator followed by a kleene-style\n+    /// repetition token (+ or *).\n     pub fn parse_sep_and_zerok(&mut self) -> (Option<token::Token>, bool) {\n         fn parse_zerok(parser: &mut Parser) -> Option<bool> {\n             match parser.token {\n@@ -2225,7 +2226,7 @@ impl<'a> Parser<'a> {\n         }\n     }\n \n-    // parse a single token tree from the input.\n+    /// parse a single token tree from the input.\n     pub fn parse_token_tree(&mut self) -> TokenTree {\n         // FIXME #6994: currently, this is too eager. It\n         // parses token trees but also identifies TTSeq's\n@@ -2341,9 +2342,9 @@ impl<'a> Parser<'a> {\n         }\n     }\n \n-    // This goofy function is necessary to correctly match parens in Matcher's.\n-    // Otherwise, `$( ( )` would be a valid Matcher, and `$( () )` would be\n-    // invalid. It's similar to common::parse_seq.\n+    /// This goofy function is necessary to correctly match parens in Matcher's.\n+    /// Otherwise, `$( ( )` would be a valid Matcher, and `$( () )` would be\n+    /// invalid. It's similar to common::parse_seq.\n     pub fn parse_matcher_subseq_upto(&mut self,\n                                      name_idx: &mut uint,\n                                      ket: &token::Token)\n@@ -2392,7 +2393,7 @@ impl<'a> Parser<'a> {\n         return spanned(lo, self.span.hi, m);\n     }\n \n-    // parse a prefix-operator expr\n+    /// Parse a prefix-operator expr\n     pub fn parse_prefix_expr(&mut self) -> Gc<Expr> {\n         let lo = self.span.lo;\n         let hi;\n@@ -2500,13 +2501,13 @@ impl<'a> Parser<'a> {\n         return self.mk_expr(lo, hi, ex);\n     }\n \n-    // parse an expression of binops\n+    /// Parse an expression of binops\n     pub fn parse_binops(&mut self) -> Gc<Expr> {\n         let prefix_expr = self.parse_prefix_expr();\n         self.parse_more_binops(prefix_expr, 0)\n     }\n \n-    // parse an expression of binops of at least min_prec precedence\n+    /// Parse an expression of binops of at least min_prec precedence\n     pub fn parse_more_binops(&mut self, lhs: Gc<Expr>,\n                              min_prec: uint) -> Gc<Expr> {\n         if self.expr_is_complete(lhs) { return lhs; }\n@@ -2554,9 +2555,9 @@ impl<'a> Parser<'a> {\n         }\n     }\n \n-    // parse an assignment expression....\n-    // actually, this seems to be the main entry point for\n-    // parsing an arbitrary expression.\n+    /// Parse an assignment expression....\n+    /// actually, this seems to be the main entry point for\n+    /// parsing an arbitrary expression.\n     pub fn parse_assign_expr(&mut self) -> Gc<Expr> {\n         let lo = self.span.lo;\n         let lhs = self.parse_binops();\n@@ -2590,7 +2591,7 @@ impl<'a> Parser<'a> {\n         }\n     }\n \n-    // parse an 'if' expression ('if' token already eaten)\n+    /// Parse an 'if' expression ('if' token already eaten)\n     pub fn parse_if_expr(&mut self) -> Gc<Expr> {\n         let lo = self.last_span.lo;\n         let cond = self.parse_expr_res(RESTRICT_NO_STRUCT_LITERAL);\n@@ -2605,7 +2606,7 @@ impl<'a> Parser<'a> {\n         self.mk_expr(lo, hi, ExprIf(cond, thn, els))\n     }\n \n-    // `|args| { ... }` or `{ ...}` like in `do` expressions\n+    /// `|args| { ... }` or `{ ...}` like in `do` expressions\n     pub fn parse_lambda_block_expr(&mut self) -> Gc<Expr> {\n         self.parse_lambda_expr_(\n             |p| {\n@@ -2634,15 +2635,15 @@ impl<'a> Parser<'a> {\n             })\n     }\n \n-    // `|args| expr`\n+    /// `|args| expr`\n     pub fn parse_lambda_expr(&mut self) -> Gc<Expr> {\n         self.parse_lambda_expr_(|p| p.parse_fn_block_decl(),\n                                 |p| p.parse_expr())\n     }\n \n-    // parse something of the form |args| expr\n-    // this is used both in parsing a lambda expr\n-    // and in parsing a block expr as e.g. in for...\n+    /// parse something of the form |args| expr\n+    /// this is used both in parsing a lambda expr\n+    /// and in parsing a block expr as e.g. in for...\n     pub fn parse_lambda_expr_(&mut self,\n                               parse_decl: |&mut Parser| -> P<FnDecl>,\n                               parse_body: |&mut Parser| -> Gc<Expr>)\n@@ -2671,7 +2672,7 @@ impl<'a> Parser<'a> {\n         }\n     }\n \n-    // parse a 'for' .. 'in' expression ('for' token already eaten)\n+    /// Parse a 'for' .. 'in' expression ('for' token already eaten)\n     pub fn parse_for_expr(&mut self, opt_ident: Option<ast::Ident>) -> Gc<Expr> {\n         // Parse: `for <src_pat> in <src_expr> <src_loop_block>`\n \n@@ -2737,12 +2738,12 @@ impl<'a> Parser<'a> {\n         return self.mk_expr(lo, hi, ExprMatch(discriminant, arms));\n     }\n \n-    // parse an expression\n+    /// Parse an expression\n     pub fn parse_expr(&mut self) -> Gc<Expr> {\n         return self.parse_expr_res(UNRESTRICTED);\n     }\n \n-    // parse an expression, subject to the given restriction\n+    /// Parse an expression, subject to the given restriction\n     pub fn parse_expr_res(&mut self, r: restriction) -> Gc<Expr> {\n         let old = self.restriction;\n         self.restriction = r;\n@@ -2751,7 +2752,7 @@ impl<'a> Parser<'a> {\n         return e;\n     }\n \n-    // parse the RHS of a local variable declaration (e.g. '= 14;')\n+    /// Parse the RHS of a local variable declaration (e.g. '= 14;')\n     fn parse_initializer(&mut self) -> Option<Gc<Expr>> {\n         if self.token == token::EQ {\n             self.bump();\n@@ -2761,7 +2762,7 @@ impl<'a> Parser<'a> {\n         }\n     }\n \n-    // parse patterns, separated by '|' s\n+    /// Parse patterns, separated by '|' s\n     fn parse_pats(&mut self) -> Vec<Gc<Pat>> {\n         let mut pats = Vec::new();\n         loop {\n@@ -2824,7 +2825,7 @@ impl<'a> Parser<'a> {\n         (before, slice, after)\n     }\n \n-    // parse the fields of a struct-like pattern\n+    /// Parse the fields of a struct-like pattern\n     fn parse_pat_fields(&mut self) -> (Vec<ast::FieldPat> , bool) {\n         let mut fields = Vec::new();\n         let mut etc = false;\n@@ -2884,7 +2885,7 @@ impl<'a> Parser<'a> {\n         return (fields, etc);\n     }\n \n-    // parse a pattern.\n+    /// Parse a pattern.\n     pub fn parse_pat(&mut self) -> Gc<Pat> {\n         maybe_whole!(self, NtPat);\n \n@@ -3126,9 +3127,9 @@ impl<'a> Parser<'a> {\n         }\n     }\n \n-    // parse ident or ident @ pat\n-    // used by the copy foo and ref foo patterns to give a good\n-    // error message when parsing mistakes like ref foo(a,b)\n+    /// Parse ident or ident @ pat\n+    /// used by the copy foo and ref foo patterns to give a good\n+    /// error message when parsing mistakes like ref foo(a,b)\n     fn parse_pat_ident(&mut self,\n                        binding_mode: ast::BindingMode)\n                        -> ast::Pat_ {\n@@ -3162,7 +3163,7 @@ impl<'a> Parser<'a> {\n         PatIdent(binding_mode, name, sub)\n     }\n \n-    // parse a local variable declaration\n+    /// Parse a local variable declaration\n     fn parse_local(&mut self) -> Gc<Local> {\n         let lo = self.span.lo;\n         let pat = self.parse_pat();\n@@ -3186,14 +3187,14 @@ impl<'a> Parser<'a> {\n         }\n     }\n \n-    // parse a \"let\" stmt\n+    /// Parse a \"let\" stmt\n     fn parse_let(&mut self) -> Gc<Decl> {\n         let lo = self.span.lo;\n         let local = self.parse_local();\n         box(GC) spanned(lo, self.last_span.hi, DeclLocal(local))\n     }\n \n-    // parse a structure field\n+    /// Parse a structure field\n     fn parse_name_and_ty(&mut self, pr: Visibility,\n                          attrs: Vec<Attribute> ) -> StructField {\n         let lo = self.span.lo;\n@@ -3211,8 +3212,8 @@ impl<'a> Parser<'a> {\n         })\n     }\n \n-    // parse a statement. may include decl.\n-    // precondition: any attributes are parsed already\n+    /// Parse a statement. may include decl.\n+    /// Precondition: any attributes are parsed already\n     pub fn parse_stmt(&mut self, item_attrs: Vec<Attribute>) -> Gc<Stmt> {\n         maybe_whole!(self, NtStmt);\n \n@@ -3315,13 +3316,13 @@ impl<'a> Parser<'a> {\n         }\n     }\n \n-    // is this expression a successfully-parsed statement?\n+    /// Is this expression a successfully-parsed statement?\n     fn expr_is_complete(&mut self, e: Gc<Expr>) -> bool {\n         return self.restriction == RESTRICT_STMT_EXPR &&\n             !classify::expr_requires_semi_to_be_stmt(e);\n     }\n \n-    // parse a block. No inner attrs are allowed.\n+    /// Parse a block. No inner attrs are allowed.\n     pub fn parse_block(&mut self) -> P<Block> {\n         maybe_whole!(no_clone self, NtBlock);\n \n@@ -3331,7 +3332,7 @@ impl<'a> Parser<'a> {\n         return self.parse_block_tail_(lo, DefaultBlock, Vec::new());\n     }\n \n-    // parse a block. Inner attrs are allowed.\n+    /// Parse a block. Inner attrs are allowed.\n     fn parse_inner_attrs_and_block(&mut self)\n         -> (Vec<Attribute> , P<Block>) {\n \n@@ -3344,15 +3345,15 @@ impl<'a> Parser<'a> {\n         (inner, self.parse_block_tail_(lo, DefaultBlock, next))\n     }\n \n-    // Precondition: already parsed the '{' or '#{'\n-    // I guess that also means \"already parsed the 'impure'\" if\n-    // necessary, and this should take a qualifier.\n-    // some blocks start with \"#{\"...\n+    /// Precondition: already parsed the '{' or '#{'\n+    /// I guess that also means \"already parsed the 'impure'\" if\n+    /// necessary, and this should take a qualifier.\n+    /// Some blocks start with \"#{\"...\n     fn parse_block_tail(&mut self, lo: BytePos, s: BlockCheckMode) -> P<Block> {\n         self.parse_block_tail_(lo, s, Vec::new())\n     }\n \n-    // parse the rest of a block expression or function body\n+    /// Parse the rest of a block expression or function body\n     fn parse_block_tail_(&mut self, lo: BytePos, s: BlockCheckMode,\n                          first_item_attrs: Vec<Attribute> ) -> P<Block> {\n         let mut stmts = Vec::new();\n@@ -3510,18 +3511,18 @@ impl<'a> Parser<'a> {\n         }\n     }\n \n-    // matches bounds    = ( boundseq )?\n-    // where   boundseq  = ( bound + boundseq ) | bound\n-    // and     bound     = 'static | ty\n-    // Returns \"None\" if there's no colon (e.g. \"T\");\n-    // Returns \"Some(Empty)\" if there's a colon but nothing after (e.g. \"T:\")\n-    // Returns \"Some(stuff)\" otherwise (e.g. \"T:stuff\").\n-    // NB: The None/Some distinction is important for issue #7264.\n-    //\n-    // Note that the `allow_any_lifetime` argument is a hack for now while the\n-    // AST doesn't support arbitrary lifetimes in bounds on type parameters. In\n-    // the future, this flag should be removed, and the return value of this\n-    // function should be Option<~[TyParamBound]>\n+    /// matches optbounds = ( ( : ( boundseq )? )? )\n+    /// where   boundseq  = ( bound + boundseq ) | bound\n+    /// and     bound     = 'static | ty\n+    /// Returns \"None\" if there's no colon (e.g. \"T\");\n+    /// Returns \"Some(Empty)\" if there's a colon but nothing after (e.g. \"T:\")\n+    /// Returns \"Some(stuff)\" otherwise (e.g. \"T:stuff\").\n+    /// NB: The None/Some distinction is important for issue #7264.\n+    ///\n+    /// Note that the `allow_any_lifetime` argument is a hack for now while the\n+    /// AST doesn't support arbitrary lifetimes in bounds on type parameters. In\n+    /// the future, this flag should be removed, and the return value of this\n+    /// function should be Option<~[TyParamBound]>\n     fn parse_ty_param_bounds(&mut self, allow_any_lifetime: bool)\n                              -> (Option<ast::Lifetime>,\n                                  OwnedSlice<TyParamBound>) {\n@@ -3588,7 +3589,7 @@ impl<'a> Parser<'a> {\n         }\n     }\n \n-    // matches typaram = (unbound`?`)? IDENT optbounds ( EQ ty )?\n+    /// Matches typaram = (unbound`?`)? IDENT optbounds ( EQ ty )?\n     fn parse_ty_param(&mut self) -> TyParam {\n         // This is a bit hacky. Currently we are only interested in a single\n         // unbound, and it may only be `Sized`. To avoid backtracking and other\n@@ -3632,10 +3633,10 @@ impl<'a> Parser<'a> {\n         }\n     }\n \n-    // parse a set of optional generic type parameter declarations\n-    // matches generics = ( ) | ( < > ) | ( < typaramseq ( , )? > ) | ( < lifetimes ( , )? > )\n-    //                  | ( < lifetimes , typaramseq ( , )? > )\n-    // where   typaramseq = ( typaram ) | ( typaram , typaramseq )\n+    /// Parse a set of optional generic type parameter declarations\n+    /// matches generics = ( ) | ( < > ) | ( < typaramseq ( , )? > ) | ( < lifetimes ( , )? > )\n+    ///                  | ( < lifetimes , typaramseq ( , )? > )\n+    /// where   typaramseq = ( typaram ) | ( typaram , typaramseq )\n     pub fn parse_generics(&mut self) -> ast::Generics {\n         if self.eat(&token::LT) {\n             let lifetimes = self.parse_lifetimes();\n@@ -3727,7 +3728,7 @@ impl<'a> Parser<'a> {\n         (args, variadic)\n     }\n \n-    // parse the argument list and result type of a function declaration\n+    /// Parse the argument list and result type of a function declaration\n     pub fn parse_fn_decl(&mut self, allow_variadic: bool) -> P<FnDecl> {\n \n         let (args, variadic) = self.parse_fn_args(true, allow_variadic);\n@@ -3762,8 +3763,8 @@ impl<'a> Parser<'a> {\n         }\n     }\n \n-    // parse the argument list and result type of a function\n-    // that may have a self type.\n+    /// Parse the argument list and result type of a function\n+    /// that may have a self type.\n     fn parse_fn_decl_with_self(&mut self, parse_arg_fn: |&mut Parser| -> Arg)\n                                -> (ExplicitSelf, P<FnDecl>) {\n         fn maybe_parse_borrowed_explicit_self(this: &mut Parser)\n@@ -3921,7 +3922,7 @@ impl<'a> Parser<'a> {\n         (spanned(lo, hi, explicit_self), fn_decl)\n     }\n \n-    // parse the |arg, arg| header on a lambda\n+    /// Parse the |arg, arg| header on a lambda\n     fn parse_fn_block_decl(&mut self) -> P<FnDecl> {\n         let inputs_captures = {\n             if self.eat(&token::OROR) {\n@@ -3953,7 +3954,7 @@ impl<'a> Parser<'a> {\n         })\n     }\n \n-    // Parses the `(arg, arg) -> return_type` header on a procedure.\n+    /// Parses the `(arg, arg) -> return_type` header on a procedure.\n     fn parse_proc_decl(&mut self) -> P<FnDecl> {\n         let inputs =\n             self.parse_unspanned_seq(&token::LPAREN,\n@@ -3979,7 +3980,7 @@ impl<'a> Parser<'a> {\n         })\n     }\n \n-    // parse the name and optional generic types of a function header.\n+    /// Parse the name and optional generic types of a function header.\n     fn parse_fn_header(&mut self) -> (Ident, ast::Generics) {\n         let id = self.parse_ident();\n         let generics = self.parse_generics();\n@@ -3999,15 +4000,15 @@ impl<'a> Parser<'a> {\n         }\n     }\n \n-    // parse an item-position function declaration.\n+    /// Parse an item-position function declaration.\n     fn parse_item_fn(&mut self, fn_style: FnStyle, abi: abi::Abi) -> ItemInfo {\n         let (ident, generics) = self.parse_fn_header();\n         let decl = self.parse_fn_decl(false);\n         let (inner_attrs, body) = self.parse_inner_attrs_and_block();\n         (ident, ItemFn(decl, fn_style, abi, generics, body), Some(inner_attrs))\n     }\n \n-    // parse a method in a trait impl, starting with `attrs` attributes.\n+    /// Parse a method in a trait impl, starting with `attrs` attributes.\n     fn parse_method(&mut self,\n                     already_parsed_attrs: Option<Vec<Attribute>>) -> Gc<Method> {\n         let next_attrs = self.parse_outer_attributes();\n@@ -4043,7 +4044,7 @@ impl<'a> Parser<'a> {\n         }\n     }\n \n-    // parse trait Foo { ... }\n+    /// Parse trait Foo { ... }\n     fn parse_item_trait(&mut self) -> ItemInfo {\n         let ident = self.parse_ident();\n         let tps = self.parse_generics();\n@@ -4062,9 +4063,9 @@ impl<'a> Parser<'a> {\n         (ident, ItemTrait(tps, sized, traits, meths), None)\n     }\n \n-    // Parses two variants (with the region/type params always optional):\n-    //    impl<T> Foo { ... }\n-    //    impl<T> ToString for ~[T] { ... }\n+    /// Parses two variants (with the region/type params always optional):\n+    ///    impl<T> Foo { ... }\n+    ///    impl<T> ToString for ~[T] { ... }\n     fn parse_item_impl(&mut self) -> ItemInfo {\n         // First, parse type parameters if necessary.\n         let generics = self.parse_generics();\n@@ -4117,15 +4118,15 @@ impl<'a> Parser<'a> {\n         (ident, ItemImpl(generics, opt_trait, ty, meths), Some(inner_attrs))\n     }\n \n-    // parse a::B<String,int>\n+    /// Parse a::B<String,int>\n     fn parse_trait_ref(&mut self) -> TraitRef {\n         ast::TraitRef {\n             path: self.parse_path(LifetimeAndTypesWithoutColons).path,\n             ref_id: ast::DUMMY_NODE_ID,\n         }\n     }\n \n-    // parse B + C<String,int> + D\n+    /// Parse B + C<String,int> + D\n     fn parse_trait_ref_list(&mut self, ket: &token::Token) -> Vec<TraitRef> {\n         self.parse_seq_to_before_end(\n             ket,\n@@ -4134,7 +4135,7 @@ impl<'a> Parser<'a> {\n         )\n     }\n \n-    // parse struct Foo { ... }\n+    /// Parse struct Foo { ... }\n     fn parse_item_struct(&mut self, is_virtual: bool) -> ItemInfo {\n         let class_name = self.parse_ident();\n         let generics = self.parse_generics();\n@@ -4217,7 +4218,7 @@ impl<'a> Parser<'a> {\n          None)\n     }\n \n-    // parse a structure field declaration\n+    /// Parse a structure field declaration\n     pub fn parse_single_struct_field(&mut self,\n                                      vis: Visibility,\n                                      attrs: Vec<Attribute> )\n@@ -4239,7 +4240,7 @@ impl<'a> Parser<'a> {\n         a_var\n     }\n \n-    // parse an element of a struct definition\n+    /// Parse an element of a struct definition\n     fn parse_struct_decl_field(&mut self) -> StructField {\n \n         let attrs = self.parse_outer_attributes();\n@@ -4251,7 +4252,7 @@ impl<'a> Parser<'a> {\n         return self.parse_single_struct_field(Inherited, attrs);\n     }\n \n-    // parse visiility: PUB, PRIV, or nothing\n+    /// Parse visiility: PUB, PRIV, or nothing\n     fn parse_visibility(&mut self) -> Visibility {\n         if self.eat_keyword(keywords::Pub) { Public }\n         else { Inherited }\n@@ -4273,8 +4274,8 @@ impl<'a> Parser<'a> {\n         }\n     }\n \n-    // given a termination token and a vector of already-parsed\n-    // attributes (of length 0 or 1), parse all of the items in a module\n+    /// Given a termination token and a vector of already-parsed\n+    /// attributes (of length 0 or 1), parse all of the items in a module\n     fn parse_mod_items(&mut self,\n                        term: token::Token,\n                        first_item_attrs: Vec<Attribute>,\n@@ -4342,7 +4343,7 @@ impl<'a> Parser<'a> {\n         (id, ItemStatic(ty, m, e), None)\n     }\n \n-    // parse a `mod <foo> { ... }` or `mod <foo>;` item\n+    /// Parse a `mod <foo> { ... }` or `mod <foo>;` item\n     fn parse_item_mod(&mut self, outer_attrs: &[Attribute]) -> ItemInfo {\n         let id_span = self.span;\n         let id = self.parse_ident();\n@@ -4380,7 +4381,7 @@ impl<'a> Parser<'a> {\n         self.mod_path_stack.pop().unwrap();\n     }\n \n-    // read a module from a source file.\n+    /// Read a module from a source file.\n     fn eval_src_mod(&mut self,\n                     id: ast::Ident,\n                     outer_attrs: &[ast::Attribute],\n@@ -4488,7 +4489,7 @@ impl<'a> Parser<'a> {\n         return (ast::ItemMod(m0), mod_attrs);\n     }\n \n-    // parse a function declaration from a foreign module\n+    /// Parse a function declaration from a foreign module\n     fn parse_item_foreign_fn(&mut self, vis: ast::Visibility,\n                              attrs: Vec<Attribute>) -> Gc<ForeignItem> {\n         let lo = self.span.lo;\n@@ -4506,7 +4507,7 @@ impl<'a> Parser<'a> {\n                                    vis: vis }\n     }\n \n-    // parse a static item from a foreign module\n+    /// Parse a static item from a foreign module\n     fn parse_item_foreign_static(&mut self, vis: ast::Visibility,\n                                  attrs: Vec<Attribute> ) -> Gc<ForeignItem> {\n         let lo = self.span.lo;\n@@ -4529,7 +4530,7 @@ impl<'a> Parser<'a> {\n         }\n     }\n \n-    // parse safe/unsafe and fn\n+    /// Parse safe/unsafe and fn\n     fn parse_fn_style(&mut self) -> FnStyle {\n         if self.eat_keyword(keywords::Fn) { NormalFn }\n         else if self.eat_keyword(keywords::Unsafe) {\n@@ -4540,8 +4541,8 @@ impl<'a> Parser<'a> {\n     }\n \n \n-    // at this point, this is essentially a wrapper for\n-    // parse_foreign_items.\n+    /// At this point, this is essentially a wrapper for\n+    /// parse_foreign_items.\n     fn parse_foreign_mod_items(&mut self,\n                                abi: abi::Abi,\n                                first_item_attrs: Vec<Attribute> )\n@@ -4642,7 +4643,7 @@ impl<'a> Parser<'a> {\n         return IoviItem(item);\n     }\n \n-    // parse type Foo = Bar;\n+    /// Parse type Foo = Bar;\n     fn parse_item_type(&mut self) -> ItemInfo {\n         let ident = self.parse_ident();\n         let tps = self.parse_generics();\n@@ -4652,8 +4653,8 @@ impl<'a> Parser<'a> {\n         (ident, ItemTy(ty, tps), None)\n     }\n \n-    // parse a structure-like enum variant definition\n-    // this should probably be renamed or refactored...\n+    /// Parse a structure-like enum variant definition\n+    /// this should probably be renamed or refactored...\n     fn parse_struct_def(&mut self) -> Gc<StructDef> {\n         let mut fields: Vec<StructField> = Vec::new();\n         while self.token != token::RBRACE {\n@@ -4669,7 +4670,7 @@ impl<'a> Parser<'a> {\n         };\n     }\n \n-    // parse the part of an \"enum\" decl following the '{'\n+    /// Parse the part of an \"enum\" decl following the '{'\n     fn parse_enum_def(&mut self, _generics: &ast::Generics) -> EnumDef {\n         let mut variants = Vec::new();\n         let mut all_nullary = true;\n@@ -4733,7 +4734,7 @@ impl<'a> Parser<'a> {\n         ast::EnumDef { variants: variants }\n     }\n \n-    // parse an \"enum\" declaration\n+    /// Parse an \"enum\" declaration\n     fn parse_item_enum(&mut self) -> ItemInfo {\n         let id = self.parse_ident();\n         let generics = self.parse_generics();\n@@ -4750,8 +4751,8 @@ impl<'a> Parser<'a> {\n         }\n     }\n \n-    // Parses a string as an ABI spec on an extern type or module. Consumes\n-    // the `extern` keyword, if one is found.\n+    /// Parses a string as an ABI spec on an extern type or module. Consumes\n+    /// the `extern` keyword, if one is found.\n     fn parse_opt_abi(&mut self) -> Option<abi::Abi> {\n         match self.token {\n             token::LIT_STR(s) | token::LIT_STR_RAW(s, _) => {\n@@ -4777,10 +4778,10 @@ impl<'a> Parser<'a> {\n         }\n     }\n \n-    // parse one of the items or view items allowed by the\n-    // flags; on failure, return IoviNone.\n-    // NB: this function no longer parses the items inside an\n-    // extern crate.\n+    /// Parse one of the items or view items allowed by the\n+    /// flags; on failure, return IoviNone.\n+    /// NB: this function no longer parses the items inside an\n+    /// extern crate.\n     fn parse_item_or_view_item(&mut self,\n                                attrs: Vec<Attribute> ,\n                                macros_allowed: bool)\n@@ -4988,7 +4989,7 @@ impl<'a> Parser<'a> {\n         self.parse_macro_use_or_failure(attrs,macros_allowed,lo,visibility)\n     }\n \n-    // parse a foreign item; on failure, return IoviNone.\n+    /// Parse a foreign item; on failure, return IoviNone.\n     fn parse_foreign_item(&mut self,\n                           attrs: Vec<Attribute> ,\n                           macros_allowed: bool)\n@@ -5011,7 +5012,7 @@ impl<'a> Parser<'a> {\n         self.parse_macro_use_or_failure(attrs,macros_allowed,lo,visibility)\n     }\n \n-    // this is the fall-through for parsing items.\n+    /// This is the fall-through for parsing items.\n     fn parse_macro_use_or_failure(\n         &mut self,\n         attrs: Vec<Attribute> ,\n@@ -5095,17 +5096,17 @@ impl<'a> Parser<'a> {\n         }\n     }\n \n-    // parse, e.g., \"use a::b::{z,y}\"\n+    /// Parse, e.g., \"use a::b::{z,y}\"\n     fn parse_use(&mut self) -> ViewItem_ {\n         return ViewItemUse(self.parse_view_path());\n     }\n \n \n-    // matches view_path : MOD? IDENT EQ non_global_path\n-    // | MOD? non_global_path MOD_SEP LBRACE RBRACE\n-    // | MOD? non_global_path MOD_SEP LBRACE ident_seq RBRACE\n-    // | MOD? non_global_path MOD_SEP STAR\n-    // | MOD? non_global_path\n+    /// Matches view_path : MOD? IDENT EQ non_global_path\n+    /// | MOD? non_global_path MOD_SEP LBRACE RBRACE\n+    /// | MOD? non_global_path MOD_SEP LBRACE ident_seq RBRACE\n+    /// | MOD? non_global_path MOD_SEP STAR\n+    /// | MOD? non_global_path\n     fn parse_view_path(&mut self) -> Gc<ViewPath> {\n         let lo = self.span.lo;\n \n@@ -5228,10 +5229,10 @@ impl<'a> Parser<'a> {\n                         ViewPathSimple(last, path, ast::DUMMY_NODE_ID));\n     }\n \n-    // Parses a sequence of items. Stops when it finds program\n-    // text that can't be parsed as an item\n-    // - mod_items uses extern_mod_allowed = true\n-    // - block_tail_ uses extern_mod_allowed = false\n+    /// Parses a sequence of items. Stops when it finds program\n+    /// text that can't be parsed as an item\n+    /// - mod_items uses extern_mod_allowed = true\n+    /// - block_tail_ uses extern_mod_allowed = false\n     fn parse_items_and_view_items(&mut self,\n                                   first_item_attrs: Vec<Attribute> ,\n                                   mut extern_mod_allowed: bool,\n@@ -5313,8 +5314,8 @@ impl<'a> Parser<'a> {\n         }\n     }\n \n-    // Parses a sequence of foreign items. Stops when it finds program\n-    // text that can't be parsed as an item\n+    /// Parses a sequence of foreign items. Stops when it finds program\n+    /// text that can't be parsed as an item\n     fn parse_foreign_items(&mut self, first_item_attrs: Vec<Attribute> ,\n                            macros_allowed: bool)\n         -> ParsedItemsAndViewItems {\n@@ -5353,8 +5354,8 @@ impl<'a> Parser<'a> {\n         }\n     }\n \n-    // Parses a source module as a crate. This is the main\n-    // entry point for the parser.\n+    /// Parses a source module as a crate. This is the main\n+    /// entry point for the parser.\n     pub fn parse_crate_mod(&mut self) -> Crate {\n         let lo = self.span.lo;\n         // parse the crate's inner attrs, maybe (oops) one"}, {"sha": "8bd74b9ca770dc0550073c54e3ed87209696ea5a", "filename": "src/libsyntax/parse/token.rs", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "blob_url": "https://github.com/rust-lang/rust/blob/4989a56448c7e3047e0538ff4ef54c49db8a5a4f/src%2Flibsyntax%2Fparse%2Ftoken.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4989a56448c7e3047e0538ff4ef54c49db8a5a4f/src%2Flibsyntax%2Fparse%2Ftoken.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Ftoken.rs?ref=4989a56448c7e3047e0538ff4ef54c49db8a5a4f", "patch": "@@ -92,9 +92,9 @@ pub enum Token {\n     LIT_BINARY_RAW(Rc<Vec<u8>>, uint), /* raw binary str delimited by n hash symbols */\n \n     /* Name components */\n-    // an identifier contains an \"is_mod_name\" boolean,\n-    // indicating whether :: follows this token with no\n-    // whitespace in between.\n+    /// An identifier contains an \"is_mod_name\" boolean,\n+    /// indicating whether :: follows this token with no\n+    /// whitespace in between.\n     IDENT(ast::Ident, bool),\n     UNDERSCORE,\n     LIFETIME(ast::Ident),"}, {"sha": "fe84eeff4f87faa4b1248903a9eb26583a1e52db", "filename": "src/libsyntax/print/pp.rs", "status": "modified", "additions": 155, "deletions": 148, "changes": 303, "blob_url": "https://github.com/rust-lang/rust/blob/4989a56448c7e3047e0538ff4ef54c49db8a5a4f/src%2Flibsyntax%2Fprint%2Fpp.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4989a56448c7e3047e0538ff4ef54c49db8a5a4f/src%2Flibsyntax%2Fprint%2Fpp.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fprint%2Fpp.rs?ref=4989a56448c7e3047e0538ff4ef54c49db8a5a4f", "patch": "@@ -8,58 +8,56 @@\n // option. This file may not be copied, modified, or distributed\n // except according to those terms.\n \n-/*\n- * This pretty-printer is a direct reimplementation of Philip Karlton's\n- * Mesa pretty-printer, as described in appendix A of\n- *\n- *     STAN-CS-79-770: \"Pretty Printing\", by Derek C. Oppen.\n- *     Stanford Department of Computer Science, 1979.\n- *\n- * The algorithm's aim is to break a stream into as few lines as possible\n- * while respecting the indentation-consistency requirements of the enclosing\n- * block, and avoiding breaking at silly places on block boundaries, for\n- * example, between \"x\" and \")\" in \"x)\".\n- *\n- * I am implementing this algorithm because it comes with 20 pages of\n- * documentation explaining its theory, and because it addresses the set of\n- * concerns I've seen other pretty-printers fall down on. Weirdly. Even though\n- * it's 32 years old. What can I say?\n- *\n- * Despite some redundancies and quirks in the way it's implemented in that\n- * paper, I've opted to keep the implementation here as similar as I can,\n- * changing only what was blatantly wrong, a typo, or sufficiently\n- * non-idiomatic rust that it really stuck out.\n- *\n- * In particular you'll see a certain amount of churn related to INTEGER vs.\n- * CARDINAL in the Mesa implementation. Mesa apparently interconverts the two\n- * somewhat readily? In any case, I've used uint for indices-in-buffers and\n- * ints for character-sizes-and-indentation-offsets. This respects the need\n- * for ints to \"go negative\" while carrying a pending-calculation balance, and\n- * helps differentiate all the numbers flying around internally (slightly).\n- *\n- * I also inverted the indentation arithmetic used in the print stack, since\n- * the Mesa implementation (somewhat randomly) stores the offset on the print\n- * stack in terms of margin-col rather than col itself. I store col.\n- *\n- * I also implemented a small change in the String token, in that I store an\n- * explicit length for the string. For most tokens this is just the length of\n- * the accompanying string. But it's necessary to permit it to differ, for\n- * encoding things that are supposed to \"go on their own line\" -- certain\n- * classes of comment and blank-line -- where relying on adjacent\n- * hardbreak-like Break tokens with long blankness indication doesn't actually\n- * work. To see why, consider when there is a \"thing that should be on its own\n- * line\" between two long blocks, say functions. If you put a hardbreak after\n- * each function (or before each) and the breaking algorithm decides to break\n- * there anyways (because the functions themselves are long) you wind up with\n- * extra blank lines. If you don't put hardbreaks you can wind up with the\n- * \"thing which should be on its own line\" not getting its own line in the\n- * rare case of \"really small functions\" or such. This re-occurs with comments\n- * and explicit blank lines. So in those cases we use a string with a payload\n- * we want isolated to a line and an explicit length that's huge, surrounded\n- * by two zero-length breaks. The algorithm will try its best to fit it on a\n- * line (which it can't) and so naturally place the content on its own line to\n- * avoid combining it with other lines and making matters even worse.\n- */\n+//! This pretty-printer is a direct reimplementation of Philip Karlton's\n+//! Mesa pretty-printer, as described in appendix A of\n+//!\n+//!     STAN-CS-79-770: \"Pretty Printing\", by Derek C. Oppen.\n+//!     Stanford Department of Computer Science, 1979.\n+//!\n+//! The algorithm's aim is to break a stream into as few lines as possible\n+//! while respecting the indentation-consistency requirements of the enclosing\n+//! block, and avoiding breaking at silly places on block boundaries, for\n+//! example, between \"x\" and \")\" in \"x)\".\n+//!\n+//! I am implementing this algorithm because it comes with 20 pages of\n+//! documentation explaining its theory, and because it addresses the set of\n+//! concerns I've seen other pretty-printers fall down on. Weirdly. Even though\n+//! it's 32 years old. What can I say?\n+//!\n+//! Despite some redundancies and quirks in the way it's implemented in that\n+//! paper, I've opted to keep the implementation here as similar as I can,\n+//! changing only what was blatantly wrong, a typo, or sufficiently\n+//! non-idiomatic rust that it really stuck out.\n+//!\n+//! In particular you'll see a certain amount of churn related to INTEGER vs.\n+//! CARDINAL in the Mesa implementation. Mesa apparently interconverts the two\n+//! somewhat readily? In any case, I've used uint for indices-in-buffers and\n+//! ints for character-sizes-and-indentation-offsets. This respects the need\n+//! for ints to \"go negative\" while carrying a pending-calculation balance, and\n+//! helps differentiate all the numbers flying around internally (slightly).\n+//!\n+//! I also inverted the indentation arithmetic used in the print stack, since\n+//! the Mesa implementation (somewhat randomly) stores the offset on the print\n+//! stack in terms of margin-col rather than col itself. I store col.\n+//!\n+//! I also implemented a small change in the String token, in that I store an\n+//! explicit length for the string. For most tokens this is just the length of\n+//! the accompanying string. But it's necessary to permit it to differ, for\n+//! encoding things that are supposed to \"go on their own line\" -- certain\n+//! classes of comment and blank-line -- where relying on adjacent\n+//! hardbreak-like Break tokens with long blankness indication doesn't actually\n+//! work. To see why, consider when there is a \"thing that should be on its own\n+//! line\" between two long blocks, say functions. If you put a hardbreak after\n+//! each function (or before each) and the breaking algorithm decides to break\n+//! there anyways (because the functions themselves are long) you wind up with\n+//! extra blank lines. If you don't put hardbreaks you can wind up with the\n+//! \"thing which should be on its own line\" not getting its own line in the\n+//! rare case of \"really small functions\" or such. This re-occurs with comments\n+//! and explicit blank lines. So in those cases we use a string with a payload\n+//! we want isolated to a line and an explicit length that's huge, surrounded\n+//! by two zero-length breaks. The algorithm will try its best to fit it on a\n+//! line (which it can't) and so naturally place the content on its own line to\n+//! avoid combining it with other lines and making matters even worse.\n \n use std::io;\n use std::string::String;\n@@ -186,107 +184,116 @@ pub fn mk_printer(out: Box<io::Writer>, linewidth: uint) -> Printer {\n }\n \n \n-/*\n- * In case you do not have the paper, here is an explanation of what's going\n- * on.\n- *\n- * There is a stream of input tokens flowing through this printer.\n- *\n- * The printer buffers up to 3N tokens inside itself, where N is linewidth.\n- * Yes, linewidth is chars and tokens are multi-char, but in the worst\n- * case every token worth buffering is 1 char long, so it's ok.\n- *\n- * Tokens are String, Break, and Begin/End to delimit blocks.\n- *\n- * Begin tokens can carry an offset, saying \"how far to indent when you break\n- * inside here\", as well as a flag indicating \"consistent\" or \"inconsistent\"\n- * breaking. Consistent breaking means that after the first break, no attempt\n- * will be made to flow subsequent breaks together onto lines. Inconsistent\n- * is the opposite. Inconsistent breaking example would be, say:\n- *\n- *  foo(hello, there, good, friends)\n- *\n- * breaking inconsistently to become\n- *\n- *  foo(hello, there\n- *      good, friends);\n- *\n- * whereas a consistent breaking would yield:\n- *\n- *  foo(hello,\n- *      there\n- *      good,\n- *      friends);\n- *\n- * That is, in the consistent-break blocks we value vertical alignment\n- * more than the ability to cram stuff onto a line. But in all cases if it\n- * can make a block a one-liner, it'll do so.\n- *\n- * Carrying on with high-level logic:\n- *\n- * The buffered tokens go through a ring-buffer, 'tokens'. The 'left' and\n- * 'right' indices denote the active portion of the ring buffer as well as\n- * describing hypothetical points-in-the-infinite-stream at most 3N tokens\n- * apart (i.e. \"not wrapped to ring-buffer boundaries\"). The paper will switch\n- * between using 'left' and 'right' terms to denote the wrapepd-to-ring-buffer\n- * and point-in-infinite-stream senses freely.\n- *\n- * There is a parallel ring buffer, 'size', that holds the calculated size of\n- * each token. Why calculated? Because for Begin/End pairs, the \"size\"\n- * includes everything between the pair. That is, the \"size\" of Begin is\n- * actually the sum of the sizes of everything between Begin and the paired\n- * End that follows. Since that is arbitrarily far in the future, 'size' is\n- * being rewritten regularly while the printer runs; in fact most of the\n- * machinery is here to work out 'size' entries on the fly (and give up when\n- * they're so obviously over-long that \"infinity\" is a good enough\n- * approximation for purposes of line breaking).\n- *\n- * The \"input side\" of the printer is managed as an abstract process called\n- * SCAN, which uses 'scan_stack', 'scan_stack_empty', 'top' and 'bottom', to\n- * manage calculating 'size'. SCAN is, in other words, the process of\n- * calculating 'size' entries.\n- *\n- * The \"output side\" of the printer is managed by an abstract process called\n- * PRINT, which uses 'print_stack', 'margin' and 'space' to figure out what to\n- * do with each token/size pair it consumes as it goes. It's trying to consume\n- * the entire buffered window, but can't output anything until the size is >=\n- * 0 (sizes are set to negative while they're pending calculation).\n- *\n- * So SCAN takes input and buffers tokens and pending calculations, while\n- * PRINT gobbles up completed calculations and tokens from the buffer. The\n- * theory is that the two can never get more than 3N tokens apart, because\n- * once there's \"obviously\" too much data to fit on a line, in a size\n- * calculation, SCAN will write \"infinity\" to the size and let PRINT consume\n- * it.\n- *\n- * In this implementation (following the paper, again) the SCAN process is\n- * the method called 'pretty_print', and the 'PRINT' process is the method\n- * called 'print'.\n- */\n+/// In case you do not have the paper, here is an explanation of what's going\n+/// on.\n+///\n+/// There is a stream of input tokens flowing through this printer.\n+///\n+/// The printer buffers up to 3N tokens inside itself, where N is linewidth.\n+/// Yes, linewidth is chars and tokens are multi-char, but in the worst\n+/// case every token worth buffering is 1 char long, so it's ok.\n+///\n+/// Tokens are String, Break, and Begin/End to delimit blocks.\n+///\n+/// Begin tokens can carry an offset, saying \"how far to indent when you break\n+/// inside here\", as well as a flag indicating \"consistent\" or \"inconsistent\"\n+/// breaking. Consistent breaking means that after the first break, no attempt\n+/// will be made to flow subsequent breaks together onto lines. Inconsistent\n+/// is the opposite. Inconsistent breaking example would be, say:\n+///\n+///  foo(hello, there, good, friends)\n+///\n+/// breaking inconsistently to become\n+///\n+///  foo(hello, there\n+///      good, friends);\n+///\n+/// whereas a consistent breaking would yield:\n+///\n+///  foo(hello,\n+///      there\n+///      good,\n+///      friends);\n+///\n+/// That is, in the consistent-break blocks we value vertical alignment\n+/// more than the ability to cram stuff onto a line. But in all cases if it\n+/// can make a block a one-liner, it'll do so.\n+///\n+/// Carrying on with high-level logic:\n+///\n+/// The buffered tokens go through a ring-buffer, 'tokens'. The 'left' and\n+/// 'right' indices denote the active portion of the ring buffer as well as\n+/// describing hypothetical points-in-the-infinite-stream at most 3N tokens\n+/// apart (i.e. \"not wrapped to ring-buffer boundaries\"). The paper will switch\n+/// between using 'left' and 'right' terms to denote the wrapepd-to-ring-buffer\n+/// and point-in-infinite-stream senses freely.\n+///\n+/// There is a parallel ring buffer, 'size', that holds the calculated size of\n+/// each token. Why calculated? Because for Begin/End pairs, the \"size\"\n+/// includes everything betwen the pair. That is, the \"size\" of Begin is\n+/// actually the sum of the sizes of everything between Begin and the paired\n+/// End that follows. Since that is arbitrarily far in the future, 'size' is\n+/// being rewritten regularly while the printer runs; in fact most of the\n+/// machinery is here to work out 'size' entries on the fly (and give up when\n+/// they're so obviously over-long that \"infinity\" is a good enough\n+/// approximation for purposes of line breaking).\n+///\n+/// The \"input side\" of the printer is managed as an abstract process called\n+/// SCAN, which uses 'scan_stack', 'scan_stack_empty', 'top' and 'bottom', to\n+/// manage calculating 'size'. SCAN is, in other words, the process of\n+/// calculating 'size' entries.\n+///\n+/// The \"output side\" of the printer is managed by an abstract process called\n+/// PRINT, which uses 'print_stack', 'margin' and 'space' to figure out what to\n+/// do with each token/size pair it consumes as it goes. It's trying to consume\n+/// the entire buffered window, but can't output anything until the size is >=\n+/// 0 (sizes are set to negative while they're pending calculation).\n+///\n+/// So SCAN takes input and buffers tokens and pending calculations, while\n+/// PRINT gobbles up completed calculations and tokens from the buffer. The\n+/// theory is that the two can never get more than 3N tokens apart, because\n+/// once there's \"obviously\" too much data to fit on a line, in a size\n+/// calculation, SCAN will write \"infinity\" to the size and let PRINT consume\n+/// it.\n+///\n+/// In this implementation (following the paper, again) the SCAN process is\n+/// the method called 'pretty_print', and the 'PRINT' process is the method\n+/// called 'print'.\n pub struct Printer {\n     pub out: Box<io::Writer>,\n     buf_len: uint,\n-    margin: int, // width of lines we're constrained to\n-    space: int, // number of spaces left on line\n-    left: uint, // index of left side of input stream\n-    right: uint, // index of right side of input stream\n-    token: Vec<Token> , // ring-buffr stream goes through\n-    size: Vec<int> , // ring-buffer of calculated sizes\n-    left_total: int, // running size of stream \"...left\"\n-    right_total: int, // running size of stream \"...right\"\n-    // pseudo-stack, really a ring too. Holds the\n-    // primary-ring-buffers index of the Begin that started the\n-    // current block, possibly with the most recent Break after that\n-    // Begin (if there is any) on top of it. Stuff is flushed off the\n-    // bottom as it becomes irrelevant due to the primary ring-buffer\n-    // advancing.\n+    /// Width of lines we're constrained to\n+    margin: int,\n+    /// Number of spaces left on line\n+    space: int,\n+    /// Index of left side of input stream\n+    left: uint,\n+    /// Index of right side of input stream\n+    right: uint,\n+    /// Ring-buffr stream goes through\n+    token: Vec<Token> ,\n+    /// Ring-buffer of calculated sizes\n+    size: Vec<int> ,\n+    /// Running size of stream \"...left\"\n+    left_total: int,\n+    /// Running size of stream \"...right\"\n+    right_total: int,\n+    /// Pseudo-stack, really a ring too. Holds the\n+    /// primary-ring-buffers index of the Begin that started the\n+    /// current block, possibly with the most recent Break after that\n+    /// Begin (if there is any) on top of it. Stuff is flushed off the\n+    /// bottom as it becomes irrelevant due to the primary ring-buffer\n+    /// advancing.\n     scan_stack: Vec<uint> ,\n-    scan_stack_empty: bool, // top==bottom disambiguator\n-    top: uint, // index of top of scan_stack\n-    bottom: uint, // index of bottom of scan_stack\n-    // stack of blocks-in-progress being flushed by print\n+    /// Top==bottom disambiguator\n+    scan_stack_empty: bool,\n+    /// Index of top of scan_stack\n+    top: uint,\n+    /// Index of bottom of scan_stack\n+    bottom: uint,\n+    /// Stack of blocks-in-progress being flushed by print\n     print_stack: Vec<PrintStackElem> ,\n-    // buffered indentation to avoid writing trailing whitespace\n+    /// Buffered indentation to avoid writing trailing whitespace\n     pending_indentation: int,\n }\n "}, {"sha": "170cb7a249c4b697c72990d3b04ff0c62772404f", "filename": "src/libsyntax/print/pprust.rs", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "blob_url": "https://github.com/rust-lang/rust/blob/4989a56448c7e3047e0538ff4ef54c49db8a5a4f/src%2Flibsyntax%2Fprint%2Fpprust.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4989a56448c7e3047e0538ff4ef54c49db8a5a4f/src%2Flibsyntax%2Fprint%2Fpprust.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fprint%2Fpprust.rs?ref=4989a56448c7e3047e0538ff4ef54c49db8a5a4f", "patch": "@@ -88,9 +88,9 @@ pub static indent_unit: uint = 4u;\n \n pub static default_columns: uint = 78u;\n \n-// Requires you to pass an input filename and reader so that\n-// it can scan the input text for comments and literals to\n-// copy forward.\n+/// Requires you to pass an input filename and reader so that\n+/// it can scan the input text for comments and literals to\n+/// copy forward.\n pub fn print_crate<'a>(cm: &'a CodeMap,\n                        span_diagnostic: &diagnostic::SpanHandler,\n                        krate: &ast::Crate,"}, {"sha": "55fff38f9913112ac0792d5a707caacde29c7664", "filename": "src/libsyntax/util/interner.rs", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "blob_url": "https://github.com/rust-lang/rust/blob/4989a56448c7e3047e0538ff4ef54c49db8a5a4f/src%2Flibsyntax%2Futil%2Finterner.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4989a56448c7e3047e0538ff4ef54c49db8a5a4f/src%2Flibsyntax%2Futil%2Finterner.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Futil%2Finterner.rs?ref=4989a56448c7e3047e0538ff4ef54c49db8a5a4f", "patch": "@@ -8,9 +8,9 @@\n // option. This file may not be copied, modified, or distributed\n // except according to those terms.\n \n-// An \"interner\" is a data structure that associates values with uint tags and\n-// allows bidirectional lookup; i.e. given a value, one can easily find the\n-// type, and vice versa.\n+//! An \"interner\" is a data structure that associates values with uint tags and\n+//! allows bidirectional lookup; i.e. given a value, one can easily find the\n+//! type, and vice versa.\n \n use ast::Name;\n "}, {"sha": "f50739a7069e05a61d18496e35027ea242a77ef0", "filename": "src/libsyntax/util/parser_testing.rs", "status": "modified", "additions": 19, "deletions": 19, "changes": 38, "blob_url": "https://github.com/rust-lang/rust/blob/4989a56448c7e3047e0538ff4ef54c49db8a5a4f/src%2Flibsyntax%2Futil%2Fparser_testing.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4989a56448c7e3047e0538ff4ef54c49db8a5a4f/src%2Flibsyntax%2Futil%2Fparser_testing.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Futil%2Fparser_testing.rs?ref=4989a56448c7e3047e0538ff4ef54c49db8a5a4f", "patch": "@@ -17,14 +17,14 @@ use parse::token;\n \n use std::gc::Gc;\n \n-// map a string to tts, using a made-up filename:\n+/// Map a string to tts, using a made-up filename:\n pub fn string_to_tts(source_str: String) -> Vec<ast::TokenTree> {\n     let ps = new_parse_sess();\n     filemap_to_tts(&ps,\n                    string_to_filemap(&ps, source_str, \"bogofile\".to_string()))\n }\n \n-// map string to parser (via tts)\n+/// Map string to parser (via tts)\n pub fn string_to_parser<'a>(ps: &'a ParseSess, source_str: String) -> Parser<'a> {\n     new_parser_from_source_str(ps,\n                                Vec::new(),\n@@ -40,51 +40,51 @@ fn with_error_checking_parse<T>(s: String, f: |&mut Parser| -> T) -> T {\n     x\n }\n \n-// parse a string, return a crate.\n+/// Parse a string, return a crate.\n pub fn string_to_crate (source_str : String) -> ast::Crate {\n     with_error_checking_parse(source_str, |p| {\n         p.parse_crate_mod()\n     })\n }\n \n-// parse a string, return an expr\n+/// Parse a string, return an expr\n pub fn string_to_expr (source_str : String) -> Gc<ast::Expr> {\n     with_error_checking_parse(source_str, |p| {\n         p.parse_expr()\n     })\n }\n \n-// parse a string, return an item\n+/// Parse a string, return an item\n pub fn string_to_item (source_str : String) -> Option<Gc<ast::Item>> {\n     with_error_checking_parse(source_str, |p| {\n         p.parse_item(Vec::new())\n     })\n }\n \n-// parse a string, return a stmt\n+/// Parse a string, return a stmt\n pub fn string_to_stmt(source_str : String) -> Gc<ast::Stmt> {\n     with_error_checking_parse(source_str, |p| {\n         p.parse_stmt(Vec::new())\n     })\n }\n \n-// parse a string, return a pat. Uses \"irrefutable\"... which doesn't\n-// (currently) affect parsing.\n+/// Parse a string, return a pat. Uses \"irrefutable\"... which doesn't\n+/// (currently) affect parsing.\n pub fn string_to_pat(source_str: String) -> Gc<ast::Pat> {\n     string_to_parser(&new_parse_sess(), source_str).parse_pat()\n }\n \n-// convert a vector of strings to a vector of ast::Ident's\n+/// Convert a vector of strings to a vector of ast::Ident's\n pub fn strs_to_idents(ids: Vec<&str> ) -> Vec<ast::Ident> {\n     ids.iter().map(|u| token::str_to_ident(*u)).collect()\n }\n \n-// does the given string match the pattern? whitespace in the first string\n-// may be deleted or replaced with other whitespace to match the pattern.\n-// this function is unicode-ignorant; fortunately, the careful design of\n-// UTF-8 mitigates this ignorance.  In particular, this function only collapses\n-// sequences of \\n, \\r, ' ', and \\t, but it should otherwise tolerate unicode\n-// chars. Unsurprisingly, it doesn't do NKF-normalization(?).\n+/// Does the given string match the pattern? whitespace in the first string\n+/// may be deleted or replaced with other whitespace to match the pattern.\n+/// this function is unicode-ignorant; fortunately, the careful design of\n+/// UTF-8 mitigates this ignorance.  In particular, this function only collapses\n+/// sequences of \\n, \\r, ' ', and \\t, but it should otherwise tolerate unicode\n+/// chars. Unsurprisingly, it doesn't do NKF-normalization(?).\n pub fn matches_codepattern(a : &str, b : &str) -> bool {\n     let mut idx_a = 0;\n     let mut idx_b = 0;\n@@ -122,9 +122,9 @@ pub fn matches_codepattern(a : &str, b : &str) -> bool {\n     }\n }\n \n-// given a string and an index, return the first uint >= idx\n-// that is a non-ws-char or is outside of the legal range of\n-// the string.\n+/// Given a string and an index, return the first uint >= idx\n+/// that is a non-ws-char or is outside of the legal range of\n+/// the string.\n fn scan_for_non_ws_or_end(a : &str, idx: uint) -> uint {\n     let mut i = idx;\n     let len = a.len();\n@@ -134,7 +134,7 @@ fn scan_for_non_ws_or_end(a : &str, idx: uint) -> uint {\n     i\n }\n \n-// copied from lexer.\n+/// Copied from lexer.\n pub fn is_whitespace(c: char) -> bool {\n     return c == ' ' || c == '\\t' || c == '\\r' || c == '\\n';\n }"}, {"sha": "9298b58c4267d9c2c3493f6b403e82946224f2a3", "filename": "src/libsyntax/visit.rs", "status": "modified", "additions": 16, "deletions": 16, "changes": 32, "blob_url": "https://github.com/rust-lang/rust/blob/4989a56448c7e3047e0538ff4ef54c49db8a5a4f/src%2Flibsyntax%2Fvisit.rs", "raw_url": "https://github.com/rust-lang/rust/raw/4989a56448c7e3047e0538ff4ef54c49db8a5a4f/src%2Flibsyntax%2Fvisit.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fvisit.rs?ref=4989a56448c7e3047e0538ff4ef54c49db8a5a4f", "patch": "@@ -8,6 +8,18 @@\n // option. This file may not be copied, modified, or distributed\n // except according to those terms.\n \n+//! Context-passing AST walker. Each overridden visit method has full control\n+//! over what happens with its node, it can do its own traversal of the node's\n+//! children (potentially passing in different contexts to each), call\n+//! `visit::visit_*` to apply the default traversal algorithm (again, it can\n+//! override the context), or prevent deeper traversal by doing nothing.\n+//!\n+//! Note: it is an important invariant that the default visitor walks the body\n+//! of a function in \"execution order\" (more concretely, reverse post-order\n+//! with respect to the CFG implied by the AST), meaning that if AST node A may\n+//! execute before AST node B, then A is visited first.  The borrow checker in\n+//! particular relies on this property.\n+//!\n use abi::Abi;\n use ast::*;\n use ast;\n@@ -17,27 +29,15 @@ use owned_slice::OwnedSlice;\n \n use std::gc::Gc;\n \n-// Context-passing AST walker. Each overridden visit method has full control\n-// over what happens with its node, it can do its own traversal of the node's\n-// children (potentially passing in different contexts to each), call\n-// visit::visit_* to apply the default traversal algorithm (again, it can\n-// override the context), or prevent deeper traversal by doing nothing.\n-//\n-// Note: it is an important invariant that the default visitor walks the body\n-// of a function in \"execution order\" (more concretely, reverse post-order\n-// with respect to the CFG implied by the AST), meaning that if AST node A may\n-// execute before AST node B, then A is visited first.  The borrow checker in\n-// particular relies on this property.\n-\n pub enum FnKind<'a> {\n-    // fn foo() or extern \"Abi\" fn foo()\n+    /// fn foo() or extern \"Abi\" fn foo()\n     FkItemFn(Ident, &'a Generics, FnStyle, Abi),\n \n-    // fn foo(&self)\n+    /// fn foo(&self)\n     FkMethod(Ident, &'a Generics, &'a Method),\n \n-    // |x, y| ...\n-    // proc(x, y) ...\n+    /// |x, y| ...\n+    /// proc(x, y) ...\n     FkFnBlock,\n }\n "}]}