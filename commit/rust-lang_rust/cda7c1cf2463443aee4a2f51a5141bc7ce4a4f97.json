{"sha": "cda7c1cf2463443aee4a2f51a5141bc7ce4a4f97", "node_id": "MDY6Q29tbWl0NzI0NzEyOmNkYTdjMWNmMjQ2MzQ0M2FlZTRhMmY1MWE1MTQxYmM3Y2U0YTRmOTc=", "commit": {"author": {"name": "bors", "email": "bors@rust-lang.org", "date": "2016-04-27T20:49:45Z"}, "committer": {"name": "bors", "email": "bors@rust-lang.org", "date": "2016-04-27T20:49:45Z"}, "message": "Auto merge of #33199 - mitaa:tokenize-responsibly, r=nrc\n\nMake some fatal lexer errors recoverable\n\nI've kept the changes to a minimum since I'm not really sure if this approach is a acceptable.\n\nfixes #12834\n\ncc @nrc", "tree": {"sha": "52be2ea17940cd98bbdfdc205d2552ce71eb127d", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/52be2ea17940cd98bbdfdc205d2552ce71eb127d"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/cda7c1cf2463443aee4a2f51a5141bc7ce4a4f97", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/cda7c1cf2463443aee4a2f51a5141bc7ce4a4f97", "html_url": "https://github.com/rust-lang/rust/commit/cda7c1cf2463443aee4a2f51a5141bc7ce4a4f97", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/cda7c1cf2463443aee4a2f51a5141bc7ce4a4f97/comments", "author": {"login": "bors", "id": 3372342, "node_id": "MDQ6VXNlcjMzNzIzNDI=", "avatar_url": "https://avatars.githubusercontent.com/u/3372342?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bors", "html_url": "https://github.com/bors", "followers_url": "https://api.github.com/users/bors/followers", "following_url": "https://api.github.com/users/bors/following{/other_user}", "gists_url": "https://api.github.com/users/bors/gists{/gist_id}", "starred_url": "https://api.github.com/users/bors/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bors/subscriptions", "organizations_url": "https://api.github.com/users/bors/orgs", "repos_url": "https://api.github.com/users/bors/repos", "events_url": "https://api.github.com/users/bors/events{/privacy}", "received_events_url": "https://api.github.com/users/bors/received_events", "type": "User", "site_admin": false}, "committer": {"login": "bors", "id": 3372342, "node_id": "MDQ6VXNlcjMzNzIzNDI=", "avatar_url": "https://avatars.githubusercontent.com/u/3372342?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bors", "html_url": "https://github.com/bors", "followers_url": "https://api.github.com/users/bors/followers", "following_url": "https://api.github.com/users/bors/following{/other_user}", "gists_url": "https://api.github.com/users/bors/gists{/gist_id}", "starred_url": "https://api.github.com/users/bors/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bors/subscriptions", "organizations_url": "https://api.github.com/users/bors/orgs", "repos_url": "https://api.github.com/users/bors/repos", "events_url": "https://api.github.com/users/bors/events{/privacy}", "received_events_url": "https://api.github.com/users/bors/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "b52d76a08528273b218f168753ed846ecfb59aec", "url": "https://api.github.com/repos/rust-lang/rust/commits/b52d76a08528273b218f168753ed846ecfb59aec", "html_url": "https://github.com/rust-lang/rust/commit/b52d76a08528273b218f168753ed846ecfb59aec"}, {"sha": "6887202ea3a1d3e3df0c88c07c754defd87b9712", "url": "https://api.github.com/repos/rust-lang/rust/commits/6887202ea3a1d3e3df0c88c07c754defd87b9712", "html_url": "https://github.com/rust-lang/rust/commit/6887202ea3a1d3e3df0c88c07c754defd87b9712"}], "stats": {"total": 222, "additions": 148, "deletions": 74}, "files": [{"sha": "789239c3ab02006a57410578b717740316073c91", "filename": "src/librustdoc/html/highlight.rs", "status": "modified", "additions": 19, "deletions": 7, "changes": 26, "blob_url": "https://github.com/rust-lang/rust/blob/cda7c1cf2463443aee4a2f51a5141bc7ce4a4f97/src%2Flibrustdoc%2Fhtml%2Fhighlight.rs", "raw_url": "https://github.com/rust-lang/rust/raw/cda7c1cf2463443aee4a2f51a5141bc7ce4a4f97/src%2Flibrustdoc%2Fhtml%2Fhighlight.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustdoc%2Fhtml%2Fhighlight.rs?ref=cda7c1cf2463443aee4a2f51a5141bc7ce4a4f97", "patch": "@@ -29,25 +29,27 @@ pub fn render_with_highlighting(src: &str, class: Option<&str>, id: Option<&str>\n \n     let mut out = Vec::new();\n     write_header(class, id, &mut out).unwrap();\n-    write_source(&sess,\n-                 lexer::StringReader::new(&sess.span_diagnostic, fm),\n-                 &mut out).unwrap();\n+    if let Err(_) = write_source(&sess,\n+                                 lexer::StringReader::new(&sess.span_diagnostic, fm),\n+                                 &mut out) {\n+        return format!(\"<pre>{}</pre>\", src)\n+    }\n     write_footer(&mut out).unwrap();\n     String::from_utf8_lossy(&out[..]).into_owned()\n }\n \n /// Highlights `src`, returning the HTML output. Returns only the inner html to\n /// be inserted into an element. C.f., `render_with_highlighting` which includes\n /// an enclosing `<pre>` block.\n-pub fn render_inner_with_highlighting(src: &str) -> String {\n+pub fn render_inner_with_highlighting(src: &str) -> io::Result<String> {\n     let sess = parse::ParseSess::new();\n     let fm = sess.codemap().new_filemap(\"<stdin>\".to_string(), src.to_string());\n \n     let mut out = Vec::new();\n     write_source(&sess,\n                  lexer::StringReader::new(&sess.span_diagnostic, fm),\n-                 &mut out).unwrap();\n-    String::from_utf8_lossy(&out[..]).into_owned()\n+                 &mut out)?;\n+    Ok(String::from_utf8_lossy(&out[..]).into_owned())\n }\n \n /// Exhausts the `lexer` writing the output into `out`.\n@@ -65,7 +67,17 @@ fn write_source(sess: &parse::ParseSess,\n     let mut is_macro = false;\n     let mut is_macro_nonterminal = false;\n     loop {\n-        let next = lexer.next_token();\n+        let next = match lexer.try_next_token() {\n+            Ok(tok) => tok,\n+            Err(_) => {\n+                lexer.emit_fatal_errors();\n+                lexer.span_diagnostic.struct_warn(\"Backing out of syntax highlighting\")\n+                                     .note(\"You probably did not intend to render this \\\n+                                            as a rust code-block\")\n+                                     .emit();\n+                return Err(io::Error::new(io::ErrorKind::Other, \"\"))\n+            },\n+        };\n \n         let snip = |sp| sess.codemap().span_to_snippet(sp).unwrap();\n "}, {"sha": "792828b3054e9bd54e4303f0582213f8d187bdba", "filename": "src/libsyntax/errors/mod.rs", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/cda7c1cf2463443aee4a2f51a5141bc7ce4a4f97/src%2Flibsyntax%2Ferrors%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/cda7c1cf2463443aee4a2f51a5141bc7ce4a4f97/src%2Flibsyntax%2Ferrors%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Ferrors%2Fmod.rs?ref=cda7c1cf2463443aee4a2f51a5141bc7ce4a4f97", "patch": "@@ -177,6 +177,7 @@ impl error::Error for ExplicitBug {\n \n /// Used for emitting structured error messages and other diagnostic information.\n #[must_use]\n+#[derive(Clone)]\n pub struct DiagnosticBuilder<'a> {\n     emitter: &'a RefCell<Box<Emitter>>,\n     level: Level,\n@@ -187,6 +188,7 @@ pub struct DiagnosticBuilder<'a> {\n }\n \n /// For example a note attached to an error.\n+#[derive(Clone)]\n struct SubDiagnostic {\n     level: Level,\n     message: String,"}, {"sha": "6b3b5ce9de9140c387417688c0fa1063d6e198f2", "filename": "src/libsyntax/ext/tt/transcribe.rs", "status": "modified", "additions": 3, "deletions": 1, "changes": 4, "blob_url": "https://github.com/rust-lang/rust/blob/cda7c1cf2463443aee4a2f51a5141bc7ce4a4f97/src%2Flibsyntax%2Fext%2Ftt%2Ftranscribe.rs", "raw_url": "https://github.com/rust-lang/rust/raw/cda7c1cf2463443aee4a2f51a5141bc7ce4a4f97/src%2Flibsyntax%2Fext%2Ftt%2Ftranscribe.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fext%2Ftt%2Ftranscribe.rs?ref=cda7c1cf2463443aee4a2f51a5141bc7ce4a4f97", "patch": "@@ -12,7 +12,7 @@ use self::LockstepIterSize::*;\n use ast;\n use ast::{TokenTree, Ident, Name};\n use codemap::{Span, DUMMY_SP};\n-use errors::Handler;\n+use errors::{Handler, DiagnosticBuilder};\n use ext::tt::macro_parser::{NamedMatch, MatchedSeq, MatchedNonterminal};\n use parse::token::{DocComment, MatchNt, SubstNt};\n use parse::token::{Token, NtIdent, SpecialMacroVar};\n@@ -50,6 +50,7 @@ pub struct TtReader<'a> {\n     pub cur_span: Span,\n     /// Transform doc comments. Only useful in macro invocations\n     pub desugar_doc_comments: bool,\n+    pub fatal_errs: Vec<DiagnosticBuilder<'a>>,\n }\n \n /// This can do Macro-By-Example transcription. On the other hand, if\n@@ -99,6 +100,7 @@ pub fn new_tt_reader_with_doc_flag(sp_diag: &Handler,\n         /* dummy values, never read: */\n         cur_tok: token::Eof,\n         cur_span: DUMMY_SP,\n+        fatal_errs: Vec::new(),\n     };\n     tt_next_token(&mut r); /* get cur_tok and cur_span set up */\n     r"}, {"sha": "881663a056c789150e455073ecfb5a69e0ef706e", "filename": "src/libsyntax/parse/lexer/mod.rs", "status": "modified", "additions": 103, "deletions": 66, "changes": 169, "blob_url": "https://github.com/rust-lang/rust/blob/cda7c1cf2463443aee4a2f51a5141bc7ce4a4f97/src%2Flibsyntax%2Fparse%2Flexer%2Fmod.rs", "raw_url": "https://github.com/rust-lang/rust/raw/cda7c1cf2463443aee4a2f51a5141bc7ce4a4f97/src%2Flibsyntax%2Fparse%2Flexer%2Fmod.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Fparse%2Flexer%2Fmod.rs?ref=cda7c1cf2463443aee4a2f51a5141bc7ce4a4f97", "patch": "@@ -29,24 +29,42 @@ mod unicode_chars;\n \n pub trait Reader {\n     fn is_eof(&self) -> bool;\n-    fn next_token(&mut self) -> TokenAndSpan;\n+    fn try_next_token(&mut self) -> Result<TokenAndSpan, ()>;\n+    fn next_token(&mut self) -> TokenAndSpan where Self: Sized {\n+        let res = self.try_next_token();\n+        self.unwrap_or_abort(res)\n+    }\n     /// Report a fatal error with the current span.\n     fn fatal(&self, &str) -> FatalError;\n     /// Report a non-fatal error with the current span.\n     fn err(&self, &str);\n+    fn emit_fatal_errors(&mut self);\n+    fn unwrap_or_abort(&mut self, res: Result<TokenAndSpan, ()>) -> TokenAndSpan {\n+        match res {\n+            Ok(tok) => tok,\n+            Err(_) => {\n+                self.emit_fatal_errors();\n+                panic!(FatalError);\n+            }\n+        }\n+    }\n     fn peek(&self) -> TokenAndSpan;\n     /// Get a token the parser cares about.\n-    fn real_token(&mut self) -> TokenAndSpan {\n-        let mut t = self.next_token();\n+    fn try_real_token(&mut self) -> Result<TokenAndSpan, ()> {\n+        let mut t = self.try_next_token()?;\n         loop {\n             match t.tok {\n                 token::Whitespace | token::Comment | token::Shebang(_) => {\n-                    t = self.next_token();\n+                    t = self.try_next_token()?;\n                 }\n                 _ => break,\n             }\n         }\n-        t\n+        Ok(t)\n+    }\n+    fn real_token(&mut self) -> TokenAndSpan {\n+        let res = self.try_real_token();\n+        self.unwrap_or_abort(res)\n     }\n }\n \n@@ -70,7 +88,7 @@ pub struct StringReader<'a> {\n     // cached:\n     pub peek_tok: token::Token,\n     pub peek_span: Span,\n-\n+    pub fatal_errs: Vec<DiagnosticBuilder<'a>>,\n     // cache a direct reference to the source text, so that we don't have to\n     // retrieve it via `self.filemap.src.as_ref().unwrap()` all the time.\n     source_text: Rc<String>,\n@@ -81,20 +99,27 @@ impl<'a> Reader for StringReader<'a> {\n         self.curr.is_none()\n     }\n     /// Return the next token. EFFECT: advances the string_reader.\n-    fn next_token(&mut self) -> TokenAndSpan {\n+    fn try_next_token(&mut self) -> Result<TokenAndSpan, ()> {\n+        assert!(self.fatal_errs.is_empty());\n         let ret_val = TokenAndSpan {\n             tok: replace(&mut self.peek_tok, token::Underscore),\n             sp: self.peek_span,\n         };\n-        self.advance_token();\n-        ret_val\n+        self.advance_token()?;\n+        Ok(ret_val)\n     }\n     fn fatal(&self, m: &str) -> FatalError {\n         self.fatal_span(self.peek_span, m)\n     }\n     fn err(&self, m: &str) {\n         self.err_span(self.peek_span, m)\n     }\n+    fn emit_fatal_errors(&mut self) {\n+        for err in &mut self.fatal_errs {\n+            err.emit();\n+        }\n+        self.fatal_errs.clear();\n+    }\n     fn peek(&self) -> TokenAndSpan {\n         // FIXME(pcwalton): Bad copy!\n         TokenAndSpan {\n@@ -108,17 +133,24 @@ impl<'a> Reader for TtReader<'a> {\n     fn is_eof(&self) -> bool {\n         self.cur_tok == token::Eof\n     }\n-    fn next_token(&mut self) -> TokenAndSpan {\n+    fn try_next_token(&mut self) -> Result<TokenAndSpan, ()> {\n+        assert!(self.fatal_errs.is_empty());\n         let r = tt_next_token(self);\n         debug!(\"TtReader: r={:?}\", r);\n-        r\n+        Ok(r)\n     }\n     fn fatal(&self, m: &str) -> FatalError {\n         self.sp_diag.span_fatal(self.cur_span, m)\n     }\n     fn err(&self, m: &str) {\n         self.sp_diag.span_err(self.cur_span, m);\n     }\n+    fn emit_fatal_errors(&mut self) {\n+        for err in &mut self.fatal_errs {\n+            err.emit();\n+        }\n+        self.fatal_errs.clear();\n+    }\n     fn peek(&self) -> TokenAndSpan {\n         TokenAndSpan {\n             tok: self.cur_tok.clone(),\n@@ -151,6 +183,7 @@ impl<'a> StringReader<'a> {\n             peek_tok: token::Eof,\n             peek_span: codemap::DUMMY_SP,\n             source_text: source_text,\n+            fatal_errs: Vec::new(),\n         };\n         sr.bump();\n         sr\n@@ -160,7 +193,10 @@ impl<'a> StringReader<'a> {\n                    filemap: Rc<codemap::FileMap>)\n                    -> StringReader<'b> {\n         let mut sr = StringReader::new_raw(span_diagnostic, filemap);\n-        sr.advance_token();\n+        if let Err(_) = sr.advance_token() {\n+            sr.emit_fatal_errors();\n+            panic!(FatalError);\n+        }\n         sr\n     }\n \n@@ -249,7 +285,7 @@ impl<'a> StringReader<'a> {\n \n     /// Advance peek_tok and peek_span to refer to the next token, and\n     /// possibly update the interner.\n-    fn advance_token(&mut self) {\n+    fn advance_token(&mut self) -> Result<(), ()> {\n         match self.scan_whitespace_or_comment() {\n             Some(comment) => {\n                 self.peek_span = comment.sp;\n@@ -261,11 +297,12 @@ impl<'a> StringReader<'a> {\n                     self.peek_span = codemap::mk_sp(self.filemap.end_pos, self.filemap.end_pos);\n                 } else {\n                     let start_bytepos = self.last_pos;\n-                    self.peek_tok = self.next_token_inner();\n+                    self.peek_tok = self.next_token_inner()?;\n                     self.peek_span = codemap::mk_sp(start_bytepos, self.last_pos);\n                 };\n             }\n         }\n+        Ok(())\n     }\n \n     fn byte_offset(&self, pos: BytePos) -> BytePos {\n@@ -1013,7 +1050,7 @@ impl<'a> StringReader<'a> {\n \n     /// Return the next token from the string, advances the input past that\n     /// token, and updates the interner\n-    fn next_token_inner(&mut self) -> token::Token {\n+    fn next_token_inner(&mut self) -> Result<token::Token, ()> {\n         let c = self.curr;\n         if ident_start(c) &&\n            match (c.unwrap(), self.nextch(), self.nextnextch()) {\n@@ -1033,144 +1070,144 @@ impl<'a> StringReader<'a> {\n                 self.bump();\n             }\n \n-            return self.with_str_from(start, |string| {\n+            return Ok(self.with_str_from(start, |string| {\n                 if string == \"_\" {\n                     token::Underscore\n                 } else {\n                     // FIXME: perform NFKC normalization here. (Issue #2253)\n                     token::Ident(str_to_ident(string))\n                 }\n-            });\n+            }));\n         }\n \n         if is_dec_digit(c) {\n             let num = self.scan_number(c.unwrap());\n             let suffix = self.scan_optional_raw_name();\n             debug!(\"next_token_inner: scanned number {:?}, {:?}\", num, suffix);\n-            return token::Literal(num, suffix);\n+            return Ok(token::Literal(num, suffix));\n         }\n \n         match c.expect(\"next_token_inner called at EOF\") {\n             // One-byte tokens.\n             ';' => {\n                 self.bump();\n-                return token::Semi;\n+                return Ok(token::Semi);\n             }\n             ',' => {\n                 self.bump();\n-                return token::Comma;\n+                return Ok(token::Comma);\n             }\n             '.' => {\n                 self.bump();\n                 return if self.curr_is('.') {\n                     self.bump();\n                     if self.curr_is('.') {\n                         self.bump();\n-                        token::DotDotDot\n+                        Ok(token::DotDotDot)\n                     } else {\n-                        token::DotDot\n+                        Ok(token::DotDot)\n                     }\n                 } else {\n-                    token::Dot\n+                    Ok(token::Dot)\n                 };\n             }\n             '(' => {\n                 self.bump();\n-                return token::OpenDelim(token::Paren);\n+                return Ok(token::OpenDelim(token::Paren));\n             }\n             ')' => {\n                 self.bump();\n-                return token::CloseDelim(token::Paren);\n+                return Ok(token::CloseDelim(token::Paren));\n             }\n             '{' => {\n                 self.bump();\n-                return token::OpenDelim(token::Brace);\n+                return Ok(token::OpenDelim(token::Brace));\n             }\n             '}' => {\n                 self.bump();\n-                return token::CloseDelim(token::Brace);\n+                return Ok(token::CloseDelim(token::Brace));\n             }\n             '[' => {\n                 self.bump();\n-                return token::OpenDelim(token::Bracket);\n+                return Ok(token::OpenDelim(token::Bracket));\n             }\n             ']' => {\n                 self.bump();\n-                return token::CloseDelim(token::Bracket);\n+                return Ok(token::CloseDelim(token::Bracket));\n             }\n             '@' => {\n                 self.bump();\n-                return token::At;\n+                return Ok(token::At);\n             }\n             '#' => {\n                 self.bump();\n-                return token::Pound;\n+                return Ok(token::Pound);\n             }\n             '~' => {\n                 self.bump();\n-                return token::Tilde;\n+                return Ok(token::Tilde);\n             }\n             '?' => {\n                 self.bump();\n-                return token::Question;\n+                return Ok(token::Question);\n             }\n             ':' => {\n                 self.bump();\n                 if self.curr_is(':') {\n                     self.bump();\n-                    return token::ModSep;\n+                    return Ok(token::ModSep);\n                 } else {\n-                    return token::Colon;\n+                    return Ok(token::Colon);\n                 }\n             }\n \n             '$' => {\n                 self.bump();\n-                return token::Dollar;\n+                return Ok(token::Dollar);\n             }\n \n             // Multi-byte tokens.\n             '=' => {\n                 self.bump();\n                 if self.curr_is('=') {\n                     self.bump();\n-                    return token::EqEq;\n+                    return Ok(token::EqEq);\n                 } else if self.curr_is('>') {\n                     self.bump();\n-                    return token::FatArrow;\n+                    return Ok(token::FatArrow);\n                 } else {\n-                    return token::Eq;\n+                    return Ok(token::Eq);\n                 }\n             }\n             '!' => {\n                 self.bump();\n                 if self.curr_is('=') {\n                     self.bump();\n-                    return token::Ne;\n+                    return Ok(token::Ne);\n                 } else {\n-                    return token::Not;\n+                    return Ok(token::Not);\n                 }\n             }\n             '<' => {\n                 self.bump();\n                 match self.curr.unwrap_or('\\x00') {\n                     '=' => {\n                         self.bump();\n-                        return token::Le;\n+                        return Ok(token::Le);\n                     }\n                     '<' => {\n-                        return self.binop(token::Shl);\n+                        return Ok(self.binop(token::Shl));\n                     }\n                     '-' => {\n                         self.bump();\n                         match self.curr.unwrap_or('\\x00') {\n                             _ => {\n-                                return token::LArrow;\n+                                return Ok(token::LArrow);\n                             }\n                         }\n                     }\n                     _ => {\n-                        return token::Lt;\n+                        return Ok(token::Lt);\n                     }\n                 }\n             }\n@@ -1179,13 +1216,13 @@ impl<'a> StringReader<'a> {\n                 match self.curr.unwrap_or('\\x00') {\n                     '=' => {\n                         self.bump();\n-                        return token::Ge;\n+                        return Ok(token::Ge);\n                     }\n                     '>' => {\n-                        return self.binop(token::Shr);\n+                        return Ok(self.binop(token::Shr));\n                     }\n                     _ => {\n-                        return token::Gt;\n+                        return Ok(token::Gt);\n                     }\n                 }\n             }\n@@ -1233,7 +1270,7 @@ impl<'a> StringReader<'a> {\n                         self.err_span_(start, last_bpos, \"lifetimes cannot use keyword names\");\n                     }\n \n-                    return token::Lifetime(ident);\n+                    return Ok(token::Lifetime(ident));\n                 }\n \n                 let valid = self.scan_char_or_byte(start,\n@@ -1255,7 +1292,7 @@ impl<'a> StringReader<'a> {\n                 };\n                 self.bump(); // advance curr past token\n                 let suffix = self.scan_optional_raw_name();\n-                return token::Literal(token::Char(id), suffix);\n+                return Ok(token::Literal(token::Char(id), suffix));\n             }\n             'b' => {\n                 self.bump();\n@@ -1266,7 +1303,7 @@ impl<'a> StringReader<'a> {\n                     _ => unreachable!(),  // Should have been a token::Ident above.\n                 };\n                 let suffix = self.scan_optional_raw_name();\n-                return token::Literal(lit, suffix);\n+                return Ok(token::Literal(lit, suffix));\n             }\n             '\"' => {\n                 let start_bpos = self.last_pos;\n@@ -1297,7 +1334,7 @@ impl<'a> StringReader<'a> {\n                 };\n                 self.bump();\n                 let suffix = self.scan_optional_raw_name();\n-                return token::Literal(token::Str_(id), suffix);\n+                return Ok(token::Literal(token::Str_(id), suffix));\n             }\n             'r' => {\n                 let start_bpos = self.last_pos;\n@@ -1368,52 +1405,52 @@ impl<'a> StringReader<'a> {\n                     token::intern(\"??\")\n                 };\n                 let suffix = self.scan_optional_raw_name();\n-                return token::Literal(token::StrRaw(id, hash_count), suffix);\n+                return Ok(token::Literal(token::StrRaw(id, hash_count), suffix));\n             }\n             '-' => {\n                 if self.nextch_is('>') {\n                     self.bump();\n                     self.bump();\n-                    return token::RArrow;\n+                    return Ok(token::RArrow);\n                 } else {\n-                    return self.binop(token::Minus);\n+                    return Ok(self.binop(token::Minus));\n                 }\n             }\n             '&' => {\n                 if self.nextch_is('&') {\n                     self.bump();\n                     self.bump();\n-                    return token::AndAnd;\n+                    return Ok(token::AndAnd);\n                 } else {\n-                    return self.binop(token::And);\n+                    return Ok(self.binop(token::And));\n                 }\n             }\n             '|' => {\n                 match self.nextch() {\n                     Some('|') => {\n                         self.bump();\n                         self.bump();\n-                        return token::OrOr;\n+                        return Ok(token::OrOr);\n                     }\n                     _ => {\n-                        return self.binop(token::Or);\n+                        return Ok(self.binop(token::Or));\n                     }\n                 }\n             }\n             '+' => {\n-                return self.binop(token::Plus);\n+                return Ok(self.binop(token::Plus));\n             }\n             '*' => {\n-                return self.binop(token::Star);\n+                return Ok(self.binop(token::Star));\n             }\n             '/' => {\n-                return self.binop(token::Slash);\n+                return Ok(self.binop(token::Slash));\n             }\n             '^' => {\n-                return self.binop(token::Caret);\n+                return Ok(self.binop(token::Caret));\n             }\n             '%' => {\n-                return self.binop(token::Percent);\n+                return Ok(self.binop(token::Percent));\n             }\n             c => {\n                 let last_bpos = self.last_pos;\n@@ -1423,8 +1460,8 @@ impl<'a> StringReader<'a> {\n                                                           \"unknown start of token\",\n                                                           c);\n                 unicode_chars::check_for_substitution(&self, c, &mut err);\n-                err.emit();\n-                panic!(FatalError);\n+                self.fatal_errs.push(err);\n+                Err(())\n             }\n         }\n     }"}, {"sha": "30dce27e7389474b386fcdfcb2a5c7da3981cf1a", "filename": "src/test/rustdoc/issue-12834.rs", "status": "added", "additions": 21, "deletions": 0, "changes": 21, "blob_url": "https://github.com/rust-lang/rust/blob/cda7c1cf2463443aee4a2f51a5141bc7ce4a4f97/src%2Ftest%2Frustdoc%2Fissue-12834.rs", "raw_url": "https://github.com/rust-lang/rust/raw/cda7c1cf2463443aee4a2f51a5141bc7ce4a4f97/src%2Ftest%2Frustdoc%2Fissue-12834.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Ftest%2Frustdoc%2Fissue-12834.rs?ref=cda7c1cf2463443aee4a2f51a5141bc7ce4a4f97", "patch": "@@ -0,0 +1,21 @@\n+// Copyright 2016 The Rust Project Developers. See the COPYRIGHT\n+// file at the top-level directory of this distribution and at\n+// http://rust-lang.org/COPYRIGHT.\n+//\n+// Licensed under the Apache License, Version 2.0 <LICENSE-APACHE or\n+// http://www.apache.org/licenses/LICENSE-2.0> or the MIT license\n+// <LICENSE-MIT or http://opensource.org/licenses/MIT>, at your\n+// option. This file may not be copied, modified, or distributed\n+// except according to those terms.\n+\n+// Tests that failing to syntax highlight a rust code-block doesn't cause\n+// rustdoc to fail, while still rendering the code-block (without highlighting).\n+\n+\n+// @has issue_12834/fn.foo.html\n+// @has - //pre 'a + b '\n+\n+/// ```\n+/// a + b \u2208 Self \u2200 a, b \u2208 Self\n+/// ```\n+pub fn foo() {}"}]}