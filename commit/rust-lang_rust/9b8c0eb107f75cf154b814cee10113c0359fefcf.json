{"sha": "9b8c0eb107f75cf154b814cee10113c0359fefcf", "node_id": "MDY6Q29tbWl0NzI0NzEyOjliOGMwZWIxMDdmNzVjZjE1NGI4MTRjZWUxMDExM2MwMzU5ZmVmY2Y=", "commit": {"author": {"name": "Dylan DPC", "email": "dylan.dpc@gmail.com", "date": "2020-10-16T00:10:17Z"}, "committer": {"name": "GitHub", "email": "noreply@github.com", "date": "2020-10-16T00:10:17Z"}, "message": "Rollup merge of #77657 - fusion-engineering-forks:cleanup-cloudabi-sync, r=dtolnay\n\nCleanup cloudabi mutexes and condvars\n\nThis gets rid of lots of unnecessary unsafety.\n\nAll the AtomicU32s were wrapped in UnsafeCell or UnsafeCell<MaybeUninit>, and raw pointers were used to get to the AtomicU32 inside. This change cleans that up by using AtomicU32 directly.\n\nAlso replaces a UnsafeCell<u32> by a safer Cell<u32>.\n\n@rustbot modify labels: +C-cleanup", "tree": {"sha": "f5e174193f2df91d0a98901333bfe9d1a4c1ec8d", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/f5e174193f2df91d0a98901333bfe9d1a4c1ec8d"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/9b8c0eb107f75cf154b814cee10113c0359fefcf", "comment_count": 0, "verification": {"verified": true, "reason": "valid", "signature": "-----BEGIN PGP SIGNATURE-----\n\nwsBcBAABCAAQBQJfiOTqCRBK7hj4Ov3rIwAAdHIIALHH9TtpDrCAr/dnMlnV3HXc\nEqwEvxm7gPCvZOP7lFz4nux+oWYf1BPJMNcFTAhYu2a370SNPJLyzeIai6jxrpLL\nX7LE6UPgvfI0COqsDPKOJcEBmx5Tu0EzI2owAWl0YngJJJEXgdjo0t7iChEFIgmo\nuypAKSn1tbzxVyAdBdZkcowWzpaqV3HpBXgOnIlKo5hc7TsQoIG3T4Rt64gbR49a\nmmKJrZ+6um2jUKFX3uQdDzFPFPGE7Z/eMheStpaFRHqV2zea2AyT95AoI/EH/ouM\nu1+9bjWtJHMjrBJrl68u248LT7wmEcJ//SjtbcXP4mqPutueH/u3pmZvXzprADg=\n=Sccb\n-----END PGP SIGNATURE-----\n", "payload": "tree f5e174193f2df91d0a98901333bfe9d1a4c1ec8d\nparent b183ef2068cf18ff43e119cfbbdafb14268dddb1\nparent b3be11efbdd274b0dfd94c720cf6e396cab98c33\nauthor Dylan DPC <dylan.dpc@gmail.com> 1602807017 +0200\ncommitter GitHub <noreply@github.com> 1602807017 +0200\n\nRollup merge of #77657 - fusion-engineering-forks:cleanup-cloudabi-sync, r=dtolnay\n\nCleanup cloudabi mutexes and condvars\n\nThis gets rid of lots of unnecessary unsafety.\n\nAll the AtomicU32s were wrapped in UnsafeCell or UnsafeCell<MaybeUninit>, and raw pointers were used to get to the AtomicU32 inside. This change cleans that up by using AtomicU32 directly.\n\nAlso replaces a UnsafeCell<u32> by a safer Cell<u32>.\n\n@rustbot modify labels: +C-cleanup\n"}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/9b8c0eb107f75cf154b814cee10113c0359fefcf", "html_url": "https://github.com/rust-lang/rust/commit/9b8c0eb107f75cf154b814cee10113c0359fefcf", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/9b8c0eb107f75cf154b814cee10113c0359fefcf/comments", "author": {"login": "Dylan-DPC", "id": 99973273, "node_id": "U_kgDOBfV4mQ", "avatar_url": "https://avatars.githubusercontent.com/u/99973273?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Dylan-DPC", "html_url": "https://github.com/Dylan-DPC", "followers_url": "https://api.github.com/users/Dylan-DPC/followers", "following_url": "https://api.github.com/users/Dylan-DPC/following{/other_user}", "gists_url": "https://api.github.com/users/Dylan-DPC/gists{/gist_id}", "starred_url": "https://api.github.com/users/Dylan-DPC/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Dylan-DPC/subscriptions", "organizations_url": "https://api.github.com/users/Dylan-DPC/orgs", "repos_url": "https://api.github.com/users/Dylan-DPC/repos", "events_url": "https://api.github.com/users/Dylan-DPC/events{/privacy}", "received_events_url": "https://api.github.com/users/Dylan-DPC/received_events", "type": "User", "site_admin": false}, "committer": {"login": "web-flow", "id": 19864447, "node_id": "MDQ6VXNlcjE5ODY0NDQ3", "avatar_url": "https://avatars.githubusercontent.com/u/19864447?v=4", "gravatar_id": "", "url": "https://api.github.com/users/web-flow", "html_url": "https://github.com/web-flow", "followers_url": "https://api.github.com/users/web-flow/followers", "following_url": "https://api.github.com/users/web-flow/following{/other_user}", "gists_url": "https://api.github.com/users/web-flow/gists{/gist_id}", "starred_url": "https://api.github.com/users/web-flow/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/web-flow/subscriptions", "organizations_url": "https://api.github.com/users/web-flow/orgs", "repos_url": "https://api.github.com/users/web-flow/repos", "events_url": "https://api.github.com/users/web-flow/events{/privacy}", "received_events_url": "https://api.github.com/users/web-flow/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "b183ef2068cf18ff43e119cfbbdafb14268dddb1", "url": "https://api.github.com/repos/rust-lang/rust/commits/b183ef2068cf18ff43e119cfbbdafb14268dddb1", "html_url": "https://github.com/rust-lang/rust/commit/b183ef2068cf18ff43e119cfbbdafb14268dddb1"}, {"sha": "b3be11efbdd274b0dfd94c720cf6e396cab98c33", "url": "https://api.github.com/repos/rust-lang/rust/commits/b3be11efbdd274b0dfd94c720cf6e396cab98c33", "html_url": "https://github.com/rust-lang/rust/commit/b3be11efbdd274b0dfd94c720cf6e396cab98c33"}], "stats": {"total": 142, "additions": 65, "deletions": 77}, "files": [{"sha": "f09bc01701b748addda915e81d794bb3130bcb59", "filename": "library/std/src/sys/cloudabi/condvar.rs", "status": "modified", "additions": 18, "deletions": 23, "changes": 41, "blob_url": "https://github.com/rust-lang/rust/blob/9b8c0eb107f75cf154b814cee10113c0359fefcf/library%2Fstd%2Fsrc%2Fsys%2Fcloudabi%2Fcondvar.rs", "raw_url": "https://github.com/rust-lang/rust/raw/9b8c0eb107f75cf154b814cee10113c0359fefcf/library%2Fstd%2Fsrc%2Fsys%2Fcloudabi%2Fcondvar.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/library%2Fstd%2Fsrc%2Fsys%2Fcloudabi%2Fcondvar.rs?ref=9b8c0eb107f75cf154b814cee10113c0359fefcf", "patch": "@@ -1,4 +1,3 @@\n-use crate::cell::UnsafeCell;\n use crate::mem;\n use crate::sync::atomic::{AtomicU32, Ordering};\n use crate::sys::cloudabi::abi;\n@@ -12,37 +11,36 @@ extern \"C\" {\n }\n \n pub struct Condvar {\n-    condvar: UnsafeCell<AtomicU32>,\n+    condvar: AtomicU32,\n }\n \n pub type MovableCondvar = Condvar;\n \n unsafe impl Send for Condvar {}\n unsafe impl Sync for Condvar {}\n \n-const NEW: Condvar =\n-    Condvar { condvar: UnsafeCell::new(AtomicU32::new(abi::CONDVAR_HAS_NO_WAITERS.0)) };\n-\n impl Condvar {\n     pub const fn new() -> Condvar {\n-        NEW\n+        Condvar { condvar: AtomicU32::new(abi::CONDVAR_HAS_NO_WAITERS.0) }\n     }\n \n     pub unsafe fn init(&mut self) {}\n \n     pub unsafe fn notify_one(&self) {\n-        let condvar = self.condvar.get();\n-        if (*condvar).load(Ordering::Relaxed) != abi::CONDVAR_HAS_NO_WAITERS.0 {\n-            let ret = abi::condvar_signal(condvar as *mut abi::condvar, abi::scope::PRIVATE, 1);\n+        if self.condvar.load(Ordering::Relaxed) != abi::CONDVAR_HAS_NO_WAITERS.0 {\n+            let ret = abi::condvar_signal(\n+                &self.condvar as *const AtomicU32 as *mut abi::condvar,\n+                abi::scope::PRIVATE,\n+                1,\n+            );\n             assert_eq!(ret, abi::errno::SUCCESS, \"Failed to signal on condition variable\");\n         }\n     }\n \n     pub unsafe fn notify_all(&self) {\n-        let condvar = self.condvar.get();\n-        if (*condvar).load(Ordering::Relaxed) != abi::CONDVAR_HAS_NO_WAITERS.0 {\n+        if self.condvar.load(Ordering::Relaxed) != abi::CONDVAR_HAS_NO_WAITERS.0 {\n             let ret = abi::condvar_signal(\n-                condvar as *mut abi::condvar,\n+                &self.condvar as *const AtomicU32 as *mut abi::condvar,\n                 abi::scope::PRIVATE,\n                 abi::nthreads::MAX,\n             );\n@@ -53,20 +51,19 @@ impl Condvar {\n     pub unsafe fn wait(&self, mutex: &Mutex) {\n         let mutex = mutex::raw(mutex);\n         assert_eq!(\n-            (*mutex).load(Ordering::Relaxed) & !abi::LOCK_KERNEL_MANAGED.0,\n+            mutex.load(Ordering::Relaxed) & !abi::LOCK_KERNEL_MANAGED.0,\n             __pthread_thread_id.0 | abi::LOCK_WRLOCKED.0,\n             \"This lock is not write-locked by this thread\"\n         );\n \n         // Call into the kernel to wait on the condition variable.\n-        let condvar = self.condvar.get();\n         let subscription = abi::subscription {\n             type_: abi::eventtype::CONDVAR,\n             union: abi::subscription_union {\n                 condvar: abi::subscription_condvar {\n-                    condvar: condvar as *mut abi::condvar,\n+                    condvar: &self.condvar as *const AtomicU32 as *mut abi::condvar,\n                     condvar_scope: abi::scope::PRIVATE,\n-                    lock: mutex as *mut abi::lock,\n+                    lock: mutex as *const AtomicU32 as *mut abi::lock,\n                     lock_scope: abi::scope::PRIVATE,\n                 },\n             },\n@@ -86,23 +83,22 @@ impl Condvar {\n     pub unsafe fn wait_timeout(&self, mutex: &Mutex, dur: Duration) -> bool {\n         let mutex = mutex::raw(mutex);\n         assert_eq!(\n-            (*mutex).load(Ordering::Relaxed) & !abi::LOCK_KERNEL_MANAGED.0,\n+            mutex.load(Ordering::Relaxed) & !abi::LOCK_KERNEL_MANAGED.0,\n             __pthread_thread_id.0 | abi::LOCK_WRLOCKED.0,\n             \"This lock is not write-locked by this thread\"\n         );\n \n         // Call into the kernel to wait on the condition variable.\n-        let condvar = self.condvar.get();\n         let timeout =\n             checked_dur2intervals(&dur).expect(\"overflow converting duration to nanoseconds\");\n         let subscriptions = [\n             abi::subscription {\n                 type_: abi::eventtype::CONDVAR,\n                 union: abi::subscription_union {\n                     condvar: abi::subscription_condvar {\n-                        condvar: condvar as *mut abi::condvar,\n+                        condvar: &self.condvar as *const AtomicU32 as *mut abi::condvar,\n                         condvar_scope: abi::scope::PRIVATE,\n-                        lock: mutex as *mut abi::lock,\n+                        lock: mutex as *const AtomicU32 as *mut abi::lock,\n                         lock_scope: abi::scope::PRIVATE,\n                     },\n                 },\n@@ -124,7 +120,7 @@ impl Condvar {\n         let mut nevents: mem::MaybeUninit<usize> = mem::MaybeUninit::uninit();\n         let ret = abi::poll(\n             subscriptions.as_ptr(),\n-            mem::MaybeUninit::first_ptr_mut(&mut events),\n+            mem::MaybeUninit::slice_as_mut_ptr(&mut events),\n             2,\n             nevents.as_mut_ptr(),\n         );\n@@ -144,9 +140,8 @@ impl Condvar {\n     }\n \n     pub unsafe fn destroy(&self) {\n-        let condvar = self.condvar.get();\n         assert_eq!(\n-            (*condvar).load(Ordering::Relaxed),\n+            self.condvar.load(Ordering::Relaxed),\n             abi::CONDVAR_HAS_NO_WAITERS.0,\n             \"Attempted to destroy a condition variable with blocked threads\"\n         );"}, {"sha": "1203d8de0c572617982342cfe526c94a22fc58fd", "filename": "library/std/src/sys/cloudabi/mutex.rs", "status": "modified", "additions": 25, "deletions": 29, "changes": 54, "blob_url": "https://github.com/rust-lang/rust/blob/9b8c0eb107f75cf154b814cee10113c0359fefcf/library%2Fstd%2Fsrc%2Fsys%2Fcloudabi%2Fmutex.rs", "raw_url": "https://github.com/rust-lang/rust/raw/9b8c0eb107f75cf154b814cee10113c0359fefcf/library%2Fstd%2Fsrc%2Fsys%2Fcloudabi%2Fmutex.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/library%2Fstd%2Fsrc%2Fsys%2Fcloudabi%2Fmutex.rs?ref=9b8c0eb107f75cf154b814cee10113c0359fefcf", "patch": "@@ -1,4 +1,4 @@\n-use crate::cell::UnsafeCell;\n+use crate::cell::Cell;\n use crate::mem;\n use crate::mem::MaybeUninit;\n use crate::sync::atomic::{AtomicU32, Ordering};\n@@ -17,7 +17,7 @@ pub struct Mutex(RWLock);\n \n pub type MovableMutex = Mutex;\n \n-pub unsafe fn raw(m: &Mutex) -> *mut AtomicU32 {\n+pub unsafe fn raw(m: &Mutex) -> &AtomicU32 {\n     rwlock::raw(&m.0)\n }\n \n@@ -50,28 +50,23 @@ impl Mutex {\n }\n \n pub struct ReentrantMutex {\n-    lock: UnsafeCell<MaybeUninit<AtomicU32>>,\n-    recursion: UnsafeCell<MaybeUninit<u32>>,\n+    lock: AtomicU32,\n+    recursion: Cell<u32>,\n }\n \n+unsafe impl Send for ReentrantMutex {}\n+unsafe impl Sync for ReentrantMutex {}\n+\n impl ReentrantMutex {\n     pub const unsafe fn uninitialized() -> ReentrantMutex {\n-        ReentrantMutex {\n-            lock: UnsafeCell::new(MaybeUninit::uninit()),\n-            recursion: UnsafeCell::new(MaybeUninit::uninit()),\n-        }\n+        ReentrantMutex { lock: AtomicU32::new(abi::LOCK_UNLOCKED.0), recursion: Cell::new(0) }\n     }\n \n-    pub unsafe fn init(&self) {\n-        *self.lock.get() = MaybeUninit::new(AtomicU32::new(abi::LOCK_UNLOCKED.0));\n-        *self.recursion.get() = MaybeUninit::new(0);\n-    }\n+    pub unsafe fn init(&self) {}\n \n     pub unsafe fn try_lock(&self) -> bool {\n         // Attempt to acquire the lock.\n-        let lock = (*self.lock.get()).as_mut_ptr();\n-        let recursion = (*self.recursion.get()).as_mut_ptr();\n-        if let Err(old) = (*lock).compare_exchange(\n+        if let Err(old) = self.lock.compare_exchange(\n             abi::LOCK_UNLOCKED.0,\n             __pthread_thread_id.0 | abi::LOCK_WRLOCKED.0,\n             Ordering::Acquire,\n@@ -80,22 +75,22 @@ impl ReentrantMutex {\n             // If we fail to acquire the lock, it may be the case\n             // that we've already acquired it and may need to recurse.\n             if old & !abi::LOCK_KERNEL_MANAGED.0 == __pthread_thread_id.0 | abi::LOCK_WRLOCKED.0 {\n-                *recursion += 1;\n+                self.recursion.set(self.recursion.get() + 1);\n                 true\n             } else {\n                 false\n             }\n         } else {\n             // Success.\n-            assert_eq!(*recursion, 0, \"Mutex has invalid recursion count\");\n+            assert_eq!(self.recursion.get(), 0, \"Mutex has invalid recursion count\");\n             true\n         }\n     }\n \n     pub unsafe fn lock(&self) {\n         if !self.try_lock() {\n             // Call into the kernel to acquire a write lock.\n-            let lock = self.lock.get();\n+            let lock = &self.lock as *const AtomicU32;\n             let subscription = abi::subscription {\n                 type_: abi::eventtype::LOCK_WRLOCK,\n                 union: abi::subscription_union {\n@@ -116,17 +111,17 @@ impl ReentrantMutex {\n     }\n \n     pub unsafe fn unlock(&self) {\n-        let lock = (*self.lock.get()).as_mut_ptr();\n-        let recursion = (*self.recursion.get()).as_mut_ptr();\n         assert_eq!(\n-            (*lock).load(Ordering::Relaxed) & !abi::LOCK_KERNEL_MANAGED.0,\n+            self.lock.load(Ordering::Relaxed) & !abi::LOCK_KERNEL_MANAGED.0,\n             __pthread_thread_id.0 | abi::LOCK_WRLOCKED.0,\n             \"This mutex is locked by a different thread\"\n         );\n \n-        if *recursion > 0 {\n-            *recursion -= 1;\n-        } else if !(*lock)\n+        let r = self.recursion.get();\n+        if r > 0 {\n+            self.recursion.set(r - 1);\n+        } else if !self\n+            .lock\n             .compare_exchange(\n                 __pthread_thread_id.0 | abi::LOCK_WRLOCKED.0,\n                 abi::LOCK_UNLOCKED.0,\n@@ -137,19 +132,20 @@ impl ReentrantMutex {\n         {\n             // Lock is managed by kernelspace. Call into the kernel\n             // to unblock waiting threads.\n-            let ret = abi::lock_unlock(lock as *mut abi::lock, abi::scope::PRIVATE);\n+            let ret = abi::lock_unlock(\n+                &self.lock as *const AtomicU32 as *mut abi::lock,\n+                abi::scope::PRIVATE,\n+            );\n             assert_eq!(ret, abi::errno::SUCCESS, \"Failed to unlock a mutex\");\n         }\n     }\n \n     pub unsafe fn destroy(&self) {\n-        let lock = (*self.lock.get()).as_mut_ptr();\n-        let recursion = (*self.recursion.get()).as_mut_ptr();\n         assert_eq!(\n-            (*lock).load(Ordering::Relaxed),\n+            self.lock.load(Ordering::Relaxed),\n             abi::LOCK_UNLOCKED.0,\n             \"Attempted to destroy locked mutex\"\n         );\n-        assert_eq!(*recursion, 0, \"Recursion counter invalid\");\n+        assert_eq!(self.recursion.get(), 0, \"Recursion counter invalid\");\n     }\n }"}, {"sha": "508de8ba47c6e2632f48bbb7333bef08dbfcf5e4", "filename": "library/std/src/sys/cloudabi/rwlock.rs", "status": "modified", "additions": 22, "deletions": 25, "changes": 47, "blob_url": "https://github.com/rust-lang/rust/blob/9b8c0eb107f75cf154b814cee10113c0359fefcf/library%2Fstd%2Fsrc%2Fsys%2Fcloudabi%2Frwlock.rs", "raw_url": "https://github.com/rust-lang/rust/raw/9b8c0eb107f75cf154b814cee10113c0359fefcf/library%2Fstd%2Fsrc%2Fsys%2Fcloudabi%2Frwlock.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/library%2Fstd%2Fsrc%2Fsys%2Fcloudabi%2Frwlock.rs?ref=9b8c0eb107f75cf154b814cee10113c0359fefcf", "patch": "@@ -1,4 +1,3 @@\n-use crate::cell::UnsafeCell;\n use crate::mem;\n use crate::mem::MaybeUninit;\n use crate::sync::atomic::{AtomicU32, Ordering};\n@@ -13,28 +12,25 @@ extern \"C\" {\n static mut RDLOCKS_ACQUIRED: u32 = 0;\n \n pub struct RWLock {\n-    lock: UnsafeCell<AtomicU32>,\n+    lock: AtomicU32,\n }\n \n-pub unsafe fn raw(r: &RWLock) -> *mut AtomicU32 {\n-    r.lock.get()\n+pub unsafe fn raw(r: &RWLock) -> &AtomicU32 {\n+    &r.lock\n }\n \n unsafe impl Send for RWLock {}\n unsafe impl Sync for RWLock {}\n \n-const NEW: RWLock = RWLock { lock: UnsafeCell::new(AtomicU32::new(abi::LOCK_UNLOCKED.0)) };\n-\n impl RWLock {\n     pub const fn new() -> RWLock {\n-        NEW\n+        RWLock { lock: AtomicU32::new(abi::LOCK_UNLOCKED.0) }\n     }\n \n     pub unsafe fn try_read(&self) -> bool {\n-        let lock = self.lock.get();\n         let mut old = abi::LOCK_UNLOCKED.0;\n         while let Err(cur) =\n-            (*lock).compare_exchange_weak(old, old + 1, Ordering::Acquire, Ordering::Relaxed)\n+            self.lock.compare_exchange_weak(old, old + 1, Ordering::Acquire, Ordering::Relaxed)\n         {\n             if (cur & abi::LOCK_WRLOCKED.0) != 0 {\n                 // Another thread already has a write lock.\n@@ -61,12 +57,11 @@ impl RWLock {\n     pub unsafe fn read(&self) {\n         if !self.try_read() {\n             // Call into the kernel to acquire a read lock.\n-            let lock = self.lock.get();\n             let subscription = abi::subscription {\n                 type_: abi::eventtype::LOCK_RDLOCK,\n                 union: abi::subscription_union {\n                     lock: abi::subscription_lock {\n-                        lock: lock as *mut abi::lock,\n+                        lock: &self.lock as *const AtomicU32 as *mut abi::lock,\n                         lock_scope: abi::scope::PRIVATE,\n                     },\n                 },\n@@ -96,11 +91,10 @@ impl RWLock {\n         assert!(RDLOCKS_ACQUIRED > 0, \"Bad lock count\");\n         let mut old = 1;\n         loop {\n-            let lock = self.lock.get();\n             if old == 1 | abi::LOCK_KERNEL_MANAGED.0 {\n                 // Last read lock while threads are waiting. Attempt to upgrade\n                 // to a write lock before calling into the kernel to unlock.\n-                if let Err(cur) = (*lock).compare_exchange_weak(\n+                if let Err(cur) = self.lock.compare_exchange_weak(\n                     old,\n                     __pthread_thread_id.0 | abi::LOCK_WRLOCKED.0 | abi::LOCK_KERNEL_MANAGED.0,\n                     Ordering::Acquire,\n@@ -109,7 +103,10 @@ impl RWLock {\n                     old = cur;\n                 } else {\n                     // Call into the kernel to unlock.\n-                    let ret = abi::lock_unlock(lock as *mut abi::lock, abi::scope::PRIVATE);\n+                    let ret = abi::lock_unlock(\n+                        &self.lock as *const AtomicU32 as *mut abi::lock,\n+                        abi::scope::PRIVATE,\n+                    );\n                     assert_eq!(ret, abi::errno::SUCCESS, \"Failed to write unlock a rwlock\");\n                     break;\n                 }\n@@ -122,7 +119,7 @@ impl RWLock {\n                     0,\n                     \"Attempted to read-unlock a write-locked rwlock\"\n                 );\n-                if let Err(cur) = (*lock).compare_exchange_weak(\n+                if let Err(cur) = self.lock.compare_exchange_weak(\n                     old,\n                     old - 1,\n                     Ordering::Acquire,\n@@ -140,8 +137,7 @@ impl RWLock {\n \n     pub unsafe fn try_write(&self) -> bool {\n         // Attempt to acquire the lock.\n-        let lock = self.lock.get();\n-        if let Err(old) = (*lock).compare_exchange(\n+        if let Err(old) = self.lock.compare_exchange(\n             abi::LOCK_UNLOCKED.0,\n             __pthread_thread_id.0 | abi::LOCK_WRLOCKED.0,\n             Ordering::Acquire,\n@@ -163,12 +159,11 @@ impl RWLock {\n     pub unsafe fn write(&self) {\n         if !self.try_write() {\n             // Call into the kernel to acquire a write lock.\n-            let lock = self.lock.get();\n             let subscription = abi::subscription {\n                 type_: abi::eventtype::LOCK_WRLOCK,\n                 union: abi::subscription_union {\n                     lock: abi::subscription_lock {\n-                        lock: lock as *mut abi::lock,\n+                        lock: &self.lock as *const AtomicU32 as *mut abi::lock,\n                         lock_scope: abi::scope::PRIVATE,\n                     },\n                 },\n@@ -184,14 +179,14 @@ impl RWLock {\n     }\n \n     pub unsafe fn write_unlock(&self) {\n-        let lock = self.lock.get();\n         assert_eq!(\n-            (*lock).load(Ordering::Relaxed) & !abi::LOCK_KERNEL_MANAGED.0,\n+            self.lock.load(Ordering::Relaxed) & !abi::LOCK_KERNEL_MANAGED.0,\n             __pthread_thread_id.0 | abi::LOCK_WRLOCKED.0,\n             \"This rwlock is not write-locked by this thread\"\n         );\n \n-        if !(*lock)\n+        if !self\n+            .lock\n             .compare_exchange(\n                 __pthread_thread_id.0 | abi::LOCK_WRLOCKED.0,\n                 abi::LOCK_UNLOCKED.0,\n@@ -202,15 +197,17 @@ impl RWLock {\n         {\n             // Lock is managed by kernelspace. Call into the kernel\n             // to unblock waiting threads.\n-            let ret = abi::lock_unlock(lock as *mut abi::lock, abi::scope::PRIVATE);\n+            let ret = abi::lock_unlock(\n+                &self.lock as *const AtomicU32 as *mut abi::lock,\n+                abi::scope::PRIVATE,\n+            );\n             assert_eq!(ret, abi::errno::SUCCESS, \"Failed to write unlock a rwlock\");\n         }\n     }\n \n     pub unsafe fn destroy(&self) {\n-        let lock = self.lock.get();\n         assert_eq!(\n-            (*lock).load(Ordering::Relaxed),\n+            self.lock.load(Ordering::Relaxed),\n             abi::LOCK_UNLOCKED.0,\n             \"Attempted to destroy locked rwlock\"\n         );"}]}