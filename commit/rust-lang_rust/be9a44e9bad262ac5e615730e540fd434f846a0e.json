{"sha": "be9a44e9bad262ac5e615730e540fd434f846a0e", "node_id": "MDY6Q29tbWl0NzI0NzEyOmJlOWE0NGU5YmFkMjYyYWM1ZTYxNTczMGU1NDBmZDQzNGY4NDZhMGU=", "commit": {"author": {"name": "bors[bot]", "email": "bors[bot]@users.noreply.github.com", "date": "2019-04-05T08:55:50Z"}, "committer": {"name": "bors[bot]", "email": "bors[bot]@users.noreply.github.com", "date": "2019-04-05T08:55:50Z"}, "message": "Merge #1111\n\n1111: Add multi-byte token support in token tree to ast item list r=matklad a=edwin0cheng\n\nAs discusion in https://github.com/rust-analyzer/rust-analyzer/pull/1105 ,  this PR add implement all multi-byte tokens in `ra_mbe` crate.\r\n\r\n\r\n\n\nCo-authored-by: Edwin Cheng <edwin0cheng@gmail.com>", "tree": {"sha": "7082f2b1398f8481a5a583a8c499d9b931c5e590", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/7082f2b1398f8481a5a583a8c499d9b931c5e590"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/be9a44e9bad262ac5e615730e540fd434f846a0e", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/be9a44e9bad262ac5e615730e540fd434f846a0e", "html_url": "https://github.com/rust-lang/rust/commit/be9a44e9bad262ac5e615730e540fd434f846a0e", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/be9a44e9bad262ac5e615730e540fd434f846a0e/comments", "author": {"login": "bors[bot]", "id": 26634292, "node_id": "MDM6Qm90MjY2MzQyOTI=", "avatar_url": "https://avatars.githubusercontent.com/in/1847?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bors%5Bbot%5D", "html_url": "https://github.com/apps/bors", "followers_url": "https://api.github.com/users/bors%5Bbot%5D/followers", "following_url": "https://api.github.com/users/bors%5Bbot%5D/following{/other_user}", "gists_url": "https://api.github.com/users/bors%5Bbot%5D/gists{/gist_id}", "starred_url": "https://api.github.com/users/bors%5Bbot%5D/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bors%5Bbot%5D/subscriptions", "organizations_url": "https://api.github.com/users/bors%5Bbot%5D/orgs", "repos_url": "https://api.github.com/users/bors%5Bbot%5D/repos", "events_url": "https://api.github.com/users/bors%5Bbot%5D/events{/privacy}", "received_events_url": "https://api.github.com/users/bors%5Bbot%5D/received_events", "type": "Bot", "site_admin": false}, "committer": {"login": "bors[bot]", "id": 26634292, "node_id": "MDM6Qm90MjY2MzQyOTI=", "avatar_url": "https://avatars.githubusercontent.com/in/1847?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bors%5Bbot%5D", "html_url": "https://github.com/apps/bors", "followers_url": "https://api.github.com/users/bors%5Bbot%5D/followers", "following_url": "https://api.github.com/users/bors%5Bbot%5D/following{/other_user}", "gists_url": "https://api.github.com/users/bors%5Bbot%5D/gists{/gist_id}", "starred_url": "https://api.github.com/users/bors%5Bbot%5D/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bors%5Bbot%5D/subscriptions", "organizations_url": "https://api.github.com/users/bors%5Bbot%5D/orgs", "repos_url": "https://api.github.com/users/bors%5Bbot%5D/repos", "events_url": "https://api.github.com/users/bors%5Bbot%5D/events{/privacy}", "received_events_url": "https://api.github.com/users/bors%5Bbot%5D/received_events", "type": "Bot", "site_admin": false}, "parents": [{"sha": "7713416477fd59348ad60d44f0ec3a3aebcf4b9f", "url": "https://api.github.com/repos/rust-lang/rust/commits/7713416477fd59348ad60d44f0ec3a3aebcf4b9f", "html_url": "https://github.com/rust-lang/rust/commit/7713416477fd59348ad60d44f0ec3a3aebcf4b9f"}, {"sha": "6ff16c7ad9c1b72c4f04bb0d6c7fc0fdc2cc00cb", "url": "https://api.github.com/repos/rust-lang/rust/commits/6ff16c7ad9c1b72c4f04bb0d6c7fc0fdc2cc00cb", "html_url": "https://github.com/rust-lang/rust/commit/6ff16c7ad9c1b72c4f04bb0d6c7fc0fdc2cc00cb"}], "stats": {"total": 159, "additions": 139, "deletions": 20}, "files": [{"sha": "a1431282160d56960b933d066f210babf115d529", "filename": "Cargo.lock", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "blob_url": "https://github.com/rust-lang/rust/blob/be9a44e9bad262ac5e615730e540fd434f846a0e/Cargo.lock", "raw_url": "https://github.com/rust-lang/rust/raw/be9a44e9bad262ac5e615730e540fd434f846a0e/Cargo.lock", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/Cargo.lock?ref=be9a44e9bad262ac5e615730e540fd434f846a0e", "patch": "@@ -1040,6 +1040,7 @@ dependencies = [\n name = \"ra_mbe\"\n version = \"0.1.0\"\n dependencies = [\n+ \"itertools 0.8.0 (registry+https://github.com/rust-lang/crates.io-index)\",\n  \"ra_parser 0.1.0\",\n  \"ra_syntax 0.1.0\",\n  \"ra_tt 0.1.0\","}, {"sha": "1d0c2a340ffa80b183d42ac3a6ed1af335814e07", "filename": "crates/ra_mbe/Cargo.toml", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/rust-lang/rust/blob/be9a44e9bad262ac5e615730e540fd434f846a0e/crates%2Fra_mbe%2FCargo.toml", "raw_url": "https://github.com/rust-lang/rust/raw/be9a44e9bad262ac5e615730e540fd434f846a0e/crates%2Fra_mbe%2FCargo.toml", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fra_mbe%2FCargo.toml?ref=be9a44e9bad262ac5e615730e540fd434f846a0e", "patch": "@@ -8,5 +8,5 @@ authors = [\"rust-analyzer developers\"]\n ra_syntax = { path = \"../ra_syntax\" }\n ra_parser = { path = \"../ra_parser\" }\n tt = { path = \"../ra_tt\", package = \"ra_tt\" }\n-\n+itertools = \"0.8.0\"\n rustc-hash = \"1.0.0\""}, {"sha": "257503de83025cc95ee83299cd3819eaf130bc0d", "filename": "crates/ra_mbe/src/syntax_bridge.rs", "status": "modified", "additions": 137, "deletions": 19, "changes": 156, "blob_url": "https://github.com/rust-lang/rust/blob/be9a44e9bad262ac5e615730e540fd434f846a0e/crates%2Fra_mbe%2Fsrc%2Fsyntax_bridge.rs", "raw_url": "https://github.com/rust-lang/rust/raw/be9a44e9bad262ac5e615730e540fd434f846a0e/crates%2Fra_mbe%2Fsrc%2Fsyntax_bridge.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fra_mbe%2Fsrc%2Fsyntax_bridge.rs?ref=be9a44e9bad262ac5e615730e540fd434f846a0e", "patch": "@@ -113,6 +113,51 @@ struct TtToken {\n     text: SmolStr,\n }\n \n+// Some helper functions\n+fn to_punct(tt: &tt::TokenTree) -> Option<&tt::Punct> {\n+    if let tt::TokenTree::Leaf(tt::Leaf::Punct(pp)) = tt {\n+        return Some(pp);\n+    }\n+    None\n+}\n+\n+struct TokenPeek<'a, I>\n+where\n+    I: Iterator<Item = &'a tt::TokenTree>,\n+{\n+    iter: itertools::MultiPeek<I>,\n+}\n+\n+impl<'a, I> TokenPeek<'a, I>\n+where\n+    I: Iterator<Item = &'a tt::TokenTree>,\n+{\n+    fn next(&mut self) -> Option<&tt::TokenTree> {\n+        self.iter.next()\n+    }\n+\n+    fn current_punct2(&mut self, p: &tt::Punct) -> Option<((char, char), bool)> {\n+        if p.spacing != tt::Spacing::Joint {\n+            return None;\n+        }\n+\n+        self.iter.reset_peek();\n+        let p1 = to_punct(self.iter.peek()?)?;\n+        Some(((p.char, p1.char), p1.spacing == tt::Spacing::Joint))\n+    }\n+\n+    fn current_punct3(&mut self, p: &tt::Punct) -> Option<((char, char, char), bool)> {\n+        self.current_punct2(p).and_then(|((p0, p1), last_joint)| {\n+            if !last_joint {\n+                None\n+            } else {\n+                let p2 = to_punct(*self.iter.peek()?)?;\n+                Some(((p0, p1, p2.char), p2.spacing == tt::Spacing::Joint))\n+            }\n+        })\n+    }\n+}\n+\n impl TtTokenSource {\n     fn new(tt: &tt::Subtree) -> TtTokenSource {\n         let mut res = TtTokenSource { tokens: Vec::new() };\n@@ -121,38 +166,53 @@ impl TtTokenSource {\n     }\n     fn convert_subtree(&mut self, sub: &tt::Subtree) {\n         self.push_delim(sub.delimiter, false);\n-        sub.token_trees.iter().for_each(|tt| self.convert_tt(tt));\n+        let mut peek = TokenPeek { iter: itertools::multipeek(sub.token_trees.iter()) };\n+        while let Some(tt) = peek.iter.next() {\n+            self.convert_tt(tt, &mut peek);\n+        }\n         self.push_delim(sub.delimiter, true)\n     }\n-    fn convert_tt(&mut self, tt: &tt::TokenTree) {\n+\n+    fn convert_tt<'a, I>(&mut self, tt: &tt::TokenTree, iter: &mut TokenPeek<'a, I>)\n+    where\n+        I: Iterator<Item = &'a tt::TokenTree>,\n+    {\n         match tt {\n-            tt::TokenTree::Leaf(token) => self.convert_token(token),\n+            tt::TokenTree::Leaf(token) => self.convert_token(token, iter),\n             tt::TokenTree::Subtree(sub) => self.convert_subtree(sub),\n         }\n     }\n-    fn convert_token(&mut self, token: &tt::Leaf) {\n+\n+    fn convert_token<'a, I>(&mut self, token: &tt::Leaf, iter: &mut TokenPeek<'a, I>)\n+    where\n+        I: Iterator<Item = &'a tt::TokenTree>,\n+    {\n         let tok = match token {\n             tt::Leaf::Literal(l) => TtToken {\n                 kind: SyntaxKind::INT_NUMBER, // FIXME\n                 is_joint_to_next: false,\n                 text: l.text.clone(),\n             },\n             tt::Leaf::Punct(p) => {\n-                let kind = match p.char {\n-                    // lexer may produce combpund tokens for these ones\n-                    '.' => DOT,\n-                    ':' => COLON,\n-                    '=' => EQ,\n-                    '!' => EXCL,\n-                    '-' => MINUS,\n-                    c => SyntaxKind::from_char(c).unwrap(),\n-                };\n-                let text = {\n-                    let mut buf = [0u8; 4];\n-                    let s: &str = p.char.encode_utf8(&mut buf);\n-                    SmolStr::new(s)\n-                };\n-                TtToken { kind, is_joint_to_next: p.spacing == tt::Spacing::Joint, text }\n+                if let Some(tt) = Self::convert_multi_char_punct(p, iter) {\n+                    tt\n+                } else {\n+                    let kind = match p.char {\n+                        // lexer may produce combpund tokens for these ones\n+                        '.' => DOT,\n+                        ':' => COLON,\n+                        '=' => EQ,\n+                        '!' => EXCL,\n+                        '-' => MINUS,\n+                        c => SyntaxKind::from_char(c).unwrap(),\n+                    };\n+                    let text = {\n+                        let mut buf = [0u8; 4];\n+                        let s: &str = p.char.encode_utf8(&mut buf);\n+                        SmolStr::new(s)\n+                    };\n+                    TtToken { kind, is_joint_to_next: p.spacing == tt::Spacing::Joint, text }\n+                }\n             }\n             tt::Leaf::Ident(ident) => {\n                 let kind = SyntaxKind::from_keyword(ident.text.as_str()).unwrap_or(IDENT);\n@@ -161,6 +221,64 @@ impl TtTokenSource {\n         };\n         self.tokens.push(tok)\n     }\n+\n+    fn convert_multi_char_punct<'a, I>(\n+        p: &tt::Punct,\n+        iter: &mut TokenPeek<'a, I>,\n+    ) -> Option<TtToken>\n+    where\n+        I: Iterator<Item = &'a tt::TokenTree>,\n+    {\n+        if let Some((m, is_joint_to_next)) = iter.current_punct3(p) {\n+            if let Some((kind, text)) = match m {\n+                ('<', '<', '=') => Some((SHLEQ, \"<<=\")),\n+                ('>', '>', '=') => Some((SHREQ, \">>=\")),\n+                ('.', '.', '.') => Some((DOTDOTDOT, \"...\")),\n+                ('.', '.', '=') => Some((DOTDOTEQ, \"..=\")),\n+                _ => None,\n+            } {\n+                iter.next();\n+                iter.next();\n+                return Some(TtToken { kind, is_joint_to_next, text: text.into() });\n+            }\n+        }\n+\n+        if let Some((m, is_joint_to_next)) = iter.current_punct2(p) {\n+            if let Some((kind, text)) = match m {\n+                ('<', '<') => Some((SHL, \"<<\")),\n+                ('>', '>') => Some((SHR, \">>\")),\n+\n+                ('|', '|') => Some((PIPEPIPE, \"||\")),\n+                ('&', '&') => Some((AMPAMP, \"&&\")),\n+                ('%', '=') => Some((PERCENTEQ, \"%=\")),\n+                ('*', '=') => Some((STAREQ, \"*=\")),\n+                ('/', '=') => Some((SLASHEQ, \"/=\")),\n+                ('^', '=') => Some((CARETEQ, \"^=\")),\n+\n+                ('&', '=') => Some((AMPEQ, \"&=\")),\n+                ('|', '=') => Some((PIPEEQ, \"|=\")),\n+                ('-', '=') => Some((MINUSEQ, \"-=\")),\n+                ('+', '=') => Some((PLUSEQ, \"+=\")),\n+                ('>', '=') => Some((GTEQ, \">=\")),\n+                ('<', '=') => Some((LTEQ, \"<=\")),\n+\n+                ('-', '>') => Some((THIN_ARROW, \"->\")),\n+                ('!', '=') => Some((NEQ, \"!=\")),\n+                ('=', '>') => Some((FAT_ARROW, \"=>\")),\n+                ('=', '=') => Some((EQEQ, \"==\")),\n+                ('.', '.') => Some((DOTDOT, \"..\")),\n+                (':', ':') => Some((COLONCOLON, \"::\")),\n+\n+                _ => None,\n+            } {\n+                iter.next();\n+                return Some(TtToken { kind, is_joint_to_next, text: text.into() });\n+            }\n+        }\n+\n+        None\n+    }\n+\n     fn push_delim(&mut self, d: tt::Delimiter, closing: bool) {\n         let (kinds, texts) = match d {\n             tt::Delimiter::Parenthesis => ([L_PAREN, R_PAREN], \"()\"),"}]}