{"sha": "327c54ed02e412013e8ffabc61c71f10c2777fe1", "node_id": "MDY6Q29tbWl0NzI0NzEyOjMyN2M1NGVkMDJlNDEyMDEzZThmZmFiYzYxYzcxZjEwYzI3NzdmZTE=", "commit": {"author": {"name": "Mazdak Farrokhzad", "email": "twingoow@gmail.com", "date": "2019-07-06T20:14:33Z"}, "committer": {"name": "GitHub", "email": "noreply@github.com", "date": "2019-07-06T20:14:33Z"}, "message": "Rollup merge of #60081 - pawroman:cleanup_unicode_script, r=varkor\n\nRefactor unicode.py script\n\nHi, I noticed that the `unicode.py` script used some deprecated escapes in regular expressions. E.g. `\\d`, `\\w`, `\\.` will be illegal in the future without \"raw strings\". This is now fixed. I have also cleaned up the script quite a bit.\n\n## Escape deprecation\n\nOK (note the `r`):\n`re.compile(r\"\\d\")`\n\nDeprecated (from Python 3.6 onwards, see [here][link1] and [here][link2]):\n`re.compile(\"\\d\")`.\n\n[link1]: https://docs.python.org/3.6/whatsnew/3.6.html#deprecated-python-behavior\n[link2]: https://bugs.python.org/issue27364\n\nThis was evident running the script using Python 3.7 like so:\n\n```\n$ python3 -Wall unicode.py\nunicode.py:227: DeprecationWarning: invalid escape sequence \\w\n  re1 = re.compile(\"^ *([0-9A-F]+) *; *(\\w+)\")\nunicode.py:228: DeprecationWarning: invalid escape sequence \\.\n  re2 = re.compile(\"^ *([0-9A-F]+)\\.\\.([0-9A-F]+) *; *(\\w+)\")\nunicode.py:453: DeprecationWarning: invalid escape sequence \\d\n  pattern = \"for Version (\\d+)\\.(\\d+)\\.(\\d+) of the Unicode\"\n```\n\nThe documentation states that\n> A backslash-character pair that is not a valid escape sequence now generates a DeprecationWarning. Although this will eventually become a SyntaxError, that will not be for several Python releases.\n\n## Testing\n\nTo test my changes, I had to add support for choosing the Unicode version to use. The script will default to latest release (which is 12.0.0 at the moment, repo has 11.0.0 checked in).\n\nThe script generates the exact same output for version 11.0.0 with Python 2.7 and 3.7 and no longer generates any deprecation warnings:\n\n```\n$ python3 -Wall unicode.py -v 11.0.0\nUsing Unicode version: 11.0.0\nRegenerated tables.rs.\n$ git diff tables.rs\n$ python2 -Wall unicode.py -v 11.0.0\nUsing Unicode version: 11.0.0\nRegenerated tables.rs.\n$ git diff tables.rs\n$ python2 --version\nPython 2.7.16\n$ python3 --version\nPython 3.7.3\n```\n\n## Extra functionality\n\nFurthermore, the script will check and download the latest Unicode version by default (without the `-v` argument). The `--help` is below:\n\n```\n$ ./unicode.py --help\nusage: unicode.py [-h] [-v VERSION]\n\nRegenerate Unicode tables (tables.rs).\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -v VERSION, --version VERSION\n                        Unicode version to use (if not specified, defaults to\n                        latest available final release).\n```\n\n## Cleanups\n\nI have cleaned up the code quite a bit, with Python best practices and code style in mind. I'm happy to provide more details and rationale for all my changes if the reviewers so desire.\n\nOne externally visible change is that the Unicode data will now be downloaded into `src/libcore/unicode/downloaded` directory suffixed by Unicode version:\n\n```\n$ pwd\n.../rust/src/libcore/unicode\n$ exa -T downloaded/\ndownloaded\n\u251c\u2500\u2500 11.0.0\n\u2502  \u251c\u2500\u2500 DerivedCoreProperties.txt\n\u2502  \u251c\u2500\u2500 DerivedNormalizationProps.txt\n\u2502  \u251c\u2500\u2500 PropList.txt\n\u2502  \u251c\u2500\u2500 ReadMe.txt\n\u2502  \u251c\u2500\u2500 Scripts.txt\n\u2502  \u251c\u2500\u2500 SpecialCasing.txt\n\u2502  \u2514\u2500\u2500 UnicodeData.txt\n\u2514\u2500\u2500 12.0.0\n   \u251c\u2500\u2500 DerivedCoreProperties.txt\n   \u251c\u2500\u2500 DerivedNormalizationProps.txt\n   \u251c\u2500\u2500 PropList.txt\n   \u251c\u2500\u2500 ReadMe.txt\n   \u251c\u2500\u2500 Scripts.txt\n   \u251c\u2500\u2500 SpecialCasing.txt\n   \u2514\u2500\u2500 UnicodeData.txt\n```", "tree": {"sha": "33d08f425344a6cd15c6e213e9020722bd02156e", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/33d08f425344a6cd15c6e213e9020722bd02156e"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/327c54ed02e412013e8ffabc61c71f10c2777fe1", "comment_count": 0, "verification": {"verified": true, "reason": "valid", "signature": "-----BEGIN PGP SIGNATURE-----\n\nwsBcBAABCAAQBQJdIQEqCRBK7hj4Ov3rIwAAdHIIACyMjBYiG5g4rFnmaY7jaKQ5\nJv7JKX8r7n3bh5rthaVGu1nRdKhegOqJbs5nbzdf7dLKNWhuhVTYAtrCQiIMddRI\nIIDlXvnhK7KQSfAYZOYj3x2UShGN/WV5T9Yovd1AA5fKiXP5ws0c5oaYK3DB0k/t\nsdTFYnI0bCSneQRjyQ37xyVih8GtDb2u5RJvLVSFkU1l8g0g+GAPrQyXszSBQeOL\nNIjxqNdjRS/mxUk80lVihbfVtYC7t8xXd/GeBwydA+KhLq/2wKpHJueL3yPQjNey\nUQf9AQOZNwkyoswBASgCmcF3vrvYwv8q7ODh1r27M+4BTL05icFdRcUX54DcSwM=\n=eeSX\n-----END PGP SIGNATURE-----\n", "payload": "tree 33d08f425344a6cd15c6e213e9020722bd02156e\nparent 254f2014954bd66da206232490824975c0c662f1\nparent 2b47a085dd418447f1dd79986df94dd051f27c79\nauthor Mazdak Farrokhzad <twingoow@gmail.com> 1562444073 +0200\ncommitter GitHub <noreply@github.com> 1562444073 +0200\n\nRollup merge of #60081 - pawroman:cleanup_unicode_script, r=varkor\n\nRefactor unicode.py script\n\nHi, I noticed that the `unicode.py` script used some deprecated escapes in regular expressions. E.g. `\\d`, `\\w`, `\\.` will be illegal in the future without \"raw strings\". This is now fixed. I have also cleaned up the script quite a bit.\n\n## Escape deprecation\n\nOK (note the `r`):\n`re.compile(r\"\\d\")`\n\nDeprecated (from Python 3.6 onwards, see [here][link1] and [here][link2]):\n`re.compile(\"\\d\")`.\n\n[link1]: https://docs.python.org/3.6/whatsnew/3.6.html#deprecated-python-behavior\n[link2]: https://bugs.python.org/issue27364\n\nThis was evident running the script using Python 3.7 like so:\n\n```\n$ python3 -Wall unicode.py\nunicode.py:227: DeprecationWarning: invalid escape sequence \\w\n  re1 = re.compile(\"^ *([0-9A-F]+) *; *(\\w+)\")\nunicode.py:228: DeprecationWarning: invalid escape sequence \\.\n  re2 = re.compile(\"^ *([0-9A-F]+)\\.\\.([0-9A-F]+) *; *(\\w+)\")\nunicode.py:453: DeprecationWarning: invalid escape sequence \\d\n  pattern = \"for Version (\\d+)\\.(\\d+)\\.(\\d+) of the Unicode\"\n```\n\nThe documentation states that\n> A backslash-character pair that is not a valid escape sequence now generates a DeprecationWarning. Although this will eventually become a SyntaxError, that will not be for several Python releases.\n\n## Testing\n\nTo test my changes, I had to add support for choosing the Unicode version to use. The script will default to latest release (which is 12.0.0 at the moment, repo has 11.0.0 checked in).\n\nThe script generates the exact same output for version 11.0.0 with Python 2.7 and 3.7 and no longer generates any deprecation warnings:\n\n```\n$ python3 -Wall unicode.py -v 11.0.0\nUsing Unicode version: 11.0.0\nRegenerated tables.rs.\n$ git diff tables.rs\n$ python2 -Wall unicode.py -v 11.0.0\nUsing Unicode version: 11.0.0\nRegenerated tables.rs.\n$ git diff tables.rs\n$ python2 --version\nPython 2.7.16\n$ python3 --version\nPython 3.7.3\n```\n\n## Extra functionality\n\nFurthermore, the script will check and download the latest Unicode version by default (without the `-v` argument). The `--help` is below:\n\n```\n$ ./unicode.py --help\nusage: unicode.py [-h] [-v VERSION]\n\nRegenerate Unicode tables (tables.rs).\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -v VERSION, --version VERSION\n                        Unicode version to use (if not specified, defaults to\n                        latest available final release).\n```\n\n## Cleanups\n\nI have cleaned up the code quite a bit, with Python best practices and code style in mind. I'm happy to provide more details and rationale for all my changes if the reviewers so desire.\n\nOne externally visible change is that the Unicode data will now be downloaded into `src/libcore/unicode/downloaded` directory suffixed by Unicode version:\n\n```\n$ pwd\n.../rust/src/libcore/unicode\n$ exa -T downloaded/\ndownloaded\n\u251c\u2500\u2500 11.0.0\n\u2502  \u251c\u2500\u2500 DerivedCoreProperties.txt\n\u2502  \u251c\u2500\u2500 DerivedNormalizationProps.txt\n\u2502  \u251c\u2500\u2500 PropList.txt\n\u2502  \u251c\u2500\u2500 ReadMe.txt\n\u2502  \u251c\u2500\u2500 Scripts.txt\n\u2502  \u251c\u2500\u2500 SpecialCasing.txt\n\u2502  \u2514\u2500\u2500 UnicodeData.txt\n\u2514\u2500\u2500 12.0.0\n   \u251c\u2500\u2500 DerivedCoreProperties.txt\n   \u251c\u2500\u2500 DerivedNormalizationProps.txt\n   \u251c\u2500\u2500 PropList.txt\n   \u251c\u2500\u2500 ReadMe.txt\n   \u251c\u2500\u2500 Scripts.txt\n   \u251c\u2500\u2500 SpecialCasing.txt\n   \u2514\u2500\u2500 UnicodeData.txt\n```\n"}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/327c54ed02e412013e8ffabc61c71f10c2777fe1", "html_url": "https://github.com/rust-lang/rust/commit/327c54ed02e412013e8ffabc61c71f10c2777fe1", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/327c54ed02e412013e8ffabc61c71f10c2777fe1/comments", "author": {"login": "Centril", "id": 855702, "node_id": "MDQ6VXNlcjg1NTcwMg==", "avatar_url": "https://avatars.githubusercontent.com/u/855702?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Centril", "html_url": "https://github.com/Centril", "followers_url": "https://api.github.com/users/Centril/followers", "following_url": "https://api.github.com/users/Centril/following{/other_user}", "gists_url": "https://api.github.com/users/Centril/gists{/gist_id}", "starred_url": "https://api.github.com/users/Centril/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Centril/subscriptions", "organizations_url": "https://api.github.com/users/Centril/orgs", "repos_url": "https://api.github.com/users/Centril/repos", "events_url": "https://api.github.com/users/Centril/events{/privacy}", "received_events_url": "https://api.github.com/users/Centril/received_events", "type": "User", "site_admin": false}, "committer": {"login": "web-flow", "id": 19864447, "node_id": "MDQ6VXNlcjE5ODY0NDQ3", "avatar_url": "https://avatars.githubusercontent.com/u/19864447?v=4", "gravatar_id": "", "url": "https://api.github.com/users/web-flow", "html_url": "https://github.com/web-flow", "followers_url": "https://api.github.com/users/web-flow/followers", "following_url": "https://api.github.com/users/web-flow/following{/other_user}", "gists_url": "https://api.github.com/users/web-flow/gists{/gist_id}", "starred_url": "https://api.github.com/users/web-flow/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/web-flow/subscriptions", "organizations_url": "https://api.github.com/users/web-flow/orgs", "repos_url": "https://api.github.com/users/web-flow/repos", "events_url": "https://api.github.com/users/web-flow/events{/privacy}", "received_events_url": "https://api.github.com/users/web-flow/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "254f2014954bd66da206232490824975c0c662f1", "url": "https://api.github.com/repos/rust-lang/rust/commits/254f2014954bd66da206232490824975c0c662f1", "html_url": "https://github.com/rust-lang/rust/commit/254f2014954bd66da206232490824975c0c662f1"}, {"sha": "2b47a085dd418447f1dd79986df94dd051f27c79", "url": "https://api.github.com/repos/rust-lang/rust/commits/2b47a085dd418447f1dd79986df94dd051f27c79", "html_url": "https://github.com/rust-lang/rust/commit/2b47a085dd418447f1dd79986df94dd051f27c79"}], "stats": {"total": 1093, "additions": 741, "deletions": 352}, "files": [{"sha": "87fec41415272bace9824cb2ab04697754a928d3", "filename": ".gitignore", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "blob_url": "https://github.com/rust-lang/rust/blob/327c54ed02e412013e8ffabc61c71f10c2777fe1/.gitignore", "raw_url": "https://github.com/rust-lang/rust/raw/327c54ed02e412013e8ffabc61c71f10c2777fe1/.gitignore", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/.gitignore?ref=327c54ed02e412013e8ffabc61c71f10c2777fe1", "patch": "@@ -36,6 +36,7 @@ __pycache__/\n /src/libcore/unicode/Scripts.txt\n /src/libcore/unicode/SpecialCasing.txt\n /src/libcore/unicode/UnicodeData.txt\n+/src/libcore/unicode/downloaded\n /stage[0-9]+/\n /target\n target/"}, {"sha": "a0539cd9ca9b6ee1bb641b83127e407b04ca2053", "filename": "src/libcore/unicode/unicode.py", "status": "modified", "additions": 740, "deletions": 352, "changes": 1092, "blob_url": "https://github.com/rust-lang/rust/blob/327c54ed02e412013e8ffabc61c71f10c2777fe1/src%2Flibcore%2Funicode%2Funicode.py", "raw_url": "https://github.com/rust-lang/rust/raw/327c54ed02e412013e8ffabc61c71f10c2777fe1/src%2Flibcore%2Funicode%2Funicode.py", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibcore%2Funicode%2Funicode.py?ref=327c54ed02e412013e8ffabc61c71f10c2777fe1", "patch": "@@ -1,147 +1,350 @@\n #!/usr/bin/env python\n \n-# This script uses the following Unicode tables:\n-# - DerivedCoreProperties.txt\n-# - DerivedNormalizationProps.txt\n-# - EastAsianWidth.txt\n-# - auxiliary/GraphemeBreakProperty.txt\n-# - PropList.txt\n-# - ReadMe.txt\n-# - Scripts.txt\n-# - UnicodeData.txt\n-#\n-# Since this should not require frequent updates, we just store this\n-# out-of-line and check the tables.rs file into git.\n+\"\"\"\n+Regenerate Unicode tables (tables.rs).\n+\"\"\"\n \n-import fileinput, re, os, sys, operator, math, datetime\n+# This script uses the Unicode tables as defined\n+# in the UnicodeFiles class.\n \n-# The directory in which this file resides.\n-fdir = os.path.dirname(os.path.realpath(__file__)) + \"/\"\n+# Since this should not require frequent updates, we just store this\n+# out-of-line and check the tables.rs file into git.\n \n-preamble = '''\n+# Note that the \"curl\" program is required for operation.\n+# This script is compatible with Python 2.7 and 3.x.\n+\n+import argparse\n+import datetime\n+import fileinput\n+import itertools\n+import os\n+import re\n+import textwrap\n+import subprocess\n+\n+from collections import defaultdict, namedtuple\n+\n+try:\n+    # Python 3\n+    from itertools import zip_longest\n+    from io import StringIO\n+except ImportError:\n+    # Python 2 compatibility\n+    zip_longest = itertools.izip_longest\n+    from StringIO import StringIO\n+\n+try:\n+    # Completely optional type hinting\n+    # (Python 2 compatible using comments,\n+    # see: https://mypy.readthedocs.io/en/latest/python2.html)\n+    # This is very helpful in typing-aware IDE like PyCharm.\n+    from typing import Any, Callable, Dict, Iterable, Iterator, List, Optional, Set, Tuple\n+except ImportError:\n+    pass\n+\n+\n+# We don't use enum.Enum because of Python 2.7 compatibility.\n+class UnicodeFiles(object):\n+    # ReadMe does not contain any Unicode data, we\n+    # only use it to extract versions.\n+    README = \"ReadMe.txt\"\n+\n+    DERIVED_CORE_PROPERTIES = \"DerivedCoreProperties.txt\"\n+    DERIVED_NORMALIZATION_PROPS = \"DerivedNormalizationProps.txt\"\n+    PROPS = \"PropList.txt\"\n+    SCRIPTS = \"Scripts.txt\"\n+    SPECIAL_CASING = \"SpecialCasing.txt\"\n+    UNICODE_DATA = \"UnicodeData.txt\"\n+\n+\n+# The order doesn't really matter (Python < 3.6 won't preserve it),\n+# we only want to aggregate all the file names.\n+ALL_UNICODE_FILES = tuple(\n+    value for name, value in UnicodeFiles.__dict__.items()\n+    if not name.startswith(\"_\")\n+)\n+\n+assert len(ALL_UNICODE_FILES) == 7, \"Unexpected number of unicode files\"\n+\n+# The directory this file is located in.\n+THIS_DIR = os.path.dirname(os.path.realpath(__file__))\n+\n+# Where to download the Unicode data.  The downloaded files\n+# will be placed in sub-directories named after Unicode version.\n+FETCH_DIR = os.path.join(THIS_DIR, \"downloaded\")\n+\n+FETCH_URL_LATEST = \"ftp://ftp.unicode.org/Public/UNIDATA/{filename}\"\n+FETCH_URL_VERSION = \"ftp://ftp.unicode.org/Public/{version}/ucd/{filename}\"\n+\n+PREAMBLE = \"\"\"\\\n // NOTE: The following code was generated by \"./unicode.py\", do not edit directly\n \n #![allow(missing_docs, non_upper_case_globals, non_snake_case)]\n \n use unicode::version::UnicodeVersion;\n use unicode::bool_trie::{{BoolTrie, SmallBoolTrie}};\n-'''.format(year = datetime.datetime.now().year)\n+\"\"\".format(year=datetime.datetime.now().year)\n \n # Mapping taken from Table 12 from:\n # http://www.unicode.org/reports/tr44/#General_Category_Values\n-expanded_categories = {\n-    'Lu': ['LC', 'L'], 'Ll': ['LC', 'L'], 'Lt': ['LC', 'L'],\n-    'Lm': ['L'], 'Lo': ['L'],\n-    'Mn': ['M'], 'Mc': ['M'], 'Me': ['M'],\n-    'Nd': ['N'], 'Nl': ['N'], 'No': ['N'],\n-    'Pc': ['P'], 'Pd': ['P'], 'Ps': ['P'], 'Pe': ['P'],\n-    'Pi': ['P'], 'Pf': ['P'], 'Po': ['P'],\n-    'Sm': ['S'], 'Sc': ['S'], 'Sk': ['S'], 'So': ['S'],\n-    'Zs': ['Z'], 'Zl': ['Z'], 'Zp': ['Z'],\n-    'Cc': ['C'], 'Cf': ['C'], 'Cs': ['C'], 'Co': ['C'], 'Cn': ['C'],\n+EXPANDED_CATEGORIES = {\n+    \"Lu\": [\"LC\", \"L\"], \"Ll\": [\"LC\", \"L\"], \"Lt\": [\"LC\", \"L\"],\n+    \"Lm\": [\"L\"], \"Lo\": [\"L\"],\n+    \"Mn\": [\"M\"], \"Mc\": [\"M\"], \"Me\": [\"M\"],\n+    \"Nd\": [\"N\"], \"Nl\": [\"N\"], \"No\": [\"N\"],\n+    \"Pc\": [\"P\"], \"Pd\": [\"P\"], \"Ps\": [\"P\"], \"Pe\": [\"P\"],\n+    \"Pi\": [\"P\"], \"Pf\": [\"P\"], \"Po\": [\"P\"],\n+    \"Sm\": [\"S\"], \"Sc\": [\"S\"], \"Sk\": [\"S\"], \"So\": [\"S\"],\n+    \"Zs\": [\"Z\"], \"Zl\": [\"Z\"], \"Zp\": [\"Z\"],\n+    \"Cc\": [\"C\"], \"Cf\": [\"C\"], \"Cs\": [\"C\"], \"Co\": [\"C\"], \"Cn\": [\"C\"],\n }\n \n-# these are the surrogate codepoints, which are not valid rust characters\n-surrogate_codepoints = (0xd800, 0xdfff)\n+# This is the (inclusive) range of surrogate codepoints.\n+# These are not valid Rust characters.\n+SURROGATE_CODEPOINTS_RANGE = (0xd800, 0xdfff)\n+\n+UnicodeData = namedtuple(\n+    \"UnicodeData\", (\n+        # Conversions:\n+        \"to_upper\", \"to_lower\", \"to_title\",\n+\n+        # Decompositions: canonical decompositions, compatibility decomp\n+        \"canon_decomp\", \"compat_decomp\",\n+\n+        # Grouped: general categories and combining characters\n+        \"general_categories\", \"combines\",\n+    )\n+)\n+\n+UnicodeVersion = namedtuple(\n+    \"UnicodeVersion\", (\"major\", \"minor\", \"micro\", \"as_str\")\n+)\n+\n+\n+def fetch_files(version=None):\n+    # type: (str) -> UnicodeVersion\n+    \"\"\"\n+    Fetch all the Unicode files from unicode.org.\n+\n+    This will use cached files (stored in `FETCH_DIR`) if they exist,\n+    creating them if they don't.  In any case, the Unicode version\n+    is always returned.\n+\n+    :param version: The desired Unicode version, as string.\n+        (If None, defaults to latest final release available,\n+         querying the unicode.org service).\n+    \"\"\"\n+    have_version = check_stored_version(version)\n+    if have_version:\n+        return have_version\n+\n+    if version:\n+        # Check if the desired version exists on the server.\n+        get_fetch_url = lambda name: FETCH_URL_VERSION.format(version=version, filename=name)\n+    else:\n+        # Extract the latest version.\n+        get_fetch_url = lambda name: FETCH_URL_LATEST.format(filename=name)\n+\n+    readme_url = get_fetch_url(UnicodeFiles.README)\n+\n+    print(\"Fetching: {}\".format(readme_url))\n+    readme_content = subprocess.check_output((\"curl\", readme_url))\n+\n+    unicode_version = parse_readme_unicode_version(\n+        readme_content.decode(\"utf8\")\n+    )\n+\n+    download_dir = get_unicode_dir(unicode_version)\n+    if not os.path.exists(download_dir):\n+        # For 2.7 compat, we don't use `exist_ok=True`.\n+        os.makedirs(download_dir)\n+\n+    for filename in ALL_UNICODE_FILES:\n+        file_path = get_unicode_file_path(unicode_version, filename)\n+\n+        if os.path.exists(file_path):\n+            # Assume file on the server didn't change if it's been saved before.\n+            continue\n+\n+        if filename == UnicodeFiles.README:\n+            with open(file_path, \"wb\") as fd:\n+                fd.write(readme_content)\n+        else:\n+            url = get_fetch_url(filename)\n+            print(\"Fetching: {}\".format(url))\n+            subprocess.check_call((\"curl\", \"-o\", file_path, url))\n+\n+    return unicode_version\n+\n+\n+def check_stored_version(version):\n+    # type: (Optional[str]) -> Optional[UnicodeVersion]\n+    \"\"\"\n+    Given desired Unicode version, return the version\n+    if stored files are all present, and `None` otherwise.\n+    \"\"\"\n+    if not version:\n+        # If no desired version specified, we should check what's the latest\n+        # version, skipping stored version checks.\n+        return None\n+\n+    fetch_dir = os.path.join(FETCH_DIR, version)\n \n-def fetch(f):\n-    path = fdir + os.path.basename(f)\n-    if not os.path.exists(path):\n-        os.system(\"curl -o {0}{1} ftp://ftp.unicode.org/Public/UNIDATA/{1}\".format(fdir, f))\n+    for filename in ALL_UNICODE_FILES:\n+        file_path = os.path.join(fetch_dir, filename)\n+\n+        if not os.path.exists(file_path):\n+            return None\n+\n+    with open(os.path.join(fetch_dir, UnicodeFiles.README)) as fd:\n+        return parse_readme_unicode_version(fd.read())\n+\n+\n+def parse_readme_unicode_version(readme_content):\n+    # type: (str) -> UnicodeVersion\n+    \"\"\"\n+    Parse the Unicode version contained in their `ReadMe.txt` file.\n+    \"\"\"\n+    # \"Raw string\" is necessary for \\d not being treated as escape char\n+    # (for the sake of compat with future Python versions).\n+    # See: https://docs.python.org/3.6/whatsnew/3.6.html#deprecated-python-behavior\n+    pattern = r\"for Version (\\d+)\\.(\\d+)\\.(\\d+) of the Unicode\"\n+    groups = re.search(pattern, readme_content).groups()\n+\n+    return UnicodeVersion(*map(int, groups), as_str=\".\".join(groups))\n+\n+\n+def get_unicode_dir(unicode_version):\n+    # type: (UnicodeVersion) -> str\n+    \"\"\"\n+    Indicate in which parent dir the Unicode data files should be stored.\n+\n+    This returns a full, absolute path.\n+    \"\"\"\n+    return os.path.join(FETCH_DIR, unicode_version.as_str)\n+\n+\n+def get_unicode_file_path(unicode_version, filename):\n+    # type: (UnicodeVersion, str) -> str\n+    \"\"\"\n+    Indicate where the Unicode data file should be stored.\n+    \"\"\"\n+    return os.path.join(get_unicode_dir(unicode_version), filename)\n \n-    if not os.path.exists(path):\n-        sys.stderr.write(\"cannot load %s\" % f)\n-        exit(1)\n \n def is_surrogate(n):\n-    return surrogate_codepoints[0] <= n <= surrogate_codepoints[1]\n-\n-def load_unicode_data(f):\n-    fetch(f)\n-    gencats = {}\n-    to_lower = {}\n-    to_upper = {}\n-    to_title = {}\n-    combines = {}\n-    canon_decomp = {}\n-    compat_decomp = {}\n-\n-    udict = {}\n+    # type: (int) -> bool\n+    \"\"\"\n+    Tell if given codepoint is a surrogate (not a valid Rust character).\n+    \"\"\"\n+    return SURROGATE_CODEPOINTS_RANGE[0] <= n <= SURROGATE_CODEPOINTS_RANGE[1]\n+\n+\n+def load_unicode_data(file_path):\n+    # type: (str) -> UnicodeData\n+    \"\"\"\n+    Load main Unicode data.\n+    \"\"\"\n+    # Conversions\n+    to_lower = {}   # type: Dict[int, Tuple[int, int, int]]\n+    to_upper = {}   # type: Dict[int, Tuple[int, int, int]]\n+    to_title = {}   # type: Dict[int, Tuple[int, int, int]]\n+\n+    # Decompositions\n+    compat_decomp = {}   # type: Dict[int, List[int]]\n+    canon_decomp = {}    # type: Dict[int, List[int]]\n+\n+    # Combining characters\n+    # FIXME: combines are not used\n+    combines = defaultdict(set)   # type: Dict[str, Set[int]]\n+\n+    # Categories\n+    general_categories = defaultdict(set)   # type: Dict[str, Set[int]]\n+    category_assigned_codepoints = set()    # type: Set[int]\n+\n+    all_codepoints = {}\n+\n     range_start = -1\n-    for line in fileinput.input(fdir + f):\n-        data = line.split(';')\n+\n+    for line in fileinput.input(file_path):\n+        data = line.split(\";\")\n         if len(data) != 15:\n             continue\n-        cp = int(data[0], 16)\n-        if is_surrogate(cp):\n+        codepoint = int(data[0], 16)\n+        if is_surrogate(codepoint):\n             continue\n         if range_start >= 0:\n-            for i in range(range_start, cp):\n-                udict[i] = data\n+            for i in range(range_start, codepoint):\n+                all_codepoints[i] = data\n             range_start = -1\n         if data[1].endswith(\", First>\"):\n-            range_start = cp\n+            range_start = codepoint\n             continue\n-        udict[cp] = data\n+        all_codepoints[codepoint] = data\n \n-    for code in udict:\n+    for code, data in all_codepoints.items():\n         (code_org, name, gencat, combine, bidi,\n          decomp, deci, digit, num, mirror,\n-         old, iso, upcase, lowcase, titlecase) = udict[code]\n+         old, iso, upcase, lowcase, titlecase) = data\n+\n+        # Generate char to char direct common and simple conversions:\n \n-        # generate char to char direct common and simple conversions\n-        # uppercase to lowercase\n+        # Uppercase to lowercase\n         if lowcase != \"\" and code_org != lowcase:\n             to_lower[code] = (int(lowcase, 16), 0, 0)\n \n-        # lowercase to uppercase\n+        # Lowercase to uppercase\n         if upcase != \"\" and code_org != upcase:\n             to_upper[code] = (int(upcase, 16), 0, 0)\n \n-        # title case\n+        # Title case\n         if titlecase.strip() != \"\" and code_org != titlecase:\n             to_title[code] = (int(titlecase, 16), 0, 0)\n \n-        # store decomposition, if given\n-        if decomp != \"\":\n-            if decomp.startswith('<'):\n-                seq = []\n-                for i in decomp.split()[1:]:\n-                    seq.append(int(i, 16))\n-                compat_decomp[code] = seq\n+        # Store decomposition, if given\n+        if decomp:\n+            decompositions = decomp.split()[1:]\n+            decomp_code_points = [int(i, 16) for i in decompositions]\n+\n+            if decomp.startswith(\"<\"):\n+                # Compatibility decomposition\n+                compat_decomp[code] = decomp_code_points\n             else:\n-                seq = []\n-                for i in decomp.split():\n-                    seq.append(int(i, 16))\n-                canon_decomp[code] = seq\n-\n-        # place letter in categories as appropriate\n-        for cat in [gencat, \"Assigned\"] + expanded_categories.get(gencat, []):\n-            if cat not in gencats:\n-                gencats[cat] = []\n-            gencats[cat].append(code)\n-\n-        # record combining class, if any\n+                # Canonical decomposition\n+                canon_decomp[code] = decomp_code_points\n+\n+        # Place letter in categories as appropriate.\n+        for cat in itertools.chain((gencat, ), EXPANDED_CATEGORIES.get(gencat, [])):\n+            general_categories[cat].add(code)\n+            category_assigned_codepoints.add(code)\n+\n+        # Record combining class, if any.\n         if combine != \"0\":\n-            if combine not in combines:\n-                combines[combine] = []\n-            combines[combine].append(code)\n-\n-    # generate Not_Assigned from Assigned\n-    gencats[\"Cn\"] = gen_unassigned(gencats[\"Assigned\"])\n-    # Assigned is not a real category\n-    del(gencats[\"Assigned\"])\n+            combines[combine].add(code)\n+\n+    # Generate Not_Assigned from Assigned.\n+    general_categories[\"Cn\"] = get_unassigned_codepoints(category_assigned_codepoints)\n+\n     # Other contains Not_Assigned\n-    gencats[\"C\"].extend(gencats[\"Cn\"])\n-    gencats = group_cats(gencats)\n-    combines = to_combines(group_cats(combines))\n+    general_categories[\"C\"].update(general_categories[\"Cn\"])\n+\n+    grouped_categories = group_categories(general_categories)\n \n-    return (canon_decomp, compat_decomp, gencats, combines, to_upper, to_lower, to_title)\n+    # FIXME: combines are not used\n+    return UnicodeData(\n+        to_lower=to_lower, to_upper=to_upper, to_title=to_title,\n+        compat_decomp=compat_decomp, canon_decomp=canon_decomp,\n+        general_categories=grouped_categories, combines=combines,\n+    )\n \n-def load_special_casing(f, to_upper, to_lower, to_title):\n-    fetch(f)\n-    for line in fileinput.input(fdir + f):\n-        data = line.split('#')[0].split(';')\n+\n+def load_special_casing(file_path, unicode_data):\n+    # type: (str, UnicodeData) -> None\n+    \"\"\"\n+    Load special casing data and enrich given Unicode data.\n+    \"\"\"\n+    for line in fileinput.input(file_path):\n+        data = line.split(\"#\")[0].split(\";\")\n         if len(data) == 5:\n             code, lower, title, upper, _comment = data\n         elif len(data) == 6:\n@@ -155,243 +358,399 @@ def load_special_casing(f, to_upper, to_lower, to_title):\n         title = title.strip()\n         upper = upper.strip()\n         key = int(code, 16)\n-        for (map_, values) in [(to_lower, lower), (to_upper, upper), (to_title, title)]:\n+        for (map_, values) in ((unicode_data.to_lower, lower),\n+                               (unicode_data.to_upper, upper),\n+                               (unicode_data.to_title, title)):\n             if values != code:\n-                values = [int(i, 16) for i in values.split()]\n-                for _ in range(len(values), 3):\n-                    values.append(0)\n-                assert len(values) == 3\n-                map_[key] = values\n-\n-def group_cats(cats):\n-    cats_out = {}\n-    for cat in cats:\n-        cats_out[cat] = group_cat(cats[cat])\n-    return cats_out\n-\n-def group_cat(cat):\n-    cat_out = []\n-    letters = sorted(set(cat))\n-    cur_start = letters.pop(0)\n-    cur_end = cur_start\n-    for letter in letters:\n-        assert letter > cur_end, \\\n-            \"cur_end: %s, letter: %s\" % (hex(cur_end), hex(letter))\n-        if letter == cur_end + 1:\n-            cur_end = letter\n-        else:\n-            cat_out.append((cur_start, cur_end))\n-            cur_start = cur_end = letter\n-    cat_out.append((cur_start, cur_end))\n-    return cat_out\n-\n-def ungroup_cat(cat):\n-    cat_out = []\n-    for (lo, hi) in cat:\n-        while lo <= hi:\n-            cat_out.append(lo)\n-            lo += 1\n-    return cat_out\n-\n-def gen_unassigned(assigned):\n-    assigned = set(assigned)\n-    return ([i for i in range(0, 0xd800) if i not in assigned] +\n-            [i for i in range(0xe000, 0x110000) if i not in assigned])\n-\n-def to_combines(combs):\n-    combs_out = []\n-    for comb in combs:\n-        for (lo, hi) in combs[comb]:\n-            combs_out.append((lo, hi, comb))\n-    combs_out.sort(key=lambda comb: comb[0])\n-    return combs_out\n-\n-def format_table_content(f, content, indent):\n-    line = \" \"*indent\n+                split = values.split()\n+\n+                codepoints = list(itertools.chain(\n+                    (int(i, 16) for i in split),\n+                    (0 for _ in range(len(split), 3))\n+                ))\n+\n+                assert len(codepoints) == 3\n+                map_[key] = codepoints\n+\n+\n+def group_categories(mapping):\n+    # type: (Dict[Any, Iterable[int]]) -> Dict[str, List[Tuple[int, int]]]\n+    \"\"\"\n+    Group codepoints mapped in \"categories\".\n+    \"\"\"\n+    return {category: group_codepoints(codepoints)\n+            for category, codepoints in mapping.items()}\n+\n+\n+def group_codepoints(codepoints):\n+    # type: (Iterable[int]) -> List[Tuple[int, int]]\n+    \"\"\"\n+    Group integral values into continuous, disjoint value ranges.\n+\n+    Performs value deduplication.\n+\n+    :return: sorted list of pairs denoting start and end of codepoint\n+        group values, both ends inclusive.\n+\n+    >>> group_codepoints([1, 2, 10, 11, 12, 3, 4])\n+    [(1, 4), (10, 12)]\n+    >>> group_codepoints([1])\n+    [(1, 1)]\n+    >>> group_codepoints([1, 5, 6])\n+    [(1, 1), (5, 6)]\n+    >>> group_codepoints([])\n+    []\n+    \"\"\"\n+    sorted_codes = sorted(set(codepoints))\n+    result = []     # type: List[Tuple[int, int]]\n+\n+    if not sorted_codes:\n+        return result\n+\n+    next_codes = sorted_codes[1:]\n+    start_code = sorted_codes[0]\n+\n+    for code, next_code in zip_longest(sorted_codes, next_codes, fillvalue=None):\n+        if next_code is None or next_code - code != 1:\n+            result.append((start_code, code))\n+            start_code = next_code\n+\n+    return result\n+\n+\n+def ungroup_codepoints(codepoint_pairs):\n+    # type: (Iterable[Tuple[int, int]]) -> List[int]\n+    \"\"\"\n+    The inverse of group_codepoints -- produce a flat list of values\n+    from value range pairs.\n+\n+    >>> ungroup_codepoints([(1, 4), (10, 12)])\n+    [1, 2, 3, 4, 10, 11, 12]\n+    >>> ungroup_codepoints([(1, 1), (5, 6)])\n+    [1, 5, 6]\n+    >>> ungroup_codepoints(group_codepoints([1, 2, 7, 8]))\n+    [1, 2, 7, 8]\n+    >>> ungroup_codepoints([])\n+    []\n+    \"\"\"\n+    return list(itertools.chain.from_iterable(\n+        range(lo, hi + 1) for lo, hi in codepoint_pairs\n+    ))\n+\n+\n+def get_unassigned_codepoints(assigned_codepoints):\n+    # type: (Set[int]) -> Set[int]\n+    \"\"\"\n+    Given a set of \"assigned\" codepoints, return a set\n+    of these that are not in assigned and not surrogate.\n+    \"\"\"\n+    return {i for i in range(0, 0x110000)\n+            if i not in assigned_codepoints and not is_surrogate(i)}\n+\n+\n+def generate_table_lines(items, indent, wrap=98):\n+    # type: (Iterable[str], int, int) -> Iterator[str]\n+    \"\"\"\n+    Given table items, generate wrapped lines of text with comma-separated items.\n+\n+    This is a generator function.\n+\n+    :param wrap: soft wrap limit (characters per line), integer.\n+    \"\"\"\n+    line = \" \" * indent\n     first = True\n-    for chunk in content.split(\",\"):\n-        if len(line) + len(chunk) < 98:\n+    for item in items:\n+        if len(line) + len(item) < wrap:\n             if first:\n-                line += chunk\n+                line += item\n             else:\n-                line += \", \" + chunk\n+                line += \", \" + item\n             first = False\n         else:\n-            f.write(line + \",\\n\")\n-            line = \" \"*indent + chunk\n-    f.write(line)\n-\n-def load_properties(f, interestingprops):\n-    fetch(f)\n-    props = {}\n-    re1 = re.compile(\"^ *([0-9A-F]+) *; *(\\w+)\")\n-    re2 = re.compile(\"^ *([0-9A-F]+)\\.\\.([0-9A-F]+) *; *(\\w+)\")\n-\n-    for line in fileinput.input(fdir + os.path.basename(f)):\n-        prop = None\n-        d_lo = 0\n-        d_hi = 0\n-        m = re1.match(line)\n-        if m:\n-            d_lo = m.group(1)\n-            d_hi = m.group(1)\n-            prop = m.group(2)\n-        else:\n-            m = re2.match(line)\n-            if m:\n-                d_lo = m.group(1)\n-                d_hi = m.group(2)\n-                prop = m.group(3)\n+            yield line + \",\\n\"\n+            line = \" \" * indent + item\n+\n+    yield line\n+\n+\n+def load_properties(file_path, interesting_props):\n+    # type: (str, Iterable[str]) -> Dict[str, List[Tuple[int, int]]]\n+    \"\"\"\n+    Load properties data and return in grouped form.\n+    \"\"\"\n+    props = defaultdict(list)   # type: Dict[str, List[Tuple[int, int]]]\n+    # \"Raw string\" is necessary for `\\.` and `\\w` not to be treated as escape chars\n+    # (for the sake of compat with future Python versions).\n+    # See: https://docs.python.org/3.6/whatsnew/3.6.html#deprecated-python-behavior\n+    re1 = re.compile(r\"^ *([0-9A-F]+) *; *(\\w+)\")\n+    re2 = re.compile(r\"^ *([0-9A-F]+)\\.\\.([0-9A-F]+) *; *(\\w+)\")\n+\n+    for line in fileinput.input(file_path):\n+        match = re1.match(line) or re2.match(line)\n+        if match:\n+            groups = match.groups()\n+\n+            if len(groups) == 2:\n+                # `re1` matched (2 groups).\n+                d_lo, prop = groups\n+                d_hi = d_lo\n             else:\n-                continue\n-        if interestingprops and prop not in interestingprops:\n+                d_lo, d_hi, prop = groups\n+        else:\n+            continue\n+\n+        if interesting_props and prop not in interesting_props:\n             continue\n-        d_lo = int(d_lo, 16)\n-        d_hi = int(d_hi, 16)\n-        if prop not in props:\n-            props[prop] = []\n-        props[prop].append((d_lo, d_hi))\n \n-    # optimize if possible\n+        lo_value = int(d_lo, 16)\n+        hi_value = int(d_hi, 16)\n+\n+        props[prop].append((lo_value, hi_value))\n+\n+    # Optimize if possible.\n     for prop in props:\n-        props[prop] = group_cat(ungroup_cat(props[prop]))\n+        props[prop] = group_codepoints(ungroup_codepoints(props[prop]))\n \n     return props\n \n-def escape_char(c):\n-    return \"'\\\\u{%x}'\" % c if c != 0 else \"'\\\\0'\"\n \n-def emit_table(f, name, t_data, t_type = \"&[(char, char)]\", is_pub=True,\n-        pfun=lambda x: \"(%s,%s)\" % (escape_char(x[0]), escape_char(x[1]))):\n+def escape_char(c):\n+    # type: (int) -> str\n+    r\"\"\"\n+    Escape a codepoint for use as Rust char literal.\n+\n+    Outputs are OK to use as Rust source code as char literals\n+    and they also include necessary quotes.\n+\n+    >>> escape_char(97)\n+    \"'\\\\u{61}'\"\n+    >>> escape_char(0)\n+    \"'\\\\0'\"\n+    \"\"\"\n+    return r\"'\\u{%x}'\" % c if c != 0 else r\"'\\0'\"\n+\n+\n+def format_char_pair(pair):\n+    # type: (Tuple[int, int]) -> str\n+    \"\"\"\n+    Format a pair of two Rust chars.\n+    \"\"\"\n+    return \"(%s,%s)\" % (escape_char(pair[0]), escape_char(pair[1]))\n+\n+\n+def generate_table(\n+    name,   # type: str\n+    items,  # type: List[Tuple[int, int]]\n+    decl_type=\"&[(char, char)]\",    # type: str\n+    is_pub=True,                    # type: bool\n+    format_item=format_char_pair,   # type: Callable[[Tuple[int, int]], str]\n+):\n+    # type: (...) -> Iterator[str]\n+    \"\"\"\n+    Generate a nicely formatted Rust constant \"table\" array.\n+\n+    This generates actual Rust code.\n+    \"\"\"\n     pub_string = \"\"\n     if is_pub:\n         pub_string = \"pub \"\n-    f.write(\"    %sconst %s: %s = &[\\n\" % (pub_string, name, t_type))\n-    data = \"\"\n+\n+    yield \"    %sconst %s: %s = &[\\n\" % (pub_string, name, decl_type)\n+\n+    data = []\n     first = True\n-    for dat in t_data:\n+    for item in items:\n         if not first:\n-            data += \",\"\n+            data.append(\",\")\n         first = False\n-        data += pfun(dat)\n-    format_table_content(f, data, 8)\n-    f.write(\"\\n    ];\\n\\n\")\n+        data.extend(format_item(item))\n \n-def compute_trie(rawdata, chunksize):\n+    for table_line in generate_table_lines(\"\".join(data).split(\",\"), 8):\n+        yield table_line\n+\n+    yield \"\\n    ];\\n\\n\"\n+\n+\n+def compute_trie(raw_data, chunk_size):\n+    # type: (List[int], int) -> Tuple[List[int], List[int]]\n+    \"\"\"\n+    Compute postfix-compressed trie.\n+\n+    See: bool_trie.rs for more details.\n+\n+    >>> compute_trie([1, 2, 3, 1, 2, 3, 4, 5, 6], 3)\n+    ([0, 0, 1], [1, 2, 3, 4, 5, 6])\n+    >>> compute_trie([1, 2, 3, 1, 2, 4, 4, 5, 6], 3)\n+    ([0, 1, 2], [1, 2, 3, 1, 2, 4, 4, 5, 6])\n+    \"\"\"\n     root = []\n-    childmap = {}\n+    childmap = {}       # type: Dict[Tuple[int, ...], int]\n     child_data = []\n-    for i in range(len(rawdata) // chunksize):\n-        data = rawdata[i * chunksize: (i + 1) * chunksize]\n-        child = '|'.join(map(str, data))\n+\n+    assert len(raw_data) % chunk_size == 0, \"Chunks must be equally sized\"\n+\n+    for i in range(len(raw_data) // chunk_size):\n+        data = raw_data[i * chunk_size : (i + 1) * chunk_size]\n+\n+        # Postfix compression of child nodes (data chunks)\n+        # (identical child nodes are shared).\n+\n+        # Make a tuple out of the list so it's hashable.\n+        child = tuple(data)\n         if child not in childmap:\n             childmap[child] = len(childmap)\n             child_data.extend(data)\n+\n         root.append(childmap[child])\n-    return (root, child_data)\n \n-def emit_bool_trie(f, name, t_data, is_pub=True):\n-    CHUNK = 64\n+    return root, child_data\n+\n+\n+def generate_bool_trie(name, codepoint_ranges, is_pub=True):\n+    # type: (str, List[Tuple[int, int]], bool) -> Iterator[str]\n+    \"\"\"\n+    Generate Rust code for BoolTrie struct.\n+\n+    This yields string fragments that should be joined to produce\n+    the final string.\n+\n+    See: `bool_trie.rs`.\n+    \"\"\"\n+    chunk_size = 64\n     rawdata = [False] * 0x110000\n-    for (lo, hi) in t_data:\n+    for (lo, hi) in codepoint_ranges:\n         for cp in range(lo, hi + 1):\n             rawdata[cp] = True\n \n-    # convert to bitmap chunks of 64 bits each\n+    # Convert to bitmap chunks of `chunk_size` bits each.\n     chunks = []\n-    for i in range(0x110000 // CHUNK):\n+    for i in range(0x110000 // chunk_size):\n         chunk = 0\n-        for j in range(64):\n-            if rawdata[i * 64 + j]:\n+        for j in range(chunk_size):\n+            if rawdata[i * chunk_size + j]:\n                 chunk |= 1 << j\n         chunks.append(chunk)\n \n     pub_string = \"\"\n     if is_pub:\n         pub_string = \"pub \"\n-    f.write(\"    %sconst %s: &super::BoolTrie = &super::BoolTrie {\\n\" % (pub_string, name))\n-    f.write(\"        r1: [\\n\")\n-    data = ','.join('0x%016x' % chunk for chunk in chunks[0:0x800 // CHUNK])\n-    format_table_content(f, data, 12)\n-    f.write(\"\\n        ],\\n\")\n+    yield \"    %sconst %s: &super::BoolTrie = &super::BoolTrie {\\n\" % (pub_string, name)\n+    yield \"        r1: [\\n\"\n+    data = (\"0x%016x\" % chunk for chunk in chunks[:0x800 // chunk_size])\n+    for fragment in generate_table_lines(data, 12):\n+        yield fragment\n+    yield \"\\n        ],\\n\"\n \n     # 0x800..0x10000 trie\n-    (r2, r3) = compute_trie(chunks[0x800 // CHUNK : 0x10000 // CHUNK], 64 // CHUNK)\n-    f.write(\"        r2: [\\n\")\n-    data = ','.join(str(node) for node in r2)\n-    format_table_content(f, data, 12)\n-    f.write(\"\\n        ],\\n\")\n-    f.write(\"        r3: &[\\n\")\n-    data = ','.join('0x%016x' % chunk for chunk in r3)\n-    format_table_content(f, data, 12)\n-    f.write(\"\\n        ],\\n\")\n+    (r2, r3) = compute_trie(chunks[0x800 // chunk_size : 0x10000 // chunk_size], 64 // chunk_size)\n+    yield \"        r2: [\\n\"\n+    data = map(str, r2)\n+    for fragment in generate_table_lines(data, 12):\n+        yield fragment\n+    yield \"\\n        ],\\n\"\n+\n+    yield \"        r3: &[\\n\"\n+    data = (\"0x%016x\" % node for node in r3)\n+    for fragment in generate_table_lines(data, 12):\n+        yield fragment\n+    yield \"\\n        ],\\n\"\n \n     # 0x10000..0x110000 trie\n-    (mid, r6) = compute_trie(chunks[0x10000 // CHUNK : 0x110000 // CHUNK], 64 // CHUNK)\n+    (mid, r6) = compute_trie(chunks[0x10000 // chunk_size : 0x110000 // chunk_size],\n+                             64 // chunk_size)\n     (r4, r5) = compute_trie(mid, 64)\n-    f.write(\"        r4: [\\n\")\n-    data = ','.join(str(node) for node in r4)\n-    format_table_content(f, data, 12)\n-    f.write(\"\\n        ],\\n\")\n-    f.write(\"        r5: &[\\n\")\n-    data = ','.join(str(node) for node in r5)\n-    format_table_content(f, data, 12)\n-    f.write(\"\\n        ],\\n\")\n-    f.write(\"        r6: &[\\n\")\n-    data = ','.join('0x%016x' % chunk for chunk in r6)\n-    format_table_content(f, data, 12)\n-    f.write(\"\\n        ],\\n\")\n-\n-    f.write(\"    };\\n\\n\")\n-\n-def emit_small_bool_trie(f, name, t_data, is_pub=True):\n-    last_chunk = max(hi // 64 for (lo, hi) in t_data)\n+\n+    yield \"        r4: [\\n\"\n+    data = map(str, r4)\n+    for fragment in generate_table_lines(data, 12):\n+        yield fragment\n+    yield \"\\n        ],\\n\"\n+\n+    yield \"        r5: &[\\n\"\n+    data = map(str, r5)\n+    for fragment in generate_table_lines(data, 12):\n+        yield fragment\n+    yield \"\\n        ],\\n\"\n+\n+    yield \"        r6: &[\\n\"\n+    data = (\"0x%016x\" % node for node in r6)\n+    for fragment in generate_table_lines(data, 12):\n+        yield fragment\n+    yield \"\\n        ],\\n\"\n+\n+    yield \"    };\\n\\n\"\n+\n+\n+def generate_small_bool_trie(name, codepoint_ranges, is_pub=True):\n+    # type: (str, List[Tuple[int, int]], bool) -> Iterator[str]\n+    \"\"\"\n+    Generate Rust code for `SmallBoolTrie` struct.\n+\n+    See: `bool_trie.rs`.\n+    \"\"\"\n+    last_chunk = max(hi // 64 for (lo, hi) in codepoint_ranges)\n     n_chunks = last_chunk + 1\n     chunks = [0] * n_chunks\n-    for (lo, hi) in t_data:\n+    for (lo, hi) in codepoint_ranges:\n         for cp in range(lo, hi + 1):\n-            if cp // 64 >= len(chunks):\n-                print(cp, cp // 64, len(chunks), lo, hi)\n+            assert cp // 64 < len(chunks)\n             chunks[cp // 64] |= 1 << (cp & 63)\n \n     pub_string = \"\"\n     if is_pub:\n         pub_string = \"pub \"\n-    f.write(\"    %sconst %s: &super::SmallBoolTrie = &super::SmallBoolTrie {\\n\"\n-            % (pub_string, name))\n+\n+    yield (\"    %sconst %s: &super::SmallBoolTrie = &super::SmallBoolTrie {\\n\"\n+           % (pub_string, name))\n \n     (r1, r2) = compute_trie(chunks, 1)\n \n-    f.write(\"        r1: &[\\n\")\n-    data = ','.join(str(node) for node in r1)\n-    format_table_content(f, data, 12)\n-    f.write(\"\\n        ],\\n\")\n-\n-    f.write(\"        r2: &[\\n\")\n-    data = ','.join('0x%016x' % node for node in r2)\n-    format_table_content(f, data, 12)\n-    f.write(\"\\n        ],\\n\")\n-\n-    f.write(\"    };\\n\\n\")\n-\n-def emit_property_module(f, mod, tbl, emit):\n-    f.write(\"pub mod %s {\\n\" % mod)\n-    for cat in sorted(emit):\n-        if cat in [\"Cc\", \"White_Space\", \"Pattern_White_Space\"]:\n-            emit_small_bool_trie(f, \"%s_table\" % cat, tbl[cat])\n-            f.write(\"    pub fn %s(c: char) -> bool {\\n\" % cat)\n-            f.write(\"        %s_table.lookup(c)\\n\" % cat)\n-            f.write(\"    }\\n\\n\")\n+    yield \"        r1: &[\\n\"\n+    data = (str(node) for node in r1)\n+    for fragment in generate_table_lines(data, 12):\n+        yield fragment\n+    yield \"\\n        ],\\n\"\n+\n+    yield \"        r2: &[\\n\"\n+    data = (\"0x%016x\" % node for node in r2)\n+    for fragment in generate_table_lines(data, 12):\n+        yield fragment\n+    yield \"\\n        ],\\n\"\n+\n+    yield \"    };\\n\\n\"\n+\n+\n+def generate_property_module(mod, grouped_categories, category_subset):\n+    # type: (str, Dict[str, List[Tuple[int, int]]], Iterable[str]) -> Iterator[str]\n+    \"\"\"\n+    Generate Rust code for module defining properties.\n+    \"\"\"\n+\n+    yield \"pub mod %s {\\n\" % mod\n+    for cat in sorted(category_subset):\n+        if cat in (\"Cc\", \"White_Space\", \"Pattern_White_Space\"):\n+            generator = generate_small_bool_trie(\"%s_table\" % cat, grouped_categories[cat])\n         else:\n-            emit_bool_trie(f, \"%s_table\" % cat, tbl[cat])\n-            f.write(\"    pub fn %s(c: char) -> bool {\\n\" % cat)\n-            f.write(\"        %s_table.lookup(c)\\n\" % cat)\n-            f.write(\"    }\\n\\n\")\n-    f.write(\"}\\n\\n\")\n-\n-def emit_conversions_module(f, to_upper, to_lower, to_title):\n-    f.write(\"pub mod conversions {\")\n-    f.write(\"\"\"\n+            generator = generate_bool_trie(\"%s_table\" % cat, grouped_categories[cat])\n+\n+        for fragment in generator:\n+            yield fragment\n+\n+        yield \"    pub fn %s(c: char) -> bool {\\n\" % cat\n+        yield \"        %s_table.lookup(c)\\n\" % cat\n+        yield \"    }\\n\\n\"\n+\n+    yield \"}\\n\\n\"\n+\n+\n+def generate_conversions_module(unicode_data):\n+    # type: (UnicodeData) -> Iterator[str]\n+    \"\"\"\n+    Generate Rust code for module defining conversions.\n+    \"\"\"\n+\n+    yield \"pub mod conversions {\"\n+    yield \"\"\"\n     pub fn to_lower(c: char) -> [char; 3] {\n         match bsearch_case_table(c, to_lowercase_table) {\n             None        => [c, '\\\\0', '\\\\0'],\n@@ -408,80 +767,109 @@ def emit_conversions_module(f, to_upper, to_lower, to_title):\n \n     fn bsearch_case_table(c: char, table: &[(char, [char; 3])]) -> Option<usize> {\n         table.binary_search_by(|&(key, _)| key.cmp(&c)).ok()\n-    }\n+    }\\n\\n\"\"\"\n+\n+    decl_type = \"&[(char, [char; 3])]\"\n+    format_conversion = lambda x: \"({},[{},{},{}])\".format(*(\n+        escape_char(c) for c in (x[0], x[1][0], x[1][1], x[1][2])\n+    ))\n+\n+    for fragment in generate_table(\n+        name=\"to_lowercase_table\",\n+        items=sorted(unicode_data.to_lower.items(), key=lambda x: x[0]),\n+        decl_type=decl_type,\n+        is_pub=False,\n+        format_item=format_conversion\n+    ):\n+        yield fragment\n+\n+    for fragment in generate_table(\n+        name=\"to_uppercase_table\",\n+        items=sorted(unicode_data.to_upper.items(), key=lambda x: x[0]),\n+        decl_type=decl_type,\n+        is_pub=False,\n+        format_item=format_conversion\n+    ):\n+        yield fragment\n+\n+    yield \"}\\n\"\n+\n+\n+def parse_args():\n+    # type: () -> argparse.Namespace\n+    \"\"\"\n+    Parse command line arguments.\n+    \"\"\"\n+    parser = argparse.ArgumentParser(description=__doc__)\n+    parser.add_argument(\"-v\", \"--version\", default=None, type=str,\n+                        help=\"Unicode version to use (if not specified,\"\n+                             \" defaults to latest release).\")\n+\n+    return parser.parse_args()\n+\n+\n+def main():\n+    # type: () -> None\n+    \"\"\"\n+    Script entry point.\n+    \"\"\"\n+    args = parse_args()\n+\n+    unicode_version = fetch_files(args.version)\n+    print(\"Using Unicode version: {}\".format(unicode_version.as_str))\n+\n+    # All the writing happens entirely in memory, we only write to file\n+    # once we have generated the file content (it's not very large, <1 MB).\n+    buf = StringIO()\n+    buf.write(PREAMBLE)\n+\n+    unicode_version_notice = textwrap.dedent(\"\"\"\n+    /// The version of [Unicode](http://www.unicode.org/) that the Unicode parts of\n+    /// `char` and `str` methods are based on.\n+    #[unstable(feature = \"unicode_version\", issue = \"49726\")]\n+    pub const UNICODE_VERSION: UnicodeVersion = UnicodeVersion {{\n+        major: {version.major},\n+        minor: {version.minor},\n+        micro: {version.micro},\n+        _priv: (),\n+    }};\n+    \"\"\").format(version=unicode_version)\n+    buf.write(unicode_version_notice)\n+\n+    get_path = lambda f: get_unicode_file_path(unicode_version, f)\n+\n+    unicode_data = load_unicode_data(get_path(UnicodeFiles.UNICODE_DATA))\n+    load_special_casing(get_path(UnicodeFiles.SPECIAL_CASING), unicode_data)\n+\n+    want_derived = {\"XID_Start\", \"XID_Continue\", \"Alphabetic\", \"Lowercase\", \"Uppercase\",\n+                    \"Cased\", \"Case_Ignorable\", \"Grapheme_Extend\"}\n+    derived = load_properties(get_path(UnicodeFiles.DERIVED_CORE_PROPERTIES), want_derived)\n+\n+    props = load_properties(get_path(UnicodeFiles.PROPS),\n+                            {\"White_Space\", \"Join_Control\", \"Noncharacter_Code_Point\",\n+                             \"Pattern_White_Space\"})\n+\n+    # Category tables\n+    for (name, categories, category_subset) in (\n+            (\"general_category\", unicode_data.general_categories, [\"N\", \"Cc\"]),\n+            (\"derived_property\", derived, want_derived),\n+            (\"property\", props, [\"White_Space\", \"Pattern_White_Space\"])\n+    ):\n+        for fragment in generate_property_module(name, categories, category_subset):\n+            buf.write(fragment)\n+\n+    for fragment in generate_conversions_module(unicode_data):\n+        buf.write(fragment)\n+\n+    tables_rs_path = os.path.join(THIS_DIR, \"tables.rs\")\n+\n+    # Actually write out the file content.\n+    # Will overwrite the file if it exists.\n+    with open(tables_rs_path, \"w\") as fd:\n+        fd.write(buf.getvalue())\n+\n+    print(\"Regenerated tables.rs.\")\n \n-\"\"\")\n-    t_type = \"&[(char, [char; 3])]\"\n-    pfun = lambda x: \"(%s,[%s,%s,%s])\" % (\n-        escape_char(x[0]), escape_char(x[1][0]), escape_char(x[1][1]), escape_char(x[1][2]))\n-    emit_table(f, \"to_lowercase_table\",\n-        sorted(to_lower.items(), key=operator.itemgetter(0)),\n-        is_pub=False, t_type = t_type, pfun=pfun)\n-    emit_table(f, \"to_uppercase_table\",\n-        sorted(to_upper.items(), key=operator.itemgetter(0)),\n-        is_pub=False, t_type = t_type, pfun=pfun)\n-    f.write(\"}\\n\\n\")\n-\n-def emit_norm_module(f, canon, compat, combine, norm_props):\n-    canon_keys = sorted(canon.keys())\n-\n-    compat_keys = sorted(compat.keys())\n-\n-    canon_comp = {}\n-    comp_exclusions = norm_props[\"Full_Composition_Exclusion\"]\n-    for char in canon_keys:\n-        if any(lo <= char <= hi for lo, hi in comp_exclusions):\n-            continue\n-        decomp = canon[char]\n-        if len(decomp) == 2:\n-            if decomp[0] not in canon_comp:\n-                canon_comp[decomp[0]] = []\n-            canon_comp[decomp[0]].append( (decomp[1], char) )\n-    canon_comp_keys = sorted(canon_comp.keys())\n \n if __name__ == \"__main__\":\n-    r = fdir + \"tables.rs\"\n-    if os.path.exists(r):\n-        os.remove(r)\n-    with open(r, \"w\") as rf:\n-        # write the file's preamble\n-        rf.write(preamble)\n-\n-        # download and parse all the data\n-        fetch(\"ReadMe.txt\")\n-        with open(fdir + \"ReadMe.txt\") as readme:\n-            pattern = \"for Version (\\d+)\\.(\\d+)\\.(\\d+) of the Unicode\"\n-            unicode_version = re.search(pattern, readme.read()).groups()\n-        rf.write(\"\"\"\n-/// The version of [Unicode](http://www.unicode.org/) that the Unicode parts of\n-/// `char` and `str` methods are based on.\n-#[unstable(feature = \"unicode_version\", issue = \"49726\")]\n-pub const UNICODE_VERSION: UnicodeVersion = UnicodeVersion {\n-    major: %s,\n-    minor: %s,\n-    micro: %s,\n-    _priv: (),\n-};\n-\"\"\" % unicode_version)\n-        (canon_decomp, compat_decomp, gencats, combines,\n-                to_upper, to_lower, to_title) = load_unicode_data(\"UnicodeData.txt\")\n-        load_special_casing(\"SpecialCasing.txt\", to_upper, to_lower, to_title)\n-        want_derived = [\"XID_Start\", \"XID_Continue\", \"Alphabetic\", \"Lowercase\", \"Uppercase\",\n-                        \"Cased\", \"Case_Ignorable\", \"Grapheme_Extend\"]\n-        derived = load_properties(\"DerivedCoreProperties.txt\", want_derived)\n-        scripts = load_properties(\"Scripts.txt\", [])\n-        props = load_properties(\"PropList.txt\",\n-                [\"White_Space\", \"Join_Control\", \"Noncharacter_Code_Point\", \"Pattern_White_Space\"])\n-        norm_props = load_properties(\"DerivedNormalizationProps.txt\",\n-                     [\"Full_Composition_Exclusion\"])\n-\n-        # category tables\n-        for (name, cat, pfuns) in (\"general_category\", gencats, [\"N\", \"Cc\"]), \\\n-                                  (\"derived_property\", derived, want_derived), \\\n-                                  (\"property\", props, [\"White_Space\", \"Pattern_White_Space\"]):\n-            emit_property_module(rf, name, cat, pfuns)\n-\n-        # normalizations and conversions module\n-        emit_norm_module(rf, canon_decomp, compat_decomp, combines, norm_props)\n-        emit_conversions_module(rf, to_upper, to_lower, to_title)\n-    print(\"Regenerated tables.rs.\")\n+    main()"}]}