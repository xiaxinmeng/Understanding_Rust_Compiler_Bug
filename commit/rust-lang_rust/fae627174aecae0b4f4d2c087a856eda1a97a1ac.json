{"sha": "fae627174aecae0b4f4d2c087a856eda1a97a1ac", "node_id": "MDY6Q29tbWl0NzI0NzEyOmZhZTYyNzE3NGFlY2FlMGI0ZjRkMmMwODdhODU2ZWRhMWE5N2ExYWM=", "commit": {"author": {"name": "bors[bot]", "email": "26634292+bors[bot]@users.noreply.github.com", "date": "2020-03-24T16:41:56Z"}, "committer": {"name": "GitHub", "email": "noreply@github.com", "date": "2020-03-24T16:41:56Z"}, "message": "Merge #3664\n\n3664: Introduce TokenConverter Trait r=matklad a=edwin0cheng\n\nThis PR add a `TokenConverter` Trait to share the conversion logic between raw `lexer` token and Syntax Node Token.\r\n\r\nRelated #2158.\n\nCo-authored-by: Edwin Cheng <edwin0cheng@gmail.com>", "tree": {"sha": "36dac98581fa4ef0e1fbb5579aaa0fdc507d0927", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/36dac98581fa4ef0e1fbb5579aaa0fdc507d0927"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/fae627174aecae0b4f4d2c087a856eda1a97a1ac", "comment_count": 0, "verification": {"verified": true, "reason": "valid", "signature": "-----BEGIN PGP SIGNATURE-----\n\nwsBcBAABCAAQBQJeejhUCRBK7hj4Ov3rIwAAdHIIADzPm3+u3F/u+dihGRfadUHY\nJkH2jRnSwt7LYb9Av7K5E+ZGmqnLRjGU94TMwfA1wtfRsVtMWcA7OzLuiGDzL4WP\n9hlpc9qGZifab7PB/XXSFXmPrm7d3Uv1EvQxLj+uz5Q83Z8Kg7j0MCsKsFvIZau5\njnrvRujOPjyVteGpL1S7iwJT9BfE5t+sQMW1PtRgvtGqXHHBZf6E736aVXgbMrgO\nbHY1KVBIA371quw3WG36wHI5zadYBPt7C4qPkR1AlSTUuAFpAYRY39Qz1NPIxVp+\nTW+fVZ8XueJfjX7jirI07FtvLY3CKr5VH1DRYPZ20a+ZHinu0SBOq2Re5G4F1OA=\n=5I+n\n-----END PGP SIGNATURE-----\n", "payload": "tree 36dac98581fa4ef0e1fbb5579aaa0fdc507d0927\nparent 9690f6bc43c9624fcad73cb71b3f5a0ffd540ddf\nparent adc54632ae294cfd070c465c964a736ec56efa94\nauthor bors[bot] <26634292+bors[bot]@users.noreply.github.com> 1585068116 +0000\ncommitter GitHub <noreply@github.com> 1585068116 +0000\n\nMerge #3664\n\n3664: Introduce TokenConverter Trait r=matklad a=edwin0cheng\n\nThis PR add a `TokenConverter` Trait to share the conversion logic between raw `lexer` token and Syntax Node Token.\r\n\r\nRelated #2158.\n\nCo-authored-by: Edwin Cheng <edwin0cheng@gmail.com>\n"}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/fae627174aecae0b4f4d2c087a856eda1a97a1ac", "html_url": "https://github.com/rust-lang/rust/commit/fae627174aecae0b4f4d2c087a856eda1a97a1ac", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/fae627174aecae0b4f4d2c087a856eda1a97a1ac/comments", "author": {"login": "bors[bot]", "id": 26634292, "node_id": "MDM6Qm90MjY2MzQyOTI=", "avatar_url": "https://avatars.githubusercontent.com/in/1847?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bors%5Bbot%5D", "html_url": "https://github.com/apps/bors", "followers_url": "https://api.github.com/users/bors%5Bbot%5D/followers", "following_url": "https://api.github.com/users/bors%5Bbot%5D/following{/other_user}", "gists_url": "https://api.github.com/users/bors%5Bbot%5D/gists{/gist_id}", "starred_url": "https://api.github.com/users/bors%5Bbot%5D/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bors%5Bbot%5D/subscriptions", "organizations_url": "https://api.github.com/users/bors%5Bbot%5D/orgs", "repos_url": "https://api.github.com/users/bors%5Bbot%5D/repos", "events_url": "https://api.github.com/users/bors%5Bbot%5D/events{/privacy}", "received_events_url": "https://api.github.com/users/bors%5Bbot%5D/received_events", "type": "Bot", "site_admin": false}, "committer": {"login": "web-flow", "id": 19864447, "node_id": "MDQ6VXNlcjE5ODY0NDQ3", "avatar_url": "https://avatars.githubusercontent.com/u/19864447?v=4", "gravatar_id": "", "url": "https://api.github.com/users/web-flow", "html_url": "https://github.com/web-flow", "followers_url": "https://api.github.com/users/web-flow/followers", "following_url": "https://api.github.com/users/web-flow/following{/other_user}", "gists_url": "https://api.github.com/users/web-flow/gists{/gist_id}", "starred_url": "https://api.github.com/users/web-flow/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/web-flow/subscriptions", "organizations_url": "https://api.github.com/users/web-flow/orgs", "repos_url": "https://api.github.com/users/web-flow/repos", "events_url": "https://api.github.com/users/web-flow/events{/privacy}", "received_events_url": "https://api.github.com/users/web-flow/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "9690f6bc43c9624fcad73cb71b3f5a0ffd540ddf", "url": "https://api.github.com/repos/rust-lang/rust/commits/9690f6bc43c9624fcad73cb71b3f5a0ffd540ddf", "html_url": "https://github.com/rust-lang/rust/commit/9690f6bc43c9624fcad73cb71b3f5a0ffd540ddf"}, {"sha": "adc54632ae294cfd070c465c964a736ec56efa94", "url": "https://api.github.com/repos/rust-lang/rust/commits/adc54632ae294cfd070c465c964a736ec56efa94", "html_url": "https://github.com/rust-lang/rust/commit/adc54632ae294cfd070c465c964a736ec56efa94"}], "stats": {"total": 421, "additions": 250, "deletions": 171}, "files": [{"sha": "e3cde9eedcb331855947c8ad5ff59f5400b9b1af", "filename": "crates/ra_mbe/src/syntax_bridge.rs", "status": "modified", "additions": 203, "deletions": 156, "changes": 359, "blob_url": "https://github.com/rust-lang/rust/blob/fae627174aecae0b4f4d2c087a856eda1a97a1ac/crates%2Fra_mbe%2Fsrc%2Fsyntax_bridge.rs", "raw_url": "https://github.com/rust-lang/rust/raw/fae627174aecae0b4f4d2c087a856eda1a97a1ac/crates%2Fra_mbe%2Fsrc%2Fsyntax_bridge.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fra_mbe%2Fsrc%2Fsyntax_bridge.rs?ref=fae627174aecae0b4f4d2c087a856eda1a97a1ac", "patch": "@@ -3,12 +3,11 @@\n use ra_parser::{FragmentKind, ParseError, TreeSink};\n use ra_syntax::{\n     ast::{self, make::tokens::doc_comment},\n-    tokenize, AstToken, NodeOrToken, Parse, SmolStr, SyntaxKind,\n+    tokenize, AstToken, Parse, SmolStr, SyntaxKind,\n     SyntaxKind::*,\n-    SyntaxNode, SyntaxTreeBuilder, TextRange, TextUnit, Token, T,\n+    SyntaxNode, SyntaxToken, SyntaxTreeBuilder, TextRange, TextUnit, Token as RawToken, T,\n };\n use rustc_hash::FxHashMap;\n-use std::iter::successors;\n use tt::buffer::{Cursor, TokenBuffer};\n \n use crate::subtree_source::SubtreeTokenSource;\n@@ -50,10 +49,8 @@ pub fn ast_to_token_tree(ast: &impl ast::AstNode) -> Option<(tt::Subtree, TokenM\n /// will consume).\n pub fn syntax_node_to_token_tree(node: &SyntaxNode) -> Option<(tt::Subtree, TokenMap)> {\n     let global_offset = node.text_range().start();\n-    let mut c = Convertor {\n-        id_alloc: { TokenIdAlloc { map: TokenMap::default(), global_offset, next_id: 0 } },\n-    };\n-    let subtree = c.go(node)?;\n+    let mut c = Convertor::new(node, global_offset);\n+    let subtree = c.go()?;\n     Some((subtree, c.id_alloc.map))\n }\n \n@@ -152,6 +149,10 @@ impl TokenMap {\n             }\n         }\n     }\n+\n+    fn remove_delim(&mut self, token_id: tt::TokenId) {\n+        self.entries.retain(|(tid, _)| *tid != token_id);\n+    }\n }\n \n /// Returns the textual content of a doc comment block as a quoted string\n@@ -237,25 +238,26 @@ impl TokenIdAlloc {\n         token_id\n     }\n \n-    fn delim(&mut self, open_abs_range: TextRange, close_abs_range: TextRange) -> tt::TokenId {\n-        let open_relative_range = open_abs_range - self.global_offset;\n-        let close_relative_range = close_abs_range - self.global_offset;\n-        let token_id = tt::TokenId(self.next_id);\n-        self.next_id += 1;\n-\n-        self.map.insert_delim(token_id, open_relative_range, close_relative_range);\n-        token_id\n-    }\n-\n     fn open_delim(&mut self, open_abs_range: TextRange) -> tt::TokenId {\n         let token_id = tt::TokenId(self.next_id);\n         self.next_id += 1;\n-        self.map.insert_delim(token_id, open_abs_range, open_abs_range);\n+        self.map.insert_delim(\n+            token_id,\n+            open_abs_range - self.global_offset,\n+            open_abs_range - self.global_offset,\n+        );\n         token_id\n     }\n \n-    fn close_delim(&mut self, id: tt::TokenId, close_abs_range: TextRange) {\n-        self.map.update_close_delim(id, close_abs_range);\n+    fn close_delim(&mut self, id: tt::TokenId, close_abs_range: Option<TextRange>) {\n+        match close_abs_range {\n+            None => {\n+                self.map.remove_delim(id);\n+            }\n+            Some(close) => {\n+                self.map.update_close_delim(id, close - self.global_offset);\n+            }\n+        }\n     }\n }\n \n@@ -264,10 +266,20 @@ struct RawConvertor<'a> {\n     text: &'a str,\n     offset: TextUnit,\n     id_alloc: TokenIdAlloc,\n-    inner: std::slice::Iter<'a, Token>,\n+    inner: std::slice::Iter<'a, RawToken>,\n }\n \n-impl RawConvertor<'_> {\n+trait SrcToken {\n+    fn kind(&self) -> SyntaxKind;\n+\n+    fn to_char(&self) -> Option<char>;\n+\n+    fn to_text(&self) -> SmolStr;\n+}\n+\n+trait TokenConvertor {\n+    type Token: SrcToken;\n+\n     fn go(&mut self) -> Option<tt::Subtree> {\n         let mut subtree = tt::Subtree::default();\n         subtree.delimiter = None;\n@@ -285,33 +297,22 @@ impl RawConvertor<'_> {\n         Some(subtree)\n     }\n \n-    fn bump(&mut self) -> Option<(Token, TextRange)> {\n-        let token = self.inner.next()?;\n-        let range = TextRange::offset_len(self.offset, token.len);\n-        self.offset += token.len;\n-        Some((*token, range))\n-    }\n-\n-    fn peek(&self) -> Option<Token> {\n-        self.inner.as_slice().get(0).cloned()\n-    }\n-\n     fn collect_leaf(&mut self, result: &mut Vec<tt::TokenTree>) {\n         let (token, range) = match self.bump() {\n             None => return,\n             Some(it) => it,\n         };\n \n-        let k: SyntaxKind = token.kind;\n+        let k: SyntaxKind = token.kind();\n         if k == COMMENT {\n-            let node = doc_comment(&self.text[range]);\n-            if let Some(tokens) = convert_doc_comment(&node) {\n+            if let Some(tokens) = self.convert_doc_comment(&token) {\n                 result.extend(tokens);\n             }\n             return;\n         }\n \n         result.push(if k.is_punct() {\n+            assert_eq!(range.len().to_usize(), 1);\n             let delim = match k {\n                 T!['('] => Some((tt::DelimiterKind::Parenthesis, T![')'])),\n                 T!['{'] => Some((tt::DelimiterKind::Brace, T!['}'])),\n@@ -321,40 +322,51 @@ impl RawConvertor<'_> {\n \n             if let Some((kind, closed)) = delim {\n                 let mut subtree = tt::Subtree::default();\n-                let id = self.id_alloc.open_delim(range);\n+                let id = self.id_alloc().open_delim(range);\n                 subtree.delimiter = Some(tt::Delimiter { kind, id });\n \n-                while self.peek().map(|it| it.kind != closed).unwrap_or(false) {\n+                while self.peek().map(|it| it.kind() != closed).unwrap_or(false) {\n                     self.collect_leaf(&mut subtree.token_trees);\n                 }\n                 let last_range = match self.bump() {\n-                    None => return,\n+                    None => {\n+                        // For error resilience, we insert an char punct for the opening delim here\n+                        self.id_alloc().close_delim(id, None);\n+                        let leaf: tt::Leaf = tt::Punct {\n+                            id: self.id_alloc().alloc(range),\n+                            char: token.to_char().unwrap(),\n+                            spacing: tt::Spacing::Alone,\n+                        }\n+                        .into();\n+                        result.push(leaf.into());\n+                        result.extend(subtree.token_trees);\n+                        return;\n+                    }\n                     Some(it) => it.1,\n                 };\n-                self.id_alloc.close_delim(id, last_range);\n+                self.id_alloc().close_delim(id, Some(last_range));\n                 subtree.into()\n             } else {\n                 let spacing = match self.peek() {\n                     Some(next)\n-                        if next.kind.is_trivia()\n-                            || next.kind == T!['[']\n-                            || next.kind == T!['{']\n-                            || next.kind == T!['('] =>\n+                        if next.kind().is_trivia()\n+                            || next.kind() == T!['[']\n+                            || next.kind() == T!['{']\n+                            || next.kind() == T!['('] =>\n                     {\n                         tt::Spacing::Alone\n                     }\n-                    Some(next) if next.kind.is_punct() => tt::Spacing::Joint,\n+                    Some(next) if next.kind().is_punct() => tt::Spacing::Joint,\n                     _ => tt::Spacing::Alone,\n                 };\n-                let char =\n-                    self.text[range].chars().next().expect(\"Token from lexer must be single char\");\n+                let char = token.to_char().expect(\"Token from lexer must be single char\");\n \n-                tt::Leaf::from(tt::Punct { char, spacing, id: self.id_alloc.alloc(range) }).into()\n+                tt::Leaf::from(tt::Punct { char, spacing, id: self.id_alloc().alloc(range) }).into()\n             }\n         } else {\n             macro_rules! make_leaf {\n                 ($i:ident) => {\n-                    tt::$i { id: self.id_alloc.alloc(range), text: self.text[range].into() }.into()\n+                    tt::$i { id: self.id_alloc().alloc(range), text: token.to_text() }.into()\n                 };\n             }\n             let leaf: tt::Leaf = match k {\n@@ -368,133 +380,168 @@ impl RawConvertor<'_> {\n             leaf.into()\n         });\n     }\n+\n+    fn convert_doc_comment(&self, token: &Self::Token) -> Option<Vec<tt::TokenTree>>;\n+\n+    fn bump(&mut self) -> Option<(Self::Token, TextRange)>;\n+\n+    fn peek(&self) -> Option<Self::Token>;\n+\n+    fn id_alloc(&mut self) -> &mut TokenIdAlloc;\n+}\n+\n+impl<'a> SrcToken for (RawToken, &'a str) {\n+    fn kind(&self) -> SyntaxKind {\n+        self.0.kind\n+    }\n+\n+    fn to_char(&self) -> Option<char> {\n+        self.1.chars().next()\n+    }\n+\n+    fn to_text(&self) -> SmolStr {\n+        self.1.into()\n+    }\n+}\n+\n+impl RawConvertor<'_> {}\n+\n+impl<'a> TokenConvertor for RawConvertor<'a> {\n+    type Token = (RawToken, &'a str);\n+\n+    fn convert_doc_comment(&self, token: &Self::Token) -> Option<Vec<tt::TokenTree>> {\n+        convert_doc_comment(&doc_comment(token.1))\n+    }\n+\n+    fn bump(&mut self) -> Option<(Self::Token, TextRange)> {\n+        let token = self.inner.next()?;\n+        let range = TextRange::offset_len(self.offset, token.len);\n+        self.offset += token.len;\n+\n+        Some(((*token, &self.text[range]), range))\n+    }\n+\n+    fn peek(&self) -> Option<Self::Token> {\n+        let token = self.inner.as_slice().get(0).cloned();\n+\n+        token.map(|it| {\n+            let range = TextRange::offset_len(self.offset, it.len);\n+            (it, &self.text[range])\n+        })\n+    }\n+\n+    fn id_alloc(&mut self) -> &mut TokenIdAlloc {\n+        &mut self.id_alloc\n+    }\n }\n \n-// FIXME: There are some duplicate logic between RawConvertor and Convertor\n-// It would be nice to refactor to converting SyntaxNode to ra_parser::Token and thus\n-// use RawConvertor directly. But performance-wise it may not be a good idea ?\n struct Convertor {\n     id_alloc: TokenIdAlloc,\n+    current: Option<SyntaxToken>,\n+    range: TextRange,\n+    punct_offset: Option<(SyntaxToken, TextUnit)>,\n }\n \n impl Convertor {\n-    fn go(&mut self, tt: &SyntaxNode) -> Option<tt::Subtree> {\n-        // This tree is empty\n-        if tt.first_child_or_token().is_none() {\n-            return Some(tt::Subtree { token_trees: vec![], delimiter: None });\n+    fn new(node: &SyntaxNode, global_offset: TextUnit) -> Convertor {\n+        Convertor {\n+            id_alloc: { TokenIdAlloc { map: TokenMap::default(), global_offset, next_id: 0 } },\n+            current: node.first_token(),\n+            range: node.text_range(),\n+            punct_offset: None,\n         }\n+    }\n+}\n \n-        let first_child = tt.first_child_or_token()?;\n-        let last_child = tt.last_child_or_token()?;\n+enum SynToken {\n+    Ordiniary(SyntaxToken),\n+    Punch(SyntaxToken, TextUnit),\n+}\n \n-        // ignore trivial first_child and last_child\n-        let first_child = successors(Some(first_child), |it| {\n-            if it.kind().is_trivia() {\n-                it.next_sibling_or_token()\n-            } else {\n-                None\n-            }\n-        })\n-        .last()\n-        .unwrap();\n-        if first_child.kind().is_trivia() {\n-            return Some(tt::Subtree { token_trees: vec![], delimiter: None });\n+impl SynToken {\n+    fn token(&self) -> &SyntaxToken {\n+        match self {\n+            SynToken::Ordiniary(it) => it,\n+            SynToken::Punch(it, _) => it,\n         }\n+    }\n+}\n \n-        let last_child = successors(Some(last_child), |it| {\n-            if it.kind().is_trivia() {\n-                it.prev_sibling_or_token()\n-            } else {\n-                None\n+impl SrcToken for SynToken {\n+    fn kind(&self) -> SyntaxKind {\n+        self.token().kind()\n+    }\n+    fn to_char(&self) -> Option<char> {\n+        match self {\n+            SynToken::Ordiniary(_) => None,\n+            SynToken::Punch(it, i) => it.text().chars().nth(i.to_usize()),\n+        }\n+    }\n+    fn to_text(&self) -> SmolStr {\n+        self.token().text().clone()\n+    }\n+}\n+\n+impl TokenConvertor for Convertor {\n+    type Token = SynToken;\n+    fn convert_doc_comment(&self, token: &Self::Token) -> Option<Vec<tt::TokenTree>> {\n+        convert_doc_comment(token.token())\n+    }\n+\n+    fn bump(&mut self) -> Option<(Self::Token, TextRange)> {\n+        if let Some((punct, offset)) = self.punct_offset.clone() {\n+            if offset.to_usize() + 1 < punct.text().len() {\n+                let offset = offset + TextUnit::from_usize(1);\n+                let range = punct.text_range();\n+                self.punct_offset = Some((punct.clone(), offset));\n+                let range = TextRange::offset_len(range.start() + offset, TextUnit::from_usize(1));\n+                return Some((SynToken::Punch(punct, offset), range));\n             }\n-        })\n-        .last()\n-        .unwrap();\n-\n-        let (delimiter_kind, skip_first) = match (first_child.kind(), last_child.kind()) {\n-            (T!['('], T![')']) => (Some(tt::DelimiterKind::Parenthesis), true),\n-            (T!['{'], T!['}']) => (Some(tt::DelimiterKind::Brace), true),\n-            (T!['['], T![']']) => (Some(tt::DelimiterKind::Bracket), true),\n-            _ => (None, false),\n+        }\n+\n+        let curr = self.current.clone()?;\n+        if !curr.text_range().is_subrange(&self.range) {\n+            return None;\n+        }\n+        self.current = curr.next_token();\n+\n+        let token = if curr.kind().is_punct() {\n+            let range = curr.text_range();\n+            let range = TextRange::offset_len(range.start(), TextUnit::from_usize(1));\n+            self.punct_offset = Some((curr.clone(), TextUnit::from_usize(0)));\n+            (SynToken::Punch(curr, TextUnit::from_usize(0)), range)\n+        } else {\n+            self.punct_offset = None;\n+            let range = curr.text_range();\n+            (SynToken::Ordiniary(curr), range)\n         };\n-        let delimiter = delimiter_kind.map(|kind| tt::Delimiter {\n-            kind,\n-            id: self.id_alloc.delim(first_child.text_range(), last_child.text_range()),\n-        });\n \n-        let mut token_trees = Vec::new();\n-        let mut child_iter = tt.children_with_tokens().skip(skip_first as usize).peekable();\n+        Some(token)\n+    }\n \n-        while let Some(child) = child_iter.next() {\n-            if skip_first && (child == first_child || child == last_child) {\n-                continue;\n+    fn peek(&self) -> Option<Self::Token> {\n+        if let Some((punct, mut offset)) = self.punct_offset.clone() {\n+            offset = offset + TextUnit::from_usize(1);\n+            if offset.to_usize() < punct.text().len() {\n+                return Some(SynToken::Punch(punct, offset));\n             }\n+        }\n \n-            match child {\n-                NodeOrToken::Token(token) => {\n-                    if let Some(doc_tokens) = convert_doc_comment(&token) {\n-                        token_trees.extend(doc_tokens);\n-                    } else if token.kind().is_trivia() {\n-                        continue;\n-                    } else if token.kind().is_punct() {\n-                        // we need to pull apart joined punctuation tokens\n-                        let last_spacing = match child_iter.peek() {\n-                            Some(NodeOrToken::Token(token)) => {\n-                                if token.kind().is_punct() {\n-                                    tt::Spacing::Joint\n-                                } else {\n-                                    tt::Spacing::Alone\n-                                }\n-                            }\n-                            _ => tt::Spacing::Alone,\n-                        };\n-                        let spacing_iter = std::iter::repeat(tt::Spacing::Joint)\n-                            .take(token.text().len() - 1)\n-                            .chain(std::iter::once(last_spacing));\n-                        for (char, spacing) in token.text().chars().zip(spacing_iter) {\n-                            token_trees.push(\n-                                tt::Leaf::from(tt::Punct {\n-                                    char,\n-                                    spacing,\n-                                    id: self.id_alloc.alloc(token.text_range()),\n-                                })\n-                                .into(),\n-                            );\n-                        }\n-                    } else {\n-                        macro_rules! make_leaf {\n-                            ($i:ident) => {\n-                                tt::$i {\n-                                    id: self.id_alloc.alloc(token.text_range()),\n-                                    text: token.text().clone(),\n-                                }\n-                                .into()\n-                            };\n-                        }\n-\n-                        let child: tt::Leaf = match token.kind() {\n-                            T![true] | T![false] => make_leaf!(Literal),\n-                            IDENT | LIFETIME => make_leaf!(Ident),\n-                            k if k.is_keyword() => make_leaf!(Ident),\n-                            k if k.is_literal() => make_leaf!(Literal),\n-                            _ => return None,\n-                        };\n-                        token_trees.push(child.into());\n-                    }\n-                }\n-                NodeOrToken::Node(node) => {\n-                    let child_subtree = self.go(&node)?;\n-                    if child_subtree.delimiter.is_none() && node.kind() != SyntaxKind::TOKEN_TREE {\n-                        token_trees.extend(child_subtree.token_trees);\n-                    } else {\n-                        token_trees.push(child_subtree.into());\n-                    }\n-                }\n-            };\n+        let curr = self.current.clone()?;\n+        if !curr.text_range().is_subrange(&self.range) {\n+            return None;\n         }\n \n-        let res = tt::Subtree { delimiter, token_trees };\n-        Some(res)\n+        let token = if curr.kind().is_punct() {\n+            SynToken::Punch(curr, TextUnit::from_usize(0))\n+        } else {\n+            SynToken::Ordiniary(curr)\n+        };\n+        Some(token)\n+    }\n+\n+    fn id_alloc(&mut self) -> &mut TokenIdAlloc {\n+        &mut self.id_alloc\n     }\n }\n "}, {"sha": "a7fcea0acee23cf28e426897bd1bb58b84b40534", "filename": "crates/ra_mbe/src/tests.rs", "status": "modified", "additions": 47, "deletions": 15, "changes": 62, "blob_url": "https://github.com/rust-lang/rust/blob/fae627174aecae0b4f4d2c087a856eda1a97a1ac/crates%2Fra_mbe%2Fsrc%2Ftests.rs", "raw_url": "https://github.com/rust-lang/rust/raw/fae627174aecae0b4f4d2c087a856eda1a97a1ac/crates%2Fra_mbe%2Fsrc%2Ftests.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fra_mbe%2Fsrc%2Ftests.rs?ref=fae627174aecae0b4f4d2c087a856eda1a97a1ac", "patch": "@@ -427,22 +427,28 @@ MACRO_ITEMS@[0; 40)\n     );\n }\n \n-#[test]\n-fn test_expand_literals_to_token_tree() {\n-    fn to_subtree(tt: &tt::TokenTree) -> &tt::Subtree {\n-        if let tt::TokenTree::Subtree(subtree) = tt {\n-            return &subtree;\n-        }\n-        unreachable!(\"It is not a subtree\");\n+fn to_subtree(tt: &tt::TokenTree) -> &tt::Subtree {\n+    if let tt::TokenTree::Subtree(subtree) = tt {\n+        return &subtree;\n     }\n+    unreachable!(\"It is not a subtree\");\n+}\n+fn to_literal(tt: &tt::TokenTree) -> &tt::Literal {\n+    if let tt::TokenTree::Leaf(tt::Leaf::Literal(lit)) = tt {\n+        return lit;\n+    }\n+    unreachable!(\"It is not a literal\");\n+}\n \n-    fn to_literal(tt: &tt::TokenTree) -> &tt::Literal {\n-        if let tt::TokenTree::Leaf(tt::Leaf::Literal(lit)) = tt {\n-            return lit;\n-        }\n-        unreachable!(\"It is not a literal\");\n+fn to_punct(tt: &tt::TokenTree) -> &tt::Punct {\n+    if let tt::TokenTree::Leaf(tt::Leaf::Punct(lit)) = tt {\n+        return lit;\n     }\n+    unreachable!(\"It is not a Punct\");\n+}\n \n+#[test]\n+fn test_expand_literals_to_token_tree() {\n     let expansion = parse_macro(\n         r#\"\n             macro_rules! literals {\n@@ -470,6 +476,22 @@ fn test_expand_literals_to_token_tree() {\n     assert_eq!(to_literal(&stm_tokens[15 + 3]).text, \"\\\"rust1\\\"\");\n }\n \n+#[test]\n+fn test_attr_to_token_tree() {\n+    let expansion = parse_to_token_tree_by_syntax(\n+        r#\"\n+            #[derive(Copy)]\n+            struct Foo;\n+            \"#,\n+    );\n+\n+    assert_eq!(to_punct(&expansion.token_trees[0]).char, '#');\n+    assert_eq!(\n+        to_subtree(&expansion.token_trees[1]).delimiter_kind(),\n+        Some(tt::DelimiterKind::Bracket)\n+    );\n+}\n+\n #[test]\n fn test_two_idents() {\n     parse_macro(\n@@ -1427,8 +1449,8 @@ impl MacroFixture {\n         let macro_invocation =\n             source_file.syntax().descendants().find_map(ast::MacroCall::cast).unwrap();\n \n-        let (invocation_tt, _) =\n-            ast_to_token_tree(&macro_invocation.token_tree().unwrap()).unwrap();\n+        let (invocation_tt, _) = ast_to_token_tree(&macro_invocation.token_tree().unwrap())\n+            .ok_or_else(|| ExpandError::ConversionError)?;\n \n         self.rules.expand(&invocation_tt).result()\n     }\n@@ -1517,6 +1539,16 @@ pub(crate) fn parse_macro(ra_fixture: &str) -> MacroFixture {\n     MacroFixture { rules }\n }\n \n+pub(crate) fn parse_to_token_tree_by_syntax(ra_fixture: &str) -> tt::Subtree {\n+    let source_file = ast::SourceFile::parse(ra_fixture).ok().unwrap();\n+    let tt = syntax_node_to_token_tree(source_file.syntax()).unwrap().0;\n+\n+    let parsed = parse_to_token_tree(ra_fixture).unwrap().0;\n+    assert_eq!(tt, parsed);\n+\n+    parsed\n+}\n+\n fn debug_dump_ignore_spaces(node: &ra_syntax::SyntaxNode) -> String {\n     let mut level = 0;\n     let mut buf = String::new();\n@@ -1662,5 +1694,5 @@ fn test_expand_bad_literal() {\n         macro_rules! foo { ($i:literal) => {}; }\n     \"#,\n     )\n-    .assert_expand_err(r#\"foo!(&k\");\"#, &ExpandError::BindingError(\"\".to_string()));\n+    .assert_expand_err(r#\"foo!(&k\");\"#, &ExpandError::BindingError(\"\".into()));\n }"}]}