{"sha": "f2d829ce6ab52c9e5b9aacd1181c87208c736ee9", "node_id": "MDY6Q29tbWl0NzI0NzEyOmYyZDgyOWNlNmFiNTJjOWU1YjlhYWNkMTE4MWM4NzIwOGM3MzZlZTk=", "commit": {"author": {"name": "Dylan DPC", "email": "dylan.dpc@gmail.com", "date": "2020-02-12T13:21:06Z"}, "committer": {"name": "GitHub", "email": "noreply@github.com", "date": "2020-02-12T13:21:06Z"}, "message": "Rollup merge of #68914 - nnethercote:speed-up-SipHasher128, r=michaelwoerister\n\nSpeed up `SipHasher128`.\n\nThe current code in `SipHasher128::short_write` is inefficient. It uses\n`u8to64_le` (which is complex and slow) to extract just the right number of\nbytes of the input into a u64 and pad the result with zeroes. It then\nleft-shifts that value in order to bitwise-OR it with `self.tail`.\n\nFor example, imagine we have a u32 input `0xIIHH_GGFF` and only need three bytes\nto fill up `self.tail`. The current code uses `u8to64_le` to construct\n`0x0000_0000_00HH_GGFF`, which is just `0xIIHH_GGFF` with the `0xII` removed and\nzero-extended to a u64. The code then left-shifts that value by five bytes --\ndiscarding the `0x00` byte that replaced the `0xII` byte! -- to give\n`0xHHGG_FF00_0000_0000`. It then then ORs that value with `self.tail`.\n\nThere's a much simpler way to do it: zero-extend to u64 first, then left shift.\nE.g. `0xIIHH_GGFF` is zero-extended to `0x0000_0000_IIHH_GGFF`, and then\nleft-shifted to `0xHHGG_FF00_0000_0000`. We don't have to take time to exclude\nthe unneeded `0xII` byte, because it just gets shifted out anyway! It also avoids\nmultiple occurrences of `unsafe`.\n\nThere's a similar story with the setting of `self.tail` at the method's end.\nThe current code uses `u8to64_le` to extract the remaining part of the input,\nbut the same effect can be achieved more quickly with a right shift on the\nzero-extended input.\n\nThis commit changes `SipHasher128` to use the simpler shift-based approach. The\ncode is also smaller, which means that `short_write` is now inlined where\npreviously it wasn't, which makes things faster again. This gives big\nspeed-ups for all incremental builds, especially \"baseline\" incremental\nbuilds.\n\nr? @michaelwoerister", "tree": {"sha": "6e0a83292130eea697f8bbb96ed79dfd387711ea", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/6e0a83292130eea697f8bbb96ed79dfd387711ea"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/f2d829ce6ab52c9e5b9aacd1181c87208c736ee9", "comment_count": 0, "verification": {"verified": true, "reason": "valid", "signature": "-----BEGIN PGP SIGNATURE-----\n\nwsBcBAABCAAQBQJeQ/vDCRBK7hj4Ov3rIwAAdHIIAFlrX6L5Vs4OycIyNcU94STb\nq74brFJ3x6yX+APTQpekWkktGGfEP9DV6tIyV5JaLIpHWj7yZ3eSsdfdj4h6UahH\nVYHxMYp5jOJfWa+kBxGWi/6vtVjGQ3K235NEJ4DjsmiD2knxdhcdXxUT8VHy7XA5\nXuhIvASML6eENJj1bILQXezvSbI2Qq+5w1awT8s/KetCTgt3rBpQqzGkIaVwbeNd\nijC7LrlU4d5ad9jV+hHlJe1a1inzbfoXYMNzFQMVxovSb7MkKhBSUnSQ9E1FQpzX\nH4gwPvI6u3F7D49tAYhaWy38jBZ4dnLVxu8/oZjk/b9M0VL+Hq5cr/5ofpTy+mg=\n=F4Qm\n-----END PGP SIGNATURE-----\n", "payload": "tree 6e0a83292130eea697f8bbb96ed79dfd387711ea\nparent 79ebf53cbb659175e863cdc4ca6341ee934376fd\nparent 9aea154e7893b498b98a3d9c8e4c385c96fbe454\nauthor Dylan DPC <dylan.dpc@gmail.com> 1581513666 +0100\ncommitter GitHub <noreply@github.com> 1581513666 +0100\n\nRollup merge of #68914 - nnethercote:speed-up-SipHasher128, r=michaelwoerister\n\nSpeed up `SipHasher128`.\n\nThe current code in `SipHasher128::short_write` is inefficient. It uses\n`u8to64_le` (which is complex and slow) to extract just the right number of\nbytes of the input into a u64 and pad the result with zeroes. It then\nleft-shifts that value in order to bitwise-OR it with `self.tail`.\n\nFor example, imagine we have a u32 input `0xIIHH_GGFF` and only need three bytes\nto fill up `self.tail`. The current code uses `u8to64_le` to construct\n`0x0000_0000_00HH_GGFF`, which is just `0xIIHH_GGFF` with the `0xII` removed and\nzero-extended to a u64. The code then left-shifts that value by five bytes --\ndiscarding the `0x00` byte that replaced the `0xII` byte! -- to give\n`0xHHGG_FF00_0000_0000`. It then then ORs that value with `self.tail`.\n\nThere's a much simpler way to do it: zero-extend to u64 first, then left shift.\nE.g. `0xIIHH_GGFF` is zero-extended to `0x0000_0000_IIHH_GGFF`, and then\nleft-shifted to `0xHHGG_FF00_0000_0000`. We don't have to take time to exclude\nthe unneeded `0xII` byte, because it just gets shifted out anyway! It also avoids\nmultiple occurrences of `unsafe`.\n\nThere's a similar story with the setting of `self.tail` at the method's end.\nThe current code uses `u8to64_le` to extract the remaining part of the input,\nbut the same effect can be achieved more quickly with a right shift on the\nzero-extended input.\n\nThis commit changes `SipHasher128` to use the simpler shift-based approach. The\ncode is also smaller, which means that `short_write` is now inlined where\npreviously it wasn't, which makes things faster again. This gives big\nspeed-ups for all incremental builds, especially \"baseline\" incremental\nbuilds.\n\nr? @michaelwoerister\n"}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/f2d829ce6ab52c9e5b9aacd1181c87208c736ee9", "html_url": "https://github.com/rust-lang/rust/commit/f2d829ce6ab52c9e5b9aacd1181c87208c736ee9", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/f2d829ce6ab52c9e5b9aacd1181c87208c736ee9/comments", "author": {"login": "Dylan-DPC", "id": 99973273, "node_id": "U_kgDOBfV4mQ", "avatar_url": "https://avatars.githubusercontent.com/u/99973273?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Dylan-DPC", "html_url": "https://github.com/Dylan-DPC", "followers_url": "https://api.github.com/users/Dylan-DPC/followers", "following_url": "https://api.github.com/users/Dylan-DPC/following{/other_user}", "gists_url": "https://api.github.com/users/Dylan-DPC/gists{/gist_id}", "starred_url": "https://api.github.com/users/Dylan-DPC/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Dylan-DPC/subscriptions", "organizations_url": "https://api.github.com/users/Dylan-DPC/orgs", "repos_url": "https://api.github.com/users/Dylan-DPC/repos", "events_url": "https://api.github.com/users/Dylan-DPC/events{/privacy}", "received_events_url": "https://api.github.com/users/Dylan-DPC/received_events", "type": "User", "site_admin": false}, "committer": {"login": "web-flow", "id": 19864447, "node_id": "MDQ6VXNlcjE5ODY0NDQ3", "avatar_url": "https://avatars.githubusercontent.com/u/19864447?v=4", "gravatar_id": "", "url": "https://api.github.com/users/web-flow", "html_url": "https://github.com/web-flow", "followers_url": "https://api.github.com/users/web-flow/followers", "following_url": "https://api.github.com/users/web-flow/following{/other_user}", "gists_url": "https://api.github.com/users/web-flow/gists{/gist_id}", "starred_url": "https://api.github.com/users/web-flow/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/web-flow/subscriptions", "organizations_url": "https://api.github.com/users/web-flow/orgs", "repos_url": "https://api.github.com/users/web-flow/repos", "events_url": "https://api.github.com/users/web-flow/events{/privacy}", "received_events_url": "https://api.github.com/users/web-flow/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "79ebf53cbb659175e863cdc4ca6341ee934376fd", "url": "https://api.github.com/repos/rust-lang/rust/commits/79ebf53cbb659175e863cdc4ca6341ee934376fd", "html_url": "https://github.com/rust-lang/rust/commit/79ebf53cbb659175e863cdc4ca6341ee934376fd"}, {"sha": "9aea154e7893b498b98a3d9c8e4c385c96fbe454", "url": "https://api.github.com/repos/rust-lang/rust/commits/9aea154e7893b498b98a3d9c8e4c385c96fbe454", "html_url": "https://github.com/rust-lang/rust/commit/9aea154e7893b498b98a3d9c8e4c385c96fbe454"}], "stats": {"total": 164, "additions": 84, "deletions": 80}, "files": [{"sha": "430f2f40caa9b7c889f5d5813ded32b923feffa8", "filename": "src/librustc_data_structures/sip128.rs", "status": "modified", "additions": 84, "deletions": 80, "changes": 164, "blob_url": "https://github.com/rust-lang/rust/blob/f2d829ce6ab52c9e5b9aacd1181c87208c736ee9/src%2Flibrustc_data_structures%2Fsip128.rs", "raw_url": "https://github.com/rust-lang/rust/raw/f2d829ce6ab52c9e5b9aacd1181c87208c736ee9/src%2Flibrustc_data_structures%2Fsip128.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibrustc_data_structures%2Fsip128.rs?ref=f2d829ce6ab52c9e5b9aacd1181c87208c736ee9", "patch": "@@ -4,7 +4,6 @@ use std::cmp;\n use std::hash::Hasher;\n use std::mem;\n use std::ptr;\n-use std::slice;\n \n #[cfg(test)]\n mod tests;\n@@ -52,46 +51,17 @@ macro_rules! compress {\n     }};\n }\n \n-/// Loads an integer of the desired type from a byte stream, in LE order. Uses\n-/// `copy_nonoverlapping` to let the compiler generate the most efficient way\n-/// to load it from a possibly unaligned address.\n-///\n-/// Unsafe because: unchecked indexing at i..i+size_of(int_ty)\n-macro_rules! load_int_le {\n-    ($buf:expr, $i:expr, $int_ty:ident) => {{\n-        debug_assert!($i + mem::size_of::<$int_ty>() <= $buf.len());\n-        let mut data = 0 as $int_ty;\n-        ptr::copy_nonoverlapping(\n-            $buf.get_unchecked($i),\n-            &mut data as *mut _ as *mut u8,\n-            mem::size_of::<$int_ty>(),\n-        );\n-        data.to_le()\n-    }};\n-}\n-\n-/// Loads an u64 using up to 7 bytes of a byte slice.\n-///\n-/// Unsafe because: unchecked indexing at start..start+len\n+/// Loads up to 8 bytes from a byte-slice into a little-endian u64.\n #[inline]\n-unsafe fn u8to64_le(buf: &[u8], start: usize, len: usize) -> u64 {\n-    debug_assert!(len < 8);\n-    let mut i = 0; // current byte index (from LSB) in the output u64\n-    let mut out = 0;\n-    if i + 3 < len {\n-        out = u64::from(load_int_le!(buf, start + i, u32));\n-        i += 4;\n-    }\n-    if i + 1 < len {\n-        out |= u64::from(load_int_le!(buf, start + i, u16)) << (i * 8);\n-        i += 2\n-    }\n-    if i < len {\n-        out |= u64::from(*buf.get_unchecked(start + i)) << (i * 8);\n-        i += 1;\n+fn u8to64_le(buf: &[u8], start: usize, len: usize) -> u64 {\n+    assert!(len <= 8 && start + len <= buf.len());\n+\n+    let mut out = 0u64;\n+    unsafe {\n+        let out_ptr = &mut out as *mut _ as *mut u8;\n+        ptr::copy_nonoverlapping(buf.as_ptr().offset(start as isize), out_ptr, len);\n     }\n-    debug_assert_eq!(i, len);\n-    out\n+    out.to_le()\n }\n \n impl SipHasher128 {\n@@ -122,42 +92,76 @@ impl SipHasher128 {\n         self.state.v1 ^= 0xee;\n     }\n \n-    // Specialized write function that is only valid for buffers with len <= 8.\n-    // It's used to force inlining of write_u8 and write_usize, those would normally be inlined\n-    // except for composite types (that includes slices and str hashing because of delimiter).\n-    // Without this extra push the compiler is very reluctant to inline delimiter writes,\n-    // degrading performance substantially for the most common use cases.\n+    // A specialized write function for values with size <= 8.\n+    //\n+    // The hashing of multi-byte integers depends on endianness. E.g.:\n+    // - little-endian: `write_u32(0xDDCCBBAA)` == `write([0xAA, 0xBB, 0xCC, 0xDD])`\n+    // - big-endian:    `write_u32(0xDDCCBBAA)` == `write([0xDD, 0xCC, 0xBB, 0xAA])`\n+    //\n+    // This function does the right thing for little-endian hardware. On\n+    // big-endian hardware `x` must be byte-swapped first to give the right\n+    // behaviour. After any byte-swapping, the input must be zero-extended to\n+    // 64-bits. The caller is responsible for the byte-swapping and\n+    // zero-extension.\n     #[inline]\n-    fn short_write(&mut self, msg: &[u8]) {\n-        debug_assert!(msg.len() <= 8);\n-        let length = msg.len();\n-        self.length += length;\n+    fn short_write<T>(&mut self, _x: T, x: u64) {\n+        let size = mem::size_of::<T>();\n+        self.length += size;\n+\n+        // The original number must be zero-extended, not sign-extended.\n+        debug_assert!(if size < 8 { x >> (8 * size) == 0 } else { true });\n \n+        // The number of bytes needed to fill `self.tail`.\n         let needed = 8 - self.ntail;\n-        let fill = cmp::min(length, needed);\n-        if fill == 8 {\n-            self.tail = unsafe { load_int_le!(msg, 0, u64) };\n-        } else {\n-            self.tail |= unsafe { u8to64_le(msg, 0, fill) } << (8 * self.ntail);\n-            if length < needed {\n-                self.ntail += length;\n-                return;\n-            }\n+\n+        // SipHash parses the input stream as 8-byte little-endian integers.\n+        // Inputs are put into `self.tail` until 8 bytes of data have been\n+        // collected, and then that word is processed.\n+        //\n+        // For example, imagine that `self.tail` is 0x0000_00EE_DDCC_BBAA,\n+        // `self.ntail` is 5 (because 5 bytes have been put into `self.tail`),\n+        // and `needed` is therefore 3.\n+        //\n+        // - Scenario 1, `self.write_u8(0xFF)`: we have already zero-extended\n+        //   the input to 0x0000_0000_0000_00FF. We now left-shift it five\n+        //   bytes, giving 0x0000_FF00_0000_0000. We then bitwise-OR that value\n+        //   into `self.tail`, resulting in 0x0000_FFEE_DDCC_BBAA.\n+        //   (Zero-extension of the original input is critical in this scenario\n+        //   because we don't want the high two bytes of `self.tail` to be\n+        //   touched by the bitwise-OR.) `self.tail` is not yet full, so we\n+        //   return early, after updating `self.ntail` to 6.\n+        //\n+        // - Scenario 2, `self.write_u32(0xIIHH_GGFF)`: we have already\n+        //   zero-extended the input to 0x0000_0000_IIHH_GGFF. We now\n+        //   left-shift it five bytes, giving 0xHHGG_FF00_0000_0000. We then\n+        //   bitwise-OR that value into `self.tail`, resulting in\n+        //   0xHHGG_FFEE_DDCC_BBAA. `self.tail` is now full, and we can use it\n+        //   to update `self.state`. (As mentioned above, this assumes a\n+        //   little-endian machine; on a big-endian machine we would have\n+        //   byte-swapped 0xIIHH_GGFF in the caller, giving 0xFFGG_HHII, and we\n+        //   would then end up bitwise-ORing 0xGGHH_II00_0000_0000 into\n+        //   `self.tail`).\n+        //\n+        self.tail |= x << (8 * self.ntail);\n+        if size < needed {\n+            self.ntail += size;\n+            return;\n         }\n+\n+        // `self.tail` is full, process it.\n         self.state.v3 ^= self.tail;\n         Sip24Rounds::c_rounds(&mut self.state);\n         self.state.v0 ^= self.tail;\n \n-        // Buffered tail is now flushed, process new input.\n-        self.ntail = length - needed;\n-        self.tail = unsafe { u8to64_le(msg, needed, self.ntail) };\n-    }\n-\n-    #[inline(always)]\n-    fn short_write_gen<T>(&mut self, x: T) {\n-        let bytes =\n-            unsafe { slice::from_raw_parts(&x as *const T as *const u8, mem::size_of::<T>()) };\n-        self.short_write(bytes);\n+        // Continuing scenario 2: we have one byte left over from the input. We\n+        // set `self.ntail` to 1 and `self.tail` to `0x0000_0000_IIHH_GGFF >>\n+        // 8*3`, which is 0x0000_0000_0000_00II. (Or on a big-endian machine\n+        // the prior byte-swapping would leave us with 0x0000_0000_0000_00FF.)\n+        //\n+        // The `if` is needed to avoid shifting by 64 bits, which Rust\n+        // complains about.\n+        self.ntail = size - needed;\n+        self.tail = if needed < 8 { x >> (8 * needed) } else { 0 };\n     }\n \n     #[inline]\n@@ -182,52 +186,52 @@ impl SipHasher128 {\n impl Hasher for SipHasher128 {\n     #[inline]\n     fn write_u8(&mut self, i: u8) {\n-        self.short_write_gen(i);\n+        self.short_write(i, i as u64);\n     }\n \n     #[inline]\n     fn write_u16(&mut self, i: u16) {\n-        self.short_write_gen(i);\n+        self.short_write(i, i.to_le() as u64);\n     }\n \n     #[inline]\n     fn write_u32(&mut self, i: u32) {\n-        self.short_write_gen(i);\n+        self.short_write(i, i.to_le() as u64);\n     }\n \n     #[inline]\n     fn write_u64(&mut self, i: u64) {\n-        self.short_write_gen(i);\n+        self.short_write(i, i.to_le() as u64);\n     }\n \n     #[inline]\n     fn write_usize(&mut self, i: usize) {\n-        self.short_write_gen(i);\n+        self.short_write(i, i.to_le() as u64);\n     }\n \n     #[inline]\n     fn write_i8(&mut self, i: i8) {\n-        self.short_write_gen(i);\n+        self.short_write(i, i as u8 as u64);\n     }\n \n     #[inline]\n     fn write_i16(&mut self, i: i16) {\n-        self.short_write_gen(i);\n+        self.short_write(i, (i as u16).to_le() as u64);\n     }\n \n     #[inline]\n     fn write_i32(&mut self, i: i32) {\n-        self.short_write_gen(i);\n+        self.short_write(i, (i as u32).to_le() as u64);\n     }\n \n     #[inline]\n     fn write_i64(&mut self, i: i64) {\n-        self.short_write_gen(i);\n+        self.short_write(i, (i as u64).to_le() as u64);\n     }\n \n     #[inline]\n     fn write_isize(&mut self, i: isize) {\n-        self.short_write_gen(i);\n+        self.short_write(i, (i as usize).to_le() as u64);\n     }\n \n     #[inline]\n@@ -239,7 +243,7 @@ impl Hasher for SipHasher128 {\n \n         if self.ntail != 0 {\n             needed = 8 - self.ntail;\n-            self.tail |= unsafe { u8to64_le(msg, 0, cmp::min(length, needed)) } << (8 * self.ntail);\n+            self.tail |= u8to64_le(msg, 0, cmp::min(length, needed)) << (8 * self.ntail);\n             if length < needed {\n                 self.ntail += length;\n                 return;\n@@ -257,7 +261,7 @@ impl Hasher for SipHasher128 {\n \n         let mut i = needed;\n         while i < len - left {\n-            let mi = unsafe { load_int_le!(msg, i, u64) };\n+            let mi = u8to64_le(msg, i, 8);\n \n             self.state.v3 ^= mi;\n             Sip24Rounds::c_rounds(&mut self.state);\n@@ -266,7 +270,7 @@ impl Hasher for SipHasher128 {\n             i += 8;\n         }\n \n-        self.tail = unsafe { u8to64_le(msg, i, left) };\n+        self.tail = u8to64_le(msg, i, left);\n         self.ntail = left;\n     }\n "}]}