{"sha": "b5590423e6ceb048dd7d792382e960d66b7615d2", "node_id": "MDY6Q29tbWl0NzI0NzEyOmI1NTkwNDIzZTZjZWIwNDhkZDdkNzkyMzgyZTk2MGQ2NmI3NjE1ZDI=", "commit": {"author": {"name": "bors", "email": "bors@rust-lang.org", "date": "2018-08-16T15:44:30Z"}, "committer": {"name": "bors", "email": "bors@rust-lang.org", "date": "2018-08-16T15:44:30Z"}, "message": "Auto merge of #53304 - dtolnay:extend, r=dtolnay\n\nTokenStream::extend\n\nTwo new insta-stable impls in libproc_macro:\n\n```rust\nimpl Extend<TokenTree> for TokenStream\nimpl Extend<TokenStream> for TokenStream\n```\n\n`proc_macro::TokenStream` already implements `FromIterator<TokenTree>` and `FromIterator<TokenStream>` so I elected to support the same input types for `Extend`.\n\n**This commit reduces compile time of Serde derives by 60% (takes less than half as long to compile)** as measured by building our test suite:\n\n```console\n$ git clone https://github.com/serde-rs/serde\n$ cd serde/test_suite\n$ cargo check --tests --features proc-macro2/nightly\n$ rm -f ../target/debug/deps/libtest_*.rmeta\n$ time cargo check --tests --features proc-macro2/nightly\nBefore: 20.8 seconds\nAfter: 8.6 seconds\n```\n\nr? @alexcrichton", "tree": {"sha": "378f5a5992fb8915a27e61d594a6497df71d6e5b", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/378f5a5992fb8915a27e61d594a6497df71d6e5b"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/b5590423e6ceb048dd7d792382e960d66b7615d2", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/b5590423e6ceb048dd7d792382e960d66b7615d2", "html_url": "https://github.com/rust-lang/rust/commit/b5590423e6ceb048dd7d792382e960d66b7615d2", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/b5590423e6ceb048dd7d792382e960d66b7615d2/comments", "author": {"login": "bors", "id": 3372342, "node_id": "MDQ6VXNlcjMzNzIzNDI=", "avatar_url": "https://avatars.githubusercontent.com/u/3372342?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bors", "html_url": "https://github.com/bors", "followers_url": "https://api.github.com/users/bors/followers", "following_url": "https://api.github.com/users/bors/following{/other_user}", "gists_url": "https://api.github.com/users/bors/gists{/gist_id}", "starred_url": "https://api.github.com/users/bors/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bors/subscriptions", "organizations_url": "https://api.github.com/users/bors/orgs", "repos_url": "https://api.github.com/users/bors/repos", "events_url": "https://api.github.com/users/bors/events{/privacy}", "received_events_url": "https://api.github.com/users/bors/received_events", "type": "User", "site_admin": false}, "committer": {"login": "bors", "id": 3372342, "node_id": "MDQ6VXNlcjMzNzIzNDI=", "avatar_url": "https://avatars.githubusercontent.com/u/3372342?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bors", "html_url": "https://github.com/bors", "followers_url": "https://api.github.com/users/bors/followers", "following_url": "https://api.github.com/users/bors/following{/other_user}", "gists_url": "https://api.github.com/users/bors/gists{/gist_id}", "starred_url": "https://api.github.com/users/bors/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bors/subscriptions", "organizations_url": "https://api.github.com/users/bors/orgs", "repos_url": "https://api.github.com/users/bors/repos", "events_url": "https://api.github.com/users/bors/events{/privacy}", "received_events_url": "https://api.github.com/users/bors/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "50503497492e9bab8bc8c5ad3fe403a3a80276d3", "url": "https://api.github.com/repos/rust-lang/rust/commits/50503497492e9bab8bc8c5ad3fe403a3a80276d3", "html_url": "https://github.com/rust-lang/rust/commit/50503497492e9bab8bc8c5ad3fe403a3a80276d3"}, {"sha": "69b9c23b3858dc87ceb6629b7640d5f579b8ed79", "url": "https://api.github.com/repos/rust-lang/rust/commits/69b9c23b3858dc87ceb6629b7640d5f579b8ed79", "html_url": "https://github.com/rust-lang/rust/commit/69b9c23b3858dc87ceb6629b7640d5f579b8ed79"}], "stats": {"total": 295, "additions": 279, "deletions": 16}, "files": [{"sha": "66afe36e7cb9ad7ff4f5a0ccb8e9194cc347be41", "filename": "src/libproc_macro/lib.rs", "status": "modified", "additions": 14, "deletions": 0, "changes": 14, "blob_url": "https://github.com/rust-lang/rust/blob/b5590423e6ceb048dd7d792382e960d66b7615d2/src%2Flibproc_macro%2Flib.rs", "raw_url": "https://github.com/rust-lang/rust/raw/b5590423e6ceb048dd7d792382e960d66b7615d2/src%2Flibproc_macro%2Flib.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibproc_macro%2Flib.rs?ref=b5590423e6ceb048dd7d792382e960d66b7615d2", "patch": "@@ -180,6 +180,20 @@ impl iter::FromIterator<TokenStream> for TokenStream {\n     }\n }\n \n+#[stable(feature = \"token_stream_extend\", since = \"1.30.0\")]\n+impl Extend<TokenTree> for TokenStream {\n+    fn extend<I: IntoIterator<Item = TokenTree>>(&mut self, trees: I) {\n+        self.extend(trees.into_iter().map(TokenStream::from));\n+    }\n+}\n+\n+#[stable(feature = \"token_stream_extend\", since = \"1.30.0\")]\n+impl Extend<TokenStream> for TokenStream {\n+    fn extend<I: IntoIterator<Item = TokenStream>>(&mut self, streams: I) {\n+        self.0.extend(streams.into_iter().map(|stream| stream.0));\n+    }\n+}\n+\n /// Public implementation details for the `TokenStream` type, such as iterators.\n #[stable(feature = \"proc_macro_lib2\", since = \"1.29.0\")]\n pub mod token_stream {"}, {"sha": "0a42325d2b6ff74a595cfbc0fac2d0ec3a66c9e9", "filename": "src/libsyntax/lib.rs", "status": "modified", "additions": 3, "deletions": 0, "changes": 3, "blob_url": "https://github.com/rust-lang/rust/blob/b5590423e6ceb048dd7d792382e960d66b7615d2/src%2Flibsyntax%2Flib.rs", "raw_url": "https://github.com/rust-lang/rust/raw/b5590423e6ceb048dd7d792382e960d66b7615d2/src%2Flibsyntax%2Flib.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Flib.rs?ref=b5590423e6ceb048dd7d792382e960d66b7615d2", "patch": "@@ -130,6 +130,9 @@ pub mod util {\n \n     mod rc_slice;\n     pub use self::rc_slice::RcSlice;\n+\n+    mod rc_vec;\n+    pub use self::rc_vec::RcVec;\n }\n \n pub mod json;"}, {"sha": "840ee299bf338dd302d268ad6fb4a1edb9641a3e", "filename": "src/libsyntax/tokenstream.rs", "status": "modified", "additions": 172, "deletions": 16, "changes": 188, "blob_url": "https://github.com/rust-lang/rust/blob/b5590423e6ceb048dd7d792382e960d66b7615d2/src%2Flibsyntax%2Ftokenstream.rs", "raw_url": "https://github.com/rust-lang/rust/raw/b5590423e6ceb048dd7d792382e960d66b7615d2/src%2Flibsyntax%2Ftokenstream.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Ftokenstream.rs?ref=b5590423e6ceb048dd7d792382e960d66b7615d2", "patch": "@@ -29,7 +29,7 @@ use parse::Directory;\n use parse::token::{self, Token};\n use print::pprust;\n use serialize::{Decoder, Decodable, Encoder, Encodable};\n-use util::RcSlice;\n+use util::RcVec;\n \n use std::borrow::Cow;\n use std::{fmt, iter, mem};\n@@ -221,7 +221,7 @@ impl TokenStream {\n                 new_slice.extend_from_slice(parts.0);\n                 new_slice.push(comma);\n                 new_slice.extend_from_slice(parts.1);\n-                let slice = RcSlice::new(new_slice);\n+                let slice = RcVec::new(new_slice);\n                 return Some((TokenStream { kind: TokenStreamKind::Stream(slice) }, sp));\n             }\n         }\n@@ -234,7 +234,7 @@ enum TokenStreamKind {\n     Empty,\n     Tree(TokenTree),\n     JointTree(TokenTree),\n-    Stream(RcSlice<TokenStream>),\n+    Stream(RcVec<TokenStream>),\n }\n \n impl From<TokenTree> for TokenStream {\n@@ -255,6 +255,60 @@ impl<T: Into<TokenStream>> iter::FromIterator<T> for TokenStream {\n     }\n }\n \n+impl Extend<TokenStream> for TokenStream {\n+    fn extend<I: IntoIterator<Item = TokenStream>>(&mut self, iter: I) {\n+        let iter = iter.into_iter();\n+        let kind = mem::replace(&mut self.kind, TokenStreamKind::Empty);\n+\n+        // Vector of token streams originally in self.\n+        let tts: Vec<TokenStream> = match kind {\n+            TokenStreamKind::Empty => {\n+                let mut vec = Vec::new();\n+                vec.reserve(iter.size_hint().0);\n+                vec\n+            }\n+            TokenStreamKind::Tree(_) | TokenStreamKind::JointTree(_) => {\n+                let mut vec = Vec::new();\n+                vec.reserve(1 + iter.size_hint().0);\n+                vec.push(TokenStream { kind });\n+                vec\n+            }\n+            TokenStreamKind::Stream(rc_vec) => match RcVec::try_unwrap(rc_vec) {\n+                Ok(mut vec) => {\n+                    // Extend in place using the existing capacity if possible.\n+                    // This is the fast path for libraries like `quote` that\n+                    // build a token stream.\n+                    vec.reserve(iter.size_hint().0);\n+                    vec\n+                }\n+                Err(rc_vec) => {\n+                    // Self is shared so we need to copy and extend that.\n+                    let mut vec = Vec::new();\n+                    vec.reserve(rc_vec.len() + iter.size_hint().0);\n+                    vec.extend_from_slice(&rc_vec);\n+                    vec\n+                }\n+            }\n+        };\n+\n+        // Perform the extend, joining tokens as needed along the way.\n+        let mut builder = TokenStreamBuilder(tts);\n+        for stream in iter {\n+            builder.push(stream);\n+        }\n+\n+        // Build the resulting token stream. If it contains more than one token,\n+        // preserve capacity in the vector in anticipation of the caller\n+        // performing additional calls to extend.\n+        let mut tts = builder.0;\n+        *self = match tts.len() {\n+            0 => TokenStream::empty(),\n+            1 => tts.pop().unwrap(),\n+            _ => TokenStream::concat_rc_vec(RcVec::new_preserving_capacity(tts)),\n+        };\n+    }\n+}\n+\n impl Eq for TokenStream {}\n \n impl PartialEq<TokenStream> for TokenStream {\n@@ -287,11 +341,11 @@ impl TokenStream {\n         match streams.len() {\n             0 => TokenStream::empty(),\n             1 => streams.pop().unwrap(),\n-            _ => TokenStream::concat_rc_slice(RcSlice::new(streams)),\n+            _ => TokenStream::concat_rc_vec(RcVec::new(streams)),\n         }\n     }\n \n-    fn concat_rc_slice(streams: RcSlice<TokenStream>) -> TokenStream {\n+    fn concat_rc_vec(streams: RcVec<TokenStream>) -> TokenStream {\n         TokenStream { kind: TokenStreamKind::Stream(streams) }\n     }\n \n@@ -434,7 +488,7 @@ impl TokenStreamBuilder {\n             match len {\n                 1 => {}\n                 2 => self.0.push(streams[0].clone().into()),\n-                _ => self.0.push(TokenStream::concat_rc_slice(streams.sub_slice(0 .. len - 1))),\n+                _ => self.0.push(TokenStream::concat_rc_vec(streams.sub_slice(0 .. len - 1))),\n             }\n             self.push_all_but_last_tree(&streams[len - 1])\n         }\n@@ -446,7 +500,7 @@ impl TokenStreamBuilder {\n             match len {\n                 1 => {}\n                 2 => self.0.push(streams[1].clone().into()),\n-                _ => self.0.push(TokenStream::concat_rc_slice(streams.sub_slice(1 .. len))),\n+                _ => self.0.push(TokenStream::concat_rc_vec(streams.sub_slice(1 .. len))),\n             }\n             self.push_all_but_first_tree(&streams[0])\n         }\n@@ -466,13 +520,13 @@ enum CursorKind {\n \n #[derive(Clone)]\n struct StreamCursor {\n-    stream: RcSlice<TokenStream>,\n+    stream: RcVec<TokenStream>,\n     index: usize,\n-    stack: Vec<(RcSlice<TokenStream>, usize)>,\n+    stack: Vec<(RcVec<TokenStream>, usize)>,\n }\n \n impl StreamCursor {\n-    fn new(stream: RcSlice<TokenStream>) -> Self {\n+    fn new(stream: RcVec<TokenStream>) -> Self {\n         StreamCursor { stream: stream, index: 0, stack: Vec::new() }\n     }\n \n@@ -495,7 +549,7 @@ impl StreamCursor {\n         }\n     }\n \n-    fn insert(&mut self, stream: RcSlice<TokenStream>) {\n+    fn insert(&mut self, stream: RcVec<TokenStream>) {\n         self.stack.push((mem::replace(&mut self.stream, stream),\n                          mem::replace(&mut self.index, 0)));\n     }\n@@ -557,7 +611,7 @@ impl Cursor {\n             CursorKind::Empty => TokenStream::empty(),\n             CursorKind::Tree(ref tree, _) => tree.clone().into(),\n             CursorKind::JointTree(ref tree, _) => tree.clone().joint(),\n-            CursorKind::Stream(ref cursor) => TokenStream::concat_rc_slice({\n+            CursorKind::Stream(ref cursor) => TokenStream::concat_rc_vec({\n                 cursor.stack.get(0).cloned().map(|(stream, _)| stream)\n                     .unwrap_or(cursor.stream.clone())\n             }),\n@@ -607,22 +661,22 @@ impl Cursor {\n /// `ThinTokenStream` is smaller, but needs to allocate to represent a single `TokenTree`.\n /// We must use `ThinTokenStream` in `TokenTree::Delimited` to avoid infinite size due to recursion.\n #[derive(Debug, Clone)]\n-pub struct ThinTokenStream(Option<RcSlice<TokenStream>>);\n+pub struct ThinTokenStream(Option<RcVec<TokenStream>>);\n \n impl From<TokenStream> for ThinTokenStream {\n     fn from(stream: TokenStream) -> ThinTokenStream {\n         ThinTokenStream(match stream.kind {\n             TokenStreamKind::Empty => None,\n-            TokenStreamKind::Tree(tree) => Some(RcSlice::new(vec![tree.into()])),\n-            TokenStreamKind::JointTree(tree) => Some(RcSlice::new(vec![tree.joint()])),\n+            TokenStreamKind::Tree(tree) => Some(RcVec::new(vec![tree.into()])),\n+            TokenStreamKind::JointTree(tree) => Some(RcVec::new(vec![tree.joint()])),\n             TokenStreamKind::Stream(stream) => Some(stream),\n         })\n     }\n }\n \n impl From<ThinTokenStream> for TokenStream {\n     fn from(stream: ThinTokenStream) -> TokenStream {\n-        stream.0.map(TokenStream::concat_rc_slice).unwrap_or_else(TokenStream::empty)\n+        stream.0.map(TokenStream::concat_rc_vec).unwrap_or_else(TokenStream::empty)\n     }\n }\n \n@@ -773,4 +827,106 @@ mod tests {\n         assert_eq!(stream.trees().count(), 1);\n     }\n \n+    #[test]\n+    fn test_extend_empty() {\n+        with_globals(|| {\n+            // Append a token onto an empty token stream.\n+            let mut stream = TokenStream::empty();\n+            stream.extend(vec![string_to_ts(\"t\")]);\n+\n+            let expected = string_to_ts(\"t\");\n+            assert!(stream.eq_unspanned(&expected));\n+        });\n+    }\n+\n+    #[test]\n+    fn test_extend_nothing() {\n+        with_globals(|| {\n+            // Append nothing onto a token stream containing one token.\n+            let mut stream = string_to_ts(\"t\");\n+            stream.extend(vec![]);\n+\n+            let expected = string_to_ts(\"t\");\n+            assert!(stream.eq_unspanned(&expected));\n+        });\n+    }\n+\n+    #[test]\n+    fn test_extend_single() {\n+        with_globals(|| {\n+            // Append a token onto token stream containing a single token.\n+            let mut stream = string_to_ts(\"t1\");\n+            stream.extend(vec![string_to_ts(\"t2\")]);\n+\n+            let expected = string_to_ts(\"t1 t2\");\n+            assert!(stream.eq_unspanned(&expected));\n+        });\n+    }\n+\n+    #[test]\n+    fn test_extend_in_place() {\n+        with_globals(|| {\n+            // Append a token onto token stream containing a reference counted\n+            // vec of tokens. The token stream has a reference count of 1 so\n+            // this can happen in place.\n+            let mut stream = string_to_ts(\"t1 t2\");\n+            stream.extend(vec![string_to_ts(\"t3\")]);\n+\n+            let expected = string_to_ts(\"t1 t2 t3\");\n+            assert!(stream.eq_unspanned(&expected));\n+        });\n+    }\n+\n+    #[test]\n+    fn test_extend_copy() {\n+        with_globals(|| {\n+            // Append a token onto token stream containing a reference counted\n+            // vec of tokens. The token stream is shared so the extend takes\n+            // place on a copy.\n+            let mut stream = string_to_ts(\"t1 t2\");\n+            let _incref = stream.clone();\n+            stream.extend(vec![string_to_ts(\"t3\")]);\n+\n+            let expected = string_to_ts(\"t1 t2 t3\");\n+            assert!(stream.eq_unspanned(&expected));\n+        });\n+    }\n+\n+    #[test]\n+    fn test_extend_no_join() {\n+        with_globals(|| {\n+            let first = TokenTree::Token(DUMMY_SP, Token::Dot);\n+            let second = TokenTree::Token(DUMMY_SP, Token::Dot);\n+\n+            // Append a dot onto a token stream containing a dot, but do not\n+            // join them.\n+            let mut stream = TokenStream::from(first);\n+            stream.extend(vec![TokenStream::from(second)]);\n+\n+            let expected = string_to_ts(\". .\");\n+            assert!(stream.eq_unspanned(&expected));\n+\n+            let unexpected = string_to_ts(\"..\");\n+            assert!(!stream.eq_unspanned(&unexpected));\n+        });\n+    }\n+\n+    #[test]\n+    fn test_extend_join() {\n+        with_globals(|| {\n+            let first = TokenTree::Token(DUMMY_SP, Token::Dot).joint();\n+            let second = TokenTree::Token(DUMMY_SP, Token::Dot);\n+\n+            // Append a dot onto a token stream containing a dot, forming a\n+            // dotdot.\n+            let mut stream = first;\n+            stream.extend(vec![TokenStream::from(second)]);\n+\n+            let expected = string_to_ts(\"..\");\n+            assert!(stream.eq_unspanned(&expected));\n+\n+            let unexpected = string_to_ts(\". .\");\n+            assert!(!stream.eq_unspanned(&unexpected));\n+        });\n+    }\n }"}, {"sha": "99fbce1ad91e1e27106e3a0590d7d9fee7b8d1b7", "filename": "src/libsyntax/util/rc_vec.rs", "status": "added", "additions": 90, "deletions": 0, "changes": 90, "blob_url": "https://github.com/rust-lang/rust/blob/b5590423e6ceb048dd7d792382e960d66b7615d2/src%2Flibsyntax%2Futil%2Frc_vec.rs", "raw_url": "https://github.com/rust-lang/rust/raw/b5590423e6ceb048dd7d792382e960d66b7615d2/src%2Flibsyntax%2Futil%2Frc_vec.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/src%2Flibsyntax%2Futil%2Frc_vec.rs?ref=b5590423e6ceb048dd7d792382e960d66b7615d2", "patch": "@@ -0,0 +1,90 @@\n+// Copyright 2017 The Rust Project Developers. See the COPYRIGHT\n+// file at the top-level directory of this distribution and at\n+// http://rust-lang.org/COPYRIGHT.\n+//\n+// Licensed under the Apache License, Version 2.0 <LICENSE-APACHE or\n+// http://www.apache.org/licenses/LICENSE-2.0> or the MIT license\n+// <LICENSE-MIT or http://opensource.org/licenses/MIT>, at your\n+// option. This file may not be copied, modified, or distributed\n+// except according to those terms.\n+\n+use std::fmt;\n+use std::ops::{Deref, Range};\n+\n+use rustc_data_structures::stable_hasher::{HashStable, StableHasher, StableHasherResult};\n+use rustc_data_structures::sync::Lrc;\n+\n+#[derive(Clone)]\n+pub struct RcVec<T> {\n+    data: Lrc<Vec<T>>,\n+    offset: u32,\n+    len: u32,\n+}\n+\n+impl<T> RcVec<T> {\n+    pub fn new(mut vec: Vec<T>) -> Self {\n+        // By default, constructing RcVec from Vec gives it just enough capacity\n+        // to hold the initial elements. Callers that anticipate needing to\n+        // extend the vector may prefer RcVec::new_preserving_capacity.\n+        vec.shrink_to_fit();\n+        Self::new_preserving_capacity(vec)\n+    }\n+\n+    pub fn new_preserving_capacity(vec: Vec<T>) -> Self {\n+        RcVec {\n+            offset: 0,\n+            len: vec.len() as u32,\n+            data: Lrc::new(vec),\n+        }\n+    }\n+\n+    pub fn sub_slice(&self, range: Range<usize>) -> Self {\n+        RcVec {\n+            data: self.data.clone(),\n+            offset: self.offset + range.start as u32,\n+            len: (range.end - range.start) as u32,\n+        }\n+    }\n+\n+    /// If this RcVec has exactly one strong reference, returns ownership of the\n+    /// underlying vector. Otherwise returns self unmodified.\n+    pub fn try_unwrap(self) -> Result<Vec<T>, Self> {\n+        match Lrc::try_unwrap(self.data) {\n+            // If no other RcVec shares ownership of this data.\n+            Ok(mut vec) => {\n+                // Drop any elements after our view of the data.\n+                vec.truncate(self.offset as usize + self.len as usize);\n+                // Drop any elements before our view of the data. Do this after\n+                // the `truncate` so that elements past the end of our view do\n+                // not need to be copied around.\n+                vec.drain(..self.offset as usize);\n+                Ok(vec)\n+            }\n+\n+            // If the data is shared.\n+            Err(data) => Err(RcVec { data, ..self }),\n+        }\n+    }\n+}\n+\n+impl<T> Deref for RcVec<T> {\n+    type Target = [T];\n+    fn deref(&self) -> &[T] {\n+        &self.data[self.offset as usize..(self.offset + self.len) as usize]\n+    }\n+}\n+\n+impl<T: fmt::Debug> fmt::Debug for RcVec<T> {\n+    fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {\n+        fmt::Debug::fmt(self.deref(), f)\n+    }\n+}\n+\n+impl<CTX, T> HashStable<CTX> for RcVec<T>\n+where\n+    T: HashStable<CTX>,\n+{\n+    fn hash_stable<W: StableHasherResult>(&self, hcx: &mut CTX, hasher: &mut StableHasher<W>) {\n+        (**self).hash_stable(hcx, hasher);\n+    }\n+}"}]}