{"sha": "1a5aa84e9f48f652e584cdbe0393616730ea73d8", "node_id": "C_kwDOAAsO6NoAKDFhNWFhODRlOWY0OGY2NTJlNTg0Y2RiZTAzOTM2MTY3MzBlYTczZDg", "commit": {"author": {"name": "Florian Diebold", "email": "flodiebold@gmail.com", "date": "2022-02-08T17:13:18Z"}, "committer": {"name": "Florian Diebold", "email": "flodiebold@gmail.com", "date": "2022-02-08T17:13:18Z"}, "message": "Track synthetic tokens, to be able to remove them again later", "tree": {"sha": "d6494be52e4b42aaf504285fe157e08996a98236", "url": "https://api.github.com/repos/rust-lang/rust/git/trees/d6494be52e4b42aaf504285fe157e08996a98236"}, "url": "https://api.github.com/repos/rust-lang/rust/git/commits/1a5aa84e9f48f652e584cdbe0393616730ea73d8", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/rust-lang/rust/commits/1a5aa84e9f48f652e584cdbe0393616730ea73d8", "html_url": "https://github.com/rust-lang/rust/commit/1a5aa84e9f48f652e584cdbe0393616730ea73d8", "comments_url": "https://api.github.com/repos/rust-lang/rust/commits/1a5aa84e9f48f652e584cdbe0393616730ea73d8/comments", "author": {"login": "flodiebold", "id": 906069, "node_id": "MDQ6VXNlcjkwNjA2OQ==", "avatar_url": "https://avatars.githubusercontent.com/u/906069?v=4", "gravatar_id": "", "url": "https://api.github.com/users/flodiebold", "html_url": "https://github.com/flodiebold", "followers_url": "https://api.github.com/users/flodiebold/followers", "following_url": "https://api.github.com/users/flodiebold/following{/other_user}", "gists_url": "https://api.github.com/users/flodiebold/gists{/gist_id}", "starred_url": "https://api.github.com/users/flodiebold/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/flodiebold/subscriptions", "organizations_url": "https://api.github.com/users/flodiebold/orgs", "repos_url": "https://api.github.com/users/flodiebold/repos", "events_url": "https://api.github.com/users/flodiebold/events{/privacy}", "received_events_url": "https://api.github.com/users/flodiebold/received_events", "type": "User", "site_admin": false}, "committer": {"login": "flodiebold", "id": 906069, "node_id": "MDQ6VXNlcjkwNjA2OQ==", "avatar_url": "https://avatars.githubusercontent.com/u/906069?v=4", "gravatar_id": "", "url": "https://api.github.com/users/flodiebold", "html_url": "https://github.com/flodiebold", "followers_url": "https://api.github.com/users/flodiebold/followers", "following_url": "https://api.github.com/users/flodiebold/following{/other_user}", "gists_url": "https://api.github.com/users/flodiebold/gists{/gist_id}", "starred_url": "https://api.github.com/users/flodiebold/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/flodiebold/subscriptions", "organizations_url": "https://api.github.com/users/flodiebold/orgs", "repos_url": "https://api.github.com/users/flodiebold/repos", "events_url": "https://api.github.com/users/flodiebold/events{/privacy}", "received_events_url": "https://api.github.com/users/flodiebold/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "1b5cd03a37e5eb8acc87fd5bcee060efd735b598", "url": "https://api.github.com/repos/rust-lang/rust/commits/1b5cd03a37e5eb8acc87fd5bcee060efd735b598", "html_url": "https://github.com/rust-lang/rust/commit/1b5cd03a37e5eb8acc87fd5bcee060efd735b598"}], "stats": {"total": 172, "additions": 133, "deletions": 39}, "files": [{"sha": "7a21e3e870130e7d6dfc47665d03fa6fc77dc46e", "filename": "crates/hir_expand/src/db.rs", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "blob_url": "https://github.com/rust-lang/rust/blob/1a5aa84e9f48f652e584cdbe0393616730ea73d8/crates%2Fhir_expand%2Fsrc%2Fdb.rs", "raw_url": "https://github.com/rust-lang/rust/raw/1a5aa84e9f48f652e584cdbe0393616730ea73d8/crates%2Fhir_expand%2Fsrc%2Fdb.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fhir_expand%2Fsrc%2Fdb.rs?ref=1a5aa84e9f48f652e584cdbe0393616730ea73d8", "patch": "@@ -5,8 +5,8 @@ use std::sync::Arc;\n use base_db::{salsa, SourceDatabase};\n use either::Either;\n use limit::Limit;\n-use mbe::{syntax_node_to_token_tree, ExpandError, ExpandResult, SyntheticToken};\n-use rustc_hash::{FxHashMap, FxHashSet};\n+use mbe::{syntax_node_to_token_tree, ExpandError, ExpandResult};\n+use rustc_hash::FxHashSet;\n use syntax::{\n     algo::diff,\n     ast::{self, HasAttrs, HasDocComments},\n@@ -442,7 +442,7 @@ fn macro_expand(db: &dyn AstDatabase, id: MacroCallId) -> ExpandResult<Option<Ar\n         ));\n     }\n \n-    fixup::reverse_fixups(&mut tt);\n+    fixup::reverse_fixups(&mut tt, &macro_arg.1);\n \n     ExpandResult { value: Some(Arc::new(tt)), err }\n }"}, {"sha": "a4acf9e987e0d371c3be651e4d070f2e3f1ce57f", "filename": "crates/hir_expand/src/fixup.rs", "status": "modified", "additions": 63, "deletions": 24, "changes": 87, "blob_url": "https://github.com/rust-lang/rust/blob/1a5aa84e9f48f652e584cdbe0393616730ea73d8/crates%2Fhir_expand%2Fsrc%2Ffixup.rs", "raw_url": "https://github.com/rust-lang/rust/raw/1a5aa84e9f48f652e584cdbe0393616730ea73d8/crates%2Fhir_expand%2Fsrc%2Ffixup.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fhir_expand%2Fsrc%2Ffixup.rs?ref=1a5aa84e9f48f652e584cdbe0393616730ea73d8", "patch": "@@ -1,10 +1,10 @@\n-use mbe::SyntheticToken;\n+use mbe::{SyntheticToken, SyntheticTokenId, TokenMap};\n use rustc_hash::FxHashMap;\n use syntax::{\n     ast::{self, AstNode},\n-    match_ast, SyntaxKind, SyntaxNode, SyntaxToken,\n+    match_ast, SyntaxKind, SyntaxNode, TextRange,\n };\n-use tt::{Leaf, Subtree};\n+use tt::Subtree;\n \n #[derive(Debug)]\n pub struct SyntaxFixups {\n@@ -16,6 +16,7 @@ pub fn fixup_syntax(node: &SyntaxNode) -> SyntaxFixups {\n     let mut append = FxHashMap::default();\n     let mut replace = FxHashMap::default();\n     let mut preorder = node.preorder();\n+    let empty_id = SyntheticTokenId(0);\n     while let Some(event) = preorder.next() {\n         let node = match event {\n             syntax::WalkEvent::Enter(node) => node,\n@@ -27,12 +28,32 @@ pub fn fixup_syntax(node: &SyntaxNode) -> SyntaxFixups {\n             preorder.skip_subtree();\n             continue;\n         }\n+        let end_range = TextRange::empty(node.text_range().end());\n         match_ast! {\n             match node {\n                 ast::FieldExpr(it) => {\n                     if it.name_ref().is_none() {\n                         // incomplete field access: some_expr.|\n-                        append.insert(node.clone(), vec![(SyntaxKind::IDENT, \"__ra_fixup\".into())]);\n+                        append.insert(node.clone(), vec![\n+                            SyntheticToken {\n+                                kind: SyntaxKind::IDENT,\n+                                text: \"__ra_fixup\".into(),\n+                                range: end_range,\n+                                id: empty_id,\n+                            },\n+                        ]);\n+                    }\n+                },\n+                ast::ExprStmt(it) => {\n+                    if it.semicolon_token().is_none() {\n+                        append.insert(node.clone(), vec![\n+                            SyntheticToken {\n+                                kind: SyntaxKind::SEMICOLON,\n+                                text: \";\".into(),\n+                                range: end_range,\n+                                id: empty_id,\n+                            },\n+                        ]);\n                     }\n                 },\n                 _ => (),\n@@ -42,28 +63,29 @@ pub fn fixup_syntax(node: &SyntaxNode) -> SyntaxFixups {\n     SyntaxFixups { append, replace }\n }\n \n-pub fn reverse_fixups(tt: &mut Subtree) {\n+pub fn reverse_fixups(tt: &mut Subtree, token_map: &TokenMap) {\n+    eprintln!(\"token_map: {:?}\", token_map);\n     tt.token_trees.retain(|tt| match tt {\n-        tt::TokenTree::Leaf(Leaf::Ident(ident)) => ident.text != \"__ra_fixup\",\n+        tt::TokenTree::Leaf(leaf) => token_map.synthetic_token_id(leaf.id()).is_none(),\n         _ => true,\n     });\n     tt.token_trees.iter_mut().for_each(|tt| match tt {\n-        tt::TokenTree::Subtree(tt) => reverse_fixups(tt),\n+        tt::TokenTree::Subtree(tt) => reverse_fixups(tt, token_map),\n         _ => {}\n     });\n }\n \n #[cfg(test)]\n mod tests {\n-    use expect_test::{Expect, expect};\n+    use expect_test::{expect, Expect};\n \n     use super::reverse_fixups;\n \n     #[track_caller]\n     fn check(ra_fixture: &str, mut expect: Expect) {\n         let parsed = syntax::SourceFile::parse(ra_fixture);\n         let fixups = super::fixup_syntax(&parsed.syntax_node());\n-        let (mut tt, _tmap) = mbe::syntax_node_to_token_tree_censored(\n+        let (mut tt, tmap) = mbe::syntax_node_to_token_tree_censored(\n             &parsed.syntax_node(),\n             fixups.replace,\n             fixups.append,\n@@ -77,9 +99,14 @@ mod tests {\n \n         // the fixed-up tree should be syntactically valid\n         let (parse, _) = mbe::token_tree_to_syntax_node(&tt, ::mbe::TopEntryPoint::MacroItems);\n-        assert_eq!(parse.errors(), &[], \"parse has syntax errors. parse tree:\\n{:#?}\", parse.syntax_node());\n+        assert_eq!(\n+            parse.errors(),\n+            &[],\n+            \"parse has syntax errors. parse tree:\\n{:#?}\",\n+            parse.syntax_node()\n+        );\n \n-        reverse_fixups(&mut tt);\n+        reverse_fixups(&mut tt, &tmap);\n \n         // the fixed-up + reversed version should be equivalent to the original input\n         // (but token IDs don't matter)\n@@ -89,48 +116,60 @@ mod tests {\n \n     #[test]\n     fn incomplete_field_expr_1() {\n-        check(r#\"\n+        check(\n+            r#\"\n fn foo() {\n     a.\n }\n-\"#, expect![[r#\"\n+\"#,\n+            expect![[r#\"\n fn foo () {a . __ra_fixup}\n-\"#]])\n+\"#]],\n+        )\n     }\n \n     #[test]\n     fn incomplete_field_expr_2() {\n-        check(r#\"\n+        check(\n+            r#\"\n fn foo() {\n     a. ;\n }\n-\"#, expect![[r#\"\n+\"#,\n+            expect![[r#\"\n fn foo () {a . __ra_fixup ;}\n-\"#]])\n+\"#]],\n+        )\n     }\n \n     #[test]\n     fn incomplete_field_expr_3() {\n-        check(r#\"\n+        check(\n+            r#\"\n fn foo() {\n     a. ;\n     bar();\n }\n-\"#, expect![[r#\"\n+\"#,\n+            expect![[r#\"\n fn foo () {a . __ra_fixup ; bar () ;}\n-\"#]])\n+\"#]],\n+        )\n     }\n \n     #[test]\n     fn field_expr_before_call() {\n         // another case that easily happens while typing\n-        check(r#\"\n+        check(\n+            r#\"\n fn foo() {\n     a.b\n     bar();\n }\n-\"#, expect![[r#\"\n-fn foo () {a . b bar () ;}\n-\"#]])\n+\"#,\n+            expect![[r#\"\n+fn foo () {a . b ; bar () ;}\n+\"#]],\n+        )\n     }\n }"}, {"sha": "a35c22c2e1137588469288eb6ce8c842911201e4", "filename": "crates/mbe/src/lib.rs", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "blob_url": "https://github.com/rust-lang/rust/blob/1a5aa84e9f48f652e584cdbe0393616730ea73d8/crates%2Fmbe%2Fsrc%2Flib.rs", "raw_url": "https://github.com/rust-lang/rust/raw/1a5aa84e9f48f652e584cdbe0393616730ea73d8/crates%2Fmbe%2Fsrc%2Flib.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fmbe%2Fsrc%2Flib.rs?ref=1a5aa84e9f48f652e584cdbe0393616730ea73d8", "patch": "@@ -31,6 +31,7 @@ pub use crate::{\n     syntax_bridge::{\n         parse_exprs_with_sep, parse_to_token_tree, syntax_node_to_token_tree,\n         syntax_node_to_token_tree_censored, token_tree_to_syntax_node, SyntheticToken,\n+        SyntheticTokenId,\n     },\n     token_map::TokenMap,\n };"}, {"sha": "7feaaaa62d8b1d6e769210e6995c7825d0866eae", "filename": "crates/mbe/src/syntax_bridge.rs", "status": "modified", "additions": 45, "deletions": 12, "changes": 57, "blob_url": "https://github.com/rust-lang/rust/blob/1a5aa84e9f48f652e584cdbe0393616730ea73d8/crates%2Fmbe%2Fsrc%2Fsyntax_bridge.rs", "raw_url": "https://github.com/rust-lang/rust/raw/1a5aa84e9f48f652e584cdbe0393616730ea73d8/crates%2Fmbe%2Fsrc%2Fsyntax_bridge.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fmbe%2Fsrc%2Fsyntax_bridge.rs?ref=1a5aa84e9f48f652e584cdbe0393616730ea73d8", "patch": "@@ -1,6 +1,6 @@\n //! Conversions between [`SyntaxNode`] and [`tt::TokenTree`].\n \n-use rustc_hash::{FxHashMap, FxHashSet};\n+use rustc_hash::FxHashMap;\n use stdx::{always, non_empty_vec::NonEmptyVec};\n use syntax::{\n     ast::{self, make::tokens::doc_comment},\n@@ -35,7 +35,16 @@ pub fn syntax_node_to_token_tree_censored(\n     (subtree, c.id_alloc.map)\n }\n \n-pub type SyntheticToken = (SyntaxKind, SmolStr);\n+#[derive(Clone, Copy, Debug, PartialEq, Eq, Hash)]\n+pub struct SyntheticTokenId(pub u32);\n+\n+#[derive(Debug, Clone)]\n+pub struct SyntheticToken {\n+    pub kind: SyntaxKind,\n+    pub text: SmolStr,\n+    pub range: TextRange,\n+    pub id: SyntheticTokenId,\n+}\n \n // The following items are what `rustc` macro can be parsed into :\n // link: https://github.com/rust-lang/rust/blob/9ebf47851a357faa4cd97f4b1dc7835f6376e639/src/libsyntax/ext/expand.rs#L141\n@@ -153,13 +162,14 @@ fn convert_tokens<C: TokenConvertor>(conv: &mut C) -> tt::Subtree {\n             Some(it) => it,\n             None => break,\n         };\n+        let synth_id = token.synthetic_id(&conv);\n \n         let kind = token.kind(&conv);\n         if kind == COMMENT {\n             if let Some(tokens) = conv.convert_doc_comment(&token) {\n                 // FIXME: There has to be a better way to do this\n                 // Add the comments token id to the converted doc string\n-                let id = conv.id_alloc().alloc(range);\n+                let id = conv.id_alloc().alloc(range, synth_id);\n                 result.extend(tokens.into_iter().map(|mut tt| {\n                     if let tt::TokenTree::Subtree(sub) = &mut tt {\n                         if let Some(tt::TokenTree::Leaf(tt::Leaf::Literal(lit))) =\n@@ -174,7 +184,7 @@ fn convert_tokens<C: TokenConvertor>(conv: &mut C) -> tt::Subtree {\n             continue;\n         }\n         let tt = if kind.is_punct() && kind != UNDERSCORE {\n-            assert_eq!(range.len(), TextSize::of('.'));\n+            // assert_eq!(range.len(), TextSize::of('.'));\n \n             if let Some(delim) = subtree.delimiter {\n                 let expected = match delim.kind {\n@@ -226,11 +236,13 @@ fn convert_tokens<C: TokenConvertor>(conv: &mut C) -> tt::Subtree {\n                     panic!(\"Token from lexer must be single char: token = {:#?}\", token);\n                 }\n             };\n-            tt::Leaf::from(tt::Punct { char, spacing, id: conv.id_alloc().alloc(range) }).into()\n+            tt::Leaf::from(tt::Punct { char, spacing, id: conv.id_alloc().alloc(range, synth_id) })\n+                .into()\n         } else {\n             macro_rules! make_leaf {\n                 ($i:ident) => {\n-                    tt::$i { id: conv.id_alloc().alloc(range), text: token.to_text(conv) }.into()\n+                    tt::$i { id: conv.id_alloc().alloc(range, synth_id), text: token.to_text(conv) }\n+                        .into()\n                 };\n             }\n             let leaf: tt::Leaf = match kind {\n@@ -245,14 +257,14 @@ fn convert_tokens<C: TokenConvertor>(conv: &mut C) -> tt::Subtree {\n                     let apostrophe = tt::Leaf::from(tt::Punct {\n                         char: '\\'',\n                         spacing: tt::Spacing::Joint,\n-                        id: conv.id_alloc().alloc(r),\n+                        id: conv.id_alloc().alloc(r, synth_id),\n                     });\n                     result.push(apostrophe.into());\n \n                     let r = TextRange::at(range.start() + char_unit, range.len() - char_unit);\n                     let ident = tt::Leaf::from(tt::Ident {\n                         text: SmolStr::new(&token.to_text(conv)[1..]),\n-                        id: conv.id_alloc().alloc(r),\n+                        id: conv.id_alloc().alloc(r, synth_id),\n                     });\n                     result.push(ident.into());\n                     continue;\n@@ -273,7 +285,7 @@ fn convert_tokens<C: TokenConvertor>(conv: &mut C) -> tt::Subtree {\n \n         conv.id_alloc().close_delim(entry.idx, None);\n         let leaf: tt::Leaf = tt::Punct {\n-            id: conv.id_alloc().alloc(entry.open_range),\n+            id: conv.id_alloc().alloc(entry.open_range, None),\n             char: match entry.subtree.delimiter.unwrap().kind {\n                 tt::DelimiterKind::Parenthesis => '(',\n                 tt::DelimiterKind::Brace => '{',\n@@ -367,11 +379,18 @@ struct TokenIdAlloc {\n }\n \n impl TokenIdAlloc {\n-    fn alloc(&mut self, absolute_range: TextRange) -> tt::TokenId {\n+    fn alloc(\n+        &mut self,\n+        absolute_range: TextRange,\n+        synthetic_id: Option<SyntheticTokenId>,\n+    ) -> tt::TokenId {\n         let relative_range = absolute_range - self.global_offset;\n         let token_id = tt::TokenId(self.next_id);\n         self.next_id += 1;\n         self.map.insert(token_id, relative_range);\n+        if let Some(id) = synthetic_id {\n+            self.map.insert_synthetic(token_id, id);\n+        }\n         token_id\n     }\n \n@@ -411,6 +430,8 @@ trait SrcToken<Ctx>: std::fmt::Debug {\n     fn to_char(&self, ctx: &Ctx) -> Option<char>;\n \n     fn to_text(&self, ctx: &Ctx) -> SmolStr;\n+\n+    fn synthetic_id(&self, ctx: &Ctx) -> Option<SyntheticTokenId>;\n }\n \n trait TokenConvertor: Sized {\n@@ -437,6 +458,10 @@ impl<'a> SrcToken<RawConvertor<'a>> for usize {\n     fn to_text(&self, ctx: &RawConvertor<'_>) -> SmolStr {\n         ctx.lexed.text(*self).into()\n     }\n+\n+    fn synthetic_id(&self, _ctx: &RawConvertor<'a>) -> Option<SyntheticTokenId> {\n+        None\n+    }\n }\n \n impl<'a> TokenConvertor for RawConvertor<'a> {\n@@ -564,21 +589,29 @@ impl SrcToken<Convertor> for SynToken {\n         match self {\n             SynToken::Ordinary(token) => token.kind(),\n             SynToken::Punch(token, _) => token.kind(),\n-            SynToken::Synthetic((kind, _)) => *kind,\n+            SynToken::Synthetic(token) => token.kind,\n         }\n     }\n     fn to_char(&self, _ctx: &Convertor) -> Option<char> {\n         match self {\n             SynToken::Ordinary(_) => None,\n             SynToken::Punch(it, i) => it.text().chars().nth((*i).into()),\n+            SynToken::Synthetic(token) if token.text.len() == 1 => token.text.chars().next(),\n             SynToken::Synthetic(_) => None,\n         }\n     }\n     fn to_text(&self, _ctx: &Convertor) -> SmolStr {\n         match self {\n             SynToken::Ordinary(token) => token.text().into(),\n             SynToken::Punch(token, _) => token.text().into(),\n-            SynToken::Synthetic((_, text)) => text.clone(),\n+            SynToken::Synthetic(token) => token.text.clone(),\n+        }\n+    }\n+\n+    fn synthetic_id(&self, _ctx: &Convertor) -> Option<SyntheticTokenId> {\n+        match self {\n+            SynToken::Synthetic(token) => Some(token.id),\n+            _ => None,\n         }\n     }\n }"}, {"sha": "ee1090945cb2d4190318fdd1998191f2563cdaa1", "filename": "crates/mbe/src/token_map.rs", "status": "modified", "additions": 11, "deletions": 0, "changes": 11, "blob_url": "https://github.com/rust-lang/rust/blob/1a5aa84e9f48f652e584cdbe0393616730ea73d8/crates%2Fmbe%2Fsrc%2Ftoken_map.rs", "raw_url": "https://github.com/rust-lang/rust/raw/1a5aa84e9f48f652e584cdbe0393616730ea73d8/crates%2Fmbe%2Fsrc%2Ftoken_map.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Fmbe%2Fsrc%2Ftoken_map.rs?ref=1a5aa84e9f48f652e584cdbe0393616730ea73d8", "patch": "@@ -5,6 +5,8 @@ use std::hash::Hash;\n use parser::{SyntaxKind, T};\n use syntax::{TextRange, TextSize};\n \n+use crate::syntax_bridge::SyntheticTokenId;\n+\n #[derive(Debug, PartialEq, Eq, Clone, Copy, Hash)]\n enum TokenTextRange {\n     Token(TextRange),\n@@ -31,6 +33,7 @@ impl TokenTextRange {\n pub struct TokenMap {\n     /// Maps `tt::TokenId` to the *relative* source range.\n     entries: Vec<(tt::TokenId, TokenTextRange)>,\n+    pub synthetic_entries: Vec<(tt::TokenId, SyntheticTokenId)>,\n }\n \n impl TokenMap {\n@@ -57,6 +60,10 @@ impl TokenMap {\n             .filter_map(move |(_, range)| range.by_kind(kind))\n     }\n \n+    pub fn synthetic_token_id(&self, token_id: tt::TokenId) -> Option<SyntheticTokenId> {\n+        self.synthetic_entries.iter().find(|(tid, _)| *tid == token_id).map(|(_, id)| *id)\n+    }\n+\n     pub fn first_range_by_token(\n         &self,\n         token_id: tt::TokenId,\n@@ -73,6 +80,10 @@ impl TokenMap {\n         self.entries.push((token_id, TokenTextRange::Token(relative_range)));\n     }\n \n+    pub(crate) fn insert_synthetic(&mut self, token_id: tt::TokenId, id: SyntheticTokenId) {\n+        self.synthetic_entries.push((token_id, id));\n+    }\n+\n     pub(crate) fn insert_delim(\n         &mut self,\n         token_id: tt::TokenId,"}, {"sha": "0316b15038cec0cc7f5170324517e50e62ab51a4", "filename": "crates/tt/src/lib.rs", "status": "modified", "additions": 10, "deletions": 0, "changes": 10, "blob_url": "https://github.com/rust-lang/rust/blob/1a5aa84e9f48f652e584cdbe0393616730ea73d8/crates%2Ftt%2Fsrc%2Flib.rs", "raw_url": "https://github.com/rust-lang/rust/raw/1a5aa84e9f48f652e584cdbe0393616730ea73d8/crates%2Ftt%2Fsrc%2Flib.rs", "contents_url": "https://api.github.com/repos/rust-lang/rust/contents/crates%2Ftt%2Fsrc%2Flib.rs?ref=1a5aa84e9f48f652e584cdbe0393616730ea73d8", "patch": "@@ -87,6 +87,16 @@ pub struct Ident {\n     pub id: TokenId,\n }\n \n+impl Leaf {\n+    pub fn id(&self) -> TokenId {\n+        match self {\n+            Leaf::Literal(l) => l.id,\n+            Leaf::Punct(p) => p.id,\n+            Leaf::Ident(i) => i.id,\n+        }\n+    }\n+}\n+\n fn print_debug_subtree(f: &mut fmt::Formatter<'_>, subtree: &Subtree, level: usize) -> fmt::Result {\n     let align = \"  \".repeat(level);\n "}]}